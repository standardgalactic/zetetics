arXiv:1812.05878v2  [math.CO]  28 Feb 2019
In Praise of Sequence (Co-)Algebra and its implementation in
Haskell
Kieran Clenaghan
Abstract
What is Sequence Algebra?
This is a question that any teacher or student of
mathematics or computer science can engage with. Sequences are in Calculus, Combi-
natorics, Statistics and Computation. They are foundational, a step up from number
arithmetic. Sequence operations are easy to implement from scratch (in Haskell) and
aﬀord a wide variety of testing and experimentation. When bits and pieces of sequence
algebra are pulled together from the literature, there emerges a claim for status as a
substantial pre-analysis topic. Here we set the stage by bringing together a variety
of sequence algebra concepts for the ﬁrst time in one paper. This provides a novel
economical overview, intended to invite a broad mathematical audience to cast an eye
over the subject. A complete, yet succinct, basic implementation of sequence opera-
tions is presented, ready to play with. The implementation also serves as a benchmark
for introducing Haskell by mathematical example.
1
Introduction
Consider these titles: Formal Power Series [64], Power Series, Power Serious [57], A
Coinductive Calculus of Streams [72], and Concrete Stream Calculus [39]. The mixing of
the classical and the modern in these papers is stimulating and suggests a re-telling of the
elementary theory and application of sequences. Casting our net wider than the citations
in those four papers, brings up a number of corroborating works, including [85, 8, 83, 84],
in which the authors call attention to the intrinsic qualities and utility of an elementary
calculus or algebra of sequences. We do more than re-advertise this work – we endeavour
to tease out the common ground, emphasising economy of statement and notation, whilst
embracing variety of approach.
Our aim is to attract those who are less well acquainted with sequence work, or those
who are unfamiliar with Haskell or both. It is so that others can enjoy “messing about”, as
Hayman [34] might put it, with sequences and their implementation. Elementary sequence
algebra provides a good answer to the question, ‘What is the smallest coherent chunk of
mathematics to set undergraduates to implement, from scratch, so that they get the
greatest reward?’.
First we must say that sequence algebra is an umbrella title for algebraic manip-
ulations of ﬁnite and inﬁnite sequences, p = [p0, p1, . . . , pn], f = [f0, f1, . . .], over some
given element set, F. It encompasses, prominently, formal power series algebra, but is not
restricted to it. A ﬁnite sequence, viewed as a sequence of coeﬃcients for powers of x, can
be expressed as a formal polynomial. Thus [0, 1] = 0x0 + 1x1 = x, or we may say that x
1

is implemented by [0, 1]. Similarly, 3x2 −x + 4 = [4, −1, 3]. A ﬁnite sequence can also be
interpreted as an inﬁnite sequence by appending an inﬁnite sequence of zeros. An inﬁnite
sequence can be expressed as a formal power series:
f =
∞
X
i=0
fixi =
X
i
fixi = [f0, f1, f2, . . .]
We write fn or [xn]f for the element at position n, called the nth term of f; it is the
coeﬃcient of xn in the power series view. The zeroth term can also be written f(0), but
otherwise the notation f(n) is reserved for function application: let p = 3x2 + 4, then
p2 = 3, but p(2) = 16; contrast p0 = p(0) = 4. This example reveals that the symbol p is
overloaded, it stands for a function of type N →F and also for another of type F →F,
and we must be careful to distinguish them.
It seems that in spite of Niven’s paper [64] receiving the Lester R Ford award, the
algebra of formal power series (FPS) has not entered the standard introductory university
texts in any substantial way. Niven’s paper has, however, been a key reference for the
ﬁrst chapters of both [36, 31], neither of which is standard undergraduate fare. In general,
we ﬁnd that elements of FPS/sequence algebra appear throughout a vast literature, but
the algebra itself is treated minimally, perfunctorily, or is taken for granted, as might be
expected in research work [21] and specialist texts [61, 2]. It also comes under generat-
ingfunctionlogy [86, 32]. The term “generating function” has proven to be a bit awkward,
because of the “function” part.
Notice how many times Wilf [86] and others have to
remind readers when convergence is not as issue. So, where some might use “generating
function”, we use the plain “sequence expression”. The term “generating function” is more
applicable when an analytic function interpretation is intended [19, Ch. VII]. However,
we freely use the standard names of core analytic functions for their Taylor sequences,
but rather than say, exp(x), we use just exp for the sequence.
The extent of material that ﬁts into elementary sequence algebra is perhaps under-
appreciated. Our goal is to raise appreciation through a modest “survey” of examples,
presented in section 3. Various notations and proof-styles appear in the literature, and an
eﬀort has been made to be inclusive and to harmonise. The one-word term “sequence” is
often preferred to the three-word “formal power series”, but both have their merits, the
latter being preferred in the multivariate case, and when formal variable substitution is
involved.
The sequence algebra we exhibit is on the same elementary level as Niven’s paper
[64]. Much detail has to be omitted so that we can cover more examples. Enough detail
is included to convey the foundational concreteness of the topic, and omitted detail is in
the literature. The Haskell implementation is given in full in section 4 so that the reader
can be in no doubt about how succinct it is, and can type it all up (or download it), and
“own” it. The more mathematically-inclined reader may dwell on section 3, and reﬂect
on the potential of a sequence algebra topic in the mathematical curriculum. The more
programming-inclined may dwell on sections 4, 5 and 6, and engage with the proposition
that sequence algebra provides an excellent vehicle for exploring learning-mathematics-
through-programming, or vice-versa. There is great scope for making a contribution to
the consolidation, reﬁnement, and application of sequence algebra as an introductory
subject.
2

To help catch, at a glance, some of the things that come under sequence algebra, we
have included a number of tables. Tables 1, 2 and 3 are instantly recognisable as belonging
to a calculus text, but here, uncommonly, the objects being related are sequences, not
analytic functions. Of course, they herald identities that hold for analytic functions, but
in agreement with Niven [64] and Tutte [84], there is something to celebrate in the fact
that the identities can be established on very elementary grounds.
There is more to
celebrate in tables 4 and 5, because many of the sequence expressions therein have a dual
interpretation as a set-theoretic structure speciﬁcation [25]. Yet more satisfaction is to be
had from table 7, because the solutions to the deﬁning diﬀerential equations for the core
sequences transliterate trivially into Haskell deﬁnitions [58].
Therefore a grounding in sequence algebra and its implementation surely pays oﬀ.
This claim is validated at least in the study of the two texts, Concrete Mathematics
[32] and Analytic Combinatorics [25]. There we see numerous examples where sequence
algebra is at play, and the implementation can be used for testing, for reinforcement, or
just for fun. Some of these appear in the next section. For example, in item W we derive
the bivariate power series S(z, u) = exp ◦uz −1
exp ◦z −1
that appears in [32, sect. 7.6].
This
generates a sequence ˘S such that ˘Sm is a polynomial, and ˘Sm(n) =
X
0≤k<n
km. That is,
˘S = [x, −1
2x + 1
2x2, 1
6x −1
2x2 + 1
3x3, . . .]. It is striking how accessible the mathematics
behind S(z, u) is, and how easy it is to deﬁne the inﬁnite S(z, u) and ˘S in a program.
2
The basics
Two sequences f and g are equal if they are equal at all indices: ∀n ≥0.[xn]f = [xn]g.
There are many instances when it is obvious that a statement is subject to universal
quantiﬁcation, and in such cases we leave the ∀part to be inferred by the reader. Becoming
ﬂuent with the clumsy-looking coeﬃcient extraction operator, [xn], pays dividends [31, 51].
It obeys the precedence rules, [xn]f + g = ([xn]f) + g, [xn]fg = [xn](fg), and [xn]f ◦g =
[xn](f ◦g). Moreover, it is a linear operator:
[xn](f + g) = [xn]f + [xn]g;
[xn]cf = c[xn]f
c is a constant
The generalisation [umzn] will be used to identify a term in a bivariate sequence. Let E
be the tail or shift-left operator: E[f0, f1, f2, . . .] = [f1, f2, . . .]. In formal power series
language, E f = 1
x(f −f0), and f = f0 + xE f; this is referred to as the head-tail property
(in [72] it is the “fundamental theorem of stream calculus”, see section 3, item P). We also
have [xn+1]f = [xn]E f and the head-tail expansion rule, [xn]f = [x0]Enf. We are free
to mix notations – here is the deﬁnition of convolution product, f ∗g, in which, typically,
the ∗is suppressed:
[xn]fg =
n
X
k=0
fkgn−k
3

1.
D a
=
0
constant
2.
D x
=
1
variable
3.
D (f + g)
=
D f + D g
sum
4.
D (fg)
=
(D f)g + f(D g)
product
5.
D (f −1)
=
(−D f)/f 2
reciprocal
6.
D (f/g)
=
((D f)g −f(D g))/g2
quotient
7.
D (f ◦)
=
((D f) ◦f ◦)−1
converse
8.
D (f n)
=
n(D f)f n−1
power
9.
D (anxn)
=
nanxn−1
monomial
10.
[xn]D
=
(n + 1)[xn+1]
power-series
11.
D (f ◦g)
=
((D f) ◦g)(D g)
composition
12.
[xn]f
=
1
n![x0]Dnf
Maclaurin
Table 1: diﬀerentiation rules
Observe that, since x1 = 1 is the only non-zero element of x = [0, 1], we have
[x0]xf = 0;
[xn]xf = x1fn−1 = fn−1,
for n > 0
Thus x[f0, f1, f2, . . .] = [0, 1] ∗[f0, f1, f2, . . .] = [0, f0, f1, f2, . . .], and x can be viewed as a
right-shift operator. The absorption law, [xn]xmf = [xn−m]f, is simple but eﬀective. The
product in both f = f0 + xE f and f = P
i fixi is convolution product, + is pointwise
addition, the element fi is automatically identiﬁed with the singleton sequence, [fi], as
required, and x = [0, 1]. A recursive equation for product is easily derived (and just as
easily translated into Haskell); E f is abbreviated to f ′:
fg = (f0 + xf ′)(g0 + xg′) = f0g0 + f0xg′ + xf ′g = f0g0 + x(f0g′ + f ′g)
(1)
Sequences, S, over an integral domain or ﬁeld F (known from context) form an integral
domain [64, 55] (F[[x]] is the standard notation for formal power series in x over F). We
will keep F = Q in mind. The subsets S0, S1, and S̸=0 comprise sequences with zeroth
term 0, 1, and non-zero, respectively. A subset SC of S0 comprises sequences f in which
f1 ̸= 0, that is E f ∈S̸=0. Unique square (and nth) roots exist for sequences in S1. Se-
quence composition f ◦g = P
k fkgk is deﬁned for g ∈S0. A unique compositional inverse,
f ◦, called converse, exists for f ∈SC. The notation follows [6] and distinguishes converse
f ◦f ◦= x from multiplicative inverse f ∗f −1 = 1. The latter exists for f ∈S̸=0. Diﬀer-
entiation, D, is term-wise, as for formal polynomials: [xn]D f = (n + 1)[xn+1]f. A little
induction gives the Maclaurin expansion rule: [xn]f = 1
n![x0]Dnf. From the deﬁnition of
R
as a right inverse to D, [xn]D
R
f = [xn]f, we deduce [xn+1]
R
f =
1
n + 1[xn]f. Setting
[x0]
R
f = 0, the fundamental theorem of sequence calculus (FTC) is immediate:
f = f0 +
Z
Df
D(
Z
f) = f
The familiar rules in tables 1 and 2 have easy sequence-algebraic proofs (the third
R
-
product rule is called the diﬀerential Baxter axiom in [10]). For example, here is a typical
4

1.
Z
anxn
=
an
n + 1xn+1
monomial
2.
Z
(af + bg)
=
a
Z
f + b
Z
g
linear
3.
(n + 1)[xn+1]
Z
=
[xn]
power series
4.
Z
((D f) ◦g)aD g
=
af ◦g + c
composition
5.
Z
fD g
=
fg −
Z
(D f)g + c
product (1)
6.
Z
fg
=
f
Z
g −
Z
((D f)
Z
g)
product (2)
7.
Z
(D f)
Z
(D h) +
Z
D (fh)
=
h
Z
D f + f
Z
D h
Baxter
Table 2: integration rules
proof of the diﬀerential composition rule, or chain rule [36, Ch. 1]. A proof by coinduction
is presented for contrast in item Q of the next section. Let f ∈S, g ∈S0, then,
D f ◦g
=
f1 + 2f2g + 3f3g2 + · · · + kfkgk−1 + · · ·
(D f ◦g)D g
=
f1D g + 2f2gD g + 3f3g2D g + · · · + kfkgk−1D g + · · ·
=
{the power rule, kgk−1D g = D gk}
f1D g + f2D g2 + f3D g3 + · · ·
[xn](D f ◦g)D g
=
{distribute [xn], use [xn]D gk = (n + 1)[xn+1]gk}
(n + 1)[xn+1](f1g1 + f2g2 + f3g3 + · · · + fn+1gn+1)
=
(n + 1)[xn+1]f ◦g
=
[xn]D(f ◦g)
3
Sequence Algebra examples
The following itemization (A-Z) of snippets provides a brief survey of the character of
sequence algebra. It is, of course, only a small fraction of the subject.
(A) Sequence algebra is foundational in the sense that it is a low-level concrete extension
of arithmetic. To appreciate this, try making the following simpler. Deﬁne exp by the
sequence diﬀerential equation D exp = exp; exp0 = 1.
Then, by the Maclaurin rule,
exp = 1 + x/1! + x2/2! + x3/3! + · · · = [1, 1, 1/2, 1/3!, . . .]. Deﬁne x∗by the sequence
diﬀerence equation E x∗= x∗; x∗
0 = 1; then, by the head-tail property, x∗= 1 + x +
x2 + · · · = [1, 1, 1, . . .] = 1/(1 −x); the notation is based on the Kleene star [19]. Let
log f = lgn◦(f −1) where lgn (for which there is no established name other than log(1+x))
is the converse of exp −1; that is, lgn = (exp −1)◦. Observe that (exp −1)◦lgn = x implies
exp ◦lgn = 1 + x, so D lgn = D (exp −1)◦= (exp ◦lgn)−1 = 1/(1 + x). The FTC gives
5

exp ◦(log f)
=
f
exp converse
lgn ◦f
=
lgn ◦g
⇒
f = g
lgn cancellation
exp ◦f
=
exp ◦g
⇒
f = g
exp cancellation
log f
=
log g
⇒
f = g
log cancellation
D (log g)
=
D g
g
log derivative
lgn ◦f
=
0
⇔
f = 0
zero lgn
log g
=
0
⇔
g = 1
zero log
log(fg)
=
log f + log g
log product
log f r
=
r log f
log rational power
(r ∈Q)
exp ◦(f + g)
=
(exp ◦f)(exp ◦g)
exp sum
expn ◦f
=
exp ◦nf
power exp
f r
=
exp ◦(r log f)
general power defn
(r ∈F)
gr ◦h
=
(g ◦h)r
general distributivity
(r ∈F)
f rf s
=
f r+s
law of exponents
(r, s ∈F)
D f r
=
rf r−1D f
diﬀerential F-power
(r ∈F)
Table 3: Rules relating to log and exp
lgn =
R
1/(1 + x), and the coeﬃcients can be calculated:
[xn+1]lgn
=
[xn+1]
Z
1
1 + x =
1
n + 1[xn]
1
1 + x =
1
n + 1(−1)n
lgn
=
x −x2
2 + x3
3 −x4
4 + · · ·
Some well-known rules relating to log and exp, derivable from the diﬀerential equation
for exp using sequence algebra, appear in table 3 (preconditions are omitted to avoid
clutter). Most of these are meticulously proven by Niven [64]. However, Niven does not
use composition (◦), but by using composition and its associativity and distributivity laws
[36], his theorem 17 and proof can be rendered as follows: let g = 1 + f, f, h ∈S0,
gr ◦h
=
exp ◦(r log g) ◦h = exp ◦r((lgn ◦f) ◦h)
=
exp ◦r(lgn ◦(f ◦h)) = exp ◦r log((f ◦h) + 1)
=
exp ◦r log((f + 1) ◦h) = exp ◦r log(g ◦h) = (g ◦h)r
Proof of Euler’s identity, exp ◦ix = cos +i sin, illustrates appeal to the uniqueness of
solution to certain diﬀerential equations. Let D sin = cos; sin0 = 0, D cos = −sin; cos0 =
1, and i2 = −1. Then both cos +i sin and exp ◦ix satisfy D g = i g; g0 = 1, and therefore
must be equal. De Moivre’s theorem follows:
(cos +i sin)n = (exp ◦ix)n = exp ◦ix ◦nx = (cos +i sin) ◦nx = cos ◦nx + i(sin ◦nx)
(B) A counting sequence, c, is either ordinary, c = [c0, c1, c2, c3 . . .] or exponential, c =
[c0, c1/1!, c2/2!, c3/3!, . . .]. In either case, cn counts the number of objects of size n gen-
erated by some structure speciﬁcation, C.
In the former case cn = [xn]c, and in the
latter case cn = n![xn]c. Roughly speaking, a structure is built from nodes, and its size
6

is the number of nodes it has. The nodes may be labelled or unlabelled. Exponential
sequences are used for labelled objects because, in that case, convolution product auto-
matically counts all possible labellings in making an ordered product. Let f and g count
labelled structures (generated by some F and G, respectively); then ordered pairs of such
structures are counted by
[xn]fg =
n
X
k=0
fkgn−k
k!(n −k)! =
n
X
k=0
n
k

fkgn−k/n!
The ordered k-fold product, F k, has counting sequence f k. Ordered lists of F-objects are
counted by list ◦f = x∗◦f = f ∗. If the order does not count, then we use set ◦f =
P
k f k/k! = exp ◦f.
The fact that permutations can be written as sets of cycles can be made explicit in
the deﬁnition of the counting sequence for permutations [25, 11]:
perm = set ◦cycle
Just put set = exp and cycle = log x∗. The sequence perm is x∗regarded as an exponential
sequence, x∗= [1, 1!/1!, 2!/2!, 3!/3! . . .].
Removal of the factorial divisors is performed
by Λ, and Λperm = [1, 1, 2, 6, 24, 120, . . .], [xn]Λperm = n![xn]perm = n!, the number of
permutations of n symbols. There are (n −1)! cyclic permutations of n symbols, and the
exponential counting sequence for these is:
cycle =
X
n>0
(n −1)!xn
n! =
X
n>0
1
nxn = −lgn ◦(−x) = −log(1 −x) = log x∗
(C) The number of ways, sn, of inserting brackets into a list of n symbols, subject to
well-formedness, is counted by the Hipparchus-Schr¨oder sequence [76, 79], s = 1
4(1 + x −
p
1 −6x + x2 ) = [0, 1, 1, 3, 11, 45, 197, 903, 4279, 20793, 103049, . . .]. This can be derived
from the speciﬁcation of a bivariate counting sequence for Schr¨oder trees:
schroeder(z,u) = z + u ∗(pluralList ◦schroeder(z,u))
(2)
Here pluralList = list −x −1. The coeﬃcient of ukzn in schroeder(z,u) gives the number
of Schr¨oder bracketings of n symbols using k pairs of brackets. Observe that equation (2)
can also be read [25] as a set-theoretic speciﬁcation of Schr¨oder trees, with z and u naming
diﬀerent kinds of nodes. Using pluralList = x∗−x −1, the above deﬁnition of s derives,
via the quadratic formula, from the equation s = schroeder(x,1) = x + (x∗−x −1) ◦s =
x + s∗−s −1 = x +
1
1 −s −s −1. Here is a foretaste of computing the Schr¨oder numbers
in Haskell, which is explained in section 4:
schroeder
=
z + u*(pluralList ‘o‘ schroeder)
> takeBiv [1..6] schroeder
[[0],[1,0],[0,1,0],[0,1,2,0],[0,1,5,5,0],[0,1,9,21,14,0]]
> takeW 11 ((1+x-sqroot(1-6*x+x^2))/4)
[0,1,1,3,11,45,197,903,4279,20793,103049]
7

This reveals, for example, [u2z5]schroeder = 9, that is 9 bracketings of 5 symbols with
2 pairs of brackets. One can see that the elements of the second sequence are totals of
corresponding elements of the ﬁrst. We remark that the sequence r = 2s/x −1 is called
the large Schr¨oder sequence [25, p. 474] (it solves r = 1 + xr + xr2).
(D) The dual interpretation of sequence expressions as set-theoretic structure speciﬁ-
cations, is exploited in [24, 25] (inﬂuenced by [44]). One might write the set-theoretic
counterpart of say, perm = set ◦cycle, as Perm ∼= Set ◦Cycle, indicating by the initial cap-
ital letters a set-theoretic interpretation. Essentially this is done in [25], but for brevity
we only give the equations deﬁning the counting sequences. Table 4 lists some univariate
examples, and table 5 some bivariate ones. These can be typed more-or-less verbatim into
Haskell, as illustrated in section 5. Many more could be lifted from chapters I-III of [25],
from the appendices of [4], and from [15, 31, 79, 76].
Let us examine a less-than-obvious expression, ascents from table 5, the origin of
which illustrates the principle of inclusion-exclusion [25, 90].
It counts permutations
according to the number of ascents. For example, the permutation |248|3679|5|1| of {1..9}
has 4 up-runs demarcated with vertical bars, 3 descents, and 5 ascents. If there are k
descents then there are k + 1 up-runs. The reversal of a permutation with k descents
delivers a permutation with k ascents. The count n![uk][zn]ascents is called an Eulerian
number, and gives the number of permutations of {1..n} with k ascents [32]. To come up
with the sequence expression, ﬁrst note that an up-run with at least one ascent corresponds
to a plural set. The counting sequence for such a set in which the k of uk records the
ascents is (pluralSet ◦uz)/u.
Let us specify permutations in which some parts of up-
runs are identiﬁed as sets, and other elements are undistinguished: b(z, u) = list ◦(z +
(pluralSet ◦uz)/u).
Now propose that ascents(z, u) is the exact counting sequence we
are after, then the inclusion-exclusion principle says that ascents(z, u + 1) = b(z, u) and
ascents(z, u) = b(z, u −1), which is cited in the table.
(E) The sequence deﬁned by D tan = 1 + tan ∗tan; tan0 = 0 is the Maclaurin expansion
of the tangent function (the ∗is explicit just for emphasis). The numbers in t = Λ tan =
[0, 1, 0, 2, 0, 16, 0, 272, 0, 7936, . . .] are called tangent numbers. The tangent numbers count
certain kinds of alternating permutations (or ordered binary trees) [78]. One can deﬁne t
also by E t = 1 + t ⊗t, where ⊗is shuﬄe (or Hurwitz [47]) product. Convolution product
(1) and shuﬄe product can be deﬁned in head-tail form (E ≡
′):
(st)′
=
s′t + s0t′
(st)0 = s0t0
(s ⊗t)′
=
s′ ⊗t + s ⊗t′
(s ⊗t)0 = s0t0
The rule E(s⊗t) = E s⊗t+s⊗E t matches D(fg) = D f +fD g and we have the Leibniz
formulae
Dn(fg) =
n
X
k=0
n
k

(Dkf)Dn−kg;
En(s ⊗t) =
n
X
k=0
n
k

Eks ⊗En−kt
Applying [x0] to the latter gives the pointwise deﬁnition of ⊗(note that a ⊗b = ab when
8

emptySet
=
1
singletonSet
=
x
singletonList
=
x
nonEmptyList
=
list −1
pluralList
=
list −singletonList −1
ordPair
=
x2
ﬁbonacci
=
list ◦(singletonList + ordPair)
cycle
=
log x∗
oneCycle
=
x
oneOrTwoCycle
=
oneCycle + x2/2
involution
=
set ◦oneOrTwoCycle
nonLoopCycle
=
cycle −singletonSet
derangement
=
set ◦nonLoopCycle
permutation
=
derangement ∗set
nonEmptySet
=
set −empty
pluralSet
=
nonEmptySet −singletonSet
setPartition
=
set ◦nonEmptySet
oddNumberOfParts
=
sinh ◦nonEmptySet
evenSizedParts
=
set ◦(cosh −1)
catalanTree
=
x(list ◦catalanTree)
cayleyTree
=
x(set ◦cayleyTree)
connectedAcyclicGraph
=
cayleyTree −1
2cayleyTree2
acyclicGraph
=
set ◦connectedAcyclicGraph
motzkinTree
=
x(1 + motzkinTree + motzkinTree2)
hipparchusSchroeder
=
(1 + x −
√
1 −6x + x2)/4
largeSchroeder
=
2 ∗hipparchusSchroeder/x −1
connectedMapping
=
cycle ◦cayleyTree
mapping
=
set ◦connectedMapping
ﬁxedPointFree
=
set ◦nonLoopCycle ◦cayleyTree
idempotent
=
set ◦(oneCycle ∗set )
partialMapping
=
mapping ∗(set ◦cayleyTree)
surjection
=
list ◦nonEmptySet
zigzag
=
2(tan + sec)
bernoulli
=
x/(exp −1)
Table 4: Some counting sequences
9

pascal
=
(u + z)∗
intComposition
=
list ◦(u ∗(nonEmptyList ◦z))
schroeder
=
z + u ∗(pluralList ◦schroeder)
catalanLeaves
=
u ∗z + z ∗(nonEmptyList ◦catalanLeaves)
cayleyLeaves
=
u ∗z + z ∗(nonEmptySet ◦cayleyLeaves)
ebinom
=
set ◦(z + uz)
cycles
=
set ◦(u ∗(cycle ◦z))
parts
=
set ◦(u ∗(nonEmptySet ◦z))
permFixedPts
=
(derangement ◦z) ∗(set ◦uz)
zigzags
=
(sin ◦u + cos ◦u)/ cos ◦(u + z)
ascents
=
list ◦(z + (pluralSet ◦(uz −z))/(u −1))
valleys
=
√1 −u
√1 −u −tanh ◦(z√1 −u)
powerSums
=
exp ◦uz −1
exp ◦z −1
bernoulliPoly
=
z exp ◦uz
exp ◦z −1
legendre
=
(1 −2uz + z2)−1/2
chebyshev
=
1 −uz
z2 −2uz + 1
laguerre
=
1
1 −z exp ◦−uz
1 −z
hermite
=
exp ◦(2uz −z2)
meixner
=
(1 + z2)−1/2 exp ◦(u arctan ◦z)
Table 5: Some bivariate sequences
10

a and b are scalars):
[xn](s ⊗t) =
n
X
k=0
n
k

([x0]Eks) ⊗([x0]En−kt) =
n
X
k=0
n
k

sktn−k
A shuﬄe inverse, s−1⊗, derives from the speciﬁcation s ⊗s−1⊗= 1 together with the
shuﬄe product rule (in exactly the same way that the rule for D f −1 is derived):
0 = 1′ = (s ⊗s−1⊗)′ = s′ ⊗s−1⊗+ s ⊗(s−1⊗)′;
(s−1⊗)′ = −s′ ⊗s−1⊗⊗s−1⊗
(F) Let SF(∗) denote the ring (SF , +, ∗, 0, 1), of sequences over some ﬁeld F (of characteris-
tic 0) with the availability of inverses implied. Then (SF (∗), D,
R
) is an integro-diﬀerential
algebra [10], and so too is (SF (⊗), E, x), where x is the right-shift operator: xf = x ∗f.
The transform (Λ, Λ−1) is an isomorphism between them, and is a formal Laplace trans-
form [67, 26]. In the previous example, the equation deﬁning tan is transformed by Λ into
the equation deﬁning t. The sequence of factorial numbers , x⊗= Λx∗, can be deﬁned by
applying Λ to x∗= 1+xx∗to get x⊗= 1+x⊗x⊗. From this we deduce x⊗= (1−x)−1⊗.
Observe, for n > 0:
[xn]x⊗= [xn]x ⊗x⊗=
n
X
k=0
n
k

[xk]x[xn−k]x⊗=
n
1

[xn−1]x⊗= n[xn−1]x⊗
Also, [xn]x⊗= n[xn−1]x⊗= (n −1)[xn−1]x⊗+ [xn−1]x⊗= [xn]x2D x⊗+ [xn]xx⊗leads to
the diﬀerential equation for the factorials:
x⊗= 1 + xx⊗+ x2D x⊗
Furthermore,
(x⊗)′ = ((1 −x)−1⊗)′ = (1 −x)−2⊗;
x⊗= 1 + x(1 −x)−2⊗
(G) We recall a classic proof [64] of the binomial theorem. Let r ∈F, rk = r(r−1) · · · (r−
k + 1) (the falling factorial), and rk = r(r + 1) · · · (r + k −1) (the rising factorial):
[xk](1 + bx)r = 1
k![x0]Dk(1 + bx)r = 1
k![x0]rkbk(1 + bx)r−k = bk
r
k

A corollary is [xk](x∗)r = [xk](1 −x)−r = (−1)k(−r)k/k! = rk/k! =
r + k −1
r −1

. This
result, and [xm] expn = [xm] exp ◦nx = nm/m!, are basic ingredients in the search for nth
term formulas. They are applied next.
(H) A Lagrange inversion formula [77, 59, 60, 29] gives an expression for the nth term of
the converse of a sequence. For example, let g = x(r ◦g), then x = g/(r ◦g) = (x/r) ◦g,
so g is the converse of x/r. Below is the Lagrange inversion formula for this case, followed
11

by its application to the counting sequences for Catalan trees, c = x(list ◦c) = x(x∗◦c),
and Cayley trees, t = x(set ◦t) = x(exp ◦t):
[xn]g
=
1
n[xn−1]rn
[xn]c
=
1
n[xn−1](x∗)n = 1
n
2n −2
n −1

[xn]t
=
1
n[xn−1] expn = 1
n
nn−1
(n −1)! = nn−1
n!
For a history of the Catalan numbers, see [66]. Cayley trees are rooted versions of the
connected acyclic graphs counted by Cayley in [13].
Cayley also counted the Catalan
trees in [12], and the ﬁrst part of Niven [64] sets out to legitimise the sequence algebra
underlying Cayley’s proof (Niven cites [42], not Cayley; but Raney [71] cites both).
A slightly more general statement of Lagrange inversion is that it solves h = g ◦f
(equivalently g = h ◦f ◦) for g, where h ∈S, f ∈SC. The theory of Lagrange inversion
sometimes employs Laurent series – series with negative powers (or sequences with negative
indicies). In the following formula for the nth term of g = h ◦f ◦, the coeﬃcient of x−1
(called the residue) is identiﬁed:
[xn]g = 1
n[x−1]D hf −n;
[x0]g = h0
Let s = x(r◦s), s = (x/r)◦, and h = g◦(x/r); then Lagrange inversion applied to g = h◦s,
gives, for h = xk, the nth term formula [xn]sk = k
n[xn−k]rn. This result specialises, when
r = x∗, to a variant of the cycle lemma [17], called Raney’s lemma in [32], which has a
history in statistics [69] related to Ballot numbers [25, p. 68]. There are various Lagrange
inversion formulas and many proofs; [14] takes an approach that also facilitates proof of
Fa`a di Bruno’s formula formula for Dn(f ◦g) [43].
(I) The (forward) diﬀerence operator, ∆s = [E −1]s = s′ −s produces the sequence of
term-to-term diﬀerences, [xn]∆s = sn+1−sn. The deﬁnition of an anti-diﬀerence operator
Σ on sequences is calculated [37] as a right identity to ∆, with (Σs)0 = 0:
∆Σs = s ⇔(Σs)′ −Σs = s ⇔(Σs)′ = Σs + s ⇔Σs = 0 + x(Σs + s) ⇔Σs =
xs
1 −x
Thus, Σs = xx∗s computes all the preﬁx sums of s (including the empty one). Applied to
∆s we get:
[xn]Σ∆s = [xn]xx∗∆s = [xn−1]x∗∆s =
n−1
X
i=0
 [xi+1]s −[xi]s

= [xn]s −[x0]s
There follows the fundamental theorem of discrete calculus (FDC) on sequences: s =
s •
0 + Σ∆s; s = ∆Σs, where a • = ax∗is the sequence with a everywhere.
12

(J) Here are the E to ∆translations extended to powers:
En = (1 + ∆)n
=
n
X
k=0
n
k

∆k
[xn]s = (Ens)0
=
n
X
k=0
n
k

(∆ks)0
(3)
∆n = (E −1)n
=
n
X
k=0
n
k

(−1)n−kEk
(∆ns)0
=
n
X
k=0
n
k

(−1)n−ksk = [xn](−x)∗⊗s
(4)
The identity
n
k

= [xn]xk/(1 −x)k+1 turns equation (3) into the Euler expansion, s =
X
k
(∆ks)0xk
(1 −x)k+1 . This expansion can also be derived from s ⊗(−x)∗= P
k(∆ks)0xk, plus
the facts (−x)∗⊗x∗= 1, and xk ⊗x∗= xk/(1 −x)k+1 (see [3] and items K and P):
s = s ⊗(−x)∗⊗x∗= (s0 + (∆s)0x + (∆2s)0x2 + · · ·) ⊗x∗=
X
k
(∆ks)0xk
(1 −x)k+1
(5)
Let g = lgn ◦−x, whence lgn = g ◦−x, and apply Euler’s expansion to g,
lgn
=
 X
k
xk
(1 −x)k+1 (∆kg)0
!
◦−x =
X
k
(−x)k
(1 + x)k+1 (∆kg)0
lgn(1)
=
X
k
(−1)k
2k+1 (∆kg)0
It is instructive to use this to approximate log(2) = lgn(1).
(K) The sequence Ns = (−x)∗⊗s = [s0, (∆s)0, (∆2s)0, . . .] is the sequence of Newton
coeﬃcients [3]. It may also be speciﬁed by (Ns)′ = N(∆s); (Ns)0 = s0. The operator
N = ((−x)∗⊗), called the Newton transform in [3], has the converse N −1 = (x∗⊗),
called the Binomial transform in [39]. The identity x∗⊗(−x)∗= 1 holds because the head
of x∗⊗(−x)∗is 1, and the tail is
(x∗⊗(1 + x)−1)′ = x∗⊗((1 + x)−1 + ((1 + x)−1)′) = 0
The following products introduce two new rings, the Hadamard ring (SR, +, −, ⊙, 0, 1 • ),
and the inﬁltration ring (SR, +, −, ↑, 0, 1) [3]:
(s ⊙t)′
=
s′ ⊙t′
(s ⊙t)0
=
s0t0
(s ↑t)′
=
s′ ↑t + s ↑t′ + s′ ↑t′
(s ↑t)0
=
s0t0
13

1.
∆(s ⊙t)
=
s ⊙∆t + ∆s ⊙t′
∆-product (1)
2.
∆(s ⊙t)
=
∆s ⊙t + s ⊙∆t + ∆s ⊙∆t
∆-product (2)
3.
Σ(s ⊙∆t)
=
s ⊙t −Σ(∆s ⊙t′) −(s ⊙t) •
0
Σ-product (1)
4.
Σ(s′ ⊙v)
=
s ⊙(Σv) −Σ(∆s ⊙Σv)
Σ-product (2)
5.
Σ∆s ⊙Σ∆u + Σ∆(s ⊙u)
=
(Σ∆s) ⊙u + s ⊙(Σ∆u)
Σ-Baxter rule
Table 6: ∆−Σ rules
The rules in table 6 apply, and (N, N −1) is an isomorphism between the Hadamard and
inﬁltration rings. The following is a point-wise deﬁnition of s ↑t:
[xn]s ↑t = [xn]N(N −1s ⊙N −1t) =
n
X
i=0
n
i

(−1)n−i[xi]((x∗⊗s) ⊙(x∗⊗t))
The proof that N is a morphism from ⊙to the new product ↑can be re-imagined as a
discovery of what the deﬁnition of ↑should be. The morphism presumption is signalled
on the right below.
(N(s ⊙t))′
=
N(∆(s ⊙t))
defn. N
=
N(∆s ⊙t + s ⊙∆t + ∆s ⊙∆t)
∆-product (2)
=
N(∆s ⊙t) + N(s ⊙∆t) + N(∆s ⊙∆t)
morphism
=
N∆s ↑Nt + Ns ↑N∆t + N∆s ↑N∆t
morphism
=
(Ns)′ ↑Nt + Ns ↑(Nt)′ + (Ns)′ ↑(Nt)′
defn. N
=
(Ns ↑Nt)′
defn. ↑
(L) The following deﬁnes permutation cycle numbers,
n
k

= n![ukzn]cycles, and set par-
tition numbers
n
k

= n![ukzn]parts. These are also called Stirling numbers of the ﬁrst
and second kind, respectively.
cycles
=
set ◦(u ∗(cycle ◦z))
parts
=
set ◦(u ∗(nonEmptySet ◦z))
The well-known recurrences [32],
n + 1
k

=

n
k −1

+ n
n
k

,
n + 1
k

=

n
k −1

+ k
n
k

translate, using c = cycles and p = parts, into
Dzc = uc + zDzc
Dzp = up + uDup
where Dz and Du are the partial diﬀerentiation operators with respect to z and u. To
see this, note that
n + 1
k

= (n + 1)![ukzn+1]c = n![ukzn]Dzc,

n
k −1

= n![uk−1zn]c =
14

n![ukzn]uc, and so on. The recurrences can be checked: ﬁrst, Dzc = Dz exp ◦(u log z∗) =
ucz∗= uc + uczz∗= uc + zDzc; and second, up + uDup = up + u(exp ◦z −1)p =
pu exp ◦z = Dzp. We may write n![zn] exp ◦(u log ◦z∗) = n![zn](1−z)−u = un. The cycles
recurrence can also be written [xk]xn = [xk−1]xn−1 + (n −1)[xk]xn−1, which follows from
xn = xn−1(x + n −1) = xxn−1 + (n −1)xn−1.
(M) A factorial polynomial uses falling factorials instead of powers. For example, let
p = 1 + 2x + x2, then the falling factorial counterpart is p = 1 + 3x1 + x2. Coeﬃcients in
p are identiﬁed by [xk]p, for example [x1]p = 3.
The symbols Σ and ∆are overloaded as operators on factorial polynomials and obey
rules identical to those for D and
R
on polynomials: let p denote a polynomial in falling
factorials, then [xk]∆p = (k + 1)[xk+1]p and [xn+1]Σp =
1
n + 1[xn]p. The fundamental
theorem of the discrete calculus on factorial polynomials is immediate: p = p0 +Σ∆p; p =
∆Σp. In [32], the theorem provides one of seven ways of deducing the polynomial for
summing squares: given ∆p = (1 + x)2 = 1 + 3x1 + x2, apply Σ to both sides,
p −p0 = 1x1 + 3/2 x2 + 1/3 x3; p = 1/6 (x + 3x2 + 2x3)
(6)
Note that if p is a polynomial nth term formula for sequence s, p(n) = sn, then (∆p)(n) =
(∆s)n and (Σ∆p)(n) = p(n) −p(0) = sn −s0 = (Σ∆s)n.
(N) An analogue of the Maclaurin rule holds: [xn]p = 1
n![x0]∆np = 1
n!(∆np)(0). The
latter equality involves yet another interpretation of ∆: (∆p)(n) = p(n + 1) −p(n).
Gregory-Newton (interpolation) formulas for p of degree m follow; the second (see also
(3)) uses nk/k! =
n
k

:
p
=
p(0)x0 + (∆p)(0)x1 + (∆2p)(0)
2!
x2 + · · · + (∆mp)(0)
m!
xm
(7)
p(n)
=
p(0)
n
0

+ (∆p)(0)
n
1

+ (∆2p)(0)
n
2

+ · · · + (∆mp)(0)
 n
m

(8)
Let s = [0, 1, 5, 14, 30, 55, . . .] be the sequence sn = 12 +22 +· · ·+n2, for which we seek the
polynomial p such that p(n) = sn. Then the above expansions produce the polynomial(s)
in (6). By contrast, the Euler expansion (5) produces the sequence expression s = (x +
x2)/(1 −x)4.
(O) We have seen [xk]xn =
n
k

, so we can express the polynomial for xn in terms of cycle
numbers, and by change of signs, also the polynomial for xn:
xn =
n
X
k=1
n
k

xk;
xn =
n
X
k=1
(−1)n−k
n
k

xk;
x0 = x0 = 1
15

This shows how to translate falling factorials into powers. The converse is
xn =
n
X
k=1
n
k

xk
[xk]xn =
n
k

and for a proof see [9, p. 343] and [32, p. 262].
(P) Inﬁnite sequences are called streams when they are identiﬁed with the ﬁnal object in
a category of head-tail coalgebras [41, 72]. This accounts for the name “stream” in the
following:
Fundamental theorem of (sequence) calculus (FTC)
f
=
f0 +
R
D f
f
=
D
R
f
Fundamental theorem of stream calculus (FSC)
f
=
f0 + x(Ef)
f
=
E(xf)
Fundamental theorem of discrete calculus (FDC)
f
=
f •
0 + Σ∆f
f
=
∆Σf
The co-algebraic stream calculus [72] introduces a proof principle called coinduction. For
example, the identity xk ⊗x∗= xk/(1 −x)k+1 = x∗(xx∗)k used in the proof of (5) can be
proved using coinduction (it can also be proved from the point-wise deﬁnition of ⊗and
the binomial theorem). Here is the gist of the coinductive proof. Propose the relation
xk ⊗x∗∼x∗(xx∗)k.
This is used as a coinductive hypothesis.
Head-equality holds,
(xk ⊗x∗)0 = (x∗(xx∗)k)0. The proof is completed by showing that the tails are equal
under the hypothesis, signiﬁed below by the use of (∼):
(x∗(xx∗)k)′
=
(x∗)′(xx∗)k + 1((xx∗)k)′
conv. product rule
=
x∗(xx∗)k + x∗(xx∗)k−1
(x∗)′ = x∗and ((xf)n)′ = f(xf)n−1
∼
xk ⊗x∗+ xk−1 ⊗x∗
coinduction hyp.
=
(xk ⊗x∗)′
shuﬄe product rule
The bracketed (co-) in the paper’s title indicates that we only touch lightly on co-algebraic
concepts. There is more to coinduction than the above example suggests, and we refer to
the survey [33] for background.
(Q) One can check from the pointwise deﬁnitions of D and ⊗that D f = (x ⊗f ′)′.
Alternatively, equality can be proved by showing that these expressions satisfy the same
head-tail equations. The head-tail equation for D f is calculated:
D f = D(f0 + xf ′) = f ′ + xD f ′ = f ′
0 + xf ′′ + xD f ′ = f1 + x(f ′′ + D f ′)
Hence, (D f)0 = f1 and (D f)′ = f ′′ + D f ′. Now let F f = (x ⊗f ′)′. We ﬁnd (F f)0 = f1,
and (F f)′ = (f ′ + (x ⊗f ′′))′ = f ′′ + F f ′. Thus, D f = F f since they satisfy the same
head-tail equations.
To give a little more feeling for the coinduction game, let us reveal the machine-
level minutiae that proves D(f ◦g) = (D f ◦g)D g. We use head-tail properties such as
16

(f ◦g)0 = f0; (f ◦g)′ = (f ′ ◦g)g′ (see section 4, equation (13)). We will also make use of
(Dh)0 = h′
0. Head equality is conﬁrmed:
(D(f ◦g))0 = (f ◦g)′
0 = (f ′ ◦g)0g′
0 = f ′
0g′
0 = (Df ◦g)0(Dg)0 = ((Df ◦g)Dg)0
Tails are proved equal under the coinductive hypothesis, D(f ◦g) ∼(Df ◦g)Dg:
((Df ◦g)Dg)′
=
(Df ◦g)′Dg + (Df ◦g)0(Dg)′
(′)-product
=
((Df)′ ◦g)g′Dg + (f ′ ◦g)0(Dg)′
(′)-composition
=
(f ′′ ◦g)g′Dg + (Df ′ ◦g)g′Dg + (f ′ ◦g)0(Dg)′
D-defn and ◦-distr
∼
(f ′ ◦g)′Dg + D(f ′ ◦g)g′ + (f ′ ◦g)0(Dg)′
(′)-comp., coinduction
=
(f ′ ◦g)′(g′ + xDg′) + D(f ′ ◦g)g′ + (f ′ ◦g)0(g′′ + Dg′)
expand D g and (D g)′
=
D(f ′ ◦g)g′ + (f ′ ◦g)′xDg′ + (f ′ ◦g)0Dg′ + (f ′ ◦g)′g′ + (f ′ ◦g)0g′′
=
D(f ′ ◦g)g′ + (f ′ ◦g)Dg′ + ((f ′ ◦g)g′)′
head-tail, h = h0 + xh′
=
D((f ′ ◦g)g′) + ((f ′ ◦g)g′)′
D-product
=
D(f ◦g)′ + (f ◦g)′′
(′)-composition, twice
=
(D(f ◦g))′
D-defn
(R) Coinduction is also used in [73] to show how continued fractions can be obtained from
combinatorially-inspired automata. For example, the tangent sequence can be deﬁned by
t = xu1, where uk = 1/(1 −k(k + 1)x2uk+1). Thus t can be displayed as a continued
fraction:
t =
x
1 −
1 ∗2x2
1 −
2 ∗3x2
1 −3 ∗4x2
...
More combinatorially-inspired continued fraction expressions for sequences appear in [23,
31, 73]. Below is one for Λzcycles = (1−z)−u⊗(Λz removes the factorial divisors of powers
of z).
(1 −z)−u⊗=
1
1 −uz −
1uz2
1 −(u + 2)z −
2(u + 1)z2
1 −(u + 4)z −3(u + 2)z2
...
Setting u = 1 and z = x gives a continued fraction for the factorials, (1 −x)−1⊗= x⊗.
(S) A kth-order linear ordinary homogeneous diﬀerential equation,
bkDkf + bk−1Dk−1f + · · · + b0f = 0
can be written b(D)f = 0.
Similarly, a diﬀerence equation (also called a recurrence
equation) can be written b(E)s = 0. Let `b = bk + bk−1x1 + · · · + b1xk−1 + b0xk be the
17

reverse of b = b0 + b1x + b2x2 + · · · + bkxk. Klarner [48] presents this fact: the solution s
to b(E)s = 0 is
s = (`b ∗inits)[0..k −1]
`b
where inits= s0 + s1x + · · · + sk−1xk−1 = s[0..k −1] are the initial k elements of s. Also
b(E)s = 0 ⇔b(D)Λ−1s = 0
For example, E2s + s = 0, s0 = 0, s1 = 1 has solution x/(1 + x2), and Λ−1s = sin, the
Maclaurin expansion for sin, that is, the solution to D2s+s = 0; s0 = 0, s1 = 1. Another
example is [zn]C −2u[zn−1]C +[zn−2]C = 0, C0 = 1, C1 = u, where [zn]C is a Chebyshev
polynomial [9] in u. Then, b = 1 −2uz + z2 = `b , and
C(z, u) = (`b ∗(1 + uz))[0..1]
`b
=
1 −uz
1 −2uz + z2
By the translation rules of item J, diﬀerence equations can be written using either E or ∆.
The equation b(E)s = 0 transforms into b(1+∆)s = 0, or ˆb(∆)s = 0, where ˆb = b◦(1+x).
The converse is b = ˆb ◦(x −1), reﬂecting ˆb(∆) = ˆb(E −1). Clearly, b = b ◦(1+ x) ◦(x −1).
(T) A sequence s is called rational if it is the quotient, s = a/b, of polynomials; it is called
a LODE solution, written LODE(s), if it is a solution of a linear ordinary homogeneous
diﬀerence equation; and it is called recognizable if it is the behaviour of a ﬁnite automaton.
Then,
rational(s) ⇔LODE(s) ⇔recognizable(s)
Following [19], a ﬁnite automaton can be modelled as a system of linear equations over
sequences, E S = AS; S(0) = v. Here, matrix A records transition labels connecting pairs
of states, and S is a vector of sequences, one for each state, with initial values Si(0) = vi.
The solution is S = (Ax)∗v where
(Ax)∗= I + Ax + A2x2 + A3x3 + · · · =
X
i≥0
Aixi
is the Kleene star. Notice that (Ax)∗can be viewed as a matrix of sequences or as a
sequence of matrices. Using (Ax)∗= (I −Ax)−1, we get (I −Ax)S = v and Cramer’s rule
applies: Si = det(I −Ax)[i ←v]
det(I −Ax)
(column i replaced by v). Thus Si is rational. Justiﬁ-
cation of the above equivalences is completed by noting that a LODE can be transformed
into a system of linear equations. We remark that a quotient of polynomials, a/b, can also
be written as the solution to a system of linear equations [72].
(U) Let b = det(xI −A) = b0 +b1x1 +· · ·+bn−1xn−1+xn be the characteristic polynomial
of matrix A. The Cayley-Hamilton theorem [19, 49] can be stated as b(A) = 0, or as
18

b(E)(Ax)∗= 0. This will hold if, taking the sequence of matrix powers, (Ax)∗, now as a
matrix of sequences, we have b(E)((Ax)∗)ij = 0, which in turn holds if ((Ax)∗)ij = a
`b
. We
have `b = xnb(1/x) = det(I −xA). So we are done if we come up with an a such that
((Ax)∗)ij =
a
det(I −Ax)
Let M = Ax, and J = M∗j = j + MJ, where j is the vector with 1 at position j and zero
elsewhere. Then, (I −M)J = j and Cramer’s rule delivers the a we are looking for:
(M∗)ij = Ji = det(I −M)[i ←j]
det(I −M)
(V) Consider the matrix exponential, exp ◦Ax = Λ−1(Ax)∗as a matrix of sequences. We
know that (Ax)∗solves E S = AS, S(0) = I, and (Ax)∗[0..k −1] = [I, A, A2, . . . , Ak−1].
Cayley-Hamilton says that b(E)(Ax)∗= 0, where b is the characteristic polynomial of A,
of degree k, say. By uniqueness of solution, we have [54, 56, 53] φ = (Ax)∗if b(E)φ = 0
and φ[0..k −1] = [I, A, A2, . . . , Ak−1]. Let S be a vector of sequences such that b(E)Si = 0
and Si[0..k −1] = xi. Set
φ = S0I + S1A + · · · Sk−1Ak−1 =
k−1
X
i=0
SiAi
Clearly φ[0..k −1] = [I, A, A2, . . . , Ak−1], and b(E)φ = 0 because
k
X
j=0
bjEjφ =
k
X
j=0
bj
 k−1
X
i=0
EjSiAi
!
=
k−1
X
i=0


k
X
j=0
bjEjSi

Ai =
k−1
X
i=0
(b(E)Si)Ai = 0
(W) The elements of the sequence B = x/(exp −1) = [1, −1/2, 1/6, 0, −1/30, 0, 1/42, 0, . . .]
are called Bernoulli numbers. The corresponding recurrence is calculated from B exp =
B + x:
n![xn]B exp = n![xn](B + x);
n
X
k=0
n
k

Bk = Bn + [n = 1]
Bernoulli numbers are used by Graham et al [32] for the most impressive of their deductions
of the polynomial that sums squares – impressive because it deﬁnes the formulas for all
powers at once. Let S(n) be the sequence such that m![xm]S(n) is the sum of the mth
powers of the naturals to n −1. Then
m![xm]S(n)
=
n−1
X
k=0
km =
n−1
X
k=0
m![xm] exp ◦kx
S(n)
=
n−1
X
k=0
expk = expn −1
exp −1
= exp ◦nx −1
exp −1
= B exp ◦nx −1
x
m![xm]S(n)
=
m!
m
X
k=0
Bm−k
(m −k)!
nk+1
(k + 1)! =
m
X
k=0
m
k

Bm−k
nk+1
k + 1
19

Now replace n by u and x by z to get the expression advertised in the introduction:
S = exp ◦uz −1
exp ◦z −1
Then m![zm]S is a polynomial of degree m + 1 in u, and (m![zm]S)(n) = m![xm]S(n).
(X) Observe that B1 = −1/2 is non-zero whilst all the other odd-degree coeﬃcients of B
appear to be zero. Perhaps if we make B1 zero then we will have a sequence which can be
proved to be even (i.e. with zeros at odd positions). Adding 1
2x to B cancels B1:
C = B + x
2 =
x
exp −1 + x
2 = x
2
exp +1
exp −1
Recall coth = exp + exp ◦−x
exp −exp ◦−x, so C = x
2(coth ◦x
2 ), from which eveness, C ◦−x = C, can
be deduced. Thus,
C = B + x
2 =
X
k
B2k
(2k)!x2k
Now C ◦2x = x coth, and, using x cot = ix(coth ◦ix) and tan = cot −2 cot ◦2x, we get
x cot = C ◦2ix
=
X
k
(−1)k22k B2k
(2k)!x2k
(9)
tan
=
X
k
(−1)k−14k(4k −1) B2k
(2k)!x2k−1
(10)
With a bit of analysis (reals, cot(x) an analytic function with period π, and uniqueness
of series expansion), one can deduce another series for x cot, due to Euler. The omitted
analysis [50, 1] is hidden in the ﬁrst equals sign:
x cot(x) = 1 −2
∞
X
n=1
x2
n2π2 −x2 = 1 −2
∞
X
n=1
x2
n2π2
 x2
n2π2
∗
= 1 −2
∞
X
k=1
x2k
π2k
∞
X
n=1
1
n2k
Equating coeﬃcients with those in the expansion (9) yields, for k > 0,
[x2k]x cot = −2
π2k
∞
X
n=1
1
n2k = (−1)k22k B2k
(2k)!
Therefore, the values of Riemann’s ζ(s) =
X
n≥1
1
ns at even positive integers is given by
ζ(2k) = (−1)k−122k−1 B2k
(2k)!π2k
20

(Y) The Formal Taylor Theorem may be expressed:
f ◦(u + z) = f ◦u + ((D f) ◦u)z + (D2f) ◦u
2!
z2 + (D3f) ◦u
3!
z3 + · · ·
Write f ◦(u + z) = g0 + g1z + g2z2 + g3z3 + · · ·. Let z = 0, then g0 = f(u). Diﬀerentiate
with respect to z: Dz(f ◦(u + z)) = g1 + 2g2z + 3g3z2 + · · ·. Note that Dz(f ◦(u + z)) =
f1 + 2f2(u + z) + 3f3(u + z)2 + · · · = (D f) ◦(u + z). Let z = 0, then g1 = (D f) ◦u.
Diﬀerentiate again: D2(f ◦(u+z)) = 2g2 +3!g3z +· · ·. Let z = 0, then g2 = ((D2f)◦u)/2.
And so on.
The Maclaurin expansion is the special case with u = 0, and the Taylor
expansion of xn ◦(u + z) is an instance of the binomial theorem. Lipson [55] uses the
theorem in the application of Newton’s iterative root-ﬁnding algorithm to polynomial
equations over sequences (see also [70]).
(Z) The following manipulations, originating with Lagrange, have a captivating charm
(even if they lack rigour). We adapt them from [32] to show that elementary sequence
algebra plays a role through to the ﬁnal chapter of that book (where, however, things
become more demanding). In the Formal Taylor theorem, let f be a polynomial, z = 1,
u = x, and employ an operator style:
Ef
=
f ◦(x + 1) = f + (D f) + D2f
2!
+ D3f
3!
+ · · · = [1 + D + D2
2! + D3
3! + · · ·]f
=
exp(D)f
Putting ∆= E −1 = exp(D) −1 together with ∆Σf = f, suggests Σ = (exp(D) −1)−1.
Then B = x/(exp −1) applied to D is DΣ, so Σ = D−1B(D). Expanding this, and writing
R
for the ﬁrst term D−1 (since B0 = 1), gives a “template” version of the Euler-Maclaurin
summation formula [32, 9].
X
=
Z
+
X
k≥1
Bk
k! Dk−1
Now introduce limits:
Pb
a f =
Z b
a
f +
X
k≥1
Bk
k! Dk−1f

b
a
(11)
An application to x2 gives yet another derivation [32] of the sum-of-squares formula:
Pn
0 x2
=
1
3n3 +

−1
2x2 + 1
122x

n
0
=
1
3n3 −1
2n2 + 1
6n
The way the limits appear on the summation sign has signiﬁcance: Pn
0 x2 =
n−1
X
x=0
x2. The
21

deﬁnite summation symbol follows the pattern of deﬁnite integration:
g = Df
⇒
Z b
a
g
=
f

b
a
=
f(b) −f(a)
g = ∆f
⇒
Pb
a g
=
f

b
a
=
f(b) −f(a)
=
b−1
X
x=a
f(x + 1) −f(x)
=
b−1
X
x=a
∆f(x)
=
b−1
X
x=a
g(x)
Let’s add f(b) to both sides of (11) and separate out B1 = −1/2:
b
X
x=a
f =
Z b
a
f + 1
2(f(b) + f(a)) +
X
k≥2
Bk
k! Dk−1f

b
a
(12)
The Euler-Maclaurin formula can also be applied to non-polynomial functions. Let us
illustrate this, without justiﬁcation. To compute
∞
X
x=1
1/x2, set S9 =
9
X
x=1
1
x2 = 1.5397677310
and then apply (12) to g = 1/(x + 10)2:
∞
X
x=1
1
x2 = S9 +
∞
X
x=0
g(x) = S9 +
Z ∞
0
g + 1
2g(0) +
X
k≥2
Bk
k! Dk−1g

∞
0
Applying the formula up to B4 gives ζ(2) = π2/6 ≈1.64493407.
4
A programming delight
McIlroy [57, 58], inﬂuenced by [45] and others, has gifted us some “tiny gems” of pro-
gram deﬁnitions for implementing sequence manipulations. The deﬁnitions are written in
Haskell, and are eﬀortlessly derived by mathematical reasoning. A textbook introduction
appears in [18]. We want to entice the reader to type up and experiment with the Haskell
code (but a down-loadable ﬁle is available). The code has been tested in the Haskell GHCi
system, and also in the Hugs98 system (an older system, but well-suited to beginners).
Both GHCi and Hugs98 are freely available on the web at www.Haskell.org.
In this section we present all of the deﬁnitions, thus duplicating some of the contents
of [57, 58]; however, there are modiﬁcations and additions. The fact that the deﬁnitions
have no pre-requisites, other than the standard Haskell Prelude, means that one can
take “deep” ownership, building things from the ground up. This contrasts to using a
sophisticated computer algebra system – something perhaps for the newcomer to move on
to with greater appreciation.
Haskell [68] has evolved to be a fairly large and sophisticated language, but we shall
stick to a modest subset. It is expected that the reader can comprehend Haskell from
examples.
The language gives types to objects and variables, and within context, the
most general type is used. Haskell’s lists are used to represent sequences (some may prefer
22

to introduce a new type for sequences, but that introduces an overhead which we want
to avoid). In Haskell, head-tail decomposition s0 + xs′ becomes s0:s’. Here are some
list-processing functions:
take n _ | n<=0
= []
take _ []
= []
take n (s0:s’)
= s0: take (n-1) s’
map f []
= []
map f (s0:s’)
= f s0 : map f s’
iterate f z
= z: iterate f (f z)
foldr f z []
= z
foldr f z (s0:s’)
= f s0 (foldr f z s’)
scanl op q s
= q: (case s of
[]
-> []
s0:s’ -> scanl op (op q s0) s’)
zip (s0:s’) (t0:t’) = (s0, t0): zip s’ t’
zip _ _
= []
zipWith op s t
= [op sn tn | (sn,tn) <- zip s t]
These deﬁnitions implement the following functions which feature in the algebra of pro-
gram calculation [7, 6]:
take n s
=
[s0, s1, . . . , sn−1]
map f s
=
[fs0, fs1, fs2, . . .]
iterate f z
=
[z, f z, f(fz), f 3z, . . .]
foldr f z s
=
f s0 (f s1 (f s2 (. . . z) . . .)
scanl ⊕q s
=
[q, q ⊕s0, (q ⊕s0) ⊕s1, ((q ⊕s0) ⊕s1) ⊕s2, . . .]
zip s t
=
[(s0, t0), (s1, t1), (s2, t2), . . .]
zipWith ⊙s t
=
[s0 ⊙t0, s1 ⊙t1, s2 ⊙t2, . . .]
The types deduced are
take
:: Int -> [a] -> [a]
map
:: (a -> b) -> [a] -> [b]
iterate :: (a -> a) -> a -> [a]
foldr
:: (a -> b -> b) -> b -> [a] -> b
scanl
:: (a -> b -> a) -> a -> [b] -> [a]
zip
:: [a] -> [b] -> [(a,b)]
zipWith :: (a -> b -> c) -> [a] -> [b] -> [c]
Type expressions are built from type names such as Int, type variables such as a, and
type constructors such as -> (which associates to the right). Two more examples of type
constructors are: [a] is the type for lists of objects of type a, and [(a,b)] is the type
for lists of pairs of objects. Clearly, functions can take functions as arguments, in which
case they are called higher-order. Lazy evaluation is used, so that for example, scanl will
produce the ﬁrst element, q, of the result without needing to know anything about its list
argument, s. The deﬁnition of zipWith illustrates the so-called list comprehension. An
alternative deﬁnition uses map and uncurry:
23

uncurry
:: (a -> b -> c) -> (a,b) -> c
uncurry op p
= op (fst p) (snd p)
zipWith op s t = map (uncurry op) (zip s t)
The partner to uncurry is curry f x y = f (x,y). An alternative deﬁnition illustrates
use of a lambda expression: curry f = \x y -> f (x,y). The reader may like to supply
the type. These two functions are so-named because Haskell Curry was an early advocate
of the associated equivalence [40]. The Haskell Standard Prelude deﬁnes zipWith without
using zip, and then deﬁnes zip = zipWith (,).
The following speciﬁes that a type a is classiﬁed as Num if it has the operations (or
methods) listed here:
class Num a where
(+), (-), (*)
:: a -> a -> a
negate, abs, signum
:: a -> a
fromInteger
:: Integer -> a
x - y
= x + negate y
negate x
= 0 - x
All of the foregoing deﬁnitions are in the Standard Prelude. From here on, the code needs
to be supplied. The program ﬁle starts with a few speciﬁed lines: the ﬁrst line hides
the Prelude deﬁnition of cycle because we are going to re-deﬁne it for other purposes;
the second line says we need rational numbers; the third line gives the order in which to
resolve ambiguity in numerical data.
import Prelude hiding (cycle)
import Data.Ratio
default (Integer, Rational, Double)
We start by declaring how sequences, [a], become an instance of Num. A prerequisite is
that a is an instance of Eq and Num, indicated by (Eq a, Num a) =>. The deﬁnition of
(-) is derived from negate.
instance (Eq a, Num a) => Num [a] where
negate
= map negate
f+[]
= f
[]+g
= g
(f0:f’)+(g0:g’)
= f0+g0 : f’ + g’
[]*_
= []
(0:f’)*g
= 0 : f’*g
_*[]
= []
(f0:f’)*g@(g0:g’) = f0*g0 : (f0*|g’ + f’*g)
fromInteger c
= [fromInteger c]
abs _
= error "abs not defined on sequences"
signum _
= error "signum not defined on sequences"
Observe that addition is not deﬁned by f+g = zipWith (+) f g (why?). Convolution
product is derived from (1), but there are some things to note. Firstly, if f0 = 0 then
24

the zeroth term of the result is 0 and is delivered immediately. This may be regarded
as a controversial quirk, but it enables certain equations to be used directly, as in the
following (Catalan) example – in examples, deﬁnitions (which are placed in a program
ﬁle) are interspersed with interactive requests for expression evaluation, indicated by the
prompt “> ”.
x :: Num a => [a]
x = [0,1]
b = 1 + x*b^2
> take 8 b
[1,1,2,5,14,42,132,429]
Secondly, the notation g@, is read as “g as”. Thirdly, there are clauses for ﬁnite sequences
– the empty list behaves here like zero (but note that 0 is embedded as [0]). Fourthly,
the term f0g′ = [f0]g′ becomes an explicit scalar product using *|, which is deﬁned as
an inﬁx operator with precedence 7 (the same as *, and higher than +). The deﬁnition
contains (a*), illustrating the creation of a function by partial application of an operator
(called sectioning).
infix 7 *|
(*|) :: Num a => a -> [a] -> [a]
a *| f = map (a*) f
A function like map is said to be polymorphic because any type can be assigned to its
type variables (subject to consistency).
By contrast, scalar multiplication (*|) has a
qualiﬁed (constrained or parametric) polymorphic type: its type variable can range over
only instances of class Num. The type stated could be omitted because it can be inferred
due to the presence of *. On the other hand, if the explicit type given to x above was
omitted, then Haskell would infer x::[Integer] and this is a monomorphic type which
would restrict the use of x. With the qualiﬁed polymorphic type, x can appear in an
expression where a sequence of elements of type N is expected, as long as N is an instance
of Num. Any instance N of Num must provide a fromInteger method that shows how to
embed integers into N, so x would be interpreted as [N.fromInteger 0, N.fromInteger
1].
The Num class invites comparison with the speciﬁcation of the signature of a ring.
Likewise, Haskell’s Fractional class may be compared to a ring-with-division, because a
(partial) division operator (/), or a multiplicative inverse (recip), is required. Rational
numbers form the archetypal instance of Fractional, and any instance F must show how
to embed the rationals in F by deﬁning fromRational ::
Rational -> F. Division on
sequences, f/g, requires calculating the quotient q satisfying f = qg:
f0 + xf ′
=
(q0 + xq′)g = q0g + xq′g = q0g0 + x(q0g′ + q′g)
f0 = q0g0
;
f ′ = q0g′ + q′g
q0 = f0/g0
;
q′ = (f ′ −q0g′)/g
q
=
f0/g0 + x(f ′ −q0g′)/g
Now we can say, at least approximately, how sequences become a ring-with-division:
25

instance (Eq a, Fractional a) => Fractional [a] where
recip f
= 1/f
_/[]
= error "divide by zero."
[]/_
= []
(0:f’)/(0:g’)
= f’/g’
(_:f’)/(0:g’)
= error "divide by zero"
(f0:f’)/g@(g0:g’) = let q0=f0/g0 in q0:((f’ - q0*|g’)/g)
fromRational c
= [fromRational c]
These simple deﬁnitions confront us with some of the diﬃculties in coding a satisfactory
division operation that works for both ﬁnite and inﬁnite sequences. One should investigate
questions like: are f/g = f ∗(1/g) and f/f = 1 faithfully implemented? To keep things
simple, compromises have to be made.
Arithmetic and the convolution product rule are used to calculate a head-tail deﬁ-
nition for square root, √f. The starting point is f = √f√f, and we calculate (√f)0 and
√f′:
f0 = (
p
f
p
f)0
=
(
p
f)0(
p
f)0
(
p
f)0
=
p
f0
f ′ = (
p
f
p
f)′
=
p
f0
p
f
′ +
p
f
′p
f
p
f
′
=
f ′/(
p
f0 +
p
f) (f0 ̸= 0)
We shall trivialise √f0 and restrict square root to fractional sequences with constant term
1. In the following code, the ﬁrst clause is suggested by the identity
p
x2f = x√f, and
the more general
p
x2nf = xn√f is handled by recursion. An alternative deﬁnition of
square root is derived in [58] by diﬀerentiating r2 = f, rearranging and then integrating
(which the reader may like to try).
sqroot (0:0:f’’) = 0:sqroot f’’
sqroot f@(1:f’)
= 1:(f’/(1+sqroot f))
Sequence composition, f ◦g =
X
n
fngn, is expanded thus:
f ◦g
=
f0 + f1g1 + f2g2 + f3g3 + · · ·
=
f0 + g(f ′ ◦g) = f0 + (g0 + xg′)(f ′ ◦g)
=
f0 + g0(f ′ ◦g) + xg′(f ′ ◦g)
When f is inﬁnite, g0(f ′ ◦g) is not computable unless g0 = 0. When g0 = 0 we get
(f ◦g)0 = f0;
(f ◦g)′ = g′(f ′ ◦g)
(13)
However, f ◦g is computable for g0 ̸= 0 when f is ﬁnite (p ◦[a] = [p(a)], p a polynomial).
So we admit a potentially non-terminating clause and it is up to us to use it with care:
[] ‘o‘ _
= []
26

(f0:f’) ‘o‘ g@(0:g’)
= f0: g’*(f’ ‘o‘ g)
(f0:f’) ‘o‘ g@(g0:g’) = [f0] + (g0*|(f’ ‘o‘ g))+
(0:g’*(f’ ‘o‘ g))
The deﬁnition f ◦g = P
n fngn reveals x to be a left and right identity of composition.
Composition distributes leftwards through sum, product, and quotient.
To calculate the converse, g = f ◦, expand the composition f ◦g = x:
f0 + xg′(f ′ ◦g) = x = 0 + x1
Hence, g′(f ′ ◦g) = 1, and
g0 = 0;
g′ = 1/(f ′ ◦g)
The program code is:
converse(0:f’) = g where g = 0: 1/(f’ ‘o‘ g)
For the reciprocal of f ′ ◦g to be deﬁned, it is necessary that f ′
0 is invertible, which entails
that f ′ is invertible. The set of such “conversible” sequences forms a group, (SC, ◦, ()◦, x).
The transforms Λ and Λ−1 are given names e2o and o2e, respectively [18]. Here they are,
together with some other useful sequences:
e2o f
= zipWith (*) f facs
o2e f
= zipWith (/) f facs
from
:: Num a=>a->[a]
from
= iterate (+1)
nats, pos, zeros, facs :: Num a=>[a]
nats
= from 0
pos
= from 1
zeros
= 0:zeros
facs
= scanl (*) 1 pos
Diﬀerentiation and integration enjoy the appropriately succinct deﬁnitions,
deriv f = zipWith (*) pos (tail f)
integ f = 0:zipWith (/) f pos
The deﬁnitions D exp = exp; exp0 = 1 and x∗= 1 + xx∗have the following solutions in
Haskell. An x is aﬃxed to prevent name clashes with existing names (for example exp in
Haskell implements the function ex). One can test x∗= Λ exp by checking the ﬁrst few
terms of their diﬀerence.
expx
:: (Eq a,Fractional a) => [a]
expx
= 1 + integ expx
starx :: (Eq a, Num a) => [a]
starx = 1 : starx
> takeW 6 (starx - e2o expx)
[0,0,0,0,0,0]
27

A rational a/b is presented in Haskell as a%b. The elements of expx are rationals, and
e2o removes the factorial divisors, yielding [1%1,1%1, ...]. The following deﬁnes takeW
n which is take n preceded by the conversion of whole rationals into integers (the (.) is
function composition, and properFraction is in the Prelude).
makeWhole r
= case properFraction r of
(n,0)
-> n
otherwise
-> error "not whole"
makeAllWhole = map makeWhole
takeW n
= take n . makeAllWhole
Table 7 contains further core sequences deﬁned by diﬀerential equations. All should be
given the type (Eq a, Fractional a) => [a], like expx above. The core sequence x∗,
which we deﬁned earlier, could be deﬁned by starx = 1+integ (starx^2), since D x∗=
(x∗)2; x∗
0 = 1 (but its elements would then be fractional). For two more examples, let us
calculate deﬁnitions for arctan and arcsin.
D tan◦= ((1 + tan2) ◦tan◦)−1 = 1/(1 + x2)
D sin◦= (cos ◦sin◦)−1 =
p
1 −sin2
◦sin◦−1
= 1/
p
1 −x2
The latter uses the Pythagorean identity, sin2 + cos2 = 1, which follows from the deﬁning
diﬀerential equations. Taking initial values into consideration, the solutions rendered in
Haskell are immediate:
atanx
= integ (1/(1+x^2))
asinx
= integ (1/(sqroot (1-x^2)))
Here are checks of exp = (sec + tan)◦gd (gd is the Guddermanian function) and D sin◦=
1/
√
1 −x2.
> takeW 6 (expx - ((secx + tanx) ‘o‘ gdx))
[0,0,0,0,0,0]
> takeW 6 (deriv (converse sinx) - (1/(sqroot (1-x^2))))
[0,0,0,0,0,0]
A bivariate sequence, b(z, u), may be regarded as a (potentially doubly-inﬁnite) matrix,
t = (bi,j), of coeﬃcients of ziuj. It is implemented as a univariate sequence, s, of (ho-
mogeneous) polynomials such that sn is the diagonal, [b0,n, b1,n−1, . . . bn,0] of t. Thus,
b0,nz0un+b1,n−1z1un−1+· · ·+bn,0znu0 is represented by sn = b0,nx0+b1,n−1x1+· · ·+bn,0xn
(z becomes x, and u is redundant). So, [un−kzk]b = [xk][xn]s; equivalently [ukzn]b =
[xn][xn+k]s. The following depicts the t to s map on a portion of t:


b0,0
b0,1
b0,2
b0,3
. . .
b1,0
b1,1
b1,2
b1,3
. . .
b2,0
b2,1
b2,2
b2,3
. . .
b3,0
b3,1
b3,2
b3,3
. . .

7→


b0,0
b0,1
b1,0
b0,2
b1,1
b2,0
b0,3
b1,2
b2,1
b3,0


28

lgnx
= integ (1/(1+x))
D lgn = 1/(1 + x); lgn0 = 0
sinx
= integ cosx
D sin = cos; sin0 = 0
cosx
= 1-integ sinx
D cos = −sin; cos0 = 1
tanx
= integ (1+tanx^2)
D tan = 1 + tan2; tan0 = 0
secx
= 1+integ (secx * tanx)
D sec = sec tan; sec0 = 1
coshx
= 1+integ sinhx
D cosh = sinh; cosh0 = 1
sinhx
= integ coshx
D sinh = cosh; sinh0 = 0
tanhx
= integ (1-tanhx^2)
D tanh = 1 −tanh2; tanh0 = 0
gdx
= integ (1/coshx)
D gd = 1/ cosh; gd0 = 0
Table 7: Some core Sequences
The ring-with-division and square root operations pertaining to b(z, u) are isomorphically
transferred to the diagonal representation s. The representations for u and z are, u =
0u0z0+(1u1z0+0u0z1) ∼= [0x0, 1x0 +0x1] = [[0], [1, 0]] and z = 0+(0u+1z) ∼= [0, 0+1x] =
[[0], [0, 1]]. Here is (u + z)∗represented by s=pascal in Haskell,
u,z, pascal :: (Eq a, Num a) => [[a]]
u
= [[0],[1,0]]
z
= [[0],[0,1]]
pascal = starx ‘o‘ (u+z)
> take 6 pascal
[[1],[1,1],[1,2,1],[1,3,3,1],[1,4,6,4,1],[1,5,10,10,5,1]]
This displays [un−kzk](u + z)∗= [xk][xn]s =
n
k

. Commonly, we want to show a portion
of t, where [xk][xn]t = bn,k = [xn][xn+k]s, or perhaps more commonly, such that [xk][xn]t =
n![ukzn]b = n![xn][xn+k]s = [xn]Λ[xn+k]s. For example, let
b = exp ◦(z + uz) = (exp ◦z)(exp ◦uz) =
X
n
n
X
k=0
zn−k
(n −k)!
ukzk
k!
=
X
n
n
X
k=0
n
k
 ukzn
n!
Then n![ukzn]b =
n
k

. The functions unDiag and unDiage2o transpose the s-representation
into the desired t-representation (the reverse of the t 7→s map depicted above). The func-
tion unDiage2o ﬁrst removes factorial divisors associated with z. The functions select
and selectW take an argument [n0, n1, . . . , nm] saying what length to take from rows 0
to m of t. The version selectW converts whole rationals to integers. Bivariate counting
sequences, bn,k, are typically zero for k > n, and from these we select a lower triangular
section. The schroeder sequence from section 3, item C, is an example which is ordinary
in u and z, whilst ebinom below is exponential in z. The sequence powerSums of poly-
nomials for summing powers has a polynomial of order n + 1 at position n, so a lower
trapezium section [2..4] is selected.
list, pluralList :: (Eq a, Num a) => [a]
list
= starx
pluralList = list - x - 1
29

schroeder
=
z + u*(pluralList ‘o‘ schroeder)
> select [1..6] (unDiag schroeder)
[[0],[1,0],[0,1,0],[0,1,2,0],[0,1,5,5,0],[0,1,9,21,14,0]]
ebinom =
expx ‘o‘ (z+u*z)
> selectW [1..6] (unDiage2o ebinom)
[[1],[1,1],[1,2,1],[1,3,3,1],[1,4,6,4,1],[1,5,10,10,5,1]]
powerSums
= ((expx ‘o‘ (u*z))-1)/((expx ‘o‘ z)-1)
> select [2..4] (unDiage2o powerSums)
[[0 % 1,1 % 1],[0 % 1,(-1) % 2,1 % 2],[0 % 1,1 % 6,(-1) % 2,1 % 3]]
A number of supporting functions are needed. The unDiag function expects its argument
to be perfectly triangular, so padTri is ﬁrst applied to ﬁll out the triangle with zeros if
necessary. Then transpose m detaches the heads of the rows of m, which make up the ﬁrst
column, c, and this becomes the ﬁrst row of the result. A recursive invocation transposes
the remaining sub-matrix, m’.
select s t
= zipWith take s t
selectW s t
= zipWith takeW s t
unDiag
:: Num a=> [[a]]->[[a]]
unDiag
= transpose . padTri
unDiage2o
:: Fractional a=> [[a]]->[[a]]
unDiage2o
= unDiag . (map e2o)
padTri t
= zipWith padRight t [1..]
padRight r k = r++(take (k-(length r)) zeros)
transpose []
= []
transpose m
= c:transpose m’
where (c,m’) = foldr detachHead ([],[]) m
detachHead
([r0]) b
= (r0:fst b,snd b)
detachHead
(r0:r’) b = (r0:fst b,r’:snd b)
There is plenty of room for adding functions to taste.
Perhaps the main diﬃculty is
deciding on an eﬀective naming convention. Here are some examples.
takeEBivW r = (selectW r) . unDiage2o
takeEBiv
r = (select r)
. unDiage2o
takeBivW
r = (selectW r) . unDiag
takeBiv
r = (select r)
. unDiag
Diﬀerentiation with respect to z (or u) is performed by dz (or du). Below are the deﬁni-
tions, plus a test based on the set partitions recurrence from section 3, item L. Instead of
using allZeros [1..6], the reader may wish to simply use selectW [1..6] and view the
zeros (incidentally, one would then observe that the diagonal representation is not always
a perfect triangle).
dz s = map deriv (tail s)
du s = map (reverse . deriv . reverse) (tail (padTri s))
set, nonEmptySet, emptySet :: (Eq a, Fractional a) => [a]
30

set
= expx
emptySet
= 1
nonEmptySet
= set - emptySet
parts
= set ‘o‘ (u*(nonEmptySet ‘o‘ z))
allEq c r
= foldr (\a b-> (a==c) && b) True r
allZeros s t = allEq True (map (allEq 0) (select s t))
> allZeros [1..6] ((dz parts) - (u*parts +u*du parts))
True
5
Exercising the implementation
As it stands, the implementation facilitates a great range of experimentation. We will
demonstrate a few concrete examples. It will be seen that the implementation is a valuable
assistant in the study of otherwise theoretical material.
The deﬁnitions in tables 4 and 5 transliterate into Haskell. Here are some examples
(see items B and D). One has to be vigilant about when to use e2o, take, takeW,
select, selectW, unDiag, unDiage2o, etc. It is not necessary to give type declarations
to all deﬁnitions, but it will be necessary for some. We leave that as a trial-and-error
exercise.
cycle, perm :: (Eq a, Fractional a) => [a]
perm
= starx
lg g
= lgnx ‘o‘ (g-1)
cycle = lg starx
> takeW 6 (perm - (set ‘o‘ cycle))
[0,0,0,0,0,0]
cayleyTree
= x*(set ‘o‘ cayleyTree)
connectedAcyclicGraph = cayleyTree - cayleyTree^2 / 2
> takeW 8 (e2o connectedAcyclicGraph)
[0,1,1,3,16,125,1296,16807]
infix 7 |^
(|^)
:: (Eq a, Fractional a) => [a] -> a -> [a]
f |^ r
= expx ‘o‘ (r *| lg f)
legendre
= (1-2*u*z+z^2) |^ [-1%2]
> select [1..4] (unDiag legendre)
[[1 % 1],[0 % 1,1 % 1],[(-1) % 2,0 % 1,3 % 2],
[0 % 1,(-3) % 2,0 % 1,5 % 2]]
hermite
= expx ‘o‘ (2*u*z-z^2)
> select [1..4] (unDiage2o hermite)
[[1 % 1],[0 % 1,2 % 1],[(-2) % 1,0 % 1,4 % 1],
[0 % 1,(-12) % 1,0 % 1,8 % 1]]
No attention has been paid to eﬃciency or prettiness of results – all the computations are
expected to work on small examples, resulting in small results. Endless examples could
be given related to items A-Z. We must choose only a few, and we aim for variety.
31

The factorials are deﬁned in the previous section using scanl; below they are
generated directly from their diﬀerential equation, by a continued fraction recurrence,
and by shuﬄe inverse (see items E and P). Shuﬄe product can also be deﬁned by
f ⊗g = Λ(Λ−1f ∗Λ−1g)) (but that involves rationals, even when f and g are integer
sequences).
fac = 1+x*fac + x^2*(deriv fac)
> take 6 fac
[1,1,2,6,24,120]
cf_fac
= 1/(cfdenom 1)
where cfdenom n = 1 - x*(2*n-1)-x^2*n^2*(1/(cfdenom (n+1)))
> takeW 6 cf_fac
[1,1,2,6,24,120]
infix 7 |><|
-- shuffle product
f@(f0:f’) |><| g@(g0:g’) = (f0 * g0): ((f’ |><| g)+(f |><| g’))
_ |><| []
= []
[] |><| _
= []
shInv f@(f0:f’) = (1/f0): (-f’ |><| ((shInv f) |><| (shInv f)))
> takeW 6 (shInv (1-x))
[1,1,2,6,24,120]
The Newton transform, (N, N −1), is an isomorphism between the Hadamard and inﬁltra-
tion rings (see item K). It is implemented here by (h2i, i2h), with a recursive variant,
rh2i. We also translate the deﬁnitions of ∆and Σ directly to delta and sigma. Later,
we shall deﬁne another version of Σ, named prefixSums, which produces a ﬁnite result
on a ﬁnite sequence. Let’s throw into our test the ubiquitous ﬁbonacci sequence, deﬁned
by fn+2 −fn+1 −fn = 0; f0 = f1 = 1. The ﬁrst part can be re-expressed b(E)f = 0,
where b = x2 −x −1, with solution f = (`b ∗[1, 1])[0..1]
`b
= 1/(1 −x −x2) (see item S).
For illustration, we let Haskell take the last step.
h2i s
= (1/(1+x)) |><| s
i2h s
= (1/(1-x)) |><| s
rh2i s@(s0:s’) = s0: rh2i (delta s)
delta s
= (tail s) - s
sigma s
= x*starx*s
recur
:: (Eq a, Num a) => a -> [a]
recur a
= a:recur a
fib
= (take 2 (rb*[1,1]))/rb where rb = reverse (x^2-x-1)
> takeW 10 (recur 1 + sigma (delta fib))
[1,1,2,3,5,8,13,21,34,55]
> takeW 10
(i2h (h2i fib))
[1,1,2,3,5,8,13,21,34,55]
The Hadamard product, |*|, and the inﬁltration product, |^| (and infProd) are now
introduced, and testing them is left as an exercise.
infix 7 |*|
32

f |*| g = zipWith (*) f g
infix 7 |^|
f@(f0:f’) |^| g@(g0:g’) = (f0*g0): ((f’|^|g)+(f|^|g’))+(f’|^|g’)
_ |^| []
= []
[] |^| _
= []
infProd f g = h2i (i2h f |*| i2h g)
Translation to and from falling factorial polynomials can be exercised as follows. There
are, of course, more eﬃcient ways of generating the data used here (cycles, parts), but
we stick to a simple translation of the mathematical deﬁnitions. In the ﬁrst test, we use
the fact (item N) that [0, 1, 5, 14, 30, 55, . . .] is representable by a polynomial of degree 3.
The second test compares falling factorials and cycle numbers.
cycles
= set ‘o‘ (u* (cycle ‘o‘ z))
fall n m
= product [n-i | i<-take m nats]
alt
:: Num a => a->[a]
alt r
= r:alt (-r)
altMat m
= zipWith op (alt 1) m
where op sign r = zipWith (*) (alt sign) r
monom2FacPoly = takeEBiv [1..] parts
facMonom2Poly = altMat (takeEBiv [1..] cycles)
toFacPoly p
= sum (zipWith (*|) p monom2FacPoly)
fromFacPoly p = sum (zipWith (*|) p facMonom2Poly)
squaresFacPoly= o2e (take 4 (h2i [0,1,5,14,30,55]))
> fromFacPoly squaresFacPoly
[0 % 1,1 % 6,1 % 2,1 % 3]
> [fall x i | i<- [0..5]] == take 6 facMonom2Poly
True
The Maclaurin and Taylor expansions (item Y) can be coded and tested:
maclaurin f = o2e (map head (iterate deriv f))
taylor f
= map o2e (zp (map (‘o‘ u) (iterate deriv f)))
where zp (g0:g’) = g0 + z* (zp g’)
bsinx, tsinx :: [[Rational]]
bsinx = sinx ‘o‘ (u+z)
tsinx = taylor sinx
> select [1..8] bsinx == select [1..8] tsinx
True
Let us move beyond the A-Z items and look at some other examples. The Logan polyno-
mials [32, sect. 6.5], have the tangent numbers as constant terms. Here they are, deﬁned
by a closed expression, (sin ◦z + u cos ◦z)/(cos ◦z −u sin ◦z), and by an iteration.
33

logan = (((sinx ‘o‘ z)+u*(cosx ‘o‘ z))/
((cosx ‘o‘ z)-u*(sinx ‘o‘ z)))
> takeEBivW [2..5] logan
[[0,1],[1,0,1],[0,2,0,2],[2,0,8,0,6]]
loganPolys = iterate (\p -> (1+x^2) * deriv p) x
> take 4 loganPolys
[[0,1],[1,0,1],[0,2,0,2],[2,0,8,0,6]]
The Entringer triangle, E [81, 78], has the tangent numbers on the ﬁrst column (dis-
regarding the ﬁrst element), and the secant numbers on the diagonal.
Below, the tri-
angle is generated ﬁrst by a backwards and forwards (boustrophedonic) computation of
partial sums, then as the diagonal (homogeneous) presentation of coeﬃcients of A =
(sin ◦u + cos ◦u)/ cos ◦(u + z)) (named zigzags in table 5. The bivariate A is exponential
in both u and z, and En,k = (n −k)!k![un−kzk]A can be shown [32, ex. 6.75]. Forward
partial sums are preﬁx sums. Earlier, in item I, we met Σs = xx∗s for computing them,
and we used this above to deﬁne sigma. But that operator always results in an inﬁnite
sequence, even when applied to a ﬁnite one. So here we use a diﬀerent deﬁnition.
prefixSums
= scanl (+) 0
suffixSums
= reverse.prefixSums.reverse
alternate f g a = a:alternate g f (f a)
entringer
= alternate suffixSums prefixSums [1]
> take 7 entringer
[[1],[1,0],[0,1,1],[2,2,1,0],[0,2,4,5,5],[16,16,14,10,5,0],
[0,16,32,46,56,61,61]]
zigzags =
(((sinx ‘o‘ u) + (cosx ‘o‘ u))/(cosx ‘o‘ (u+z)))
ue2o
:: Fractional a => [a]->[a]
ue2o
= reverse.e2o.reverse
> selectW [1..7] (map e2o (map ue2o zigzags))
[[1],[1,0],[0,1,1],[2,2,1,0],[0,2,4,5,5],[16,16,14,10,5,0],
[0,16,32,46,56,61,61]]
The Moessner sieve generates the sequence Mr = [1r, 2r, 3r, 4r, . . .], given a positive integer
r. Kozen and Silva [52] cite a variety of proofs including sequence-calculational [38] and
coinduction methods [63]. Let us see how easily we can implement some of the compu-
tations in [52]. There it is shown that the Moessner procedure can be described as the
computation of a succession of bivariate sequences, bn(z, u), usefully represented in diago-
nal (homogeneous) form, sn, and that [u0zr]bn = [xr][xr]sn = nr. The sequence of triangles
begins with Pascal’s triangle, b0 = p = (u + z)∗, represented in diagonal form by s0. Then
the triangle-to-triangle step, sn 7→sn+1, is: take the row ρ = [xr]sn = [ρ0, ρ1, . . . , ρr],
representing hn(z, u) = ρ0urz0 + ρ1ur−1z1 + · · · + ρru0zr, and compute sn+1 representing
bn+1 = hn(z, 1) ∗p = (ρ0z0 + ρ1z1 + · · · + ρrzr) ∗p. Here is the computation of the ﬁrst
three triangles, followed by the selection of nr = [xr][xr]sn =sn!!r!!r from the ﬁrst 5
triangles. The function x2z converts ρ = [xr]sn = ρ0 + ρ1x + · · · + ρrxr to hn(z, 1) in
bivariate diagonal form (there are many ways of doing this).
x2z rho
= sum (zipWith (\c zn->[[c]]*zn) rho zPowers)
where zPowers = (iterate (*z) 1)
34

moessnerT r = iterate (\s -> (x2z (s!!r))*pascal) pascal
> select [5,5,5] (moessnerT 4)
[[[1],[1,1],[1,2,1],[1,3,3,1],[1,4,6,4,1]],
[[1],[1,5],[1,6,11],[1,7,17,15],[1,8,24,32,16]],
[[1],[1,9],[1,10,33],[1,11,43,65],[1,12,54,108,81]]]
nats2Power r = [sn!!r!!r | sn <- moessnerT r]
> take 5 (nats2Power 4)
[1,16,81,256,625]
The function moessnerT is easily changed to one which produces the n-indexed sequence
[xr]sn representing hn(z, 1), and this makes way for a generalisation. The iteration step
is [xr]sn 7→[xr]sn+1, and the iteration starts with [xr]s0 = 1, representing h0(z, 1).
moessnerH r = iterate (\rho -> ((x2z rho)*pascal)!!r) 1
> select [5,5,5,5] (moessnerH 4)
[[1],[1,4,6,4,1],[1,8,24,32,16],[1,12,54,108,81]]
Kozen and Silva generalise Moessner’s theorem to encompass theorems by Long and
Paasche [52]. In the generalised implementation, there are two new parameters, h0(z, 1)
(regarded as univariate, to be converted to bivariate by x2z), and d, a sequence [d0, d1, . . .]
of non-negative integers.
The iteration step implements the following recurrence, in
which the ﬁnal subscript indicates the selection of the homogeneous polynomial of de-
gree (deg hn) + dn:
hn+1(z, u) = [hn(z, 1) ∗p](deg hn)+dn
Thus, rather than a simple iteration, we scan along d, because step n requires dn. Let
cn, n > 0, be the leading coeﬃcient of hn(z, 1) (which we extract using last, since
the highest-order coeﬃcient is at the end). The generalised theorem entails: (a) when
h0(z, 1) = 1 and d = [r, 0, 0, . . .] we get Moessner’s result, cn = nr; (b) when h0(z, 1) =
b + (a −b)z and d = [r, 0, 0, . . .], we get Long’s result, cn = (a + (n −1)b)nr; and (c) when
h0(z, 1) = 1 and d = [d0, d1, . . .], we get Paasche’s result, cn =
n−1
Y
i=0
(n −i)di. Here is a
rather succinct implementation.
ksmlp h0 d = map last (scanl step h0 d)
where step hn dn = ((x2z hn)*pascal)!!((length hn - 1)+dn)
moessner r = ksmlp 1 (r:zeros)
long a b r = ksmlp [b,a-b] (r:zeros)
paascheFac = ksmlp 1 [1,1..]
superFac
= ksmlp 1 [1,2..]
The above computations convey some Haskell by example, and demonstrate a wealth of
experimentation assisted by sequence operations. The core set of deﬁnitions are kept to
a minimum, so that they are manageable in one ﬁle, and should not daunt beginners. In
keeping to this principle, we have not implemented an equivalent of formal Laurent series,
35

so we cannot accommodate a sequence for cot (and csc, and so on). However, we can
deﬁne x cot = (x cos)/ sin. Then, with reference to item X, let c(r) = rx(coth ◦rx), and
test x coth = c(1/2) ◦(2x), c(1/2) ◦−x = c(1/2) and c(i) = x cot (Gaussian rationals,
introducing i, are in the next section):
xcotx, xcothx :: (Eq a, Fractional a) => [a]
xcotx
= (x*cosx)/sinx
xcothx
= (x*coshx)/sinhx
xcth r
= [r]*(x*(coshx ‘o‘ ([r]*x)))/(sinhx ‘o‘ ([r]*x))
> take 10 xcothx == take 10 ((xcth (1%2)) ‘o‘ (2*x))
True
> take 10 (xcth (1%2)) == take 10 ((xcth (1%2)) ‘o‘ (-x))
True
> take 10 xcotx == take 10 (xcth i)
True
Sometimes there is simply extra work to be done to convert a sequence expression into
a form acceptable by our deﬁnitions. For example, the following expression for counting
permutations by number of valleys is derived in [22]:
K(z, u) = 1 −1
u +
√u −1
u
tan ◦(z
√
u −1 + arctan ◦
1
√u −1)
This fails to compute in our implementation for three reasons (can you spot them?). But,
by using the double-angle identity for tan, and i tanh = tan ◦ix, it can be manipulated
into the following form [20], which does compute:
K(z, u) =
√1 −u
√1 −u −tanh ◦(z√1 −u)
valleys = r/(r - (tanhx ‘o‘ (z*r))) where r = sqroot (1-u)
> takeEBivW [1..6] valleys
[[1],[1,0],[2,0,0],[4,2,0,0],[8,16,0,0,0],[16,88,16,0,0,0]]
The question of how to circumscribe a minimal core set of deﬁnitions that perhaps manifest
a timeless quality, is a challenging one. It seems only too easy to keep adding stuﬀ, as the
next section testiﬁes.
6
Building on the implementation
Implementing sequence algebra is an example of mathematics-programming synergy, as
found for example, in [62, 55, 80, 74, 65, 87, 82, 18]. One should note the chronology
of language use: [62] uses Fortran, [55] uses pseudo-Algol, [80] uses Pascal, [74] uses
Standard ML, [87] uses Maple, [82] uses Scheme, and [65, 18] use Haskell. Haskell is one
of the most recent and ambitious in the evolution of programming languages. The story
of its development [40] is an informative account of collaborative design in a scientiﬁc
36

context. It clearly reveals the tensions between the pursuit of elegant tried-and-tested
universal concepts, and pragmatically-motivated more complex and speculative features.
One has to face the fact that Haskell presents the casual newcomer with subtleties,
some of which cause baﬄement. This slightly detracts from our goal, but also means that
the implementation of sequence algebra is a ﬁne benchmark test: Haskell ought to host it
well for relative beginners. There are two prominent sources of subtleties: lazy evaluation
and type classes. The former might be discovered in working with inﬁnite matrices, for
example try rewriting transpose. The latter is likely to cause the most frustration. One
could write an elucidation of potential “surprises” centred around implementing sequence
algebra. That is beyond our scope, but we draw attention to the fact that some type
declarations can be omitted, and some not. To take just one example, the ﬁnal test of the
previous section, take 10 xcotx == take 10 (xcth i), does not go through if the type
declaration for xcotx is omitted (then the system doesn’t know to translate the rationals
in xcotx to Gaussian rationals for comparison). On the other hand, we may omit an
explicit type for xcot and use the test
makeReal (r:&0)
= r
makeReal _
= error "not real"
makeAllReal g
= map makeReal g
> take 10 xcotx == makeAllReal (take 10 (xcth i))
True
However, if the g is omitted from the deﬁnition of makeAllReal, then makeAllReal is given
a diﬀerent type and the test fails to type-check. Of course, such things have interesting
explanations, but they are potentially oﬀ-putting for beginners.
These remarks notwithstanding, one cannot resist adding to the implementation in
a myriad of ways. Here are a few next-steps, which the author has already taken, and
which are left as fruitful exercises.
• Translate [87], and elements of [80], to use Haskell.
• Introduce Gaussian rationals as an instance of Num and Fractional, and test De
Moivre’s theorem (item A). Here is part of a deﬁnition and a test of Euler’s identity:
infix
6
:&
data Gaussian a
= a :& a deriving (Eq, Read, Show)
i
:: (Eq a, Num a) => Gaussian a
i
= 0:&1
ix
:: (Eq a, Num a) => [Gaussian a]
ix = [0,i]
instance
(Num a) => Num (Gaussian a)
where
-- define negate, +, abs, signum, fromInteger
(x:&y) * (x’:&y’) =(x*x’-y*y’) :& (x*y’+y*x’)
instance
(Fractional a) => Fractional (Gaussian a)
where
(x:&y) / (x’:&y’) =
(x*x’+y*y’) / d :& (y*x’-x*y’) / d
where d
= x’*x’ + y’*y’
37

fromRational a
= fromRational a :& 0
> take 10 (cosx + [i]*sinx) == take 10 (expx ‘o‘ ix)
True
• Introduce an instance, Shuffle a, of class Num, so that one can write s |><| t as
S s * S t, and shuﬄe power sn⊗as (S s)^n. Extend the following code, making
Shuffle a an instance of Fractional. The ﬁrst test below illustrates shuﬄe power.
The second test involves the secant numbers, s = Λ sec. These can be deﬁned (see
items E and F) by applying Λ to the diﬀerential equation for sec to give s′ = s ⊗
(Λ tan), and since sec0 = 1 we get the Haskell secNums = 1:secNums |><| tanNums
(where tanNums=e2o tanx). Contrast this to the use of S:
newtype Shuffle a = S [a] deriving (Eq, Read, Show)
unS (S s) = s
instance (Eq a, Num a) => Num (Shuffle a) where
negate (S s)
= S (negate s)
(S s) + (S t) = S (s+t)
(S s) * (S t) = S (s |><| t)
fromInteger n = S [fromInteger n]
abs _
= error "abs undefined on Shuffle"
signum _
= error "signum undefined on Shuffle"
> takeW 6 (unS ((S starx)^2))
[1,2,4,8,16,32]
tanNums = e20 tanx
secNums = 1:unS (S secNums * (S tanNums))
> takeW 10 secNums
[1,0,1,0,5,0,61,0,1385,0]
• Introduce matrix computations. To keep the deﬁnitions simple, use the type [[a]]
for a matrix, and presume, controversially, that it is used responsibly, in the sense
that a matrix is presented as a list of rows of agreed length. Transpose is already
deﬁned in our implementation (written to work also for inﬁnite matrices). Opera-
tions to deﬁne include determinant, characteristic polynomial, adjugate, Gaussian
elimination, and diﬀerent methods of inversion. Then one can test computations
in proofs of the Cayley-Hamilton theorem, and experiment with bivariate Lagrange
inversion (using 2 × 2 Jacobians).
• Sequences of sequences can become confused with matrices, so it is instructive to
deﬁne:
data Matrix a
= M [[a]] | D a deriving (Eq, Read, Show)
instance (Eq a, Num a) => Num (Matrix a) where
negate (M m)
= M (map (map negate) m)
negate (D r)
= D (negate r)
(M a) + (M b)
= M ...
38

... clauses for + and *
fromInteger n = D (fromInteger n)
The idea is that if s is a square matrix of type [[a]], then we can have M s. Deﬁni-
tions of addition, M s + M t, and multiplication, M s * M t, can (with dereliction
of duty) assume that s and t are square of the same dimension. The element D r
stands for the square diagonal matrix (of any dimension) with r along the diago-
nal. The instance deﬁnitions of addition and multiplication each require four clauses
(MM, DM, MD, DD), negate has two clauses (M, D):
• Rewrite part III of [55] to use Haskell, making good use of classes and instances
to reﬂect the algebraic structure. At one level this can be approached as a pro-
gram translation exercise, and is rewarding in demonstrating Haskell to be a good
host language. At other levels it invites study of a good bit of theory (Euclidean do-
mains, ﬁnite ﬁelds, Chinese Remainder Theorem, interpolation, homomorphic image
schemes, Fast Fourier Transform, and Newton’s algorithm applied to power series).
Further to these tried-and-tested steps, there is, of course, unlimited scope for add-
ons.
Related software can be found in the Hackage repository of the Haskell website
(www.Haskell.org).
7
Concluding remarks
It is clear that sequence algebra serves calculus: many sequence identities foretell rela-
tionships between analytic functions; it serves combinatorics: many counting sequences
for discrete structures can be derived by sequence algebra; and it serves computation: it
expresses the behaviour of certain kinds of automata; it leads to interpolation methods
and summation formulae, and supports program calculation. The theory could hardly
be more foundational, and constructing an implementation from scratch emphasises its
concreteness, and has the potential to reinforce understanding.
We have exercised the implementation on examples from [32, 25], demonstrating
that it makes a valuable companion to those texts. It could be applied to other texts, for
example [15, 31, 4, 79, 77]. It can also serve as a centre-piece in a course on functional
programming in mathematics. And, indeed, the experience of typing up and experiment-
ing with the code, confronts one with intriguing issues in programming language design.
There is zero-testing on sequence elements, which could be used to open a discussion on
computability.
The on-line encyclopaedia of integer sequences [75] has hundreds of thousands of
sequences. The sequences we have mentioned can be found using the OEIS search facility.
It will be noticed that many of the sequences are accompanied by generating code written
in various languages, including Haskell.
One may like to investigate how many OEIS
entries can be expressed in the “language” of tables 4 and 5. A Haskell interface to the
OEIS is reported in [88].
Needless to say, to elaborate the topic more fully, with proof details and examples,
one needs a book-sized exposition (draft portions of a book may be requested from the
39

author). Beyond that, the obvious question is how to make a seamless progression. A
few programming-oriented suggestions are in section 6.
On the theory side, we must
acknowledge that sequence algebra is so low in the mathematical hierarchy, that it doesn’t
determine a narrow range of follow-up topics. Nevertheless, we mention a few. One is
the classiﬁcation of sequences, taking a lead from [33] and [76, Ch. 6]. Related to this is
the computer algebra work done under the heading “generating functions” or “holonomic
functions” [35, 46].
It remains to construct a bridge from the elementary level of the
present paper to the use of a computer algebra package.
Established results on diﬀerential equations, including computer-algebraic, may be
revisited with an eye to drawing out those which become particularly accessible when
specialised to sequences. One suggestion is to bring the method of characteristics as used,
for example in [22], into common parlance for sequence work. Another is to ﬁnd a smooth
passage from the level of the present paper to results obtained using the language of
Species, for example those in [4, 70] (an introduction to Species for Haskell programmers
is [89]).
Various multivariate directions beckon, including formal languages [5, 3] and mul-
tivariate Lagrange inversion [30]. We have also arrived at the threshold of analysis but
we have not crossed it, except for bringing π into item X. It is natural to ask whether
ﬂuency in inﬁnite sequences, as promoted here, has any bearing on how students approach
Cauchy sequences and analytic functions. Related to this is the progression from chapter
1 to chapter 2 in [36] (and chapter VII of [19]). On another tack, one may use sequence
algebra to motivate abstract algebra. For example, Eilenberg [19, ch. XVI, sect. 10] gives
a proof of the Cayley Hamilton theorem using module concepts, and module concepts are
used in [26, 27, 28] – papers whose titles echo [48, 49], but which involve a quantum-leap
in mathematical sophistication. As a ﬁnal remark, we note that the eponymous Haskell
B. Curry, also abstracted from concrete operations on formal power series [16].
Acknowledgements
This work originated (some years ago) when I was an occasional visitor at the University
of York. I am greatly indebted to Colin Runciman for providing that opportunity, and
to Colin, Jeremy Jacob, and Detlef Plump for encouragement. Special thanks are due to
Daniel Siemssen, Patrik Jansson, Tim Sears and Peter Thiemann for comments on work
related to this paper. (Also, if you are an anonymous JFP reviewer of an earlier related
paper, then my thanks to you too!) Tim Sears has placed a version of the Haskell code
on www.GitHub.com (under TimSears/SequenceAlgebra).
References
[1] M. Aigner and G.M. Ziegler. Proofs from THE BOOK, 3rd edn. Springer, 2004.
[2] W. Basler. Formal Power Series and Linear Systems of Meromorphic Diﬀerential
Equations. Springer, 2000.
[3] H Basold, H Hansen, J-´E Pin, and J Rutten. Newton series, coinductively: a com-
parative study of composition. Mathematical Structures in Computer Science, pages
1–29, 2017.
40

[4] F. Bergeron, G. Labelle, and P. Leroux. Combinatorial species and tree-like struc-
tures, volume 67 of Encyclopaedia of Mathematics. Cambridge University Press, 1998.
Translated from 1994 original in French.
[5] J. Berstel and C. Reutenauer. Rational Series and their Languages, volume 12 of
EATCS Monographs on Theoretical Computer Science. Springer Verlag, 1988.
[6] R. Bird and O. de Moor.
Algebra of Programming.
Series in Computer Science.
Prentice Hall International, 1997.
[7] R.S. Bird. Algebraic identities for program calculation. Computer Journal, 32(2):122–
126, 1989.
[8] L. Brand. A Division Algebra for Sequences and Its Associated Operational Calculus.
The American Mathematical Monthly, 71(7):719–728, 1964.
[9] L. Brand. Diﬀerential and Diﬀerence Equations. J. Wiley, 1966.
[10] B. Buchberger and M. Rosenkranz. Transforming problems from analysis to algebra:
A case study in linear boundary problems. J. Symbolic Computation, 47:589–609,
2012.
[11] P. Cameron. Notes on Counting: an Introduction to Enumerative Combinatorics.
Australian Mathematical Society Lecture Series. Cambridge University Press, 2017.
[12] A. Cayley. On the anayltical form called Trees. Second Part. Philosophical Magazine,
XVIII:374–378, 1859.
[13] A. Cayley. A theorem on trees. Quart. J. Pure Appl. Math., 23:376–378, 1889.
[14] W.Y.C. Chen. Context-free grammars, diﬀerential operators, and formal power series.
Theoretical Computer Science, 117:113–129, 1993.
[15] L. Comtet.
Advanced Combinatorics: The Art of Finite and Inﬁnite Expansions.
Springer, 1974.
[16] H. B. Curry. Abstract Diﬀerential Operators and Interpolation Formulas. Portugaliae
Mathematica, 10(4):135–162, 1951.
[17] N. Dershowitz and S Zaks. The Cycle Lemma and Some Applications. European J.
Combinatorics, 11(1):35–40, 1990.
[18] K. Doets and J. van Eijck. The Haskell Road to Logic, Maths and Programming,
volume 4 of Texts in Computing. College Publications, 2012.
[19] S. Eilenberg. Automata, Languages, and Machines, volume A. Academic Press, 1974.
[20] S. Elizalde and M. Noy. Consecutive patterns in permutations. Adv. Applied Mathe-
matics, 30:110–125, 2003.
[21] G. Everest, A. van der Poorten, I. Shparlinski, and T. Ward. Recurrence Sequences,
volume 104 of Mathematical Surveys and Monographs. The American Mathematical
Society, 2003.
41

[22] C.J. Fewster and D. Siemssen. Enumerating Permutations by their Run Structure.
Electronic Journal of Combinatorics, 21(4), 2014.
[23] P. Flajolet. Combinatorial Aspects of Continued Fractions. Discrete Mathematics,
32:125–161, 1980.
[24] P. Flajolet, B. Salvy, and P. Zimmermann. Automatic average-case analysis of algo-
rithms. Theoretical Computer Science, 79:37–109, 1991.
[25] P. Flajolet and R. Sedgewick. Analytic Combinatorics. Cambridge University Press,
2009.
[26] L. Gatto. Linear ODEs: an Algebraic Perspective. Instituto Nacional de Mathe´atica
e Aplicada, Rio de Janeiro, 2012.
[27] L. Gatto and D Laksov. From linear recurrence relations to linear ODEs with constant
coeﬃcients. J. Algebra and its Applicatins, 5(6):3–19, 2016.
[28] L.
Gatto
and
I.
Scherbak.
Remarks
on
the
Cayley-Hamilton
theorem.
arXiv:1510.03022, 2015.
[29] I. M. Gessel. Lagrange inversion. J. Comb. Theory Ser. A, 144(C):212–249, November
2016.
[30] I.M. Gessel. A Combinatorial Proof of the Multivariate Lagrange Inversion Formula.
J. Combinatorial Theory, Series A, 45:178–195, 1987.
[31] I.P. Goulden and D.M. Jackson. Combinatorial Enumeration. John Wiley & Sons,
1983.
[32] R.L. Graham, D.E. Knuth, and O. Patashnik.
Concrete Mathematics, 2nd edn.
Addison-Wesley, 1994.
[33] H.H. Hansen, C. Kupke, and J. Rutten. Stream Diﬀerential Equations: Speciﬁcation
Formats and Solution Methods. Logical Methods in Computer Sciience, 13(1:3):1–51,
2017.
[34] W.K. Hayman. Review of Generatingfunctionoly by H.S. Wilf. Bulletin of Americam
Math. Soc., 21(1):104–106, 1991.
[35] W. Hebisch and M. Rubey. Extended rate, more gfun. J. Symb. Comput., 46(8):889–
903, August 2011.
[36] P. Henrici. Applied and Computational Complex Analysis, Vol 1. John Wiley and
Sons, 1974.
[37] R Hinze.
Functional pearl: Streams and Unique Fixed Points.
ACM SIGPLAN
Notices, 43(9):189–200, 2008.
[38] R Hinze. Scans and convolutions – a calculational proof of Moessner’s theorem. In
Sven-Bodo Scholtz, editor, Proc. 20th Intl. Symposium on The Implementation and
Application of Functional Languages (IFL 08), vol. 5836 Lecture Notes in Computer
Science. Springer-Verlag, 2008.
42

[39] R. Hinze. Concrete Stream Calculus: An Extended Study. J. Functional Program-
ming, 20(5–6):463–535, November 2010.
[40] P. Hudak, J. Hughes, S. Peyton-Jones, and P. Wadler. A History of Haskell: Being
Lazy With Class. In Proc. Third ACM SIGPLAN History of Programming Languages
Conf. ACM, 2007.
[41] B. Jacobs and J. Rutten. A Tutorial on (Co)Algebras and (Co)Induction. Bulletin
of the EATCS, 62:222–259, 1997.
[42] N. Jacobson. Lectures in Abstract Algebra, Vol 1. Van Nostrand, 1951.
[43] W.P. Johnson.
The Curious History of Fa`a di Bruno’s Formula.
The American
Mathematical Monthly, 109:217–234, 2002.
[44] A. Joyal. Une th´eorie combinatoire des s´eries formelles. Advances in Mathematics,
42(1):1–82, 1981.
[45] J. Karczmarczuk. Generating power of lazy semantics. Theoretical Computer Science,
187:203–219, 1997.
[46] M. Kauers. The Holonomic Toolkit. In Carsten Schneider and Johannes Bl¨umlein,
editors, Computer Algebra in Quantum Field Theory: Integration, Summation and
Special Functions, pages 119–144. Springer Vienna, 2013.
[47] W.F. Keigher. On the ring of hurwitz series. Communications in Algebra, 25(6):1845–
1859, 1997.
[48] D. A. Klarner. Algebraic theory for diﬀerence and diﬀerential equations. The Amer-
ican Mathematical Monthly, 76(4):366–373, 1969.
[49] D.A. Klarner.
Some Remarks on the Cayley-Hamilton Theorem.
The American
Mathematical Monthly, 83(5):367–369, 1976.
[50] K. Knopp. Theory and Application of Inﬁnite Series, 4th edn. Blackie & Son, 1951.
[51] D.E. Knuth. Bracket Notation for the ‘Coeﬃcient of’ Operator. In A. W. Roscoe,
editor, A Classical Mind, pages 247–258. Prentice Hall International (UK) Ltd., 1994.
[52] D. Kozen and A. Silva.
On Moessner’s Theorem.
The American Mathematical
Monthly, 120(2):131–139, 2013.
[53] M. Kwapisz. The Power of a Matrix. SIAM Rev., 40(3):703–705, 1998.
[54] I.E. Leonard. The Matrix Exponential. SIAM Rev., 38(3):507–512, 1996.
[55] J.D. Lipson. Elements of Algebra and Algebraic Computing. Addison-Wesley, 1981.
[56] E. Liz. A Note on the Matrix Exponential. SIAM Rev., 40(3):700–702, 1998.
[57] M.D. McIlroy. Power series, power serious. J. of Functional Programming, 3(9):325–
337, 1999.
[58] M.D. McIlroy. The Music of Streams. Inf. Proc. Letts., 77:189–195, 2001.
43

[59] D. Merlini, R. Sprugnoli, and M.C. Verri. Lagrange Inversion: When and How. Acta
Applicandae Mathematica, 94(3):233–249, 2006.
[60] D. Merlini, R. Sprugnoli, and M.C. Verri. The Method of Coeﬃcients. The American
Mathematical Monthly, 114:40–57, 2007.
[61] H. Niederhausen. Finite Operator Calculus With Applications to Linear Recursions.
Florida Altlantic University, 2010.
[62] A. Nijenhuis and H.S. Wilf. Combinatorial Algorithms, 2nd edn. Academic Press,
1978.
[63] Milad Niqui and M. Rutten. An exercise in coinduction : Moessner ’ s theorem.
Technical Report SEN-1103, CWI, Amsterdam, 2011.
[64] I. Niven. Formal Power Series. The American Mathematical Monthly, 76:871–889,
1969.
[65] J.T. O’Donnell, C.V. Hall, and R. Page. Discrete Mathematics using a Computer,
2nd edn. Springer, 2006.
[66] I. Pak. History of Catalan Numbers. In R.P. Stanley, The Catalan Numbers. Cam-
bridge University Press, 2014.
[67] D. Pavlovi´c and M. Escard´o. Calculus in Coinductive Form. In Proc. of the 13th
Annual IEEE Symposium on Logic in Computer Science, pages 408–417. IEEE Com-
puter Society Press, 1998.
[68] S. Peyton-Jones, editor. The Haskell 98 Language Report. www.Haskel.org, 2002. See
also The Haskell 2010 Language Report, e.d. S. Marlow.
[69] J. Pitman.
Enumerations of trees and forests related to branching processes and
random walks. In Microsurveys in Discrete Probability, number 41 in DIMACS Ser.
Discrete Math. Theoret. Comp. Sci, pages 163–180, 1998.
[70] C. Pivoteau, B. Salvy, and M. Soria. Algorithms for combinatorial structures: Well-
founded systems and Newton iterations. Journal of Combinatorial Theory, Series A,
119:1711–1773, 2012.
[71] G.N. Raney.
Functional composition patterns and power series reversion.
Trans.
American Mathematical Soc., 94:441–451, 1960.
[72] J. J. M. M. Rutten. A Coinductive Calculus of Streams. Math. Struct. in Comp. Sci.,
15(1):93–147, February 2005.
[73] J.J.M.M. Rutten. Coinductive counting with weighted automata. J. Automata, Lan-
guages and Combinatorics, 8(2):319–352, 2003a.
[74] D.E. Rydeheard and R.M. Burstall. Computational Category Theory. Prentice-Hall,
1988.
[75] N.J.A. Sloane, editor. The On-Line Encyclopedia of Integer Sequences. Published
electronically at https://oeis.org.
44

[76] R.P. Stanley. Hipparchus, Plutarch, Schr¨oder, and Hough. The American Mathemat-
ical Monthly, 104:344–350, 1997.
[77] R.P. Stanley. Enumerative Combinatorics: vol 2. Cambridge Studies in Advanced
Mathematics. Cambridge University Press, 1999. With a contribution by S. Fomin.
[78] R.P. Stanley. A Survey of Alternating Permutations. Contemp. Math., 531:165–196,
2010.
[79] R.P. Stanley. Enumerative Combinatorics: vol 1 (2nd edn.). Cambridge Studies in
Advanced Mathematics. Cambridge University Press, 2011. (First ed. 1986).
[80] D. Stanton and D. White.
Constructive Combinatorics.
Undergraduate Texts in
Mathematics. Springer-Verlag, 1986.
[81] R. Street. Trees, Permutations and the Tangent Function. Reﬂections, Math. Assoc.
of NSW, 27(2):19–23, 2002.
[82] G.J. Sussman and J. Wisdom. Functional Diﬀerential Geometry. M.I.T. Press, 2012.
with W. Farr.
[83] J. F. Traub. Generalized Sequences with Applications to the Discrete Calculus. Math-
ematics of Computation, 19(90):177–200, 1965.
[84] W.T. Tutte. On Elementary Calculus and the Good Formula. Journal of Combina-
torial Theory (B), 18:79–137, 1975.
[85] M. Ward. A Calculus of Sequences. American Journal of Mathematics, 58:255–266,
1936.
[86] H.S. Wilf.
Generatingfunctionology.
Academic Press, 1990.
2nd Edition (1994)
available at http://www.cis.upenn.edu/~wilf.
[87] H.S. Wilf. East Side, West Side ... an introduction to combinatorial families – with
Maple programming. Available at http://www.cis.upenn.edu/~wilf, 2002.
[88] J. Winter. QStream: A Suite of Streams. In R. Heckel and S. Milius, editors, Algebra
and Coalgebra in Computer Science. CALCO 2013. Springer, 2013.
[89] B A. Yorgey. Species and functors and types, oh my! SIGPLAN Not., 45(11):147–158,
September 2010.
[90] D. Zeilberger. Garsia and Milne’s bijective proof of the Inclusion-Exclusion principle.
Discrete Mathematics, 51:109–110, 1984.
45

