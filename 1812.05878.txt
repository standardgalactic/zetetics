arXiv:1812.05878v2  [math.CO]  28 Feb 2019
In Praise of Sequence (Co-)Algebra and its implementation in
Haskell
Kieran Clenaghan
Abstract
What is Sequence Algebra?
This is a question that any teacher or student of
mathematics or computer science can engage with. Sequences are in Calculus, Combi-
natorics, Statistics and Computation. They are foundational, a step up from number
arithmetic. Sequence operations are easy to implement from scratch (in Haskell) and
aï¬€ord a wide variety of testing and experimentation. When bits and pieces of sequence
algebra are pulled together from the literature, there emerges a claim for status as a
substantial pre-analysis topic. Here we set the stage by bringing together a variety
of sequence algebra concepts for the ï¬rst time in one paper. This provides a novel
economical overview, intended to invite a broad mathematical audience to cast an eye
over the subject. A complete, yet succinct, basic implementation of sequence opera-
tions is presented, ready to play with. The implementation also serves as a benchmark
for introducing Haskell by mathematical example.
1
Introduction
Consider these titles: Formal Power Series [64], Power Series, Power Serious [57], A
Coinductive Calculus of Streams [72], and Concrete Stream Calculus [39]. The mixing of
the classical and the modern in these papers is stimulating and suggests a re-telling of the
elementary theory and application of sequences. Casting our net wider than the citations
in those four papers, brings up a number of corroborating works, including [85, 8, 83, 84],
in which the authors call attention to the intrinsic qualities and utility of an elementary
calculus or algebra of sequences. We do more than re-advertise this work â€“ we endeavour
to tease out the common ground, emphasising economy of statement and notation, whilst
embracing variety of approach.
Our aim is to attract those who are less well acquainted with sequence work, or those
who are unfamiliar with Haskell or both. It is so that others can enjoy â€œmessing aboutâ€, as
Hayman [34] might put it, with sequences and their implementation. Elementary sequence
algebra provides a good answer to the question, â€˜What is the smallest coherent chunk of
mathematics to set undergraduates to implement, from scratch, so that they get the
greatest reward?â€™.
First we must say that sequence algebra is an umbrella title for algebraic manip-
ulations of ï¬nite and inï¬nite sequences, p = [p0, p1, . . . , pn], f = [f0, f1, . . .], over some
given element set, F. It encompasses, prominently, formal power series algebra, but is not
restricted to it. A ï¬nite sequence, viewed as a sequence of coeï¬ƒcients for powers of x, can
be expressed as a formal polynomial. Thus [0, 1] = 0x0 + 1x1 = x, or we may say that x
1

is implemented by [0, 1]. Similarly, 3x2 âˆ’x + 4 = [4, âˆ’1, 3]. A ï¬nite sequence can also be
interpreted as an inï¬nite sequence by appending an inï¬nite sequence of zeros. An inï¬nite
sequence can be expressed as a formal power series:
f =
âˆ
X
i=0
fixi =
X
i
fixi = [f0, f1, f2, . . .]
We write fn or [xn]f for the element at position n, called the nth term of f; it is the
coeï¬ƒcient of xn in the power series view. The zeroth term can also be written f(0), but
otherwise the notation f(n) is reserved for function application: let p = 3x2 + 4, then
p2 = 3, but p(2) = 16; contrast p0 = p(0) = 4. This example reveals that the symbol p is
overloaded, it stands for a function of type N â†’F and also for another of type F â†’F,
and we must be careful to distinguish them.
It seems that in spite of Nivenâ€™s paper [64] receiving the Lester R Ford award, the
algebra of formal power series (FPS) has not entered the standard introductory university
texts in any substantial way. Nivenâ€™s paper has, however, been a key reference for the
ï¬rst chapters of both [36, 31], neither of which is standard undergraduate fare. In general,
we ï¬nd that elements of FPS/sequence algebra appear throughout a vast literature, but
the algebra itself is treated minimally, perfunctorily, or is taken for granted, as might be
expected in research work [21] and specialist texts [61, 2]. It also comes under generat-
ingfunctionlogy [86, 32]. The term â€œgenerating functionâ€ has proven to be a bit awkward,
because of the â€œfunctionâ€ part.
Notice how many times Wilf [86] and others have to
remind readers when convergence is not as issue. So, where some might use â€œgenerating
functionâ€, we use the plain â€œsequence expressionâ€. The term â€œgenerating functionâ€ is more
applicable when an analytic function interpretation is intended [19, Ch. VII]. However,
we freely use the standard names of core analytic functions for their Taylor sequences,
but rather than say, exp(x), we use just exp for the sequence.
The extent of material that ï¬ts into elementary sequence algebra is perhaps under-
appreciated. Our goal is to raise appreciation through a modest â€œsurveyâ€ of examples,
presented in section 3. Various notations and proof-styles appear in the literature, and an
eï¬€ort has been made to be inclusive and to harmonise. The one-word term â€œsequenceâ€ is
often preferred to the three-word â€œformal power seriesâ€, but both have their merits, the
latter being preferred in the multivariate case, and when formal variable substitution is
involved.
The sequence algebra we exhibit is on the same elementary level as Nivenâ€™s paper
[64]. Much detail has to be omitted so that we can cover more examples. Enough detail
is included to convey the foundational concreteness of the topic, and omitted detail is in
the literature. The Haskell implementation is given in full in section 4 so that the reader
can be in no doubt about how succinct it is, and can type it all up (or download it), and
â€œownâ€ it. The more mathematically-inclined reader may dwell on section 3, and reï¬‚ect
on the potential of a sequence algebra topic in the mathematical curriculum. The more
programming-inclined may dwell on sections 4, 5 and 6, and engage with the proposition
that sequence algebra provides an excellent vehicle for exploring learning-mathematics-
through-programming, or vice-versa. There is great scope for making a contribution to
the consolidation, reï¬nement, and application of sequence algebra as an introductory
subject.
2

To help catch, at a glance, some of the things that come under sequence algebra, we
have included a number of tables. Tables 1, 2 and 3 are instantly recognisable as belonging
to a calculus text, but here, uncommonly, the objects being related are sequences, not
analytic functions. Of course, they herald identities that hold for analytic functions, but
in agreement with Niven [64] and Tutte [84], there is something to celebrate in the fact
that the identities can be established on very elementary grounds.
There is more to
celebrate in tables 4 and 5, because many of the sequence expressions therein have a dual
interpretation as a set-theoretic structure speciï¬cation [25]. Yet more satisfaction is to be
had from table 7, because the solutions to the deï¬ning diï¬€erential equations for the core
sequences transliterate trivially into Haskell deï¬nitions [58].
Therefore a grounding in sequence algebra and its implementation surely pays oï¬€.
This claim is validated at least in the study of the two texts, Concrete Mathematics
[32] and Analytic Combinatorics [25]. There we see numerous examples where sequence
algebra is at play, and the implementation can be used for testing, for reinforcement, or
just for fun. Some of these appear in the next section. For example, in item W we derive
the bivariate power series S(z, u) = exp â—¦uz âˆ’1
exp â—¦z âˆ’1
that appears in [32, sect. 7.6].
This
generates a sequence Ë˜S such that Ë˜Sm is a polynomial, and Ë˜Sm(n) =
X
0â‰¤k<n
km. That is,
Ë˜S = [x, âˆ’1
2x + 1
2x2, 1
6x âˆ’1
2x2 + 1
3x3, . . .]. It is striking how accessible the mathematics
behind S(z, u) is, and how easy it is to deï¬ne the inï¬nite S(z, u) and Ë˜S in a program.
2
The basics
Two sequences f and g are equal if they are equal at all indices: âˆ€n â‰¥0.[xn]f = [xn]g.
There are many instances when it is obvious that a statement is subject to universal
quantiï¬cation, and in such cases we leave the âˆ€part to be inferred by the reader. Becoming
ï¬‚uent with the clumsy-looking coeï¬ƒcient extraction operator, [xn], pays dividends [31, 51].
It obeys the precedence rules, [xn]f + g = ([xn]f) + g, [xn]fg = [xn](fg), and [xn]f â—¦g =
[xn](f â—¦g). Moreover, it is a linear operator:
[xn](f + g) = [xn]f + [xn]g;
[xn]cf = c[xn]f
c is a constant
The generalisation [umzn] will be used to identify a term in a bivariate sequence. Let E
be the tail or shift-left operator: E[f0, f1, f2, . . .] = [f1, f2, . . .]. In formal power series
language, E f = 1
x(f âˆ’f0), and f = f0 + xE f; this is referred to as the head-tail property
(in [72] it is the â€œfundamental theorem of stream calculusâ€, see section 3, item P). We also
have [xn+1]f = [xn]E f and the head-tail expansion rule, [xn]f = [x0]Enf. We are free
to mix notations â€“ here is the deï¬nition of convolution product, f âˆ—g, in which, typically,
the âˆ—is suppressed:
[xn]fg =
n
X
k=0
fkgnâˆ’k
3

1.
D a
=
0
constant
2.
D x
=
1
variable
3.
D (f + g)
=
D f + D g
sum
4.
D (fg)
=
(D f)g + f(D g)
product
5.
D (f âˆ’1)
=
(âˆ’D f)/f 2
reciprocal
6.
D (f/g)
=
((D f)g âˆ’f(D g))/g2
quotient
7.
D (f â—¦)
=
((D f) â—¦f â—¦)âˆ’1
converse
8.
D (f n)
=
n(D f)f nâˆ’1
power
9.
D (anxn)
=
nanxnâˆ’1
monomial
10.
[xn]D
=
(n + 1)[xn+1]
power-series
11.
D (f â—¦g)
=
((D f) â—¦g)(D g)
composition
12.
[xn]f
=
1
n![x0]Dnf
Maclaurin
Table 1: diï¬€erentiation rules
Observe that, since x1 = 1 is the only non-zero element of x = [0, 1], we have
[x0]xf = 0;
[xn]xf = x1fnâˆ’1 = fnâˆ’1,
for n > 0
Thus x[f0, f1, f2, . . .] = [0, 1] âˆ—[f0, f1, f2, . . .] = [0, f0, f1, f2, . . .], and x can be viewed as a
right-shift operator. The absorption law, [xn]xmf = [xnâˆ’m]f, is simple but eï¬€ective. The
product in both f = f0 + xE f and f = P
i fixi is convolution product, + is pointwise
addition, the element fi is automatically identiï¬ed with the singleton sequence, [fi], as
required, and x = [0, 1]. A recursive equation for product is easily derived (and just as
easily translated into Haskell); E f is abbreviated to f â€²:
fg = (f0 + xf â€²)(g0 + xgâ€²) = f0g0 + f0xgâ€² + xf â€²g = f0g0 + x(f0gâ€² + f â€²g)
(1)
Sequences, S, over an integral domain or ï¬eld F (known from context) form an integral
domain [64, 55] (F[[x]] is the standard notation for formal power series in x over F). We
will keep F = Q in mind. The subsets S0, S1, and SÌ¸=0 comprise sequences with zeroth
term 0, 1, and non-zero, respectively. A subset SC of S0 comprises sequences f in which
f1 Ì¸= 0, that is E f âˆˆSÌ¸=0. Unique square (and nth) roots exist for sequences in S1. Se-
quence composition f â—¦g = P
k fkgk is deï¬ned for g âˆˆS0. A unique compositional inverse,
f â—¦, called converse, exists for f âˆˆSC. The notation follows [6] and distinguishes converse
f â—¦f â—¦= x from multiplicative inverse f âˆ—f âˆ’1 = 1. The latter exists for f âˆˆSÌ¸=0. Diï¬€er-
entiation, D, is term-wise, as for formal polynomials: [xn]D f = (n + 1)[xn+1]f. A little
induction gives the Maclaurin expansion rule: [xn]f = 1
n![x0]Dnf. From the deï¬nition of
R
as a right inverse to D, [xn]D
R
f = [xn]f, we deduce [xn+1]
R
f =
1
n + 1[xn]f. Setting
[x0]
R
f = 0, the fundamental theorem of sequence calculus (FTC) is immediate:
f = f0 +
Z
Df
D(
Z
f) = f
The familiar rules in tables 1 and 2 have easy sequence-algebraic proofs (the third
R
-
product rule is called the diï¬€erential Baxter axiom in [10]). For example, here is a typical
4

1.
Z
anxn
=
an
n + 1xn+1
monomial
2.
Z
(af + bg)
=
a
Z
f + b
Z
g
linear
3.
(n + 1)[xn+1]
Z
=
[xn]
power series
4.
Z
((D f) â—¦g)aD g
=
af â—¦g + c
composition
5.
Z
fD g
=
fg âˆ’
Z
(D f)g + c
product (1)
6.
Z
fg
=
f
Z
g âˆ’
Z
((D f)
Z
g)
product (2)
7.
Z
(D f)
Z
(D h) +
Z
D (fh)
=
h
Z
D f + f
Z
D h
Baxter
Table 2: integration rules
proof of the diï¬€erential composition rule, or chain rule [36, Ch. 1]. A proof by coinduction
is presented for contrast in item Q of the next section. Let f âˆˆS, g âˆˆS0, then,
D f â—¦g
=
f1 + 2f2g + 3f3g2 + Â· Â· Â· + kfkgkâˆ’1 + Â· Â· Â·
(D f â—¦g)D g
=
f1D g + 2f2gD g + 3f3g2D g + Â· Â· Â· + kfkgkâˆ’1D g + Â· Â· Â·
=
{the power rule, kgkâˆ’1D g = D gk}
f1D g + f2D g2 + f3D g3 + Â· Â· Â·
[xn](D f â—¦g)D g
=
{distribute [xn], use [xn]D gk = (n + 1)[xn+1]gk}
(n + 1)[xn+1](f1g1 + f2g2 + f3g3 + Â· Â· Â· + fn+1gn+1)
=
(n + 1)[xn+1]f â—¦g
=
[xn]D(f â—¦g)
3
Sequence Algebra examples
The following itemization (A-Z) of snippets provides a brief survey of the character of
sequence algebra. It is, of course, only a small fraction of the subject.
(A) Sequence algebra is foundational in the sense that it is a low-level concrete extension
of arithmetic. To appreciate this, try making the following simpler. Deï¬ne exp by the
sequence diï¬€erential equation D exp = exp; exp0 = 1.
Then, by the Maclaurin rule,
exp = 1 + x/1! + x2/2! + x3/3! + Â· Â· Â· = [1, 1, 1/2, 1/3!, . . .]. Deï¬ne xâˆ—by the sequence
diï¬€erence equation E xâˆ—= xâˆ—; xâˆ—
0 = 1; then, by the head-tail property, xâˆ—= 1 + x +
x2 + Â· Â· Â· = [1, 1, 1, . . .] = 1/(1 âˆ’x); the notation is based on the Kleene star [19]. Let
log f = lgnâ—¦(f âˆ’1) where lgn (for which there is no established name other than log(1+x))
is the converse of exp âˆ’1; that is, lgn = (exp âˆ’1)â—¦. Observe that (exp âˆ’1)â—¦lgn = x implies
exp â—¦lgn = 1 + x, so D lgn = D (exp âˆ’1)â—¦= (exp â—¦lgn)âˆ’1 = 1/(1 + x). The FTC gives
5

exp â—¦(log f)
=
f
exp converse
lgn â—¦f
=
lgn â—¦g
â‡’
f = g
lgn cancellation
exp â—¦f
=
exp â—¦g
â‡’
f = g
exp cancellation
log f
=
log g
â‡’
f = g
log cancellation
D (log g)
=
D g
g
log derivative
lgn â—¦f
=
0
â‡”
f = 0
zero lgn
log g
=
0
â‡”
g = 1
zero log
log(fg)
=
log f + log g
log product
log f r
=
r log f
log rational power
(r âˆˆQ)
exp â—¦(f + g)
=
(exp â—¦f)(exp â—¦g)
exp sum
expn â—¦f
=
exp â—¦nf
power exp
f r
=
exp â—¦(r log f)
general power defn
(r âˆˆF)
gr â—¦h
=
(g â—¦h)r
general distributivity
(r âˆˆF)
f rf s
=
f r+s
law of exponents
(r, s âˆˆF)
D f r
=
rf râˆ’1D f
diï¬€erential F-power
(r âˆˆF)
Table 3: Rules relating to log and exp
lgn =
R
1/(1 + x), and the coeï¬ƒcients can be calculated:
[xn+1]lgn
=
[xn+1]
Z
1
1 + x =
1
n + 1[xn]
1
1 + x =
1
n + 1(âˆ’1)n
lgn
=
x âˆ’x2
2 + x3
3 âˆ’x4
4 + Â· Â· Â·
Some well-known rules relating to log and exp, derivable from the diï¬€erential equation
for exp using sequence algebra, appear in table 3 (preconditions are omitted to avoid
clutter). Most of these are meticulously proven by Niven [64]. However, Niven does not
use composition (â—¦), but by using composition and its associativity and distributivity laws
[36], his theorem 17 and proof can be rendered as follows: let g = 1 + f, f, h âˆˆS0,
gr â—¦h
=
exp â—¦(r log g) â—¦h = exp â—¦r((lgn â—¦f) â—¦h)
=
exp â—¦r(lgn â—¦(f â—¦h)) = exp â—¦r log((f â—¦h) + 1)
=
exp â—¦r log((f + 1) â—¦h) = exp â—¦r log(g â—¦h) = (g â—¦h)r
Proof of Eulerâ€™s identity, exp â—¦ix = cos +i sin, illustrates appeal to the uniqueness of
solution to certain diï¬€erential equations. Let D sin = cos; sin0 = 0, D cos = âˆ’sin; cos0 =
1, and i2 = âˆ’1. Then both cos +i sin and exp â—¦ix satisfy D g = i g; g0 = 1, and therefore
must be equal. De Moivreâ€™s theorem follows:
(cos +i sin)n = (exp â—¦ix)n = exp â—¦ix â—¦nx = (cos +i sin) â—¦nx = cos â—¦nx + i(sin â—¦nx)
(B) A counting sequence, c, is either ordinary, c = [c0, c1, c2, c3 . . .] or exponential, c =
[c0, c1/1!, c2/2!, c3/3!, . . .]. In either case, cn counts the number of objects of size n gen-
erated by some structure speciï¬cation, C.
In the former case cn = [xn]c, and in the
latter case cn = n![xn]c. Roughly speaking, a structure is built from nodes, and its size
6

is the number of nodes it has. The nodes may be labelled or unlabelled. Exponential
sequences are used for labelled objects because, in that case, convolution product auto-
matically counts all possible labellings in making an ordered product. Let f and g count
labelled structures (generated by some F and G, respectively); then ordered pairs of such
structures are counted by
[xn]fg =
n
X
k=0
fkgnâˆ’k
k!(n âˆ’k)! =
n
X
k=0
n
k

fkgnâˆ’k/n!
The ordered k-fold product, F k, has counting sequence f k. Ordered lists of F-objects are
counted by list â—¦f = xâˆ—â—¦f = f âˆ—. If the order does not count, then we use set â—¦f =
P
k f k/k! = exp â—¦f.
The fact that permutations can be written as sets of cycles can be made explicit in
the deï¬nition of the counting sequence for permutations [25, 11]:
perm = set â—¦cycle
Just put set = exp and cycle = log xâˆ—. The sequence perm is xâˆ—regarded as an exponential
sequence, xâˆ—= [1, 1!/1!, 2!/2!, 3!/3! . . .].
Removal of the factorial divisors is performed
by Î›, and Î›perm = [1, 1, 2, 6, 24, 120, . . .], [xn]Î›perm = n![xn]perm = n!, the number of
permutations of n symbols. There are (n âˆ’1)! cyclic permutations of n symbols, and the
exponential counting sequence for these is:
cycle =
X
n>0
(n âˆ’1)!xn
n! =
X
n>0
1
nxn = âˆ’lgn â—¦(âˆ’x) = âˆ’log(1 âˆ’x) = log xâˆ—
(C) The number of ways, sn, of inserting brackets into a list of n symbols, subject to
well-formedness, is counted by the Hipparchus-SchrÂ¨oder sequence [76, 79], s = 1
4(1 + x âˆ’
p
1 âˆ’6x + x2 ) = [0, 1, 1, 3, 11, 45, 197, 903, 4279, 20793, 103049, . . .]. This can be derived
from the speciï¬cation of a bivariate counting sequence for SchrÂ¨oder trees:
schroeder(z,u) = z + u âˆ—(pluralList â—¦schroeder(z,u))
(2)
Here pluralList = list âˆ’x âˆ’1. The coeï¬ƒcient of ukzn in schroeder(z,u) gives the number
of SchrÂ¨oder bracketings of n symbols using k pairs of brackets. Observe that equation (2)
can also be read [25] as a set-theoretic speciï¬cation of SchrÂ¨oder trees, with z and u naming
diï¬€erent kinds of nodes. Using pluralList = xâˆ—âˆ’x âˆ’1, the above deï¬nition of s derives,
via the quadratic formula, from the equation s = schroeder(x,1) = x + (xâˆ—âˆ’x âˆ’1) â—¦s =
x + sâˆ—âˆ’s âˆ’1 = x +
1
1 âˆ’s âˆ’s âˆ’1. Here is a foretaste of computing the SchrÂ¨oder numbers
in Haskell, which is explained in section 4:
schroeder
=
z + u*(pluralList â€˜oâ€˜ schroeder)
> takeBiv [1..6] schroeder
[[0],[1,0],[0,1,0],[0,1,2,0],[0,1,5,5,0],[0,1,9,21,14,0]]
> takeW 11 ((1+x-sqroot(1-6*x+x^2))/4)
[0,1,1,3,11,45,197,903,4279,20793,103049]
7

This reveals, for example, [u2z5]schroeder = 9, that is 9 bracketings of 5 symbols with
2 pairs of brackets. One can see that the elements of the second sequence are totals of
corresponding elements of the ï¬rst. We remark that the sequence r = 2s/x âˆ’1 is called
the large SchrÂ¨oder sequence [25, p. 474] (it solves r = 1 + xr + xr2).
(D) The dual interpretation of sequence expressions as set-theoretic structure speciï¬-
cations, is exploited in [24, 25] (inï¬‚uenced by [44]). One might write the set-theoretic
counterpart of say, perm = set â—¦cycle, as Perm âˆ¼= Set â—¦Cycle, indicating by the initial cap-
ital letters a set-theoretic interpretation. Essentially this is done in [25], but for brevity
we only give the equations deï¬ning the counting sequences. Table 4 lists some univariate
examples, and table 5 some bivariate ones. These can be typed more-or-less verbatim into
Haskell, as illustrated in section 5. Many more could be lifted from chapters I-III of [25],
from the appendices of [4], and from [15, 31, 79, 76].
Let us examine a less-than-obvious expression, ascents from table 5, the origin of
which illustrates the principle of inclusion-exclusion [25, 90].
It counts permutations
according to the number of ascents. For example, the permutation |248|3679|5|1| of {1..9}
has 4 up-runs demarcated with vertical bars, 3 descents, and 5 ascents. If there are k
descents then there are k + 1 up-runs. The reversal of a permutation with k descents
delivers a permutation with k ascents. The count n![uk][zn]ascents is called an Eulerian
number, and gives the number of permutations of {1..n} with k ascents [32]. To come up
with the sequence expression, ï¬rst note that an up-run with at least one ascent corresponds
to a plural set. The counting sequence for such a set in which the k of uk records the
ascents is (pluralSet â—¦uz)/u.
Let us specify permutations in which some parts of up-
runs are identiï¬ed as sets, and other elements are undistinguished: b(z, u) = list â—¦(z +
(pluralSet â—¦uz)/u).
Now propose that ascents(z, u) is the exact counting sequence we
are after, then the inclusion-exclusion principle says that ascents(z, u + 1) = b(z, u) and
ascents(z, u) = b(z, u âˆ’1), which is cited in the table.
(E) The sequence deï¬ned by D tan = 1 + tan âˆ—tan; tan0 = 0 is the Maclaurin expansion
of the tangent function (the âˆ—is explicit just for emphasis). The numbers in t = Î› tan =
[0, 1, 0, 2, 0, 16, 0, 272, 0, 7936, . . .] are called tangent numbers. The tangent numbers count
certain kinds of alternating permutations (or ordered binary trees) [78]. One can deï¬ne t
also by E t = 1 + t âŠ—t, where âŠ—is shuï¬„e (or Hurwitz [47]) product. Convolution product
(1) and shuï¬„e product can be deï¬ned in head-tail form (E â‰¡
â€²):
(st)â€²
=
sâ€²t + s0tâ€²
(st)0 = s0t0
(s âŠ—t)â€²
=
sâ€² âŠ—t + s âŠ—tâ€²
(s âŠ—t)0 = s0t0
The rule E(sâŠ—t) = E sâŠ—t+sâŠ—E t matches D(fg) = D f +fD g and we have the Leibniz
formulae
Dn(fg) =
n
X
k=0
n
k

(Dkf)Dnâˆ’kg;
En(s âŠ—t) =
n
X
k=0
n
k

Eks âŠ—Enâˆ’kt
Applying [x0] to the latter gives the pointwise deï¬nition of âŠ—(note that a âŠ—b = ab when
8

emptySet
=
1
singletonSet
=
x
singletonList
=
x
nonEmptyList
=
list âˆ’1
pluralList
=
list âˆ’singletonList âˆ’1
ordPair
=
x2
ï¬bonacci
=
list â—¦(singletonList + ordPair)
cycle
=
log xâˆ—
oneCycle
=
x
oneOrTwoCycle
=
oneCycle + x2/2
involution
=
set â—¦oneOrTwoCycle
nonLoopCycle
=
cycle âˆ’singletonSet
derangement
=
set â—¦nonLoopCycle
permutation
=
derangement âˆ—set
nonEmptySet
=
set âˆ’empty
pluralSet
=
nonEmptySet âˆ’singletonSet
setPartition
=
set â—¦nonEmptySet
oddNumberOfParts
=
sinh â—¦nonEmptySet
evenSizedParts
=
set â—¦(cosh âˆ’1)
catalanTree
=
x(list â—¦catalanTree)
cayleyTree
=
x(set â—¦cayleyTree)
connectedAcyclicGraph
=
cayleyTree âˆ’1
2cayleyTree2
acyclicGraph
=
set â—¦connectedAcyclicGraph
motzkinTree
=
x(1 + motzkinTree + motzkinTree2)
hipparchusSchroeder
=
(1 + x âˆ’
âˆš
1 âˆ’6x + x2)/4
largeSchroeder
=
2 âˆ—hipparchusSchroeder/x âˆ’1
connectedMapping
=
cycle â—¦cayleyTree
mapping
=
set â—¦connectedMapping
ï¬xedPointFree
=
set â—¦nonLoopCycle â—¦cayleyTree
idempotent
=
set â—¦(oneCycle âˆ—set )
partialMapping
=
mapping âˆ—(set â—¦cayleyTree)
surjection
=
list â—¦nonEmptySet
zigzag
=
2(tan + sec)
bernoulli
=
x/(exp âˆ’1)
Table 4: Some counting sequences
9

pascal
=
(u + z)âˆ—
intComposition
=
list â—¦(u âˆ—(nonEmptyList â—¦z))
schroeder
=
z + u âˆ—(pluralList â—¦schroeder)
catalanLeaves
=
u âˆ—z + z âˆ—(nonEmptyList â—¦catalanLeaves)
cayleyLeaves
=
u âˆ—z + z âˆ—(nonEmptySet â—¦cayleyLeaves)
ebinom
=
set â—¦(z + uz)
cycles
=
set â—¦(u âˆ—(cycle â—¦z))
parts
=
set â—¦(u âˆ—(nonEmptySet â—¦z))
permFixedPts
=
(derangement â—¦z) âˆ—(set â—¦uz)
zigzags
=
(sin â—¦u + cos â—¦u)/ cos â—¦(u + z)
ascents
=
list â—¦(z + (pluralSet â—¦(uz âˆ’z))/(u âˆ’1))
valleys
=
âˆš1 âˆ’u
âˆš1 âˆ’u âˆ’tanh â—¦(zâˆš1 âˆ’u)
powerSums
=
exp â—¦uz âˆ’1
exp â—¦z âˆ’1
bernoulliPoly
=
z exp â—¦uz
exp â—¦z âˆ’1
legendre
=
(1 âˆ’2uz + z2)âˆ’1/2
chebyshev
=
1 âˆ’uz
z2 âˆ’2uz + 1
laguerre
=
1
1 âˆ’z exp â—¦âˆ’uz
1 âˆ’z
hermite
=
exp â—¦(2uz âˆ’z2)
meixner
=
(1 + z2)âˆ’1/2 exp â—¦(u arctan â—¦z)
Table 5: Some bivariate sequences
10

a and b are scalars):
[xn](s âŠ—t) =
n
X
k=0
n
k

([x0]Eks) âŠ—([x0]Enâˆ’kt) =
n
X
k=0
n
k

sktnâˆ’k
A shuï¬„e inverse, sâˆ’1âŠ—, derives from the speciï¬cation s âŠ—sâˆ’1âŠ—= 1 together with the
shuï¬„e product rule (in exactly the same way that the rule for D f âˆ’1 is derived):
0 = 1â€² = (s âŠ—sâˆ’1âŠ—)â€² = sâ€² âŠ—sâˆ’1âŠ—+ s âŠ—(sâˆ’1âŠ—)â€²;
(sâˆ’1âŠ—)â€² = âˆ’sâ€² âŠ—sâˆ’1âŠ—âŠ—sâˆ’1âŠ—
(F) Let SF(âˆ—) denote the ring (SF , +, âˆ—, 0, 1), of sequences over some ï¬eld F (of characteris-
tic 0) with the availability of inverses implied. Then (SF (âˆ—), D,
R
) is an integro-diï¬€erential
algebra [10], and so too is (SF (âŠ—), E, x), where x is the right-shift operator: xf = x âˆ—f.
The transform (Î›, Î›âˆ’1) is an isomorphism between them, and is a formal Laplace trans-
form [67, 26]. In the previous example, the equation deï¬ning tan is transformed by Î› into
the equation deï¬ning t. The sequence of factorial numbers , xâŠ—= Î›xâˆ—, can be deï¬ned by
applying Î› to xâˆ—= 1+xxâˆ—to get xâŠ—= 1+xâŠ—xâŠ—. From this we deduce xâŠ—= (1âˆ’x)âˆ’1âŠ—.
Observe, for n > 0:
[xn]xâŠ—= [xn]x âŠ—xâŠ—=
n
X
k=0
n
k

[xk]x[xnâˆ’k]xâŠ—=
n
1

[xnâˆ’1]xâŠ—= n[xnâˆ’1]xâŠ—
Also, [xn]xâŠ—= n[xnâˆ’1]xâŠ—= (n âˆ’1)[xnâˆ’1]xâŠ—+ [xnâˆ’1]xâŠ—= [xn]x2D xâŠ—+ [xn]xxâŠ—leads to
the diï¬€erential equation for the factorials:
xâŠ—= 1 + xxâŠ—+ x2D xâŠ—
Furthermore,
(xâŠ—)â€² = ((1 âˆ’x)âˆ’1âŠ—)â€² = (1 âˆ’x)âˆ’2âŠ—;
xâŠ—= 1 + x(1 âˆ’x)âˆ’2âŠ—
(G) We recall a classic proof [64] of the binomial theorem. Let r âˆˆF, rk = r(râˆ’1) Â· Â· Â· (râˆ’
k + 1) (the falling factorial), and rk = r(r + 1) Â· Â· Â· (r + k âˆ’1) (the rising factorial):
[xk](1 + bx)r = 1
k![x0]Dk(1 + bx)r = 1
k![x0]rkbk(1 + bx)râˆ’k = bk
r
k

A corollary is [xk](xâˆ—)r = [xk](1 âˆ’x)âˆ’r = (âˆ’1)k(âˆ’r)k/k! = rk/k! =
r + k âˆ’1
r âˆ’1

. This
result, and [xm] expn = [xm] exp â—¦nx = nm/m!, are basic ingredients in the search for nth
term formulas. They are applied next.
(H) A Lagrange inversion formula [77, 59, 60, 29] gives an expression for the nth term of
the converse of a sequence. For example, let g = x(r â—¦g), then x = g/(r â—¦g) = (x/r) â—¦g,
so g is the converse of x/r. Below is the Lagrange inversion formula for this case, followed
11

by its application to the counting sequences for Catalan trees, c = x(list â—¦c) = x(xâˆ—â—¦c),
and Cayley trees, t = x(set â—¦t) = x(exp â—¦t):
[xn]g
=
1
n[xnâˆ’1]rn
[xn]c
=
1
n[xnâˆ’1](xâˆ—)n = 1
n
2n âˆ’2
n âˆ’1

[xn]t
=
1
n[xnâˆ’1] expn = 1
n
nnâˆ’1
(n âˆ’1)! = nnâˆ’1
n!
For a history of the Catalan numbers, see [66]. Cayley trees are rooted versions of the
connected acyclic graphs counted by Cayley in [13].
Cayley also counted the Catalan
trees in [12], and the ï¬rst part of Niven [64] sets out to legitimise the sequence algebra
underlying Cayleyâ€™s proof (Niven cites [42], not Cayley; but Raney [71] cites both).
A slightly more general statement of Lagrange inversion is that it solves h = g â—¦f
(equivalently g = h â—¦f â—¦) for g, where h âˆˆS, f âˆˆSC. The theory of Lagrange inversion
sometimes employs Laurent series â€“ series with negative powers (or sequences with negative
indicies). In the following formula for the nth term of g = h â—¦f â—¦, the coeï¬ƒcient of xâˆ’1
(called the residue) is identiï¬ed:
[xn]g = 1
n[xâˆ’1]D hf âˆ’n;
[x0]g = h0
Let s = x(râ—¦s), s = (x/r)â—¦, and h = gâ—¦(x/r); then Lagrange inversion applied to g = hâ—¦s,
gives, for h = xk, the nth term formula [xn]sk = k
n[xnâˆ’k]rn. This result specialises, when
r = xâˆ—, to a variant of the cycle lemma [17], called Raneyâ€™s lemma in [32], which has a
history in statistics [69] related to Ballot numbers [25, p. 68]. There are various Lagrange
inversion formulas and many proofs; [14] takes an approach that also facilitates proof of
Fa`a di Brunoâ€™s formula formula for Dn(f â—¦g) [43].
(I) The (forward) diï¬€erence operator, âˆ†s = [E âˆ’1]s = sâ€² âˆ’s produces the sequence of
term-to-term diï¬€erences, [xn]âˆ†s = sn+1âˆ’sn. The deï¬nition of an anti-diï¬€erence operator
Î£ on sequences is calculated [37] as a right identity to âˆ†, with (Î£s)0 = 0:
âˆ†Î£s = s â‡”(Î£s)â€² âˆ’Î£s = s â‡”(Î£s)â€² = Î£s + s â‡”Î£s = 0 + x(Î£s + s) â‡”Î£s =
xs
1 âˆ’x
Thus, Î£s = xxâˆ—s computes all the preï¬x sums of s (including the empty one). Applied to
âˆ†s we get:
[xn]Î£âˆ†s = [xn]xxâˆ—âˆ†s = [xnâˆ’1]xâˆ—âˆ†s =
nâˆ’1
X
i=0
 [xi+1]s âˆ’[xi]s

= [xn]s âˆ’[x0]s
There follows the fundamental theorem of discrete calculus (FDC) on sequences: s =
s â€¢
0 + Î£âˆ†s; s = âˆ†Î£s, where a â€¢ = axâˆ—is the sequence with a everywhere.
12

(J) Here are the E to âˆ†translations extended to powers:
En = (1 + âˆ†)n
=
n
X
k=0
n
k

âˆ†k
[xn]s = (Ens)0
=
n
X
k=0
n
k

(âˆ†ks)0
(3)
âˆ†n = (E âˆ’1)n
=
n
X
k=0
n
k

(âˆ’1)nâˆ’kEk
(âˆ†ns)0
=
n
X
k=0
n
k

(âˆ’1)nâˆ’ksk = [xn](âˆ’x)âˆ—âŠ—s
(4)
The identity
n
k

= [xn]xk/(1 âˆ’x)k+1 turns equation (3) into the Euler expansion, s =
X
k
(âˆ†ks)0xk
(1 âˆ’x)k+1 . This expansion can also be derived from s âŠ—(âˆ’x)âˆ—= P
k(âˆ†ks)0xk, plus
the facts (âˆ’x)âˆ—âŠ—xâˆ—= 1, and xk âŠ—xâˆ—= xk/(1 âˆ’x)k+1 (see [3] and items K and P):
s = s âŠ—(âˆ’x)âˆ—âŠ—xâˆ—= (s0 + (âˆ†s)0x + (âˆ†2s)0x2 + Â· Â· Â·) âŠ—xâˆ—=
X
k
(âˆ†ks)0xk
(1 âˆ’x)k+1
(5)
Let g = lgn â—¦âˆ’x, whence lgn = g â—¦âˆ’x, and apply Eulerâ€™s expansion to g,
lgn
=
 X
k
xk
(1 âˆ’x)k+1 (âˆ†kg)0
!
â—¦âˆ’x =
X
k
(âˆ’x)k
(1 + x)k+1 (âˆ†kg)0
lgn(1)
=
X
k
(âˆ’1)k
2k+1 (âˆ†kg)0
It is instructive to use this to approximate log(2) = lgn(1).
(K) The sequence Ns = (âˆ’x)âˆ—âŠ—s = [s0, (âˆ†s)0, (âˆ†2s)0, . . .] is the sequence of Newton
coeï¬ƒcients [3]. It may also be speciï¬ed by (Ns)â€² = N(âˆ†s); (Ns)0 = s0. The operator
N = ((âˆ’x)âˆ—âŠ—), called the Newton transform in [3], has the converse N âˆ’1 = (xâˆ—âŠ—),
called the Binomial transform in [39]. The identity xâˆ—âŠ—(âˆ’x)âˆ—= 1 holds because the head
of xâˆ—âŠ—(âˆ’x)âˆ—is 1, and the tail is
(xâˆ—âŠ—(1 + x)âˆ’1)â€² = xâˆ—âŠ—((1 + x)âˆ’1 + ((1 + x)âˆ’1)â€²) = 0
The following products introduce two new rings, the Hadamard ring (SR, +, âˆ’, âŠ™, 0, 1 â€¢ ),
and the inï¬ltration ring (SR, +, âˆ’, â†‘, 0, 1) [3]:
(s âŠ™t)â€²
=
sâ€² âŠ™tâ€²
(s âŠ™t)0
=
s0t0
(s â†‘t)â€²
=
sâ€² â†‘t + s â†‘tâ€² + sâ€² â†‘tâ€²
(s â†‘t)0
=
s0t0
13

1.
âˆ†(s âŠ™t)
=
s âŠ™âˆ†t + âˆ†s âŠ™tâ€²
âˆ†-product (1)
2.
âˆ†(s âŠ™t)
=
âˆ†s âŠ™t + s âŠ™âˆ†t + âˆ†s âŠ™âˆ†t
âˆ†-product (2)
3.
Î£(s âŠ™âˆ†t)
=
s âŠ™t âˆ’Î£(âˆ†s âŠ™tâ€²) âˆ’(s âŠ™t) â€¢
0
Î£-product (1)
4.
Î£(sâ€² âŠ™v)
=
s âŠ™(Î£v) âˆ’Î£(âˆ†s âŠ™Î£v)
Î£-product (2)
5.
Î£âˆ†s âŠ™Î£âˆ†u + Î£âˆ†(s âŠ™u)
=
(Î£âˆ†s) âŠ™u + s âŠ™(Î£âˆ†u)
Î£-Baxter rule
Table 6: âˆ†âˆ’Î£ rules
The rules in table 6 apply, and (N, N âˆ’1) is an isomorphism between the Hadamard and
inï¬ltration rings. The following is a point-wise deï¬nition of s â†‘t:
[xn]s â†‘t = [xn]N(N âˆ’1s âŠ™N âˆ’1t) =
n
X
i=0
n
i

(âˆ’1)nâˆ’i[xi]((xâˆ—âŠ—s) âŠ™(xâˆ—âŠ—t))
The proof that N is a morphism from âŠ™to the new product â†‘can be re-imagined as a
discovery of what the deï¬nition of â†‘should be. The morphism presumption is signalled
on the right below.
(N(s âŠ™t))â€²
=
N(âˆ†(s âŠ™t))
defn. N
=
N(âˆ†s âŠ™t + s âŠ™âˆ†t + âˆ†s âŠ™âˆ†t)
âˆ†-product (2)
=
N(âˆ†s âŠ™t) + N(s âŠ™âˆ†t) + N(âˆ†s âŠ™âˆ†t)
morphism
=
Nâˆ†s â†‘Nt + Ns â†‘Nâˆ†t + Nâˆ†s â†‘Nâˆ†t
morphism
=
(Ns)â€² â†‘Nt + Ns â†‘(Nt)â€² + (Ns)â€² â†‘(Nt)â€²
defn. N
=
(Ns â†‘Nt)â€²
defn. â†‘
(L) The following deï¬nes permutation cycle numbers,
n
k

= n![ukzn]cycles, and set par-
tition numbers
n
k

= n![ukzn]parts. These are also called Stirling numbers of the ï¬rst
and second kind, respectively.
cycles
=
set â—¦(u âˆ—(cycle â—¦z))
parts
=
set â—¦(u âˆ—(nonEmptySet â—¦z))
The well-known recurrences [32],
n + 1
k

=

n
k âˆ’1

+ n
n
k

,
n + 1
k

=

n
k âˆ’1

+ k
n
k

translate, using c = cycles and p = parts, into
Dzc = uc + zDzc
Dzp = up + uDup
where Dz and Du are the partial diï¬€erentiation operators with respect to z and u. To
see this, note that
n + 1
k

= (n + 1)![ukzn+1]c = n![ukzn]Dzc,

n
k âˆ’1

= n![ukâˆ’1zn]c =
14

n![ukzn]uc, and so on. The recurrences can be checked: ï¬rst, Dzc = Dz exp â—¦(u log zâˆ—) =
uczâˆ—= uc + uczzâˆ—= uc + zDzc; and second, up + uDup = up + u(exp â—¦z âˆ’1)p =
pu exp â—¦z = Dzp. We may write n![zn] exp â—¦(u log â—¦zâˆ—) = n![zn](1âˆ’z)âˆ’u = un. The cycles
recurrence can also be written [xk]xn = [xkâˆ’1]xnâˆ’1 + (n âˆ’1)[xk]xnâˆ’1, which follows from
xn = xnâˆ’1(x + n âˆ’1) = xxnâˆ’1 + (n âˆ’1)xnâˆ’1.
(M) A factorial polynomial uses falling factorials instead of powers. For example, let
p = 1 + 2x + x2, then the falling factorial counterpart is p = 1 + 3x1 + x2. Coeï¬ƒcients in
p are identiï¬ed by [xk]p, for example [x1]p = 3.
The symbols Î£ and âˆ†are overloaded as operators on factorial polynomials and obey
rules identical to those for D and
R
on polynomials: let p denote a polynomial in falling
factorials, then [xk]âˆ†p = (k + 1)[xk+1]p and [xn+1]Î£p =
1
n + 1[xn]p. The fundamental
theorem of the discrete calculus on factorial polynomials is immediate: p = p0 +Î£âˆ†p; p =
âˆ†Î£p. In [32], the theorem provides one of seven ways of deducing the polynomial for
summing squares: given âˆ†p = (1 + x)2 = 1 + 3x1 + x2, apply Î£ to both sides,
p âˆ’p0 = 1x1 + 3/2 x2 + 1/3 x3; p = 1/6 (x + 3x2 + 2x3)
(6)
Note that if p is a polynomial nth term formula for sequence s, p(n) = sn, then (âˆ†p)(n) =
(âˆ†s)n and (Î£âˆ†p)(n) = p(n) âˆ’p(0) = sn âˆ’s0 = (Î£âˆ†s)n.
(N) An analogue of the Maclaurin rule holds: [xn]p = 1
n![x0]âˆ†np = 1
n!(âˆ†np)(0). The
latter equality involves yet another interpretation of âˆ†: (âˆ†p)(n) = p(n + 1) âˆ’p(n).
Gregory-Newton (interpolation) formulas for p of degree m follow; the second (see also
(3)) uses nk/k! =
n
k

:
p
=
p(0)x0 + (âˆ†p)(0)x1 + (âˆ†2p)(0)
2!
x2 + Â· Â· Â· + (âˆ†mp)(0)
m!
xm
(7)
p(n)
=
p(0)
n
0

+ (âˆ†p)(0)
n
1

+ (âˆ†2p)(0)
n
2

+ Â· Â· Â· + (âˆ†mp)(0)
 n
m

(8)
Let s = [0, 1, 5, 14, 30, 55, . . .] be the sequence sn = 12 +22 +Â· Â· Â·+n2, for which we seek the
polynomial p such that p(n) = sn. Then the above expansions produce the polynomial(s)
in (6). By contrast, the Euler expansion (5) produces the sequence expression s = (x +
x2)/(1 âˆ’x)4.
(O) We have seen [xk]xn =
n
k

, so we can express the polynomial for xn in terms of cycle
numbers, and by change of signs, also the polynomial for xn:
xn =
n
X
k=1
n
k

xk;
xn =
n
X
k=1
(âˆ’1)nâˆ’k
n
k

xk;
x0 = x0 = 1
15

This shows how to translate falling factorials into powers. The converse is
xn =
n
X
k=1
n
k

xk
[xk]xn =
n
k

and for a proof see [9, p. 343] and [32, p. 262].
(P) Inï¬nite sequences are called streams when they are identiï¬ed with the ï¬nal object in
a category of head-tail coalgebras [41, 72]. This accounts for the name â€œstreamâ€ in the
following:
Fundamental theorem of (sequence) calculus (FTC)
f
=
f0 +
R
D f
f
=
D
R
f
Fundamental theorem of stream calculus (FSC)
f
=
f0 + x(Ef)
f
=
E(xf)
Fundamental theorem of discrete calculus (FDC)
f
=
f â€¢
0 + Î£âˆ†f
f
=
âˆ†Î£f
The co-algebraic stream calculus [72] introduces a proof principle called coinduction. For
example, the identity xk âŠ—xâˆ—= xk/(1 âˆ’x)k+1 = xâˆ—(xxâˆ—)k used in the proof of (5) can be
proved using coinduction (it can also be proved from the point-wise deï¬nition of âŠ—and
the binomial theorem). Here is the gist of the coinductive proof. Propose the relation
xk âŠ—xâˆ—âˆ¼xâˆ—(xxâˆ—)k.
This is used as a coinductive hypothesis.
Head-equality holds,
(xk âŠ—xâˆ—)0 = (xâˆ—(xxâˆ—)k)0. The proof is completed by showing that the tails are equal
under the hypothesis, signiï¬ed below by the use of (âˆ¼):
(xâˆ—(xxâˆ—)k)â€²
=
(xâˆ—)â€²(xxâˆ—)k + 1((xxâˆ—)k)â€²
conv. product rule
=
xâˆ—(xxâˆ—)k + xâˆ—(xxâˆ—)kâˆ’1
(xâˆ—)â€² = xâˆ—and ((xf)n)â€² = f(xf)nâˆ’1
âˆ¼
xk âŠ—xâˆ—+ xkâˆ’1 âŠ—xâˆ—
coinduction hyp.
=
(xk âŠ—xâˆ—)â€²
shuï¬„e product rule
The bracketed (co-) in the paperâ€™s title indicates that we only touch lightly on co-algebraic
concepts. There is more to coinduction than the above example suggests, and we refer to
the survey [33] for background.
(Q) One can check from the pointwise deï¬nitions of D and âŠ—that D f = (x âŠ—f â€²)â€².
Alternatively, equality can be proved by showing that these expressions satisfy the same
head-tail equations. The head-tail equation for D f is calculated:
D f = D(f0 + xf â€²) = f â€² + xD f â€² = f â€²
0 + xf â€²â€² + xD f â€² = f1 + x(f â€²â€² + D f â€²)
Hence, (D f)0 = f1 and (D f)â€² = f â€²â€² + D f â€². Now let F f = (x âŠ—f â€²)â€². We ï¬nd (F f)0 = f1,
and (F f)â€² = (f â€² + (x âŠ—f â€²â€²))â€² = f â€²â€² + F f â€². Thus, D f = F f since they satisfy the same
head-tail equations.
To give a little more feeling for the coinduction game, let us reveal the machine-
level minutiae that proves D(f â—¦g) = (D f â—¦g)D g. We use head-tail properties such as
16

(f â—¦g)0 = f0; (f â—¦g)â€² = (f â€² â—¦g)gâ€² (see section 4, equation (13)). We will also make use of
(Dh)0 = hâ€²
0. Head equality is conï¬rmed:
(D(f â—¦g))0 = (f â—¦g)â€²
0 = (f â€² â—¦g)0gâ€²
0 = f â€²
0gâ€²
0 = (Df â—¦g)0(Dg)0 = ((Df â—¦g)Dg)0
Tails are proved equal under the coinductive hypothesis, D(f â—¦g) âˆ¼(Df â—¦g)Dg:
((Df â—¦g)Dg)â€²
=
(Df â—¦g)â€²Dg + (Df â—¦g)0(Dg)â€²
(â€²)-product
=
((Df)â€² â—¦g)gâ€²Dg + (f â€² â—¦g)0(Dg)â€²
(â€²)-composition
=
(f â€²â€² â—¦g)gâ€²Dg + (Df â€² â—¦g)gâ€²Dg + (f â€² â—¦g)0(Dg)â€²
D-defn and â—¦-distr
âˆ¼
(f â€² â—¦g)â€²Dg + D(f â€² â—¦g)gâ€² + (f â€² â—¦g)0(Dg)â€²
(â€²)-comp., coinduction
=
(f â€² â—¦g)â€²(gâ€² + xDgâ€²) + D(f â€² â—¦g)gâ€² + (f â€² â—¦g)0(gâ€²â€² + Dgâ€²)
expand D g and (D g)â€²
=
D(f â€² â—¦g)gâ€² + (f â€² â—¦g)â€²xDgâ€² + (f â€² â—¦g)0Dgâ€² + (f â€² â—¦g)â€²gâ€² + (f â€² â—¦g)0gâ€²â€²
=
D(f â€² â—¦g)gâ€² + (f â€² â—¦g)Dgâ€² + ((f â€² â—¦g)gâ€²)â€²
head-tail, h = h0 + xhâ€²
=
D((f â€² â—¦g)gâ€²) + ((f â€² â—¦g)gâ€²)â€²
D-product
=
D(f â—¦g)â€² + (f â—¦g)â€²â€²
(â€²)-composition, twice
=
(D(f â—¦g))â€²
D-defn
(R) Coinduction is also used in [73] to show how continued fractions can be obtained from
combinatorially-inspired automata. For example, the tangent sequence can be deï¬ned by
t = xu1, where uk = 1/(1 âˆ’k(k + 1)x2uk+1). Thus t can be displayed as a continued
fraction:
t =
x
1 âˆ’
1 âˆ—2x2
1 âˆ’
2 âˆ—3x2
1 âˆ’3 âˆ—4x2
...
More combinatorially-inspired continued fraction expressions for sequences appear in [23,
31, 73]. Below is one for Î›zcycles = (1âˆ’z)âˆ’uâŠ—(Î›z removes the factorial divisors of powers
of z).
(1 âˆ’z)âˆ’uâŠ—=
1
1 âˆ’uz âˆ’
1uz2
1 âˆ’(u + 2)z âˆ’
2(u + 1)z2
1 âˆ’(u + 4)z âˆ’3(u + 2)z2
...
Setting u = 1 and z = x gives a continued fraction for the factorials, (1 âˆ’x)âˆ’1âŠ—= xâŠ—.
(S) A kth-order linear ordinary homogeneous diï¬€erential equation,
bkDkf + bkâˆ’1Dkâˆ’1f + Â· Â· Â· + b0f = 0
can be written b(D)f = 0.
Similarly, a diï¬€erence equation (also called a recurrence
equation) can be written b(E)s = 0. Let `b = bk + bkâˆ’1x1 + Â· Â· Â· + b1xkâˆ’1 + b0xk be the
17

reverse of b = b0 + b1x + b2x2 + Â· Â· Â· + bkxk. Klarner [48] presents this fact: the solution s
to b(E)s = 0 is
s = (`b âˆ—inits)[0..k âˆ’1]
`b
where inits= s0 + s1x + Â· Â· Â· + skâˆ’1xkâˆ’1 = s[0..k âˆ’1] are the initial k elements of s. Also
b(E)s = 0 â‡”b(D)Î›âˆ’1s = 0
For example, E2s + s = 0, s0 = 0, s1 = 1 has solution x/(1 + x2), and Î›âˆ’1s = sin, the
Maclaurin expansion for sin, that is, the solution to D2s+s = 0; s0 = 0, s1 = 1. Another
example is [zn]C âˆ’2u[znâˆ’1]C +[znâˆ’2]C = 0, C0 = 1, C1 = u, where [zn]C is a Chebyshev
polynomial [9] in u. Then, b = 1 âˆ’2uz + z2 = `b , and
C(z, u) = (`b âˆ—(1 + uz))[0..1]
`b
=
1 âˆ’uz
1 âˆ’2uz + z2
By the translation rules of item J, diï¬€erence equations can be written using either E or âˆ†.
The equation b(E)s = 0 transforms into b(1+âˆ†)s = 0, or Ë†b(âˆ†)s = 0, where Ë†b = bâ—¦(1+x).
The converse is b = Ë†b â—¦(x âˆ’1), reï¬‚ecting Ë†b(âˆ†) = Ë†b(E âˆ’1). Clearly, b = b â—¦(1+ x) â—¦(x âˆ’1).
(T) A sequence s is called rational if it is the quotient, s = a/b, of polynomials; it is called
a LODE solution, written LODE(s), if it is a solution of a linear ordinary homogeneous
diï¬€erence equation; and it is called recognizable if it is the behaviour of a ï¬nite automaton.
Then,
rational(s) â‡”LODE(s) â‡”recognizable(s)
Following [19], a ï¬nite automaton can be modelled as a system of linear equations over
sequences, E S = AS; S(0) = v. Here, matrix A records transition labels connecting pairs
of states, and S is a vector of sequences, one for each state, with initial values Si(0) = vi.
The solution is S = (Ax)âˆ—v where
(Ax)âˆ—= I + Ax + A2x2 + A3x3 + Â· Â· Â· =
X
iâ‰¥0
Aixi
is the Kleene star. Notice that (Ax)âˆ—can be viewed as a matrix of sequences or as a
sequence of matrices. Using (Ax)âˆ—= (I âˆ’Ax)âˆ’1, we get (I âˆ’Ax)S = v and Cramerâ€™s rule
applies: Si = det(I âˆ’Ax)[i â†v]
det(I âˆ’Ax)
(column i replaced by v). Thus Si is rational. Justiï¬-
cation of the above equivalences is completed by noting that a LODE can be transformed
into a system of linear equations. We remark that a quotient of polynomials, a/b, can also
be written as the solution to a system of linear equations [72].
(U) Let b = det(xI âˆ’A) = b0 +b1x1 +Â· Â· Â·+bnâˆ’1xnâˆ’1+xn be the characteristic polynomial
of matrix A. The Cayley-Hamilton theorem [19, 49] can be stated as b(A) = 0, or as
18

b(E)(Ax)âˆ—= 0. This will hold if, taking the sequence of matrix powers, (Ax)âˆ—, now as a
matrix of sequences, we have b(E)((Ax)âˆ—)ij = 0, which in turn holds if ((Ax)âˆ—)ij = a
`b
. We
have `b = xnb(1/x) = det(I âˆ’xA). So we are done if we come up with an a such that
((Ax)âˆ—)ij =
a
det(I âˆ’Ax)
Let M = Ax, and J = Mâˆ—j = j + MJ, where j is the vector with 1 at position j and zero
elsewhere. Then, (I âˆ’M)J = j and Cramerâ€™s rule delivers the a we are looking for:
(Mâˆ—)ij = Ji = det(I âˆ’M)[i â†j]
det(I âˆ’M)
(V) Consider the matrix exponential, exp â—¦Ax = Î›âˆ’1(Ax)âˆ—as a matrix of sequences. We
know that (Ax)âˆ—solves E S = AS, S(0) = I, and (Ax)âˆ—[0..k âˆ’1] = [I, A, A2, . . . , Akâˆ’1].
Cayley-Hamilton says that b(E)(Ax)âˆ—= 0, where b is the characteristic polynomial of A,
of degree k, say. By uniqueness of solution, we have [54, 56, 53] Ï† = (Ax)âˆ—if b(E)Ï† = 0
and Ï†[0..k âˆ’1] = [I, A, A2, . . . , Akâˆ’1]. Let S be a vector of sequences such that b(E)Si = 0
and Si[0..k âˆ’1] = xi. Set
Ï† = S0I + S1A + Â· Â· Â· Skâˆ’1Akâˆ’1 =
kâˆ’1
X
i=0
SiAi
Clearly Ï†[0..k âˆ’1] = [I, A, A2, . . . , Akâˆ’1], and b(E)Ï† = 0 because
k
X
j=0
bjEjÏ† =
k
X
j=0
bj
 kâˆ’1
X
i=0
EjSiAi
!
=
kâˆ’1
X
i=0
ï£«
ï£­
k
X
j=0
bjEjSi
ï£¶
ï£¸Ai =
kâˆ’1
X
i=0
(b(E)Si)Ai = 0
(W) The elements of the sequence B = x/(exp âˆ’1) = [1, âˆ’1/2, 1/6, 0, âˆ’1/30, 0, 1/42, 0, . . .]
are called Bernoulli numbers. The corresponding recurrence is calculated from B exp =
B + x:
n![xn]B exp = n![xn](B + x);
n
X
k=0
n
k

Bk = Bn + [n = 1]
Bernoulli numbers are used by Graham et al [32] for the most impressive of their deductions
of the polynomial that sums squares â€“ impressive because it deï¬nes the formulas for all
powers at once. Let S(n) be the sequence such that m![xm]S(n) is the sum of the mth
powers of the naturals to n âˆ’1. Then
m![xm]S(n)
=
nâˆ’1
X
k=0
km =
nâˆ’1
X
k=0
m![xm] exp â—¦kx
S(n)
=
nâˆ’1
X
k=0
expk = expn âˆ’1
exp âˆ’1
= exp â—¦nx âˆ’1
exp âˆ’1
= B exp â—¦nx âˆ’1
x
m![xm]S(n)
=
m!
m
X
k=0
Bmâˆ’k
(m âˆ’k)!
nk+1
(k + 1)! =
m
X
k=0
m
k

Bmâˆ’k
nk+1
k + 1
19

Now replace n by u and x by z to get the expression advertised in the introduction:
S = exp â—¦uz âˆ’1
exp â—¦z âˆ’1
Then m![zm]S is a polynomial of degree m + 1 in u, and (m![zm]S)(n) = m![xm]S(n).
(X) Observe that B1 = âˆ’1/2 is non-zero whilst all the other odd-degree coeï¬ƒcients of B
appear to be zero. Perhaps if we make B1 zero then we will have a sequence which can be
proved to be even (i.e. with zeros at odd positions). Adding 1
2x to B cancels B1:
C = B + x
2 =
x
exp âˆ’1 + x
2 = x
2
exp +1
exp âˆ’1
Recall coth = exp + exp â—¦âˆ’x
exp âˆ’exp â—¦âˆ’x, so C = x
2(coth â—¦x
2 ), from which eveness, C â—¦âˆ’x = C, can
be deduced. Thus,
C = B + x
2 =
X
k
B2k
(2k)!x2k
Now C â—¦2x = x coth, and, using x cot = ix(coth â—¦ix) and tan = cot âˆ’2 cot â—¦2x, we get
x cot = C â—¦2ix
=
X
k
(âˆ’1)k22k B2k
(2k)!x2k
(9)
tan
=
X
k
(âˆ’1)kâˆ’14k(4k âˆ’1) B2k
(2k)!x2kâˆ’1
(10)
With a bit of analysis (reals, cot(x) an analytic function with period Ï€, and uniqueness
of series expansion), one can deduce another series for x cot, due to Euler. The omitted
analysis [50, 1] is hidden in the ï¬rst equals sign:
x cot(x) = 1 âˆ’2
âˆ
X
n=1
x2
n2Ï€2 âˆ’x2 = 1 âˆ’2
âˆ
X
n=1
x2
n2Ï€2
 x2
n2Ï€2
âˆ—
= 1 âˆ’2
âˆ
X
k=1
x2k
Ï€2k
âˆ
X
n=1
1
n2k
Equating coeï¬ƒcients with those in the expansion (9) yields, for k > 0,
[x2k]x cot = âˆ’2
Ï€2k
âˆ
X
n=1
1
n2k = (âˆ’1)k22k B2k
(2k)!
Therefore, the values of Riemannâ€™s Î¶(s) =
X
nâ‰¥1
1
ns at even positive integers is given by
Î¶(2k) = (âˆ’1)kâˆ’122kâˆ’1 B2k
(2k)!Ï€2k
20

(Y) The Formal Taylor Theorem may be expressed:
f â—¦(u + z) = f â—¦u + ((D f) â—¦u)z + (D2f) â—¦u
2!
z2 + (D3f) â—¦u
3!
z3 + Â· Â· Â·
Write f â—¦(u + z) = g0 + g1z + g2z2 + g3z3 + Â· Â· Â·. Let z = 0, then g0 = f(u). Diï¬€erentiate
with respect to z: Dz(f â—¦(u + z)) = g1 + 2g2z + 3g3z2 + Â· Â· Â·. Note that Dz(f â—¦(u + z)) =
f1 + 2f2(u + z) + 3f3(u + z)2 + Â· Â· Â· = (D f) â—¦(u + z). Let z = 0, then g1 = (D f) â—¦u.
Diï¬€erentiate again: D2(f â—¦(u+z)) = 2g2 +3!g3z +Â· Â· Â·. Let z = 0, then g2 = ((D2f)â—¦u)/2.
And so on.
The Maclaurin expansion is the special case with u = 0, and the Taylor
expansion of xn â—¦(u + z) is an instance of the binomial theorem. Lipson [55] uses the
theorem in the application of Newtonâ€™s iterative root-ï¬nding algorithm to polynomial
equations over sequences (see also [70]).
(Z) The following manipulations, originating with Lagrange, have a captivating charm
(even if they lack rigour). We adapt them from [32] to show that elementary sequence
algebra plays a role through to the ï¬nal chapter of that book (where, however, things
become more demanding). In the Formal Taylor theorem, let f be a polynomial, z = 1,
u = x, and employ an operator style:
Ef
=
f â—¦(x + 1) = f + (D f) + D2f
2!
+ D3f
3!
+ Â· Â· Â· = [1 + D + D2
2! + D3
3! + Â· Â· Â·]f
=
exp(D)f
Putting âˆ†= E âˆ’1 = exp(D) âˆ’1 together with âˆ†Î£f = f, suggests Î£ = (exp(D) âˆ’1)âˆ’1.
Then B = x/(exp âˆ’1) applied to D is DÎ£, so Î£ = Dâˆ’1B(D). Expanding this, and writing
R
for the ï¬rst term Dâˆ’1 (since B0 = 1), gives a â€œtemplateâ€ version of the Euler-Maclaurin
summation formula [32, 9].
X
=
Z
+
X
kâ‰¥1
Bk
k! Dkâˆ’1
Now introduce limits:
Pb
a f =
Z b
a
f +
X
kâ‰¥1
Bk
k! Dkâˆ’1f

b
a
(11)
An application to x2 gives yet another derivation [32] of the sum-of-squares formula:
Pn
0 x2
=
1
3n3 +

âˆ’1
2x2 + 1
122x

n
0
=
1
3n3 âˆ’1
2n2 + 1
6n
The way the limits appear on the summation sign has signiï¬cance: Pn
0 x2 =
nâˆ’1
X
x=0
x2. The
21

deï¬nite summation symbol follows the pattern of deï¬nite integration:
g = Df
â‡’
Z b
a
g
=
f

b
a
=
f(b) âˆ’f(a)
g = âˆ†f
â‡’
Pb
a g
=
f

b
a
=
f(b) âˆ’f(a)
=
bâˆ’1
X
x=a
f(x + 1) âˆ’f(x)
=
bâˆ’1
X
x=a
âˆ†f(x)
=
bâˆ’1
X
x=a
g(x)
Letâ€™s add f(b) to both sides of (11) and separate out B1 = âˆ’1/2:
b
X
x=a
f =
Z b
a
f + 1
2(f(b) + f(a)) +
X
kâ‰¥2
Bk
k! Dkâˆ’1f

b
a
(12)
The Euler-Maclaurin formula can also be applied to non-polynomial functions. Let us
illustrate this, without justiï¬cation. To compute
âˆ
X
x=1
1/x2, set S9 =
9
X
x=1
1
x2 = 1.5397677310
and then apply (12) to g = 1/(x + 10)2:
âˆ
X
x=1
1
x2 = S9 +
âˆ
X
x=0
g(x) = S9 +
Z âˆ
0
g + 1
2g(0) +
X
kâ‰¥2
Bk
k! Dkâˆ’1g

âˆ
0
Applying the formula up to B4 gives Î¶(2) = Ï€2/6 â‰ˆ1.64493407.
4
A programming delight
McIlroy [57, 58], inï¬‚uenced by [45] and others, has gifted us some â€œtiny gemsâ€ of pro-
gram deï¬nitions for implementing sequence manipulations. The deï¬nitions are written in
Haskell, and are eï¬€ortlessly derived by mathematical reasoning. A textbook introduction
appears in [18]. We want to entice the reader to type up and experiment with the Haskell
code (but a down-loadable ï¬le is available). The code has been tested in the Haskell GHCi
system, and also in the Hugs98 system (an older system, but well-suited to beginners).
Both GHCi and Hugs98 are freely available on the web at www.Haskell.org.
In this section we present all of the deï¬nitions, thus duplicating some of the contents
of [57, 58]; however, there are modiï¬cations and additions. The fact that the deï¬nitions
have no pre-requisites, other than the standard Haskell Prelude, means that one can
take â€œdeepâ€ ownership, building things from the ground up. This contrasts to using a
sophisticated computer algebra system â€“ something perhaps for the newcomer to move on
to with greater appreciation.
Haskell [68] has evolved to be a fairly large and sophisticated language, but we shall
stick to a modest subset. It is expected that the reader can comprehend Haskell from
examples.
The language gives types to objects and variables, and within context, the
most general type is used. Haskellâ€™s lists are used to represent sequences (some may prefer
22

to introduce a new type for sequences, but that introduces an overhead which we want
to avoid). In Haskell, head-tail decomposition s0 + xsâ€² becomes s0:sâ€™. Here are some
list-processing functions:
take n _ | n<=0
= []
take _ []
= []
take n (s0:sâ€™)
= s0: take (n-1) sâ€™
map f []
= []
map f (s0:sâ€™)
= f s0 : map f sâ€™
iterate f z
= z: iterate f (f z)
foldr f z []
= z
foldr f z (s0:sâ€™)
= f s0 (foldr f z sâ€™)
scanl op q s
= q: (case s of
[]
-> []
s0:sâ€™ -> scanl op (op q s0) sâ€™)
zip (s0:sâ€™) (t0:tâ€™) = (s0, t0): zip sâ€™ tâ€™
zip _ _
= []
zipWith op s t
= [op sn tn | (sn,tn) <- zip s t]
These deï¬nitions implement the following functions which feature in the algebra of pro-
gram calculation [7, 6]:
take n s
=
[s0, s1, . . . , snâˆ’1]
map f s
=
[fs0, fs1, fs2, . . .]
iterate f z
=
[z, f z, f(fz), f 3z, . . .]
foldr f z s
=
f s0 (f s1 (f s2 (. . . z) . . .)
scanl âŠ•q s
=
[q, q âŠ•s0, (q âŠ•s0) âŠ•s1, ((q âŠ•s0) âŠ•s1) âŠ•s2, . . .]
zip s t
=
[(s0, t0), (s1, t1), (s2, t2), . . .]
zipWith âŠ™s t
=
[s0 âŠ™t0, s1 âŠ™t1, s2 âŠ™t2, . . .]
The types deduced are
take
:: Int -> [a] -> [a]
map
:: (a -> b) -> [a] -> [b]
iterate :: (a -> a) -> a -> [a]
foldr
:: (a -> b -> b) -> b -> [a] -> b
scanl
:: (a -> b -> a) -> a -> [b] -> [a]
zip
:: [a] -> [b] -> [(a,b)]
zipWith :: (a -> b -> c) -> [a] -> [b] -> [c]
Type expressions are built from type names such as Int, type variables such as a, and
type constructors such as -> (which associates to the right). Two more examples of type
constructors are: [a] is the type for lists of objects of type a, and [(a,b)] is the type
for lists of pairs of objects. Clearly, functions can take functions as arguments, in which
case they are called higher-order. Lazy evaluation is used, so that for example, scanl will
produce the ï¬rst element, q, of the result without needing to know anything about its list
argument, s. The deï¬nition of zipWith illustrates the so-called list comprehension. An
alternative deï¬nition uses map and uncurry:
23

uncurry
:: (a -> b -> c) -> (a,b) -> c
uncurry op p
= op (fst p) (snd p)
zipWith op s t = map (uncurry op) (zip s t)
The partner to uncurry is curry f x y = f (x,y). An alternative deï¬nition illustrates
use of a lambda expression: curry f = \x y -> f (x,y). The reader may like to supply
the type. These two functions are so-named because Haskell Curry was an early advocate
of the associated equivalence [40]. The Haskell Standard Prelude deï¬nes zipWith without
using zip, and then deï¬nes zip = zipWith (,).
The following speciï¬es that a type a is classiï¬ed as Num if it has the operations (or
methods) listed here:
class Num a where
(+), (-), (*)
:: a -> a -> a
negate, abs, signum
:: a -> a
fromInteger
:: Integer -> a
x - y
= x + negate y
negate x
= 0 - x
All of the foregoing deï¬nitions are in the Standard Prelude. From here on, the code needs
to be supplied. The program ï¬le starts with a few speciï¬ed lines: the ï¬rst line hides
the Prelude deï¬nition of cycle because we are going to re-deï¬ne it for other purposes;
the second line says we need rational numbers; the third line gives the order in which to
resolve ambiguity in numerical data.
import Prelude hiding (cycle)
import Data.Ratio
default (Integer, Rational, Double)
We start by declaring how sequences, [a], become an instance of Num. A prerequisite is
that a is an instance of Eq and Num, indicated by (Eq a, Num a) =>. The deï¬nition of
(-) is derived from negate.
instance (Eq a, Num a) => Num [a] where
negate
= map negate
f+[]
= f
[]+g
= g
(f0:fâ€™)+(g0:gâ€™)
= f0+g0 : fâ€™ + gâ€™
[]*_
= []
(0:fâ€™)*g
= 0 : fâ€™*g
_*[]
= []
(f0:fâ€™)*g@(g0:gâ€™) = f0*g0 : (f0*|gâ€™ + fâ€™*g)
fromInteger c
= [fromInteger c]
abs _
= error "abs not defined on sequences"
signum _
= error "signum not defined on sequences"
Observe that addition is not deï¬ned by f+g = zipWith (+) f g (why?). Convolution
product is derived from (1), but there are some things to note. Firstly, if f0 = 0 then
24

the zeroth term of the result is 0 and is delivered immediately. This may be regarded
as a controversial quirk, but it enables certain equations to be used directly, as in the
following (Catalan) example â€“ in examples, deï¬nitions (which are placed in a program
ï¬le) are interspersed with interactive requests for expression evaluation, indicated by the
prompt â€œ> â€.
x :: Num a => [a]
x = [0,1]
b = 1 + x*b^2
> take 8 b
[1,1,2,5,14,42,132,429]
Secondly, the notation g@, is read as â€œg asâ€. Thirdly, there are clauses for ï¬nite sequences
â€“ the empty list behaves here like zero (but note that 0 is embedded as [0]). Fourthly,
the term f0gâ€² = [f0]gâ€² becomes an explicit scalar product using *|, which is deï¬ned as
an inï¬x operator with precedence 7 (the same as *, and higher than +). The deï¬nition
contains (a*), illustrating the creation of a function by partial application of an operator
(called sectioning).
infix 7 *|
(*|) :: Num a => a -> [a] -> [a]
a *| f = map (a*) f
A function like map is said to be polymorphic because any type can be assigned to its
type variables (subject to consistency).
By contrast, scalar multiplication (*|) has a
qualiï¬ed (constrained or parametric) polymorphic type: its type variable can range over
only instances of class Num. The type stated could be omitted because it can be inferred
due to the presence of *. On the other hand, if the explicit type given to x above was
omitted, then Haskell would infer x::[Integer] and this is a monomorphic type which
would restrict the use of x. With the qualiï¬ed polymorphic type, x can appear in an
expression where a sequence of elements of type N is expected, as long as N is an instance
of Num. Any instance N of Num must provide a fromInteger method that shows how to
embed integers into N, so x would be interpreted as [N.fromInteger 0, N.fromInteger
1].
The Num class invites comparison with the speciï¬cation of the signature of a ring.
Likewise, Haskellâ€™s Fractional class may be compared to a ring-with-division, because a
(partial) division operator (/), or a multiplicative inverse (recip), is required. Rational
numbers form the archetypal instance of Fractional, and any instance F must show how
to embed the rationals in F by deï¬ning fromRational ::
Rational -> F. Division on
sequences, f/g, requires calculating the quotient q satisfying f = qg:
f0 + xf â€²
=
(q0 + xqâ€²)g = q0g + xqâ€²g = q0g0 + x(q0gâ€² + qâ€²g)
f0 = q0g0
;
f â€² = q0gâ€² + qâ€²g
q0 = f0/g0
;
qâ€² = (f â€² âˆ’q0gâ€²)/g
q
=
f0/g0 + x(f â€² âˆ’q0gâ€²)/g
Now we can say, at least approximately, how sequences become a ring-with-division:
25

instance (Eq a, Fractional a) => Fractional [a] where
recip f
= 1/f
_/[]
= error "divide by zero."
[]/_
= []
(0:fâ€™)/(0:gâ€™)
= fâ€™/gâ€™
(_:fâ€™)/(0:gâ€™)
= error "divide by zero"
(f0:fâ€™)/g@(g0:gâ€™) = let q0=f0/g0 in q0:((fâ€™ - q0*|gâ€™)/g)
fromRational c
= [fromRational c]
These simple deï¬nitions confront us with some of the diï¬ƒculties in coding a satisfactory
division operation that works for both ï¬nite and inï¬nite sequences. One should investigate
questions like: are f/g = f âˆ—(1/g) and f/f = 1 faithfully implemented? To keep things
simple, compromises have to be made.
Arithmetic and the convolution product rule are used to calculate a head-tail deï¬-
nition for square root, âˆšf. The starting point is f = âˆšfâˆšf, and we calculate (âˆšf)0 and
âˆšfâ€²:
f0 = (
p
f
p
f)0
=
(
p
f)0(
p
f)0
(
p
f)0
=
p
f0
f â€² = (
p
f
p
f)â€²
=
p
f0
p
f
â€² +
p
f
â€²p
f
p
f
â€²
=
f â€²/(
p
f0 +
p
f) (f0 Ì¸= 0)
We shall trivialise âˆšf0 and restrict square root to fractional sequences with constant term
1. In the following code, the ï¬rst clause is suggested by the identity
p
x2f = xâˆšf, and
the more general
p
x2nf = xnâˆšf is handled by recursion. An alternative deï¬nition of
square root is derived in [58] by diï¬€erentiating r2 = f, rearranging and then integrating
(which the reader may like to try).
sqroot (0:0:fâ€™â€™) = 0:sqroot fâ€™â€™
sqroot f@(1:fâ€™)
= 1:(fâ€™/(1+sqroot f))
Sequence composition, f â—¦g =
X
n
fngn, is expanded thus:
f â—¦g
=
f0 + f1g1 + f2g2 + f3g3 + Â· Â· Â·
=
f0 + g(f â€² â—¦g) = f0 + (g0 + xgâ€²)(f â€² â—¦g)
=
f0 + g0(f â€² â—¦g) + xgâ€²(f â€² â—¦g)
When f is inï¬nite, g0(f â€² â—¦g) is not computable unless g0 = 0. When g0 = 0 we get
(f â—¦g)0 = f0;
(f â—¦g)â€² = gâ€²(f â€² â—¦g)
(13)
However, f â—¦g is computable for g0 Ì¸= 0 when f is ï¬nite (p â—¦[a] = [p(a)], p a polynomial).
So we admit a potentially non-terminating clause and it is up to us to use it with care:
[] â€˜oâ€˜ _
= []
26

(f0:fâ€™) â€˜oâ€˜ g@(0:gâ€™)
= f0: gâ€™*(fâ€™ â€˜oâ€˜ g)
(f0:fâ€™) â€˜oâ€˜ g@(g0:gâ€™) = [f0] + (g0*|(fâ€™ â€˜oâ€˜ g))+
(0:gâ€™*(fâ€™ â€˜oâ€˜ g))
The deï¬nition f â—¦g = P
n fngn reveals x to be a left and right identity of composition.
Composition distributes leftwards through sum, product, and quotient.
To calculate the converse, g = f â—¦, expand the composition f â—¦g = x:
f0 + xgâ€²(f â€² â—¦g) = x = 0 + x1
Hence, gâ€²(f â€² â—¦g) = 1, and
g0 = 0;
gâ€² = 1/(f â€² â—¦g)
The program code is:
converse(0:fâ€™) = g where g = 0: 1/(fâ€™ â€˜oâ€˜ g)
For the reciprocal of f â€² â—¦g to be deï¬ned, it is necessary that f â€²
0 is invertible, which entails
that f â€² is invertible. The set of such â€œconversibleâ€ sequences forms a group, (SC, â—¦, ()â—¦, x).
The transforms Î› and Î›âˆ’1 are given names e2o and o2e, respectively [18]. Here they are,
together with some other useful sequences:
e2o f
= zipWith (*) f facs
o2e f
= zipWith (/) f facs
from
:: Num a=>a->[a]
from
= iterate (+1)
nats, pos, zeros, facs :: Num a=>[a]
nats
= from 0
pos
= from 1
zeros
= 0:zeros
facs
= scanl (*) 1 pos
Diï¬€erentiation and integration enjoy the appropriately succinct deï¬nitions,
deriv f = zipWith (*) pos (tail f)
integ f = 0:zipWith (/) f pos
The deï¬nitions D exp = exp; exp0 = 1 and xâˆ—= 1 + xxâˆ—have the following solutions in
Haskell. An x is aï¬ƒxed to prevent name clashes with existing names (for example exp in
Haskell implements the function ex). One can test xâˆ—= Î› exp by checking the ï¬rst few
terms of their diï¬€erence.
expx
:: (Eq a,Fractional a) => [a]
expx
= 1 + integ expx
starx :: (Eq a, Num a) => [a]
starx = 1 : starx
> takeW 6 (starx - e2o expx)
[0,0,0,0,0,0]
27

A rational a/b is presented in Haskell as a%b. The elements of expx are rationals, and
e2o removes the factorial divisors, yielding [1%1,1%1, ...]. The following deï¬nes takeW
n which is take n preceded by the conversion of whole rationals into integers (the (.) is
function composition, and properFraction is in the Prelude).
makeWhole r
= case properFraction r of
(n,0)
-> n
otherwise
-> error "not whole"
makeAllWhole = map makeWhole
takeW n
= take n . makeAllWhole
Table 7 contains further core sequences deï¬ned by diï¬€erential equations. All should be
given the type (Eq a, Fractional a) => [a], like expx above. The core sequence xâˆ—,
which we deï¬ned earlier, could be deï¬ned by starx = 1+integ (starx^2), since D xâˆ—=
(xâˆ—)2; xâˆ—
0 = 1 (but its elements would then be fractional). For two more examples, let us
calculate deï¬nitions for arctan and arcsin.
D tanâ—¦= ((1 + tan2) â—¦tanâ—¦)âˆ’1 = 1/(1 + x2)
D sinâ—¦= (cos â—¦sinâ—¦)âˆ’1 =
p
1 âˆ’sin2
â—¦sinâ—¦âˆ’1
= 1/
p
1 âˆ’x2
The latter uses the Pythagorean identity, sin2 + cos2 = 1, which follows from the deï¬ning
diï¬€erential equations. Taking initial values into consideration, the solutions rendered in
Haskell are immediate:
atanx
= integ (1/(1+x^2))
asinx
= integ (1/(sqroot (1-x^2)))
Here are checks of exp = (sec + tan)â—¦gd (gd is the Guddermanian function) and D sinâ—¦=
1/
âˆš
1 âˆ’x2.
> takeW 6 (expx - ((secx + tanx) â€˜oâ€˜ gdx))
[0,0,0,0,0,0]
> takeW 6 (deriv (converse sinx) - (1/(sqroot (1-x^2))))
[0,0,0,0,0,0]
A bivariate sequence, b(z, u), may be regarded as a (potentially doubly-inï¬nite) matrix,
t = (bi,j), of coeï¬ƒcients of ziuj. It is implemented as a univariate sequence, s, of (ho-
mogeneous) polynomials such that sn is the diagonal, [b0,n, b1,nâˆ’1, . . . bn,0] of t. Thus,
b0,nz0un+b1,nâˆ’1z1unâˆ’1+Â· Â· Â·+bn,0znu0 is represented by sn = b0,nx0+b1,nâˆ’1x1+Â· Â· Â·+bn,0xn
(z becomes x, and u is redundant). So, [unâˆ’kzk]b = [xk][xn]s; equivalently [ukzn]b =
[xn][xn+k]s. The following depicts the t to s map on a portion of t:
ï£®
ï£¯ï£¯ï£°
b0,0
b0,1
b0,2
b0,3
. . .
b1,0
b1,1
b1,2
b1,3
. . .
b2,0
b2,1
b2,2
b2,3
. . .
b3,0
b3,1
b3,2
b3,3
. . .
ï£¹
ï£ºï£ºï£»7â†’
ï£®
ï£¯ï£¯ï£°
b0,0
b0,1
b1,0
b0,2
b1,1
b2,0
b0,3
b1,2
b2,1
b3,0
ï£¹
ï£ºï£ºï£»
28

lgnx
= integ (1/(1+x))
D lgn = 1/(1 + x); lgn0 = 0
sinx
= integ cosx
D sin = cos; sin0 = 0
cosx
= 1-integ sinx
D cos = âˆ’sin; cos0 = 1
tanx
= integ (1+tanx^2)
D tan = 1 + tan2; tan0 = 0
secx
= 1+integ (secx * tanx)
D sec = sec tan; sec0 = 1
coshx
= 1+integ sinhx
D cosh = sinh; cosh0 = 1
sinhx
= integ coshx
D sinh = cosh; sinh0 = 0
tanhx
= integ (1-tanhx^2)
D tanh = 1 âˆ’tanh2; tanh0 = 0
gdx
= integ (1/coshx)
D gd = 1/ cosh; gd0 = 0
Table 7: Some core Sequences
The ring-with-division and square root operations pertaining to b(z, u) are isomorphically
transferred to the diagonal representation s. The representations for u and z are, u =
0u0z0+(1u1z0+0u0z1) âˆ¼= [0x0, 1x0 +0x1] = [[0], [1, 0]] and z = 0+(0u+1z) âˆ¼= [0, 0+1x] =
[[0], [0, 1]]. Here is (u + z)âˆ—represented by s=pascal in Haskell,
u,z, pascal :: (Eq a, Num a) => [[a]]
u
= [[0],[1,0]]
z
= [[0],[0,1]]
pascal = starx â€˜oâ€˜ (u+z)
> take 6 pascal
[[1],[1,1],[1,2,1],[1,3,3,1],[1,4,6,4,1],[1,5,10,10,5,1]]
This displays [unâˆ’kzk](u + z)âˆ—= [xk][xn]s =
n
k

. Commonly, we want to show a portion
of t, where [xk][xn]t = bn,k = [xn][xn+k]s, or perhaps more commonly, such that [xk][xn]t =
n![ukzn]b = n![xn][xn+k]s = [xn]Î›[xn+k]s. For example, let
b = exp â—¦(z + uz) = (exp â—¦z)(exp â—¦uz) =
X
n
n
X
k=0
znâˆ’k
(n âˆ’k)!
ukzk
k!
=
X
n
n
X
k=0
n
k
 ukzn
n!
Then n![ukzn]b =
n
k

. The functions unDiag and unDiage2o transpose the s-representation
into the desired t-representation (the reverse of the t 7â†’s map depicted above). The func-
tion unDiage2o ï¬rst removes factorial divisors associated with z. The functions select
and selectW take an argument [n0, n1, . . . , nm] saying what length to take from rows 0
to m of t. The version selectW converts whole rationals to integers. Bivariate counting
sequences, bn,k, are typically zero for k > n, and from these we select a lower triangular
section. The schroeder sequence from section 3, item C, is an example which is ordinary
in u and z, whilst ebinom below is exponential in z. The sequence powerSums of poly-
nomials for summing powers has a polynomial of order n + 1 at position n, so a lower
trapezium section [2..4] is selected.
list, pluralList :: (Eq a, Num a) => [a]
list
= starx
pluralList = list - x - 1
29

schroeder
=
z + u*(pluralList â€˜oâ€˜ schroeder)
> select [1..6] (unDiag schroeder)
[[0],[1,0],[0,1,0],[0,1,2,0],[0,1,5,5,0],[0,1,9,21,14,0]]
ebinom =
expx â€˜oâ€˜ (z+u*z)
> selectW [1..6] (unDiage2o ebinom)
[[1],[1,1],[1,2,1],[1,3,3,1],[1,4,6,4,1],[1,5,10,10,5,1]]
powerSums
= ((expx â€˜oâ€˜ (u*z))-1)/((expx â€˜oâ€˜ z)-1)
> select [2..4] (unDiage2o powerSums)
[[0 % 1,1 % 1],[0 % 1,(-1) % 2,1 % 2],[0 % 1,1 % 6,(-1) % 2,1 % 3]]
A number of supporting functions are needed. The unDiag function expects its argument
to be perfectly triangular, so padTri is ï¬rst applied to ï¬ll out the triangle with zeros if
necessary. Then transpose m detaches the heads of the rows of m, which make up the ï¬rst
column, c, and this becomes the ï¬rst row of the result. A recursive invocation transposes
the remaining sub-matrix, mâ€™.
select s t
= zipWith take s t
selectW s t
= zipWith takeW s t
unDiag
:: Num a=> [[a]]->[[a]]
unDiag
= transpose . padTri
unDiage2o
:: Fractional a=> [[a]]->[[a]]
unDiage2o
= unDiag . (map e2o)
padTri t
= zipWith padRight t [1..]
padRight r k = r++(take (k-(length r)) zeros)
transpose []
= []
transpose m
= c:transpose mâ€™
where (c,mâ€™) = foldr detachHead ([],[]) m
detachHead
([r0]) b
= (r0:fst b,snd b)
detachHead
(r0:râ€™) b = (r0:fst b,râ€™:snd b)
There is plenty of room for adding functions to taste.
Perhaps the main diï¬ƒculty is
deciding on an eï¬€ective naming convention. Here are some examples.
takeEBivW r = (selectW r) . unDiage2o
takeEBiv
r = (select r)
. unDiage2o
takeBivW
r = (selectW r) . unDiag
takeBiv
r = (select r)
. unDiag
Diï¬€erentiation with respect to z (or u) is performed by dz (or du). Below are the deï¬ni-
tions, plus a test based on the set partitions recurrence from section 3, item L. Instead of
using allZeros [1..6], the reader may wish to simply use selectW [1..6] and view the
zeros (incidentally, one would then observe that the diagonal representation is not always
a perfect triangle).
dz s = map deriv (tail s)
du s = map (reverse . deriv . reverse) (tail (padTri s))
set, nonEmptySet, emptySet :: (Eq a, Fractional a) => [a]
30

set
= expx
emptySet
= 1
nonEmptySet
= set - emptySet
parts
= set â€˜oâ€˜ (u*(nonEmptySet â€˜oâ€˜ z))
allEq c r
= foldr (\a b-> (a==c) && b) True r
allZeros s t = allEq True (map (allEq 0) (select s t))
> allZeros [1..6] ((dz parts) - (u*parts +u*du parts))
True
5
Exercising the implementation
As it stands, the implementation facilitates a great range of experimentation. We will
demonstrate a few concrete examples. It will be seen that the implementation is a valuable
assistant in the study of otherwise theoretical material.
The deï¬nitions in tables 4 and 5 transliterate into Haskell. Here are some examples
(see items B and D). One has to be vigilant about when to use e2o, take, takeW,
select, selectW, unDiag, unDiage2o, etc. It is not necessary to give type declarations
to all deï¬nitions, but it will be necessary for some. We leave that as a trial-and-error
exercise.
cycle, perm :: (Eq a, Fractional a) => [a]
perm
= starx
lg g
= lgnx â€˜oâ€˜ (g-1)
cycle = lg starx
> takeW 6 (perm - (set â€˜oâ€˜ cycle))
[0,0,0,0,0,0]
cayleyTree
= x*(set â€˜oâ€˜ cayleyTree)
connectedAcyclicGraph = cayleyTree - cayleyTree^2 / 2
> takeW 8 (e2o connectedAcyclicGraph)
[0,1,1,3,16,125,1296,16807]
infix 7 |^
(|^)
:: (Eq a, Fractional a) => [a] -> a -> [a]
f |^ r
= expx â€˜oâ€˜ (r *| lg f)
legendre
= (1-2*u*z+z^2) |^ [-1%2]
> select [1..4] (unDiag legendre)
[[1 % 1],[0 % 1,1 % 1],[(-1) % 2,0 % 1,3 % 2],
[0 % 1,(-3) % 2,0 % 1,5 % 2]]
hermite
= expx â€˜oâ€˜ (2*u*z-z^2)
> select [1..4] (unDiage2o hermite)
[[1 % 1],[0 % 1,2 % 1],[(-2) % 1,0 % 1,4 % 1],
[0 % 1,(-12) % 1,0 % 1,8 % 1]]
No attention has been paid to eï¬ƒciency or prettiness of results â€“ all the computations are
expected to work on small examples, resulting in small results. Endless examples could
be given related to items A-Z. We must choose only a few, and we aim for variety.
31

The factorials are deï¬ned in the previous section using scanl; below they are
generated directly from their diï¬€erential equation, by a continued fraction recurrence,
and by shuï¬„e inverse (see items E and P). Shuï¬„e product can also be deï¬ned by
f âŠ—g = Î›(Î›âˆ’1f âˆ—Î›âˆ’1g)) (but that involves rationals, even when f and g are integer
sequences).
fac = 1+x*fac + x^2*(deriv fac)
> take 6 fac
[1,1,2,6,24,120]
cf_fac
= 1/(cfdenom 1)
where cfdenom n = 1 - x*(2*n-1)-x^2*n^2*(1/(cfdenom (n+1)))
> takeW 6 cf_fac
[1,1,2,6,24,120]
infix 7 |><|
-- shuffle product
f@(f0:fâ€™) |><| g@(g0:gâ€™) = (f0 * g0): ((fâ€™ |><| g)+(f |><| gâ€™))
_ |><| []
= []
[] |><| _
= []
shInv f@(f0:fâ€™) = (1/f0): (-fâ€™ |><| ((shInv f) |><| (shInv f)))
> takeW 6 (shInv (1-x))
[1,1,2,6,24,120]
The Newton transform, (N, N âˆ’1), is an isomorphism between the Hadamard and inï¬ltra-
tion rings (see item K). It is implemented here by (h2i, i2h), with a recursive variant,
rh2i. We also translate the deï¬nitions of âˆ†and Î£ directly to delta and sigma. Later,
we shall deï¬ne another version of Î£, named prefixSums, which produces a ï¬nite result
on a ï¬nite sequence. Letâ€™s throw into our test the ubiquitous ï¬bonacci sequence, deï¬ned
by fn+2 âˆ’fn+1 âˆ’fn = 0; f0 = f1 = 1. The ï¬rst part can be re-expressed b(E)f = 0,
where b = x2 âˆ’x âˆ’1, with solution f = (`b âˆ—[1, 1])[0..1]
`b
= 1/(1 âˆ’x âˆ’x2) (see item S).
For illustration, we let Haskell take the last step.
h2i s
= (1/(1+x)) |><| s
i2h s
= (1/(1-x)) |><| s
rh2i s@(s0:sâ€™) = s0: rh2i (delta s)
delta s
= (tail s) - s
sigma s
= x*starx*s
recur
:: (Eq a, Num a) => a -> [a]
recur a
= a:recur a
fib
= (take 2 (rb*[1,1]))/rb where rb = reverse (x^2-x-1)
> takeW 10 (recur 1 + sigma (delta fib))
[1,1,2,3,5,8,13,21,34,55]
> takeW 10
(i2h (h2i fib))
[1,1,2,3,5,8,13,21,34,55]
The Hadamard product, |*|, and the inï¬ltration product, |^| (and infProd) are now
introduced, and testing them is left as an exercise.
infix 7 |*|
32

f |*| g = zipWith (*) f g
infix 7 |^|
f@(f0:fâ€™) |^| g@(g0:gâ€™) = (f0*g0): ((fâ€™|^|g)+(f|^|gâ€™))+(fâ€™|^|gâ€™)
_ |^| []
= []
[] |^| _
= []
infProd f g = h2i (i2h f |*| i2h g)
Translation to and from falling factorial polynomials can be exercised as follows. There
are, of course, more eï¬ƒcient ways of generating the data used here (cycles, parts), but
we stick to a simple translation of the mathematical deï¬nitions. In the ï¬rst test, we use
the fact (item N) that [0, 1, 5, 14, 30, 55, . . .] is representable by a polynomial of degree 3.
The second test compares falling factorials and cycle numbers.
cycles
= set â€˜oâ€˜ (u* (cycle â€˜oâ€˜ z))
fall n m
= product [n-i | i<-take m nats]
alt
:: Num a => a->[a]
alt r
= r:alt (-r)
altMat m
= zipWith op (alt 1) m
where op sign r = zipWith (*) (alt sign) r
monom2FacPoly = takeEBiv [1..] parts
facMonom2Poly = altMat (takeEBiv [1..] cycles)
toFacPoly p
= sum (zipWith (*|) p monom2FacPoly)
fromFacPoly p = sum (zipWith (*|) p facMonom2Poly)
squaresFacPoly= o2e (take 4 (h2i [0,1,5,14,30,55]))
> fromFacPoly squaresFacPoly
[0 % 1,1 % 6,1 % 2,1 % 3]
> [fall x i | i<- [0..5]] == take 6 facMonom2Poly
True
The Maclaurin and Taylor expansions (item Y) can be coded and tested:
maclaurin f = o2e (map head (iterate deriv f))
taylor f
= map o2e (zp (map (â€˜oâ€˜ u) (iterate deriv f)))
where zp (g0:gâ€™) = g0 + z* (zp gâ€™)
bsinx, tsinx :: [[Rational]]
bsinx = sinx â€˜oâ€˜ (u+z)
tsinx = taylor sinx
> select [1..8] bsinx == select [1..8] tsinx
True
Let us move beyond the A-Z items and look at some other examples. The Logan polyno-
mials [32, sect. 6.5], have the tangent numbers as constant terms. Here they are, deï¬ned
by a closed expression, (sin â—¦z + u cos â—¦z)/(cos â—¦z âˆ’u sin â—¦z), and by an iteration.
33

logan = (((sinx â€˜oâ€˜ z)+u*(cosx â€˜oâ€˜ z))/
((cosx â€˜oâ€˜ z)-u*(sinx â€˜oâ€˜ z)))
> takeEBivW [2..5] logan
[[0,1],[1,0,1],[0,2,0,2],[2,0,8,0,6]]
loganPolys = iterate (\p -> (1+x^2) * deriv p) x
> take 4 loganPolys
[[0,1],[1,0,1],[0,2,0,2],[2,0,8,0,6]]
The Entringer triangle, E [81, 78], has the tangent numbers on the ï¬rst column (dis-
regarding the ï¬rst element), and the secant numbers on the diagonal.
Below, the tri-
angle is generated ï¬rst by a backwards and forwards (boustrophedonic) computation of
partial sums, then as the diagonal (homogeneous) presentation of coeï¬ƒcients of A =
(sin â—¦u + cos â—¦u)/ cos â—¦(u + z)) (named zigzags in table 5. The bivariate A is exponential
in both u and z, and En,k = (n âˆ’k)!k![unâˆ’kzk]A can be shown [32, ex. 6.75]. Forward
partial sums are preï¬x sums. Earlier, in item I, we met Î£s = xxâˆ—s for computing them,
and we used this above to deï¬ne sigma. But that operator always results in an inï¬nite
sequence, even when applied to a ï¬nite one. So here we use a diï¬€erent deï¬nition.
prefixSums
= scanl (+) 0
suffixSums
= reverse.prefixSums.reverse
alternate f g a = a:alternate g f (f a)
entringer
= alternate suffixSums prefixSums [1]
> take 7 entringer
[[1],[1,0],[0,1,1],[2,2,1,0],[0,2,4,5,5],[16,16,14,10,5,0],
[0,16,32,46,56,61,61]]
zigzags =
(((sinx â€˜oâ€˜ u) + (cosx â€˜oâ€˜ u))/(cosx â€˜oâ€˜ (u+z)))
ue2o
:: Fractional a => [a]->[a]
ue2o
= reverse.e2o.reverse
> selectW [1..7] (map e2o (map ue2o zigzags))
[[1],[1,0],[0,1,1],[2,2,1,0],[0,2,4,5,5],[16,16,14,10,5,0],
[0,16,32,46,56,61,61]]
The Moessner sieve generates the sequence Mr = [1r, 2r, 3r, 4r, . . .], given a positive integer
r. Kozen and Silva [52] cite a variety of proofs including sequence-calculational [38] and
coinduction methods [63]. Let us see how easily we can implement some of the compu-
tations in [52]. There it is shown that the Moessner procedure can be described as the
computation of a succession of bivariate sequences, bn(z, u), usefully represented in diago-
nal (homogeneous) form, sn, and that [u0zr]bn = [xr][xr]sn = nr. The sequence of triangles
begins with Pascalâ€™s triangle, b0 = p = (u + z)âˆ—, represented in diagonal form by s0. Then
the triangle-to-triangle step, sn 7â†’sn+1, is: take the row Ï = [xr]sn = [Ï0, Ï1, . . . , Ïr],
representing hn(z, u) = Ï0urz0 + Ï1urâˆ’1z1 + Â· Â· Â· + Ïru0zr, and compute sn+1 representing
bn+1 = hn(z, 1) âˆ—p = (Ï0z0 + Ï1z1 + Â· Â· Â· + Ïrzr) âˆ—p. Here is the computation of the ï¬rst
three triangles, followed by the selection of nr = [xr][xr]sn =sn!!r!!r from the ï¬rst 5
triangles. The function x2z converts Ï = [xr]sn = Ï0 + Ï1x + Â· Â· Â· + Ïrxr to hn(z, 1) in
bivariate diagonal form (there are many ways of doing this).
x2z rho
= sum (zipWith (\c zn->[[c]]*zn) rho zPowers)
where zPowers = (iterate (*z) 1)
34

moessnerT r = iterate (\s -> (x2z (s!!r))*pascal) pascal
> select [5,5,5] (moessnerT 4)
[[[1],[1,1],[1,2,1],[1,3,3,1],[1,4,6,4,1]],
[[1],[1,5],[1,6,11],[1,7,17,15],[1,8,24,32,16]],
[[1],[1,9],[1,10,33],[1,11,43,65],[1,12,54,108,81]]]
nats2Power r = [sn!!r!!r | sn <- moessnerT r]
> take 5 (nats2Power 4)
[1,16,81,256,625]
The function moessnerT is easily changed to one which produces the n-indexed sequence
[xr]sn representing hn(z, 1), and this makes way for a generalisation. The iteration step
is [xr]sn 7â†’[xr]sn+1, and the iteration starts with [xr]s0 = 1, representing h0(z, 1).
moessnerH r = iterate (\rho -> ((x2z rho)*pascal)!!r) 1
> select [5,5,5,5] (moessnerH 4)
[[1],[1,4,6,4,1],[1,8,24,32,16],[1,12,54,108,81]]
Kozen and Silva generalise Moessnerâ€™s theorem to encompass theorems by Long and
Paasche [52]. In the generalised implementation, there are two new parameters, h0(z, 1)
(regarded as univariate, to be converted to bivariate by x2z), and d, a sequence [d0, d1, . . .]
of non-negative integers.
The iteration step implements the following recurrence, in
which the ï¬nal subscript indicates the selection of the homogeneous polynomial of de-
gree (deg hn) + dn:
hn+1(z, u) = [hn(z, 1) âˆ—p](deg hn)+dn
Thus, rather than a simple iteration, we scan along d, because step n requires dn. Let
cn, n > 0, be the leading coeï¬ƒcient of hn(z, 1) (which we extract using last, since
the highest-order coeï¬ƒcient is at the end). The generalised theorem entails: (a) when
h0(z, 1) = 1 and d = [r, 0, 0, . . .] we get Moessnerâ€™s result, cn = nr; (b) when h0(z, 1) =
b + (a âˆ’b)z and d = [r, 0, 0, . . .], we get Longâ€™s result, cn = (a + (n âˆ’1)b)nr; and (c) when
h0(z, 1) = 1 and d = [d0, d1, . . .], we get Paascheâ€™s result, cn =
nâˆ’1
Y
i=0
(n âˆ’i)di. Here is a
rather succinct implementation.
ksmlp h0 d = map last (scanl step h0 d)
where step hn dn = ((x2z hn)*pascal)!!((length hn - 1)+dn)
moessner r = ksmlp 1 (r:zeros)
long a b r = ksmlp [b,a-b] (r:zeros)
paascheFac = ksmlp 1 [1,1..]
superFac
= ksmlp 1 [1,2..]
The above computations convey some Haskell by example, and demonstrate a wealth of
experimentation assisted by sequence operations. The core set of deï¬nitions are kept to
a minimum, so that they are manageable in one ï¬le, and should not daunt beginners. In
keeping to this principle, we have not implemented an equivalent of formal Laurent series,
35

so we cannot accommodate a sequence for cot (and csc, and so on). However, we can
deï¬ne x cot = (x cos)/ sin. Then, with reference to item X, let c(r) = rx(coth â—¦rx), and
test x coth = c(1/2) â—¦(2x), c(1/2) â—¦âˆ’x = c(1/2) and c(i) = x cot (Gaussian rationals,
introducing i, are in the next section):
xcotx, xcothx :: (Eq a, Fractional a) => [a]
xcotx
= (x*cosx)/sinx
xcothx
= (x*coshx)/sinhx
xcth r
= [r]*(x*(coshx â€˜oâ€˜ ([r]*x)))/(sinhx â€˜oâ€˜ ([r]*x))
> take 10 xcothx == take 10 ((xcth (1%2)) â€˜oâ€˜ (2*x))
True
> take 10 (xcth (1%2)) == take 10 ((xcth (1%2)) â€˜oâ€˜ (-x))
True
> take 10 xcotx == take 10 (xcth i)
True
Sometimes there is simply extra work to be done to convert a sequence expression into
a form acceptable by our deï¬nitions. For example, the following expression for counting
permutations by number of valleys is derived in [22]:
K(z, u) = 1 âˆ’1
u +
âˆšu âˆ’1
u
tan â—¦(z
âˆš
u âˆ’1 + arctan â—¦
1
âˆšu âˆ’1)
This fails to compute in our implementation for three reasons (can you spot them?). But,
by using the double-angle identity for tan, and i tanh = tan â—¦ix, it can be manipulated
into the following form [20], which does compute:
K(z, u) =
âˆš1 âˆ’u
âˆš1 âˆ’u âˆ’tanh â—¦(zâˆš1 âˆ’u)
valleys = r/(r - (tanhx â€˜oâ€˜ (z*r))) where r = sqroot (1-u)
> takeEBivW [1..6] valleys
[[1],[1,0],[2,0,0],[4,2,0,0],[8,16,0,0,0],[16,88,16,0,0,0]]
The question of how to circumscribe a minimal core set of deï¬nitions that perhaps manifest
a timeless quality, is a challenging one. It seems only too easy to keep adding stuï¬€, as the
next section testiï¬es.
6
Building on the implementation
Implementing sequence algebra is an example of mathematics-programming synergy, as
found for example, in [62, 55, 80, 74, 65, 87, 82, 18]. One should note the chronology
of language use: [62] uses Fortran, [55] uses pseudo-Algol, [80] uses Pascal, [74] uses
Standard ML, [87] uses Maple, [82] uses Scheme, and [65, 18] use Haskell. Haskell is one
of the most recent and ambitious in the evolution of programming languages. The story
of its development [40] is an informative account of collaborative design in a scientiï¬c
36

context. It clearly reveals the tensions between the pursuit of elegant tried-and-tested
universal concepts, and pragmatically-motivated more complex and speculative features.
One has to face the fact that Haskell presents the casual newcomer with subtleties,
some of which cause baï¬„ement. This slightly detracts from our goal, but also means that
the implementation of sequence algebra is a ï¬ne benchmark test: Haskell ought to host it
well for relative beginners. There are two prominent sources of subtleties: lazy evaluation
and type classes. The former might be discovered in working with inï¬nite matrices, for
example try rewriting transpose. The latter is likely to cause the most frustration. One
could write an elucidation of potential â€œsurprisesâ€ centred around implementing sequence
algebra. That is beyond our scope, but we draw attention to the fact that some type
declarations can be omitted, and some not. To take just one example, the ï¬nal test of the
previous section, take 10 xcotx == take 10 (xcth i), does not go through if the type
declaration for xcotx is omitted (then the system doesnâ€™t know to translate the rationals
in xcotx to Gaussian rationals for comparison). On the other hand, we may omit an
explicit type for xcot and use the test
makeReal (r:&0)
= r
makeReal _
= error "not real"
makeAllReal g
= map makeReal g
> take 10 xcotx == makeAllReal (take 10 (xcth i))
True
However, if the g is omitted from the deï¬nition of makeAllReal, then makeAllReal is given
a diï¬€erent type and the test fails to type-check. Of course, such things have interesting
explanations, but they are potentially oï¬€-putting for beginners.
These remarks notwithstanding, one cannot resist adding to the implementation in
a myriad of ways. Here are a few next-steps, which the author has already taken, and
which are left as fruitful exercises.
â€¢ Translate [87], and elements of [80], to use Haskell.
â€¢ Introduce Gaussian rationals as an instance of Num and Fractional, and test De
Moivreâ€™s theorem (item A). Here is part of a deï¬nition and a test of Eulerâ€™s identity:
infix
6
:&
data Gaussian a
= a :& a deriving (Eq, Read, Show)
i
:: (Eq a, Num a) => Gaussian a
i
= 0:&1
ix
:: (Eq a, Num a) => [Gaussian a]
ix = [0,i]
instance
(Num a) => Num (Gaussian a)
where
-- define negate, +, abs, signum, fromInteger
(x:&y) * (xâ€™:&yâ€™) =(x*xâ€™-y*yâ€™) :& (x*yâ€™+y*xâ€™)
instance
(Fractional a) => Fractional (Gaussian a)
where
(x:&y) / (xâ€™:&yâ€™) =
(x*xâ€™+y*yâ€™) / d :& (y*xâ€™-x*yâ€™) / d
where d
= xâ€™*xâ€™ + yâ€™*yâ€™
37

fromRational a
= fromRational a :& 0
> take 10 (cosx + [i]*sinx) == take 10 (expx â€˜oâ€˜ ix)
True
â€¢ Introduce an instance, Shuffle a, of class Num, so that one can write s |><| t as
S s * S t, and shuï¬„e power snâŠ—as (S s)^n. Extend the following code, making
Shuffle a an instance of Fractional. The ï¬rst test below illustrates shuï¬„e power.
The second test involves the secant numbers, s = Î› sec. These can be deï¬ned (see
items E and F) by applying Î› to the diï¬€erential equation for sec to give sâ€² = s âŠ—
(Î› tan), and since sec0 = 1 we get the Haskell secNums = 1:secNums |><| tanNums
(where tanNums=e2o tanx). Contrast this to the use of S:
newtype Shuffle a = S [a] deriving (Eq, Read, Show)
unS (S s) = s
instance (Eq a, Num a) => Num (Shuffle a) where
negate (S s)
= S (negate s)
(S s) + (S t) = S (s+t)
(S s) * (S t) = S (s |><| t)
fromInteger n = S [fromInteger n]
abs _
= error "abs undefined on Shuffle"
signum _
= error "signum undefined on Shuffle"
> takeW 6 (unS ((S starx)^2))
[1,2,4,8,16,32]
tanNums = e20 tanx
secNums = 1:unS (S secNums * (S tanNums))
> takeW 10 secNums
[1,0,1,0,5,0,61,0,1385,0]
â€¢ Introduce matrix computations. To keep the deï¬nitions simple, use the type [[a]]
for a matrix, and presume, controversially, that it is used responsibly, in the sense
that a matrix is presented as a list of rows of agreed length. Transpose is already
deï¬ned in our implementation (written to work also for inï¬nite matrices). Opera-
tions to deï¬ne include determinant, characteristic polynomial, adjugate, Gaussian
elimination, and diï¬€erent methods of inversion. Then one can test computations
in proofs of the Cayley-Hamilton theorem, and experiment with bivariate Lagrange
inversion (using 2 Ã— 2 Jacobians).
â€¢ Sequences of sequences can become confused with matrices, so it is instructive to
deï¬ne:
data Matrix a
= M [[a]] | D a deriving (Eq, Read, Show)
instance (Eq a, Num a) => Num (Matrix a) where
negate (M m)
= M (map (map negate) m)
negate (D r)
= D (negate r)
(M a) + (M b)
= M ...
38

... clauses for + and *
fromInteger n = D (fromInteger n)
The idea is that if s is a square matrix of type [[a]], then we can have M s. Deï¬ni-
tions of addition, M s + M t, and multiplication, M s * M t, can (with dereliction
of duty) assume that s and t are square of the same dimension. The element D r
stands for the square diagonal matrix (of any dimension) with r along the diago-
nal. The instance deï¬nitions of addition and multiplication each require four clauses
(MM, DM, MD, DD), negate has two clauses (M, D):
â€¢ Rewrite part III of [55] to use Haskell, making good use of classes and instances
to reï¬‚ect the algebraic structure. At one level this can be approached as a pro-
gram translation exercise, and is rewarding in demonstrating Haskell to be a good
host language. At other levels it invites study of a good bit of theory (Euclidean do-
mains, ï¬nite ï¬elds, Chinese Remainder Theorem, interpolation, homomorphic image
schemes, Fast Fourier Transform, and Newtonâ€™s algorithm applied to power series).
Further to these tried-and-tested steps, there is, of course, unlimited scope for add-
ons.
Related software can be found in the Hackage repository of the Haskell website
(www.Haskell.org).
7
Concluding remarks
It is clear that sequence algebra serves calculus: many sequence identities foretell rela-
tionships between analytic functions; it serves combinatorics: many counting sequences
for discrete structures can be derived by sequence algebra; and it serves computation: it
expresses the behaviour of certain kinds of automata; it leads to interpolation methods
and summation formulae, and supports program calculation. The theory could hardly
be more foundational, and constructing an implementation from scratch emphasises its
concreteness, and has the potential to reinforce understanding.
We have exercised the implementation on examples from [32, 25], demonstrating
that it makes a valuable companion to those texts. It could be applied to other texts, for
example [15, 31, 4, 79, 77]. It can also serve as a centre-piece in a course on functional
programming in mathematics. And, indeed, the experience of typing up and experiment-
ing with the code, confronts one with intriguing issues in programming language design.
There is zero-testing on sequence elements, which could be used to open a discussion on
computability.
The on-line encyclopaedia of integer sequences [75] has hundreds of thousands of
sequences. The sequences we have mentioned can be found using the OEIS search facility.
It will be noticed that many of the sequences are accompanied by generating code written
in various languages, including Haskell.
One may like to investigate how many OEIS
entries can be expressed in the â€œlanguageâ€ of tables 4 and 5. A Haskell interface to the
OEIS is reported in [88].
Needless to say, to elaborate the topic more fully, with proof details and examples,
one needs a book-sized exposition (draft portions of a book may be requested from the
39

author). Beyond that, the obvious question is how to make a seamless progression. A
few programming-oriented suggestions are in section 6.
On the theory side, we must
acknowledge that sequence algebra is so low in the mathematical hierarchy, that it doesnâ€™t
determine a narrow range of follow-up topics. Nevertheless, we mention a few. One is
the classiï¬cation of sequences, taking a lead from [33] and [76, Ch. 6]. Related to this is
the computer algebra work done under the heading â€œgenerating functionsâ€ or â€œholonomic
functionsâ€ [35, 46].
It remains to construct a bridge from the elementary level of the
present paper to the use of a computer algebra package.
Established results on diï¬€erential equations, including computer-algebraic, may be
revisited with an eye to drawing out those which become particularly accessible when
specialised to sequences. One suggestion is to bring the method of characteristics as used,
for example in [22], into common parlance for sequence work. Another is to ï¬nd a smooth
passage from the level of the present paper to results obtained using the language of
Species, for example those in [4, 70] (an introduction to Species for Haskell programmers
is [89]).
Various multivariate directions beckon, including formal languages [5, 3] and mul-
tivariate Lagrange inversion [30]. We have also arrived at the threshold of analysis but
we have not crossed it, except for bringing Ï€ into item X. It is natural to ask whether
ï¬‚uency in inï¬nite sequences, as promoted here, has any bearing on how students approach
Cauchy sequences and analytic functions. Related to this is the progression from chapter
1 to chapter 2 in [36] (and chapter VII of [19]). On another tack, one may use sequence
algebra to motivate abstract algebra. For example, Eilenberg [19, ch. XVI, sect. 10] gives
a proof of the Cayley Hamilton theorem using module concepts, and module concepts are
used in [26, 27, 28] â€“ papers whose titles echo [48, 49], but which involve a quantum-leap
in mathematical sophistication. As a ï¬nal remark, we note that the eponymous Haskell
B. Curry, also abstracted from concrete operations on formal power series [16].
Acknowledgements
This work originated (some years ago) when I was an occasional visitor at the University
of York. I am greatly indebted to Colin Runciman for providing that opportunity, and
to Colin, Jeremy Jacob, and Detlef Plump for encouragement. Special thanks are due to
Daniel Siemssen, Patrik Jansson, Tim Sears and Peter Thiemann for comments on work
related to this paper. (Also, if you are an anonymous JFP reviewer of an earlier related
paper, then my thanks to you too!) Tim Sears has placed a version of the Haskell code
on www.GitHub.com (under TimSears/SequenceAlgebra).
References
[1] M. Aigner and G.M. Ziegler. Proofs from THE BOOK, 3rd edn. Springer, 2004.
[2] W. Basler. Formal Power Series and Linear Systems of Meromorphic Diï¬€erential
Equations. Springer, 2000.
[3] H Basold, H Hansen, J-Â´E Pin, and J Rutten. Newton series, coinductively: a com-
parative study of composition. Mathematical Structures in Computer Science, pages
1â€“29, 2017.
40

[4] F. Bergeron, G. Labelle, and P. Leroux. Combinatorial species and tree-like struc-
tures, volume 67 of Encyclopaedia of Mathematics. Cambridge University Press, 1998.
Translated from 1994 original in French.
[5] J. Berstel and C. Reutenauer. Rational Series and their Languages, volume 12 of
EATCS Monographs on Theoretical Computer Science. Springer Verlag, 1988.
[6] R. Bird and O. de Moor.
Algebra of Programming.
Series in Computer Science.
Prentice Hall International, 1997.
[7] R.S. Bird. Algebraic identities for program calculation. Computer Journal, 32(2):122â€“
126, 1989.
[8] L. Brand. A Division Algebra for Sequences and Its Associated Operational Calculus.
The American Mathematical Monthly, 71(7):719â€“728, 1964.
[9] L. Brand. Diï¬€erential and Diï¬€erence Equations. J. Wiley, 1966.
[10] B. Buchberger and M. Rosenkranz. Transforming problems from analysis to algebra:
A case study in linear boundary problems. J. Symbolic Computation, 47:589â€“609,
2012.
[11] P. Cameron. Notes on Counting: an Introduction to Enumerative Combinatorics.
Australian Mathematical Society Lecture Series. Cambridge University Press, 2017.
[12] A. Cayley. On the anayltical form called Trees. Second Part. Philosophical Magazine,
XVIII:374â€“378, 1859.
[13] A. Cayley. A theorem on trees. Quart. J. Pure Appl. Math., 23:376â€“378, 1889.
[14] W.Y.C. Chen. Context-free grammars, diï¬€erential operators, and formal power series.
Theoretical Computer Science, 117:113â€“129, 1993.
[15] L. Comtet.
Advanced Combinatorics: The Art of Finite and Inï¬nite Expansions.
Springer, 1974.
[16] H. B. Curry. Abstract Diï¬€erential Operators and Interpolation Formulas. Portugaliae
Mathematica, 10(4):135â€“162, 1951.
[17] N. Dershowitz and S Zaks. The Cycle Lemma and Some Applications. European J.
Combinatorics, 11(1):35â€“40, 1990.
[18] K. Doets and J. van Eijck. The Haskell Road to Logic, Maths and Programming,
volume 4 of Texts in Computing. College Publications, 2012.
[19] S. Eilenberg. Automata, Languages, and Machines, volume A. Academic Press, 1974.
[20] S. Elizalde and M. Noy. Consecutive patterns in permutations. Adv. Applied Mathe-
matics, 30:110â€“125, 2003.
[21] G. Everest, A. van der Poorten, I. Shparlinski, and T. Ward. Recurrence Sequences,
volume 104 of Mathematical Surveys and Monographs. The American Mathematical
Society, 2003.
41

[22] C.J. Fewster and D. Siemssen. Enumerating Permutations by their Run Structure.
Electronic Journal of Combinatorics, 21(4), 2014.
[23] P. Flajolet. Combinatorial Aspects of Continued Fractions. Discrete Mathematics,
32:125â€“161, 1980.
[24] P. Flajolet, B. Salvy, and P. Zimmermann. Automatic average-case analysis of algo-
rithms. Theoretical Computer Science, 79:37â€“109, 1991.
[25] P. Flajolet and R. Sedgewick. Analytic Combinatorics. Cambridge University Press,
2009.
[26] L. Gatto. Linear ODEs: an Algebraic Perspective. Instituto Nacional de MatheÂ´atica
e Aplicada, Rio de Janeiro, 2012.
[27] L. Gatto and D Laksov. From linear recurrence relations to linear ODEs with constant
coeï¬ƒcients. J. Algebra and its Applicatins, 5(6):3â€“19, 2016.
[28] L.
Gatto
and
I.
Scherbak.
Remarks
on
the
Cayley-Hamilton
theorem.
arXiv:1510.03022, 2015.
[29] I. M. Gessel. Lagrange inversion. J. Comb. Theory Ser. A, 144(C):212â€“249, November
2016.
[30] I.M. Gessel. A Combinatorial Proof of the Multivariate Lagrange Inversion Formula.
J. Combinatorial Theory, Series A, 45:178â€“195, 1987.
[31] I.P. Goulden and D.M. Jackson. Combinatorial Enumeration. John Wiley & Sons,
1983.
[32] R.L. Graham, D.E. Knuth, and O. Patashnik.
Concrete Mathematics, 2nd edn.
Addison-Wesley, 1994.
[33] H.H. Hansen, C. Kupke, and J. Rutten. Stream Diï¬€erential Equations: Speciï¬cation
Formats and Solution Methods. Logical Methods in Computer Sciience, 13(1:3):1â€“51,
2017.
[34] W.K. Hayman. Review of Generatingfunctionoly by H.S. Wilf. Bulletin of Americam
Math. Soc., 21(1):104â€“106, 1991.
[35] W. Hebisch and M. Rubey. Extended rate, more gfun. J. Symb. Comput., 46(8):889â€“
903, August 2011.
[36] P. Henrici. Applied and Computational Complex Analysis, Vol 1. John Wiley and
Sons, 1974.
[37] R Hinze.
Functional pearl: Streams and Unique Fixed Points.
ACM SIGPLAN
Notices, 43(9):189â€“200, 2008.
[38] R Hinze. Scans and convolutions â€“ a calculational proof of Moessnerâ€™s theorem. In
Sven-Bodo Scholtz, editor, Proc. 20th Intl. Symposium on The Implementation and
Application of Functional Languages (IFL 08), vol. 5836 Lecture Notes in Computer
Science. Springer-Verlag, 2008.
42

[39] R. Hinze. Concrete Stream Calculus: An Extended Study. J. Functional Program-
ming, 20(5â€“6):463â€“535, November 2010.
[40] P. Hudak, J. Hughes, S. Peyton-Jones, and P. Wadler. A History of Haskell: Being
Lazy With Class. In Proc. Third ACM SIGPLAN History of Programming Languages
Conf. ACM, 2007.
[41] B. Jacobs and J. Rutten. A Tutorial on (Co)Algebras and (Co)Induction. Bulletin
of the EATCS, 62:222â€“259, 1997.
[42] N. Jacobson. Lectures in Abstract Algebra, Vol 1. Van Nostrand, 1951.
[43] W.P. Johnson.
The Curious History of Fa`a di Brunoâ€™s Formula.
The American
Mathematical Monthly, 109:217â€“234, 2002.
[44] A. Joyal. Une thÂ´eorie combinatoire des sÂ´eries formelles. Advances in Mathematics,
42(1):1â€“82, 1981.
[45] J. Karczmarczuk. Generating power of lazy semantics. Theoretical Computer Science,
187:203â€“219, 1997.
[46] M. Kauers. The Holonomic Toolkit. In Carsten Schneider and Johannes BlÂ¨umlein,
editors, Computer Algebra in Quantum Field Theory: Integration, Summation and
Special Functions, pages 119â€“144. Springer Vienna, 2013.
[47] W.F. Keigher. On the ring of hurwitz series. Communications in Algebra, 25(6):1845â€“
1859, 1997.
[48] D. A. Klarner. Algebraic theory for diï¬€erence and diï¬€erential equations. The Amer-
ican Mathematical Monthly, 76(4):366â€“373, 1969.
[49] D.A. Klarner.
Some Remarks on the Cayley-Hamilton Theorem.
The American
Mathematical Monthly, 83(5):367â€“369, 1976.
[50] K. Knopp. Theory and Application of Inï¬nite Series, 4th edn. Blackie & Son, 1951.
[51] D.E. Knuth. Bracket Notation for the â€˜Coeï¬ƒcient ofâ€™ Operator. In A. W. Roscoe,
editor, A Classical Mind, pages 247â€“258. Prentice Hall International (UK) Ltd., 1994.
[52] D. Kozen and A. Silva.
On Moessnerâ€™s Theorem.
The American Mathematical
Monthly, 120(2):131â€“139, 2013.
[53] M. Kwapisz. The Power of a Matrix. SIAM Rev., 40(3):703â€“705, 1998.
[54] I.E. Leonard. The Matrix Exponential. SIAM Rev., 38(3):507â€“512, 1996.
[55] J.D. Lipson. Elements of Algebra and Algebraic Computing. Addison-Wesley, 1981.
[56] E. Liz. A Note on the Matrix Exponential. SIAM Rev., 40(3):700â€“702, 1998.
[57] M.D. McIlroy. Power series, power serious. J. of Functional Programming, 3(9):325â€“
337, 1999.
[58] M.D. McIlroy. The Music of Streams. Inf. Proc. Letts., 77:189â€“195, 2001.
43

[59] D. Merlini, R. Sprugnoli, and M.C. Verri. Lagrange Inversion: When and How. Acta
Applicandae Mathematica, 94(3):233â€“249, 2006.
[60] D. Merlini, R. Sprugnoli, and M.C. Verri. The Method of Coeï¬ƒcients. The American
Mathematical Monthly, 114:40â€“57, 2007.
[61] H. Niederhausen. Finite Operator Calculus With Applications to Linear Recursions.
Florida Altlantic University, 2010.
[62] A. Nijenhuis and H.S. Wilf. Combinatorial Algorithms, 2nd edn. Academic Press,
1978.
[63] Milad Niqui and M. Rutten. An exercise in coinduction : Moessner â€™ s theorem.
Technical Report SEN-1103, CWI, Amsterdam, 2011.
[64] I. Niven. Formal Power Series. The American Mathematical Monthly, 76:871â€“889,
1969.
[65] J.T. Oâ€™Donnell, C.V. Hall, and R. Page. Discrete Mathematics using a Computer,
2nd edn. Springer, 2006.
[66] I. Pak. History of Catalan Numbers. In R.P. Stanley, The Catalan Numbers. Cam-
bridge University Press, 2014.
[67] D. PavloviÂ´c and M. EscardÂ´o. Calculus in Coinductive Form. In Proc. of the 13th
Annual IEEE Symposium on Logic in Computer Science, pages 408â€“417. IEEE Com-
puter Society Press, 1998.
[68] S. Peyton-Jones, editor. The Haskell 98 Language Report. www.Haskel.org, 2002. See
also The Haskell 2010 Language Report, e.d. S. Marlow.
[69] J. Pitman.
Enumerations of trees and forests related to branching processes and
random walks. In Microsurveys in Discrete Probability, number 41 in DIMACS Ser.
Discrete Math. Theoret. Comp. Sci, pages 163â€“180, 1998.
[70] C. Pivoteau, B. Salvy, and M. Soria. Algorithms for combinatorial structures: Well-
founded systems and Newton iterations. Journal of Combinatorial Theory, Series A,
119:1711â€“1773, 2012.
[71] G.N. Raney.
Functional composition patterns and power series reversion.
Trans.
American Mathematical Soc., 94:441â€“451, 1960.
[72] J. J. M. M. Rutten. A Coinductive Calculus of Streams. Math. Struct. in Comp. Sci.,
15(1):93â€“147, February 2005.
[73] J.J.M.M. Rutten. Coinductive counting with weighted automata. J. Automata, Lan-
guages and Combinatorics, 8(2):319â€“352, 2003a.
[74] D.E. Rydeheard and R.M. Burstall. Computational Category Theory. Prentice-Hall,
1988.
[75] N.J.A. Sloane, editor. The On-Line Encyclopedia of Integer Sequences. Published
electronically at https://oeis.org.
44

[76] R.P. Stanley. Hipparchus, Plutarch, SchrÂ¨oder, and Hough. The American Mathemat-
ical Monthly, 104:344â€“350, 1997.
[77] R.P. Stanley. Enumerative Combinatorics: vol 2. Cambridge Studies in Advanced
Mathematics. Cambridge University Press, 1999. With a contribution by S. Fomin.
[78] R.P. Stanley. A Survey of Alternating Permutations. Contemp. Math., 531:165â€“196,
2010.
[79] R.P. Stanley. Enumerative Combinatorics: vol 1 (2nd edn.). Cambridge Studies in
Advanced Mathematics. Cambridge University Press, 2011. (First ed. 1986).
[80] D. Stanton and D. White.
Constructive Combinatorics.
Undergraduate Texts in
Mathematics. Springer-Verlag, 1986.
[81] R. Street. Trees, Permutations and the Tangent Function. Reï¬‚ections, Math. Assoc.
of NSW, 27(2):19â€“23, 2002.
[82] G.J. Sussman and J. Wisdom. Functional Diï¬€erential Geometry. M.I.T. Press, 2012.
with W. Farr.
[83] J. F. Traub. Generalized Sequences with Applications to the Discrete Calculus. Math-
ematics of Computation, 19(90):177â€“200, 1965.
[84] W.T. Tutte. On Elementary Calculus and the Good Formula. Journal of Combina-
torial Theory (B), 18:79â€“137, 1975.
[85] M. Ward. A Calculus of Sequences. American Journal of Mathematics, 58:255â€“266,
1936.
[86] H.S. Wilf.
Generatingfunctionology.
Academic Press, 1990.
2nd Edition (1994)
available at http://www.cis.upenn.edu/~wilf.
[87] H.S. Wilf. East Side, West Side ... an introduction to combinatorial families â€“ with
Maple programming. Available at http://www.cis.upenn.edu/~wilf, 2002.
[88] J. Winter. QStream: A Suite of Streams. In R. Heckel and S. Milius, editors, Algebra
and Coalgebra in Computer Science. CALCO 2013. Springer, 2013.
[89] B A. Yorgey. Species and functors and types, oh my! SIGPLAN Not., 45(11):147â€“158,
September 2010.
[90] D. Zeilberger. Garsia and Milneâ€™s bijective proof of the Inclusion-Exclusion principle.
Discrete Mathematics, 51:109â€“110, 1984.
45

