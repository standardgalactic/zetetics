Synthesis Lectures on
Information Concepts, Retrieval, and Services
Lei Zhu · Jingjing Li · Weili Guan
Multi-modal 
Hash Learning
Efficient Multimedia Retrieval and 
Recommendations

Synthesis Lectures on Information
Concepts, Retrieval, and Services
Series Editor
Gary Marchionini, School of Information and Library Science, The University of North
Carolina at Chapel Hill, Chapel Hill, NC, USA

This series publishes short books on topics pertaining to information science and applica-
tions of technology to information discovery, production, distribution, and management.
Potential topics include: data models, indexing theory and algorithms, classiﬁcation,
information architecture, information economics, privacy and identity, scholarly com-
munication, bibliometrics and webometrics, personal information management, human
information behavior, digital libraries, archives and preservation, cultural informatics,
information retrieval evaluation, data fusion, relevance feedback, recommendation sys-
tems, question answering, natural language processing for retrieval, text summarization,
multimedia retrieval, multilingual retrieval, and exploratory search.

Lei Zhu · Jingjing Li · Weili Guan
Multi-modal Hash Learning
Efficient Multimedia Retrieval
and Recommendations

Lei Zhu
Shandong Normal University
Jinan, China
Weili Guan
Monash University
Sydney, NSW, Australia
Jingjing Li
University of Electronic Science
and Technology of China
Chengdu, China
ISSN 1947-945X
ISSN 1947-9468 (electronic)
Synthesis Lectures on Information Concepts, Retrieval, and Services
ISBN 978-3-031-37290-2
ISBN 978-3-031-37291-9 (eBook)
https://doi.org/10.1007/978-3-031-37291-9
© The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature
Switzerland AG 2024
This work is subject to copyright. All rights are solely and exclusively licensed by the Publisher, whether the whole
or part of the material is concerned, speciﬁcally the rights of translation, reprinting, reuse of illustrations, recitation,
broadcasting, reproduction on microﬁlms or in any other physical way, and transmission or information storage
and retrieval, electronic adaptation, computer software, or by similar or dissimilar methodology now known or
hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication does
not imply, even in the absence of a speciﬁc statement, that such names are exempt from the relevant protective
laws and regulations and therefore free for general use.
The publisher, the authors, and the editors are safe to assume that the advice and information in this book are
believed to be true and accurate at the date of publication. Neither the publisher nor the authors or the editors give
a warranty, expressed or implied, with respect to the material contained herein or for any errors or omissions that
may have been made. The publisher remains neutral with regard to jurisdictional claims in published maps and
institutional afﬁliations.
This Springer imprint is published by the registered company Springer Nature Switzerland AG
The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland

This book is dedicated to every researcher who
works on large-scale multimedia retrieval and
recommendation.

Preface I
Heterogeneous multi-modal data are increasing explosively nowadays in the big data
era. Multimedia retrieval and recommendation are facing unprecedented challenges on
both computation speed and storage cost. The technique of hashing can project high-
dimensional data into compact binary hash codes. With hashing, the most time-consuming
semantic similarity computation during the multimedia retrieval and recommendation
process can be signiﬁcantly accelerated with fast Hamming distance computation, and
meanwhile, the storage cost can be greatly reduced through binary embedding. Hence,
multi-modal hashing has recently received considerable attention to support large-scale
multimedia retrieval and recommendation.
This book is the ﬁrst book dedicated to multi-modal hash learning, which learns binary
representations in a low-dimensional Hamming space while preserving the heteroge-
neous multi-modal semantics for large-scale multimedia retrieval and recommendation.
Multi-modal hash learning has become one of the promising techniques in recent years
to support large-scale multimedia applications and has received great attention in both
academia and industry.
This book serves as a systematic introduction to multi-modal hash learning for retrieval
and recommendation, including a survey of current developments and the state-of-the-art
in this research ﬁeld. It not only comprehensively covers the key contents and recent
advancements of multi-modal hashing, including context-aware hashing, cross-modal
hashing, composite multi-modal hashing, and multi-modal discrete collaborative ﬁltering,
but also presents the hashing applications in multimedia retrieval and recommendation.
Besides, the book provides a platform and practice of multi-modal hash learning. As the
ﬁrst book on this theme, it summarizes the latest developments and presents cutting-edge
research on multi-modal hash learning for multimedia retrieval and recommendation. It
may provide researchers with an understanding of the important problems and a good
entry point for working on this research area.
The authors of this book have contributed substantial research on multi-modal hash
learning and related topics. Lei Zhu is one of the leading researchers on multi-modal
hashing. Jingjing Li is a long-term collaborator with Lei Zhu on research into multi-modal
hashing. Weili Guan is a rising star scholar in multimedia retrieval and recommendation.
vii

viii
Preface I
The book systematically summarizes their contributions in the direction of multi-modal
hash learning for efﬁcient multimedia retrieval and recommendation. It can be used as an
excellent self-contained take-off point for beginning researchers in multimedia retrieval.
Jinan, China
Chengdu, China
Sydney, Australia
Lei Zhu
Jingjing Li
Weili Guan

Preface II
In recent years, many multimedia applications such as search engines, social websites,
and online shopping platforms have developed at an unprecedented speed. While these
network services provide great convenience to our daily life, they generate a large amount
of multimedia data, such as text, image, audio, and video. Multimedia data is not only
large in quantity, but also complex in structure and diverse in content. These character-
istics bring challenges to many research problems and great opportunities. In particular,
the demands of users for multimedia data retrieval, content recommendation, and other
technologies are increasing day by day. An urgent need is for efﬁcient learning models to
organize and manage large-scale multimedia data.
Multi-modal hash learning can encode data from multiple different modalities into
compact binary hash codes. It has the desirable advantages of fast retrieval speed and
low storage cost, and can effectively support large-scale multimedia retrieval and recom-
mendation. Therefore, multi-modal hash learning has recently gained increasing attention.
Multi-modal learning to hash indeed has been well-studied in the past decade. However,
multi-modal hashing for multimedia retrieval and recommendation in a big data environ-
ment has its unique properties and corresponding challenges, including but not limited to
the following points:
(1) Heterogeneous modality gap. Multi-modal data features belong to different repre-
sentation spaces, it is a challenge to directly build the correlation structures across
heterogeneous modalities in the process of multi-modal hash learning.
(2) Ineffective multi-modal modeling. Existing methods usually exploit linear or simple
nonlinear functions for multi-modal hash projection. They cannot effectively capture
the intrinsic multi-modal data structure, which is important for modeling the multi-
modal correlation and semantics.
(3) Inefﬁcient hash optimization process. This problem leads to extremely high time and
space complexity of multi-modal hashing in multimedia retrieval and recommenda-
tion. Such a large computational cost makes extending existing methods to large-scale
scenarios difﬁcult.
ix

x
Preface II
(4) Cold-start and explainable recommendation with binary hashing. Existing hashing-
based recommendation systems employ user-item interactions and single auxiliary
information to learn the binary hash codes. But the full interaction history is not
always available and single auxiliary information may be missing. Moreover, existing
hashing-based recommendation systems remain black boxes without any explainable
outputs that illustrate why the system recommends the items.
In this book, to tackle the above research challenges, we present several state-of-the-
art multi-modal hashing learning methods and verify them through extensive experiments.
Speciﬁcally, we ﬁrst introduce two context-aware hashing methods for large-scale image
retrieval. One approach considers contextual social tags as a kind of semantic resource,
and another approach considers exploring semantic information from image structure.
We then present two cross-modal hashing learning frameworks to seek the multi-modal
complementary space and learn hash functions to support unsupervised and super-
vised cross-modal retrieval, respectively. Following that, we work toward two composite
multi-modal hashing methods. We not only design a self-weighted fusion strategy that
adaptively preserves multi-modal feature information into hash codes by exploiting the
complementarity of multi-modal features, but we also excavate bit-wise semantic concepts
and align the heterogeneous modalities at the concept level for multi-modal hash learn-
ing. Thereafter, we introduce two hashing-based multi-modal recommendation methods:
multi-modal discrete collaborative ﬁltering and explainable discrete collaborative ﬁltering.
We ﬁnally conclude the book and present the future research directions in multi-modal
hash learning, e.g., deep multi-modal modeling, multi-modal hash learning under open
dynamic environment, lightweight hash model design, etc.
This book represents preliminary research on multi-modal hash learning for multimedia
retrieval and recommendation. We hope that it can help beginners understand the ﬁeld,
and hope that it can arouse active researchers to work in this exciting ﬁeld. If, in this
book, we have been able to dream further than others have, it is because we are standing
on the shoulders of giants.
Jinan, China
Chengdu, China
Sydney, Australia
September 2022
Lei Zhu
Jingjing Li
Weili Guan

Acknowledgments
This book would not have been completed, or at least not be what it looks like today, with-
out the support of many colleagues, especially those from the Big Media Data Computing
Lab at Shandong Normal University. It is a pleasure to take this opportunity to acknowl-
edge them for their contributions to this time-consuming book project. Their contributions
have supplied ingredients for insightful discussions related to the writing of this book, and
hence we are greatly appreciative.
Our ﬁrst thanks undoubtedly go to Dr. Hui Cui, Dr. Yang Xu, Mr. Wentao Tan at
Shandong Normal University, Mr. Xize Wu at Southeast University, Dr. Xu Lu at Shan-
dong Agricultural University, Dr. Liqiang Nie, and Dr. Zheng Zhang at Harbin Institute of
Technology (Shenzhen), Dr. Zhiyong Cheng at Shandong Artiﬁcial Intelligence Institute,
Dr. Yang Yang at the University of Electronic Science and Technology of China, Dr.
Junwei Han at Northwestern Polytechnical University, as well as Dr. Huaxiang Zhang
at Shandong Normal University. We consulted with them on some speciﬁc technical
chapters of the book and they are also the major contributors to some chapters. Their
constructive feedback and comments at various stages have been signiﬁcantly helpful in
shaping the book. We also take this opportunity to thank Prof. Heng Tao Shen at the
University of Electronic Science and Technology of China who never hesitated to offer
his advice and share his valuable experience whenever the authors needed him.
We are grateful to Editor Mrs. Susanne Filler for her great efforts on this book. They
also help to make the book published smoothly and enjoyable.
Last, but certainly not least, our thanks go to our beloved families for their selﬂess
consideration, endless love, and unconditional support.
xi

xii
Acknowledgments
This work was supported in part by the National Natural Science Foundation of China
under Grant 62172263, in part by the Natural Science Foundation of Shandong, China,
under Grant ZR2020YQ47 and Grant ZR2019QF002, in part by the Youth Innovation
Project of Shandong Universities, China, under Grant 2019KJN040.
September 2022
Lei Zhu
Jingjing Li
Weili Guan

Contents
1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.1
Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.2
Challenges . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
2
1.3
Our Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
1.4
Book Structure . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4
2
Context-Aware Hashing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
2.1
Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
2.2
Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
8
2.2.1
Supervised Hashing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
8
2.2.2
Unsupervised Hashing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9
2.2.3
Social Image Hashing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
10
2.3
Dual-Level Semantic Transfer Deep Hashing . . . . . . . . . . . . . . . . . . . . . . . .
11
2.3.1
Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
11
2.3.2
Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
12
2.3.3
Experiment
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
20
2.4
Two-Pronged Strategy: Lightweight Augmented Graph Network
Hashing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
27
2.4.1
Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
27
2.4.2
Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
28
2.4.3
Experiment
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
33
2.5
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
40
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
41
3
Cross-Modal Hashing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
45
3.1
Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
45
3.2
Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
45
3.2.1
Supervised Cross-Modal Hashing . . . . . . . . . . . . . . . . . . . . . . . . . . . .
45
3.2.2
Unsupervised Cross-Modal Hashing . . . . . . . . . . . . . . . . . . . . . . . . . .
46
xiii

xiv
Contents
3.3
Efﬁcient Hierarchical Message Aggregation Hashing . . . . . . . . . . . . . . . . . .
47
3.3.1
Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
47
3.3.2
Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
48
3.3.3
Experiment
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
54
3.4
Correlation-Identity Reconstruction Hashing . . . . . . . . . . . . . . . . . . . . . . . . .
69
3.4.1
Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
69
3.4.2
Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
70
3.4.3
Experiment
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
77
3.5
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
85
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
86
4
Composite Multi-modal Hashing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
91
4.1
Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
91
4.2
Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
92
4.2.1
Unsupervised Composite Multi-modal Hashing . . . . . . . . . . . . . . . .
92
4.2.2
Supervised Composite Multi-modal Hashing . . . . . . . . . . . . . . . . . .
92
4.2.3
Deep Composite Multi-modal Hashing
. . . . . . . . . . . . . . . . . . . . . . .
93
4.3
Online Multi-modal Hashing with Dynamic Query-Adaption
. . . . . . . . . .
93
4.3.1
Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
93
4.3.2
Problem Formulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
95
4.3.3
Supervised Multi-modal Hashing with Online
Query-Adaption . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
97
4.3.4
Unsupervised Multi-modal Hashing with Online
Query-Adaption . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
105
4.3.5
Experiment
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
111
4.4
Bit-aware Semantic Transformer Hashing
. . . . . . . . . . . . . . . . . . . . . . . . . . .
126
4.4.1
Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
126
4.4.2
Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
129
4.4.3
Experiment
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
135
4.5
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
141
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
142
5
Multi-modal Discrete Collaborative Filtering . . . . . . . . . . . . . . . . . . . . . . . . . . .
145
5.1
Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
145
5.2
Related Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
146
5.2.1
Recommendation with Multi-modal Auxiliary Information . . . . . .
146
5.2.2
Hashing-Based Recommendation . . . . . . . . . . . . . . . . . . . . . . . . . . . .
146
5.3
Multi-modal Discrete Collaborative Filtering . . . . . . . . . . . . . . . . . . . . . . . . .
148
5.3.1
Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
148
5.3.2
Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
148
5.3.3
Experiment
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
161

Contents
xv
5.4
Explainable Discrete Collaborative Filtering . . . . . . . . . . . . . . . . . . . . . . . . .
170
5.4.1
Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
170
5.4.2
Methodology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
171
5.4.3
Experiment
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
178
5.5
Summary
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
190
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
191
6
Research Frontiers . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
197
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
199

About the Authors
Lei Zhu is currently a professor with the School of Informa-
tion Science and Engineering, Shandong Normal University.
He received his B.Eng. and Ph.D. degrees from Wuhan
University of Technology in 2009 and Huazhong Univer-
sity Science and Technology in 2015, respectively. He was
a Research Fellow at the University of Queensland (2016–
2017). His research interests are in the area of large-scale mul-
timedia content analysis and retrieval. Zhu has co-/authored
more than 100 peer-reviewed papers, such as ACM SIGIR,
ACM MM, IEEE TPAMI, IEEE TIP, IEEE TKDE, and ACM
TOIS. His publications have attracted more than 6,200 Google
citations. At present, he serves as the Associate Editor of
IEEE TBD, ACM TOMM, and Information Sciences. He has
served as the Area Chair of ACM MM/IEEE ICME, Senior
Program Committee for SIGIR/CIKM/AAAI. He won ACM
SIGIR 2019 Best Paper Honorable Mention Award, ADMA
2020 Best Paper Award, ChinaMM 2022 Best Student Paper
Award, ACM China SIGMM Rising Star Award, Shandong
Provincial Entrepreneurship Award for Returned Students, and
Shandong Provincial AI Outstanding Youth Award.
xvii

xviii
About the Authors
Jingjing Li is currently a professor at the School of Computer
Science and Engineering, University of Electronic Science
and Technology of China (UESTC). He received his B.Eng.,
M.Sc., and Ph.D. degrees from UESTC in 2010, 2013, and
2015, respectively. His research interests are in the area of
domain adaptation and zero-shot learning. He has co-authored
more than 70 peer-reviewed papers, such as IEEE TPAMI,
IEEE TIP, IEEE TKDE, CVPR, ICCV, AAAI, IJCAI, and
ACM Multimedia. He won the Excellent Doctoral Disser-
tation Award from the Chinese Institute of Electronics in
2018.
Weili Guan is currently a ﬁnal year Ph.D. student with the
Faculty of Information Technology, Monash University Clay-
ton Campus, Australia. Her research interests are multimedia
computing and information retrieval. She received her bache-
lor’s degree from Huaqiao University. She then obtained her
master’s degree from the National University of Singapore.
After that, she joined Hewlett Packard Enterprise Singapore
as a software engineer and worked there for around ﬁve years.
She has published dozens of papers at the top conferences and
journals, like ACM SIGIR, IEEE TIP, and IEEE TPAMI.

Abbreviations
ABinCF
Adversarial Binary Collaborative Filtering framework
ABQ
Adaptive Binary Quantization
ACR
Adjacent Correlation Reconstruction
AGCH
Aggregation-based Graph Convolutional Hashing
AGH
Anchor Graph Hashing
ALM
Augmented Lagrangian Multiplier
AP
Average Precision
ASCSH
Asymmetric Supervised Consistent and Speciﬁc Hashing
ATanh
Adaptive Tanh
BATCH
scalaBle AsymmeTric discrete Cross-modal Hashing
BoVW
Bag of Visual Words
BoW
Bag of Words
BP
Back-Propagation
BSTH
Bit-aware Semantic Transformer Hashing
CCA
Canonical Correlation Analysis
CCA-ITQ
ITerative Quantization with Canonical Correlation Analysis
CCQ
Composite Correlation Quantization
CCR
Coding Consistency Reconstruction
CDL
Collaborative Deep Learning
CF
Collaborative Filtering
CIRH
Correlation-Identity Reconstruction Hashing
CMFH
Collective Matrix Factorization Hashing
CM-MAN
Cross-Modal Message Aggregation Network
CNNH
Convolutional Neural Network Hashing
CPAH
Consistency-Preserving Adversarial Hashing
CSA
Cross-modal Semantic Aggregation
CTR
Collaborative Topic Regression
CVH
Cross-View Hashing
DBN
Deep Belief Network
DBRC
Deep Binary ReConstruction
xix

xx
Abbreviations
DCC
Discrete Cyclic Coordinate descent
DCD
Discrete Coordinate Descent
DCF
Discrete Collaborative Filtering
DCHUC
Deep Cross-modal Hashing with hashing functions and Uniﬁed hash
Codes jointly learning
DCMF
Discrete Content-aware Matrix Factorization
DCMH
Deep Cross-Modal Hashing
DCMVH
Deep Collaborative Multi-View Hashing
DDL
Discrete Deep Learning
DeepMF
Deep Matrix Factorization
DFM
Discrete Factorization Machines
DIS
DIScretization
DJSRH
Deep Joint-Semantics Reconstructing Hashing
DMFH
Deep Multiscale Fusion Hashing
DMVH
Discrete Multi-View Hashing
DPSH
Deep Pairwise Supervised Hashing
DSDH
Deep Supervised Discrete Hashing
DSR
Discrete Social Recommendation
DSTDH
Dual-level Semantic Transfer Deep Hashing
DTMF
Discrete Trust-aware Matrix Factorization
EDCF
Explainable Discrete Collaborative Filtering
FastHash
Fast supervised Hashing
FC
Fully Connected layer
FDMH
Flexible Discrete Multi-view Hashing
FGCMH
Flexible Graph Convolutional Multi-modal Hashing
FOMH
Flexible Online Multi-modal Hashing
GAN
Generative Adversarial Network
GCH
Graph Convolutional Hashing
GCN
Graph Convolutional Network
HCG
Hash Code Generation
HMAH
Hierarchical Message Aggregation Hashing
HSS
Hierarchical Sequence-to-Sequence
ICM
Iterated Conditional Modes
IMH
Inter-Media Hashing
IM-MAN
Intra-Modal Message Aggregation Networks
ISR
Identity Semantic Reconstruction
ITQ
ITerative Quantization
JDSH
Joint-modal Distribution-based Similarity Hashing
KSH
Supervised Hashing with Kernels
LAGNH
Lightweight Augmented Graph Network Hashing
LLE
Locally Linear Embedding

Abbreviations
xxi
LSH
Locality Sensitive Hashing
LSMH
Latent Semantic Minimal Hashing
LSSH
Latent Semantic Sparse Hashing
LSTM
Long Short-Term Memory
MAH
Multi-view Alignment Hashing
MAP
Mean Average Precision
MCGC
Multi-modal Collaborated Graph Construction
MDCF
Multi-modal Discrete Collaborative Filtering
MF
Matrix Factorization
MFDCF
Multi-Feature Discrete Collaborative Filtering
MFH
Multiple Feature Hashing
MFKH
Multiple Feature Kernel Hashing
MGRN
Masked visual semantic Graph-based Reasoning Network
MLP
Multi-Layer Perceptron
MRF
Markov Random Field
MTFH
Matrix Tri-Factorization Hashing
MvDH
Multi-view Discrete Hashing
MVLH
Multi-View Latent Hashing
NDCG
Normalized Discounted Cumulative Gain
NeuHash-CF
Neural Hashing-based Collaborative Filtering
NINH
Network In Network Hashing
NLL
Negative Log Loss
PCAH
Principal Component Analysis Hashing
PMF
Probabilistic Matrix Factorization
SADH
Similarity-Adaptive Deep Hashing
SAH
Semantic-Aware Hashing
SAPMH
Supervised Adaptive Partial Multi-view Hashing
SCADH
SCAlable Deep Hashing
SCRATCH
Scalable disCRete mATrix faCtorization Hashing
SDH
Supervised Discrete Hashing
SDMH
Supervised Discrete Multi-view Hashing
SGH
Scalable Graph Hashing
SH
Spectral Hashing
SKLSH
Locality-Sensitive Hashing with Shift-invariant Kernels
SMFH
Supervised Matrix Factorization Hashing
SMH-OQA
Supervised Multi-modal Hashing with Online Query-Adaption
SOTA
State-Of-The-Art
SRCH
Semantic-Rebased Cross-modal Hashing
SSDH
Semantic Structure-based Deep Hashing
SuperSDH
Supervised Semantics-preserving Deep Hashing
SVD
Singular Value Decomposition

xxii
Abbreviations
TBH
Twin-Bottleneck Hashing
UDCMH
Unsupervised Deep Cross-Modal Hashing
UH-BDNN
Unsupervised Hashing with Binary Deep Neural Network
UMH-OQA
Unsupervised Multi-modal Hashing with Online Query-Adaption
WDHT
Weakly supervised Deep Hashing using Tag embeddings
WMH
Weakly supervised Multi-modal Hashing
ZSR
Zero-Shot Recommendation

1
Introduction
1.1
Background
In recent years, with the development of Internet technology and the popularization of
portable devices, multimedia applications such as search engines, social networks, and
online shopping platforms have become indispensable parts of people’s lives. These net-
work services bring great convenience to our daily life. As a result, a rapidly growing
volume of multimedia data has been generated, i.e., text, image, audio, and video, as shown
in Figure 1.1. For example, Facebook, one of the most famous social networks in the world,
said in its 2020 annual report that every 60 seconds, about 510,000 text comments are
posted, about 136,000 images are uploaded, and about 293,000 status updates with texts,
images, and videos. Multimedia data is not only large in quantity but also complex, diverse,
high-dimensional, and multi-modal. These characteristics bring challenges to many research
questions. In particular, the increasing demand for multimedia data retrieval, recommenda-
tion, and other technologies leads to the urgent need to develop efﬁcient learning models to
organize, understand and manage multimedia data.
As one of the approximate nearest neighbor techniques, multi-modal hashing can encode
thehigh-dimensionaldatafrommultipledifferentmodalitiesintocompactbinaryhashcodes.
With it, the most time-consuming semantic similarity computation during the multimedia
retrieval and recommendation process can be signiﬁcantly accelerated with fast Hamming
distance computation, and meanwhile, the storage cost can be reduced greatly by the binary
embeddings. Therefore, multi-modal hash learning has gained more and more attention and
has been studied in the past decade. However, multi-modal hashing in a big data environment
has its unique properties and corresponding study challenges. Thus, it is highly desirable to
develop effective multi-modal hash learning frameworks for large-scale multimedia retrieval
and recommendation.
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024
L. Zhu et al., Multi-modal Hash Learning, Synthesis Lectures on Information
Concepts, Retrieval, and Services, https://doi.org/10.1007/978-3-031-37291-9_1
1

2
1
Introduction
Multimedia Data
Text
Image
Audio
Video
Fig.1.1 Multi-modal data from different applications
1.2
Challenges
In this work, we focus on multi-modal hash learning for efﬁcient multimedia retrieval and
recommendation. Since multimedia data come from different sources, in the big data envi-
ronment, they are not only large in data volume, but also high in complexity, diverse in
content, and heterogeneous. Efﬁcient and effective multi-modal hashing for multimedia
retrieval and recommendation still faces several important challenges.
Theﬁrstchallengeistheheterogeneousmodalitygap.Multimediadatausuallyusesmulti-
ple modalities to describe an instance. Although these different modalities are semantically
complementary, they belong to different representation spaces. Existing methods usually
learn common semantics among different modalities with a mandatory feature alignment
strategy, which is hard to bridge the heterogeneous multi-modal semantic gaps and produce
important semantic loss at the fusion stage. In the process of multi-modal hash learning,
how identifying a common space to establish the correlation of heterogeneous modalities is
still a challenging problem.
The second challenge is multi-modal semantic modeling. Most existing methods exploit
linear or simple nonlinear functions for multi-modal hash projection. They cannot effectively
model the intrinsic multi-modal data structure, which is important for capturing the latent
multi-modal semantics. In recent years, with the ﬂourishing of deep learning, it is promising
to leverage deep networks to design multi-modal hash learning frameworks.
The third challenge is the cold-start and explainable recommendation problem. Existing
hashing-based recommendation systems mainly rely on user-item interactions and a single
speciﬁc content feature. When the interaction history or the content feature is unavailable

1.3
Our Solutions
3
(the cold-start problem), their performance will seriously deteriorate. Furthermore, existing
recommendation methods with binary hashing still suffer from an important limitation in
practical applications that they are still black boxes and cannot provide users with reasonable
explanations for the recommendation results.
Last, but not least, the research problem we are facing is ineffective and inefﬁcient hash
optimization. Hash learning is an NP-hard problem due to the discrete constraint imposed
on hash codes. It is difﬁcult to solve it directly. To deal with this challenging optimization
problem, most existing methods adopt a simple two-step optimization strategy, which ﬁrst
solves a relaxed optimization problem and then generates hash codes by binary thresholding.
Thisstrategyindeedsimpliﬁesthe solutionprocess,but it mayleadtosigniﬁcant quantization
loss. Besides, some hashing methods adopt the bit-by-bit hash optimization to learn binary
hash codes directly. It is still time-consuming because each step only optimizes one bit and
multiple iterations are required to learn all hash bits.
1.3
Our Solutions
To address the above research challenges, we present several state-of-the-art multi-modal
hash learning methods and verify them over extensive experiments.
Weﬁrstintroducetwocontext-awarehashingmethodsforlarge-scaleimageretrieval.One
approach aims at learning the semantically enhanced hash codes by especially exploiting the
user-generated contextual tags associated with the social images. It ﬁrst directly preserves
the instance-level semantics into hash codes from the associated tags with adverse noise
removal. Then, it constructs an image-concept hypergraph for indirectly transferring the
latent high-order semantic correlations of images and tags into hash codes. Moreover, the
hash codes are obtained simultaneously with the deep representation learning by the discrete
hash optimization strategy. Another approach extracts the inner structure of the images as
the auxiliary semantics to enhance the semantic supervision of the unsupervised image
hash learning process. Moreover, it designs a lightweight network with the assistance of
auxiliary semantics, which greatly reduces the number of network parameters that need to
be optimized and thus greatly accelerates the training process.
Then, we present two cross-modal hash learning frameworks to seek the multi-modal
complementary space and learn hash functions to support supervised and unsupervised
cross-modal retrieval, respectively. For supervised cross-modal hashing, we design an efﬁ-
cient teacher-student learning framework. The teacher end develops hierarchical message
aggregation networks to construct a multi-modal complementary space by aggregating the
semantic messages hierarchically across different modalities. The student end trains a couple
of student modules that learn hash functions to support cross-modal retrieval. Furthermore,
we design a cross-modal correlation knowledge distillation strategy that seamlessly trans-
fers the modeled ﬁne-grained multi-modal semantic correlations from the teacher module
to the lightweight student modules. For unsupervised cross-modal hashing, a collaborated

4
1
Introduction
multi-modal matrix is constructed to supervise the relation learning of hash codes and
a cross-modal aggregation module is designed to bridge the heterogeneous modality gap
and generate latent common representations. Besides, an asynchronous learning strategy is
adopted to learn hash codes and hash functions separately.
We also focus on the composite multi-modal hashing methods. We design a self-weighted
fusion strategy to adaptively encode the multi-modal data into hash codes by exploiting their
complementarity. Based on the strategy, we propose a supervised multi-modal hashing with
an online query-adaption method and an unsupervised multi-modal hashing with the online
query-adaption method. The former one learns hash codes with the supervision of pair-
wise semantic labels and avoids the challenging symmetric similarity matrix factorization.
The latter one learns hash codes using codebooks and a set of complementary multi-modal
prototypes. Furthermore, we propose a bit-aware semantic transformer hashing framework to
extract bit-wise semantic concepts and simultaneously align the heterogeneous modalities at
the concept-level. Then, we perform the concept-level multi-modal fusion and further encode
the fused concept representations to the corresponding hash bits via bit-wise hash functions.
Further, to supervise the bit-aware transformer module, we develop a label prototype learning
module to learn prototype embeddings for all categories that capture the explicit semantic
correlations on the category-level by considering the co-occurrence priors.
To improve the efﬁciency and reduce the space consumption of the recommendation
system, we introduce two binary hashing-based multi-modal recommendation methods. The
ﬁrst one is a fast cold-start recommendation method. It designs a low-rank self-weighted
multi-feature fusion module to adaptively project the multiple content features into binary
hash codes by fully exploiting their complementarity. Additionally, it develops a fast-discrete
optimization algorithm to directly compute the binary hash codes with simple operations.
The second one is an end-to-end discrete recommendation framework based on multi-task
learning to simultaneously perform explainable and efﬁcient recommendations. It learns
hash codes by adaptively exploiting the correlations between the preference prediction task
and the explanation generation task. At the online recommendation stage, it makes efﬁcient
top-K recommendations by calculating Hamming distances between the hash codes and
simultaneously generates natural language explanations for recommendation results through
an explanation generation module.
1.4
Book Structure
The remainder of this book consists of ﬁve chapters. Chapter 2 presents two context-aware
hashing methods for image retrieval that exploit semantic information from contextual social
tags and image structure, respectively. Chapter 3 introduces two cross-modal hashing learn-
ing frameworks for supervised and unsupervised cross-modal retrieval. In Chap.4, we ﬁrst
design one supervised composite multi-modal hashing and one unsupervised composite

1.4
Book Structure
5
multi-modal hashing based on the proposed self-weighted fusion strategy. Then, we design
a bit-aware semantic transformer hashing framework for composite multi-modal hashing
learning. In Chap.5, we introduce two hashing-based multi-modal recommendation meth-
ods: multi-modal discrete collaborative ﬁltering and explainable discrete collaborative ﬁl-
tering. We conclude this book and discuss future research directions in Chap.6.

2
Context-Aware Hashing
2.1
Background
With the development of social media and mobile computing technology, the past decade has
witnessed tremendous growth in social images. Consequently, there has been an increasing
interest in information retrieval and multimedia computing communities to study intelli-
gent image retrieval techniques. In particular, image retrieval techniques [1, 2], where only
an image is used as the query, are gaining importance due to a wide range of promising
applications.
To provide high-quality content-based search services over a huge volume of image
collections, both efﬁciency and effectiveness are important. Advanced indexing structure is
essential to scale the big data space and facilitate accurate search. As one of the emerging
technologies to support fast and accurate image search, hashing has received great attention
and became a very active research domain in the last decade [3, 4]. Its basic idea of hashing is
to encode the high-dimensional image representations into binary codes in low-dimensional
Hamming space so that similarities of images can be efﬁciently measured by simple but
efﬁcient bit-wise operations. Generally, hashing enjoys two major advantages: (1) Fast query
response–The retrieval process can be completed quickly because bit-wise operations can
be efﬁciently implemented. (2) Low storage consumption–The storage of high-dimensional
image representations can be greatly reduced as a result of binary embedding.
However, due to the binary embedding of continuous feature space, semantic information
in original image features may be lost during hashing, which degrades the performance of
image retrieval. To enrich the semantics of hash codes, many machine learning methods and
deep neural network-based methods have been applied and several hashing schemes have
been proposed. They include supervised hashing [5–8] and unsupervised hashing [9–12].
Supervised hashing learns hash codes with explicit semantic labels, which can strengthen the
discriminative semantics of hash codes. However, this paradigm requires labeled images in
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024
L. Zhu et al., Multi-modal Hash Learning, Synthesis Lectures on Information
Concepts, Retrieval, and Services, https://doi.org/10.1007/978-3-031-37291-9_2
7

8
2
Context-Aware Hashing
Fig.2.1 Image samples. Two similar images of The Statue of Liberty collected from Wikipedia and
Flickr, respectively
thetrainingprocess.Thisrequirementmaybenotsatisﬁedintheimageretrievalprocesssince
good quality-labeled images are scarce in practical scenarios, while it requires a signiﬁcant
amount of manual effort and domain expertise. On the other hand, unsupervised hashing has
the advantage of good scalability as its whole learning process is independent of expensive
semantic labels. It is more suitable for practical retrieval applications. However, without
considering labels, the hash codes learned from unsupervised hashing may suffer from
limited discriminative semantics, which is important for retrieval performance. Images (such
as pictures in social networks) are usually associated with noisy but informative tags or
textual descriptions. Figure2.1 shows two image samples about The Statue of Liberty from
Wikipedia and Flickr, respectively. It is not hard to ﬁnd that both images are accompanied
by the texts including valuable semantic elements. These observations inspire us to exploit
the context of images to boost the quality of hashing. The core challenge is how to develop
the unsupervised learning scheme to intelligently extract and integrate the semantics from
the context-aware modalities of images into hash codes. In this chapter, we introduce two
context-aware hashing methods for large-scale image retrieval, which exploit contextual
tags and image structures to assist the image hash process.
2.2
Related Work
2.2.1
Supervised Hashing
Supervised hashing methods take advantage of the discriminative semantic labels as super-
vised information to learn the hash codes.
Representative supervised shallow hashing methods include: Supervised Hashing with
Kernels (KSH) [13], ITerative Quantization with Canonical Correlation Analysis

2.2
Related Work
9
(CCA-ITQ) [9], Fast Supervised Hashing (FastHash) [14], and Supervised Discrete Hashing
(SDH) [15]. KSH exploits the kernel-based hash functions to handle linearly inseparable
data and requires a few labels, i.e., similar and dissimilar example pairs. CCA-ITQ ﬁrst
utilizes the dimension reduction technology, i.e., Canonical Correlation Analysis (CCA) , to
pre-computed the features. Then, it minimizes the quantization error to learn binary codes.
FastHash applies the graph cuts algorithm to solve binary hash codes, which uses each bit of
the learned binary codes as classiﬁcation labels to train a classiﬁer. SDH makes full use of
label information. It is formulated as a least squares classiﬁcation that regressed each hash
code to its corresponding label.
Typical supervised deep hashing methods include: Convolutional Neural Network Hash-
ing (CNNH) [5], Network In Network Hashing (NINH) [6], Deep Pairwise Supervised
Hashing (DPSH) [7], Deep Supervised Discrete Hashing (DSDH) [16], and Supervised
Semantics-preserving Deep Hashing (SuperSDH) [17]. CNNH is the ﬁrst one which adopts
deep neural network for supervised hashing. It ﬁrst learns hash codes from pairwise labels,
and then tries to learn the hash functions and feature representations from image pixels based
on the learned hash codes. DPSH proposes an end-to-end learning framework, which per-
forms simultaneous feature learning and hash code learning for applications with pairwise
labels. Based on the assumption that the learned binary codes should be ideal for classiﬁ-
cation, DSDH uses both the pairwise label information and the classiﬁcation information
to learn the hash codes within one stream framework. SuperSDH assumes that the labels
are governed by several latent attributes and classiﬁcation relies on these attributes. It con-
structs hash functions as a latent layer in a deep network and the binary codes are learned by
minimizing an objective function deﬁned over classiﬁcation error and other desirable hash
codes properties.
Under the supervision of explicit semantic labels, the performance of the supervised
hashing methods is superior to the unsupervised methods. Nevertheless, the heavy burden of
manually annotating semantic labels hinders the application of supervised hashing methods
in large-scale image retrieval.
2.2.2
Unsupervised Hashing
Unsupervised hashing methods do not utilize semantic labels to learn hash functions during
their training processes. They basically preserve the geometric structure of original data into
hash codes with various unsupervised learning paradigms.
Typical unsupervised shallow hashing methods are as follows. Spectral Hashing (SH)
[18] performs spectral analysis on high-dimensional data, and transforms the binary hash
coding problem into a dimension reduction problem of the graph Laplacian by relaxing
the constraints. Anchor Graph Hashing (AGH) [19] constructs anchor graphs for solving
the eigenfunctions of the graph Laplacians, which improves the efﬁciency of graph hashing.
Principal Component Analysis Hashing (PCAH) [20] employs principal component analysis

10
2
Context-Aware Hashing
to learn a hash projection matrix. Iterative Quantization (ITQ) [9] ﬁrst projects data into
a low-dimensional space by principal component analysis, and then ﬁnds an orthogonal
transformation matrix to minimize the quantization loss. Scalable Graph Hashing (SGH)
[10] approximates the similarity graph by feature transformation, and proposes a strategy to
learn hash functions in a bit-wise manner. Latent Semantic Minimal Hashing (LSMH) [21]
reﬁnes the original features by matrix factorization, and minimizes the encoding loss with
an orthogonal transformation to learn discriminative hash codes.
Recently, a handful of deep learning based unsupervised hashing approaches have been
proposed. Unsupervised Hashing with Binary Deep Neural Network (UH-BDNN) [11]
regards the output of the penultimate layer of the designed neural network as hash codes,
and enforces the desirable properties of hash codes during the learning process: similarity
preserving, independence and balance. DeepBit [22] introduces a deep convolutional neu-
ral network and enforces three important criteria on the top layer of the network to learn
hash functions and discriminative binary codes. Semantic Structure-based Deep Hashing
(SSDH) [23] constructs a pair-wise semantic similarity structure by analyzing the data dis-
tribution of deep features. Then, the learned similarity structure is integrated into the deep
learning architecture to obtain the similarity-preserving hash codes. Similarity-Adaptive
Deep Hashing (SADH) [24] alternately trains three modules in one deep learning frame-
work. The preceding module guides the subsequent module, which makes the hash codes
to be more compatible with the deep hash functions. Twin-Bottleneck Hashing (TBH) [25]
designs an auto-encoding twin-bottleneck: a binary bottleneck and a continuous bottleneck.
The former explores the underlying data structure by adaptively constructing code-driven
similarity graphs, while the latter adopts data relevance information from the binary codes
for high-quality decoding and reconstruction.
With the powerful representation capability of deep networks, unsupervised deep hashing
methods could achieve signiﬁcant performance improvement compared with the shallow
methods. However, the binary hash codes learned by current unsupervised deep hashing
methods still suffer from insufﬁcient discriminative semantics, which are important for the
ultimate retrieval performance. Furthermore, since a large number of parameters are involved
in the deep neural networks and no explicit semantic supervision can be exploited to guide
the hash learning process, the current unsupervised deep hashing models are very difﬁcult
to optimize and their learning processes suffer from low training efﬁciency. The retrieval
accuracy and training efﬁciency of existing unsupervised deep hashing are still limited.
2.2.3
Social Image Hashing
Many researchers focus on the exploration of context image auxiliary semantics for hashing
performance enhancement. Representative work of this kind is social image hashing with the
exploitation of social tags, which are freely generated by individuals when uploading images
to social networks to describe the images and naturally contain relevant image semantics
[26, 27].

2.3
Dual-Level Semantic Transfer Deep Hashing
11
Semantic-Aware Hashing (SAH) [28] and Weakly supervised Multimodal Hashing
(WMH) [29] are two typically shallow hashing methods, which learn the hash codes by
hand-crafted features and social tags. SAH exploits tags to obtain binary codes by preserv-
ing the semantic structure and uses images to handle the possible adverse noises in the
visual structure. WMH simultaneously preserves the local discriminative structure and the
geometric structure in visual space to avoid over-ﬁtting the semantic information in tags.
Weakly supervised Deep Hashing using Tag embeddings (WDHT) [30], Masked visual
semantic Graph-based Reasoning Network (MGRN) [31], and SCAlable Deep Hashing
(SCADH) [32] are the representative deep social image hashing methods, which exploit
the auxiliary semantics, learn image representations and hash codes, in the uniﬁed deep
learning frameworks. WDHT extracts the word2vec semantic embeddings of the tags and
use the information contained in them for constraining the hash learning. MGRN constructs
a relation graph to capture the interactions among its associated tags and performs reason-
ing with Graph Attention Networks. The proposed GAE model captures the dependence
between the image and its associated tags, which can well address the problem of noisy
tags. SCADH leverages tags to enhance the discriminative capability of hash codes with
the involved adverse noise removal. Furthermore, it develops a discrete hash optimization
strategy to directly solve the hash codes and avoid the quantization loss in conventional
hashing methods. The above methods have shown promising performance improvements
compared to the unsupervised deep image hashing methods.
2.3
Dual-Level Semantic Transfer Deep Hashing
2.3.1
Motivation
Recent literature witness the birth and success of unsupervised deep hashing [11, 22,
24, 33], which enhances the representation capability of hash codes via joint deep image
representation and hash learning. Nevertheless, existing unsupervised deep hashing methods
still suffer from various shortcomings. Several works train the deep neural network by
directly minimizing the quantization loss [22] or data reconstruction errors [11]. They fail
to preserve the similarities of images into the hash codes. SADH [24] learns the hash codes
by considering the visual relations in deep neural network framework. But, the semantics
of the learned hash codes are still insufﬁcient. Besides, it explicitly computes the graph for
visual modeling, which is time-consuming when optimizing the hash codes. GreedyHash
[33] employs the greedy principle to solve deep hash optimization problem, which is also
time-consuming.
Fortunately, social images are generally accompanied with informative tags voluntarily
provided by users [29]. These freely provided tags are semantically relevant to the image
contents. Technically, they provide rich free semantic sources to enhance the discriminative
capability of hash codes. With this motivation, several hashing works [26, 28, 29] have

12
2
Context-Aware Hashing
been speciﬁcally developed for social image retrieval. SAH [28] and WMH [29] are shallow
hashing methods. They simply learn the hash codes by hand-crafted features and social tags.
However, the hand-crafted features involve limited semantics of images. In addition, these
two methods directly exploit the social tags for hash learning. They ignore the involved
adverse noises that may negatively affect the hashing performance. TWSH [26] proposes
a deep hashing framework consisting of the weakly-supervised pre-training stage and the
supervised ﬁne-tuning stage. Because it uses label information to assist hash code learning,
it has the same label dependence as supervised hashing.
Discrete optimization is important for generating hash codes. It is very hard to han-
dle the hash optimization problem because of the binary constraints on hash codes. Two
common optimization strategies are adopted in existing methods. The ﬁrst is two-step
relaxing+rounding optimization strategy (social hashing methods [26, 28, 29] adopt this
approach). It relaxes the discrete constraints on hash codes ﬁrstly, and then solves the ﬁnally
hash codes by mean-thresholding. This hash optimization method may produce signiﬁcant
quantization errors, which will lead to suboptimal solutions. The other is the bit-by-bit hash
optimization based on Discrete Cyclic Coordinate descent (DCC) [15]. It is still time con-
suming because each step only optimizes one bit and multiple iterations are required to
learn all hash bits. Motivated by above analyses, we propose a Dual-level Semantic Transfer
Deep Hashing (DSTDH) method to alleviate this problem with a uniﬁed deep hash learning
framework. Our model targets at learning the semantically enhanced deep hash codes by
specially exploiting the user-generated tags associated with the social images. Speciﬁcally,
we design a complementary dual-level semantic transfer mechanism to efﬁciently discover
the potential semantics of tags and seamlessly transfer them into binary hash codes. On the
one hand, instance-level semantics are directly preserved into hash codes from the associated
tags with adverse noise removing. Besides, an image-concept hypergraph is constructed for
indirectly transferring the latent high-order semantic correlations of images and tags into
hash codes. Moreover, the hash codes are obtained simultaneously with the deep representa-
tion learning by the discrete hash optimization strategy. Extensive experiments on two public
social image retrieval datasets validate the superior performance of DSTDH compared with
state-of-the-art hashing methods.
2.3.2
Methodology
Notations Deﬁnition
Thevectorsarerepresentedasboldfacelowercasecharactersandthematricesarerepresented
as boldface uppercase characters. For matrix A ∈Rm×n, the ith column and the (i, j)th
element are represented by ai and ai j, respectively. Tr(A) denotes the trace of A. AT is
the transpose of A. The Frobenius norm and the ℓ2,1-norm of A are denoted by ∥A∥2
F =
m
i=1
n
j=1 a2
i j and∥A∥2,1 = m
i=1
n
j=1 a2
i j,respectively.Thediagonalmatrixoperator

2.3
Dual-Level Semantic Transfer Deep Hashing
13
Table 2.1 Summary of
main notations
Notation
Explanation
n
Number of images
r
Hash code length
m
Number of anchors
a
Number of concepts
X
Visual feature matrix of images
Y
Image-tag relation matrix
Z
Hash codes matrix
P
Semantic transfer matrix
LG
Visual graph Laplacian matrix
LH
Hypergraph Laplacian matrix
A, B, EA, EB
Auxiliary matrices of ALM
H
Incidence matrix of hypergraph
Dv
Vertex degree matrix in hypergraph
De
Edge degree matrix in hypergraph
Dw
Edge weight matrix in hypergraph
is denoted as diag(·). The sign function is represented by sgn(·), which returns 1 for
positive, and −1 otherwise. In ∈Rn×n is the identity matrix, and 1 denotes the vector in
which all the elements are 1.
Suppose that X = [x1, . . . , xn] ∈Rd×n is a social image dataset, which contains n
images.Eachimagexi ∈Rd (d isthedimensionalityofimagefeature)isassignedwithc non-
overlapping social tags by users. We denote the image-tag relation as Y =

y1, . . . , yn

∈
Rc×n. The tag set of each image as yi ∈Rc, where y ji = 1 if xi is associated with jth tag
and y ji = 0 otherwise. Our objective is to learn the hash function h(·), which can generate
r-bits hash codes Z = [z1, . . . , zn] ∈[−1, 1]r×n for social images. The main notations and
system overview are shown in Table2.1 and Fig.2.2, respectively.
Visual Similarity Preservation
The objective of social image retrieval is to retrieve semantically similar images for the query.
Hence, mapping visually similar social images to the binary codes with the short Hamming
distance is signiﬁcant for the retrieval performance. In this work, we resort to graph model
[10] to preserve semantic similarities of images in binary hash codes. Speciﬁcally, heavy
penalty will be incurred if two similar images are mapped far apart. To this end, we seek to
minimize the weighted Hamming distance of hash codes

14
2
Context-Aware Hashing
Fig. 2.2 The basic framework of dual-level semantic transfer deep hashing method. The retrieval
system consists of two parts. The ofﬂine part trains a deep hash model, which can both generate
image feature representations and project images into hash codes. The features of images assist in
updating the image-concept hypergraph and the similarity graph. The semantics of hash codes can
be strengthened by image-concept hypergraph, similarity graph, and denoised tags. Besides, the hash
codes are directly learned based on the efﬁcient discrete optimization with low computation and
storage cost. The online part ﬁrst obtains the hash codes of a new query image by the hash functions
learned at ofﬂine stage. Then, the Hamming distances between the hash codes of the query image and
those of database samples are calculated. Finally, the database samples are sorted in the ascending
order of their corresponding Hamming distances
min
Z∈{−1,1}r×n
n

i, j=1
si j
zi −z j
2
F,
⇔
min
Z∈{−1,1}r×n Tr(ZLGZT),
(2.1)
where S ∈Rn×n is the afﬁnity matrix of visual graph, LG = diag(S1) −S is the corre-
sponding Laplacian matrix.
The calculation of S and LG will consume O(n2), which is unacceptable in large-scale
social image retrieval. In this work, we adopt anchor graph [19] to avoid this problem.
Speciﬁcally, we approximate the afﬁnity matrix as S = V3−1VT, where V ∈Rn×m is a
matrix which is obtained by computing the similarities between images and m anchor points,
and 3 = diag(VT1) ∈Rm×m. The resulted graph Laplacian matrix can be represented as

2.3
Dual-Level Semantic Transfer Deep Hashing
15
LG = In −V3−1VT. According to above analysis, we rewrite Eq.(2.1) as the following
equivalent form:
min
Z∈{−1,1}r×n Tr(Z(In −V3VT)ZT)
⇒
min
Z∈{−1,1}r×n −Tr(ZV3−1VTZT).
(2.2)
Direct Semantic Transfer
Due to the semantic gap, visual feature inherently has the limitation on representing high-
level semantics. As aforementioned above, social images are usually associated with user-
provided tags, which generally contain high-level semantics from users. Therefore, in this
work, we exploit the auxiliary social tags and directly transfer their semantics into hash
codes. Speciﬁcally, we directly correlate the hash codes with tags with a semantic transfer
matrix P, in order to directly transfer the involved instance-level semantics into the hash
codes. P is deﬁned as P =

p1, . . . , pr

∈Rc×r, where pi ∈Rc×1 is the semantic transfer
vector for the i-bits hash codes. Formally, we optimize the following problem to learn P
min
P
∥Z −PTY∥2,1.
(2.3)
Equation(2.3) adopts ℓ2,1-norm [34], which is designed to automatically remove the
noisesfromsocialtags.Thetagswithstrongersemanticdiscriminativecapabilityareretained
for semantic transfer.
Indirect Semantic Transfer
Social images are stored and disseminated in social networks. The latent semantic correla-
tions of images are important for social image retrieval. However, the above visual graph
and direct semantic transfer fail to fully capture the semantic correlations, due to the well-
known semantic gap and instance-level semantic transfer. Hence, it is promising to involve
the semantic correlations of images characterized by social tags into the hash codes. Intrin-
sically, the latent semantic correlations of images are high-order. It is common that a single
social image will be described by multiple tags, and a tag may be shared by multiple social
images (as shown in Fig.2.2). Theoretically, social images that share more tags are more
likely to have similar visual contents.
Inspired by this observation, in this work, we propose to construct an image-concept
hypergraph for indirectly transferring the semantic correlations of images. We ﬁrst pack
the visual feature matrix with the image-tag relation matrix, and then detect a concepts
by k-means [35] on the integrated matrix. In this case, the images are transformed as
ˆX =

ˆx1, . . . , ˆxn

∈R(d+c)×n. The detected concepts are represented as E = [e1, . . . , ea] ∈
R(d+c)×a. To model the latent high-order semantic correlations of images, we determine the
images as vertices, and the concepts as hyperedges. They jointly comprise the image-concept
hypergraph. In this way, an image represents several semantic concepts, and several images
may be latently included with more than one concept. Under such circumstance, the high-
order semantic correlations are naturally modelled. Mathematically, the hypergraph can be

16
2
Context-Aware Hashing
represented with a n × a incidence matrix H. The incidence value between vertex ˆx ∈ˆX
and hyperedge e ∈E in H is calculated as
h(ˆx, e) = exp(−
ˆx −e
2
F /2σ2),
(2.4)
where σ is bandwidth parameter. With H, the degree of hyperedge e is calculated as
δ(e) =

ˆx∈ˆX
h(ˆx, e).
(2.5)
We assume that the concepts are evenly distributed in database. Thus, we set the weights
of all hyperedges to 1, w(e) = 1. Accordingly, the degree of vertex ˆx is calculated as
d(ˆx) =

e∈E
w(e)h(ˆx, e) =

e∈E
h(ˆx, e).
(2.6)
Principally, images that belong to more identical hyperedges will describe similar seman-
tic concepts with greater probability. Therefore, they should be mapped to similar binary
codes within a short Hamming distance. To achieve this aim, we derive the following formula
to learn hash codes:
min
Z∈{−1,1}r×n
1
2

e∈E

ˆxi,ˆx j∈ˆX
h(ˆxi, e)h(ˆx j, e)
δ(e)
(
zi

d(ˆxi)
−
z j

d(ˆx j)
)
2
(2.7)
⇒
min
Z∈{−1,1}r×n Tr(ZLHZT),
(2.8)
where LH denotes the Laplacian matrix of hypergraph. To reduce the computation com-
plexity, we avoid explicitly computing the LH, and substitute it with the following form
LH = In −D−1/2
v
HDwD−1
e HTD−1/2
v
,
(2.9)
where Dv, De and Dw are diagonal matrices of vertex degrees, hyperedge degrees and
hyperedge weights, respectively. Equation(2.8) can be transformed as
min
Z∈{−1,1}r×n −Tr(ZD−1/2
v
HDwD−1
e HTD−1/2
v
ZT).
(2.10)
As shown in the optimization, this strategy can successfully reduce the computation cost.
Deep Representation Learning
Shallow hashing methods with deep features as input still obtain sub-optimal performance
because the separately learned hash codes may not be optimally compatible with the hash
function learning. In this work, we propose to jointly perform feature learning and hash
function learning in a uniﬁed deep framework. We adopt VGG-16 model [36] as our basic
deep hash model and the parameters are initialized with that are pre-trained on the ImageNet
dataset [36]. We modify the number of neurons in the last fully connected layer as the hash

2.3
Dual-Level Semantic Transfer Deep Hashing
17
code length and choose tanh function as the activation function. (xi; ) is determined
as the outputs of the last fully connected layer with data xi, where  is the parameters of
the deep network model. ˆ(xi; ˆ) represents the feature representation model, which is the
outputs of the second fully connected layer (FC7) associated with data xi, ˆ denotes all
the parameters of previous layers before the last fully connected layer. By encoding images
with ˆ(xi; ˆ), we can obtain more powerful deep image representations. Meanwhile, by
minimizing the quantization loss between the outputs of the deep model (xi; ) and the
learned binary codes zi, the energy of the learned compact features can be preserved into
the binary features, that is
min

∥(X; ) −Z∥2
F.
(2.11)
Overall Objective Function
By comprehensively considering the above factors, we derive the following objective for-
mulation
min
Z∈{−1,1}r×n,P,
∥Z −PTY∥2,1 −αTr(ZV3−1VTZT)
−βTr(ZD−1/2
v
HDwD−1
e HTD−1/2
v
ZT)
+ν ∥(X; ) −Z∥2
F.
(2.12)
Here α, β and ν are used to balance the effect of different terms. By joint learning, the
visual similarity preservation and dual-level semantic transfer can steer the feature repre-
sentation and hash function learning. On the other hand, the updated deep representation
and hash model can further beneﬁt the similarity preservation and semantic transfer.
Efﬁcient Discrete Optimization
Optimizing the hash codes in Eq.(2.12) is actually NP-hard problem due to the discrete
constraint and ℓ2,1-norm imposed on the hash codes. Existing discrete optimization methods
cannot be directly applied for solving our problem. In this work, we propose an efﬁcient
discrete optimization solution based on Augmented Lagrangian Multiplier (ALM) [37],
which keeps the binary constraints Z ∈{−1, 1}r×n and solves the hash codes directly. Our
idea is introducing auxiliary variables to move the discrete constraints out of ℓ2,1-norm, and
transform the optimization problem to an equivalent one that can be tackled more easily.
First, auxiliary variables A and B are added as A = Z −PTY, B = Z. With the substitution
of variables, the objective function is then reformulated as
min
Z∈{−1,1}r×n,P,A,B,
∥A∥2,1 + μ
2 ∥Z −PTY −A + EA
μ ∥2
F
+ μ
2 ∥Z −B + EB
μ ∥2
F −αTr(BV3−1VTZT)
−βTr(BD−1/2
v
HDwD−1
e HTD−1/2
v
ZT)
+ ν ∥(X; ) −Z∥2
F,
(2.13)

18
2
Context-Aware Hashing
Algorithm 2.1 Key steps of DSTDH
Input: Training image matrix X ∈Rd×n, image-tag relation matrix Y ∈Rc×n, the number of
anchor points m, the number of concepts a, hash code length r, parameter α, β and ν.
Output: Deep hash function h(x).
1: Initialize the deep model parameters  by the pre-trained VGG-16 model on ImageNet.
2: Randomly initialize Z = B as {−1, 1}r×n.
3: Initialize A, P, EA and EB as the matrices with all elements as 0.
4: repeat
5:
Update A by Eq.(2.16).
6:
Update P by Eq.(2.18).
7:
Update B by Eq.(2.20).
8:
Update Z and  by Eqs.(2.23) and (2.11).
9:
Update EA, EB and μ by Eq.(2.24).
10: until convergence.
11: Return h(x) = sgn((X; )).
where EA and EB measure the difference between the target and auxiliary variables, μ
adjusts the balance between the regularization terms. With the transformation, we can drive
the following iterative optimization steps to solve Eq.(2.13):
Update A. The optimization formula for A is
min
A ∥A∥2,1 + μ
2 ∥Z −PTY −A + EA
μ ∥2
F.
(2.14)
The Eq.(2.14) can be written in the following form
min
A
1
2∥A −T∥2
F + 1
μ∥A∥2,1,
(2.15)
where T = Z −PTY + EA
μ . The solution of A is
A (:, i) =
	 ∥ti∥−1
μ
∥ti∥ti, i f
1
μ < ∥ti∥
0,
otherwise.
(2.16)
Update P. Similarly, the optimization formula for P is
min
P
μ
2 ∥Z −PTY −A + EA
μ ∥2
F.
(2.17)
By calculating the derivative of Eq.(2.17) w.r.t P and setting it to 0, we can obtain that
P = (YYT)−1Y(ZT −AT + EA
μ ).
(2.18)

2.3
Dual-Level Semantic Transfer Deep Hashing
19
-Update B. The optimization formula for B becomes
min
B
μ
2 ∥Z −B + EB
μ ∥2
F −αTr(BV3−1VTZT)
−βTr(BD−1/2
v
HDwD−1
e HTD−1/2
v
ZT).
(2.19)
By calculating the derivative of Eq.(2.19) w.r.t B and setting it to 0, we derive that
B = Z + EB
μ + α
μZV3−1VT + β
μZD−1/2
v
HDwD−1
e HTD−1/2
v
.
(2.20)
-Update Z and . The optimization formula for Z is
min
Z∈{−1,1}r×n
μ
2 ∥Z −PTY −A + EA
μ ∥2
F
+ μ
2 ∥Z −B + EB
μ ∥2
F −αTr(BV3−1VTZT)
−βTr(BD−1/2
v
HDwD−1
e HTD−1/2
v
ZT)
+ ν ∥(X; ) −Z∥2
F.
(2.21)
Equation(2.21) can be transformed to
min
Z∈{−1,1}r×n −Tr(ZT(μA + μPTY + μB −EA −EB
+ αBV3−1VT + βBD−1/2
v
HDwD−1
e HTD−1/2
v
+ 2ν(X; ))).
(2.22)
The solution of Z can be obtained by the following closed form
Z = sgn(μA + μPTY + μB −EA −EB + αBV3−1VT
+βBD−1/2
v
HDwD−1
e HTD−1/2
v
+ 2ν(X; )).
(2.23)
As shown in Eq.(2.23), the hash codes Z are directly solved with a single hash code-
solving operation, which is faster than iterative hash codes solving step in DCC.
After that, the learned Z is taken into the Sect.2.3.2. Deep Representation Learning to
updatetheneuralnetworkparametersthroughthestandardbackpropagationandstochastic
gradient descent.
-Update EA, EB and μ. The ALM parameters are updated by
EA = EA + μ(Z −PTY −A),
EB = EB + μ(Z −B),
μ = ρμ,
(2.24)
here ρ controls the convergence speed.

20
2
Context-Aware Hashing
The key steps of proposed DSTDH are illustrated in Algorithm 2.1.
Discussions
(1) Convergency Analysis. In the discrete optimization process, when other variables are
ﬁxed, the objective function Eq.(2.13) is convex for one variable. Therefore, the value of
the objective function will remain unchanged or monotonically decrease in each iterative
optimization processes, and ﬁnally reaches the local minimum after several iterations. In
addition,theexperimentalresultsinSect.2.3.3alsovalidatetheconvergenceofourapproach.
(2) Computation Complexity Analysis. The computation complexity of visual anchor
graph construction and hypergraph construction are O(dmn) + O(dan). At each iteration of
discrete optimization, it takes O(rcn) for updating A, O(c3 + rcn + 2c2n) for updating P,
O((m + a)rn) for updating B, and O((m + a)rn) for updating Z. Finally, the computation
complexity of the discrete optimization process is O(tn), where t is the number of iterations
and t ≪n. Since the computation complexity of our method is linearly related to the number
of images in the social image dataset, the fast social image retrieval can be achieved.
2.3.3
Experiment
Datasets and Evaluation Metrics
We adopt two publicly available social image retrieval datasets to conduct experiments,
MIR Flickr [39] and NUS-WIDE [40]. They are both downloaded from Flickr. Table2.2
summarizes the basic statistics of these two datasets.
We evaluate the social image retrieval performance by Mean Average Precision (MAP)
[41, 42] and precision-recall curve [21]. The labels are adopted as the groundtruth for
evaluation. According to whether two images share at least one identical label, they can be
judged as semantically similar or not. For the two evaluation protocols, the higher value
indicates better performance.
Table 2.2 Statistics of the experimental datasets
Datasets
MIR Flickr
NUS-WIDE
#Categories
24
21
#Retrieval set
17,749
193,749
#Query set
2,266
2,085
#Training set
5,325
5,000
Tag feature (BOW)
1,386-D
1,000-D
Hand-crafted image feature (BOVW) [38]
1,000-D
500-D
Deep image feature (VGG)
4,096-D
4,096-D

2.3
Dual-Level Semantic Transfer Deep Hashing
21
Evaluation Baselines
We compare DSTDH method and its linear variant DSTDH-L with several state-of-the-art
unsupervised hashing methods. DSTDH-L employs the linear hash function h(x) = z =
sgn(WT
l x). It learns the hash functions with linear regression model, which minimizes
the differences between the generated binary codes and the projected continuous ones by
min
Wl
Z −WT
l X
2
F. Then we can get Wl = (XXT)−1XZT. The baseline methods are divided
into four categories:
• Eight unsupervised shallow hashing methods using the handcrafted feature: Locality
Sensitive Hashing (LSH) [43], Locality-Sensitive Hashing with Shift-invariant Kernels
(SKLSH) [44], PCA Hashing (PCAH) [20], Spectral Hashing (SH) [18], Scalable Graph
Hashing (SGH) [10], Iterative Quantization (ITQ) [9], Latent Semantic Minimal Hashing
(LSMH)[21]andSimilarity-AdaptiveDeepHashingwithlinearhashfunction(SADH-L)
[24].
• Aforementioned unsupervised shallow hashing methods using 4,096-dimensional deep
feature extracted by our feature learning model (we denote it as VGG feature for clarity).
• Four unsupervised deep hashing methods: Unsupervised Deep Learning of Compact
Binary Descriptors (DeepBit) [22], Unsupervised Hashing with Binary Deep Neural Net-
work (UH-BDNN) [11], Similarity-Adaptive Deep Hashing (SADH) [24], and Greedy-
Hash [33]. Following their original works, UH-BDNN uses the VGG feature, and the
others use the raw images.
• Two social image hashing methods: Semantic-Aware Hashing (SAH) [28] and Weakly-
supervised Multimodal Hashing (WMH) [29]. As SAH and WMH are shallow hashing
methods, we use VGG feature as the input in our experiments to be fair.
Implementation Details
α,β andν balancetheeffectsofeachregularizationtermintheobjectivefunction.m anda are
the number of anchor points and concepts, respectively. ρ and μ are for ALM method. In our
experiments, the optimal performance of DSTDH is achieved when {α = 0.01, β = 0.001,
ν = 0.001, m = 100, a = 500, ρ = 1.1, μ = 0.01} and {α = 10, β = 0.0001, ν = 10, m =
700, a = 700, ρ = 2, μ = 1} on MIR Flickr and NUS-WIDE, respectively. Our DSTDH is
implemented through the open source Caffe [45] framework. Speciﬁcally, the mini-batch
size is set as 15, the learning rate is set as 0.01, the momentum is set as 0.9, and the weight
decay is set as 0.0005. We implement SAH and WMH according to their corresponding
works since the codes of them are not available. The parameters of the two social image
hashing methods are carefully tuned to ﬁt the datasets. For other baseline methods, we
employ the codes provided by authors for our comparison experiments.
Performance Comparison
We ﬁrst present the comparison results of DSTDH and its shallow variant DSTDH-L with
the baselines in the ﬁrst three categories. Then, the comparison results with social image

22
2
Context-Aware Hashing
hashing methods SAH and WMH are provided. All the experiments are performed with 16,
32, 64 and 128 hash code length on NUS-WIDE and MIR Flickr.
(1) Comparison Results with Unsupervised Hashing. In Tables2.3 and 2.4, we show
the MAP comparison results. In Figs.2.3 and 2.4, we present the precision-recall curves of
the unsupervised shallow hashing methods (with VGG feature as input) and the unsupervised
deep hashing methods. Based on these results, we have the following observations: (a) The
MAP values of our DSTDH method consistently outperform the compared methods in all
cases. The DSTDH-L with linear hash function achieves better values than all the shallow
methods with hand-crafted and VGG feature as input. Those results clearly demonstrate the
effectiveness of our methods for hash function learning by exploiting the denoised social tags
Table 2.3 Comparison results with unsupervised hashing on MIR Flickr. The best result in each
column is marked with bold. The below is the same
Methods
Feature
MAP
16 bits
32 bits
64 bits
128 bits
LSH
BOVW
0.5830
0.5830
0.5827
0.5839
SKLSH
BOVW
0.5791
0.5896
0.5895
0.5939
PCAH
BOVW
0.5822
0.5825
0.5821
0.5832
SH
BOVW
0.5806
0.5854
0.5878
0.5903
SGH
BOVW
0.5871
0.5847
0.5844
0.5837
ITQ
BOVW
0.5820
0.5826
0.5820
0.5824
LSMH
BOVW
0.5821
0.5827
0.5815
0.5828
SADH-L
BOVW
0.5823
0.5822
0.5825
0.5809
DSTDH-L
BOVW
0.6111
0.6291
0.6355
0.6389
LSH
VGG
0.6103
0.6372
0.6513
0.6736
SKLSH
VGG
0.5823
0.5824
0.5826
0.5828
PCAH
VGG
0.6603
0.6519
0.6418
0.6320
SH
VGG
0.6510
0.6373
0.6292
0.6288
SGH
VGG
0.5841
0.5850
0.5893
0.5958
ITQ
VGG
0.7308
0.7392
0.7413
0.7445
LSMH
VGG
0.7079
0.7054
0.6979
0.6739
SADH-L
VGG
0.6182
0.6058
0.6019
0.5985
DSTDH-L
VGG
0.7388
0.7436
0.7481
0.7515
UH-BDNN
VGG
0.7215
0.7028
0.6854
0.6715
DeepBit
Raw
0.6134
0.6328
0.6527
0.6722
SADH
Raw
0.7477
0.7458
0.7565
0.7601
GreedyHash
Raw
0.6780
0.6889
0.7131
0.7287
DSTDH
Raw
0.7664
0.7769
0.7893
0.7980

2.3
Dual-Level Semantic Transfer Deep Hashing
23
Table 2.4 Comparison results with unsupervised hashing on NUS-WIDE
Methods
Feature
MAP
16 bits
32 bits
64 bits
128 bits
LSH
BOVW
0.3634
0.3642
0.3650
0.3648
SKLSH
BOVW
0.3555
0.3560
0.3572
0.3567
PCAH
BOVW
0.3642
0.3645
0.3647
0.3642
SH
BOVW
0.3561
0.3573
0.3526
0.3524
SGH
BOVW
0.3668
0.3662
0.3664
0.3657
ITQ
BOVW
0.3640
0.3641
0.3643
0.3637
LSMH
BOVW
0.3640
0.3640
0.3646
0.3635
SADH-L
BOVW
0.3638
0.3639
0.3643
0.3649
DSTDH-L
BOVW
0.4126
0.4272
0.4464
0.4604
LSH
VGG
0.4242
0.4572
0.4862
0.5085
SKLSH
VGG
0.3632
0.3637
0.3641
0.3644
PCAH
VGG
0.5234
0.5053
0.4889
0.4821
SH
VGG
0.4401
0.4674
0.4578
0.4980
SGH
VGG
0.3652
0.3679
0.3722
0.3823
ITQ
VGG
0.5808
0.5965
0.5998
0.6116
LSMH
VGG
0.5607
0.5639
0.5526
0.5218
SADH-L
VGG
0.4201
0.4152
0.4043
0.3988
DSTDH-L
VGG
0.5843
0.6032
0.6099
0.6150
UH-BDNN
VGG
0.5609
0.5445
0.5221
0.5180
DeepBit
Raw
0.4209
0.4264
0.4587
0.5224
SADH
Raw
0.5722
0.5919
0.6049
0.6002
GreedyHash
Raw
0.4921
0.5463
0.5765
0.5914
DSTDH
Raw
0.6324
0.6442
0.6526
0.6551
0
0.2
0.4
0.6
0.8
1
Recall @ 16 bits
0.55
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
Precision
0
0.2
0.4
0.6
0.8
1
Recall @ 32 bits
0.55
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
Precision
0
0.2
0.4
0.6
0.8
1
Recall @ 64 bits
0.55
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
Precision
0
0.2
0.4
0.6
0.8
1
Recall @ 128 bits
0.55
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
Precision
DSTDH
GreedyHash
SADH
DeepBit
UH-BDNN
LSH
SKLSH
PCAH
ITQ
SH
SGH
LSMH
SADH_L
DSTDH_L
Fig.2.3 The precision-recall curves on MIR Flickr

24
2
Context-Aware Hashing
0
0.2
0.4
0.6
0.8
1
Recall @ 16 bits
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Precision
0
0.2
0.4
0.6
0.8
1
Recall @ 32 bits
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Precision
0
0.2
0.4
0.6
0.8
1
Recall @ 64 bits
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Precision
0
0.2
0.4
0.6
0.8
1
Recall @ 128 bits
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Precision
DSTDH
GreedyHash
SADH
DeepBit
UH-BDNN
LSH
SKLSH
PCAH
ITQ
SH
SGH
LSMH
SADH_L
DSTDH_L
Fig.2.4 The precision-recall curves on NUS-WIDE
and dual-level semantic transfer. (b) DSTDH achieves higher MAP values than DSTDH-
L. This is because DSTDH jointly performs deep image representation learning and hash
function learning. The learned image feature representation can be compatible with the hash
function. (c) The MAP values of DSTDH increase when the hash code length becomes
longer, while that of several methods (e.g. UH-BDNN with the raw image, LSMH with
deep feature) gradually decrease. These results indicate that more discriminative semantics
can be obtained by our DSTDH method if longer hash codes are adopted. (d) Our method
using the shorter code length can obtain better performance than the baseline methods using
the longer code length. For instance, on NUS-WIDE, the MAP value of DSTDH is 0.6442
when the code length is 32 bits, which is higher than 0.6002 obtained by SADH of 128
bits. (e) In most cases, the MAP values of the shallow methods using the VGG feature are
signiﬁcantly higher than those using the BOVW feature. This is because the deep neural
network has better representation capability. (f) The precision-recall curves show that, in
most situations, the area of DSTDH is larger than other competing approaches. This indicates
that our DSTDH method can return more semantically similar images to the query at the
top of the retrieval results compared with the baselines.
(2) Comparison Results with Social Image Hashing. The MAP comparison results
are presented in Table2.5. It shows that the MAP values of DSTDH are higher than those
of SAH and WMH on all hash code lengths and datasets. The superior performance is
attributed to four reasons: (1) WMH and SAH directly exploit the noisy social tags during
the hash learning process, which may pose negative impact on the hashing performance.
(2) With joint representation learning and hash code learning, the hash codes generated by
DSTDH are empowered with better representation capability. (3) DSTDH adopts discrete
optimization strategy to efﬁciently solve hash codes within a single code-solving operation,
while SAH and WMH use the relaxed optimization strategy, which may result in signiﬁcant
quantization loss. (4) DSTDH jointly performs the deep hash function and code learning.
Besides, it adopts an effective dual-level semantic transfer to fully exploit the semantics of
images and tags.
Ablation Analysis
We provide the ablation analysis on the ﬁve variant approaches. All the experiments are
conducted on MIR Flickr with different hash code lengths (16, 32, 64 and 128 bits) and

2.3
Dual-Level Semantic Transfer Deep Hashing
25
Table 2.5 Comparison results with social image hashing
Methods
Feature
MAP
16 bits
32 bits
64 bits
128 bits
MIR Flickr
SAH
VGG
0.7360
0.7364
0.7355
0.7434
WMH
VGG
0.6412
0.6686
0.6787
0.6951
DSTDH
Raw
0.7664
0.7769
0.7893
0.7980
NUS-WIDE
SAH
VGG
0.4398
0.4489
0.4664
0.4840
WMH
VGG
0.4874
0.4806
0.4647
0.4427
DSTDH
Raw
0.6324
0.6442
0.6526
0.6551
Table 2.6 Ablation experiment results on MIR Flickr
Methods
Feature
MAP
16 bits
32 bits
64 bits
128 bits
DSTDH-D
Raw
0.7537
0.7590
0.7710
0.7835
DSTDH-I
Raw
0.7541
0.7751
0.7799
0.7939
DSTDH-NT
Raw
0.7499
0.7583
0.7712
0.7843
DSTDH-T
Raw
0.7472
0.7527
0.7657
0.7839
DSTDH-R
Raw
0.6790
0.6851
0.7058
0.7319
DSTDH
Raw
0.7664
0.7769
0.7893
0.7980
the experiment results are presented in Table2.6. Note that similar results can be found on
NUS-WIDE. We omit them due to the space limitation.
(1) Effects of Dual-level Semantic Transfer. To validate the effects of dual-level seman-
tic transfer, we compare the proposed DSTDH with its variants DSTDH-D and DSTDH-
I which remove the direct semantic transfer part and the indirect semantic transfer part,
respectively. From Table2.6, we can clearly ﬁnd that DSTDH outperforms DSTDH-D and
DSTDH-I on all code lengths. These results demonstrate that the two-levels of semantic
transfer are complementary to each other, and they are effective on transferring the seman-
tics from the social tags into the binary hash codes.
(2) Effects of Tag Guidance. To evaluate the effects of tag guidance on hash learning,
we compare the performance with the variant approach DSTDH-NT without any tags. The
experimental results in Table2.6 show that DSTDH performs better than DSTDH-NT, which
infer that using the tags (with noise removal) can improve the hashing performance.
(3) Effects of Noise Removal. To demonstrate the effects of ℓ2,1-norm on reﬁning the
user-generated tags, we compare DSTDH with its variant method DSTDH-T which directly

26
2
Context-Aware Hashing
correlates original tags with hash codes without removing the noises. As can be seen in
Table2.6, DSTDH consistently outperforms DSTDH-T on different hash code lengths. Thus,
we conclude that DSTDH can effectively alleviate the negative effects of noisy semantics
on the hash codes. Besides, we can ﬁnd that the MAP values of DSTDH-T are inferior to
DSTDH-NT. This phenomenon illustrates that the noises contained in user-provided tags
have adverse effects on hashing performance.
(4) Effects of Discrete Optimization. To demonstrate the effectiveness of the proposed
discrete optimization strategy, we utilize the variant approach DSTDH-R for comparison. It
ﬁrstly relaxes the discrete constraints on hash codes, and then obtains the ﬁnal hash codes
by mean-thresholding. In Table2.6, the comparison results show that DSTDH is obviously
superior to DSTDH-R. This phenomenon reveals that discrete optimization can effectively
alleviate the binary quantization loss and improve the retrieval performance.
Convergence and Parameter Analysis
In this subsection, we ﬁrst analyze the convergence of our DSTDH method on MIR Flickr
when the hash code length is ﬁxed as 32 bits. The convergence curve is shown in Fig.2.6a.
It can be seen that the objective function value ﬁrst decreases monotonically, and eventually
reaches the local minimum after several iterations (less than 10). These results verify the
convergence of our proposed discrete hash optimization strategy.
Then, we conduct parameter sensitivity analysis experiments to observe the variation of
MAP values under different α, β, ν, m and a. The experiments are performed on MIR Flickr
when the hash code length varying from 16 to 128 bits. We vary the value of α, β, ν from
{10−6, 10−4, 10−2, 0, 100, 101, 102}, while the others are ﬁxed. Moreover, the number of
anchor m and concept a are both varied from the range of {100, 200, ..., 1000} by ﬁxing
other parameters.
From Fig.2.5a–c, we can ﬁnd that when α is in the range of {10−6, 0}, β is in the range
of {10−6, 0} and ν is in the range of {10−6, 0}, the performance is relatively stable. From
Fig.2.5d, we can ﬁnd that the MAP values ﬂuctuate in several hash code lengths but the
overall trend is downward with the number of anchor points increasing. From Fig.2.5e, we
can ﬁnd that the MAP values relatively stable when a is varied from 100 to 1000 on all hash
code lengths.
Time Cost with Training Size
In this subsection, investigate the training time cost variations with the training size. Specif-
ically, we conduct experiment on MIR Flickr and record time cost when the training size
is varied from 500 to 5,000 by ﬁxing the hash code length as 32. Figure2.6b demonstrates
the training time increases linearly with the training size. It validates the linear scalability
of DSTDH, which is consistent with the aforementioned theoretical analysis.

2.4
Two-Pronged Strategy:Lightweight Augmented Graph Network Hashing
27
1e-06 0.0001 0.01  
0     
1     
10    
100   
0.72
0.74
0.78
MAP
MIR Flickr
16-bit
32-bit
64-bit
128-bit
(a) α
1e-06 0.0001 0.01  
0     
1     
10    
100   
0.56
0.63
0.7
0.77
MAP
MIR Flickr
(b) β
1e-06 0.0001 0.01  
0     
1     
10    
100   
0.68
0.7
0.72
0.74
0.78
MAP
MIR Flickr
(c) ν
100 
200 
300 
400 
500 
600 
700 
800 
900 
1000
0.74
0.75
0.76
0.77
0.78
0.79
0.8
MAP
MIR Flickr
(d) m
100 
200 
300 
400 
500 
600 
700 
800 
900 
1000
0.76
0.765
0.77
0.775
0.78
0.785
0.79
0.795
0.8
MAP
MIR Flickr
(e) a
Fig.2.5 MAP value variations with parameters of DSTDH
Fig.2.6 a Objective function
value variations with the
number of iterations.
b Training time cost variations
with the training size
0
10
20
30
40
50
60
70
80
90
100
#iter
1300
1400
1500
1600
1700
1800
1900
2000
Objective Value
MIR Flickr on 32 bits
(a)
Training Size
6
8
10
12
14
16
18
20
22
Training Time (seconds)
MIR Flickr on 32 bits
(b)
2.4
Two-Pronged Strategy: Lightweight Augmented Graph
Network Hashing
2.4.1
Motivation
Pioneering hashing methods are based on shallow learning models. They generally achieve
sub-optimal performance, as they separate the feature representation and hash code learning
into two independent processes [9, 10, 13–15, 19, 21, 46]. With the success of deep learning
and representation learning, recent hashing methods have moved towards deep hashing
which simultaneously learns feature representations and hash functions within a uniﬁed
deep neural network. Supervised deep hashing methods [5–8] train the deep hashing models
on labeled data and preserve the semantics from explicit labels into the hash codes. They
rely on a large amount of labeled data, which impedes the scalability of supervised hashing.

28
2
Context-Aware Hashing
In contrast, unsupervised deep hashing methods [11, 12, 33, 47] learn hash functions and
hash codes without this reliance, and thus they support scalable image retrieval well.
Initially, unsupervised deep hashing methods usually enforce several important criteria
on the hash layer of the deep neural network to learn the hash functions [12, 22]. Since
they fail to preserve the semantic similarities between images into the hash codes during the
hash learning process, their performance is far from satisfactory. To address this problem,
several approaches use image reconstruction strategy as an indirect way to preserve the
similarities of images, such as BGAN [49] and TBH [25]. Besides, several approaches are
proposed to construct the semantic similarity structures based on the image features as the
self-supervised information to guide the hash model training, such as SSDH [23], SADH
[24], and DU3H [50]. Nevertheless, unsupervised deep hashing is very difﬁcult to optimize
and it suffers from low training efﬁciency, as a large number of parameters is involved
in the deep neural network and no explicit semantic supervision can be provided. How to
reduce the number of parameters to be optimized and simultaneously enhance the semantic
representation capability of deep networks are two important problems that need urgent
solutions.
We propose a Lightweight Augmented Graph Network Hashing (LAGNH) method to
address the problems with an effective two-pronged strategy. Our model takes full advan-
tage of the free auxiliary semantics extracted from the inner structure of the image to improve
both the semantic representation capability of binary hash codes and model training efﬁ-
ciency. For one thing, we extract the auxiliary semantics and employ them as the supervision
to guide the training process of hash learning, which overcomes the shortage of semantic
supervision in unsupervised deep hashing. In particular, we ﬁrst design a cross-modal atten-
tion module with the auxiliary semantic information to adaptively mitigate the noise in the
deep image features. Then, the attentive image features and the auxiliary semantics are used
together to construct the augmented semantic graph, which captures the intrinsic semantic
relations of images. Afterwards, the hash codes are learned by an adversarial regularized
graphconvolutionalnetworkbasedontheattentiveimagefeaturesandtheaugmentedseman-
tic graph. For another, with the assistance of the auxiliary semantics, a lightweight attention
module and a graph convolutional network are constructed, which avoids the over-abundant
parameters in deep networks. Moreover, we reconstruct a similarity graph to further transfer
the captured semantic relations of images into the hash codes. Via this two-pronged strat-
egy, the retrieval accuracy and training efﬁciency of our unsupervised deep hashing can be
improved simultaneously.
2.4.2
Methodology
Assuming that a training set contains n images, our goal is to learn the hash functions which
project the input images into compact binary codes B = [b1, . . . , bn] ∈{−1, 1}r×n, where
r is the length of the hash code. We denote the deep image features as X = [x1, . . . , xn] ∈

2.4
Two-Pronged Strategy:Lightweight Augmented Graph Network Hashing
29
Rd×n, where d is the dimensionality of each image vector. The auxiliary semantic informa-
tion extracted from the inner structure of the image is denoted as Y =

y1, . . . , yn

∈Rc×n,
where c is the number of semantic categories. yi is a binary-valued vector, in which yi j = 1
indicates that the jth semantic label is associated with the ith image, and yi j = 0 otherwise.
Framework Overview
Figure2.7 shows the basic learning framework of LAGNH. It mainly consists of four com-
ponents:
• Preparation. The deep image features and auxiliary semantic information are extracted
by deep learning models.
• Cross-modal Attention Denoising. Identifying the important parts of the image features
that are highlighted by the auxiliary semantic information and mitigating the adverse
effects in the features.
• Augmented Semantic Graph Construction. Integrating the attentive image features and
auxiliary semantics to construct the augmented semantic graph for preserving semantic
relations.
• Hash Code Learning. Learning semantically enhanced hash codes by an adversarial
regularized graph convolution network and a similarity graph reconstruction strategy.
Below, we elaborate our method from the aspects of cross-modal attention denoising, aug-
mented semantic graph construction, and hash learning based on the adversarial regularized
graph convolutional network.
Fig. 2.7 The basic learning framework of the proposed LAGNH method. We ﬁrst extract the deep
image features with the pre-trained VGG16 model [36] and detect the auxiliary semantics from
the inner structure of the image with Faster R-CNN [48]. Then, a cross-modal attention module is
designed to identify the important parts of the images that are highlighted by the auxiliary semantic
information. The attentive image features and auxiliary semantics are integrated together to construct
an augmented semantic graph. Afterwards, an adversarial regularized graph convolutional network
is proposed to learn the hash codes by reconstructing the semantic relations of images

30
2
Context-Aware Hashing
Cross-Modal Attention Denoising
Deep hashing takes advantage of the representation capability of deep neural networks to
obtain features that can capture the global visual information in images. However, the deep
image features contain noise, which needs to be removed to alleviate the adverse effects on
hashing performance. Since each image is more semantically relevant to speciﬁc auxiliary
semantic information, we develop a cross-modal attention module to identify the important
parts of the image that are highlighted by the auxiliary semantic information. Speciﬁcally,
for each image feature xi in the training set, we calculate the cosine similarities with all
the auxiliary semantics

y j
n
j=1 to get the attention score matrix Aatt ∈Rn×n. Each item
in Aatt is calculated as below:
αi j =

cos(¯xi, ¯y j)

+ ,
(2.25)
where ¯xi ∈Rd′ and ¯y j ∈Rd′ are the linearly transformed representation of xi and y j,
respectively. cos(·, ·) returns the cosine value of the input vectors and the operator [α]+ =
max (α, 0). Then the attentive feature of xi is calculated by adding the weighted sum of the
auxiliary semantic information to the linearly transformed representation ¯xi, namely,
xatt
i
=
n
j=1 αi j ¯y j
n
j=1 αi j
+ ¯xi.
(2.26)
Augmented Semantic Graph Construction
To enhance the semantic representation capability of the deep network, we jointly consider
the deep image features and the auxiliary semantic information. The visual similarity based
on the attentive deep image features can capture the relations between images at a coarse-
grained level. The similarity based on auxiliary semantics extracts the relations from the
inner structures of the images, and it can discover the ﬁne-grained semantic association.
Thus, the visual similarity and the auxiliary semantic similarity are complementary to each
other. By integrating the two forms of similarities together, we construct the augmented
semantic graph as the supervision to guide our hash model training.
We use the Gaussian kernel function to calculate the visual similarity graph. For any
two images, such as image i and image j, the visual similarity can be described with the
following formula:
Sv
i j = exp
⎛
⎜⎝
−
xatt
i
−xatt
j

2
2σ2
⎞
⎟⎠,
(2.27)
where σ is the kernel bandwidth.
Following the previous studies [51, 52], the inner product of yi and y j is regarded as the
auxiliary similarity between image i and j, that is
Sa
i j = (yi)T y j.
(2.28)

2.4
Two-Pronged Strategy:Lightweight Augmented Graph Network Hashing
31
Ultimately, we get the augmented semantic graph S which employs the information of
both image features and auxiliary semantics as follows:
S = μSv + Sa,
(2.29)
where μ is the fusion factor to balance the importance of each similarity structure. At this
point, the intrinsic semantic relations of images are captured and modeled.
Hash Code Learning
The goal of hashing is to project the images into binary hash codes while preserving their
semantic similarities. Recent studies have demonstrated that Graph Convolutional Networks
(GCNs) are capable of generating similarity-preserving node representations based on the
node features and topological structure of the graph [53–55]. In this work, with the semantic
assistance of auxiliary semantics, we introduce a lightweight two-layer GCN to fuse the fea-
ture of the node in the augmented semantic graph S with its neighbor nodes. The propagation
rules of each graph convolutional layer can be represented as
Z(1) = σ1

W(1)Xatt ˜S

,
(2.30)
Z(2) = σ2

W(2)Z(1) ˜S

.
(2.31)
˜S = D−1/2SD−1/2 ∈Rn×n is the normalized augmented semantic graph, where Dii =

j Si j. W(1) and W(2) are the matrices of ﬁlter parameters, which need to be learned
during training. σl (·) denotes the activation function of the lth layer. Z(2) represents the
outputs of our GCN. We replace it with Z for simplicity.
In essence, Z aggregates the semantic relationships from the graph structure and node
features. Based on it, we try to minimize the gap between the hash codes and the output
node representations of the GCN, that is
min Lquan = ∥B −Z∥2
F .
(2.32)
After that, instead of reconstructing the original images, we heuristically employ the
cosine similarity of the outputs of the GCN to reconstruct the auxiliary similarity graph. The
reason why we reconstruct the auxiliary similarity graph instead of the augmented semantic
graph will be explained in the experiments section. We formulate this part as
min Lrecons =
kSa −[cos(ZT, Z)]+
2
F,
(2.33)
where k is the hyper-parameter to make the reconstruction more ﬂexible. The new represen-
tations can be viewed as the continuous representations of the hash codes, so the similarities
between images are transferred into the hash codes. Equation(2.33) works as follows: the
cosine similarities between the node representations can reﬂect the angular relations of hash

32
2
Context-Aware Hashing
codes in Hamming space, and these angular relations are equivalent to the Hamming distance
of hash codes.
In addition, we enforce the data distribution of the new representations to match the real
data distribution. This part is implemented by Generative Adversarial Nets (GANs) [56].
A discriminator is designed to distinguish whether a representation is from the real data
distribution (positive) or from the outputs of the GCN (negative), and our GCN is regarded
as the generator. The objective function of the adversarial training strategy is formulated as
min
G max
D LG AN = Ez′∼pz′ [log D(Z′)]
+ Ez∼pz[log(1 −D(G(Z)))],
(2.34)
where G (·) and D (·) indicate the generator and discriminator respectively, z′ is sampled
from the real data distribution. In our work, we use simple Gaussian distribution as p(z′).
This training scheme will regularize the outputs of the GCN to be more robust graph repre-
sentations.
Furthermore, as we have obtained the auxiliary semantic information, it is natural to
consider using it to directly guide our hash function learning, which can be represented as
min
Z Lcl =
n

i=1
yi log(δ(zi)) + (1 −yi) log(1 −δ(zi)),
(2.35)
where δ(zi) =
1
1+e−zi .
By combining Eqs.(2.32)–(2.35), the overall objective function of our hash learning
framework is given as
L = LG AN + λ1Lrecons + λ2Lquan + λ3Lcl,
(2.36)
where λ1, λ2, λ3 are tradeoff parameters.
We summarize the network conﬁguration of our hash code learning module in Table2.7,
which contains a two-layer GCN and a three-layer discriminator. As can be seen from the
table, our hash learning model has a simple network structure and only few parameters, so
it is easy to be optimized and thus can accelerate the training process. We will demonstrate
the advantage of our proposed method on training efﬁciency in Sect.2.4.3.
After several iterative optimizations, our model is fully trained and the optimal network
parameters are obtained. For any query image q, we can apply its auxiliary semantic infor-
mation and the trained model parameters to generate the binary code. Speciﬁcally, we ﬁrst
get the low-dimensional representation zq of our GCN by forward propagation. Then, the
hash code bq is obtained by a simple quantization, that is
bq = sgn(zq).
(2.37)

2.4
Two-Pronged Strategy:Lightweight Augmented Graph Network Hashing
33
Table 2.7 Network conﬁguration of the proposed hash code learning module, where d′ denotes the
dimensionality of ¯x or ¯y , r denotes the length of hash codes. Note that we set the output dimension
of GC Layer 1 to 1024 on MS COCO and 2048 on NUS-WIDE, respectively
Layer
Conﬁguration
Activation
Graph convolutional network (Generator)
GC Layer 1
d′× 1024/2048
ReLU
GC Layer 2
1024/2048 ×r
–
Discriminator
FC1
r×64
ReLU
FC2
64×32
ReLU
FC3
32×1
–
sgn(·) denotes the element-wise sign function which returns 1 if the element is positive and
returns −1 otherwise.
2.4.3
Experiment
Datasets and Evaluation Metrics
We conduct extensive experiments to evaluate the performance of the proposed method on
two public image retrieval datasets. These two datasets have been widely evaluated in recent
unsupervised deep hashing methods [24, 25, 49].
• MS COCO [57] is a large-scale dataset provided by the Microsoft team for object detec-
tion, segmentation, and captioning. It contains 118,287 training images and 5,000 vali-
dation images, each of which is associated with at least one of 80 object categories. In
our experiments, we randomly select 5,000 images from the provided training data as
our training set, and the remaining images as our retrieval set. The provided validation
data is directly used as our query set.
• NUS-WIDE [40] is the largest publicly available dataset used for evaluating image
retrieval performance. Each image in the dataset is associated with user-provided tags.
We ﬁrst select images that fall into the 21 most common tags. Then, we remove the images
that do not belong to the 21 most common categories and obtain 145,733 experimental
images. We randomly select 200 images from each category, and ﬁnally obtain the query
set with 3,156 images after removing the duplicate ones. For the remaining images, 5,000
images are selected as the training set, and 137,577 images are selected as the retrieval
set. Table2.8 summarizes the basic statistics of the datasets in our experiments.

34
2
Context-Aware Hashing
Table 2.8 Statistics of the experimental data
Datasets
MS COCO
NUS-WIDE
#Categories
80
21
#Training
5,000
5,000
#Query
5,000
3,156
#Retrieval
113,287
137,577
Following the settings as existing hashing methods [31, 58, 59], we use Mean Average
Precision (MAP) and topK-Precision curve as the evaluation metrics in our experiments.
Since both of the above testing datasets are multi-labeled, two images are deemed as seman-
tically similar if they share at least one label (category). For the evaluation metrics, higher
values indicate better outcomes. Note that the label information is only used for evaluating
the retrieval performance, and it is not used during the training process.
Implementation Details
Our LAGNH method is implemented on Pytorch1 platform (an open source machine learning
framework). All the experiments are performed on a workstation with one NVIDIA GeForce
GTX 1080Ti GPU and 64 GB memory. Our network is trained by the Adam optimizer [60],
and the learning rate is set to 0.0001 in optimization. The batchsize is set to 1024 for MS
COCO and 2048 for NUS-WIDE at the training stage.
We use the VGG16 model [36] pre-trained on ImageNet dataset [61] as our feature
extractor. The 4,096-dimensional deep image features are acquired from the last fully con-
nected layer (FC7) of VGG16. The computer vision model Faster R-CNN [48] is used to
parse the inner structures of the images, where the identiﬁed objects with high prediction
scores are taken as the auxiliary semantic information of MS COCO. As Faster R-CNN is
not applicable to NUS-WIDE, we employ the available user-provided tags directly as an
alternative.
The hyper-parameters are determined by cross-validation and are ﬁnally set as {λ1 = 100,
λ2 = 1, λ3 = 1, k = 1, μ = 1} and {λ1 = 100, λ2 = 1, λ3 = 0.1, k = 1, μ = 1} on MS
COCO and NUS-WIDE, respectively. The whole hash learning framework is optimized for
300 epochs in total.
Baseline Methods
We compare our method with several representative unsupervised hashing methods, includ-
ing LSH [43], SKLSH [44], SH [18], PCAH [20], ITQ [9], SGH [10], UH-BDNN [11],
DeepBit [22], SSDH [23], SADH [24] and TBH [25]. The former six are shallow hashing
methods while the latter ﬁve and ours are deep hashing methods. For fair comparisons, the
shallow hashing methods and UH-BDNN abandon hand-crafted features and also use deep
features extracted by VGG16 model for the experiments. The source codes of all the baseline
1 https://pytorch.org/.

2.4
Two-Pronged Strategy:Lightweight Augmented Graph Network Hashing
35
methods are provided by their authors. We carefully tune the parameters of the models to ﬁt
our datasets and report their best results for comparison.
Retrieval Accuracy Comparison
We compare our method with all the baseline methods to evaluate the retrieval accuracy. The
MAP@1000 values and topK-Precision curves with the hash code length varying from 16
to 128 on MS COCO and NUS-WIDE are presented in Table2.9 and Fig.2.8, respectively.
Based on these results, we have the following observations:
(1) The MAP values of our LAGNH method consistently outperform those of the baseline
methods on all hash code lengths of both datasets. Particularly on MS COCO dataset,
our LAGNH has obvious advantages. The MAP values of different hash code lengths in
our method are 0.8476, 0.8655, 0.8718, 0.8751, while the best results in the baselines are
0.6942,0.7605,0.7981,0.8161,respectively.Ourmethodimprovestheretrievalperformance
by more than 5–14% compared with the second best results. On NUS-WIDE dataset, our
method also has 2–3% improvement of retrieval performance compared with the second best
results on four hash code lengths. The results in Table2.9 clearly illustrate the effectiveness
of our LAGNH on image retrieval. The superior performance of the proposed method is
Table 2.9 MAP@1000 results on MS COCO and NUS-WIDE
Methods
Reference
MS COCO
NUS-WIDE
16 bits
32 bits
64 bits
128 bits
16 bits
32 bits
64 bits
128 bits
VGG+LSH
[43]
VLDB99
0.4548
0.5474
0.6310
0.7149
0.4777
0.5314
0.6066
0.6761
VGG+SKLSH
[44]
NeurIPS09
0.3847
0.4622
0.5016
0.6029
0.4250
0.4632
0.5146
0.5633
VGG+SH [18]
NeurIPS08
0.6654
0.7055
0.7321
0.7482
0.6249
0.6393
0.6667
0.6807
VGG+PCAH
[20]
TPAMI12
0.6619
0.7054
0.7251
0.7300
0.6338
0.6490
0.6615
0.6774
VGG+ITQ [9]
TPAMI13
0.6920
0.7607
0.7895
0.8067
0.6705
0.6976
0.7218
0.7368
VGG+SGH
[10]
IJCAI15
0.6769
0.7416
0.7716
0.7929
0.6576
0.6833
0.7083
0.7250
VGG+UH-
BDNN
[11]
ECCV16
0.6866
0.7472
0.7430
0.7789
0.6678
0.6845
0.6770
0.7037
Deepbit [22]
CVPR16
0.4649
0.5360
0.6346
0.7120
0.4552
0.5464
0.6032
0.6733
SSDH [23]
IJCAI18
0.4353
0.4667
0.4619
0.5319
0.5303
0.5339
0.5352
0.5311
SADH [24]
TPAMI18
0.6728
0.7605
0.7981
0.8161
0.6506
0.6892
0.7215
0.7436
TBH [25]
CVPR20
0.6942
0.7134
0.7299
0.6912
0.6560
0.6780
0.6791
0.6981
Ours
(LAGNH)
Proposed
0.8476
0.8655
0.8718
0.8751
0.7061
0.7242
0.7530
0.7644

36
2
Context-Aware Hashing
Fig.2.8 The topK-Precision curves on MS COCO and NUS-WIDE
attributed to the reason that our approach signiﬁcantly reduces the number of parameters to
be optimized and exploits the auxiliary semantics to enhance the representation capability
of deep networks.
(2) We ﬁnd that promising MAP results can be obtained by shallow methods using
deep features as the model input. The results show that they achieve comparable or better
performance than the deep methods. For example, on NUS-WIDE, ITQ with deep features
obtains the MAP of 0.6705, 0.6976, and 0.7218 when the hash code length is 16 bits, 32
bits, and 64 bits respectively, which are the second best results. This phenomenon indicates
that the deep neural network has powerful representation capability which can be easily
transferred to other models. Also, these results demonstrate that the existing unsupervised
deep hashing methods have not exerted the powerful representation capability of the deep
model yet. This might be due to the fact that the large amount of parameters in deep neural
network models cannot be optimized well without proper semantic supervision under the
existing unsupervised deep hash learning framework.
(3) From the topK-Precision curves in Fig.2.8, it can be obviously observed that with the
increasing number of retrieved images, the retrieval accuracy of our LAGNH consistently
outperforms other baselines on both datasets. These ﬁgures demonstrate similar trends as
the MAP results in Table2.9.
(4) We note that Deepbit and SSDH do not perform satisfactorily in our experiments. The
reason for DeepBit is that it only enforces three objectives (rotation invariant, quantization
loss minimization, and evenly distribution) on the hash layer of the network, without con-
sidering the similarity preserving. As for SSDH, the potential reason is that the assumption
that the similarity distribution of the sample points is two half-Gaussian distributions does
not apply to all datasets.
Run Time Comparison
To demonstrate the advantage of our lightweight network architecture, we conduct experi-
ments to compare the training and encoding time between our LAGNH and the deep baseline

2.4
Two-Pronged Strategy:Lightweight Augmented Graph Network Hashing
37
Table 2.10 Training and encoding time on MS COCO and NUS-WIDE with 32 bits. The results are
reported in seconds
Methods
Training time (s)
Encoding time (s)
MS COCO
UH-BDNN [11]
6301.7021
3.9999
Deepbit [22]
1802.0045
1509.9155
SSDH [23]
1263.5262
4618.4699
SADH [24]
3253.1953
1478.9802
TBH [25]
4938.3519
3.2057
Ours
466.3686
9.8646
NUS-WIDE
UH-BDNN [11]
6324.5006
4.7136
Deepbit [22]
1858.2623
1801.1218
SSDH [23]
877.389
2630.8497
SADH [24]
3453.3845
1778.8724
TBH [25]
4945.0841
3.266
Ours
586.2117
11.6804
methods. Table2.10 presents the comparison results on MS COCO and NUS-WIDE when
the code length is ﬁxed to 32 bits. From the experimental results, we can observe that the
advantages of our method on training efﬁciency are obvious. The training time of LAGNH
is 466.3686s on MS COCO, and 586.2117s on NUS-WIDE. They are at least 8.4x faster
than UH-BDNN and TBH, and 5.8x faster than SADH. The encoding time of our method is
similar to those of UH-BDNN and TBH, and is much faster than Deepbit, SSDH and SADH.
The improved efﬁciency of our LAGNH is mainly because that, we design the lightweight
attention module and adversarial regularized graph convolutional network with the assis-
tance of auxiliary semantics, which greatly reduces the network parameters to be optimized
at the training phase and the nonlinear transformations to be carried out at the testing phase.
In addition, we conduct experiments to verify the impact of the training epochs on the
performance of LAGNH. We report the MAP values for different epochs on MS COCO and
NUSWIDE when the code length is ﬁxed to 32 bits. The number of epochs starts at 50 and
gradually increases from 100 to 1000 with a step size of 100. From Fig.2.9, we can see that
our method can achieve satisfactory performance with fewer epochs (about 300 epochs) on
both datasets. These experimental results show that our LAGNH method can increase the
MAP value quickly, and thus has the advantage on training efﬁciency.
Model Analysis
(1) Discussion of different model components. To verify the effectiveness of different
components in our model, we design several variants for comparison:

38
2
Context-Aware Hashing
Fig.2.9 Variations of MAP value with the number of training epochs
Table 2.11 MAP comparison results of the variants of our approach on MS COCO and NUS-WIDE
Methods
MS COCO
NUS-WIDE
16 bits
32 bits
64 bits
128 bits
16 bits
32 bits
64 bits
128 bits
LAGNH-no-aux
0.6163
0.5704
0.5550
0.5726
0.5302
0.4979
0.3780
0.5435
LAGNH-no-att
0.8424
0.8631
0.8681
0.8746
0.7017
0.7190
0.7490
0.7581
LAGNH-only-Sv
0.7279
0.7578
0.7539
0.7672
0.5774
0.5677
0.5899
0.6193
LAGNH-only-Sa
0.7961
0.8121
0.8192
0.8156
0.5981
0.5962
0.6075
0.6068
LAGNH-recons-ZTZ
0.7927
0.8210
0.8407
0.8514
0.5443
0.5192
0.5956
0.5850
LAGNH-recons-feat
0.7690
0.7955
0.8232
0.8399
0.6747
0.6872
0.7374
0.7501
LAGNH-recons-S
0.7312
0.7317
0.7329
0.7586
0.7011
0.7226
0.7503
0.7560
LAGNH-recons-Sv
0.7402
0.7079
0.7019
0.7080
0.6946
0.7303
0.7515
0.7539
LAGNH-recons-Sa(Ours)
0.8476
0.8655
0.8718
0.8751
0.7061
0.7242
0.7530
0.7644
• LAGNH-no-aux: removes the auxiliary semantic information in the whole framework.
• LAGNH-no-att: removes the attention component and uses deep image features and
auxiliary semantic information directly to construct the augmented semantic graph.
• LAGNH-only-Sv: only feeds the visual similarity graph to the GCN.
• LAGNH-only-Sa: only feeds the auxiliary similarity graph to the GCN.
The comparative results on MAP are shown in Table2.11. From the results, we ﬁnd that our
method performs better than the other four variants on two datasets with the code length
varying from 16 bits to 128 bits. Particularly, the largest MAP drop is caused when no
auxiliary semantic information is exploited for guiding the network. Using only the visual
similarity graph or the auxiliary similarity graph can also result in obvious performance
degradation. These results validate that our architecture indeed strengthens the important
information and captures the complicated relationships of images, which can help our model
to learn more robust hash codes.
(2) Discussion of similarity graph reconstruction. For reconstruction, we aim to discuss
the following three questions:

2.4
Two-Pronged Strategy:Lightweight Augmented Graph Network Hashing
39
• Q1: How about the performance of the proposed method if we directly reconstruct the
augmented semantic graph by the outputs of the GCN?
• Q2: How about the performance of the proposed method if we reconstruct the original
images?
• Q3: How about the performance of the proposed method if different similarity graphs
are reconstructed?
We design the variant approaches LAGNH-recons-ZTZ and LAGNH-recons-feat to
answer Q1 and Q2, respectively. LAGNH-recons-ZTZ minimizes the augmented seman-
tic graph and the inner products of the outputs of the GCN, it can be formulated as
min Lrecons =
kSa −ZTZ
2
F.
LAGNH-recons-feat imports the outputs of the GCN into a decoder, which generates
reconstructed image features. We minimize the errors between the original image features
and the reconstructed image features, that is
min Lrecons =
X −ˆX

2
F.
The experiments are conducted on MS COCO and NUS-WIDE with the hash code length
varied from 16 bits to 128 bits. Table2.11 presents the MAP comparison results. The results
show that neither the reconstruction of the augmented semantic graph by the inner products
of the outputs of the GCN nor the direct reconstruction of the original image features can
achieve satisfactory performance.
In addition, to verify which graph is the best choice for reconstruction, we test the
following schemes:
• LAGNH-recons-S: reconstructs the augmented semantic graph.
• LAGNH-recons-Sv: reconstructs the visual similarity graph.
• LAGNH-recons-Sa: reconstructs the auxiliary similarity graph.
The experimental results are also listed in Table2.11. We can see from the results, on
NUS-WIDE, the impacts of the reconstruction of different graph structures on performance
are relatively small, while on MS COCO, the performance changes are large. This may
be because the auxiliary semantic information of the two datasets comes from different
sources. The semantics of the detected objects from MS COCO are more accurate than
those in the tags from NUW-WIDE. In order to unify the framework, we optimize our
model by reconstructing the auxiliary similarity graph, because it can obtain relatively good
results on both datasets.

40
2
Context-Aware Hashing
Fig.2.10 MAP variations with respect to different parameters on MS COCO and NUS-WIDE on 32
bits
Parameter Analysis
We conduct a sensitivity analysis of the parameters λ1, λ2, λ3 on MS COCO and NUS-
WIDE when the hash code length is ﬁxed as 32 bits. Similar results can be found with other
code lengths. The curves in Fig.2.10 show that our method obtains relatively stable retrieval
performance when λ1 is in the range of {101, 103}, λ2 is in the range of {10−5, 1}, λ3 is in
the range of {10−5, 102} on MS COCO, and λ1 is in the range of {101, 103}, λ2 is in the
range of {10−5, 1}, λ3 is in the range of {10−5, 101} on NUS-WIDE, respectively.
2.5
Summary
In this chapter, we study the task of context-aware hashing. We ﬁrst present a Dual-level
Semantic Transfer Deep Hashing method (DSTDH) for efﬁcient social image retrieval. In
particular, DSTDH develops a weakly-supervised deep hash learning framework that simul-
taneously learns powerful image representation and discriminative hash codes by exploiting
the social tags and considering dual-level semantic transfer. Therefore, the semantics of
tags can be efﬁciently discovered and seamlessly transferred into the binary hash codes.
Moreover, DSTDH directly solves the binary hash codes by a discrete optimization strategy
to alleviate the relaxing quantization errors and thus guarantee the transfer effectiveness.
Extensive experiments on two public social image retrieval benchmarks demonstrate the
superiority of DSTDH.
In addition, we present a simple yet effective unsupervised deep hashing method, termed
Lightweight Augmented Graph Network Hashing (LAGNH) with a two-pronged strategy.
LAGNH avoids the large amount of parameters in deep hashing networks and the shortage of
semantic supervision in unsupervised hashing. Particularly, LAGNH employs the auxiliary
semantics freely extracted from the inner structure of the image as the supervision to guide
the training process of hash learning, which enhances the semantic representation capability
of the deep network. Simultaneously, LAGNH develops a lightweight network structure
with the assistance of the auxiliary semantics, which signiﬁcantly reduces the number of

References
41
parameters to be optimized and thus greatly speedups the training process. Experimental
results on two public image retrieval datasets demonstrate the superior performance of the
proposed two-pronged strategy.
References
1. Su, J. H., Huang, W. J., Philip, S. Y., & Tseng, V. S. (2011). Efﬁcient relevance feedback for
content-based image retrieval by mining user navigation patterns. IEEE Transactions on Knowl-
edge and Data Engineering, 23(3), 360–372.
2. Wu, P., Hoi, S. C. H., Zhao, P., Miao, C., & Liu, Z. Y. (2016). Online multi-modal distance
metric learning with application to image retrieval. IEEE Transactions on Knowledge and Data
Engineering,28(2), 454–467.
3. Wang, J., Liu, W., Kumar, S., & Chang, S. F. (2016). Learning to hash for indexing big data - a
survey. Proceedings of IEEE,104(1), 34–57.
4. Wang, J., Zhang, T., Sebe, N., & Shen, H. T. (2018). A survey on learning to hash. IEEE
Transactions on Pattern Analysis and Machine Intelligence,40(4), 769–790.
5. Xia, R., Pan, Y., Lai, H., Liu, C., & Yan, S. (2014). Supervised hashing for image retrieval via
image representation learning. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence
(pp. 2156–2162).
6. Lai, H., Pan, Y., Liu, Y., & Yan, S. (2015). simultaneous feature learning and hash coding with
deep neural networks. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition (pp. 3270–3278).
7. Li, W. J., Wang, S., & Kang, W. C. (2016). feature learning based deep supervised hashing with
pairwise labels . In Proceedings of the International Joint Conference on Artiﬁcial Intelligence
(pp. 1711–1717).
8. Cao, Y., Long, M., Liu, B., & Wang, J. (2018). deep cauchy hashing for hamming space retrieval.
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 1229–
1237).
9. Gong, Y., Lazebnik, S., Gordo, A., & Perronnin, F. (2013). Iterative quantization: A procrustean
approach to learning binary codes for large-scale image retrieval. IEEE Transactions on Pattern
Analysis and Machine Intelligence,35(12), 2916–2929.
10. Jiang, Q. Y., & Li, W. J. (2015). Scalable graph hashing with feature transformation. In Proceed-
ings of the International Joint Conference on Artiﬁcial Intelligence (pp. 2248–2254).
11. Do, T. T., Doan, A. D., & Cheung, N. M. (2016). Learning to hash with binary deep neural
network. In Proceedings of the European Conference on Computer Vision (pp. 219–234).
12. Lu, J., Liong, V. E., & Zhou, J. (2017).. Deep Hashing for Scalable Image Search. IEEE Trans-
actions on Image Processing,26(5), 2352–2367.
13. Liu, W., Wang, J., Ji, R., Jiang, Y. G., & Chang, S. F. (2012). Supervised Hashing with Kernels.
In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 2074–
2081).
14. Lin, G., Shen, C., & Van den Hengel, A. (2015). Supervised hashing using graph cuts and boosted
decision trees. IEEE Transactions on Pattern Analysis and Machine Intelligence,37(11), 2317–
2331.
15. Shen, F., Shen, C., Liu, W., & Tao Shen, H. (2015). Supervised discrete hashing. In Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 37–45).
16. Li, Q., Sun, Z., He, R., & Tan, T. (2017). Deep Supervised Discrete Hashing. In Proceedings of
the Advances in Neural Information Processing Systems (pp. 2482–2491).

42
2
Context-Aware Hashing
17. Yang, H. F., Lin, K., & Chen, C. S. (2018) Supervised learning of semantics-preserving hash
via deep convolutional neural networks. IEEE Transactions on Pattern Analysis and Machine
Intelligence(TPAMI),40(2), 437–451.
18. Weiss, Y., Torralba, A., & Fergus, R. (2008). Spectral hashing. In Proceedings of the Advances
in Neural Information Processing Systems (pp. 1753–1760).
19. Liu, W., Wang, J., Kumar, S., & Chang, S. F. (2011) Hashing with graphs. In Proceedings of the
International Conference on Machine Learning (pp. 1–8).
20. Wang, J., Kumar, S., & Chang, S. F. (2012). Semi-supervised hashing for large-scale search.
IEEE Transactions on Pattern Analysis and Machine Intelligence,34(12), 2393–2406.
21. Lu, X., Zheng, X., & Li, X. (2017). Latent semantic minimal hashing for image retrieval. IEEE
Transactions on Image Processing,26(1), 355–368.
22. Lin, K., Lu, J., Chen, C. S., & Zhou, J. (2016). Learning compact binary descriptors with unsu-
pervised deep neural networks. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (pp. 1183–1192).
23. Yang, E., Deng, C., Liu, T., Liu, W., & Tao, D. (2018). Semantic structure-based unsupervised
deep hashing. In Proceedings of the International Joint Conference on Artiﬁcial Intelligence (pp.
1064–1070).
24. Shen, F., Xu, Y., Liu, L., Yang, Y., Huang, Z., & Shen, H. T. (2018). Unsupervised deep hashing
with similarity-adaptive and discrete optimization. IEEE Transactions on Pattern Analysis and
Machine Intelligence,40(12), 3034–3044.
25. Shen, Y., Qin, J., Chen, J., Yu, M., Liu, L., Zhu, F., Shen, F., & Shao, L. (2020). Auto-encoding
twin-bottleneck hashing. In Proceedings of IEEE Conference on Computer Vision and Pattern
Recognition (pp. 2818–2827).
26. Guan, Z., Xie, F., Zhao, W., Wang, X., Chen, L., Zhao, W., & Peng, J. (2018). Tag-based weakly-
supervised hashing for image retrieval. In Proceedings of the International Joint Conference on
Artiﬁcial Intelligence (pp. 3776–3782).
27. Wang, M., Zhou, W., Tian, Q., & Li, H. (2022). Deep enhanced weakly-supervised hashing with
iterative tag reﬁnement. IEEE Transactions on Multimedia, 24(2022), 2779–2790.
28. Tang,J.,Li,Z.,Zhang,L.,&Huang,Q.(2015).Semantic-awarehashingforsocialimageretrieval.
In Proceedings of the ACM International Conference on Multimedia Retrieval (pp. 483–486).
29. Tang, J., & Li, Z. (2018) Weakly supervised multimodal hashing for scalable social image
retrieval. IEEE Transactions on Circuits and Systems for Video Technology,28(10), 2730–2741.
30. Gattupalli, V., Zhuo, Y., & Li, B. (2019). Weakly supervised deep image hashing through tag
embeddings.InProceedingsoftheIEEEConferenceonComputerVisionandPatternRecognition
(pp. 10375–10384).
31. Jin, L., Li, Z., Pan, Y., & Tang, J. (2020). Weakly-supervised image hashing through masked
visual-semantic graph-based reasoning. In Proceedings of the ACM International Conference on
Multimedia (pp. 916–924).
32. Cui, H., Zhu, L., Li, J., Yang, Y., & Nie, L. [n. d.]. Scalable Deep Hashing for Large-Scale Social
Image Retrieval. ([n. d.]).
33. Su, S., Zhang, C., Han, K., & Tian, Y. (2018). Greedy hash: Towards fast optimization for accurate
hash coding in CNN. In Proceedings of the Advances in Neural Information Processing Systems
(pp. 806–815).
34. Nie, F., Huang, H., Cai, X., & Ding, C. H. Q. (2010). Efﬁcient and robust feature selection via
joint l2,1-norms minimization. In Proceedings of the Advances in Neural Information Processing
Systems (pp. 1813–1821).
35. Macqueen, J. (1967). Classiﬁcation and analysis of multivariate observations. In Proceedings of
the Berkeley Symposium on Mathematical Statistics and Probability (pp. 281–297).

References
43
36. Simonyan, K., & Zisserman, A. (2015). Very deep convolutional networks for large-scale image
recognition. In Proceedings of the International Conference on Learning Representations.
37. Bertsekas, D. P. (1997). Nonlinear Programming. Journal of the Operational Research Soci-
ety,48(3), 334–334.
38. Sivic, J., & Zisserman, A. (2003). Video Google: A text retrieval approach to object matching
in videos. In Proceedings of the IEEE International Conference on Computer Vision (pp. 1470–
1477).
39. Huiskes, M. J., & Lew, M. S. (2008). The MIR Flickr retrieval evaluation. In Proceedings of the
ACM SIGMM International Conference on Multimedia Information Retrieval (pp. 39–43).
40. Chua, T. S., Tang, J., Hong, R., Li, H., Luo, Z., & Zheng, Y. (2009). NUS-WIDE: A real-
world web image database from National University of Singapore. In Proceedings of the ACM
International Conference on Image and Video Retrieval, 48(1–48), 9.
41. Hu, M., Yang, Y., Shen, F., Xie, N., Hong, R., & Shen, H. T. (2019). Collective reconstructive
embeddings for cross-modal hashing. IEEE Transactions on Image Processing, 28(6), 2770–
2784.
42. Yang, Y., Shen, F., Shen, H. T., Li, H., & Li, X. (2015). Robust discrete spectral hashing for
large-scale image semantic indexing. IEEE Transactions on Big Data, 1(4), 162–171.
43. Gionis, A., Indyk, P., & Motwani, R. (1999). Similarity search in high dimensions via hashing.
In Proceedings of the International Conference on Very Large Data Bases (pp. 518–529).
44. Raginsky,M.,&Lazebnik,S.(2009).Locality-sensitivebinarycodesfromshift-invariantkernels.
In Proceedings of the Advances in Neural Information Processing Systems (pp. 1509–1517).
45. Jia, Y., Shelhamer, E., Donahue, J., Karayev, S., Long, J., Girshick, R., Guadarrama, S., & Darrell,
T. (2014). Caffe: Convolutional architecture for fast feature embedding. In Proceedings of the
ACM International Conference on Multimedia (pp. 675–678).
46. Zhang, P., Zhang, W., Li, W. J., & Guo, M. (2014). Supervised hashing with latent factor models.
In Proceedings of the International ACM SIGIR Conference on Research and Development in
Information Retrieval (pp. 173–182).
47. Yang, E., Liu, T., Deng, C., Liu, W., & Tao, D. (2019). DistillHash: Unsupervised deep hashing
by distilling data pairs. In Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition (pp. 2946–2955).
48. Ren, S., He, K., Girshick, R. B., & Sun, J. (2017). Faster R-CNN: Towards real-time object
detection with region proposal networks. IEEE Transactions on Pattern Analysis and Machine
Intelligence, 39(6), 1137–1149.
49. Song, J., He, T., Gao, L., Xu, X., Hanjalic, A., & Shen, H. T. (2018). Binary generative adversarial
networks for image retrieval. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence
(pp. 394–401).
50. Zhang, W., Wu, D., Zhou, Y., Li, B., Wang, W., & Meng, D. (2020). Deep unsupervised hybrid-
similarityhadamardhashing.InProceedingsoftheACMInternationalConferenceonMultimedia
(pp. 3274–3282).
51. iang, Q. Y., & Li, W. J. (2017). Deep cross-modal hashing. In Proceedings of the IEEE Conference
on Computer Vision and Pattern Recognition (pp. 3232–3240).
52. Jiang, Q. Y., & Li, W. J. (2018). Asymmetric deep supervised hashing. In Proceedings of the
AAAI Conference on Artiﬁcial Intelligence (pp. 3342–3349).
53. Kipf, T. N., & Welling, M. (2017). Semi-supervised classiﬁcation with graph convolutional
networks. In Proceedings of the International Conference on Learning Representations (pp.
1–14).
54. Kipf, T. N., & Welling, M. (2016). Variational graph auto-encoders. arXiv:1611.07308.

44
2
Context-Aware Hashing
55. Pan, S., Hu, R., Long, G., Jiang, J., Yao, L., & Zhang, C. (2018). Adversarially regularized
graph autoencoder for graph embedding. In Proceedings of the International Joint Conference
on Artiﬁcial Intelligence (pp. 2609–2615).
56. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville,
A., Bengio, Y. (2014). Generative adversarial nets. In Proceedings of the Advances in Neural
Information Processing Systems (pp. 2672–2680).
57. Lin, T.-Y., Maire, M., Belongie, S. J., Hays, J., Perona, P., Ramanan, D., et al. (2014). Microsoft
COCO: Common objects in context. In Proceedings of the European Conference on Computer
Vision, 8693, 740–755.
58. Hu, Q., Wu, J., Cheng, J., Wu, L., & Lu, H. (2017). Pseudo label based unsupervised deep
discriminative hashing for image retrieval. In Proceedings of the ACM International Conference
on Multimedia (pp. 1584–1590).
59. Lu, X., Zhu, L., Cheng, Z., Li, J., Nie, X., & Zhang, H. (2019). Flexible online multi-modal
hashingforlarge-scalemultimediaretrieval.InProceedingsoftheACMInternationalConference
on Multimedia (pp. 1129–1137).
60. Kingma, D. P., & Ba, J. (2015). Adam: A method for stochastic optimization. In Proceedings of
the International Conference on Learning Representations.
61. Deng, J., Dong, W., Socher, R., Li, L. J., Li, K., & Fei-Fei, L. (2009). Imagenet: A large-scale
hierarchical image database. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (pp. 248–255).

3
Cross-Modal Hashing
3.1
Background
Cross-modal retrieval [1] aims to retrieve semantically relevant items in other modalities
given queries from a speciﬁc modality. To support large-scale cross-modal retrieval, hashing
[2] has been proposed due to its low storage cost and high retrieval speed. The main idea of
cross-modalhashingistoencodetheheterogeneousmulti-modaldataintoacommonisomor-
phic Hamming space while preserving their semantic similarities. As a promising solution,
cross-modal hashing projects semantically similar heterogeneous multi-modal data to sim-
ilar hash codes to achieve efﬁcient semantic retrieval. According to whether the semantic
label is used or not, existing cross-modal hashing methods [3–6] can be roughly divided into
supervised and unsupervised learning methods. Supervised cross-modal hashing exploits
high-quality semantic labels to accurately supervise the process of hash code learning.
Contrastively, unsupervised cross-modal hashing learns the hash codes without involving
any semantic labels during the training process, which makes it more suitable for large-
scale deployment in real-world scenarios. In this chapter, to achieve efﬁcient and accurate
cross-modal retrieval, we focus on cross-modal hashing under supervised and unsupervised
learning paradigms.
3.2
Related Work
3.2.1
Supervised Cross-Modal Hashing
Under the supervised learning paradigm, cross-modal hashing methods exploit explicit
semantic labels or precise neighbor relations to supervise the models to learn semantically
consistent hash codes between heterogeneous modalities. Initially, many shallow methods
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024
L. Zhu et al., Multi-modal Hash Learning, Synthesis Lectures on Information
Concepts, Retrieval, and Services, https://doi.org/10.1007/978-3-031-37291-9_3
45

46
3
Cross-Modal Hashing
have been proposed with graph learning or matrix factorization. Supervised Matrix Fac-
torization Hashing (SMFH) [7] adopts a Laplacian term to consider the intra-modal local
geometric consistency and inter-modal label consistency to learn uniﬁed hash codes based
on collaborative matrix factorization. Scalable disCRete mATrix faCtorization Hashing
(SCRATCH) [8] performs the collective matrix factorization [9] on the kernelized features,
and directly uses the label matrix instead of the similarity matrix to synchronously learn
the hash codes and functions. Matrix Tri-Factorization Hashing (MTFH) [10] learns two
types of unequal length hash codes for heterogeneous modality features with the pair-wise
similarity matrix. It simultaneously learns two semantic correlation matrices to correlate two
types of unequal length hash codes, and ﬁnally completes cross-modal retrieval. Asymmet-
ric Supervised Consistent and Speciﬁc Hashing (ASCSH) [11] utilizes the consistent and
modality-speciﬁc information to generate hash codes with ground-truth labels and pair-wise
similarity. ScalaBle Asymmetric discreTe Cross-modal Hashing (BATCH) [12] performs
collaborative matrix factorization to directly embed the labels into hash codes by learning a
common orthogonal semantic space and minimizing the quantization loss.
Inspired by the success of the powerful deep neural networks [13–15], many super-
vised deep cross-modal hashing methods have been proposed. Deep Cross-Modal Hash-
ing (DCMH) [4] proposes an end-to-end deep learning framework to generate discrete
hash codes without relaxation. Deep Multiscale Fusion Hashing (DMFH) [16] aggregates
the multi-scale semantics by applying multi-scale fusion modules for each modality, and
embedding them into the ﬁnal hash codes. Consistency-Preserving Adversarial Hashing
(CPAH) [17] proposes a consistency reﬁned module to generate the common and private
representations of different modality features, then, it retains the consistency between the
common representations of different modalities through the adversarial learning module
[18] to generate the compact hash codes. Graph Convolutional Hashing (GCH) [19] designs
a semantic encoder learned from the ground-truth labels to guide the feature encoding
network and learn discriminative features. It uses Graph Convolutional Network (GCN)
[20, 21] to embed the inherent similarities between image-text pairs into the hash codes.
Deep Cross-Modal Hashing with Hashing Functions and Uniﬁed Hash Codes Jointly Learn-
ing (DCHUC) [22] proposes an end-to-end deep cross-modal learning framework to jointly
learn hash codes and functions. The learned hash codes and functions can supervise each
other during the optimization process.
3.2.2
Unsupervised Cross-Modal Hashing
Although supervised cross-modal hashing achieves promising retrieval performance with
explicit semantic labels, obtaining true and correct labels is very expensive and time-
consuming in real applications. Contrastively, unsupervised cross-modal hashing is not
constrained with such requirement. By extending spectral hashing [23] to the cross-modal
domain, Cross-View Hashing (CVH) [24] minimizes the weighted Hamming distance

3.3
Efficient Hierarchical Message Aggregation Hashing
47
between image-text pairs to generate hash codes and solves generalized eigenvalue prob-
lems to obtain hash functions. Inter-Media Hashing (IMH) [25] ﬁrst constructs two afﬁnity
matrices through nearest neighbors, and then explores intra- and inter-modal consistencies
to generate compact hash codes. Latent Semantic Sparse Hashing (LSSH) [3] uses sparse
coding and matrix decomposition to learn image and text features, respectively. Then, it
maps those features to a joint space to generate hash codes. Collective Matrix Factorization
Hashing (CMFH) [26] uses collaborative matrix factorization technology to jointly learn
uniﬁed hash codes and hash functions. Semantic-Rebased Cross-modal Hashing (SRCH)
[27] initializes a graph structure through the intra-modal geometric relation and alternately
updates the edges of the graph according to the hashing results. Finally, it preserves the
similarity information of the graph into the hash codes.
Unsupervised deep cross-modal hashing methods have been proposed to capture more
deeper intra-modal and inter-modal semantic information with the advanced deep network
models. Deep Binary ReConstruction (DBRC) model [28] completes heterogeneous modal
relation modeling and hash code learning by the proposed binary reconstruction model.
Unsupervised Deep Cross-Modal Hashing (UDCMH) [29] integrates matrix factorization
and deep learning module into a uniﬁed framework to learn semantic hash codes and func-
tions. Deep Joint-Semantics Reconstructing Hashing (DJSRH) [5] proposes a joint seman-
tic relation matrix by integrating similarity matrices in different modalities, and preserves
semantic information by exploiting the hash codes to reconstruct the joint matrix. Joint-
modal Distribution-based Similarity Hashing (JDSH) [30] proposes a joint modal similarity
matrix to retain the original semantic relevance, and adopts a sampling and weighting strat-
egy to generate hash codes. Aggregation-based Graph Convolutional Hashing (AGCH) [6]
constructs a similarity matrix by aggregating multi-modal similarity matrices generated by
different similarity measurement methods, and uses GCN to preserve the semantic structure
in the hash codes.
3.3
Efficient Hierarchical Message Aggregation Hashing
3.3.1
Motivation
Although great success has been achieved so far, two key problems have not been solved
well yet by existing deep cross-modal hashing methods: (1) With advanced neural network
models, how to seek the multi-modal alignment space which can effectively model the
intrinsic multi-modal correlations and reduce the heterogeneous modality gaps. To address
this problem, most existing methods generally adopt the mandatory modality alignment
between heterogeneous modalities to construct the shared cross-modal space. However,
this strategy will inevitably lead to important information loss of each modality and thus
reduce the cross-modal retrieval performance. (2) How to effectively and efﬁciently preserve
the modelled multi-modal correlations into the binary hash codes under the deep learning

48
3
Cross-Modal Hashing
paradigm. To solve this problem, most existing methods impose pairwise reconstruction
semantic loss on the ﬁnal hash code generation layer to learn the semantic-preserving hash
codes with the supervision of a pre-calculated semantic afﬁnity matrix. It cannot preserve the
ﬁne-grained semantics into the binary hash codes because of the discreteness of the semantic
matrix calculated by discrete binary label annotations. Besides, most existing methods need
to ﬁnetune the pre-trained backbone network to generate the binary hash codes, which brings
considerable training cost.
To solve the above problems, we propose a Hierarchical Message Aggregation Hash-
ing (HMAH) method for cross-modal retrieval within an efﬁcient teacher-student learning
framework. On the teacher end, we develop hierarchical message aggregation networks to
construct a multi-modal complementary space by performing the semantic message aggre-
gation hierarchically across different modalities, which can better align the heterogeneous
modalities and model the ﬁne-grained multi-modal correlations. On the student end, with
the correlation-preserved outputs from teacher module, we learn a couple of hash functions
to support cross-modal retrieval. We design a cross-modal correlation knowledge distillation
strategy to supervise the training process of the student modules. The continuous real-valued
outputs from the teacher module can provide the ﬁne-grained semantic supervision knowl-
edge and thus enhance the semantic representation capability of hash functions. Moreover,
the training procedure of the whole framework does not need to ﬁnetune the pre-trained
deep neural network models and it is more computationally efﬁcient.
3.3.2
Methodology
Notation
In this section, we ﬁrst give the notation deﬁnitions. We use boldface uppercase letters, e.g.,
W, to represent matrices and boldface lowercase letters, e.g., w, to indicate vectors, respec-
tively. || · ||F denotes the Frobenius norm. Suppose we have a training dataset O = {oi}N
i=1
containing two different modalities, e.g., images and texts, denoted as X = {xi|xi ∈R1×dx }
and Y = {yi|yi ∈R1×dy}, where N is the number of training instances and d∗, ∗= {x, y},
denotes the dimension of the corresponding modality features. For the ith paired instance,
oi = {xi, yi, li}, li ∈{0, 1}1×c represents the corresponding semantic labels and li j = 1 if
oi belongs to the j-th class, otherwise li j = 0. We project the original coupled cross-modal
data into Hamming representation space and compute the binary codes B ∈{−1, 1}N×k,
with k being the length of hash code. It is worth noting that the following descriptions are
based on a mini-batch, i.e., X ∈RNb×dx and Y ∈RNb×dy, where Nb is the mini-batch size.
Figure3.1 illustrates the basic teacher-student learning framework. Thereafter, we describe
the details of each module in subsequent sections.
Hierarchical Message Aggregation Networks
On the teacher end, we ﬁrst build a couple of feature encoding networks (ImageNet and
T ext Net) to project the extracted features into dimensionally consistent spaces as follows:

3.3
Efficient Hierarchical Message Aggregation Hashing
49
Images
CNN-F
Texts
ImageNet
TextNet
BoW
Discriminator
Real/Fake
Concat
(0,1)k
c
R
N(0,1)k
c
-1
1
-1
1
1
-1
-1
1
-1
1
-1
1
1
-1
-1
1
Complementary 
Space
...
...
Hierarchical Message 
Aggregaon Networks
Cross-Modal Correlaon 
Knowledge Disllaon
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
...
Image Correlaon
Text Correlaon
Feature Extractor
Teacher Module
Student Modules
IM-MAN
IM-MAN
CM-MAN
K-bits
Fusion
Binary 
Constraint
Fig. 3.1 The basic teacher-student learning framework of our proposed HMAH. It is composed
of three parts: feature extractor, teacher module and student modules. Speciﬁcally, we ﬁrst extract
semantic-rich deep features by the pre-trained models and build a couple of feature encoding networks
to project the extracted deep features into the representation spaces with consistent dimensions. Then,
on the teacher end, we develop hierarchical message aggregation networks to construct a multi-modal
complementary space that captures the multi-modal correlations and alleviates the heterogeneous
modality gaps. On the student end, we train a couple of student modules that learn the hash functions to
support the cross-modal retrieval. Finally, we design a cross-modal correlation knowledge distillation
strategy to seamlessly transfer the learned ﬁne-grained cross-modal semantic correlation knowledge
from the teacher module to lightweight student modules. ‘IM-MAN’ denotes intra-modal message
aggregation network and ‘CM-MAN’ denotes cross-modal message aggregation network
Fx = ImageNet(X; θx), Fy = T ext Net(Y; θy),
(3.1)
where θx and θy denote the trainable parameters of the corresponding feature encoding
networks, Fx, Fy ∈RNb×dcom are the network outputs.
Most existing methods adopt mandatory modality alignment constraints to learn a shared
cross-modal space, which inevitably results in the loss of important semantic information.
To avoid this problem, in this work, we construct a multi-modal complementary space with
hierarchical message aggregation networks to align the heterogeneous modalities and model
the intrinsic multi-modal correlations. Speciﬁcally, our model can hierarchically aggregate
the semantic messages from different heterogeneous modalities as shown in Fig.3.2. We
ﬁrst design two Intra-Modal Message Aggregation Networks (IM-MAN) and one Cross-
Modal Message Aggregation Network (CM-MAN) to capture intra-modal and inter-modal
correlations respectively.
Speciﬁcally, IM-MAN is fed with F∗in Eq.(3.1), ∗= {x, y}, and similarity matrix S∗∈
RNb×Nb. The propagation rule of each layer in our IM-MAN can be written as follows:
H(g+1)
∗
= σ(g)
∗(˜S∗H(g)
∗W(g)
∗),
(3.2)

50
3
Cross-Modal Hashing
Text
Image Node
Text Node
Intra-modal Message 
Aggregaon
Inter-modal Message 
Aggregaon
Image Self-Connecon
Text Self-Connecon
Conneon
Image
Fig. 3.2 Hierarchical Message Aggregation Networks. The ﬁgure shows intra- and cross- modal
message aggregation
where H(g)
∗
∈RNb×dg
∗and H(g+1)
∗
∈RNb×dg+1
∗
represent the input and the output features of
the gth layer in IM-MAN, respectively. ˜S∗= D
−1
2
∗S∗D
−1
2
∗
denotes the normalized similarity
matrix and D∗ii = 
j S∗i j. W(g)
∗
∈Rdg
∗×dg+1
∗
denotes convolutional ﬁlters of the gth layer
in IM-MAN. σ(g)
∗(·) is an activation function, e.g., ReLU(·). Speciﬁcally, H(0)
∗
= {Fx, Fy}.
Under the supervised learning paradigm, Sx = Sy = S, where S indicates the multi-label
similarity matrix:
Si j =
2
1 + e−li·lT
j
−1,
(3.3)
whichcontainsﬁne-grainedassociationsbetweenrelevantsamplesandSi j ∈[0,
2
1+e−c −1].
For CM-MAN, we introduce a hierarchical similarity matrix Sc as follows:
Sc =
 I
Si2t
St2i
I

,
(3.4)
where Si2t and St2i are two cross-modal similarity matrices. Si2t(i j) denotes the similarity
between the ith image sample and the jth text sample. In particular, we have Si2t = St2i = S
in the supervised learning scenario. I ∈RNb×Nb is an identity matrix. The propagation rule
of CM-MAN can be written as follows:

H(g+1)
xc
H(g+1)
yc

= σ(g)
c

˜Sc

H(g)
xc
H(g)
yc

W(g)
c

,
(3.5)
where ˜Sc is the normalized similarity matrix of Sc and W(g)
c
serves as convolutional ﬁlters
of the gth layer in CM-MAN. H(g)
xc and H(g+1)
xc
denote the image-partial inputs and outputs
of the gth layer in CM-MAN. We concatenate the latent representations of image and text

3.3
Efficient Hierarchical Message Aggregation Hashing
51
modalities in row-wise to obtain the aggregated messages, written as

H(g)
xc
H(g)
yc

. Speciﬁcally,
Fx
Fy

serves as the inputs of the ﬁrst layer in CM-MAN. Based on Eq.(3.5), the outputs
of the gth layer can incorporate self-modality latent representations with the neighborbood
features from the other modality. Our proposed hierarchical message aggregation networks
including Eqs.(3.2) and (3.5) can aggregate the semantic messages hierarchically across
differentmodalitiesandmodeltheunderlyingmulti-modalcorrelations.Wethenconcatenate
the intra-modal and the inter-modal latent representations respectively from IM-MAN and
CM-MAN in column-wise to obtain complementary features as follows:
Cx = [ f x
h (Hx), f xc
h (Hxc)], Cy = [ f y
h (Hy), f yc
h (Hyc)],
(3.6)
where H#, # = {x, xc, y, yc}, denotes the outputs of the above hierarchical message aggre-
gation networks, and f #
h (·) indicates the relaxation hash layers which project H# into a low-
dimensional latent space RNb× k
4 . Then we obtain complementary features Cx, Cy ∈RNb× k
2 .
In order to construct the modality-invariant binary codes, we directly concatenate comple-
mentary representations corresponding to different modalities:
Ts = [Cx, Cy], Btea = sgn(Ts),
(3.7)
where Btea ∈{−1, 1}Nb×k denotes the teacher hash codes quantiﬁed by the joint comple-
mentary representations Ts ∈RNb×k and sgn(·) is an element-wise sign function. Ts denotes
our obtained multi-modal complementary space. It is comprised of continuous values, which
can characterize the ﬁne-grained intra-modal and inter-modal correlations. Therefore, it can
better guide the training process of our designed student modules to learn a couple of
correlation-preserved hash functions.
Toensurethatthelearnedhashcodescanretainthesemanticcorrelationsbetweencoupled
samples of the original space, under the supervised learning paradigm, the following loss
function is developed to reconstruct the similarity matrix S via the cosine similarity.
min
θtea
J = ∥cos (Btea, Btea) −S∥2
F, s.t. Btea ∈{−1, 1}Nb×k.
(3.8)
Under the deep learning framework with the standard back-propagation [31] algorithm,
the above objective function cannot be directly solved on hash codes, which is the NP-hard
discrete optimization problem (discrete constraints are imposed on the binary hash codes).
Speciﬁcally, the hash codes Btea are generated with sign operation and are not differentiable.
Therefore, we use the relaxed representations Ts instead of Btea in Eq.(3.8) and introduce
Eq.(3.10) to minimize the quantization errors, where Btea = sgn(Ts) is shown in Eq.(3.7).
Then, in this work, we derive the ﬁnal objective functions shown in the following Eqs.(3.9)
and (3.10) to support the optimization process.

52
3
Cross-Modal Hashing
min
θtea
J1 = ∥cos (Ts, Ts) −S∥2
F,
(3.9)
where θtea denote all the trainable parameters in the teacher module for image and text
modalities. cos (·, ·) returns the values in [−1, 1] and we constrain the cosine similarity to
reconstruct the ﬁne-grained similarity matrix S. Speciﬁcally, if Si j=0, which denotes that the
i-th sample is dissimilar with the j-th sample, Si j constrains the cosine similarity between
the i-th sample and the j-th sample to be equal to 0, indicating that the coupled samples are
orthogonal. Without loss of generality, we set J2 to estimate quantization error, written as:
min
θtea
J2 = ∥Ts −Btea∥2
F.
(3.10)
Finally, we build a discriminator D to constrain the joint complementary representations
Ts to follow the Gaussian distribution via an adversarial learning manner [17, 32] as follows:
min
θD
JD = ∥D(Ts) −0∥2
F + ∥D(Rc) −1∥2
F,
min
θtea
J3 = ∥D(Ts) −1∥2
F,
(3.11)
where Rc are obtained by randomly sampling from the Gaussian distribution and its shape
is same as Ts and θD are trainable parameters of discriminator. The overall teacher module
is regarded as a generator. The discriminator D acts as a classiﬁer to distinguish the teacher
module outputs Ts and Rc from the Gaussian distribution.
Cross-Modal Correlation Knowledge Distillation
Most existing methods learn the semantic-preserving hash functions by reconstructing the
similarity matrix calculated by binary label annotations, which cannot achieve ﬁne-grained
semantic supervision. In order to learn ﬁne-grained correlation-preserved hash function
for each modality, we build the lightweight student modules, which are trained under the
guidance of the teacher module outputs. Speciﬁcally, we propose a cross-modal correlation
knowledge distillation strategy to seamlessly transfer intra-modal and inter-modal correla-
tion knowledge from the teacher module to the student modules.
The student modules consist of stacked fully-connected layers to learn binary represen-
tations, which can preserve implicit inter-modal and intra-modal semantic correlations in
the original space. For the i-th coupled data (xi, yi) in the multimodal dataset, we learn
a couple of hash functions, written as Hx and Hy, to project the pre-extracted features of
multi-modal data into a common Hamming space, the matrix form can be formulated as
follows:
Hstu
x
= Hx(X; θstu
x ), Bstu
x
= sgn(Hstu
x ),
Hstu
y
= Hy(Y; θstu
y ), Bstu
y
= sgn(Hstu
y ),
(3.12)
where Bstu
∗
∈{−1, 1}Nb×k denotes the student hash codes quantiﬁed by Hstu
∗
∈RNb×k,
∗= {x, y}.

3.3
Efficient Hierarchical Message Aggregation Hashing
53
In the retrieval scenarios with multi-modal data, the retrieved samples in database usually
contain multiple modalities. Therefore, we introduce a bit-wise fusion module to jointly learn
bit-wise semantics by bit-aligned multi-modal representations Hstu
x
and Hstu
y , and further
enhance the discrimination of hash codes. The fusion strategy can be written as follows:
Hstu
f
= Hstu
x
⊙w f + Hstu
y
⊙(1 −w f ),
Bstu
f
= sgn(Hstu
f ),
(3.13)
where ⊙represents Hadamard product and w f ∈(0, 1)1×k is a trainable fusion weight
vector. Equation (3.13) shows that different samples share the same fusion weight vector,
while different hash bits have different fusion weights.
Based on the hierarchical message aggregation networks in the teacher module, which
effectively captures the correlations of different modalities, we further propose an effective
cross-modal correlation knowledge distillation strategy to transfer the knowledge from the
teacher module to the student modules. Speciﬁcally, we design inter-modal complementary
correlation constraints and intra-modal correlation constraints to jointly preserve inter-modal
and intra-modal correlations via a cosine similarity reconstruction strategy. Figure3.3 illus-
tratesourproposedcross-modalcorrelationknowledgedistillationstrategy.Thelossfunction
of cross-modal correlation knowledge distillation can be written as follows:
min
θstu
x
,θstu
y ,w f
Q1 = ∥cos (Hstu
f , Hstu
f ) −cos (Ts, Ts)∥2
F
+ ∥cos (Hstu
x , Hstu
y ) −cos (Ts, Ts)∥2
F
+ ∥cos (Hstu
x , Hstu
x ) −cos (Tx
s , Tx
s )∥2
F
+ ∥cos (Hstu
y , Hstu
y ) −cos (Ty
s , Ty
s )∥2
F,
(3.14)
Fig.3.3 The cross-modal
correlation knowledge
distillation strategy. The ﬁgure
on the left shows the relations
between coupled samples have
preserved by cosine similarity
constraint, denoted as ⊗. The
right represents our proposed
cross-modal correlation
knowledge distillation contains
inter-modal complementary
correlation constraint and
intra-modal correlation
constraint
Teacher Module
Student Module
y
stu
H
y
sT
x
sT
sT
Intra-modal Correlaon
...
K-bits
......
Inter-modal  
Correlaon
......
x
stu
H
stu
f
H

54
3
Cross-Modal Hashing
where the ﬁrst two terms are the inter-modal complementary correlation reconstruction
loss, and the last two terms are the intra-modal correlation reconstruction loss. Tx
s =
[Hx, Hxc], Ty
s = [Hy, Hyc] denote the image and text part of the joint complementary rep-
resentation Ts. As shown in Eq. (3.14), correlations of the fusion hash codes Hstu
f
and cross-
modal correlations between Hstu
x
and Hstu
y
preserve cross-modal correlation knowledge, and
the corresponding intra-modal correlation knowledge is preserved within each modality. We
can obtain ﬁne-grained correlation matrices by calculating the cosine similarity on the con-
tinuous real-valued Ts, which is regarded as the guidance for the student modules in the hash
function learning process. The ﬁne-grained multi-modal correlation knowledge in Ts can
be seamlessly transformed to the student modules from both inter-modal and intra-modal
perspectives.
Then, to alleviate the quantization loss and achieve hash bit alignment among different
modalities, we minimize the differences between the relaxation vectors and the discrete hash
codes learned in the teacher module, which has the following formulation:
min
θstu
x
,θstu
y ,w f
Q2 = ∥Hstu
x
−Btea∥2
F + ∥Hstu
y
−Btea∥2
F
+ ∥Hstu
f
−Btea∥2
F.
(3.15)
When the student modules are trained, we can directly obtain the hash codes for new
instances at the query stage based on Eq.(3.12).
Objective Function
Based on the above discussions, the overall objective function of the teacher module can be
written as follows:
min
θtea
Ltea = αJ1 + βJ2 + J3, min
θD
Ladv = JD,
(3.16)
and for the student modules, it can be represented as follows:
min
θstu
x
,θstu
y ,w f
Lstu = μQ1 + Q2,
(3.17)
where α, β and μ are tradeoff hyper-parameters of the teacher module and student modules,
respectively. All the trainable parameters in our model are learned by a standard back-
propagation [31] algorithm based on the above objective function.
3.3.3
Experiment
Experimental Datasets
We conduct our experiments on three public cross-modal retrieval datasets: MIR Flickr [33],
NUS-WIDE [34] and MS COCO [35]. They are both composed of images and corresponding

3.3
Efficient Hierarchical Message Aggregation Hashing
55
texts. These datasets are widely tested in recent cross-modal hashing methods [19, 22, 32,
36, 37].
• MIR Flickr contains 25,000 samples which are collected from Flickr website.1 20,015
samples are selected to perform the cross-modal retrieval task and each sample belongs
to at least one of the 24 unique categories. The user-provided tags are regarded as textual
information. Following the experimental settings in [19, 32], we randomly select 2,000
samples to construct the query set and the remaining 18,015 samples are served as the
retrieval set. Finally, we randomly choose 10,000 samples as the training set.
• NUS-WIDE contains 269,648 web images and each instance has corresponding user-
provided tags, which can be regarded as textual modality. Each pair is annotated by one or
multiple labels from 81 semantic concepts. Following the experimental settings in SSAH
[32], we choose 190,421 image-text pairs that belong to the 21 most frequent concepts
by removing the pairs without labels or tags. Then 2,100 pairs are randomly selected as
the query set. And the rest 188,321 coupled samples are served as the retrieval set. We
further randomly choose 10,500 instances from the retrieval set for training.
• MS COCO contains 82,783 training items and 40,504 validation items. MS COCO is also
a multi-label dataset and each coupled data belongs to at least one of the 80 categories.
Following the experimental settings in [19, 38],2 we remove the unlabeled items and
get 122,218 items. We randomly sample 5,000 items to obtain the query set and the
remaining 117,218 image-text pairs are served as the retrieval set. Finally, we randomly
select 10,000 instances from the retrieval set to construct the training set.
Evaluation Metrics and Baselines
In our experiments, we employ two evaluation criteria: Mean Average Precision (MAP) [39,
40] with MAP@R=500 and topK-precision [41, 42] curves. These evaluation criteria have
been widely used for evaluating the cross-modal hashing methods [12, 19, 32, 36].
We compare our proposed HMAH with several state-of-the-art cross-modal hashing
methods including CVH [24], SCM [43], LSSH [3], SePH [44], STMH [36], SCRATCH
[8], DLFH [45], MTFH [10], BATCH [12], DCMH [4], SSAH [32], GCH [19] and DCHUC
[22]. DCMH, SSAH, GCH, DCHUC and ours are deep cross-modal hashing methods, while
the others are shallow methods. The same deep features are served as the inputs for image
modality in shallow models. For all compared baselines, we carefully adjust the hyper-
parameters in their models. All the results are computed by averaging ﬁve experiments. We
evaluate the performance of all compared approaches on the I2T and T2I retrieval tasks.
Speciﬁcally, I2T task uses images as the queries to search the relevant texts in the retrieval
set, and T2I task is similar to I2T.
1 https://www.ﬂickr.com/.
2 SSAH [32] only chooses 85,000 samples on MS COCO in their experiments. Hence, for MS COCO,
we do not follow the experimental setting of SSAH.

56
3
Cross-Modal Hashing
Implementation Details
In this section, we explain the details of the experiments in detail, including data processing,
network structure and training settings.
• Feature Extractor: Following the recent cross-modal hashing works [4, 32, 46], we
extract 4,096-dimensional deep features by the pre-trained CNN-F [47] model for image
modality on three datasets. For textual modality, the texts are respectively represented
as 1,386-dimensional, 1,000-dimensional and 2,000-dimensional BoW vectors on MIR
Flickr, NUS-WIDE and MS COCO datasets.
• Feature Encoders: Both ImageNet and T ext Net consist of three fully-connected lay-
ers (dx →2,048 →1,024 →512, dy →300 →512 →512). Each fully-connected layer
is followed by a batch normalization layer and an activation function Tanh(·).
• Hierarchical Message Aggregation Networks: In experiments, both IM-MAN and CM-
MAN contain a single layer. Our experimental results indicate that more aggregation
layers lead to over-smoothing problem. Besides, we set the number of the input and
output neurons of IM-MAN and CM-MAN to 512.
• Discriminator: We stack three fully-connected layers to build the discriminator (k →
64 →32 →1), and each layer is followed by an activation function Relu(·).
• Hash Functions: Both Hx and Hy in Eq.(3.12) consist of three fully-connected layers
(dx →2,048 →512 →k, dy →300 →512 →k). Each layer is followed by a batch
normalization layer and an activation function Tanh(·).
• Training Settings: At the training stage, we adopt the ADAM optimizer [48] and set the
batch size as 1,024. The learning rate is empirically set to 0.0001 for both teacher and
student modules. The training epochs are set to 40 on MIR Flickr, 20 on NUS-WIDE and
100 on MS COCO. In addition, HMAH is implemented via PyTorch3 and we conduct
all our experiments on a server with a single NVIDIA RTX 2080Ti GPU and a 2.20GHz
Intel (R) Xeon (R) Silver 4214 CPU.
Retrieval Accuracy Comparison
Tables3.1, 3.2 and 3.3 show the MAP@500 results of all compared approaches on three
datasets with varied hash code lengths. The ‘-’ in the above tables indicates that the corre-
sponding result has not been provided in their works. From the above results, we can obtain
the following observations:
• Our method consistently outperforms all the state-of-the-art methods on all hash code
lengths and datasets. Speciﬁcally, compared with the second best results, HMAH achieves
obvious performance improvements. The average improvements are 2.41% on MIR
Flickr, 3.7% on NUS-WIDE and 4.74% on MS COCO. The improvement ranges are
1.3%∼4.2%, 2.5%∼4.5% and 1.3%∼7.2% on three datasets respectively.
3 https://pytorch.org/.

3.3
Efficient Hierarchical Message Aggregation Hashing
57
Table 3.1 MAP@500 on MIR Flickr. The best results are shown in boldface and the second best
results are underlined. The following tables are the same
Task
Method
MIR Flickr
16 bits
32 bits
64 bits
128 bits
256 bits
I2T
CVH [24]
0.749
0.747
0.742
0.731
0.727
SCM [43]
0.886
0.894
0.900
0.906
0.906
LSSH [3]
0.803
0.822
0.826
0.841
0.846
SePH [44]
0.915
0.923
0.934
0.948
0.954
STMH [36]
0.825
0.798
0.850
0.811
0.789
SCRATCH [8]
0.918
0.935
0.939
0.941
0.941
DLFH [45]
0.838
0.862
0.886
0.890
0.898
MTFH [10]
0.856
0.875
0.903
0.906
0.908
BATCH [12]
0.914
0.929
0.933
0.941
0.943
DCMH [4]
0.724
0.731
0.731
0.800
0.834
SSAH [32]
0.903
0.922
0.925
0.945
0.958
GCH [19]
0.833
0.857
0.869
–
–
DCHUC [22]
0.895
0.916
0.926
0.911
–
Ours
0.960
0.965
0.969
0.971
0.971
T2I
CVH [24]
0.769
0.771
0.765
0.755
0.749
SCM [43]
0.883
0.890
0.897
0.902
0.902
LSSH [3]
0.769
0.782
0.812
0.821
0.833
SePH [44]
0.870
0.877
0.898
0.909
0.912
STMH [36]
0.799
0.814
0.829
0.829
0.806
SCRATCH [8]
0.888
0.888
0.898
0.901
0.889
DLFH [45]
0.877
0.904
0.914
0.912
0.919
MTFH [10]
0.871
0.884
0.888
0.892
0.896
BATCH [12]
0.896
0.906
0.904
0.911
0.909
DCMH [4]
0.764
0.749
0.780
0.837
0.860
SSAH [32]
0.896
0.906
0.915
0.910
0.887
GCH [19]
0.892
0.910
0.907
–
–
DCHUC [22]
0.897
0.893
0.919
0.893
–
Ours
0.915
0.925
0.938
0.940
0.942
• Several methods, such as DCMH, SSAH and DCHUC, achieve promising performance
on MIR Flickr, but relatively lower retrieval precision on NUS-WIDE and MS COCO.
The results demonstrate that the performance of these methods cannot be generalized well
on large-scale NUS-WIDE and MS COCO. In contrast, our method does not demonstrate
this phenomenon.

58
3
Cross-Modal Hashing
Table 3.2 MAP@500 on NUS-WIDE
Task
Method
NUS-WIDE
16 bits
32 bits
64 bits
128 bits
256 bits
I2T
CVH [24]
0.499
0.495
0.489
0.472
0.457
SCM [43]
0.646
0.675
0.681
0.684
0.688
LSSH [3]
0.620
0.649
0.676
0.696
0.707
SePH [44]
0.707
0.754
0.766
0.780
0.796
STMH [36]
0.594
0.609
0.615
0.565
0.540
SCRATCH [8]
0.768
0.785
0.797
0.808
0.808
DLFH [45]
0.564
0.613
0.617
0.635
0.622
MTFH [10]
0.716
0.750
0.765
0.787
0.790
BATCH [12]
0.757
0.781
0.797
0.806
0.812
DCMH [4]
0.568
0.561
0.596
0.707
0.697
SSAH [32]
0.691
0.727
0.728
0.771
–
GCH [19]
0.693
0.719
0.753
–
–
DCHUC [22]
0.707
0.672
0.738
0.802
0.785
Ours
0.813
0.825
0.840
0.849
0.856
T2I
CVH [24]
0.557
0.542
0.533
0.512
0.498
SCM [43]
0.667
0.712
0.724
0.728
0.731
LSSH [3]
0.594
0.640
0.683
0.711
0.729
SePH [44]
0.715
0.759
0.769
0.778
0.793
STMH [36]
0.544
0.596
0.679
0.680
0.660
SCRATCH [8]
0.754
0.771
0.775
0.778
0.786
DLFH [45]
0.701
0.754
0.774
0.783
0.792
MTFH [10]
0.694
0.735
0.733
0.736
0.758
BATCH [12]
0.753
0.767
0.778
0.787
0.789
DCMH [4]
0.558
0.591
0.616
0.594
0.576
SSAH [32]
0.658
0.673
0.666
0.637
–
GCH [19]
0.732
0.766
0.761
–
–
DCHUC [22]
0.612
0.695
0.739
0.752
0.752
Ours
0.783
0.796
0.814
0.820
0.827
• Compared with deep-based models, the improvements of our model are more obvious.
The four deep-based baselines all directly take discrete labels as the learning guidance
and simultaneously obtain a shared space among different modalities. In contrast, our pro-
posed cross-modal correlation knowledge distillation strategy can provide ﬁne-grained
cross-modal correlation messages which are learned by the hierarchical message aggre-
gation networks for hash functions to learn a complementary space. The results validate

3.3
Efficient Hierarchical Message Aggregation Hashing
59
Table 3.3 MAP@500 on MS COCO
Task
Method
MS COCO
16 bits
32 bits
64 bits
128 bits
256 bits
I2T
CVH [24]
0.481
0.488
0.505
0.499
0.491
SCM [43]
0.567
0.615
0.643
0.660
0.613
LSSH [3]
0.550
0.586
0.630
0.674
0.700
SePH [44]
0.546
0.564
0.583
0.606
0.622
STMH [36]
0.498
0.518
0.523
0.464
0.475
SCRATCH [8]
0.622
0.687
0.750
0.751
0.753
DLFH [45]
0.582
0.623
0.641
0.664
0.675
MTFH [10]
0.597
0.647
0.717
0.754
0.763
BATCH [12]
0.662
0.695
0.719
0.739
0.747
DCMH [4]
0.505
0.536
0.557
0.566
–
SSAH [32]
0.632
0.669
0.668
0.724
–
GCH [19]
0.648
0.686
0.708
–
–
DCHUC [22]
0.513
0.550
0.558
0.517
0.477
Ours
0.691
0.732
0.763
0.790
0.808
T2I
CVH [24]
0.516
0.526
0.552
0.546
0.544
SCM [43]
0.608
0.684
0.781
0.813
0.696
LSSH [3]
0.525
0.574
0.636
0.693
0.749
SePH [44]
0.628
0.672
0.705
0.758
0.782
STMH [36]
0.516
0.543
0.539
0.504
0.506
SCRATCH [8]
0.669
0.755
0.856
0.858
0.856
DLFH [45]
0.554
0.608
0.652
0.697
0.701
MTFH [10]
0.529
0.649
0.769
0.804
0.848
BATCH [12]
0.743
0.796
0.837
0.866
0.876
DCMH [4]
0.549
0.572
0.605
0.611
–
SSAH [32]
0.583
0.556
0.664
0.677
–
GCH [19]
0.745
0.797
0.830
–
–
DCHUC [22]
0.527
0.500
0.520
0.542
0.545
Ours
0.800
0.869
0.904
0.934
0.947
that the complementary space can better align heterogeneous modalities than the shared
space with less semantic loss. Our approach can reduce the heterogeneous modalities
gaps and enhance the semantic representation capability of the hash codes.
• We ﬁnd that the performance of several shallow cross-modal hashing methods is even
higher than that of the deep-based models. These results validate our previous claims

60
3
Cross-Modal Hashing
that existing deep-based cross-modal hashing methods have limitations on modelling the
multi-modal correlations and preserving them in binary hash codes.
• Most compared baselines achieve different MAP performance improvements as the hash
code length increases. However, several methods show a decreasing trend, such as SSAH
with 256 bits on MIR Flickr, DCMH with 256 bits on NUS-WIDE and DCHUC with
128 and 256 bits on MS COCO. Different from these methods, our proposed method
improves the performance with the increasing of hash codes. The underlying reason is
that these methods cannot capture more semantic information based on discrete labels
in longer hash code. In our HMAH, the complementary latent representations from the
teacher module can carry more informative semantic information as the hash code length
grows, and further provide ﬁne-grained correlation knowledge to guide the hash functions
learning in the student modules by a distillation strategy.
The topK-precision curves on MIR Flickr, NUS-WIDE and MS COCO datasets are
plotted in Figs.3.4, 3.5 and 3.6 respectively. This metric can reﬂect the precision of retrieved
samples in the ﬁrst returned K samples. The results also show that HMAH consistently
outperforms all the comparison methods on all hash code lengths. With the increase of K,
the retrieval precision of our model is relatively stable.
Efﬁciency Comparison
We conduct the run time comparison experiment on three datasets with the length of the
hash codes ﬁxed to 128 bits. Note that the training and testing time of our proposed model
include the pre-trained CNN-F feature extraction time. As shown in Table3.4, the training
and testing time of our model are close to several shallow cross-modal hashing baselines and
signiﬁcantly faster than all the deep-based ones (Training time: 3.8×∼606.9× speedup on
MIR Flickr, 13.9×∼575.5× speedup on NUS-WIDE and 3.2×∼244.6× speedup on MS
COCO, Testing time: 15.5×∼19.9× speedup on MIR Flickr, 13.0×∼28.0× speedup on
NUS-WIDE and 12.6×∼18.2× speedup on MS COCO). Different from most deep cross-
modal hashing methods, our model does not need to ﬁnetune the pre-trained deep feature
extraction model and pays more attention to the downstream task, that is, mining cross-modal
correlations and preserving them into discriminative hash codes. In addition, our proposed
hierarchical message aggregation networks can effectively model the intrinsic multi-modal
correlations and achieve satisfactory performance with less training epochs. It should be
noted that the majority of the testing time come from the feature extraction process, and
the pre-trained CNN-F model can be easily replaced with other lightweight deep feature
extraction models to perform efﬁcient retrieval in large-scale scenarios.

3.3
Efficient Hierarchical Message Aggregation Hashing
61
Fig.3.4 TopK-precision curves on MIR Flickr
Ablation Study
We design six variants to verify the effectiveness of the hierarchical message aggregation
networks and the cross-modal correlation knowledge distillation in HMAH:
• H-1: It retains the student modules to be learned under the guidance of discrete labels
and removes the teacher module, which is used to demonstrate the effect of cross-modal
correlation knowledge in the complementary space of the teacher module on the student
modules. The loss function in Eq.(3.17) of the student modules can be replaced with the
following form:
min
θstu
x
,θstu
y ,w f
Lstu = ϕQ
′
1 + Q
′
2,
(3.18)

62
3
Cross-Modal Hashing
Fig.3.5 TopK-precision curves on NUS-WIDE
where ϕ is tradeoff hyper-parameter and is set to 1. Q
′
1 is the similarity-preserved loss
by reconstructing S deﬁned in Eq.(3.3):
Q
′
1 = ∥cos (Hstu
x , Hstu
x ) −S∥2
F
+ ∥cos (Hstu
y , Hstu
y ) −S∥2
F
+ ∥cos (Hstu
x , Hstu
y ) −S∥2
F
+ ∥cos (Hstu
f , Hstu
f ) −S∥2
F,
(3.19)
and the quantization loss is deﬁned as

3.3
Efficient Hierarchical Message Aggregation Hashing
63
Fig.3.6 TopK-precision curves on MS COCO
Q
′
2 = ∥Hstu
x
−Bstu
f ∥2
F + ∥Hstu
y
−Bstu
f ∥2
F
+ ∥Hstu
f
−Bstu
f ∥2
F.
(3.20)
• H-2: To further illustrate the effectiveness of our proposed complementary space com-
pared with the shared space, we design the variant ‘H-2’. Speciﬁcally, it removes the
CM-MAN shown in Eq.(3.5) and the concatenation operations shown in Eqs.(3.6)–(3.7).
Then the teacher module generates a couple of outputs with the IM-MAN corresponding
to image and text modalities, respectively. Then these outputs are adopted to guide the
student modules as follows:
Q
′
3 = ∥Hstu
x
−f x
h (Hx)∥2
F + ∥Hstu
y
−f y
h (Hy)∥2
F,
(3.21)

64
3
Cross-Modal Hashing
Table 3.4 Training and testing time in seconds
Method
MIR Flickr
NUS-WIDE
MS COCO
Train
Test
Train
Test
Train
Test
CVH [24]
7.5
0.9
6.3
7.5
7.1
5.3
SCM [43]
164.8
0.3
116.3
2.1
243.9
1.6
LSSH [3]
180.7
18.9
165.5
168.8
182.4
113.7
SePH [44]
314.6
2.3
325.3
19.1
338.6
14.6
STMH [36]
439.5
7.5
410.0
57.2
480.1
54.6
SCRATCH [8]
4.9
2.5
4.8
13.2
5.0
9.8
DLFH [45]
101.5
0.2
104.7
1.5
97.8
1.1
MTFH [10]
395.1
0.1
416.5
0.2
430.3
0.1
BATCH [12]
1.5
2.6
1.5
14.2
1.7
11.5
DCMH [4]
3969.9
97.9
3957.1
943.5
4161.0
519.9
SSAH [32]
5865.8
116.5
5558.4
1719.1
6954.9
754.7
GCH [19]
73734.2
125.4
41495.7
802.1
78630.4
700.2
DCHUC [22]
461.3
114.3
1001.8
1006.9
1022.3
644.6
Ours
121.5
6.3
72.1
61.5
321.5
41.4
where f x
h (Hx) and f y
h (Hy) are low-dimensional relaxation representations denoted in
Eq. (3.6), and their dimensionalities should keep consistent with Hstu
x
and Hstu
y
by adjust-
ing the relaxation hash layers f x
h (·) and f y
h (·). In addition, the student modules adopt
Eq.(3.18) as the objective to learn a shared space. Compared with the variant ‘H-1’,
‘H-2’ adopts the teacher’s outputs to transfer the intra-modal correlation knowledge to
the student modules.
• H-3: ‘H-3’ removes the CM-MAN and only performs the IM-MAN on the correspond-
ing modality in our teacher module. The outputs of the IM-MAN are concatenated to the
complementary representations by adjusting the corresponding dimensionalities, which
can verify the signiﬁcance of cross-modal complementary information learned by the
CM-MAN in the hierarchical message aggregation networks. Compared with ‘H-2’,
‘H-3’ performs the concatenation operation similar to Eq.(3.7), which can obtain com-
plementary representations. ‘H-3’ adopts our proposed cross-modal correlation knowl-
edge distillation strategy in Eq. (3.14) to transfer the learned knowledge in the above
complementary representations to the student modules.
• H-4: It removes our proposed bit-wise fusion module shown in Eq.(3.13).
• H-5: It replaces our bit-wise fusion module shown in Eq.(3.13) with a concatenation
strategy as follows:

3.3
Efficient Hierarchical Message Aggregation Hashing
65
Table 3.5 The ablation study results on MIR Flickr. H denotes our proposed model. The following
tables are the same
MIR Flickr
Task
Var.
16 bits
32 bits
64 bits
128 bits
256 bits
I2T
H
0.960
0.965
0.969
0.971
0.971
H-1
0.934
0.952
0.958
0.962
0.964
H-2
0.945
0.954
0.963
0.966
0.966
H-3
0.952
0.958
0.962
0.966
0.967
H-4
0.924
0.933
0.941
0.946
0.950
H-5
0.939
0.954
0.965
0.967
0.969
H-6
0.948
0.962
0.963
0.966
0.968
T2I
H
0.915
0.925
0.938
0.940
0.942
H-1
0.900
0.919
0.925
0.934
0.934
H-2
0.904
0.910
0.923
0.934
0.935
H-3
0.911
0.918
0.928
0.933
0.932
H-4
0.907
0.911
0.926
0.934
0.935
H-5
0.903
0.918
0.925
0.912
0.933
H-6
0.896
0.920
0.930
0.933
0.934
Hstu
f
= [ f x
s (Hstu
x ), f y
s (Hstu
y )] ∈RNb×k,
Bstu
f
= sgn(Hstu
f ),
(3.22)
where f x
s (·) and f y
s (·) represent modality-speciﬁc projection functions with a single
fully-connected layer to reduce the dimensionalities of Hstu
x
and Hstu
y
to RNb× k
2 .
• H-6: It removes the discriminator and the corresponding loss functions shown in
Eq.(3.11).
The MAP@500 results shown in Tables3.5, 3.6 and 3.7 indicate our proposed hierarchical
message aggregation networks and cross-modal correlation knowledge distillation strategy
in the HMAH have important impacts on the ﬁnal performance. These six ablation settings
consistently degrade the retrieval accuracy on three datasets. Based on these results, we can
get the following analyses:
• In the variant ‘H-1’, Eq.(3.19) is equivalent to constraining different modalities to learn
a shared space by discrete labels, which can reduce cross-modal semantic gaps to a
certain extent. However, ‘H-1’ signiﬁcantly degrades the performance compared with
the original model ‘H’. Consequently, the complementary space based on multi-modal

66
3
Cross-Modal Hashing
Table 3.6 The ablation study results on NUS-WIDE
NUS-WIDE
Task
Var.
16 bits
32 bits
64 bits
128 bits
256 bits
I2T
H
0.813
0.825
0.840
0.849
0.856
H-1
0.781
0.806
0.822
0.826
0.828
H-2
0.795
0.813
0.822
0.830
0.835
H-3
0.801
0.822
0.834
0.843
0.847
H-4
0.771
0.791
0.818
0.828
0.833
H-5
0.769
0.809
0.833
0.846
0.853
H-6
0.804
0.821
0.834
0.846
0.852
T2I
H
0.783
0.796
0.814
0.820
0.827
H-1
0.752
0.775
0.789
0.797
0.798
H-2
0.734
0.764
0.797
0.799
0.801
H-3
0.760
0.784
0.799
0.813
0.818
H-4
0.755
0.777
0.790
0.802
0.819
H-5
0.734
0.779
0.803
0.818
0.824
H-6
0.774
0.788
0.811
0.815
0.824
Table 3.7 The ablation study results on MS COCO
MS COCO
Task
Var.
16 bits
32 bits
64 bits
128 bits
256 bits
I2T
H
0.691
0.732
0.763
0.790
0.808
H-1
0.660
0.709
0.718
0.729
0.716
H-2
0.680
0.721
0.742
0.755
0.762
H-3
0.672
0.723
0.753
0.772
0.773
H-4
0.666
0.721
0.762
0.774
0.800
H-5
0.675
0.704
0.752
0.773
0.788
H-6
0.677
0.688
0.746
0.784
0.781
T2I
H
0.800
0.869
0.904
0.934
0.947
H-1
0.704
0.767
0.785
0.801
0.787
H-2
0.747
0.806
0.839
0.857
0.865
H-3
0.769
0.845
0.901
0.919
0.929
H-4
0.728
0.803
0.847
0.873
0.880
H-5
0.765
0.848
0.895
0.913
0.936
H-6
0.778
0.861
0.899
0.925
0.936

3.3
Efficient Hierarchical Message Aggregation Hashing
67
representations is more conducive to the reduction of cross-modal semantic gaps than
the shared space based on the discrete labels only.
• Based on the variant ‘H-1’, ‘H-2’ adds the outputs of the IM-MAN to excavate the
intra-modal complementary knowledge and transfer the learned knowledge to the stu-
dent modules by Eq.(3.21). We can ﬁnd that ‘H-2’ outperforms ‘H-1’ but still degrades
the performance compared with the original model ‘H’. Therefore, the IM-MAN can
effectively capture the complementary information among different samples within a
speciﬁc modality.
• Compared with ‘H-2’, ‘H-3’ performs the concatenation operation on the outputs of the
IM-MAN in the teacher module to obtain complementary representations and adopts our
proposed cross-modal correlation knowledge distillation strategy to transfer the learned
knowledge to the student modules. ‘H-3’ outperforms ‘H-2’, which can illustrate the
effects of our proposed complementary space and indicate the correlation knowledge
distillation strategy is better than Eq.(3.21). But ‘H-3’ still degrades the performance
of ‘H’, because ‘H-3’ does not consider the cross-modal message aggregation among
different samples, which is performed by the CM-MAN.
• The results of ‘H-4’ and ‘H-6’ indicate that the bit-wise fusion module and the discrim-
inator can improve the retrieval performance. The former can better learn joint bit-wise
semantics by the adaptive cross-modal fusion strategy. The latter can constrain the dis-
tribution of cross-modal correlation knowledge in the teacher module.
• ‘H-5’ degrades the performance on three datasets. The main reason is that f x
s (·) and f y
s (·)
in Eq.(3.22) reduce the dimensionalities of the bit-aligned multi-modal representations
Hstu
x
and Hstu
y
to perform concatenation operation, which may lose some semantic infor-
mation. In addition, when the number of modalities of samples is variable, our bit-wise
fusion module can easily be extended to generate fusion representations, but the concate-
nation strategy can not directly handle this scenario, which needs to design the matching
dimensionality in advance for concatenation. So ‘H-5’ explains why we do not adopt the
concatenation pipeline in Eq. (3.13) similar to the teacher module.
Based on the above analyses, we can explain why our proposed complementary space is
better than the previous shared space that is adopted by many existing cross-modal hash-
ing methods. Our teacher module performs the hierarchical message aggregation networks
to construct a complementary space, which simultaneously considers complementarities
among different samples and different modalities, and models intra-modal and inter-modal
semantic correlations. Therefore, our teacher module can generate semantic-rich hash codes
in the complementary space, and these hash codes are further adopted to guide the hash func-
tion learning process in the student modules. But in the previous works, e.g., DCMH, SSAH,
GCH, DCHUC, they adopt mandatory modality alignment constraints to learn a shared space
by aligning the heterogeneous modalities with the binary labels, which inevitably leads to
information loss for multi-modal data and cannot fully model the ﬁne-grained correlations
among different modalities.

68
3
Cross-Modal Hashing
Parameter Experiments
There are mainly three hyper-parameters in our model: α, β and μ. In order to choose suitable
values, we tune them in the range of {0.001, 0.01, 0.1, 1, 10, 100}. We conduct the parameter
experiments on MIR Flickr, NUS-WIDE and MS COCO with 128 bits, respectively. The
results are shown in Fig.3.7. We can ﬁnd that the performance of our model is relatively
stable with different values of the parameters β and μ. Based on the above experimental
results, we set the hyper-parameter values to {α=100, β=0.001, μ=1} on three datasets.
Fig.3.7 The parameter sensitivity results on MIR Flickr, NUS-WIDE and MS COCO

3.4
Correlation-Identity Reconstruction Hashing
69
3.4
Correlation-Identity Reconstruction Hashing
3.4.1
Motivation
Many shallow and deep unsupervised cross-modal hashing methods are proposed with var-
ious learning frameworks. Although promising progresses have been made so far, existing
methods still suffer from limited capability on modeling and preserving the intrinsic multi-
modal semantics. Several problems have not been solved well: (1) Identity and correlation
semantics are considered separately. Identity semantics can be understood as the intrinsic
semantics embedded in each instance. In this case, each instance is considered indepen-
dently, and the correlations with other instances are not considered. Correlation semantics
can be understood as the adjacency semantics between instances. In this case, each instance
has varying degrees of dependencies with other instances. Existing methods [3, 26, 28,
49] generally preserve identity semantics into hash codes, which considers each instance
independently and ignores the correlations with other instances. A few methods [5, 6, 30]
are proposed to preserve the correlation semantics into hash codes. However, both iden-
tity semantics and correlation semantics are important components of intrinsic multi-modal
semantics in the cross-modal retrieval application, and they are indispensable for learning
high-quality hash codes. (2) Complementarity between heterogeneous modalities is ignored
during semantic modeling. Existing methods generally use matrix factorization [26, 29, 49]
or similarity constraints [5, 30] to collaboratively project heterogeneous modality features
into a shared Hamming space. In fact, there is a lot of complementary information in het-
erogeneous modalities, which is very important for identifying a discriminative shared joint
representation. However, existing methods ignore this and thus lead to a huge heterogeneous
modality gap that is difﬁcult to bridge. (3) Multi-modal semantics are not preserved dur-
ing the hash function learning. The goal of cross-modal hashing methods is to learn hash
functions with strong representational power to generate discriminative hash codes at the
retrieval stage. However, most existing methods [5, 6, 26, 28–30, 50] simply minimize the
quantization loss between the learned hash codes and the outputs of neural network of hash
functions. They fail to transfer the multi-modal semantics captured during the hash code
learning process into the neural networks of deep hash functions, which makes the learned
hash functions suffer from the limited representational capability.
To address the aforementioned problems, we propose a novel Correlation-Identity Recon-
struction Hashing (CIRH) method to solve the limitations of existing methods. Our main
learning framework is presented in Fig.3.8. At the stage of hash code learning, the proposed
method simultaneously models and preserves the heterogeneous multi-modal correlation
and identity semantics into the hash codes. Speciﬁcally, we perform cross-modal message
aggregation on a heterogeneous graph convolutional network through the proposed Cross-
modal Semantic Aggregation (CSA). CSA effectively reduces the heterogeneous semantic
gap, and then generates a shared joint semantic representation to support the hash code
learning process. In addition, an Identity Semantic Reconstruction (ISR) component is pro-

70
3
Cross-Modal Hashing
Fig.3.8 The basic learning framework of the proposed CIRH. At the stage of hash code learning, we
ﬁrst extract the image and text features using CNN and BoW models. Then, a multi-modal collab-
orated graph is generated by the proposed Multi-modal Collaborated Graph Construction (MCGC)
component. MCGC embeds the correlation between instances into the subsequent feature learning
process. By minimizing the adjacent correlation reconstruction loss, the multi-modal correlation
semantics can be effectively preserved into the ﬁnal hash codes. Simultaneously, we introduce an
Identity Semantic Reconstruction (ISR) component to reconstruct hash codes to the input modality
features, which involve the hash codes with identity semantics. In addition, to bridge the heteroge-
neous modality gap, we propose a Cross-modal Semantic Aggregation (CSA) component to enhance
their information interaction and successively aggregate them into a shared joint semantic represen-
tation which participates in the subsequent semantic reconstruction process of adjacent correlations.
At the stage of hash function learning, we learn two speciﬁc hash functions for cross-modal retrieval
tasks
posed, which enriches the semantic of the ﬁnal hash codes with identity semantics from
the input features based on the framework of Auto-Encoder. Particularly, to drive the hash
code learning process, we design three reconstruction losses to collaboratively preserve the
correlation and identity semantics into the generated binary hash codes. The same semantic
preservation process is also applied to the stage of hash function learning. That is, the hash
codes generated by hash functions can maintain the multi-modal correlation consistency
and the adjacent structure with the hash codes learned at the stage of hash code learning.
3.4.2
Methodology
Notation
We ﬁrst formulate the notations involved in this paper. Speciﬁcally, we use bold uppercase
letters (e.g. A) to denote matrices. The ith row vector of A is denoted as Ai∗, and the element

3.4
Correlation-Identity Reconstruction Hashing
71
located in the ith row and jth column of A is denoted as Ai j. sign(∗) represents the element-
wise sign function which returns 1 if ∗is greater 0, and −1 otherwise. σ(·) represents a ReLU
activation function used in GCN. ∥· ∥F denotes Frobenius norm of a matrix. ⌈∗⌉denotes
rounding up the number ∗. In this paper, similar to many cross-modal hashing methods,
we only consider two modalities, image and text for convenience of discussion. They are
denoted as XI ∈Rn×dI and XT ∈Rn×dT , where n represents the number of instances and
dI (or dT ) represents the feature dimensionality of image (or text). Our purpose is to learn
two hash functions (i.e. fI ( · ; θi) and fT ( · ; θt)) for heterogeneous modality instances, so
as to obtain the uniﬁed compact hash codes B ∈{−1, 1}n×r, where r denotes the hash code
length.
Framework Overview
Figure3.8 shows the basic learning framework of the proposed method, which consists of
the two processes: hash code learning and hash function learning.
At the stage of hash code learning, our framework contains four main components:
• Multi-modal Collaborated Graph Construction (MCGC): Constructing a multi-
modal collaborated graph S with the input heterogeneous features XI and XT . The generated
graph supports the subsequent intra- and inter-modal semantic aggregation.
• Cross-modal Semantic Aggregation (CSA): Enhancing the modality interaction and
reducing the heterogeneous multi-modal gap by performing cross-modal semantic aggrega-
tion on a heterogeneous graph convolutional network.
• Identity Semantic Reconstruction (ISR): Enriching the semantics of the hidden
relaxed representations HI and HT with identity semantics from the input modality fea-
tures XI and XT based on the framework of Auto-Encoder.
• Hash Code Generation (HCG): Generating cross-modal hash codes from multi-modal
complementary representations comprised of VJ, HI and HT from the CSA and ISR com-
ponents, respectively.
At the stage of hash function learning, our goal is to learn two modality-speciﬁc hash
functions fI ( · ; θi) and fT ( · ; θt) based on the multi-modal collaborated graph and the
hash codes B learned at the stage of hash code learning, to map all instances into compact
hash codes to achieve efﬁcient cross-modal retrieval.
Multi-modal Collaborated Graph Construction
Supervised cross-modal hashing methods generally achieve superior retrieval performance
with the accurate and noise-free semantic labels. Contrastively, unsupervised methods
generally establish the associations between instances only from original features, which
inevitably introduces noisy adjacency relations and thus causes severe performance degra-
dation. Therefore, in this paper, we are committed to learning a multi-modal collaborated
graph S, which ﬁlters out the noisy adjacent correlations between instances. Speciﬁcally,
we ﬁrst construct a rough adjacency matrix S1 as:
S1 = α1SI + (1 −α1)ST ,
(3.23)

72
3
Cross-Modal Hashing
where SI and ST represent the similarity matrices corresponding to the image and text
modality, respectively, α1 balances their importance. Each element in SI is calculated by:
SIi j = cos(XIi∗, XI j∗),
(3.24)
where XIi∗and XI j∗represent the features for the ith and jth instance of the image modality,
and cos(·, ·) represents the Cosine similarity between the two vectors. ST shares the same
calculation rules as SI .
S1i j indicates the multi-modal semantic similarity between the ith and jth instance. We
have two considerations: (1) For the top k pairs of the most similar instances, the elements
at their corresponding positions in S1 obtain the top k largest values in turn. (2) Conversely,
for the k pairs of the least similar instances, the elements at their corresponding positions in
S1 obtain the last k smallest values in turn. We think (1) is more reasonable. This is because,
for a query in cross-modal retrieval, we generally concern on the top-ranked instances with
larger semantic relevance. This practice requires us to guarantee that more similar instance
pairs have greater similarity values. We think (2) is not exactly the case. This is because the
small similarity value may easily introduce noises. Therefore, the relative relations between
dissimilar pairs are inaccurate adjacency with high probability, which may be harmful to
unsupervised learning. To ﬁlter out these noisy adjacent relations, we directly set the k
minimum values in each row of S1 to −1. Formally, we design as follows:
S1i j =
	
−1,
S1i j ∈ei(k)
S1i j,
otherwise
,
(3.25)
where ei(k) is a set consisting of k minimum values in the ith row of S1. After the processing
of Eq.(3.25), S1 is optimized as a matrix after noise reduction, which forces the adjacency
value of its k least similar instance pairs in each row of S1 to be the minimum.
In addition, we expect to enhance the adjacency correlation semantics by nonlin-
early strengthening the adjacent correlation between the more similar pairs. Formally, the
enhanced adjacency graph can be obtained as follows:
S2 = 2Sigmoid(S1) −1 + I
=
2
1 + e−S1 −1 + I,
(3.26)
where Sigmoid(·) is an element-wise activation function, 1 ∈Rn×n represents an all-one
matrix and I ∈Rn×n represents an identity matrix. In Eq.(3.26), we introduce I to increase
thesimilarityoftheinstanceitself,whichcanwidenthegapofsimilaritywithotherinstances.
Sigmoid(·) can squeeze the input to a value that is close to 0 or close to 1, which widens
the gap between the similarity value of similar pairs and the similarity value of dissimilar
pairs.

3.4
Correlation-Identity Reconstruction Hashing
73
To better indicate the similarity correlations between instances to supervise the adjacency
semantic preservation, we combine the denoised adjacency relation in Eq.(3.25) and the
enhanced adjacency graph in Eq.(3.26) to generate the ﬁnal multi-modal collaborated graph:
S = α2S1 + (1 −α2)S2,
(3.27)
where α2 balances the importance between S1 and S2.
Discrete Hash Code Learning
With the constructed multi-modal collaborated graph, we learn the hash codes of all training
instances.
Identity Semantic Reconstruction (ISR). For a pair of image and text, we use the modality-
speciﬁc feature extractors to extract their features, denoted as XI and XT , respectively. Then,
we import them into their respective encoders to obtain latent semantic representations VI
and VT . To employ the graph structure to capture the high-order semantic relations between
instances, we introduce GCN to obtain the graph structure representations VGI and VGT .
Speciﬁcally, the lth layer of the GCN can be written as:
H(l)
m = σ( ˜D
−1
2 S ˜D
−1
2 H(l−1)
m
W(l)
m ),
(3.28)
where m ∈{I, T } represents the image or text modality, ˜Dii =  jSi j, H(l−1)
m
and H(l)
m
represent the input and output of the lth layer corresponding to the m modality and W(l)
m
represents the learnable parameters of the lth layer.
Next, as shown in Fig.3.8, we adopt a fully connected layer to aggregate VJ to VGI to
generate latent representation HI , and then feed it into the decoder of the image branch to
reconstruct the original input feature XI . Note that VJ is a shared joint semantic represen-
tation generated by the CSA component, which is described in detail in the next subsection.
Compared with VGI , HI includes more interaction semantics between different modalities.
In the text branch, HT shares a similar reconstruction process with HI , and XT shares a
similar reconstruction process with XI .
Formally, we strictly constrain the whole process through the Coding Consistency Recon-
struction (CCR) loss:
lossCC R = ∥X
′
I −XI ∥2
F + ∥X
′
T −XT ∥2
F,
(3.29)
where X
′
I and X
′
T denote the image and text features reconstructed by the proposed ISR,
respectively.
Cross-modal Semantic Aggregation (CSA). The cross-modal interaction is generally
ignored in existing deep cross-modal hashing methods. Differently, in this paper, we con-
catenate the latent image and text features VI and VT into a new representation VC. Then,

74
3
Cross-Modal Hashing
we input VC into a multi-modal GCN to enhance the interaction between heterogeneous
modality features, and embed the instance-wise neighbor correlations into the feature learn-
ing process. Finally, the CSA component outputs a shared joint semantic representation VJ.
Formally, the interaction process can be represented as:
H(l)
c
= σ( ˜D
−1
2
c
Sc ˜D
−1
2
c
H(l−1)
c
W(l)
c )
= σ( ˜D
−1
2
c
S I
I S

˜D
−1
2
c

V(l−1)
I
V(l−1)
T

W(l)
c )
s.t. Sc =
S I
I S

, H(l−1)
c
=

V(l−1)
I
V(l−1)
T

,
(3.30)
where ˜Dcii =  jSci j and H(0)
c
= VC. Beneﬁting from the CSA component, the hetero-
geneous semantic gap is reduced through the message aggregation among heterogeneous
node features, and the multi-modal adjacency correlation between the original instances is
seamlessly preserved into the shared joint semantic representation VJ.
Hash Code Generation (HCG). HI and HT contain the identity semantics through ISR, and
VJ characterizes the cross-modal correlations via CSA. They contain different semantics
and are complementary. In this paper, we directly concatenate HI , HT and VJ to generate a
composite multi-modal complementary representation HJ. We expect that the latent repre-
sentations HI, HT , and HJ can reﬂect the correlations between the original instances, and
the similarity structure can be preserved into the hash codes B as much as possible. To this
end, we introduce an Adjacency Correlation Reconstruction (ACR) loss as follows:
lossAC R = ∥S −cos(HI , HI )∥2
F
+ ∥S −cos(HT , HT )∥2
F
+ ∥S −cos(HJ, HJ)∥2
F,
(3.31)
where∥S −cos(HI , HI )∥2
F (or∥S −cos(HT , HT )∥2
F)indicatesthattheadjacencyrelations
are preserved into the latent features HI (or HT ), and ∥S −cos(HJ, HJ)∥2
F represents that
the structural similarity correlations are seamlessly embedded into HJ. We show this part
in Fig.3.9. Subsequently, we transfer the semantic information from HJ to the binary hash
codes B by minimizing the proposed DIScretization (DIS) loss:
lossDI S = ∥B −HJ∥2
F,
s.t. B = sign(HJ).
(3.32)

3.4
Correlation-Identity Reconstruction Hashing
75
Fig.3.9 The basic principle of the proposed Adjacent Correlation Reconstruction (ACR) loss. Note
that fI denotes fI (XI ; θi) and fT denotes fT (XT ; θt)
Deep Hash Function Learning
In this section, based on the hash codes B learned in Sect. 3.4.2 and the pre-constructed multi-
modal collaborated graph S, we learn two modality-speciﬁc hash functions (i.e. fI ( · ; θi)
and fT ( · ; θt)) which project XI and XT to the uniﬁed compact hash codes B. Speciﬁcally,
the loss of learning the hash functions are designed as follows:
loss f 1 = ∥fI (XI ; θi) −fT (XT ; θt)∥2
F
+ ∥B −fI (XI; θi)∥2
F + ∥B −fT (XT ; θt)∥2
F,
(3.33)
where fI (XI ; θi) and fT (XT ; θt) denote the relaxed representations of hash codes corre-
sponding to the image and text modality, respectively. Then, the hash codes of the two
modalities can be expressed as sign( fI(XI ; θi)) and sign( fT (XT ; θt)).
Equation(3.33) only ensures the numerical consistency between the hash codes generated
by the hash functions and the hash codes B. However, the consistency of the structural
similarity correlations between multi-modal data is unfortunately ignored during the hash
function learning process. This point has also been ignored by existing cross-modal hashing
methods at the stage of hash function learning. In this work, we propose a correlation-identity
consistent hash function learning strategy to ensure the multi-modal correlation semantics
be transferred to the hash functions, and formally design the following loss:
loss f 2 = ∥S −cos( fI (XI ; θi), fI(XI ; θi))∥2
F
+ ∥S −cos( fT (XT ; θt), fT (XT ; θt))∥2
F
+ ∥S −cos( fI (XI ; θi), fT (XT ; θt))∥2
F.
(3.34)

76
3
Cross-Modal Hashing
With Eq.(3.34), the correlations between the learned hash codes at the stage of hash code
learning is consistent with the correlations between the generated relaxed hash codes by
hash functions.
Algorithm 1 Optimization process for CIRH
Input: n training image-text pairs; hash code length r; batch size b; learning rate of the hash codes
and functions lrc and lr f ; number of epochs t, hyper-parameter α1, α2, k, λ1, λ2, β.
Output: Network parameters of the to-be-learned hash functions: θi and θt.
1: Initialize the hash code and function networks: θc, θi and θt.
2: Extract image features XI and text features XT to construct a multi-modal collaborated graph S.
3: repeat
4:
// Hash code learning
5:
for i ∈[1, ⌈n
b ⌉] do
6:
Randomly select b training instance pairs;
7:
Generate HI , HT and HJ by forward propagation;
8:
Calculate loss with Eqs.(3.29), (3.31) and (3.32);
9:
Update the parameter θc by backpropagation;
10:
end for
11: until (current epoch == t)
12: Generate hash codes B of training instances;
13: repeat
14:
// Hash function learning
15:
for i ∈[1, ⌈n
b ⌉] do
16:
Randomly select b training instance pairs;
17:
Generate fI (XI ; θi) and fT (XT ; θt) by forward propagation;
18:
Calculate loss with Eqs.(3.33) and (3.34);
19:
Update the parameter θi and θt by backpropagation;
20:
end for
21: until convergence
Overall Objective Formulation and Optimization
By integrating Eqs.(3.29), (3.31) and (3.32) into a uniﬁed learning framework, we derive
the overall objective function for hash code learning as:
min
θc
losscodes = λ1lossCC R + λ2lossAC R + lossDI S,
(3.35)

3.4
Correlation-Identity Reconstruction Hashing
77
where λ1 and λ2 are two trade-off hyper-parameters to adjust the importance of the ﬁrst and
second regularization items.
Similarly, by integrating Eqs.(3.33) and (3.34) into a uniﬁed learning framework, the
overall objective function for hash function learning is formulated as:
min
θi,θt
loss f unctions = loss f 1 + βloss f 2,
(3.36)
where β represents a balance factor to adjust the learning bias of the hash functions.
We summarize the basic learning process of the proposed CIRH in Algorithm 1. The hash
codes B are optimized by Eq.(3.35), and reach a convergence stable state after t epochs.
Under the supervision of hash codes B, the parameters (i.e. θi and θt) of two deep hash
function networks are optimized immediately and reach a stable state of convergence after
several epochs. For out-of-sample query image ˇXI or text ˇXT , their corresponding hash
codes can be generated by sign( fI( ˇXI ; θi)) or sign( fT ( ˇXT ; θt)).
3.4.3
Experiment
Evaluation Datasets
In this section, we evaluate the performance of the proposed method on three widely tested
cross-modal retrieval datasets: MIR Flickr [33], NUS-WIDE [34], and MS COCO [35].
We conduct comparison experiments with baselines to verify the superior performance of
our method, and design the ablation experiments to validate the effects of each learning
component.
• MIR Flickr [33] consists of 25,000 image-tags pairs with 24 unique concepts. We remove
those pairs that have less than 20 tags, and ﬁnally obtain 20,015 image-tags pairs. To keep
the consistence with the experimental settings of the compared cross-modal hashing
methods [5, 6, 29, 30], we randomly select 2,000 pairs as the query set and the remaining
18,015 pairs as the retrieval set, and select 5,000 pairs from the retrieval set as the training
set. We extract 4,096-dimensional CNN (AlexNet [13]) features to represent each image,
and 1,386-dimensional Bag-of-Words (BoW) [51] features to represent each text.
• NUS-WIDE [34] includes 269,648 images with a total of 81 concepts. With the same
experimental setting as the previous methods [5, 6, 29, 30], we select 186,577 image-text
pairs corresponding to the 10 most common concepts to form the experimental dataset.
2,000 image-tag pairs are randomly selected as the query set and the remaining 184,577
image-text pairs are regarded as the retrieval set, 5,000 image-text pairs from the retrieval
set are randomly selected to form the training set. We use the 4,096-dimensional features
extracted by AlexNet [13] to represent the visual contents and the 1,000-dimensional
BoW [51] features to represent the text contents.

78
3
Cross-Modal Hashing
• MS COCO [35] contains a total of 123,287 image-text pairs in 80 independent categories.
Similar to the data partition settings of MIR Flickr and NUS-WIDE, the query set and
training set are composed of 2,000 and 5,000 randomly selected instances respectively,
and the retrieval set contains a total of 121,287 instances. We use 4,096-dimensional
features extracted by AlexNet [13] to represent each image, and 2,000-dimensional BoW
[51] features to represent its corresponding text.
Baselines and Evaluation Criterion
We compare the proposed method with nine state-of-the-art unsupervised baselines: Cross-
View Hashing (CVH) [24], Inter-Media Hashing (IMH) [25], Latent Semantic Sparse Hash-
ing (LSSH) [3], Collective Matrix Factorization Hashing (CMFH) [26], Deep Binary Recon-
struction (DBRC) [28], Unsupervised Deep Cross-Modal Hashing (UDCMH) [29], Deep
Joint-Semantics Reconstructing Hashing (DJSRH)[5], Joint-modal Distributionbased Simi-
larity Hashing (JDSH) [30] and Aggregation-based Graph Convolutional Hashing (AGCH)
[6]. CVH, IMH, LSSH and CMFH are shallow methods, while DBRC, UDCMH, DJSRH,
JDSH and AGCH are deep methods.
Similar to the previous works [5, 6, 29, 30], we use the commonly adopted Mean Average
Precision (MAP) [42, 52] to evaluate the retrieval performance of all methods. Speciﬁcally,
for a query, we ﬁrst calculate its Average Precision (AP) and then obtain MAP by averaging
AP values of all queries. In addition, we also draw precision-recall and precision-topN curves
to evaluate the performance of all methods.
Implementation Details
As shown in Fig.3.8, our model ﬁrst uses CNN network [53] and BoW [51] to extract image
and text features, respectively. In our experiment, to keep consistence with the compared
baselines [5, 6, 29, 30], we use AlexNet [13] as the backbone network. Speciﬁcally, we
remove its last fully connected layer and then obtain the image representation. The image
encoder is a fully connected layer (dI →512 →BN →ReLU) and its decoder is a multi-
layer perceptron structure (r →512 →BN →ReLU →dI →BN), where dI represents
the image feature dimension (i.e. 4096-dim), BN represents the BatchNorm layer, ReLU
is an activation function, and r is the code length. The structure of the text encoder and
decoder is similar to that of the image. All the GCNs involved in our model include single
graph convolutional layer (512 →512). To obtain rich semantic information, we ﬁrst con-
nect VGI and VJ (or VGT and VJ) and then add a fully connected layer (1024 →r) to obtain
HI (or HT ), and V
′
J is learned by a FC layer (512 →r). The relaxed hash representation
HJ can be learned by adding a hash layer (3r →r). Our hash function is composed of a
two-layer multi-layer perceptron with tanh(·): fI (·; θi) is (dI →dI →r) and fT (·; θt) is
(dT →dT →r).
Two parameters λ1 and λ2 are involved at the stage of hash code learning to play the trade-
off between the identity semantic reconstruction and adjacency correlation reconstruction,

3.4
Correlation-Identity Reconstruction Hashing
79
Table 3.8 Comparison results of MAP@50 on I→T and T→I retrieval tasks on three datasets at different code lengths. The best results in each column
are marked with bold and the second best results are underlined. The below is the same. Since the source code of UDCMH is not released, we ﬁll the
corresponding space with —
Task
Methods
MIR Flickr
NUS-WIDE
MS COCO
16 bits
32 bits
64 bits
128 bits
16 bits
32 bits
64 bits
128 bits
16 bits
32 bits
64 bits
128 bits
I→T
CVH [24]
0.606
0.599
0.596
0.589
0.372
0.363
0.404
0.390
0.505
0.509
0.519
0.510
IMH [25]
0.612
0.601
0.592
0.579
0.470
0.473
0.476
0.459
0.570
0.615
0.613
0.587
LSSH [3]
0.584
0.599
0.602
0.614
0.481
0.489
0.507
0.507
0.652
0.707
0.746
0.773
CMFH [26]
0.621
0.624
0.625
0.627
0.455
0.459
0.465
0.467
0.621
0.669
0.525
0.562
DBRC [28]
0.617
0.619
0.620
0.621
0.424
0.459
0.447
0.447
0.567
0.591
0.617
0.627
UDCMH [29]
0.689
0.698
0.714
0.717
0.511
0.519
0.524
0.558
–
–
–
–
DJSRH [5]
0.810
0.843
0.862
0.876
0.724
0.773
0.798
0.817
0.678
0.724
0.743
0.768
JDSH [30]
0.832
0.853
0.882
0.892
0.736
0.793
0.832
0.835
0.694
0.738
0.769
0.788
AGCH [6]
0.865
0.887
0.892
0.912
0.809
0.830
0.831
0.852
0.741
0.772
0.789
0.806
CIRH
0.901
0.913
0.929
0.937
0.815
0.836
0.854
0.862
0.797
0.819
0.830
0.849
T→I
CVH [24]
0.591
0.583
0.576
0.576
0.401
0.384
0.442
0.432
0.543
0.553
0.560
0.542
IMH [25]
0.603
0.595
0.589
0.580
0.478
0.483
0.472
0.462
0.641
0.709
0.705
0.652
LSSH [3]
0.637
0.659
0.659
0.672
0.577
0.617
0.642
0.663
0.612
0.682
0.742
0.795
CMFH [26]
0.642
0.662
0.676
0.685
0.529
0.577
0.614
0.645
0.627
0.667
0.554
0.595
DBRC [28]
0.618
0.622
0.626
0.628
0.455
0.459
0.468
0.473
0.635
0.671
0.697
0.735
UDCMH [29]
0.692
0.704
0.718
0.733
0.637
0.653
0.695
0.716
–
–
–
–
DJSRH [5]
0.786
0.822
0.835
0.847
0.712
0.744
0.771
0.789
0.650
0.753
0.805
0.823
JDSH [30]
0.825
0.864
0.878
0.880
0.721
0.785
0.794
0.804
0.703
0.759
0.793
0.825
AGCH [6]
0.829
0.849
0.852
0.880
0.769
0.780
0.798
0.802
0.746
0.774
0.797
0.817
CIRH
0.867
0.885
0.900
0.901
0.774
0.803
0.810
0.817
0.811
0.847
0.872
0.895

80
3
Cross-Modal Hashing
respectively. For hash function learning, we have a parameter β to adjust the importance
of loss f 1 and loss f 2. In our experiment, for simplicity, we set λ1 =10, λ2 =1, β =0.01 on
three datasets. To construct the multi-modal collaborated graph, we introduce two weighting
factors α1, α2 and a hyper-parameter k to represent the noisy number to be ﬁltered. We set
α1 = 0.6, α2 = 0.6 and k = 3,000 on MIR Flickr; α1 = 0.5, α2 = 0.5 and k = 3,300 on
NUS-WIDE; α1 = 0.1, α2 = 0.3 and k = 3,000 on MS COCO. We adopt Adam optimization
algorithm [48] and set the learning rate of the hash code and hash function learning to 0.001
and 0.0001 respectively on MIR Flickr, 0.0001 and 0.0001 respectively on NUS-WIDE,
0.0005 and 0.001 respectively on MS COCO. The batch size is ﬁxed to 512. The iteration
number is set to 60, 120 and 100 on MIR Flikcr, NUS-WIDE and MS COCO, respectively.
Note that, with the same experimental setting, we directly use MAP@50 results provided
in the original papers of the unsupervised cross-modal hashing baselines, and carefully
conduct the experiments of all methods on MS COCO and record their results.4 Since the
source code of UDCMH is not released, we ﬁll the table with ‘—’. Since the experimental
settings of the supervised baselines are different from that of the unsupervised baselines, we
carefully conduct all experiments on three benchmarks and record them. We run all methods
ﬁve times on a workstation with a CPU (Intel(R) Xeon(R) Gold @2.30GHz) and a GPU
(TITAN RTX) and report their averaged results.
Retrieval Accuracy Comparison
We carefully perform the experiment on three datasets with the code length of 16 bits, 32 bits,
64 bits, 128 bits and list the main results in Table3.8. Observing these results, we have the
following analysis: (1) The proposed method CIRH achieves the best retrieval accuracy with
hash code length varying from 16 bits to 128 bits. Speciﬁcally, on I→T and T→I retrieval
tasks, the retrieval accuracy achieved by our method outperforms that of the best deep
baseline AGCH by 2.5%∼4% and 1.2%∼3.8% on MIR Flickr, respectively, by 0.6%∼2.2%
and 0.5%∼1.8% on NUS-WIDE, respectively, and by 4.1%∼5.6% and 6.5%∼7.3% on MS
COCO, respectively. The superior performance is attributed to the reason that the proposed
CIRH can capture and preserve the multi-modal semantics better into hash codes with joint
correlation-identity semantic reconstruction.
(2) Further, we plot the precision-recall and precision-topN curves on three benchmarks
with the code length of 128 bits and present them in Figs.3.10 and 3.11, respectively. From
the ﬁgures, we can clearly observe that the retrieval accuracy achieved by our method is
always higher than that of all compared baselines. The results demonstrate that the proposed
CIRH can retrieve more correct instances, which is consistent with MAP@50 results on
proving the superiority of our method.
Efﬁciency Comparison
To verify the efﬁciency of our method, we compare the training and hash code generation
time of CIRH with several deep unsupervised cross-modal hashing baselines DJSRH, JDSH,
AGCH on three benchmarks with the code length ﬁxed as 128 btis. Table3.9 presents the
4 No results are reported in compared baselines on MS COCO.

3.4
Correlation-Identity Reconstruction Hashing
81
Fig.3.10 The precision-recall curves of compared approaches on MIR Flickr, NUS-WIDE and MS
COCO with code length of 128 bits
Fig.3.11 The recision-topN curves of all compared approaches on MIR Flickr, NUS-WIDE and MS
COCO at code length of 128 bits

82
3
Cross-Modal Hashing
Table3.9 Thecomparisonresultsoftrainingtimeandhashcodegenerationtimeonthreebenchmarks
with 128 bits. The results are recorded in seconds
Methods
Training time (s)
MIR Flickr
NUS-WIDE
MS COCO
DJSRH
1137.24
1135.81
1335.27
JDSH
1256.04
1422.86
2510.21
AGCH
313.76
328.56
439.79
Ours
49.32
117.26
101.16
Methods
Hash code generation time (s)
MIR Flickr
NUS-WIDE
MS COCO
DJSRH
373.14
523.47
657.33
JDSH
14.63
116.35
236.18
AGCH
23.89
201.61
354.14
Ours
14.22
117.84
241.47
efﬁciency comparison results. We can ﬁnd from the table that our method is 3∼4 times
faster compared to the second best method at the training stage, and consumes comparable
hash code generation time at the retrieval stage. The efﬁciency improvement of our method
is mainly attributed to the reasons as follows: (1) The updating process for the parameters
of feature extractor is removed. Our method only needs to update the parameters in the
subsequent learning modules. (2) Our method only needs a few iterations to reach a stable
convergence state. (3) Our model is lightweight, less parameters are required to optimize.
To prove the analysis (2), we record MAP variations with the epoch on MIR Flickr
and report them in Fig.3.12. According to the results, we can ﬁnd that CIRH can achieve
the best retrieval performance after about 50 epoches. These experimental results verify
the efﬁciency of the training process of our method, which are consistent with our above
analysis.
Ablation Experiments
In this section, we perform ablation studies to verify the effect of each learning component
in the proposed CIRH. Table3.10 lists all the experimental results on three datasets. Specif-
ically, ‘CIRH-*’ means removing the component ‘*’ from the proposed CIRH. From this
table, we arrive at the following analyses:
• CIRH-ACR: CIRH-ACR represents that we remove the adjacency correlation recon-
struction loss in CIRH. Observing the results in Table3.10, we can ﬁnd that our method
achieves better retrieval performance than this variant. This comparison result proves that
the proposed ACR loss indeed improve retrieval performance by preserving the adjacency
multi-modal correlation into the hash codes.

3.4
Correlation-Identity Reconstruction Hashing
83
10 20 30 40 50 60 70 80 90 100 110 120
 Epoch
0.825
0.84
0.855
0.87
0.885
0.9
0.915
0.93
0.945
 MAP
MIR Flickr@16 bits
I2T
T2I
10 20 30 40 50 60 70 80 90 100 110 120
 Epoch
0.825
0.84
0.855
0.87
0.885
0.9
0.915
0.93
0.945
 MAP
MIR Flickr@32 bits
I2T
T2I
10 20 30 40 50 60 70 80 90 100 110 120
Epoch
0.825
0.84
0.855
0.87
0.885
0.9
0.915
0.93
0.945
 MAP
MIR Flickr@64 bits
I2T
T2I
10 20 30 40 50 60 70 80 90 100 110 120
 Epoch
0.825
0.84
0.855
0.87
0.885
0.9
0.915
0.93
0.945
 MAP
MIR Flickr@128 bits
I2T
T2I
Fig.3.12 The MAP variations with the number of epochs on MIR Flickr at various code lengths
Table 3.10 Experimental results of our method and all variants on MIR Flickr, NUS-WIDE and MS
COCO. Note that ‘CIRH-*’ means to remove the * component from CIRH
Methods
MIR Flickr
NUS-WIDE
MS COCO
I→T
T→I
I→T
T→I
I→T
T→I
CIRH-ACR
0.882
0.864
0.781
0.761
0.742
0.763
CIRH-ISR
0.889
0.848
0.806
0.767
0.677
0.695
CIRH-CSA
0.874
0.851
0.793
0.768
0.760
0.793
CIRH-MCGC
0.881
0.856
0.793
0.766
0.772
0.790
CIRH-GCN
0.887
0.864
0.803
0.765
0.766
0.788
CIRH-FUN
0.891
0.859
0.797
0.768
0.777
0.789
CIRH
0.901
0.867
0.815
0.774
0.797
0.811
• CIRH-ISR: CIRH-ISR is a variant that removes the identity semantic reconstruction com-
ponent, and formally remove the coding consistency reconstruction loss in Eq.(3.29).
Clearly, we can observe that this variant has a signiﬁcant performance degradation com-
pared with our method, especially on MS COCO. The results show that our ISR compo-
nent can effectively preserve the semantic information into ﬁnal hash codes.

84
3
Cross-Modal Hashing
• CIRH-CSA: To observe the actual effect of the proposed cross-modal semantic aggre-
gation component in CIRH, we delete Eq.(3.30) and use a FC layer to replace the CSA
component. We term this variant as CIRH-CSA and perform the comparison experiments
with our method. The comparison results in Table3.10 validate that the CSA component
can improve the retrieval performance of the framework by strengthening the interaction
between heterogeneous features.
• CIRH-MCGC: CIRH-MCGC denotes the variant method that replaces the multi-modal
collaborated graph in CIRH by S = αSI + (1 −α)ST . The experimental results on three
benchmarks prove that our MCGC component can capture more accurate similarity infor-
mation between instances with the constructed multi-modal collaborated graph to super-
vise model training, and ﬁnally obtain higher retrieval accuracy.
• CIRH-GCN: CIRH-GCN represents a variant method after removing all GCN compo-
nents in CIRH. We can clearly ﬁnd that CIRH-GCN achieves lower retrieval performance
compared to our method, and higher performance compared to CIRH-CSA. These results
also verify that: (1) the GCN components can improve retrieval performance by exploiting
the adjacency relations between instances, (2) the preservation of inter-modal adjacency
relations is extremely important in cross-modal retrieval, while preserving intra-modal
adjacency relations only may introduce intra-modal noises, which destroys the inter-
modal semantic alignment and thus degrades the model performance.
• CIRH-FUN: To verify the effect of the proposed correlation-identity consistent hash
function learning strategy, we introduce a variant by replacing Eq.(3.36) with ∥B −
fI (XI ; θi)∥2
F + ∥B −fT (XT ; θt)∥2
F. The comparison results on three datasets verify
that the modelled multi-modal semantics can indeed improve the retrieval performance.
• CIRH-Sigmoid(·): To validate the effects of Sigmoid(·), in this section, we design a
variant method CIRH-Sigmoid(·) by removing the function Sigmoid(·) in Eq.(3.26),
and evaluate its performance on NUS-WIDE and MS COCO. The experimental results
are shown in Table3.11. From the comparison results, we can ﬁnd that CIRH obtains
higher retrieval performance than the variant method. The comparison results verify that
Sigmoid(·) can indeed improve the retrieval performance by strengthening the adjacency
correlation, which further validates our theoretical analysis mentioned in Eq.(3.26).
Parameter Sensitivity Analysis
In this section, we analyze the sensitivity of six hyper-parameters involved in the model:
α1, α2, k, λ1, λ2, β. Speciﬁcally, the ﬁrst three hyper-parameters are used to construct the
multi-modal collaborated graph, and the last three are used to balance the importance of
each component when the model learns hash codes and functions. We test the sensitivity of
α1, α2, λ1 and λ2 from 0.1 to 1.0 with a step size of 0.1, k from 0 to 4000 with a step size of
1000, and β from 0.001 to 100 with a 10-fold increase. On MIR Flickr with code length of
128 bits, we evaluate the retrieval performance of all hyper-parameters and report the results
in Fig.3.13. According to these experimental results, our observations are as follows: (1)
the multi-modal collaborated graph is insensitive to α1 and α2. (2) the process of the hash

3.5
Summary
85
Table 3.11 Comparison results of MAP@50 on I→T and T→I retrieval tasks between out method
and w/o S
Task
Methods
NUS-WIDE
MS COCO
16 bits 32 bits 64 bits 128
bits
16 bits 32 bits 64 bits 128
bits
I→T
CIRH-Sigmoid(·)
0.806
0.832
0.848
0.856
0.762
0.813
0.828
0.845
CIRH
0.815
0.836
0.854
0.862
0.797
0.819
0.830
0.849
T→I
CIRH-Sigmoid(·)
0.769
0.795
0.805
0.816
0.786
0.845
0.867
0.885
CIRH
0.774
0.803
0.810
0.817
0.811
0.847
0.872
0.895
Fig. 3.13 Parameter sensitivity analysis for I→T and T→I retrieval tasks on MIR Flickr at code
length of 128bits
code learning is also insensitive to λ1 and λ2, which means that the proposed CIRH can
also achieve the signiﬁcant accuracy in a wide range of value. (3) k is very important for
supervised model learning because it directly determines how many adjacencies are ﬁltered
out. The performance of our model reaches the best when it is equal to 3000. (4) Our method
is stable when β varies in a wide range.
3.5
Summary
In this chapter, we ﬁrst describe the background and existing works of cross-modal hash-
ing, and then study the supervised and unsupervised cross-modal hashing respectively. For
supervised cross-modal hashing, we propose a Hierarchical Message Aggregation Hashing

86
3
Cross-Modal Hashing
(HMAH) within an efﬁcient teacher-student learning framework. On the teacher end, we
develop hierarchical message aggregation networks to seek a multi-modal complementary
space where the multi-modal correlations can be more effectively modelled. On the student
end, we learn a couple of hash functions to support the cross-modal retrieval. Besides, we
design a cross-modal correlation knowledge distillation technology to seamlessly transfer
the modelled ﬁne-grained multi-modal semantic correlations from the teacher module to
the lightweight student modules. Extensive experiments demonstrate the signiﬁcant perfor-
mance improvement of HMAH on both retrieval accuracy and model training efﬁciency.
Also, we propose a Correlation-Identity Reconstruction Hashing (CIRH) method for
unsupervised cross-modal retrieval. With the reconstruction strategy, our method jointly
preserves the multi-modal correlation and identity semantics into binary hash codes. A
multi-modal collaborated graph is designed to model the intrinsic multi-modal correlation
semantics. The heterogeneous modality gap is reduced by performing cross-modal semantic
aggregation on a heterogeneous graph convolutional network. The correlation semantics are
preserved into hash codes by minimizing the corresponding reconstruction loss. Simultane-
ously, we propose an identity semantic reconstruction component to make the generated hash
codes involve the identity semantics by reconstructing the input modality features. Finally,
a correlation-identity consistent hash function learning strategy is proposed to transfer the
modelled multi-modal semantics into the neural networks of modality-speciﬁc deep hash
functions. Experimental results on three widely evaluated benchmarks prove the superior
performance of the proposed method.
References
1. Peng, Y., Huang, X., & Zhao, Y. (2018). An overview of cross-media retrieval: Concepts, Method-
ologies, benchmarks, and challenges. IEEE Transactions on Circuits and Systems for Video
Technology, 28(9), 2372–2385.
2. Wang, J., Zhang, T., Sebe, N., & Shen, H. T. (2018) A survey on learning to hash. IEEE Trans-
actions on Pattern Analysis and Machine Intelligence, 40(4), 769–790.
3. Zhou, J., Ding, G., & Guo, Y. (2014). Latent semantic sparse hashing for cross-modal similarity
search. In Proceedings of the International ACM SIGIR Conference on Research and Develop-
ment in Information Retrieval (pp. 415–424).
4. Jiang, Q. Y., & Li, W. J. (2017). Deep cross-modal hashing. In Proceedings of the IEEE Confer-
ence on Computer Vision and Pattern Recognition (pp. 3232–3240).
5. Su, S., Zhong, Z., & Zhang, C. (2019). Deep joint-semantics reconstructing hashing for large-
scale unsupervised cross-modal retrieval. In Proceedings of IEEE International Conference on
Computer Vision (pp. 3027–3035).
6. Zhang, P.-F., Li, Y., Huang, Z., & Xin-Shun, X. (2022). Aggregation-based graph convolutional
hashing for unsupervised cross-modal retrieval. IEEE Transactions on Multimedia, 24(2022),
466–479.
7. Tang, J., Wang, K., & Shao, L. (2016). Supervised matrix factorization hashing for cross-modal
retrieval. IEEE Transactions on Image Processing, 25(7), 3157–3166.

References
87
8. ZChen, Z. D., Li, C. X., Luo, X., Nie, L., Zhang, W., & Xu, X. S. (2020). SCRATCH: A scalable
discrete matrix factorization hashing framework for cross-modal retrieval. IEEE Transactions
on Circuits and Systems for Video Technology, 30(7), 2262–2275.
9. Singh, A. P., & Gordon, G. J. (2008). Relational learning via collective matrix factorization. In
Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery and Data
Mining (pp. 650–658).
10. Liu, X., Hu, Z., Ling, H., & Cheung, Y. M. (2019). MTFH: A matrix tri-factorization hash-
ing framework for efﬁcient cross-modal retrieval. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 43(3), 964–981.
11. Meng, M., Wang, H., Jun, Yu., Chen, H., & Jigang, W. (2020). Asymmetric supervised con-
sistent and speciﬁc hashing for cross-modal retrieval. IEEE Transactions on Image Processing,
30(2020), 986–1000.
12. Wang, Y., Luo, X., Nie, L., Song, J., Zhang, W., & Xu, X. S. (2021). BATCH: A scalable asym-
metric discrete cross-modal hashing. IEEE Transactions on Knowledge and Data Engineering,
33(11), 3507–3519.
13. Krizhevsky, A., Sutskever, I., & Hinton, G. E. (2012). ImageNet classiﬁcation with deep convo-
lutional neural networks. In Proceedings of Advances in Neural Information Processing Systems
(pp. 1106–1114).
14. He, K., Zhang, X., Ren, S., & Sun, J. (2016). Deep residual learning for image recognition. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 770–778).
15. Simonyan, K., & Zisserman, A. (2015). Very deep convolutional networks for large-scale image
recognition. In Proceedings of the International Conference on Learning Representations.
16. Nie, X., Wang, B., Li, J., Hao, F., Jian, M., & Yin, Y. (2020). Deep multiscale fusion hashing for
cross-modal retrieval. IEEE Transactions on Circuits and Systems for Video Technology, 31(1),
401–410.
17. Xie, D., Deng, C., Li, C., Liu, X., & Tao, D. (2020). Multi-task consistency-preserving adversarial
hashing for cross-modal retrieval. IEEE Transactions on Image Processing, 29(2020), 3626–
3637.
18. Creswell, A., White, T., Dumoulin, V., Arulkumaran, K., Sengupta, B., & Bharath, A. A. (2014).
Generative adversarial nets. In Proceedings of the Advances in Neural Information Processing
Systems (pp. 2672–2680).
19. Xu, R., Li, C., Yan, J., Deng, C., & Liu, X. (2019). Graph convolutional network hashing for cross-
modal retrieval. In Proceedings of the International Joint Conference on Artiﬁcial Intelligence
(pp. 982–988).
20. Bruna, J., Zaremba, W., Szlam, A., & LeCun, Y. (2014). Spectral networks and locally connected
networks on graphs. In Proceedings of the International Conference on Learning Representa-
tions.
21. Kipf, T. N., & Welling, M. (2017). Semi-supervised classiﬁcation with graph convolutional
networks. In Proceedings of the International Conference on Learning Representations (pp.
1–14).
22. Tu, R. C., Mao, X. L., Ma, B., Hu, Y., Yan, T., Wei, W., & Huang, H. (2022). Deep cross-modal
hashing with hashing functions and uniﬁed hash codes jointly learning. IEEE Transactions on
Knowledge and Data Engineering, 34(2), 560–572.
23. Weiss, Y., Torralba, A., & Fergus, R. (2008). Spectral hashing. In Proceedings of the Advances
in Neural Information Processing Systems (pp. 1753–1760).
24. Kumar, S., & Udupa, R. (2011). Learning hash functions for cross-view similarity search. In
Proceedings of the International Joint Conference on Artiﬁcial Intelligence (pp. 1360–1365).

88
3
Cross-Modal Hashing
25. Song, J., Yang, Y., Yang, Y., Huang, Z., & Shen, H. T. (2013). Inter-media hashing for large-scale
retrieval from heterogeneous data sources. In Proceedings of the ACM SIGMOD International
Conference on Management of Data (pp. 785–796).
26. Ding, G., Guo, Y., & Zhou, J. (2014). Collective matrix factorization hashing for multimodal
data. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (pp.
2075–2082).
27. Wang, W., Shen, Y., Zhang, H., Yao, Y., & Liu, L. (2020). Set and rebase: Determining the seman-
tic graph connectivity for unsupervised cross-modal hashing. In Proceedings of the International
Joint Conference on Artiﬁcial Intelligence (pp. 853–859).
28. Hu, D., Nie, F., & Li, X. (2018). Deep binary reconstruction for cross-modal hashing. IEEE
Transactions on Multimedia, 21(4), 973–985.
29. Wu, G., Lin, Z., Han, J., Liu, L., Ding, G., Zhang, B., Shen, J. (2018). Unsupervised deep
hashing via binary latent factor models for large-scalecross-modal retrieval. In Proceedings of
the International Joint Conference on Artiﬁcial Intelligence (pp. 2854–2860).
30. Liu, S., Qian, S., Guan, Y., Zhan, J., & Ying, L. (2020). Joint-modal distribution-based similarity
hashing for large-scale unsupervised deep cross-modal retrieval. In Proceedings of the Inter-
national ACM SIGIR Conference on Research and Development in Information Retrieval (pp.
1379–1388).
31. Goodfellow, I., Bengio, Y., & Courville, A. (2016). 6.5 back-propagation and other differentiation
algorithms. Deep Learning, 2016, 200–220.
32. Li, C., Deng, C., Li, N., Liu, W., Gao, X., & Tao, D. (2018). Self-supervised adversarial hashing
networks for cross-modal retrieval. In Proceedings of IEEE Conference on Computer Vision and
Pattern Recognition (pp. 4242–4251).
33. Huiskes, M. J., & Lew, M. S. (2008). The MIR Flickr retrieval evaluation. In Proceedings of the
ACM SIGMM International Conference on Multimedia Information Retrieval (pp. 39–43).
34. Chua, T. S., Tang, J., Hong, R., Li, H., Luo, Z., & Zheng, Y. (2009). NUS-WIDE: A real-
world web image database from National University of Singapore. In Proceedings of the ACM
International Conference on Image and Video Retrieval, 48(1–48), 9.
35. Lin, T.-Y., Maire, M., Belongie, S. J., Hays, J., Perona, P., Ramanan, D., et al. (2014). Microsoft
COCO: Common objects in context. In Proceedings of the European Conference on Computer
Vision, 8693, 740–755.
36. Wang, D., Gao, X., Wang, X., & He, L. (2015). Semantic topic multimodal hashing for cross-
media retrieval. In Proceedings of International Joint Conference on Artiﬁcial Intelligence (pp.
3890–3896).
37. Li, C., Deng, C., Wang, L., Xie, D., & Liu, X. (2019). Coupled CycleGAN: Unsupervised
hashing network for cross-modal retrieval. In Proceedings of the AAAI Conference on Artiﬁcial
Intelligence (pp. 176–183).
38. Li, N., Li, C., Deng, C., Liu, X., & Gao, X. (2018). Deep joint semantic-embedding hashing. In
Proceedings of the International Joint Conference on Artiﬁcial Intelligence (pp. 2397–2403).
39. Cui, H., Zhu, L., Li, J., Yang, Y., & Nie, L. [n. d.]. Scalable Deep Hashing for Large-Scale Social
Image Retrieval. ([n. d.]).
40. Lei Zhu, X. L., Cheng, Z., Li, J., & Zhang, H. (2020). Deep collaborative multi-view hashing
for large-scale image search. IEEE Transactions on Image Processing, 29(2020), 4643–4655.
41. Zhu, L., Lu, X., Cheng, Z., Li, J., & Zhang, H. (2020). Flexible multi-modal hashing for scalable
multimedia retrieval. ACM Transactions on Intelligent Systems and Technology, 11(2), 14:1–
14:20.
42. Zheng, C., Zhu, L., Lu, X., Li, J., Cheng, Z., & Zhang, H. (2020). Fast discrete collaborative
multi-modal hashing for large-scale multimedia retrieval. IEEE Transactions on Knowledge and
Data Engineering, 32(11), 2171–2184.

References
89
43. Zhang, D., & Li, W-J. (2014). Large-scale supervised multimodal hashing with semantic cor-
relation maximization. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence (pp.
2177–2183).
44. Lin, Z., Ding, G., Hu, M., & Wang, J. (2015). Semantics-preserving hashing for cross-view
retrieval. In Proceedings of IEEE Conference on Computer Vision and Pattern Recognition (pp.
3864–3872).
45. Jiang, Q. Y., & Li, W. J. (2019). Discrete latent factor model for cross-modal hashing. IEEE
Transactions on Image Processing, 28(7), 3490–3501.
46. Ma, X., Zhang, T., & Xu, C. (2020). Multi-level correlation adversarial hashing for cross-modal
retrieval. IEEE Transactions on Multimedia, 22(12), 3101–3114.
47. Chatﬁeld, K., Simonyan, K., Vedaldi, A., & Zisserman, A. (2014). Return of the Devil in the
Details: Delving Deep into Convolutional Nets.
48. Kingma, D. P., & Ba, J. (2015). Adam: A method for stochastic optimization. In Proceedings of
the International Conference on Learning Representations.
49. Cheng, M., Jing, L., & Ng, M. K. (2020). Robust unsupervised cross-modal hashing for multi-
media retrieval. ACM Transactions on Information Systems, 38(3), 1–25.
50. Hoang, T., Do, T.-T., Nguyen, T. V., & Cheung, N.-M. (2020). Unsupervised deep cross-modality
spectral hashing. IEEE Transactions on Image Processing, 29(2020), 8391–8406.
51. Ko, Y. (2012). A study of term weighting schemes using class information for text classiﬁcation.
In Proceedings of the International ACM SIGIR Conference on Research and Development in
Information Retrieval (pp. 1029–1030).
52. Lu, X., Zhu, L., Cheng, Z., Nie, L., & Zhang, H. 2019. Online multi-modal hashing with dynamic
query-adaption. In Proceedings of the International ACM SIGIR Conference on Research and
Development in Information Retrieval (pp. 715–724).
53. Hinton, G. E., Srivastava, N., Krizhevsky, A., Sutskever, I., & Salakhutdinov, R. R. (2012).
Improving neural networks by preventing co-adaptation of feature detectors. arXiv:1207.0580.

4
Composite Multi-modal Hashing
4.1
Background
Multi-modal data describe the multi-perception observed information of the object in the real
world from various aspects. Different from uni-modal [1–3] and cross-modal hashing [4–8]
mentioned in the above chapters, composite multi-modal hashing technology [9–15] focuses
on how to collaboratively fuse heterogeneous multi-modal features and perform the retrieval
among different instances based on fused features. Overall, existing methods follow the
projection-based hashing learning paradigm, in which an explicit mapping (hash) function
is learned to directly embed the multi-modal data into hash codes. The learning process is
mainly comprised of two stages: (1) Multi-modal modeling. Modality-speciﬁc semantics
are ﬁrst represented and subsequently fused by exploiting the complementarity of multi-
modal features. (2) Binary encoding. The learned composite multimodal representations are
encoded into hash codes.
Based on this pipeline, various composite multi-modal hashing methods are proposed in
the literature. Unsupervised composite multi-modal hashing mainly performs hash learning
with multiple graphs [9] or latent matrix factorization model [10–12]. In contrast, supervised
composite multi-modal hashing [13–15] mainly focuses on enhancing the discriminative
capability of hash codes with the supervision of semantic labels. From the perspective of
model architecture, shallow composite multi-modal hashing methods can solve the hash
functions quickly and handle the NP-hard binary optimization problem by discrete opti-
mization technology. Differently, deep-based models can fully learn non-linear feature rep-
resentations to enhance model performance. In this chapter, we propose a shallow composite
hashing method, named online multi-modal hashing with dynamic query-adaption, and a
deep-based framework to fully model the implicit semantic correlations among different
modalities, named bit-aware semantic transformer hashing.
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024
L. Zhu et al., Multi-modal Hash Learning, Synthesis Lectures on Information
Concepts, Retrieval, and Services, https://doi.org/10.1007/978-3-031-37291-9_4
91

92
4
Composite Multi-modal Hashing
4.2
Related Work
4.2.1
Unsupervised Composite Multi-modal Hashing
Compared with uni-modal hashing, the composite multi-modal hashing methods [9–12,
14, 16–18] generate binary representations for the efﬁcient multimedia retrieval system by
capturing the correlations among different modalities and performing multi-modal fusion.
Generally, composite multi-modal hashing methods can be divided into supervised and unsu-
pervised ones. Unsupervised methods directly models multi-modal correlations to learn hash
codes without any supervised semantic information. For example, Multiple Feature Hash-
ing (MFH) [9] exploits the local structure of each modality and considers these structures
together to learn multi-modal fused hash codes for near-duplicate video retrieval. Multiview
Alignment Hashing (MAH) [10] introduces kernelized nonnegative matrix factorization to
optimize the hash functions and learns the binary codes by exploiting the latent semantics
in different views and preserving joint probability distribution. Multi-view Latent Hashing
(MVLH) [11] learns the hash codes by the shared latent factors and designs an adaptive
weighting strategy to fuse all views. Multiview Discrete Hashing (MvDH) [12] ensures the
consistence between the hash codes learning by matrix factorization and the cluster labels
with spectral clustering to enhance the discrimination of the hash codes.
4.2.2
Supervised Composite Multi-modal Hashing
Different from the unsupervised composite multi-modal hashing methods, several super-
vised methods [14, 16, 19–21] have been proposed to generate more discriminative hash
codes by exploiting explicit semantic labels. Compact Kernel Hashing with Multiple Fea-
tures (MFKH) [13] formulates the multi-view feature learning framework as a similarity pre-
serving hashing problem with optimal linearly-combined multiple kernels. Discrete Multi-
view Hashing (DMVH) [14] exploits Locally Linear Embedding (LLE) [22] to construct
the afﬁnity matrix, which can preserve local similarity structure and the semantic similarity
between samples. Supervised Discrete Multi-view Hashing (SDMH) [15] learns the shared
hash codes by exploiting the complementary features among different views and removing
the redundant information. Flexible Online Multi-modal Hashing (FOMH) [23] proposes
an online composite multi-modal hashing framework, which can adaptively fuse heteroge-
neous multi-modal data in a self-weighted manner and learn the discriminative hash codes
with an asymmetric supervision strategy. Flexible Discrete Multi-view Hashing (FDMH)
[19] develops an adaptive dictionary learning strategy, which combines the shared collective
latent embeddings of multi-view data and semantic information to learn the discriminative
hash codes. Supervised Adaptive Partial Multi-view Hashing (SAPMH) [20] introduces a
parameter-free learning strategy to adjust the fusion weights of multi-modal features under
the partial multi-view scenario.

4.3
Online Multi-modal Hashing with Dynamic Query-Adaption
93
4.2.3
Deep Composite Multi-modal Hashing
Recently, several deep composite multi-modal hashing methods [16, 18, 21] are proposed to
generate multi-modal fused hash codes with deep learning. Deep Collaborative Multi-view
Hashing (DCMVH) [16] ﬁrst proposes a deep composite multi-modal hashing method to
perform multi-view feature fusion and learn the hash codes with the supervision of pairwise
semantic matrix. Flexible Graph Convolutional Multi-modal Hashing (FGCMH) [21] intro-
duces multiple Graph Convolutional Networks (GCNs) [24] to exploit the intra-modality
and the fusion-modality structural similarity, and capture the complementary correlations
among the different views when learning the hash codes.
4.3
Online Multi-modal Hashing with Dynamic Query-Adaption
4.3.1
Motivation
The exiting methods suffer from several important problems:
(1) Ineffective multi-modal modeling. Existing methods exploit linear or simple nonlinear
functions for hash projection. They cannot effectively capture the intrinsic multi-modal
data structure, which is important for modeling the multi-modal correlation. Besides,
they simply adopt ﬁxed modality weights to fuse heterogeneous multi-modal features.
This simple weighting scheme can exploit the complementarity of multi-modal features
for hash learning to some extent. However, it introduces an additional hyper-parameter
to avoid over-ﬁtting [12]. This parameter is conﬁrmed to be data-related and manually
determining the optimal value is unrealistic and inefﬁcient, especially at the online
query stage where the semantic labels of streaming queries are unknown (no semantic
supervision can be exploited to determine the best value).
(2) Binary optimization challenge. Hash codes are binary. Solving them is actually an NP-
hard optimization problem [12]. The pioneering composite multi-modal hashing meth-
ods [9, 10, 13] adopt relaxing optimization. They ﬁrst project the high-dimensional
features into a continuous isomorphic latent space by maximizing the modality corre-
lations, and then quantize the isomorphic embedding into binary hash codes by thresh-
olding. Their adopted strategy indeed simpliﬁes the optimization process, however, it
will cause signiﬁcant quantization errors [12]. Hence, several discrete composite multi-
modal hashing models [11, 12] are recently proposed to learn the binary hash codes
without relaxing by discrete optimization. Nevertheless, they perform the discrete hash
code learning process with Discrete Cyclic Coordinate descent (DCC) [2, 11, 12], where
hash codes are learned bit-by-bit. Under such learning circumstance, the optimization
process consumes considerable computation and storage cost, which is unacceptable
in large-scale multimedia retrieval scenarios.

94
4
Composite Multi-modal Hashing
To address the above mentioned problems, we propose a composite multi-modal hash-
ing method with online query-adaption in supervised learning paradigm, named as Super-
vised Multi-modal Hashing with Online Query-adaption (SMH-OQA). In this method, a
self-weighted fusion strategy is designed to adaptively preserve the multi-modal feature
information into hash codes by exploiting their complementarity. Besides, the hash codes
are learned with the supervision of pair-wise semantic labels to enhance their discriminative
capability while avoiding the challenging symmetric similarity matrix factorization. Under
such a learning framework, the binary hash codes are directly obtained with efﬁcient (both
computation and storage) operations and without quantization errors.
In addition, considering the semantic labeling is expensive in the real multimedia retrieval
system, we further extend SMH-OQA to an unsupervised learning paradigm and propose
an Unsupervised Multi-modal Hashing with Online Query-adaption (UMH-OQA) method.
Speciﬁcally,were-takethequantizationstrategytoaddressthebinaryoptimizationchallenge
of composite multi-modal hashing. By leveraging the quantization strategy, we could reduce
the binary quantization errors instead of devoting much carefulness to designing special
hash learning formulations to support discrete optimization. The core idea of quantization
strategy is to learn a representative codebook (comprised of prototypes from the database)
to characterize the intrinsic data structure, and then generate the binary codes by measuring
the similarities to these binary prototypes. Despite the success, no quantization strategy has
been explored for composite multi-modal hashing as far as we know. It seems possible to
directly borrow the existing quantization strategies from cross-modal hashing to composite
multi-modal hashing. However, composite multi-modal hashing collaborates multi-modal
features at both model learning and online query stages to learn binary hash codes [9–15].
The learning objective is different from cross-modal hashing which targets at discovering the
shared hash space for supporting the retrieval tasks across different modalities. Also, directly
applying the quantization strategy for uni-modal hashing with the concatenated multi-modal
features is also not optimal, as it fails to effectively exploit the complementarity of multi-
modal features, which is important for multi-modal fusion. In this article, we ﬁrst extract the
heterogeneous multi-modal features and apply matrix factorization to seek an orthogonal
semantic space for each modality. Speciﬁcally, to ensure training efﬁciency, we approximate
themulti-modallatentrepresentationsusingasetofrepresentativecodebooks,thehashcodes
to be learned, and the modality fusion weights. Finally, the hash codes are learned with an
efﬁcient iterative optimization method. Note that, in our proposed methods, to accurately
capture the query variations at the online retrieval stage, we design a parameter-free online
hashing module which can adaptively learn the query hash codes according to the dynamic
query contents.
To sum up, the main contributions are summarized as follows:
• We develop a composite multi-modal hashing method with online query-adaption in
both unsupervised and supervised learning paradigms to learn the multi-modal feature
fused hash codes. Instead of adopting the ﬁxed modality combination weights to generate

4.3
Online Multi-modal Hashing with Dynamic Query-Adaption
95
online query hash codes as existing composite multi-modal hashing methods, we propose
a query-adaptive and self-weighted online hashing module to accurately capture the
variations of different queries. Moreover, the online module is parameter-free. It could
avoid time-consuming and inaccurate parameter adjustment in the unsupervised query
hashing process.
• In Supervised Multi-modal Hashing with Online Query-adaption (SMH-OQA) method,
we develop an efﬁcient hash code learning module to simultaneously correlate the learned
hash codes with low-level data distribution and high-level semantics. In particular, this
design not only enhances the discriminative capability of hash codes with pair-wise
semantics, but also avoids the challenging symmetric semantic matrix factorization and
storage cost of the semantic graph.
• In Unsupervised Multi-modal Hashing with Online Query-adaption (UMH-OQA)
method, we design a new adaptive multi-modal quantization strategy to automatically
compute adaptive modality combination weights according to the varied multimedia
contents. We learn complementary multi-modal prototypes to effectively characterize
the complex multi-modal correlation. With them, the binary quantization errors can be
alleviated without the reliance on particular hash learning formulations.
4.3.2
Problem Formulation
In this work, we target at dealing with the problem of multimedia retrieval. Our com-
posite multi-modal hashing with the online query-adaption model mainly consists of two
components: ofﬂine training and online multimedia retrieval. Ofﬂine training stage aims
to learn hash function for out-of-sample queries. Suppose O = {oi}N
i=1 is the training
dataset, which contains N training samples represented with L different modality features.
Xl = [xl
1, xl
2, ..., xl
N] ∈Rdl×N is the feature matrix in the lth modality. Here, l = 1, .., L
and dl is the corresponding feature dimension. Our composite multi-modal hashing models
aim to learn hash codes B ∈{−1, 1}r×N to represent multimedia data samples, where r is
the hash code length. At the multimedia retrieval stage, when a new query comes, its binary
codes can be easily generated by the learned hash function and then be utilized for fast
retrieval.
Additionally, in this work, we use boldface lowercase and uppercase characters to denote
the vectors and matrices, respectively. M is the number of modalities. Different modalities
of one sample share the same semantic, that is, they belong to the same category. S ∈
{−1, 1}N×N is a pair-wise semantic matrix, its element in the ith row and jth column is
deﬁned as
si j =

1, if xi and x j belong to the same category,
−1,
vice versa.
(4.1)

96
4
Composite Multi-modal Hashing
where E denotes a matrix with all elements as 1. 0 denotes a vector or matrix with all
elements as 0. tr(·) denotes the trace operation on a matrix. It is calculated as the sum of
diagonal values of the corresponding matrix. || · ||F denotes Frobenius norm. △k = {x ∈
Rk | xi ≥0, 1Tx = 1} is the probabilistic simplex. To facilitate the illustration, we list the
main notations used throughout this work in Table4.1.
Table 4.1 Main notations used in this work
Notation
Description
Xl ∈Rdl×N
Feature matrix of the lth modality data
B ∈Rr×N
Hash codes
Xlq ∈Rdl×nq
Feature matrix of the lth modality data for the query instances
Bq ∈Rnq×r
Hash codes for the query instances
dl
The dimension of the lth modality feature
N
The number of training samples
r
Hash code length
L
The number of modalities
μl
The adaptive weight of the lth modality data
nq
The number of the query samples
ϕ(Xl) ∈Rp×n
Nonlinearly transformed lth modality feature in SMH-OQA
˜Y ∈Rc×N
Instance-wise semantic matrix
S ∈RN×N
Pair-wise semantic matrix
Ul ∈Rp×r
Basis matrix in SMH-OQA
G ∈Rr×c
Semantic transfer matrix in SMH-OQA
W ∈Rr×p
Linear projection matrix in SMH-OQA
c
The number of semantic categories
p
The number of anchor points
C ∈RD×MK
Codebooks in UMH-OQA
Rl ∈Rdl×D
Transformation matrix of the lth modality data in UMH-OQA
Hl ∈RD×N
Basis matrix of the lth modality data in UMH-OQA
Rlq ∈Rdl×D
Transformation matrix of the lth modality data for the query instances
Hlq ∈RD×N
Basis matrix of the lth modality data for the query instances
D
The dimension of modality-consistent latent space
M
The number of codebooks
K
The number of prototypes in each codebook

4.3
Online Multi-modal Hashing with Dynamic Query-Adaption
97
4.3.3
Supervised Multi-modal Hashing with Online Query-Adaption
In this section, we introduce our Supervised Multi-modal Hashing with Online Query-
adaption (SMH-OQA) model to tackle the problems in existing composite multi-modal
hashing methods: (1) ineffective multi-modal modeling, (2) binary optimization challenge,
and (3) limited semantic or scalability. Figure4.1 summarizes the sketch of the proposed
model. In the following subsections, we describe this model in detail.
Consensus Multi-modal Feature Mapping
To learn effective binary projection for multi-modal data, the projected features should
comprehensively preserve multi-modal information. The ﬁrst task is to model the multi-
modaldatainformationsothatthebinaryprojectionlearningcanbeperformed.Mostexisting
methods [9, 10, 25, 26] construct graphs to accomplish this task. The graph construction
Fig. 4.1 The basic framework of the proposed SMH-OQA-based multimedia retrieval system. At
the ofﬂine model learning stage, we design a self-weighted fusion strategy to adaptively preserve
the multi-modal feature information into hash codes by exploiting their complementarity. The hash
codes are learned with the supervision of pair-wise semantic labels to enhance their discriminative
capability, while avoiding the challenging symmetric similarity matrix factorization. At the online
retrieval stage, we design a parameter-free online hashing module which can adaptively learn the
query hash codes according to the dynamic query contents

98
4
Composite Multi-modal Hashing
process costs O(N 2) computation and storage complexity, which is practically unacceptable
for large-scale multimedia retrieval. In SMH-OQA, we propose an efﬁcient consensus multi-
modal feature mapping to reduce the complexity to O(N). Given the lth modality feature
Xl = [xl
1, ..., xl
N] ∈Rdl×N, we ﬁrst obtain the nonlinearly transformed representation ϕ(xl
i)
as [exp( ∥xl
i−al
1∥2F
2σ2
l
), ..., exp(
∥xl
i−al
p∥2
F
2σ2
l
)]T, where {al
1, ..., al
p} are p anchors that are randomly
selected from the training samples in the lth modality, σl is the Gaussian kernel parameter.
ϕ(Xl) = [ϕ(xl
1), ..., ϕ(xl
N)] ∈Rp×N preserves the modality-speciﬁc sample correlations
by simply characterizing the correlations between the sample and certain anchors.
In multimedia retrieval, the heterogeneous modality gap and inter-modality redundancy
in multi-modal data are detrimental to hashing learning. In this work, we propose to col-
laboratively project the nonlinearly transformed representation ϕ(Xl)|L
l=1 into a consensus
multi-modal factor H ∈Rr×N as the hash code learning basis. Motivated by these consid-
erations, we can formulate this part as
min
μl,Wl,H
L

l=1
μl∥H −Wlϕ(Xl)∥2
F + ζ∥µ∥2
F, s.t. µ = [μ1, μ2, ..., μl]T, µ ∈L,
(4.2)
where Wl ∈Rr×p,l = 1, .., L is the projection matrix of the lth modality, H ∈Rr×N is the
consensus multi-modal factor, μl is the weight of the lth modality and it measures the impor-
tance of modality feature. To combine multiple features, Eq.(4.2) adopts a weight parameter
μl to explore the complementarity among all modalities for hash learning and an additional
hyper-parameter ζ to balance between the regularization terms to avoid over-ﬁtting. As
mentioned above, the ﬁxed weights cannot capture the content variation of multi-modal
contents, which are streaming and dynamic in real retrieval systems. However, the adjust-
ment process for weight-related hyper-parameter is unrealistic and inefﬁcient, especially at
the online query stage where the semantic labels of queries are unknown. We introduce a
virtual weight to achieve the same goal as Eq.(4.2) without additional hyper-parameters.
Speciﬁcally, we formulate this part as
min
Wl,H
L

l=1
∥H −Wlϕ(Xl)∥F.
(4.3)
We derive the following theorem.
Theorem 4.1 Equation(4.3) is equivalent to
min
µ∈L,Wl,H
L

l=1
1
μl ∥H −Wlϕ(Xl)∥2
F.
(4.4)
The detailed proof for this theorem is shown in Appendix A. As shown in the computation
formula for μl|L
l=1, μl can be considered as a virtual weight, which plays the role of the real

4.3
Online Multi-modal Hashing with Dynamic Query-Adaption
99
weight. Theoretically, if the lth modality is discriminative, the corresponding virtual weight
1
μl of the lth modality is large. Similarly, a weak modality will be assigned with a small
weight. This proves the usefulness of the virtual weight.
Asymmetric Supervised Learning
In this work, we further leverage explicit semantic labels to guide the projection learning
process and thus to enhance the discriminative capability of hash codes. Let B ∈{−1, 1}r×N
denote the hash codes to be learned. Intuitively, it can be learned by preserving the high-level
semantic correlations (described by similarity matrix S) into the hash codes
min
B ∥rS −BTB∥2
F, s.t. B ∈{−1, 1}r×N,
(4.5)
where S ∈RN×N is the pair-wise similarity matrix. However, directly solving discrete B
in Eq.(4.5) is very challenging due to the discrete symmetric factorization, not to men-
tion the fast hashing optimization. Furthermore, storing the elements of S will consume
O(N 2), which is unacceptable in large-scale multimedia retrieval. In this work, to avoid
these problems, we develop an asymmetric hashing learning module that transfers seman-
tics from pair-wise semantic matrix S to hash codes. Speciﬁcally, we substitute one of B
with the rotated consensus multi-modal factor RH (R ∈Rr×r is rotation matrix) and keep
their consistency during the optimization process. The formula is
min
B,R,H ∥rS −BTRH∥2
F + β∥B −RH∥2
F, s.t. B ∈{−1, 1}r×N, RTR = Ir.
(4.6)
This formulation has two advantages: (1) The symmetric matrix factorization can be obvi-
ously avoided. Only one of the decomposed variable is imposed with discrete constraint. The
second regularization term can guarantee the acceptable information loss. (2) The learned
hash codes can not only reﬂect the low-level multi-modal data distribution via H, but also
involve the high-level semantics in S. As shown below, with the support of asymmetric
hashing learning, the hash codes can be learned with a simple sgn(·) operation instead of
bit-by-bit discrete optimization as existing discrete composite multi-modal hashing meth-
ods. In addition, as shown below, the O(N 2) storage cost brought by S can be reduced to
O(N) when representing S with the label matrix in our approach.
By integrating the above two parts into a uniﬁed learning framework, we derive the overall
objective function of hash code learning in SMH-OQA as
min
Wl,H,B,R
L

l=1
∥H−Wlϕ(Xl)∥F+α∥rS−BTRH∥2
F+β∥B−RH∥2
F+δ
L

l=1
∥Wl∥2
F,
s.t. B∈{−1, 1}r×N, RTR=Ir,
(4.7)
where α, β, and δ are balance parameters. The ﬁrst term performs consensus multi-modal
feature mapping to combine multiple modalities, bridge the heterogeneous modality gap,

100
4
Composite Multi-modal Hashing
and avoid inter-modality redundancy. The second and the third terms perform asymmetric
supervised hashing learning. The last term is a regularization term to avoid over-ﬁtting.
Iterative Optimization in SMH-OQA
Solving hash codes is actually an NP-hard problem due to the discrete constraint. It is always
a challenging task from the birth of this technique. Most existing composite multi-modal
hashing methods [9, 10, 13, 25, 26] adopt two-step relaxing+rounding optimization strategy.
They basically solve the relaxed continuous solutions ﬁrst and then calculate the binary hash
codes by thresholding. However, this simpliﬁed strategy will lead to signiﬁcant quantization
loss. Although there exist discrete multi-modal hash methods [12], they learn the hash codes
bit-by-bit with DCC, which is still time-consuming.
In this work, with the support of objective formulation, we propose to directly learn the
discrete multi-modal hash code. Moreover, different from existing composite multi-modal
hashing methods, we avoid explicitly computing the similarity matrix S, which can achieve
linear computation and storage efﬁciency. According to Theorem 4.1, Eq.(4.7) is equivalent
to the following problem
min
μl,Wl,R,H,B
L

l=1
1
μl ∥H −Wlϕ(Xl)∥2
F + α∥rS −BTRH∥2
F + β∥B −RH∥2
F + δ
L

l=1
∥Wl∥2
F,
s.t. B ∈{−1, 1}r×N, RTR = Ir, µ ∈L.
(4.8)
Besides, we propose a new and effective optimization algorithm based on Augmented
Lagrangian Multiplier (ALM) [27] to solve the problem in Eq.(4.8). Our idea is to introduce
auxiliary variables to separate constraints, and transform the objective function to an equiv-
alent one that can be tackled more easily. Formally, we introduce two auxiliary variables
ZR and ZB, and set ZR = R and ZB = B. Equation(4.8) is transformed as
min
L

l=1
1
μl ∥H −Wlϕ(Xl)∥2
F + α∥rS −BTRH∥2
F + β∥B −RH∥2
F+
δ
L

l=1
∥Wl∥2
F + λ
2 (∥R −ZR + GR
λ ∥2
F + ∥B −ZB + GB
λ ∥2
F),
s.t. B, ZB ∈{−1, 1}r×N, RTR = Ir, ZT
RZR = Ir, µ ∈L,
(4.9)
where GR ∈Rr×r and GB ∈Rr×N measure the difference between the target and auxiliary
variables, λ > 0 adjusts the balance between terms. Speciﬁcally, we drive the following
iterative optimization steps to solve Eq.(4.9). The overall learning process of SMH-OQA is
described in Algorithm 4.1.

4.3
Online Multi-modal Hashing with Dynamic Query-Adaption
101
Step 1: Update μl. For convenience, we denote ∥H −Wlϕ(Xl)∥F by hl. The original
problem can be written as:
min
μl≥0,1Tµ=1
L

l=1
(hl)2
μl ,
(4.10)
which combining with Cauchy-Schwarz inequality gives
L

l=1
(hl)2
μl
(a)
= (
L

l=1
(hl)2
μl )(
L

l=1
μl)
(b)
≥(
L

l=1
hl)2,
(4.11)
Algorithm 4.1 Key steps of Supervised Multi-modal Hashing with Online Query-adaption
(SMH-OQA)
Require: Training set {Xl ∈Rdl×N }L
l=1, pair-wise semantic matrix S ∈RN×N , hash code length
r, the number of anchor points p, parameters α, β, and δ.
Ensure: Projection matrix Wl ∈Rr×p.
1: Initialize μl = 1
L .
2: Initialize B and ZB as {−1, 1}r×N randomly.
3: Randomly initialize R and ZR.
4: Initialize {Wl}L
l=1 by Eq.(4.13).
5: Randomly select the anchor point set in each modality.
6: Construct the nonlinearly transformed representation ϕ(Xl).
7: repeat
8:
Update μl by Eq.(4.12).
9:
Update Wl by Eq.(4.13).
10:
Update R by solving Eq.(4.15).
11:
Update H by Eq.(4.19).
12:
Update B by Eq.(4.23).
13:
Update ZB and ZR by Eqs.(4.25) and (4.26), respectively.
14:
Update GB and GR by Eq.(4.27).
15: until convergence.
where (a) holds since 1Tµ = 1 and the equality in (b) holds when

μl ∝
hl
√
μl . Since the
right-hand side of Eq.(4.11) is constant, the optimal μl in Eq.(4.10) can be obtained by
μl =
hl
L
l=1 hl .
(4.12)
Step 2: Update Wl. We set the derivative of objective function with respect to Wl to
zero, and then we have

102
4
Composite Multi-modal Hashing
Wl =
 1
μl HϕT(Xl)
  1
μl ϕ(Xl)ϕT(Xl) + δIp
−1
.
(4.13)
Step 3: Update R. The objective function with respect to R can be represented as
min
RTR=Ir
tr

−2βRTBHT −2αrRTBSHT + αRTBBTRHHT −λRT

ZR −GR
λ

.
(4.14)
We substitute RTBBTRHHT with RTBBTZRHHT and the above equation can be trans-
formed with the following equivalent form
max
RTR=Ir
tr(RTC),
(4.15)
where C = βBHT + αrBSHT −αBBTZRHHT + λZR −GR. The optimal R is deﬁned as
R = PQT, where P and Q are comprised of left-singular and right-singular vectors of C,
respectively [28].
Note that, the S ∈RN×N is included in the term BSHT when updating R. If we compute
S explicitly, the computational complexity is O(N 2). In this work, we utilize c × N matrix
˜Y (c is the number of semantic categories) to store the label information instead of directly
calculating S, and can reduce the computational complexity to O(N). Let ˜Yki =
yki
∥yi∥2 , as
the element at the kth row and the ith column in the matrix ˜Y. Then we can get the similarity
matrix ˜S = ˜Y
T ˜Y. The semantic similarity matrix S can be calculated as
S = 2˜S −E = 2 ˜Y
T ˜Y −1N1T
N,
(4.16)
where 1N is an all-one column vector with length N, and E is a matrix with all elements as
1. Then we can get
BSHT = 2(BYT)(HYT)T −(B1N)(H1N)T.
(4.17)
Thus the calculation of C can be transformed as
C = βBHT + αrBSHT = βBHT + 2αr(BYT)(HYT)T + 2αr(B1N)(H1N)T,
(4.18)
which consumes O(N).
Step 4: Update H. We set the derivative of objective function with respect to H to zero,
and then we have
H =
L
l=1
1
μl Wlϕ(Xl) + βRTB + αrRTBS
L
l=1
1
μl + α + β
,
(4.19)
where S is also transformed using Eq.(4.16), then we have
RTBS = BSRT = 2(B ˜Y
T)(R ˜Y
T)T + (B1N)(R1N)T.
(4.20)
The time complexity of computing RTBS is reduced to O(N).

4.3
Online Multi-modal Hashing with Dynamic Query-Adaption
103
Step 5: Update B. The objective function with respect to B can be presented as
min
B∈{−1,1}r×Ntr(−2αrBTRHS + αBTRHHTRTB −2βBTRH −λBTZB + GB).
(4.21)
We substitute BTRHHTRTB with BTRHHTRTZB. Thus the above equation is equivalent
to
min
B∈{−1,1}r×N tr(−BT(2αrRHST −αRHHTRTZB + 2βRH + λZB −GB)).
(4.22)
We can obtain the closed solution of B as
B = sgn(2αrRHST −αRHHTRTZB + 2βRH + λZB −GB),
(4.23)
where S is also transformed using Eq.(4.16), then we have
RHST = 2RH ˜Y
T ˜Y −RH1N1T
N.
(4.24)
The time complexity of computing RHST is reduced to O(N).
Step 6: Update ZB. We can obtain the closed solution of ZB as
ZB = sgn(−αRHHTRTB + λB + GB).
(4.25)
Step 7: Update ZR. The objective function respect to ZR can be represented as
max
ZT
RZR=Ir
tr(ZT
RCzr),
(4.26)
where C = −αBBTRTHHT + λR + GR. The optimal ZR is deﬁned as ZR = PzrQT
zr.
Step 8: Update GB and GR. The update rules are
GB = GB + λ(B −ZB), GR = GR + λ(R −ZR).
(4.27)
Online Hashing in SMH-OQA
In the online retrieval process, we aim to map the new coming query instances into binary
hash codes with the learned hash projection matrix {Wl}L
l=1. All existing composite multi-
modal hashing methods simply adopt equal or ﬁxed weights obtained from ofﬂine hash code
learning to combine multi-modal features. However, the ﬁxed weights obtained from ofﬂine
hash code learning cannot capture the variations of the dynamic queries. This motivates us
to develop query-adaptive online hashing. More importantly, we should avoid bringing any
additional parameter in weighting scheme.
In this work, with the support of the ofﬂine hash code learning framework, we present
a parameter-free query-adaptive online hashing with a self-weighting scheme. The weights
are actually virtual and the hash codes are iteratively updated online without any parameters
by considering the speciﬁc query contents. With this design, we could obtain more accurate

104
4
Composite Multi-modal Hashing
query hash codes for fast multimedia retrieval while relying less on parameter searching.
Speciﬁcally, the objective function of our query-adaptive online hashing process in SMH-
OQA is formulated as
min
Bq∈{−1,1}r×nq
L

l=1
∥Bq −Wlϕ(Xl
q)∥F,
(4.28)
where Wl ∈Rr×p is the linear projection matrix from Eq.(4.7), ϕ(Xl
q) ∈Rp×nq is the
nonlinearly projected representation of the newly coming query instances, Bq ∈{−1, 1}r×nq
is the binary hash code of the newly coming query instances, and nq is the number of samples
of the query set.
As Theorem 4.1, Eq.(4.28) can be shown to be equivalent to
min
Bq∈{−1,1}r×nq ,µ∈L
L

l=1
1
μlq
∥Bq −Wlϕ(Xl
q)∥2
F.
(4.29)
To solve Eq.(4.29), we drive the following iterative steps
Step 1: Update μl
q. As Sect.4.3.3, we denote ∥Bq −Wlϕ(Xl
q)∥F by hl
q. The optimal μl
q
is given by
μl
q =
hl
q
L
l=1 hlq
.
(4.30)
Step 2: Update Bq. We can obtain the closed solution of Bq as
Bq = sgn
 L

l=1
1
μlq
Wlϕ(Xl
q)
	
.
(4.31)
Time Complexity Analysis
In this subsection, we analyze the time complexity of SMH-OQA at the ofﬂine training
stage and the online retrieval stage. Assuming that N≫r and N≫iter, where N denotes the
number of the training samples, r denotes the hash code length and iter denotes the number
of iterations.
For SMH-OQA, at the ofﬂine training stage, the time complexity of constructing the non-
linear feature mapping ϕ(Xl) of each modality is O(Np). It takes O(pr N) for updating μl.
The computational complexity of updating Wl is O(pr N). The computational complexity
of updating R is O(r2N). Updating B requires O(pr N). The computational complexity
of updating H is O(r N). Then the computational complexity of optimization process is
O(iter × pr N), where iter is the number of iterations. This process scales linearly with
N. At the online multimedia retrieval process, it takes O(iter × prnq) to generate binary
hash codes for queries. In addition, in the discrete optimization process, we avoid explicitly
computing the pair-wise similarity matrix S, but substituting it with the expression of ˜L.
We successfully reduce the space complexity to O(N). In sum, both the computational and

4.3
Online Multi-modal Hashing with Dynamic Query-Adaption
105
the space complexity of SMH-OQA are linear with the size of the dataset. Our approach is
scalable for large-scale multimedia retrieval.
4.3.4
Unsupervised Multi-modal Hashing with Online Query-Adaption
Considering the label scarcity in real practice, we extend the composite multi-modal hashing
with online query-adaption in unsupervised learning paradigms. Speciﬁcally, we perform
the composite multi-modal hashing learning process with quantization strategy. Figure4.2
summarizes the sketch of UMH-OQA. In the following, we describe our UMH-OQA model
in detail.
Adaptive Multi-modal Quantized Hashing
Existing projection-based composite multi-modal hashing methods either adopt relaxing
optimization that may cause signiﬁcant quantization errors or discrete optimization which
relies on particular hash learning formulations to generate hash codes. They cannot minimize
Online Multi-modal Retrieval
CNN Feature
BoW Feature
Image Modality
Text Modality
Offline Training
001
010
011
000
101
110
111
100
  Adaptive Multi-modal Quantized Hashing
Image Feature
Text Feature
Image Modality
Text Modality
Feature Extraction
Adaptive Online Quantized Hashing
Image Modality 
Basis Matrix 
Text Modality
 Basis Matrix
Transformation 
Transformation 
Text Modality
 Basis Matrix 
Codebook
Prototype Distribution Map
Image Modality 
Basis Matrix
Adaptive 
Quantization
Feature Extraction
Training Samples
Transformation 
Hash Codes
Adaptive 
Quantization
001
010
011
000
101
110
111
100
Codebook
Prototype Distribution Map
Hash Codes
Multi-modal Query
Fig. 4.2 The basic framework of the proposed UMH-OQA-based multimedia retrieval system.
The system mainly includes two main components: ofﬂine model learning and online multi-modal
retrieval. At the ofﬂine model learning stage, we ﬁrst extract the heterogeneous multi-modal features
and apply matrix factorization to seek an orthogonal semantic space for each modality. Then, we
approximate the multi-modal latent representations using a set of representative codebooks, the hash
codes to be learned, and the modality fusion weights. Finally, the hash codes are learned with an
efﬁcient iterative optimization method. At the online retrieval stage, when a new query comes, the
heterogeneous multi-modal features are ﬁrst extracted. Then, the binary codes of a query can be easily
generated by the learned codebooks with adaptive modality weights from the ofﬂine model learning
stage, and then be applied for fast multimedia retrieval

106
4
Composite Multi-modal Hashing
the quantization error well or effectively preserve the internal structure of multi-modal data.
To tackle this problem, we propose to learn hash codes using a set of complementary multi-
modal prototypes, which can better characterize the intrinsic structure of multimedia data.
Speciﬁcally, we ﬁrst apply matrix factorization to seek a D dimensional orthogonal semantic
space for each modality rather than performing the quantization directly in the original
data. This process can reduce the modality noises and redundant information. Then, we
approximate the multi-modal latent representations using a set of representative codebooks,
the hash codes to be learned, and the modality fusion weights Formally, the computation
form is
min
B,Hl,C,Rl,μl α
L

l=1
∥Xl −RlHl∥2
F +
L

l=1
N

n=1
μl∥Hl −
M

m=1
Cmbmn∥2
F + ζ∥µ∥2
F,
s.t. ∥bmn∥0 = 1, bmn ∈{0, 1}logK
2 , (Rl)TRl = ID×D, µ = [μ1, μ2, ..., μL]T, µ ∈△L,
(4.32)
where α controls the latent representation learning process. Rl ∈Rdl×D is the latent
space basis, which is orthogonal to reduce the intra-modality redundancy and noises, and
Hl ∈RD×N is the latent representation for lth modality. To approximate the data struc-
ture, we learn M complementary codebooks C = [C1, C2, ..., Cm, ..., CM] ∈RD×MK and
each codebook Cm = [Cm1, ..., CmK ] ∈RD×K contains K codewords. With the M code-
books, we can get the hash codes bn = [b1n, ..., bMn] ∈R1×r for the nth data point, where
r = Mlog2K. Speciﬁcally, we use bmn ∈{0, 1}logK
2 to indicate which one (and only one)
of K codewords is selected from mth codebook to approximate the nth data point. The
constraint ∥bmn∥0 = 1 guarantees that only one codeword per codebook can be activated
to approximate the input data, where ∥.∥0 is the l0 norm that can simply count the number
of nonzero elements in the vectors. Similar to SMH-OQA, we also learn adaptive modality
combination weights. Following this strategy, we reformulate Eq.(4.32) as the following
one:
min
B,Hl,C,Rl α
L

l=1
∥Xl −RlHl∥2
F +
L

l=1
N

n=1
∥Hl −
M

m=1
Cmbmn∥F



Adaptive Quantization
,
s.t. ∥bmn∥0 = 1, bmn ∈{0, 1}logK
2 , (Rl)TRl = ID×D.
(4.33)
We can derive the following theorem. The detailed proof is shown in Appendix B.
Theorem 4.2 Equation(4.33) can achieve the same goal as follows:
min
B,Hl,C,Rl,μl α
L

l=1
∥Xl −RlHl∥2
F +
L

l=1
N

n=1
1
μl ∥Hl −
M

m=1
Cmbmn∥2
F,
s.t. ∥bmn∥0 = 1, bmn ∈{0, 1}logK
2 , (Rl)TRl = ID×D, µ ∈L.
(4.34)

4.3
Online Multi-modal Hashing with Dynamic Query-Adaption
107
Algorithm 4.2 Key steps of Unsupervised Multi-modal Hashing with Online Query-
Adaption (UMH-OQA)
Require: Training set {Xl ∈Rdl×N }L
l=1, hash code length r, dimension D, the number of prototypes
K in each codebook, parameter α.
Ensure: The codebooks C.
1: Initialize the modality weight μl = 1
L .
2: Randomly Initialize Rl|L
l=1, C and B.
3: Initialize Hl = (α(Rl)TRl)−1(α(Rl)TXl + 1
μl Cν(B)).
4: repeat
5:
Update B by Eq.(4.35).
6:
Update Rl by Eq.(4.38).
7:
Update Hl by Eq.(4.41).
8:
Update C by Eq.(4.44).
9:
Update μl by Eq.(4.48).
10: until convergence.
In a sum, in the ﬁrst term in Eq.(4.33), the heterogeneous multi-modal features Xl are
transformed into a latent space Hl, which avoids the intra-modality redundancy and noises.
Notably, this term can be substituted by other formulations that can learn modality-speciﬁc
presentations. The second term controls the quantization process based on the codebook
that characterizes the multi-modal data structure. It approximates the intermediate latent
representation Hl with the support of weight μl that adaptively exploits the complementarity
of multi-modal features, and thus converts the isomorphic latent features into compact binary
codes.
Iterative Optimization in UMH-OQA
We optimize the objective function with respect to each variable while ﬁxing all the other
variables. In addition, to have an overall look at the proposed method, we summarize the
iterative optimization steps of UMH-OQA in Algorithm 4.2.
Step 1: Update B. Since the hash code bn is independent of each other, we divide the
optimization problem for B into N subproblems. Speciﬁcally, the optimization problem in
this work can be solved as a high-order Markov Random Field (MRF) [29] problem. We
adopt the widely used Iterated Conditional Modes (ICM) [30] algorithm to solve it. By ﬁxing
all variables but B, the objective formulation in Eq.(4.33) reduces to
min
B
L

l=1
N

n=1
1
μl ∥Hl −
M

m=1
Cmbmn∥2
F, s.t. ∥bmn∥0 = 1, bmn ∈{0, 1}logK
2 .
(4.35)
With {bm′n}m′̸=m ﬁxed, we ﬁrst update bmn by exhaustively checking all the codewords in
codebooks, to ﬁnd the codeword such that the objective in Eq.(4.35) is minimized. Then,
we accordingly set the corresponding entry of bmn as 1 and the rest as 0.

108
4
Composite Multi-modal Hashing
Step 2: Update Rl. The optimization formula for Rl is
min
(Rl)TRl=ID×D
∥Xl −RlHl∥2
F.
(4.36)
With the constraint (Rl)TRl = ID×D, the above equation can be transformed to the
following maximization problem
max
(Rl)TRl=ID×D
tr((Rl)TXl(Hl)T) = tr((Rl)TG),
(4.37)
where G = Xl(Hl)T. The optimal Rl can be solved by
Rl = PQT,
(4.38)
wherePandQarecomprisedoftheleft-singularandright-singularvectorsofG,respectively.
Step 3: Update Hl. By removing the irrelevant terms, we obtain the following problem
for Hl
min
Hl α∥Xl −RlHl∥2
F +
N

n=1
1
μl ∥Hl −
M

m=1
Cmbmn∥2
F.
(4.39)
For brevity, we use an index matrix ν(B) ∈RMK×N to indicate whether a prototype is
selected or not. Then, Eq.(4.39) can be rewritten into the following equivalent problem
min
Hl α∥Xl −RlHl∥2
F + 1
μl ∥Hl −Cν(B)∥2
F.
(4.40)
By setting the derivative of Eq.(4.40) with respect to Hl to zero, and we can obtain
Hl = (α(Rl)TRl)−1(α(Rl)TXl + 1
μl Cν(B)).
(4.41)
Step 4: Update C. By ﬁxing the other variables, the updating formula for C can be
formulated as
min
C
L

l=1
N

n=1
1
μl ∥Hl −
M

m=1
Cmbmn∥2
F.
(4.42)
Similarly, it is simpliﬁed as
min
C
L

l=1
1
μl ∥Hl −Cν(B)∥2
F.
(4.43)
By setting the gradient with respect to C to zero, we have the closed form of C as

4.3
Online Multi-modal Hashing with Dynamic Query-Adaption
109
C =
 L

l=1
1
μl Hl(ν(B)
	T
)
 L

l=1
1
μl ν(B)(ν(B))T
	−1
.
(4.44)
Step 5: Update μl. We optimize μl by ﬁxing other variables. Then, the objective function
in Eq.(4.33) can be reduced as
min
μl≥0,1Tµ=1
L

l=1
N

n=1
1
μl ∥Hl −
M

m=1
Cmbmn∥2
F.
(4.45)
Similarly, this equation is equivalent to
min
μl≥0,1Tµ=1
L

l=1
1
μl ∥Hl −Cν(B)∥2
F.
(4.46)
As we have proved before
L

l=1
1
μl ∥Hl −Cν(B)∥2
F =
⎛
⎝
L

l=1
1
μl ∥Hl −Cν(B)∥2
F
⎞
⎠
⎛
⎝
L

l=1
μl
⎞
⎠≥
⎛
⎝
L

l=1
∥Hl −Cν(B)∥F
⎞
⎠
2
.
(4.47)
Combining with the Cauchy-Schwarz inequality, the equality holds when L
l=1 μl = 1
and

μl ∝
1
√
μl ∥Hl −Cν(B)∥F. Since the right-hand side of Eq.(4.47) is a constant, the
optimal μl in Eq.(4.46) is computed as follows:
μl =
∥Hl −Cν(B)∥F
L
l=1∥Hl −Cν(B)∥F
.
(4.48)
As shown in the above equation, if the lth modality is discriminative, then the value
of ∥Hl −Cν(B)∥F will be small, and thus the corresponding virtual weight 1
μl of the lth
modality is large. Similarly, a weak modality will be assigned with a small weight.
Online Hashing in UMH-OQA
Existing composite multi-modal hashing methods simply adopt ﬁxed weights for different
modality features at the online query stage. Apparently, they cannot capture the variations of
queries.Further,intheunsupervisedonlinehashingprocess,thesemanticlabelofthequeryis
unknown in advance. Under such circumstances, parameter adjustment will become unreli-
able. Moreover, manual hyper-parameter adjustment for all queries is practically impossible.
With those considerations, we derive the objective function of the query-adaptive quantized
online hashing module for UMH-OQA as

110
4
Composite Multi-modal Hashing
min
Bq,Hl
q,Rl
q
α
L

l=1
∥Xl
q −Rl
qHl
q∥2
F +
L

l=1
N

n=1
∥Hl
q −
M

m=1
Cmb′
mn∥F,
s.t. ∥b′
mn∥0 =1, b′
mn ∈{0, 1}, (Rl
q)TRl
q = ID×D.
(4.49)
As proved in Theorem 4.2, this equation is equivalent to
min
Bq,Hl
q,Rl
q,µq
α
L

l=1
∥Xl
q −Rl
qHl
q∥2
F +
L

l=1
N

n=1
1
μlq
∥Hl
q −
M

m=1
Cmb′
mn∥2
F,
s.t. µq ∈L, ∥b′
mn∥0 = 1, b′
mn ∈{0, 1}, (Rl
q)TRl
q = ID×D,
(4.50)
where α is a balance parameter. Rl
q and Hl
q are the latent space basis and the latent repre-
sentation, respectively. Cm is the codebook learned from the ofﬂine learning process. The
aim of the online hashing is to learn hash codes Bq = [b′
1; ...; b′
n; ...; b′
nq] for nq queries,
where b′
n = [b′
1n, ..., b′
Mn] is the hash code matrix of the nth query sample. μl
q is the adap-
tive weight for the newly coming query, which is designed to capture the variations of the
queries, and thus can boost the retrieval performance. Similarly, we derive the following
efﬁcient iterative optimization steps to solve the query hash optimization problem.
Step 1: Update μl
q. By ﬁxing the other variables, the objective function with respect to
μl
q can be represented as
min
μlq≥0,1Tµq=1
L

l=1
N

n=1
1
μlq
∥Hl
q −
M

m=1
Cmb′
mn∥2
F.
(4.51)
As we proved before, the solution with regard to μl
q can be solved combining with the
Cauchy-Schwarz inequality when L
l=1 μl
q = 1 and

μlq ∝
1

μlq
∥Hl
q −Cν(Bq)∥F.
μl
q =
∥Hl
q −Cν(Bq)∥F
L
l=1∥Hl
q −Cν(Bq)∥F
,
(4.52)
where index matrix ν(Bq) is used to indicate whether a prototype is selected or not.
Step 2: Update Hl
q. By treating other variables as a constant, the updating for Hl
q becomes
min
Hl
q
α∥Xl
q −Rl
qHl
q∥2
F +
N

n=1
1
μlq
∥Hl
q −
M

m=1
Cmb′
mn∥2
F.
(4.53)
We calculate the deviation to Hl
q and set it to zero. Then, Hl
q has the following closed
solution:
Hl
q = (α(Rl
q)TRl
q)−1(α

Rl
q)TXl
q + 1
μlq
Cν(Bq)
	
.
(4.54)

4.3
Online Multi-modal Hashing with Dynamic Query-Adaption
111
Step 3: Update Rl
q. With the other variables ﬁxed, the updating for Rl
q becomes
min
(Rl
q)TRl
q=ID×D
∥Xl
q −Rl
qHl
q∥2
F.
(4.55)
With the constraint (Rl
q)TRl
q = ID×D, Rl
q can be solved by
Rl
q = MNT,
(4.56)
where M and N are left-singular and right-singular vectors of Xl
q(Hl
q)T, respectively.
Step 4: Update Bq. Similarly, the optimization problem for Bq = [b′
1; ...; b′
nq] is divided
into nq subproblems. By ﬁxing other variables, the original problem for Bq can be written
as
min
Bq
L

l=1
N

n=1
1
μlq
∥Hl
q −
M

m=1
Cmb′
mn∥2
F, s.t. ∥b′
mn∥0 = 1, b′
mn ∈{0, 1}logK
2 .
(4.57)
Given {b′
m′n}m′̸=m, we update b′
mn by exhaustively checking all the codewords in the
codebooks learned from the ofﬂine stage, ﬁnding the codeword such that the objective in
Eq.(4.57) is minimized, and setting the corresponding entry of b′
mn to 1 and the rest to 0.
Time Complexity Analysis
In this subsection, we analyze the time complexity of UMH-OQA at the ofﬂine training
stage and the online retrieval stage. Assuming that N≫r and N≫iter, where N denotes the
number of the training samples, r denotes the hash code length and iter denotes the number
of iterations.
For UMH-OQA, at the ofﬂine training stage, it takes O(DN M) for updating the hash
codes B. The time complexity of updating Rl is O(dl D2). The time complexity of updating
Hl is O(Ddl N). The time complexity of updating the codebooks C is O(DNr). It takes
O(DN) for updating µl. Thus, the overall computation complexity of ofﬂine learning stage
is O(iter × Ddl N), this process scales linearly with N. At the online retrieval stage, it
takes O(Dnq) for updating μl
q. It takes O(dl D2) for updating the transformation matrix Rl
q
for the query samples. Updating Hl
q requires O(Ddlnq). It takes O(Dnq M) for updating
Bq. Therefore, it takes O(iter × Ddlnq) to generate binary hash codes for queries. The
computational complexity of the online stage is linear to the query size nq.
4.3.5
Experiment
To verify the effectiveness and efﬁciency of our models, we conduct a series of experiments
on three publicly available datasets, which are commonly used in multimedia retrieval sys-
tems. We aim to answer the following research questions:

112
4
Composite Multi-modal Hashing
Table 4.2 General statistics of MIR Flickr, MS COCO and NUS-WIDE. CNN is short for Convolu-
tional Neural Networks and BoW is short for Bag of Words, respectively
Datasets
MIR Flickr
MS COCO
NUS-WIDE
Retrieval size
17,772
82,783
193,749
Query size
2,243
5,981
2,085
Training size
5,000
18,000
21,000
Visual modality
CNN (4,096-D)
CNN (4,096-D)
CNN (4,096-D)
Text modality
BoW (1,386-D)
BoW (2,000-D)
BoW (1,000-D)
• RQ 1: How about the retrieval accuracy of our proposed models compared with state-
of-the-art hashing methods?
• RQ 2: How about the efﬁciency of our models compared with state-of-the-art hashing
methods?
• RQ 3: How does the adaptive multi-modal fusion strategy affect the performance of
SMH-OQA and UMH-OQA?
• RQ 4: How does the nonlinear feature mapping and the discrete hash code optimization
affect the performance of SMH-OQA?
• RQ 5: How does the key parameter setting impose inﬂuence on the performance of our
models?
• RQ 6: Are the proposed models converged? How many iterations are required to achieve
the convergence?
In what follows, we introduce the detailed settings of our experiments, including exper-
imental datasets, evaluation baselines, evaluation metrics, and implementation details.
Experimental Datasets
In this work, we conduct all experiments on three publicly available datasets: MIR Flickr
[31], MS COCO [32], and NUS-WIDE [33]. These datasets are widely adopted to eval-
uate the performance of composite multi-modal hashing for efﬁcient multimedia retrieval
[9, 12, 15]. Table4.2 summarizes the basic statistics of these three datasets. The detailed
experimental settings are as follows:
• MIR Flickr1 [31] contains 25,000 images annotated by 24 provided labels. We select
those images which have at least 20 tags, and ﬁnally obtain 20,015 image-tag pairs for our
experiment. After deleting the duplicate images without tags or manual annotation tags,
we ﬁnally obtain 2,243 image-tag pairs and use them as the query set, and the remaining
17,772 image-tag pairs are treated as retrieval set. For the training set, we randomly select
5,000 instances from the retrieval set as the training set. We use a 4,096-dimensional
1 http://lear.inrialpes.fr/people/guillaumin/data.php.

4.3
Online Multi-modal Hashing with Dynamic Query-Adaption
113
feature extracted by the Caffe implementation of VGG-16 Net [34] to represent each
image. Besides, we use a 1,386-dimensional bag-of-words feature vector to represent the
corresponding tag.
• MS COCO2 [32] is comprised of 82,783 training images and 40,504 validation images
with at least one of 80 object categories. In our experiments, all the training images are
treated as the retrieval set.In addition, we randomly select 80 images for each class from
the validation images and obtain the ﬁnal 5,981 images to form the query set after elim-
inating images without textual tags. In the remaining retrieval set other than queries, we
randomly choose 18,000 images to serve as the training set. We use a 4,096-dimensional
feature extracted by the Caffe implementation of VGG-16 Net [34] to represent each
image. The texts are expressed as 2,000-dimensional bag-of-words feature vectors.
• NUS-WIDE3 [33] collects 269,648 web images from Flickr4 with 81 semantic con-
cepts. In experiments, 21 most frequent concepts are selected and the corresponding
195,834 images are preserved for the experiment. For each concept, 100 images with
their corresponding tags are randomly chosen to serve as the query set and the remaining
image-tag pairs serve as the retrieval set. Within the retrieval set, 21,000 instances are
randomly selected to form the training set. We use a 4,096-dimensional feature extracted
by the Caffe implementation of VGG-16 Net [34] to represent each image. The text is
represented by a 1,000-dimensional bag-of-words feature vector.
Evaluation Baselines
To evaluate the retrieval performance, we compare our models with several state-of-the-art
composite multi-modal hashing methods. They are brieﬂy described as follows:
• Multiple Feature Hashing (MFH) [9]. This method preserves the local and global data
structures to learn a set of hash functions.
• Multi-view Alignment Hashing (MAH) [10]. It combines multiple information sources
and meanwhile discards feature redundancies based on the regularized kernel nonnegative
matrix factorization.
• Multi-view Latent Hashing (MVLH) [11]. It learns the hash functions within a uni-
ﬁed kernel feature space and computes the ﬁxed combination weights according to the
reconstruction errors.
• Multi-view Discrete Hashing (MvDH) [12]. This method jointly performs the pseudo
label learning and latent hash space discovery. The hash code learning process is super-
vised with the learned pseudo labels.
• Multiple Feature Kernel Hashing (MFKH) [13]. It is one of the state-of-the-art super-
vised composite multi-modal hashing methods. It formulates the composite multi-modal
2 http://cocodataset.org/.
3 http://lms.comp.nus.edu.sg/research/NUS-WIDE.htm.
4 https://www.ﬂickr.com/.

114
4
Composite Multi-modal Hashing
hashing as a similarity preserving problem with optimal linearly combined multiple ker-
nels.
• Discrete Multi-view Hashing (DMVH) [14]. This method is a supervised composite
multi-modal hashing method, which optimizes the discrete codes directly instead of
relaxing the binary constraints.
• Supervised Discrete Multi-view Hashing (SDMH) [15]. It learns discriminative binary
hash codes by multi-modal feature integration and explicit supervised label regression.
Among these baselines, MFKH, DMVH and SDMH are supervised, and others are unsu-
pervised.
Speciﬁcally, UMH-OQA is a quantization-based hashing method. Thus, we also compare
UMH-OQA with several quantization-based uni-modal and cross-modal hashing methods.
Three quantization-based models are the following:
• Iterative Quantization (ITQ) [1]. It decreases the binary embedding errors by ﬁnding
an orthogonal rotation to adjust the feature projection from the original data to binary
codes.
• Adaptive Binary Quantization (ABQ) [35]. It proposes an adaptive uni-modal quanti-
zation method by jointly considering the original space and the Hamming space to deﬁne
the hash function. Speciﬁcally, the prototype set and the code set are jointly discovered
to characterize the data distribution.
• Composite Correlation Quantization (CCQ) [36]. This method is a cross-modal hash-
ing method, which ﬁrst transforms different modalities into isomorphic latent space and
then converts the isomorphic latent features into compact binary codes to support unsu-
pervised cross-modal retrieval.
We conduct the experiments on a computer with 3.6GHz Intel Xeon(R) CPUs, 64 GB
RAM and 64-bit Ubuntu system. All baselines used in this work are implemented in Matlab
R2017b. In our work, all comparison experiments are conducted on the same datasets with
the same extracted features (Image: VGG-16 deep CNN feature; Text: BoW feature) and
evaluation metrics. For a fair comparison, we use the same training, query and retrieval sets
for all of the methods on each dataset, and also carefully tune the optimal parameters of
baseline methods and ﬁnally report their best results.
Evaluation Metrics
We adopt two widely used evaluation protocols, i.e., Mean Average Precision (MAP) [9,
13] and topK-precision curves [15, 37], to evaluate the retrieval performance of different
hashing approaches. We adopt Hamming distance to measure the similarity between the
binary hash codes of the query instances and that of the multimedia database. Speciﬁcally,
if two samples share at least one semantic tag, then they are considered to be semantically
similar.

4.3
Online Multi-modal Hashing with Dynamic Query-Adaption
115
• MAP: For all the queries, we ﬁrst calculate their Average Precision (AP) and then obtain
the average value as MAP. The AP for a new query is deﬁned as:
AP(q) = 1
ω
n

r=1
P(r)δ(r),
(4.58)
where ω denotes the number of ground-true neighbors of query q, n denotes the size
of the dataset, and P(r) denotes the precision of the top r retrieved results. If the rth
retrieved entity is the true neighbor, δ(r) = 1, otherwise δ(r) = 0.
• topK-precision curves: It is expressive for multimedia retrieval by reﬂecting the change
of precision with respect to the number of top-ranked k instances (images or texts).
For both evaluation metrics, a larger value indicates better retrieval performance. To
remove any randomness caused by splits of training and testing sets, all experiments in this
work are repeated 5 times and the averaged results are reported.
Implementation Details
Four parameters could affect the performance of SMH-OQA: the number of anchors p
and hyper-parameters α, β, and δ. α and β are balance parameters to support asymmet-
ric supervised learning, and δ is a regularization parameter to avoid over-ﬁtting. In order
to choose appropriate parameters, a set of possible values are ﬁrst deﬁned, and the ﬁnal
parameters are selected through experiments. In experiments, the best performance of SMH-
OQA is achieved when the number of anchors p is set as 500, 1,000, and 1,200 on MIR
Flickr, MS COCO, and NUS-WIDE, respectively. The best performance of SMH-OQA
is achieved when {α = 10−5, β = 10−1, γ = 10−3}, {α = 10−3, β = 103, γ = 10−5}, and
{α = 101, β = 105, γ = 10−5} on MIR Flickr, MS COCO, and NUS-WIDE, respectively.
Three parameters could affect the performance of UMH-OQA: hyper-parameter α, which
controls the latent representation learning process, the dimension D of the modality-speciﬁc
latent space, and the number of prototypes K in each codebook. We set the number of
iterations to 30. The best performance of UMH-OQA is achieved when α = 103, D = 103,
K = 24 on MIR Flickr, and α = 103, D = 102, K = 24 on MS COCO, and α = 102,
D = 103, K = 24 on NUS-WIDE, respectively.
Retrieval Accuracy Comparison (RQ 1)
We compare our proposed models with state-of-the-art composite multi-modal hashing
methods to answer RQ 1. The MAP results on three datasets (MIR Flickr, MS COCO and
NUS-WIDE) are listed from Tables4.3, 4.4 and 4.5, respectively. We use the t-test results to
analyze whether there are meaningful differences between the experimental results of our
method and several competitive baselines. The null hypothesis is the experimental result
of our proposed supervised and unsupervised methods are not signiﬁcantly different from
that of the baselines. The t-test results are shown in Table4.6. The topK-precision curves
on MIR Flickr, MS COCO and NUS-WIDE with different numbers of hash code length (32
bits and 128 bits) are shown in Fig.4.3. From these results, we can ﬁnd that SMH-OQA

116
4
Composite Multi-modal Hashing
Table 4.3 MAP comparison results varying with different hash code lengths (16 bits, 32 bits, 64
bits, and 128 bits) on MIR Flickr. The best results for supervised and unsupervised methods are
highlighted in bold
Methods
16 bits
32 bits
64 bits
128 bits
Supervised
MFKH [13]
0.6369
0.6128
0.5985
0.5807
DMVH [14]
0.7231
0.7326
0.7495
0.7641
SDMH [15]
0.7246
0.7609
0.7634
0.7832
Our-SMH-OQA
0.7988
0.8047
0.8154
0.8184
Unsupervised MFH [9]
0.5795
0.5824
0.5831
0.5836
MAH [10]
0.6488
0.6649
0.6990
0.7114
MVLH [11]
0.6541
0.6421
0.6044
0.5982
MvDH [12]
0.6828
0.7210
0.7344
0.7527
Our-UMH-OQA
0.7335
0.7414
0.7517
0.7796
Table 4.4 MAP comparison results varying with different hash code lengths (16 bits, 32 bits, 64
bits, and 128 bits) on MS COCO. The best results for supervised and unsupervised methods are
highlighted in bold
Methods
16 bits
32 bits
64 bits
128 bits
Supervised
MFKH [13]
0.4216
0.4211
0.4230
0.4229
DMVH [14]
0.4123
0.4288
0.4355
0.4563
SDMH [15]
0.5055
0.5218
0.5422
0.5553
Our-SMH-OQA
0.5059
0.5148
0.5263
0.5488
Unsupervised MFH [9]
0.3948
0.3966
0.3960
0.3980
MAH [10]
0.3967
0.3943
0.3966
0.3988
MVLH [11]
0.3993
0.4012
0.4065
0.4099
MvDH [12]
0.3978
0.3966
0.3977
0.3998
Our-UMH-OQA
0.4142
0.4232
0.4241
0.4346
and UMH-OQA consistently outperform the corresponding supervised and unsupervised
methods. To be speciﬁc, we have the following observations:
• SMH-OQA obtains obtains the superior or comparable MAP values on three datasets.
Speciﬁcally, when the code length is set to 64 bits, the MAP value of our SMH-OQA is
0.8154 on MIR Flickr, while the MAP value of the second best supervised comparison
method is 0.7634 on MIR Flickr. The topK-precision curves on three datasets demonstrate
a similar trend as the MAP results. From these curves, we can ﬁnd that, with the number
of retrieved samples increasing, the precision of SMH-OQA consistently outperforms the
baselines by a large margin. These results clearly validate the superiority of SMH-OQA.

4.3
Online Multi-modal Hashing with Dynamic Query-Adaption
117
Table 4.5 MAP comparison results varying with different hash code lengths (16 bits, 32 bits, 64
bits, and 128 bits) on NUS-WIDE. The best results for supervised and unsupervised methods are
highlighted in bold
Methods
16 bits
32 bits
64 bits
128 bits
Supervised
MFKH [13]
0.4768
0.4359
0.4342
0.3956
DMVH [14]
0.5676
0.5883
0.6092
0.6279
SDMH [15]
0.6111
0.6416
0.6585
0.6743
Our-SMH-OQA
0.6439
0.6521
0.6641
0.6781
Unsupervised
MFH [9]
0.3603
0.3611
0.3625
0.3629
MAH [10]
0.4633
0.4945
0.5381
0.5476
MVLH [11]
0.4182
0.4092
0.3789
0.3897
MvDH [12]
0.4947
0.5661
0.5789
0.6122
Our-UMH-OQA
0.6343
0.6485
0.6517
0.6642
Table 4.6 The t-test results on three datasets. h = 1 implies that t-test rejects the null hypothesis at
the default signiﬁcance level of 5%, and h = 0 implies that t-test accepts the null hypothesis
Methods
MIR Flickr
MS COCO
NUS-WIDE
h
p
h
p
h
p
Supervised
MFKH [13]
1
1.1e-03
1
1.4e-03
1
2.5e-03
DMVH [14]
1
7.4e-04
1
1.3e-05
1
1.8e-03
SDMH [15]
1
8.7e-03
0
1.1e-03
0
1.4e-01
Unsupervised
MFH [9]
1
3.7e-04
1
4.3e-03
1
1.6e-05
MAH [10]
1
1.9e-03
1
5.4e-03
1
2.2e-03
MVLH [11]
1
1.2e-02
1
2.9e-03
1
3.9e-04
MvDH [12]
1
3.2e-02
1
6.2e-03
1
1.9e-02
• For UMH-OQA, we observe that UMH-OQA outperforms all unsupervised baselines by
a signiﬁcant margin. For example, the MAP of UMH-OQA is 0.7517 on MIR Flickr,
0.4241 on MS COCO, and 0.6517 on NUS-WIDE when the code length is set to 64 bits,
while that of MvDH is 0.7344 on MIR Flickr, 0.4065 on MS COCO, and 0.5789 on NUS-
WIDE. In addition, we can ﬁnd a similar trend as the MAP results from the topK-precision
curves on three datasets. Besides, UMH-OQA even achieves higher retrieval precision
than several supervised competitors. Although UMH-OQA is an unsupervised method,
it still performs better than most supervised baselines, which shows its effectiveness. For
instance, the MAP value of UMH-OQA (0.7335 on MIR Flickr, 0.4142 on MS COCO,
and 0.6343 on NUS-WIDE) is higher than that of DMVH (0.7231 on MIR Flickr, 0.4123
on MS COCO, and 0.5676 on NUS-WIDE) when the hash code length is ﬁxed to 16 bits.

118
4
Composite Multi-modal Hashing
1000
2000
3000
4000
5000
6000
7000
8000
9000
10000
 Number of top retrieved samples
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
 Precision
MFH
MAH
MVLH
MvDH
MFKH
DMVH
SDMH
Our-SMH-OQA
Our-UMH-OQA
(a) MIR Flickr on 32 bits
1000
2000
3000
4000
5000
6000
7000
8000
9000
10000
 Number of top retrieved samples
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
 Precision
MFH
MAH
MVLH
MvDH
MFKH
DMVH
SDMH
Our-SMH-OQA
Our-UMH-OQA
(b) MS COCO on 32 bits
1000
2000
3000
4000
5000
6000
7000
8000
9000
10000
 Number of top retrieved samples
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
 Precision
MFH
MAH
MVLH
MvDH
MFKH
DMVH
SDMH
Our-SMH-OQA
Our-UMH-OQA
(c) NUS-WIDE on 32 bits
1000
2000
3000
4000
5000
6000
7000
8000
9000
10000
 Number of top retrieved samples
0.5
0.55
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
 Precision
MFH
MAH
MVLH
MvDH
MFKH
DMVH
SDMH
Our-SMH-OQA
Our-UMH-OQA
(d) MIR Flickr on 128 bits
1000
2000
3000
4000
5000
6000
7000
8000
9000
10000
 Number of top retrieved samples
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
 Precision
MFH
MAH
MVLH
MvDH
MFKH
DMVH
SDMH
Our-SMH-OQA
Our-UMH-OQA
(e) MS COCO on 128 bits
1000
2000
3000
4000
5000
6000
7000
8000
9000
10000
 Number of top retrieved samples
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
 Precision
MFH
MAH
MVLH
MvDH
MFKH
DMVH
SDMH
Our-SMH-OQA
Our-UMH-OQA
(f) NUS-WIDE on 128 bits
Fig. 4.3 The topK-precision curves varying with different hash code lengths. a MIR Flickr on 32
bits, b MS COCO on 32 bits, c NUS-WIDE on 32 bits, d MIR Flickr on 128 bits. e MS COCO on
128 bits. f NUS-WIDE on 128 bits
• We observe that the performance of SMH-OQA and UMH-OQA improves with the
increase of hash code length from 16 bits to 128 bits, while the performance of baselines
such as MFKH and MVLH decreases signiﬁcantly. This result shows that longer hash
codes can bring more discriminative information in our models. Besides, in Table4.6, the
results show that our models not only have superior MAP performance, but also represent
statistically signiﬁcant improvements over the comparison methods in most cases.
The main reasons for the superior performance of SMH-OQA come from four aspects:
(1) SMH-OQA learns the consensus multi-modal factor collaboratively from multi-modal
data, which can explore the complementarity of multi-modal features to boost the hash
performance.(2)WedevelopanasymmetricsupervisedhashlearningmoduleinSMH-OQA,
where the learned hash codes are simultaneously correlated with the low-level consensus
multi-modal factor and the high-level semantic labels to enhance discriminative capability.
(3) The discrete hash code optimization solves hash code directly without quantization
errors. (4) With a query-adaptive online hashing strategy, the hash codes are adaptively
learned according to the speciﬁc query contents.
In addition, the main reasons for the superior performance of UMH-OQA come from two
aspects: (1) The learned codebooks can effectively capture the intrinsic structure of multi-
modal data, which is important for unsupervised composite multi-modal hashing. (2) The
proposed quantization scheme could adaptively exploit the complementarity of multi-modal
features and capture the dynamic variations of multimedia contents.

4.3
Online Multi-modal Hashing with Dynamic Query-Adaption
119
Table 4.7 MAP comparison results between quantization-based methods and UMH-OQA on MIR
Flickr, MS COCO and NUS-WIDE. The best result in each row is highlighted in bold
Quantization-based methods
Uni-modal
Cross-modal
Multi-modal
ITQ [1]
ABQ [35]
CCQ [36]
Our-UMH-OQA
MIR Flickr
16 bits
0.7063
0.5890
0.6745
0.7335
32 bits
0.6978
0.5930
0.6899
0.7414
64 bits
0.6985
0.5972
0.6924
0.7517
128 bits
0.7002
0.5976
0.7223
0.7796
MS COCO
16 bits
0.3993
0.3925
0.3924
0.4142
32 bits
0.3998
0.3909
0.3922
0.4232
64 bits
0.4006
0.3928
0.3981
0.4241
128 bits
0.4029
0.3956
0.4011
0.4346
NUS-WIDE
16 bits
0.5163
0.4915
0.5143
0.6343
32 bits
0.5166
0.4157
0.5261
0.6485
64 bits
0.5176
0.4125
0.5453
0.6517
128 bits
0.5217
0.3888
0.5572
0.6642
As UMH-OQA is based on the quantization strategy, we further compare UMH-OQA
with quantization-based uni-modal and cross-modal hashing methods. We re-implement the
uni-modal hashing method ITQ [1], ABQ [35] and cross-modal hashing method CCQ [36].
Especially, for uni-modal hashing methods, we concatenate the multi-modal features into a
uniﬁed vector and import it directly into the model at both ofﬂine model training and online
query stages. For cross-modal hashing method CCQ, it jointly converts images and texts into
isomorphic latent space and learns combinative hash codes by composite quantization, and
thus, it can be implemented for multi-modal retrieval. The results are plotted in Table4.7.
As can be seen, UMH-OQA outperforms other competitors by about 0.05 on MIR Flickr,
0.02 on MS COCO, and 0.13 on NUS-WIDE when the hash code length is 64 bits.
Efﬁciency Comparison (RQ 2)
In this section, we conduct experiments to investigate the training and query efﬁciency of
our models and other baselines to answer RQ 2. For fair comparison on the efﬁciency, all
methods in this section are implemented with Matlab R2017b and all experiments are run
on the same machine. Table4.8 shows the training and query times of all the compared
algorithms on MIR Flickr, MS COCO and NUS-WIDE. Without loss of generality, we set
the code length to 128 bits for all methods here.
From Table4.8, we can obviously observe that SMH-OQA consumes the least or the
comparable time at the training and query stages. Compared with unsupervised composite
multi-modal hashing methods, we also observe that the training and query time of UMH-
OQA is competitive with that of other methods on different datasets. On MS COCO, the
training time of UMH-OQA is 10 times faster than that of MFH, and even nearly 150 times

120
4
Composite Multi-modal Hashing
Table 4.8 Comparison of training and query time (seconds) on MIR Flickr, MS COCO and NUS-
WIDE. The best result is highlighted in bold
Methods
Training time (s)
Query time (s)
MIR Flickr
MS COCO
NUS-WIDE
MIR Flickr
MS COCO
NUS-WIDE
MFKH
855.12
7714.43
7472.85
18.67
162.53
158.65
DMVH
891.43
12163.53
9677.87
15.88
216.42
144.55
SDMH
11.84
36.83
34.43
11.39
161.57
141.24
Our-SMH-OQA
11.32
42.63
52.63
12.52
165.77
149.64
SMH-OQA-III
25.32
58.43
72.54
23.322
182.32
171.45
MFH
20.56
681.64
689.56
135.13
2728.53
1482.59
MAH
34.85
9211.53
9163.59
163.81
2974.72
1600.25
MVLH
15.46
105.43
107.78
146.42
2811.63
1581.05
MvDH
212.28
870.32
845.90
959.01
9462.63
8054.72
Our-UMH-OQA
18.53
61.41
81.53
126.70
2592.53
834.63
faster than that of MAH. The reason is that MFH and MAH construct graphs to preserve
the semantic correlation of samples in multiple modalities. The computation and space
consumption for constructing such graph of N samples is O(N 2), which cannot scale well
forlarge-scalemultimediaretrieval.Besides,theylearnthehashcodesviarelaxing+rounding
hash optimization strategy, which is also very time-consuming. Differently, to support large-
scale retrieval, we learn the hash codes directly in a fast mode to alleviate the quantization
errors with simple efﬁcient operations, which have high computation and storage efﬁciency.
In addition, at the query stage, UMH-OQA outperforms other baselines by a large margin.
Especially, the query time of UMH-OQA is 834.63s on NUS-WIDE, while that of the second
best competitor MFH is 1482.59s. Although our models adaptively learn hash codes of the
new queries at the online retrieval stage, this process does not seriously reduce the query
efﬁciency.
In order to gain more insights, we further report the training time variations with the
size of the training set on MIR Flickr. We set the number of training samples from 1,000
to 10,000 with step size 1,000. Figure4.4 presents the results when the hash code length is
ﬁxed to 128 bits. We observe that the training time of SMH-OQA and UMH-OQA increases
linearly with the training size, which validates the linear computational complexity of the
proposed models. The training time of our method is competitive even if the training scale is
large. These results further demonstrate that our proposed models are efﬁcient and scalable.

4.3
Online Multi-modal Hashing with Dynamic Query-Adaption
121
1000
2000
3000
4000
5000
6000
7000
8000
9000
10000
 Training size
0
5
10
15
20
25
30
35
40
 Training Time(seconds)
Our-SMH-OQA
(a) SMH-OQA on MIR Flickr
1000
2000
3000
4000
5000
6000
7000
8000
9000
10000
 Training size
0
10
20
30
40
50
60
70
80
 Training Time(seconds)
Our-UMH-OQA
(b) UMH-OQA on MIR Flickr
Fig.4.4 The training time variations of a SMH-OQA and b UMH-OQA with the size of training set
on MIR Flickr
Effects of Adaptive Multi-modal Fusion Strategy (RQ 3)
In our models, the modality combination weights are adaptively computed to quantize the
modality features into the binary codes. To answer RQ 3, we conduct experiments on MIR
Flickr by varying the hash code length in the range of {16, 32, 64, 128} bits and implement
ﬁve variants of SMH-OQA for comparison:
• SMH-OQA-img only inputs the image modality feature into the hashing model and
retrieves relevant image from an image database.
• SMH-OQA-txt only inputs the text modality feature into the hashing model and retrieves
relevant text from a text database.
• SMH-OQA-con simply concatenates image and text features into a uniﬁed vector and
then imports it into the hashing model.
• SMH-OQA-I adopts ﬁxed modality weights learned from the ofﬂine learning stage to
generatethequeryhashcodes,whichcanbecomputedbyBq = sgn(L
l=1
1
μl Wlϕ(Xl
q)).
Here, different from μl
q in Eq.(4.31), μl is the modality weight learned from the ofﬂine
learning stage.
• SMH-OQA-II ﬁxes the weight of each modality to 1 at both the ofﬂine learning and online
retrieval stages. We use Bq = sgn(L
l=1 Wlϕ(Xl
q)) to learn the query hash cades.
SMH-OQA-img and SMH-OQA-txt are the typical settings for the uni-modal hashing
methods [2, 38] corresponding to the tasks that image retrieves image, text retrieves text,
respectively. Besides, we also design ﬁve variants of UMH-OQA for comparison, includ-
ing: UMH-OQA-img, UMH-OQA-txt, UMH-OQA-con, UMH-OQA-I, and UMH-OQA-II.
Speciﬁcally, the hash codes of UMH-OQA-I can be learned using Iterated Conditional
Modes algorithm by solving

122
4
Composite Multi-modal Hashing
min
Bq
L

l=1
N

n=1
1
μl ∥Hl
q −
M

m=1
Cmb′
mn∥2
F, s.t. ∥b′
mn∥0 = 1, b′
mn ∈{0, 1}logK
2 .
(4.59)
where
1
μl and the modality weights learned from the ofﬂine learning process, Cm is the
codebook learned from the ofﬂine learning process. Besides, the hash codes for UMH-
OQA-II can be learned by
min
Bq
L

l=1
N

n=1
∥Hl
q −
M

m=1
Cmb′
mn∥2
F, s.t. ∥b′
mn∥0 = 1, b′
mn ∈{0, 1}logK
2 .
(4.60)
The results in Fig.4.5 show that SMH-OQA and UMH-OQA obtain the best performance
compared with their ﬁve variants. These results indicate that the proposed self-weighted
multi-modal fusion can effectively exploit the complementarity of multi-modal features,
and thus enhance the retrieval performance.
Further, to visually show the variation of parameters, we randomly select 100 samples
and record the change of different modality weights with queries in UMH-OQA and SMH-
OQA. The results in Fig.4.6 show that the modality weight learned by our method can
automatically adapt to the varied query contents.
16
32
64
128
Hash code length
0.5
0.6
0.7
0.8
0.9
1
1.1
MAP
SMH-OQA-img
SMH-OQA-txt
SMH-OQA-con
Our-SMH-OQA
UMH-OQA-img
UMH-OQA-txt
UMH-OQA-con
Our-UMH-OQA
(a)
16
32
64
128
Hash code length
0.6
0.65
0.7
0.75
0.8
0.85
0.9
0.95
1
MAP
SMH-OQA-I
SMH-OQA-II
Our-SMH-OQA
UMH-OQA-I
UMH-OQA-II
Our-UMH-OQA
(b)
Fig. 4.5 The comparison results to show the effects of the adaptive multi-modal fusion strategy on
MIR Flickr by varying the hash code length in the range of {16,32,64,128} bits. a SMH-OQA-img
only inputs the image modality feature into the hashing model. SMH-OQA-txt only inputs the text
modality feature into the hashing model. SMH-OQA-con simply concatenates image and text features
into a uniﬁed vector and then imports it into the hashing model. b SMH-OQA-I adopts ﬁxed modality
weights learned from the ofﬂine learning stage to generate the query hash codes. SMH-OQA-II ﬁxes
the weight of each modality to 1 at both the ofﬂine learning and online retrieval stage. Similarly,
we also design ﬁve variants of UMH-OQA for comparison: UMH-OQA-img, UMH-OQA-txt, UMH-
OQA-con, UMH-OQA-I, and UMH-OQA-II

4.3
Online Multi-modal Hashing with Dynamic Query-Adaption
123
Fig.4.6 Modality weights adapted to dynamic queries. a–c SMH-OQA and d–f UMH-OQA on MIR
Flickr, MS COCO and NUS-WIDE, respectively
Effects of the Nonlinear Feature Mapping and the Discrete Hash Code Optimization
in SMH-OQA (RQ 4)
SMH-OQA nonlinearly projects features of different modalities into a new representation
based on the anchors, which help to reduce the computational complexity of hash code
learning to O(N). In addition, it can effectively characterize the nonlinear sample rela-
tions and thus has stronger description capability than the original feature. To validate its
effectiveness, we design a variant of SMH-OQA, named SMH-OQA-III, for comparison.
SMH-OQA-III simply imports the original features of different modalities into the hashing
model. The retrieval performance comparison between SMH-OQA-III and SMH-OQA is
shown in Table4.9. We can ﬁnd that SMH-OQA achieves superior retrieval precision than
SMH-OQA-III. Besides, we record the training and query efﬁciency of SMH-OQA-III on
three datasets when the code length is ﬁxed on 128 bits and show the results in Table4.8. The
results show that both the training and query efﬁciency are reduced without the nonlinear
feature mapping. For example, on MS COCO, SMH-OQA-III spends 182.32s at the query
phase, which takes 16.55s more than SMH-OQA. These results demonstrate that the non-
linear feature mapping can reduce the computational complexity and improve the retrieval
precision.
In SMH-OQA, we propose an efﬁcient discrete optimization method to directly learn
binary hash codes instead of the relaxing+rounding optimization strategy commonly used
in the previous works. To validate its effects, we design a variant of our method named SMH-

124
4
Composite Multi-modal Hashing
Table4.9 MAP comparison with variants of SMH-OQA on MIR Flickr, MS COCO and NUS-WIDE.
The best result in each column is marked with bold
Methods
MIR Flickr
MS COCO
NUS-WIDE
16 bits
32 bits
64 bits
128
bits
16 bits
32 bits
64 bits
128
bits
16 bits
32 bits
64 bits
128
bits
SMH-
OQA-III
0.6936
0.7172
0.7183
0.7270
0.4846
0.4997
0.5020
0.5066
0.4855
0.4973
0.4955
0.5043
SMH-
OQA-IV
0.7718
0.7516
0.6527
0.6232
0.4870
0.4946
0.5075
0.5210
0.5732
0.6142
0.6373
0.6453
SMH-
OQA
0.7988
0.8047
0.8154
0.8184
0.5059
0.5148
0.5263
0.5488
0.6439
0.6521
0.6641
0.6781
OQA-IV for comparison. During hash code optimization, SMH-OQA-IV ﬁrstly relaxes the
discrete constraints to learn hash codes and then obtains binary codes by mean-thresholding.
The objective function of SMH-OQA-IV is similar to our method as Eq.(4.7) and the
difference is on the updating rule of the hash code matrix B as
B = (βIr + αRHHT RT )−1(βRH + αrRHST ).
(4.61)
The ﬁnal binary hash codes B are obtained by mean-thresholding. The optimization of
other variables is similar to that of our proposed SMH-OQA. The retrieval performance of
SMH-OQA-IV and SMH-OQA are shown in Table4.9. We can observe that the performance
of our proposed method is signiﬁcantly better than that of SMH-OQA-IV on three datasets.
With discrete optimization, the quantization loss can be effectively reduced and thus the
retrieval performance can be improved.
Parameter Sensitivity Analysis (RQ 5)
In this section, we perform a grid search to tune the key parameters of SMH-OQA and
UMH-OQA while ﬁxing the other parameters. We report the MAP results on MIR Flickr
when the hash code length is ﬁxed to 128 bits in Fig.4.7.
We ﬁrst conduct empirical experiments to observe the performance variations with the
involved parameters p, α, β and δ in SMH-OQA. The number of anchor points p is varied
from the range of {100, 200, ..., 1000} by ﬁxing other parameters. From Fig.4.7a, we
ﬁnd that the performance is relatively stable when the number of anchor points is varied
from 100 to 1000. Since α, β and δ are equipped in the overall objective function, we
vary the value of them from the range of {10−5, 10−3, 10−1, 10, 103, 105} while ﬁxing the
others. From Fig.4.7b–d, we can ﬁnd that the performance is relatively stable when α is
in the range of {10−5, 10−3}, β is in the range of {10−1, 101}, and δ is in the range of
{10−5, 10−3, 10−1, 101}.
In addition, we conduct experiments to observe the performance variations of UMH-OQA
with the involved three parameters: balance parameter α, the dimension D of modality-
speciﬁc latent space, and the number of prototypes K in each codebook. Balance parameter

4.3
Online Multi-modal Hashing with Dynamic Query-Adaption
125
Fig. 4.7 Performance variations with the key parameters of our models on MIR Flickr when the
hash code length is ﬁxed to 128 bits. a The number of anchors p in SMH-OQA. b–d The relation
between α, β and δ in SMH-OQA. e Balance parameter α in UMH-OQA. f The dimension D of
modality-consistent latent space in UMH-OQA. g The number of prototypes K in each codebook in
UMH-OQA
α controls the latent representation learning process at the ofﬂine model learning stage
in Eq.(4.33) and the online hash learning stage in Eq.(4.49). We evaluate the impact of
performance with different α in the range of {10−4, 10−3, 10−2, 10−1, 101, 102, 103, 104}
while ﬁxing the other parameters. The results in Fig.4.7e show that the performance of
UMH-OQA is relatively stable when α is larger than 1. Also, the experiment is conducted to
reveal the impact of the dimension D by ﬁxing α and K. Figure4.7f shows the performance
of UMH-OQA variations with respect to D in the range of {10, 50, 100, 500, 1000}. As
shown in the results, the performance of UMH-OQA is relatively stable when we set D in
a range of {100, 1000}. As mentioned in Sect.4.3.4, the hash code length r = Mlog2K,
where M is the number of codebooks and K is the number of prototypes in each codebook.
To observe the impact of K, we tune the value of K from {22, 24, 26} and vary the number of
codebook from {M = 1, M = 2, M = 3, M = 4, M = 5}, getting the performance change
of different hash codes, and observe the MAP results. Figure4.7g shows that K makes little
difference to the performance, thus we set K = 24, which is enough to get a satisfactory
performance.
Convergence Analysis (RQ 6)
To verify the convergence property of our models, we report the variations of objective
function value with 30 iterations at the ofﬂine training and online retrieval stage, respectively.
We report the results on MIR Flickr when the code length is ﬁxed to 128 bits. From the
results in Fig.4.8, we observe that there are two stages for each curve: at the ﬁrst stage,

126
4
Composite Multi-modal Hashing
0
5
10
15
20
25
30
 Number of iterations
2.608
2.61
2.612
2.614
2.616
2.618
2.62
 Objective value
1012
Offline model learning in SMH-OQA
(a)
0
5
10
15
20
25
30
 Number of iterations
3.115
3.12
3.125
3.13
 Objective value
1011
Online hash learning in SMH-OQA
(b)
0
5
10
15
20
25
30
 Number of iterations
25
30
35
40
 Objective value
Offline model learning in UMH-OQA
(c)
0
5
10
15
20
25
30
 Number of iterations
10
15
20
25
30
 Objective value
Online hash learning in UMH-OQA
(d)
Fig. 4.8 Variations of objective function value of a–b SMH-OQA and c–d UMH-OQA with the
number of iterations at the ofﬂine model learning and online hash learning stage on MIR Flickr when
the hash code length is ﬁxed to 128 bits
the objective function value decreases monotonically, and at the second stage, the objective
function value stops decreasing as the iteration number increases after several iterations.
These results demonstrate the convergence of our proposed models. Besides, the results
show that the convergence of the online hash learning stage needs shorter iterations than
that at the ofﬂine stage. For example, at the online stage of UMH-OQA, it converges within
5 iterations, but it takes about 10 iterations to converge at the ofﬂine stage. The main reason
is that the online stage does not need to optimize the codebooks C and thus it converges
faster.
4.4
Bit-aware Semantic Transformer Hashing
4.4.1
Motivation
There are three important bottlenecks that have not been solved well yet in recent works as
follows:
Limited semantic representation capability with shallow learning. Composite multi-
modal hashing methods can be roughly divided into shallow and deep-based ones based on
the learning framework. As shown in Table4.10, most existing composite multi-modal hash-
ing methods are shallow architectures. These shallow methods [11, 13] generally optimize
hash codes and functions based on shallow learning models, so they can not fully capture
the complex semantic information in multi-modal data [16]. Beneﬁting from the develop-
ment of deep neural networks, some methods [16, 21] introduce pre-trained networks as the
backbone structures to ﬁrst extract the representation features of the original multi-modal
data, and further design speciﬁc strategies to fuse the extracted features and generate the
binary codes. However, these deep-based methods ignore the modeling of the latent ﬁne-
grained semantic information and cannot fully exploit the powerful potential capability of
deep neural networks.
Mandatory feature-level multi-modal fusion ignores heterogeneous multi-modal
semantic gaps. As shown in Table4.10, all the existing methods perform mandatory

4.4
Bit-aware Semantic Transformer Hashing
127
Table 4.10 The main differences between our proposed method and all the compared methods.
“Model” represents the type of model architecture. “Fusion Level” shows the multi-modal fusion
level with two typical branches, i.e. feature-level and concept-level. ∗represents the corresponding
method adopts a bit-by-bit optimization strategy but still fuses the multi-modal data on the feature-
level. “Learning” indicates whether to use labels for supervision
Methods
Model
Fusion level
Learning
MFH [9]
Shallow
Feature-level
Unsupervised
MAH [10]
Shallow
Feature-level
Unsupervised
MVLH [11]
Shallow
Feature-level∗
Unsupervised
MvDH [12]
Shallow
Feature-level∗
Unsupervised
MFKH [13]
Shallow
Feature-level
Supervised
DMVH [14]
Shallow
Feature-level
Supervised
SDMH [15]
Shallow
Feature-level
Supervised
FOMH [23]
Shallow
Feature-level
Supervised
FDMH [19]
Shallow
Feature-level
Supervised
DCMVH [16]
Deep
Feature-level
Supervised
SAPMH [20]
Shallow
Feature-level
Supervised
FGCMH [21]
Deep
Feature-level
Supervised
BSTH (ours)
Deep
Concept-level
Supervised
multi-modal fusion on the coarse-grained feature-level without considering the ﬁne-grained
implicit semantic correlations among different modalities, which cannot fully bridge hetero-
geneous multi-modal semantic gaps. Principally, different modalities can describe a sample
from the multiple perspectives and they share ﬁne-grained implicit semantic concepts among
the multi-modal representations. These heterogeneous modalities can be potentially aligned
based on these shared implicit semantic concepts, which can further assist the multi-modal
fusion on the ﬁne-grained concept-level. In addition, these implicit semantic concepts can
be encoded to the hash bits, which can effectively enhance the discrimination of hash codes.
Direct coarse pairwise semantic preserving cannot effectively capture the ﬁne-
grained semantic correlations. Most existing composite multi-modal hashing methods
directly use the annotated label information to supervise the hash code learning process by
the well-designed pairwise semantic preserving objective functions, but they fail to consider
ﬁne-grained explicit semantic correlations among different samples. In the real-world sce-
nario, multiple different objects often simultaneously appear in a picture, a sentence, or a
piece of audio. The different-frequent co-occurrences of these objects indicate the seman-
tic relevance among them. These co-occurrence priors should contribute to exploiting the
explicit semantic correlations on the category-level, and further to capturing the ﬁne-grained
semantic correlations among different samples.

128
4
Composite Multi-modal Hashing
Motivated by the above analyses, in this work, we propose a Bit-aware Semantic Trans-
former Hashing (BSTH) framework to excavate bit-wise semantic concepts and simultane-
ously align the heterogeneous modalities for multi-modal hash learning on the concept-level.
Speciﬁcally: (1) The whole proposed framework is performed under the deep neural network
architecture, which can capture more intrinsic multi-modal semantics. (2) To better reduce
the heterogeneous multi-modal semantic gaps, we propose a bit-aware semantic transformer
module to align and fuse multi-modal data on the ﬁne-grained concept-level. Concretely, we
ﬁrst introduce a transformer encoder [39] to excavate bit-wise implicit semantic concepts
in a self-attention manner, which can achieve implicit semantic alignment among heteroge-
neous modalities. Then we perform multi-modal fusion on the ﬁne-grained concept-level to
enhance the semantic representation capability of each implicit concept. Finally, we adopt
bit-wise hash functions to encode the fused semantic concepts to the corresponding hash
bits. (3) To supervise the bit-aware transformer module, we propose a label prototype learn-
ing module to model the explicit semantic correlations on the category-level by considering
the co-occurrence priors. This module learns prototype embeddings for all categories and
utilizes them to generate the supervising hash codes, which can preserve the ﬁne-grained
semantic correlations among different samples. These supervising hash codes are used to
guide the learning process of the bit-aware semantic transformer module. The main contri-
butions of our method can be summarized as follows:
• We propose a Bit-aware Semantic Transformer Hashing (BSTH) framework to excavate
bit-wise semantic concepts and simultaneously align the heterogeneous modalities for
multi-modal hash learning on the concept-level. Speciﬁcally, we introduce the trans-
former encoder to extract bit-wise implicit semantic concepts in a self-attention manner
and achieve implicit semantic alignment among different modalities on the ﬁne-grained
concept-level. Then we perform the concept-level multi-modal fusion to enhance the
semantic representation capability of each implicit concept and design bit-wise hash
functions to encode these fused concept representations to the corresponding hash bits.
• Tosupervisethebit-awaretransformermodule,alabelprototypelearningmoduleisdevel-
oped to learn prototype embeddings for all categories that capture the explicit semantic
correlations on the category-level by considering the co-occurrence priors. This module
generates the supervising hash codes by the learned label prototype embeddings, which
can better guide the learning process of the bit-aware semantic transformer module.
• Experiments on three widely tested multi-modal retrieval datasets demonstrate the supe-
riority of the proposed method from various aspects. Particularly, in the large-scale sce-
nario, our method outperforms the second best methods on the MAP metric by 6.81%
and 7.92% on NUS-WIDE and MS COCO, with 128 bits, respectively.

4.4
Bit-aware Semantic Transformer Hashing
129
4.4.2
Methodology
The overview of our proposed Bit-aware Semantic Transformer Hashing (BSTH) framework
is shown in Fig.4.9, which is mainly composed of a bit-aware semantic transformer module
and a label prototype learning module. The bit-aware semantic transformer module learns
the implicit semantic concept corresponding to each hash bit in a self-attention manner and
performs the multi-modal fusion on the ﬁne-grained concept-level. These fused concept
representations are encoded to the corresponding hash bits via bit-wise hash functions. The
label prototype learning module learns the prototype embeddings for all categories by con-
sidering the co-occurrence priors, which can capture the explicit semantic correlations on
the category-level. This module generates the supervising hash codes by linear combina-
tions of these embeddings, which can guide the learning process of the bit-aware semantic
transformer module. In the next sections, we will ﬁrst give notations and problem deﬁnition,
and further introduce the motivation and details of these two modules.
Notations and Problem Deﬁnition
Notations. In this work, we utilize boldface uppercase letters, e.g., Y, and boldface lowercase
letters, e.g., y, to denote matrices and vectors, respectively. || · ||2 denotes the L2-norm and
|| · ||F denotes the Frobenius norm. For the convenience of description, we present the
Transformer
Encoder
Modality-1
(Image)
Modality-2
(Text)
Backbone-2
Backbone-1
Embedding
Embedding
Co-occurrence Priors
Transformer
Encoder
MLP
MLP
...
...
DeLayer
DeLayer
...
...
...
...
...
...
...
HashLayer
dog
cat
car
person
...
dog
cat
car
person
...
Transformer
Encoder
...
Coarse Concepts
Refined Concepts
...
Label Embeddings
HashLayer
Fused Concepts
Label Prototype Embeddings
k-relaxed-value
c
k
k
k-relaxed-
value
c
Bit-aware Semantic Transformer
Label Prototype Learning
Supervising 
Hash Code
sign
c
il
k-bits
k-bits
Objective
Data Flow
Addition
Multiplication 
Objective
Data Flow
Addition
Multiplication 
Final Hash 
Code
sign
Classification 
Loss
Pairwise 
Similarity Loss
Quantization 
Loss
Classification 
Loss
Co-occurrence Statistic 
Reconstruction Loss
Quantization 
Loss
Bit Balance 
Loss
( )
I
ix
( )
T
ix
( )
I
if
( )
T
if
Classificatio
Loss
Pairwi
Similarity
Quantizati
Loss
Classifica
Loss
Quantiza
Loss
Co-occ
Recon
Bit Balanc
Loss
Fig. 4.9 The framework of our proposed BSTH. BSTH mainly contains the Bit-aware Semantic
Transformer module and the Label Prototype Learning module. In the Bit-aware Semantic Trans-
former module, the modality-speciﬁc feature representation is ﬁrst decomposed to the coarse implicit
semantic concepts by the feature decoupling layer, and then, the transformer encoder reﬁnes them in a
self-attention manner. Finally, the multi-modal fusion is performed on the ﬁne-grained concept-level
and these fused concept representations are independently coded to the corresponding hash bits by
bit-wise hash functions. The label prototype learning module learns the prototype embeddings for all
categories by considering the co-occurrence priors and these embeddings are further used to generate
the supervising hash codes to guide the learning process of the bit-aware semantic transformer module

130
4
Composite Multi-modal Hashing
subsequent explanations under the bi-modal scenario, i.e. image and text. As shown in the
Fig.4.9, the modality-speciﬁc branches in the bit-aware semantic transformer module are
structurally same, except for the corresponding feature extractors. Therefore, our proposed
framework can be easily extended to three or more modalities cases.
Suppose that the training dataset Otr = {oi}n
i=1 consists of n samples with image and
text modalities. We utilize the modality-speciﬁc feature extractors, e.g., VGGNet [34] and
Bag-of-Words (BoW), to extract the feature representations for original visual and textual
data, denoted as X(I) = [x(I)
1 , x(I)
2 , ..., x(I)
n ] ∈Rn×d(I) and X(T ) = [x(T )
1 , x(T )
2 , ..., x(T )
n ] ∈
Rn×d(T ), where d(I) and d(T ) indicate the corresponding dimensionalities of the extracted
visual feature representations and textual feature representations, respectively. L =
[l1, l2, ..., ln] ∈{0, 1}n×c denotes the label matrix of the training data, where li j = 1 indi-
cates the ith sample belongs to the jth category and vice versa, c is the number of the
categories. B = [b1, b2, ..., bn] ∈{−1, 1}n×k represents the k-bits hash code matrix.
Problem Deﬁnition. When multi-modal samples arrive, the hashing-based multi-modal
retrieval system needs to integrate multi-modal heterogeneous information and generate joint
feature representations. Then these representations are quantiﬁed to hash codes to support
efﬁcient search via Hamming distance.
Bit-aware Semantic Transformer
Motivation and Discussion. Generally, existing composite multi-modal hashing methods
[11, 16, 21] adopt mandatory coarse-grained feature-level multi-modal fusion, e.g., addition
and concatenation fusion strategies, which is hard to bridge heterogeneous semantic gaps and
capture ﬁne-grained semantic correlations among different modalities. Principally, each hash
bit can represent a corresponding implicit semantic concept. Hence, we can determine bit-
wise implicit semantic concepts as a bridge to perform the ﬁne-grained semantic alignment
and concept-level fusion among multiple modalities, which can simultaneously reduce the
multi-modal semantic gaps and enhance concept representation capability.
Based on the above discussion, we ﬁrst design a feature decoupling layer to decompose
the modality-speciﬁc feature representations to the corresponding coarse implicit semantic
concepts. The transformer encoder is further introduced to adaptively capture the potential
correlations among these implicit semantic concepts and reﬁne them in a self-attention
manner. These reﬁned implicit semantic concepts can align multiple modalities and reduce
the heterogeneous multi-modal semantic gaps. Then we perform multi-modal fusion on
the ﬁne-grained concept-level to generate more representative semantic concepts. Finally,
these fused semantic concepts are independently encoded to the corresponding hash bits by
bit-wise hash functions, which can generate the more discriminative hash codes.
Details. As shown in Fig.4.9, the extracted modality-speciﬁc feature representations
x(∗)
i
are mapped to the feature representations f (∗)
i
with the same dimensionality via the
corresponding Multi-Layer Perceptron (MLP) architectures MLP(∗)(·; θ(∗)
mlp) as follows:
f (∗)
i
= MLP(∗)(x(∗)
i
; θ(∗)
mlp), s.t.∗∈{I, T },
(4.62)

4.4
Bit-aware Semantic Transformer Hashing
131
where θ(∗)
mlp is the trainable parameters. Then, we design feature decoupling layers
DeLayer(∗)(·; θ(∗)
de ) to decompose the projected feature representations f (∗)
i
∈R1×dc into
a sequence C(∗)
i
with k coarse implicit semantic concepts:
C(∗)
i
= [c(∗)
i1 , c(∗)
i2 , ..., c(∗)
ik ]
= DeLayer(∗)(f (∗)
i
; θ(∗)
de ),
(4.63)
where c(∗)
ik ∈R1×dc represents the kth coarse implicit semantic concept representation of the
ith sample with the speciﬁc modality, dc denotes the dimensionality of the concept repre-
sentation, θ(∗)
de is the trainable parameters. Speciﬁcally, we implement this layer by a linear
projection layer to map the f (∗)
i
to ˜f
(∗)
i
∈R1×(k×dc), and a reshape operator to convert the ˜f
(∗)
i
to C(∗)
i
∈Rk×dc, denoting k coarse implicit semantic concept representations. Intuitively,
these concepts are decoupled from an individual modality-speciﬁc feature representation
f (∗)
i
, so they can collaboratively represent the original ith sample with the speciﬁc modality.
We further introduce the modality-speciﬁc transformer encoder to capture the potential
correlations among these k coarse implicit semantic concepts in a self-attention manner and
generate the reﬁned implicit semantic concepts. It can be formulated as follows:
˜C
(∗)
i
= TransformerEncoder(∗)(C(∗)
i
; θ(∗)
enc),
(4.64)
where ˜C
(∗)
i
∈Rk×dc represents the reﬁned semantic concept sequence of ith sample and
θ(∗)
enc is the trainable parameters. The transformer encoder is composed of three submodules:
positional encoding submodule, multi-head self-attention submodule, and feed-forward net-
work. The details of these submodules can refer to [39]. These implicit semantic concepts
can align the heterogeneous modalities to reduce the semantic gaps. Then we can better
perform multi-modal fusion on the ﬁne-grained concept-level as follows:
˜C
f
i =

∗∈{I,T }
˜C
(∗)
i
, ˜C
f
i ∈Rk×dc,
(4.65)
˜C
f
i = [˜c f
i1, ˜c f
i2, ..., ˜c f
ik],
(4.66)
where ˜C
f
i represents the fused implicit semantic concept sequence of ith sample, denoted in
Eq.(4.66). Finally, we adopt bit-wise hash functions to encode these fused semantic concept
representations into the corresponding hash bits as follows:
hi = Concat(H1(˜c f
i1; θh1), H2(˜c f
i2; θh2), ..., Hk(˜c f
ik; θhk)),
(4.67)
bi = sign(hi), hi ∈R1×k, bi ∈{−1, 1}1×k,
(4.68)
where Hk(·; θhk) with the trainable parameters θhk represents the bit-wise hash function
corresponding to the kth hash bit, which is implemented by linear projections with the acti-

132
4
Composite Multi-modal Hashing
vation function tanh(·). Hk(·; θhk) projects kth fused implicit semantic concept representa-
tion ˜c f
ik ∈R1×dc into a relaxed value of the corresponding hash bit. Then we concatenate
these k relaxed values into hi and perform an element-wise sign function, i.e. sign(·), to
generate the ﬁnal hash code bi. The bit-wise encoding process can effectively enhance the
independence of each hash bit.
For preserving the label semantic information, we utilize a fully-connected layer
FC(·; θ f c) with the trainable parameters θ f c to predict the pseudo labels based on the
learned relaxed representation hi as follows:
˜li = sigmoid(FC(hi; θ f c)).
(4.69)
Label Prototype Learning
Motivation and Discussion. As we all know, in the real world, several objects often appear
simultaneously in a scene, and the statistical co-occurrence probabilities between paired
objects are different. The explicit semantic correlations among different objects can be
capturedbyconsideringthesestatisticalco-occurrencepriors.However,theexistingmethods
regard the label vector as a whole for directly guiding the learning processes of their models,
which cannot model explicit semantic correlations among different categories.
Therefore, we design a label prototype learning module to capture these explicit semantic
correlations on the category-level by considering the co-occurrence priors on the dataset.
This module learns the prototype embeddings for all categories to generate the supervising
hash codes by linear combinations of the learned embeddings. Therefore, these supervising
hash codes can better preserve the ﬁne-grained semantic correlations and guide the learning
process of the bit-aware semantic transformer module.
Details. Firstly, all categories are projected into the feature embeddings with an
Embedding(·; θel) layer as follows:
EL = Embedding(S L; θel), EL ∈Rc×k,
(4.70)
where S L is the category sequence, e.g., [“dog”, “cat”, “person”, ...], and θel is the trainable
parameters. To learn the label prototype embeddings with preserving the explicit semantic
correlations among different categories, we introduce a transformer encoder to model these
correlations as follows:
˜E
L = TransformerEncoder(L)(EL; θL
enc), ˜E
L ∈Rc×k,
(4.71)
˜E
L = [˜eL
1 , ˜eL
2 , ..., ˜eL
c ],
(4.72)
where θL
enc is the trainable parameters. Then we can learn the ﬁnal label prototype embed-
dings by the category-wise hash function Hp
c (·) as follows:

4.4
Bit-aware Semantic Transformer Hashing
133
PL = [pL
1 , pL
2 , ..., pL
c ]
= [Hp
1 (˜eL
1 ; θ p
h1), Hp
2 (˜eL
2 ; θ p
h2), ..., Hp
c (˜eL
c ; θ p
hc)],
(4.73)
where PL ∈Rc×k denotes all the label prototype embeddings and θ p
hc is the trainable param-
eters. To avoid the quantization errors caused by sign(·) operator, we adopt the relaxed label
prototype embeddings rather than binary representations. When the label prototype embed-
dings are learned, we use the annotated vector to generate the supervising hash code by
linear combination of these embeddings as follows:
hL
i = liPL, hL
i ∈R1×k,
(4.74)
bL
i = sign(hL
i ), bL
i ∈{−1, 1}1×k.
(4.75)
So this supervising hash code can preserve the explicit semantic correlations and is used
to guide the learning process of the bit-aware semantic transformer module.
Finally, similar to Eq.(4.69), we predict the pseudo label to preserve the label semantic
information into the supervising hash code with FC(L)(·; θL
f c) as follows:
˜l
L
i = sigmoid(FC(L)(hL
i ; θL
f c)),
(4.76)
where θL
f c is the trainable parameters.
Objective Function
Bit-aware Semantic Transformer. Under the supervised scenario, we utilize the classiﬁ-
cation loss Lcl f to preserve the annotated semantic information into the predicted pseudo
label:
Lcl f = ||˜li −li||2
2.
(4.77)
To minimize the quantization errors by sign(·) operator, we utilize the supervising hash
code bL
i output from the label prototype learning module in Eq.(4.75) to guide the learning
process of the relaxation representation hi in Eq.(4.67):
Lsign = ||hi −bL
i ||2
2.
(4.78)
To preserve the pairwise correlations between samples, the following loss function is
designed with cosine similarity:
Lsim = || cos(hi, h j) −Si j||2
2,
(4.79)
whereSistheafﬁnitymatrix,whichcanmodeltheﬁne-grainedassociationsbetweenrelevant
samples. Si j is constructed as follows:
Si j =
2
1 + e−lilT
j
−1,
(4.80)

134
4
Composite Multi-modal Hashing
where Si j ∈[0,
2
1+e−c −1].
Finally, we can obtain the whole objective function in the learning stage of the bit-aware
semantic transformer module as follows:
min
BaT L = β1Lcl f + β2Lsign + β3Lsim,
(4.81)
where β1, β2, β3 are the trade-off hyper-parameters, BaT denotes all the above trainable
parameters in the bit-aware semantic transformer module.
Label Prototype Learning. Similar to Eq.(4.77), we also utilize the label information
to guide the training process of the label prototype learning module, that is:
Jcl f = ||˜l
L
i −li||2
2.
(4.82)
The quantization loss can be formulated as follows:
Jsign = ||hL
i −bL
i ||2
2.
(4.83)
To preserve the explicit semantic correlations among different categories into the
label prototype embeddings by considering the co-occurrence priors, we develop the co-
occurrence statistic reconstruction loss as follows:
Jproto = || cos(PL, PL) −˜R||2
F,
(4.84)
where ˜R ∈Rc×c is the normalized co-occurrence statistic matrix. ˜Ri j represents the co-
occurrence frequency on the training dataset between the ith and jth category.
In addition, the bit balance loss is introduced to maximize the information entropy [40]
of the hash code, that is:
Jbal = ||hL
i 1||2
2,
(4.85)
where 1 is a column vector ﬁlled with 1.
Finally, the whole objective function of the label prototype learning module can be
formulated as follows:
min
L PL J = γ1Jcl f + γ2Jsign + γ3Jproto + γ4Jbal,
(4.86)
where γ1, γ2, γ3, γ4 are the trade-off hyper-parameters, L PL denotes all the above trainable
parameters in the label prototype learning module.
Optimization and Out-of-Sample Extension
We ﬁrst optimize the label prototype learning module and generate the supervising hash
codes for all the training samples. Then, we use these supervising hash codes to guide the
learning process of the bit-aware semantic transformer module. When a new multi-modal
query arrives, we can generate the corresponding hash code by the bit-aware semantic
transformer module.

4.4
Bit-aware Semantic Transformer Hashing
135
4.4.3
Experiment
Implementation Details
The implementation details of all the mentioned submodules are summarized as follows:
MLP(∗)(·): Both MLP(I)(·) and MLP(T )(·) for visual and textual modalities consist of
two linear projection layers (d(I) →2, 048 →dc) and (d(T ) →1, 024 →dc), respectively,
where dc = 128.
TransformerEncoder(∗)(·): The visual and textual modalities adopt same settings, i.e.
single-head and two encoder layers.
Embedding(·): The Embedding layer in PyTorch5 is used to transfer all categories into
the corresponding embeddings.
TransformerEncoder(L)(·): It adopts the setting with single-head and one encoder layer.
H(·): All the bit-wise hash functions in Eq.(4.67) consist of two linear projection lay-
ers (dc →dc
2 →1), which project the concept representations into relaxed values of the
corresponding hash bits.
Hp(·): All the hash functions in Eq.(4.73) contain one linear projection layer (k →k).
The values of all the trade-off hyper-parameters are same on three datasets: {β1 = 1,
β2 = 0.01, β3 = 1, γ1 = 0.001, γ2 = 100, γ3 = 1, γ4 = 0.01}. At the training stage, we
set the batch size as 1,024 and optimize the whole network by a standard back-propagation
(BP) algorithm with Adam optimizer [41]. The learning rate is empirically set to 0.001 for
both the bit-aware semantic transformer module and the label prototype learning module.
Our method is implemented via PyTorch and all the experiments are conducted on a server
with a single GPU (NVIDIA RTX 2080Ti) and a CPU (2.20GHz Intel (R) Xeon (R) Silver
4214).
Evaluation Baselines
We compare our proposed method with twelve state-of-the-art (SOTA) composite multi-
modal hashing methods, including four unsupervised methods: MFH [9], MAH [10], MVLH
[11], MvDH [12], and eight supervised methods: MFKH [13], DMVH [14], SDMH [15],
FOMH [23], FDMH [19], DCMVH [16], SAPMH [20], FGCMH [21]. All the compared
methods are summarized in Sect.4.2. Related Work.
Quantitative Results and Comparisons
The MAP results of our method and all the compared baselines varying with the different
hash code lengths (i.e. 16, 32, 64, and 128 bits) on three datasets are reported in Table4.11.
Note that, for fair comparisons, the MAP results of the compared baselines refer to the results
provided in the original works directly. From the Table4.11, we can obtain the following
observations and analyses.
Compared with all the state-of-the-art baselines, our method consistently achieves sig-
niﬁcant performance improvements with all hash code lengths on NUS-WIDE and MS
COCO. Speciﬁcally, our method outperforms the second best methods by 3.13%, 4.66%,
5 https://pytorch.org/.

136
4
Composite Multi-modal Hashing
Table 4.11 MAP comparison results on MIR Flickr, NUS-WIDE and MS COCO. The best result in each column is marked with bold. The second
best result in each column is underlined
Methods
Ref.
MIR Flickr
NUS-WIDE
MS COCO
16 bits
32 bits
64 bits
128 bits
16 bits
32 bits
64 bits
128 bits
16 bits
32 bits
64 bits
128 bits
MFH [9]
TMM13 0.5795
0.5824
0.5831
0.5836
0.3603
0.3611
0.3625
0.3629
0.3948
0.3966
0.3960
0.3980
MAH [10]
TIP15
0.6488
0.6649
0.6990
0.7114
0.4633
0.4945
0.5381
0.5476
0.3967
0.3943
0.3966
0.3988
MVLH [11]
MM15
0.6541
0.6421
0.6044
0.5982
0.4182
0.4092
0.3789
0.3897
0.3993
0.4012
0.4065
0.4099
MvDH [12]
TIST18
0.6828
0.7210
0.7344
0.7527
0.4947
0.5661
0.5789
0.6122
0.3978
0.3966
0.3977
0.3998
MFKH [13]
MM12
0.6369
0.6128
0.5985
0.5807
0.4768
0.4359
0.4342
0.3956
0.4216
0.4211
0.4230
0.4229
DMVH [14]
ICMR17 0.7231
0.7326
0.7495
0.7641
0.5676
0.5883
0.6092
0.6279
0.4123
0.4288
0.4355
0.4563
SDMH [15]
TMM19 0.7316
0.7400
0.7568
0.7723
0.6321
0.6346
0.6626
0.6648
–
–
–
–
FOMH [23]
MM19
0.7557
0.7632
0.7654
0.7705
0.6329
0.6456
0.6678
0.6791
0.5008
0.5148
0.5172
0.5294
FDMH [19]
NPL20
0.7802
0.7963
0.8094
0.8181
0.6575
0.6665
0.6712
0.6823
0.5404
0.5485
0.5600
0.5674
DCMVH [16]
TIP20
0.8097
0.8279
0.8354
0.8467
0.6509
0.6625
0.6905
0.7023
0.5378
0.5427
0.5490
0.5576
SAPMH [20]
TMM21 0.7657
0.8098
0.8188
0.8191
0.6503
0.6703
0.6898
0.6901
0.5467
0.5502
0.5563
0.5672
FGCMH [21]
MM21
0.8173
0.8358
0.8377
0.8406
0.6677
0.6874
0.6936
0.7011
0.5641
0.5723
0.5797
0.5862
BSTH (ours)
–
0.8145
0.8340
0.8482
0.8571
0.6990
0.7340
0.7505
0.7704
0.5831
0.6245
0.6459
0.6654

4.4
Bit-aware Semantic Transformer Hashing
137
Table 4.12 MAP results on the extended MIR Flickr
Methods
Ref.
16 bits
32 bits
64 bits
128 bits
FDMH [19]
NPL20
0.7897
0.8131
0.8314
0.8414
DCMVH [16]
TIP20
0.6379
0.7490
0.7759
0.7946
SAPMH [20]
TMM21
0.7595
0.8125
0.8166
0.8302
BSTH (ours)
–
0.8791
0.8998
0.9141
0.9205
5.69%, 6.81% on NUS-WIDE, and 1.9%, 5.22%, 6.62%, 7.92% on MS COCO, with 16,
32, 64 and 128 bits, respectively. On MIR Flickr, our method achieves the comparable per-
formances compared with competitive DCMVH and FGCMH. We can ﬁnd that our method
performs better on the large-scale NUS-WIDE and MS COCO datasets. In addition, most
compared baselines and ours achieve different MAP performance improvements as the hash
code length increases, and our method improves MAP performance more obviously with
longer hash code. The main reason for the above observations is that our proposed method
can excavate more accurate and comprehensive bit-wise semantic concepts with more train-
ing samples and longer hash code, which can better assist the ﬁne-grained concept-level
multi-modal fusion. Hence, our method can be effectively extended to large-scale retrieval
scenarios. In addition, the compared methods adopt directly coarse semantic preserving
strategies based on label vectors without considering ﬁne-grained semantic correlations.
In contrast, our method learns the prototype embeddings for all categories by considering
the co-occurrence priors and generates the supervising hash codes, which can preserve the
ﬁne-grained semantic correlations among different samples. These supervising hash codes
can fully exploit the annotated semantic information and guide the learning process of the
bit-aware semantic transformer module.
To further validate the above analysis, we design a supplementary experiment on MIR
Flickr, which serves the entire retrieval set as the training set and compares our method with
several most competitive baselines, i.e. FDMH, DCMVH and SAPMH. The comparison
results are shown in Table4.12. We can ﬁnd that FDMH, SAPMH and our method achieve
different MAP performance improvements by enlarging the training set, and our method
is more signiﬁcant. Our method outperforms the second best methods by 8.94%, 8.67%,
8.27%, and 7.91% with 16, 32, 64 and 128 bits, respectively. However, the performance of
DCMVH shows a decreasing trend, which demonstrates the limited ability of DCMVH to
be extended to large-scale retrieval scenarios.
As shown in Table4.11, compared with the shallow methods, the deep-based methods (i.e.
DCMVH and FGCMH) achieve different but limited MAP performance improvements. In
contrast, our method outperforms them more signiﬁcantly on NUS-WIDE and MS COCO,
which indicates that our method can fully exploit the powerful representation capability of
deep neural networks and be more effectively extended to large-scale scenarios. In addition,
the supervised methods perform better than the unsupervised methods except MFKH on

138
4
Composite Multi-modal Hashing
MIR Flickr and NUS-WIDE. Compared with unsupervised methods, supervised methods
can exploit semantic information and improve retrieval performance.
To verify the scalability of our proposed method, we report the training time and test-
ing time of several composite multi-modal hashing baselines on three benchmark datasets,
shown in Table4.14. Compared with the shallow methods, our approach consumes more
time in the training stage. The main reason is that we need more training iterations to fully
excavate bit-wise implicit semantic concepts to align heterogeneous modalities and reduce
multi-modal semantic gaps in the bit-aware semantic transformer module. As the training
stage is in the off-line mode, this time consumption is acceptable. In addition, compared with
the deep-based method, i.e. DCMVH, we take more time in the testing stage because our
method needs to perform multi-modal fusion on the ﬁne-grained concept-level rather than
feature-level. Within acceptable testing time increases, our method signiﬁcantly improves
MAP performance.
Ablation Study
To evaluate the effectiveness of the key components in our framework, we design seven
variants to conduct the ablation study with the different hash code lengths (i.e. 16, 32,
64, and 128 bits) on three datasets, and the comparison results are shown in Table4.13.
These variants are summarized as follows: BSTH-T: It only utilizes the textual modality
to perform hashing-based retrieval. BSTH-I: It only utilizes the visual modality to perform
hashing-based retrieval. BSTH-FMLP: We remove the implicit concept learning submod-
ule, containing the decoupling layer and transformer structure, and replace it with a MLP
architecture. It fuses the heterogeneous multi-modal data on the feature-level rather than
the ﬁne-grained concept-level. BSTH-SH: To verify the effectiveness of bit-wise hash code
generation process, we remove the bit-wise hash functions. The k implicit semantic con-
cept representations are fused via sum operation and generate the ﬁnal hash codes with a
single hash function. BSTH-NC: It removes the guidance of co-occurrence priors in the
label prototype learning module. BSTH-PMLP: It removes the label prototype learning
module, and utilizes the original label vectors to learn the supervising hash codes in the
label prototype learning module. BSTH-SHARE: We replace the modality-speciﬁc trans-
former encoder with a shared transformer encoder. Speciﬁcally, the modality-speciﬁc coarse
implicit semantic concepts are jointly input into the shared transformer encoder. As shown
in Table4.13, we can get the following analyses:
BSTH-T and BSTH-I: Multi-modal fusion can effectively improve the retrieval perfor-
mance compared with uni-modal.
BSTH-FMLP and BSTH-SH: Our proposed method achieves fairly acceptable perfor-
mance by excavating bit-wise implicit semantic concepts to align the heterogeneous modali-
ties and performing multi-modal fusion on the ﬁne-grained concept-level. The bit-wise hash
functions can effectively enhance the discrimination of the hash codes.
BSTH-NC and BSTH-PMLP: The results of them validate the effectiveness of our
proposed label prototype learning module. The co-occurrence priors can model the explicit
semantic correlations on the category-level. Compared with directly semantic preserving by

4.4
Bit-aware Semantic Transformer Hashing
139
Table 4.13 The ablation study results on MIR Flickr, NUS-WIDE and MS COCO
Vars.
MIR Flickr
NUS-WIDE
MS COCO
16 bits
32 bits
64 bits
128 bits
16 bits
32 bits
64 bits
128 bits
16 bits
32 bits
64 bits
128 bits
BSTH-T
0.7050
0.7128
0.7222
0.7271
0.5890
0.6197
0.6411
0.6538
0.5773
0.6219
0.6390
0.6571
BSTH-I
0.7986
0.8259
0.8318
0.8326
0.6655
0.6936
0.7194
0.7326
0.4007
0.4004
0.4043
0.4018
BSTH-FMLP
0.7903
0.8032
0.8103
0.8270
0.6601
0.6696
0.6979
0.6952
0.5305
0.5513
0.5620
0.5738
BSTH-SH
0.8076
0.8153
0.8224
0.8149
0.6888
0.7154
0.7174
0.7291
0.5617
0.5610
0.5487
0.5417
BSTH-NC
0.8054
0.8165
0.8224
0.8212
0.6936
0.7168
0.7457
0.7479
0.5623
0.5817
0.6024
0.6090
BSTH-PMLP
0.8048
0.8195
0.8383
0.8450
0.6900
0.7119
0.7339
0.7525
0.5299
0.5717
0.5959
0.6189
BSTH-SHARE
0.8130
0.8272
0.8372
0.8499
0.6931
0.7131
0.7488
0.7632
0.5371
0.5604
0.6029
0.6201
BSTH
0.8145
0.8340
0.8482
0.8571
0.6990
0.7340
0.7505
0.7704
0.5831
0.6245
0.6459
0.6654

140
4
Composite Multi-modal Hashing
Table 4.14 Comparisons of training time and testing time in seconds with 64 bits on three datasets
Methods
MIR Flickr
NUS-WIDE
MS COCO
Train
Test
Train
Test
Train
Test
MFH [9]
32.05
0.52
866.17
4.59
426.19
2.17
MVLH [11]
30.14
1.63
418.04
13.99
123.29
6.57
MvDH [12]
77.28
313.52
279.68
2813.03
345.74
1847.18
MFKH [13]
150.40
0.04
235.44
0.17
155.92
0.09
SDMH [15]
13.96
2.55
46.99
19.39
35.29
8.72
FOMH [23]
3.75
2.26
12.64
20.17
12.82
11.13
FDMH [19]
0.29
0.04
0.76
0.27
0.72
0.13
DCMVH [16]
298.52
0.14
490.29
0.80
882.52
0.51
SAPMH [20]
42.55
2.10
40.92
33.19
54.29
16.79
BSTH (ours)
135.11
1.61
259.46
16.81
231.99
9.71
Table 4.15 The positional encoding on MS COCO
Image
Text
MS COCO
16 bits
32 bits
64 bits
128 bits
0.5707
0.6136
0.6329
0.6462
✓
0.5806
0.6235
0.6337
0.6527
✓
0.5799
0.6163
0.6347
0.6541
✓
✓
0.5831
0.6245
0.6459
0.6654
label vectors, the learned label prototype embeddings can better preserve the ﬁne-grained
semantic correlations among samples into the supervising hash codes.
BSTH-SHARE: We can observe that adopting the shared transformer encode leads to
a decrease of MAP performance. The reason is that the modality heterogeneity interferes
with the correlation modeling of implicit semantic concepts in the transformer encoder.
In addition, we observe the inﬂuence of the position encoding on MAP performance in
different modalities. The comparison results on MS COCO are shown in Table4.15. We can
observe that the position encodings for both visual modality and textual modality achieve
different performance improvements. The potential reason is that the position encoding can
enhance the diversity of the implicit semantic concepts.

4.5
Summary
141
0.0001 0.001
0.01
0.1
1
10
100
0
0.2
0.4
0.6
0.8
1
MAP
γ1
0.0001 0.001
0.01
0.1
1
10
100
0
0.2
0.4
0.6
0.8
1
MAP
γ2
0.0001 0.001
0.01
0.1
1
10
100
0
0.2
0.4
0.6
0.8
1
MAP
γ3
0.0001 0.001
0.01
0.1
1
10
100
0
0.2
0.4
0.6
0.8
1
MAP
γ4
0.0001 0.001
0.01
0.1
1
10
100
0
0.2
0.4
0.6
0.8
1
MAP
β1
0.0001 0.001
0.01
0.1
1
10
100
0
0.2
0.4
0.6
0.8
1
MAP
β2
0.0001 0.001
0.01
0.1
1
10
100
0
0.2
0.4
0.6
0.8
1
MAP
β3
Fig.4.10 The parameter sensitivity results on MS COCO with 64 bits
Parameter Sensitivity
There are seven hyper-parameters in our method to trading off all the loss function terms: β1,
β2, β3, γ1, γ2, γ3 and γ4, as shown in Eqs.(4.81) and (4.86). To observe MAP performance
variationswiththesehyper-parameters,weconducttheparameterexperimentsonMSCOCO
with 64 bits and tune them in the range of {0.0001, 0.001, 0.01, 0.1, 1, 10, 100}. As shown
in Fig.4.10, we can ﬁnd that the MAP performance is relatively stable in a wide range of
γ1, γ2, γ3 and γ4, and in a certain range of β1, β2. When β3 = 1, the best performance can
be achieved.
4.5
Summary
In this chapter, we mainly introduce two proposed methods, named online multi-modal
hashing with dynamic query-adaption and bit-aware semantic transformer hashing, respec-
tively. For the ﬁrst work, we propose composite multi-modal hashing with the online query
adaption model in both unsupervised and supervised learning paradigms to learn hash codes
for efﬁcient multimedia retrieval. A simple but effective adaptive multi-modal fusion strat-
egy is designed in our models to learn hash codes by automatically computing the modality
weights according to the speciﬁc multimedia contents. In our proposed Supervised Multi-
modal Hashing with Online Query-adaption (SMH-OQA) method, we develop an efﬁcient
hash code learning module to enhance the discriminative capability of hash codes with pair-
wise semantics, and at the same time, avoid the challenging symmetric semantic matrix
factorization and storage cost of the semantic graph. In our proposed Unsupervised Multi-
modalHashingwithOnline Query-adaption(UMH-OQA)method,wedesignanewadaptive
multi-modal quantization strategy to automatically compute adaptive modality combination
weights according to the varied multimedia contents. The binary quantization errors can be
alleviated without the reliance on the particular hash learning formulations. In both models,

142
4
Composite Multi-modal Hashing
we propose a query-adaptive and self-weighted online hashing module to accurately capture
the variations of different queries. The online module is parameter-free to avoid the time-
consuming and inaccurate parameter adjustment in the unsupervised query hashing process.
For another work, the Bit-aware Semantic Transformer Hashing (BSTH) model can exca-
vate bit-wise semantic concepts and simultaneously align the heterogeneous modalities for
multi-modal hash learning on the concept-level. We design bit-wise hash functions to encode
the implicit semantic concepts to the corresponding hash bits. To supervise the bit-aware
transformer module, we develop a label prototype learning module to learn prototype embed-
dings for all categories that capture the explicit semantic correlations on the category-level
by considering the co-occurrence priors. This module generates the supervising hash codes
for guiding the bit-aware semantic transformer module. Extensive experiments demonstrate
the superiority of these two models from various aspects.
References
1. Gong, Y., Lazebnik, S., Gordo, A., & Perronnin, F. (2013). Iterative quantization: A procrustean
approach to learning binary codes for large-scale image retrieval. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 35(12), 2916–2929.
2. Shen, F., Shen, C., Liu, W., & Tao Shen, H. (2015). Supervised discrete hashing. In Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition (pp. 37–45).
3. Liu, W., Cun, M., Kumar, S., & Chang, S.-F. (2014). Discrete graph hashing. Proceedings of the
Advances in Neural Information Processing Systems, 2014, 3419–3427.
4. Kumar, S., & Udupa, R. (2011). Learning hash functions for cross-view similarity search. In
Proceedings of the International Joint Conference on Artiﬁcial Intelligence (pp. 1360–1365).
5. Jiang, Q. Y., & Li, W. J. (2017). Deep cross-modal hashing. In Proceedings of the IEEE Confer-
ence on Computer Vision and Pattern Recognition (pp. 3232–3240).
6. Xu, X., Shen, F., Yang, Y., Shen, H. T., & Li, X. (2017). Learning discriminative binary codes for
large-scale cross-modal retrieval. IEEE Transactions on Image Processing, 26(5), 2494–2507.
7. Mandal, D., Chaudhury, K. N., & Biswas, S. (2018). Generalized semantic preserving hashing
for cross-modal retrieval. IEEE Transactions on Image Processing, 28(1), 102–112.
8. Zhu, L., Huang, Z., Li, Z., Xie, L., & Shen, H. T. (2018). Exploring auxiliary context: discrete
semantic transfer hashing for scalable image retrieval. IEEE Transactions on Neural Networks
and Learning Systems, 29(11), 5264–5276.
9. Song, J., Yang, Y., Huang, Z., Shen, H. T., & Luo, J. (2013). Effective multiple feature hashing for
large-scale near-duplicate video retrieval. IEEE Transactions on Multimedia, 15(8), 1997–2008.
10. Liu, L., Yu, M., & Shao, L. (2015). Multiview alignment hashing for efﬁcient image search.
IEEE Transactions on Image Processing, 24(3), 956–966.
11. Shen, X., Shen, F., Sun, Q. S., & Yuan, Y. H. (2015). Multi-view latent hashing for efﬁcient
multimedia search. In Proceedings of the ACM International Conference on Multimedia (pp.
831–834).
12. Shen, X., Shen, F., Liu, L., Yuan, Y. H., Liu, W., & Sun, Q. S. (2018). Multiview discrete hashing
for scalable multimedia search. ACM Transactions on Intelligent Systems and Technology, 9(5),
53:1–53:21.
13. Liu, X., He, J., Liu, D., & Lang, B. (2012). Compact kernel hashing with multiple features. In
Proceedings of the ACM International Conference on Multimedia (pp. 881–884).

References
143
14. Yang, R., Shi, Y., & Xu, X. S. (2017). Discrete multi-view hashing for effective image retrieval.
In Proceedings of the ACM International Conference on Multimedia Retrieval (pp. 175–183).
15. Lu, X., Zhu, L., Li, J., Zhang, H., & Shen, H. T. (2020). Efﬁcient supervised discrete multi-view
hashing for large-scale multimedia search. IEEE Transactions on Multimedia, 22(8), 2048–2060.
16. Lei Zhu, X. L., Cheng, Z., Li, J., & Zhang, H. (2020). Deep collaborative multi-view hashing
for large-scale image search. IEEE Transactions on Image Processing, 29(2020), 4643–4655.
17. Zheng, C., Zhu, L., Lu, X., Li, J., Cheng, Z., & Zhang, H. (2020). Fast discrete collaborative
multi-modal hashing for large-scale multimedia retrieval. IEEE Transactions on Knowledge and
Data Engineering, 32(11), 2171–2184.
18. Yan, C., Gong, B., Wei, Y., & Gao, Y. (2021). Deep multi-view enhancement hashing for image
retrieval. IEEE Transactions on Pattern Analysis and Machine Intelligence, 43(4), 1445–1451.
19. Liu, L., Zhang, Z., & Huang, Z. (2020)). Flexible discrete multi-view hashing with collective
latent feature learning. Neural Processing Letters, 52(3), 1765–1791.
20. Zheng, C., Zhu, L., Cheng, Z., Li, J., & Liu, A.-A. (2021). Adaptive partial multi-view hashing
for efﬁcient social image retrieval. IEEE Transactions on Multimedia, 23(2021), 4079–4092.
21. Lu,X.,Zhu,L.,Liu,L.,Nie,L.,&Zhang,H.(2021).Graphconvolutionalmulti-modalhashingfor
ﬂexiblemultimediaretrieval.InProceedingsoftheACMInternationalConferenceonMultimedia
(pp. 1414–1422).
22. Roweis, S. T., & Saul, L. K. (2000). Nonlinear dimensionality reduction by locally linear embed-
ding. Science, 290(5500), 2323–2326.
23. Lu, X., Zhu, L., Cheng, Z., Li, J., Nie, X., & Zhang, H. (2019). Flexible online multi-modal
hashingforlarge-scalemultimediaretrieval.InProceedingsoftheACMInternationalConference
on Multimedia (pp. 1129–1137).
24. Kipf, T. N., & Welling, M. (2017). Semi-supervised classiﬁcation with graph convolutional
networks. In Proceedings of the International Conference on Learning Representations (pp.
1–14).
25. Zhang, D., Wang, F., & Si, L. (2011). Composite hashing with multiple information sources.
In Proceedings of the International ACM SIGIR Conference on Research and Development in
Information Retrieval (pp. 225–234).
26. Kim, S., & Choi, S. (2013). Multi-view anchor graph hashing. In Proceedings of the IEEE
International Conference on Acoustics, Speech, and Signal Processing (pp. 3123–3127).
27. Bazaraa, M. S., Sherali, H. D., & Shetty, C. M. (2013). Nonlinear programming: Theory and
algorithms. Wiley.
28. Zhu, L., Shen, J., Xie, L., & Cheng, Z. (2017). Unsupervised visual hashing with semantic assis-
tant for content-based image retrieval. IEEE Transactions on Knowledge and Data Engineering,
29(2), 472–486.
29. Li, S. Z. (2012). Markov random ﬁeld modeling in computer vision. Springer Science & Business
Media.
30. Besag, J. (1986). On the statistical analysis of dirty pictures. Journal of the Royal Statistical
Society: Series B (Methodological), 48(3), 259–279.
31. Huiskes, M. J., & Lew, M. S. (2008). The MIR Flickr retrieval evaluation. In Proceedings of the
ACM SIGMM International Conference on Multimedia Information Retrieval (pp. 39–43).
32. Lin, T.-Y., Maire, M., Belongie, S. J., Hays, J., Perona, P., Ramanan, D., et al. (2014). Microsoft
COCO: Common objects in context. In Proceedings of the European Conference on Computer
Vision, 8693, 740–755.
33. Chua, T. S., Tang, J., Hong, R., Li, H., Luo, Z., & Zheng, Y. (2009). NUS-WIDE: A real-
world web image database from National University of Singapore. In Proceedings of the ACM
International Conference on Image and Video Retrieval, 48(1–48), 9.

144
4
Composite Multi-modal Hashing
34. Simonyan, K., & Zisserman, A. (2015). Very deep convolutional networks for large-scale image
recognition. In Proceedings of the International Conference on Learning Representations.
35. Liu, X., Li, Z., Deng, C., & Tao, D. (2017). distributed adaptive binary quantization for fast
nearest neighbor search. IEEE Transactions on Image Processing, 26(11), 5324 – 5336.
36. Long, M., Cao, Y., Wang, J., & Yu, P. S. (2016). Composite correlation quantization for efﬁcient
multimodal retrieval. In Proceedings of the International ACM SIGIR Conference on Research
and Development in Information Retrieval (pp. 579–588).
37. Lu, X., Zhu, L., Cheng, Z., Nie, L., & Zhang, H. (2019). Online multi-modal hashing with
dynamic query-adaption. In Proceedings of the International ACM SIGIR Conference on
Research and Development in Information Retrieval (pp. 715–724).
38. Zhang, P., Zhang, W., Li, W. J., & Guo, M. (2014). Supervised hashing with latent factor models.
In Proceedings of the International ACM SIGIR Conference on Research and Development in
Information Retrieval (pp. 173–182).
39. Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., Kaiser, L., &
Polosukhin, I. (2017). Attention is all you need. In Proceedings of the Advances in Neural
Information Processing Systems (pp. 5998–6008).
40. Zhou, X., Shen, F., Liu, L., Liu, W., Nie, L., Yang, Y., & Shen, H. T. (2020). Graph convolutional
network hashing. IEEE Transactions on Cybernetics, 50(4), 1460–1472.
41. Kingma, D. P., & Ba, J. (2015). Adam: A method for stochastic optimization. In Proceedings of
the International Conference on Learning Representations.

5
Multi-modal Discrete Collaborative Filtering
5.1
Background
With the development of E-commerce, recommender systems have been widely adopted
by many online services for helping their customers ﬁnd desirable products to purchase.
However, the ever-growing scales of products and users render recommendation more chal-
lenging than ever before [1]. For example, there are more than 0.82 billion active Taobao1
users and over one billion products for sale now. Consequently, it is challenging to make the
immediate response to match products for potential customers accurately and efﬁciently, by
analyzing large-scale yet sparse user interaction history.
As a critical class of recommendation methods, Collaborative Filtering (CF), as exem-
pliﬁed by Matrix Factorization (MF) algorithms have demonstrated great success in both
academia and industry. MF factorizes an n × m user-item rating matrix to project both users
and items into a r-dimensional latent feature space, where the user’s preference scores for
items are predicted by the inner product between their latent features. However, the time
complexity for generating top-k items recommendation for all users is O(nmr + nm log k)
[2]. Therefore, MF-based methods are often computationally expensive and inefﬁcient when
handling the large-scale recommendation applications [3, 4].
Hashing is an effective technique to address this challenge due to its desirable advan-
tages in terms of similarity computation and storage efﬁciency. It has been successfully
applied in various research ﬁelds, such as computer vision [5, 6], information retrieval
[7–11], and data mining [12, 13]. In recent years, researchers have paid more attention to
applying hashing technology to the recommender systems, and we refer to such methods
as hashing-based recommendations [14–17]. The core idea of hashing-based recommen-
dation is converting the original features of users and items into low-dimensional binary
1 https://www.taobao.com/.
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024
L. Zhu et al., Multi-modal Hash Learning, Synthesis Lectures on Information
Concepts, Retrieval, and Services, https://doi.org/10.1007/978-3-031-37291-9_5
145

146
5
Multi-modal Discrete Collaborative Filtering
hash codes, where the Hamming similarities between users and items can be computed
very efﬁciently using an Exclusive-Or operation to provide efﬁcient top-K recommenda-
tion. The hashing-based recommendation algorithm considering multi-modal information
is an effective way to implement efﬁcient and accurate recommendation. In this chapter,
we introduce two hashing-based multi-modal recommendation methods in the subsequent
sections: Multi-modal Discrete Collaborative Filtering (MDCF) and Explainable Discrete
Collaborative Filtering (EDCF).
5.2
Related Work
5.2.1
Recommendation with Multi-modal Auxiliary Information
Collaborative ﬁltering is one of the most widely used techniques in recommender systems.
However, in the cold-start scenario, since the collaborations between users or items are not
available, CF-based models become ineffective. To alleviate the cold-start problem, one of
the main strategies is to use auxiliary information such as demographic data, trust relations
or user reviews beside the collaborative ﬁltering method [18–20]. In general, a large amount
of descriptive information about items and users is available in real-world applications,
such as visual descriptions and textual descriptions of movie. Making full use of multi-
modal auxiliary information can improve our understanding of items and users [21, 22].
Due to the success of hybrid methods which incorporate the auxiliary information and the
collaborative ﬁltering, most current multi-modal recommendation algorithms are based on
thehybridmodels[23–26].Forinstance,[23]proposesadeepusers’multimodalpreferences-
based recommendation method to capture the textual and visual matching of users and items
for recommendation. Reference [24] learns the modal-speciﬁc representations of users and
items by utilizing information interchange between users and items in multiple modalities for
micro-video recommendation. Reference [26] adopts deep matrix factorization architecture
to learn the concept representation of multi-model data.
5.2.2
Hashing-Based Recommendation
Early studies of hashing-based recommendation frameworks focus on learning hash codes
by employing a two-step optimization strategy, which consists of a relaxed optimization
step and a binary quantization step. Under this strategy, the real-valued representations
of users and items are ﬁrst learned by the relaxed optimization, and then converted to
binary codes by the quantization. A pioneering work in this research area is [27], which
employs Locality-Sensitive Hashing (LSH) [28] and Jaccard measure method to learn binary
codes for efﬁciently searching Google News. Based on this, Karatzoglou [29] generates
binary codes from real-valued representations of users and items by employing random

5.2
Related Work
147
projections. Reference [30] follows the idea of Iterative Quantization (ITQ) [31], which is
widely used in the ﬁeld of hashing-based image retrieval, to rotate and binarize real-valued
latent representations of users and items. To improve recommendation performance, the
decorrelated constraint [32] and constant feature norm constraint [33] are imposed on real-
valued latent factors of users and items before quantization. These two-step optimization
strategiescanlearnbinaryhashcodesforusersanditems.Buttheycaneasilybringsigniﬁcant
quantization errors [34].
To alleviate quantization loss, Discrete Collaborative Filtering (DCF) [34] employs the
Discrete Coordinate Descent (DCD) [35] for the ﬁrst time to learn the hash codes of users
and items directly, rather than through the two-step optimization strategy. DCF adds bal-
anced and decorrelated constraints to matrix factorization formulation and learns hash codes
using only rating information. Inspired by DCF, Discrete Content-aware Matrix Factoriza-
tion (DCMF) [18] is the ﬁrst hashing-based recommendation method that considers side
information to improve recommendation performance and support cold-start recommenda-
tion scenarios. DCMF learns the hash codes directly by DCD method. The content features
in the side information are obtained by optimizing a multi-objective loss function and are
preserved in the hash codes. This strategy allows DCMF to learn the hash codes of users
or items in a cold-start setting. Similar to DCMF, Discrete Deep Learning (DDL) [36]
extracts content features of items from side information by employing Deep Belief Network
(DBN) [37], and generates hash codes by solving a relaxed optimization problem with an
alternating optimization strategy. Discrete Factorization Machines (DFM) [15] applies the
factorization machines model to exploit the pair-wise correlations between content features
and generate hash codes. Discrete Trust-aware Matrix Factorization (DTMF) [38] and Dis-
crete Social Recommendation (DSR) [39] learn hash codes by reconstructing the rating and
social relationship.
Recently, deep learning techniques demonstrate promising performance in hashing-based
recommendation tasks [16, 17, 40]. For instance, DGCN-BinCF [16] utilizes Graph Convo-
lutional Network (GCN) [41] to model high-order features of users and items, and distills the
ranking information derived from GCN into binarized collaborative ﬁltering to generate hash
codes.DuetothesuccessofGenerativeAdversarialNetworks(GAN)[42],[17]appliesGAN
to the hashing-based recommendation task, and proposes an Adversarial Binary Collabo-
rative Filtering framework (ABinCF), which is optimized by approximating sign function
and Bernoulli distribution. Reference [40] investigates the problem of unsupervised deep
hashing with graph neural network for recommendation, and proposes a framework called
HashGNN, which simultaneously learns deep hash functions and graph representations in
an end-to-end manner.

148
5
Multi-modal Discrete Collaborative Filtering
5.3
Multi-modal Discrete Collaborative Filtering
5.3.1
Motivation
Hashing is an effective technique to improve the efﬁciency of a large-scale recommender
system by representing both users and items into binary codes. However, existing hashing-
based recommendation methods still suffer from two important problems: (1) Cold-start.
They employ the user-item interactions and single auxiliary information to learn the binary
hash codes. But the full interaction history is not always available and the single auxiliary
information may be missing. (2) Efﬁcient optimization. They learn the hash codes with
two-step relaxed optimization or one-step discrete hash optimization based on the discrete
cyclic coordinate descent, which results in signiﬁcant quantization loss or still consumes
considerable computation time.
In this section, we propose a Multi-modal Discrete Collaborative Filtering (MDCF)
method for efﬁcient cold-start recommendation. In MDCF, it extract multi-modal features
from the cold-start objects, and simultaneously map them into the compact binary hash
codes by sufﬁciently exploiting their complementarity. More importantly, different from
existing cold-start recommendation solutions [14, 15, 18, 36], we propose a self-weighted
multi-modal binary mapping method to adaptively fuse the multi-modal features into hash
codes with automatically generated fusion weights. Besides, in real-world large-scale rec-
ommender systems, data sparsity is also a signiﬁcant challenge. To solve this problem, we
additionally impose low-rank constraint on multi-modal fusion module, which handles the
extremely sparse user-item interaction data and helps highlight the latent shared features
across different users and items. To support large-scale recommender systems, we develop
an efﬁcient discrete optimization approach based on augmented Lagrangian multiplier to
directly solve binary hash codes by simple and efﬁcient operations with alleviating the
quantization errors. Moreover, in the online recommendation stage, the proposed method
can efﬁciently fuse multi-modal features by using dynamic modality weights and adaptively
generate the hash codes for cold-start users and items.
5.3.2
Methodology
Notations and Problem Formulation
Assume that there are n users and m items, and the user-item rating matrix S is of size
n × m, where each entry si j ∈R indicates rating of a user i for an item j. Suppose that
Ox = oxi|n
i=1 is the user training dataset, which contains multi-modal auxiliary information
of n users represented with Mx different modality features (e.g. demographic information,
location and interaction preference extracted from tags and reviews), and Oy = oyi|m
i=1 is the
item training dataset, which contains multi-modal auxiliary information of m items repre-
sented with My different modality features (e.g. audiovisual materials of items, descrip-

5.3
Multi-modal Discrete Collaborative Filtering
149
-1
1 1 1
1
-1
-1
-1
-1 -1
1
1
1
1
1 1 1 -1 -1 -1 -1
Fast discrete
optimization
Multi-modal discrete
collaborative filtering
Offline Training
1 1 1
-1 -1 -1
1
Modal-adaptive
online hashing
Online recommendation
Fig. 5.1 The basic framework of the proposed MDCF. The framework consists of two main parts:
ofﬂine training and online recommendation. The main task of the ofﬂine training stage is to learn hash
functions and generate hash codes for users and items by fusing multi-modal auxiliary information
of them. In the online recommendation stage, when cold-start objects arrive, the binary hash codes
can be quickly generated by the proposed modality-adaptive hashing method with dynamic modality
weights and the learned hash functions in the ofﬂine training stage
tions, tags and reviews). The kth modality feature of users and items are denoted as
X(k) = [x(k)
1 , x(k)
2 , ..., x(k)
n ] ∈Rdk×n and Y(k) = [y(k)
1 , y(k)
2 , ..., y(k)
m ] ∈Rdk×m respectively,
where dk is the feature dimension of the kth modality. Our proposed method aims at learn-
ing hash codes B ∈{−1, 1}r×n for users and D ∈{−1, 1}r×m for items to represent their
latent factors in the ofﬂine training stage, where r is the hash code length. The hash codes
of the cold-start users and items that have no collaborative information or few collabora-
tive information are obtained by efﬁcient online hashing, and recommendation results are
quickly generated by calculating the Hamming distance between hash codes in the online
recommendation stage. The basic framework of the proposed MDCF is illustrated in Fig.5.1.
Throughout this section, we use bold lowercase letters to represent vectors and bold
uppercase letters to represent matrices. All of the vectors in this section denote column
vectors. Non-bold letters represent scalars. We denote tr(·) as the trace of a matrix and ∥· ∥F
as the Frobenius norm of a matrix. m
def
= {x ∈Rm|xi ≥0, 1⊤x = 1} is the probabilistic
simplex. sgn(·) : R →±1 is the sign function which returns −1 for x < 0 and 1 for x ≥0.
Main notations used in this section are listed in Table5.1.

150
5
Multi-modal Discrete Collaborative Filtering
Table 5.1 Main notations used in this section
Notation
Description
B
Binary hash code matrix of n users
D
Binary hash code matrix of m items
S
The user-item rating matrix
X(k)/Y(k)
Feature matrix of the kth modality data of users/items
φ(X(k))/φ(Y(k))
Nonlinear transformed representation of X(k)/Y(k)
Hx/H y
Multi-modal shared factor representation of users/items
W(k)
x /W(k)
y
Mapping matrix of the kth modality data of users/items
Rx, Ry
Rotation matrix
ZRx , ZRy
Auxiliary discrete variable
μ(k)
x /μ(k)
y
Weight of the kth modality data of users/items
n
The number of users
m
The number of items
p
The number of anchors
r
Hash code length
Low-rank Self-weighted Multi-modal Fusion
Given a training dataset O = oi|l
i=1, which contains l multi-modal auxiliary informa-
tion represented with M different modality features. The kth modality feature is X(k) =
[x(k)
1 , ..., x(k)
l
] ∈Rdk×l. We ﬁrst obtain the nonlinear transformed representation φ(x(k)
i ) as
[exp( ||x(k)
i
−a(k)
1 ||2
F
2σ2
k
), ..., exp( ||x(k)
i
−a(k)
p ||2
F
2σ2
k
)] where {a(k)
1 , ..., a(k)
p } are p anchors that are ran-
domly selected from the training samples in the mth modality, σk is the Gaussian kernel
parameter. φ(X(k)) = [φ(x(k)
1 ), ..., φ(x(k)
l
)] ∈Rp×l preserves the modality-speciﬁc sample
correlations by characterizing correlations between the sample and certain anchors. Since
the heterogeneous modality gap and inter-modality redundancy in multi-modal data are
detrimental to hashing learning. In this section, we aim at adaptively mapping the nonlinear
transformed representation φ(X(k))|M
k=1 into a consensus shared multi-modal representation
H ∈Rr×l (r is the hash code length) in a shared homogeneous space. Speciﬁcally, consid-
ering that the complementarity of multi-modal features and the generalization ability of the
fusion module are very important, we formulate this part as:
min
μ(k),W(k),H
M

k=1
μ(k)||H −W(k)φ(X(k))||2
F + ζ||μ||2
F,
s.t.μ = [μ(1), μ(2), ..., μ(M)]⊤, μ ∈M,
(5.1)

5.3
Multi-modal Discrete Collaborative Filtering
151
where W(k) ∈Rr×p, k = 1, ..., M is the mapping matrix of the kth modality feature, μ(k)
is the weight of the kth modality and it measures the importance of modality feature. By
setting weights, the complementarity of multi-modal features can be fully exploited. Similar
to previous multi-modal fusion method [8], we introduce the second term in Eq.(5.1) to
smooth the weight distribution. ζ is a hyper-parameter used to balance the fusion weights of
each modality in the multi-modal fusion process. If there is no ζ or if ζ tends to 0, the weight
of the optimal modality with the minimum reconstruction loss will be assigned to 1, while
the weights of the other modalities will be assigned to 0. On the other hand, If ζ approaches
inﬁnity, each modality will be assigned an equal weight. Under such circumstance, the
fusion approach will not be able to exploit the complementarity of multi-modal features.
Therefore, it is advisable to involve an additional parameter ζ in this parameter-weighted
hash learning, whose optimal value is conﬁrmed to be data related. In practice, introducing
additional parameter means that more time will be consumed on parameter adjustment in
the ofﬂine training process, and the parameter adjustment requirement is also contradictory
to the fact that we cannot manually assign an appropriate parameter value for each cold-start
user and item in the online recommendation process.
To address this problem, we introduce a virtual weight and propose a self-weighted multi-
modal mapping approach which can achieve the same goal as Eq.(5.1) without additional
hyper-parameter. The formula is
min
W(k),H
M

k=1
||H −W(k)φ(X(k))||F,
(5.2)
where || · ||F is the Frobenius norm of the matrix. We can derive the following theorem.
Theorem 51 Equation(5.2) is equivalent to
min
µ∈M,W(k),H
M

k=1
1
μ(k) ||H −W(k)φ(X(k))||2
F.
(5.3)
Proof Note that,
M

k=1
1
μ(k) ||H −W(k)φ(X(k))||2
F
(a)
=
 M

k=1
1
μ(k) ||H −W(k)φ(X(k))||2
F
  M

k=1
μ(k)

(b)
≥
 M

k=1
||H −W(k)φ(X(k))||F
2
,
(5.4)

152
5
Multi-modal Discrete Collaborative Filtering
where (a) holds since M
k=1 = μ(k) = 1 and (b) holds according to the Cauchy-Schwarz
inequality. This equation indicates
 M

k=1
||H −W(k)φ(X(k))||F
2
= min
µ∈M
M

k=1
1
μ(k) ||H −W(k)φ(X(k))||2
F,
(5.5)
It is easy to derive
min
W(k),H
M

k=1
||H −W(k)φ(X(k))||F
⇔min
W(k),H
 M

k=1
||H −W(k)φ(X(k))||F
2
⇔
min
µ∈M,W(k),H
M

k=1
1
μ(k) ||H −W(k)φ(X(k))||2
F,
(5.6)
which completes the proof.
□
As shown in Eq.(5.3), if the kth modality feature is discriminative, then the value of ||H −
W(k)φ(X(k))||F should be small and the corresponding
1
μ(k) should be large. Accordingly,
if the modality feature is indiscriminative, it should have a small
1
μ(k) . Therefore,
1
μ(k) can be
considered as a virtual weight of the kth modality feature, and it measures the importance
of this modality.
In this section, we focus on enabling the recommendation task for cold-start users
and items. Given the training nonlinear transformed representations φ(X(k))|Mx
k=1 and
φ(Y(k))|My
k=1. We aim at mapping them into corresponding consensus shared multi-modal
representations Hx and Hy by the form of Eq.(5.2), respectively. We formulate this part as
min
H LLLH = min
H
Mx

k=1
1
μ(k)
x
||Hx −W(k)
x φ(X(k))||2
F
+
My

k=1
1
μ(k)
y
||H y −W(k)
y φ(Y(k))||2
F,
(5.7)
where H denotes the variables to be learned.
In practical recommender systems, such as Taobao2 and Amazon,3 there are a huge
number of users and items, which have rich and diverse auxiliary information. However, a
speciﬁc user only has a small number of interactions with limited items, and the auxiliary
2 www.taobao.com.
3 www.amazon.com.

5.3
Multi-modal Discrete Collaborative Filtering
153
information is also complex and varied. Consequently, we need to map large mount of
heterogeneous and high-dimensional sparse feature into a homogeneous shared space. To
avoid spurious correlations caused by the mapping matrix, we impose a low-rank constraint
on W x and W y:
min
H LLLH + γ
⎛
⎝
Mx

k=1
rank(W(k)
x ) +
My

k=1
rank(W(k)
y )
⎞
⎠,
(5.8)
where γ is a balance parameter and rank(·) is the rank operator of a matrix. The low-rank
constraint on mapping matrix helps highlight the latent shared features across different users
or items and handles the extremely spare observations. Meanwhile, the low-rank constraint
makes the optimization more difﬁcult for the reason that low-rank optimization is a well-
known NP-hard problem. As an alternative method, the nuclear norm is well-known to be
a convex surrogate to the matrix rank, and is widely used to encourage low-rankness in
previous work [43, 44]. However, the nuclear norm optimizes the singular values of the
matrix, but the changes of the singular values are not always lead to a change of the rank.
To tackle this problem, we adopt an explicit form of low-rank constraint as follows:
min
M

k=1
rank(W(k)) ⇔min
M

k=1
lk

i=d+1
(σi(W(k)))2,
(5.9)
where σi(W(k)) denotes the ith singular value of W(k). lk is the total number of singular
values of W(k). Note that
lk

i=d+1
(σi(W(k)))2 = tr(V (k)⊤W(k)W(k)⊤V (k)),
(5.10)
where V (k) consists of singular vectors which correspond to the (lk −d)-smallest singular
values of W(k)W(k)⊤. Based on Eqs.(5.9) and (5.10), we can rewrite the low-rank constraint
on Wx and Wy as follows:
min
V (k)
x ,V (k)
y
LLLR =
min
V (k)
x ,V (k)
y
Mx

k=1
tr(V (k)
x
⊤W(k)
x W(k)
x
⊤V (k)
x )
+
My

k=1
tr(V (k)
y
⊤W(k)
y W(k)
y
⊤V (k)
y ).
(5.11)
The multi-modal fusion module can be rewritten as:
min
H ,V (k)
x ,V (k)
y
LLLH + γLLLR.
(5.12)

154
5
Multi-modal Discrete Collaborative Filtering
Multi-modal Discrete Collaborative Filtering
To obtain feature representations applicable to efﬁcient cold-start recommendation task,
we propose to preserve multi-modal shared factors into binary hash codes with matrix
factorization, which can support large-scale collaborative ﬁltering problems.
Given a rating matrix S of size n × m, where n and m are the number of users and items,
respectively. Let bi ∈{±1}r denote the binary hash codes for the ith user, and d j ∈{±1}r
denote the binary hash codes for the jth item, the rating of user i for item j is approximated
by Hamming similarity ( 1
2 + 1
2r b⊤
i d j). Thus, our goal is to learn binary hash code matrix
B = [b1, ..., bn] ∈{±1}r×n and D = [d1, ..., dm] ∈{±1}r×mfor users and items respec-
tively, where r ≪min(n, m) is the hash code length. Similar to the problem of conventional
collaborative ﬁltering, the basic discrete collaborative ﬁltering can be formulated as:
min
B,D ||S −B⊤D||2
F,
s.t.B ∈{±1}r×n, D ∈{±1}r×m.
(5.13)
To address the sparse and cold-start problem, we integrate auxiliary information of users
and items into the above model, by substituting B and D with the rotated multi-modal shared
factors Rx Hx and Ry Hy respectively (Rx, Ry ∈Rr×r are rotation matrices), and keeping
their consistency during the optimization process. The formula is given as follows:
min
Rx,Ry,B,D ||S −H⊤
x R⊤
x Ry H y||2
F
+ α1||B −Rx Hx||2
F + α2||D −Ry H y||2
F,
s.t. B ∈{±1}r×n, D ∈{±1}r×m, R⊤
x Rx = R⊤
y Ry = Ir.
(5.14)
This formulation has two advantages: (1) All of the decomposed variable are not directly
subjecttodiscreteconstraint.Asshownintheoptimizationpart,thehashcodescanbelearned
with a simple sgn(·) operation instead of bit-by-bit discrete optimization. The second and
third terms can guarantee the acceptable information loss. (2) The learned hash codes can
reﬂect the multi-modal features of users and items via Hx and H y respectively, and involve
the latent interactive features in S simultaneously.
Overall Objective Formulation
By integrating the above two parts into a uniﬁed learning framework, we derive the overall
objective formulation of Multi-modal Discrete Collaborative Filtering (MDCF) as:
min
 ||S −H⊤
x R⊤
x Ry H y||2
F + α1||B −Rx Hx||2
F
+ α2||D −Ry H y||2
F + βLLLH + γLLLR,
s.t. B ∈{±1}r×n, D ∈{±1}r×m, R⊤
x Rx = R⊤
y Ry = Ir,
(5.15)

5.3
Multi-modal Discrete Collaborative Filtering
155
where α1, α2, β, γ are balance parameters. The ﬁrst three terms minimize the information
loss during the process of preserving multi-modal features and interaction features into hash
codes.LLLH projectsmulti-modalfeaturesofusersanditemsintocorrespondinghomogeneous
shared space, respectively. LLLR is a low-rank constraint for mapping matrix, which can
highlight the latent shared features across different users or items.
Fast Discrete Optimization
Solving hash codes in Eq.(5.15) is essentially an NP-hard problem due to the discrete
constraint. Existing hashing-based recommendation methods always learn the hash codes
bit-by-bit with DCC [34]. Although this strategy alleviates the quantization loss problem
caused by conventional relaxing-rounding optimization strategy, it is still time-consuming.
With the favorable support of objective formulation, we propose to directly learn the dis-
crete hash codes with fast optimization. Speciﬁcally, different from existing hashing-based
recommendation methods [15, 18, 34, 36], we avoid explicitly computing the rating matrix
S, and achieve linear computation and storage efﬁciency. We propose an effective optimiza-
tion algorithm based on augmented Lagrangian multiplier (ALM) [45, 46]. In particular, we
introduce two auxiliary variables, ZRx and ZRy, to separate the constraint on Rx and Ry
respectively, and transform the objective function Eq.(5.15) to an equivalent one that can
be tackled more easily. Let
LZ = ||Rx −ZRx + G Rx
λ ||2
F + ||Ry −ZRy + G Ry
λ ||2
F,
(5.16)
where Z⊤
Rx ZRx = Z⊤
Ry ZRy = Ir. G Rx , G Ry ∈Rr×r measure the difference between the
targets and auxiliary variables. Then the Eq.(5.15) is transformed as:
min
 ||S −H⊤
x R⊤
x Ry H y||2
F + α1||B −Rx Hx||2
F
+ α2||D −Ry H y||2
F + βLLLH + γLLLR + λ
2 LZ,
s.t. B ∈{±1}r×n, D ∈{±1}r×m, R⊤
x Rx = R⊤
y Ry = Ir,
(5.17)
where  denotes the variables that need to be solved in the objective function, λ > 0 is a
balance parameter. With this transformation, we follow the alternative optimization process
by updating each of variables, given the others ﬁxed.
Step 1: learning μ(k)
x
and μ(k)
y . For convenience, we denote ||Hx −Wx(k)φ(X(k))||F as
h(k)
x . By ﬁxing the other variables, we ignore the term that is irrelevant to μ(k)
x . The original
problem can be rewritten as:
min
μ(k)
x ≥0,1⊤µx=1
Mx

k=1
h(k)
x
2
μ(k)
x
.
(5.18)

156
5
Multi-modal Discrete Collaborative Filtering
With Cauchy-Schwarz inequality, we derive that
Mx

k=1
h(k)
x
2
μ(k)
x
(a)
=
⎛
⎝
Mx

k=1
h(k)
x
2
μ(k)
x
⎞
⎠
 Mx

k=1
μ(k)
x

(b)
≥
 Mx

k=1
h(k)
x
2
,
where (a) holds since 1⊤µx = 1 and the equality in (b) holds when

μ(k)
x
∝
h(k)
x

μ(k)
x
. Since
(
Mx

k=1
h(k)
x )2 = const, we can obtain the optimal μ(k)
x
in Eq.(5.18) by
μ(k)
x
=
h(k)
x
Mx
k=1 h(k)
x
.
(5.19)
Similar to μ(k)
x , the optimal μ(k)
y
can be obtained by
μ(k)
y
=
h(k)
y
My
k=1 h(k)
y
.
(5.20)
Step 2: learning Wxk and Wyk. Since Wxk and Wyk are learned in a similar way, for
convenience, we ﬁrst introduce the learning method of mapping matrix Wxk. Removing the
terms that are irrelevant to the Wxk, the optimization formula is rewritten as
min
W(k)
x
Mx

k=1
β
μ(k)
x
||Hx −W(k)
x φ(X(k))||2
F
+ γ
Mx

k=1
tr(V (k)
x
⊤W(k)
x W(k)
x
⊤V (k)
x ).
(5.21)
We calculate the derivative of Eq.(5.21) with respect to Wx and set it to zero,
Mx

k=1
β
μ(k)
x
||Hx −W(k)
x φ(X(k))||2
F
+ γ
Mx

k=1
tr(V (k)
x
⊤W(k)
x W(k)
x
⊤V (k)
x ) = 0
⇒γV (k)
x V (k)
x
⊤W(k)
x + β
μ(k)
x
W(k)
x φ(X(k))φ(X(k))
⊤
=
β
μ(k)
x
Hxφ(X(k))
⊤.
(5.22)

5.3
Multi-modal Discrete Collaborative Filtering
157
By using the following substitutions,
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
Ax = γV (k)
x V (k)
x
⊤
Bx =
β
μ(k)
x φ(X(k))φ(X(k))⊤
Cx =
β
μ(k)
x Hxφ(X(k))⊤
,
(5.23)
Equation(5.22) can be rewritten as AxW x + W x Bx = Cx, which can be efﬁciently
solved by Sylvester operation in Matlab. Similar to W(k)
x , the Sylvester equation with respect
to W(k)
y
is AyW y + W y By = C y, where
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩
Ay = γV (k)
y V (k)
y
⊤
By =
β
μ(k)
y φ(Y(k))φ(Y(k))⊤
C y =
β
μ(k)
y H yφ(Y(k))⊤
.
(5.24)
Step 3: learning Rx and Ry. The optimization formula for updating Rx can be repre-
sented as
min
R⊤
x Rx=Ir
tr

R⊤
x Ry H y H⊤
y R⊤
y Rx Hx H⊤
x −2α1R⊤
x BH⊤
x
−2R⊤
x Ry H y S⊤H⊤
x −λR⊤
x

ZRx −G Rx
λ
 
.
(5.25)
It is challenging to solve Rx directly due to the orthogonal constraint. For the term
of R⊤
x Ry H y H⊤
y R⊤
y Rx Hx H⊤
x in Eq.(5.25), we use an auxiliary variable ZRx ∈Rr×r to
substitute the second Rx, and simultaneously keep the equivalence of them during the
optimization. With the constraint R⊤
x Rx = Ir, the above equation can be transformed as:
max
R⊤
x Rx=Ir
tr(R⊤
x Cx) ,
Cx = −Ry H y H⊤
y R⊤
y ZRx Hx H⊤
x + 2α1BH⊤
x
+ 2Ry H y S⊤H⊤
x + λZRx −G Rx .
(5.26)
With transformation, the optimal Rx is deﬁned as Rx = Px Q⊤
x , where Px and Qx are
comprised of left-singular and right-singular vectors of Cx, respectively [47]. The objective
function with respect to Ry can be represented as
min
R⊤
y Ry=Ir
tr

R⊤
y Rx Hx H⊤
x R⊤
x Ry H y H⊤
y −2α2 R⊤
y DH⊤
y
−2R⊤
y Rx Hx S⊤H⊤
y −λR⊤
y

ZRy −G Ry
λ
 
.
(5.27)

158
5
Multi-modal Discrete Collaborative Filtering
We introduce an auxiliary variable ZRy and substitute R⊤
y Rx Hx H⊤
x R⊤
x Ry H y H⊤
y with
R⊤
y Rx Hx H⊤
x R⊤
x ZRy H y H⊤
y . Thus, the Eq.(5.27) can be transformed into the following
form
max
R⊤
y Ry=Ir
tr(R⊤
y C y) ,
C y = −Rx Hx H⊤
x R⊤
x ZRy H y H⊤
y + 2α2 DH⊤
y
+ 2Rx Hx S⊤H⊤
y + λZRy −G Ry.
(5.28)
The optimal Ry is deﬁned as Ry = P y Q⊤
y , where P y and Qy are comprised of left-
singular and right-singular vectors of C y, respectively.
Note that, the user-item rating matrix S ∈Rn×m is included in the terms 2Ry H y S⊤H⊤
x
and 2Rx Hx S⊤H⊤
y when updating Rx and Ry, respectively. In real-world retail giants, such
as Taobao and Amazon, there are hundreds of millions of users and even more items. In
consequence, the user-item rating matrix S would be pretty enormous and sparse. If we use
S directly, the computational complexity will be O(mnr) and it is extremely expensive to
calculate and store S. We apply the singular value decomposition to obtain the left singular
and right singular vectors as well as the corresponding singular values of S. We utilize a
diagonal matrix S to store the o-largest (o ≪min{m, n}) singular values, and employ
an n × o matrix P S, an o × m matrix QS to store the corresponding left singular and
right singular vectors respectively. We substitute S with P SS QS and the computational
complexity can be reduced to O(max{mor, nor})(r, o ≪min{m, n}). With transformation,
both the computation and storage cost can be decreased with the guarantee of accuracy.
Step 4: learning Hx and H y. We calculate the derivative of objective function with
respect to Hx and H y and set them to zero, then we get
Hx =
 Mx

k=1
β
μ(k)
x
Ir + α1Ir + R⊤
x Ry H y H⊤
y R⊤
y Rx
−1
 Mx

k=1
β
μ(k)
x
W(k)
x φ(X(k)) + α1R⊤
x B + R⊤
x Ry H y S⊤

,
(5.29)
H y =
⎛
⎝
My

k=1
β
μ(k)
y
Ir + α2Ir + R⊤
y Rx Hx H⊤
x R⊤
x Ry
⎞
⎠
−1
⎛
⎝
My

k=1
β
μ(k)
y
W(k)
y φ(Y(k)) + α2 R⊤
y D + R⊤
y Rx Hx S
⎞
⎠,
(5.30)
where S is substituted with P SS QS, and the time complexity of computing R⊤
x Ry H y S⊤
and R⊤
y Rx Hx S are reduced to O(max{mor, nor}).
Step 5: learning B and D. We can obtain the closed solutions of B, D as

5.3
Multi-modal Discrete Collaborative Filtering
159
B = sgn(Rx Hx),
D = sgn(Ry H y).
(5.31)
Step 6: learning V (k)
x
and V (k)
y . As described in Sect.3.2, V (k)
x
and V (k)
y
are stacked by
the singular vectors which correspond to the (lk −d)-smallest singular values of W(k)
x W(k)
x
⊤
and W(k)
y W(k)
y
⊤, respectively. Thus we can solve the eigen-decomposition problem to get
V (k)
x , V (k)
y :
V (k)
x
←svd(W(k)
x W(k)
x
⊤), V (k)
y
←svd(W(k)
y W(k)
y
⊤).
(5.32)
Step 7: learning ZRx and ZRy. The objective function with respect to ZRx and ZRy can
be transformed as
max
Z⊤
Rx ZRx =Ir
tr(Z⊤
Rx Czrx),
Czrx = λRx −Ry H y H⊤
y R⊤
y Rx Hx H⊤
x ,
(5.33)
max
Z⊤
Ry ZRy =Ir
tr(Z⊤
Ry Czry),
Czry = λRy −Rx Hx H⊤
x R⊤
x Ry H y H⊤
y ,
(5.34)
where the optimal ZRx and ZRy can be solved with theorem 2.
Step 8: learning G Rx and G Ry. By ﬁxing other variables, the update rules of G Rx and
G Ry are
G Rx = G Rx + λ(Rx −ZRx ),
(5.35)
G Ry = G Ry + λ(Ry −ZRy).
(5.36)
Modality-Adaptive Online Hashing for Cold-Start Recommendation
In the online recommendation stage, we aim to map multi-modal features of the target
users and items into binary hash codes with the learned hash projection matrix {W(k)
x }Mx
k=1
and {W(k)
y }My
k=1, respectively. When cold-start users and items have no rating history in the
training set and are only associated with auxiliary information of certain modality, the ﬁxed
modality weights obtained from ofﬂine hash learning cannot address the modality-missing
problem, and may fail to effectively capture the variations of cold-start objects.
With the support of online hash learning, we propose to generate hash codes for cold-start
objects with a self-weighting scheme. The objective functions for cold-start users and items
are formulated as
min
ˆB,µx′∈Mx
Mx

k=1
1
μ(k)
x′
|| ˆB −W(k)
x φ( ˆX
(k))||2
F,
(5.37)
min
ˆD,µy′∈My
My

k=1
1
μ(k)
y′
|| ˆD −W(k)
y φ( ˆY
(k))||2
F,
(5.38)

160
5
Multi-modal Discrete Collaborative Filtering
where W(k)
x
and W(k)
y
are the mapping matrixes from Eq.(5.15). ˆX
(k) and ˆY
(k) are kth
modality features of target users and items, respectively. ˆB and ˆD are binary feature matrices
of target users and items, respectively. μ(k)
x′ and μ(k)
y′ are the dynamic modality weights to be
learned.
We employ alternating optimization to update ˆB, ˆD, μ(k)
x′ and μ(k)
y′ . The update rules are
μ(k)
x′ =
h(k)
x′
Mx
k=1 h(k)
x′
, h(k)
x′ = || ˆB −W(k)
x φ( ˆX
(k))||F,
μ(k)
y′ =
h(k)
y′
My
k=1 h(k)
y′
, h(k)
y′ = || ˆD −W(k)
y φ( ˆY
(k))||F,
ˆB = sgn
 Mx

k=1
1
μ(k)
x′
W(k)
x φ( ˆX
(k))

,
ˆD = sgn
⎛
⎝
My

k=1
1
μ(k)
y′
W(k)
y φ( ˆY
(k))
⎞
⎠.
(5.39)
Initialization
Since MDCF deals with the discrete matrix factorization problem, initialization is important
forfastconvergence.Weproposeaneffectiveinitializationstrategy,whichﬁrstsolvesrelaxed
problem of Eq.(5.15), and then quantizes the real-valued representation to obtain the initial
hash code for users and items. We also use alternate optimization to optimize the relaxed
problem of Eq.(5.15). The objective function for optimization is the same as Eq.(5.17)
except the binary constraints and the updating rules are the same except B and D because
the binary constraints are removed. With the favorable support of Eq.(5.15), we can obtain
real-valued feature matrices of users and items, B∗and D∗, by removing the sign function
in Eq.(5.31), respectively. Then, the optimization can be done alternatively. B and D are
initialized to feasible solutions sgn(B∗) and sgn(D∗) respectively, and the other variables
are initialized as solutions of the relaxed problem of Eq.(5.15).
Complexity Analysis
This section provides space and time complexity analysis of MDCF. Directly adopting the
n × m rating matrix S will bring the space complexity O(nm), which is unacceptable in
large-scale recommender systems. In our method, we substitute S with P SS QS where S
is a diagonal matrix consist of the o-largest singular values, P S and QS are the corresponding
left singular and right singular vectors respectively. Assuming that Mx = My = M. Since
the space complexity of storing P S and QS is O(o(m + n)) and that of storing modality
features of users and items is O(Mp(m + n)), the space complexity of ofﬂine hash code
learning stage can be reduced to O((o + Mp)(m + n)).
The overall time cost of constructing φ(X) and φ(Y) is O(Mp(m + n)). The time
cost total O(Mpr(m + n)) for updating μx and μy. The overall time cost of comput-

5.3
Multi-modal Discrete Collaborative Filtering
161
ing W x and W y is O(2Mr2(d −lk) + Mp(p + r)(m + n)). The overall time cost of
updating Rx and Ry is O((7r2 + 2ro)(m + n) + 6r3 + 2ro2). Updating Hx and H y
require O((5r2 + 2ro + Mpr)(m + n) + 2ro2 + 8r3) in total. Updating B and D require
O(r2(m + n)) in total. The time cost total O(4r2(m + n) + 4r3) for updating ZRx and ZRy.
The overall time cost of updating V x and V y is O(2r2(r + p)). Updating GRx and GRy
only require O(2r) in total. Suppose the entire algorithm requires iter iterations for con-
vergence, the overall time complexity of optimization process is O(iter × Mpr(m + n)),
where r is the hash code length and the convergence experiment results in Fig.5.6a indi-
cate that iter is usually 5 ∼10. In summary, training MDCF is efﬁcient since it scales
linearly with m + n. In the online stage, it takes O(Mpr ˆn) to generate binary hash code for
ˆn cold-start users/items.
Based on the above analysis, both space and time complexity of MDCF is linear with the
size of the dataset (m + n), which is scalable for large-scale recommender systems.
Convergence Analysis
The objective function Eq.(5.17) is convex to one variable by ﬁxing the others. Thus, opti-
mizing one variable in each step will cause the value of the objective function to decrease
or equal. Our iterative update rule will monotonically reduce the objective function value.
After several iterations, the optimization process will eventually reach a local minimum.
In addition, we will empirically verify the convergence of the proposed MDCF on three
datasets in experiments.
5.3.3
Experiment
Evaluation Datasets
We evaluate the proposed method on three widely used public datasets: MovieLens-1M4,
MovieLens-10M4 and BookCrossing.5 In these three datasets, each user has only one rating
for an item.
• MovieLens-1M: This dataset is collected from the MovieLens website by GroupLens
Research. It originally includes 1,000,000 ratings from 6,040 users for 3,952 movies.
The rating score is from 1 to 5 with 1 granularity. The users in this dataset are associated
with demographic information, and the movies are related to 3–5 genre labels.
• MovieLens-10M: This dataset contains 10,000,054 ratings and 95580 tags applied to
10,681 movies by 71,567 users of the MovieLens. Unlike MovieLens-1M dataset, demo-
graphic information is not included in this dataset and ratings are made on a 5-star scale,
with half-star increments.
4 https://grouplens.org/datasets/movielens/.
5 https://grouplens.org/datasets/book-crossing/.

162
5
Multi-modal Discrete Collaborative Filtering
Table 5.2 Statistics of experimental datasets
Dataset
#User
#Item
#Rating
Sparsity (%)
BookCrossing
2,151
6,830
180,595
98.77
MovieLens-1M
6,040
3,952
1,000,209
95.81
MovieLens-10M
71,567
10,681
10,000,054
98.69
• BookCrossing: This dataset is collected by Cai-Nicolas Ziegler from the Book-Crossing
community. It contains 278,858 users providing 1,149,780 ratings about 271,379 books.
The rating score is from 1 to 10 with 1 interval. Most users in this dataset are associated
with demographic information.
Considering the extreme sparsity of the original BookCrossing dataset, we remove the
users with less than 20 ratings and the items rated by less than 20 users. After the ﬁltering,
there are 2,151 users, 6,830 items, and 180,595 ratings left in the BookCrossing dataset. For
the MovieLens-1M and MovieLens-10M datasets, we keep all users and items without any
ﬁltering. The statistics of the datasets are summarized in Table5.2.
To obtain multi-modal information about items, we crawl the item-related information
from the web to extend the original datasets. For the BookCrossing dataset, we crawl descrip-
tions and reviews on Amazon.com based on the book IDs provided in the dataset. For the
MovieLens-1M and MovieLens-10M datasets, we crawl directors, writers, cast, storyline,
plot keywords, genres and reviews on IMDB.com based on the movie links provided in the
datasets. Since some of the users lack multi-modal auxiliary information, we use the user-
item interaction data and auxiliary information of users to generate the user’s multi-modal
preference features, which reﬂect the user’s preference for different modality features of
items, and apply them as the user’s multi-modal features for model learning.
We refer to all reviews written by a user as user document. Also, an item document can
be formed by merging all reviews written for the item. Then, LDA is used to extract the
auxiliary features from descriptions, storylines and reviews. The one-hot encoding approach
is adopted to generate feature representation for directors, writers, cast, plot keywords, and
genres.
In our experiments, we randomly select 20% users as cold-start users, while 20% items
as cold-start items, and randomly keep their θ ratings and other ratings are removed from
the training set and transferred to the testing set. We repeat the experiments with 5 random
splits and report the average values as the experimental results.
Evaluation Baselines and Evaluation Metrics
We compare our approach with three continuous value based recommendation methods and
three hashing-based recommendation methods.

5.3
Multi-modal Discrete Collaborative Filtering
163
• Zero-shot recommendation (ZSR) [48] considers cold-start recommendation problem
as a zero-shot learning problem [49]. It extracts real-valued representation of user prefer-
ence for each item from user attribute. The parameters λ and β for the relax and low-rank
constraints are tuned within {10−5, 10−3, 10−2, 10−1, 1, 5, 10, 100}.
• Collaborative topic regression (CTR) [19] is a hybrid recommendation algorithm that
combinestopicmodel,collaborativeﬁlteringandprobabilisticmatrixfactorization(PMF)
[50]. CTR generates real-valued latent representations of users and items by exploiting
user’s collection data and content data of items. The coefﬁcient for the balanced regular-
ization λu, λv and topic parameter β are tuned within {10, 100, 1000, 10000}.
• Collaborative deep learning (CDL) [20] is a probabilistic model that learns a proba-
bilistic SDAE [51] and CF jointly. CDL leverages an effective deep learning framework
to learn real-valued latent representation from interaction data and content data. The
parameter λu, λv, λn and λw are tuned within {10−4, 10−2, 1, 102, 104}, and the layer
structure of SDAE is set as [8000, 200, 50] according to the results of their experiments.
• Discrete Collaborative Filtering (DCF) [34] is the ﬁrst binarized CF method that can
directly optimize the binary codes for users and items.
• Discrete content-aware matrix factorization (DCMF) [18] is the state-of-the-art bina-
rized method for CF with auxiliary information. It is the extension of DMF based on
the regression-based modeling. The parameters λ1 and λ2 for modeling user and item
auxiliary features are tuned within {1, 10, 50, 100, 500, 1000}.
• Discretefactorizationmachines(DFM)[15]istheﬁrstbinarizedfactorizationmachines
method that learns the hash codes for any auxiliary feature and models the pair-wise inter-
action between feature codes. In DFM, the parameter β for the softened de-correlation
constraint is tuned within {10−4, 10−3, 10−2, 10−1, 1, 10, 100} according to the results
of their sensitive analysis.
• Discrete deep learning (DDL) [36] is a binary deep recommendation approach. It adopts
Deep Belief Network to extract item representation from item auxiliary information, and
combines the DBN with DCF to solve the cold-start recommendation problem. The
parameters α, β and λ are tuned within {10−4, 10−3, 10−2, 10−1, 1, 10, 100}. The layer
structure of DBN is set as [8000,800,30].
• Multi-feature discrete collaborative ﬁltering (MFDCF) [52] is the method we pro-
posed earlier.
In the experiments, we adopt 5-fold cross validation method on random split of training
data to tune the optimal hyper-parameters of all compared approaches. All the best hyper-
parameters are found by grid search.
The goal of our proposed method is to ﬁnd out the top-k items that the cold-start user
may be interested in, or top-k users who are most likely to interact with the cold-start item.
In our experiments, we adopt two common ranking evaluation methods: Accuracy@k and
Normalized Discounted Cumulative Gain (NDCG), to evaluate the quality of the recom-
mendation list. NDCG is a widely used measure for evaluating recommendation algorithms

164
5
Multi-modal Discrete Collaborative Filtering
Fig.5.2 Comparison of MDCF with baseline algorithms on the BookCrossing, MovieLens-1M and
MovieLens-10M datasets in both cold-start item recommendation (a) and cold-start user recommen-
dation scenarios (b)
[53,54],owingtoitscomprehensiveconsiderationofbothrankingprecisionsandtheposition
of ratings. Accuracy@k is widely used as a metric for previous ranking based recommender
systems [36, 55] to test whether the target user’s favorite items that appears in the top-k
recommendation list.
Comparison with the State-of-the-Art
In this subsection, we evaluate the recommendation performance of MDCF and the baselines
in cold-start recommendation scenario when the cold-start threshold θ is set as 1. Figure5.2
demonstrates the recommendation performance, including Accuracy@k and NDCG@k of
MDCF and competing baselines on three real-world datasets for the both cold-start item
(Fig.5.2a) and cold-start user (Fig.5.2b) recommendation tasks when the hash code length
r is set as 8.
Compared with existing hashing-based recommendation methods, the proposed MDCF
algorithm consistently and signiﬁcantly outperforms the state-of-the-arts with respect to
Accuracy@k and NDCG@k in the both cold-start recommendation scenarios. Since DCF
does not make use of content information, it does not support cold-start recommendation suf-
ﬁciently. DFM exploits the factorization machine to model the potential relevance between
user characteristics and item features. However, it ignores the collaborative interaction. DDL
is based on the discrete collaborative ﬁltering. It adopts DBN to generate item feature repre-
sentation from their auxiliary information. Nevertheless, the structure of DBN is independent
with the overall optimization process, which limits the learning capability of DDL. DCMF
integrates auxiliary information about users and items on the basis of matrix factorization.
However, it does not take into account the weight of multi-modal auxiliary information
and the online learning of cold-start objects, which reduce the cold-start recommendation
performance. Moreover, these experimental results show that the proposed MDCF outper-

5.3
Multi-modal Discrete Collaborative Filtering
165
forms the compared continuous value based hybrid recommendation methods under the
same cold-start settings. The better performance of MDCF than ZSR, CTR and CDL vali-
dates the effectiveness of the proposed multi-modal fusion strategy, and demonstrates that
the hash codes with discriminative feature representation capability can be learned by our
proposed online multi-modal hashing method. Additionally, it can be easily observed that
the proposed MDCF consistently outperforms MFDCF on three datasets, showing the effec-
tiveness of the initialization module and MDCF for modeling auxiliary information of users
and items.
Run Time Comparison
In these experiments, we compare the computation efﬁciency of our proposed MDCF with
three state-of-the-art hashing-based recommendation methods DMF, DCMF and DDL on
BookCrossing dataset. Similar results can be found on other datasets. The algorithms are
implemented via MATLAB. Table5.3 demonstrates the efﬁciency of these methods in both
initialization stage and training stage on BookCrossing dataset using a 2.0GHz Intel®
Core(TM) i7-4750HQ CPU.
From the Table5.3, we can see that MDCF achieves signiﬁcant speedups. Compared with
DFM, our proposed MDCF is about 73 to 139 times faster than DFM in the initialization
stage and about 34 to 77 times faster in the training stage. DDL needs to pre-train the
DBN once during the initialization stage, and updates the DBN in each round of iteration
during the training stage, which slows down the model training considerably. Compared
with DDL, MDCF is about 10 to 88 times faster than DDL in the initialization stage and
thousands of times faster in the training stage. Speciﬁcally, DCMF has a faster training speed
compared to other baselines, because it is a recommendation framework based on the matrix
factorization, just like our proposed MDCF. However, from the Table5.3, we can see that our
proposed MDCF is about 6 to 12 times faster than DCMF in the initialization stage and about
3 to 7 times faster in the training stage. This is because DCMF is also based on the DCC
optimization strategy, similar to DDL and DFM. Since DCC optimizes hash codes by the bit-
wise learning, the update rule is applied among bits iteratively until convergence. Denoting
the number of the bit-wise iteration as Tb and the number of entire algorithm iteration as
T . The overall time complexity for DCC-based algorithms is O(Tr2(Tb|V| + m + n)) [34]
where V is observed rating set. As discussed in previous section, the overall time complexity
of training MDCF is O(T Mpr(m + n)). Since M, p,r ≪min(m, n) in practice, the time
complexity of DCC-based algorithms is O(T Tbr2|V|) more than that of MDCF. It is shown
that our proposed fast optimization strategy is more efﬁcient than discrete cyclic coordinate
descent theoretically and experimentally.
Ablation Analysis
In this section, we propose a self-weighted multi-modal fusion strategy to preserve the
multi-modal auxiliary information into hash codes while exploring their complementarity,
and the hash codes of cold-start objects are adaptively learned in an online mode. The self-
weighted scheme addresses the modality-missing problem for cold-start objects and learns
adaptive modality weights to dynamically fuse the multi-modal features of cold-start objects.

166
5
Multi-modal Discrete Collaborative Filtering
Table 5.3 Efﬁciency comparison between MDCF and the other hashing-based recommendation methods where the code length ranges from 8 to 256
on the BookCrossing dataset
Methods
Initialization time/iteration (s)
Training time/iteration (s)
8 bits
16 bits
32 bits
64 bits
128 bits
256 bits
8 bits
16 bits
32 bits
64 bits
128 bits
256 bits
DDL
2.3398
3.0564
3.4605
3.6305
3.9811
7.7034
56.0339
80.8218
131.7272
349.6231
889.4721
6675.9737
DFM
2.4203
2.9404
4.0259
8.0024
23.0475
102.7623
2.0538
2.3503
2.7921
4.0821
9.1515
26.8376
DCMF
0.1876
0.2234
0.3486
0.8306
2.6959
6.6021
0.1734
0.1957
0.2338
0.4333
0.7743
3.1239
MDCF
0.0256
0.0332
0.0512
0.0995
0.2011
0.6981
0.0261
0.0331
0.0504
0.1038
0.2361
0.7436

5.3
Multi-modal Discrete Collaborative Filtering
167
Fig. 5.3 Accuracy@20 and NDCG@20 using MDCF with/without initializations on three datasets
(r = 8) in both cold-start item (a–b) and cold-start user (c–d) recommendation scenarios. We can
see that the proposed initialization strategy helps to achieve higher performance
Fig. 5.4 Effects of our proposed multi-modal self-weight learning strategy in both cold-start item
recommendation (a–b) and cold-start user recommendation scenarios (c–d)
Thus, in this subsection, we design ﬁve variants of our method to evaluate the effectiveness
of the proposed self-weighted scheme: (1) MDCF-ﬁxed: It adopts ﬁxed modality fusion
weights obtained from the ofﬂine learning to generate the hash codes of cold-start objects.
(2) MDCF-equal: It ﬁxes the weight of each modality to 1 at both the ofﬂine learning and
online hashing phases. (3) MDCF-1 and MDCF-2: They extract content features from only
the ﬁrst and the second modality of auxiliary information, respectively. (4) MDCF-miss: It
randomly removes 50% auxiliary information of users and items in the test set, then tests
the performance of MDCF in the modality-missing case. Figure5.3 shows the comparison
of the cold-start recommendation performance. From the ﬁgures, we can observe that the
performance of our method is obviously higher than that of the variants in both cold-start
recommendation scenarios. In addition, the use of more auxiliary information can be helpful
on improving recommendation performance, and the performance of MDCF is minimally
affected by the modality-missing problem. These results validate that the proposed self-
weighted strategy can indeed explore the complementarity and consistency of multi-modal
auxiliary information, adaptively fuse multiple modalities and improve the recommendation
accuracy.
Additionally, we design experiments to test the performance of the proposed initialization
module. In the later section, we discussed the efﬁciency of the initialization module. Here, we
focus on evaluating its effectiveness. The results are shown in Fig.5.4. It is easily observed
that the MDCF with initialization consistently outperforms MDCF without initialization on
the three datasets in both cold-start recommendation scenarios, showing the effectiveness
of initialization module in MDCF.

168
5
Multi-modal Discrete Collaborative Filtering
Fig. 5.5 Sensitive analysis of the MDCF on the three datasets in both cold-start item recommenda-
tion (a–d) and cold-start user recommendation scenarios (e–h), where Normalized Accuracy@20 is
obtained from dividing each Accuracy@20 by the maximum with respect to the parameter
Parameter Sensitivity Analysis and Convergency
We conduct experiments to observe the performance variations with the involved param-
eters α1, α2, β and γ. We ﬁx the hash code length as 8 bits and report results on
three datasets in both cold-start recommendation scenarios. Since α1, α2, β, and γ are
equipped in the same objective function, we change their values from the range of
{10−6, 10−3, 10−1, 100, 101, 103, 106} while ﬁxing other parameters. Detailed experimen-
tal results are presented in Fig.5.5. α1 is the balance parameter used to control the multi-
modal fusion of the user’s auxiliary information. From the Fig.5.5, we can observe that,
in the cold-start item recommendation scenario, α1 ∈[10−6, 10−3] can lead to better rec-
ommendation performance on both MovieLens datasets and α1 should be set to a larger
value on BookCrossing dataset. Speciﬁcally, a similar pattern is also found in the cold-start
user recommendation scenario for α2, which is the balance parameter used to control the
multi-modal fusion of items. This ﬁgure also shows that the performance is relatively better
when β is in the range of [10−3, 100] for the cold-start item recommendation, while in the
range of [10−1, 100] for the cold-start user recommendation. In addition, MDCF obtains
better performance when the value of γ is from 10−1 to 101 in both cold-start scenarios.
The performance variations with γ shows that the low-rank constraint can take effect on
preserving more discriminative binary codes.
MDCF is suitable for the cold-start recommendation where we want to compute predic-
tions for users or items that have no collaborative information (θ = 0) or few collaborative
information. The results of cold-start threshold analysis are shown in Fig.5.6b–c. With the
increase of cold-start threshold θ from 0 (no collaborative information) to 20, the recom-
mendation performance gradually improves in all of the three datasets and more rapidly in
the sparser datasets, indicating MDCF can work well in different cold-start scenarios.
To evaluate the convergency of the proposed method, we further perform experimental
analysis on three datasets with the hash code length ﬁxed as 8 bits. The convergence curves
recording the variations of objective function with the number of iterations are shown on

5.3
Multi-modal Discrete Collaborative Filtering
169
Fig.5.6 Variations of objective function value with the number of iterations on BookCrossing dataset
(a), Normalized NDCG@20 of MDCF in both cold-start user (b) and cold-start item (c) recommen-
dation scenarios given different cold-start threshold θ
the BookCrossing dataset in Fig.5.6a. As shown in the ﬁgure, the updating of variables
monotonically decreases the objective function value at each iteration. When the number
of iterations is less than 5, the objective function value drops sharply. Speciﬁcally, when
the initialization stage is over and entering the training stage, the binarization of B and
D matrices leads to a negligible rise in the objective function value, after which it will
rapidly converge. From the Fig.5.6a, we can observe that MDCF achieves a stable minimum
within 10 iterations on BookCrossing dataset. Similar convergence results were found on
the MovieLens-1M and MovieLens-10M datasets. These results indicate that our proposed
method can converge effectively.
Evaluating the Two-Stage Recommender Systems
Similar to [14], we design a two-stage recommender system, consisting of a hashing-based
recalling stage and a ﬁne-ranking stage, to help us better understand how MDCF can accel-
erate practical recommender systems. It is easy to note that our proposed MDCF can be
changed to a continuous value based recommendation algorithm with the design idea of
initialization module. Thus, in this evaluation, the ﬁrst stage is to exploit MDCF for recall-
ing the top-K potentially candidates, and the second stage is to use real-valued variant of
MDCF for ﬁne-ranking. Moreover, we use the real-valued variant of MDCF as a baseline
to evaluate the performance of the proposed two-stage recommender system.
The evaluation results on the three datasets in the cold-start user recommendation scenario
are shown in Fig.5.7. We can see that when the number of recalled items is small, the two-
stage recommender system will have a large performance loss, but the recommendation
can be accelerated by more than 20 times, and even up to around 38 in the MovieLens-
10M dataset with the largest number of items. Speciﬁcally, when the number of recalled
items increases, the performance loss will decrease rapidly, but the speedup ratio is also
decreasing. Similar results can be found in the cold-start item recommendation scenario.
Therefore, in a practical recommender system, we usually need to ﬁnd a balance between
efﬁciency and effectiveness. Additionally, from Fig.5.7a–c, we observe that the speedup
ratio is usually larger in the datasets with more items, because the recommendation method
based on continuous values takes more time to retrieve large item sets, and the efﬁciency
advantage of hash codes will be more obvious. From the description of previous section,

170
5
Multi-modal Discrete Collaborative Filtering
Fig.5.7 Results of evaluating the two-stage model on the three datasets (a–c) and speedup ratio with
respect to hash code length (d)
we know that the hash code length is an important factor that inﬂuences the efﬁciency of
MDCF. Therefore, we conduct experiments to observe the efﬁciency variations with respect
to the hash code length for the two-stage recommender system on three datasets in cold-start
user recommendation scenario. The evaluation results are shown in Fig.5.7d. We can easily
observe that as the length of the hash code increases, the speedup ratio gets larger and larger.
This is because increasing the length of the continuous value features makes the similarity
computation time-consuming, while the efﬁciency of calculating Hamming distance is less
affected by the hash code length.
5.4
Explainable Discrete Collaborative Filtering
5.4.1
Motivation
Using hashing to learn the binary codes of users and items signiﬁcantly improves the efﬁ-
ciency and reduces the space consumption of the recommender system. However, existing
hashing-based recommender systems remain black boxes without any explainable outputs
that illustrate why the system recommends the items. In this section, we present a new
end-to-end discrete recommendation framework based on the multi-task learning to simul-
taneously perform explainable and efﬁcient recommendation. Toward this goal, an Explain-
able Discrete Collaborative Filtering (EDCF) method is proposed to preserve the user-item
interaction features and semantic text features into binary hash codes by adaptively exploit-
ing the correlations between the preference prediction task and the explanation generation
task. At the online recommendation stage, EDCF makes efﬁcient top-K recommendation
by calculating the Hamming distances between the feature hash codes, and simultaneously
generates natural language explanations for recommendation results through the explanation
generation module. To obtain the hash codes directly from the end-to-end neural network,
we introduce an attentive TextCNN and an Adaptive Tanh layer in the preference prediction
task. For explanation generation, Long Short-Term Memory is employed to generate the
explanations for recommendation results from the binary hash codes of user and item.

5.4
Explainable Discrete Collaborative Filtering
171
5.4.2
Methodology
Overview
Due to the success of multi-task learning technique in academia and industry, it is widely
applied to generation-based explainable recommendation tasks. Reference [56] proposes
a Hierarchical Sequence-to-Sequence (HSS) model based on multi-task learning for per-
sonalized recommendation and natural language explanation generation, which adopts an
auto-denoising mechanism that selects sentences containing item features for model train-
ing. Reference [57] proposes a deep framework named NRT based on multi-task learning
which can simultaneously predict ratings and leverage gated recurrent neural networks to
generate abstractive tips for the recommendation results. Reference [58] proposes a multi-
task recommendation model, which integrates a sequence-to-sequence learning model with
matrix factorization and jointly performs rating prediction and recommendation explana-
tion. Reference [59] proposes an encoder-selector-decoder architecture for explainable rec-
ommendation inspired by human’s information-processing model in cognitive psychology.
The authors exploit the correlations between the recommendation task and the explanation
task through co-attentive multi-task learning. These approaches utilize a multi-task learning
strategy to generate natural language explanations of the recommendation results while pre-
dicting the ratings. However, in the large-scale recommendation scenario, these approaches
still consume considerable computation time to generate top-K recommendations from the
enormous number of candidate items.
Different from existing explainable recommendation algorithms, the proposed EDCF
method has the following advantages. First, EDCF can generate binary representations of
users and items to support efﬁcient top-K recommendation, while translating the hash codes
into personalized natural language explanations. Second, EDCF utilizes attentive TextCNN
and ATanh layer to achieve end-to-end binary optimization for multiple tasks. Finally, EDCF
can automatically adjust the multi-task weights during model training according to the task
difﬁculty and model state.
The goal of our model is to learn the hash codes of users and items, as well as to train an
explanation generation module that can translate the hash codes into natural language expla-
nation. To this end, we introduce an Explainable Discrete Collaborative Filtering (EDCF)
model for efﬁcient explainable recommendation. The architecture of the proposed model is
shown in Fig.5.8. The model is based on a multi-task learning strategy and contains two
major components: neural discrete collaborative ﬁltering on the left and natural language
explanation generation on the right, which correspond to preference prediction task and
explanation generation task respectively.
For neural discrete collaborative ﬁltering, the input consists of the user set U, the item
set V, the ratings and the reviews. Each user is represented as its ID u ∈U and each
item is denoted as the item ID v ∈V. Ratings are normalized through dividing by their
maximum value. The reviews posted by the user u are represented as a document Cu =
(C1
u, C2
u, ..., Cmd
u ), where md denotes the maximum number of reviews. If the number of

172
5
Multi-modal Discrete Collaborative Filtering
User interaction
      features
User review 
   features
…
Adaptive Tanh layer
Item interaction 
       features
Item review 
   features
User’s hash code
Item’s hash code
…
Rating
1
-1
One-hot encoding
Nice
guitar
Nice
cable
guitar
.
cable
<eos>
.
Review:
Item attentive
   TextCNN
User attentive 
   TextCNN
Item 
reviews
User
reviews
Rating matrix
   Context
information
SVD
User MLP
Item MLP
Task1: Preference prediction
Task2: Explanation generation
LSTM
Adaptive multi-task learning
Fig.5.8 The basic learning framework of the proposed EDCF
reviews posted by the user is larger than md, a random selection strategy is applied to
form Cu. Each review Ci
u is denoted by a set of words in the review. Similarly, document
Cv = (C1
v, C2
v, ..., Cmd
v ) denotes the set of reviews received by the item v.
Given user u’s review document Cu, the user attentive TextCNN module embeds the
words,reviewsanduserID,andgeneratesreviewfeaturerepresentationofuseru.Inaddition,
we obtain user u’s interaction features from the rating matrix using the Singular Value
Decomposition (SVD) method. And then, a Multi-Layer Perceptron (MLP) is employed
to project the concatenation of interaction features and review features of user u to a real-
valued representation via several layers of non-linear transformations. Speciﬁcally, we adopt
an ATanh layer to transform the user’s real-valued representation into a binary hash code.
The same process is applied for item modeling network with similar layers. Then, we utilize
Hamming similarity for preference prediction. The Hamming similarity between user ui
and item v j is calculated as:
Hsim(ui, v j) = 1
2 + 1
2r bT
i d j,
(5.40)
where r is the hash code length, bi ∈{−1, 1}r×1 and d j ∈{−1, 1}r×1 are the hash codes of
user ui and item v j, respectively. For natural language explanation generation, a sequence
decoding model based on LSTM is proposed to translate the hash codes of ui and v j into a
sequence of words W
W
Wi j = (w1, w2, ..., wme), which illustrates why user ui likes or dislikes
item v j. wk denotes the kth word in the explanation, and me represents the maximum
number of words in the generated natural language explanation. Moreover, Hsim(ui, v j) is
used as a part of the input for the decoding model to control the sentiment of the generated
explanations. At the online recommendation stage, there is a signiﬁcant efﬁciency difference
between the neural network computing and the hash code matching. To address this issue, we
ﬁrst obtain the top-K recommendation results from the massive candidate items by matching
the hash codes of the target user and candidate items rapidly, and then the hash codes of

5.4
Explainable Discrete Collaborative Filtering
173
Table 5.4 Main notations used in this section
Notation
Description
Cu
The user u’s review document
W
W
Wi j
The explanation of recommending item v j to user ui
R
The user-item rating matrix
P
The interaction feature matrix of users
Q
The interaction feature matrix of items
ˆSu
The learned review feature of user u
B
The binary hash code matrix of n users
D
The binary hash code matrix of m items
VVV
The vocabulary of words in the reviews and explanations
n
The number of users
m
The number of items
α
The learnable scaling parameter in ATanh layer
ϕ1, ϕ2
The uncertainty in the two tasks
the target user and top-K items are fed into the explanation generation network to generate
natural language explanation for the recommendation results. All the neural parameters and
the hash codes of users and items are learnt by a multi-task learning approach. The model can
betrainedefﬁcientlybyanend-to-endlearningparadigmusingback-propagationalgorithms.
Throughout this section, we use bold lowercase letters to represent vectors and bold
uppercase letters to represent matrices. All of the vectors in this section denote column
vectors. Non-bold letters represent scalars. Main notations used in this section are listed in
Table5.4.
Neural Discrete Collaborative Filtering
As shown in Fig.5.8, the neural discrete collaborative ﬁltering component consists of two
parallel neural networks: user modeling network and item modeling network. In the fol-
lowing, we focus on illustrating the process for user modeling network in detail. The same
process is applicable to item modeling network with similar layers.
At the ﬁrst stage of user modeling network, given a user review document Cu, an attentive
TextCNN network is applied to process Cu. Figure5.9 gives the architecture of the attentive
TextCNN. In the ﬁrst layer, a word embedding function is used to map each word in the
user review document Cu to a d-dimensional word embedding vector, and then the given
Cu is transformed to a ﬁxed-size embedding tensor V u ∈Rmd×mw×d, where mw denotes
the maximum number of words in each review (padded with zero in Cu when the review
length is less than mw). Following the embedding layer is the convolution layer, where
N f ﬁlters wi ∈Rh×d, i ∈{1, 2, ..., N f } with a window of h words are applied to extract
features by convolution operations on the word vectors. Let V k
u denote the embedding matrix

174
5
Multi-modal Discrete Collaborative Filtering
Words
Reviews
Word embedding
Number
of filters
Number
of filters
Word embedding tensor
representation of reviews
Convolutional layer with multiple 
filter widths and feature maps
Max-pooling and concatenating
User
reviews
  Embedding layer
Review representation
ID embedding
Attention layer
Flatten
User review features
Fig.5.9 The attentive TextCNN architecture
corresponding to the kth review of the user u. Then, the features generated by the ith ﬁlter
wi can be expressed as:
zik
u = f (wi ⊗V k
u + µi),
(5.41)
where µi is a bias term, f (·) denotes a non-linear activation function such as ReLU [60],
and ⊗represents the convolution operation. Then, we apply a max-pooling operation [61]
over the features zik
u and take the maximum value ˆzik
u = max{zik
u } as the extracted feature
of the kth review corresponding to the ﬁlter wi. The idea of max-pooling is to capture the
most important feature-one with the highest value. We concatenate the features extracted by
these N f ﬁlters, denoted as
sk
u = [ ˆz1k
u , ˆz2k
u , ...,
ˆ
z
N f k
u
]T .
(5.42)
In addition, the proposed model employs Ng sets of ﬁlters with different window sizes
to extract multiple review features. The representation of the kth review of the given user
is the concatenation of multiple review features obtained by Ng sets of ﬁlters with varying
window sizes, denoted by:
Sk
u = [sk,1
u ; sk,2
u ; ...; sk,Ng
u
].
(5.43)
The ﬁnal output of the convolutional layer is
Su = [S1
u, S2
u, ..., Smd
u ].
(5.44)
Existing works tend to improve the model performance by exploiting latent features in
the user’s textual reviews [57, 62]. However, in practice, not all reviews are of high quality
and have high availability. Currently, most studies do not consider the difference in the
contribution of each review to user and item modeling, which is not robust in real life as
each review has a different ability to represent user interests and item features. To address
this issue, we introduce an attention mechanism in our model, which can help to learn the
weight of each review in user and item modeling.
We employ a two-layer network to compute attention score ak
u. The input to the attention
layer in the user modeling network contains two components: a feature matrix composed
of md review features of the given user and embedding representations of the item IDs ck
u.
Intuitively, the ID embedding is added to identify the items that are not distinctive and have

5.4
Explainable Discrete Collaborative Filtering
175
low reference value. For the item modeling network, user ID embedding is applied to the
network to identify users who frequently post spam reviews. We formulate this part as
ˆak
u = W T
h f (Ws Sk
u + Wuck
u + µ1) + μ2.
(5.45)
where Wh ∈Rh, Ws ∈Rh×(N f ×Ng), Wu ∈RId×h, µ1 ∈Rh, μ2 ∈R1 are model parame-
ters, Id is the dimension of ID embedding, h represents the hidden layer size of the attention
network. The ﬁnal attention score of the reviews can be obtained by using the softmax func-
tion to normalize the above scores, which can be interpreted as the contribution of the ith
review to model the preference of user u:
ak
u =
exp(ˆak
u)
md
k=1 exp(ˆaku).
(5.46)
After that, the review feature of user u can be represented as a weighted concatenation
as follows:
ˆSu = [a1
u S1
u; a2
u S2
u; ...; amd
u Smd
u ].
(5.47)
Speciﬁcally, we consider both user review features and interaction features in the model.
Given a rating matrix R of size n × m, where n and m are the numbers of users and items,
respectively. The user interaction features are obtained by SVD method:
R = P QT ,
(5.48)
where the eigenvalue matrix is left blended into the feature vectors, P ∈Rn×l denotes the
interaction feature matrix of users, where the ith row pi is the interaction feature of the
user ui, Q ∈Rm×l represents the interaction feature matrix of items, where the jth row q j
denotes the interaction feature of the item v j, l denotes the interaction feature length.
At the ofﬂine stage, we learn and store the hash codes of users and items. At the online
recommendation stage, we make efﬁcient top-K recommendations by matching the hash
codes quickly. To this end, instead of using a neural latent factor model to predict ratings as
in [63], we employ a multi-layer perceptron network to map the user’s interaction feature
and review feature into a shared latent space to obtain a real-valued representation of the
user:
p1
i = [ pi; ˆSui ],
p2
i = W T
2 p1
i + b2,
......
pL
i = W T
L p(L−1)
i
+ bL.
(5.49)

176
5
Multi-modal Discrete Collaborative Filtering
In this section, we aim to generate the binary representations B, D ∈{−1, +1} for the
users and items, respectively. Since binary constraints are difﬁcult to optimize in networks,
we employ an Adaptive Tanh (ATanh) layer in our model, which is formulated as:
f (x) = tanh(αx), α > 0,
(5.50)
where x is the activation of previous layers, α is a learnable scaling parameter. In practice,
the learned α should be large enough to make the activations of ATanh fall into the binary
value {−1, +1}. Since the ATanh is differentiable everywhere, it can be jointly trained with
other layers via back-propagation, and directly generate the binary codes.6
Then, the preference of user ui for item v j can be evaluated by Eq.(5.40). The loss
function of the preference prediction module can be formulated as:
LLLP P =

(ui,v j)∈Y+∪Y−

ri j
max(R) log Hsim(ui, v j)
+

1 −
ri j
max(R)

log(1 −Hsim(ui, v j))

,
(5.51)
where ri j indicates the rating of user ui for item v j, Y+ denotes the observed interactions,
Y−means the set of negative samples, which are randomly selected from the user-item
pairs without observations in a certain proportion, Y+ ∪Y−means all training interactions,
max(R) represents the max score in all ratings, e.g., 5 in a 5-star rating system.
Natural Language Explanation Generation
At the online recommendation stage, the input is only the target user ID, and the binary hash
code of the target user can be obtained from matrix B. Then the top-K recommendation is
fast generated by computing the Hamming distance between hash codes of the target user
and the candidate items. In this section, we aim to generate corresponding explanations while
making top-K recommendations. Since the input does not contain any textual information,
it is a challenging task to generate natural language explanations only based on the hash
codes of the target user and the item.
Based on the above issues and considering the outstanding performance of Long Short-
Term Memory (LSTM) model in text generation-related tasks, we adopt LSTM as the basic
model fortheexplanationgenerationmodule.The right part ofFig.5.8showsourexplanation
generation module, whose main idea is expressed as follows:
p(wt
i j|w1
i j, w2
i j, ..., wt−1
i j ,CCCi j) = fh(ht),
(5.52)
where wt
i j is the tth word of the explanation W
W
Wi j, CCCi j denotes the context information about
user ui and item v j, which will be described in the following section, fh is a non-linear
6 Although ATanh is very close to the sign function, there are still very few activations that are not -1
or +1. For these activations, we binarize them directly.

5.4
Explainable Discrete Collaborative Filtering
177
mapping function that decodes ht into word wt
i j, ht is the hidden state at the time t and it
depends on the input xt at the time t and the previous hidden state ht−1:
ht = LST M(ht−1, xt).
(5.53)
Sequence hidden state is updated based on the following operations:
f t = σ(W f ⊗[ht−1, xt] + b f ),
it = σ(Wi ⊗[ht−1, xt] + bi),
Ct = tanh(Wc ⊗[ht−1, xt] + bc),
Ct = f t ∗Ct−1 + it ∗Ct,
ot = σ(Wo ⊗[ht−1, xt] + bo),
ht = ot ∗tanh(Ct),
(5.54)
where xt is the embedding vector for the word et of the explanation and the vector is also
obtained during the end-to-end learning process of our framework, f t denotes the forget
gate and it is the input gate, ⊗represents element-wise multiplication and σ(·) means the
softmax function.
As shown in Fig.5.8, when t = 1, the LSTM model has no word vector as an input.
Therefore, we adopt the context information CCCi j as the input at t = 1. CCCi j will guide the
explanation generation module to generate natural language explanation for the correspond-
ing user-item pair, which will directly affect the performance of the recommender systems.
In our framework, the context information CCCi j consists of predicted preference value ˆri j,
and corresponding binary hash codes bi and d j of user ui and item v j, respectively.
For the hash codes bi and d j, we can directly ﬁnd them from the matrices B and D,
respectively. To control the sentiment tendency of the generated explanation, we add the
predicted preference value ˆri j to CCCi j. Similar to NARRE [63], ˆri j will be vectorized into its
one-hot form. For example, ˆri j = 0.34, in the 5-star rating system, we scale it to the range
[0, 5] and round it, then we get the vector ˆri j = (0, 0, 0, 1, 0, 0)T .
After obtaining the context information Ci j = [bi, d j, ˆri j], we transform it into the initial
input xi j
1 at t = 1 by a non-linear mapping:
xi j
1 = tanh(WCCCCi j + bC).
(5.55)
where WC and bC are parameters to be learned.
Then, LSTM can perform sequence decoding and generate the hidden state sequence.
The hidden state hi j
t will be mapped into a |V|-size vector ˆei j
t through the ﬁnal generation
layer, where V is the vocabulary of words in the reviews and explanations:
ˆei j
t = σ(Wehi j
t + be),
(5.56)

178
5
Multi-modal Discrete Collaborative Filtering
where We ∈R|VVV|×dh and be ∈R|VVV|, dh is dimensionality of the hidden state space, σ(·)
denotes the softmax function. Then the word with the highest probability in step t is selected
as the tth word of the explanation:
ˆwi j
t = arg max ˆei j
t .
(5.57)
At the training stage, we employ Negative Log Loss (NLL) as the loss function for the
explanation generation module:
LLLEG = −

(ui,v j)∈Y+

w∈W
W
Wi j
log ˆe(Iw)
i j
,
(5.58)
where Iw is the vocabulary index of the word w.
Adaptive Multi-task Learning
Traditionalmulti-tasklearningtendstosimplyconductaweightedlinearsumofthelossesfor
each individual task, which usually requires manual adjustment of their weights. However,
the learning performance of the model is very sensitive to the weights, and it is difﬁcult to
manually adjust them to obtain a better model for multiple tasks simultaneously. In addition,
manually adjusting these weights is a very time-consuming task in practice.
To address this problem, we introduce task-dependent uncertainty [64] for adaptive multi-
task learning, and the overall objective function of the proposed model is as follows:
L = 1
ϕ2
1
LP P + 1
ϕ2
2
LEG + λ1 log ϕ1ϕ2 + λ2||α−1||2
2 + λ3||||2
2,
(5.59)
where ϕ1 and ϕ2 denote the uncertainty in the two tasks, respectively, ||α−1||2
2 is a penalty
term, which is a convenient way to make the α in Eq.(5.50) increase during the training
process,  denotes the set of neural parameters, λ1, λ2 and λ3 are regularization parameters.
The whole framework can be effectively trained end-to-end using back-propagation.
Intuitively, the larger the ϕ1(ϕ2), the greater the uncertainty of the task and the smaller
the weight of the corresponding task. That is, during the training process, the model will
preferentially learn tasks that are less noisy and easier to learn. However, it does not mean
that the noisier and harder tasks are not emphasized. The effect of introducing uncertainty
on model learning will be explored in detail in our experiments.
5.4.3
Experiment
Evaluation Datasets
We conduct the experiments on four well-known datasets. They have been widely tested
in recent hashing-based recommendation methods [14, 15, 18, 34, 36, 65]. The ratings in
these datasets are all based on the 5-star rating system. Speciﬁcally, three datasets are from

5.4
Explainable Discrete Collaborative Filtering
179
Table 5.5 Statistics of experimental datasets
Dataset
#User
#Item
#Rating
Sparsity (%)
|V|
Kindle store
68,223
61,934
982,619
99.98
44,656
Movies & TV
123,960
50,052
1,697,533
99.97
69,434
Electronics
192,403
63,001
1,689,188
99.99
50,023
Yelp
365,665
159,108
5,766,970
99.99
83,553
Amazon 5-core7: Kindle Store, Movies&TV and Electronics. These datasets contain user
ratings and reviews of various products, and all users and products correspond to at least 5
reviews. Another dataset is Yelp,8 which contains user ratings and reviews for locations such
as restaurants, hotels, and shopping centers. Considering the sparse user reviews in the Yelp
dataset, we remove the users with less than 5 reviews. After the ﬁltering, there are 365,665
users, 159,108 items, as well as 5,766,970 ratings and reviews left in the Yelp dataset. For
all these datasets, we ﬁlter out the stop words and low-frequency words from the reviews,
and then build a vocabulary V for each dataset. The statistics of the datasets are summarized
in Table5.5.
Evaluation Baselines
To evaluate the performance of preference prediction, we compare our method with the
following state-of-the-art baselines:
• Discrete Collaborative Filtering (DCF) [34] is the ﬁrst binarized collaborative ﬁltering
method that can directly optimize the hash codes for users and items, which outperforms
almost all two-stage hash code learning methods for collaborative ﬁltering.
• Discrete Content-aware Matrix Factorization (DCMF) [18] is the state-of-the-art
binarized method for CF with auxiliary information. It improves the discriminative
ability of hash codes by encoding context information on the basis of DCF. The
parameters λ1 and λ2 for modeling user and item auxiliary features are tuned within
{1, 10, 50, 100, 500, 1000}.
• Discrete Factorization Machines (DFM) [15] is the ﬁrst binarized factorization
machines method to resolve the rating prediction problem. It binarizes the real-valued
model parameters of each feature embedding into binary codes. In DFM, the parameter
β for the softened de-correlation constraint is tuned within {10−4, 10−3, 10−2, 10−1, 1,
10, 100} according to the results of their sensitive analysis.
• Discrete Deep Learning (DDL) [36] is a binary deep recommendation approach. It uses
a Deep Belief Network (DBN) to generate item hash codes from auxiliary information,
and combines the DBN with DCF to solve the cold-start recommendation problem. The
7 http://jmcauley.ucsd.edu/data/amazon.
8 https://www.yelp.com/dataset/.

180
5
Multi-modal Discrete Collaborative Filtering
parameters α, β and λ are tuned within {10−4, 10−3, 10−2, 10−1, 1, 10, 100}. The layer
structure of DBN is set as [8000, 800, 30].
• Neural Hashing-based Collaborative Filtering (NeuHash-CF) [65] is a deep hashing-
based recommendation approach, which utilizes two joint autoencoder architectures to
generate user and item hash codes from content information, respectively. This method
uses only the user’s ID to generate the user hash code and the auxiliary information of the
item to generate the item hash code. This strategy makes it have better performance in
both cold-start and general settings. We tune the parameters of this method as described
in the corresponding work.
• Deep Matrix Factorization (DeepMF) [66] is a matrix factorization model with a neural
network structure. It uses a deep architecture to learn low-dimensional space represen-
tations of users and items from their explicit ratings and implicit feedback. We set the
learning rate to 0.0001 and randomly initialize model parameters according to the original
description.
To evaluate the performance of explanation generation, we compare EDCF with two
explainable recommendation methods based on textual sentence explanation and a reﬁned
retrieval-based explanation generation method:
• NARRE [63] is a neural attentional regression model with review-level explanations
for recommendation. It calculates the usefulness of user reviews while predicting rat-
ings. NARRE selects useful reviews that provide detailed information about an item and
valid purchase suggestions as an explanation of the recommendation results. The model
parameters are tuned as described in the original text.
• NRT [57] is a generation-based explanation method. NRT uses a deep architecture to
predict user ratings and utilizes a gated recurrent neural networks to generate a short
text (tips) as an explanation of the recommendation. We set the dimension of latent
factor as 300 and hidden size as 400. The parameters λr, λc, λs and λn are tuned within
{10−4, 10−2, 1}.
• LexRank [67] is a classical method in the ﬁeld of text summarization. It uses a graph-
based approach to compute the correlation between sentences, and selects core sentences
to generate summarization. Similar to [57], we reﬁne LexRank to make it capable of
extracting sentences for explanation generation. For a detailed description of the reﬁne-
ments, please refer to [57].
Evaluation Metrics
The goal of our proposed framework is to ﬁnd out the top-K items that are most interesting to
the target user, while generating natural language explanations of recommendation results.
To evaluate the performance of preference prediction, in our experiments, we adopt two
common ranking evaluation methods: Accuracy@K and Normalized Discounted Cumula-
tiveGain(NDCG),toevaluatethequalityoftherecommendationlist.Accuracy@Kiswidely

5.4
Explainable Discrete Collaborative Filtering
181
used as a metric for previous ranking based recommender systems [36, 55] to test whether
the target user’s favorite items appear in the top-K recommendation list. NDCG is a widely
used measure for evaluating recommendation algorithms [53, 54], which incorporates both
ranking precisions and the position of ratings.
For the evaluation of explanation generation, we take the review written by the user for
the item as the ground truth and use ROUGE [68] and BLEU [69] as our evaluation metrics.
They are widely used to test the performance of explainable recommendation algorithms
[57, 59]. In our experiments, we use Recall, Precision and F-measure metrics of ROUGE-
1, ROUGE-2 and ROUGE-L, as well as BLEU to evaluate the quality of the generated
explanation.
Experimental Settings
To evaluate the performance of top-K recommendation, we adopt the widely used leave-
one-out evaluation strategy [70, 71]. For each user, we select their latest interaction data
to construct the test set, and the remaining interaction data are used for training. At the
testing stage, we follow the common strategy [71, 72] that randomly selects 99 items that
have not interacted with the target user as interference items for each ground-truth item,
and rank these 100 items (99 interference items and the ground-truth item) to test the top-K
recommendation performance of our method.
When training our model, inspired by [66], we randomly select two negative instances
for each positive instance to form negative instance set Y−. Speciﬁcally, we set the ratings
of negative user-item pairs to zero. However, since negative instances have no review infor-
mation, they are only used to train the preference prediction module and are not involved
in the training of the explanation generation module. The Word2Vec9 model pre-trained
on Google News is used to initialize the word embedding matrix, and the words without
pre-trained are randomly initialized and updated during the training process.
For neural network, we randomly initialize model parameters with a Gaussian distribution
and select Adam [73] as training optimizer. All the best hyper-parameters are found by grid
search. We implement EDCF based on TensorFlow, and the code is published on Github.10
Preference Prediciton
In this subsection, we evaluate the preference prediction performance of our proposed EDCF
method through a ranking task. Figure5.10 demonstrates the ranking performance, including
Accuracy@K and NDCG@K of EDCF and competing baselines on four real-world datasets
when the hash code length is set as 128.
In our experiments, we select the prevailing hashing-based recommendation method as
well as prevailing continuous value based recommendation method as baselines. Speciﬁ-
cally, to evaluate the preference prediction performance of EDCF on the single-task case, we
design a variant of the EDCF method called EDCF_SR. It removes the explanation genera-
tion module from EDCF and keeps the attentive TextCNN and neural discrete collaborative
9 https://code.google.com/archive/p/word2vec/.
10 https://github.com/zzmylq/EDCF.

182
5
Multi-modal Discrete Collaborative Filtering
Fig.5.10 Comparison of EDCF with baseline algorithms on Kindle Store, Movies and TV, Electron-
ics and YELP
ﬁltering modules. From Fig.5.10, we can observe that the proposed EDCF framework con-
sistentlyandsigniﬁcantlyoutperformsallcomparativemethodswithrespecttoAccuracy@K
and NDCG@K on all datasets.
DCF is a classical hashing-based recommendation algorithm that adopts a bit-by-bit
optimization strategy to learn hash codes. However, it only uses interaction information for
rating prediction, which limits its recommendation performance. DCMF extends DCF and
considers the content information of users and items. However, it fails to fully exploit the
non-linear correlation between content features. DFM adopts the factorization machine to
model the potential relevance between features, but ignores the collaborative interaction.
DDL applies DBN to extract item features, which is not trained in an end-to-end deep
network architecture. In other words, DDL extends DCF by adding DBN, which cannot
fully exploit the feature learning capability of deep networks.
Moreover, these experimental results show that the proposed EDCF outperforms both
the hashing-based deep recommendation method NeuHash-CF and the continuous value
based deep recommendation method DeepMF. The better performance of EDCF_SR than
NeuHash-CF and DeepMF validates the effectiveness of neural discrete collaborative ﬁl-
tering module, and demonstrates that the discriminative hash codes can be learned by our
proposed deep hashing architecture. Additionally, from the experimental results, we notice
that the proposed EDCF outperforms EDCF_SR, which demonstrates the effectiveness of
the proposed multi-task learning strategy. A detailed analysis of EDCF_SR will be provided
in later section.

5.4
Explainable Discrete Collaborative Filtering
183
Explanation Generation
In the online recommendation stage, the top-k recommendation task is executed very fast
because the Hamming similarity can be computed very efﬁciently using an Exclusive-Or
operation. Differently, the explanation generation task relies on the LSTM model, and the
generation process of natural language explanation requires computation operations in con-
tinuous numerical space, so there is a signiﬁcant efﬁciency difference between the two
tasks. To address this issue, we ﬁrst generate top-K recommendations by efﬁcient hash code
matching, and then the LSTM-based explanation generation module is used to generate the
corresponding K explanations. In addition, the base model of the explanation generation
module can be replaced according to speciﬁc application scenarios.
To evaluate the explanation generation performance of EDCF in the single-task case, we
designed a variant of the EDCF method called EDCF_SE. It removes the neural discrete
collaborative ﬁltering module from EDCF, and uses the features of target user and item
candidates extracted by the attentive TextCNN module as the input to the LSTM to predict
the target users’ reviews of the item candidates.
The evaluation results of explanation generation of EDCF and the comparative methods
are given in Tables5.6, 5.7, 5.8 and 5.9. In order to show more details, we also report Recall,
Precision, and F1-measure of ROUGE-1, ROUGE-2 and ROUGE-L. From the experimental
results, we can see that our proposed EDCF achieves signiﬁcant improvement in all metrics
compared to competitive methods on all four datasets.
Table 5.6 Performance of explanation generation on Kindle Store
Methods
ROUGE-1
ROUGE-2
ROUGE-L
BLEU
R
P
F1
R
P
F1
R
P
F1
LexRank
0.17969
0.18116
0.17904
0.02636
0.02759
0.02637
0.13934
0.14072
0.13877
0.25085
NARRE
0.28953
0.15674
0.19324
0.02769
0.01246
0.01607
0.18544
0.09936
0.12269
0.22911
NRT
0.23923
0.15213
0.17317
0.03572
0.01716
0.02111
0.20891
0.12989
0.14879
0.15575
EDCF_SE 0.30162
0.41416
0.34645
0.05887
0.06047
0.05948
0.25454
0.34982
0.29243
0.31524
EDCF
(ours)
0.32911
0.43030
0.37061
0.07388
0.07526
0.07440
0.28307
0.37014
0.31873
0.33833
Table 5.7 Performance of explanation generation on Movies and TV
Methods
ROUGE-1
ROUGE-2
ROUGE-L
BLEU
R
P
F1
R
P
F1
R
P
F1
LexRank
0.16431
0.16416
0.16298
0.02213
0.02294
0.02209
0.12820
0.12814
0.12703
0.24137
NARRE
0.22172
0.13928
0.16310
0.01090
0.00588
0.00720
0.14685
0.09072
0.10655
0.22944
NRT
0.21127
0.13716
0.15433
0.03247
0.01503
0.01915
0.18428
0.11646
0.13219
0.14708
EDCF_SE 0.27832
0.39083
0.32242
0.04590
0.04726
0.04642
0.23322
0.32798
0.27025
0.29155
EDCF
(ours)
0.29605
0.39821
0.33701
0.05367
0.05484
0.05407
0.25091
0.33780
0.28567
0.30834

184
5
Multi-modal Discrete Collaborative Filtering
Table 5.8 Performance of explanation generation on electronics
Methods
ROUGE-1
ROUGE-2
ROUGE-L
BLEU
R
P
F1
R
P
F1
R
P
F1
LexRank
0.17127
0.17313
0.17083
0.01713
0.01832
0.01718
0.13202
0.13372
0.13162
0.23948
NARRE
0.24037
0.15655
0.18331
0.01949
0.01059
0.01322
0.15395
0.09860
0.11604
0.24274
NRT
0.21521
0.14617
0.16119
0.03423
0.01617
0.01994
0.19198
0.12858
0.14246
0.13812
EDCF_SE 0.28375
0.37252
0.31982
0.04504
0.04573
0.04527
0.23839
0.31327
0.26875
0.28923
EDCF
(ours)
0.29773
0.38154
0.33232
0.05064
0.05127
0.05084
0.25135
0.32236
0.28060
0.30410
Table 5.9 Performance of explanation generation on Yelp
Methods
ROUGE-1
ROUGE-2
ROUGE-L
BLEU
R
P
F1
R
P
F1
R
P
F1
LexRank
0.14798
0.14987
0.14801
0.01500
0.01585
0.01504
0.11515
0.11687
0.11518
0.22774
NARRE
0.31342
0.10629
0.15315
0.02617
0.00657
0.01006
0.19497
0.06550
0.09449
0.15703
NRT
0.19902
0.13559
0.15169
0.02473
0.01217
0.01515
0.17101
0.11436
0.12867
0.15719
EDCF_SE 0.27616
0.38478
0.31946
0.04682
0.04822
0.04744
0.23567
0.32898
0.27281
0.29208
EDCF
(ours)
0.32308
0.41699
0.35989
0.05742
0.05209
0.05398
0.25274
0.32509
0.28099
0.29846
From the results, we notice that explanation generation performance of EDCF is much
better than EDCF_SE, which illustrates the effectiveness of the multi-task learning strat-
egy. The better performance of EDCF_SE than LexRank, NARRE and NRT validates the
effectiveness of the natural language explanation generation module. A detailed analysis
of EDCF_SE will be provided in later section. Moreover, the experimental results show
that the proposed EDCF outperforms NRT, which is also a generation-based explainable
recommendation algorithm. The reason is that the goal of NRT is to generate tips as natu-
ral language explanation of the recommendation results. In the original work, the authors
use the summary ﬁeld in the Amazon dataset as tips for training and use reviews as auxil-
iary information to participate in model training. However, summaries and reviews are not
always available. Since most recommender systems only have review information available,
the performance of NRT will be degraded in this case.
Case Analysis
We provide some real cases in Table5.10 to analyze the linguistic quality and the sentiment
correlation between the predicted ratings and the generated explanations.
Beneﬁting from the information sharing in multi-task learning, EDCF can take into
account user interests and item features when generating natural language explanations.
For example, from the analyses of case 1 and case 2, we can observe that the generated
explanations accurately predict the users’ preferences for the author Debora Geary, and
predict that the book belongs to a series. Speciﬁcally, the generated explanations not only
align with the ground truth ratings in terms of overall sentiment but also provide accurate

5.4
Explainable Discrete Collaborative Filtering
185
Table 5.10 Examples of the predicted ratings and the generated explanations
Case
Ground truth
Prediction
Rating
Review
Rating
Review
1
5
I started reading Debora’s books
from the very beginning
of the series, and they keep
getting better and better. I am
disappointed to see this line of
the series end but cannot
wait for the new one to start
4.878
I love reading Geary’s story the
very beginning,
see how author and book are
better and better.
I wait for the next story
2
5
I love this series by Debora. I
tripped into it then realized
it was a series. I have read them
out of order and still love
them. I hope she continues to
write them because the
characters are really developing
4.805
I love this series and the Geary.
I have read all
of the order, and the story are so
good
3
1
This was a good storyline but
the grammar and phrasing
were terrible. I would have
enjoyed this read a lot more if
I did not keep tripping over the
horriﬁc misspellings and
completely wrong words
0.821
It was a good story. But the
spelling errors, I not
like the book
4
1
The story was ok, but the writer
was terrible. There were
so many grammatical errors and
spelling errors.
I wouldn’t buy another book by
this author
1.289
The story line is good, but the
author did many
errors. I haven’t recommend this
book
explanations for the recommendation results at a ﬁne-grained aspect level. For example,
the generated explanations of case 3 and case 4 are shown that although users rate the book
positively in terms of storyline, they end up with a one-star rating due to signiﬁcant problems
with the author’s writing, which is consistent with the semantics expressed by the ground
truth reviews.
For linguistic quality, the syntax of some generated explanations is problematic and
some redundant stop words may be generated. In addition, the words used in the generated
explanations may be inaccurate, but these words are semantically similar to the ground-truth.
In general, although the language quality still needs to be improved, EDCF can generate
ﬁne-grained semantic-informative reviews to illustrate why the system recommends the
items from multiple perspectives.

186
5
Multi-modal Discrete Collaborative Filtering
Ablation Analysis
In this section, we propose a multi-task deep hashing framework for explainable recom-
mendation that extracts user and item features from reviews via an attentive TextCNN, and
adaptively adjusts the weights of multiple tasks during training by modeling their uncer-
tainty. In this subsection, we design ﬁve variants of our method to evaluate the feature
extraction performance of the attentive TextCNN and the efﬁctiveness of adaptive weight
learning strategy: (1) EDCF_WA: It replaces the review feature extraction module with a
simple TextCNN network. (2) EDCF_EMW: It ﬁxes the weight of each task to 1 during
the training process. (3) EDCF_FMW: We perform a full training of EDCF in advance and
obtain the weights w f inal
1
and w f inal
2
of the two tasks that converge at the end of the training.
EDCF_FMW ﬁxes the weights of the two tasks as w f inal
1
and w f inal
2
respectively during
the training process. (4) EDCF_SR and EDCF_SE: Two variants of our method described
in previous sections.
Table5.11 shows the comparison of the top-K recommendation and explanation genera-
tion performance. It can be easily observed that the performance of our method is obviously
higher than that of the variants in both top-K recommendation and explanation generation
tasks.
The performance of EDCF is higher than EDCF_WA on both tasks, demonstrating the
effectiveness of the attention module on extracting text features. In addition, the experi-
mental results of EDCF_SR and EDCF_SE illustrate that multiple tasks can be mutually
reinforced in the adaptive multi-task learning process. Additionally, the performance degra-
dation of EDCF_EMW and EDCF_FMW also validates the effectiveness of the adaptive
weight learning strategy. Speciﬁcally, the detailed experiments and analyses of the adaptive
weight learning strategy are provided in the following subsection.
Effects of Adaptive Multi-task Learning
At the beginning of the training, we initialize both ϕ1 and ϕ2 to 1. The uncertainty-based
adaptive weight learning strategy will tend to learn the easier tasks ﬁrst. From Fig.5.11,
we can see that the value of ϕ1 is consistently larger than ϕ2, which means that explana-
tion generation is the easier task during training, while the preference prediction task is
more difﬁcult. Therefore, we can see that the BLEU metric, which measures the perfor-
mance of explanation generation, achieves a high-level preferentially after a few epochs of
training. However, the HR metric, which measures the performance of preference predic-
tion, improves slowly during this period. Then, the ϕ1 starts to decrease, i.e., the weight of
the preference prediction task starts to increase, and the HR metric also improves rapidly.
Finally, the performance evaluation metrics of both tasks tend to converge. That is, during
the training process, the model gives priority to the easier explanation generation task, and
then focuses on the more difﬁcult preference prediction task, until both tasks are better
solved.

5.4
Explainable Discrete Collaborative Filtering
187
Table 5.11 Ablation experimental results on Kindle Store and Movies and TV
Data
Methods
A@20
N@20
ROUGE-1
ROUGE-2
ROUGE-L
BLEU
R
P
F
R
P
F
R
P
F
KS
EDCF
0.4311
0.1922
0.3291
0.4303
0.3706
0.0738
0.0752
0.0744
0.2830
0.3701
0.3187
0.3383
EDCF_WA
0.4155
0.1751
0.3265
0.4276
0.3678
0.0724
0.0738
0.0729
0.2804
0.3672
0.3158
0.3360
EDCF_EMW
0.4051
0.1691
0.3255
0.4275
0.3671
0.0719
0.0732
0.0723
0.2794
0.3670
0.3150
0.3357
EDCF_FMW
0.4295
0.1907
0.3278
0.4296
0.3694
0.0734
0.0748
0.0739
0.2822
0.3700
0.3181
0.3373
EDCF_SR
0.4284
0.1894
–
–
–
–
–
–
–
–
–
–
EDCF_SE
–
–
0.3016
0.4141
0.3464
0.0588
0.0604
0.0594
0.2545
0.3498
0.2924
0.3152
M&T
EDCF
0.5997
0.3014
0.2960
0.3982
0.3370
0.0536
0.0548
0.0540
0.2509
0.3378
0.2856
0.3083
EDCF_WA
0.5552
0.2725
0.2925
0.3953
0.3336
0.0518
0.0530
0.0522
0.2473
0.3346
0.2821
0.3045
EDCF_EMW
0.5278
0.2500
0.2909
0.3960
0.3328
0.0514
0.0526
0.0518
0.2461
0.3354
0.2817
0.3031
EDCF_FMW
0.5901
0.2974
0.2944
0.3975
0.3357
0.0529
0.0542
0.0534
0.2496
0.3373
0.2847
0.3071
EDCF_SR
0.5784
0.2892
–
–
–
–
–
–
–
–
–
–
EDCF_SE
–
–
0.2789
0.4002
0.3262
0.0472
0.0487
0.0479
0.2330
0.3348
0.2726
0.2905

188
5
Multi-modal Discrete Collaborative Filtering
Fig.5.11 Effects of the multi-task learning strategy on Kindle Store (a–b) and Electronics (c–d)
Hashing Performance Analysis
We conduct experiments to analyze the performance of hash learning. Table5.12 shows
the performance of EDCF on preference prediction and explanation generation tasks with
different hash code lengths. From Table5.12 we can observe that the performance metrics
of EDCF are improving as the hash code length increases. However, when the hash code
length is larger than 128 bits, the doubling of the hash code length has a limited effect on
the performance improvement. Therefore, the storage space and performance requirements
of the model need to be balanced in practical applications.
Next, we analyze the improvement of recommendation efﬁciency by using binary hash
codes. Figure5.12c shows the speedup of the binary hash codes in feature matching for
different hash code lengths, which is compared with the continuous-valued feature repre-
sentation method of equal length. We can see that the improvement of matching speed by
the binary hash codes method becomes more and more signiﬁcant as the length of the hash
codes increases.
Additionally, we add ATanh layer to the neural network in order to enable end-to-end
deep hash learning. The most important parameter in ATanh is α, which determines whether
the learned feature representations are binary or not. Therefore, we design experiments to
observe the effect of α on hash learning and model performance. From Figs.5.12a, b we
can see that when α ≥103, the learned feature representations are almost binary, and when
α ≥104, the model can output stable binary feature representations. We also note that the
performance degradation is about 2.5–12.5 using the binary feature representation (α ≥104)
compared to the continuous-valued feature representation (α = 1). However, considering
the signiﬁcant efﬁciency gain by using binary feature representation shown in Fig.5.12c,
this approach has important practical value, especially in two-stage recommender systems,
which consists of a hashing-based recalling stage and a ﬁne-ranking stage.
Finally, we investigate the effect of the number of negative instances in the training set
on hash learning. Figure5.13 shows the effect of the ratio of negative instances to positive
instances on the performance of the model. We can observe that introducing a moderate
amount of negative instances is advantageous, but a large number of negative instances
damage the performance of the model due to the interference of huge noises. Therefore, in
our experiments, we adopt the strategy of assigning two negative instances for each positive
instance to introduce a moderate amount of negative instances.

5.4
Explainable Discrete Collaborative Filtering
189
Table 5.12 Performance w.r.t. different hash code length
Dataset
HL
A@20
N@20
ROUGE-1
ROUGE-2
ROUGE-L
BLEU
R
P
F
R
P
F
R
P
F
KS
32
0.3213
0.1144
0.3110
0.4201
0.3548
0.0645
0.0659
0.0651
0.2646
0.3575
0.3018
0.3210
64
0.3831
0.1541
0.3249
0.4268
0.3664
0.0716
0.0730
0.0721
0.2789
0.3663
0.3145
0.3347
128
0.4311
0.1922
0.3283
0.4296
0.3698
0.0737
0.0751
0.0742
0.2826
0.3698
0.3182
0.3384
256
0.4366
0.1942
0.3291
0.4303
0.3706
0.0738
0.0752
0.0744
0.2830
0.3701
0.3187
0.3383
M&T
32
0.4631
0.2050
0.2905
0.3962
0.3326
0.0512
0.0525
0.0517
0.2454
0.3349
0.2809
0.3030
64
0.5336
0.2545
0.2909
0.3955
0.3326
0.0516
0.0528
0.0520
0.2462
0.3351
0.2816
0.3034
128
0.5997
0.3014
0.2960
0.3982
0.3370
0.0536
0.0548
0.0540
0.2509
0.3378
0.2856
0.3083
256
0.6054
0.3031
0.2970
0.3987
0.3379
0.0541
0.0552
0.0545
0.2519
0.3384
0.2866
0.3086

190
5
Multi-modal Discrete Collaborative Filtering
Fig.5.12 Effects of ATanh with different α, and the speedup ratio w.r.t hash code length
Fig.5.13 Effects of selection ratio of negative samples on Kindle Store and Movies and TV
5.5
Summary
In this chapter, we ﬁrst state the research background and related works of multi-modal dis-
crete collaborative ﬁltering, and then propose two hash-based multi-modal recommendation
methods. Speciﬁcally, we propose a Multi-modal Discrete Collaborative Filtering (MDCF)
method that projects multi-modal auxiliary information of users and items into the binary
hash codes to support efﬁcient cold-start recommendation. The proposed method can handle
the data sparsity problem with low-rank constraint, enhance the discriminative capability
of hash codes with self-weighted multi-modal binary fusing, generate hash codes for the
cold-start objects online by modality-adaptive hashing, and achieve computation and storage
efﬁcient with discrete binary optimization. The experimental results show that MDCF has
signiﬁcant performance improvement compared with baselines.
Also, we propose an Explainable Discrete Collaborative Filtering (EDCF) method that
projects user-item interaction and review information into hash codes to support efﬁcient
top-K recommendation while generating the natural language explanation of the recommen-
dation results. The proposed method can generate binary hash codes directly during end-to-
end deep network training, automatically adjust multi-task weights through an uncertainty-
based adaptive weight strategy, and generate natural language explanations using binary
hash codes as input. Moreover, our method only uses users’ publicly available ratings and
reviews on the web as input, thus effectively protecting users’ privacy. Extensive experiments
show that EDCF outperforms the state-of-the-art baselines on both preference prediction
and explanation generation tasks.

References
191
References
1. Batmaz, Z., Yurekli, A., Bilge, A., & Kaleli, C. (2019). A review on deep learning for recom-
mender systems: Challenges and remedies. Artiﬁcial Intelligence Review, 52(1), 1–37.
2. Zhang, Y., Lian, D., & Yang, G. (2017). Discrete personalized ranking for fast collaborative
ﬁltering from implicit feedback. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence
(pp. 1669–1675).
3. Cheng, Z., Ding, Y., He, X., Zhu, L., Song, X., & Kankanhalli, M. S. (2018). A3NCF: An adaptive
aspect attention model for rating prediction. In Proceedings of the International Joint Conference
on Artiﬁcial Intelligence (pp. 3748–3754).
4. Cheng, Z., Chang, X., Zhu, L., Kanjirathinkal, R. C., & Kankanhalli, M. 2019. MMALFM:
Explainable recommendation by leveraging reviews and images. ACM Transactions on Infor-
mation Systems, 37(2), 1–28.
5. Li, S., Li, X., Lu, J., & Zhou, J. (2021). Self-supervised video hashing via bidirectional trans-
formers. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
(pp. 13549–13558).
6. Wang, X., Zhang, Z., Wu, B., Shen, F., & Lu, G. (2021). Prototype-supervised adversarial network
for targeted attack of deep hashing. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (pp. 16357–16366).
7. Lei Zhu, X. L., Cheng, Z., Li, J., & Zhang, H. (2020). Deep collaborative multi-view hashing
for large-scale image search. IEEE Transactions on Image Processing, 29(2020), 4643–4655.
8. Lu, X., Zhu, L., Cheng, Z., Nie, L., & Zhang, H. (2019). Online multi-modal hashing with
dynamic query-adaption. In Proceedings of the International ACM SIGIR Conference on
Research and Development in Information Retrieval (pp. 715–724).
9. Cui, H., Zhu, L., Li, J., Yang, Y., & Nie, L. [n. d.]. Scalable Deep Hashing for Large-Scale Social
Image Retrieval. ([n. d.]).
10. Xie, L., Shen, J., & Zhu, L. (2016). Online cross-modal hashing for web image retrieval. In
Proceedings of the AAAI Conference on Artiﬁcial Intelligence (pp. 294–300).
11. Xie, L., Shen, J., Han, J., Zhu, L., & Shao, L. (2017). Dynamic multi-view hashing for online
image retrieval. In Proceedings of the International Joint Conference on Artiﬁcial Intelligence
(pp. 3133–3139).
12. Shen, H. T., Liu, L., Yang, Y., Xu, X., Huang, Z., Shen, F., & Hong, R. (2021). Exploiting
subspace relation in semantic labels for cross-modal hashing. IEEE Transactions on Knowledge
and Data Engineering, 33(10), 3351–3365.
13. Wang, Y., Luo, X., Nie, L., Song, J., Zhang, W., & Xu, X. S. (2021). BATCH: A scalable asym-
metric discrete cross-modal hashing. IEEE Transactions on Knowledge and Data Engineering,
33(11), 3507–3519.
14. Lian, D., Xie, X., & Chen, E. (2021). Discrete matrix factorization and extension for fast item
recommendation. IEEE Transactions on Knowledge and Data Engineering, 33(5), 1919–1933.
15. Liu, H., He, X., Feng, F., Nie, L., Liu, R., & Zhang, H. (2018). Discrete factorization machines
for fast feature-based recommendation. In Proceedings of the International Joint Conference on
Artiﬁcial Intelligence (pp. 3449–3455).
16. Wang, H., Lian, D., & Ge, Y. (2019). Binarized collaborative ﬁltering with distilling graph convo-
lutional network. In Proceedings of the International Joint Conference on Artiﬁcial Intelligence
(pp. 4802–4808).
17. Wang, H., Shao, N., & Lian, D. (2019). Adversarial binary collaborative ﬁltering for implicit
feedback. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence (pp. 5248–5255).

192
5
Multi-modal Discrete Collaborative Filtering
18. Lian, D., Liu, R., Ge, Y., Zheng, K., Xie, X., & Cao, L. (2017). Discrete Content-aware matrix
factorization. In Proceedings of the ACM SIGKDD International Conference on Knowledge
Discovery and Data Mining (pp. 325–334).
19. Wang, C., & Blei, D. M. (2011). Collaborative topic modeling for recommending scientiﬁc
articles. In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery
and Data Mining (pp. 448–456).
20. Wang, H., Wang, N., & Yeung, D. Y. (2015). Collaborative deep learning for recommender
systems. In Proceedings of the ACM SIGKDD International Conference on Knowledge Discovery
and Data Mining (pp. 1235–1244).
21. Zhao, W., Guan, Z., Chen, L., He, X., Cai, D., Wang, B., & Wang, Q. (2018). Weakly-supervised
deep embedding for product review sentiment analysis. IEEE Transactions on Knowledge and
Data Engineering, 30(1), 185–197.
22. Zhao, W., Tan, S., Guan, Z., Zhang, B., Gong, M., Cao, Z., & Wang, Q. (2018). Learning to
map social network users by uniﬁed manifold alignment on hypergraph. IEEE Transactions on
Neural Network and Learning Systems, 29(12), 5834–5846.
23. Xu, C., Guan, Z., Zhao, W., Wu, Q., Yan, M., Chen, L., & Miao, Q. (2021). Recommendation
by users’ multimodal preferences for smart city applications. IEEE Transactions on Industrial
Informatics, 17(6), 4197–4205.
24. Wei, Y., Wang, X., Nie, L., He, X., Hong, R., & Chua, T. S. (2019). MMGCN: Multi-modal
graph convolution network for personalized recommendation of micro-video. In Proceedings of
the ACM International Conference on Multimedia (pp. 1437–1445).
25. Li, J., Lu, K., Huang, Z., & Shen, H. T. (2021). On both cold-start and long-tail recommendation
with social data. IEEE Transactions on Knowledge and Data Engineering, 33(1), 194–208.
26. Zhao, W., Xu, C., Guan, Z., & Liu, Y. (2021). Multiview concept learning via deep matrix
factorization. IEEE Transactions on Neural Network and Learning Systems, 32(2), 814–825.
27. Das, A. S., Datar, M., Garg, A., & Rajaram, S. (2007). Google news personalization: Scalable
online collaborative ﬁltering. In Proceedings of the International Conference on World Wide Web
(pp. 271–280).
28. Gionis, A., Indyk, P., & Motwani, R. (1999). Similarity search in high dimensions via hashing.
In VLDB (pp. 518–529).
29. Karatzoglou, A., Smola, A., & Weimer, M. (2010). Collaborative ﬁltering on a budget. In Pro-
ceedings of the International Conference on Artiﬁcial Intelligence and Statistics (pp. 389–396).
30. Zhou, K., & Zha, H. (2012). Learning binary codes for collaborative ﬁltering. In Proceedings of
the International Conference on Knowledge Discovery and Data Mining (pp. 498–506).
31. Gong, Y., Lazebnik, S., Gordo, A., & Perronnin, F. (2013). Iterative quantization: A procrustean
approach to learning binary codes for large-scale image retrieval. IEEE Transactions on Pattern
Analysis and Machine Intelligence, 35(12), 2916–2929.
32. Liu, X., He, J., Deng, C., & Lang, B. (2014). Collaborative hashing. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition (pp. 2147–2154).
33. Zhang, Z., Wang, Q., Ruan, L., & Si, L. (2014). Preference preserving hashing for efﬁcient
recommendation. In Proceedings of the International ACM SIGIR Conference on Research and
Development in Information Retrieval (pp. 183–192).
34. Zhang, H., Shen, F., Liu, W., He, X., Luan, H., & Chua, T. S. (2016). Discrete collaborative
ﬁltering. In Proceedings of the 39th International ACM SIGIR conference on Research and
Development in Information Retrieval (pp. 325–334).
35. Shen, F., Shen, C., Liu, W., & Tao Shen, H. (2015). Supervised discrete hashing. In Proceedings
of the IEEE Conference on Computer Vision and Pattern Recognition, (pp. 37–45).

References
193
36. Zhang, Y., Yin, H., Huang, Z., Du, X., Yang, G., & Lian, D. (2018). Discrete deep learning for
fast content-aware recommendation. In Proceedings of the ACM International Conference on
Web Search and Data Mining (pp. 717–726).
37. Hinton, G. E., Osindero, S., & Teh, Y. W. (2006). A fast learning algorithm for deep belief nets.
Neural Computation, 18(7), 1527–1554.
38. Guo, G., Yang, E., Shen, L., Yang, X., & He, X. (2019). Discrete trust-aware matrix factorization
for fast recommendation. In Proceedings of the International Joint Conference on Artiﬁcial
Intelligence (pp. 1380–1386).
39. Liu, C., Wang, X., Lu, T., Zhu, W., Sun, J., & Hoi, S. (2019). Discrete social recommendation.
In Proceedings of the AAAI Conference on Artiﬁcial Intelligence (pp. 208–215).
40. Tan, Q., Liu, N., Zhao, X., Yang, H., Zhou, J., & Hu, X. (2020). Learning to hash with graph
neural networks for recommender systems. In Proceedings of the International Conference on
World Wide Web (pp. 1988–1998).
41. Kipf, T. N., & Welling, M. (2017). Semi-supervised classiﬁcation with graph convolutional
networks. In Proceedings of the International Conference on Learning Representations (pp.
1–14).
42. Creswell, A., White, T., Dumoulin, V., Arulkumaran, K., Sengupta, B., & Bharath, A. A. (2014).
Generative adversarial nets. In Proceedings of the Advances in Neural Information Processing
Systems (2672–2680).
43. Li, J., Wu, Y., Zhao, J., & Lu, K. (2017). Low-rank discriminant embedding for multiview
learning. IEEE Transactions on Cybernetics, 47(11), 3516–3529.
44. Ding, Z., & Fu, Y. (2019). Deep transfer low-rank coding for cross-domain learning. IEEE
Transactions on Neural Networks and Learning Systems, 30(6), 1768–1779.
45. Lin, Z., Chen, M., & Ma, Y. (2010). The Augmented Lagrange Multiplier Method for Exact
Recovery of Corrupted Low-Rank Matrices. arXiv:1009.5055.
46. Murty, K. G. (2007). Nonlinear programming theory and algorithms. Technometrics, 49(1), 105.
47. Zhu, L., Shen, J., Xie, L., & Cheng, Z. (2017). Unsupervised visual hashing with semantic assis-
tant for content-based image retrieval. IEEE Transactions on Knowledge and Data Engineering,
29(2), 472–486.
48. Li, J., Jing, M., Lu, K., Zhu, L., Yang, Y., & Huang, Z. (2019). From zero-shot learning to
cold-start recommendation. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence
(pp. 4189–4196).
49. Li, J., Jing, M., Lu, K., Ding, Z., Zhu, L., & Huang, Z. (2019). Leveraging the invariant side of
generative zero-shot learning. In Proceedings of the IEEE Conference on Computer Vision and
Pattern Recognition (pp. 7402–7411).
50. Salakhutdinov, R., & Mnih, A. (2007). Probabilistic matrix factorization. In Proceedings of the
Advances in Neural Information Processing Systems (pp. 1257–1264).
51. Vincent, P., Larochelle, H., Lajoie, I., Bengio, Y., & Manzagol, P.-A. (2010). Stacked denoising
autoencoders: Learning useful representations in a deep network with a local denoising criterion.
Journal of Machine Learning Research, 11(2010), 3371–3408.
52. Xu,Y.,Zhu,L.,Cheng,Z.,Li,J.,&Sun,J.(2020).Multi-featurediscretecollaborativeﬁlteringfor
fast cold-start recommendation. In Proceedings of the AAAI Conference on Artiﬁcial Intelligence
(pp. 270–278).
53. Cheng, Z., & Shen, J. (2016). On effective location-aware music recommendation. ACM Trans-
actions on Information Systems, 34(2), 13:1–13:32.
54. Li, C., Niu, X., Luo, X., Chen, Z., & Quan, C. (2019). A review-driven neural model for sequential
recommendation. In Proceedings of the International Joint Conference on Artiﬁcial Intelligence
(pp. 2866–2872).

194
5
Multi-modal Discrete Collaborative Filtering
55. Wang, W., Yin, H., Huang, Z., Wang, Q., Du, X., & Nguyen, Q. V. H. (2018). Streaming ranking
based recommender systems. In Proceedings of the International ACM SIGIR Conference on
Research and Development in Information Retrieval (pp. 525–534).
56. Chen, H., Chen, X., Shi, S., & Zhang, Y. (2021). Generate Natural Language Explanations for
Recommendation. arXiv:2101.03392.
57. Li, P., Wang, Z., Ren, Z., Bing, L., & Lam, W. (2017). Neural rating regression with abstractive
tips generation for recommendation. In Proceedings of the International ACM SIGIR Conference
on Research and Development in Information Retrieval (pp. 345–354).
58. Lu, Y., Dong, R., & Smyth, B. (2018). Why I like it: Multi-task learning for recommendation
and explanation. In Proceedings of the ACM Conference on Recommender Systems (pp. 4–12).
59. Chen, Z., Wang, X., Xie, X., Wu, T., Bu, G., Wang, Y., & Chen, E. (2019). Co-attentive multi-task
learning for explainable recommendation. In Proceedings of the International Joint Conference
on Artiﬁcial Intelligence (pp. 2137–2143).
60. Nair, V., & Hinton, G. E. (2010). Rectiﬁed linear units improve restricted boltzmann machines.
In Proceedings of the International Conference on Machine Learning (pp. 807–814).
61. Collobert, R., Weston, J., Bottou, L., Karlen, M., Kavukcuoglu, K., & Kuksa, P. P. (2011). Natural
language processing (almost) from scratch. Journal of Machine Learning Research, 12(2011),
2493–2537.
62. Fu, W., Peng, Z., Wang, S., Xu, Y., & Li, J. (2019). Deeply fusing reviews and contents for cold
start users in cross-domain recommendation systems. In Proceedings of the AAAI Conference
on Artiﬁcial Intelligence (pp. 94–101).
63. (2018). Neural attentional rating regression with review-level explanations. In Proceedings of
the International Conference on World Wide Web (pp. 1583–1592).
64. Kendall, A., Gal, Y., & Cipolla, R. (2018). Multi-task learning using uncertainty to weigh losses
for scene geometry and semantics. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition (pp. 7482–7491).
65. Hansen, C., Hansen, C., Simonsen, J. G., Alstrup, S., & Lioma, C. (2020). Content-aware neural
hashing for cold-start recommendation. In Proceedings of the International ACM SIGIR Con-
ference on Research and Development in Information Retrieval (pp. 971–980).
66. Xue, H. J., Dai, X., Zhang, J., Huang, S., & Chen, J. (2017). Deep matrix factorization models
for recommender systems. In Proceedings of the International Joint Conference on Artiﬁcial
Intelligence (pp. 3203–3209).
67. Erkan, G., & Radev, D. R. (2004). LexRank: Graph-based lexical centrality as salience in text
summarization. Journal of Artiﬁcial Intelligence Research, 22(2004), 457–479.
68. Lin, C.-Y. (2004). Rouge: A package for automatic evaluation of summaries. Proceedings of the
Annual Meeting of the Association for Computational Linguistics, 8(2004), 74–81.
69. Papineni, K., Roukos, S., Ward, T., & Zhu, W. J. (2002). Bleu: A method for automatic eval-
uation of machine translation. In Proceedings of the Annual Meeting of the Association for
Computational Linguistics (pp. 311–318).
70. He, X., Zhang, H., Kan, M. Y., & Chua, T. S. (2016). Fast matrix factorization for online recom-
mendation with implicit feedback. In Proceedings of the International ACM SIGIR Conference
on Research and Development in Information Retrieval (pp. 549–558).
71. He, X., Liao, L., Zhang, H., Nie, L., Hu, X., & Chua, T. S. (2017). Neural collaborative ﬁltering.
In Proceedings of the International Conference on World Wide Web (pp. 173–182). ACM.

References
195
72. Elkahky, A. M., Song, Y., & He, X. (2015). A multi-view deep learning approach for cross domain
user modeling in recommendation systems. In Proceedings of the International Conference on
World Wide Web (pp. 278–288).
73. Kingma, D. P., & Ba, J. (2015). Adam: A method for stochastic optimization. In Proceedings of
the International Conference on Learning Representations.

6
Research Frontiers
Thus far, in this book, we have provided an in-depth introduction to multi-modal hash
learning for large-scale multimedia retrieval and recommendation. In particular, we ﬁrst
introduce the great demand to develop effective multi-modal hash learning frameworks in
big data environments. Then we analyze the prominent research challenges toward this end,
such as the heterogeneous modality gap, multi-modal semantic modeling, cold-start and
explainable recommendation problem, and ineffective and inefﬁcient hash optimization.
To address these issues, we present a series of multi-modal hashing methods, comprising
context-aware hashing for image retrieval, cross-modal hashing for unsupervised and super-
visedcross-modalretrieval,compositemulti-modalhashing,andhashing-basedmulti-modal
recommendation methods. Although the above studies have shed some light on large-scale
multimedia retrieval and recommendation, we have to admit that this research line is still at
the young and up-and-coming stage. Here we list a few promising future research directions
with their corresponding challenges.
(1) Constructing large-scale multimedia retrieval datasets. Although various effective
approaches have been developed to improve the performance of multimedia retrieval, there
is still a lack of large-scale multi-modal benchmark datasets with abundant multi-modal
instances to verify the performance of these models. Existing multimedia retrieval datasets,
such as the widely-used datasets Wiki [1], MIR Flickr [2], NUS-WIDE [3], and MS COCO
[4], only contain two modalities (image and text). The PKU XMedia [5] dataset contains
ﬁve modalities, yet its involved multi-modal instances are small-scale. Theoretically, the
experiments on such small-scale datasets cannot verify performance on large-scale datasets.
Hence, to develop effective multi-modal hashing models for real-world multimedia retrieval
and advance this research area, it is important to construct large-scale multimedia retrieval
datasets.
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2024
L. Zhu et al., Multi-modal Hash Learning, Synthesis Lectures on Information
Concepts, Retrieval, and Services, https://doi.org/10.1007/978-3-031-37291-9_6
197

198
6
Research Frontiers
(2) Deep multi-modal semantic modeling and binary embedding. Multi-modal semantic
modeling is the basis of the subsequent hash learning. As the semantics of binary hash codes
only come from the modeling results, the multi-modal semantic modeling performance will
signiﬁcantly impact the hashing retrieval accuracy. A traditional and widely-applied deep
hashing strategy is to extract the heterogeneous deep features through deep neural networks
ﬁrst and then map them into a common Hamming space, where the multi-modal data are
directly measured. However, this kind of method is too coarse to effectively alleviate the
modality gap among heterogeneous modalities. Existing studies [6, 7] have veriﬁed that
the existing complex deep cross-modal hashing models cannot outperform simple shallow
cross-modal hashing methods. These results empirically show that the current deep networks
for multi-modal hashing have not sufﬁciently exploited the powerful representation of deep
neural networks to model the multi-modal semantics. Therefore, how to model/reveal the
complex multi-modal heterogeneous semantic correlations by the advanced deep models
and effectively preserve them into binary hash codes is a challenge worth investigating.
(3) Open-world deep multi-modal hash learning. Existing research work mainly focuses
on the multi-modal hash learning problem under the closed environment or the open-world
shallow multi-modal hash learning. In the open-world multimedia retrieval scenario, there is
a large amount of incomplete and noisy multi-modal data. For example, on social networks,
images are voluntarily uploaded by various users. Images, uploaded by various users without
texts, or noisy image-text pairs, which results in a large amount of partial-modal and noisy
data. Most existing deep multi-modal hashing methods only train the hashing model on a
large amount of clean and fully-paired samples. To enable the hash learning process, they
should ﬁrst remove the partial and noisy multi-modal data directly, and then train the hashing
model. Under such circumstances, directly removing these data will reduce the number of
training samples and cannot sufﬁciently exploit the available expensive multi-modal training
data. Moreover, in practice, labeled large-scale multi-modal data is time-consuming and
expensive to obtain. Therefore, how to exploit these available noisy and limited multi-modal
data for effective deep multi-modal hash learning will be an important research direction
in the future. Finally, in the open-world retrieval scenario, the multi-modal data is usually
streaming. How to effectively and efﬁciently update the deep multi-modal hashing model
with the online streaming multi-modal data is another interesting topic to investigate.
(4) Adversarial multi-modal hash learning. Existing advanced multi-modal hash learn-
ing methods are based on various deep neural networks. However, the adversarial examples
deteriorate the reliability and robustness of deep neural networks, which restrains the perfor-
mance of deep learning models in multi-modal hashing retrieval systems. In the presence of
this, recent years have seen an emerging surge of literature on adversarial machine learning,
which spans both the theoretical or practical analyses of vulnerabilities in machine learning
models and algorithmic defense techniques that can yield more robust learning architectures.
Therefore, it is a very highly demanding research task to develop preferable advanced attack

References
199
and defense mechanisms regarding the weaknesses of modern machine learning and deep
learning architectures in multi-modal hashing retrieval.
(5) Lightweight multi-modal hashing model. The objective of multi-modal hashing is to
improve the efﬁciency of multimedia systems, where the hashing model is expected to be
light-weighted and highly efﬁcient. With the emergence of complex and large-scale multi-
modal data, how to design efﬁcient multimedia retrieval algorithms is still an important
research direction in the future. Besides, how to exploit the efﬁcient multi-modal hashing
model to improve the efﬁciency of various multimedia applications is another interesting
study to investigate.
References
1. Rasiwasia, N., Costa Pereira, J., Coviello, E., Doyle, G., Lanckriet, G. R., Levy, R., & Vasconcelos,
N. (2010). A new approach to cross-modal multimedia retrieval. In Proceedings of the ACM
International Conference on Multimedia (pp. 251–260).
2. Huiskes, M. J., & Lew, M. S. (2008). The MIR Flickr retrieval evaluation. In Proceedings of the
ACM SIGMM International Conference on Multimedia Information Retrieval (pp. 39–43).
3. Chua, TatSeng, Tang, Jinhui, Hong, Richang, Li, Haojie, Luo, Zhiping, & Zheng, Yantao. (2009).
NUS-WIDE: A real-world web image database from National University of Singapore. In Pro-
ceedings of the ACM International Conference on Image and Video Retrieval., 48(1–48), 9.
4. Lin, Tsung-Yi., Maire, Michael, Belongie, Serge J., Hays, James, Perona, Pietro, Ramanan, Deva,
et al. (2014). Microsoft COCO: Common objects in context. In Proceedings of the European
Conference on Computer Vision, 8693, 740–755.
5. Peng, Y., Huang, X., & Zhao, Y. (2018). An overview of cross-media retrieval: Concepts, method-
ologies, benchmarks, and challenges. IEEE Transactions on Circuits and Systems for Video Tech-
nology, 28(9), 2372–2385.
6. Chen, Z. D., Li, C. X., Luo, X., Nie, L., Zhang, W., & Xu, X. S. (2020). SCRATCH: A scalable
discrete matrix factorization hashing framework for cross-modal retrieval. IEEE Transactions on
Circuits and Systems for Video Technology, 30(7), 2262–2275.
7. Wang,Y.,Luo,X.,Nie,L.,Song,J.,Zhang,W.,&Xu,X.S.(2021).BATCH:Ascalableasymmetric
discrete cross-modal hashing. IEEE Transactions on Knowledge and Data Engineering, 33(11),
3507–3519.

