POLITECNICO DI MILANO
Department of Environmental and Civil Engineering
Doctoral programme in Environmental and Infrastructure Engineering
Bayesian gravity inversion
by Monte Carlo methods
Doctoral dissertation by:
Lorenzo Rossi
820437
Supervisor:
Dr. Mirko Reguzzoni
Tutor:
Prof. Giovanna Venuti
The Chair of Doctoral programme:
Prof. Alberto Guadagnini
XXIX Cycle (2013 - 2016)


Acknowledgement
I wish to express my deepest gratitude to my supervisor Dr. Mirko Reguzzoni for
the encouragement, support and thoughtful guidance throughout the development
of this research project. Your enthusiasm and critical attitude have stimulated my
interest in the research activity and greatly improved the quality of this thesis. I
will never stop thanking you for that.
I would like also to express my gratefulness to prof. Fernando Sans`o for laying
the foundations of this project about ﬁve years ago. Without your experience, all
the fruitful discussions and advices received during these years, this project would
never have been concluded.
Another owe thanks goes to the staﬀof GReD s.r.l., in particular to Dr. Daniele
Sampietro. We shared various activity in this project and it has been a pleasure
working together.
The test case presented here cannot be realized without the precious contribu-
tion of Fabio Mantovani, Virginia Strati, Marica Baldoncini and Ivan Callegari of
University of Ferrara. I would like to thank you for the work done to retrieve the
geological data necessary to apply the developed algorithm on a real test case.
A special thanks to the two reviewers of this research manuscript, prof. Bat-
tista Benciolini and prof. Wolf-Dieter Schuh. Your thoughtful criticism and your
suggestions increased a lot the quality of the ﬁnal version of this manuscript.
Last, but not least, I would like to thank Greta for all the support received
support during these years.
It has been very important to conclude this work,
especially during the last months. A special thanks also for patiently digitizing the
ﬁgures of the original manuscript.

4

Contents
Abstract
7
Conventions
9
1
The gravity ﬁeld and its observation
11
1.1
Theory of gravitation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
11
1.2
Earth’s gravity ﬁeld
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
15
1.3
Gravimetry . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
17
1.3.1
Absolute gravimetry . . . . . . . . . . . . . . . . . . . . . . . . . . .
17
1.3.2
Relative gravimetry
. . . . . . . . . . . . . . . . . . . . . . . . . . .
19
1.3.3
Gravimetry from moving platforms . . . . . . . . . . . . . . . . . . .
21
1.3.4
Gradiometry
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
22
1.4
Data reduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
23
1.4.1
Normal Gravity . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
24
1.4.2
Free-air correction . . . . . . . . . . . . . . . . . . . . . . . . . . . .
25
1.4.3
Bouguer correction . . . . . . . . . . . . . . . . . . . . . . . . . . . .
26
1.4.4
Tidal correction
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
28
1.4.5
Other eﬀects
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
28
2
Gravity interpretation
29
2.1
Geometrical approximation
. . . . . . . . . . . . . . . . . . . . . . . . . . .
30
2.2
Forward methods . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
31
2.3
Inverse methods
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
32
2.3.1
Linear inverse problem . . . . . . . . . . . . . . . . . . . . . . . . . .
33
2.3.2
Solution to the inverse gravimetric problem . . . . . . . . . . . . . .
34
2.4
The Bayesian approach
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
37
2.4.1
Prior probability . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
38
2.4.2
The Bayesian approach in gravity inversion . . . . . . . . . . . . . .
39
3
Optimization by Monte Carlo methods
41
3.1
Simulated annealing
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
43
3.2
Gibbs sampler . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
45
3.3
Markov random ﬁelds
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
48
3.4
Deterministic optimization . . . . . . . . . . . . . . . . . . . . . . . . . . . .
51
4
The Bayesian inversion algorithm
55
4.1
Problem setting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
55
4.2
Posterior probability formalization . . . . . . . . . . . . . . . . . . . . . . .
59
4.2.1
Likelihood probability distribution . . . . . . . . . . . . . . . . . . .
59
5

Bayesian gravity inversion by Monte Carlo methods
4.2.2
Prior probability distribution . . . . . . . . . . . . . . . . . . . . . .
59
4.2.3
Posterior probability distribution . . . . . . . . . . . . . . . . . . . .
65
4.3
Optimization algorithm
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
66
4.3.1
Stochastic optimization
. . . . . . . . . . . . . . . . . . . . . . . . .
66
4.3.2
Deterministic optimization
. . . . . . . . . . . . . . . . . . . . . . .
72
4.4
Accuracy assessment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
73
4.5
Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
74
5
Empirical setup of parameters
75
5.1
Physical parameters setting . . . . . . . . . . . . . . . . . . . . . . . . . . .
75
5.2
Weights tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
79
5.3
Temperature law . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
81
6
The developed software
87
6.1
Structure of the software . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
87
6.2
Forward modelling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
91
6.3
Inverse of the density correlation matrix . . . . . . . . . . . . . . . . . . . .
94
6.4
Sampling algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
98
6.4.1
Labels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
98
6.4.2
Densities
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 100
7
Oil exploration test case
101
7.1
Problem setting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
7.2
Prior probability of labels . . . . . . . . . . . . . . . . . . . . . . . . . . . . 105
7.3
Inverse solution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 109
7.4
Eﬀect of the density correlation matrix . . . . . . . . . . . . . . . . . . . . . 113
8
Crustal determination test case
119
8.1
Motivation
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 119
8.2
Gravity signal and 3D voxel model . . . . . . . . . . . . . . . . . . . . . . . 120
8.3
Geological setting of the area . . . . . . . . . . . . . . . . . . . . . . . . . . 123
8.4
Geophysical input
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 124
8.5
Building prior probability . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128
8.6
Estimated crustal model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132
8.7
Comparison with a classical solution . . . . . . . . . . . . . . . . . . . . . . 139
9
Conclusions
143
A Normal probability
145
A.1 Conditional distribution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145
A.2 Product of normal distributions . . . . . . . . . . . . . . . . . . . . . . . . . 147
A.3 Normal likelihood distribution . . . . . . . . . . . . . . . . . . . . . . . . . . 150
Bibliography
153
6

Abstract
The inverse gravimetric problem consists in the reconstruction of the Earth mass
density distribution from the observation of its gravitational ﬁeld.
The solution
to this problem is generally ill-conditioned and non-unique, but, introducing very
strong constraints or numerical regularization, a unique solution can be retrieved.
This could be not representative of the actual mass distribution, because of non-
physical or too restrictive constraints. Moreover, a strong regularization may cause
very smooth solutions in terms of estimated density. In this case, the identiﬁcation
of the boundaries between diﬀerent materials becomes a hard task. To overcome
these limitations, a possible option is to apply the Bayesian approach that easily
allows to introduce prior information on the unknown parameters. Moreover, sec-
ondary parameters can be easily introduced, e.g. to characterize the diﬀerent types
of material, allowing a sharp classiﬁcation of the subsurface.
In the present work, a Bayesian gravity inversion algorithm is developed, with
the aim of estimating a mass density distribution together with a classiﬁcation of
the diﬀerent types of material. The latter allows to identify the boundaries of the
diﬀerent materials and is developed by borrowing image analysis techniques. Con-
sequently, the investigated volume is subdivided into volume units (voxels), each of
them characterized by two random variables: the label deﬁning the type of material
(discrete) and the density (continuous). The a-priori geological information is trans-
lated in terms of this model, providing the mean density with the corresponding
variability for each class of materials and the a-priori most probable label for each
voxel with a set of neighbour rules. These rules have the aim to obtain a clustered
model in terms of geometry with smooth boundary surfaces between the diﬀerent
materials. The ﬁnal solution is retrieved by invoking the Maximum A Posteriori
principle (MAP). The MAP is determined by applying Markov Chain Monte Carlo
optimization algorithms. In particular, a simulated annealing performed by a Gibbs
sampler is chosen to maximize the posterior distribution. The obtained result is
then reﬁned by a deterministic optimization algorithm.
The proposed method has been implemented into a hybrid Matlab/C code. The
software is then applied in the frameworks of oil exploration and crustal investigation.
The former application is performed by a simulated scenario, avoiding border eﬀects
in the gravity data reduction and allowing a better understanding of the meaning of
the parameters that deﬁne the prior probability. The latter is related to the inversion
of the crustal structure in south China, with the aim of improving its knowledge just
below a detector of neutrinos and geoneutrinos. The obtained Bayesian solution is
also compared with a classical solution to the inverse gravimetric problem, showing
a good agreement in terms of estimated geometry (diﬀerences with about 1 km of
7

Bayesian gravity inversion by Monte Carlo methods
standard deviation). However, the Bayesian solution improve the smoothness of the
estimated geometry and allows to infer horizontal and vertical density variations.
In general, all the tests show that the method is able to retrieve an estimated
model that is consistent with the given prior information and ﬁts the gravity ob-
servations according to their accuracy.
In this sense, one can conclude that the
developed method is a sort of “artiﬁcial intelligence”, supporting the user by au-
tomatizing the trial-and-error techniques.
In fact, an initial guess on the model
parameters is required, but diﬀerently from a trial-and-error solution these parame-
ters are automatically adapted to ﬁt the gravity observations, according to the rules
introduced by the user as prior probability. If the solution is not satisfactory, this
should be attributed to the weak information provided by the gravity or to the wrong
or incompatible geological information supplied by the user.
8

Conventions
a, A
Vector
a⊺
Transpose of the vector a
aj
i
Vector of elements from i to j, i.e. aj
i = [xi, xi+1, . . . , xj−1, xj]⊺
A
Matrix
A−1
Inverse of a matrix
ai
Column i of the matrix A
A−i
Matrix A without the column i
Aij
Element (i, j) of the matrix A
(A−1)ij
Element (i, j) of the matrix A−1
I
Identity matrix
ei
Column i of an identity matrix I
e
Vector with all the elements equal to 1
ex, ey, ez
Unit vector representing the Cartesian axes
F(·)
Fourier transform of (·)
F−1(·)
Inverse Fourier transform of (·)
χ[a,b](·)
Characteristic function
(
1
a ≤· ≤b
0
otherwise
N[µ,C](•)
Normal probability distribution function exp
 1
2(• −µ)⊺C−1(· −µ)
	
√
2π det C
U[a,b](·)
Uniform probability distribution function
1
b−aχ[a,b](·)
d·
Diﬀerential of ·
log(·)
Natural logarithm of ·
9

10

Chapter 1
The gravity ﬁeld and its
observation
The measurement of the Earth’s gravity ﬁeld has implications in many topics,
like geophysics, geodesy, geodynamics, and ocean circulation. In the former, gravity
has two big advantages compared to other geophysical prospecting measurements:
it can be observed without being directly in contact with the ground, thanks to
shipborne, airborne or satellite data acquisition, and it does not require any active
source, as happens e.g. in seismic data acquisition. In fact, the gravity ﬁeld spatial
variation is directly related to the density variation inside the Earth. Therefore,
measuring the gravity anomalies, i.e. the variation of the gravity with respect to a
normal one, the density anomalies, namely the variation of the density with respect
to the one attributed to a “normal” Earth, can be inferred.
Gravity prospecting is commonly used to investigate crustal structure at basin or
regional scale and to oil and hydrocarbon exploration. In particular, in the last activ-
ity gravity prospecting is used for reconnaissance survey, thanks to its homogeneous
accuracy and regular spacing during the acquisition, high beneﬁts-costs ratio, and
fast measurement time, or, whit a higher resolution, when seismic observations are
not eﬀective. That is why this method was used since 19th century (Krynski, 2012),
but only starting from the 1980s it had an increasing diﬀusion, correspondingly to
the advent of airborne and shipborne gravimetry, and later of satellite gravimetry.
In fact, these measurement techniques allow to reduce surveying time and costs and
to acquire accurate, homogeneous and regularly distributed observations even in
inaccessible areas (Reynolds, 1997).
The aim of this chapter is to understand the principles of the gravitation theory,
the Earth gravity ﬁeld, and the basic principle of its observation.
1.1
Theory of gravitation
The fundamental equation related to gravity observation is the universal law
of gravitation by Newton: “The magnitude of the gravitational force between two
masses is proportional to each mass and inversely proportional to the square of their
11

Bayesian gravity inversion by Monte Carlo methods
separation” (Newton, 1687), shown in Equation 1.1:
fPQ = −GmPmQ
rQP
r3
QP
(1.1)
where fPQ is the attractive force generated by the point mass mQ on the point
mass mP, according to Figure 1.1, G is the universal gravitational constant which
approximately assumes the value G = 6.67 × 10−11m3/s2kg, and rQP is the vector
joining points P and Q, rPQ magnitude. Using Cartesian coordinates, the position
vector can be expressed as:
rQP = (xP −xQ) ex + (yP −yQ) ey + (zP −zQ) ez
(1.2)
where ex, ey, ez represents the unit vector deﬁning the three axes, and
x
y
z
P mP
mP
fQP
rP Q
Q mQ
fP Q
Figure 1.1:
Gravitational attraction
between two point masses.
x
y
z
rQ
Q(rQ)
rP Q
P(rP )
rP
ρ(rQ)
B
O
Figure 1.2: Gravitational ﬁeld due to
a generic mass distribution.
Dividing Equation 1.1 by mP the Newtonian gravitational attraction gN(P) ex-
erted by Q on P is retrieved, i.e. the acceleration felt by P from Q is:
gN(P) = −GmQ
rQP
r3
QP
(1.3)
The three components of the gravitational attraction in a Cartesian coordinate sys-
tem can be computed by introducing Equation 1.2 into 1.3:















gN,x(P) = gN(P) · ex = −GmQ
xP −xQ
r3
QP
gN,y(P) = gN(P) · ey = −GmQ
yP −yQ
r3
QP
gN,z(P) = gN(P) · ez = −GmQ
zP −zQ
r3
QP
(1.4)
12

CHAPTER 1.
THE GRAVITY FIELD AND ITS OBSERVATION
Another important property of the gravitational attraction is that it is a vectorial
ﬁeld which can be described by a scalar potential.
In other words the vectorial
gravitational ﬁeld can be expressed as the gradient of a scalar ﬁeld, the so-called
gravitational potential V (Heiskanen and Moritz, 1967):
gN(P) = ∇V (P)
(1.5)
The expression of the potential ﬁeld V generated by the mass mQ at a generic point
P is:
V (P) = G mQ
rQP
(1.6)
It is worth to notice that the expression of the gravitational acceleration through
the potential V , as shown in Equation 1.5, allows to express the three components
of g(P) by means of a scalar ﬁeld.
The potential generated by a set o masses mq, where q = 1, 2, 3, . . . , n, can
be computed by applying the superposition principle, as the sum of the individual
contribution of each point mass (Kellogg, 1929):
V (P) = G
n
X
q=1
mq
rqP
(1.7)
Assuming that masses are distributed continuously inside a volume B, as shown in
Figure 1.2, their density becomes:
ρ = dm
dv
(1.8)
where dv is the inﬁnitesimal element of volume and dm the inﬁnitesimal element
of mass. Now, deﬁning Q as a generic point inside the body B, the sum shown in
Equation 1.7 becomes an integral:
V (P) = G
Z
B
dm
|rQP| = G
Z
B
ρ(Q)
|rQP|dv
(1.9)
Assuming xP, yP, and zP the coordinates of the observation point and x, y, and z
the coordinates of the “running” point inside B, the inﬁnitesimal volume dv is equal
to dx · dy · dz. Introducing this assumption, Equation 1.9 turns in:
V (xP, yP, zP) = G
ZZZ
B
ρ(x, y, z)
q
(xP −x)2 + (yP −y)2 + (zP −z)2 dxdydz
(1.10)
Applying Equation 1.5, we can compute the value of the three components of the
gravity acceleration of the continuous body B. For example, the component along
the z coordinate becomes:
gN,z(xP, yP, zP) = ∂V
∂zP
=
= G ∂
∂zP
ZZZ
B
ρ(x, y, z)
q
(xP −x)2 + (yP −y)2 + (zP −z)2 dxdydz =
13

Bayesian gravity inversion by Monte Carlo methods
= G
ZZZ
B
ρ(x, y, z) ∂
∂zP
1
q
(xP −x)2 + (yP −y)2 + (zP −z)2 dxdydz =
= −G
ZZZ
B
ρ(x, y, z)
zP −z

(xP −x)2 + (yP −y)2 + (zP −z)23/2dxdydz
(1.11)
By properly setting the domain of the integral of Equation 1.10 or 1.11 according to
the shape of the body B, the potential or the gravity acceleration contribution gen-
erated by simple bodies with diﬀerent shapes, e.g. a sphere, a cylinder, a rectangular
prism, etc., can be computed.
In the case of a rectangular prism, assuming a Cartesian coordinate system with
the origin in the point P, such that xP = 0, yP = 0, zP = 0 as depicted in Figure
1.3, and a mass density that is constant inside the prism itself, the potential can be
computed starting from Equation 1.10:
V (P) = Gρ
Z x2
x1
Z y2
y1
Z z2
z1
dxdydz
p
x2 + y2 + z2
(1.12)
Finally, integrating Equation 1.12 the potential of the rectangular prism is retrieved
as (Nagy et al, 2000):
V (P) = Gρ


xy log(z + r) + yz log(x + r) + zx log(y + r)
−x2
2 arctan yz
xr −y2
2 arctan zx
yr −z2
2 arctan xy
zr

x2
x1

y2
y1

z2
z1
(1.13)
where r =
p
x2 + y2 + z2 is the modulus of the position vector of the“running”point
inside the prism. Deriving Equation 1.13, the gravitational acceleration generated
x
y
z
x1
x2
y1
y2
z1
z2
P
Figure 1.3: Prism geometry.
14

CHAPTER 1.
THE GRAVITY FIELD AND ITS OBSERVATION
by the prism can be computed. For example, along the z direction, the Newtonian
attraction of the prism becomes (Nagy et al, 2000):
gN,z(P) = ∂V (P)
∂z
= Gρ


x log(y + r) + y log(x + r) −z arctan xy
zr

x2
x1

y2
y1

z2
z1
(1.14)
According to the Syst`eme Internationale (International System, abbreviated SI)
the unit of measurement of gravitational acceleration is m/s2. Nevertheless, a unit of
measurement that is very often used in gravimetry and geodesy ﬁelds is the Gal, that
derives from the CGS. In particular, 1 Gal = 1 cm/s2 = 10−2 m/s2. Furthermore,
according to the order of magnitude of the observed anomalies, usually sub-multiples
of the Gal are used in gravimetric application, namely mGal and µGal.
1.2
Earth’s gravity ﬁeld
The gravitation theory, explained in the previous section, is valid only when an
inertial reference system is considered. However, and Earth-ﬁxed system cannot be
considered inertial because there are at least two important non-uniform motions:
the revolution of the earth around the sun and the rotation of earth around its own
axis.
The former eﬀect can be neglected even if its contribution is quite large (of the
order of 0.6 Gal) because the Earth is in a free fall motion with respect to the
sun, namely the centrifugal force (revolution) and the centripetal one (heliocentric
attraction) are balanced at the Earth barycentre.
Consequently, a point on the
Earth surface suﬀers only of a small residual attraction, i.e. the tidal potential, of
the order of 0.025 mGal (Sans`o and Sideris, 2012). Therefore, an Earth centred
reference system with the ZQI axis along its rotation axis and XQI and YQI pointing
always in the same direction with respect to the ﬁxed stars can be considered quasi-
inertial, i.e. Newton law’s holds with a quite good approximation.
Now, we can deﬁne also an Earth-ﬁxed reference system, where the Z axis is the
same of the previous one, i.e. Z = ZQI, while X and Y rotate around the Z axis with
a constant angular speed with respect to XQI and YQI. This assumption modiﬁes
the fundamental law of dynamics of a point P with mass m. In fact, starting from
the second law of dynamics:
m¨rQI(P) = f(P) + mgN(P)
(1.15)
where the subscript QI denotes that the acceleration ¨rQI is expressed in the quasi-
inertial reference frame, gN(P) is the Newtonian attraction acting on P and f(P) is
the sum of the other forces acting on m. Recalling Coriolis theorem, the acceleration
¨rQI present in Equation 1.15 can be expressed in terms of the Earth-ﬁxed reference
frame as (Arnold, 1978):
m[¨r + 2ω × ˙r + ω × (ω × r) + ˙ω × r] = f(P) + mgN(P)
(1.16)
where ω is the angular velocity of the Earth.
If the measurement point has no
velocity with respect to the Earth, i.e. ˙r = 0, the terms 2ω × ˙r and ¨r vanish.
15

Bayesian gravity inversion by Monte Carlo methods
Furthermore, the angular velocity of the Earth can be considered a constant in
time, thus also the term ˙ω × r assumes null value. Solving Equation 1.16 under
the assumption that the Z axis of the Earth-ﬁxed reference system is parallel to
the earth rotation axis, the force f(P) required to keep the point P clamped to the
Earth can be computed as (Sans`o and Sideris, 2012):
f(P) = m[ω × (ω × r) −gN(P)] = m

−ω2(xeX + yeY ) −gN(P)

(1.17)
where eX and eY are the unit vector representing the Cartesian axes of the Earth-
ﬁxed system. If the point mass is ﬁxed to the Earth, it feels an acceleration equal
to −1
mf(P), that is the so-called gravity acceleration g(P):
g(P) = −1
mf(P) = gN(P) + ω2(xex + yey) = gN(P) + gC(P)
(1.18)
where gC(P) is the centrifugal acceleration.
It is worth to notice that also the
centrifugal acceleration can be expressed as the gradient of a potential:
gC(P) = ω2(xex + yey) = ∇
1
2ω2 x2 + y2
= ∇VC
(1.19)
where VC is the centrifugal potential. This turns in the fact that the Earth’s gravity
ﬁeld is a potential ﬁeld too, allowing to derive the gravity ﬁeld potential W(P) as:
W(P) = V (P) + VC(P)
(1.20)
The gravity acceleration vector g represents the total acceleration felt by a body. Its
magnitude g = |g| is called gravity and the vector is directed in the direction of the
plumb line (Heiskanen and Moritz, 1967). As it will be shown in the next section,
gravity is a quantity that is directly observable.
It is important to remark that the previously neglected Coriolis acceleration plays
a role when the observation point P is not Earth-ﬁxed. In fact when ˙r and ¨r are not
equal to 0 the term m[2ω × ˙r], i.e the so-called Coriolis force present in Equation
1.16, has to be considered when the gravity acceleration is observed from a moving
body, e.g. satellite or airborne platform. This term is proportional to the relative
velocity between the moving body and the Earth (Heiskanen and Moritz, 1967).
An approximate value of the gravity potential is the so called normal potential
U(P), obtained by approximating the Earth with an ellipsoid of revolution:
U(P) = VE(P) + VC(P)
(1.21)
where VE(P) is the ellipsoidal gravity potential that is a function of two ellipsoidal
parameters, e.g. major and minor semiaxes a and b, or major semiaxis a and eccen-
tricity e, and of the total mass of the Earth M. The diﬀerence between the potential
W(P) and the normal potential U(P) is the so called anomalous potential T(P):
T(P) = W(P) −U(P) = V (P) −VE(P)
(1.22)
The anomalous potential highlights the deviation of the real gravity ﬁeld with respect
to the normal one, i.e. it shows mass anomalies with respect to a reference density
model consistent with the normal potential.
16

CHAPTER 1.
THE GRAVITY FIELD AND ITS OBSERVATION
The gravitational potential is harmonic outside the masses. Therefore, in case of
the anomalous potential, solving the Laplace equation:
∇2T = 0
(1.23)
by separation of variables in an outer spherical domain, lead to the fact that the
anomalous potential can be expressed as a spherical harmonic series (Heiskanen and
Moritz, 1967; Blackely, 1996; Sans`o and Sideris, 2012).
1.3
Gravimetry
As we have already introduced in the previous section, gravity is an accelera-
tion. Therefore, its measurement should involve determinations of length and time.
However, such an apparently simple task is not easily achievable at the accuracy
required in gravity surveying.
Gravimetry can be subdivided in two main classes: absolute or relative. As sug-
gested by the name, the former deals with the modulus of the gravity that is directly
observed, while the latter deals with gravity variations at diﬀerent stations. Absolute
gravimetry usually requires long period of observation with bulky and very expensive
instruments, while relative gravimetry is featured with faster measurement time and
lighter and cheaper instruments. Furthermore, to interpret gravity anomalies there
is no need to know the absolute value of gravity, as it will shown later. Nevertheless,
thanks to networks of gravity stations covering all over the world, the absolute value
of gravity can be retrieved through relative measurement when at least one diﬀer-
ence is observed with respect to one of this station. The most important network is
the International Gravity Standardisation Net (IGSN 71) that provides the value of
the absolute gravity at each station (Morelli et al, 1974; Krynski, 2012).
The instruments used to measure gravity are called gravimeters or gravity meters
and rely on one of the following physical laws (Torge, 1989; Fedi and Rapolla, 1993):
pendulum oscillation, free-falling body or Hooke’s law describing spring elongation.
Gravity was traditionally measured from the ground, but starting from the 1980s
(Schwarz and Li, 1997) measurements from moving platforms became feasible. This
turned in a wider use of gravimetry, since it became possible to observe the gravity
even in inaccessible areas with uniform data distribution, e.g. by using an airborne
platform.
In the next subsections absolute and relative gravimetry will be discussed, then
the principle of airborne and shipborne gravimetry will be introduced. In particular,
the focus will be put on the physical principles behind the observations and on the
achievable accuracy. Finally, also gradiometry, a kind of measurement also used in
satellite missions, are brieﬂy explained.
1.3.1
Absolute gravimetry
Absolute gravimeters are able to observe the modulus of the gravity vector. The
two main physical principles used are: the free-fall body and the pendulum.
17

Bayesian gravity inversion by Monte Carlo methods
The free-fall body method is based on the equation of motion, considering that
only the gravity ﬁeld is acting:
m¨z = mg(z)
(1.24)
where ¨z = d2z/dt2, z is the vertical coordinate and t is the time. Considering the
gravity ﬁeld constant along the vertical direction Equation 1.24 can be integrated
two times as follows:
˙z = ˙z0 + gt
(1.25)
z = z0 + ˙z0t + g
2t2
(1.26)
Since the initial position z0 and velocity ˙z0 are not well known, it is required to
measure the position and the time at least three times during the falling path as
shown in Figure 1.4. This allows the estimation of the initial conditions together
with the value of gravity acceleration. Modern instruments, called multiposition
experiments, allows to measure more than three positions, thus introducing redun-
dancy in the estimation of gravity and initial conditions that can be reckoned by
using a least squares approach. Position and time are measured with laser inter-
ferometers and atomic clocks, respectively.
Those instruments are able to reach
accuracies of the order of ±0.2 nm (interferometers) and ±0.1 ns (clocks). Propa-
gating these values, a ﬁnal accuracy of the order of the µGal in the observed gravity
can be obtained, considering a common falling distance of 0.2 m and a consequently
falling time of about 0.2 s (Torge, 1989). However, to reach such a level of accuracy,
a sequence of multiposition experiments distributed over diﬀerent days is required.
In fact, the repetition allows reducing random errors, like errors in the resolution
of the interference-fringe-signal and time, errors in associating the signal with time
impulses, and microseismic eﬀects. Moreover, there are other systematic errors that
have to be modelled in order to avoid biases in the ﬁnal estimation, e.g. length and
time standards used, light path, electronic time measurements and non-gravitational
forces (residual atmospheric pressure eﬀects, magnetic disturbing forces, electrical
currents, etc.).
Figure 1.4:
Distance-time diagram
representing proof-mass position in the
free-falling experiment.
The three
points (at least) where position and
time are observed are shown over the
graph.
z
x
ϕ
ϕ0
g
m
ℓ
g sin ϕ
Figure 1.5: Scheme of the forces act-
ing on an absolute pendulum.
18

CHAPTER 1.
THE GRAVITY FIELD AND ITS OBSERVATION
The pendulum method is based on the measurement of the oscillation period T
and of the pendulum length ℓof a freely swinging pendulum. Starting from the
equation of oscillation, according to Figure 1.5, the following relationship holds:
mℓ¨ϕ(t) + mg sin ϕ(t) = 0
(1.27)
where ϕ(t) is the phase angle and ¨ϕ(t) is the angular acceleration.
Integrating
Equation 1.27 over a full period leads to an elliptical integral. After expansion into
a series, the period T can be evaluated as (Torge, 1989):
T = 2π
s
ℓ
g

1 + ϕ2
0
16 + . . .

(1.28)
If the amplitude ϕ0 is kept small, the absolute value of g can be derived by measuring
T and ℓ. It is worth to notice that Equation 1.28 is valid in case of an ideal pendulum,
while in case of a real pendulum the mass distribution and the shape of the pendulum
itself has to be taken into account for retrieving the gravity.
The pendulum method is no longer applied today due to its poor accuracy that
is of about 0.1 mGal.
The main errors aﬀecting the observations are related to
temperature variations that causes length variations, amplitude reductions, and de-
formations that happen during the oscillation process. Nevertheless, this kind of
instruments governed gravimetry for about 300 years, therefore it has a fundamen-
tal importance in the gravity observation. Moreover, recent results of pendulum
observations are still contained in some gravity networks (Torge, 2001).
1.3.2
Relative gravimetry
A relative gravity measurement deals with the gravity variation between two
stations or with gravity variation in time. This requires to measure only the time
or the length, keeping the other quantity as ﬁxed. As a consequence, relative mea-
surements can be performed more easily and more accurately than absolute ones.
Relative gravimeters can rely on dynamic or static methods. In the ﬁrst case, the
period of a pendulum oscillation is observed, while in the second one the observation
is a length, usually the length variation of a spring.
The dynamic method is based on the pendulum principle. In fact, by using the
same pendulum at two stations P1 and P2, then measuring the oscillation period
and, ﬁnally, applying twice Equation 1.28, the gravity diﬀerence can be reckoned as:
g2
g1
= T 2
1
T 2
2
→g2 −g1 = −2g1
T2 −T1
T2
+ g1
(T2 −T1)2
T 2
2
(1.29)
Also in relative gravimetry, this method is no more used because of its accuracy.
In fact, by using relative pendulums only an accuracy of the order of 0.1 mGal can
be achieved. Therefore, starting from the 1930’s they were superseded by elastic
spring gravimeters. However, they were still used until 1960’s for gravimeter cali-
bration lines. In fact, they directly retrieve gravity in its own measurement units
without requiring a calibration, as it happens in case of static gravimeters, as shown
afterwards.
19

Bayesian gravity inversion by Monte Carlo methods
The static gravimeters use a counterforce to keep a test mass in equilibrium with
the gravity force. Measuring the counterforce, the gravity variations can be detected
by applying a transformation, derived from a calibration function. Usually, an elas-
tic counterforce is used, but also a magnetic counterforce could be an alternative.
Elastic spring gravimeters are based on the principle of the spring balance. In fact,
by changing the applied gravity force, the elongation of a spring changes follow-
ing Hooke’s law. Spring balance gravimeters can be subdivided into translational
systems and rotational systems, as shown in Figure 1.6.
For a translational system, also called vertical spring balance gravimeter, see
Figure 1.6(a), the equilibrium condition is (Torge, 1989):
mg −k(ℓ−ℓ0) = 0
(1.30)
where k is the elastic spring constant and ℓand ℓ0 the spring length with or without
the force applied, respectively. Observing two diﬀerent lengths ℓ1 and ℓ2 at two dif-
ferent stations P1 and P2, their gravity diﬀerence can be reckoned applying Equation
1.30 at the two points:
g2 −g1 = k
m(ℓ2 −ℓ1)
(1.31)
The calibration consists into determining the value of the ratio k/m used to convert
the observed length diﬀerence into the gravity variation.
z
m
mg
ℓ0
ℓ
k(ℓ−ℓ0)
(a)
z
τ(α0 + α)
α
a
m
mg
O
(b)
z
δ
d
h
ℓ
k(ℓ−ℓ0)
m
mg
a
b
α
O
(c)
Figure 1.6: Elastic spring gravimeter principle: a) vertical spring balance, b) lever torsion spring
balance c) lever spring balance.
Rotational systems use a lever to support the proof mass m, able to rotate around
an axis O. Equilibrium is reached through a torsion spring, see Figure 1.6(b), or
through a helical spring, see Figure 1.6(c). According to Figure 1.6(b), in torsion
spring gravimeters the equilibrium condition is given by:
mga sin α −τ(α0 + α) = 0
(1.32)
where a is the lever length, α the angle between vertical and the lever, α0 and τ
the pretension angle and the torsion constant of the spring, respectively. Gravity
diﬀerences generate angular changes in Equation 1.32, thus solving this non-linear
system it is possible to retrieve gravity variation between the stations. Furthermore,
20

CHAPTER 1.
THE GRAVITY FIELD AND ITS OBSERVATION
thanks to this non-linearity, the sensitivity of the instruments can be increased by
properly choosing the system parameters (Torge, 1989).
The torsion spring can be substituted by a helical spring, as shown in Figure
1.6(c). In this case the equilibrium condition becomes:
mga sin(α + δ) −kbdℓ−ℓ0
ℓ
sin α = 0
(1.33)
As it happens in torsion lever gravimeters, by properly setting the parameters of the
system, its sensitivity can be increased.
Comparing lever spring with vertical spring gravimeters, the sensibility increases
of about 2000 times. That is the reason why the lever spring is the widest used
principle when building relative gravimeters.
It is worth to notice that in order to reach an accuracy of the order of 0.01 mGal
a high precision reading system and a high stability of the counterforce in time are
required. In other words, the elasticity of the spring should be stable while moving
the instrument between diﬀerent stations. Another limitation of spring gravimeters
is that they have a range of measurement related to the maximum extension of the
spring. Modern instruments, e.g. LaCoste and Romberg model G (LaCoste, 1934),
presents a range of about 7000 mGal, while in the 1950’s maximum gravity range
was about 2000 mGal.
Despite all the eﬀorts to protect instruments from environmental disturbances,
spring gravimeters suﬀers of drift eﬀects. They are caused by ageing of the spring
material and short-time changes that occurs during ﬁeld survey. In order to evaluate
the systematic eﬀects of the drift, usually repeated occupations of one or more points
during the same survey are performed. Therefore, under the assumption that gravity
is constant at the same location, the drift function is interpolated experimentally
from observations and then it is removed from the measurements taken at stations
surveyed only once.
1.3.3
Gravimetry from moving platforms
The usage of moving platform is a very interesting task for rapid and high reso-
lution gravimetric survey, especially facing with inacessible areas, e.g. oceans, polar
regions, high mountains, tropical forest, etc. The most used platforms are airplanes
and ships, but also helicopters or land vehicle could be used as an alternative.
The principle used in kinematic platform is based on the equation of motion by
Newton in an inertial reference system:
¨r = f + g
(1.34)
where r is the position vector, ¨r is the acceleration, f is the force acting on a
unitary proof mass, i.e. the spring counterforce in case of a spring gravimeter, and
g is the gravitational eﬀect acting on the proof mass. Thanks to Global Navigation
Satellite System (GNSS) sensors and using an Inertial Measurement Unit (IMU) or
a modiﬁed land gravimeter mounted on a damped two-axes stabilized platform the
acceleration ¨r and the force f can be measured, respectively.
21

Bayesian gravity inversion by Monte Carlo methods
Equation 1.34 can be expressed in a local system, with the z axis oriented as the
vertical and the x and y axes towards the eastern and northern direction respectively,
in order to retrieve the gravity at a certain point:
gL = ¨rL −RL
Bf B +
 2ωL
IE + ωL
EB

× ˙rL
(1.35)
where L denotes quantities expressed in the local frame, RL
B is the rotation matrix
between body and local frames, ωL
IE and ωL
EB the angular velocity of the Earth-ﬁxed
reference frame E with respect to the inertial reference frame I and the angular veloc-
ity of the platform B with respect to the Earth-ﬁxed reference frame E, respectively.
Equation 1.35 can be applied to retrieve the scalar value of the gravity, or the full
gravity vector.
The damped platforms use two gyroscope or accelerometer pairs operating in a
feed-back mode, to realize the local system and to observe the magnitude of the
gravity vector. The latter can be determined starting from 1.35 that, projecting the
vectors on the vertical direction, becomes (Torge, 2001):
g = fz −¨z + 2ωcosϕ sin αv + v2
r
(1.36)
where fz is the vertical acceleration observed by the gravimeter, ¨z is the vertical
component of the acceleration of the platform, ω is the Earth angular speed, ϕ the
geodetic latitude, α the geodetic azimuth, v the platform velocity with respect to
the Earth, and r the distance to the centre of the Earth. The velocity dependent
term in Equation 1.36 represents the so-called E¨otv¨os correction which increases
(for a west-east directed course) the angular velocity of the Earth rotation and the
centrifugal acceleration arising from the angular velocity v/r of the platform around
the centre of the Earth (Torge, 1989).
The kind of disturbing acceleration, namely fz, varies depending on the used
platform, e.g. in shipborne measurements with stabilized platforms they have a pe-
riod between 2 and 20 s, while in airborne measurements their period is between 1
and 300 s. These speciﬁc platform dependent characteristics depends on the diﬀerent
vehicle velocity (i.e. 10 ÷ 20 km/h for a ship, 250 ÷ 450 km/h for an airplane) and
has an inpact on the kind of data ﬁltering to be applied at the observations. The
platform velocity also inﬂuences the ﬁnal resolution of the observed data. Typically,
resolutions from 0.1 to 2 km in case of shipborne acquisition and of about some 10
km in case of airborne acquisition can be achieved (Torge, 2001).
The accuracy of the observed gravity signal is generally comprised between 1 and
6 mGal, depending on the survey condition (e.g. sea state, air turbulence, velocity of
the platform, etc.) and on the quality of the separation between gravity and disturb-
ing accelerations, mainly inﬂuenced by the eﬀectiveness of the damping system, the
ﬁltering techniques, and the accuracy of velocity observation (Torge, 2001; Schwarz
and Li, 1997).
1.3.4
Gradiometry
Gradiometry is the measurement of the second derivatives of the Earth gravity
potential, that represents the curvature of the ﬁeld. It is represented by a second
22

CHAPTER 1.
THE GRAVITY FIELD AND ITS OBSERVATION
order tensor, the so called gravity gradient tensor (Marussi, 1985):
W = grad(g) =


Wxx
Wxy
Wxz
Wyx
Wyy
Wyz
Wzx
Wzy
Wzz


(1.37)
The tensor contains local gravity ﬁeld information and it is interesting for high-
resolution gravity ﬁeld determination. Its units of measurement is s−2, but it is
generally expressed in 10−9s−2 = ns−2 traditionally called E¨otv¨os unit (E). It can be
determined by means of gravity gradiometer, by observing the diﬀerent reaction of
neighbour proof masses. So, a gradiometer is composed by a couple of accelerometers
rigidly connected. The observed quantity is the force diﬀerence at the centre of mass
C of the system, namely:
f2 −f1 = WC(r2 −r1)
(1.38)
where r1 and r1 are the position vector of the two sensors. In order to recover the
full gravity gradient tensor, a series of couple of accelerometers disposed on diﬀerent
axes is used.
This technology was also used in satellite measurements. In fact, thanks to force
diﬀerentiation, no GNSS observations are required to retrieve gravity gradients, thus
allowing the usage of GNSS only for Satellite to Satellite Tracking (SST) technique
and with the aim of giving a position to the observations. A recent satellite gradiom-
etry missions was GOCE by ESA launched in 2009, designed with a nearly polar
circular orbit at altitudes of about 250 km. The GOCE mission was able to estimate
the gravity ﬁeld with about 1 mGal of accuracy at about 70 km × 70 km resolution.
1.4
Data reduction
Sections 1.2 and 1.3 describe the Earth gravity ﬁeld and its observations. Nev-
ertheless, to interpret observed signal in terms of local density variations, the data
have to be to reduced with the aim to isolate only the signal due to the investigated
body. In fact, these bodies usually cause variations in the gravity ﬁeld of the order
of 100 mGal, as shown in ﬁgure 1.7, thus making crucial the data reduction in their
identiﬁcation.
Figure 1.7: Main gravitational constituent of terrestrial gravity ﬁeld (ESA, 2008).
In order to isolate the eﬀects of the investigated bodies, it is required to model
all the other components of the gravity signal, which in general can be thought as
the sum of the following eﬀects (Blackely, 1996):
23

Bayesian gravity inversion by Monte Carlo methods
1. attraction of the reference ellipsoid and eﬀect of the Earth rotation (normal
gravity);
2. eﬀect of observation point elevation above sea level (free-air);
3. eﬀect of “normal” masses above sea level (Bouguer and terrain);
4. time-dependent variation of the eﬀect of the attraction of the sun and of the
moon (tidal correction);
5. eﬀect of density variation with respect to the“normal”density considered before.
The last eﬀect is the one generated by local crustal structures that usually are the
target of the gravity inversion. Therefore, to isolate this contribution, all the others
has to be modelled and the observations has to be reduced by removing them. In
the following, the various reduction applied to the observed data will be described.
1.4.1
Normal Gravity
The so-called normal gravity is the gravity contribution generated by a reference
ellipsoid. Starting from the normal potential, shown in Equation 1.20, and applying
the gradient operator, as discussed in Equation 1.5, the normal gravity γ can be
computed. The outcome is a series that, truncated at the second order, assumes the
shape (Blackely, 1996):
γ(ϕ) = γ0
 1 + α sin2 ϕ + β sin2 2ϕ

(1.39)
where α and β are constants depending on the parameters of the chosen reference
ellipsoid, e.g. the total mass M, the ﬂattening f, the major semi-axis a, the angular
velocity of the Earth ω, on the gravity at the Equator γ0, i.e. γ(ϕ = 0), and on the
latitude of the computational point ϕ. Nevertheless, also a closed-form of the normal
gravity equation exists, the so-called Somigliana equation (Somigliana, 1929):
γ(ϕ) = γ0
 
1 + k sin2 ϕ
p
1 −e2 sin2 ϕ
!
(1.40)
where k is a constant depending on the ellipsoid parameters and e is the ellipsoid
eccentricity. Again the ellipsoid parameters used in Equation 1.40 depend on the
considered reference ellipsoid. The set of parameters in case of diﬀerent standard
reference ellipsoid can be found in Heiskanen and Moritz (1967), Blackely (1996),
and Sans`o and Sideris (2012).
The observed data reduced for the normal the gravity are the so-called gravity
anomaly ∆g, whose expression in a point P becomes:
∆g(P) = go(P) −γ(ϕP)
(1.41)
where go(P) is the observed value of gravity at point P and γ(ϕP) is the above
explained normal gravity.
24

CHAPTER 1.
THE GRAVITY FIELD AND ITS OBSERVATION
1.4.2
Free-air correction
Equations 1.39 and 1.40 do not take into account the elevation hP of the point
P above the reference ellipsoid. In other words, they consider elevation as a null
quantity. Therefore, to correctly take into account the decay of the gravity signal
for the distance to the reference ellipsoid, the so-called free-air correction has to
be applied. As a ﬁrst approximation, the free-air correction can be recovered by a
Taylor expansion of the gravity as (Heiskanen and Moritz, 1967):
g(P) = γ(ϕP) + ∂g
∂hhP + 1
2!
∂g2
∂2hh2
P + . . .
(1.42)
where the zero order term is exactly the normal gravity and the series starting
from the ﬁrst order term is what usually is called free-air correction to the normal
gravity. Truncating the expansion at the ﬁrst order the free-air correction assumes
the following value (Blackely, 1996):
γfa(P) = ∂g
∂hhP ∼= −0.3086 mGal/m hP
(1.43)
Therefore, applying the correction to the observed gravity, the so-called free-air
gravity anomaly is retrieved as:
∆gfa(P) = go(P) −γ(ϕP) −γfa(hP)
(1.44)
Summarizing, the free-air gravity anomaly represents the gravitational eﬀect at
point P of the local density variations with respect to the density of the reference
Topography
Moho
Density
anomalies
Crust
Core
Mantle
Crust
Mantle
Core
observed gravity
normal gravity
gravity anomaly
−
=
−
=
Figure 1.8: Free-air gravity anomaly computation: the masses generating the observed gravity
signal minus the masses generating the normal gravity results in the masses generating the free-
air gravity anomaly. They are represented together with their signals in terms of gravitational
acceleration. It is possible to see how the free-air gravity anomaly highlights eﬀect of local density
variations.
25

Bayesian gravity inversion by Monte Carlo methods
ellipsoid. It is worth to notice that the gravitational attraction of the masses outside
the ellipsoid, namely the masses composing the topography, is not taken into account
into the free-air anomaly. In fact, with the normal gravity principle those masses
are considered anomalies with respect to the reference ellipsoid, with a contribution,
in terms of density contrast, equal to their density, since the background density
outside ellipsoid is null. The worldwide resulting free-air gravity anomaly and a
scheme of its causative masses are graphically shown in Figure 1.8. In practice,
the free-air gravity anomaly highlights the eﬀects of local density variations with
respect to a layered ellipsoidal Earth, thus making possible to distinguish them from
the global eﬀect due to Earth’s mean shape and rotation.
1.4.3
Bouguer correction
As previously stated, the free-air gravity anomaly does not take into account the
eﬀect due to the masses outside the reference ellipsoid, namely the masses of the
topography, bathymetry, ices, etc.
As a ﬁrst approximation, when the observation is directly taken at a ground level,
namely the height above the ellipsoid of the observation point corresponds to the
height of the topography, the masses of topography can be considered an inﬁnitely
extended slab, i.e. the so-called Bouguer slab, with a thickness corresponding to the
elevation of the observation point. Therefore, according to the gravitational eﬀect
of an inﬁnite slab, the so-called simple Bouguer correction is given by (Heiskanen
and Moritz, 1967):
gsb(P) = 2πGρChP
(1.45)
where ρC is the crustal density, typically assumed to be ρC = 2670 kg/m3. Intro-
ducing this value into Equation 1.45, it becomes (Blackely, 1996):
gsb(P) = 0.1119 mGal/m hP
(1.46)
It is worth to recall that density equal to ρC = 2670 kg/m3 is an approximation that
is valid only when the point P is located on the ground. However, it is possible to
take also observations in contact with the surface of the sea, e.g. by shipborne gravity
survey. In that case the density anomaly to be considered in Equation 1.45 is the
diﬀerence between the“true”density, namely the one of the water ρW = 1030 kg/m3,
and the density of the crust present in the reference ellipsoid, whose mean value is
usually assumed to be ρC = 2670 kg/m3, as recalled above; the thickness of the slab
is assumed to be the bathymetric depth under the point P. This last reduction can
be thought as an ideal ﬁlling of the bathymetry with crust.
A remark is that the choice of crustal density at ρC = 2670 kg/m3 is done
according to its mean value all over the Earth.
This translates in the fact that
applying the simple Bouguer correction, the crustal density anomalies with respect to
the chosen reducing density ρC are reﬂected in the ﬁnal reduced signal. Consequently,
if more information about the local geological characteristics of the crust is available,
this reference density value has to be chosen according to them, e.g. in case of oceanic
crust a good choice of the reference density could be ρC = 2900 kg/m3.
As stated before the simple Bouguer correction is an approximation. In fact, it
takes into account only the shape of the terrain just above the observation points,
26

CHAPTER 1.
THE GRAVITY FIELD AND ITS OBSERVATION
observed gravity
×105
ρ = 2670 kg/m3
ρ = 3070 kg/m3
ρ = 2970 kg/m3
60
40
20
0
9.790
9.795
9.800
−60
−40 −20
0
20
40
60
Distance [km]
Depth [km]
[mGal]
(a)
observed gravity
normal gravity
ρ = 2970 kg/m3
ρ = 2670 kg/m3
∆ρ = −400 kg/m3
∆ρ = 300 kg/m3
60
40
20
0
−1000
−500
0
−60
−40 −20
0
20
40
60
Distance [km]
Depth [km]
[mGal]
(b)
free-air anomaly
ρ = 2970 kg/m3
ρ = 2670 kg/m3
∆ρ = −400 kg/m3
∆ρ = 300 kg/m3
60
40
20
0
−200
0
200
400
−60
−40 −20
0
20
40
60
Distance [km]
Depth [km]
[mGal]
(c)
complete Bouguer
simple Bouguer
∆ρ = −400 kg/m3
∆ρ = 300 kg/m3
60
40
20
0
−150
−100
−50
0
−60
−40 −20
0
20
40
60
Distance [km]
Depth [km]
[mGal]
(d)
Figure 1.9: Example of gravity data reduction (Blackely, 1996). Starting from (a) the observed
signal, the eﬀect of the various corrections is shown: (b) normal gravity reduction, (c) free-air
correction, and (d) Bouguer correction.
without considering variations in the elevation around the point P. This translates
into the fact that valleys or mountains near the points are not correctly modelled,
thus tending to overcompensate the terrain eﬀect, as shown in Figure 1.9. To avoid
27

Bayesian gravity inversion by Monte Carlo methods
overcompensation, simple Bouguer correction should be replaced by a proper terrain
correction, also called complete Bouguer correction.
Terrain correction is obtained by modelling the gravitation signal of the topo-
graphic masses gtc of the area surrounding observation points. The topography could
be usually approximated with a digital elevation model, which gravitational attrac-
tion is computed by means of forward modelling, e.g. by means of a set of regular
prisms (Equation 1.14), point masses (Equation 1.4), tesseroids (Uieda et al, 2016),
etc. In order to speed-up the computation parallel computing and/or Fourier tech-
nique are often used (Godson and Plouﬀ, 1988; Tscherning et al, 1992; Biagi and
Sans`o, 2001; Sampietro et al, 2016) to perform this task.
The ﬁnal reduced value of gravity is called complete Bouguer gravity anomaly
and is derived as follows:
∆gb(P) = go(P) −γ(ϕP) −γfa(hP) −gtc(P)
(1.47)
1.4.4
Tidal correction
The tidal correction has to be applied to compensate the time-varying eﬀect of
the attraction caused by the sun and the moon, that can be detected by gravimeters.
The magnitude of this variation depends on the latitude of the station and is always
smaller than 0.3 mGal. In particular, it decreases with the latitude and it has a
strong periodic component, with a period of the order of 12 hours (Blackely, 1996).
Despite formulas to calculate the tidal eﬀect at any time and at any place on the
Earth exist (Longman, 1959), a common technique (especially for less accurate sur-
veys) is to empirically remove this eﬀect together with other time-dependent eﬀects
(e.g. the instrument drift) by means of repeated occupation of a point (Blackely,
1996).
1.4.5
Other eﬀects
Up to now we discussed eﬀects that should be removed during the data reduction
independently from the measurement technique and interpretation target. In fact,
according to these two factors also other reduction could be added to the reduc-
tion workﬂow shown in Figure 1.9. In particular, when the goal of the interpretative
problem is a speciﬁc crustal features or buried bodies (e.g. a salt dome), eﬀects of the
known surrounding density anomalies should be removed from the observed signal,
thus leaving into the observation only the gravitational contribution of the investi-
gated structure. Also this further reduction is performed by forward modelling, as
already discussed for terrain correction.
According to Figure 1.9, when investigating local crustal structure there is also
the big contribution of the Moho to be removed, namely the density contrast of crust
and mantle with respect to the normal reduction. In this case, the reduction can be
performed by taking into account an isostatic compensating Moho (Heiskanen and
Moritz, 1967) or by introducing a global crustal model, e.g. CRUST1.0 (Laske et al,
2013) or GEMMA (Reguzzoni and Sampietro, 2015).
An other contribution that should be removed is the eﬀect of moving platforms
in data acquisition (E¨otv¨os eﬀect), explained in Section 1.3.3.
28

Chapter 2
Gravity interpretation
Gravity interpretation consists in using observed gravity anomalies to determine
one or some characteristics of the crustal density anomalies or buried bodies (e.g. a
salt dome). Examples of the characteristics that can be retrieved are: the depth,
the shape or the (anomalous) density distribution. In other words, starting from the
link between the parameters and the data:
y = F (x)
(2.1)
the aim of the gravity interpretation is to obtain an estimate of the parameters
x for which the corresponding estimated data y agree as best as possible with
the observations. The relationship F (x) is called forward function and, in case of
gravity interpretation, it can be expressed by the well known Newton’s law (see
Section 1.1). In practice, the forward function maps the model space into the data
space. Therefore, by means of this function, the external gravity ﬁeld can be uniquely
determined for every admissible model and this operation is called forward modelling.
Nevertheless, diﬀerent models can generate the same gravity eﬀect. This implies that
the solution to the gravity interpretation problem is inherently non-unique.
The gravity interpretation problem can be faced by means of diﬀerent approaches.
They can be mainly classiﬁed into: forward methods or inverse methods. As sug-
gested by the name, the former are based on the forward modelling, that is a trial-
and-error procedure is adopted to retrieve the best values of the unknown parameters
according to the observed gravity and to the experience of the operator. On the other
hand, the latter retrieve the unknown parameters by analytically inverting the for-
ward function. In this case the non-uniqueness has to be formally solved. This task
can be in turn performed by diﬀerent approaches, all of them trying to additionally
exploit the available geological knowledge of the region.
The aim of this chapter is to brieﬂy describe the diﬀerent approaches to the
gravity interpretation. In particular, the ﬁrst section will be dedicated to the geo-
metrical approximation used to describe the model parameters, the second to a basic
description of the forward methods. As for the inverse methods, they will be treated
more in details, dedicating a section to the general concept and another section to
the Bayesian approach to inverse problems, thus introducing the fundamental idea
of the present work.
29

Bayesian gravity inversion by Monte Carlo methods
2.1
Geometrical approximation
The geometrical approximation used to describe the mass model is assumed
independently from the type of method used to solve the interpretative problem.
This choice aﬀects the shape of the forward function and the number and the kind
of parameters present in the vector x. There are three possible approximations (Fedi
and Rapolla, 1993): 3D, 2D, and 21/2D.
The 3D approximation is the one that better describes the real world. Using
this approximation, the volume of the model is ﬁnite and its geometry is variable
along the three dimensions of the space, as shown in Figure 2.1(a).
Therefore,
the forward function is derived by integrating Equation 1.9 over a three-dimensional
domain and considering the density as a function of all the three coordinates, namely
ρ = ρ(x, y, z) in a Cartesian coordinate system.
As for the 2D and 21/2D approximations, they are very similar. In both cases the
body parameters are a function only of two coordinates, namely a horizontal and a
vertical ones, and they are used to interpret gravity proﬁles, namely a set of gravity
observations acquired along the straight line deﬁning the horizontal axis. Therefore,
the assumptions performed on the model parameters are:
- the density is considered as a constant in the direction perpendicular to the
vertical plane, namely ρ = ρ(x, z), assuming that the coordinate x identiﬁes the
main horizontal direction;
- the geometry of the body is described by a surface in the x-z plane and does
not vary in the y direction, as show in Figure 2.1(b). This allows to simplify the
domain over which the integral of Equation 1.9 is computed, since it depends on
two coordinates only.
The diﬀerence between 2D and 21/2D is in the way in which the domain of this
integral is deﬁned along the y direction. In fact, in the ﬁrst case the integral domain
is assumed inﬁnite in that direction, while it is ﬁnite in the second case. These two
geometrical approximations simplify the problem from the computational point of
view, but have to be carefully managed. In fact, the choice of the main horizontal
axis direction is crucial to get a realistic approximation. A good choice is to set the
x
y
z
(a) 3D approximation
x
y
z
(b) 2D approximation
Figure 2.1: Geometrical approximation used to describe the parameters of the investigated body
in the forward function.
30

CHAPTER 2. GRAVITY INTERPRETATION
x axis laying on the shorter symmetry axis of the investigated structure when using
the 2D approximation, and on one of the symmetry axes of the structure when using
the 21/2D approximation (Fedi and Rapolla, 1993).
The choice of the kind of modellization inﬂuences the number of parameters to
be estimated in the interpretative method. Usually, the 3D modellization requires a
huge number of unknown parameters that are instead signiﬁcantly reduced in the 2D
and 21/2D cases. On the other hand, when reducing parameters additional approxi-
mations have to be introduced, thus reducing the quality of the solution too. That
is why the 21/2D case could be a good compromise between model simpliﬁcation and
number of unknown parameters, even though the 3D modellization is increasingly
chosen thanks to the increasing computational power.
2.2
Forward methods
Using forward methods the solution to the interpretative problem is found by
following a trial-and-error approach. In practice, the operator iteratively builds a
model of the investigated crustal feature or buried body(ies) and veriﬁes the consis-
tency between the modelled forward signal and the observed gravity anomaly. The
procedure is repeated until a satisfactory ﬁtting of the observed gravity anomaly is
reached, as shown in Figure 2.2. The starting model is derived from the geologi-
cal information available into the investigated region, and the operator, driven by
the observed gravity, manually modiﬁes it until a satisfactory solution is reached.
Therefore, the key point in this kind of methods is the experience and sensibility of
the operator to both geology and gravity modelling.
Gravity 
observations
Compare the model
anomaly with the
observed anomaly
Guess the initial
model parameters
(from geology)
Estimated
model
Compute
the model 
gravity anomaly
Do they 
match?
Update the model
parameters
YES
NO
Figure 2.2: Flow chart of a trial-and-error procedure.
In order to increase the number of possible trials, it is required the usage of
fast forward techniques. Examples of forward techniques can be found in literature:
Talwani and Ewing (1960) introduce an algorithm that is able to compute the eﬀect
of any body by approximating its shape with a horizontal lamina with the boundary
of an irregular polygon, while Goetze et al (1982) and G¨otze and Lahmeyer (1988)
use polyhedra to represents bodies. This last approach is more versatile, since it
allows a larger number of possible shapes. Furthermore, in the last years it becomes
31

Bayesian gravity inversion by Monte Carlo methods
reliable the usage of a set of simple shapes in building complex geometry, like point
masses, prisms (Nagy et al, 2000; Zhang and Jiang, 2017), spheres, or tesseroids
(Uieda et al, 2016), especially thanks to the available computational power.
Anyway, the usage of these methods is very time-consuming, because the space of
possible solutions is explored by using trials, and the ﬁnal result cannot be evaluated
from a statistical point of view. In fact, the obtained solution is data driven but it
largely depends on the operator choices, thus giving the best solution according to
the “experience of the operator”. Some results of trial-and-error solutions coherent
with the geological a-priori information and the gravitational signal can be found in
Oezsen (2004); Caratori Tontini et al (2009); Gordon et al (2012).
2.3
Inverse methods
Inverse methods solve the gravity interpretation problem by analytically inverting
the forward function. An important feature of this kind of solution is that they can
be also evaluated from a statistical point of view, ﬁlling a gap present in forward
methods.
The inverse theory is a general theory that can be applied in diﬀerent physical
science ﬁelds. In fact, the aim is to make inferences about physical parameters from
a set of observed data. In some ideal circumstances there is an exact theory that
explains how the data should be treated in order to recover the correct model. For
instance, if the seismic velocity inside the Earth depends only on depth, the velocity
model can be reconstructed exactly from the measurement of the arrival time as a
function of the distance of seismic waves using an Abel transform (Herglotz, 1907;
Wiechert, 1907).
Nevertheless, these exact inversion schemes holds only for few limited cases
(Snieder and Trampert, 2000). In fact, they are usually applicable only for ide-
alistic situations that may not hold in practice. and they are often very unstable.
Moreover, in many inverse problems the model that one aims to determine is a con-
tinuous function of the space variables. This means that the model has inﬁnitely
many degrees of freedom, but in a realistic experiment the amount of data that
can be used for the determination of the model is usually ﬁnite. A simple count of
variables shows that the data cannot carry suﬃcient information to determine the
model uniquely. Therefore, the model obtained from the inversion of the data does
not necessarily correspond to the true model that one seeks.
In general there are two reasons why the estimated model diﬀers from the true
model. The ﬁrst reason is the non-uniqueness of the inverse problem, that causes
several (usually inﬁnitely many) models to ﬁt the data. The second reason is that
real data are always contaminated with errors that are consequently propagated into
the estimated model.
Depending on the shape of the forward function F (x) the inverse problem can be
linear or non-linear with respect to the parameters. Nevertheless, non-linear inverse
problems are signiﬁcantly more diﬃcult than linear ones and presently no satisfactory
theory exists for them (Snieder and Trampert, 2000). However, a possible solution
is to linearize them around an approximate value of the parameters, reducing them
32

CHAPTER 2. GRAVITY INTERPRETATION
to the inverse linear problem theory. That is why in the following we brieﬂy analyse
only the solution of the linear inverse problem.
2.3.1
Linear inverse problem
The inverse problem is formalized considering the set of equations linking the
data y ∈Rm to the set of parameters x ∈Rn, applying the forward function
F(x) : Rn 7→Rm, as shown in Equation 2.1.
When the relation between data
and parameters is linear, Equation 2.1 can be written by a matrix product in the
following way:
y = Fx + ν
(2.2)
where F is the so-called forward matrix of dimensions m×n and ν is the observation
noise.
Notice that a certain arbitrariness can be used to choose the parameters
composing the vector x. Taking as an example the inverse gravimetric problem,
a possible choice is to describe the density of the investigated volume as a linear
function with respect to the depth, while another possible alternative is to discretize
the continuous density function with a given resolution. Obviously, there are other
many possibilities, but these examples show that the same problem can be modelled
with diﬀerent parameter vectors x and consequently also with diﬀerent forward
matrices F.
This means that the parameters contained in x do not necessarily
describe the real word in a perfect way, but they already introduce some restrictions
on the class of models that can be reconstructed. In the following, we assume that
the true model is represented by the ideal value of these parameters, as if they
completely represent the truth.
The inverse solution consists of retrieving an estimate ˆx of the model parameters,
starting from the observed data y. In literature, there are diﬀerent ways to deﬁne
the inverse operators (Menke, 1984; Parker, 1994; Tarantola, 2005). However, the
most general linear mapping from data to the estimated model can be written as:
ˆx = F−gy
(2.3)
where the operator F−g is the generalized inverse of the matrix F. This deﬁnition is
given due to the fact that the number of data is usually diﬀerent from the number of
parameters and therefore a formal inverse of F does not exist (Snieder and Trampert,
2000). However, for the general purposes of this section this deﬁnition is suﬃcient,
without entering into the details of the diﬀerent methods used to compute the inverse
operator.
The estimated model and the true model can be put in relationship by introduc-
ing Equation 2.2 into Equation 2.3, obtaining:
ˆx = F−gFx + F−gν = x +
 F−gF −I

x
|
{z
}
limited resolution
+
F−gν
| {z }
error propagation
(2.4)
where the operator F−gF = R is called resolution kernel.
Since the true and the estimated models have to be equal in an ideal case, namely
ˆx = x, the last two terms of Equation 2.4 represent the blurring and the artefacts
33

Bayesian gravity inversion by Monte Carlo methods
introduced in the by the estimation process. The former, namely (R −I) x, ac-
counts for the fact that the components of the estimated parameter vector are a
linear combination of the true model parameters. In practice, only an averaging of
the true parameters is retrieved, thus obtaining a blurred solution. The blurring
occurs because the mean is a smoothing operator and therefore the ﬁnest details
cannot be described. To avoid the blurring eﬀect the resolution kernel has to be
equal to an identity matrix, namely R = I that causes the vanishing of the limited
resolution term. This condition means that the inverse problem has a perfect res-
olution. Consequently, the resolution kernel can be used to understand how much
the estimated model parameters are independent from the estimation process itself.
However, a remark is that this resolution matrix does not completely explain the
relation between the estimated model and the real physical model, because it does
not take into account how the initial choice of the parameters reduces the possible
models that can be reconstructed by the inversion. In practice, it is not possible to
know how much the chosen model class is really representative of the truth.
The last term in Equation 2.4, namely F−gν, describes how the errors present
into the data are mapped into the estimated model. Since these errors are not deter-
ministically know, a statistical analysis is required, e.g. the errors in the estimated
model caused by errors in the data are evaluated through the well known covariance
propagation law. In a general case, the covariance matrix of the estimated model
Cˆxˆx can be retrieved starting from the covariance matrix of the observation noise
Cνν as:
Cˆxˆx = F−gCνν
 F−g⊺
(2.5)
Equation 2.5 shows that a model without errors is obtained only if the generalized
inverse matrix is a null matrix, leading to the (absurd) estimated model ˆx = 0. On
the other hand, this error-free solution also implies that the resolution kernel is equal
to zero. This condition is far from the ideal condition of the resolution kernel equal
to an identity matrix. Hence, a perfect resolution and no errors in the estimated
model cannot be simultaneously obtained and, in practice, an acceptable trade-oﬀ
between error-propagation and limitations in the resolution has to be found.
A ﬁnal remark is that each element of the data vector is a weighted average
of all the parameters. This lead to the fact that the matrix F act as a smoothing
operator, although the observation points are outside the causative body. Therefore,
the inverse operator F−g is an “unsmoothing” operator and may cause numerical
instability, namely small variations in the data can cause large variations in the
solution (Blackely, 1996).
2.3.2
Solution to the inverse gravimetric problem
We have already seen that inverse problems can suﬀer of non-uniqueness and
instability. The former is caused by the fact that the same gravitational signal can
be generated by diﬀerent bodies (Roy, 1962), while the latter is due to the shape of
the forward operator, that is a smoothing function.
These problems can be overcome mainly by two approaches: by introducing
very restrictive a-priori assumptions or by introducing numerical regularization. In
practice, the former acts on the choice of the parameters reducing the class of the
34

CHAPTER 2. GRAVITY INTERPRETATION
estimated models and allowing a perfect resolution of the system. The latter, instead
of reducing the number of parameters, introduces additional conditions among the
parameters of the model such that an almost perfect resolution is guaranteed.
A common restrictive hypothesis is to reduce the geometry of the model to the
presence of two layers, as shown in Figure 2.3. The two layer approximation may be
a valid approach for example when the crustal-mantle discontinuity (i.e. the Moho)
has to be inferred from the gravity observations. The notation of Figure 2.3 reﬂects
this case, denoting the density of the crust and the mantle with the symbols ρC and
ρM, respectively, and the geometry of the discontinuity surface and the topography
with D(ξ) and H(ξ), respectively.
z
0
ξ
D(ξ)
H(ξ)
ρM
ρC
Figure 2.3: Two layer model of crust and mantle. The surface D(ξ) divides the two layers of
density ρC (crust) and ρM (mantle), while the surface H(ξ) represents the topography.
Nevertheless, the restrictions in geometry are not suﬃcient and further restrictive
hypotheses have to be introduced to make the solution unique. Sampietro and Sans`o
(2012) demonstrated that the solution is unique at least in the following three cases:
- the only unknown of the problem is the density of one of the two layers depending
only on the planar coordinates, e.g. ρC(ξ), once the geometry of the discontinuity
D(ξ), the topography H(ξ) and the density of the other one, e.g. ρM(ξ, z), are
given;
- the only unknown is the geometry of the surface between the two layers D(ξ),
once the density of the two layers ρC(ξ, z) and ρM(ξ, z) and the topography H(ξ)
are ﬁxed to a known value;
- the density of the crust ρC(ξ, z) is modelled as a linearly varying function with
the depth and the only unknown is its vertical gradient, once the geometry of the
surface between the layers D(ξ), the topography H(ξ), the density of the mantle
ρM(ξ, z) and density of the crust at the topography ρC(ξ, z = H(ξ)) are given.
Another classical formulation of the inverse problem that guarantees the unique-
ness of the solution is represented by the so-called theory of ideal bodies (Parker,
1974, 1975; Dumrongchai, 2007). Here, ﬁnite bodies with known density contrast
are used and some parameters of these bodies are estimated uniquely.
However, both the theory of ideal bodies and the theorems on the two layer model
above presented may be too restrictive to produce good quality solutions, especially
in areas where complex geological structures are present. To overcome the above
35

Bayesian gravity inversion by Monte Carlo methods
mentioned limitations, various solutions to the inverse gravimetric problem exist,
but there is not one better than the others, rather there are methods more appro-
priated for some cases according to the considered geological structure. Basically,
these methods rely on some numerical regularization, that is used to remove non-
uniqueness and instability. This regularization is usually introduced in a Tikhonov
fashion (Tikhonov, 1963). Therefore, while solving the inverse problem stated in
Equation 2.2 by using a least squares approach, namely looking for the minimum
norm of residuals between observed and modelled signal:
min
x ∥y −Fx∥2
(2.6)
regularization can be introduced for instance by looking for the parameters that
minimize the hybrid norm of the gravity residuals:
min
x ∥y −Fx∥2 + ∥Γx∥2
(2.7)
where Γ is the regularizing matrix, taken as an identity matrix. However also other
conditions apart from the minimum norm of parameters can be introduced.
In
fact, by properly setting the matrix Γ, it is possible to minimize also the ﬁrst or
the second derivative of the parameters in one or more directions. Tikhonov like
regularization is largely used in literature. Considering a linear problem, namely
describing the gravitational signal of a volume by using a series of simple geometric
element (e.g. prisms, point masses, etc.) and estimating their density, the solution
is unique only when the number of elements is conveniently taken smaller than
the number of observations. Moreover, this relation is highly unstable. Therefore,
decreasing the dimensions of the elements and approaching the continuous setting,
the non-uniqueness and the instability are large, as recalled above. Then, to carry
out the solution some mathematical regularization should be introduced (Li and
Oldenburg, 1998) like approximate equality constraints (Medeiros and Silva, 1996).
On the other hand, when constraints in terms of density contrast (coming e.g. from
geological information on the area) are available, the inversion can be focused on
the geometry of discontinuity surfaces (Barbosa et al, 1997, 1999) thus facing a non-
linear problem. In this approach the depth of the discontinuity surface at each point
is the parameter to be estimated from the linearized least squares adjustment. The
solution is retrieved by introducing approximate equality constraints or weighted
smoothness constraints on depth. This kind of solution is particularly useful when
determining the basement of a relief.
Furthermore, other kinds of regularization can be introduced. For instance it is
possible to impose that some geometrical properties of the causative body are mini-
mum in the retrieved solution, like the volume of the sources (Last and Kubik, 1983)
or the moment of inertia with respect to the barycentre or to a speciﬁc axis (Guillen
and Menichetti, 1984). In order to apply those methods, an a-priori information on
the speciﬁc kind of body is required. In fact, the volume minimization avoids source
dispersion that is very useful in case of sedimentary basin, while moment of inertia
minimization is very useful in case of spheres or dipping dikes.
Recent studies have introduced some innovative inversion practice like the adaptive-
learning procedure (Silva Dias et al, 2009) where the density contrast is iteratively
36

CHAPTER 2. GRAVITY INTERPRETATION
estimated determining also the shape of the body, or the inversion by planting
anomalous densities (Uieda and Barbosa, 2012), where a growth algorithm recovers
the shape of the investigated bodies starting from some seeds with known density
contrast.
It is worth to recall that all the presented methods introduce numerical restric-
tive hypotheses. The choice of the kind of method to be used, and consequently of
the kind of constraints to be introduced, should be derived from external and inde-
pendent information about the investigated area. Usually, the required information
can be derived from geological studies or from other kinds of geophysical studies
(seismic, magnetic, etc.). However, it could happen that the numerical regulariza-
tion cannot correctly consider the geological information, therefore it is required to
supply it in a diﬀerent way.
An alternative to the above presented deterministic approach is to model both the
observations and the unknown parameters as random variables. This choice allows
to introduce the regularization and the constraints in a stochastic way, allowing a
greater ﬂexibility especially when representing complex geological structures. This
stochastic approach is usually called Bayesian approach, since its theory is based on
the well known Bayes theorem to combine the information carried by the data with
the a-priori information about the model parameters. Since the aim of this work is
to develop a solution to the gravimetric inverse problem based on this approach, it
is treated in a dedicated section.
2.4
The Bayesian approach
The Bayesian approach consists in associating a certain degree of knowledge (or a
degree of ignorance) to all the terms playing a role in the ﬁnale estimate of the inverse
problem, namely the model parameters. This degree of knowledge is described by
means of probability distribution. In other words, all the variables are considered as
random variables.
Recalling the vector of data y with dimension m and the vector of parameters
x with dimension n, the conditional distribution of the parameters given a set of
observations of the data becomes:
P(x|y) = P(y, x)
P(y)
= P(y|x) P(x)
P(y)
(2.8)
where the marginal probability of data can be computed as:
P(y) =
(R
Rn P(y|x) P(x) dx
for x contionuos
P
x P(y|x) P(x)
for x discrete
(2.9)
Equation 2.8 is known as the Bayes theorem (Bayes, 1763; Box and Tiao, 1973)
and it is of fundamental importance for the Bayesian statistics. In the following we
analyse the four terms composing the theorem:
- P(x) is the so-called prior probability distribution, or simply prior, of x and
represents the knowledge on the unknown parameters x without the contribution
of the observed data;
37

Bayesian gravity inversion by Monte Carlo methods
- P(x|y) is the so-called posterior probability distribution, or simply posterior, of
x given y and describes the new level of knowledge on the unknown parameters
x obtained introducing also the information derived from the observed data y;
- P(y|x) is the so-called likelihood and it is a function of x since the value of the
data y are observed and therefore known; the likelihood is usually deﬁned with
the symbol L(y|x) and describes the way in which the observed data modify
the a-priori knowledge on the parameters x. In practice, it is a measure of the
“degree” of ﬁt between the observed and predicted data;
- P(y) is simply a normalization constant, required to guarantee that the sum or
the integral of the posterior probability over all the possible values of x is equal
to 1 and often it is omitted.
Therefore, the Bayes theorem can be also expressed as:
P(x|y) ∝L(y|x) P(x)
(2.10)
disregarding the normalization constant. Notice that in this way only the shape of
the distribution is known, allowing to retrieve only the relative probability between
two diﬀerent sets of parameters.
The aim of the Bayesian approach is to compute the posterior probability. This
target can be achieved in two ways: by analytically expressing the posterior distri-
bution in a closed-form or, when an analytical solution is not possible, by sampling
it. The latter can be performed e.g. by means of Monte Carlo methods that are
often coupled with the Bayesian approach (Mosegaard and Tarantola, 1995, 2002).
However, also the sampling of the posterior is not always reliable, especially when
the number of parameters increases. In these cases, the Bayesian solution can be
translated into an optimization problem, by looking for instance at the maximum
a posteriori, i.e. to the set of parameters that maximizes the posterior probability.
This task can be solved by means of deterministic or stochastic optimization meth-
ods, depending on the shape of the posterior distribution, as widely discussed in
literature (Kirkpatrick et al, 1983; Geman and Geman, 1984; Azencott, 1988; Smith
and Roberts, 1993; Robert and Casella, 2004; Tarantola, 2005; Sans`o et al, 2011).
2.4.1
Prior probability
The Bayesian approach simpliﬁes and makes more ﬂexible some typical problems
that are usually present in the estimation theory; for instance it allows to simulta-
neous use continuous and discrete variables or to introduce some constraints on the
possible values of the parameters. As stated above, the prior probability is the way
in which the prior knowledge (or ignorance) on the unknown parameters x is given,
without considering the contribution of the observed data y. This is a key point in
the Bayesian approach, providing the above mentioned ﬂexibility, since through the
prior probability it becomes possible to model very diﬀerent kinds of information.
In the following we brieﬂy analyse two kinds of prior distributions: the non
informative prior and the conjugate informative prior.
The non informative prior is useful to model a completely ignorance about the
value of the parameters x. In these cases the prior probability is neutral with respect
38

CHAPTER 2. GRAVITY INTERPRETATION
to the likelihood and does not inﬂuence the posterior distribution.
In practice,
using a non informative prior, the maximum a posteriori corresponds to a maximum
likelihood estimation. Assuming x to be one-dimensional and the likelihood a data-
translated one, the latter can be expressed as (Tanner, 1991):
L(y|x) = A(y) G(S(y) , x) = A(y) G(S(y) −x)
(2.11)
where A(·) and G(·) are two generic functions and S(y) a suﬃcient statistic for x
(Box and Tiao, 1973). Equation 2.11 shows that the data y give only information
about the position of the likelihood, therefore a purely non informative prior must not
inﬂuence the position of the likelihood itself. In other words, the prior probability
should guarantee that all the values of x are equally probable.
Consequently, a
uniform distribution on R is adopted. Nevertheless, this is an improper distribution,
since the normalization condition does not hold. However, the Bayesian approach
admits the use of improper prior probabilities, provided that the integral of Equation
2.9 is deﬁnite, namely:
P(y) < ∞
(2.12)
More in general, it may be diﬃcult to write the likelihood in a data-translated
format. In this case the Jeﬀreys rule (Jeﬀreys, 1961) can be used. For more details
see e.g. Tanner (1991).
The conjugate prior is useful to derive the analytical expression of the posterior
distribution. In fact, if the posterior distribution P(x|y) is in the same family of the
prior probability distribution P(x), the prior and posterior are then called conjugate
distributions, and the prior is called a conjugate prior for the likelihood function
(Tanner, 1991; Gelman et al, 1995). For example, the Gaussian family is conjugate
to itself (or self-conjugate) with respect to a Gaussian likelihood function: if the
likelihood function is Gaussian, choosing a Gaussian prior over the mean will ensure
that the posterior distribution is Gaussian too. Conjugate priors have at least two
big practical advantages: simpliﬁcation of the posterior computation and easiness in
data interpretation, since the posterior probability has an analytical expression.
A remark is that the deterministic approaches, explained in the previous section,
can be seen as a particular case of the Bayesian approach, namely the case in which
a non informative prior is used. However, the Bayesian approach introduces also
the possibility to “weight” the estimation driven by the data, by means of the prior
probability on the parameters.
2.4.2
The Bayesian approach in gravity inversion
As in the case of the determistic approaches, when the Bayesian approach is
applied to the inverse gravimetric problem there are several solutions that basically
diﬀer in the way in which the prior probability is deﬁned. The main idea is to use
the Bayesian approach to model the available geological knowledge in terms of prior
probability (Tarantola and Valette, 1982; Mosegaard and Tarantola, 1995).
A ﬁrst choice can be to only retrieve the shape of one or more bodies, by assuming
their density as known. In this case the prior probability is used to model the partial
knowledge on the shape of the body that can be obtained from seismic prospecting
39

Bayesian gravity inversion by Monte Carlo methods
or geological information. For instance this is the case of salt structures in the crust
(Nagihara and Hall, 2001; Roy et al, 2005).
A second alternative is to use the Bayesian approach to estimate both the ge-
ometry and the density spatial distribution into the investigated volume. This can
be done by introducing a set of parameters that are used to describe the spatial
distribution of the geological materials (i.e. a label) present into the volume in ad-
dition to the variables that already describe the physical properties (Bosch, 1999).
The idea is to use the prior probability to model also the relationship between these
two kinds of variables together with the geological knowledge about physical and
geometrical characteristics of the investigated region. This approach is also a good
starting point to introduce more than one kind of geophysical observations (Bosch,
1999). Some examples of Bayesian inverse solutions, obtained by means of this ap-
proach, can be found in Bosch and McGaughey (2001), Calcagno et al (2008), and
Guillen et al (2008). The main diﬀerence among these methods is the way in which
the prior probability is formalized, thus inﬂuencing the shape of the posterior. The
latter is sampled or maximized by means of Monte Carlo methods or random walks
(Mosegaard and Tarantola, 1995), that are typical tools for the Bayesian inference.
In the present work a gravity inversion algorithm based on the Bayesian approach
is developed. The inversion is performed by using the 3D geometrical approxima-
tion and the investigated volume is discretized by means of volume units (voxels).
Similarly to the above cited works, the chosen parameters for each voxel are the
physical quantity inﬂuencing the forward relationship (i.e. the density) and a label
deﬁning the geometry of the model. In this framework, a big eﬀort has been em-
ployed to analytically express the shape of the posterior probability. This target
has been reached by carefully designing the prior probability, especially thanks to
the assumption that the labels are a Markov Random Field. This last choice is
crucial, since it allows to describe only the correlation among neighbour labels with-
out looking at the model geometry as a whole. This approach is mutated from the
image analysis (Geman and Geman, 1984). The information about the regularity
of the boundaries between diﬀerent classes is introduced into the prior probability
through the above mentioned correlation between neighbour labels. The estimate
of the parameters is retrieved by invoking the maximum a posteriori principle. The
optimization of the posterior is performed by means of Markov Chain Monte Carlo
methods, that well couple with the Bayesian approach. In particular, the chosen al-
gorithm is the simulated annealing aided by a Gibbs sampler. The latter is preferred
to the classical Metropolis algorithm (Metropolis et al, 1953) because the Markov
Random Field modelling of the labels allows to analytically express the posterior
probability conditional distributions.
The fundamental idea of this work and a very preliminary formalization of the
problem were already discussed in Rossi et al (2015). The aim of this thesis is to
deeply expand this published paper, by formally deﬁning all the terms composing
the posterior probability and by developing a software that makes the proposed
algorithm practically applicable.
40

Chapter 3
Optimization by Monte Carlo
methods
As already introduced in the previous chapter, when treating inverse problems
the optimization of a target function is a very common task. In particular, there
are at least two interesting cases of optimization: the evaluation of the maximum
likelihood or, more in general in the Bayesian statistics, the computation of the
maximum a posteriori and the least squares. Practically, all the inverse methods
presented in Section 2.3 rely on the solution of one of these optimization problems.
In the former case, starting from a vector of data y related to a vector of param-
eters x and by introducing the likelihood distribution L(y|x), the optimal solution
is the set of parameters that maximize the likelihood. When working in a Bayesian
framework also a prior probability distribution P(x) of the parameters x is given and
multiplied by the likelihood provides the posterior probability P(x|y) (see Equation
2.10). In this case the inverse solution can be retrieved by applying optimization
techniques, looking for the maximum posteriori (MAP) of P(x|y) over a limited
domain Q, namely:
max
x∈Q
L(y|x) P(x)
R
Q L(y|x) P(x) dx
(3.1)
Notice that when the prior probability P(x) tends to be non informative with respect
to x, maximizing L(y|x) or P(x|y) leads to the same result (see Section 2.4.1),
therefore the maximum likelihood solution can be also seen as a particular case of
the more general Bayesian theory.
As for the least squares approach, this method aims to ﬁnd the minimum of:
min
x∈Q[y −F(x)]⊺Q−1[y −F(x)]
(3.2)
where F(x) is the forward relationship, as already shown in Equation 2.1, and Q
is the cofactor matrix of the observations, proportional to their accuracy. In this
case, the optimization, namely the research of the minimum, can be more or less
complicated depending on the shape of the function F(x) and on the presence of
one or more minimum solutions. For example, when F(x) is linear, namely the
relationship can be represented through a product between a matrix and a vector of
parameters (see Equation 2.2), Equation 3.2 admits only one solution if this matrix
is not rank deﬁcient (Koch, 2013).
41

Bayesian gravity inversion by Monte Carlo methods
The outcome is that solving inverse problems often requires to perform the opti-
mization of a target function, deﬁning the optimization problem as the minimization
of a target function into the closed bounded domain Q, that is:
min
x∈Q E(x) = E(ˆx)
(3.3)
The opposite problem of maximizing a function Γ(x) can be always treated, by
assuming that E(x) = −Γ(x). It could happen that the research of the minimum
in Equation 3.3 should be extended to the full Rn, where n is the dimension of the
vector of parameters x. In this case, it is often suﬃcient to understand the behaviour
of E(x) when |x| →∞to deﬁne a reasonable limited Q such that:
min
x∈Q E(x) ≡min
x∈Rn E(x)
(3.4)
In the following we suppose that this space limitation is already taken into account
and that Q is large enough to be sure that the minimum is inside Q and not on its
boundaries and that the function is smooth, namely its ﬁrst and second derivatives
exist and are continuous.
Literature provides several optimization algorithms that can be subdivided into
deterministic or stochastic ones. The former include the Newton’s method, relax-
ation algorithms, steepest descent, conjugate gradient, linear programming methods,
the simplex algorithm, etc. (Tanner, 1991; Boyd and Vandenberghe, 2004; Tarantola,
2005), while the latter include algorithms like the simulated annealing (Kirkpatrick
et al, 1983), random search (Zhigljavsky, 1991), cross-entropy method (Rubinstein
and Kroese, 2004), etc. Stochastic algorithms relies on sampling procedures; in that
framework, Monte Carlo Markov Chains (MCMC) methods could be used. Among
them, the most common are the Metropolis algorithm and its modiﬁed versions
(Metropolis et al, 1953; Hastings, 1970) and the Gibbs sampler (Geman and Ge-
man, 1984).
In the proposed Bayesian inversion solution the main optimization algorithm
used will be the simulated annealing. As recalled above, it belongs to the class of
stochastic algorithms and in particular it is a MCMC methods. Consequently, during
its evolution it is required to draw samples from a probability distribution by visiting
a Markov chain. Moreover, a subset of the problem parameters will be described
as a Markov Random Field (MRF), then the required sampling procedure will be
performed by means of the Gibbs sampler. In fact, it is particularly suitable for
the optimization of a MRF, although it can be used each time that the conditional
distributions of a multivariate probability distribution are known.
The following sections explain the basics of the theory behind the simulated an-
nealing, the Gibbs sampler and the principal properties of the MRF. Finally, a short
description of a deterministic optimization algorithm, namely the simple relaxation,
will be performed, since it will be used to reﬁne the stochastic optimization in the
proposed solution algorithm.
42

CHAPTER 3.
OPTIMIZATION BY MONTE CARLO METHODS
3.1
Simulated annealing
The simulated annealing algorithm was ﬁrstly introduced by Metropolis et al
(1953) with the aim of minimizing a target function on a ﬁnite set with a very large
size. However it can also be applied to the optimization on a continuous set and
to simulation (Kirkpatrick et al, 1983; Ackley et al, 1985; Aldous, 1987; Azencott,
1988; Aldous, 1990; Neal, 1993, 1998; Robert and Casella, 2004) .
The fundamental idea of the simulated annealing method is that a change of scale
allows for faster moves on the surface of the target function E(x) to be minimized.
Therefore, rescaling avoids, or better limits, the trapping attraction of the local
minima.
This algorithm uses an analogy among the process of the physical annealing
of a metal and the mathematical problem of obtaining the global minimum of a
function that may also have local minima (metastable states). This analogy leads to
borrow the terminology from the statistical mechanics, where the target function is
assimilated to the energy E(x) and the scale factor to the temperature T controlling
the process. The family of distributions governing the simulated annealing process
has the shape of a Gibbs distribution, namely:
PT(x) = 1
AT
exp

−1
T E(x)

(3.5)
where the constants AT is computed by applying the normalization condition, as:
AT =
Z
Q
exp

−1
T E(x)

dx
(3.6)
The full process is based on the study of the impact of the temperature T into the
family of distributions of Equation 3.5. In particular, it can be noticed that the
eﬀect of a high value of temperature is to ﬂat the distribution PT(x), resulting into
a uniform distribution:
lim
T→∞PT(x) =
(
1
µ(Q)
x ∈Q
0
x /∈Q
(3.7)
where µ(Q) is the measure of the set Q over which the research is concentrated.
On the other hand, reducing the temperature the maxima of PT(x), corresponding
to the minima of E(x), are emphasized. Moreover, when T →0 the distribution
becomes an impulsive one, highlighting the optimal value of the parameters ˆx:
lim
T→0 PT(x) ∼= δ(x −ˆx)
(3.8)
where δ(x) is the Dirac delta function. Therefore, starting from Equation 3.8 and
choosing a family of random variables XT with the distribution PT(x) shown in
Equation 3.5, when T →0 the following relationship holds (Pincus, 1968, 1970;
Sans`o et al, 2011):
lim
T→0 E[XT] = ˆx
(3.9)
where the point ˆx is the global minimum of the function E(x).
43

Bayesian gravity inversion by Monte Carlo methods
The results of Equation 3.9 can be demonstrated by starting from the deﬁnition
of the mean of a random variable, namely:
E[XT] = φ(T) =
Z
Q
xPT(x) dx =
R
Q x exp

−1
T E(x)
	
dx
R
Q exp

−1
T E(x)
	
dx
(3.10)
and by computing the following limit:
lim
T→0∥φ(T) −ˆx∥
(3.11)
Introducing the mean deﬁnition, shown in Equation 3.10, into the limit presented in
Equation 3.11, the latter becomes:
lim
T→0∥φ(T) −ˆx∥= lim
T→0∥E[XT] −ˆx∥=
= lim
T→0∥E[XT −ˆx]∥= lim
T→0

R
Q(x −ˆx) exp

−1
T E(x)
	
dx
R
Q exp

−1
T E(x)
	
dx

(3.12)
Now, expanding the limit of Equation 3.12, it can be proved that for an arbitrary
value ε the following relationship holds (Pincus, 1968, 1970; Sans`o et al, 2011):
lim
T→0∥φ(T) −ˆx∥< ε
⇒
lim
T→0 E[XT] = ˆx
(3.13)
The result of Equation 3.13 seems to suggest an easy way to obtain an approx-
imate value of ˆx: to sample the distribution PT(x) with a very small value of the
temperature and then obtain the optimal point by computing the mean of the drawn
samples. Nevertheless, sampling this distribution is not an aﬀordable task. In fact,
when the temperature is very low the distribution PT(x) is very close to an impulsive
distribution, and the majority of sampling methods, e.g. acceptance-rejection, grid
sampler, etc., fails in this case. For instance, if the acceptance-rejection method is
used, the majority of the samples will be rejected, then making impossible to retrieve
a good set of samples.
Therefore, the solution introduced in the algorithm is mutated from the statis-
tical mechanics, as recalled above, by simulating the annealing process of a metal.
Practically, the samples from the distribution PT(x) are ﬁrstly drawn assuming a
high value of the temperature T. Then the system is slowly frozen, thus allowing
the sampling algorithm to correctly approaches the maximum density peak of the
probability distribution, namely the minimum of E(x). A common choice in the
sampling procedure used at each step of the simulated annealing is to perform it
by means of a Markov chain sampling, like the Metropolis algorithm or the Gibbs
sampler, by assuming PT(x) as the limit distribution of the chain. Summarizing, in
the practice, the simulated annealing algorithm assumes the following structure:
1. ﬁx the highest value of temperature and start drawing samples, e.g. by means of
a Markov chain;
2. compute the mean of the drawn samples, according to Equation 3.13;
3. decrease the temperature and draw a new chain of samples;
44

CHAPTER 3.
OPTIMIZATION BY MONTE CARLO METHODS
4. iterate the procedure of 1. and 2., decreasing the temperature until its smallest
value is reached and the condition T →0 is achieved;
5. assume the mean of the samples of the last chain as the optimal point ˆx mini-
mizing the function E(x);
6. (optional) reﬁne the estimate of ˆx by means of a deterministic optimization
algorithm.
The great advantage of performing the sampling procedure is that for each tem-
perature value, the chain is initialized at the mean of the previous one. In practice,
this particular operation helps the drawn samples to be attracted from the global
minimum and allows to correctly sampling the last peaked probability distribution,
since the peak is slowly approached in the previous states of the simulated anneal-
ing. Practically, the simulated annealing build a path to reach the optimum state
without evaluating the target function exhaustively, since this operation is not usu-
ally possible from a computation point of view. Notice that, if the temperature is
very slowly decreased, the algorithm can be applied by directly sampling only one
Markov chain, in which the temperature is varied at each iteration. This operation
speeds up the process, but could reduce the quality of the convergence.
As for the sampling at each iteration of the simulated annealing, it can be per-
formed by Markov chains method like the Metropolis algorithm (Metropolis et al,
1953; Hastings, 1970) or by the Gibbs sampler. The latter in particular is very use-
ful when working with a Markov Random Field (MRF) and will be explained in the
next section since it is the one used in the proposed Bayesian inversion algorithm.
3.2
Gibbs sampler
The Gibbs sampler is a MCMC algorithm to obtain a sequence of samples from
a multivariate probability distribution, when its direct sampling is diﬃcult. This
algorithm is described by Geman and Geman (1984). Its characteristic is that the
transition matrix of the Markov chain is designed from the conditional probability
distributions P(xi|x−i) of each variable xi. Therefore, the Gibbs sampler is appli-
cable when the joint distribution P(x) is not known explicitly or it is diﬃcult to
be directly sampled, but the conditional distributions P(xi|x−i) are known and can
be easily sampled. In fact, the Gibbs sampler algorithm is an update routine which
draw a sample from the conditional distribution of a variable xi at each iteration,
given the current values of all the other variables. It can be shown that this sequence
of samples constitutes a Markov chain, and that the stationary distribution of that
Markov chain is exactly the expected joint distribution P(x).
Notice that the sampling algorithm is valid when the P(x) distribution satisﬁes
the strictly positivity condition on the manifold Q, namely (Robert and Casella,
2004):
P(x) > 0,
∀x ∈Q
(3.14)
since this guarantees to build an aperiodic and irreducible Markov chain. In fact,
only when this last condition is satisﬁed, the joint distribution can be described by
means of its conditional distributions. Therefore, assuming a n-dimensional random
45

Bayesian gravity inversion by Monte Carlo methods
variable X, such that:
X =


X1
X2
...
Xn


(3.15)
and starting from the conditional probability distribution of a variable given all the
others, namely:
P(xi|x−i) = Pi
 xi|xi−1
1
, xn
i+1

(3.16)
where (x−1)⊺=

xi−1
1
xn
i+1

represent the vector x⊺without the element i, it can be
proved that the joint distribution can be reckoned, according to Hammersley and
Cliﬀord (Hammersley, 1974; Moussouris, 1974; Robert and Casella, 2004), as:
P(x)
P(y) =
n
Y
i=1
Pi
 xi|xi−1
1
, yn
i+1

Pi
 yi|yi−1
1
, xn
i+1

(3.17)
In fact, by maintaining the variable y ﬁxed the right term of Equation 3.17 is propor-
tional to the joint distribution P(x), that can be determined by the normalization
condition.
For sake of simplicity Equation 3.17 will be demonstrated for n = 3. It is worth
to remember that to verify the condition shown in Equation 3.14 also the marginal
conditional probabilities have to be strictly positive deﬁnite. Moreover, also the
following relationship holds:
Pi
 xi|xi−1
1
, yn
i+1

Pi
 yi|yi−1
1
, xn
i+1
 = P
 xi−1
1
, xi, yn
i+1

P
 xi−1
1
, xi, yn
i+1

(3.18)
which allows to express the product present in Equation 3.17 as:
P(x1, y2, y3) P(x1, x2, y3) P(x1, y2, x3)
P(y1, x2, x3) P(y1, y2, x3) P(y1, y2, y3) = P(x)
P(y)
(3.19)
It can be observed that if the conditional distributions P(xi|x−i) are strictly pos-
itive, also the joint distribution computed in Equation 3.17 will be strictly positive.
Therefore, starting from Equation 3.19 and deﬁning the transition from the time t
to the time t + 1, namely:
X(t) = ξ →X(t+1) = η
(3.20)
performed by sampling the following probability distribution:
X(t+1)
i
∼P
 ηi|ηi−1
1
, ξn
i+1

(3.21)
for i = 1, 2, . . . , n, a homogeneous Markov chain can be generated. According to the
sampling procedure, the elements of the associated transition matrix are deﬁned as:
Pξη = P
 X(t+1) = η|X(t) = ξ

=
n
Y
i=1
P
 ηi|ηi−1
1
, ξn
i+1

(3.22)
46

CHAPTER 3.
OPTIMIZATION BY MONTE CARLO METHODS
It can be demonstrated that a chain built by using the transition matrix described in
Equation 3.22 converges to the joint probability P(x). In fact, recalling the deﬁnition
of the asymptotic distribution of a Markov chain, the following relationship holds at
the equilibrium:
P(η) =
X
ξ
PξηP(ξ)
(3.23)
To show that the conditional distributions satisfy the equilibrium condition, we use a
state vector of dimension n = 3 and discrete variables for sake of simplicity, although
the same proof can be extended to every ﬁnite value of n and to the continuous case.
Therefore, starting from Equation 3.23, we obtain:
P(η) =
X
ξ
PξηP(ξ) =
=
X
ξ3
X
ξ2
X
ξ1
P(η3|η1, η2) P(η2|η1, ξ3) P(η1|ξ2, ξ3) P(ξ1, ξ2, ξ3) =
= P(η3|η1, η2)
X
ξ3
X
ξ2
P(η2|η1, ξ3) P(η1|ξ2, ξ3) P(ξ2, ξ3) =
= P(η3|η1, η2)
X
ξ3
P(η2|η1, ξ3)
X
ξ2
P(η1, ξ2, ξ3) =
= P(η3|η1, η2)
X
ξ3
P(η2|η1, ξ3) P(η1, ξ3) =
= P(η3|η1, η2) P(η1, η2) = P(η1, η2, η3)
(3.24)
thus verifying the equilibrium condition and demonstrating the possibility to sample
a joint distribution from its conditional ones. A remark is that, thanks to this algo-
rithm a multivariate distribution can be sampled without knowing its normalization
constant. In fact, the knowledge of this constant can be often a numerical task that
is hardly solvable.
Summarizing, the Gibbs sampler can be applied when an explicit formulation of
the conditional probabilities exists. Then, to draw samples from the joint probability
P(x), the algorithm presents the following structure:
1. initialize the vector x with random values in the sampling domain Q (notice that
the starting point is not relevant);
2. draw a sample of the ﬁrst variable from its conditional distribution P(xi|x−i),
assuming the values of the conditioning variables at their current realization;
3. upgrade the realization of x with the drawn sample;
4. move to the next variable and draw a new sample from P(xi|x−i), using the
previously updated realization of x for the conditioning variables;
5. upgrade the realization of x with the drawn sample and repeat the point 4. and
5. for all the n variables;
6. given the new realization of x come back to the point 2. and iterate the method
to build a chain;
7. at the end of the chain, each realization of x represent a sample drawn from its
joint probability distribution P(x).
47

Bayesian gravity inversion by Monte Carlo methods
A remark is that the Gibbs sampler is a MCMC algorithm, then it generates a
Markov chain of samples. Therefore, each of these samples is correlated with the
nearby ones. As a result, when independent samples are required a good practice is
to thin the resulting chain of samples by only taking every kth value, e.g. every 100th
values. In addition, samples from the beginning of the chain may not accurately
represent the desired distribution, since this is the so-called burn-in period.
3.3
Markov random ﬁelds
Markov Random Fields (MRF) extend the theory of Markov chains to Rn, oth-
erwise valid only in R.
The stochastic ﬁeld is identiﬁed by a vector X which elements Xi are random
variables associated to the knots of a ﬁnite lattice Λ. In this introduction, we consider
only the case Λ ⊂R2, but the theory can be extended to Rn.
The knots of the lattice are distributed over a regular grid and, assuming unitary
grid step, each knot will be identiﬁed by a couple of integer indexes:
i = [i1, i2]⊺
(3.25)
where i1, i2 can assumes values from 1 to n1, n2, respectively. Moreover, it is nec-
essary a function to deﬁne the relationship between the index i of the stochastic
variable and its position in the lattice, namely i ↔i. For instance, this transforma-
tion could be implemented as:
i = i1 + (i2 −1) n1
(3.26)



i2 =
i −1
n1

+ 1
i1 = i −(i2 −1) n1
(3.27)
where ⌊·⌋is the ﬂoor operator. An example is shown in Figure 3.1, where the lattice
and its knot are represented.
Λ
i1
i2
i = [3, 4]⊺→i = 21
Figure 3.1: Lattice and correspondence between single the index i and the double indexes i1, i2.
48

CHAPTER 3.
OPTIMIZATION BY MONTE CARLO METHODS
Furthermore, also a neighbourhood system ∆i can be deﬁned in Λ, as a family
of subsets for which the following relationship holds:
(
i /∈∆i
j ∈∆i ⇔i ∈∆j
(3.28)
For instance, the neighbour set ∆i could be deﬁned as following (Geman and Geman,
1984):
∆i =

(j1, j2) ∈Λ : 0 <

(i1 −j1)2 + (i2 −j2)2
≤c
	
(3.29)
By setting the value of c, the dimension of the neighbour changes. In particular, to
obtain the nearest neighbour knots of i, it can be assumed c = 1 or c = 2, obtaining
the shapes of ∆i shown in Figures 3.2(a) and 3.2(b), respectively.
This kind of
neighbour sets represents the physical concept of the nearest neighbour, since only
the closest knots with respect to the central one are considered at each set.
i
(a)
i
(b)
Figure 3.2: Two kinds of neighbour of a knot i obtained by setting: a) c = 1 and b) c = 2.
Moreover, a subset C of Λ, composed by a single element or by more than
one element with the condition that each pair of elements of C are neighbour, can
be deﬁned. This subset is called clique and the set of all the possible cliques in
the lattice will be identiﬁed with C, hereafter. When the neighbourhood system is
deﬁned assuming c = 1, only cliques of order 1 or 2 are possible, as shown in Figure
3.3(a). On the other hand, when extending the size of the neighbour by assuming
c = 2, also cliques of order 3 and 4 become possible, thus allowing both the ones of
Figure 3.3(a) and of Figure 3.3(b).
For each clique it is also possible to deﬁne a potential, namely a function of the
ﬁeld X, VC(x) = VC(xC), depending only on the values of the variables composing
a clique C. The Gibbs energy of the ﬁeld is a function E(x) that can be expressed
as the sum of clique potentials, that is:
E(x) =
X
C∈C
VC(xC)
(3.30)
The key point in a MRF is that its joint probability distribution assumes always the
shape of a Gibbs distribution, namely:
P(x) = 1
Z exp{−λE(x)}
(3.31)
49

Bayesian gravity inversion by Monte Carlo methods
Clique of order 1
Cliques of order 2
(a)
Cliques of order 2
Cliques of order 3
Clique of order 4
(b)
Figure 3.3: Possible cliques of the neighbourhood ∆i of: (a) Figure 3.2(a) and (a + b) Figure
3.2(b).
where Z is the so-called partition function, namely in case of discrete variables:
Z =
X
x
exp{−λE(x)}
(3.32)
where, according to the mechanical statistics, E(x) represents the energy and λ
the inverse of the temperature T. The shape assumed by the energy E(x), shown in
Equation 3.30, makes the Gibbs sampler very suitable in the sampling of such a ﬁeld.
In fact, the conditional distribution of a variable xi depends only on the variables
composing its neighbour ∆i, thus allowing to very easily retrieve its shape. This can
be demonstrated starting from the deﬁnition of conditional probability. For sake of
simplicity this characteristic is demonstrated for discrete random variables, but it
can be easily extended to the continuous case:
P(xi|x−i) =
P(xi, x−i)
P
xi P(xi, x−i) =
=
exp

−λ P
C:i∈C VC(xC) −λ P
C:i/∈C VC(xC)
	
P
xi exp

−λ P
C:i∈C VC(xC) −λ P
C:i/∈C VC(xC)
	 =
=
exp

−λ P
C:i∈C VC(xC)
	
P
xi exp

−λ P
C:i∈C VC(xC)
	 = h(xi, x∆i)
(3.33)
The last equality of Equation 3.33 derives from the fact that:
i ∈C ⇒C ⊂(∆i ∪i) ⇒VC(xC) = f(xi, x∆i)
(3.34)
The conclusion is that the result of Equation 3.33 shows that the neighbour condition
presented in Equation 3.28 is satisﬁed in case of a Gibbs distribution. This conclusion
will be useful afterwards, when developing the inversion method.
50

CHAPTER 3.
OPTIMIZATION BY MONTE CARLO METHODS
A ﬁnal remark is that the Gibbs distribution representing the joint probability of
a MRF is the same used in the simulated annealing (see Equation 3.5). Therefore,
being able to sample a Gibbs distribution and consequently a MRF is equivalent to
be able to minimize E(x).
3.4
Deterministic optimization
Deterministic optimization methods allow to ﬁnd the minimum of convex func-
tions, in a convex domain. A function f(x) is deﬁned as a convex one into a convex
domain Q when the following condition holds:
∀λ ∈[0, 1] , ∀x0, x1 ∈Q : f(λx1 + (1 −λ) x0) ≤λf(x1) + (1 −λ) f(x0)
(3.35)
Moreover, the function is deﬁned strictly convex when the inequality of Equation
3.35 becomes a strict inequality ∀λ : 0 < λ < 1
In the following we focus on the simple relaxation algorithm, because of its
similarity with the Gibbs sampler. This similarity makes it a good deterministic
method to reﬁne the solution of a stochastic optimization by simulated annealing
performed by a Gibbs sampler. For sake of simplicity only the two-dimensional case
is analysed, but again this method can be extended to any ﬁnite number of variables.
As already stated, the aim of the method is to minimize a function in R2, namely
to ﬁnd the point for which the following condition holds:
min
x∈Q E(x, y)
(3.36)
An arbitrary point is required to initialize the algorithm, under the only condition
that this point is inside the domain Q, namely:
(x0, y0) ∈Q
(3.37)
Then, the solution is retrieved by iterating from the step i to the step i + 1 the
following scheme:



xi+1 = arg min
x∈Q E(x, yi)
yi+1 = arg min
y∈Q E(xi+1, y)
(3.38)
whose resulting path can be seen in Figure 3.4. This scheme allows to perform a
mono-dimensional optimization at each step that has to be performed by an analyt-
ical method or by other deterministic optimization algorithm, e.g Newton’s method.
The convergence of the method can be demonstrated if the function is convex
into a convex domain (Boyd and Vandenberghe, 2004; Sans`o et al, 2011). However,
the convergence of the method does not guarantee its eﬃciency. In fact, this depends
on how the function is oriented with respect to the axes. In Figure 3.5 two examples
for which the convergence speed is very diﬀerent are depicted. Both the examples
show a function with elliptical contour lines, but in the ﬁrst case, see Figure 3.5(a),
the axes of the ellipses are parallel to the Cartesian reference frame and an exact
convergence is reached in only two steps. On the other hand, in the second case, see
51

Bayesian gravity inversion by Monte Carlo methods
Q
x
y
(x0, y0)
(x1, y0)
(x1, y1)
(x2, y1)
Figure 3.4: Path followed by the simple relaxation algorithm.
x
y (x0, y0)
(x1, y0)
(x1, y1) ≡(ˆx, ˆy)
(a)
x
y
(ˆx, ˆy)
(x0, y0)
(xn, yn)
(b)
Figure 3.5: Contour lines of elliptical shape functions, showing the path followed to reach the
minimum.
Figure 3.5(b), the function is 45° oriented with respect to the Cartesian axes and
the convergence results very slow.
It is worth to notice that literature provides also more eﬃcient methods, e.g. the
steepest descent algorithm that looks very similar to the simple relaxation but
with an increased convergence speed, due to the fact that at each step the mono-
dimensional optimization is performed along the direction in which the function
E(x) has the maximum slope. Nevertheless, the analogy of the simple relaxation
with respect to the Gibbs sampler makes this method very easy to be applied after a
stochastic optimization by simulated annealing aided by the Gibbs sampler. In fact,
while in the stochastic method the target function is iteratively sampled along each
direction, in the deterministic method the minimum is iteratively found along each
direction. This means that, from a practical point of view, it is possible to follow
the same algorithm by replacing the sampling procedure with the mono-dimensional
optimization.
A remark is that when applying a deterministic method to reﬁne the result of a
52

CHAPTER 3.
OPTIMIZATION BY MONTE CARLO METHODS
stochastic optimization and when the original function is not convex, it is usually
suﬃcient to reduce the domain around the approximate solution to guarantee the
convexity condition, thus allowing the deterministic improvement of the stochastic
solution.
53

54

Chapter 4
The Bayesian inversion algorithm
In the current chapter we investigate and discuss the methodological approach
used in the proposed Bayesian solution. As already introduced in Section 2.4.2, the
method aims to implement a 3D inverse solution to retrieve both the density and
geometry inside the investigated volume. Due to the huge number of parameters
generated by this choice, the solution to the inverse problem is retrieved by invoking
the MAP principle, thus leading to an optimization problem. In fact, the huge num-
ber of parameters makes practically not feasible to evaluate the posterior probability
distribution by means of a sampling algorithm, as usual in the Bayesian approach.
The optimization is performed by means of a stochastic algorithm, namely the sim-
ulated annealing aided by Gibbs sampler. This choice allows to easily introduce
continuous and discrete variables simultaneously, as well as to forget about an ap-
proximate solution to initialize the optimization algorithm, whatever is the shape of
the posterior probability.
In the following sections the Bayesian algorithm is presented from the theoretical
point of view. In particular, we start explaining the problem setting (Section 4.1),
namely which model parameters are chosen and their spatial distribution. Then, the
posterior probability is derived, by deﬁning the prior and the likelihood distributions
(4.2). After that, the optimization task is analysed, retrieving the probability distri-
bution used in the simulated annealing algorithm and its conditional distributions,
required to apply the Gibbs sampler (Section 4.3). However, the optimization task
allows to ﬁnd the MAP solution, but does not retrieve any information about its
accuracy. Therefore, possible ways to get also this information are discussed in a
general form (Section 4.4). The chapter is concluded by a discussion introducing the
problem of adding constraints to the solution (Section 4.5).
4.1
Problem setting
The investigated volume is discretized by means of a set of voxels. Voxel trans-
lates in the 3D geometry the concept of pixel, namely they are the smallest unit
constituting the full investigated volume. Analogously to pixels, voxel position is
represented by means of its index i on a 3D grid, as shown in Figure 4.1. Alterna-
tively a vector of three indexes i = [i1, i2, i3]⊺can be used. Knowing the number
of voxels n1, n2, n3 along the three dimensions, it is possible to deﬁne a univocal
55

Bayesian gravity inversion by Monte Carlo methods
transformation i ↔i1, i2, i3. Letting i1, i2, i3 assume values ranging from 1 to n1, n2,
n3, respectively, the chosen transformation is represented by the following equations:
i = i1 + (i2 −1) n1 + (i3 −1) n1n2
(4.1)













i3 =
i −1
n1n2

+ 1
i2 =
i −1
n2
−(i3 −1) n3

+ 1
i1 = i −(i2 −1) n1 −(i3 −1) n1n2
(4.2)
where ⌊·⌋represents the ﬂoor operation.
Figure 4.1: Grid used in voxel deﬁnition.
In the proposed inversion algorithm each voxel i is characterized by a couple
of random variables: the density ρi and the label Li. The density is a continuous
random variable and it is considered a constant inside the volume associated to
the voxel.
The label is a discrete random variable and it is used to deﬁne the
material present inside the voxel itself. It can assume a ﬁnite set of M possible
values {1, 2, . . . , M} denoting the diﬀerent possible materials present in the region,
e.g. water, sediments, salt, etc. The full set of densities and labels deﬁnes the vector
of unknown parameters x of the problem:
x =

ρ1, ρ2, . . . , ρn, L1, L2, . . . , Ln
⊺=
ρ
L

(4.3)
The regular grid allows to assume that the labels are a MRF (see Section 3.3).
Therefore, deﬁning the lattice of voxels I = {i : 1, 2, . . . , n} at all the knots of the
grid, we deﬁne also the neighbourhood system in I as the family of the subsets ∆i
of I with the following shape:
56

CHAPTER 4. THE BAYESIAN INVERSION ALGORITHM
∆i = {(j1, j2, j3) ∈I : 0 < max[|i1 −j1| , |i2 −j2| , |i3 −j3|] ≤c}
(4.4)
where c deﬁne the size. Notice that voxels at or near the boundary have less neigh-
bours than the interior ones; this is the so-called “free boundary” (Geman and Ge-
man, 1984) and in theory they could be avoid by using periodic boundaries (e.g. a
circle in a 1D space, a torus in a 2D space, etc.), but this is not physical when
isolating only a local volume of the subsurface. This neighbourhood respects the
conditions shown in Equation 3.28 that is useful assume the labels as a MRF.
Moreover, each voxel i has to be also associated to a position vector ri, in order
to perform the forward modelling. The position is referred to the barycentre of a
simple body used to model the contribution of each voxel. The chosen body can
vary depending on the coordinate system and on the computational time required
in the forward modelling itself. The simple body is described by a set of parameters
deﬁning its shape, that is a body parameter vector di. For instance, when using a
Cartesian coordinate system the rectangular prism can be used, while with spherical
or ellipsoidal coordinate system a good choice could be the usage of the tesseroid.
The simple shapes just presented are the more natural that can be obtained when
each index i1, i2, i3 discretize each of the three coordinates.
Taking as example Cartesian coordinates, the position vector of a generic voxel
i can be described as:
ri = xiex + yiey + ziez
(4.5)
Then each of the three Cartesian axes is associated to an index, e.g. i1 counts voxels
along y axis, i2 counts voxels along x axis and i3 counts voxels along z axis. A
remark is that the index i1 is not associated to the x axis because according to the
relation i ↔i1, i2, i3 described by Equations 4.1 and 4.2 the column-major order is
chosen. According to this convention the ﬁrst index is associated to the ordinate
and the second the abscissa. This will become useful when translating the algorithm
into a software.
When mapping coordinates to indexes, the simplest and most intuitive assump-
tion is to set constant steps in the three dimensions. This translates into the fact
that the body parameter vector di is the same for all the voxels, composed by the
three grid steps ∆x, ∆y, ∆z. It is worth to notice that the grid step could even be
not constant in one ore more direction, by introducing a dependency of the step on
the value of the grid index in the respective direction, namely:





∆x = ∆x(i2)
∆y = ∆y(i1)
∆z = ∆z(i3)
(4.6)
An example of variable step grid is shown in Figure 4.2. The variability and the
voxels resolution could be calibrated according to the distance of the voxels with
respect to the observation points, such that all the voxels has similar forward contri-
bution, and according to the signal accuracy. When the geometrical relationship is
fully deﬁned, it is possible to compute the forward signal of the gravity functional(s)
used in the inversion, by applying the principle of eﬀect superimposition. There-
fore the modelled signal, i.e. the observable quantity, becomes the summation of the
57

Bayesian gravity inversion by Monte Carlo methods
1
2
3
4
5
6
7
8
9
10
i2
0
10
20
30
x [km]
x grid
1
2
3
4
5
6
7
8
9
10
i1
0
10
20
30
y [km]
y grid
1
1.5
2
2.5
3
3.5
4
4.5
5
i3
-15
-10
-5
0
z [km]
z grid
-15
30
0
-10
z [km]
-5
10
20
0
y [km]
x [km]
10
20
30
0
Figure 4.2: Cartesian voxel grid when deﬁning variable grid steps in the three dimensions. On
the left it is possible to see the centres of the voxels in each direction, while on the right the result
obtained when describing the voxels using prisms.
contribution of each voxel:
y(P) =
n
X
i=1
F(ρi, rP, ri, di)
(4.7)
where F is the forward function, that is shown in Equation 1.14 when considering the
prism discretization and the gravitational attraction as the functional to be inverted,
and rP is the position vector of the observation points. Moreover, the assumption of
a constant density inside each voxel leads to a linear forward equation with respect
to the density itself, thus allowing the introduction of a linear system describing the
forward eﬀect of all the voxels on all the computational points; Equation 4.7 can be
therefore rewritten in the following shape:
y = F(rP, ri, di) ρ
(4.8)
where F is the forward matrix of dimensions m × n depending on the m vectors rP,
describing the position of observation points, and the n vectors ri and di, describing
the voxels.
58

CHAPTER 4. THE BAYESIAN INVERSION ALGORITHM
4.2
Posterior probability formalization
We start from the Bayes theorem in its usual form, recalling Equation:
P(x|y) ∝L(y|x) P(x)
(4.9)
where y is the vector of observations and x the vector of unknown parameters, as
described in the previous section, L(y|x) is the likelihood distribution, P(x) the
prior probability distribution of the model parameters and P(x|y) the posterior
distribution of model parameters given the observations. Notice that working in a
Bayesian framework both x and y are assumed to be random variables
In the following we analyse from the formal point of view how the likelihood and
the prior probability are designed in the proposed solution.
4.2.1
Likelihood probability distribution
The likelihood distribution describes the degree of ﬁt between the observations
and modelled observables, namely the gravity functional(s) used in the inversion. In
other words, it represents the observation distribution knowing the model parame-
ters. It is derived by assuming that the observation noise ν is normally distributed
with zero mean and a covariance matrix Cνν, meaning that:
P(ν) = N[0,Cνν](ν)
(4.10)
Therefore, deﬁning the relationship between the observations y and the observables
y(x) depending on the model parameters x as:
y = y(x) + ν
(4.11)
the likelihood assumes the following shape:
L(y|x) = N[y(x),Cνν](y)
(4.12)
Then, recalling the linear relationship between the parameters and the observables
shown in Equation 4.8, the likelihood probability distribution can be ﬁnally expressed
as:
L(y|x) =
1
p
(2π)m det(Cνν)
exp

−1
2(y −Fρ)⊺C−1
νν (y −Fρ)

(4.13)
where m represents the number of observations.
4.2.2
Prior probability distribution
When setting the problem the volume is subdivided in voxels, each of them
represented by a couple of random variables: the density and the label. Therefore,
the prior P(x) can be thought as the product between the conditional probability of
the densities given the labels and the marginal probability of the labels, according
to the deﬁnition of joint probability:
P(x) = P(ρ|L) P(L)
(4.14)
59

Bayesian gravity inversion by Monte Carlo methods
meaning that for a vector of known labels L the densities ρ will follow the law
P(ρ|L) and that, using the marginal distribution, we can design the label probability
independently from the densities.
To simplify the analytical computation of the posterior probability, it is assumed
that the densities have a normal distribution and that the labels have a Gibbs distri-
bution. The former is the conjugate distribution with respect to the likelihood, while
the latter allows to describe the labels as a MRF, making easier the optimization
by a Gibbs sampler. In the following Subsections 4.2.2 and 4.2.2 we will write the
prior probability according to these principles, while Section 4.3 will clarify why this
choice, which is seemingly not straightforward, is convenient for the optimization
procedure.
Densities
As a ﬁrst assumption the density of each voxel can be considered normally dis-
tributed once its label is given, that is:
P(ρi|Li) =
1
q
2πσ2
ρ(Li)
exp
(
−[ρi −µρ(Li)]2
2σ2
ρ(Li)
)
= N[µρ(Li),σ2ρ(Li)](ρi)
(4.15)
where µρ(Li) and σ2
ρ(Li) are the mean and the variance of the normal distribution,
which depends on the value assumed by the label Li, namely for each possible
geological unit Li = 1, 2, . . . , M at voxel i a diﬀerent couple of mean and variance is
deﬁned. Assuming no correlation between densities of neighbour voxels, Equation
4.15 translates into:
P(ρ|L) =
n
Y
i=1
P(ρi|Li) =
n
Y
i=1
N[µρ(Li),σ2ρ(Li)](ρi)
(4.16)
However, this deﬁnition could lead to a density realization aﬀected by variations
of the order of σρ(Li) when neighbour voxels assume the same label. In other words,
if a volume with the same label is considered, its density distribution could be
aﬀected by white random noise with a standard deviation equal to σρ(Li). When
approaching high resolutions in the voxel model, this fact is not physical, because it
is reasonable thinking about smooth variations of density inside the same material
in the majority of cases. Therefore, a smoother density variation can be modelled
by introducing a full covariance matrix in the multivariate normal distribution of
Equation 4.16, namely P(ρ|L) becomes:
P(ρ|L) = N[µρ(L),Cρρ(L)](ρ) =
=
1
p
2π det[Cρρ(L)]
exp

−1
2[ρ −µρ(L)]⊺C−1
ρρ (L) [ρ −µρ(L)]

(4.17)
where the mean vector µρ(L) contains the mean at each voxel i once Li is given,
that is µρ(L) = [µρ(L1) , µρ(L2) , . . . µρ(Ln)]⊺and the covariance matrix Cρρ(L) con-
tains both the above mentioned information on density variances and the correlation
60

CHAPTER 4. THE BAYESIAN INVERSION ALGORITHM
between the voxels. In fact, the covariance matrix and its inverse can be expressed
in the following way:
Cρρ(L) = Dρ(L) R(L) Dρ(L)
(4.18)
C−1
ρρ (L) = D−1
ρ (L) R−1(L) D−1
ρ (L)
(4.19)
where Dρ(L) is a diagonal matrix containing the standard deviation σρ(Li) of each
voxel i once Li is given, namely:
Dρ(L) =


σρ(L1)
0
0
0
0
σρ(L2)
0
0
0
0
...
0
0
0
0
σρ(Ln)


(4.20)
and its inverse D−1
ρ (L) is:
D−1
ρ (L) =


1/σρ(L1)
0
0
0
0
1/σρ(L2)
0
0
0
0
...
0
0
0
0
1/σρ(Ln)


(4.21)
Moreover, R(L) is the symmetric correlation matrix composed by the Pearson linear
correlation coeﬃcients rij(Li, Lj) of each couple of voxel i and j:
R(L) =


1
r12(L1, L2)
. . . r1n(L1, Ln)
r21(L2, L1)
1
. . . r2n(L2, Ln)
...
...
...
...
rn1(Ln, L1) rn2(Ln, L2)
. . .
1


(4.22)
The description shown in Equation 4.22 allows to impose correlation between den-
sities managing the diﬀerent materials. In other words, diﬀerent correlations for
diﬀerent materials can be introduced, e.g. by assuming:
rij(Li, Lj) =
(r(ri −rj, Li)
if Li = Lj
0
if Li ̸= Lj
(4.23)
where rij is a function of the vector position ri and rj of the two voxels i and j,
respectively, and of the same value assumed by Li and Lj.
This kind of modellization requires the knowledge of the complete correlation
matrix that varies for each realization of the vector L. When the number of voxels
increases, approaching a high resolution discretization, this could be unfeasible due
to the huge dimensions of the matrix itself. This problem can be overcome by dis-
regarding the dependency on the labels, namely building the matrix from a unique
stationary correlation function, just depending on the diﬀerence between voxel po-
sition vectors ri and rj. This assumption introduces the same correlation for all
the labels, but also introduces correlation between voxels with diﬀerent label. In
practice, this means that the densities have the same level of smoothing everywhere
61

Bayesian gravity inversion by Monte Carlo methods
in the investigated volume, independently from the speciﬁc realization of the vector
L. A further simpliﬁcation that can be performed is to let the correlation be a
function of the distance in terms of grid index i and j in the voxel grid, rather than
the physical distance. These assumptions introduced into Equation 4.23 lead to:
rij(Li, Lj) =r(i −j) =r(i1 −j1, i2 −j2, i3 −j3)
(4.24)
Obviously, when the voxel grid is associated to a Cartesian coordinate system with a
constant spacing, Equations 4.23 and 4.24 deﬁne the same correlation criteria apart
from the label dependency.
Disregarding label dependency, namely R(L) = R,
the correlation matrix R assumes the shape of a three-dimensional Toeplitz matrix.
Therefore, it can be described by deﬁning the shape of a correlation function, thus
reducing the number of elements to be stored and allowing to manage the correlation
between the full set of voxels. It is worth to notice that the inverse of the correlation
matrix R−1 can be approximated by a Toeplitz matrix which kernel can be computed
by means of Fourier transform methods (Gray, 2006), as it will shown in Section 6.3.
According to the proposed modellization of the covariance matrix Cρρ(L) its
determinant, used in Equation 4.17, can be computed recalling that the determinant
of a diagonal matrix is the product of its diagonal elements:
det[Cρρ(L)] = det[Dρ(L)] det(R) det[Dρ(L)] =
n
Y
i=1
σ2
ρ(Li) det(R)
(4.25)
Finally, it is useful to limit the domain of ρ avoiding that, while ﬁxing the label
Li, the associated density ρi could practically assume values on the whole R space.
Nevertheless, by looking at the marginal distribution of each ρi it can be observed
that the probability to be outside the range −3σρ(Li) ≤ρi −µρ(Li) ≤3σρ(Li) is
smaller than 0.3%. This fact has also a physical evidence, since it could be reasonable
assuming that densities too far from the mean value are not realistic. Therefore,
introducing the above explained constraint into the density prior probability P(ρ|L)
shown in Equation 4.17, it becomes:
P(ρ|L) = 1
Aρ
N[µρ(L),Dρ(L) R Dρ(L)](ρ)
n
Y
i=1
χ[ρ1(Li),ρ2(Li)](ρi) =
= exp

−1
2[ρ −µρ(L)]⊺D−1
ρ (L) R−1 D−1
ρ (L) [ρ −µρ(L)]

·
·
1
Aρ
q
2π Qn
i=1 σ2
ρ(Li) det(R)
n
Y
i=1
χ[ρ1(Li),ρ2(Li)](ρi)
(4.26)
where:
(
ρ1(Li) = µρ(Li) −3σρ(Li)
ρ2(Li) = µρ(Li) + 3σρ(Li)
(4.27)
The constant Aρ must be introduced to guarantee that P(ρ|L) integrated over its
full domain assumes the unitary value. It is worth to notice that this normaliza-
tion constant is independent from the conditioning labels.
This can be seen by
62

CHAPTER 4. THE BAYESIAN INVERSION ALGORITHM
standardizing the densities ρ. Therefore, assuming:
zρ(L) = D−1
ρ (L) [ρ −µρ(Li)] =
=
ρ1 −µρ(L1)
σρ(L1)
, ρ2 −µρ(L2)
σρ(L2)
, . . . , ρn −µρ(Ln)
σρ(Ln)
⊺
(4.28)
the constant Aρ in Equation 4.26 can be evaluated as:
Aρ =
Z
Rn
exp

−1
2[zρ(L)]⊺R−1[zρ(L)]
	
q
(2π)n Qn
i=1 σ2
ρ(Li) det(R)
n
Y
i=1

χ[−3,3](zρ(Li)) σρ(Li)

dzρ(L) =
=
Z 3
−3
. . .
Z 3
−3
exp

−1
2[zρ(L)]⊺R−1[zρ(L)]
	
p
(2π)n det(R)
dzρ(L) =
=
Z 3
−3
. . .
Z 3
−3
N[0,R](zρ(L)) dzρ(L)
(4.29)
Equation 4.29 clariﬁes that the value of the constant Aρ does not depend on
labels.
In fact, the integration domains is separable and its boundaries do not
depend on the label L anymore, as well as the mean vector and the covariance
matrix of the normal distribution. Thanks to this fact the value of the constant is
not required in the following, according to the Gibbs sampler properties, as it will
be shown afterwards.
Labels
As for the marginal prior probability distribution of labels P(L), we assume to
have a Gibbs distribution (Azencott, 1988; Geman and Geman, 1984):
P(L) ∝exp{−λE(L)}
(4.30)
where the energy E(L) depends on the cliques of order one and two, namely from the
voxel itself (order one) and from all the couples of voxels inside the neighbour that
contains the central one (order two). The neighbourhood is determined by assuming
c = 1 in its deﬁnition (Equation 4.4) and assumes the shape shown in Figure 4.3.
According to MRF properties the total energy can be written as the sum of the
potential of all the cliques, therefore P(L) becomes:
P(L) ∝exp
(
−γ
2
n
X
i=1
s2
i (Li) −λ
2
n
X
i=1
X
j∈∆i
q2(Li, Lj)
)
(4.31)
where the function s2
i (Li) represents the contribution of the cliques of order one and
q2(Li, Lj) represent the cliques of order two. Practically, the clique potential can be
seen as a penalty function; s2
i (Li) (order one) penalize diﬀerences with respect to an
a-priori label realization and q2(Li, Lj) (order two) encourage all the voxels of ∆i to
assume the same label of the central one, as far as it is possible. The weights λ and
γ are introduced to reciprocally balance the information carried by the two terms.
63

Bayesian gravity inversion by Monte Carlo methods
Figure 4.3: Shape of the neighbourhood ∆i, obtained assuming c = 1 in Equation 4.4. The j
index identify all the voxels composing the neighbour set, denoting the centre of each voxel.
The eﬀect of using penalty functions is that the most probable label realization is
the one for which the paid penalty is the smallest one.
It is worth to notice that the two weights γ and λ have to be empirically tuned,
as well as the value assumed by the functions s2
i (Li) and q2(Li, Lj). Therefore, to
deﬁne the penalty function their physical meaning has to be analysed.
The s2
i (Li) function has the aim to represent the contribution of an a-priori
guessed distribution of the diﬀerent geological units. In practice, the given input
aims to deﬁne the possible label values and their probability at each voxel. In other
words, it is exactly the label prior probability P(L) when it is assumed λ = 0 and
γ = 1 in Equation 4.31. Under this assumption, the prior probability of each voxel
is independent voxel by voxel and can be expressed as:
P(L|γ = 1, λ = 0) =
n
Y
i=1
Pγ(Li)
(4.32)
where:
Pγ(Li) =











p1(i)
if Li = 1
p2(i)
if Li = 2
...
pM(i)
if Li = M
(4.33)
This modellization translates into the fact that the function s2
i (Li) can be deﬁned
directly as:
s2
i (Li) =











−2 log[p1(i)]
if Li = 1
−2 log[p2(i)]
if Li = 2
...
−2 log[pM(i)]
if Li = M
(4.34)
This way of modelling the prior probability allows an easy translation of an a-priori
geometrical model, given in terms of discontinuities between diﬀerent materials. In
64

CHAPTER 4. THE BAYESIAN INVERSION ALGORITHM
fact, based on these surfaces and their uncertainty the voxel model and the associated
probability can be easily retrieved.
As for the contribution related to cliques of order two, it can be modelled by
means of a function with the shape:
q2(Li, Lj) =
(
0
if Li = Lj
q2
ij
if Li ̸= Lj
(4.35)
where, by properly setting the values of q2
ij, it is possible to advantage or disadvantage
the proximity of the material associated to Li and Lj. Therefore, a table containing
all the possible correspondences between the M possible values of Li and the M
possible values of Lj can be deﬁned, associating to each elements the penalty in case
of proximity. Introducing these values into a matrix, Equation 4.35 can be expressed
as:
q2 =


0
q2
12
. . .
q2
1M
q2
21
0
. . .
q2
2M
...
...
...
...
q2
M1
q2
M2
. . .
0


(4.36)
where the row index represents the possible values of Li and the column index Lj.
Analogously to what is performed in case of the function s2
i (Li), it is possible to
deﬁne a sort of “clique correlation” ranging between 0 and 1, that could be used to
compute the correspondent weight in Equation 4.36 in the following way:
q =


0
−2 log(p12)
. . .
−2log(p1M)
−2 log(p21)
0
. . .
−2log(p2M)
...
...
...
...
−2 log(pM1)
−2 log(pM2)
. . .
0


(4.37)
In particular, the “clique correlation” equal to 1 means that this is the best neigh-
bourhood couple and is associated only to couples with the same label, while when
it assumes a value close to 0 this means that a model containing this two labels in
two neighbour voxels has very low probability value associated.
4.2.3
Posterior probability distribution
The posterior probability distribution of the parameters can be derived by in-
troducing 4.13, 4.26, 4.31 into Equation 4.9, describing the Bayes theorem. The
posterior distribution P(x|y) = P(ρ, L|y) becomes:
P(ρ, L|y) = 1
A exp

−1
2(y −Fρ)⊺C−1
νν (y −Fρ) +
−η
2[ρ −µρ(L)]⊺D−1
ρ (L) R−1 D−1
ρ (L) [ρ −µρ(L)] +
−γ
2
n
X
i=1
s2
i (Li) −λ
2
n
X
i=1
X
j∈∆i
q2(Li, Lj)
)
n
Y
i=1
χ[ρ1(Li),ρ2(Li)](ρi)
σρ(Li)
(4.38)
65

Bayesian gravity inversion by Monte Carlo methods
where A is a normalization constant grouping all the constants present in both the
prior and the likelihood probability.
Notice that the weight η is introduced in the term related to the prior proba-
bility on the densities P(ρ|L). This weight is used to compensate the unbalancing
eﬀects caused by the diﬀerent number of observations with respect to the number
of parameters. The latter may be much larger than the former, especially if the
resolution of the voxel model is very high. Despite the covariance matrices of both
densities and observations have a physical meaning, the introduction of this weight
prevents that the prior probability becomes dominant with respect to the likelihood,
thus reducing the (already weak) contribution of the gravity observations.
As it happens for the λ and γ weights, also η has to be empirically tuned.
A possible empirical procedure for all those weights will be discussed in the next
chapter.
4.3
Optimization algorithm
The inverse solution is found by retrieving the MAP. This translates into ﬁnding
the combination of parameters that maximize the posterior probability shown in
Equation 4.38. Due to the complexity of this distribution, the usage of deterministic
optimization methods (e.g. simple relaxation, steepest descent, etc.) could lead to
local maxima while ﬁnding the solution. In fact, these iterative solutions require
an initialization of the parameters as close as possible to the maximum.
These
problems can be overcome by using a stochastic optimization method. In partic-
ular, among MCMC methods the simulated annealing aided by a Gibbs sampler
is chosen. Nevertheless, once the stochastic optimization is performed, the quality
of its solution can be improved by starting an iterative deterministic optimization
algorithm. In this case, the simple relaxation is chosen, because of its analogy with
the Gibbs sampler, as already introduced in Section 3.4. In the following subsec-
tions the stochastic optimization and the subsequent deterministic reﬁnement will
be described, as implemented in the proposed solution.
4.3.1
Stochastic optimization
As already explained in Section 3.1, the simulated annealing principle makes pos-
sible to ﬁnd the set of parameters that minimize a target function E(x) by sampling
the distribution probability PT(x) when T →0. The simulated annealing principle,
already shown in Equation 3.9, is here recalled in Equation 4.39:
arg min
x E(x) = lim
T→0 E[X]
(4.39)
where X is a random variable with the following probability distribution:
PT(x) = 1
AT
exp

−1
T E(x)

(4.40)
Therefore, to ﬁnd the MAP, the target function is deﬁned as the opposite of the
logarithm of the posterior distribution P(ρ, L|y). Therefore starting from Equation
66

CHAPTER 4. THE BAYESIAN INVERSION ALGORITHM
4.38:
E(x) = E(ρ, L|y) = −log[P(ρ, L|y)]
(4.41)
the maximization problem is translated into a minimization one, suitable for the
simulated annealing:
arg max
ρ,L {P(ρ, L|y)} = arg min
ρ,L {E(ρ, L|y)}
(4.42)
Now, introducing Equation 4.41 into Equation 4.40, the probability density function
PT(ρ, L|y) to be sampled in the simulated annealing procedure can be determined
as:
PT(ρ, L|y) = 1
AT
exp

−1
2T (y −Fρ)⊺C−1
νν (y −Fρ) +
−η
2T [ρ −µρ(L)]⊺D−1
ρ (L) R−1 D−1
ρ (L) [ρ −µρ(L)] −γ
2T
n
X
i=1
s2
i (Li) +
−λ
2T
n
X
i=1
X
j∈∆i
q2(Li, Lj)
) " n
Y
i=1
χ[ρ1(Li),ρ2(Li)](ρi)
σρ(Li)
# 1
T
(4.43)
To sample the distribution PT(ρ, L|y) the Gibbs sampler is introduced. In partic-
ular, it allows sampling a multidimensional random variables by drawing samples
from its conditional probability distributions, as already explained in Section 3.2. In
the proposed solution, at each step of the iterative procedure a sample of the couple
label-density is drawn from their conditional distribution:
PT(ρi, Li|ρ−i, L−i, y) =
P(ρ, L|y)
P(ρ−i, L−i|y)
(4.44)
where the marginal probability distribution of the conditioning variables represents
the normalization constant. This two-dimensional conditional probability can be
decomposed in the product between the conditional probability of the density ρi
given the label Li and the marginal probability of the label Li, namely:
PT(ρi, Li|ρ−i, L−i, y) = PT(Li|ρ−i, L−i, y) PT(ρi|Li, ρ−i, L−i, y)
(4.45)
where the label marginal probability distribution can be computed as:
PT(Li|ρ−i, L−i, y) =
Z ∞
−∞
PT(ρi, Li|ρ−i, L−i, y) dρi
(4.46)
Therefore, the two-dimensional joint probability of label and density shown in
Equation 4.45 can be sampled by ﬁrst drawing a sample from the label marginal
distribution PT(Li|ρ−i, L−i, y) and then, according to the extracted label value,
from the density conditional one PT(ρi|Li, ρ−i, L−i, y). This procedure is repeated
iteratively for all the voxels, updating each time the previous parameters realization.
Once all the voxels are updated, the temperature is slowly decreased and a new stage
of the Gibbs sampler is started again, as shown by the ﬂow chart in Figure 4.4.
67

Bayesian gravity inversion by Monte Carlo methods
GIBBS SAMPLER
SIMULATED ANNEALING
Set a starting 
model
Temperature
schedule
Gravity 
observations
Translate prior information into: 
μρ(Li), σρ(Li), si
2(Li), q2(Li, Lj) 
Tune empirically the values
of η, λ, γ
ESTIMATED 
MODEL of label 
and density
NO
YES
Sample Li from
PT (Li | ρ‒i, L‒i, y) 
and update the model
Sample ρi from
PT (ρi | Li, ρ‒i, L‒i, y) 
and update the model
i ≤ n?
i = i + 1
Prior
information
k ≤ kmax?
T = T(k)
k = k + 1
i = 1
NO
YES
k = 1
Figure 4.4: Flow chart of the proposed solution algorithm.
68

CHAPTER 4. THE BAYESIAN INVERSION ALGORITHM
In order to compute the conditional probability shown in Equation 4.45, it is
required to isolate the terms depending on ρi and Li in the simulated annealing
joint probability distribution (Equation 4.43), while the others can be included into
the normalization constant. Therefore, recalling the vector of standardized densities
zρ(L) of Equation 4.28 and introducing a new normalization constant Ai(ρ−i, L−i),
Equation 4.43 becomes:
PT(ρi, Li|ρ−i, L−i, y) =
1
Ai(ρ−i, L−i)
" n
Y
i=1
χ[ρ1(Li),ρ2(Li)](ρi)
σρ(Li)
# 1
T
·
· exp

−1
2T (y −F−iρ−i −fiρi)⊺C−1
νν (y −F−iρ−i −fiρi) +
−η
2T [I−1zρ(L−i) + eizρ(Li)]⊺R−1[I−1zρ(L−i) + eizρ(Li)] +
−γ
2T s2
i (Li) −λ
2T
X
j∈∆i

q2(Li, Lj) + q2(Lj, Li)

+
−γ
2T
X
h̸=i
s2
h(Lh) −λ
2T
X
h̸=i
X
j∈∆h
j /∈∆i
q2(Lh, Lj)
)
(4.47)
where P
h̸=−i is the summation over all the elements h from 1 to n apart from the
element i. For the other notation, please refer to the conventions reported at page
9.
Expanding the product in the exponent, Equation 4.47 assumes the following
shape:
PT(ρi, Li|ρ−i, L−i, y) =
1
Ai(ρ−i, L−i)
" n
Y
i=1
χ[ρ1(Li),ρ2(Li)](ρi)
σρ(Li)
# 1
T
·
· exp
(
−f ⊺
i C−1
νν fi
2T

ρi −f ⊺
i C−1
νν (y −F−iρ−i)
f ⊺
i C−1
νν fi
2
+
−1
2T (y −F−iρ−i)⊺

C−1
νν −C−1
νν fif ⊺
i C−1
νν
f ⊺
i C−1
νν fi

(y −F−iρ−i) +
−ηe⊺
i R−1ei
2T

zρ(Li) + e⊺
i R−1I−izρ(L−i)
e⊺
i R−1ei
2
+
−η[e⊺
i R−1I−izρ(L−i)]2
2Te⊺
i R−1ei
−η
2T [zρ(L−i)]⊺I⊺
−iR−1I−i[zρ(L−i)] +
−γ
2T s2
i (Li) −λ
2T
X
j∈∆i

q2(Li, Lj) + q2(Lj, Li)

+
−γ
2T
X
h̸=−i
s2
h(Lh) −λ
2T
X
h̸=−i
X
j∈∆h
j /∈∆i
q2(Lh, Lj)
)
(4.48)
The algebraic details of this computation are reported in Appendix A. In particular,
69

Bayesian gravity inversion by Monte Carlo methods
x and C are substituted by zρ(L) and R, respectively, into Equation A.4, and Ax
are substituted by Fρ, into Equation A.22.
Now, grouping all the terms independent from ρi and Li in a new normalization
constant A′(ρ−i, L−i) and restoring the non-standardized density zρ(Li) = ρi−µρ(Li)
σρ(Li) ,
Equation 4.48 can be written in the following way:
PT(ρi, Li|ρ−i, L−i, y) =
χ[ρ1(Li),ρ2(Li)](ρi)
A′
i(ρ−i, L−i) [σρ(Li)]
1
T ·
· exp
(
−f ⊺
i C−1
νν fi
2T

ρi −f ⊺
i C−1
νν (y −F−iρ−i)
f ⊺
i C−1
νν fi
2
+
−ηe⊺
i R−1ei
2Tσ2
ρ(Li)

ρi −µρ(Li) + σρ(Li) e⊺
i R−1I−izρ(L−i)
e⊺
i R−1ei
2
+
−γ
2T s2
i (Li) −λ
2T
X
j∈∆i

q2(Li, Lj) + q2(Lj, Li)

)
(4.49)
where the above recalled normalization constant A′
i(ρ−i, L−i) assumes the following
value:
A′
i(ρ−i, L−i) = exp

−1
2T (y −F−iρ−i)⊺

C−1
νν −C−1
νν fif ⊺
i C−1
νν
f ⊺
i C−1
νν fi

(y −F−iρ−i) +
−η[e⊺
i R−1I−izρ(L−i)]2
2Te⊺
i R−1ei
−η
2T [zρ(L−i)]⊺I⊺
−iR−1I−i[zρ(L−i)] +
−γ
2T
X
h̸=i
s2
h(Lh) + −λ
2T
X
h̸=i
X
j∈∆h
j /∈∆i
q2(Lh, Lj)
)
Ai(ρ−i, L−i)
Q
h̸=i[σρ(Lh)]
1
T
(4.50)
The two quadratic form present in the exponent of Equation 4.48 denotes two
normal distributions, as explained in Appendix A.1 and A.3. Therefore, the following
quantities can be deﬁned, according to the normal distribution shape:
µρ(Li) = µρ(Li) −σρ(Li) e⊺
i R−1I−izρ(L−i)
e⊺
i R−1ei
=
= µρ(Li) −σρ(Li)
(R−1)i,i
e⊺
i R−1I−izρ(L−i) =
= µρ(Li) −σρ(Li)
(R−1)i,i
X
j̸=i
 R−1
i,j
ρj −µρ(Li)
σρ(Li)
(4.51)
σ2
ρ(Li) =
σ2
ρ(Li)
e⊺
i R−1ei
= σ2
ρ(Li)
(R−1)i,i
(4.52)
µy = f ⊺
i C−1
νν (y −F−iρ−i)
f ⊺
i C−1
νν fi
(4.53)
σ2
y =
1
f ⊺
i C−1
νν fi
(4.54)
70

CHAPTER 4. THE BAYESIAN INVERSION ALGORITHM
An overview on how these quantities are derived can be found in Appendix A, and
in particular into Equations A.7 and A.23. Then, introducing them into Equation
4.49, we obtain:
P(ρi, Li|ρ−i, L−i, y) =
χ[ρ1(Li),ρ2(Li)](ρi)
A′
i(ρ−i, L−i) [σρ(Li)]
1
T ·
· exp

−
1
2Tσ2
y
 ρi −µy
2 −
η
2Tσ2
ρ(Li)

ρi −µ2
ρ(Li)
2 +
−γ
2T s2
i (Li) −λ
2T
X
j∈∆i

q2(Li, Lj) + q2(Lj, Li)

)
(4.55)
As explained above, to sample the couple ρi, Li it is required to compute the
marginal probability distribution PT(Li|ρ−i, L−i, y) of a label, given the values of
labels and densities ρ−i and L−i of all the other voxels, and the conditional proba-
bility distribution PT(ρi|Li, ρ−i, L−i, y) of a density, given all the labels and all the
other densities.
Introducing the results of Equation 4.55 into the label marginal distribution of
Equation 4.46 and recalling the deﬁnition of Gaussian function shown into Equation
A.1, we obtain:
PT(Li|ρ−i, L−i, y) =
Z ∞
−∞
P(ρi, Li|ρ−i, L−i, y) dρi =
q
(2πT)2 σ2
yσ2
ρ(Li) η−1
A′(ρ−i, L−i)

σ2
ρ(Li)
 1
T ·
· exp
(
−γ
2T s2
i (Li) −λ
2T
X
j∈∆i

q2(Li, Lj) + q2(Lj, Li)

)
·
·
Z ∞
−∞
N[µρ(Li),Tσ2ρ(Li)η−1](ρi) N[µy,Tσ2y](ρi) χ[ρ1(Li),ρ2(Li)](ρi) dρi
(4.56)
The two normal distributions inside the integral can be combined as shown in Section
A.2 and in particular recalling Equation A.12, we can deﬁne:
µ(Li) =
µy
Tσ2y +
ηµρ(Li)
Tσ2ρ(Li)
1
Tσ2y +
η
Tσ2ρ(Li)
= ησ2
yµρ(Li) + σ2
ρ(Li) µy
ησ2
y + σ2
ρ(Li)
(4.57)
σ2(Li) =
1
1
Tσ2y +
η
Tσ2ρ(Li)
= T
σ2
y σ2
ρ(Li)
ησ2
y + σ2
ρ(Li)
(4.58)
Recalling also Equations A.15 and A.19 describing the product of normal distribu-
tion, introducing Equations 4.57 and 4.58 and grouping all the terms independent
from the current label Li into a new constant A′′(ρ−i, L−i), the label marginal prob-
71

Bayesian gravity inversion by Monte Carlo methods
ability can be ﬁnally computed as:
PT(Li|ρ−i, L−i, y) =
1
A′′(ρ−i, L−i) [σρ(Li)]
1
T ·
· exp
(
−γ
2T s2
i (Li) −λ
2T
X
j∈∆i

q2(Li, Lj) + q2(Lj, Li)

)
·
·
s
σ2
yσ2
ρ(Li)
ησ2
y + σ2
ρ(Li) exp
(
−η

µy −µρ(Li)
2
2T

ησ2
y + σ2
ρ

) Z ρ2(Li)
ρ1(Li)
N[µ(Li),σ2(Li)](ρi) dρi
(4.59)
where the integral can be evaluated by means of the error function as (Abramowitz
and Stegun, 1964):
J(Li) =
Z ρ2(Li)
ρ1(Li)
N[µ(Li),σ2(Li)](ρi) dρi =
= 1
2

erf
ρ2(Li) −µ(Li)
σ(Li)
√
2

−erf
ρ1(Li) −µ(Li)
σ(Li)
√
2

(4.60)
It is worth to notice that since the label is a discrete variable, the normalization
constant A′′(ρ−i, L−i) can be easily numerically evaluated each time as:
A′′(ρ−i, L−i) =
M
X
i=1
exp
(
−γ
2T s2
i (Li) −λ
2T
X
j∈∆i

q2(Li, Lj) + q2(Lj, Li)

)
·
·
q
σ2
yσ2
ρ(Li)
[σρ(Li)]
1
T
q
ησ2
y + σ2
ρ(Li)
exp
(
−η

µy −µρ(Li)
2
2T

ησ2
y + σ2
ρ

)
J(Li)
(4.61)
As for the density conditional distribution can be computed by ﬁxing the value of
label Li into Equation 4.55, applying the proper normalization. Recalling Equations
4.57, 4.58, and 4.60 the probability distribution of density ρi assumes the shape of
a truncated normal distribution:
PT(ρi|Li, ρ−i, L−i, y) =
1
J(Li)N[µ(Li),σ2(Li)](ρi) χ[ρ1(Li),ρ2(Li)](ρi)
(4.62)
4.3.2
Deterministic optimization
As recalled above, the advantage of using a stochastic optimization method like
the simulated annealing is that the starting approximate solution is not necessary.
However, the convergence with a good numerical approximation to the optimal so-
lution is long and strongly dependent on the temperature schedule, namely from
the values of the initial and ﬁnal temperature and the total number of iterations.
Therefore, in order to speed up the process and to increase the numerical accuracy
of the optimal solution, a good compromise is to use the simulated annealing to
reach a solution close to the optimal one and then to start an iterative deterministic
method to reach the closest maximum (or minimum) close to this point.
72

CHAPTER 4. THE BAYESIAN INVERSION ALGORITHM
As explained in Section 3.4, the algorithm obtained by combining the simulated
annealing and the Gibbs sampler can be seen as the stochastic version of the sim-
ple relaxation algorithm. This is the reason why the deterministic optimization is
performed by the simple relaxation, allowing to practically follow the same scheme.
In fact, to apply this optimization method, it is suﬃcient to use the Gibbs sampler
scheme shown in Figure 4.4, but replacing the sampling procedure with the research
of the maximum into Equation 4.55 and setting the temperature T = 1. This last
assumption implies that the function to be optimized is the posterior probability,
since the scale factor caused by the temperature is disregarded. Notice that a diﬀer-
ent value of T would not change the result of the optimization procedure. Finding
the maximum of Equation 4.55 becomes an easy task. In fact, since the label is
a discrete variable, maximizing Equation 4.55 correspond to ﬁnd the maximum of
Equation 4.62 for all the possible M values of the label Li and then to choose the
maximum of the maxima. Actually, the shape of Equation 4.62 is a truncated nor-
mal, therefore its maximum corresponds to the mean of the normal µ(Li) when it
is inside the truncation domain, or to one of the two boundaries values ρ1(Li) or
ρ2(Li) of the domain when the mean falls outside.
The presented procedure has to be iterated for all the n voxels until the conver-
gence is reached, namely the new parameters estimate diﬀers from the previous one
less than the required numerical accuracy.
4.4
Accuracy assessment
By invoking the MAP principle, the proposed solution aims to ﬁnd the set of
parameters for which their posterior probability distribution assumes the maximum
value. A drawback of this choice is that no information about the variability of the
probability distribution is available. In other words, following this approach it is
not possible to evaluate the shape of the posterior probability or to understand if
there are other local maxima and how much they diﬀers from the global one. On the
other hand, evaluating the shape of the posterior probability is usually the aim of a
Bayesian approach, but this is in contrast with the dimensions of this problem. As
already introduced in Section 2.4, the classical way to face this problem is to sample
the posterior distribution, when it becomes diﬃcult to analytically or exhaustively
evaluate the probability of the possible parameters.
Unfortunately, sampling looses its quality when the space of possible solution
becomes bigger and bigger, because of the huge number of unknown parameters. By
approximating the investigated volume in voxels it is very easy to reach this sam-
pling limits and that is why the MAP principle was adopted. However, alternative
solutions can be required to understand, at least around the maximum, the shape
of the posterior probability. It is worth to notice that this evaluation represents the
Bayesian way to assess the accuracy of the estimated parameters, as it is usually
called in the frequentist approach. Therefore, this becomes a crucial task in the
solution of the inverse problem.
Possible approaches to accuracy assessment are introduced in the following, even
if they are not the aim of this work and therefore their study and implementation are
73

Bayesian gravity inversion by Monte Carlo methods
left to future works. In particular two ideas to approach the problem are proposed.
The former is to sample the posterior probability by using a MCMC method
like the Gibbs sampler, initializing the chain with the estimated optimal solution.
Although the number of samples is not suﬃcient to completely describe the posterior
probability, starting from the optimal point should allows to evaluate the shape of
the posterior probability just around its maximum. Therefore, it should be possible
to understand how picked is the posterior probability around its maximum.
The latter aims to evaluate the accuracy of the estimated densities, ﬁxing the
estimated label realization. The idea is to ﬁnd the maximum amplitude of the white
noise, that, used to perturb the current density solution, allows an acceptable level of
ﬁt of the gravity observation. The noise amplitude is described through its standard
deviation, that is assumed to be the accuracy of the densities. This solution relies
on the meaning of“acceptable ﬁt”. In practice, starting from the gravity observation
accuracy, it is necessary to deﬁne a threshold over which the ﬁt is no more considered
acceptable.
4.5
Discussion
A problem that is not faced in the present chapter is the introduction of further
constraints on the possible solutions, especially in terms of density. In fact, the
presented solution contains a stochastic “constraint” that is introduced by means
of the covariance matrix Cρρ.
Nevertheless, this modellization does not impose,
e.g. when a strictly vertical increasing density is supposed, that an upper and a
lower voxel satisfy the condition, even if the use of a covariance matrix makes these
values similar and the vertical gradient can be introduced through the mean density.
However, these kinds of constraints are quite common in gravity inversion.
For
instance, a vertical increasing density is a requirement when facing with sedimentary
rocks. In fact, they increase their density with depth due to the particle compacting
caused by their own weight, as explained above. In the proposed solution, these
constraints will be introduced by simply limiting the solution space when optimizing
the posterior probability.
In the following examples, the impact of this kind of
constraints will be shown (see Chapter 7).
74

Chapter 5
Empirical setup of parameters
As largely discussed in the previous chapter, the prior probability is deﬁned by
a set of parameters that are used to translate the geological information in terms of
probability. The aim of this chapter is to look into the meaning of these parameters
and to understand how their value can be empirically tuned. The parameters inﬂu-
encing the shape of the posterior distribution can be classiﬁed in two categories: the
physical parameters and the weights. The former represent the translation of the
available geological information into prior probability, namely the density distribu-
tion parameters (mean and variance/covariance of the diﬀerent kinds of material)
and the penalty functions that are related to the model geometry. Each of these
parameters is tuned almost independently from the others. Therefore, the various
terms of the prior probability have to be homogenized and balanced to correctly
reproduce the geological guessing and this is done by means of the weights.
Another important contribution to the ﬁnal estimate is given by the temperature
schedule, that controls the convergence of the solution algorithm and, partially, the
required computational time.
In the following the empirical operations that can be performed to deﬁne the
value of all these parameters are presented. In particular, the chapter is divided
into three sections that are devoted to the physical parameters, the weights and the
temperature schedule, respectively.
5.1
Physical parameters setting
The prior probability requires the knowledge of information about geometry and
density of the possible materials inside the investigated area. This information has
to be used to derive the functions µρ(Li), σρ(Li), s2
i (Li), q2(Li, Lj), used to describe
the prior probability of densities and labels (see Equations 4.26 and 4.31).
As a ﬁrst point, it is required to perform a geological study of the area. Usually,
this study starts from a geological map, where the outcrops at the terrain level of
diﬀerent rock types or geologic strata are depicted with diﬀerent colour or symbols.
The classiﬁcation at the surface level is completed by information about bedding
planes and structural features such as faults, folds, foliations, and lineations, that
are shown reporting their strike and dip or trend and plunge symbols, to give their
three-dimensional orientations.
Also stratigraphic contour lines may be used to
75

Bayesian gravity inversion by Monte Carlo methods
illustrate the surface of a selected stratum illustrating the subsurface topographic
trends of the strata. Integrating the geological map with an isopach map, where
information about the thickness of the stratigraphic units below the topographic
surface is reported by means of contour lines, geological sections and more in general
a three-dimensional models of the investigated volume can be retrieved. An example
of geological section is shown in Figure 5.1 and methodology on how to translate
geological information into three-dimensional model is largely present in literature,
see e.g. Wu et al (2005), Kaufmann and Martin (2008), or Calcagno et al (2008).
Figure 5.1: Example of geological section.
An alternative way to obtain information about discontinuities between the dif-
ferent kinds of rocks or crust, especially at regional scale, can be to directly extract
from the gravity signal itself by means of image analysis based techniques. In par-
ticular, by applying transformations or by using combinations of diﬀerent gravity
functionals, high density variations in the subsurface, corresponding to diﬀerent ma-
terials, can be highlighted. Examples of this approach can be found in Braitenberg
(2015), where, applying the simple Bouguer correction to a GOCE model, the resid-
uals are used to determine the geological provinces of the studied Africa continent, in
Sampietro (2015), where a Bayesian approach is used to reﬁne an a-priori geological
province model of the Balkan area, or in Ma and Li (2012), where the normalized
total horizontal derivative is used for the detection of edges and applied to northern
China.
Moreover, the model can be integrated with information derived by seismic data
interpretation as well as by well logs, obtained by drilling the subsurface.
It is
worth to remember that they are not usually uniformly distributed, but they are
more accurate than models derived from geological maps. Therefore, they are used
to improve those models and to evaluate their accuracy. An example of seismic
data in case of a salt dome are shown in Figure 5.2, where the three-dimensional
model and a seismic proﬁle taken at a central cross-section of the salt dome itself
are presented. It is worth to notice that in this case seismic data are not able to
recover a resolute and sharpen image of the bottom part of the salt structure, due
to salt elastic characteristics, and this is a typical case happening in oil exploration,
where gravity could be used to improve the knowledge on the area.
The results obtained while combining all the above recalled diﬀerent informa-
tion are highly inﬂuenced by the geologist experience and sensibility.
Once this
three-dimensional model is derived, it is easy to discretize it by using the voxel ap-
proximation, according to the chosen grid step and coordinate system. Starting from
76

CHAPTER 5. EMPIRICAL SETUP OF PARAMETERS
Figure 5.2: Example of proﬁle section of a salt dome. On the left the three-dimensional model
of the dome, while on the right a seismic proﬁle acquired at a central cross-section in the area.
this model and from its accuracy, derived considering the source of information at
each point, the penalty function s2
i (Li) can be deﬁned by assign to each voxel the
possible materials with their probabilities. These probabilities can be, for instance,
derived according to the distance of the considered voxel with respect to the border
of the class at which it belong in the geological model and to the uncertainty of this
border. As for the penalty function q2(Li, Lj), its deﬁnition is not so simple. In
fact based on the geological model, the materials that should not be conterminous
can be deﬁned, but the penalties to be assigned to all the other couples of materials
have to be empirically tuned based also on the degree of smoothness required for the
class boundaries, e.g. by evaluating realizations of the prior probability considering
diﬀerent values of these penalties.
The second big task to deﬁne the prior information is about the density. Several
sources of density data maybe available with widely diﬀering reliabilities. In par-
ticular the density of the rocks is mainly controlled by three factors: the material
composing grains forming the rock, the porosity and the ﬂuid ﬁlling the pore-space
(Reynolds, 1997). There are some “rules of thumbs” that can be used (Nettleton,
1971; Dampney, 1977; Telford et al, 1990), based on the geological history of the re-
gion and on how the materials are vertically disposed, to retrieve information about
these parameters and to deﬁne a possible mean value of the density of each class and
its uncertainty. In particular, a possible density range is known for almost all the
rocks, derived by evaluating the density of samples. For some of the most common
rocks, this range is reported in Figure 5.3.
Apart from these general rules, there are other way to obtain the factual density
of rocks present into the investigated volume. In fact, in case of wells presence, it is
possible to directly observe the density at the well position, therefore knowing the
real density at this point with high accuracy. Another alternative way is to derive
density information starting from the observed seismic velocity proﬁle, usually the
same proﬁles already used to determine the geometry of the model. A review of
77

Bayesian gravity inversion by Monte Carlo methods
this kind of methods can be found in Gardner et al (1974) and Christensen and
Mooney (1995), where some relations between p-waves velocities and densities are
empirically estimated, under diﬀerent geological conditions.
Combining all the presented information about densities, the parameters µρ(Li)
and σρ(Li) can be ﬁnally deﬁned. These can be tuned in order to introduce also
vertical or later variation trends, meaning that they depends on both the class and
the voxel position. For instance, this is the case of sedimentary rocks, which usually
present an increasing density with depth due to sediment compaction.
Figure 5.3: Variation in rock density for diﬀerent kinds of rocks. The black dot indicates the
approximate average density of the rock, while the bar its variability. Data taken from Reynolds
(1997).
78

CHAPTER 5. EMPIRICAL SETUP OF PARAMETERS
5.2
Weights tuning
Before entering in the discussion about the weight tuning, it is useful to recall the
target function to be optimized E(ρ, L|y), in order to understand the meaning of the
considered weights. The target function can be retrieved by introducing Equation
4.38 into the deﬁnition given by Equation 4.41:
E(ρ, L|y) = log(A) +
n
X
i=1
log[σρ(Li)] + 1
2(y −Fρ)⊺C−1
νν (y −Fρ) +
+ η
2[ρ −µρ(L)]⊺D−1
ρ (L) R−1D−1
ρ (L) [ρ −µρ(L)] +
+ γ
2
n
X
i=1
s2
i (Li) −λ
2
n
X
i=1
X
j∈∆i
q2(Li, Lj)
(5.1)
where ρ : {µρ(Li) −3σρ(Li) ≤ρi ≤µρ(Li) + 3σρ(Li)}. In order to write Equation
5.1 in a more compact form, it is useful to deﬁne the following quantities:
∆y(ρ) = 1
2(y −Fρ)⊺C−1
νν (y −Fρ)
(5.2)
∆ρ(ρ, L) = 1
2[ρ −µρ(L)]⊺D−1
ρ (L) R−1D−1
ρ (L) [ρ −µρ(L)]
(5.3)
Γ(L) = 1
2
n
X
i=1
s2
i (Li)
(5.4)
Λ(L) = 1
2
n
X
i=1
X
j∈∆i
q2(Li, Lj)
(5.5)
Introducing these quantities into Equation 5.1, the target function can be written
as:
E(ρ, L|y) = log(A) +
n
X
i=1
log[σρ(Li)] + ∆y(ρ) + η∆ρ(ρ, L) + γΓ(L) + λΛ(L) (5.6)
where the contribution from the various terms of prior and likelihood probability
distributions are highlighted together with their relative weight. Due to the fact
that we are facing with a probability distribution, while changing the weights also
the normalization constant changes to keep the integral equal to 1. Nevertheless,
evaluating numerically the constant as shown in Equation 4.61 is not a problem, due
to the fact that the weights are chosen once for all at the beginning of the simulated
annealing.
The empirical approach used to determine the value of γ, λ, and η is an equal-
ization criterion. The idea is that the terms ∆y(ρ), η∆ρ(ρ, L), γΓ(L), and λΛ(L),
composing the target function in Equation 5.6, should account for the same order of
magnitude when the parameters ρ and L are close to the optimal solution. In that
way each of the terms play an active role in the ﬁnal solution. Therefore, considering
a set of values ˆρ, ˆL close to the optimal solution, the equalization condition can be
expressed as:
∆y( ˆρ) = η∆ρ( ˆρ, ˆL) = γΓ( ˆL) = λΛ( ˆL)
(5.7)
79

Bayesian gravity inversion by Monte Carlo methods
Actually, it is required only the expected value of ∆y( ˆρ) at the solution, with-
out knowing the related set of densities ˆρ.
Recalling that the observation noise
is assumed to be normally distributed, as introduced while deriving the likelihood
probability (see Equation 4.10), and considering the m-dimensional normal random
variable ν with zero mean and covariance matrix equal to Cνν, it can be demon-
strated that:
ν⊺C−1
νν ν ∼χ2
m
⇓
E[ν⊺C−1
νν ν] = E[χ2
m] = m
(5.8)
with m representing the degrees of freedom (Papoulis, 1965). It can be clearly seen
that Equation 5.2 has the same shape of Equation 5.8, thus the expected value
of ∆y(ρ) can be computed, according to Equation 5.8, as the half the number of
observations m:
∆y( ˆρ) ≃m
2
(5.9)
The same principle is applicable in the case of ∆ρ( ˆρ, ˆL). In fact, Equation 5.3 can
be seen as the summation of squared normal variables, therefore Equation 5.8 holds.
This translates into the fact that the variability of the density around its mean is
expected to be the one described by Cρρ(L) = Dρ(L) RDρ(L), close to the solution.
Thus, ∆ρ( ˆρ, ˆL) assumes a value equal to half the number of voxels n:
∆ρ( ˆρ, ˆL) ≃n
2
(5.10)
Recalling the conditions stated in Equations 5.7, the value of the weight η can be
determined starting from the assumption performed in 5.9 and 5.10:
∆y( ˆρ) = η∆ρ( ˆρ, ˆL) ⇒η = m
n
(5.11)
The tuning of the weights for the terms Λ(L) and Γ(L) has to be faced in a
diﬀerent way, since a theoretical law that provides their expected values at the
solution does not exist. Therefore, the order of magnitude of Λ(L) and Γ(L) when
the set of parameters is close to the solution has to be empirically evaluated. The
evaluation is performed by starting from the physical meaning of the two terms: Λ(L)
is related to the “smoothness” of the clustering of the model, while Γ(L) represents
the closeness with respect to a geological reference model. This model is obtained
as the most probable prior label distribution, considering only the given Pγ(Li) (see
Equation 4.33) at each voxel. This model will be called geological reference model
and identiﬁed with ˆLγ, hereafter. Then, the evaluation of the order of magnitude
starts from the following further empirical assumptions:
1. the geological reference model is representative of the expected “smoothness” of
clustering in the solution;
2. label distribution in the geological reference model is “not too far” from the real
solution.
According to the ﬁrst assumption, the value of Λ( ˆLγ) computed for the geolog-
ical reference model is considered representative of its value at the solution Λ( ˆL),
translating into:
Λ( ˆL) = Λ( ˆLγ)
(5.12)
80

CHAPTER 5. EMPIRICAL SETUP OF PARAMETERS
Then, recalling the equalization condition shown in Equation 5.7, the value of the
weight λ can be reckoned as:
∆y( ˆρ) = λΛ( ˆLγ) ⇒λ =
m
2Λ( ˆLγ)
(5.13)
The second assumption is used to determine the γ weight. As recalled above,
the geometry of the geological reference model represents our guess in terms of la-
bels. However, introducing also the most probable density in the model through
the mean µρ(Li) at each voxel, the computed forward signal does not usually ﬁt
the observed one. However, by means of a simple relaxation algorithm the target
function of Equation 5.6 can be optimized setting γ = 0 and the other weights to
the previously deﬁned values. Practically, the geological reference model is modi-
ﬁed according to the given “smoothness” and density variability, thus obtaining the
closest label model Γ( ˆL∗) ﬁtting the gravity observations and following the“smooth-
ness” rule. Therefore, using ˆL∗the value of Γ( ˆL∗) can be reckoned. The principle
behind this assumption is that Γ( ˆL∗) is representative of the modiﬁcation to be per-
formed to the geological reference model to ﬁt the gravity observations. Using this
value to determine the order of magnitude of Γ(L) at the solution and recalling the
equalization principle introduced in Equation 5.7, the value of parameter γ can be
determined as:
∆y( ˆρ) = γΓ( ˆL∗) ⇒γ =
m
2Γ( ˆL∗)
(5.14)
It is important to recall that this is one of the possible procedures to deﬁne the
values of the weights η, λ, and γ. The ﬁnal solution depends on these values, so the
proposed strategy aims to be only a starting point that gives the order of magnitude
of the three weights allowing all the terms present in the posterior distribution to
play an active role in the ﬁnal solution. Once the solution is computed with the
current set of weights, their values can and should be manually modulated by the
user if the solution is not satisfactory because it does not reﬂect the expectation
from the prior information.
5.3
Temperature law
The optimization of the target function E(ρ, L|y) is performed by a simulated
annealing, as largely discussed in the previous chapter (see Section 4.3). As already
explained in Section 3.1, the simulated annealing is a stochastic optimization algo-
rithm that allows to ﬁnd the global minimum of the target function by sampling
the probability distribution function PT(x) shown in Equation 4.39. The role of
temperature in Equation 4.39 is to ﬂatten the original target function for high tem-
perature values and to peak the original target function around its global minimum
at low temperature values. An example on how it acts in case of a mono-dimensional
function can be seen in Figure 5.4. Nevertheless, in order to be able to reach the
global minimum, low temperature values has to be approached very slowly. In fact,
the simulated annealing guarantees the converge to the global minimum only when
the temperature decreases with an inﬁnitesimal rate. This problem is widely treated
81

Bayesian gravity inversion by Monte Carlo methods
(a) Target function E(x)
(b) Eﬀect of the temperature law on PT (x)
Figure 5.4: Example of a mono-dimensional target function E(x), and how the simulated anneal-
ing probability function PT (x) is modulated varying the temperature from a high value (yellow)
to a low one (blue)
in literature and various temperature laws are present. However, there are basically
two cooling schedules (see Figure 5.5) that are universally known:
- geometric schedule (Walsh, 2004):
T(k) = T0αk
(5.15)
- inverse-logarithmic schedule (Geman and Geman, 1984):
T(k) =
C
log(1 + k)
(5.16)
where k is the counter of the simulated annealing iterations, C and T0 are parameters
used to set the initial value of the temperature and α set the decreasing rate. These
parameters have to be tuned based on the speciﬁc case. In fact, the concepts of high
and low temperature are relative with respect to the shape and the amplitude of the
target function.
82

CHAPTER 5. EMPIRICAL SETUP OF PARAMETERS
0
100
200
300
400
500
600
700
800
900
1000
Iterations (k)
0
1
2
3
4
5
6
T
Geometric
Inverse-logarithmic
Figure 5.5: Graph of the two universal temperature laws.
Based on its amplitude, namely the diﬀerence between the maximum and mini-
mum value of E(x), the initial temperature can be calibrated. In fact, the eﬀect of
the initial temperature should be to ﬂatten the target function, meaning that the
ratio rin between the value of PT(x) computed at the maximum and minimum of
the target function should be close to 1:
rin =
exp
n
−1
TinE(xmax)
o
exp
n
−1
TinE(xmin)
o →1
(5.17)
Therefore, the initial temperature can be computed as:
Tin = E(xmin) −E(xmax)
log(rin)
(5.18)
where rin is the above mentioned ratio, and xmax and xmin the set of parameters
that maximize and minimize the target function. The value of rin has to be assumed
the most possible close to 1, depending on how much the original function has to
be ﬂatten. A remark is that the value 1 cannot be used because for rin →1 the
initial temperature Tin →∞, thus making useless this calibration. However, rin can
be ﬁxed at a value larger than 0.9. It is worth to notice that, while performing this
operation, it is not required the knowledge of the set of parameters maximizing and
minimizing the target function, since only its expected value or, at least, its order
of magnitude at the maximum and the minimum are necessary.
Considering the ﬁnal value of the temperature a similar procedure can be applied.
In particular, the aim of the lowest temperature value is to emphasize the minimum.
Therefore, the ratio rend between the target function evaluated around the minimum
and its value at the minimum should be around 0:
rend =
exp
n
−
1
TendE(xmin + ε)
o
exp
n
−
1
TendE(xmin)
o
→0
(5.19)
83

Bayesian gravity inversion by Monte Carlo methods
Therefore, the ﬁnal temperature can be computed as:
Tend = E(xmin) −E(xmin + ε)
log(rend)
(5.20)
where rend is the above mentioned ratio, and xmin is the set of minimizing parameters
E(x) and ε is their inﬁnitesimal increment. Also in this case the value of the minimiz-
ing parameters is not required, but it is suﬃcient to deﬁne the minimum detectable
variation δE = E(xmin) −E(xmin + ε). Similarly to the initial temperature the ratio
cannot be exactly 0, thus it is convenient to ﬁx rend at a value smaller than 0.1. A
remark is that choosing δE looks very similar to selecting a convergence threshold
while optimizing by using an iterative deterministic algorithm. In particular, δE is
the translation of the convergence threshold in terms of target function.
According to Equations 5.18 and 5.20 it is necessary to evaluate the maximum
amplitude and the minimum detectable variation of the target function governing the
inversion algorithm. To perform this operation, the hypotheses introduced during
the weights tuning have to be recalled. In particular, starting from the assumption
that the reference geological model is “not too far” from the solution, this starting
point can be used to ﬁnd the closest minimum and the closest maximum of the
target function. These two values can be retrieved by means of a simple relaxation
algorithm and can be used to determine the initial temperature Tin. However, the
assumption behind this choice is that the amplitude around the geological reference
model is representative of the amplitude of the target function. It could happens that
this assumption is not true. Nevertheless, the so chosen initial temperature should
allow to move outside the concavity of the target function, where the geological
reference model is positioned.
To obtain the minimum detectable variation of the target function, we can start
again from the minimum just computed. Then, it can be perturbed by applying
to each parameter a variation of the order of the required numerical accuracy. If
the perturbation is performed variable by variable (namely at each voxel all the
possible values of the label are tested and the density is perturbed), the smallest
variation δE of the target function can be reckoned. Starting from this value the ﬁnal
temperature Tend is evaluated. In that case, the assumption behind the temperature
value determination is that the shape of the minimum close to the geological reference
model is representative of the global minimum of the target function.
When performing the above discussed temperature calibration, the two pre-
sented laws (Equations 5.15, 5.16) show some drawbacks. In particular, the inverse-
logarithmic one guarantees a good convergence to the solution (Geman and Geman,
1984) but it is very slow in reaching this state. In fact, it has only a parameter
that can be used to govern the starting temperature, while the ﬁnal temperature
value can be reached by acting only on the total number of iteration. This fact
usually leads to a huge number of iterations that, especially in the present case, is
not feasible because of the total number of unknown parameters. The geometrical
temperature law partially solves this problem, since with two parameters this tem-
perature law can start and arrive to the chosen temperature values in the chosen
number of iterations. However, this usually causes a very fast freezing due to the
high ratio between the initial and the ﬁnal temperature. To overcome this problem,
84

CHAPTER 5. EMPIRICAL SETUP OF PARAMETERS
a modiﬁed version of this law is introduced into the solution algorithm, where it is
possible to control the decreasing speed. The modiﬁed geometrical temperature law
assumes the following shape:
T(k) = Tend exp{(ak + b)c}
(5.21)
where a, b, and c are the parameters governing the shape of the descending curve,
while Tend is the temperature at the last iteration of the simulated annealing, namely
when k = kmax. Figure 5.6 shows that the geometric temperature law (dashed black
line) is a particular case of the introduced law.
To calibrate the three parameters of the temperature law presented in Equation
5.21, it is required to impose a further condition on the temperature, apart from
the initial and the ﬁnal temperature. In particular it can be useful to deﬁne the
iteration k and the temperature value Tk at the end of the transitory, namely the
temperature after which the chain is considered stable around the minimum solution
and the simulated annealing is converging. Summarizing, the three conditions used
to deﬁne the new temperature law are:
1. the temperature at the ﬁrst iteration is T(1) = Tin;
2. the temperature at the last iteration is T(kmax) = Tend;
3. the temperature at the end of the transitory of the chain is T
 k

= (1 + δ) Tend.
By applying these three conditions, it is possible to retrieve the values of the
0
200
400
600
800
1000
Iterations (k)
0
20
40
60
80
100
T
(a) Normal y-axis scale
0
200
400
600
800
1000
Iterations (k)
10−2
10−1
100
101
102
T
0.1
1
10
20
100
600
δ
(b) Logarithmic y-axis scale
Figure 5.6: Temperature schedule as a function of the iterations k, according to Equation 5.21,
assuming Tin = 100, Tend = 0.01, k = 800. δ assumes diﬀerent values in order to show how it plays
a role in the function. The black dashed line represents the standard geometrical (see Equation
5.15) law with the same initial and ﬁnal temperature.
85

Bayesian gravity inversion by Monte Carlo methods
three parameters a, b, and c as:
c =
log

log(1 + δ)
log(Tin) −log(Tend)

log
 kmax −k

−log(kmax −1)
(5.22)
b = kmax −1
kmax
log
 Tin
Tend
 1
c
(5.23)
a = −
b
kmax
(5.24)
A remark is that the values of k and δ have to be empirically tuned. However, the
length of the transitory is usually included in the range between 50% and 80% of
the total number of iterations. Finally, the value of the temperature at iteration k
has to be tuned by setting the parameter δ at a value greater than 1. It acts by
increasing or decreasing the convergence velocity. If the temperature value does not
become enough low, there is the risk that the simulated annealing solution does not
converge. Usually, this can be veriﬁed by launching the algorithm more than one
time and verifying that the convergence point remains almost the same.
86

Chapter 6
The developed software
The software implementing the presented Bayesian gravity inversion algorithm
is developed by joining Matlab and C programming languages. The combination is
directly performed into the Matlab environment by means of the MEX library API
by Mathworks. The advantage of coupling the two programming languages is that
the ﬁnal software maintains the easier Matlab data management and visualization,
upgraded with the computational speed that is typical of the compiled C code. The
performance improvement due to the C language is important when facing with
loops, that can run until ten times faster on the same machine. Moreover, while de-
veloping C functions, optimization in the memory management is allowed, thanks to
the fact that the memory allocation is directly controlled by the programmer. In the
developed software, the C routines are already optimized by using BLAS/LAPACK
libraries for matrix operations and/or OpenMP libraries for parallelization on multi-
core processors. A remark is that the parallelization is performed only for few limited
routines, due to the natural sequentiality of the Markov Chain Monte Carlo methods
used in the solution.
The chapter is organized in the following way: the ﬁrst section introduces the
structure of the software, analysing the input parameters, the possible working
modes and highlighting some critical issues. These issues are mainly related to mem-
ory management for the forward modelling and for the density covariance matrices.
Therefore, two sections are dedicated to explain how these problems are solved in
practice and which limitations or approximations the proposed solution introduces.
Finally, a fourth section describes the sampling algorithm and its implementation,
showing the core of the stochastic algorithm and the performed optimization.
6.1
Structure of the software
The software is developed by introducing a voxel regular grid into a Cartesian
coordinate system, applying the planar approximation. The regular grid is intro-
duced by deﬁning a constant step in the two horizontal directions, but leaving the
possibility to choose a ﬁxed or a variable step in the vertical one. Varying the step
size along the vertical direction can be useful to balance the smaller gravity contri-
bution by the deeper voxels with respect to the shallowest ones. In this framework,
the forward modelling is performed by means of the regular prism approximation in
87

Bayesian gravity inversion by Monte Carlo methods
terms of Newtonian gravitational attraction (see Equation 1.14). The horizontal grid
resolution is deﬁned directly by the user through the given input. In fact, the prior
information is supplied in terms of raster georeferenced 2D maps. In particular, for
each of the possible labels in the area, a map describing the value of the following
quantities at each knot of the horizontal grid has to be given:
- the most probable depth of the top surface;
- the accuracy of the given surface depth, in terms of standard deviation (1 map)
or in terms of uncertainty range (2 maps, representing the maximum and the
minimum depth allowed to the surface);
- the mean density;
- the vertical gradient of the mean density;
- the standard deviation of the density.
Using these maps it is possible to easily describe layered models (see Figure 6.1(a)).
Nevertheless, also non-layered models are allowed. This problem is solved by adding
to a layered model (used as background) some ﬁctitious maps used to described a
body, e.g. a salt dome, superimposed on the background. An example is shown
in Figure 6.1(b), where a cross-section of the four ﬁctitious surfaces deﬁning the
salt body (in red and green) is represented. Notice that, the above described maps
contain all the prior information derived from geological investigation of the region,
as previously explained in Section 5.1.
(a) Crustal structure of the Horn of Africa region
(depth magniﬁcation 20×).
(b) Salt dome in the Gulf of Mexico (depth mag-
niﬁcation 10×).
Figure 6.1: Vertical cross-section of density distribution of the two possible input geometries to
the software: (a) layered and (b) non-layered. The colorbars represents the density assigned to the
model, while black lines represent the maps given as input to generate the layered model. In (b)
red and green lines represent the trace of the maps used to generate the salt body. In particular,
here two bodies are introduced and then merged together. It is possible to see some 0 thickness
part of the salt body where it does not exist.
The ﬂow of the data inside the software is shown in Figure 6.2; it is possible
to see how the explained input maps are used to compute the diﬀerent terms, re-
quired in the deﬁnition of the prior probability. In particular, the penalty function
s2
i (Li), denoting the similarity to the geological reference model, is directly derived
by merging the information of depth and depth accuracy maps, translating them
in terms of Pγ(Li) probability (Equation 4.33). As for the mean density at each
88

CHAPTER 6. THE DEVELOPED SOFTWARE
YES
NO
Working mode
(SA, SR, SA+SR)
Simple relaxation
solution
Verify geological 
reference model
Map of 
(Li) for
each possible label
Accuracy of each 
depth map
Compute the most proba-
ble label configuration
Compute the most proba-
ble density configuration
ESTIMATED 
3D MODEL
YES
NO
SR only?
Depth map of each 
possible label
Find a starting point by 
adapting the geological 
reference model
YES
NO
Map of 
(Li) for
each possible label
Gradients of 
(Li)
for each possible label
Function q2(Li, Lj)
Define si
2(Li)
at each voxel 
Grid of gravity
observations
Compute the prism 
forward operator
Simulated annealing
solution
Length of the 
transitory
k , k (scale of 
auto-tuned weights)
SA only?
NO
Auto-calibration of
, , 
Define the 
temperature schedule
Density and/or label
constraints
Maximum number
of iterations
is the solution
satisfactory?
Update
k , k
Figure 6.2: Flow chart of the software.
89

Bayesian gravity inversion by Monte Carlo methods
voxel µρ(Li), it is derived by combining the mean density and the density gradient
maps, thus obtaining the value of the mean density for all the possible labels at each
voxel. Notice that, while applying density gradients, the mean value (given by the
density map) is assigned to the voxel at the most probable interface (given by the
depth maps), then the linear gradient is applied in both the upward and downward
direction. The standard deviation σρ(Li) associated to each density mean value is
deﬁned by assuming that the values given by the maps are constant in the vertical
direction for each of the possible labels, allowing only the lateral variations. Finally,
the last input of the prior probability, namely the penalty function q2, has to be
given directly by the user under the shape of a M × M matrix, as shown in Equa-
tion 4.36. In order to balance the terms of the prior, the automatic weights tuning
explained in Section 5.2 is implemented in the software, but the user can also man-
ually increase or decrease their value by setting two scale coeﬃcients kγ, kλ, such
that the auto-calibrated value are scaled by this two coeﬃcients, namely:
γ = kγγauto
λ = kλλauto
(6.1)
This setting is very useful when the auto-tuning procedure does not lead to a satis-
factory solution, namely a solution where the initial guessing and the gravity ﬁtting
are respected. In this case, the scale coeﬃcients can be iteratively modiﬁed until the
prior probability is satisfactory translated into the ﬁnal solution.
Once all the input are deﬁned, the solution can be retrieved by choosing between
the three following optimization procedures:
1. simulated annealing only;
2. simulated annealing followed by a simple relaxation increasing the numerical
accuracy of the solution;
3. simple relaxation only, useful to ﬁnd the closest solution to the geological refer-
ence model.
Based on the chosen procedure, it is necessary to deﬁne some parameters: when
simple relaxation is present, it is required the maximum number of iterations to
be performed if the numerical convergence is not reached, while for the simulated
annealing the input to be given are the number of iterations and the length of
the transitory, that are used to automatically compute the temperature schedule
according to Equation 5.21 and the procedure described in the previous section.
It is worth to notice that the software is able to manage also the case in which
all the labels are ﬁxed, as well as the case in which the densities are ﬁxed to their
mean value once the label is given (namely the optimization is performed by acting
only on the labels). The former can be achieved by setting the values of the depth
variability map equal to 0 everywhere, while the second by assuming σρ(Li) →0
at all the voxel for all the possible labels. However, also hybrid situations, where
only a part of labels or densities are ﬁxed, can be managed by properly combining
the given prior information. Nevertheless, the generated geological reference model
could not satisfy the constraints imposed by neighbourhood penalty function q2(Li)
or by some other constraints that can be introduced to further limit the solution
space (e.g. maximum lateral or vertical density variations). Therefore, a veriﬁcation
90

CHAPTER 6. THE DEVELOPED SOFTWARE
step is required to identify possible problems and, in case, where they are located.
These problems can be empirically ﬁxed, thus generating a valid starting point for
the optimization step. The software is equipped with a routine that performs an
auto-ﬁxing procedure. However, when launching the simple relaxation only solution
the auto-ﬁxing routine has to be carefully used. In fact, it modiﬁes the geological
reference model to obtain a new starting point, thus inﬂuencing the ﬁnal solution.
In practice, it is not guaranteed that the ﬁnal solution is the minimum closer to the
geological reference model. However, the geological reference model is modiﬁed only
where it does not satisfy all the constraints and usually this happens at very few
voxels.
Figure 6.1 shows also the order of magnitude of the investigated region exten-
sion that is typical when dealing with local gravimetric studies: 100 ÷ 1000 km in
the horizontal direction and 10 ÷ 50 km in the vertical one. Assuming resolutions
between 1 and 50 km in the horizontal direction and between 0.1 and 1 km in the
vertical one, the number of voxels describing the model can vary from some thou-
sands to some millions. These numbers can lead to memory leaking while storing the
forward matrix F or the density correlation matrix R present in the target function.
Nevertheless, thanks to the fact that the Gibbs sampler treats one voxel at a time,
only a part of these matrices is necessary at each iteration. Therefore, there are
two options: the ﬁrst is to re-compute the required terms every times, the second
is to found a smart way to obtain these terms by data reordering or by performing
simple computation only. The software is developed by following the second philos-
ophy, even if it introduces limitation, e.g. on the observation point location or on
the correlation matrix shape, as it will be shown later.
To overcome the above mentioned memory limitation and to allow the software
to run on a common workstation even in the case of millions of voxels, a numerical
optimization is required. Moreover, also numerical instability can arise, caused by
the presence of exponential operations and by the temperature that can assume
values very close to 0. These problems are solved and in the following sections the
implemented solution is explained. The ﬁnal version of the software is tested on a
common workstation, where the solution of models up to 300 × 300 × 100 voxels is
estimated in a feasible time. It is worth to recall that the software runs mainly on
one CPU core only, even when multi-core CPU are present. This is due to the fact
that the algorithm has a sequential nature, due to the structure of the chosen Monte
Carlo Markov Chains methods.
6.2
Forward modelling
As already explained before, the forward operator is modelled by means of prisms
and the chosen functional is the vertical Newtonian gravitational attraction. This
choice allows to introduce gravity anomalies observations as input. Usually gravity
data are reduced, as explained in Section 1.4, and commonly the Bouguer or the
free-air gravity anomalies are used during inversion. The software can use both, but
the correct prior information has to give as input. Practically, the topography and
the bathymetry have to be introduced in the geological model in case of the free-air
91

Bayesian gravity inversion by Monte Carlo methods
anomaly, while the correct density of the “ﬁlled” bathymetry has to be put in case of
the Bouguer anomaly. Recalling the concept of gravity anomalies, it is required to
model also the“normal Earth”generating the normal gravity, as explained in Section
1.4.1. In practice, the layered ellipsoid used ideally to compute the normal gravity
has to be known. However, changing thickness or density of the layers modiﬁes only
the mean of the observed gravity anomalies at a global scale. Notice that, at a local
scale, this is not exactly a mean but it is a trend, that should be modelled even if
it is often approximated with a mean. Due to the fact that thickness and density of
the layered ellipsoid are a convention, but they are not exactly known, basically the
problem can be faced following one to this two approaches:
- to choose a normal model, which gravitational eﬀect has to be removed from the
observations;
- to remove an“arbitrary”mean from the forward modelled signal in order to ﬁt the
observations, namely the ﬁt is performed on the similarity between the modelled
and observed signal independently from their “distance”.
In the ﬁrst case the solution strongly depends from the chosen model of normal den-
sities and its calibration is not so easy. Therefore, the second approach is applied
while developing the software. In particular, assuming that the forward modelling
should describe only the shape of the observed signal, it is possible to remove the
mean from both the observations and the forward model. The former is obtained
simply by computing and removing the sample mean from the current set of ob-
servations, while, recalling Equation 4.8, the “zero mean” forward operator F0 is
computed starting from:
y(ρ) −E[y(ρ)] = Fρ −E[Fρ] =

I −1
mee⊺

Fρ
(6.2)
and deﬁning the “zero mean” forward matrix F0 equal to:
F0 =

I −1
mee⊺

F
(6.3)
In practice, this solution does not change the likelihood shape shown in Equation
4.13, apart from the fact that the forward matrix F has to be replaced with the
above computed F0 and the mean has to be removed from the observation vector.
Usually, the voxel model resolution has to be chosen according also to the gravity
observations resolution. In fact, increasing the spatial resolution of the observations,
their sensitivity to smaller details of the subsurface increases too, still depending on
the observation accuracy. The conclusion is that the model resolution cannot be
chosen independently from signal resolution and accuracy. However, increasing the
number of voxels has an impact on the size of the forward matrix. Figure 6.3 shows
how the F matrix memory usage increases by doubling each time both observation
and model resolution. Notice that the growing ratio is not linear due to the fact
that doubling the resolution means taking 8 times the current number of voxel,
but only 4 times the number of observations.
This eﬀect is related to the fact
that the observations have a two-dimensional distribution, while the voxel model a
three-dimensional one. In other words, it is like projecting a volume onto a surface.
92

CHAPTER 6. THE DEVELOPED SOFTWARE
The outcome is that storing the forward matrix become unfeasible due to memory
limitation of common workstations (max 32 GB) or even of high-end computational
machines (max 128 GB on a single node), as shown in Figure 6.3.
Figure 6.3:
Memory used to store the for-
ward matrix related to the model size. At each
step the gravity signal has the same horizon-
tal resolution of the voxel model (namely one
observation per voxel in the horizontal plane).
The diﬀerent memory usage storing the full ma-
trix and the kernel matrix is shown (black and
green, respectively). Red dashed lines represent
common RAM equipment of workstations and
high-end computational machines (single node).
Figure 6.4: Example of the forward elements
generated by one voxel at the observation points
located on the same horizontal grid of voxel
model. Therefore, the obtained forward mod-
elling depends only on the coordinate diﬀer-
ences for all the voxel at a constant vertical co-
ordinate zo resulting invariant by translation,
namely the function represented in the ﬁgure is
the convolution kernel.
To overcome memory limitations, the strategy of reordering and combining few
elements to determine the required portion of forward matrix fi during the sampling
procedure (to be used into Equation 4.49 and the subsequent) is adopted. This
strategy avoids to compute the vector fi at each Gibbs sampler step, repeating this
operation also for the total number of iterations of the simulated annealing algorithm.
In fact, this would lead to huge requirements in terms of computational burden,
since Equation 1.14 should be evaluated for all the observation points every time.
However, the reordering algorithm introduces a limitation on the possible location
of the observations; they have to be positioned at the same horizontal location of
the knots of the voxel grid, at constant height zo, as shown in Figure 6.4. Under
this assumption, for each value of the index i3 in the vertical direction the forward
operator of a single voxel becomes invariant by translation, thanks to the constant
horizontal spacing of the grid. Therefore, recalling Equation 1.14 expressing the
gravitational attraction of a prism, the contribution of a voxel i ↔i1, i2, i3 at an
observation point j ↔j1, j2, namely the element j, i of the forward matrix, can be
expressed as:
Fji = f(xj2 −xi2, yj1 −yi1, zo −zi3)
(6.4)
Introducing also the constraints of a grid with horizontal constant spacing ∆x and
∆y, Equation 6.4 can be rewritten as:
Fji = f(∆x[j2 −i2] , ∆y[j1 −i1] , zo −zi3)
(6.5)
93

Bayesian gravity inversion by Monte Carlo methods
where the possible values assumed by k1 = [j1 −i1] and k2 = [j2 −i2] belong to the
following set:
k1 = {k ∈Z : −n1 ≤k ≤n1}
k2 = {k ∈Z : −n1 ≤k ≤n2}
(6.6)
counting 2n1 −1 and 2n2 −1 elements, respectively. By evaluating Equation 6.5 for
all the possible combinations of k1, k2 and i3, all the elements composing the forward
matrix are known. Then, by properly reordering these elements all the columns fi
of the forward matrix can be determined. Notice that, by storing only the values
above explained the memory usage strongly decreases, allowing the storage of the
forward matrix of models with very high resolutions.
A remark is that Equation 6.5 shows also that the forward function becomes
invariant by translation, therefore the computation of the forward signal of the full
voxel model at each constant depth becomes a bi-dimensional convolution summa-
tion:
yj1,j2(ρi3) =
n1
X
i1=1
n2
X
i2=1
f(∆x[j2 −i2] , ∆y[j1 −i1] , zo −zi3) ρi1,i2,i3
(6.7)
Applying the convolution theorem (Papoulis, 1977) the forward of each set of voxel
at a constant depth can be computed by means of the discrete Fourier transform,
which increases the computational speed when the full forward is required.
It is worth to notice that the explained modellization is valid only when working
with the gravitational attraction forward matrix F. However, when moving to the
“zero mean” forward F0, it is required to analyse the meaning of the terms present
in Equation 6.3. In particular it can be noticed that the term related to the mean
can be expressed as:
1
mee⊺F = ef
⊺
(6.8)
where each element i of the vector f is the mean of the associated column fi of the
forward matrix F:
 f

i = 1
m
m
X
j=1
Fji
(6.9)
Storing also this vector allows to obtain the required portion of F0 by reordering the
elements of the stored kernels and removing to the reordered values the corresponding
element of f. The memory usage when storing both the kernels and the vector f
is shown in Figure 6.3. Its reduction makes feasible the storage of the full “zero
mean” forward matrix, that is well coupled with the usage of the forward matrix
performed in the solution algorithm since at each iteration only a column of this
matrix is required.
6.3
Inverse of the density correlation matrix
When the model size increases memory problems can be also caused by the
density correlation matrix R that enters into the algorithm through its inverse R−1,
as shown in Equation 4.17 and subsequent. Due to the large number of elements,
the correlation matrix usually cannot be stored. Nevertheless, to make the software
94

CHAPTER 6. THE DEVELOPED SOFTWARE
working it is just required the knowledge of the inverse of the correlation matrix, and
in particular only the column correspondent to the current voxel is necessary at each
step of the Gibbs sampler, according to Equation 4.23. Using the inverse matrix does
not solve the memory problems, due to the fact that R−1 has the same size of R.
However, if it assumes a Toeplitz shape, the required memory drops down, because
it is suﬃcient to store its generating kernel only. Figure 6.5 shows the memory usage
when implementing the full matrix or the kernel-only solution. The latter guarantees
a much more higher number of voxels than the former. Moreover, if a limited kernel
is found, evaluating Equation 4.51 becomes lighter in terms of computational time.
30 × 30 × 6
120 × 120 × 24
210 × 210 × 42
300 × 300 × 60
Number of voxels
0.0001
0.0005
0.0020
0.0078
0.0313
0.125
0.5
2
8
32
128
512
2048
8192
32768
131072
524288
Memory Usage [GB]
Complete matrix
Kernel matrix
Figure 6.5: Memory usage by inverse of the density correlation matrix. The diﬀerent memory
usage by the full matrix and by the kernel of a Toeplitz matrix is shown (black and green, re-
spectively). Red dashed lines represent common RAM equipment of workstations and high-end
computational machines (single node).
Therefore, it is required to ﬁnd a limited Toeplitz-like inverse density correlation
matrix. Unfortunately, the inverse of a Toeplitz matrix is not Toeplitz anymore
(Schuh, 1996), meaning that the assumption about the shape of the correlation ma-
trix done in Section 4.2.2 cannot be completely satisﬁed.
Nevertheless, it exists
an approximate solution to determine the inverse of a Toeplitz matrix as another
Toeplitz matrix, with the drawback of introducing some border eﬀects, which how-
ever have a low impact, especially when the number of elements of the covariance
matrix is much higher than the correlation length.
In fact, starting from the deﬁnition of the inverse symmetric matrix:
I = RR−1 = R−1R
(6.10)
and introducing a correlation function c(τ), e.g. the one shown into Figure 6.6, it is
possible to deﬁne the R matrix as a Toeplitz matrix in the following way:
R =


c(0)
c(τ1)
c(τ2)
. . .
c(τn)
c(−τ1)
c(0)
c(τ1)
. . .
c(τn−1)
c(−τ2)
c(τ1)
c(0)
. . .
c(τn−2)
...
...
...
...
...
c(−τn)
c(−τn−1)
c(−τn−2)
. . .
c(0)


(6.11)
95

Bayesian gravity inversion by Monte Carlo methods
Figure 6.6: Covariance function
(a)
(b)
Figure 6.7: Example of mono-dimensional covariance matrix (a) and its inverse (b) derived form
the covariance function of Figure 6.6.
(a)
(b)
Figure 6.8:
Inverse of covariance matrix computed by Fourier transform (b) and associated
covariance matrix (a), derived form the covariance function of Figure 6.6 and approximating the
results of Figure 6.8. In (a) it is possible to see the magnitude of border eﬀects.
96

CHAPTER 6. THE DEVELOPED SOFTWARE
Recalling the identity shown in Equation 6.10, the product between the matrix
R and the vector composing one column of its inverse results in the corresponding
column of the identity matrix. It is worth to notice that the mentioned product can
be seen as a convolution summation, thus it can be solved by means of the Fourier
transform, applying the convolution theorem. However, the fact that R is not circu-
lant can be overcome by the symmetry property of the correlation function, namely
c(τ) = c(−τ), and by applying zero padding to the inverse vector. Disregarding the
eﬀects introduced with the zero padding and recalling that the modulus of all the
Fourier transform coeﬃcients of each vector composing the identity matrix assumes
a value equal to 1, the approximated kernel of the inverse matrix c−1(τ) can be
computed as:
c−1(τ) = F−1

1
F[c(τ)]

(6.12)
This approximation correspond to a correlation matrix R close to the original one,
apart from some border eﬀects (Schuh, 1996; Reguzzoni and Tselfes, 2009). Figures
6.7 and 6.8 show the correlation matrix and the approximated ones, when the trian-
gular correlation function (Figure 6.6) is considered. The introduced border eﬀects
can be seen by comparing Figure 6.7(a) and 6.8(a).
It is worth to notice that using the Fourier transform it is also possible to verify
that the correlation matrix is positive-deﬁnite. This can be done by verifying that all
the coeﬃcients of the kernel Fourier transform are greater or equal than 0 according
to the Wiener-Kinchin theorem (Wiener, 1964).
As for the second request, namely to have a limited inverse covariance kernel
in order to reduce computational time, it can be obtained by properly choosing
the generating covariance function (Sans`o and Schuh, 1987). For instance, when
using an exponential covariance function its inverse kernel goes asymptotically to
zero, therefore by setting an adequate threshold its dimension can be reduced by
multiplying it by a triangular function deﬁned according to this threshold. This
leads to a slightly shortening of the correlation length, with a big improvement in
terms of computational time when evaluating Equation 4.51.
The software is implemented by translating all the explained concepts in terms of
three-dimensional correlation functions and kernels, that can be managed by means
of the three-dimensional Fourier transforms. In particular, an exponential covariance
function multiplied by a triangular function is implemented. To deﬁne it two diﬀerent
correlation lengths are required: one for the horizontal and the other for the vertical
direction. By means of these two parameters the shape of the exponential and of the
triangular functions are automatically calibrated. As already explained in Section
4.2.2, the distance for computing the covariance is taken in terms of voxel grid
indexes i1, i2 and i3 instead of voxel coordinates. This choice is performed according
to the fact that the vertical grid step could be variable, thus not guaranteeing the
Toeplitz structure of the correlation matrix and not allowing its storage as explained
above. Notice that, although the product between an exponential and a triangular
function is positive deﬁnite, the software verify that the obtained covariance remains
positive deﬁnite, by means of the above mentioned Wiener-Kinchin theorem.
97

Bayesian gravity inversion by Monte Carlo methods
6.4
Sampling algorithm
The optimization algorithm requires to draw a sample of ρi and Li from their
conditional probability distributions PT(Li|ρi−, L−i, y) and PT(ρi|Li, ρi−, L−i, y) at
each iteration of the Gibbs sampler, as widely explained in Section 4.3 where Equa-
tions 4.59 and 4.62 are derived. The sampling procedure is performed by means of
the direct inversion of the cumulative distribution function of the two probabilities.
The chosen sampling method requires the knowledge of the cumulative distribu-
tion function, namely F(x), and of its inverse, namely F −1(ξ) →x, where x is a
generic random variable. In practice, the method works by drawing a sample ˆξ from
a uniform distribution U[0,1](ξ). The domain of the uniform distribution is conve-
niently taken in the range 0 ÷ 1, representing the possible values assumed by F(x).
Given the sample ˆξ and applying the known inversion formula of the cumulative
distribution function F −1(x), a sample ˆx from the desired probability distribution
can be retrieved as shown in Figure 6.9.
Figure 6.9: Direct sampling by inversion of the cumulative distribution function.
The cumulative distribution function of both labels and density can be easily
inverted. In particular, the former is a discrete variable, thus its cumulative dis-
tribution assumes the shape of a monotonically increasing step function, while the
latter has a normal shape, thus allowing the determination of the inverse cumulative
function through the inverse error function (Abramowitz and Stegun, 1964). Notice
that this is not the only method to sample a normal distribution, see e.g. Box et al
(1958), but it is convenient to manage asymmetrical truncation around the mean.
In the following, they are the numerical instabilities that could arise are evaluated
and the proposed solution is explained.
6.4.1
Labels
Each label Li is a discrete variable assuming M possible values. Therefore, by
evaluating its probability distribution PT(Li|ρi−, L−i, y) its cumulative function can
98

CHAPTER 6. THE DEVELOPED SOFTWARE
be numerically computed. As recalled above, it assumes the shape of a monoton-
ically increasing step function. Nevertheless, the normalization constant has to be
numerically computed, as explained in Equations 4.59 and 4.61. The results could
fall into numerical instability, mainly related to the exponential terms present in
PT(Li|ρi−, L−i, y). To overcome this problem, it is convenient expressing the prob-
ability distribution in the following way, starting from Equation 4.46:
PT(Li|ρ−i, L−i, y) = 1
A′′ exp{−EL(Li)}
(6.13)
where the function EL(Li) is equal to:
EL(Li) = 1
T log[σρ(Li)] + γ
2T s2
i (Li) + λ
2T
X
j∈∆i

q2(Li, Lj) + q2(Lj, Li)

+
+ η

µy −µρ(Li)
2
2T

ησ2
y + σ2
ρ
 −1
2 log

σ2
yσ2
ρ(Li)
ησ2
y + σ2
ρ(Li)

−log[J(Li)]
(6.14)
When evaluating EL(Li) it could happen that its value becomes too high to compute
the exponential function present in PT(Li|ρ−i, L−i, y). Therefore, the corresponding
probability can result equal to 0 very easily, even when double precision is used. If
the 0 happens it is necessary to distinguish between two cases: the function EL(Li)
assumes a value tending to ∞due to the two penalty functions s2
i and q2 or the
function EL(Li) assumes very high values especially related to a very low temperature
value in the ratio 1
T . In the ﬁrst case the result exp{−EL(Li)} = 0 is the correct one,
meaning that the corresponding label is practically impossible. In the second case,
the 0 is related only to numerical instability. In fact, since the temperature value
is the same for all the possible labels, it is very likely that the 0 value is obtained
for all of them. When this happens it is impossible to evaluate the constant A′′ and
consequently the probability for the diﬀerent values cannot be evaluated.
Therefore, it is required to solve this numerical instability thus allowing very low
values of the temperature. Introducing a new constant B, it could be observed that
the following relationship holds:
PT(Li|ρ−i, L−i, y) =
exp{−EL(Li)}
PM
Li=1 exp{EL(Li)}
=
=
B exp{−EL(Li)}
B PM
Li=1 exp{−EL(Li)}
=
exp{log B −EL(Li)}
PM
Li=1 exp{log B −EL(Li)}
(6.15)
By properly choosing the value of B, at least one of the M label values for which
exp{−EL(Li)} results in a value diﬀerent from 0 can be obtained. Then, assuming
that exp{log B −EL(Li)} = 1 at the most probable label, the value of the constant
is determined as:
max
Li exp{log B −EL(Li)} = 1 →log B = min
Li EL(Li)
(6.16)
that deﬁnitively solves the instability. It is worth to notice that low value of tem-
perature has to be carefully managed, avoiding the ratio
1
T goes to ∞. In fact, if
this happens there is no solution to the instability.
99

Bayesian gravity inversion by Monte Carlo methods
Looking at Equations 6.13 and 6.14, also other instabilities can arises. In par-
ticular, it can happen that the values of σρ(Li) or J(Li) goes to 0. The meaning
of an almost 0 value of σρ(Li) is that the density is ﬁxed at the mean value given
by the label. In practice, it is suﬃcient to limit its value toward 0, by assigning
it a meaningless variability of density, e.g. σρ(Li) = 0.01 kg/m3. As for the value
of J(Li), its value is computed by means of the error function, as already shown
in Equation 4.60. However, when the mean of the normal distribution is too far
from the truncation domain, the error function could assume the value of −1
2 or 1
2
at both the limiting interval, resulting into J(Li) = 0. This condition, practically,
means that the admissible domain falls into one of the two tails of the normal distri-
bution. Therefore, a very rough approximation is a horizontal straight line passing
through the value of N[µ(Li),σ2(Li)] computed at the central point of the domain.
Consequently, as required in Equation 6.14, the logarithm of J(Li) can be directly
computed as:
log[J(Li)] = log

N[µ(Li),σ2(Li)]
ρ2(Li) + ρ1(Li)
2

[ρ2(Li) −ρ1(Li)]

=
= −1
2 log

2πσ2(Li)

+ log[ρ2(Li) −ρ1(Li)] −[ρ2(Li) + ρ1(Li) −2µ(Li)]2
8σ2(Li)
(6.17)
This expression allows the rough computation of the integral, guaranteeing the sta-
bility of the solution.
6.4.2
Densities
Each sample of density ρi is drawn from the truncated normal probability shown
in Equation 4.62. As for the previous case, it is necessary to invert the cumulative
distribution function.
This is possible through the inverse of the error function.
However, when limiting the domain it is required to perform a small variation to
the sampling algorithm. Practically, the uniform sample ˆξ has to be drawn from
a uniform probability which domain reﬂects the eﬀect of the truncation, instead of
assuming it into the 0 ÷ 1 range, as:
ξ ∼UhR ρ1
−∞N[µ(Li),σ2(Li)](ρi)dρi,
R ρ2
−∞N[µ(Li),σ2(Li)](ρi)dρi
i
(6.18)
Therefore, by inverting the error function the aimed sample can be retrieved.
A remark is that, when the truncation domain is too far from the mean of the
normal, namely it is located in one of the two tails of the distribution, the evaluation
of the domains of the uniform distribution of Equation 6.18 is not possible. In other
words, the two integral, that are evaluated by means of the error function, assume
the same value. When this happens, the most probable value, namely one of the
two boundaries ρ1 or ρ2, is assumed to be the sample. This approximation has to be
introduced, since this condition usually happens when the temperature is very low
and the result has to be picked around the optimal value.
100

Chapter 7
Oil exploration test case
The aim of the example presented in this chapter is to investigate a classical
scenario that can be found in oil exploration, that is to retrieve features in the sub-
surface when seismic prospecting is not able to obtain a reliable solution. This is the
case of the presence of salt, since the migration of the seismic data does not allow
a sharp imaging of its bottom surface. This test case is performed on simulated
data with the additional purpose to better understand the eﬀects of the parameters
deﬁning the prior probability on the solution. In fact, the advantage of working in
a simulated scenario is to know the true model generating the gravitational signal.
Therefore, uncontrolled border eﬀects on the observations, caused e.g. by approx-
imations in the data reduction, or possible non-modelled density anomalies in the
surrounding of the inversion region, do not disturb the solution.
The chapter is divided in four sections. In the ﬁrst one the problem setting is
deﬁned showing the true model and the two simulated geological reference models.
The second section analyses the shape of the prior probability of labels, by testing
diﬀerent values of the parameters used in its deﬁnition. Then, the Bayesian inverse
solution is retrieved in the third section. Finally, in the last section the impact of
the density correlation matrix on the estimated solution is evaluated.
7.1
Problem setting
The chosen synthetic model is a simpliﬁed version the Hess VTI model, freely
available at http://software.seg.org/datasets/2D/Hess_VTI/.
In particular,
this model is commonly used as a simulated test case in oil exploration by seismic
imaging. However, for sake of simplicity in the presented case, the model is simpliﬁed
by reducing its horizontal extension and the total number of layers. The resulting
true model (TM, hereafter) is shown in Figure 7.1 in terms of geometries and density,
together with its gravitation attraction eﬀect.
The materials present in the model are ﬁve: water, two layers of sediments, the
salt dome and the basement. Their density is assumed to be a constant in space,
apart from the density of one of the two sediment layers, for which a vertical gradient
is introduced. An overview of the materials, their labels and densities assumed in
the TM and generating the observed gravitational signal are shown in Table 7.1.
The extension of the model is about 17 km in the principal horizontal direction
101

Bayesian gravity inversion by Monte Carlo methods
Figure 7.1: Synthetic salt dome model (TM) and its gravitational attraction.
Table 7.1: Densities of the TM model.
Material
Label
Density [kg/m3]
Water
1
1030
Sediments 1
2
2200
Sediments 2
3
2300 (top) ÷ 2600 (bottom)
Salt Dome
4
2100
Basement
5
2700
and about 9 km in the vertical one.
As for the secondary horizontal direction,
perpendicular to the principal one, the geometry and the density are assumed as a
constant, thus using a 21/2D modellization. In practice, the model is extending for
10 km in this direction, discretized by a unique voxel. The total number of voxels in
the other directions is 170 (horizontal) and 90 (vertical), assuming a voxel resolution
of 100 m × 10 km × 100 m.
The choice of the 21/2D modellization has been performed to simplify the data
visualization and to better understand the behaviour of the inversion algorithm. In
fact, the aim of this test is not to show the computational power of the developed
software, rather trying to clarify the meaning of the prior probability parameters
and how they translate the prior guess in terms of the ﬁnal solution.
As mentioned above, salt structures are very common in oil exploration. Usually,
their presence can be well detected by means of seismic imaging. However, due to
the elastic characteristics of the salt, the bottom surface of the salt dome could
102

CHAPTER 7. OIL EXPLORATION TEST CASE
result blurred. This is the case in which gravity observations can be used to ﬁll this
lack of knowledge, working in a high constrained environment. According to these
considerations, two geological reference models are deﬁned as input to the software
to perform the tests. Notice that in order to give this kind of models as input to
the software it is necessary to deﬁne a background model (black continuous lines in
Figure 7.2), a body model (red continuous lines in Figure 7.2) and their uncertainty
(dashed lines Figure 7.2), as explained in Section 6.1.
The two geological reference model, GRM1 and GRM2 hereafter, are built by
assuming that the geometry of the background is exactly known, namely it is equal
to the one of TM. Moreover also the geometry of the top of the salt dome is assumed
to be know at its true value. As for the bottom surface of the dome, it is assumed
to be in a range of ±1 km with respect to its real position into TM in both the
GRM models. However GRM1 presents this surface in its real position, while GRM2
presents a modiﬁed surface. The two surfaces are shown in Figures 7.2(a) and 7.2(b),
respectively. As for the densities, in both the models the mean value is assumed to be
the true one, even though they could have a variability. In particular, the density of
water and of the ﬁrst layer of sediments are assumed to be exactly known, while for
the second layer of sediments and the basement a standard deviation of ±50 kg/m3
is assumed and for the salt dome a standard deviation of ±20 kg/m3 is assumed,
both in the GRM1 and GRM2, as shown in Table 7.2.
(a) GRM1
(b) GRM2
Figure 7.2: Surfaces used as input for the geological reference model in the two simulated sce-
narios. Black lines represents the background sedimentary layers, while red ones the salt body.
The red dashed lines represent the given uncertainty of the salt dome basement, supposed to be
unknown.
Table 7.2: Mean density and standard deviation for both the GRM models.
Material
Mean [kg/m3]
Std. dev. [kg/m3]
Water
1030
0
Sediments 1
2200
0
Sediments 2
2300 (top) ÷ 2600 (bottom)
±50
Salt Dome
2100
±20
Basement
2700
±50
103

Bayesian gravity inversion by Monte Carlo methods
The forward modelling is performed by means of rectangular prisms (see Equation
1.14) and to simulate the gravity observations the forward signal generated by TM is
perturbed with a Gaussian white noise. According to the software requirements, the
observations have to be located at the same knot of the horizontal grid deﬁning the
voxel model. Therefore a set of 170 observations with a spacing of 100 m is deﬁned.
To set up the noise amplitude, the impact of a random perturbed density model
(according to the given standard deviation) on the forward signal is evaluated. This
leads to a signal noise standard deviation of 0.001 mGal. Notice that this value is
hardly achievable in the real measurement practice (see Section 1.3). However, for
the simulation purposes this is not so relevant and is stricly related with the size of
the salt dome and to the depth of the surface, for which gravity is not too sensible
when trying to determine its shape.
The noise added to the signal can be seen in Figure 7.3(a), since it is exactly
the residuals between observed signal and the forward signal of GRM1. On the
other hand, Figure 7.3(b) shows the typical situation of the geological reference
model, when its gravity ﬁt is some order of magnitude larger than the measurement
accuracy without a noisy behaviour.
The aim of the following tests is to evaluate the impact of the parameters gov-
erning the prior probability on the ﬁnal solution and, in particular, to understand
how these parameters are translated in terms of prior guess on geometries. This is
performed by evaluating the label prior probability by means of a sampling proce-
dure. Then, the impact of these parameters is shown in terms of inverse solution
by applying the proposed inverse algorithm and looking at the way in which they
aﬀect the ﬁnal solution. Finally, it is investigated also the contribution due to the
density correlation matrix, evaluating how it regularizes the solution and its impact
in terms of computational burden.
(a) GRM1
(b) GRM2
Figure 7.3: Mean density given as input for the geological reference model in the two simulated
scenarios. For each model, the residuals between the simulated observed gravity and the gravity
signal produce by the forward operator is reported.
104

CHAPTER 7. OIL EXPLORATION TEST CASE
7.2
Prior probability of labels
The aim of this section is to understand the meaning of the weight γ and λ in
the label prior probability, whose formulation is shown in Equation 4.31. This task
is achieved by sampling the mentioned probability with diﬀerent sets of the γ and λ
values by means of a Gibbs sampler. Each sampling chain relies on 10,000 samples,
whose ﬁrst 20% is disregarded in the ﬁnal statistics to avoid the initial training of the
chain. The following results will be presented in terms of the marginal probability
of each label at each voxel approximated by means of its relative frequency, that is
directly computed from the drawn samples.
All the tests are performed using as input the GRM2 model. The value of Pγ(Li),
used to deﬁne the s2
i (Li) function, is assumed to be, at each voxel, 0.8 for the most
probable label, namely the label derived by discretizing GRM2 into voxels, while the
residual probability of 0.2 is uniformly distributed among the other possible labels,
computed according to the uncertainty intervals. Notice that, if only one label is
possible at a certain voxel, its probability is set equal to 1. Therefore, for the GRM2
model the probability described by Pγ(L) assumes the shape shown in Figure 7.4.
(a) Water
(b) Sediments 1
(c) Sediments 2
(d) Salt Dome
(e) Basement
Figure 7.4: Translation of the GRM2 in terms of Pγ(Li). The probability of each possible label
at each voxel is shown. The horizontal axis represents the x coordinate, while the vertical one the
depth. Both are expressed in [km].
As for the q(Li, Lj) function its values can be seen as a penalty function encour-
aging or not labels to be neighbours. As already introduced in Section 4.2.2, the
function can be given as a matrix; the diagonal is chosen equal to 0, meaning that if
two voxels have the same label no penalty is paid, and it is symmetrically deﬁned,
such that q2(Li, Lj) = q2(Lj, Li). Therefore, it is deﬁned as:
q2 =


0.0000
1.3863
23.0259
23.0259
23.0259
1.3863
0.0000
1.3863
23.0259
23.0259
23.0259
1.3863
0.0000
2.4079
1.3863
23.0259
23.0259
2.4079
0.0000
41.4465
23.0259
23.0259
1.3863
41.4465
0.0000


(7.1)
105

Bayesian gravity inversion by Monte Carlo methods
Notice that the values are chosen to allow salt dome and sediments 2 to be facilitated
to be neighbour, while a very high penalty is set for the couple salt dome-basement
thus expressing a very low probability that these two materials are neighbour. All
the values present in the matrix are derived by tuning the “cliques correlation”, as
explained in Section 4.2.2.
The ﬁrst set of tests is performed by assuming γ = 0 and letting λ varies over a
set of values. Therefore, the similarity with respect to the prior model is completely
disregarded. However, when Pγ(Li) = 1, namely only one label is possible at a
voxel, this is treated as a constraint, assuming that this label is ﬁxed and cannot
be changed even though γ = 0.
Since the choice performed with the geological
reference model practically inﬂuences labels 3 (sediments 2) and 4 (salt dome), only
the relative frequency of label 4 (salt dome) is shown in the following ﬁgures. In fact,
the frequency value for label 3 corresponds to the value to be summed to frequency
of label 4 to reach the value of 1. In practice, the tests are performed starting from
the auto-calibrated value of λ, and varying its scale factor kλ (see Section 6.1) in
the range 0.1 ÷ 30, as shown in Figure 7.5. The result of these tests denotes that
by increasing the value of λ the boundary between the labels assumes a smoother
and smoother shape, until the only possible solution become the smoothest surface
allowed by the constraints. Moreover, considering the space where it is possible to
have the label equal to 4 (salt dome), it can be noticed that practically it cannot be
close to label 5 (basement), according to the prior request in the q2 function. Notice
that the reported relative frequency value represents the marginal probability of
each label, therefore it is not possible to highlight the correlation between neighbour
(a) kλ = 0.1
(b) kλ = 1
(c) kλ = 2
(d) kλ = 3
(e) kλ = 4
(f) kλ = 5
(g) kλ = 10
(h) kλ = 20
(i) kλ = 30
Figure 7.5: Prior probability of label 4, setting kγ = 0 and testing diﬀerent kλ values.
106

CHAPTER 7. OIL EXPLORATION TEST CASE
voxels that is introduced by the q2(Li, Lj) function.
The second set of tests is performed by setting λ = 0 and letting γ varies over a
set of values. Also in this case, all the tests are designed by evaluating the eﬀect of the
scale factor kγ (see Section 6.1), applied to the auto-calibrated γ value. The results
show that practically the sampled probability has the same shape of the Pγ(Li),
apart from the fact that the chosen value of 0.8 assigned to the most probable label
is modulated according to the choice of the weight, as shown in Figure 7.6. The
outcome is that the probability starts from a uniform distribution at low values of
kγ, giving the same probability to all the labels for all the not constrained voxels,
and reaches a state, for high value of kγ (see Section 6.1), where the only admissible
solution is exactly the GRM2.
(a) kγ = 0.1
(b) kγ = 1
(c) kγ = 2
(d) kγ = 3
(e) kγ = 4
(f) kγ = 5
(g) kγ = 7
(h) kγ = 10
(i) kγ = 20
Figure 7.6: Prior probability of label 4, setting kλ = 0 and testing diﬀerent kγ values.
The last set of tests is performed by evaluating the eﬀect of the variation of both
the λ and γ values at the same time. Also in this case the variation is evaluated
by applying the scale factor kγ and kλ to the auto-calibrated values of the γ and
λ parameters. The results of some combinations of the two can be seen in Figure
7.7. They show that the two eﬀects shown in the previous cases are combined based
on the power of the two scale factors. Practically, the most probable solution at
each couple of scale factors is a shape that is similar to the GRM2, with diﬀerent
smoothing levels. In particular, increasing the kγ value the probability leads to a
solution less smooth and closer to the GRM2, while increasing the kλ weight the
solution leads practically to a smoothed version of the GRM2. It is also possible to
see that, when kλ is much higher than kγ, the contribution of s2
i (Li) is practically
107

Bayesian gravity inversion by Monte Carlo methods
disregarded and only the clustering eﬀect of the q2(Li, Lj) remains, thus coming back
to the situation proposed in the ﬁrst set of tests.
A ﬁnal remark is that it is diﬃcult to tune the value of the weights, due to their
non-linear behaviour. Moreover, it could happen that when they are combined with
the likelihood, the weights are not suﬃcient to reach the aimed degree of clustering of
the voxels and of ﬁtting with respect to the geological reference model. Therefore, the
tests here presented could be useful to understand how to set this weights based on
the expected prior probability. Nevertheless, it is worth to remember that to obtain
the desired results in the solution, also a further balancing factor with respect to the
likelihood could be required. In the following section this situation will be analysed,
to better understand how to work with the calibration of these scale factors.
(a) kγ = 1, kλ = 1
(b) kγ = 1, kλ = 2
(c) kγ = 1, kλ = 1.5
(d) kγ = 1, kλ = 1.2
(e) kγ = 0.5, kλ = 1.2
(f) kγ = 0.5, kλ = 2
(g) kγ = 0.5, kλ = 3
(h) kγ = 0.5, kλ = 5
(i) kγ = 0.5, kλ = 10
(j) kγ = 0.5, kλ = 20
(k) kγ = 0.5, kλ = 30
(l) kγ = 1, kλ = 30
(m) kγ = 2, kλ = 30
(n) kγ = 3, kλ = 30
(o) kγ = 3, kλ = 50
Figure 7.7: Prior probability of label 4, evaluating diﬀerent combination of kγ and kλ values.
108

CHAPTER 7. OIL EXPLORATION TEST CASE
7.3
Inverse solution
To compute the inverse solution a series of tests has been performed. In particu-
lar, the solution should be driven to the best one, by acting on the values of kγ and
kλ that modify the impact of the prior probability, as shown in the previous section.
All the tests in this section will be performed assuming that the weights η, γ and λ
assume the auto-calibrated values, while the penalty functions s2
i (Li) and q2(Li) are
the one shown in the previous section, according to Figure 7.4 (in case of GRM2)
and Equation 7.1, respectively. The s2
i (Li) function for the GRM1 is determined
in the same way of the one of GRM2 in the previous section. Therefore, it looks
similar to the one of Figure 7.4, but reporting the shape of the salt dome deﬁned for
GRM1, shown in Figure 7.2(a). Moreover, the initial and ﬁnal temperature values
are automatically computed by the software, while the total number of iterations
is set to 10,000 for both the simulated annealing and the subsequent simple relax-
ation. The transitory length, namely the value of the iteration k used to determine
the temperature law (see Section 5.3) is ﬁxed at the 80% of the total number of
iterations.
All the solutions presented in this section and in the following section are com-
puted starting from both the GRM1 and the GRM2 as input. Moreover, the ﬁrst
set of solution is retrieved by assuming the density correlation matrix equal to the
an identity matrix, thus not aﬀecting the computational speed. The impact of this
matrix will be investigated in the next section, in terms both of quality improvement
of the solution and computational burden.
The ﬁrst test is performed by assuming that both kγ and kλ are equal to 1,
namely this solution is the one that can be automatically computed with the smallest
interaction possible with the software. The results obtained starting from the GRM1
and the GRM2 can be seen in Figure 7.8.
Unfortunately, these results cannot be considered satisfactory. In fact, it seems
that in both the cases the terms of the target function accounting for the closeness
to the geological reference model and for the model clusterization are not playing
an active role during the optimization procedure, while the gravity signal ﬁtting is
acceptable according to the given accuracy. In fact, it can be seen in Figures 7.9(a)
and 7.9(b) that the gravity residuals are compatible with the 1 µGal simulated noise
in both the cases, presenting a standard deviation of the same order of magnitude.
Notice that the zero mean of the ﬁt is related to the choice explained in Section 6.2
about the “zero mean” forward matrix.
As for the simulated annealing evolution, Figures 7.10(a) and 7.10(c) show that
the minimization of the target function seems to work correctly.
Unfortunately,
looking to Figure 7.8 the clusterization requested by the prior label probability is
not reasonably respected. Therefore, it is necessary to investigate why this problem
arises. A possible answer can be found in Figures 7.10(b) and 7.10(d), where the
four components of the target function (see Equations 5.2, 5.3, 5.4 and 5.5) are
represented, each of them already scaled by the corresponding weight. It can be
noticed that at the beginning of the chain, namely when the temperature is high,
the value of the γΓ and λΛ are very small if compared with the one of ∆y. Practically,
during this phase, whose aim is to draw samples approaching the minimum of the
109

Bayesian gravity inversion by Monte Carlo methods
(a) GRM1
(b) GRM2
Figure 7.8: Label and density at the solution computed with kγ = 1 and kλ = 1.
(a) GRM1
(b) GRM2
Figure 7.9: Gravity residuals of the solution computed with kγ = 1 and kλ = 1.
target function, the ∆y term is the only contribution inﬂuencing the optimization
procedure. Therefore, it is possible that the instability and the non-uniqueness of the
inverse gravimetric problem do not allow to correctly reach the minimum. In fact,
the non-uniqueness of the problem allows to visit a lot of states very far from the
optimal prior label realization, but satisfying the gravity ﬁt term, that practically
is minimized disregarding the contribution of the prior probability. Probably, when
the terms γΓ and λΛ assume an order of magnitude comparable with the one of ∆y,
it could be too late to allow signiﬁcant changes in the label distribution, since the
temperature is no more high enough.
To overcome this problem, a possible solution is to modify to the auto-calibrated
parameters, by acting on the weight kγ and kλ. This allows to change the order of
magnitude of the label related components in the target function, allowing them to
play an active role in the minimization also when temperature is high. To achieve
this task a series of tests is performed and the two scale factors are increased until
the gravity ﬁt remains at an acceptable level according to the observation noise. The
obtained scale factors combination is kγ = 1000 and kλ = 1000 in both the cases of
the GRM1 and GRM2 input. The solutions retrieved by using these values of the
110

CHAPTER 7. OIL EXPLORATION TEST CASE
(a) Evolution of the target function (GRM1).
(b) Evolution of the target function components
(GRM1). Logarithmic scale is used for E(x).
(c) Evolution of the target function (GRM2).
(d) Evolution of the target function components
(GRM2). Logarithmic scale is used for E(x).
Figure 7.10: Evolution of the target function of the solution computed with kγ = 1 and kλ = 1.
scale factors can be seen in Figure 7.11.
Those solutions satisfy the expectation of the prior probability in terms of ge-
ometry. As stated before, also the gravity ﬁtting remains at an acceptable level
according to the noise standard deviation, as shown in Figures 7.12(a) and 7.12(b).
In particular, it can be observed that increasing the weight of the label prior prob-
ability, no worsening is introduced in the gravity ﬁt with respect to the previous
solutions. Also the minimization procedure seems to correctly work by looking at
(a) GRM1
(b) GRM2
Figure 7.11: Label and density at the solution computed with kγ = 1000 and kλ = 1000.
111

Bayesian gravity inversion by Monte Carlo methods
(a) GRM1
(b) GRM2
Figure 7.12: Gravity residuals of solution computed with kγ = 1000 and kλ = 1000.
Figures 7.13(a) and 7.13(c). In fact, the target function is globally minimized. How-
ever, Figures 7.13(b) and 7.13(d) clearly show that the obtained new solution is no
more equalized at the equilibrium, but it can be seen that all the components of the
target function play an active role during the optimization process. In fact, each of
them follows a minimization trend.
(a) Evolution of the target function (GRM1).
(b) Evolution of the target function components
(GRM1). Logarithmic scale is used for E(x).
(c) Evolution of the target function (GRM2).
(d) Evolution of the target function components
(GRM2). Logarithmic scale is used for E(x).
Figure 7.13: Evolution of the target function of the solution computed with kγ = 1000 and
kλ = 1000.
A remark is that the algorithm acts by following the expected prior behaviour;
in practice, in each case the shape of the salt dome is retrieved according to the
shape of the geological reference model to which only very few smoothing is applied.
In fact, the most probable shape allows a density realization that ﬁts the gravity
signal according to the given density standard deviation for both GRM1 and GRM2.
This eﬀect is the power of the method, namely it aims to be a replacement of a
trial-and-error approach, due to the fact that it adapts the supposed prior model
to the ﬁt of the gravity. In the presented cases the admissible density variation
112

CHAPTER 7. OIL EXPLORATION TEST CASE
given as input, namely its standard deviation, is not enough restrictive to limit the
non-uniqueness of the solution. In fact, in both the cases a shape practically identical
to the geological reference model is allowed, while maintaining an acceptable gravity
ﬁtting.
Nevertheless, the densities assume a noisy realization, as it can be observed in
Figures 7.8 and 7.11. This eﬀect is due to the fact that no limits on the density
variation among neighbour voxels is introduced in the density prior probability.
This problem can be partially solved by introducing the density correlation matrix,
as explained in Equation 4.17. Its impact will be evaluated in the next section.
7.4
Eﬀect of the density correlation matrix
A series of test is performed to evaluate the eﬀect of the density correlation
matrix on the results. As explained in Section 6.3, the inverse of the correlation
matrix is approximated with a Toeplitz matrix, allowing to store only the kernel in
memory. The approximate correlation functions used to derive the inverse kernel
are shown in Figure 7.14. The correlation matrix is built as a combination of the
two exponential functions, describing the correlation in the horizontal and vertical
directions. All the solutions that will be computed in this section use this correlation
matrix, in the case of both GRM1 and GRM2 input. The other parameters of the
software, namely the temperature law and the prior input, are chosen at the same
values of the previous section.
As in the previous section, the ﬁrst solution is computed by setting the value of
kγ and kλ at 1. The obtained results are shown in Figure 7.15. The gravity residuals
are still acceptable, as shown in Figures 7.16(a) and 7.16(b), and the minimization
seems to be eﬀective, according to Figures 7.17(a) and 7.17(c). Nevertheless, the
problems in terms of geometry still remain, even if a smoother density distribution
can be noticed, especially in the upper part of the sediment 2 layer. This is the
only eﬀect of the correlation matrix, that however could not inﬂuence the label
distribution, because it is label independent, due to the cited memory problems.
Therefore, the same problems found in the case without the density correlation
matrix arise. In fact, looking at Figures 7.17(b) and 7.17(d) the terms related to
the labels do not play an active role in the optimization process, probably due to
the non-uniqueness and ill-conditioning of the inverse problem, again. On the other
(a) Horizontal
(b) Vertical
Figure 7.14: Correlation function used to generate the correlation matrix R and its inverse R−1.
113

Bayesian gravity inversion by Monte Carlo methods
(a) GRM1
(b) GRM2
Figure 7.15: Label and density at the solution computed with kγ = 1 and kλ = 1 and using the
density correlation matrix.
(a) GRM1
(b) GRM2
Figure 7.16: Gravity residuals of solution computed with kγ = 1 and kλ = 1 and with the density
correlation matrix.
hand, it is possible to see how the eﬀect of the correlation matrix allows the term η∆ρ
to play a more important role, due to its magnitude that increases for the presence
of the correlation terms.
To avoid the instability problems, the strategy of acting on the two scale factors
kγ and kλ is adopted again. For sake of simplicity, the same values chosen before
are tested, verifying that the gravity ﬁt remains acceptable.
As in the previous
case, both the terms related to the labels play an active role when the two scale
factors assume the value 1000. Therefore, the corresponding solution in terms of
densities and labels is shown in Figure 7.18. Their behaviour in terms of geometry
is very similar to the case without correlation. In fact, the estimated geometry is
practically the one given by the geological reference model, considering both GRM1
and GRM2 as input. This happens because it exists a density model that allows an
acceptable gravity ﬁt with this geometry, even when the density correlation matrix
is introduced. The ﬁt obtained in the two cases can be seen in Figures 7.19(a) and
7.19(b), which reveals that the standard deviation of the residuals remains of the
114

CHAPTER 7. OIL EXPLORATION TEST CASE
(a) Evolution of the target function (GRM1).
(b) Evolution of the target function components
(GRM1). Logarithmic scale is used for E(x).
(c) Evolution of the target function (GRM2).
(d) Evolution of the target function components
(GRM2). Logarithmic scale is used for E(x).
Figure 7.17: Evolution of the target function of the solution computed with kγ = 1 and kλ = 1
and with the density correlation matrix.
order of 1 µGal, according to the simulated signal noise.
As it happens in the previous case with kγ and kλ equal to 1, the eﬀect of the
density correlation matrix is visible in the estimated densities. In fact, the smoothing
eﬀect introduced can be clearly noticed by comparing Figure 7.18 and Figure 7.11.
In particular, it can be seen in the upper part of the background layer of sediment
(a) GRM1
(b) GRM2
Figure 7.18: Label and density at the solution computed with kγ = 1000 and kλ = 1000 and
using the density correlation matrix.
115

Bayesian gravity inversion by Monte Carlo methods
(a) GRM1
(b) GRM2
Figure 7.19: Gravity residuals of solution computed with kγ = 1000 and kλ = 1000 and with the
density correlation matrix.
(a) Evolution of the target function (GRM1).
(b) Evolution of the target function components
(GRM1). Logarithmic scale is used for E(x).
(c) Evolution of the target function (GRM2).
(d) Evolution of the target function components
(GRM2). Logarithmic scale is used for E(x).
Figure 7.20: Evolution of the target function of the solution computed with kγ = 1000 and
kλ = 1000 and with the density correlation matrix.
(sediment 2), where the noisy density estimation resulting in the case without the
correlation matrix is mitigated. This is an important task, since the previous density
solution is not physical and assumes that shape to ﬁt also the noise of gravity signal.
In practice, the density covariance matrix acts as a smoothing operator, as expected.
A remark is that the solution is not able to retrieve a surface of the bottom of the
salt dome that is diﬀerent from the geological reference model. In fact, the solution
with the same geometry of this model and a modiﬁed density realization is allowed
by the prior probability. Therefore, the solution cannot be diﬀerent from the prior
guess, conﬁrming that the method is working correctly.
The last remark is about the computational time. The solution here presented
take about 10 minutes to be computed in the case without the correlation matrix.
A problem arises when introducing the correlation matrix. In fact, in this case the
116

CHAPTER 7. OIL EXPLORATION TEST CASE
computational time grows up to about 6 hours, making very annoying to retrieve
the solution. This is mainly due to the computation of the products performed in
Equation 4.51, that has to be computed at each iteration. However, this problem can
be solved by choosing a covariance matrix with a limited inverse matrix, allowing
to reduce the dimensions of this product, thanks to the zeros value present in the
inverse matrix itself. An alternative strategy could be to introduce some constraints
on the values of neighbour densities. This second strategy will be followed in the
test case on real data, allowing to reduce the computational time and to obtain a
smooth density solution, as it will be presented in the next chapter.
117

118

Chapter 8
Crustal determination test case
The case study presented in this chapter aims to investigate the crustal struc-
ture of the Guandgong region, in the southeastern part of China. The inversion is
performed by using a real dataset, showing the kind of data available as geological
and geophysical information and how they can be converted into a prior probability.
Moreover, when moving to a real case, it is also required to manage border eﬀects,
mainly caused by the fact that each point on the Earth feels the gravitational at-
traction of the rest of the world, and to tune the parameters governing the solution,
according to the qualitative expectations given by the geological information. In
particular, the gravity data to be inverted derives from a GOCE global model, while
the a-priori information is taken from geological and seismic data available from
literature.
The chapter is structured in seven sections. The ﬁrst one (Section 8.1) explains
why a crustal model of the area is required. Then, in Section 8.2, the geometrical
discretization and the observed gravity signal are presented.
The following two
sections, namely Sections 8.3 and 8.4, are dedicated to a review of the geological
and geophysical data already available from literature in the region. Then, Section
8.5 shows how the geophysical and geological information is introduced in the prior
probability. Finally, the Bayesian solution is retrieved (Section 8.6) and compared
with the one obtained by a classical approach (Section 8.7).
8.1
Motivation
Increasing the knowledge of the crustal structure of this area is required to inves-
tigate the geoneutrino ﬂux from the inner Earth layers at the Jiangmen Underground
Neutrino Observatory (JUNO), currently under construction in Kaiping, Jiangmen,
Guangdong Province (south China).
Geoneutrinos are electron antineutrinos emitted in beta minus decays along the
238U and 232Th decay chains: they propagate almost without interacting, provid-
ing instantaneous insights on the radiogenic heat power sustaining the Earth’s dy-
namo.
By measuring their ﬂux and energy spectrum, it is possible to infer the
global amount, distribution and ratio of 238U and 232Th in the crust and in the man-
tle, essential ingredients for the discrimination among diﬀerent bulk silicate Earth
compositional models (ˇSr´amek et al, 2013). Recent measurements from the Kam-
119

Bayesian gravity inversion by Monte Carlo methods
LAND (Japan) (KamLAND Collaboration, 2013) and Borexino (Italy) (Borexino
Collaboration, 2015) experiments are opening the way to multiple-sites geoneutrino
studies aimed at distinguishing the site-dependent crustal components (∼75% of the
signal) from the almost constant mantle component (∼25% of the signal) (Fiorentini
et al, 2007). Therefore, a crustal structure model is required to distinguish between
the mantle and the crustal contributions in the observed geoneutrino ﬂux.
The present crustal study is part of a multidisciplinary collaboration between
the Politecnico di Milano (Department of Civil and Environmental Engineering),
its spin-oﬀGReD s.r.l., and the University of Ferrara (Department of Physics and
Earth Science).1 During the project, the presented inversion algorithm is applied to
determine the best crustal model according to the prior information given by the lit-
erature and the homogeously observed gravity signal. Then, the crustal contribution
to the geoneutrino ﬂux will be evaluated according to the estimated model.
8.2
Gravity signal and 3D voxel model
The extension and the resolution of the three-dimensional voxel model are cali-
brated according to the sensibility of the geoneutrino ﬂux computation: in particular,
a horizontal resolution of about 50 km × 50 km and an area of at least 2° around
the reactor position are the requirements for the geoneutrino modellization.
Therefore, the geometrical resolution of the voxel model has been ﬁxed to a
horizontal resolution of 50 km × 50 km and a vertical one of 100 m. The horizontal
resolution is designed according to the above explained requirements for geoneutrino
ﬂux computation, while the vertical one is chosen as a trade-oﬀbetween gravity
sensibility and expected variability of the sediment boundary surfaces, as it will be
explained afterwards.
The observations to be inverted are the gravity anomalies synthetized from a
global gravity model. A requirement of the developed software is that the ground
resolution of the signal has to be the same of the voxel model at a constant altitude
(see Section 6.2).
Consequently, the gravity signal is computed at a horizontal
resolution of 50 km and at an ellipsoidal height of 600 m, namely the altitude closest
to the terrain but outside masses, thus fulﬁlling the two requirements.
The global gravity model is chosen between a satellite-only solution, with the
advantage of being computed by homogeneous data, and a combined solution, having
a higher spectral resolution.
In particular, the latest release of the GOCE-only
space-wise model up to degree and order 330 (Reguzzoni and Tselfes, 2009) and the
combined EIGEN-6C4 model up to degree and order 2190 (F¨orste et al, 2014) are
1The project is jointly developed by the two research groups. In the present thesis work also a
part of the work done at the University of Ferrara is reported, because it is preliminary to the gravity
inversion and it is useful to understand the possible prior information that can be introduced in
the presented Bayesian inversion algorithm. In particular, this activity is presented in Sections 8.3
and 8.4 and is related to the geological setting of the area and to the raw geophysical data selection
and interpretation. Then this information is translated in terms of a geological reference model
to be given as input of the inversion software. Notice that the conversion of the pre-elaborated
prior data into a prior probability and their inversion is part of this thesis work. The results here
presented are not the deﬁnitive ones, since not all the available geophysical information is already
introduced in this solution.
120

CHAPTER 8. CRUSTAL DETERMINATION TEST CASE
considered. The choice of the signal to be used for the inversion is performed by
comparing the empirical covariance of the gravity anomalies synthetized from these
two models (up to diﬀerent degrees and orders) with the one of the forward signal
generated by the geological reference model of the region, whose construction will be
explained in the following. To avoid the introduction of useless high frequencies that
cannot be interpreted by a voxel model with the given geometrical resolution, the
observed signal and the one generated by the geological reference model should have
a similar stochastic behaviour. Figure 8.1 shows that the correlation length of the
GOCE-only space-wise model truncated at degree and order 200 is very similar to
the one of the signal generated from the a-priori model, therefore this is the chosen
model for the observation synthesis. The accuracy of the chosen observed signal
can be recovered from the model itself and it is homogeneously distributed with a
standard deviation of about 1 mGal.
Figure 8.1: Empirical covariance of the gravity anomalies synthetized from two global models
truncated at diﬀerent degrees and orders, compared to the empirical covariance of the forward
signal of the a-priori model.
The correlation length was used as the parameter to choose the
adopted model for the computation of the signal to be inverted
The studied region around the JUNO detector, including the investigated volume
by the inverse problem, has an extension of 3° in west and east direction and of
2° in north and south direction around the detector itself, according to the above
introduced requirements for the geoneutrino ﬂux computation. Consequently, the
total extension of the target region is about 6°×4°, corresponding to the yellow area
marked with (1) in Figure 8.2. Moreover, also other two regions are deﬁned. Each
of them has a border of 3° around previous one, thus extending the 6° × 4° region to
a 18° × 16° one. The two border regions just deﬁned are marked with (2) and (3) in
Figure 8.2.
The gravity signal is synthetized from the global model only on points overlying
the area (1) centred at JUNO. Nevertheless, to avoid border problems the region
included in the investigated volume is enlarged up to the region (2). This choice al-
lows to retrieve the solution by acting on the variables (label and density) associated
to voxels falling inside both (1) and (2) regions. The last border, namely the region
121

Bayesian gravity inversion by Monte Carlo methods
Figure 8.2: Total extension of the 3D voxel model.
Three diﬀerent areas are identiﬁed: (1)
the investigated region to be inverted, (2) the region inverted to compensate border eﬀects of the
gravity signal, (3) region considered known from a global crustal model used to reduce the signal.
(3), is used only to reduce the observed signal for the eﬀect of the “external” world,
since the masses farther than this border cause variations in the observed signal
much smaller than its accuracy. Therefore, the voxels present in the most external
region do not enter in the solution algorithm as unknown variables. In fact, their
associated variables (label and density) are ﬁxed to an a-priori value, taken from a
global crustal model. In practice, the observed signal is reduced according to the
forward contribution of region (3) and in the inversion software only the ﬁrst voxel of
the border between (2) and (3) is introduced. Its presence is a requirement in order
to obtain a ﬁnal solution that is smoothly joined to the chosen global crustal model,
according to the given prior probability. The eﬀect of the explained reduction, by
introducing the global crustal model can be seen in Figure 8.3.
Notice that the synthetized gravity signal is the free-air gravity anomaly ex-
plained in Section 1.4. This means that the signal is not reduced for the eﬀect of
the topography and the bathymetry, that are directly considered in the voxel model
(a) GOCE gravity anomaly
(b) Reduced gravity anomaly
Figure 8.3: Observed gravity signal from the GOCE space-wise model release 5 (SPWR5), with
and without the removed border eﬀects. The horizontal resolution is 50 km × 50 km at 600 m
height. The map includes areas (1) and (2), but the signal overlies only area (1).
122

CHAPTER 8. CRUSTAL DETERMINATION TEST CASE
to correctly interpret the gravity observations.
8.3
Geological setting of the area
The study area is located in the southeast of China and includes mainland and
the bordering sea, as shown in Figure 8.4. It comprises the Guangdong region and
the southeastern part of Guangxi region. In the southeastern sector, it includes the
northern margin of the South China Sea. It is a part of the South China Block (SCB)
that has a complex tectonic history (John et al, 1990; Li, 1994; Zeng et al, 1997) with
a composition and thickness poorly understood (Zheng and Zhang, 2007). SCB is
composed of two collided Neoproterozoic continental crustal blocks (He et al, 2013):
the Yangtze, in the northwest sector, which forms a stable cratonic area, and the
Cathaysia (hereafter CB) in the southeast (Xu et al, 2007) that includes the study
area. The boundary between these blocks is still an object of debate (He et al, 2013;
Deng et al, 2014; Xia et al, 2015).
The CB is characterized by Mesozoic and Cenozoic tectono-magmatic events, it
consists of Palaeo and Mesoproterozoic intensely folded basement rocks (gneisses,
amphibolites and migmatites) with superimposed Mesozoic and early Cenozoic vol-
canism and granitic intrusions (a total area of about 220,000 km2), covered by
Sinian to Mesozoic sedimentary and volcanic rocks. The granitoids of CB have inter-
ested various tectonic settings, with heterogeneous sources and repeated processes of
crustal melting, mixing and fractional crystallization (Chen and Jahn, 1998; Jiang
et al, 2009; Wang et al, 2010). In the southeastern sector, it comprises the north-
eastern part of the South China sea (Pearl River Mouth Basin) mainly characterized
by the CB oﬀshore continental margin. Starting from the northwest to the south-
east, the continental crust is characterized by a lateral variations in thickness and
composition, and also in P-wave velocity, reaching a transition zone which continues
until the oceanic crust of the South China Sea (Li et al, 2007). The crust exhibits
a seismogenic layer distribution into upper, middle, and lower crusts and refers to
a felsic and intermediate composition (Li et al, 2007), mainly for the continental
sector. In the transition zone, toward the oceanic crust, that corresponds to the
southeast area of the region (1), this distinction is less recognizable. The CB shows
a general younger trend, from inland to coast, and an increasing of the intrusion
(mainly in the upper portion of the crust) moving from northwest to northeast. The
CB can be subdivided in three parts by two distinct tectonic regional elements (see
Figure 8.4), the Shi-Hang Zone (SHZ) and the Lishui-Haifeng Fault (LHF). From
northwest to southeast they are: the Cathaysia Interior (CI), the Cathaysia Folded
Belt (CFB) and the Southeast Coast Magmatic Belt (SCMB) (Chen et al, 2008; Xia
et al, 2015). The SHZ has been interpreted as an intra-arc rift (back-arc extensional
zone related to the paleo-Paciﬁc plate subduction) that has aﬀected the middle to
late Jurassic felsic and maﬁc magmatism in southeast China. It played an important
role in the reworking of the crust and lithosphere in the study region (Jiang et al,
2009; Xia et al, 2015). Together with the high angle strike slip fault (LHF), these
elements appear to be a discriminating factor for the distribution of Mesozoic mag-
matic rocks. The Triassic granites are mainly distributed in the CI and CFB, the
123

Bayesian gravity inversion by Monte Carlo methods
Figure 8.4: a) Location map of the study area; b) simpliﬁed tectonic map where the JUNO
detector will be located (green star), showing the tectonic partition of the Cathaysia Block in three
parts; the six 2° × 2° tiles, outlined by green square and centred at JUNO, cover the region (1); c)
schematic crustal cross-section showing the seismogenic vertical layer distribution inferred by the
input seismic data with the average P-wave velocity, that has a lateral variation until the transition
zone (TZ) towards the oceanic crust. The top layer (parallel black lines) represent the sedimentary
cover (Sinian-Mesozoic). The colored triangles highlighted the increasing and a general younger
trend of the intrusion from northwest to southeast, in grey the Jurassic and in red the Cretaceous.
Cretaceous granitoids in the SCMB and the Jurassic rocks in the CFB (Chen et al,
2008). The SCMB consists of intermediate to maﬁc compositions, compared to the
felsic compositions in the CFB and the CI (Xia et al, 2015). It can be emphasized
that CB is characterized by the exposition of widespread Mesozoic granitic and vol-
canic rocks, particularly in the coastal area and by the slightly decrease degree of
acidity moving from west to east.
8.4
Geophysical input
The constraints for the deﬁnition of the geological reference model are obtained
from published studies, including Deep Seismic Sounding proﬁles (DSS), Receiver
Functions (RF), teleseismic P-wave velocity models (virtual sections, VS) and Moho
depth maps (MM). The ﬁnal model will be described by three layers overlying the
mantle, namely Lower Crust (LC), Middle Crust (MC) and Upper Crust (UC).
124

CHAPTER 8. CRUSTAL DETERMINATION TEST CASE
Therefore, three discontinuities are deﬁned: the Moho Depth (MD), the Top of the
Lower Crust (TLC) and the Top of the Middle Crust (TMC), recognized in terms of
diﬀerent P-wave velocity values. Moreover, also information derived from a global
crustal model (GM) is used to have at least as a raw information where no local
measurements are present.
Figure 8.5: Input geophysical data used for the construction of the 3D model in the 6° × 4°
area (1) centered at the JUNO detector location. Deep seismic sounding proﬁles, P-wave velocity
models proﬁles and locations of seismograph stations reported in the ﬁgure correspond to the input
seismic data used to build the a-priori model for the inversion of gravimetric data. The observed
gravity anomalies and the limit of the three areas (black dot-dashed lines) above explained are also
represented. The red dot-dashed lines and the red circle identify the cross-section AA′ and the
point B that will be used afterwards.
The discontinuities depth and P-wave velocity for the diﬀerent crustal layers are
obtained from DSS by digitizing approximately 900 depth-controlling points, for 7
seismic proﬁles, as shown in Figure 8.5. The P-wave velocity contours for each model
were used as benchmarks for the depth of the modelled geophysical surfaces. The MD
uncertainty (1σ) is estimated by considering the picking error from each work. Then,
a conservative value of the MD uncertainty is obtained by assuming a maximum
possible variation equal to ±3σ. As for the TMC and TLC, two quality classes A1
and A2 are deﬁned based on their clarity during the digitalization. Consequently,
the uncertainty of TMC and TLC is obtained by multiplying the MD uncertainty
by 1.5 for A2 and by 1.2 for A1. The ﬁnal accuracies of the diﬀerent kinds of data
are reported in Table 8.1.
In the following paragraphs the diﬀerent kinds of input are analysed and their
geophysical interpretation in terms of depth is brieﬂy explained. Notice that, after
this step, it is required to merge all the obtained depth information into a unique
geological reference model. This last step will be explained in the next section.
125

Bayesian gravity inversion by Monte Carlo methods
Table 8.1: Depth uncertainty (±3σ) in [km] for the 3 discontinuities used to parameterize the
crust.
Input
MD
[km]
TLC
[km]
TMC
[km]
Reference
DSS
LG
3.9
4.7A1
4.7A1
Zhang and Wang (2007)
OOS2004
4.5
6.8A2
5.4A1
Xia et al (2010)
Baiyan J
2.4
2.9A1
2.9A1
Jia et al (2006)
ESP 1 9
3
4.5A2
4.5A2
Nissen et al (1995)
OBS1993
4.5
6.8A2
5.4A1
Xia et al (2010)
ESP 11 17
3
4.5A2
4.5A2
Nissen et al (1995)
OBHIV
3.6
4.3A1
5.4A2
Qiu et al (2001)
RF
QIZ
2.1
-
-
Tkalˇci´c et al (2011)
GZH
4.8
-
-
Tkalˇci´c et al (2011)
SZN
4.8
-
-
Tkalˇci´c et al (2011)
GUL
1.8
-
-
Tkalˇci´c et al (2011)
GYA
1.5
-
-
Tkalˇci´c et al (2011)
CNS
2.5
-
-
Tkalˇci´c et al (2011)
NNC
5.1
-
-
Tkalˇci´c et al (2011)
QZH
1.5
-
-
Tkalˇci´c et al (2011)
WZH
8.5
-
-
Tkalˇci´c et al (2011)
WHN
3.3
-
-
Tkalˇci´c et al (2011)
VS
Line C
11.8
-
-
Sun and Toks¨oz (2006)
Line D
11.8
-
-
Sun and Toks¨oz (2006)
Line E
11.8
-
-
Sun and Toks¨oz (2006)
Line G
11.8
-
-
Sun and Toks¨oz (2006)
Line H
11.8
-
-
Sun and Toks¨oz (2006)
Line I
11.8
-
-
Sun and Toks¨oz (2006)
Line J
11.8
-
-
Sun and Toks¨oz (2006)
MM
Hao 2014 ﬁg3
9
-
-
Hao et al (2014)
He 2013
9
-
-
He et al (2013)
Xia 2015
6
-
-
Xia et al (2015)
GM
RRM
12
12
12
Huang et al (2013)
Deep Seismic Sounding proﬁles
The LG proﬁle (Zhang and Wang, 2007) is located in the northeastern part of
the target region (1) crossing the CFB and the SCMB, as shown in Figure 8.5. The
interpretation is performed according to the results reported in Figure 3b of Zhang
and Wang (2007) in which the P-wave events P1 and P4 are identiﬁed respectively
as the TMC and the TLC corresponding to the velocities reported in Table 8.2.
For the southeastern margin of the SCB, the ESP 1 9 and ESP 11 17 (Nissen
et al, 1995) proﬁles are used, as shown in Figure 8.5. Starting from the velocity-
depth models reported in Nissen et al (1995), the contours velocity reported in Table
126

CHAPTER 8. CRUSTAL DETERMINATION TEST CASE
8.2 are adopted to deﬁne the depths of the TMC, TLC and MD. Along each line,
there is an evident crustal thinning toward the continent-ocean boundary associated
with the rifting of continental crust occurred during the formation of South China
Sea (Ji et al, 2017).
The OBS2001 proﬁle is located in the transition zone between the Catahysia
Block and the northeastern part of the of South China Sea, which is included as
input adopting the P-wave velocity interfaces displayed in the model reported in
Wang et al (2006).
Regarding the other DSS located in the South China Sea, namely the proﬁles
OOS2004 and OBS1993, the adopted interpretations of the 2D models consider
beneath a sediment layer (1.5 ÷ 2.0 km of thickness) a simply layered structure with
a thickness of the UC in the range 11÷14 km and of the LC in the range 10÷13 km
(Xia et al, 2010). As for the MC, it is not interpreted in this model. Therefore, the
MC limit is deﬁned as the 6 km/s contour, in agreement with the LG proﬁle, which
is the closest to OOS2004 and OBS1993.
Beyond the southern margin of the target region (1) the proﬁle OBHIV is used.
It crosses the Xisha Trough, a supposed failed rift with an advancing degree of rifting
from west to east (Qiu et al, 2001). Beneath a 1÷4 km thick Cenezoic Sedimentary
sequence, the top of the crust is characterized by low velocity (see Table 8.2) and
the average crustal thickness is 25 km, but in the middle of the section it gradually
decreases down to 8 km.
The interpretation of the Bayan J proﬁle by (Jia et al, 2006) shows an average
crustal velocity of 5.7 ÷ 6.2 km/s to be compared with the 6.2 ÷ 6.3 km/s typical of
the SCB. This crustal thinning, and the low P-wave velocity values (see Table 8.2),
can be linked to the magmatic activities and eruptions that aﬀected this area during
Holocene.
Table 8.2: P-wave velocity for the three crustal layers of the curst model, and variation of Moho
depth from DSS.
Input
Crustal
thickness
range [km]
MD
[km/s]
TLC
[km/s]
TMC
[km/s]
Reference
LG
30 ÷ 34
8.0
6.7
6.2
Zhang and Wang (2007)
OOS2004
24 ÷ 26
8.0
6.4
6.0
Xia et al (2010)
Baiyan J
25 ÷ 26
8.1
6.3
6.1
Jia et al (2006)
ESP 1 9
15 ÷ 28
8.0
6.6
6.1
Nissen et al (1995)
OBS1993
12 ÷ 26
8.0
6.4
6.0/5.7
Xia et al (2010)
ESP 11 17
16 ÷ 32
8.0
6.6
6.1
Nissen et al (1995)
OBHIV
15 ÷ 25
8.0
6.4
6.1
Qiu et al (2001)
Receiver Functions
Additional punctual constraints for the MD are added on the base of the tele-
seismic receiver functions analysis. Beside the results obtained from the 2 stations
127

Bayesian gravity inversion by Monte Carlo methods
located in the target region (1) close to the JUNO detector, i.e. GZH and SZN (see
Figure 8.5), also the information from other 9 stations is included, namely QIZ,
GUL, GYA, CNS, NNC, QZH, WZH, WHN. According to Tkalˇci´c et al (2011) the
more reliable MD is estimated (along with their uncertainties) from GRID SEARCH
and H-k methods. Beneath each station we estimate MD as a weighted mean of the
result deriving from the three methods. The MD uncertainty is given as the semi-
dispersion of the two measures, to account for the variability of the diﬀerent methods,
plus the weighted uncertainty for each method. This value is multiplied by 3 to be
conservative, leading to value present in Table 8.1.
Virtual Section
We further used a 3D P-wave velocity model from Sun and Toks¨oz (2006) to
constrain the MD, by digitizing about 90 MD depth controlling points in 8 virtual
cross sections (C, D, E, F, G I, J and H) reported in Figures 10 and 11 of Sun
and Toks¨oz (2006). The MD uncertainty is estimated from the ﬁnal model standard
deviation of the travel time corresponding to 0.49 s, which, multiplied by 8 km/s
(the velocity in the lower crust), gives a MD error of 3.9 km. Again, the uncertainty
is assumed three times the error, as shown in Table 8.1.
Moho Maps
Last constraint for the MD comes from published MD models (Hao et al, 2014;
He et al, 2013; Xia et al, 2010). Starting from the 1 km interval contours reported
in each map, three regular grids with 10 km × 10 km of horizontal resolution and
with diﬀerent spatial coverages are digitized. Then this information is considered as
punctual. The accuracy is directly taken from the reference paper of each map and
multiplied by 3, as in the previous case. The obtained uncertainty values are shown
in Table 8.1.
Global crustal model
The last input used is a global crustal model. Its accuracy is chosen such that
it is the less reliable data, due to its global nature. The model used is the Reﬁned
Reference Model (RRM) (Huang et al, 2013) interpolated from its original resolution
of 0.5°×0.5° to the 50 km×50 km resolution of the voxel model, by means of a nearest
neighbour algorithm. The uncertainty is assumed to be ±4 km, that multiplied by
3 to be conservative (as it happens for the other inputs) leads to an uncertainty of
12 km, as shown in Table 8.1.
8.5
Building prior probability
The geophysical and geological input explained in Sections 8.3 and 8.4 need to
be combined together to deﬁne the prior probability, namely the penalty functions
s2
i (Li) and q2(Li, Lj) have to be tuned, as well as the mean density of each label at
each voxel µρ(Li) with the corresponding standard deviation σρ(Li), according to
Equations 4.34, 4.35 and 4.26 respectively.
128

CHAPTER 8. CRUSTAL DETERMINATION TEST CASE
The ﬁrst step is to deﬁne the depth surfaces and their accuracies needed by
the software as input to build the geological reference model. The depth surfaces
are used to retrieve the most probable label distribution in the voxel model, while
the accuracy is used to tune the above recalled penalty functions. The model is
supposed to be a layered one, according to the possible software input as explained
into Section 6.1. When deﬁning the surfaces, it is useful to recall that the observed
signal is the free-air gravity anomaly. Therefore, the voxel model must include also
the topography and the bathymetry, as explained before. That is why above the
four investigated crustal layers, namely UC, MC, LC, and mantle, three layers of
sediments and the water are directly introduced from the global crustal model. This
requires to deﬁne four more surfaces, apart from the already deﬁned ones, namely
TMC, TLC and MD. The four surfaces are: the Top of Upper Crust (TUC), the Top
of Sediment 3 (TS3), the Top of Sediment 2 (TS2), and the Digital Elevation Model
(DEM) describing both the topography and the bathymetry. Two remarks are in
order that the topography/bathymetry are taken from the GEBCO08 1 minute grid
(Monahan, 2008), that is a more reﬁned model in terms of resolution than the RRM
and that the introduced sedimentary layers are assumed as known both in terms
of geometry and density. A summary of the input source used while building the
geological reference model with the associated uncertainty is shown in Table 8.3 and
8.4 for geometry and density, respectively.
Table 8.3: Input used for the geometry of the geological reference model and their accuracy.
Interface
Input
Uncertainty
DEM
GEBCO08 1 minute grid
-
TS2
RRM
-
TS3
RRM
-
TUC
RRM
-
TMC
RRM + DSS + RF
Table 8.1
TLC
RRM + DSS + RF
Table 8.1
MD
RRM + DSS + RF + MM+ VS
Table 8.1
Table 8.4: Input used for the density of the geological reference model and their accuracy.
Layer
Input
Uncertainty
[kg/m3]
Sediments 1
RRM
-
Sediments 2
RRM
-
Sedimetns 3
RRM
-
UC
RRM
±10/±20/±30/±40
MC
RRM
±10/±20/±30/±40
LC
RRM
±10/±20/±30/±40
Upper mantle
RRM
±10/±20/±30/±40
The four ﬁxed surfaces, namely DEM, TS2, TS3 and TUC, are determined by
129

Bayesian gravity inversion by Monte Carlo methods
interpolating their value at the horizontal grid directly from the relative global model.
As for the other three surfaces, namely TMC, TLC and MD, they are determined
by merging the above explained geophysical and geological data. In particular, the
following procedure is adopted: each point of DSS, RF, VS, and MM is assigned to
the closest grid knots. Then, the admissible depth range is computed by evaluating
the intersection of the uncertainty range of all of the diﬀerent geophysical information
present at the knot. An example for the knot B (for its location see Figure 8.5) is
shown in Figure 8.6 where the admissibility range is retrieved for MD, TLC and
TMC.
(a) MD
(b) TLC
(c) TMC
Figure 8.6: Evaluation of the possible surface intervals at the point B, marked with red circle in
Figure 8.5. For each surface the available data with their uncertainty are represented. The possible
surface interval is taken as the intersection of all the uncertainties and is represented with the two
dashed lines.
Nevertheless, this approach does not give as output a value of the surfaces. More-
over, neighbour knots could have very diﬀerent ranges, thus making very diﬃcult
the choice of the a-priori surface. As a matter of fact, this surface must reﬂect the
expected smoothness of the model. Therefore the most probable geological surface
is determined by means of a regularized least squares adjustment, such that the
obtained surface is smooth and lays inside its admissibility boundary. Figure 8.7
shows the vertical cross-section AA′ (for its location see Figure 8.5) with the admis-
sibility range (dashed lines) and the prior reference surfaces (continuous lines). The
information on depth ranges and closeness to these a-priori surface are used to set
up the penalty functions s2
i (Li), by means of the Pγ(Li) probability (see Equation
4.33). As in the case of the salt dome, a value ˆpγ is chosen and assigned to the most
probable label at each voxel, while its complement to 1, namely 1 −pγ, is uniformly
distributed among the other possible labels.
The other penalty function q2(Li, Lj) is used to prevent layers with null thickness,
e.g. the middle crust and the mantle can never be neighbour, and to obtain smooth
surfaces in the estimated model.
The last a-priori information is about the mean density of each label at each
voxel, as well as its variability. The value of the mean density is directly taken from
the densities of the RRM, interpolated over the horizontal grid by means a nearest
neighbour interpolation. Its variability is assumed as a constant for all the layers and
ﬁxed to a value to be tuned, choosing among four values in the range 10÷40 kg/m3,
130

CHAPTER 8. CRUSTAL DETERMINATION TEST CASE
Figure 8.7: Section AA′ derived from the given geophysical input (depth magniﬁcation 20×).
as shown in the above Table 8.4. Notice that the mean density of each label is
assumed as a constant in the vertical direction, while it allows variability in the
lateral direction according to the RRM information. The shape of the mean density
for each of the not-ﬁxed label, namely UC, MC, LC, and upper mantle, is presented
in Figure 8.8.
A further remark is that some values needed as input of the prior probability
(a) Upper crust
(b) Middle crust
(c) Lower crust
(d) Mantle
Figure 8.8: Mean density assumed as prior value for each label. The mean density is taken from
the RRM model (Huang et al, 2013) and it is considered a constant in the vertical direction, namely
there is no vertical gradient.
131

Bayesian gravity inversion by Monte Carlo methods
are left as parameters to be tuned while retrieving the solution, so that they allow
the prior probability to reﬂect the expectation. In particular, these parameters are
the value ˆpγ and the value of σρ(Li) that will be empirically tuned through a set of
solutions, where diﬀerent values are tested. In the next section, these tests will be
explained, as well as the adopted procedure to ﬁnd the best solution.
8.6
Estimated crustal model
The solution is retrieved by estimating the most probable model by giving diﬀer-
ent sets of parameters as input of the software. Then, among the retrieved solutions,
the one that has a satisfactory ﬁt of the gravity signal, as well as the best interpre-
tation of the prior guess is chosen as the estimated model. The prior guess is that
the surfaces of the layers and the density variation between neighbour voxels are
as smooth as possible, satisfying all the constraints deriving from the accuracy of
the discontinuity surfaces. Moreover, according to the physical properties of the
rocks into the region, it is also requested that the density increases with depth. To
improve the quality of the solution this condition is imposed through deterministic
constraints, by limiting the solution space along the simulated annealing evolution.
Notice that to reduce the computational time the density correlation matrix is not
used, but other deterministic constraints are introduced to limit the lateral and ver-
tical density variation and to obtain smoother and smoother surfaces. In particular,
the imposed deterministic constraints are three:
- the density has to increase with depth for each “column” of voxel with the same
horizontal location;
- the density allows a maximum variation as a percentage hρ of the maximum
theoretical variation allowed between two neighbour voxels. This constraint can
assume diﬀerent values for the vertical and the horizontal direction;
- the neighbourhood of each voxel has to be constituted at least by a number hL
of voxels with the same label.
Notice that the second constraint on the densities is the deterministic “version” of
the density correlation matrix. The choice to introduce this constraint is related to
computational time problems while using the correlation matrix.
Nevertheless, introducing these constraints and especially changing the weights
of the prior probability (i.e. γ, λ) it is no more easy to compare the obtained solutions
in terms of the value of their target function, since its order of magnitude can be
inﬂuenced by the chosen input. Therefore, to be able to compare the solutions and to
evaluate their quality in terms of good translation of the prior guess, two numerical
indicators are deﬁned: one for the geometry and one for the density distribution. In
the case of the geometry it is necessary to come back to a surface description of the
model, retrieving the depth of each discontinuity surface from the voxel model at
each knot of the horizontal grid. The indicator is computed by ﬁrstly evaluating the
slope mij of the surface at each knot of the grid i with respect to the value at its 8
neighbours j. Then, the ﬁnal indicator ˆm is deﬁned as the mean over all the knots
of the maximum slope at each knot. The best model in terms of geometry presents
a low value of this indicator. In fact, the smallest is this value, the smoothest is
132

CHAPTER 8. CRUSTAL DETERMINATION TEST CASE
globally the model. Also in the case of the density something similar is performed.
In particular, at each voxel the density variation with respect to the 26 neighbours
is evaluated and divided by their distance. The outcome ˆrij is something that can
be thought as a density slope. Then, as in the previous case, the ﬁnal indicator
ˆr is deﬁned as the mean over all the voxels of the maximum slopes at each voxel.
Also in this case, the smallest is this value, the smoothest (and the best) is the
density variation in the estimated model. Furthermore, also the gravity ﬁt has to be
considered as a quality indicator of the solution. In particular, the considered value
for the indicator is the standard deviation ˆσν of the residuals between the observed
and the modelled signal.
Finally, the tests are performed by varying the kλ and kγ scale factors of the
auto-tuned weights γ and λ (see Section 6.1), the two parameters used to deﬁne the
prior probability pγ and σρ(Li) (see the the previous section), and the parameters
governing the above explained constraints hL and hρ. The last has two values: one
for the constraint in the horizontal direction and the other for the constraint in the
vertical one. Table 8.5 shows the input to the 82 estimated solution, together with
the three indicators computed for each estimated model. Each test is performed by
running the simulated annealing for 1000 iterations and by using the software in the
simulated annealing plus simple relaxation mode. Each solution requires between 5
and 10 minutes of computational time, allowing to compute the 82 solution in less
than a night.
Table 8.5: Parameters used in the performed test and values of the three indicators computed
from each solution.
N.
hL
hρ [%]
kλ
kγ
ˆpγ
σρ(Li)
[kg/m3]
ˆσν
[mGal]
ˆr
[kg/m4]
ˆm
[%]
h
v
1
13
10
10
1
1
0.67
30
1.08
4.04 · 10−3
1.42
2
10
10
10
1
1
0.67
30
0.89
3.88 · 10−3
1.91
3
10
10
10
102
1
0.67
30
0.82
3.94 · 10−3
1.86
4
10
10
10
102
102
0.67
30
0.87
4.02 · 10−3
1.85
5
10
10
10
102
103
0.67
30
1.04
4.20 · 10−3
1.91
6
13
10
10
1
1
0.67
40
0.23
3.33 · 10−3
1.43
7
0
10
10
1
1
0.67
30
0.26
2.83 · 10−3
6.00
8
16
10
10
1
1
0.67
30
1.11
3.97 · 10−3
1.53
9
3
10
10
1
1
0.67
30
0.17
2.67 · 10−3
5.98
10
5
10
10
1
1
0.67
30
0.17
2.61 · 10−3
3.68
11
8
10
10
1
1
0.67
30
0.56
3.50 · 10−3
2.78
12
13
10
10
1
1
0.67
20
2.74
2.83 · 10−3
1.44
13
13
10
10
1
1
0.67
10
6.13
1.44 · 10−3
1.46
14
9
10
10
1
1
0.67
10
6.12
1.69 · 10−3
2.66
15
9
10
10
1
1
0.67
20
2.42
2.89 · 10−3
2.76
16
9
10
10
1
1
0.67
30
0.55
3.59 · 10−3
2.54
17
9
10
10
1
1
0.67
40
0.09
2.99 · 10−3
2.35
18
9
10
10
1
10
0.67
30
0.65
3.65 · 10−3
2.66
19
9
10
10
1
102
0.67
30
0.67
3.74 · 10−3
2.61
133

Bayesian gravity inversion by Monte Carlo methods
N.
hL
hρ [%]
kλ
kγ
ˆpγ
σρ(Li)
[kg/m3]
ˆσν
[mGal]
ˆr
[kg/m4]
ˆm
[%]
h
v
20
9
10
10
1
103
0.67
30
0.68
3.96 · 10−3
2.51
21
9
10
10
10
1
0.67
40
0.14
3.35 · 10−3
2.23
22
9
10
10
102
1
0.67
40
0.15
3.56 · 10−3
2.12
23
9
10
10
103
1
0.67
40
0.24
3.94 · 10−3
2.09
24
9
10
10
10
1
0.67
30
0.56
3.66 · 10−3
2.51
25
9
10
10
102
1
0.67
30
0.64
3.85 · 10−3
2.38
26
9
10
10
103
1
0.67
30
0.72
3.99 · 10−3
2.28
27
9
10
10
10
10
0.67
30
0.57
3.53 · 10−3
2.50
28
9
10
10
102
102
0.67
30
0.62
3.90 · 10−3
2.40
29
9
10
10
103
103
0.67
30
0.63
3.94 · 10−3
2.25
30
9
8
8
1
1
0.67
30
1.85
4.18 · 10−3
2.62
31
9
5
5
1
1
0.67
30
4.49
4.34 · 10−3
2.87
32
9
2
2
1
1
0.67
30
10.82
2.03 · 10−3
3.22
33
9
1
1
1
1
0.67
30
16.08
1.07 · 10−3
2.69
34
9
10
10
1
1
0.40
30
0.66
3.70 · 10−3
2.59
35
9
10
10
1
1
0.50
30
0.66
3.70 · 10−3
2.59
36
9
10
10
1
1
0.60
30
0.62
3.59 · 10−3
2.50
37
9
10
10
1
1
0.70
30
0.60
3.69 · 10−3
2.58
38
9
10
10
1
1
0.80
30
0.61
3.64 · 10−3
2.56
39
9
10
10
1
1
0.90
30
0.59
3.52 · 10−3
2.58
40
9
10
10
1
1
0.67
30
1.10
3.62 · 10−3
1.90
41
9
10
10
1
1
0.67
30
0.40
3.05 · 10−3
3.19
42
9
10
10
104
1
0.67
30
0.84
4.05 · 10−3
2.23
43
9
10
10
1
104
0.67
30
0.93
4.04 · 10−3
2.43
44
9
10
10
104
104
0.67
30
0.90
4.23 · 10−3
2.21
45
9
8
10
1
1
0.67
30
1.84
4.48 · 10−3
2.67
46
9
5
10
1
1
0.67
30
4.83
4.58 · 10−3
2.83
47
9
2
10
1
1
0.67
30
10.43
2.72 · 10−3
3.05
48
9
1
10
1
1
0.67
30
20.44
1.75 · 10−3
2.83
49
9
10
8
1
1
0.67
30
0.59
3.69 · 10−3
2.56
50
9
10
5
1
1
0.67
30
0.54
3.39 · 10−3
2.51
51
9
10
2
1
1
0.67
30
0.78
2.63 · 10−3
2.58
52
9
10
1
1
1
0.67
30
0.81
2.01 · 10−3
2.50
53
5
10
10
1
1
0.67
20
0.97
2.18 · 10−3
4.50
54
5
10
10
10
10
0.67
30
0.32
2.94 · 10−3
3.51
55
5
10
10
102
102
0.67
30
0.33
3.84 · 10−3
3.41
56
5
10
10
103
103
0.67
20
2.26
3.07 · 10−3
3.63
57
1
10
10
1
1
0.67
30
0.18
2.84 · 10−3
5.93
58
2
10
10
1
1
0.67
30
0.21
2.84 · 10−3
4.75
59
3
10
10
1
1
0.67
30
0.10
2.60 · 10−3
5.90
60
4
10
10
1
1
0.67
30
1.02
2.79 · 10−3
5.32
61
5
10
10
1
1
0.67
30
0.09
2.44 · 10−3
4.25
62
6
10
10
1
1
0.67
30
0.10
2.47 · 10−3
3.75
134

CHAPTER 8. CRUSTAL DETERMINATION TEST CASE
N.
hL
hρ [%]
kλ
kγ
ˆpγ
σρ(Li)
[kg/m3]
ˆσν
[mGal]
ˆr
[kg/m4]
ˆm
[%]
h
v
63
7
10
10
1
1
0.67
30
0.12
2.51 · 10−3
3.01
64
8
10
10
1
1
0.67
30
0.39
3.35 · 10−3
2.78
65
9
10
10
1
1
0.67
30
0.44
3.34 · 10−3
2.50
66
10
10
10
1
1
0.67
30
0.99
3.88 · 10−3
2.23
67
11
10
10
1
1
0.67
30
1.03
3.80 · 10−3
1.89
68
12
10
10
1
1
0.67
30
0.99
3.92 · 10−3
1.65
69
13
10
10
1
1
0.67
30
1.08
4.04 · 10−3
1.43
70
14
10
10
1
1
0.67
30
1.14
4.02 · 10−3
1.53
71
15
10
10
1
1
0.67
30
1.13
3.96 · 10−3
1.53
72
16
10
10
1
1
0.67
30
1.11
3.97 · 10−3
1.53
73
17
10
10
1
1
0.67
30
1.13
3.98 · 10−3
1.53
74
18
10
10
1
1
0.67
30
1.13
3.98 · 10−3
1.53
75
19
10
10
1
1
0.67
30
1.13
3.98 · 10−3
1.53
76
20
10
10
1
1
0.67
30
1.13
3.98 · 10−3
1.53
77
21
10
10
1
1
0.67
30
1.13
3.98 · 10−3
1.53
78
22
10
10
1
1
0.67
30
1.13
3.98 · 10−3
1.53
79
23
10
10
1
1
0.67
30
1.13
3.98 · 10−3
1.53
80
24
10
10
1
1
0.67
30
1.13
3.98 · 10−3
1.53
81
25
10
10
1
1
0.67
30
1.13
3.98 · 10−3
1.53
82
26
10
10
1
1
0.67
30
1.13
3.98 · 10−3
1.53
When all the solutions are computed, they have to be compared in order to
choose the best one. The choice should reﬂect the expected smoothness both in
terms of density and geometry, according to the given geological and geophysical
information. As explained in Section 8.5, the prior probability is built to satisfy all
the punctual geometrical constraints, given by the intersection of the uncertainty of
the diﬀerent geophysical data. Moreover, also other constraints are introduced to
obtain the“smoothest”solution. All this information is deterministic, therefore each
solution for sure will respect them. However, especially for the regularity constraints,
the parameters governing their strength has to be tuned. As explained before, this
task is achieved by evaluating three indicators for each solution (see Table 8.5 and
Figure 8.9).
To ﬁlter the possible solutions, as a ﬁrst operation situations that
are not physical are rejected. Therefore, some thresholds are deﬁned, and solutions
outside their range are rejected. In particular, the thresholds are set on the standard
deviation of the gravity residuals, for which solutions not included in the range
0.75÷1.25 mGal are rejected. This constraint is shown by red dashed lines in Figure
8.9(a). The two threshold values are chosen according to the signal accuracy of 1
mGal. In fact, a too high value means that the signal is not correctly interpreted,
while a too low value means that the model is explaining also the measurement
noise. The second threshold aims to ﬁlter the solution based on the mean slope
of the estimated surfaces. As shown in Figure 8.9(b) a value of 2% is considered
the maximum mean slope admissible. This choice is performed since it is required
that the surfaces are very smooth while passing through the constraints given by
135

Bayesian gravity inversion by Monte Carlo methods
geophysical data, and such a value allows to highlight only the very smooth ones.
As for the density indicator, a remark is that smoothest solutions in terms of density
usually correspond to very bad gravity ﬁtting, as it can be seen by comparing Figures
8.9(a) and 8.9(c). Therefore, no thresholds are deﬁned over the possible values of this
indicator, also because after the rejection performed with the ﬁrst two, the possible
solutions presents very similar value of that one.
(a) Gravity residuals standard deviation (y axis has a logarithmic scale).
(b) Overall mean of the maximum surface slopes for every knot of the grid.
(c) Overall mean of the maximum density slopes for every voxel.
Figure 8.9: Indicator used to choose the best solution among the 82 computed ones. The red
dashed lines indicates thresholds used to reject not physical solutions.
Finally to decide the best solution, a further indicator is computed as the com-
bination of the three presented ones. It is deﬁned as the sum of the three indicators,
each divided by its mean for all the solutions, making them unitless. Therefore, for
136

CHAPTER 8. CRUSTAL DETERMINATION TEST CASE
a solution i, it assumes the following shape:
ci =
ˆσνi
1
N
PN
j=1 ˆσνj
+
ˆmi
1
N
PN
j=1 ˆmj
+
ˆri
1
N
PN
j=1 ˆrj
(8.1)
where N = 82 is the total number of computed solutions. The computed index is
shown in Figure 8.10 only for the solutions that are not rejected.
Figure 8.10: Combined index used to choose the best solution. Notice that, according to the
thresholds, only possible solutions are represented.
According to the deﬁnition of ˆm, ˆr and ˆσν the smallest is the c index, the
smoothest and best ﬁtting is the solution. Therefore the best solution is the number
1, whose results are reported in Figure 8.11 in terms of estimated depth of TMC,
TLC and MD, and in Figure 8.12 in terms of estimated density.
(a) TMC
(b) TLC
(c) MD
Figure 8.11: Estimated solution in terms of depth surfaces delimiting the layers.
137

Bayesian gravity inversion by Monte Carlo methods
Figure 8.12: Vertical cross-sections estimated density of the best solution. Horizontal axis rep-
resents the East coordinate, while the vertical one the depth with a magniﬁcation of 20×, both
expressed in [km]. The sections are at each step of the horizontal grid, starting from north (1) to
south (23).
138

CHAPTER 8. CRUSTAL DETERMINATION TEST CASE
A remark is about the ﬁt of the gravity of the chosen solution, reported in Figure
8.13. It can be noticed that over the mainland the residuals present some picks with
amplitude signiﬁcantly greater than its standard deviation (about 6 mGal vs. 1 mGal).
This is due to the choice of not performing the terrain correction, but considering
both bathymetry and topography directly in the model used during the inversion.
Probably, the spikes are due to the density of the shallowest layers not correctly
modelled or to a too low resolution of the model for this purposes. Notice that these
eﬀects are present not only on this solution, but in the majority of the retrieved
solution, thus conﬁrming that this is a general problem almost independent from
the input set of parameters.
Figure 8.13: Gravity residuals of the chosen solution.
8.7
Comparison with a classical solution
The ﬁnal estimate of the crustal model below the JUNO detector is ﬁnally com-
pared with a classical inverse solution, based on the iterative Parker–Oldenburg
method (Parker, 1973; Oldenburg, 1974). This procedure is based on a relationship
between the Fourier transform of the gravity anomaly and the sum of the Fourier
transform of the interface topography, allowing to invert the gravity by using a one
layer model with ﬁxed density contrast between the upper and lower material. In
fact, given the mean depth z0 of the density interface and the density contrast ∆ρ
between the two materials, the three-dimensional geometry of the interface can be
iteratively calculated. This solution has been implemented into the freely available
3DINVER Matlab routine by G´omez-Ortiz and Agarwal (2005), that is used to
retrieve the classical inverse solution in the current section. To enhance the con-
vergence in the iterative process, they introduce a high-cut ﬁlter in the frequency
domain, requiring the minimum cut-oﬀwavelength λmin as input.
The limitation of this kind of approach to the gravity inversion is that only one
discontinuity surface can be estimated, by introducing a known density contrast.
Therefore, the comparison with the Bayesian solution is performed in terms of Moho
depth only, since it represents one of the largest anomalous contribution to the
observed gravity signal. The density contrast ∆ρ is ﬁxed at the diﬀerence between
the mean crustal density and the mean upper mantle density computed from the
geological reference model presented in 8.5. Starting from this model also the terrain
139

Bayesian gravity inversion by Monte Carlo methods
correction is performed, because the classical approach requires the Bouguer gravity
anomaly as input. In fact, it is necessary to reduce the observed signal for all the
other anomalies apart from the one to be estimated, i.e. the Moho depth.
The
3DINVER software allows to determine a surface with the same size and resolution
of the observed signal. Therefore, to avoid border eﬀects, the observed signal is
enlarged with respect to the study region.
In particular, the observed signal is
synthetised from the global gravity model up to region (2). Region (3) is used to
perform a zero-padding to further reduce border eﬀects, as suggested by G´omez-
Ortiz and Agarwal (2005).
To retrieve the classical solution, diﬀerent tests with diﬀerent input parameters
have been performed. In particular, the chosen input parameters are presented in
Table 8.6. In practice, the solutions are computed by varying only the minimum cut-
oﬀwavelength, since the other parameters are ﬁxed to values taken directly from
the available geological information.
Table 8.6: Input data common to classical solutions.
Data
Value
mean crustal density
ρC
2824 kg/m3
mean mantle density
ρM
3307 kg/m3
density contrast
∆ρ = ρM −ρC
483 kg/m3
mean Moho depth
z0
27 km
minimum cut-oﬀwavelength
λmin
50 km ÷ 170 km
The six computed solutions, corresponding to six diﬀerent values of the minimum
cut-oﬀwavelength, are shown in Figure 8.14, in terms of Moho depth. Comparing
these solutions with the one obtained by applying the presented Bayesian algorithm,
shown in Figure 8.11, it can be qualitatively observed that the global trend of the
Moho surface looks similar. Moreover, it can be also observed that the smoothness of
(a) λmin = 50 km
(b) λmin = 70 km
(c) λmin = 100 km
(d) λmin = 130 km
(e) λmin = 150 km
(f) λmin = 170 km
Figure 8.14: Estimated solution in terms of depth surfaces delimiting the layers.
140

CHAPTER 8. CRUSTAL DETERMINATION TEST CASE
the estimated surface also increases by increasing the value of the minimum cut-oﬀ
wavelength. Notice that these comparisons, as well all as the statistics computed in
the following, are performed only for the region (1), since the region (2) presents some
residuals border eﬀects in the classical solutions, caused by the usage of the Fourier
transform, as recalled above. To quantitatively evaluate the agreement between the
solutions, the mean and the standard deviation of MD diﬀerence ∆Z between the
classical and Bayesian solutions are computed. In particular, each of the solutions
of Figure 8.14 is compared with the Bayesian estimated MD of Figure 8.11(c).
As for the quality of the classical solutions, it is evaluated by means of some
indicators, as in the previous case. In particular, two of the three indicators used in
the previous section are chosen: the standard deviation ˆσν of the gravity residuals
and the overall mean of the maximum slopes for every knot ˆm. The statistics on the
densities are meaningless because they are no more unknowns of the problem.
The diﬀerent indicators and the statistics on the agreement, computed for each
one of the six classical solutions, are reported in Table 8.7.
Table 8.7:
Indicators computed for the estimated classical solutions, to evaluate quality and
diﬀerences with respect to the Bayesian one.
λmin
[km]
∆Z [km]
ˆσν
[mGal]
ˆm
[%]
ˆµ
ˆσ
50
1.02
1.65
0.92
4.76
70
1.01
1.54
2.09
4.10
100
1.00
1.42
3.90
3.42
130
1.00
1.35
5.18
3.13
150
1.01
1.32
5.87
2.94
170
1.02
1.28
6.71
2.70
The results conﬁrm the congruency between the geometry estimated by the clas-
sical and the Bayesian approaches. In fact, independently from the chosen cut-oﬀ
frequency, the mean of the diﬀerences is about 1 km with a standard deviation of
the order of 1 ÷ 1.5 km, showing a good agreement.
However, recalling the thresholds used in Section 8.6 to choose of the best solution
(0.75 ÷ 1.25 mGal for ˆσν, 2% for ˆm), it can be noticed that by using the classical
approach they cannot be satisﬁed.
In fact, to reduce the standard deviation of
the gravity ﬁt, it is necessary to increase the roughness of the surface, and vice
versa. This makes impossible to achieve the quality of the solution retrieved with
the Baysian approach, namely ˆσν = 1.08 mGal and ˆm = 1.42%.
This comparison shows the quality improvement obtained by introducing more
parameters in the solution and by controlling them through the prior probability, as
it happens in the presented Bayesian solution. In fact, the impossibility to reach a
satisfying solution with the classical approach is caused by the hypotheses introduced
to mitigate the non-uniqueness of the solution, that are too restrictive with respect
to the assumed physical behaviour (smooth geometry) of the interface surfaces.
141

142

Chapter 9
Conclusions
The goal of this work was the study and the development of a Bayesian algorithm
to solve the inverse gravimetric problem. The main proposal of this algorithm is to
allow the introduction of geological information, modelled in terms of prior proba-
bility, to reduce the non-uniqueness of the solution that represents a typical issue of
the inverse gravimetric problem.
The main idea behind the algorithm is to invert the gravity observations, by
estimating the densities causing the observed signal, but at the same time to classify
the estimated densities to retrieve the spatial distribution of the diﬀerent geological
materials. This classiﬁcation is performed by borrowing some techniques typical of
the image analysis, particularly suitable to be solved by Monte Carlo methods. The
proposed approach allows ﬂexibility in the possible kinds of geological composition
to be treated. In fact, the inverse algorithm can be applied in various scenarios,
e.g. layered models, typical in crustal structure investigation, or buried body models
like salt domes, typical of the oil exploration, maintaining the same structure with
a diﬀerent set of input parameters. This ﬂexibility is possible thanks to the model-
lization chosen for the prior probability of the labels used to classify the investigated
volume, typical of the image analysis; they are based on neighbour relationship be-
tween voxels (volume units) by means of the concept of clique. This concept allows
to manage bodies and crustal structures never looking directly at their overall shape.
In fact, by properly setting the neighbour relationships, the diﬀerent kind of models
can be treated. This is a big advantage with respect to classical methods which,
instead, introduces constraints that are speciﬁc for the kind of problem, requiring
diﬀerent methods for diﬀerent geological conditions.
The drawback of this ﬂexibility is that it is not always so simple to deﬁne the
input parameters that correctly translate the geological information. The hybrid
Matlab/C software, developed to implement the inversion algorithm, tries to give
an answer to this problem, by standardizing the set of input. However, as shown by
the test case on real data, this standardization is not enough and a preprocessing
phase to merge all the available geological information is required, before using it as
input of the software. This phase can vary case by case and is not integrated into
the software, since it has to be manually performed by the operator. This could
imply that if new geological information is available the preprocessing phase has to
be completely repeated, instead of simply adding the new data to the list of input.
143

Bayesian gravity inversion by Monte Carlo methods
Nevertheless, once the preprocessing phase is performed the software can be used
in a quasi-automatic way. In fact, the prior information has to be translated in terms
of a geological reference model, that with very few other information, is converted
by the software in terms of prior probability, by automatically calibrating the re-
quired weights and clique functions. The software is developed by integrating C
code into Matlab through the MEX API, as a compromise between the computa-
tional speed and the simple data management. Moreover, the computational core is
optimized to manage the available memory even when the model is very extended
and the resolution is high. This optimization is required especially for the forward
modelling and the density correlation. It makes possible to run real cases even on
common workstations. Due to the sequentiality of the algorithm, the computational
time increases linearly with the number of the voxels and of the iterations of the
optimization algorithm. The latter can be deﬁned can be deﬁned directly by the
user. However, the number of voxels and the number of iterations are not the only
parameters inﬂuencing the computational time. In fact, the most critical issue is
the density correlation, introduced to mitigate the noisy eﬀect in the density esti-
mation. Unfortunately, the computational time does not linearly increase with the
correlation length, leading to a not aﬀordable computational burden. An example
was shown by solving the synthetic test case, when the time increases from some
minutes to six hours by introducing the density correlation.
An attempt to solve this problem is performed by restricting the set of possi-
ble densities, imposing deterministic constraints on the solution to obtain a smooth
density ﬁeld. This is the approach adopted in the real test case, leading to good
results in terms of density smoothness, without increasing the computational time
with respect to the non constrained and non correlated solution. However, that
constraints has to be further investigated. In fact, introducing constraints that are
dynamically modiﬁed depending on the current density realization could cause a
slower convergence or problems in the achievement of the optimal solution. Never-
theless, this is not the only possible solution to control the computational speed and
to obtain smoother densities. In fact, another alternative is to introduce neighbour
relationships also for the densities, promoting similar values when the labels of the
couple are the same. Notice that further improvements can be introduced also for
the labels; an example is to introduce new penalty functions, considering also cliques
of order 3 and 4, that could increase the homogeneity of the estimated classiﬁca-
tion and produce even smoother material boundaries (if necessary). These solutions
can be implemented by integrating the prior probability with few terms, showing
that the developed algorithm is a good starting point for further developments and
improvements.
The performed study shows that the developed inversion algorithm is correctly
working, since the examples show that it is able to retrieve a density model that ﬁts
the gravity according to the observation accuracy, as well as follow the chosen prior
probability. The presented test cases also demonstrate the ﬂexibility on diﬀerent
scenarios, showing how the prior information has to be supplied. This algorithm can
be used to support the trial-and-error procedures, allowing to automatically adapt
diﬀerent geological model to the gravity observations and avoiding this work to the
operator that has only to supply the geological model with all the prior information.
144

Appendix A
Normal probability
The aim of this appendix is to explain some properties of the normal distribution,
useful during the derivation of the posterior probability and into its optimization.
A random variable X is assumed to be normally distributed when its probability
density function assumes a Gaussian shape, namely:
X ∼N[µ,σ2] →P(x) =
1
√
2πσ2 exp
(
−(x −µ)2
2σ2
)
(A.1)
where µ represents the mean and σ the standard deviation. Moreover, when the
normal random variable X is n-dimensional, it assumes the shape of a multivariate
normal distribution, that is:
X ∼N[µ,C] →P(x) =
1
p
(2π)n det C
exp

−1
2(x −µ)⊺C−1(x −µ)

(A.2)
where the vector µ contains the mean of each of the n random variables and C is
their covariance matrix.
The following three sections are dedicated to retrieve the conditional distribution
of a joint multivariate normal probability, expanding the product of one-dimensional
normal distribution and to understand the impact of normal likelihood probability
into the posterior distribution when working in a Bayesian framework.
A.1
Conditional distribution
To compute the conditional probability distribution of a random variable Xi from
the joint distribution of the n-dimensional X normally distributed random variable,
the deﬁnition of conditional probability has to be ﬁrstly recalled:
P(xi|x−i) =
P(xi, x−i)
R ∞
−∞P(xi, x−i) dxi
(A.3)
It can be observed that the marginal probability P(x−i) =
R ∞
−∞P(xi, x−i) dxi is
a constant, because the values of X−i are given.
Consequently, to evaluate the
145

Bayesian gravity inversion by Monte Carlo methods
conditional distribution it is suﬃcient to isolate all the terms containing xi in the
joint probability and to apply the normalization condition to this result.
Therefore, the ﬁrst step requires to isolate all the terms containing xi in the
joint normal probability, above recalled in Equation A.2. Since only the exponent
depends on x, the following derivation is performed directly on this term, obtaining:
(x −µ)⊺C−1(x −µ) =
= [ei(xi −µi) + I−i(x−i −µ−i)]⊺C−1[ei(xi −µi) + I−i(x−i −µ−i)] =
= e⊺
i C−1ei(xi −µi)2 + 2e⊺
i C−1I−i(x−i −µ−i) (xi −µi) +
+ (x−i −µ−i)⊺I⊺
−iC−1I−i(x−i −µ−i) =
= e⊺
i C−1ei

x2
i + µ2
i −2xiµi+
+2e⊺
i C−1I−i(x−i −µ−i)
e⊺
i C−1ei
xi −2e⊺
i C−1I−i(x−i −µ−i)
e⊺
i C−1ei
µi

+
+ (x−i −µ−i)⊺I⊺
−iC−1I−i(x−i −µ−i) =
= e⊺
i C−1ei

x2
i −2

µi −e⊺
i C−1I−i(x−i −µ−i)
e⊺
i C−1ei

xi+
−2e⊺
i C−1I−i(x−i −µ−i)
e⊺
i C−1ei
µi + µ2
i

+
+ (x−i −µ−i)⊺I⊺
−iC−1I−i(x−i −µ−i) =
= e⊺
i C−1ei
"
xi −µi + e⊺
i C−1I−i(x−i −µ−i)
e⊺
i C−1ei
2
+
−

µi −e⊺
i C−1I−i(x−i −µ−i)
e⊺
i C−1ei
2
−2e⊺
i C−1I−i(x−i −µ−i)
e⊺
i C−1ei
µi + µ2
i
#
+
+ (x−i −µ−i)⊺I⊺
−iC−1I−i(x−i −µ−i) =
= e⊺
i C−1ei

xi −µi + e⊺
i C−1I−i(x−i −µ−i)
e⊺
i C−1ei
2
+
−[e⊺
i C−1I−i(x−i −µ−i)]2
e⊺
i C−1ei
+ (x−i −µ−i)⊺I⊺
−iC−1I−i(x−i −µ−i) =
= e⊺
i C−1ei

xi −µi + e⊺
i C−1I−i(x−i −µ−i)
e⊺
i C−1ei
2
+
+ (x−i −µ−i)⊺I⊺
−i

C−1 −C−1eie⊺
i C−1
e⊺
iC−1ei

I−i(x−i −µ−i)
(A.4)
146

APPENDIX A. NORMAL PROBABILITY
Now, the retrieved expression of the exponent (Equation A.4), where the dependency
from xi is made explicit, is introduced into the multivariate normal distribution of
Equation A.2. Consequently, the joint probability P(xi, x−i) becomes:
P(xi, x−i) =
1
p
(2π)n det C
exp
(
−e⊺
i C−1ei
2

xi −µi + e⊺
i C−1I−i(x−i −µ−i)
e⊺
i C−1ei
2
+
−1
2(x−i −µ−i)⊺I⊺
−i

C−1 −C−1eie⊺
i C−1
e⊺
iC−1ei

I−i(x−i −µ−i)

(A.5)
Therefore, starting from the result obtained in Equation A.5 and recalling the
deﬁnition of conditional probability in Equation A.3, the latter, namely P(xi|x−i),
becomes:
P(xi|x−i) =
exp

−e⊺
i C−1ei
2

xi −µi + e⊺
i C−1I−i(x−i−µ−i)
e⊺
i C−1ei
2
R ∞
−∞exp

−e⊺
i C−1ei
2

xi −µi + e⊺
i C−1I−i(x−i−µ−i)
e⊺
i C−1ei
2
dxi
=
=
r
e⊺
i C−1ei
2π
exp
(
−e⊺
i C−1ei
2

xi −µi + e⊺
i C−1I−i(x−i −µ−i)
e⊺
i C−1ei
2)
=
= N[µ,σ2](xi)
(A.6)
Equation A.6 shows that also the conditional probability is a normal distribution,
whose mean and variance are:







µ = µi −e⊺
i C−1I−i(x−i −µ−i)
e⊺
i C−1ei
σ2 =
1
e⊺
i C−1ei
(A.7)
The consequence of the conditional distribution computed in Equation A.6 is
that the marginal probability P(x−i) can be retrieved as:
P(x−i) =
Z ∞
−∞
P(xi, x−i) dxi = P(xi, x−i)
P(xi|x−i) =
1
q
(2π)n−1 det(C) e⊺
i C−1ei
·
· exp

−1
2(x−i −µ−i)⊺I⊺
−i

C−1 −C−⊺eie⊺
i C−1
e⊺
iC−1ei

I−i(x−i −µ−i)

(A.8)
A.2
Product of normal distributions
This section aims to analyse the eﬀect of the product of one-dimensional nor-
mal distributions, each of them presenting diﬀerent mean and variance. Therefore,
starting from Equation A.1, the product results:
f(x) =
n
Y
i=1
N[µi,ρi](x) =
n
Y
i=1
1
p
2πσ2
i
exp
(
−(x −µi)2
2σ2
i
)
(A.9)
147

Bayesian gravity inversion by Monte Carlo methods
For sake of simplicity in the following computation, it is assumed:
1
σ2
i
= wi
(A.10)
and substituting Equation A.10 into Equation A.9, the latter becomes:
f(x) =
n
Y
i=1
rwi
2π exp
n
−wi
2
 x2 + µ2
i −2xµi
o
=
=
n
Y
i=1
rwi
2π exp
(
−1
2
 
x2
n
X
i=1
wi −2x
n
X
i=1
µiwi +
n
X
i=1
µ2
i wi
!)
=
=
n
Y
i=1
rwi
2π exp

−
Pn
i=1 wi
2

x2 −2x Pn
i=1 µiwi
Pn
i=1 wi
+
Pn
i=1 µ2
i wi
Pn
i=1 wi
+
+
Pn
i=1 µiwi
Pn
i=1 wi
2
−
Pn
i=1 µiwi
Pn
i=1 wi
2#)
=
=
n
Y
i=1
rwi
2π exp
(
−
Pn
i=1 wi
2
"
x −
Pn
i=1 µiwi
Pn
i=1 wi
2
+
Pn
i=1 µ2
i wi
Pn
i=1 wi
+
−
Pn
i=1 µiwi
Pn
i=1 wi
2#)
(A.11)
According to Equation A.11, f(x) assumes a normal shape. Therefore, deﬁning the
following quantities, namely an overall mean and an overall variance:











σ2 =
 n
X
i=1
wi
!−1
=
 n
X
i=1
1
σ2
i
!−1
µ = σ2
n
X
i=1
µiwi = σ2
n
X
i=1
µi
σ2
i
(A.12)
and restoring the substitution suggested by Equation A.10, the function f(x) be-
comes:
f(x) = N[µ,σ2](x) exp
(
−
n
X
i=1
µ2
i
2σ2
i
+ µ2
2σ2
)
√
2πσ2
Qn
i=1
p
2πσ2
i
=
= N[µ,σ2](x)
Qn
i=1 N[µi,σ2
i](x = 0)
N[µ,σ2](x = 0)
(A.13)
Notice that f(x) assumes a normal shape, but it is no more a probability distribution,
since its integral over R does not assume a unitary value. This is caused by the fact
148

APPENDIX A. NORMAL PROBABILITY
that, performing the product, the following constant term, called H hereafter, is
present:
H =
Qn
i=1 N[µi,σ2
i](x = 0)
N[µ,σ2](x = 0)
(A.14)
When n = 2, the constant term H can be expanded as:
H =
Qn
i=1 N[µi,σ2
i](x = 0)
N[µ,σ2](x = 0)
=
q
2π σ2
1σ2
2
σ2
1+σ2
2
q
(2π)2 σ2
1σ2
2
exp





−µ2
1
2σ2
1
−µ2
2
2σ2
2
+

σ2
2µ1+σ2
1µ2
σ2
1+σ2
2
2
2 σ2
1σ2
2
σ2
1+σ2
2





=
=
1
p
2π(σ2
1 + σ2
2)
exp
(
−σ2
2(σ2
1 + σ2
2) µ2
1 −σ2
1(σ2
1 + σ2
2) µ2
2 + (σ2
2µ1 + σ2
1µ2)2
2σ2
1σ2
2(σ2
1 + σ2
2)
)
=
=
1
p
2π(σ2
1 + σ2
2)
exp
−σ2
1σ2
2µ2
1 −σ2
1σ2
2µ2
2 + 2σ2
1σ2
2µ1µ2
2σ2
1σ2
2(σ2
1 + σ2
2)

=
=
1
p
2π(σ2
1 + σ2
2)
exp
(
−(µ1 −µ2)2
2(σ1 + σ2)
)
(A.15)
A remark is about the value of the standard deviation of the two initial normal
distributions. In fact, it could happen that one of the two becomes very small value,
e.g. σ1 →0. In this case, starting from Equation A.12, the overall mean and variance
assumes the following values:









lim
σ1→0 σ2 = lim
σ1→0
 1
σ2
1
+ 1
σ2
2
−1
=
σ2
1σ2
2
σ2
1 + σ2
2
= σ2
1
lim
σ1→0 µ = lim
σ1→0
µ1
σ2
1
+ µ2
σ2
2

σ2
1σ2
2
σ2
1 + σ2
2
= lim
σ1→0
µ1σ2
2 + µ2σ2
1
σ2
1 + σ2
2
= µ1
(A.16)
and the constant H (starting from Equation A.15) becomes:
lim
σ1→0 H =
1
p
2πσ2
2
exp
(
−(µ1 −µ2)2
2σ2
2
)
= N[µ2,σ2
2](x = µ1)
(A.17)
Combining the outcome of Equations A.16 and A.17, the value of f(x) when σ1 →0
can be retrieved as:
lim
σ1→0 f(x) = N[µ2,σ2
2](x = µ1) N[µ1,σ2
1](x)
(A.18)
Notice that when n > 2 and one of the standard deviations σˆi →0, the result
can be always reconduced to the case n = 2. It is suﬃcient to ﬁrstly perform the
product excluding the ˆi term, and then multiply the given result by the excluded
term presenting σˆi →0. This makes possible to apply Equation A.18.
Finally, also the integral of f(x) can be computed, that results proportional to
149

Bayesian gravity inversion by Monte Carlo methods
the normal cumulative distribution, as follows:
F(x) =
Z x
−∞
f(x) dx =
Qn
i=1 N[µi,σ2
i](x = 0)
N[µ,σ2](x = 0)
Z x
−∞
N[µ,σ2]dx =
= H
Z x
−∞
N[µ,σ2]dx
(A.19)
A.3
Normal likelihood distribution
The normal distribution can be used to describe the likelihood distribution when
working in a Bayesian framework. Recalling that y is the vector of data, x is the
vector of unknown and applying the deﬁnition of joint probability the following
relationship holds:
(
P(x, y) = L(y|x) P(x)
P(x, y) = P(xi|x−i, y) P(x−i, y)
⇒P(xi|x−i, y) ∝L(y|x) P(x)
(A.20)
In that case, the aim is to retrieve the conditional distribution of one of the param-
eters xi given the value of all the other parameters x−i and of the data observations
y. The assumption is that the likelihood has a normal distribution, according to the
assumption of Section 4.2.1, namely:
L(y|x) = N[Ax,C](y|x)
(A.21)
where A represents the linear model between parameters and data and C the covari-
ance matrix of the observation noise. In the following the terms containing xi are
isolated in L(y|xi, x−i), thus making possible to determine P(xi|x−i, y) apart from
a normalization constant. Therefore, starting from the expression of the likelihood,
obtained introducing Equation A.2 into A.21, and disregarding the normalization
constant, the following relationship holds:
L(y|xi, x−i) = N[Ax,C](y|xi, x−i) ∝exp

−1
2[y −Ax]⊺C−1[y −Ax]

=
= exp

−1
2[y −A−ix−i −aixi]⊺C−1[y −A−ix−i −aixi]

=
= exp

−1
2

a⊺
i C−1aix2
i −2a⊺
i C−1(y −A−ix−i) xi+
+(y −A−ix−i)⊺C−1(y −A−ix−i)
 
=
= exp

−a⊺
i C−1ai
2

x2
i −2a⊺
i C−1(y −A−ix−i)
a⊺
i C−1ai
xi+
+(y −A−ix−i)⊺C−1(y −A−ix−i)
a⊺
i C−1ai

=
150

APPENDIX A. NORMAL PROBABILITY
= exp
(
−a⊺
i C−1ai
2

xi −a⊺
i C−1(y −A−ix−i)
a⊺
i C−1ai
2
+
+[a⊺
i C−1(y −A−ix−i)]2
2a⊺
i C−1ai
−1
2(y −A−ix−i)⊺C−1(y −A−ix−i)
)
=
= exp
(
−a⊺
i C−1ai
2

xi −a⊺
i C−1(y −A−ix−i)
a⊺
i C−1ai
2
+
+ [a⊺
i C−1(y −A−ix−i)]⊺[a⊺
i C−1(y −A−ix−i)]
2a⊺
i C−1ai
+
−1
2(y −A−ix−i)⊺C−1(y −A−ix−i)

=
= exp
(
−a⊺
i C−1ai
2

xi −a⊺
i C−1(y −A−ix−i)
a⊺
i C−1ai
2
+
+ (y −A−ix−i)⊺C−1aia⊺
i C−1(y −A−ix−i)
2a⊺
i C−1ai
+
−1
2(y −A−ix−i)⊺C−1(y −A−ix−i)

=
= exp
(
−a⊺
i C−1ai
2

xi −a⊺
i C−1(y −A−ix−i)
a⊺
i C−1ai
2
+
−1
2(y −A−ix−i)⊺

C−1 −C−1aia⊺
i C−1
a⊺
i C−1ai

(y −A−ix−i)

=
(A.22)
The result of Equation A.22 show that, once the terms relative to xi are explicit,
the shape of the function remain normal, with the following mean and variance:







σ2
i =
1
a⊺
i C−1ai
µi = −a⊺
i C−1(y −A−ix−i)
a⊺
i C−1ai
= σ2
i a⊺
i C−1(y −A−ix−i)
(A.23)
Therefore, introducing the results of Equations A.22 and A.23 into the deﬁnition of
the conditional probability shown in Equation A.20, P(xi|x−i, y) becomes:
P(xi|x−i, y) ∝L(y|x) P(x) =
1
p
(2π)m det C
exp

−1
2σ2
i
(xi −µi)2 +
−1
2(y −A−ix−i)⊺

C−1 −C−1aia⊺
i C−1
a⊺
i C−1ai

(y −A−ix−i)

P(xi, x−i) ∝
∝K(y, x−i) N[µi,σ2
i](xi) P(xi, x−i)
(A.24)
151

Bayesian gravity inversion by Monte Carlo methods
where K(y, x−i) is a constant grouping all the terms not dependent on xi.
A remark is that the results obtained into Equation A.24 depends also on the
shape of the prior probability P(x) that is not the focus of this section, where the
interesting results is the impact of a normally distributed likelihood into a Bayesian
approach.
152

Bibliography
Abramowitz M, Stegun IA (1964) Handbook of mathematical functions: with for-
mulas, graphs, and mathematical tables, vol 55. Courier Corporation
Ackley DH, Hinton GE, Sejnowski TJ (1985) A learning algorithm for Boltzmann
machines. Cognitive science 9(1):147–169
Aldous D (1987) On the Markov chain simulation method for uniform combina-
torial distributions and simulated annealing. Probability in the Engineering and
Informational Sciences 1(1):33–46
Aldous DJ (1990) The random walk construction of uniform spanning trees and
uniform labelled trees. SIAM Journal on Discrete Mathematics 3(4):450–465
Arnold V (1978) Graduate Texts in Mathematics, Vol. 60, Mathematical Methods
of Classical Mechanics
Azencott R (1988) Simulated annealing. Seminaire Bourbaki 30:223–237
Barbosa V, Silva J, Medeiros W (1997) Gravity inversion of basement relief using
approximate equality constraints on depths. Geophysics 62(6):1745–1757
Barbosa V, Silva J, Medeiros W (1999) Gravity inversion of a discontinuous relief
stabilized by weighted smoothness constraints on depth. Geophysics 64(5):1429–
1437
Bayes T (1763) An essay toward solving a problem in the doctrine of chances. Philo-
sophical Transactions of the Royal Society of London 53:370–418
Biagi L, Sans`o F (2001) TC Light: a new technique for fast RTC computation. In:
Sideris MG (ed) International Association of Geodesy Symposia. Gravity, Geoid
and Geodynamics 2000, vol 123, Springer, pp 61–66
Blackely RJ (1996) Potenntial Theory in Gravity and magnetic Applications. Cam-
bridge University Press
Borexino Collaboration (2015) Spectroscopy of geoneutrinos from 2056 days of
Borexino data. Physical Review D 92(3):31,101
Bosch M (1999) Lithologic tomography: From plural geophysical data to lithology
estimation. Journal of Geophysical Research: Solid Earth 104(B1):749–766
153

Bayesian gravity inversion by Monte Carlo methods
Bosch M, McGaughey J (2001) Joint inversion of gravity and magnetic data under
lithologic constraints. The Leading Edge 20(8):877–881
Box G, Tiao G (1973) Bayesian inference in statistical analysis. John Wiley & Sons
Box GE, Muller ME, et al (1958) A note on the generation of random normal devi-
ates. The annals of mathematical statistics 29(2):610–611
Boyd S, Vandenberghe L (2004) Convex optimization. Cambridge university press
Braitenberg C (2015) Exploration of tectonic structures with GOCE in Africa and
across-continents. International Journal of Applied Earth Observation and Geoin-
formation 35:88–95
Calcagno P, Chil`es JP, Courrioux G, Guillen A (2008) Geological modelling from ﬁeld
data and geological knowledge: Part I. Modelling method coupling 3D potential-
ﬁeld interpolation and geological rules. Physics of the Earth and Planetary Inte-
riors 171(1):147–157
Caratori Tontini F, Cocchi L, Carmisciano C (2009) Rapid 3-D forward model of
potential ﬁelds with application to the Palinuro Seamount magnetic anomaly
(southern Tyrrhenian Sea, Italy). Journal of Geophysical Research: Solid Earth
114(B2):1978–2012
Chen CH, Lee CY, Shinjo R (2008) Was there Jurassic paleo-Paciﬁc subduction in
South China?: constraints from 40 Ar/39 Ar dating, elemental and Sr–Nd–Pb
isotopic geochemistry of the Mesozoic basalts. Lithos 106(1):83–92
Chen J, Jahn Bm (1998) Crustal evolution of southeastern China: Nd and Sr isotopic
evidence. Tectonophysics 284(1):101–133
Christensen NI, Mooney WD (1995) Seismic velocity structure and composition of
the continental crust: A global view. Journal of Geophysical Research: Solid Earth
100(B6):9761–9788
Dampney C (1977) Gravity interpretation for hydrocarbon exploration? a workshop
manual or learning to see geology in a gravity proﬁle. Exploration Geophysics
8(4):161–180
Deng Y, Zhang Z, Badal J, Fan W (2014) 3-D density structure under South China
constrained by seismic velocity and gravity data. Tectonophysics 627:159–170
Dumrongchai P (2007) Small anomalous mass detection from airborne gradiometry.
Tech. Rep. 482, The Ohio State University
ESA (2008) http://www.esa.int/Our_Activities/Observing_the_Earth/GOCE/
A_force_that_shapes_our_planet. Website, ESA oﬃcial website, last checked:
04.06.2017
Fedi M, Rapolla A (1993) I metodi gravimetrico e magnetico nella geoﬁsica della
Terra solida. Liguori Editore
154

BIBLIOGRAPHY
Fiorentini G, Lissia M, Mantovani F (2007) Geo-neutrinos and Earth’s interior.
Physics Reports 453(5):117–172
F¨orste C, Bruinsma S, Abrikosov O, Lemoine JM, Schaller T, G¨otze HJ, Ebbing
J, Marty JC, Flechtner F, Balmino G, Biancale R (2014) EIGEN-6C4-The latest
combined global gravity ﬁeld model including GOCE data up to degree and order
1949 of GFZ Potsdam and GRGS Toulouse. 5th GOCE User Workshop
Gardner G, Gardner L, Gregory A (1974) Formation velocity and density - The
diagnostic basics for stratigraphic traps. Geophysics 39(6):770–780
Gelman A, Carlin JB, Stern HS, Rubin DB (1995) Bayesian data analysis. Chapman
and Hall
Geman S, Geman D (1984) Stochastic Relaxation, Gibbs Distributions, and the
Bayesian Restoration of Images. IEEE Transactions on Pattern Analysis and Ma-
chine Intelligence PAMI-6:721–741
Godson RH, PlouﬀD (1988) BOUGUER Version 1.0: a microcomputer gravity-
terrain-correction program. Tech. rep., US Geological Survey,
Goetze H, Keller F, Lahmeyer B, Rosenbach O (1982) Interactive modeling and
interpretation of three dimensional gravity data. In: SEG Technical Program Ex-
panded Abstracts 1982, Society of Exploration Geophysicists, pp 343–344
G´omez-Ortiz D, Agarwal BN (2005) 3DINVER.M: a MATLAB program to invert
the gravity anomaly over a 3D horizontal density interface by Parker–Oldenburg’s
algorithm. Computers & geosciences 31(4):513–520
Gordon A, Mohriak W, Barbosa V (2012) Crustal architecture of the Almada Basin,
NE Brazil: An example of a non-volcanic rift segment of the South Atlantic passive
margin. Geological Society, London, Special Publications 369
Gray RM (2006) Toeplitz and Circulant Matrices: A Review. Foundations and
Trends in Communications and Information Theory 2(3):155–239
G¨otze H, Lahmeyer B (1988) Application of three-dimensional interactive modeling
in gravity and magnetics. Geophysics 53(8):1096–1108
Guillen A, Menichetti V (1984) Gravity and magnetic inversion with minimization
of a speciﬁc functional. Geophysics 49(8):1354–1360
Guillen A, Calcagno P, Courrioux G, Joly A, Ledru P (2008) Geological modelling
from ﬁeld data and geological knowledge: part II. Modelling validation using
gravity and magnetic data inversion. Physics of the Earth and Planetary Interiors
171(1):158–169
Hammersley J (1974) Discussion of Mr Besag’s paper. J Roy Statist Soc Ser B
36:230–231
155

Bayesian gravity inversion by Monte Carlo methods
Hao TY, Hu WJ, Xing J, Hu LT, Xu Y, Qin JX, Liu SH, Lei SM (2014) The Moho
depth map (1: 5000000) in the land and seas of China and adjacent areas and its
geological implications. Chinese Journal of Geophysics (in Chinese) 57(12):3869–
3883
Hastings WK (1970) Monte Carlo sampling methods using Markov chains and their
applications. Biometrika 57(1):97–109
He C, Dong S, Santosh M, Chen X (2013) Seismic evidence for a geosuture between
the Yangtze and Cathaysia Blocks, South China. Scientiﬁc reports 3
Heiskanen WA, Moritz H (1967) Physical geodesy. W. H. Freeman and company
Herglotz G (1907) ¨Uber das benndorfsche problem des fortpﬂanzungsgeschwindigkeit
der erdbebenstrahlen. Zeitschrift f´ur Geophys 8:145–147
Huang Y, Chubakov V, Mantovani F, Rudnick RL, McDonough WF (2013) A ref-
erence Earth model for the heat-producing elements and associated geoneutrino
ﬂux. Geochemistry, Geophysics, Geosystems 14(6):2003–2029
Jeﬀreys H (1961) The theory of probability. Oxford University Press
Ji S, Wang Q, Salisbury MH, Wang Y, Jia D (2017) Reprint of: P-wave velocities
and anisotropy of typical rocks from the Yunkai Mts.(Guangdong and Guangxi,
China) and constraints on the composition of the crust beneath the South China
Sea. Journal of Asian Earth Sciences 141:213–234
Jia SX, Li ZX, Xu ZF, Shen FL, Zhao WJ, Yang ZX, Yang J, Lei W (2006) Crustal
structure features of the Leiqiong depression in Hainan Province. Chinese Journal
of Geophysics 49(5):1255–1266
Jiang YH, Jiang SY, Dai BZ, Liao SY, Zhao KD, Ling HF (2009) Middle to late
Jurassic felsic and maﬁc magmatism in southern Hunan province, southeast China:
Implications for a continental arc to rifting. Lithos 107(3):185–204
John B, Zhou X, Li J (1990) Formation and tectonic evolution of southeastern China
and Taiwan: isotopic and geochemical constraints. Tectonophysics 183(1-4):145–
160
KamLAND Collaboration (2013) Reactor on-oﬀantineutrino measurement with
KamLAND. Physical Review D 88(3):033,001
Kaufmann O, Martin T (2008) 3D geological modelling from boreholes, cross-sections
and geological maps, application over former natural gas storages in coal mines.
Computers & Geosciences 34(3):278–290
Kellogg OD (1929) Foundations of potential theory. Verlag Von Julius Springer
Kirkpatrick S, Gelatt CD, Vecchi MP, et al (1983) Optimization by simulated an-
nealing. Science 220(4598):671–680
156

BIBLIOGRAPHY
Koch KR (2013) Parameter estimation and hypothesis testing in linear models.
Springer Science & Business Media
Krynski J (2012) Gravimetry for geodesy and geodynamics-brief historical review.
Reports on Geodesy
LaCoste LJ (1934) A new type long period vertical seismograph. Physics 5(7):178–
180
Laske G, Ma Z, Masters G, Pasyanos M (2013) https://igppweb.ucsd.edu/~gabi/
crust1.html. Website, last checked: 04.06.2017
Last B, Kubik K (1983) Compact gravity inversion. Geophysics 48(6):713–721
Li X, Li W, Li ZX, et al (2007) On the genetic classiﬁcation and tectonic implications
of the Early Yanshanian granitoids in the Nanling Range, South China
Li Y, Oldenburg DW (1998) 3-D inversion of gravity data. Geophysics 63(1):109–119
Li ZX (1994) Collision between the North and South China blocks:
a crustal-
detachment model for suturing in the region east of the Tanlu fault. Geology
22(8):739–742
Longman I (1959) Formulas for computing the tidal accelerations due to the moon
and the sun. Journal of Geophysical Research 64(12):2351–2355
Ma G, Li L (2012) Edge detection in potential ﬁelds with the normalized total
horizontal derivative. Computers & Geosciences 41:83–87
Marussi A (1985) Intrinsic geodesy. Springer
Medeiros W, Silva J (1996) Geophysical inversion using approximate equality con-
straints. Geophysics 61(6):1678–1688
Menke W (1984) Geophysical data analysis: discrete inverse theory. Academic press
Metropolis N, Rosenbluth AW, Rosenbluth MN, Teller AH, Teller E (1953) Equation
of state calculations by fast computing machines. The journal of chemical physics
21(6):1087–1092
Monahan D (2008) Mapping the ﬂoor of the entire world ocean: the general bathy-
metric chart of the oceans. Journal of Ocean Technology 3:108
Morelli C, Gantar C, Honkasalo T, McConnell R, Tanner J, Szabo B, Uotila U,
Whalen C (1974) The international gravity standardization net 1971 (IGSN 71).
Tech. rep., IUGG-IAG - Publ. Spec. 4
Mosegaard K, Tarantola A (1995) Monte Carlo sampling of solutions to inverse
problems. Journal of Geophysical Research: Solid Earth 100(B7):12,431–12,447
Mosegaard K, Tarantola A (2002) Probabilistic approach to inverse problems. Inter-
national Geophysics 81:237–265
157

Bayesian gravity inversion by Monte Carlo methods
Moussouris J (1974) Gibbs and Markov random systems with constraints. Journal
of statistical physics 10(1):11–33
Nagihara S, Hall S (2001) Three-dimensional gravity inversion using simulated an-
nealing: Constraints on the diapiric roots of allochthonous salt structures. Geo-
physics 66(5):1438–1449
Nagy D, Papp G, Benedek J (2000) The gravitational potential and its derivatives
for the prism. Journal of Geodesy 74(7-8):552–560
Neal RM (1993) Probabilistic inference using Markov chain Monte Carlo methods.
Tech. rep.
Neal RM (1998) Suppressing random walks in Markov chain Monte Carlo using
ordered overrelaxation. NATO Science Series D: Behavioural and Social Sciences
89:205–230
Nettleton LL (1971) Elementary gravity and magnetics for geologists and seismolo-
gists. Society of Exploration Geophysicists
Newton I (1687) Philosophiae Natvralis Principia Mathematica
Nissen SS, Hayes DE, Bochu Y, Zeng W, Chen Y, Nu X (1995) Gravity, heat
ﬂow, and seismic constraints on the processes of crustal extension: Northern
margin of the South China Sea. Journal of Geophysical Research: Solid Earth
100(B11):22,447–22,483
Oezsen R (2004) Velocity modelling and prestack depth imaging below complex
salt structures: a case history from on-shore Germany. Geophysical prospecting
52(6):693–705
Oldenburg DW (1974) The inversion and interpretation of gravity anomalies. Geo-
physics 39(4):526–536
Papoulis A (1965) Probability, random variables, and stochastic processes. McGraw-
Hill
Papoulis A (1977) Signal Analysis, vol. 191
Parker R (1973) The rapid calculation of potential anomalies. Geophysical Journal
International 31(4):447–455
Parker R (1975) The theory of ideal bodies for gravity interpretation. Geophysical
Journal International 42(2):315–334
Parker RL (1974) Best bounds on density and depth from gravity data. Geophysics
39(5):644–649
Parker RL (1994) Geophysical inverse theory. Princeton university press
Pincus M (1968) A closed form solution of certain programming problems. Opera-
tions Research 16(3):690–694
158

BIBLIOGRAPHY
Pincus M (1970) A Monte Carlo method for the approximate solution of certain types
of constrained optimization problems. Operations Research 18(6):1225–1228
Qiu X, Ye S, Wu S, Shi X, Zhou D, Xia K, Flueh ER (2001) Crustal structure across
the Xisha trough, northwestern South China Sea. Tectonophysics 341(1):179–193
Reguzzoni M, Sampietro D (2015) GEMMA: An Earth crustal model based on
GOCE satellite data. International Journal of Applied Earth Observation and
Geoinformation 35:31–43
Reguzzoni M, Tselfes N (2009) Optimal multi-step collocation: application to the
space-wise approach for GOCE data analysis. Journal of Geodesy 83(1):13–29
Reynolds JM (1997) An introduction to applied and environmental geophysic. John
Wiley and sons
Robert CP, Casella A (2004) Monte carlo methods. Springer
Rossi L, Reguzzoni M, Sampietro D, Sans`o F (2015) Integrating geological prior
information into the inverse gravimetric problem: the bayesian approach. In: VIII
Hotine-Marussi Symposium on Mathematical Geodesy, Springer, pp 317–324
Roy A (1962) Ambiguity in geophysical interpretation. Geophysics 27(1):90–99
Roy L, Sen M, Blankenship D, Stoﬀa P, Richter T (2005) Inversion and uncertainty
estimation of gravity data using simulated annealing: An application over Lake
Vostok, East Antarctica. Geophysics 70(1):J1–J12
Rubinstein RY, Kroese DP (2004) The cross-entropy method: A uniﬁed approach
to Monte Carlo simulation, randomized optimization and machine learning. Infor-
mation Science & Statistics, Springer Verlag, NY
Sampietro D (2015) Geological units and Moho depth determination in the Western
Balkans exploiting GOCE data. Geophysical Journal International 202(2):1054–
1063
Sampietro D, Sans`o F (2012) Uniqueness theorems for inverse gravimetric problems.
IAG Symp 137:111–115
Sampietro D, Capponi M, Triglione D, Mansi AH, Marchetti P, Sans`o F (2016)
GTE: a new software for gravitational terrain eﬀect computation: theory and
performances. Pure and Applied Geophysics 173(7):2435–2453
Sans`o F, Schuh WD (1987) Finite covariance functions. Bulletin g´eod´esique
61(4):331–347
Sans`o F, Sideris MG (2012) Geoid Determination: Theory and Methods. Lecture
Notes in Earth System Sciences, Springer
Sans`o F, Reguzzoni M, Triglione D (2011) Metodi Monte Carlo e delle Catene di
Markov: una introduzione (in Italian). Maggioli Editore
159

Bayesian gravity inversion by Monte Carlo methods
Schuh WD (1996) Tailored numerical solution strategies for the global determination
of the earth’s gravity ﬁeld: study of the complementary use of the gradiometry
and global positioning system (GPS) for the determination of the Earth’s gravity
ﬁeld; technical report. Techn. Univ. Graz, Inst. f¨ur Theoret. Geod¨asie
Schwarz K, Li Z (1997) An introduction to airborne gravimetry and its boundary
value problems. In: Sans`o F, Rummel R (eds) Geodetic boundary value problems
in view of the one centimeter geoid, Springer, pp 312–358
Silva Dias F, Barbosa V, Silva J (2009) 3D gravity inversion through an adaptive-
learning procedure. Geophysics 74(3):I9–I21
Smith A, Roberts G (1993) Bayesian computation via the Gibbs sampler and related
Markov chain Monte Carlo methods. Journal of the Royal Statistical Society Series
B (Methodological) 55:3–23
Snieder R, Trampert J (2000) Linear and nonlinear inverse problems. In: Dermanis
A, Gr¨un A, Sans`o F (eds) Geomatic methods for the analysis of data in the Earth
sciences, Lecture Notes in Earth Science, vol 95, pp 93–164
Somigliana C (1929) Teoria generale del campo gravitazionale dell’ellissoide di ro-
tazione. Memorie della Societ`a Astronomica Italiana 4:425
ˇSr´amek O, McDonough WF, Kite ES, Leki´c V, Dye ST, Zhong S (2013) Geophysical
and geochemical constraints on geoneutrino ﬂuxes from Earth’s mantle. Earth and
Planetary Science Letters 361:356–366
Sun Y, Toks¨oz MN (2006) Crustal structure of China and surrounding regions from
P wave traveltime tomography. Journal of Geophysical Research: Solid Earth
111(B3)
Talwani M, Ewing M (1960) Rapid computation of gravitational attraction of three-
dimensional bodies of arbitrary shape. Geophysics 25(1):203–225
Tanner MA (1991) Tools for statistical inference, vol 3. Springer
Tarantola A (2005) Inverse problem theory and methods for model parameter esti-
mation. SIAM
Tarantola A, Valette B (1982) Inverse problems = quest for information. Journal of
Geophysics 50(3):150–170
Telford WM, Telford W, Geldart L, SheriﬀRE (1990) Applied geophysics, vol 1.
Cambridge university press
Tikhonov A (1963) Solution of incorrectly formulated problems and the regulariza-
tion method. Soviet Math Dokl 5:1035–1038
Tkalˇci´c H, Chen Y, Liu R, Zhibin H, Sun L, Chan W (2011) Multistep modelling
of teleseismic receiver functions combined with constraints from seismic tomog-
raphy: Crustal structure beneath southeast China. Geophysical Journal Interna-
tional 187(1):303–326
160

BIBLIOGRAPHY
Torge W (1989) Gravimetry. Walter de Gruyter
Torge W (2001) Geodesy. Walter de Gruyter
Tscherning CC, Forsberg R, Knudsen P (1992) The GRAVSOFT package for geoid
determination. In: Proceedings 1st IAG Continental Workshop of the Geoid in
Europe, Prague, pp 327–334
Uieda L, Barbosa VC (2012) Robust 3D gravity gradient inversion by planting
anomalous densities. Geophysics 77(4):G55–G66
Uieda L, Barbosa VC, Braitenberg C (2016) Tesseroids: Forward-modeling gravita-
tional ﬁelds in spherical coordinates. Geophysics 81(5):F41–F48
Walsh B (2004) Markov Chain Monte Carlo and Gibbs sampling. Lecture notes for
EEB 581, version 26
Wang TK, Chen MK, Lee CS, Xia K (2006) Seismic imaging of the transitional
crust across the northeastern margin of the South China Sea. Tectonophysics
412(3):237–254
Wang Y, Zhang F, Fan W, Zhang G, Chen S, Cawood PA, Zhang A (2010) Tectonic
setting of the South China Block in the early Paleozoic: Resolving intracontinental
and ocean closure models from detrital zircon U-Pb geochronology. Tectonics 29(6)
Wiechert E (1907) ¨Uber erdbebenwellen I. Theoretisches ¨uber die Ausbreitung der
erdbebenwellen. Nachrichten von der Gesellschaft der Wissenschaften zu G¨ottin-
gen, Mathematisch-Physikalische Klasse pp 415–549
Wiener N (1964) Time Series. M.I.T. Press
Wu Q, Xu H, Zou X (2005) An eﬀective method for 3D geological modeling with
multi-source data integration. Computers & Geosciences 31(1):35–43
Xia S, Zhao M, Qiu X, Xu H, Shi X (2010) Crustal structure in an onshore–oﬀshore
transitional zone near Hong Kong, northern South China Sea. Journal of Asian
Earth Sciences 37(5):460–472
Xia S, Shen Y, Zhao D, Qiu X (2015) Lateral variation of crustal structure and com-
position in the Cathaysia block of South China and its geodynamic implications.
Journal of Asian Earth Sciences 109:20–28
Xu X, O’Reilly SY, Griﬃn WL, Wang X, Pearson N, He Z (2007) The crust of
Cathaysia: age, assembly and reworking of two terranes. Precambrian Research
158(1):51–78
Zeng H, Zhang Q, Li Y, Liu J (1997) Crustal structure inferred from gravity anoma-
lies in South China. Tectonophysics 283(1):189–203
Zhang J, Jiang L (2017) Analytical expressions for the gravitational vector ﬁeld of
a 3-D rectangular prism with density varying as an arbitrary-order polynomial
function. Geophysical Journal International (In printing)
161

Bayesian gravity inversion by Monte Carlo methods
Zhang Z, Wang Y (2007) Crustal structure and contact relationship revealed from
deep seismic sounding data in South China. Physics of the Earth and Planetary
Interiors 165(1):114–126
Zheng Y, Zhang S (2007) Formation and evolution of Precambrian continental crust
in South China. Chinese Science Bulletin 52(1):1–12
Zhigljavsky A (1991) Theory of global random search. In: Pint´er J (ed) Mathematics
and its Applications (Soviet Series), vol 65, Kluwer Academic Publishers Group,
Dordrecht
162

