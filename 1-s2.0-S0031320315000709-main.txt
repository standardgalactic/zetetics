A novel visual codebook model based on fuzzy geometry for
large-scale image classiﬁcation
Yanshan Li a, Qinghua Huang b,c,n, Weixin Xie a, Xuelong Li d
a ATR National Key Laboratory of Defense Technology, Shenzhen University, Shenzhen 518060, PR China
b School of Electronic and Information Engineering, South China University of Technology, Guangzhou 510640, PR China
c National Engineering Research Center for Tissue Restoration and Reconstruction, Guangzhou, PR China
d The Center for OPTical IMagery Analysis and Learning (OPTIMAL), State Key Laboratory of Transient Optics and Photonics, Xi’an Institute of Optics
and Precision Mechanics, Chinese Academy of Sciences, Xi’an 710119, Shaanxi, PR China
a r t i c l e i n f o
Article history:
Received 1 October 2014
Received in revised form
12 January 2015
Accepted 16 February 2015
Available online 4 March 2015
Keywords:
Codebook
Fuzzy geometry
Fuzzy set theory
Image classiﬁcation
a b s t r a c t
The codebook model has been developed as an effective means for image classiﬁcation. However, the
inherent operation of assigning visual words to image feature vectors in traditional codebook approaches
causes serious ambiguities in image classiﬁcation. In particular, the nearest word may not be the best ﬁt to a
feature, and multiple words may be equally appropriate for one speciﬁc feature. To resolve these ambiguities,
we propose a novel visual codebook model based on the n-dimensional fuzzy geometry (n-D FG) theory,
where all visual words and features are modeled as fuzzy points in the n-D FG space, and appropriate
uncertainty is introduced to each fuzzy point to enhance the representation capacity. This n-D FG-codebook
model not only inherits advantages from the fuzzy set theory, but also facilitates the analysis and
determination of the relationship between visual words and features in geometric form. By explicitly taking
into account the ambiguities, we propose a novel measure of similarity between the visual words and fuzzy
features. Following the proposed codebook model and the novel similarity measure, we develop two useful
image classiﬁcation algorithms by modifying popular image coding algorithms (i.e. SPM and LLC). Finally,
experimental results demonstrate that the classiﬁcation accuracy of the proposed algorithms is dramatically
improved for a standard large-scale image database. For example, with a codebook size of 256, the proposed
algorithms achieve similar performance as traditional algorithms with a codebook size of 1024, indicating
that the proposed algorithms reduce the computational cost by 75% while achieving almost identical
classiﬁcation accuracy to traditional algorithms. Thus, the proposed algorithms represent a more efﬁcient
and appropriate scheme for big image data.
& 2015 Elsevier Ltd. All rights reserved.
1. Introduction
In recent years, the rapid growth of digital image data brings
huge challenges to traditional image classiﬁcation [3–12,15–30,
36–47]. The codebook approach, thanks to its simplicity and
effectiveness, is becoming more and more popular in image
classiﬁcation. Among the state-of-the-art image classiﬁcation
algorithms, e.g., bag of words (BoW) [7], spatial pyramid matching
(SPM) [38], Fuzzy-BoW [41], spatial pyramid matching using
sparse coding (ScSPM) [19], and locality-constrained linear coding
(LLC) [15], the codebook model plays a basic and key role.
Generally, the codebook approach for a given image classiﬁca-
tion problem includes the following ﬁve steps [7,15,38,41], as
depicted in Fig. 1. In the ﬁrst step, local features such as scale
invariant feature transform (SIFT) are extracted sparsely or densely
from the image [4,40]. In the second step, it is required to learn a
codebook from the extracted features using the k-means algorithm
[7,35], sparse coding [19], etc. In the third step, the features are
encoded into vectors using the codebook via a coding algorithm.
In the fourth step, pooling algorithm is adopted to construct an
image descriptor. In the last step, the classiﬁcation of the image
descriptor into categories is performed using classiﬁers such as
K-Nearest Neighbor (KNN) or support vector machine (SVM).
2. Related works
The codebook approach is easy to implement and it enjoys the
advantage of universal encoding of any type of images. Numerous
Contents lists available at ScienceDirect
journal homepage: www.elsevier.com/locate/pr
Pattern Recognition
http://dx.doi.org/10.1016/j.patcog.2015.02.010
0031-3203/& 2015 Elsevier Ltd. All rights reserved.
n Corresponding author at: School of Electronic and Information Engineering,
South China University of Technology, Guangzhou 510640, PR China.
Tel.: þ86 15013262380.
E-mail address: qhhuang@scut.edu.cn (Q. Huang).
Pattern Recognition 48 (2015) 3125–3134

works have been devoted to improving its performance in image
classiﬁcation. Ji et al. [32] tried to construct the codebook via
supervised learning. A Hidden Markov Random Field model was
used to supervise feature space quantization with special con-
siderations to label correlations. Sprechmann et al. [28] investi-
gated a set of dictionaries, each of which corresponds to a
category. Wang et al. [11] proposed a dictionary construction
method using the sparse coding model. Yang et al. [16] proposed
an optimization framework that uniﬁes codebook generation with
classiﬁer training. Shen et al. [15] proposed a codebook learning
method by taking advantage of hierarchical category correlation to
obtain discriminative codebook. Yan et al. [34] proposed a non-
local dictionary learning method using multi-resolution structure
and sparsity of wavelets in each decomposition level of the
wavelets. Aharon et al. [26] proposed a novel algorithm for
adapting dictionaries named as K-SVD which can learn and update
the dictionary in the process of representing an image.
3. Motivation and contribution
The state-of-the-art codebook approaches suffer from inherent
shortcomings due to ambiguous mapping between words and
features [20], giving rise to two serious issues, i.e. words uncer-
tainty and words plausibility. The words uncertainty refers to the
problem of selecting appropriate word out of two or more relevant
candidates. The words plausibility denotes the problem of select-
ing a word without a suitable candidate in the codebook. The
reason for the ambiguity lies in various aspects. First, local features
extracted from an image are imperfect information because they
cover only partial intrinsic variation in visual appearance. When
they are used to learn the codebook, the uncertainty translates
into
the
codebook
as
well.
Second,
in
the
state-of-the-art
approaches, the visual words of codebook are generated by the
cluster algorithms where the centers of the clusters are viewed as
visual words. This process also introduces ambiguity to the
mapping between the visual words and the features as the cluster
is an approximate method for partition data and the centers
themselves lose a large amount of information of the features.
Third, traditional approaches assign the visual words to a feature
according to the similarity. Unfortunately, the similarity measure
and assignment
both introduce ambiguity to the codebook
approach for image classiﬁcation.
The main objective of this paper is to resolve the ambiguities in
traditional image classiﬁcation. The main methodology adopted in
this paper is the Fuzzy Geometry (FG) [1,2,13,14,31,33,35], a
branch of fuzzy set theory (FST) that deals with uncertainty and
plausibility well. Speciﬁcally, we propose a novel visual codebook
model based on the n-dimensional fuzzy geometry (n-D FG)
theory [33], where all visual words and features are modeled as
fuzzy points in the n-D FG space and appropriate uncertainty is
introduced to each fuzzy point to enhance the representation
capacity. In addition, we propose a novel measure of similarity
between visual words and fuzzy features by explicitly taking into
account the ambiguities. Based on the proposed codebook model
and the novel similarity measure, we develop two useful image
classiﬁcation algorithms which substantially reduce the computa-
tional cost yet achieving almost identical classiﬁcation accuracy to
traditional algorithms, thus constituting an efﬁcient and promising
scheme for big image data.
The rest of the paper is organized as follows. Section 4
introduces preliminaries of the n-D FG theory. Section 5 proposes
the fuzzy codebook model based on n-D FG, and the novel
similarity measure is subsequently introduced. Section 6 develops
two modiﬁed image coding algorithms for image classiﬁcation.
Section 7 presents experimental results and the corresponding
analysis. Finally, conclusion remarks are given in Section 8.
4. Preliminaries of n-D FG
The FG has attracted a great deal of research interests since
fuzzy sets were ﬁrst introduced in 1968. The geometrical proper-
ties of fuzzy sets have been extensively studied in various aspects
including fuzzy point, fuzzy line, and distance measure between
fuzzy points and fuzzy circle [1,2,13,14]. Following the work of
Buckley and Eslami [13,14], Wang [33] developed the n-D fuzzy
geometry theory including fundamental concepts and properties
of fuzzy point, distance between fuzzy points, fuzzy vector, fuzzy
line and fuzzy quadratic hyper-surface on ℝn(nZ1), etc. The FG
provides an effective framework to analyze and compute fuzzy
information in a geometric form. It has been successfully used in
information processing [31,33,35]. To exploit the advantages of FG,
this paper uses the n-D FG to analyze and process the ambiguity
information in the codebook approach for image classiﬁcation.
According to Buckley et al. [13], we introduce the following
notation and deﬁnitions. Capital or small letters with a bar (e.g., A,
N, W, Q , or a, w) denote fuzzy subsets or fuzzy points of ℝn. The
membership function of a fuzzy set A of ℝn is represented by
μðxjAÞ, xAℝn, with μðxjAÞD½0; 1.
Let domain X be a ﬁnite or countable set and A ¼ x1; x2; :::; xn
f
g.A
is the fuzzy set in A and μðxijAÞ is the fuzzy membership function.
Thus, A is expressed as
A ¼ μðx1j AÞ=x1 þμðx2jAÞ=x2 þ⋯þμðxnjAÞ=xn ¼
X
n
i ¼ 1
μðxij AÞ=xi
ð1Þ
where xi AA, n is the number of the elements in A, μðxijAÞ=xi
denotes the fuzzy degree of xi to A, and the symbol Pn
i ¼ 1 depicts
the relationship between the elements and the membership
function.
Fig. 1. Block diagram of the state-of-the-art codebook approach for image classiﬁcation.
Y. Li et al. / Pattern Recognition 48 (2015) 3125–3134
3126

Fuzzy number is the basic concept of FG, which is described as
follows.
Deﬁnition 1. [31]. A fuzzy set N of ℝis called a fuzzy real number
(fuzzy number for short) if its membership function μ satisﬁes the
following properties:
1. μðxj NÞ ¼ 1 is upper semi-continuous in x;
2. μðxj NÞ ¼ 1 for x outside some interval[c, d];
3. For some real numbers with crarbrd, μðxjNÞ is monotoni-
cally increasing in ½c; a, monotonically decreasing in ½b; d, and
μðxj NÞ ¼ 1 for x in ½a; b.
Let FðℝÞ denote the set of all fuzzy numbers in ℝ. There are
three basic types of fuzzy numbers explained as follows.
(1) Interval number:
Fuzzy number is the extension of interval number. If A  ℝis a
closed interval, A is an interval number.
(2) Triangular fuzzy number:
Given AAF ℝ
ð Þ, A is a triangular fuzzy number, if its fuzzy
membership function can be represented as follows:
AðxÞ ¼
0;
xoAL
x AL
AC AL;
AL rxrAC
x AR
AC AR;
AC rxrAR
0;
x4AR
8
>
>
>
>
>
<
>
>
>
>
>
:
ð2Þ
where AC is the center, AL is the left bound, and AR is the right
bound. Triangular fuzzy number is denoted as an ordered
triple A ¼
AL; AC; AR


.
(3) Normal fuzzy number:
Given AAF ℝ
ð Þ, A is a normal fuzzy number, if its fuzzy
membership function can be represented as follows:
AðxÞ ¼ e ðxaÞ=σ
ðσ 40; a; xAℝÞ
ð3Þ
where a is the expectation and σ is the standard deviation (SD).
Normal fuzzy number is denoted as A ¼ Nða; σÞ.
Deﬁnition 2. [33]. Let A be a fuzzy set on ℝn. The α-Cut of A is
denoted by AðαÞ and deﬁned as follows:
AðαÞ ¼
x : μðxjAÞZα
n
o
if
0oαr1
x : μðxjAÞ40
n
o
if
α ¼ 0
8
>
<
>
:
ð4Þ
where μðUÞ is the membership function of x to A. When N is a
fuzzy number, NðαÞ is a bounded closed interval for all α.
Deﬁnition 3. [33]. Let p ¼ ða1; a2; …; anÞ be a point in ℝn(nZ1).
A fuzzy point at p, written as Pða1; …; anÞ, is deﬁned by its
membership function:
(1) μ ðx1; …; xnÞjPða1; …; anÞ

 is upper semi-continuous;
(2) μ ðx1; …; xnÞjPða1; …; anÞ

 ¼ 1
if
and
only
if
ðx1; …; xnÞ ¼
ða1; …; anÞ;
(3) PðαÞ is a compact convex subset of ℝn for all α in [0,1].
The n-D fuzzy point is the basic element for computation and
analysis on n-D FG space. The set of all fuzzy points in ℝn is
denoted as FðℝnÞ. A fuzzy point can be viewed as a collection of
points with different membership values. If X ¼ ðx1; …; xnÞAℝn is a
point and P is a fuzzy point in ℝn, the membership function of P is
μððx1; …; xnÞj PÞ,
where
0rμððx1; …; xnÞjPÞr1.
μððx1; …; xnÞjPÞ
reﬂects the degree of membership of X to P. The αcut of P is
deﬁned as fx : μððx1; …; xnÞj PÞZαg, written as AðαÞ, 0oαr1.
5. Codebook on n-D FG
5.1. Fuzzy codebook model on n-D FG
The traditional algorithm of generating the visual codebook
based on the k-means algorithm can be described as follows.
Step 1. Extracting the features from the training images to form a
feature set.
Step 2. Constructing the codebook with the k-means algorithm.
The
feature
set
after
k-means
clustering
algorithm
is
S_cluster ¼ C1; C2; …; Ck

,
where
Ci ¼ Q i;1; Q i;2; …;

Q i;NigAℝnNi, Qi;j is the local feature belonging to cluster
Ci extracted from the training image, and Ni is the number
of feature points in cluster Ci. The set of the centers of
S_cluster serves as a traditional visual codebook, i.e.
B ¼ W1; W2; …; Wk

.
As mentioned in Section 2, there exists fuzzy ambiguity in
traditional codebook ambiguity. The fuzzy ambiguity may be
caused by the afﬁne transformation or noises introduced due to
the process of imaging, transmission, and processing. It can also be
caused during the generation of the codebook or in other stages of
the codebook approaches. The complex cause makes it difﬁcult to
model the fuzzy ambiguity with an exact mathematical model. It is
well known that fuzzy theory is effective for solving fuzzy
ambiguity. However, the feature and visual word in codebook
approaches are represented by high dimensional vectors and they
are difﬁcult to be modeled and analyzed with common fuzzy
theories. To tackle these challenges, we adopt fuzzy geometry to
solve the fuzzy ambiguity by taking advantage of the fuzzy set
theory and geometry. We model feature and visual word as fuzzy
points on n-D FG with speciﬁc fuzzy membership function and use
hypersphere on ℝn to design the membership function of the
fuzzy point. The motivation of adopting fuzzy point and hyper-
sphere here is to add fuzzy extension information to the feature
and visual word to enhance their representation ability.
In this study, we propose a novel codebook model on n-D FG,
referred to as n-D FG-Codebook (FG-Codebook for short). In FG-
Codebook, each visual word is modeled as a fuzzy point in the n-D
FG space. Appropriate uncertainty is added to each visual word to
enhance its representation capacity. Firstly, we give the deﬁnition
of fuzzy visual word and fuzzy codebook based on n-D FG as
follows.
Deﬁnition 4. Let B ¼ W1; W2; …; Wk

 be the traditional visual
codebook, Wi ¼ wi;1; …; wi;n

Aℝn be a crisp visual word, the
fuzzy visual word on Wi written as Wi wi;1; …; wi;n

 is deﬁned
by its membership function:
(1) μ ðx1; …; xnÞjWi wi;1; …; wi;n



 is upper semi-continuous;
(2) μ ðx1; …; xnÞjWi wi;1; …; wi;n



 if and only if ðx1; …; xnÞ ¼
ðwi;1; …; wi;nÞ;
(3) WiðαÞ is a compact convex subset of ℝn for all α in [0,1].
Deﬁnition 5. A Fuzzy codebook based on n-D FG is deﬁned as the
subset of fuzzy points on n-D FG, denoted as B ¼ W1; W2; …;

Wkg,
where Wi AFðℝnÞ is the ith fuzzy visual word, and k is size of the
codebook.
Y. Li et al. / Pattern Recognition 48 (2015) 3125–3134
3127

Fig. 2 shows a toy example of the FG-Codebook on 2-D space.
The gray level denotes the membership of a real point to the fuzzy
point. Actually, a fuzzy word is a fuzzy set on ℝn space. Every fuzzy
word Wi AB has its own membership function. The membership
function determines the shape of a fuzzy word on ℝn space.
Let Wi ¼ wi;1; wi;2; …; wi;n

 be a fuzzy word on crisp visual
word Wi ¼ wi;1; wi;2; …; wi;n

Aℝn, where wi;j is a fuzzy number.
Since every dimension of a word is equivalent, all elements of Wi
can be treated as a fuzzy number with identical membership
function. In this paper, we suppose that the elements of the fuzzy
word are triangular fuzzy numbers. The reasons for using trian-
gular fuzzy numbers lie in the following two respects. First, the
membership value of a point to a fuzzy word should be inversely
proportional to the distance between this point and the center of
the fuzzy word in the proposed fuzzy word model. The smaller the
distance is, the higher the membership value will be. Second,
compared with other types of fuzzy numbers, the triangular fuzzy
number is simpler and more suitable for modeling visual word
with high dimensions. Therefore, according to the deﬁnition of
triangular fuzzy number, wi;j can be denoted as follows:
wi;j ¼
wL
i;j; wC
i;j; wR
i;j


ð5Þ
j¼1…n, where wC
i;j is the center, wL
i;j is the left bound, and wi;jR
is the right bound of the fuzzy number wi;j. Now, it is obvious that
the
center
of
fuzzy
visual
word
Wi
is
actually
Wi ¼ wi;1; …; wi;n

Aℝn. Thus, wC
i;j can be replaced by wi;j, and
wi;j is represented as follows:
wi;j ¼
wL
i;j; wi;j; wR
i;j


ð6Þ
As explained earlier, every element of a fuzzy word is a fuzzy
triangular number with identical membership function. Therefore,
fuzzy visual word Wi can be deemed as a hypersphere in n-D
space. Assuming that the radius of the hypersphere is ρi, wi;j is
represented as follows:
wi;j ¼ ðwi;j ρi; wi;j; wi;j þρiÞ
ð7Þ
The set of all the radiuses for words in B is written as
Ρ ¼ ρ1; ρ2; …; ρk

. Hence, the fuzzy membership function of wi;j
can be represented as follows:
μðxj wi;jÞ ¼
0;
xowi;j ρi
x wi;j þρi
ρi
;
wi;j ρi rxrwi;j
x wi;j ρi
ρi
;
wi;j rxrwi;j þρi
0;
x4wi;j þρi
8
>
>
>
>
>
<
>
>
>
>
>
:
ð8Þ
The radius ρi of the hypersphere is obtained as follows:
ρi ¼
PNi
j ¼ 1 dðWi; Q jÞ
Ni
 η
ð9Þ
where Ni denotes the number of features in cluster Ci, η is a
parameter
determining
the
radius
of
hypersphere
Wi,
and
dðWi; QjÞ is the Euclidean distance between feature point Q j and
the cluster center of Ci. All of the fuzzy words in the fuzzy
codebook share the same value of η, through which we can adjust
the radiuses of all fuzzy words into suitable ranges. However, the
fuzzy words in the fuzzy codebook are different, as witnessed by
the different centers and different radiuses of hyperspheres. Wi is
regarded as the crisp part of the fuzzy word, which determines the
center of the ρi regarded as the fuzzy part which determines the
radius of hypersphere of Wi. The factor PNi
j ¼ 1 dðWi; Q jÞ=Ni in ρi of
(9) determines the difference of radius of hypersphere.
The membership function is designed based on the triangular
number, hypersphere, and the property of the visual word. It not
only maintains the original information of visual word, but also
adds the fuzzy extension information to the visual word. Speciﬁ-
cally, Fuzzy visual word Wi is determined by Wi and ρi. Wi is the
original information of the visual word, and ρi makes the fuzzy
visual word have extension information. Furthermore, the mem-
bership function based on the triangular number reﬂects that the
membership value is inversely proportional to the distance
between the point on n-D space and Wi. Therefore, the member-
ship function is reasonable.
5.2. Summary of the generation of FG-Codebook
The proposed FG-Codebook has two advantages. First, the
representation capacity of each visual word is improved by adding
appropriate uncertainty. Second, the n-D FG is capable of analyz-
ing and computing the relationship between visual word and
feature in geometric form. The process of generating FG-Codebook
is described as follows:
Step 1. Extract the features from training images to form a
feature set.
Step 2. Partition the feature set by the k-means algorithm into k
clusters represented by S_cluster. The centers of S_cluster
form the traditional visual codebook, i.e. B ¼ W1; W2;
f
…; Wkg.
Step 3. Calculate the values of Ρ ¼ ρ1; ρ2; …; ρk

 with Eq. (9).
Step 4. Generate the FG-Codebook B ¼ W1; W2; …; Wk


based on
B and Ρ via Eqs. (7) and (8).
5.3. Fuzzy feature on n-D FG
Suppose that S is the feature set extracted from images, and is
represented as follows:
S ¼ Q 1; Q2; …; QM


ð10Þ
where M is the number of features, and Qi Aℝn is the ith local
feature represented as follows:
Qi ¼ ðqi;1; qi;2; …; qi;nÞ
ð11Þ
Deﬁnition 6. Let Q i ¼ ðqi;1; qi;2; …; qi;nÞ be a local feature extracted
from one image. The fuzzy feature point on Q i, written as
Qiðqi;1; …; qi;nÞ, is deﬁned by its membership function:
(1) μ ðx1; …; xnÞjQiðqi;1; …; qi;nÞ


is upper semi-continuous;
Fig. 2. A toy example of the FG-Codebook on 2-D space.
Y. Li et al. / Pattern Recognition 48 (2015) 3125–3134
3128

(2) μ ðx1; …; xnÞjQiðqi;1; …; qi;nÞ


¼1 if and only if ðx1; …; xnÞ ¼
ðqi;1; …; qi;nÞ;
(3) Q ðαÞ is a compact convex subset of ℝn for all α in [0,1].
Given that qi;j is a triangular fuzzy number whose center is qi;j,
we can represent qi;j as follows:
qij ¼ ðqi;j τ; qi;j; qi;j þτÞ
ð12Þ
where τAℝ, qi;j is the center, qi;j τ is the left bound, and qi;j þτ is
the right bound of the fuzzy number qi;j. Then, the fuzzy feature
point Qi on Q i can be represented as follows:
Qi ¼ ðqi;1; qi;2; …; qi;nÞ
ð13Þ
The membership of a fuzzy point xAℝto qj is as follows:
μðxj qi;jÞ ¼
0;
xoqi;j τ
x qi;j þτ
τ
;
qi;j τrxrqi;j
x qi;j τ
τ
;
qi;j rxrqi;j þτ
0;
x4qi;j þτ
8
>
>
>
>
>
<
>
>
>
>
>
:
ð14Þ
The set of fuzzy features is represented by S ¼
Q 1; Q2;
n
…; Q Mg,
where M is the number of fuzzy features. It can be seen that Qi is
determined by Qi and τ. Q i is the crisp part of Q i and τ determines
the fuzzy part of Q i. The generation of the fuzzy ambiguity of a
feature is complex and is difﬁcult to be modeled. Hence, all of the
fuzzy features share the same value of τ in this paper.
5.4. Similarity measure between features and words on n-D FG
In codebook approach for image classiﬁcation, the word to be
assigned to a feature is determined by the similarity measure
between features and words. As visual words and features are
modeled with fuzzy points on n-DFG, the similarity between fuzzy
words and fuzzy features cannot be directly measured by Eucli-
dean distance or Geodesic distance, which are used to measure the
distance between crisp points. According to the property of fuzzy
words and fuzzy features, a new similarity measure based on n-D
FG is designed and its correctness is proved.
Deﬁnition 7. Let fuzzy point Wj AB be a fuzzy visual word, fuzzy
point
Qi AS
be
a
fuzzy
feature,
Wj ¼ Wðwj;1; …; wj;nÞ
and
Q i ¼ Q ðqi;1; …; qi;nÞ, the similarity measure between Wj and Q i is
deﬁned by the map dF : FðℝnÞ  FðℝnÞ-½0; þ1Þ:
dFðWj; QiÞ ¼
Z 1
0
supfdλðu; vÞ : dλðu; vÞAΩðλÞgdλÞ
 
!1=2
ð15Þ
where FðℝnÞ is the set of all fuzzy points in ℝn, u AWjðλÞ and
vAQiðλÞ, λ is the membership value, dλðu; vÞ is the distance
measured between points u and v in ℝn with the given λ,
ΩðλÞ ¼ fdλðu; vÞ : u AWjðλÞ;
vAQ iðλÞ g,
and
supfdλðu; vÞ :
dλðu; vÞAΩðλÞg
is
the
supremum
(sup
for
short)
of
subset
fdλðu; vÞ : dλðu; vÞAΩðλÞg. In this paper, as u and v are crisp points,
dλðu; vÞ uses Euclidean distance. We discuss the deﬁned map dF as
follows.
According to Deﬁnitions 4 and 6, both Wj and Q i can be seen as
two hyperspheres with center Wjðwj;1; …; wj;nÞ and Qiðqi;1; …; qi;nÞ,
respectively. As triangular fuzzy number is employed and is
inversely proportional to the distance between the point and
center of the hypersphere, the fuzzy membership function of
point u to fuzzy point Wj can be deﬁned by the ratio of the
distance between u and the center of the hypersphere and the
radius of the hypersphere, as follows:
μðuj WjÞ ¼ 1dðu; WjÞ
ρj
ð16Þ
where uAℝn, Wj AFðℝnÞ, dðn; nÞ is Euclidean distance measure,
and ρj is the radius of the hypersphere.
Likewise, the deﬁnition of membership function of a point to a
fuzzy feature can be obtained as follows:
μðvjQ iÞ ¼ 1dðv; QiÞ
τ
ð17Þ
where vAℝn, Q i AFðℝnÞ, dðn; nÞ is the Euclidean distance measure,
and τ is the radius of the hypersphere. Then, the following
properties can be obtained.
dðu; WjÞ ¼ ð1μðujWjÞÞUρj
ð18Þ
dðv; QiÞ ¼ ð1μðvjQiÞÞUτ
ð19Þ
Then, the map dF can be represented as follows:
dFðWj; Q iÞ ¼
Z 1
0
supfdλðu; vÞ : dλðu; vÞAΩðλÞgdλÞ
 
!1=2
¼
Z 1
0
dðu; WjÞþdðWj; Q iÞþdðQi; vÞdλÞ
 
!1=2
¼
Z 1
0
ð1λÞρj þð1λÞτdλÞþdðWj; QiÞ
 
!1=2
¼ dðWj; QiÞþ1
2 ðρj þτÞ

1=2
ð20Þ
Theorem 1. ðFðℝnÞ; dFÞ is a metric space.
Proof. To prove that ðFðℝnÞ; dFÞ is a metric space, we need to
prove that dF
satisﬁes the following conditions for
8P1; P2;
P3 AFðℝnÞ.
(1) dFðP1; P2ÞZ0;
(2) dFðP1; P2Þ ¼ 0 if and only if P1 ¼ P2;
(3) dFðP1; P2Þ ¼ dFðP2; P1Þ;
(4) dFðP1; P2ÞrdFðP1; P3ÞþdFðP3; P2Þ should be satisﬁed.
It can be seen that the conditions (1)–(3) are obvious. Hence,
we give the proof of condition (4) as follows. Suppose that the
radiuses of hyperspheres of P1; P2 and P3 are ρ; τ and ω respec-
tively. From Eq. (20), we have
d2
FðP1; P2Þ ¼ dðP1; P2Þþ1
2 ðρþτÞ
rdðP1; P3ÞþdðP3; P2Þþ1
2 ðρþωÞþ1
2 ðωþτÞ
¼ d2
FðP1; P3Þþd2
FðP3; P2Þ
ð21Þ
Due to d2
FðP1; P3ÞZ0 and d2
FðP3; P2ÞZ0, we have the following
equation
d2
FðP1; P3Þþd2
FðP3; P2Þ
rd2
FðP1; P3Þþd2
FðP3; P2Þþ2dFðP1; P3ÞdFðP3; P2Þ
¼ dFðP1; P3ÞþdFðP3; P2Þ

2
ð22Þ
Obviously,
dFðP1; P2ÞrdFðP1; P3ÞþdFðP3; P2Þ
ð23Þ
Therefore, ðFðℝnÞ; dFÞ is a metric space. dF can be used as a
similarity measure between fuzzy features and fuzzy visual words
on n-D FG.
Y. Li et al. / Pattern Recognition 48 (2015) 3125–3134
3129

5.5. Reﬁne the similarity set with α  Cut of fuzzy set on n-D FG
Given a fuzzy feature Q i and a fuzzy codebook Wj ¼ Wj;1; Wj;2;

…; Wj;kg, the similarity measure is calculated as follows:
χi;j ¼ dFðQi; WjÞ
ð24Þ
The set of similarity measures between Q i and all fuzzy visual
words in Wj is represented as follows:
Γi ¼ fχi;1; χi;2; …; χi;kg
ð25Þ
The α  Cut of fuzzy set is employed here to select fuzzy words,
as follows:
Γi' ¼
0;
χi;j oα
Γi;
χi;j Zα
8
<
:
ð26Þ
6. Application in image classiﬁcation
Several image coding algorithms for image classiﬁcation have
been proposed in recent years. In order to verify the effectiveness
of the proposed FG-Codebook, we apply it in image classiﬁcation
under state-of-the-art image classiﬁcation models. In this study
we choose SPM and LLC as image coding framework. Before
introducing our algorithms, we introduce these two algorithms
brieﬂy, and more detailed information can be found in [38] and
[15].
SPM, developed from BoW by Lazebnik et al. [38], is a popular
and effective image coding algorithm for image classiﬁcation. It
overcomes the shortcoming of BoW in coding image that spatial
information of local features is not considered. It pools the features
by using the technology of Spatial Pyramid Matching. The algo-
rithm proposed in [38] can be described brieﬂy as follows.
Step 1. Extract local features from training images.
Step 2. Learn the codebook by using k-means algorithm on the
features extracted.
Step 3. Encode features extracted from a test image by using hard-
VQ.
Step 4. Pool the features by using SPM.
Step 5. Output the image descriptor for image classiﬁcation.
LLC is another popular codebook based coding algorithm for
image classiﬁcation proposed in 2010. The image classiﬁcation
accuracy base on it reaches front rank among the existing similar
algorithms on several benchmarks. It utilizes locality constraints to
project each descriptor into its local-coordinate system. Then the
projected coordinates are integrated by max pooling to generate
the ﬁnal representation. The algorithm proposed in [15] can be
described brieﬂy as follows:
Step 1. Extract local features from training images.
Step 2. Learn the codebook by using k-means algorithm on the
features extracted.
Step 3. Find K-Nearest Neighbors (KNN) of a feature from the
codebook.
Step 4. Encode features extracted from a test image by solving the
constrained least square ﬁtting problem.
Step 5. Pool the features by using multiscale spatial max pooling.
Step 6. Output the image descriptor for image classiﬁcation.
Based on the introduction of the two algorithms, we give the
main reasons for choosing these algorithms. Firstly, they use
different typical methods to encode features: hard VQ and locality
constraints coding. This ensures that the proposed FG-Codebook is
suitable for many feature coding methods. Secondly, these models
are widely used and perform well in image classiﬁcation. The
proposed algorithms are named as FG-SPM and FG-LLC. In the
following
we
give
a
detailed
description
of
the
proposed
algorithms.
6.1. FG-SPM
Based on the proposed FG-Codebook, we develop a new
image coding algorithm for classiﬁcation named FG-SPM. We
replace the codebook of SPM by FG-Codebook, and measure the
similarity between features and words via Eq. (26) in step 3 of the
SPM algorithm. Algorithm 1 gives the details of the proposed
FG-SPM.
Algorithm 1. The proposed FG-SPM algorithm.
Input:
The local features set S, the FG-Codebook
B ¼ W1; W2; …; Wk

.
output:
The coding vector for the image H.
1.
Generate fuzzy feature set S ¼
Q1; Q 2; …; QM
n
o
from S via
Eqs. (12)–(14).
2.
For i¼1 to k do
3.
For j¼1 to M do
4.
Measure similarity χi;j between Wi and Qj via Eq. (20).
5.
End for
6.
Obtain the set of the similarity Γi ¼ fχi;1; χi;2; …; χi;kg.
Find the most similar word to quantize the feature via Eq.
(26).
7.
End for
8.
Spatial pooling as [38] did, and forming the image
descriptor H.
9.
Output the image descriptor H for image classiﬁcation.
6.2. FG-LLC
The second image classiﬁcation algorithm we propose is based
on LLC and FG-Codebook, named FG-LLC. We replace the codebook
of LLC by FG-Codebook, and measure the similarity between
features and words via Eq.(26) in step 3 of the LLC algorithm.
The Algorithm 2 shows the detail of the FG-LLC.
Algorithm 2. The proposed FG-LLC algorithm.
Input:
The local features set S, the FG-Codebook
B ¼ W1; W2; …; Wk

.
output:
The coding vector for the image H.
1.
Generate fuzzy feature set S ¼
Q1; Q 2; …; QM
n
o
from S via
Eqs. (12)–(14).
2.
For i¼1 to k do
3.
For j¼1 to M do
4.
Measure similarity χi;j between Wi and Qj with
Eq. (20).
5.
End for
6.
Reﬁne Γi and ﬁnd the k-nearest neighbor words to
quantize the feature by using α-Cut of fuzzy set via Eq. (26).
7.
End for
Y. Li et al. / Pattern Recognition 48 (2015) 3125–3134
3130

8.
LLC approximation coding as [15] did, and forming the
image descriptor H.
9.
Output the image descriptor H for image classiﬁcation.
7. Experimental setup
We experimentally compare the proposed algorithm against
the traditional algorithm using two large-scale datasets: i.e. Fifteen
Scene dataset [7,35] and Caltech-101 [8]. As Li et al. [8] did in the
experimental setup, we evaluate the proposed algorithms with
respect to image classiﬁcation accuracy. Following the common
benchmarking procedures, we repeat the experimental process for
10 runs with randomly selected training and testing images to
obtain reliable results. The average of per-class recognition rates is
recorded for each run. In addition, we report our ﬁnal results by
the mean and SD of the recognition rates. We use the 128-
dimensional SIFT feature in our experiments, and linear SVM as
the classiﬁer. The size of the codebooks (i.e. traditional codebook
and FG-Codebook) is set to 256, 1024, 2048 and 4086. All the
experiments are conducted on a HP ProLiant DL580 G8Server with
16 G memory and a Xeon E7-4820 v2 2 GHz CPU.
8. Experimental results and analysis
8.1. Experiments on Fifteen Scene dataset
The Fifteen Scene dataset consists of a total of 4485 images that
splits into 15 different object categories, i.e. bedroom, suburb,
industrial, kitchen, livingroom, coast, forest, highway, insidecity,
mountain, opencountry, street, tallbuilding, ofﬁce, and store. It can
be
obtained
from
http://www-cvr.ai.uiuc.edu/ponce_grp/data.
There are 200–400 images in each category, and each image is of
about 300  250 pixels in size. Fig. 3 lists several images selected
from the Fifteen Scene dataset. We randomly select 100 images
per category as the training set and the remaining images as the
test set.
In Table 1, we give the image classiﬁcation accuracies with the
different values of η and τ via FG-LLC. The set of values of η are 0.01,
0.05, 0.1, 0.5, and 1. And the set of values of τ are 0.1, 1, 5 and 10.
It is demonstrated that the image classiﬁcation accuracy is the
highest when η¼0.05 and τ¼5. This parameter setting maximizes
the classiﬁcation accuracy and hence is used in the following
experiments. Then, we give the discussion of inference of the
values of η and τ on the image classiﬁcation accuracy.
It can be seen that the classiﬁcation accuracy increases with the
values of η until η reaches 0.05 where τ equals 5. However as η
further increases, the image classiﬁcation accuracy decreases.
Considering special cases where the value of η is set to 0, the
fuzzy visual word becomes a crisp point and the similarity
calculated by Eq. (21) mainly depends on dðW; QÞ. The classiﬁca-
tion accuracy of the proposed algorithm will be the same as that of
traditional LLC when η¼0. As the value of η increases, the fuzzy
part of the fuzzy word plays an increasingly important role in the
similarity measure. When the value of η becomes extremely large,
however, the similarity measure is mainly determined by the fuzzy
part of the visual word, implying that the distance between the
feature and word becomes an insensitive factor for membership
function. In this case, the similarity measure cannot reﬂect the
MITopencountry
MITstreet 
MITtallbuilding
PARoffice
store
Bedroom 
CALsuburb 
industrial  
kitchen 
livingroom
MITcoast
MITforest   
MIThighway
MITinsidecity
MITmountain
Fig. 3. Example images of the Fifteen Scene dataset.
Table 1
Classiﬁcation accuracies with different values of η and τ.
Classiﬁcation accuracy
τ¼0.1
τ¼1
τ¼5
τ¼10
τ¼50
η¼0.01
76.3
76.1
77.7
73.4
62.1
η¼0.05
78.1
79.4
82.0
78.2
63.5
η¼0.1
76.3
76.6
77.5
72.1
59.2
η¼0.5
65.4
66.7
69.2
68.2
54.3
η¼1
62.1
64.3
65.7
63.8
51.9
η¼5
50.2
50.6
52.7
50.2
48.5
η¼10
48.6
49.2
51.3
48.2
41.6
Y. Li et al. / Pattern Recognition 48 (2015) 3125–3134
3131

relationship between the features and the words, leading to
inevitable loss in performance.
Similar to η, τ determines the fuzzy part of fuzzy feature. It can
be seen that the classiﬁcation accuracy increases with the values of
τ until τ reaches 5 where η equals 0.05. However as τ further
increases, the image classiﬁcation accuracy decreases. When τ is
small, the fuzzy part plays a minor role in the representation of
feature. Especially, when τ equals 0, fuzzy feature becomes the
crisp feature. When τ is large, the fuzzy part plays a major role in
the representation of feature. Especially, when τ become extre-
mely large, fuzzy feature contains all the features. However, these
two values of η and τ are not optimal, and the optimal values can
be obtained by optimization algorithms.
The proposed FG-SPM and FG-LLC are compared with the SPM
and LLC respectively. Table 2 shows the comparison results in
terms of the classiﬁcation accuracy. It can be seen that the
classiﬁcation accuracies of the proposed algorithms outperform
the original SPM and LLC algorithms. The performances of the FG-
SPM with different codebook sizes are all better than those of the
SPM. Especially, when the codebook size is 256, an average
percent of 1.67% for performance improvement is achieved. Mean-
while, the performance improvement of the FG-LLC comparing to
the LLC is greater, indicating that the proposed codebook approach
is more advantageous.
It can also be seen that the classiﬁcation accuracy increases as
the codebook size increases. Interestingly, when the codebook size
is 256, the accuracies of the proposed algorithms are similar to
those of the original SPM and LLC algorithms with a codebook size
of 1024. It indicates that the proposed algorithms can reduce the
size of the codebook by over three fourths without decreasing the
classiﬁcation accuracy. When the proposed algorithms are applied
to big data, the computational efﬁciency and classiﬁcation accu-
racy would be improved greatly.
Table 3 shows the image classiﬁcation accuracies in one round
of test on the Fifteen Scene dataset with different sizes of code-
book. It can be seen that the proposed algorithm (FG-LLC) results
in better accuracies on all of the categories than traditional LLC
algorithm, especially on the MITopencountry, bedroom, industrial
and living room (see the values in bold). Among all of the
categories, the accuracies on the CAL suburb are the best. When
the size of the codebook is larger than 1024, the accuracies with
different sizes of codebook are all over 98%, and the accuracies of
the LLC and FG-LLC are similar on this category. Because the
accuracies on a few categories, e.g. MITopencountry, bedroom,
industrial and livingroom, are relatively low using the LLC, the FG-
LLC improves the accuracy more signiﬁcantly on these categories.
The reasons of the accuracy improvement of the proposed
algorithms lie in the following aspects. First, the FG-Codebook
used in the proposed algorithms utilizes the FG to model the visual
words. The element of the FG-Codebook is not a crisp point but a
fuzzy point. The property of the traditional visual word and its
fuzzy degree are both contained in the fuzzy visual word. The
visual word uncertainty can be quantitatively described using the
FG. Second, the local features extracted from the images are also
modeled using the FG to fuzzy points. It is well known that the
local feature would vary greatly with illuminant change, view
point change, scale change, etc. for the images. These changes can
be modeled as fuzzy parts of the local features. Hence, the
proposed fuzzy visual word can strengthen the representation of
the local features. Third, the proposed similarity measure is
plausible. It measures the similarity between the fuzzy visual
word and fuzzy feature based on FG theory, leading to the
computation of the distance from the fuzzy visual word to fuzzy
feature with high plausibility and hence making the coding of the
features using the fuzzy visual words more effective.
8.2. Experiments on Caltech-101
The Caltech-101 dataset was collected in September 2003 by
Li et al. [8]. It contains 8677 images, divided into 101 object
categories, where the number of images in each category varies
Table 2
Image classiﬁcation rate (mean7SD) on Fifteen
Scene dataset.
Algorithm (size of
codebook)
Classiﬁcation
rate
SPM (256)
63.5170.52
FG-SPM (256)
65.1871.24
SPM (1024)
65.1570.71
FG-SPM (1024)
66.3971.20
SPM (4096)
66.4671.13
FG-SPM (4096)
66.8971.07
LLC (256)
77.4370.83
FG-LLC (256)
81.8970.81
LLC (1024)
81.3470.88
FG-LLC (1024)
83.1870.54
LLC (2048)
82.5670.59
FG-LLC (2048)
83.9270.53
LLC (4096)
83.0870.65
FG-LLC (4096)
84.1770.62
Table 3
Image classiﬁcation rate (mean) on Fifteen Scene dataset in one test round.
Category
LLC (256)
FG-LLC (256)
LLC (1024)
FG-LLC (1024)
LLC (2048)
FG-LLC (2048)
LLC (4096)
FG-LLC (4096)
CALsuburb
95.26
96.34
100.00
99.45
98.55
99.23
98.58
99.42
MITcoast
84.85
85.27
87.15
87.61
85.77
86.35
85.91
86.05
MITforest
94.30
94.57
93.97
94.73
96.05
95.61
95.97
95.21
MIThighway
84.70
89.46
83.26
83.46
85.00
87.49
85.13
88.34
MITinsidecity
76.35
79.61
80.22
81.94
80.77
83.60
81.5
82.27
MITmountain
88.23
90.69
91.56
92.79
90.88
92.12
90.94
89.39
MITopencountry
64.19
67.84
70.84
73.32
73.87
77.52
73.89
78.80
MITstreet
82.06
82.96
88.02
88.21
90.63
91.09
91.46
92.57
MITtallbuilding
86.05
87.66
87.89
89.01
91.02
89.61
93.13
94.36
PARofﬁce
92.95
94.39
92.17
95.64
94.78
97.07
95.59
97.38
Bedroom
62.15
65.42
62.07
68.97
68.06
70.29
68.38
71.57
Industrial
57.42
74.09
79.62
81.35
64.93
79.88
65.40
69.17
Kitchen
64.78
67.37
80.00
83.18
75.45
76.17
76.33
79.81
Livingroom
66.12
69.33
64.55
68.99
65.61
69.38
65.76
66.29
Store
73.12
74.52
74.88
75.70
73.49
72.22
74.42
72.74
Y. Li et al. / Pattern Recognition 48 (2015) 3125–3134
3132

from 31 to 800. It is a diverse dataset. Fig. 4 shows several example
images of the Caltech-101 set.
We compare the proposed algorithms based on the FG-
Codebook to the SPM and LLC using the Caltech-101 dataset. In
every experiment, we randomly choose 30 images from each class
as the training data set for the SVM classiﬁer and the rest as
testing data set. The classiﬁcation results are shown in Table 4.
Similarly, the proposed algorithms' performance is better than the
original SMP and LLC algorithms under the same experimental
conditions. It further demonstrates the advantage of the proposed
FG-Codebook and the similarity measure.
It can also be seen that the proposed algorithms perform better
on Fifteen Scene dataset than on Caltech-101. It is mainly due to
different datasets and experimental setups. Caltech-101 dataset
has more categories than Fifteen Scene dataset. The image classi-
ﬁcation performance decreases as the number of image category
increases. For the experimental setup, the number of training
images is larger when training the SVM classiﬁer for Fifteen Scene
dataset than that for Caltech-101 dataset. Generally, the perfor-
mance increases as the number of training images increases.
Above all, for both on the Fifteen Scene dataset and the
Caltech-101 dataset, the proposed algorithms based on the FG-
Codebook achieve better performance. The classiﬁcation accura-
cies of the proposed algorithms are signiﬁcantly improved on a
standard large-scale image database. In particular, using a code-
book size of 256, the performance of the proposed algorithms is
similar to that of traditional algorithms with a codebook size of
1024. This indicates that the proposed algorithms can improve
computational efﬁciency by decreasing the codebook size at about
75% without any loss of classiﬁcation accuracy, which suggests
that the proposed algorithms are more appropriate for big image
data. The reason for the improvement of the performance is that
appropriate uncertainty is added to each visual word and the
similarity measure considers not only the distance between
feature and word, but also the fuzzy factor of feature and visual
word in FG-Codebook.
9. Conclusions
This paper proposes a novel visual codebook model based on
n-D FG theory. It treats the visual words and features as fuzzy
points in n-D FG. In addition, a new similarity measure between
fuzzy features and fuzzy visual words considering the uncertainty
of the words and features is proposed. Moreover, based on the
frameworks of some popular image classiﬁcation algorithms, we
develop two image coding algorithms for image classiﬁcation
where the proposed codebook model and similarity measure are
applied. The experiments are carried out for two large and varied
datasets: Fifteen Scene dataset and Caltech-101. The experiments
compare the proposed algorithms with existing popular algo-
rithms under different codebook sizes, which demonstrate that
the proposed algorithms are more efﬁcient and appropriate for big
image data.
Conﬂict of interest
None declared.
Acknowledgments
We thank Dr. Peng Liu for the help in polishing the English of
this paper. The research was partially supported by National
Natural Science Foundation of China (Nos. 61401286, 61125106,
61372007, and 61375015), the Key Research Program of the
Chinese Academy of Sciences (Grant no. KGZD-EW-T03), Shaanxi
Key Innovation Team of Science and Technology (Grant no.
2012KCT-04), and Natural Science Foundation of SZU (No. 201415).
References
[1] A. Rosenfeld, The diameter of a fuzzy set, Fuzzy Sets Syst. 13 (1984) 241–246.
[2] A. Rosenfeld, S. Haber, The perimeter of a fuzzy set, Pattern Recognit. 18 (1985)
125–130.
Table 4
Image classiﬁcation rate (mean7SD) on Caltech-101.
Algorithms
Classiﬁcation rate
SPM (256)
50.3873.62
FG-SPM (256)
54.5576.85
SPM (1024)
62.2772.73
FG-SPM (1024)
66.2975.28
SPM (4096)
63.8572.85
FG-SPM (4096)
65.4174.82
LLC (256)
71.7471.26
FG-LLC (256)
76.2970.76
LLC (1024)
76.3372.44
FG-LLC (1024)
78.8273.68
LLC (4096)
79.2571.31
FG-LLC (4096)
79.2772.84
accordion   
chair      
cup      
strawberry  
snoopy
faces      
sunflower  
umbrella   
watch  
wild_cat
Fig. 4. Example images of the Caltech-101 dataset.
Y. Li et al. / Pattern Recognition 48 (2015) 3125–3134
3133

[3] D. Tao, L. Liang, L. Jin, Y. Gao, Similar handwritten Chinese character recogni-
tion using discriminative locality alignment manifold learning, ICDAR (2011)
1012–1016.
[4] D.G. Lowe, Object recognition from local scale-invariant features, ICCV, 1999.
[5] D. Tao, L. Jin, Discriminative information preservation for face recognition,
Neurocomputing 91 (2012) 11–20.
[6] D. Tao, L. Jin, Y. Zhao, X. Li, Rank preserving sparse learning for kinect based
scene classiﬁcation, IEEE Trans. Cybern. 43 (5) (2013) 1406–1417.
[7] F. Li, P. Pietro, A Bayesian hierarchical model for learning natural scene
categories, CVPR 2005.
[8] F. Li, F. Rob, P. Pietro, Learning generative visual models from few training
examples: an incremental Bayesian approach tested on 101 object categories,
Comput. Vis. Image Underst. 106 (1) (2007) 59–70.
[9] F. Zhu, L. Shao, Weakly-supervised cross-domain dictionary learning for visual
recognition, Int. J. Comput. Vis. 109 (1-2) (2014) 42–59.
[10] H. Cheng, Z. Liu, L. Yang, X. Chen, Sparse representation and learning in visual
recognition: theory and applications, Signal Process. 93 (6) (2013) 1408–1425.
[11] H.R. Wang, C.F. Yuan, W.M. Hu, C.Y. Sun, Supervised class-speciﬁc dictionary
learning for sparse modeling in action recognition, Pattern Recognit. 45 (11)
(2012) 3902–3911.
[12] H. Lv, X. Lu, Y. Yuan, Data-dependent semi-supervised hyperspectral image
classiﬁcation, ChinaSIP, 2013.
[13] J.J. Buckley, E. Eslami, Fuzzy plane geometry I: Points and lines, Fuzzy Sets Syst.
86 (2) (1997) 179–187.
[14] J.J. Buckley, E. Eslami, Fuzzy plane geometry II: Circles and polygons, Fuzzy
Sets Syst. 87 (1997) 79–85.
[15] J. Wang, J. Yang, K. Yu, F. Lv, T. Huang, Y. Gong, Locality-constrained linear
coding for image classiﬁcation, in: Computer Vision and Pattern Recognition,
CVPR, 2010.
[16] L. Yang, R. Jin, R. Sukthankar, F. Jurie, Unifying discriminative visual codebook
generation with classiﬁer training for object category recognition, CVPR, 2008.
[17] L. Shao, L. Ji, Y. Liu, J. Zhang, Human action segmentation and recognition via
motion and shape analysis, Pattern Recognit. Lett. 33 (4) (2012) 438–445.
[18] L. Shen, S.H. Wang, G. Sun, S.Q. Jiang, Q.M. Huang, Multi-level discriminative
dictionary learning towards hierarchical visual categorization, CVPR, 2013.
[19] J. Yang, K. Yu, Y. Gong, T. Huang, Linear spatial pyramid matching using sparse
coding for image classiﬁcation, CVPR 2009.
[20] J. Gemert, C. Veenman, A. Smeulders, J. Geusebroek, Visual word ambiguity,
IEEE Trans. Pattern Anal. Mach. Intell. 32 (7) (2010) 1271–1283.
[21] L. Shao, D. Wu, X. Li, Learning deep and wide: a spectral method for learning
deep networks, IEEE Trans. Neural Netw. Learn. Syst. 99 (2014) 1–12.
[22] L. Shao, L. Liu, X. Li, Feature learning for image classiﬁcation via multiobjective
genetic programming, IEEE Trans. Neural Netw. Learn. Syst. 25 (7) (2014)
1359–1371.
[23] L. Zhang, X. Zhen, L. Shao, Learning object-to-class kernels for scene classiﬁca-
tion, IEEE Trans. Image Process. 23 (8) (2014) 3241–3253.
[24] L. Shao, X. Zhen, D. Tao, X. Li, Spatio-temporal Laplacian pyramid coding for
action recognition, IEEE Trans. Cybern. 44 (6) (2014) 817–827.
[25] L. Liu, L. Shao, X. Li, Evolutionary Compact Embedding for Large-scale Image
Classiﬁcation, Information Sciences, online 2014. http://dx.doi.org/10.1016/j.
ins.2014.06.030.
[26] M. Aharon, M. Elad, A. Bruckstein, K-SVD: an algorithm for designing over-
complete dictionaries for sparse representation, IEEE Trans. Signal Process. 54
(11) (2006) 4311–4322.
[27] N.R. Pal, S.K. Pal, A review on image segmentation techniques, Pattern
Recognit. 26 (9) (1993) 1277–1294.
[28] P. Sprechmann, G. Sapiro, Dictionary learning and sparse coding for unsuper-
vised clustering, ICASSP, 2010.
[29] Q. Wang, X. Li, Shrink image by feature matrix decomposition, Neurocomput-
ing 140 (2014) 162–171.
[30] Q. Huang, D. Tao, X. Li, L. Jin, G. Wei, Exploiting local coherent patterns for
unsupervised feature ranking, IEEE Trans. Syst. Man Cybern. Part B Cybern. 41
(6) (2011) 1471–1482.
[31] R. Goetschel, W. Voxman, Topological properties of fuzzy numbers, Fuzzy Sets
Syst. 10 (1983) 87–99.
[32] R. Ji, H. Yao, X. Sun, B. Zhong, W. Gao, Towards semantic embedding in visual
vocabulary, CVPR, 2010.
[33] R. Wang, Study of coverage and localization methods based on fuzzy
information processing in sensor networks (Doctoral thesis), 2009.
[34] R. Yan, L. Shao, Y. Liu, Nonlocal hierarchical dictionary learning using wavelets
for image denoising, IEEE Trans. Image Process. 22 (12) (2013) 4689–4698.
[35] S.K. Pal, Ghosh Ashish, Fuzzy geometry in image analysis, Fuzzy Sets Syst. 48
(1) (1992) 23–40.
[36] S. Wang, G. Xu, R. Du, Restricted Bayesian classiﬁcation networks, Sci. China
Inf. Sci. 56 (7) (2013) 078105 (15).
[37] S. Yan, X. Xu, D. Xu, S. Lin, X. Li, Image classiﬁcation with densely sampled
image windows and generalized adaptive multiple kernel learning, IEEE Trans.
Cybern. (2014), http://dx.doi.org/10.1109/TCYB.2014.2326596.
[38] S. Lazebnik, C. Schmid, J. Ponce, Beyond bags of features: spatial pyramid
matching for recognizing natural scene categories, CVPR, 2006.
[39] W. Cao, N. Liu, Q. Kong, H. Feng, Content-based image retrieval using high-
dimensional information geometry, Sci. China Inf. Sci. 57 (7) (2014) 072116 (11).
[40] Y. Li, W. Liu, X. Li, Q. Huang, X. Li, GA-SIFT: a new scale invariant feature
transform for multispectral image using geometric algebra, Inf. Sci. 281 (10)
(2014) 559–572.
[41] Y. Li, W. Liu, Q. Huang, X. Li, Fuzzy bag of words for social image description,
Multimed. Tools Appl., in press.
[42] Y. Dong, D. Tao, X. Li, J. Ma, J. Pu, Texture classiﬁcation and retrieval using
shearlets and linear regression, IEEE Trans. Cybern. (2014), http://dx.doi.org/
10.1109/TCYB.2014.2326059.
[43] Y. Gao, R. Ji, R. Hong, Q. Liu, D. Tao, X. Li, Spectral-spatial constraint
hyperspectral image classiﬁcation, IEEE Trans. Geosci. Remote. Sens. 52 (3)
(2014) 1811–1824.
[44] Z. Zha, M. Wang, Y. Zheng, Y. Yang, R. Hong, T. Chua, Interactive video indexing
with statistical active learning, IEEE Trans. Multimed. 14 (1) (2012) 17–27.
[45] Z. Zha, L. Yang, T. Mei, M. Wang, Z. Wang, T.S. Chua, X. Hua, Visual query
suggestion: towards capturing user intent in internet image search, ACM
Trans. Multimed. Comput. Commun. Appl. 6 (3) (2010) (Article No. 13).
[46] Z. Zha, H. Zhang, M. Wang, H. Luan, T.S. Chua, Detecting Group, Activities with
multi-camera context, IEEE Trans. Circuits Syst. Video Technol. 23 (5) (2013)
856–869.
[47] Z. Liu, W. Xie, P. Wang, Y. Yu, A sequential GM-based PHD ﬁlter for a linear
Gaussian system, Sci. China Inf. Sci. 56 (10) (2013) 102302 (10).
Yanshan Li received M.Sc. degree from Zhejiang University of Technology in 2005. He is currently an associate professor at College of Information Engineering, Shenzhen
University. His research interest covers computer vision, machine learning and image analysis.
Qinghua Huang received BE and ME degrees in automatic control and pattern recognition, both from the University of Science and Technology of China, China, in 1999 and
2002, respectively. He received Ph.D. degree in biomedical engineering in 2007 from the Hong Kong Polytechnic University, Hong Kong. He is now a full professor in the
School of Electronic and Information Engineering, South China University of Technology, China. His research interests include ultrasonic imaging, medical image analysis,
data mining, bioinformatics, intelligent computation and its applications.
Weixin Xie graduated from Xidian University, Xi’an and remained at the university as a faculty member in 1965. From 1981 to 1983, he was a visiting scholar at the
University of Pennsylvania, USA. In 1989, he was invited to the same university as a visiting professor. He is currently working in the School of Information Engineering,
Shenzhen University, China. His research interests include intelligent information processing, fuzzy information processing, image processing, and pattern recognition.
Xuelong Li is a Researcher (full professor) with the Center for OPTical IMagery Analysis and Learning (OPTIMAL), State Key Laboratory of Transient Optics and Photonics,
Xi’an Institute of Optics and Precision Mechanics, Chinese Academy of Sciences, Xi’an 710119, Shaanxi, PR China.
Y. Li et al. / Pattern Recognition 48 (2015) 3125–3134
3134

