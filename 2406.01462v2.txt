The Importance of Online Data:
Understanding Preference Fine-tuning via Coverage
Yuda Song1
Gokul Swamy1
Aarti Singh1
J. Andrew Bagnell1,2
Wen Sun3
1Carnegie Mellon University
2Aurora Innovation
3 Cornell University
{yudas,gswamy,aarti}@cs.cmu.edu, dbagnell@aurora.tech, ws455@cornell.edu
Abstract
Learning from human preference data has emerged as the dominant paradigm for fine-tuning large language models
(LLMs). The two most common families of techniques – online reinforcement learning (RL) such as Proximal Policy
Optimization (PPO) and offline contrastive methods such as Direct Preference Optimization (DPO) – were positioned
as equivalent in prior work due to the fact that both have to start from the same offline preference dataset. To further
expand our theoretical understanding of the similarities and differences between online and offline techniques for
preference fine-tuning, we conduct a rigorous analysis through the lens of dataset coverage, a concept that captures how
the training data covers the test distribution and is widely used in RL. We prove that a global coverage condition is both
necessary and sufficient for offline contrastive methods to converge to the optimal policy, but a weaker partial coverage
condition suffices for online RL methods. This separation provides one explanation of why online RL methods
can perform better than offline methods, especially when the offline preference data is not diverse enough. Finally,
motivated by our preceding theoretical observations, we derive a hybrid preference optimization (HyPO) algorithm
that uses offline data for contrastive-based preference optimization and online unlabeled data for KL regularization.
Theoretically and empirically, we demonstrate that HyPO is more performant than its pure offline counterpart DPO,
while still preserving its computation and memory efficiency.
1
Introduction
Due to the difficulty of manually specifying reward functions for complex tasks (Casper et al., 2023), preference-based
learning has emerged as a critical component in the fine-tuning procedure for large language models (LLMs) (Stiennon
et al., 2020; Ouyang et al., 2022; Touvron et al., 2023; Team et al., 2023). There are two predominant flavors of
preference learning for LLMs: online reinforcement learning (RL) methods such as PPO (Christiano et al., 2017;
Ouyang et al., 2022) and offline contrastive methods like Direct Preference Optimization (DPO) (Rafailov et al., 2023)
and Identity Preference Optimization (IPO) (Azar et al., 2024).
Online RL methods usually follow the two-stage procedure prescribed in Ouyang et al. (2022): one first trains a reward
model (classifier) on a fixed offline preference dataset before using it to provide reward labels for on-policy generations,
which are then fed to a downstream RL algorithm like Proximal Policy Optimization (PPO) (Schulman et al., 2017).
Since the reward model is learned from static offline preference data, to avoid over-optimizing the reward model (Gao
et al., 2023), one typically adds a reverse KL penalty to encourage the model to stay close to some reference policy. We
will refer to this procedure as reinforcement learning from human feedback (RLHF) in this paper. While empirically
performant, RLHF requires repeated querying of the reward model (which is often itself an LLM) as well as sampling
from the current policy. In response to the computational expense and relatively complex nature of this procedure,
purely offline methods like DPO (Rafailov et al., 2023) and IPO (Azar et al., 2024) have been proposed as alternative
methods for preference fine-tuning. These methods do not need to fit separate reward models, instead opting to simply
train the policy directly on the offline preference dataset via a ranking loss.
Offline contrastive methods like DPO are usually derived via applying a reparameterization trick to the closed-form
solution of the minimum relative entropy problem (Ziebart et al., 2008) that RLHF techniques attempt to approximate.
Thus, several authors have described these methods as equivalent (at least in theory) to the standard RLHF procedure
1
arXiv:2406.01462v2  [cs.LG]  16 Jul 2024

(Rafailov et al., 2023; Azar et al., 2024). However, recent (mostly empirical) work has contradicted this perspective:
Tang et al. (2024) find that online methods out-perform offline methods and attribute this fundamentally to on-policy
sampling, Xu et al. (2024b) argues that the online RL methods produce an often desirable subset of the possible DPO
loss minimizers, and Tajwar et al. (2024) provide empirical support for the claim that online and contrastive training
provide orthogonal benefits. However, a rigorous theoretical separation is still lacking in the pre-existing literature,
which motivates our key questions:
What is the statistical separation between the online RLHF method and offline contrastive methods? What
causes this separation and what does it imply?
To answer these questions, we focus on the coverage of the preference dataset, a key concept that is widely used in
RL (Kakade and Langford, 2002; Munos and Szepesvári, 2008; Zhan et al., 2022) for analyzing the impact of offline
or exploratory data distributions. Through the lens of coverage of the offline preference dataset, we make the following
contributions:
• We prove that the global coverage condition (Munos and Szepesvári, 2008), the strongest possible coverage
condition in RL, is necessary for offline contrastive algorithms like DPO to converge to the optimal policy.
In contrast, we identify a weaker local coverage condition that is sufficient for online RLHF algorithms, thus
provably separating the two types of algorithms. The separation is due to the difference in reward modeling and
on/offline regularization – in short, there is no free lunch from bypassing explicit reward learning and online
rollouts. As global coverage might sometimes be violated in practice, our separation result can perhaps explain
why RLHF works better than offline methods (Tajwar et al., 2024; Tang et al., 2024; Yuan et al., 2024).
• Although offline contrastive methods are derived from a reverse-KL objective, we prove that the policies trained
via offline methods can still have infinite reverse-KL in the partial coverage setting. In contrast, we show that
RLHF can always control the reverse KL via directly optimizing reverse KL using online samples. This means
that on realistic problems, RLHF has stronger guarantees for remaining close to the reference policy than offline
contrastive methods.
• We propose Hybrid Preference Optimization (HyPO) to address the deficiencies of offline contrastive methods
while maintaining some of their computational simplicity. HyPO is a hybrid RL algorithm (Xie et al., 2021b;
Song et al., 2022) where offline data is used for the DPO objective while online samples are used to explicitly
control the reverse KL divergence to the reference policy. We empirically demonstrate that HyPO outperforms
DPO, on the TL;DR summarization task (Stiennon et al., 2020) on all metrics including both the GPT4 win-rate
and the reverse KL divergence to the reference policy, and on general chat benchmarks such as AlpacaEval 2.0
(Dubois et al., 2024), trained with the UltraFeedback dataset (Cui et al., 2023). In addition, HyPO also mitigates
the overfitting issues observed in the offline constrastive based methods (Tang et al., 2024).
• We provide an explanation of why RLHF and offline contrastive methods decrease the probability of both
preferred and rejected responses during training. In particular, under our function approximation-based global
coverage condition, we show that such behavior is actually desirable for DPO and RLHF policies to extrapolate
and generalize to optimal actions that do not appear in the training dataset. However, without function approx-
imation, algorithms like DPO can mistakenly increase the likelihood of sub-optimal actions. This establishes
the importance of function approximation for the success of the algorithms such as DPO.
Taken together, our results establish the critical role coverage plays in terms of convergence properties of preference
learning algorithms as well as in the design of new, performant empirical approaches.
2
Related Work
Preference Fine-Tuning (PFT).
As discussed in the introduction of our work, there are two major paradigms for
preference fine-tuning of LLMs. The first one, online RL methods (Ouyang et al., 2022), proposes to first train a reward
model (classifier) to predict human preferences, followed by running an RL method to optimize this learned reward
function. While PPO (Schulman et al., 2017) is the most popular RL algorithm used in the online RLHF framework
by far (Stiennon et al., 2020; Ouyang et al., 2022; Touvron et al., 2023), more recent work by Ahmadian et al. (2024)
shows that simpler online RL algorithms like REINFORCE (Williams, 1992) also work well. The second class of
methods, offline contrastive techniques (Zhao et al., 2023; Rafailov et al., 2023; Azar et al., 2024), avoid explicit reward
2

modeling and directly optimize their objective on the offline preference dataset. Recently there are hybrid methods
that combine offline preference data with online preference labels (Xiong et al., 2023; Guo et al., 2024; Rosset et al.,
2024; Azar et al., 2024) – we leave extending our analysis to this setting to future work. Throughout our paper, we
assume for simplicity of analysis that preferences are generated by an underlying utility function and therefore contain
no intransitivities (Munos et al., 2023; Swamy et al., 2024).
Understanding PFT.
Prior work has studied different parts of the standard RLHF recipe (Gao et al., 2023; Kirk et al.,
2023; Singhal et al., 2023; Eisenstein et al., 2023) and the impact of preference data quality (Sharma et al., 2024). In
our work, we instead take a converge-based perspective on the relationship between online RL methods and offline
contrastive methods. Although derived from the same minimum relative entropy objective (Ziebart et al., 2008) and
perceived as equivalent by some early work (Rafailov et al., 2023; Azar et al., 2024), more recent work has started to
unravel the distinctions between these two classes of methods. Tang et al. (2024) repeatedly observe better performance
from online rather than offline methods and after rigorously validating a variety of hypotheses, conclude that on-policy
sampling is indispensable for ensuring a high quality policy. Tajwar et al. (2024) perform an in-depth study of the
effects of preference data, contrastive losses, and on-policy sampling and conclude that a combination of contrastive
losses and interactive training is most preferable in practice. (Xu et al., 2024b) also observe better performance from
online PPO than from offline DPO and argue this is because the former is able to eliminate a larger set of policies that
are undesirable from the perspective of the later. We supplement these mostly empirical observations with a rigorous
theoretical explanation for the observed behavior through the lens of dataset coverage, as well as designing an algorithm
that addresses the key weaknesses of offline contrastive approaches.
Recent work (Yuan et al., 2024; Pal et al., 2024; Rafailov et al., 2024) has observed an interesting effect of the DPO
procedure: a simultaneous decrease in the likelihood of both preferred and rejected responses. This behavior is
surprising at first glance because one would expect that DPO will increase the likelihood of preferred responses and
decrease the likelihood of rejected responses. We provide a rigorous statistical explanation of this behavior and show
that this behavior is natural when the offline preference data only contains sub-optimal responses but the function
approximation allows DPO to extrapolate and generalize to the correct optimal responses. This highlights the role of
function approximation in the success of offline contrastive based methods.
Coverage.
We analyze online RLHF and offline contrastive-based methods via the concept of coverage. Coverage
measures how well an offline (data) distribution covers the support of the policy of interest, which has been the key
technical tool in offline RL (Munos and Szepesvári, 2008; Xie et al., 2021a; Uehara and Sun, 2021; Zhan et al., 2022),
offline-online RL (Ross and Bagnell, 2012; Xie et al., 2021b; Song et al., 2022; Amortila et al., 2024) and online RL
(Kakade and Langford, 2002; Bagnell et al., 2003; Xie et al., 2023). The data coverage plays an important role in our
analysis since both online RLHF and offline contrastive-based methods rely on an offline preference dataset for learning.
3
Preliminaries
Following a wide range of recent works (Rafailov et al., 2023; Azar et al., 2024), we consider the RLHF problem
in the contextual bandit formulation (Langford and Zhang, 2008). This is a reasonable simplification, as one can
consider the generated sequence of tokens as one single action, due to the fact that the states are the generated tokens,
and the dynamics are deterministic. We denote the context (prompt) space as X, and the action (response) space as
Y. Note that due to the finiteness of the possible tokens, the action space is finite but combinatorially large. We use
ρ ∈∆(X) to denote the distribution of the prompts, and π : X →∆(Y) as policies (LLMs) that map prompts to a
distribution of responses. We also consider the reward function class R : X × Y →R, which assigns a reward to each
context-response pair.
We assume access to a reference policy πref, which is usually referred to as the policy learned using supervised data
when training the LLM, that needs to be further fine-tuned to align with human values. An offline preference dataset
is collected in the format of D = {x, y+, y−} triplets: given context x ∼ρ, the preference policy samples two
responses y1, y2 ∼µ(· | x), where µ is the offline response distribution. Previous works assume either µ to be the same
distribution as πref (Rafailov et al., 2023) or different offline distribution (Azar et al., 2024; Rosset et al., 2024; Gao
et al., 2024). Then, y1 is labelled as y+ (thus y2 as y−) with probability p∗(y1 ≻y2 | x), where p∗is defined by the
3

Bradley-Terry model (Bradley and Terry, 1952):
p∗(y1 ≻y2 | x) =
exp(r∗(x, y1))
exp(r∗(x, y1)) + exp(r∗(x, y2)),
where r∗is the human’s implicit reward function. Note that this rules out intransitive preferences (Swamy et al., 2024;
Munos et al., 2023). Throughout the paper, we will make the following assumption on the reward function:
Assumption 3.1 (Boundedness of the reward). ∥r∗∥∞≤R.
In many previous works, this formulation has been the canonical way to model the preference data in the RLHF
literature (Christiano et al., 2017; Rafailov et al., 2023; Azar et al., 2024). The goal is to learn a policy π to maximize
the objective J(π), where
J(π) = Ex∼ρ

Ey∼π(·|x)[r∗(x, y)] −βKL(π(· | x)||πref(· | x))

,
(1)
i.e., we want to both maximize the human implicit reward, and not deviate too much from the reference policy. We
denote the optimal policy π∗∈argmaxπ∈Π J(π). Here we call KL(π(· | x)||πref(· | x)) reverse KL because π – the
policy to be optimized, appears first. We will call KL(πref(· | x)||π(· | x)) forward KL. By the definition of KL, we have
Definition of reverse KL:
KL(π(· | x)||πref(· | x)) := Ey∼π(·|x)[ln(π(y|x)/πref(y|x))].
(2)
Note that the expectation in reverse KL is under π, indicating that evaluating and optimizing reverse KL requires
drawing online samples from π. In contrast, evaluating forward KL only requires offline samples drawn from πref. As
we will show, this key difference between reverse KL and forward KL plays an important role of separating online
RLHF and offline contrastive methods such as DPO. In this paper, we consider two types of algorithms: online RL-based
algorithms, and offline contrastive-based algorithms.
Online RLHF Algorithms.
We consider algorithms such as Christiano et al. (2017); Ahmadian et al. (2024) as
the online RL based methods. We abstract these algorithms as the following procedure: the algorithm performs the
following two-stage procedure: one first trains a reward model br that minimizes the Bradley-Terry loss 1
br ∈argmax
r∈R
bEx,y+,y−∼D

log

exp(r(x, y+))
exp(r(x, y+)) + exp(r(x, y−))

,
(3)
and perform policy optimization (such as PPO (Schulman et al., 2017)) to optimize the policy with the reward model br:
πrlhf ∈argmax
π
bEx∼D

Ey∼π(·|x)[br(x, y)] −βKL(π(· | x)||πref(· | x))

.
However, this policy optimization step requires extensive online sampling, and possibly training an additional critic
model (e.g., PPO), in addition to the reward model and policy.
Offline Contrastive Algorithms.
To circumvent the above-mentioned computational burden, several purely offline
contrastive-based methods (i.e., without RL) have been proposed. In this paper, we focus on the following two most
representative methods. The first is Direct Preference Optimization (DPO) (Rafailov et al., 2023), where the objective is
πdpo ∈argmaxπ ℓdpo(π) with
ℓdpo(π) = bEx,y+,y−∼D

log


exp

β log

π(y+|x)
πref(y+|x)

exp

β log

π(y+|x)
πref(y+|x)

+ exp

β log

π(y−|x)
πref(y−|x)




.
(4)
Another offline contrastive method we will discuss in our paper is Identity Preference Optimization (Azar et al., 2024),
but we will defer its technical details to the appendix.
1We use bE to denote the empirical expectation over the dataset.
4

4
Offline Contrastive Methods Require a Stronger Coverage Condition than
Online RL Methods
We start by introducing the mathematical formulation of the coverage framework. The strongest coverage condition
is the following global coverage condition (Munos and Szepesvári, 2008): we say any offline distribution µ covers a
policy π if we have maxx,y:ρ(x)>0
π(y|x)
µ(y|x) ≤Cglo. Throughout this section, we will adopt the setting where µ = πref
(Rafailov et al., 2023). Formally, we assume the following condition:
Assumption 4.1 (Global Coverage). For all π, we have
max
x,y:ρ(x)>0
π(y | x)
πref(y | x) ≤Cglo.
For the coverage terms, we always adopt the convention that 0
0 = 0. Note that one sufficient condition for this
assumption is that, for any prompt x, and any token sequence y, we have πref(y | x) ≥1/Cglo.
As has been recognized in the offline RL literature, global coverage is a strong assumption, and efforts have been
made to circumvent this assumption with more relaxed coverage conditions (Uehara and Sun, 2021; Chen and Jiang,
2022; Zhan et al., 2022). In this paper, we will consider the following partial coverage assumption that is weaker than
Assumption 4.1:
Assumption 4.2 (Local KL-ball Coverage). For all εkl < ∞and all policy π such that Ex∼ρ[KL(π(· | x)||πref(· | x))] ≤
εkl, we have
max
x,y:ρ(x)>0
π(y | x)
πref(y | x) ≤Cεkl.
Note that Cεkl depends on εkl. This coverage notion is relatively new in the RL literature and only appeared in previous
analysis of RLHF algorithms (Chang et al., 2024). We call this local coverage condition since it only requires πref to
cover the policies that is within some KL-divergence ball centered at πref. The intuition of this assumption is, for any
algorithm that can control the reverse KL of the output policy, we can leverage the coverage condition to relate the
error under the output policy to its error under the offline distribution, and thus guarantee its performance. Finally, we
note that since the policies with bounded KL is a subset of all policies, for a fixed πref, we always have Cεkl ≤Cglo.
Remark 4.1. Taking a closer look at Assumption 4.2, we can see that this assumption is always true in the sense
that for any policy with εkl < ∞, maxx,y:ρ(x)>0
π(y|x)
πref(y|x) < ∞, i.e., Cεkl < ∞, for any εkl. However, while being
bounded, Cεkl can be large. Indeed a simple calculation can show that maxx,y:ρ(x)>0
π(y|x)
πref(y|x) can be as large as
maxx,y:π(y|x)>0 exp

εkl
π(y|x)

. This can be undesirable because this suggests bounded reverse KL itself is not enough
to guarantee optimality: the error can have an exponential amplification when switching from πref to π. Thus this
motivates Assumption 4.2, which assumes that Cεkl is reasonably small.
In what follows, we will show that the global coverage assumption (Assumption 4.1) is necessary for offline contrastive-
based algorithms such as DPO and IPO, but partial coverage assumption such as Assumption 4.2 is sufficient for online
RL based algorithms. This establishes a separation between the two types of algorithms. We emphasize this theoretical
separation explains why in practice online methods is less prone to problems such as reward hacking and producing
out-of-distribution responses that are due to dataset with insufficient coverage.
4.1
Global Coverage is Necessary for Offline Contrastive Algorithms
Failure of DPO Under Partial Coverage.
Now we show that if the strong coverage Assumption 4.1 breaks, then
DPO can not guarantee any performance with respect to the objective function Eq. (1). The intuition is based on a
rather common observation of the DPO algorithm: the DPO policy πdpo may generate out of distribution responses,
while in contrast, RLHF does not generate responses outside of the support of πref due to online reverse-KL constraint.
For example, (Xu et al., 2024b) provides a construction where πdpo chooses a response where RLHF policy assigns 0
mass, thus proving that RLHF policies are a subset of DPO policies.
5

However, such construction assumes that the reward learning procedure of DPO makes arbitrarily large errors. Also, pre-
vious constructions assume deterministic preference, which is only true if the underlying reward function is unbounded.
This violates the natural assumption of Assumption 3.1. In the following, we relax these constraints and thus show
that DPO fails to guarantee any performance in a rather strong sense. Concretely, DPO constructs the following implicit
reward class with the policy class Π: Rdpo =
n
β log

π(y|x)
πref(y|x)Z(x)

| π ∈Π
o
, where Z(x) is a partition function that
maps context to a real number and is independent of y. Plugging this formulation into the BT loss (Eq. (3)) recovers
exactly the DPO loss (Eq. (4)) as the partition functions are canceled. Now we can characterize the returned policy
by DPO as exactly whose corresponding reward function is accurate in distribution:
Assumption 4.3 (In Distribution Reward Learning). We assume the DPO policy πdpo satisfies that:
Ex,y∼ρ◦πref
"
β log

πdpo(y | x)
πref(y | x)Z(x)

−r∗(x, y)
2#
≤εdpo.
Note that this is a rather strong assumption for BT loss – by Lemma A.2, at best one can only hope: for any learned
reward function br, for each context x, there exists a constant c(x) such that
Ex,y∼ρ◦πref
h
(br(x, y) −r∗(x, y) −c(x))2i
≤ε,
(5)
i.e., the reward model predicts the human reward up to a gap that is independent of y. This is due to the fact that BT loss
only requires the reward function to capture the relative difference, or in other word, any constant shift (with respect
to context) in the reward will be canceled in the BT loss. However, for the rest of the section, we will make the stronger
learning assumption that the gap c(x) = 0 (such as in the case of Assumption 4.3). Previous counterexamples analysis
violates this assumption, but we will show that even under this strong assumption, DPO still can not guarantee any
performance.
Proposition 4.1. Denote πref as any reference policy such that Assumption 4.1 breaks. Let Πdpo be the set of DPO
returned policies such that Assumption 4.3 holds. Then there exists policy π ∈Πdpo such that J(π) = −∞.
Proof sketch. Without loss of generality, we consider a promptless setting, and assume that the response space is
Y = {y1, y2, y3}. Again without loss of generality, we assume πref only covers y1 and y2, and thus Assumption 4.1
breaks. We assume partition function Z = 1 for all π but we will be rigorous in the formal proof. Then consider the
following policy π such that
β log
 π(y1)
πref(y1)

= r∗(y1) −√εdpo,
and
β log
 π(y2)
πref(y2)

= r∗(y2) −√εdpo,
One can check π satisfies Assumption 4.3. Now consider the optimal policy π∗(yi) = πref(yi) exp

1
β r∗(yi)

, for
i ∈{1, 2}, and π∗(y3) = 0. Since π∗(y1) + π∗(y2) = 1, combining everything we get π(y3) > 0, which implies
KL(π||πref) is unbounded, thus we complete the proof.
One can first relate the above construction to the parital coverage assumption Assumption 4.2: since the policy π
considered in the proof has unbounded reverse KL with respect to πref, thus it is not in the KL-ball of εkl around πref,
which implies that Assumption 4.2 is not sufficient for DPO. Next we show that global coverage is necessary for the
IPO algorithm.
Failure of IPO Under Partial Coverage.
To show that the global coverage is necessary for IPO, we can even assume
a stronger in-distribution learning guarantee, that is, the returned policy achieves the smallest error on its population
loss in distribution.
Proposition 4.2 (Informal). Denote πref as any reference policy such that Assumption 4.1 breaks. Let Πipo be the set
of IPO returned policies such that it is the minimizer of in-distribution error on its population loss. Then there exists
policy π ∈Πipo such that J(π) = −∞.
6

We defer the detailed setup and formal version to Appendix C, but the construction for the above proofs share the same
intuition: the reverse KL term in the objective function can be unbounded. For offline contrastive-based algorithms, the
KL regularization is only enforced under the data distribution, and thus the algorithm can not guarantee bounded reverse
KL if the reference policy does not cover the response space well. Although we only showed counterexamples for DPO
and IPO, we conjecture that the same intuition holds for other offline contrastive-based algorithms. One natural question
at this point would be: how about the forward KL? Not surprisingly, the forward KL for DPO (but we conjecture for
other offline constructive-based methods as well) is vacuously large, and we formalize this result in Appendix B.2.
Remark 4.2. The folklore that DPO is equivalent to RLHF is often based on some assumption that is much stronger than
Assumption 4.3: it requires that the learned policy has a point-wise accuracy guarantee β ln(πdpo(y|x)/πref(y|x)) =
r∗(x, y) for all x, y. Such a point-wise guarantee is unrealistic in reality and does not hold in general in the supervised
learning sense. The in-distribution style guarantee in Assumption 4.3 is the best one could hope for from a supervised
learning algorithm.
4.2
Global Coverage is Sufficient for Offline Contrastive Algorithms
After showing that global coverage is necessary for DPO to guarantee any performance, we now show that it is sufficient
for the performance guarantee.
Theorem 4.1. Let πref be any reference policy such that Assumption 4.1 holds. For any policy πdpo such that the event
in Assumption 4.3 holds, we have that
J(π∗) −J(πdpo) = O(Cglo√εdpo).
Proof. By Lemma A.1, we have
J(π∗) −J(πdpo) ≤Ex∼ρ

Ey1∼π∗(·|x),y2∼πdpo(·|x)

r∗(x, y1) −d
rdpo(x, y1) −r∗(x, y2) + d
rdpo(x, y2)

≤
r
Ex∼ρ
h
Ey1∼π∗(·|x),y2∼πdpo(·|x)
h
(r∗(x, y1) −d
rdpo(x, y1) −r∗(x, y2) + d
rdpo(x, y2))2ii
≤
r
C2
gloEx∼ρ
h
Ey1,y2∼πref(·|x)
h
(r∗(x, y1) −d
rdpo(x, y1) −r∗(x, y2) + d
rdpo(x, y2))2ii
,
and we can complete the proof by plugging in the error guarantee from Assumption 4.3.
Note that as the proof suggests, the result holds with the more general reward learning guarantee as in Lemma A.2 –
one only needs to be accurate in predicting the relative rewards between response pairs.
4.3
Online RL Method Under Partial Coverage
Finally, we contrast the previous negative results in Section 4.1 for offline contrastive-based algorithms to a positive
result for online RL-based algorithms, under the partial coverage setting. We will show that in general global coverage
is not necessary for RLHF, i.e., it can guarantee performance under partial coverage. In fact, one might still be able to
show an impossibility result for RLHF under partial coverage, by reusing the same counterexample as in the previous
section (c.r., Proposition 4.1). Concretely, as long as the learned reward br(y3) →∞, πrlhf(y3) will be 1 and thus the
reverse KL will be unbounded. However, this is a rather unrealistic scenario, as the construction requires a reward
model (e.g., a neural network) to output an unbounded value. Thus this motivates the following assumption:
Assumption 4.4. For all learned reward model br from the reward model class, we have that ∥br∥∞≤R′.
At this point, one might ask why a similar assumption is missing for the offline contrastive-based analysis, since in
Remark 4.2 we argued that a point-wise learning guarantee is unrealistic but Assumption 4.4 is indeed also a point-wise
boundedness assumption. The reason lies in the different construction of the model class br for those algorithms: for
DPO and IPO, the reward model is constructed as d
rdpo = β log

π
πref·Z

, and there is no natural function class for π
such that point-wise assumptions such as the one in Remark 4.2 or Assumption 4.4 holds. In contrast, post-processing
such as clipping, offline normalization and on-the-fly normalization of rewards is standard in practice, which means the
7

Algorithm 1 Hybrid Preference Optimization (HyPO)
require Pretrained LLM πθ0, reference policy πref, offline data D, learning rate α, KL coefficient λ.
1: for t = 1, . . . , T do
2:
Sample a minibatch of offline data Doff := {x, y+, y−} ∼D.
3:
Compute DPO loss ℓdpo := P
x,y+,y−∈Doff log

σ

β log

πθt−1(y+|x)
πref(y+|x)

−β log

πθt−1(y−|x)
πref(y−|x)

.
4:
Sample (unlabeled) online data Don := {x, y} where x ∼D, y ∼πθt−1(x).
5:
Compute ℓkl := P
x,y∈Don log(πθt−1(y|x)) · sg

log
 (πθt−1(y|x))
(πref(y|x))

.
6:
Update θt = θt−1 + α · ∇θt−1(ℓdpo −λℓkl).
return πT .
policy will always witness bounded rewards (Chang et al., 2023, 2024; Gao et al., 2024; Ahmadian et al., 2024) during
online RL training (e.g., PPO). As we will show in the following, the difference in the reward function (which is tied to
the offline vs. online nature of the algorithms) can explain the different coverage requirement of the algorithms. Note
that we use the same in-distribution reward learning assumption for both types of methods.
To relate to Assumption 4.2, we first show that the reverse KL divergence of the RLHF policy is always bounded under
Assumption 4.4.
Lemma 4.1. Suppose that Assumption 4.4 holds. Then for any RLHF policy πrlhf, we have that
KL(πrlhf||πref) := Ex∼ρ

Ey∼πrlhf(·|x)

log
πrlhf(y | x)
πref(y | x)

≤2R′
β .
Then we can show that the RLHF algorithm can guarantee performance under partial coverage:
Theorem 4.2. Suppose that Assumption 4.4 holds. Then for any reference policy πref for which Assumption 4.2 holds
with εkl = 2R′
β , and any RLHF policy πrlhf with br such that (c.r. Assumption 4.3)
Ex,y∼ρ◦πref
h
(r∗(x, y) −br(x, y))2i
≤εreward,
we have
J(π∗) −J(πrlhf) ≤O(Cεkl
√εreward).
Conditioned on Lemma 4.1, the proof of this theorem is similar to that of Theorem 4.1 so we defer it to Appendix C. Sim-
ilar to Theorem 4.1, we note that Theorem 4.2 holds under a weaker reward learning guarantee as in Lemma A.2. We also
remark that as long as εkl is finite, Cεkl is finite, so the bound is never vacuous. Since Cεkl ≤Cglo for all εkl, it indicates
the regret bound of RLHF is never worse and can be much better than the regret bound of DPO. Combining Theorem 4.1
and Theorem 4.2, we complete the separation result between offline contrastive methods and online RL methods.
A natural question at this point could be: can we further relax the local KL-ball coverage condition in Assumption
4.2 to a single-policy coverage condition, i.e., just assuming maxx,y π∗(y|x)/πref(y|x) ≤C? Prior work Zhan et al.
(2023) shows that with explicit pessimism, it is possible. However, using pessimism makes the algorithm from Zhan
et al. (2023) not computationally tractable and hard to scale to LLM experiments. Our conjecture is that for the RLHF
policy πrlhf, it is not possible to achieve meaningful regret under the single policy coverage condition, due to KL not
being strong enough to induce pessimism (i.e., bounded KL between π and πref can still imply exponentially large
density ratio π/πref). Developing a lower bound for πrlhf under single policy coverage in this case can be an interesting
future work.
5
Hybrid Preference Optimization: Regularizing Offline Learning with Un-
labeled Online Samples
In this section, we will provide a practical algorithm that bridges the gap between the offline contrastive-based algorithms
and the online RL-based algorithms. As we see in the previous sections, the difference between the two types of
8

Table 1: Results on TL;DR dataset. Winrate is evaluated by GPT4 and RM score is from the trained reward model.
Experiments are repeated for 3 random seeds. Mean and standard deviation are reported.
Model size
Algorithm
Winrate (↑)
RM score (↑)
KL(π||πref)(↓)
1.4B
DPO
42.17% (2.5%)
0.16 (0.05)
44.90 (1.29)
HyPO
46.44% (2.39%)
0.37 (0.05)
27.07 (2.34)
2.8B
DPO
44.39% (0.4%)
2.43 (0.10)
68.95 (3.08)
HyPO
50.50% (1.89%)
2.51 (0.13)
48.98 (4.23)
Table 2: Results on general chat benchmarks. We evaluate the base model (Meta-Llama-3-8B-Instruct), DPO-fine-tuned
model, and HyPO-fine-tuned model.
Model
MT-Bench
AlpacaEval 2.0
1st Turn
2nd Turn
Average
LC Win Rate
Win Rate
Meta-Llama-3-8B-Instruct (Meta, 2024)
8.31
7.89
8.10
26.0
25.3
DPO-Llama-3
8.08
7.41
7.75
28.4
30.9
HyPO-Llama-3
8.43
7.75
8.09
30.7
32.2
algorithms is their reward model parametrization, and whether to perform online rollouts. In the following, we will
show that these two properties are in fact tightly intervened with each other.
Here we will focus on the DPO algorithm. One way to fix the issue of the unbounded reverse KL of DPO (which is
caused by the unbounded reward model class) is to consider the following ideal procedure: at the beginning of the
algorithm, we first go through the policy class Π, and then we filter out all the policies such that KL(π||πref) ≥2R′
β ,
where R′ is the boundedness of the reward function class for RLHF. Now applying the same analysis of Theorem 4.2,
we can show that this revised DPO algorithm can guarantee performance under the partial coverage assumption, because
now the Lemma 4.1, a sufficient condition for Theorem 4.2, is explicitly enforced by the constraints. We defer the
detailed statement and analysis to Appendix D.1.
However, such a filtering procedure is not possible in practice, but we can instead consider the following constrained
optimization problem: we call the definition of DPO loss in Eq. (4), we want to solve
max
π
ℓdpo(π)
s.t.
KL(π||πref) ≤2R′
β ,
(6)
using the KKT conditions, we can show that the following Lagrangian form is equivalent to Eq. (6):
max
π
ℓdpo(π) −λKL(π||πref),
(7)
where λ is the Lagrange multiplier. However, in reality, since we do not know the exact value of R′, we can consider
setting λ to be a hyperparameter. We present the pseudocode in Algorithm 1. Note that due to the reverse KL term, the
Hybrid Preference Optimization (HyPO) algorithm optimizes Eq. (7) via both offline and online samples where the
offline samples are used for constructing and optimizing ℓdpo (here σ denotes the sigmoid function), and the online
samples y ∼π(· | x) are for KL (i.e., ℓkl). Note that regularizing with reverse KL via online samples is widely used in
online RLHF (e.g., PPO (Stiennon et al., 2020), APA (Zhu et al., 2023), REBEL (Gao et al., 2024)). Here sg refers
to the stop gradient operation, which is a common practice in optimizing reverse KL in the LLM fine-tuning setting
(Ouyang et al., 2022; von Werra et al., 2020). We also remark that a few recent and concurrent works (Xu et al., 2024a;
Fisch et al., 2024; Liu et al., 2024) propose to regulate DPO with an additional forward KL term, where the samples are
directly from the reference policy, while HyPO uses reverse KL which requires sampling from the learned policy online.
Finally, previous iterative RLHF methods (Xiong et al., 2024) can be interpreted as hybrid methods as well, but they
require labeling online samples from an additional reward model while HyPO only requires unlabeled online samples.
9

Table 3: Results on Open LLM leaderboard. We evaluate the base model (Meta-Llama-3-8B-Instruct), DPO-fine-tuned
model, and HyPO-fine-tuned model.
Model
MMLU
(5-shot)
GSM8K
(5-shot)
Arc
(25-shot)
TruthfulQA
(0-shot)
HellaSwag
(10-shot)
Average
Meta-Llama-3-8B-Instruct
(Meta, 2024)
65.68
74.91
62.12
43.88
78.76
65.07
DPO-Llama-3
65.82
73.62
63.14
45.02
79.1
65.34
HyPO-Llama-3
65.74
73.84
62.71
45.55
79.74
65.51
5.1
Experimental Results
5.1.1
Summarization
Our first experiment is on the TL;DR dataset (Stiennon et al., 2020). Our experiment setup mostly follows (Gao et al.,
2024): we use a maximum context length of 512 and a maximum generation length of 53. We use Pythia 1.4B and
Pythia 2.8B (Biderman et al., 2023) as the pre-trained model. For the supervised fine-tuning (SFT) model, we train it
over 1 epoch of the dataset with human reference responses as labels. We train the reward model on top of the SFT over
1 epoch of preference data. Both HyPO and DPO are trained over 1 epoch of preference data with Low-rank Adaptation
(LoRA) (Hu et al., 2021). We defer more experiment details in Appendix D.
We summarize the results in Table 1: HyPO outperforms DPO in terms of GPT4 win-rage and reverse KL. Particularly,
the significant reduction in reverse KL implies the impact of including a reverse KL term explicitly into the DPO objec-
tive. While comparing with PPO (e.g., Table 1 in Gao et al. (2024)), HyPO’s performance is still lower in winrate, HyPO
does preserve the key advantages of DPO over PPO: we avoid training additional reward model and a value network.
5.1.2
General Chat
In the general chat setting, the model is required to produce a response y given user instruction x. We again follow the
experiment setup in Gao et al. (2024), where we finetune the Meta-Llama-3-8B-Instruct (Meta, 2024) model on the
ultrafeedback dataset (Cui et al., 2023). Due to the computation constrain, we follow the setup in Gao et al. (2024)
where we only train the last 4 layers of the network for both HyPO and DPO.
For evaluation, we use the common metrics including AlpacaEval 2.0 (Dubois et al., 2024), MT-bench (Zheng et al.,
2024) and Open LLM leaderboard tasks: MMLU (Hendrycks et al., 2020), GSM8K (Cobbe et al., 2021), Arc (Clark
et al., 2018), TruthfulQA (Lin et al., 2021) and HellaSwag (Zellers et al., 2019). We provide the results for AlpacaEval
and MT-bench in Table 2, and the the results of the remaining tasks can be found in Table 3.
5.1.3
HyPO Mitigates Overfitting in Contrastive Methods
Since the offline contrastive based methods only work with a static offline dataset, the overfitting issue has been
observed (Tang et al., 2024). In our last experiment, we show that HyPO can effectively address the overfitting issue by
leveraging the unlabeled online data. We follow the setup of the summarization task with Pythia-2.8B base model. We
train DPO and HyPO for 5 epochs respectively, and evaluate on the first 300 data in the validation dataset. We plot the
validation KL in Figure 1: we observe that HyPO is better at preventing the deviation from the reference policy caused
by overfitting from training on excessive epochs, even though the methods theoretically both have KL regularization to
the reference policy.
6
Function Approximation Coverage: Can Fine-tuned Policies Extrapolate?
Our final result is a theoretical explanation of the extrapolation behavior of preference fine-tuning algorithms under
the global coverage assumption in the function approximation (FA) setting. The extrapolation behavior refers to the
phenomenon of RLHF algorithms (e.g., DPO) can improve SFT models despite the fact that during training the policies
10

0
2500
5000
7500 10000 12500 15000 17500
Timestep
0
25
50
75
100
125
150
175
Mean Score
Mean Validation KL
HyPO
DPO
Figure 1: Mean validation reverse KL to the reference policy when DPO and HyPO are trained for 5 epoch on the
TL;DR dataset. We repeat the experiment for 3 random seeds and plot the median and the shaded areas denote the min
and max over the 3 repetitions.
assign decreasing likelihood to both the preferred and rejected responses (i.e., they must increase the likelihood of
responses outside of the training data) (Pal et al., 2024).
A previous attempt (Rafailov et al., 2024) to explain this behavior is based on the assumption that the responses from
the reference policy have the same distribution as the preferred responses from the dataset, i.e., y+ ∼µ
d= y ∼πref.
However, as mentioned in Section 3, more realistically, one should assume that y ∼µ
d= y ∼πref since it is more
natural to use the reference policy to generate pairs of responses to collect labels; or even more generally by considering
supp(D) ⊂supp(πref). The latter is common in practice, for example, the dataset is often precollected, or the reference
policy might have a small mass on some responses, so with a high probability they are not sampled during the data
collection process.
In the following example, we illustrate this behavior using linear function approximation. We use an offline dataset that
does not contain the optimal action. We show that thanks to the linear function approximation and the dataset coverage,
DPO has hope to extrapolate correctly, i.e., it can increase the model’s likelihood of the optimal action while decreasing
the likelihood of both the preferred and rejected actions from the offline data.
Example 6.1. Consider a promptless setting, where the response space is Y = {y1, y2, y3}. Consider the linear
function approximation setting with feature map ϕ, where ϕ(y1) = [1, 0], ϕ(y2) = [1/2, 1/2], ϕ(y3) = [0, 1]. Suppose
all policies are parametrized as softmax linear policies, i.e., π(y) ∝exp(w⊤
π ϕ(y)). Let wref = [1, 1], then we have
πref(yi) = 1/3, ∀i ∈{1, 2, 3}.
Consider the ground truth reward function r∗(y) = [10, 1]⊤ϕ(y), and suppose supp(µ) = {y1, y2}, i.e., the data only
covers y1 and y2. And as always, the preference is based on the ground truth reward function under the Bradley-Terry
model.
We can first check that the data distribution indeed has global coverage in the linear function approximation case
(Xiong et al., 2022), i.e., let Σµ = Ey∼µϕ(y)ϕ(y)⊤, then for all π,
Ey∼π∥ϕ(y)∥2
Σ−1
µ
≤Cπ.
If we parameterize br(y) = bw⊤ϕ(y) (or in case of DPO, we can still check and see that d
rdpo(y) = d
wdpo
⊤ϕ(y) because
of the softmax linear parametrization of the policies), for either direct reward learning or DPO, we can have the learned
reward function br(y) = [10, 1]⊤ϕ(y) + c, where c is the constant reward shift (c.r. Eq. (5)). Then a simple calculation
(by π(y) ∝πref(y) exp(br(y)/β)) shows that, as long as c is small enough, the policies will decrease the likelihood of
y1 and y2 and increase the likelihood of y3.
◁
The above example shows when the training dataset together with the function approximation allow the learned function
to generalize (e.g., learn a function that can predict well on test examples beyond the training data — a property
supervised learning can have), algorithms like DPO can extrapolate correctly, i.e., they can push up the likelihood of the
optimal responses outside of the training data while pushing down the likelihood of all the responses in the training data.
11

0
200
400
600
800
1000
Iteration
−10
−8
−6
−4
−2
0
Log Probability
Online RLHF with Linear FA
Preferred Probability
Best Probability
0
1000
2000
3000
4000
5000
Iteration
−7
−6
−5
−4
−3
−2
Log Probability
DPO with Linear FA
0
1000
2000
3000
4000
5000
Iteration
0.00200
0.00225
0.00250
0.00275
0.00300
0.00325
0.00350
0.00375
0.00400
Probability
DPO without FA
Out of Distribution Probability
Figure 2: Left and middle: Extrapolation behavior of Online RL method and DPO under linear function approximation
(FA). We plot the mean log probability of the preferred responses and the log probability of the best response, which is
unseen in the training data. We see that both algorithms correctly assigns increasing probability to the best response.
Right: Extrapolation behavior of DPO without function approximation. We plot the average probability of out-of-
distribution responses along the training and DPO assigns increasing probability to out-of-distribution responses.
6.1
Synthetic Experiment for Extrapolation
To validate our theory result, in this section we perform a synthetic experiment on global coverage with linear function
approximation. As shown in Figure 2, this extrapolation behavior is observed in both online RL method and DPO.
6.1.1
Extrapolation with Function Approximation
We first describe our experiment setup. We consider linear function approximation setting where we have 100 responses
(|Y| = 100). We consider a 16-dimensional feature vector ϕ : Y →R16, and we generate ϕ(y) by simply sampling
99 random 16-dimensional vectors where the ℓ1 norm of each vector is 1. We add one final ϕ(y) = [1, 0, 0, . . . ]. We
construct the implicit human reward r∗(y) = w∗⊤ϕ(y), where w∗= [5, ...], and the rest of the entries are sampled from
Unif(-2,2). We parametrize the policies as softmax linear policies, i.e., we parametrize each policy π with wπ ∈R16
such that π(y) =
wπ⊤ϕ(y)
P
y∈Y wπ⊤ϕ(y). One can check in this formulation the implicit reward in DPO (d
rdpo) is linear in ϕ.
We generate 10000 preference pairs, according to the BT model under r∗, for the first 50 responses. We checked that
the first responses indeed span R16. Thus the offline data has global coverage in linear function approximation setting.
For on-policy RL methods, we first train a reward model. Then we simply perform gradient descent on the KL-
regularized bandit loss (we assume πref is uniform). For DPO, we simply perform SGD on the offline preference dataset.
We track two qualities over the training: the mean log probability of a random subset of preferred responses, and the
log probability of best response ϕ(y) = [1, 0, 0, . . . ]. We plot the results in Figure 2 (Left and middle). We observe
that both methods have the extrapolation behavior – the probability of preferred responses decays but the probability
of the optimal response goes up.
6.1.2
Extrapolation without Function Approximation
To further demonstrate the importance of function approximation and generalization, we conduct the same experiments
but without the linear function approximation (i.e., treat each action as independent just like one would do in the classic
multi-armed bandit setting). In this case, trying one action does not give us information about the other action. We
see that in this case, DPO can erroneously assign a higher probability to unseen suboptimal responses instead of the
unseen optimal response, indicating DPO can fail to extrapolate and generalize correctly. Our investigation identifies
that function approximation and generalization play an important role in the success of RLHF and DPO algorithms.
Now we describe the setting where function approximation fails, and this reduces to a Multi-arm bandit setting. We
set |Y| = 500, and the offline data only covers the first half of the responses. The r∗(y) is set by sampling from
Unif(-10,10), and we generate 10000 offline samples by uniformly sampling pairs of responses from the first half of the
response space, and then labeling them with BT model under r∗. We train DPO with 5000 iterations, and plot the mean
probability of the responses outside of the data support in Figure 2 (Right): we observe that the mean probability of the
out-of-distribution responses is increasing, however, this could be an undesirable behavior because the reward of the
out-of-distribution responses could be arbitrarily bad.
12

7
Discussion
There are a few limitations of our work:
• Our theoretical analysis only considers the statistical perspective of each algorithm, but we believe our result is
complementary to the other work that considers the optimization perspectives (Tajwar et al., 2024).
• The experiment result shows that HyPO’s performance is still below the one of online RLHF: this might suggest
that our theory does not fully explain the benefit of all the components of online RLHF. For example, one
hypothesis is that the learn reward function may have better generalization ability.
• It is not clear that the KL-ball coverage is necessary for online RL-based methods. However, as we discussed,
since a bounded reverse KL might still induce exponential error amplification, we conjecture that at least the
single policy coverage (Zhan et al., 2022) is not sufficient for online RLHF-based methods that use reverse KL.
• We only use the online sample for regularizing reverse KL. One might also leverage the online sample to query
new preference information or perform length control.
We believe these limitations lead to several interesting further directions. Finally, our method may not explicitly address
the potential hallucinations or toxic behavior of LLMs, which is a common shortcoming of general-purpose fine-tuning
algorithms.
Acknowledgments
YS thanks Audrey Huang for the valuable discussions. AS and YS acknowledge and thank the support of ONR grant
N000142212363 and NSF AI Institute for Societal Decision Making AI-SDM grant IIS2229881. WS acknowledges the
support of NSF grant IIS-2154711 and NSF CAREER grant 2339395.
13

References
Arash Ahmadian, Chris Cremer, Matthias Gallé, Marzieh Fadaee, Julia Kreutzer, Ahmet Üstün, and Sara Hooker.
Back to basics: Revisiting reinforce style optimization for learning from human feedback in llms. arXiv preprint
arXiv:2402.14740, 2024.
Philip Amortila, Dylan J Foster, Nan Jiang, Ayush Sekhari, and Tengyang Xie. Harnessing density ratios for online
reinforcement learning. In The Twelfth International Conference on Learning Representations, 2024. URL https:
//openreview.net/forum?id=THJEa8adBn.
Mohammad Gheshlaghi Azar, Zhaohan Daniel Guo, Bilal Piot, Remi Munos, Mark Rowland, Michal Valko, and Daniele
Calandriello. A general theoretical paradigm to understand learning from human preferences. In International
Conference on Artificial Intelligence and Statistics, pages 4447–4455. PMLR, 2024.
James Bagnell, Sham M Kakade, Jeff Schneider, and Andrew Ng. Policy search by dynamic programming. Advances
in neural information processing systems, 16, 2003.
Stella Biderman, Hailey Schoelkopf, Quentin Gregory Anthony, Herbie Bradley, Kyle O’Brien, Eric Hallahan, Moham-
mad Aflah Khan, Shivanshu Purohit, USVSN Sai Prashanth, Edward Raff, et al. Pythia: A suite for analyzing large
language models across training and scaling. In International Conference on Machine Learning, pages 2397–2430.
PMLR, 2023.
Ralph Allan Bradley and Milton E Terry. Rank analysis of incomplete block designs: I. the method of paired
comparisons. Biometrika, 39(3/4):324–345, 1952.
Stephen Casper, Xander Davies, Claudia Shi, Thomas Krendl Gilbert, Jérémy Scheurer, Javier Rando, Rachel Freedman,
Tomasz Korbak, David Lindner, Pedro Freire, et al. Open problems and fundamental limitations of reinforcement
learning from human feedback. arXiv preprint arXiv:2307.15217, 2023.
Jonathan D Chang, Kiante Brantley, Rajkumar Ramamurthy, Dipendra Misra, and Wen Sun. Learning to generate better
than your llm. arXiv preprint arXiv:2306.11816, 2023.
Jonathan D Chang, Wenhao Shan, Owen Oertell, Kianté Brantley, Dipendra Misra, Jason D Lee, and Wen Sun. Dataset
reset policy optimization for rlhf. arXiv preprint arXiv:2404.08495, 2024.
Jinglin Chen and Nan Jiang. Offline reinforcement learning under value and density-ratio realizability: the power of
gaps. In Uncertainty in Artificial Intelligence, pages 378–388. PMLR, 2022.
Paul F Christiano, Jan Leike, Tom Brown, Miljan Martic, Shane Legg, and Dario Amodei. Deep reinforcement learning
from human preferences. Advances in neural information processing systems, 30, 2017.
Peter Clark, Isaac Cowhey, Oren Etzioni, Tushar Khot, Ashish Sabharwal, Carissa Schoenick, and Oyvind Tafjord.
Think you have solved question answering? try arc, the ai2 reasoning challenge. arXiv preprint arXiv:1803.05457,
2018.
Karl Cobbe, Vineet Kosaraju, Mohammad Bavarian, Mark Chen, Heewoo Jun, Lukasz Kaiser, Matthias Plappert, Jerry
Tworek, Jacob Hilton, Reiichiro Nakano, et al. Training verifiers to solve math word problems. arXiv preprint
arXiv:2110.14168, 2021.
Ganqu Cui, Lifan Yuan, Ning Ding, Guanming Yao, Wei Zhu, Yuan Ni, Guotong Xie, Zhiyuan Liu, and Maosong Sun.
Ultrafeedback: Boosting language models with high-quality feedback. arXiv preprint arXiv:2310.01377, 2023.
Yann Dubois, Balázs Galambosi, Percy Liang, and Tatsunori B Hashimoto. Length-controlled alpacaeval: A simple
way to debias automatic evaluators. arXiv preprint arXiv:2404.04475, 2024.
Jacob Eisenstein, Chirag Nagpal, Alekh Agarwal, Ahmad Beirami, Alex D’Amour, DJ Dvijotham, Adam Fisch,
Katherine Heller, Stephen Pfohl, Deepak Ramachandran, et al. Helping or herding? reward model ensembles mitigate
but do not eliminate reward hacking. arXiv preprint arXiv:2312.09244, 2023.
Adam Fisch, Jacob Eisenstein, Vicky Zayats, Alekh Agarwal, Ahmad Beirami, Chirag Nagpal, Pete Shaw, and Jonathan
Berant. Robust preference optimization through reward model distillation. arXiv preprint arXiv:2405.19316, 2024.
14

Leo Gao, John Schulman, and Jacob Hilton. Scaling laws for reward model overoptimization. In International
Conference on Machine Learning, pages 10835–10866. PMLR, 2023.
Zhaolin Gao, Jonathan D Chang, Wenhao Zhan, Owen Oertell, Gokul Swamy, Kianté Brantley, Thorsten Joachims,
J Andrew Bagnell, Jason D Lee, and Wen Sun. Rebel: Reinforcement learning via regressing relative rewards. arXiv
preprint arXiv:2404.16767, 2024.
Shangmin Guo, Biao Zhang, Tianlin Liu, Tianqi Liu, Misha Khalman, Felipe Llinares, Alexandre Rame, Thomas
Mesnard, Yao Zhao, Bilal Piot, et al. Direct language model alignment from online ai feedback. arXiv preprint
arXiv:2402.04792, 2024.
Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. Measuring
massive multitask language understanding. arXiv preprint arXiv:2009.03300, 2020.
Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.
Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685, 2021.
Sham Kakade and John Langford. Approximately optimal approximate reinforcement learning. In Proceedings of the
Nineteenth International Conference on Machine Learning, pages 267–274, 2002.
Robert Kirk, Ishita Mediratta, Christoforos Nalmpantis, Jelena Luketina, Eric Hambro, Edward Grefenstette,
and Roberta Raileanu. Understanding the effects of rlhf on llm generalisation and diversity. arXiv preprint
arXiv:2310.06452, 2023.
Wouter Kool, Herke van Hoof, and Max Welling. Buy 4 REINFORCE samples, get a baseline for free! 2019. URL
https://openreview.net/forum?id=r1lgTGL5DE.
John Langford and Tong Zhang. The epoch-greedy algorithm for multi-armed bandits with side information. In
Advances in neural information processing systems, pages 817–824, 2008.
Stephanie Lin, Jacob Hilton, and Owain Evans. Truthfulqa: Measuring how models mimic human falsehoods. arXiv
preprint arXiv:2109.07958, 2021.
Zhihan Liu, Miao Lu, Shenao Zhang, Boyi Liu, Hongyi Guo, Yingxiang Yang, Jose Blanchet, and Zhaoran Wang.
Provably mitigating overoptimization in rlhf: Your sft loss is implicitly an adversarial regularizer. arXiv preprint
arXiv:2405.16436, 2024.
Meta.
Introducing meta llama 3: The most capable openly available llm to date, 2024.
URL https://ai. meta.
com/blog/meta-llama-3, 2024.
Rémi Munos and Csaba Szepesvári. Finite-time bounds for fitted value iteration. Journal of Machine Learning Research,
9(5), 2008.
Rémi Munos, Michal Valko, Daniele Calandriello, Mohammad Gheshlaghi Azar, Mark Rowland, Zhaohan Daniel Guo,
Yunhao Tang, Matthieu Geist, Thomas Mesnard, Andrea Michi, et al. Nash learning from human feedback. arXiv
preprint arXiv:2312.00886, 2023.
Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini
Agarwal, Katarina Slama, Alex Ray, et al. Training language models to follow instructions with human feedback.
Advances in neural information processing systems, 35:27730–27744, 2022.
Arka Pal, Deep Karkhanis, Samuel Dooley, Manley Roberts, Siddartha Naidu, and Colin White. Smaug: Fixing failure
modes of preference optimisation with dpo-positive. arXiv preprint arXiv:2402.13228, 2024.
Rafael Rafailov, Archit Sharma, Eric Mitchell, Christopher D Manning, Stefano Ermon, and Chelsea Finn. Direct
preference optimization: Your language model is secretly a reward model. Advances in Neural Information Processing
Systems, 36, 2023.
Rafael Rafailov, Joey Hejna, Ryan Park, and Chelsea Finn. From r to q∗: Your language model is secretly a q-function.
arXiv preprint arXiv:2404.12358, 2024.
Stephane Ross and J Andrew Bagnell. Agnostic system identification for model-based reinforcement learning. arXiv
preprint arXiv:1203.1007, 2012.
15

Corby Rosset, Ching-An Cheng, Arindam Mitra, Michael Santacroce, Ahmed Awadallah, and Tengyang Xie. Direct nash
optimization: Teaching language models to self-improve with general preferences. arXiv preprint arXiv:2404.03715,
2024.
John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, and Oleg Klimov. Proximal policy optimization
algorithms. arXiv preprint arXiv:1707.06347, 2017.
Archit Sharma, Sedrick Keh, Eric Mitchell, Chelsea Finn, Kushal Arora, and Thomas Kollar. A critical evaluation of ai
feedback for aligning large language models. arXiv preprint arXiv:2402.12366, 2024.
Prasann Singhal, Tanya Goyal, Jiacheng Xu, and Greg Durrett. A long way to go: Investigating length correlations in
rlhf. arXiv preprint arXiv:2310.03716, 2023.
Yuda Song, Yifei Zhou, Ayush Sekhari, J Andrew Bagnell, Akshay Krishnamurthy, and Wen Sun. Hybrid rl: Using
both offline and online data can make rl efficient. arXiv preprint arXiv:2210.06718, 2022.
Nisan Stiennon, Long Ouyang, Jeffrey Wu, Daniel Ziegler, Ryan Lowe, Chelsea Voss, Alec Radford, Dario Amodei,
and Paul F Christiano. Learning to summarize with human feedback. Advances in Neural Information Processing
Systems, 33:3008–3021, 2020.
Gokul Swamy, Christoph Dann, Rahul Kidambi, Zhiwei Steven Wu, and Alekh Agarwal. A minimaximalist approach
to reinforcement learning from human feedback. arXiv preprint arXiv:2401.04056, 2024.
Fahim Tajwar, Anikait Singh, Archit Sharma, Rafael Rafailov, Jeff Schneider, Tengyang Xie, Stefano Ermon, Chelsea
Finn, and Aviral Kumar. Preference fine-tuning of llms should leverage suboptimal, on-policy data. arXiv preprint
arXiv:2404.14367, 2024.
Yunhao Tang, Daniel Zhaohan Guo, Zeyu Zheng, Daniele Calandriello, Yuan Cao, Eugene Tarassov, Rémi Munos,
Bernardo Ávila Pires, Michal Valko, Yong Cheng, et al. Understanding the performance gap between online and
offline alignment algorithms. arXiv preprint arXiv:2405.08448, 2024.
Gemini Team, Rohan Anil, Sebastian Borgeaud, Yonghui Wu, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan
Schalkwyk, Andrew M Dai, Anja Hauth, et al. Gemini: a family of highly capable multimodal models. arXiv
preprint arXiv:2312.11805, 2023.
Hugo Touvron, Louis Martin, Kevin Stone, Peter Albert, Amjad Almahairi, Yasmine Babaei, Nikolay Bashlykov,
Soumya Batra, Prajjwal Bhargava, Shruti Bhosale, et al. Llama 2: Open foundation and fine-tuned chat models.
arXiv preprint arXiv:2307.09288, 2023.
Masatoshi Uehara and Wen Sun. Pessimistic model-based offline reinforcement learning under partial coverage. arXiv
preprint arXiv:2107.06226, 2021.
Leandro von Werra, Younes Belkada, Lewis Tunstall, Edward Beeching, Tristan Thrush, Nathan Lambert, and Shengyi
Huang. Trl: Transformer reinforcement learning. https://github.com/huggingface/trl, 2020.
Ronald J Williams. Simple statistical gradient-following algorithms for connectionist reinforcement learning. Machine
learning, 8:229–256, 1992.
Tengyang Xie, Ching-An Cheng, Nan Jiang, Paul Mineiro, and Alekh Agarwal. Bellman-consistent pessimism for
offline reinforcement learning. Advances in neural information processing systems, 34:6683–6694, 2021a.
Tengyang Xie, Nan Jiang, Huan Wang, Caiming Xiong, and Yu Bai. Policy finetuning: Bridging sample-efficient offline
and online reinforcement learning. Advances in neural information processing systems, 34:27395–27407, 2021b.
Tengyang Xie, Dylan J Foster, Yu Bai, Nan Jiang, and Sham M. Kakade. The role of coverage in online reinforcement
learning. In The Eleventh International Conference on Learning Representations, 2023. URL https://openreview.
net/forum?id=LQIjzPdDt3q.
Wei Xiong, Han Zhong, Chengshuai Shi, Cong Shen, Liwei Wang, and Tong Zhang. Nearly minimax optimal offline
reinforcement learning with linear function approximation: Single-agent mdp and markov game. arXiv preprint
arXiv:2205.15512, 2022.
16

Wei Xiong, Hanze Dong, Chenlu Ye, Ziqi Wang, Han Zhong, Heng Ji, Nan Jiang, and Tong Zhang. Iterative preference
learning from human feedback: Bridging theory and practice for rlhf under kl-constraint. In ICLR 2024 Workshop on
Mathematical and Empirical Understanding of Foundation Models, 2023.
Wei Xiong, Hanze Dong, Chenlu Ye, Ziqi Wang, Han Zhong, Heng Ji, Nan Jiang, and Tong Zhang. Iterative preference
learning from human feedback: Bridging theory and practice for rlhf under kl-constraint. In Forty-first International
Conference on Machine Learning, 2024.
Haoran Xu, Amr Sharaf, Yunmo Chen, Weiting Tan, Lingfeng Shen, Benjamin Van Durme, Kenton Murray, and
Young Jin Kim. Contrastive preference optimization: Pushing the boundaries of llm performance in machine
translation. arXiv preprint arXiv:2401.08417, 2024a.
Shusheng Xu, Wei Fu, Jiaxuan Gao, Wenjie Ye, Weilin Liu, Zhiyu Mei, Guangju Wang, Chao Yu, and Yi Wu. Is dpo
superior to ppo for llm alignment? a comprehensive study. arXiv preprint arXiv:2404.10719, 2024b.
Lifan Yuan, Ganqu Cui, Hanbin Wang, Ning Ding, Xingyao Wang, Jia Deng, Boji Shan, Huimin Chen, Ruobing Xie,
Yankai Lin, et al. Advancing llm reasoning generalists with preference trees. arXiv preprint arXiv:2404.02078, 2024.
Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. Hellaswag: Can a machine really finish your
sentence? arXiv preprint arXiv:1905.07830, 2019.
Wenhao Zhan, Baihe Huang, Audrey Huang, Nan Jiang, and Jason Lee. Offline reinforcement learning with realizability
and single-policy concentrability. In Conference on Learning Theory, pages 2730–2775. PMLR, 2022.
Wenhao Zhan, Masatoshi Uehara, Nathan Kallus, Jason D Lee, and Wen Sun. Provable offline preference-based
reinforcement learning. In The Twelfth International Conference on Learning Representations, 2023.
Yao Zhao, Rishabh Joshi, Tianqi Liu, Misha Khalman, Mohammad Saleh, and Peter J Liu. Slic-hf: Sequence likelihood
calibration with human feedback. arXiv preprint arXiv:2305.10425, 2023.
Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan
Li, Dacheng Li, Eric Xing, et al. Judging llm-as-a-judge with mt-bench and chatbot arena. Advances in Neural
Information Processing Systems, 36, 2024.
Banghua Zhu, Hiteshi Sharma, Felipe Vieira Frujeri, Shi Dong, Chenguang Zhu, Michael I Jordan, and Jiantao Jiao.
Fine-tuning language models with advantage-induced policy alignment. arXiv preprint arXiv:2306.02231, 2023.
Brian D Ziebart, Andrew L Maas, J Andrew Bagnell, Anind K Dey, et al. Maximum entropy inverse reinforcement
learning. In Aaai, volume 8, pages 1433–1438. Chicago, IL, USA, 2008.
17

A
Auxiliary Lemmas
Lemma A.1 (Objective decomposition). Let J(π) be the objective function defined in (1), and for reward function ˆr,
we let
ˆπ ∈argmax
π
Ex∼ρ

Ey∼π(·|x)[ˆr(x, y)] −βKL(π(· | x)||πref(· | x))

,
(8)
then we have
J(π∗) −J(ˆπ) ≤Ex∼ρ

Ey1∼π∗(·|x),y2∼ˆπ(·|x)

r∗(x, y1) −ˆr(x, y1) −r∗(x, y2) + ˆr(x, y2)

.
Proof. We have
J(π∗) −J(ˆπ)
=Ex∼ρ

Ey∼π∗(·|x)[r∗(x, y)] −βKL(π∗(· | x)||πref(· | x))

−Ex∼ρ

Ey∼ˆπ(·|x)[r∗(x, y)] + βKL(ˆπ(· | x)||πref(· | x))

=Ex∼ρ

Ey∼π∗(·|x)[r∗(x, y)] −βKL(π∗(· | x)||πref(· | x))

−
 Ex∼ρ

Ey∼ˆπ(·|x)[r∗(x, y)] −βKL(ˆπ(· | x)||πref(· | x))

+ Ex∼ρ

Ey∼ˆπ(·|x)[ˆr(x, y)] −βKL(ˆπ(· | x)||πref(· | x))

−
 Ex∼ρ

Ey∼ˆπ(·|x)[ˆr(x, y)] −βKL(ˆπ(· | x)||πref(· | x))

≤Ex∼ρ

Ey∼π∗(·|x)[r∗(x, y)] −βKL(π∗(· | x)||πref(· | x))

−
 Ex∼ρ

Ey∼π∗(·|x)[ˆr(x, y)] −βKL(π∗(· | x)||πref(· | x))

+ Ex∼ρ

Ey∼ˆπ(·|x)[ˆr(x, y)] −βKL(ˆπ(· | x)||πref(· | x))

−
 Ex∼ρ

Ey∼ˆπ(·|x)[r∗(x, y)] −βKL(ˆπ(· | x)||πref(· | x))

=Ex∼ρ

Ey∼π∗(·|x)[r∗(x, y) −ˆr(x, y)]

−Ex∼ρ

Ey∼ˆπ(·|x)[r∗(x, y) −ˆr(x, y)]

,
where the inequality is due to Eq. (8). To complete the proof, note that
Ex∼ρ

Ey∼π∗(·|x)[r∗(x, y) −ˆr(x, y)] −Ex∼ρEy∼ˆπ(·|x)[r∗(x, y) −ˆr(x, y)]

=Ex∼ρ

Ey1∼π∗(·|x),y2∼ˆπ(·|x)[r∗(x, y1) −ˆr(x, y1)]

−Ex∼ρ

Ey1∼π∗(·|x),y2∼ˆπ(·|x)[r∗(x, y2) −ˆr(x, y2)]

=Ex∼ρ

Ey1∼π∗(·|x),y2∼ˆπ(·|x)

r∗(x, y1) −ˆr(x, y1) −r∗(x, y2) + ˆr(x, y2)

.
Lemma A.2 (Lemma C.2 from (Chang et al., 2024)). Assume that r∗is bounded, let R be the reward function class,
and Let
ˆr = argmin
r∈R
ˆEx,y+,y−∼D

log

exp(r(x, y+))
exp(r(x, y+)) + exp(r(x, y−))

,
then we have with probability at least 1 −δ that
Ex,y1,y2∼µ◦πref
h r∗(x, y1) −r∗(x, y2) −ˆr(x, y1) + ˆr(x, y2)
2i
≤cκ2 log(|R|/δ)
N
,
where κ measures the non-linearity of the link function, and c is a constant, N := |D| is the size of the offline dataset.
B
Additional Results
B.1
Results for IPO
In this section we give detailed technical details for IPO, and the negative results for IPO under partial coverage. Recall
that the empirical objective of IPO is is πipo ∈argminπ c
ℓipo(π), where
c
ℓipo(π) = bEx,y+,y−∼D
"
log
π(y+ | x)πref(y−| x)
π(y−| x)πref(y+ | x)

−β−1
2
2#
.
18

The empirical objective is derived from the following population loss
ℓipo(π) = Ex,y1,y2∼ρ◦πref
h hπ
 y1, y2
−I
 y1, y2
/β
2i
,
(9)
where
hπ(y1, y2) = log
π(y1)πref(y2)
π(y2)πref(y1)

,
and I(y1, y2) is a Bernoulli random variable with parameter p = p∗(y1 ≻y2), where here p∗can be any underlying
human preference (that is not necessarily parametrized by the Bradley Terry model). To show the negative result, we
can make the following learning assumption:
Assumption B.1 (In distribution guarantee for IPO). We assume that the returned policy πipo satisfies that
πipo = argmin
π∈Π
ℓipo(π),
i.e., the returned policy πipo induces the smallest possible in-distribution error on its population loss.
With the setup, we can state and prove the formal version of the result:
Proposition B.1 (Formal version of of Proposition 4.2). Denote πref as any reference policy such that Assumption 4.1
breaks. Let Πipo be the set of IPO returned policies such that Assumption B.1 holds. Then there exists policy π ∈Πipo
such that J(π) = −∞.
Proof. Without loss of generality, we consider a promptless setting, and assume that the response space is Y =
{y1, y2, y3}. Again without loss of generality, we assume πref only covers y1 and y2, and thus Assumption 4.1 breaks.
Specifically, let πref(y1) = πref(y2) = 1/2. Then we have
πipo = argmin
π∈Π
Ey1,y2∼πref
"
log
π(y1)
π(y2)

−I
 y1, y2
/β
2#
,
which gives
log
πipo(y1)
πipo(y2)

= p∗(y1 ≻y2)/β,
and thus we have the relation that
πipo(y1) = πipo(y2) · exp(p∗(y1 ≻y2)/β).
Let πipo(y2) = α ∈(0, 1], then for any α such that πipo(y3) = 1 −(1 + exp(p∗(y1 ≻y2)/β))α > 0, we will have that
KL(πipo||πref) is unbounded, and thus we complete the proof.
B.2
DPO Has Vacuous Forward KL
In this section, we show that in the worst case, the forward KL of DPO is vacuously large. We first see how we can
relate the forward KL divergence of the DPO policy with the reward learning guarantee. Consider any DPO policy
πdpo and its corresponding reward model d
rdpo. By construction of the DPO algorithm, we have, for any x, y pair that is
covered in the dataset,
πdpo(y | x) = πref(y | x) exp(d
rdpo(x, y)/β)
Z(x)
,
where Z(x) = P
y πref(y | x) exp(d
rdpo(x, y)/β). Then the forward KL divergence is
KL(πref||πdpo) = Ex∼ρ[KL(πref(· | x)||πdpo(· | x))] = Ex∼ρEy∼πref(·|x)

log
 πref(y | x)
πdpo(y | x)

= Ex∼ρEy∼πref(·|x)

log

Z(x)
exp(d
rdpo(x, y)/β)

= Ex∼ρEy∼πref(·|x)

−d
rdpo(x, y)
β
+ log(Z(x))

.
19

Although the first term can be easily related to the reward learning guarantee, the second term (Ex∼ρ[log(Z(x))]) can
unfortunately be vacuous without further assumptions. We formalize in the following result:
Proposition B.2. There exist πdpo such that Assumption 4.3 holds, but KL(πref||πdpo) is arbitrarily large.
Proof. First without loss of generality let us consider that r∗> 0. Now suppose there exists ˜y such that πref(˜y | x) =
1
n4
for all x, where n will be determined soon. Now suppose that for all x, d
rdpo(x, ˜y) −r∗(x, ˜y) = n and d
rdpo(x, y) =
r∗(x, y) for all y ̸= ˜y. Now we can check that
Ex∼ρEy∼πref(·|x)
h
(d
rdpo(x, y) −r∗(x, y))2i
= 1
n2 ,
which is diminishing if we take n to be big enough. We can also check that
Ex∼ρEy∼πref(·|x)

−d
rdpo(x, y)
β

≥−1
n3β −R
n4β
and thus the first term will have little impact on the final bound. However, the second term can be lower bounded as
follows:
log
 X
y
πref(y | x) exp(br(x, y)/β)
!
= log
 X
y
πref(y | x) exp
r∗(x, y) + br(x, y) −r∗(x, y)
β
!
≥log
 X
y
πref(y | x) exp
br(x, y) −r∗(x, y)
β
!
= log

πref(˜y | x) exp
br(x, ˜y) −r∗(x, ˜y)
β

= n
β −4 log(n).
Putting everything together, we have
KL(πref||πdpo) ≥n
β −4 log(n) −
1
n3β −R
n4β
and since we can take n arbitrarily big we complete the proof.
C
Missing Proofs
C.1
Proof of Proposition 4.1
Proposition C.1 (Restatement of Proposition 4.1). Denote πref as any reference policy such that Assumption 4.1 breaks.
Let Πdpo be the set of DPO returned policies such that Assumption 4.3 holds. Then there exists policy π ∈Πdpo such
that J(π) = −∞.
Proof. Again as in the proof sketch, without loss of generality, we consider a promptless setting, and assume that the
response space is Y = {y1, y2, y3}. Again without loss of generality, we assume πref only covers y1 and y2, and thus
Assumption 4.1 breaks. Now consider the optimal policy
π∗(y) = πref(y | x) exp(r∗(y)/β)
Z∗(t)
, ∀y ∈Y,
where Z∗= P
y∈Y πref(y | x) exp(r∗(y)/β), note that by construction π∗(y3) = 0.
Then consider the following policy π such that
β log

π(y1)
πref(y1) · Z∗

= r∗(y1) −√εdpo,
and
β log

π(y2)
πref(y2) · Z∗

= r∗(y2) −√εdpo,
20

Then we have
Ey∼πref
"
β log

πdpo(y)
πref(y | x) · Z∗

−r∗(x, y)
2#
= εdpo,
thus π satisfies Assumption 4.3. Rearranging we can see that π(y1) < π∗(y1) and π(y2) < π∗(y2). Now since π∗= 0,
we have
π∗(y1) + π∗(y2) = 1,
and combine we get π(y3) > 0, which implies KL(π||πref) is unbounded, since πref(y3) = 0.
C.2
Proof of Theorem 4.2
In this section we prove Theorem 4.2:
Theorem C.1 (Restatement of Theorem 4.2). Suppose that Assumption 4.4 holds. Then for any reference policy πref
such that Assumption 4.2 holds with εkl = 2R′
β , for any RLHF policy πrlhf with br such that (c.r. Assumption 4.3),
Ex,y∼ρ◦πref
h
(r∗(x, y) −br(x, y))2i
≤εreward,
or more generally, the event in Lemma A.2 holds for br, we have
J(π∗) −J(πrlhf) ≤O(Cεkl
√εreward).
To prove this we first prove the following lemma so we can leverage Assumption 4.2:
Lemma C.1 (Restatement of Lemma 4.1). Suppose that Assumption 4.4 holds. Then for any RLHF policy πrlhf, we
have that
KL(πrlhf||πref) := Ex∼ρEy∼πrlhf(·|x)

log
πrlhf(y | x)
πref(y | x)

≤2R′
β .
Proof. since we have that πrlhf(y | x) = πref(y|x) exp(ˆr(x,y)/β)
Z(x)
for all x ∈supp(ρ), y ∈Y, we have
KL(πrlhf||πref) = Ex∼ρEy∼πrlhf(·|x)

log
exp(ˆr(x, y))
βZ(x)

= Ex∼ρEy∼πrlhf(·|x)
 ˆr(x, y)
β
−log(Z(x))

.
Plugging in the definition of Z(x) we get
log(Z(x)) = log

Ey∼πref(·|x)

exp
 ˆr(x, y)
β

≥Ey∼πref(·|x)
 ˆr(x, y)
β

due to Jensen’s inequality. Thus we have
KL(πrlhf||πref) ≤Ex∼ρEy∼πrlhf(·|x)
 ˆr(x, y)
β

−Ex∼ρEy∼πrlhf(·|x)
 ˆr(x, y)
β

≤2R′
β .
Now with Lemma 4.1, we can prove Theorem 4.2:
21

Proof. By Lemma A.1, we have
J(π∗) −J(πrlhf)
≤Ex∼ρEy1∼π∗(·|x),y2∼πrlhf(·|x)

r∗(x, y1) −br(x, y1) −r∗(x, y2) + br(x, y2)

≤
r
Ex∼ρEy1∼π∗(·|x),y2∼πrlhf(·|x)
h
(r∗(x, y1) −br(x, y1) −r∗(x, y2) + br(x, y2))2i
≤
r
C2
gloEx∼ρEy1,y2∼πref(·|x)
h
(r∗(x, y1) −br(x, y1) −r∗(x, y2) + br(x, y2))2i
(Lemma 4.1 and Assumption 4.2)
≤C√εreward.
(Lemma A.2)
D
Details of Section 5
D.1
Theoretical guarantee
In this section, we consider the constrained optimization version of HyPO (Eq. (6)). Note that the reward function
class is identical to DPO, i.e., Rhypo =
n
β log

π(y|x)
πref(y|x)Z(x)

| π ∈Π
o
, where Z(x) is the partition function. Then
for each output policy πhypo, we can denote its implicit reward function [
rhypo(x, y) := β
πhypo(y|x)
πref(y|x)·Z(x), and similarly to
Theorem 4.2, we can obtain the following guarantee in the partial coverage condition:
Theorem D.1. For any reference policy πref such that Assumption 4.2 holds with εkl = 2R′
β , for any HyPO policy πhypo
such that the event in Lemma A.2 holds, i.e.,
Ex,y1,y2∼µ◦πref
h r∗(x, y1) −r∗(x, y2) −[
rhypo(x, y1) + [
rhypo(x, y2)
2i
≤εhypo,
we have
J(π∗) −J(πhypo) ≤O(Cεkl
√εhypo).
Proof. The proof mostly follows the proof of Theorem 4.2. It remains to show the following two properties:
1) Note that Theorem 4.2 requires Assumption 4.4, which does not hold for [
rhypo (note that [
rhypo is only bounded
under ρ, but not for all x), but we only use it to prove the sufficient condition in Lemma 4.1, which is satisfied by the
constraint of HyPO.
2) We need to check that the premise of Lemma A.1 holds, i.e.,
πhypo ∈argmax
π
Ex∼ρ

Ey∼π(·|x)[[
rhypo(x, y)] −βKL(π(· | x)||πref(· | x))

,
note that with the reparametrization between πhypo and [
rhypo, πhypo is always among the minimizer of the unconstrained
policy set, so we can still invoke Lemma A.2. The rest of the proof now follows the proof of Theorem 4.2 so we omit
the details.
Finally, we remark the connection to the negative result of DPO, i.e, Proposition 4.1: note that given KL(πhypo||πref) ≤
∞, we have that for all x such that ρ(x) > 0, we have for all y, β log

πhypo(y|x)
πref(y|x)

< ∞, (again with the convention that
0
0 = 0), which breaks the construction of Proposition 4.1.
22

D.2
Experiment details
D.2.1
Summarization
In this section, we provide more details of our summarization experiment. We use the Pythia 1.4B and 2.8B model
(Biderman et al., 2023) with hugging face model cards: EleutherAI/pythia-1.4b-deduped and EleutherAI/pythia-2.8b-
deduped. The TL;DR dataset is available at https://github.com/openai/summarize-from-feedback. The human
reference dataset contains 117k training, 6.45K validation and 6.55K testing data. The preference dataset contains
92.9K training and 83.8K validation data. The reward evaluation and KL computation is performed on the whole
validation data of the reference dataset. The GPT winrate is computed on a subset of 600 samples from the validation
data. The GPT API checkpoint we use is gpt-4-0613. We follow the standard prompt for the winrate evaluation (e.g.,
see Appendix D.3 of Gao et al. (2024)). Below we provide the hyperparameter for HyPO and DPO. Note that to optmize
the online KL, we use Reinforce with Leave One Out (RLOO) (Kool et al., 2019) with two generations per prompt
(k = 2) and optimize trajectory-level KL.
For our experiment, we run on a cluster of mixture of Nvidia A6000 and L40 GPUs with 48 GB VRAM. We use 4
GPUs in parallel for training, and for DPO the experiment time varies from 1 hour to 2 hours to finish, and for HyPO
the time varies between 4 hours to 5 hours.
Table 4: RM/SFT hyperparameters.
Learning rate
3e-6
Batch size
64
Learning rate scheduler
cosine
Optimizer
Adamw
LoRA
False
Table 5: DPO hyperparameters.
Learning rate
3e-6
Batch size
64
Learning rate scheduler
cosine
Optimizer
Adamw
β
0.05
Table 6: HyPO hyperparameters.
Learning rate
3e-6
Batch size
64
Learning rate scheduler
cosine
Optimizer
Adamw
β
0.05
λ
0.0001
RLOO k
2
Table 7: Lora configurations.
r
1024
α
2048
Dropout
0
D.2.2
General Chat
For the base model of general chat experiments, we use Llama3-8B-Instruct (Meta, 2024) with hugging face model
card: meta-llama/Meta-Llama-3-8B-Instruct. The dataset card of the Ultrafeedback dataset (Cui et al., 2023) is
HuggingFaceH4/ultrafeedback_binarized. In addition to the KL penalty, in the general chat task we add an additional
length penalty, and the online penalty of a generation y with context x becomes log

π(y|x)
πref(y|x)

+ α|y|. We summarize
the hyperparameter of each baseline below.
We run the general chat experiment on a node of 8 Nvidia A100 80GB GPUs. DPO takes 3 hours to train one epoch
while HyPO takes 18 hours to train one epoch.
23

Table 8: HyPO hyperparameters.
Learning rate
3e-6
Batch size
8
Learning rate scheduler
linear
Optimizer
Adamw
β
0.05
λ
0.0002
RLOO k
2
α
0.02
Table 9: DPO hyperparameters.
Learning rate
3e-6
Batch size
8
Learning rate scheduler
linear
Optimizer
Adamw
β
0.05
24

