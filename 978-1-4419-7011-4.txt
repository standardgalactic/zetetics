
Sparse and Redundant 
Representations 

Sparse and Redundant 
Representations 
From Theory to Applications in  
Signal and Image Processing 
Michael Elad 
The Technion – Israel Institute of Technology 
Haifa, Israel 

 
 
L
 
  
 
All rights reserved.  This work may not be translated or copied in whole or in part without the written 
The use in this publication of trade names, trademarks, service marks, and similar terms, even if they 
are not identified as such, is not to be taken as an expression of opinion as to whether or not they are 
subject to proprietary rights. 
Printed on acid-free paper 
connection with any form of information storage and retrieval, electronic adaptation, computer software, 
or by similar or dissimilar methodology now known or hereafter developed is forbidden. 
© Springer Science+Business Media, LLC 2010 
ibrary of Congress Control Number: 2010933513
permission of the publisher (Springer Science+Business Media, LLC, 233 Spring Street, New York, 
Mathematics Subject Classification (2010):  94A12, 62H35, 62M40, 68U10, 94A08, 94H60, 46N10 
Springer New York Dordrecht Heidelberg London 
DOI 10.1007/978-1-4419-7011-4 
Michael Elad 
The Technion – Israel Institute of Technology 
Computer Science Department 
32000 Haifa, Israel 
ISBN 978-1-4419-7010-7 
e-ISBN 978-1-4419-7011-4 
NY 10013, USA), except for brief excerpts in connection with reviews or scholarly analysis.  Use in 
elad@cs.technion.ac.il 
Springer is part of Springer Science+Business Media (www.springer.com) 

To my loving and devoted family,
My parents, Rita and Shmuel Elbaz, who
have shown me the way,
My children Sharon, Adar, and Yuval, who
give purpose to this way, and
My wife, Ritta, who constantly paves this
way with love and understanding.

Foreword
A long long time ago, echoing philosophical and aesthetic principles that existed
since antiquity, William of Ockham enounced the principle of parsimony, better
known today as Ockham’s razor: “Entities should not be multiplied without neces-
sity.” This principle enabled scientists to select the ”best” physical laws and theories
to explain the workings of the Universe and continued to guide scientiﬁc research,
leading to beautiful results like the minimal description length approach to statistical
inference and the related Kolmogorov complexity approach to pattern recognition.
However, notions of complexity and description length are subjective concepts
and depend on the language “spoken” when presenting ideas and results. The ﬁeld of
sparse representations, that recently underwent a Big-Bang-like expansion, explic-
itly deals with the Yin-Yang interplay between the parsimony of descriptions and
the “language” or “dictionary” used in them, and it became an extremely exciting
area of investigation.It already yielded a rich crop of mathematically pleasing, deep
and beautiful results that quickly translated into a wealth of practical engineering
applications.
You are holding in your hands the ﬁrst guide-book to Sparseland, and I am sure
you’ll ﬁnd in it both familiar and new landscapes to see and admire, as well as ex-
cellent pointers that will help you ﬁnd further valuable treasures. Enjoy the journey
to Sparseland!
Haifa, Israel, December 2009
Alfred M. Bruckstein
vii

Preface
This book was originally written to serve as the material for an advanced one-
semester (fourteen 2-hour lectures) graduate course for engineering students at the
Technion, Israel. It is originally based on a review paper that I wrote together with
Alfred M. Bruckstein and David L. Donoho, which appeared in SIAM Review on
February 2009. The content of this paper has been massively modiﬁed and extended
to become an appropriate material for an advanced graduate course. In this book
I introduce the topic of sparse and redundant representations, show the questions
posed in this ﬁeld, and the answers they get, present the ﬂavor of the research in this
arena, the people involved, and more. It is my desire to expose the readers of this
book (and the students in the course) to the beauty of this ﬁeld, and the potential it
encompasses for various disciplines.
So, what is it all about? Mathematicians active in this ﬁeld (and there are many
of those) would mention the deep relation between this emerging ﬁeld and harmonic
analysis, approximation theory, and wavelets, in their answer. However, this is not
my answer. As an engineer, my interest is more in the practical sides of this ﬁeld,
and thus my entire motivation for studying sparse and redundant representations
comes from the applications they serve in signal and image processing. This is not
to say that I ﬁnd little interest in the theoretical sides of the activity in sparse and
redundant representations. However, these theoretical results should be considered
in the context of the wider picture.
As I see it, this ﬁeld is all about one speciﬁc mathematical model for signal
sources. Modeling sources is key in signal and image processing. Armed with a
proper model, one can handle various tasks such as denoising, restoration, separa-
tion, interpolation and extrapolation, compression, sampling, analysis and synthe-
sis, detection, recognition, and more. Indeed, a careful study of the signal and image
processing literature reveals that there is an evolution of such models and their use
in applications. This course is about one such model, which I call Sparse-Land for
brevity. This speciﬁc model is intriguing and fascinating because of the beauty of its
theoretical foundations, the superior performance it leads to in various applications,
its universality and ﬂexibility in serving various data sources, and its uniﬁed view,
which makes all the above signal processing tasks clear and simple.
ix

x
Preface
At the heart of this model lies a simple linear system of equations, the kind of
which seems long studied in linear algebra. A full-rank matrix A ∈IRn×m with n < m
generates an underdetermined system of linear equations Ax = b having inﬁnitely
many solutions. We are interested in seeking its sparsest solution, i.e., the one with
the fewest nonzero entries. Can such a solution ever be unique? If so, when? How
can such a solution be found in reasonable time? It is hard to believe, but these
questions and more of the like form the core engine to this model and the wide
ﬁeld that has grown around it. Positive and constructive answers to these questions
in recent years expose a wide variety of surprising phenomena, and those set the
stage for Sparse-Land to serve in practice. Clearly, research in this ﬁeld calls for an
intensive use of knowledge from linear algebra, optimization, scientiﬁc computing,
and more.
This ﬁeld is relatively young. Early signs of its core ideas appeared in a pio-
neering work by Stephane Mallat and Zhifeng Zhang in 1993, with the introduction
of the concept of dictionaries, replacing the more traditional and critically-sampled
wavelet transform. Their work put forward some of the core ideas that later became
central in this ﬁeld, such as a greedy pursuit technique that approximates a sparse
solution to an underdetermined linear system of equations, characterization of dic-
tionaries by their coherence measure, and more.
A second key contribution in 1995 was by Scott Shaobing Chen, David Donoho,
and Michael Saunders, who introduced another pursuit technique that uses the ℓ1-
norm for evaluating sparsity. Surprisingly, they have shown that the quest for the
sparsest solution could be tackled as a convex programming task, often leading to
the proper solution.
With these two contributions, the stage was set for a deeper analysis of these al-
gorithms and their deployment in applications. A crucial step towards this goal was
made in 2001, with the publication of the work by Donoho and Huo. In their dar-
ing paper, Donoho and Huo deﬁned and (partly) answered what later became a key
question in this ﬁeld: Can one guarantee the success of a pursuit technique? Under
what conditions? This line of analysis later became the skeleton of this ﬁeld, pro-
viding the necessary theoretical backbone for the Sparse-Land model. It is amazing
to see how fast and how vast this area of research has grown in recent years, with
hundreds of interested researchers, various workshops, sessions, and conferences,
and an exponentially growing number of papers.
The activity in this ﬁeld is spread over all major universities and research orga-
nizations around the world, with leading scientists from various disciplines. As this
ﬁeld of research lies at the intersection between signal processing and applied math-
ematics, active in this ﬁeld are mathematicians interested in approximation theory,
applied mathematicians interested in harmonic analysis, statisticians, and engineer-
ing from various ﬁelds (computer science, electrical engineering, geophysics, and
more).
A little bit about myself: I started my activity in this area after a short visit by
David Donoho to the Technion, where he gave a talk describing the results of the
above-mentioned paper. I am ashamed to admit that I did not attend the talk! Nev-
ertheless, my good friend and mentor, Freddy Bruckstein, was there to appreciate

Preface
xi
the opportunity. Freddy’s sixth sense for important research directions is already
famous at the Technion, and did not fail him this time too. Freddy insisted that we
work on improving the results that Donoho presented, and few months later we had
our ﬁrst result in this ﬁeld. This result led to a joint work with Donoho during a
post-doc period at Stanford, and I have found myself deeply involved in research on
these topics ever since.
My origins as an engineer, and the fact that I worked with a leading mathemati-
cian, give me a unique perspective which I bring to this book. I present a coherent,
well-structured, and ﬂowing story that includes some of the theoretical foundations
to sparse and redundant representations, numerical tools and algorithm for actual
use, and applications in signal and image processing that beneﬁt from these. I should
stress that I do not pretend to give a well-balanced view of this entire ﬁeld, and in-
stead, I am giving my own point-of-view. In particular, I am not covering all the
accumulated knowledge in this book because: (i) it is impossible to mention every-
thing; (ii) not all details are important to the story I have to tell; (iii) new results in
this ﬁeld are emerging on a daily basis, making it impossible to give a completely
updated view; and (iv) I have to admit that I do not understand all the results in this
ﬁeld.
Compressed-Sensing is a recent branch that separated from sparse and redun-
dant representations, becoming a center of interest of its own. Exploiting sparse
representation of signals, their sampling can be made far more eﬀective compared
to the classical Nyquist-Shannon sampling. In a recent work that emerged in 2006
by Emmanuel Candes, Justin Romberg, Terence Tao, David Donoho, and others
that followed, the theory and practice of this ﬁeld were beautifully formed, sweep-
ing many researchers and practitioners in excitement. The impact this ﬁeld has is
immense, strengthened by the warm hug by information-theorists, leading mathe-
maticians, and others. So popular has this ﬁeld become that many confuse it with
being the complete story of sparse representation modeling. In this book I discuss
the branch of activity on Compressed-Sensing very brieﬂy, and mostly so as to tie it
to the more general results known in sparse representation theory. I believe that the
accumulated knowledge on compressed-sensing could easily ﬁll a separate book.
The CS Department, the Technion, Israel
Michael Elad
September 2009

Acknowledgments
This book would not have been written without the encouragement and support from
the many friends and colleagues that surround me. First and foremost, I am grate-
ful to Freddy Bruckstein, David Donoho, and Gene Golub, my mentors, teachers,
and friends. It is their encouragement and guidance that brought me thus far. Gene
passed away 3 years ago (November 16th, 2007), but he is with me all the time.
This book is mostly built on the accumulated knowledge that I gathered in the
past decade in this fascinating ﬁeld of sparse and redundant representation model-
ing. This vast work was possible because of the excellent researchers I collaborated
with, my students, friends, and colleagues.
I owe a lot to my MSc and PhD students, Amir Adler, Michal Aharon, Zvika
Ben-Haim, Raviv Brueler, Ori Bryt, Dima Datsenko, Tomer Factor, Raja Giryes,
Einat Kidron, Boaz Matalon, Matan Protter, Svetlana Raboy, Ron Rubinstein, Dana
Segev, Neta Shoham, Yossi Shtok, Javier Turek, and Roman Zeyde. I am grateful
for the experience of working with them, teaching and guiding them, and learning
so much from them in this process.
I am thankful to the colleagues and friends that were willing to work with me and
guide me, forgiving my shortcomings and being so patient with me. These include
Yonina Eldar, Arie Feuer, Mario Figueiredo, Yakov Hel-Or, Ron Kimmel, Yi Ma,
Julien Mairal, Stephane Mallat, Peyman Milanfar, Guillermo Sapiro, Yoav Schech-
ner, Jean-Luc Starck, Volodya Temlyakov, Irad Yavneh, and Michael Zibulevsky. I
would like to express my gratitude to my friends and colleagues, Remi Gribonval,
Jalal Fadili, Ron Kimmel, Miki Lustig, Gabriel Peyre, Doron Shaked, Joel Tropp,
and Pierre Vandergheynst, for insightful discussions throughout the years, and their
long-standing support.
Finally, special thanks are in order to Michael Bronstein, who designed the book
cover, and to Allan Pinkus, who proof-read portions of this book, and made various
valuable suggestions, which improved it.
More broadly, and without mentioning additional names, I should add that work-
ing in this ﬁeld means that I am part of well-formed research community. I have
been very lucky to belong to this diverse, vivid, ever growing, and warm group of
researchers, which make my working experience such a pleasure.
xiii

Contents
Part I Sparse and Redundant Representations – Theoretical and Numerical
Foundations
1
Prologue . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
1.1
Underdetermined Linear Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
1.2
Regularization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
4
1.3
The Temptation of Convexity. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5
1.4
A Closer Look at ℓ1 Minimization. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6
1.5
Conversion of (P1) to Linear Programming . . . . . . . . . . . . . . . . . . . . .
8
1.6
Promoting Sparse Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
8
1.7
The ℓ0-Norm and Implications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 12
1.8
The (P0) Problem – Our Main Interest . . . . . . . . . . . . . . . . . . . . . . . . . 13
1.9
The Signal Processing Perspective . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14
2
Uniqueness and Uncertainty . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
2.1
Treating the Two-Ortho Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 17
2.1.1
An Uncertainty Principle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 18
2.1.2
Uncertainty of Redundant Solutions. . . . . . . . . . . . . . . . . . . . . 21
2.1.3
From Uncertainty to Uniqueness . . . . . . . . . . . . . . . . . . . . . . . 23
2.2
Uniqueness Analysis for the General Case . . . . . . . . . . . . . . . . . . . . . . 23
2.2.1
Uniqueness via the Spark . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 23
2.2.2
Uniqueness via the Mutual-Coherence. . . . . . . . . . . . . . . . . . . 25
2.2.3
Uniqueness via the Babel Function . . . . . . . . . . . . . . . . . . . . . 27
2.2.4
Upper-Bounding the Spark . . . . . . . . . . . . . . . . . . . . . . . . . . . . 28
2.3
Constructing Grassmannian Matrices . . . . . . . . . . . . . . . . . . . . . . . . . . 29
2.4
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 30
Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
xv

xvi
Contents
3
Pursuit Algorithms – Practice . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
3.1
Greedy Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
3.1.1
The Core Idea . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 35
3.1.2
The Orthogonal-Matching-Pursuit . . . . . . . . . . . . . . . . . . . . . . 36
3.1.3
Other Greedy Methods. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 39
3.1.4
Normalization . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 41
3.1.5
Rate of Decay of the Residual in Greedy Methods . . . . . . . . . 43
3.1.6
Thresholding Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 45
3.1.7
Numerical Demonstration of Greedy Algorithms . . . . . . . . . . 46
3.2
Convex Relaxation Techniques . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
3.2.1
Relaxation of the ℓ0-Norm. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 48
3.2.2
Numerical Algorithms for Solving (P1) . . . . . . . . . . . . . . . . . . 51
3.2.3
Numerical Demonstration of Relaxation Methods . . . . . . . . . 51
3.3
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 52
Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 53
4
Pursuit Algorithms – Guarantees . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
4.1
Back to the Two-Ortho Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 55
4.1.1
OMP Performance Guarantee . . . . . . . . . . . . . . . . . . . . . . . . . . 55
4.1.2
BP Performance Guarantee . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58
4.2
The General Case . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 64
4.2.1
OMP Performance Guarantee . . . . . . . . . . . . . . . . . . . . . . . . . . 65
4.2.2
Thresholding Performance Guarantee . . . . . . . . . . . . . . . . . . . 67
4.2.3
BP Performance Guarantee . . . . . . . . . . . . . . . . . . . . . . . . . . . . 68
4.2.4
Performance of Pursuit Algorithms – Summary . . . . . . . . . . . 71
4.3
The Role of the Sign-Pattern . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71
4.4
Tropp’s Exact Recovery Condition . . . . . . . . . . . . . . . . . . . . . . . . . . . . 73
4.5
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76
Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 76
5
From Exact to Approximate Solutions . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79
5.1
General Motivation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79
5.2
Stability of the Sparsest Solution . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 80
5.2.1
Uniqueness versus Stability – Gaining Intuition . . . . . . . . . . . 80
5.2.2
Theoretical Study of the Stability of (Pϵ
0) . . . . . . . . . . . . . . . . 82
5.2.3
The RIP and Its Use for Stability Analysis . . . . . . . . . . . . . . . 86
5.3
Pursuit Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89
5.3.1
OMP and BP Extensions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89
5.3.2
Iteratively-Reweighed-Least-Squares (IRLS) . . . . . . . . . . . . . 91
5.3.3
The LARS Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 95
5.3.4
Quality of Approximations Obtained . . . . . . . . . . . . . . . . . . . . 98
5.4
The Unitary Case. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
5.5
Performance of Pursuit Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103
5.5.1
BPDN Stability Guarantee . . . . . . . . . . . . . . . . . . . . . . . . . . . . 103
5.5.2
Thresholding Stability Guarantee . . . . . . . . . . . . . . . . . . . . . . . 104

Contents
xvii
5.6
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107
Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108
6
Iterative-Shrinkage Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111
6.1
Background . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 111
6.2
The Unitary Case - A Source of Inspiration . . . . . . . . . . . . . . . . . . . . . 112
6.2.1
Shrinkage For the Unitary case . . . . . . . . . . . . . . . . . . . . . . . . . 112
6.2.2
The BCR Algorithm and Variations . . . . . . . . . . . . . . . . . . . . . 113
6.3
Developing Iterative-Shrinkage Algorithms . . . . . . . . . . . . . . . . . . . . . 115
6.3.1
Surrogate Functions and the Prox Method. . . . . . . . . . . . . . . . 115
6.3.2
EM and Bound-Optimization Approaches. . . . . . . . . . . . . . . . 117
6.3.3
An IRLS-Based Shrinkage Algorithm . . . . . . . . . . . . . . . . . . . 119
6.3.4
The Parallel-Coordinate-Descent (PCD) Algorithm . . . . . . . . 120
6.3.5
StOMP: A Variation on Greedy Methods . . . . . . . . . . . . . . . . 123
6.3.6
Bottom Line – Iterative-Shrinkage Algorithms . . . . . . . . . . . . 125
6.4
Acceleration Using Line-Search and SESOP . . . . . . . . . . . . . . . . . . . . 127
6.5
Iterative-Shrinkage Algorithms: Tests . . . . . . . . . . . . . . . . . . . . . . . . . . 127
6.6
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 132
Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 134
7
Towards Average PerformanceAnalysis . . . . . . . . . . . . . . . . . . . . . . . . . . . 137
7.1
Empirical Evidence Revisited . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 137
7.2
A Glimpse into Probabilistic Analysis . . . . . . . . . . . . . . . . . . . . . . . . . 140
7.2.1
The Analysis Goals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 140
7.2.2
Two-Ortho Analysis by Candes & Romberg . . . . . . . . . . . . . . 141
7.2.3
Probabilistic Uniqueness . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143
7.2.4
Donoho’s Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 143
7.2.5
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144
7.3
Average Performance of Thresholding . . . . . . . . . . . . . . . . . . . . . . . . . 144
7.3.1
Preliminaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 144
7.3.2
The Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145
7.3.3
Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148
7.4
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150
Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 150
8
The Dantzig-Selector Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 153
8.1
Dantzig-Selector versus Basis-Pursuit . . . . . . . . . . . . . . . . . . . . . . . . . 153
8.2
The Unitary Case. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 155
8.3
Revisiting the Restricted Isometry Machinery . . . . . . . . . . . . . . . . . . . 156
8.4
Dantzig-Selector Performance Guaranty . . . . . . . . . . . . . . . . . . . . . . . 157
8.5
Dantzig-Selector in Practice . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 163
8.6
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 164
Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 165

xviii
Contents
Part II From Theory to Practice – Signal and Image Processing Applications
9
Sparsity-Seeking Methods in Signal Processing . . . . . . . . . . . . . . . . . . . . 169
9.1
Priors and Transforms for Signals . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 169
9.2
The Sparse-Land Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 172
9.3
Geometric Interpretation of Sparse-Land . . . . . . . . . . . . . . . . . . . . . . . 173
9.4
Processing of Sparsely-Generated Signals . . . . . . . . . . . . . . . . . . . . . . 176
9.5
Analysis Versus Synthesis Signal Modeling. . . . . . . . . . . . . . . . . . . . . 178
9.6
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 180
Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181
10
Image Deblurring – A Case Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 185
10.1 Problem Formulation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 185
10.2 The Dictionary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 186
10.3 Numerical Considerations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 188
10.4 Experiment Details and Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 191
10.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 198
Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 199
11
MAP versus MMSE Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 201
11.1 A Stochastic Model and Estimation Goals . . . . . . . . . . . . . . . . . . . . . . 201
11.2 Background on MAP and MMSE . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 202
11.3 The Oracle Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 204
11.3.1 Developing the Oracle Estimator . . . . . . . . . . . . . . . . . . . . . . . 204
11.3.2 The Oracle Error. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 206
11.4 The MAP Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 208
11.4.1 Developing the MAP Estimator . . . . . . . . . . . . . . . . . . . . . . . . 208
11.4.2 Approximating the MAP Estimator . . . . . . . . . . . . . . . . . . . . . 211
11.5 The MMSE Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 212
11.5.1 Developing the MMSE Estimator . . . . . . . . . . . . . . . . . . . . . . . 212
11.5.2 Approximating the MMSE Estimator . . . . . . . . . . . . . . . . . . . 215
11.6 MMSE and MAP Errors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 218
11.7 More Experimental Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 220
11.8 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 224
Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 224
12
The Quest for a Dictionary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 227
12.1 Choosing versus Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 227
12.2 Dictionary-Learning Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 228
12.2.1 Core Questions in Dictionary-Learning . . . . . . . . . . . . . . . . . . 229
12.2.2 The MOD Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 230
12.2.3 The K-SVD Algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 231
12.3 Training Structured Dictionaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 237
12.3.1 The Double-Sparsity Model . . . . . . . . . . . . . . . . . . . . . . . . . . . 239
12.3.2 Union of Unitary Bases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 241
12.3.3 The Signature Dictionary. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 242

Contents
xix
12.4 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 244
Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 244
13
Image Compression – Facial Images . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 247
13.1 Compression of Facial Images . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 247
13.2 Previous Work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 249
13.3 Sparse-Representation-Based Coding Scheme. . . . . . . . . . . . . . . . . . . 250
13.3.1 The General Scheme . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 251
13.3.2 VQ Versus Sparse Representations. . . . . . . . . . . . . . . . . . . . . . 253
13.4 More Details and Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 254
13.4.1 K-SVD Dictionaries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 255
13.4.2 Reconstructed Images . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 255
13.4.3 Run-Time and Memory Usage . . . . . . . . . . . . . . . . . . . . . . . . . 260
13.4.4 Comparing to Other Techniques . . . . . . . . . . . . . . . . . . . . . . . . 261
13.4.5 Dictionary Redundancy . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 262
13.5 Post-Processing for Deblocking . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 263
13.5.1 The Blockiness Artifacts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 263
13.5.2 Possible Approaches For Deblocking . . . . . . . . . . . . . . . . . . . 265
13.5.3 Learning-Based Deblocking Approach . . . . . . . . . . . . . . . . . . 266
13.6 Deblocking Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 267
13.7 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 268
Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 269
14
Image Denoising . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 273
14.1 General Introduction – Image Denoising . . . . . . . . . . . . . . . . . . . . . . . 273
14.2 The Beginning: Global Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 274
14.2.1 The Core Image-Denoising Algorithm. . . . . . . . . . . . . . . . . . . 274
14.2.2 Various Improvements . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 276
14.3 From Global to Local Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 278
14.3.1 The General Methodology. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 278
14.3.2 Learning the Shrinkage Curves . . . . . . . . . . . . . . . . . . . . . . . . . 279
14.3.3 Learned Dictionary and Globalizing the Prior. . . . . . . . . . . . . 286
14.3.4 The Non-Local-Means Algorithm . . . . . . . . . . . . . . . . . . . . . . 292
14.3.5 3D-DCT Shrinkage: BM3D Denoising . . . . . . . . . . . . . . . . . . 296
14.4 SURE for Automatic Parameter Setting . . . . . . . . . . . . . . . . . . . . . . . . 297
14.4.1 Development of the SURE . . . . . . . . . . . . . . . . . . . . . . . . . . . . 298
14.4.2 Demonstrating SURE to Global-Threhsolding . . . . . . . . . . . . 300
14.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 302
Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 303
15
Other Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 309
15.1 General . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 309
15.2 Image Separation via MCA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 310
15.2.1 Image = Cartoon + Texture . . . . . . . . . . . . . . . . . . . . . . . . . . . . 310
15.2.2 Global MCA for Image Separation. . . . . . . . . . . . . . . . . . . . . . 312

15.2.3 Local MCA for Image Separation . . . . . . . . . . . . . . . . . . . . . . 316
15.3 Image Inpainting and Impulsive Noise Removal . . . . . . . . . . . . . . . . . 324
15.3.1 Inpainting Sparse-Land Signals – Core Principles . . . . . . . . . 324
15.3.2 Inpainting Images – Local K-SVD . . . . . . . . . . . . . . . . . . . . . . 327
15.3.3 Inpainting Images – The Global MCA . . . . . . . . . . . . . . . . . . . 335
15.3.4 Impulse-Noise Filtering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 338
15.4 Image Scale-Up . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 341
15.4.1 Modeling the Problem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 343
15.4.2 The Super-Resolution Algorithm . . . . . . . . . . . . . . . . . . . . . . . 346
15.4.3 Scaling-Up Results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 349
15.4.4 Image Scale-Up: Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . 351
15.5 Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 353
Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 354
16
Epilogue . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 359
16.1 What is it All About? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 359
16.2 What is Still Missing? . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 359
16.3 Bottom Line . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 360
Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 371
Notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Acronyms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
. . . . 363
. 369
Contents
xx

Part I
Sparse and Redundant Representations –
Theoretical and Numerical Foundations

Chapter 1
Prologue
A central achievement of classical linear algebra is a thorough examination of the
problem of solving systems of linear equations. The results – deﬁnite, timeless,
and profound – give the subject a completely settled appearance. As linear systems
of equations are the core engine in many engineering developments and solutions,
much of this knowledge is practically and successfully deployed in applications.
Surprisingly, within this well-understood arena there is an elementary problem that
has to do with sparse solutions of linear systems, which only recently has been ex-
plored in depth; we will see that this problem has surprising answers, and it inspires
numerous practical developments. In this chapter we shall concentrate on deﬁning
this problem carefully, and set the stage for its answers in later chapters.
1.1 Underdetermined Linear Systems
Consider a matrix A ∈IRn×m with n < m, and deﬁne the underdetermined linear
system of equations Ax = b. This system has more unknowns than equations, and
thus it has either no solution, if b is not in the span of the columns of the matrix A,
or inﬁnitely many solutions. In order to avoid the anomaly of having no solution, we
shall hereafter assume that A is a full-rank matrix, implying that its columns span
the entire spaceRn.
In engineering we often encounter problems formulated as such underdetermined
linear systems of equations. As an example from image processing, consider the
image scale-up problem, where an unknown image undergoes a blur and scale-down
operation, and the outcome is given as a lower quality and smaller image b. The
matrix A stands for the degradation operations, and our aim is to reconstruct the
original image x from the given measurements b. Clearly, there are inﬁnitely many
possible images x that may “explain” b, among which there are some that may look
better than others. How can we ﬁnd the proper x?
3
© Springer Science+Business Media, LLC 2010
and Image Processing, DOI 10.1007/978-1-4419-7011-4_1,
M. Elad, Sparse and Redundant Representations: From Theory to Applications in Signal 

4
1 Prologue
1.2 Regularization
Both in the above example and in many other problems that assume the same for-
mulation, we desire a single solution, and the fact that there are inﬁnitely many of
those stands as a major obstacle. In order to narrow this choice to one well-deﬁned
solution, additional criteria are needed. A familiar way to do this is regularization,
where a function J(x) that evaluates the desirability of a would-be solution x is
introduced, with smaller values being preferred. Deﬁning the general optimization
problem (PJ)
(PJ) :
min
x
J(x) subject to b = Ax,
(1.1)
it is now in the hands of J(x) to govern the kind of solution(s) we may obtain.
Returning to the image scale-up example, a function J(x) that prefers smooth or,
better yet, piecewise smooth results, is typically used.
The most known choice of J(x) is the squared Euclidean norm ∥x∥2
2. The problem
(P2) that results from such a choice has in-fact a unique solution ˆx – the so called
minimum-norm solution. Using Lagrange multipliers, we deﬁne the Lagrangian
L(x) = ∥x∥2
2 + λT(Ax −b),
(1.2)
with λ being the Lagrange multipliers for the constraint set. Taking a derivative of
L(x) with respect to x, we obtain the requirement
∂L(x)
∂x
= 2x + ATλ.
(1.3)
Thus the solution is obtained as1
ˆxopt = −1
2ATλ.
(1.4)
Plugging this solution into the constraint Ax = b leads to
Aˆxopt = −1
2AATλ = b
⇒
λ = −2(AAT)−1b.
(1.5)
Assigning this to Equation (1.4) gives the well-known closed-form pseudo-inverse
solution
ˆxopt = −1
2ATλ = AT(AAT)−1b = A+b.
(1.6)
Note that since we have assumed that A is full-rank, the matrix AAT is positive-
deﬁnite and thus invertible. We also note that for a more general choice of the form
J(x) = ∥Bx∥2
2 (such that BTB is invertible), a closed-form expression of the solution
1 Here and elsewhere in this book we use matrix calculus formulae.

1.3 The Temptation of Convexity
5
also exists,
ˆxopt = (BTB)−1AT 
A(BTB)−1AT−1 b,
(1.7)
following the same line of steps.
The use of ℓ2 is widespread in various ﬁelds of engineering, and this is mainly
due to its simplicity as manifested by the above closed-form and unique solution. In
signal and image processing, this regularization has been practiced quite often for
various inverse problems, representation of signals, and elsewhere. This, however,
is by no means a declaration that ℓ2 is the truly best choice for the various problems
it has been brought to serve. In many cases, the mathematical simplicity of ℓ2 is a
misleading fact that diverts engineers from making better choices for J(·). Indeed,
in image processing, after three decades of trying to successfully activate the (ℓ2-
norm based) Wiener ﬁlter, it was eventually realized that the solution resides with
a diﬀerent, robust-statistics based choice of J(·), of the kind we shall often meet in
this book.
1.3 The Temptation of Convexity
The fact that ℓ2 leads to a unique solution is a manifestation of a wider phenomenon,
as selecting any strictly convex function J(·) guarantees such uniqueness. Let us
recall the deﬁnition of convexity, ﬁrst for sets, and then for functions.
Deﬁnition 1.1. A set Ωis convex if ∀x1, x2 ∈Ωand ∀t ∈[0, 1], the convex combi-
nation x = tx1 + (1 −t)x2 is also in Ω.
It is easy to verify that the set Ω= {x| Ax = b} is convex, using the above
deﬁnition. Thus, the feasible set of solutions of the optimization problem posed in
Equation (1.1) is convex. In order for this optimization problem to be convex as
a whole, we have to add a convexity requirement on the penalty J(x). The core
deﬁnition for such a property is given by:
Deﬁnition 1.2. A function J(x) : Ω→IR is convex if ∀x1, x2 ∈Ωand ∀t ∈[0, 1],
the convex combination point x = tx1 + (1 −t)x2 satisﬁes
J(tx1 + (1 −t)x2) ≤tJ(x1) + (1 −t)J(x2).
(1.8)
Another way to understand the above deﬁnition is this: the epigraph of J(x) (the
area deﬁned by {(x, y) | y ≥J(x)}) is a convex set in IRm+1.
If J(·) is twice continuously diﬀerentiable, its derivatives can be used for alterna-
tive deﬁnitions of convexity:
Deﬁnition 1.3. A function J(x) : Ω→IR is convex if ∀x1, x2 ∈Ωif and only if
J(x2) ≥J(x1) + ∇J(x1)T(x2 −x1),
(1.9)

6
1 Prologue
or alternatively, if and only if ∇2J(x1) is positive-deﬁnite.
Using the deﬁnition of the positive-deﬁniteness of the Hessian makes the proof
of the convexity of the squared ℓ2-norm trivial, since ∇2∥x∥2
2 = 2I ⪰0. Indeed, the
squared ℓ2-norm is strictly convex since this Hessian is strictly positive-deﬁnite for
all x. Returning to the problem posed in Equation (1.2), since the constraint-set is
convex and the penalty is strictly so, a unique solution is guaranteed.
So far we have dwelled with the choice J(x) = ∥x∥2
2. However, there are many
other choices of functions J(·) that are convex or strictly so. Even though these
options of J(·) rarely lead to a closed-form solution, the fact that they are strictly
convex implies a unique solution.2 Perhaps more importantly, carefully chosen op-
timization algorithms for solving (1.2) are guaranteed to converge to the global min-
imizer of such optimization problems.
These properties make convex problems very appealing in engineering in gen-
eral, and in signal and image processing in particular. Again, this does not mean
that there is no value to non-convex problems – it simply says that when dealing
with non-convex optimization tasks, one should be aware of problems that might be
encountered because of their imperfection.
Special cases of interest for convex functions are all the ℓp-norms for p ≥1 (use
the Hessian to verify that). These are deﬁned as
∥x∥p
p =
X
i
|xi|p.
(1.10)
In particular, the ℓ∞-norm (obviously, without the p-th power3) and the ℓ1-norm are
very interesting and popular; the ℓ∞considers the maximal entry of the vector x,
and ℓ1 sums the absolute entries. We shall have a special interest in ℓ1 due to its
tendency to sparsify the solution – a fact that will be discussed and established as
we move forward in this book.
1.4 A Closer Look at ℓ1 Minimization
The choice J(x) = ∥x∥1 is convex but not strictly so, a fact easily veriﬁed by noticing
that if x1 and x2 are in the same quadrant (i.e., they have the same sign-pattern), then
their convex combination gives an equality in Equation (1.8). Thus, the problem
(P1) :
min
x
∥x∥1 subject to b = Ax
(1.11)
may have more than one solution. Nevertheless, even if there are inﬁnitely many
possible solutions to this problem, we can claim that (i) these solutions are gathered
2 For non-strict convexity, uniqueness is not guaranteed.
3 The ℓp-norm with 1 ≤p < ∞without the p-th power is a convex function but not strictly so. This
is because these functions behave like a cone with a linear slope, implying that a carefully chosen
pair of points (two scaled points) can give exact equality in Equation (1.8).

1.4 A Closer Look at ℓ1 Minimization
7
in a set that is bounded and convex, and (ii) among these solutions, there exists at
least one with at most n non-zeros (as the number of constraints).
The convexity of the solution set (in the ﬁrst property) is immediate, stemming
as a direct consequence of the fact that all optimal solutions have the same penalty
(ℓ1-length in this case); any convex combination of two such solutions must give a
lower or equal penalty due to the convexity of J(x) = ∥x∥1, and since the two chosen
solutions are optimal, a lower penalty is impossible to obtain. Thus, any convex
combination of two optimal solutions is also an optimal solution.
The fact that the solution set is bounded is a direct consequence of the fact that
all optimal solutions give a penalty of the same height, vmin = ∥xopt∥1 < ∞. Given
any two optimal solutions, x1
opt and x2
opt, the distance between them satisﬁes
∥x1
opt −x2
opt∥1 ≤∥x1
opt∥1 + ∥x2
opt∥1 = 2vmin,
implying that all solutions are nearby.
In order to show the second property, let us assume that an optimal solution xopt
has been found for (P1), with k > n non-zeros. Clearly, the k columns combined
linearly by xopt are linearly-dependent, and thus there exists a non-trivial vector h
that combines these columns to zero (i.e., the support of h is contained within the
support of xopt), Ah = 0.
We consider the vector x = xopt+ϵh, for very small values of ϵ that guarantee that
no entry in the new vector changes its sign. Any value satisfying |ϵ| ≤mini |xi
opt|/|hi|
is suitable. First, it is clear that this vector is guaranteed to satisfy the linear con-
straint Ax = 0, and as such, it is a feasible solution to (P1). Furthermore, since xopt
is assumed to be optimal, we must assume that
∀|ϵ| ≤min
i
|xi
opt|
|hi| , ∥x∥1 = ∥xopt + ϵh∥1 ≥∥xopt∥1.
We argue that the above inequality is in fact an equality. The above relationship
applies to both positive and negative values of ϵ in a region where the ℓ1 function is
continuous and diﬀerentiable (because all the vectors xopt + ϵh have the same sign-
pattern). Thus, as claimed, the only way this could be true is if the above inequality
is satisﬁed as an equality. This implies that the addition/substraction of h under those
circumstances does not change the ℓ1-length of the solution. As an example, if xopt
has only positive entries, then the entries of h should be both positive and negative
and sum to zero. More generally, we should require that hTsign(xopt) = 0.
Our next step is to tune ϵ in such a way that one entry in xopt is nulled, while
keeping the solution’s ℓ1 length. We choose the index i that gives the minimum ratio
|xi
opt|/|hi|, and we pick ϵ = −xi
opt/hi. In the resulting vector xopt + ϵh, the i-th entry
is nulled, while all the others keep their sign, and we also have that ∥xopt + ϵh∥1 =
∥xopt∥1. This way we got a new optimal solution with k −1 non-zeros at the most
(because it may be that more than one entry have been simultaneously nulled). This
process can be repeated until k = n. Below this, it is possible to null entries only if
the columns in A are linearly-dependent, and this would rarely be the case.

8
1 Prologue
From the above we have learned that the ℓ1-norm has a tendency to prefer sparse
solutions. The property we have proven is well known and considered as the fun-
damental property of linear programming — a tendency towards basic (sparse) so-
lutions. However, as we shall see later on, while we are very much interested in
sparsity, getting n non-zeros would be considered as far too dense for our needs,
and deeper sparsity would be sought.
1.5 Conversion of (P1) to Linear Programming
Suppose that in (P1), the unknown x is replaced by x = u −v, where u, v ∈IRn are
both non-negative vectors such that u takes all the positive entries in x, other entries
null, and v does the same for the negative entries in x. With this replacement, and
by denoting the concatenated vector z = [uT, vT]T ∈IR2n, it is easy to see that4
∥x∥1 = 1T(u + v) = 1Tz and Ax = A(u −v) = [A, −A]z. Thus, the (P1) optimization
problem posed in Equation (1.11) can be re-written as
min
z
1Tz subject to b = [A, −A]z and z ≥0.
(1.12)
In this way, rather than minimize an ℓ1-norm problem, we face a new problem hav-
ing a classical Linear-Programming (LP) structure. For the two problems ((P1) and
the LP) to be equivalent, we must show that our assumption about the decomposi-
tion of x to positive and negative entries is satisﬁed, and solutions to (1.12) cannot
be such that uTv , 0 (i.e., the supports of u and v overlap).
This is easily veriﬁed by observing that if in a given optimal solution of (1.12) the
k-th entry in both u and v is non-zero (and positive, due to the last constraint), then
these two coeﬃcients multiply the same column in A with opposing signs. Without
loss of generality, if we assume uk > vk, then by replacing these two entries by
u′
k = uk −vk and v′
k = 0 we satisfy both the positivity and the linear constraints while
also reducing the penalty by uk −vk > 0, contradicting the optimality of the initial
solution. Thus, the supports of u, v do not overlap, and LP is indeed equivalent to
(P1).
1.6 Promoting Sparse Solutions
As we move from ℓ2 regularization towards ℓ1, we promote sparser solutions. Using
this rationale, we can consider ℓp-“norms” with p < 1. One has to be careful as
such ℓp are no longer formal norms for p < 1, as the triangle inequality is no longer
4 The notation 1 stands for a vector of ones of the proper length.

1.6 Promoting Sparse Solutions
9
satisﬁed.5 Nevertheless, we shall use the term norm for these functions as well,
keeping in mind this reservation.
Do these norms lead to sparser solutions? In order to get a feeling for the behavior
of these norms, we consider the following problem: Assume that x is known to have
unit length in ℓp. Let us ﬁnd the shortest such vector in ℓq for q < p. Putting this as
an optimization problem, this reads
min
x
∥x∥q
q subject to ∥x∥p
p = 1.
(1.13)
We shall assume that x has a non-zeros (we shall further assume that these values are
all positive, since their sign does not aﬀect the analysis that follows) as the leading
entries in the vector, and the rest are zeros. We deﬁne the Lagrangian function as
L(x) = ∥x∥q
q + λ(∥x∥p
p −1) = −λ +
a
X
k=1
(|xk|q + λ|xk|p) .
(1.14)
This function is separable, handling every entry in x independently of the others.
The optimal solution is given by xp−q
k
= Const for all k, implying that all the non-
zero entries are supposed to be the same. The constraint ∥x∥p
p = 1 leads to xk = a−1/p,
and the ℓq-norm for this solution is given by ∥x∥q
q = a1−q/p. Since q < p, this means
that the shortest ℓq-norm is obtained for a = 1, having only one non-zero entry in x.
The above result states that for any pair of ℓp- and ℓq-norms with q < p, a unit-
length ℓp-norm vector becomes the shortest in ℓq when it is the sparsest possible.
A geometrical interpretation of the analysis given above is the following: The unit
ℓp-ball surface in IRm represents the feasible set of the problem posed in (1.13). We
blow an ℓq “balloon” in the same space and search for the ﬁrst intersection of it with
the ℓp-ball. The result we got implies that this intersection takes place along the
axes, where all the entries apart from one are zeros. This is demonstrated in Figure
1.1.
Another way to illustrate the tendency of ℓp-norm to drive results to become
sparse is the following: Consider the problem we have been looking at,
(Pp) :
min ∥x∥p
p subject to Ax = b.
(1.15)
The linear set of equations forming the constraint deﬁne a feasible set of solutions
that are on an aﬃne subspace (a subspace shifted by a constant vector). This shift
can be any possible solution of this system, x0. A linear combination of x0 and any
vector from the null-space of A forms a feasible solution as well. Geometrically,
this set appears as a hyperplane of dimension IRm−n embedded in the IRm space.
It is within this space that we seek the solution to the problem posed as (Pp).
Geometrically speaking, solving (Pp) is done by “blowing” again an ℓp balloon
5 The properties a norm should satisfy are: (i) zero vector: ∥v∥= 0 if and only if v = 0; (ii) absolute
homogeneity: ∀t , 0, ∥tu∥= |t|∥u∥, and (iii) the triangle inequality: ∥u + v∥≤∥u∥+ ∥v∥. Note that
from properties (ii) and (iii) it is implied that a norm is a convex function because ∥tu + (1 −t)v∥≤
t∥u∥+ (1 −t)∥v∥for all t ∈[0, 1], implying convexity.

10
1 Prologue
−1.5
−1
−0.5
0
0.5
1
1.5
−1.5
−1
−0.5
0
0.5
1
1.5
−1.5
−1
−0.5
0
0.5
1
1.5
−1.5
−1
−0.5
0
0.5
1
1.5
Fig. 1.1 Demonstrating the fact that a unit-length ℓp-norm vector (dashed) becomes shortest in ℓq
(p > q) (solid) when it is the sparsest possible. On the left this is demonstrated for p = 2 and
q = 1, and the right shows it for p = 1 and q = 0.5. The dash-dotted curves in both graphs show
the reverse claim – maximizing the ℓq length leads to the most non-sparse outcome.
centered around the origin, and stopping its inﬂation when it ﬁrst touches the fea-
sible set. The question is what characterizes such an intersection point? Figure 1.2
presents a simple demonstration of this process for a tilted hyperplane (serving as
the constraint-set) and several p values: 2, 1.5, 1, and 0.7. One can see that norms
with p ≤1 tend to give that the intersection point is on the ball-corners, which take
place on the axes. This implies that 2 of the 3 coordinates are zeros, which is the
tendency to sparsity we are referring to. As opposed to this, ℓ2 and even ℓ1.5 give
intersection points which are not sparse, with three non-zero coordinates.
Put more generally, an intersection of an aﬃne subspace and an ℓp ball is ex-
pected to take place on the axes for p ≤1, and thus lead to a sparse solution. Even
the ℓ1-ball enjoys this property, and in fact, one has to be highly unlucky with the
angle of the aﬃne subspace to avoid a sparse outcome.
Another similar measure promoting sparsity is the weak ℓp-norm. Denoting by
N(ϵ, x) the number of entries in x exceeding ϵ, we deﬁne the weak ℓp-norm as
∥x∥p
wℓp = sup
ϵ>0
N(ϵ, x) · ϵ p.
(1.16)
Here, just as before, 0 < p ≤1 is the interesting range of p, giving very powerful
sparsity measure. The weak ℓp norm is a popular measure of sparsity used by the
mathematical analysis community. The usual ℓp-norm is almost equivalent, and in
fact, the two become the same for a vector with uniform spread of its energy among
its non-zero entries. The plain ℓp is easier to handle and thus more popular in most
cases.
It would seem very natural, based on our discussion above, to attempt to solve a
problem of the form
(Pp) :
min ∥x∥p
p subject to Ax = b,
(1.17)

1.6 Promoting Sparse Solutions
11
Fig. 1.2 The intersection between the ℓp-ball and the set Ax = b deﬁnes the solution of (Pp). This
intersection is demonstrated here in 3D for p = 2 (top left), p = 1.5 (top right), p = 1 (bottom
left), and p = 0.7 (bottom right). When p ≤1, the intersection takes place at a corner of the ball,
leading to a sparse solution.
for example, with p = 2/3, p = 1/2, or even smaller p, tending to zero. Unfortu-
nately, each choice 0 < p < 1 leads to a non-convex optimization problem, and this
raises some diﬃculties, as we have mentioned above. Nevertheless, from an engi-
neering point of view, if sparsity is a desired property, and we know that ℓp serves it
well, this problem can and should be used, despite its weaknesses.
While all the discussion above focuses on the ℓp-norm, there are other functions
of x that promote sparsity just as well. In fact, any function J(x) = P
i ρ(xi) with
ρ(x) being symmetric, monotonically non-decreasing, and with a monotonic non-
increasing derivative for x ≥0 will serve the same purpose of promoting sparsity.
As classic examples of this family, we mention ρ(x) = 1−exp(|x|), ρ(x) = log(1+|x|),
and ρ(x) = |x|/(1 + |x|).

12
1 Prologue
−1.5
−1
−0.5
0
0.5
1
1.5
0
0.5
1
1.5
2
p=2
p=1
p=0.5
p=0.1
Fig. 1.3 The behavior of |x|p for various values of p. As p tends to zero, |x|p approaches the
indicator function, which is 0 for x = 0 and 1 elsewhere.
1.7 The ℓ0-Norm and Implications
The extreme among all the sparsifying norms is the case of p →0. We denote the
ℓ0-norm as6
∥x∥0 = lim
p→0 ∥x∥p
p = lim
p→0
m
X
k=1
|xk|p = #{i : xi , 0}.
(1.18)
This is a very simple and intuitive measure of sparsity of a vector x, counting the
number of nonzero entries in it. In order to see this counting behavior, Figure 1.3
presents the scalar weight function |x|p – the core of the norm computation – for
various values of p. It shows that as p goes to zero, this curve becomes an indicator
function, being 0 for x = 0 and 1 for every other value. Thus, summation of such
measures on all the entries of x becomes a count of the non-zeros in this vector, as
claimed.
The term ℓ0-norm is misleading, as this function does not satisfy all the axiomatic
requirements of a norm. More speciﬁcally, if we consider the ℓ0 as a continuation
of the ℓp-norm for p →0, then for checking its behavior we need to take its 0-
th root,7 and this is impossible. Alternatively, we can simply refer to the function
∥x∥0 as a candidate function for a norm. While this function satisﬁes the triangle
inequality, ∥u + v∥0 ≤∥u∥0 + ∥v∥0, the homogeneity property is not met: for t , 0,
∥tu∥0 = ∥u∥0 , t∥u∥0. This loss of sensitivity to scale will return to haunt us from
time to time.
6 A more accurate notation would be ∥x∥0
0, but we shall stick to the one described above, so as to
be consistent with the literature.
7 The homogeneity and the triangle inequality properties are both tested with the p-th root of the
function described in Equation (1.10).

1.8 The (P0) Problem – Our Main Interest
13
We should note that the ℓ0 norm, while providing a very simple and easily
grasped notion of sparsity, is not necessarily the right notion for empirical work.
A vector of real data would rarely be representable by a vector of coeﬃcients con-
taining many zeros. A more relaxed and forgiving notion of sparsity can and should
be built on the notion of approximately representing a vector using a small number
of nonzeros; this can be quantiﬁed by the weak-ℓp and the usual ℓp-norms described
above. Nevertheless, we proceed the discussion here with the assumption that ℓ0 is
the measure of interest.
1.8 The (P0) Problem – Our Main Interest
Consider the problem (P0) obtained from the general prescription (PJ) with the
choice J(x) = J0(x) ≡∥x∥0; explicitly:
(P0) :
min
x
∥x∥0 subject to Ax = b.
(1.19)
Sparsity optimization (1.19) looks superﬁcially like the minimum ℓ2-norm problem
(P2), but the notational similarity masks some startling diﬀerences. The solution
to (P2) is always unique, and is readily available through now-standard tools from
computational linear algebra. (P0) has probably been considered as a possible goal
from time to time for many years, but it seems initially to pose many conceptual
challenges that have inhibited its widespread study and application. These are rooted
in the discrete and discontinuous nature of the ℓ0 norm; the standard convex analysis
ideas which underpin the analysis of (P2) do not apply. Many of the most basic
questions about (P0) seem immune to immediate insight:
• Can uniqueness of a solution be claimed? Under what conditions?
• If a candidate solution is available, can we perform a simple test to verify that
the solution is actually the global minimizer of (P0)?
Perhaps in some instances, with very special matrices A and left hand sides b, ways
to answer such questions are apparent, but for general classes of problem instances
(A, b), such insights initially seem unlikely.
Beyond conceptual issues of uniqueness and veriﬁcation of solutions, one is eas-
ily overwhelmed by the apparent diﬃculty of solving (P0). This is a classical prob-
lem of combinatorial search; one sweeps exhaustively through all possible sparse
subsets, generating corresponding subsystems b = AS xS where AS denotes the ma-
trix having |S | columns chosen from those columns of A with indices in S ; and
checking if b = AS xS can be solved.
Let us illustrate this complexity by the following simple example: Assume that
A is of size 500 × 2000 (n = 500, m = 2000), and suppose that we know that the
sparsest solution to (P0) has |S | = 20 non-zeros. We desire to ﬁnd the proper set of
|S | columns. Thus, we exhaustively sweep through all
 m
|S |

≈3.9E+47 such options,
and per each test the small linear system b = AS xS for equality. Assume that each

14
1 Prologue
individual test of such a system requires 1E−9 seconds. A simple calculation reveals
that it will take more than 1.2E + 31 years(!!!) to conclude these series of tests.
The complexity of exhaustive search is exponential in m, and indeed, it has been
proven that (P0) is, in general, NP-Hard. Thus, a mandatory and crucial set of ques-
tions arise: Can (P0) be eﬃciently solved by some other means? Can approximate
solutions be accepted? How accurate can those be? What kind of approximations
will work? These are the questions we aim to answer in this book.
1.9 The Signal Processing Perspective
We know today that ﬁnding sparse solutions to underdetermined linear systems may
become better-behaved and may be a much more practically relevant notion than we
might have supposed just a few years ago. In parallel with this development, another
insight has been developing in signal and image processing, where it has been found
that many media types (still imagery, video, acoustic) can be sparsely represented
using transform-domain methods, and in fact many important tasks dealing with
such media can fruitfully be viewed as ﬁnding sparse solutions to underdetermined
systems of linear equations. Many readers will be familiar with the media encoding
standard JPEG and its successor, JPEG2000. Both standards are based on the notion
of transform encoding that leads to a sparse representation.
We can thus make a simple connection between the above algebraic discussion
and the signal representation problem formal as follows. Let b denote the vector
of signal/image values to be represented, let A be the matrix whose columns are
the elements of the diﬀerent bases to be used in the representation. The problem
we have posed in Equation (1.1) aims to provide a single representation among the
many possible ones for b. When using the ℓ2-norm measure, the outcome is a linear
operator A+ that multiplies b in order to compute x – this is the well-known Frame
approach towards redundant representations, which applies linear operations both
for the forward transform (from b to x) and its inverse (from x to b).
On the other extreme, the problem (P0) posed in Equation (1.19) oﬀers literally
the sparsest representation of the signal content. This way, while the inverse trans-
form is linear, the forward one is highly non-linear, and quite complex in general.
The appeal in such a transform is in the compact representation it provides. This op-
tion motivates a closer study of the mathematical problems mentioned above. Much
more on this connection will be given in the second part of this book, where we
explore ways to harness the understanding of sparse solutions of linear systems of
equations to applications in signal and image processing.
Further Reading
1. D.P. Bertsekas, Nonlinear Programming, 2nd Edition, Athena Scientiﬁc, 2004.

Further Reading
15
2. S. Boyd and L. Vandenberghe, Convex Optimization, Cambridge University
Press, 2004.
3. A.M. Bruckstein, D.L. Donoho, and M. Elad, From sparse solutions of systems
of equations to sparse modeling of signals and images, SIAM Review, 51(1):34–
81, February 2009.
4. S.S. Chen, D.L. Donoho, and M.A. Saunders, Atomic decomposition by basis
pursuit, SIAM Journal on Scientiﬁc Computing, 20(1):33–61 (1998).
5. S.S. Chen, D.L. Donoho, and M.A. Saunders, Atomic decomposition by basis
pursuit, SIAM Review, 43(1):129–159, 2001.
6. C. Daniel and F.S. Wood, Fitting Equations to Data: Computer Analysis of
Multifactor Data, 2nd Edition, John Wiley and Sons, 1980.
7. G. Davis, S. Mallat, and Z. Zhang, Adaptive time-frequency decompositions,
Optical-Engineering, 33(7):2183–91, 1994.
8. G.H. Golub and C.F. Van Loan, Matrix Computations, Johns Hopkins Studies
in Mathematical Sciences, Third edition, 1996.
9. R.A. Horn C.R. Johnson, Matrix Analysis, New York: Cambridge University
Press, 1985.
10. A.K. Jain, Fundamentals of Digital Image Processing, Englewood Cliﬀs, NJ,
Prentice-Hall, 1989.
11. D. Luenberger, Linear and Nonlinear Programming, 2nd Edition, Addison-
Wesley, Inc., Reading, Massachusetts 1984.
12. S. Mallat, A Wavelet Tour of Signal Processing, Academic-Press, 1998.
13. S. Mallat and E. LePennec, Sparse geometric image representation with ban-
delets, IEEE Trans. on Image Processing, 14(4):423–438, 2005.
14. S. Mallat and Z. Zhang, Matching pursuits with time-frequency dictionaries,
IEEE Trans. Signal Processing, 41(12):3397–3415, 1993.
15. M. Marcus and H. Minc, A Survey of Matrix Theory and Matrix Inequalities,
Prindle, Weber & Schmidt, Dover, 1992.

Chapter 2
Uniqueness and Uncertainty
We return to the basic problem (P0), which is at the core of our discussion,
(P0) :
min
x
∥x∥0 subject to b = Ax.
While we shall refer hereafter to this problem as our main goal, we stress that we
are quite aware of its two major shortcomings in leading to any practical tool:
1. The equality requirement b = Ax is too strict, as there are small chances for any
vector b to be represented by a few columns from A. A better requirement would
be one that allows for small deviation.
2. The sparsity measure is too sensitive to very small entries in x, and a better
measure would adopt a more forgiving approach towards such small entries.
Both these considerations will be included in later analysis, but for this to succeed,
we must start with the stylized version of the problem as indeed posed by (P0).
For the underdetermined linear system of equations, Ax = b (a full-rank matrix
A ∈IRn×m with n < m), we pose the following questions:
Q1: When can uniqueness of the sparsest solution be claimed?
Q2: Can a candidate solution be tested to verify its (global) optimality?
This section addresses these questions, and some of their extensions. Rather than an-
swering the above questions directly, we ﬁrst consider special matrices A for which
the analysis seems to be easier, and then extend our answers to the general A. In do-
ing so, we also follow the path taken by researchers who originally addressed these
questions.
2.1 Treating the Two-Ortho Case
We shall ﬁrst discuss the (P0) problem deﬁned above in a concrete setting: the case
where A is the concatenation of two orthogonal matrices, Ψ and Φ. As a classic
© Springer Science+Business Media, LLC 2010
M. Elad, Sparse and Redundant Representations: From Theory to Applications in Signal 
and Image Processing, DOI 10.1007/978-1-4419-7011-4_2,
17

18
2 Uniqueness and Uncertainty
example, we can consider the amalgam of the identity and the Fourier matrices
A = [I, F]. In such a setting, the fact that the system b = Ax is underdetermined
means, concretely, that there are many ways of representing a given signal b as a
superposition of spikes (i.e., columns from the identity matrix) and sinusoids (i.e.,
columns from the Fourier matrix). A sparse solution of such a system is a repre-
sentation of said signal as a superposition of a few sinusoids and a few spikes. The
uniqueness of such a sparse solution seemed surprising when ﬁrst noticed.
2.1.1 An Uncertainty Principle
Before addressing sparse solutions for the linear system [Ψ, Φ] x = b, we shall con-
sider a (seemingly) diﬀerent problem, inspired by the setting of classical uncertainty
principles. As the reader no doubt knows, the classical uncertainty principle states
that two conjugate variables (e.g., position and momentum, or any other pair cou-
pled by the Fourier transform) cannot both be known with arbitrary precision. Turn-
ing to its mathematical formulation, it states that any function f(x) and its Fourier
transform F(ω) must obey the inequality1
Z ∞
−∞
x2| f(x)|2dx ·
Z ∞
−∞
ω2|F(ω)|2dω ≥1
2,
(2.1)
where we have assumed that these functions are ℓ2-normalized,
Z ∞
−∞
| f(x)|2dx = 1.
This claim says that a signal cannot be tightly concentrated both in time and in
frequency, and there is a lower bound on the product of the spread in time and the
spread in frequency.
A comparable claim in our terminology would be that a signal cannot be sparsely
represented both in time and in frequency. We now turn to develop this exact view-
point, as it will be helpful for understanding some of the discussion that follows.
Suppose we have a non-zero vector b ∈IRn (a signal, say) and two orthobases Ψ
and Φ. Then b can be represented either as a linear combination of columns of Ψ or
as a linear combination of columns of Φ:
b = Ψα = Φβ.
(2.2)
Clearly, α and β are uniquely deﬁned. In a particularly important case, Ψ is simply
the identity matrix, and Φ is the matrix of the Fourier transform. Then α is the
time-domain representation of b while β is the frequency-domain representation.
For an arbitrary pair of bases Ψ, Φ, an interesting phenomenon occurs: either
α can be sparse, or β can be sparse, but not both! However, this claim is clearly
1 The bound 0.5 stated here depends on a speciﬁc deﬁnition of the Fourier transform.

2.1 Treating the Two-Ortho Case
19
dependent on the distance between Ψ and Φ, since if the two are the same, we
can easily construct b to be one of the columns in Ψ and get the smallest possible
cardinality (being 1) in both α and β. Thus, we turn now to deﬁne the proximity
between two bases by their mutual-coherence.
Deﬁnition 2.1. For an arbitrary pair of orthogonal bases Ψ, Φ that construct the ma-
trix A, we deﬁne the mutual-coherence µ(A) as the maximal inner product between
columns from these two bases,
proximity(Ψ, Φ) = µ(A) = max
1≤i,j≤n |ψT
i φj|.
(2.3)
The mutual-coherence of such two-ortho matrices satisﬁes 1/ √n ≤µ(A) ≤1,
where the lower bound is achievable for certain pairs of orthogonal bases, such as
the identity and the Fourier, the identity and the Hadamard, and more. To see that
this is indeed the lower bound on the possible coherence, one simply notices that
ΨTΦ is an orthonormal matrix, having the sum of squares of its entries in each
column equal to 1. All entries cannot therefore be less than 1/ √n since then we
would have that the sum of all squared entries is less than 1. Using Deﬁnition 2.3
above, we have the following inequality result:
µ(A), and for an arbitrary non-zero vector b ∈IRn with representations α and β cor-
respondingly, the following inequality holds true:
Uncertainty Principle 1:
∥α∥0 + ∥β∥0 ≥
2
µ(A).
(2.4)
Proof: We shall assume hereafter, without loss of generality, that ∥b∥2 = 1. Since
b = Ψα = Φβ, and bTb = 1, we have that
1 = bTb
(2.5)
= αTΨTΦβ
=
n
X
i=1
n
X
j=1
αiβ jψT
i φj ≤µ(A) ·
n
X
i=1
n
X
j=1
|αi| · |β j|.
Here we have exploited the deﬁnition of the coherence between the two bases. This
inequality leads to
1 ≤µ(A) ·
n
X
i=1
n
X
j=1
|αi| · |βj| = µ(A) · ∥α∥1∥β∥1.
(2.6)
This could be interpreted as yet another uncertainty principle for the ℓ1-norm case,
suggesting that two representations cannot be jointly ℓ1-short. Indeed, using the
relation between the geometric and the algebraic means (∀a, b ≥0,
√
ab ≤(a +
b)/2), we have
Theorem 2.1. For an arbitrary pair of orthogonal bases Ψ, Φ with mutual-coherence

20
2 Uniqueness and Uncertainty
∥α∥1∥β∥1 ≥
1
µ(A)
⇒
∥α∥1 + ∥β∥1 ≥
2
p
µ(A)
.
(2.7)
However, this is a distraction from our goal, and we return to an ℓ0-based uncertainty
principle.
Let us consider the following problem: Among all possible representations α that
satisfy ∥α∥2 = 1 and have A non-zeros (i.e., ∥α∥0 = A), what would be the one giving
the longest ℓ1 length? This deﬁnes an optimization problem of the form
max
α
∥α∥1 subject to ∥α∥2
2 = 1 and ∥α∥0 = A.
(2.8)
Suppose that this problem leads to a solution g(A) = g(∥α∥0). Similarly, a paral-
lel deﬁnition for β with B non-zeros gives a result g(∥β∥0). This means that using
Equation (2.6) we have an inequality of the form
1
µ(A) ≤∥α∥1∥β∥1 ≤g(∥α∥0) · g(∥β∥0),
(2.9)
since each ℓ1 length is replaced with its upper-bound. Such an inequality is our
target, and thus solution of the problem posed in Equation (2.8) is needed.
We can assume, without loss of generality, that the A non-zeros in α are its ﬁrst
entries, the rest being zeros. We further assume that all of these entries are strictly
positive (since only absolute values are used in this problem). Using Lagrange mul-
tipliers, the ℓ0 constraint vanishes, and we obtain
L(α) =
A
X
i=1
αi + λ
1 −
A
X
i=1
α2
i
.
(2.10)
The derivative of this Lagrangian is
∂L(α)
∂αi
= 1 −2λαi = 0,
(2.11)
implying that the optimal values are given by αi = 1/2λ, and are all the same. This
means that the optimal value (due to the ℓ2 constraint) is αi = 1/
√
A, and thus
g(A) = A/
√
A =
√
A is the value of the maximal ℓ1-norm of the vector α. Using this
and a parallel result for β, plugged into (2.9) leads to
1
µ(A) ≤∥α∥1∥β∥1 ≤g(∥α∥0) · g(∥β∥0) =
p
∥α∥0 · ∥β∥0.
(2.12)
Using again the geometric-algebraic mean relationship we obtain
1
µ(A) ≤
p
∥α∥0 · ∥β∥0 ≤1
2 (|α∥0 + ∥β∥0) ,
(2.13)
as claimed.
□

2.1 Treating the Two-Ortho Case
21
An alternative and simpler proof (by Allan Pinkus) is the following: Since Φ and
Ψ are unitary matrices, we have that ∥b∥2 = ∥α∥2 = ∥β∥2. Let us denote the support
of α by I. From b = Ψα = P
i∈I αiψi we have
|βj|2 = |bTφj|2 =

X
i∈I
αiψT
i φj

2
(2.14)
≤∥α∥2
2 ·

X
i∈I
(ψT
i φj)2

≤∥b∥2
2 · |I| · µ(A)2,
where we have used the Cauchy-Schwartz inequality,2 and the deﬁnition of the
mutual-coherence. Summing the above over all j ∈J, the support of β, we obtain
X
j∈J
|βj|2 = ∥b∥2
2 ≤∥b∥2
2 · |I| · |J| · µ(A)2.
(2.15)
This leads to the inequality we have posed in Equation (2.13), thus proving the
theorem.
□
This result suggests that if the mutual-coherence of two bases is small, then α and
β cannot both be very sparse. For example, if, as above, Ψ is the identity and Φ is the
Fourier matrix, then µ([Ψ, Φ]) = 1/ √n. It follows that a signal cannot have fewer
than 2 √n nonzeros in both the time and frequency-domains. In such a case, we also
know that this is a tight relationship, since the picket-fence signal with a uniform
spacing of √n (assuming it is an integer) is converted by the Fourier transform (due
to Poisson formula) to the same signal, thus accumulating 2 √n non-zeros. Figure
2.1 shows this signal.
Heisenberg’s classical uncertainty principle, in the discrete setting, says that, if
we view α and β as probability distributions (by taking the absolute value of the
entries and normalizing) then the product of their variances satisﬁes σ2
ασ2
β ≥const.
In contrast, (2.4) gives a lower bound on the sum of the nonzeros, regardless of their
locations.
2.1.2 Uncertainty of Redundant Solutions
We now make a connection to the uniqueness problem. Consider the problem of
ﬁnding a solution to Ax = [Ψ, Φ]x = b in light of the uncertainty principle (2.4).
Suppose there are two solutions, x1, x2 for the underlying linear system, and that one
is very sparse. We will see that the other cannot be very sparse as well. Necessarily,
the diﬀerence e = x1 −x2 must be in the null-space of A. Partition e into sub-vectors
2 The Cauchy-Schwartz inequality is given by: |xTy|2 ≤∥x∥2
2 ·∥y∥2
2. Equality is obtained if and only
if x and y are linearly-dependent.

22
2 Uniqueness and Uncertainty
0
10
20
30
40
50
60
0
0.2
0.4
0.6
0.8
1
Fig. 2.1 The picket-fence signal for n = 64. It has 8 uniformly spread non-zeros with equal height.
The Discrete-Fourier-Transform (DFT) of this signal looks exactly the same.
eψ and eφ of the ﬁrst n entries and last n entries of e, respectively. We have
ΨeΨ = −ΦeΦ = y , 0.
(2.16)
The vector y is nonzero because e is nonzero, and both Ψ and Φ are nonsingular.
Now invoke (2.4):
∥e∥0 = ∥eΨ∥0 + ∥eΦ∥0 ≥
2
µ(A).
(2.17)
Since e = x1 −x2, we have
(Uncertainty Principle 2) :
∥x1∥0 + ∥x2∥0 ≥∥e∥0 ≥
2
µ(A).
(2.18)
Here we have used the triangle inequality for the ℓ0 norm, ∥x1∥0 + ∥x2∥0 ≥∥x1 −
x2∥0, which is trivially veriﬁed, by counting the non-zeros of the two vectors and
considering no overlap of supports (which leads to equality) and an overlap (which
gives this inequality). To summarize, we have proven the following result:
Theorem 2.2. Any two distinct solutions x1, x2 of the linear system [Ψ, Φ]x = b
cannot both be very sparse, governed by the following uncertainty principle:
(Uncertainty Principle 2) :
∥x1∥0 + ∥x2∥0 ≥
2
µ(A).
We refer to this result as an uncertainty of redundant solutions, as we discuss here
solutions to the underdetermined system.

2.2 Uniqueness Analysis for the General Case
23
2.1.3 From Uncertainty to Uniqueness
A direct consequence of inequality (2.18) is a uniqueness result:
Theorem 2.3. If a candidate solution for [Ψ, Φ]x = b has fewer than 1/µ(A) non-
zeros, then it is necessarily the sparsest one possible, and any other solution must
be “denser.”
This seemingly simple claim is wonderful and powerful. At least for the special
case discussed here, A = [Ψ, Φ], we have a complete answer for the two questions
we have posed at the beginning of this chapter. Namely, we can certainly claim
uniqueness for sparse enough solutions, and once such a suﬃciently sparse solution
is given to us, we can immediately claim its global optimality. Notice that in general
non-convex optimization problems, a given solution can at best be veriﬁed as being
locally optimal, and here we have the ability to verify its optimality globally.
Since the discussion so far concentrated on the two-ortho case, it is now time to
dare and address general matrices A, using similar treatment. Clearly, though, an
uncertainty result of the kind posed in Theorem 1 would be impossible to imitate,
and we will have to bypass it somehow.
2.2 Uniqueness Analysis for the General Case
2.2.1 Uniqueness via the Spark
A key property that is crucial for the study of uniqueness is the spark of the matrix
A, a term coined and deﬁned by Donoho and Elad in 2003. The spark is a way of
characterizing the null-space of a matrix A using the ℓ0-norm. We start with the
following deﬁnition:
Deﬁnition 2.2. : The spark of a given matrix A is the smallest number of columns
from A that are linearly-dependent.
Recall that the rank of a matrix is deﬁned as the largest number of columns from
A that are linearly independent. Clearly, there is a resemblance between these two
deﬁnitions – replace largest with smallest, and independent by dependent, and you
return to the deﬁnition of the spark. Nevertheless, the spark of a matrix is far more
diﬃcult to obtain, compared to its rank, as it calls for a combinatorial search over
all possible subsets of columns from A.
The importance of this property of matrices for the study of the uniqueness of
sparse solutions has been unraveled already by Rao and Gorodnitsky in 1998. In-
terestingly, this property has previously appeared in the literature of psychometrics
(termed Kruskal rank), used in the context of studying uniqueness of tensor decom-
position. The spark is also related to established notions in matroid theory; formally
it is precisely the girth of the linear matroidmatroid deﬁned by A, i.e., the length of

24
2 Uniqueness and Uncertainty
the shortest cycle in that matroidmatroid. Finally, if we consider the same deﬁnition
where the arithmetic underlying the matrix product is performed not over the ﬁelds
of real or complex numbers but instead over the ring of integers mod q, the same
quantity arises in coding theory, where it allows to compute the minimum distance
of a code. The resemblance between all these concepts is striking and instructive.
The spark gives a simple criterion for uniqueness of sparse solutions. By def-
inition, the vectors in the null-space of the matrix Ax = 0 must satisfy ∥x∥0 ≥
spark(A), since these vectors combine linearly columns from A to give the zero
vector, and at least spark such columns are necessary by deﬁnition. Using the spark
we obtain the following result:
Theorem 2.4. (Uniqueness – Spark): If a system of linear equations Ax = b has
a solution x obeying ∥x∥0 < spark(A)/2, this solution is necessarily the sparsest
possible.
Proof: Consider an alternative solution y that satisﬁes the same linear system Ay =
b. This implies that x −y must be in the null-space of A, i.e., A(x −y) = 0. By
deﬁnition of spark,
∥x∥0 + ∥y∥0 ≥∥x −y∥0 ≥spark(A).
(2.19)
The left-most term in the above inequality simply states that the number of non-
zeros in the diﬀerence vector x −y cannot exceed the sum of the number of non-
zeros within each of the vectors x and y separately – this is the triangle inequality
mentioned before. Since we have a solution satisfying ∥x∥0 < spark(A)/2, we con-
clude that any alternative solution necessarily has more than spark(A)/2 non-zeros,
as claimed.
□
Again, this result is very elementary and yet quite surprising, bearing in mind that
(P0) is a highly complicated optimization task of combinatorial ﬂavor. In general
combinatorial optimization problems, when considering a proposed solution, one
hopes only to check local optimality – i.e., that no simple modiﬁcation gives a better
result. Here, we ﬁnd that simply checking the solution sparsity, and comparing that
with the spark, lets us check global optimality.
Clearly, the value of spark can be very informative, and large values of spark are
evidently very useful. How large can spark be? By deﬁnition, spark must be in the
range3 2 ≤spark(A) ≤n + 1. For example, if A comprises random independent
and identically distributed entries (say Gaussian), then with probability 1 we have
spark(A) = n + 1, implying that no n columns are linearly-dependent. The same
spark is obtained for Vandermonde matrices constructed from distinct m scalars. In
these cases, uniqueness is ensured for every solution with n/2 or fewer non-zero
entries. Similarly, the spark of the two-ortho identity-Fourier pair is4 2 √n, using
3 The spark can be as low as 1 if there exists a zero-column in A, but we do not consider such a
case as relevant to our analysis.
4 If n is prime, the spark of the identity-Fourier pair becomes n + 1, as the picket-fence signals are
no longer relevant.

2.2 Uniqueness Analysis for the General Case
25
Poisson formula – a concatenation of two picket-fence signals, each with √n peaks
evenly spread is the sparsest possible vector in the null-space of the matrix [I, F].
2.2.2 Uniqueness via the Mutual-Coherence
The spark is at least as diﬃcult to evaluate as solving (P0). Thus, simpler ways
to guarantee uniqueness are of interest. A very simple way exploits the mutual-
coherence of the matrix A, deﬁned by generalizing the deﬁnition given for the two-
ortho case. In the two ortho case, computation of the Gram matrix ATA leads to
ATA =
"
I
ΨTΦ
ΦTΨ
I
#
.
(2.20)
Thus, the above-deﬁned mutual-coherence for this case is obtained as the maximal
oﬀ-diagonal entry (in absolute value) in this Gram matrix. Similarly, we propose a
generalization of this deﬁnition, as follows:
Deﬁnition 2.3. : The mutual-coherence of a given matrix A is the largest absolute
normalized inner product between diﬀerent columns from A. Denoting the k-th col-
umn in A by ak, the mutual-coherence is given by
µ(A) =
max
1≤i, j≤m, i, j
aT
i a j

∥ai∥2 ·
aj
2
.
(2.21)
The mutual-coherence is a way to characterize the dependence between columns
of the matrix A. For a unitary matrix, columns are pairwise orthogonal, and so the
mutual-coherence is zero. For general matrices with more columns than rows, m >
n, µ is necessarily strictly positive, and we desire the smallest possible value so as
to get as close as possible to the behavior exhibited by unitary matrices.
We have seen that for structured two-ortho matrices A = [Ψ, Φ] the mutual-
coherence satisﬁes 1/ √n ≤µ(A) ≤1. When considering random orthogonal matri-
ces of size n × m, the work in Donoho and Huo has shown that they tend to be inco-
herent, implying that µ(An,m) is typically proportional to
p
log(nm)/n for n →∞.
It has been shown that for full-rank matrices of size n × m the mutual-coherence is
bounded from below by
µ ≥
r m −n
n(m −1) .
Equality is obtained for a family of matrices named Grassmannian Frames. The
set of columns in such matrices are called equiangular lines. Indeed, this family of
matrices has spark(A) = n + 1, the highest value possible. Numerical construction
of such matrices has been addressed by Tropp et al. using an iterative projection

26
2 Uniqueness and Uncertainty
onto (sometimes not so) convex sets. We will return to this topic towards the end of
this chapter.
We also mention work by Calderbank in quantum information theory, construct-
ing error-correcting-codes using a collection of orthogonal bases with minimal co-
herence, obtaining similar bounds on the mutual-coherence for amalgams of orthog-
onal bases. Also related to this line of activity is the more recent contribution by
Sochen, Gurevitz, and Hadani, on constructions of signal sets such that their shifts
exhibit low coherence.
Mutual-coherence is relatively easy to compute, and as such, it allows us to
lower-bound the spark, which is often hard to compute.
Lemma 2.1. For any matrix A ∈IRn×m, the following relationship holds:
spark(A) ≥1 +
1
µ(A).
(2.22)
Proof: First, modify the matrix A by normalizing its columns to be of unit ℓ2-norm,
obtaining ˜A. This operation preserves both the spark and the mutual-coherence. The
entries of the resulting Gram matrix G = ˜AT ˜A satisfy the following properties:
Gk,k = 1 : 1 ≤k ≤m	 and
n
|Gk, j| ≤µ(A) : 1 ≤k, j ≤m, k , j
o
,
Consider an arbitrary leading minor from G of size p × p, built by choosing a sub-
group of p columns from ˜A and computing their sub-Gram matrix. From the Gersh-
gorin disk theorem,5 if this minor is diagonally-dominant — i.e., if P
j,i |Gi, j| < |Gi,i|
for every i — then this sub-matrix of G is positive-deﬁnite, and so those p columns
from ˜A are linearly-independent. The condition 1 > (p −1)µ →
p < 1 + 1/µ
implies positive-deﬁniteness of every p × p minor. Thus, p = 1 + 1/µ is the small-
est possible number of columns that might lead to linear dependence, and thus
spark(A) ≥1 + 1/µ.
□
We have the following analog of the previous uniqueness theorems, and this time
based on the mutual-coherence.
Theorem 2.5. (Uniqueness – Mutual-Coherence): If a system of linear equations
Ax = b has a solution x obeying ∥x∥0 < 1
2 (1 + 1/µ(A)), this solution is necessarily
the sparsest possible.
Compare Theorems 2.4 and 2.5. They are parallel in form, but with diﬀerent
assumptions. In general, Theorem 2.4, which uses spark, is sharp and is far more
powerful than Theorem 2.5, which uses the coherence and so only a lower bound on
spark. The coherence can never be smaller than 1/ √n, and therefore, the cardinality
bound of Theorem 2.5 is never larger than √n/2. However, the spark can easily be
as large as n, and Theorem 2.4 then gives a bound as large as n/2.
5 For a general (possibly complex) matrix H of size n×n, Gershgorin’s disks are the n disks formed
by the centers h(i, i) and radiuses P
j,i |h(i, j)|. The theorem states that all eigenvalues of H must
lie within the union of these disks.

2.2 Uniqueness Analysis for the General Case
27
In fact, for the special matrix A = [Ψ, Φ] we have also obtained such a rule.
Interestingly, the lower bound for the general case becomes (1 + 1/µ(A))/2 while
the special two-ortho case gave a stronger (i.e., higher) lower bound. The general
case bound is nearly a factor of 2 weaker than (2.18), because (2.18) uses the special
structure A = [Ψ, Φ].
2.2.3 Uniqueness via the Babel Function
In the proof of Lemma 2.1 we considered minors of size p × p extracted from the
Gram matrix of the normalized matrix ˜A. The positive-deﬁniteness of all such mi-
nors imply that every p columns are linearly-independent. However, for simpliﬁca-
tion of the analysis we bounded all the oﬀ-diagonal entries of G by a single value
µ(A), and thus, lost robustness to possibly few extreme entries in this matrix.
Since we need to check whether the sum of the p−1 oﬀ-diagonal entries in every
row in these minors is less than 1 (for the Gershgorin property to hold true), we can
deﬁne the following Babel function, following Tropp:
Deﬁnition 2.4. For a given matrix ˜A with normalized columns, we consider a subset
Λ of p columns from ˜A, and compute the sum of the absolute values of their inner
product with a column outside this set. Maximizing over both the set Λ and the
outside column j we obtain the Babel function:
µ1(p) = max
Λ, |Λ|=p max
j<Λ
X
i∈Λ
|˜aT
i ˜a j|.
(2.23)
Clearly, for p = 1, we get that µ1(1) = µ(A). For p = 2 the above implies that
we need to sweep through all possible triplets, considering two as those belonging
to Λ, and the third as the external vector to compute inner products with. This deﬁ-
nition implies that this function is monotonically non-decreasing, and the slower its
growth the better the analysis it leads to, compared to the use of the cruder coherence
measure.
It might seem that computing this function for large p becomes exponential and
thus prohibitive, but this is in fact not true. By computing |G| = | ˜AT ˜A|, and sorting
every row in descending order, we obtain the matrix GS . The ﬁrst entry in every row
is 1, being the main diagonal entry, and thus should be disregarded. Computing the
sum of the p leading entries in every row (second and beyond), we get for every j
in the above deﬁnition the set of worst Λ’s, and among them we should choose the
maximal one, thus
µ1(p) = max
1≤j≤m
p+1
X
i=2
|GS ( j, i)|.
(2.24)

28
2 Uniqueness and Uncertainty
We note that for every p we have that µ1(p) ≤p · µ(A), and the two become equal
for Grassmannian matrices, where all the oﬀ-diagonal entries in the Gram matrix
are of the same magnitude.
How can we use the Babel function for better assessing the uniqueness? It is
clear that if µ1(p) < 1, we deduce that all p + 1 sets are linearly-independent. Thus,
a lower-bound on the spark could be
spark(A) ≥min
1≤p≤n {p| µ1(p −1) ≥1}.
(2.25)
The uncertainty and uniqueness properties follow immediately.
2.2.4 Upper-Bounding the Spark
The spark is impossible to evaluate in general, as it is even harder than solving
(P0). This is because its evaluation requires a sweep through all possible groups of
columns from A with varying cardinalities, seeking for a linearly-dependent subset
of columns. This is a combinatorial search of exponential complexity with m.
While this diﬃculty explains the necessity to replace the spark with the mutual-
coherence, the price paid in loss of tightness of the uniqueness result may be consid-
ered as too dear to be permitted. This motivates an alternative method for approx-
imating the spark, and this time using an upper bound. Such a bound implies that
we cannot guarantee uniqueness based on the obtained value, but it would give us a
rough evaluation of the region of uniqueness.
In order to develop the upper-bound, we redeﬁne the spark as the outcome of m
optimization problems (Pi
0) for i = 1, 2, . . . , m of the form:
(Pi
0) :
xi
opt = arg min
x
∥x∥0 subject to Ax = 0 and xi = 1.
(2.26)
Each of these problems assumes that the sparsest vector in the null-space of A uses
the i-th entry. By solving this sequence of (P0)-like problems, the sparsest result
among {xi
opt}m
i=1 gives the spark,
spark(A) = min
1≤i≤m ∥xi
opt∥0.
(2.27)
Since the set of problems (Pi
0) is too complex, we deﬁne an alternative set of
problems, replacing the ℓ0 with the ℓ1-norm,
(Pi
1) :
zi
opt = arg min
x
∥x∥1 subject to Ax = 0 and xi = 1.
(2.28)
As we have seen in Chapter 1, these problems have a linear programming structure,
they are convex, and solvable in reasonable time. Furthermore, it is clear that for
every i, we have that ∥xi
opt∥0 ≤∥zi
opt∥0, since ∥xi
opt∥0 is the sparsest solution possible
for this problem by deﬁnition, and thus we have the promised bound

2.3 Constructing Grassmannian Matrices
29
spark(A) ≤min
1≤i≤m ∥zi
opt∥0.
(2.29)
Numerical experiments show that this bound tends to be quite tight, and close to the
true spark.
2.3 Constructing Grassmannian Matrices
A Grassmannian (real) matrix A of size n×m with m ≥n is a matrix with normalized
columns such that its Gram matrix G = ATA satisﬁes
∀k , j, |Gk, j| =
r m −n
n(m −1).
(2.30)
As we have stated above, this is the smallest possible mutual-coherence possible.
Such matrices do not necessarily exist, and in particular, they are possible only if
m < min(n(n + 1)/2, (m −n)(m −n + 1)/2).
Grassmannian matrices are special in the sense that the angle between each and
every pair of columns in it is the same, and it is also the smallest possible. Thus, con-
struction of such matrices has a strong connection with packing of vectors/subspaces
in the IRn-space. While the case m = n leads trivially to unitary matrices that are easy
to construct, constructing a general Grassmannian matrix is very hard. A numerical
algorithm for this task was proposed by Tropp et al. and we bring it here as an il-
lustration of such design procedures. We should add that while image processing
ﬁnds little interest in such constructions, they ﬁnd important application in channel
coding, wireless communication, and more.
The key idea in the proposed algorithm is to iterate between projections onto the
requirements such matrix should satisfy. Starting with an arbitrary matrix A, these
projections should refer to the following requirements:
1. The columns in A are ℓ2-normalized: This can be forced by normalizing every
column.
2. Property (2.30): We should compute G = ATA, detect the oﬀ-diagonal entries
that are above some threshold, and decrease them. Similarly, since too small val-
ues are also not permitted in such Gram matrices, one might also add an increase
to such oﬀ-diagonal entries.
3. The rank of G should not exceed n: Since the above modiﬁcations cause G to
become a full-rank one, an SVD operation and truncation of the singular-values
beyond the ﬁrst n ones brings the new Gram matrix to the proper rank.
This process can and should be repeated many times. There is no guarantee for
convergence, or arrival at a Grassmannian matrix, but tests show that one can get
closer to such matrices with this numerical scheme. A Matlab code that follows
these guidelines is given in Figure 2.2.

30
2 Uniqueness and Uncertainty
D=randn(N,L); % initialization
D=D*diag(1./sqrt(diag(D’*D))); % normalize columns
G=D’*D; % compute the Gram matrix
mu=sqrt((L-N)/N/(L-1));
for k=1:1:Iter,
% shrink the high inner products
gg=sort(abs(G(:)));
pos=ﬁnd(abs(G(:))>gg(round(dd1*(L*L-L))) & abs(G(:))) <1;
G(pos)=G(pos)*dd2;
% reduce the rank back to N
[U,S,V]=svd(G);
S(N+1:end,1+N:end)=0;
G=U*S*V’;
% Normalize the columns
G=diag(1./sqrt(diag(G)))*G*diag(1./sqrt(diag(G)));
% Show status
gg=sort(abs(G(:)));
pos=ﬁnd(abs(G(:))>gg(round(dd1*(L*L-L))) & abs(G(:))) <1;
disp([k,mu,mean(abs(G(pos))),max(abs(G(pos)))]);
end;
[U,S,V]=svd(G);
D=sqrt(S(1:N,1:N))*U(:,1:N)´;
Fig. 2.2 Matlab code for building a Grassmannian matrix.
Figure 2.3 shows the results for this procedure.6 For a matrix of size 50×100, the
minimal possible coherence is √1/99 = 0.1005. Figure 2.3 shows the initial Gram
matrix and the one obtained after 10, 000 iterations, using dd1 = dd2 = 0.9. Figure
2.4 shows the sorted oﬀ-diagonal entries in these two Gram matrices, and as can be
seen, the iterative procedure succeeds very well in obtaining a matrix very close to
a Grassmannian one. In fact, in this run, the maximal oﬀ-diagonal entry is 0.119.
Figure 2.5 presents the mutual-coherence as it evolves through the iterations of the
algorithm.
2.4 Summary
We have now given answers to the questions posed at the start of this chapter. We
have seen that any suﬃciently sparse solution is guaranteed to be unique among all
possible solutions. Consequently, any suﬃciently sparse solution is necessarily the
global optimizer of (P0). These results show that searching for a sparse solution can
lead to a well-posed question with interesting properties. We now turn to discuss
practical methods for obtaining solutions for (P0).
6 Courtesy of Joel Tropp.

Further Reading
31
 
 
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
Fig. 2.3 The initial Gram matrix (top-left) and the ﬁnal one (top-right) in training a Grassmannian
matrix of size 50 × 100. The bottom part of the ﬁgure shows the absolute of the ﬁnal Gram matrix,
showing that the oﬀ-diagonal elements tend to have the same value, as required.
Further Reading
1. A.R. Calderbank and P.W. Shor, Good quantum error-correcting codes exist,
Phys. Rev. A, 54(2):1098–1105, August 1996.
2. D.L. Donoho and M. Elad, Optimally sparse representation in general (non-
orthogonal) dictionaries via l1 minimization, Proc. of the National Academy of
Sciences, 100(5):2197–2202, 2003.
3. D.L. Donoho and X. Huo, Uncertainty principles and ideal atomic decomposi-
tion, IEEE Trans. On Information Theory, 47(7):2845–2862, 1999.
4. D.L. Donoho and P.B. Starck, Uncertainty principles and signal recovery, SIAM
Journal on Applied Mathematics, 49(3):906–931, June, 1989.

32
2 Uniqueness and Uncertainty
0
1000
2000
3000
4000
5000
6000
7000
8000
9000
−0.5
−0.4
−0.3
−0.2
−0.1
0
0.1
0.2
0.3
0.4
0.5
 
 
Initial Gram
Final Gram
Optimal µ
Fig. 2.4 The sorted oﬀ-diagonal entries in the initial Gram matrix and the ﬁnal one.
10
0
10
1
10
2
10
3
10
4
0
0.1
0.2
0.3
0.4
0.5
 
 
Obtained µ
mean coherence
Optimal µ
Fig. 2.5 The evolved mutual-coherence of the matrix as a function of the iteration.
5. M. Elad and A.M. Bruckstein, A generalized uncertainty principle and sparse
representation in pairs of bases, IEEE Trans. On Information Theory, 48:2558–
2567, 2002.
6. I.F. Gorodnitsky and B.D. Rao, Sparse signal reconstruction from limited data
using FOCUSS: A re-weighted norm minimization algorithm, IEEE Trans. On
Signal Processing, 45(3):600–616, 1997.
7. S. Gurevich, R. Hadani, and N. Sochen, The ﬁnite harmonic oscillator and
its associated sequences Proc. Natl. Acad. Sci. USA, 105(29):9869–9873, July,
2008.

Further Reading
33
8. S. Gurevich, R. Hadani, and N. Sochen, On some deterministic dictionaries
supporting sparsity, Journal of Fourier Analysis and Applications, 14(5-6):859–
876, December, 2008.
9. R. Gribonval and M. Nielsen, Sparse decompositions in unions of bases, IEEE
Trans. on Information Theory, 49(12):3320–3325, 2003.
10. W. Heisenberg, The physical principles of the quantum theory, (C. Eckart and
F.C. Hoyt, trans.), University of Chicago Press, Chicago, IL, 1930.
11. R.A. Horn C.R. Johnson, Matrix Analysis, New York: Cambridge University
Press, 1985.
12. X. Huo, Sparse Image representation Via Combined Transforms, PhD thesis,
Stanford, 1999.
13. J.B. Kruskal, Three-way arrays: rank and uniqueness of trilinear decomposi-
tions, with application to arithmetic complexity and statistics, Linear Algebra
and its Applications, 18(2):95–138, 1977.
14. Title: P.W.H. Lemmens and J.J. Seidel, Equiangular lines, Journal of Algebra,
24(3):494–512, 1973.
15. X. Liu and N.D. Sidiropoulos, Cramer-Rao lower bounds for low-rank de-
composition of multidimensional arrays, IEEE Trans. on Signal Processing,
49(9):2074–2086, 2001.
16. B.K. Natarajan, Sparse approximate solutions to linear systems, SIAM Journal
on Computing, 24:227–234, 1995.
17. W.W. Peterson and E.J. Weldon, Jr., Error-Correcting Codes, 2nd edition, MIT
Press: Cambridge, Mass., 1972.
18. A. Pinkus, N-Width in Approximation Theory, Springer, Berlin, 1985.
19. T. Strohmer and R. W. Heath, Grassmannian frames with applications to coding
and communication, Applied and Computational Harmonic Analysis, 14:257–
275, 2004.
20. J.A. Tropp, Greed is good: Algorithmic results for sparse approximation, IEEE
Trans. On Information Theory, 50(10):2231–2242, October 2004.
21. J.A. Tropp, I.S. Dhillon, R.W. Heath Jr., and T. Strohmer, Designing structured
tight frames via alternating projection, IEEE Trans. Info. Theory, 51(1):188–
209, January 2005.

Chapter 3
Pursuit Algorithms – Practice
It is now time to consider reliable and eﬃcient methods for solving (P0), as a
straightforward approach seems hopeless. We now discuss methods which, it seems,
have no hope of working – but which, under speciﬁc conditions, will work. Looking
at the problem (P0),
(P0) :
min
x
∥x∥0 subject to b = Ax,
one observes that the unknown x is composed of two eﬀective parts to be found
– the support of the solution, and the non-zero values over this support. Thus, one
way to attack the numerical solution of (P0) is to focus on the support, with the
understanding that once found, the non-zero values of x are easily detected by plain
Least-Squares. As the support is discrete in nature, algorithms that seek it are dis-
crete as well. This line of reasoning leads to the family of greedy algorithms that
will be presented hereafter.
An alternative view of (P0) can disregard the support, and consider the unknown
as a vector x ∈IRm over the continuum. Smoothing the penalty function ∥x∥0, one
can adopt a continuous optimization point of view for solving (P0). The relaxation
methods that are presented later in this chapter adopt this view, by smoothing the ℓ0-
norm in various forms, and handling the revised problem as a smooth optimization.
3.1 Greedy Algorithms
3.1.1 The Core Idea
Suppose that the matrix A has spark(A) > 2, and the optimization problem (P0)
has value val(P0) = 1 (at the optimal solution), so b is a scalar multiple of some
column of the matrix A, and this solution is known to be unique. We can identify
this column by applying m tests – one per column of A. The j-th test can be done
© Springer Science+Business Media, LLC 2010
M. Elad, Sparse and Redundant Representations: From Theory to Applications in Signal 
and Image Processing, DOI 10.1007/978-1-4419-7011-4_3,
35

36
3 Pursuit Algorithms – Practice
by minimizing ϵ( j) = ∥a jzj −b∥2, leading to z∗
j = aT
j b/∥aj∥2
2. Plugged back into the
error expression, the error becomes
ϵ( j) = min
z j ∥ajzj −b∥2
2 =

aT
j b
∥a j∥2
2
aj −b

2
2
(3.1)
= ∥b∥2
2 −2
(aT
j b)2
∥a j∥2
2
+
(aT
j b)2
∥a j∥2
2
= ∥b∥2
2 −
(aT
j b)2
∥a j∥2
2
.
If this error is zero, we have found the proper solution. Thus the test to be done is
simply
a j
2
2 ∥b∥2
2 = (aT
j b)2 (which is equivalent to the statement that the Cauchy-
Schwartz inequality should be satisﬁed with equality), indicating that b and a j are
parallel. This procedure requires order O(mn) ﬂops, which may be considered rea-
sonable.
Proceeding with the same rationale, suppose that A has spark(A) > 2k0, and
the optimization problem is known to have value val(P0) = k0. Then b is a linear
combination of at most k0 columns of A. Generalizing the previous solution, one
might think to enumerate all
m
k0

= O(mk0) subsets of k0 columns from A, and test
each. Enumeration takes O(mk0nk02) ﬂops, which seems prohibitively slow in many
settings.
A greedy strategy abandons exhaustive search in favor of a series of locally op-
timal single-term updates. Starting from x0 = 0 it iteratively constructs a k-term
approximant xk by maintaining a set of active columns – initially empty – and, at
each stage, expanding that set by one additional column. The column chosen at each
stage maximally reduces the residual ℓ2 error in approximating b from the currently
active columns. After constructing an approximant including the new column, the
residual ℓ2 error is evaluated; if it now falls below a speciﬁed threshold, the algo-
rithm terminates.
3.1.2 The Orthogonal-Matching-Pursuit
Figure 3.1 presents a formal description of this strategy, and its associated nota-
tion. This procedure is known in the literature of signal processing by the name
Orthogonal-Matching-Pursuit (OMP), but is very well known (and was used much
earlier) by other names in other ﬁelds – see below.
Note that the Sweep stage gives error values of the following form, which is very
similar to what we have seen above in Equation (3.1),
ϵ( j) = min
z j ∥a jzj −rk−1∥2
2 =

aT
j rk−1
∥aj∥2
2
a j −rk−1

2
2
(3.2)

3.1 Greedy Algorithms
37
Task: Approximate the solution of (P0): minx ∥x∥0 subject to Ax = b.
Parameters: We are given the matrix A, the vector b, and the error threshold
ϵ0.
Initialization: Initialize k = 0, and set
•
The initial solution x0 = 0.
•
The initial residual r0 = b −Ax0 = b.
•
The initial solution support S0 = S upport{x0} = ∅.
Main Iteration: Increment k by 1 and perform the following steps:
•
Sweep: Compute the errors ϵ(j) = minzj ∥ajz j −rk−1∥2
2 for all j using the
optimal choice z∗
j = aT
j rk−1/∥aj∥2
2.
•
Update Support: Find a minimizer, j0 of ϵ(j): ∀j < Sk−1, ϵ(j0) ≤ϵ(j), and
update Sk = Sk−1 ∪{j0}.
•
Update Provisional Solution: Compute xk, the minimizer of ∥Ax−b∥2
2 sub-
ject to S upport{x} = Sk.
•
Update Residual: Compute rk = b −Axk.
•
Stopping Rule: If ∥rk∥2 < ϵ0, stop. Otherwise, apply another iteration.
Output: The proposed solution is xk obtained after k iterations.
Fig. 3.1 Orthogonal-Matching-Pursuit – a greedy algorithm for approximating the solution of (P0).
=
rk−12
2 −2
(aT
j rk−1)2
∥aj∥2
2
+
(aT
j rk−1)2
∥aj∥2
2
=
rk−12
2 −
(aT
j rk−1)2
∥aj∥2
2
.
Thus, the quest for the smallest error is actually equivalent to the quest for the largest
(in absolute value) inner product between the residual rk−1 and the normalized vec-
tors of the matrix A.
In the Update Provisional Solution stage, we minimize the term ∥Ax −b∥2
2 with
respect to x, such that its support is Sk. We denote ASk as a matrix of size n × |Sk|
that contains the columns from A that belong to this support. Thus, the problem to
be solved is a minimization of ∥ASkxSk −b∥2
2, where xSk is the non-zero portion of
the vector x. The solution is given by zeroing the derivative of this quadratic form,
AT
Sk(ASkxSk −b) = −AT
Skrk = 0.
(3.3)
Here we have used the formula of the residual in the k-th iteration, rk = b −Axk =
b −ASkxSk. The above relation suggests that the columns in A that are part of the
support Sk are necessarily orthogonal to the residual rk. This in turn implies that
in the next iteration (and afterwards), these columns will not be chosen again for
the support. This orthogonalization is the reason for the name of the algorithm —
Orthogonal-Matching-Pursuit.

38
3 Pursuit Algorithms – Practice
A more complex and somewhat better behaving variant of the above algorithm
can be suggested, where each of the tests in the sweep stage is done as full Least-
Squares (LS), considering all the accumulated columns and the candidate one to-
gether, and solving for all the coeﬃcients at once. As in the OMP, m −|Sk−1| LS
steps are performed at the k-th step. This method is known as LS-OMP. The LS
steps can be made eﬃcient using the formula
" M b
bT c
#−1
=
" M−1 + pM−1bbTM−1 −pM−1b
−pbTM−1
p
#
,
(3.4)
where p = 1/(c −bTM−1b). Denoting the sub-matrix that gathers the k −1 chosen
columns from A as As, the LS problem we should solve is
min
xs,z

[As ai] " xs
z
#
−b

2
2
.
(3.5)
The solution is given by zeroing the derivative of the above expression
" AT
s
aT
i
# [As ai] " xs
z
#
−
" AT
s
aT
i
#
b = 0,
(3.6)
leading to
" xs
z
#
opt
=
" AT
s As AT
s ai
aT
i As ∥ai∥2
2
#−1 "AT
s b
aT
i b
#
.
(3.7)
Thus, based on the formula given in Equation (3.4), if we have stored the matrix
(AT
s As)−1, then the above formula becomes easy, and leads to an update of this ma-
trix for k columns. Once the solution is found, the residual error should be evaluated,
and the column ai that leads to the smallest error should be chosen. Observe that if ai
is orthogonal to the columns of As, the above matrix to be inverted becomes block-
diagonal, and the result would be xs = A+
s b and z = aT
i b/∥ai∥2
2. This means that the
coeﬃcients of the previous solution remain intact, and a new non-zero is introduced
with a coeﬃcient z.
There are numerical shortcuts for the evaluation of the residuals, but we shall not
explore this matter here. Note that the above recursive approach towards the Least-
Squares task in the LS-OMP is also relevant for speeding up the OMP, implying
that the Update Provisional Solution stage can be made far more eﬃcient.
If the delivered approximation has k0 non-zeros, the LS-OMP and OMP methods
require O(k0mn) ﬂops in general; this can be dramatically better than the exhaus-
tive search, which requires O(nmk0k02) ﬂops. Thus, the single-term-at-a-time strat-
egy can be much more eﬃcient than exhaustive search – if it works! The strategy
can fail badly, i.e., there are explicit examples constructed by Vladimir Temlyakov
where a simple k-term representation is possible, but this approach yields an n-term
(i.e., dense) representation. In general, all that can be said is that among single-
term-at-a-time strategies, the approximation error is always reduced by as much as

3.1 Greedy Algorithms
39
possible, given the starting approximation and the single-term-at-a-time constraint.
This property has earned this algorithm the name “Greedy algorithm” in approxi-
mation theory.
3.1.3 Other Greedy Methods
Many variants on the above algorithm are available, oﬀering improvements either
in accuracy and/or in complexity. This family of greedy algorithms is well known
and extensively used, and in fact, these algorithms have been re-invented in various
ﬁelds. In the setting of statistical modeling, greedy stepwise Least-Squares is called
forward stepwise regression, and has been widely practiced since at least the 1960s.
When used in the signal processing setting this goes by the name of Matching-
Pursuit (MP) or Orthogonal-Matching-Pursuit (OMP). Approximation theorists re-
fer to these algorithms as Greedy Algorithms (GA), and consider several variants of
them – the Pure (PGA), the Orthogonal (OGA), the Relaxed (RGA), and the Weak
Greedy Algorithm (WGA).
The MP algorithm is similar to the OMP, but with an important diﬀerence that
makes it simpler, and thus less accurate. In the main iteration, after the sweep and
the update support stages, rather than solving a Least-Squares for re-evaluating all
the coeﬃcients in x, the coeﬃcients of the Sk−1 original entries remain unchanged,
and the new coeﬃcient that refers to the new member j0 ∈Sk is chosen as z∗
j0. This
algorithm is described in Figure 3.2.
The Weak-MP is a further simpliﬁcation of the MP algorithm, allowing for a sub-
optimal choice of the next element to be added to the support. Embarking from the
MP algorithm description in Figure 3.2, the update support is relaxed by choosing
any index that is factor t (in the range (0, 1]) away from the optimal choice. This
algorithm is described in Figure 3.3.
Let us explain and motivate this method: We have seen the equivalence between
the inner products computed, |aT
j rk−1| (up to a normalization factor), and the corre-
sponding errors ϵ( j) among which we seek the minimum. Rather than searching for
the largest inner-product value, we settle for the ﬁrst found that exceeds a t-weaker
threshold. The Cauchy-Schwartz inequality gives
(aT
j rk−1)2
∥a j∥2
2
≤max
1≤j≤m
(aT
j rk−1)2
∥a j∥2
2
≤∥rk−1∥2
2.
(3.8)
This puts an upper bound on the maximal achievable inner-product. Thus, we can
compute ∥rk−1∥2
2 at the beginning of the Sweep stage, and as we search for j0 that
gives the smallest error ϵ( j), we choose the ﬁrst that gives
(aT
j0rk−1)2
∥aj0∥2
2
≥t2 · ∥rk−1∥2
2 ≥t2 · max
1≤j≤m
(aT
j rk−1)2
∥aj∥2
2
.
(3.9)

40
3 Pursuit Algorithms – Practice
Task: Approximate the solution of (P0): minx ∥x∥0 subject to Ax = b.
Parameters: We are given the matrix A, the vector b, and the error threshold
ϵ0.
Initialization: Initialize k = 0, and set
•
The initial solution x0 = 0.
•
The initial residual r0 = b −Ax0 = b.
•
The initial solution support S0 = S upport{x0} = ∅.
Main Iteration: Increment k by 1 and perform the following steps:
•
Sweep: Compute the errors ϵ(j) = minzj ∥ajz j −rk−1∥2
2 for all j using the
optimal choice z∗
j = aT
j rk−1/∥aj∥2
2.
•
Update Support: Find a minimizer, j0 of ϵ(j): ∀1 ≤j ≤m, ϵ(j0) ≤ϵ(j),
and update Sk = Sk−1 ∪{j0}.
•
Update Provisional Solution: Set xk = xk−1, and update the entry xk(j0) =
xk( j0) + z∗
j.
•
Update Residual: Compute rk = b −Axk = rk−1 −z∗
j0aj0.
•
Stopping Rule: If ∥rk∥2 < ϵ0, stop. Otherwise, apply another iteration.
Output: The proposed solution is xk obtained after k iterations.
Fig. 3.2 Matching-Pursuit for approximating the solution of (P0).
Task: Approximate the solution of (P0): minx ∥x∥0 subject to Ax = b.
Parameters: We are given the matrix A, the vector b, the error threshold ϵ0,
and the scalar 0 < t < 1.
Initialization: Initialize k = 0, and set
•
The initial solution x0 = 0.
•
The initial residual r0 = b −Ax0 = b.
•
The initial solution support S0 = S upport{x0} = ∅.
Main Iteration: Increment k by 1 and perform the following steps:
•
Sweep: Compute the errors ϵ(j) = minzj ∥a jzj −rk−1∥2
2 for j = 1, 2, . . .
using the optimal choice z∗
j
=
aT
j rk−1/∥a j∥2
2. Stop the sweep when
|aT
j rk−1|/∥aj∥2 ≥t · ∥rk−1∥2.
•
Update Support: Update Sk = Sk−1 ∪{ j0}, with j0 found in the sweep
stage.
•
Update Provisional Solution: Set xk = xk−1, and update the entry xk(j0) =
xk( j0) + z∗
j.
•
Update Residual: Compute rk = b −Axk = rk−1 −z∗
j0aj0.
•
Stopping Rule: If ∥rk∥2 < ϵ0, stop. Otherwise, apply another iteration.
Output: The proposed solution is xk obtained after k iterations.
Fig. 3.3 Weak-Matching-Pursuit for approximating the solution of (P0).

3.1 Greedy Algorithms
41
for a pre-chosen t in the range (0, 1). The above condition can be written alterna-
tively as
(aT
j0rk−1)2 ≥t2 · ∥rk−1∥2
2 · ∥aj0∥2
2.
(3.10)
This way, the chosen index clearly points to a column that is at the most factor t
away from the possible maximum. Note that it is possible that the complete sweep
is performed without any index satisfying the above condition, and then we simply
choose the maximum that was found as a by-product of the search. This way, the
search could become much faster, with the understanding that the rate-of-decay of
the residual has been somewhat compromised.
3.1.4 Normalization
All the above greedy algorithms (OMP, MP, and weak-MP) are described for a
general matrix A, which may have columns that are not of unit ℓ2-norm. We can
normalize the columns by the operation ˜A = AW, using a diagonal matrix W con-
taining 1/∥ai∥2 on the main-diagonal, and then use ˜A in the above algorithms. Will
the outcome be diﬀerent? The answer is stated in the following Theorem.
Theorem 3.1. The greedy algorithms (OMP, MP, and weak-MP) produce the same
solution support Sk when using either the original matrix A or its normalized ver-
sion ˜A.
Proof: We start with the OMP’s k-th step, and observe that the choice of the next
index is made by ﬁnding j0 that leads to the smallest of the errors, given by ϵ(j) =
∥rk−1∥2
2 −(aT
j rk−1)2/∥aj∥2
2. Denoting ˜aj = a j/∥aj∥2, we clearly have that ∥˜aj∥2 = 1,
and thus we get
ϵ( j) = ∥rk−1∥2
2 −

aT
j rk−1
∥aj∥2

2
(3.11)
= ∥rk−1∥2
2 −(˜aT
j rk−1)2
= ∥rk−1∥2
2 −

˜aT
j rk−1
∥˜aj∥2

2
.
Thus, using the normalized columns leads to the same outcome for the choice of the
index j0.
The Least-Squares step that follows ﬁnds the solution to minx ∥Ax−b∥2
2, subject
to the found support Sk. Denoting by AS the sub-matrix of A that contains the
columns from the support, the solution to the above problem is easily given by
xk
S =

AT
SAS
−1 AT
Sb,
(3.12)

42
3 Pursuit Algorithms – Practice
and the resulting residual is
rk = b −ASxk
S =

I −AS

AT
SAS
−1 AT
S

b.
(3.13)
Working instead with the normalized matrix, we use the sub-matrix ˜AS = ASWS,
where WS is the extracted portion from W referring to the support S. The residual
obtained by the OMP that uses this normalized matrix is
˜rk =

I −˜AS
 ˜AT
S ˜AS
−1 ˜AT
S

b
(3.14)
=

I −ASWS

WSAT
SASWS
−1 WSAT
S

b
=

I −ASWSW−1
S

AT
SAS
−1 W−1
S WSAT
S

b
=

I −AS

AT
SAS
−1 AT
S

b = rk.
We got that the residual is the same for the two options, original or normalized
matrix A. Therefore, as the residual drives the next stage of choosing the support,
the OMP is indiﬀerent to the normalization, as claimed.
MP uses the same procedure for choosing the index j0 and as we have already
shown, this does not change under normalization. The residual update in MP is even
simpler, done by the formula
rk = rk−1 −z∗
j0aj0 = rk−1 −
aT
j0rk−1
∥a j0∥2
2
a j0.
(3.15)
Applying the same step with the normalized matrix gives
˜rk = ˜rk−1 −˜z∗
j0 ˜a j0 = ˜rk−1 −˜aT
j0˜rk−1˜a j0.
(3.16)
Assuming that ˜rk−1 = rk−1 and using the connection ˜a j = a j/∥aj∥2 we obtain
˜rk = ˜rk−1 −˜aT
j0˜rk−1˜aj0
(3.17)
= rk−1 −
 a j0
∥a j0∥2
!T
rk−1
 a j0
∥aj0∥2
!
= rk−1 −
aT
j0rk−1
∥a j0∥2
2
aj0 = rk,
and again we got that the residual remain unchanged as well, guaranteeing the MP
steps are also indiﬀerent to the normalization.
The last algorithm to consider, the weak-MP, searches for j0 that satisﬁes
|aT
j0rk−1|/∥aj0∥2 ≥t · |aT
j rk−1|/∥aj∥2,
(3.18)

3.1 Greedy Algorithms
43
when using the original matrix. The same step with the normalized matrix leads
to the very same rule, and thus the same choice of index, due to the relation ˜a j =
aj/∥aj∥2. Also, the residual update is the very same one used with the MP, and thus
the residual does not change too, leading to the fact that weak-MP is also indiﬀerent
to the normalization of the matrix A, and this concludes this proof.
□
It is simpler to work with the normalized matrix, since then plain inner-products
are used in all these algorithms for choosing the next support index. Therefore, we
shall assume hereafter that these matrices are preceded with a normalization.
Note that if the original problem is given with a non-normalized matrix, the nor-
malization does have an eﬀect on the obtained result xk. Thus, if a normalized matrix
is used in any of these algorithms, a post de-normalization step of the form xk = W˜xk
must be performed. This is because the algorithm provides a solution that satisﬁes
˜A˜xk = b, and since ˜A = AW we get that
b = ˜A˜xk = AW˜xk = Axk
⇒xk = W˜xk.
(3.19)
3.1.5 Rate of Decay of the Residual in Greedy Methods
In the MP algorithm we have seen (see Figure 3.2) that the residual is updated by
the recursive formula1
rk = rk−1 −z∗
j0aj0 = rk−1 −(aT
j0rk−1)aj0,
(3.20)
where j0 is chosen such that |aT
j rk−1| is maximized. Thus, the energy of the residual
behaves like
ϵ( j0) = ∥rk∥2
2
(3.21)
= ∥rk−1∥2
2 −2(aT
j0rk−1)2 + (aT
j0rk−1)2
= ∥rk−1∥2
2 −(aT
j0rk−1)2
= ∥rk−1∥2
2 −max
1≤j≤m (aT
j rk−1)2.
Let us deﬁne the following decay-factor, following the idea developed by Mallat
and Zhang (1993):
Deﬁnition 3.1. For the matrix A with m normalized columns {aj}m
j=1, and an arbi-
trary vector v, the decay-factor δ(A, v) is deﬁned by
δ(A, v) = max
1≤j≤m
|aT
j v|2
∥v∥2
2
.
(3.22)
1 From now on we shall assume that the columns of A are normalized.

44
3 Pursuit Algorithms – Practice
Clearly, by this deﬁnition we have that the relation ∥v∥2
2 · δ(A, v) = maxj |aT
j v|. Also
evident is the fact that 0 ≤δ(A, v) ≤1 by its deﬁnition as a normalized correlation.
The maximum is attained for v = a j. The minimum could be attained only if A is
rank-deﬁcient. We shall return to this discussion shortly, after using this deﬁnition
for analyzing the MP error decay rate.
Based on the above deﬁnition we now deﬁne the following universal decay-factor
that minimizes the above over all possible vectors v.
Deﬁnition 3.2. For the matrix A with m normalized columns {a j}m
j=1, the universal
decay-factor δ(A) is deﬁned by
δ(A) = inf
v
max
1≤j≤m
|aT
j v|2
∥v∥2
2
= inf
v
δ(A, v).
(3.23)
This deﬁnition seeks the vector v that leads to the smallest possible inner product
with the columns of A. As we show next, this implies the weakest decay of the
residual. Returning to Equation (3.21) and using δ(A), the decay-factor of the matrix
A, we obtain
∥rk∥2
2 = ∥rk−1∥2
2 −max
1≤j≤m |aT
j rk−1|2
(3.24)
= ∥rk−1∥2
2 −max
1≤j≤m
|aT
j rk−1|2
∥rk−1∥2
2
· ∥rk−1∥2
2
= ∥rk−1∥2
2 −δ(A, rk−1) · ∥rk−1∥2
2
≤∥rk−1∥2
2 −δ(A)∥rk−1∥2
2 = (1 −δ(A)) · ∥rk−1∥2
2.
Application of this formula recessively leads to
∥rk∥2
2 ≤(1 −δ(A))k · ∥r0∥2
2 = (1 −δ(A))k · ∥b∥2
2,
(3.25)
establishing an exponential rate-of-convergence bound of the residual. A similar
analysis for the OMP reveals that this is also the bound on its residual rate of decay,
as the LS update in the OMP reduces the error even more, when compared to the
MP. Similarly, the Weak-MP leads to an exponential rate of convergence with tδ(A)
replacing δ(A), leading to a weaker decay-factor.
For all the above to show convergence, we must verify that δ(A) is strictly posi-
tive and not zero. Since the A we consider is a full-rank matrix with more columns
than rows, its columns span the entireRn space. Thus, no vector v could be orthog-
onal to all these columns, and thus δ(A) > 0.
As an interesting example, if A = I (or any orthogonal matrix for that matter), it is
easy to verify that δ(A) = 1/n. Interestingly, the explanation for this is very similar
to the one explaining the smallest mutual-coherence for two-ortho matrices. Take
the identity matrix, concatenate the vector v, and then compute the Gram matrix.
The decay-factor δ(A) is simply the largest oﬀ-diagonal of this Gram matrix, and
this is the square of the largest entry in v. Thus, the (normalized) vector that gives
the smallest possible inner product with the columns of I is vopt = 1/ √n.

3.1 Greedy Algorithms
45
Task: Approximate the solution of (P0): minx ∥x∥0 subject to Ax = b.
Parameters: A, b are given, along with the number of atoms desired, k.
Quality Evaluation: Compute the errors ϵ(j) = minzj ∥ajzj −b∥2
2 for all j using
the optimal choice z∗
j = aT
j b/∥aj∥2
2.
Update Support: Find the set of indices S of cardinality k that contains the
smallest errors: ∀j ∈S, ϵ(j) ≤mini<S ϵ(i).
Update Provisional Solution: Compute xk, the minimizer of ∥Ax −b∥2
2 subject
to S upport{x} = Sk.
Output: The proposed solution is x, the minimizer of ∥Ax −b∥2
2 subject to
S upport{x} = S.
Fig. 3.4 The Thresholding Algorithm – an algorithm for approximating the solution of (P0).
An alternative expression can be used for the deﬁnition of δ(A), by normalizing
the columns of A as we did before, obtaining ˜A. Using this leads to
δ( ˜A) = inf
v
max
1≤j≤m
|˜aT
j v|2
∥v∥2
2
(3.26)
= inf
v
1
∥v∥2
2
max
1≤j≤m |˜aT
j v|2
= inf
v
∥˜ATv∥2
∞
∥v∥2
2
.
If the ℓ∞would have been replaced by ℓ2, this would have become the smallest
eigenvalue of the matrix ˜A ˜AT, which is also known to be strictly positive since this
matrix is positive-deﬁnite.
3.1.6 Thresholding Algorithm
We conclude the discussion on the greedy algorithms by introducing another method,
which is far simpler than all the previous ones, and slightly diﬀerent in the way the
greediness is practiced. A simpliﬁcation of the OMP algorithm can be proposed,
where the decision made about the support of the solution is based on the ﬁrst
projection alone. The idea is to choose the k largest inner products as the desired
support. This method, called the thresholding algorithm, is depicted in Figure 3.4.
The error ϵ( j) according to which the support is chosen is the same one used in
earlier algorithms, and is given by
ϵ( j) = min
zj ∥ajzj −b∥2
2
(3.27)

46
3 Pursuit Algorithms – Practice
=

aT
j b
∥aj∥2
2
· a j −b

2
2
= ∥b∥2
2 −
(aT
j b)2
∥aj∥2
2
,
Thus, the same result is obtained by ﬁrst normalizing the columns in the matrix A
and then using the simpler formula,
ϵ( j) = ∥b∥2
2 −(aT
j b)2.
(3.28)
This also implies that the search for the k elements of the support amounts to a
simple sort of the entries of the vector |ATb|. Clearly, this algorithm is far simpler
than the greedy techniques presented above.
We note that in the above algorithm description, we assume that k, the number
of required non-zeros, is known. Alternatively, we can increase k until the error
∥Axk −b∥2 reaches a pre-speciﬁed value ϵ0.
3.1.7 Numerical Demonstration of Greedy Algorithms
We conclude the discussion on the various greedy algorithms by proposing a sim-
ple demonstration of their comparative behavior on a simple case study. We create
a random matrix A of size 30 × 50 with entries drawn from the normal distribu-
tion. We normalize the columns of this matrix to have a unit ℓ2-norm. We generate
sparse vectors x with independent and identically-distributed (iid) random supports
of cardinalities in the range [1, 10], and non-zero entries drawn as random uniform
variables in the range [−2, −1] ∪[1, 2]. We deliberately choose the non-zeroes to be
away from zero, as it has a strong eﬀect on the success of greedy methods (and es-
pecially the thresholding algorithm). Once x is generated, we compute b = Ax, and
then apply the-above described algorithms to seek for x. We perform 1, 000 such
tests per each cardinality, and present average results. We note that based on ideas
developed in Chapter 2 we know that in all our tests the original solution is also the
sparsest, as the spark of A is 31.
When testing the success of an approximation algorithm, there are many ways to
deﬁne the distance between the solution it proposes, ˆx and the ideal one x. Here we
present two such measures – ℓ2-error, and recovery of the support. The ℓ2-error is
computed as the ratio ∥x−ˆx∥2/∥x∥2, indicating that we are interested in ℓ2-proximity
between the two solutions, and we measure this distance as relative to the energy in
the true solution.
Such measure does not reveal the complete story of these algorithms, as it fails
to indicate when the support is recovered perfectly or at least partially. Thus, we add
another measure, computing the distance between the supports of the two solutions.
Denoting the two supports as ˆS and S, we deﬁne this distance by

3.1 Greedy Algorithms
47
0
2
4
6
8
10
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Cardinality of the true solution
Average and Relative L2−Error
 
 
LS−OMP
OMP
MP
Weak−MP (t=0.5)
Thresholding
Fig. 3.5 Greedy algorithms’ performance in terms of relative ℓ2 recovery error.
dist( ˆS, S) = max{| ˆS|, |S|} −| ˆS ∩S|
max{| ˆS|, |S|}
.
(3.29)
If the two supports are the same, the distance is zero. If they are diﬀerent, the dis-
tance is dictated by the size of their intersection, relative to the length of the longer
of the two. A distance close to 1 indicate that the two supports are entirely diﬀerent,
with no overlap.
The algorithms compared are the LS-OMP, the OMP, the MP, the Weak-MP
with t = 0.5, and the thresholding algorithm. All these algorithms seek the proper
solution till the residual is below a certain threshold (∥rk∥2
2 ≤1e −4). The results of
this experiment are summarized in Figures 3.5 and 3.6.
As expected, the best performing method is the LS-OMP, closely followed by
the OMP. There is a slight diﬀerence between the MP and its weak version. The
thresholding algorithm performs poorly, being the worst among these techniques.
Overall, these algorithms behave quite well for low-cardinalities, in both measures
presented, and there is a general agreement between the two measures on how the
algorithms are ordered from best to worst. We shall further analyze some of these
algorithms theoretically in the next chapter, which will help in understanding the
results obtained.

48
3 Pursuit Algorithms – Practice
1
2
3
4
5
6
7
8
9
10
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Cardinality of the true solution
Probability of Error in Support
 
 
LS−OMP
OMP
MP
Weak−MP (t=0.5)
Thresholding
Fig. 3.6 Greedy algorithms’ performance in terms of the success rate in detecting the true support.
3.2 Convex Relaxation Techniques
3.2.1 Relaxation of the ℓ0-Norm
As already mentioned, a second way to render (P0) more tractable is to relax the
(highly discontinuous) ℓ0-norm, replacing it by a continuous or even smooth approx-
imation. Examples of such relaxation options include replacing it with ℓp norms for
some p ∈(0, 1] or even by smooth functions such as P
j log(1+αx2
j), P
j x2
j/(α+ x2
j),
or P
j(1 −exp(−αx2
j)).
An interesting example of this family is the FOcal Underdetermined System
Solver (FOCUSS) algorithm by Gorodnitsky and Rao. This method uses a method
called Iterative-Reweighed-Least-Squares (IRLS) to represent ℓp (for some ﬁxed
p ∈(0, 1]) as a weighted ℓ2-norm. In an iterative algorithm, given a current ap-
proximate solution xk−1, set Xk−1 = diag(|xk−1|q). Assuming, for a moment, that this
matrix is invertible, the term ∥X−1
k−1x∥2
2 is equal to ∥x∥2−2q
2−2q, since every entry in x is
raised to the power 2−2q and then they are summed. Thus, by choosing q = 1−p/2,
this expression imitates the ℓp-norm, ∥x∥p
p.
Instead of using the inverse of Xk−1 as done above, we use the pseudo-inverse,
∥X+
k−1x∥2
2, deﬁned as plain inversion for non-zero entries xi and zeros elsewhere.
Note that with this change the above interpretation of the ℓp-norm is not ruined, as
those entries contribute zeros to the summation of the norm anyhow. Based on this,
if we attempt to solve the problem

3.2 Convex Relaxation Techniques
49
(Mk) :
min
x
∥X+
k−1x∥2
2 subject to b = Ax,
(3.30)
this is an imitation of the (Pp) problem around the previous solution xk−1. In partic-
ular, for q = 1, this would be a variant of the (P0) problem. However, as opposed to
the direct formulation of (Pp), this problem is solvable using standard linear algebra
and Lagrange multipliers, as the penalty uses a plain ℓ2-norm. Thus
L(x) = ∥X+
k−1x∥2
2 + λT(b −Ax)
(3.31)
⇒
∂L(x)
∂x
= 0 = 2(X+
k−1)2x −ATλ.
If we assume that X+
k−1 is invertible, this implies that X+
k−1 has no zero entries on the
main-diagonal, and thus (X+
k−1)−1 = Xk−1. In this case the solution is given by
xk = 0.5X2
k−1ATλ.
(3.32)
In the general case, where some entries in the main-diagonal of Xk−1 are zero, the
solution given above nulls the corresponding entries in xk. Assuming hereafter that
a zero entry in xk−1 should remain as such in subsequent iterations (in order to
promote sparsity in the solution), this formula remains true and behaves properly.
Plugging this to the constraint Ax = b gives
0.5A(Xk−1)2ATλ = b ⇒λ = 2

AX2
k−1AT−1 b.
(3.33)
Here again the inversion must be done with care, and in fact should be replaced by
a pseudo-inverse.2 Thus,
xk = X2
k−1AT 
AX2
k−1AT+ b.
(3.34)
The approximate solution xk is used to update the diagonal matrix Xk and a new
iteration can begin. This algorithm is formally described in Figure 3.7.
While this algorithm is guaranteed to converge to a ﬁxed-point, it is not necessar-
ily the optimal one. Interestingly, Gorodnitsky and Rao show that the sequence of
solutions obtained necessarily give a descent over the function Qm
i=1 |xi| for p = 0.
Another intriguing property, already mentioned above, and obvious from Equation
(3.34), is the fact that once an entry in xk is nulled, it is never revived. This implies
that initialization of this algorithm should be non-zero for all entries, to enable each
to be chosen in the converged result.
2 The matrix Xk−1 chooses a subset of columns from A. If the number of columns is n or more,
and those span the complete space, then the inversion is the same as the pseudo-one. In case the
number of columns is below n or if their rank is smaller than n, there might be a diﬀerence between
the two inversions. Nevertheless, If in addition those columns span a space that contains b, then the
pseudo-inverse deployed provides the proper solution for λ. Otherwise, b is not contained in those
columns’ span, and no λ satisﬁes this equation anyhow. However, this option never takes place if
the algorithm prunes the columns of A wisely (requires a proof).

50
3 Pursuit Algorithms – Practice
Task: Find x that approximates the solution of (Pp): minx ∥x∥p
p subject to b =
Ax.
Initialization: Initialize k = 0, and set
•
The initial approximation x0 = 1.
•
The initial weight matrix X0 = I.
Main Iteration: Increment k by 1, and apply these steps:
•
Solve: the linear system
xk = (Xk−1)2AT 
AX2
k−1AT+ b.
either directly or iteratively (several Conjugate-Gradient iterations may
sufﬁce), producing result xk.
•
Weight Update: Update the diagonal weight matrix X using xk: Xk(j, j) =
|xk( j)|1−p/2.
•
Stopping Rule: If ∥xk −xk−1∥2 is smaller than some predetermined thresh-
old, stop. Otherwise, apply another iteration.
Output: The desired result is xk.
Fig. 3.7 The IRLS strategy for solving (Pp).
The FOCUSS algorithm is a practical strategy, but little is known about circum-
stances where it will be successful, i.e., when a numerical local minimum will actu-
ally be a good approximation to a global minimum of (P0). Another popular strategy
is to replace the ℓ0 norm by the ℓ1-norm, which is, in a natural sense, its best convex
approximant; many optimization tools are available ‘oﬀthe shelf’ for solving (P1)
(see next section).
Turning from (P0) to its relaxed version, (Pp) with 0 < p ≤1, care must be taken
with respect to normalization of the columns in A. While the ℓ0-norm is indiﬀerent
to the magnitude of the non-zero entries in x, the ℓp-norms tend to penalize higher
magnitudes, and thus bias the solution towards choosing to put non-zero entries in
x in locations that multiply large norm columns in A. In order to avoid this bias, the
columns should be scaled appropriately. Convexifying with the ℓ1-norm, the new
objective becomes:
(P1) :
min
x
∥W−1x∥1 subject to b = Ax.
(3.35)
The matrix W is a diagonal positive-deﬁnite matrix that introduces the above-
described pre-compensating weights. A natural choice for the (i, i) entry in this ma-
trix for this case is w(i, i) = 1/∥ai∥2. Assuming that A has no zero columns, all these
norms are strictly positive and the problem (P1) is well-deﬁned. The case where all
the columns of A are normalized (and thus W = I) was named Basis-Pursuit (BP)
by Chen, Donoho, and Saunders (1995). We will use this name for the more general
setup in Equation (3.35) hereafter.

3.2 Convex Relaxation Techniques
51
Considering (3.35) and deﬁning ˜x = W−1x, this problem can be reformulated as
(P1) :
min
˜x
∥˜x∥1 subject to b = AW˜x = ˜A˜x.
(3.36)
This means that, just as before, we can normalize the columns of A and use its
normalized version ˜A, getting the classic BP format. Just as for the greedy methods,
once a solution ˜x has been found, it should be de-normalized to provide the required
vector x. Thus, from now on we can safely assume that (P1) is given in a canonic
structure with a normalized matrix.
3.2.2 Numerical Algorithms for Solving (P1)
We have seen that the problem (P1) can be cast as a linear programming (LP) prob-
lem. As such, it can be solved using modern interior-point methods, simplex meth-
ods, homotopy methods, or other techniques. Such algorithms are far more sophis-
ticated than the greedy algorithms mentioned earlier, as they obtain the global solu-
tion of a well-deﬁned optimization problem. However, this also means that program-
ming these techniques is far more complicated, compared to the greedy algorithms.
There are several well-developed software packages that handle this problem, which
are freely shared over the web. These include the ℓ1-magic by Candes and Romberg,
the CVX and the L1-LS developed by Boyd and his students, Sparselab managed
by David Donoho, SparCo by Michael Friedlander, SPAMS by Julien Mairal, and
more.
We also note that the FOCUSS method we have seen above is an interesting and
far simpler algorithm for a numerical approximate solution of (P1). However, even
though (P1) is convex, FOCUSS is not guaranteed to converge to its global mini-
mum. Instead, it may get stuck on a steady-state solution (not a local minimum, as
there are none of those!!), or even give ﬂuctuations instead of descending monoton-
ically.
3.2.3 Numerical Demonstration of Relaxation Methods
We return to the tests presented in Section 3.1.7 and add two relaxation-based algo-
rithms to the comparison. The ﬁrst is the IRLS as described above, for p = 1 (i.e., it
is expecting to approximate the ℓ1 minimization). The second method tested is the
Linear-Programming solver by Matlab, for computing the solution of the Basis Pur-
suit. We should note that the BP and IRLS take much more time to run compared to
the OMP (we shall not provide a complexity analysis of these algorithms – see the
relevant literature), and therefore in this experiment we used only 200 experiments
per cardinality.

52
3 Pursuit Algorithms – Practice
1
2
3
4
5
6
7
8
9
10
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
Cardinality of the true solution
Average and Relative L2−Error
 
 
OMP
IRLS
BP by Linear Prog.
Fig. 3.8 IRLS and BP (using Matlab’s Linear-Programming) algorithms’ performance in terms of
relative ℓ2 recovery error.
Figures 3.8 and 3.9 present the obtained results. For the convenience of compar-
ison to the greedy techniques, we add the OMP performance to these graphs. It is
clear that the relaxation methods are performing much better in both measures of
quality evaluated. It is also evident that IRLS provides a good approximation to the
ℓ1 minimization as done by the Basis-Pursuit.
3.3 Summary
In this chapter we have concentrated on numerical solutions to (P0), by either greedy
techniques or relaxation ones. Each family of methods has its own merits, and
choosing between the various options depends on the application at hand. All the
presented algorithms are approximation ones, implying that they may fail from time
to time to provide the truly sparest solution of the linear system Ax = b. The natural
question to ask is whether there could be conditions under which such algorithms
are guaranteed to operate well. This is exactly the topic of our next chapter.

Further Reading
53
1
2
3
4
5
6
7
8
9
10
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
Cardinality of the true solution
Probability of Error in Support
 
 
OMP
IRLS
BP by Linear Prog.
Fig. 3.9 IRLS and BP (using Matlab’s Linear-Programming) algorithms’ performance in terms of
the success rate in detecting the truer support.
Further Reading
1. S.S. Chen, D.L. Donoho, and M.A. Saunders, Atomic decomposition by basis
pursuit, SIAM Journal on Scientiﬁc Computing, 20(1):33–61 (1998).
2. S.S. Chen, D.L. Donoho, and M.A. Saunders, Atomic decomposition by basis
pursuit, SIAM Review, 43(1):129–159, 2001.
3. A. Cohen, R.A. DeVore, P. Petrushev, and H. Xu, Nonlinear approximation and
the space BV(IR2), American Journal of Mathematics, 121(3):587-628, June,
1999.
4. G. Davis, S. Mallat, and M. Avellaneda, Adaptive greedy approximations, Jour-
nal of Constructive Approximation, 13:57–98, 1997.
5. G. Davis, S. Mallat, and Z. Zhang, Adaptive time-frequency decompositions,
Optical-Engineering, 33(7):2183–91, 1994.
6. R.A. DeVore and V. Temlyakov, Some Remarks on Greedy Algorithms, Ad-
vances in Computational Mathematics, 5:173–187, 1996.
7. I.F. Gorodnitsky and B.D. Rao, Sparse signal reconstruction from limited data
using FOCUSS: A re-weighted norm minimization algorithm, IEEE Trans. on
Signal Processing, 45(3):600–616, 1997.
8. R. Gribonval and P. Vandergheynst, On the exponential convergence of Match-
ing Pursuits in quasi-incoherent dictionaries, IEEE Trans. Information Theory,
52(1):255–261, 2006.
9. L.A. Karlovitz, Construction of nearest points in the ℓp, p even and ℓ∞norms,
Journal of Approximation Theory, 3:123–127, 1970.

54
3 Pursuit Algorithms – Practice
10. S. Mallat, A Wavelet Tour of Signal Processing, Academic-Press, 1998.
11. S. Mallat and Z. Zhang, Matching pursuits with time-frequency dictionaries,
IEEE Trans. Signal Processing, 41(12):3397–3415, 1993.
12. D. Needell and J. A. Tropp, CoSaMP: Iterative signal recovery from incomplete
and inaccurate samples, Applied Computational Harmonic Analysis, 26:301–
321, May 2009.
13. B.D. Rao and K. Kreutz-Delgado, An aﬃne scaling methodology for best basis
selection, IEEE Trans. on signal processing, 47(1):187–200, 1999.
14. V.N. Temlyakov, Greedy algorithms and m-term approximation, Journal of Ap-
proximation Theory, 98:117–145, 1999.
15. V.N. Temlyakov, Weak greedy algorithms, Advances in Computational Mathe-
matics, 5:173–187, 2000.
16. J.A. Tropp, Greed is good: Algorithmic results for sparse approximation, IEEE
Trans. On Information Theory, 50(10):2231–2242, October 2004.

Chapter 4
Pursuit Algorithms – Guarantees
Assume that the linear system Ax = b has a sparse solution with k0 non-zeros,
i.e., ∥x∥0 = k0. Furthermore, assume that k0 < spark(A)/2. Will matching pursuit
or Basis-Pursuit succeed in recovering the sparsest solution? Clearly, such success
cannot be expected for all k0 and for all matrices A, since this would conﬂict with
the known NP-hardness of the problem in the general case. However, if the equa-
tion actually has a “suﬃciently sparse” solution, the success of these algorithms in
addressing the original objective (P0) can be guaranteed.
We present here several such results that correspond to the matching pursuit
(OMP, as described in Figure 3.1), the basis pursuit (i.e., solving (P1) in place of
(P0)), and the thresholding method. We start our discussion with the special case
where A is a union of two unitary matrices, A = [Ψ, Φ], and then turn to develop
these results for the general matrix A.
Before we embark to the exciting discussion ahead of us, we should mention
that all the results derived in this chapter are worst-case ones, implying that the
kind of guarantees we obtain are over-pessimistic, as they are supposed to hold for
all signals, and for all possible supports of a given cardinality. We shall revisit this
matter in a later chapter, discussing ways to relax those assumptions.
4.1 Back to the Two-Ortho Case
4.1.1 OMP Performance Guarantee
In order to simplify our notations, we shall assume that by a rearrangement of the
columns in Ψ and Φ, the vector b is created as a linear combination of the ﬁrst kp
columns in Ψ, and the ﬁrst kq columns in Φ, such that k0 = kp + kq. Thus,
b = Ax =
kp
X
i=1
xψ
i ψi +
kq
X
i=1
xφ
i φi.
(4.1)
© Springer Science+Business Media, LLC 2010
M. Elad, Sparse and Redundant Representations: From Theory to Applications in Signal 
and Image Processing, DOI 10.1007/978-1-4419-7011-4_4,
55

56
4 Pursuit Algorithms – Guarantees
We shall further denote all the support indices as Sp and Sq, respectively, where
clearly we have that |Sp| = kp and |Sq| = kq.
At the ﬁrst step (k = 0) of the algorithm rk = r0 = b, and the set of computed
errors from the Sweep step (see Figure 3.1) are given by
ϵ( j) = min
z j ∥a jzj −b∥2
2 =
(aT
j b)aj −b
2
2 = ∥b∥2
2 −(aT
j b)2 ≥0.
Thus, for the ﬁrst step to choose one of the k0 non-zero entries in the proper support,
we should require that for all j < Sp and j < Sq we have that
(i)
ψT
1 b
 >
ψT
j b
 and (ii)
ψT
1 b
 >
φT
j b
 .
(4.2)
Here we have assumed, without loss of generality, that xψ
1 is the largest among the
non-zeros in x, and we desire to see it chosen in this greedy step. We shall return
to this assumption in a short while and handle the opposite case, when the largest
entry belongs to Sq.
We refer hereafter to the requirement (i), and then turn to treat the second in a
similar manner. Substitution of this in Equation (4.1), this requirement translates
into

kp
X
i=1
xψ
i ψT
1 ψi +
kq
X
i=1
xφ
i ψT
1 φi

>

kp
X
i=1
xψ
i ψT
j ψi +
kq
X
i=1
xφ
i ψT
j φi

.
(4.3)
In order to consider the worst-case scenario, we should construct a lower bound for
the left-hand-side, an upper-bound for the right-hand-side, and then pose the above
inequality again. For the left hand side we have

kp
X
i=1
xψ
i ψT
1 ψi +
kq
X
i=1
xφ
i ψT
1 φi

≥|xψ
1| −
kq
X
i=1
|xφ
i | · µ(A)
(4.4)
≥|xψ
1|

1 −kq · µ(A)

.
Here we have exploited the orthogonality of the columns of Ψ that causes all the
elements of the ﬁrst term to drop, apart from the ﬁrst one. We used the deﬁnition
of the mutual-coherence µ(A) in Equation (2.3), and the assumption that |xψ
1| is the
largest non-zero entry in x. We also used the inequality relations |a + b| ≥|a| −|b|
and | P
i ai| ≤P
i |ai|.
A similar treatment for the right-hand-side of Equation (4.3), this time bounding
from above, leads to

kp
X
i=1
xψ
i ψT
j ψi +
kq
X
i=1
xφ
i ψT
j φi

≤|xψ
1| · kq · µ(A).
(4.5)
Using these two bounds plugged into the inequality in (4.3), we obtain

4.1 Back to the Two-Ortho Case
57
|xψ
1|

1 −kq · µ(A)

> |xψ
1| · kq · µ(A) ⇒kq <
1
2µ(A).
(4.6)
Since all the above is done with the assumption that Sp contains the largest non-zero
entry, a parallel requirement kp <
1
2µ(A) is also necessary for the opposite case.
Returning now to requirement (ii) above, and following the same treatment, the
lower bound on the left-hand-side remains unchanged, whereas the right-hand side
becomes

kp
X
i=1
xψ
i φT
j ψi +
kq
X
i=1
xφ
i φT
j φi

≤|xψ
1| · kp · µ(A),
(4.7)
leading to the requirement
|xψ
1|

1 −kqµ(A)

> |xψ
1| · kp · µ(A) ⇒kp + kq <
1
µ(A).
(4.8)
This requirement is non-informative, as it is covered by the previous two inequal-
ities, kp, kq < 1/2µ(A). Satisfying these guarantees that the ﬁrst step of the OMP
performs well, in the sense that a non-zero is chosen from within the proper sup-
port.
Once done, the next step is an update of the residual r1. In this update, we sub-
tract a coeﬃcient multiplying the column ψtp(1), and thus the new residual is also
a linear combination of k0 non-zeros at the most, with the same supports Sp and
Sq. Thus, using the same set of steps as above guarantees that the algorithm ﬁnds
again an index from the true support of the solution, if the conditions on kp, kq are
met. Furthermore, due to the orthogonality between the chosen directions and the
residual (a property enforced by the LS,1) we never choose the same index twice.
Repeating this reasoning, the same holds true for k0 such iterations, hence the
algorithm selects always values from the correct set of indices, and always an index
that has not been chosen yet. After k0 such iterations exactly, the residual becomes
zero and the algorithm stops, ensuring the success of the overall algorithm in recov-
ering the correct solution x. All this can now be posed as a theorem for the success
of the OMP.
Theorem 4.1. (Equivalence – OMP – Two-Ortho Case): For a system of linear
equations Ax = [Ψ, Φ]x = b with two ortho-matrices Ψ and Φ of size n × n, if a
solution x exists such that it has kp non-zeros in its ﬁrst half and kq non-zeros in the
second, and the two obey
1 In the OMP algorithm, when we update the residual, we solve minxsk ∥Askxsk −b∥2
2, and the
optimal result satisﬁes AT
sk(Askx∗
sk −b) = AT
skrk = 0. This means that the current support columns
are all orthogonal to the new residual, and thus in the next sweep phase they give zero inner-
product, and are not chosen. On the other hand, in the MP algorithm the residual update is given
by rk = rk−1 −ajz∗
j0, where z∗
j0 is chosen as the minimizer of ϵ(j) = ∥ajz −rk−1∥2
2. Thus, as the
derivative of this error for the optimal z is aT
j0(aj0z∗
j0 −rk−1) = −aT
j0rk = 0, we have that the residual
is orthogonal only to the last chosen column, and thus the other might be chosen.

58
4 Pursuit Algorithms – Guarantees
max(kp, kq) <
1
2µ(A),
(4.9)
OMP run with threshold parameter ϵ0 = 0 is guaranteed to ﬁnd it exactly in k0 =
kp + kq steps.
Proof: Given above.
□
This result is the ﬁrst of its kind we show here, and its implications are far-
reaching. The ability to claim that OMP safely leads to the necessary result is won-
derful and encouraging. Interestingly, this result never appeared in a published pa-
per, and instead, a generalization of it to the general matrix A was shown. Neverthe-
less, this result is important by its own, because it will help us understand better the
gap in behaviors between OMP and BP. This leads us now naturally to the discus-
sion of the two-ortho case handled by the Basis-Pursuit.
4.1.2 BP Performance Guarantee
We now turn to analyze the performance of the Basis-Pursuit in the two-ortho case.
This follows the work of Donoho and Huo (2001) and the later improvements by
Elad and Bruckstein (2002).
Theorem 4.2. (Equivalence – Basis Pursuit – Two-Ortho Case): For a system of
linear equations Ax = [Ψ, Φ]x = b with two ortho-matrices Ψ and Φ of size n×n, if
a solution x exists such that it has kp non-zeros in its ﬁrst half and kp ≥kq non-zeros
in the second, and the two obey
2µ(A)2kpkq + µ(A)kp −1 < 0
(4.10)
that solution is both the unique solution of (P1) and the unique solution of (P0).
Since the above condition may seem obscure, we provide a weaker but more
“civilized” version of the above requirement, which is simpler to interpret:
∥x∥0 = kp + kq <
√
2 −0.5
µ(A)
.
(4.11)
Figure 4.1 presents a comparison between these two bounds and the result obtained
for the OMP. It is clear that OMP is found to be weaker compared to the BP in han-
dling the (P0) problem, and denser solutions are anticipated to lead to a successful
result with the BP. The question (left open at this stage) is whether the two bounds
are tight? The tightness of the BP result was established by Feuer and Nemirovsky,
but the tightness of the OMP remains questionable.
Proof: Deﬁne the following set of alternative solutions:

4.1 Back to the Two-Ortho Case
59
0
1
2
3
4
5
6
7
8
9
10
0
1
2
3
4
5
6
7
8
9
10
kp
kq
 
 
Uniqueness bound: 1/µ
BP bound: 0.91/µ
BP exact bound
OMP bound: max(kp,kq)=1/2µ
Fig. 4.1 The uniqueness and the equivalence bounds for the OMP and BP for the two-ortho case,
assuming µ = 0.1.
C =
(
y

y , x,
∥y∥1 ≤∥x∥1
∥y∥0 > ∥x∥0, and A(y −x) = 0
)
.
(4.12)
This set contains all the possible solutions that are diﬀerent from x, have larger sup-
port, satisfy the linear system of equations Ay = b, and are equivalent or better in an
ℓ1-length perspective. This set being non-empty implies that there is an alternative
solution that the Basis-Pursuit might ﬁnd, rather than the desired x.
The condition in Equation (4.10) guarantees, due to Theorem 2.3, that kp + kq =
∥x∥0 < 1/µ(A), x is necessarily the unique sparsest possible solution (see Figure
4.1). Thus, alternative solutions (y , x) are necessarily “denser,” and the require-
ment ∥y∥0 > ∥x∥0 can be omitted from the deﬁnition of C.
Deﬁning e = y −x, we can rewrite C as a shifted version of it around x,
Cs = {e | e , 0,
∥e + x∥1 −∥x∥1 ≤0,
and Ae = 0} .
(4.13)
The strategy of the proof we are about to present is to enlarge this set and show that
even this enlarged set is empty. This will prove that Basis-Pursuit indeed succeeds

60
4 Pursuit Algorithms – Guarantees
in recovering x. The enlargement will be done so as to simplify the description of
the set, up to the point where its volume can be assessed.
We start with the requirement ∥e + x∥1 −∥x∥1 ≤0. We shall denote by xp and xq,
and similarly ep and eq, as the two parts of the vectors x and e, respectively, each
portion corresponding to one of the unitary matrices Ψ and Φ. Using the notations
from the OMP analysis, Sp and Sq of size kp and kq representing the non-zero
supports over the two parts of x, this requirement can be re-written as
0 ≥∥e + x∥1 −∥x∥1 =
n
X
i=1
|ep
i + xp
i | −|xp
i | +
n
X
i=1
|eq
i + xq
i | −|xq
i |
(4.14)
=
X
i<Sp
|ep
i | +
X
i<Sq
|eq
i |
+
X
i∈Sp
(|ep
i + xp
i | −|xp
i |) +
X
i∈Sq
(|eq
i + xq
i | −|xq
i |).
Here we have exploited the facts that xp
i and xq
i are zeros outside their support. Using
the inequality |a+b|−|b| ≥−|a|, we can relax the above condition and demand instead
0 ≥
X
i<Sp
|ep
i | +
X
i<Sq
|eq
i |
(4.15)
+
X
i∈Sp
(|ep
i + xp
i | −|xp
i |) +
X
i∈Sq
(|eq
i + xq
i | −|xq
i |)
≥
X
i<Sp
|ep
i | +
X
i<Sq
|eq
i | −
X
i∈Sp
|ep
i | −
X
i∈Sq
|eq
i |.
The new inequality poses a weaker requirement over the vector e, and thus, with
this replacement of the condition we have enlarged the set Cs. The new inequality
can be written more compactly by adding and subtracting the terms P
i∈Sp |ep
i | and
P
i∈Sq |eq
i | and denoting them as 1T
p · |ep| and 1T
q · |eq| to indicate that it is a sum of the
non-zeros of the vector |ep| and |eq|, respectively. This leads to
∥ep∥1 + ∥eq∥1 −2 · 1T
p · |ep| −2 · 1T
q · |eq| ≤0,
(4.16)
Thus, substituting into the deﬁnition of Cs we get
Cs ⊆
e

e , 0
∥ep∥1 + ∥eq∥1 −2 · 1T
p · |ep| −2 · 1T
q · |eq| ≤0
and Ae = 0
.
(4.17)
We shall denote this set as C1
s.
We now turn to handle the requirement Ae = Ψep + Φeq = 0, replacing it with
a relaxed requirement that expands the set C1
s further. First, a multiplication by AT
yields the conditions

4.1 Back to the Two-Ortho Case
61
ep + ΨTΦeq = 0 and ΦTΨep + eq = 0.
(4.18)
Notice that every entry in the matrix ΨTΦ (and its adjoint) is an inner product
bounded from above by the mutual-coherence µ(A). Thus, applying absolute value
on the above relationships leads to
|ep| = |ΨTΦeq| ≤µ(A) · 1 · |eq|
and
(4.19)
|eq| = |ΦTΨep| ≤µ(A) · 1 · |ep|.
The term 1 stands for a rank-1 n × n matrix ﬁlled with ones. Returning to the set C1
s,
we can write
C1
s ⊆

e

e , 0
∥ep∥1 + ∥eq∥1 −2 · 1T
p · |ep| −2 · 1T
q · |eq| ≤0
|ep| ≤µ(A) · 1 · |eq|
and |eq| ≤µ(A) · 1 · |ep|

= C2
s.
(4.20)
Deﬁning f p = |ep| and fq = |eq|, the above can be written diﬀerently as
C f =

f

f , 0
1Tf p + 1Tfq −2 · 1T
p · f p −2 · 1T
q · fq ≤0
f p ≤µ(A) · 1 · fq
fq ≤µ(A) · 1 · f p
and f p ≥0, fq ≥0

.
(4.21)
The obtained set Cf is unbounded since if f ∈C f then αf ∈C f for all α ≥0.
Thus, in order to study its behavior, we can restrict our quest for normalized vectors,
1Tf = 1Tf p + 1Tfq = 1. This new set, denoted as Cr becomes
Cr =

f

f , 0
1 −2 · 1T
p · f p −2 · 1T
q · fq ≤0
f p ≤µ(A) · 1 · fq
fq ≤µ(A) · 1 · f p
1T(f p + fq) = 1
and f p ≥0, fq ≥0

.
(4.22)
The set Cr is very comfortable to analyze because of several reasons:
• Compared to the deﬁnition of the set C, in Cr there is no explicit use for A, and
instead, the mutual-coherence represents it. Similarly, there is no trace to the
optimal solution x.
• The set Cr is posed in terms of linear constraints and positivity, which make it
possible to convert to a Linear-Programming problem.
• The order of the non-zeros in f p and fq has no inﬂuence on the solution, and thus,
without loss of generality, we can assume that the kp,kq non-zeroes are located at
the start of these vectors.

62
4 Pursuit Algorithms – Guarantees
n = 50; kp = 7; kq = 9; mu = 0.1; % Setting parameters
C = [ones(1,kp), ...
% Penalty
zeros(1,n-kp), ...
ones(1,kq), ...
zeros(1,n-kq)];
A = [ones(1,2*n);
% Inequality Constraints Matrix
-ones(1,2*n);
eye(n), -ones(n)*mu;
-ones(n)*mu, eye(n)];
b=[1; -1; zeros(2*n,1)];
% Inequality Constraints vector
x=linprog(-C,A,b,[],[],zeros(2*n,1)); % Linear Programming Solver
plot(x,’.’);
% Showing the Result
Fig. 4.2 Matlab code for solving the LP problem posed in Equation (4.23).
We can now deﬁne the following Linear-Programming problem based on Cr,
max
f p,fq
1T
p · f p + 1T
q · fq
subject to
f p ≤µ(A) · 1 · fq
fq ≤µ(A) · 1 · f p
1T(f p + fq) = 1
f p ≥0, fq ≥0
.
(4.23)
The idea is the following. Suppose we are able to ﬁnd the solution to this problem,
f p
opt, fq
opt. If the penalty term satisﬁes 1T
p · f p
opt + 1T
q · fq
opt < 0.5, this implies that the
ﬁrst requirement in the deﬁnition of the set Cr is not met, implying that this set is
empty, and thus BP succeeds. As we shall show, for kp and kq small enough, this is
exactly the situation.
Solving the above LP problem is quite easy, and especially so when a numerical
solution is considered. Figure 4.2 presents a short Matlab program that does exactly
that – try it, and see how the solution behaves.
Based on such numerical tests, if we assume kq ≥kp, the following parametric
solution be be proposed:
f p
opt = [α · 1T
kp, β · 1T
n−kp]T
and fq
opt = [γ · 1T
kq, δ · 1T
n−kq]T,
(4.24)
i.e., the ﬁrst kp entries in f p
opt are the same, equal to α, and the rest are also the same,
equal to β. A similar structure is imposed on fq
opt with values γ and δ. This solution
can be tested for feasibility and lead to the desired result, but there is a problem
with such an approach: Suppose we show that the proposed result does not lead to
a penalty below 1/2. How can we be sure that this is truly the optimal solution, and
we have truly exhausted all other possibilities?
The approach taken in the original work by Elad and Bruckstein was to introduce
the dual to the above problem. The dual is a minimization problem, known to bound
the primal (original) penalty from above, and have the same optimal value. Thus, by

4.1 Back to the Two-Ortho Case
63
suggesting (wisely) a solution to the dual and checking that it does not exceed 1/2,
we prove conclusively that the primal is smaller than 1/2 as well.
Here we shall take a diﬀerent, and simpler, route. We address the solution of
(4.23), and denote ∥f p
opt∥1 = α, and thus ∥fq
opt∥1 = 1 −α, due to the last constraint,
1T(f p + fq) = 1. The ﬁrst and second constraints translate into
f p ≤µ(A) · 1 · fq = (1 −α)µ(A) · 1
(4.25)
fq ≤µ(A) · 1 · f p = αµ(A) · 1.
Since the objective is the maximization of the term 1T
p · f p + 1T
q · fq, it is natural to
choose all the non-zero entries as large as possible, based on the constraints posed
in Equation (4.25). Thus,
1T
p · f p + 1T
q · fq = kp(1 −α)µ(A) + kqαµ(A)
(4.26)
= kpµ(A) −α(kp −kq)µ(A).
Since we assume that kp ≥kq, it is clear that we should choose α as small as possible
so as to maximize this expression. We now add the following two conditions, stating
that the energy allocated to the non-zeros in the two vectors should not exceed the
entire (ℓ1) energy,
∥f p
opt∥1 = 1Tf p = α ≥kp(1 −α)µ(A)
(4.27)
∥fq
opt∥1 = 1Tfq = (1 −α) ≥kqαµ(A).
These lead to the following inequalities, that pose limits on the value of α:
α ≥kp(1 −α)µ(A)
⇒α ≥
kpµ(A)
1 + kpµ(A)
(4.28)
(1 −α) ≥kq · αµ(A)
⇒α ≤
1
1 + kqµ(A).
For these two conditions to have an intersection we should require
kpµ(A)
1 + kpµ(A) ≤
1
1 + kqµ(A)
⇒kpkq ≤
1
µ(A)2 .
(4.29)
Assuming that this condition is met (and we should verify this at the end, but looking
at Figure 4.1, this is obviously met), the smallest possible value for α is
kpµ(A)
1+kpµ(A), and
thus the maximal penalty in Equation (4.26) becomes
1T
p · f p + 1T
q · fq = kpµ(A) −αµ(A)(kp −kq)
(4.30)
= kpµ(A) −
kpµ(A)
1 + kpµ(A)µ(A)(kp −kq)
= kpµ(A) + kpkqµ(A)2
1 + kpµ(A)
.

64
4 Pursuit Algorithms – Guarantees
For BP to succeed, we should require that this expression is strictly smaller than 1/2
leading to
kpµ(A) + kpkqµ(A)2
1 + kpµ(A)
< 1
2
(4.31)
⇒2kpkqµ(A)2 + kpµ(A) −1 < 0,
which is the claimed requirement.
In order to prove the alternative inequality ∥x∥0 = kp + kq <
√
2−0.5
µ(A) , we will take
the above requirement, isolate the term kq and then add kp to it, leading to
kp + kq = ∥x∥0 < kp + 1 −kpµ(A)
2µ(A)2kp
(4.32)
=
2µ(A)2k2
p + 1 −µ(A)kp
2µ(A)2kp
=
1
µ(A) · 2(µ(A)kp)2 + 1 −(µ(A)kp)
2(µ(A)kp)
,
By minimizing the upper-bound with respect to the term µ(A)kp we obtain
f(u) = 2u2 −u + 1
2u
⇒
f ′(u) = 2u(4u −1) −2(2u2 −u + 1)
4u2
= 2u2 −1
2u2
= 0.
The optimal value (should be checked for being a minimum and not maximum) is
thus µ(A)kp = ±
√
0.5. The negative solution is irrelevant (since both µ(A) and kp
and non-negative), and we obtain
kp + kq = ∥x∥0 <
√
2 −0.5
µ(A)
(4.33)
≤
1
µ(A) · 2(µ(A)kp)2 + 1 −(µ(A)kp)
2(µ(A)kp)
,
as claimed.
□
4.2 The General Case
We now turn to handle the general case with an arbitrary matrix A. As already shown
in the previous chapter, we can assume, without loss of generality, that the columns

4.2 The General Case
65
of this matrix are normalized, since if this is not the case, we can normalize it and
de-normalize the outcome at the end, getting the very same solution.
4.2.1 OMP Performance Guarantee
The following result generalizes the one in Theorem 4.1 for general matrices, and
therefore it is necessarily weaker. The origin of this weakness is the fact that in the
two-ortho case, the Gram matrix contains some inner-products that are known to
be zero, while here we do not assume such a structure. The proof provided for the
general theorem is very much similar to the one shown above, with several necessary
modiﬁcations.
Theorem 4.3. (Equivalence – Orthogonal Greedy Algorithm): For a system of
linear equations Ax = b (A ∈IRn×m full-rank with n < m), if a solution x exists
obeying
∥x∥0 < 1
2
 
1 +
1
µ(A)
!
,
(4.34)
OMP (OGA) run with threshold parameter ϵ0 = 0 is guaranteed to ﬁnd it exactly.
Proof: Suppose, without loss of generality, that the sparsest solution of the linear
system is such that all its k0 non-zero entries are at the beginning of the vector, in
decreasing order of the values |xj|. Thus,
b = Ax =
k0
X
t=1
xtat.
(4.35)
At the ﬁrst step (k = 0) of the algorithm rk = r0 = b, and the set of computed errors
from the Sweep step are given by
ϵ( j) = min
z j ∥a jzj −b∥2
2 =
ajaT
j b −b
2
2 = ∥b∥2
2 −(aT
j b)2 ≥0.
Thus, for the ﬁrst step to choose one of the ﬁrst k0 entries in the vector (and thus do
well), we must require that for all i > k0 (columns outside the true support)
aT
1 b
 >
aT
i b
 .
(4.36)
Substitution of this in Equation (4.35), this requirement translates into

k0
X
t=1
xtaT
1 at
 >

k0
X
t=1
xtaT
i at
 .
(4.37)

66
4 Pursuit Algorithms – Guarantees
Again, we construct a lower bound for the left-hand-side, an upper-bound for the
right-hand-side, and then pose the above requirement again. For the left-hand-side
we have

k0
X
t=1
xtaT
1 at
 ≥|x1| −
k0
X
t=2
|xt| ·
aT
1 at

≥|x1| −
k0
X
t=2
|xt| · µ(A)
(4.38)
≥|x1| (1 −µ(A)(k0 −1)) .
Here we have exploited the deﬁnition of the mutual-coherence µ(A) in Equation
(2.21), and the descending ordering of the values |x j|. Similarly, the right-hand-side
term is bounded by

k0
X
t=1
xtaT
i at
 ≤
k0
X
t=1
|xt| ·
aT
i at

≤
k0
X
t=1
|xt| · µ(A)
(4.39)
≤|x1| · µ(A)k0.
Using these two bounds plugged into the inequality in (4.37), we obtain

k0
X
t=1
xtaT
1 at
 ≥|x1| · (1 −µ(A)(k0 −1))
(4.40)
> |x1| · µ(A)k0
≥

k0
X
t=1
xtaT
i at
 ,
which leads to
1 + µ(A) > 2µ(A)k0
⇒
k0 < 1
2
 
1 +
1
µ(A)
!
,
(4.41)
which is exactly the condition of sparsity stated above. This condition guarantees
the success of the ﬁrst stage of the algorithm, implying that the chosen element
must be in the correct support of the sparsest decomposition.
Once done, the next step is an update of the residual, and since this is done by
decreasing a term proportional to a1 (or any other atom from within the correct
support), this residual is also a linear combination of the same k0 columns in A at
the most. Using the same set of steps we obtain that condition (4.41) guarantees
that the algorithm ﬁnds again an index from the true support of the solution, and
the orthogonality enforced by the LS guarantees that the same entry is never chosen

4.2 The General Case
67
again. After k0 such iterations, the residual becomes zero and the algorithm stops,
ensuring the success of the overall algorithm in recovering the correct solution x as
the theorem claims.
□
4.2.2 Thresholding Performance Guarantee
The thresholding algorithm presented in the previous chapter is very appealing due
to its evident simplicity. The question we should ask is whether such simple method
could lead to a guaranteed performance? We provide the following analysis that fol-
lows much of the ﬂavor of the above greedy method, and using the same notations.
Success of the thresholding algorithm is guaranteed by the requirement
min
1≤i≤k0 |aT
i b| > max
j>k0 |aT
j b|,
(4.42)
Plugging the expression b = Pk0
t=1 xtat, the left-hand-side term becomes
min
1≤i≤k0 |aT
i b| = min
1≤i≤k0

k0
X
t=1
xtaT
i at
 .
(4.43)
We exploit the fact that the columns of A are normalized, their maximal inner prod-
uct is bounded from above by µ(A), and we also use the relation |a + b| ≥|a| −|b|.
All these enable us to lower-bound this term by
min
1≤i≤k0

k0
X
t=1
xtaT
i at
 = min
1≤i≤k0
xi +
X
1≤t≤k0, t,i
xtaT
i at

(4.44)
≥min
1≤i≤k0
|xi| −

X
1≤t≤k0, t,i
xtaT
i at


≥min
1≤i≤k0 |xi| −max
1≤i≤k0

X
1≤t≤k0, t,i
xtaT
i at

≥|xmin| −(k0 −1)µ(A)|xmax|.
|xmin| and |xmax| and the minimum and maximum values of the vector |x| on the
support 1 ≤t ≤k0.
Turning to the right-hand-side term in Equation (4.42), and following similar
steps, we aim to upper-bound this expression. This gives
max
j>k0 |aT
j b| = max
j>k0

k0
X
t=1
xtaT
j at
 ≤k0µ(A)|xmax|.
(4.45)

68
4 Pursuit Algorithms – Guarantees
Thus, requiring
|xmin| −(k0 −1)µ(A)|xmax| > k0µ(A)|xmax|
(4.46)
necessarily leads to the satisfaction of the condition posed in Equation (4.42). Put
diﬀerently, this reads as the condition
k0 < 1
2
 |xmin|
|xmax|
1
µ(A) + 1
!
(4.47)
which guarantees the success of the thresholding algorithm. As can be seen, this
requirement is more strict, compared to the OMP requirement found above, and
becomes equivalent to it if all the non-zero entries in x are of the same magnitude.
We conclude by posing this as a theorem.
Theorem 4.4. (Equivalence – Thresholding Algorithm): For a system of linear
equations Ax = b (A ∈IRn×m full-rank with n < m), if a solution x (with minimal
non-zero value |xmin| and maximal one |xmax|) exists obeying
∥x∥0 < 1
2
 
1 +
1
µ(A)
|xmin|
|xmax|
!
,
(4.48)
the Thresholding algorithm run with threshold parameter ϵ0 = 0 is guaranteed to
ﬁnd it exactly.
4.2.3 BP Performance Guarantee
We now turn to consider Basis-Pursuit, i.e., the replacement of (P0) by (P1) as an
optimization problem. Surprisingly, the same bound on the sparsity as in the OMP
analysis also guarantees the success of basis pursuit. Note, however, that this does
not imply that the two algorithms are always expected to perform similarly! Indeed,
we have already seen a gap between these two methods for the two-ortho case.
These two general case Theorems do suggest that the worst-case behavior of the two
techniques is the same, provided that the two bounds are tight. Empirical evidence
will be presented in later chapter.
Theorem 4.5. (Equivalence – Basis Pursuit): For the system of linear equations
Ax = b (A ∈IRn×m full-rank with n < m), if a solution x exists obeying
∥x∥0 < 1
2
 
1 +
1
µ(A)
!
,
(4.49)
that solution is both the unique solution of (P1) and the unique solution of (P0).
Proof: The proof is similar to the one provided for the two-ortho case, with some
necessary modiﬁcations. In fact, this proof is easier in some ways.

4.2 The General Case
69
Deﬁne the following set of alternative solutions:
C =

y

y , x
∥y∥1 ≤∥x∥1
∥y∥0 > ∥x∥0
and A(y −x) = 0

.
(4.50)
This set contains all the possible solutions that are diﬀerent from x, have larger
support, satisfy the linear system of equations Ay = b, and are at least as good
from the weighted ℓ1 perspective. This set being non-empty implies that there is an
alternative solution that the Basis-Pursuit will ﬁnd, rather than the desired x.
In view of Theorem 2.5 and the fact that ∥x∥0 < (1 + 1/µ(A))/2, x is necessarily
the unique sparsest possible solution, hence alternative solutions (y , x) are nec-
essarily “denser.” Thus, this requirement can be omitted from the deﬁnition of C.
Deﬁning e = y −x, we can rewrite C as a shifted version of it around x,
Cs = {e | e , 0,
∥e + x∥1 −∥x∥1 ≤0,
and Ae = 0} .
(4.51)
The strategy of the proof we are about to present is to enlarge this set and show that
even this enlarged set is empty. This will prove that Basis-Pursuit indeed succeeds
in recovering x.
We start with the requirement ∥e + x∥1 −∥x∥1 ≤0. Assuming, without loss of
generality, that by a simple column permutation of A, the k0 non-zeros in x are
located at the beginning of the vector, this requirement can be written as
∥e + x∥1 −∥x∥1 =
k0
X
j=1
|e j + xj| −|x j| +
X
j>k0
|ej| ≤0
(4.52)
Using the inequality |a+b|−|b| ≥−|a|, we can relax the above condition and demand
instead
−
k0
X
j=1
|e j| +
X
j>k0
|e j| ≤
k0
X
j=1
|ej + xj| −|xj| +
X
j>k0
|e j| ≤0.
(4.53)
This inequality can be written more compactly by adding and subtracting the term
Pk0
j=1 |e j| and denoting it as 1T
k0 · |e| to indicate that it is a sum of the ﬁrst k0 entries of
the vector |e|. This leads to
∥e∥1 −21T
k0 · |e| ≤0.
(4.54)
Thus, substituting into the deﬁnition of Cs we get
Cs ⊆
n
e
 e , 0,
∥e∥1 −21T
k0 · |e| ≤0,
and Ae = 0
o
= C1
s.
(4.55)

70
4 Pursuit Algorithms – Guarantees
As stated above, the modiﬁcation from the condition ∥e + x∥1 −∥x∥1 ≤0 to the new
alternative ∥e∥1 −21T
k0 · |e| ≤0 is eﬀectively enlarging the set, because every vector
e satisfying the initial condition must also satisfy the new one, but not vice-versa,
We now turn to handle the requirement Ae = 0, replacing it with a relaxed re-
quirement that expands the set C1
s further. First, a multiplication by AT yields the
condition ATAe = 0, which does not yet change the set C1
s. Every entry in the matrix
ATA is a normalized inner product used for the deﬁnition of the mutual-coherence
µ(A). Also, the main-diagonal of this matrix contains ones. Thus, this condition can
be rewritten by adding and removing e, as follows
−e = (ATA −I)e.
(4.56)
Taking an entry-wise absolute value on both sides we relax the requirement on e and
obtain
|e| = |(ATA −I)e| ≤|ATA −I| · |e|
(4.57)
≤µ(A)(1 −I) · |e|.
Here we have used the relation | P
i givi| ≤P
i |gi||vi|. This relation could be inter-
preted as the eﬀect of one row of a matrix G multiplying the vector v, implying that
the j-th element in such a multiplication satisﬁes |Gv| j ≤(|G||v|)j.
The term 1 stands for a rank-1 matrix ﬁlled with ones. In the last step above we
have used the deﬁnition of the mutual-coherence and the fact that it bounds from
above all normalized inner products of the columns of A. Returning to the set C1
s,
we can write
C1
s ⊆

e
 e , 0, ∥e∥1 −21T
k0 · |e| ≤0
and |e| ≤
µ(A)
1+µ(A)1 · |e|
= C2
s.
(4.58)
The obtained set C2
s is unbounded since if e ∈C2
s then αe ∈C2
s for all α , 0.
Thus, in order to study its behavior, we can restrict our quest for normalized vectors,
∥e∥1 = 1. This new set, denoted as Cr becomes
Cr =
(
e
 ∥e∥1 = 1, 1 −21T
k0 · |e| ≤0, and |e| ≤
µ(A)
1 + µ(A)1
)
.
(4.59)
In the last condition we have used the relation 1|e| = 1 · 1T|e|, and the fact that
1T|e| = ∥e∥1 = 1.
In order for the vector e to satisfy the requirement 1 −21T
k0 · |e| ≤0, one needs to
concentrate its energy in its ﬁrst k0 entries. However, the requirements ∥e∥1 = 1 and
|ej| ≤µ(A)/(1 + µ(A)) restrict these k0 entries to be exactly |ej| = µ(A)/(1 + µ(A)),
because these are the maximal allowed values. Thus, returning to the ﬁrst condition
we get the requirement
1 −21T
k0 · |e| = 1 −2k0
µ(A)
1 + µ(A) ≤0.
(4.60)

4.3 The Role of the Sign-Pattern
71
This means that if k0 is less than (1 + 1/µ(A))/2, the set will be necessarily empty,
hence implying that Basis-Pursuit leads to the desired solution as claims.
□
The above proof amounts to showing that if there are two solutions to an inco-
herent underdetermined system, one of them being sparse, moving along the line
segment between the two solutions causes an increase in the ℓ1-norm as we move
away from the sparse solution.
Historically, The general BP result was found before the OMP one, and the two
were preceded by the two-ortho case analysis, which provided a kind of existence
proof that something interesting could be said about solving underdetermined sys-
tems of equations under some conditions. The fact that the same form of assump-
tions lead to the same form of result for both algorithms is tantalizing: is there some
deeper meaning? To our aid comes the two-ortho case, in which we saw that the two
methods actually depart, with better performance to BP.
4.2.4 Performance of Pursuit Algorithms – Summary
The above-described theorems motivate using OMP and BP to approximate the so-
lution of (P0), and as such, they are conceptually important. However, these results
are weak; they guarantee success only when the object is extremely sparse, with
fewer than √n non-zeros out of n. In many cases such sparsity is out of the ques-
tion.
Attempts to further strengthen the above results were made based on the Babel
function (see Chapter 2), leading to somewhat sharper results. However, in cases
where the matrix A has an unfavorable coherence value, i.e., µ(A) ≈1, even the
‘strengthened’ conclusions are quite weak. Actually, incoherence oﬀers only a par-
tial description of the situations under which sparsity-seeking methods succeed. In
the case of random matrices A, much more favorable results are possible. Empiri-
cal evidence by extensive simulations consistently show that OMP and BP perform
well even in situations violating the above bounds. This reﬂects the worst-case na-
ture of the analysis behind the above results. We shall return to this matter at a later
chapter, considering a probabilistic viewpoint that yields reasonable and hence more
optimistic bounds.
4.3 The Role of the Sign-Pattern
Suppose that for a linear system of equations Ax = b we hold a candidate sparse
solution x with ∥x∥0 < spark(A)/2. Clearly, this is the sparsest possible solution
for this system, and thus, it is the global minimizer of (P0). In the analysis of BP
(posed as the problem (P1)) we consider a competitive solution, y , x, such that
Ay = b, which may lead to a shorter ℓ1 length. Thus, we have studied the diﬀerence

72
4 Pursuit Algorithms – Guarantees
∥y∥1 −∥x∥1, such that if it is negative, it implies a failure of BP. In our attempt to
simplify the analysis, we deﬁned e = y−x, and bounded this diﬀerence from below,
by
∥e + x∥1 −∥x∥1 ≥1T
sc · |e| −1T
s · |e| = ∥e∥1 −21T
s · |e|,
(4.61)
where 1s and 1sc are indicator vectors, with 1-es on the support s of x and zero
elsewhere or vice-versa.
The vector zx = sign(x) is zero outside the support of x, and its values are ±1 on
the support, depending on the sign of the entries in s. Clearly, 1s = |zx|, and thus, the
above lower-bound can be written diﬀerently as
∥e + x∥1 −∥x∥1 ≥∥e∥1 −2|zx|T · |e|.
(4.62)
However, this alternative expression is of no important consequence.
If y is indeed a competitive solution with a shorter ℓ1-length (i.e., ∥x∥1 > ∥y∥1),
then ∥x + ϵe∥1 < ∥x∥1 for any 0 < ϵ ≤1, due to the convexity of the ℓ1 function:
∥x + ϵe∥1 = ∥(1 −ϵ)x + ϵy∥1
(4.63)
≤(1 −ϵ)∥x∥1 + ϵ∥y∥1
= ∥x∥1 −ϵ(∥x∥1 −∥y∥1) < ∥x∥1.
Thus, we choose ϵ such that ϵ|e| < |x| on the support of x, and clearly ϵ > 0. With
this change, we can write
∥e + x∥1 −∥x∥1 = 1T
sc · |e| + zT
x · e.
(4.64)
The ﬁrst term, 1T
sc · |e| corresponds to the oﬀ-support part of the summation. The
second term, zT
x · e, stems from the formula2 |a + b| −|a| = sign(a)b for |b| < |a|.
Adding and subtracting 1T
s · |e| = |zx|T · |e| in the above equation leads to
∥e + x∥1 −∥x∥1 = 1T
sc · |e| + zT
x · e = ∥e∥1 −|zx|T · |e| + zT
x · e.
(4.65)
Note that this is not a bound, but an exact equality. Therefore, the study of the
success of BP concentrates on checking whether the set
n
e| e , 0, ∥e∥1 −|zx|T · |e| + zT
x · e < 0, and Ae = 0
o
(4.66)
is non-empty. This implies that in analyzing whether there could exist a competi-
tive solution to x, all that is actually used is its sign-pattern vector zx, and any two
sparse vectors having the same sign-pattern will necessarily behave the same (lead
to success or failure). Thus, in checking the performance of BP for representations
with cardinality k, all we need to do is to sweep through all the possible 2k−1 sign-
2 This formula is easy to verify: If a¿0 and —a—=a¿—b—, then |a + b| −|a| = a + b −a = b.
Similarly, if a¡0 we have |a + b| −|a| = −a −b + a = −b, as claimed.

4.4 Tropp’s Exact Recovery Condition
73
patterns,3 and check for each. While this is unreasonable for large values of k, it is
still far better than a test that takes into account magnitudes of the non-zeros as well.
4.4 Tropp’s Exact Recovery Condition
It would be impossible to conclude our discussion on the guarantee results for pur-
suit performance without mentioning the alternative and illuminating analysis de-
veloped by Joel A. Tropp in his “Greed Is Good” paper. This analysis is based on
the deﬁnition of an Exact Recovery Condition (ERC). While this condition by itself
is non-constructive, it brings an additional and interesting insight towards the per-
formance limits of the OMP and the BP, and it eventually leads to a diﬀerent and
easier way to prove Theorems 4.3 and 4.5.
We shall denote a support of interest as S, and the sub-matrix from A that con-
tains the columns of this support by AS. We now turn to deﬁne the ERC.
Deﬁnition 4.1. For a given support S and for a matrix A, the Exact Recovery Con-
dition (ERC) is given by
ERC(A, S) :
max
i<S ∥A+
Sai∥1 < 1.
(4.67)
This condition essentially considers linear systems of equations of the form
ASx = ai, where ai is a column from A that is outside the support S. The ERC
states that the minimum (ℓ2-)energy solution to all these systems should all have an
ℓ1-length smaller than 1.
The importance of the ERC comes from its strong connection to the success of
pursuit techniques:
Theorem 4.6. (ERC and Pursuit Performance:) For a sparse vector x over the
support S, which is the solution of the linear system Ax = b, if the ERC is met, then
both OMP and BP are guaranteed to succeed in recovering x.
While this is a stronger condition for the success of the pursuit algorithms, com-
pared to the ones posed using the mutual-coherence, it is not constructive – one
cannot test it unless the support is known. Furthermore, if we only know the cardi-
nality of the solution, |S|, and we desire to use the ERC, we should test this condition
for all the
 m
|S|

possibilities, in order to guarantee a success, and this is of course pro-
hibitive. Let us prove the above theorem, and show how the ERC is tied to the OMP
and BP success, and then discuss what should be done to turn this vague test into a
more practical claim. Our next discussion follows the steps taken by Tropp.
Proof: We start with the OMP analysis, and assume that we are at the ﬁrst stage of
the algorithm. For this round to end well, we should require that the term ∥AT
Sb∥∞
3 Note that if a given sign-pattern is known to lead to failure/success, this immediately states the
same for its negative pattern, −zx, simply by multiplying the solution for e by −1.

74
4 Pursuit Algorithms – Guarantees
(maximal inner product of b with one of the support atoms) is larger than the maxi-
mal inner product with atoms outside the support, ∥AT
Scb∥∞. Thus, we require
ρ =
∥AT
Scb∥∞
∥AT
Sb∥∞
< 1.
(4.68)
Now, recall that b = ASxS, and thus
(AT
S)+AT
Sb = AS(AT
SAS)−1AT
Sb
= AS(AT
SAS)−1AT
SASxS
= ASxS
= b,
as this is a linear projection operation onto the span of AS. Thus,
ρ =
∥AT
Scb∥∞
∥AT
Sb∥∞
=
∥AT
Sc(AT
S)+AT
Sb∥∞
∥AT
Sb∥∞
≤∥AT
Sc(AT
S)+∥∞.
(4.69)
Here we have replaced the original ratio with the ℓ∞-induced norm.4 on the operator
AT
Sc(AT
S)+. This norm is computed as the maximum absolute row-sum (∥B∥∞=
maxi
P
j |bij), and it is equal to the maximum absolute column-sum of the transposed
matrix, which is non other than the ℓ1-induced norm, ∥B∥1 = maxj
P
i |bij). Note that
the pseudo-inverse and the transpose operations applied on AS commute. Thus we
obtain
∥AT
Sc(AT
S)+∥∞= ∥A+
SASc∥1.
(4.70)
Thus, by requiring ∥A+
SASc∥1 < 1, we guarantee that ρ < 1 and thus the ﬁrst step
of the OMP is successful. The obtained requirement is non-other than the ERC, as
posed in Equation (4.67) (as it is the maximum over all the absolute column-sums).
The above implies that the ﬁrst atom to be chosen is from within the true support
S, and thus the residual obtained as a consequence is necessarily within the span
of AS as before. Thus, all the above analysis remains the same, with the residual
rk replacing b, and this leads to the success of all |S| steps of the overall OMP, as
claimed.
Turning to the analysis of the BP, we consider a candidate alternative solution y,
as we did in Section 4.2.3. We deﬁne the diﬀerence between this solution and the
true one as e = y −x, and we already saw that this implies that e in the null-space
of A. Breaking e into two parts eS and eSc, such that 0 = Ae = ASeS + ASceSc, we
get that
eS = −A+
SASceSc.
(4.71)
4 An induced-norm of a matrix B is deﬁned via a vector norm, by maximizing the norm of the
vector ∥Bv∥over all vectors v on the sphere ∥v∥= 1, i.e., ∥B∥= maxv ∥Bv∥/∥v∥.

4.4 Tropp’s Exact Recovery Condition
75
We already saw in Equation (4.53) that for the BP to succeed, we should demand
that this error vector concentrates most of its ℓ1-energy outside the support, namely,
∥eS∥1 < ∥eSc∥1.
(4.72)
Plugging in the relation we have in Equation (4.71), this reads
∥eS∥1 = ∥A+
SASceSc∥1 < ∥eSc∥1.
(4.73)
Since ∥A+
SASceSc∥1 ≤∥A+
SASc∥1 · ∥eSc∥1 (with the ℓ1-induced norm), this means that
if the ERC is met, BP is guaranteed to succeed, as claimed.
□
In order to make all the above practical, one needs to ﬁnd a simple alternative
to verify the correctness of the ERC. As Tropp shows in his work, this can be done
using the mutual-coherence or the babel function. We shall concentrate here in the
relation to the mutual-coherence, in a way that immediately reveals the connection
between this discussion and Theorems 4.3 and 4.5.
Theorem 4.7. For a matrix A with mutual-coherence µ(A), if k < 0.5(1 + 1/µ(A)),
then for all supports S with cardinality equal or smaller than k, the ERC is satisﬁed.
Proof: For an arbitrary i-th column from the oﬀ-support, we require ∥A+
Sai∥1 =
∥(AT
SAS)−1AT
Sai∥1 < 1. Replacing this with a more strict requirement can be done
by demanding ∥(AT
SAS)−1∥1 · ∥AT
Sai∥1 < 1. The vector AT
Sai has entries in the range
[−µ(A), µ(A)], and thus ∥AT
Sai∥1 ≤|S|µ(A).
The matrix AT
SAS is strictly diagonally-dominant (SDD), for which using the
Ahlberg – Nilson – Varah bound we obtain5
∥(AT
SAS)−1∥1 ≤
1
1 −(|S| −1)µ(A).
(4.74)
Combined with the above we get the condition
∥(AT
SAS)−1∥1 · ∥ASai∥1 ≤
|S|µ(A)
1 −(|S| −1)µ(A) < 1.
(4.75)
This leads to the claimed relationship |S| < 0.5(1 + 1/µ(A)). As this condition is
the same for all columns ai, we conclude that if |S| < 0.5(1 + 1/µ(A)), the ERC is
satisﬁed.
□
5 Proving this very same bound for the ℓ2-induced norm ∥(AT
SAS)−1∥2 is very easy, stemming from
the minimal eigenvalue of AT
SAS being 1 −(|S| −1)µ(A). However, for the ℓ1-induced norm, we
rely on the above-mentioned paper.

76
4 Pursuit Algorithms – Guarantees
4.5 Summary
In a concentrated eﬀort during the years 2001-2006, the above-described equiva-
lence results have been obtained in a series of works. While the ability to guarantee
the success of pursuit techniques is surprising and encouraging, it is heavily shad-
owed by the relatively pessimistic bounds presented here. In Chapter 7 we return to
this problem, demonstrate it, and describe more recent work that aimed at getting
more optimistic bounds to reﬂect the empirical observations.
Another limitation of the results posed here is the fact that (P0) is rarely the prac-
tical problem we would be interested to solve, as requiring exact equality Ax = b is
too strict and thus naive. We now turn to a new chapter that relaxes this assumption.
Further Reading
1. J.H. Ahlberg and E.N. Nilson, Convergence properties of the spline ﬁt, J. SIAM,
11:95–104, 1963.
2. S.S. Chen, D.L. Donoho, and M.A. Saunders, Atomic decomposition by basis
pursuit, SIAM Journal on Scientiﬁc Computing, 20(1):33–61 (1998).
3. S.S. Chen, D.L. Donoho, and M.A. Saunders, Atomic decomposition by basis
pursuit, SIAM Review, 43(1):129–159, 2001.
4. C. Couvreur and Y. Bresler, On the optimality of the Backward Greedy Algo-
rithm for the subset selection problem, SIAM Journal on Matrix Analysis and
Applications, 21(3):797–808, 2000.
5. D.L. Donoho and M. Elad, Optimally sparse representation in general (non-
orthogonal) dictionaries via l1 minimization, Proc. of the National Academy of
Sciences, 100(5):2197–2202, 2003.
6. D.L. Donoho and X. Huo, Uncertainty principles and ideal atomic decomposi-
tion, IEEE Trans. on Information Theory, 47(7):2845–2862, 1999.
7. D.L. Donoho and J. Tanner, Sparse nonnegative solutions of underdetermined
linear equations by linear programming, Proceedings of the National Academy
of Sciences, 102(27):9446–9451, July 2005.
8. D.L. Donoho and J. Tanner, Neighborliness of randomly-projected Simplices in
high dimensions, Proceedings of the National Academy of Sciences, 102(27):9452–
9457, July 2005.
9. M. Elad and A.M. Bruckstein, A generalized uncertainty principle and sparse
representation in pairs of bases, IEEE Trans. on Information Theory, 48:2558–
2567, 2002.
10. A. Feuer and A. Nemirovsky, On sparse representation in pairs of bases, IEEE
Trans. on Information Theory, 49:1579–1581, June 2002.
11. J.J. Fuchs, On sparse representations in arbitrary redundant bases, IEEE Trans.
on Information Theory, 50:1341–1344, 2004.
12. R. Gribonval and M. Nielsen, Sparse decompositions in unions of bases, IEEE
Trans. on Information Theory, 49(12):3320–3325, 2003.

Further Reading
77
13. X. Huo, Sparse Image representation Via Combined Transforms, PhD thesis,
Stanford, 1999.
14. S. Mallat and Z. Zhang, Matching pursuits with time-frequency dictionaries,
IEEE Trans. Signal Processing, 41(12):3397–3415, 1993.
15. N. Moraˇca, Bounds for norms of the matrix inverse and the smallest singular
value, Linear Algebra and its Applications, 429:2589–2601, 2008.
16. J.A. Tropp, Greed is good: Algorithmic results for sparse approximation, IEEE
Trans. on Information Theory, 50(10):2231–2242, October 2004.
17. J.M. Varah, A lower bound for the smallest singular value of a matrix, Linear
Algebra Appl. 11:3–5, 1975.

Chapter 5
From Exact to Approximate Solutions
5.1 General Motivation
The exact constraint Ax = b is often relaxed, with an approximated equality mea-
sured using the quadratic penalty function Q(x) = ∥Ax−b∥2
2. Such relaxation allows
us to (i) deﬁne a quasi-solution in case no exact solution exists (even in cases where
A has more rows than columns); (ii) exploit ideas from optimization theory; (iii)
measure the quality of a candidate solution; and more.
Following the rationale of the previous sections, one may reconsider (P0) and
tolerate a slight discrepancy between Ax and b. We deﬁne an error-tolerant version
of (P0), with error tolerance ϵ > 0, by
(Pϵ
0) :
min
x
∥x∥0 subject to ∥b −Ax∥2 ≤ϵ.
(5.1)
The ℓ2-norm used here for evaluating the error b −Ax can be replaced by other
options, such as ℓ1, ℓ∞, or a weighted ℓ2-norm.
In this problem a discrepancy of size ϵ is permitted between a proposed repre-
sentation Ax and the signal b. When (P0) and (Pϵ
0) are applied on the same problem
instance, the error-tolerant problem must always give results at least as sparse as
those arising (P0), since the feasible set is wider. Indeed, for a typical general prob-
lem instance (A, b), the solution of (P0) will have n nonzeros. On the other hand,
in some real-world problems (as we shall see below), although the solution of (P0)
would be dense, the solution of (Pϵ
0) can be seen to have far fewer non-zeros, tending
towards sparsity.
An alternative and more natural interpretation for the problem (Pϵ
0) is one of noise
removal. Consider a suﬃciently sparse vector x0, and assume that b = Ax0+e, where
e is a nuisance vector of ﬁnite energy ∥e∥2
2 = ϵ2. Roughly speaking (Pϵ
0) aims to ﬁnd
x0, i.e., to do roughly the same thing as (P0) would do on noiseless data b = Ax0.
In later chapters we shall return to this interpretation and make it precise, using
statistical estimators that lead to formulations similar to (Pϵ
0).
© Springer Science+Business Media, LLC 2010
M. Elad, Sparse and Redundant Representations: From Theory to Applications in Signal 
and Image Processing, DOI 10.1007/978-1-4419-7011-4_5,
79

80
5 From Exact to Approximate Solutions
Past work studied this problem in various forms, and in this chapter we dis-
cuss some of what is now known. Results are in some ways parallel to those in the
noiseless case. Speciﬁcally, we should discuss the uniqueness property – conditions
under which a suﬃciently sparse solution is known to be the global minimizer of
(Pϵ
0); practical pursuit techniques to approximate the solution of this problems; and
equivalence – theoretical guarantees for their successful recovery of the desired so-
lution. As we shall see next, however, the notions of uniqueness and equivalence no
longer apply – they are replaced by the notion of stability.
5.2 Stability of the Sparsest Solution
Before turning to approximate the solution of (Pϵ
0), we must answer a far more
fundamental question: Suppose that a sparse vector x0 is multiplied by A, and we
obtain a noisy observation of this product, b = Ax0+e with ∥b−Ax0∥2 ≤ϵ. Consider
applying (Pϵ
0) to obtain an approximation to x0, and getting a solution xϵ
0,
xϵ
0 = arg min
x
∥x∥0 subject to ∥b −Ax∥2 ≤ϵ.
How good shall this approximation be? How its accuracy is aﬀected by the spar-
sity of x0? These questions are the natural extension for the uniqueness property of
sparse solutions we have discussed in the context of the (P0) problem in Chapter 2.
5.2.1 Uniqueness versus Stability – Gaining Intuition
As we show next, we can no longer claim uniqueness for (Pϵ
0) in the general case.
In order to demonstrate that, we present a simple experiment: A is chosen to be the
two-ortho [I, F] of size 2 × 4, x0 = [0 0 1 0]T. We generate a random noise e with
pre-speciﬁed norm ϵ = ∥e∥2 and create the vector b = Ax0 + e. Thus,
Ax0 + e =
" 1 0 0.707 0.707
0 1 0.707 −0.707
# 
0
0
1
0

+
" e1
e2
#
=
"0.707 + e1
0.707 + e2
#
Figure 5.1 presents the locations of Ax0, b, the regions {v| ∥v −Ax0∥2 ≤ϵ}, and
{v| ∥v −b∥2 ≤ϵ}, with ϵ = 0.2. The last of those is the region of the image (i.e., after
multiplication by A) of all the feasible solutions x to the problem (Pϵ
0). Naturally, the
vector x0 poses a feasible solution, while being very sparse. Indeed, it is an optimal
solution to (Pϵ
0), in the sense that no sparser solution exists (a sparser solution is only
the null vector, and it is outside the feasible set). Could there be alternative feasible

5.2 Stability of the Sparsest Solution
81
−0.2
0
0.2
0.4
0.6
0.8
1
1.2
1.4
−0.2
0
0.2
0.4
0.6
0.8
1
1.2
1.4
 
 
 Ax0
 b
v | ||v− Ax0||< ε
v | ||v− b||< ε
||z ⋅ a3 − b||  < ε
Other sparse solutions
Fig. 5.1 A 2D demonstration of the lack of uniqueness for the noisy case, with a relatively weak
noise.
solutions that are sparse? Figure 5.1 shows that solutions of the form x0 = [0 0 x 0]T
with some values of x are also feasible, while having the same cardinality.
Figure 5.2 presents the same experiment, this time with a stronger noise, ϵ = 0.6.
This leads to a diﬀerent scenario, where not only we have lost uniqueness with re-
spect to the same support, but other supports with cardinality ∥x∥0 = 1 are possible,
and in fact, even the null solution is included, implying that this is the optimal solu-
tion to (Pϵ
0).
Here is a more formal way of explaining this. We shall denote xS and AS the
portions of x and A that contain the support S elements/columns, respectively. Sup-
pose that x is a sparse candidate solution to this problem over the support S, with
∥x∥0 = |S|, and it satisﬁes the constraint, ∥b −ASxS∥2 ≤ϵ.
If it so happens that xS is also the minimizer of the term fS(z) = ∥b −ASz∥2,
and fS(xopt
S ) = ϵ, we can propose no alternative solution over this support, since
any perturbation around xS leads to an increase in this term and thus violation of
the constraint. In terms of Figure 5.1, this case takes place when the closest point
to b on the green line is Ax0, or put diﬀerently, if the distortion e = b −ASxS is
orthogonal to the columns of AS.1 In all other cases, the fact that minz fS(z) < ϵ
implies an ability to perturb the so-called optimal solution xS in a way that preserves
its feasibility and the support, and thus we get a set of solutions that are as good as x.
1 As the minimizer of fS(z) = ∥b −ASz∥2, the vector xS should satisfy AT
S(b −ASxS) = AT
Se = 0.

82
5 From Exact to Approximate Solutions
−0.2
0
0.2
0.4
0.6
0.8
1
1.2
1.4
−0.2
0
0.2
0.4
0.6
0.8
1
1.2
1.4
 
 
 Ax0
 b
v | ||v− Ax0||< ε
v | ||v− b||< ε
||z ⋅ a1 − b||  < ε
||z ⋅ a2 − b||  < ε
||z ⋅ a3 − b||  < ε
||z ⋅ a4 − b||  < ε
Other sparse solutions
Fig. 5.2 A 2D demonstration of the lack of uniqueness for the noisy case, as shown in Figure 5.1,
but with a stronger noise, that permits alternative solutions with a diﬀerent support.
Furthermore, if some of the non-zero entries in x are small enough, this perturbation
may null them, leading to a sparser solution.
5.2.2 Theoretical Study of the Stability of (Pϵ
0)
So, returning to the original question we have posed, instead of claiming uniqueness
of a sparse solution, we replace this with a notion of stability – a claim that if a
suﬃciently sparse solution is found, then all alternative solutions necessarily resides
very close to it. The following analysis, taken from the work by Donoho, Elad, and
Temlyakov, leads to a stability claim of this sort.
We start by returning to the deﬁnition of the spark and extending it by consider-
ing a relaxed notion of linear-dependency. In the noiseless case we considered two
competing solutions x1 and x2 to the linear system Ax = b, and this led to the rela-
tion A(x1 −x2) = Ad = 0. This motivates a study of the sparsity of vectors d in the
null-space of A, which naturally leads to the deﬁnition of the spark.
Following the same rationale, we should consider now two feasible solutions x1
and x2 to the requirement ∥Ax −b∥2 ≤ϵ. Considering b as the center of a sphere of
radius ϵ, both Ax1 and Ax2 reside in it or on its surface. Thus, the distance between

5.2 Stability of the Sparsest Solution
83
b
Ax1
Ax2
ε
||Ax1 − Ax2||2
Fig. 5.3 The vector b as the center of a circle, and the image of two candidate solutions, Ax1 and
Ax2 that are ϵ away. This leads to a distance of 2ϵ at the most between Ax1 and Ax2.
these two vectors is 2ϵ at the most, as shown in Figure 5.3, and we have the relation
∥A(x1 −x2)∥2 = ∥Ad∥2 ≤2ϵ. A diﬀerent way to see this is the following, which
exploits the triangle inequality:
∥A(x1 −x2)∥2 = ∥Ax1 −b + b −Ax2∥2
(5.2)
≤∥Ax1 −b∥2 + ∥Ax2 −b∥2 ≤2ϵ.
Therefore we should generalize the spark to allow for ϵ-proximity to the null-space,
as the following deﬁnition suggests.
Deﬁnition 5.1. Given a matrix A ∈IRn×m, we consider all possible sub-sets of s
columns, each such set forms a sub-matrix As ∈IRn×s. We deﬁne sparkη(A) as the
smallest possible s (number of columns) that guarantees
min
s σs(As) ≤η.
(5.3)
In words, this is the smallest (integer) number of columns that can be gathered from
A, such that the smallest singular-value2 of As is no larger than η.
For η = 0, this demand reduces to linear dependence of the chosen columns, and
thus we have spark0(A) = spark(A). Naturally, sparkη(A) is monotone decreasing
in η, and
∀0 ≤η ≤1, 1 ≤sparkη(A) ≤spark(A) ≤n + 1.
(5.4)
2 This is the last one among the s singular-values, and its index is therefore s.

84
5 From Exact to Approximate Solutions
We also know that for η = 1 we get sparkη(A) = 1, since any single (and normal-
ized) column has one singular-value equal to 1.
A fundamental property of the spark is that the equation Av = 0 implies ∥v∥0 ≥
spark(A). Similarly, a generalized version of this property is the following:
Lemma 5.1. If ∥Av∥2 ≤η and ∥v∥2 = 1, then ∥v∥0 ≥sparkη(A).
Proof: This property is a direct consequence of the above deﬁnition, and the fun-
damental properties of singular-values. By the deﬁnition of sparkη(A), there exists
at least one subset of columns from A with sparkη(A) columns, such that its small-
est singular-value is η or below. Also, the deﬁnition states that any set of less than
sparkη(A) columns guarantees that all their singular-values are strictly above η.
Now, consider a case where ∥v∥0 = s < sparkη(A). By fundamental properties of
singular-values, the corresponding sub-matrix As of size n × s (s < n) must satisfy
∥Av∥2 = ∥Asvs∥2 ≥σs∥vs∥2 = σs for any normalized vector v. Since s < sparkη(A)
we know that σs > η, and thus we got that ∥Av∥2 ≥σs > η. As this contradicts the
knowledge that ∥Av∥2 ≤η, we conclude that s = ∥v∥0 ≥sparkη(A).
□
We now turn to a more fundamental observation, that ties sparkη(A) to the
mutual-coherence of this matrix:
Lemma 5.2. If A has normalized columns and mutual-coherence µ(A), then
sparkη(A) ≥1 −η2
µ(A) + 1.
(5.5)
Proof: We use a simple observation about eigenvalues of diagonally-dominant ma-
trices, based on Gershgorin’s disk theorem (see Chapter 2). Given an s-by-s sym-
metric matrix H with diagonal entries equal to 1 and oﬀ-diagonal entries ≤µ in
amplitude, we have that the smallest eigenvalue of H can be bounded from below
by λmin(H) ≥1 −(s −1)µ.
With A obeying the above assumptions (normalized columns, and ﬁxed coher-
ence), every leading minor in the Gram matrix G = ATA has ones on the diagonal
(assumed normalized columns), and its oﬀ-diagonal entries are bounded by µ(A) in
absolute value. Thus, the above claim gives a lower bound on the minimum eigen-
value of every such minor of size s by s, denoted by AT
s As. We have
λmin(AT
s As) ≥1 −(s −1)µ(A).
(5.6)
⇒
s ≥1 −λmin(AT
s As)
µ(A)
+ 1.
For the choice λmin(AT
s As) ≤η2, every set of s such columns have singular-values
bounded from above by η and thus sparkη(A) ≥1−η2
µ(A) + 1.
□
Based on the above developed tools, we now turn to present a variant of the
uncertainty result we had for the noiseless case.

5.2 Stability of the Sparsest Solution
85
Lemma 5.3. If x1 and x2 satisfy ∥b −Axi∥2 ≤ϵ, i = 1, 2, then
∥x1∥0 + ∥x2∥0 ≥sparkη(A), where η =
2ϵ
∥x1 −x2∥2
.
(5.7)
Proof. The triangle inequality yields ∥A (x1 −x2) ∥2 ≤2ϵ. Put diﬀerently, we have
∥Av∥2 ≤η, where v = (x1 −x2)/∥x1 −x2∥2. Based on the property shown in Lemma
5.1, we get ∥v∥0 ≥sparkη(A). But
∥x1∥0 + ∥x2∥0 ≥∥x1 −x2∥0 = ∥v∥0,
(5.8)
which establishes (5.7).
□
Now, just as for the exact case, we use the above uncertainly rule to derive a
uniqueness result, but this time in the form of a localization to a single Euclidean
ball.
Theorem 5.1. Given a distance D ≥0 and ϵ, set η = 2ϵ/D. Suppose there are two
approximate representations xi, i = 1, 2 both obeying
∥b −Axi∥2 ≤ϵ
and ∥xi∥0 ≤1
2 sparkη(A).
(5.9)
Then ∥x1 −x2∥2 ≤D.
Proof: Using the details above and Lemma 5.3 we have that
sparkη(A) ≥∥x1∥0 + ∥x2∥0 ≥sparkν(A)
(5.10)
for ν = 2ϵ/∥x1 −x2∥2. Because of the monotonicity of sparkη(A) this implies
η = 2ϵ
D ≤ν =
2ϵ
∥x1 −x2∥2
,
(5.11)
which leads to the claimed result.
□
Finally, we are now ready to pose a stability result that claims that the solution
of (Pϵ
0) cannot be far of from a suﬃciently sparse initial vector x0.
Theorem 5.2. (Stability of (Pϵ
0)): Consider the instance of problem (Pϵ
0) deﬁned
by the triplet (A, b, ϵ). Suppose that a sparse vector x0 ∈IRm satisﬁes the sparsity
constraint ∥x0∥0 < (1 + 1/µ(A))/2, and gives a representation of b to within error
tolerance ϵ (i.e., ∥b −Ax0∥2 ≤ϵ). Every solution xϵ
0 of (Pϵ
0) must obey
∥xϵ
0 −x0∥2
2 ≤
4ϵ2
1 −µ(A)(2∥x0∥0 −1).
(5.12)
Note that this result parallels Theorem 5 that presents the uniqueness result for the
noiseless case, and indeed it reduces to it exactly for the case of ϵ = 0.

86
5 From Exact to Approximate Solutions
Proof: The solution to (Pϵ
0), denoted as xϵ
0, is at least as sparse as the ideal sparse
solution x0, since x0 is only one of the many possible feasible solutions, while (P0,ϵ)
seeks the sparsest of them all. Since ∥x0∥0 < (1 + 1/µ(A))/2, there exists3 a value of
η ≥0 such that
∥x0∥0, ∥xϵ
0∥0 ≤1
2 sparkη(A),
(5.13)
and using Lemma 5.2 we replace this inequality with a more strict requirement
∥x0∥0 ≤1
2
 1 −η2
µ(A) + 1
!
≤1
2 sparkη(A),
(5.14)
which poses a lower bound on η being
η2 ≤1 −µ(A) · (2∥x0∥0 −1) .
(5.15)
Invoking Theorem 5.1, we have two feasible and sparse solutions, each with cardi-
nality below 1
2 sparkη(A), with η as given above. Thus, η should satisfy η = 2ϵ/D
where D ≥∥x0 −xϵ
0∥2 and combining this with the upper bound on η in Equation
(5.15) we obtain
∥x0 −xϵ
0∥2
2 ≤D2 = 4ϵ2
η2 ≤
4ϵ2
1 −µ(A) · (2∥x0∥0 −1)
(5.16)
as claimed.
□
5.2.3 The RIP and Its Use for Stability Analysis
Before concluding the discussion on the stability of (Pϵ
0), we introduce a somewhat
diﬀerent point of view, which leads to an alternative stability claim. In order to
present this theoretical analysis, we start by introducing a new measure of quality
of a given matrix A that replaces the mutual-coherence and the spark. This measure
developed by Candes and Tao, termed Restricted Isometry Property (RIP), is pow-
erful in grasping a key property of the matrix A, that makes consequent analysis
easy.
Deﬁnition 5.2. For a matrix A of size n × m (m > n) with ℓ2-normalized columns,
and for an integer scalar s ≤n, consider sub-matrices As containing s columns from
A. Deﬁne δs as the smallest quantity such that
∀c ∈IRs
(1 −δs)∥c∥2
2 ≤∥Asc∥2
2 ≤(1 + δs)∥c∥2
2,
(5.17)
3 Since spark0(A) ≥(1 + 1/µ(A))/2.

5.2 Stability of the Sparsest Solution
87
hold true for any choice of s columns. Then A is said to have an s-RIP with a
constant δs.
The key idea in the above deﬁnition is the claim that any subset of s columns from
A behave like an orthogonal transform that loses/gains almost no energy. Clearly,
this deﬁnition is only informative for δs < 1.
It is quite easy to see the resemblance between the RIP and the sparkη properties.
Whereas sparkη is the minimal number of required columns s to get η2 away from
singularity,4 RIP ﬁxes s and seeks the minimal value of 1−δs (5.17), again implying
that these s columns are (1 −δs)-away from singularity. Note that, in that respect,
RIP is richer, as it also proposes a bound from above. As we shall see next, RIP
makes the stability analysis far easier. Figure 5.4 describes the connection between
sparkη and RIP.
Given a matrix A, it is hard or even impossible to evaluate δs for s ≫1, as
this requires a sweep through all the
m
s

supports. In that respect, this measure is
as complex as the spark for assessment. Indeed, just as we have bounded the spark
using the mutual-coherence, this can be done here, leading to δs ≤(s−1)µ(A). Note
that for s = 1 we have δ1 = 0, since the columns are normalized. The above bound
on the RIP constant is easily seen by
∥Asc∥2
2 = cTAT
s Asc
(5.18)
≤|c|T I + µ(A)(1 −I) |c|
≤(1 −µ(A))∥c∥2
2 + µ(A)∥c∥2
1
≤[1 + (s −1)µ(A)]∥c∥2
2.
In the above we used the structure of the minor from the Gram matrix, having ones
on the main-diagonal and oﬀ-diagonal entries bounded by µ(A). We also used the
norm-equivalence inequality ∥c∥1 ≤√s∥c∥2 for vectors c of length s. The lower-
bound can be established similarly as
∥Asc∥2
2 = cTAT
s Asc
(5.19)
≥|c|T I −µ(A)(1 −I) |c|
≥(1 + µ(A))∥c∥2
2 −µ(A)∥c∥2
1
≥[1 −(s −1)µ(A)]∥c∥2
2.
Here we used again the norm-equivalence inequality ∥c∥1 ≤√s∥c∥2.
An alternative and easier derivation of the above bounds (from above and be-
low) on δs can be obtained using the Gershgorin disk theorem, that states that the
eigenvalues of AT
s As are necessarily bounded by
1 −(s −1)µ(A) ≤λ(AT
s As) ≤1 + (s −1)µ(A),
(5.20)
and this directly leads to the above claim, since
4 In terms of the smallest eigenvalue, and not singular one.

88
5 From Exact to Approximate Solutions
Spark
 2
Spark
1
0
1
A
B

s
s
Spark
0
1
1
B
A
1-

s
Fig. 5.4 The top graph presents the behavior of sparkη as a function of η2. Point A in this graph
corresponds to η = 0, leading to the regular (noseless) spark. Point B is the other extreme, where
η = 1 and thus sparkη = 1. The bottom graph presents the RIP constant δs as a function of s.
There is an exact analogy between points A and B in the two graphs, when considering 1 −δs as a
function of s. As such, sparrkη and RIP are related to each other as a function and its inverse.
λmin(AT
s As) · ∥c∥2
2 ≤∥Asc∥2
2 ≤λmax(AT
s As) · ∥c∥2
2.
(5.21)
Armed with this deﬁnition of RIP, we make the following observation: Suppose
that a sparse vector x0 of cardinality s0 is multiplied by A, and contaminated by an
ϵ-bounded-norm vector e (i.e., ∥e∥2 ≤ϵ), generating b = Ax0 + e. Thus, clearly,
∥b −Ax0∥2 ≤ϵ.
Now, assume that we solve (Pϵ
0) and get a candidate solution, denoted by ˜x.
Clearly, this solution is sparse, with s0 non-zeros at the most (since x0 is also fea-
sible, and it has s0 non-zeros). Also, this candidate solution satisﬁes the inequality
∥b −A˜x∥2 ≤ϵ.

5.3 Pursuit Algorithms
89
Deﬁning d = ˜x −x0, we have that ∥Ax0 −A˜x∥2 = ∥Ad∥2 ≤2ϵ, since both these
vectors (Ax0 and A˜x) are ϵ away from the same center vector b. We also observe
that d is a sparse vector with 2s0 non-zeros as the most, since ∥d∥0 = ∥˜x −x0∥0 ≤
∥x0∥0 + ∥˜x∥0 ≤2s0.
Let us assume that the matrix A satisﬁes the RIP property for 2s0 with the cor-
responding parameter δ2s0 < 1. Thus, using this property and exploiting the lower-
bound part in Equation (5.17), we get
(1 −δ2s0)∥d∥2
2 ≤∥Ad∥2
2 ≤4ϵ2,
(5.22)
and thus, we get a stability claim of the form
∥d∥2
2 = ∥˜x −x0∥2
2 ≤
4ϵ2
1 −δ2s0
.
(5.23)
Using our bound for δ2s0 from above using the mutual-coherence, this reads
∥˜x −x0∥2
2 ≤
4ϵ2
1 −δ2s0
≤
4ϵ2
1 −(2s0 −1)µ(A).
(5.24)
The obtained result is the same as the (Pϵ
0) stability claim reported in Theorem 5.2.
Note that this bound is true only if the denominator is positive, implying that we
require s0 < 0.5(1 + 1/µ(A). If ϵ = 0, the stability result obtained suggests that
˜x = x0, which is aligned with the results of Chapter 2
The very same analysis as shown above can be made more general. Assume
that an arbitrary approximation algorithm is used for solving (Pϵ
0), and a feasible
candidate solution ˜x is proposed with ∥˜x∥0 = s1 non-zeros. Using the same rationale
we get that the proposed solution is necessarily close to the ideal one,
∥˜x −x0∥2
2 ≤
4ϵ2
1 −δs0+s1
≤
4ϵ2
1 −(s0 + s1 −1)µ(A),
(5.25)
due to its sparsity. This claim brings us naturally to the next Section, where we
discuss speciﬁc approximation algorithms and their tendency to yield such sparse
solutions.
5.3 Pursuit Algorithms
5.3.1 OMP and BP Extensions
Since (P0) is impractical to solve in the general case, it seems unlikely that a direct
attack on (Pϵ
0) is a sensible goal. The pursuit algorithms discussed above can be
adapted to allow error tolerances; how will they perform? Referring to the two op-

90
5 From Exact to Approximate Solutions
tions we had for devising such algorithms – the greedy approach and the relaxation
of the ℓ0 functional – variants of these methods may be investigated.
Consider, for example, the greedy algorithm described in Figure 3.1 – the OMP
algorithm. By choosing ϵ0 = ϵ in the stopping rule, the algorithm accumulates non-
zero elements in the solution vector until the constraint ∥b −Ax∥2 ≤ϵ is satis-
ﬁed. With this minor change, the OMP is very easy to implement and use, and this
explains its popularity. The same modiﬁcation holds true for all the other greedy
methods.
Similarly, relaxing ℓ0 to an ℓ1-norm, we get the following variant of (P1), known
in the literature as basis pursuit denoising (BPDN) (note that just as in the exact
case, we assume that A has normalized columns, and thus there is no need for a
weighting matrix):
(Pϵ
1) :
min
x
∥x∥1 subject to ∥b −Ax∥2 ≤ϵ,
(5.26)
This can be written as a standard problem in optimization with a linear penalty and a
quadratic and linear inequality constraints. Such problems are very well-studied by
specialists in optimization and there are many practical methods for solving them –
practically the whole of modern convex optimization theory is applicable, and par-
ticularly the recent advances in solving large systems by interior-point and related
methods. We shall not review that literature here, and instead discuss two relatively
simple and tailored approaches for this problem, suitable for readers without back-
ground in convex optimization.
There are various specialized optimization packages that take into account the
tendency of the solution of our problem to be sparse (ℓ1-magic by Candes and
Romberg, CVX and L1-LS by Stephen Boyd and students, Sparselab by David
Donoho and students, GPSR by Mario Figueiredo, SparCo by Michael Friedlander,
PDSCO by Michael Saunders, ShrinkPack by Elad, Zibulevsky, and Shtok, MCAlab
by Fadili, Starck, Donoho, and Elad, and more). These pose a good way to get a reli-
able solution to (Pϵ
1) with little programming eﬀort – one only has to set up (Pϵ
1) as a
problem of the type each optimizer can solve. We note, however, that for large-scale
applications general-purpose quadratic programming optimizers seem slow and can
perhaps be improved by special purpose techniques, as indeed provided by some of
the above-mentioned packages.
For an appropriate Lagrange multiplier lambda, the solution to (5.26) is precisely
the solution to the unconstrained optimization problem
(Qλ
1) :
min
x
λ∥x∥1 + 1
2∥b −Ax∥2
2,
(5.27)
where the Lagrange multiplier λ is a function of A, b, and ϵ.
We mention below the Iterative-Reweighed-Least-Squares (IRLSindexIterative-
Reweighed-Least-Squares) algorithm, which was already mentioned in the context
of the exact problem (P0). We shall also brieﬂy present the LARS algorithm due
to its feature to provide the complete regularization path (i.e., solutions for all λ).
We stress, however, that there are other options as well for solving the relaxed prob-

5.3 Pursuit Algorithms
91
Task: Find x that approximately solves (Qλ
1): minx λ∥x∥1 + 1
2 · ∥b −Ax∥2
2.
Initialization: Initialize k = 0, and set
•
The initial approximation (chosen arbitrarily) x0 = 1.
•
The initial weight matrix X0 = I.
Main Iteration: Increment k by 1, and apply these steps:
•
Regularized Least-Squares: approximately solve the linear system

2λX−1
k−1 + ATA

x = ATb
iteratively (several Conjugate-Gradient iterations may sufﬁce), producing
result xk.
•
Weight Update: Update the diagonal weight matrix X using xk: Xk(j, j) =
|xk( j)| + ϵ.
•
Stopping Rule: If ∥xk −xk−1∥2 is smaller than some predetermined thresh-
old, stop. Otherwise, apply another iteration.
Output: The desired result is xk.
Fig. 5.5 The IRLS strategy for approximately solving (Qλ
1).
lem (e.g., Homotopy, gradient projection, and more). Speciﬁcally, in Chapter 6 we
present a family of specialized algorithms for the minimization of (Qλ
1), based on
iterative-shrinkage procedures.
5.3.2 Iteratively-Reweighed-Least-Squares (IRLS)
A simple strategy to attack (Qλ
1) is the Iterative-Reweighed-Least-Squares (IRLS)
algorithm, similar to the way we took in the exact case in Chapter 3. We note,
though, that while the core idea is similar, the derivation here is somewhat diﬀerent.
Setting X = diag(|x|), we have ∥x∥1 ≡xTX−1x. Thus we may view the ℓ1-norm as
an (adaptively-weighted) version of the squared ℓ2-norm. Given a current approxi-
mate solution xk−1, set Xk−1 = diag(|xk−1|) and attempt to solve
(Mk) :
min
x
λxTX−1
k−1x + 1
2∥b −Ax∥2
2;
(5.28)
this is a quadratic optimization problem, solvable using standard linear algebra. Ob-
tain an (approximate) solution xk (say); a diagonal matrix Xk is constructed with
the entries of xk on the diagonal, and a new iteration can begin. This algorithm is
formally described in Figure 5.5.
Let us demonstrate this algorithm on a speciﬁc case study, and through this also
gain some insight into the problems (Qλ
1) and (Pϵ
1). We generate a random matrix A

92
5 From Exact to Approximate Solutions
10
−4
10
−3
10
−2
10
−1
10
0
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
λ
Normalized L2−Error
Fig. 5.6 Normalized ℓ2-error of the solution (∥ˆxλ −x0∥2/∥x0∥2) as a function of λ. Each point in
this curve is the outcome of 15 iterations of the IRLS algorithm. The dashed curve presents the
value of | log(∥Aˆxλ −b∥2/(nσ2))|, getting a value close to zero implies that the residual is equal to
the noise power.
of size 100 × 200, create a random sparse vector x0 of length 200 with cardinality
∥x0∥0 = 4, where its non-zero entries are drawn uniformly from the range [−2, −1]∪
[1, 2]. We compute b = Ax0 + e, where e is an additive random Gaussian iid noise
vector with standard-deviation σ = 0.1. Our goal is to recover x0, and for that we
shall solve (Qλ
1) with the IRLS algorithm.
The ﬁrst problem we encounter is the choice of λ. Whereas the problem (Pϵ
1) can
be clearly posed with ϵ ≈√nσ to represent the deviation we expect between Ax and
b, turning to the format of (Qλ
1) loses this simple intuition. The thumb-rule we shall
use is to choose λ as approximately the ratio between σ and the standard-deviation
of the expected non-zeros in the solution. More on this choice and its justiﬁcation
are given in Chapter 11. As the non-zero’s STD is roughly 2, we choose λ around
the value σ/2 = 0.05.
Figure 5.6 presents the quality of the solution, measured by ∥ˆxλ −x0∥2/∥x0∥2,
as a function of λ. Each solution is obtained using 15 iterations of the IRLS algo-
rithm. The same graph shows also the term | log(∥Aˆxλ −b∥2/(nσ2))|, which shows
for which λ the residual ∥Aˆxλ −b∥2 becomes equal to the noise power nσ2. This
is another thumb-rule that can be used for determining λ, and as we see, there is a
good agreement between this choice of λ and the optimal one.
Figure 5.7 presents three possible solutions, for the best found λ, and two values
– one above and and one below it. These solutions are compared to the ideal x0,
and as can be seen, the optimal λ value indeed ﬁnds a good solution with a good
detection of the non-zeros in the ideal vector x0. A smaller λ leads to a more dense
solution, while a larger one further sparsiﬁes it.

5.3 Pursuit Algorithms
93
0
50
100
150
200
−2
0
2
λ=0.0097006
0
50
100
150
200
−2
0
2
λ=0.095543
0
50
100
150
200
−2
0
2
λ=0.94101
Fig. 5.7 Solutions obtained by the IRLS algorithm (15 iterations) for three values of λ – the best
one (in the middle), a smaller one (top), and a larger one (bottom). The true non-zero values of x0
are added as dots for reference.
There is an interesting way to present all the solutions for all λ values together
in one graph. Building a 2D array that contains all the solutions as its columns,
simply plot all its rows as graphs. This approach is commonly used in the context
of an algorithm called Least-Absolute-Shrinkage-and-Selection-Operator (LASSO)
and another termed Least-Angle-Regression (LARS) (see next subsection). These
methods expose the path each entry takes from zero (for large enough value λ) to a
possibly non-zero value. Such a graph is presented in Figure 5.8. we see that values
of λ above 1 lead to a zero solution. As λ becomes smaller, more and more entries
are “awakened” (become non-zeros). The dotted curves represents the entries of x0,
and thus stand as the ideal result we should strive to.
All the above discussion is general and could refer to any alternative solver of
(Qλ
1). We now turn to show the speciﬁc behavior of the IRLS. We already see in some
of the above ﬁgures that the IRLS does not fully sparsiﬁes the proposed solution,
even for the optimal λ. Figure 5.9 presents the (Qλ
1) function value for the computed
solution as a function of the iteration. We see that the IRLS is very eﬀective in the
ﬁrst several iterations, but then slows down dramatically. Furthermore, we see that it
is even not guaranteed to give a descent in every iteration. These observations mean
that this method can be eﬀective mostly if we are interested in a crude solution. By
a relaxation of the IRLS iteration, one can give better convergence behavior and
guarantee a descent in each iteration, but we shall not dwell on this here.

94
5 From Exact to Approximate Solutions
10
−4
10
−3
10
−2
10
−1
10
0
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
λ
Solution Entries
Fig. 5.8 All the IRLS solutions for all λ values. Each curve represents one entry in the solution
and the path it takes as a function of λ. The dotted curves are those of x0.
10
0
10
1
10
2
1
1.2
1.4
1.6
1.8
2
2.2
Iterations
Function Value
Fig. 5.9 The function value (λ∥x∥1 + 1
2 · ∥b −Aˆx∥2
2) as a function of the iterations of the IRLS, for
the optimal value of λ.

5.3 Pursuit Algorithms
95
5.3.3 The LARS Algorithm
In the statistical machine learning community, the problem we have deﬁned as (Qλ
1)
is known as LASSO (Least Absolute Shrinkage and Selection Operator), used for
regression over a wide set of features. LASSO was conceived by Friedman, Hastie,
and Tibshirani, three researchers from the statistics department at Stanford Univer-
sity. This took place around the very same time the Basis-Pursuit was invented for
signal processing purposes by Chen, Donoho, and Saunders – from the same depart-
ment (actually, the same corridor!!).
In the LASSO formulation, the columns of A represent each a feature with vary-
ing values, and the vector b is the output of a complex system. We desire to ﬁnd
a simple linear combination of few features that could explain the output b. Thus,
solving (Qλ
1) not only provides a way to get such regression, but it also selects a
small subset of features, known as model selection.
While LASSO and BPDN target diﬀerent applications, their mathematical for-
mulation is exactly the same. In the statistical machine learning community that
studies LASSO, the goal is a numerical method that could solve (Qλ
1) for all possi-
ble values of λ at once. We refer to this type of solutions as the complete path of
the regularization. The motivation is natural – this can help us in migrating from the
solution of (Qλ
1) to (Pϵ
1) easily, and better tune ϵ or λ for our needs.
Surprisingly, such path of solutions can be obtained with almost the same com-
putational eﬀort as solving for one value of λ alone. Indeed, in order to see why
this is at all possible, think of the OMP as a technique that also sweeps through
λ, as the residual energy is decreased by adding one atom at a time. LARS (Least
Angle Regression Stagewise), proposed by the LASSO team and Efron, is very sim-
ilar to OMP, but with a guarantee that the solution path is the global optimizer of
(Qλ
1). We now turn to describe the core of this algorithm, adopting an interesting and
appealing interpretation of this algorithm proposed by Julien Mairal.
We start by observing that for a solution to be the optimal minimizer of (Qλ
1), it
should lead to a sub-gradient set that contains the zero. For the convex function
f(x) = λ∥x∥1 + 1
2∥b −Ax∥2
2,
(5.29)
the sub-gradient set is given as the set of all vectors
∂f(x) =
n
AT(Ax −b) + λz
o
∀z =

+1
x[i] > 0
[−1, +1] x[i] = 0
−1
x[i] < 0
.
(5.30)
When searching the minimizer of the function f(x), we should seek both x and z
that suites it, such that 0 ∈∂f(x).
The sub-gradient set at location x0 deﬁnes all the possible directions v that satisfy
f(x) −f(x0) ≥vT(x −x0) for a suﬃciently small neighborhood, ∥x −x0∥2 ≤δ.
This should remind the reader of tangent planes, passing through x0 and bounding
the convex function f(x) from below at x0. For diﬀerentiable functions this aligns

96
5 From Exact to Approximate Solutions
with the single-vector regular gradient. The fact that the sub-gradient may contain
more than just one vector is a direct consequence of the non-diﬀerentiability of f(x)
around zero.
We now turn to describe how this sub-gradient requirement is used for the cre-
ation of LARS, where we focus on the main theme of this method, disregarding
small (but important) details.
Step 1: Starting with λ →∞, we obviously have xλ = 0 as our optimal solution.
Observe that as λ is decreased towards zero, for all values λ ≥∥ATb∥∞, the zero
solution remains optimal, because using Equation (5.30), the requirement 0 ∈∂f(x)
and the assumption xλ = 0 imply
0 = −ATb + λzλ
(5.31)
and zλ = ATb/λ satisﬁes the condition posed in Equation (5.30), i.e. all the entries
in z are in the range [−1, 1].
Step 2: As λ is decreased and gets to ∥ATb∥∞, assume that it is the i-th entry in this
vector that gets the maximal value. Keeping xλ[i] = 0 for slightly smaller λ clearly
violates (5.30). Thus, we change this entry and accordingly set zλ[i] = sign(xλ[i]).
This single non-zero value xλ[i] is obtained by solving
0 = aT
i (ax −b) + λzλ[i]
(5.32)
⇒
xλ[i] = aT
i b −λzλ[i]
aT
i ai
= aT
i b −λsign(xλ[i])
aT
i ai
.
It is easily seen that this value has the same sign as the inner product aT
i b, as re-
quired.
The proposed solution is correct for λ = ∥ATb∥∞and nearby values below, and
the condition on z translates to the assignment zλ = AT(b −Axλ)/λ. The i-th row of
this requirement is satisﬁed trivially with equality due to (5.32), and the rest are met
by setting zλ based on the above formula.
Step 3: We proceed by decreasing the value of λ, changing linearly the solution xλ
based on Equation (5.32), and updating zλ based on the above assignment. Assume
that for the current value of λ we hold the support of x, denoted by S . We denote by
xs
λ the non-zero part of xλ, and As as the sub-matrix of A that contains the chosen
columns. The current solution is thus given by
xs
λ = (AT
s As)−1(AT
s b −λzs
λ),
(5.33)
where zs
λ = sign(xs
λ), is the sign-vector of the non-zero portion of the vector xλ.
This solution changes linearly with λ. As this process of decreasing λ continues,
two situations can be encountered:
1. The i-th entry in zλ outside the support of xλ becomes ±1 and is about to exit
the permitted range [−1, +1]. In this case we add this entry to the support, and
update the solution to include it, using the above formula.

5.3 Pursuit Algorithms
97
10
−2
10
−1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
λ
Normalized L2−Error
 
 
LARS
IRLS
Fig. 5.10 Normalized ℓ2-error of the solution (∥ˆxλ −x0∥2/∥x0∥2) as a function of λ. Each point in
these curves is the outcome of LARS (solid) or 15 iterations of the IRLS algorithm (x marks).
2. One of the non-zero entries in xs
λ becomes zero. In this case it should be removed
from the support and the solution updated, again using the above formula.
In practice, the value of λ is decreased from one check-point to the next, testing for
these cases, and updating the support S , the solution xλ, and the vector zλ. Note that
throughout the algorithm we assume that there is a unique optimal solution to (Qλ
1),
and build it explicitly. Also, note that we assume that at every stage only one entry
in the solution joins/leaves the support. In cases where these assumptions are not
met, the algorithm requires delicate modiﬁcations.
Before concluding this discussion, we turn to demonstrate LARS. We perform
the same test we have applied on the IRLS (and in fact, on the same data). We use
a Matlab software written by Karl Sj¨ostrand from the Technical University of Den-
mark, which provides LARS’ full regularization path. As the results are quite close
to those of the IRLS, we present only the following two ﬁgures that correspond to
Figure 5.6. Figure 5.10 shows both the LARS and the IRLS (15 iterations) curves,
and as can be seen, they are quite similar, and especially so around the optimal value
of λ. Figure 5.11 zooms-in on the area around the minimum, showing that a diﬀer-
ence does exist in favor of LARS. Applying more iterations by IRLS is not likely
to close this gap, due to the much-slower convergence we have observed. LARS is
more accurate, and also cheaper in terms of computations, making it an ideal solver
for low-dimensional problems. As the number of steps required by LARS roughly
equals the dimensions of x, using this technique in high dimensions is prohibitive.

98
5 From Exact to Approximate Solutions
10
−1
0.05
0.1
λ
Normalized L2−Error
 
 
LARS
IRLS
Fig. 5.11 A zoomed-in version of Figure 5.10 that exposes the small diﬀerences between the two
algorithms.
5.3.4 Quality of Approximations Obtained
LARS and IRLS, as solvers of (Qλ
1), are only approximation algorithms to our true
desire – the solution of (Pϵ
0). In this context, OMP and other greedy methods also tar-
get this objective, which gives us common grounds to compare these very diﬀerent
techniques. How good are these approximations? A partial answer appears in Figure
5.10, showing that ˆxλ obtained by LARS/IRLS tends to be close to x0 for a proper
choice of λ. Just as in Chapter 3, we can also ask how good are those algorithms in
recovering the true support and entry-values of x0, but this time noisy measurements
are involved. We address these questions here, by an experiment similar to the one
reported in Sections 3.1.7 and 3.2.3, in which we compare LARS and OMP.
We create a random matrix A of size 30 × 50 with entries drawn from the normal
distribution, and then we normalize its columns. We generate sparse vectors x0 with
iid random supports of cardinalities in the range [1, 15], and non-zero entries drawn
as random uniform variables in the range [−2, −1]∪[1, 2]. Once x0 is generated, we
compute b = Ax + e, where e is a zero mean iid random Gaussian noise. We set
its standard-deviation to σ = 0.1. We then apply the LARS and OMP algorithms to
seek for x. Both methods provide a set of solutions, both adding one atom at a time.
From those we choose the sparsest solution that satisﬁes ∥Aˆx −b∥2
2 ≤2nσ2 (chosen
empirically to give good results). We perform 200 such tests per each cardinality,
and present average results on ℓ2 recovery error (between x0 and its estimate ˆx), and
on the support distance (see Section 3.1.7 for more details on these performance
measures). These results are given in Figures 5.12 and 5.13.

5.3 Pursuit Algorithms
99
2
4
6
8
10
12
14
0
0.002
0.004
0.006
0.008
0.01
0.012
0.014
0.016
0.018
0.02
Cardinality of the true solution
Average and Relative L2−Error
 
 
OMP
LARS
LARS+Projection
Fig. 5.12 OMP and LARS performance as approximate solvers of (Pϵ
0). The performance shown
here refers to a relative ℓ2 recovery error. The projected-LARS takes the proposed LARS solution,
adopts only its support, and ﬁnd the non-zero coeﬃcients by Least-Squares ﬁtting.
2
4
6
8
10
12
14
0
0.1
0.2
0.3
0.4
0.5
Cardinality of the true solution
Probability of Error in Support
 
 
OMP
LARS
Fig. 5.13 OMP and LARS performance as approximate solvers of (Pϵ
0). The performance shown
here refers to the success rate in detecting the true support.

100
5 From Exact to Approximate Solutions
2
4
6
8
10
12
14
5
10
15
20
25
Cardinality of the true solution
Average Cardinality Estimated
 
 
OMP
LARS
True−cardinality
Fig. 5.14 OMP and LARS performance as approximate solvers of (Pϵ
0). The performance shown
here refers to the average cardinality of the obtained solutions, and our goal is getting as sparse as
possible solution.
The results we see in these two ﬁgures indicate that OMP is better in recovering
the support, while LARS is better in getting smaller ℓ2 error. In terms of ℓ2 error,
it seems that both algorithms perform well, as the errors obtained are quite small.
Since LARS minimizes (Qλ
1), it tends to under-estimate the values of the non-zeros,
as those are penalized by the ℓ1 term. Thus, we can adopt the support given by the
LARS solution, and re-evaluate the non-zero entries in ˆx by solving a simple Least-
Squares problem of the form minx ∥ASx −b∥2
2. As can be seen in Figure 5.12, this
helps in slightly reducing the error.
Figure 5.14 shows another aspect of the OMP and LARS performance. For the
error threshold chosen, both methods ultimately aim to give the sparsest possible
solution, and thus it is natural to ask, how sparse is the proposed solution? In Figure
5.14 we show the average cardinality of the solutions obtained by the OMP and
the LARS in the above-described experiment. It is evident that OMP performs very
well, getting nearly the original cardinality, whereas LARS is far behind, tending to
give a denser solution. This aligns well with the results shown in Figure 5.13 that
indicate that LARS gives a poor detection of the support – we now know that this is
due to its tendency to give too many non-zeros.

5.4 The Unitary Case
101
5.4 The Unitary Case
If A is unitary, the minimization of (Pϵ
1), (Qλ
1), and even (Pϵ
0) can all be manipulated
in a sequence of simple steps and turned into a set of n independent and identi-
cal one-dimensional optimization tasks that are easily handled. Let us start with a
treatment of (Pϵ
0), as deﬁned in Equation (5.1),
(Pϵ
0) :
min
x
∥x∥0 subject to ∥b −Ax∥2 ≤ϵ.
Using the identity AAT = I, we get an equivalent problem of the form
(Pϵ
0) :
min
x
∥x∥0 subject to ∥ATb −x∥2 ≤ϵ.
(5.34)
Here we have exploited the fact that ℓ2-norm is unitary-invariant, which implies the
equality.
∥b −Ax∥2 = ∥AT(b −Ax)∥2.
Denoting x∗= ATb, we obtain a simpliﬁed problem of the form
(Pϵ
0) :
min
x
∥x∥0
(5.35)
subject to ∥x∗−x∥2
2 =
n
X
i=1
(x∗[i] −x[i])2 ≤ϵ2.
this problem deﬁnes the optimal solution as the sparsest vector that is ϵ-close to x∗.
Thus, the optimal solution is simply obtained by sorting the entries of the vector
x∗= ATb in a descending order of their absolute values, and choosing the leading
terms till the constraint ∥x∗−x∥2 ≤ϵ is satisﬁed. This implies that there exists a
threshold value T(ϵ) such that
xϵ
0 =
( xϵ
0[i] = x∗[i] |x∗[i]| ≥T(ϵ)
xϵ
0[i] = 0
|x∗[i]| < T(ϵ) ,
(5.36)
where T(ϵ) is chosen as the largest possible value that satisﬁes ∥x∗−xϵ
0∥2 ≤ϵ. The
operation described in Equation (5.36) is known as hard-thresholding, used often in
signal processing for sparsifying vectors. Note that the solution we have obtained is
the globally optimal one, even though the initial problem is non-convex and highly
combinatorial in nature.
Turning to handle (Pϵ
1) and (Qλ
1), the treatment is similar, although a bit diﬀerent.
Following the same steps, we get the equivalent problems
(Pϵ
1) :
min
x
∥x∥1 subject to ∥x∗−x∥2
2 ≤ϵ2
(5.37)
(Qλ
1) :
min
x
λ∥x∥1 + 1
2∥x∗−x∥2
2.
(5.38)

102
5 From Exact to Approximate Solutions
As already mentioned above, these two problems can be made equivalent by a proper
choice of λ, as a function of ϵ and x∗. Thus, we proceed with the formulation of (Qλ
1),
and observe that its penalty function is separated into n disjoint, independent, and
equally formulated 1D problems of the form (x∗is now a scalar),
xopt = arg min
x
λ|x| + 1
2(x −x∗)2.
(5.39)
Since the absolute value function is not diﬀerentiable at the origin, we replace the
classical approach of zeroing the derivative with the requirement that the zero is
included in the sub-gradient set (see the discussion about the LARS algorithm).
This poses a requirement on xopt of the form
0 ∈x −x∗+ λ

+1
x > 0
[−1, +1] x = 0
−1
x < 0
.
(5.40)
This states that the solution should be in the set that nulls the sub-gradient of the
function. Denoting
z =

+1
x > 0
[−1, +1] x = 0
−1
x < 0
,
(5.41)
we require x−x∗+λz = 0. Assuming that x∗≥λ, we get xopt = x∗−λ as the optimal
solution by assigning z = 1. Similarly, for x∗≤−λ, the solution is xopt = x∗+ λ
for the assignment z = −1. For −λ < x∗< λ, the optimal solution is necessarily
xopt = 0, with the choice z = x∗/λ, which is indeed in the range [−1, +1]. Thus,
returning to the problems (Pϵ
1) and (Qλ
1) posed in Equation (5.37), we propose the
solution
xϵ
1 =

xϵ
1[i] = x∗[i] −λ x∗[i] ≥λ
xϵ
1[i] = 0
|x∗[i]| ≥λ
xϵ
1[i] = x∗[i] + λ x∗[i] ≤−λ
(5.42)
= sign(x∗[i]) · [|x∗[i]| −λ]+ ,
where λ should be chosen as the largest possible value that satisﬁes ∥x∗−xϵ
0∥2 ≤ϵ,
if we are solving (Pϵ
1). This operation is known as soft-thresholding.
We see that the three problems (Pϵ
0), (Pϵ
1) and (Qλ
1) admit simple and closed-form
optimal solutions. Furthermore, as already said, even though (Pϵ
0) is non-convex, we
guarantee that the solution we provide is globally optimal.
Interestingly, if the simple thresholding algorithm (see Chapter 3) is applied for
approximating the solution of (Pϵ
0) in this case, it leads to the optimal result. This
is because it computes the vector ATb and chooses its leading entries (in absolute
value), just as the optimal formula suggests. Furthermore, although less obvious to
see, MP and OMP also lead to the optimal solution in this case. Their ﬁrst iteration
chooses the largest entry from the vector |ATb|, and this aligns well with the optimal

5.5 Performance of Pursuit Algorithms
103
solution formula. Subsequent iterations require the evaluation of the residual, but
due to the orthogonality of A, this is equivalent to the removal of the chosen column
from A. Thus, all iterations remove leading entries from |ATb|, and the stopping rule
guarantees that the smallest possible non-zeros are used.
5.5 Performance of Pursuit Algorithms
Can pursuit methods approximately solve (Pϵ
0)? A partial and empirical answer has
been given above. We bring in this section several theoretical results that address this
question. The ﬁrst part is taken from the work by Donoho, Elad, and Temlyakov,
with some modiﬁcations, corresponding to the BPDN stability. Rather than pre-
senting a parallel result for OMP, we develop such a property for the far simpler
thresholding algorithm.
5.5.1 BPDN Stability Guarantee
Theorem 5.3. (Stability of BPDN): Consider the instance of problem (Pϵ
1) deﬁned
by the triplet (A, b, ϵ). Suppose that a vector x0 ∈IRm is a feasible solution to (Pϵ
1)
satisfying the sparsity constraint ∥x0∥0 < (1 + 1/µ(A))/4. The solution xϵ
1 of (Pϵ
1)
must obey
∥xϵ
1 −x0∥2
2 ≤
4ϵ2
1 −µ(A)(4∥x0∥0 −1).
(5.43)
Proof: In this proof we follow closely with the steps of proof of Theorem 4.5, with
necessary modiﬁcations.
Both x0 and xϵ
1 are feasible and thus satisfy ∥b −Ax0∥2 ≤ϵ and ∥b −Axϵ
1∥2 ≤ϵ.
Considering the vector b as a center of a sphere of radius ϵ, we know that both Ax0
and Axϵ
1 are on the surface of this sphere, and thus the distance between them is at
most 2ϵ. Denoting d = xϵ
1 −x0, we therefore have
∥Axϵ
1 −Ax0∥2 = ∥Ad∥2 ≤2ϵ.
(5.44)
We write this requirement diﬀerently using the Gram matrix ATA = G, exploiting
the fact that its main-diagonal contains ones, and its oﬀ-diagonal entries are bounded
by the mutual-coherence µ(A). This leads to
4ϵ2 ≥∥Ad∥2
2 = dTGd
(5.45)
= ∥d∥2
2 + dT(G −I)d
≥∥d∥2
2 −|d|T|G −I||d|
≥∥d∥2
2 −µ(A) · |d|T|1 −I||d|

104
5 From Exact to Approximate Solutions
= ∥d∥2
2 −µ(A) ·

|d|T1|d| −|d|T|d|

≥(1 + µ(A))∥d∥2
2 −µ(A)∥d∥2
1.
In the above we used the property |d|T1|d| = |d|T11T|d| = ∥d∥2
1.
We know that ∥xϵ
1∥1 = ∥d+x0∥1 ≤∥x0∥1, reﬂecting the fact that, as the solution to
(Pϵ
1), xϵ
1 has the shortest possible ℓ1-norm. Using this and the inequality developed
in the proof of Theorem 4.5, we obtain
∥d∥1 −21T
s |d| ≤∥d + x0∥1 −∥x0∥1 ≤0,
(5.46)
where 1s is a vector of length m with ones on the support S of the vector x0. Thus,
∥d∥1 ≤21T
s |d| + ∥d + x0∥1 −∥x0∥1 ≤21T
s |d|.
(5.47)
Using the ℓ1-ℓ2-norms relation, ∀v ∈IRn, ∥v∥1 ≤√n∥v∥2, we obtain
∥d∥1 ≤21T
s |d| ≤2
s
|S|
X
i∈S
|di|2 ≤2
p
|S|∥d∥2.
(5.48)
Plugged back into the inequality in Equation (5.45), we obtain
4ϵ2 ≥(1 + µ(A))∥d∥2
2 −µ(A)∥d∥2
1 ≥(1 + µ(A) −4|S|µ(A))∥d∥2
2.
(5.49)
This leads directly to the claimed result. Note that we must require 1 + µ(A) −
4|S|µ(A) > 0 for the above inequality to be true, and thus the requirement on the
cardinality of x0 as the theorem states.
□
For ϵ = 0, we return to the noiseless case. This reveals a loss of tightness of the
stated result, as the sparsity required is half the amount mentioned in Theorem 4.5.
Another important observation is that the obtained bound on the distance between
x0 and xϵ
1 is larger than 4ϵ2, which is four times all the “noise” energy in the vector
b. This presumably suggests that there is no denoising eﬀect in solving (Pϵ
1). This is
a direct consequence of the worst-case analysis that was practiced here, along with
an assumption about the noise being adversary.
In addition to the above stability claim, results on successful recovery of the
support would also be of interest. Work reporting such results exist, requiring more
strict (and thus less realistic) conditions. Similarly, considering the above as signal
a denoising procedure, there is an interest in the expected performance, but this will
not be explored here.
5.5.2 Thresholding Stability Guarantee
Rather than providing a comparable stability result for the OMP, as in the paper
by Donoho, Elad, and Temlyakov, we develop a similar theorem for the far sim-

5.5 Performance of Pursuit Algorithms
105
pler thresholding algorithm. This way we show the two extreme methods and their
guaranteed performance. Again, we rely on the structure of the proof as shown in
Theorem 4.4 that handled the noiseless case. First, we state the theorem, and then
provide its proof.
Theorem 5.4. (Thresholding Performance): Consider the thresholding algorithm
applied to the problem instance (Pϵ
0) with the triplet (A, b, ϵ). Suppose that a vector
x0 ∈IRm satisﬁes ∥b −Ax0∥2 ≤ϵ and the sparsity constraint
∥x0∥0 < 1
2
 |xmin|
|xmax|
1
µ(A) + 1
!
−
ϵ
µ(A)|xmax|,
(5.50)
where |xmin| and |xmax| are the minimum and maximum values of the vector |x0| within
its support. The result produced by the thresholding algorithm must obey
∥xTHR −x0∥2
2 ≤
ϵ2
1 −µ(A)(∥x0∥0 −1).
(5.51)
Furthermore, this algorithm is guaranteed to recover a solution with the correct
support.
Before turning to prove this result, a short discussion is in order. The sparsity
requirement posed in Equation (5.50) is clearly sensitive to both the contrast of the
non-zeros in the solution x, and to the noise level. This is substantially diﬀerent
from the BPDN requirement that shows no similar sensitivities. On the other hand,
the error bound in Equation (5.51) is much better than the one obtained for the
BPDN. We assume that this is due to loss of tightness of the BPDN results.
Proof: We shall assume without loss of generality that the |S| = k0 non-zeros in
x0 are its ﬁrst entries, and thus b = e + Pk0
t=1 x0[t]at. Success of the thresholding
algorithm is guaranteed by the requirement
min
1≤i≤k0 |aT
i b| > max
j>k0 |aT
j b|,
(5.52)
Plugging the expression for b, the left-hand-side term becomes
min
1≤i≤k0 |aT
i b| = min
1≤i≤k0
aT
i e +
k0
X
t=1
x0[t]aT
i at
 .
(5.53)
We exploit the fact that the columns of A are normalized, and their maximal inner
product is bounded from above by µ(A). We also use the relations |a + b| ≥|a| −|b|,
and vTu ≤∥v∥2 · ∥u∥2. All these enable to lower-bound this term by
min
1≤i≤k0
aT
i e +
k0
X
t=1
xtaT
i at
 =
(5.54)
= min
1≤i≤k0
xi + aT
i e +
X
1≤t≤k0, t,i
xtaT
i at


106
5 From Exact to Approximate Solutions
≥min
1≤i≤k0
|xi| −|aT
i e| −

X
1≤t≤k0, t,i
xtaT
i at


≥min
1≤i≤k0 |xi| −max
1≤i≤k0

X
1≤t≤k0, t,i
xtaT
i at
 −ϵ
≥|xmin| −(k0 −1)µ(A)|xmax| −ϵ.
Turning to the right-hand-side term in Equation (5.52), and following similar steps,
we aim to upper-bound this expression. This gives
max
j>k0 |aT
j b| = max
j>k0
aT
i e +
k0
X
t=1
xtaT
j at
 ≤k0µ(A)|xmax| + ϵ.
(5.55)
Thus, requiring
|xmin| −(k0 −1)µ(A)|xmax| > k0µ(A)|xmax| + 2ϵ
(5.56)
necessarily leads to the satisfaction of the condition posed in Equation (5.52). Put
diﬀerently, this reads as the condition
k0 < 1
2
 |xmin|
|xmax|
1
µ(A) + 1
!
−
ϵ
µ(A)|xmax|
(5.57)
which guarantees the success of the thresholding algorithm in recovering the proper
support.
As for the deviation in the obtained solution, we shall assume that the above
requirement is fulﬁlled, and thus the proper support has been recovered. Thus, the
thresholding algorithm amounts to a simple Least-Squares solution of the form
xs
THR = arg min
x ∥Asx −b∥2
2 = (AT
s As)−1AT
s b
(5.58)
where this result refers only to the non-zero entries of the solution over the proper
columns from A, accumulated by the sub-matrix As. We denote similarly xs
0 as the
portion of the vector x0 on the proper support. Exploiting the fact that ∥Asxs
0 −b∥2 ≤
ϵ we obtain
∥xs
THR −xs
0∥2
2 = ∥(AT
s As)−1AT
s b −xs
0∥2
2
(5.59)
= ∥(AT
s As)−1AT
s b −(AT
s As)−1(AT
s As)xs
0∥2
2
= ∥(AT
s As)−1AT
s (b −Asxs
0)∥2
2
≤∥(AT
s As)−1AT
s ∥2
2 · ∥b −Asxs
0∥2
2
≤ϵ2 · ∥(AT
s As)−1AT
s ∥2
2 = ϵ2 · ∥A+
s ∥2
2.
The term ∥A+
s ∥2
2 is the ℓ2-induced (operator) norm, deﬁned as

5.6 Summary
107
∥B∥2
2 = max
v∈IR
n
∥Bv∥2
2
∥v∥2
2
= λmax(BTB) = λmax(BBT).
The fact that λmax(BTB) = λmax(BBT) can be directly derived by exploiting the
SVD5 of the matrix B. Returning to our analysis of the thresholding algorithm, we
need to bound from above the spectral radius of the matrix A+
s (A+
s )T = (AT
s As)−1. If
0 < λmin ≤· · · ≤λmax are the eigenvalues of the matrix AT
s As, then their reciprocals
are the eigenvalues of the inverse matrix, and thus the norm of the inverse is 1/λmin,
and thus
∥xs
THR −xs
0∥2
2 ≤ϵ2 · ∥A+
s ∥2
2
(5.60)
= ϵ2 · ∥(AT
s As)−1∥2
2
=
ϵ2
λmin(ATs As).
Thus, we need to bound this eigenvalue from below. Using the Gershgorin’s disk
theorem, all eigenvalues of AT
s As must be inside the disk centered around 1, with
radius (|S| −1)µ(A) = (k0 −1)µ(A) and thus λmin(AT
s As) ≥1 −(k0 −1)µ(A). This
leads to
∥xs
THR −xs
0∥2
2 ≤
ϵ2
λmin(ATs As) ≤
ϵ2
1 −(k0 −1)µ(A),
(5.61)
as claimed.
□
5.6 Summary
In this chapter we have generalized (P0) to allow for an error in the equality Ax = b,
and shown how most of the results from Chapters 2, 3, and 4 can be generalized to
cover this more practical problem. As mentioned in Chapter 4, here as well we are
under the dark shadows of too-pessimistic bounds, and these arise because of three
main reasons:
1. Noise modeling: Throughout this chapter, the noise e contaminating the vec-
tor Ax0 is assumed as a deterministic adversary vector of known length ϵ. This
means that it may choose the worst realization in order to interfere the analy-
sis/algorithm. By migrating from such a noise model to a random vector, better
results can be obtained, accompanied by a (near 1) probability for the claimed
bounds to be true. See Chapter 8, where we adopt such an approach for the anal-
ysis of the Dantzig-Selector.
5 Assuming that the SVD of B is given by B = UΣVT, we have that BBT = UΣVTVΣTUT =
UΣΣTUT, whereas BTB = VΣTUTUΣVT = VΣTΣVT. In both cases, the eigenvalues are simply
the squared singular ones.

108
5 From Exact to Approximate Solutions
2. Allowing rare failures: The analysis we perform in this chapter does not allow a
failure within the guaranteed range of the pursuit success/uniqueness. By allow-
ing a very small fraction of such failures, the bounds could become much higher,
and more optimistic. For most applications, this also reﬂects the true desire of
the implementor – a willingness to sacriﬁce a pursuit problem from time to time,
with the ability to generally claim a high rate of success. We adopt this point of
view in Chapter 7.
3. Worst-case characterization of A: The coherence, the RIP, and the spark, are all
worst-case measures that characterize the matrix A. Even if we improve our anal-
ysis with respect to the above two themes, using these measures in our bounds is
likely to lead to overly pessimistic results. The solution is a replacement of these
measures by others, more relaxed, such as a probabilistic RIP/coherence, etc.
There are more recent contributions that address the analysis of pursuit algorithms
with better results, taking the above ideas into consideration. We mention some of
those in Chapter 7, and we refer the interested readers to recent work by Ben-Haim
et al.
A fundamental optimization problem that has been deﬁned in this chapter is (Qλ
1).
This problem will accompany us throughout the next chapters, as it is key in the so-
lution of various problems in this ﬁeld. A reliable numerical solution of this problem
would be of great value, and especially so for high-dimensional problems, where
OMP, LARS, and IRLS cannot be trusted to perform well. In the next chapter we
focus on this problem and introduce a family of algorithms suited for handling (Qλ
1).
Further Reading
1. Z. Ben-Haim, Y.C. Eldar, and M. Elad, Coherence-based performance guar-
antees for estimating a sparse vector under random noise, submitted to IEEE
Trans. on Signal Processing, 2009.
2. S.S. Chen, D.L. Donoho, and M.A. Saunders, Atomic decomposition by basis
pursuit, SIAM Journal on Scientiﬁc Computing, 20(1):33–61 (1998).
3. S.S. Chen, D.L. Donoho, and M.A. Saunders, Atomic decomposition by basis
pursuit, SIAM Review, 43(1):129–159, 2001.
4. G. Davis, S. Mallat, and M. Avellaneda, Adaptive greedy approximations, Jour-
nal of Constructive Approximation, 13:57–98, 1997.
5. G. Davis, S. Mallat, and Z. Zhang, Adaptive time-frequency decompositions,
Optical-Engineering, 33(7):2183–91, 1994.
6. D.L. Donoho and M. Elad, On the stability of the basis pursuit in the presence
of noise, Signal Processing, 86(3):511–532, March 2006.
7. D.L. Donoho, M. Elad, and V. Temlyakov, Stable recovery of sparse overcom-
plete representations in the presence of noise, IEEE Trans. on Information The-
ory, 52(1):6–18, 2006.
8. B. Efron, T. Hastie, I.M. Johnstone, and R. Tibshirani, Least angle regression,
The Annals of Statistics, 32(2):407–499, 2004.

Further Reading
109
9. A.K. Fletcher, S. Rangan, V.K. Goyal, and K. Ramchandran, Analysis of de-
noising by sparse approximation with random frame asymptotics, IEEE Int.
Symp. on Inform. Theory, 2005.
10. A.K. Fletcher, S. Rangan, V.K. Goyal, and K. Ramchandran, Denoising by
sparse approximation: error bounds based on rate-distortion theory, EURASIP
Journal on Applied Signal Processing, Paper No. 26318, 2006.
11. J.J. Fuchs, Recovery of exact sparse representations in the presence of bounded
noise, IEEE Trans. on Information Theory, 51(10):3601–3608, 2005.
12. A.C. Gilbert, S. Muthukrishnan, and M.J. Strauss, Approximation of functions
over redundant dictionaries using coherence, 14th Ann. ACM-SIAM Sympo-
sium Discrete Algorithms, 2003.
13. I.F. Gorodnitsky and B.D. Rao, Sparse signal reconstruction from limited data
using FOCUSS: A re-weighted norm minimization algorithm, IEEE Trans. on
Signal Processing, 45(3):600–616, 1997.
14. R. Gribonval, R. Figueras, and P. Vandergheynst, A simple test to check the
optimality of a sparse signal approximation, Signal Processing, 86(3):496–510,
March 2006.
15. T. Hastie, R. Tibshirani, and J.H. Friedman, Elements of Statistical Learning.
New York: Springer, 2001.
16. L.A. Karlovitz, Construction of nearest points in the ℓp, p even and ℓ∞norms,
Journal of Approximation Theory, 3:123–127, 1970.
17. S. Mallat, A Wavelet Tour of Signal Processing, Academic Press, 1998.
18. M.R. Osborne, B. Presnell, and B.A. Turlach, A new approach to variable selec-
tion in least squares problems, IMA J. Numerical Analysis, 20:389–403, 2000.
19. V.N. Temlyakov, Greedy algorithms and m-term approximation, Journal of Ap-
proximation Theory, 98:117–145, 1999.
20. V.N. Temlyakov, Weak greedy algorithms, Advances in Computational Mathe-
matics, 5:173–187, 2000.
21. J.A. Tropp, Just relax: Convex programming methods for subset selection and
sparse approximation, IEEE Trans. on Information Theory, 52(3):1030–1051,
March 2006.
22. J.A. Tropp, A.C. Gilbert, S. Muthukrishnan, and M.J. Strauss, Improved sparse
approximation over quasi-incoherent dictionaries, IEEE International Confer-
ence on Image Processing, Barcelona, September 2003.
23. B. Wohlberg, Noise sensitivity of sparse signal representations: Reconstruc-
tion error bounds for the inverse problem. IEEE Trans. on Signal Processing,
51(12):3053–3060, 2003.

Chapter 6
Iterative-Shrinkage Algorithms
6.1 Background
In this chapter, our goal is the minimization of a function of the form
f(x) = λ1Tρ(x) + 1
2∥b −Ax∥2
2,
(6.1)
which we have seen before in several forms. The function ρ(x) operates entry-wise
on the vector x. As an example, for ρ(x) = |x|p we get that 1Tρ(x) = ∥x∥p
p, which
gives us the freedom to choose any p we ﬁnd as ﬁt. In this chapter we shall keep the
discussion general, and allow any “sparsity-promoting” function ρ(·). In particular,
the function posed above is a generalized version of what we have deﬁned as (Qλ
1).
Minimization of such functions can be treated using various classic iterative op-
timization algorithms ranging from Steepest-Descent and Conjugate-Gradient to
more involved interior-point algorithms. However, all these general purpose meth-
ods are often found ineﬃcient, requiring too many iterations and too many compu-
tations to reach their destination. This is especially the case for high-dimensional
problems, as often encountered in image processing. In such cases, the IRLS, OMP,
and LARS methods presented in previous chapters are also performing very poorly,
and an alternative is needed.
In recent years, a new family of numerical algorithms has been gradually built,
addressing the above optimization problem very eﬀectively. This family is the
Iterative-Shrinkage algorithms, that extend the classical Donoho-Johnston shrink-
age method for signal denoising. Roughly speaking, in these iterative methods, each
iteration comprises of a multiplication by A and its adjoint, along with a scalar
shrinkage step on the obtained result. Despite their simple structure, these algo-
rithms are shown to be very eﬀective in minimizing f(x) in Equation (6.1). A thor-
ough theoretical analysis that took place in the past several years proves the con-
vergence of these techniques, guarantees that the solution is the global minimizer
for convex f (for a convex choice of ρ), and studies the rate of convergence these
algorithms exhibit.
© Springer Science+Business Media, LLC 2010
M. Elad, Sparse and Redundant Representations: From Theory to Applications in Signal 
and Image Processing, DOI 10.1007/978-1-4419-7011-4_6,
111

112
6 Iterative-Shrinkage Algorithms
There are various Iterative-Shrinkage methods, with variations between them.
These algorithms are shown to emerge from very diﬀerent considerations, such as
the Expectation-Maximization (EM) algorithm in statistical estimation theory, prox-
imal point and surrogate functions, the use of the ﬁxed-point strategy, employment
of a parallel Coordinate-Descent (CD) algorithm, variations on greedy methods, and
more. This exposes the wealth with which one could design a solver for the above-
described optimization task. In this chapter we give a broad view of this group of
methods, their derivation, their speedup, and a little bit of their comparative perfor-
mance. We should note that in this chapter our main focus is the actual construction
of these algorithms, and the core intuition they are built on, and therefore some
important topics, such as convergence studies, are omitted.
6.2 The Unitary Case - A Source of Inspiration
6.2.1 Shrinkage For the Unitary case
We already saw in Chapter 5 that if A is unitary, the minimization of (Qλ
1) can be
very easy, with a closed-form solution that leads to shrinkage. The function f(x) is
somewhat more general, but as we shall see next, its treatment is exactly the same;
in particular, f(x) can be manipulated in a sequence of simple steps and turned into
a set of m (m = n) independent and identical one-dimensional optimization tasks
that are easily handled. Starting from Equation (6.1), using the identity AAT = I,
and exploiting the fact that ℓ2-norm is unitary-invariant we get
f(x) = 1
2∥b −Ax∥2
2 + λ1Tρ(x)
(6.2)
= 1
2∥A(ATb −x)∥2
2 + λ1Tρ(x)
= 1
2∥ATb −x∥2
2 + λ1Tρ(x) .
Denoting x0 = ATb we get
f(x) = 1
2∥x0 −x∥2
2 + λ1Tρ(x)
(6.3)
=
m
X
k=1
"1
2(x0[k] −x[k])2 + λρ(x[k])
#
=
m
X
k=1
g(x[k], x0[k]).
The minimization of the scalar function of the form g(x, a) = 0.5(x −a)2 + λρ(x)
with respect to x requires that we either zero the gradient (in the case of a smooth
ρ(x)), or show that the sub-gradient of g contains the zero, for the non-diﬀerentiable
case. These can be solved (analytically in some cases, or numerically in others –
this is done once and oﬀ-line), leading to the expression ˆxopt = Sρ,λ(a) that ﬁnds

6.2 The Unitary Case - A Source of Inspiration
113
the global minimizer of the scalar objective function g(x, a) described above. Note
that for non-convex choices of ρ there might be several solutions zeroing the (sub-
)gradient. Thus, all solutions must be found and compared by their value of the
function.
The obtained Shrinkage function is a curve that maps an input value a to the de-
sired output value ˆxopt. This function maps values near the origin to zero (Sρ,λ(a) = 0
for |a| ≤T), and those outside this interval are “shrinked,” thus the name of this op-
erator. The threshold T and the shrinkage eﬀects are both functions of the choices
of ρ and λ.
Returning to our original problem, we have found a closed-form solution to the
minimizer of f(x), based on the following two steps: (i) Compute x0 = ATb; and
(ii) Apply the operator Sρ,λ to each entry in x0 and obtain the desired ˆx. Note that
even if the original function is non-convex, the found solution is globally optimal if
Sρ,λ was designed properly.
The natural question raised by the above discussion is: When turning from a
unitary matrix A to a non-unitary (and perhaps non-square) one, must we lose all this
simplicity? As we show in the next section, one could handle the general problem
using similar shrinkage tools, but will be required to perform a sequence of them to
obtain the desired result.
6.2.2 The BCR Algorithm and Variations
At least when it comes to a matrix A built as a union of several unitary matrices, the
answer to the above question is positive. This was observed by Sardy, Bruce, and
Tseng in 1998, leading to their Block-Coordinate-Relaxation (BCR) algorithm. We
present this method here very brieﬂy, as it is a direct extension of the unitary case
solver shown above. For simplicity, we shall assume that A = [Ψ, Φ], where Ψ are
Φ are two n × n unitary matrices. Out minimization can be re-written with respect
to the two n-length portions of x, denoted as xΨ and xΦ,
f(x) = 1
2∥b −Ax∥2
2 + λ1Tρ(x)
(6.4)
= 1
2∥b −ΨxΨ −ΦxΦ∥2
2 + λ1Tρ(xΨ) + λ1Tρ(xΦ) = f(xΨ, xΦ).
The core idea in the BCR algorithm is to minimize f(xΨ, xΦ) with respect to the
two parts of x separately and alternately. This is why this algorithm is named Block-
Coordinate-Relaxation. Assume that we hold a current solution at the k-th iteration,
denoted by xk, built of the two portions, xk
Ψ and xk
Φ. Assuming that xk
Φ is kept ﬁxed,
the function f(xΨ, xk
Φ) can be written as
f(xΨ, xk
Φ) = 1
2∥˜b −ΨxΨ∥2 + λ1Tρ(xΨ),
(6.5)

114
6 Iterative-Shrinkage Algorithms
where ˜b = b −Φxk
Φ. This is the same function as in the unitary case, for which a
closed-form solution is readily available to us. This solution is given as
xk+1
Ψ
= Sρ,λ

ΨT ˜b

= Sρ,λ

ΨT(b −Φxk
Φ)

.
(6.6)
Similarly, once xk+1
Ψ
has been computed and now kept ﬁxed, the function f(xk+1
Ψ , xΦ)
is easily minimized with respect to xΦ with a closed-form expression, given by
xk+1
Φ
= Sρ,λ

ΦT(b −Ψxk+1
Ψ )

.
(6.7)
This way, alternating between these two update stages, the overall function is mono-
tonically decreasing, and proven to converge to a local minimum of the penalty
function (which becomes the global one for a convex ρ(·)).
An alternative to the above sequential process can be proposed, where the two
updates are done in parallel. As before, we start with a current solution at the k-th
iteration, denoted by xk, built of the two portions, xk
Ψ and xk
Φ. We can propose an
update of the two parts in parallel using the formula pair
xk+1
Ψ
= Sρ,λ

ΨT(b −Φxk
Φ)

xk+1
Φ
= Sρ,λ

ΦT(b −Ψxk
Ψ)

.
The only diﬀerence from the previous algorithm is in the update of xk+1
Φ
that uses xk
Ψ
instead of xk+1
Ψ . Merging these two into one formula can be done by the followings
simple steps, and exploiting the fact that the shrinkage operator operates on scalars
independently,
xk+1 =
" xk+1
Ψ
xk+1
Φ
#
=

Sρ,λ

ΨT(b −Φxk
Φ)

Sρ,λ

ΦT(b −Ψxk
Ψ)


(6.8)
= Sρ,λ
 " ΨT(b −Axk + Ψxk
Ψ)
ΦT(b −Axk + Φxk
Φ)
#!
= Sρ,λ
 " ΨT(b −Axk) + xk
Ψ
ΦT(b −Axk) + xk
Φ
#!
= Sρ,λ

AT(b −Axk) + xk
.
This way we got an interesting closed-form formula for the update of the entire
solution from one iteration to the next. Note that in this new form, nothing indicates
that A is expected to be of a special form (two-ortho). Naturally, we should wonder
whether this formula could be used for more general matrices A. Surprisingly, the
answer is positive, with some restrictions,1 as we shall see next.
We now turn to develop iterative algorithms of similar form that use a shrinkage
step within each iteration. These algorithms consider a general matrix A, and thus
1 Those restrictions refer to the operator norm of the matrix A that should be taken into account.

6.3 Developing Iterative-Shrinkage Algorithms
115
generalize the BCR approach done above. As we shall see, the formula obtained for
the parallel update emerges naturally in various forms.
6.3 Developing Iterative-Shrinkage Algorithms
There are diﬀerent ways to develop Iterative-Shrinkage algorithms. The ﬁrst to ex-
plicitly propose such an techniques were Starck, Murtagh, and Bijaoui in 1995, in
the context of image deblurring using wavelet. However, their construction was done
without a clear connection to the objective function in (6.1).
We choose to start our discussion with the proximal (or surrogate) functions that
were used by Daubechies, Defrise, and De-Mol in their construction. As we show,
this is also the foundation for the EM and the Bound-Optimization algorithm, as
proposed by Figueiredo and Nowak. Then we turn to describe two entirely diﬀerent
constructions: the ﬁrst, by Adeyemi and Davies, is based on the ﬁxed-point method
in optimization, coupled with the IRLS algorithm. The second method uses a paral-
lel coordinate descent algorithm. Finally, we discuss the StOMP method by Donoho,
Drori, Starck, and Tsaig, that develops an Iterative-Shrinkage method based on the
matching pursuit algorithm.
6.3.1 Surrogate Functions and the Prox Method
The discussion below is based on the work of Daubechies, Defrise, and De-Mol.
Considering the original function in Equation (6.1),
f(x) = 1
2∥b −Ax∥2
2 + λ1Tρ(x) ,
let us add to it the following term
d(x, x0) = c
2∥x −x0∥2
2 −1
2∥Ax −Ax0∥2
2 .
The parameter c will be chosen such that the function d(·) is strictly convex, imply-
ing that we require its Hessian to be positive-deﬁnite, cI−ATA ≻0. This is satisﬁed
by the choice c > ∥ATA∥2 = λmax(ATA). This new objective function
˜f(x) = 1
2∥b −Ax∥2
2 + λ1Tρ(x) + c
2∥x −x0∥2
2 −1
2∥Ax −Ax0∥2
2 ,
(6.9)
is the surrogate function that will be used in the proposed algorithm. As we shall see
next, the fact that the term ∥Ax∥2
2 drops in the new function, turns the minimization
task into a much simpler and eﬀective one. Opening the various terms in Equation
(6.9) and re-organizing it, we obtain a new expression of the form

116
6 Iterative-Shrinkage Algorithms
˜f(x) = 1
2∥b∥2
2 + 1
2∥Ax0∥2
2 + c
2∥x0∥2
2 −bTAx + λ1Tρ(x)
(6.10)
+c
2∥x∥2
2 −cxTx0 + xTATAx0
= Const1 −xT h
AT (b −Ax0) + cx0
i
+ λ1Tρ(x) + c
2∥x∥2
2 .
The constant in the above expression contains all the terms that are dependent on b
and x0 alone. Using the deﬁnition
v0 = 1
cAT (b −Ax0) + x0 ,
(6.11)
we can rewrite ˜f(x) (after division by c which has no impact on the minimization
goal) as
˜f(x) = Const2 −xTv0 + λ
c 1Tρ(x) + 1
2∥x∥2
2
(6.12)
= Const3 + λ
c 1Tρ(x) + 1
2∥x −v0∥2
2.
As it turns out, this penalty function is exactly the one we minimized for the unitary
case (see Equation (6.3)), and the solution is given as
xopt = Sρ,λ/c(v0) = Sρ,λ/c
 1
cAT (b −Ax0) + x0
!
.
(6.13)
This is a global minimizer of the function ˜f(x) in Equation (6.9).
So far we managed to convert the original function f to a new function ˜f, for
which we are able to get a closed-form expression for its global minimizer. This
change of the objective function depends on the choice of the vector x0. The core
idea of using the surrogate function is that we minimize the function f iteratively,
producing the sequence of results {xi}k, where at the (k+1)-th iteration we minimize
˜f with the assignment x0 = xk. Perhaps the more surprising of all is the fact that the
sequence of solutions {xk}k is proven to converge to the (local) minimizer of the
original function f. Thus, the proposed algorithm is simply an iterative application
of Equation (6.14) in the form
xk+1 = Sρ,λ/c
 1
cAT (b −Axk) + xk
!
.
(6.14)
We shall refer hereafter to this algorithm as the Separable Surrogate Functionals
(SSF) method, and it is described in details in Figure 6.1, with a possible addition
of a line-search. For the choice µ = 1 this reduces to the plain SSF algorithm as
developed here.
The above approach can be interpreted as the proximal-point algorithm, which
is well known in optimization theory. The function d(x, x0) is a distance measure to
the previous solution. When added to the original function at the k-th iteration, it

6.3 Developing Iterative-Shrinkage Algorithms
117
Task: Find x that minimizes f(x = λ1Tρ(x) + 1
2 · ∥b −Ax∥2
2.
Initialization: Initialize k = 0, and set
•
The initial solution x0 = 0.
•
The initial residual r0 = b −Axk = b.
Main Iteration: Increment k by 1, and apply these steps:
•
Back-Projection: Compute e = ATrk−1.
•
Shrinkage: Compute es = Shrink (xk−1 + e/c) with threshold λ.
•
Line search (optional): Choose µ to minimize the real valued function
f(xk−1 + µ(es −xk−1)).
•
Update Solution: Compute xk = xk−1 + µ(es −xk−1) .
•
Update Residual: Compute rk = b −Axk.
•
Stopping Rule: If ∥xk −xk−1∥2
2 is smaller than some predetermined thresh-
old, stop. Otherwise, apply another iteration.
Output: The result is xk.
Fig. 6.1 The SSF iterative-shrinkage algorithm with line-search.
promotes proximity between subsequent estimates of the iterative process. Surpris-
ingly, while this is expected to have an eﬀect of slowing down the algorithm, in the
case discussed here it is actually leading to a substantial speed-up. As a quadratic
and convex term, this distance function has the shape of an ellipsoid. For direc-
tions x −x0 that are close to the null-space of A, the distance is nearly Euclidean,
c∥x −x0∥2
2/2. For directions that are spanned by the rows of A, the distance drops
to nearly zero. Thus, d(x, x0) poses some sort of a narrow tunnel of permitted (or
preferred) solutions, which supports the overall problem.
As a ﬁnal point in this section, we draw the reader’s attention to the similarity
between the SSF algorithm and the iterative formula obtained for the parallel-BCR
in Equation (6.8). In fact, there is a slight diﬀerence – if the SSF is applied to a
two-ortho matrix A, then as λmax(ATA) = 2, we need to add a factor of 0.5 in
the SSF formula. However, such a factor is not necessary in practice – in a recent
analysis done by Combettes and Wajs it is shown that the constant c should satisfy
c > 0.5λmax(ATA) in order to guarantee a convergence of the SSF algorithm, exactly
as the formula in Equation (6.8) employs.
6.3.2 EM and Bound-Optimization Approaches
The discussion above could be replaced with a diﬀerent set of considerations,
and yet, one that leads to the very same algorithm. This new perspective is taken
from the work of Figueiredo, Nowak, and Bioucas-Dias, that uses the Expectation-

118
6 Iterative-Shrinkage Algorithms
Maximization (EM) estimator, or better-yet, its alternative deterministic optimiza-
tion foundation - the Bound-Optimization method.2
Starting from the function f(x) in Equation (6.1), which is hard to minimize, the
bound-optimization method suggests to use a related function Q(x, x0) that has the
following properties:
1. Equality at x = x0: Q(x0, x0) = f(x0);
2. Upper-bounding the original function: Q(x, x0) ≥f(x) for all x; and
3. Tangent3 at x0: ∇Q(x, x0)|x=x0 = ∇f(x)|x=x0.
Such a function necessarily exists if the Hessian of the original function is bounded
from above by ∇2 f(x) ⪯H, because then one can propose a bounding quadratic
function of the form Q(x, x0) = f(x0) + ∇f(x0)T(x −x0) + 0.5(x −x0)TH(x −x0),
satisfying these conditions exactly.
Using the function Q(x0, x0), the sequence of solutions generated by the recurrent
formula
xk+1 = arg min
x Q(x, xk)
(6.15)
is guaranteed to converge to a local minimum of the original function f(x). As can
be easily veriﬁed, the choice from the previous sub-section,
Q(x, x0) = 1
2∥b −Ax∥2
2 + λ1Tρ(x) + c
2∥x −x0∥2
2 −1
2∥Ax −Ax0∥2
2,
satisﬁes all these three conditions, and as such, leads to the desired optimization
algorithm. Note that beyond the above three conditions, we need to make sure that
minimization of Q is easier, as indeed happens, due to its separability. Also, the
value f(xi) is necessarily decreasing since
f(xk+1) ≤Q(xk+1, xk) = min
x Q(x, xk) ≤Q(xk, xk) = f(xk).
(6.16)
In the above inequality, the relation f(xk+1) ≤Q(xk+1, xk) is a direct consequence of
the second property of Q. Also, the relation minx Q(x, xk) ≤Q(xk, xk) is guaranteed
to give a strong decrease in the function’s value due to the second property. By the
Tailor expansion, it is easily veriﬁed that being tangent to the original function f at
xk implies a descent of at least ∇f(xk)TH−1∇f(xk), where H is an upper bound on
the original function’s Hessian.4
2 This technique is also known as the Majorization-Maximization method.
3 In fact, this requirement can be removed without harming the claims made.
4 Taking the quadratic function proposed above and minimizing it, the descent obtained is the one
mentioned here.

6.3 Developing Iterative-Shrinkage Algorithms
119
6.3.3 An IRLS-Based Shrinkage Algorithm
As we have seen in Chapter 5, a popular algorithm for minimization of f(x) in
Equation (6.1) is the Iterative-Reweighed Least-Squares (IRLS) algorithm. It con-
verts non-ℓ2-norms to ℓ2 ones by an introduction of weighting. In this approach, an
expression of the form 1Tρ(x) is replaced by 0.5xTW−1(x)x, where W(x) is a di-
agonal matrix, holding the values W[k, k] = 0.5x[k]2/ρ(x[k]) in its main-diagonal.
While this change by itself is trivial, when plugged wisely into an iterative process,
it leads to an appealing algorithm. Consider the function
f(x) = 1
2∥b −Ax∥2
2 + λ1Tρ(x) = 1
2∥b −Ax∥2
2 + λ
2xTW−1(x)x .
Assume that we hold a current solution x0, and we aim to update it. This is done in
two stages. The ﬁrst updates x assuming a ﬁxed W. This requires a minimization of
simple quadratic function, leading to the linear system
∇f(x) = −AT(b −Ax) + λW−1(x)x = 0 .
(6.17)
Thus, inversion of the matrix ATA + λW−1, either directly (for low-dimensional
problems) or iteratively (several Conjugate-Gradient steps) leads to the desired up-
date. Once found, we update W based on the new values of the found solution. This
is the IRLS core as already described in Chapter 5, and its performance is poor,
especially for high-dimensional problems.
An interesting twist over the above algorithm is proposed by Adeyemi and
Davies, leading to yet another Iterative-Shrinkage algorithm. Consider Equation
(6.17), and write it slightly diﬀerent, by adding and subtracting cx. The constant
c ≥1 is a relaxation constant, whose role is to be discovered soon. Thus,
−ATb + (ATA −cI)x + (λW−1(x) + cI)x = 0 .
(6.18)
A well-known technique in optimization, known as the ﬁxed-point iteration method,
suggests a way to construct iterative algorithms by assigning (wisely! so as to guar-
antee convergence and also allow for a simple computation) iteration counter to the
appearances of x in the equation. In our case, the proposed assignment would be
ATb −(ATA −cI)xk = (λW−1(xk) + cI)xk+1 ,
(6.19)
leading to the iterative equation
xk+1 =
λ
c W−1(xk) + I
−1  1
cATb −1
c(ATA −cI)xk
!
(6.20)
= S ·
 1
cAT(b −Axk) + xk
!
.
where we have deﬁned the diagonal matrix

120
6 Iterative-Shrinkage Algorithms
S =
λ
c W−1(xk) + I
−1
=
λ
c I + W(xk)
−1
W(xk) .
(6.21)
This matrix plays the role of shrinkage of the entries in the vector 1
cAT(b−Axk)+xk,
just as it is done in Equation (6.14). Indeed, every entry is multiplied by the scalar
value
0.5xk[i]2/ρ(xk[i])
λ
c + 0.5xk[i]2/ρ(xk[i]) =
xk[i]2
2λ
c ρ(xk[i]) + xk[i]2 .
(6.22)
This way, for large values of |xk[i]| this factor is nearly 1, whereas for small values
of |xk[i]| this value tends to zero, just as shrinkage does.
Note that this algorithm cannot be initialized with zeros, as in previous methods,5
since this vector is a stable solution to this ﬁxed-point iteration. Interestingly, once
an entry in the solution becomes zero, it can never be “revived,” implying that this
algorithm may get stuck, avoiding the near-by local minimum. Figure 6.2 presents
this algorithm in details.
The constant c is crucial for the convergence of this algorithm, and its choice is
similar to the one in the SSF algorithm. The iterative algorithm in Equation (6.20)
applies the matrix S·( 1
cATA−I) sequentially. For convergence, we must require that
the spectral radius of this matrix is below 1. Since ∥S∥2 ≤1, this implies that c should
be chosen such that the matrix 1
cATA −I is convergent. As the eigenvalues of this
matrix are in the range [−1, −1+λmax(ATA)/c], we should require c > λmax(ATA)/2,
which is twice more tolerant, compared to the choice made in the SSF algorithm.
6.3.4 The Parallel-Coordinate-Descent (PCD) Algorithm
We now turn to describe the PCD algorithm, developed by Elad, Matalon, and
Zibulevsky. The construction below starts from a simple coordinate descent algo-
rithm, but then merges a set of such descent steps into one easier joint step, leading
to the Parallel-Coordinate-Descent (PCD) Iterative-Shrinkage method. The ratio-
nale practiced here should remind the reader of the BCR algorithm by Sardy, Bruce,
and Tseng, that handled a special case of A being a union of unitary matrices.
Returning to f(x) in Equation (6.1), we start by proposing a Coordinate Descent
(CD) algorithm that updates one entry at a time in x while keeping the rest un-
touched. A sequence of such rounds of m steps (addressing each of the entries in
x ∈IRm) is necessarily converging. Interestingly, as we are about to show, each of
these steps is obtained via shrinkage.
Assuming that the current solution is x0, we desire to update the i-th entry around
its current value x0[i]. This leads to a 1D function of the form
g(z) = 1
2 ∥b −Ax0 −ai(z −x0[i])∥2
2 + λρ(z) .
(6.23)
5 In our tests we use a vector with a constant (0.001) in all its entries.

6.3 Developing Iterative-Shrinkage Algorithms
121
Task: Find x that minimizes f(x = λ1Tρ(x) + 1
2 · ∥b −Ax∥2
2.
Initialization: Initialize k = 0, and set
•
The initial solution x0 = 1.
•
The initial residual r0 = b −Axk = b.
Main Iteration: Increment k by 1, and apply these steps:
•
Back-Projection: Compute e = ATrk−1.
•
Shrink-Update:
Compute
the
diagonal
matrix
S
by
S [i, i]
=
xk[i]2/( 2λ
c ρ(xk[i]) + xk[i]2).
•
Shrinkage: Compute es = S (xk−1 + e/c).
•
Line search (optional): Choose µ to minimize the real valued function
f(xk−1 + µ(es −xk−1)).
•
Update Solution: Compute xk = xk−1 + µ(es −xk−1) .
•
Update Residual: Compute rk = b −Axk.
•
Stopping Rule: If ∥xk −xk−1∥2
2 is smaller than some predetermined thresh-
old, stop. Otherwise, apply another iteration.
Output: The result is xk.
Fig. 6.2 The IRLS-based iterative-shrinkage algorithm with line-search.
The vector ai is the i-th column in A. The term ai(z −x0[i]) removes the eﬀect of
the old value and adds the new one. Written diﬀerently, and denoting ˜b = b −Ax0 +
x0[i]ai, this function becomes
g(z) = 1
2
˜b −aiz
2
2 + λρ(z)
(6.24)
= 1
2∥˜b∥2
2 −˜bTaiz + 1
2∥ai∥2
2 · z2 + λρ(z)
= ∥ai∥2
2

∥˜b∥2
2
2∥ai∥2
2
−aT
i ˜b
∥ai∥2
2
z + z2
2 +
λ
∥ai∥2
2
ρ(z)

= ∥ai∥2
2

1
2
z −aT
i ˜b
∥ai∥2
2

2
+
λ
∥ai∥2
2
ρ(z)
+ Const.
The 1D function obtained has exactly the same form as the one shown in Equation
(6.3) for the unitary case, for which a shrinkage computation provides the minimizer.
Thus, the solution for obtaining the optimal z is given by
zopt = Sρ,λ/∥ai∥2
2

aT
i ˜b
∥ai∥2
2

(6.25)
= Sρ,λ/∥ai∥2
2

1
∥ai∥2
2
aT
i (b −Ax0) + x0[i]
.

122
6 Iterative-Shrinkage Algorithms
While this algorithm may work well in low-dimensional cases, it is impractical in
high-dimension problems in general, where the matrix A is not held explicitly, and
only multiplications by it and its adjoint are possible. Thus as this algorithm requires
an extraction of columns from A one at a time, it becomes prohibitive.
We consider a modiﬁcation of the above method that overcomes the problem
posed, and we rely on the following property: When minimizing a function, if there
are several descent directions, then any non-negative combination of them is also a
descent direction. Thus, we propose a simple linear combination of the set of all the
above m steps in Equation (6.25). Since each of these steps handles one entry in the
destination vector, we can write this sum as
v0 =
m
X
i=1
ei · Sρ,λ/∥ai∥2
2

1
∥ai∥2
2
aT
i (b −Ax0) + x0[i]
=
(6.26)
=

Sρ,λ/∥a1∥2
2

1
∥a1∥2
2 aT
1 (b −Ax0) + x0[1]

...
Sρ,λ/∥ai∥2
2

1
∥ai∥2
2 aT
i (b −Ax0) + x0[i]

...
Sρ,λ/∥am∥2
2

1
∥am∥2
2 aT
m(b −Ax0) + x0[m]


.
In the above expression, ei is the trivial m-dimensional vector with zeros in all entries
and 1 in the i-th one. Rewriting of this expression leads to the following simpler
formula
v0 = Sρ,diag(AT A)−1λ

diag(ATA)−1AT(b −Ax0) + x0

.
(6.27)
The term diag(ATA) consists of the norms of the columns of the dictionary A. These
are used both for the weighting of the back-projected error AT(b −Ax0), and for
the shrinkage operator. This set of weights can be computed oﬀ-line, before the
algorithm starts, and there exists a fast way to approximate these weights.6 Notice
that the obtained formula does not call for an extraction of columns from A as in
Equation (6.25), and the operations required are a direct multiplication of A and its
adjoint by vectors, as in Equation (6.14).
While each of the CD directions is guaranteed to descend, their linear combi-
nation is not necessarily descending without a proper scaling. Thus, we consider
this direction and perform a Line Search (LS) along it. This means that the actual
iterative algorithm is of the form
xk+1 = xk + µ (vk −xk) =
(6.28)
6 A stochastic algorithm can be proposed, based on the fact that for a white Gaussian vector u ∼
N(0, I) ∈IRn, the multiplication ATu has a covariance matrix ATA. Thus, by choosing a set of
such random vectors and computing their multiplication by AT, the variance of each entry is an
approximation of the required norms.

6.3 Developing Iterative-Shrinkage Algorithms
123
Task: Find x that minimizes f(x = λ1Tρ(x) + 1
2 · ∥b −Ax∥2
2.
Initialization: Initialize k = 0, and set
•
The initial solution x0 = 0.
•
The initial residual r0 = b −Axk = b.
Prepare the weights W = diag(ATA)−1.
Main Iteration: Increment k by 1, and apply these steps:
•
Back-Projection: Compute e = ATrk−1.
•
Shrinkage: Compute es = Shrink (xk−1 + We), using thresholds given as
λW[i, i].
•
Line search (optional): Choose µ to minimize the real valued function
f(xk−1 + µ(es −xk−1)).
•
Update Solution: Compute xk = xk−1 + µ(es −xk−1).
•
Update Residual: Compute rk = b −Axk.
•
Stopping Rule: If ∥xk −xk−1∥2
2 is smaller than some predetermined thresh-
old, stop. Otherwise, apply another iteration.
Output: The result is xk.
Fig. 6.3 The PCD iterative-shrinkage algorithm with line-search.
= xk + µ

Sρ,diag(AT A)−1λ

diag(ATA)−1AT(b −Axk) + xk

−xk

,
with µ chosen by a line-search algorithm of some sort. This means that we are
required to perform 1D optimization of the function
h(µ) = 1
2∥b −A (xk + µ(vk −xk)) ∥2
2 + λ1Tρ(xk + µ(vk −xk)) ,
which requires only two more multiplications of A by vk and xk. This overall process
will be referred to hereafter as the Parallel-Coordinate-Descent (PCD) algorithm.
This algorithm is presented in Figure 6.3.
As we can see, compared to the SSF in Equation (6.14), the method derived here
is diﬀerent in two ways: (i) the norms of the atoms in A play an important role in
weighting the back-projected error, whereas the previous algorithm uses a constant;
and (ii) the new algorithm requires a line-search to obtain a descent. We will return
to discuss these diﬀerences towards the end of this section, in an attempt to bridge
between the two methods.
6.3.5 StOMP: A Variation on Greedy Methods
The last algorithm we describe within this family of Iterative-Shrinkage methods
is the Stage-wise Orthogonal-Matching-Pursuit (StOMP). As opposed to the previ-

124
6 Iterative-Shrinkage Algorithms
ous methods, its construction does not emerge from an attempt to minimize f(x)
in Equation (6.1), although it could be attributed to the minimization of this func-
tion with the ℓ0-norm as the sparsity penalty. Instead, the creators of StOMP em-
bark from the Orthogonal-Matching-Pursuit (OMP) and the Least-Angle Regression
(LARS) algorithms. We should also mention that the StOMP algorithm is especially
tailored for cases where the matrix A is random, such as in compressed-sensing, al-
though it may be used in other cases with some success. Despite these major diﬀer-
ences between StOMP and the other methods in this chapter, we chose to describe
it here due to its general resemblance to an iterative-shrinkage procedure. StOMP
algorithm comprises of the following steps:
Initialization: We initialize the by ﬁxing the solution to be zero, x0 = 0, and thus
the residual vector is r0 = b −Ax0 = b. We also deﬁne the support of the found
solution and set it to be empty, I0 = φ. Turning to the ﬁrst iteration and beyond
(k ≥1), the following steps are performed.
Back-Project the Residual: Compute ek = AT(b −Axk−1) = ATrk−1. Most of
the obtained values are assumed to be zero-mean iid Gaussian due to the assumed
randomness of the dictionary. Few outliers in the above vector are those referring
to possible atoms in the construction of b.
Thresholding: We deﬁne the set of dominant entries in ek as Jk = {i| 1 ≤i ≤
m, |ek[i]| > T}, with a threshold chosen based on the permitted error ∥b −Ax∥2
2.
If the threshold is chosen such that |Jk| = 1, we get the OMP, as only one entry is
found and updated. In the more general case considered by StOMP, several and
even many entries can be chosen at once.
Union of Supports: We update the support of the desired solution to be Ik =
Ik−1 ∪Jk. Thus, this support is incrementally growing.
Restricted Least-Squares: Given the support and assuming that |Ik| < n, we
solve the Least-Squares problem
arg min
x ∥b −APx∥2
2 ,
(6.29)
where P ∈IRm×|Ik| is an operator that chooses the |Ik| entries from the vector x
that are permitted to be non-zeros. This minimization is done using a sequence
of K0 Conjugate-Gradient (CG) iterations, where each iteration multiplies by AP
and its adjoint.
Update of the Residual: Having found the updated vector xk, we compute the
residual to be rk = b −Axk.
As said above, when the thresholding step chooses one entry at a time, this is ex-
actly the OMP method. For the StOMP mode of operation with many entries cho-
sen at each step, the above process should be iterative several times (the authors
recommend 10 iterations, as each iteration can potentially add many elements to
the support), leading to the desired sparse solution of the approximated linear sys-
tem b ≈Ax. A shortcoming in StOMP is its tendency to over-populate the detected
support of the solution. A more recent work by Needell and Tropp proposes an alter-

6.3 Developing Iterative-Shrinkage Algorithms
125
native algorithm termed CoSaMP, that includes a mechanism that enables removal
of on-support elements based on their weak contribution.
StOMP comes with a thorough theoretical analysis that considers the case of ran-
dom dictionaries. In such a case, the values in ATei could be interpreted as a sparse
noisy vector, where the noise can be reliably assumed to be white zero-mean Gaus-
sian. Thus, a thresholding step that follows is nothing but a denoising algorithm. In
such a scenario, the creators of StOMP show an interesting phase transition from
success to failure.
We add the following important observation: Because of the K0 CG iterations
required, every iteration of StOMP parallels in complexity to K0 iterations of the
SSF, IRLS-based, and the PCD algorithms. In that respect, this structure should
remind the reader with the interior-point and the truncated Newton algorithms’ inner
solver.
6.3.6 Bottom Line – Iterative-Shrinkage Algorithms
Figure 6.4 presents the four Iterative Shrinkage algorithms described above as block
diagrams. As can be seen, although diﬀerent from each other in various ways, all
rely on the same main computational stages, such as the term b −Ax, its back
projection by multiplication by AT, and a thresholding/shrinkage step.
In this chapter we deliberately avoided discussing in depth the theoretical study
of the algorithms presented here. Nevertheless, we should state the following few
facts: the SSF and the PCD algorithms are guaranteed to converge to a local mini-
mizer of the function in Equation (6.1). For ρ(x) = |x|, where this objective function
becomes convex, a convergence to the global minimizer is guaranteed.
Among the four core algorithms described, the SSF, the IRLS-based, and the
PCD are closer to each other. Which of these is faster? Recall that the constant c
in SSF must satisfy c > λmax(ATA), and in the IRLS method half this value. On
the other hand, the PCD uses a scaling of the same term using the weight matrix
diag(ATA)−1. As an example, for a dictionary built as a union of N unitary matrices,
the SSF requires c > N, implying a weight of 1/N in Equation (6.14). In the PCD
algorithm, the weight matrix is simply identity due to the normalized columns in A.
Thus, PCD gives O(N) times stronger eﬀect to the term AT(b −Axk), and because
of that we expect it to perform better. We shall verify this assumption by conduct a
series of comparative experiments. We should mention that the convergence speed is
also expected to improve dramatically due to the accelerations, which are discussed
next.

126
6 Iterative-Shrinkage Algorithms
A
+
+
-
k
x
b
AT
1/c
+
+
+
S
k 1
 
x
 
c
 
A
+
+
-
k
x
b
AT
1/c
+
+
+
S
k 1
 
x
 
c
 
(a): SSF
(b): IRLS-Based
A
+
+
-
k
x
b
AT
W
+
+
+
S
 
W
 
Line
S
h
k 1
 
x
Search
A
+
+
-
k
x
b
AT
S
 
!
Merge
Supports
Least
Squares
k 1
 
x
(c): PCD
(d): StOMP
Fig. 6.4
Block diagrams of the four obtained Iterative-Shrinkage algorithms. Note that while
these four block-diagrams seems to be very similar, there are delicate diﬀerences between them in
various locations.

6.5 Iterative-Shrinkage Algorithms: Tests
127
6.4 Acceleration Using Line-Search and SESOP
All the above algorithms can be further accelerated in several ways. First, the idea of
performing a line search, as described in the PCD, is relevant to the other algorithms
just as well.7 All that is required is to use Equations (6.14) or (6.20) to compute a
temporary result xtemp, and then deﬁne the solution as xk+1 = xk + µ(xtemp −xk),
optimizing f(xk+1) with respect to the scalar µ.
A second, and much more eﬀective speed-up option is the deployment of the
Sequential Subspace Optimization (SESOP) method. The original SESOP algorithm
proposes to obtain the next iterate xk+1 via optimization of function f over an aﬃne
subspace spanned by the set of q recent steps {xk−i −xk−j−1}q−1
i=0 , and the current
gradient. This q+1-dimensional optimization task can be addressed using a Newton
algorithm, as this problem is deﬁned over a very low-dimension space. The main
computational burden in this process is the need to multiply these directions by A,
but these q multiplications can be stored in previous iterations, thus enabling the
SESOP speed-up algorithm with hardly any additional cost.
In the simple quadratic optimization case, SESOP algorithm with q ≥1 amounts
to regular CG. The q = 0 case is equivalent to a line-search. Preconditioning, like
multiplication of the current gradient by inverse of the Hessian diagonal, may sig-
niﬁcantly accelerate the method.
In the context of iterative-shrinkage algorithms, one can use the direction (xtemp−
xk), provided by SSF, IRLS-based, PCD, or even the StOMP algorithm, instead of
the gradient used in the standard SESOP. We shall demonstrate this approach in the
next section and show how eﬀective it is.
6.5 Iterative-Shrinkage Algorithms: Tests
In this section we present a synthetic experiment aiming to study the behavior of
the various algorithms described in this chapter. Our tests do not include StOMP,
because of the conceptual diﬀerences mentioned earlier. The experiment we perform
aims to minimize of the function
f(x) = 1
2∥b −Ax∥2
2 + λ∥x∥1.
The ingredients forming this problem are:
• The matrix A: We use a construction of the form A = PoutHPin, where H is the
216×216 Hadamard matrix, Pin is a diagonal matrix (of the same size) with values
on the main-diagonal growing linearly in the range [1, 5], and Pout is built as the
identity matrix from which we choose a random subset of 214 rows. Thus, the
overall matrix A is of size 214 × 216, that scales the entries of the input vector x,
7 Even though StOMP has not been derived as a minimizer of the function in Equation (6.1), one
can still apply this line-search and extract the best of each iteration.

128
6 Iterative-Shrinkage Algorithms
rotates it in the IR216-space, and then chooses a subset of n = 214 of the resulting
entries. The multiplication by A or its adjoint can be done very fast, due to the
availability of a Fast-Hadamard Transform (FHT) operation.
• the reference solution x0: We generate a sparse vector x0 of length m = 216
that contains 2000 non-zero in random locations, and with values drawn from
the normal Gaussian distribution.
• The vector b: The vector b is computed by b = Ax0 + v, where v is a random
vector with iid entries, drawn from a Gaussian distribution with zero mean and
standard-deviation σ = 0.2. This choice of variance leads to ∥Ax0∥2
2/∥b−Ax0∥2
2 ≈
8, implying that the “signal” part ∥Ax0∥2
2 is 8 times stronger than the noise added
∥v∥2
2.
• Choice of λ: We choose λ empirically so as to lead to a residual that satisﬁes
1
2∥b −Ax∥2
2 ≈nσ2. The value chosen (after several experiments) is λ = 1.
The algorithms we compare herein are the SSF, the IRLS-based algorithm, and the
PCD. For the SSF, we consider its regular form, as given described in Figure 6.1
(with µ = 1), a line-search (LS) version (ﬁnd µ), and an accelerated version with
SESOP of dimension 5. Similarly, we test the IRLS method and a SESOP-5 accel-
erated version of it. As for the PCD, we test its regular mode that uses a line-search,
and a SESOP-5 acceleration.
Throughout the simulations presented in this chapter, the inner optimization for
the line-search and SESOP are done using 50 plain Steepest-Descent iterations. In
Chapter 10 we shall return to the iterative-shrinkage algorithms in the context of a
speciﬁc image processing application. There we consider a smoother version of ℓ1
for the function ρ, thus enabling a Newton optimization.
Before turning to present the results obtained, we should discuss the constant
c in the SSF and IRLS algorithms. This should be chosen as c > ∥ATA∥2
2 =
∥PT
inHTPT
outPoutHPin∥2
2. If we omit Pout (e.g., assume that n = 216) then this gives
c > 25, due to the maximal magniﬁcation by the main-diagonal of Pin. The fact
that Pout extracts some of this energy by choosing a subset of n entries implies that
the choice c = 25 is correct and close to be the best possible value. Similarly, the
squared-norms of the atoms in A, which are used by PCD, are easily obtained by
taking the entries from the main-diagonal of Pin squaring them, and multiplying by
n/m. These are the exact norms, due to the properties of H.
We now turn to present the results obtained. First, we show graphs presenting
the value of f(x) as a function of the iterations per each method. These are shown
in Figures 6.5-6.7, where the minimum value of f(x) (found by the best of these
iterative methods) is subtracted. The ﬁrst of these ﬁgures presents the various SSF
algorithms (plain, LS, and SESOP-5). The second ﬁgures presents the IRLS results,
compared to the SSF, and the third show the PCD methods compared to the SSF.
We also show the quality of the solution obtained, measured by the ratio ∥xk −
x0∥2
2/∥x0∥2
2. These values should be below 1 and close to 0 to indicate that the relative
error in the solution is small. These results are shown in Figures 6.8- 6.10.
Several conclusions can be drawn from these results:

6.5 Iterative-Shrinkage Algorithms: Tests
129
0
10
20
30
40
50
10
−1
10
0
10
1
10
2
10
3
Iterations
f(xk)−fmin
 
 
SSF
SSF−LS
SSF−SESOP−5
Fig. 6.5
The objective function value as a function of the iterations for the various tested algo-
rithms – the plain SSF, the SSF with a line-search, and the SSF with a SESOP-5 acceleration.
0
10
20
30
40
50
10
−1
10
0
10
1
10
2
10
3
Iterations
f(xk)−fmin
 
 
SSF−LS
IRLS
IRLS−SESOP5
Fig. 6.6
The objective function value as a function of the iterations for the various tested algo-
rithms – the SSF with a line-search, the plain IRLS, and the a SESOP-5 acceleration of it.

130
6 Iterative-Shrinkage Algorithms
0
10
20
30
40
50
10
−1
10
0
10
1
10
2
10
3
Iterations
f(xk)−fmin
 
 
SSF−LS
PCD−LS
PCD−SESOP−5
Fig. 6.7
The objective function value as a function of the iterations for the various tested algo-
rithms – the SSF with a line-search, the PCD (with a line-search), and a SESOP-5 accelerated
version of it.
0
10
20
30
40
50
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Iterations
||xk − x0||2
 
 
SSF
SSF−LS
SSF−SESOP−5
Fig. 6.8 The solution’s quality measure as a function of the iterations for the various tested algo-
rithms – the plain SSF, the SSF with a line-search, and the SSF with a SESOP-5 acceleration.

6.5 Iterative-Shrinkage Algorithms: Tests
131
0
10
20
30
40
50
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Iterations
||xk − x0||2
 
 
SSF−LS
IRLS
IRLS−SESOP5
Fig. 6.9 The solution’s quality measure as a function of the iterations for the various tested algo-
rithms – the SSF with a line-search, the plain IRLS, and the a SESOP-5 acceleration of it.
0
10
20
30
40
50
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Iterations
||xk − x0||2
 
 
SSF−LS
PCD−LS
PCD−SESOP−5
Fig. 6.10
The solution’s quality measure as a function of the iterations for the various tested
algorithms – the SSF with a line-search, the PCD (with a line-search), and a SESOP-5 accelerated
version of it.

132
6 Iterative-Shrinkage Algorithms
• There is a general agreement between the two measures shown, thus enabling us
to concentrate on Figures 6.5-6.7.
• The best performing method among the three (SSF, IRLS, and PCD) is the PCD.
It provides a nearly perfect converged result after 15 iterations.
• A line-search added to these algorithms helps their convergence properties. Sim-
ilarly, SESOP acceleration is indeed helping. The accelerated PCD requires 10
iterations to converge. A smaller eﬀect is observed by the accelerated SSF.
• As expected, the IRLS-based algorithm is weaker, and tends to get stuck.
How good is the result obtained in terms of recovering the true sparse combination
of atoms that generated b? Figure 6.11 presents the original vector x0 and the solu-
tion obtained by the PCD after 10 iterations. As said above, there are 2000 non-zeros
in the original vector, i.e., ∥x0∥0 = 2000. The PCD solution detects 2560 non-zeros,
out of those 1039 are from the true support (the dominant ones, and mostly those
far from the origin, due to the relative strength of the non-zero versus the noise).
A zoomed-in graph in the bottom of 6.11 shows that indeed most major peaks in
x0 are detected well, and attributed with a value that is somewhat smaller. This ten-
dency is expected, since the objective function we use penalizes for the magnitude
of the non-zeros. We can ﬁx this by taking the found support from x, denoted as S,
and solving a constrained Least-Squares problem of the form
min
x
∥Ax −b∥2
2 subject to Support(x) = S,
(6.30)
which can be performed easily by an iterative procedure of the form
xk+1 = ProjS
h
xk −µAT(Ax −b)
i
.
(6.31)
The operator ProjS projects the vector on the support, essentially nulling the entries
outside it. The result of this procedure are shown in Figure 6.12, and as can be seen,
the non-zero values are indeed getting closer to those of the original x0. Note that in
terms of the measure ∥xk −x0∥2
2/∥x0∥2
2, whereas the original value obtained by the 10
PCD iterations is 0.36, after this adjustment it becomes 0.224, showing a substantial
improvement.
Just before leaving this chapter, we should draw the reader’s attention to the fact
that in Chapter 10 we revisit the topic of iterative-shrinkage algorithms, deployed to
an image deblurring application. The experiments there include further comparisons
between some of the algorithms mentioned here, and thus provide both a practical
view for the need for these methods, along with extended comparison of SSF, PCD,
and their accelerated versions.
6.6 Summary
In the ﬁeld of sparse representation modeling, minimization of the function

6.6 Summary
133
1
2
3
4
5
6
x 10
4
−4
−3
−2
−1
0
1
2
3
4
index [1 to m]
Amplitude
 
 
x0
xk
4
4.05
4.1
4.15
4.2
x 10
4
−2.5
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
2.5
index [1 to m]
Amplitude
 
 
x0
xk
Fig. 6.11 The solution as obtained after 10 iterations of the PCD algorithm. The graphs show the
original vector x0 and the computed one xk, where the top shows the complete vectors, and the
bottom graph presents a small slice of the horizontal support.

134
6 Iterative-Shrinkage Algorithms
4
4.05
4.1
4.15
4.2
x 10
4
−2.5
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
2.5
index [1 to m]
Amplitude
 
 
x0
xk
xk adjusted
Fig. 6.12 The same graph as the one shown in the bottom of Figure 6.11, with the least-squared
adjusted solution over the support found.
f(x) = λ1Tρ(x) + 1
2∥b −Ax∥2
2,
plays a vital role for various applications, and this will be clearly seen, as we move
to later chapters that introduce the practical sides of this ﬁeld. In this chapter we
introduced a family of eﬃcient optimization techniques to achieve this goal. These
are the iterative-shrinkage algorithms, which employ iterations that consist of mul-
tiplication by the matrix A and its adjoint, and a scalar shrinkage operation on the
temporary solution found. These methods are extremely important when dealing
with high-dimensional problems.
Beyond their relative eﬃciency in minimizing f(x), an important feature of these
methods is the close relationship they have with the more classical shrinkage algo-
rithm that was developed originally for the unitary case. We have introduced several
such algorithms, and presented a basic experiment to demonstrate their behavior.
The research on these methods is ongoing, exploring new techniques (e.g. Breg-
man’s and Nesterov’s multi-stage algorithms), analyzing convergence behavior, and
proposing applications that employ these in practice.
Further Reading
1. S. Becker, J. Bobin and E. Candes, NESTA: A fast and accurate ﬁrst-order
method for sparse recovery, Preprint, 2009.

Further Reading
135
2. J. Bioucas-Dias, Bayesian wavelet-based image deconvolution: a GEM algo-
rithm exploiting a class of heavy-tailed priors, IEEE Trans. on Image process-
ing, 15(4):937–951, April 2006.
3. T. Blumensath and M.E. Davies, Iterative thresholding for sparse approxima-
tions, Journal of Fourier Analysis and Applications, 14(5):629–654, 2008.
4. J. Bobin, Y. Moudden, J.-L. Starck, and M. Elad, Morphological diversity and
source separation, IEEE Signal Processing Letters, 13(7):409–412, July 2006.
5. J. Cai, S. Osher, and Z. Shen, Convergence of the linearized Bregman iteration
for l1-norm minimization, Mathematics of Computation, 78:2127–2136, 2009.
6. J. Cai, S. Osher, and Z. Shen, Linearized Bregman iterations for compressed
sensing, Mathematics of Computation, 78:1515–1536, 2009.
7. J. Cai, S. Osher, and Z. Shen, Linearized Bregman iteration for frame based
image deblurring, SIAM Journal on Imaging Sciences , 2(1):226-252, 2009.
8. P.L. Combettes and V.R. Wajs, Signal recovery by proximal forward-backward
splitting, Multiscale Modeling and Simulation, 4(4):1168–1200, November
2005.
9. I. Daubechies, M. Defrise, and C. De-Mol, An iterative thresholding algorithm
for linear inverse problems with a sparsity constraint, Communications on Pure
and Applied Mathematics, LVII:1413–1457, 2004.
10. D.L. Donoho, De-noising by soft thresholding, IEEE Trans. on Information
Theory, 41(3):613–627, 1995.
11. D.L. Donoho and I.M. Johnstone, Ideal spatial adaptation by wavelet shrinkage,
Biometrika, 81(3):425–455, 1994.
12. D.L. Donoho, I.M. Johnstone, G. Kerkyacharian, and D. Picard, Wavelet shrink-
age - asymptopia, Journal of the Royal Statistical Society Series B - Method-
ological, 57(2):301–337, 1995.
13. D.L. Donoho and I.M. Johnstone, Minimax estimation via wavelet shrinkage,
Annals of Statistics, 26(3):879–921, 1998.
14. D.L. Donoho, Y. Tsaig, I. Drori, and J.-L. Starck, Sparse solution of underde-
termined linear equations by stagewise orthogonal matching pursuit, Technical
Report, Stanford University, 2006.
15. M. Elad, Why simple shrinkage is still relevant for redundant representations?,
to appear in the IEEE Trans. on Information Theory.
16. M. Elad, B. Matalon, and M. Zibulevsky, Image denoising with shrinkage and
redundant representations, IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), NY, June 17-22, 2006.
17. M. Elad, B. Matalon, and M. Zibulevsky, On a Class of Optimization meth-
ods for Linear Least Squares with Non-Quadratic Regularization, to appear in
Applied and Computational Harmonic Analysis.
18. M. Elad, J.-L. Starck, P. Querre, and D.L. Donoho, Simultaneous cartoon
and texture image inpainting using morphological component analysis (MCA),
Journal on Applied and Comp. Harmonic Analysis, 19:340–358, 2005.
19. M.J. Fadili, J.-L. Starck, Sparse representation-based image deconvolution by
iterative thresholding, Astronomical Data Analysis ADA’06, Marseille, France,
September, 2006.

136
6 Iterative-Shrinkage Algorithms
20. M.A. Figueiredo and R.D. Nowak, An EM algorithm for wavelet-based image
restoration, IEEE Trans. Image Processing, 12(8):906–916, 2003.
21. M.A. Figueiredo, and R.D. Nowak, A bound optimization approach to wavelet-
based image deconvolution, IEEE International Conference on Image Process-
ing - ICIP 2005, Genoa, Italy, 2:782–785, September 2005.
algorithms for wavelet-based image restoration, to appear in the IEEE Trans. on
Image Processing.
23. D. Needell and J. A. Tropp, CoSaMP: Iterative signal recovery from incomplete
and inaccurate samples. to appear in Appl. Comp. Harmonic Anal..
24. Y. Nesterov, Smooth minimization of non-smooth functions, Math. Program.,
Serie A., 103:127–152, 2007.
25. S. Sardy, A.G. Bruce, and P. Tseng, Block coordinate relaxation methods for
nonparametric signal denoising with wavelet dictionaries, Journal of computa-
tional and Graphical Statistics, 9:361–379, 2000.
26. J.-L. Starck, M. Elad, and D.L. Donoho, Image decomposition via the combina-
tion of sparse representations and a variational approach. IEEE Trans. on Image
Processing, 14(10):1570–1582, 2005.
27. J.-L. Starck, F. Murtagh, and A. Bijaoui, Multiresolution support applied to im-
age ﬁltering and restoration, Graphical Models and Image Processing, 57:420–
431, 1995.
22. M.A. Figueiredo, J.M. Bioucas-Dias, and R.D. Nowak, Majorization-minimization

Chapter 7
Towards Average PerformanceAnalysis
The analysis presented so far presents a simple but limited portrait of the ability of
concrete algorithms to ﬁnd sparse solutions and near-solutions. In this chapter we
brieﬂy point to the interesting and challenging research territory that lies beyond
these worst-case results. We start with some simple simulations to motivate this
discussion.
7.1 Empirical Evidence Revisited
We have presented experiments that compare pursuit algorithms, both in Chapter 3
and later in Chapter 5. We return to these here, with an emphasis on the success in
recovering the support of the true solution.
Consider a random matrix A of size 100×200, with entries independently drawn
by random from a Gaussian distribution of zero mean and unit variance, N(0, 1).
The spark of this matrix is 101 with probability 1, implying that every solution for
the system Ax = b with 50 entries and below is necessarily the sparsest possible,
and as such, it is the solution of (P0). By randomly generating such suﬃciently
sparse vectors x0 (choosing the non-zero locations uniformly over the support in
random, and their values from N(0, 1)), we generate vectors b. This way, we know
the sparsest solution to Ax0 = b, and we are able to compare this to algorithmic
results.
The graph presented in Figure 7.1 shows the success rate for both OMP and
BP (implemented by Matlab’s Linear-Programming solver) in recovering the true
(sparsest) solution. The success is measured by the computing ∥ˆx −x0∥2
2/∥x0∥2
2 and
checking that is below a negligible value (in our experiments this is set to 1e −5),
to indicate a perfect recovery of the original sparse vector x0. For each cardinality,
100 repetitions are conducted and their results averaged to give the probability of
success. The value of the mutual-coherence of A in this experiment is µ(A) = 0.424,
so that only for cardinalities lower than (1 + 1/µ(A))/2 = 1.65 pursuit methods
are guaranteed to succeed. As we can see, both pursuit algorithms succeed in the
© Springer Science+Business Media, LLC 2010
M. Elad, Sparse and Redundant Representations: From Theory to Applications in Signal 
137
and Image Processing, DOI 10.1007/978-1-4419-7011-4_7,

138
7 Towards Average PerformanceAnalysis
0
10
20
30
40
50
60
70
0
0.2
0.4
0.6
0.8
1
Cardinality of the solution
Probability of success
 
 
Basis Pursuit
Matching Pursuit
Fig. 7.1 Probability of success of pursuit algorithms in the recovery of the sparsest solution of the
linear system Ax = b. The results are shown as a function of the cardinality of the desired solution.
recovery of the sparsest solution for 1 ≤∥x0∥0 ≤26, far beyond the coverage of
the above-described theoretical bound. We can also see that the greedy algorithm
(OMP) is performing somewhat better than the BP.
Turning to the noisy case (solution of (Pϵ
0)), we use the same matrix A, and gener-
ate a random vector x0 with a pre-speciﬁed cardinality of non-zeros. It is normalized
so that ∥Ax0∥2 = 1. We compute b = Ax0+e, where e is a random vector with prede-
termined norm ∥e∥2 = ϵ = 0.1. Thus, the original vector x0 is a feasible solution for
(Pϵ
0), and close to the optimal due to its sparsity. The theoretical study in Chapter 5
suggests a stable recovery of x0 (within an error of order ϵ) for cardinalities smaller
than (1 + 1/µ(A))/2 (or even division by 4 for the BP case), and again these lead to
overly pessimistic prediction of performance.
The graph presented in Figure 7.2 shows the relative error for OMP and BP
(we use the LARS solver) for this test.1 This relative error is deﬁned as the ratio
∥ˆx −x0∥2
2/ϵ2, averaged over 100 experiments. In both tested algorithms, the solution
is such that ∥Aˆx −b∥2 ≤ϵ. As can be seen, both methods do very well for all cardi-
nalities tested, showing strong tendency to stability, while the theoretical results set
a much lower bound for stability to hold.
As the solution of (Pϵ
0) may be interpreted as noise cleaning, we also present in
Figure 7.3 the averaged relative error ∥ˆx−x0∥2
2/∥x0∥2
2 obtained in these experiments.
This ratio should be as small as possible, and a value of 1 is comparable to an
1 The observant reader may detect the similarity between the results shown in this graph, and those
that were given in Chapter 5 Figure 5.12. Note, however, that the experiment setting is somewhat
diﬀerent.

7.1 Empirical Evidence Revisited
139
0
10
20
30
40
50
60
70
−0.1
0
0.1
0.2
0.3
0.4
0.5
Cardinality of the solution
Relative L2 Error
 
 
Basis Pursuit
Matching Pursuit
Fig. 7.2 The tendency of pursuit algorithms to provide a stable recovery of the sparsest solution
of the linear system Ax = b in the presence of noise.
0
10
20
30
40
50
60
70
−0.1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Cardinality of the solution
||x−x0||2/||x0||2
 
 
Basis Pursuit
Matching Pursuit
Fig. 7.3 The noise cleaning eﬀect of pursuit algorithms, as a function of the cardinality of x0.
approximation of plain zeros for ˆx. We see that the two methods tested exhibit good
noise cleaning behavior, and as expected, it is of stronger eﬀect for low-cardinalities
in x0.
These simulations show that the obtained bounds do not tell the whole story.
When working with matrices having speciﬁc properties, or with problems exhibit-

140
7 Towards Average PerformanceAnalysis
ing structured sparsity, we might ﬁnd that the worst-case analysis gives very weak
guarantees compared to what actually happens.
7.2 A Glimpse into Probabilistic Analysis
7.2.1 The Analysis Goals
Realizing that there is such a marked gap, several researchers developed in recent
years (2006-2009) new results on the performance of pursuit algorithms, put for-
ward using a probabilistic machinery. In this section we review some of these results
and bring them without proofs. In the next section we shall concentrate on one such
work by Schnass and Vandergheynst and bring its complete derivation. Their work
studies the performance of the thresholding algorithm in the noiseless setting, and
it illustrates the potential in adopting a probabilistic point of view when analyzing
pursuit algorithms. The appeal in this speciﬁc work is the relative ease with which
one can obtain stronger bounds, a fact that does not migrate well to other pursuit
methods.
Before turning to describe speciﬁc attempts to alternative analysis, we should
state our goal clearly. The ultimate objective would be a theoretical analysis that
provides the average performance of pursuit algorithms, namely, the probability
performance curves shown in the previous chapter’s experiments. As this may be
too diﬃcult to obtain, we could relax our goals and be satisﬁed with probabilis-
tic performance curves that bound the true ones from below, thus providing some
guarantees for error-rates per a given cardinality.
Since we are mostly interested in the region of near-perfect performance, we
could focus on the low-cardinalities zone in the above curves. Considering a
success-rate of 1 −δ and beyond (higher) for an arbitrarily small value δ, we may
propose an analysis that aims to show that such error-rates are achievable by some
cardinalities below an (hopefully high) upper-bound threshold, which lower-bounds
the true point on the empirical curve. In this respect, earlier worst-case results cor-
respond to such an analysis for δ = 0 (assuming that the results obtained are tight).
Such line of reasoning implies that we allow a small but controlled fraction of
errors. Indeed, many of the existing methods that take this analysis route choose to
specify their results in an asymptotic terminology with respect to the dimensions of
the problem, aiming to show that δ(n) is tending to zero, implying that encountering
such failures becomes extremely rare. Another immediate and fundamental impli-
cation of this type of study is that the problem set deﬁned by the triplet {A, b, ϵ}
must be modelled probabilistically, so as to enable any claim about probability of
failure. Diﬀerent works provide diﬀerent modeling of this randomness, as we shall
see below.

7.2 A Glimpse into Probabilistic Analysis
141
7.2.2 Two-Ortho Analysis by Candes & Romberg
A pioneering and the very ﬁrst attempt reported on probabilistic analysis of pursuit
algorithms is the work (developed during 2004, and published in 2006) by Em-
manuel Candes and Justin Romberg. This work revisits the uniqueness and BP-
equivalence results for the two-ortho noiseless case, as presented in Chapters 2 and
4. Their work provides a robustiﬁed analysis that permits a negligible and dimin-
ishing amount of outliers, and thus leads to stronger claims on both the uniqueness
and equivalence fronts. This way, they avoid the worst-case limitations enforced in
earlier studies.
As already mentioned above, in order to provide a probabilistic analysis of any
sort, one must start with a probabilistic modeling of the problem. The work by Can-
des and Romberg considers a deterministic two-ortho matrix A = [Ψ, Φ] of size
n × 2n, multiplied by random sparse vectors x. These sparse vectors are randomly
generated by considering their two portions, xΨ and xΦ, each having a ﬁxed and
a-priori chosen cardinality NΨ and NΦ. All possible
 n
NΨ

supports for xΨ are con-
sidered as equably probable, and one of them is drawn at random. The non-zeros
in this vector are iid random variables, drawn from an arbitrary continuous and cir-
cularly symmetric (for complex numbers) probability density function. The same
process is repeated for the creation of xΦ. The outcome of the multiplication is
b = Ax = ΨxΨ + ΦxΦ, leading to the deﬁnition of the linear system {A, b} for
which we are interested in the solution of (P0).
Having deﬁned our problem suite, the ﬁrst question addressed refers to the
uniqueness of a sparse solutions of (P0). Allowing for a small and negligible num-
ber of outliers, what would be an upper bound on NΨ + NΦ so as to guarantee
that such x is the unique (sparsest) solver of (P0)? The second question considers
a similar bound that guarantees a successful recovery of such sparse vectors x by
Basis-Pursuit, again allowing of a small amount of failures.
Recall that for both the uniqueness and the equivalence to hold true, previously
stated results set bounds proportional to 1/µ(A), which in turn is proportional to √n
at best. The novelty in the work by Candes and Romberg is the establishment of new
bounds, proportional to 1/µ(A)2 with some log factor (to the power of 6), and thus
at best behave like n/ log6 n. Those bounds guarantee the uniqueness of the sparsest
solution, and the fact that it is recovered by the solution of (P1), and both are shown
to be correct with high-conﬁdence as n →∞. More speciﬁcally, the following two
theorems are developed by Candes and Romberg.
Theorem 7.1. Consider the linear system Ax = b with A = [Ψ, Φ] and b = Ax0,
where x0 is drawn at random as described above with cardinalities NΨ and NΦ such
that
NΨ + NΦ ≤
C′
β
µ(A)2 log6 n
.
(7.1)

142
7 Towards Average PerformanceAnalysis
Then, with probability at least 1 −O(n−β), the vector x0 is the solution of (P0), i.e.,
it is the sparsest possible solution to Ax = b.
Theorem 7.2. Consider the linear system Ax = b with A = [Ψ, Φ] and b = Ax0,
where x0 is drawn at random as described above with cardinalities NΨ and NΦ such
that
NΨ + NΦ ≤
C′′
β
µ(A)2 log6 n
.
(7.2)
Then, with probability at least 1 −O(n−β), the vector x0 is the solution of (P1), i.e.,
it is found exactly by the Basis-Pursuit.
The constants C′
β > C′′
β > 0 are not speciﬁed, but they are relatively small, and
depend only on β, which in turn controls the accuracy of these claims. Both this,
and the fact that these Theorems fail with probability proportional to n−β imply that
these new results become relevant and stronger than the worst-case ones for large
enough values of n.
While we will not provide the proof for these two results (they are too much in-
volved), we shall brieﬂy explain one critical ingredient in their derivations. Denoting
by ΦNΨ and ΨNΦ the sub-matrices that contain the columns over the chosen support,
the proofs rely strongly on the analysis of the norm of the NΨ × NΦ matrix ΦT
NΨ ΦNΦ,
aiming to show that it is small enough. This should remind the reader of parallel
attempts seen earlier to analyze a portion of the Gram matrix AT
s As (where s stands
for the restriction to the support), and claim that it is positive-deﬁnite for as large
as possible |s|. Put diﬀerently, considering the matrix AT
s As −I, which removes the
main-diagonal, our desire was to show that this matrix has a small spectral radius
(and thus norm).
While our attempts in the worst-case analysis were tunnelled towards the desire
to have small oﬀ-diagonal entries in this portion of the Gram matrix, leading our
worst-case analysis to the deﬁnition of the mutual-coherence, handling such sub-
matrices as a whole, and taking into account the entries’ signs may lead to better
bounds on the cardinalities that still guarantee positivity of AT
s As. This is the very
source from which Candes and Romberg draw their improved bound.
Interestingly, in a more recent work (2008), Tropp extends this treatment to gen-
eral (non two-ortho) matrices, showing that such leading minors from the Gram
matrix of size proportional to n are still guaranteed to behave well. This leads to the-
orems similar to the two mentioned above, but for general matrices. Tropp’s results
too obtain bounds that have the squared mutual-coherence in their denominator.
The fact that two theorems shown above (and Tropp’s results) are relying on
the mutual-coherence (even though it is squared) should be disturbing. While well-
chosen matrices lead to O(n/ log n) bounds, other choices (e.g., the Discrete Cosine
Transform (DCT) and wavelet as a two-ortho matrix, or redundant DCT as a gen-
eral one) give poor coherence and thus poor overall bounds. Indeed, the mutual-
coherence is a worst-case measure by its deﬁnition, and as such, should have been
avoided in such an analysis. Nevertheless, the mutual-coherence appears quite often

7.2 A Glimpse into Probabilistic Analysis
143
in probabilistic analysis of pursuit algorithms, probably because it helps in simpli-
fying complex proofs.
7.2.3 Probabilistic Uniqueness
A diﬀerent result, presented by Elad, addresses only the uniqueness property, gen-
eralizing the spark-based result in Chapter 2. This work, too, assumes a probabil-
ity density function on the sparse vectors x, constructed from an iid set of entries
with a ﬁxed cardinality. While the worst-case analysis suggests that only below
spark(A)/2 uniqueness is guaranteed, this work shows that uniqueness can be still
claimed with high probability for cardinalities beyond this bound.
The key property used for the construction of this result is the fact that if a ran-
dom representation x is obtained with s = ∥x∥0 < spark(A) non-zeros, it implies
that the corresponding vector b = Ax resides in a non-degenerate s-dimensional
space (since the columns from A used are necessarily linearly-independent). Any
competing sparser representation xt must lead to vectors Axt that reside in s −1-
dimensional space and below, and while there are many such constructions, their
number is ﬁnite. Thus, their overlap with the original s-dimensional space is nec-
essarily zero, and therefore the original representation is unique with probability
1.
Surprisingly, even for representations with spark(A) ≤s = ∥x∥0 < n, some sort
of uniqueness can be claimed, with high-probability. In order to build such a re-
sult, this work proposes to characterize the matrix A in a way that extends the spark,
forming the signature of the matrix. The signature of A is a function S igA(s), count-
ing the relative2 number of supports of cardinality s that are linearly-dependent.
Whereas the spark “thinks” worst-case, the signature gets the more general picture
by gathering all the subsets of columns from A that are linearly-dependent.
Using the (impossible to get!) signature of A, the probability for the existence
of a sparser solution is shown to be 1 −S igA(s). This is easy to derive, since by
considering all possible supports of cardinality s, S igA(s) of which are linearly-
dependent, and are thus reducible, leading to sparser representations. As for the rest,
they refer to linearly-independent groups of columns, for which alternative sparser
representations cannot be found, leaning on the same rational we have exercised
earlier.
7.2.4 Donoho’s Analysis
Another work that considers the average performance of the BP is reported by
Donoho (2006). Whereas the work by Candes and Romberg assumes a ﬁxed (and
2 normalized with respect to the number of such supports,
n
s

.

144
7 Towards Average PerformanceAnalysis
structured!) matrix A, the randomness being due to b (via the randomization of the
solution x), the analysis provided by Donoho considers A as a random matrix, with
columns chosen uniformly on the unit-sphere. Despite this major change in point
of view, the results obtained are similar in ﬂavor, suggesting that both uniqueness
and equivalence are guaranteed with high probability for O(n) non-zeros in x. Here
too the results hold asymptotically with growing dimension n and a diminishing
failure probability. Followup results by various authors extend the above results to
the approximate case, handling the relation between the solutions of (Pϵ
0) and (Pϵ
1),
thereby establishing a wider zone of guaranteed stability.
7.2.5 Summary
Other results that exploit the probabilistic view aimed at getting stronger bounds ex-
ist, see e.g. recent work by Candes and Tao, Tropp, DeVore and Cohen, Donoho and
Tanner, Shtok and Elad, and others. While all these attempts aim to show the same
eﬀect – an equivalence (or stability in case of added noise) of pursuit algorithms
for O(n) non-zeros in x – their treatment varies substantially, their characterization
of the matrix A may diﬀer, and so do the assumptions these works rely on. At this
point, the literature on this topic is growing so rapidly that it is diﬃcult to do any
justice at all to this ﬁeld, its achievements, and results. Adding to the confusion
in this arena is the fact that many of the newly introduced results are tailored for
compressed-sensing, where A constitutes a set of random projections. The results
obtained are somewhat relevant, with partial ties to the problem posed here.
7.3 Average Performance of Thresholding
7.3.1 Preliminaries
We have seen that among the various greedy techniques, the thresholding is the
simplest possible one, as it derives the solution in one projection step, by calculating
|ATb| and choosing its largest entries till the desired representation error is met.
In Chapters 4 and 5 we presented the worst-case analysis of this algorithm, and
now we turn to analyze its behavior under stochastic settings. The simplicity of
this algorithm makes such analysis relatively easy, compared to the more involved
study required in the other pursuit methods. The analysis we provide below follows
closely with the work of Schnass and Vandergheynst, and the worst-case analysis
that was presented in Chapter 4 for the noiseless case, which is the case considered
here.
We assume that b is constructed as b = Ax = P
t∈S xtat, where the non-zeros of
x are chosen as arbitrary positive values in the range [|xmin|, |xmax|] (where |xmin| is

7.3 Average Performance of Thresholding
145
strictly positive), multiplied by iid Rademacher random variables ϵt assuming values
±1 with equal probabilities. S is an arbitrary support of cardinality k.
In the analysis that follows, we shall use the following large-deviation inequality,
true for any T > 0, an arbitrary sequence of iid Rademacher random variables {ϵt}t,
and an arbitrary vector v:
P


X
t
ϵtvt
 ≥T
≤2 exp
−
T 2
32∥v∥2
2
.
(7.3)
This inequality is given in the book by Ledoux and Talagrand (Chapter 5, Section
4). As we shall see next, this inequality plays a vital role in the development of the
probabilistic bound for the thresholding algorithm.
7.3.2 The Analysis
As we have already shown in Chapter 4, the probability of failure of the thresholding
algorithm is given by
P (Failure) = P
 
min
i∈S |aT
i b| ≤max
j<S |aT
j b|
!
.
(7.4)
Choosing an arbitrary threshold value T, the above probability can be bounded from
above by
P (Failure) ≤P

min
i∈S |aT
i b| ≤T

+ P
 
max
j<S |aT
j b| ≥T
!
.
(7.5)
This step relies on the following rationale: Considering two non-negative random
values a and b we have that
P(a ≤b) =
Z ∞
t=0
P(a = t)P(b ≥t)dt
=
Z T
t=0
P(a = t)P(b ≥t)dt +
Z ∞
t=T
P(a = t)P(b ≥t)dt.
In the ﬁrst integral (that sums over the range [0, T]) we use the relation P(b ≥t) ≤1.
Similarly, in the second integral we use the fact that P(b ≥t) ≤P(b ≥T) since
t ≥T. This leads to
P(a ≤b) ≤
Z T
t=0
P(a = t)dt +
Z ∞
t=T
P(a = t)P(b ≥T)dt
= P(a ≤T) + P(b ≥T) ·
Z ∞
t=T
P(a = t)dt
= P(a ≤T) + P(b ≥T)P(a ≥T) ≤P(a ≤T) + P(b ≥T).

146
7 Towards Average PerformanceAnalysis
Fig. 7.4 A graphic explanation for the inequality P(a ≤b) ≤P(a ≤T) + P(b ≥T). Considering
the joint probability P(a, b), the left graph presents the area to integrate over this joint probability,
in order to obtain P(a ≤b). The right graph shows the two terms, P(a ≤T) and P(b ≥T), and
clearly, their sum covers the previous area and beyond.
This is exactly the step taken in Equation (7.5). Figure 7.4 provides an alternative
graphic explanation for the inequality P(a ≤b) ≤P(a ≤T) + P(b ≥T).
Considering the ﬁrst probability in the right-hand-side, it is increased if we de-
crease the term mini∈S |aT
i b|. Similarly, an increase of maxj<S |aT
j b| in the second
term increases its probability to be above T. Thus, we follow (some of) the same
steps as done for the worst-case analysis in simplifying these expressions. We start
with the ﬁrst term in Equation (7.5), plug the expression b = P
t∈S xtat, and exploit
the fact that ∥ai∥2 = 1, obtaining
min
i∈S |aT
i b| = min
i∈S

X
t∈S
xtaT
i at
 = min
i∈S

xi +
X
t∈S\i
xtaT
i at

.
(7.6)
Using the relation |a + b| ≥|a| −|b|, the above can be lower-bounded by
min
i∈S

xi +
X
t∈S\i
xtaT
i at

≥min
i∈S
|xi| −

X
t∈S\i
xtaT
i at


(7.7)
≥|xmin| −max
i∈S


X
t∈S\i
xtaT
i at


≥|xmin| −max
i∈S


X
t∈S\i
ϵt|xt|aT
i at

.
Thus, returning to Equation (7.5), we get

7.3 Average Performance of Thresholding
147
Fig. 7.5 A graphic explanation for the inequality P(max(a, b)) ≤P(a ≥T)+P(b ≥T). Considering
the joint probability P(a, b), the left graph presents the area to integrate over this joint probability,
in order to obtain P(max(a, b). The right graph shows the two terms, P(a ≥T) and P(b ≥T), and
clearly, their some covers the previous area.
P

min
i∈S |aT
i b| ≤T

(7.8)
≤P
|xmin| −max
i∈S


X
t∈S\i
ϵt|xt|aT
i at

≤T

= P
max
i∈S


X
t∈S\i
ϵt|xt|aT
i at

≥|xmin| −T
.
Using the rule P(max(a, b) ≥T) ≤P(a ≥T) + P(b ≥T) (demonstrated in Figure
7.5), we get
P

min
i∈S |aT
i b| ≤T

≤
X
i∈S
P


X
t∈S\i
ϵt|xt|aT
i at

≥|xmin| −T
.
(7.9)
Plugging now the large-deviation inequality presented in Equation (7.3) we ﬁ-
nally obtain
P

min
i∈S |aT
i b| ≤T

≤
X
i∈S
P


X
t∈S\i
ϵt|xt|aT
i at

≥|xmin| −T

(7.10)
≤2
X
i∈S
exp
(
−
(|xmin| −T)2
32 P
t∈S\i |xt|2(aT
i at)2
)
≤2|S| exp
(
−
(|xmin| −T)2
32|xmax|2µ2(|S| −1)
)

148
7 Towards Average PerformanceAnalysis
≤2|S| exp
(
−
(|xmin| −T)2
32|xmax|2µ2(|S|)
)
.
where we have deﬁned
µ2(k) = max
|S|=k max
i<S
X
t∈S
(aT
i at)2.
(7.11)
Note that we obviously have µ2(k) ≤kµ(A)2.
Turning to the second probability term in Equation (7.5) and following similar
steps, we get
P
 
max
j<S |aT
j b| ≥T
!
= P
max
j<S

X
t∈S
ϵt|xt|aT
j at
 ≥T

(7.12)
≤
X
j<S
P


X
t∈S
ϵt|xt|aT
j at
 ≥T

≤2
X
j<S
exp
−
T 2
32 P
t∈S |xt|2(aT
j at)2

≤2(m −|S|) exp
(
−
T 2
32|xmax|2µ2(|S|)
)
.
Returning to Equation (7.5), we have that the probability of a failure is bounded by
P (Failure) ≤2|S| exp
(
−
(|xmin| −T)2
32|xmax|2µ2(|S|)
)
(7.13)
+ 2(m −|S|) exp
(
−
T 2
32|xmax|2µ2(|S|)
)
.
The above bound is a function of T and it should be minimized with respect to T so
as to lead to the most tight upper-bound. The optimal value of T is hard to obtain in
closed form, and therefore we simply assign T = |xmin|/2 leading to
P (Failure) ≤2m exp
−
 xmin
xmax
!2
·
1
128µ2(|S|)
.
(7.14)
7.3.3 Discussion
Let us explore the implications of this result. Consider the case where A is of size
n × ρn (ρ > 1 is ﬁxed), and we use the thresholding algorithm to recover the spars-
est solution of the linear system Ax = b. We use the relation µ2(k) ≤kµ(A)2, as
mentioned above, and we further assume that µ(A) ∝1/ √n (as for Grassmannian

7.3 Average Performance of Thresholding
149
Frames or some unitary amalgams). We shall denote r = |xmin|
|xmax|, so as to simplify the
expressions obtained. Thus, the probability of failure of the thresholding algorithm
in recovering a solution of cardinality k becomes under these assumptions
P (Failure) ≤2ρn · exp
(
−r2n
128k
)
.
(7.15)
Thus, with cardinalities of the form k = cn/ log n, this probability becomes
P (Failure) ≤2ρn · exp
(
−r2
128c log n
)
= 2ρ · n1−r2
128c .
(7.16)
When choosing c < r2/128, this probability becomes arbitrarily small as n →∞.
This should be contrasted with the worst-case analysis that guarantees recovery of
k ∝√n cardinalities.
For the more general case, where the relation µ(A) ∝1/ √n is not true, sparse
solutions are recoverable if their cardinality is k = c/(µ(A)2 log n), following the
same rationale, and using the same condition on c.
We could improve the above analysis of the thresholding algorithm in several
ways, and this is very likely to lead to more optimistic constants in the above
claims. One such improvement is a replacement of the term µ2(k) by a smaller value.
The deﬁnition given in Equation (7.11) seeks the worst sum of squares of k inner-
products aT
i at, and as such, it necessarily leads to a too pessimistic value. Suppose
we have an evaluation of the probability density function of such sums over all
m
k

supports. Rather than choosing the maximal value, we can discard of the tail of this
distribution (thereby assuming that on those cases of volume δ we fail), and use a
smaller value in the probability bound. This probability should then be multiplied
by 1 −δ to reﬂect the added failures.
The obtained result is one in line of a sequence of similar results about the suc-
cessful recovery of sparse solutions with cardinalities that are near-linear with re-
spect to the dimension n. This is the simplest to obtain, due to the simplicity of
the underlying algorithm. Nevertheless, it reﬂects well the ﬂavor common to other
studies. We summarize this analysis with a Theorem that states the essence of our
ﬁndings:
Theorem 7.3. (Probable Equivalence – Thresholding Algorithm): For a system
of linear equations Ax = b (A ∈IRn×ρn full-rank with ρ > 1), if a solution x (with
minimal non-zero value |xmin| and maximal one |xmax|) exists obeying
∥x∥0 <
1
128
 xmin
xmax
!2
·
1
µ(A)2 log n,
(7.17)
then the Thresholding algorithm run with threshold parameter ϵ0 = 0 is likely to
ﬁnd it exactly, with probability of failure vanishing to zero for n →∞.

150
7 Towards Average PerformanceAnalysis
7.4 Summary
Swinging between the initial excitement about results of the form shown in Chapter
3, and the later disappointment from their tendency to be overly pessimistic, re-
searchers are still interested in the core question of theoretical bounds that predict
the success of pursuit algorithms, in a way that matches empirical evidence. In this
chapter we have shown some of these attempts, and gave a detailed description of
one of those results that corresponds to the thresholding algorithm.
Has the main question been answered already? Unfortunately the answer to this
question is negative. Despite the above results, we still do not posses a good and
constructive answer for a given matrix A of ﬁxed size, which will predict reliably
the performance as seen in experiments. In that respect, it would be a mistake to keep
using measures such as the mutual-coherence or the RIP in forming such bounds,
since this immediately implies a worst-case ﬂavor.
Further Reading
1. E.J. Cand`es and J. Romberg, Practical signal recovery from random projections,
in Wavelet XI, Proc. SPIE Conf. 5914, 2005.
2. E.J. Cand`es, J. Romberg, and T. Tao, Robust uncertainty principles: exact signal
reconstruction from highly incomplete frequency information, IEEE Trans. on
Information Theory, 52(2):489–509, 2006.
3. E. Cand`es, J. Romberg, and T. Tao, Quantitative robust uncertainty principles
and optimally sparse decompositions, to appear in Foundations of Computa-
tional Mathematics.
4. E. Cand`es, J. Romberg, and T. Tao, Stable signal recovery from incomplete and
inaccurate measurements, to appear in Communications on Pure and Applied
Mathematics.
5. E.J. Cand`es and T. Tao, Decoding by linear programming, IEEE Trans. on In-
formation Theory, 51(12):4203–4215, December 2005.
6. D.L. Donoho, For most large underdetermined systems of linear equations,
the minimal ℓ1-norm solution is also the sparsest solution, Communications on
Pure and Applied Mathematics, 59(6):797–829, June 2006.
7. D.L. Donoho, For most large underdetermined systems of linear equations, the
minimal ℓ1-norm near-solution approximates the sparsest near-solution, Com-
munications on Pure and Applied Mathematics, 59(7):907–934, July 2006.
8. D.L. Donoho and J. Tanner, Neighborliness of randomly-projected simplices in
high dimensions, Proceedings of the National Academy of Sciences, 102(27):9452–
9457, March 2005.
9. D.L. Donoho and J. Tanner, Counting faces of randomly-projected polytopes
when the projection radically lowers dimension, Journal of the AMS , 22(1):1–
53, 2009.

Further Reading
151
10. M. Elad, Sparse representations are most likely to be the sparsest possible,
EURASIP Journal on Applied Signal Processing, Paper No. 96247, 2006.
11. B. Kashin, The widths of certain ﬁnite-dimensional sets and classes of smooth
functions, Izv. Akad. Nauk SSSR Ser. Mat., 41, pp. 334–351, 1977.
12. S. Mendelson, A. Pajor, , and N. Tomczak-Jaegermann, Uniform uncertainty
principle for Bernoulli and subgaussian ensembles, to appear in Constructive
Approximation, 28(3):277–289, December 2008.
13. S. Mendelson, A. Pajor, and N. Tomczak-Jaegermann, Reconstruction and sub-
gaussian processes, Comptes Rendus Mathematique, 340(12):885–888, 2005.
14. K. Schnass and P. Vandergheynst, Average performance analysis for threshold-
ing, IEEE Signal Processing Letters, 14(11):828–831, November 2007.
15. S. Szarek, Condition number of random matrices, J. Complexity, 7, pp. 131–
149, 1991.
16. S. Szarek, Spaces with large distance to ℓ∞and random matrices, Amer. J.
Math., 112:899–942, 1990.

Chapter 8
The Dantzig-Selector Algorithm
In this chapter we present an appealing and surprising alternative pursuit algorithm
for sparse approximation. This algorithm is competitive with the Basis Pursuit De-
noising (BPDN) and the OMP. This algorithm was proposed in 2007 by Candes and
Tao, and termed Dantzig-Selector (DS). The name chosen pays tribute to George
Dantzig, the father of the simplex algorithm that solves Linear Programming (LP)
problems. The connection to LP will become evident shortly.
As we show in this chapter, the elegance in the work of Candes and Tao does
not stop in the proposal for an eﬀective pursuit algorithm. Their work is mainly fo-
cused on providing theoretical guarantees for this method’s aﬀectivity in accurately
recovering sparse solutions. In this chapter we bring one such result taken from their
paper.
8.1 Dantzig-Selector versus Basis-Pursuit
In Chapter 5 we discussed the following problem: a sparse vector x0 is multiplied
by a matrix A of size n × m, where in particular n could be smaller than m. We
shall assume hereafter that the columns of A are ℓ2-normalized. The outcome is
contaminated by a white Gaussian additive noise1 e, with iid entries ei ∼N(0, σ2),
and we obtain b = Ax0 + e. Our goal is to estimate reliably x0. Using the ideas
presented in Chapter 5, it would be natural to propose the estimator of the form
(Pϵ
0)
ˆxϵ
0 = arg min
x
∥x∥0 subject to ∥b −Ax∥2 ≤ϵ,
(8.1)
with ϵ chosen proportional to √nσ. We referred to this as (Pϵ
0), and as this problem
is too complex to solve in general, an option we discussed is a replacement of the ℓ0
1 In Chapter 5 we assumed that the “noise” is a deterministic vector of bounded energy ∥e∥2 = ϵ.
Such an assumption makes this distortion an adversary noise that may assume any form so as to
work against the reconstruction goals. By replacing this with a stochastic assumption, the bounds
developed become stronger, but at the price of a somewhat more involved analysis.
© Springer Science+Business Media, LLC 2010
M. Elad, Sparse and Redundant Representations: From Theory to Applications in Signal 
153
and Image Processing, DOI 10.1007/978-1-4419-7011-4_8,

154
8 The Dantzig-Selector Algorithm
by an ℓ1-norm, leading to the Basis Pursuit Denoising algorithm,
(Pϵ
1)
ˆxϵ
1 = arg min
x
∥x∥1 subject to ∥b −Ax∥2 ≤ϵ.
(8.2)
We now turn to describe the Dantzig-Selector (DS), which is a very interesting al-
ternative to (Pϵ
1). Candes and Tao propose to estimate x0 by solving the problem
(Pλ
DS )
ˆxλ
DS = arg min
x
∥x∥1 subject to ∥AT(b −Ax)∥∞≤λσ.
(8.3)
As can be seen, the constraint has been changed! The original requirement ∥b −
Ax∥2 ≤ϵ makes sense, as the residual is expected to be of a limited norm. Further-
more, this expression also emerges as the log-likelihood of the vector b given x, and
as such, it ties the formulation of (Pϵ
1) to the MAP estimation – a relation that will
be made explicit in Chapter 11. Nevertheless, one cannot overlook the plain fact
that this constraint does not force the residual b −Ax to behave like random white
Gaussian noise. In particular, structured and correlated residuals are permitted by
this constraint, despite our knowledge that they are highly improbable.
The alternative proposed by DS computes the residual r = b −Ax, and then
evaluates its inner products with the columns of A. These inner products form vari-
ous weighted combinations of the entries in r and all those are expected to be small.
The constraint thus proceeds by requiring that all these are below some pre-speciﬁed
threshold, λσ.
In order to get an intuition as to why to multiply by AT, consider the following
scenario: Suppose that the proposed solution x is close to x0, but one of non-zeros
in x0 is omitted, and this entry refers to the column ai from A. This omission is
reﬂected in a residual of the form r = b −Ax0 + cai = e + cai. This vector correlates
strongly with ai, since aT
i r = aT
i e+c. Whereas the ﬁrst part is likely to be near-zero,
the addition of c could be large (especially if this atom is dominant in the creation of
b), and this would serve as a detector for the infeasibility of the proposed solution.
This way, by sweeping though all columns of A, considering an inner product with
each of them, and requiring all to be relatively small, we make sure that the residual
is “whitened,” and no traces of the atoms remain in it.
The problem posed in Equation (8.3) has a linear programming form. This is eas-
ily observed by adding an auxiliary variable u ≥|x|, and reformulating the problem
to be
min
u,x
1Tu subject to

−u ≤x ≤u
and
−σλ1 ≤AT(b −Ax) ≤σλ1
.
(8.4)
All the terms (penalty and constraints) are linear with respect to the unknowns, and
thus this is a classic LP, for which speciﬁc solvers can be tailored (e.g., an iterative-
shrinkage algorithm). As a convex problem, obtaining the global minimizer of this
problem is within reach, and this estimator is therefore quite appealing.
Another way to convert DS into a linear programming form is to split the un-
known x into two vectors that contain its positive and negative entries, x = u −v.

8.2 The Unitary Case
155
Then the problem can be re-written as
min
u,v
1Tu + 1Tv
(8.5)
subject to

u ≥0 v ≥0
and
−σλ1 ≤AT(b −Au + Av) ≤σλ1
.
This approach should remind the reader of the technique used to convert the ℓ1 min-
imization into an LP form, in Chapter 1. Similar to the discussion there, we should
convince ourselves that the supports of the two vectors u and v do not overlap, or
put diﬀerently, that uTv = 0. This is easily veriﬁed using the same rationale – if two
such matching elements become non-zeros in the two vectors, then by decreasing
both by the same amount such that the smallest of them becomes zero, the penalty
of the problem gets smaller, while the constraint remains unchanged.
Is DS better than the Basis-Pursuit-Denoising for recovering x0? Based on the
description given above, the answer one would give is positive – after all, it seems
that the constraint DS uses is far more informative than the one posed by the BP.
As we shall see next, this intuition may be quite misleading, and at least when A is
unitary, there is no diﬀerence between the two.
8.2 The Unitary Case
When the matrix A is unitary, we have seen that both (Pϵ
0) and (Pϵ
1) admit closed-
form solutions, in the form of hard or soft shrinkage. Can the same be said for the
DS algorithm?
Looking at the constraint ∥AT(b −Ax)∥∞≤λσ, and using the fact that ATA = I,
this now becomes the requirement ∥ATb −x∥∞≤λσ. Thus, DS becomes a set of m
independent scalar optimization problems of the form
min
x
|x| subject to |aT
i b −x| ≤λσ,
i = 1, 2, . . . , m.
(8.6)
The solution is easily obtained as
xopt
i
=

aT
i b −λσ aT
i b ≥λσ
0
|aT
i b| < λσ
aT
i b + λσ aT
i b ≤−λσ
.
(8.7)
This is a classic soft-thresholding operation, exactly equivalent to the solution ob-
tained for the problem (Pϵ
1) (for a proper choice of parameters λσ versus ϵ). This
equivalence between (Pϵ
1) and (Pλ
DS ) is not true in general, though.
Returning to the general case, the above result also suggests that it might be pos-
sible to propose iterative-shrinkage algorithms for handling DS, just as we harnessed
such methods for the ℓ2-based penalty.

156
8 The Dantzig-Selector Algorithm
8.3 Revisiting the Restricted Isometry Machinery
In order to present a theoretical analysis of DS, we shall use the RIP as deﬁned in
Chapter 5. We bring this deﬁnition again for completeness of the presentation, and
then add a new and related deﬁnition that will be found helpful in the analysis of the
Dantzig-Selector.
Deﬁnition 8.1. For a matrix A of size n × m (m > n) with ℓ2-normalized columns,
and for an integer scalar s ≤n, consider submatrices As containing s columns from
A. Deﬁne δs as the smallest quantity such that
∀c ∈IRs
(1 −δs)∥c∥2
2 ≤∥Asc∥2
2 ≤(1 + δs)∥c∥2
2,
(8.8)
hold true for any choice of s columns. Then A has an s-restricted isometry property
with a constant δs.
A second deﬁnition, in line with the above, is for the Restricted Orthogonality
Property (ROP):
Deﬁnition 8.2. For a matrix A of size n × m (m > n) with ℓ2-normalized columns,
and for two integer scalars s1, s2 ≤n, consider two disjoint sets of s1 and s2 columns
from A, denoted as the submatrices As1 and As2. Deﬁne θs1,s2 as the smallest quantity
such that ∀c1 ∈IRs1 and c2 ∈IRs2
|cT
1 AT
s1As2c2| = |⟨As1c1, As2c2⟩| ≤θs1,s2∥c1∥2∥c2∥2,
(8.9)
holds true for any choices of s1, s2 columns. Then A has an s1, s2-restricted orthog-
onality property with a constant θs1,s2.
This deﬁnition aims to characterize an independence between disjoint sets of
columns, such that they do not interfere with each other in an attempt to represent the
same vector. As for the RIP, evaluation of the value of θs1,s2 is impractical, and here
as well one could bound θs1,s2 using the mutual-coherence by θs1,s2 ≤√s1s2µ(A),
but we will not dwell on this here, as these bounds are too crude for practical use.
We note two important observations regarding the above deﬁnitions:
1. While for a general matrix A it is virtually impossible to evaluate δs and θs1,s2,
one could evaluate these quite reliably for random matrices. This is why such
measures are popular for compressed-sensing applications, where A can be made
stochastic.
2. For a general matrix A, it is most likely that the RIP and the ROP measures
lead to pessimistic theoretical results. This is because those measures are, by
their deﬁnitions, worst-case measures, and as such, they are unforgiving to trivial
situations, such as near-by columns.

8.4 Dantzig-Selector Performance Guaranty
157
8.4 Dantzig-Selector Performance Guaranty
Armed with the above deﬁnitions, we are now able to analyze the performance of
DS. We bring here only the ﬁrst among several of the results that Candes and Tao
provide in their important paper, as this result by itself is suﬃcient to see the strength
of DS and how the above deﬁnitions can be used.
Theorem 8.1. (Stability of DS): Consider the instance of problem (Pλ
DS ) deﬁned
by the triplet (A, b, σ) and λ =
p
2(1 + a) log m. Suppose that a vector x0 ∈IRm is
s-sparse (∥x0∥0 = s), where s satisﬁes δ2s + θs,2s < 1, and b = Ax0 + e, where e is a
zero-mean random Gaussian noise with iid entries having variance σ2.
Then, the solution xλ
DS of (Pλ
DS ) obeys
∥xλ
DS −x0∥2
2 ≤C2 · 2 log m · s · σ2,
(8.10)
with probability exceeding 1 −(
p
π log mma)−1, and C = 4/(1 −δ2s + θs,2s).
Before we turn to prove this result, we brieﬂy discuss its meaning and implica-
tions. We draw attention to the following observations:
• There is a strong resemblance between this result and the one posed in Theorem
5.3 in Chapter 5, for the BPDN stability. However, the bound here is far better,
as it is proportional to sσ2 up to a constant (and log) factors, whereas there the
bound is proportional to nσ2, absorbing all the noise power. As was explained
earlier, the reason for this diﬀerence is the change from an adversary noise to a
stochastic one.
• The way this theorem is posed here may mislead the reader to assume that this is
an average performance analysis of DS, as the result is conditioned with a proba-
bility of correctness. However, this probability has to do with the randomness of
the noise and nothing to do with a probabilistic ensemble of supports or random
generators of x0.
• The sparsity condition posed is obscure, being indirectly dictated by the RIP and
ROP parameters. Thus, this result becomes both clearer and stronger as we turn
to handle random matrices A. Performance prediction of this theorem for general
matrices A may be too pessimistic, as it is also of worst-case nature.
• This result is surprising! In order to validate the strength of the bound obtained,
consider an estimation of x0 by an oracle that knows the correct support s. In
such a case, the estimation boils down to a Least-Squares problem of the form
zopt = arg min
z
∥Asz −b∥2
2 = (AT
s As)−1AT
s b.
(8.11)
The error in such a case is only reﬂected on the support s, as the two vectors are
the same (zeros) elsewhere. Using the fact that b = AS xs
0 + e, the expectation of
is error becomes
E

∥zopt −xs
0∥2
2

= E

∥(AT
s As)−1AT
s b −xs
0∥2
2

(8.12)

158
8 The Dantzig-Selector Algorithm
= E

∥(AT
s As)−1AT
s (Asxs
0 + e) −xs
0∥2
2

= E

∥(AT
s As)−1AT
s e∥2
2

= trace
n
(AT
s As)−1AT
s E(eeT)As(AT
s As)−1o
= σ2 · trace
n
(AT
s As)−1o
≥s · σ2
1 + δs
.
We have used the simple relation E(eeT) = σ2I, and also the fact that the eigen-
values of AT
s As are known to be in the range [1 −δs, 1 + δs] (due to the RIP),
and thus those of the inverse are in the range [1/(1 + δs), 1/(1 −δs)]. As the trace
sums the eigenvalues, the lower bound above is established.
All this means that DS gets a result very close to the best possible one, as the
oracle delivers, and this is done without having any clue about the support. This
is the surprise mentioned.
We now turn to provide the proof for this result. We follow closely the steps taken
by Candes and Tao in their work, with small modiﬁcations for better clarity of the
proof.
Proof: We have two candidate solutions to consider, x0 and xλ
DS , and we are inter-
ested in the distance between them, d = xλ
DS −x0. While xλ
DS is clearly feasible, x0
is not necessarily so. Thus, we shall look only at those fraction of cases where x0 is
feasible, implying that
∥AT(Ax0 −b)∥∞= ∥ATe∥∞≤λσ.
(8.13)
Every item in the vector ATe is a Gaussian random variable ∼N(0, σ) (since the
columns of A are normalized), and thus
P
 1
σ∥ATe∥∞≤λ
!
=
 
1 −2
Z ∞
λ
1
√
2π
e−x2
2 dx
!m
(8.14)
≥1 −2m
√
2π
Z ∞
λ
e−x2
2 dx,
where we have used the inequality (1 −ϵ)m ≥1 −mϵ, true for 0 ≤ϵ ≤1. Notice
that the vector ATe/σ is normalized and thus the canonic expressions. Using the
inequality (veriﬁed easily by taking a derivative of this function)
Z ∞
u
e−x2
2 dx ≤e−u2
2
u ,
we obtain
P
 1
σ∥ATe∥∞≤λ
!
≥1 −2m
√
2π
Z ∞
λ
e−x2
2 dx
(8.15)
≥1 −
2m
√
2πλ
e−λ2
2 .

8.4 Dantzig-Selector Performance Guaranty
159
Plugging to this expression λ =
p
2(1 + a) log m, we get
P
 1
σ∥ATe∥∞≤λ
!
≥1 −
2m
√
2πλ
e−λ2
2
(8.16)
= 1 −
1
p
π(1 + a) log m
· m−a.
For suﬃciently large a, this probability can be made arbitrarily small, and thus we
consider the majority of the cases in which x0 is feasible.
We deﬁne the residual created by the optimal solution of DS as r = b −Axλ
DS .
We have that
ATAd = ATA(x0 −xλ
DS )
(8.17)
= AT(Ax0 −b) −AT(Axλ
DS −b)
= AT(r −e).
Since the two solutions are feasible, we have that ∥ATe∥∞≤λσ and ∥ATr∥∞≤λσ.
Thus, using the triangle inequality,
∥ATAd∥∞= ∥AT(e −r)∥∞≤∥ATe∥∞+ ∥ATr∥∞≤2λσ.
(8.18)
Another direction we take exploits the fact that since xλ
DS is the solution of DS,
it has the shortest ℓ1-length, compared to any other feasible solution, and in partic-
ular, x0. Thus, ∥xλ
DS ∥1 = ∥x0 + d∥1 ≤∥x0∥1. We denote ∥v∥1,s as the ℓ1-norm of v
restricted to the support s of the sparse vector x0, and similarly, ∥v∥1,sc is restricted
to summation of the oﬀ-support entries of v. We use the inequality
∥x0 + d∥1 = ∥x0 + d∥1,s + ∥x0 + d∥1,sc
(8.19)
≥∥x0∥1,s −∥d∥1,s + ∥d∥1,sc,
where we have used the relations ∥x0 + d∥1,sc = ∥d∥1,sc and ∥x0 + d∥1,s ≥∥x0∥1,s −
∥d∥1,s. Since ∥x0∥1 = ∥x0∥1,s, we return to the inequality ∥x0 +d∥1 ≤∥x0∥1 and obtain
the requirement
∥d∥1,s ≥∥d∥1,sc.
(8.20)
This condition says that the diﬀerence vector d must have its ℓ1-energy concentrated
on the support s.
To summarize thus far, the diﬀerence vector d must satisfy two restrictions posed
in Equations (8.18) and (8.20). Among all these, we aim to search for the highest
∥d∥2
2 and show that it is in fact bounded from above, as the theorem suggests.
In order to proceed, we introduce the following notation: whereas we use s to de-
note the support of x0, q stands now for a support of size 2s, containing the original
support, and the next largest (in absolute value) s entries from d. The sub-matrix Aq
is of size n × 2s, containing the columns indicated by q. Using the RIP relation we

160
8 The Dantzig-Selector Algorithm
have that for any vector v of length 2s,
p
1 −δ2s∥v∥2 ≤∥Aqv∥2 ≤
p
1 + δ2s∥v∥2,
(8.21)
and we assume that δ2s < 1, implying that Q = AT
q Aq is positive-deﬁnite. The above
inequality states that (1 −δ2s)vTv ≤vTQv. For the assignment w = Q0.5v, this can
be re-stated as (1 −δ2s)wTQ−1w ≤wTw. Assigning w = AT
q Ad we have
∥w∥2
2 = dTATAqAT
q Ad = ∥AT
q Ad∥2
2,
(8.22)
and
wTQ−1w = dTATAq(AT
q Aq)−1AT
q Ad
(8.23)
= dTATPqAd
= dTATP2
qAd
=
PqAd
2
2 .
The term Pq = Aq(AT
q Aq)−1AT
q is a projection onto the space spanned by the
columns of Aq, and as such it is nilpotent (P2
q = Pq). All this leads to the inequality
(1 −δ2s)
PqAd
2
2 ≤∥AT
q Ad∥2
2.
(8.24)
Using the inequality posed in (8.18), ∥ATAd∥∞≤2λσ, we know that every entry
in the vector ATAd is bounded by 2λσ. The vector AT
q Ad accumulates 2s of those
entries, and thus
PqAd
2
2 ≤
1
1 −δ2s
∥AT
q Ad∥2
2 ≤2s · (4λ2σ2)
1 −δ2s
.
(8.25)
We now turn to handle the left-hand-side of the above expression, bounding it from
below. We slice the diﬀerence vector d into a set of vectors {d j}j=0,1,2,..., all of the
same length m. Each d j contains s non-zeros, where d0 has the support s, d1 has a
support q \ s, and so on, in descending order of the magnitude of the non-zeros in d.
Thus, clearly we have that d = P
j dj, and we can therefore write
PqAd = PqAd0 + PqAd1 +
X
j>1
PqAd j
(8.26)
= Aqd +
X
j>1
PqAd j.
The ﬁrst two terms are simpliﬁed by removing the projection operation, as these two
vectors are in the space projected upon. Also, we use the fact that Ad0 +Ad1 = Aqd.
Using the triangle inequality in reversed mode (∥v∥2 = ∥v+u−u∥2 ≤∥v+u∥2+∥u∥2),
we get

8.4 Dantzig-Selector Performance Guaranty
161
PqAd
2 =

Aqd +
X
j>1
PqAdj
2
(8.27)
≥∥Aqd∥2 −

X
j>1
PqAdj
2
≥∥Aqd∥2 −
X
j>1
PqAdj
2 .
The RIP relation posed in Equation (8.21) gives that ∥Aqd∥2 ≥√1 −δ2s∥d∥2,q, and
thus
PqAd
2 ≥
p
1 −δ2s∥d∥2,q −
X
j>1
PqAd j
2 .
(8.28)
For j > 1, the term
PqAd j
2
2 can be written as an inner product between the vector
PqAdj that is in the span of the 2s columns of As, and Ad j, spanned by a disjoint set
of s columns from A (again using the property Pq = P2
q). Using the ROP we have
that such an inner product is bounded by
PqAd j
2
2 = ⟨PqAd j, Ad j⟩≤θ2s,s · ∥PqAdj∥2∥Adj∥2.
(8.29)
Exploiting the RIP again, we have that ∥Adj∥2 ≤√1 + δs∥d j∥2, since this multipli-
cation combines s columns from A. Thus
PqAdj
2 ≤θ2s,s ·
p
1 + δs∥dj∥2
(8.30)
≤
θ2s,s
√1 −δs
∥dj∥2
≤
θ2s,s
√1 −δ2s
∥dj∥2.
Here we have used the properties 1 + δs ≤(1 −δs)−1 and δ2s ≥δs.
We now observe that for j > 1, the s non-zero entries of dj are smaller than the
mean of the entries in |d j−1|, expressed as ∥d j−1∥1/s. Summing this upper bound on
the entries over the s non-zeros in dj leads to ∥d j∥2 ≤∥d j−1∥1/ √s. Thus,
X
j>1
PqAd j
2 ≤
θ2s,s
√1 −δ2s
X
j>1
∥d j∥2
(8.31)
≤
θ2s,s
√s · √1 −δ2s
X
j>0
∥d j∥1
=
θ2s,s
√s · √1 −δ2s
∥d∥1,sc.
Returning to the inequality posed in (8.28), plugging the above leads to

162
8 The Dantzig-Selector Algorithm
PqAd
2 ≥
p
1 −δ2s∥d∥2,q −
θ2s,s
√s · √1 −δ2s
∥d∥1,sc.
(8.32)
Combining this with (8.25) we get
∥d∥2,q ≤
√
2s · (2λσ)
1 −δ2s
+
θ2s,s
√s · (1 −δ2s)∥d∥1,sc.
(8.33)
Using the requirement posed in (8.20) and the ℓ1 −ℓ2 relationship for vectors of
length s, ∥v∥1 ≤√s∥v∥2 , we obtain ∥d∥1,sc ≤∥d∥1,s ≤√s∥d∥2,s, and thus
∥d∥2,q ≤
√
2s · (2λσ)
1 −δ2s
+
θ2s,s
1 −δ2s
∥d∥2,s.
(8.34)
Using the fact that ∥d∥2,s ≤∥d∥2,q we write
∥d∥2,q ≤
√
2s · (2λσ)
1 −δ2s
+
θ2s,s
1 −δ2s
∥d∥2,q,
(8.35)
and rearranging terms leads to
 
1 −
θ2s,s
1 −δ2s
!
∥d∥2,q ≤
√
2s · (2λσ)
1 −δ2s
(8.36)
⇒∥d∥2,q ≤
√
2s · (2λσ)
1 −δ2s −θ2s,s
,
conditioned on the requirement 1 −δ2s −θ2s,s > 0.
To conclude the proof, we return to the observation made earlier, stating that
∥dj∥2 ≤∥dj−1∥1/ √s, and thus
∥d∥2
2,qc =
X
j>1
∥dj∥2
2 ≤1
s
X
j>0
∥d j∥2
1 = 1
s ∥d∥2
1,sc.
(8.37)
Using this, the relation (8.20), and the fact that ∥d∥2
2,s ≤∥d∥2
2,q, we obtain
∥d∥2
2 = ∥d∥2
2,q + ∥d∥2
2,qc
(8.38)
≤∥d∥2
2,q + 1
s∥d∥2
1,sc
≤∥d∥2
2,q + 1
s∥d∥2
1,s
≤2∥d∥2
2,q,
which leads to the claimed bound in the theorem by plugging the expression from
(8.36).
□

8.5 Dantzig-Selector in Practice
163
8.5 Dantzig-Selector in Practice
So, how does DS perform in practice? How does it compare to the Basis-Pursuit? Of
course, we already know that the two are exactly equivalent if A is unitary. In this
section we present a limited experiment that suggests answers to these questions. We
generate a matrix A of size 50 × 80, by taking the unitary DCT matrix, choosing a
subset of 50 of its rows at random, and normalizing the columns. We then generate
a random vector x0 of length 80, such that ∥x0∥0 = 10, in arbitrary and random
locations, and with random normal non-zero values. Finally, we compute b = Ax0 +
e, such that e has zero-mean Gaussian iid entries with standard-deviation σ = 0.05.
This forms our problem suite, and we apply DS and BP in an attempt to recover x0.
The DS is formulated as
(Pλ
DS )
ˆxλ
DS = arg min
x
∥x∥1 subject to ∥AT(b −Ax)∥∞≤λσ.
We solve this problem using Matlab’s linear programming tool, using the second
of the two conversions discussed in the beginning of this chapter. As we do not
know which λ to use, we sweep through a set of values in the range [0.1, 100] (the
top value guarantees that the zero solution is also considered, since in this case
∥ATb∥∞≤λmaxσ). The BP is formulated as
(Pµ
BP)
ˆxµ
BP = arg min
x
∥x∥1 subject to ∥b −Ax∥2 ≤µσ,
and we use the LARS solver mentioned in Chapter 5 for solving this problem for
various thresholds µσ, such that we cover all possible output cardinalities, from the
empty to the one that contains 49 non-zeros.
As we obtain a series of solutions (as a function of the varying thresholds) for
each method, we organize these as a graph that presents the relative accuracy of the
solution, ∥ˆx −x0∥2
2/∥x0∥2
2, as a function of the number of non-zeroes in the solution,
∥ˆx∥0. Figure 8.1 shows such graphs, averaged over 200 experiments. The ﬁgure also
presents the oracle performance, obtained by a simple projection of the vector b on
the columns of A in the true support. We see that DS and BP are nearly equivalent,
and their best performance is obtained for a cardinality of ≈25, with an error that is
4 times larger than the oracle.
Recall that the two methods are biased towards smaller non-zero entries, due to
the ℓ1-penalty. Thus, adopting only the support of the solution and solving for the
non-zero values by Least-Squares, we may obtain improved results, and especially
so if the found support is near-correct. The projected errors are also plotted in Fig-
ure 8.1, and indeed, both methods gain from this modiﬁcation. This time, the best
performance for both methods is obtained for a cardinality of ≈10, which is the
true one. Again, both methods are nearly equivalent, and getting closer (factor 2.8
between the DS/BP and the oracle errors) to the oracle.
Figure 8.2 is very similar to the one shown in 8.1, but with errors measured as
∥Aˆx−Ax0∥2
2/∥Ax0∥2
2. This diﬀerence is important in some cases, where the objective
is not the recovery of the sparse representation x0 but rather the separation of the

164
8 The Dantzig-Selector Algorithm
0
10
20
30
40
50
10
−3
10
−2
10
−1
10
0
Cardinality: ||x||0
Accuracy: ||x−x0||2
2/||x0||2
2
 
 
BP
BP−Projected
DS
DS−Projected
Oracle
Fig. 8.1 Accuracy of the solution, measured by ∥ˆx −x0∥2
2/∥x0∥2
2, versus its cardinality for the
Dantzig-Selector and the Basis-Pursuit. The diﬀerent points in these graphs are created by varying
the threshold of the algorithm. The graph shows the ﬁnal results of the two algorithms and also
their Least-Squares projections onto the found support.
clean signal Ax0 from the noise e. As can be seen, the overall behavior is very
similar to the graph in Figure 8.1, leading to the same conclusions.
Could we rely on the above experiment and conclude that BP and DS are nearly
equivalent in general? The answer is certainly not. First, this experiment is one
among many that could have been done, and it may well be that a diﬀerent problem-
setting would lead to diﬀerent conclusions. The question of the relative performance
between DS and BP is still an open question, which would occupy both theoreticians
and practitioners in coming years. One interesting question that comes out from the
above experiment is whether the combination of the two (i.e., solving an optimiza-
tion problem with the two constraints) could lead to better results.
8.6 Summary
The Dantzig-Selector is a relatively new pursuit technique in the realm of sparse
representation modeling, although earlier signs of it appeared in the literature in
2001 (see the work by Starck, Donoho, and Candes). Many questions regarding this

Further Reading
165
0
10
20
30
40
50
10
−3
10
−2
10
−1
10
0
Cardinality: ||x||0
Accuracy: ||Ax−Ax0||2
2/||Ax0||2
2
 
 
BP
BP−Projected
DS
DS−Projected
Oracle
Fig. 8.2 Accuracy of the solution, measured by ∥Aˆx −Ax0∥2
2/∥Ax0∥2
2, versus its cardinality for
the Dantzig-Selector and the Basis-Pursuit. The various graphs are very similar to those shown in
Figure 8.1.
tool remain open at this stage – its relation to estimation theory, its comparative
performance versus other pursuit techniques, limits on its average performance, nu-
merical techniques to practically and eﬃciently solve it, methods to improve it by
merging its force with other existing techniques, and more. These topics and others
would occupy many interested scientists and engineers in the coming years.
One important property that has drawn our attention to the Dantzig-Selector is the
ability to claim a near-oracle performance for it. Similar results emerged recently,
suggesting similar claims for the Basis-Pursuit, the OMP, and even the thresholding
algorithm. Still, all these results are of a worst-case ﬂavor, and as such, there is much
room for future improvements.
Further Reading
1. Z. Ben-Haim, Y.C. Eldar, and M. Elad, Coherence-based performance guar-
antees for estimating a sparse vector under random noise, submitted to IEEE
Transactions on Signal Processing, 2009.

166
8 The Dantzig-Selector Algorithm
2. P.J. Bickel, Y. Ritov, and A. Tsybakov, Simultaneous analysis of Lasso and
Dantzig selector, to appear in Ann. Statist., 2008.
3. E.J. Cand`es and T. Tao, Decoding by linear programming, IEEE Trans. on In-
formation Theory, 51(12):4203–4215, December 2005.
4. E.J. Cand`es and T. Tao, The Dantzig selector: Statistical estimation when p is
much larger than n, Annals of Statistics, 35(6):2313–2351, June 2007.
5. G.M. James, P. Radchenko, and J. Lv, DASSO: connections between the Dantzig
selector and lasso, Journal of the Royal Statist. Soc. B, 71(1):127–142, 2009.
6. N. Meinshausen, G. Rocha, and B. Yu, Discussion: A tale of three cousins:
Lasso, L2Boosting and Dantzig, Ann. Statist., 35(6):2373–2384, 2007.
7. J.L. Starck, D.L. Donoho and E. Cand`es, Very high quality image restoration,
in SPIE conference on Signal and Image Processing: Wavelet Applications in
Signal and Image Processing IX, Vol 4478, 2001.

Part II
From Theory to Practice – Signal and
Image Processing Applications

Chapter 9
Sparsity-Seeking Methods in Signal Processing
All the previous chapters have shown us that the problem of ﬁnding a sparse solu-
tion to an underdetermined linear system of equation, or approximation of it, can
be given a meaningful deﬁnition and, contrary to expectation, can also be computa-
tionally tractable. We now turn to discuss the applicability of these ideas to signal
and image processing. As we argue in this chapter, modeling of informative signals
is possible using their sparse representation over a well-chosen dictionary. This will
give rise to linear systems and their sparse solution, as dealt with earlier.
In the following discussion, we shall replace the notation b in the linear system
Ax = b to be y, so as to reﬂect the nature of this vector as a signal of interest.
9.1 Priors and Transforms for Signals
Consider a family of signals – a set of vectors Y =
n
y j
o
j ∈IRn. To make our
discussion more concrete, we shall assume hereafter that each such signal is a [ √n-
by- √n] pixels image patch, representing natural and typical image content. The
same discussion can be directed towards other data sources, such as sound signals,
seismic data, medical signals, ﬁnancial time series, and more.
While image patches are very diverse vectors in IRn, we have no reason to assume
that they occupy the entire space. Said more accurately, assuming that the pixels in
y have values in the range [0, 1), these image patches do not populate or sample the
hyper-cube [0, 1)n ⊂IRn uniformly. For example, we know that spatially smooth
patches occur much more often in images, whereas highly non-smooth and disor-
ganized image content is highly unlikely to exist. This mode of reasoning naturally
leads to the Bayesian framework of imposing a Probability-Density-Function (PDF)
on the signals – a ‘prior’ distribution P(y).
Priors are extensively used in signal processing, serving in inverse problems,
compression, anomaly detection, and more. This is because they provide a system-
atic way of measuring the probability of a signal vector. For example, consider the
© Springer Science+Business Media, LLC 2010
M. Elad, Sparse and Redundant Representations: From Theory to Applications in Signal 
and Image Processing, DOI 10.1007/978-1-4419-7011-4_9,
169

170
9 Sparsity-Seeking Methods in Signal Processing
noise cleaning (denoising) problem: a given image y is known to be a noisy version
of a clean image y0, contaminated by an additive perturbation vector v, known to
have a ﬁnite energy ∥v∥2 ≤ϵ, i.e., y = y0 + v. The unknown image y0 must be in the
ball ∥y0 −y∥2 ≤ϵ. The optimization problem
max
ˆy
P(ˆy) subject to ∥ˆy −y∥2 ≤ϵ
(9.1)
leads to the most probable image ˆy in this sphere, which is an eﬀective estimate of
y0. This way the prior is exploited for solving the denoising problem. The above for-
mulation of the denoising problem is in fact that Maximum-A-posteriori-Probability
(MAP) estimator, and more detailed and accurate version of it will be introduced
later on in Chapter 11.
Much eﬀort has been allocated in the signal and image processing communities
for forming priors as closed-form expressions. One very common way to construct
P(y) is to guess its structure based on intuitive expectations from the data content.
For example, the Gibbs distribution P(y) = Const · exp{−λ∥Ly∥2
2} uses a Laplacian
matrix (deﬁned as the linear space-invariant operation that applies the Laplacian
ﬁlter to the image y) to give an evaluation of the probability of the image y. In such
a prior, smoothness, measured by the Laplacian operator, is used for judging the
probability of the signal.
This prior is well-known and extensively used in signal and image processing,
and is known to be related to both Tikhonov regularization on one hand, and to
Wiener ﬁltering on the other. Returning to Equation (9.1), this prior leads to an
optimization problem of the form
min
ˆy
∥Lˆy∥2
2
subject to ∥ˆy −y∥2 ≤ϵ,
(9.2)
which can be converted to
min
ˆy
∥Lˆy∥2
2 + µ∥ˆy −y∥2
2,
(9.3)
where we have replaced the constraint by an equivalent penalty. The solution is
easily obtained as
ˆy = µ
h
LTL + µI
i−1 y.
(9.4)
The value of µ should be chosen such that the constraint ∥ˆy −y∥2 ≤ϵ is met.
A similar problem, where y = Hy0 + v, with H representing a linear degradation
operator (e.g., blur), leads to the solution
ˆy = µ
h
LTL + µHTH
i−1 y.
(9.5)
This is the well-known Wiener ﬁlter, which for space-invariant and cyclic operators,
L and H, it can be solved directly in the frequency-domain.

9.1 Priors and Transforms for Signals
171
The above speciﬁc prior stressing smoothness is known to cause blurring of the
image when used in various image enhancement and restoration tasks. The rem-
edy for this problem was found to be the replacement of the ℓ2-norm by a more
robust measure, such as an ℓ1-norm, that allows heavy tails for the distribution of
the values of Ly. Thus, a prior of the form P(y) = Const · exp{−λ∥Ly∥1} is far
more versatile and thus became popular in recent years. Similar to this option is
the Total-Variation (TV) prior (Rudin, Osher, and Fatemi, 1993) that also promotes
smoothness, but diﬀerently, by replacing the Laplacian with gradient norms, thereby
using ﬁrst-derivatives rather than second ones. Note that with the intuition gained in
previous chapters we see that the adoption of the ℓ1-norm leads to an enforcement
of sparsity of the signal/image derivatives.
A diﬀerent property that can be used for constructing a prior is assuming a struc-
ture on the signal’s transform-coeﬃcients. One such example is the JPEG compres-
sion algorithm, which relies on the fact that 2D-DCT coeﬃcients of small image-
patches tend to behave in a predicted way (being concentrated around the origin).
A diﬀerent example refers to the wavelet transform of signals and images, where
the coeﬃcients are expected to be sparse, most of them tending to zero while few
remain active.
The above two examples are an excellent illustration for the notion of linear ver-
sus non-linear approximation. In linear approximation, we apply a transform and
choose a pre-speciﬁed set of M coeﬃcients to approximate it, just as done with the
2D-DCT coeﬃcients above,1 and PCA in general (see next). A non-linear approxi-
mation takes the dominant M coeﬃcients, regardless of their location. Using these
M-term approximations, the non-linear approach is expected to lead to a better rep-
resentation of the original signal. Indeed, this is the approach used on the wavelet
coeﬃcients.
Lets us look more closely at the wavelet transform, and how it leads to a prior.
For a signal y, the wavelet transform is given by Ty where the matrix T is a spe-
cially designed orthogonal matrix that contains in its rows spatial derivatives of
varying scale, thereby providing what is known as “multi-scale” analysis of the sig-
nal. Therefore, the prior in this case becomes P(y) = Const · exp{−λ∥Ty∥p
p} with
p ≤1 to promote sparsity. Here the resemblance to the Total-Variation and the
Laplacian priors becomes evident, as in all these cases derivatives of some sort and
a robust measure are combined in forming the prior distribution P(y).
Wavelet-based priors shed light on a rich family of signal priors that assign like-
lihood for an image based on the behavior of its transform coeﬃcients Ty. In the
signal and image processing literature, such priors were postulated in conjunction
with a variety of transforms, such as the Discrete-Fourier-Transform (DFT), the dis-
crete cosine transform (DCT), the Hadamard, and even data-dependent transforms
such as the Principal-Component-Analysis (PCA, also known in the signal process-
ing community as the Karhunen-Loeve-Transform - KLT).
1 Actually, JPEG operates slightly diﬀerent, by considering the leading DCT coeﬃcients to be
active, but allowing some outliers, and in that respect, this is not an exact linear approximation.

172
9 Sparsity-Seeking Methods in Signal Processing
Constructing a PCA-based prior starts by gathering a database of signal exam-
ples, Y =
n
y j
o
j ∈IRn, as described above. We compute the center of this cloud of
n-dimensional points, c = 1
N
P
j yj, as the mean of all the examples. Then we com-
pute the auto-correlation matrix R =
1
N
P
j(y j −c)(yj −c)T. The eﬀective prior is
given as the multivariate Gaussian distribution
P(y) = Const · exp
(1
2(y −c)TR−1(y −c)
)
.
(9.6)
This model assumes that the cloud of examples Y behaves like a high-dimensional
ellipsoid in IRn, where c is its center and R describes its shape. If we assume that
c = 0 (i.e., if we deal with signals shifted to the origin), then this model is very
close to the Tikhonov regularization mentioned earlier, by assigning LTL = R−1.
This model is the one that leads to the 2D-DCT in JPEG mentioned above.
One may adopt a common point of view towards all these signal priors, regarding
them as mere attempts to describe a random generator machine M that supposedly
generates the signals of interest. However, in all these cases, while the relative2
evaluation of the probability for a given signal is easy via the P(y) formulae, drawing
a random sample from such distributions is quite diﬃcult. This brings us to the
Sparse-Land model, which is a way for synthesizing signals according to a prior
deﬁned by a transform.
9.2 The Sparse-Land Model
Let us return to the linear system Ax = y and interpret it as a way of constructing
signals y. Every column in A is a possible signal in IRn – we refer to these m columns
as atomic signals, and the matrix A displays a dictionary of atoms. One can consider
A as the periodic table of the fundamental elements in the “chemistry” that describes
our signals.
The multiplication of A by a sparse vector x with ∥x∥0
0 = k0 ≪n produces a
linear combination of k0 atoms with varying portions, generating the signal y. The
vector x that generates y will be called its representation, since it describes which
atoms and what “portions” thereof were used for its construction. This process of
combining atoms linearly to form a signal (think of it as a molecule in the chemistry
of our signals) may be referred to as atomic-composition.
Consider all the possible sparse representation vectors with cardinality ∥x∥0
0 =
k0 ≪n, and assume that this set of
m
k0

possible cardinalities are drawn with uniform
probability. Assume further that the non-zero entries in x are drawn from the zero-
mean Gaussian distribution Const·exp{−αx2
i }. This gives us a complete deﬁnition of
the PDF of x, and thus also for the way signals y are generated – this constitutes the
2 Due to the need for the normalization coeﬃcient.

9.3 Geometric Interpretation of Sparse-Land
173
random signal generator M{A,k0,α}. Note that the signal y is the outcome of a mixture
of Gaussians, each of dimension k0 with equal probability.
A modiﬁcation that can be introduced into the above model is the replacement of
the ﬁxed cardinality with a random one. In such case, rather than assuming that every
signal is composed of a ﬁxed k0 atoms, one could assume that the cardinality itself is
random, with emphasis on sparse representations, such as a probability proportional
to exp{−k}. Such a change is not signiﬁcant to our discussion here.
An important further addition to the deﬁnition of the signal generation model
M could be postulating a random perturbation (noise) vector e ∈IRn with bounded
power ∥e∥2 ≤ϵ, such that y = Ax+e. Such an additive perturbation is useful because
of two important reasons:
• The model M imposes too strong restrictions on the signals, and using it in prac-
tical applications will necessarily lead to a mismatch between actual observed
signals and their imposed model. The introduction of the ϵ perturbation mends
this problem.
• The case ϵ = 0 implies that the volume of signals generated by M is of “zero
measure” within IRn. This is because we assume a ﬁnite number of possible sup-
ports,
m
k0

, each leading to a k0-dimensional subspace signals in the IRn space.
If ϵ increases, this set assumes a small but positive volume, via the “thickened”
cloud around those subspaces.
With the addition of such a perturbation, we refer hereafter to the Sparse-Land
model as M(A, k0, α, ϵ).
At this point the reader might be troubled by the arbitrariness of the Sparse-Land
model, and wonder why should such a description ﬁt signals of interest? In order
to partially answer this question, we should note that despite the very diﬀerent ap-
pearance of this model, it is in fact tightly related to methods we discussed earlier
in Section 9.1. Consider, for example, the case where A is square and invertible.
The relation Ax = y can be converted to be x = A−1y = Ty, where T = A−1, This
means that the sparse representation vector we use here plays a role of a transform-
coeﬃcients vector. Thus, a model of the sort P(y) = Const · exp{−λ∥Ty∥p
p}, as de-
scribed earlier, is in fact equivalent to a model P(y) = Const · exp{−λ∥x∥p
p}, which
describes the probability of the signal y in terms of the sparsity of its representa-
tion x, just as Sparse-land does. Does this imply that Sparse-Land is included in
the transform-based models mentioned earlier? The answer is negative, as we argue
towards the end of this chapter, when we discuss analysis versus synthesis models.
9.3 Geometric Interpretation of Sparse-Land
In order to gain a geometric insight into the Sparse-Land model and some alterna-
tives to it, we return to the beginning of this chapter, considering a large corpus of
signal-examples (vectors) Y =
n
y j
o
j ∈IRn. We concentrate on one arbitrary signal

174
9 Sparsity-Seeking Methods in Signal Processing
y0 ∈Y and its δ-neighborhood, and aim to study this neighborhood’s behavior in
the n-th dimensional space.
For small enough value of δ, moving from y0 along the directions e = y −y0
represent small permissible perturbation directions that lead to feasible signals. The
question we pose is: do those ﬁll the entire IRn space? We denote this set of permis-
sible directions as
Ωδ
y0 = {e | e = y −y0, where y ∈Y and ∥y −y0∥2 ≤δ}.
(9.7)
Gathering all vectors e ∈Ωδ
y0 into an n×|Ωδ
y0| matrix Ey0, we aim to study the behav-
ior of its singular-values. We will be interested in families of structured signals, for
which the eﬀective rank of such matrices is ky0 ≪n, for any y0 ∈Y. This is equiv-
alent to the statement that the ky0 + 1 singular-value and beyond in these matrices
tend to be near-zero and are therefore negligible.
Signals satisfying the above local low-dimensionality assumption essentially
give a local behavior that is approximately a linear subspace of dimension ky0,
shifted around y0. While this subspace and its dimension may vary from one point to
another in the signal space, all of them are characterized as being far smaller than the
ambient n dimensions. Experimental studies show that most signals we work with,
such as images, audio, and more, follow such a structure, and this can be exploited
in processing them.
As a simple example for how to exploit this structure, consider again the denois-
ing problem. A clean signal y0 is to be recovered from a noisy version of it y, where
∥y −y0∥2 ≤δ0 ≪δ. Suppose that from the complete set Y we extract all examples
that are in the δ-neighborhood of y. We compute the average vector of this set, c,
and subtract it,3 getting a good approximation to Ωδ
y0, since δ0 ≪δ. The accuracy of
this approximation is dictated by the assumption that the local permissible subspace
changes smoothly, enabling the assumption δ ≫δ0.
The knowledge of Ωδ
y0 implies the knowledge of the ky0-dimensional subspace
spanning its members (the columns of Ey0). Let us denote this subspace by the n×ky0
matrix Qy0, such that its columns form an orthogonal set of vectors. The permissable
signals that may explain the noisy signal y can be formulated as a linear combination
of the columns of Qy0 with a shift c, i.e., the possible solutions are of the form
Qy0z + c. Thus, an eﬀective denoising is obtained by computing the projection of y
onto this aﬃne subspace,
zopt = arg min
z
∥Qy0z + c −y∥2
2 = QT
y0(y −c),
(9.8)
leading to
ˆy = Qy0QT
y0(y −c) + c.
(9.9)
3 The set gathered forms as aﬃne subspace, and the average computed is the estimation of its
origin.

9.3 Geometric Interpretation of Sparse-Land
175
Fig. 9.1 A graphic demonstration of the denoising process that uses local neighbors in order to
compute the center c, and the projection directions Qy.
The essence of this process is the assumption that any content orthogonal to the
directions in Qy0 is known to be pure noise and thus should be discarded. Figure 9.1
presents this explanation graphically.
Based on the above description, it is tempting to suggest a modeling of a signal
source by holding many instances of example signals Y, and using them directly
in the denoising process. We can repeat the above process, of seeking the δ-close
neighbors, estimating the local subspace, and projecting onto it. This is known as a
local Principal-Component-Analysis (PCA) modeling. Its major drawbacks are the
need for a very large set of examples in order to guarantee eﬀective modeling, and
the high-complexity algorithm for cleaning any signal by sweeping through all this
database.
In the above local-PCA model, Qy0 are the principal directions of the signal for
the location y0, and those are permitted to vary freely as a function of y0. In general,
a complete model of this sort requires either a storage of Y and extraction of Qy0
per need, or a storage of all the possible unitary bases, gathered oﬀ-line for every
y ∈B. Both options are prohibitive in most cases.
If we can further assume that a ﬁnite and small number of such projection-
matrices and center-vectors, {Qp, cp}P
p=1 (say few hundreds or thousands), cover all
possible cases, this model can be made more eﬃcient. This calls for a (possibly
fuzzy) clustering of the signals y ∈Y to subgroups that correspond each to a dif-
ferent matrix Qp and a center cp. Once a signal is to be processed using this model,
we start by assigning it to one of these groups (by searching its nearest-neighbor in
Y), and then attaching the proper projection-matrix and center vector {Qp, cp} to it
for further processing. Such a model has been proposed by Yi Ma and others, under
the name Generalized PCA, and shown to be very eﬀective in grasping the images’
behavior.
The Sparse-Land model is one step further in an attempt to make such local
model more concise. Assuming for simplicity that all these local subspaces are of

176
9 Sparsity-Seeking Methods in Signal Processing
the same dimensionality, k, using sparse representations of cardinality k over a dic-
tionary A with m columns, we have at our disposal
m
k

subspaces, just by allowing k-
sparse representations with all the possible supports. This way, our dictionary holds
very eﬃciently all the key directions required to represent the signal anywhere in
IRn, while enabling a very large number of possibilities. Clearly, though, this comes
at the price of further restricting the structure of the model by assuming that the
atoms in the dictionary are principal directions shared by many sub-spaces.
For completeness of the above presentation, we should mention yet another ap-
proach that could beneﬁt from the availability of the example set Y. Given a noisy
version of y0, denoted by y, such that ∥y −y0∥2 ≤δ0, we can simply search for
the nearest-neighbor for y in Y, and consider this as the denoised result. This direct
example-based technique is very appealing, but it would work well only under the
very restricting assumption that Y is sampled very densely, so as to hold the clean
signal y0 or others very close to it.
This approach could be considered as an extreme sparse representation modeling
of the signal y, such that the δ-local neighborhood serves as its local (and varying)
dictionary, and we allow for use of one atom with a coeﬃcient being 1 to represent
it.
Alternatively, one could seek the K-nearest-neighbor signals to y in Y and aver-
age them. This tends towards the vector c used in the above aﬃne modeling as the
estimated center, and this result is inferior to the local-PCA method. Thus, the bene-
ﬁts that parameterized models, such as clustered-local-PCA and Sparse-Land, bring
are the ability to work with reasonably modest example sets, and a stable estimation
of the local characteristics.
9.4 Processing of Sparsely-Generated Signals
How do we practice signal processing for Sparse-Land signals? Suppose we have a
signal y which has been generated from the model M(A, k0, α, ϵ) and the parameters
of the model are known. There are numerous signal processing tasks that could be
of interest to us; let us discuss them and see how sparsity-seeking representations
might enter in.
• Analysis: Given y, can we determine the underlying vector x0 which generated
it? This process may be called atomic-decomposition, as it leads us to infer the
individual atoms that actually generated y. Clearly, the true underlying represen-
tation obeys ∥Ax0 −y∥2 ≤ϵ, but there will be many other vectors x generating
similarly good approximations to y. Suppose we can actually solve (Pϵ
0):
(Pϵ
0) :
min
x
∥x∥0 subject to ∥y −Ax∥2 ≤ϵ.
Under our assumptions, the solution xϵ
0 of this problem, though not necessarily
the underlying x0, will also be sparse and have k0 or fewer non-zeros. Results

9.4 Processing of Sparsely-Generated Signals
177
described in earlier chapters show that if k0 is small enough, then the solution of
(Pϵ
0) is at most O(ϵ) away from x0.
• Compression: Nominally, y requires a description by n numbers. However, if
we can solve (Pδ
0) where δ ≥ϵ, then the resulting solution xδ
0 aﬀords an approxi-
mation ˆy = Axδ
0 to y using at most 2k0 scalars, with an approximation error of at
most δ. By increasing δ we obtain a deeper compression with fewer non-zeros,
and a larger approximation error. This way, we obtain a rate-distortion curve for
a compression mechanism.
• Denoising: Suppose that we observe, not y, but instead a noisy version ˜y = y + v
where the noise is known to obey ∥v∥2 ≤δ. If we can solve (Pδ+ϵ
0 ), then the
resulting solution xδ+ϵ
0
will have at most k0 non-zeros; our earlier results show
that if k0 is small enough, then xϵ+δ
0
is at most O(ϵ + δ) away from x0. Once
found, we obtain an approximation of our cleaned signal by Axδ+ϵ
0 .
• Inverse Problems: Even more generally, suppose that we observe, not y, but
a noisy indirect measurement of it, ˜y = Hy + v. Here the linear operator H
could represent blurring, masking (which leads to in-painting), projection, down-
scaling, or some other kind of linear degradation, and v is noise as before. If we
could solve
min
x
∥x∥0 subject to ∥˜y −HAx∥2 ≤δ + ϵ,
then we would expect to identify directly the sparse components of the underly-
ing signal and obtain an approximation Axδ+ϵ
0 .
• Compressed-Sensing: For signals which are sparsely generated, one can obtain
good reconstructions from a reduced number of measurements – thereby com-
pressing the sensing process rather than the traditionally sensed data. Let P be a
random j0 × n matrix with Gaussian iid entries, and suppose that it is possible to
directly measure c = Py, which has j0 ≪n entries, rather than sensing y, which
requires n values. Attempt recovery by solving
min
x
∥x∥0 subject to ∥c −PAx∥2 ≤ϵ,
to obtain the sparse representation and then synthesizing an approximate recon-
struction using Axϵ
0. Results from compressed-sensing suggest that if j0 > 2k0,
this recovery is likely to succeed with high-probability.
• Morphological Component Analysis (MCA): Suppose that the observed signal
is a superposition of two diﬀerent sub-signals y1, y2 (i.e., y = y1 + y2) where
y1 is sparsely generated using model M1 and y2 is sparsely generated using
model M2. Can we separate the two sources? Such source separation problems
are fundamental in processing of acoustic signals, for example in the separation
of speech from impulsive noise by Independent Component Analysis (ICA) al-
gorithms. Turning to the signal model presented here, if we could solve
min
x1,x2 ∥x1∥0 + ∥x2∥0 subject to ∥y −A1x1 −A2x2∥2
2 ≤ϵ2
1 + ϵ2
2,

178
9 Sparsity-Seeking Methods in Signal Processing
the resulting solution (xϵ
1, xϵ
2) would generate a plausible solution ˆy1 = Axϵ
1,
ˆy2 = Axϵ
2 to the separation problem.
There have been several successful trials of this idea by Starck and others, ﬁrst
in acoustics and later in image processing. An appealing image processing ap-
plication that relies on MCA is inpainting, where missing pixels in an image are
ﬁlled-in, based on a sparse representation of the existing pixels. MCA is neces-
sary because the piecewise smooth (cartoon) and texture contents of the image
must be separated as part of this recovery process.
A wide range of other applications including encryption, watermarking, scrambling,
target detection, recognition, feature extraction, and more, can also be envisioned.
All these application ideas call for the solution of (Pδ
0), or variants. We have inten-
tionally described all these proposed applications in the conditional mood, since in
general (Pδ
0) is not known to be tractable or even well deﬁned. However, our earlier
discussion of (Pδ
0) shows that the problem, under the right conditions, is sensible
and can be approximately solved by practical algorithms.
9.5 Analysis Versus Synthesis Signal Modeling
Just before leaving this chapter, we return to the core of the Sparse-Land model def-
inition, and contrast it with an interesting alternative. Sparse-Land is built around
a generative model for the signal y, by constructing it as a linear combination of
atoms, Ax = y. An alternative, more in line with the priors mentioned in the be-
ginning of this chapter, is an analysis modeling that assigns a probability P(y) to y
by analyzing it through an analysis operator T, eﬀectively computing the analysis-
representation Ty.
Consider, as an example, denoising with Sparse-Land. Solving (Pϵ+δ
0 ) as pro-
posed above,
ˆy = A · arg min
x
∥x∥0 subject to ∥y −Ax∥2 ≤ϵ + δ,
leads to denoising. Assume now that A is square and invertible and deﬁne z = Ax.
The above denoising process can be re-written equivalently as
ˆy = arg min
z
∥A−1z∥0 subject to ∥y −z∥2 ≤ϵ + δ.
Thus, the matrix A−1 becomes an analysis operator T, and the formulation obtained
here coincides with the one that appeared in Equation (9.1).
A classic example for such a pair of analysis and synthesis formulations that are
equivalent is the 1D Total-Variation analysis operator and the Heaviside synthesis
dictionary. The 1D Total-Variation performs a simple derivative by convolving the
signal with the ﬁlter [+1, −1]. Let us assume that at the right border of the signal, we
proceed with zero padding (implying that the rightmost sample is multiplied by 1).
This results with an n × n Toeplitz matrix with the main-diagonal containing (+1)-

9.5 Analysis Versus Synthesis Signal Modeling
179
 
 
20
40
60
80
100
20
40
60
80
100
−1
−0.5
0
0.5
1
 
 
20
40
60
80
100
20
40
60
80
100
0
0.2
0.4
0.6
0.8
1
Fig. 9.2 The matrix T for the 1D Total-Variation (left) and the corresponding synthesis operator
A, the Heaviside basis (right).
es, and the ﬁrst diagonal to the right containing (−1)-es. We compute its inverse,
A = T−1, and the resulting matrix gives the Heaviside basis. Its columns are simple
step functions in all possible n locations. Figure 9.2 presents these two matrices,
both of size 100 × 100.
The equivalence between these two operators is natural since in analysis terms,
we seek a piecewise smooth signal that shows little energy in the sum of the ab-
solute derivatives. In a generative way, this implies that such a signal is built as a
linear combination of few step functions. This way, we see that our proposed model
can cover known Markov-Random-Field (MRF) models that are extensively used in
signal and image processing.
The above may suggest that analysis and synthesis based modelings may become
equivalent in the general case, where A is a full-rank matrix of size n×m with n < m,
with the following two presumably equivalent formulations for denoising:
(PS ynthesis) ˆys = A · arg min
x
∥x∥0 subject to ∥y −Ax∥2 ≤ϵ + δ
(PAnalysis) ˆya = arg min
z
∥Tz∥0 subject to ∥y −z∥2 ≤ϵ + δ,
for A = T+ = (TTT)−1TT. In fact, starting from the analysis formulation, we deﬁne
Tz = x and thus we can replace the term ∥Tz∥0 by the simpler, ∥x∥0. Multiplying this
equation by TT, we obtain the relation TTTz = TTx. Based on the full-rank assump-
tion made above regarding A (and thus T), this implies that z = (TTT)−1TTx = Ax,
and therefore the ℓ2-term in the constraint can be replaced to from ∥y −z∥2 to
∥y −Ax∥2.
While this seems like a perfect transfer from the analysis to the synthesis formu-
lation, it is in fact missing a key element. From the two equations we got, Tz = x and
z = (TTT)−1TTx, immediately follows the requirement on x, T(TTT)−1TTx = x.
This requirement simply states that x must reside in the range of T. Adding this as a
constraint to the synthesis formulation, we get an exact equivalence, and otherwise,
the synthesis gets a larger number of degrees of freedom, and thus its minimum is
deeper (in fact, it is upper-bounded by the analysis formulation).

180
9 Sparsity-Seeking Methods in Signal Processing
This way we see that the two models depart in general. The discussion throughout
this chapter is steered towards the synthesis model, and this would be the focus of
the signal processing modeling we adopt here. However, we stress that there is no
clear answer to the question which of the two modeling approaches is better.
9.6 Summary
In this chapter we have proposed a generative model for describing signals. This
model, denoted by M(A, k0, α, ϵ) assumes that the signals of interest are created
as random sparse combinations of atoms from a given dictionary, with a permitted
small perturbation. We saw that this model is related to other ones that are exten-
sively used in signal and image processing, such as MRF, Total-Variation, PCA and
local-PCA, and more.
It must be clear that any mathematical model for signals, including the Sparse-
Land model presented here, must do some injustice to the signals modelled, as it
cannot describe their content exactly. Nevertheless, and despite this deviation from
the true signal behavior, such models can be used successfully in various signal pro-
cessing applications. The reason for this success could be explained in the following
way. Consider a true signal source of dimension n, and further assume that the prob-
ability density function (prior) for this source is given as the function P(y). When
we propose a model we essentially propose an alternative PDF, ˆP(y).
A perfect model would be one that provides a very close approximation to this
PDF, implying for example that ∥P(y) −ˆP(y)∥is very small. Less favorable models,
but ones that may still perform well, would be those that give a very small weighted
error ∥P(y)(P(y)−ˆP(y))∥. This means that the error is small in the active zone of the
signal distribution, while getting a somewhat larger error in the inactive zone. This
implies that such models are excellent in stating whether a signal is likely to occur,
but are weaker in detecting whether a signal is not.
Put diﬀerently, assume that the portion covered in IRn by true signals is µtrue ≪1
(the zone where P(y) is relatively high). The less favorable models mentioned above
may provide a coverage of µtrue ≪µmodel ≪1 portion in this space, such that it
covers all/most-of the true zone and an additional (wrong!!) part. Synthesizing a
signal from this model is very likely to result with a non-natural signal, since most
of the area endorsed by this model is outside the true zone of likely signals. Thus,
synthesis of signals from such a model leads to a failure. Nevertheless, cleaning an
additive noise with such a model may work very well. This is because the additive
noise puts us in a region where P(y) is very low, and most chances are that in this
area ˆP(y) is also very low (because µmodel ≪1). Denoising amounts to a search of ˆy
in the vicinity of y such that it gives the highest value of ˆP(y), and if this aligns with
maximizing the true probability P(y), we have a good denoising algorithm.
Further work is required to establish the above preliminary ideas and show that
they hold true for popular families of signals of interest.

Further Reading
181
Further Reading
1. J. Bobin, Y. Moudden, J.-L. Starck, and M. Elad, Morphological diversity and
source separation, IEEE Signal Processing Letters, 13(7):409–412, July 2006.
2. J. Bobin, J.-L. Starck, M.J. Fadili, and Y. Moudden, SZ and CMB reconstruc-
tion using generalized morphological component analysis, Astronomical Data
Analysis ADA’06, Marseille, France, September, 2006.
3. R.W. Buccigrossi and E.P. Simoncelli, Image compression via joint statisti-
cal characterization in the wavelet domain, IEEE Trans. on Image Processing,
8(12):1688–1701, 1999.
4. E.J. Cand`es and D.L. Donoho, Recovering edges in ill-posed inverse problems:
optimality of curvelet frames, Annals of Statistics, 30(3):784–842, 2000.
5. E.J. Cand`es and D.L. Donoho, New tight frames of curvelets and optimal repre-
sentations of objects with piecewise-C2 singularities, Comm. Pure Appl. Math.,
57:219-266, 2002.
6. E.J. Cand`es and J. Romberg, Practical signal recovery from random projections,
in Wavelet XI, Proc. SPIE Conf. 5914, 2005.
7. E.J. Cand`es, J. Romberg, and T. Tao, Robust uncertainty principles: exact signal
reconstruction from highly incomplete frequency information, IEEE Trans. on
Information Theory, 52(2):489–509, 2006.
8. E. Cand`es, J. Romberg, and T. Tao, Stable signal recovery from incomplete and
inaccurate measurements, to appear in Communications on Pure and Applied
Mathematics.
9. E.J. Cand`es and T. Tao, Decoding by linear programming, IEEE Trans. on In-
formation Theory, 51(12):4203–4215, December 2005.
10. V. Chandrasekaran, M. Wakin, D. Baron, R. Baraniuk, Surﬂets: a sparse rep-
resentation for multidimensional functions containing smooth discontinuities,
IEEE Symposium on Information Theory, Chicago, IL, 2004.
11. S.S. Chen, D.L. Donoho, and M.A. Saunders, Atomic decomposition by basis
pursuit, SIAM Journal on Scientiﬁc Computing, 20(1):33–61 (1998).
12. S.S. Chen, D.L. Donoho, and M.A. Saunders, Atomic decomposition by basis
pursuit, SIAM Review, 43(1):129–159, 2001.
13. R. Coifman and D.L. Donoho, Translation-invariant denoising. Wavelets and
Statistics, Lecture Notes in Statistics, 103:120–150, 1995.
14. R.R. Coifman, Y. Meyer, S. Quake, and M.V. Wickerhauser, Signal process-
ing and compression with wavelet packets. In Progress in wavelet analysis and
applications (Toulouse, 1992), pp. 77–93.
15. R.R. Coifman and M.V. Wickerhauser, Adapted waveform analysis as a tool for
modeling, feature extraction, and denoising, Optical Engineering, 33(7):2170–
2174, July 1994.
16. R.A. DeVore, B. Jawerth, and B.J. Lucier, Image compression through wavelet
transform coding, IEEE Trans. Information Theory, 38(2):719–746, 1992.
17. M.N. Do and and M. Vetterli, The ﬁnite ridgelet transform for image represen-
tation, IEEE Trans. on Image Processing, 12(1):16–28, 2003.

182
9 Sparsity-Seeking Methods in Signal Processing
18. M.N. Do and and M. Vetterli, Framing pyramids, IEEE Trans. on Signal Pro-
cessing, 51(9):2329–2342, 2003.
19. M.N. Do and M. Vetterli, The contourlet transform: an eﬃcient directional mul-
tiresolution image representation, IEEE Trans. Image on Image Processing,
14(12):2091–2106, 2005.
306, April 2006.
21. D.L. Donoho, De-noising by soft thresholding, IEEE Trans. on Information
Theory, 41(3):613–627, 1995.
22. D.L. Donoho, For most large underdetermined systems of linear equations,
the minimal ℓ1-norm solution is also the sparsest solution, Communications on
Pure and Applied Mathematics, 59(6):797–829, June 2006.
23. D.L. Donoho, For most large underdetermined systems of linear equations, the
minimal ℓ1-norm near-solution approximates the sparsest near-solution, Com-
munications on Pure and Applied Mathematics, 59(7):907–934, July 2006.
24. D.L. Donoho and Y. Tsaig, Extensions of compressed sensing, Signal Process-
ing, 86(3):549–571, March 2006.
25. M. Elad, B. Matalon, and M. Zibulevsky, Image denoising with shrinkage and
redundant representations, IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), NY, June 17-22, 2006.
26. M. Elad, J.-L. Starck, P. Querre, and D.L. Donoho, Simultaneous cartoon
and texture image inpainting using morphological component analysis (MCA),
Journal on Applied and Comp. Harmonic Analysis, 19:340–358, 2005.
27. R. Eslami and H. Radha, The contourlet transform for image de-noising using
cycle spinning, in Proceedings of Asilomar Conference on Signals, Systems,
and Computers, pp. 1982–1986, November 2003.
28. R. Eslami and H. Radha, Translation-invariant contourlet transform and its ap-
plication to image denoising, IEEE Trans. on Image Processing, 15(11):3362–
3374, November 2006.
29. O.G. Guleryuz, Nonlinear approximation based image recovery using adaptive
sparse reconstructions and iterated denoising - Part I: Theory, IEEE Trans. on
Image Processing, 15(3):539–554, 2006.
30. O.G. Guleryuz, Nonlinear approximation based image recovery using adaptive
sparse reconstructions and iterated denoising - Part II: Adaptive algorithms,
IEEE Trans. on Image Processing, 15(3):555–571, 2006.
31. W. Hong, J. Wright, K. Huang, and Y. Ma, Multi-scale hybrid linear mod-
els for lossy image representation, IEEE Transactions on Image Processing,
15(12):3655–3671, December 2006.
32. A.K. Jain, Fundamentals of Digital Image Processing, Englewood Cliﬀs, NJ,
Prentice-Hall, 1989.
33. M. Jansen, Noise Reduction by Wavelet Thresholding, Springer-Verlag, New
York 2001.
34. E. Kidron, Y.Y. Schechner, and M. Elad, Cross-modality localization via spar-
sity, to appear in the IEEE Trans. on Signal Processing.
20. D.L. Donoho,Compressedsensing, IEEE Trans.on InformationTheory,52(4):1289–

Further Reading
183
35. M. Lang, H. Guo, and J.E. Odegard, Noise reduction using undecimated dis-
crete wavelet transform, IEEE Signal Processing Letters, 3(1):10–12, 1996.
36. M. Lustig, D.L. Donoho, and J.M. Pauly, Sparse MRI: The application of com-
pressed sensing for rapid MR imaging, submitted to Magnetic Resonance in
Medicine.
37. M. Lustig, J.M. Santos, D.L. Donoho, and J.M. Pauly, k-t SPARSE: High frame
rate dynamic MRI exploiting spatio-temporal sparsity, Proceedings of the 13th
Annual Meeting of ISMRM, Seattle, 2006.
38. Y. Ma, A. Yang, H. Derksen, and R. Fossum, Estimation of subspace arrange-
ments with applications in modeling and segmenting mixed data, SIAM Review,
50(3):413–458, August 2008.
39. D.M. Malioutov, M. Cetin, and A.S. Willsky, Sparse signal reconstruction per-
spective for source localization with sensor arrays, IEEE Trans. on Signal Pro-
cessing, 53(8):3010–3022, 2005.
40. F.G. Meyer, A. Averbuch, and J.O. Stromberg, Fast adaptive wavelet packet
image compression, IEEE Trans. on Image Processing, 9(5):792–800, 2000.
41. F.G. Meyer, A.Z. Averbuch, R.R. Coifman, Multilayered image representa-
tion: Application to image compression, IEEE Trans. on Image Processing,
11(9):1072–1080, 2002.
42. F.G. Meyer and R.R. Coifman, Brushlets: a tool for directional image analy-
sis and image compression, Applied and Computational Harmonic Analysis,
4:147–187, 1997.
43. P. Moulin and J. Liu, Analysis of multiresolution image denoising schemes us-
ing generalized Gaussian and complexity priors, IEEE Trans. on Information
Theory, 45(3):909–919, 1999.
44. G. Peyr´e, Sparse modeling of textures, Journal of Mathematical Imaging and
Vision, 34(1):17-31, May 2009.
45. G. Peyr´e, Manifold models for signals and images, Computer Vision and Image
Understanding, 113(2):249–260, February 2009.
46. J. Portilla, V. Strela, M.J. Wainwright, and E.P. Simoncelli, Image denoising
using scale mixtures of gaussians in the wavelet domain, IEEE Trans. on Image
Processing, 12(11):1338–1351, 2003.
47. L. Rudin, S. Osher, and E. Fatemi, Nonlinear total variation based noise removal
algorithms, Physica D, 60:259–268, 1992.
48. E.P. Simoncelli and E.H. Adelson, Noise removal via Bayesian wavelet coring,
Proceedings of the International Conference on Image Processing, Laussanne,
Switzerland. September 1996.
49. E.P. Simoncelli, W.T. Freeman, E.H. Adelson, and D.J. Heeger, Shiftable multi-
scale transforms, IEEE Trans. on Information Theory, 38(2):587–607, 1992.
50. J.-L. Starck, E.J. Cand`es, and D.L. Donoho, The curvelet transform for image
denoising, IEEE Trans. on Image Processing, 11:670–684, 2002.
51. J.-L. Starck, M. Elad, and D.L. Donoho, Redundant multiscale transforms and
their application for morphological component separation, Advances in Imaging
and Electron Physics, 132:287–348, 2004.

184
9 Sparsity-Seeking Methods in Signal Processing
52. J.-L. Starck, M. Elad, and D.L. Donoho, Image decomposition via the combina-
tion of sparse representations and a variational approach. IEEE Trans. on Image
Processing, 14(10):1570–1582, 2005.
53. J.-L. Starck, M.J. Fadili, and F. Murtagh, The undecimated wavelet decomposi-
tion and its reconstruction, IEEE Transactions on Image Processing, 16(2):297–
309, 2007.
54. D.S. Taubman and M.W. Marcellin. JPEG 2000: Image Compression Funda-
mentals, Standards and Practice, Kluwer Academic Publishers, Norwell, MA,
USA, 2001.
55. J.A. Tropp and A.A. Gilbert, Signal recovery from random measurements via
orthogonal matching pursuit, Submitted for publication, April 2005.
56. R. Vidal, Y. Ma, and S. Sastry, Generalized principal component analysis
(GPCA), IEEE Transactions on Pattern Analysis and Machine Intelligence,
27(12):1945–1959, December 2005.
57. M. Zibulevsky and B.A. Pearlmutter, Blind source separation by sparse decom-
position in a signal dictionary, Neural Computation, 13(4):863–882, 2001.

Chapter 10
Image Deblurring – A Case Study
In this chapter we present an application of the Sparse-Land model to image de-
blurring, in order to demonstrate the applicative side of the above-discussed model
and algorithms. As we show next, this long-studied problem can be handled quite
eﬀectively using the fundamentals of the model with hardly any changes. The con-
tent of this chapter follows closely with the work by M.A.T. Figueiredo and R.D.
Nowak that appeared in ICIP 2005, and a later paper by M. Elad, B. Matalon, and
M. Zibulevsky (2007). While there exists a more recent work that leads to some-
what improved results, the appeal in this work is the relative simplicity with which
near-state-of-the-art results are obtained.
10.1 Problem Formulation
We assume that an original image y of size √n × √n pixels (and in our case
√n = 256) goes through a known linear space-invariant blur operation H, followed
by an additive zero-mean white Gaussian noise with known variance σ2. Thus, we
measure a blurred and noisy image of the same size, ˜y = Hy + v. We aim to recover
y, with the assumption that y could be described as Ax with a known dictionary A
and a sparse vector x. As we have seen in Chapters 6 and 9, this problem leads to
the following minimization problem
ˆx = arg min
x
1
2∥˜y −HAx∥2
2 + λ · 1T · ρ(x) ,
(10.1)
where ρ(x) operates on every entry in x separately. A commonly used choice of
ρ(x) is ρ(x) = |x|p with p ≤1, which leads to the ℓp-norm, but many other options
exist. The restored image is given by ˆy = Aˆx, and our goal is to get as close as
possible to the original image, in terms of the mean-squared-error (MSE). As we
will be employing synthetic experiments, in which we start from an original image,
degrade it, and then restore ˆy, this error can be evaluated.
© Springer Science+Business Media, LLC 2010
M. Elad, Sparse and Redundant Representations: From Theory to Applications in Signal 
and Image Processing, DOI 10.1007/978-1-4419-7011-4_10,
185

186
10 Image Deblurring – A Case Study
There is a similarity between the restoration technique posed here and the Total-
Variation (TV) image restoration, in which the penalty to be minimized is given
by
ˆy = arg min
y
1
2∥˜y −Hy∥2
2 + λ · 1T · ρTV(y).
(10.2)
The term ρTV(y) computes the local (discrete) 2D-gradient length at each pixel,
q
(∂y(h, v)/∂h)2 + (∂y(h, v)/∂v)2,
(h and v represent horizontal and vertical directions) and list those in one column
vector of length n.
In terms of our deﬁnitions in Chapter 9, this is an analysis-based inverse problem,
since the unknown is the image itself and not its representation. We have seen that in
the 1D case, the ﬁrst-derivative in the analysis-form corresponds to a Heaviside basis
in the synthesis counterpart. As we are dealing here with a 2D image, such a relation
no longer holds. Nevertheless, one could replace the TV penalty 1T · ρTV(y) with
an approximated version of it that computes four ﬁrst-derivatives in orientations
0◦, 45◦, 90◦, and 135◦, and sums their absolute values with proper weights,
∥∂y/∂d0∥1 + 1√
2
∥∂y/∂d45∥1 + ∥∂y/∂d90∥1 + 1√
2
∥∂y/∂d135∥1.
This penalty can be written as ∥Ty∥1, where T is of size 4n × n, containing the
four derivative operations. The resulting scheme has an exact analysis structure,
for which we may explore the corresponding synthesis one. We note that while the
obtained operator T is nearly rotation-invariant, it is not of a multi-scale nature, i.e.,
all the computed derivatives are of the same scale. This should be contrasted with
the wavelet alternative which we introduce next.
10.2 The Dictionary
For the formulation in Equation 10.1 to lead to a practical image deblurring algo-
rithm, we have to specify the dictionary A, such that it could lead to sparse represen-
tation modeling of image content. Following the work by Figueiredo and others, we
use the translation-invariant (undecimated) wavelet transform using Daubechies’ 2-
taps ﬁlter (Haar), with 2 layers of resolution, leading to a redundancy factor of 7 : 1.
The chosen Haar transform is obtained by ﬁltering the image with the 1D kernels
[+1, +1]/2 and [+1, −1]/2 horizontally, and applying the same ﬁlters vertically on
the two outputs. This gives 4 output images of the same size as the input one. The
process proceeds with the low-low pass image (the one that went through the kernel
[+1, +1]/2 in both directions), applying the same set of operations, thus getting an
overall of 7 images of size √n× √n each, at the output. This entire process describes

10.2 The Dictionary
187
0 .5 ,
0 .5
 
 
!
"
#
$
0 .5
0 .5
 
!
"
#
$
 
%
&
  
0 .5
 
!
"
#
$
0 .5 ,
0 .5
 
!
"
#
$
%
0 .5
0 .5
 
!
"
#
$
%
&
'
0 .5
0 .5
 
!
"
#
$
 
%
&





0 .5 ,
0 .5
 
 
!
"
#
$
0 .5
 
!
"
#
$
0 .5
0 .5
 
!
"
#
$
%
&
'
&
'
0 .5
 
!
"
#
$
0 .5
0 .5
 
!
"
#
$
%
&
'
 






0 .5 ,
0 .5
 
!
"
#
$
%
0 .5
 
!
"
#
$
0 .5
0 .5
 
!
"
#
$
%
&
'
HL2
HH2
Fig. 10.1 The forward ﬁltering process that computes ATy.
the multiplication ATy in our terminology. Figure 10.1 shows this ﬁltering process.
Figure 10.2 shows the inverse operation that folds these ﬁltered images back to
the original image, thus computing Ax. Figure 10.3 presents the 7 bands obtained
using the traditional wavelet presentation, although, as opposed to the unitary Haar
wavelet described here, each band gives an image of the same size.
We note that this choice of dictionary is far from being the optimal choice for
sparsely representing images, and better results may be obtained if a more adequate
dictionary is used. Such a transform is available within Wavelab, the Rice wavelet
toolbox, and other public domain wavelet packages.
The operation Ax is accomplished by the inverse Haar transform for this redun-
dant case. Since this transform represents a tight Frame (i.e., AAT = I), the adjoint
and the (pseudo) inverse are the same.1 The adjoint operation can admit a ﬁltering
interpretation, much like the operation ATy, but we shall not dwell on this here.
1 Verifying that the inverse transform is also the adjoint (transpose) can be done by computing the
inner-product between the forward transform of two random vectors, v and u, ⟨ATv, ATu⟩, and
verifying that it coincides with vTu. Several such tests leading to exact equality are suﬃcient.

  
0 .5 ,
0 .5
 
 
!
"
#
$
0 .5
0 .5
 
!
"
#
$
 
%
&





0 .5
 
!
"
#
$
0 .5 ,
0 .5
 
!
"
#
$
%
0 .5
0 .5
 
!
"
#
$
%
&
'
0 .5
0 .5
 
!
"
#
$
 
%
&
 



 
0 .5 ,
0 .5
 
 
!
"
#
$
0 .5
 
!
"
#
$
0 .5
0 .5
 
!
"
#
$
%
&
'
&
'
0 .5
 
!
"
#
$
0 .5
0 .5
 
!
"
#
$
%
&
'
HL2
HH2
0 .5 ,
0 .5
 
!
"
#
$
%
0 .5
 
!
"
#
$
0 .5
0 .5
 
!
"
#
$
%
&
'
Fig. 10.2 The inverse ﬁltering process that computes Ax.
Applying the inverse transform on a vector x = ei (i.e., only the i-th entry is
equal to 1 and all the rest are zeros) gives the i-th column in A, and this can be used
for visualizing the atoms in this dictionary. Figure 10.4 presents 20 such atoms that
were chosen at random, out of 2, 800 atoms that serve for representing images of
size 20 × 20 pixels with the same redundancy factor.
10.3 Numerical Considerations
The numerical solvers used in our experiments for the deblurring task are the SSF
and the PCD, as described in Chapter 6, using 100 iterations, with and without a
line search or SESOP speedup. The parameter λ is set manually to lead to the best
results. The method used in the original work by Figueiredo and Nowak is the plain
SSF, given by
188
10 Image Deblurring – A Case Study

10.3 Numerical Considerations
189
Fig. 10.3 The 7 bands of the 2 resolution layers Haar wavelet. This graphic description ﬁts the uni-
tary Haar wavelet, and in our implementation every band leads to an image of the same (original)
size.
xk+1 = Sρ,λ/c
 
xk + 1
cATHT (˜y −HAxk)
!
,
(10.3)
with c = 1, since ∥HA∥2
2 = 1.
In our tests we used ρ(x) = |x| −s log(1 + |x|/s), which leads to near-ℓ1-norm
for small values of s > 0 (s = 0.01 in our experiments). Figure 10.5 presents the
function ρ(x) for several choices of the parameter s. Despite its ﬁrst appearance, this
function is smooth everywhere and convex, as its derivatives manifest,
dρ(x)
dx
= |x|sign(x)
s + |x|
and
d2ρ(x)
dx2
=
s
(s + |x|)2 .
One important advantage of this choice of ρ is the fact that it leads to a closed-form
analytical expression for the shrinkage step. Considering the function
f(x) = (x −x0)2
2
+ λρ(x),
(10.4)
the minimum is obtained for
0 = d f(x)
dx
= x −x0 + λ|x|sign(x)
s + |x| .
(10.5)
Assuming that x0 ≥0, this necessarily implies that the solution to the above equa-
tion, xopt, is also non-negative. This is because in the ﬁrst term ((x −x0)2/2) in
Equation (10.4) would prefer a positive value over a negative one, and the second
term (λρ(x)) is indiﬀerent to this sign. Therefore, the equation we face is
LL2
LH2
HH2
HL2
LH1
HH1
HL1

190
10 Image Deblurring – A Case Study
Fig. 10.4 A set of 20 randomly chosen atoms from a dictionary of size 400× 2800, used for repre-
senting images of size 20×20 pixels. These atoms refer to the Haar translation-invariant transform
with 2 resolution levels. We see in these atoms horizontal, vertical, or combined derivatives, we
see two possible support sizes, and we also see atoms which are constant over their support, repre-
senting the low-low part of the transform.

10.4 Experiment Details and Results
191
0 = d f(x)
dx
= x −x0 + λ
x
s + x = (x + s)(x −x0) + λx
s + x
.
(10.6)
The solution to this equation is given by (the negative option is discarded)
xopt = (x0 −s −λ) +
p
(s + λ −x0)2 + 4sx0
2
.
(10.7)
Naturally, for x0 < 0 the same formula applies with negating and mirroring this
curve. This analytic shrinkage rule for λ = 1 is presented in Figure 10.6 for the
same choices of the parameter s as in Figure 10.5. As can be seen, as s gets smaller,
we are approaching a soft-thresholding function.
Another beneﬁt in the use of the above smoothed ℓ1-norm expression, is the abil-
ity to apply Newton optimization within the line-search (or SESOP optimization)
that takes place within every iteration.
The work by Figueiredo and Nowak is diﬀerent with respect to the choice of
the function ρ, as they consider an ℓp-norm with p = 0.7, or an alternative called
Jeﬀrey’s uninformative prior. Another diﬀerence between the two solutions refers
to the initialization: Our simulations are initialized with x0 = 0, whereas the work
by Figueiredo et al., proposes a potentially better initialization based on an adaptive
Wiener restoration. For a convex penalty function as the one we use, this has no
important consequences, but as we tend to non-convex choices of ρ(x) this diﬀerence
may be critical.
10.4 Experiment Details and Results
In the following experiments we apply the above algorithms for the restoration of
the image shown in Figure 10.7. We assume that the blur kernel is a 15 × 15 ﬁlter
with values 1/(i2 + j2 + 1) for −7 ≤i, j ≤7, normalized to have a unit sum. This
blur kernel is shown in Figure 10.8.
We start by presenting the deblurring performance for σ2 = 2 (λ = 0.075). Figure
10.9 presents the comparative performance of the SSF (plain, with line-search, and
with SESOP-5 acceleration) and the PCD (regular and with SESOP-5 too). The
graphs show the value of the objective as a function of the iteration. Similarly to
the conclusions drawn in Chapter 6, it is clear that (i) line-search is helpful for SSF;
(ii) SESOP acceleration helps further in accelerating SSF and PCD; and (iii) PCD-
SESOP-5 is the fastest to converge.
For a quantitative evaluation of the results, we use the Improvement-Signal-to-
Noise Ratio (ISNR) measure, deﬁned as
IS NR = 10 · log

∥y −˜y∥2
2
∥Aˆx −y∥2
2
[dB] .
(10.8)

192
10 Image Deblurring – A Case Study
−1
−0.5
0
0.5
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
x
ρ(x)
 
 
s=1
s=0.1
s=0.01
s=0.001
−0.05
0
0.05
0
0.005
0.01
0.015
0.02
0.025
0.03
0.035
0.04
0.045
0.05
x
ρ(x)
 
 
s=1
s=0.1
s=0.01
s=0.001
Fig. 10.5
The function ρ(x) for several choices of the parameter s. The top graph shows the
general structure, and the bottom one presents a zoomed version of this graph around the origin.

10.4 Experiment Details and Results
193
−3
−2
−1
0
1
2
3
−3
−2
−1
0
1
2
3
x
Sρ,λ(x)
 
 
s=1
s=0.1
s=0.01
s=0.001
y=x
Fig. 10.6
The shrinkage rule for the function ρ(x), for several choices of the parameter s and
assuming λ = 1.
Fig. 10.7 The original image cameraman, on which the restoration experiments are performed.

194
10 Image Deblurring – A Case Study
0
5
10
15
0
5
10
15
0
0.02
0.04
0.06
0.08
 
 
5
10
15
5
10
15
0.02
0.04
0.06
Fig. 10.8
The blur kernel used, a 15 × 15 ﬁlter with values 1/(i2 + j2 + 1) for −7 ≤i, j ≤7,
normalized to a unit sum. The blur is presented both as a mesh-graph and as an image.
This way, if we propose ˜y as the solution, IS NR = 0dB. For a restored image of
better quality, the ISNR is expected to be positive, and the higher it is, the better
the image quality obtained. Figure 10.10 shows the ISNR results for the test with
σ2 = 2, as a function of the iteration for the various iterative-shrinkage algorithms
tested. As can be seen, PCD (with or even without a SESOP speedup) is the fastest
to obtain the maximal quality.2
Figure 10.11 shows the restored images obtained after 30 iterations of the PCD
algorithm. Figure 10.12 similarly shows the result obtained for σ2 = 8 (for which
we choose λ = 0.15 for best ISNR). As can be seen, the resulting images are restored
very well, even by this very simple and crude algorithm.
As a teaser, just before we conclude this chapter, we draw the reader’s attention
to the following strange behavior. As we minimize the penalty function discussed
in this chapter, f(x) = 0.5∥Ax −y∥2
2 + λ · 1T · ρ(x), our goal is the recovery of the
sparse representation x of the image we are restoring. As such, we expect this vector
to be sparse, so as to fulﬁl the Sparse-Land model assumptions. Thus, it is natural
to ask: How sparse is this vector at the solution? Figure 10.13 shows a sorting of the
absolute values of the entries in x. As can be seen, the result is not sparse at all!
2 There are ways to stop these algorithms when they get to the peak-ISNR value based on the
SURE estimator.

10.4 Experiment Details and Results
195
0
20
40
60
80
100
10
2
10
3
10
4
10
5
10
6
10
7
Iteration
f(xk)−fmin
 
 
SSF
SSF−LS
SSF−SESOP−5
0
20
40
60
80
100
10
2
10
3
10
4
10
5
10
6
10
7
Iteration
f(xk)−fmin
 
 
PCD−LS
PCD−SESOP−5
SSF results
Fig. 10.9 The value of the penalty function in Equation 10.1 as a function of the iteration, shown
for several iterative-shrinkage algorithms: (i) Plain SSF; (ii) SSF boosted by a line-search; (iii)
PCD (with line-search); (iv) SSF with SESOP speedup; and (v) PCD with SESOP speedup.

196
10 Image Deblurring – A Case Study
0
20
40
60
80
100
0
1
2
3
4
5
6
7
Iteration
ISNR(xk)
 
 
SSF
SSF−LS
SSF−SESOP−5
0
20
40
60
80
100
0
1
2
3
4
5
6
7
Iteration
ISNR(xk)
 
 
PCD−LS
PCD−SESOP−5
SSF results
Fig. 10.10
Deblurring results in terms of ISNR as a function of the iteration, shown for several
iterative-shrinkage algorithms: (i) Plain SSF; (ii) SSF boosted by a line-search; (iii) PCD (with
line-search); (iv) SSF with SESOP speedup; and (v) PCD with SESOP speedup.

10.4 Experiment Details and Results
197
Fig. 10.11 Deblurring results with 30 iterations of the PCD algorithm: Measured blurry and noisy
image (left), and restored (right). This result refers to the experiment with σ2 = 2 (resulting with
IS NR = 7.05dB).
Fig. 10.12 Deblurring results with 30 iterations of the PCD algorithm: Measured blurry and noisy
image (left), and restored (right). This result refers to the experiment with σ2 = 8 (resulting with
IS NR = 5.17dB).

198
10 Image Deblurring – A Case Study
0.5
1
1.5
2
2.5
3
3.5
4
4.5
x 10
5
10
−10
10
−8
10
−6
10
−4
10
−2
10
0
10
2
index
Sorted absolute values
Fig. 10.13 The PCD solution after 30 iterations presented by sorting it absolute entries. As can be
seen, the result is not sparse at all!
The reason for this result is clearly the choice of λ, which gives a too weak
emphasis to the second term in the penalty, thus allowing a non-sparse solution. As
we have said above, this choice of λ is deliberate, giving the best result in terms of
ISNR (or the smallest Mean-Squared-Error – the two are coupled). Thus we should
ask: Why the best performing λ leads to a dense result, and is this a contradiction
to the Sparse-Land model we have described so enthusiastically in the previous
chapter? An interesting answer to this question is provided in Chapter 11, where
we consider explicit ways to compute the best (in terms of Mean-Squared-Error)
estimate of x for the Sparse-Land model.
10.5 Summary
It seems that a direct use of the Sparse-Land model from Chapter 9 to the image
deblurring problem leads to a successful application. This stands as the ﬁrst among
many testimonials for the practicality of this model for image processing tasks. The
reason we have chosen to begin our journey to applications with this problem in
particular is the purity and immediacy with which we have migrated from the core
ideas in Chapter 9 to an actual solution of this inverse problem.

Further Reading
199
Better image deblurring algorithms exist, and one such method is the BM3D-
based method by Dabov et al. Interestingly, while this method diﬀers quite substan-
tially from the algorithm posed here, it is also based on the Sparse-Land model. This
method is applied on image patches, with a collaborative treatment of the image con-
tent. We shall return to these themes and discuss patch-based sparsity methods and
BM3D in particular, in later chapters.
This chapter also revisits the topic of iterative-shrinkage algorithms from Chapter
6. We see again the behavior of the SSF and the PCD and their accelerated versions.
The bottom line we have reached here suggests that in a matter of 10−20 iterations,
each applying one multiplication by A and its adjoint, one can get to the desired
result. There is one fact shadowing this conclusion – we observed that the outcome
does not show any tendency to be sparse. Does this stand in contradiction to the
Sparse-Land model? We now turn to Chapter 11, where this question is given an
interesting and surprising answer.
Further Reading
1. J. Bioucas-Dias, Bayesian wavelet-based image deconvolution: a GEM algo-
rithm exploiting a class of heavy-tailed priors, IEEE Trans. on Image process-
ing, 15(4):937–951, April 2006.
2. K. Dabov, A. Foi, and K. Egiazarian, Image restoration by sparse 3D transform-
domain collaborative ﬁltering, Proc. SPIE Electronic Imaging ’08, no. 6812-07,
San Jose, California, USA, January 2008.
3. I. Daubechies, M. Defrise, and C. De-Mol, An iterative thresholding algorithm
for linear inverse problems with a sparsity constraint, Communications on Pure
and Applied Mathematics, LVII:1413–1457, 2004.
4. M. Elad, B. Matalon, and M. Zibulevsky, On a Class of Optimization meth-
ods for Linear Least Squares with Non-Quadratic Regularization, Applied and
Computational Harmonic Analysis, 23:346–367, November 2007.
5. M. Elad, B. Matalon, J. Shtok, and M. Zibulevsky, A wide-angle view at iterated
shrinkage algorithms, SPIE (Wavelet XII) 2007, San-Diego CA, August 26-29,
2007.
6. M.J. Fadili, J.-L. Starck, Sparse representation-based image deconvolution by
iterative thresholding, Astronomical Data Analysis ADA’06, Marseille, France,
September, 2006.
7. M.A. Figueiredo and R.D. Nowak, An EM algorithm for wavelet-based image
restoration, IEEE Trans. Image Processing, 12(8):906–916, 2003.
8. M.A. Figueiredo, and R.D. Nowak, A bound optimization approach to wavelet-
based image deconvolution, IEEE International Conference on Image Process-
ing - ICIP 2005, Genoa, Italy, 2:782–785, September 2005.
algorithms for wavelet-based image restoration, IEEE Trans. on Image Process-
ing, 16(12):2980–2991, December 2007.
9. M.A. Figueiredo, J.M. Bioucas-Dias, and R.D. Nowak, Majorization-minimization

200
10 Image Deblurring – A Case Study
10. L. Rudin, S. Osher, and E. Fatemi, Nonlinear total variation based noise removal
algorithms, Physica D, 60:259–268, 1992.
11. H. Takeda, S. Farsiu, and P. Milanfar, Deblurring using regularized locally-
adaptive kernel regression, IEEE Trans. on Image Processing, 17(4):550–563,
April 2008

Chapter 11
MAP versus MMSE Estimation
So far we kept the description of the pursuit algorithms on a deterministic level, as an
intuitive optimization procedure. We mentioned in Chapter 9 that these algorithms
correspond to an approximation of the Maximum-A’posteriori-Probability (MAP)
estimator, but this connection was not explicitly derived. In this chapter we make this
claim exact by deﬁning the quest for sparse representations as an estimation task.
As we shall see, this calls for a clear and formal deﬁnition of the stochastic model
assumed to generate the sparse representation vector. A beneﬁt of such treatment is
an ability to derive the Minimum-Mean-Squared-Error (MMSE) estimator as well,
and this in turn leads to the need to approximate it. These and more are the topics
we cover in this chapter.
11.1 A Stochastic Model and Estimation Goals
We start by deﬁning a random generator of sparse vectors. While this deﬁnition
is arbitrary and can be modiﬁed in various ways, it grasps the essence of sparse
representations and their general behavior.
The generator we deﬁne produces a sparse vector x ∈IRm by ﬁrst drawing at
random an integer scalar |s| from the Probability-Density-Function (PDF) Ps(|s|),
which represents the cardinality, ∥x∥0 = |s|. A typical behavior of Ps(|s|) would be
one that promotes sparser vectors, such as Ps(|s|) ∼1/|s|, Ps(|s|) ∼exp(−α|s|), or
even Ps(|s|) ∼δ(k −|s|) that imposes a ﬁxed k non-zeros. For simplicity, we shall
proceed with this limited case where |s| = k.
Once k is chosen (or set, as mentioned above), we consider all
m
k

possible sup-
ports as equi-probable, and we draw one support s at random, such that |s| = k.
Finally, we assume that the non-zero entries in s are drawn from N(0, σ2
x) as iid
entries. This completes the description of how x is generated, and this description
induces a PDF for this source, denoted by PX(x). We shall refer to this random
generator of vectors as G1.
© Springer Science+Business Media, LLC 2010
M. Elad, Sparse and Redundant Representations: From Theory to Applications in Signal 
201
and Image Processing, DOI 10.1007/978-1-4419-7011-4_11,

202
11 MAP versus MMSE Estimation
As stated above, this model can be modiﬁed in various ways. Examples for gener-
alizing this model include the possibility to assign diﬀerent variances to the chosen
non-zero coeﬃcients, an ability to assign diﬀerent probabilities for each entry to be
chosen, instead of spreading this uniformly, replacing the Gaussianity of the entries
with a heavy tailed distribution, and more.
Another way to pose a prior on x is to assign to each atom a probability Pi to
participate in the active support. Once chosen, its entry value follows the same iid
Gaussian rule as above. Such a model may result with various supports, from the
very empty to the very full. Assuming that all atoms share the same probability and
the same variance σ2
x, this implies that 2 parameters govern the complete behavior
of this model. The great appeal in this model is the total independence between the
m elements of x, as opposed to the (somewhat hidden) dependency that exists in G1.
We shall refer to this model as G2. Much of the analysis presented in this chapter
refers to both models, G1 and G2, equally well.
Returning to our random generator G1 described above, the random vector x is
assumed to be multiplied by the dictionary A of size n × m, giving a signal z that
is known to have a sparse representation. This signal is multiplied by a degradation
matrix H of size q × n. The measurement is the vector y = HAx + e, where e is a
zero-mean white Gaussian noise vector with iid entries, drawn from N(0, σ2
e). The
estimation goal is the recovery of x (or z) based on y and the statistical information
available to us on x and e.
The estimation we shall construct depends on various considerations. First, we
should deﬁne the goal for the estimator. The MAP estimator aims to ﬁnd ˆx that
maximizes the posterior probability P(x|y). The MMSE aims to ﬁnd the solution
ˆx that minimizes the expected ℓ2-error of the estimation. Other goals and estima-
tors can be proposed, but we shall mostly restrict the discussion in this chapter to
these two. Beyond the objective the estimator targets, we shall see that practical
numerical considerations are also of interest, and especially so for cases where the
exact estimation cannot be computed or derived. As we shall see next, all the practi-
cal estimators formed for addressing the above deﬁned problem are in general only
approximations.
11.2 Background on MAP and MMSE
Since we use the MAP and the MMSE estimators quite extensively in this chapter,
we provide here a general deﬁnition of these two Bayesian tools. Given a mea-
surement vector y and an unknown vector to be estimated, x, the MAP estimator
proposes the following estimate:
ˆxMAP = max
x
P(x|y),
(11.1)
where P(x|y) is the posterior probability of x given the measurement y. Clearly, the
complexity of constructing the MAP estimate is mainly manifested in formulating

11.2 Background on MAP and MMSE
203
well this conditional probability. Once this is done, the estimation problem becomes
an optimization task. In cases where this optimization is non-convex with many
possible local minimum solutions, the estimation becomes problematic and may
divert from the true objective posed.
Using Bayes’ rule, the posterior probability of x given the measurements is given
by
P(x|y) = P(y|x)P(x)
P(y)
.
(11.2)
Considering the fact that the denominator P(y) is not a function of the unknown x,
and as such it can be disregarded, the MAP estimation amounts to
ˆxMAP = max
x
P(x|y) = max
x
P(y|x)P(x).
(11.3)
The probability P(y|x) is known as the likelihood function, and the probability P(x)
is the unknown’s prior. If this prior probability is ﬂat and uninformative, the above
process eﬀectively maximizes the likelihood function, leading to the well-known
Maximum-Likelihood estimator.
Turning to the MMSE, a well-known and classic result states that this estimator
is obtained as the conditional mean of the unknown. This is easily seen by writing
the mean-squared error as
MSEx = E

∥ˆx −x∥2
2|y

=
Z
x
∥ˆx −x∥2
2P(x|y)dx,
(11.4)
and taking a derivative with respect to the estimate ˆx in order to minimize the error.
This leads to
∂MSEx
∂ˆx
= 2
Z
x
(ˆx −x)P(x|y)dx = 0.
(11.5)
Thus, the optimal (MMSE) estimator is given as
ˆxMMS E =
R
x xP(x|y)dx
R
x P(x|y)dx
=
Z
x
xP(x|y)dx = E (x|y) .
(11.6)
The denominator in the above expression equals 1 since it is a complete integration
over the probability rule P(x|y).
Interestingly, if we deﬁne our goal as minimizing a weighted error with a matrix
W, the error considered is
WMSEx = E

∥W(ˆx −x)∥2
2|y

(11.7)
=
Z
x
∥W(ˆx −x)∥2
2P(x|y)dx.
The WMMSE estimator is obtained by requiring

204
11 MAP versus MMSE Estimation
∂WMSEx
∂ˆx
= 2WTW
Z
x
(ˆx −x)P(x|y)dx = 0,
(11.8)
If WTW is invertible, the above expression leads to the same estimator as before,
since the multiplication by WTW can be omitted. If this matrix is not invertible,
this implies that our estimate is not unique, as it can remain optimal by adding any
portion that is found in the null-space of WTW.
Considering our estimation task, y = HAx + e, we could consider the minimum
MSE in the representation domain, E(∥x −ˆx∥2
2|y), or focus on the error in the signal
domain, E(∥Ax −ˆz∥2
2|y). It is quite easy to see that the two estimation tasks align,
both requiring the minimizer of E(∥x−ˆx∥2
2|y), and if we desire the signal, we further
multiply by A. This is because
∂E(∥Ax −ˆz∥2
2|y)
∂ˆz
= ∂
∂ˆz
Z
x
∥Ax −ˆz∥2
2P(x|y)dx
(11.9)
= 2
Z
x
(ˆz −Ax)P(x|y)dx = 0,
which leads to
ˆz = A
Z
x
xP(x|y)dx = AE(x|y).
(11.10)
11.3 The Oracle Estimation
11.3.1 Developing the Oracle Estimator
We have discussed the oracle in the context of the Dantzig-Selector, and we return to
it now, developing it for the estimation problem at hand. The oracle is an infeasible
estimator, as it assumes the knowledge of the chosen support for x, information that
is unknown in the actual problem. Nevertheless, there are several good reasons to
consider its construction:
• The oracle is the core ingredient in the practical estimators we derive next;
• The oracle gives a reference performance quality to compare against;
• The oracle provides simple and convenient closed-form expressions for the esti-
mation and the induced MSE; and
• The oracle is relatively easy to derive, and its development reveals some interest-
ing insight into the more complicated estimators that follow.
Since the support s is known, all that remains to estimate are the values of the non-
zero entries in x. We denote by xs a vector of length s containing those unknowns.
Similarly, As is a sub-matrix of size n × k that corresponds to the support. Clearly,
y = HAsxs + e, where xs ∼N(0, σ2
xI). Using Bayes’ rule, the posterior probability
of xs given the measurements is given by

11.3 The Oracle Estimation
205
P(xs|y) = P(y|xs)P(xs)
P(y)
.
(11.11)
The denominator P(y) can be disregarded as it is a constant factor normalizing this
probability. The probabilities P(y|xs) and P(xs) are easily obtained as simple multi-
variate Gaussians,
P(xs) =
1
(2π)s/2σsx
exp
(
−xT
s xs
2σ2x
)
,
(11.12)
and
P(y|xs) =
1
(2π)q/2σq
e
exp
−∥HAsxs −y∥2
2
2σ2e
.
(11.13)
Thus, the posterior for the oracle is given, up to a normalization factor, as
P(xs|y) ∝exp
−xT
s xs
2σ2x
−∥HAsxs −y∥2
2
2σ2e
.
(11.14)
The MAP-oracle estimation would be the vector xs that maximizes the above prob-
ability, and this is given by
ˆxMAP−oracle
s
= max
xs P(xs|y)
(11.15)
= max
xs exp
−xT
s xs
2σ2x
−∥HAsxs −y∥2
2
2σ2e

=
 1
σ2e
AT
s HTHAs + 1
σ2x
I
!−1 1
σ2e
AT
s HTy.
Notice a (linear) shrinkage eﬀect in the above expression, as the term 1
σ2x I causes the
result to be somewhat smaller, compared to a direct pseudo-inverse of HAs.
Rather than developing the MAP, the oracle can target instead a minimization
of the MSE as its design goal. As we have seen above, the MMSE-oracle can be
obtained as the mean of the posterior probability P(xs|y). Since this probability is
Gaussian, its peak is also its mean, which implies that the MAP-oracle is also the
MMSE one. This outcome is classic and well-known, in the context of the Wiener
ﬁlter.
If we desire to minimize the oracle’s error in the signal domain, the error to
minimize is E

∥As(ˆx −x)∥2
2|y

. Assuming that AT
s As is invertible, we have seen that
the same estimator ˆx remains optimal for this goal as well. This implies that we
assume n ≥k, which is readily satisﬁed for the sparse vectors x we are interested
in. Similarly, if we target the error in the measurements domain, non-singularity
of AT
s HTHAs implies the same estimator, and this requirement also suggests that
q ≥k.

206
11 MAP versus MMSE Estimation
11.3.2 The Oracle Error
We now turn to discuss the MSE induced by the oracle estimator. Since we have seen
that the expression obtained ﬁts the two discussed estimators (MMSE and MAP),
we shall refer to it hereafter as ˆxoracle. The MSE is given by
E
ˆxoracle
s
−xs
2
2

= E


 1
σ2e
AT
s HTHAs + 1
σ2x
I
!−1 1
σ2e
AT
s HTy −xs

2
2

= E


 1
σ2e
AT
s HTHAs + 1
σ2x
I
!−1 1
σ2e
AT
s HT(HAsxs + e) −xs

2
2
,
where we have used the fact that y = HAsxs + e. Rearranging terms and deﬁning
Qs =
1
σ2e AT
s HTHAs + 1
σ2x I, we obtain
E
ˆxoracle
s
−xs
2
2

= E
 Q−1
s
 1
σ2e
AT
s HTHAs −1
σ2e
AT
s HTHAs −1
σ2x
I
!
xs
+Q−1
s
1
σ2e
AT
s HTe

2
2

= E

−Q−1
s
1
σ2x
xs + Q−1
s
1
σ2e
AT
s HTe

2
2

= trace
 1
σ4x
Q−2
s E(xsxT
s ) −
2
σ2eσ2x
Q−2
s AT
s HT E(exT
s )
+ 1
σ4e
Q−2
s AT
s HT E(eeT)HAs
!
= trace
 
Q−2
s
" 1
σ2x
I + 1
σ2e
AT
s HTHAs
#!
= trace

Q−1
s

.
Here we have used the fact that E(exT
s ) = E(e)E(xT
s ) = 0, because the two are
independent. Thus, the oracle gives an MSE error
E
ˆxoracle
s
−xs
2
2

= trace

" 1
σ2e
AT
s HTHAs + 1
σ2x
I
#−1.
(11.16)
As an example, if the columns of HAs are orthogonal and H is square n × n, this
error becomes σ2
xσ2
e · k/(σ2
x + σ2
e). Whereas the additive noise power is nσ2
e, the
oracle error is less than k/n times this energy (it is in fact slightly smaller because of

11.3 The Oracle Estimation
207
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
0
0.02
0.04
0.06
0.08
0.1
0.12
0.14
0.16
0.18
0.2
σe
Relative Mean−Squared−Error
 
 
Oracle (empirical)
Oracle (formula)
k/n factor
Fig. 11.1 The oracle performance as a function of the input noise. The performance is shown both
by an empirical calculation of the oracle estimate and a comparison to the ground-truth, and by
using the exact formula for the expected error. Results are shown relative to the input noise energy
nσ2
e.
the additional factor σ2
x/(σ2
x + σ2
e)). For non-orthogonal columns, this noise decay
is smaller.
Figure 11.1 presents the oracle results for the following experiment: A random
dictionary A of size 20 × 30 is generated with iid Gaussian entries, and with nor-
malized columns. We assume hereafter that H = I. Sparse random vectors x are
generated, using the G1 model, with a ﬁxed cardinality k = 3, and σx = 1. A Gaus-
sian iid noise vector is added to Ax, with standard-deviation in the range [0.1, 2].
For each such parameter setting we perform 1, 000 experiments, and average the
outcomes.
The results in Figure 11.1 are presented as the ratio between the oracle’s error and
the input noise E(∥e∥2) = nσ2
e, such that a ratio smaller than 1 implies an eﬀective
noise removal. One graph presents the empirical error obtained by computing ˆxoracle
using Equation (11.15) and evaluating the error directly by ∥x−ˆxoracle∥2
2. The second
graph shows the error as predicted by the formula in Equation (11.16). This ﬁgure
also shows the constant factor k/n = 0.15, which is the expected noise decay-factor
for the unitary case. Since this experiment uses a general matrix A, the actual ratio
obtained by the oracle error is not necessarily smaller than k/n, but as we can see, it
tends to meet this ratio and beyond.

208
11 MAP versus MMSE Estimation
11.4 The MAP Estimation
11.4.1 Developing the MAP Estimator
We return to the estimation problem deﬁned at the beginning of this chapter, and
assume no knowledge about the support. In order to propose a MAP estimator, we
should formulate the posterior P(x|y). Marginalizing over all the possible supports
s ∈Ω, we have
P(x|y) =
X
s∈Ω
P(x|s, y)P(s|y).
(11.17)
Consider a candidate x – if it has a support that does not align with any s ∈Ω, the
probabilities P(x|s, y) in the above summation are all zeros, and the overall prob-
ability of such a candidate solution is zero as well. Therefore, such a solution can
hardly compete as being the MAP estimate, maximizing of the posterior.
Thus, we consider a potential solution x with a speciﬁc support s∗∈Ω. Clearly,
since s∗is diﬀerent from all other supports in Ω\ s∗, only the term that corresponds
to s∗remains in the above summation, and the probability for such x reduces to
P(x|y) = P(x|s∗, y)P(s∗|y). In fact, as we consider a speciﬁc s∗on which x is sup-
ported, we can replace x by xs, as the remaining entries are known to be zeros. Using
this rationale, the MAP estimation is simpliﬁed to
ˆxMAP = arg max
x
P(x|y) = arg max
s∈Ω,xs P(xs|s, y)P(s|y).
(11.18)
This means that our search for the support s and the vector xs that corresponds
to this support are coupled into one process. In a way, one could regard this as a
model selection problem, where the MAP chooses the model (the support s) that best
explains the measurements. Using Bayes’ rule, we can rewrite the above probability
expression as
P(xs|s, y)P(s|y) = P(y|s, xs)P(xs|s)
P(y|s)
· P(y|s)P(s)
P(y)
= P(y|s, xs)P(xs|s)P(s)
P(y)
.
The MAP estimator is further simpliﬁed by removing the division by P(y), because
it is constant with respect to the unknowns, leading to
ˆxMAP = arg max
x
P(x|y)
(11.19)
= arg max
x,s
P(y|s, xs)P(xs|s)P(s).
If we assume that s is known, as the expression P(xs|s) suggests, the random vector
xs becomes a multivariate Gaussian with zero mean and σ2
xI as its covariance matrix
, and thus

11.4 The MAP Estimation
209
P(xs|s) =
1
(2π)k/2σkx
exp
−∥xs∥2
2
2σ2x
.
(11.20)
Similarly, for a known xs and support s, the measurement y is a sum of a determin-
istic vector HAsxs and the random noise e, and thus this vector is also Gaussian,
P(y|s, xs) =
1
(2π)q/2σq
e
exp
−∥HAsxs −y∥2
2
2σ2e
.
(11.21)
Plugging these two expressions into the MAP deﬁnition in Equation (11.19) and ap-
plying a −log on the obtained expression, we get an equivalent optimization prob-
lem of the form
ˆxMAP = arg min
x,s

∥HAsxs −y∥2
2
2σ2e
+ ∥xs∥2
2
2σ2x
+ k log(
√
2πσx) −log(P(s))
.
The term k log(
√
2πσx) comes from the normalization factor of the probability
P(xs|s). If k is ﬁxed, this term can be disregarded, but when it is not, it must be
inserted, as it aﬀects the number of non-zeros, k, in the optimal solution.
For example, if we replace the originally assumed support-prior, P(s) = δ(|s|−k),
by the more general option, P(s) = Const · exp(−α|s|), this leads to
ˆxMAP = arg min
x,s

∥HAsxs −y∥2
2
2σ2e
+ ∥xs∥2
2
2σ2x
(11.22)
+(α + log(
√
2πσx))|s|
o
.
Written diﬀerently, and exploiting that fact that |s| = ∥x∥0, we get
ˆxMAP = arg min
x

∥HAx −y∥2
2
2σ2e
+ ∥x∥2
2
2σ2x
(11.23)
+(α + log(
√
2πσx))∥x∥0
o
.
This formulation reveals the connection between this estimation and the problems
we have posed and treated in earlier chapters – this is a variation of the (Pϵ
0) problem.
Interestingly, the term ∥x∥2
2 did not appear in these chapters, and its presence here is
part of the more exact formulation that we derive, and in particular the fact that the
non-zeros in x are Gaussian.
Let us return to the assumption that only supports with a pre-speciﬁed cardinality
k are permitted (i.e., P(s) = δ(|s| −k)), and P(s) is uniform over all these, being
equal to 1/
m
k

. Thus, the last term can be discarded from Equation (11.22) as it does
not inﬂuence the maximization. For a ﬁxed s, the minimization posed in Equation
(11.22) is easy, and we have seen it already in the context of developing the oracle,
ˆx∗
s =
 1
σ2e
AT
s HTHAs + 1
σ2x
I
!−1 1
σ2e
AT
s HTy
(11.24)

210
11 MAP versus MMSE Estimation
= 1
σ2e
Q−1
s AT
s HTy.
However, we have to sweep over all possible supports s ∈Ω, compute this expres-
sion, plug it into the penalty function posed in Equation (11.22), and choose the
one that leads to the minimal value. Plugging this expression into the penalty as
suggested gives (after some tedious algebraic steps):
Penalty = ∥HAsx∗
s −y∥2
2
2σ2e
+ ∥x∗
s∥2
2
2σ2x
(11.25)
=
∥1
σ2e HAsQ−1
s AT
s HTy −y∥2
2
2σ2e
+
∥1
σ2e Q−1
s AT
s HTy∥2
2
2σ2x
= yT
 1
2σ6e
(HAsQ−1
s AT
s HT)2 −1
σ4e
(HAsQ−1
s AT
s HT)
+
1
2σ2e
I +
1
2σ4eσ2x
HAsQ−2
s AT
s HT
!
y
= ∥y∥2
2
2σ2e
−1
σ4e
yTHAsQ−1
s
 
I −
1
2σ2x
Q−1
s
−
1
2σ2e
AT
s HTHAsQ−1
s
!
AT
s HTy.
= ∥y∥2
2
2σ2e
−
1
2σ4e
yTHAsQ−1
s AT
s HTy.
In the last step in the above equation we have used the fact that the term in the
parenthesis is equal to 0.5I. This expression suggests that the search for the MAP
support should be done by maximizing the term
Val(s) = yTHAsQ−1
s AT
s HTy = ∥Q−0.5
s
AT
s HTy∥2
2
This implies a sweep through all
m
k

possible supports in Ω, an impossible task in
general. To summarize, MAP estimation is obtained by
ˆxMAP = 1
σ2e
Q−1
s∗AT
s∗HTy
(11.26)
where s∗= arg max
s∈Ω∥Q−0.5
s
AT
s HTy∥2
2.
In the case where HA is square and unitary, this estimation can be further simpli-
ﬁed. The matrix Qs becomes proportional to I, and thus does not inﬂuence the above
maximization. Using the notation β = ATHTy, the MAP optimal support is found
by computing this vector and choosing its k largest entries in absolute value. Thus,
as opposed to the general MAP that cannot be practically computed, the unitary case
admits a closed-form and cheap solution.

11.4 The MAP Estimation
211
Just before turning to approximations of the MAP estimation, we note the follow-
ing. An alternative way to deﬁne the MAP estimation is by maximizing the posterior
P(s|y), i.e., seeking the most probable support that explains the measured data. Us-
ing Bayes’ rule, this would be equivalent to the maximization of P(y|s)P(s), where
P(y|s) is a multivariate Gaussian distribution that can be built by marginalizing over
xs (see Equation (11.31) and beyond). In the case considered here (with a ﬁxed car-
dinality |s| = k), this approach leads to a very similar solution as the one we derive
above. In more complex scenarios, though, this approach may lead to an entirely
diﬀerent and more stable result—see a discussion about this issue in the paper by
Turek et al.
11.4.2 Approximating the MAP Estimator
For general cardinality k, the process described above is infeasible, as it requires a
sweep through all
m
k

possibilities, and thus there is no hope to provide an exact
MAP estimate for our problem. Interestingly, for k = 1, we have only
m
1

= m
options to consider and this becomes practical. Indeed, denoting the m columns of
the matrix HA by ˜ai for i = 1, 2, . . . , m, for k = 1 the above expression uses these
single columns maximizing the term
Val(i) =
∥˜aT
i y∥2
2
∥˜ai∥2
2
σ2e + 1
σ2x
.
(11.27)
If we further assume that all the columns ˜ai have the same ℓ2-norm, this expression
simply suggests to maximize the absolute inner product of the measurement vector
with each column in HA in the quest for the best column (and thus, support of
cardinality k = 1).
Based on this insight, one could propose a greedy algorithm that accumulates
the k elements of the support one by one – apply the process for k = 1 and choose
the ﬁrst support element. Then proceed to the next stage by adding one column to
the support, performing m −1 tests (since one column has been already chosen).
This process should be repeated k times, accumulating the k elements of the support
in a greedy fashion. This leads to a variant of the Matching-Pursuit algorithm, as
this method too relies on maximizing inner products between the columns in HA
and the measurements. Thus, we can regard the OMP (or any of its variants) as an
approximation of the MAP for the estimation problem we have posed.
As a side note we add that the OMP is slightly diﬀerent from the greedy algo-
rithm that emerges above, and this might suggest that modiﬁcations to the OMP
are required to better suit our estimation goals, depending on the validity of the
proposed model. For example, considering the second stage in the above-proposed
greedy method, assume that i1 is the index chosen in the ﬁrst stage, and we are
searching now for the second one, i2. The term to maximize is

212
11 MAP versus MMSE Estimation
Val(i2) = yTHAsQ−1
s AT
s HTy
(11.28)
=
h
˜aT
i1y ˜aT
i2y
i 
∥˜ai1∥2
2
σ2e
+ 1
σ2x
˜aT
i1 ˜ai2
σ2e
˜aT
i1 ˜ai2
σ2e
∥˜ai2∥2
2
σ2e
+ 1
σ2x

−1 " ˜aT
i1y
˜aT
i2y
#
and this is clearly diﬀerent from computing the residual from y after removal of the
ﬁrst column and then repeating the k = 1 test again. While the above numerical
process might seem daunting, there are numerical shortcuts that can be proposed,
such that every iteration requires roughly the same amount of computations as the
ﬁrst iteration.
Returning to the experiment for which we have demonstrated the oracle’s be-
havior in Figure 11.1, we can now add the MAP performance. Figure 11.2 shows
the empirical MAP error, obtained by computing the estimate ˆxMAP using Equation
(11.26), and evaluating the error directly by ∥x −ˆxMAP∥2
2. This ﬁgure also shows an
approximation of the MAP by the OMP. The method used here is diﬀerent from the
ones discussed above – we employ the very same OMP as discussed in Chapter 5,
extracting a solution with cardinality ∥x∥0 = 3. Using this as the chosen support for
our approximate estimate, we compute the ﬁnal result as an oracle estimate over this
support. As can be seen, this approximation is quite close to the exhaustive MAP
that sweeps over all
m
k

= 4060 supports in searching the optimal one.
11.5 The MMSE Estimation
11.5.1 Developing the MMSE Estimator
We have already seen that the MMSE estimate is given by E(x|y), and using the
expression for the posterior probability in Equation (11.17) this gives
E(x|y) =
Z
x
xP(x|y)dx
(11.29)
=
X
s∈Ω
P(s|y)
Z
x
xP(x|s, y)dx
The inner integral stands for the MMSE estimate for the case where the support s is
known, and this is the oracle we have already seen, given by
Z
x
xP(x|s, y)dx = E(x|y, s) = ˆxoracle
s
(11.30)
=
 1
σ2e
AT
s HTHAs + 1
σ2x
I
!−1 1
σ2e
AT
s HTy.

11.5 The MMSE Estimation
213
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
0
0.1
0.2
0.3
0.4
0.5
σe
Relative Mean−Squared−Error
 
 
Oracle (empirical)
Exact MAP (empirical)
Approx. MAP by OMP
k/n factor
Fig. 11.2 The exact (and thus exhaustive) and the approximated MAP performance as a function
of the input noise. The approximation is obtained by the plain OMP used to detect the support, and
then an oracle formula over this support. Results are shown relative to the input noise energy nσ2
e.
The above already indicates that the MMSE estimator is about to be a weighted
average of many “oracle” solutions, each with a diﬀerent support, and each weighted
by its likelihood to be correct, deﬁned as P(s|y). We therefore turn now to develop
an expression for this probability.
Using Bayes’ rule again, we have that P(s|y) = P(y|s)P(s)/P(y). The probability
P(s) is constant over all the supports s ∈Ωsuch that |s| = k, and thus can be omitted.
Similarly, P(y) can be considered as a normalizing factor in our development of
P(s|y) and thus can be disregarded as well. Thus, we end up with the need to evaluate
P(y|s). This probability can be written by marginalization over xs, integrating over
all possible values of the non-zeros in the support s,
P(y|s) =
Z
xs
P(y|s, xs)P(xs|s)dxs.
(11.31)
This integral is k-dimensional, averaging P(y|s, xs) over all xs ∈IRk. The probability
P(xs|s) is known to be Gaussian, N(0, σ2
xI). Similarly, once both the support and the
non-zero values are given (s and xs), y is also Gaussian, with HAsxs as its mean,
and σ2
eI as its covariance. Plugging these into Equation (11.31), we have
P(y|s) =
Z
xs
P(y|s, xs)P(xs|s)dxs
(11.32)

214
11 MAP versus MMSE Estimation
∝
Z
v∈IR
k exp
−∥HAsv −y∥2
2
2σ2e
−∥v∥2
2
2σ2x
dv.
Rearrangement of the term in the exponent gives
∥HAsv −y∥2
2
2σ2e
+ ∥v∥2
2
2σ2x
= 1
2(v −hs)TQs(v −hs)
(11.33)
−1
2hT
s Qshs +
1
2σ2e
∥y∥2
2,
where
Qs = 1
σ2e
AT
s HTHAs + 1
σ2x
I
and
(11.34)
hs = 1
σ2e
Q−1
s AT
s HTy,
Qs is the very same matrix deﬁned earlier, and hs is the oracle solution for the
support s. Thus, plugging these into Equation (11.32) we obtain
P(y|s) ∝
Z
v∈IR
k exp
−∥HAsv −y∥2
2
2σ2e
−∥v∥2
2
2σ2x
dv
(11.35)
= exp
(1
2hT
s Qshs −
1
2σ2e
∥y∥2
2
)
·
Z
v∈IR
k exp
(
−1
2(v −hs)TQs(v −hs)
)
dv.
The integral in the above expression is over the complete volume of a multivariate
Gaussian distribution, apart from the missing constant coeﬃcient. Thus,
Z
v∈IR
k exp
(
−1
2(v −hs)TQs(v −hs)
)
dv =
q
(2π)k det(Q−1
s ).
(11.36)
This leads to the result
P(y|s) ∝exp
(1
2hT
s Qshs
)
·
q
det(Q−1
s )
(11.37)
= exp
(1
2hT
s Qshs + 1
2 log(det(Q−1
s ))
)
.
Notice that we have removed the terms exp
n
−∥y∥2
2/2σ2
e
o
and (2π)k, since both can
be absorbed into the proportionality constant, as we are only interested in the terms
that depend on the support s. To conclude, in order to compute the MMSE estimate,
we should compute the terms

11.5 The MMSE Estimation
215
qs = exp
(1
2hT
s Qshs + 1
2 log(det(Q−1
s ))
)
(11.38)
= exp
(yTHAsQ−1
s AT
s HTy
2σ4e
+ 1
2 log(det(Q−1
s ))
)
for all s ∈Ω, and normalize them to have a unit sum. The values obtained are in
fact P(s|y) to be used in the weighted sum in Equation (11.30) for the construc-
tion of the MMSE estimate. We should draw the reader’s attention to the term
yTHAsQ−1AT
s HTy in the above expression – this is the very same term used for
testing the supports in the MAP (see Equation (11.25)).
To summarize, the MMSE estimation is obtained by combining Equations (11.29)
and (11.30), and the deﬁnition of qs as given in Equation (11.38):
ˆxMMS E =
P
s∈Ωqs
 1
σ2e AT
s HTHAs + 1
σ2x I
−1 1
σ2e AT
s HTy
P
s∈Ωqs
.
(11.39)
Since we have seen that the MAP estimation admits a closed-form and cheap
solution for the unitary case, it is of interest to know if a similar claim can be made
with respect to the MMSE estimation. We shall not provide the analysis here, but
we do state that such a closed-form formula can and has been established (see recent
papers by Protter et. al. and Turek et. al.) for the MMSE estimation, avoiding the
combinatorial complexity altogether. Furthermore, under what we have deﬁned as
model G2 for generating x, the MMSE estimator takes the shape of a shrinkage
operation, just like the MAP. The diﬀerence between the two is thus only in the
shape of the shrinkage curve.
11.5.2 Approximating the MMSE Estimator
Whereas the exact MAP requires a sweep through all s ∈Ωin a search for the
most appropriate support, the MMSE requires a similar sweep, but for computing an
oracle estimate for each, and averaging these estimates with proper weights. Clearly,
the MMSE is even more complex than the MAP, and as infeasible as the MAP in
general, having an exponentially large possibilities to explore.
Just as we did for the MAP, we consider the simpliﬁed case of k = 1. In this case,
qi i = 1, 2, . . . , m in Equation (11.38) become
qi = exp
(yTHAsQ−1
s AT
s HTy
2σ4e
+ 1
2 log(det(Q−1
s ))
)
(11.40)
= exp

1
2σ2e
·
(˜aT
i y)2
∥˜ai∥2
2 + σ2e
σ2x
+ 1
2 log

∥˜ai∥2
2
σ2e
+ 1
σ2x

.

216
11 MAP versus MMSE Estimation
These can be computed and used for a weighted averaging of all the m cardinality
k = 1 possibilities. Assuming that the columns ˜ai are normalized, these weights are
proportional to exp(C(˜aT
i y)2), giving preference to atoms parallel to the measure-
ment vector.
An approximation algorithm for the MMSE can be proposed for the case of k > 1,
based on pruning of small qi. If we choose a few (say p ≪m) indices 1 ≤i ≤
m for which the value qi are the largest and omit the rest, we keep most of the
important elements in the weighted average. Turning to k = 2, rather than exploring
all
m
2

support possibilities, we explore only the p(m −1) options that correspond
to the p indices already chosen for k = 1, and for each of these we test the m −1
remaining atoms as candidate second ones. For these p(m−1) tests we have a quality
assessment attached, qs, as in Equation (11.38). Thus, we can again omit most of
the elements and choose the best p. The process than repeats for k = 3 and beyond,
and when we reach k we remain with a set of p chosen supports that were found
greedily to give high probability qs. A weighted average of these could replace the
exhaustive MMSE, approximating its outcome.
An alternative algorithm for approximating the MMSE for a general value of k
can be proposed based on randomization. This approach relies on a random sam-
pling of the supports s ∈Ωin order to draw a sub-set to average upon. For k = 1,
the qi (after their normalization) are the exact probabilities P(s|y) for supports with
cardinality |s| = 1. Rather than choosing the top ones deterministically, as proposed
above, we could draw indices at random with probabilities proportional to qi. Hav-
ing a set of such drawn indices, a plain average of the solutions for these indices is
an unbiased estimate for the MMSE weighted average.
The above process could in principle be done for |s| = k > 1 as well, randomly
drawing supports and averaging their oracle estimates. However, for the approxima-
tion of the average to be relatively accurate, we should use too-many random draws,
which makes this idea impractical. Instead, we may suggest to greedily accumulate
the drawn supports. For example, after choosing the ﬁrst atom i1 as described above,
we compute the m −1 probabilities qi = P(s = [i1, i]|y) for pairs that include the
chosen atom i1 as the ﬁrst item, and considering all the m −1 possibilities for i. Us-
ing this distribution, we can draw the second atom and this way choose i2. As this
process proceeds, we arrive at a support of cardinality k by k such random draws,
each from a set of O(m) probabilities (it is in fact m −j + 1 probabilities at the j-th
step).
The Random-OMP algorithm is a simpliﬁed version of the above procedure, fol-
lowing the same general structure of choosing the next atom based on the m scalar
probabilities qi. Starting with the ordinary OMP, we replace the deterministic mech-
anism for choosing the best atom to add to the support, by a random draw, just as
above. However, the probabilities qi are re-evaluated by Equation (11.40) after each
greedy step, considering the residual signal as the one to project onto the atoms in
A. Running this random algorithm several times, we obtain several (potentially) dif-
ferent supports and their oracle estimates. These are averaged (without weighting!!),
leading to the approximate MMSE.

11.5 The MMSE Estimation
217
Task: Approximate a MMSE estimate given in Equation (11.39).
Parameters: The pair (A, y), the cardinality |s|, the values of σe and σx, and
the number of runs J.
Main Iteration: Run the random algorithm below J times:
•
Initialization: Initialize k = 0, and set
–
The initial solution x0 = 0.
–
The initial residual r0 = y −Ax0 = y.
–
The initial solution support S0 = S upport{x0} = ∅.
•
Greedy Iteration: Increment k by 1 and perform the following steps |s|
times:
–
Random Draw: Draw j0 at random with probability proportional to
qi = exp

1
2σ2e ·
(aT
i rk−1)2
∥a∥2
2+ σ2e
σ2x
+ 1
2 log

∥ai∥2
2
σ2e +
1
σ2x
.
–
Update Support: Update the support by Sk = Sk−1 ∪{j0}.
–
Increment Solution: Compute xk, the minimizer of ∥Ax −y∥2
2 subject
to S upport{x} = Sk.
–
Update Residual: Compute rk = y −Axk.
•
Finalize the j-th Solution: Set s = Sk, the support detected, and compute
the solution using the oracle’s formula,
ˆxj =
 1
σ2e
AT
s As + 1
σ2x
I
!−1 1
σ2e
AT
s y.
Output: The proposed approximation is obtained as a simple average of the J
random results,
ˆx = 1
J
J
X
j=1
ˆxj.
Fig. 11.3 Random-OMP for computing the approximate MMSE estimation (assuming H = I for
simplicity).
One could regard the above two processes as approximate Gibbs samplers for the
supports that constitute the peaks of the probability function P(y|s). The Random-
OMP algorithm is depicted in Figure 11.3.
We return again to the experiment behind Figures 11.1 and 11.2, this time adding
MMSE performance curves. Using Equation (11.39), we compute ˆxMMS E and eval-
uate the error directly by ∥x −ˆxMMS E∥2
2. We also apply an approximation of the
MMSE by the Random-OMP, by running it 25 times and plain-averaging the results.
Since all the atoms in A have a unit norm, the random draw of the atoms within the
Random-OMP is proportional to exp(C(aT
i y)2), with 1/C = 2σ2
e(1 + σ2
e/σ2
x) – this
constant should be used by the Random-OMP function. Each run of the Random-
OMP is used to extract the support, for which an oracle estimate is generated. These
are averaged to obtained the ﬁnal result.

218
11 MAP versus MMSE Estimation
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
0
0.1
0.2
0.3
0.4
0.5
σe
Relative Mean−Squared−Error
 
 
Oracle (empirical)
Exact MAP (empirical)
Exact MMSE (empirical)
Approx. MMSE − RandOMP
k/n factor
Fig. 11.4 The exact (and thus exhaustive) and the approximated MMSE performance as a function
of the input noise. The approximation is obtained by averaging 25 runs of the Random-OMP.
Results are shown relative to the input noise energy nσ2
e.
The results of this experiment are brought in Figure 11.4, and we see that this ap-
proximation is quite close to the exhaustive MMSE, which has to sweep and average
the oracle estimates over all
m
k

= 4060 supports.
11.6 MMSE and MAP Errors
We have obtained expressions for the MAP and the MMSE estimates, and now we
target the development of similar expressions for the actual MSE these estimators
lead to. Our analysis starts with the error for a general estimate ˆx, observing that it
can be written as
E(∥ˆx −x∥2
2) =
Z
x∈IR
n ∥ˆx −x∥2
2P(x|y)dx
(11.41)
=
X
s∈Ω
P(s|y)
Z
x∈IR
n ∥ˆx −x∥2
2P(x|s, y)dx,
where we have used the marginalization of the posterior as proposed in Equation
(11.17).

11.6 MMSE and MAP Errors
219
The integral above can be rewritten diﬀerently by modifying the term ∥ˆx −x∥2
2,
adding and subtracting an oracle estimate ˆxoracle
s
that corresponds to the known sup-
port s. This is not to be confused with the ideal oracle estimate for the complete
estimation problem, for which we have to know the true support s∗that created the
measurements y. Thus, we use ∥ˆx −x∥2
2 = ∥ˆx −ˆxoracle
s
+ ˆxoracle
s
−x∥2
2, yielding
Z
x∈IR
n ∥ˆx −x∥2
2P(x|s, y)dx =
Z
x∈IR
n ∥ˆxoracle
s
−x∥2
2P(x|s, y)dx
(11.42)
+
Z
x∈IR
n ∥ˆx −ˆxoracle
s
∥2
2P(x|s, y)dx.
Note that the integral over the cross-term (ˆxoracle
s
−x)T(ˆx−ˆxoracle
s
) vanishes, since the
term (ˆx−ˆxoracle
s
) is deterministic and can thus be moved outside the integration. The
remaining expression in the integral is zero, since ˆxoracle
s
= E(x|y, s) by deﬁnition.
Returning to Equation (11.42), the ﬁrst term represents an oracle MSE for a given
support s, and this has been already given in Equation (11.16) as
E
ˆxoracle
s
−xs
2
2

= trace

" 1
σ2e
AT
s HTHAs + 1
σ2x
I
#−1= trace

Q−1
s

.
The second term is even simpler: Since ∥ˆx −ˆxoracle
s
∥2
2 is deterministic, it is moved
outside the integration, and the remaining part is simply 1. Thus,
Z
x∈IR
n ∥ˆx −x∥2
2P(x|s, y)dx = ∥ˆx −ˆxoracle
s
∥2
2 + trace

Q−1
s

.
Using the fact that P(s|y) ∝qs as developed in Equation (11.38), we have
E(∥ˆx −x∥2
2) =
1
P
s∈Ωqs
·
X
s∈Ω
qs ·
h
∥ˆx −ˆxoracle
s
∥2
2
(11.43)
+ trace

Q−1
s
i
.
By plugging into this expression ˆx = ˆxMMS E we get the MMSE error. Interestingly,
if we minimize the above expression with respect to ˆx we obtain the MMSE estima-
tor formula as given in Equation (11.39), since nulling the derivative of the above
expression with respect to ˆx gives
∂E(∥ˆx −x∥2
2)
∂ˆx
= 2
P
s∈Ωqs(ˆx −ˆxoracle
s
)
P
s∈Ωqs
= 0
⇒
ˆxMMS E =
P
s∈Ωqsˆxoracle
s
P
s∈Ωqs
.
As an interesting example, for the case where H = I and A is unitary, the term
AT
s HTHAs is an identity matrix of size |s| = k, and thus trace

Q−1
s

= kσ2
xσ2
e/(σ2
x +
σ2
e) is constant for any s ∈Ω. Turning to the value of qs, we observe that for our

220
11 MAP versus MMSE Estimation
case qs ∝exp(C · ∥βs∥2
2/2σ2
e), where βs = AT
s y is a projection of y onto the columns
of the support s, and the constant is C = σ2
x/(σ2
e + σ2
x). Finally, the s-oracle is given
by ˆxoracle
s
= σ2
xβs/(σ2
e + σ2
x) = Cβs. Combining all these we get
E(∥ˆx −x∥2
2) =
P
s∈Ωexp

C·∥βs∥2
2
2σ2e

· ∥ˆx −cβs∥2
2
P
s∈Ωexp

C·∥βs∥2
2
2σ2e

+ kσ2
xσ2
e
σ2x + σ2e
.
(11.44)
Returning to the general case, we observe that the general error expression found
in Equation (11.43) can be written diﬀerently by adding and subtracting ˆxMMS E,
giving
E(∥ˆx −x∥2
2) =
P
s∈Ωqs ·
h
∥ˆx −ˆxoracle
s
∥2
2 + trace

Q−1
s
i
P
s∈Ωqs
(11.45)
=
P
s∈Ωqs
h
∥ˆx −ˆxMMS E + ˆxMMS E −ˆxoracle
s
∥2
2 + trace

Q−1
s
i
P
s∈Ωqs
= ∥ˆx −ˆxMMS E∥2
2
+
P
s∈Ωqs ·
h
∥ˆxMMS E −ˆxoracle
s
∥2
2 + trace

Q−1
s
i
P
s∈Ωqs
= ∥ˆx −ˆxMMS E∥2
2 + E(∥ˆxMMS E −x∥2
2).
Again, the cross term (ˆx −ˆxMMS E)T(ˆxMMS E −ˆxoracle
s
) drops from the above expres-
sion since the ﬁrst part can be moved outside the summation, and the second part
vanishes due to the formula of ˆxMMS E as a linear combination of oracle estimates.
In particular, this means that
E(∥ˆxMAP −x∥2
2) = ∥ˆxMAP −ˆxMMS E∥2
2
(11.46)
+E(∥ˆxMMS E −x∥2
2).
In Figure 11.5 we show the tendency of the formulae developed here for the
MMSE and the MAP to describe accurately the empirical performance of these
estimators. The ﬁgure shows the empirical results as in Figures 11.2 and 11.4, and
on top the expected error is manifested for the MMSE in Equation (11.43) and for
the MAP as in Equation (11.46).
11.7 More Experimental Results
All the simulations describes thus far correspond to a very-low dimensional prob-
lem, for which the exact and exhaustive estimators can be computed in reasonable
time. We now turn to a very similar simulation in higher dimension. We use a ran-
dom dictionary, just as before, but with size 200 × 400. We set the cardinality of

11.7 More Experimental Results
221
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
0
0.1
0.2
0.3
0.4
0.5
σe
Relative Mean−Squared−Error
 
 
Oracle (empirical)
Exact MAP (empirical)
Exact MAP (formula)
Exact MMSE (empirical)
Exact MMSE (formula)
k/n factor
Fig. 11.5 The MAP and the MMSE performance, presented both by their empirical evaluations,
and by the formulae using the expected error formulae (11.43) and (11.46). Results are shown
relative to the input noise energy nσ2
e.
the unknown representation to be k = 20. Figure 11.6 shows the performance of
the OMP and the Random-OMP (with 25 runs averaged), varying the additive noise
variance, and averaging the results over 100 independent experiments. We see that
the results resemble those that we have observed in the low-dimensional tests.
This ﬁgure contains a third and new estimator, referred to as the “Sparsiﬁed
Random-OMP”. This estimate is obtained by taking the largest k entries in the
Random-OMP solution, and deﬁning this as the support of interest. Computing an
oracle’s estimate over this support, we obtained this proposed estimation. In some
problems, we desire to get an ℓ2 accurate estimation, while also providing sparsity.
This for example is the case in compression or recognition tasks.
We see that this approach gives performance somewhere between the previous
two (OMP and Random-OMP). The fact that it loses in quality compared to the
Random-OMP is quite expected, bearing in mind that the Random-OMP is a rel-
atively good approximation of the MMSE estimation, one can oﬀer no alternative
that outperforms it. We also see that this estimator tends to be better than the OMP,
indicating that this approach is not the MAP estimator in disguise.
How many random runs should be applied to enable the Random-OMP to get
close to the MMSE estimate? Figure 11.7 shows the performance obtained for vary-
ing number of such runs, using the same experiment settings as above. As can be
observed, even 4 runs are suﬃcient to get quite close to the desired results.

222
11 MAP versus MMSE Estimation
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
0.45
0.5
σe
Relative Mean−Squared−Error
 
 
Oracle (empirical)
Approx. MAP by OMP
Approx. MMSE by RandOMP
Sparsified RandOMP
k/n factor
Fig. 11.6 The performance curves of the OMP (approximate MAP), the Random-OMP (approx-
imate MMSE), and a sparsiﬁed version of the Random-OMP, as a function of the additive noise
variance. Results are shown relative to nσ2
e.
0.2
0.4
0.6
0.8
1
1.2
1.4
1.6
1.8
2
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
0.4
σe
Random−OMP Relative Mean−Squared−Error
 
 
J=1
J=2
J=4
J=8
J=16
J=32
Fig. 11.7 Random-OMP performance with varying number of random runs averaged.

11.7 More Experimental Results
223
0
50
100
150
200
250
300
350
400
−2
−1.5
−1
−0.5
0
0.5
1
1.5
index
Amplitude
 
 
Random−OMP Result
x0
Fig. 11.8 The Random-OMP result (J = 100) compared to the true representation x0.
Our last test focuses on the Random-OMP, and in particular on its tendency to
provide a non-sparse solution. We perform a single estimation test, similar to the
ones appearing above – the dictionary used is an over-complete DCT matrix of size
200 × 400, obtained by taking every second row from a unitary DCT matrix of size
400×400. The chosen parameters are k = 10, σe = 0.2, σx = 1, and J = 100. Figure
11.8 shows the Random-OMP result, and as expected, while the result tends to be
close to the original representation x0, it is not sparse, due to the massive averaging
that takes place in its computation. Figure 11.9 further investigates this behavior by
presenting the sorted absolute values of these two representations, showing clearly
that the Random-OMP solution tends to be dense. This tendency is further strength-
ened if the noise is stronger, or when the dictionary’s coherence is bad.
The fact that the Random-OMP (and indeed, the MMSE it approximates) gives
a non-sparse solution brings us back to the issue raised towards the end of Chapter
10. We have seen there that the best λ (in terms of MSE) leads to a dense represen-
tation solution. We can now explain this very simply as being an approximation of
the MMSE, which is also not expected to be sparse. Furthermore, and more broadly,
even if the unknown we seek to estimate is sparse, and even with a known cardinal-
ity, this does not mean that its MMSE estimate is going to be sparse. An interesting
question with this regard is what is the k-sparse MMSE estimate for our problem?
Is it similar to the sparsiﬁed Random-OMP we have introduced? This remains as an
open question at the moment.

224
11 MAP versus MMSE Estimation
50
100
150
200
250
300
350
400
10
−10
10
−8
10
−6
10
−4
10
−2
10
0
Sorted index
Absolute Amplitude
 
 
x0
Random−OMP Result
Fig. 11.9 Sorted absolute value of the entries in the Random-OMP result and the true representa-
tion x0.
11.8 Summary
In the context of estimation theory, the Sparse-Land model leads to an optimiza-
tion problem of the form deﬁned as Pϵ
0, for the MAP estimator. This chapter clearly
mapped this relationship, making it precise. A far more important achievement of
the adopted estimation point of view, is the ability to consider the MMSE estima-
tion. We have seen that while both these estimators lead to exponentially growing
amounts of computations, reasonable approximations of them are within reach. This
analysis gives a more solid foundation for the quest for the sparsest representation
and the use of pursuit techniques in signal processing applications.
Further Reading
1. F. Abramovich, T. Sapatinas and B.W. Silverman, Wavelet thresholding via a
Bayesian approach, J. R. Statist. Soc. B, 60:725–749, 1998.
2. A. Antoniadis, J. Bigot, and T. Sapatinas, Wavelet estimators in nonparametric
regression: a comparative simulation study, J. Stat. Software, 6(6):1–83, 2001.

Further Reading
225
3. M. Clyde and E.I. George, Empirical Bayes estimation in wavelet nonparamet-
ric regression. In Bayesian Inference in Wavelet Based Models, P. Muller and
B. Vidakovic (Eds.), Lect. Notes Statist., 141:309–322, New York, Springer-
Verlag, 1998.
4. M. Clyde and E.I. George, Flexible empirical Bayes estimation for wavelets, J.
R. Statist. Soc. B, 62:681–698, 2000.
5. M. Clyde, G. Parmigiani and B. Vidakovic, Multiple shrinkage and subset se-
lection in wavelets, Biometrika, 85:391–401, 1998.
6. M. Elad and I. Yavneh, A weighted average of sparse representations is bet-
ter than the sparsest one alone, IEEE Transactions on Information Theory,
55(10):4701–4714, October 2009.
7. J. Turek, I. Yavneh, M. Protter, and M. Elad, On MMSE and MAP denoising
under sparse representation modeling over a unitary dictionary, submitted to
Applied Computational Harmonic Analysis.
8. E. Larsson and Y. Selen, Linear regression with a sparse parameter vector, IEEE
Transactions on Signal Processing, 55:451–460, 2007.
9. S. Mallat and Z. Zhang, Matching pursuits with time-frequency dictionaries,
IEEE Trans. on Signal Processing, 41(12):3397–3415, 1993.
10. P. Moulin and J. Liu, Analysis of multiresolution image denoising schemes
using generalized Gaussian and complexity priors, IEEE Trans. Inf. Theory,
45(3):909–919, April 1999.
11. M. Protter, I. Yavneh and M. Elad, Closed-form MMSE for denoising signals
under sparse-representation modelling, The IEEE 25-th Convention of Electri-
cal and Electronics Engineers in Israel, Eilat Israel, December 3-5, 2008.
12. M. Protter, I. Yavneh and M. Elad, Closed-Form MMSE estimation for signal
denoising under sparse representation modelling over a unitary dictionary, sub-
mitted to IEEE Trans. on Signal Processing.
13. E.P. Simoncelli and E.H. Adelson, Noise removal via Bayesian wavelet coring,
in Proc. ICIP, Laussanne, Switzerland, pp. 379-382, September 1996.
14. P. Schnitter, L. C. Potter, and J. Ziniel, Fast Bayesian matching pursuit, Proc.
Workshop on Information Theory and Applications (ITA), (La Jolla, CA), Jan.
2008.
15. P. Schintter, L.C. Potter, and J. Ziniel, Fast Bayesian matching pursuit: Model
uncertainty and parameter estimation for sparse linear models, submitted to
IEEE Transactions on Signal Processing.

Chapter 12
The Quest for a Dictionary
A fundamental ingredient in the deﬁnition of Sparse-Land’s signals and its deploy-
ment to applications is the dictionary A. How can we wisely choose A to perform
well on the signals in question? This is the topic of this chapter, and our emphasis
is put on learning methods for dictionaries, based on a group of examples.
12.1 Choosing versus Learning
In the quest for the proper dictionary to use in applications, one line of work con-
siders choosing pre-constructed dictionaries, such as undecimated wavelets (as in
Chapter 10), steerable wavelets, contourlets, curvelets, and more. Many of these
recently proposed dictionaries are tailored speciﬁcally to images, and in particular
to stylized ‘cartoon-like’ image content, assumed to be piecewise smooth and with
smooth boundaries.
Some of these proposed dictionaries (which are often referred to also as trans-
forms) are accompanied by a detailed theoretical analysis establishing the sparsity of
the representation coeﬃcients for such simpliﬁed content of signals. This is typically
done in terms of the rate of decay of the M-term approximation – the representation
of the signal using the best M non-zeros from the transform coeﬃcients.
Alternatively, one can use a tunable selection of a dictionary, in which a basis or
Frame is generated under the control of a particular parameter (discrete or contin-
uous). The two most known examples that belong to this category are the wavelet
packets and the bandelets. The wavelet packets, due to Coifman at. al., suggest a
control over the time-frequency subdivision, tuned to optimize performance over
a speciﬁed instance of a signal. The bandelets, due to Mallat et al., are spatially
adapted to better treat images, by orienting the regular wavelet atoms along the
principal direction of the local area processed.
While pre-constructed or adapted dictionaries typically lead to fast transforms
(of complexity O(n log n) instead of nm in a direct use), they are typically limited in
their ability to sparsify the signals they are designed to handle. Furthermore, most
© Springer Science+Business Media, LLC 2010
M. Elad, Sparse and Redundant Representations: From Theory to Applications in Signal 
and Image Processing, DOI 10.1007/978-1-4419-7011-4_12,
227

228
12 The Quest for a Dictionary
of those dictionaries are restricted to signals/images of a certain type, and cannot be
used for a new and arbitrary family of signals of interest. This leads us to yet another
approach for obtaining dictionaries that overcomes these limitations – by adopting
a learning point-of-view.
This learning option starts by building a training database of signal instances,
similar to those anticipated in the application, and constructing an empirically-
learned dictionary, in which the generating atoms come from the underlying em-
pirical data, rather than from some theoretical model. Such a dictionary can then be
used in the application as a ﬁxed and redundant dictionary. We shall explore this
third option in more detail in this chapter.
As opposed to the pre-constructed and adapted dictionaries, the learning method
is able to adapt to any family of signals that complies with the Sparse-Land model.
However, this comes at the price of a much higher computational load – learned
dictionaries are held as explicit structure-less matrices, and using them in applica-
tions involves many more computations, compared to the pre-constructed dictio-
naries. Another shortcoming of the training methodology is its restriction1 to low-
dimensional signals. This is why handling of images is done with such dictionaries
on small patches. We will discuss ways to overcome these two limitations later in
this chapter.
12.2 Dictionary-Learning Algorithms
We now turn to discuss the learning methodology for constructing A. Assume that a
training database {yi}M
i=1 is given, and thought to have been generated by some ﬁxed
but unknown model M{A,k0,α,ϵ}. Can this training database allow us to identify the
generating model, and speciﬁcally the dictionary A? This rather diﬃcult problem
has been studied initially by Field and Olshausen in 1996, who were motivated by
an analogy between the atoms of a dictionary and the population of simple cells in
the visual cortex. They thought that learning a dictionary empirically might model
evolutionary processes that led to the existing collection of simple cells; and they
indeed were able to ﬁnd a rough empirical match between the properties of a learned
dictionary and some known properties of the population of simple cells.
Later work by Lewicki, Engan, Rao, Gribonval, Aharon, and others, extended
their methodology and algorithm in various ways. Here we describe two training
mechanisms, the ﬁrst named Method of Optimal Directions (MOD) by Engan et al.,
and the second named K-SVD, by Aharon et al..
1 At least for now. Work on the construction of learned multi-scale dictionaries is underway, and
hopefully this shortcoming should be removed in the near future.

12.2 Dictionary-Learning Algorithms
229
12.2.1 Core Questions in Dictionary-Learning
Assume that ϵ – the model deviation – is known, and our aim is the estimation of A.
Consider the following optimization problem:
min
A,{xi}M
i=1
M
X
i=1
∥xi∥0
subject to ∥yi −Axi∥2 ≤ϵ, 1 ≤i ≤M.
(12.1)
This problem describes each given signal yi as the sparsest representation xi over
the unknown dictionary A, and aims to jointly ﬁnd the proper representations and
the dictionary. Clearly, if a solution has been found such that every representation
has k0 or fewer non-zero entries, a candidate feasible model M has been found.
The roles of the penalty and the constraints in Equation (12.1) might also be
reversed, if we choose to constrain the sparsity and obtain the best ﬁt for it,
min
A,{xi}M
i=1
M
X
i=1
∥yi −Axi∥2
2
subject to ∥xi∥0 ≤k0, 1 ≤i ≤M.
(12.2)
Are these two problems properly posed? Do they have meaningful solutions? There
are some obvious indeterminacies (scaling and permutation of the columns). Still,
if we ﬁx a scale and ordering, is there a meaningful solution to the problems posed
in Equations (12.1) and (12.2) in general? As we show next, the answers to these
questions are positive.
In terms of well-posedness of the dictionary-learning problem presented above,
a fundamental question is whether there is a uniqueness property underlying this
problem, implying that only one dictionary exists, that sparsely explains the set of
training vectors. Surprisingly, at least for the case of ϵ = 0, there is an answer to
this question, as shown by Aharon et al.. Suppose there exists a dictionary A0 and
a suﬃciently diverse database of examples, all of which are representable using at
most k0 < spark(A0)/2 atoms. Then, up to a re-scaling and permutation of the
columns, A0 is the unique dictionary that achieves this sparsity for all the elements
in the training database.
Some readers may prefer to think in terms of matrix factorizations. Concatenate
all the database vectors column-wise, forming an n × M matrix Y, and similarly, all
the corresponding sparse representations into a matrix X of size m× M; thus the dic-
tionary satisﬁes Y = AX in the noiseless case. The problem of discovering an under-
lying dictionary is thus the same as the problem of discovering a factorization of the
matrix Y as AX where A and X have the indicated sizes, and X has sparse columns.
The matrix factorization viewpoint connects this problem with related problems of
nonnegative matrix factorization and in particular, sparse nonnegative matrix factor-
ization.

230
12 The Quest for a Dictionary
12.2.2 The MOD Algorithm
Clearly, there is no general practical algorithm for solving problem (12.1) or (12.2),
for the same reasons that there is no general practical algorithm for solving (P0),
only more so. However, just as with (P0), the lack of general guarantees is no reason
not to try heuristic methods and see how they perform in speciﬁc cases.
We can view the problem posed in Equation (12.1) as a nested minimization
problem: an inner minimization of the number of nonzeros in the representation
vectors xi, for a given ﬁxed A and an outer minimization over A. A strategy of alter-
nating minimization thus seems very natural; at the k-th step, we use the dictionary
A(k−1) from the k −1-th step and solve M instances of (Pϵ
0), one for each database
entry yi, and each using the dictionary A(k−1). This gives us the matrix X(k), and we
then solve for A(k) by Least-Squares,
A(k) = arg min
A
∥Y −AX(k)∥2
F
(12.3)
= YXT
(k)

X(k)XT
(k)
−1
= YX+
(k),
where we evaluate the error using a Frobenius norm. We may also re-scale the
columns of the obtained dictionary. We increment k and unless we have satis-
ﬁed a convergence criterion, we repeat the above loop. Such a Block-Coordinate-
Relaxation algorithm has been proposed by Engan et al., and termed Method of
Optimal Directions (MOD). This algorithm is described in Figure 12.1.
Figure 12.2 demonstrates the MOD for a synthetic experiment. A random dic-
tionary (iid Gaussian entries, normalized columns) of size 30 × 60 is generated,
and from it we produce 4, 000 signal examples, each by a random combination of
4 atoms, with coeﬃcients drawn from the normal distribution N(0, 1). Each such
signal is further contaminated by a random zero-mean Gaussian noise with σ = 0.1.
This implies a signal-to-noise ratio of ≈8 in this experiment. Performing 50 itera-
tions of the MOD on this set of signals, we attempt to recover the original dictionary.
The MOD is used with a ﬁxed cardinality k0 = 4, and initialized by adopting the
ﬁrst 60 examples as the dictionary atoms. Figure 12.2 (right) shows the average
representation error as a function of the iteration.
Since this is a synthetic experiment, where the ground-truth dictionary is known,
we can evaluate how close the estimated dictionary is to its objective. Figure 12.2
(left) presents a graph of the relative count of recovered atoms. An atom ai from
the true dictionary is considered as recovered if it ﬁnds a match ˆa j in the estimated
dictionary, such that |aT
i ˆaj| > 0.99. Note that if the estimated atoms were to be
contaminated by the same amount of noise as the training data (i.e., ˆaj = ai + ej
with ej ∼N(0, σ2I)), the variance of this inner-product would have been
E

(aT
i ˆa j −aT
i ai)2
= aT
i E

ejeT
j

ai = σ2 = 0.01,

12.2 Dictionary-Learning Algorithms
231
Task: Train a dictionary A to sparsely represent the data {yi}M
i=1, by approxi-
mating the solution to the problem posed in Equation (12.2).
Initialization: Initialize k = 0, and
•
Initialize Dictionary: Build A(0) ∈IRn×m, either by using random entries,
or using m randomly chosen examples.
•
Normalization: Normalize the columns of A(0).
Main Iteration: Increment k by 1, and apply
•
Sparse Coding Stage: Use a pursuit algorithm to approximate the solution
of
ˆxi = arg min
x
∥yi −A(k−1)x∥2
2
subject to ∥x∥0 ≤k0.
obtaining sparse representations ˆxi for 1 ≤i ≤M. These form the matrix
X(k).
•
MOD Dictionary Update Stage: Update the dictionary by the formula
A(k) = arg min
A
∥Y −AX(k)∥2
F = YXT
(k)

X(k)XT
(k)
−1 .
•
Stopping Rule: If the change in ∥Y −A(k)X(k)∥2
F is small enough, stop.
Otherwise, apply another iteration.
Output: The desired result is A(k).
Fig. 12.1 The MOD dictionary-learning algorithm.
where we have exploited the fact that aT
i ai = 1. Thus, requiring a standard deviation
of 0.01 in these inner products is 10 times smaller than the expected ﬂuctuation, and
as such, it is quite demanding.
Both graphs show that MOD performs very well on this test, recovering most of
the atoms in the original dictionary, and doing so in a matter of 20 −30 iterations.
Moreover, using 4 atoms per each example, MOD manages to get to an average
representation error which is below 0.1 – the noise level. This means that the found
dictionary is a feasible solution to our problem.
12.2.3 The K-SVD Algorithm
A diﬀerent update rule for the dictionary can be proposed, in which the atoms (i.e.,
columns) in A are handled sequentially. This leads to the K-SVD algorithm, as
developed by Aharon et al. Keeping all the columns ﬁxed apart from the j0-th one,
aj0, this column can be updated along with the coeﬃcients that multiply it in X. We
isolate the dependency on a j0 by rewriting (12.3) as 2
2 To simplify notation, we now omit the iteration number k.

232
12 The Quest for a Dictionary
0
10
20
30
40
50
0
10
20
30
40
50
60
70
80
90
Iteration
Relative # of Recovered Atoms
0
10
20
30
40
50
0.08
0.1
0.12
0.14
0.16
0.18
0.2
0.22
Iteration
Average Representation Error
Fig. 12.2 The MOD behavior in a synthetic experiment. The left graph presents the relative num-
ber (in %) of correctly recovered atoms, and the right one shows the average representation error.
∥Y −AX∥2
F =

Y −
m
X
j=1
a jxT
j

2
F
(12.4)
=

Y −
X
j, j0
a jxT
j
−a j0xT
j0

2
F
.
In this description, xT
j stands for the j-th row of X. The update step targets both aj0
and xT
j0, and refers to the term in parentheses,
E j0 = Y −
X
j, j0
a jxT
j
(12.5)
as a known pre-computed error matrix.
The optimal aj0 and xT
j0 minimizing Equation (12.4) are the rank-1 approximation
of E j0, and can be obtained via an SVD, but this typically would yield a dense vector
xT
j0, implying that we increase the number of non-zeros in the representations in X.
In order to minimize this term while keeping the cardinalities of all the represen-
tations ﬁxed, a subset of the columns of E j0 should be taken – those that correspond
to the signals from the example-set that are using the j0-th atom, namely those
columns where the entries in the row xT
j0 are non-zero. This way, we allow only the
existing non-zero coeﬃcients in xT
j0 to vary, and the cardinalities are preserved.
Therefore, we deﬁne a restriction operator, Pj0, that multiplies Ej0 from the right
to remove the non-relevant columns. The matrix Pj0 has M rows (the number of
overall examples), and Mj0 columns (the number of examples using the j0-th atom).
We deﬁne (xR
j0)T = xT
j0Pj0 as the restriction on the row xT
j0, choosing the non-zero
entries only.
For the sub-matrix, Ej0Pj0, a rank-1 approximation via SVD can be applied, up-
dating both the atom a j0 and the corresponding coeﬃcients in the sparse represen-

12.2 Dictionary-Learning Algorithms
233
tations, xR
j0. This simultaneous update may lead to a substantial speedup in the con-
vergence of the training algorithm. Figure 12.3 describes the K-SVD algorithm in
detail.
Note that one need not use the full SVD as only the ﬁrst rank part of Ej0Pj0 is
required. An alternative numerical approach for ﬁnding a j0 and xR
j0 can be proposed,
following the same block-coordinate rationale: Fixing a j0, we can update xR
j0 by a
plain Least-Squares that solving
min
xR
j0
Ej0Pj0 −aj0(xR
j0)T2
F
⇒
xR
j0 =
PT
j0ET
j0aj0
∥a j0∥2
2
.
Once updated, it is kept ﬁxed, and we update a j0 by
min
a j0
Ej0Pj0 −aj0(xR
j0)T2
F
⇒
aj0 =
E j0Pj0xR
j0
∥xR
j0∥2
2
.
Few such rounds of updates for these two unknowns is suﬃcient for getting the
desired dictionary update.
Interestingly, if the above process is considered for the case where k0 = 1, and
constraining the representation coeﬃcients to be binary (1 or 0), this problem re-
duces to a simple clustering task. Furthermore, in such a case, both the above train-
ing algorithms simplify to become the well-known K-means algorithm. While each
iteration of K-means computes the means of K diﬀerent subsets, the K-SVD algo-
rithm performs an SVD for each of the K diﬀerent sub-matrices, and thus the name
K-SVD (K is used as the number of columns in A).
The fact that the dictionary-learning problem is a generalization of clustering
reveals more on the learning task considered here:
• Just as for the clustering problem and the K-Means which aims to solve it, we
cannot guarantee that the MOD and K-SVD algorithms obtain the global min-
imum of the penalty function posed in Equation (12.2). Indeed, even a local
minimum solution cannot be guaranteed, as these algorithms may get stuck on a
saddle-point steady-state solution.
• Still on the matter of convergence, since the pursuit applied in the sparse coding
stage may get to a sub-optimal solution, we cannot guarantee a monotonic non-
increasing penalty value as a function of the iterations. However, we can modify
the algorithm such that after the pursuit we adopt only those examples that show
a descent in the representation error, thus guaranteeing an overall descent.
• The similarity between the K-Means and the K-SVD (and MOD) enables us
to exploit the vast empirical experience accumulated on ways to accelerate the
K-Means. These techniques include a gradual change in the number of atoms
within the learning procedure, a similar change in the number of atoms to use per
example, a multi-scale approach, where the examples are represented by multi-
scale pyramids, and the training is done on lower-dimensions ﬁrst, and more. We
shall not dwell on these techniques here.

234
12 The Quest for a Dictionary
Task: Train a dictionary A to sparsely represent the data {yi}M
i=1, by approxi-
mating the solution to the problem posed in Equation (12.2).
Initialization: Initialize k = 0, and
•
Initialize Dictionary: Build A(0) ∈IRn×m, either by using random entries,
or using m randomly chosen examples.
•
Normalization: Normalize the columns of A(0).
Main Iteration: Increment k by 1, and apply
•
Sparse Coding Stage: Use a pursuit algorithm to approximate the solution
of
ˆxi = arg min
x
∥yi −A(k−1)x∥2
2
subject to ∥x∥0 ≤k0.
obtaining sparse representations ˆxi for 1 ≤i ≤M. These form the matrix
X(k).
•
K-SVD Dictionary-Update Stage: Use the following procedure to update
the columns of the dictionary and obtain A(k): Repeat for j0 = 1, 2, . . . , m
–
Deﬁne the group of examples that use the atom aj0,
Ωj0 = {i| 1 ≤i ≤M, X(k)[j0, i] , 0}.
–
Compute the residual matrix
Ej0 = Y −
X
j,j0
ajxT
j ,
where xj are the j’th rows in the matrix X(k).
–
Restrict E j0 by choosing only the columns corresponding to Ωj0, and
obtain ER
j0.
–
Apply SVD decomposition ER
j0 = U∆VT. Update the dictionary atom
a j0 = u1, and the representations by xR
j0 = ∆[1, 1] · v1.
•
Stopping Rule: If the change in ∥Y −A(k)X(k)∥2
F is small enough, stop.
Otherwise, apply another iteration.
Output: The desired result is A(k).
Fig. 12.3 The K-SVD dictionary-learning algorithm.
Figure 12.4 presents the results obtained with the K-SVD for the synthetic exper-
iment described in the previous section. For both algorithms we add a correction
stage after the dictionary update, in which, if an atom in the dictionary is rarely
used, or if it is found to be too similar to another atom in the dictionary, it is re-
placed by the worst-represented example.
As can be seen, in this experiment the K-SVD gives slightly better results, com-
pared to the MOD, both in terms of the ﬁnal outcome (100% dictionary recovery
with an average representation error of 0.09), and the speed of convergence. Gener-

12.2 Dictionary-Learning Algorithms
235
0
10
20
30
40
50
0
10
20
30
40
50
60
70
80
90
100
Iteration
Relative # of Recovered Atoms
 
 
MOD
K−SVD
0
10
20
30
40
50
0.08
0.1
0.12
0.14
0.16
0.18
0.2
0.22
Iteration
Average Representation Error
 
 
MOD
K−SVD
Fig. 12.4
The K-SVD behavior in a synthetic experiment. The left graph presents the relative
number (in %) of correctly recovered atoms, and the right one shows the average representation
error.
ally speaking, these two algorithms behave similarly and comparably, with a small
advantage to the K-SVD.
We now turn to present an elementary experiment performed on real data. We
train a dictionary for sparsely representing patches of size 8 × 8 extracted from the
image Barbara, shown in Figure 12.5. This image is of size 512×512, implying that
there are (512−7)2 = 255, 025 possible patches, considering all overlaps. We extract
one tenth of these patches (uniformly spread) to train on, using both the MOD and
the K-SVD with 50 iterations. In both cases, we set the target cardinality to k0 = 4
atoms per patch.
Both algorithms are initialized with a 2D separable DCT dictionary of size 64 ×
121. This dictionary is created by forming ﬁrst a 1D-DCT matrix A1D of size 8×11,
where the k-th atom (k = 1, 2, . . . , 11) is given by a1D
k
= cos((i −1)(k −1)π/11), i =
1, 2, . . . , 8. All atoms apart from the ﬁrst are further processed by removing their
mean. The ﬁnal dictionary is obtained by a Kronecker-product A2D = A1D ⊗A1D,
which implies separability of this matrix when operating on a 2D image-patch.
Figure 12.6 presents the average representation error on the training set for both
algorithms. We see that even though the DCT is considered to be a good dictionary
for sparsely representing image patches, both algorithms improve the error substan-
tially beyond its initial value. The MOD and the K-SVD perform comparably in this
test, leading to almost the same error. The DCT and the two resulting dictionaries
are presented in Figure 12.7.
Both learning procedures lead to a a dictionary that consists of piecewise smooth
atoms and textured ones, because of the nature of the image trained on. One may
be tempted to believe that the MOD and K-SVD dictionaries are identical or nearly
so, however, using the same procedure mentioned above for measuring the distance
between the two dictionaries, we ﬁnd that there is an overlap of only ≈14% of the
atoms, the rest being considered as diﬀerent (implying that their cross-inner product
is below 0.99).

236
12 The Quest for a Dictionary
Fig. 12.5 The original image Barbara, on which the dictionary training is evaluated.
0
10
20
30
40
50
7.5
8
8.5
9
9.5
10
10.5
11
Iteration
Average Representation Error
 
 
MOD
K−SVD
Fig. 12.6 The average representation error on the training set for the MOD and the K-SVD algo-
rithms, as a function of the iteration.
Since we train the dictionaries using a small fraction of the image patches avail-
able to us, a natural question to pose is whether the found dictionaries sparsify the
rest of the patches as well. Performing OMP on all the patches, the average repre-
sentation error obtained with the DCT dictionary is 10.97, the MOD dictionary gives
an error of 7.82, and the K-SVD leads to an error of 7.80. Both trained dictionaries
lead to an error very close to the errors observed on the training set and far better

12.3 Training Structured Dictionaries
237
Fig. 12.7
The DCT (top), the MOD (bottom left) and the K-SVD (bottom right) dictionaries,
trained using 8 × 8 patches from the image Barbara.
than the DCT, implying that the sparsiﬁcation property is generalized well for the
other patches in the image.
12.3 Training Structured Dictionaries
So far the discussion has concentrated on the core dictionary learning problem and
two of its possible solutions. We should remind the reader that, along side with the
elegance and success of this methodology, there are also important shortcomings to
it, and these include:
• Speed and Memory Problems: The trained dictionary A ∈IRn×m is held and
used as an explicit matrix. When plugged to applications, each multiplication of
the form Ax or ATy requires nm operations. In comparison, structured dictionar-
ies require much less computation. For example, a separable 2D dictionary of the

238
12 The Quest for a Dictionary
same size3 requires 2n √m. As another example, a wavelet dictionary requires
only O(m) operations. Thus, the explicit storage and use of learned dictionar-
ies as described above is much less eﬃcient, compared to traditional transform
methods.
• Restriction to Low-Dimensions: The learning process shown above is restricted
to low-dimensional signals, i.e., n ≤1000 is a reasonable limit not to be sur-
passed. An attempt to go beyond this raises a series of problems, starting with a
need for an intolerable amount of training data, a very slow learning procedure,
and a risk of over-ﬁtting because of the too many free parameters in the sought
dictionary. These problems pose a hard restriction on how learned dictionaries
should be used in practice. For example, in a series of image processing algo-
rithms (some of them shown in later chapters), learned dictionaries are used on
small image patches, rather than on the whole image.
• Operating on a Single-Scale: Learned dictionaries as obtained by the MOD
and the K-SVD operate on signals by considering only their native scale. Past
experience with the wavelet transform teaches us that often times it is beneﬁcial
to process the signal in several scales, and operate on each scale diﬀerently. This
shortcoming is related to the above mentioned limits on the dimensionality of the
signals involved (which does not leave much freedom for multi-scale processing)
and the speed issue (multi-scale dictionaries are potentially faster to multiply
with).
• Lack of Invariances: In some applications we desire the dictionary we compose
to have speciﬁc invariance properties. The most classical examples are shift-,
rotation-, and scale-invariances. These imply that when the dictionary is used on
a shifted/rotated/scaled version of an image, we expect the sparse representation
obtained to be tightly related to the representation of the original image. Injecting
these invariance properties to dictionary-learning is therefore valuable, and the
above methodology has not addressed this matter.
All the above shortcomings reﬂect properties that exist in pre-designed dictionaries,
and it would be advantageous to transfer these to learned dictionaries as well, thus
creating a more appealing alternative to the classical transforms. In this respect, the
above-presented dictionary-learning methodology constitutes only the ﬁrst step in
replacing classical transforms.
In this section we present three initial branches of activity on dictionary-learning
that aim to solve some of the above problems. More work is required in order to
bring the learning methodology to a mature level, such that it competes favorably
with pre-chosen or adapted alternatives, on all aspects – quality of results, speed,
and elegance.
3 Operating on an image patch of size √n × √n, the multiplication by the dictionary is performed
by multiplying this patch from the left by a matrix of size √m × √n, and a similar multiplication
from the right with a matrix of size √n × √m.

12.3 Training Structured Dictionaries
239
12.3.1 The Double-Sparsity Model
The double-sparsity method described in this section aims to address the limit on
the input signal dimension, along-side with the speed of deployment of the obtained
dictionary. We return to the dictionary-learning formulation as posed in Equation
(12.2),
min
A,{xi}M
i=1
M
X
i=1
∥yi −Axi∥2
2
subject to ∥xi∥0 ≤k1, 1 ≤i ≤M,
where every signal is known to be constructed from k1 atoms (the change from k0
in the original notation to k1 will become clear shortly). We shall assume that each
of the m atoms in A ∈IRn×m is exhibiting a very speciﬁc inner structure, which will
help in getting faster dictionaries to multiply with.
The structure we choose assumes that each atom in A can be described as a sparse
combination of k0 pre-atoms extracted from a pre-speciﬁed dictionary A0 ∈IRn×m0.
Thus, our assumption can be formulated as A = A0Z, where Z ∈IRm0×m is a sparse
matrix with k0 non-zeros per column. This structure is motivated by the appearance
of the typical dictionaries for image-patches, showing piecewise smooth, periodic,
or textured content. The key for the worthiness of this method is the choice of A0. On
one hand, it should be chosen such that the assumption made above is nearly correct.
On the other hand, it should be a matrix that has a fast multiplication algorithm, so
that operating with A and its adjoint are cheap.
Putting this structure into the dictionary-learning optimization task, as posed in
(12.2), we obtain
min
{xi}M
i=1,{zj}
m
j=1
PM
i=1 ∥yi −A0Zxi∥2
2
(12.6)
subject to
( ∥zj∥0 ≤k0, 1 ≤j ≤m
∥xi∥0 ≤k1, 1 ≤i ≤M
)
.
Minimizing this problem can be done following the above described algorithms that
iterate between an update of {xi}M
i=1 by sparse coding, and an update of
n
zj
om
j=1 for
the eﬀective dictionary update.
Minimization with respect to the sparse representations xi, while assuming that
Z is ﬁxed, can be done as usual, as a sparse coding stage that employs OMP. This
stage is cheaper than the regular MOD/K-SVD sparse-coding stage, because the
multiplication of any vector by A (or its adjoint) is expected to be much cheaper
than O(nm). For example, a unitary and separable 2D-DCT applied on an image
patch of size √n× √n requires O(n log n). A redundant and separable 2D-DCT does
not have a fast n log n algorithm, but the separability by itself is an asset, leading to
O(n1.5) operations, instead of the regular O(n2).
Fixing X = {xi}M
i=1, the update of
n
z j
om
j=1 may seem more demanding, but in-fact
amounts to a very similar sparse coding stage. Considering one column zj at a time,

240
12 The Quest for a Dictionary
we are imitating the K-SVD algorithm in the sense that one atom A0z j from the
overall dictionary is updated. Just as with the K-SVD, we consider hereafter only
those examples that use the j-th atom in their construction. Therefore, the expression
Y−AZX contains now a subset of the complete M columns. We denote the j-th row
(and not column!!) from X by ˜xT
j . The expression ZX can be written as the sum of
m0 outer products of the form zj˜xT
j . Thus, the ℓ2 term in Equation (12.6) becomes
(recall that the summation over i considers only relevant examples that use the j-th
atom)
X
i
∥yi −A0Zxi∥2
2 =

Y −A0
m
X
j=1
z j˜xT
j

2
F
.
(12.7)
Thus, minimization of Equation (12.6) with respect to z j leads to the optimization
problem
min
zj
Ej −A0z j˜xT
j
2
F
subject to ∥zj∥0 ≤k0.
(12.8)
where we have deﬁned E j = Y−A0
Pm
k=1, k, j zk˜xT
k . Note that the row ˜xT
j contains all
the coeﬃcients of the j-th atom, and since we have restricted the error to examples
that use this atom, this vector is dense.
The obtained optimization is treated by an alternating minimization over ˜xT
j and
z j. Minimization of this problem with respect to ˜xT
j is a simple Least-Squares step
that gives
zT
j AT
0 (E j −A0z j˜xT
j ) = 0
⇒
˜xT
j =
1
zT
j AT
0 A0z j
zT
j AT
0 E j.
(12.9)
The denominator stands for the norm of the j-th atom, and we can assume it is
normalized, by a proper normalization of z j.
Minimization with respect to the sparse vector z j can be shown4 to be equivalent
to the solution of
min
z j
Ej˜x j −Az j
2
2
subject to ∥zj∥0 ≤k0,
(12.10)
and this is a classic sparse coding operation that can be handled by OMP. Several
such rounds of updates as described above lead to the updated atom zj, and also ˜xj,
which gives is the updated representation coeﬃcients that use this atom.
Beyond the gained speed, the introduction of structure to the dictionary is beneﬁ-
cial in two other ways: (i) Since the number of degrees of freedom in the dictionary
is smaller, fewer examples are suﬃcient for an eﬀective learning, and faster conver-
gence of the learning algorithm is obtained; and (ii) Higher-dimensional signals can
4 It is easily veriﬁed that
Ej −A0zj˜xT
j

2
F =
Ej˜xj −A0zj
2
2 + f(Ej, ˜x j).

12.3 Training Structured Dictionaries
241
be treated. More on these beneﬁts and demonstration of this approach are brought
in the work by Rubinstein et al.
12.3.2 Union of Unitary Bases
Another approach for injecting structure into the learned dictionary is to force the
dictionary to be a union of unitary matrices. This structure guarantees that the ob-
tained dictionary is a tight Frame, for which the adjoint and the pseudo-inverse are
the same. We now turn to describe this approach in more details.
Starting again with the dictionary-learning formulation as posed in Equation
(12.2),
min
A,{xi}M
i=1
M
X
i=1
∥yi −Axi∥2
2
subject to ∥xi∥0 ≤k0, 1 ≤i ≤M,
assume now that A is chosen to be a union of unitary matrices, as proposed by
Lesage et al. One good reason for this structure is the Block-Coordinate-Relaxation
(BCR) algorithm that was mentioned in Chapter 6 for the sparse coding stage. This
algorithm requires multiplications by unitary matrices and simple shrinkage steps,
and is thus very eﬃcient. Other reasons for this structure could be the ease of the
training (as we show next), and the natural tightness of the Frame A. We shall as-
sume, for simplicity, that only two unitary matrices are involved in the dictionary,
i.e., A = [Ψ, Φ] ∈IRn×2n.
Focusing on the dictionary update stage, and assuming that the sparse represen-
tations are ﬁxed, our goal is to solve
min
Ψ,Φ ∥ΨXΨ + ΦXΦ −Y∥2
F subject to ΨTΨ = ΦTΦ = I.
(12.11)
Here we use the decomposition AX = ΨXΨ + ΦXΦ. Fixing Ψ, optimizing the
above with respect to Φ leads the well-known orthogonal Procrastes problem. This
problem admits a closed-form solution, which we derive next. Thus, we can iterate
between an update of Φ and Ψ several times, and this way perform the dictionary
update stage.
As for the Procrastes problem, it is formulated as a plain Least-Squares problem
of the form ∥A −QB∥2
F, where A, B are arbitrary pair of matrices of the same size,
and the unknown Q is a square and orthogonal matrix that should rotate the columns
of B towards those of A, such that the squared-error is minimized. First we observe
that
∥A −QB∥2
F = tr{ATA} + tr{BTB} −2tr{QBAT}.
(12.12)
Thus, our minimization task is equivalent to the maximization of tr{QBAT}. Assum-
ing that the SVD of the matrix BAT is available to us, BAT = UΣVT, our penalty

242
12 The Quest for a Dictionary
can be written as
tr{QBAT} = tr{QUΣVT} = tr{VTQUΣ} = tr{ZΣ}.
(12.13)
Here we have used the property tr{AB} = tr{BA} (for an arbitrary square matrix
pair), and the notation Z = VTQU. Z is a unitary matrix as well, while Σ is a
diagonal matrix with the singular-values {σi}n
i=1 on the main-diagonal. Thus,
tr{ZΣ} =
n
X
i=1
σizii ≤
n
X
i=1
σi.
(12.14)
The last inequality stems from the fact that entries in a unitary matrix are in the
range [−1, 1]. Fulﬁlling the upper-bound is obtained for the choice Z = I, which in
turn implies that Q = VUT — this is the closed-form solution mentioned above.
12.3.3 The Signature Dictionary
An interesting structure that leads to a shift-invariance property is the Image Signa-
ture Dictionary (ISD), as proposed by Aharon et al. We shall present this method
for a simpliﬁed 1D case, so as to reduce the notational burden.
The dictionary in mind is A ∈IRn×m, as represented throughout this chapter.
However, this dictionary is constructed from a single signal a0 ∈IRm×1, by extracting
all the possible patches of length n (including cyclic shifts). We refer to a0 as the
signature signal that deﬁnes the eﬀective dictionary. Note that in the 1D case, this
dictionary has a circulant structure. While the dictionary have mn entries, m free
parameters alone dictate its entire content.
Denoting by Rk the operator that extracts a patch of length n from location k from
a0, the k-th atom in A is given by ak = Rka0. The representation of a signal y as a
linear combination of atoms from this dictionary is given by
y =
m
X
k=1
xkak =
m
X
k=1
xkRka0.
(12.15)
Armed with this structure, we now return to the dictionary-learning goal, as posed
in the problem
min
A,{xi}M
i=1
M
X
i=1
∥yi −Axi∥2
2
subject to ∥xi∥0 ≤k0, 1 ≤i ≤M.
(12.16)
Adopting the same block-coordinate descent algorithm, the sparse coding stage that
updates xi remains unchanged, since A can be formed as usual. Indeed, we could
numerically beneﬁt from its structure by using the fast-Fourier-Transform (FFT) for
a fast convolution that replaces the exhaustive inner-products.

12.3 Training Structured Dictionaries
243
The dictionary update stage diﬀers, so as to take into account the inner structure
of A. Using the formulation in Equation (12.15) to replace the term Axi in Equation
(12.16), we get
M
X
i=1
∥yi −Axi∥2
2 =
M
X
i=1
yi −
m
X
k=1
xi[k]Rka0

2
2
.
(12.17)
Thus, an update to the dictionary is obtained by optimizing the above ℓ2 expression
with respect to a0. We obtain the equation
0 =
M
X
i=1

m
X
k=1
xi[k]Rk

T yi −
m
X
k=1
xi[k]Rka0
,
(12.18)
which leads to
aopt
0
=

m
X
k=1
m
X
j=1

M
X
i=1
xi[k]xi[ j]
RT
k R j

−1
M
X
i=1
m
X
k=1
xi[k]RT
k yi.
(12.19)
Note that the term ρ(k, j) = PM
i=1 xi[k]xi[ j] in the above equation stands for the
cross-correlation between the entries in the representation vectors. The operation
v = RT
k u creates a zero vector v, and then puts u (of length n) in its k-th location.
We do not provide here further analysis of this formula and its implications, beyond
the obvious statement that this can be used for an easy update of a0. More details
can be found in the recent work by Aharon and Elad, where this structure has been
used in 2D for images. Beneﬁts attributed to this structure are the following:
• Since the number of degrees of freedom is much smaller than nm, this implies
that less training data is required, and faster convergence of the learning proce-
dure can be obtained.
• The better-behaved penalty function for the learning procedure implies a smaller
tendency towards local minimum.
• As the dictionary emerges from a single signal (or image) with plain shifts, work-
ing on two shifted signals can be made more eﬃcient by applying a sparse coding
on the ﬁrst, and simply shifting these atoms to approximate the second. This idea
was used successfully in the work by Aharon and Elad to reduce the complexity
of sparse coding for image patches.
• This structure is the ﬁrst in which atoms of varying sizes are easily accommo-
dated.
Because of these reasons and more, the ISD seems to be a promising concept, and
more work is required to expose and exploit its potential.

244
12 The Quest for a Dictionary
12.4 Summary
With a proper dictionary, the Sparse-Land model can lead to better performance in
the various applications it is harnessed to. Dictionary-learning seems to be a promis-
ing direction for obtaining a favorable model. In this chapter we presented several
algorithms and ideas that have already appeared in the literature. Nevertheless, it
is clear that far more should be done in order to establish this approach as truly
competitive, stable, and reliable.
Still on the dictionary-learning problem, a diﬀerent front that requires more work
refers to the desire to extend the Sparse-Land model. Extensions to consider are
dependencies between the atoms, varying probabilities for the atoms to be used, and
perhaps the introduction of non-linearities. In all these cases, the dictionary-learning
algorithms must be updated to cope with these changes.
As a last point in this chapter we mention that among the various things to be
desired for a good dictionary, a truly multi-scale structure is of an utmost impor-
tance. Learning of such structures has not been treated thoroughly so far, as it poses
numerous diﬃculties. When such a structure will be properly introduced, it might
open new horizons for the vast activity on wavelet and X-let transforms, and lead
to a convergence of these two branches (learning dictionaries, and designing multi-
scale frames).
Further Reading
1. M. Aharon, M. Elad, and A.M. Bruckstein, K-SVD and its non-negative variant
for dictionary design, Proceedings of the SPIE conference wavelets, Vol. 5914,
July 2005.
2. M. Aharon, M. Elad, and A.M. Bruckstein, On the uniqueness of overcomplete
dictionaries, and a practical way to retrieve them, Journal of Linear Algebra
and Applications, 416(1):48-67, July 2006.
3. M. Aharon, M. Elad, and A.M. Bruckstein. K-SVD: An algorithm for designing
of overcomplete dictionaries for sparse representation, IEEE Trans. on Signal
Processing, 54(11):4311–4322, November 2006.
4. E.J. Cand`es and D.L. Donoho, Recovering edges in ill-posed inverse problems:
optimality of curvelet frames, Annals of Statistics, 30(3):784–842, 2000.
5. E.J. Cand`es and D.L. Donoho, New tight frames of curvelets and optimal repre-
sentations of objects with piecewise-C2 singularities, Comm. Pure Appl. Math.,
57:219-266, 2002.
6. A. Cichocki, R. Zdunek, A.H. Phan, and S.I. Amari, Nonnegative Matrix and
Tensor Factorizations Applications to Exploratory Multi-way Data Analysis
and Blind Source Separation, Wiley, Tokyo, Japan, 2009.
7. V. Chandrasekaran, M. Wakin, D. Baron, R. Baraniuk, Surﬂets: a sparse rep-
resentation for multidimensional functions containing smooth discontinuities,
IEEE Symposium on Information Theory, Chicago, IL, 2004.

Further Reading
245
8. R.R. Coifman and M.V. Wickerhauser, Adapted waveform analysis as a tool for
modeling, feature extraction, and denoising, Optical Engineering, 33(7):2170–
2174, July 1994.
9. M.N. Do and M. Vetterli, Rotation invariant texture characterization and re-
trieval using steerable wavelet-domain hidden Markov models, IEEE Trans. On
Multimedia, 4(4):517–527, December 2002.
10. M.N. Do and and M. Vetterli, The ﬁnite ridgelet transform for image represen-
tation, IEEE Trans. On Image Processing, 12(1):16–28, 2003.
11. M.N. Do and and M. Vetterli, Framing pyramids, IEEE Trans. On Signal Pro-
cessing, 51(9):2329–2342, 2003.
12. M.N. Do and M. Vetterli, The contourlet transform: an eﬃcient directional mul-
tiresolution image representation, IEEE Trans. Image on Image Processing,
14(12):2091–2106, 2005.
13. D.L. Donoho and V. Stodden, When does non-negative matrix factorization give
a correct decomposition into parts? Advances in Neural Information Processing
16 (Proc. NIPS 2003), MIT Press, 2004.
14. M. Elad and M. Aharon, Image denoising via learned dictionaries and sparse
representation, International Conference on Computer Vision and pattern Recog-
nition, New-York, June 17-22, 2006.
15. M. Elad and M. Aharon, Image denoising via sparse and redundant representa-
tions over learned dictionaries, IEEE Trans. on Image Processing 15(12):3736–
3745, December 2006.
16. K. Engan, S.O. Aase, and J.H. Husoy, Multi-frame compression: Theory and
design, EURASIP Signal Processing, 80(10):2121–2140, 2000.
17. R. Eslami and H. Radha, The contourlet transform for image de-noising using
cycle spinning, in Proceedings of Asilomar Conference on Signals, Systems,
and Computers, pp. 1982–1986, November 2003.
18. R. Eslami and H. Radha, Translation-invariant contourlet transform and its ap-
plication to image denoising, IEEE Trans. on Image Processing, 15(11):3362–
3374, November 2006.
19. A. Gersho and R.M. Gray, Vector Quantization And Signal Compression,
Kluwer Academic Publishers, Dordrecht, Netherlands, 1992.
20. G.H. Golub and C.F. Van Loan, Matrix Computations, Johns Hopkins Studies
in Mathematical Sciences, Third edition, 1996.
21. P.O. Hoyer, Non-negative matrix factorization with sparseness constraints, Jour-
nal of Machine Learning Research, 1457–1469, 2004.
22. K. Kreutz-Delgado, J.F. Murray, B.D. Rao, K. Engan, T-W, Lee, and T.J. Se-
jnowski, Dictionary learning algorithms for sparse representation, Neural Com-
putation, 15(2)349–396, 2003.
23. D. Lee and H. Seung, Learning the parts of objects by non-negative matrix
factorization, Nature, 788–791, 1999.
24. S. Lesage, R. Gribonval, F. Bimbot, and L. Benaroya, Learning unions of or-
thonormal bases with thresholded singular value decomposition, ICASSP 2005
(IEEE Conf. on Acoustics, Speech and Signal Proc.).

246
12 The Quest for a Dictionary
25. M.S. Lewicki and B.A. Olshausen, A probabilistic framework for the adaptation
and comparison of image codes, Journal of the Optical Society of America A:
Optics, Image Science and Vision, 16(7):1587–1601, 1999.
26. M.S. Lewicki and T.J. Sejnowski, Learning overcomplete representations, Neu-
ral Computation, 12:337–365, 2000.
27. Y. Li, A. Cichocki, and S.-i. Amari, Analysis of sparse representation and blind
source separation, Neural Computation, 16(6):1193–1234, 2004.
28. F.G. Meyer, A. Averbuch, and J.O. Stromberg, Fast adaptive wavelet packet
image compression, IEEE Trans. on Image Processing, 9(5):792–800, 2000.
29. F.G. Meyer and R.R. Coifman, Brushlets: a tool for directional image analy-
sis and image compression, Applied and Computational Harmonic Analysis,
4:147–187, 1997.
30. B.A. Olshausen and D.J. Field, Natural image statistics and eﬃcient coding,
Network – Computation in Neural Systems, 7(2):333–339, 1996.
31. B.A. Olshausen and B.J. Field, Emergence of simple-cell receptive ﬁeld prop-
erties by learning a sparse code for natural images Nature, 381(6583):607–609,
1996.
32. B.A. Olshausen and B.J. Field, Sparse coding with an overcomplete basis set:
A strategy employed by V1? Vision Research, 37(23):3311-3325, 1997.
33. E.P. Simoncelli, W.T. Freeman, E.H. Adelson, and D.J. Heeger, Shiftable multi-
scale transforms, IEEE Trans. on Information Theory, 38(2):587–607, 1992.
34. J.-L. Starck, E.J. Cand`es, and D.L. Donoho, The curvelet transform for image
denoising, IEEE Trans. on Image Processing, 11:670–684, 2002.

Chapter 13
Image Compression – Facial Images
The sparse-representation viewpoint discussed so far, along with dictionary-learning,
is merely that – a viewpoint. The theoretical results we have given merely tell us that
sparse modeling is, in favorable cases, a mathematically well-founded enterprize
with practically useful computational tools. The only way to tell if sparse modeling
works in the real world is to apply it, and see how it performs! We have already
seen the use of sparse representation modeling for image deblurring , and the re-
sults seem to be promising. In this and the next chapters, we review selected results
applying this viewpoint to several image processing tasks.
In this chapter we consider the problem of facial image compression. The content
of this chapter follows up closely with the work by Bryt and Elad. Compression
using sparse representation modeling seems as a very natural approach. Therefore, it
is tempting to develop an image/video compression scheme that relies on this model.
Nevertheless, developing such a compression scheme that competes favorably with
JPEG2000 (for still images) and H-264 (for video) is very hard, if not impossible.
The reason is that in these methods, the entropy-coding stage has been perfected.
Thus, it is not suﬃcient to change the elementary steps of the coding algorithm –
one must provide an appealing entropy-coding as well.
Because of this, we choose to address a very speciﬁc coding task, where the
model by itself could make the desired diﬀerence. This brings us to the handling of
facial images, which is the topic of this chapter.
13.1 Compression of Facial Images
Compression of still images is a very active and matured ﬁeld of research, vivid
in both research and engineering communities. Compression of images is possible
because of their vast spatial redundancy and the ability to absorb moderate errors
in the reconstructed image. This ﬁeld of work oﬀers many contributions, some of
which became standard algorithms that are wide-spread and popular. Among the
many methods for image compression, one of the best is the JPEG2000 standard
© Springer Science+Business Media, LLC 2010
M. Elad, Sparse and Redundant Representations: From Theory to Applications in Signal 
and Image Processing, DOI 10.1007/978-1-4419-7011-4_13,
247

248
13 Image Compression – Facial Images
– a general purpose wavelet-based image compression algorithm with very good
compression performance.
At its core, JPEG2000 relies on the fact that images are expected to be sparse
when represented using wavelets. In this sense, JPEG2000 is already a supporting
testimony for the eﬃciency and relevancy of Sparse-Land as a model for images.
Nevertheless, and as said above, part of the success of the JPEG2000 is attributed
to the well-designed entropy-coding within this compression algorithm. Our goal is
to address a compression task, where the entropy-coding becomes secondary in its
importance.
When considering the compression of a speciﬁc and narrow image class, the
amount of redundancy increases, thus enabling a better compression performance.
Such is the case with compression of facial images. Indeed, this expected gain has
been observed and exploited in several recent publications that oﬀer tailored com-
pression algorithms for facial images. Among those contributions, the more recent
ones show performance surpassing those of the JPEG2000.
Compression of facial images is an appealing problem, both because of the re-
search challenges it provides, and the important applications it serves. From a re-
search perspective, one should primarily wonder how to exploit the additional re-
dundancy that such a focused image class exhibits, and how to surpass general pur-
pose compression algorithms this way. This is not an easy challenge due to the vast
eﬀorts put to the general purpose algorithms, and especially their entropy-coding
parts.
Application-wise, facial images are perhaps the most popular images, held in
large databases by various organizations, such as police and law-enforcement,
schools and universities, states, and in databases of employees in large companies.
Eﬃcient storage of such images is of value, and especially so when considering
photo-ID in electronic ID cards. In such applications, very low bit-rate compression
is to be considered, where general purpose algorithms fail utterly.
Motivated by the above, this chapter addresses the task of compression of facial
images, as described above. We target photo-ID images of pre-speciﬁed and ﬁxed
size 179×221 grayscale images with 8 bits per pixel.1 The goal is very low bit rates
(roughly speaking, we consider compression ratios ≈100), where most algorithms
are unable to show recognizable faces. We use a database containing nearly 4, 500
such facial images, some of which are used for training and tuning the algorithm,
and the others for testing it.
As already mentioned above, the compression algorithm described in this chap-
ter is taken from the work by Bryt and Elad. This algorithm relies strongly on recent
advancements made in using sparse and redundant representation of signals, and
learning their sparsifying dictionaries. The K-SVD algorithm is used for learning
dictionaries for representing small image patches in a locally adaptive way, and us-
ing the obtained dicitonaries to sparse-code the patches’ content. This is a relatively
1 This database contains color photos taken by a 4 Mega-pixel digital camera (Fuji FinePix A400),
against a white and uniform background, with the highest compression settings for best quality.
These photos were cropped and scaled to the above-mentioned size, and also converted to a gray-
value format.

13.2 Previous Work
249
simple and straight-forward algorithm with hardly any entropy-coding stage. Yet,
it is shown to be superior to several competing algorithms: (i) the JPEG and the
JPEG2000, (ii) the VQ-based algorithm presented by Goldenberg et. al., and (iii) A
Principal-Component-Analysis (PCA) approach.2
13.2 Previous Work
Among the thousands of papers that study still image compression algorithms, there
are relatively few that consider the treatment of facial images. Among those, the
most recent and the best performing algorithm is the one by Goldenberg et. al. That
work also provides a thorough literature survey that compares the various methods
and discusses similarities and diﬀerences between them. We concentrate here on the
description of this speciﬁc algorithm, as it serves as the foundation for the sparse-
representation-based method described in this chapter.
The algorithm by Goldenberg et. al., like some others before it, starts with a ge-
ometrical alignment of the input image, so that the main facial features (ears, nose,
mouth, hair-line, etc.) are aligned with those of a database of pre-aligned facial im-
ages. Such alignment further increases the redundancy in the handled image, due to
its high cross similarity to the database. The warping is done by an automatic detec-
tion of 13 feature points on the face, and moving them to pre-determined canonical
locations. These points deﬁne a slicing of the input image into disjoint and covering
set of triangles, each exhibiting an aﬃne warp, being a function of the motion of its
three vertices. Side information on these 13 feature locations enables a reverse warp
of the reconstructed image in the decoder. Figure 13.1 (left side) shows the located
features and the induced triangles. After the warping, the image is sliced into square
and non-overlapping uniform patches (of size 8 × 8 pixels), each of which is coded
separately. Such possible slicing (for illustration purposes we show this slicing with
larger patches) is shown in Figure 13.1 right side.
Coding of the image patches is done using Vector-Quantization (VQ). The VQ
dictionaries are trained using tree-K-Means per each patch separately, using patches
taken from the same location extracted from 4, 500 training images. This way, each
VQ is adapted to the expected local content, and thus the high performance pre-
sented by this algorithm. The number of code-words in the VQ is a function of the
bit-allocation for the patches. As we argue in the next section, while VQ enjoys a
well-founded theoretical support as a near-ideal coder, VQ coding is limited by the
available number of examples and the desired rate, forcing relatively small patch
sizes. This, in turn, leads to a loss of some redundancy between adjacent patches,
and thus a loss in further potential compression.
Another ingredient in this algorithm that partly compensates for the above-
described shortcoming is a multi-scale coding scheme. The image is scaled down
2 The PCA algorithm was developed by Bryt and Elad as a competitive benchmark, and while it is
generally performing very well, it is inferior to the main algorithm presented in their work, which
uses sparse and redundant representations.

250
13 Image Compression – Facial Images
Fig. 13.1 Left: Piecewise aﬃne warping of the image by triangulation. Right: A uniform slicing
to disjoint square patches for coding purposes.
and VQ-coded using patches of size 8×8. Then it is interpolated back to the original
resolution, and the residual is coded using VQ on 8 × 8 pixels patches once again.
This method can be applied on a Laplacian pyramid of the original (warped) image
with several scales.
As already mentioned above, the results shown in Goldenberg’s work surpass
those obtained by JPEG2000, both visually and in Peak-Signal-to-Noise Ratio
(PSNR) quantitative comparisons.3 In this chapter we introduce an alternative al-
gorithm that replaces the coding stage from VQ to sparse and redundant representa-
tions. This leads us to the next section, where we describe the principles behind this
coding strategy.
13.3 Sparse-Representation-Based Coding Scheme
The algorithm we are about to describe below is based on sparse representation
modeling of image patches and trained dictionaries, very much in line with the
Sparse-Land model presented in Chapter 9, and learned dictionaries, as advocated
in Chapter 12. In a way, we generalize the VQ point of view of the previous work
and replace it with sparse representations, thereby leading to better results.
3 PSNR is computed by 10 · log10(2552/MSE), where the MSE is the Mean-Squared-Error per
pixel.

13.3 Sparse-Representation-Based Coding Scheme
251
13.3.1 The General Scheme
A block diagram of the encoder and decoder are given in Figure 13.2. This algorithm
consists of two main processes: An oﬀ-line K-SVD training process and an on-line
image compression/decompression processes.
The K-SVD training process is an oﬀ-line procedure, preceding any image com-
pression. The training produces a set of K-SVD dictionaries that are then considered
ﬁxed for the image compression-decompression stage. A single dictionary is trained
for each 15 × 15 patch over a set of examples that are the learning set. The train-
ing follows the description in Chapter 12, with parameters detailed below. Prior to
the training process for each patch, The mean patch image of the examples in the
learning set is calculated and subtracted from all the examples in this set. This is
equivalent to the statement that the mean-facial image of the learning database is
computed and subtracted from all images, serving as a rough prediction of the in-
coming images to be coded. The entire coding is done on the residual, and this mean
image (stored both in the encoder and the decoder) is added after decompression.
The image compression process uses the following steps in the encoder, followed
by a mirror application of them at the decoder:
• Pre-Processing: This is the same pre-process stage of geometrical warping as
described above. This leads to better conditioning of the input image, increasing
its redundancy versus the image database. The parameters of the warp are sent as
side information for the inverse warp at the decoder. Those parameters are coded
using 20 bytes.
• Slicing to Patches: Just as the VQ algorithm, the sparse-representation-based ap-
proach also works on ﬁxed-size square patches, coding each patch separately and
adaptively. However, the coding strategy is diﬀerent, being based on sparse and
redundant representations. The change in methods leads to the ability to handle
larger patches, and thus get a more eﬃcient coding. In the simulations reported
here we use patches of size 15 × 15 pixels4 (we shall denote by n the length
of the patch when organized as a vector, and thus √n = 15 pixels), although
larger patch sizes are also possible. Another beneﬁt the sparse-representation-
based algorithm has is the ability to use the very same dictionaries for various
bit-rates. This is markedly diﬀerent from the VQ solution that needs diﬀerent
trained code-books per each rate.
• Removal of the Mean: The patches must ﬁt the dictionary in their content, and
thus the same mean patch images that were calculated for the learning set before
the training process are subtracted from the relevant patches in the incoming
image.
• Sparse-Coding: Every patch location has a pre-trained dictionary of code-words
(atoms) Ai j ∈IRn×m. The size we use for m is 512 – a discussion on this choice
appears in the results section. These dictionaries are generated using the K-
SVD algorithm on a set of 4, 400 training examples. The coding itself is done
4 Due to the inconsistency between the image and patch sizes, some of the patches at the right and
bottom boundaries are of diﬀerent sizes than the rest.

252
13 Image Compression – Facial Images
Fig. 13.2 Detailed block diagram of the proposed encoding/decoding method.
by assigning a linear combination of a few atoms to describe the patch content
(sparse-coding). Thus, information about the patch content includes both the lin-
ear weights and the chosen indices. Note that the number of atoms vary from
one patch to another, based on its average complexity, and this varied number is
known at the decoder.
The encoder and the decoder are storing the dictionaries. The sparse-coding stage
for encoding of the patches is done using the OMP algorithm. The decoder is
simply aggregating the proper atoms with the proper weights to reconstruct the
patch, building the output one patch at a time independent of the others.
• Entropy-Coding and Quantization: We use a straight-forward Huﬀman table
applied only to the indices. The representations’ weights are quantized with a 7
bits uniform quantization, with boundaries chosen based on the training infor-
mation for each patch separately.
Since the above compression scheme strongly relies on a successful detection of
the features to warp by, a natural question is whether errors in this stage are de-
structive to the overall compression algorithm. When the detection of the features
fails utterly, the compression necessarily leads to very low performance. Since the
compression is performed on an incoming image, we have access to both the re-
sulting PSNR, and also the average PSNR we expect to get for this bit-allocation.
Therefore, if the compression PSNR is below some threshold, we obviously know

13.3 Sparse-Representation-Based Coding Scheme
253
that the detection failed (or possibly the input image is not a facial image). In such a
case, and in a completely automatic fashion, we can employ the JPEG2000, which
is indeed weaker, but still reasonable. A second option we have in such a system is
to prompt the user to detect the features manually for this image. Thus, even rare
situations (less than 1% in our experiments) where the features are not found do not
lead to a breakdown in a system that employs the proposed algorithm.
13.3.2 VQ Versus Sparse Representations
As mentioned earlier, the destination compression ratio and the number of training
examples pose a hard limit on the allowed patch size when using VQ. This is because
as the patch size grows while keeping the rate (bits-per-pixel) ﬁxed, the dictionary is
required to grow exponentially, reaching sizes far beyond the number of examples to
train on. More speciﬁcally, using patch size of √n × √n pixels, with m code-words
in the VQ dictionary, considering an image with P pixels, and a target rate of B bits
for the entire image, all these ingredients are related to each other by the equation
B = P
n · log2(m) ⇒
m = 2
nB
P .
(13.1)
For example, coding an image of size 179 × 221 (P = 39, 559) pixels5 with a target
rate of B = 500 × 8 bits, a patch of size √n = 8 leads to m = 88, which is reason-
able. However, increasing the patch size to √n = 12 already leads to m ≈24, 000,
which cannot be trained from 4, 500 examples. Thus, the VQ approach is limited to
relatively small patch sizes (8 × 8 used by Goldenberg et. al.), implying that much
of the spatial redundancy between adjacent patches is overlooked.
When turning to sparse and redundant dictionaries, the picture changes dramati-
cally. We consider a patch size of √n × √n pixels, with m atoms in the dictionary,
k0 of which are used on average6 in the representation (assuming 7 bits quantization
of the weights). Handling of an image with P pixels, and a target rate of B bits for
the entire image, all these ingredients lead to the relation
B = P
n · k0 · (log2(m) + 7).
(13.2)
This means that per patch, instead of using log2 m bits when working with VQ, we
need k0(log2 m + 7) bits on average. Thus, if the required amount of bits per patch
is too high (as indeed happens when n grows), it can be absorbed by varying k0. For
example, for an image with P = 39, 600 pixels, using m = 512 atoms, a target rate
5 This is the actual size used in our experiments.
6 If k0 varies from one patch to another, side information is necessary in order to instruct the
decoder how many atoms to use in each patch. However, in the proposed scheme, while k0 indeed
varies spatially, it remains ﬁxed for all images and thus the decoder knows the number of atoms
allocated per patch location with no additional side information.

254
13 Image Compression – Facial Images
10
15
20
25
30
0
2
4
6
8
10
12
Block size n0.5
Average cardinality k0
B=500 Bytes
B=1000 Bytes
Fig. 13.3 The Required average number of atoms k0 as a function of the patch size √n, with the
inﬂuence of the overall allocated number of bits B.
of B = 500 ∼1000 bytes, and a patch sizes in the range √n = 8 ∼30, all these
lead to a required average number of atoms k0 as shown in Figure 13.3. As can be
seen, the values are reasonable and so we have the ﬂexibility to control the rate by
changing k0 in the range 1 ∼12.
13.4 More Details and Results
The work by Bryt and Elad conducts several experiments in order to evaluate the
performance of the proposed compression method and the quality of its resulting
images. In this section we show some statistical results as well as reconstructed
images from this compression method and a comparison between these results to
several other competitive compression techniques.
Bryt and Elad use 4, 400 facial images as the training set and a diﬀerent 100 im-
ages as the test set. All the images in both sets are of a ﬁxed size of 179×221 pixels.
The slicing to disjoint and covering set of patches is uniform over the image with
a size of 15 × 15 pixels per each. Examples of such pre-processed training/testing
images can be seen in Figure 13.4.
Before turning to present the results we should add the following: While all the
results shown here refer to the speciﬁc database we operate on, the overall scheme
proposed is general and should apply to other facial image databases just as well.
Naturally, some changes in the parameters might be necessary, and among these, the
patch size is the most important to consider. We also note that as one shifts from one

13.4 More Details and Results
255
Fig. 13.4 Examples of pre-processed images used in the training/testing sets.
source of images to another, the relative size of the background in the photos may
vary, and this necessarily leads to changes in performance. More speciﬁcally, when
the background regions are larger (e.g., the images we use here have relatively small
such regions), the compression performance is expected to improve.
13.4.1 K-SVD Dictionaries
Using 100 iterations of the K-SVD algorithm, the required dictionaries are learned.
Every obtained dictionary contains 512 patches of size 15 × 15 pixels as atoms. In
Figure 13.5 we see the dictionary that was trained for patch number 80 (the left eye)
for k0 = 4 sparse coding atoms, and similarly, in Figure 13.6 we see the dictionary
that was trained for patch number 87 (the right nostril) also for k0 = 4 sparse coding
atoms. It can be seen that both dictionaries contain atoms similar in nature to the
image patch for which they were trained for. A similar behavior is observed in other
dictionaries.
13.4.2 Reconstructed Images
The proposed coding strategy allows us to learn which parts of the image are more
diﬃcult than others to code. This is done by assigning the same representation error
threshold to all the patches, and observing how many atoms are required for the
representation of each patch on average. Clearly, patches with a small number of
allocated atoms are simpler to represent than others. We would expect that the repre-
sentation of smooth areas of the image such as the background, parts of the face and
maybe parts of the clothes will be simpler than the representation of areas contain-
ing detailed parts such as the hair or the eyes. Figure 13.7 shows maps of the atom
allocation per patch and representation error (Root Mean-Squared-Error – RMSE)
per patch for the images in the test set at two diﬀerent bit-rates. It can be seen that
more atoms are allocated to patches containing the facial details (hair, mouth, eyes,

256
13 Image Compression – Facial Images
Fig. 13.5 The Dictionary obtained by K-SVD for patch no. 80 (the left eye) using the OMP method
with k0 = 4.
Fig. 13.6 The Dictionary obtained by K-SVD for patch no. 87 (the right nostril) using the OMP
method with k0 = 4.
facial borders), and that the representation error is higher in these patches. It can also
be seen that the overall number of allocated atoms increases and the representation
error decreases with the image bit-rate.
As in other compression methods, the reconstructed images suﬀer from several
kinds of artifacts. These artifacts are caused by the fact that we operate on dis-
joint patches, coding them independently. Contrary to methods such as JPEG and
JPEG2000, the reconstructed images at the presented method do not have a strong
smearing artifact all over the image, but only local small areas in which the im-

13.4 More Details and Results
257
Fig. 13.7 For coding to 400 bytes, (a) shows the atom allocation map and (b) shows the represen-
tation error map. Similarly, (c) and (d) show the same maps for 820 bytes.
age is smeared. Other artifacts include inaccurate representation of high frequency
elements in the image, inability to represent complicated textures (mainly in the
clothes), and inconsistency in the spatial location of several facial elements compar-
ing to the original image. These artifacts are caused by the nature of the algorithm
developed, which has a limited ability to build the reconstructed image out of the
given dictionaries.
In Figure 13.8 we can see two original images from the training set and their
reconstructed images in a bit-rate of 630 bytes. The mentioned artifacts can be seen
in these images especially in the areas of the chin, the neck, the mouth, the clothes
and the hairline. Although there are areas in the images that contain a smearing
eﬀect, the majority of the image is clear and sharp, and certainly having a high
visual quality.
Figure 13.9 shows three original images from the test set and their reconstructed
images at a bit-rate of 630 bytes. As in the previous images, the mentioned artifacts
can be seen in these images as well, in the same image areas as before. As expected,
the quality of the images from the test set is not as high as the quality of images
from the learning set, but these images too are clear and sharp, and have a high
visual quality.
Much like other compression methods, the quality of the reconstructed images
in the proposed method improves as the bit-rate increases. However, the contri-
bution gained from such a rate increment is not divided equally over the image.

258
13 Image Compression – Facial Images
Fig. 13.8 Examples of original (top) and reconstructed (bottom) images from the training set, with
a relatively low error (4.04, on the left) and a relatively high error (5.9, on the right) representation
Root-MSE, both at a bit rate of 630 bytes.

13.4 More Details and Results
259
Fig. 13.9 Examples of original and reconstructed images from the test set, with a relatively low er-
ror (5.3, left), an average error (6.84, middle) and a relatively high error (11.2, right) representation
RMSE, all three at a bit-rate of 630 bytes.
Additional bits are allocated to patches with higher representation error, and those
are improved ﬁrst. This property is directly caused by the nature of the compres-
sion process, which is RMSE-oriented and not bit-rate-oriented. The compression
process sets a single RMSE threshold for all the patches, forcing each of them to
reach it without ﬁxing the number of allocated atoms per patch. Patches with simple
(smooth) content are most likely to have a representation error far below the thresh-
old, even when using one atom or less, whereas patches with more complex content
are expected to have a representation error very close to the threshold. Such prob-
lematic patches will be forced to improve their representation error by increasing
the number of atoms they use as the RMSE threshold is decreased, while simple
patches with a representation error below the threshold will not change their atom
allocation. Figure 13.10 illustrates the gradual improvement in the image quality
as the bit-rate increases. As can be seen, not all the patches improve as the bit-rate
increases but only some of them, such as several patches in the clothes area, in the
ears and in the hairline. These patches are more diﬃcult to represent than others.

260
13 Image Compression – Facial Images
Fig. 13.10 Comparing the visual quality of a reconstructed image from the test set at several bit-
rates. From top left, clockwise: 285 bytes (RMSE 10.6), 459 bytes (RMSE 8.27), 630 bytes (RMSE
7.61), 820 bytes (RMSE 7.11), 1021 bytes (RMSE 6.8), Original image.
13.4.3 Run-Time and Memory Usage
In assessing the required memory for the proposed compression method we need to
take into account all the dictionaries that were produced for each patch, the previ-
ously mentioned mean patch images, the Huﬀman tables, the coeﬃcient usage tables
and the quantization levels for each patch. All these lead to less than 20 Mbytes.
Practically, the training and testing processes have been all implemented using
non-optimized Matlab code on a regular PC (Pentium 2, 2GHz, 1GByte RAM).
Training of all the required dictionaries requires 100 hours to complete (and thus
has been implemented on several computers in parallel, each handling a diﬀerent
group of patches). The compression of a single image requires 5 seconds, and its
decompression takes less than 1 second.

13.4 More Details and Results
261
13.4.4 Comparing to Other Techniques
An important part in assessing the performance of this compression method is its
comparison to known and competitive compression techniques. As mentioned be-
fore, we compare the obtained results with JPEG, JPEG2000, the VQ-Based com-
pression method described earlier, and a PCA-Based compression method that was
built especially for this work as a competitive benchmark. We therefore start with a
brief description of the PCA technique.
The PCA-Based compression method is very similar to the scheme described
above, simply replacing the K-SVD dictionaries with a Principal Component Anal-
ysis (PCA) ones. These dictionaries are square matrices storing the eigenvectors of
the auto-correlation matrices of the training examples in each patch, sorted by a de-
creasing order of their corresponding eigenvalues. Another induced diﬀerence is the
replacement of the sparse coding representation with a simple linear approximation
that projects on the ﬁrst several eigenvectors, until the representation error decreases
below the speciﬁed threshold. Comparison to the PCA-Based compression method
is important here, because whereas the proposed technique could be interpreted as
an adaptation of the JPEG2000 to the image family by training, the PCA could be
seen as a similar step emerging from JPEG.
Figures 13.11, 13.12 and 13.13 show a visual comparison of the obtained results
with the JPEG, JPEG2000, and the PCA methods at three diﬀerent bit-rates. Note
that Figure 13.11 does not contain JPEG examples because the bit-rate is below the
possible minimum in this case. These ﬁgures clearly show the visual and quanti-
tative advantage of the proposed sparse-representation-based method over all the
alternatives. We can especially notice the strong smearing eﬀect in the JPEG and
JPEG2000 images, contrary to the images obtained by the proposed algorithm.
Figure 13.14 shows a rate-distortion curves comparison of the compression meth-
ods mentioned above, averaged over the 100 test images. The PSNR shown here
corresponds to the aligned grayscale images for all methods, in order to evaluate the
representation error from the sparse coding and reconstruction stages alone, without
the error that results from the geometrical warp process. Adding the error induced
by the geometrical warp implies a decrease of 0.2 −0.4dB for the VQ, PCA, and
K-SVD methods, which does not change the overall ordering of these algorithms.
In addition to these rate-distortion curves we added a curve for a “Clean”
JPEG2000 method, which is simply a horizontally shifted version of the JPEG2000
curve taking into account the header embedded in the JPEG2000 standard. This
header was found to be approximately 220 bytes.
It can be seen from this comparative graph that the K-SVD compression method
presents the best performance and especially so in the very low bit-rates of 300 −
1000 bytes, which is the area of interest.

262
13 Image Compression – Facial Images
Fig. 13.11 Facial images compression at a bit-rate of 400 bytes, comparing results of JPEG2000,
the PCA, and the K-SVD methods. The values in the brackets are the representation RMSE.
13.4.5 Dictionary Redundancy
Dictionary redundancy, or over-completeness, is deﬁned as m/n, being 1 for com-
plete (non-redundant) representations, and above it in the experiments reported
above. A natural question is whether such redundancy is truly necessary for get-
ting the compression results shown. Fixing the patch size, n, what would be the
eﬀect of changing the number of atoms, m? Figure 13.15 shows four curves of the
averaged PSNR on the test set images with varying redundancy. Each curve shows
this behavior for a diﬀerent ﬁxed image bit-rate. Note that since the patch size is
15 × 15 pixels, having 225 atoms implies a complete (non-redundant) dictionary,
and when having 150 atoms, we are in fact in an under-complete regime.

13.5 Post-Processing for Deblocking
263
Fig. 13.12 Facial images compression at a bit-rate of 550 bytes, comparing results of JPEG,
JPEG2000, the PCA, and the K-SVD methods. The values in the brackets show the representa-
tion RMSE.
We can see in Figure 13.15 that the quality of the images is improving (PSNR-
wise) with the dictionary redundancy, but this increase is subsiding. Intuitively, we
would expect a change in the tendency of the curves at some point, partly due to
over-ﬁtting of the dictionaries (due to the limited number of examples), and partly
because too high redundancy is expected to cause a deterioration in performance,
due to the cost of transmitting the indices. Such change in tendency has not been
observed because of limitations in the training data size.
13.5 Post-Processing for Deblocking
13.5.1 The Blockiness Artifacts
The motivation for a deblocking method in this scheme is the appearance of blocki-
ness artifacts in the reconstructed images that result from the nature of the proposed

264
13 Image Compression – Facial Images
Fig. 13.13 Facial images compression at a bit-rate of 820 bytes, comparing results of JPEG,
JPEG2000, the PCA, and the K-SVD methods. The values in the brackets show the representa-
tion RMSE.
compression algorithm. These disturbing artifacts are visible all over the image in
varying degrees of strength, and all the reconstructed images suﬀer from them. The
blockiness artifacts are created because the coding of each image is done in uniform
non-overlapping patches, and these patches are coded independently according to
their relevant trained dictionaries. Therefore, the nature of the representation in each
patch is diﬀerent, and especially so in patches which require a diﬀerent number of
atoms or which have a diﬀerent pattern of data in them. These diﬀerences can cre-
ate false edges between the patches, as can be seen in Figures 13.11-13.13. This
phenomenon is expected to appear in every compression technique in which the en-
coding is done in non-overlapping patches, but it is more signiﬁcant in the presented
method, due to the deep compression ratio, and the size of the patches. Naturally,
patch sizes can be decreased and dictionaries can be extended, but doing so will
not eliminate the artifacts completely and will severely hurt the performance of the
compression algorithm as the bit-rate of each compressed image will increase. Such
a solution is thus ineﬃcient from a wider point of view.

13.5 Post-Processing for Deblocking
265
0
200
400
600
800
1000
1200
18
20
22
24
26
28
30
32
34
PSNR [dB]
Compressed Image Size [Bytes]
 
 
K−SVD
VQ
PCA
JPEG
JPEG2000
"Clean" JPEG2000
Fig. 13.14 Rate-Distortion curves for the JPEG, JPEG2000, “Clean” JPEG2000 (i.e., after removal
of the header bits), PCA, VQ, and K-SVD methods.
13.5.2 Possible Approaches For Deblocking
There are several possible approaches one can take to eliminate the blockiness arti-
facts, and these mostly divide into two main categories: Encoding stage approaches
and post-processing approaches. The encoding stage approaches suggest attacking
the problem by preventing the blockiness artifacts from being created in the ﬁrst
place. Such possible approaches include encoding in overlapping patches, train dic-
tionaries on two or more shifted grids, train dictionaries with respect to their neigh-
boring dictionaries and other approaches along these lines. These approaches oﬀer
a clean structural solution to the problem and are designed to eliminate the blocki-
ness artifacts completely. However, the disadvantages of these methods should not
be overlooked – they are complex, some are complicated to implement, and they
can lead to an increase in the bit-rate of the compressed images, which is highly
undesirable.
In contrary to encoding stage approaches, post-processing approaches attempt to
eliminate the blockiness artifacts after they have already been created by the com-
pression algorithm. Such possible approaches include various ﬁltering techniques

266
13 Image Compression – Facial Images
0
1
2
3
4
5
6
29.5
30
30.5
31
31.5
32
32.5
33
Redundancy [Atoms/Pixels]
PSNR [dB]
 
 
Fixed 400 Bytes
Fixed 550 Bytes
Fixed 675 Bytes
Fixed 900 Bytes
Fig. 13.15 The eﬀect of changing the redundancy in the trained dictionaries on the quality of the
test set images in PSNR for a ﬁxed bit-rate (four diﬀerent ﬁxed values).
applied to the compressed-decompressed image. These approaches are simple and
easy to implement, and they do not increase the rate of the compressed image. The
main disadvantage of these methods is of course that they cannot guaranty to elim-
inate the blockiness artifacts in all cases and all images, as the method of operation
is limited and does not have an access to the source of the problem.
13.5.3 Learning-Based Deblocking Approach
The deblocking method we present here belongs to the second category, as it is an
application of specially created ﬁlters over the reconstructed image. We deﬁne a
three pixel wide border-frame of each patch as the Region-Of-Interest (ROI) to be
ﬁxed. These areas suﬀer the most from the blockiness artifacts, and their location is
known to us, once the grid is set. For each pixel in the ROI of each diﬀerent patch we
calculate a diﬀerent 5×5 linear ﬁlter to correct the blockiness artifacts. This is done
using a learning set for the same pixel, accumulated over many (1, 000 in this case)

13.6 Deblocking Results
267
similar images. The learning set consists of original pixels and the reconstructed
ones after compression-decompression.
In general, this approach is similar in its spirit to the principles of the proposed
compression algorithm, in which the encoding stage for new images is done ac-
cording to dictionaries that were trained according to a learning set. In this case,
however, we calculate the optimal ﬁlters by the Least-Squares penalty
ˆhk = arg min
hk
∥YRkhk −Xek∥2
2,
(13.3)
where hk is the desired 5 × 5 ﬁlter for the kth pixel, ordered as a column vector. The
matrix X contains the original images in the learning set as its rows, and similarly,
Y is the matrix containing the decoded images in the learning set as rows. Rk is a
matrix that extracts from Y the appropriate pixels in the 5×5 environment of the kth
pixel, and ek is a vector that selects the kth pixel out of the images X.
The goal of this minimization problem is to design a linear ﬁlter that attempts to
best ﬁt the kth pixel in the reconstructed image to the one in the original image, using
only the pixel’s 5 × 5 neighborhood. Equation 13.3 has a simple analytic solution
for ˆhk in the form of
ˆhk = (RT
k YTYRk)−1 · RT
k YTXek.
(13.4)
Note that the needed ﬁlters vary from one bit-rate to another, and thus should be
calculated for each separately.7 For a given bit-rate group, the trained ﬁlters can
be combined as the rows of a large matrix Hr, such that when it multiplies the
compressed-decompressed image, all the ﬁlters are activated, and deblocking is per-
formed.
This ﬁlter calculation process is simple, easy to implement and does not increase
the bit-rate of the compression, as the resulting matrices of ﬁlters for each bit-rate
are calculated in an oﬀ-line mode and are stored in the decoder. The method of
using these ﬁlters on any given reconstructed image is also very convenient, simply
by multiplying the matrix Hr by the lexicographic representation of the image in the
post-processing stage of the compression algorithm.
13.6 Deblocking Results
We use 1, 000 facial images as our learning set and 100 diﬀerent images as our test
set. All the images in both sets are of size 179 × 221 after pre-processing. Figure
13.16 shows a rate-distortion curves comparison as in Figure 13.14, with a new
curve that corresponds to the deblocked results. As can be seen, an improvement of
0.4dB is obtained because of the proposed deblocking method.
7 Doing this for each bit-rate is impractical, and not necessary. Our bit-rate range (300-1000 bytes)
can be divided to groups, and each should get its own set of ﬁlters.

268
13 Image Compression – Facial Images
0
200
400
600
800
1000
1200
18
20
22
24
26
28
30
32
34
PSNR [dB]
Compressed Image Size [Bytes]
 
 
K−SVD
VQ
PCA
JPEG
JPEG2000
"Clean" JPEG2000
K−SVD + Deblocking
Fig. 13.16 Rate-Distortion curves for the JPEG, JPEG2000, “Clean” JPEG2000, PCA, VQ, and
the two K-SVD methods.
In order to visually show the eﬀectiveness of the presented deblocking method,
we apply it on the same images as in Figure 13.12, referring to a rate of 550
bytes. We also add an additional image where the blockiness artifacts are particulary
strong. The ﬁlters matrix that was calculated for this bit-rate is used on the recon-
structed images as a post-processing operation and the results can be seen in Figure
13.17. In spite of its limitations, it can be clearly seen that the deblocking method
has eliminated the blockiness artifacts in most places and reduced them signiﬁcantly
in others. Although the deblocking ﬁlters create a small degree of smoothing along
the block edges where they are applied, the images still appear sharp and their over-
all visual quality is improved. In addition to the improvement in the visual quality,
which is our primary goal, we also gain an improvement in the PSNR of the images,
as seen in Figure 13.16.
13.7 Summary
In this chapter we have demonstrated how sparse and redundant representations and
the K-SVD learning algorithm can be used for handling a practical image compres-

Further Reading
269
Fig. 13.17 A comparison of reconstructed images at the rate of 550 bytes before (top) and after
(bottom) applying the linear deblocking method.
sion application – coding of facial images. The devised scheme is practical, simple,
and direct, and yet it leads to performance surpassing the JPEG2000 algorithm.
Unfortunately, the path taken in this chapter cannot be imitated in order to pro-
pose a general purpose image compression algorithm, and this remains as a major
challenge for the near future. Still, from the experiments demonstrated here we have
learned several important lessons:
• Contrary to intuition, redundancy can be of value in improving compression per-
formance;
• Sparse and redundant representations can successfully replace a VQ scheme un-
der some conditions, and lead to better results; and
• In the broad comparison between linear and non-linear approximation (PCA ver-
sus K-SVD in our tests), we have seen here that despite the additional cost of
transmitting indices, the non-linear approximation performs better.
Further Reading
1. P.J. Burt and E.H. Adelson, The Laplacian pyramid as a compact image code,
IEEE Trans. on Communications, 31:532–540, 1983.

270
13 Image Compression – Facial Images
2. O. Bryt and M. Elad, Compression of facial images using the K-SVD algorithm,
Journal of Visual Communication and Image Representation, 19(4):270–283,
May 2008.
3. O. Bryt and M. Elad, Improving the K-SVD facial image compression using a
linear deblocking method, The IEEE 25-th Convention of Electrical and Elec-
tronics Engineers in Israel, Eilat, Israel, December 3-5, 2008.
4. P. Cosman, R. Gray, and M. Vetterli, Vector quantization of images subbands:
A survey, IEEE Trans. Image Processing, 5:202–225, 1996.
5. M. Elad, R. Goldenberg, and R. Kimmel, Low bit-rate compression of facial
images, IEEE Trans. on Image Processing, 16:2379–2383, 2007.
6. A.J. Ferreira and M.A.T. Figueiredo, Class-adapted image compression using
independent component analysis, in: Proceedings of the International Confer-
ence on Image Processing (ICIP), Barcelona, Spain, pp. 14–17, September
2003.
7. A.J. Ferreira and M.A.T. Figueiredo, Image compression using orthogonalized
independent components bases, in: Proceedings of the IEEE XIII Workshop on
Neural Networks for Signal Processing, Toulouse, France, 17–19, September
2003.
8. A.J. Ferreira and M.A.T. Figueiredo, On the use of independent component
analysis for image compression, Signal Processing: Image Communication,
21:378–389, 2006.
9. O.N. Gerek and C. Hatice, Segmentation based coding of human face images
for retrieval, Signal-Processing, 84:1041–1047, 2004.
10. A. Gersho and R.M. Gray, Vector Quantization And Signal Compression,
Kluwer Academic Publishers, Dordrecht, Netherlands, 1992.
11. T. Hazan, S. Polak, and A. Shashua, Sparse image coding using a 3D non-
negative tensor factorization, in: Proceedings of the Tenth IEEE International
Conference on Computer Vision (ICCV), Beijing, China, pp. 17–21, October
2005.
12. J.H. Hu, R.S. Wang, and Y. Wang, Compression of personal identiﬁcation pic-
5:198–203, 1996.
13. J. Huang and Y. Wang, Compression of color facial images using feature correc-
tion two-stage vector quantization, IEEE Trans. on Image Processing, 8:102–
109, 1999.
14. K. Inoue and K. Urahama, DSVD: a tensor-based image compression and
recognition method, in: Proceedings of the IEEE International Symposium on
Circuits and Systems (ISCAS), Kobe, Japan, pp. 23–26, May 2005.
15. A. Lanitis, C.J. Taylor, and T.F. Cootes, Automatic interpretation and coding of
face images using ﬂexible models, IEEE Trans. on Pattern Analysis and Ma-
chine Intelligence, 19:743–756, 1997.
16. Y. Linde, A. Buzo, and R.M. Gray, An algorithm for vector quantiser design,
IEEE Trans. on Communications, 28:84–95,1980.
tures using vector quantization with facial feature correction, Optical-Engineering,

Further Reading
271
17. B. Moghaddam and A. Pentland, An automatic system for model-based coding
of faces, in: Proceedings of DCC ’95 Data Compression Conference, Snowbird,
UT, USA, pp. 28–30, March 1995.
18. L. Peotta, L. Granai, and P. Vandergheynst, Image compression using an edge
adapted redundant dictionary and wavelets, Signal- Processing, 86:444–456,
2006.
19. Z. Qiuyu and W. Suozhong, Color personal ID photo compression based on ob-
ject segmentation, in: Proceedings of the Paciﬁc Rim Conference on Commu-
nications, Computers and Signal Processing (PACRIM), Victoria, BC, Canada,
pp. 24–26, August 2005.
20. M. Sakalli and H. Yan, Feature-based compression of human face images,
Optical-Engineering, 37:1520–1529, 1998.
21. M. Sakalli, H. Yan, and A. Fu, A region-based scheme using RKLT and predic-
tive classiﬁed vector quantization, Computer Vision and Image Understanding,
75:269–280, 1999.
22. M. Sakalli, H. Yan, K.M. Lam, and T. Kondo, Model-based multi-stage com-
pression of human face images, in: Proceedings of 14th International Confer-
ence on Pattern Recognition (ICPR), Brisbane, Qld., Australia, pp. 16–20, Au-
gust 1998.
23. A. Shashua and A. Levin, Linear image coding for regression and classiﬁcation
using the tensor-rank principle, in: Proceedings of the IEEE Computer Society
Conference on Computer Vision and Pattern Recognition (CVPR), Kauai, HI,
USA, pp. 8–14, December 2001.
24. D.S. Taubman and M.W. Marcellin, JPEG2000: Image Compression Funda-
mentals, Standards and Practice, Kluwer Academic Publishers, Norwell, MA,
USA, 2001.

Chapter 14
Image Denoising
14.1 General Introduction – Image Denoising
Images often contain noise, which may arise due to sensor imperfection, poor il-
lumination, or communication errors. Removing such noise is of great beneﬁt in
many applications, and this may explain the vast interest in this problem and its so-
lution. However, the importance of the image denoising problem goes beyond the
evident applications it serves. Being the simplest possible inverse problem, it pro-
vides a convenient platform over which image processing ideas and techniques can
be tested and perfected. Indeed, numerous contributions in the past 50 years or so
address this problem from many and diverse points of view. Statistical estimators
of all sorts, spatial adaptive ﬁlters, stochastic analysis, partial diﬀerential equations,
transform-domain methods, splines and other approximation theory methods, mor-
phological analysis, diﬀerential geometry, order statistics, and more, are some of the
many directions and tools explored in studying this problem.
In this chapter we have no intention of providing a survey of this vast activity
on image denoising. Instead, we concentrate on a speciﬁc family of algorithms that
are of interest in the context of this book – algorithms that use sparse and redun-
dant representation modeling. These methods have been found in recent years to
be highly eﬀective and promising, often leading to the best known performance in
terms of the noise removal results they achieve.
Our exposition starts with an algorithm that builds on the rough core of sparse
representation modeling, similar to the content in Chapter 10, and adopting a global
treatment of the image denoising problem. Then we present various improvements
of this paradigm that propose local processing, learning of the shrinkage curve, in-
corporating dictionary training, and more. We cover also the Non-Local-Means
(NLM) algorithm, because of an interesting tie we expose to sparse-representation
methods covered. We conclude this chapter with an introduction of the Stein Un-
biased Risk Estimator (SURE), used for an automatic tuning of the parameters of
some image denoising algorithms.
© Springer Science+Business Media, LLC 2010
M. Elad, Sparse and Redundant Representations: From Theory to Applications in Signal 
and Image Processing, DOI 10.1007/978-1-4419-7011-4_14,
273

274
14 Image Denoising
We start by a proper modeling of the problem. An ideal image y0 ∈IRN (of size
√
N ×
√
N pixels) is measured in the presence of an additive zero-mean white and
homogeneous Gaussian noise, v, with a known standard-deviation σ. The measured
image, y, is thus
y = y0 + v.
(14.1)
We aim to design an algorithm that can remove the noise from y, returning as close
as possible to the original image, y0. We start by a direct application of the ideas
posed in Chapters 9 and 10 for solving this problem, and evolve from there to more
advanced techniques.
14.2 The Beginning: Global Modeling
14.2.1 The Core Image-Denoising Algorithm
The signal denoising problem has been mentioned over and over again throughout
this book, and when it comes to solving it using sparse and redundant representation
modeling, we were always led to some variant of the optimization problem (Pϵ
0),
(Pϵ
0)
min
x
∥x∥0 subject to ∥Ax −y∥2
2 ≤ϵ.
(14.2)
The threshold ϵ is closely tied with the noise power, and a natural choice of it would
be cNσ2, with 0.5 ≤c ≤1.5. The solution to this problem, denoted by ˆx, is the
sparse representation that represents the desired clean image. Thus, the denoised
output is ˆy = Aˆx.
In previous chapters we have seen this method being recommended from both a
deterministic and a stochastic points of view, we have introduced ways to ﬁne-tune
it to perform better by adding assumptions on the statistics of the non-zero entries
in x, we have explored various options to further improve the denoising eﬀect by
approximating the MMSE estimator, and more. All these facts lead to the claim that
this formulation is solid and reliable.
Naturally, the above formulation calls for a proper choice of the dictionary A.
Using sparse representations with speciﬁc choices of A as a driving force for de-
noising of images has drawn a lot of research attention in the past decade or so.
Initially, sparsity of the unitary wavelet coeﬃcients was considered, leading to the
celebrated shrinkage algorithm, as already mentioned in Chapter 6. However, the
performance of this method was found to be insuﬃcient and leaving much room for
improvements, leading researchers to consider redundant representations.
One reason to use redundant representations is the desire to have a shift-invariance
property. In terms of the denoising algorithm, this implies that no matter where the
edges are, their treatment will remain the same. Also, with the growing realization
that regular separable 1D wavelets are inappropriate for handling images (i.e., these

14.2 The Beginning: Global Modeling
275
transforms are unable to represent images sparsely), several new tailored multi-scale
and directional redundant transforms were introduced, including the curvelets, con-
tourlets, wedgelets, bandelets, and the steerable wavelets. The use of these dictio-
naries within the above formulation of the image denoising solver leads to far better
results, when compared to the unitary wavelet shrinkage method. For a while (the
early 2000’s), these algorithms were considered as the best available image denois-
ing methods.
Let us demonstrate an elementary image denoising algorithm that relies on a re-
dundant dictionary, in the spirit of the above discussion. Using the redundant Haar
transform for A, as used in Chapter 10,1 we aim to denoise an image. For the ap-
proximation of the solution of the problem posed in Equation (14.2) we use the
thresholding algorithm. Thus, the overall denoising process is comprised of the for-
mula
ˆy = AST

ATy

,
(14.3)
where ST is a scalar hard-thresholding operator (i.e., ST(z) = 0 for |z| < T, and
ST(z) = z otherwise).
Since the columns of A are not ℓ2-normalized, we should scale the thresholds by
these norms, implying that the i-th entry aT
i y in the vector ATy is to be thresholded
by the value ∥ai∥2 ·T. Put diﬀerently, we can used a ﬁxed value of threshold T in the
formula
ˆy = AWST

W−1ATy

,
(14.4)
where W is a diagonal matrix that contains the norms of the atoms, ∥ai∥2. Note
that this algorithm could be interpreted as the ﬁrst iteration in an iterative-shrinkage
algorithm, which is initialized with zeros. More on the reason to use W appears
next.
Figure 14.1 presents the quality of the outcome (in terms of PSNR) of this algo-
rithm as a function of the parameter T, for the input image Barbara, contaminated
by additive Gaussian noise of standard-deviation σ = 20.2 As can be seen, for the
best choice of T, we obtain an improvement of more than 5.2dB over the input noisy
image. Figure 14.2 shows the original image, the noisy one, and the result obtained
for the best choice of T, which seems to be clear of most of the initial noise.
1 This matrix represents a tight Frame with redundancy factor of 7 : 1.
2 This image and the denoising of it (speciﬁcally, the removal of a zero-mean white and additive
Gaussian noise with σ = 20) will accompany us throughout this chapter, as a benchmark for
comparing various denoising strategies.

276
14 Image Denoising
0
50
100
150
200
20
21
22
23
24
25
26
27
28
Threshold Value T
PNSR
 
 
PNSR for various T values
PSNR of y
Fig. 14.1 The PSNR obtained by the thresholding algorithm with the redundant Haar dictionary,
as a function of the threshold parameter T. While the input image has PSNR = 22.11dB, the best
possible result by the thresholding procedure leads to PSNR = 27.33dB.
14.2.2 Various Improvements
In order to further improve the performance of these methods, various additional
ideas and tools have been proposed. One such simple but valuable idea, is the as-
signment of a diﬀerent variance to each non-zero coeﬃcient in the representation.
This stems from the observation that the magnitude of typical sparse representa-
tions of images show a varying variance per scale and orientation. Plugged into the
Basis-Pursuit formulation, this leads to the problem
ˆx = min
x
λ ∥Wx∥1 + 1
2∥Ax −y∥2
2,
(14.5)
where W is a diagonal matrix that contains the inverse of the variances chosen, just
as described earlier in Equation (14.4). We have seen in Chapter 3 that this structure
implies a variation of the norms of the atoms in the dictionary A. The diagonal of
W can be gathered oﬀ-line by exploring the general behavior of the representations
obtained for various images. Alternatively, these weights can be learned on-line
from the given image, as proposed by Chang, Yu, and Vetterli, and then plugged to
the solver.
Other, more involved, ideas that lead to improved denoising performance are
those which somehow tie the representation coeﬃcients one to the other. For exam-
ple, assume that we have the estimated variances of the entries in the representation

14.2 The Beginning: Global Modeling
277
Fig. 14.2 The original image (top-left), the noisy one with σ = 20 grey-values (top-right), and the
denoised one obtained by the thresholding algorithm with the redundant Haar dictionary (bottom).
The input image quality is PSNR = 22.11dB, and the result shown here has PSNR = 27.33dB.
x. Also, for each entry xi in the representation we deﬁne its neighborhood N(i) (e.g.,
near-by entries in the spatial domain having similar orientation and scale). The for-
mulation
ˆx = min
x
λ
X
i

X
j∈N(i)
 xj
σj
!2
1
2
+ 1
2∥Ax −y∥2
2,
(14.6)
replaces the weighted ℓ1-norm with a new ℓ1 −ℓ2 mixed measure that promotes
neighborhood groups of entries to behave similarly (either all the elements in the
neighborhood are zeros, or all become active). For the case where the neighborhood
contains only the center entry, this formulation coincides with the one posed in
Equation (14.5).

278
14 Image Denoising
While we do not explore this method here, we mention that this technique pro-
poses to exploit the known correlation that exists between the absolute values of
neighboring coeﬃcients, in order to constrain the denoising algorithm towards a bet-
ter behaved image. The same motivation stands behind more complicated denoising
schemes such as the Bayesian Least-Squares method with the use of the Gaussian
Scale-Mixture (GSM) by Portilla et. al. (which leads to a PSNR of 30.32dB for the
Barbara test introduced above). While these techniques indeed show better perfor-
mance, they lose some of their appeal due to their overall complicated structure. As
we shall see next, alternative simple and intuitive image denoising algorithms can
be proposed, while leading to even better results.
To conclude this discussion, we observe that all the above is done while imposing
a global model on the image x. One major drawback with such a model is the need
to provide a dictionary that sparsiﬁes images as a whole. An appealing alternative
to this global approach is a local processing of the image by operating on small
patches. As we show next, this will allow us to introduce trained dictionaries, learn
the shrinkage curve, and propose simple algorithms that are easily implemented and
are highly parallelizable.
14.3 From Global to Local Modeling
14.3.1 The General Methodology
Instead of working on the whole image at once, we can consider small image patches
p of size √n× √n pixels, where n ≪N. We organize these patches lexicographically
as column vectors p ∈IRn. We shall assume that these patches follow the Sparse-
Land model M(A, ϵ, k0). This implies that there exists a dictionary A ∈IRn×m (m ≥
n), such that every patch p has a corresponding representation q ∈IRm satisfying
∥Aq −p∥2 ≤ϵ, with ∥q∥0 = k0. At the moment we shall assume that A is known and
ﬁxed.
The above model assumption imposes a prior on images in terms of the behavior
of their local patches everywhere. How can this be used for image denoising? One
natural option is to denoise each of those patches separately and tile the results.
However, in doing so, visible artifacts may occur on block boundaries. One could
also propose to work on overlapping patches and average the results in order to
prevent such blockiness artifacts, as indeed practiced in various recent contributions.
As an example for such a local denoising approach, consider the following al-
gorithm, as initially proposed by Guleryuz and later improved by others. The dic-
tionary to be used is the unitary 2D-DCT, which leads to two important beneﬁts:
(i) multiplication by A and its adjoint are fast (both because of separability, and the
relation to FFT); and (ii) sparse-coding with this dictionary is simple and direct.
Denoising the complete image y can be done by the following stages.

14.3 From Global to Local Modeling
279
1. Consider every pixel in y as a center of a patch of size √n × √n (typical value
of n is 64). We denote these patches as pij.
2. Apply denoising of each such patch, pij, using the thresholding algorithm:
a. Compute ˜qij = ATpi j (the forward 2D-DCT transform).
b. Pass the obtained vector ˜qij through a hard-thresholding operation with a
pre-speciﬁed threshold (dependent on the noise power, and possibly diﬀerent
for each entry) and obtain ˆqij.
c. Compute ˆpij = Aˆqij. This is the denoised version of the patch.
3. Merge the denoised patches by averaging them one on top of the other. There is
also a possibility to use a weighted averaging to gain a further improvement, but
we shall not explore this option here.
This simple algorithm seems to be surprisingly eﬀective for image denoising, and
in the next subsections we shall explore several further improvements of it. Interest-
ingly, here and elsewhere, the overlaps between the patches are found to be crucial
for the good performance of the algorithm, and not just because of the blockiness
eﬀects. The obtained averaging has the same ﬂavor as the averaging proposed in
Chapter 11 for approximating the MMSE estimator.
14.3.2 Learning the Shrinkage Curves
The ideas presented in this sub-section are taken from the recent work by Yakov
Hel-Or and Doron Shaked.
The patch denoising process in the above-described algorithm can be written as
the expression
ˆpij = Aˆqij = AS
n
ATpij
o
,
(14.7)
where S represents the element-wise shrinkage applied on the coeﬃcients in ˜qij.
Assume that we hold a large corpus of clean image patches, {p0
k}M
k=1, to train on.
We create from these a new set of their noisy versions, {pk}M
k=1, simply by adding
white additive Gaussian noise with the pre-speciﬁed variance σ2. Using these M
pairs, {p0
k, pk}M
k=1, we aim to ﬁnd the optimal shrinkage rule, replacing the hard-
thresholding practiced above, and also ﬁnding the best set of threshold parameters
in general. We start with the following penalty function,
Flocal (S) =
M
X
k=1
p0
k −AS
n
ATpk
o
2
2 .
(14.8)
In this penalty function, which is a function of the shrinkage operation, we aim to
apply the proposed algorithm on the noisy patches, and get as close as possible to
the clean (original) ones.

280
14 Image Denoising
We shall assume hereafter that the shrinkage operation is applied diﬀerently to
every element in the input vector. This is a natural assumption, since the near-DC
coeﬃcients are less sensitive to noise compared to the high-AC ones, and as such,
they should go through a less aggressive shrinkage operation. Thus, the operation
S{u} on an arbitrary vector u ∈IRm is broken into
S{u} =

S1{u[1]}
S2{u[2]}
...
Sm{u[m]}

,
(14.9)
where each Si may operate diﬀerently. Returning to the penalty function in Equation
(14.8), and denoting uk = ATpk, it can be written as
Flocal (S1, S2, . . . , Sm) =
M
X
k=1
p0
k −AS
n
ATpk
o
2
2
(14.10)
=
M
X
k=1
p0
k −
m
X
i=1
aiSi {uk[i]}

2
2
.
Note that if the dictionary used is unitary, and we introduce the notations u0
k = ATp0
k
and uk = ATpk, this penalty function can be written alternatively as
Flocal (S1, S2, . . . , Sm) =
M
X
k=1
p0
k −AS
n
ATpk
o
2
2
(14.11)
=
M
X
k=1
ATp0
k −S
n
ATpk
o
2
2
=
M
X
k=1
u0
k −S {uk}
2
2
=
m
X
i=1
M
X
k=1

u0
k[i] −Si {uk[i]}
2 .
This reveals that the m shrinkage curves can be learned independently of each other,
as we have obtained m decoupled penalties, each handling one shrinkage curve.
Returning to the more general case, the next step we take is a proper representa-
tion of the shrinkage operator, so as to optimize its curve. We propose a polynomial
of the form
Si{u} =
J
X
j=0
ci[ j]u j,
(14.12)

14.3 From Global to Local Modeling
281
and thus the choice of {ci[ j]}J
j=0 characterizes this operator fully. The advantage in
this representation is the fact that when plugged to the penalty function in Equa-
tion (14.10), the overall expression admits a simple quadratic form, for which plain
Least-Squares can ﬁnd the optimal solution. There are other representations that can
be proposed for Si{u}, such as perfect anti-symmetric polynomial with odd powers
only, a spline representation, and more. In their work, Hel-Or and Shaked propose a
piecewise linear curve, in which the connecting points are trained.
Returning to our formulation and plugging in the polynomial model of the shrink-
age, the penalty we consider becomes
Flocal (S1, S2, . . . , Sm) =
M
X
k=1
p0
k −
m
X
i=1
aiSi {uk[i]}

2
2
(14.13)
=
M
X
k=1

p0
k −
m
X
i=1
ai
J
X
j=0
ci[j]uk[i] j

2
2
.
We proceed by introducing the following matrix notation, which will enable easy
minimization of the above penalty. We deﬁne the vector c ∈IRmJ as the vector that
contains all the {ci[ j]}J
j=0 series, for i = 1, . . . , m. Thus
cT = [c1[0], c1[1], . . . , c1[J], c2[0], c2[1],
. . . , cm[0], cm[1], . . . , cm[J]] .
Similarly, we deﬁne the matrix Uk ∈IRm×mJ, as a block-diagonal matrix with m
blocks,
Uk =

bk[1]
0
· · ·
0
0
bk[2] · · ·
0
...
...
...
0
0
bk[m]

.
(14.14)
Each of the blocks bk[i] is of size 1 × J, with the content
bk[i] = [uk[i]0, uk[i]1, . . . , uk[i]J], for i = 1, 2, . . . , m.
Using these notations, the function we aim to minimize is given by
Flocal (c) =
M
X
k=1
p0
k −AUkc
2
2 .
(14.15)
The optimal set of parameters that deﬁne the shrinkage curves is given by
∂Flocal (c)
∂c
= 0 =
M
X
k=1
UT
k AT 
AUkc −p0
k

,
(14.16)
which leads to

282
14 Image Denoising
Fig. 14.3 A 200 × 200 portion of the image Lena, on which we train the shrinkage curves.
copt =

M
X
k=1
UT
k ATAUk

−1
M
X
k=1
UT
k ATp0
k.
(14.17)
Once the curves have been trained, the denoising algorithm can be applied on new
images that are suﬀering from an additive noise with the same power σ. If these
images are similar to the ones the curves were trained on, the performance of the
overall denoising is expected to be very good.
To demonstrate this algorithm in practice, we present the results of the following
experiment. We extract all (200 −5)2 patches of size 6 × 6 (n = 6) from a 200 × 200
portion of the image Lena, as shown in Figure 14.3, and its noisy version with
σ = 20. Those patch-pairs are used to train 36 shrinkage curves, which are to applied
on the unitary 2D-DCT coeﬃcients of these patches. The objective is the very same
one posed in Equation (14.15), and we exploit the fact that the diﬀerent curves
can be optimized separately and independently. Because of numerical reasons, we
operate on the input patches after normalization of the form (pk −127)/128. The
obtained shrinkage curves are presented in Figure 14.4. We see that the near-DC
(the top-left corner graphs) coeﬃcients are hardly touched, while some of the high
frequencies are strongly suppressed.
Once these curves have been obtained by training, we may apply them to other
images, in order to remove noise of similar strength (σ = 20). Figure 14.5 presents
the outcome obtained on the image Barbara. Note that since the shrinkage curves
have been parameterized as polynomials, their behavior for large entries may be
completely distorted. As such, an input value outside the training range may be
violently magniﬁed. We treat this sensitivity by checking whether the input is within
the learning data range, and if not, we do not modify it at all.
Notice that the result obtained here is approximately 1dB better than the result we
have shown for the global thresholding algorithm. We may wonder how rich is the
training data we use and what is the eﬀect it has on the result obtained. Repeating the
same experiment, this time training on the complete image Barbara, the result is

14.3 From Global to Local Modeling
283
−4−2 0 2 4
−4
−2
0
2
4
−2
0
2
−2
0
2
−1 0 1
−1
0
1
−1
0
1
−1
0
1
−0.5 0 0.5
−0.5
0
0.5
−0.5 0 0.5
−0.5
0
0.5
−2
0
2
−2
0
2
−1 0 1
−1
0
1
−1
0
1
−1
0
1
−0.5 0 0.5
−0.5
0
0.5
−0.5 0 0.5
−0.5
0
0.5
−0.5 0 0.5
−0.5
0
0.5
−1 0 1
−1
0
1
−1
0
1
−1
0
1
−1
0
1
−1
0
1
−0.5 0 0.5
−0.5
0
0.5
−0.5 0 0.5
−0.5
0
0.5
−0.5 0 0.5
−0.5
0
0.5
−1
0
1
−1
0
1
−1
0
1
−1
0
1
−0.5 0 0.5
−0.5
0
0.5
−0.5 0 0.5
−0.5
0
0.5
−0.5 0 0.5
−0.5
0
0.5
−0.5 0 0.5
−0.5
0
0.5
−1
0
1
−1
0
1
−0.5 0 0.5
−0.5
0
0.5
−0.5 0 0.5
−0.5
0
0.5
−0.5 0 0.5
−0.5
0
0.5
−0.5 0 0.5
−0.5
0
0.5
−0.5 0 0.5
−0.5
0
0.5
−0.5 0 0.5
−0.5
0
0.5
−0.5 0 0.5
−0.5
0
0.5
−0.5 0 0.5
−0.5
0
0.5
−0.5 0 0.5
−0.5
0
0.5
−0.5 0 0.5
−0.5
0
0.5
−0.5
0
0.5
−0.5
0
0.5
Fig. 14.4 The 36 shrinkage-curves obtained by training using the penalty posed in Equation
(14.15). Each of these curves corresponds to one of the unitary-2D-DCT transform coeﬃcients.
PSNR = 28.75, which is indeed better, but not by far. This validates that the training
data from Lena is indeed suﬃcient.
Interestingly, despite the deviation from the original problem handled here, this
paradigm can be used for other inverse problems, such as deblurring and deblocking
of JPEG images. If we assume that the algorithm can cope with these disturbances,
then this training method will be able to ﬁnd the proper curves for best behavior.
One important ingredient is still missing in the above description, and this is the
fact that the overall denoised image is created by averaging the overlapped patches.
Thus, it would be better to optimize the shrinkage curves with respect to an error
between the original complete image and its estimated version, rather than consider
the intermediate errors in each patch. Formulating this is somewhat more tricky, but
still manageable.
We start with the assumption that a training image y0 is given to us, containing
M pixels. We extract all its patches {p0
k}M
k=1 with overlaps, to serve the training pro-
cedure. The enumeration k stands for a speciﬁc position [i, j] in the image, such that
there is a one-to-one correspondence between these two indexing options. For the
training procedure, we also need a noisy version of these patches, {pk}M
k=1. These are

284
14 Image Denoising
Fig. 14.5 The original image (top-left), the noisy one with σ = 20 grey-values (top-right), and the
denoised one obtained by the patch-based unitary-2D-DCT learned shrinkage (bottom). The input
image quality is PSNR = 22.11dB, and the result shown here has PSNR = 28.36dB.
obtained by extracting all the patches from a noisy version of y0, contaminated by
white additive Gaussian noise with power σ.
We deﬁne the operator Rk ∈IRn×M as the operator that extracts the k-th patch, by
p0
k = Rky0. The transpose of this operation puts back a patch into the k-th position
in the constructed image, padded with zeros elsewhere. Thus, returning to Equation
(14.8), the proper penalty should be
Fglobal (S) =
y0 −1
n
M
X
k=1
RT
k AS
n
ATRky
o

2
2
.
(14.18)

14.3 From Global to Local Modeling
285
Note that the division by n corresponds to the fact that in every pixel there are n
contributions due to the full-overlaps,3 and those are averaged. However, this coef-
ﬁcient can be absorbed into the unknown vector c, and thus removed. Following the
same treatment as presented above, we obtain the penalty function
Fglobal (S) =
y0 −
M
X
k=1
RT
k AUkc

2
2
.
(14.19)
The optimal set of shrinkage parameters is given by
copt =


M
X
j=1
UT
j ATR j


M
X
k=1
RT
k AUk


−1
(14.20)
·

M
X
k=1
UT
k ATRk
y0.
A simulation of this algorithm, following closely with the description of the pre-
vious experiment, leads to better results. Training on the same database of patches
as above, but this time targeting the minimization of the penalty posed in Equation
(14.19), the obtained curves are very diﬀerent, as seen in Figure 14.6. An interesting
phenomenon is obtained in some of the curves found – those show an over-shrinkage
of small entries, such that these are not only nulled, but in fact multiplied by a neg-
ative factor to reverse their eﬀect.
Applying these curves in the very same manner as before on the image Barbara,
we obtain a further improved result with PSNR = 29.05dB (for a patch size of 8 × 8
pixels, the result is PSNR = 29.41dB).4 This result is shown in Figure 14.7.
The appeal in the above described patch-based image denoising algorithm is the
eventual simplicity with which an incoming image is treated. Another beneﬁt this
scheme puts forward is the non-iterative, local, and independent processing that the
image patches undergo in the cleaning process. This implies that hardware imple-
mentation of this algorithm can beneﬁt greatly from parallel processing.
One could think of various ways to further improve these results, and among
those, it is most natural to propose to optimize the performance by modifying the
dictionary A. While we shall not take this path here, this brings us to the next sub-
section, where we introduce an image denoising algorithm which targets the design
of the dictionary as its primary goal.
3 And assuming a cyclic boundary condition, implying that every pixel in y0 serves as a top-left
corner of a corresponding patch, and when the patch falls outside the image, a periodic extension
of the image is assumed.
4 The result that corresponds to this experiment, as reported by Hel-Or and Shaked in their paper, is
≈30dB. The diﬀerence should be attributed to the diﬀerences in the patch size used, the shrinkage-
curve representation, and the training data.

286
14 Image Denoising
−4−2 0 2 4
−4
−2
0
2
4
−2
0
2
−2
0
2
−1
0
1
−1
0
1
−0.500.5
−0.5
0
0.5
−0.5 0 0.5
−0.5
0
0.5
−0.5 0 0.5
−0.5
0
0.5
−2
0
2
−2
0
2
−1 0 1
−1
0
1
−1
0
1
−1
0
1
−0.5 0 0.5
−0.5
0
0.5
−0.5 0 0.5
−0.5
0
0.5
−0.5 0 0.5
−0.5
0
0.5
−1 0 1
−1
0
1
−1
0
1
−1
0
1
−1
0
1
−1
0
1
−0.5 0 0.5
−0.5
0
0.5
−0.5 0 0.5
−0.5
0
0.5
−0.5 0 0.5
−0.5
0
0.5
−1
0
1
−1
0
1
−1
0
1
−1
0
1
−0.5 0 0.5
−0.5
0
0.5
−0.5 0 0.5
−0.5
0
0.5
−0.5 0 0.5
−0.5
0
0.5
−0.5 0 0.5
−0.5
0
0.5
−0.5 0 0.5
−0.5
0
0.5
−0.5 0 0.5
−0.5
0
0.5
−0.5 0 0.5
−0.5
0
0.5
−0.5 0 0.5
−0.5
0
0.5
−0.5 0 0.5
−0.5
0
0.5
−0.5 0 0.5
−0.5
0
0.5
−0.5 0 0.5
−0.5
0
0.5
−0.5 0 0.5
−0.5
0
0.5
−0.5 0 0.5
−0.5
0
0.5
−0.5 0 0.5
−0.5
0
0.5
−0.5 0 0.5
−0.5
0
0.5
−0.5 0 0.5
−0.5
0
0.5
Fig. 14.6 The 36 shrinkage-curves obtained by training using the penalty posed in Equation
(14.19), which takes into account the overlaps between the patches.
14.3.3 Learned Dictionary and Globalizing the Prior
The local algorithm described in the above sections (with or without the shrinkage
curve learning) can be modiﬁed by using any redundant dictionary for the image
patches, and by replacing the thresholding method with OMP or BP. We now turn
to describe an algorithm that essentially leads to this approach in a systematic way.
This follows closely with the work of Aharon and Elad.
Assuming that our knowledge on the unknown complete image y0 is fully ex-
pressed in the fact that every patch in it should belong to the M(A, ϵ, k0)-Sparse-
Land model, the MAP estimator that takes this prior into account would be
n
{ˆqk}M
k=1, ˆy
o
= arg min
z,{qk}k
λ∥z −y∥2
2 +
(14.21)
M
X
k=1
µk∥qk∥0 +
M
X
k=1
∥Aqk −Rkz∥2
2.

14.3 From Global to Local Modeling
287
Fig. 14.7 Top-left - the original image, and Top-Right - the noisy image. The denoising results of
the two experiments are given at the bottom: Left - by training on patches directly and therefore
disregarding the overlaps between the patches (PSNR = 28.36dB); Right - by training the shrinkage
curves on the overall denoising error, taking the overlaps into account (PSNR = 29.05dB).
In this expression the ﬁrst term is the log-likelihood global force that demands the
proximity between the measured image, y, and its denoised (and unknown) version
z. The second and the third terms are the image prior that makes sure that in the
constructed image, z, every patch pk = Rkz of size √n × √n in every location
(thus the summation by k) has a sparse representation with bounded error. As for
the coeﬃcients µk, those must be location dependent, so as to comply with a set of
constraints of the form ∥Aqk −pk∥2 ≤ϵ. As we shall see next, these coeﬃcients are
not needed, and are replaced by known quantities.
When the underlying dictionary A is assumed known, the proposed penalty term
in (14.21) has two kinds of unknowns: the sparse representations ˆqk per each loca-
tion, and the overall output image z. Instead of addressing both together, we propose
a sub-optimal approach of block-coordinate minimization algorithm that starts with
an initialization z = y, and then seeks the optimal ˆqk. In doing so, we get a complete

288
14 Image Denoising
decoupling of the minimization task to M smaller ones, each of the form
ˆqk = arg min
q
µk∥q∥0 + ∥Aq −pk∥2
2,
(14.22)
handling one image patch. Approximating the solution to this problem using the
OMP is easy, gathering one atom at a time, and stopping when the error ∥Aq −pk∥2
2
goes below cnσ2, for some pre-chosen value of the parameter c. This way, the choice
of µk has been handled implicitly. Thus, this stage works as a sliding window sparse
coding stage, operated on each block of size √n × √n at a time.
Given all ˆqk, we can now ﬁx those and turn to update z. Returning to Equation
(14.21), we need to solve
ˆy = arg min
z
λ∥z −y∥2
2 +
M
X
k=1
∥Aˆqk −Rkz∥2
2.
(14.23)
This is a simple quadratic term that has a closed-form solution, given by
ˆy =
λI +
M
X
k=1
RT
k Rk

−1 λy +
M
X
k=1
RT
k Aˆqk
.
(14.24)
This rather cumbersome expression may mislead, as all it suggests is that averaging
of the denoised patches is to be performed, with some relaxation obtained by aver-
aging with the original noisy image. The matrix to invert in the above expression is
a diagonal one, and thus the calculation of (14.24) can be also done on a pixel-by-
pixel basis, following the previously described sliding window sparse coding step.
So far we have seen that the obtained denoising algorithm calls for sparse coding
of small patches, and an averaging of their outcomes. However, if minimization
of (14.21) is our goal, then this process should proceed. Given the updated z, we
can repeat the sparse coding stage, this time working on patches from the already
denoised image. Once this is done, a new averaging should be calculated, and so on
and so forth. The problem with this approach is the fact that the additive noise in
the treated image is no longer white nor Gaussian, and OMP may be inadequate in
handling it. Furthermore, even if we intend to use the OMP, the threshold to use is
unknown, as the noise has been partially removed.
The denoising method developed above assumes that A is known. This may sug-
gest that we should form a library of many clean image patches and train a dictionary
oﬀ-line, using MOD or K-SVD, and then plug it into the above scheme. The results
of such an experiment are reported below. A 64 × 256 dictionary has been trained
(using 180 iterations of the K-SVD) over 100, 000 patches of size 8 × 8 pixels, ex-
tracted from various images. This dictionary is shown in Figure 14.8. Plugged to the
above denoising scheme with c = 1.15 (see above), the denoised image obtained
with λ = 0.5 is shown in Figure 14.9, and its PSNR is 28.93dB.
While this approach performs reasonably well, one could do much better. A more
ambitious goal is to develop a dictionary adapted to the image at hand, by learn-
ing the dictionary from the noisy image itself! This may yield sparser representa-

14.3 From Global to Local Modeling
289
Fig. 14.8 A globally trained K-SVD dictionary for general images.
Fig. 14.9 The denoising result when using the globally trained dictionary – PSNR = 28.93dB.
tions and a more eﬀective denoising strategy. An interesting property of dictionary-
learning algorithms is their robustness to noise, which implies that learning from the
noisy image is likely to lead to a well-behaved and nearly noise-free dictionary.
An elegant way to lead to this very algorithm is to return to the MAP penalty in
Equation (14.21), and consider A as unknown as well. Proceeding with the Block-
Coordinate-Relaxation method, we start by ﬁxing z = y, and initializing A somehow
(e.g., redundant DCT, as described in Chapter 12).
Updating the sparse representations of the patches, qk is done as before, by a
sliding windowed OMP. Once done, the dictionary can be updated using either the
MOD or the K-SVD method. Iterating between these two steps leads to a dictionary-
learning algorithm that eﬀectively works on the noisy patches of the image y. Once

290
14 Image Denoising
Fig. 14.10 The redundant DCT dictionary, which is used as initialization tothe K-SVD, and the
K-SVD dictionary trained on the noisy Barbara image directly, as obtained after 15 iterations.
converged (after such ∼10 rounds), the ﬁnal denoising image can be computed,
just as done in Equation (14.24). This way, we have merged the dictionary learning
into the actual denoising process. The performance of this method is superior to the
previous oﬀ-line training procedure, as we now demonstrate.
The results for this adaptive approach are summarized in Figures 14.10 and
14.11, following the same experiment outline as for the globally trained dictionary
described above. Figure 14.10 shows the initial (separable) redundant 2D-DCT dic-
tionary, and the one obtained after 15 iterations of K-SVD reﬁnement steps. We see
that the adapted dictionary contains atoms that correspond to the textured regions
in the image Barbara, indicating that the dictionary adapts well to the content of
interest.
Looking closely at the K-SVD dictionary in Figure 14.10 (right), it appears that
some of the atoms obtained are very noisy and uninformative. At best, this may
imply that these atoms are useless and are never used for representing the image
patches. At worst, these atoms may become involved in these representations, and
then noise is injected to the image. In any case, this suggests that we may use a
lower redundancy in A, while keeping the quality of the denoising results, or even
improving it.
Figure 14.11 shows the denoised images obtained with these two dictionaries.
Even the initial dictionary gives a very good result of PSNR = 30.01dB. This re-
sult is even better than the globally trained dictionary, because, as opposed to this
globally trained dictionary, the redundant DCT contains oscillating atoms that ﬁt
well with the large textured areas in Barbara. The denoising result is further im-
proved to PSNR = 30.98dB with the K-SVD modiﬁed dictionary, demonstrating the
eﬃciency of this model and its adaptation to the data it operates on.

14.3 From Global to Local Modeling
291
Fig. 14.11 Top: Original (left) and noisy image (right). Bottom: Denoised results using (i) the
initial separable 2D-DCT dictionary (left), with PSNR = 30.01dB; and (ii) the K-SVD dictionary
after 15 iterations (right), with PSNR = 30.98dB.
As a side note, we draw the reader’s attention to to the following observation:
When considering the initial redundant 2D-DCT dictionary, the algorithm that leads
to the result in Figure 14.11 (left) is very similar to the one discussed in previous
sections (the Hel-Or–Shaked method, or even its predecessor by Onur Guleryuz).
Indeed, the diﬀerences are in using a redundant matrix A (instead of the unitary 2D-
DCT), and applying OMP instead of plain thresholding. As we can see, with these
two modiﬁcations, we were able to boost the denoising performance quite markedly,
even without trying to shape the shrinkage curves.
We conclude this section by drawing attention to two extensions of the above
K-SVD denoising algorithms, the ﬁrst to color images, and the second to video. In
both cases, we do not dive into the ﬁne details of these extensions, and instead aim
to present the qualitative ideas that enable them.
Handling color images can be done by converting the image from RGB to YCbCr
(or other color representations that separate the intensity and the color details). In

292
14 Image Denoising
the new domain, the K-SVD denoising can be used on the intensity image, while ﬁl-
tering the color layers can be done using a simpler approach – even plain smoothing.
The approach taken by Mairal, Elad, and Sapiro (2008) is diﬀerent and better: The
very same K-SVD denoising algorithm is employed directly on the color image, by
operating on 3D patches that contain the three color layers (R, G, and B). This way,
the learned dictionary contains color atoms, and all the treatment as described above
remains the same. A small modiﬁcation, however, is introduced into the ℓ2 distance
measure between patches used in the OMP, in order to avoid color artifacts. Figure
14.12 shows the results obtained from such an algorithm, operating on patches of
size 6 × 6 × 3.
Turning to video, which is a sequence of frames, the above K-SVD denoising
algorithm can be applied to each frame independently. However, one could do far
better if adjacent frames are processed jointly. One way to incorporate information
between frames is to consider the complete spatio-temporal volume of the video
as a 3D image, and operate on it this way. This implies that the K-SVD algorithm
processes small 3D patches, and similarly, the atoms in the trained dictionary are
3D patches as well. A further improvement of this scheme can be proposed, where
the trained dictionary is adapted to each frame in the sequence. In this case, the
denoising is performed on each frame sequentially (thus giving an on-line ﬁltering
algorithm, rather than a batch denoising mode of operation), and the dictionary is
trained on 3D patches extracted from the current frame and its neighbors. While this
may seem like a much more demanding algorithm, the dictionary can be propagated
from the previous frame and updated with much fewer iterations to better serve the
current frame. The results of such an algorithm (operating on patches of size 5×5×5
pixels) are shown in Figure 14.13 for the Garden sequence. We should note that the
single-frame K-SVD denoising on this sequence leads to an inferior outcome with
a diﬀerence of 1.8dB. More details on this algorithm and its performance are found
in the paper by Protter and Elad (2009).
14.3.4 The Non-Local-Means Algorithm
The above K-SVD image denoising algorithm relies on a learning procedure that
works directly on the corrupted image. Since there are M patches in the image, and
as we assume that m ≪M, this implies that in this training process the noise is
damped and hardly eﬀects the obtained dictionary.
We could take this adaptivity idea further, and propose an adapted dictionary per
each area in the image. Segmenting the image into parts, and training a diﬀerent
dictionary per each, could lead to improved results. Indeed, in such an approach we
could use reduced redundancy, as it is no longer required. Taken to the extreme, we
could train a diﬀerent dictionary per every pixel, using the patches in its neighbor-
hood. However, this clearly implies a very high computational load. Therefore, we
shall propose a simpler approach with a similar eﬀect.

14.3 From Global to Local Modeling
293
Fig. 14.12 Top: The original (left) and the noisy images (right) – PSNR = 20.17dB. Bottom: Color
K-SVD denoised result(left) – PSNR = 30.83dB, and a zoomed-in portion of the result (right).

294
14 Image Denoising
Fig. 14.13 Video denoising using the K-SVD algorithm. Left: The original 10th and 20th frames,
Middle: These noisy frames (σ = 50, PSNR = 14.88dB), Right: Denoised results (PSNR
= 22.80dB).
Assume that per every pixel k we accumulate the surrounding patches in locations
k ∈N(k), and form a dictionary from them (without any training). The size of this
dictionary is n × m, where m = |N(k)|. We denote this dictionary as Ak, and assume
that the center patch for which this dictionary has been constructed is pk = Rky.
We shall assume a sparse representation of pk with respect to the dictionary Ak.
However, in order to improve the denoising performance, we could approximate
the MMSE estimator by proposing several such representations and averaging them
with proper weights. Those weights would be exactly those developed in Chapter 11.
This leads to a variant of the Non-Local-Means (NLM) algorithm. Indeed, it aligns
perfectly with the NLM if the above-mentioned sparse representations contain only
one atom, forced to have a unit coeﬃcient, and if after the averaging we adopt only
the center pixel and throw the rest.
The NLM algorithm was introduced by Coll, Buades, and Morel, as an extension
of the bilateral-ﬁlter, and the above description is far from the way it was conceived.
The NLM proposes a denoising of an image y by performing a local weighted aver-
age, of the form
ˆyi j =
P
[k,l]∈N(i,j) wij[k, l]y[i −k, j −l]
P
[k,l]∈N(i,j) wij[k, l]
.
(14.25)
The key for the success of the NLM is the choice of the weights. Those are computed
by
wij[k, l] = exp
−∥Ri, jy −Ri−k, j−ly∥2
2
2T
.
(14.26)

14.3 From Global to Local Modeling
295
Fig. 14.14 The noisy image and its denoised result using the NLM ﬁlter, with PSNR = 29.59dB.
Thus, the NLM amounts to a weighted average of surrounding pixels, with weights
that are built inversely proportional to the ℓ2 distance between the patches centered
around the two pixels to be merged. This process can be applied iteratively, improv-
ing the above result.
The denoising result of the image Barbara (additive noise with σ = 20, as in the
rest of this chapter) using the NLM is given in Figure 14.14. The patch-size used
is 7 × 7 pixels, the neighborhood size is 17 × 17 pixels, and T = 200. The result
obtained gives a PSNR of 29.59dB. While this is not necessarily the best possible
set of parameters for the NLM, it is not far from extracting the most from this ﬁlter.
The above interpretation of the NLM as an extreme case of sparse representation
and MMSE estimation gives rise to many possibilities and improvements. Those
include options such as improving the local dictionary, allowing more atoms in every
representation, using the complete patch and not just its center, and more.
Before we conclude the discussion on the NLM, we should note that the interpre-
tation given here for this algorithm is not the only one possible. Ties to other, more
classical, techniques in image processing are also possible. One connection which
we mentioned above exists between the NLM and the bilateral-ﬁlter. The bilateral-
ﬁlter can be regarded as a special case of the NLM when using patches of size 1 × 1
pixel. This ﬁlter also adds an exponential damping eﬀect to the weights as a func-
tion of the distance between the center pixel and the one averaged with it. This in
turn is closely related to the Beltrami-ﬂow image denoising algorithm, as proposed
by Kimmel, Malladi, and Sochen. In fact, as Spira et. al. show, the bilateral-ﬁlter
is the eﬀective short-time kernel of the Beltrami algorithm. Similarly, one could tie
the NLM directly to a variant of the Beltrami framework, which performs the diﬀu-
sion operation on the image in a diﬀerent feature-space, so as to accommodate the
patch-processing that is used.

296
14 Image Denoising
Fig. 14.15 The noisy image and its denoised result using the BM3D ﬁlter, with PSNR = 31.78dB.
14.3.5 3D-DCT Shrinkage: BM3D Denoising
As a last algorithm to be presented within this family of local patch-based processing
that use sparse representations, we introduce a recently proposed method by Dabov,
Foi, Katkovnik, and Egiazarian – the Block-Matching 3D (BM3D) algorithm. This
algorithm shares some similarities with the NLM and the other algorithms men-
tioned above. We shall not dive into the very ﬁne details of this method, and instead
concentrate on its main theme. We note that to date, this is among the best per-
forming algorithms for image denoising, although the others mentioned are not far
behind. Figure 14.15 shows the denoising result of Barbara, removing an additive
noise with σ = 20, and as can be seen, this is the best result shown in this chapter,
with PSNR = 31.78dB.
The BM3D algorithm operates on every pixel from y, and performs the same
denoising procedure explained below: Consider the k-th pixel (which corresponds
to a speciﬁc location [i, j]), and extract the patch pk of size √n× √n centered around
it. The denoising process starts by ﬁnding a set Sk of similar patches throughout the
image. Similarity is measured by the ℓ2-norm (a block-matching process). In order
to reduce the bad eﬀect of noise, the distance computation is applied on denoised
versions of the patches, where denoising is done by the same thresholding algorithm,
as presented in an earlier section of this chapter.
Once found, these |Sk| patches are stacked together to form a cube of size
√n × √n × |Sk|. Joint denoising on this volume is done by using the very same
thresholding algorithm, only this time with the 3D-DCT transform. The idea is that
the high correlation between the patches and the spatial redundancy should both
lead to strong sparsity of the 3D-DCT coeﬃcients. After this (second) thresholding,
an inverse 3D-DCT is applied and we obtain a cleaned version of this set of patches.

14.4 SURE for Automatic Parameter Setting
297
The cleaned patches are returned to their original locations in a destination can-
vas, and averaged with other patches that overlap them. This process is repeated
for every pixel, such that every patch may be cleaned several times, and with sev-
eral contexts. The averaging between the overlapped patches is done using weights
that are inversely proportional to the number of non-zeros that each patch got in the
thresholding stage. This way, highly sparse patches get higher priority in the overall
solution.
Although diﬀerent from the aforementioned algorithms, there are some clear
ties and similarities. Processing patches, the thresholding algorithm used, block-
matching as in the NLM, the overlaps averaging, and more, indicate that the BM3D
indeed belongs to the same family of denoising techniques. Furthermore, based on
what we have already seen, it is natural to propose improvements for this method,
such as shrinkage curve learning, dictionary-learning, and more.
As said above, the BM3D is among the best performing algorithms for image
denoising. However, with the extensive work on image denoising, it is very likely
that this claim will become wrong in a short while. Indeed, a recent work by Mairal
et. al. proposes a variant of the K-SVD denoising algorithm, which already out-
performs the BM3D. We mention this work because it poses an interesting fusion
between the K-SVD and the BM3D algorithms.
Embarking from the K-SVD denoising, Mairal et. al. suggest the following el-
ementary (but crucial) modiﬁcation: instead of applying sparse coding on each
patch independently, follow the BM3D line-of-reasoning by seeking similar patches,
forming the set Sk, and then apply a joint-sparse coding to these. The joint-sparsity
process forces the diﬀerent patches to decompose using the same subset of atoms.
Once done, each patch is returned to its location, and the averaging resumes as in
BM3D. Two sources of strength in this method boost its performance: (i) the joint-
sparsity poses a further constraint on the atom-decomposition; and (ii) the extra
averaging that is caused by the fact that each patch may serve in several such sets
Sk.
14.4 SURE for Automatic Parameter Setting
Before we conclude this chapter, we turn to discuss a diﬀerent topic, which is of an
utmost importance to all the above algorithms – the need to tune various parameters
in an attempt to get better results. Could there be an automatic method for this task?
In this section we describe the Stein-Unbiased-Risk-Estimator (SURE), which is
indeed an elegant way to answer the above question positively for some problems.

298
14 Image Denoising
14.4.1 Development of the SURE
In many algorithms in image processing there is a set of unknown parameters that
needs tuning. As an example that shall accompany us in this section, consider the
problem posed in Equation (14.4), where the parameter T is unknown. Clearly, this
parameter can be chosen such that ∥ˆy −y∥2 ≈Nσ2, but this does not necessarily
lead to the lowest possible Mean-squared-Error (MSE). Given a noisy input image,
could we propose a method that tunes T automatically so as to optimize the MSE
with respect to the unknown ideal image, or get close to it? At ﬁrst glance, this seems
to be an impossible task, since we do not have access to the true image y0. However,
as we show next, there is a way which comes close to answering positively this
question.
For this task, we introduce the Stein Unbiased Risk Estimator (SURE). It pro-
vides an unbiased estimation formula of the MSE, given an estimator (i.e., denoising
algorithm) ˆy = h(y, θ) and the measurements y. The vector θ stands for the various
parameters we need to set when applying this denoising method.
We operate within the settings of Equation (14.1), where v is an iid white Gaus-
sian noise with known variance σ2. The MSE estimator proposed by Stein, denoted
by η(ˆy, y) = η(h(y, θ), y), is an unbiased estimator of the MSE, implying that
E η(ˆy, y) = E
h
∥ˆy −y0∥2
2
i
.
(14.27)
Thus, if we have an access to the function η(ˆy, y) that estimates the MSE, while
being dependent on θ, we can choose the value of θ so as to minimize this function,
and aim this way at minimizing the true MSE, as a consequence.
To derive the SURE we start from the MSE expression, remembering that our
goal is minimizing it and not estimating its exact value. Thus we can ignore con-
stants that are independent of our estimator ˆy = h(y, θ). Opening the MSE expres-
sion we obtain
E

∥ˆy −y0∥2
2

= E

∥h(y, θ) −y0∥2
2

(14.28)
= E

∥h(y, θ)∥2
2

−2E
h
h(y, θ)Ty0
i
+ ∥y0∥2
2
= E

∥h(y, θ)∥2
2

−2E

h(y, θ)Ty0

+ const.
Only the second element in the equation is a function of the unknown y0, and we
proceed by further treating it. Recalling that y0 = y −v we get
E

h(y, θ)Ty0

= E

h(y, θ)Ty

−E

h(y, θ)Tv

.
(14.29)
Denoting the i-th entry in the vector h(y, θ) by hi(y, θ), and the i-th entry in the noise
vector as vi, this reads
E

h(y, θ)Ty0

= E

h(y, θ)Ty

−
N
X
i=1
E (hi(y, θ)vi)
(14.30)

14.4 SURE for Automatic Parameter Setting
299
= E

h(y, θ)Ty

−
N
X
i=1
Z ∞
−∞
1
√
2πσ
hi(y, θ)vi exp
−v2
i
2σ2
dvi.
We now turn to handle the last term with the integration in the above expres-
sion. Note that the expression hi(y, θ) is a function of vi as well, as yi = y0
i + vi,
and thus it remains inside the integration. We shall assume hereafter that hi(y, θ)
is diﬀerentiable with respect to y. Using integration by parts,
R
u(x)v′(x)dx =
u(x)v(x) −
R
u′(x)v(x)dx, we obtain
E (hi(y, θ)vi) =
1
√
2πσ
Z ∞
−∞
hi(y, θ)vi exp
−v2
i
2σ2
dvi
(14.31)
= −
σ2
√
2πσ
Z ∞
−∞
hi(y, θ)

d
dvi
exp
−v2
i
2σ2

dvi
= −σ
√
2π

hi(y, θ) exp
−v2
i
2σ2


∞
−∞
−
Z ∞
−∞
d
dvi
hi(y, θ) exp
−v2
i
2σ2
dvi
.
Assuming that abs(h(y, θ)) is bounded, the ﬁrst term in Equation (14.31) vanishes,
and we get
E (hi(y, θ)vi) =
σ
√
2π
Z ∞
−∞
dhi(y, θ)
dyi
exp
−v2
i
2σ2
dvi
(14.32)
= σ2E
 dhi(y, θ)
dyi
!
.
Here we have used the fact that dyi/dvi = 1, which implies that dhi(y, θ)/dvi =
dhi(y, θ)/dyi · dyi/dvi = dhi(y, θ)/dyi.
Gathering this result and the ones in Equations (14.28) and (14.30), we ﬁnally
get that
MS E = E

∥h(y, θ) −y0∥2
2

(14.33)
= const + E
h
∥h(y, θ)∥2
2
i
−2E

h(y, θ)Ty

+ 2σ2
N
X
i=1
E
 dhi(y, θ)
dyi
!
= const + E
h
∥h(y, θ)∥2
2 −2h(y, θ)Ty + 2σ2∇· h(y, θ)
i
.
By removing the expectation we obtain an expression that serves as our unbiased
estimation of the MSE, and thus the SURE formula (that estimates the MSE up to a
constant) is given by
η(h(y, θ), y) = ∥h(y, θ)∥2
2 −2h(y, θ)Ty + 2σ2∇· h(y, θ).
(14.34)

300
14 Image Denoising
Note that in the above development we have used the fact that the noise is iid
and Gaussian, and thus the formula obtained is limited to this case. An extension of
SURE has been derived by Y.C. Eldar for the more general scenario y = Hy0 + v,
where H is an arbitrary linear operator, and v is a noise from the exponential family
(for which Gaussian noise is just one special case).
14.4.2 Demonstrating SURE to Global-Threhsolding
In order to demonstrate the use of SURE for parameter setting, we consider the
global thresholding algorithm for the denoising of an image, as described and ex-
perimented in Section 14.2.1. The denoising algorithm is given by (see Equation
(14.4))
h(y, T) = A2ST(AT
1 y),
(14.35)
where θ governs the shrinkage-curve, and we use the notation A1 = AW−1 and
A2 = AW. This denoising algorithm is governed by the single parameter T, which
we shall set using SURE.
For the SURE formula in Equation (14.34), we need to calculate the divergence
of this estimator. We will do this using the equality ∇· h(y, θ) = tr(dh(y, θ)/dy). The
derivative (given as the Jacobian matrix) of h(y, θ) with respect to y is given by
dh(y, T)
dy
= A2S′
T(AT
1 y)AT
1 .
(14.36)
The expression S′
T(AT
1 y) stands for a diagonal matrix. Its main diagonal contains
an element-wise application of the derivative of the shrinkage-curve on the vector
AT
1 y. Here we have conveniently assumed that the shrinkage-curve is diﬀerentiable.
In order to allow this, we use a smoothed version of the hard-thresholding curve,
given by
ST(z) =
zk+1
zk + T k = z ·
 z
T
k
 z
T
k + 1
S′
T(z) = z2k + (k + 1)(zT)k
(zk + T k)2
=
 z
T
2k + (k + 1)
 z
T
k
 z
T
k + 1
2
.
for an even and high value of k. This function is shown in Figure 14.16.
Plugging these into the SURE formula in Equation (14.34), we get the function
η(h(y, T), y) = ∥A2ST(AT
1 y)∥2
2 −2ST(AT
1 y)TAT
2 y +
(14.37)
+2σ2tr(A2S′
T(AT
1 y)AT
1 ).

14.4 SURE for Automatic Parameter Setting
301
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
−2
−1.5
−1
−0.5
0
0.5
1
1.5
2
 
 
k=10
k=20
Hard−Thresholding
Fig. 14.16 The smoothed hard-thresholding curve used within the SURE.
In order to ﬁnd the best T (which minimizes the above expression), we perform an
exhaustive search over a range of possible values, and choose the best one. More
eﬃcient methods could be envisioned, such as the golden-section search (since the
MSE behaves as a unimodal function), but this is not crucial for the exposition given
here.
As for the complexity of computing this expression for a single value of T, it
starts with the denoising algorithm, ﬁrst computing u = AT
1 y, then r = ST(u),
followed by ˆy = A2r. The two ﬁrst expressions in Equation (14.37) require an
additional m operations each (m is the number of atoms in A) to complete.
As for the third term, we should start by computing t = S′
T(u), again requiring
m operations. Using the relations A1 = AW−1 and A2 = AW, we can simplify the
third expression to
tr(A2S′
T(AT
1 y)AT
1 ) = tr(S′
T(AT
1 y)AT
1 A2)
(14.38)
= tr(S′
T(AT
1 y)W−1ATAW)
= tr(WS′
T(AT
1 y)W−1ATA).
Since S′
T(u) is a diagonal matrix, the left and right multiplications by W and W−1
cancel out, leading to
tr(A2S′
T(AT
1 y)AT
1 ) = tr(S′
T(AT
1 y)W2).
(14.39)

302
14 Image Denoising
0
50
100
150
200
20
21
22
23
24
25
26
27
28
Threshold Value T
PNSR
 
 
SURE PSNR
True PNSR
PSNR of y
Fig. 14.17 The PSNR as a function of T using both the SURE formula and a direct computation
using the ground-truth.
This requires a computation of the atoms’ norm, and their element-wise multiplica-
tion by the elements of S′
T(u). Thus, having these norms pre-computed, another set
of m operations builds this expression, and an overall of 4m operations leads to the
evaluation of the complete SURE estimate for a speciﬁc value of T.
In Figure 14.17 we show the PSNR as a function of the parameter T, very simi-
larly to the presentation in Figure 14.1. The dotted curve is obtained using a direct
computation of the PSNR using the clean image. The SURE expression in Equa-
tion (14.37) leads to the continuous curve, which clearly ﬁts very well with the true
PSNR quality. Note that for these two curves to ﬁt so well, we have to add a constant
to the SURE (in the MSE domain), and this was done as part of building this ﬁgure.
It is clear that in this experiment, choosing the peak in the SURE curve leads to the
best denoising result, as if we had access to the ground-truth. This is not always
the case with SURE, and especially not when the number of parameters to be set is
large.
14.5 Summary
The topic of image denoising is very hot, active, and timely, with numerous publi-
cations. The query
Topic=((image and (denoise or denoising))

Further Reading
303
or
(image and noise and (removal or filter or clean)))
in ISI leads to 7, 520 papers, out of which nearly half are journal papers. An analysis
of the publication year of these papers reveals that the interest in this problem and
its solutions is only growing.
In this chapter we merely scratched the surface of this vast activity, by concen-
trating on Sparse-Land-related denoising algorithms. All these assume an additive
white Gaussian noise, and consider its ﬁltering from still B/W images. While the
algorithms presented here are few, they are also among the best at their task, which
testiﬁes to the importance and the success of sparse and redundant representation
modeling.
Just as we explore noise ﬁltering from B/W still images, we may also be inter-
ested in color images, or in video. The Sparse-Land model has been brought to these
problems as well, by generalizing some of the above algorithms. We brought in this
chapter representative examples for each of these extensions. We choose to omit the
details, and refer the interested readers to recent papers on these subjects.
This chapter oﬀers a series of denoising methods, with a natural hierarchy in
their output quality. Table 14.1 presents this list of methods, ordering them by the
resulting PSNR on the Barbara denoising experiment. Note that this ordering is
not completely correct, as each method may give a diﬀerent result by modifying its
parameters, or when operating on other images.
Looking at the PSNR alone gives a partial picture on this comparison, because
each algorithm requires a diﬀerent eﬀort in terms of computations and memory. In-
deed, the NLM, the BM3D, and the K-SVD denoising all require much more eﬀort,
compared to the other algorithms. Speedup techniques for each of these algorithms
exist, and the struggle to propose a combination of fast and eﬀective denoising al-
gorithms is still very much alive.
One last comment: Throughout this chapter we have been using the PSNR as
a measure of quality. While this measure is comfortable and easy to use, it does
not always reﬂect the true quality of the resulting images. An alternative measure,
called Structural Similarity (SSIM), has been drawing attention in recent years, as
a better quality-assessment method. Table 14.1 presents the SSIM measures for the
various results as well. SSIM computes a value in the range [0, 1, such that 1 stands
for perfect quality. As can be seen, the ordering of the results is not aﬀected much
by the change of the quality measure. This could be explained by the fact that in
denoising (and especially in high-quality algorithms, as those explored here) we do
not get artifacts of the kind handled by SSIM.
Further Reading
1. M. Aharon, M. Elad, and A.M. Bruckstein, K-SVD: An algorithm for designing
of overcomplete dictionaries for sparse representation, IEEE Trans. on Signal
Processing, 54(11):4311–4322, November 2006.

304
14 Image Denoising
Algorithm
PSNR [dB]
SSIM
The noisy image
22.11
0.4771
Global thresholding with redundant Haar dictionary
27.33
0.7842
Patch-based shrinkage-curve training (disregard overlaps)
28.36
0.7881
Patch-based OMP denoising with a ﬁxed dictionary
28.93
0.8465
Patch-based shrinkage-curve training (exploit overlaps)
29.05
0.8375
Patch-based Non-Local-Means ﬁltering
29.59
0.8539
Patch-based OMP denoising with redundant DCT
30.01
0.8665
Patch-based denoising with an adaptive K-SVD dictionary
30.98
0.8837
Patch-based Block-Matching and 3D shrinkage (BM3D)
31.78
0.9050
Table 14.1 The various image denoising algorithms presented in this chapter, and the quality of
their results for denoising of Barbara with σ = 20, measured by PSNR and SSIM.
2. A. Benazza-Benyahia and J.-C. Pesquet, Building robust wavelet estimators for
multicomponent images using Steins principle, IEEE Trans. on Image Process-
ing, 14(11):1814-1830, November 2005.
3. A. Buades, B. Coll, and J.M. Morel, A review of image denoising algorithms,
with a new one, Multiscale Modeling and Simulation, 4(2):490–530, 2005.
4. T. Blu, F. Luisier, The SURE-LET approach to image denoising, IEEE Trans.
on Image Processing, 16(11):2778–2786, November 2007.
5. E.J. Cand`es and D.L. Donoho, Recovering edges in ill-posed inverse problems:
optimality of curvelet frames, Annals of Statistics, 30(3):784–842, 2000.
6. E.J. Cand`es and D.L. Donoho, New tight frames of curvelets and optimal repre-
sentations of objects with piecewise-C2 singularities, Comm. Pure Appl. Math.,
57:219-266, 2002.
7. S.G. Chang, B. Yu, and M. Vetterli, Adaptive wavelet thresholding for im-
age denoising and compression, IEEE Trans. on Image Processing, 9(9):1532–
1546, 2000.
8. S.G. Chang, B. Yu, and M. Vetterli, Wavelet thresholding for multiple noisy
image copies, IEEE Trans. on Image Processing, 9(9):1631–1635, 2000.
9. S.G. Chang, B. Yu, and M. Vetterli, Spatially adaptive wavelet thresholding
with context modeling for image denoising, IEEE Trans. on Image Processing,
9(9):1522–1530, 2000.
10. R. Coifman and D.L. Donoho, Translation-invariant denoising. Wavelets and
Statistics, Lecture Notes in Statistics, 103:120–150, 1995.
11. K. Dabov, A. Foi, V. Katkovnik, and K. Egiazarian, Image denoising by sparse
3D transformdomain collaborative ﬁltering, IEEE Trans. on Image Processing,
16 (2007), pp. 20802095.
12. M.N. Do and and M. Vetterli, The ﬁnite ridgelet transform for image represen-
tation, IEEE Trans. on Image Processing, 12(1):16–28, 2003.
13. M.N. Do and and M. Vetterli, Framing pyramids, IEEE Trans. on Signal Pro-
cessing, 51(9):2329–2342, 2003.

Further Reading
305
14. M.N. Do and M. Vetterli, The contourlet transform: an eﬃcient directional mul-
tiresolution image representation, IEEE Trans. Image on Image Processing,
14(12):2091–2106, 2005.
15. D.L. Donoho, De-noising by soft thresholding, IEEE Trans. on Information
Theory, 41(3):613–627, 1995.
16. D.L. Donoho and I.M. Johnstone, Ideal denoising in an orthonormal basis cho-
sen from a library of bases, Comptes Rendus del’Academie des Sciences, Series
A, 319:1317–1322, 1994.
17. D.L. Donoho and I.M. Johnstone, Ideal spatial adaptation by wavelet shrinkage,
Biometrika, 81(3):425–455, 1994.
18. D.L. Donoho and I. M. Johnstone, Adapting to unknown smoothness via
wavelet shrinkage, J. Amer. Statist. Assoc., 90(432):1200-1224, December 1995.
19. D.L. Donoho, I.M. Johnstone, G. Kerkyacharian, and D. Picard, Wavelet shrink-
age - asymptopia, Journal Of The Royal Statistical Society Series B - Method-
ological, 57(2):301–337, 1995.
20. D.L. Donoho and I.M. Johnstone, Minimax estimation via wavelet shrinkage,
Annals of Statistics, 26(3):879–921, 1998.
21. M. Elad and M. Aharon, Image denoising via learned dictionaries and sparse
representation, International Conference on Computer Vision and pattern Recog-
nition, New-York, June 17-22, 2006.
22. M. Elad and M. Aharon, Image denoising via sparse and redundant representa-
tions over learned dictionaries, IEEE Trans. on Image Processing 15(12):3736–
3745, December 2006.
23. M. Elad, B. Matalon, and M. Zibulevsky, Image denoising with shrinkage and
redundant representations, IEEE Conference on Computer Vision and Pattern
Recognition (CVPR), NY, June 17-22, 2006.
24. Y. Eldar, Generalized SURE for exponential families: applications to regular-
ization, IEEE Trans. on Signal Processing, 57(2):471–481, February 2009.
25. R. Eslami and H. Radha, The contourlet transform for image de-noising using
cycle spinning, in Proceedings of Asilomar Conference on Signals, Systems,
and Computers, pp. 1982–1986, November 2003.
26. R. Eslami and H. Radha, Translation-invariant contourlet transform and its ap-
plication to image denoising, IEEE Trans. on Image Processing, 15(11):3362–
3374, November 2006.
27. O.G. Guleryuz, Nonlinear approximation based image recovery using adaptive
sparse reconstructions and iterated denoising – Part I: Theory, IEEE Trans. on
Image Processing, 15(3):539–554, 2006.
28. O.G. Guleryuz, Nonlinear approximation based image recovery using adaptive
sparse reconstructions and iterated denoising – Part II: Adaptive algorithms,
IEEE Trans. on Image Processing, 15(3):555–571, 2006.
29. O.G. Guleryuz, Weighted averaging for denoising with overcomplete dictionar-
ies, IEEE Trans. on Image Processing, 16:30203034, 2007.
30. M. Lang, H. Guo, and J.E. Odegard, Noise reduction using undecimated dis-
crete wavelet transform, IEEE on Signal Processing Letters, 3(1):10–12, 1996.

306
14 Image Denoising
31. Y. Hel-Or and D. Shaked, A Discriminative approach for Wavelet Shrinkage
Denoising, IEEE Trans. on Image Processing, 17(4):443–457, April 2008.
32. R. Kimmel, R. Malladi, and N. Sochen. Images as Embedded Maps and Min-
imal Surfaces: Movies, Color, Texture, and Volumetric Medical Images. Inter-
national Journal of Computer Vision, 39(2):111-129, September 2000.
33. J. Mairal, F. Bach, J. Ponce, G. Sapiro and A. Zisserman, Non-local sparse
models for image restoration, International Conference on Computer Vision
(ICCV), Tokyo, Japan, 2009.
34. J. Mairal, M. Elad, and G. Sapiro, Sparse representation for color image restora-
tion, IEEE Trans. on Image Processing, 17(1):53–69, January 2008.
35. J. Mairal, G. Sapiro, and M. Elad, Learning multiscale sparse representations
for image and video restoration, SIAM Multiscale Modeling and Simulation,
7(1):214–241, April 2008.
36. S. Mallat and E. LePennec, Sparse geometric image representation with ban-
delets, IEEE Trans. on Image Processing, 14(4):423–438, 2005.
37. S. Mallat and E. Le Pennec, Bandelet image approximation and compression,
SIAM Journal of Multiscale Modeling and Simulation, 4(3):992–1039, 2005.
38. P. Moulin and J. Liu, Analysis of multiresolution image denoising schemes us-
ing generalized Gaussian and complexity priors, IEEE Trans. on Information
Theory, 45(3):909–919, 1999.
39. J.-C. Pesquet and D. Leporini, A new wavelet estimator for image denoising,
in Proc. 6th Int. Conf. on Image Processing and Its Applications, 1:249-253,
1997.
40. J. Portilla, V. Strela, M.J. Wainwright, and E.P. Simoncelli, Image denoising
using scale mixtures of Gaussians in the wavelet domain, IEEE Trans. on Image
Processing, 12(11):1338–1351, 2003.
41. M. Protter and M. Elad, Image sequence denoising via sparse and redundant
representations, IEEE Trans. on Image Processing, 18(1):27–36, January 2009.
42. G. Rosman, L. Dascal, A. Sidi, and R. Kimmel, Eﬃcient Beltrami image ﬁlter-
ing via vector extrapolation methods, SIAM J. Imaging Sciences, 2(3):858–878,
2009.
43. E.P. Simoncelli and E.H. Adelson, Noise removal via Bayesian wavelet coring,
Proceedings of the International Conference on Image Processing, Laussanne,
Switzerland. September 1996.
44. E.P. Simoncelli, W.T. Freeman, E.H. Adelson, and D.J. Heeger, Shiftable multi-
scale transforms, IEEE Trans. on Information Theory, 38(2):587–607, 1992.
45. N. Sochen, R. Kimmel, and A.M. Bruckstein. Diﬀusions and confusions in
signal and image processing, Journal of Mathematical Imaging and Vision,
14(3):195-209, 2001.
46. A. Spira, R. Kimmel, and N. Sochen, A short time Beltrami kernel for smooth-
ing images and manifolds, IEEE Trans. on Image Processing, 16(6):1628-1636,
2007.
47. J.-L. Starck, E.J. Cand`es, and D.L. Donoho, The curvelet transform for image
denoising, IEEE Trans. on Image Processing, 11:670–684, 2002.

Further Reading
307
48. J.-L. Starck, M.J. Fadili, and F. Murtagh, The undecimated wavelet decomposi-
tion and its reconstruction, IEEE Trans. on Image Processing, 16(2):297–309,
2007.
49. C.M. Stein, Estimation of the mean of a multivariate distribution, Proc. Prague
Symp. Asymptotic Statist., pp. 345–381, 1973.
50. C.M. Stein, Estimation of the mean of a multivariate normal distribution, Ann.
Stat., 9(6):1135–1151, November 1981.
51. C. Vonesch, S. Ramani and M. Unser, Recursive risk estimation for non-linear
image deconvolution with a wavelet-domain sparsity constraint, the 15th Inter-
national Conference on Image Processing, (ICIP), pp.665–668, 12-15 October
2008.
52. Z. Wang, A.C. Bovik, H.R. Sheikh and E.P. Simoncelli, Image quality assess-
ment: From error visibility to structural similarity, IEEE Trans. on Image Pro-
cessing, 13(4):600–612, April 2004.

Chapter 15
Other Applications
15.1 General
This book is not meant to be a comprehensive textbook on image processing, and
therefore it is not our intention to show how every familiar application in image
processing ﬁnds a good use for the Sparse-Land model. Indeed, such a claim would
not be true to begin with, as there are image processing problems for which the
relation to this model has not been (and perhaps will never be) shown.
So far, we touched on deblurring and denoising of images, and also took Sparse-
Land to compression of speciﬁc families of images. In this chapter we continue
this journey by considering several more applications in image processing. Com-
mon to them all is the fact that these where shown to beneﬁt well from the use of
Sparse-Land. We begin with the MCA paradigm and its use for separation of im-
age content. We shall show how this becomes useful for ﬁlling-in holes in images,
a problem known as inpainting. Both the MCA and inpainting are highly ill-posed
inverse problems, and yet they are handled quite well by using sparse and redun-
dant representations. Such is also the case with the image scale-up problem, where
a single image is enlarged while attempting to keep its sharpness and visual quality.
We shall discuss this application as well, and show several ways to solve it using
Sparse-Land.
Just before we embark on this journey, which ends this book, we draw the
reader’s attention to the following interesting observation: In the applications dis-
cussed herein, we shall meet the same pattern that appeared already in Chapter 14, of
operating on images as a whole, or turning to operate on small overlapping patches.
In fact, some of the techniques we shall describe here have great resemblance to the
denoising algorithms we introduced before, and this should not be surprising.
© Springer Science+Business Media, LLC 2010
M. Elad, Sparse and Redundant Representations: From Theory to Applications in Signal 
and Image Processing, DOI 10.1007/978-1-4419-7011-4_15,
309

310
15 Other Applications
15.2 Image Separation via MCA
In Chapter 9 we introduced the core ability of the Sparse-Land model to decompose
a mixture of signals into their ingredients, assuming that each piece has a diﬀerent
(and known) dictionary. We assume that the observed signal is a superposition of
two diﬀerent sub-signals y1, y2 and noise v (i.e. y = y1 + y2 + v). We further assume
that y1 is sparsely generated using model M1 (with a dictionary A1), and similarly,
y2 is sparsely generated using model M2 (with a dictionary A2). By solving
min
x1,x2 ∥x1∥0 + ∥x2∥0 subject to ∥y −A1x1 −A2x2∥2
2 ≤δ,
(15.1)
the resulting solution (xδ
1, xδ
2) would generate a plausible solution ˆy1 = Axδ
1, ˆy2 =
Axδ
2 to the separation problem. The parameter δ should take into account both the
noise energy, and the model inaccuracies in representing sparsely the signals y1 and
y2.
Following the terminology adopted by Jean-Luc Starck, we shall refer hereafter
to this separation idea as Morphological Component Analysis (MCA). Note that the
above equation is very similar to the denoising formulation,
min
x
∥xa∥0 subject to ∥y −Aaxa∥2
2 ≤δ,
(15.2)
with one eﬀective dictionary being Aa = [A1, A2] (‘a’ in the subscript stands for
‘all’), and a corresponding representation xT
a = [xT
1 , xT
2 ].
15.2.1 Image = Cartoon + Texture
Can we use MCA to separate image content? What are the ingredients an image
would be assumed to be built of? Early work in image processing assumed that
images are mostly composed of a piecewise smooth (cartoon-like) content. One
strong manifestation to this belief is the way inverse problems in image processing
have been regularized, using an expression that penalizes non-piecewise smooth
behavior. A further evidence to this common belief is the way various Frames and
transforms (e.g., curvelets, contourlets) were conceived and tested.
Today, there is a growing agreement that images are in fact a mixture of cartoon
and texture parts, which are combined linearly, very much in line with the above
model. This means that we may use MCA to separate image content to cartoon
and texture images. However, for this to be done, we should answer three critical
questions:
1. What is the beneﬁt in such a separation?
2. What should the proper dictionaries be for these two kinds of contents? and
3. How should we numerically solve the problem posed in Equation (15.1)?

15.2 Image Separation via MCA
311
Addressing the ﬁrst question, and generally speaking, the beneﬁt in such a separa-
tion would be the potential ability to process the image better by treating each of the
parts using a properly tailored tool.
As a ﬁrst example for such a beneﬁt, consider compression of still images. While
a wavelet-based coding algorithm could do well for the cartoon part in the image,
it is expected to perform poorly on the texture content. Disregarding this fact im-
plies that the overall coding performance may be compromised. Instead, coding
each piece using its proper method could lead to far better results. In that respect,
coding of the texture should not target the same error measure (MSE). We may al-
low a generation of a similar texture in the reconstructed image, which is visually
indistinguishable from the original one, while being in fact very diﬀerent in MSE
terms.
Another application that may beneﬁt from MCA separation of images is denois-
ing. Consider, for example, the global formulation of the image denoising problem
posed in Equation (14.5). The dictionary A should contain a part that corresponds
to cartoon (e.g., Haar), and another part concatenated to it, which covers texture
content. This may explain the inferior denoising results we have obtained for the
Barbara image, using the global method with the Haar dictionary alone, as the
texture dictionary was missing.
Moving from the global to a local denoising with a trained dictionary, we are
in-fact implicitly using MCA. As can be seen in Figure 14.10 (right), the overall
dictionary A contains both cartoon (one dominant oriented edge per atom) and tex-
ture (periodic content) atoms, suggesting that it is already a combination of the two
desired parts. Note, however, that the task in denoising is oblivious to the separation
goal, and the carton/texture atoms are mixed as an unordered set of columns in the
overall dictionary A.
There are other applications that may beneﬁt from image separation to cartoon
and texture, such as structured noise ﬁltering, meaningful edge detection in textured
images, object recognition, image content editing, and more. In the next section
we shall discuss in more details the image inpainting problem – ﬁlling in missing
information in images – which also beneﬁts from such a separation.
In order to answer the second and the third questions posed above (which dictio-
naries to use, and how to numerically solve (15.1)), we should ﬁrst choose whether
to operate globally on the image, or locally using patches. We start with the global
method that considers the image as a whole, following the same structure as in
Chapter 14.
Before we turn to describe the global and the local MCA paradigms, we should
mention that separation of an image to cartoon and texture can be done using other
methods. Indeed, the ﬁrst to consider such a separation were Mayer, Vese, and Os-
her in 2002. Their approach towards the image separation task focused on varia-
tional tools, extending the Total-Variation by proposing a dual norm for handling
the texture part. We shall not dwell on these ideas here, but we should mention that
despite the very diﬀerent tools and language used, there are some striking similari-
ties between the variational formulation of the separation, and the MCA paradigm
discussed here.

312
15 Other Applications
15.2.2 Global MCA for Image Separation
We begin by modeling the problem: An ideal image y0 ∈IRN (of size
√
N ×
√
N
pixels) is composed of two parts – a cartoon and texture images of the same size, yc
and yt, respectively. The combined image y0 = yc + yt is measured in the presence
of an additive zero-mean white and homogeneous Gaussian noise, v, with a known
standard deviation σ. The measured image, y, is thus
y = y0 + v = yc + yt + v.
(15.3)
We desire to design an algorithm that can recover the two parts of y0, yc and yt.
Using the MCA formulation mentioned above, we propose to solve
ˆxc, ˆxt = arg min
xc,xt
λ∥xc∥1 + λ∥xt∥1 + 1
2∥y −Acxc −Atxt∥2
2.
(15.4)
Notice that we have chosen to replace the proximity constraint in Equation (15.1) by
a penalty, and the ℓ0-norm by an ℓ1-norm, both as shown earlier in various contexts.
The two unknowns to be found are the corresponding representations of the two
parts. Once computed, the two estimated parts are obtained by ˆyc = Acˆxc and ˆyt =
Atˆxt.
The matrices Ac ∈IRN×Mc and At ∈IRN×Mt are to be chosen such that they enable
sparse representation of cartoon and texture contents respectively. The matrix Ac
could be some type of wavelet, or perhaps ridgelets, curvelets, or contourlets, de-
pending on the type of cartoon content we expect to ﬁnd in the image (see examples
below). The texture dictionary At should contain oscillatory atoms, such as found in
the Gabor transform, the global DCT, or the local one.1 The sizes Mc and Mt are the
number of atoms in the two dictionaries, and those typically satisfy Mc, Mt ≫N.
For example, the curvelet dictionary with 6 resolution levels has Mc = 129N. Simi-
larly, the local-DCT applied on patches of 8 × 8 pixels has Mt = 64N.
An alternative to the above formulation can be proposed, where we move from
the synthesis model to an analysis one (as explained in Chapter 9). Assuming (only
for the moment!!) that Ac and At are square and invertible, we could use the as-
signments ˆxc = A−1
c ˆyc and ˆxt = A−1
t ˆyt and obtain an alternative formulation of our
problem in the form
ˆyc, ˆyt = arg min
yc,yt
λ∥Tcyc∥1 + λ∥Ttyt∥1 + 1
2∥y −yc −yt∥2
2,
(15.5)
where we have denoted Tc = A−1
c and Tt = A−1
t .
In the more general case where the dictionaries are non-square, we can still use
the analysis formulation, and assume instead that Tc = A+
c and Tt = A+
t . However,
1 The global DCT is a regular DCT transform applied to the whole image. The local DCT is
obtained by applying the unitary DCT transform on image patches with full-overlaps. This consti-
tutes the pseudo-inverse of At, and the atoms of At are therefore the unitary-DCT basis functions
positioned in all possible locations in the image.

15.2 Image Separation via MCA
313
as opposed to the square and invertible case discussed above, the analysis and syn-
thesis formulations are no longer equivalent. One appealing beneﬁt that the analysis
formulation in Equation (15.5) has is the fact that the unknowns are directly the
image parts we desire, and those are typically of a lower-dimension (2N instead of
Mc + Mt), compared to their sparse representations.
As for the numerical solution of the problems posed in (15.4) or (15.5), various
methods can be envisioned. In particular, iterative-shrinkage algorithms (see Chap-
ter 6) can be used to solve directly the synthesis formulation in (15.4). For example,
the SSF algorithm, as described in Chapter 6, leads to the iterative update,
ˆxk+1
a
= Sλ
 1
cAa
T 
y −Aaˆxk
a

+ ˆxk
a
!
(15.6)
which is posed in terms of the combined dictionary and the combined representation
as in Equation (15.2). Breaking this into the two representation parts we get
ˆxk+1
c
= Sλ
 1
cAc
T 
y −Acˆxk
c −Atˆxk
t

+ ˆxk
c
!
(15.7)
ˆxk+1
t
= Sλ
 1
cAt
T 
y −Acˆxk
c −Atˆxk
t

+ ˆxk
t
!
,
where S λ(r) is an element-wise soft-thresholding operation on r with a threshold λ,
and the parameter c should be chosen such that2 c > λmax(AT
a Aa).
The analysis alternative formulation in (15.5) can be treated similarly, with some
necessary modiﬁcations to the above algorithm. More speciﬁcally, assuming that the
two dictionaries are redundant (Mc, Mt ≥N) and full-rank, following the description
given in Chapter 9 we can turn back the analysis into a synthesis form by denoting
xc = Tcyc →yc = T+
c xc = Acxc
(15.8)
xt = Ttyt →yt = T+
t xt = Atxt.
This leads to an optimization problem of the form
ˆxc, ˆxt = arg min
xc,xt
λ∥xc∥1 + λ∥xt∥1 + 1
2∥y −Acxc −Atxt∥2
2
(15.9)
Subject to TcAcxc = xc and TtAtxt = xt.
The additional constraints appear because in the migration from yc to xc (and sim-
ilarly from yt to xt) we should demand that xc is in the column-span of Tc (Tt,
respectively).
Exploiting the analysis formulation in Equation (15.9), we could use the very
same iteration formula as in Equation (15.7), since the penalty term in Equation
(15.9) is the same one as in Equation (15.4), and thus this iteration induces a descent
2 Using the Singular-Value-Decomposition of Aa, it is easy to show that this requirement is equiv-
alent to c > λmax(AaAT
a ) = λmax(AcAT
c + AtAT
t ). For tight Frames we have that AcAT
c = AtAT
t = I,
implying that c should be larger than 2.

314
15 Other Applications
on the penalty. Thus, we start by the update
ˆx
k+ 1
2
c
= Sλ
 1
cAc
T 
y −Acˆxk
c −Atˆxk
t

+ ˆxk
c
!
(15.10)
ˆx
k+ 1
2
t
= Sλ
 1
cAt
T 
y −Acˆxk
c −Atˆxk
t

+ ˆxk
t
!
,
as above. However, this should be followed by a projection onto the constraints, by
computing3
ˆxk+1
c
= TcAcˆx
k+ 1
2
c
(15.11)
ˆxk+1
t
= TtAtˆx
k+ 1
2
t
.
While this algorithm indeed handles the analysis-formulation as we desired, it
lost its main appeal since it is formulated in terms of the representations rather than
the separated images. This can be ﬁxed by modifying Equations (15.10) and (15.11)
to be
ˆyk+1
c
= Ac · Sλ
 1
cAc
T 
y −ˆyk
c −ˆyk
t

+ Tcˆyk
c
!
(15.12)
ˆyk+1
t
= At · Sλ
 1
cAt
T 
y −ˆyk
c −ˆyk
t

+ Ttˆyk
t
!
,
where we have used the assignments as given in Equation (15.8).
We now turn to present several experimental results, obtained using the global
analysis paradigm. All the results shown here are taken from the work by Starck,
Elad, and Donoho (IEEE-TIP, 2004), and therefore, we do not include details on
parameter settings, etc., and focus on the qualitative results obtained. We start with
a synthetically generated image. This image is composed of a natural scene (con-
sidered here as the cartoon part), a texture, and an additive white Gaussian noise
(σ = 10). Figure 15.1 shows the ingredients of this image.
The dictionaries chosen for this separation are the curvelets for the cartoon part,
and a global DCT transform for the texture. The very low frequency components
of the image are ﬁrst subtracted from it, and then added to the cartoon part after
the separation. The reason for this is the evident overlap that exists between the two
dictionaries – both consider the low-frequency content to belong to them as both
can represent it eﬃciently. Thus, by removing this content prior to the separation we
avoid a separation ambiguity. Also, by returning this content later to the curvelets
part, we use our expectation to see the low frequencies as belonging to the piecewise
smooth image. Figure 15.2 shows the results of this experiment. As we can see, the
three ingredients of the image (cartoon, texture, and noise) are separated very well.
3 Given a vector v, we seek the closest alternative u such that the constraint u = Pu is satisﬁed,
where P is a projection matrix (square, symmetric, with eigenvalues being either zeros or ones).
Solving this using Lagrange multipliers leads to the optimal solution u = Pv.

15.2 Image Separation via MCA
315
Fig. 15.1 The original cartoon and texture parts (top left and right), and the overall image (bottom)
to be processed.
In a second experiment, we apply the same separation algorithm to the image
Barbara. This time we use the curvelets and local-DCT (blocks of size 32 × 32
pixels) dictionaries. Figure 15.3 shows the original image and the separated cartoon
and texture parts. Figure 15.4 presents a magniﬁed part of the face.
We conclude the experimental part by showing two simple applications for the
separation obtained. The ﬁrst is an edge detection in an image. Such a process may
become a crucial processing stage in many computer-vision applications. When the
texture is highly contrasted, most of the detected edges are due to the texture rather
than the cartoon (which is the more informative) part. By ﬁrst separating the two
components we can detect the true object’s edges by applying edge detection on the
cartoon part alone. Figure 15.5 shows the edges detected by the Canny edge detector
algorithm on both the original image and the cartoon reconstructed component.
Figure 15.6 shows a galaxy imaged with the GEMINI-OSCIR instrument. The
data is contaminated by white additive noise and a stripping artifact (assumed to be
the texture in the image) due to the instrument electronics. As the galaxy is isotropic,

316
15 Other Applications
Fig. 15.2 The original noisy image (top left), the separated texture part (top right), the separated
cartoon (bottom left), and the residual noise component (bottom right).
the isotropic wavelet is used instead of the curvelets for representing the cartoon
part. The global DCT is used again for the texture, due to its structure. Figure 15.6
summarizes the results of this separation where a successful isolation of the galaxy,
the textured disturbance, and the additive noise is seen.
15.2.3 Local MCA for Image Separation
Just like in the image denoising application, MCA can be performed on small and
overlapping patches. The immediate beneﬁts of such a mode of operation are (i) the
locality of the processing, which lends itself to eﬃcient parallel implementation; and
(ii) the ability to incorporate learned dictionaries into the MCA. This last beneﬁt –
the ability to work with trained dictionaries – has far-reaching implications, as we

15.2 Image Separation via MCA
317
Fig. 15.3 The separated cartoon (left) and texture (right) parts, and the original image Barbara
from which these were extracted.
can use it to modify the separation task to any mixture of contents. For example,
two (or more) mixed textures can be separated using MCA, provided that we have
dictionaries ﬁtting each of the ingredients.
There are various ways to turn MCA into a locally-processing algorithm, similar
to the wealth with which the denoising task has been shown to be handled locally
in Chapter 14. In this chapter we shall explore one such scheme, that builds on the
K-SVD denoising algorithm. As we shall see next, by a simple extension of this
method, we get state-of-the-art separation results as a side-beneﬁt.
Following the details given in Section 3.3 in Chapter 14, we start by assuming
that the unknown mixed image y0 = yc + yt is fully expressed by the fact that for
every patch in it, Rky0 = Rkyc + Rkyt, there exist two sparse representation vectors
qk
c and qk
t , such that
∥Rkyc + Rkyt −Acqk
c −Atqk
t ∥2 ≤ϵ,
(15.13)

318
15 Other Applications
Fig. 15.4 A zoomed-in portion of the cartoon and texture parts (top left and right),and the original
image (bottom), all taken from the results presented in Figure 15.3
Fig. 15.5 The detected edges on the original image (left), and those detected using the cartoon part
only (right).

15.2 Image Separation via MCA
319
Fig. 15.6 The original GEMINI image (top left), the texture component (top right), the cartoon
component (bottom left), and the residual noise (bottom right).
where Ac and At are two ﬁxed dictionaries. Rather than using this notation, we
return temporarily to the combined dictionary Aa and the corresponding con-
catenated representation for this patch, qk
a. Thus, the above can be rewritten as
∥Rky0 −Aaqk
a∥2 ≤ϵ. Using this within a global MAP estimator leads to
n
{ˆqk
a}N
k=1, ˆy
o
= arg min
z,{qka}k
λ∥z −y∥2
2 +
(15.14)
N
X
k=1
µk∥qk
a∥0 +
N
X
k=1
∥Aaqk
a −Rkz∥2
2.
In this expression the ﬁrst term is the log-likelihood global force that demands a
proximity between the measured image, y, and its denoised (and unknown) version
z. The second and the third terms are the image prior that makes sure that in the

320
15 Other Applications
constructed image, z, every patch pk = Rkz of size √n × √n in every location (thus
the summation by k) has a sparse representation with bounded error.
The problem posed in Equation (15.14) is one of denoising, and there is no trace
for the separation goal behind it. This is natural, since whether y0 is mixed or not,
it still requires denoising in order to remove the additive noise, and the fact that
patches in y0 can be assumed to have sparse representations with respect to the
dictionary Aa is suﬃcient for denoising it eﬃciently.
Therefore, we treat the above denoising just as described in the previous chapter,
by initializing the dictionary to be the redundant DCT, and iterating between updates
of the representations {ˆqk
a}N
k=1 and the dictionary Aa. This sequence of steps forms
an eﬀective K-SVD dictionary-learning that trains on the corrupted image itself.
The regular image denoising algorithm would proceed after such steps by ﬁxing the
representations and the dictionary, and turning to compute the clean image by the
formula
ˆy =
λI +
N
X
k=1
RT
k Rk

−1 λy +
N
X
k=1
RT
k Aˆqk
a
.
(15.15)
However, this is where our local MCA algorithm departs from the denoising process
introduced earlier. Our alternative steps would be to (i) decompose the overall dic-
tionary Aa into the cartoon and texture dictionaries, and then conclude the algorithm
by (ii) using the two separated dictionaries in order to separate the image.
The trained dictionary Aa contains the atoms of Ac and At in an unordered fash-
ion. We can identify the texture/cartoon atoms, by their Total-Variation measure,
since we expect cartoon atoms to be smooth or piecewise smooth, while texture
atoms are expected to oscillate strongly.
As an example, the above algorithm is applied to a noisy (σ = 10, as in the
previous experiments) version of the image Barbara for 25 iterations, and it leads
to the dictionary presented in Figure 15.7 (top). For an atom a, organized as a patch
of size √n × √n, we use an activity measure similar to TV, given by
Activity(a) =
n
X
i=2
n
X
j=1
|a[i, j] −a[i −1, j]|
(15.16)
+
n
X
i=1
n
X
j=2
|a[i, j] −a[i, j −1]|.
These values (normalized such that their maximal value is 1) for all the dictionary
atoms are shown in Figure 15.7(bottom). As can be seen, these values reﬂects well
the degree of “textureness” in each atom.
Figure 15.8 presents the classiﬁcation of each atom as belonging to the texture
or the cartoon dictionaries, based on thresholding the above (normalized) measure
with T = 0.27 (this value was chose manually for best performance. A value above
this threshold indicates a texture atom). Note that from the overall 256 atoms, only
36 are attributed to the cartoon, while the rest are considered as texture atoms.

15.2 Image Separation via MCA
321
 
 
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Fig. 15.7 Top: The K-SVD dictionary obtained on a noisy version (σ = 10) of Barbara after 25
iterations; Bottom: The total-variation-like activity measure per each atom.
With the decomposition of the dictionary Aa into Ac and At we also obtain a
corresponding separation of the representations qk
a to qk
c and qk
t . This is done by
simply allocating the coeﬃcients that correspond to cartoon atoms to qk
c, and simi-
larly, allocating the coeﬃcients that correspond to texture atoms to qk
t for the texture
part.
We are left with the need to recover ˆyc and ˆyt, exploiting the decomposition of the
dictionary. For this, we return to Equation (15.14), and deﬁne two new unknowns,
zc and zt, such that z = zc + zt. In order to enable a separation, we modify the term
∥Aaqk
a −Rkz∥2
2 into
∥Aaqk
a −Rkz∥2
2 = ∥Acqk
c + Atqk
t −Rkz∥2
2
(15.17)

322
15 Other Applications
Fig. 15.8 A mapping (right) that shows the texture (white) and the cartoon (dark gray) atoms,
obtained by thresholding the TV activity measure by T = 0.27.
= ∥Acqk
c + Atqk
t −Rkzc −Rkzt∥2
2
≈∥Acqk
c −Rkzc∥2
2 + ∥Atqk
t −Rkzt∥2
2.
The reason for the last approximate equality is our belief that the representation
errors for the cartoon and the texture patches are not expected to correlate, i.e.,
(Acqk
c −Rkzc)T(Atqk
t −Rkzt) ≈0.
(15.18)
With these changes, Equation (15.14) becomes
{ˆyc, ˆyt} = arg min
zc,zt
λ∥zc + zt −y∥2
2
(15.19)
+
N
X
k=1
∥Acqk
c −Rkzc∥2
2 +
N
X
k=1
∥Atqk
t −Rkzt∥2
2.
Notice that the ℓ0 term that forces sparsity of the representations has been removed.
This is done because we assume now that qk
c and qk
t are ﬁxed and known, and our
goal is the recovery of zc and zt. Taking the derivatives with respect to zc and zt
gives the separation result
" ˆyc
ˆyt
#
=
" λI + PN
k=1 RT
k Rk
λI
λI
λI + PN
k=1 RT
k Rk
#−1
(15.20)
·
"λy + PN
k=1 RT
k Acqk
c
λy + PN
k=1 RT
k Atqk
t
#
.
In the extreme case of λ = 0, the solution is given by

15.2 Image Separation via MCA
323
Fig. 15.9 The separated cartoon (left) and texture (right) parts, using the local MCA with the
K-SVD dictionary (λ = 0).
ˆyc =

N
X
k=1
RT
k Rk

−1
N
X
k=1
RT
k Acqk
c
(15.21)
and
ˆyt =

N
X
k=1
RT
k Rk

−1
N
X
k=1
RT
k Atqk
t ,
which is very similar to the denoising solution posed in Equation (15.15). In the
more general case, the above inversion can be done very easily, exploiting the 2 × 2
block matrix inversion formula,4 and exploiting the fact that all the blocks in our
case are diagonal and easily invertible.
Figure 15.9 shows the results obtained with this local method for the simple case
of λ = 0 on the image Barbara. As can be seen, the separation is very success-
ful, and with results that are generally comparable to those obtained by the global
method described above, and this is achieved without the need to pre-specify the
dictionaries for the two parts.
4 This formula is given by
" A B
C D
#−1
=
"
Q
−QBD−1
−D−1CQ D−1 + D−1CQBD−1
#
,
where Q = (A −BD−1C)−1.

324
15 Other Applications
15.3 Image Inpainting and Impulsive Noise Removal
Image inpainting refers to ﬁlling-in missing pixels in known locations in the image,
due to undesired scratches, occlusions, or because of image editing needs (when
removing an object from the scene). Extensive work on this problem is reported in
the image processing literature, using various techniques. In this section we consider
this problem, and more speciﬁcally, we focus on ways to inpaint images using the
Sparse-Land signal model. We start our discussion with the core ideas for inpainting
sparsely represented signals, and then extend this discussion to images.
15.3.1 Inpainting Sparse-Land Signals – Core Principles
Assume that y0 ∈IRn is a simpliﬁed Sparse-Land signal, meaning that there exists
a sparse representation vector x0 with ∥x0∥0 = k0 such that y0 = Ax0. Assume
further that we measure y = My0, where M is a degradation operator that removes
p samples from the signal. This matrix is of size (n−p)×n, built by taking the n×n
identity matrix and removing the p rows that correspond to the dropped samples.
Could we recover these missing values? How?
Considering this as a classic inverse problem in the form described in Chapter 9,
we may formulate this task as
min
x ∥x∥0 subject to y = MAx.
(15.22)
We should ﬁnd the representation that solves this problem, ˆx0, and the recovered
(full) signal ˆy0 would be Aˆx0. However, the question remains: Why should this
work well?
In order to answer this question, we return to the worst-case analysis that was in-
troduced in Chapter 2. Assume that the original representation x0 is sparse enough,
namely, k0 < 0.5 · spark(MA). In such a case we know that the original representa-
tion x0 is necessarily the sparsest possible solution for the linear system y = MAx.
As such, if we could solve the problem posed in Equation (15.22), we should have
that ˆx0 = x0. This implies a perfect recovery of the original signal, ˆy0 = y0. There-
fore, if x0 is sparse enough, inpainting is guaranteed to succeed perfectly.
How does M aﬀect the spark of the matrix MA? This multiplication removes p
rows from the dictionary A, and with such a modiﬁcation, the spark necessarily dete-
riorates (gets smaller). This is because every set of spark(A)−1 columns from A are
linearly-independent, by deﬁnition of the spark. Removal of an arbitrary row from
A may either leave all of these sets intact (and thus the spark remains unchanged),
or modify one or more of those groups to become linearly-dependent, causing a de-
crease in the spark. Once a set of columns becomes dependent, removal of other
rows cannot turn such a set to become independent. Thus, the value spark(MA)
is non-increasing as a function of the removal count p. In particular, we have that

15.3 Image Inpainting and Impulsive Noise Removal
325
Fig. 15.10 The image Peppers on which we perform the inpainting test.
spark(MA) ≤spark(A), which means that our assumption k0 < 0.5 · spark(MA)
implies that x0 is the sparsest representation for the original signal y0, as well.
The above is a simpliﬁed inpainting problem, and yet it clariﬁes why inpainting
of Sparse-Land signals could conceptually succeed. The core idea is the understand-
ing that since the signal y0 has only 2k0 degrees of freedom (atom indices and their
weights), we can recover this signal completely from less than the full n samples. In
the extreme case,5 a set of 2k0 samples may be suﬃcient, meaning that p could be
as high as n −2k0.
When noise is added both to the model and to the measurements, the actual in-
painting should be done by approximating the solution of
min
x ∥x∥0 subject to ∥y −MAx∥2 ≤δ.
(15.23)
Spark considerations no-longer apply, but as we have seen in Chapter 5, alterna-
tive analysis exists to support the success of such a recovery algorithm, both for the
worst-case scenario, and for the more optimistic and more realistic average perfor-
mance.
Will this methodology work in practice? We answer this question by introducing
a preliminary experiment that illustrates how the above simple guide-lines can lead
to a reasonable recovery of missing samples. We shall later return to this experiment
and improve it in various ways. We operate on the image Peppers (of size 256×256
pixels) shown in Figure 15.10. This image is contaminated by an additive white
Gaussian noise with σ = 20, and then sub-sampled in random locations, such that a
portion of the pixels remains.
The inpainting we propose to perform operates on patches of size 8 × 8 pix-
els, extracted with no overlaps from the corrupted image. Using a redundant DCT
5 This depends on A and the eﬀect of M on it. If spark(MA) has full spark (2k0 + 1), then 2k0
measurements are indeed suﬃcient.

326
15 Other Applications
0
20
40
60
80
100
0
10
20
30
40
50
60
70
80
Percentage of remaining pixels
Root−Mean−Squared−Error
 
 
Inpainting Results
Noise Level
Fig. 15.11 Simple inpainting results – operating locally with no overlaps and using the redundant
DCT dictionary: This graph shows the RMSE (per-pixel) of the inpainting recovered image, as a
function of the percentage of remaining pixels.
dictionary A with 256 atoms (as described in Chapter 14), we aim to recover the
original patches by approximating the solution of Equation (15.23). This process
is performed per patch using the OMP algorithm with δ = 1.1 ·
p
64 −p · σ (the
value of δ should be proportional to the amount of noise that exists in the remaining
samples of the patch). The ﬁnal inpainted image is created by reconstructing each of
these patches from its sparse representation, and placing them back onto the image
canvas.6
Figure 15.11 displays the inpainting results, as a graph of the reconstruction
RMSE per-pixel as a function of the quantity of remaining pixels in the image. As
expected, this graph descends, indicating that the quality of the recovered patches
improves as more pixels are available. Also, when compared to the noise standard-
deviation σ = 20, this graph shows that beyond recovering the missing pixels, this
procedure has a denoising eﬀect as well for less than 50% missing pixels. Figure
15.12 presents the inpainted images obtained using the above method for the three
tests (removing 25%, 50%, and 75% of the pixels). The root-MSE values computed
between the recovered images and the original one are 14.55, 19.61, and 29.70 grey-
values respectively.
The above is a simple inpainting algorithm, being a direct implementation of the
very core ideas presented above, and yet, its results are reasonable (and especially so
for less than 50% missing pixels). Still, this method could and should be improved,
6 We also add a clipping operation that forces the resulting image to have grey-values in the range
[0, 255].

15.3 Image Inpainting and Impulsive Noise Removal
327
Fig. 15.12 Simple inpainting results – operating locally with no overlaps and using the redundant
DCT dictionary: This ﬁgure show the given images (top) and their inpainted results (bottom), for
25% missing pixels (left, RMSE= 14.55), 50% missing pixels (center, RMSE= 19.61), and 75%
missing pixels (right, RMSE= 29.7).
and this can be done in various ways. Could we inpaint more complex images with
texture? Could we handle larger holes in the image? Can we imitate the local de-
noising algorithms and gain in performance by operating on overlapping patches?
Can we adapt the dictionary to the image and to the inpainting goals? These ques-
tions bring us to the next subsections, where we discuss local and then global image
inpainting algorithms of better quality that rely on the above ideas.
15.3.2 Inpainting Images – Local K-SVD
In Figures 15.11 and 15.12 we presented the results of a simple inpainting algorithm,
that operates on each patch in the image separately, and concatenates the results to
form the full image. Can we do better, while still operating locally? Figure 15.13
shows that a substantial improvement is within reach by a simple modiﬁcation of
the algorithm: Instead of processing non-overlapped patches, we perform the same
inpainting process on fully overlapped patches. The resulting image is obtained by
averaging the recovered patches in the resulting image canvas. As can be seen, the
results are much improved, and especially so for the case where 75% of the pixels

328
15 Other Applications
Fig. 15.13 Improved inpainting results – operating locally with full-overlaps and using the re-
dundant DCT dictionary: This ﬁgure shows the given images (top) and their inpainting results
(bottom) using the redundant DCT dictionary, for 25% missing pixels (left, RMSE= 9.00), 50%
missing pixels (center, RMSE= 11.55), and 75% missing pixels (right, RMSE= 18.18).
are missing. Table 15.1 summarizes these results versus those shown above for the
non-overlapping treatment.
RMSE for
RMSE for
RMSE for
Algorithm
25% missing pixels 50% missing pixels 75% missing pixels
No-overlaps
14.55
19.61
29.70
Full-overlaps
9.00
11.55
18.18
Table 15.1 RMSE for the inpainting tests applied locally on patches with and without overlaps,
all using the redundant DCT dictionary.
Let us formulate the inpainting problem in a way that leads to the above algorithm
in a natural way. Following our treatment for the denoising problem in Chapter 14,
let us assume that the unknown and complete image y0 is such that every patch in it
is expected to have a sparse representation with respect to a known dictionary A. We
shall denote the mask that removes samples from y0 as a rectangular matrix M. We
shall further assume that the remaining pixels are noisy, contaminated by white zero-
mean additive Gaussian noise with variance σ2. Thus, the MAP estimator would be

15.3 Image Inpainting and Impulsive Noise Removal
329
n
{ˆqk}M
k=1, ˆy
o
= arg min
z,{qk}k
λ∥Mz −y∥2
2 +
(15.24)
X
k
µk∥qk∥0 +
X
k
∥Aqk −Rkz∥2
2.
This expression parallels the one posed in Equation (14.21), requiring a proximity
between the measured image y and the unknown version z, only in the existing
pixels (this is reﬂected by the multiplication by the mask M). The second and third
terms are the image prior that ensures that in the reconstructed image z every patch
pk = Rkz of size √n × √n in every location k should have a sparse representation
with bounded error.
In the case discussed above, the underlying dictionary A is assumed known, be-
ing the redundant DCT. We are supposed to minimize this function with respect to
both the sparse representations qk, and the overall output image z. As done before,
we adopt a block-coordinate minimization algorithm that starts with an initializa-
tion z = MTy, and then seeks the optimal ˆqk. Note, however, that in choosing this
initialization, the pixels in z that correspond to missing values are totally corrupted,
and this should be taken into account in the evaluation of ˆqk. We arrive at a complete
decoupling of the minimization task into smaller problems of the form
ˆqk = arg min
q
∥q∥0 subject to ∥Mk(Aq −pk)∥2
2 ≤cnkσ2.
(15.25)
First, notice that we have turned the quadratic penalty in Equation (15.24) into a
constraint, thus removing the need to choose the parameter µk. Also, the energy of
the patch-error Aq−pk is evaluated using only the existing pixels in this patch, as in-
dicated by the multiplication by Mk = RkMTMRT
k — a local mask that corresponds
to the k-th patch.
The threshold to compare with must also take into account the count of existing
pixels in this patch, nk = 1TMk1. Thus, this stage works as a sliding window sparse
coding stage, operated on each block of size √n× √n at a time, performing the very
same process as described in Equation (15.23).7
Given all ˆqk, we can now ﬁx those and turn to update z. Returning to Equation
(15.24), we need to solve
ˆy = arg min
z
λ∥Mz −y∥2
2 +
X
k
∥Aˆqk −Rkz∥2
2,
(15.26)
which leads to
ˆy =
λMTM +
X
k
RT
k Rk

−1 λMTy +
X
k
RT
k Aˆqk
.
(15.27)
7 The formulation of the two problems might seem diﬀerent, because Mk here is square, zeroing
the unknown elements, whereas M in Equation (15.23) is rectangular, removing these elements.
Beyond this semantic diﬀerence, the two formulations are the same.

330
15 Other Applications
This expression is almost identical with what we have proposed above for the de-
noising problem, and in particular, the matrix to invert is diagonal, introducing nor-
malization factors per each of the pixels in the resulting image. For missing pixels,
this expression suggests a simple averaging of the contributions of the recovered
patches directly. For existing pixels, this formula leads to an averaging as well, but
one that takes into account the measured values with a proper weight. Thus, the re-
sults shown in Figure 15.13 and Table 15.1 correspond to this very algorithm with
λ set to be zero.
The natural next step would be to adapt the dictionary to the image using a
dictionary-learning approach. Following the K-SVD image denoising algorithm as
presented in Chapter 14, we could suggest the following process:
1. We should extract all possible patches with full-overlaps from the corrupted im-
age, and train a dictionary on these. As opposed to the denoising algorithm, the
sparse-coding and the dictionary update stages to be performed within this pro-
cess must take into account the mask of the missing pixels.
2. Once the dictionary is computed, we should sparse-code each of the patches,
and place them back to the image canvas, averaging overlaps, just as done in the
above method.
This algorithm is nothing but a minimization of the MAP function posed in Equation
(15.25), this time with respect to A as well. Fixing A, we perform sparse coding
exactly as described above. Once the sparse representations {ˆqk}k are computed,
they are kept ﬁxed, and the dictionary is updated, such that the expression
Error(A) =
X
k
∥Mk(Aqk −pk)∥2
2
(15.28)
is minimized. This can be done using either an MOD or K-SVD update steps. Both
are more challenging in this case, as compared to the core description in Chapter
12, and this is due to the mask matrices, which require a closer attention. The MOD
step is obtained by nulling the derivative of Error(A), resulting with the expression
∇AError(A) =
X
k
MT
k Mk(Aqk −pk)qT
k = 0,
(15.29)
from which it is hard to obtained a closed-form analytic formula for the update of
A. Exploiting properties of the Kronecker-product, we can use the formula ABC =
(CT ⊗A)BCS , where BCS is a vector obtained by a lexicographic ordering of the
columns of B. This leads to
ACS =

X
k
(qkqT
k ) ⊗(MT
k Mk)

−1 X
k
(pkqT
k )CS .
(15.30)
In our case, the matrix to be inverted is of size mn×mn. For example, for a dictionary
of size 64 × 256 as in the above experiments, this matrix size is 214 × 214. Thus, a

15.3 Image Inpainting and Impulsive Noise Removal
331
direct use of this equation for deriving A is prohibitive in most cases. The alternative
is to use the gradient in Equation (15.29) within a steepest decent algorithm.
The K-SVD counterpart is better behaved. We target the update of the columns
of A one at a time. Considering the j-th column, aj, we should address the error
expression in Equation (15.28) only for patches k that use this atom. Denoting this
set by Ωk, we thus minimize
Error(A) =
X
k∈Ωj
∥Mk(Aqk −pk)∥2
2
(15.31)
=
X
k∈Ωj
∥Mk(Aqk −ajqk( j) −pk) + Mkajqk(j)∥2
2.
We denote y j
k = pk −Aqk + a jqk( j). This is the residual in the representation of the
k-th patch, where all the atoms apart from the j-th are used. Thus, our task would be
min
a j,{qk(j)}k∈Ωj
X
k∈Ωj
∥Mk(y j
k −a jqk(j))∥2
2.
(15.32)
While the above could be posed as a rank-1 approximation problem (that ties it
to SVD), a simpler approach is a short iterative (2-3 iterations) process where we
update the two unknowns alternatingly. We ﬁx {qk(j)}k∈Ωj and update aj, by
ˆaj =

X
k∈Ωj
Mkqk( j)2

−1 X
k∈Ωj
Mkyj
kqk(j),
(15.33)
where we exploited the fact that MT
k Mk = Mk. Once computed, we should normal-
ize this vector. The update of qk( j) is obtained by
ˆqk( j) =
h
aT
j Mkaj
i−1 aT
j Mky j
k,
(15.34)
computed for all k ∈Ωj.
Table 15.2 presents the results of such K-SVD algorithm on the same tests per-
formed above, with 15 iterations of this sparse-coding – dictionary update alternat-
ing algorithm. These experiments indicate that a substantial gain in performance can
be obtained, and even more so if the initial dictionary is non-ideal for the inpainting
task. The image Peppers seems to agree with the redundant DCT, thus leaving a
smaller room for an improvement.
Figures 15.14, 15.15, and 15.16 present the results of a similar experiment, per-
formed on the image Fingerprint, with a mask that consists of carved text. As
before, we start with the redundant DCT dictionary and obtain RMS E = 16.13
grey-values. After 15 iterations of the K-SVD inpainting algorithm, the result im-
proves by 2.05dB to RMS E = 12.74 grey-values. Figure 15.14 shows the original
image, the given noisy image (with σ = 20), and these two results. Figure 15.15
shows the absolute error images for these two results, and Figure 15.16 presents the

332
15 Other Applications
RMSE for
RMSE for
RMSE for
Algorithm
25% missing pixels 50% missing pixels 75% missing pixels
DCT: No-overlaps
14.55
19.61
29.70
DCT: Full-overlaps
9.00
11.55
18.18
K-SVD: Full-overlaps
8.1 (0.85dB)
10.05(1.25dB)
17.74(0.15dB)
Table 15.2 RMSE for the K-SVD inpainting algorithm on Peppers with 15 iterations. The results
in the ﬁrst two rows are the ones presented in Table 15.1. The last row gives the K-SVD results,
both in terms of the RMSE and the gain achieved[dB].
Fig. 15.14 Top left: The original Fingerprint image (portion of 200 × 200 pixels); Top Right:
The measured image with the additive noise (σ = 20) and the missing pixels (text mask); Bottom
left: Redundant DCT inpainting result (RMS E = 16.13); Bottom right: K-SVD (15 iterations)
result (RMS E = 12.74).
histogram of these two errors, indicating that the K-SVD error is more concentrated
around the origin.
The ability to inpaint an image using local and simple operations, as described
above, has been extended by Mairal. et. al. to video and color images. We bring here

15.3 Image Inpainting and Impulsive Noise Removal
333
Fig. 15.15 Redundant DCT absolute error (left), and K-SVD absolute error (right).
−150
−100
−50
0
50
100
150
0
200
400
600
800
1000
1200
1400
1600
Error
Count
 
 
DCT error
K−SVD error
Fig. 15.16 Histogram of the two error images – the redundant DCT and the K-SVD ones.
two examples that illustrate these capabilities, and we point the interested reader to
the papers by Mairal, Elad, and Sapiro for more details.
Figure 15.17 presents an inpainting result obtained on a color image. The red text
represents missing pixels, and these are recovered using the K-SVD inpainting. The
patches used in this experiment are 3D ones (of size 9 × 9 × 3, chosen large enough
to ﬁll-in the holes), containing the three color layers within each patch. This also
means that the trained dictionary has color atoms.
Figure 15.18 shows the results obtained for video inpainting. The sequence
Table-Tennis (images of size 240 × 352) has been corrupted by randomly eras-
ing 80% of its pixels. This ﬁgure shows 5 frames from the original sequence, the

334
15 Other Applications
Fig. 15.17 Inpainting result using the local-K-SVD algorithm and taking into account the color in-
formation (this result is taken from the work by Mairal, Elad, and Sapiro (2008)). Top: the original
image, Center: the degraded image with red text representing missing pixels, Bottom: the recovered
image (PSNR= 32.45dB).

15.3 Image Inpainting and Impulsive Noise Removal
335
corrupted frames, and the recovered results. The results include inpainting of im-
ages independently, as described in this chapter, and video inpainting, where the
spatio-temporal volume is treated as a 3D image. Thus, 3D patches are used for the
inpainting, and the dictionary is propagated from one frame to the next, as described
in Chapter 14 for the K-SVD video denoising. Another important modiﬁcation in-
troduced in this experiment is the use of a multi-scale dictionary, implying that the
atoms are structured in several scales.
15.3.3 Inpainting Images – The Global MCA
In the section above we treated the image inpainting problem by operating on small
patches. Just as with the image denoising or image separation tasks, this can be
replaced with a globally operating algorithm, that transforms the complete image.
In this section we brieﬂy describe this option, and show several representative results
to illustrate it.
An important beneﬁt of the migration to a global treatment, is the ability to handle
larger holes in the image. On the down side, we leave aside the notion of trained
dictionaries, and use instead ﬁxed ones. This also means that, as opposed to trained
dictionaries that may contain both texture and cartoon atoms, the ﬁxed dictionaries
should be chosen such that they accommodate these two types of image content.
This ties us back to the image separation discussion given above.
Indeed, when we discussed the task of image separation to cartoon and texture
in Section 15.2.1, we mentioned that inpainting is an application that may ﬁnd this
separation useful. As we show next, the framework we adopt to describe the global
inpainting algorithm is the very same one used for the global separation of images,
with a simple modiﬁcation.
We start our treatment of the global inpainting problem by using the analysis-
based image separation formulation given in Equation (15.5). We modify this to
give the following objective, which takes into account the mask operator M,
ˆyc, ˆyt = arg min
yc,yt
λ∥Tcyc∥1 + λ∥Ttyt∥1
(15.35)
+ 1
2∥y −M(yc + yt)∥2
2.
Similar to the conversion done to a synthesis-like formulation in Equation (15.9),
we propose the alternative optimization problem
ˆxc, ˆxt = arg min
xc,xt
λ∥xc∥1 + λ∥xt∥1
(15.36)
+ 1
2∥y −MAcxc −MAtxt∥2
2
Subject to TcAcxc = xc and TtAtxt = xt.

336
15 Other Applications
Fig. 15.18 Results obtained with the proposed multiscale K-SVD video inpainting for 5 frames
of the table-tennis sequence. Each row corresponds to one frame, with the following from left
to right: the original frame, the same frame with 80% missing data, the results obtained when
applying the image inpainting algorithm to each frame separately (PSNR=24.38dB), and the result
of the video inpainting K-SVD ( PSNR=28.49dB). The last row shows a zoomed version of the
last frame.

15.3 Image Inpainting and Impulsive Noise Removal
337
The only diﬀerence between this and Equation (15.9) is the presence of the mask op-
erator. Thus, the very same numerical algorithm can be proposed for solving (15.36),
with a slight modiﬁcation. Any appearance of Ac (or At) in the SSF formula should
be replaced with MAc (MAt, correspondingly). Thus,
ˆx
k+ 1
2
c
= Sλ
 1
cAc
TMT 
y −MAcˆxk
c −MAtˆxk
t

+ ˆxk
c
!
ˆx
k+ 1
2
t
= Sλ
 1
cAt
TMT 
y −MAcˆxk
c −MAtˆxk
t

+ ˆxk
t
!
,
The projection stage that follows should not be touched, as it is operating on the
complete representation vectors,
ˆxk+1
c
= TcAcˆx
k+ 1
2
c
and
ˆxk+1
t
= TtAtˆx
k+ 1
2
t
.
We turn now to show several results of the global inpainting method, taken from
the paper by Elad, Starck, Querre, and Donoho (2005). We start with a synthetic
experiment, where we compose the image Adar as a combination of texture and
cartoon. Figure 15.19 shows this image Adar with two cases of missing data. The
results of the global inpainting method using curvelets and global DCT dictionaries
are shown in this ﬁgure as well. Both results show no trace of the original holes, and
look near-perfect.
Figure 15.20 shows the results of a similar experiment, where the existing pixels
in the image are also contaminated by a zero-mean white Gaussian noise (σ =
10). As can be seen, the residual is almost featureless, implying that the noise was
removed successfully, without absorbing true texture of cartoon content.
The results shown in Figure 15.21 correspond to an experiment similar to the
one shown in Figure 15.19, but on the image Barbara, which is already a mixture
of cartoon and texture. The global inpainting method applied here uses the Haar
wavelet for the cartoon, and a homogeneous decomposition level wavelet packets to
represent the texture. Again, we see near-perfect recovery of the missing pixels.
Figure 15.22 presents the Barbara image and its ﬁlled-in results for three ran-
dom patterns of 20%, 50%, and 80% missing pixels. The unstructured form of the
mask makes the reconstruction task easier. As before, the inpainting method ap-
plied here used wavelet and wavelet packets to represent the cartoon and the texture
respectively, and again, the results look natural and artifact-free.
The last ﬁgure in this section, Figure 15.23, presents a similar experiment on the
Barbara image, this time ﬁlling in 9 blocks of sizes 8 × 8, 16 × 16, and 32 × 32
pixels. Filling-in large holes is more challenging, compared to the random mask. We
see that as the regions of missing pixels grow, the recovery deteriorates, as expected,
and smooth behavior is enforced.

338
15 Other Applications
Fig. 15.19 Two synthetic Adar images (top and bottom left) with combined cartoon and texture,
and imposed missing pixels. The results of the global inpainting algorithm are given in the top and
bottom right.
15.3.4 Impulse-Noise Filtering
We conclude our discussion on the image inpainting problem by introducing another
application related to it – impulsive noise removal. The objective of this section is
to show the fundamental ability of the above inpainting algorithms to handle this
noise removal problem.
The topic of denoising has been discussed in length in Chapter 14, but with an
emphasis on white additive Gaussian noise. Another very commonly encountered
noise in images is an impulsive noise, where a fraction of the pixels in the image
are corrupted by a large negative or positive deviated value. Figure 15.24 presents
the original image Peppers and a noisy version of it with such a noise. In this case,
10% of the pixels (in random locations) are corrupted. These pixels are modiﬁed by
adding or subtracting 50 grey-values8 to their original value. This way, we get dark
and bright points on the image, representing the noise. This noise is also commonly
referred to as salt and pepper noise because of its appearance as salt and pepper
grains spread over the image.
8 The ﬁnal grey-value is clipped to the range [0, 255], and thus the eventual deviation may be
smaller than 50 grey-values.

15.3 Image Inpainting and Impulsive Noise Removal
339
Fig. 15.20 Global inpainting results with two missing pixels masks – curves (top) and text (bot-
tom)), both contaminated by an additive noise. Left: The inpainting results, Right: the residual.
Salt-and-Pepper noise is typically handled well by the median ﬁlter, which re-
places every pixel value by the median of its small neighborhood. Figure 15.25
(middle right) shows the result obtained with such a ﬁlter, and it is clearly seen that
it is working well, leading in this case to an RMSE of 7.42 grey-values. The median
ﬁlter used here applies a 3 × 3 neighborhood – experiments with larger or smaller
sized neighborhood were found to give poorer results.
The question is: could we do better than the median ﬁlter? If we knew the location
of the noisy pixels, we could have addressed this denoising problem as an inpainting
of these as missing pixels. In this section we aim to demonstrate this alternative
denoising approach.
Estimating the location of the corrupted pixels can be done by evaluating the dif-
ference between the noisy image and its median-ﬁltered version, attributing large
diﬀerences to noisy pixels. Figure 15.25 (middle left) shows the estimated mask by
thresholding this diﬀerence with 27 grey-value. This threshold value was found em-
pirically to lead to good performance. The image contains 6, 554 corrupted pixels,
and the mask detects 6, 298 pixels, of which only 5, 211 are true-noisy ones. The
pixels that were undetected correspond to clipped noisy pixels with small deviation,
or others in which the noise is not as evident (near-edge pixels, etc.).
Using the estimated mask, we can proceed with an inpainting process, which
aims the recovery of these as missing pixels. The result of a local inpainting al-

340
15 Other Applications
Fig. 15.21 The Barbara image (top and bottom left) with two imposed masks of missing pixels.
The results of the global inpainting algorithm are given in the top and bottom right.
gorithm that uses the redundant DCT dictionary is shown in Figure 15.25 (bottom
right). The sparse-coding is performed with the OMP algorithm with a small error
threshold (σ = 5). Once done, only the missing pixels are replaced, ﬁxing the rest
as correct ones. The RMSE of this method is 5.86, which is clearly better than the
median ﬁltering result.
If the estimated mask is indeed reliable, we could use it to improve the median
ﬁltering as well, by applying the median ﬁltering only on the corrupted pixels. This
result is shown in Figure 15.25 (bottom left), with an RMSE of 6.52. While this
gives an improvement, it is clear that the inpainting approach is still superior. Fig-
ure 15.26 presents the absolute error images for the three denoising processes dis-
cussed above (median ﬁltering, masked median ﬁltering, and the inpainting-based
approach). Again, we see that the inpainting approach leads to the smallest error.
We should note that there is an extensive literature on impulsive noise removal,
using variational methods, diverse median and order-ﬁltering methods, and more.
This literature also oﬀers more elegant and reliable ways to estimate the location of
the corrupted pixels. In this section we deliberately restricted the discussion to ele-
mentary ideas and algorithms, focusing on the core ability of inpainting algorithms
to serve this problem as well.

15.4 Image Scale-Up
341
 
   
 
    
 
 
   
Fig. 15.22 Three Barbara images with 20%, 50%, and 80% missing pixels (right), and the global
inpainting results (left).
15.4 Image Scale-Up
We end this chapter with the image scale-up problem, often referred to in the liter-
ature as super-resolution.9 Our goal is to up-sample a single image while produc-
ing a sharp result. This task can be posed as an inverse problem, where the given
low-resolution image is assumed to be the measurement obtained from an original
9 The term super-resolution may be confusing, as it is also used in the context of fusing several
low-resolution images into one high-resolution result. Here we are referring to a diﬀerent process
of scaling-up a single image.

342
15 Other Applications
     
 
     
 
     
 
Fig. 15.23 Three Barbara images with three patterns of missing pixels – 9 blocks of size 8 × 8,
16 × 16, and 32 × 32 pixels (right), and the global inpainting results (left).
high-resolution image that went through a blur, a decimation, and additive noise
eﬀects.
The literature oﬀers various ways to address the image scale-up problem, ranging
from simple linear space-invariant interpolation schemes (bilinear, bicubic, Lanc-
zos), to spatially adaptive and non-linear ﬁlters of various sorts. Sparse-Land-based
algorithms for handling this problem also exist, and here as well we can divide the
various methods to global and local ones. In this chapter we shall focus on the algo-
rithm proposed by Yang et. al., which operates locally on patches. While we follow
Yang’s work to large extent, we also introduce several important diﬀerences that
lead to improved results.

15.4 Image Scale-Up
343
Fig. 15.24 The original image Peppers and its noisy version, with 10% of the pixels corrupted by
an impulsive noise.
15.4.1 Modeling the Problem
We start by describing the problem model. The given low-resolution image, zl is
created from the high-resolution one, yh by
zl = SHyh + v,
(15.37)
where H and S are the blur and decimation operators, respectively. We shall assume
that S performs a decimation by an integer factor, by discarding rows/columns from
the input image. The vector v is an additive zero-mean white and homogeneous
Gaussian noise, with a known standard-deviation σ.
In order to avoid complexities caused by the diﬀerent resolutions of zl and yh,
we shall assume hereafter that the image zl is scaled-up by a simple interpola-
tion scheme that ﬁlls-in the missing rows/columns, returning to the size of yh. This
scaled-up image shall be denoted by yl,
yl = QSHyh + Qv = Lallyh + ˜v.
(15.38)
The operator Q stands for the interpolation scale-up operation. Our goal is to process
yl and produce a result ˆyh, which will get as close as possible to the original high-
resolution image, yh.
The processing we are about to present operates on patches extracted from yl,
aiming to estimate the corresponding patch from yh. Let ph
k = Rkyh ∈IRn be a
high-resolution image patch of size √n × √n, extracted by the operator Rk from
the image yh in location k. We shall assume that the locations to consider, k, are
only those centered around true pixels in the low-resolution image yl (as opposed
to ﬁlled-in pixels due to the interpolation). We shall refer hereafter to this set of
samples as Ω.
It is now time to invoke the Sparse-Land model: We shall further assume that ph
k
can be represented sparsely by qk ∈IRm over the dictionary Ah ∈IRn×m, namely,
ph
k = Ahq, where ∥q∥0 ≪n.

344
15 Other Applications
Fig. 15.25 The original image Peppers (top left), its noisy version (top right), the original image
with the estimated mask on top (middle left), and three denoised results obtained with (i) the
median ﬁlter (middle right, RMS E = 7.42), (ii) the median ﬁlter applied only on the detected
points in the estimated mask (bottom left , RMS E = 6.52), and (iii) the redundant DCT inpainting
applied with the estimated mask (bottom right, RMS E = 5.86).
Consider the corresponding low-resolution patch pl
k = Rkyl, extracted from yl in
the same location10 k, such that its size is √n × √n. Since the operator Lall = QSH
transforms the complete high-resolution image yh to the low-resolution one, yl, we
can assume that pl
k = Lph
k + ˜vk, where L is a local operator being a portion of Lall,
and ˜vk is the additive noise in this patch. Note that L is a spatially independent
operator, as we consider only locations k ∈Ωfrom yl.
Since we have assumed that ph
k = Ahq, multiplication of this equation by L gives
10 We assume that pl
k and ph
k are centered around the same pixel.

15.4 Image Scale-Up
345
Fig. 15.26 The absolute error images between the original image and: (i) the noisy image (top
left), (ii) the median result (top right), (iii) the masked median ﬁltered image (bottom left), and (iv)
the inpainted result (bottom right).
Lph
k = LAhq.
(15.39)
Exploiting the relation between the low-resolution and the high-resolution patches,
pl
k = Lph
k + ˜vk, we thus get
LAhq = Lph
k = pl
k −˜vk
(15.40)
implying that
∥pl
k −LAhq∥2 ≤ϵ,
(15.41)
where ϵ is related to the noise power σ of ˜v.
The key observation from the above derivations is that the low-resolution patch
should be represented by the same sparse vector q over the eﬀective dictionary Al =
LAh, with a controlled error ϵl. This implies that for a given low-resolution patch
pl
k, we should ﬁnd its sparse representation vector, q, and then we can recover ph
k
by simply multiplying this representation by the dictionary Ah. This is the core idea
behind the image scale-up algorithm as developed by Yang et. al, as we are about to
present.

346
15 Other Applications
15.4.2 The Super-Resolution Algorithm
The scale-up algorithm we are about to present consists of a training phase that
includes the following steps:
1. Patch-Pairs Construction: For a set of training images that are given in high and
low-resolution pairs {y j
h, yj
l }j, we extract pairs of matching patches that form
the training database, P = {ph
k, pl
k}j. Each of these patch-pairs undergoes a pre-
processing stage that removes the low-frequencies from ph
k and extracts features
from pl
k.
2. Dictionary Training: A dictionary Al is trained for the low-resolution patches,
such that they can be represented sparsely. A corresponding dictionary Ah is con-
structed for the high-resolution patches, such that it matches the low-resolution
one.
The above training phase is done oﬀ-line, producing the two dictionaries, Al and
Ah, to be used in the super-resolution reconstruction.
Given a test low-resolution image yl to be scaled-up (recall that yl is already
interpolated to the destination size, and all we need to perform is a spatial non-linear
ﬁltering that sharpens this image), we extract pre-processed patches pl
k from each
location k ∈Ω, and then sparse-code it using the trained dictionary Al. The found
representation qk is then used to recover the high-resolution patch by multiplying it
with Ah. We now turn to describe all the above process is more details.
15.4.2.1 Training Phase
The training phase starts by collecting several images {y j
h}j, which are the high-
resolution examples. Each of these images is blurred and down-scaled by a factor s.
This leads to the formation of the corresponding low-resolution images {z j
l }j, which
are then scaled up back to the original size, resulting with the set {yj
l }j. Thus,
yj
l = Lallyj
h + ˜vj.
It is important to note that the same operator Lall should be used both in the training
and the testing phases.
The next step to perform is pre-processing. Rather than extracting small image-
patches and applying this step on them, we employ the desired pre-processing di-
rectly on the full images, and only then extract the patches. This avoids boundary
problems due to the small patch size.11
The pre-processing applied on the high-resolution images consists of a re-
moval of their low-frequencies. This is done by computing the diﬀerence images
11 A patch of size √n × √n in yl should correspond to a larger patch in yh, because of the spatial
extent of the blur and the scale-up operations. Nevertheless, this additional “band” of pixels can be
disregarded, as if we concentrate on predicting only the center of the destination patch from yh.

15.4 Image Scale-Up
347
e j
h = y j
h −yj
l . The reason for this step is the desire to focus the training on charac-
terizing the relation between the low-resolution patches, and the edges and texture
content within the corresponding high-resolution patches. As for the pre-processing
of the low-resolution images, these are ﬁltered using K high-pass ﬁlters, in order
to extract local features that correspond to their high-frequency content. Thus, each
low-resolution image y j
l leads to a set of K ﬁltered images, fk⊗yj
l for i = 1, 2, . . . , K
(⊗stands for a convolution).
After the two pre-processing steps described above, we are ready to extract the
local patches, and form the data-set P = {ph
k, pl
k}k. Considering only locations k ∈
Ω, we extract patches of size √n × √n pixels from the high-resolution images ej
h.
The corresponding low-resolution patches are extracted from the same locations
in the ﬁltered images fk ⊗y j
l and using the same size ( √n × √n pixels). Thus,
every corresponding K such low-resolution patches are merged into one vector ˜pl
k
of length nK. Note that the patch size should be at least of size s × s so as to cover
the high-resolution image. If the patch size is larger, we get overlaps that improve
the reconstruction result.
The last step before turning to the dictionary-learning stage is to reduce the
dimension of the input low-resolution patches, {˜pl
k}k. We apply the Principal-
Component-Analysis (PCA) algorithm on these vectors, seeking a subspace on
which we could project these patches while preserving 99.9% of their average en-
ergy. The motivation for this step is the knowledge that the low-resolution patches
could not occupy the full nK space, since they are eﬀectively built of patches of size
√n/s × √n/s in the low-resolution image z j
l that go through a set of K linear ﬁlters.
The scaling up and the ﬁltering are not expected to increase the dimensionality of
the resulting patches. By such a dimensionality reduction we expect to save com-
putations in the subsequent training and super-resolution algorithms. We denote by
B ∈IRnl×nK the projection operator that transforms the patch ˜pl
k to its reduced feature
vector, pl
k = B˜pl
k ∈IRnl.
Turning to the dictionary-learning part in the training, we start with the low-
resolution training patches {pl
k}k. We apply a regular MOD or K-SVD training pro-
cedure for these patches, resulting with the dictionary Al ∈IRnl×m. Within this learn-
ing procedure, sparse-coding is performed using the OMP with a ﬁxed number of
atoms per example, L. As a side product of this training, we obtain the sparse repre-
sentation vector qk that corresponds to the training patch pl
k.
After constructing Al, we proceed to the high-resolution dictionary construction.
Recall that it is our intention to recover the patch ph
k by approximating it as being
ph
k ≈Ahqk, namely, use the found sparse representation vector for the low-resolution
patch, and multiply it by the high-resolution dictionary. Thus, we should seek a
dictionary Ah such that this approximation is as exact as it can be. Thus, we deﬁne
this dictionary to be the solver of the problem
Ah = arg min
Ah
X
k
∥ph
k −Ahqk∥2
2
(15.42)
= arg min
Ah ∥Ph −AhQ∥2
F,

348
15 Other Applications
where we have constructed the matrix Ph, such that the high-resolution training
patches, {ph
k}k from its columns, and similarly, Q contain {qk}k as its column. The
solution of the problem is given by
Ah = PhQ+ = PhQT(QQT)−1.
(15.43)
We should note that the above approach disregards the fact that the high-
resolution patches overlap, and thus a better training procedure can be envisioned
for computing Ah. Bearing in mind that the eventual high-resolution images (in the
test stage) would be constructed by positioning these patches and averaging over
their overlaps, we should optimize Ah such that these resulting images are as close
as possible to their origin. Put formally, and using the similar treatment given in
Chapter 14 (Section 14.3), the image ˆyj
h should be constructed by the formula
ˆyj
h = y j
l +

X
k∈Ω
RT
k Rk

−1 
X
k∈Ω
RT
k Ahqk
.
(15.44)
Thus, it is natural to deﬁne the best dictionary Ah as the solution of the optimization
task
Ah = arg min
Ah
X
j
yj
h −y j
l −ˆyj
h

2
2
(15.45)
= arg min
Ah
X
j
yj
h −y j
l −

X
k∈Ω
RT
k Rk

−1 
X
k∈Ω
RT
k Ahqk


2
2
.
The reason y j
l appears in the errors computation is the fact that the patches in Ph
are constructed from the diﬀerence images e j
h = yj
h −y j
l , and this means that for the
image ˆyj
h to be constructed, we should return these low-frequencies.
While this minimization task is more demanding, it is expected to lead to better
output quality. We shall not explore this here, and in the experiments given below,
we adopt the simpler way to derive Ah.
This concludes the training phase of the super-resolution algorithm.
15.4.2.2 Testing Phase: Scaling-Up an Image
We are given a test low-resolution image zl to be magniﬁed. This image is assumed
to have been generated from a high-resolution image yh by the same blur and scale-
down operations as used in the training. The following are the steps performed in
order to complete the super-resolution algorithm:
1. Scale this image up by a factor of s using bicubic interpolation, resulting with yl.
2. Filter the image yl using the same K high-pass ﬁlters that were used for feature
extraction in the training, and obtain fk ⊗yl.

15.4 Image Scale-Up
349
3. Extract patches from these K images, each of size √n× √n from locations k ∈Ω.
Every K such patches that correspond to the same location are to be concatenated
to form a patch vector ˜pl
k. This leads to the set {˜pl
k}k.
4. Multiply the found patches {˜pl
k}k by the projection operator B to reduce their
dimensionality, resulting with the set {pl
k}k, each patch of length nl (≈30).
5. Apply the OMP algorithm on {pl
k}k, allocating L atoms to their representation,
and ﬁnd the sparse representation vectors {qk}k.
6. Multiply the representation vectors {qk}k by the high-resolution dictionary Ah,
and obtain the approximated high-resolution patches, {Ahqk}k = {ˆph
k}k.
7. Construct the ﬁnal super-resolved image by putting ˆph
k to their proper location,
averaging in overlap regions, and adding yl to the ﬁnal image. Put diﬀerently,
compute the ﬁnal image by the formula, which performs exactly the above-
described procedure:
ˆyh = yl +

X
k∈Ω
RT
k Rk

−1 X
k∈Ω
RT
k ˆph
k.
(15.46)
15.4.3 Scaling-Up Results
We present two experiments that demonstrate the above-described scaling-up algo-
rithm in action.
The ﬁrst test is applied on images showing a printed text. The training image
(screen-grabbed from a PDF ﬁle) is shown in Figure 15.27 – note that only one im-
age is used for training, and adding more is expected to improve the results. The
global operator Lall in this experiment is implemented by ﬁrst blurring the high-
resolution images y j
h with a 1D ﬁlter [1, 3, 4, 3, 1]/12 both horizontally and verti-
cally, and then down-sampling it by a factor s = 3. I.e., the scaled-down image zl is
one-ninth of the original image size. The image yl is created by bicubic interpolation
of zl, returning to the original size.
Extraction of features from the low-resolution images is done using 4 ﬁlters that
perform 1-st and 2-nd horizontal and vertical derivatives: f1 = [1, −1] = f T
2 and
f3 = [1, −2, 1] = f T
4 . These ﬁlters are applied such that only sampled pixels are used
in the ﬁltering computation.12 The patch size used is n = 9, and the PCA results with
a reduction from 4 · 81 = 324 to nl ≈30. The dictionary training procedure applied
40 iterations of the K-SVD algorithm, with m = 1, 000 atoms in the dictionary, and
allocating L = 3 atoms per patch-representation.
The test image (a diﬀerent image, grabbed from a diﬀerent page, but having the
same scale) is shown in Figure 15.28. This ﬁgure shows the original test image, and
the scaled-down version that should be scaled-up. The scaling-up results are shown
12 This means that we either ﬁlter zl and then interpolate, or we ﬁlter yl with zero-padded ﬁlters of
the form f1 = [0, 0, 1, 0, 0, −1] = f T
2 and f3 = [1, 0, 0, −2, 0, 0, 1] = f T
4 .

350
15 Other Applications
Fig. 15.27 First experiment: The training image for the image scaling-up algorithm. This image is
of size 717 × 717 pixels, and it provides a set of 54, 289 training patch-pairs.
in Figure 15.29, and it is evident that the outcome is far better, compared to the
bicubic interpolation, showing an improvement of 2.27dB.
The second experiment is applied on the image Building. Starting from the
original image yh of size 800 × 800 pixels, we ﬁlter this image with the separable
ﬁlter [1, 2, 1]/4 (horizontally and vertically), and then down-scale it by factor s = 2
to obtain zl of size 400 × 400.
In this experiment we train the dictionaries using the very same image, by further
scaling it down by a factor s = 2, resulting with the image zll of size 200 × 200. The
image pair {zl, zll} is used for the training, based on the expectation that the relation
between these two images reﬂects also the relation that should be used to go from
zl to yh.
Extraction of features from the low-resolution images is done using the same 4
ﬁlters, and the dimensionality reduction leads this time to nl ≈42. The training data
contains 37, 636 pairs of low- and high-resolution patches to be modeled. The pa-
rameters of the dictionary training all remain the same (40 iterations of the K-SVD
algorithm, m = 1000 atoms in the dictionary, and L = 3 atoms per representation).
Figure 15.30 shows the original image yh, the bicubic scaled-up image yl, and
the result of the scaling up algorithm, ˆyh. The diﬀerence between the two images is
3.32dB, and in order to see where these diﬀerences reside, the ﬁgure also shows the
the image |ˆyh −yh|. Figure 15.31 shows two 100 × 100 portions extracted from yh,
yl, and ˆyh, to better show the visual gain achieved by the scaling-up algorithm.

15.4 Image Scale-Up
351
Fig. 15.28 First experiment: The test image zl to be scaled-up (left), and its original high-resolution
version yh (right). The image zl is of size 120 × 120 pixels, and it provides a set of 12, 996 patches
to operate on.
Fig. 15.29 First experiment: The scaling up results – the image yl that was scaled-up using a
bicubic interpolation (left, RMSE= 47.06), and the algorithm’s result ˆyh (right, RMSE= 36.22).
15.4.4 Image Scale-Up: Summary
There are various ways to scale-up an image while preserving edges and small de-
tails. In this chapter we introduced only one such algorithm that illustrates how
sparse representation modeling and dictionary-learning can be used for this goal.
The presented algorithm is based on the method proposed by Yang et. al., with few
modiﬁcation. This method is relatively simple, and yet it produces a substantial im-
provement over bicubic interpolation. The algorithm operates by training a pair of

352
15 Other Applications
Fig. 15.30 Second experiment: The original Building image yh (top-left), the bicubic interpo-
lated image yl (bottom-left, RMSE= 12.78), the algorithm’s result ˆyh (bottom-right, RMSE= 8.72),
and the diﬀerence image |ˆyh −yh| magniﬁed by 5 (top-right).
low- and high-resolution dictionaries, using either training images, or exploiting a
lower-resolution version of the same image to be handled.
Various further improvements can be considered, and these are likely to enhance
the algorithm’s output quality. One such option is back-projection, as suggested by
Yang et. al.: The image ˆyh we construct does not necessarily conforms with the
requirement Lallˆyha ≈yl. Thus, we may project the result ˆyh to this constraint. An-
other option is to force the overlapping patches ˆph
k to better align with each other.
This can be done by operating sequentially on the incoming patches pl
k, and when
applying the sparse coding stage (to produce qk, add a penalty on the distance be-
tween the newly constructed patch, ˆph
k, and the ones already computed.

15.5 Summary
353
Fig. 15.31 Second experiment: Portions from the original Building image yh (left), the bicubic
interpolated image yl (middle), and the algorithm’s result ˆyh (right. Notice that the original portions
show some compression artifacts, which do not appear in the scaled-up results.
15.5 Summary
In this chapter we have seen how three diﬀerent applications in image processing
are treated using the same tools and methodologies, all based on the Sparse-Land
model. Each of these problems can be attacked using either global or local sparse
representation modeling, and there are close similarities between the various algo-
rithms considered.
Here and in the previous chapter, we have seen a migration from the core Sparse-
Land model to solutions of practical problems in image processing. This path from
theory to practice is not direct nor trivial, as there is a lot of freedom in how to tie the
model to the problem. Addressing applications in image processing with this model
is therefore a work of art, and in this chapter we have seen several very successful
exemplars of this creative process.
We end the technical part of this book here, but we should note that the work
on other applications in image processing that use Sparse-Land is on-going. Topics
such as image and video compression, multi-modal (e.g., video-audio) processing,
image segmentation, target detection in images, fusion of images, super-resolution
from a set of images, and more, are likely to beneﬁt from the Sparse-Land model.
Hopefully, many more ideas and contributions will appear in the near-future, along
the lines described here.

354
15 Other Applications
Further Reading
1. E. Abreu, M. Lightstone, S.K. Mitra SK, and K. Arakawa, A new eﬃcient ap-
proach for the removal of impulse noise from highly corrupted images, IEEE
Trans. on Image Processing, 5(6):1012–1025, June, 1996.
2. P. Abrial, Y. Moudden, J.-L. Starck, J. Bobin, M.J. Fadili, B. Afeyan and M.
Nguyen, Morphological component analysis and inpainting on the sphere: Ap-
plication in physics and astrophysics, Journal of Fourier Analysis and Applica-
tions (JFAA), special issue on “Analysis on the Sphere”, 13(6):729–748, 2007.
3. M. Aharon, M. Elad, and A.M. Bruckstein, The K-SVD: An algorithm for de-
signing of overcomplete dictionaries for sparse representation, IEEE Trans. on
Signal Processing, 54(11):4311–4322, November 2006.
4. M. Antonini, M. Barlaud, P. Mathieu, and I. Daubechies, Image coding us-
ing wavelet transform, IEEE Trans. on Image Processing, 1(2):205-220, April
1992.
5. J. Aujol, G. Aubert, L. Blanc-Feraud, and A. Chambolle, Image decomposi-
tion: Application to textured images and SAR images, INRIA Project ARIANA,
Sophia Antipolis, France, Tech. Rep. ISRN I3S/RR- 2003-01-FR, 2003.
6. J. Aujol and A. Chambolle, Dual norms and image decomposition models, IN-
RIA Project ARIANA, Sophia Antipolis, France, Tech. Rep. ISRN 5130, 2004.
7. J. Aujol and B. Matei, Structure and texture compression, INRIA Project ARI-
ANA, Sophia Antipolis, France, Tech. Rep. ISRN I3S/RR-2004-02-FR, 2004.
8. M. Bertalmio, G. Sapiro, V. Caselles, and C. Ballester, Image in-painting, in
Proc. 27th Annu. Conf. Computer Graphics and Interactive Techniques, pp.
417–424, 2000.
9. M. Bertalmio, L. Vese, G. Sapiro, and S. Osher, Simultaneous structure and
texture image inpainting, IEEE Trans. on Image Processing, 12(8):882–889,
August 2003.
10. A.L. Bertozzi, M. Bertalmio, and G. Sapiro, NavierStokes ﬂuid dynamics and
image and video inpainting, IEEE Computer Vision and Pattern Recognition
(CVPR), 2001.
11. J. Bobin, Y. Moudden,J.-L. Starck, M.J. Fadili, and N. Aghanim, SZ and CMB
reconstruction using GMCA, Statistical Methodology, 5(4):307–317, 2008.
12. J. Bobin, Y. Moudden, J.-L. Starck and M. Elad, Morphological diversity and
source separation, IEEE Trans. on Signal Processing, 13(7):409–412, 2006.
13. J. Bobin, J.-L. Starck, M.J. Fadili, and Y. Moudden, Sparsity, morphologi-
cal diversity and blind source separation, IEEE Trans. on Image Processing,
16(11):2662–2674, 2007.
14. J. Bobin, J.-L. Starck J. Fadili, Y. Moudden and D.L Donoho, Morphological
component analysis: an adaptive thresholding strategy, IEEE Trans. on Image
Processing, 16(11):2675–2681, 2007.
15. E.J. Cand`es and F. Guo, New multiscale transforms, minimum total variation
synthesis: Applications to edge-preserving image reconstruction, Signal Pro-
cessing, 82(5):1516–1543, 2002.

Further Reading
355
16. V. Caselles, G. Sapiro, C. Ballester, M. Bertalmio, J. Verdera, Filling-in by joint
interpolation of vector ﬁelds and grey levels, IEEE Trans. on Image Processing,
10:1200–1211, 2001.
17. T. Chan and J. Shen, Local inpainting models and TV inpainting, SIAM J. Ap-
plied Mathematics, 62:1019–1043, 2001.
18. T. Chen T, K.K. Ma, and L.H. Chen, Tri-state median ﬁlter for image denoising,
IEEE Trans.on Image Processing, 8(12):1834–1838, December, 1999.
19. R. Coifman and F. Majid, Adapted waveform analysis and denoising, in Progress
in Wavelet Analysis and Applications, Frontiers ed., Y. Meyer and S. Roques,
Eds., pp. 6376, 1993.
20. A. Criminisi, P. Perez, and K. Toyama, Object removal by exemplar based in-
painting, IEEE Computer Vision and Pattern Recognition (CVPR), Madison,
WI, June 2003.
21. J.S. De Bonet, Multiresolution sampling procedure for analysis and synthesis
of texture images, Proceedings of SIGGRAPH, 1997.
22. A.A. Efros and T.K. Leung, Texture synthesis by non-parametric sampling,
International Conference on Computer Vision, Corfu, Greece, pp. 1033-1038,
September 1999.
23. M. Elad and M. Aharon, Image denoising via learned dictionaries and sparse
representation, IEEE Computer Vision and Pattern Recognition, New-York,
June 17-22, 2006.
24. M. Elad and M. Aharon, Image denoising via sparse and redundant representa-
tions over learned dictionaries, IEEE Trans. on Image Processing 15(12):3736–
3745, December 2006.
25. M. Elad, J-L. Starck, P. Querre, and D.L. Donoho, Simultaneous cartoon and
texture image inpainting using morphological component analysis (MCA),
ber 2005.
26. H.L. Eng and K.K. Ma, Noise adaptive soft-switching median ﬁlter, IEEE
Trans.on Image Processing, 10(2):242–251, February, 2001.
27. M.J. Fadili, J.-L. Starck and F. Murtagh, Inpainting and zooming using sparse
representations, The Computer Journal, 52(1):64–79, 2009.
28. G. Gilboa, N. Sochen, and Y.Y. Zeevi, Texture preserving variational denoising
using an adaptive ﬁdelity term, in Proc. VLSM, Nice, France, pp. 137144, 2003.
29. O.G. Guleryuz, Nonlinear approximation based image recovery using adaptive
sparse reconstructions and iterated denoising - Part I: Theory, IEEE Trans. on
Image Processing, 15(3):539–554, 2006.
30. O.G. Guleryuz, Nonlinear approximation based image recovery using adaptive
sparse reconstructions and iterated denoising - Part II: Adaptive algorithms,
IEEE Trans. on Image Processing, 15(3):555–571, 2006.
31. J. Mairal, F. Bach, J. Ponce, G. Sapiro and A. Zisserman, Discriminative learned
dictionaries for local image analysis, IEEE Conference on Computer Vision and
Pattern Recognition, Anchorage, Alaska, USA, 2008.
32. J. Mairal, M. Elad, and G. Sapiro, Sparse representation for color image restora-
tion, IEEE Trans. on Image Processing, 17(1):53–69, January 2008.
Journal on Applied and Computational Harmonic Analysis, 19:340–358, Novem-

356
15 Other Applications
33. J. Mairal, M. Leordeanu, F. Bach, M. Hebert and J. Ponce, Discriminative
sparse image models for class-speciﬁc edge detection and image interpretation,
European Conference on Computer Vision (ECCV) Marseille, France, 2008.
34. J. Mairal, G. Sapiro, and M. Elad, Learning multiscale sparse representations
for image and video restoration, SIAM Multiscale Modeling and Simulation,
7(1):214–241, April 2008.
35. F. Malgouyres, Minimizing the total variation under a general convex constraint
for image restoration, IEEE Trans. on Image Processing, 11(12):1450-1456,
December 2002.
36. F. Meyer, A. Averbuch, and R. Coifman, Multilayered image representation:
1080, September 2002.
37. Y. Meyer, Oscillating patterns in image processing and non linear evolution
equations, in Univ. Lecture Ser., vol. 22, AMS, 2002.
38. M. Nikolova, A variational approach to remove outliers and impulse noise,
Journal Of Mathematical Imaging And Vision, 20(1-2):99–120, January 2004.
39. G. Peyr´e, J. Fadili and J.-L. Starck, Learning the morphological diversity, to
appear in SIAM Journal on Imaging Sciences.
40. L. Rudin, S. Osher, and E. Fatemi, Nonlinear total variation noise removal al-
gorithm, Phys. D, 60:259-268, 1992.
41. A. Said and W. Pearlman, A new, fast, and eﬃcient image codec based on
set partitioning in hierarchial trees, IEEE Trans. on Circuits Systems for Video
Technology, 6(3):243–250, June 1996.
42. J. Shapiro, Embedded image coding using zerotrees of wavelet coeﬃcients,
IEEE Trans. on Signal Processing, 41(12):3445–3462, December 1993.
43. N. Shoham and M. Elad, Alternating KSVD-denoising for texture separation,
The IEEE 25-th Convention of Electrical and Electronics Engineers in Israel,
Eilat Israel, December, 2008.
44. J.-L. Starck, E.J. Cand`es, and D. Donoho, The curvelet transform for image
denoising, IEEE Trans. on Image Processing, 11(6):131-141, June 2002.
45. J.-L. Starck, D. Donoho, and E.J. Cand`es, Very high quality image restora-
tion, the 9th SPIE Conf. Signal and Image Processing: Wavelet Applications in
Signal and Image Processing, A. Laine, M. Unser, and A. Aldroubi, Eds., San
Diego, CA, August 2001.
46. J.L. Starck, M. Elad, and D.L. Donoho, Image decomposition via the combina-
tion of sparse representations and a variational approach, IEEE Trans. on Image
Processing, 14(10):1570–1582, October 2005.
47. J.-L. Starck, M. Elad, and D.L. Donoho, Redundant multiscale transforms and
their application for morphological component analysis, Journal of Advances
in Imaging and Electron Physics, 132:287–348, 2004.
48. J.-L. Starck and F. Murtagh, Astronomical Image and Data Analysis, New York:
Springer-Verlag, 2002.
49. J.-L. Starck, F. Murtagh, and A. Bijaoui, Image Processing and data analysis:
The multiscale approach, Cambridge, U.K.: Cambridge Univ. Press, 1998.
Application to image compression, IEEE Trans. on Image Processing, 11(9):1072–

Further Reading
357
50. J.-L. Starck, M. Nguyen, and F. Murtagh, Wavelets and curvelets for image
deconvolution: A combined approach, Signal Processing, 83(10):2279–2283,
2003.
51. G. Steidl, J. Weickert, T. Brox, P. Mrazek, and M. Welk, On the equivalence of
soft wavelet shrinkage, total variation diﬀusion, total variation regularization,
and sides, Dept. Math., Univ. Bremen, Bremen, Germany, Tech. Rep. 26, 2003.
52. L. Vese and S. Osher, Modeling textures with total variation minimization
and oscillating patterns in image processing, Journal of Scientiﬁc Computing,
19:553-577, 2003.
53. M. Vetterli, Wavelets, approximation, and compression, IEEE Signal Process-
ing Magazine, 18(5):59–73, September 2001.
54. J. Yang, J. Wright, T. Huang, and Y. Ma, Image super-resolution as sparse repre-
sentation of raw image patches, IEEE Computer Vision and Pattern Recognition
(CVPR), 2008.
55. J. Yang, J. Wright, T. Huang, and Y. Ma, Image super-resolution via sparse rep-
resentation, submitted to IEEE Trans. on Image Processing, September 2009.
56. M. Zibulevsky and B. Pearlmutter, Blind source separation by sparse decompo-
sition in a signal dictionary, Neur. Comput., 13:863-882, 2001.

Chapter 16
Epilogue
16.1 What is it All About?
The ﬁeld of image processing oﬀers an unusually fertile playground for applied
mathematicians, where the distance between an abstract mathematical idea and an
application or even a product may be small. This explains both the presence of so
many mathematicians in this ﬁeld, and the fact that research in it became so math-
ematically oriented in the past two decades. The activity on sparse and redundant
representation modeling is just one manifestation of both these trends.
In this book we have explored the concept of sparse representations for signals
and images. Although sparse representation is a poorly deﬁned problem and a com-
putationally impractical goal in general, we bring in this book a long series of recent
mathematical results, showing that under certain conditions one can obtain results of
a positive nature, guaranteeing uniqueness, stability, and computational practicality.
Inspired by such positive results, we discussed in this book how these can be
used to form an appealing and strong source model for signals and images. We have
shown several potential applications that use this model of sparse representation in
real image processing settings, and showed cases where this leads to state-of-the-
art results. More speciﬁcally, problems such as image denoising, image deblurring,
facial image compression, image inpainting, and image scale-up, all beneﬁt from
this model.
16.2 What is Still Missing?
This book is far from being complete. There are many theoretical, numerical, and
applicative topics related to sparse representations, with an already existing and
solid knowledge, which have been deliberately omitted from this book. These in-
clude topics such as joint-sparsity, compressed-sensing, low-rank matrix comple-
© Springer Science+Business Media, LLC 2010
M. Elad, Sparse and Redundant Representations: From Theory to Applications in Signal 
and Image Processing, DOI 10.1007/978-1-4419-7011-4_16,
359

360
16 Epilogue
tion, non-negative matrix factorization and non-negative sparse coding, blind source
separation, multi-modal sparse representation, relation to wavelet theory, and more.
Furthermore, in recent years we have seen more and more application branches
that successfully lean on this model, going beyond the image processing activity
we have explored in this book. These include tasks in machine learning, computer-
(mostly studying the human visual system), inverse problems in geo-sciences, MRI
imaging, error-correcting-codes, and more.
Beyond all these missing parts that correspond to an existing knowledge, this
book also lacks in the fact that there are some holes in its presentation, which corre-
spond to yet unstudied pieces. While this book aims to give a ﬂuent and compelling
view of the understanding and knowledge that have grown around Sparse-Land,
there are key questions that remain open at this stage. For example:
• A fundamental issue in using the Sparse-Land model in signal/image processing
is the question of the suitability of this model for such signals in the ﬁrst place.
The approach taken in this book (and in the research it documents) is an empir-
ical “try and see” kind of answer. More work is required to carefully map the
connections between Sparse-Land and other models, such as Markov-random-
ﬁeld (MRF), PCA and its generalizations, and more. Perhaps a more ambitious
goal would be to characterize the kind of signal-sources that ﬁnd Sparse-Land as
a suitable model.
• Another fundamental question refers to ﬂaws that exist in the Sparse-Land
model. For example, this model ignores the existing dependencies between atoms
in the sparse representation. Another manifestation of these ﬂaws is exhibited by
the fact that one cannot synthesize reasonable signals from this model – for ex-
ample, a direct approach of randomly generating a sparse vector x with iid entries
and multiplying it by the dictionary does not lead to a natural image, even if the
dictionary is of good quality for images. Model extensions to better match this
model to true data are desired, and these are likely to take applications to the next
level of performance.
Other fundamental issues to be studied and solved include topics such as analy-
sis of pursuit techniques with non-worst-case measures (as opposed to the mutual-
coherence and the RIP), development of a multi-scale dictionary-learning paradigm,
getting performance guarantees for dictionary learning algorithms, setting the proper
redundancy for a dictionary, and more. Hopefully, the research to be done in the
coming years will address some of these issues.
16.3 Bottom Line
Despite these holes in the presentation and the missing pieces mentioned above, this
book oﬀers an important and organized view of this ﬁeld, setting the foundations on
which all the above missing parts are (or could be) built. As the activity in the ﬁeld of
vision, pattern-recognition, wireless communication, audio processing, brain-research

16.3 Bottom Line
361
sparse and redundant representations is so vivid and expanding, we feel it is time to
look back at the achievements made in the past two decades or so, and summarize
them clearly, as a way to understand where we are standing today, and this way,
better plan for the future research in this area. This is the approach taken in this
book.
It is very probable that other books will follow in the near future, in an attempt
to cover better these missing parts and the novel research projects that will emerge.
Indeed, perhaps a second edition of this very book would make such an attempt.
Time will tell.

Notation
(By order of their ﬁrst appearance in the book)
Chapter 1:
n
Signal’s (vector’s) dimension
m
Number of atoms (columns) in the dictionary (matrix)
IR
Real numbers
IRn
n-dimensional Euclidean real vector space
IRn×m
Euclidean space for real matrices of size n × m
A, b
The elements of the linear system Ax = b
AT
The transpose of the matrix A
A−1
The inverse of the matrix A
A+
The Moore-Penrose pseudo-inverse of the matrix A
J(x)
Penalty function
L
Lagrangian functional
Ωor C
Sets
∥x∥p
p
The p-th power of the ℓp-norm of the vector x
∥x∥2
2
The squared-ℓ2-norm – a sum of the squares of the entries in x
∥x∥1
The ℓ1-norm – a sum of absolute values of the entries in x
∥x∥∞
The ℓ∞-norm – a maximal absolute value among the entries in x
∥x∥wℓp
The weak-ℓp-norm
∥x∥0
The ℓ0-norm – a count of the non-zeros in the vector x
(P1)
Seeking the ℓ1-shortest solution of a linear system
(Pp)
Seeking the ℓp-shortest solution of a linear system
(P0)
Seeking the sparest solution of a linear system
Chapter 2:
Ψ, Φ
Unitary matrices
xΨ, xΦ
The two portions of the sparse representation in the two-ortho case
I
The identity matrix
F
The Fourier matrix
363

364
Notation
µ(A)
Mutual Coherence of the matrix A
rank(A)
The rank of the matrix A
spark(A)
The smallest number of columns in A that are linearly dependent
G
The Gram matrix
µ1(A)
Tropp’s babel function for the matrix A
UΣVT
The Singular Value Decomposition of a matrix
Chapter 3:
As
A portion of the matrix A containing the columns in the set s
ak
The k-th column in A
rk
Residual vector at the k-th step
Sk
The estimated support at the k-th step
δ(A)
The universal decay-factor for OMP
S
The support (active set) of x
W
A diagonal weight matrix
X
A diagonal matrix containing |x|q on its main diagonal
Chapter 4:
k0
Cardinality – the number of non-zeros in the representation
xΨ, xΦ
The two parts of the representation in the two-ortho case
kp, kq
Number of non-zeros in the vectors xΨ, xΦ in the two-ortho case
C
A set of vectors
xmin
The minimal absolute entry in x
xmax
The maximal absolute entry in x
Chapter 5:
(Pϵ
0)
Seeking the sparest ϵ-approximate solution of a linear system
ϵ, δ
Allowed error
v, e
Noise or perturbation vectors
σs
The s singular value of a matrix
Sparkη(A)
The smallest number of columns in A that are at most η away (in terms
of singular values) from linear dependence
δs
The RIP constant
λmin(A)
Smallest singular value of A
λmax(A)
Largest singular value of A
(Pϵ
1)
Seeking the ℓ1-shortest ϵ-approximate solution of a linear system
(Qλ
1)
A variant of (Pϵ
1) with penalty replacing the constraint
λ
A coeﬃcient weighting the sparsity penalty
∂f
A sub-gradient of f
σ
Random noise standard deviation
Chapter 6:
ρ(x)
A separable sparsity penalty term
Sρ,λ(x)
An element-wise shrinkage on x, whose shape is dictated by ρ, and thresh-
old by λ

Notation
365
d(x1, x2))
Distance between the vectors x1 and x2
diag(A)
The diagonal of the square matrix A
P
Projection operator
Chapter 7:
NΨ, NΦ
Same as kp, kq, used in Candes and Romberg’s results
P(event)
Probability of event
Chapter 8:
(Pλ
DS )
The Danzig-Selector optimization problem
θs1,s2
The restricted orthogonality property parameter
Chapter 9:
Y
A set of example signals
P(y)
A prior probability on the signal y
ˆP(y)
An estimated prior probability on the signal y
MA,k0,α,ϵ
The Sparse-Land model
Ωδ
y0
A set containing the δ neighborhood of y0
c
A center vector of a neighborhood, computed as the mean
Qy0
A projection matrix that considers the permissible subspace around y0
T
An analysis operator (transform)
µtrue
The relative volume in IRn with high-probability signals
µmodel
The relative volume in IRn with high-probability signals, based on the
model
Chapter 10:
H
A linear degradation operator (e.g., blur)
ˆy
Measured signal
ˆx
Estimated representation
Chapter 11:
Ps(|s|)
The probability density function of the cardinality |s|
PX(x)
The probability density function of the representation vector x
R1, R2
Random generators of sparse representation vectors
σx
The variance of the non-zero entries in the representation vector x
σe
The variance of the noise entries in the vector e
ˆxMAP
The maximum-a’posteriori probability estimation
ˆxMMS E
The oracle estimation (that knows the correct support)
ˆxoracle
The minimum mean-squared-error estimation
P(x|y)
Conditional probability of x given y
E(expression)
The expected value of the random expression
xs
The non-zero part in the vector x
qs
The relative weights of the various solutions that are merged in the MMSE
estimate
det(A)
The determinant of the matrix A

366
Notation
Chapter 12:
Y
A matrix containing signal examples as its columns
X
A matrix containing representation vectors as its columns
E j
A matrix containing signal representation errors as its columns, excluding
the j atom
⊗
Kronecker product
A0
A ﬁxed dictionary
Z
A matrix containing sparse representation of the dictionary atoms
tr(A)
The trace of the matrix A
Rk
Patch extraction operator from location k in an image
Chapter 13:
B
Budget of bits
P
Number of pixels in the image
n
Block-size
m
Code-book (or dictionary) size
hk
A deblocking ﬁlter for the k-th location
ek
The k-th element in the trivial basis
Chapter 14:
y0
An ideal image of size N
ˆy
The estimated image (denoised)
ˆx
The estimated image’s representation
pi j
A small patch extracted from an image at location [i, j]
qi j
A patch’s pi j sparse representation
˜qi j, ˆqi j
Estimated sparse representations of pij
c
Coeﬃcients shaping the shrinkage functions
θ
Set of parameters governing the denoising algorithm
h(y, θ)
The denoising formula/algorithm
η(ˆy, y)
An unbiased estimator of the MSE
Chapter 15:
yc
The cartoon-part in an image
yt
The texture-part in an image
ˆyc
The estimated cartoon-part in an image
ˆyt
The estimated texture-part in an image
Mc, Mt
Number of atoms in the carton and texture dictionaries,. respectively
Tc, Ac
Analysis/synthesis operators for the cartoon part
Tt, At
Analysis/synthesis operators for the texture part
Aa
The total dictionary, containing both Ac and At
qc, qt
The cartoon and texture sparse-representations vector for an image patch
M
A mask matrix, describing the remaining samples in the signal/image
S
A decimation operator

Notation
367
Q
An interpolation operator
yh
An ideal high-resolution image
zl
A blurred-decimated-noisy version of yh
yl
An interpolated version of zl
eh
The diﬀerence image between yl and yh
Lall
The overall operator that relates yl to yh
Al
A dictionary for the low-resolution features
Ah
A dictionary corresponding to Ah, for the high-resolution content
fk
Derivative ﬁlters that are used to extract features
P = {ph
k, pl
k}
Training set of high- and low-resolution patches
B
A dimensionality reduction projection operator for the low-resolution
patch features

Acronyms
(Alphabetical ordered)
2D
Two Dimensional
BCR
Block Coordinate Relaxation
BM3D
Block-Matching and 3D ﬁltering
BP
Basis Pursuit
BPDN
Basis Pursuit Denoising
B/W
Black and White
CD
Coordinate Descent
CG
Conjugate Gradient
CMB
Cosmic Microwave Background
CoSaMP
Compressive Sampling Matching Pursuit
CS
Compressed Sensing
DCT
Discrete Cosine Transform
DS
Danzig Selector
ERC
Exact Recovery Condition
FFT
Fast Fourier Transfrom
FOCUSS
FOcal Under-determined System Solver
GSM
Gaussian Scale Mixture
ICA
Independent Component Analysis
iid
Independent and Identically Distributed
IRLS
Iterated Reweighed Least Squares
ISNR
Improvement Signal to Noise Ratio
JPEG
Joint Photographic Experts Group
KLT
Karhunen Loeve Transform
K-SVD
K times Singular Value Decomposition
K-Means
K times Mean computation
LARS
Least Angle Regression
LASSO
Least Absolute Shrinkage and Selection Operator
LP
Linear Programming
LS
Least Squares
369

370
Acronyms
MAP
Maximum A’posteriori Probability
MCA
Morphological Component Analysis
MMSE
Minimum Mean-Squared-Error
MOD
Method of Directions
MP
Matching Pursuit
MRF
Markov Random Field
MRI
Magnetic Resonance Imaging
MSE
Mean Squared Error
NLM
Non Local Means
NP
Non Polynomial
OMP
Orthogonal Matching Pursuit
PCA
Principal Component Analysis
PCD
Parallel Coordinate Relaxation
PDF
Probability Density Function
PSNR
Peak Signal to Noise Ratio
QP
Quadratic Programming
RIP
(or RIC) Restricted Isometry Property (or Condition)
RMSE
Root Mean Squared Error
ROP
Restricted Orthogonality Property
SESOP
Sequential Subspace Optimization
SNR
Signal to Noise Ratio
SSF
Separable Surrogate Functionals
SSIM
Structural Similarity
StOMP
Stage-wise Orthogonal Matching Pursuit
SURE
Stein Unbiased Risk Estimator
TV
Total Variation
SVD
Singular Value Decomposition
VQ
Vector Quantization

Index
accelerate/acceleration, 127, 128, 132, 191,
199, 233
acoustic, 14, 177, 178
adversary, 104, 107, 153, 157
amalgam, 18, 26, 149
analysis, 178, 179, 186, 312–314, 335
anomaly, 3, 169
anti-symmetric, 281
artifact, 256, 257, 263, 265, 266, 268, 278,
292, 303, 315, 337
asymptotic, 140, 144
atom, 66, 74, 95, 98, 123, 124, 128, 132,
154, 172, 173, 176, 178, 180, 188, 202,
216, 217, 227–235, 239, 240, 242–244,
251–253, 255, 259, 262, 264, 276, 288,
290, 292, 294, 295, 297, 301, 302, 311,
312, 320, 321, 325, 326, 331, 333, 335,
347, 349, 350, 360
atomic-decomposition, 172, 176
audio, 174, 353, 360
auto-correlation, 172, 261
average performance, 137, 143, 144, 157
B/W, 303
Babel function, 27, 28, 71, 75
back-project, 117, 121–124, 352
bandelet, 227, 275
Basis-Pursuit, 50–52, 55, 58, 59, 62, 64, 68,
69, 71–75, 89, 95, 137, 138, 141–143,
153, 155, 163–165, 276, 286
Basis-Pursuit-Denoising, 95, 103, 105,
153–155, 157
Bayes, 203, 204, 208, 211, 213
Bayesian, 169, 202, 278
Beltrami-ﬂow, 295
bicubic, 342, 348–351
bilateral-ﬁlter, 294, 295
bilinear, 342
binary, 233
bit-allocation, 249, 252
bit-rate, 255, 257, 261, 264, 265, 267, 268
Block-Coordinate
descent, 242
minimization, 287, 329
rationale, 233
Relaxation, 113, 115, 117, 120, 230, 241,
289
block-matching, 296, 297
blockiness, 263–266, 268, 278, 279
BM3D, 199, 296, 297, 303
Bound-Optimization, 115, 117, 118
BP, see Basis-Pursuit
BPDN, see Basis-Pursuit-Denoising
Bregman, 134
Canny edge detector, 315
cartoon, 178, 227, 310–312, 314–316,
320–322, 335, 337
Cauchy-Schwartz inequality, 21, 36, 39
CD, see Coordinate-Descent
CG, see Conjugate-Gradient
chemistry, 172
circulant, 141, 242
color, 248, 291, 303, 332, 333
combinatorial, 13, 23, 24, 28, 101, 215
compressed-sensing, 124, 144, 156, 177, 359
compression, 169, 171, 177, 221, 247–249,
251–257, 260–262, 264, 265, 267, 269,
309, 311, 353, 359
computer-vision, 315, 360
Conjugate-Gradient, 50, 91, 111, 119, 124, 127
contourlet, 227, 275, 310, 312
converge, 6, 49, 51, 114, 116, 118, 125, 132,
191, 290
371

372
Index
convergence, 29, 44, 93, 97, 111, 112, 117,
119, 120, 125, 132, 134, 230, 233, 234,
240, 243, 244
convergent, 120
convex, 5–7, 9, 13, 26, 28, 48, 50, 51, 90, 95,
111, 114, 115, 117, 125, 154, 189, 191
convexifying, see convex
Convexity, see convex
convolution, 242, 347
Coordinate-Descent, 112, 120
cortex, see visual cortex
CoSaMP, 125
cosine, 171
covariance, 122, 208, 213
cross-correlation, 243
curvelet, 227, 275, 310, 312, 314–316, 337
CVX, 51, 90
cyclic, 170, 242, 285
Dantzig-Selector, 107, 153–159, 163–165, 204
DCT, see Discrete-Cosine-Transform
deblock, 263, 265–269, 283
deblurring, 115, 132, 185, 186, 188, 191, 198,
199, 247, 283, 309, 359
decay-factor, 43, 44, 207
decimation, 342, 343
decoder, 249, 251–253, 267
decompression, 251, 260, 267
denoising, 104, 111, 125, 170, 174, 175,
177–180, 273–276, 278, 279, 282,
285, 288, 290–292, 294–298, 300–303,
309–311, 316, 317, 320, 323, 326–328,
335, 338–340, 359
detection, 92, 100, 169, 178, 249, 252, 253,
311, 315, 353
DFT, see Discrete-Fourier-Transform
diagonal
-block, 38, 281
-main, 41, 49, 87, 103, 119, 128, 142, 178,
242
-oﬀ, 25, 27–30, 44, 84, 87, 103, 142
main, 27, 70
matrix, 41, 49, 91, 119, 127, 242, 275, 276,
300, 301
diagonally-dominant, 26, 75, 84
dictionary-learning, 228, 229, 231, 233, 234,
238, 239, 241, 242, 244, 247, 289, 297,
320, 330, 347, 351, 360
dictionary-update, 231, 233, 234, 241, 243,
330, 331
diﬀerentiable, 5, 7, 95, 102, 112, 299, 300
diﬀusion, 295
discrete, 13, 21, 35, 186, 227
Discrete-Cosine-Transform, 142, 171, 235,
237
1D-, 235
2D-, 171, 172, 239, 279, 282, 290, 291
3D-, 296
global, 312, 314, 316, 337
local, 312, 315
over-complete, 223
redundant, 239, 289, 290, 320, 325, 329,
331, 340
unitary, 163, 223, 278, 312
Discrete-Fourier-Transform, 171
disk, 26, 84, 87, 107
double-sparsity, 239
down-sampling, 349
down-scale, 346, 350
DS, see Dantzig-Selector
dual, 62, 63, 311
edge, 264, 268, 274, 311, 315, 339, 347, 351
eigenvalue, 26, 45, 75, 84, 87, 107, 120, 158,
261, 314
eigenvector, 261
ellipsoid, 117, 172
EM, see Expectation-Maximization
empirical, 13, 68, 71, 76, 98, 103, 128, 137,
140, 150, 207, 212, 220, 228, 233, 339,
360
encoder, 251, 252
encryption, 178
entropy-coding, 247–249, 252
equiangular, 25
equivalence, 39, 57, 58, 65, 68, 76, 80, 141,
144, 149, 155, 179
equivalent, 8, 10, 36, 37, 59, 68, 101–103, 127,
155, 163, 164, 170, 173, 174, 178, 179,
209, 211, 240, 241, 251, 313
ERC, see Exact Recovery Condition
error-correcting-code, 26, 360
Exact Recovery Condition, 73
Exact-recovery-Condition, 75
Expectation-Maximization, 112, 117, 118
facial, 247–249, 251, 253–257, 267, 269, 359
factorization, 229, 360
FFT, see Fourier
FHT, see Hadamard
ﬁlling-in, 309, 324, 337
ﬁrst-derivative, 171, 186
ﬁxed-point, 49, 112, 115, 119, 120
FOCUSS, 48, 50, 51
Fourier, 19, 278
Discrete-Fourier-Transform, 22, 171
Fast-Fourier-Transform (FFT), 242

Index
373
identity-Fourier, 24
matrix, 18, 21
transform, 18, 21
Frame, 14, 25, 149, 187, 227, 241, 275, 310,
313
frequency, 18, 21, 170, 227, 257, 314, 347
full-overlap, 285, 312, 330
full-rank, 3, 4, 17, 25, 29, 44, 65, 68, 149, 179,
313
galaxy, 315, 316
Gaussian, 24, 92, 98, 122, 124, 125, 128, 137,
153, 154, 157, 158, 163, 172, 173, 177,
185, 202, 205, 207–209, 211, 213, 214,
230, 274, 275, 278, 279, 284, 288, 298,
300, 303, 312, 314, 325, 328, 337, 338,
343
Gaussian-Scale-Mixture, 278
generalized, 84, 107, 111, 175, 237
geo-science, 360
Gershgorin, 26, 27, 84, 87, 107
global, 114, 278, 287, 311, 314, 319, 323, 335,
342, 349, 353
DCT, 312, 314, 316, 337
inpainting, 335, 337
inpainting, 327
MAP, 319
MCA, 311, 312
minimum/er, 6, 13, 50, 51, 71, 80, 113, 116,
125, 154, 233
model/ing, 274, 278
optimality/er, 17, 23, 24, 30, 95
solution, 51
thresholding, 282, 300
treatment, 273
globally optimal, 101, 102, 113
globally trained, 290
GPSR, 90
gradient, 112, 127, 171, 331
2D-, 186
Conjugate-, 50, 91, 111, 119, 124, 125, 127
projection, 91
sub-, 95, 96, 102, 112, 113
Grassmannian, 25, 28–31, 148
gray-value, 248
grayscale, 248, 261
greedy, 35, 36, 39, 41, 43, 45, 46, 51, 52, 65,
67, 90, 98, 112, 123, 138, 144, 211, 216,
217
GSM, see Gaussian-Scale-Mixture
Haar, 186, 311
dictionary, 276, 277, 311
transform, 186, 187, 190, 275
wavelet, 187, 189, 337
Hadamard, 19, 127, 171
Fast-Hadamard-Transform, 128
Heaviside, 178, 179, 186
Heisenberg’s uncertainty principle, 21
Hessian, 6, 115, 118, 127
histogram, 332
holes, 309, 327, 333, 335, 337, 360
homogeneity, 9, 12
Homotopy, 51, 91
hyper-cube, 169
hyperplane, 9, 10
iid, see independent and identically-distributed
ill-posed , 309
Image-Signature-Dictionary, 242, 243
Improvement-Signal-to-Noise Ratio, 194, 198
incoherence, 71
incoherent, 25, 71
independent and identically-distributed, 46, 92,
98, 124, 128, 141, 143, 145, 153, 157,
163, 177, 201, 202, 207, 230, 298, 300,
360
inpainting, 178, 309, 311, 324–328, 331, 333,
335, 337–340, 359, 373
invariant
-rotation, 186
-space, 170, 185, 342
-translation, 186
-unitary, 101, 112
invertible, 4, 48, 49, 173, 178, 204, 205, 312,
313, 323
IRLS, see Iterative-Reweighed-Least-Squares
ISD, see Image-Signature-Dictionary
ISI, 303
ISNR, see Improvement-Signal-to-Noise Ratio
isometry, 86, 156
isotropic, 315
Iterative-Reweighed-Least-Squares, 48, 51, 52,
90–93, 97, 98, 108, 111, 115, 119, 125,
127, 128, 132
iterative-shrinkage, 91, 111, 112, 115, 119,
120, 123–125, 127, 128, 132, 134, 154,
155, 194, 199, 275, 313
Jeﬀrey’s uninformative prior, 191
joint-sparsity, 297, 359
JPEG, 14, 171, 172, 249, 256, 261, 283
JPEG2000, 14, 247–250, 253, 256, 261, 269
K-Means, 233
K-SVD, 228, 233–236, 238–240, 261, 288,
289, 330–332

374
Index
algorithm, 231, 233, 240, 248, 251, 297,
331, 350
denoising, 291, 292, 297, 303, 317, 330
dictionary, 235, 255, 261, 290
inpainting, 327, 331, 333
learning, 268, 320
training, 251, 347
Karhunen-Loeve-Transform, 171
kernel, 186, 191, 194, 295
KLT, see Karhunen-Loeve-Transform
Kronecker-product, 235, 330
Kruskal-rank, 23
L1-LS, 51, 90
L1-Magic, 90
Lagrange multiplier, 4, 20, 49, 90, 314
Lagrangian, 4, 9, 20
Lanczos, 342
Laplacian, 170, 171, 250
large-deviation, 145, 147
LARS, see Least-Angle-Regression
LASSO, see Least-Absolute-Shrinkage-and-
Selection-Operator
Least-Absolute-Shrinkage-and-Selection-
Operator, 93, 95
Least-Angle-Regression, 90, 93, 95–98, 100,
108, 111, 124, 138
Least-Squares, 35, 38, 39, 41, 100, 106, 108,
111, 115, 124, 132, 157, 163, 230, 233,
240, 241, 267, 278, 281
lexicographic, 267, 278, 330
likelihood, 171, 203, 213
line-search, 116, 123, 127, 128, 132, 191
Linear-Programming, 8, 51, 62, 153–155
linearly-dependent, 7, 21, 23, 24, 28, 143, 324
linearly-independent, 26–28, 143, 324
local
-PCA, 175, 176, 180
DCT, 312, 315
K-SVD, 327
MCA, 311, 316, 320
minimum, 50, 51, 114, 116, 118, 120, 125,
203, 233, 243
log-likelihood, 154, 287, 319
low-dimension, 97, 119, 122, 127, 174, 221,
228, 238
LP, see Linear-Programming
LS, see Least-Squares
Majorization-Maximization, 118
MAP, see Maximum-A’posteriori-Probability
MAP-oracle, 205
marginal, 208, 211, 213, 218
Markov-Random-Field, 179, 180, 360
Matching-Pursuit, 39, 211
Least-Squares-Orthogonal- (LS-OMP), 38,
47
Orthogonal- (OMP), 36–39, 41, 42, 44, 45,
47, 51, 52, 55, 57, 58, 60, 65, 68, 71, 73,
74, 89, 90, 95, 98, 100, 102–104, 108,
111, 124, 138, 153, 165, 211, 212, 216,
221, 236, 239, 240, 252, 286, 288, 289,
291, 292, 326, 340, 347, 349
Random-Orthogonal- (Random-OMP), 216,
217, 221–223
Stagewise-Orthogonal- (StOMP), 123–125,
127
Weak- (W-MP), 39, 44, 47
Matlab, 29, 51, 62, 97, 137, 163, 260
matroid, 23
Maximum-A’posteriori-Probability, 154, 170,
201–203, 205, 206, 208–212, 215, 218,
220, 221, 224, 286, 289, 319, 328, 330
Maximum-Likelihood, 203
MCA, see Morphological Component Analysis
MCAlab, 90
Mean-Squared-Error, 185, 198, 204–206, 218,
219, 223, 250, 298, 299, 301, 302, 311
median, 339, 340
Method of Optimal Directions, 228, 230, 231,
233, 235, 236, 238, 239, 288, 289, 330,
347
Minimum-Mean-Squared-Error, 201–203, 205,
206, 212–221, 223, 224, 274, 279, 294,
295
minor, 26, 27, 84, 87, 90, 142
mixture, 173, 310, 317, 337
MMSE, see Minimum-Mean-Squared-Error
MMSE-oracle, 205
MOD, see Method of Optimal Directions
molecule, 172
monotone, 11, 27, 51, 83, 85, 114, 233
Morphological Component Analysis, 177, 178,
309–312, 316, 317, 320, 335
MP, see Matching-Pursuit
MRF, see Markov-Random-Field
MRI, 360
MSE, see Mean-Squared-Error
multi-modal, 353, 360
multi-scale, 171, 186, 228, 233, 238, 244, 249,
275, 335, 360
mutual-coherence, 19, 21, 25, 26, 28–30, 44,
56, 61, 66, 70, 73, 75, 84, 86, 87, 89,
103, 137, 142, 150, 156, 360
nearest-neighbor, 175, 176
Nesterov, 134
NLM, see Non-Local-Means

Index
375
non-convex, 6, 11, 23, 101, 102, 113, 191, 203
non-degenerate, 143
Non-Local-Means, 273, 292, 294–297, 303
non-negative, 8, 64, 122, 145, 189, 360
non-singularity, 205
nonsingular, 22
norm-equivalence, 87
NP-Hard, 14, 55
null-space, 9, 21, 23–25, 28, 74, 82, 83, 117,
204
oﬀ-support, 72, 75, 159
OGA - Orthogonal Greedy Algorithm, 39, 65
OMP, see Matching-Pursuit
on-support, 125
oracle, 157, 158, 163, 165, 204–207, 209, 212,
215–217, 219, 220
orientation, 186, 276, 277
orthogonal, 17, 19, 25, 26, 37, 38, 44, 81, 87,
171, 174, 175, 206, 207, 241
orthogonalily, 56
orthogonality, 103, 156
orthogonalization, 37
orthonormal, 19
oscillate, 290, 312, 320
over-complete, 223, 262
over-ﬁtting, 238, 263
Parallel-Coordinate-Descent, 120, 123, 125,
127, 128, 132, 188, 191, 194, 199
PCA, see Principal-Component-Analysis
PCD, see Parallel-Coordinate-Descent
PDF, see Probability-Density-Function
PDSCO, 90
Peak-Signal-to-Noise Ratio, 250, 252, 261,
262, 268, 275, 278, 285, 288, 290, 295,
296, 302, 303
periodic, 239, 285, 311
periodic table, 172
perturbation, 81, 82, 170, 173, 174, 180
PGA - Pure Greedy Algorithm, 39
photo-ID, 248
picket-fence, 21, 24, 25
piecewise linear, 281
piecewise smooth, 4, 178, 179, 227, 235, 239,
310, 314, 320
Poisson formula, 21, 25
polynomial, 280–282
positive-deﬁnite, 4, 6, 26, 27, 45, 50, 115, 142,
160
preconditioning, 127
primal, 62, 63
Principal-Component-Analysis, 171, 172, 175,
176, 180, 249, 261, 265, 269, 347, 349,
360
probabilistic, 71, 108, 140, 141, 143, 145, 157
Probability-Density-Function, 169, 172, 180,
201
Procrastes, 241
proximal, 112, 115, 116
pseudo-inverse, 4, 48, 49, 74, 205, 241, 312
PSNR, see Peak-Signal-to-Noise Ratio
pyramid, 233, 250
quadrant, 6
quantization, 252, 253, 260
Rademacher, 145
radius, 82, 103, 107
Random-OMP, see Matching-Pursuit
rank-deﬁcient, 44
rate-distortion, 177, 261, 267
rate-of-convergence, see convergence
rate-of-decay, 41
recognition, 178, 221, 311, 360
recursive, 38, 43
redundant, 14, 21, 22, 142, 187, 228, 239,
248–251, 253, 262, 268, 269, 273–275,
286, 289–291, 303, 309, 313, 320, 325,
329, 331, 340, 359, 361
Region-Of-Interest, 266
regularization, 4, 5, 8, 90, 95, 97, 170, 172,
310
relaxation, 35, 48, 51, 52, 79, 90, 93, 119, 288
Restricted-Isometry-Property, 86, 87, 89, 108,
150, 156–159, 161, 360
Restricted-Orthogonality-Property, 156, 157,
161
reweighed, see Iterative-Reweighed-Least-
Squares
RGA - relaxed Greedy Algorithm, 39
RGB, 291
ridgelet, 312
RIP, see Restricted-Isometry-Property
RMSE, see Root-Mean-Squared-Error
robust-statistics, 5
ROI, see Region-Of-Interest
Root-Mean-Squared-Error, 255, 259, 326, 331,
340
rotation-invariant, 186
saddle-point, 233
scale-invariance, 238
scale-up, 3, 4, 309, 341–343, 345, 346, 351,
359
scrambling, 178

376
Index
Separable Surrogate Functionals, 116, 117,
120, 123, 125, 127, 128, 132, 188, 191,
199, 313, 337
separation, 163, 177, 178, 309–312, 314–317,
320–322, 335, 360
Sequential Subspace Optimization, 127, 128,
132, 188, 191, 194
SESOP, see Sequential Subspace Optimization
shift-invariance, 242, 274
shrinkage, 111–114, 120, 122, 125, 134, 155,
189, 191, 215, 241, 273, 275, 278–282,
285, 286, 291, 296, 297, 300
ShrinkPack, 90
sign-pattern, 6, 7, 71–73
signature, 143, 242
singular-value, 29, 83, 84, 174, 242
Singular-Value-Decomposition, 29, 107, 232,
233, 241, 331
singularity, 87
software, 51, 97
space-invariant, 170, 185, 342
SPAMS, 51
Sparco, 51
Spark, 23–26, 28, 29, 46, 82, 84, 86, 87, 108,
137, 143, 324, 325
Sparse-Land, 172, 173, 175, 176, 178, 180,
185, 194, 198, 199, 224, 227, 228, 244,
248, 250, 278, 286, 303, 309, 310, 324,
325, 342, 343, 353, 360
Sparselab, 51, 90
spectral radius, 107, 120, 142
sphere, 74, 82, 103, 144, 170
spline, 273, 281
SSF, see Separable Surrogate Functionals
SSIM, see Structural Similarity
stability, 80, 82, 85–87, 89, 103, 104, 138, 144,
157, 359
standard-deviation, 92, 98, 128, 163, 207, 231,
274, 275, 326, 343
Steepest-Descent, 111, 128
steerable wavelets, 227, 275
StOMP, see Matching-Pursuit
strictly
convex, 5, 6, 115
diagonally-dominant, 75
positive, 20, 25, 44, 45, 50, 145
positive-deﬁnite, 6
Structural Similarity, 303
sub-gradient, 95, 96, 102, 112
sub-sample, 325
super-resolution, 341, 346–348, 353
SURE, 194, 273, 297–300, 302
surrogate function, 112, 115, 116
SVD, see Singular-Value-Decomposition
symmetric, 11, 84, 141, 314
synthesis, 173, 178–180, 186, 312, 313, 335
tangent, 95, 118
tensor, 23
texture, 178, 235, 239, 257, 290, 310–312,
314–317, 320, 321, 327, 335, 337, 347
thresholding, 45–47, 55, 67, 68, 102–107, 124,
125, 140, 144, 145, 148–150, 165, 275,
279, 286, 291, 296, 297, 320, 339
global-, 282, 300
hard-, 101, 275, 279, 300
soft-, 102, 155, 191, 313
tight, 28, 29, 58, 68, 104, 105, 140, 148, 187,
241, 275
Tikhonov, 170, 172
tolerance, 79, 85, 89
Total-Variation, 171, 178, 180, 186, 311, 320
translation-invariant, 186
transpose, 74, 187, 284
tree-K-Means, 249
TV, see Total-Variation
two-ortho, 19, 23–25, 27, 44, 58, 65, 68, 71,
80, 114, 117, 141, 142
uncertainty, 17–19, 21, 23, 28, 84, 373
undecimated, 186, 227
unimodal, 301
uniqueness, 5, 6, 13, 17, 18, 21, 23–28, 80–82,
85, 108, 141, 143, 229, 359
unitary, 21, 25, 29, 55, 60, 101, 112–114, 116,
120, 121, 125, 134, 149, 155, 163, 175,
189, 207, 210, 215, 219, 239, 241, 242,
274, 275, 278, 280, 282, 291, 312
Vector-Quantization, 249–251, 253, 261, 265,
269
video, 14, 247, 291, 292, 303, 333, 335, 353
visual cortex, 228
VQ, see Vector-Quantization
watermarking, 178
wavelet, 115, 142, 171, 186, 187, 227, 238,
244, 248, 274, 275, 311, 312, 316, 337,
360
wavelet packet, 227, 337
Weak-MP, see Matching-Pursuit
wedgelet, 275
Weighted-Minimum-Mean-Squared-Error, 203
well-posed, 30, 229
WGA - Weak Greedy Algorithm, 39
Wiener ﬁlter, 5, 170, 191, 205
WMMSE, see Weighted-Minimum-Mean-
Squared-Error
YCbCr, 291
zoom, 97, 132

