arXiv:1705.03394v1  [physics.pop-ph]  27 Apr 2017
That is not dead which can eternal lie: the
aestivation hypothesis for resolving Fermi’s
paradox
Anders Sandberg∗, Stuart Armstrong†, Milan ´Cirkovi´c‡
May 10, 2017
Abstract
If a civilization wants to maximize computation it appears rational to
aestivate until the far future in order to exploit the low temperature en-
vironment: this can produce a 1030 multiplier of achievable computation.
We hence suggest the “aestivation hypothesis”: the reason we are not
observing manifestations of alien civilizations is that they are currently
(mostly) inactive, patiently waiting for future cosmic eras. This paper
analyzes the assumptions going into the hypothesis and how physical law
and observational evidence constrain the motivations of aliens compatible
with the hypothesis.
Keywords: Fermi paradox, information physics, physical eschatology, extrater-
restrial intelligence
1
Introduction
Answers to the Fermi question (”where are they?”) typically fall into the groups
“we are alone” (because intelligence is very rare, short-lived, etc.), “aliens exist
but not here” (due to limitations of our technology, temporal synchronization
etc.)
or “they’re here” (but not interacting in any obvious manner).
This
paper will examine a particular version of the third category, the aestivation
hypothesis.
In previous work [1], we showed that a civilization with a certain threshold
technology could use the resources of a solar system to launch colonization
devices to eﬀectively every solar system in a sizeable part of the visible universe.
While the process as a whole would last eons, the investment in each bridgehead
∗Future of Humanity Institute, University of Oxford. Littlegate House, Suite 1, 16/17 St.
Ebbe’s Street. Oxford OX1 1PT. United Kingdom. anders.sandberg@philosophy.ox.ac.uk
†Future of Humanity Institute. stuart.armstrong@philosophy.ox.ac.uk
‡Future of Humanity Institute and Astronomical Observatory of Belgrade, Volgina 7, 11000
Belgrade, Serbia. mcirkovic@aob.rs
1

system would be small and brief. Also, the earlier the colonization was launched
the larger the colonized volume. These results imply that early civilizations have
a far greater chance to colonize and pre-empt later civilizations if they wish to
do so.
If these early civilizations are around, why are they not visible? The aes-
tivation hypothesis states that they are aestivating1 until a later cosmological
era.
The argument is that the thermodynamics of computation make the cost of
a certain amount of computation proportional to the temperature.
Our astrophysical and cosmological knowledge indicates that the universe is
cooling down with cosmic time. Not only star formation within galaxies winds
down and dies out on timescales of 109 - 1010 yrs but even the cosmic background
radiation temperature is becoming exponentially colder.
As the universe cools down, one Joule of energy is worth proportionally more.
This can be a substantial (1030) gain. Hence a civilization desiring to maximize
the amount of computation will want to use its energy endowment as late as
possible: using it now means far less total computation can be done. Hence an
early civilization, after expanding to gain access to enough raw materials, will
settle down and wait until it becomes rational to use the resources. We are not
observing any aliens since the initial expansion phase is brief and intermittent
and the aestivating civilization and its infrastructure is also largely passive and
compact.
The aestivation hypothesis hinges on a number of assumptions that will be
examined in this paper. The main goal is to ﬁnd what physical and motivational
constraints make it rational to aestivate and see if these can be met. If they
cannot, then the aestivation hypothesis is not a likely answer for the Fermi
question.
This paper is very much based on the physical eschatology research direction
started by Freeman Dyson in [2] combined with the SETI approach dubbed
“Dysonian SETI” [3].
Physical eschatology attempts to map the long-term
future of the physical universe given current knowledge of physics [4, 5]. This
includes the constraints for life and information processing as the universe ages.
Dysonian SETI takes the approach to widen the search to look for detectable
signatures of highly advanced civilizations, in particular megascale engineering.
Such signatures would not be deliberate messages, could potentially outlast their
creators, and are less dependent on biological assumptions about the originating
species (rather, they would stand out because of unusual physics compared to
normal phenomena).
It should be noted that in the physical eschatology context aestivation/hibernation
has been an important issue for other reasons: Dyson suggested it as a vi-
able strategy to stretch available energy and heat dissipation resources [2], and
Krauss and Starkman critiqued this approach [6].
1“Hibernation” might be a more familiar term but aestivation is the correct one for sleeping
through too warm summer months.
2

2
Aestivation
The Old Ones were, the Old Ones are, and the Old Ones shall be.
Not in the spaces we know, but between them. They walk serene
and primal, undimensioned and to us unseen.
H.P. Lovecraft, The Dunwich Horror and Others
An explanation for the Fermi question needs to account for a lack of observ-
able aliens given the existence of human observers in the present (not taking
humans into account can lead to anthropic biases: we are not observing a ran-
domly selected sample from all possible universes but rather a sample from
universes compatible with our existence [7]).
The aestivation hypothesis makes the following assmptions:
1. There are civilizations that mature much earlier than humanity.
2. These civilizations can expand over sizeable volumes, gaining power over
their contents.
3. These civilizations have solved their coordination problems.
4. A civilization can retain control over its volume against other civilizations.
5. The fraction of mature civilizations that aestivate is non-zero
6. Aestivation is largely invisible.
Assumption 1 is plausible, given planetary formation estimates [8]. Even a
billion years of lead time is plenty to dominate the local supercluster, let alone
the galaxy. If this assumption is not true (for example due to global synchro-
nization of the emergence of intelligence [9]) we have an alternative answer to
the Fermi question.
We will assume that the relevant civilizations are mature enough to have
mastered the basic laws of nature and their technological implications to what-
ever limit this implies. More than a million years of technological development
ought to be enough.
Assumption 2 is supported by our paper [1]. In particular, if a civilization
can construct self-replicating technological systems there is essentially no limit
to the amount of condensed matter in solar systems that can be manipulated.
These technological systems are still potentially active in the current era. Non-
expanding aestivators are entirely possible but would not provide an answer to
the Fermi question.
If assumption 2 is wrong, then the Fermi question is answered by a low
technology ceiling and that intelligence tends to be isolated from each other by
distance.
Assumption 3: This paper will largely treat entire civilizations as the actors
rather than their constituent members. In this regard they are “singletons”
in the sense of Nick Bostrom: having a highest level of agency that can solve
3

internal coordination problems [10]. Singletons may be plausible because they
provide a solution to self-induced technological risks in young technological civ-
ilizations (preventing subgroups from accidentally or deliberately wiping out
the civilization). Anecdotally, humanity has shown an increasing ability to co-
ordinate on ever larger scales thanks to technological improvements (pace the
imperfections of current coordination).
In colonization scenarios such as [1]
coordinating decisions made before launch could stick (especially if they have
self-enforcing aspects) even when colonies are no longer in contact with each
other. However, the paper does not assume all advanced civilizations will have
perfect coordination, merely very good coordination.
Assumption 4 needs to handle both civilizations emerging inside the larger
civilization and encounters with other mature civilizations at edges of coloniza-
tion. For the ﬁrst case the assumption to some degree follows from assumption
2: if desired the civilization could prevent misbehavior of interior primitive civi-
lizations (the most trivial way is to prevent their emergence, but this makes the
hypothesis incompatible with our existence [11]). Variants of the zoo hypothe-
sis work but the mature civilization must put some upper limit to technological
development or introduce the young civilizations into their system eventually.
The second case deals with civilizations on the same level of high technological
maturity: both can be assumed to have roughly equal abilities and resources. If
it can be shown that colonized volumes cannot be reliably protected against in-
vasion, then the hypothesis falls. Conversely, if it could be proven that invasion
over interstellar distances is logistically unfeasible or unproﬁtable, this could be
regarded as a (weak) probabilistic support for our hypothesis.
Assumption 5 is a soft requirement.
In principle it might be enough to
argue that aestivation is possible and that we just happen to be inside an aesti-
vation region. However, arguing that we are not in a typical galaxy weakens the
overall argument. More importantly, the assumption explains the absence of
non-aestivating civilizations. Below we will show that there are strong reasons
for civilizations to aestivate: we are not arguing that all civilizations must do it
but that it is a likely strategy. Below we will discuss the issues of civilizational
strategies and discounting that strengthen this assumption.
Assumption 6 is the key assumption for making the hypothesis viable as a
Fermi question answer: aestivation has to leave the universe largely unchanged
in order to be compatible with evidence. The assumption does not imply delib-
erate hiding (except, perhaps, of the core part of the civilization) but rather that
advanced civilizations do not dissipate much energy in the present era, making
them eﬀectively stealthy. The main empirical prediction of the hypothesis is
that we should observe a strong dampening of processes that reduce the utility
for far future computation.
3
Civilizational goals
While it may be futile to speculate on the values of advanced civilizations, we
would argue that there likely exists convergent instrumental goals.
4

What has value? Value theories assign value to either states of the world or
actions.
State value models require resources to produce high-value states. If happi-
ness is the goal, using the resources to produce the maximum number of max-
imally happy minds (with a tradeoﬀbetween number and state depending on
how utilities aggregate) would maximize value. If the goal is knowledge, the re-
sources would be spent on processing generating knowledge and storage, and so
on. For these cases the total amount of produced value increases monotonically
with the amount of resources, possibly superlinearly.
Actions also require resources and for actions with discernible eﬀects the
start and end states must be distinguishable: there is an information change.
Even if value resides in the doing, it can be viewed as information processing.
The assumption used in this paper is that nearly all goals beneﬁt from more
resources and that information processing and storage are good metrics for
measuring the potential of value creation/storage.
They are not necessarily
ﬁnal goals but instrumentally worth getting for nearly all goals for nearly all
rational agents [12, 13].
As we will see below, maintaining communication between diﬀerent parts
of an intergalactic civilization becomes impossible over time. If utility requires
contact (e.g. true value is achieved by knowledge shared across a civilization)
then these limits will strongly reduce the utility of expanding far outside a
galactic supercluster. If utility does not require contact (e.g. the happiness or
beauty of local entities is what is valuable in itself) then expanding further is
still desirable.
There may also be normative uncertainty: even if a civilization has a convinc-
ing reason to have certain ultimate values, it might still regard this conclusion
as probabilistic and seek to hedge its bets. Especially if the overall plan for col-
onization needs to be formulated at an early stage in its history and it cannot
be re-negotiated once underway it may be rational to ensure that alternative
value theories (especially if easily accommodated) can be implemented. This
may include encountered alien civilizations, that might possibly have diﬀerent
but valid ultimate goals.
4
Discounting
Another relevant property of a civilization is the rate of its temporal discounting.
How much is the far future worth relative to the present? There are several
reasons to suspect advanced civilizations have very long time horizons.
In a dangerous or uncertain environment it is rational to rapidly discount the
value of a future good since survival to that point is not guaranteed. However,
mature expanding civilizations have likely reduced their existential risks to a
minimum level and would have little reason to discount strongly (individual
members, if short-lived, may of course have high discount rates). More generally,
the uncertainty of the future will be lower and this also implies lower discount
rates.
5

Speciﬁc policies that take a long time to implement such as megascale en-
gineering or interstellar travel also favor low discount rates. There could be a
selection eﬀect here: high discount rates may prevent such long-range projects
and hence only low-discount rate civilizations will be actors on the largest scales.
It also appears likely that a suﬃciently advanced civilization could regulate
its “mental speed”, either by existing as software running on hardware with a
variable clock speed, or by simply hibernating in a stable state for a period.
If this is true, then the value of something after a period of pause/hibernation
would be determined not by the chronological external time but how much
time the civilization would subjectively experience in waiting for it. Changes
in mental speed can hence make temporally remote goods more valuable if the
observer can pause until they become available and there is no alternative cost
for other goods.
This is linked to a reduction of opportunity costs: advanced civilizations
have mainly “seen it all” in the present universe and do not gain much more
information utility from hanging around in the early era2
There are also arguments that future goods should not be discounted in
cases like this. What really counts is fundamental goods (such as well-being or
value) rather than commodities; while discounting prices of commodities makes
economic sense it may not make sense to discount value itself [14].
This is why even a civilization with some temporal discounting can ﬁnd
it rational to pause in order to gain a huge reward in the far future. If the
subjective experience is an instant astronomical multiplication of goods (with
little risk) it is rational to make the jump. However, it requires a certain degree
of internal coordination to overcome variation in individual time preferences
(hence assumption 3).
5
Thermodynamics of extreme future computa-
tion
Computation is a fundamentally physical process, tied into thermodynamic con-
siderations.
Foremost for our current purposes is Landauer’s limit: at least
E ≥kT ln(2) J
(1)
need to be dissipated for an irreversible change of one bit of information [15].
Logically irreversible manipulation of information must be accompanied by an
entropy increase somewhere.
It should be noted that the thermodynamic cost can be paid by using other
things than energy. An ordered reservoir of spin [16] or indeed any other con-
served quantity [17] can function as payment. The cost is ln(2)/λ where λ is
related to the average value of the conserved quantity. However, making such a
2Some exploration, automated or crewed, might of course still occur during the aestivation
period, to be reported to the main part of the civilization at its end.
6

negentropy reservoir presumably requires physical work unless it can be found
naturally untapped.
5.1
Reversible and quantum computation
There exists an important workaround for the Landauer limit: logically re-
versible computations do not increase entropy and can hence be done in principle
without any need of energy dissipation.
It has been shown that any logically irreversible computation can be ex-
pressed as a reversible computation by storing all intermediate results, out-
putting the result, and then retracing the steps leading up to the ﬁnal state in
reverse order, leaving the computer in the original state. The only thermody-
namic costs would be setting the input registers and writing the output [18].
More eﬀective reversible implementation methods are also known, although the
number of elementary gates needed to implement a n-bit irreversible function
scales somewhere between n2n/ log n and n2n [19].
Quantum computation is also logically reversible since quantum circuits are
based on unitary mappings. While the following analysis will be speaking of
bits, the actual computations might occur among qubits and the total reversible
computational power would be signiﬁcantly higher.
If advanced civilizations do all their computations as reversible computa-
tions, then it would seem unnecessary to gather energy resources (material re-
sources may still be needed to process and store the information). However,
irreversible operations must occur when new memory is created and in order to
do error correction.
In order to create N zeroed bits of memory at least kT ln(2)N J have to be
expended, beside the work embodied in making the bit itself.
Error correction can be done with arbitrary ﬁdelity thanks to error correcting
codes but the actual correction is an irreversible operation. The cost can be
deferred by transferring the tainted bit to an ancilla bit but in order to re-use
ancillas they have to be reset.
Error rates are suppressed by lower temperature and larger/heavier storage.
Errors in bit storage occur due to classical thermal noise (with a probability
proportional to e−Eb/kT where Eb is the barrier height) and quantum tunneling
(probability approximately e−2r√2mEb/ℏwhere m is the mass of the movable
entity storing the bit and r is the size of the bit).
The minimum potential
height compatible with any computation is
Emin
b
≈kT ln(2) + ℏ(ln(2))2/8mr2
(2)
[20].
If a system of size R has mass M divided into N bits, each bit will have size
≈R/N 1/3. If a fraction of the mass is used to build potential barriers, we can
approximate the height of the barrier Eb in each bit as the energy in all the
covalent bonds in the barrier. If we assume diamond as a building material, the
total bond energy is 1.9 kJ/mol = 1.6 · 105 J/kg.
7

Using a supercluster r = 50 Mpc, M = 1043 kg, it can be subdivided into
1061 bits without reaching the limit given the current 3K background tempera-
ture. For the future TdS temperature (see below) the limit is instead near 1075
bits (each up to 13 cm across); here the limiting factor is tunneling (the shift
occurs around T = 10−8 K). However, this would require bits far lighter than
individual atoms or even electrons3. In practice, the ultimate limiting capacity
due to matter chunkiness would be on the order of 1069 bits. However, these
“heavy” bits would have 15 orders of magnitude of safety margin relative to
the potential height and hence have minuscule tunneling probabilities. So while
error correction and repair will have to be done (as noted by Dyson, over suf-
ﬁciently long timescales (1065 years) matter is a liquid), the rate can be very
low.
Quantum computing is also aﬀected by environmental temperature and
quantum error correction can only partially compensate for this [21]. In theory
there might exist topological quantum codes that are stable against ﬁnite tem-
perature disturbances [22] but again there might be thermal or tunneling events
undermining the computing system hardware itself.
Hence, even civilizations at the boundary of physical feasibility will have
to perform some dissipative computational operations. They cannot wait in-
deﬁnitely (since slow cosmological processes will erode their infrastructure and
data) and will hence eventually run out of energy.
5.2
Cooling
While it is possible for a civilization to cool down parts of itself to any low
temperature, the act of cooling is itself dissipative since it requires doing work
against a hot environment. The most eﬃcient cooling merely consists of linking
the computation to the coldest heat-bath naturally available. In the future this
will be the cosmological background radiation4, which is also conveniently of
maximal spatial extent.
The mean temperature of the background radiation is redshifted and de-
clines as T (t) = T0/a(t) where a(t) is the cosmological scale factor [24]. Using
the long term de Sitter behavior a(t) = eHt produces T (t) = T0e−Ht. This
means that one unit of energy will be worth an exponentially growing amount
of computations if one waits long enough.
However, the background radiation is eventually redshifted below the con-
stant de Sitter horizon radiation temperature TdS =
p
Λ/12π2 = H/2π ≈
2.67 · 10−30 K [24, 6]. This occurs at time t = H ln(T0/TdS), in about 1.4 · 1012
years. There will be some other radiation ﬁelds (at this point there are still
3In theory neutrinos or very light WIMPs such as axions could fulﬁll the role but there
is no known mechanism for conﬁning them in such a way that they could store information
reliably.
4The alternative might be supermassive black hole horizons at T = ℏc3/8πGMk, which in
the present era can be far colder than the background radiation [23]. Supermassive 109M⊙
black holes will become warmer than the background radiation in 520 Gyr (assuming constant
mass).
8

active stars) but the main conclusion is that there is a ﬁnal temperature of the
universal heat bath. This also resolves a problem pointed out in [6]: in open
universes the total number of photons eventually received from the background
radiation is ﬁnite and all systems decouple thermally from it. In this case this
never happens.
One consequence is that there will always be a ﬁnite cost to irreversible com-
putations: without inﬁnite resources only a ﬁnite number of such computations
can be done in the future of any civilization.
This factor also avoids the “paradox of the indeﬁnitely postponed splurge”,
where if it is always beneﬁcial to postpone exploitation. Hence, it is rational at
some point for aestivating civilizations to start consuming resources. The limit-
ing temperature of T = 10−8 K where error correction stops being temperature-
limited and instead becomes quantization-limited might be another point where
it is rational to start exploitation (this will occur in around 270 billion years).
Had the temperature decline been slower, the limit might have been set by
galactic evaporation (1016 years) or proton decay (at least 1034 years).
5.3
Value of waiting
A comparison of current computational resources to late era computational
resources hence suggest a potential multiplier of 1030!
Even if only the resources available in a galactic supercluster are exploited,
later-era exploitation produces a payoﬀfar greater than any attempt to colonize
the rest of the accessible universe and use the resources early. In fact, the mass-
energy of just the Earth itself (5.9 · 1024 kg) would be more than enough to
power more computations than could currently be done by burning the present
observable universe! (6 · 1052 kg)5
In practice the eﬃciency will be lower but the multiplier tends to remain
astronomical.
A spherical blackbody civilization of radius r using energy at a rate P sur-
rounded by a TdS background will have an equilibrium temperature (neglecting
external and internal sources)
T = [P/(4πσr2) + T 4
dS]1/4.
(3)
For a r = 50 Mpc super-cluster sized civilization this means that maintaining a
total power of P = 1 W would keep it near 2.8 · 10−11 K. At this temperature
the civilization could do 3.8 · 1033 irreversible computations per second. The
number P/kT ln(2) bit erasures per second increases as P 3/4. At ﬁrst this might
suggest that it is better to ignore heat and use all resources quickly. However,
a civilization with ﬁnite energy E will run out of it after time E/P and the
5This might suggest that “stay at home” civilizations might hence abound, content to wait
out the future in their local environment since their bounded utility functions can be satisﬁed
eventually.
Such civilizations might be common but they are not a solid explanation for
the Fermi question since their existence does not preclude impatient (and hence detectable)
civilizations. However, see [25]. In addition, this strategy is risky since expansive civilizations
may be around.
9

total amount of erasures will then be E/kT ln(2): this declines as P −1/4. Slow
energy use produces a vastly larger computational output if one is patient.
As noted by Gershenfeld, optimal computation needs to make sure all inter-
nal states are close to the most probable state of the system, since otherwise
there will be extra dissipation [26]. Hence there is a good reason to perform op-
erations slowly. Fortunately, time is an abundant resource in the far future. In
addition, a civilization whose subjective time is proportional to the computation
rate will not internally experience the slowdown.
The Margolus-Levitin limit shows that it takes at least time πℏ/2E to move
to a new orthogonal state [27]. For E = kTdS this creates a natural “clock
speed” of 3.8 · 1011 years. However, some of the energy will be embodied in
computational hardware; even if the hardware is just single electrons the clock
speed would be 2.0·10−21 s: this particular limit is not a major problem for this
style of future computation.
A stronger constraint is the information transmission lags across the civiliza-
tion. For this example the time to send a message across the civilization is 326
million years. Whether this requires a very slow clock time or not depends on
the parallelizability of the computations done by the civilization, which in turn
depends on its values and internal structure. Civilizations with more paralleliz-
able goals such as local hedonic generation would be able to have more clock
cycles per external second than more “serial” civilizations where a global state
needs to be fully updated before the next step can be taken. However, external
time is of little importance given the paucity of external events. Even if proton
decay in 1034 years puts a ﬁnal deadline to the computation, this would still
correspond to 1025 clock cycles.
Heat emission at very low temperature is also a cause of slowdown. The time
to radiate away the entropy of a single bit erasure scales as trad = k ln(2)/4πσr2T 3.
For a 50 Mpc radius system this is 5.6 · 10−66T −3 s: for 10−8 K the time is on
the order of 10−42 s but at 10−30 K it takes 1017 years.
If the civilization does have a time limit tmax, then it is rational to use
P = E/tmax and the total number of operations will be proportional to t1/4
max.
Time-limited civilizations do have a reason to burn the candle at both ends.
Time-limited civilizations still gain overall computational success by waiting
until the universe cools down enough so their long-term working temperature
T is eﬃcient. At least for long time limits like proton decay a trillion years is a
short wait.
The reader might wonder whether starting these computations now is ratio-
nal since the universe is quickly cooling and will soon (compared to the overall
lifespan of the civilization) reach convenient temperatures. The computational
gain of doing computations at time t is ∝exp(Ht): it increases exponentially
until the temperature is dominated by the internal heating rather than the out-
side temperature. Since most of the integrated value accrues within the last
e-folding and the energy used early was used exponentially ineﬃciently, it is
not worth starting early even if the wait is a minuscule fraction of the overall
lifespan.
10

A civilization burning through the baryonic mass of a supercluster before
proton decay in 1033 years has a power of 5.7 · 1021 W (similar to a dim red
dwarf star) and a temperature of 7.6·10−6 K, achieving 1080 erasures. The most
mass-limited version instead runs close to 10−30 K, has a power of somewhere
around 10−75 W and achievs 10115 erasures – but each bit erasure, when it
happens causes a 100 quadrillion year hiatus. A more realistic system (given
quantization constraints) runs at 10−8 K and would hence have power 1010 W
(similar to a large present-day power plant), running for 5.7 · 1044 years and
achieving 1093 erasures.
6
Resources
The amount of resources available to advanced civilizations depends on what
can ultimately be used. Conservatively, condensed molecular matter such as
planets and asteroids are known to be useful for both energy, computation, and
information storage. Stars represent another form of high density matter that
could plausibly be exploited, both as energy sources and material. Degenerate
stars (white and black dwarfs, neutron stars) are also potential high density
resources. Less, conservatively, there are black holes, from which mass-energy
can be extracted [28, 29] (and, under some conditions, can act as heat sinks as
mentioned above). Beyond this, there is interstellar gas, intergalactic gas, and
dark matter halos.
For the purposes of this paper we will separate the resources into energy
resources that can power computations and matter resources that can be used
to store information, process it or (in a pinch) be converted into energy. Mass-
energy diﬀused as starlight or neutrinos, and stars lost from galaxies are assumed
to have become too dilute to be useful. Dark energy appears to be unusable
in principle. Dark matter may be useful as an energy resource by annihilation
even if it cannot sustain information processing structures.
Not all resources in the universe can be reached and exploited. The light-
speed limit forces civilizations to remain within a light-cone and the accelerating
expansion further limits how far probes can be sent. Based on the assumptions
in [1] the amount of resources that can be reached within a 100 Mpc supercluster
or by traveling at 50%, 80% or 99% c are listed in table 1.
6.1
Changes over time
6.1.1
Stellar fusion
Stellar lifetime energy emissions are proportional to mass (with high luminosity
stars releasing more faster), Elife = LT = L⊙(M/M⊙)3.16 · 1017 J, leading to a
lifetime mass loss through energy emission of
Mloss = 1.35 · 1027(M/M⊙) kg = 6.79 · 10−4M⊙(M/M⊙).
(4)
Lighter stars loose less mass but since the mean mass star is 0.7M⊙this is on
the order of a typical value.
11

100 Mpc
1.24 Gpc
2.33 Gpc
4.09 Gpc
Planetary
bodies
and
condensed matter.
3.92 · 1042
7.47 · 1045
4.96 · 1046
2.68 · 1047
Stars
(including
white
dwarfs,
neutron
stars
and substellar objects)
2.74 · 1045
5.23 · 1048
3.47 · 1049
1.88 · 1050
Black holes
8.25 · 1043
1.57 · 1047
1.04 · 1048
5.65 · 1048
Interstellar gas
8.70 · 1044
1.66 · 1048
1.10 · 1049
5.95 · 1049
Intergalactic gas
4.66 · 1046
8.89 · 1049
5.90 · 1050
3.19 · 1051
Dark matter
2.81 · 1047
5.36 · 1050
3.55 · 1051
1.92 · 1052
Total
3.31 · 1047
6.31 · 1050
4.19 · 1051
2.27 · 1052
Table 1: Resources available to civilizations expanding at diﬀerent speeds, or
within a single supercluster. Estimates based on [30] and [31]. Distances mea-
sured in co-moving coordinates, mass in kilograms.
Hence stellar fusion is not a major energy waste if mass can be converted
into energy. This is important for the aestivation hypothesis: had stellar fusion
been a major source of mass loss it would had been rational to stop it during the
aestivation period, or at least to gather up the lost energy using Dyson shells:
both very visible activities that can be observationally ruled out (at least as
large-scale activities).
Fusion processes also produce nuclei that might be more useful for compu-
tation without any need for intervention. Long term elemental mass fractions
approach 20% hydrogen, 60% helium and 20% other elements over 1012 year
timescales [5].
6.1.2
Black hole formation
Stellar black holes permanently reduces the amount of baryonic matter available
even if their mass-energy is exploitable. At present a mass fraction ≈2.5% of
the star formation budget is lost this way [31].
To prevent this star formation of masses above 25M⊙needs to be blocked.
This could occur by interventions that cause extra fragmentation of clouds con-
densing into heavy stars. Adding extra dust to the protostellar cloud could
induce rapid cooling and hence fragmentation. This would require dust densi-
ties on the order of 10−5ρgas and for a stellar formation rate ≈1M⊙per year
would require seeding galactic clouds with ≈10−5M⊙per year. Average nucle-
osynthetic yields are ≈0.0025, so there would be enough metals produced per
year to seed the clouds by about two orders of magnitude.
Other ways of inducing premature fragmentation might be to produce radi-
ation bursts (from antimatter charges or directed illumination from Dyson-shell
surrounded stars) or pressure waves, while magnetic ﬁelds might slow core col-
lapse. The energies involved would be of the order of the cloud potential energy;
for a Bok globule this might require (3G/5)(50M 2
⊙/1 ly) = 4.2·1036 J, about 103
sun-years of luminosity. Given that cloud collapse is a turbulent process it might
12

be possible to prevent massive star formation through relatively energy-eﬃcient
chaos control.
While these methods might be invisible over long distances their eﬀects ought
to be noticeable due to a suppression of starbursts and heavy blue-white stars.
6.1.3
Galactic winds
While stars can lose signiﬁcant amount of mass by outﬂows, this gas is recycled
through the interstellar medium into new stars. However, some of this medium
may be lost due to galactic winds and may hence become long-term inaccessible.
Galactic winds are likely proportional to the stellar formation rate (M ′/SFR
around 0.01-10) plus contributions from active galactic nuclei, and may have
signiﬁcantly depleted early galaxies. Starbursts can lose 105 −106M⊙in dwarf
galaxies and 108 −1010 in ULIRGs, partially by entraining neutral matter. [32]
However, for large galaxies the actual escape fractions may be low due to dark
matter halos keeping the gas bound and dampening the outﬂow through drag,
keeping it below 4% [33]. There can also be ongoing infall due to the halo that
more than compensates for the loss.
Due to the uncertainty about the wind budget it is not clear whether a civ-
ilization prioritizing baryonic matter might want to prevent galactic winds. A
few approaches may be possible. One way, as described above, is to prevent too
vigorous star formation. Such a civilization would also be interested in keep-
ing the galactic nucleus quiescent, perhaps using the “stellar billiards” method
suggested in the next section to manipulate orbits near the central black hole.
These methods would likely be hard to detect, except for the reduction in hot
stars6.
6.1.4
Galaxy collisions
Galactic “splashes” and mergers may lose signiﬁcant amount of gas, stars and
halos to intergalactic space.
Can collisions be prevented? Typical galactic peculiar velocities are on the
order of 200-300 km/s: in order to avoid a collision the velocity of one or both
galaxies need to be changed by this amount by expelling some mass-energy. The
rocket equation hence implies a ﬁnal mass m1 = m0e−∆v/ve. If ve ≈1000 km/s,
such as in the case of deliberate ejection of hypervelocity stars7, the mass loss
6Another, somewhat extreme, approach would be to bag the galaxy: at a surface density
of 0.77· 10−6 kg/m2 a graphene balloon encompassing a Milky Way-sized galaxy (r = 20 kpc)
would weigh 1.8 · 106M⊙. Given a current carbon mass fraction of 0.0046 this would require
mining 0.4 · 109M⊙stars or gas clouds, or fusing several million solar masses. Since the saved
gas mass would be just one order of magnitude larger than the balloon weight, it is likely that
this approach is not eﬀective.
7This can be achieved in a step-wise process. First, Shkladov thrusters [34] are built around
many stars. While these thrusters are unable to change stellar velocities signiﬁcantly over a
Gyr, they are enough to engineer close encounters with passing stars within a few million
years. This allows a bootstrap process where gravity assists can bring velocity changes of
stars up to the average stellar velocities of 100 km/s as well as allow course corrections. Stars
are then sent on slingshot orbits past the central black hole, ejecting them and accelerating
13

will be 10%. For an optimal exhaust velocity of c (using some form of mass-
energy conversion) the loss is around 3.5%. While this is a small fraction of the
entire galactic mass, most of it (> 95%) is in the form of a dark matter halo: if
only baryonic matter can be used to power the evasive maneuver practically all
of it would be lost in the process.
Hence, if a civilization seeks to retain signiﬁcant amounts of baryonic matter
it is not rational in general to prevent galactic collisions. Civilizations that in-
stead care about overall mass-energy may be more motivated to prevent splashes
of halo material by sacriﬁcing some mass-energy in a very visible maneuver. If
the maneuver takes one Gyr and uses mass-energy conversion, the average lu-
minosity will be 3.5% · 1012M⊙c2/(109 yr) = 2 · 1041 W = 5 · 1014L⊙. Streams
of 1010 hypervelocity stars would also likely be very noticeable.
6.1.5
Expansion
Given the current ΛCDM model of the universe, the expansion rate is increasing
and approaching a de Sitter expansion. This leads to the eventual separation of
all gravitationally bound systems from each other by insurmountable distances,
leaving each an “island universe” within their cosmological horizon. The local
group of galaxies will likely be separated from the Virgo supercluster and in 100
Gyr entirely separate [35]. Busha et al. ﬁnd a criterion for structures remaining
bound,
Mobj/1012M⊙> 3h2
70(r0/ 1Mpc)3
(5)
where h70 = H0/70 km/s/Mpc. The paper also gives an estimate of the isolation
time, about 120 Gyr for typical clusters [36].
This expansion dynamics is a key constraint on the ambitions of far-future
civilizations. Most matter within not just the observable but the colonizable (at
least given assumptions as in [1]) universe will be lost. While no doubt advanced
civilizations might wish to aﬀect the expansion rate it seems unlikely that such
universal parameters are changeable8. Depending on the utility function of the
civilization, this either implies that there is little extra utility after colonizing the
largest achievable bound cluster (utilities dependent on causal connectedness),
or that future parts of the civilization will be disconnected from each other
(utilities not valuing total connectedness).
Civilizations desiring guaranteed
separation – for example, to prevent competitors from invading – would also
value the exponentially growing moats.
Is it possible to gather more mass into gravitationally bound systems? In or-
der to move mass, whether a rocket or a galaxy, energy or reaction mass need to
the galaxy. This scheme mainly converts stellar kinetic and potential energy into thrust.
8A civilization will by necessity be spatially local, so any change in Λ or dark energy
parameters would have to be local; beside the inherent problem of how it could be aﬀected, any
change would also likely merely propagate at light-speed and hence will not reach indeﬁnitely
far. Worse, even a minor change or gradient in the 68.3% of the total mass-energy represented
by dark energy would correspond to massive amounts of normal energy: the destructive eﬀects
may be as dramatic as vacuum decay scenarios.
14

be ejected at high velocity to induce motion9. This is governed by the relativistic
rocket equation ∆v = c tanh((Isp/c) ln(m0/m1)) (we ignore the need for slowing
down at arrival). The best possible speciﬁc impulse is Isp = c. The remaining
mass arriving at the destination will then be m1 = m0 exp(−tanh−1(∆v/c)).
In order to overcome the Hubble ﬂow ∆v > H0r (we here ignore that the
acceleration of the expansion will require higher velocities). Putting it together,
we will get the bound
m(r) < 4πρr2 exp(−tanh−1(H0r/c))
(6)
for a concentric shell of radius r.
Setting k = H0/c, this can be integrated from r0 (the border of the super-
cluster) to 1/k (the point where it is just barely possible to send back matter
at lightspeed):
Mcollect = (2πρ/3k3)
hp
1 −k2x2(2k2x2 −3kx + 4) + 3 sin−1(kx)
i1/k
r0
(7)
If we use r0 = 50 Mpc (typical supercluster size) we get Mcollect = 3.8 ·
1078ρ kg, where ρ is the collectable mass density. For ρ = 2.3 · 10−27 kg/m3
this is 8.9 · 1051 kg. The ratio to mass inside the supercluster (here densities
are assumed to be 20 times larger inside) is Mcollect/Mcluster = 12, 427. The
collected mass is 35% of the entire mass inside the reachable volume; the rest is
used up.
Another approach would be to convert mass into radiant energy locally,
beaming half of it (due to momentum conservation) inwards to a receiver such
as a black hole from which it could be extracted later. The main losses would
be due to redshift during transmission.
However, this ignores the problem that in order to reach remote locations
to send back matter home the civilization needs to travel there. If colonization
occurs at lightspeed the colonies will have to deal with an expansion factor
eH0r/c larger, producing the far tighter bound
m(r) < 4πρr2 exp(−tanh−1(H0eH0r/cr/c)).
(8)
Integrating numerically from r0 to the outer limit W(1)/k ≈2.52 Gpc (where
W is Lambert’s W function) produces a more modest Mcollect = 8.32·1077ρ and
Mcollect/Mcluster = 2, 705. Of the total reachable mass, only 7.7% remains.
This calculation assumes all mass can be used; if diﬀuse gas and dark matter
cannot be used to power the move, not only does the total mass yield go down
by two orders of magnitude but there are going to be signiﬁcant energy losses
in climbing out of cluster potential wells. Nevertheless, it still looks possible
to increase the long-term available mass of superclusters by a few orders of
magnitude.
9In principle gravitational wave propulsion or spacetime swimming [37] are possible alter-
natives but appear unlikely to be useful in this case.
15

If it is being done it would be very visible, since at least the acceleration
phases would convert a fraction of entire galaxies mass-energy into radiation.
A radial process would also send this radiation in all outward directions, and
backscatter would likely be very noticeable even from the interior.
6.1.6
Galactic evaporation
Over long periods stars in galaxies scatter from each other when they have
encounters, causing a large fraction to be ejected and the rest are swallowed by
the central supermassive black hole. This occurs on a timescale of 1019 years
[5].
However, due to the thermodynamic considerations above, it becomes ratio-
nal to exploit the universe long before galactic evaporation becomes a problem.
The same applies to the other forms of long-term deterioration of the uni-
verse, such as proton decay, black hole decay, quantum liquefaction of matter
etc. [5]
7
Interactions with other civilizations
The aestivation hypothesis at ﬁrst appears to suﬀer the same cultural con-
vergence assumption as many other Fermi question answers: they assume all
suﬃciently advanced civilizations – and members of these civilizations – will be-
have in the same way. While some convergence on instrumental goals is likely,
convergence strong enough to ensure an answer to the Fermi question appears
implausible since it only takes one unusual civilization (or group within it) any-
where to break the explanation. Even if it is rational for every intelligent being
to do something, this does not guarantee that all intelligent beings are rational.
However, cultural convergence can be enforced. Civilizations could coordi-
nate as a whole to prevent certain behaviors of their own constituents, or of
younger civilizations within their sphere of inﬂuence. While current human-
ity shows the tremendous problems inherent in achieving global coordination,
coordination may both be important for surviving the emergence of powerful
technologies (acting as a ﬁlter, leaving mainly coordinated civilizations on the
technologically mature side). Even if coordination leading to enforcement of
some Fermi question explaining behavior is not guaranteed, we could happen
to live inside a domain of a large and old civilization that happens to enforce
it (even if there are defectors elsewhere). If such civilizations are very large (as
suggested by our intergalactic colonization argument [1]) this would look to us
like global convergence.
The aestivation hypothesis strengthens this argument by implying a need
for protecting resources during the aestivation period. If a civilization merely
aestivates it may ﬁnd its grip on its domain supplanted by latecomers. Leaving
autonomous systems to monitor the domain and preventing activities that de-
crease its value would be the rational choice. They would ensure that parts of
the originating civilization do not start early but also that invaders or young civ-
16

ilizations are stopped from value-decreasing activities. One plausible example
would be banning the launch of self-replicating probes to do large-scale coloniza-
tion. In this scenario cultural convergence is enforced along some dimensions.
It might be objected that devices left behind cannot survive the eons required
for restarting the main civilization. While the depths of space are a stable en-
vironment that might be benign for devices constructed to be functional there,
there are always some micrometeors, cosmic rays or other mishaps. However,
having redundant copies greatly reduce the chance of all being destroyed simul-
taneously. It is possible to show that by slowly adding backup capacity (at a
logarithmic rate) a system can ensure a ﬁnite probability of enduring for inﬁ-
nite time10 [38]. The infrastructure left behind could hence be both extremely
long-lasting and require a minuscule footprint, even if it is imperfect.
One can make the argument that defenders are likely to win since the amount
of materiel they have at home can easily dwarf the amount of materiel that
can be moved into place, since long-range transport is expensive.
However,
an interior point in an aestivator domain can be targeted with resources rising
quadratically with time as the message goes out. Which eﬀects wins out depends
on the relative scaling of the costs and resources11.
One interesting observation is that if we are inside an aestivating civilization,
then other aestivators are also likely: the probability of intelligence arising per
spacetime hypervolume is high enough that the large civilization will likely en-
counter external mature civilizations and hence needs to have a strategy against
them.
Two mature large-scale civilizations encountering each other will be essen-
tially spherical, expanding at the same rate (set by convergence towards the
limits set by physics). At the point of contact the amount of resources avail-
able will be the intersection of an expanding communications sphere and the
overall colonization sphere: for large civilizations this means that they will have
nearly identical resources. Given the maturity assumption they would hence be
evenly matched12. They have several choices: battle each other for resources,
maintain a tangential hyperboloid boundary, or, if their utilities are compati-
ble, join. Given the intention of using resources later, expending them in the
present is only rational if it prevents larger losses in the long run. If both civi-
lizations are only interested in resources per se, it may be possible to make any
invasion irrational by scorched earth tactics: the enemy will not gain anything
from invading and merely expend its resources. This is no guarantee that all
10In practice physics places a number of limitations on the durability such as proton decay
or quantum tunneling but these limitations are largely outside of the timescales considered in
this paper.
11For example, using the Lanchester square law model of warfare [39] for a spherical de-
fender domain (of value ∝r3) and quadratically arriving attackers it will resist for time
t ∝α1/2r2/3η−1/2, where α is the defender ﬁrepower and η is the resource eﬃciency of
transporting attack materiel. The cost to the attacker will scale as t3 ∝α3/2r2η−3/2. For
suﬃciently large α or low η it may hence be rational to overlook small emergent civilizations
– or ensure that they do not appear in the ﬁrst place.
12The smaller civilization would have a slight disadvantage due to the greater curvature of
its surface but if interaction is settled early this might not come into play.
17

possible civilizations will be peaceful vis-´a-vis each other, since there might be
other value considerations (e.g.
a negative utilitarian civilization encounter-
ing one planning to maintain a large amount of suﬀering: the ﬁrst civilization
would have a positive utility in reducing the resources of the second as much
as possible, even at the expense of future computation). However, given the
accelerating expansion maintaining borders could in principle be left to physics.
8
Discussion
This paper has shown that very big civilizations can have small footprint by
relocating most of their activity to the future.
Of the 6 assumptions of the aestivation hypothesis, 1 (early civilizations)
and 2 (broad expansion) are already likely (and if not true, provide alternative
answers to the Fermi question). The ﬁfth assumption, that many civilizations
wish to aestivate, is supported by the vast increases in computational ability.
The third assumption, that coordination problems can be resolved, remains
hard to judge. It should be noted that planning for aestivation is something
that is only rational for a civilization once it has solved urgent survival issues.
A species that has not reduced its self-generated existential risk enough has
good reason to discount the far future due to uncertainty.
Conversely, the
enormous potential value of a post-aestivation future makes the astronomical
waste argument [40] stronger: given the higher stakes, the importance of early
risk mitigation – and hence coordination – increases.
The fourth assumption is at present also hard to judge.
It seems likely
that the technology allowing long-range expansion (automation, interplanetary
manufacturing, self-replication and long-lived autonomous devices) would en-
able maintaining arbitrarily large local stockpiles of equipment to fend oﬀin-
cursions.
Assumption six, the invisibility of aestivation may at ﬁrst appear hard to
test – nearly any number of advanced civilizations with nearly no energy usage
could easily hide somewhere in the galactic halo [13]. However, in order for it
to make sense to aestivate the amount of resources lost during the wait must be
small enough that they are dwarfed by the resource costs of eﬀorts to prevent
them.
Are there reasons to perform visible megascale engineering to preserve re-
sources?
This depends primarily on whether baryonic matter “construction
material” or mass-energy is regarded as the limiting factor for future compu-
tation. If baryonic matter is the key factor considerations of stellar activity,
galactic wind or galactic collision mass loss do not seem to imply much utility
in megascale engineering to preserve matter. However, if dark matter halos rep-
resent signiﬁcant value (the mass-energy case) reduction of collision loss would
be rational, likely doable and very visible. The lack of such galactic engineer-
ing hence puts a limit on the existence of such energy-concerned aestivating
civilizations in the surveyed universe.
Engineering large-scale galactic movement to prevent separation of super-
18

clusters would also be highly visible and the lack of such activity implies that
there are no civilizations seeking to optimize long-term causally connected mass-
energy concentrations.
Together, these considerations suggest that if there are aestivating civiliza-
tions in our vicinity they are either local (no interest outside their own galaxies
or supercluster, utility functions that place no or little value extra matter or
energy) or they have utility functions that may drive universal expansion but
hold little interest in causal connectedness.
From a Dysonian SETI perspective, the aestivation hypothesis makes a very
clear prediction: look for inhibition of processes that permanently lose matter
to inter-cluster or inter-galactic space or look for the gravitationally bound
structures more massive than what the standard ΛCDM cosmology predicts for
a given lengthscale.
8.1
Cosmology/physics assumptions
What if we are wrong about the model of the universe used in this paper? A
few “nearby” models have clear implications. In Big Rip scenarios where the
scale factor becomes inﬁnite at some point due to phantom energy it is rational
to use up available energy before this point. For plausible values of w this is
likely far in the future and hence aestivation still makes sense. If there is no
horizon radiation, then it is rational to delay until proton or black hole decay,
or when the heating due to the civilization becomes on par with the universe.
Again aestivation makes sense.
More fundamentally there is the uncertainty inherent in analysing extreme
future scenarios or future technology: even when we base the arguments on
well-understood and well-tested physics, there might exist unexpected ways of
circumventing this. Unfortunately there is little that can be done about this.
The aestivation hypothesis assumes that the main driver of advanced civi-
lizations is computations whose cost are temperature dependent. More philo-
sophically, what if there are other forms of value that can be generated? Turning
energy straight into value without computation would break the temperature
dependency, and hence the scenario.
This suggests an interesting line of investigation: what is the physics of
value? Until recently the idea that information was physical (or indeed, a mea-
surable thing) was exotic but currently we are seeing a renaissance of investi-
gations into the connections between computation and physics. The idea that
there are bounds set by physics on how much information can be stored and
processed by one kilogram of matter is no longer strange. Could there exist
similar bounds on how much value one kilogram of matter could embody?
8.2
Anthropics
Does the aestivation hypothesis have any anthropic implications? The main
consequence of the physical eschatology considerations in this paper is that
future computation could vastly outweigh current computation and we should
19

hence expect most observers to exist in the far future rather than the early
stelliferous era.
The Self-Indication Assumption (SIA) states that we should reason as if
we were randomly selected from the set of all possible observers [7]. This is
normally assumed to support that we are in a world with many observers. We
should expect aliens should exist (since a universe with humans and aliens have
more observers, especially if the aliens become a very large post-aestivation
civilization). The aestivation hypothesis suggests that initially sparse worlds
may have far more observers than worlds that have much activity going on
early (and then run out of resources), so the SIA suggests we should believe in
the hypothesis.
The competing Self-Sampling Assumption states that we should reason as if
we were randomly selected from the actually existent observers (past, present,
future). This gives a more pessimistic outcome, where the doomsday argument
suggests that we may not be going to survive. However, both the SIA and SSA
may support the view that we are more likely to be a history simulation [41]
running in the post-aestivation era (if that era is possible) than the sole early
ancestor population.
8.3
Final words
The aestivation hypothesis came about as a result of physical eschatology con-
siderations of what the best possible outcome for a civilization would be, not
directly an urge to solve the Fermi question. However, it does seem to provide
one new possible answer to the question:
That is not dead which can eternal lie.
And with strange aeons even death may die.
H.P. Lovecraft
Acknowledgments
We wish to acknowledge Nick Beckstead,Daniel Dewey, Eric Drexler, Carl Frey,
Vincent M¨uller, Toby Ord, Andrew Snyder-Beattie, Cecilia Tilli, Owen Cotton-
Barratt, Robin Hanson and Carl Shulman for helpful and stimulating discussion.
References
[1] S. Armstrong and A. Sandberg, “Eternity in six hours: Intergalactic spread-
ing of intelligent life and sharpening the fermi paradox,” Acta Astronautica,
vol. 89, pp. 1–13, 2013.
[2] F. J. Dyson, “Time without end: Physics and biology in an open universe,”
Reviews of Modern Physics, vol. 51, no. 3, p. 447, 1979.
20

[3] R. J. Bradbury, M. M. ´Cirkovi´c, and G. Dvorsky, “Dysonian approach
to seti: a fruitful middle ground?,” Journal of the British Interplanetary
Society, vol. 64, no. 5, p. 156, 2011.
[4] M. M. ´Cirkovi´c, “Resource letter: Pes-1: physical eschatology,” American
Journal of Physics, vol. 71, no. 2, pp. 122–133, 2003.
[5] F. C. Adams and G. Laughlin, “A dying universe: the long-term fate and
evolutionof astrophysical objects,” Reviews of Modern Physics, vol. 69,
no. 2, p. 337, 1997.
[6] L. M. Krauss and G. D. Starkman, “Life, the universe, and nothing:
Life and death in an ever-expanding universe,” The Astrophysical Jour-
nal, vol. 531, no. 1, p. 22, 2000.
[7] N. Bostrom et al., Anthropic bias: Observation selection eﬀects in science
and philosophy. Routledge, 2002.
[8] C. H. Lineweaver, “An estimate of the age distribution of terrestrial plan-
ets in the universe: quantifying metallicity as a selection eﬀect,” Icarus,
vol. 151, no. 2, pp. 307–313, 2001.
[9] M. M. ´Cirkovi´c and B. Vukoti´c, “Astrobiological phase transition: towards
resolution of fermis paradox,” Origins of Life and Evolution of Biospheres,
vol. 38, no. 6, pp. 535–547, 2008.
[10] N. Bostrom, “What is a singleton?,” Linguistic and Philosophical Investi-
gations, vol. 5, no. 2, pp. 48–54, 2006.
[11] A. Sandberg and S. Armstrong, “Hunters in the dark: Game theory anal-
ysis of the deadly probes scenario,” in Poster presented at the National
Astronomy Meeting of the Royal Astonomical Society (NAM2013), 2013.
[12] N. Bostrom, “The superintelligent will: Motivation and instrumental ratio-
nality in advanced artiﬁcial agents,” Minds and Machines, vol. 22, no. 2,
pp. 71–85, 2012.
[13] M. M. ´Cirkovi´c and R. J. Bradbury, “Galactic gradients, postbiological
evolution and the apparent failure of seti,” New Astronomy, vol. 11, no. 8,
pp. 628–639, 2006.
[14] J. Broome, “Discounting the future,” Philosophy & Public Aﬀairs, vol. 23,
no. 2, pp. 128–156, 1994.
[15] R. Landauer, “Irreversibility and heat generation in the computing pro-
cess,” IBM journal of research and development, vol. 5, no. 3, pp. 183–191,
1961.
21

[16] J. A. Vaccaro and S. M. Barnett, “Information erasure without an en-
ergy cost,” in Proceedings of the Royal Society of London A: Mathematical,
Physical and Engineering Sciences, p. rspa20100577, The Royal Society,
2011.
[17] S. M. Barnett and J. A. Vaccaro, “Beyond Landauer erasure,” Entropy,
vol. 15, no. 11, pp. 4956–4968, 2013.
[18] C. H. Bennett, “Logical reversibility of computation,” IBM journal of Re-
search and Development, vol. 17, no. 6, pp. 525–532, 1973.
[19] M. Saeedi and I. L. Markov, “Synthesis and optimization of reversible cir-
cuitsa survey,” ACM Computing Surveys (CSUR), vol. 45, no. 2, p. 21,
2013.
[20] V. V. Zhirnov, R. K. Cavin, J. A. Hutchby, and G. I. Bourianoﬀ, “Limits
to binary logic switch scaling-a gedanken model,” Proceedings of the IEEE,
vol. 91, no. 11, pp. 1934–1939, 2003.
[21] C. Cafaro and P. van Loock, “Approximate quantum error correction for
generalized amplitude-damping errors,” Physical Review A, vol. 89, no. 2,
p. 022316, 2014.
[22] A. Hamma, C. Castelnovo, and C. Chamon, “Toric-boson model: Toward
a topological quantum memory at ﬁnite temperature,” Physical Review B,
vol. 79, no. 24, p. 245122, 2009.
[23] T. Opatrn`y, L. Richterek, and P. Bakala, “Life under a black sun,” Amer-
ican Journal of Physics, vol. 85, no. 1, pp. 14–22, 2017.
[24] J. P. Zibin, A. Moss, and D. Scott, “Evolution of the cosmic microwave
background,” Physical Review D, vol. 76, no. 12, p. 123010, 2007.
[25] B. Parkinson, “Thoughts on the evolution of consciousness,” Journal of the
British Interplanetary Society, vol. 57, pp. 60–66, 2004.
[26] N. Gershenfeld, “Signal entropy and the thermodynamics of computation,”
IBM Systems Journal, vol. 35, no. 3.4, pp. 577–586, 1996.
[27] N. Margolus and L. B. Levitin, “The maximum speed of dynamical evo-
lution,” Physica D: Nonlinear Phenomena, vol. 120, no. 1, pp. 188–195,
1998.
[28] W. G. Unruh and R. M. Wald, “How to mine energy from a black hole,”
General Relativity and Gravitation, vol. 15, no. 3, pp. 195–199, 1983.
[29] A. Lawrence and E. Martinec, “Black hole evaporation along macroscopic
strings,” Physical Review D, vol. 50, no. 4, p. 2680, 1994.
22

[30] P. A. Ade, N. Aghanim, C. Armitage-Caplan, M. Arnaud, M. Ashdown,
F. Atrio-Barandela, J. Aumont, C. Baccigalupi, A. J. Banday, R. Barreiro,
et al., “Planck 2013 results. xvi. cosmological parameters,” Astronomy &
Astrophysics, vol. 571, p. A16, 2014.
[31] M. Fukugita and P. Peebles, “The cosmic energy inventory,” The Astro-
physical Journal, vol. 616, no. 2, p. 643, 2004.
[32] S. Veilleux, G. Cecil, and J. Bland-Hawthorn, “Galactic winds,” Annu.
Rev. Astron. Astrophys., vol. 43, pp. 769–826, 2005.
[33] H. Zhao, “Dynamical limits on galactic winds, halo machos and intergalac-
tic globular clusters,” Monthly Notices of the Royal Astronomical Society,
vol. 336, no. 1, pp. 159–167, 2002.
[34] V. Badescu and R. B. Cathcart, “Use of class a and class c stellar engines
to control sun movement in the galaxy,” Acta Astronautica, vol. 58, no. 3,
pp. 119–129, 2006.
[35] K. Nagamine and A. Loeb, “Future evolution of nearby large-scale struc-
tures in a universe dominated by a cosmological constant,” New Astronomy,
vol. 8, no. 5, pp. 439–448, 2003.
[36] M. T. Busha, F. C. Adams, R. H. Wechsler, and A. E. Evrard, “Future evo-
lution of cosmic structure in an accelerating universe,” The Astrophysical
Journal, vol. 596, no. 2, p. 713, 2003.
[37] J. Wisdom, “Swimming in spacetime: motion by cyclic changes in body
shape,” Science, vol. 299, no. 5614, pp. 1865–1869, 2003.
[38] A. Sandberg and S. Armstrong, “Indeﬁnite survival through backup
copies,” Future of Humanity Institute Technical Report, Future of Human-
ity Institute, University of Oxford, http://www. fhi. ox. ac. uk/indeﬁnite-
survivalbackup. pdf (accessed 5 March 2013), 2012.
[39] F. W. Lanchester, “Mathematics in warfare,” The world of mathematics,
vol. 4, pp. 2138–2157, 1956.
[40] N. Bostrom, “Astronomical waste: The opportunity cost of delayed tech-
nological development,” Utilitas, vol. 15, no. 03, pp. 308–314, 2003.
[41] N. Bostrom, “Are we living in a computer simulation?,” The Philosophical
Quarterly, vol. 53, no. 211, pp. 243–255, 2003.
23

