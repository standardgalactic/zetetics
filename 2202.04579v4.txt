Neural Sheaf Diffusion: A Topological Perspective on
Heterophily and Oversmoothing in GNNs
Cristian Bodnar∗
University of Cambridge
cristian.bodnar@cl.cam.ac.uk
Francesco Di Giovanni†
Twitter
fdigiovanni@twitter.com
Benjamin P. Chamberlain
Twitter
Pietro Liò
University of Cambridge
Michael Bronstein
University of Oxford & Twitter
Abstract
Cellular sheaves equip graphs with a “geometrical” structure by assigning vector
spaces and linear maps to nodes and edges. Graph Neural Networks (GNNs)
implicitly assume a graph with a trivial underlying sheaf. This choice is reﬂected
in the structure of the graph Laplacian operator, the properties of the associated
diffusion equation, and the characteristics of the convolutional models that dis-
cretise this equation. In this paper, we use cellular sheaf theory to show that
the underlying geometry of the graph is deeply linked with the performance of
GNNs in heterophilic settings and their oversmoothing behaviour. By considering
a hierarchy of increasingly general sheaves, we study how the ability of the sheaf
diffusion process to achieve linear separation of the classes in the inﬁnite time limit
expands. At the same time, we prove that when the sheaf is non-trivial, discretised
parametric diffusion processes have greater control than GNNs over their asymp-
totic behaviour. On the practical side, we study how sheaves can be learned from
data. The resulting sheaf diffusion models have many desirable properties that
address the limitations of classical graph diffusion equations (and corresponding
GNN models) and obtain competitive results in heterophilic settings. Overall, our
work provides new connections between GNNs and algebraic topology and would
be of interest to both ﬁelds.
Figure 1: A sheaf (G, F) shown for a single edge
of the graph. The stalks are isomorphic to R2. The
restriction maps Fv⊴e, Fu⊴e and their adjoints
move the vector features between these spaces. In
practice, we learn the sheaf (i.e. the restrictions
maps) from data via a parametric function Φ.
Figure 2: Analogy between parallel trans-
port on a sphere and transport on a discrete
vector bundle (cellular sheaf). A tangent vec-
tor is moved from F(w) →F(v) →F(u)
and back. Because the vector returns in a
different position, the transport is not path-
independent.
∗Work done as a research intern at Twitter.
†Proved the results in Section 3.1.
Our code is available at https://github.com/twitter-research/neural-sheaf-diffusion.
36th Conference on Neural Information Processing Systems (NeurIPS 2022).
arXiv:2202.04579v4  [cs.LG]  6 Jan 2023

1
Introduction
Graph Neural Networks (GNNs) [12, 20, 27–29, 39, 58, 64] have recently become very popular in the
ML community as a model of choice to deal with relational and interaction data due to their multiple
successful applications in domains ranging from social science and particle physics to structural
biology and drug design. In this work, we focus on two main problems often observed in GNNs:
their poor performance in heterophilic graphs [75] and their oversmoothing behaviour [48, 50]. The
former arises from the fact that many GNNs are built on the strong assumption of homophily, i.e.,
that nodes tend to connect to other similar nodes. The latter refers to a phenomenon of some deeper
GNNs producing features that are too smooth to be useful.
Contributions. We show that these two fundamental problems are linked by a common cause: the
underlying “geometry” of the graph (used here in a very loose sense). When this geometry is trivial,
as is typically the case, the two phenomena described above emerge. We make these statements
precise through the lens of (cellular) sheaf theory [10, 18, 26, 44, 56, 62], a subﬁeld of algebraic
topology and geometry. Intuitively, a cellular sheaf associates a vector space to each node and edge
of a graph, and a linear map between these spaces for each incident node-edge pair (Figure 1).
In Section 3, we analyse how by considering a hierarchy of increasingly general sheaves, starting
from a trivial one, a diffusion equation based on the sheaf Laplacian [34] can solve increasingly
more complicated node-classiﬁcation tasks in the inﬁnite time limit. In this regime, we show that
oversmoothing and problems due to heterophily can be avoided by equipping the graph with the right
sheaf structure for the task. In Section 4, we study the behaviour of a non-linear, parametric, and
discrete version of this process. This results in a Sheaf Convolutional Network [32] that generalises
Graph Convolutional Networks [39]. We prove that this discrete diffusion process is more ﬂexible
and has greater control over its asymptotic behaviour than GCNs [13, 51]. All these results are based
on the properties of the harmonic space of the sheaf Laplacian, which we study from a spectral
perspective in Section 3.1. We provide a new Cheeger-type inequality for the spectral gap of the sheaf
Laplacian and note that these results might be of independent interest for spectral sheaf theory [34].
Finally, in Section 5, we apply our theory to designing simple and practical GNN models. We describe
how to construct Sheaf Neural Networks by learning sheaves from data, thus making these types of
models applicable beyond the toy experimental setting where they were originally introduced [32].
The resulting models obtain competitive results both in heterophilic and homophilic graphs.
2
Background
Cellular Sheaves.
A cellular sheaf [18, 62] over a graph (Figure 1) is a mathematical object
associating a vector space to each node and edge in the graph and a map between these spaces for
each incident node-edge pair. We deﬁne this formally below:
Deﬁnition 1. A cellular sheaf (G, F) on an undirected graph G = (V, E) consists of:
• A vector space F(v) for each v ∈V .
• A vector space F(e) for each e ∈E.
• A linear map Fv⊴e : F(v) →F(e) for each incident v ⊴e node-edge pair.
The vector spaces of the nodes and edges are called stalks, while the linear maps are referred to as
restriction maps. The space formed by all the spaces associated with the nodes of the graph is called
the space of 0-cochains C0(G; F) := L
v∈V F(v), where L denotes the direct sum of vector spaces.
For a 0-cochain x ∈C0(G; F), we use xv to refer to the vector in F(v) of node v. Hansen and Ghrist
[35] have constructed a convenient mental model for these objects based on opinion dynamics. In this
context, xv is the ‘private opinion’ of node v, while Fv⊴exv expresses how that opinion manifests
publicly in a ‘discourse space’ formed by F(e). A particularly important subspace of C0(G; F) is
the space of global sections H0(G; F) := {x ∈C0(G; F) : Fv⊴exv = Fu⊴exu} containing those
private opinions x for which all neighbours (v, u) agree with each other in the discourse space. Given
a cellular sheaf (G, F), we can deﬁne a sheaf Laplacian operator [34] measuring the aggregated
‘disagreement of opinions’ at each node:
Deﬁnition 2. The sheaf Laplacian of a sheaf (G, F) is a linear map LF : C0(G, F) →C0(G, F)
deﬁned node-wise as LF(x)v := P
v,u⊴e F⊤
v⊴e(Fv⊴exv −Fu⊴exu).
2

{
Figure 3: A graph (left), the Laplacian matrix of a sheaf with d-dimensional stalks over the graph
(middle) , and a 0-cochain x represented as a block-vector stacking the vectors of all nodes (right).
The sheaf Laplacian is a positive semi-deﬁnite block matrix (Figure 3). The diagonal blocks are
LF vv = P
v⊴e F⊤
v⊴eFv⊴e, while the non-diagonal blocks LF vu = −F⊤
v⊴eFu⊴e. Denoting by D
the block-diagonal of LF, the normalised sheaf Laplacian is given by ∆F := D−1/2LFD−1/2. For
simplicity, we assume that all the stalks have a ﬁxed dimension d. In that case, the sheaf Laplacian is
a nd × nd real matrix, where n is the number of nodes of G. When the vector spaces are set to R
(i.e., d = 1 ) and the linear maps to the identity map over R, the underlying sheaf is trivial and one
recovers the well-known n × n graph Laplacian matrix and its normalised version ∆0. In general,
∆F is preferred to LF for most practical purposes due to its bounded spectrum and, therefore, we
focus on the former. A cochain x is called harmonic if LFx = 0 or, equivalently, if x ∈ker(LF).
This means harmonic cochains are characterised by zero disagreements along all the edges of the
graph, and it is not difﬁcult to see that, in fact, H0(G; F) and ker(LF) are isomorphic as vector
spaces [35].
The sheaves with orthogonal maps (i.e. Fv⊴e ∈O(d) the Lie group of d × d orthogonal matrices)
provide a more geometric interpretation of sheaves and play an important role in our analysis.
Such sheaves are called discrete O(d) bundles and can be seen as a discrete version of vector
bundles [24, 60, 73] from differential geometry [67]. Intuitively, these objects describe vector spaces
attached to the points of a manifold. In our discrete case, the role of the manifold is played by the
graph, and the sheaf Laplacian describes how the elements of a vector space are transported via
rotations in another neighbouring vector space similarly to how tangent vectors are moved across a
manifold via parallel transport (connection; see Figure 2). Due to this analogy, the sheaf Laplacian
on O(d) bundles is also referred to as connection Laplacian [63].
Heat Diffusion and GCNs. Consider a graph with adjacency matrix A, diagonal degree matrix D,
normalised graph Laplacian ∆0 := I −D−1/2AD−1/2, and an n × f feature matrix X. We can
deﬁne the heat diffusion equation and its Euler discretisation with a unit step as follows:
˙X(t) = −∆0X(t)
/ X(t + 1) = X(t) −∆0X(t) = (I −∆0)X(t).
(1)
Comparing this with the Graph Convolutional Network [39] model, we observe that GCN is an
augmented heat diffusion process with an additional f × f weight matrix W and a nonlinearity σ:
GCN(X, A) := σ(D−1/2AD−1/2XW) = σ((I −∆0)XW).
(2)
From this perspective, it is perhaps not surprising that GCN is particularly affected by heterophily and
oversmoothing since heat diffusion makes the features of neighbouring nodes increasingly smooth.
In what follows, we consider a much more general and powerful family of (sheaf) diffusion processes
leading to more expressive sheaf convolutions.
3
The Expressive Power of Sheaf Diffusion
Preliminaries.
Let us now assume G to be a graph with d-dimensional node feature vectors
xv ∈F(v). The features of all nodes are represented as a single vector x ∈C0(G; F) stacking all
the individual d-dimensional vectors (Figure 3). Additionally, if we allow for f feature channels,
everything can be represented as a matrix X ∈R(nd)×f, whose columns are vectors in C0(G; F).
We are interested in the spatially discretised sheaf diffusion process governed by the following PDE:
X(0) = X,
˙X(t) = −∆FX(t).
(3)
3

It can be shown that in the time limit, each feature channel is projected into ker(∆F) [34]. As
described above (up to a D−1/2 normalisation), this space contains the signals that agree with the re-
striction maps of the sheaf along all the edges. Thus, sheaf diffusion can be seen as a ‘synchronisation’
process over the graph, where all the private opinions converge towards global agreement.
In this section, we investigate the expressive power of this process within the inﬁnite time limit.
Because the asymptotic behaviour of sheaf diffusion is determined by the properties of ker(∆F), in
Section 3.1, we investigate when this subspace is non-trivial (i.e. it contains more than just the zero
vector). In Section 3.2, we use this characterisation of the harmonic space to study what sort of sheaf
diffusion processes will asymptotically produce projections into ker(∆F) that can linearly separate
the classes for various kinds of graphs and initial conditions. Since diffusion converges exponentially
fast, the following results are also relevant for models with ﬁnite integration time or layers.
3.1
Harmonic Space of Sheaf Laplacians
A major role in the analysis below is played by discrete vector bundles, and we concentrate on this
case. We note though that our results below generalise to the general linear group Fv⊴e ∈GL(d),
the Lie group of d × d invertible matrices, provided we can also control the norm of the restriction
maps from below. Given a discrete O(d)-bundle, F⊤
v⊴eFv⊴e = Id and the block diagonal of LF
has a diagonal structure since LF vv = dvId, where dv is the degree of node v. Accordingly, if
a signal ˜x ∈ker(LF), then the signal x : v 7→√dv˜xv ∈ker(∆F) and similarly for the inverse
transformation.
Key to our analysis is studying transport operators induced by the restriction maps of the sheaf.
Given nodes v, u ∈V and a path γv→u = (v, v1, . . . , vℓ, u) from v to u, we consider a notion of
transport from the stalk F(v) to the stalk F(u), constructed by composing restriction maps (and
their transposes) along the edges:
Pγ
v→u := (F⊤
u⊴eFvℓ⊴e) . . . (F⊤
v1⊴eFv⊴e) : F(v) →F(u).
For general sheaf structures, the graph transport is path dependent, meaning that how the vectors are
transported across two nodes depends on the path between them (see Figure 2). In fact, we show that
this property characterises the spectral gap of a sheaf Laplacian, i.e. the smallest eigenvalue of ∆F.
Proposition 3.
If F
is a discrete O(d) bundle over a connected graph and r
:=
maxγv→u,γ′v→u||Pγ
v→u −Pγ′
v→u||, then we have λF
0 ≤r2/2.
A consequence of this result is that there is always a non-trivial harmonic space (i.e. λF
0 = 0) if the
transport maps generated by an orthogonal sheaf are path-independent (i.e. r = 0). Next, we address
the opposite direction.
Proposition 4. If F is a discrete O(d) bundle over a connected graph and x ∈H0(G, F), then for
any cycle γ based at v ∈V we have xv ∈ker(Pγ
v→v −I).
This proposition highlights the interplay between the graph and the sheaf structure. A simple
consequence of this result is that for any cycle-free subset S ⊂V , we have that any sheaf (or
connection-) Laplacian restricted to S always admits a non-trivial harmonic space. A natural question
connected to the previous result is whether a Cheeger-like inequality holds in the other direction.
This turns out to be the case:
Proposition 5. Let F be a discrete O(d) bundle over a connected graph G with n nodes and let
||(Pγ
v→v −I)xv|| ≥ϵ||xv|| for all cycles γv→v. Then λF
0 ≥ϵ2(2diam(G)n dmax)−1.
While the bound above is of little use in practice, it shows how the spectral gap of a sheaf Laplacian
is indeed related to the deviation of the transport maps from being path-independent, as measured by
ϵ. We note that the Cheeger-like inequality presented here is not unique, and other types of bounds
on λF
0 have been derived [2]. We conclude this section by further analysing the dimensionality of the
harmonic space of discrete O(d)-bundles:
Lemma 6. Let F be a discrete O(d) bundle over a connected graph G. Then dim(H0) ≤d and
dim(H0) = d if and only if the transport is path-independent.
4

Figure 4: Diffusion process on O(2)-bundles progressively separates the classes of the graph.
3.2
The Linear Separation Power of Sheaf Diffusion
In what follows, we use the results above to analyse the ability of certain classes of sheaves to linearly
separate the features in the limit of the diffusion processes they induce. We utilise this as a proxy for
the capacity of certain diffusion processes to avoid oversmoothing.
Deﬁnition 7. A hypothesis class of sheaves with d-dimensional stalks Hd has linear separation power
over a family of graphs G if for any labelled graph G = (V, E) ∈G, there is a sheaf (F, G) ∈Hd
that can linearly separate the classes of G in the time limit of Equation 3 for almost all initial
conditions.
Note that the restriction to almost all initial conditions is necessary because, in the limit, diffusion
behaves like a projection in the harmonic space and there will always be degenerate initial conditions
(e.g. the zero matrix) that will yield a zero projection. We will now show how the choice of the sheaf
impacts the behaviour of the diffusion process. For this purpose, we will consider a hierarchy of
increasingly general classes of sheaves.
Symmetric invertible.
Hd
sym := {(F, G) : Fv⊴e = Fu⊴e, det(Fv⊴e) ̸= 0}. We note that for
d = 1, the sheaf Laplacians induced by this class of sheaves coincides with the set of the well-
known weighted graph Laplacians with strictly positive weights, which also includes the usual graph
Laplacian (see proof in Appendix B). Therefore, this hypothesis class is of particular interest since it
includes those graph Laplacians typically used by graph convolutional models such as GCN [39] and
ChebNet [20]. We ﬁrst show that this class of sheaf Laplacians can linearly separate the classes in
binary classiﬁcation settings under certain homophily assumptions:
Proposition 8. Let G be the set of connected graphs G = (V, E) with two classes A, B ⊂V such
that for each v ∈A, there exists u ∈A and an edge (v, u) ∈E. Then H1
sym has linear separation
power over G.
In contrast, under certain heterophilic conditions, this hypothesis class is not powerful enough to
linearly separate the two classes no matter what the initial conditions are:
Proposition 9. Let G be the set of connected bipartite graphs G = (A, B, E), with partitions A, B
forming two classes and |A| = |B|. Then H1
sym cannot linearly separate the classes of any graph in
G for any initial conditions X(0) ∈Rn×f.
Non-symmetric invertible.
Hd := {(F, G) : det(Fv⊴e) ̸= 0}. This larger hypothesis class
addresses the above limitation by allowing non-symmetric relations:
Proposition 10. Let G contain all the connected graphs G = (V, E) with two classes A, B ⊆V .
Consider a sheaf (F; G) ∈H1 with Fv⊴e = −αe if v ∈A and Fu⊴e = αe if u ∈B with αe > 0
for all e ∈E. Then the diffusion induced by (F; G) can linearly separate the classes of G for almost
all initial conditions, and H1 has linear separation power over G.
Since F⊤
v⊴eFu⊴e = ±α2
e, the type of sheaf above can be interpreted as a discrete O(1)-bundle over
a weighted graph with edge weights α2
e and transport maps F⊤
v⊴eFu⊴e = −1 for the inter-class
edges and +1 for the intra-class edges. Intuitively, this type of transport, which is path-independent,
polarises the features of the two classes and forces them to take opposite signs in the inﬁnite limit.
This provides a sheaf-theoretic explanation for why negatively-weighted edges have been widely
adopted in heterophilic settings [7, 17, 72].
So far we have only studied the effects of changing the type of sheaves in dimension one. We now
consider the effects of adjusting the dimension of the stalks and begin by stating a fundamental
limitation of (sheaf) diffusion when d = 1.
Proposition 11. Let G be a connected graph with C ≥3 classes. Then, H1 cannot linearly separate
the classes of G for any initial conditions X(0) ∈Rn×f.
5

This is essentially a consequence of dim
 ker(∆F)

≤1 in this case, by virtue of Lemma 6. From a
GNN perspective, this means that in the inﬁnite depth setting, sufﬁcient stalk width (i.e., dimension
d) is needed in order to solve tasks involving more than two classes. Note that d is different from the
classical notion of feature channels f. As the result above shows, the latter has no effect on the linear
separability of the classes in d = 1. Next, we will see that the former does.
Diagonal invertible.
Hd := {(F, G) : diagonal Fv⊴e, det(Fv⊴e) ̸= 0}. The sheaves in this
class can be seen as d independent sheaves from H1 encoded in the d-dimensional diagonals of their
restriction maps. This perspective allows us to generalise Proposition 10 to a multi-class setting:
Proposition 12. Let G be the set of connected graphs with nodes belonging to C ≥3 classes. Then
for d ≥C, Hd
diag has linear separation power over G.
This result illustrates the beneﬁts of using higher-dimensional stalks while maintaining a simple and
computationally convenient class of diagonal restriction maps. Next, with more complex restriction
maps, we can show that lower-dimensional stalks can be used to achieve linear separation in the
presence of even more classes.
Orthogonal.
Hd
orth := {(F, G) : Fv⊴e ∈O(d)} is the class of O(d)-bundles. Orthogonal maps
are able to make more efﬁcient use of the space available to them than diagonal restriction maps:
Proposition 13. Let G be the class of connected graphs with C ≤2d classes. Then, for all d ∈{2, 4},
Hd
orth has linear separation power over G.
Figure 4 includes an example diffusion process over an O(2)-bundle.
Summary: Different sheaf classes give rise to different behaviours of the diffusion process and,
consequently, to different separation capabilities. Taken together, these results show that solving
any node classiﬁcation task can be reduced to performing diffusion with the right sheaf.
4
Expressive Power of Sheaf Convolutions
Analogously to how GCN augments heat diffusion, we can construct a Sheaf Convolutional Network
(SCN) augmenting the sheaf diffusion process. In this section, we analyse the capacity of SCNs to
change, if necessary, their asymptotic behaviour compared to the base diffusion process. Since the
sheaf structure will be ultimately learned from data, this is particularly important for the common
setting when the learned sheaf is different from the “ground truth” sheaf for the task to be solved.
The continous diffusion process from Equation 3 has the Euler discretisation with unit step-size
X(t + 1) = X(t) −∆FX(t) = (Ind −∆F)X(t). Assuming X ∈Rnd×f1, we can equip the right
side with weight matrices W1 ∈Rd×d, W2 ∈Rf1×f2 and a non-linearity σ to arrive at the following
model originally proposed by Hansen and Gebhart [32]:
Y = σ
 Ind −∆F

(In ⊗W1)XW2

∈Rnd×f2,
(4)
where f1, f2 are the number of input and output feature channels, and ⊗denotes the Kronecker
product. Here, W1 multiplies from the left the vector feature of all the nodes in all channels (i.e.
W1xi
v for all v and channels i), while W2 multiplies the features from the right and can adjust the
number of feature channels, just like in GCNs. As one would expect, when using a trivial sheaf,
∆F = ∆0, W1 becomes a scalar and one recovers the GCN of Kipf and Welling [39]. To see how
SCNs behave compared to their base diffusion process, we investigate how SCN layers affect the
sheaf Dirichlet energy EF(x), which sheaf diffusion is known to minimise over time.
Deﬁnition 14. EF(x) := x⊤∆Fx = 1
2
P
e:=(v,u)∥Fv⊴eD−1/2
v
xv −Fu⊴eD−1/2
u
xu∥2
2
Similarly, for multiple channels the energy is EF(X) := trace(X⊤∆FX). This is a measure of how
close a signal x is to ker(∆F) and it is easy to see that x ∈ker(∆F) ⇔EF(x) = 0. We begin by
studying the sheaves for which the energy decreases and representations end up asymptotically in
ker(∆F). Let λ∗:= maxi>0(λF
i −1)2 ≤1 and denote by H1
+ := {(F, G) | Fv⊴eFu⊴e > 0}.
Theorem 15. For (F, G) ∈H1
+ and σ being (Leaky)ReLU, EF(Y) ≤λ∗∥W1∥2
2∥W⊤
2 ∥2
2EF(X).
6

This generalises existent results for GCNs [13, 51] and proves that SCNs using this family of
Laplacians, which includes all weighted graph Laplacians, exponentially converge to ker(∆F) if
λ∗∥W1∥2
2∥W⊤
2 ∥2
2 < 1. In particular, if EF(X) = 0, then EF(Y) = 0 and the representations
remain trapped inside the kernel no matter what the norm of the weights is. Therefore, in settings as
those described by Propositions 9 and 11, the linear separation capabilities of this class of models are
severely limited (see Corollaries 36, 37 in Appendix B).
Finally, the Theorem also extends to bundles with symmetric maps, Hd
orth,sym := Hd
orth ∩Hd
sym:
Theorem 16. If (F, G) ∈Hd
orth,sym and σ = (Leaky)ReLU, EF(Y) ≤λ∗∥W1∥2
2∥W⊤
2 ∥2
2EF(X).
In some sense, this is not surprising because, for this class, ker(∆F) contains the same information
as the kernel of the classical normalised graph Laplacian (see Proposition 29 in Appendix C).
More generally, SCNs with sheaves outside Hd
sym, are much more ﬂexible and can easily increase the
Dirichlet energy using an arbitrarily small linear transformation W1:
Proposition 17. For any connected graph G and ε > 0, there exist a sheaf (G, F) /∈Hd
sym , W1
with ∥W1∥2 < ε and feature vector x such that EF((I ⊗W1)x) > EF(x).
Importantly, this proves that this family of SCNs can, if necessary, escape the kernel of the Laplacian.
Summary: Not only that sheaf diffusion is more expressive than heat diffusion as shown in
Section 3.2, but SCNs are also more expressive than GCNs in the sense that they are generally
not constrained to decrease the Dirichlet energy when using low-norm weights. This provides
them with greater control than GCNs over their asymptotic behaviour.
5
Neural Sheaf Diffusion and Sheaf Learning
In the previous sections, we discussed the various advantages provided by sheaf diffusion and sheaf
convolutions. However, in general, the ground truth sheaf is unknown or unspeciﬁed. Therefore, we
aim to learn the underlying sheaf from data end-to-end, thus allowing the model to pick the right
geometry for solving the task.
Neural Sheaf Diffusion. We propose the diffusion-type model from Equation 5. We note that by
setting W1, W2 to identity and σ(x) = ELU(ϵx)/ϵ with ϵ > 0 small enough or simply σ = id, we
recover (up to a scaling) the sheaf diffusion equation. Therefore, the model is at least as expressive as
sheaf diffusion and beneﬁts from all the positive properties outlined in Section 3.2.
˙X(t) = −σ

∆F(t)(In ⊗W1)X(t)W2

,
(5)
Crucially, the sheaf Laplacian ∆F(t) is that of a sheaf (G, F(t)) that evolves over time. More
speciﬁcally, the evolution of the sheaf structure is described by a learnable function of the data
(G, F(t)) = g(G, X(t); θ). This allows the model to use the latest available features to manipulate the
underlying geometry of the graph and, implicitly, the behaviour of the diffusion process. Additionally,
We use an MLP followed by a reshaping to map the raw features of the dataset to a matrix X(0) of
shape nd × f and a ﬁnal linear layer to perform the node classiﬁcation.
In our experiments, we focus on the time-discretised version of this model from Equation 6, which
allows us to use a new set of weights at each layer t while maintaining the nice theoretical properties
of the model above.
Xt+1 = Xt −σ

∆F(t)(I ⊗Wt
1)XtWt
2

(6)
We note that this model is different from the SCN model from Equation 4 in two major ways. First,
Hansen and Gebhart [32] used a hand-crafted sheaf with d = 1, constructed in a synthetic setting with
full knowledge of the data-generating process. In contrast, we learn a sheaf, which makes our model
applicable to any real-world graph dataset, even in the absence of a sheaf structure. Additionally,
motivated by our theoretical results, we use the full generality of sheaves by using stalks with d ≥1
and higher-dimensional maps. Second, our model uses a residual parametrisation of the discretised
diffusion process, which empirically improves its performance.
Sheaf Learning. The restriction maps are learned using locally available information. Each d × d
matrix Fv⊴e is learned via a parametric matrix-valued function Φ, with Fv⊴e:=(v,u) = Φ(xv, xu).
7

Figure 5: (Left) Train and (Middle) test accuracy as a function of diffusion time. (Right) Histogram
of the learned scalar transport maps. The performance of the sheaf diffusion model is superior to that
of weighted-graph diffusion and correctly learns to invert the features of the two classes.
This function must be non-symmetric to be able to learn asymmetric transport maps along each edge.
In practice, we set Φ(xv, xu) = σ(V[xv||xu]) followed by a reshaping of the output, where V is
a weight matrix. For simplicity, the equations above use a single feature channel, but in practice,
all channels are supplied as input. More generally, we can show that if the function Φ has enough
capacity and the features are diverse enough, we can learn any sheaf over a graph.
Proposition 18. Let G = (V, E) be a ﬁnite graph with features X. Then, if (xv, xu) ̸= (xw, xz) for
any (v, u) ̸= (w, z) ∈E and Φ is an MLP with sufﬁcient capacity, Φ can learn any sheaf (F; G).
First, this result formally motivates learning a sheaf at each layer since the model can learn to
distinguish more nodes after each aggregation step. Second, this suggests that more expressive
models (in the Weisfeiler-Lehman sense [8, 9, 46, 71]) could learn a more general family of sheaves.
We leave a deeper investigation of these aspects for future work. In what follows, we distinguish
between several types of functions Φ depending on the type of matrix they learn.
Diagonal. The main advantage of this parametrisation is that fewer parameters need to be learned
per edge, and the sheaf Laplacian ends up being a matrix with diagonal blocks, which also results in
fewer operations in sparse matrix multiplications. The main disadvantage is that the d dimensions of
the stalks interact only via the left W1 multiplication.
Orthogonal.
In this case, the model effectively learns a discrete vector bundle. Orthogonal
matrices provide several advantages: (1) they can mix the various dimension of the stalks, (2) the
orthogonality constraint prevents overﬁtting while reducing the number of parameters, (3) they have
better understood theoretical properties, and (4) the resulting Laplacians are easier to normalise
numerically since the diagonal entries correspond to the degrees of the nodes. In our model, we build
orthogonal matrices from a composition of Householder reﬂections [45].
General. Finally, we consider the most general option of learning arbitrary matrices. The maximal
ﬂexibility these maps provide can be useful, but it also comes with the danger of overﬁtting. At
the same time, the sheaf Laplacian is more challenging to normalise numerically since one has to
compute D−1/2 for a positive semi-deﬁnite matrix D. To perform this at scale, one has to rely on
SVD, whose gradients can be inﬁnite if D has repeated eigenvalues. Therefore, this model is more
challenging to train.
Computational Complexity. The GCN from Equation 2 has complexity O(nc2 + mc), where c
is the number of channels and m the number of edges. Assume a sheaf diffusion model with stalk
dimension d and f channels such that d×f = c (i.e. same representation size). Then, when the model
uses diagonal maps, the complexity is O(nc2 + mdc). When using orthogonal or general matrices,
the complexity becomes O(n(c2 + d3) + m(cd2 + d3)) (see Appendix E.1 for detailed derivations).
In practice, we use 1 ≤d ≤5 , which effectively results in a constant overhead compared to GCN.
6
Experiments
Synthetic experiments.
We consider a simple setup given by a connected bipartite graph with
equally sized partitions. We sample the features from two overlapping isotropic Gaussian distributions
to make the classes linearly non-separable at initialisation time. From Proposition 9, we know that
diffusion models using symmetric restriction maps cannot separate the classes in the limit, while a
diffusion process using negative transport maps can. Therefore, we use two vanilla sheaf diffusion
processes by setting d = 1, W1 = Id, W2 = If and σ = id in Equation 5. In both models, we learn
8

Table 1: Results on node classiﬁcation datasets sorted by their homophily level. Top three models are
coloured by First, Second, Third. Our models are marked NSD.
Texas
Wisconsin
Film
Squirrel
Chameleon
Cornell
Citeseer
Pubmed
Cora
Hom level
0.11
0.21
0.22
0.22
0.23
0.30
0.74
0.80
0.81
#Nodes
183
251
7,600
5,201
2,277
183
3,327
18,717
2,708
#Edges
295
466
26,752
198,493
31,421
280
4,676
44,327
5,278
#Classes
5
5
5
5
5
5
7
3
6
Diag-NSD
85.67±6.95
88.63±2.75
37.79±1.01
54.78±1.81
68.68±1.73
86.49±7.35
77.14±1.85
89.42±0.43
87.14±1.06
O(d)-NSD
85.95±5.51
89.41±4.74
37.81±1.15
56.34±1.32
68.04±1.58
84.86±4.71
76.70±1.57
89.49±0.40
86.90±1.13
Gen-NSD
82.97±5.13
89.21±3.84
37.80±1.22
53.17±1.31
67.93±1.58
85.68±6.51
76.32±1.65
89.33±0.35
87.30±1.15
GGCN
84.86±4.55
86.86±3.29
37.54±1.56
55.17±1.58
71.14±1.84
85.68±6.63
77.14±1.45
89.15±0.37
87.95±1.05
H2GCN
84.86±7.23
87.65±4.98
35.70±1.00
36.48±1.86
60.11±2.15
82.70±5.28
77.11±1.57
89.49±0.38
87.87±1.20
GPRGNN
78.38±4.36
82.94±4.21
34.63±1.22
31.61±1.24
46.58±1.71
80.27±8.11
77.13±1.67
87.54±0.38
87.95±1.18
FAGCN
82.43±6.89
82.94±7.95
34.87±1.25
42.59±0.79
55.22±3.19
79.19±9.79
N/A
N/A
N/A
MixHop
77.84±7.73
75.88±4.90
32.22±2.34
43.80±1.48
60.50±2.53
73.51±6.34
76.26±1.33
85.31±0.61
87.61±0.85
GCNII
77.57±3.83
80.39±3.40
37.44±1.30
38.47±1.58
63.86±3.04
77.86±3.79
77.33±1.48
90.15±0.43
88.37±1.25
Geom-GCN
66.76±2.72
64.51±3.66
31.59±1.15
38.15±0.92
60.00±2.81
60.54±3.67
78.02±1.15
89.95±0.47
85.35±1.57
PairNorm
60.27±4.34
48.43±6.14
27.40±1.24
50.44±2.04
62.74±2.82
58.92±3.15
73.59±1.47
87.53±0.44
85.79±1.01
GraphSAGE
82.43±6.14
81.18±5.56
34.23±0.99
41.61±0.74
58.73±1.68
75.95±5.01
76.04±1.30
88.45±0.50
86.90±1.04
GCN
55.14±5.16
51.76±3.06
27.32±1.10
53.43±2.01
64.82±2.24
60.54±5.30
76.50±1.36
88.42±0.50
86.98±1.27
GAT
52.16±6.63
49.41±4.09
27.44±0.89
40.72±1.55
60.26±2.50
61.89±5.05
76.55±1.23
87.30±1.10
86.33±0.48
MLP
80.81±4.75
85.29±3.31
36.53±0.70
28.77±1.56
46.21±2.99
81.89±6.40
74.02±1.90
87.16±0.37
75.69±2.00
a sheaf at t = 0 as a function of X(0), and we keep the sheaf constant over time. For the ﬁrst model,
we learn a sheaf with general maps Fv⊴e ∈R. For the second model, we use a similar layer but
constraint Fv⊴e = Fu⊴e, obtaining a weighted graph Laplacian.
Figure 5 presents the results across ﬁve seeds. As expected, for diffusion time zero (i.e. no diffusion),
we see that a linear classiﬁer cannot separate the classes. At later times, the diffusion process using
symmetric maps cannot perfectly ﬁt the data. In contrast, with the more general sheaf diffusion, as
time increases and the signal approaches the harmonic space, the model gets better and the features
become linearly separable. In the last subﬁgure, we take a closer look at the sheaf that the model
learns in the time limit by plotting a histogram of all the transport (scalar) maps F⊤
v⊴eFu⊴e. In
accordance with Proposition 10, the model learns a negative transport map for all edges. This shows
that the model manages to avoid oversmoothing (see Appendix F for an experiment with d > 1).
Real-world experiments.
We test our models on multiple real-world datasets [47, 53, 57, 61, 66]
with an edge homophily coefﬁcient h ranging from h = 0.11 (very heterophilic) to h = 0.81 (very
homophilic). Therefore, they offer a view of how a model performs over this entire spectrum. We
evaluate our models on the 10 ﬁxed splits provided by Pei et al. [53] and report the mean accuracy and
standard deviation. Each split contains 48%/32%/20% of nodes per class for training, validation and
testing, respectively. As baselines, we use an ample set of GNN models that can be placed in three
categories: (1) classical: GCN [39], GAT [68], GraphSAGE [31]; (2) models speciﬁcally designed
for heterophilic settings: GGCN [72], Geom-GCN [53], H2GCN [75], GPRGNN [17], FAGCN [7],
MixHop [1]; (3) models addressing oversmoothing: GCNII [16], PairNorm [74]. All the results are
taken from Yan et al. [72], except for FAGCN and MixHop, which come from Lingam et al. [41]
and Zhu et al. [75], respectively. All of these were evaluated on the same set of splits as ours. In
Appendix F we also include experiments with continuous GNN models.
Results.
From Table 1 we see that our models are ﬁrst in 5/6 benchmarks with high heterophily
(h < 0.3) and second-ranked on the remaining one (i.e. Chameleon). At the same time, NSD also
shows strong performance on the homophilic graphs by being within approximately 1% of the top
model. Overall, NSD models are among the top three models on 8/9 datasets. The O(d)-bundle
diffusion model performs best overall conﬁrming the intuition that it can better avoid overﬁtting, while
also transforming the vectors in sufﬁciently complex ways. We also remark on the strong performance
of the model learning diagonals maps, despite the simpler functional form of the Laplacian.
7
Related Work, Discussion, and Conclusion
Sheaf Neural Networks & Sheaf Learning.
Sheaf Neural Networks [32] with a hand-crafted
sheaf Laplacian were originally introduced in a toy experimental setting. Since then, they have
remained completely unexplored, and we hope this paper will ﬁll this lacuna. In contrast to [32], we
provide an ample theoretical analysis justifying the use of sheaves in Graph ML and study for the
ﬁrst time how sheaves can be learned from data using neural networks. Furthermore, we present the
ﬁrst successful application of Sheaf Neural Networks on real-world datasets. Hansen and Ghrist [33]
9

have also considered learning a sheaf Laplacian by minimising directly in matrix space a regularised
Dirichlet energy metric. Different from their approach, we learn the sheaf as part of an end-to-end
model and use an efﬁcient parametrisation that is independent of the size of the graph.
Follow-up works have also experimented with inferring a connection Laplacian directly from data at
pre-processing time [3], combining sheaves with attention [4], and designing models based on the
wave equation on sheaves [65]. Besides the sheaf Laplacians employed in all these works and ours,
one can also use higher-order sheaf (connection) Laplacians that operate on higher-order tensors.
These were shown to encode important information about the underlying symmetries in the data [55],
which hints at the powerful data properties that Sheaf Neural Networks could potentially extract from
these operators.
Heterophily and Oversmoothing.
While good empirical designs jointly addressing these two
problems have been proposed before [17, 72], Yan et al. [72] is the only other work connecting the
two theoretically. Their analysis [72] is very different in terms of methods and assumptions and,
therefore, their results are completely orthogonal. Concretely, the authors analyse the performance
of linear SGCs [69] (i.e. GCN without nonlinearities) on random attributed graphs. In contrast, our
analysis is not probabilistic, focuses on diffusion PDEs and also extends to GCNs in the non-linear
regime. Furthermore, we employ a new set of mathematical tools from cellular sheaf theory, which
brings a new language and new tools to analyse these problems. Perhaps the only commonality is that
both works ﬁnd evidence for the beneﬁts of negatively signed edges in GNNs, although with different
mathematical motivations. At the same time, other recent works [21, 42] have shown that GCNs with
ﬁnite layers (typically one) can perform well in heterophilic graphs (including bipartite). This is in
no contradiction with our results, which consider an inﬁnite time/layer regime (i.e. not ﬁnite) and
perfect linear separation (i.e. a model that cannot ﬁt the data can still achieve high accuracy).
Category Theory and GNNs.
From the perspective of category theory [43], cellular sheaves are a
functor from a category describing the incidence structure of the graph to a category describing the
data living on top of the graph. Informally, this says that the vertices and edges are mapped to some
type of data (e.g. vector spaces) and the incidence relations between vertices and edges are mapped
to some type of relation between the assigned data (e.g. linear maps between the vector spaces). The
generality provided by this perspective could be used to extend the models described in this work to
more exotic types of data such as lattices and their associated sheaf Laplacians [25]. At the same
time, our work echoes other recent efforts to place GNNs on a categorical foundation [19, 22].
Message Passing Neural Networks.
The layer from Equation 6 can be seen as a form of GNN-
FiLM layer [11, 54], where each node learns a linear message function conditioned on the features of
the neighbours. Such models have been recently shown to perform well empirically in heterophilic
settings [52]. At the same time, the model bares an algorithmic resemblance to GAT [68]. For a
central node v and a neighbouring node u, GAT learns an attention coefﬁcient avu, while our model
learns a matrix given by the block (v, u) of ∆F. Finally, a message-passing procedure based on
parallel transport has also been proposed by Haan et al. [30] in the context of geometric graphs
(meshes). In the absence of a natural geometric structure on arbitrary graphs, in our case, the transport
structure is learned from data end-to-end.
Limitations and societal impact.
One of the main limitations of our theoretical analysis is that it
does not address the generalisation properties of sheaves, but this remains a major impediment for
the entire ﬁeld of deep learning. Nonetheless, our setting was sufﬁcient to produce many valuable
insights about heterophily and oversmoothing and a basic understanding of what various types of
sheaves can and cannot do. Much more work remains to be done in this direction, and we expect to
see further cross-fertilization between ML and algebraic topology in the future. Finally, due to the
theoretical nature of this work, we do not foresee any immediate negative societal impacts.
Conclusion.
In this work, we used cellular sheaf theory to provide a novel topological perspective
on heterophily and oversmoothing in GNNs. We showed that the underlying sheaf structure of the
graph is intimately connected with both of these important factors affecting the performance of GNNs.
To mitigate this, we proposed a new paradigm for graph representation learning where models not
only evolve the features at each layer but also the underlying geometry of the graph. In practice, we
demonstrated that this framework achieves competitive results in heterophilic settings.
10

Acknowledgments and Disclosure of Funding
We are grateful to Iulia Duta, Dobrik Georgiev and Jacob Deasy for valuable comments on an earlier
version of this manuscript. CB would also like to thank the Twitter Cortex team for making the
research internship a fantastic experience. This research was supported in part by ERC Consolidator
grant No. 724228 (LEMAN).
References
[1] Sami Abu-El-Haija, Bryan Perozzi, Amol Kapoor, Hrayr Harutyunyan, Nazanin Alipourfard,
Kristina Lerman, Greg Ver Steeg, and Aram Galstyan. Mixhop: Higher-order graph convo-
lutional architectures via sparsiﬁed neighborhood mixing. In The Thirty-sixth International
Conference on Machine Learning (ICML), 2019. URL http://proceedings.mlr.press/
v97/abu-el-haija19a/abu-el-haija19a.pdf.
[2] Afonso S Bandeira, Amit Singer, and Daniel A Spielman. A Cheeger inequality for the graph
connection laplacian. SIAM Journal on Matrix Analysis and Applications, 34(4):1611–1630,
2013.
[3] Federico Barbero, Cristian Bodnar, Haitz Sáez de Ocáriz Borde, Michael Bronstein, Petar
Veliˇckovi´c, and Pietro Liò. Sheaf neural networks with connection laplacians. In ICML 2022
Workshop on Topology, Algebra, and Geometry in Machine Learning, 2022.
[4] Federico Barbero, Cristian Bodnar, Haitz Sáez de Ocáriz Borde, and Pietro Lio. Sheaf attention
networks. In NeurIPS 2022 Workshop on Symmetry and Geometry in Neural Representations,
2022.
[5] Lukas Biewald. Experiment tracking with weights and biases, 2020. URL https://www.
wandb.com/. Software available from wandb.com.
[6] Christopher M. Bishop. Pattern Recognition and Machine Learning (Information Science and
Statistics). Springer-Verlag, Berlin, Heidelberg, 2006. ISBN 0387310738.
[7] Deyu Bo, Xiao Wang, Chuan Shi, and Huawei Shen. Beyond low-frequency information in
graph convolutional networks. In AAAI. AAAI Press, 2021.
[8] Cristian Bodnar, Fabrizio Frasca, Nina Otter, Yu Guang Wang, Pietro Liò, Guido Montúfar, and
Michael Bronstein. Weisfeiler and Lehman Go Cellular: CW Networks. In NeurIPS, 2021.
[9] Cristian Bodnar, Fabrizio Frasca, Yuguang Wang, Nina Otter, Guido F Montufar, Pietro Lió,
and Michael Bronstein. Weisfeiler and Lehman Go Topological: Message Passing Simplicial
Networks. In ICML, 2021.
[10] Glen E Bredon. Sheaf theory, volume 170. Springer Science & Business Media, 2012.
[11] Marc Brockschmidt. Gnn-ﬁlm: Graph neural networks with feature-wise linear modulation. In
International Conference on Machine Learning, pages 1144–1152. PMLR, 2020.
[12] Joan Bruna, Wojciech Zaremba, Arthur Szlam, and Yann LeCun. Spectral networks and locally
connected networks on graphs. In ICLR, 2014.
[13] Chen Cai and Yusu Wang.
A note on over-smoothing for graph neural networks.
arXiv:2006.13318, 2020.
[14] Benjamin Paul Chamberlain, James Rowbottom, Davide Eynard, Francesco Di Giovanni, Dong
Xiaowen, and Michael M Bronstein. Beltrami ﬂow and neural diffusion on graphs. In NeurIPS,
2021.
[15] Benjamin Paul Chamberlain, James Rowbottom, Maria Goronova, Stefan Webb, Emanuele
Rossi, and Michael M Bronstein. Grand: Graph neural diffusion. In ICML, 2021.
11

[16] Ming Chen, Zhewei Wei, Zengfeng Huang, Bolin Ding, and Yaliang Li. Simple and deep
graph convolutional networks. In Hal Daumé III and Aarti Singh, editors, Proceedings of the
37th International Conference on Machine Learning, volume 119 of Proceedings of Machine
Learning Research, pages 1725–1735. PMLR, 13–18 Jul 2020. URL https://proceedings.
mlr.press/v119/chen20v.html.
[17] Eli Chien, Jianhao Peng, Pan Li, and Olgica Milenkovic. Adaptive universal generalized
pagerank graph neural network. In International Conference on Learning Representations,
2021. URL https://openreview.net/forum?id=n6jl7fLxrP.
[18] Justin Michael Curry. Sheaves, cosheaves and applications. University of Pennsylvania, 2014.
[19] Pim de Haan, Taco Cohen, and Max Welling. Natural graph networks. In NeurIPS, 2020.
[20] Michaël Defferrard, Xavier Bresson, and Pierre Vandergheynst. Convolutional neural networks
on graphs with fast localized spectral ﬁltering. In NIPS, 2016.
[21] Lun Du, Xiaozhou Shi, Qiang Fu, Xiaojun Ma, Hengyu Liu, Shi Han, and Dongmei Zhang.
Gbk-gnn: Gated bi-kernel graph neural networks for modeling both homophily and heterophily.
In Proceedings of the ACM Web Conference 2022, pages 1550–1558, 2022.
[22] Andrew Dudzik and Petar Veliˇckovi´c. Graph neural networks are dynamic programmers. arXiv
preprint arXiv:2203.15544, 2022.
[23] Vijay Prakash Dwivedi, Chaitanya K Joshi, Thomas Laurent, Yoshua Bengio, and Xavier
Bresson. Benchmarking graph neural networks. arXiv:2003.00982, 2020.
[24] Tingran Gao, Jacek Brodzki, and Sayan Mukherjee. The geometry of synchronization problems
and learning group actions. Discrete & Computational Geometry, 65(1):150–211, 2021.
[25] Robert Ghrist and Hans Riess.
Cellular sheaves of lattices and the tarski laplacian.
arXiv:2007.04099, 2020.
[26] Robert W Ghrist. Elementary applied topology, volume 1. Createspace Seattle, 2014.
[27] Justin Gilmer, Samuel S Schoenholz, Patrick F Riley, Oriol Vinyals, and George E Dahl. Neural
message passing for quantum chemistry. In ICML, 2017.
[28] Christoph Goller and Andreas Kuchler. Learning task-dependent distributed representations by
backpropagation through structure. In ICNN, 1996.
[29] Marco Gori, Gabriele Monfardini, and Franco Scarselli. A new model for learning in graph
domains. In IJCNN, 2005.
[30] Pim De Haan, Maurice Weiler, Taco Cohen, and Max Welling. Gauge equivariant mesh
CNNs: Anisotropic convolutions on geometric graphs. In International Conference on Learning
Representations, 2021. URL https://openreview.net/forum?id=Jnspzp-oIZE.
[31] William L Hamilton, Rex Ying, and Jure Leskovec. Representation learning on graphs: Methods
and applications. IEEE Data Engineering Bulletin, 2017.
[32] Jakob Hansen and Thomas Gebhart. Sheaf neural networks. In NeurIPS 2020 Workshop on
Topological Data Analysis and Beyond, 2020.
[33] Jakob Hansen and Robert Ghrist.
Learning sheaf laplacians from smooth signals.
In
ICASSP 2019-2019 IEEE International Conference on Acoustics, Speech and Signal Processing
(ICASSP), pages 5446–5450. IEEE, 2019.
[34] Jakob Hansen and Robert Ghrist. Toward a spectral theory of cellular sheaves. Journal of
Applied and Computational Topology, 3(4):315–358, 2019.
[35] Jakob Hansen and Robert Ghrist. Opinion dynamics on discourse sheaves. SIAM Journal on
Applied Mathematics, 81(5):2033–2060, 2021.
12

[36] Kurt Hornik. Approximation capabilities of multilayer feedforward networks. Neural networks,
4(2):251–257, 1991.
[37] Kurt Hornik, Maxwell Stinchcombe, and Halbert White. Multilayer feedforward networks are
universal approximators. Neural networks, 2(5):359–366, 1989.
[38] Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. In ICLR,
2015.
[39] Thomas N Kipf and Max Welling. Semi-supervised classiﬁcation with graph convolutional
networks. In ICLR, 2017.
[40] Hyun Deok Lee. On some matrix inequalities. Korean Journal of Mathematics, 16(4):565–571,
2008.
[41] Vijay Lingam, Rahul Ragesh, Arun Iyer, and Sundararajan Sellamanickam. Simple truncated
svd based model for node classiﬁcation on heterophilic graphs. arXiv preprint arXiv:2106.12807,
2021.
[42] Sitao Luan, Chenqing Hua, Qincheng Lu, Jiaqi Zhu, Mingde Zhao, Shuyuan Zhang, Xiao-Wen
Chang, and Doina Precup. Is heterophily a real nightmare for graph neural networks to do node
classiﬁcation? arXiv preprint arXiv:2109.05641, 2021.
[43] Saunders Mac Lane. Categories for the working mathematician, volume 5. Springer Science &
Business Media, 2013.
[44] Saunders MacLane and Ieke Moerdijk. Sheaves in geometry and logic: A ﬁrst introduction to
topos theory. Springer Science & Business Media, 2012.
[45] Zakaria Mhammedi, Andrew Hellicar, Ashfaqur Rahman, and James Bailey. Efﬁcient orthogonal
parametrisation of recurrent neural networks using householder reﬂections. In International
Conference on Machine Learning, pages 2401–2409. PMLR, 2017.
[46] Christopher Morris, Martin Ritzert, Matthias Fey, William L Hamilton, Jan Eric Lenssen,
Gaurav Rattan, and Martin Grohe. Weisfeiler and Leman go neural: Higher-order graph neural
networks. In AAAI, 2019.
[47] Galileo Namata, Ben London, Lise Getoor, Bert Huang, and U Edu. Query-driven active
surveying for collective classiﬁcation. In 10th International Workshop on Mining and Learning
with Graphs, volume 8, page 1, 2012.
[48] H Nt and T Maehara. Revisiting graph neural networks: all we have is low pass ﬁlters. arXiv
preprint arXiv:1812.08434v4, 2019.
[49] Anton Obukhov. Efﬁcient householder transformation in pytorch, 2021. URL www.github.
com/toshas/torch-householder. Version: 1.0.1, DOI: 10.5281/zenodo.5068733.
[50] K Oono and T Suzuki. Graph neural networks exponentially lose expressive power for node
classiﬁcation. In International Conference on Learning Representations, 2020.
[51] Kenta Oono and Taiji Suzuki. Graph neural networks exponentially lose expressive power for
node classiﬁcation. arXiv:1905.10947, 2019.
[52] John Palowitch, Anton Tsitsulin, Brandon Mayer, and Bryan Perozzi. Graphworld: Fake graphs
bring real insights for gnns. arXiv preprint arXiv:2203.00112, 2022.
[53] Hongbin Pei, Bingzhe Wei, Kevin Chen-Chuan Chang, Yu Lei, and Bo Yang. Geom-gcn:
Geometric graph convolutional networks. arXiv preprint arXiv:2002.05287, 2020.
[54] Ethan Perez, Florian Strub, Harm De Vries, Vincent Dumoulin, and Aaron Courville. Film:
Visual reasoning with a general conditioning layer. In Proceedings of the AAAI Conference on
Artiﬁcial Intelligence, volume 32, 2018.
[55] David Pfau, Irina Higgins, Alex Botev, and Sébastien Racanière. Disentangling by subspace
diffusion. Advances in Neural Information Processing Systems, 33:17403–17415, 2020.
13

[56] Daniel Rosiak. Sheaf theory through examples. MIT Press, 2022.
[57] Benedek Rozemberczki, Carl Allen, and Rik Sarkar. Multi-scale attributed node embedding.
Journal of Complex Networks, 9(2):cnab014, 2021.
[58] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini.
The graph neural network model. IEEE Trans. Neural Networks, 20(1):61–80, 2008.
[59] Richard D Schafer. An introduction to nonassociative algebras. Courier Dover Publications,
2017.
[60] Luis Scoccola and Jose A Perea. Approximate and discrete euclidean vector bundles. arXiv
preprint arXiv:2104.07563, 2021.
[61] Prithviraj Sen, Galileo Namata, Mustafa Bilgic, Lise Getoor, Brian Galligher, and Tina Eliassi-
Rad. Collective classiﬁcation in network data. AI magazine, 29(3):93–93, 2008.
[62] Allen Dudley Shepard. A cellular description of the derived category of a stratiﬁed space. PhD
thesis, Brown University, 1985.
[63] Amit Singer and H-T Wu. Vector diffusion maps and the connection laplacian. Communications
on pure and applied mathematics, 65(8):1067–1144, 2012.
[64] Alessandro Sperduti. Encoding labeled graphs by labeling RAAM. In NIPS, 1994.
[65] Julian Suk, Lorenzo Giusti, Tamir Hemo, Miguel Lopez, Konstantinos Barmpas, and Cristian
Bodnar. Surﬁng on the neural sheaf. In NeurIPS 2022 Workshop on Symmetry and Geometry in
Neural Representations, 2022.
[66] Jie Tang, Jimeng Sun, Chi Wang, and Zi Yang. Social inﬂuence analysis in large-scale networks.
In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery
and data mining, pages 807–816, 2009.
[67] Loring W Tu. Manifolds. In An Introduction to Manifolds, pages 47–83. Springer, 2011.
[68] Petar Veliˇckovi´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro Lio, and Yoshua
Bengio. Graph attention networks. In ICLR, 2018.
[69] Felix Wu, Tianyi Zhang, Amauri Holanda de Souza Jr, Christopher Fifty, Tao Yu, and Kilian Q
Weinberger. Simplifying graph convolutional networks. In ICML, 2019.
[70] Louis-Pascal Xhonneux, Meng Qu, and Jian Tang. Continuous graph neural networks. In ICML,
2020.
[71] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural
networks? In ICLR, 2019.
[72] Yujun Yan, Milad Hashemi, Kevin Swersky, Yaoqing Yang, and Danai Koutra. Two sides
of the same coin: Heterophily and oversmoothing in graph convolutional neural networks.
arXiv:2102.06462, 2021.
[73] Fouad El Zein and Jawad Snoussi. Local systems and constructible sheaves. In Arrangements,
local systems and singularities, pages 111–153. Springer, 2009.
[74] Lingxiao Zhao and Leman Akoglu. Pairnorm: Tackling oversmoothing in gnns. In International
Conference on Learning Representations, 2020. URL https://openreview.net/forum?
id=rkecl1rtwB.
[75] Jiong Zhu, Yujun Yan, Lingxiao Zhao, Mark Heimann, Leman Akoglu, and Danai Koutra. Be-
yond homophily in graph neural networks: Current limitations and effective designs. Advances
in Neural Information Processing Systems, 33, 2020.
14

A
Harmonic Space Proofs
Proposition 3.
If F
is a discrete O(d) bundle over a connected graph and r
:=
maxγv→u,γ′v→u||Pγ
v→u −Pγ′
v→u||, then we have λF
0 ≤r2/2.
Proof. We ﬁrst note that on a discrete O(d) bundle the degree operator Dv = dvI since by orthogo-
nality F⊤
v⊴eFv⊴e = I. We can use the Rayleigh quotient to characterize λF
0 as
λF
0 = min
x∈Rnd
⟨x, ∆Fx⟩
||x||2
.
Fix v ∈V and choose a minimal path γv→u for all u ∈V . For an arbitrary non-zero zv, consider the
signal zu = P γ
v→uzv and we set ˜zu
√du = zu.
||Fu⊴e
zu
√du
−Fw⊴e
zw
√dw
||2 = ||˜zu −(F⊤
u⊴eFw⊴e)˜zw||2 = ||P γ
v→u˜zv −(F⊤
u⊴eFw⊴e)Pγ
v→w˜zv||2,
where we have again used that the maps are orthogonal. Since (F⊤
u⊴eFw⊴e)Pγ
v→w = Pγ′
v→u we ﬁnd
that the right hand side can be bound from above by r2||˜zv||2. Therefore, by using Deﬁnition 14 we
ﬁnally obtain
λF
0 = min
x∈Rnd
⟨x, ∆Fx⟩
||x||2
≤⟨z, ∆Fz⟩
||z||2
= 1
2
P
u∼w||Fu⊴e zu
√du −Fw⊴e zw
√dw ||2
||z||2
≤r2
2
P
u∼w||˜zv||2
||z||2
.
Since the transport maps are all orthogonal we get
||z||2 =
X
u
du||Pγ
v→u˜zv||2 =
X
u
du||˜zv||2 =
X
u∼w
||˜zv||2.
We conclude that
λF
0 ≤r2
2
P
u∼w||˜zv||2
||z||2
= r2
2 .
Proposition 4. If F is a discrete O(d) bundle over a connected graph and x ∈H0(G, F), then for
any cycle γ based at v ∈V we have xv ∈ker(Pγ
v→v −I).
Proof. Assume that x ∈H0(G, F) and consider v ∈V and any cycle based at v denoted by
γv→v = (v0 = v, v1, . . . , vL = v). According to the Hodge Theorem we have that
Fvi+1⊴exvi+1 = Fvi⊴exvi =⇒xvi+1 = (F⊤
vi+1Fvi)xvi := ρvi→vi+1xvi.
By composing all the maps we ﬁnd:
xv = ρvL−1→vL · · · ρv0→v1xv = Pγ
v→vxv
which completes the proof.
Proposition 5. Let F be a discrete O(d) bundle over a connected graph G with n nodes and let
||(Pγ
v→v −I)xv|| ≥ϵ||xv|| for all cycles γv→v. Then λF
0 ≥ϵ2(2diam(G)n dmax)−1.
Proof. If ϵ = 0 there is nothing to prove. Assume that ϵ > 0. By Proposition 4 we derive that the
harmonic space is trivial and hence λF
0 > 0. Consider a unit eigenvector x ∈ker(∆F −λF
0 I) and let
v ∈V such that ||xv|| ≥||xu|| for u ̸= v. There exists a cycle γ based at v such that Pγ
vxv ̸= xv for
otherwise we could extend xv ̸= 0 to any other node independently of the path choice and hence ﬁnd
a non-trivial harmonic signal. In particular, we can assume this cycle to be non-degenerate, otherwise
if there existed a non-trivial degenerate loop contained in γ that does not ﬁx x we could consider this
loop instead of γ for our argument. Let us write this path as (v0 = v, v1, . . . , vL = v) and consider
the rescaled signal ˜xv
√
dv = xv. By assumption we have
ϵ||˜xv|| ≤||(Pγ
v→v −I)˜xv|| = ||(ρvL−1→vL · · · ρv0→v1 −I)˜xv||
= ||FvL−1ρvL−2→vL−1 · · · ρv0→v1 ˜xv −FvL=v˜xv||
= ||FvL−1ρvL−2→vL−1 · · · ρv0→v1 ˜xv −FvL−1 ˜xvL−1 + FvL−1 ˜xvL−1 −FvL=v˜xv||
≤||ρvL−2→vL−1 · · · ρv0→v1 ˜xv −˜xvL−1|| + ||FvL−1 ˜xvL−1 −FvL=v˜xv||.
15

By iterating the approach above we ﬁnd:
ϵ||˜xv|| ≤
L
X
i=0
||Fvi ˜xvi −Fvi+1 ˜xvi+1|| ≤
√
L
 L
X
i=0
||Fvi ˜xvi −Fvi+1 ˜xvi+1||2
! 1
2
=
√
L
 L
X
i=0
||Fvi
xvi
p
dvi
−Fvi+1
xvi+1
pdvi+1
||2
! 1
2
.
From Deﬁnition 14 we derive that the last term can be bounded from above by
p
2LEF(x) =
p
2L⟨x, ∆Fx⟩. Therefore, we conclude:
ϵ||xv||
√dv
≤
p
2L⟨x, ∆Fx⟩=
q
2LλF
0 ||x|| ≤2
q
diam(G)λF
0 .
By construction we get ||xv|| ≥1/√n, meaning that
λF
0 ≥
ϵ2
2diam(G)
1
n dmax
.
Lemma 6. Let F be a discrete O(d) bundle over a connected graph G. Then dim(H0) ≤d and
dim(H0) = d if and only if the transport is path-independent.
Proof. We ﬁrst note that the argument below extends to weighted O(d)-bundles as well. Let x ∈
H0(G, F). According to Proposition 4, given v, u ∈V , we see that xu = Pγ
v→uxv for any path
γv→u. It means that the harmonic space is uniquely determined by the choice of xv ∈F(v).
Explicitly, given any cycle γ based at v, we know that xv ∈ker(Pγ
v→v −I). If the transport
is everywhere path-independent, then the kernel coincides with the whole stalk F(v) and hence
we can extend any basis {xvi} ∈F(v) ∼= Rd to a basis in H0(G, F) via the transport maps, i.e.
dim(H0(G, F)) = d. If instead there exists a transport map over a cycle γv→v with non-trivial ﬁxed
points, then ker(Pγ
v→v −I) < F(v) ∼= Rd and hence dim(H0(G, F)) < d.
B
Proofs for the Power of Sheaf Diffusion
Deﬁnition 19. Let G = (V, W) be a weighted graph, where W is a matrix with wvu = wuv ≥0
for all v ̸= u ∈V , wvv = 0 for all v ∈V , and (v, u) is an edge if and only if wvu > 0.
The graph Laplacian of a weighted graph is L = D−W, where D is the diagonal matrix of weighted
degrees (i.e. dv = P
u wvu). Its normalised version is eL = D−1/2LD−1/2.
Proposition 20. Let G be a graph. The set {∆F | (G, F) ∈H1
sym} is isomorphic to the set of all
possible weighted graph Laplacians over G.
Proof. We prove only one direction. Let W be a choice of valid weight matrix for the graph
G. We can construct a sheaf (G, F) ∈H1
sym such that for all edges v, u ⊴e we have that
Fv⊴e = Fu⊴e = ±√wvu. Then, Lvu = −wvu and Lvv = P
e∥Fv⊴e∥2 = P
u wvu. The equality
for the normalised version of the Laplacians follows directly.
We state the following Lemma without proof based on Theorem 3.1 in Hansen and Ghrist [35].
Lemma 21. Solutions X(t) to the diffusion in Equation 3 converge as t →∞to the orthogonal
projection of X(0) onto ker(∆F).
Due to this Lemma, the proofs below rely entirely on the structure of ker(∆F) that one obtains for
certain (G, F).
Proposition 8. Let G be the set of connected graphs G = (V, E) with two classes A, B ⊂V such
that for each v ∈A, there exists u ∈A and an edge (v, u) ∈E. Then H1
sym has linear separation
power over G.
16

Proof. Let G = (V, E) be a graph with two classes A, B ⊂V such that for each v ∈A, there
exists u ∈A and an edge (v, u) ∈E. Additionally, let x(0) be any channel of the feature matrix
X(0) ∈Rn×f.
We can construct a sheaf (F, G) ∈H1
sym as follows. For all nodes v ∈V and edges e ∈E,
F(v) ∼= F(e) ∼= R. For all v, u ∈A and edge (u, v) ∈E, set Fv⊴e = Fu⊴e = √α > 0. Otherwise,
set Fv⊴e = 1.
Denote by hv the number of neighbours of node v in the same class as v . Note that based on the
assumptions, hv > 1 if v ∈A. Then the only harmonic eigenvector of ∆F is:
av =
p
dv + hv(α −1),
v ∈A
√dv,
v ∈B
(7)
Denote its unit-normalised version ea :=
a
∥a∥. In the limit of the diffusion process, the features
converge to h = ⟨x(0), ea⟩ea by Lemma 21. Assuming, x(0) /∈ker(∆F)⊥, which is nowhere dense
in Rn and, without loss of generality, that ⟨x(0), ea⟩> 0, for sufﬁciently large α, eav ≥eau for all
v ∈A, u ∈B.
Proposition 9. Let G be the set of connected bipartite graphs G = (A, B, E), with partitions A, B
forming two classes and |A| = |B|. Then H1
sym cannot linearly separate the classes of any graph in
G for any initial conditions X(0) ∈Rn×f.
Proof. Let G = (A, B, E) be a bipartite graph with |A| = |B| and let x(0) ∈Rn be any channel of
the feature matrix X(0) ∈Rn×f.
Consider an arbitrary sheaf (G, F) ∈H1
sym. Since the graph is connected, the only harmonic
eigenvector of ∆F is y ∈Rn with yv =
qP
v⊴e∥Fv⊴e∥2 (i.e. the square root of the weighted
degree). Based on Lemma 21, the diffusion process converges in the limit (up to a scaling) to ⟨x, y⟩y.
For the features to be linearly separable we require that ⟨x, y⟩̸= 0 and, without loss of generality,
for all v ∈A, u ∈B that yv < yu ⇔P
v⊴e∥Fv⊴e∥2 < P
u⊴e∥Fu⊴e∥2.
Suppose for the sake of contradiction there exists a sheaf in H1
sym with such a harmonic eigenvector.
Then, because |A| = |B|:
X
v∈A
X
v⊴e
∥Fv⊴e∥2 <
X
u∈B
X
u⊴e
∥Fu⊴e∥2 ⇔
X
v∈A
X
v⊴e
∥Fv⊴e∥2 −
X
u∈B
X
u⊴e
∥Fu⊴e∥2 < 0
⇔
X
e∈E
∥Fv⊴e∥2 −∥Fu⊴e∥2 < 0
However, because (F, G) ∈H1
sym, we have Fv⊴e = Fu⊴e and the sum above is zero.
Proposition 10. Let G contain all the connected graphs G = (V, E) with two classes A, B ⊆V .
Consider a sheaf (F; G) ∈H1 with Fv⊴e = −αe if v ∈A and Fu⊴e = αe if u ∈B with αe > 0
for all e ∈E. Then the diffusion induced by (F; G) can linearly separate the classes of G for almost
all initial conditions, and H1 has linear separation power over G.
Proof. Let G = (V, E) be a connected graph with two classes A, B ⊂V . Additionally, let x(0)
be any channel of the feature matrix X(0) ∈Rn×f. Any sheaf of the described type has a single
harmonic eigenvector by virtue of Lemma 6, and it has the form:
yv =



+
qP
v⊴e αe,
v ∈A
−
qP
v⊴e αe,
v ∈B
(8)
Assume x(0) /∈ker(∆F)⊥, which is nowhere dense in Rn and, without loss of generality, that
⟨x(0), y⟩> 0. Then, yv > 0 > yu for all v ∈A, u ∈B.
Next, we showed that using signed relations is necessary in d = 1 and simply using positive
asymmetric relations is not sufﬁcient in this dimension.
17

Deﬁnition 22. The class of sheaves over G with non-zero maps, one-dimensional stalks, and similarly
signed restriction maps H1
+ := {(F, G) | Fv⊴eFu⊴e > 0}
Proposition 23. Let G be the connected graph with two nodes belonging to two different classes.
Then H1
+ cannot linearly separate the two nodes for any initial conditions X ∈R2×f.
Proof. Let G be the connected graph with two nodes V = {v, u}. Then any sheaf (F, G) ∈H1
+(G)
has restriction maps of the form Fv⊴e = α, Fu⊴e = β and (without loss of generality) α, β > 0. As
before, the only (unnormalized) harmonic eigenvector for a sheaf of this form is y = (|α|β, α|β|) =
(αβ, αβ). Since this is a constant vector, the two nodes are not separable in the diffusion limit.
We state the following result without a proof (see Exercise 4.1 in Bishop [6]).
Lemma 24. Let A and B be two sets of points in Rn. If their convex hulls intersect, the two sets of
points cannot be linearly separable.
Proposition 11. Let G be a connected graph with C ≥3 classes. Then, H1 cannot linearly separate
the classes of G for any initial conditions X(0) ∈Rn×f.
Proof. If the sheaf has a trivial global section, all features converge to zero in the diffusion limit.
Suppose H0(G, F) is non-trivial. Since G is connected and all the restriction maps are invertible, by
Lemma 6, dim(H0) = 1.
In that case, let h be the unit-normalised harmonic eigenvector of ∆F. By Lemma 21, for any node
v, its scalar feature in channel k ≤f is given by xk
v(∞) = ⟨xk(0), h⟩hv. Note that we can always
ﬁnd three nodes v, u, w belonging to three different classes such that hv ≤hu ≤hv. Then, there
exists a convex combination hu = αhv + (1 −α)hw, with α ∈[0, 1]. Therefore:
xk
u(∞) = ⟨xk(0), h⟩hu = α⟨xk(0), h⟩hv + (1 −α)⟨xk(0), h⟩hw = αxk
v(∞) + (1 −α)xk
w(∞).
(9)
Since this is true for all channels k ≤f, it follows that xu(∞) = αxv(∞)+(1−α)xw(∞). Because
xu(∞) is in the convex hull of the points belonging to other classes, by Lemma 24, the class of v is
not linearly separable from the other classes.
Proposition 12. Let G be the set of connected graphs with nodes belonging to C ≥3 classes. Then
for d ≥C, Hd
diag has linear separation power over G.
Proof. Let G = (V, E) be a connected graph with C classes and (F, G), an arbitrary sheaf in
Hd
diag. Because F has diagonal restriction maps, there is no interaction during diffusion between the
different dimensions of the stalks. Therefore, the diffusion process can be written as d independent
diffusion processes, where the i-th process uses a sheaf Fi with all stalks isomorphic to R and
Fi
v⊴e = Fv⊴e(i, i) for all v ∈V and incident edges e. Therefore, we can construct d sheaves
Fi ∈H1(G) with i < d as in Proposition 10, where (in one vs all fashion) the two classes are given
by the nodes in class i and the nodes belonging to the other classes.
It remains to restrict that the projection of x(0) on any of the harmonic eigenvectors of ∆F in the
standard basis is non-zero. Formally, we require xi(0) /∈ker(∆Fi)⊥for all positive integers i ≤d.
Since ker(∆Fi)⊥is nowhere dense in Rn, x(0) belongs to the direct sum of dense subspaces, which
is dense.
Lemma 25. Let G = (V, E) be a graph and (F, G) a (weighted) orthogonal vector bundle over G
with path-independent parallel transport and edge weights αe. Consider an arbitrary node v∗∈V
and denote by ei the i-th standard basis vector of Rd. Then {h1, . . . , hd} form an orthogonal
eigenbasis for the harmonic space of ∆F, where:
hi
v =
(
ei
p
dF
v
Pv→wei
p
dF
v
=



ei
qP
v⊴e α2e,
v = v∗
Pv∗→wei
qP
v⊴e α2e,
otherwise
(10)
18

(a) Aligning the features of each
class with the axis of coordinates
in a 2D space. Dotted lines in-
dicate linear decision boundaries
for each class.
(b) Separating an arbitrary num-
ber of classes when the graph is
regular. Dotted line shows an ex-
ample decision boundary for one
of the classes.
Figure 6: Proof sketch for Lemma 27 and Proposition 28.
Proof. First, we show that hi
v is harmonic.
EF(hi
v) = 1
2
X
v,u,e:=(v,u)
∥
1
p
dF
v
Fv≤ehv −
1
p
dF
u
Fu⊴ehu∥2
2
(11)
= 1
2
X
v,u,e:=(v,u)
∥Fv≤ePv∗→vei −Fu⊴ePv∗→uei∥2
2
(12)
= 1
2
X
v,u,e:=(v,u)
∥Fv≤ePu→vPv∗→uei −Fu⊴ePv∗→uei∥2
2
By path independence
(13)
= 1
2
X
v,u,e:=(v,u)
∥Fv≤eF⊤
v⊴eFu⊴ePv∗→uei −Fu⊴ePv∗→uei∥2
2
By deﬁnition of Pu→v
(14)
= 1
2
X
v,u,e:=(v,u)
∥Fu⊴ePv∗→uei −Fu⊴ePv∗→uei∥2
2 = 0
Orthogonality of Fv⊴e
(15)
For orthogonality, notice that for any i, j ≤d and v ∈V , it holds that:
⟨hi
v, hj
v⟩= ⟨Pv∗→wei
q
dF
v , Pv∗→wej
q
dF
v ⟩=
q
dF
v
q
dF
v ⟨ei, ej⟩= 0
(16)
Lemma 26. Let R1, R2 be two 2D rotation matrices and e1, e2 the two standard basis vectors of
R2. Then ⟨R1e1, R2e2⟩= −⟨R1e2, R2e1⟩.
Proof. The angle between e1 and e2 is π
2 . Letting φ, θ be the positive rotation angles of the two
matrices, the ﬁrst inner product is equal to cos(π/2+(φ−θ)) while the second is cos(π/2−(φ−θ)).
The result follows from applying the trigonometric identity cos(π/2 + x) = −sin x.
We ﬁrst prove Theorem 13 in dimension two in the following lemma and then we will look at the
general case.
Lemma 27. Let G be the class of connected graphs with C ≤4 classes. Then, H2
orth(G) has linear
separation power over G.
Proof. Idea: We can use rotation matrices to align the harmonic features of the classes with the axis
of coordinates as in Figure 6a. Then, for each side of each axis, we can ﬁnd a separating hyperplane
separating each class from all the others.
19

Let G be a connected graph with C ≤4 classes. Denote by P the following set of rotation matrices
together with their signed-ﬂipped counterparts:
R1 =

1
0
0
1

,
R2 =

0
−1
1
0

(17)
and by C = {1, . . . , C} the set of all class labels. Then, ﬁx a node v∗∈V and construct an injective
map g : C →P assigning each class label one of the signed basis vectors such that g(c(v∗)) = R1,
where c(v∗) denotes the class of node v∗.
Then, we can construct a sheaf (G, F) ∈H2
orth(G) in terms of certain parallel transport maps along
each edge, that will depend on P. For all nodes v and edges e, F(v) ∼= F(e) ∼= R2. For each u ∈V ,
we set Pv∗→u = g(c(u)). Then for all v, u ∈V , set Pv→u = Pv∗→vP−1
u→v∗. It is easy to see that
the resulting parallel transport is path-independent because it depends purely on the classes of the
endpoints of the path.
Based on Lemma 25, the i-th eigenvector of ∆F is hi ∈R2×n with hi
u = Pv∗→uei
√du. Now we
will show that the projection of x(0) in this subspace will have a conﬁguration as in Figure 6a up to a
rotation.
Let u, w be two nodes belonging to two different classes. Denote by αi = ⟨x(0), hi⟩. Then the inner
product between the features of nodes u, w in the limit of the diffusion process is:
⟨Pv∗→u
X
i
αiei
p
du, Pv∗→w
X
j
αjej
p
dw⟩=
=
p
dudw
h X
i̸=j
αiαj⟨Pv∗→uei, Pv∗→wej⟩+
X
k
α2
k⟨Pv∗→uek, Pv∗→wek⟩
i
=
p
dudw
h X
i<j
αiαj

⟨Pv∗→uei, Pv∗→wej⟩+ ⟨Pv∗→uej, Pv∗→wei⟩

+
X
k
α2
k⟨Pv∗→uek, Pv∗→wek⟩
i
(18)
=
X
k
α2
k⟨Pv∗→uek, Pv∗→wek⟩
(by Lemma 26)
It can be checked that by substituting the transport maps Pv∗→u, Pv∗→w with any Ra, Rb from P
such that Ra ̸= ±Rb, the inner product above is zero. Similarly, substituting any Ra = −Rb, the
inner product is −√dudw
P
k α2
k = −√dudw∥x(0)∥2, which is equal to the product of the norms of
the two vectors. Therefore, the diffused features of different classes are positioned at π
2 , π, 3π
2 from
each other, as in Figure 6a.
Proposition 13. Let G be the class of connected graphs with C ≤2d classes. Then, for all d ∈{2, 4},
Hd
orth has linear separation power over G.
Proof. To generalise the proof in Lemma 27, we need to ﬁnd a set P of size d containing rotation
matrices that make the projected features of different classes be pairwise orthogonal for any projection
coefﬁcients α. For that, each term in Equation 18 must be zero for any coefﬁcients α.
Therefore, P = {P0, . . . , Pd−1} must satisfy the following requirements:
1. P0 = I ∈P, since transport for neighbours in the same class must be the identity. Therefore,
P0Pk = PkP0 = Pk for all k.
2. Since ⟨P0ei, Pkei⟩= 0 for all i and k ̸= 0, it follows that the diagonal elements of Pk are
zero.
3. From ⟨P0ei, Pkej⟩= −⟨P0ej, Pkei⟩for all i ̸= j, k ̸= 0 and point (2) it follows that
P−1
k
= P⊤
k = −Pk. Therefore, PkPk = −I for all k ̸= 0.
4. We have ⟨Pkei, Plei⟩= 0 for all i and k ̸= l. Together with (3), it follows that the diagonal
elements of PkPl are zero.
20

5. We have ⟨Pkei, Plej⟩= −⟨Pkej, Plei⟩for all i ̸= j, and k ̸= l, with k, l ̸= 0. Together
with point (4) it follows that (PkPl)⊤= −PkPl. Similarly, from point (3) we have
that (PkPl)⊤= P⊤
l P⊤
k = (−Pl)(−Pk) = PlPk. Therefore, the two matrices are
anti-commutative: PkPl = −PlPk.
We remark that points (1), (3), (5) coincide with the deﬁning algebraic properties of the algebra
of complex numbers, quaternions, octonions, sedenions and their generalisations based on the
Cayley-Dickson construction [59]. Therefore, the matrices in P must be a representation of one
of these algebras. Firstly, such algebras exist only for d that are powers of two. Secondly, matrix
representations for these algebras exist only in dimensions two and four. This is because the algebra
of octonions and their generalisations, unlike matrix multiplication, is non-associative. As a sanity
check, note that the matrices R1, R2 from Lemma 27 are a well-known representation of the unit
complex numbers.
We conclude this section by giving out the matrices for d = 4, which are the real matrix representations
of the four unit quaternions:
R1 =


1
0
0
0
0
1
0
0
0
0
1
0
0
0
0
1

,
R2 =


0
−1
0
0
1
0
0
0
0
0
0
−1
0
0
1
0

,
(19)
R3 =


0
0
−1
0
0
0
0
1
1
0
0
0
0
−1
0
0

,
R4 =


0
0
0
−1
0
0
−1
0
0
1
0
0
1
0
0
0

.
It can be checked that these matrices respect the properties outlined above. Thus, in d = 4, we can
select the transport maps from the set {±R1, ±R2, ±R3, ±R4} containing eight matrices, which
also form a group. Therefore, following the same procedure as in Lemma 27, we can linearly separate
up to eight classes.
Proposition 28. Let G be the class of connected regular graphs with a ﬁnite number of classes. Then,
H2
orth(G) has linear separation power over G.
Proof. Idea: Since the graph is regular, the harmonic features of the nodes will be uniformly scaled
and thus positioned on a circle. The aim is to place different classes at different locations on the
circle, which would make the classes linearly separable as shown in Figure 6b.
Let G be a regular graph with C classes and deﬁne θ = 2π
C . Denote by Ri the 2D rotation matrix:
Ri =

cos(iθ)
−sin(iθ)
sin(iθ)
cos(iθ)

(20)
Then let P = {Ri | 0 ≤i ≤C −1, i ∈N } the set of rotation matrices with an angle multiple of
θ. Then we can deﬁne a bijection g : C →P and a sheaf (G, F) ∈H2
orth(G) as in the proof above.
Checking the inner-products from Equation 18 between the harmonic features of the nodes, we can
verify that the angle between any two classes is different from zero. By Lemma 26, the cross terms of
the inner product vanish:
X
k
α2
k⟨Ri[k], Rj[k]⟩=
X
k
α2
k cos((i −j)θ) = cos((i −j)θ)∥x∥2
(21)
Thus, the angle between classes i, j is (i −j)θ.
C
Energy Flow Proofs
Proposition 29. If F is an O(d)-bundle in Hd
orth,sym, then x ∈ker∆F if and only if xk ∈ker ∆0
for all 1 ≤k ≤d.
21

Proof of Proposition 29. Let x ∈H0(G, F). Then we have
0 = EF(x) = 1
2
X
(v,u)∈E
||Fv⊴eD
−1
2
v
xv −Fu⊴eD
−1
2
u
xu||2
= 1
2
X
(v,u)∈E
||Fe

D
−1
2
v
xv −D
−1
2
u
xu

||2
= 1
2
X
(v,u)∈E
||d
−1
2
v
xv −d
−1
2
u xu||2.
The last term vanishes if and only if xk ∈ker ∆0 for each 1 ≤k ≤d.
Proposition 17. For any connected graph G and ε > 0, there exist a sheaf (G, F) /∈Hd
sym , W1
with ∥W1∥2 < ε and feature vector x such that EF((I ⊗W1)x) > EF(x).
Proof. Let F be an O(d)-bundle over G and ε > 0. Assume that Fv⊴e = Fu⊴e for each (u, v) ̸=
(u0, v0) and that F⊤
v0⊴eFu0⊴e −I := B ̸= 0 with dim(ker(B)) > 0. Then there exist a linear map
W ∈Rd×d with ||W||2 = ε and x ∈H0(G, F) such that EF((I⊗W)x) > 0. We sketch the proof.
Let g ∈ker(B). Deﬁne then x ∈C0(G, F) by
xv =
p
dvg.
Then x ∈H0(G, F). If we now take W = εPkerB⊥the rescaled orthogonal projection in the
orthogonal complement of the kernel of B we verify the given claim.
We provide below a proof for the equality in Deﬁnition 14.
Proposition 30.
x⊤∆Fx = 1
2
X
e:=(v,u)
∥Fv⊴eD−1/2
v
xv −Fu⊴eD−1/2
u
xu∥2
2
Proof. We prove the result for the normalised sheaf Laplacian, and other versions can be obtained as
particular cases.
E(x) = x⊤∆Fx =
X
v
x⊤
v ∆vvxv +
X
w̸=z
(w,z)∈E
x⊤
w∆wzxz
(22)
=
X
v⊴e
x⊤
v D−1/2
v
F⊤
v⊴eFv⊴eD−1/2
v
xv +
X
w<z
(w,z)∈E
x⊤
w∆wzxz + x⊤
z ∆zwxw
(23)
= 1
2
X
v,w⊴e

x⊤
v D−1/2
v
F⊤
v⊴eFv⊴eD−1/2
v
xv + x⊤
wD−1/2
w
F⊤
w⊴eFw⊴eD−1/2
w
xw
(24)
+ x⊤
v D−1/2
v
F⊤
v⊴eFw⊴eD−1/2
w
xw + x⊤
wD−1/2
w
F⊤
w⊴eFv⊴eD−1/2
v
xv

(25)
= 1
2
X
v,w⊴e
x⊤
v D−1/2
v
F⊤
v⊴e
 Fv⊴eD−1/2
v
xv −Fw⊴eD−1/2
w
xw

(26)
−x⊤
wD−1/2
w
F⊤
w⊴e
 Fv⊴eD−1/2
v
xv −Fw⊴eD−1/2
w
xw

(27)
= 1
2
X
v,w⊴e
 x⊤
v D−1/2
v
F⊤
v⊴e −x⊤
wD−1/2
w
F⊤
w⊴e
 Fv⊴eD−1/2
v
xv −Fw⊴eD−1/2
w
xw

(28)
Note that Dv is symmetric for any node v and so is any D−1/2
v
. Therefore, the two vectors in the
parenthesis are the transpose of each other and the result is their inner product. Thus, we have:
EF(x) = 1
2
X
v,w⊴e
∥Fv⊴eD−1/2
v
xv −Fw⊴eD−1/2
w
xw∥2
2
(29)
22

The result follows identically for other types of Laplacian. For the augmented normalized Laplacian,
one should simply replace D with ˜D = D + I and for the non-normalised Laplacian, one should
simply remove D from the equation.
Theorem 16. If (F, G) ∈Hd
orth,sym and σ = (Leaky)ReLU, EF(Y) ≤λ∗∥W1∥2
2∥W⊤
2 ∥2
2EF(X).
Proof. We ﬁrst prove a couple of Lemmas before proving the Theorem. The proof structure follows
that of Cai and Wang [13], which in turn generalises that of Oono and Suzuki [51]. The latter proof
technique is not directly applicable to our setting because it makes some strong assumptions about
the harmonic space of the Laplacian (i.e. that the eigenvectors of the harmonic space have positive
entries).
λ∗= max
 (λmin −1)2, (λmax −1)2
, where λmin, λmax are the smallest and largest non-zero
eigenvalues of ∆F.
Lemma 31. For P = I −∆F, EF(Px) < λ∗EF(x).
Proof. We can write x = P
i cihi as a sum of the eigenvectors {hi} of ∆F. Then x⊤∆Fx =
P
i c2
i λi, where {λi} are the eigenvalues of ∆F.
EF(Px) = x⊤P⊤∆FPx = x⊤P∆FPx =
X
i
c2
i λi(1 −λi)2 ≤λ∗
X
i
c2
i λi = λ∗EF(x) (30)
The inequality follows from the fact that the eigenvectors of the normalised sheaf Laplacian are in
the range [0, 2] [34, Proposition 5.5]. We note that the original proof of Cai and Wang [13] bounds
the expression by (1 −λmin)2 instead of λ∗, which appears to be an error.
Lemma 32. EF(XW) ≤∥W⊤∥2
2EF(X)
Proof. Following the proof of Cai and Wang [13] we have:
EF(XW) = Tr(W⊤X⊤∆FXW)
(31)
= Tr(X⊤∆FXWW⊤)
trace cyclic property
(32)
≤Tr(X⊤∆FX)∥WW⊤∥2
see Lemma 3.1 in Lee [40]
(33)
= Tr(X⊤∆FX)∥W⊤∥2
2
(34)
Lemma 33. For conditions as in Theorem 16, EF
 (In ⊗W)x

≤∥W∥2
2EF(x).
Proof. First, we note that for orthogonal matrices, Dv = I P
v⊴e α2
e = Idv [34, Lemma 4.4]
EF
 (I ⊗W)x

= 1
2
X
v,w⊴e
∥Fv⊴eD−1/2
v
Wfv −Fw⊴eD−1/2
w
Wxw∥2
2
(35)
= 1
2
X
v,w⊴e
∥FeW
 d−1/2
v
xv −d−1/2
w
xw

∥2
2
(36)
= 1
2
X
v,w⊴e
∥W
 d−1/2
v
xv −d−1/2
w
xw

∥2
2
Fe is orthogonal
(37)
≤1
2
X
v,w⊴e
∥W∥2
2∥d−1/2
v
xv −d−1/2
w
xw∥2
2
(38)
= 1
2
X
v,w⊴e
∥W∥2
2∥Fe
 d−1/2
v
xv −d−1/2
w
xw

∥2
2
Fe is orthogonal
(39)
= 1
2∥W∥2
2
X
v,w⊴e
∥Fe
 D−1/2
v
xv −D−1/2
w
xw

∥2
2
(40)
= ∥W∥2
2EF(x)
(41)
23

The proof can also be extended easily extended to vector bundles over weighted graphs (i.e. allowing
weighted edges as in Hansen and Ghrist [34]). For the non-normalised Laplacian, the assumption
that Fe is orthogonal can be relaxed to being non-singular and then the upper bound will also depend
on the maximum conditioning number over all Fe.
Lemma 34. For conditions as in Theorem 16, EF
 σ(x)

≤EF(x).
Proof.
E
 σ(x)

= 1
2
X
v,w⊴e
∥Fv⊴eD−1/2
v
σ(xv) −Fw⊴eD−1/2
w
σ(xw)∥2
2
(42)
= 1
2
X
v,w⊴e
∥Fe
 d−1/2
v
σ(xv) −d−1/2
w
σ(xw)

∥2
2
(43)
= 1
2
X
v,w⊴e
∥d−1/2
v
σ(xv) −d−1/2
w
σ(xw)∥2
2
orthogonality of Fe
(44)
= 1
2
X
v,w⊴e
∥σ
 xv
√dv

−σ
 xw
√dw

∥2
2
cReLU(x) = ReLU(cx), c > 0
(45)
≤1
2
X
v,w⊴e
∥xv
√dv
−xw
√dw
∥2
2
Lipschitz continuity of ReLU
(46)
= 1
2
X
v,w⊴e
∥Fe
 d−1/2
v
xv −d−1/2
w
xw

∥2
2
orthogonality of Fe
(47)
= EF(x)
(48)
Combining these three lemmas for an entire diffusion layer proves the Theorem.
Theorem 15. For (F, G) ∈H1
+ and σ being (Leaky)ReLU, EF(Y) ≤λ∗∥W1∥2
2∥W⊤
2 ∥2
2EF(X).
Proof. If d = 1, then Lemma 33 becomes superﬂuous as W1 becomes a scalar that can be absorbed
into the right-weights. It remains to verify that a version of Lemma 34 holds in this case.
Lemma 35. For conditions as in Theorem 15, EF
 σ(x)

≤EF(x).
Proof.
E
 σ(x)

= 1
2
X
v,w⊴e
∥Fv⊴eD−1/2
v
σ(xv) −Fw⊴eD−1/2
w
σ(xw)∥2
2
(49)
= 1
2
X
v,w⊴e
∥|Fv⊴e|D−1/2
v
σ(xv) −|Fw⊴e|D−1/2
w
σ(xw)∥2
2
Fv⊴eFw⊴e > 0
(50)
= 1
2
X
v,w⊴e
∥σ
|Fv⊴e|xv
√dv

−σ
|Fw⊴e|xw
√dw

∥2
2
cσ(x) = σ(cx), c > 0 (51)
≤1
2
X
v,w⊴e
∥|Fv⊴e|xv
√dv
−|Fw⊴e|xw
√dw
∥2
2
ReLU Lipschitz cont. (52)
= 1
2
X
v,w⊴e
∥Fv⊴eD−1/2
v
xv −Fw⊴eD−1/2
w
xw∥2
2
Fv⊴eFw⊴e > 0
(53)
= EF(x)
(54)
24

We note that if Fv⊴eFw⊴e < 0 (i.e. the relation is signed), then it is very easy to ﬁnd counter-
examples where ReLU does not work anymore. However, the result still holds in the deep linear
case.
If the features of an SCN/GCN oversmoothing as in Theorem 15 converge to ker(∆F), then the model
will no longer be able to linearly separate the classes. This is shown by the following Corollaries.
Corollary 36. Consider an SCN model f with k layers and a sheaf (F; G) ∈H1
sym over a bipartite
graph G as in Proposition 9. Then for any ﬁnite k, Y := f(X) is not linearly separable for any input
with EF(X) = 0.
Proof. By Theorem 15, if EF(X) = 0, then EF(Y) = 0 and, therefore, Yi ∈ker(∆F) for any
column i. The proof of Proposition 9 showed that the classes of such a bipartite graph cannot be
linearly separated for any such feature matrix Y.
Corollary 37. Consider an SCN model f with k layers and a sheaf (F; G) ∈H1
+ over any graph G
with more than two classes as in Proposition 11. Then for any ﬁnite k, Y := f(X) is not linearly
separable for any input with EF(X) = 0.
Proof. By Theorem 15, if EF(X) = 0, then EF(Y) = 0 and, therefore, Yi ∈ker(∆F) for any
column i. The proof of Proposition 11 showed that the classes of such a graph cannot be linearly
separated for any such feature matrix Y.
D
Sheaf Learning Proof
Proposition 18. Let G = (V, E) be a ﬁnite graph with features X. Then, if (xv, xu) ̸= (xw, xz) for
any (v, u) ̸= (w, z) ∈E and Φ is an MLP with sufﬁcient capacity, Φ can learn any sheaf (F; G).
Proof. Assume that the node features are k-dimensional and, therefore, the graph feature matrix
has shape X ∈Rn×k. Deﬁne the ﬁnite set A := {(xv, xu) : v →u ∈E} ⊂R2k containing the
concatenated features of the nodes for all the oriented edges v →u of the graph. Then, because
each (xv, xu) is unique, for any dimension d, there exists a (well-deﬁned) function g : A →Rd×d
sending (xv, xu) 7→Fv⊴e=(v,u). We now show that this function can be extended to a smooth
function f : R2k →Rd×d and, therefore, it can be approximated by an MLP due to the Universal
Approximation Theorem [36, 37].
Let I be an index set for the elements of A. Then, because A is ﬁnite, for any ai∈I, we can ﬁnd
a sufﬁciently small neighbourhood Ui ⊂R2k such that ai ∈Ui and aj /∈Ui for j ̸= i ∈I.
Furthermore, for each i ∈I, we can ﬁnd a (smooth) bump function ψi : R2k →R such that
ψi(ai) = 1 and ψi(a) = 0 if a /∈Ui. Then, the function f(a) := P
i∈I g(ai)ψi(a) is smooth and
f|A = g.
E
Additional model details and hyperparameters
Hybrid transport maps.
Consider the transport maps −F⊤
v⊴eFu⊴e appearing in the off-diagonal
entires of the sheaf Laplacian LF. When learning a sheaf Laplacian, there exists the risk that the
features are not sufﬁciently good in the early layers (or in general) and, therefore, it might be useful
to consider a hybrid transport map of the form −F⊤
v⊴eFu⊴e
L F, where L is the direct sum of
two matrices and F represents a ﬁxed (non-learnable map). In particular, we consider maps of the
form −F⊤
v⊴eFu⊴e
L I1
L −I1 which essentially appends a diagonal matrix with 1 and −1 on the
diagonal to the learned matrix. From a signal processing perspective, these correspond to a low-pass
and a high-pass ﬁlter that could produce generally useful features. We treat the addition of these ﬁxed
parts as an additional hyper-parameter.
25

Adjusting the activation magnitudes.
We note that in practice we ﬁnd it useful to learn an
additional parameter ε ∈[−1, 1]d (i.e. a vector of size d) in the discrete version of the models:
Xt+1 = (1 + ε)Xt −σ

∆F(t)(I ⊗Wt
1)XtWt
2

.
(55)
This allows the model to adjust the relative magnitude of the features in each stalk dimension. This is
used across all of our experiments in the discrete models.
Augmented normalised sheaf Laplacian.
Similarly to GCN which normalises the Laplacian by
the augmented degrees (i.e. (D + In)−1/2, where D is the usual diagonal matrix of node degrees),
we similarly use (D + Ind)−1/2 for normalisation to obtain greater numerical stability. This is
particularly helpful when learning general sheaves as it increases the numerical stability of SVD.
Table 2: Hyper-parameter ranges for the discrete and continous models.
Discrete Models
Continous Models
Hidden channels
(8, 16, 32) (WebKB) and (8, 16, 32, 64) (others)
(8, 16, 32, 64)
Stalk dim d
1 −5
1 −5
Layers
2 −8
N/A
Learning rate
0.02 (WebKB) and 0.01 (others)
Log-uniform [0.01, 0.1]
Activation
ELU
ELU
Weight decay (regular parameters)
Log-uniform [−4.5, 11.0]
Log-uniform [−6.9, 13.8]
Weight decay (sheaf parameters)
Log-uniform [−4.5, 11.0]
Log-uniform [−6.9, 13.8]
Input dropout
Uniform [0, 0.9]
Uniform [0, 0.9]
Layer dropout
Uniform [0, 0.9]
N/A
Patience (epochs)
100 (Wiki) and 200 (others)
50
Max training epochs
1000 (Wiki) and 500 (others)
50.
Integration time
N/A
Uniform [1.0, 9.0].
Optimiser
Adam [38]
Adam
Hyperparameters and training procedure.
We train all models for a ﬁxed maximum number of
epochs and perform early stopping when the validation metric has not improved for a pre-speciﬁed
number of patience epochs. We report the results at the epoch where the best validation metric
was obtained for the model conﬁguration with the best validation score among all models. We use
the hyperparameter optimisation tools provided by Weights and Biases [5] for this procedure. The
complete hyperparameter ranges we optimised over can be found in Table 2. All models were trained
and ﬁne-tuned on an Amazon AWS p2.xlarge machine containing 8 NVIDIA K80 GPUs and using a
2.3 GHz (base) and 2.7 GHz (turbo) Intel Xeon E5-2686 v4 Processor.
E.1
Computational Complexity
We can split the computational complexity into the following computational steps:
1. The linear transformation X′ = (I ⊗Wt
1)XtWt
2.
W1 is a d × d matrix and W2 is
an f × f matrix. Therefore, the complexity is O
 n(d2f + df 2)

= O (n(cd + cf)) =
O(nc2).
2. Message Passing. Since ∆F is a sparse matrix, the message passing is implemented as
a sparse-dense matrix multiplication ∆FX′. When the restriction maps are diagonal, the
complexity of this operation is O (mc), since the multiplication of each block matrix in
∆F and block vector in X′ reduces to a an element-wise vector multiplication. When the
restriction maps are non-diagonal, the complexity is O (mdc) because each matrix-vector
multiplication is O(d2) and we need to perform f of them for each node and edge.
3. Learning the Sheaf.
Assume we learn the restriction maps via Φ(xv, xu)
=
σ(V[vec(Xv)||vec(Xu)]), where vec(·) converts the d × f matrix into a df-sized vec-
tor. This operation has to be performed for each incident node-edge pair. Therefore, the
complexity is O(md2f) = O(mcd), when learning diagonal maps since V is a d × 2df
matrix. When learning a non-diagonal matrix, the number of rows of V is O(d2) and the
complexity becomes O(md3f) = O(mcd2). Note, however, that in general, the complexity
of learning the restriction maps can be signiﬁcantly reduced to O(mc) (in the diagonal case)
and O(m(c + d2)) (in the non-diagonal case) by, for instance, using an MLP with constant
hidden-size.
26

Figure 7: (Left) Train accuracy as a function of diffusion time. (Middle) Test accuracy as a function
of diffusion time. (Right) Histogram of the learned rotation angle of the 2D transport maps. The
performance of the bundle model is superior to that of the one-dimensional sheaf. The transport maps
learned by the model are aligned with our expectation: the model learns to rotate more (i.e. to move
away) the neighbours belonging to different classes than the neighbours belonging to the same class.
4. Constructing the Laplacian. To build the Laplacian, we need to perform the matrix-matrix
multiplications involved in computing each of the blocks. The complexity of that is O(md)
in the diagonal case and O(md3) in the non-diagonal case. Computing the normalisation of
the Laplacian is O(nd) in the diagonal case and O(nd3) in the non-diagonal case.
Putting everything together, the ﬁnal complexity is O(nc2 + mcd) in the diagonal case and
O
 n(c2 + d3) + m(cd2 + d3)

in the non-diagonal case.
When learning the sheaf via an
MLP with constant hidden size, the complexity reduces to O(nc2 + mc) (same as GCN) and
O
 n(c2 + d3) + m(c + d3)

, respectively.
For learning orthogonal matrices, we rely on the library Torch Householder [49] which provides
support for fast transformations with large batch sizes.
F
Additional Experiments
In this section, we provide a series of additional experiments and ablation studies.
Two-dimensional synthetic experiment.
In the main text we focused on a synthetic example
involving sheaves with one-dimensional stalks. We now consider a graph with three classes and
two-dimensional features, with edge homophily level 0.2. We use 80% of the nodes for training
and 20% for testing. First, we know that a discrete vector bundle with two-dimensional stalks that
can solve the task in the limit exists from Theorem 13, while based on Proposition 11 no sheaf with
one-dimensional stalks can perfectly solve the tasks.
Therefore, similarly to the synthetic experiment in the main text, we compare two similar models
learning the sheaf from data: one using 1D stalks and another using 2D stalks. As we see from
Figure 7, the discrete vector bundle model has better training and test-time performance than the
one-dimensional counterpart. Nonetheless, none of the two models manages to match the perfect
performance of the ideal sheaf on this more challenging dataset. From the ﬁnal subﬁgure, we also see
that the model learns to rotate more across the heterophilic edges in order to push away the nodes
belonging to other classes. The prevalent angle of this rotation is 2 radians, which is just under
120◦= 360◦/C, where C = 3 is the number of classes. Thus the model learns to position the three
classes at approximately equal arc lengths from each other for maximum linear separability.
Continous Models.
To also understand how the continuous version of our models performs against
other PDE-based GNNs we include a category of such SOTA models: CGNNs [70], GRAND [15],
and BLEND [14]. Results are included in Table 3. Generally, continuous models do not perform
as well as the discrete ones because they are constrained to use the same set of weights for the
entire integration time and cannot use dropout. Therefore, the model capacity is difﬁcult to increase
without overﬁtting. Nonetheless, our continuous models generally outperform other state-of-the-art
continuous models, which also share the same limitations. Finally, we note that the baselines were
ﬁne-tuned over the same hyper-parameter ranges as in Table 2
27

Table 3: Results on node classiﬁcation datasets sorted by their homophily level. Top three models are
coloured by First, Second, Third. Our models are marked NSD.
Texas
Wisconsin
Film
Squirrel
Chameleon
Cornell
Citeseer
Pubmed
Cora
Hom level
0.11
0.21
0.22
0.22
0.23
0.30
0.74
0.80
0.81
#Nodes
183
251
7,600
5,201
2,277
183
3,327
18,717
2,708
#Edges
295
466
26,752
198,493
31,421
280
4,676
44,327
5,278
#Classes
5
5
5
5
5
5
7
3
6
Cont Diag-NSD
82.97±4.37
86.47±2.55
36.85±1.21
38.17±9.29
62.06±3.84
80.00±6.07
76.56±1.19
89.47±0.42
86.88±1.21
Cont O(d)-NSD
82.43±5.95
84.50±4.34
36.39±1.37
40.40±2.01
63.18±1.69
72.16±10.40
75.19±1.67
89.12±0.30
86.70±1.24
Cont Gen-NSD
83.78±6.62
85.29±3.31
37.28±0.74
52.57±2.76
66.40±2.28
84.60±4.69
77.54±1.72
89.67±0.40
87.45±0.99
BLEND
83.24±4.65
84.12±3.56
35.63±0.89
43.06±1.39
60.11±2.09
85.95±6.82
76.63±1.60
89.24±0.42
88.09±1.22
GRAND
75.68±7.25
79.41±3.64
35.62±1.01
40.05±1.50
54.67±2.54
82.16±7.09
76.46±1.77
89.02±0.51
87.36±0.96
CGNN
71.35±4.05
74.31±7.26
35.95±0.86
29.24±1.09
46.89±1.66
66.22±7.69
76.91±1.81
87.70±0.49
87.10±1.35
Positional encoding ablation.
Based on Proposition 18 we proceed to analyse the impact of
increasing the expressive power of the model by making the nodes more distinguishable. For that,
we equip our datasets with additional features consisting of graph Laplacian positional encodings
as originally done in Dwivedi et al. [23]. In Table 4 we see that positional encodings do indeed
improve the performance of the continuous models compared to the numbers reported in the main
table. Therefore, we conclude that the interaction between the problem of sheaf learning and that of
the expressivity of graph neural networks represents a promising avenue for future research.
Table 4: Ablation study for positional encodings. Positional encodings improve performance on some
of our models.
Eigenvectors
Texas
Wisconsin
Cornell
Cont Diag-SD
0
82.97 ± 4.37
86.47 ± 2.55
80.00 ± 6.07
2
3.51 ± 5.05
85.69 ± 3.73
81.62 ± 8.00
8
85.41 ± 5.82
86.28 ± 3.40
82.16 ± 5.57
16
82.70 ± 3.86
85.88 ± 2.75
81.08 ± 7.25
Cont O(d)-SD
0
82.43 ± 5.95
84.50 ± 4.34
72.16 ± 10.40
2
84.05 ± 5.85
85.88 ± 4.62
83.51 ± 9.70
8
84.87 ± 4.71
86.86 ± 3.83
84.05 ± 5.85
16
83.78 ± 6.16
85.88 ± 2.88
83.51 ± 6.22
Cont Gen-SD
0
83.78 ± 6.62
85.29 ± 3.31
84.60 ± 4.69
2
83.24 ± 4.32
84.12 ± 3.97
81.08 ± 7.35
8
82.70 ± 5.70
84.71 ± 3.80
83.24 ± 6.82
16
82.16 ± 6.19
86.47 ± 3.09
82.16 ± 6.07
Visualising diffusion.
To develop a better intuition of the limiting behaviour of sheaf diffusion for
node classiﬁcation tasks we plot the diffusion process using an oracle discrete vector bundle for two
graphs with C = 3 (Figure 9) and C = 4 (Figure 8) classes. The diffusion processes converge in the
limit to a conﬁguration where the classes are rotated at 2π
C from each other, just like in the cartoon
diagrams of Figure 6. Note that in all cases, the classes are linearly separable in the limit.
We note that this approach generalises to any number of classes, but beyond C = 4 it is not guaranteed
that they will be linearly separable in 2D. However, they are still well separated. We include an
example with C = 10 classes in Figure 10.
Figure 8: Sheaf diffusion process disentangling the C = 4 classes over time. The nodes are coloured
by their class.
28

Figure 9: Sheaf diffusion process disentangling the C = 3 classes over time. The nodes are coloured
by their class.
Figure 10: Sheaf diffusion process disentangling the C = 10 classes over time. The nodes are
coloured by their class.
29

