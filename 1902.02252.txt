REVIEW
Recent Advances in Maximum Entropy Biasing Techniques for
Molecular Dynamics
D. Amirkulovaa and A. D Whitea
a University of Rochester, Department of Chemical Engineering, Rochester, NY, USA
ARTICLE HISTORY
Compiled June 29, 2021
ABSTRACT
This review describes recent advances by the authors and others on the topic of
incorporating experimental data into molecular simulations through maximum en-
tropy methods. Methods which incorporate experimental data improve accuracy in
molecular simulation by minimally modifying the thermodynamic ensemble. This
is especially important where force ﬁelds are approximate, such as when employing
coarse-grain models, or where high accuracy is required, such as when attempting to
mimic a multiscale self-assembly process. The authors review here the experiment
directed simulation (EDS) and experiment directed metadynamics (EDM) methods
that allow matching averages and distributions in simulations, respectively. Impor-
tant system-speciﬁc considerations are discussed such as using enhanced sampling
simultaneously, the role of pressure, treating uncertainty, and implementations of
these methods. Recent examples of EDS and EDM are reviewed including applica-
tions to ab initio molecular dynamics of water, incorporating environmental ﬂuctu-
ations inside of a macromolecular protein complex, improving RNA force ﬁelds, and
the combination of enhanced sampling with minimal biasing to model peptides.
KEYWORDS
molecular dynamics; maximum entropy
1. Introduction
A common task in molecular simulation is ensuring that observables of the simula-
tion match experimentally measured values. For example, in simulations of protein
structure[1] or liquids[2], quantitative agreement with experiments is the standard for
assessing correctness of a model. When there is no quantitative agreement, changing
the potential energy function or adding additional components to the simulation are
possible ways to improve the ﬁt. Making such changes can be an ambiguous and chal-
lenging process, especially if the potential energy function has multiple terms that
can be modiﬁed. Minimal biasing techniques are a class of methods that modify a
potential energy function to improve quantitative agreement with experimental values
while minimizing the the change in the potential energy function. The deﬁnition of
“minimal” and the way the potential energy function is modiﬁed vary from method
to method.
Recent reviews of minimal biasing methods can be found in Sormanni et al. [3],
Bonomi et al. [4], and Olsson et al. [5]. Table 1 in Bonomi et al. [4] provides an
CONTACT A. D. White. Email: andrew.white@rochester.edu
arXiv:1902.02252v1  [physics.chem-ph]  6 Feb 2019

overview of 28 minimal biasing methods, categorizing them by whether they maxi-
mize entropy, maximize parsimony, or use Bayesian inference. These three categories
correspond broadly to the criteria used to ensure that the biasing function introduces
a minimal change to the potential energy function. This review focuses on two tech-
niques developed by the authors that are categorized as maximum entropy methods:
experiment directed simulation[6] (EDS) and experiment directed metadynamics[7]
(EDM). EDS is for matching ensemble average scalars and EDM is for matching free
energy surfaces (probability distributions of observables).
EDS, like other minimal biasing methods, modiﬁes a potential energy function to
change ensemble averages of observables to match a speciﬁc value. These observables
are typically equivalent to collective variables, but are intended to be only those that
area experimentally veriﬁable quantities. What separates EDS from other methods is
that it does not use replicas and can be used to construct a continuous NVE trajectory.
For example, the Bayesian landscape tilting method of Beauchamp et al. [8] relies on
post-processing so that there cannot be a continuous NVE trajectory. Another example
is the replica method of Lindorﬀ-Larsen et al. [9] which relies on replica-exchange of
biasing forces and thus cannot result in a continuous trajectory. This does not mean
EDS is “better”; indeed these two methods appear to provide better sampling and
scaling than EDS. However, the ability to compute an NVE trajectory allows dynamic
observables like hydrogen-bonding lifetimes to be computed. The key results from EDS
have been to improve thermodynamic observables and indirectly improve dynamic
observables. For example, EDS was recently used to create a state-of-the-art DFT
water model that gives near perfect agreement with X-ray scattering results, water
diﬀusivity, and proton-hopping behavior by improving only the water oxygen-oxygen
coordination number[10].
EDM is a maximum entropy method that matches ensemble probability distribu-
tions, or equivalently free energy surfaces, using prescribed functions. So far, EDM has
proved most useful for matching radial distribution functions (RDFs), e.g. to then do
coarse-grained modeling. EDM is less often used because it is rare to have experimen-
tal data that gives a probability distribution, other than a normal distribution (which
is better treated with methods like in Hummer and K¨oﬁnger [11]). Other groups have
since arrived at the same approach as EDM and typically it is now called “targeted
metadynamics” because it uses the method of metadynamics to arrive at a target
free energy surface[12,13]. EDM is also equivalent to variationally enhanced sampling
metadynamics[14], although there the target is a tool to improve sampling, and the bi-
ased simulation is not the goal. The name EDM was chosen by the authors because at
the time there were new metadynamics methods that could target a collective variable
domain[15]. Now the term “targeted” seems to apply exclusively to target distribu-
tions, and EDM is probably best described as falling under the umbrella of targeted
metadynamics methods.
Here we review the maximum entropy derivations that underpin both EDS and
EDM, describe some of the recent research beneﬁting from these methods, and discuss
implementation details such as understanding the eﬀect of EDS and EDM on the virial
and accounting for uncertainty in the experimental data.
2. Theory
EDS minimally modiﬁes an ensemble so that the ensemble average of some scalar
matches a desired value, such as a value obtained from an experiment. The EDS bias
2

•
P
′(⃗r)
•
P0(⃗r)
∆Srel
P(⃗r)
∈⟨s(⃗r)⟩= ˆs
Figure 1.
A schematic of the minimum relative entropy derivation. P(⃗r) is the unbiased probability distribu-
tion from the unbiased potential energy. We are ﬁnding a biased probability distribution, P⃗r), that is consistent
with Equation 2. There is a hypersurface of possible such probability distributions. With the condition that we
minimimize relative entropy, or “distance” in this schematic, we ﬁnd a unique point in the hypersurface P ′(⃗r).
is minimal because the resulting biased ensemble maximizes entropy[16,17]. Maximum
entropy and minimum relative entropy are equivalent approaches to derive these mini-
mal biasing equations[18]. We will use the minimum relative entropy approach because
it has an intuitive interpretation as a distance metric. Figure 1 illustrates how the bi-
ased ensemble is one of many choices that matches our constraints, but is as close as
possible to the unbiased ensemble as measured via relative entropy.
Consider an unbiased potential energy function, U(⃗r), which has a probability dis-
tribution of P(⃗r) under the NVT ensemble, following the Boltzmann distribution:
P(⃗r) ∝exp (−βU(⃗r)). ⃗r is the set of coordinate vectors of the N particles of the
ensemble and β =
1
kT . k is the Boltzmann constant and T is absolute temperature.
We would like to ﬁnd a biased ensemble P ′(⃗r) which is as similar as possible to the
unbiased ensemble. Similarity can be deﬁned via the relative entropy:
∆Srel =
Z
d⃗r P(⃗r) ln P(⃗r)
P ′(⃗r)
(1)
where P ′(⃗r) is the biased ensemble probability distribution, and the integral is taken
over all coordinates. Having a lower ∆Srel means that the biased and unbiased ensem-
bles are more similar. The biased ensemble should also have an observable average that
matches a target value, which could be obtained from an experiment. This constraint
is represented as
⟨s(⃗r)⟩=
Z
d⃗r P ′(⃗r)s(⃗r) = ˆs
(2)
where s(⃗r) is an instantaneous value for our observable (collective variable) which we
are matching to ˆs, the desired scalar value. This is sometimes called a forward model
and depends only on positions. We will relax this assumption below.
These equations represent (1) a constraint (to have our average simulation value
match the target ˆs), and (2) a scalar to minimize (the relative entropy). When these
conditions are present, we can ﬁnd the optimal value via the method of Lagrange
multipliers. The Lagrangian is:
L

λ, P(⃗r), P ′(⃗r)

= ∆Srel −λ
Z
d⃗r P ′(⃗r)s(⃗r) −ˆs

(3)
3

Equation 3 can be minimized by taking the functional derivative equal to zero:
δL
δP ′(⃗r) = 0. Solving for P ′(⃗r) gives this expression for the biased ensemble:
P ′(⃗r) = 1
Z′ e−β(U(⃗r)+λs(⃗r))
(4)
where Z′ is a normalization constant. This gives an expression for the biased potential
energy as U′(⃗r) = U(⃗r) + λs(⃗r). This result shows that if the bias is linear in the
instantaneous observable, the ensemble is minimally biased. This derivation can be
repeated for multiple dimensions[16] and for functions instead of scalars[7]. The general
result is:
U′(⃗r) = U(⃗r) +
X
i
λisi(⃗r) +
X
j
µj [v(⃗r)]
(5)
where i is the index of ensemble averages that are matched to a desired value and j
is the index of free energy surfaces that are matched to desired functions. µj [v(⃗r)] is
a bias added that depends on v(⃗r), another instantaneous observable, and causes the
biased ensemble to match a speciﬁc distribution in the free energy surfaces Ft [v(⃗r)].
Speciﬁcally,
Z
d⃗rδ(v(⃗r) −v′)P ′(⃗r) = q(v′)
(6)
where δ is the Dirac delta function and q(v′) is a desired probability distribution
for v(⃗r) (i.e., q[v(⃗r)] = −1
β ln Ft [v(⃗r)]). q(v) could be obtained for example from a
scattering or FRET experiment[19].
The derivation above gives the form of the biased potential energy that is minimally
biased, but it does not enable calculation of the Lagrange multipliers. That is the
purpose of the EDS and EDM methods. EDS and EDM are time-dependent methods
that change the potential energy of a simulation to arrive at the bias in Equation 5.
EDS is intended to be used in a two-step process: ﬁnding the Lagrange multiplier
(adaptive) and then running a standard MD simulation using the modiﬁed force ﬁeld
given by Equation 5.
During the adaptive phase of EDS, the Lagrange multipliers are called coupling
constants to distinguish from the time-independent Lagrange multipliers. These are
indicated as ατ where τ is a discrete step index. The Lagrange multipliers are set to
be the average of ατ. ατ is deﬁned as
ατ+1 = ατ + ητgτ
(7)
gτ = 2β
w (⟨s⟩τ −ˆs)

s2
τ −⟨s⟩2
τ

(8)
where w is an arbitrary constant used to ensure unit homogeneity, ⟨·⟩τ is the ensemble
average between step τ −1 and τ, and ηt is
ητ =
A
pPτ
i g2
i
(9)
4

where A is a user-deﬁned constant that controls the size of the ﬁrst step. The point
of ητ is to reduce the size of the steps over time. Note that because gτ is in the sum,
|α0| = A. Typically the gradients should be clipped for stability and A is a natural
upper/lower bound for clipping it. Thus a user of this method must choose A and
the time between updates. This update procedure is derived in White and Voth [6]
and is based on per-coordinate inﬁnite horizon stochastic gradient descent[20]. Hocky
et al. [21] recently found that Equation 8 can be modiﬁed to use covariance in multiple
dimensions to improve convergenece and that replacing Equation 7 with Levenberg–
Marquardt optimization further improves convergence.
The EDM method ﬁnds the µ [v(⃗r)] bias function in Equation 5. For compactness,
we will now omit the dependence on ⃗r. Unlike EDS, EDM consists of a single phase
where µ(v) changes less over time. The update equation is
µ(v)τ+1 = µ(v)τ +
1
q(vτ) exp(−θ(vτ, v)τ)G(v, vτ)
(10)
where vτ is the value of v(⃗r) at time τ, G is a kernel function (e.g., Gaussian), q(vτ)
is the probability at position vτ from the target probability distribution and θ(vτ, v)τ
is a function which controls convergence. Following the above update rule causes the
simulation to converge to the following distributions[7,22], depending on the choice of
θ:
θ(vτ, v)τ =



1,
non-convergent
ˆµτ,
q(v)
β∆Tµ(vτ)τ/T,
q(v)∆T/(∆T+T)P(v)T/(∆T+T)
(11)
where ∆T is the “tempering factor”[7], P(v) is the marginal unbiased distribution
for v(⃗r), and ˆµ is the total or average of all previous µτ. The ﬁrst condition, θ = 1,
is similar to normal metadynamics; it reaches a distribution similar to q(v) but may
oscillate due to a lack of dampening in the update size. Condition two, called globally
tempered, converges correctly to q(v). The last condition, locally tempered, converges
to an adjustable mixture of the unbiased and target distribution. EDM is traditionally
implemented as globally tempered, so that the the ﬁnal distribution is indeed the
target. The ﬁrst condition is good for tuning parameters, since it makes progress more
quickly. The ﬁnal condition, locally tempered, allows a pseudo-Bayesian tuning of prior
belief in the unbiased ensemble. It is pseudo-Bayesian because the ratio of inﬂuence
from the prior belief to evidence is computed, not set, in Bayesian modeling.
An important consideration of both EDS and EDM is that they add potential energy
during the update step which quickly becomes kinetic energy. Thus, it is important
that the thermostat used to maintain constant temperature in the NVT can dissipate
energy faster than it is added by the update steps. This is necessary during the adaptive
phase of EDS and EDM prior to convergence.
2.1. Treating Uncertainty in Experimental Data
One complication of minimal biasing methods is uncertainty in the experimental data.
For example, the experimental data could be the radius of gyration of a polymer with
a reported uncertainty in the mean of 5 nm. Should the radius of gyration be matched
exactly, or only to within 5 nm? It is possible to stop the adaptive phase of EDS early,
5

for example when the average is within the uncertainty of the experimental data. An
early stop is ad-hoc and part of the maximum entropy derivation unlike other methods
which are built to address uncertainty[11,23].
Cesari et al. [24] proposed a modiﬁcation to the maximum entropy derivation above
to address experimental uncertainty. Instead of the constraint in Equation 2, this
constraint is used
Z
d⃗r P ′′(⃗r, ϵ) [s(⃗r) + ϵ] = ˆs
(12)
where ϵ is an auxiliary variable that allows deviations in the average of s(⃗r) and P ′′(·)
is the probability distribution that will be solved after maximizing entropy. ϵ is a
random variable from a prior distribution P0(ϵ) that describes the uncertainty in the
experimental data. For example, P0(ϵ) could be a normal distribution or a Laplace
distribution. Cesari et al. [24] show that P ′′(⃗r, ϵ) = 1/Z′′P ′(⃗r)P0(ϵ)eλϵ. This leads to
a diﬀerent update step of
gτ = 2β
w (⟨s⟩τ + ξ(ατ) −ˆs)

s2
τ + ∂ξ(ατ)
∂α
−⟨s⟩2
τ

(13)
where ξ(ατ) is the analytic posterior of ϵ:
ξ(ατ) =
R
dϵ P0(ϵ)e−ατϵϵ
R
dϵ P0(ϵ)e−ατϵ
(14)
Returning to the radius of gyration example above, if the uncertainty (P0(ϵ) is assumed
to be a normal distribution normal with σ = 5nm, then ξ(ατ) = −25ατ. This new
update-step can be used to rigorously include experimental uncertainty in to the EDS
method.
EDM is able to tune the relative importance of the target distribution and the unbi-
ased ensemble with the locally-tempered variant in Equation 11. This could be used to
match intuition. For example, if you believe that the target distribution which comes
from experimental data is twice as accurate as the molecular dynamics simulation, you
could choose ∆T/(∆T + T) = 0.66 giving about twice as much weight to the target
distribution.
3. Applications of EDS and EDM
A model 1-D system is shown in Figure 2 as a probability distribution function, P(r).
EDS is being used to modify the average value of r to match a new set point, ˆr.
EDS adds a linear bias, whose strength is indicated with the red dot, to create the
biased PDF P ′(r) according to Equation 5. Notice how the features of P(r) are mostly
maintained in P ′(r). The bottom plot shows how each value of ˆr corresponds to a
unique biasing strength.
A more sophisticated system which demonstrates the capabilities of improving dy-
namic observables is the recent work on EDS ab initio molecular dynamics (AIMD)
simulations of water[10]. DFT water with the BLYP exchange functional poorly rep-
resents water structure as seen in Figure 3 (black line). It is over-structured and has
water self-diﬀusion coeﬃcients that are too high (0.005-0.005 ˚A2 / ps)[26] compared
6

0
2
4
6
8
10
r
0.0
0.2
0.4
0.6
P (r)
P(r)
P′(r)
r
r
0
2
4
6
8
10
r
2
0
2
(r)
Figure 2.
A 1-D EDS calculation where the mean of a probability distribution function (PDF) is being biased
to match the dash vertical orange line. The top plot shows the unbiased and biased PDFs. The biased PDF
shows as much of the shape of the unbiased PDF as possible while matching the new biased mean. The bottom
shows the Lagrange multipliers that give all possible biased means. The red dot indicates the current Lagrange
multiplier for the biased PDF. There is a unique λ for all possible biased means, as discussed in the Theory
section.
Ow-Ow g(r)
Figure 3.
Water oxygen-oxygen radial distribution function from ab inito molecular dynamics at 300 K NVT
and experiments from Skinner et al. [25]. BLYP and BLYP-D3 are from DFT with and without dispersion
corrections with the BLYP exchange functional. Note that BLYP-D3 is typically done at 330K, which gives
much better agreement[26]. See White et al. [10] for complete system details. The BYLP-EDS line is DFT with
EDS bias added to the water oxygen-oxygen coordination number. EDS shows near quantitative agreement
with experiment. Copyright 2017 AIP Publishing LLC.
7

with the experimental value of 0.23 ˚A2[27]. EDS was used to improve the coordination
number of the oxygen-oxygen (Ow-Ow) water molecules and resulted in near perfect
agreement with experimental scattering data (Figure 3). A number of unrelated ob-
servables improved as well, including RDFs and the water self-diﬀusion coeﬃcient,
which increased to 0.06± ˚A2/ps. White et al. [10] further demonstrated that the EDS
bias could be transferred to excess proton-water simulations and that when combined
with DFT dispersion corrections[28], the agreement further improves. The accuracy
improvement with the dispersion corrections shows that EDS is general in its applica-
bility to DFT methods.
Cortina et al. [29] used EDS to study the KPC-2 carbapenemase enzyme, which is
responsible for drug resistance in the majority of carbapenem-resistant Gram-negative
bacteria[30]. They used EDS to modify protein-protein distances while doing a com-
mittor analysis[31] to identify transition states in the carbapenem-enzyme complex.
Cesari et al. [24] used a method similar to EDS (modiﬁed update step) to improve
agreement with experimental NMR 3J coupling data for RNA oligonucleotids. After
biasing, they used the simulation results to improve the underlying Amber force ﬁeld
and thus create a transferrable model. Cesari et al. [24] also developed a novel ap-
proach to account for experimental data uncertainty by adding auxiliary variables to
the EDS update step.
EDM has been used less than EDS due to the rarity of experimental data giving an
exact probability distribution. One example explored in the original EDM paper was
to construct a mean ﬁeld bias that mimics an alanine dipeptide being in the backbone
of a protein[7]. This was done by computing a potential of mean force (PMF) for
φ, ψ dihedral angles from PDB crystallography data for the alanine-alanine sequence
in protein structures. This corresponds to a probability distribution and a molecular
dynamics simulation of alanine dipeptide was done with its φ, ψ dihedral angles biased
to match the desired values. The result was a molecular dynamics simulation where
the dipeptide was biased to behave as part of a longer protein structure. A similar
approach was later used by Hocky et al. [21] with EDS to built a pseudo-mean ﬁeld
model for actin ﬁlaments.
Another example of EDM can be found in Gil-Ley et al. [13] who used it (called
targeted metadynamics) to improve agreement of RNA oligonucleatides’ dihedral an-
gles with PDB crystallography data. They similarly built dihedral angle PMFs using
the crystallography data and biased molecular dynamics simulations of RNA to adopt
the dihedral angle probability distribution functions. Gil-Ley et al. [13] then took the
converged EDM bias and transferred it to a diﬀerent simulation of an RNA tetramer
and found little to no improvement of agreement with the crystallography data. This
may have been due to the underlying assumption that a PMF derived from crystal-
lographic data is not representative of the room temperature ensemble. Nevertheless,
EDM is a promising technique for incorporating experimental data as a type of aux-
iliary potential energy function in simulations.
3.1. Coarse-Grain Modeling
EDS and EDM are well-suited for coarse-grain (CG) modeling because they guarantee
that a CG model matches experimental data. Dannenhoﬀer-Lafage et al. [32] showed
that EDS can be applied before force-matching (a CG method) and the improvements
of agreement with experimental data is maintained. Dannenhoﬀer-Lafage et al. [32]
began with all atom molecular dynamics simulations of Ethylene carbonate with the
8

force ﬁeld from Masia et al. [33], which is known to not match center-of-mass coordi-
nation numbers from more accurate DFT calculations from Borodin and Smith [34].
Dannenhoﬀer-Lafage et al. [32] biased the all-atom simulations with EDS to match
these known coordination numbers. They then used force-matching to create multi-
ple CG models with one, two, and three site CG beads, using either biased all-atom
or unbiased all-atom simulation data. The biased all-atom simulations showed better
agreement with coordination numbers, indicating that the improvement in all-atom
simulations translates to improvement in the CG model with EDS. Of course, EDS
could be used directly on the CG model but that can lead to complications in how
observables are calculated on CG models[35].
EDS has been applied to CG simulations of the G- and F-actin proteins as monomers
and trimers[21]. Actin proteins are globular proteins that can polymerize into long
semi-ﬂexible ﬁlaments. Their hypothesis was that they could model a subsystem from
within the polymerized actin structure by incorporating information about structural
ﬂuctuations from simulations of the larger system via EDS. By biasing the ﬁrst and
second moment of two important collective variables in actin monomer structure, they
were able to observe ﬁlament-like conformations, and importantly, the ﬂuctuations of
these observables for an actin monomer, even in the system which contained only a
single monomer solvated in water. Hocky et al. [21] also studied a number of questions
about the EDS method in their system and found the following conclusions: (1) the
linear term of EDS better matches target values and maintains system ﬂuctuations
than harmonic biases; (2) replacing the variance term in Equation 8 with a covariance
matrix for all biased dimensions has faster convergence; (3) Levenberg–Marquardt[36]
converges faster than stochastic gradient descent. This paper also derived a simple
equation that can be used to guess the value of λ without doing a stochastic mini-
mization (whose accuracy depends on the distance of the target observables from the
unbiased observables), and hence can serve as a good initial guess for starting an EDS
simulation.
3.2. Enhanced Sampling
EDM may require enhanced sampling if there are slow degrees of freedom orthogonal
to the biased collective variable. This is most conveniently treated via the extensive
literature on enhanced sampling with metadynamics, since EDM is a type of metady-
namics and typically implemented within a metadynamics code. EDS is not as simple
because it requires that the ⟨·⟩τ term in Equation 8 be taken over an NVT ensemble.
This limits the enhanced sampling techniques to those that still give correct NVT
ensemble averages. One example is parallel-tempering replica-exchange. It is possible
to use metadynamics if an appropriate estimator[37] is done to compute the averages
but this has not been explored in practice.
Amirkulova and White [38] demonstrated the use of enhanced sampling and EDS
with the parallel-tempering well-tempered ensemble (PT-WTE)[39]. The PT-WTE
method is a enhancement of enhanced parallel-tempering replica-exchange that im-
proves exchange rates and reduces the required number of replicas[40]. PT-WTE sat-
isﬁes the observable that the ensemble averages, ⟨·⟩τ, can be computed during the
course of the simulation because PT-WTE only changes the magnitude of potential
energy ﬂuctuations, not their expectations. One apparent drawback is that the method
loses the one-replica observable of EDS that allows computation of dynamic observ-
ables. However, this only applies to the adaptive phase. During the ﬁxed-bias second
9

Figure 4.
Free energy surface of GYG peptide along dihedral angles (Ramachandran plot) from Amirkulova
and White [38]. a) EDS and enhanced sampling with PT-WTE and b) no EDS and no enhanced sampling.
The diﬀerence between panels a and b shows that EDS changes the global free energy minimum, which better
matches data from Ting et al. [41], and PT-WTE improves sampling based on explored regions. Copyright
2018 World Scientiﬁc Publishing Company.
phase of EDS, one replica can again be used to allow analysis of dynamic observables.
Amirkulova and White [38] studied the GYG peptide with the EDS plus enhanced
sampling approach. Eight simulations were conducted, each with 1, 8, or 16 replicas.
The simulations had EDS, PT-WTE, and/or parallel-tempering. EDS was used to
bias proton chemical shifts to improve agreement with experimental NMR data. The
simulation results showed that PT-WTE improves sampling in the EDS method and
does not change agreement with experimental data. PT-WTE also converged the EDS
bias with fewer replicas than the PT method. One consideration in all EDS simulations
is the eﬀect on unbiased observables. The Ramachandran plots of the simulations with
and without EDS bias are compared in Figure 4. When EDS and enhanced sampling
are used (Figure 4 a)), the simulation explores a larger region of conﬁgurational space
relative to the control simulation (Figure 4 b)). Also, the global minimum changes
when using EDS in Figure 4, bringing it closer to what was found in Ting et al. [41]
for the GYG sequence.
4. The Role of Pressure
Thus far we have discussed NVT and NVE ensembles. EDM and EDS add new forces
to the simulation and thus aﬀect the system virial. This can lead to undesirable density
changes when the bias is subsequently used in an NPT simulation. It is possible to
rectify this change in virial, and thus density, by adding a further constraint that the
virial be unchanged while still maximizing entropy. As shown in the appendix, this
leads to an unsolvable equation and thus it is not possible to simultaneously set the
virial and other constraints. One intuitive reason for this is that the virial theorem
requires the average virial be proportional to average temperature. Therefore if we
tried to bias an ensemble so that its virial is ﬁxed, then the average temperature
would change, violating our constant average temperature.
One way around this challenge lies in the correlation between biased observables in
EDS. When biasing multiple observables which are correlated, there is in fact no longer
a unique set of Lagrange multipliers[16]. These extra degrees of freedom in the choice
10

of Lagrange multipliers that maximize Equation 3 enable us to choose the Lagrange
multipliers that minimally change the virial. Equation 8 can be modiﬁed so that our
coupling constants minimize the additional pressure which they exert[42]:
gτ = 2β
w (⟨s⟩τ −ˆs)

s2
τ −⟨s⟩2
τ

+ 2ν∆pτ
∂∆pτ
∂ατ
(15)
∆pτ = −ατ
w
ds
dV = −ατ
w
X
ij
 ∂s
∂vij
∂vij
∂V

(16)
where ν is a parameter that controls the importance of minimizing the virial, ∆pτ
is the change in the system virial pressure due to the EDS bias,
∂s
∂vij is the partial
derivative of the observable with respect to one component of the triclinic box matrix,
and ∂vij
∂V can be computed from the adjugate of the triclinic box matrix. When biasing
multiple observables, ∆pτ should be the total over all observables. gτ is in units of
s2 per energy, so ν must be in units of volume squared times s2 per energy squared.
Practically we can choose a unitless ν∗, which is deﬁned from
ν = ν∗w2β2
ρ2
(17)
Equation 15 was used in a molecular dynamics simulation of 128 modiﬁed SPC/E
water molecules. This modiﬁed SPC/E water has increased charges (qO = −0.94) to
distort its coordination number. EDS was then applied to correct the coordination
number using experimentally derived coordination numbers from Skinner et al. [25]
with varying strengths of the virial correction. The coordination number moment
deﬁnition may be found in White and Voth [6]. Figure 6 shows that coordination
number and its moments are still correctly biased with the virial term. The EDS
parameters were a range (A) of 50 kj/mol, a period of 25 fs, and the Levenberg–
Marquardt optimization procedure. A Nose-Hoover thermostat with a time constant
of 25 fs and a timestep of 0.5 fs were used for the molecular dynamics in the LAMMPS
simulation engine[43]. The NPT barostat was Parrinello-Rahman.
Figure 5 shows the eﬀect of the virial penalty term on the EDS virial contribution.
The virial contribution plotted and computed here is the mean per-particle virial
energy contribution. That is ∆pτ/ρ/kT which removes the eﬀect of particle number
and energy scale. Panel a shows the virial contribution of each collective variable with
ν∗= 0 (no virial minimization). The net average virial contribution is 39.2 kT. Panel
b shows ν∗= 10 and there is a much lower contribution of 5.02 kT. Panel c compares
the net virial contribution of three diﬀerent ν∗values.
Figure 6 shows the impact on the convergence of the biased coordination number
and its moments with the virial minimizing terms. Interestingly, adding a virial mini-
mization term actually improves convergence. This is due to the well-known eﬀect of
regularization, which improves convergence in optimization[44]. The virial minimiza-
tion term is proportional to ατ, so it brings down the magnitude of ατ. This prevents
the large swings seen in the ν = 0 system. Thus, adding the virial minimization term
not only reduces the virial contribution but can improve convergence by minimizing
magnitudes of the coupling constants. The true test can be seen in the NPT portion
past 20ps where the bias is ﬁxed but the box dimensions are free. EDS with ν∗= 0
11

a)
b)
c)
Figure 5.
The EDS contribution to virial as EDS is applied to water coordination number moments 0 through
3 in molecular dynamics simulation of over-structured SCP/E water model. Panels a and b compare the virial
contribution in a normal EDS and EDS with virial minimization strength of ν∗= 1. There is lower per-collective
variable virial contribution with virial minimization. Panel c shows the eﬀect of diﬀerent ν∗values on the total
virial contribution from EDS.
gives poor densities in NPT with a bias computed in NVT. The virial minimization
reduces the change in density with a trade-oﬀin match to the bias set-points. Virial
minimization is in general recommended due to its enhanced convergence and better
representation of virial forces.
5. Implementations
There are three implementations of EDS available: Colvars[45], Plumed1.3[46], and
Plumed2[47]. The Colvars and Plumed1.3 implementations match the original EDS
manuscript. The Plumed2 implementation is actively maintained as a plugin[48] and
has features from more recent EDS articles[21]. For example, the covariance term in
Equation 8 can be computed using the full sample covariance matrix or the sample
variance. The update steps can also be computed using the Levenberg–Marquardt
method[21,36], instead of Equation 7. The Plumed2 implementation is recommended.
EDM is implemented in Plumed2 as well, under the “targeted metadynamics” key-
word within the metadynamics biasing class. One of the common use-cases for EDM is
to bias RDFs, which is not exactly a collective variable. RDFs are “function collective
variables” in the sense that there is a distribution at each timestep. Thus the bias
update equation, shown in Equation 10, applies to each observed point in the RDF at
each step (see White et al. [7] for equations). This can lead to thousands of updates per
timestep on even small systems. To improve performance and scaling, we have created
an implementation in LAMMPS that is more tightly integrated with the simulation
engine than Plumed2 as described in White et al. [7]. Note that the addition of hills
12

1.00
1.25
CN-0
= 0
CV Value
Set Point
Unbiased Avg
1.00
1.25
CN-0
Adaptive
NPT
= 1.0
1.00
1.25
CN-0
= 10.0
1.00
1.25
CN-1
1.00
1.25
CN-1
1.00
1.25
CN-1
1.00
1.25
CN-2
1.00
1.25
CN-2
1.00
1.25
CN-2
1.00
1.25
CN-3
1.00
1.25
CN-3
1.00
1.25
CN-3
0
10
20
30
Time [ps]
1.00
1.25
Density [g / cm3]
0
10
20
30
Time [ps]
1.00
1.25
Density [g / cm3]
0
10
20
30
Time [ps]
1.00
1.25
Density [g / cm3]
Figure 6.
EDS applied to water coordination number moments 0 through 3 in molecular dynamics simulation
of over-structured SCP/E water model. The columns show increasing strength of virial minimization. Each plot
is collective variable scaled by its set-point. The vertical dashed line spearates the NVT adaptive simulation
and a ﬁxed bias NPT phase. The results show that increasing the strength of the virial minimization actually
improves convergence by reducing large-magnitude changes in biasing force. Without virial minimization, EDS
can produce nonphysical densities when the NVT bias is transferred to NPT. The change in density moves the
CVs. For ν∗= 0, this meant CV values beyond the y-limits of the plot.
13

is parallelized here, which is unusual and enables the scaling as a function of particle
size. Hill addition is 90% of the CPU utilization. This was the implementation used
in White et al. [7] for an ethylene carbonate electrolyte simulation and Lennard-Jones
benchmark systems.
6. Conclusions
Minimal biasing techniques are an emerging area that improve the accuracy of molec-
ular simulation and better utilize comparable experimental data. EDS and EDM are
maximum entropy techniques that minimally bias an ensemble so that average observ-
ables or probability distributions of observables match set values. EDS and EDM both
converge the potential to Equation 5, allowing an NVE simulation with ﬁxed potential
energy in a single replica so that dynamic observables can be computed. EDS and EDM
can bias multiple observables simultaneously and be combined with enhanced sampling
methods. It is possible to treat uncertainty in experimental data using methods from
Cesari et al. [24]. Explicitly setting pressure is not possible in maximum entropy bias-
ing, but in EDS it is possible to minimize the change to pressure when biasing multiple
collective variables. This minimization can actually improve convergence of EDS by
acting as a regularization. A variety of example systems have been presented here
and there are implementations available in most simulation engines for both EDS and
EDM.
Acknowledgement(s)
The authors thank Prof. Pengfei Huo for reviewing the appendix mathematics, Rainier
Barrett for help preparing the manuscript and Prof. Glen Hocky for assistance in
surveying the literature and preparing the manuscript.
Funding
This work was supported by the National Science Foundation CBET Div Of Chem,
Bioeng, Env, & Transp Sys (Grant #1751471).
7. References
References
[1] Kresten Lindorﬀ-Larsen, Stefano Piana, Kim Palmo, Paul Maragakis, John L. Klepeis,
Ron O. Dror, and David E. Shaw. Improved side-chain torsion potentials for the Amber
ﬀ99SB protein force ﬁeld. Proteins., 78(8):1950–1958, 2010. ISSN 08873585. .
[2] Devleena Shivakumar, Joshua Williams, Yujie Wu, Wolfgang Damm, John Shelley, and
Woody Sherman. Prediction of absolute solvation free energies using molecular dynamics
free energy perturbation and the opls force ﬁeld. J. Chem. Theory Comput., 6(5):1509–
1519, 2010. ISSN 15499626. .
[3] Pietro Sormanni, Damiano Piovesan, Gabriella T Heller, Massimiliano Bonomi, Pre-
drag Kukic, Carlo Camilloni, Monika Fuxreiter, Zsuzsanna Dosztanyi, Rohit V Pappu,
M Madan Babu, Sonia Longhi, Peter Tompa, A Keith Dunker, Vladimir N Uversky, Silvio
14

C E Tosatto, and Michele Vendruscolo. Simultaneous quantiﬁcation of protein order and
disorder. Nat. Chem. Biol., 13(4):339–342, 2017. ISSN 1552-4450. .
[4] Massimiliano Bonomi, Gabriella T. Heller, Carlo Camilloni, and Michele Vendruscolo.
Principles of protein structural ensemble determination. Curr. Opin. Struct. Biol., 42:
106–116, 2017. ISSN 1879033X. .
[5] Simon Olsson, Jes Frellsen, Wouter Boomsma, Kanti V Mardia, and Thomas. Hamelryck.
Inference of structure ensembles of ﬂexible biomolecules from sparse, averaged data. PLoS
One, 8(11):e79439, 2013. ISSN 1932-6203. .
[6] Andrew D White and Gregory A Voth. Eﬃcient and minimal method to bias molecular
simulations with experimental data. J. Chem. Theory Comput., 10(8):3023–3030, 2014.
[7] Andrew White, James Dama, and Gregory a. Voth.
Designing Free Energy Surfaces
that Match Experimental Data with Metadynamics. J. Chem. Theory Comput., pages
2451–2460, 2015. ISSN 1549-9618. .
[8] Kyle A Beauchamp, Vijay S Pande, and Rhiju. Das. Bayesian Energy Landscape Tilting:
Towards Concordant Models of Molecular Ensembles.
Biophys. J., 106(6):1381–1390,
2014. ISSN 0006-3495. .
[9] Kresten Lindorﬀ-Larsen, Robert B Best, Mark A DePristo, Christopher M Dobson, and
Michele. Vendruscolo. Simultaneous determination of protein structure and dynamics.
Nature, 433(7022):128–132, 2005. ISSN 0028-0836. .
[10] Andrew D. White, Chris Knight, Glen M. Hocky, and Gregory A. Voth. Communication:
Improved ab initio molecular dynamics by minimally biasing with experimental data. J.
Chem. Phys., 146(4), 2017. ISSN 00219606. .
[11] Gerhard Hummer and J¨urgen K¨oﬁnger. Bayesian ensemble reﬁnement by replica simula-
tions and reweighting. J. Chem. Phys., 143(24):243150, 2015. ISSN 00219606. .
[12] Fabrizio Marinelli and Jos´e D. Faraldo-G´omez. Ensemble-Biased Metadynamics: A Molec-
ular Simulation Method to Sample Experimental Distributions.
Biophys. J., 108(12):
2779–2782, 2015. ISSN 15420086. .
[13] Alejandro Gil-Ley, Sandro Bottaro, and Giovanni Bussi. Empirical Corrections to the
Amber RNA Force Field with Target Metadynamics. J. Chem. Theory Comput., 12(6):
2790–2798, 2016. ISSN 15499626. .
[14] Omar Valsson and Michele Parrinello. Variational approach to enhanced sampling and
free energy calculations. Phys. Rev. Lett, 113(9):090601, 2014.
[15] James F. Dama, Glen M. Hocky, Rui Sun, and Gregory A. Voth. Exploring Valleys without
Climbing Every Peak: More Eﬃcient and Forgiving Metabasin Metadynamics via Robust
On-the-Fly Bias Domain Restriction. J. Chem. Theory Comput., 11(12):5638–5650, 2015.
ISSN 15499626. .
[16] Jed W. Pitera and John D. Chodera. On the Use of Experimental Observations to Bias
Simulated Ensembles. J. Chem. Theory Comput., 8(10):3445–3451, 2012. ISSN 1549-9618.
.
[17] E T Jaynes. Information Theory and Statistical Mechanics. Phys. Rev., 106(4):620–630,
1957. .
[18] Benoˆıt Roux and Jonathan Weare. On the statistical equivalence of restrained-ensemble
simulations with the maximum entropy method. J. Chem. Phys., 138(8), 2013. ISSN
00219606. .
[19] E. Boura, B. Rozycki, D. Z. Herrick, H. S. Chung, J. Vecer, W. A. Eaton, D. S. Caﬁso,
G. Hummer, and J. H. Hurley. Solution structure of the ESCRT-I complex by small-
angle X-ray scattering, EPR, and FRET spectroscopy. Proc. Natl. Acad. Sci., 108(23):
9437–9442, 2011. ISSN 0027-8424. .
[20] HB McMahan and Matthew Streeter. Adaptive bound optimization for online convex
optimization. arXiv, pages 1–19, 2010. ISSN 09505849. .
[21] Glen M. Hocky, Thomas Dannenhoﬀer-Lafage, and Gregory A. Voth. Coarse-Grained
Directed Simulation. J. Chem. Theory Comput., 13(9):4593–4603, 2017. ISSN 15499626.
.
[22] James F. Dama, Michele Parrinello, and Gregory a. Voth. Well-Tempered Metadynamics
15

Converges Asymptotically. Phys. Rev. Lett., 112(24):240602, 2014. ISSN 0031-9007. .
[23] David H. Brookes and Teresa Head-Gordon. Experimental Inferential Structure Determi-
nation of Ensembles for Intrinsically Disordered Proteins. J. Am. Chem. Soc., 138(13):
4530–4538, 2016. ISSN 15205126. .
[24] Andrea Cesari, Alejandro Gil-Ley, and Giovanni Bussi. Combining Simulations and So-
lution Experiments as a Paradigm for RNA Force Field Reﬁnement. J. Chem. Theory
Comput., 12(12):6192–6200, 2016. ISSN 15499626. .
[25] Lawrie B. Skinner, Congcong Huang, Daniel Schlesinger, Lars G.M. Pettersson, Anders
Nilsson, and Chris J. Benmore. Benchmark oxygen-oxygen pair-distribution function of
ambient water from x-ray diﬀraction measurements with a wide Q-range. J. Chem. Phys.,
138(7), 2013. ISSN 00219606. .
[26] Ying Lung Steve Tse, Chris Knight, and Gregory A. Voth. An analysis of hydrated proton
diﬀusion in ab initio molecular dynamics. J. Chem. Phys., 142(1), 2015. ISSN 00219606.
.
[27] Kazimierz Krynicki, Christopher D Green, and David W Sawyer. Pressure and tempera-
ture dependence of self-diﬀusion in water. Faraday Discuss. Chem. Soc., 66(0):199–208,
1978. .
[28] M. Sitarz, E. Wirth-Dzieciolowska, and P. Demant. Loss of heterozygosity on chromosome
5 in vicinity of the telomere in γ-radiation-induced thymic lymphomas in mice. Neoplasma,
47(3):148–150, 2000. ISSN 00282685. .
[29] George A. Cortina, Jennifer M. Hays, and Peter M. Kasson. Conformational Intermediate
That Controls KPC-2 Catalysis and Beta-Lactam Drug Resistance. ACS Catal., 8(4):
2741–2747, 2018. ISSN 21555435. .
[30] Orville A Pemberton, Xiujun Zhang, and Yu Chen. Molecular Basis of Substrate Recog-
nition and Product Release by the Klebsiella pneumoniae Carbapenemase (KPC-2). J.
Med. Chem, 60(8):3525–3530, 2017. .
[31] Rose Du, Vijay S Pande, Alexander Yu Grosberg, Toyoichi Tanaka, and Eugene S
Shakhnovich.
On the transition coordinate for protein folding.
J. Chem. Phys., 108
(1):334–350, 1998.
[32] Thomas Dannenhoﬀer-Lafage, Andrew D White, and Gregory A Voth. A Direct Method
for Incorporating Experimental Data into Multiscale Coarse-grained Models. J. Chem.
Theory Comput., 12(5):2144–2153, 2016. ISSN 1549-9626. .
[33] Marco Masia, Michael Probst, and Rossend. Rey. Ethylene Carbonate-Li+: A Theoretical
Study of Structural and Vibrational Properties in Gas and Liquid Phases. J. Phys. Chem.
B, 108(6):2016–2027, 2004. ISSN 1520-6106. .
[34] Oleg Borodin and Grant D Smith. Quantum Chemistry and Molecular Dynamics Simula-
tion Study of Dimethyl Carbonate: Ethylene Carbonate Electrolytes Doped with LiPF6.
J. Phys. Chem. B, 113(6):1763–1776, 2009. ISSN 1520-6106. .
[35] Jacob W. Wagner, James F. Dama, Aleksander E.P. Durumeric, and Gregory A. Voth.
On the representability problem and the physical meaning of coarse-grained models. J.
Chem. Phys., 145(4), 2016. ISSN 00219606. .
[36] Panagiotis Stinis. A maximum likelihood algorithm for the estimation and renormalization
of exponential densities. J. Comput. Phys., 208(2):691–703, 2005. ISSN 00219991. .
[37] Pratyush Tiwary and Michele Parrinello. A Time-Independent Free Energy Estimator for
Metadynamics. J. Phys. Chem. B, 119(3):736–742, 2015. ISSN 1520-6106. .
[38] Dilnoza B. Amirkulova and Andrew D. White.
Combining enhanced sampling with
experiment-directed simulation of the GYG peptide. J. Theor. Comput. Chem., 17(03):
1840007, 2018. ISSN 0219-6336. .
[39] M. Bonomi and M. Parrinello. Enhanced sampling in the well-tempered ensemble. Phys.
Rev. Lett., 104(19):1–4, 2010. ISSN 00319007. .
[40] Michael Deighan, Massimiliano Bonomi, and Jim Pfaendtner.
Eﬃcient simulation of
explicitly solvated proteins in the well-tempered ensemble. J. Chem. Theory Comput., 8
(7):2189–2192, 2012. ISSN 15499618. .
[41] Daniel Ting, Guoli Wang, Maxim Shapovalov, Rajib Mitra, Michael I. Jordan, and
16

Roland L. Dunbrack.
Neighbor-dependent Ramachandran probability distributions of
amino acids developed from a hierarchical dirichlet process model. PLOS Comput. Biol.,
6(4), 2010. ISSN 1553734X. .
[42] Manuel J. Louwerse and Evert Jan Baerends. Calculation of pressure in case of periodic
boundary conditions. Chem. Phys. Lett., 421(1-3):138–141, 2006. ISSN 00092614. .
[43] Steve Plimpton. Fast parallel algorithms for short-range molecular dynamics. J. Comput.
Phys., 117(1):1 – 19, 1995. ISSN 0021-9991. .
[44] Bernhard Scholkopf and Alexander J Smola. Learning with kernels: support vector ma-
chines, regularization, optimization, and beyond. MIT press, 2001.
[45] Giacomo Fiorin, Michael L Klein, and J´erˆome H´enin. Using collective variables to drive
molecular dynamics simulations. Mol. Phys., 111(22-23):3345–3362, 2013. .
[46] Massimiliano Bonomi, Davide Branduardi, Giovanni Bussi, Carlo Camilloni, Davide
Provasi, Paolo Raiteri, Davide Donadio, Fabrizio Marinelli, Fabio Pietrucci, Ricardo A
Broglia, and Michele Parrinello. {PLUMED}: A portable plugin for free-energy calcula-
tions with molecular dynamics. Comput. Phys. Commun., 180(10):1961–1972, 2009. ISSN
00104655. .
[47] Gareth A Tribello, Massimiliano Bonomi, Davide Branduardi, Carlo Camilloni, and Gio-
vanni Bussi. PLUMED 2: New feathers for an old bird. Comput. Phys. Commun, 185(2):
604–613, 2014. ISSN 0010-4655. .
[48] Giovanni Bussi and Gareth A Tribello. Analyzing and biasing simulations with plumed.
arXiv, 2018.
Appendix A. Minimal Biasing Pressure Derivation
We would like to constrain the virial contribution to pressure while minimizing relative
entropy in a biased ensemble. The constraints are:
Z
d⃗r P ′(⃗r) = 1
(A1)
which enforces normalization. The other constraint is to set the virial pressure,
V [P ′(⃗r)], to be equal to the scalar pe.
V [P ′(⃗r)] = 1
3V
Z
d⃗r ⃗r · ∂ln P ′(⃗r)
∂⃗r
P ′(⃗r) =
Z
d⃗r ⃗r · ∇P ′(⃗r) = pe
(A2)
where V is the volume. The quantity to be minimized is relative entropy of the
biased probability distribution with respect to the unbiased probability distribution:
Z
P(⃗r) ln P ′(⃗r)
P(⃗r)
(A3)
To solve this system, we compute the functional derivative of the Lagrangian with
respect to the biased probability distribution P ′(⃗r). This requires a functional deriva-
tive of Equation A2, which is:
17

∆p[P ′(⃗r)]
δP ′(⃗r)
= −∇[r1, r2, ...] = −N
(A4)
Some intuition for Equation A4 can be gained from the deﬁnition of the functional
derivative. Consider adding an arbitrary function φ(⃗r) to the probability distribution
scaled by ϵ, where ϵ is small. The change in the virial with this additional function is
Z
d⃗r ⃗r · ∇[P(⃗r) + ϵφ(⃗r)] −
Z
d⃗r ⃗r · ∇P(⃗r) ≈−N
Z
dr ϵφ(⃗r)
(A5)
The right-hand side is independent of the probability distribution, so that changing
the probability distribution decreases the virial by N regardless of where the change
occurs. With the functional derivative of the virial being constant, the functional
derivative of the Lagrangian is
δL
δP ′(⃗r) = ln P ′(⃗r) −ln P(⃗r) + λ1 −λ2N
(A6)
which has no solution for P ′(⃗r) because λ1 and λ2 are colinear. Thus there is no
way to constrain pressure.
18

