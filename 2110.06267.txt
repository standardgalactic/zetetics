Twice regularized MDPs and the equivalence between
robustness and regularization
Esther Derman∗
Technion
Matthieu Geist
Google Research, Brain Team
Shie Mannor
Technion, NVIDIA Research
Abstract
Robust Markov decision processes (MDPs) aim to handle changing or partially
known system dynamics. To solve them, one typically resorts to robust optimization
methods. However, this signiﬁcantly increases computational complexity and limits
scalability in both learning and planning. On the other hand, regularized MDPs
show more stability in policy learning without impairing time complexity. Yet,
they generally do not encompass uncertainty in the model dynamics. In this work,
we aim to learn robust MDPs using regularization. We ﬁrst show that regularized
MDPs are a particular instance of robust MDPs with uncertain reward. We thus
establish that policy iteration on reward-robust MDPs can have the same time
complexity as on regularized MDPs. We further extend this relationship to MDPs
with uncertain transitions: this leads to a regularization term with an additional
dependence on the value function. We ﬁnally generalize regularized MDPs to twice
regularized MDPs (R2 MDPs), i.e., MDPs with both value and policy regularization.
The corresponding Bellman operators enable developing policy iteration schemes
with convergence and robustness guarantees. It also reduces planning and learning
in robust MDPs to regularized MDPs.
1
Introduction
MDPs provide a practical framework for solving sequential decision problems under uncertainty
[30]. However, the chosen strategy can be very sensitive to sampling errors or inaccurate model
estimates. This can lead to complete failure in common situations where the model parameters
vary adversarially or are simply unknown [22]. Robust MDPs aim to mitigate such sensitivity by
assuming that the transition and/or reward function (P, r) varies arbitrarily inside a given uncertainty
set U [17, 27]. In this setting, an optimal solution maximizes a performance measure under the
worst-case parameters. It can be thought of as a dynamic zero-sum game with an agent choosing
the best action while Nature imposes it the most adversarial model. As such, solving robust MDPs
involves max-min problems, which can be computationally challenging and limits scalability.
In recent years, several methods have been developed to alleviate the computational concerns raised
by robust reinforcement learning (RL). Apart from [23, 24] that consider speciﬁc types of coupled
uncertainty sets, all rely on a rectangularity assumption without which the problem can be NP-hard
[2, 40]. This assumption is key to deriving tractable solvers of robust MDPs such as robust value
iteration [2, 11] or more general robust modiﬁed policy iteration (MPI) [18]. Yet, reducing time
complexity in robust Bellman updates remains challenging and is still researched today [15, 11].
At the same time, the empirical success of regularization in policy search methods has motivated
a wide range of algorithms with diverse motivations such as improved exploration [12, 20] or
stability [34, 13]. Geist et al. [10] proposed a uniﬁed view from which many existing algorithms
can be derived. Their regularized MDP formalism opens the path to error propagation analysis
∗Contact author: estherderman@campus.technion.ac.il
35th Conference on Neural Information Processing Systems (NeurIPS 2021).
arXiv:2110.06267v1  [cs.LG]  12 Oct 2021

in approximate MPI [33] and leads to the same bounds as for standard MDPs. Nevertheless, as
we further show in Sec. 3, policy regularization accounts for reward uncertainty only: it does not
encompass uncertainty in the model dynamics. Despite a vast literature on how regularized policy
search works and convergence rates analysis [36, 5], little attention has been given to understanding
why it can generate strategies that are robust to external perturbations [13].
To our knowledge, the only works that relate robustness to regularization in RL are [6, 16, 9]. Derman
& Mannor [6] employ a distributionally robust optimization approach to regularize an empirical
value function. Unfortunately, computing this empirical value necessitates several policy evaluation
procedures, which is quickly unpractical. Husain et al. [16] provide a dual relationship with robust
MDPs under uncertain reward. Their duality result applies to general regularization methods and
gives a robust interpretation of soft-actor-critic [13]. Although these two works justify the use
of regularization for ensuring robustness, they do not enclose any algorithmic novelty. Similarly,
Eysenbach & Levine [9] speciﬁcally focus on maximum entropy methods and relate them to either
reward or transition robustness. We shall further detail on these most related studies in Sec. 7.
The robustness-regularization duality is well established in statistical learning theory [41, 35, 19], as
opposed to RL theory. In fact, standard setups such as classiﬁcation or regression may be considered
as single-stage decision-making problems, i.e., one-step MDPs, a particular case of RL setting.
Extending this robustness-regularization duality to RL would yield cheaper learning methods with
robustness guarantees. As such, we introduce a regularization function ΩU that depends on the
uncertainty set U and is deﬁned over both policy and value spaces, thus inducing a twice regularized
Bellman operator (see Sec. 5). We show that this regularizer yields an equivalence of the form
vπ,U = vπ,ΩU, where vπ,U is the robust value function for policy π and vπ,ΩU the regularized one.
This equivalence is derived through the objective function each value optimizes. More concretely, we
formulate the robust value function vπ,U as an optimal solution of the robust optimization problem:
max
v∈RS⟨v, µ0⟩s. t. v ≤
inf
(P,r)∈U T π
(P,r)v,
(RO)
where T π
(P,r) is the evaluation Bellman operator [30]. Then, we show that vπ,U is also an optimal
solution of the convex (non-robust) optimization problem:
max
v∈RS⟨v, µ0⟩s. t. v ≤T π
(P0,r0)v −ΩU(π, v),
(CO)
where (P0, r0) is the nominal model. This establishes equivalence between the two optimization
problems. Moreover, the inequality constraint of (CO) enables to derive a twice regularized (R2)
Bellman operator deﬁned according to ΩU, a policy and value regularizer. For ball-constrained
uncertainty sets, ΩU has an explicit form and under mild conditions, the corresponding R2 Bellman
operators are contracting. The equivalence between the two problems (RO) and (CO) together with
the contraction properties of R2 Bellman operators enable to circumvent robust optimization problems
at each Bellman update. As such, it alleviates robust planning and learning algorithms by reducing
them to regularized ones, which are known to be as complex as classical methods.
To summarize, we make the following contributions: (i) We show that regularized MDPs are a
speciﬁc instance of robust MDPs with uncertain reward. Besides formalizing a general connection
between the two settings, our result enables to explicit the uncertainty sets induced by standard
regularizers. (ii) We extend this duality to MDPs with uncertain transition and provide the ﬁrst
regularizer that recovers robust MDPs with s-rectangular balls and arbitrary norm. (iii) We introduce
twice regularized MDPs (R2 MDPs) that apply both policy and value regularization to retrieve robust
MDPs. We establish contraction of the corresponding Bellman operators. This leads us to proposing
an R2 MPI algorithm with similar time complexity as vanilla MPI, thus opening new perspectives
towards practical and scalable robust RL.
Notations. We designate the extended reals by R := {−∞, ∞}. Given a ﬁnite set Z, the class of
real-valued functions (resp. probability distributions) over Z is denoted by RZ (resp. ∆Z), while
the constant function equal to 1 over Z is denoted by 1Z. Similarly, for any set X, ∆X
Z denotes the
class of functions deﬁned over X and valued in ∆Z. The inner product of two functions a, b ∈RZ
is deﬁned as ⟨a, b⟩:= P
z∈Z a(z) b(z), which induces the ℓ2-norm ∥a∥:=
p
⟨a, a⟩. The ℓ2-norm
coincides with its dual norm, i.e., ∥a∥= max∥b∥≤1⟨a, b⟩=: ∥a∥∗. Let a function f : RZ →R.
The Legendre-Fenchel transform (or convex conjugate) of f is f ∗(y) := maxa∈RZ{⟨a, y⟩−f(a)}.
Given a set Z ⊆RZ, the characteristic function δZ : RZ →R is δZ(a) = 0 if a ∈Z; +∞otherwise.
The Legendre-Fenchel transform of δZ is the support function σZ(y) = maxa∈Z⟨a, y⟩[3, Ex. 1.6.1].
2

2
Preliminaries
This section describes the background material that we use throughout our work. Firstly, we recall
useful properties in convex analysis. Secondly, we address classical discounted MDPs and their
linear program (LP) formulation. Thirdly, we brieﬂy detail on regularized MDPs and the associated
operators and lastly, we focus on the robust MDP setting.
Convex Analysis.
Let Ω: ∆Z →R be a strongly convex function. Throughout this study, the
function Ωplays the role of a policy and/or value regularization function. Its Legendre-Fenchel
transform Ω∗satisﬁes several smoothness properties, hence its alternative name "smoothed max
operator" [25]. Our work makes use of the following result [14, 25].
Proposition 2.1. Given Ω: ∆Z →R strongly convex, the following properties hold:
(i) ∇Ω∗is Lipschitz and satisﬁes ∇Ω∗(y) = arg maxa∈∆Z⟨a, y⟩−Ω(a), ∀y ∈RZ.
(ii) For any c ∈R, y ∈RZ, Ω∗(y +c1Z) = Ω∗(y) + c.
(iii) The Legendre-Fenchel transform Ω∗is non-decreasing.
Discounted MDPs and LP formulation.
Consider an inﬁnite horizon MDP (S, A, µ0, γ, P, r)
with S and A ﬁnite state and action spaces respectively, 0 < µ0 ∈∆S an initial state distribution
and γ ∈(0, 1) a discount factor. Denoting X := S × A, P ∈∆X
S is a transition kernel mapping
each state-action pair to a probability distribution over S and r ∈RX is a reward function. A policy
π ∈∆S
A maps any state s ∈S to an action distribution πs ∈∆A, and we evaluate its performance
through the following measure:
ρ(π) := E
" ∞
X
t=0
γtr(st, at)
 µ0, π, P
#
= ⟨vπ
(P,r), µ0⟩.
(1)
Here, the expectation is conditioned on the process distribution determined by µ0, π and P, and for
all s ∈S, vπ
(P,r)(s) = E[P∞
t=0 γtr(st, at)|s0 = s, π, P] is the value function at state s. Maximizing
(1) deﬁnes the standard RL objective, which can be solved thanks to the Bellman operators:
T π
(P,r)v := rπ + γP πv
∀v ∈RS, π ∈∆S
A,
T(P,r)v := max
π∈∆S
A
T π
(P,r)v
∀v ∈RS,
G(P,r)(v) := {π ∈∆S
A : T π
(P,r)v = T(P,r)v}
∀v ∈RS,
where rπ := [⟨πs, r(s, ·)⟩]s∈S and P π = [P π(s′|s)]s′,s∈S with P π(s′|s) := ⟨πs, P(s′|s, ·)⟩. Both
T π
(P,r) and T(P,r) are γ-contractions with respect to (w.r.t.) the supremum norm, so each admits a
unique ﬁxed point vπ
(P,r) and v∗
(P,r), respectively. The set of greedy policies w.r.t. value v deﬁnes
G(P,r)(v), and any policy π ∈G(P,r)(v∗
(P,r)) is optimal [30]. For all v ∈RS, the associated function
q ∈RX is given by q(s, a) = r(s, a) + γ⟨P(·|s, a), v⟩
∀(s, a) ∈X. In particular, the ﬁxed point
vπ
(P,r) satisﬁes vπ
(P,r) = ⟨πs, qπ
(P,r)(s, ·)⟩where qπ
(P,r) is its associated q-function.
The problem in (1) can also be formulated as an LP [30]. Given a policy π ∈∆S
A, we characterize its
performance ρ(π) by the following v-LP [30, 26]:
min
v∈RS⟨v, µ0⟩subject to (s.t.) v ≥rπ + γP πv.
(Pπ)
This primal objective provides a policy view on the problem. Alternatively, one may take a state
visitation perspective by studying the dual objective instead:
max
µ∈RS⟨rπ, µ⟩s. t. µ ≥0 and (IdRS −γP π
∗)µ = µ0,
(Dπ)
where P π
∗is the adjoint policy transition operator2: [P π
∗µ](s) := P
¯s∈S P π(s|¯s)µ(¯s)
∀µ ∈RS,
and IdS is the identity function in RS. Let I(s′|s, a) := δs′=s
∀(s, a, s′) ∈X × S the trivial transi-
tion matrix and deﬁne its adjoint transition operator as I∗µ(s) := P
(¯s,¯a)∈X I(s|¯s, ¯a)µ(¯s, ¯a)
∀s ∈
2It is the adjoint operator of P π in the sense that ⟨P πv, v′⟩= ⟨v, P π
∗v′⟩
∀v, v′ ∈RS.
3

S. The correspondence between occupancy measures and policies lies in the one-to-one mapping
µ 7→µ(·,·)
I∗µ(·) =: πµ and its inverse π 7→µπ given by
µπ(s, a) :=
∞
X
t=0
γtP

st = s, at = a
µ0, π, P

∀(s, a) ∈X .
As such, one can interchangeably work with the primal LP (Pπ) or the dual (Dπ).
Regularized MDPs.
A regularized MDP is a tuple (S, A, µ0, γ, P, r, Ω) with (S, A, µ0, γ, P, r)
an inﬁnite horizon MDP as above, and Ω:= (Ωs)s∈S a ﬁnite set of functions such that for all
s ∈S, Ωs : ∆A →R is strongly convex. Each function Ωs plays the role of a policy regularizer
Ωs(πs). With a slight abuse of notation, we shall denote by Ω(π) := (Ωs(πs))s∈S the family of
state-dependent regularizers.3 The regularized Bellman evaluation operator is given by
[T π,Ω
(P,r)v](s) := T π
(P,r)v(s) −Ωs(πs)
∀v ∈RS, s ∈S,
and the regularized Bellman optimality operator by T ∗,Ω
(P,r)v := maxπ∈∆S
A T π,Ω
(P,r)v
∀v ∈RS [10].
The unique ﬁxed point of T π,Ω
(P,r) (respectively T ∗,Ω
(P,r)) is denoted by vπ,Ω
(P,r) (resp. v∗,Ω
(P,r)) and deﬁnes the
regularized value function (resp. regularized optimal value function). Although the regularized MDP
formalism stems from the aforementioned Bellman operators in [10], it turns out that regularized
MDPs are MDPs with modiﬁed reward. Indeed, for any policy π ∈∆S
A, the regularized value
function is vπ,Ω
(P,r) = (IS −γP π)−1(rπ −Ω(π)), which corresponds to a non-regularized value
with expected reward ˜rπ := rπ −Ω(π). Note that the modiﬁed reward ˜rπ(s) is no longer linear
in πs because of the strong convexity of Ωs. Also, this modiﬁcation does not apply to the reward
function r but only to its expectation rπ, as we cannot regularize the original reward without making
it policy-independent.
Robust MDPs.
In general, the MDP model is not explicitly known but rather estimated from
sampled trajectories. As this may result in over-sensitive outcome [22], robust MDPs reduce such
performance variation. Formally, a robust MDP (S, A, µ0, γ, U) is an MDP with uncertain model
belonging to U := P × R, i.e., uncertain transition P ∈P ⊆∆X
S and reward r ∈R ⊆RX [17, 40].
The uncertainty set U typically controls the conﬁdence level of a model estimate, which in turn
determines the agent’s level of robustness. It is given to the agent, who seeks to maximize performance
under the worst-case model (P, r) ∈U. Although untractable in general, this problem can be solved
in polynomial time for rectangular uncertainty sets, i.e., when U = ×s∈SUs = ×s∈S(Ps × Rs)
[40, 23]. For any policy π ∈∆S
A and state s ∈S, the robust value function at s is vπ,U(s) :=
min(P,r)∈U vπ
(P,r)(s) and the robust optimal value function v∗,U(s) := maxπ∈∆A
S vπ,U(s). Each of
them is the unique ﬁxed point of the respective robust Bellman operators:
[T π,Uv](s) :=
min
(P,r)∈U T π
(P,r)v(s)
∀v ∈RS, s ∈S, π ∈∆A
S ,
[T ∗,Uv](s) := max
π∈∆S
A
[T π,Uv](s)
∀v ∈RS, s ∈S,
which are γ-contractions. For all v ∈RS, the associated robust q-function is given by q(s, a) =
min(P,r)∈U{r(s, a) + γ⟨P(·|s, a), v⟩}
∀(s, a) ∈X, so that vπ,U = ⟨πs, qπ,U(s, ·)⟩where qπ,U is
the robust q-function associated to vπ,U.
3
Reward-robust MDPs
This section focuses on reward-robust MDPs, i.e., robust MDPs with uncertain reward but known
transition model. We ﬁrst show that regularized MDPs represent a particular instance of reward-robust
MDPs, as both solve the same optimization problem. This equivalence provides a theoretical
motivation for the heuristic success of policy regularization. Then, we explicit the uncertainty set
3In the formalism of Geist et al. [10], Ωs is initially constant over S. However, later in the paper [10, Sec. 5],
it changes according to policy iterates. Here, we alternatively deﬁne a family Ωof state-dependent regularizers,
which accounts for state-dependent uncertainty sets (see Sec. 5 below).
4

underlying some standard regularization functions, thus suggesting an interpretable explanation of
their empirical robustness.
We ﬁrst show the following proposition 3.1, which applies to general robust MDPs and random
policies. It slightly extends [17], as Lemma 3.2 there focuses on uncertain-transition MDPs and
deterministic policies. For completeness, we provide a proof of Prop. 3.1 in Appx. A.1.
Proposition 3.1. For any policy π ∈∆S
A, the robust value function vπ,U is the optimal solution of
the robust optimization problem:
max
v∈RS⟨v, µ0⟩s. t. v ≤T π
(P,r)v for all (P, r) ∈U.
(PU)
In the robust optimization problem (PU), the inequality constraint must hold over the whole uncertainty
set U. As such, a function v ∈RS is said to be robust feasible for (PU) if v ≤T π
(P,r)v for all
(P, r) ∈U or equivalently, if max(P,r)∈U{v(s)−T π
(P,r)v(s)} ≤0 for all s ∈S . Therefore, checking
robust feasibility requires to solve a maximization problem. For properly structured uncertainty sets,
a closed form solution can be derived, as we shall see in the sequel. As standard in the robust RL
literature [32, 15, 1], the remaining of this work focuses on uncertainty sets centered around a known
nominal model. Formally, given P0 (resp. r0) a nominal transition kernel (resp. reward function), we
consider uncertainty sets of the form (P0 + P) × (r0 + R). The size of P × R quantiﬁes our level
of uncertainty or alternatively, the desired degree of robustness.
Reward-robust and regularized MDPs: an equivalence.
We now focus on reward-robust MDPs,
i.e., robust MDPs with U = {P0} × (r0 + R). Thm. 3.1 establishes that reward-robust MDPs are
in fact regularized MDPs whose regularizer is given by a support function. Its proof can be found
in Appx. A.2. This result brings two take-home messages: (i) policy regularization is equivalent to
reward uncertainty; (ii) policy iteration on reward-robust MDPs has the same convergence rate as
regularized MDPs, which in turn is the same as standard MDPs [10].
Theorem 3.1 (Reward-robust MDP). Assume that U = {P0} × (r0 + R). Then, for any policy
π ∈∆S
A, the robust value function vπ,U is the optimal solution of the convex optimization problem:
max
v∈RS⟨v, µ0⟩s. t. v(s) ≤T π
(P0,r0)v(s) −σRs(−πs) for all s ∈S .
Thm. 3.1 clearly highlights a convex regularizer Ωs(πs) := σRs(−πs)
∀s ∈S. We thus recover a
regularized MDP by setting [T π,Ωv](s) = T π
(P0,r0)v(s) −σRs(−πs)
∀s ∈S. In particular, when
Rs is a ball of radius αr
s, the support function (or regularizer) can be written in closed form as
Ωs(πs) := αr
s∥πs∥, which is strongly convex. We formalize this below (see proof in Appx. A.3).
Corollary 3.1. Let π ∈∆S
A and U = {P0} × (r0 + R). Further assume that for all s ∈S, the
reward uncertainty set at s is Rs := {rs ∈RA : ∥rs∥≤αr
s}. Then, the robust value function vπ,U
is the optimal solution of the convex optimization problem:
max
v∈RS⟨v, µ0⟩s. t. v(s) ≤T π
(P0,r0)v(s) −αr
s∥πs∥for all s ∈S .
While regularization induces reward-robustness, Thm. 3.1 and Cor. 3.1 suggest that, on the other hand,
speciﬁc reward-robust MDPs recover well-known policy regularization methods. In the following
section, we explicit the reward-uncertainty sets underlying some of these regularizers.
Related Algorithms.
Consider a reward uncertainty set of the form R := ×(s,a)∈X Rs,a. This
deﬁnes an (s, a)-rectangular R (a particular type of s-rectangular R) whose rectangles Rs,a are
independently deﬁned for each state-action pair. For the regularizers below, we derive appropriate
Rs,a-s that recover the same regularized value function. Detailed proofs are in Appx. A.4. There,
we also include a summary table that reviews the properties of some RL regularizers, as well as
our R2 function which we shall introduce later in Sec. 5. Note that the reward uncertainty sets here
depend on the policy. This is due to the fact that standard regularizers are deﬁned over the policy
space and not at each state-action pair. It similarly explains why the reward modiﬁcation induced
by regularization does not apply to the original reward function, as already mentioned in Sec. 2.
5

Negative Shannon entropy. Let RNS
s,a(π) := [ln (1/πs(a)) , +∞)
∀(s, a) ∈X. The associated sup-
port function enables to write:
σRNS
s (π)(−πs) =
max
r(s,·):r(s,a′)∈RNS
s,a′(π),a′∈A
X
a∈A
−r(s, a)πs(a) =
X
a∈A
πs(a) ln(πs(a)),
where the last equality comes from maximizing −r(s, a) over [ln (1/πs(a)) , +∞) for each a ∈A.
We thus recover the negative Shannon entropy Ω(πs) = P
a∈A πs(a) ln(πs(a)) [13].
Kullback-Leibler divergence. Given an action distribution 0 < d ∈∆A, let RKL
s,a(π) := ln (d(a)) +
RNS
s,a(π)
∀(s, a) ∈X. It amounts to translating the interval RNS
s,a by the given constant. Similarly
writing the support function yields Ω(πs) = P
a∈A πs(a) ln (πs(a)/d(a)), which is exactly the KL
divergence [34].
Negative Tsallis entropy. Letting RT
s,a(π) := [(1−πs(a))/2, +∞)
∀(s, a) ∈X, we recover the
negative Tsallis entropy Ω(πs) = 1
2(∥πs∥2 −1) [20].
Policy-gradient for reward-robust MDPs.
The equivalence between reward-robust and regular-
ized MDPs leads us to wonder whether we can employ policy-gradient [37] on reward-robust MDPs
using regularization. The following result establishes that a policy-gradient theorem can indeed be
established for reward-robust MDPs (see proof in Appx. A.5).
Proposition 3.2. Assume that U = {P0} × (r0 + R) with Rs = {rs ∈RA : ∥rs∥≤αr
s}. Then, the
gradient of the reward-robust objective JU(π) := ⟨vπ,U, µ0⟩is given by
∇JU(π) = E(s,a)∼µπ

∇ln πs(a)

qπ,U(s, a) −αr
s
πs(a)
∥πs∥

,
where µπ is the occupancy measure under the nominal model P0 and policy π.
Although Prop. 3.2 is an application of [10, Appx. D.3] for a speciﬁc regularized MDP, its reward-
robust formulation is novel and suggests another simpliﬁcation of robust methods. Indeed, previous
works that exploit policy-gradient on robust MDPs involve the occupancy measure of the worst-
case model [21], whereas our result sticks to the nominal. In practice, Prop. 3.2 enables to learn
a robust policy by sampling transitions from the nominal model instead of all uncertain models.
This has a twofold advantage: (i) it avoids an additional computation of the minimum as done in
[29, 21, 7], where the authors sample next-state transitions and rewards based on all parameters from
the uncertainty set, then update a policy based on the worst outcome; (ii) it releases from restricting
to ﬁnite uncertainty sets. In fact, our regularizer accounts for robustness regardless of the sampling
procedure, whereas the parallel simulations of [29, 21, 7] require the uncertainty set to be ﬁnite.
Technical difﬁculties are yet to be addressed for generalizing our result to transition-uncertain MDPs,
because of the interdependence between the regularizer and the value function (see Secs. 4-5). We
detail more on this issue in Appx. A.5.
4
General robust MDPs
Now that we have established policy regularization as a reward-robust problem, we would like to
study the opposite question: can any robust MDP with both uncertain reward and transition be solved
using regularization instead of robust optimization? If so, is the regularization function easy to
determine? In this section, we answer positively to both questions for properly deﬁned robust MDPs.
This greatly facilitates robust RL, as it avoids the increased complexity of robust planning algorithms
while still reaching robust performance.
The following theorem establishes that similarly to reward-robust MDPs, robust MDPs can be
formulated through regularization (see proof in Appx. B.1). Although the regularizer is also a support
function in that case, it depends on both the policy and the value objective, which may further explain
the difﬁculty of dealing with robust MDPs.
Theorem 4.1 (General robust MDP). Assume that U = (P0 + P) × (r0 + R). Then, for any policy
π ∈∆S
A, the robust value function vπ,U is the optimal solution of the convex optimization problem:
max
v∈RS⟨v, µ0⟩s. t. v(s) ≤T π
(P0,r0)v(s) −σRs(−πs) −σPs(−γv · πs) for all s ∈S,
(2)
where [v · πs](s′, a) := v(s′)πs(a)
∀(s′, a) ∈X.
6

The upper-bound in the inequality constraint (2) is of the same spirit as the regularized Bellman
operator: the ﬁrst term is a standard, non-regularized Bellman operator on the nominal model (P0, r0)
to which we subtract a policy and value-dependent function playing the role of regularization. This
function reminds that of [6, Thm. 3.1] also coming from conjugacy. This is the only similarity
between both regularizers: in [6], the Legendre-Fenchel transform is applied on a different type of
function and results in a regularization term that has no closed form but can only be bounded from
above. Moreover, the setup considered there is different since it studies distributionally robust MDPs.
As such, it involves general convex optimization, whereas we focus on the robust formulation of an LP.
The support function further simpliﬁes when the uncertainty set is a ball, as shown below. Yet, the
dependence of the regularizer on the value function prevents us from readily applying the tool-set of
regularized MDPs. We shall study the properties of this new regularization function in Sec. 5.
Corollary 4.1. Assume that U = (P0 + P) × (r0 + R) with Ps := {Ps ∈RX : ∥Ps∥≤αP
s } and
Rs := {rs ∈RA : ∥rs∥≤αr
s} for all s ∈S. Then, the robust value function vπ,U is the optimal
solution of the convex optimization problem:
max
v∈RS⟨v, µ0⟩s. t. v(s) ≤T π
(P0,r0)v(s) −αr
s∥πs∥−αP
s γ∥v∥∥πs∥for all s ∈S .
(3)
One can actually take two different norms for the reward and the transition uncertainty sets. Similarly,
Cor. 4.1 can be rewritten with an arbitrary norm, which would reveal a dual norm ∥·∥∗instead of
∥·∥in Eq. (3) (see proof in Appx. B.2). Here, we restrict our statement to the ℓ2-norm for notation
convenience only, the dual norm of ℓ2 being ℓ2 itself. Thus, our regularization function recovers a
robust value function independently of the chosen norm, which extends previous results from [15, 11].
Indeed, Ho et al. [15] lighten complexity of robust planning for the ℓ1-norm only, while Grand-
Clément & Kroer [11] focus on KL and ℓ2 ball-constrained uncertainty sets. Both works rely on the
speciﬁc structure induced by the divergence they consider to derive more efﬁcient robust Bellman
updates. Differently, our method circumvents these updates using a generic, problem-independent
regularization function while still encompassing s-rectangular uncertainty sets as in [15, 11].
5
R2 MDPs
In Sec. 4, we showed that for general robust MDPs, the optimization constraint involves a regular-
ization term that depends on the value function itself. This adds a difﬁculty to the reward-robust
case where the regularization only depends on the policy. Yet, we provided an explicit regularizer for
general robust MDPs that are ball-constrained. In this section, we introduce R2 MDPs, an extension
of regularized MDPs that combines policy and value regularization. The core idea is to further
regularize the Bellman operators with a value-dependent term that recovers the support functions
we derived from the robust optimization problems of Secs. 3-4.
Deﬁnition 5.1 (R2 Bellman operators). For all v ∈RS, deﬁne Ωv,R2 : ∆A →R as Ωv,R2(πs) :=
∥πs∥(αr
s + αP
s γ∥v∥). The R2 Bellman evaluation and optimality operators are deﬁned as
[T π,R2v](s) := T π
(P0,r0)v(s) −Ωv,R2(πs)
∀s ∈S,
[T ∗,R2v](s) := max
π∈∆S
A
[T π,R2v](s) = Ω∗
v,R2(qs)
∀s ∈S .
For any function v ∈RS, the associated unique greedy policy is deﬁned as
πs = arg max
πs∈∆A T π,R2v(s) = ∇Ω∗
v,R2(qs),
∀s ∈S,
that is, in vector form, π = ∇Ω∗
v,R2(q) =: GΩR2 (v) ⇐⇒T π,R2v = T ∗,R2v.
The R2 Bellman evaluation operator is not linear because of the functional norm appearing in the
regularization function. Yet, under the following assumption, it is contracting and we can apply
Banach’s ﬁxed point theorem to deﬁne the R2 value function.
Assumption 5.1 (Bounded radius). For all s ∈S, there exists ϵs > 0 such that
αP
s ≤min




1 −γ −ϵs
γ
p
|S|
;
min
uA∈RA
+,∥uA∥=1
vS∈RS
+,∥vS∥=1
u⊤
AP0(·|s, ·)vS



.
7

Asm. 5.1 requires to upper bound the ball radius of transition uncertainty sets. The ﬁrst term in the
minimum is needed for establishing contraction of R2 Bellman operators (item (iii) in Prop. 5.1),
while the second one is used for ensuring monotonicity (item (i) in Prop. 5.1). We remark that the
former depends on the original discount factor γ: radius αP
s must be smaller as γ tends to 1 but can
arbitrarily grow as γ decreases to 0, without altering contraction. Indeed, larger γ implies longer
time horizon and higher stochasticity, which explains why we need tighter level of uncertainty then.
Otherwise, value and policy regularization seem unable to handle the mixed effects of parameter
and stochastic uncertainties. The additional dependence on the state-space size comes from the
ℓ2-norm chosen for the ball constraints. In fact, for any ℓp-norm of dual ℓq, |S|
1
q replaces
p
|S|
in the denominator, so it becomes independent of |S| as (p, q) tends to (1, ∞) (see Appx. C.1).
Although we recognize a generalized Rayleigh quotient-type problem in the second minimum [28],
its interpretation in our context remains unclear. Asm. 5.1 enables the R2 Bellman operators to admit
a unique ﬁxed point, among other nice properties. We formalize this below (see proof in Appx. C.1).
Proposition 5.1. Suppose that Asm. 5.1 holds. Then, we have the following properties:
(i) Monotonicity: For all v1, v2 ∈RS such that v1 ≤v2, we have T π,R2v1 ≤T π,R2v2 and
T ∗,R2v1 ≤T ∗,R2v2.
(ii) Sub-distributivity: For all v1 ∈RS, c ∈R, we have T π,R2(v1 + c1S) ≤T π,R2v1 + γc1S and
T ∗,R2(v1 + c1S) ≤T ∗,R2v1 + γc1S, ∀c ∈R.
(iii) Contraction:
Let ϵ∗
:=
mins∈S ϵs
>
0.
Then,
for all v1, v2
∈
RS,
∥T π,R2v1 −T π,R2v2∥∞≤(1 −ϵ∗)∥v1 −v2∥∞and ∥T ∗,R2v1 −T ∗,R2v2∥∞≤(1 −ϵ∗)∥v1 −v2∥∞.
We should note that the contracting coefﬁcient in Prop. 5.1 is different from the original discount
factor γ, since here we have 1 −ϵ∗. Yet, as Asm. 5.1 suggests it, an intrinsic dependence between γ
and ϵ∗makes the R2 Bellman updates similar to the standard ones: when γ tends to 0, the value of
ϵ∗required for Asm. 5.1 to hold increases, which makes the contracting coefﬁcient 1 −ϵ∗tend to 0
as well, i.e., the two contracting coefﬁcients behave similarly. The contraction of both R2 Bellman
operators ﬁnally leads us to introduce the R2 value functions.
Deﬁnition 5.2 (R2 value functions). (i) The R2 value function vπ,R2 is deﬁned as the unique ﬁxed
point of the R2 Bellman evaluation operator: vπ,R2 = T π,R2vπ,R2. The associated q-function is
qπ,R2(s, a) = r0(s, a)+γ⟨P0(·|s, a), vπ,R2⟩. (ii) The R2 optimal value function v∗,R2 is deﬁned as the
unique ﬁxed point of the R2 Bellman optimal operator: v∗,R2 = T ∗,R2v∗,R2. The associated q-function
is q∗,R2(s, a) = r0(s, a) + γ⟨P0(·|s, a), v∗,R2⟩.
The monotonicity of the R2 Bellman operators plays a key role in reaching an optimal R2 policy, as
we show in the following. A proof can be found in Appx. C.2.
Theorem 5.1 (R2 optimal policy). The greedy policy π∗,R2 = GΩR2 (v∗,R2) is the unique optimal
R2 policy, i.e., for all π ∈∆S
A, vπ∗,R2 = v∗,R2 ≥vπ,R2.
Remark 5.1. An optimal R2 policy may be stochastic. This is due to the fact that our R2 MDP
framework builds upon the general s-rectangularity assumption. Robust MDPs with s-rectangular un-
certainty sets similarly yield an optimal robust policy that is stochastic [40, Table 1]. Nonetheless, the
R2 MDP formulation recovers a deterministic optimal policy in the more speciﬁc (s, a)-rectangular
case, which is in accordance with the robust MDP setting (see proof in Appx. C.3). 4
Algorithm 1: R2 MPI
Result: πk+1, vk+1
Initialize vk ∈RS;
while not converged do
πk+1 ←GΩR2 (vk);
vk+1 ←(T πk+1,R2)mvk;
end
All of the results above ensure convergence of MPI in R2 MDPs.
We call that method R2 MPI and provide its pseudo-code in Alg. 1.
The convergence proof follows the same lines as in [30]. More-
over, the contracting property of the R2 Bellman operator ensures
the same convergence rate as in standard and robust MDPs, i.e., a
geometric convergence rate. On the other hand, R2 MPI reduces
the computational complexity of robust MPI by avoiding to solve
a max-min problem at each iteration, as this can take polynomial
time for general convex programs. Advantageously, the only
optimization involved in R2 MPI lies in the greedy step: it amounts to projecting onto the simplex,
4The stochasticity of an optimal entropy-regularized policy as in the examples of Sec. 3 is not contradicting.
Indeed, even though the corresponding uncertainty set is (s, a)-rectangular there, it is policy-dependent.
8

which can efﬁciently be performed in linear time [8]. Still, such projection is not even necessary in
the (s, a)-rectangular case: as mentioned in Rmk. 5.1, it then sufﬁces to choose a greedy action in
order to eventually achieve an optimal R2 value function.
6
Numerical Experiments
We aim to compare the computing time of R2 MPI with that of MPI [30] and robust MPI [18]. The
code is available at https://github.com/EstherDerman/r2mdp. To do so, we run experiments
on an Intel(R) Core(TM) i7-1068NG7 CPU @ 2.30GHz machine, which we test on a 5×5 grid-world
domain. In that environment, the agent starts from a random position and seeks to reach a goal state
in order to maximize reward. Thus, the reward function is zero in all states but two: one provides a
reward of 1 while the other gives 10. An episode ends when either one of those two states is attained.
The partial evaluation of each policy iterate is a building block of MPI. As a sanity check, we evaluate
the uniform policy through both R2 and robust policy evaluation (PE) sub-processes, to ensure that
the two value outputs coincide. For simplicity, we focus on an (s, a)-rectangular uncertainty set and
take the same ball radius α (resp. β) at each state-action pair for the reward function (resp. transition
function). Parameter values and other implementation details are deferred to Appx. D. We obtain the
same value for R2 PE and robust PE, which numerically conﬁrms Thm. 4.1. On the other hand, both
are strictly smaller than their non-robust, non-regularized counterpart, but as expected, they converge
to the standard value function when all ball radii tend to 0 (see Appx. D). More importantly, R2 PE
converges in 0.02 seconds, whereas robust PE takes 54.8 seconds to converge, i.e., 2740 times longer.
This complexity gap comes from the minimization problems being solved at each iteration of robust
PE, something that R2 PE avoids thanks to regularization. R2 PE still takes 2.5 times longer than its
standard, non-regularized counterpart, because of the additional computation of regularization terms.
Table 1 shows the time spent by each algorithm until convergence.
Table 1: Computing time (in sec.) of planning algorithms using vanilla, R2 and robust approaches.
Each cell displays the mean ± standard deviation obtained from 5 running seeds.
Vanilla
R2
Robust
PE
0.008 ± 0.
0.02 ± 0.
54.8 ± 1.2
MPI (m = 1)
0.01 ± 0.
0.03 ± 0.
118.6 ± 1.3
MPI (m = 4)
0.01 ± 0.
0.03 ± 0.
98.1 ± 4.1
We then study the overall MPI process for each approach. We know that in vanilla MPI, the greedy
step is achieved by simply searching over deterministic policies [30]. Since we focus our experiments
on an (s, a)-rectangular uncertainty set, the same applies to robust MPI [40] and to R2 MPI, as
already mentioned in Rmk. 5.1. We can see in Table 1 that the increased complexity of robust MPI
is even more prominent than its PE thread, as robust MPI takes 3953 (resp. 3270) times longer
than R2 MPI when m = 1 (resp. m = 4). Robust MPI with m = 4 is a bit more advantageous
than m = 1, as it needs less iterations (31 versus 67), i.e., less optimization solvers to converge.
Interestingly, for both m ∈{1, 4}, progressing from PE to MPI did not cost much more computing
time to either the vanilla or the R2 version: both take less than one second to run.
7
Related Work
Connections between regularization and robustness have been established in standard statistical
learning settings such as support vector machines [41], logistic regression [35] or maximum
likelihood estimation [19]. As stated in Sec. 1, these represent particular RL problems as they concern
a single-stage decision-making process. In that regard, the generalization of robustness-regularization
duality to sequential decision-making has seldom been studied in the RL literature.
Two works that view policy regularization from a robustness perspective are [16] and [9]. In [16],
regularization is applied on the dual objective instead of the primal. This has two shortcomings: (i)
It prevents from deriving regularized Bellman operators and dynamic programming methods; (ii)
The feasible set is that of occupancy measures, so the connection with standard policy regularization
remains unclear. Furthermore, their work focuses on reward robustness. Differently, Eysenbach &
Levine [9] address both reward and transition uncertainty by showing that policies with maximum
9

entropy regularization solve a particular type of robust MDP. Yet, their analysis separately treats the
uncertainty on P and r, which questions the robustness of the resulting policy when the whole model
(P, r) is adversarial. Moreover, the dual relation they establish between entropy regularization and
transition-robust MDPs is weak and applies to speciﬁc uncertainty sets. Both of these works treat
robustness as a side-effect of regularization more than an objective on its own, whereas we aim to do
the opposite, namely, use regularization to solve robust RL problems.
Derman & Mannor [6] similarly use regularization as a tool for achieving robust policies. Through
distributionally robust MDPs, they show upper and lower bounds between transition-robustness and
regularization. There again, duality is weak and reward uncertainty is not addressed. Moreover, since
the exact regularization term has no explicit form, it is usable through its upper bound only. Finally,
regularization is applied on the mean of several value functions vπ
( ˆ
Pi,r), where each ˆPi is a transition
model estimated from an episode run. Computing this quantity requires as many policy evaluations
as the number of model estimates available, which results in a linear complexity blowup at least.
Previous studies analyze robust planning algorithms to provide convergence guarantees. The works
[2, 27] propose robust value iteration, while [17, 40] introduce robust policy iteration. Kaufman &
Schaefer [18] generalize both schemes by proposing a robust MPI and determine the conditions under
which it converges. The polynomial time within which all these works guarantee a robust solution is
often insufﬁcient, as the complexity of a Bellman update grows cubically in the number of states [15].
In order to reduce the time complexity of robust planning algorithms, Ho et al. [15] propose two algo-
rithms that compute robust Bellman updates in O(|S||A| log(|S||A|)) operations for ℓ1-constrained
uncertainty sets. Advantageously, our regularization approach reduces each such update to its stan-
dard, non-robust complexity of O(|S||A|). Moreover, although Ho et al. [15] address both (s, a) and
s-rectangular uncertainty sets, they focus on transition uncertainty, whereas we tackle both reward
and transition uncertainties in the general s-rectangular case. Finally, their contribution relies on LP
formulations that necessitate restricting to the ℓ1-norm while our method applies to any norm. This
may come from the fact that our main Theorems (Thms. 3.1 and 4.1) use Fenchel-Rockafellar duality
[31, 4], a generalization of LP duality (see Appx. A.2 and B.1). More recently, Grand-Clément
& Kroer [11] propose a ﬁrst-order method to accelerate robust value iteration under s-rectangular
uncertainty sets that are either ellipsoidal or KL-constrained. To our knowledge, our study is the ﬁrst
one reducing the time complexity of robust planning when the uncertainty set is s-rectangular and
constrained with an arbitrary norm.
8
Conclusion and future work
In this work, we established a strong duality between robust MDPs and twice regularized MDPs.
This revealed that the regularized MDPs of [10] are in fact robust MDPs with uncertain reward,
which enabled us to derive a policy-gradient theorem for reward-robust MDPs. When extending this
robustness-regularization duality to general robust MDPs, we found that the regularizer depends on
the value function besides the policy. We thus introduced R2 MDPs, a generalization of regularized
MDPs with both policy and value regularization. The related R2 Bellman operators lead us to
propose a converging R2 MPI algorithm that achieves the optimal robust value function within
similar computing time as standard MPI.
This study settles the theoretical foundations for scalable robust RL. We should note that our results
naturally extend to continuous but compact action spaces in the same manner as standard MDPs do
[30]. Extension to inﬁnite state-space would be more involved because of the state-dependent regular-
izer in R2 MDPs. In fact, it would be interesting to study the R2 MDP setting under function approx-
imation, as such approximation would have a direct effect on the regularizer. Similarly, one could an-
alyze approximate dynamic programming for R2 MDPs in light of its robust analog [38, 1]. Although
our theory focused on planning, the R2 Bellman operators and their contracting properties open the
path to learning algorithms that can (i) use existing RL algorithms and robustify them by simply chang-
ing the regularizer; (ii) scale to deep learning settings. Apart from its practical effect, we believe our
work opens the path to more theoretical contributions in robust RL. For example, extending R2 MPI
to the approximate case [33] would be an interesting problem to solve because of the R2 evaluation
operator being non-linear. So would be a sample complexity analysis for R2 MDPs with a comparison
to robust MDPs [42]. Another line of research is to extend policy-gradient to R2 MDPs, as this would
avoid parallel learning of adversarial models [7, 39] and be very useful for continuous control.
10

Acknowledgements
We would like to thank Raphael Derman for his useful suggestions that improved the clarity of the
text. Thanks also to Stav Belogolovsky for reviewing a previous version of this paper. Funding in
direct support of this work: ISF grant.
References
[1] Badrinath, K. P. and Kalathil, D. Robust reinforcement learning using least squares policy
iteration with provable performance guarantees. In International Conference on Machine
Learning, pp. 511–520. PMLR, 2021.
[2] Bagnell, J. A., Ng, A. Y., and Schneider, J. G. Solving uncertain Markov decision processes.
2001.
[3] Bertsekas, D. P. Convex optimization theory. Athena Scientiﬁc Belmont, 2009.
[4] Borwein, J. and Lewis, A. S. Convex analysis and nonlinear optimization: theory and examples.
Springer Science & Business Media, 2010.
[5] Cen, S., Cheng, C., Chen, Y., Wei, Y., and Chi, Y. Fast global convergence of natural policy
gradient methods with entropy regularization. arXiv preprint arXiv:2007.06558, 2020.
[6] Derman, E. and Mannor, S. Distributional robustness and regularization in reinforcement
learning. ICML Workshop, 2020.
[7] Derman, E., Mankowitz, D., Mann, T., and Mannor, S. Soft-robust actor-critic policy-gradient.
AUAI press for Association for Uncertainty in Artiﬁcial Intelligence, pp. 208–218, 2018.
[8] Duchi, J., Shalev-Shwartz, S., Singer, Y., and Chandra, T. Efﬁcient projections onto the l
1-ball for learning in high dimensions. In Proceedings of the 25th international conference on
Machine learning, pp. 272–279, 2008.
[9] Eysenbach, B. and Levine, S. Maximum entropy RL (provably) solves some robust RL problems.
arXiv preprint arXiv:2103.06257, 2021.
[10] Geist, M., Scherrer, B., and Pietquin, O. A theory of regularized Markov decision processes. In
International Conference on Machine Learning, pp. 2160–2169. PMLR, 2019.
[11] Grand-Clément, J. and Kroer, C. Scalable ﬁrst-order methods for robust mdps. In Proceedings
of the AAAI Conference on Artiﬁcial Intelligence, volume 35, pp. 12086–12094, 2021.
[12] Haarnoja, T., Tang, H., Abbeel, P., and Levine, S. Reinforcement learning with deep energy-
based policies. In International Conference on Machine Learning, pp. 1352–1361. PMLR,
2017.
[13] Haarnoja, T., Zhou, A., Abbeel, P., and Levine, S. Soft actor-critic: Off-policy maximum
entropy deep reinforcement learning with a stochastic actor. In International Conference on
Machine Learning, pp. 1861–1870. PMLR, 2018.
[14] Hiriart-Urruty, J.-B. and Lemaréchal, C. Fundamentals of convex analysis. Springer Science &
Business Media, 2004.
[15] Ho, C. P., Petrik, M., and Wiesemann, W. Fast Bellman updates for robust MDPs. In Interna-
tional Conference on Machine Learning, pp. 1979–1988. PMLR, 2018.
[16] Husain, H., Ciosek, K., and Tomioka, R. Regularized policies are reward robust. In International
Conference on Artiﬁcial Intelligence and Statistics, pp. 64–72. PMLR, 2021.
[17] Iyengar, G. N. Robust dynamic programming. Mathematics of Operations Research, 30(2):
257–280, 2005.
[18] Kaufman, D. L. and Schaefer, A. J. Robust modiﬁed policy iteration. INFORMS Journal on
Computing, 25(3):396–410, 2013.
11

[19] Kuhn, D., Esfahani, P. M., Nguyen, V. A., and Shaﬁeezadeh-Abadeh, S. Wasserstein distri-
butionally robust optimization: Theory and applications in machine learning. In Operations
Research & Management Science in the Age of Analytics, pp. 130–166. INFORMS, 2019.
[20] Lee, K., Choi, S., and Oh, S. Sparse Markov decision processes with causal sparse Tsallis
entropy regularization for reinforcement learning. IEEE Robotics and Automation Letters, 3(3):
1466–1473, 2018.
[21] Mankowitz, D., Mann, T., Bacon, P.-L., Precup, D., and Mannor, S. Learning robust options. In
Proceedings of the AAAI Conference on Artiﬁcial Intelligence, volume 32, 2018.
[22] Mannor, S., Simester, D., Sun, P., and Tsitsiklis, J. N. Bias and variance approximation in value
function estimates. Management Science, 53(2):308–322, 2007.
[23] Mannor, S., Mebel, O., and Xu, H. Lightning does not strike twice: Robust MDPs with coupled
uncertainty. ICML, 2012.
[24] Mannor, S., Mebel, O., and Xu, H. Robust MDPs with k-rectangular uncertainty. Mathematics
of Operations Research, 41(4):1484–1509, 2016.
[25] Mensch, A. and Blondel, M. Differentiable dynamic programming for structured prediction and
attention. In International Conference on Machine Learning, pp. 3462–3471. PMLR, 2018.
[26] Nachum, O. and Dai, B. Reinforcement learning via Fenchel-Rockafellar duality. arXiv preprint
arXiv:2001.01866, 2020.
[27] Nilim, A. and El Ghaoui, L. Robust control of Markov decision processes with uncertain
transition matrices. Operations Research, 53(5):780–798, 2005.
[28] Parlett, B. N. The Rayleigh quotient iteration and some generalizations for nonnormal matrices.
Mathematics of Computation, 28(127):679–693, 1974.
[29] Pinto, L., Davidson, J., Sukthankar, R., and Gupta, A. Robust adversarial reinforcement learning.
In International Conference on Machine Learning, pp. 2817–2826. PMLR, 2017.
[30] Puterman, M. L. Markov decision processes: discrete stochastic dynamic programming. John
Wiley & Sons, 2014.
[31] Rockafellar, R. T. Convex analysis, volume 36. Princeton university press, 1970.
[32] Roy, A., Xu, H., and Pokutta, S. Reinforcement learning under model mismatch. NIPS, 2017.
[33] Scherrer, B., Ghavamzadeh, M., Gabillon, V., Lesner, B., and Geist, M. Approximate modiﬁed
policy iteration and its application to the game of Tetris. J. Mach. Learn. Res., 16:1629–1676,
2015.
[34] Schulman, J., Levine, S., Abbeel, P., Jordan, M., and Moritz, P. Trust region policy optimization.
In International conference on machine learning, pp. 1889–1897. PMLR, 2015.
[35] Shaﬁeezadeh-Abadeh, S., Esfahani, P. M., and Kuhn, D. Distributionally robust logistic
regression. NIPS, 2015.
[36] Shani, L., Efroni, Y., and Mannor, S. Adaptive trust region policy optimization: Global
convergence and faster rates for regularized MDPs. In Proceedings of the AAAI Conference on
Artiﬁcial Intelligence, volume 34, pp. 5668–5675, 2020.
[37] Sutton, R. S., McAllester, D. A., Singh, S. P., Mansour, Y., et al. Policy gradient methods
for reinforcement learning with function approximation. In NIPs, volume 99, pp. 1057–1063.
Citeseer, 1999.
[38] Tamar, A., Mannor, S., and Xu, H. Scaling up robust MDPs using function approximation. In
International Conference on Machine Learning, pp. 181–189. PMLR, 2014.
[39] Tessler, C., Efroni, Y., and Mannor, S. Action robust reinforcement learning and applications in
continuous control. In International Conference on Machine Learning, pp. 6215–6224. PMLR,
2019.
12

[40] Wiesemann, W., Kuhn, D., and Rustem, B. Robust Markov decision processes. Mathematics of
Operations Research, 38(1):153–183, 2013.
[41] Xu, H., Caramanis, C., and Mannor, S. Robustness and regularization of support vector
machines. Journal of machine learning research, 10(7), 2009.
[42] Yang, W. and Zhang, Z. Non-asymptotic performances of Robust Markov Decision Processes.
arXiv preprint arXiv:2105.03863, 2021.
13

Checklist
1. For all authors...
(a) Do the main claims made in the abstract and introduction accurately reﬂect the paper’s
contributions and scope? [Yes] The main contributions are consistently listed in the
abstract and introduction. These are: Showing that regularized MDPs are a particular
instance of robust MDPs with uncertain reward (Sec. 3); Extending this relationship to
MDPs with uncertain transitions (Sec. 4); Generalizing regularized MDPs to R2 MDPs
(Sec. 5).
(b) Did you describe the limitations of your work? [Yes] These are described when
discussing Prop. 3.2 and Asm. 5.1, near the corresponding statements.
(c) Did you discuss any potential negative societal impacts of your work? [N/A]
(d) Have you read the ethics review guidelines and ensured that your paper conforms to
them? [Yes]
2. If you are including theoretical results...
(a) Did you state the full set of assumptions of all theoretical results? [Yes] Assumptions
are introduced by "Assume that..." or "If..." in each theoretical result.
(b) Did you include complete proofs of all theoretical results? [Yes] All proofs can be
found in the Appendix. At each theoretical statement in the text body, we precisely
refer to the corresponding proof.
3. If you ran experiments...
(a) Did you include the code, data, and instructions needed to reproduce the main ex-
perimental results (either in the supplemental material or as a URL)? [Yes] The
code is available at https://github.com/EstherDerman/r2mdp, which includes
all instructions needed to reproduce the results.
(b) Did you specify all the training details (e.g., data splits, hyperparameters, how they
were chosen)? [Yes] These are included in the experiments section (Sec. 6) and
detailed in Appx. D.
(c) Did you report error bars (e.g., with respect to the random seed after running exper-
iments multiple times)? [Yes] Table 1 displays mean ± standard deviation for each
type of experiment. The ﬁgure plots in Appx. D also include error bars.
(d) Did you include the total amount of compute and the type of resources used (e.g., type
of GPUs, internal cluster, or cloud provider)? [Yes] See Sec. 6.
4. If you are using existing assets (e.g., code, data, models) or curating/releasing new assets...
(a) If your work uses existing assets, did you cite the creators? [N/A]
(b) Did you mention the license of the assets? [N/A]
(c) Did you include any new assets either in the supplemental material or as a URL? [N/A]
(d) Did you discuss whether and how consent was obtained from people whose data you’re
using/curating? [N/A]
(e) Did you discuss whether the data you are using/curating contains personally identiﬁable
information or offensive content? [N/A]
5. If you used crowdsourcing or conducted research with human subjects...
(a) Did you include the full text of instructions given to participants and screenshots, if
applicable? [N/A]
(b) Did you describe any potential participant risks, with links to Institutional Review
Board (IRB) approvals, if applicable? [N/A]
(c) Did you include the estimated hourly wage paid to participants and the total amount
spent on participant compensation? [N/A]
14

Appendix: Twice regularized MDPs and the equivalence between robustness
and regularization
This appendix provides proofs for all of the results stated in the paper. We ﬁrst recall the following
theorem used in the sequel and referred to as Fenchel-Rochafellar duality [4, Thm 3.3.5].
Theorem (Fenchel-Rockafellar duality). Let X, Y two Euclidean spaces, f : X →R and g : Y →R
two proper, convex functions, and A : X →Y a linear mapping such that 0 ∈core(dom(g) −
A(dom(f))).5 Then, it holds that
min
x∈X f(x) + g(Ax) = max
y∈Y −f ∗(−A∗y) −g∗(y).
(4)
A
Reward-Robust MDPs
A.1
Proof of Proposition 3.1
Proposition. For any policy π ∈∆S
A, the robust value function vπ,U is the optimal solution of the
robust optimization problem:
max
v∈RS⟨v, µ0⟩s. t. v ≤T π
(P,r)v for all (P, r) ∈U.
(PU)
Proof. Let v∗an optimal point of (PU).
By deﬁnition of the robust value function, vπ,U =
T π,Uvπ,U = min(P,r)∈U T π
(P,r)vπ,U. In particular, vπ,U ≤T π
(P,r)vπ,U for all (P, r) ∈U, so the
robust value is feasible and by optimality of v∗, we get ⟨v∗, µ0⟩≥⟨vπ,U, µ0⟩. Now, we aim to show
that any feasible v ∈RS satisﬁes v ≤vπ,U. Let an arbitrary ϵ > 0. Then, there exists (Pϵ, rϵ) ∈U
such that
T π,Uvπ,U + ϵ > T π
(Pϵ,rϵ)vπ,U.
(5)
This yields:
v −vπ,U = v −T π,Uvπ,U
[vπ,U = T π,Uvπ,U]
< v + ϵ −T π
(Pϵ,rϵ)vπ,U
[By Eq. (5)]
≤T π,Uv + ϵ −T π
(Pϵ,rϵ)vπ,U
[v is feasible for (PU)]
≤T π,Uv + ϵ −T π,Uvπ,U
[T π,Uu =
min
(P,r)∈U T π
(P,r)u ≤T π
(Pϵ,rϵ)u
∀u ∈RS]
= T π,U(v −vπ,U) + ϵ.
Thus, v −vπ,U ≤T π,U(v −vπ,U) + ϵ, which we iteratively apply as follows:
v −vπ,U ≤T π,U(v −vπ,U) + ϵ
≤T π,U(T π,U(v −vπ,U) + ϵ) + ϵ
[By [27], u ≤w =⇒T π,Uu ≤T π,Uw]
= (T π,U)2(v −vπ,U) + γϵ + ϵ
≤(T π,U)2(T π,U(v −vπ,U) + ϵ) + γϵ + ϵ
...
≤(T π,U)n+1(v −vπ,U) +
n
X
k=0
γkϵ
= (T π,U)n+1v −vπ,U +
n
X
k=0
γkϵ.
[vπ,U = T π,Uvπ,U]
Setting n →∞yields v −vπ,U ≤
ϵ
1−γ . Since both ϵ > 0 and v were taken arbitrarily, v∗≤vπ,U,
while we have already shown that ⟨v∗, µ0⟩≥⟨vπ,U, µ0⟩. By positivity of the probability distribution
µ0, it results that ⟨v∗, µ0⟩= ⟨vπ,U, µ0⟩, and since µ0 > 0, we obtain vπ,U = v∗.
5Given C ⊆RS, we say that x ∈core(C) if for all d ∈RS there exists a small enough t ∈R such that
x + td ∈C [4].
15

A.2
Proof of Theorem 3.1
Theorem (Reward-robust MDP). Assume that U = {P0} × (r0 + R). Then, for any policy π ∈∆S
A,
the robust value function vπ,U is the optimal solution of the convex optimization problem:
max
v∈RS⟨v, µ0⟩s. t. v(s) ≤T π
(P0,r0)v(s) −σRs(−πs) for all s ∈S .
Proof. For all s ∈S, deﬁne: F(s) := max(P,r)∈U {v(s) −rπ(s) −γP πv(s)}. It corresponds to
the robust counterpart of (PU) at s ∈S. Thus, the robust value function vπ,U is the optimal solution
of:
max
v∈RS⟨v, µ0⟩s. t. F(s) ≤0 for all s ∈S .
(6)
Based on the structure of the uncertainty set U = {P0}×(r0+R), we compute the robust counterpart:
F(s) =
max
r′∈r0+R {v(s) −r′π(s) −γP π
0 v(s)}
=
max
r′:r′=r0+r,r∈R {v(s) −r′π(s) −γP π
0 v(s)}
= max
r∈R {v(s) −(rπ
0 (s) + rπ(s)) −γP π
0 v(s)}
[(r0 + r)π = rπ
0 + rπ
∀π ∈∆S
A]
= max
r∈R {v(s) −rπ(s) −rπ
0 (s) −γP π
0 v(s)}
= max
r∈R
n
v(s) −rπ(s) −T π
(P0,r0)v(s)
o
[T π
(P0,r0)v(s) = rπ
0 (s) + γP π
0 v(s)]
= max
r∈R{−rπ(s)} + v(s) −T π
(P0,r0)v(s)
= max
r∈RX{−rπ(s) −δR(r′)} + v(s) −T π
(P0,r0)v(s)
= −min
r∈RX{rπ(s) + δR(r)} + v(s) −T π
(P0,r0)v(s)
= −min
r∈RX{⟨rs, πs⟩+ δR(r)} + v(s) −T π
(P0,r0)v(s).
[rπ(s) = ⟨rs, πs⟩]
By the rectangularity assumption, R = ×s∈SRs and for all r := (rs)s∈S ∈RX , we have δR(r) =
P
s′∈S δRs′ (rs′). As such,
F(s) = −min
r∈RX{⟨rs, πs⟩+
X
s′∈S
δRs′(rs′)} + v(s) −T π
(P0,r0)v(s)
= −min
r∈RX{⟨rs, πs⟩+ δRs(rs)} + v(s) −T π
(P0,r0)v(s),
where the last equality holds since the objective function is minimal if and only if rs ∈Rs.
We now aim to apply Fenchel-Rockafellar duality to the minimization problem. Let the function
f : RA →R deﬁned as rs 7→⟨rs, πs⟩, and consider the support function δRs : RA →R together
with the identity mapping IdA : RA →RA. Clearly, dom(f) = RA, dom(δRs) = Rs, and
dom(δRs) −IdA(dom(f)) = Rs −RA = RA. Therefore, core(dom(δRs) −A(dom(f))) =
core(RA) = RA and 0 ∈RA. We can thus apply Fenchel-Rockafellar duality: noting that IdA =
(IdA)∗and (δRs)∗(y) = σRs(y), we get
min
rs∈RA{f(rs) + δRs(rs)} = −min
y∈RA{f ∗(−y) + (δRs)∗(y)} = −min
y∈RA{f ∗(−y) + σRs(y)}.
It remains to compute
f ∗(−y) = max
rs∈RA −⟨rs, y⟩−⟨rs, πs⟩= max
rs∈RA⟨rs, −y −πs⟩=
0 if −y −πs = 0
+∞otherwise
,
and obtain
F(s) = min
y∈RA{f ∗(−y) + σRs(y)} + v(s) −T π
(P0,r0)v(s) = σRs(−πs) + v(s) −T π
(P0,r0)v(s).
We can thus rewrite the optimization problem (6) as:
max
v∈RS⟨v, µ0⟩s. t. σRs(−πs) + v(s) −T π
(P0,r0)v(s) ≤0 for all s ∈S,
which concludes the proof.
16

A.3
Proof of Corollary 3.1
Corollary. Let π ∈∆S
A and U = {P0} × (r0 + R). Further assume that for all s ∈S, the reward
uncertainty set at s is Rs := {rs ∈RA : ∥rs∥≤αr
s}. Then, the robust value function vπ,U is the
optimal solution of the convex optimization problem:
max
v∈RS⟨v, µ0⟩s. t. v(s) ≤T π
(P0,r0)v(s) −αr
s∥πs∥for all s ∈S .
Proof. We evaluate the support function:
σRs(−πs) =
max
rs∈RA:∥rs∥≤αrs
⟨rs, −πs⟩
(1)
= αr
s∥−πs∥= αr
s∥πs∥,
where equality (1) holds by deﬁnition of the dual norm. Applying Thm. 3.1, the robust value function
vπ,U is the optimal solution of: maxv∈RS⟨v, µ0⟩s. t. αr
s∥πs∥+v(s)−T π
(P0,r0)v(s) ≤0 for all s ∈S,
which concludes the proof.
Ball-constraint with arbitrary norm. In the case where reward ball-constraints are deﬁned according
to an arbitrary norm ∥·∥a with dual norm ∥·∥a∗, the support function becomes:
σRs(−πs) =
max
rs∈RA:∥rs∥a≤αrs
⟨rs, −πs⟩= αr
s∥−πs∥a∗= αr
s∥πs∥a∗.
A.4
Related Algorithms: Uncertainty sets from regularizers
Table 2: Summary table of existing policy regularizers and generalization to our R2 function.
Negative Shannon
KL divergence
Negative Tsallis
R2 function
Regularizer Ω
X
s∈S
πs(a) ln(πs(a))
X
s∈S
πs(a) ln
πs(a)
d(a)

1
2(∥πs∥2 −1)
∥πs∥(αr
s+αP
s γ∥v∥)
Conjugate Ω∗
ln
 X
a∈A
eqs(a)
!
ln
 X
a∈A
d(a)eqs(a)
!
1
2+1
2
X
a∈A
(qs(a)2−τ(qs)2)
Not in closed-form
Gradient ∇Ω∗
πs(a) =
eqs(a)
P
b∈A eqs(b)
πs(a) =
eqs(a)
P
b∈A eqs(b)
πs(a) = (qs(a)−τ(qs))+
Not in closed-form
Reward Uncer-
tainty
(s, a)-rectangular
(s, a)-rectangular
(s, a)-rectangular
s-rectangular
RNS
s,a(π)
=
h
ln

1
πs(a)

, +∞

ln (d(a)) + RNS
s,a(π)
1 −πs(a)
2
, +∞

B∥·∥(r0s, αr
s)
Transition Un-
certainty
(s, a)-rectangular
(s, a)-rectangular
(s, a)-rectangular
s-rectangular
{P0(·|s, a)}
{P0(·|s, a)}
{P0(·|s, a)}
B∥·∥(P0s, αP
s )
17

Negative Shannon entropy. Each (s, a)-reward uncertainty set is RNS
s,a(π) := [ln (1/πs(a)) , +∞). We
compute the associated support function:
σRNS
s (π)(−πs) =
max
rs∈RNS
s (π)⟨rs, −πs⟩
=
max
r(s,a′):r(s,a′)∈RNS
s,a′(π),a′∈A
X
a∈A
−r(s, a)πs(a)
=
max
r(s,a′):r(s,a′)≥ln(1/πs(a)),a′∈A −
X
a∈A
πs(a)r(s, a)
=
X
a∈A
πs(a) ln(πs(a)),
(7)
where the last equality results from the fact that πs ≥0, and −r(s, a)πs(a) is maximal when r(s, a)
is minimal. We thus obtain the negative Shannon entropy.
KL divergence. Similarly, given d ∈∆A, let RKL
s,a(π) := ln(d(a)) + RNS
s,a(π)
∀(s, a) ∈X. Then
σRKL
s (π)(−πs) =
max
r(s,a′):r(s,a′)∈RKL
s,a′(π),a′∈A
X
a∈A
−r(s, a)πs(a)
=
max
r(s,a′)+ln(d(a)):
r(s,a′)∈RNS
s,a′(π),a′∈A
X
a∈A
−r(s, a)πs(a)
=
max
r(s,a′):
r(s,a′)∈RNS
s,a′(π),a′∈A
X
a∈A
−(r(s, a) + ln(d(a))πs(a)
=
max
r(s,a′):
r(s,a′)∈RNS
s,a′(π),a′∈A
{−
X
a∈A
πs(a)r(s, a)} −
X
a∈A
πs(a) ln(d(a))
=
X
a∈A
πs(a) ln(πs(a)) −
X
a∈A
πs(a) ln(d(a)),
where the last equality uses Eq. (7).
We thus recover the KL divergence Ω(πs)
=
P
a∈A πs(a) ln (πs(a)/d(a)).
Negative Tsallis entropy. Given RT
s,a(π) :=
h
1−πs(a)
2
, +∞

∀(s, a) ∈X, we compute:
σRTs(π)(−πs) =
max
r(s,a′):r(s,a′)∈RT
s,a′(π),a′∈A
X
a∈A
−r(s, a)πs(a)
=
max
r(s,a′):r(s,a′)∈
h
1−πs(a′)
2
,+∞

,a′∈A
X
a∈A
−r(s, a)πs(a)
=
X
a∈A
−1 −πs(a)
2
πs(a)
(8)
= −1
2
X
a∈A
πs(a) + 1
2
X
a∈A
πs(a)2 = −1
2 + 1
2∥πs∥2,
where Eq. (8) also comes from the fact that πs ≥0, and −r(s, a)πs(a) is maximal when r(s, a) is
minimal. We thus obtain the negative Tsallis entropy Ω(πs) = 1
2(∥πs∥2 −1).
The reward uncertainty sets associated to both KL and Shannon entropy are similar, as the former
amounts to translating the latter by a negative constant (translation to the left). As such, both yield
reward values that can be either positive or negative. This is not the case of the negative Tsallis, as its
minimal reward is 0, attained for a deterministic action policy, i.e., when πs(a) = 1.
Table 2 summarizes the properties of each regularizer.
For the Tsallis entropy, we denote
by τ
: RA
→R the function qs
7→
P
a∈A(qs) qs(a)−1
|A(qs)|
, where A(qs) ⊆A is a subset
of actions: A(qs) = {a ∈A : 1 + iqs(a(i)) > Pi
j=0 qs(a(j)), i ∈{1, · · · , |A|}}, and
a(i) is the action with the i-th maximal value [20].
18

A.5
Proof of Proposition 3.2
Proposition. Assume that U = {P0} × (r0 + R) with Rs = {rs ∈RA : ∥rs∥≤αr
s}. Then, the
gradient of the reward-robust objective JU(π) := ⟨vπ,U, µ0⟩is given by
∇JU(π) = E(s,a)∼µπ

∇ln πs(a)

qπ,U(s, a) −αr
s
πs(a)
∥πs∥

,
where µπ is the occupancy measure under the nominal model P0 and policy π.
We prove the following more general result. To establish Prop. 3.2, we then set αP
s = 0 and apply
Thm. 4.1 to replace vπ,R2 = vπ,U and qπ,R2 = qπ,U.
Theorem. Set Ωv,R2(πs) := ∥πs∥(αr
s + αP
s γ∥v∥). Then, the gradient of the R2 objective JR2(π) :=
⟨vπ,R2, µ0⟩is given by
∇JR2(π) = Es∼dµ0,π
"X
a∈A
πs(a)∇ln πs(a)qπ,R2(s, a) −∇Ωv,R2(πs)
#
,
where dµ0,π := µ⊤
0 (IS −γP π
0 )−1, with µ0 ∈RS ×1 the initial state distribution.
Proof. By linearity of the gradient operator, ∇JR2(π) = ⟨∇vπ,R2, µ0⟩. We thus need to compute
∇vπ,R2. Using the ﬁxed point property of vπ,R2 w.r.t. the R2 Bellman operator yields:
∇vπ,R2(s)
=∇

rπ
0 (s) + γP π
0 vπ,R2(s) −Ωv,R2(πs)

=∇
 X
a∈A
πs(a)(r0(s, a) + γ⟨P0(·|s, a), vπ,R2⟩) −Ωv,R2(πs)
!
=
X
a∈A
∇πs(a)

r0(s, a) + γ⟨P0(·|s, a), vπ,R2⟩

+ γ
X
a∈A
πs(a)⟨P0(·|s, a), ∇vπ,R2⟩−∇Ωv,R2(πs)
[Linearity of gradient and product rule]
=
X
a∈A
∇πs(a)qπ,R2(s, a) + γ
X
a∈A
πs(a)⟨P0(·|s, a), ∇vπ,R2⟩−∇Ωv,R2(πs)
[qπ,R2(s, a) = r0(s, a) + γ⟨P0(·|s, a), vπ,R2⟩]
=
X
a∈A
πs(a)(∇ln πs(a)qπ,R2(s, a) + γ⟨P0(·|s, a), ∇vπ,R2⟩) −∇Ωv,R2(πs)
[∇πs = πs∇ln(πs)]
=
X
a∈A
πs(a)(∇ln πs(a)qπ,R2(s, a) −∇Ωv,R2(πs) + γ⟨P0(·|s, a), ∇vπ,R2⟩).
Thus, the components of ∇vπ,R2 are the non-regularized value functions corresponding to the modiﬁed
reward R(s, a) := ∇ln πs(a)qπ,R2(s, a) −∇Ωv,R2(πs). By the ﬁxed point property of the standard
Bellman operator, it results that:
∇vπ,R2(s) = (IS −γP π
0 )−1
 X
a∈A
π·(a)(∇ln π·(a)qπ,R2(·, a) −∇Ωv,R2(π·))
!
(s)
and
∇JR2(π) =
X
s∈S
µ0(s)∇vπ,R2(s)
=
X
s∈S
µ0(s)(IS −γP π
0 )−1
 X
a∈A
π·(a)(∇ln π·(a)qπ,R2(·, a) −∇Ωv,R2(π·))
!
(s)
=
X
s∈S
dµ0,π(s)
 X
a∈A
πs(a)∇ln πs(a)qπ,R2(s, a) −∇Ωv,R2(πs)
!
,
by deﬁnition of dµ0,π.
19

The subtraction by ∇Ωv,R2(πs) also appears in [10]. However, here, the gradient includes partial
derivatives that depend on both the policy and the value itself. Let’s try to compute the gradient of
the double regularizer Ωv,R2(πs) = ∥πs∥(αr
s + αP
s γ∥vπ,R2∥). By the chain-rule we have that:
∇Ωv,R2(πs) =
X
a∈A
∂Ωv,R2
∂πs(a)∇πs(a) +
X
s∈S
∂Ωv,R2
∂vπ,R2(s)
∇vπ,R2(s)
=
X
a∈A
(αr
s + αP
s γ∥vπ,R2∥)πs(a)
∥πs∥∇πs(a) +
X
s∈S
αP
s γ∥πs∥vπ,R2(s)
∥vπ,R2∥
∇vπ,R2(s)
=
X
a∈A
πs(a)
 
αr
s + αP
s γ∥vπ,R2∥
∥πs∥
∇πs(a) +
X
s∈S
αP
s γ∥πs∥qπ,R2(s, a)
∥vπ,R2∥
∇vπ,R2(s)
!
.
We remark here an interdependence between ∇Ωv,R2(πs) and ∇vπ,R2(s): computing the gradient
∇Ωv,R2(πs) requires to know ∇vπ,R2(s) and vice versa. There may be a recursion that still enables
to compute these gradients, which we leave for future work.
B
General robust MDPs
B.1
Proof of Theorem 4.1
Theorem (General robust MDP). Assume that U = (P0 + P) × (r0 + R). Then, for any policy
π ∈∆S
A, the robust value function vπ,U is the optimal solution of the convex optimization problem:
max
v∈RS⟨v, µ0⟩s. t. v(s) ≤T π
(P0,r0)v(s) −σRs(−πs) −σPs(−γv · πs) for all s ∈S,
where [v · πs](s′, a) := v(s′)πs(a)
∀(s′, a) ∈X.
Proof. The robust value function vπ,U is the optimal solution of:
max
v∈RS⟨v, µ0⟩s. t. F(s) ≤0 for all s ∈S,
(9)
where F(s) := max(P,r)∈U {v(s) −rπ(s) −γP πv(s)} is the robust counterpart of (PU) at s ∈S.
Let’s compute it based on the structure of the uncertainty set U = (P0 + P) × (r0 + R):
F(s) =
max
(P ′,r′)∈(P0+P)×(r0+R) {v(s) −r′π(s) −γP ′πv(s)}
=
max
P ′:P ′=P0+P,P ∈P
r′:r′=r0+r,r∈R
{v(s) −r′π(s) −γP ′πv(s)}
=
max
P ∈P,r∈R {v(s) −(rπ
0 (s) + rπ(s)) −γ(P π
0 + P π)v(s)}
[(P0 + P)π = P π
0 + P π,
(r0 + r)π = rπ
0 + rπ]
=
max
P ∈P,r∈R {v(s) −rπ
0 (s) −rπ(s) −γP π
0 v(s) −γP πv(s)}
=
max
P ∈P,r∈R
n
v(s) −T π
(P0,r0)v(s) −rπ(s) −γP πv(s)
o
[T π
(P0,r0)v(s) = rπ
0 (s) + γP π
0 v(s)]
= max
P ∈P{−γP πv(s)} + max
r∈R{−rπ(s)} + v(s) −T π
(P0,r0)v(s)
= −min
P ∈P{γP πv(s)} −min
r∈R{rπ(s)} + v(s) −T π
(P0,r0)v(s)
= −
min
P ∈RX × S{γP πv(s) + δP(P)} −min
r∈RX{rπ(s) + δR(r)}
+ v(s) −T π
(P0,r0)v(s)
= −
min
P ∈RX × S{γ⟨P π
s , v⟩+ δP(P)} −min
r∈RX{⟨rs, πs⟩+ δR(r)}
+ v(s) −T π
(P0,r0)v(s).
[P πv(s) = ⟨P π
s , v⟩, rπ(s) = ⟨rs, πs⟩]
20

As shown in the proof of Thm. 3.1, minr∈RX {⟨rs, πs⟩+ δR(r)} = minrs∈RA{⟨rs, πs⟩+ δRs(rs)}
thanks to the rectangularity assumption. Similarly, by rectangularity of the transition uncertainty set,
for all P := (Ps)s∈S ∈RX , we have δP(P) = P
s′∈S δPs′(Ps′). As such,
min
P ∈RX × S{γ⟨P π
s , v⟩+ δP(P)} =
min
P ∈RX × S{γ⟨P π
s , v⟩+
X
s′∈S
δPs′(Ps′)}
= min
Ps∈RX{γ⟨P π
s , v⟩+ δPs(Ps)},
where the last equality holds since the objective function is minimal if and only if Ps ∈Ps. Finally,
F(s) = −min
Ps∈RX{γ⟨P π
s , v⟩+ δPs(Ps)} −min
rs∈RA{⟨rs, πs⟩+ δRs(rs)} + v(s) −T π
(P0,r0)v(s).
Referring to the proof of Thm. 3.1, we know that −minr∈RX {⟨rs, πs⟩+ δR(r)} = σRs(−πs), so
F(s) = −min
Ps∈RX{γ⟨P π
s , v⟩+ δPs(Ps)} + σRs(−πs) + v(s) −T π
(P0,r0)v(s).
Let the matrix v · πs ∈RX deﬁned as [v · πs](s′, a) := v(s′)πs(a) for all (s′, a) ∈X. Further deﬁne
ϕ(Ps) := γ⟨P π
s , v⟩, which we can rewrite as ϕ(Ps) = γ⟨Ps, v · πs⟩. Then, we have that:
min
Ps∈RX{γ⟨P π
s , v⟩+ δPs(Ps)} = min
Ps∈RX{ϕ(Ps) + δPs(Ps)} = −min
B∈RX{ϕ∗(−B) + σPs(B)},
where the last equality results from Fenchel-Rockafellar duality and the fact that (δPs)∗= σPs. It
thus remains to compute the convex conjugate of ϕ:
ϕ∗(−B) = max
Ps∈RX {⟨Ps, −B⟩−ϕ(Ps)}
= max
Ps∈RX {⟨Ps, −B⟩−γ⟨Ps, v · πs⟩}
= max
Ps∈RX⟨Ps, −B −γv · πs⟩
=
0 if −B −γv · πs = 0
+∞otherwise,
which yields minB∈RX {ϕ∗(−B) + σPs(B)} = σPs(−γv · πs). Finally, the robust counterpart
rewrites as: F(s) = σPs(−γv · πs) + σRs(−πs) + v(s) −T π
(P0,r0)v(s), and plugging it into the
optimization problem (9) yields the desired result.
B.2
Proof of Corollary 4.1
Corollary. Assume that U = (P0 + P) × (r0 + R) with Ps := {Ps ∈RX : ∥Ps∥≤αP
s } and
Rs := {rs ∈RA : ∥rs∥≤αr
s} for all s ∈S. Then, the robust value function vπ,U is the optimal
solution of the convex optimization problem:
max
v∈RS⟨v, µ0⟩s. t. v(s) ≤T π
(P0,r0)v(s) −αr
s∥πs∥−αP
s γ∥v∥∥πs∥for all s ∈S .
Proof. As we already showed in Cor. 3.1, the support function of the reward uncertainty set is
σRs(−πs) = αr
s∥πs∥. For the transition uncertainty set, we similarly have:
σPs(−γv · πs) =
max
Ps∈RX :
∥Ps∥≤αP
s
⟨Ps, −γv · πs⟩
= αP
s ∥−γv · πs∥
= αP
s γ∥v · πs∥
= αP
s γ∥v∥∥πs∥.
[∥v · πs∥2 =
X
(s′,a)∈X
(v(s′)πs(a))2
=
X
s′∈S
v(s′)2 X
a∈A
πs(a)2 = ∥v∥2∥πs∥2]
21

Now we apply Thm. 3.1 and replace each support function by their explicit form to get that the robust
value function vπ,U is the optimal solution of:
max
v∈RS⟨v, µ0⟩s. t. v(s) ≤T π
(P0,r0)v(s) −αr
s∥πs∥−αP
s ∥πs∥· γ∥v∥for all s ∈S .
Ball-constraints with arbitrary norms. As seen in the proof of Thm. 3.1 and Cor. 3.1, for ball-
constrained rewards deﬁned with an arbitrary norm ∥·∥a of dual ∥·∥a∗, the corresponding support
function is σRs(−πs) = αr
s∥πs∥a∗. Similarly, for ball-constrained transitions based on a norm ∥·∥b
of dual ∥·∥b∗, we have:
σPs(−γv · πs) =
max
Ps∈RX :
∥Ps∥b≤αP
s
⟨Ps, −γv · πs⟩= αP
s ∥−γv · πs∥b∗,
in which case the robust value function vπ,U is the optimal solution of:
max
v∈RS⟨v, µ0⟩s. t. v(s) ≤T π
(P0,r0)v(s) −αr
s∥πs∥a∗−αP
s ∥−γv · πs∥b∗for all s ∈S .
C
R2 MDPs
C.1
Proof of Proposition 5.1
Proposition. Suppose that Asm. 5.1 holds. Then, we have the following properties:
(i) Monotonicity: For all v1, v2 ∈RS such that v1 ≤v2, we have T π,R2v1 ≤T π,R2v2 and
T ∗,R2v1 ≤T ∗,R2v2.
(ii) Sub-distributivity: For all v1 ∈RS, c ∈R, we have T π,R2(v1 + c1S) ≤T π,R2v1 + γc1S and
T ∗,R2(v1 + c1S) ≤T ∗,R2v1 + γc1S, ∀c ∈R.
(iii) Contraction:
Let ϵ∗
:=
mins∈S ϵs
>
0.
Then,
for all v1, v2
∈
RS,
∥T π,R2v1 −T π,R2v2∥∞≤(1 −ϵ∗)∥v1 −v2∥∞and ∥T ∗,R2v1 −T ∗,R2v2∥∞≤(1 −ϵ∗)∥v1 −v2∥∞.
Proof. Proof of (i). Consider the evaluation operator and let v1, v2 ∈RS such that v1 ≤v2. For all
s ∈S,
[T π,R2v1 −T π,R2v2](s)
= T π
(P0,r0)v1(s) −αr
s∥πs∥−αP
s γ∥v1∥∥πs∥
−(T π
(P0,r0)v2(s) −αr
s∥πs∥−αP
s γ∥v2∥∥πs∥)
= T π
(P0,r0)v1(s) −T π
(P0,r0)v2(s) + αP
s γ∥πs∥(∥v2∥−∥v1∥)
= γP π
0 (v1 −v2)(s) + αP
s γ∥πs∥(∥v2∥−∥v1∥)
= γ⟨πs, P0s(v1 −v2)⟩+ αP
s γ∥πs∥(∥v2∥−∥v1∥)
[∀v ∈RS, P π
0 v(s) =
X
(s′,a)∈X
πs(a)P0(s′|s, a)v(s′)
=
X
a∈A
πs(a)[P0sv](a) = ⟨πs, P0sv⟩]
= γ∥πs∥
 πs
∥πs∥, P0s(v1 −v2)

+ αP
s (∥v2∥−∥v1∥)

≤γ∥πs∥
 πs
∥πs∥, P0s(v1 −v2)

+ αP
s (∥v2 −v1∥)

[∀v, w ∈RS, ∥v∥−∥w∥≤|∥v∥−∥w∥| ≤∥v −w∥].
By Asm. 5.1, we also have
αP
s ≤
min
uA∈RA
+,∥uA∥=1
vS∈RS
+,∥vS∥=1
u⊤
AP0(·|s, ·)vS
=
min
uA∈RA
+,∥uA∥=1
vS∈RS
+,∥vS∥=1
⟨uA, P0(·|s, ·)vS⟩≤
 πs
∥πs∥, P0(·|s, ·) (v2 −v1)
∥v2 −v1∥

,
22

so that
[T π,R2v1 −T π,R2v2](s) ≤γ∥πs∥
 πs
∥πs∥, P0s(v1 −v2)

+
 πs
∥πs∥, P0(·|s, ·) (v2 −v1)
∥v2 −v1∥

∥v2 −v1∥

= γ∥πs∥
 πs
∥πs∥, P0s(v1 −v2)

+
 πs
∥πs∥, P0(·|s, ·)(v2 −v1)

= 0,
where we switch notations to designate P0(·|s, ·) = P0s ∈RA × S. This proves monotonicity.
Proof of (ii). We now prove the sub-distributivity of the evaluation operator. Let v ∈RS, c ∈R. For
all s ∈S,
[T π,R2(v + c1S)](s)
=[T π
(P0,r0)(v + c1S)](s) −αr
s∥πs∥−αP
s γ∥v + c1S∥∥πs∥
=T π
(P0,r0)v(s) + γc −αr
s∥πs∥−αP
s γ∥v + c1S∥∥πs∥
[T π
(P0,r0)(v + c1S) = T π
(P0,r0)v + γc1S]
≤T π
(P0,r0)v(s) + γc −αr
s∥πs∥−αP
s γ∥πs∥(∥v∥+ ∥c1S∥)
=T π
(P0,r0)v(s) + γc −αr
s∥πs∥−αP
s γ∥πs∥∥v∥
−αP
s γ∥πs∥∥c1S∥
=[T π,R2v](s) + γc −αP
s γ∥πs∥∥c1S∥
≤[T π,R2v](s) + γc.
[γ > 0, αP
s > 0, ∥·∥≥0]
Proof of (iii). We prove the contraction of a more general evaluation operator with ℓp regularization,
p ≥1. This will establish contraction of the R2 operator T π,R2 by simply setting p = 2. Deﬁne as q
the conjugate value of p, i.e., such that 1
p + 1
q = 1. As seen in the proof of Thm. 4.1, for balls that are
constrained according to the ℓp-norm ∥·∥p, the robust value function vπ,U is the optimal solution of:
max
v∈RS⟨v, µ0⟩s. t. v(s) ≤T π
(P0,r0)v(s) −αr
s∥πs∥q −αP
s ∥−γv · πs∥q for all s ∈S,
because ∥·∥q is the dual norm of ∥·∥p, and we can deﬁne the R2 operator accordingly:
[T π,R2
q
v](s) := T π
(P0,r0)v(s) −αr
s∥πs∥q −αP
s γ∥v · πs∥q
∀v ∈RS, s ∈S .
We make the following assumption:
Assumption (Aq). For all s ∈S, there exists ϵs > 0 such that αP
s ≤1−γ−ϵs
γ|S|
1
q .
23

Let v1, v2 ∈RS. For all s ∈S,
[T π,R2
q
v1](s) −[T π,R2
q
v2](s)

= | T π
(P0,r0)v1(s) −αr
s∥πs∥q −αP
s γ∥v1 · πs∥q
−(T π
(P0,r0)v2(s) −αr
s∥πs∥q −αP
s γ∥v2 · πs∥q) |
=
T π
(P0,r0)v1(s) −T π
(P0,r0)v2(s)
 +
αP
s γ(∥v2 · πs∥q −∥v1 · πs∥q)

=
T π
(P0,r0)v1(s) −T π
(P0,r0)v2(s)
 + αP
s γ|∥v2 · πs∥q −∥v1 · πs∥q|
≤
T π
(P0,r0)v1(s) −T π
(P0,r0)v2(s)
 + αP
s γ∥v2 · πs −v1 · πs∥q
[∀A, B ∈RX , |∥A∥q −∥B∥q| ≤∥A −B∥q]
≤γ∥v1 −v2∥∞+ αP
s γ∥v2 · πs −v1 · πs∥q
[∥T π
(P0,r0)v1 −T π
(P0,r0)v2∥∞≤γ∥v1 −v2∥∞]
=γ∥v1 −v2∥∞+ αP
s γ∥(v2 −v1) · πs∥q
[∀v, w ∈RS, v · πs −w · πs = (v −w) · πs]
≤γ∥v1 −v2∥∞+ αP
s γ∥v2 −v1∥q
[∀v ∈RS, ∥v · πs∥q ≤∥v∥q]
≤γ∥v1 −v2∥∞+ αP
s γ|S|
1
q ∥v1 −v2∥∞
[∀v, w ∈RS, ∥v −w∥q ≤|S|
1
q ∥v −w∥∞]
=γ(1 + αP
s |S|
1
q )∥v1 −v2∥∞
≤γ

1 + 1 −γ −ϵs
γ

∥v1 −v2∥∞
[αP
s ≤1 −γ −ϵs
γ|S|
1
q
by Asm. (Aq)]
=(1 −ϵs)∥v1 −v2∥∞
≤(1 −ϵ∗)∥v1 −v2∥∞,
where ϵ∗:= mins∈S ϵs. Setting q = 2 and remarking that: (i) the ﬁrst bound in Asm. 5.1 recovers
Asm. (Aq); (ii) T π,R2
2
= T π,R2, establishes contraction of the R2 evaluation operator. For the
optimality operator, the proof is exactly the same as that of [10, Prop. 3], using Prop. 2.1.
C.2
Proof of Theorem 5.1
Theorem (R2 optimal policy). The greedy policy π∗,R2 = GΩR2 (v∗,R2) is the unique optimal R2 policy,
i.e., for all π ∈∆S
A, vπ∗,R2 = v∗,R2 ≥vπ,R2.
Proof. By strong convexity of the norm, the R2 function Ωv,R2 : πs 7→∥πs∥(αr
s + αP
s γ∥v∥) is
strongly convex in πs. As such, we can invoke Prop. 2.1 to state that the greedy policy π∗,R2 is the
unique maximizing argument for v∗,R2. Moreover, by construction,
T π∗,R2 ,R2v∗,R2 = T ∗,R2v∗,R2 = v∗,R2.
Supposing that Asm. 5.1 holds, the evaluation operator T π∗,R2 ,R2 is contracting and has a unique
ﬁxed point vπ∗,R2 ,R2. Therefore, v∗,R2 being also a ﬁxed point, we have vπ∗,R2 ,R2 = v∗,R2. It remains
to show the last inequality: the proof is exactly the same as that of [10, Thm. 1], and relies on the
monotonicity of the R2 operators.
C.3
Proof of Remark 5.1
Remark C.1. An optimal R2 policy may be stochastic. This is due to the fact that our R2 MDP
framework builds upon the general s-rectangularity assumption. Robust MDPs with s-rectangular un-
certainty sets similarly yield an optimal robust policy that is stochastic [40, Table 1]. Nonetheless, the
R2 MDP formulation recovers a deterministic optimal policy in the more speciﬁc (s, a)-rectangular
case, which is in accordance with the robust MDP setting.
24

Proof. In the (s, a)-rectangular case, the uncertainty set is structured as U = ×(s,a)∈X U(s, a), where
U(s, a) := P0(·|s, a) × r0(s, a) + P(s, a) × R(s, a). The robust counterpart of problem (PU) is:
F(s) = max
(P,r)∈U {v(s) −rπ(s) −γP πv(s)}
=
max
(P (·|s,a),r(s,a))∈P(s,a)×R(s,a) {v(s) −rπ
0 (s) −rπ(s) −γP π
0 v(s) −γP πv(s)}
=
max
(P (·|s,a),r(s,a))∈P(s,a)×R(s,a) {−rπ(s) −γP πv(s)} + v(s) −rπ
0 (s) −γP π
0 v(s)
=
max
r(s,a)∈R(s,a) {−rπ(s)} + γ
max
P (·|s,a)∈P(s,a) {−P πv(s)} + v(s) −T π
(P0,r0)v(s)
=
max
r(s,a)∈R(s,a)
(
−
X
a∈A
πs(a)r(s, a)
)
+ γ
max
P (·|s,a)∈P(s,a)
(
−
X
a∈A
πs(a)⟨P(·|s, a), v⟩
)
+ v(s) −T π
(P0,r0)v(s)
=
X
a∈A
πs(a)

max
r(s,a)∈R(s,a) {−r(s, a)} + γ
max
P (·|s,a)∈P(s,a) {⟨P(·|s, a), −v⟩}

+ v(s) −T π
(P0,r0)v(s).
In particular, if we have ball uncertainty sets P(s, a) := {P(·|s, a) ∈RS : ∥P(·|s, a)∥≤αP
s,a} and
R(s, a) := {r(s, a) ∈R : |r(s, a)| ≤αr
s,a} for all (s, a) ∈X, then we can explicitly compute the
support functions:
max
r(s,a):|r(s,a)|≤αrs,a
−r(s, a) = αr
s,a and
max
P (·|s,a):∥P (·|s,a)∥≤αP
s,a
⟨P(·|s, a), −v⟩= αP
s,a∥v∥.
Therefore, the robust counterpart rewrites as:
F(s) =
X
a∈A
πs(a)(αr
s,a + γαP
s,a∥v∥) + v(s) −T π
(P0,r0)v(s),
and the robust value function vπ,U is the optimal solution of the convex optimization problem:
max
v∈RS⟨v, µ0⟩s. t. v(s) ≤T π
(P0,r0)v(s) −
X
a∈A
πs(a)(αr
s,a + γαP
s,a∥v∥) for all s ∈S .
This derivation enables us to derive an R2 Bellman evaluation operator for the (s, a)-rectangular case.
Indeed, the R2 regularization function now becomes Ωv,R2(πs) := P
a∈A πs(a)(αr
s,a + γαP
s,a∥v∥),
which yields the following R2 operator:
[T π,R2v](s) := T π
(P0,r0)v(s) −Ωv,R2(πs),
∀s ∈S .
We aim to show that we can ﬁnd a deterministic policy πd ∈∆S
A such that [T πd,R2v](s) = [T ∗,R2v](s)
for all s ∈S. Given an arbitrary policy π ∈∆S
A, we ﬁrst rewrite:
[T π,R2v](s) = rπ
0 (s) + γP π
0 v(s) −Ωv,R2(πs)
=
X
a∈A
πs(a)r0(s, a) + γ
X
a∈A
πs(a)⟨P0(·|s, a), v⟩−
 X
a∈A
πs(a)(αr
s,a + γαP
s,a∥v∥)
!
=
X
a∈A
πs(a)

r0(s, a) −αr
s,a + γ(⟨P0(·|s, a), v⟩−αP
s,a∥v∥)

By [30, Lemma 4.3.1], we have that:
X
a∈A
πs(a)

r0(s, a) −αr
s,a + γ(⟨P0(·|s, a), v⟩−αP
s,a∥v∥)

≤max
a∈A

r0(s, a) −αr
s,a + γ(⟨P0(·|s, a), v⟩−αP
s,a∥v∥)

,
and since the action set is ﬁnite, there exists an action a∗∈A reaching the maximum. Setting
πd(a∗) = 1 thus gives the desired result. We just derived a regularized formulation of robust MDPs
25

with (s, a)-rectangular uncertainty set and ensured that the corresponding R2 Bellman operators yield
a deterministic optimal policy. In that case, the optimal R2 Bellman operator becomes:
[T ∗,R2v](s) = max
a∈A

r0(s, a) −αr
s,a + γ(⟨P0(·|s, a), v⟩−αP
s,a∥v∥)

.
D
Numerical experiments
Table 3: Hyperparameter set to obtain the results from Table 1
Number of seeds per experiment
5
Discount factor γ
0.9
Convergence Threshold θ
1e-3
Reward Radius α
1e-3
Transition Radius β
1e-5
In the following experiment, we play with the radius of the uncertainty set and analyze the distance
of the robust/R2 value function to the vanilla one obtained after convergence of MPI. Except for the
radius parameters of Table 3, all other parameters remain unchanged. In both ﬁgures 1 and 2, we see
that the distance norm converges to 0 as the size of the uncertainty set gets closer to 0: this sanity
check ensures an increasing relationship between the level of robustness and the radius value. As
shown in Fig. 1, the plots for robust MPI and R2 MPI coincide in the reward-robust case, but they
diverge from each other as the transition model gets more uncertain. This does not contradict our
theoretical ﬁndings from Thms. 3.1-4.1. In fact, each iteration of robust MPI involves an optimization
problem which is solved using a black-box solver and yields an approximate solution. As such, errors
propagate across iterations and according to Fig. 2, they are more sensitive to transition than reward
uncertainty. This is easy to understand: as opposed to the reward function, the transition kernel
interacts with the value function at each Bellman update, so errors on the value function also affect
those on the optimum and vice versa.
Figure 1: Distance norm between the optimal
robust/R2 value and the vanilla one as a function
of α (β = 0) after 5 runs of robust/R2 MPI
Figure 2: Distance norm between the optimal
robust/R2 value and the vanilla one as a function
of β (α = 0) after 5 runs of robust/R2 MPI
26

