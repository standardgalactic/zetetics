RealFusion
360◦Reconstruction of Any Object from a Single Image
Luke Melas-Kyriazi
Iro Laina
Christian Rupprecht
Andrea Vedaldi
Visual Geometry Group, Department of Engineering Science, University of Oxford
{lukemk,iro,chrisr,vedaldi}@robots.ox.ac.uk
https://lukemelas.github.io/realfusion
Figure 1. RealFusion generates a full 360◦reconstruction of any object given a single image of it (left column). It does so by leveraging
an existing diffusion-based 2D image generator. From the given image, it synthesizes a prompt that causes the diffusion model to “dream
up” other views of the object. It then extracts a neural radiance ﬁeld from the original image and the diffusion model-based prior, thereby
reconstructing the object in full. Both appearance and geometry are reconstructed faithfully and extrapolated in a plausible manner (see
the textured and shaded reconstructions from different viewpoints).
Abstract
We consider the problem of reconstructing a full 360◦
photographic model of an object from a single image of it.
We do so by ﬁtting a neural radiance ﬁeld to the image,
but ﬁnd this problem to be severely ill-posed. We thus take
an off-the-self conditional image generator based on diffu-
sion and engineer a prompt that encourages it to “dream
up” novel views of the object. Using the recent DreamFu-
sion method, we fuse the given input view, the conditional
prior, and other regularizers in a ﬁnal, consistent recon-
struction. We demonstrate state-of-the-art reconstruction
results on benchmark images when compared to prior meth-
ods for monocular 3D reconstruction of objects. Qualita-
tively, our reconstructions provide a faithful match of the
input view and a plausible extrapolation of its appearance
and 3D shape, including to the side of the object not visible
in the image.
1. Introduction
We consider the problem of obtaining a 360◦photo-
graphic reconstruction of any object given a single image
of it. The challenge is that a single image does not con-
tain sufﬁcient information for 3D reconstruction. Without
access to multiple views, an image only provides weak ev-
idence about the 3D shape of the object, and only for one
side of it. Even so, there is proof that this task can be solved:
any skilled 3D artist can take a picture of almost any object
and, given sufﬁcient time and effort, create a plausible 3D
model of it. The artist can do so by tapping into her vast
knowledge of the natural world and of the objects it con-
tains, making up for the information missing in the image.
1
arXiv:2302.10663v2  [cs.CV]  23 Feb 2023

To solve this problem algorithmically, one must then
marry visual geometry with a powerful statistical model of
the 3D world. The recent explosion of 2D image generators
like DALL-E [36], Imagen [42], and Stable Diffusion [40]
suggests that such models might not be far behind. By using
diffusion, these methods can solve highly-ambiguous gen-
eration tasks, obtaining plausible 2D images from textual
descriptions, semantic maps, partially-complete images, or
simply unconditionally from random noise. Clearly, these
models possess high-quality priors—if not of the 3D world,
then at least of the way it is represented in 2D images.
Hence, in theory, a 3D diffusion model trained on vast quan-
tities of 3D data should be capable of producing 3D recon-
structions, either unconditionally or conditioned on a 2D
image. However, training such a model is infeasible be-
cause, while one can access billions of 2D images [43], the
same cannot be said about 3D data.
The alternative to training a 3D diffusion model is to ex-
tract 3D information from an existing 2D model. A 2D im-
age generator can in fact be used to sample or validate mul-
tiple views of a given object; these multiple views can then
be used to perform 3D reconstruction. With early GAN-
based generators, authors showed some success for simple
data like faces and synthetic objects [3, 9, 12, 30, 31, 54].
With the availability of large-scale models like CLIP [34]
and, more recently, diffusion models, increasingly complex
results have been obtained. The most recent example is
DreamFusion [33], which generates high-quality 3D mod-
els from textual descriptions alone.
Despite these advances, the problem of single-image 3D
reconstruction remains largely unsolved. In fact, these re-
cent methods do not solve this problem. They either sample
random objects, or, like in the case of DreamFusion, start
from a textual description.
A problem in extending generators to reconstruction is
coverage (sometimes known as mode collapse). For exam-
ple, high-quality face generators based on GANs are usually
difﬁcult to invert: they may be able to generate many differ-
ent high-quality images, and yet are usually unable to gen-
erate most images [1]. Conditioning on an image provides a
much more detailed and nuanced speciﬁcation of the object
than, say, a textual description. It is not obvious if the gen-
erator model would be able to satisfy all such constraints.
In this paper, we study this problem in the context of
diffusion models. We express the object’s 3D geometry and
appearance by means of a neural radiance ﬁeld. Then, we
train the radiance ﬁeld to reconstruct the given input image
by minimizing the usual rendering loss. At the same time,
we sample random other views of the object, and constrain
them with the diffusion prior, using a technique similar to
DreamFusion.
We ﬁnd that, out of the box, this idea does not work well.
Instead, we need to make a number of improvements and
modiﬁcations. The most important change is to adequately
condition the diffusion model. The idea is to conﬁgure the
prior to “dream up” or sample images that may plausibly
constitute other views of the given object. We do so by en-
gineering the diffusion prompt from random augmentations
of the given image. Only in this manner does the diffu-
sion model provide sufﬁciently strong constraints to allow
meaningful 3D reconstruction.
In addition to setting the prompt correctly, we also add
some regularizers: shading the underlying geometry and
randomly dropping out texture (also similar to DreamFu-
sion), smoothing the normals of the surface, and ﬁtting the
model in a coarse-to-ﬁne fashion, capturing ﬁrst the overall
structure of the object and only then the ﬁne-grained details.
We also focus on efﬁciency and base our model on Instant-
NGP [29]. In this manner, we achieve reconstructions in the
span of hours instead of days if we were to adopt traditional
MLP-based NeRF models.
We assess our approach by using random images cap-
tured in the wild as well as existing benchmark datasets.
Note that we do not train a fully-ﬂedged 2D-to-3D model
and we are not limited to speciﬁc object categories; rather,
we perform reconstruction on an image-by-image basis us-
ing a pretrained 2D generator as a prior. Nonetheless, we
can surpass quantitatively and qualitatively previous single-
image reconstructors, including Shelf-Supervised Mesh
Prediction [58], which uses supervision tailored speciﬁcally
for 3D reconstruction.
More impressively, and more importantly, we obtain
plausible 3D reconstructions that are a good match for the
provided input image (Fig. 1). Our reconstructions are not
perfect, as the diffusion prior clearly does its best to explain
the available image evidence but cannot always match all
the details. Even so, we believe that our results convinc-
ingly demonstrate the viability of this approach and trace a
path for future improvements.
To summarize, we make the following contributions:
(1) We propose RealFusion, a method that can extract from
a single image of an object a 360◦photographic 3D recon-
struction without assumptions on the type of object imaged
or 3D supervision of any kind; (2) We do so by leveraging
an existing 2D diffusion image generator via a new single-
image variant of textual inversion; (3) We also introduce
new regularizers and provide an efﬁcient implementation
using InstantNGP; (4) We demonstrate state-of-the-art re-
construction results on a number of in-the-wild images and
images from existing datasets when compared to alternative
approaches.
2. Related work
Image-based reconstruction of appearnce and geometry.
Much of the early work on 3D reconstruction is based on
principles of multi-view geometry [11]. These classic meth-
2

ods use photometry only to match image features and then
discard it and only estimate 3D shape.
The problem of reconstructing photometry and geome-
try together has been dramatically revitalized by the intro-
duction of neural radiance ﬁelds (RFs). NeRF [26] in par-
ticular noticed that a coordinate MLP provides a compact
and yet expressive representation of 3D ﬁelds, and can be
used to model RFs with great effectiveness. Many variants
of NeRF-like models have since appeared. For instance,
some [24, 48, 50] use sign distance functions (SDFs) to
recover cleaner geometry. These approaches assume that
dozens if not hundreds of views of each scene are avail-
able for reconstruction. Here, we use them for single-image
reconstruction, using a diffusion model to “dream up” the
missing views.
Few-view reconstruction.
Many authors have attempted
to improve the statistical efﬁciency of NeRF-like models, by
learning or incorporating various kinds of priors. Quite re-
lated to our work, NeRF-on-a-Diet [17] reduces the number
of images required to learn a NeRF by generating random
views and measuring their “semantic compatibility” with
the available views via CLIP embeddings [35], but they still
require several input views.
While CLIP is a general-purpose model learned on 2D
data, other authors have learned deep networks speciﬁcally
for the goal of inferring NeRFs from a small number of
views. Examples include IBRNet [51], NeRF-WCE [13],
PixelNeRF [60], NeRFormer [38], and ViewFormer [22].
These models still generally require more than one input
view at test time, require multi-view data for training, and
are often optimized for speciﬁc object categories.
Single-view
reconstruction.
Some
authors
have
at-
tempted to recover full radiance ﬁelds from single im-
ages, but this generally requires multi-view data for train-
ing, as well as learning models that are speciﬁc to a spe-
ciﬁc object category. 3D-R2N2 [5], Pix2Vox [55, 55], and
LegoFormer [57] learn to reconstruct volumetric represen-
tation of simple objects, mainly from synthetic data like
ShapeNet [4]. More recently, CodeNeRF [19] predicts a
full radiance ﬁeld, including reconstructing the photometry
of the objects. AutoRF [28] learns a similar autoencoder
speciﬁcally for cars.
Extracting 3D models from 2D generators.
Several au-
thors have proposed to extract 3D models from 2D image
generators, originally using GANs [3, 9, 12, 30, 31, 54].
More related to our work, CLIP-Mesh [20] and Dream
Fields [16] do so by using the CLIP embedding and can
condition 3D generation on text. Our model is built on the
recent Dream Fusion approach [33], which builds on a sim-
ilar idea using a diffusion model as prior.
However, these models have been used as either pure
generators or generators conditioned on vague cues such as
Input Image
Render
Reconstruction 
View
Novel View
Prompt with optimized 
embedding token <e>
Reconstruction 
Objective
Diffusion
Model
Prior Objective
Novel View
Novel View
Novel View
Single-Image
Textual 
Inversion
Diffusion
Model
Prior Objective
Diffusion
Model
Prior Objective
Diffusion
Model
Prior Objective
Figure 2. Method diagram. Our method optimizes a neural ra-
diance ﬁeld using two objectives simultaneously: a reconstruction
objective and a prior objective. The reconstruction objective en-
sures that the radiance ﬁeld resembles the input image from a spe-
ciﬁc, ﬁxed view. The prior objective uses a large pre-trained dif-
fusion model to ensure that the radiance ﬁeld looks like the given
object from randomly sampled novel viewpoints. The key to mak-
ing this process work well is to condition the diffusion model on a
prompt with a custom token ⟨e⟩, which is generated prior to recon-
struction using single-image textual inversion. This diagram does
not display our coarse-to-ﬁne training strategy or regularization
terms, both of which improve qualitative results.
class identity or text. Here, we build on similar ideas, but
we apply them to the case of single-view reconstruction.
Recently, the authors of [53] have proposed to directly
generate multiple 2D views of an object, which can then be
reconstructed in 3D using a NeRF-like model. This is also
reminiscent of our approach, but their model requires multi-
view data for training, is only tested on synthetic data, and
requires to explicitly sample multiple views for reconstruc-
tion (in our case they remain implicit).
Diffusion
Models.
Diffusion
denoising
probabilistic
models are a class of generative models based on iteratively
reversing a Markovian noising process.
In vision, early
works formulated the problem as learning a variational
lower bound [14], or framed it as optimizing a score-
based generative model [45, 46] or as the discretization
of a continuous stochastic process [47].
Recent im-
provements includes the use of faster and deterministic
sampling [14, 25, 52], class-conditional models [7, 46],
text-conditional models [32],
and modeling in latent
space [41].
3. Method
We provide an overview and notation for the background
material ﬁrst (Sec. 3.1), and then discuss our RealFusion
method (Sec. 3.2).
3

Illustration 
of the information 
contained in <e>:
“A sketch of 
a <e>”
Automatic Mask
Input 
Image
RealFusion
auto-prompt:
“An image of 
a <e>”
“A davinci 
painting of a <e>”
RealFusion:
Novel View 1
RealFusion:
Novel View 2
ewf
ℒSDS
ℒrec
Figure 3. Examples demonstrating the level of detail of infor-
mation captured by the optimized embedding ⟨e⟩. Rows 1-
2 show input images and masks. The images are used to opti-
mize ⟨e⟩via our single-image textual inversion process. Rows 3-5
show examples of 2D images generated using ⟨e⟩in new prompts,
which we hope demonstrate the type of information encoded in
⟨e⟩.
Rows 6-7 show RealFusion’s output, optimized using the
prompt “An image of a ⟨e⟩”.
3.1. Radiance ﬁelds and DreamFusion
Radiance ﬁelds.
A radiance ﬁeld (RF) is a pair of func-
tions (σ(x), c(x)) mapping a 3D point x ∈R3 to an opac-
ity value σ(x) ∈R+ and a color value c(x) ∈R3. The RF
is called neural when these two functions are implemented
by a neural network.
The RF represents the shape and appearance of an object.
In order to generate an image of it, one renders the RF using
the emission-absorption model. Let I ∈R3×H×W be an
image, so that I(u) ∈R3 is the color of pixel u. In order
to compute I(u), one casts a ray ru from the camera center
through the pixel, interpreted as a point on the 3D image
plane (this implicitly accounts for the camera viewpoint π ∈
SE(3)). Then, one takes a certain number of samples (xi ∈
ru)i∈N , for indices N = {1, . . . , N} taken with constant
spacing ∆. The color is obtained as:
I(u) = R(u; σ, c) =
X
i∈N
(Ti+1 −Ti)c(xi),
(1)
where Ti = exp(−∆Pi−1
j=0 σ(xj)) is the probability that
a photon is transmitted from point xi back to the camera
sensor without being absorbed by the material.
Importantly, the rendering function R(u; σ, c) is differ-
entiable, which allows training the model by means of a
standard optimizer. Speciﬁcally, the RF is ﬁtted to a dataset
D = {(I, π)} of images I with known camera parameters
by minimizing the L2 image reconstruction error
Lrec(σ, c; D) =
1
|D|
X
(I,π)∈D
∥I −R(·; σ, c, π)∥2.
(2)
In order to obtain good quality results, one typically re-
quires a dataset of dozens or hundreds of views.
Here, we consider the case in which we are given ex-
actly one input image I0 corresponding to some (unknown)
camera π0. In this case, we can also assume any standard
viewpoint π0 for that single camera. Optimizing Eq. (2)
with a single training image leads to severe over-ﬁtting: it
is straightforward to ﬁnd a pair (σ, c) that has zero loss and
yet does not capture any sensible 3D model of the object.
Below we will leverage a pre-trained 2D image prior to (im-
plicitly) dream up novel views of the object and provide the
missing information for 3D reconstruction.
Diffusion models.
A diffusion model draws a sample
from a probability distribution p(I) by inverting a pro-
cess that gradually adds noise to the image I. The diffu-
sion process is associated with a variance schedule {βt ∈
(0, 1)}T
t=1, which deﬁnes how much noise is added at each
time step. The noisy version of sample I at time t can then
be written It = √¯αtI + √1 −¯αtϵ where ϵ ∼N(0, I), is a
sample from a Gaussian distribution (with the same dimen-
sionality as I), αt = 1 −βt, and ¯αt = Qt
i=1 αi. One then
learns a denoising neural network ˆϵ = Φ(It; t) that takes as
input the noisy image It and the noise level t and tries to
predict the noise component ϵ.
In order to draw a sample from the distribution p(I),
one starts by drawing a sample IT ∼N(0, I). Then, one
progressively denoises the image by iterated application of
Φ according to a speciﬁed sampling schedule [15, 25, 44],
which terminates with I0 sampled from p(I).
Modern diffusion models are trained on large collections
D′ = {I} of images by minimizing the loss
Ldiff(Φ; D′) =
1
|D′|
X
I∈D′
||Φ(√¯αtI +
√
1 −¯αtϵ, t) −ϵ||2.
(3)
This model can be easily extended to draw samples from
a distribution p(x|e) conditioned on a prompt e. Condition-
ing on the prompt is obtained by adding e as an additional
input of the network Φ, and the strength of conditioning can
be controlled via classiﬁer-free guidance [7].
DreamFusion and Score Distillation Sampling (SDS).
Given a 2D diffusion model p(I|e) and a prompt e, Dream-
Fusion extracts from it a 3D rendition of the corresponding
concept, represented by a RF (σ, c). It does so by randomly
sampling a camera parameter π, rendering a corresponding
view Iπ, assessing the likelihood of the view based on the
4

model p(Iπ|e), and updating the RF to increase the likeli-
hood of the generated view based on the model.
In practice, DreamFusion uses the denoiser network as a
frozen critic and takes a gradient step
∇(σ,c)LSDS(σ, c; π, e, t) =
Et,ϵ
h
w(t)(Φ(αtI + σtϵ; t, e) −ϵ) · ∇(σ,c)I
i
,
(4)
where I = R(·; σ, c, π). is the image rendered from a given
viewpoint π and prompt e. This process is called Score
Distillation Sampling (SDS).
Note that Eq. (4) differs from simply optimizing the
standard diffusion model objective because it does not in-
clude the Jacobian term for Φ. In practice, removing this
term both improves generation quality and reduces compu-
tational and memory requirements.
One ﬁnal aspect of DreamFusion is essential for under-
standing our contribution in the following section: Dream-
Fusion ﬁnds that it is necessary to use classiﬁer-free guid-
ance [7] with a very high guidance weight of 100, much
larger than one would use for image sampling, in order to
obtain good 3D shapes. As a result, the generations tend
to have limited diversity; they produce only the most likely
objects for a given prompt, which is incompatible with our
goal of reconstructing any given object.
3.2. RealFusion
Our goal is to reconstruct a 3D model of the object con-
tained in a single image I0, utilizing the prior captured in
the diffusion model Φ to make up for the missing informa-
tion. We will achieve this by optimizing a radiance ﬁeld
using two simultaneous objectives: (1) a reconstruction ob-
jective Eq. (2) from a ﬁxed viewpoint, and (2) a SDS-based
prior objective Eq. (4) on novel views randomly sampled
at each iteration. Figure 2 provides a diagram of the entire
system.
Single-image textual inversion as a substitute for al-
ternative views.
The most important component of our
method is the use of single-image textual inversion as a sub-
stitute for alternative views. Ideally, we would like to condi-
tion our reconstruction process on multi-view images of the
object in I0, i.e. on samples from p(I|I0). Since these im-
ages are not available, we instead synthesize a text prompt
e(I0) speciﬁcally for our image I0 as a proxy for this multi-
view information.
Our idea, then, is to engineer a prompt e(I0) to provide
a useful approximation of p(I|I0). We do so by generating
random augmentations g(I0), g ∈G of the input image,
which serve as pseudo-alternative-views. We use these aug-
mentations as a mini-dataset D′ = {g(I0)}g∈G and opti-
mize the diffusion loss Eq. (3) Ldiff(Φ(·; e(I0))) with respect
to the prompt e(I0), while freezing all other text embeddings
and model parameters.
In practice, our prompt is derived automatically from
templates like “an image of a ⟨e⟩”, where “⟨e⟩” (= e(I0))
is a new token introduced to the vocabulary of the text
encoder of our diffusion model (see Appendix A for de-
tails).
Our optimization procedure mirrors and general-
izes the recently-proposed textual-inversion method of [10].
Differently from [10], we work in the single-image set-
ting and utilize image augmentations for training rather than
multiple views.
To help convey the intuition behind ⟨e⟩, consider an at-
tempt at reconstructing an image of a ﬁsh using the generic
text prompt “An image of a ﬁsh” with losses Eqs. (3)
and (4). In our experience, this often produces a reconstruc-
tion which looks like the input ﬁsh from the input view-
point, but looks like some different, more-generic ﬁsh from
the backside. By contrast, using the prompt “An image of a
⟨e⟩”, the reconstruction resembles the input ﬁsh from all an-
gles. An example of exactly this case is shown in Figure 7.
Finally, Figure 3 demonstrates the amount of detail cap-
tured in the embedding ⟨e⟩.
Coarse-to-ﬁne training.
In order to describe our coarse-
to-ﬁne training methodology, it is necessary to ﬁrst brieﬂy
introduce our underlying RF model, a InstantNGP [29]. In-
stantNGP is a grid-based model which stores features at the
vertices of a set of feature grids {Gi}L
i=1 at multiple res-
olutions. The resolution of these grids is chosen to be a
geometric progression between the coarsest and ﬁnest reso-
lutions, and feature grids are trained simultaneously.
We choose a InstantNGP over a conventional MLP-
based NeRF due to its computational efﬁciency and training
speed. However, the optimization procedure occasionally
produces small irregularities on the surface of the object.
We ﬁnd that training in a coarse-to-ﬁne manner helps to al-
leviate these issues: for the ﬁrst half of training we only op-
timize the lower-resolution feature grids {Gi}L/2
i=1, and then
in the second half of training we optimize all feature grids
{Gi}L
i=1. Using this strategy, we obtain the beneﬁts of both
efﬁcient training and high-quality results.
Normal vector regularization.
Next, we introduce a new
regularization term to encourage our geometry to have
smooth normals. The introduction of this term is motivated
by the observation that our RF model occasionally gener-
ated noisy-looking surfaces with low-level artifacts. To ad-
dress these artifacts, we encourage our RF to have smoothly
varying normal vectors. Notably, we perform this regular-
ization in 2D rather than in 3D.
At each iteration, in addition to computing RGB and
opacity values, we also compute normals for each point
along the ray and aggregate these via the raymarching equa-
5

Figure 4. Qualitative results. RealFusion reconstructions from a single input view. Each pair of columns shows the textured object and
the underlying 3D shape, as a shaded surface. Different pairs of columns show different viewpoints.
tion to obtain normals N ∈RH×W ×3.1 Our loss is:
Lnormals = ∥N −stopgrad(blur(N, k))∥2
(5)
where stopgrad is a stop-gradient operation and blur(·, k) is
a Gaussian blur with kernel size k (we use k = 9).
Although it may be more common to regularize normals
in 3D, we found that operating in 2D reduced the variance
of the regularization term and led to superior results.
Mask loss.
In addition to the input image, our model also
utilizes a mask of the object that one wishes to reconstruct.
In practice, we use an off-the-shelf image matting model to
obtain this mask for all images.
We incorporate this mask in a simple manner by adding a
simple L2 loss term on the difference between the rendered
opacities from the ﬁxed reference viewpoint R(σ, π0) ∈
RH×W and the object mask M: Lrec,mask = ||O −M||2
1Normals may be computed either by taking the gradient of the density
ﬁeld or by using ﬁnite differences. We found that using ﬁnite differences
worked well in practice.
Our ﬁnal objective then consists of four terms:
∇σ,cL = ∇LSDS + λnormals · ∇Lnormals
+ λimage · ∇Limage + λmask · ∇Lmask
(6)
where the top line in the equation above corresponds to our
prior objective and the bottom line corresponds to our re-
construction objective.
4. Experiments
4.1. Implementation details
Regarding hyperparameters, we use essentially the same
set of hyper-parameters for all experiments—there is no
per-scene hyper-parameter optimization.2. For our diffusion
model prior, we employ the open-source Stable Diffusion
model [41] trained on the LAION [43] dataset of text-image
pairs. For our InstantNGP [29] model, we use a model with
2There is one small exception to this rule, which is that for a few num-
ber of images where the camera angle was clearly at an angle higher than
15◦, we took a camera angle of 30 or 40deg.
6

Figure 5. Qualitative comparison with prior work. We show
the results of our method and the category-level method of [59]
on real-world images from the CO3D dataset [38]. Each pair of
rows show two novel views produced by [59] and our method. For
[59], we use category-speciﬁc models for each CO3D category
(in this case, motorcycles, cups, and backpacks). Despite not re-
quiring any category-speciﬁc information, our method is able to
reconstruct objects at a higher level of detail than [59].
Figure 6. A demonstration of multi-modal image reconstruc-
tion. Above, we see our method’s ability to generate a diverse set
of object reconstructions given the same input image. In partic-
ular, the method produces different textures on the backsides of
the generated objects, despite all objects matching the input image
from the reference view.
16 resolution levels, a feature dimension of 2, and a maxi-
mum resolution of 2048, trained in a coarse-to-ﬁne manner
as explained above.
The camera for reconstruction is placed looking at the
origin on a sphere of radius 1.8, at an angle of 15deg above
the plane. At each optimization step, we ﬁrst render from
the reconstruction camera and compute our reconstruction
losses Lrec and Lrec,mask. We then render from a randomly
sampled camera to obtain a novel view, and use this view
for Lsds and Lnormals. We use λimage = 5.0, λmask = 0.5,
and λnormal = 0.5.
Regarding camera sampling, lighting, and shading, we
keep nearly all parameters the same as [33].
This in-
Table 1. Quantitative comparison. We compare our method with
Shelf-Supervised [59] on seven object categories. The F-score and
CLIP-similarity metrics are designed to measure the quality of re-
construction shape and appearance, respectively. For both metrics,
higher is better. Metrics are averaged over three images per cate-
gory. Our method outperforms [59] in aggregate, despite the fact
that [59] uses a different category-speciﬁc model for each category.
F-score
CLIP-similarity
[59]
Ours
[59]
Ours
Backpack
7.58
12.22
0.72
0.74
Chair
8.26
10.23
0.65
0.76
Motorcycle
8.66
8.72
0.69
0.70
Orange
6.27
10.16
0.71
0.74
Skateboard
7.74
5.89
0.74
0.74
Teddybear
12.89
10.08
0.73
0.82
Vase
6.30
9.72
0.69
0.71
Mean
8.24
9.58
0.70
0.74
cludes the use of diffuse and textureless shading stochas-
tic throughout the course of optimization, after an initial
warmup period of albedo-only shading. Complete details
regarding this and other aspects of our training setup are
provided in the supplementary material.
4.2. Quantitative results
There are only few methods that attempt to recon-
struct arbitrary objects in 3D. The most recent and best-
performing of these is Shelf-Supervised Mesh Predic-
tion [58], which we compare here. They provide 50 pre-
trained category-level models for 50 different categories in
OpenImages [23]. Since we aim to compute metrics using
3D or multi-view ground truth, we evaluate on seven cate-
gories in the CO3D dataset [39] with corresponding Open-
Images categories. For each of these seven categories, we
select three images at random and run both RealFusion and
Shelf-Supervised to obtain reconstructions.
We ﬁrst test the quality of the recovered 3D shape in
Fig. 5. Shelf-Supervised directly predicts a mesh. We ex-
tract one from our predicted radiance ﬁelds using march-
ing cubes.
CO3D comes with sparse point-cloud recon-
struction of the objects obtained using multi-view geome-
try. For evaluation, we sample points from the reconstructed
meshes and align them optimally with the ground truth point
cloud by ﬁrst estimating a scaling factor and then using Iter-
ated Closest Point (ICP). Finally, we compute F-score with
threshold 0.05 to measure the distance between the pre-
dicted and ground truth point clouds. Results are shown
in Tab. 1.
In order to evaluate the quality of the reproduced appear-
ance, we also compare novel-view renderings from our and
their method (Tab. 1). Ideally, these renderings should pro-
7

Figure 7. A visualization of the effect of single-image textual inversion on reconstruction quality. In each pair of rows, the top row
shows the result of utilizing a standard text prompt for our diffusion-model-based loss (e.g. “An image of a statue of a cat”). The bottom
row shows the result of utilizing a text prompt optimized for the input image in a fully-automatic manner; this textual inversion process
dramatically improves object reconstruction.
Figure 8. Effect of coarse-to-ﬁne training. The top row of each
pair is generated by optimizing all levels of a multi-resolution 3D
feature grid from the ﬁrst optimization step, whereas he bottom
row is optimized in a coarse-to-ﬁne manner.
duce views that are visually close to the real views. In or-
der to test this hypothesis, we check whether the generated
views are close or not to the other views given in CO3D. We
then report the CLIP embedding similarity of the generated
images with respect to the closest CO3D view available (i.e.
the view with maximum similarity).
4.3. Qualitative results
Figure 4 shows additional qualitative results from multi-
ple viewpoints. Having a single image of an object means
that several 3D reconstructions are possible. Figure 6 ex-
plores the ability of RealFusion to sample the space of pos-
Figure 9. Effect of normal smoothness on reconstruction qual-
ity. Each pair of rows show the reconstruction without and with
the normal smoothness regularization term equation 5. The regu-
larizer improves the visual appearance of surfaces and reduces the
number of irregularities on the surface of reconstructed objects. In
most cases, we also ﬁnd that it helps to improve the overall realism
of the reconstructed shape.
sible solutions by repeating the reconstruction several times,
starting from the same input image. There is little variance
in the reconstructions of the front of the object, but quite a
large variance for its back, as expected.
Figure 11 shows two typical failure modes of RealFu-
sion: in some cases the model fails to converge, and in oth-
ers it copies the front view to the back of the object, even if
this is not semantically correct.
8

Figure 10.
Comparing Stable Diffusion and CLIP priors.
Results from two different priors:
Stable Diffusion [41] and
CLIP [34]. Stable Diffusion yields much higher-quality recon-
structions, capturing more plausible object shapes.
Figure 11. Failure cases. In the ﬁrst two examples, the model
simply fails to properly reconstruct the object geometry, and pro-
duces a semi-transparent scene which lacks a well-deﬁned geom-
etry. The third case is different in that the geometry is highly re-
alistic, but the texture paints two Pikachu faces, one on each side
of the object; this problem is sometimes called the Janus problem,
after the two-faced Roman god.
4.4. Analysis and Ablations
One of the key components of RealFusion is our use of
single-image textual inversion, which allows the model to
correctly imagine novel views of a speciﬁc object. Figure 7
shows that this component plays indeed a critical role in the
quality of the reconstructions. Without texual inversion, the
model often reconstructs the backside of the object in the
form of a generic instance from the object category. For ex-
ample, the backside of the cat statue in the top row of Fig. 7
is essentially a different statue of a more generic-looking
cat, whereas the model trained with textual inversion resem-
bles the true object from all angles.
Other components of the model are also signiﬁcant.
Figure 9 shows that the normal smoothness regularizer
of Eq. (5) results in smoother, more realistic meshes and
reduces the number of artifacts. Figure 8 shows that coarse-
to-ﬁne optimization reduces the presence of low-level ar-
tifacts and results in smoother, visually pleasing surfaces.
Fig. 10 shows that using Stable Diffusion works signiﬁ-
cantly better than relying on an alternative such as CLIP.
5. Conclusions
We have introduced RealFusion, a new approach to ob-
tain full 360◦photographic reconstructions of any object
given a single image of it. Given an off-the-shelf diffusion
model trained using only 2D images and no special super-
vision for 3D reconstruction, as well as a single view of
the target object, we have shown how to select the model
prompt to imagine other views of the object. We have used
this conditional prior to learn an efﬁcient, multi-scale radi-
ance ﬁeld representation of the reconstructed object, incor-
porating an additional regularizer to smooth out the recon-
structed surface. The resulting method can generate plausi-
ble 3D reconstructions of objects captured in the wild which
are faithful to the input image. Future works include spe-
cializing the diffusion model for the task of new-view syn-
thesis and incorporating dynamics to reconstruct animated
3D scenes.
Ethics.
We use the CO3D dataset in a manner compatible
with their terms. CO3D does not contain personal infor-
mation. Please see https://www.robots.ox.ac.
uk/˜vedaldi/research/union/ethics.html
for further information on ethics.
Acknowledgments.
L. M. K. is supported by the Rhodes
Trust. A. V., I. L. and C.R. are supported by ERC-UNION-
CoG-101001212.
C.
R.
is also supported by VisualAI
EP/T028572/1.
References
[1] David Bau, Jun-Yan Zhu, Jonas Wulff, William S. Peebles,
Bolei Zhou, Hendrik Strobelt, and Antonio Torralba. Seeing
what a GAN cannot generate. In Proc. ICCV, 2019. 2
[2] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv´e J´egou,
Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-
ing properties in self-supervised vision transformers.
In
Proc. ICCV, 2021. 15
[3] Eric R. Chan, Connor Z. Lin, Matthew A. Chan, Koki
Nagano, Boxiao Pan, Shalini De Mello, Orazio Gallo,
Leonidas J. Guibas, Jonathan Tremblay, Sameh Khamis,
Tero Karras, and Gordon Wetzstein.
Efﬁcient geometry-
aware 3D generative adversarial networks. In Proc. CVPR,
2022. 2, 3
[4] Angel X. Chang, Thomas A. Funkhouser, Leonidas J.
Guibas, Pat Hanrahan, Qi-Xing Huang, Zimo Li, Silvio
Savarese, Manolis Savva, Shuran Song, Hao Su, Jianxiong
Xiao, Li Yi, and Fisher Yu. ShapeNet an information-rich 3d
model repository. arXiv.cs, abs/1512.03012, 2015. 3
[5] Christopher B. Choy, Danfei Xu, JunYoung Gwak, Kevin
Chen, and Silvio Savarese.
3d-r2n2: A uniﬁed approach
9

for single and multi-view 3d object reconstruction. In Proc.
ECCV, 2016. 3
[6] Kangle Deng, Andrew Liu, Jun-Yan Zhu, and Deva Ra-
manan.
Depth-supervised NeRF: Fewer views and faster
training for free. In Proceedings of the IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition (CVPR),
June 2022. 13
[7] Prafulla Dhariwal and Alexander Nichol. Diffusion models
beat gans on image synthesis. In Advances in Neural Infor-
mation Processing Systems, volume 34, pages 8780–8794,
2021. 3, 4, 5
[8] Christiane Fellbaum.
WordNet:
An Electronic Lexical
Database. Bradford Books, 1998. 13
[9] Matheus Gadelha, Subhransu Maji, and Rui Wang. 3D shape
induction from 2D views of multiple objects. In arXiv, 2016.
2, 3
[10] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patash-
nik, Amit H Bermano, Gal Chechik, and Daniel Cohen-
Or.
An image is worth one word: Personalizing text-to-
image generation using textual inversion.
arXiv preprint
arXiv:2208.01618, 2022. 5, 13
[11] R. I. Hartley and A. Zisserman.
Multiple View Geometry
in Computer Vision.
Cambridge University Press, ISBN:
0521540518, second edition, 2004. 2
[12] Philipp Henzler, Niloy J. Mitra, and Tobias Ritschel. Escap-
ing plato’s cave using adversarial training: 3D shape from
unstructured 2D image collections. In Proc. ICCV, 2019. 2,
3
[13] Philipp Henzler, Jeremy Reizenstein, Patrick Labatut, Ro-
man Shapovalov, Tobias Ritschel, Andrea Vedaldi, and
David Novotny. Unsupervised learning of 3d object cate-
gories from videos in the wild. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition
(CVPR), 2021. 3
[14] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising diffu-
sion probabilistic models. In Proc. NeurIPS, 2020. 3
[15] Jonathan Ho, Ajay Jain, and Pieter Abbeel. Denoising dif-
fusion probabilistic models. In Advances in Neural Infor-
mation Processing Systems, volume 33, pages 6840–6851,
2020. 4
[16] Ajay Jain, Ben Mildenhall, Jonathan T. Barron, Pieter
Abbeel, and Ben Poole. Zero-shot text-guided object gen-
eration with dream ﬁelds. In Proc. CVPR, 2022. 3
[17] Ajay Jain, Matthew Tancik, and Pieter Abbeel. Putting nerf
on a diet: Semantically consistent few-shot view synthesis.
In Proc. ICCV, 2021. 3
[18] Ajay Jain, Matthew Tancik, and Pieter Abbeel. Putting nerf
on a diet: Semantically consistent few-shot view synthe-
sis. In Proceedings of the IEEE/CVF International Confer-
ence on Computer Vision (ICCV), pages 5885–5894, October
2021. 13
[19] Wonbong Jang and Lourdes Agapito. CodeNeRF: Disentan-
gled neural radiance ﬁelds for object categories. In Proc.
ICCV, 2021. 3
[20] Nasir Mohammad Khalid, Tianhao Xie, Eugene Belilovsky,
and Pop Tiberiu. CLIP-Mesh: Generating textured meshes
from text using pretrained image-text models. SIGGRAPH
Asia, 2022. 3
[21] Diederik P Kingma and Jimmy Ba. Adam: A method for
stochastic optimization.
arXiv preprint arXiv:1412.6980,
2014. 12, 13
[22] Jon´aˇs Kulh´anek, Erik Derner, Torsten Sattler, and Robert
Babuˇska.
ViewFormer: NeRF-free neural rendering from
few images using transformers. In Proc. ECCV, 2022. 3
[23] Alina Kuznetsova, Hassan Rom, Neil Alldrin, Jasper Ui-
jlings, Ivan Krasin, Jordi Pont-Tuset, Shahab Kamali, Stefan
Popov, Matteo Malloci, Alexander Kolesnikov, Tom Duerig,
and Vittorio Ferrari. The open images dataset v4: Uniﬁed
image classiﬁcation, object detection, and visual relationship
detection at scale. IJCV, 2020. 7
[24] Chen-Hsuan Lin, Chaoyang Wang, and Simon Lucey. SDF-
SRN: learning signed distance 3d object reconstruction from
static images. In Hugo Larochelle, Marc’Aurelio Ranzato,
Raia Hadsell, Maria-Florina Balcan, and Hsuan-Tien Lin,
editors, Proc. NeurIPS, 2020. 3
[25] Luping Liu, Yi Ren, Zhijie Lin, and Zhou Zhao.
Pseudo
numerical methods for diffusion models on manifolds. In In-
ternational Conference on Learning Representations, 2021.
3, 4
[26] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,
Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. NeRF:
Representing scenes as neural radiance ﬁelds for view syn-
thesis. In Proc. ECCV, 2020. 3
[27] Ben Mildenhall, Pratul P. Srinivasan, Matthew Tancik,
Jonathan T. Barron, Ravi Ramamoorthi, and Ren Ng. Nerf:
Representing scenes as neural radiance ﬁelds for view syn-
thesis. In ECCV, 2020. 13
[28] Norman
M¨uller,
Andrea
Simonelli,
Lorenzo
Porzi,
Samuel
Rota
Bul`o,
Matthias
Nießner,
and
Peter
Kontschieder.
Autorf: Learning 3d object radiance ﬁelds
from single view observations.
CoRR, abs/2204.03593,
arXiv.cs. 3
[29] Thomas Muller, Alex Evans, Christoph Schied, and Alexan-
der Keller. Instant neural graphics primitives with a multires-
olution hash encoding. ACM Trans. Graph., 41(4):102:1–
102:15, July 2022. 2, 5, 6
[30] Thu Nguyen-Phuoc, Chuan Li, Lucas Theis, Christian
Richardt, and Yong-Liang Yang. HoloGAN: Unsupervised
learning of 3D representations from natural images. arXiv.cs,
abs/1904.01326, 2019. 2, 3
[31] Thu Nguyen-Phuoc, Christian Richardt, Long Mai, Yong-
Liang Yang, and Niloy J. Mitra.
Blockgan: Learning 3d
object-aware scene representations from unlabelled images.
In Proc. NeurIPS, 2020. 2, 3
[32] Alex Nichol, Prafulla Dhariwal, Aditya Ramesh, Pranav
Shyam, Pamela Mishkin, Bob McGrew, Ilya Sutskever, and
Mark Chen. Glide: Towards photorealistic image generation
and editing with text-guided diffusion models. arXiv preprint
arXiv:2112.10741, 2021. 3
[33] Ben Poole, Ajay Jain, Jonathan T. Barron, and Ben Milden-
hall. Dreamfusion: Text-to-3d using 2d diffusion. arXiv.cs,
abs/2209.14988, 2022. 2, 3, 7, 12, 13, 14
[34] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen
Krueger, and Ilya Sutskever.
Learning transferable visual
models from natural language supervision, 2021. 2, 9
[35] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
10

Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen
Krueger, and Ilya Sutskever.
Learning transferable visual
models from natural language supervision. In Proc. ICML,
volume 139, pages 8748–8763, 2021. 3
[36] Aditya Ramesh,
Mikhail Pavlov,
Gabriel Goh,
Scott
Gray, Chelsea Voss, Alec Radford, Mark Chen, and Ilya
Sutskever.
Zero-shot text-to-image generation.
arXiv
preprint arXiv:2102.12092, 2021. 2
[37] Ren´e Ranftl,
Katrin Lasinger,
David Hafner,
Konrad
Schindler, and Vladlen Koltun. Towards robust monocular
depth estimation: Mixing datasets for zero-shot cross-dataset
transfer. arXiv:1907.01341, 2019. 14
[38] Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler,
Luca Sbordone, Patrick Labatut, and David Novotny. Com-
mon objects in 3d: Large-scale learning and evaluation of
real-life 3d category reconstruction. In International Con-
ference on Computer Vision, 2021. 3, 7
[39] Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler,
Luca Sbordone, Patrick Labatut, and David Novotny. Com-
mon Objects in 3D: Large-scale learning and evaluation of
real-life 3D category reconstruction. In Proc. CVPR, 2021.
7
[40] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj¨orn Ommer.
High-resolution image
synthesis with latent diffusion models. In IEEE Conference
on Computer Vision and Pattern Recognition, pages 10684–
10695, 2022. 2
[41] Robin Rombach, Andreas Blattmann, Dominik Lorenz,
Patrick Esser, and Bj¨orn Ommer. High-resolution image syn-
thesis with latent diffusion models. In Proc. CVPR, 2022. 3,
6, 9, 12, 17
[42] Chitwan Saharia, William Chan, Saurabh Saxena, Lala
Li, Jay Whang, Emily Denton, Seyed Kamyar Seyed
Ghasemipour,
Burcu Karagol Ayan,
S Sara Mahdavi,
Rapha Gontijo Lopes, et al.
Photorealistic text-to-image
diffusion models with deep language understanding. arXiv
preprint arXiv:2205.11487, 2022. 2
[43] Christoph Schuhmann, Romain Beaumont, Richard Vencu,
Cade Gordon,
Ross Wightman,
Mehdi Cherti,
Theo
Coombes, Aarush Katta, Clayton Mullis, Mitchell Worts-
man, et al.
Laion-5b:
An open large-scale dataset for
training next generation image-text models. arXiv preprint
arXiv:2210.08402, 2022. 2, 6
[44] Jiaming Song, Chenlin Meng, and Stefano Ermon. Denois-
ing diffusion implicit models. In International Conference
on Learning Representations, 2020. 4
[45] Yang Song and Stefano Ermon. Generative modeling by es-
timating gradients of the data distribution. In Proc. NeurIPS,
2019. 3
[46] Yang Song and Stefano Ermon.
Improved techniques for
training score-based generative models. Advances in neural
information processing systems, 33:12438–12448, 2020. 3
[47] Yang Song, Jascha Sohl-Dickstein, Diederik P. Kingma, Ab-
hishek Kumar, Stefano Ermon, and Ben Poole. Score-based
generative modeling through stochastic differential equa-
tions. In Proc. ICLR, 2021. 3
[48] Itsuki Ueda, Yoshihiro Fukuhara, Hirokatsu Kataoka, Hi-
roaki Aizawa, Hidehiko Shishido, and Itaru Kitahara. Neural
Density-Distance ﬁelds. In Proc. ECCV, 2022. 3
[49] Dor Verbin, Peter Hedman, Ben Mildenhall, Todd Zickler,
Jonathan T Barron, and Pratul P Srinivasan. Ref-nerf: Struc-
tured view-dependent appearance for neural radiance ﬁelds.
In 2022 IEEE/CVF Conference on Computer Vision and Pat-
tern Recognition (CVPR), pages 5481–5490. IEEE, 2022. 12
[50] Peng Wang, Lingjie Liu, Yuan Liu, Christian Theobalt, Taku
Komura, and Wenping Wang. NeuS: Learning neural im-
plicit surfaces by volume rendering for multi-view recon-
struction. arXiv.cs, abs/2106.10689, 2021. 3
[51] Qianqian Wang, Zhicheng Wang, Kyle Genova, Pratul P.
Srinivasan, Howard Zhou, Jonathan T. Barron, Ricardo
Martin-Brualla, Noah Snavely, and Thomas A. Funkhouser.
Ibrnet: Learning multi-view image-based rendering. In Proc.
CVPR, 2021. 3
[52] Daniel Watson, William Chan, Jonathan Ho, and Moham-
mad Norouzi. Learning fast samplers for diffusion models
by differentiating through sample quality. In International
Conference on Learning Representations, 2021. 3
[53] Daniel Watson, William Chan, Ricardo Martin-Brualla,
Jonathan
Ho,
Andrea
Tagliasacchi,
and
Mohammad
Norouzi.
Novel view synthesis with diffusion models.
arXiv.cs, abs/2210.04628, 2022. 3
[54] Jiajun Wu, Chengkai Zhang, Tianfan Xue, Bill Freeman, and
Josh Tenenbaum. Learning a probabilistic latent space of ob-
ject shapes via 3D generative-adversarial modeling. In Proc.
NeurIPS, 2016. 2, 3
[55] Haozhe Xie, Hongxun Yao, Shengping Zhang, Shangchen
Zhou, and Wenxiu Sun. Pix2vox++: Multi-scale context-
aware 3d object reconstruction from single and multiple im-
ages. Int. J. Comput. Vis., 128(12), 2020. 3
[56] Dejia Xu, Yifan Jiang, Peihao Wang, Zhiwen Fan, Humphrey
Shi, and Zhangyang Wang. Sinnerf: Training neural radiance
ﬁelds on complex scenes from a single image. 2022. 13
[57] Farid Yagubbayli, Alessio Tonioni, and Federico Tombari.
Legoformer: Transformers for block-by-block multi-view 3d
reconstruction. arXiv.cs, abs/2106.12102, 2021. 3
[58] Yufei
Ye,
Shubham
Tulsiani,
and
Abhinav
Gupta.
Shelf-supervised mesh prediction in the wild.
CoRR,
abs/2102.06195, 2021. 2, 7
[59] Yufei Ye, Shubham Tulsiani, and Abhinav Gupta.
Shelf-
supervised mesh prediction in the wild. In Computer Vision
and Pattern Recognition (CVPR), 2021. 7
[60] Alex Yu, Vickie Ye, Matthew Tancik, and Angjoo Kanazawa.
pixelnerf: Neural radiance ﬁelds from one or few images.
In Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pages 4578–4587, 2021. 3,
13
[61] Richard Zhang, Phillip Isola, Alexei A Efros, Eli Shechtman,
and Oliver Wang. The unreasonable effectiveness of deep
features as a perceptual metric. In CVPR, 2018. 14
11

A. Implementation Details
In this section, we provide full implementation details
which were omitted from the main text due to space con-
straints. Most of these details follow [33], but a few are
slightly modiﬁed.
Shading.
We consider three different types of shading:
albedo, diffuse, and textureless. For albedo, we simply ren-
der the RGB color of each ray as given by our model:
I(u) = Iρ(u) = R(u; σ, c)
For diffuse, we also compute the surface normal n as the
normalized negative gradient of the density with respect to
u. Then, given a point light l with color lρ and an ambient
light with color la, we render
I(u) = Iρ(u) ◦(lρ ◦max(0, n ·
l−u
||l−u|| + la))
For textureless, we use the same equation with Iρ(u) re-
placed by white (1, 1, 1).
For the reconstruction view, we only use albedo shad-
ing. For the random view (i.e. the view used for the prior
objectives), we use albedo shading for the ﬁrst 1000 steps
of training by setting la = 1.0 and lρ = 0.0. Afterwards
we use la = 0.1 and lρ = 0.9, and we select stochastically
between albedo, diffuse, and textureless with probabilities
0.2, 0.4, and 0.4, respectively.
We obtain the surface normal using ﬁnite differences:
n =
1
2 · ϵ


I(u + ϵx) −I(u −ϵx)
I(u + ϵy) −I(u −ϵy)
I(u + ϵz) −I(u −ϵz)


where ϵx = (ϵ, 0, 0), ϵy = (0, ϵ, 0), and ϵz = (0, 0, ϵ)
Density bias.
As in [33], we add a small Gaussian blob of
density to the origin of the scene in order to assist with the
early stages of optimization. This density takes the form
σinit(µ) = λ · e−||µ||2/(2ν2)
with λ = 5 and ν = 0.2.
Camera.
The ﬁxed camera for reconstruction is placed at
a distance of 1.8 from the origin, oriented toward the ori-
gin, at an elevation of 15◦above the horizontal plane. For
a small number of scenes in which the object of interest
is clearly seen from overhead, the reconstruction camera is
placed at an elevation of 40◦.
The camera for the prior objectives is sampled randomly
at each iteration. Its distance from the origin is sampled
uniformly from [1.0, 1.5]. Its azimuthal angle is sampled
uniformly at random from the 360◦around the object. Its el-
evation is sampled uniformly in degree space from −10◦to
90◦with probability 0.5 and uniformly on the upper hemi-
sphere with probability 0.5. The ﬁeld of view is uniformly
sampled between 40 and 70. The camera is oriented toward
the origin. Additionally, every tenth iteration, we place the
prior camera near the reconstruction camera: its location
is sampled from the prior camera’s location perturbed by
Gaussian noise with mean 0 and variance 1.
Lighting.
We sample the position of the point light by
adding a noise vector η ∼N(0, 1) to the position of the
prior camera.
View-Dependent Prompt.
We add a view-dependent suf-
ﬁx to our text prompt based on the location of the prior cam-
era relative to the reconstruction camera. If the prior camera
is placed at an elevation of above 60◦, the text prompt re-
ceives the sufﬁx “overhead view.” If it is at an elevation
below 0◦, the text receives “bottom view.” Otherwise, for
azimuthal angles of ±30◦, ±30 −90◦, or ±90 −180◦in
either direction of the reconstruction camera, it receives the
sufﬁces “front view,” “side view,” or “bottom view,” respec-
tively.
InstantNGP.
Our InstantNGP parameterizes the density
and albedo inside a bounding box around the origin with
side length 0.75. It is a multi-resolution feature grid with 16
levels. With coarse-to-ﬁne training, only the ﬁrst 8 (lowest-
resolution) levels are used during the ﬁrst half of training,
while the others are masked with zeros. Each feature grid
has dimensionality 2.
The features from these grids are
stacked and fed to a 3-layer MLP with 64 hidden units.
Rendering and diffusion prior.
We render at resolution
96px. Since Stable Diffusion [41] is designed for images
with resolution 512px, we upsample renders to 512px be-
fore passing them to the Stable Diffusion latent space en-
coder (i.e. the VAE). We add noise in latent space, sam-
pling t ∼U(0.02, 0.98). We use classiﬁer-free guidance
strength 100. We found that results with classiﬁer-free guid-
ance strength above 30 produced good results; below 30 led
to many more geometric deformities. Although we do not
backpropagate through the Stable Diffusion UNet for LSDS,
we do backpropagate through the latent space encoder.
Optimization.
We optimize using the Adam [21] opti-
mizer with learning rate 1e −3 for 5000 iterations. The
optimization process takes approximately 45 minutes on a
single V100 GPU.
Background model.
For our background model, we use
a two-layer MLP which takes the viewing direction as in-
put. This model is purposefully weak, such that the model
cannot trivially optimize its objectives by using the back-
ground.
Additional regularizers.
We additionally employ two
regularizers on our density ﬁeld. The ﬁrst is the orientation
loss from Ref-NeRF [49], also used in DreamFusion [33],
for which we use λorient = 0.01. The second is an entropy
loss which encourages points to be either fully transparent
or fully opaque: Lentropy = (w·log2(w)−(1−w)·log2(1−
12

t r a n s f o r m = T . Compose ( [
T . RandomApply ( [ T . RandomRotation ( degrees =10 ,
f i l l =255)] , p =0.75) ,
T . RandomResizedCrop ( image size ,
s c a l e =(0.70 ,
1 . 3 ) ) ,
T . RandomApply ( [ T . C o l o r J i t t e r ( 0 . 0 4 ,
0.04 ,
0.04 ,
0 . 0 4 ) ] ,
p =0.75) ,
T . RandomGrayscale ( p =0.10) ,
T . RandomApply ( [ T . GaussianBlur (5 ,
( 0 . 1 ,
2 ) ) ] ,
p =0.10) ,
T . RandomHorizontalFlip ( ) ,
] )
Figure 12. PyTorch code for the image augmentations used for single-image textual inversion.
w) where w is the cumulative sum of density weights com-
puted as part of the NeRF rendering equation (Equation 1).
Single-image textual inversion.
Our single-image tex-
tual inversion step, which is a variant of textual inver-
sion [10], entails optimizing a token e introduced into the
diffusion model text encode to match an input image. The
key to making this optimization successful given only a sin-
gle image is the use of heavy image augmentations, shown
in Fig. 12. We optimize using these augmentations for a to-
tal of 3000 steps using the Adam optimizer [21] with image
size 512px, batch size 16, learning rate 5 · 10−4, and weight
decay 1 · 10−2.
The embedding e can be initialized either randomly,
manually (by selecting a token from the vocabulary that
matches the object), or using an automated method.
One automated method that we found to be successful
was to use CLIP (which is also the text encoder of the Sta-
ble Diffusion model) to infer a starting token to initialize
the inversion procedure. For this automated procedure, we
begin by considering the set of all tokens in the CLIP text
tokenizer which are nouns, according to the WordNet [8]
database. We use only nouns because we aim to reconstruct
objects, not reproduce styles or visual properties. We then
compute text embeddings for captions of the form “An im-
age of a ⟨token⟩” using each of these tokens. Separately,
we compute the image embedding for the input image. Fi-
nally, we take the token whose caption is most similar to the
image embedding as initialization for our textual inversion
procedure.
We use the manual initialization method for the exam-
ples in the main paper and we use the automated initializa-
tion method for the examples in the supplemental material
(i.e. those included below).
B. Method diagram
We provide a diagram illustrating our method in Fig. 2.
C. Additional Qualitative Examples
In Fig. 13, we show additional examples of reconstruc-
tions from our model. We see that our method is often able
to reconstruct plausible geometries and object backsides.
D. Additional Comparisons
We provide additional comparisons to recent single-view
reconstruction methods on the lego scene from the synthetic
NeRF [27] dataset. We compare on the special test set cre-
ated by SinNeRF [56], which consists of 60 views very
close to the reference view. We emphasize that our method
is not tailored to this setting, whereas the other methods are
designed speciﬁcally for it. For example, some other meth-
ods work by warping the input image, which only performs
well for novel views close to the reference view.
Table 2. Novel view synthesis comparison. A comparison of
RealFusion against recent single-view reconstruction methods on
the task of novel view synthesis on the synthetic lego scene from
NeRF [27]. These numbers are computed on the test set rendered
by SinNeRF [56], which contains 60 views very close to the ref-
erence view. This is a setting highly favorable to methods that use
depth supervision, such as DS-NeRF and SinNeRF .
Depth?
PSNR
SSIM
LPIPS
PixelNeRF [60]
14.3
0.72
0.22
DietNeRF [18]
15.0
0.72
0.20
DS-NeRF [6]

16.6
0.77
0.16
SinNeRF [56]

21.0
0.82
0.09
RealFusion
16.5
0.76
0.25
E. Text-to-Image-to-3D
In this section, we explore the idea of reconstructing a
3D object from a text prompt alone by ﬁrst using the text
prompt to generate an image, and then reconstructing this
image using RealFusion.
We show examples of text-to-image-to-3D generation in
Fig. 14.
Compared to the one-step procedure of [33] (i.e. text-
to-3D), this two-step procedure (i.e. text-to-image-to-3D)
has the advantage that it may be easier for users to control.
Under our setup, users can ﬁrst sample a large number of
images from a 2D diffusion model such as Stable Diffusion,
13

select their desired image, and then lift it to 3D using Real-
Fusion. It is possible that this setup could help help address
the issue of diversity of generation discussed in [33]. Addi-
tionally, tn this setting, we ﬁnd that it is usually not neces-
sary to use single-image textual inversion, since the images
sampled in the ﬁrst stage are already extremely well-aligned
with their respective prompts.
F. Analysis of Failure Cases
In Fig. 15, we show additional examples of failure cases
from our model. Below, we analyzed what we ﬁnd to be our
three most common failure cases. The techniques we ap-
ply in RealFusion (single-image textual inversion, normals
smoothing, and coarse-to-ﬁne training) make these failure
cases less frequent and less severe, but they still occur on
various images.
Neural ﬁelds lacking well-deﬁned geometry.
One fail-
ure case of our method consists of the generation of a semi-
transparent neural ﬁeld which does not have a well-deﬁned
geometry. These ﬁelds tend to look like the input image
when seen from the reference viewpoint, but do not resem-
ble plausible objects when seen from other viewpoints. We
note that this behavior is extremely common when using
CLIP as a prior model, but it occurs occasionally even when
using Stable Diffusion and LSDS.
Floaters.
Another failure case involves “ﬂoaters,” or dis-
connected parts of the scene which appear close to the cam-
era. These ﬂoaters sometimes appear in front of the refer-
ence view as to make the corresponding render look like the
input image. Without image-speciﬁc prompts, these ﬂoaters
are a very big issue, appearing in the majority of recon-
structions. When using image-speciﬁc prompts, the issue
of ﬂoaters is greatly (but not entirely) alleviated.
The Janus Problem.
Named after the two-faced Roman
god Janus, the “Janus problem” refers to reconstructions
which have two or more faces. This problem arises because
the loss function tries to make the render of every view look
like the input image, at least to a certain extent.
Our use of view-speciﬁc prompting partially alleviates
this issue. For example, when we render an image of a
panda from the back, we optimize using the text prompt
“An image of a ⟨object⟩, back view”, where “⟨object⟩” is
our image-speciﬁc token corresponding to the image of a
panda. However, even with view-speciﬁc prompting, this
problem still occurs. This problem is visible with the panda
in Fig. 14 (row 2). We note that this problem is not unique
to our method; it can also be seen with [33] (see Figure 9,
last row).
G. Unsuccessful Experiments and Regulariza-
tion Losses
In the process of developing our method, we exper-
imented with numerous ideas, losses, and regularization
terms which were not included in our ﬁnal method because
they either did not improve reconstruction quality or did not
improve it enough to justify their complexity. Here, we
describe some of these ideas for the beneﬁt of future re-
searchers working on this problem.
Using DM for reconstruction loss.
One idea we tried in-
volved using the diffusion model within our reconstruction
objective as well as our prior objective. This involved a
modiﬁed version of LSDS in which we compared the noise
predicted by the diffusion model for our noisy rendered im-
age to the noise predicted by the diffusion model for a noisy
version of our input image. We found that with this loss we
were able to reconstruct the input image to a certain degree,
but that we did not match the exact input image colors or
textures.
Normals smoothing in 3D.
Our normals smoothing term
operates in 2D, using normals rendered via the NeRF equa-
tion. We also tried different ways of smoothing normals in
3D. However, possibly due to our grid-based radiance ﬁeld
and/or our ﬁnite difference-based normals computation, we
found that these regularization terms were all very noisy and
harmful to reconstruction quality.
Using monocular depth.
We tried incorporating monoc-
ular depth predictions into the pipeline, using pre-trained
monocular depth networks such as MiDaS [37]. Speciﬁ-
cally, we enforced that the depth rendered from the refer-
ence view matched the depth predicted by MiDaS for the in-
put image. We found that this additional depth loss in most
instances did not noticeably improve reconstruction quality
and in some cases was harmful. Nonetheless, these results
are not conclusive and future work could pursue other ways
of integrating these components.
Using LPIPS and SSIM reconstruction losses.
We tried
using LPIPS [61] and SSIM losses in place of our L2 recon-
struction loss. We found that LPIPS performed similarly to
L2, but incurred additional computation and memory usage.
We found that SSIM without either L2 and LPIPS resulted
in worse reconstruction quality, but that it yielded ﬁne re-
sults when combined with them. We did not include it in
our ﬁnal objective for the sake of simplicity.
Rendering at higher resolutions.
Since Stable Diffusion
operates on images of resolution 512px, it is conceivable
that rendering at higher resolution would be beneﬁtial with
regard to the prior loss. However, we found no noticeable
difference in quality when rendering at higher resolutions
than 96px or 128px. For computational purposes, we used
resolution 96px for all experiments in the main paper.
14

Using DINO-based prior losses.
Similarly to the CLIP
prior loss, one could imagine using other networks to en-
courage renders from novel views to be semantically sim-
ilar to the input image. Due to the widespread success of
the DINO [2] models in unsupervised learning, we tried
using DINO feature losses in addition to the Stable Diffu-
sion prior loss. Speciﬁcally, for each image rendered from
a novel view, we computed a DINO image embedding and
maximized its cosine similarity with the DINO image em-
bedding of the reference image. We found that this did not
noticeably improve or degrade performance. For purposes
of simplicity, we did not include it.
H. Links to Images for Qualitative Results
For our qualitative results, we primarily use images from
datasets such as Co3D. We also use a small number of im-
ages sourced directly from the web to show that our method
works on uncurated web data. We provide links to all of
these images on our project website.
15

Figure 13. Additional qualitative examples. This ﬁgure presents additional qualitative examples from our model. The ﬁrst column shows
the input image. The second column shows the reconstruction from the reference viewpoint. The following columns show renders from
novel viewpoints, demonstrating that our model is able to reconstruct plausible object shapes.
16

Figure 14. Text-to-Image-to-3D. This ﬁgure presents examples from our model using images generated directly from text prompts using
Stable Diffusion [41]. The images were generated with the prompt “An image of a
” where the blank space is replaced by “deer”,
“ﬂamingo”, “hen”, “pencil drawing of a horse”, “squirrel”, “dolphin”, and “unicorn”, respectively. The results demonstrate that our
method is able to reconstruct plausible object shapes even from synthetic images.
17

Figure 15. Additional failure cases. This ﬁgure presents additional failure cases from our model. The ﬁrst column shows the input image.
The second column shows the reconstruction from the reference viewpoint. The following columns show renders from novel viewpoints,
which make clear why these examples are failure cases. Note that some examples (for example, the panda bear in the second row and the
hamster in the last row) suffer from the Janus problem.
18

Figure 16. A visualization of the effect of single-image textual inversion on reconstruction quality. An expanded version of Figure
7 in the main paper showing the effect of single-image textual inversion on reconstruction quality. The top row in each pair of rows
shows reconstruction results using a standard text prompt, whereas the bottom row shows reconstruction results using single-image textual
inversion. The novel views are chosen to show the back side of the object; note how the examples without textual inversion look like
highly-generic versions of the objects in the input image.
19

Figure 17. An example of variation across random seeds for a challenging input image. As described in the main paper, our model is
able to generate multiple reconstructions for a given input image. For this ﬁgure, we apply our method (in a text-to-image-to-3D manner)
to a highly challenging image produced by Stable Diffusion from the text prompt “An image of an astronaut riding a horse.” We run
reconstruction using two different seeds: one of these (top) yields a reasonable shape, whereas the other is a failure case that does not yield
a reasonable shape. This example both highlights the ability of our method to reconstruct highly challenging shapes and also demonstrates
how future work could aim to improve reconstruction consistency and quality.
20

