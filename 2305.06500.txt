InstructBLIP: Towards General-purpose
Vision-Language Models with Instruction Tuning
Wenliang Dai†1,2∗Junnan Li†,B,1
Dongxu Li1
Anthony Meng Huat Tiong1,3
Junqi Zhao3
Weisheng Wang3
Boyang Li3
Pascale Fung2
Steven HoiB,1
1Salesforce Research
2Hong Kong University of Science and Technology
3Nanyang Technological University, Singapore
https://github.com/salesforce/LAVIS/tree/main/projects/instructblip
†Equal contribution BCorresponding authors: {junnan.li,shoi@salesforce.com}
Abstract
General-purpose language models that can solve various language-domain tasks
have emerged driven by the pre-training and instruction-tuning pipeline. How-
ever, building general-purpose vision-language models is challenging due to the
increased task discrepancy introduced by the additional visual input. Although
vision-language pre-training has been widely studied, vision-language instruction
tuning remains relatively less explored. In this paper, we conduct a systematic
and comprehensive study on vision-language instruction tuning based on the pre-
trained BLIP-2 models. We gather a wide variety of 26 publicly available datasets,
transform them into instruction tuning format and categorize them into two clusters
for held-in instruction tuning and held-out zero-shot evaluation. Additionally,
we introduce instruction-aware visual feature extraction, a crucial method that
enables the model to extract informative features tailored to the given instruction.
The resulting InstructBLIP models achieve state-of-the-art zero-shot performance
across all 13 held-out datasets, substantially outperforming BLIP-2 and the larger
Flamingo. Our models also lead to state-of-the-art performance when ﬁnetuned on
individual downstream tasks (e.g., 90.7% accuracy on ScienceQA IMG). Further-
more, we qualitatively demonstrate the advantages of InstructBLIP over concurrent
multimodal models. All InstructBLIP models have been open-sourced.
1
Introduction
It has been a longstanding aspiration in various ﬁelds of Artiﬁcial Intelligence (AI) to build uniﬁed
models that can generalize to a wide variety of tasks. In natural language processing (NLP), instruction
tuning [45, 7] has achieved promising success in achieving this goal. By ﬁnetuning a large language
model (LLM) on various tasks described through natural language instructions, instruction tuning
enables the model to process and follow novel instructions. Recently, instruction-tuned LLMs
have been leveraged for vision-language tasks. For example, BLIP-2 [20] effectively adapts frozen
instruction-tuned LLMs to understand visual inputs, which exhibits preliminary capabilities in
following instructions for image-to-text generation.
However, different from NLP tasks, vision-language tasks are more diverse in nature due to the
additional visual inputs from various domains. This makes it more challenging to build a uniﬁed
model that can generalize to a wide range of vision-language tasks. Previous methods either perform
multitask training [6] or directly resort to the inherent generalization ability of LLMs [20, 4]. Nev-
ertheless, multitask training, despite using a uniﬁed input-output format, fails to adequately bridge
∗Work done during internship at Salesforce.
Preprint. Under review.
arXiv:2305.06500v1  [cs.CV]  11 May 2023

What could have happened based on the
current scene?
Based on the current scene in the image, it is
possible that a hurricane or severe weather event
caused significant damage to the buildings and
infrastructure in the area. The person walking
through the debris-covered area suggests that they
are inspecting the damage caused by the disaster.
The presence of palm trees indicates that the
location may be in a tropical or sub-tropical region,
which can be more susceptible to hurricanes and
other natural disasters.
Introduce me this painting
in detail.
The painting depicts a young girl with a pearl earring,
also known as "The Girl with the Pearl Earring" or
"Girl with a Pearl Earring". She is wearing a blue
dress and has a pearl earring on her left ear. Her hair
is pulled back into a bun, and she is looking directly
at the viewer. This painting is a famous work of art
by Dutch painter Johannes Vermeer, created in 1665.
It is considered one of his masterpieces and is widely
recognized for its intricate details and realistic
portrayal of the young girl’s facial expression.
Can yon tell me about this image in detail?
The image depicts a collection of various
vegetables including carrots, cucumbers,
tomatoes, and nuts arranged on a stone surface.
There are several jars filled with different types of
ingredients, such as peanuts, cashews, sunflower
seeds, and pumpkin seeds. These ingredients are
likely to be part of a healthy meal or snack.
Can you give me an example of such meal?
An example of a healthy meal or snack that
incorporates some of the ingredients in the image
could be a salad made with mixed greens, carrots,
cucumbers, tomatoes, and nuts. The salad can be
dressed with a homemade vinaigrette made with
olive oil, lemon juice, salt, and pepper.
Additionally, the salad can be topped with
pumpkin seeds or sunflower seeds for added
crunch and nutritional value.
Sounds good! Show me steps of making this salad.
Sure! Here are the steps to make this healthy salad:
1. Wash and chop the mixed greens, carrots,
cucumbers, and tomatoes into bite-sized pieces.
2. In a small bowl, whisk together olive oil, lemon
juice, salt, and pepper to create a homemade
vinaigrette dressing.
3. Place the chopped vegetables in a large bowl and
toss them with the vinaigrette dressing.
4. Sprinkle pumpkin seeds or sunflower seeds on top
of the salad for added crunch and nutritional value.
5. Serve the salad immediately or store it in the
refrigerator until ready to enjoy.
Can you describe this image
in detail?
The image depicts a person standing in front of an
open doorway, looking out into space. They are
wearing a jacket and appears to be contemplating the
vastness of the universe beyond the doorway. In the
background, there are multiple stars visible, creating
a stunning celestial scene. The person's gaze is
directed towards the center of the image, where a
bright star can be seen shining brightly. There are
also smaller stars scattered throughout the scene,
adding to the overall cosmic atmosphere.
What if they jump over the doorway?
If the person were to jump over the doorway, they
would be leaving the confines of their current location
and venturing into the vastness of space. This could be
a metaphorical representation of stepping out of one’s
comfort zone or embarking on a new adventure.
However, it is important to note that such an action
would be extremely dangerous and potentially life-
threatening due to the harsh conditions of space.
What makes this dog special?
The dog in the image is wearing futuristic armor and
has a glowing green eye, giving it a unique and
futuristic appearance. This combination of elements
makes the dog stand out from other dogs and adds an
element of fantasy or sci-fi to the image.
Additionally, the dog's posture and facial expression
convey a sense of confidence and determination,
further emphasizing its unique features.
Figure 1: A few qualitative examples generated by our InstructBLIP Vicuna model. Here, a range of
its diverse capabilities are demonstrated, including complex visual scene understanding and reasoning,
knowledge-grounded image description, multi-turn visual conversation, etc.
2

the discrepancies among tasks. On the other hand, LLMs tuned on text-only instructions are more
effective on the zero-shot generalization for NLP tasks but shows less satisfactory performance for
vision-language tasks.
To address the aforementioned challenges, this paper presents InstructBLIP, a vision-language
instruction tuning framework that enables general-purpose models to solve a wide range visual tasks
through a uniﬁed natural language interface. InstructBLIP is initialized from a pre-trained BLIP-2
model consisting of an image encoder, an LLM, and a Q-Former to bridge the two. During instruction
tuning, we ﬁnetune the Q-Former while keeping the image encoder and LLM frozen. Our paper
makes the following key contributions:
• We perform a comprehensive and systematic study on vision-language instruction tuning. We
transform 26 datasets into the instruction tuning format and group them into 11 task categories.
Among them, 13 held-in datasets are used for instruction tuning and 13 held-out datasets are
employed for zero-shot evaluation. Moreover, four task categories are purposely withheld for
task-level zero-shot evaluation. Exhaustive quantitative and qualitative results demonstrate the
effectiveness of InstructBLIP on vision-language zero-shot generalization.
• We propose instruction-aware visual feature extraction, a novel mechanism that enables ﬂexible
and informative feature extraction according to the given instructions. Speciﬁcally, the instruc-
tion is not only given to the frozen LLM as a condition for generating the text, but also provided
to the Q-Former as a condition for extracting visual features from the frozen image encoder.
• We evaluate and open-source a suite of InstructBLIP models using two families of LLMs: 1)
FlanT5 [7], an encoder-decoder LLM ﬁnetuned from T5 [33]; 2) Vicuna [2], a decoder-only
LLM ﬁnetuned from LLaMA [40]. The InstructBLIP models achieve state-of-the-art zero-shot
performance on a wide range of vision-language tasks. Furthermore, InstructBLIP models lead
to state-of-the-art ﬁnetuning performance when used as the model initialization on individual
downstream tasks.
2
Vision-Language Instruction Tuning
InstructBLIP aims to address the unique challenges in vision-language instruction tuning and provide
a systematic study on the models’ improved generalization ability to unseen data and tasks. In this
section, we ﬁrst introduce the construction of instruction-tuning data, followed by the training and
evaluation protocols. Then we delineate two techniques to improve instruction-tuning performance
from the model and data perspective, respectively. Lastly we present the implementation details.
Image Captioning 
COCO Caption
Web CapFilt
NoCaps
Flickr30K
Image Question Answering
VQAv2
Visual Reasoning
GQA
Visual Spatial
Reasoning
IconQA
Image Question Generation
VQAv2
OKVQA
A-OKVQA
Visual Conversational QA
Visual Dialog
Video Question Answering
MSVD QA
MSRVTT QA
iVQA
LLaVA-Instruct-150K
Visual Conversation
Complex Reasoning
Image Classification
VizWiz
Detailed Image
Description
HatefulMemes
Image Captioning 
Reading Comprehension
TextCaps
Image Question Answering
Reading Comprehension
OCR-VQA
TextVQA
Knowledge Grounded
Image Question Answering
OKVQA
A-OKVQA
ScienceQA
Figure 2: Tasks and their corresponding datasets used for vision-language instruction tuning. The
yellow and white colors represent the held-in and held-out datasets, respectively.
3

Input Image
Queries
…
Q-Former
…
Response
Fully
Connected
…
LLM
left one
Instruction
Image
Encoder
Instruction
Queries
Feed Forward
Self Attention
Feed Forward
Cross Attention
Q-Former
Choose the correct option to the 
following question: which picture 
shows the pizza inside the over?
Options: (a) left one (b) right one. 
Answer:
Image
Embeddings
…
Instruction
…
…
…
…
Instruction
Image Embeddings
Figure 3: Model architecture of InstructBLIP. The Q-Former extracts instruction-aware visual features
from the output embeddings of the frozen image encoder, and feeds the visual features as soft prompt
input to the frozen LLM. We instruction-tune the model with the language modeling loss to generate
the response.
2.1
Tasks and Datasets
To ensure the diversity of instruction tuning data while considering their accessibility, we gather
a wide range of publicly available vision-language datasets, and transform them into instruction
tuning format. As shown in Figure 2, the ﬁnal collection covers 11 task categories and 28 datasets,
including image captioning [23, 3, 50], image captioning with reading comprehension [37], visual
reasoning [16, 24, 28], image question answering [11, 12], knowledge-grounded image question
answering [29, 35, 27], image question answering with reading comprehension [30, 38], image
question generation (inversed from the QA datasets), video question answering [46, 48], visual
conversational question answering [8], image classiﬁcation [18], and LLaVA-Instruct-150K [25]. We
include detailed descriptions and statistics of each dataset in Appendix C.
For every task, we meticulously craft 10 to 15 distinct instruction templates in natural language.
These templates serve as the foundation for constructing instruction tuning data, which articulates
the task and delineates the objective. For public datasets inherently favoring short responses, we
use terms such as short and brieﬂy into some of their corresponding instruction templates to reduce
the risk of the model overﬁtting to always generating short outputs. For the LLaVA-Instruct-150K
dataset, we do not incorporate additional instruction templates since it is naturally structured in the
instruction format. The full list of instruction templates can be found in Appendix D.
2.2
Training and Evaluation Protocols
To encompass a wide range of tasks for training while simultaneously reserving an adequate amount
of unseen data for comprehensive zero-shot evaluations, we divide the 26 datasets into 13 held-in
datasets and 13 held-out datasets, denoted by yellow and white colors in Figure 2. We employ
training sets of held-in datasets for instruction tuning and utilize their validation or test sets for held-in
evaluation.
For held-out evaluation, our aim is to understand how instruction tuning improves the model’s zero-
shot generalization performance on unseen data. In this paper, we deﬁne two types of held-out data:
1) datasets not exposed to the model during training, but whose tasks are present within the held-in
cluster; 2) datasets and their associated tasks that remain entirely unseen during the training process.
Addressing the ﬁrst type of held-out evaluation is nontrivial due to image distribution shift between
held-in and held-out datasets. As for the second type, we hold out several tasks completely, including
visual reasoning, video question answering, visual conversational QA, and image classiﬁcation.
Datasets are selected carefully to avoid data contamination (no evaluation data appear in the held-in
training cluster across different datasets). During instruction tuning, we mix all the held-in training
sets and sample instruction templates uniformly for each dataset. The models are trained with the
4

standard language modeling loss to directly generate responses given instruction. Furthermore, for
datasets that involve scene texts, we add OCR tokens in the instruction as supplementary information.
2.3
Instruction-aware Visual Feature Extraction
Existing zero-shot image-to-text generation methods, including BLIP-2, take an instruction-agnostic
approach when extracting visual features. Namely, the visual input to the LLM is unaware of the
instruction, which detriments the ﬂexibility of the model across tasks. In contrast, an instruction-
aware vision model can enhance the model’s ability to learn from and follow different instructions.
Consider the following examples: 1) Given the same image, the model is instructed to complete two
different tasks; 2) Given two different images, the model is instructed to complete the same task. In
the ﬁrst example, an instruction-aware vision model could extract different features from the same
image tailored to the instruction, rendering more informative features in solving different tasks. In the
second example, an instruction-aware vision model could leverage the common knowledge embodied
in the instruction to extract features for two different images, enabling better knowledge transfer
between images.
InstructBLIP proposes an instruction-aware visual feature extraction approach by taking full advantage
of the Q-Former architecture in BLIP-2 models. As shown in Figure 3, the Q-Former is designed to
extract visual features from the output of a frozen image encoder. Speciﬁcally, a set of learnable query
embeddings interact with the frozen image encoder through cross-attention layers. The output features
of these queries are then projected as input visual prompt to the frozen LLM. The Q-Former has been
pretrained in two stages following the BLIP-2 paper, through which it learns to extract text-aligned
visual features that are digestible by the LLM. During inference, an instruction is appended after the
visual prompt to guide the LLM towards performing different tasks as speciﬁed.
In InstructBLIP, the instruction text is not only given as input to the LLM, but also to the Q-
Former. The instruction interacts with the queries through self-attention layers of the Q-Former,
which inﬂuences the queries towards extracting image features that are more informative of the
task as described by the instruction. The LLM thus receives more useful visual information for it
to complete the task. In Table 2, we demonstrate that instruction-aware visual feature extraction
provides non-trivial performance improvement for both held-in and held-out evaluations.
2.4
Training Dataset Balancing
Due to the large number of training datasets and the signiﬁcant differences in the size of each dataset,
mixing them uniformly could cause the model to overﬁt on smaller datasets while underﬁtting on
larger datasets. To mitigate such problem, we propose to sample datasets by their sizes (i.e., the
number of training samples) with a square root smoothing. Generally, given D datasets with sizes
{S1, S2, . . . , SD}, the probability of a data sample being selected from a dataset d during training is
formulated as: pd =
√Sd
PD
i=1
√Si . In addition to this weighting formular, we make manual adjustments
to the weights of certain datasets to improve convergence. This is necessary due to the inherent
differences in various datasets and tasks that require varying levels of training intensity, even when
they have similar sizes. Speciﬁcally, we lower the weight of A-OKVQA (multi-choice) and increase
the weight of OKVQA. In Table 2, we show that the data balancing strategy improves overall
performance for both held-in evaluation and held-out generalization.
2.5
Inference Methods
During inference time, we adopt two slightly different generation approaches for evaluation on
different datasets. For the majority of datasets, such as image captioning and open-ended VQA, the
instruction-tuned model is directly prompted to generate responses, which are subsequently compared
to the ground truth to calculate metrics. On the other hand, for classiﬁcation and multi-choice VQA
tasks, we employ a vocabulary ranking method following previous works [45, 22, 21]. Speciﬁcally,
we still prompt the model to generate answers, but restrict its vocabulary to a list of candidates. Then,
we calculate log-likelihood for each candidate and select the one with the highest value as the ﬁnal
prediction. This ranking method is applied to ScienceQA, IconQA, A-OKVQA (multiple-choice),
HatefulMemes, and Visual Dialog datasets. Furthermore, for binary classiﬁcation, the positive and
5

NoCaps Flickr
30K
GQA VSR IconQA TextVQA Visdial HM VizWiz SciQA
IMG
MSVD
QA
MSRVTT
QA
iVQA
Flamingo-3B [4]
-
60.6
-
-
-
30.1
-
53.7
28.9
-
27.5
11.0
32.7
Flamingo-9B [4]
-
61.5
-
-
-
31.8
-
57.0
28.8
-
30.2
13.7
35.2
Flamingo-80B [4]
-
67.2
-
-
-
35.0
-
46.4
31.6
-
35.6
17.4
40.7
BLIP-2 (FlanT5XL) [20]
104.5
76.1
41.9
60.5
45.5
43.1
45.7
53.0
29.8
54.9
33.7
16.2
40.4
BLIP-2 (FlanT5XXL) [20]
98.4
73.7
42.4
68.2
45.4
44.1
46.9
52.0
29.4
64.5
34.4
17.4
45.8
BLIP-2 (Vicuna-7B)
107.5
74.9
41.3
50.0
39.7
40.1
44.9
50.6
25.3
53.8
18.3
9.2
27.5
BLIP-2 (Vicuna-13B)
103.9
71.6
32.3
50.9
40.6
42.5
45.1
53.7
19.6
61.0
20.3
10.3
23.5
InstructBLIP (FlanT5XL)
119.9
84.5
48.4
64.8
50.0
46.6
46.6
56.6
32.7
70.4
43.4
25.0
53.1
InstructBLIP (FlanT5XXL)
120.0
83.5
47.9
65.6
51.2
46.6
48.5
54.1
30.9
70.6
44.3
25.6
53.8
InstructBLIP (Vicuna-7B)
123.1
82.4
49.2
54.3
43.1
50.1
45.2
59.6
34.5
60.5
41.8
22.1
52.2
InstructBLIP (Vicuna-13B)
121.9
82.8
49.5
52.1
44.8
50.7
45.4
57.5
33.4
63.1
41.2
24.8
51.0
Table 1: Zero-shot results on the held-out datasets. Here, Visdial, HM and SciQA denote the Visual
Dialog, HatefulMemes and ScienceQA datasets, respectively. For ScienceQA, we only evalute the set
with image context. Following previous works [4, 48, 31], we report the CIDEr score [41] for NoCaps
and Flickr30K, iVQA accuracy for iVQA, AUC score for HatefulMemes, and Mean Reciprocal Rank
(MRR) for Visual Dialog. For all remaining datasets, we report the top-1 accuracy (%).
negative labels are expanded into a broader vocabulary set to more effectively ﬁt the model’s language
modeling probability mass (e.g., yes and true for positive, and no and false for negative).
For the video question answering task, we utilize four uniformly-sampled frames per video sample.
Each frame is processed by the image encoder and Q-Former individually, and their resulting query
embeddings are concatenated together before fed into the LLM.
2.6
Implementation Details
Architecture.
We implement InstructBLIP in LAVIS library [19]. Thanks to the ﬂexibility enabled
by the modular architectural design of BLIP-2, we can quickly adapt the model to incorporate various
LLMs. In our experiments, we adopt four variations of BLIP-2 with the same image encoder (ViT-
g/14 [10]) but different frozen LLMs, including FlanT5-XL (3B), FlanT5-XXL (11B), Vicuna-7B
and Vicuna-13B. FlanT5 [7] is an instruction-tuned model based on the encoder-decoder Transformer
T5 [33]. Vicuna [2], on the other hand, is a recently released decoder-only Transformer instruction-
tuned from LLaMA [40]. During vision-language instruction tuning, we initialize the model from
pre-trained BLIP-2 checkpoints, and only ﬁnetune the parameters of Q-Former while keeping both
the image encoder and the LLM frozen. Since the original BLIP-2 models do not include Vicuna as
LLMs, we perform pre-training with Vicuna following the same procedure as BLIP-2.
Training and Hyper-parameters.
We instruction-tune all models with a maximum of 60K steps
and validate model’s performance every 3K steps. For each model, a single optimal checkpoint is
selected and used for evaluations on all datasets. We employ a batch size of 192, 128, and 64 for
the 3B, 7B, and 11/13B models, respectively. The AdamW [26] optimizer is used, with β1 = 0.9,
β2 = 0.999, and a weight decay of 0.05. Additionally, we apply a linear warmup of the learning
rate during the initial 1K steps, increasing from 10−8 to 10−5, followed by a cosine decay with a
minimum learning rate of 0. All models are trained utilizing 16 Nvidia A100 (40G) GPUs and are
completed within two days.
3
Results and Analysis
3.1
Zero-shot Evalatuion
We ﬁrst evaluate InstructBLIP models on the set of 13 held-out datasets. We compare InstructBLIP
with the previous SOTA models BLIP-2 and Flamingo. As demonstrated in Table 1, we achieve new
zero-shot SOTA results on all datasets. InstructBLIP consistently surpasses its original backbone,
BLIP-2, by a signiﬁcant margin across all LLMs, demonstrating the effectiveness of vision-language
instruction tuning. For instance, InstructBLIP FlanT5XL yields an average relative improvement
6

Model
Held-in Avg.
GQA
ScienceQA (IMG)
IconQA
VizWiz
iVQA
InstructBLIP (FlanT5XL)
94.1
48.4
70.4
50.0
32.7
53.1
−Instruction-aware Visual Feature
89.8
45.9 (↓2.5)
63.4 (↓7.0)
45.8 (↓4.2)
25.1 (↓7.6)
47.5 (↓5.6)
−Data Balancing
92.6
46.8 (↓1.6)
66.0 (↓4.4)
49.9 (↓0.1)
31.8 (↓0.9)
51.1 (↓2.0)
InstructBLIP (Vicuna-7B)
100.8
49.2
60.5
43.1
34.5
52.2
−Instruction-aware Visual Feature
98.9
48.2 (↓1.0)
55.2 (↓5.3)
41.2 (↓1.9)
32.4 (↓2.1)
36.8 (↓15.4)
−Data Balancing
98.8
47.8 (↓1.4)
59.4 (↓1.1)
43.5 (↑0.4)
32.3 (↓2.2)
50.3 (↓1.9)
Table 2: Results of ablation studies by removing the instruction-aware visual feature extraction
mechanism (Section 2.3) and the data balancing strategy (Section 2.4). For held-in evaluation, we
compute the average score of four datasets, including COCO Caption, OKVQA, A-OKVQA, and
TextCaps. For held-out evaluation, we show ﬁve datasets from different tasks.
of 15.0% when compared to BLIP-2 FlanT5XL. Furthermore, instruction tuning boosts zero-shot
generalization on unseen task categories such as video QA. InstructBLIP achieves up to 47.1%
relative improvement on MSRVTT-QA over the previous SOTA despite never being trained with
temporal video data. Finally, our smallest InstructBLIP FlanT5XL with 4B parameters outperforms
Flamingo-80B on all six shared evaluation datasets with an average relative improvement of 24.8%.
For the Visual Dialog dataset, we choose to report the Mean Reciprocal Rank (MRR) over the
Normalized Discounted Cumulative Gain (NDCG) metric. This is because NDCG favors generic and
uncertain answers while MRR prefers certain responses [31], making MRR better aligned with the
zero-shot evaluation scenario.
3.2
Ablation Study on Instruction Tuning Techniques
To investigate the impact of the instruction-aware visual feature extraction (Section 2.3) and the
dataset balancing strategy (Section 2.4), we conduct ablation studies by removing each one of
them respectively during the instruction tuning process. As illustrated in Table 2, the absence of
instruction awareness in visual features downgrades performance signiﬁcantly across all datasets. This
performance drop is more severe in datasets that involve spatial visual reasoning (e.g., ScienceQA) or
temporal visual reasoning (e.g., iVQA), where the instruction input to the Q-Former can guide it to
pay more attention to more informative image embeddings. With regard to the data balancing strategy,
its removal causes unstable training patterns, as different datasets achieve their peak performance at
signiﬁcantly disparate training steps. Consequently, this instability harms the overall performance.
3.3
Qualitative Evaluation
Besides the systematic evaluation on public benchmarks, we further qualitatively examine Instruct-
BLIP with more diverse images and instructions. As illustrated in Figure 1, InstructBLIP demonstrates
its capacity for complex visual reasoning. For example, it can reasonably infer what could have hap-
pened from the visual scene and deduce the type of disaster by extrapolating the location of the scene
from visual evidence (the palm trees). Moreover, InstructBLIP is capable of connecting visual input
with embedded textual knowledge and generate informative responses, such as intruducing a famous
painting. Furthermore, InstructBLIP exhibits the ability to comprehend the deeper, metaphorical
implications underlying the visual representation. Finally, we show that InstructBLIP can engage in
multi-turn conversations, effectively considering the dialog history when making new responses.
In Appendix B, we compare InstructBLIP with concurrent multimodal models (GPT-4 [32],
LLaVA [25], MiniGPT4 [51]). Although all models are capable of generating long-form responses,
InstructBLIP’s outputs generally contains more proper visual details and exhibits logically coherent
reasoning steps. Importantly, we argue that long-form responses are not always preferable. For
example, in Figure 6, InstructBLIP directly addresses the user’s intent by adaptively adjusting the
response length, while LLaVA and MiniGPT-4 generate long and less-relevant sentences. These
advantages of InstructBLIP are a result of the diverse instruction tuning data used and effective
architectural design.
7

3.4
Instruction Tuning vs. Multitask Learning
40
45
50
55
46.1
46.3
45.5
46.8
52.9
60
75
90
105
67.8
92.5
89.0
93.7
93.8
BLIP-2 Zero-shot
Train w/ Plain Input
Eval w/ Instruction
Train w/ Dataset Name
Eval w/ Instruction
Train w/ Dataset Name
Eval w/ Dataset Name
InstructBLIP
Multi-task
Held-out Avg.
Held-in Avg.
Figure 4: Comparison of instruction tuning and multitask training based on BLIP-2 FlanT5XL
backbone. For held-in evaluation, we compute the average score across all held-in datasets. For
held-out evaluation, we compute the average score across GQA, TextVQA, VSR, HatefulMemes,
IconQA, ScienceQA, iVQA, VizWiz.
A direct analogue to instruction tuning is multitask learning, a widely used method that involves
the simultaneous training of multiple datasets with the goal of improving the performance of each
individual dataset. To investigate whether the improvement in zero-shot generalization observed in
instruction tuning is mainly from the formatting of instructions or merely from multitasking, we
conduct a comparative analysis between these two approaches under identical training settings.
Following [45], we consider two multitask training approaches. In the ﬁrst approach, the model is
trained using the vanilla input-output format of the training datasets without instructions. During
evaluation, instructions are still provided to the model, indicating the speciﬁc task to be performed.
However, an exception is made for image captioning, as the model achieves better scores when only
receiving the image as input. For the second approach, we take a step towards instruction tuning by
prepending a [Task:Dataset] identiﬁer to the text input during training. For example, we prepend
[Visual question answering:VQAv2] for the VQAv2 dataset. During evaluation, we explore
both instructions and this identiﬁer. Particularly, for the identiﬁer of held-out datasets, we only use
the task name since the model never sees the dataset name.
The results are shown in Figure 4, including BLIP-2 zero-shot, multitask training, and instruction
tuning. All of these models are based on the BLIP-2 FlanT5XL backbone and adhere to the identical
training conﬁgurations delineated in Section 2. Overall, we can conclude two insights from the
results. Firstly, instruction tuning and multitask learning exhibit similar performance on the held-in
datasets. This suggests that the model can ﬁt these two different input patterns comparably well, as
long as it has been trained with such data. On the other hand, instruction tuning yields a signiﬁcant
improvement over multitask learning on unseen held-out datasets, whereas multitask learning still
performs on par with the original BLIP-2. This indicates that instruction tuning is the key to enhance
the model’s zero-shot generalization ability.
3.5
Finetuning InstructBLIP on Downstream Tasks
We further ﬁnetune the InstructBLIP models to investigate its performance on learning a speciﬁc
dataset. Compared to most previous methods (e.g., Flamingo, BLIP-2) which increase the input
image resolution and ﬁnetune the visual encoder on downstream tasks, InstructBLIP maintains the
same image resolution (224×224) during instruction tuning and keeps the visual encoder frozen
during ﬁnetuning. This signiﬁcantly reduces the number of trainable parameters from 1.2B to 188M,
thus greatly improves ﬁnetuning efﬁciency.
The results are shown in Table 3. Compared to BLIP-2, InstructBLIP leads to better ﬁnetuning
performance on all datasets, which validates InstructBLIP as a better weight initialization model
for task-speciﬁc ﬁnetuning. InstructBLIP sets new state-of-the-art ﬁnetuning performance on Sci-
8

ScienceQA
IMG
OCR-VQA
OKVQA
A-OKVQA
Direct Answer
Multi-choice
Val
Test
Val
Test
Previous SOTA
LLaVA [25]
89.0
GIT [42]
70.3
PaLM-E(562B) [9]
66.1
[15]
56.3
[36]
61.6
[15]
73.2
[36]
73.6
BLIP-2 (FlanT5XXL)
89.5
72.7
54.7
57.6
53.7
80.2
76.2
InstructBLIP (FlanT5XXL)
90.7
73.3
55.5
57.1
54.8
81.0
76.7
BLIP-2 (Vicuna-7B)
77.3
69.1
59.3
60.0
58.7
72.1
69.0
InstructBLIP (Vicuna-7B)
79.5
72.8
62.1
64.0
62.1
75.7
73.4
Table 3: Results of ﬁnetuning BLIP-2 and InstructBLIP on downstream datasets. Compared to
BLIP-2, InstructBLIP provides a better weight initialization model and achieves SOTA performance
on three out of four datasets.
enceQA (IMG), OCR-VQA, A-OKVQA, and is outperformed on OKVQA by PaLM-E [9] with
562B parameters.
Additionally, we observe that the FlanT5-based InstructBLIP is superior at multi-choice tasks,
whereas Vicuna-based InstructBLIP is generally better at open-ended generation tasks. This disparity
can be primarily attributed to the capabilities of their frozen LLMs, as they both employ the same
image encoder. Although FlanT5 and Vicuna are both instruction-tuned LLMs, their instruction data
signiﬁcantly differ. FlanT5 is mainly ﬁnetuned on NLP benchmarks containing many multi-choice
QA and classiﬁcation datasets, while Vicuna is ﬁnetuned on open-ended instruction-following data.
4
Related Work
Instruction tuning has been shown to improve the generalization performance of language models
to unseen tasks. Some methods collect instruction tuning data by converting existing NLP datasets
into instruction format [45, 7, 34, 44], others use LLMs (e.g., GPT3 [5]) to generate instruction
data [2, 13, 43, 39].
Instruction-tuned LLMs have been adapted to image-to-text generation by injecting visual information
to the LLMs. BLIP-2 [20] uses frozen FlanT5 models, and trains a Q-Former to extract visual features
as input to the LLMs. MiniGPT4 [51] uses the same pre-trained visual encoder and Q-Former
from BLIP-2, but uses Vicuna [2] as the LLM and performs training using longer image captions
(generated by ChatGPT [1]) than the ones used by BLIP-2. LLaVA [25] directly projects the output
of a visual encoder as input to a LLaMA/Vinuca LLM, and ﬁnetunes the LLM on vision-language
conversational data generated by GPT-4 [32]. mPLUG-owl [49] performs low-rank adaption [14]
to ﬁnetune a LLaMA [40] model using both text instruction data and vision-language instruction
data from LLaVA. A separate work is MultiInstruct [47], which performs vision-language instruction
tuning without a pre-trained LLM, leading to less competitive performance.
As shown in this paper, InstructBLIP presents instruction-tuned models with stronger performance
than existing models. InstructBLIP also provides a comprehensive analysis on various aspects of
vision-language instruction tuning, validating its advantage on generalizing to unseen tasks.
5
Conclusion
In this paper, we present InstructBLIP, a simple yet novel instruction tuning framework towards gen-
eralized vision-language models. We perform a comprehensive study on vision-language instruction
tuning and demonstrate the capability of InstructBLIP models to generalize to a wide range of unseen
tasks with state-of-the-art performance. Qualitative examples also exhibit InstructBLIP’s various
capabilities on instruction following, such as complex visual reasoning, knowledge-grounded image
description, and multi-turn conversations. Furthermore, we show that InstructBLIP can serve as an
enhanced model initialization for downstream task ﬁnetuning, achieving state-of-the-art results. We
hope that InstructBLIP can spur new research in general-purpose multimodal AI and its applications.
9

References
[1] Chatgpt. https://openai.com/blog/chatgpt, 2023. 9
[2] Vicuna. https://github.com/lm-sys/FastChat, 2023. 3, 6, 9
[3] Harsh Agrawal, Karan Desai, Yufei Wang, Xinlei Chen, Rishabh Jain, Mark Johnson, Dhruv Batra, Devi
Parikh, Stefan Lee, and Peter Anderson. nocaps: novel object captioning at scale. In ICCV, pages
8948–8957, 2019. 4, 16
[4] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, Antoine Miech, Iain Barr, Yana Hasson, Karel Lenc,
Arthur Mensch, Katherine Millican, Malcolm Reynolds, Roman Ring, Eliza Rutherford, Serkan Cabi,
Tengda Han, Zhitao Gong, Sina Samangooei, Marianne Monteiro, Jacob L Menick, Sebastian Borgeaud,
Andy Brock, Aida Nematzadeh, Sahand Sharifzadeh, Mikoł aj Bi´nkowski, Ricardo Barreira, Oriol Vinyals,
Andrew Zisserman, and Karén Simonyan. Flamingo: a visual language model for few-shot learning. In
S. Koyejo, S. Mohamed, A. Agarwal, D. Belgrave, K. Cho, and A. Oh, editors, NeurIPS, 2022. 1, 6
[5] Tom B. Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, Sandhini Agarwal, Ariel Herbert-Voss,
Gretchen Krueger, Tom Henighan, Rewon Child, Aditya Ramesh, Daniel M. Ziegler, Jeffrey Wu, Clemens
Winter, Christopher Hesse, Mark Chen, Eric Sigler, Mateusz Litwin, Scott Gray, Benjamin Chess, Jack
Clark, Christopher Berner, Sam McCandlish, Alec Radford, Ilya Sutskever, and Dario Amodei. Language
models are few-shot learners. arXiv preprint arXiv:2005.14165, 2020. 9
[6] Jaemin Cho, Jie Lei, Hao Tan, and Mohit Bansal. Unifying vision-and-language tasks via text generation.
arXiv preprint arXiv:2102.02779, 2021. 1
[7] Hyung Won Chung, Le Hou, Shayne Longpre, Barret Zoph, Yi Tay, William Fedus, Eric Li, Xuezhi
Wang, Mostafa Dehghani, Siddhartha Brahma, Albert Webson, Shixiang Shane Gu, Zhuyun Dai, Mirac
Suzgun, Xinyun Chen, Aakanksha Chowdhery, Sharan Narang, Gaurav Mishra, Adams Yu, Vincent Y.
Zhao, Yanping Huang, Andrew M. Dai, Hongkun Yu, Slav Petrov, Ed H. Chi, Jeff Dean, Jacob Devlin,
Adam Roberts, Denny Zhou, Quoc V. Le, and Jason Wei. Scaling instruction-ﬁnetuned language models.
arXiv preprint arXiv:2210.11416, 2022. 1, 3, 6, 9
[8] Abhishek Das, Satwik Kottur, Khushi Gupta, Avi Singh, Deshraj Yadav, Jose M. F. Moura, Devi Parikh,
and Dhruv Batra. Visual dialog. In CVPR, 2017. 4, 16
[9] Danny Driess, Fei Xia, Mehdi S. M. Sajjadi, Corey Lynch, Aakanksha Chowdhery, Brian Ichter, Ayzaan
Wahid, Jonathan Tompson, Quan Vuong, Tianhe Yu, Wenlong Huang, Yevgen Chebotar, Pierre Sermanet,
Daniel Duckworth, Sergey Levine, Vincent Vanhoucke, Karol Hausman, Marc Toussaint, Klaus Greff,
Andy Zeng, Igor Mordatch, and Pete Florence. Palm-e: An embodied multimodal language model, 2023. 9
[10] Yuxin Fang, Wen Wang, Binhui Xie, Quan-Sen Sun, Ledell Yu Wu, Xinggang Wang, Tiejun Huang,
Xinlong Wang, and Yue Cao. Eva: Exploring the limits of masked visual representation learning at scale.
ArXiv, abs/2211.07636, 2022. 6
[11] Yash Goyal, Tejas Khot, Douglas Summers-Stay, Dhruv Batra, and Devi Parikh. Making the v in vqa
matter: Elevating the role of image understanding in visual question answering. In CVPR, July 2017. 4, 16
[12] Danna Gurari, Qing Li, Abigale J. Stangl, Anhong Guo, Chi Lin, Kristen Grauman, Jiebo Luo, and Jeffrey P.
Bigham. Vizwiz grand challenge: Answering visual questions from blind people. In CVPR, 2018. 4, 16
[13] Or Honovich, Thomas Scialom, Omer Levy, and Timo Schick. Unnatural instructions: Tuning language
models with (almost) no human labor. ArXiv, abs/2212.09689, 2022. 9
[14] Edward J. Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and
Weizhu Chen. Lora: Low-rank adaptation of large language models. In ICLR, 2022. 9
[15] Yushi Hu, Hang Hua, Zhengyuan Yang, Weijia Shi, Noah A. Smith, and Jiebo Luo. Promptcap: Prompt-
guided task-aware image captioning, 2023. 9
[16] Drew A. Hudson and Christopher D. Manning. Gqa: A new dataset for real-world visual reasoning and
compositional question answering. In CVPR, 2019. 4, 16
[17] Andrej Karpathy and Li Fei-Fei. Deep visual-semantic alignments for generating image descriptions. In
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR), June 2015. 16
[18] Douwe Kiela, Hamed Firooz, Aravind Mohan, Vedanuj Goswami, Amanpreet Singh, Pratik Ringshia,
and Davide Testuggine. The hateful memes challenge: Detecting hate speech in multimodal memes. In
NeurIPS, 2020. 4, 16
[19] Dongxu Li, Junnan Li, Hung Le, Guangsen Wang, Silvio Savarese, and Steven CH Hoi. Lavis: A library
for language-vision intelligence. arXiv preprint arXiv:2209.09019, 2022. 6
[20] Junnan Li, Dongxu Li, Silvio Savarese, and Steven Hoi. Blip-2: Bootstrapping language-image pre-training
with frozen image encoders and large language models. In ICML, 2023. 1, 6, 9, 16
10

[21] Junnan Li, Dongxu Li, Caiming Xiong, and Steven Hoi. Blip: Bootstrapping language-image pre-training
for uniﬁed vision-language understanding and generation. In ICML, 2022. 5, 16
[22] Junnan Li, Ramprasaath Selvaraju, Akhilesh Gotmare, Shaﬁq Joty, Caiming Xiong, and Steven Chu Hong
Hoi. Align before fuse: Vision and language representation learning with momentum distillation. In
NeurIPS, 2021. 5
[23] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár,
and C. Lawrence Zitnick. Microsoft coco: Common objects in context. In ECCV, 2014. 4, 16
[24] Fangyu Liu, Guy Edward Toh Emerson, and Nigel Collier. Visual spatial reasoning. Transactions of the
Association for Computational Linguistics, 2023. 4
[25] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. Visual instruction tuning. 2023. 4, 7, 9, 16
[26] Ilya Loshchilov and Frank Hutter. Decoupled weight decay regularization. In ICLR, 2019. 6
[27] Pan Lu, Swaroop Mishra, Tony Xia, Liang Qiu, Kai-Wei Chang, Song-Chun Zhu, Oyvind Tafjord, Peter
Clark, and Ashwin Kalyan. Learn to explain: Multimodal reasoning via thought chains for science question
answering. In NeurIPS, 2022. 4, 16
[28] Pan Lu, Liang Qiu, Jiaqi Chen, Tony Xia, Yizhou Zhao, Wei Zhang, Zhou Yu, Xiaodan Liang, and
Song-Chun Zhu. Iconqa: A new benchmark for abstract diagram understanding and visual language
reasoning. In NeurIPS Track on Datasets and Benchmarks, 2021. 4, 16
[29] Kenneth Marino, Mohammad Rastegari, Ali Farhadi, and Roozbeh Mottaghi. Ok-vqa: A visual question
answering benchmark requiring external knowledge. In CVPR, 2019. 4, 16
[30] Anand Mishra, Shashank Shekhar, Ajeet Kumar Singh, and Anirban Chakraborty. Ocr-vqa: Visual question
answering by reading text in images. In ICDAR, 2019. 4, 16
[31] Vishvak Murahari, Dhruv Batra, Devi Parikh, and Abhishek Das. Large-scale pretraining for visual dialog:
A simple state-of-the-art baseline. In Andrea Vedaldi, Horst Bischof, Thomas Brox, and Jan-Michael
Frahm, editors, ECCV, 2020. 6, 7
[32] OpenAI. Gpt-4 technical report. ArXiv, abs/2303.08774, 2023. 7, 9
[33] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J Liu. Exploring the limits of transfer learning with a uniﬁed text-to-text transformer.
The Journal of Machine Learning Research, 2020. 3, 6
[34] Victor Sanh, Albert Webson, Colin Raffel, Stephen H. Bach, Lintang Sutawika, Zaid Alyafeai, An-
toine Chafﬁn, Arnaud Stiegler, Arun Raja, Manan Dey, M Saiful Bari, Canwen Xu, Urmish Thakker,
Shanya Sharma Sharma, Eliza Szczechla, Taewoon Kim, Gunjan Chhablani, Nihal V. Nayak, Debajyoti
Datta, Jonathan Chang, Mike Tian-Jian Jiang, Han Wang, Matteo Manica, Sheng Shen, Zheng Xin Yong,
Harshit Pandey, Rachel Bawden, Thomas Wang, Trishala Neeraj, Jos Rozen, Abheesht Sharma, Andrea
Santilli, Thibault Févry, Jason Alan Fries, Ryan Teehan, Teven Le Scao, Stella Biderman, Leo Gao, Thomas
Wolf, and Alexander M. Rush. Multitask prompted training enables zero-shot task generalization. In ICLR,
2022. 9
[35] Dustin Schwenk, Apoorv Khandelwal, Christopher Clark, Kenneth Marino, and Roozbeh Mottaghi. A-
okvqa: A benchmark for visual question answering using world knowledge. In Shai Avidan, Gabriel
Brostow, Moustapha Cissé, Giovanni Maria Farinella, and Tal Hassner, editors, ECCV, 2022. 4, 16
[36] Zhenwei Shao, Zhou Yu, Meng Wang, and Jun Yu. Prompting large language models with answer heuristics
for knowledge-based visual question answering. Computer Vision and Pattern Recognition (CVPR), 2023.
9
[37] Oleksii Sidorov, Ronghang Hu, Marcus Rohrbach, and Amanpreet Singh. Textcaps: a dataset for image
captioningwith reading comprehension. 2020. 4, 16
[38] Amanpreet Singh, Vivek Natarjan, Meet Shah, Yu Jiang, Xinlei Chen, Devi Parikh, and Marcus Rohrbach.
Towards vqa models that can read. In CVPR, pages 8317–8326, 2019. 4, 16
[39] Rohan Taori, Ishaan Gulrajani, Tianyi Zhang, Yann Dubois, Xuechen Li, Carlos Guestrin, Percy Liang,
and Tatsunori B. Hashimoto. Stanford alpaca: An instruction-following llama model. https://github.
com/tatsu-lab/stanford_alpaca, 2023. 9
[40] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix,
Baptiste Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, Aurelien Rodriguez, Armand Joulin, Edouard
Grave, and Guillaume Lample. Llama: Open and efﬁcient foundation language models. arXiv preprint
arXiv:2302.13971, 2023. 3, 6, 9
[41] Ramakrishna Vedantam, C. Lawrence Zitnick, and Devi Parikh. Cider: Consensus-based image description
evaluation. In 2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pages
4566–4575, 2015. 6
11

[42] Jianfeng Wang, Zhengyuan Yang, Xiaowei Hu, Linjie Li, Kevin Lin, Zhe Gan, Zicheng Liu, Ce Liu, and
Lijuan Wang. Git: A generative image-to-text transformer for vision and language, 2022. 9
[43] Yizhong Wang, Yeganeh Kordi, Swaroop Mishra, Alisa Liu, Noah A. Smith, Daniel Khashabi, and
Hannaneh Hajishirzi. Self-instruct: Aligning language model with self generated instructions. ArXiv,
abs/2212.10560, 2022. 9
[44] Yizhong Wang, Swaroop Mishra, Pegah Alipoormolabashi, Yeganeh Kordi, Amirreza Mirzaei, Atharva
Naik, Arjun Ashok, Arut Selvan Dhanasekaran, Anjana Arunkumar, David Stap, Eshaan Pathak, Giannis
Karamanolakis, Haizhi Lai, Ishan Purohit, Ishani Mondal, Jacob Anderson, Kirby Kuznia, Krima Doshi,
Kuntal Kumar Pal, Maitreya Patel, Mehrad Moradshahi, Mihir Parmar, Mirali Purohit, Neeraj Varshney,
Phani Rohitha Kaza, Pulkit Verma, Ravsehaj Singh Puri, Rushang Karia, Savan Doshi, Shailaja Keyur
Sampat, Siddhartha Mishra, Sujan Reddy A, Sumanta Patro, Tanay Dixit, and Xudong Shen. Super-
NaturalInstructions: Generalization via declarative instructions on 1600+ NLP tasks. In EMNLP, 2022.
9
[45] Jason Wei, Maarten Bosma, Vincent Y. Zhao, Kelvin Guu, Adams Wei Yu, Brian Lester, Nan Du, Andrew M.
Dai, and Quoc V. Le. Finetuned language models are zero-shot learners. In ICLR, 2022. 1, 5, 8, 9
[46] Dejing Xu, Zhou Zhao, Jun Xiao, Fei Wu, Hanwang Zhang, Xiangnan He, and Yueting Zhuang. Video
question answering via gradually reﬁned attention over appearance and motion. In Proceedings of the 25th
ACM International Conference on Multimedia, page 1645–1653, 2017. 4, 16
[47] Zhiyang Xu, Ying Shen, and Lifu Huang. Multiinstruct: Improving multi-modal zero-shot learning via
instruction tuning. ArXiv, abs/2212.10773, 2022. 9
[48] Antoine Yang, Antoine Miech, Josef Sivic, Ivan Laptev, and Cordelia Schmid. Just ask: Learning to answer
questions from millions of narrated videos. In ICCV, pages 1686–1697, 2021. 4, 6, 16
[49] Qinghao Ye, Haiyang Xu, Guohai Xu, Jiabo Ye, Ming Yan, Yi Zhou, Junyan Wang, Anwen Hu, Pengcheng
Shi, Yaya Shi, Chenliang Li, Yuanhong Xu, Hehong Chen, Junfeng Tian, Qiang Qi, Ji Chao Zhang, and
Feiyan Huang. mplug-owl: Modularization empowers large language models with multimodality. 2023. 9
[50] Peter Young, Alice Lai, Micah Hodosh, and Julia Hockenmaier. From image descriptions to visual
denotations: New similarity metrics for semantic inference over event descriptions. Transactions of the
Association for Computational Linguistics, 2, 2014. 4, 16
[51] Deyao Zhu, Jun Chen, Xiaoqian Shen, Xiang Li, and Mohamed Elhoseiny. Minigpt-4: Enhancing
vision-language understanding with advanced large language models, 2023. 7, 9
12

A
Broader Impact
InstructBLIP uses off-the-shelf frozen LLMs. Therefore it inherits some of the shortcomings from the
original LLMs, such as hallucinating ungrounded text or generating outputs with bias. We mitigate
such shortcomings by improving the model’s grounding on the vision and instruction input, and
performing vision-language instruction tuning on a diverse set of high-quality datasets. Nevertheless,
we do not recommend applying InstructBLIP models to any downstream applications without a prior
assessment on safety and fairness speciﬁc to that application.
B
More Case Studies
[Input Image]
What is unusual about this image?
[Model Response]
InstructBLIP
The unusual aspect of this image is that a man is ironing clothes on the
back of a yellow taxi cab while it is driving down a bus city street. He is
standing on top of the car's trunk, holding an iron and using it to press his
clothing items. This situation is not only unconventional but also
potentially hazardous, as the man could be at risk of falling off the vehicle
or causing an accident due to his actions.
LLaVA
The unusual aspect of this image is a man ironing clothes on the back of a
minivan or van. This is not a typical place to perform this activity, as one
would usually iron clothes in a more stationary and safe location, such as
a home, using a regular ironing board. The scene depicted in the image is
peculiar as it involves a makeshift ironing setup on a vehicle, which can
be both unsafe and unconventional. Additionally, it is not clear how the
man is able to maintain balance and stability while ironing clothes in such
an unstable environment.
MiniGPT-4
The image shows a man standing on the back of a yellow taxi, with a
yellow shirt and black pants, and a blue backpack on his back. The taxi is
driving on a city street with othr cars and taxis in the background.
GPT-4
The unusual thing about this image is that a man is ironing clothes on an
ironing board attached to the roof of a moving taxi.
[Input Instruction]
Figure 5: The response from InstructBLIP is more comprehensive than GPT-4, more visually-
grounded than LLaVA, and more logical than MiniGPT-4. The responses of GPT-4 and LLaVA
are obtained from their respective papers, while the ofﬁcial demo is used for MiniGPT-4. Source:
https://www.barnorama.com/wp-content/uploads/2016/12/03-Confusing-Pictures.jpg
13

[Input Image]
Do you know who drew this painting?
[Model Response]
InstructBLIP
LLaVA
MiniGPT-4
The painting depicts a woman, commonly believed to be Mona Lisa, the
famous artwork by Leonardo da Vinci. It is a portrait painting that
showcases the woman's enigmatic smile and has become one of the most
famous and iconic art pieces in the world. The original work is displayed
in the Louvre Museum in Paris, and it is known for its intricate details,
use of oil paint, and the artist's innovative techniques that contributed to
its enduring appeal and mystery.
The painting is the Mona Lisa, also known as the Mona Lisa, by Leonardo
da Vinci. It is one of the most famous paintings in the world and is known
for its enigmatic smile and the mysterious identity of the sitter. The
painting is housed in the Louvre Museum in Paris, France.
Leonardo da Vinci.
[Input Instruction]
Figure 6: In this example, we illustrate that long-form responses are not always preferable. Instruct-
BLIP can directly address the user’s intent by adaptively adjusting the response length, while other
models tend to generate lengthy paragraphs with less-relevant sentences. The response from LLaVA
is taken from the paper, and for MiniGPT-4, we utilize its ofﬁcial demo.
14

[Input Image]
[Input Instruction]
Introduce me this painting in detail.
[Model Response]
InstructBLIP
LLaVA
MiniGPT-4
The painting features a woman with a headscarf, possibly of Dutch origin,
dressed in blue and white clothing. She is wearing a large blue hat and has
a red mouth, which adds a vibrant touch to the portrait. The woman is
looking directly at the viewer, creating a sense of connection. The
background of the painting is predominantly blue, further emphasizing the
color theme of the piece. The overall composition and attire of the woman
suggest that the painting might be inspired by the style of Rembrandt.
The painting depicts a young woman with a pearl earring, wearing a blue
dress and a white apron. She is shown with a serious expression, looking
directly at the viewer. The background is a dark, neutral tone, with no
visible objects or elements. The overall mood of the painting is somber
and contemplative.
The painting is well-executed, with a high level of detail and realism in
the depiction of the woman’s features and clothing. The use of shading
and lighting creates a sense of depth and dimension, making the woman
appear three-dimensional. The artist has used a limited color palette, with
the woman’s dress and apron providing the only sources of color in the
painting.
The painting is well-composed, with the woman’s head and shoulders
positioned in the center of the canvas, creating a sense of balance and
symmetry. The use of negative space around the woman’s figure adds to
the overall somber mood of the painting.
Overall, this is a well-executed and thought-provoking painting that
effectively conveys a sense of contemplation and introspection.
The painting depicts a young girl with a pearl earring, also known as "The
Girl with the Pearl Earring" or "Girl with a Pearl Earring". She is wearing
a blue dress and has a pearl earring on her left ear. Her hair is pulled back
into a bun, and she is looking directly at the viewer. This painting is a
famous work of art by Dutch painter Johannes Vermeer, created in 1665.
It is considered one of his masterpieces and is widely recognized for its
intricate details and realistic portrayal of the young girl’s facial
expression.
Figure 7: InstructBLIP shows the ability to connect textual knowledge with visual input, while the
other models depict the picture plainly. MiniGPT-4 exhibits poorer results, which may be due to its
training with only long captions. Responses of LLaVA and MiniGPT-4 are generated by their ofﬁcial
demos.
15

C
Instruction Tuning Datasets
Dataset Name
Held-out
Dataset Description
COCO Caption [23]

We use the large-scale COCO dataset for the image captioning task. Speciﬁcally, Karpathy
split [17] is used, which divides the data into 82K/5K/5K images for the train/val/test sets.
Web CapFilt

14M image-text pairs collected from the web with additional BLIP-generated synthetic
captions, used in BLIP [21] and BLIP-2 [20].
NoCaps [3]
 (val)
NoCaps contains 15,100 images with 166,100 human-written captions for novel object
image captioning.
Flickr30K [50]
 (test)
The Flickr30k dataset consists of 31K images collected from Flickr, each image has ﬁve
ground truth captions. We use the test split as the held-out which contains 1K images.
TextCaps [37]

TextCaps is an image captioning dataset that requires the model to comprehend and reason
the text in images. Its train/val/test sets contain 21K/3K/3K images, respectively.
VQAv2 [11]

VQAv2 is dataset for open-ended image question answering. It is split into 82K/40K/81K
for train/val/test.
VizWiz [12]
 (test-dev)
A dataset contains visual questions asked by people who are blind. 8K images are used for
the held-out evaluation.
GQA [16]
 (test-dev)
GQA contains image questions for scene understanding and reasoning. We use the bal-
anced test-dev set as held-out.
Visual Spatial Reasoning
 (test)
VSR is a collection of image-text pairs, in which the text describes the spatial relation of
two objects in the image. Models are required to classify true/false for the description. We
use the zero-shot data split given in its ofﬁcial github repository.
IconQA [28]
 (test)
IconQA measures the abstract diagram understanding and comprehensive cognitive rea-
soning abilities of models. We use the test set of its multi-text-choice task for held-out
evaluation.
OKVQA [29]

OKVQA contains visual questions that require outside knowledge to answer. It has been
split into 9K/5K for train and test.
A-OKVQA [35]

A-OKVQA is a successor of OKVQA with more challenging and diverse questions. It has
17K/1K/6K questions for train/val/test.
ScienceQA [27]
 (test)
ScienceQA covers diverse science topics with corresponding lectures and explanations. In
out settings, we only use the part with image context (IMG).
Visual Dialog [8]
 (val)
Visual dialog is a conversational question answering dataset. We use the val split as the
held-out, which contains 2,064 images and each has 10 rounds.
OCR-VQA [30]

OCR-VQA contains visual questions that require models to read text in the image. It has
800K/100K/100K for train/val/test, respectively.
TextVQA [38]
 (val)
TextVQA requires models to comprehend visual text to answer questions.
HatefulMemes [18]
 (val)
A binary classiﬁcation dataset to justify whether a meme contains hateful content.
LLaVA-Instruct-150K [25]

An instruction tuning dataset which has three parts: detailed caption (23K), reasoning
(77K), conversation (58K).
MSVD-QA [46]
 (test)
We use the test set (13K video QA pairs) of MSVD-QA for held-out testing.
MSRVTT-QA [46]
 (test)
MSRVTT-QA has more complex scenes than MSVD, with 72K video QA pairs as the test
set.
iVQA [48]
 (test)
iVQA is a video QA dataset with mitigated language biases. It has 6K/2K/2K samples for
train/val/test.
Table 4: Description of datasets in our held-in instruction tuning and held-out zero-shot evaluations.
16

D
Instruction Templates
Task
Instruction Template
Image
Captioning
<Image>A short image caption:
<Image>A short image description:
<Image>A photo of
<Image>An image that shows
<Image>Write a short description for the image.
<Image>Write a description for the photo.
<Image>Provide a description of what is presented in the photo.
<Image>Brieﬂy describe the content of the image.
<Image>Can you brieﬂy explain what you see in the image?
<Image>Could you use a few words to describe what you perceive in the photo?
<Image>Please provide a short depiction of the picture.
<Image>Using language, provide a short account of the image.
<Image>Use a few words to illustrate what is happening in the picture.
VQA
<Image>{Question}
<Image>Question: {Question}
<Image>{Question} A short answer to the question is
<Image>Q: {Question} A:
<Image>Question: {Question} Short answer:
<Image>Given the image, answer the following question with no more than three words. {Question}
<Image>Based on the image, respond to this question with a short answer: {Question}. Answer:
<Image>Use the provided image to answer the question: {Question} Provide your answer as short as possible:
<Image>What is the answer to the following question? "{Question}"
<Image>The question "{Question}" can be answered using the image. A short answer is
VQG
<Image>Given the image, generate a question whose answer is: {Answer}. Question:
<Image>Based on the image, provide a question with the answer: {Answer}. Question:
<Image>Given the visual representation, create a question for which the answer is "{Answer}".
<Image>From the image provided, craft a question that leads to the reply: {Answer}. Question:
<Image>Considering the picture, come up with a question where the answer is: {Answer}.
<Image>Taking the image into account, generate an question that has the answer: {Answer}. Question:
Table 5: Instruction templates used for transforming held-in datasets into instruction tuning data. For
datasets with OCR tokens, we simply add “OCR tokens:” after the image query embeddings.
17

