On the Low-Dimensional Structure of
Bayesian Inference
by
Alessio Spantini
B.Sc., Politecnico di Milano (2011)
S.M., Massachusetts Institute of Technology (2013)
Submitted to the Department of Aeronautics and Astronautics
in partial fulfillment of the requirements for the degree of
Doctor of Philosophy in Statistical Inference and Applied Probability
at the
MASSACHUSETTS INSTITUTE OF TECHNOLOGY
September 2017
¬© Massachusetts Institute of Technology 2017. All rights reserved.
Author . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Department of Aeronautics and Astronautics
August 24, 2017
Certified by. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Youssef M. Marzouk
Associate Professor of Aeronautics and Astronautics
Thesis Supervisor
Certified by. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Themistoklis Sapsis
Associate Professor of Mechanical Engineering
Thesis Committee Member
Certified by. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Alan S. Willsky
Professor of Electrical Engineering
Thesis Committee Member
Accepted by . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
Hamsa Balakrishnan
Chairman, Graduate Program Committee

2

On the Low-Dimensional Structure of
Bayesian Inference
by
Alessio Spantini
Submitted to the Department of Aeronautics and Astronautics
on August 24, 2017, in partial fulfillment of the
requirements for the degree of
Doctor of Philosophy in Statistical Inference and Applied Probability
Abstract
The Bayesian approach to inference characterizes model parameters and predictions
through the exploration of their posterior distributions, i.e., their distributions con-
ditioned on available data. The Bayesian paradigm provides a flexible, principled
framework for quantifying uncertainty, wherein heterogeneous and incomplete sources
of information (e.g., prior knowledge, noisy observations, imperfect models) can be
properly rationalized. Yet a major obstacle to deploying Bayesian inference in realis-
tic applications is computational: characterizing the associated high-dimensional and
non-Gaussian posterior distributions remains a challenging task.
While the Bayesian formulation is quite general, essential features of a statistical
model can bring additional structure to the Bayesian update. For instance, the prior
distribution often encodes some kind of regularity in the parameters; observations
might be sparse and corrupted by noise; observations might also be indirect, related to
the parameters by a forward operator that filters out some information; the posterior
distribution might satisfy conditional independence assumptions that reflect local
probabilistic interactions; and in some cases we might be uninterested in the posterior
distribution per se, but rather in specific prediction goals.
In this thesis we: (1) provide a rigorous mathematical characterization of low-
dimensional structures that enable efficient Bayesian inference in high-dimensional
and continuous parameter spaces; and (2) exploit this characterization to devise new
structure-exploiting and computationally efficient inference algorithms.
Our contributions encompass multiple related topics. First we characterize optimal
low-rank approximations of linear‚ÄìGaussian Bayesian inverse problems, and of their
goal-oriented extensions. Then we turn to inference in the nonlinear non-Gaussian
setting‚Äîanalyzing the sparsity, decomposability, and low-rank structure of determin-
istic couplings between distributions. These couplings facilitate efficient computation
of posterior expectations in generically non-Gaussian settings. Based on this anal-
ysis, we introduce a number of approaches for representing non-Gaussian Markov
random fields and for exploiting their conditional independence structure in compu-
tation by means of sparse nonlinear transport maps. We also develop new variational
3

algorithms for nonlinear smoothing and sequential parameter estimation. These algo-
rithms can be understood as the natural generalization‚Äîto the non-Gaussian case‚Äîof
the square-root Rauch‚ÄìTung‚ÄìStriebel Gaussian smoother. Finally, we outline a new
class of nonlinear filters induced by local couplings, for inference in high-dimensional
spatiotemporal processes with chaotic dynamics.
Thesis Supervisor: Youssef M. Marzouk
Title: Associate Professor of Aeronautics and Astronautics
Thesis Committee Member: Themistoklis Sapsis
Title: Associate Professor of Mechanical Engineering
Thesis Committee Member: Alan S. Willsky
Title: Professor of Electrical Engineering
4

Contents
1
Introduction
17
1.1
Low-dimensional structure in Bayesian inference . . . . . . . . . . . .
17
1.2
Thesis contributions and roadmap . . . . . . . . . . . . . . . . . . . .
20
2
Notation
25
3
Optimal low-rank approximations of linear inverse problems
29
3.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
29
3.2
Optimal approximation of the posterior covariance matrix
. . . . . .
33
3.2.1
Defining the approximation class
. . . . . . . . . . . . . . . .
34
3.2.2
Optimality criteria: metrics between distributions . . . . . . .
34
3.2.3
Optimality results . . . . . . . . . . . . . . . . . . . . . . . . .
37
3.2.4
Computing eigenpairs of (ùêª, Œì‚àí1
pr ) . . . . . . . . . . . . . . . .
38
3.3
Properties of the optimal covariance approximation . . . . . . . . . .
39
3.3.1
Interpretation of the eigendirections . . . . . . . . . . . . . . .
40
3.3.2
Optimal projector . . . . . . . . . . . . . . . . . . . . . . . . .
41
3.3.3
Comparison with optimality in Frobenius norm
. . . . . . . .
42
3.3.4
Suboptimal posterior covariance approximations . . . . . . . .
44
3.4
Optimal approximation of the posterior mean
. . . . . . . . . . . . .
47
3.4.1
Optimality results . . . . . . . . . . . . . . . . . . . . . . . . .
48
3.4.2
Connection with ‚Äúpriorconditioners‚Äù . . . . . . . . . . . . . . .
52
3.5
Numerical examples . . . . . . . . . . . . . . . . . . . . . . . . . . . .
54
3.5.1
Example 1: Hessian and prior with controlled spectra . . . . .
54
5

3.5.2
Example 2: X-ray tomography . . . . . . . . . . . . . . . . . .
58
3.5.3
Example 3: Heat equation . . . . . . . . . . . . . . . . . . . .
66
3.6
Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
72
4
Goal-oriented optimal approximations of linear inverse problems
75
4.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
75
4.2
Theory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
78
4.2.1
Approximation of the posterior covariance of the QoI . . . . .
78
4.2.2
Approximation of the posterior mean of the QoI . . . . . . . .
88
4.3
Proof-of-concept example . . . . . . . . . . . . . . . . . . . . . . . . .
90
4.4
Numerical examples . . . . . . . . . . . . . . . . . . . . . . . . . . . .
92
4.4.1
Forward, observational and prior models . . . . . . . . . . . .
93
4.4.2
Goal-oriented linear inverse problem
. . . . . . . . . . . . . .
95
4.4.3
A nonlinear QoI . . . . . . . . . . . . . . . . . . . . . . . . . .
98
4.5
Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
99
5
The structure of low-dimensional couplings
107
5.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
107
5.2
Triangular transport maps: a building block . . . . . . . . . . . . . .
110
5.3
Markov networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
114
5.4
Sparsity of triangular transport maps . . . . . . . . . . . . . . . . . .
117
5.4.1
Sparsity bounds . . . . . . . . . . . . . . . . . . . . . . . . . .
117
5.4.2
Connection to Gaussian Markov random fields . . . . . . . . .
120
5.4.3
Ordering of triangular maps . . . . . . . . . . . . . . . . . . .
123
5.5
Decomposability of transport maps . . . . . . . . . . . . . . . . . . .
125
5.5.1
Preliminary notions . . . . . . . . . . . . . . . . . . . . . . . .
126
5.5.2
Decomposition and graph sparsification . . . . . . . . . . . . .
128
5.5.3
Recursive decompositions
. . . . . . . . . . . . . . . . . . . .
131
5.5.4
Computation of decomposable transports . . . . . . . . . . . .
135
5.6
Sequential inference on state-space models: variational algorithms . .
138
5.6.1
Smoothing and filtering: the full Bayesian solution
. . . . . .
139
6

5.6.2
The linear Gaussian case: connection with the RTS smoother
145
5.6.3
Transport maps in filtering: examples in the literature
. . . .
146
5.6.4
Sequential joint parameter and state estimation . . . . . . . .
148
5.6.5
Fixed-point smoothing . . . . . . . . . . . . . . . . . . . . . .
151
5.7
Low-rank transport maps . . . . . . . . . . . . . . . . . . . . . . . . .
152
5.7.1
General result . . . . . . . . . . . . . . . . . . . . . . . . . . .
152
5.7.2
Bayesian inference: local likelihood and the prior map . . . . .
155
5.7.3
Low-rank likelihood . . . . . . . . . . . . . . . . . . . . . . . .
158
5.8
Numerical examples . . . . . . . . . . . . . . . . . . . . . . . . . . . .
159
5.8.1
Stochastic volatility model with hyperparameters
. . . . . . .
159
5.8.2
Log-Gaussian Cox point process with sparse observations . . .
161
5.9
Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
169
6
A class of nonlinear filters induced by local couplings
175
6.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
175
6.1.1
Problem setting . . . . . . . . . . . . . . . . . . . . . . . . . .
175
6.1.2
Approach
. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
176
6.2
Transport maps from samples:
conditional simulation
. . . . . . . . . . . . . . . . . . . . . . . . . .
178
6.3
Intuition for an abstract problem
. . . . . . . . . . . . . . . . . . . .
181
6.3.1
Parameterizing a high dimensional inverse map
. . . . . . . .
186
6.3.2
Introducing Markov assumptions
. . . . . . . . . . . . . . . .
187
6.4
Numerical example: Lorenz 96 . . . . . . . . . . . . . . . . . . . . . .
190
6.4.1
Configuration of the nonlinear filter . . . . . . . . . . . . . . .
192
6.4.2
Numerical results . . . . . . . . . . . . . . . . . . . . . . . . .
193
A Rao‚Äôs metric between distributions
197
B Generalized Knothe-Rosenblatt rearrangement
199
C Proofs for Chapter 3
203
7

D Proofs for Chapter 4
213
E Proofs for Chapter 5
221
8

List of Figures
3-1
Randomized test case: variable hessian spectrum . . . . . . . . . . . .
58
3-2
Randomized test case: variable prior spectrum . . . . . . . . . . . . .
59
3-3
X-ray tomography problem: model setup . . . . . . . . . . . . . . . .
62
3-4
X-ray tomography problem: approximation of the posterior covariance
63
3-5
X-ray tomography problem: approximation of the posterior mean in
the limited-angle case . . . . . . . . . . . . . . . . . . . . . . . . . . .
66
3-6
X-ray tomography problem: error in the approximation of the posterior
mean for the low-rank and the low-rank update approximation . . . .
67
3-7
X-ray tomography problem: generalized eigenvaluse in the limited- and
full-angle configurations
. . . . . . . . . . . . . . . . . . . . . . . . .
67
3-8
X-ray tomography problem: approximation of the posterior mean in
the full-angle configuration . . . . . . . . . . . . . . . . . . . . . . . .
68
3-9
Heat equation problem: model setup
. . . . . . . . . . . . . . . . . .
71
3-10 Heat equation problem: approximation of the posterior mean . . . . .
71
3-11 Heat equation problem: comparison eigenvectors . . . . . . . . . . . .
72
4-1
Proof-of-concept example for goal-oriented approximations . . . . . .
92
4-2
CPU cooling problem: model setup . . . . . . . . . . . . . . . . . . .
100
4-3
CPU cooling problem: true temperature field and difference between
prior and posterior variance
. . . . . . . . . . . . . . . . . . . . . . .
100
4-4
CPU cooling problem: leading eigenvectors for the optimal approxi-
mation of the posterior covariance of the parameters . . . . . . . . . .
101
9

4-5
CPU cooling problem: leading eigenvectors for the optimal approxi-
mation of the posterior covariance of the QoI . . . . . . . . . . . . . .
101
4-6
CPU cooling problem: comparison error between different approxima-
tions of the posterior covariance of the QoI . . . . . . . . . . . . . . .
102
4-7
CPU cooling problem: approximation of the posterior mean of the QoI 103
4-8
CPU cooling problem: a nonlinear QoI . . . . . . . . . . . . . . . . .
104
5-1
Computation of transport maps . . . . . . . . . . . . . . . . . . . . .
115
5-2
Marginal graphs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
121
5-3
Predicted sparsity patterns for a triangular map . . . . . . . . . . . .
121
5-4
Stochastic volatility model: Markov structure and sparsity inverse map 122
5-5
Sparsity of a triangular map under different reorderings . . . . . . . .
125
5-6
Example of a decomposable map
. . . . . . . . . . . . . . . . . . . .
137
5-7
Markov structure of a typical state-space model . . . . . . . . . . . .
139
5-8
Markov structure of a typical state-space model with static parameters 148
5-9
Online fixed-point smoothing
. . . . . . . . . . . . . . . . . . . . . .
152
5-10 Stochastic volatility model: filtering and smoothing marginals
. . . .
162
5-11 Stochastic volatility model: posterior marginals of the static parameters162
5-12 Stochastic volatility model: reference MCMC solution for posterior
marginals of the state . . . . . . . . . . . . . . . . . . . . . . . . . . .
163
5-13 Stochastic volatility model: reference MCMC solution for posterior
marginals of the static parameters . . . . . . . . . . . . . . . . . . . .
163
5-14 Stochastic volatility model: posterior predicitve distribution
. . . . .
164
5-15 Log-Gaussian Cox problem: model setup . . . . . . . . . . . . . . . .
167
5-16 Log-Gaussian Cox: mean and standard deviation of the posterior latent
process computed via transport maps . . . . . . . . . . . . . . . . . .
168
5-17 Log-Gaussian Cox: mean and standard deviation of the posterior latent
process computed via MCMC . . . . . . . . . . . . . . . . . . . . . .
168
6-1
Cycle graph . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
190
6-2
Lorenz 96: sparse Markov structure . . . . . . . . . . . . . . . . . . .
194
10

6-3
Lorenz 96: dense Markov structure
. . . . . . . . . . . . . . . . . . .
195
6-4
Lorenz 96: accuracy filtering mean
. . . . . . . . . . . . . . . . . . .
196
11

Acknowledgments
I am indebted to my Ph.D. advisor Youssef Marzouk for his mentorship, friendship,
and guidance throughout these years. His passion and dedication will continue to be
a source of inspiration. He allowed me to pursue my academic interests and supported
me in every step of the way. He always believed in me and for that I will always be
grateful. I was fortunate to have crossed his path.
I want to thank the members of my thesis committee, Youssef Marzouk, Themis-
toklis Sapsis and Alan S. Willsky, and my readers, Pierre Jacob and Luis Tenorio,
for their support and countless suggestions that have greatly enhanced this thesis
project.
Luis Tenorio has been like a second advisor to me, an example to live by and a
friend. Thank you.
Research is never an individual activity.
It is a fruitful connection of minds,
thoughts and insights, and this work builds on this spirit. I am indebted to every
collaborator that took part in any project related to this work: Antti Solonen, Daniele
Bigoni, James Martin, Karen Willcox, Luis Tenorio, Olivier Zahm, Ricardo Baptista,
Tiangang Cui. This thesis would have not been possible without their help.
ACDL has been an exciting and fun place to grow, both as a person and as a
researcher. Many people crossed the lab during these years and every one of them
added something unique to the chemistry of this group.
I want to thank Antoni
Musolas, Alex Gorodetsky, Benjamin Zhang, Chad Lieberman, Chaitanya Talnikar,
Chi Feng, Daniele Bigoni, Elizabeth Qian, Ferran Vidal, Florian Augustin, Jayanth
Jagalur, Luca Tosatto, Lucio Di Ciaccio, Matthew Parno, Olivier Zahm, Pablo Fer-
nandez, Patrick Blonigan, Rebecca Morrison, R√©mi Lam, Ricardo Baptista, Sergio
Amaral, Tarek El Moselhy, Xun Huan, Zheng Wang, and so many others for such
memorable moments and for giving me a family away from home.
I want to thank the US Department of Energy, Office of Advanced Scientific
Computing Research (ASCR), for generously supporting my research.
Last, I want to dedicate this thesis to my friends, family, and loved ones. Their
12

unconditional support and encouragement kept me strong along the way, and motivate
me now, more than ever, to strive even harder to achieve my future goals.
13

14

Previously Published Material
‚Ä¢ Chapter 3 revises a previous publication [261]: A. Spantini, A. Solonen, T. Cui,
J. Martin, L. Tenorio, and Y. Marzouk, Optimal low-rank approximations of
Bayesian linear inverse problems, SIAM Journal on Scientific Computing, 37
(2015), pp. A2451‚ÄìA2487.
‚Ä¢ Chapter 4 revises a previous publication [260]: A. Spantini, T. Cui, K. Will-
cox, L. Tenorio, and Y. M. Marzouk, Goal-oriented optimal approximations of
Bayesian linear inverse problems, SIAM Journal on Scientific Computing, in
press (2017).
Other work by the author, not presented in this thesis but nonetheless related to its
primary themes, includes:
‚Ä¢ A. Spantini, D. Bigoni, and Y. Marzouk, Inference via low-dimensional cou-
plings, arXiv:1703.06131, (2017), [259]
‚Ä¢ A. Spantini, D. Bigoni, and Y. Marzouk, Variational inference via decomposable
transports: algorithms for Bayesian filtering and smoothing, NIPS workshop on
Approximate Inference, (2016), [258]
‚Ä¢ D. Bigoni, A. Spantini, and Y. Marzouk, Adaptive construction of measure
transports for Bayesian inference, NIPS workshop on Approximate Inference,
(2016), [29]
‚Ä¢ Y. Marzouk, T. Moselhy, M. Parno, and A. Spantini, Sampling via measure
transport: An introduction, in Handbook of Uncertainty Quantification, R.
Ghanem, D. Higdon, and H. Owhadi, editors, Springer, 2016, [187]
15

‚Ä¢ T. Cui, J. Martin, Y. Marzouk, A. Solonen, and A. Spantini, Likelihood-
informed dimension reduction for nonlinear inverse problems, Inverse Problems,
30 (2014), p. 114015, [74]
16

Chapter 1
Introduction
1.1
Low-dimensional structure in Bayesian inference
This thesis is concerned with the solution of Bayesian inference problems in high
dimensional and continuous parameter spaces. Many of these problems arise naturally
from a statistical reformulation of classical inverse problems, where a typical goal is to
characterize a parameter field of interest (e.g., the permeability of a porous medium)
from noisy observations, perhaps limited in number or resolution, that are indirectly
related to the parameters through the action of a forward model [268]. But we will
also consider Bayesian inference in much more general settings, ranging from spatial
statistics to sequential observations and dynamical models (e.g., filtering, smoothing,
and sequential parameter estimation in the nonlinear and non-Gaussian setting). A
common feature of the inference problems that we will investigate is the presence
of structure‚Äîsome notion of low dimensionality‚Äîthat can be exploited for efficient
computation. A goal of this thesis is precisely to define, detect, and exploit such
low-dimensional structure.
Inference arises in virtually every aspect of science that needs to process data:
radar, optical imaging, tomography, astronomy, and atmospheric data assimilation,
to name a few.
In this thesis we consider several examples inspired by practical
applications.
For instance, in Chapter 3 we consider real-time X-ray imaging of
logs that enter a sawmill for automatic quality control [125, 261]. In Chapter 5, we
17

consider an online joint parameter and state estimation problem for the volatility
of the pound-dollar exchange rate [149, 237]. Also in Chapter 5 we perform infer-
ence with a log-Gaussian Cox process, frequently used to model spatially aggregated
point patterns in seismology and neuroimaging [193]. In Chapter 6, we consider the
nonlinear filtering of strongly chaotic and fully turbulent dynamical systems via the
Lorenz-96 model [180], which is intended to reproduce coarse features of the mid-
latitude atmosphere [182] and is frequently used as a testbed for numerical weather
prediction algorithms. Additionally, in related published work by the author [74], not
included in this thesis, we consider the the problem of estimating atmospheric trace
gas concentrations using measurements from the GOMOS (Global Ozone MOnitoring
System) satellite instrument [114, 74]. Also in [74], we infer the permeability field of
a groundwater system governed by elliptic PDEs. These examples hint at the broad
applicability of the ideas developed in this thesis.
In Bayesian inference, the parameters of interest are treated as random variables,
endowed with a prior distribution that encodes (possibly subjective) beliefs before
data are collected [232, 20]. The distribution of the data conditioned on any value of
the parameters is specified through the likelihood model. Bayes‚Äô rule then combines
prior and likelihood information to yield the posterior distribution, i.e., the distribu-
tion of the parameters conditioned on the data. The posterior distribution defines the
Bayesian solution to the inference problem. Thus, the Bayesian approach to inference
formalizes the characterization of the parameters through exploration of the posterior
distribution [268, 145, 265]. Computing expectations with respect to the posterior
distribution yields not only point estimates of the parameters (e.g., the posterior
mean), but a complete description of their uncertainty. From the posterior distribu-
tion one can extract the posterior covariance and higher moments, marginal distribu-
tions, quantiles, and event probabilities. Uncertainty in parameter-dependent predic-
tions can be quantified by integrating over the posterior distribution. The Bayesian
paradigm offers a coherent framework to account for all sources of uncertainty in
an inference problem, including stochastic forward models, noisy observations, and
model error. Not surprisingly, the laws of probability formalize intuitive ‚Äúdesiderata
18

of rationality‚Äù and promote ‚Äúplausible reasoning‚Äù when dealing with uncertainty [138].
The major obstacle to deploying Bayesian inference in high-dimensional real world
applications is computational: characterizing a high-dimensional and generically non-
Gaussian posterior distribution can be a challenging task. In some sense, this is the
price to pay for demanding more than a point estimate of the parameters or of model
predictions (cf. Tikhonov regularization [269]). While the Bayesian formulation is
quite general, essential features of the statistical model can bring additional structure
to the Bayesian update. For instance, the prior distribution often encodes some kind
of smoothness or correlation among the inversion parameters; observations might be
finite, local, few in number, and corrupted by noise; observations might also be in-
direct, related to the parameters by the action of a forward operator that filters out
some information; the posterior distribution might satisfy conditional independence
assumptions that model local probabilistic interactions among the underlying random
variables; and in some cases we might not even be interested in the posterior distri-
bution per se, but rather in specific prediction goals. A key consequence of all this
structure in the inference model is that Bayesian computations in high dimensions
are far from being hopeless.
Clearly, not every problem has every feature mentioned above. But most prob-
lems do possess some combination thereof. This observation is not mere coincidence.
Inference relies ultimately on a statistical model. Each model strives to achieve a
compromise between predictive power and computational efficiency. Understanding
what types of structure enable efficient Bayesian computation can have a profound
impact on modeling choices: we will favor models that have such structure. And in
many cases we would be able to use rather complex models without sacrificing any
predictive power. This is not to say that we should always aim for the most complex
available statistical model: in many scenarios simple models are also the most appro-
priate [50, 104, 123]. But we should at least reach a point where no modeling effort
is limited in accuracy by a lack of computational tractability.
This thesis sets forward several goals: (1) to provide a rigorous mathematical
characterization of various notions of low-dimensional structure that enable efficient
19

Bayesian inference in high-dimensional and continuous parameter spaces; (2) to de-
velop techniques to test for such low-dimensional structure in a given inference prob-
lem; (3) to exploit this characterization of low-dimensional structure to devise new
computationally efficient and structure-exploiting algorithms for Bayesian computa-
tion.
1.2
Thesis contributions and roadmap
The contributions of this thesis range over a number of related topics.
Here we
preview these contributions while giving an outline of each chapter of the thesis.
‚Ä¢ Optimal low-rank approximations of linear inverse problems. In Chap-
ter 3, we introduce statistically optimal approximations of linear Gaussian in-
verse problems.
We exploit how data are often informative, relative to the
prior, only about a handful of directions in the parameter space [261]. These
optimality results support computationally efficient and structure exploiting
algorithms for Bayesian inference in high-dimensional linear Gaussian models
[96] and are the building block for many approaches to the solution of nonlinear
Bayesian inverse problems [186, 40, 42, 74, 73]. We first investigate the ap-
proximation of the posterior covariance matrix as a low-rank negative update
of the prior covariance matrix. We prove optimality of a particular update,
based on the leading eigendirections of the matrix pencil defined by the Hessian
of the negative log-likelihood and the prior precision, for a broad class of loss
functions. This class includes the natural geodesic distance on the manifold of
symmetric and positive definite matrices, as well as the Kullback-Leibler diver-
gence and the Hellinger distance between the associated distributions. We also
propose two fast approximations of the posterior mean and prove their opti-
mality with respect to a weighted Bayes risk under squared-error loss. These
approximations are deployed in an offline-online manner, where a more costly
but data-independent offline calculation is followed by fast online evaluations.
As a result, these approximations are particularly useful when repeated poste-
20

rior mean evaluations are required for multiple data sets. We demonstrate our
theoretical results with several numerical examples, including high-dimensional
X-ray tomography and an inverse heat conduction problem. In both of these
examples, the low-dimensional structure of the inverse problem can be exploited
while producing results that are essentially indistinguishable from the exact so-
lution. For an extension of the material of this chapter to nonlinear problems
we refer the reader to [74].
‚Ä¢ Goal-oriented optimal approximations of linear inverse problems. In
Chapter 4, we extend the statistically optimal approximations of Chapter 3 to
the case of goal oriented linear Gaussian inverse problems, where the quantity of
interest (QoI) is a function of the inversion parameters. We introduce new algo-
rithms for the efficient characterization of the posterior statistics of the quantity
of interest [260]. These optimal approximations avoid the explicit computation
of the full posterior distribution of the parameters and instead focus on direc-
tions in the parameter space that are well informed by the data and relevant to
the QoI. These directions stem from a balance among all the components of the
goal‚Äìoriented inverse problem: prior information, forward model, measurement
noise, and ultimate goals. In particular, we show how including ultimate goals
in the formulation of the inverse problem reduces the dimension of the Bayesian
update. We illustrate the theory using a high-dimensional inverse problem in
heat transfer.
‚Ä¢ The structure of low-dimensional couplings.
In Chapter 5, we switch
gears to general non-Gaussian problems and study the structure of determin-
istic couplings between distributions. A principled approach to posterior sam-
pling in the general non-Gaussian case is to seek a coupling between a tractable
‚Äúreference‚Äù distribution (e.g., a standard Gaussian) and the posterior. Deter-
ministic couplings are induced by transport maps, which are multivariate (and
possibly nonlinear) transformations that enable direct simulation from the pos-
terior simply by evaluating the transport map at samples from the reference
21

distribution. Representing, computing and evaluating such a map, however,
grows challenging in high dimensions. The central contribution of this chapter
is to establish an explicit link between the Markov properties of the reference-
posterior pair and the existence of certain low-dimensional couplings, induced
by transport maps that are sparse, decomposable, and/or low-rank. Our analysis
not only facilitates the construction of couplings in high-dimensional settings,
but also suggests new inference methodologies. For instance, our analysis of
sparse triangular maps provides a general framework for describing continuous
and non-Gaussian Markov random fields, and for exploiting the conditional in-
dependence structure of these fields in computation. In particular, this analysis
shows that the inverse of the Knothe‚ÄìRosenblatt (KR) rearrangement‚Äîa par-
ticular coupling‚Äîis the natural generalization to the non-Gaussian case of the
Cholesky factor of the precision matrix of a Gaussian Markov random field‚Äîin
that both the inverse KR rearrangement (a potentially nonlinear map) and the
Cholesky factor (a linear map) have the same sparsity pattern given posterior
distributions with the same Markov structure. Thus the KR rearrangement
can be used to extend well-known modeling and sampling techniques for high-
dimensional Gaussian MRFs [236] to non-Gaussian fields. As another example,
in the context of nonlinear and non-Gaussian state space models, we describe
new variational algorithms for filtering, smoothing, and sequential parameter
estimation. These algorithms implicitly characterize‚Äîvia a transport map‚Äî
the full posterior distribution of the sequential inference problem using local
operations only incrementally more complex than regular filtering, while avoid-
ing importance sampling or resampling. These algorithms can be understood
as the natural generalization‚Äîto the non-Gaussian case‚Äîof the square-root
Rauch‚ÄìTung‚ÄìStriebel Gaussian smoother [226, 210].
‚Ä¢ A class of nonlinear filters induced by local couplings.
In Chapter
6, we introduce a new class of structure-exploiting nonlinear filters for high-
dimensional state-space models. These filters can exploit the following struc-
22

ture: (1) locality in the observations (e.g., pointwise likelihoods), (2) decay
of correlations, and (3) approximate conditional independence in the filtering
distribution. Thus we focus on applications where the state is usually the dis-
cretization of a distributed process, and where the dynamic is typically governed
by a (chaotic) partial differential equation that we treat as a black box; that
is, we assume that no gradients of the forward model (or transition kernel)
are available.
The idea is to transform the forecast ensemble (i.e., samples
from the bootstrap proposal) into samples from the current filtering distribu-
tion (i.e., conditioned on the new observations), by means of a sequence of
local (in state-space) nonlinear couplings computed mostly via low-dimensional
convex optimization. This sequence of low-dimensional transformations implic-
itly approximate the projection of the filtering distribution onto a manifold of
sparse Markov random fields (not necessarily Gaussian) and can be carried out
with very limited ensemble sizes. Many variations of the ensemble Kalman fil-
ter (EnKF) can be interpreted as special instances of the proposed framework
when we restrict our attention exclusively to linear transformations, and when
we neglect approximately sparse Markov structure in the filtering distribution
[93]. A key feature of the proposed algorithms, however, is that depending on
the forecast ensemble size, we can inject arbitrary non-Gaussian structure into
the problem in a stable way by exploiting locality of the transformations, and
thus we can reduce the EnKF bias that results from restricting the Bayesian
update to linear functions of the forecast ensemble.
In this thesis we consider some, but certainly not all, sources of low-dimensional
structure within the context of Bayesian inference. Some structure is perhaps yet
to be discovered, but other notions of low dimensionality have already been studied
extensively in the literature. We will explore connections with the relevant literature
within each chapter of the thesis, once the appropriate methodologies and terminology
are introduced. There are some important topics, however, that we will not touch
upon, and we refer the reader to the original contributions for further details. For
instance, there exists a large body of work on how to exploit conditional independence
23

structure in discrete and/or Gaussian (graphical) models (e.g., [156, 161, 179, 141]),
on how to exploit multiscale/multiresolution models [52, 53], and on conditionally
Gaussian or low-rank structure in filtering (e.g., [183, 253, 217, 57, 94]). We note
that most of the techniques proposed in this thesis are in fact complementary to
existing efforts in dimensionality reduction for Bayesian inference, and could thus be
used in conjunction with well-established and successful methodologies.
Finally, we note that this thesis presents a natural progression along two fronts:
(1) from the study of linear-Gaussian inverse problems to the more general case of
nonlinear non-Gaussian inference, and (2) from stationary inverse problems to se-
quential inference (e.g., filtering, smoothing and joint parameter-state estimation).
The chapters are relatively self-contained and can be read independently: broadly
speaking they are divided between Gaussian (Chapters 3 and 4) and non-Gaussian
(Chapters 5 and 6) problems. A reader interested exclusively in the computational
aspects of sequential inference can focus mostly on Section 5.6 and Chapter 6. Chap-
ter 6 restricts its attention to high-dimensional filtering for chaotic spatiotemporal
processes [230]. Chapter 2 contains some relevant notation used throughout the thesis
(mostly needed for Chapter 5), whereas the proofs of the main results are collected in
dedicated appendices for each chapter (see Appendices C-D-E). Appendix A reviews
Rao‚Äôs notion of a metric between distributions [224], whereas Appendix B focuses
on some background material for the KR rearrangement. Concluding remarks are
offered at the end of each chapter.
24

Chapter 2
Notation
Here, we collect some useful notation used throughout the thesis.
Notation for functions, sets, and graphs
For a pair of functions ùëìand ùëî, we
denote their composition by ùëì‚àòùëî. We denote by ùúïùëòùëìthe partial derivative of ùëìwith
respect to its ùëòth input variable. By ùúïùëòùëì= 0, we mean that the function ùëìdoes not
depend on its ùëòth input variable. Depending on the context, we can identify a matrix
ùëÑwith its corresponding linear map, given by ùë•‚Ü¶‚ÜíùëÑùë•.
For all ùëõ> 0, we let Nùëõ= {1, . . . , ùëõ} denote the set of the first ùëõintegers. For
any pair of sets, ùíú‚äÇ‚Ñ¨means that ùíúis a subset of ‚Ñ¨(including the possibility of
ùíú= ‚Ñ¨). We denote by |ùíú| the cardinality of ùíú.
Given a graph ùí¢= (ùí±, ‚Ñ∞) with vertices ùí±and edges ‚Ñ∞, we denote by Nb(ùëò, ùí¢) the
neighborhood of a node ùëòin ùí¢, while for any set ùíú‚äÇùí±, we denote by ùí¢ùíú= (ùí±‚Ä≤, ‚Ñ∞‚Ä≤)
the subgraph given by ùí±‚Ä≤ = ùíúand ‚Ñ∞‚Ä≤ = ‚Ñ∞‚à©(ùíú√ó ùíú).
Notation for measures and densities
In this thesis, we mostly consider prob-
ability measures on Rùëõthat are absolutely continuous with respect to the Lebesgue
measure, ùúÜ, and that are fully supported. We denote the set of such measures by
M+(Rùëõ). The density of a measure will always be intended with respect to ùúÜ. For a
pair of measures ùúà1, ùúà2, ùúà1 ‚â™ùúà2 means that ùúà1 is absolutely continuous with respect
to ùúà2.
25

For any measure ùúàand measurable map ùëá, we denote by ùëá‚ôØùúàthe pushforward
measure given by ùúà‚àòùëá‚àí1, where for any set ‚Ñ¨, ùëá‚àí1(‚Ñ¨) is the set-valued preimage
of ‚Ñ¨under ùëá. Similarly, we denote by ùëá‚ôØùúàthe pullback measure given by ùúà‚àòùëá.
Given a measure ùúàwith density ùúãand a map ùëá, we denote by ùëá‚ôØùúãthe density of
ùëá‚ôØùúà, provided it exists (depending on ùëá). We call ùëá‚ôØùúãthe pushforward density of ùúã
by ùëá. Similarly, we define the pullback density ùëá‚ôØùúãas the density of ùëá‚ôØùúà, provided it
exists. Whether the map ùëápreserves the absolute continuity of the measure depends
on the regularity of ùëá. For instance, if ùëá: Rùëõ‚ÜíRùëõis a diffeomorphism‚Äîi.e., a
differentiable bijection with differentiable inverse‚Äîthen one has:
ùëá‚ôØùúã(ùë•) = ùúã(ùëá‚àí1(ùë•)) | det ‚àáùëá‚àí1(ùë•)|,
ùëá‚ôØùúã(ùë•) = ùúã(ùëá(ùë•)) | det ‚àáùëá(ùë•)|,
(2.1)
where ‚àáùëá(ùë•) denotes the Jacobian of ùëáat ùë•. The regularity assumptions on ùëácan be
substantially weakened as long as one modifies (2.1) appropriately [235, 262, 101]. We
will give one such example shortly when dealing with triangular maps (see Section 5.2
or Appendix B). We denote by
‚à´Ô∏Ä
ùëì(ùë•) ùúà(dùë•) the integration of a measurable function
ùëì: Rùëõ‚ÜíR with respect to a measure ùúà. For the Lebesgue measure, we simplify our
notation as
‚à´Ô∏Ä
ùëì(ùë•) ùúÜ(dùë•) =
‚à´Ô∏Ä
ùëì(ùë•) dùë•. Given a pair ùúÇ, ùúãof probability densities and
a map ùëá: Rùëõ‚ÜíRùëõ, we say that ùëápushes forward ùúÇto ùúãif and only if ùëácouples
the corresponding probability measures, i.e., ùëá‚ôØùúàùúÇ= ùúàùúã, with ùúàùúÇ(‚Ñ¨) =
‚à´Ô∏Ä
‚Ñ¨ùúÇ(ùë•) dùë•and
ùúàùúã(‚Ñ¨) =
‚à´Ô∏Ä
‚Ñ¨ùúã(ùë•) dùë•for all measurable sets ‚Ñ¨. (Notice that ùëá‚ôØùúÇneed not be given by
(2.1) since we are not specifying any regularity on ùëá.)
When it is clear from context, we will freely omit the qualifier a.e. to indicate a
property that holds up to a set of measure zero.
Notation for random variables
We use boldface capital letters, e.g., ùëã, to denote
random variables on Rùëõwith ùëõ> 1, while we write scalar-valued random variables
as ùëã. The law of a random variable ùëãdefined on a probability space (Œ©, P) is given
by ùëã‚ôØP. For a measure ùúà, ùëã‚àºùúàmeans that ùëãhas law ùúà. If ùëã= (ùëã1, . . . , ùëãùëù)
is a collection of random variables and ùíú‚äÇNùëù, then ùëãùíú= (ùëãùëñ, ùëñ‚ààùíú) denotes
26

a subcollection of ùëã. In the same way, for ùëó< ùëò, ùëãùëó:ùëò= (ùëãùëó, ùëãùëó+1, . . . , ùëãùëò). If
ùëã= (ùëã1, . . . , ùëãùëù) has joint density ùúãand ùíú‚äÇNùëù, we denote by ùúãùëãùíúthe marginal
of ùúãalong ùëãùíú, i.e., ùúãùëãùíú(ùë•ùíú) =
‚à´Ô∏Ä
ùúã(ùë•) dùë•Nùëù‚àñùíú. If ùúãis the density of ùëç= (ùëã, ùëå),
we denote by ùúãùëã|ùëåthe density of ùëãgiven ùëå, where
ùúãùëã|ùëå(ùë•|ùë¶) =
‚éß
‚é™
‚é®
‚é™
‚é©
ùúãùëã,ùëå(ùë•, ùë¶)/ùúãùëå(ùë¶)
if ùúãùëå(ùë¶) Ã∏= 0
0
otherwise.
(2.2)
We denote independence of a pair of random variables ùëã, ùëåby ùëã‚ä•‚ä•ùëå. In the
same way, ùëã‚ä•‚ä•ùëå|ùëÖmeans that ùëãand ùëåare independent given a third random
variable ùëÖ.
27

28

Chapter 3
Optimal low-rank approximations of
linear inverse problems
3.1
Introduction
In this chapter we investigate approximation methods for finite-dimensional Bayesian
linear inverse problems with Gaussian measurement and prior distributions. We char-
acterize approximations of the posterior distribution that are structure-exploiting and
that are optimal in a sense to be defined below. Since the posterior distribution is
Gaussian, it is completely determined by its mean and covariance. We therefore focus
on approximations of these posterior characteristics. Optimal approximations will re-
duce computation and storage requirements for high-dimensional inverse problems,
and will also enable fast computation of the posterior mean in a many-query setting.
We consider approximations of the posterior covariance matrix in the form of low-
rank negative updates of the prior covariance matrix. This class of approximations
exploits the structure of the prior-to-posterior update, and also arises naturally in
Kalman filtering techniques (e.g., [10, 11, 254]); the challenge is to find an optimal
update within this class, and to define in what sense it is optimal. We will argue
that a suitable loss function with which to define optimality is the natural geodesic
distance on the manifold of symmetric and positive definite matrices [99], and will
show that this metric generalizes to a broader class of loss functions that emphasize
29

relative differences in covariance. We will derive the optimal low-rank update for this
entire class of loss functions. In particular, we will show that the prior covariance
matrix should be updated along the leading generalized eigenvectors of the pencil
(ùêª, Œì‚àí1
pr ) defined by the Hessian of the negative log-likelihood and the prior precision
matrix. If we assume exact knowledge of the posterior mean, then our results extend
to optimality statements between distributions (e.g., optimality in Kullback-Leibler
divergence and in Hellinger distance). The form of this low-rank update of the prior
is not new [40, 43, 96, 186], but previous work has not shown whether‚Äîand if so,
in exactly what sense‚Äîit yields optimal approximations of the posterior.
A key
contribution of this chapter is to establish and explain such optimality.
Properties of the generalized eigenpairs of (ùêª, Œì‚àí1
pr ) and related matrix pencils
have been studied previously in the literature, especially in the context of classical
regularization techniques for linear inverse problems1 [270, 206, 121, 120, 89]. The
joint action of the log-likelihood Hessian and the prior precision matrix has also
been used in related regularization methods [47, 44, 46, 131]. However, these efforts
have not been concerned with the posterior covariance matrix or with its optimal
approximation, since this matrix is a property of the Bayesian approach to inversion.
One often justifies the assumption that the posterior mean is exactly known by
arguing that it can easily be computed as the solution of a regularized least-squares
problem [128, 207, 3, 190, 17]; indeed, evaluation of the posterior mean to machine
precision is now feasible even for million-dimensional parameter spaces [40]. If, how-
ever, one needs multiple evaluations of the posterior mean for different realizations
of the data (e.g., in an online inference context), then solving a linear system to de-
termine the posterior mean may not be the most efficient strategy. A second goal of
this chapter is to address this problem. We will propose two computationally effi-
cient approximations of the posterior mean based on: (i) evaluating a low-rank affine
function of the data; or (ii) using a low-rank update of the prior covariance matrix
in the exact formula for the posterior mean. The optimal approximation in each case
1In the framework of Tikhonov regularization [269], the regularized estimate coincides with the
posterior mean of the Bayesian linear model we consider here, provided that the prior covariance
matrix is chosen appropriately.
30

is defined as the minimizer of the Bayes risk for a squared-error loss weighted by the
posterior precision matrix. We provide explicit formulas for these optimal approx-
imations and show that they can be computed by exploiting the optimal posterior
covariance approximation described above. Thus, given a new set of data, computing
an optimal approximation of the posterior mean becomes a computationally trivial
task.
Low-rank approximations of the posterior mean that minimize the Bayes risk for
squared-error loss have been proposed in [63, 66, 65, 64, 62] for a general non-Gaussian
case. Here, instead we develop analytical results for squared-error loss weighted by the
posterior precision matrix. This choice of norm reflects the idea that approximation
errors in directions of low posterior variance should be penalized more strongly than
errors in high-variance directions, as we do not want the approximate posterior mean
to fall outside the bulk of the posterior probability distribution. Remarkably, in this
case, the optimal approximation only requires the leading eigenvectors and eigenvalues
of a single eigenvalue problem.
This is the same eigenvalue problem we solve to
obtain an optimal approximation of the posterior covariance matrix, and thus we can
efficiently obtain both approximations at the same time.
While the efficient solution of large-scale linear-Gaussian Bayesian inverse prob-
lems is of standalone interest [96], optimal approximations of Gaussian posteriors
are also a building block for the solution of nonlinear Bayesian inverse problems.
For example, the stochastic Newton Markov chain Monte Carlo (MCMC) method
[186] uses Gaussian proposals derived from local linearizations of a nonlinear forward
model; the parameters of each Gaussian proposal are computed using the optimal
approximations analyzed in this chapter.
To tackle even larger nonlinear inverse
problems, [40] uses a Laplace approximation of the posterior distribution wherein
the Hessian at the mode of the log-posterior density is itself approximated using the
present approach. Similarly, approximations of local Gaussians can facilitate the con-
struction of a nonstationary Gaussian process whose mean directly approximates the
posterior density [42]. Alternatively, [74] combines data-informed directions derived
from local linearizations of the forward model‚Äîa direct extension of the posterior co-
31

variance approximations described in this chapter‚Äîto create a global data-informed
subspace. A computationally efficient approximation of the posterior distribution is
then obtained by restricting MCMC to this subspace and treating complementary
directions analytically. Moving from the finite to the infinite-dimensional setting, the
same global data-informed subspace is used to drive efficient dimension-independent
posterior sampling for inverse problems in [73].
Earlier work on dimension reduction for Bayesian inverse problems used the Karhunen-
Lo√®ve expansion of the prior distribution [188, 166] to describe the parameters of
interest. To reduce dimension, this expansion is truncated; this step renders both
the prior and posterior distributions singular‚Äîi.e., collapsed onto the prior mean‚Äîin
the neglected directions. Avoiding large truncation errors then requires that the prior
distribution impose significant smoothness on the parameters, so that the spectrum
of the prior covariance kernel decays quickly. In practice, this requirement restricts
the choice of priors. Moreover, this approach relies entirely on properties of the prior
distribution and does not incorporate the influence of the forward operator or the
observational errors. Alternatively, [171] constructs a reduced basis for the param-
eter space via greedy model-constrained sampling, but this approach can also fail
to capture posterior variability in directions uninformed by the data. Both of these
earlier approaches seek reduction in the overall description of the parameters. This
notion differs fundamentally from the dimension reduction technique advocated in
this chapter, where low-dimensional structure is sought in the change from prior to
posterior.
The rest of this chapter is organized as follows. In Section 3.2 we introduce the
posterior covariance approximation problem and derive the optimal prior-to-posterior
update with respect to a broad class of loss functions. The structure of the optimal
posterior covariance matrix approximation is examined in Section 3.3. Several in-
terpretations are given in this section, including an equivalent reformulation of the
covariance approximation problem as an optimal projection of the likelihood function
onto a lower dimensional subspace. In Section 3.4 we characterize optimal approxi-
mations of the posterior mean. In Section 3.5 we provide several numerical examples.
32

Section 3.6 offers concluding remarks. Appendix C collects proofs of the theorems
stated throughout the chapter, along with additional technical results. The material
presented in this chapter can be also found in [261].
3.2
Optimal approximation of the posterior covari-
ance matrix
Consider the Bayesian linear model defined by a Gaussian likelihood and a Gaussian
prior with a non-singular covariance matrix Œìpr ‚âª0 and, without loss of generality,
zero mean:2
ùëå| ùëã‚àºùí©(ùê∫ùëã, Œìobs),
ùëã‚àºùí©(0, Œìpr).
(3.1)
Here ùëãrepresents the parameters to be inferred, ùê∫is the linear forward operator,
and ùëåare the observations, with Œìobs ‚âª0. The statistical model (3.1) also follows
from:
ùëå= ùê∫ùëã+ ‚Ñ∞
where ‚Ñ∞‚àºùí©(0, Œìobs) is independent of ùëã.
It is easy to see that the posterior
distribution is again Gaussian (see, e.g., [49]):
ùëã| ùëå‚àºùí©(ùúápos(ùëå), Œìpos), with
mean and covariance matrix given by
ùúápos(ùëå) = Œìpos ùê∫‚ä§Œì‚àí1
obs ùëå
and
Œìpos =
(Ô∏Ä
ùêª+ Œì‚àí1
pr
)Ô∏Ä‚àí1 ,
(3.2)
where
ùêª= ùê∫‚ä§Œì‚àí1
obsùê∫
(3.3)
is the Hessian of the negative log-likelihood (i.e., the Fisher information matrix).
Since the posterior is Gaussian, the posterior mean coincides with the posterior mode:
ùúápos(ùëå) = arg maxùë•ùúãpos(ùë•; ùëå), where ùúãpos is the posterior density. Note that the
posterior covariance matrix does not depend on the data.
2ùëå|ùëãrefers to a random variable distributed according to the measure of ùëåconditioned on ùëã.
33

3.2.1
Defining the approximation class
We will seek an approximation, ÃÇÔ∏ÄŒìpos, of the posterior covariance matrix that is optimal
in a class of matrices to be defined shortly. As we can see from (3.2), the posterior
precision matrix Œì‚àí1
pos is a non-negative update of the prior precision matrix Œì‚àí1
pr :
Œì‚àí1
pos = Œì‚àí1
pr + ùëçùëç‚ä§, where ùëçùëç‚ä§= ùêª.
Similarly, using Woodbury‚Äôs identity we
can write Œìpos as a non-positive update of Œìpr: Œìpos = Œìpr ‚àíùêæùêæ‚ä§, where ùêæùêæ‚ä§=
Œìpr ùê∫‚ä§Œì‚àí1
ùëåùê∫Œìpr and Œìùëå= Œìobs + ùê∫Œìpr ùê∫‚ä§is the covariance matrix of the marginal
distribution of ùëå[144]. This update of Œìpr is negative semidefinite because the data
add information: the posterior variance in any direction is always smaller than the
corresponding prior variance. Moreover, the update is usually low rank for exactly
the reasons described in the introduction: there are directions in the parameter space
along which the data are not very informative, relative to the prior. For instance,
ùêªmight have a quickly decaying spectrum [41, 247]. Note, however, that Œìpos itself
might not be low-rank. Low-rank structure, if any, lies in the update of Œìpr that yields
Œìpos. Hence, a natural class of matrices for approximating Œìpos is the set of negative
semi-definite updates of Œìpr, with a fixed maximum rank, that lead to positive definite
matrices:
‚Ñ≥ùëü=
{Ô∏Ä
Œìpr ‚àíùêæùêæ‚ä§‚âª0 : rank(ùêæ) ‚â§ùëü
}Ô∏Ä
.
(3.4)
This class of approximations of the posterior covariance matrix takes advantage of
the structure of the prior-to-posterior update.
3.2.2
Optimality criteria: metrics between distributions
To measure the quality of the approximation of a posterior distribution we employ
a metric first introduced by Rao in [223] based on the Fisher information. Rao‚Äôs
approach to comparing distributions is rooted in differential geometry. The idea is to
turn a parametric family of distributions into a Riemannian manifold endowed with
a metric based on the geodesic distance [224]. The Riemannian structure is induced
by a quadratic form defined by the Fisher information matrix. (See Appendix A for
the explicit construction of Rao‚Äôs distance.) For a Gaussian family of distributions
34

this metric can be written explicitly at least in two particular cases [9, 250]. If the
family consists of Gaussian distributions with the same covariance matrix, Œì, then
the metric reduces to the Mahalanobis distance between the means [181, 224]:
ùëë‚Ñõ(ùúà1, ùúà2) = ‚Äñùúá1 ‚àíùúá2‚ÄñŒì‚àí1,
ùúà1 = ùí©(ùúá1, Œì), ùúà2 = ùí©(ùúá2, Œì),
(3.5)
where ‚Äñùëß‚Äñ2
Œì‚àí1 := ùëß‚ä§Œì‚àí1ùëß. We will use this metric to define optimal approximations
of the posterior mean in Section 3.4. Notice that this metric emphasizes differences
in the mean along eigendirections of Œì corresponding to low variance.
If, on the other hand, the family consists of Gaussian distributions with the same
mean, ùúá, then the metric reduces to:
ùëë‚Ñõ(ùúà1, ùúà2) =
‚àöÔ∏É
1
2
‚àëÔ∏Å
ùëñ
ln2(ùúéùëñ),
ùúà1 = ùí©(ùúá, Œì1), ùúà2 = ùí©(ùúá, Œì2),
(3.6)
where (ùúéùëñ) are the generalized eigenvalues of the pencil (Œì1, Œì2) [139]. That is, (ùúéùëñ)
are the roots of the characteristic polynomial det(Œì1 ‚àíùúéŒì2) and satisfy the equation
Œì1 ùë£ùëñ= Œì2 ùë£ùëñùúéùëñfor a collection of generalized eigenvectors (ùë£ùëñ) [110]. Since this family
of distributions can be identified with the cone of SPD matrices, Sym+, (3.6) can
also be used as a Riemannian metric on Sym+ [99, 24]. We will use this Riemannian
metric to define optimal approximations of the posterior covariance matrix. This
metric on Sym+ is also the unique geodesic distance that satisfies the following two
important invariance properties:
ùëë‚Ñõ(ùê¥, ùêµ) = ùëë‚Ñõ(ùê¥‚àí1, ùêµ‚àí1)
and
ùëë‚Ñõ(ùê¥, ùêµ) = ùëë‚Ñõ(ùëÄùê¥ùëÄ‚ä§, ùëÄùêµùëÄ‚ä§),
(3.7)
for any nonsingular matrix ùëÄand matrices ùê¥, ùêµ‚ààSym+ (e.g., [35]) making it an
ideal candidate to compare covariance matrices. Moreover, ùëë‚Ñõtreats under- and over-
approximations similarly in the sense that ùëë‚Ñõ(Œìpos, ùõºÃÇÔ∏ÄŒìpos) ‚Üí‚àûas ùõº‚Üí0 and as
ùõº‚Üí‚àû.3 This metric has been used successfully in a variety of applications (e.g.,
3This behavior is shared by Stein‚Äôs loss function, which has been proposed to assess estimates
of a covariance matrix [137]. Stein‚Äôs loss function is just the Kullback-Leibler distance between two
35

[252, 215, 192, 15, 132, 97]). Notice that the flat distance induced by the Frobenius
norm does not satisfy the invariance properties (3.7), and has often been shown to be
inadequate for comparing covariance matrices [97, 7, 255].
In the most general case of manifolds of Gaussian families parameterized by both
the mean and covariance, there seems to be no explicit form for the geodesic distance.
We will show that our posterior covariance matrix approximation is optimal not
only in terms of the Riemannian metric ùëë‚Ñõ, but also in terms of the following more
general class, ‚Ñí, of loss functions for SPD matrices.
Definition 3.1 (Loss functions). The class ‚Ñíis defined as the collection of functions
of the form
ùêø(ùê¥, ùêµ) =
ùëõ
‚àëÔ∏Å
ùëñ=1
ùëì(ùúéùëñ),
(3.8)
where ùê¥and ùêµare SPD matrices, (ùúéùëñ) are the generalized eigenvalues of the pencil
(ùê¥, ùêµ), and
ùëì‚ààùí∞= {ùëî‚ààùíû1(R+) : ùëî‚Ä≤(ùë•)(1 ‚àíùë•) < 0 for ùë•Ã∏= 1, and lim
ùë•‚Üí‚àûùëî(ùë•) = ‚àû}.
(3.9)
Elements of ùí∞are differentiable real-valued functions defined on the positive axis
that decrease on ùë•< 1, increase on ùë•> 1, and tend to infinity as ùë•‚Üí‚àû. The
squared metric ùëë2
‚Ñõbelongs to the class of loss functions defined by (3.8), whereas the
distance induced by the Frobenius norm does not.
Lemma 3.1, whose proof can be found in Appendix C, justifies the importance of
the class ‚Ñí. In particular, it shows that optimality of the covariance matrix approxi-
mation with respect to any loss function in ‚Ñíleads to an optimal approximation of the
posterior distribution using a Gaussian (with the same mean) in terms of other famil-
iar criteria used to compare probability measures, such as the Hellinger distance and
the Kullback-Leibler (K-L) divergence [209]. More precisely, we have the following
result:
Gaussian distributions with the same mean, but it is not a metric for SPD matrices.
36

Lemma 3.1 (Equivalence of approximations). If ùêø‚àà‚Ñí, then a matrix ÃÇÔ∏ÄŒìpos ‚àà‚Ñ≥ùëü
minimizes the Hellinger distance and the K-L divergence between ùí©(ùúápos(ùëå), Œìpos)
and the approximation ùí©(ùúápos(ùëå), ÃÇÔ∏ÄŒìpos) iff it minimizes ùêø( Œìpos, ÃÇÔ∏ÄŒìpos ).
Remark 3.1. We note that neither the Hellinger distance nor the K-L divergence
between the distributions ùí©(ùúápos(ùëå), Œìpos) and ùí©(ùúápos(ùëå), ÃÇÔ∏ÄŒìpos) depends on the data
ùëå. Optimality in distribution does not necessarily hold when the posterior means
are different.
3.2.3
Optimality results
We are now in a position to state one of the main results of this chapter. Henceforth
whenever we write that (ùõº, ùë£) are eigenpairs of (ùê¥, ùêµ) we mean that (ùõº, ùë£) are the
generalized eigenvalue‚Äìeigenvector pairs of the matrix pencil (ùê¥, ùêµ).
Theorem 3.1 (Optimal posterior covariance approximation). Let (ùõø2
ùëñ, ÃÇÔ∏Äùë§ùëñ) be the
eigenpairs of the pencil:
(ùêª, Œì‚àí1
pr ),
(3.10)
with the ordering ùõø2
ùëñ‚â•ùõø2
ùëñ+1, and ùêª= ùê∫‚ä§Œì‚àí1
obsùê∫as in (3.3). Let ùêøbe a loss function
in the class ‚Ñídefined in (3.8).
Then:
(i) A minimizer, ÃÇÔ∏ÄŒìpos, of the loss, ùêø, between Œìpos and an element of ‚Ñ≥ùëüis given
by:
ÃÇÔ∏ÄŒìpos = Œìpr ‚àíùêæùêæ‚ä§,
ùêæùêæ‚ä§=
ùëü
‚àëÔ∏Å
ùëñ=1
ùõø2
ùëñ
1 + ùõø2
ùëñ
ÃÇÔ∏Äùë§ùëñÃÇÔ∏Äùë§‚ä§
ùëñ.
(3.11)
The corresponding minimum loss is given by:
ùêø(ÃÇÔ∏ÄŒìpos, Œìpos) = ùëì(1) ùëü+
‚àëÔ∏Å
ùëñ>ùëü
ùëì
(Ô∏Ç
1
1 + ùõø2
ùëñ
)Ô∏Ç
.
(3.12)
(ii) The minimizer (3.11) is unique if the first ùëüeigenvalues of (ùêª, Œì‚àí1
pr ) are different.
Theorem 3.1 provides a way to compute the best approximation of Œìpos by matrices
in ‚Ñ≥ùëü: it is just a matter of computing the eigenpairs corresponding to the decreasing
37

sequence of eigenvalues of the pencil (ùêª, Œì‚àí1
pr ) until a stopping criterion is satisfied.
This criterion can be based on the minimum loss (3.12). Notice that the minimum
loss is a function of the generalized eigenvalues (ùõø2
ùëñ)ùëñ‚â•ùëüthat have not been computed.
This is quite common in numerical linear algebra (e.g., error in the truncated SVD
[90, 110]). However, since the eigenvalues (ùõø2
ùëñ) are computed in a decreasing order,
the minimum loss can be easily bounded.
The generalized eigenvectors, ÃÇÔ∏Äùë§ùëñ, are orthogonal with respect to the inner product
induced by the prior precision matrix, and they maximize the Rayleigh ratio,
ÃÇÔ∏Ä‚Ñõ(ùëß) = ùëß‚ä§ùêªùëß
ùëß‚ä§Œì‚àí1
pr ùëß,
over subspaces of the form ÃÇÔ∏Å
ùí≤ùëñ= span‚ä•( ÃÇÔ∏Äùë§ùëó)ùëó<ùëñ. Intuitively, the vectors ÃÇÔ∏Äùë§ùëñassociated
with generalized eigenvalues greater than one correspond to directions in the param-
eter space (or subspaces thereof) where the curvature of the log-posterior density is
constrained more by the log-likelihood than by the prior.
3.2.4
Computing eigenpairs of (ùêª, Œì‚àí1
pr )
If a square root factorization of the prior covariance matrix Œìpr = ùëÜprùëÜ‚ä§
pr is available,
then the Hermitian generalized eigenvalue problem can be reduced to a standard one:
find the eigenpairs, (ùõø2
ùëñ, ùë§ùëñ), of ùëÜ‚ä§
prùêªùëÜpr, and transform the resulting eigenvectors ac-
cording to ùë§ùëñ‚Ü¶‚ÜíùëÜprùë§ùëñ[13, Section 5.2]. An analogous transformation is also possible
when a square root factorization of Œì‚àí1
pr is available. Notice that only the actions of ùëÜpr
and ùëÜ‚ä§
pr on a vector are required. For instance, evaluating the action of ùëÜpr might in-
volve the solution of an elliptic PDE [174]. There are numerous examples of priors for
which a decomposition Œìpr = ùëÜprùëÜ‚ä§
pr is readily available, e.g., [275, 85, 174, 281, 265].
Either direct methods or, more often, matrix-free algorithms (e.g., Lanczos iteration
or its block version [159, 204, 75, 111]) can be used to solve the standard Hermitian
eigenvalue problem [13, Section 4]. Reference implementations of these algorithms are
available in ARPACK [164]. We note that the Lanczos iteration comes with a rich
literature on error analysis (e.g., [147, 203, 205, 143, 239, 110]). Alternatively, one can
38

use randomized methods [116], which offer the advantage of parallelism (asynchronous
computations) and robustness over standard Lanczos methods [40]. If a square root
factorization of Œìpr is not available, but it is possible to solve linear systems with
Œì‚àí1
pr , we can use a Lanczos method for generalized Hermitian eigenvalue problems
[13, Section 5.5] where a Krylov basis orthogonal with respect to the inner product
induced by Œì‚àí1
pr is maintained. Again, ARPACK provides an efficient implementation
of these solvers. When accurately solving linear systems with Œì‚àí1
pr is a difficult task,
we refer the reader to alternative algorithms proposed in [243] and [112].
Remark 3.2. If a factorization Œìpr = ùëÜprùëÜ‚ä§
pr is available, then it is straightforward to
obtain an expression for a non-symmetric square root of the optimal approximation
of Œìpos (3.11) as in [43]:
ÃÇÔ∏ÄùëÜpos = ùëÜpr
(Ô∏É
ùëü
‚àëÔ∏Å
ùëñ=1
[Ô∏É
1
‚àöÔ∏Ä
1 + ùõø2
ùëñ
‚àí1
]Ô∏É
ùë§ùëñùë§‚ä§
ùëñ+ ùêº
)Ô∏É
(3.13)
such that ÃÇÔ∏ÄŒìpos = ÃÇÔ∏ÄùëÜpos ÃÇÔ∏ÄùëÜ‚ä§
pos and ùë§ùëñ= ùëÜ‚àí1
pr ÃÇÔ∏Äùë§ùëñ. This expression can be used to efficiently
sample from the approximate posterior distribution ùí©(ùúápos(ùëå), ÃÇÔ∏ÄŒìpos) (e.g., [96, 186]).
Alternative techniques for sampling from high-dimensional Gaussian distributions can
be found, for instance, in [211, 100].
3.3
Properties of the optimal covariance approxima-
tion
Now we discuss several implications of the optimal approximation of Œìpos introduced
in the previous section. We start by describing the relationship between this ap-
proximation and the directions of greatest relative reduction of prior variance. Then
we interpret the covariance approximation as the result of projecting the likelihood
function onto a ‚Äúdata-informed‚Äù subspace. Finally, we contrast the present approach
with several other approximation strategies: using the Frobenius norm as a loss func-
tion for the covariance matrix approximation, or developing low-rank approximations
39

based on prior or Hessian information alone. We conclude by drawing the connections
with the BFGS Kalman filter update.
3.3.1
Interpretation of the eigendirections
Thanks to the particular structure of loss functions in ‚Ñí, the problem of approxi-
mating Œìpos is equivalent to that of approximating Œì‚àí1
pos. Yet the form of the optimal
approximation of Œì‚àí1
pos is important, as it explicitly describes the directions that con-
trol the ratio of posterior to prior variance. The following corollary to Theorem 3.1
characterizes these directions.
Corollary 3.1 (Optimal posterior precision approximation). Let (ùõø2
ùëñ, ÃÇÔ∏Äùë§ùëñ) and ùêø‚àà‚Ñí
be defined as in Theorem 3.1. Then:
(i) A minimizer of ùêø(ùêµ, Œì‚àí1
pos) for
ùêµ‚àà‚Ñ≥‚àí1
ùëü
:=
{Ô∏Ä
Œì‚àí1
pr + ùêΩùêΩ‚ä§: rank(ùêΩ) ‚â§ùëü
}Ô∏Ä
(3.14)
is given by
ÃÇÔ∏ÄŒì‚àí1
pos = Œì‚àí1
pr + ùëàùëà‚ä§,
ùëàùëà‚ä§=
ùëü
‚àëÔ∏Å
ùëñ=1
ùõø2
ùëñÃÉÔ∏Äùë§ùëñÃÉÔ∏Äùë§‚ä§
ùëñ,
ÃÉÔ∏Äùë§ùëñ= Œì‚àí1
pr ÃÇÔ∏Äùë§ùëñ.
(3.15)
The minimizer (3.15) is unique if the first ùëüeigenvalues of (ùêª, Œì‚àí1
pr ) are different.
(ii) The optimal posterior precision matrix (3.15) is precisely the inverse of the
optimal posterior covariance matrix (3.11).
(iii) The vectors ÃÉÔ∏Äùë§ùëñare generalized eigenvectors of the pencil (Œìpos, Œìpr):
Œìpos ÃÉÔ∏Äùë§ùëñ=
1
1 + ùõø2
ùëñ
Œìpr ÃÉÔ∏Äùë§ùëñ.
(3.16)
Note that the definition of the class ‚Ñ≥‚àí1
ùëü
is analogous to that of ‚Ñ≥ùëü. Indeed,
Lemma C.2 in Appendix C defines a bijection between these two classes.
40

The vectors ÃÉÔ∏Äùë§ùëñ= Œì‚àí1
pr ÃÇÔ∏Äùë§ùëñare orthogonal with respect to the inner product defined
by Œìpr. By (3.16), we also know that ÃÉÔ∏Äùë§ùëñminimizes the generalized Rayleigh quotient,
‚Ñõ(ùëß) = ùëß‚ä§Œìposùëß
ùëß‚ä§Œìpr ùëß= Var(ùëß‚ä§ùë•| ùë¶)
Var(ùëß‚ä§ùë•) ,
(3.17)
over subspaces of the form ÃÉÔ∏Å
ùí≤ùëñ= span‚ä•( ÃÉÔ∏Äùë§ùëó)ùëó<ùëñ. This Rayleigh quotient is precisely
the ratio of posterior to prior variance along a particular direction, ùëß, in the param-
eter space. The smallest values that ‚Ñõcan take over the subspaces ÃÉÔ∏Å
ùí≤ùëñare exactly
the smallest generalized eigenvalues of (Œìpos, Œìpr). In particular, the data are most
informative along the first ùëüeigenvectors ÃÉÔ∏Äùë§ùëñand, since
‚Ñõ( ÃÉÔ∏Äùë§ùëñ) = Var( ÃÉÔ∏Äùë§‚ä§
ùëñùë•| ùë¶)
Var( ÃÉÔ∏Äùë§‚ä§
ùëñùë•)
=
1
1 + ùõø2
ùëñ
,
(3.18)
the posterior variance is smaller than the prior variance by a factor of (1 + ùõø2
ùëñ)‚àí1. In
the span of the other eigenvectors, ( ÃÉÔ∏Äùë§ùëñ)ùëñ>ùëü, the data are not as informative. Hence,
( ÃÉÔ∏Äùë§ùëñ) are the directions along which the ratio of posterior to prior variance is mini-
mized. Furthermore, a simple computation shows that these directions also maximize
the relative difference between prior and posterior variance normalized by the prior
variance. Indeed, if the directions ( ÃÉÔ∏Äùë§ùëñ) minimize (3.17) then they must also maximize
1 ‚àí‚Ñõ(ùëß), leading to:
1 ‚àí‚Ñõ( ÃÉÔ∏Äùë§ùëñ) = Var( ÃÉÔ∏Äùë§‚ä§
ùëñùë•) ‚àíVar( ÃÉÔ∏Äùë§‚ä§
ùëñùë•| ùë¶)
Var( ÃÉÔ∏Äùë§‚ä§
ùëñùë•)
=
ùõø2
ùëñ
1 + ùõø2
ùëñ
.
(3.19)
3.3.2
Optimal projector
Since the data are most informative on a subspace of the parameter space, it should
be possible to reduce the effective dimension of the inference problem in a manner
that is consistent with the posterior approximation. This is essentially the content of
the following corollary, which follows by a simple computation.
Corollary 3.2 (Optimal projector). Let ÃÇÔ∏ÄŒìpos and the vectors ( ÃÇÔ∏Äùë§ùëñ, ÃÉÔ∏Äùë§ùëñ) be defined as
in Theorems 3.1 and 3.1. Consider the reduced forward operator ÃÇÔ∏Äùê∫ùëü= ùê∫‚àòùëÉùëü, where
41

ùëÉùëüis the oblique projector (i.e., ùëÉ2
ùëü= ùëÉùëübut ùëÉ‚ä§
ùë°Ã∏= ùëÉùëü):
ùëÉùëü=
ùëü
‚àëÔ∏Å
ùëñ=1
ÃÇÔ∏Äùë§ùëñÃÉÔ∏Äùë§‚ä§
ùëñ.
(3.20)
Then ÃÇÔ∏ÄŒìpos is precisely the posterior covariance matrix corresponding to the Bayesian
linear model:
ùëå| ùëã‚àºùí©( ÃÇÔ∏Äùê∫ùëüùëã, Œìobs),
ùëã‚àºùí©(0, Œìpr).
(3.21)
The projected Gaussian linear model (3.21) reveals the intrinsic dimensionality of
the inference problem. The introduction of the optimal projector (3.20) is also useful
in the context of dimensionality reduction for nonlinear inverse problems. In this case
a particularly simple and effective approximation of the posterior density, ùúãpos(ùë•|ùë¶), is
of the form ÃÇÔ∏Äùúãpos(ùë•|ùë¶) ‚àùùúã(ùë¶; ùëÉùëüùë•) ùúãpr(ùë•), where ùúãpr is the prior density and ùúã(ùë¶; ùëÉùëüùë•)
is the density corresponding to the likelihood function with parameters constrained
by the projector. The range of the projector can be determined by combining locally
optimal data-informed subspaces from high-density regions in the support of the
posterior distribution. This approximation is the subject of a related work [74].
Returning to the linear inverse problem, notice also that the posterior mean of
the projected model (3.21) might be used as an efficient approximation of the exact
posterior mean. We will show in Section 3.4 that this posterior mean approximation
in fact minimizes the Bayes risk for a weighted squared-error loss among all low-rank
linear functions of the data.
3.3.3
Comparison with optimality in Frobenius norm
Thus far our optimality results for the approximation of Œìpos have been restricted to
the class of loss functions ‚Ñígiven in Definition 3.1. However, it is also interesting to
investigate optimality in the metric defined by the Frobenius norm. Given any two
matrices ùê¥and ùêµof the same size, the Frobenius distance between them is defined
as ‚Äñùê¥‚àíùêµ‚Äñ, where ‚Äñ ¬∑‚Äñ is the Frobenius norm. Note that the Frobenius distance does
not exploit the structure of the positive definite cone of symmetric matrices. The
42

matrix ÃÇÔ∏ÄŒìpos ‚àà‚Ñ≥ùëüthat minimizes the Frobenius distance from the exact posterior
covariance matrix is given by:
ÃÇÔ∏ÄŒìpos = Œìpr ‚àíùêæùêæ‚ä§,
ùêæùêæ‚ä§=
ùëü
‚àëÔ∏Å
ùëñ=1
ùúÜùëñùë¢ùëñùë¢‚ä§
ùëñ,
(3.22)
where (ùë¢ùëñ) are the directions corresponding to the ùëülargest eigenvalues of Œìpr ‚àíŒìpos.
This result can be very different from the optimal approximation given in Theorem
3.1. In particular, the directions (ùë¢ùëñ) are solutions of the eigenvalue problem
Œìpr ùê∫‚ä§Œì‚àí1
ùëåùê∫Œìpr ùë¢= ùúÜùë¢,
(3.23)
which maximize
ùë¢‚ä§(Œìpr ‚àíŒìpos)ùë¢= Var(ùë¢‚ä§ùëã) ‚àíVar(ùë¢‚ä§ùëã| ùëå).
(3.24)
That is, while optimality in the Riemannian metric ùëë‚Ñõidentifies directions that max-
imize the relative difference between prior and posterior variance, the Frobenius dis-
tance favors directions that maximize only the absolute value of this difference. There
are many reasons to prefer the former. For instance, data might be informative along
directions of low prior variance (perhaps due to inadequacies in prior modeling); a
covariance matrix approximation that is optimal in Frobenius distance may ignore
updates in these directions entirely. Also, if parameters of interest (i.e., components
of ùëã) have differing units of measurement, relative variance reduction provides a
unit-independent way of judging the quality of a posterior approximation; this notion
follows naturally from the second invariance property of ùëë‚Ñõin (3.7). From a computa-
tional perspective, solving the eigenvalue problem (3.23) is quite expensive compared
to finding the generalized eigenpairs of the pencil (ùêª, Œì‚àí1
pr ). Finally, optimality in the
Frobenius distance for an approximation of Œìpos does not yield an optimality state-
ment for the corresponding approximation of the posterior distribution, as shown in
Lemma 3.1 for loss functions in ‚Ñí.
43

3.3.4
Suboptimal posterior covariance approximations
Hessian-based and prior-based reduction schemes
The posterior approximation described by Theorem 3.1 uses both Hessian and prior
information. It is instructive to consider approximations of the linear Bayesian inverse
problem that rely only on one or the other.
As we will illustrate numerically in
Section 3.5.1, these approximations can be viewed as natural limiting cases of our
approach. They are also closely related to previous efforts in dimensionality reduction
that propose only Hessian-based [168] or prior-based [188] reductions. In contrast
with these previous efforts, here we will consider versions of Hessian- and prior-based
reductions that do not discard prior information in the remaining directions. In other
words, we will discuss posterior covariance approximations that remain in the form
of (3.4)‚Äîi.e., updating the prior covariance only in ùëüdirections.
A Hessian-based reduction scheme updates Œìpr in directions where the data have
greatest influence in an absolute sense (i.e., not relative to the prior). This involves
approximating the negative log-likelihood Hessian (3.3) with a low-rank decomposi-
tion as follows: let (ùë†2
ùëñ, ùë£ùëñ) be the eigenvalue-eigenvector pairs of ùêªwith the ordering
ùë†2
ùëñ‚â•ùë†2
ùëñ+1. Then a best low-rank approximation of ùêªin the Frobenius norm is given
by:
ùêª‚âà
ùëü
‚àëÔ∏Å
ùëñ=1
ùë†2
ùëñùë£ùëñùë£‚ä§
ùëñ= ùëâùëüùëÜùëüùëâ‚ä§
ùëü,
where ùë£ùëñis the ùëñth column of ùëâùëüand ùëÜùëü= diag{ùë†2
1, . . . , ùë†2
ùëü}. Using Woodbury‚Äôs identity
we then obtain an approximation of Œìpos as a low-rank negative semidefinite update
of Œìpr:
Œìpos ‚âà
(Ô∏Ä
ùëâùëüùëÜùëüùëâ‚ä§
ùëü+ Œì‚àí1
pr
)Ô∏Ä‚àí1 = Œìpr ‚àíŒìprùëâùëü
(Ô∏Ä
ùëÜ‚àí1
ùëü
+ ùëâ‚ä§
ùëüŒìprùëâùëü
)Ô∏Ä‚àí1 ùëâ‚ä§
ùëüŒìpr.
(3.25)
This approximation of the posterior covariance matrix belongs to the class ‚Ñ≥ùëü. Thus,
Hessian-based reductions are in general suboptimal when compared to the optimal
approximations defined in Theorem 3.1. Note that an equivalent way to obtain (3.25)
is to use a reduced forward operator of the form ÃÇÔ∏Äùê∫= ùê∫‚àòùëâùëüùëâ‚ä§
ùëü, which is the compo-
44

sition of the original forward operator with a projector onto the leading eigenspace
of ùêª. In general, the projector ùëÉùëü= ùëâùëüùëâ‚ä§
ùëü
is different from the optimal projector
defined in Corollary 3.2 and is thus suboptimal.
To achieve prior-based reductions, on the other hand, we restrict the Bayesian
inference problem to directions in the parameter space that explain most of the prior
variance. More precisely, we look for a rank-ùëüorthogonal projector, ùëÉùëü, that minimizes
the mean squared-error defined as:
‚Ñ∞(ùëÉùëü) = E
(Ô∏Ä
‚Äñùëã‚àíùëÉùëüùëã‚Äñ2)Ô∏Ä
,
(3.26)
where the expectation is taken over the prior distribution (assumed to have zero
mean) and ‚Äñ¬∑‚Äñ is the standard Euclidean norm [133]. Let (ùë°2
ùëñ, ùë¢ùëñ) be the eigenpairs of
Œìpr ordered as ùë°2
ùëñ‚â•ùë°2
ùëñ+1. Then a minimizer of (3.26) is given by a projector, ùëÉùëü, onto
the leading eigenspace of Œìpr defined as: ùëÉùëü= ‚àëÔ∏Äùëü
ùëñ=1 ùë¢ùëñùë¢‚ä§
ùëñ= ùëàùëüùëà‚ä§
ùëü, where ùë¢ùëñis the
ùëñth column of ùëàùëü. The actual approximation of the linear inverse problem consists
of using the projected forward operator, ÃÇÔ∏Äùê∫= ùê∫‚àòùëàùëüùëà‚ä§
ùëü. By direct comparison with
the optimal projector defined in Corollary 3.2, we see that prior-based reductions are
suboptimal in general. Also in this case, the posterior covariance matrix with the
projected Gaussian model can be written as a negative semidefinite update of Œìpr:
Œìpos ‚âàŒìpr ‚àíùëàùëüùëáùëü[ ( ùëà‚ä§
ùëüùêªùëàùëü)‚àí1 + ùëáùëü]‚àí1ùëáùëüùëà‚ä§
ùëü,
where ùëáùëü= diag{ùë°2
1, . . . , ùë°2
ùëü}. The double matrix inversion makes this low-rank up-
date computationally challenging to implement. It is also not optimal, as shown in
Theorem 3.1.
To summarize, the Hessian and prior-based dimensionality reduction techniques
are both suboptimal. These methods do not take into account the interactions be-
tween the dominant directions of ùêªand those of Œìpr, nor the relative importance of
these quantities. Accounting for such interaction is a key feature of the optimal covari-
ance approximation described in Theorem 3.1. Section 3.5.1 will illustrate conditions
under which these interactions become essential.
45

Connections with the BFGS Kalman filter
The linear Bayesian inverse problem analyzed in this chapter can be interpreted as
the analysis step of a linear Bayesian filtering problem [93]. If the prior distribution
corresponds to the forecast distribution at some time ùë°, the posterior coincides with
the so-called analysis distribution. In the linear case, with Gaussian process noise
and observational errors, both of these distributions are Gaussian. The Kalman filter
is a Bayesian solution to this filtering problem [146]. In [10] the authors propose a
computationally feasible way to implement (and approximate) this solution in large-
scale systems. The key observation is that when solving an SPD linear system of
the form ùê¥ùë•= ùëèby means of BFGS or limited memory BFGS (L-BFGS [176]),
one typically obtains an approximation of ùê¥‚àí1 for free. This approximation can be
written as a low-rank correction of an arbitrary positive definite initial approximation
matrix ùê¥‚àí1
0 . The matrix ùê¥‚àí1
0
can be, for instance, the scaled identity. Notice that
the approximation of ùê¥‚àí1 given by L-BFGS is full rank and positive definite. This
approximation is in principle convergent as the storage limit of L-BFGS increases
[198]. An L-BFGS approximation of ùê¥is also possible [276].
There are many ways to exploit this property of the L-BFGS method. For example,
in [10] the posterior covariance is written as a low-rank update of the prior covariance
matrix: Œìpos = Œìpr ‚àíŒìprùê∫‚ä§Œì‚àí1
ùëåùê∫Œìpr, where Œìùëå= Œìobs + ùê∫Œìprùê∫‚ä§, and Œì‚àí1
ùëå
itself
is approximated using the L-BFGS method. Since this approximation of Œìùëåis full
rank, however, this approach does not exploit potential low-dimensional structure of
the inverse problem. Alternatively, one can obtain an L-BFGS approximation of Œìpos
when solving the linear system Œì‚àí1
pos ùë•= ùê∫‚ä§Œì‚àí1
obsùëåfor the posterior mean ùúápos(ùëå) [11].
If one uses the prior covariance matrix as an initial approximation matrix, ùê¥‚àí1
0 , then
the resulting L-BFGS approximation of Œìpos can be written as a low-rank update of
Œìpr. This approximation format is similar to the one discussed in [96] and advocated
in this chapter. However, the approach of [11] (or its ensemble version [254]) does not
correspond to any known optimal approximation of the posterior covariance matrix,
nor does it lead to any optimality statement between the corresponding probability
46

distributions. This is an important contrast with the present approach, which we will
revisit numerically in Section 3.5.1.
3.4
Optimal approximation of the posterior mean
In this section, we develop and characterize fast approximations of the posterior mean
that can be used, for instance, to accelerate repeated inversion with multiple data
sets.
Note that we are not proposing alternatives to the efficient computation of
the posterior mean for a single realization of the data. This task can already be ac-
complished with current state-of-the-art iterative solvers for regularized least-squares
problems [128, 207, 3, 190, 17]. Instead, we are interested in constructing statistically
optimal approximations4 of the posterior mean as linear functions of the data. That
is, we seek a matrix ùê¥, from an approximation class to be defined shortly, such that
the posterior mean can be approximated as ùúápos(ùëå) ‚âàùê¥ùëå.
We will investigate
different approximation classes for ùê¥; in particular, we will only consider approxima-
tion classes for which applying ùê¥to a vector ùëåis relatively inexpensive. Computing
such a matrix ùê¥is more expensive than solving a single linear system associated with
the posterior precision matrix to determine the posterior mean. However, once ùê¥is
computed, it can be applied inexpensively to any realization of the data.5 Our ap-
proach is therefore justified when the posterior mean must be evaluated for multiple
instances of the data. This approach can thus be viewed as an offline‚Äìonline strategy,
where a more costly but data-independent offline calculation is followed by fast online
evaluations. Moreover, we will show that these approximations can be obtained from
an optimal approximation of the posterior covariance matrix (cf. Theorem 3.1) with
minimal additional cost. Hence, if one is interested in both the posterior mean and
covariance matrix (as is often the case in the Bayesian approach to inverse problems),
then the approximation formulas we propose can be more efficient than standard
approaches even for a single realization of the data.
4 We will precisely define this notion of optimality in Section 3.4.1.
5In particular, applying ùê¥is much cheaper than solving a linear system.
47

3.4.1
Optimality results
For the Bayesian linear model defined in (3.1), the posterior mode is equal to the
posterior mean, ùúápos(ùëå) = E(ùëã|ùëå), which is in turn the minimizer of the Bayes risk
for squared-error loss [163, 175]. We first review this fact and establish some basic
notation. Let ùëÜbe an SPD matrix and let
ùêø(ùõø(ùëå), ùëã) = (ùëã‚àíùõø(ùëå))‚ä§ùëÜ(ùëã‚àíùõø(ùëå)) = ‚Äñùëã‚àíùõø(ùëå)‚Äñ2
ùëÜ
be the loss incurred by the estimator ùõø(ùëå) of ùëã. The Bayes risk, ùëÖ(ùõø(ùëå), ùëã), of
ùõø(ùëå) is defined as the average loss over the joint distribution of ùëãand ùëå[49, 163]:
ùëÖ(ùõø(ùëå), ùëã) = E ( ùêø(ùõø(ùëå), ùëã) ). Since
ùëÖ(ùõø(ùëå), ùëã) = E
(Ô∏Ä
‚Äñ ùõø(ùëå) ‚àíùúápos(ùëå) ‚Äñ2
ùëÜ
)Ô∏Ä
+ E
(Ô∏Ä
‚Äñùúápos(ùëå) ‚àíùëã‚Äñ2
ùëÜ
)Ô∏Ä
,
(3.27)
it follows that ùõø(ùëå) = ùúápos(ùëå) minimizes the Bayes risk over all estimators of ùëã.
To study approximations of ùúápos(ùëå), we use the squared-error loss function defined
by the Mahalanobis distance [61] induced by Œì‚àí1
pos: ùêø(ùõø(ùëå), ùëã) = ‚Äñùõø(ùëå) ‚àíùëã‚Äñ2
Œì‚àí1
pos .
This loss function accounts for the geometry induced by the posterior measure on the
parameter space, penalizing errors in the approximation of ùúápos(ùëå) more strongly in
directions of lower posterior variance.
Under the assumption of zero prior mean, ùúápos(ùëå) is a linear function of the data.
Hence we seek approximations of ùúápos(ùëå) of the form ùê¥ùëå, where ùê¥is a matrix in
a class to be defined. Our goal is to obtain fast posterior mean approximations that
can be applied repeatedly to multiple realizations of ùëå. We consider two classes of
approximation matrices:
ùíúùëü:= {ùê¥: rank(ùê¥) ‚â§ùëü}
and
ÃÇÔ∏Ä
ùíúùëü:=
{Ô∏Ä
ùê¥= (Œìpr ‚àíùêµ) ùê∫‚ä§Œì‚àí1
obs : rank(ùêµ) ‚â§ùëü
}Ô∏Ä
.
(3.28)
The class ùíúùëüconsists of low-rank matrices; it is standard in the statistics literature
[133]. The class ÃÇÔ∏Ä
ùíúùëü, on the other hand, can be understood via comparison with (3.2);
48

it simply replaces Œìpos with a low-rank negative semidefinite update of Œìpr. We shall
henceforth use ùíúto denote either of the two classes above.
Let ùëÖùíú(ùê¥ùëå, ùëã) be the Bayes risk of ùê¥ùëåsubject to ùê¥‚ààùíú. We may now restate
our goal as: find a matrix, ùê¥* ‚ààùíú, that minimizes the Bayes risk ùëÖùíú(ùê¥ùëå, ùëã). That
is, find ùê¥* ‚ààùíúsuch that
ùëÖùíú(ùê¥*ùëå, ùëã) = min
ùê¥‚ààùíúE( ‚Äñùê¥ùëå‚àíùëã‚Äñ2
Œì‚àí1
pos ).
(3.29)
The following two theorems show that for either class of approximation matrices, ùíúùëü
or ÃÇÔ∏Ä
ùíúùëü, this problem admits a particularly simple analytical solution that exploits the
structure of the optimal approximation of Œìpos. The proofs of the theorems rely on a
result developed independently by Sondermann [256] and Friedland & Torokhti [102],
and are given in Appendix C. We also use the fact that E
(Ô∏Å
‚Äñùúápos(ùëå) ‚àíùëã‚Äñ2
Œì‚àí1
pos
)Ô∏Å
= ‚Ñì,
where ‚Ñìis the dimension of the parameter space.
Theorem 3.2. Let (ùõø2
ùëñ, ÃÇÔ∏Äùë§ùëñ) be defined as in Theorem 3.1 and let (ÃÇÔ∏Äùë£ùëñ) be generalized
eigenvectors of the pencil (ùê∫Œìprùê∫‚ä§, Œìobs) associated with a non-increasing sequence
of eigenvalues, with the normalization ÃÇÔ∏Äùë£‚ä§
ùëñŒìobs ÃÇÔ∏Äùë£ùëñ= 1. Then:
(i) A solution of (3.29) for ùê¥‚ààùíúùëüis given by:
ùê¥* =
ùëü
‚àëÔ∏Å
ùëñ=1
ùõøùëñ
1 + ùõø2
ùëñ
ÃÇÔ∏Äùë§ùëñÃÇÔ∏Äùë£‚ä§
ùëñ,
(3.30)
(ii) The corresponding minimum Bayes risk over ùíúùëüis given by:
ùëÖùíúùëü(ùê¥*ùëå, ùëã) = E
(Ô∏Å
‚Äñùê¥*ùëå‚àíùúápos(ùëå)‚Äñ2
Œì‚àí1
pos
)Ô∏Å
+E
(Ô∏Å
‚Äñùúápos(ùëå) ‚àíùëã‚Äñ2
Œì‚àí1
pos
)Ô∏Å
=
‚àëÔ∏Å
ùëñ>ùëü
ùõø2
ùëñ+‚Ñì.
(3.31)
Notice that the rank-ùëüposterior mean approximation given by Theorem 3.2 co-
incides with the posterior mean of the projected linear Gaussian model defined in
(3.21). Thus, applying this approximation to a new realization of the data requires
only a low-rank matrix-vector product, a computationally trivial task. We define the
49

quality of a posterior mean approximation as the minimum Bayes risk (3.31). No-
tice, however, that for a given rank ùëüof the approximation, (3.31) depends on the
eigenvalues that have not yet been computed. Since (ùõø2
ùëñ) are determined in order
of decreasing magnitude, (3.31) can be easily bounded (cf. discussion after Theorem
3.1). The forthcoming minimum Bayes risk (3.34) can be bounded analogously.
Remark 3.3. Equation (3.30) can be interpreted as the truncated GSVD solution
of a Tikhonov regularized linear inverse problem [121] (with unit regularization pa-
rameter). Hence, Theorem 3.2 also describes a Bayesian property of the (frequentist)
truncated GSVD estimator.
Remark 3.4. If factorizations of the form Œìpr = ùëÜprùëÜ‚ä§
pr and Œìobs = ùëÜobsùëÜ‚ä§
obs are read-
ily available, then we can characterize the triplets (ùõøùëñ, ÃÇÔ∏Äùë§ùëñ, ÃÇÔ∏Äùë£ùëñ) from a singular value
decomposition, ùëÜ‚àí1
obsùê∫ùëÜpr = ‚àëÔ∏Ä
ùëñ‚â•1 ùõøùëñùë£ùëñùë§‚ä§
ùëñ, of the matrix ùëÜ‚àí1
obsùê∫ùëÜpr with the transfor-
mations ÃÇÔ∏Äùë§ùëñ= ùëÜprùë§ùëñ, ÃÇÔ∏Äùë£ùëñ= ùëÜ‚àí‚ä§
obs ùë£ùëñand the ordering ùõøùëñ‚â•ùõøùëñ+1.
In particular, the
approximate posterior mean can be written as:
ùúá(ùëü)
pos(ùëå) = ùëÜpr(ùëÜ‚àí1
obsùê∫ùëÜpr)Tikh
ùëü
ùëÜ‚àí1
obsùëå
(3.32)
where (ùëÜ‚àí1
obsùê∫ùëÜpr)Tikh
ùëü
is the best rank-ùëüapproximation to a Tikhonov regularized
inverse.6 That is, for any matrix ùê¥, (ùê¥)ùëüis the best rank-ùëüapproximation of ùê¥(e.g.,
computed via SVD), whereas (ùê¥)Tikh := (ùê¥‚ä§ùê¥+ ùêº)‚àí1ùê¥‚ä§.
Theorem 3.3. Let ÃÇÔ∏ÄŒìpos ‚àà‚Ñ≥ùëübe the optimal approximation of Œìpos defined in The-
orem 3.1. Then:
(i) A solution of (3.29) for ùê¥‚ààÃÇÔ∏Ä
ùíúùëüis given by:
ÃÇÔ∏Äùê¥* = ÃÇÔ∏ÄŒìpos ùê∫‚ä§Œì‚àí1
obs.
(3.33)
6With unit regularization parameter and identity regularization operator [122].
50

(ii) The corresponding minimum Bayes risk over ÃÇÔ∏Ä
ùíúùëüis given by:
ùëÖÃÇÔ∏Ä
ùíúùëü( ÃÇÔ∏Äùê¥*ùëå, ùëã) = E
(Ô∏Ç‚É¶‚É¶‚É¶ÃÇÔ∏Äùê¥*ùëå‚àíùúápos(ùëå)
‚É¶‚É¶‚É¶
2
Œì‚àí1
pos
)Ô∏Ç
+E
(Ô∏Å
‚Äñùúápos(ùëå) ‚àíùëã‚Äñ2
Œì‚àí1
pos
)Ô∏Å
=
‚àëÔ∏Å
ùëñ>ùëü
ùõø6
ùëñ+‚Ñì.
(3.34)
Once the optimal approximation of Œìpos described in Theorem 3.3 is computed, the
cost of approximating ùúápos(ùëå) for a new realization of ùëåis dominated by the adjoint
and prior solves needed to apply ùê∫‚ä§and Œìpr, respectively. Combining the optimal
approximations of ùúápos(ùëå) and Œìpos given by Theorems 3.3 and 3.1, respectively, yields
a complete approximation of the Gaussian posterior distribution. This is precisely the
approximation adopted by the stochastic Newton MCMC method [186] to describe
the Gaussian proposal distribution obtained from a local linearization of the forward
operator of a nonlinear Bayesian inverse problem. Our results support the algorithmic
choice of [186] with precise optimality statements.
It is worth noting that the two optimal Bayes risks, (3.31) and (3.34), depend
on the parameter, ùëü, that defines the dimension of the corresponding approximation
classes ùíúùëüand ÃÇÔ∏Ä
ùíúùëü. In the former case, ùëüis the rank of the optimal matrix that de-
fines the approximation. In the latter case, ùëüis the rank of a negative update of Œìpr
that yields the posterior covariance matrix approximation. We shall thus refer to the
estimator given by Theorem 4.2 as the low-rank approximation and to the estimator
given by Theorem 3.3 as the low-rank update approximation. In both cases, we shall
refer to ùëüas the order of the approximation. A posterior mean approximation of order
ùëüwill be called under-resolved if more than ùëügeneralized eigenvalues of the pencil
(ùêª, Œì‚àí1
pr ) are greater than one. If this is the case, then using the low-rank update ap-
proximation is not appropriate because the associated Bayes risk includes high-order
powers of eigenvalues of (ùêª, Œì‚àí1
pr ) that are greater than one. Thus, under-resolved
approximations tend to be more accurate when using the low-rank approximation.
As we will show in Section 4.4, this estimator is also less expensive to compute than
its counterpart in Theorem 3.3. If, on the other hand, fewer than ùëüeigenvalues of
(ùêª, Œì‚àí1
pr ) are greater than one, then the optimal low-rank update estimator will have
51

better performance than the optimal low-rank estimator in the following sense:
0 < ùëÖùíúùëü(ùê¥*ùëå, ùëã) ‚àíùëÖÃÇÔ∏Ä
ùíúùëü( ÃÇÔ∏Äùê¥*ùëå, ùëã) =
‚àëÔ∏Å
ùëñ>ùëü
ùõø2
ùëñ
(Ô∏Ä
1 + ùõø2
ùëñ
)Ô∏Ä(Ô∏Ä
1 ‚àíùõø2
ùëñ
)Ô∏Ä
.
3.4.2
Connection with ‚Äúpriorconditioners‚Äù
In this subsection we draw connections between the low-rank approximation of the
posterior mean given in Theorem 3.2 and the regularized solution of a discrete ill-
posed inverse problem, ùëå= ùê∫ùëã+‚Ñ∞(using the notation of this chapter), as presented
in [47, 44]. In [47, 44], the authors propose an early stopping regularization using
iterative solvers preconditioned by prior statistical information on the parameter of
interest, say ùëã‚àºùí©(0, Œìpr), and on the noise, say ‚Ñ∞‚àºùí©(0, Œìobs).7
That is, if
factorizations Œìpr = ùëÜprùëÜ‚ä§
pr and Œìobs = ùëÜobsùëÜ‚ä§
obs are available, then [47] provides a
solution, ùë•= ùëÜpr ùëû, to the inverse problem, where ùëûcomes from an early stopping
regularization applied to the preconditioned linear system:8
ùëÜ‚àí1
obsùê∫ùëÜpr ùëû= ùëÜ‚àí1
obsùë¶.
(3.35)
The iterative method of choice in this case is the CGLS algorithm [47, 119] (or GMRES
for nonsymmetric square systems [45]) equipped with a proper stopping criterion
(e.g., the discrepancy principle [144]). Although the approach of [47] is not exactly
Bayesian, we can still use the optimality results of Theorem 3.2 to justify the observed
good performance of this particular form of regularization. By a property of the CGLS
algorithm, the ùëüth iterate, ùë•ùëü= ùëÜprùëûùëü, satisfies:
ùëûùëü= argmin
ùëû‚ààùí¶ùëü( ÃÇÔ∏Ä
ùêª,ÃÇÔ∏Äùë¶)
‚Äñ ùëÜ‚àí1
obsùë¶‚àíùëÜ‚àí1
obsùê∫ùëÜprùëû‚Äñ.
(3.36)
where ùí¶ùëü( ÃÇÔ∏Äùêª, ÃÇÔ∏Äùë¶) is the ùëü‚Äìdimensional Krylov subspace associated with the matrix
ÃÇÔ∏Äùêª= ùëÜ‚ä§
prùêªùëÜpr and starting vector ÃÇÔ∏Äùë¶= ùëÜ‚ä§
prùê∫‚ä§Œì‚àí1
obsùë¶. It was shown in [127] that the
7It suffices to consider a Gaussian approximation to the distribution of ùëãand ‚Ñ∞
8We can think of ùë¶as a particular realization of ùëå.
52

CGLS solution, at convergence, can be written as ùë•* = ùëÜpr(ùëÜ‚àí1
obsùê∫ùëÜpr)‚Ä†ùëÜ‚àí1
obsùë¶, where
( ¬∑ )‚Ä† denotes the Moore-Penrose pseudoinverse [194, 216]. To highlight the differences
between the CGLS solution and (3.32), we assume that ùí¶ùëü( ÃÇÔ∏Äùêª, ùë¶) ‚âàran(ùëäùëü) for all
ùëü, where ùëäùëü= [ùë§1 | ¬∑ ¬∑ ¬∑ | ùë§ùëü], ran(ùê¥) denotes the range of a matrix ùê¥, and ÃÇÔ∏Äùêª=
‚àëÔ∏Ä
ùëñùõø2
ùëñùë§ùëñùë§‚ä§
ùëñis an SVD of ÃÇÔ∏Äùêª. Notice that the condition ùí¶ùëü( ÃÇÔ∏Äùêª, ùë¶) ‚âàran(ùëäùëü) is usually
quite reasonable for moderate values of ùëü. This practical observation is at the heart of
the Lanczos iteration for symmetric eigenvalue problems [159]. With simple algebraic
manipulations we conclude that:
ùë•ùëü‚âàùëÜpr(ùëÜ‚àí1
obsùê∫ùëÜpr)‚Ä†
ùëüùëÜ‚àí1
obsùë¶.
(3.37)
Recall from (3.32) that the optimal rank‚Äìùëüapproximation of the posterior mean
defined in Theorem 3.2 can be written as:
ùúá(ùëü)
pos(ùëå) = ùëÜpr(ùëÜ‚àí1
obsùê∫ùëÜpr)Tikh
ùëü
ùëÜ‚àí1
obsùëå.
(3.38)
The only difference between (3.37) and (3.38) is the use of a Tikhonov-regularized in-
verse in (3.38) as opposed to a Moore-Penrose pseudoinverse. If ùëÜ‚àí1
obsùê∫ùëÜpr = ‚àëÔ∏Ä
ùëñ‚â•1 ùõøùëñùë£ùëñùë§‚ä§
ùëñ
is a reduced SVD of the matrix ùëÜ‚àí1
obsùê∫ùëÜpr, then:
(ùëÜ‚àí1
obsùê∫ùëÜpr)‚Ä†
ùëü=
‚àëÔ∏Å
ùëñ‚â§ùëü
1
ùõøùëñ
ùë§ùëñùë£‚ä§
ùëñ,
(ùëÜ‚àí1
obsùê∫ùëÜpr)Tikh
ùëü
=
‚àëÔ∏Å
ùëñ‚â§ùëü
ùõøùëñ
1 + ùõø2
ùëñ
ùë§ùëñùë£‚ä§
ùëñ.
(3.39)
These two matrices are nearly identical for values of ùëücorresponding to ùõø2
ùëügreater than
one9 (assuming the ordering ùõø2
ùëñ‚â•ùõø2
ùëñ+1). Beyond this regime, it might be convenient
to stop the CGLS solver to obtain (3.37) (i.e., early stopping regularization). The
similarity of these expressions is quite remarkable since (3.38) was derived as the
minimizer of the optimization problem (3.29) with ùíú= ùíúùëü. This informal argument
may explain why priorconditioners perform so well in applications [46, 131].
Yet
9In Section 3.5 we show that by the time we start including generalized eigenvalues ùõø2
ùëñ‚âà1 in
(3.30), the approximation of the posterior mean is usually already satisfactory.
Intuitively, this
means that all the directions in parameter space where the data are more informative than the prior
have been considered.
53

we remark that the goals of Theorem 3.2 and [47] are still quite different; [47] is
concerned with preconditioning techniques for early stopping regularization of ill-
posed inverse problems, whereas Theorem 3.2 is concerned with statistically optimal
approximations of the posterior mean in the Bayesian framework.
Algorithm 1 Optimal low-rank approximation of the posterior mean
INPUT: forward and adjoint models ùê∫, ùê∫‚ä§; prior and noise precisions Œì‚àí1
pr , Œì‚àí1
obs;
approximation order ùëü‚ààN
OUTPUT: approximate posterior mean ùúá(ùëü)
pos(ùëå)
1: Find the ùëüleading generalized eigenvalue-eigenvector pairs (ùõø2
ùëñ, ÃÇÔ∏Äùë§ùëñ) of the pencil
(ùê∫‚ä§Œì‚àí1
obsùê∫, Œì‚àí1
pr )
2: Find the ùëüleading generalized eigenvector pairs (ÃÇÔ∏Äùë£ùëñ) of the pencil (ùê∫Œìprùê∫‚ä§, Œìobs)
3: For each new realization of the data ùëå, compute ùúá(ùëü)
pos(ùëå) = ‚àëÔ∏Äùëü
ùëñ=1 ùõøùëñ(1 + ùõø2
ùëñ)‚àí1 ÃÇÔ∏Äùë§ùëñÃÇÔ∏Äùë£‚ä§
ùëñùëå.
Algorithm 2 Optimal low-rank update approximation of the posterior mean
INPUT: forward and adjoint models ùê∫, ùê∫‚ä§; prior and noise precisions Œì‚àí1
pr , Œì‚àí1
obs;
approximation order ùëü‚ààN
OUTPUT: approximate posterior mean ÃÇÔ∏Äùúá(ùëü)
pos(ùëå)
1: Obtain ÃÇÔ∏ÄŒìpos as described in Theorem 3.1.
2: For each new realization of the data ùëå, compute ÃÇÔ∏Äùúá(ùëü)
pos(ùëå) = ÃÇÔ∏ÄŒìpos ùê∫‚ä§Œì‚àí1
obs ùëå.
3.5
Numerical examples
Now we provide several numerical examples to illustrate the theory developed in
the preceding sections. We start with a synthetic example to demonstrate various
posterior covariance matrix approximations, and continue with two more realistic
linear inverse problems where we also study posterior mean approximations.
3.5.1
Example 1: Hessian and prior with controlled spectra
We begin by investigating the approximation of Œìpos as a negative semidefinite update
of Œìpr. We compare the optimal approximation obtained in Theorem 3.1 with the
Hessian-, prior-, and BFGS-based reduction schemes discussed in Section 3.3.4. The
idea is to reveal differences between these approximations by exploring regimes where
54

the data have differing impacts on the prior information. Since the directions defining
the optimal update are the generalized eigenvectors of the pencil (ùêª, Œì‚àí1
pr ), we shall
also refer to this update as the generalized approximation.
To compare these approximation schemes, we start with a simple example with
diagonal Hessian and prior covariance matrices: ùê∫= ùêº, Œìobs = diag{ùúé2
ùëñ}, and Œìpr =
diag{ùúÜ2
ùëñ}. Since the forward operator ùê∫is the identity, this problem can (loosely) be
thought of as denoising a signal ùëã. In this case, ùêª= Œì‚àí1
obs and Œìpos = diag{ùúÜ2
ùëñùúé2
ùëñ/(ùúé2
ùëñ+
ùúÜ2
ùëñ)}. The ratios of posterior to prior variance in the canonical directions (ùëíùëñ) are
Var(ùëí‚ä§
ùëñùëã| ùëå)
Var(ùëí‚ä§
ùëñùëã)
=
1
1 + ùúÜ2
ùëñ/ùúé2
ùëñ
.
We note that if the observation variances ùúé2
ùëñare constant, ùúéùëñ= ùúé, then the directions
of greatest variance reduction are those corresponding to the largest prior variance.
Hence the prior distribution alone determines the most informed directions, and the
prior-based reduction is as effective as the generalized one. On the other hand, if
the prior variances ùúÜ2
ùëñare constant, ùúÜùëñ= ùúÜ, but the ùúéùëñvary, then the directions of
highest variance reduction are those corresponding to the smallest noise variance.
This time the noise distribution alone determines the most important directions, and
Hessian-based reduction is as effective as the generalized one. In the case of more
general spectra, the important directions depend on the ratios ùúÜ2
ùëñ/ùúé2
ùëñand thus one
has to use the information provided by both the noise and prior distributions. This
is done naturally by the generalized reduction.
We now generalize this simple case by moving to full matrices ùêªand Œìpr with
a variety of prescribed spectra. We assume that ùêªand Œìpr have SVDs of the form
ùêª= ùëàŒõùëà‚ä§and Œìpr = ùëâÃÉÔ∏ÄŒõùëâ‚ä§, where Œõ = diag{ùúÜ1, . . . , ùúÜùëõ} and ÃÉÔ∏ÄŒõ = diag{ÃÉÔ∏ÄùúÜ1, . . . , ÃÉÔ∏ÄùúÜùëõ}
with
ùúÜùëò= ùúÜ0/ùëòùõº+ ùúè
and
ÃÉÔ∏ÄùúÜùëò= ÃÉÔ∏ÄùúÜ0/ùëòÃÉÔ∏Äùõº+ ÃÉÔ∏Äùúè.
To consider many different cases, the orthogonal matrices ùëàand ùëâare randomly
and independently generated uniformly over the orthogonal group [264], leading to
different realizations of ùêªand Œìpr. In particular, ùëàand ùëâare computed with a ùëÑùëÖ
55

decomposition of a square matrix of independent standard Gaussian entries using a
Gram-Schmidt orthogonalization. (In this case, the standard Householder reflections
cannot be used.)
Before discussing the results of the first experiment, we explain our implementa-
tion of BFGS-based reduction. We ran the BFGS optimizer with a dummy quadratic
optimization target ùí•(ùë•) = 1
2 ùë•‚ä§Œì‚àí1
posùë•and used Œìpr as the initial approximation ma-
trix for Œìpos. Thus, the BFGS approximation of the posterior covariance matrix can
be written as Œìpos = Œìpr ‚àíùêæùêæ‚ä§for some rank‚Äìùëümatrix ùêæ. The rank‚Äìùëüupdate is
constructed by running the BFGS optimizer for ùëüsteps from random initial condi-
tions as shown in [11]. Note that in order to obtain results for sufficiently high-rank
updates, we use BFGS rather than L-BFGS in our numerical examples. While [10, 11]
in principle employ L-BFGS, the results in these papers use a number of optimization
steps roughly equal to the number of vectors stored in L-BFGS; our approach thus is
comparable to [10, 11]. Nonetheless, some results for the highest-rank BFGS updates
are not plotted in Figures 3-1 and 3-2, as the optimizer converged so close to the
optimum that taking further steps resulted in numerical instabilities.
Figure 3-1 summarizes the results of the first experiment. The top row shows
the prescribed spectra of ùêª‚àí1 (red) and Œìpr (blue). The parameters describing the
eigenvalues of Œìpr are fixed to ÃÉÔ∏ÄùúÜ0 = 1, ÃÉÔ∏Äùõº= 2, and ÃÉÔ∏Äùúè= 10‚àí6. The corresponding
parameters for ùêªare given by ùúÜ0 = 500 and ùúè= 10‚àí6 with ùõº= 0.345 (left), ùõº= 0.690
(middle), and ùõº= 1.724 (right). Thus, moving from the leftmost column to the
rightmost column, the data become increasingly less informative. The second row
in the figure shows the Riemannian distance ùëë‚Ñõbetween Œìpos and its approximation,
ÃÇÔ∏ÄŒìpos = Œìpr‚àíùêæùêæ‚ä§, as a function of the rank of ùêæùêæ‚ä§for 100 different realizations of ùêª
and Œìpr. The third row shows, for each realization of (ùêª, Œìpr) and for each fixed rank of
ùêæùêæ‚ä§, the difference between the distance obtained with a prior-, Hessian-, or BFGS-
based dimensionality reduction technique and the minimum distance obtained with
the generalized approximation. All of these differences are positive‚Äîa confirmation
of Theorem 3.1. But Figure 3-1 also shows interesting patterns consistent with the
observations made for the simple example above. When the spectrum of ùêªis basically
56

flat (left column), the directions along which the prior variance is reduced the most
are likely to be those corresponding to the largest prior variances, and thus a prior-
based reduction is almost as effective as the generalized one (as seen in the bottom
two rows on the left). As we move to the third column, eigenvalues of ùêª‚àí1 increase
more quickly. The data provide significant information only on a lower-dimensional
subspace of the parameter space. In this case, it is crucial to combine this information
with the directions in the parameter space along which the prior variance is the
greatest. The generalized reduction technique successfully accomplishes this task,
whereas the prior and Hessian reductions fail as they focus either on Œìpr or ùêªalone;
the key is to combine the two. The BFGS update performs remarkably well across
all three configurations of the Hessian spectrum, although it is clearly suboptimal
compared to the generalized reduction.
In Figure 3-2 the situation is reversed and the results are symmetric to those of
Figure 3-1. The spectrum of ùêª(red) is now kept fixed with parameters ùúÜ0 = 500,
ùõº= 1, and ùúè= 10‚àí9, while the spectrum of Œìpr (blue) has parameters ÃÉÔ∏ÄùúÜ0 = 1 and
ÃÉÔ∏Äùúè= 10‚àí9 with decay rates increasing from left to right: ÃÉÔ∏Äùõº= 0.552 (left), ÃÉÔ∏Äùõº= 1.103
(middle), and ÃÉÔ∏Äùõº= 2.759 (right). In the first column, the spectrum of the prior is
nearly flat. That is, the prior variance is almost equally spread along every direction in
the parameter space. In this case, the eigenstructure of ùêªdetermines the directions of
greatest variance reduction, and the Hessian-based reduction is almost as effective as
the generalized one. As we move towards the third column, the spectrum of Œìpr decays
more quickly. The prior variance is restricted to lower-dimensional subspaces of the
parameter space. Mismatch between prior- and Hessian-dominated directions then
leads to poor performance of both the prior- and Hessian-based reduction techniques.
However, the generalized reduction performs well also in this more challenging case.
The BFGS reduction is again empirically quite effective in most of the configurations
that we consider. It is not always better than the prior- or Hessian-based techniques
when the update rank is low, or when the prior spectrum decays slowly; for example,
Hessian-based reduction is more accurate than BFGS across all ranks in the first
column of Figure 3-2. But when either the prior covariance or the Hessian have quickly
57

decaying spectra, the BFGS approach performs almost as well as the generalized
reduction. Though this approach remains suboptimal, its approximation properties
deserve further theoretical study.
0
50
100
10
‚àí4
10
‚àí2
10
0
eigenvalue
index i
Œì p r
H ‚àí1
0
50
100
10
‚àí2
10
‚àí1
10
0
10
1
d F
pr i o r
hes s i a n
B FG S
g ener a l i zed
20
40
60
80
10
‚àí2
10
0
rank of update
diÔ¨Äerence in d F
pr i o r - g ener a l i zed
hes s i a n - g ener a l i zed
B FG S - g ener a l i zed
0
50
100
10
‚àí4
10
‚àí2
10
0
index i
20
40
60
80
10
‚àí2
10
0
20
40
60
80
10
‚àí2
10
0
rank of update
0
50
100
10
‚àí4
10
‚àí2
10
0
index i
20
40
60
80
10
0
20
40
60
80
10
‚àí2
10
0
rank of update
Figure 3-1: Top row: Eigenspectra of Œìpr (blue) and ùêª‚àí1 (red) for three values for the
decay rate of the eigenvalues of ùêª: ùõº= 0.345 (left), ùõº= 0.690 (middle) and ùõº= 1.724
(right). Second row: Riemannian distance ùëë‚Ñõbetween Œìpos and its approximation
versus the rank of the update for 100 realizations of Œìpr and ùêªusing prior-based
(blue), Hessian-based (green), BFGS-based (magenta) and optimal (red) updates.
Bottom row: Differences of posterior covariance approximation error (measured with
ùëë‚Ñõ) between the prior-based and optimal updates (blue), between the Hessian-based
and optimal updates (green), and between the BFGS-based and optimal updates
(magenta).
3.5.2
Example 2: X-ray tomography
We consider a classical inverse problem of X-ray computed tomography (CT), where
X-rays travel from sources to detectors through an object of interest. The intensities
from multiple sources are measured at the detectors, the goal is to reconstruct the
density of the object. In this framework, we investigate the performance of the optimal
58

0
50
100
10
‚àí6
10
‚àí4
10
‚àí2
10
0
eigenvalue
index i
Œì p r
H ‚àí1
20
40
60
80
100
10
0
10
1
d F
pr i o r
hes s i a n
B FG S
g ener a l i zed
20
40
60
80
10
‚àí2
10
0
rank of update
diÔ¨Äerence in d F
pr i o r - g ener a l i zed
hes s i a n - g ener a l i zed
B FG S - g ener a l i zed
0
50
100
10
‚àí6
10
‚àí4
10
‚àí2
10
0
index i
20
40
60
80
100
10
‚àí1
10
0
20
40
60
80
10
‚àí2
10
0
rank of update
0
50
100
10
‚àí6
10
‚àí4
10
‚àí2
10
0
index i
20
40
60
80
10
0
20
40
60
80
10
‚àí2
10
0
rank of update
Figure 3-2: Analogous to Figure 3-1 but this time the spectrum of ùêªis fixed, while
that of Œìpr has varying decay rates: ÃÉÔ∏Äùõº= 0.552 (left), ÃÉÔ∏Äùõº= 1.103 (middle) and ÃÉÔ∏Äùõº=
2.759 (right).
59

mean and covariance matrix approximations presented in Sections 3.2 and 3.4. This
synthetic example is motivated by a real application: real-time X-ray imaging of logs
that enter a saw mill for the purpose of automatic quality control. For instance, in the
system commercialized by Bintec (www.bintec.fi), logs enter the X-ray system on fast-
moving conveyer belt and fast reconstructions are needed. The imaging setting (e.g.,
X-ray source and detector locations) and the priors are fixed; only the data changes
from one log cross-section to another. The basis for our posterior mean approximation
can therefore be pre-computed, and repeated inversions can be carried out quickly
with direct matrix formulas.
We model the absorption of an X-ray along a line, ‚Ñìùëñ, using Beer‚Äôs law:
ùêºùëë= ùêºùë†exp
(Ô∏Ç
‚àí
‚à´Ô∏Å
‚Ñìùëñ
ùë•(ùë†)ùëëùë†
)Ô∏Ç
,
(3.40)
where ùêºùëëand ùêºùë†are the intensities at the detector and at the source, respectively,
and ùë•(ùë†) is the density of the object at position ùë†on the line ‚Ñìùëñ. The computational
domain is discretized into a grid and the density is assumed to be constant within
each grid cell. The line integrals are approximated as
‚à´Ô∏Å
‚Ñìùëñ
ùë•(ùë†)ùëëùë†‚âà
# of cells
‚àëÔ∏Å
ùëó=1
ùëîùëñùëóùë•ùëó,
(3.41)
where ùëîùëñùëóis the length of the intersection between line ‚Ñìùëñand cell ùëó, and ùë•ùëóis the
unknown density in cell ùëó.
The vector of absorptions along ùëölines can then be
approximated as
ùêºùëë‚âàùêºùë†exp (‚àíùê∫ùë•) ,
(3.42)
where ùêºùëëis the vector of ùëöintensities at the detectors and ùê∫= (ùëîùëñùëó) is the ùëö√ó ùëõ
matrix of intersection lengths for each of the ùëölines.
Even though the forward
operator (3.42) is nonlinear, the inference problem can be recast in a linear fashion
by taking logarithm of both sides of (3.42). This leads to the following linear model
for the inversion: ùëå= ùê∫ùëã+ ‚Ñ∞, where the measurement vector is ùëå= ‚àílog(ùêºùëë/ùêºùë†)
and the measurement errors are assumed to be iid Gaussian, ‚Ñ∞‚àºùí©(0, ùúé2 I).
60

The setup for the inference problem, borrowed from [?
], is as follows.
The
rectangular domain is discretized with an ùëõ√óùëõgrid. The true object consists of three
circular inclusions, each of uniform density, inside an annulus. Ten X-ray sources
are positioned on one side of a circle, and each source sends a fan of 100 X-rays
that are measured by detectors on the opposite side of the object.
Here, the 10
sources are distributed evenly so that they form a total illumination angle of 90
degrees, resulting in a limited-angle CT problem.
We use the exponential model
(3.40) to generate synthetic data in a discretization-independent fashion by computing
the exact intersections between the rays and the circular inclusions in the domain.
Gaussian noise with standard deviation ùúé= 0.002 is added to the simulated data.
The imaging setup and data from one source are illustrated in Figure 3-3.
The unknown density is estimated on a 128√ó128 grid. Thus the discretized vector,
ùëã, has length 16384, and direct computation of the posterior mean and the posterior
covariance matrix, as well as generation of posterior samples, can be computationally
nontrivial. To define the prior distribution, ùëãis modeled as a discretized solution of
a stochastic PDE of the form:
ùõæ
(Ô∏Ä
ùúÖ2‚Ñê‚àí‚ñ≥
)Ô∏Ä
ùë•(ùë†) = ùí≤(ùë†),
ùë†‚ààŒ©,
(3.43)
where ùí≤is a white noise process, ‚ñ≥is the Laplacian operator, and ‚Ñêis the identity
operator. The solution of (3.43) is a Gaussian random field whose correlation length
and variance are controlled by the free parameters ùúÖand ùõæ, respectively. A square
root of the prior precision matrix of ùëã(which is positive definite) can then be easily
computed (see [174] for details). We use ùúÖ= 10 and ùõæ=
‚àö
800 in our simulations.
Our first task is to compute an optimal approximation of Œìpos as a low-rank
negative update of Œìpr (cf. Theorem 3.1). Figure 3-4 (top row) shows the convergence
of the approximate posterior variance as the rank of the update increases. The zero-
rank update corresponds to Œìpr (first column). For this formally 16384-dimensional
problem, a good approximation of the posterior variance is achieved with a rank 200
update; hence the data are informative only on a low-dimensional subspace. The
61

quality of the covariance matrix approximation is also reflected in the structure of
samples drawn from the approximate posterior distributions (bottom row). All five of
these samples are drawn using the same random seed and the exact posterior mean,
so that all the differences observed are due to the approximation of Œìpos. Already
with a rank 100 update, the small-scale features of the approximate posterior sample
match those of the exact posterior sample. In applications, agreement in this ‚Äúeye-ball
norm‚Äù is important. Of course, Theorem 3.1 also provides an exact formula for the
error in the posterior covariance; this error is shown in the right panel of Figure 3-7
(blue curve).
0
20
40
60
80
100
0.9
0.92
0.94
0.96
0.98
1
1.02
detector pixel
intensity
Figure 3-3:
X-ray tomography problem.
Left:
Discretized domain, true object,
sources (red dots), and detectors corresponding to one source (black dots).
The
fan transmitted by one source is illustrated in gray. The density of the object is 0.006
in the outer ring and 0.004 in the three inclusions; the background density is zero.
Right: The true simulated intensity (black line) and noisy measurements (red dots)
for one source.
Our second task is to assess the performances of the two optimal posterior mean
approximations given in Section 3.4. We will use ùúá(ùëü)
pos(ùëå) to denote the low-rank
approximation and ÃÇÔ∏Äùúá(ùëü)
pos(ùëå) to denote the low-rank update approximation. Recall that
both approximations are linear functions of the data ùëå, given by ùúá(ùëü)
pos(ùëå) = ùê¥*ùëå
with ùê¥* ‚ààùíúùëüand ÃÇÔ∏Äùúá(ùëü)
pos(ùëå) = ÃÇÔ∏Äùê¥*ùëåwith ÃÇÔ∏Äùê¥* ‚ààÃÇÔ∏Ä
ùíúùëü, where the classes ùíúùëüand ÃÇÔ∏Ä
ùíúùëüare
defined in (3.28). As in Section 3.4, we shall use ùíúto denote either of the two classes.
Figure 3-5 shows the normalized error ‚Äñùúá(ùëå)‚àíùúápos(ùëå)‚ÄñŒì‚àí1
pos/‚Äñùúápos(ùëå)‚ÄñŒì‚àí1
pos for dif-
ferent approximations ùúá(ùëå) of the true posterior mean ùúápos(ùëå) and a fixed realization
62

prior
‚àí5.8
‚àí5.6
‚àí5.4
rank = 50
rank = 100
rank = 200
posterior
‚àí7
‚àí6.8
‚àí6.6
‚àí6.4
‚àí6.2
Figure 3-4: X-ray tomography problem. First column: Prior variance field, in log scale
(top), and a sample drawn from the prior distribution (bottom). Second through last
columns (left to right): Variance field, in log scale, of the approximate posterior as
the rank of the update increases (top); samples from the corresponding approximate
posterior distributions (bottom) assuming exact knowledge of the posterior mean.
{ùëå= ùë¶} of the data. The error is a function of the order ùëüof the approximation
class ùíú.
Snapshots of ùúá(ùë¶) are shown along the two error curves.
For reference,
ùúápos(ùë¶) is also shown at the top. We see that the errors decrease monotonically, but
that the low-rank approximation outperforms the low-rank update approximation for
lower values of ùëü. This is consistent with the discussion at the end of Section 3.4; the
crossing point of the error curves is also consistent with that analysis. In particular,
we expect the low-rank update approximation to outperform the low-rank approxi-
mation only when the approximation starts to include generalized eigenvalues of the
pencil (ùêª, Œì‚àí1
pr ) that are less than one‚Äîi.e., once the approximations are no longer
under-resolved. This can be confirmed by comparing Figure 3-5 with the decay of the
generalized eigenvalues of the pencil (ùêª, Œì‚àí1
pr ) in the right panel of Figure 3-7 (blue
curve).
On top of each snapshot in Figure 3-5, we show the relative CPU time required
to compute the corresponding posterior mean approximation for each new realization
of the data. The relative CPU time is defined as the time required to compute this
approximation10 divided by the time required to apply the posterior precision matrix
10This timing does not include the computation of (3.30) or (3.33), which should be regarded as
offline steps. Here we report the time necessary to apply the optimal linear function to any new
realization of the data, i.e., the online cost.
63

to a vector. This latter operation is essential to computing the posterior mean via
an iterative solver, such as a Krylov subspace method. These solvers are a standard
choice for computing the posterior mean in large-scale inverse problems. Evaluating
the ratio allows us to determine how many solver iterations could be performed with
a computational cost roughly equal to that of approximating the posterior mean
for a new realization of the data. Based on the reported times, a few observations
can be made. First of all, as anticipated in Section 3.4, computing ùúá(ùëü)
pos(ùëå) for any
new realization of the data is faster than computing ÃÇÔ∏Äùúá(ùëü)
pos(ùëå). Second, obtaining an
accurate posterior mean approximation requires roughly ùëü= 200, and the relative
CPU times for this order of approximation are 7.3 for ùúá(ùëü)
pos(ùëå) and 29.0 for ÃÇÔ∏Äùúá(ùëü)
pos(ùëå).
These are roughly the number of iterations of an iterative solver that one could
take for equivalent computational cost. That is, the speedup of the posterior mean
approximation compared to an iterative solver is not particularly dramatic in this
case, because the forward model ùê¥is simply a sparse matrix that is cheap to apply.
This is different for the heat equation example discussed in Section 3.5.3.
Note that the above computational time estimates exclude other costs associated
with iterative solvers. For instance, preconditioners are often applied; these signifi-
cantly decrease the number of iterations needed for the solvers to converge but, on
the other hand, increase the cost per iteration. A popular approach for solving the
posterior mean efficiently is to use the prior covariance as the preconditioner [40].
In the limited-angle tomography problem, including the application of this precondi-
tioner in the reference CPU time would reduce the relative CPU time of our ùëü= 200
approximations to 0.48 for ùúá(ùëü)
pos(ùëå) and 1.9 for ÃÇÔ∏Äùúá(ùëü)
pos(ùëå). That is, the cost of com-
puting our approximations is roughly equal to one iteration of a prior-preconditioned
iterative solver. The large difference compared to the case without preconditioning
is due to the fact that the cost of applying the prior here is computationally much
higher than applying the forward model.
64

Figure 3-6 (left panel) shows unnormalized errors in the approximation of ùúápos(ùë¶),
‚Äñùëí(ùë¶)‚Äñ2
Œì‚àí1
pos = ‚Äñùúá(ùëü)
pos(ùë¶) ‚àíùúápos(ùë¶)‚Äñ2
Œì‚àí1
pos
and
‚ÄñÃÇÔ∏Äùëí(ùë¶)‚Äñ2
Œì‚àí1
pos = ‚ÄñÃÇÔ∏Äùúá(ùëü)
pos(ùë¶) ‚àíùúápos(ùë¶)‚Äñ2
Œì‚àí1
pos,
(3.44)
for the same realization of ùëåused in Figure 3-5. In the same panel we also show the
expected values of these errors over the prior predictive distribution of ùëå, which is
exactly the ùëü-dependent component of the Bayes risk given in Theorems 4.2 and 3.3.
Both sets of errors decay with increasing ùëüand show a similar crossover between the
two approximation classes. But the particular error ‚Äñùëí(ùë¶)‚Äñ2
Œì‚àí1
pos departs consistently
from its expectation; this is not unreasonable in general (the mean estimator has a
nonzero variance), but the offset may be accentuated in this case because the data
are generated from an image that is not drawn from the prior. (The right panel of
Figure 3-6, which comes from Example 3, represents a contrasting case.)
By design, the posterior approximations described in this chapter perform well
when the data inform a low-dimensional subspace of the parameter space. To better
understand this effect, we also consider a full-angle configuration of the tomography
problem, wherein the sources and detectors are evenly spread around the entire un-
known object. In this case, the data are more informative than in the limited-angle
configuration. This can be seen in the decay rate of the generalized eigenvalues of the
pencil (ùêª, Œì‚àí1
pr ) in the center panel of Figure 3-7 (blue and red curves); eigenvalues
for the full-angle configuration decay more slowly than for the limited-angle configu-
ration. Thus, according to the optimal loss given in (3.12) (Theorem 3.1), the prior-
to-posterior update in the full-angle case must be of greater rank than the update in
the limited-angle case for any given approximation error. Also, good approximation
of ùúápos(ùëå) in the full-angle case requires higher order of the approximation class ùíú,
as is shown in Figure 3-8. But because the data are strongly informative, they allow
an almost perfect reconstruction of the underlying truth image. The relative CPU
times are similar to the limited angle case: roughly 8 for ùúá(ùëü)
pos(ùëå) and 14 for ÃÇÔ∏Äùúá(ùëü)
pos(ùëå).
If preconditioning with the prior covariance is included in the reference CPU time
calculation, the relative CPU times drop to 1.5 for ùúá(ùëü)
pos(ùëå) and to 2.6 for ÃÇÔ∏Äùúá(ùëü)
pos(ùëå).
65

We remark that in realistic applications of X-ray tomography, the limited angle setup
is extremely common as it is cheaper and more flexible (yielding smaller and lighter
devices) than a full-angle configuration.
0
50
100
150
200
250
300
350
400
10
‚àí4
10
‚àí2
10
0
10
2
10
4
normalized error
order of approximating class
19.0860
1.9139
22.0409
3.8533
28.9616
7.3060
34.9874
11.5087
¬µ p os
Figure 3-5: Limited-angle X-ray tomography: Comparison of the optimal posterior
mean approximations, ùúá(ùëü)
pos(ùëå) (blue) and ÃÇÔ∏Äùúá(ùëü)
pos(ùëå) (black) of ùúápos(ùëå) for a fixed
realization of the data {ùëå= ùë¶}, as a function of the order ùëüof the approximating
classes ùíúùëüand ÃÇÔ∏Ä
ùíúùëü, respectively. The normalized error for an approximation ùúá(ùë¶)
is defined as ‚Äñùúá(ùë¶) ‚àíùúápos(ùë¶)‚ÄñŒì‚àí1
pos / ‚Äñùúápos(ùë¶)‚ÄñŒì‚àí1
pos. The numbers above or below the
snapshots indicate the relative CPU time of the corresponding mean approximation‚Äî
i.e., the time required to compute the approximation divided by the time required to
apply the posterior precision matrix to a vector.
3.5.3
Example 3: Heat equation
Our last example is the classic linear inverse problem of solving for the initial con-
ditions of an inhomogeneous heat equation. Let ùë¢(ùë†, ùë°) be the time dependent state
of the heat equation on ùë†= (ùë†1, ùë†2) ‚ààŒ© = [0, 1]2, ùë°‚â•0, and let ùúÖ(ùë†) be the heat
conductivity field. Given initial conditions, ùë¢0(ùë†) = ùë¢(ùë†, 0), the state evolves in time
66

50
100
150
200
250
300
350
400
10
0
10
2
10
4
10
6
10
8
10
10
10
12
order of approximating class
‚à•e(y)‚à•2
Œì‚àí1
p o s
‚à•be(y)‚à•2
Œì‚àí1
p o s
Ey
h
‚à•e(y)‚à•2
Œì‚àí1
p o s
i
Ey
h
‚à•be (y)‚à•2
Œì‚àí1
p o s
i
50
100
150
200
250
300
10
‚àí5
10
0
10
5
10
10
10
15
10
20
order of approximating class
‚à•e(y)‚à•2
Œì‚àí1
p o s
‚à•be(y)‚à•2
Œì‚àí1
p o s
Ey
h
‚à•e(y)‚à•2
Œì‚àí1
p o s
i
Ey
h
‚à•be (y)‚à•2
Œì‚àí1
p o s
i
Figure 3-6: The errors ‚Äñùëí(ùë¶)‚Äñ2
Œì‚àí1
pos (blue) and ‚ÄñÃÇÔ∏Äùëí(ùë¶)‚Äñ2
Œì‚àí1
pos (black) defined by (3.44), and
their expected values in green and red, respectively; for Example 2 (left panel) and
Example 3 (right panel).
index i
0
100
200
300
400
500
eigenvalues
10-8
10-7
10-6
10-5
10-4
10-3
prior
limited angle
full angle
index i
100
200
300
400
500
generalized eigenvalues
10-2
10-1
100
101
102
103
104
105
limited angle
full angle
rank of update
100
200
300
400
500
dF
100
101
102
103
limited angle
full angle
Figure 3-7:
Left: Leading eigenvalues of Œìpr and ùêª‚àí1 in the limited-angle and
full-angle X-ray tomography problems. Center: Leading generalized eigenvalues of
the pencil (ùêª, Œì‚àí1
pr ) in the limited-angle (blue) and full-angle (red) cases.
Right:
ùëë‚Ñ±(Œìpos, ÃÇÔ∏ÄŒìpos) as a function of the rank of the update ùêæùêæ‚ä§, with ÃÇÔ∏ÄŒìpos = Œìpr ‚àíùêæùêæ‚ä§,
in the limited-angle (blue) and full-angle (red) cases.
67

0
100
200
300
400
500
600
700
800
10
‚àí5
10
‚àí3
10
‚àí1
10
1
10
3
10
5
normalized error
order of approximating class
6.8947
1.1752
10.1510
4.3366
14.0623
8.0357
18.2506
11.2724
¬µ p os
Figure 3-8: Same as Figure 3-5, but for full-angle X-ray tomography (sources and
receivers spread uniformly around the entire object).
according to the linear heat equation:
ùúïùë¢(ùë†, ùë°)
ùúïùë°
=
‚àí‚àá¬∑ (ùúÖ(ùë†)‚àáùë¢(ùë†, ùë°)),
ùë†‚ààŒ©, ùë°> 0,
ùúÖ(ùë†)‚àáùë¢(ùë†, ùë°) ¬∑ ùëõ(ùë†)
=
0,
ùë†‚ààùúïŒ©, ùë°> 0,
(3.45)
where ùëõ(ùë†) denotes the outward-pointing unit normal at ùë†‚ààùúïŒ©. We place ùëõùë†= 81
sensors at the locations ùë†1, . . . , ùë†ùëõùë†, uniformly spaced within the lower left quadrant
of the spatial domain, as illustrated by the black dots in Figure 3-9. We use a finite-
dimensional discretization of the parameter space based on the finite element method
on a regular 100√ó100 grid, {ùë†‚Ä≤
ùëñ}. Our goal is to infer the vector ùëã= (ùë¢0(ùë†‚Ä≤
ùëñ)) of initial
conditions on the grid. Thus, the dimension of the parameter space for the inference
problem is ùëõ= 104. We use data measured at 50 discrete times ùë°= ùë°1, ùë°2, . . . , ùë°50,
where ùë°ùëñ= ùëñ‚ñ≥ùë°, and ‚ñ≥ùë°= 2 √ó 10‚àí4. At each time ùë°ùëñ, pointwise observations of the
state ùë¢are taken at these sensors, i.e.,
ùëëùëñ= ùíûùë¢(ùë†, ùë°ùëñ),
(3.46)
68

where ùíûis the observation operator that maps the function ùë¢(ùë†, ùë°ùëñ) to
ùëë= (ùë¢(ùë†1, ùë°ùëñ), . . . , ùë¢(ùë†ùëõ, ùë°ùëñ))‚ä§.
(3.47)
The vector of observations is then ùëë= [ùëë1; ùëë2; . . . ; ùëë50]. The noisy data vector is
ùëå= ùëë+ ‚Ñ∞, where ‚Ñ∞‚àºùí©(0, ùúé2ùêº) and ùúé= 10‚àí2. Note that the data are a linear
function of the initial conditions, perturbed by Gaussian noise. Thus the data can be
written as:
ùëå= ùê∫ùëã+ ‚Ñ∞,
‚Ñ∞‚àºùí©(0, ùúé2ùêº).
(3.48)
where ùê∫is a linear map defined by the composition of the forward model (3.45) with
the observation operator (3.46), both linear.
We generate synthetic data by evolving the initial conditions shown in Figure
3-9.
This ‚Äútrue‚Äù value of the inversion parameters ùëãis a discretized realization
of a Gaussian process satisfying an SPDE of the same form used in the previous
tomography example, but now with a non-stationary permeability field.
In other
words, the truth is a draw from the prior in this example (unlike in the previous
example), and the prior Gaussian process satisfies the following SPDE:
ùõæ
(Ô∏Ä
ùúÖ2‚Ñê‚àí‚àá¬∑ c(ùë†)‚àá
)Ô∏Ä
ùë•(ùë†) = ùí≤(ùë†)
ùë†‚ààŒ©,
(3.49)
where c(ùë†) is the space-dependent permeability tensor.
Figure 3-10 and the right panel in Figure 3-6 show our numerical results. They
have the same interpretations as Figures 3-5 and 3-6 in the tomography example. The
trends in the figures are consistent with those encountered in the previous example
and confirm the good performance of the optimal low-rank approximation. Notice
that in Figures 3-10 and 3-6 the approximation of the posterior mean appears to be
nearly perfect (visually) once the error curves for the two approximations cross. This
is somewhat expected from the theory since we know that the crossing point should
occur when the approximations start to use eigenvalues of the pencil (ùêª, Œì‚àí1
pr ) that
are less than one‚Äîthat is, once we have exhausted directions in the parameter space
69

where the data are more constraining than the prior.
Again, we report the relative CPU time for each posterior mean approximation
above/below the corresponding snapshot in Figure 3-10. The results differ signifi-
cantly from the tomography example. For instance, at order ùëü= 200, which yields
approximations that are visually indistinguishable from the true mean, the relative
CPU times are 0.001 for ùúá(ùëü)
pos(ùëå) and 0.53 for ÃÇÔ∏Äùúá(ùëü)
pos(ùëå). Therefore we can compute
an accurate mean approximation for a new realization of the data much more quickly
than taking one iteration of an iterative solver. Recall that, consistent with the setting
described at the start of Section 3.4, this is a comparison of online times, after the
matrices (3.30) or (3.33) have been precomputed. The difference between this case
and tomography example of Section 3.5.2 is due to the higher CPU cost of apply-
ing the forward and adjoint models for the heat equation‚Äîsolving a time dependent
PDE versus applying a sparse matrix. Also, because the cost of applying the prior
covariance is negligible compared to that of the forward and adjoint solves in this
example, preconditioning the iterative solver with the prior would not strongly affect
the reported relative CPU times, unlike the tomography example.
Figure 3-11 illustrates some important directions characterizing the heat equation
inverse problem. The first two columns show the four leading eigenvectors of, re-
spectively, Œìpr and ùêª. Notice that the support of the eigenvectors of ùêªconcentrates
around the sensors. The third column shows the four leading directions ( ÃÇÔ∏Äùë§ùëñ) defined
in Theorem 3.1.
These directions define the optimal prior-to-posterior covariance
matrix update (cf. (3.11)). This update of Œìpr is necessary to capture directions ( ÃÉÔ∏Äùë§ùëñ)
of greatest relative difference between prior and posterior variance (cf. Corollary 3.1).
The four leading directions ( ÃÉÔ∏Äùë§ùëñ) are shown in the fourth column. The support of
these modes is again concentrated around the sensors, which intuitively makes sense
as these are directions of greatest variance reduction.
70

conductivity Ô¨Åeld
0
0.5
1
0
0.5
1
t = 0
0
0.5
1
0
0.5
1
t = 10‚ñ≥t
0
0.5
1
0
0.5
1
t = 20‚ñ≥t
0
0.5
1
0
0.5
1
t = 30‚ñ≥t
0
0.5
1
0
0.5
1
t = 40‚ñ≥t
0
0.5
1
0
0.5
1
t = 50‚ñ≥t
0
0.5
1
0
0.5
1
Figure 3-9: Heat equation problem. Initial condition (top left) and several snapshots
of the states at different times. Black dots indicate sensor locations.
0
50
100
150
200
250
300
350
400
10
‚àí10
10
‚àí5
10
0
10
5
normalized error
order of approximating class
0.5178
0.0014
0.5189
0.0040
0.5252
0.0098
0.5324
0.0150
¬µ p os
Figure 3-10: Same as Figure 3-5, but for Example 3.
71

prior
Hessian
bw i
ew i
Figure 3-11: Heat equation problem.
First column: Four leading eigenvectors of
Œìpr. Second column: Four leading eigenvectors of ùêª. Third column: Four leading
directions ( ÃÇÔ∏Äùë§ùëñ) (cf. (3.11)). Fourth column: Four leading directions ( ÃÉÔ∏Äùë§ùëñ) (cf. Corollary
3.1)
3.6
Discussion
This chapter has presented and characterized optimal approximations of the Bayesian
solution of linear inverse problems, with Gaussian prior and noise distributions defined
on finite-dimensional spaces. In a typical large-scale inverse problem, observations
may be informative‚Äîrelative to the prior‚Äîonly on a low-dimensional subspace of the
parameter space. Our approximations therefore identify and exploit low-dimensional
structure in the update from prior to posterior.
We have developed two types of optimality results.
In the first, the posterior
covariance matrix is approximated as a low-rank negative semidefinite update of the
prior covariance matrix. We describe an update of this form that is optimal with re-
spect to a broad class of loss functions between covariance matrices, exemplified by the
natural geodesic distance on the manifold of symmetric positive definite matrices [99].
We argue that this is the appropriate class of loss functions with which to evaluate
approximations of the posterior covariance matrix, and show that optimality in such
metrics identifies directions in parameter space along which the posterior variance is
72

reduced the most, relative to the prior. Optimal low-rank updates are derived from
a generalized eigendecomposition of the pencil defined by the negative log-likelihood
Hessian and the prior precision matrix. These updates have been proposed in previous
work [96], but our work complements these efforts by characterizing the optimality
of the resulting approximations. Under the assumption of exact knowledge of the
posterior mean, our results extend to optimality statements between the associated
distributions (e.g., optimality in the Hellinger distance and in the Kullback-Leibler
divergence). Second, we have developed fast approximations of the posterior mean
that are useful when repeated evaluations thereof are required for multiple realizations
of the data (e.g., in an online inference setting). These approximations are optimal
in the sense that they minimize the Bayes risk for squared-error loss induced by the
posterior precision matrix. The most computationally efficient of these approxima-
tions expresses the posterior mean as the product of a single low-rank matrix with
the data. We have demonstrated the covariance and mean approximations numeri-
cally on a variety of inverse problems: synthetic problems constructed from random
Hessian and prior covariance matrices; an X-ray tomography problem with different
observation scenarios; and inversion for the initial condition of a heat equation, with
localized observations and a non-stationary prior.
The material in this chapter has several possible extensions of interest. First, it is
natural to generalize the present approach to infinite-dimensional parameter spaces
endowed with Gaussian priors. This setting is essential to understanding and for-
malizing Bayesian inference over function spaces [43, 265]. Here, by analogy with
the current results, one would expect the posterior covariance operator to be well
approximated by a finite-rank negative perturbation of the prior covariance opera-
tor. A further extension could allow the data to become infinite-dimensional as well.
Another important task is to generalize the present methodology to inverse problems
with nonlinear forward models. One approach for doing so is presented in [74] and will
not be described in this thesis; other approaches are certainly possible. See [282] for
a recent contribution that generalizes the approximations presented in this chapter to
the nonlinear case by means of logarithmic Sobolev inequalities. See also Chapter 5
73

for a related discussion on low-rank couplings. Yet another interesting research topic
is the study of analogous approximation techniques for sequential inference. We note
that the assimilation step in a linear (or linearized) data assimilation scheme can be
already tackled within the framework presented here. But the nonstationary setting,
where inference is interleaved with evolution of the state, should introduce the possi-
bility for even more tailored and structure-exploiting approximations. Lastly, we note
that an important extension of this work is to account for ultimate goals in inference
the task. We address this topic in the forthcoming Chapter 4.
74

Chapter 4
Goal-oriented optimal approximations
of linear inverse problems
4.1
Introduction
As we saw in Chapter 3, the posterior distribution defines the Bayesian solution to
the inverse problem. Characterizing this posterior distribution is of primary interest
in a variety of engineering and science applications. For instance, we might be inter-
ested in the posterior marginals, the posterior probability of some functionals of the
parameters, or the probability of rare events under the posterior measure. In all these
cases we may need to draw samples from the posterior distribution. This sampling
task tends to be extremely challenging in large-scale applications, especially when the
parameters represent a finite-dimensional approximation to a distributed stochastic
process like a permeability or a temperature field. In many applications, however, we
are only interested in a particular function of the parameters (e.g., the temperature
field over a subregion of the entire domain or the probability that the temperature
exceeds a critical value). In this chapter we exploit such ultimate goals to reduce the
cost of inversion.
We begin by considering our usual finite-dimensional linear-Gaussian inverse prob-
lem of the form
ùëå= ùê∫ùëã+ ‚Ñ∞,
(4.1)
75

where ùëã‚ààRùëõrepresents the unknown parameters, ùëå‚ààRùëëdenotes the noisy ob-
servations, ùê∫‚ààRùëë√óùëõis a deterministic linear forward operator, and ‚Ñ∞‚àºùí©(0, Œìobs)
is a zero-mean additive Gaussian noise, statistically independent of ùëãand with co-
variance matrix Œìobs ‚âª0.
We prescribe a Gaussian prior distribution, ùí©(0, Œìpr),
on ùëãand assume, without loss of generality, zero prior mean and Œìpr ‚âª0.
In
Chapter 3, we were concerned with the posterior distribution of the parameters,
ùëã|ùëå‚àºùí©(ùúápos(ùëå), Œìpos), which has mean and covariance given by
ùúápos(ùëå) = Œìpos ùê∫‚ä§Œì‚àí1
obs ùëå,
Œìpos = (ùêª+ Œì‚àí1
pr )‚àí1,
(4.2)
where ùêª:= ùê∫‚ä§Œì‚àí1
obs ùê∫is the Hessian of the negative log-likelihood. In this chapter,
however, we are not interested in the parameters ùëãper se, but rather in a quantity
of interest (QoI) ùëçthat is a function of the parameters,
ùëç= ùí™ùëã,
(4.3)
for some linear and, without loss of generality, full row-rank operator ùí™‚ààRùëù√óùëõwith
ùëù< ùëõ. Our interests are thus goal-oriented, as we wish to characterize only ùëçand
not the parameters ùëã. Including such ultimate goals in the inference formulation is
an important modeling step in many applications of Bayesian inverse problems. This
additional step should reduce the computational complexity of inference by making
the ultimate goals explicit. Nevertheless, it is still not clear how to leverage ultimate
goals to yield more efficient Bayesian inference algorithms. The present chapter will
address this issue.
The full Bayesian solution to the goal-oriented inverse problem is the posterior
distribution of the QoI, i.e., ùëç|ùëå. It is easy to see that ùëç|ùëåis once again Gaussian
with mean and covariance matrix given by
ùúáùëç|ùëå(ùëå) = ùí™ùúápos(ùëå),
Œìùëç|ùëå= ùí™Œìpos ùí™‚ä§.
(4.4)
The goal of this work is to characterize statistically optimal, computationally effi-
76

cient, and structure‚Äìexploiting approximations of the statistics of ùëç|ùëåwhenever the
use of direct formulas such as (4.4) is challenging or impractical (perhaps due to high
computational complexity or excessive storage requirements). We will approximate
Œìùëç|ùëåas a low-rank negative update of the prior covariance of the QoI. Optimality will
be defined with respect to the natural geodesic distance on the manifold of symmet-
ric and positive definite (SPD) matrices [99]. The posterior mean ùúáùëç|ùëå(ùëå) will be
approximated as a low-rank function of the data, where optimality is defined by the
minimization of the Bayes risk for squared-error loss weighted by Œì‚àí1
ùëç|ùëå. The essence
of these approximations is the restriction of the inference process to directions in the
parameter space that are informed by the data relative to the prior and that are
relevant to the QoI. These directions correspond to the leading generalized eigenpairs
of a suitable matrix pencil that is fundamentally different from the one analyzed in
Chpater 3.
This chapter extends, in several different ways, the work on goal-oriented inference
originally presented in [169]. First of all, we introduce the notion of optimal approx-
imation, rather than exact computation, for both the posterior covariance matrix
and the posterior mean of the QoI. We propose computationally efficient algorithms
to determine these optimal approximations. The complexity of our algorithms scales
with the intrinsic dimensionality of the goal-oriented problem‚Äîwhich here reflects the
dimension of the parameter subspace that is simultaneously relevant to the QoI and
informed by the data, as noted above. In particular, the full posterior distribution of
the parameters need not be computed at any stage of the algorithms. This is a key
contrast with [169]. Moreover, we make it possible to handle high-dimensional QoIs
such as those arising from the discretization of a distributed stochastic process. This
class of problems is frequently encountered in applications (see, e.g., Section 4.4).
The ideas and algorithms presented in this chapter are primarily developed in the
linear-Gaussian case. On one hand, their application to goal-oriented linear inverse
problems is of standalone interest [169]. On the other hand, in the context of high-
dimensional nonlinear Bayesian inverse problems, the Gaussian approximation is often
the only tractable approximation of the posterior distribution [40, 135]. For example,
77

in [135], a linearization of the parameter-to-observable map and a linearization of the
parameter-to-prediction map are performed for a high-dimensional ice sheet model,
reducing the nonlinear inverse problem to a goal-oriented linear Gaussian inverse
problem. Moreover, the rigorous analysis of dimensionality reduction ideas in linear
inverse problems often leads to computationally efficient dimensionality reduction
strategies for nonlinear inverse problems (see, e.g., [261, 74] or [169, 170]).
The remainder of this chapter is organized as follows. In Section 4.2 we introduce
statistically optimal approximations of the posterior statistics of the QoI. Section
4.3 contains a simple proof-of-concept example, while in Section 4.4 we illustrate
the theory using a more realistic inverse problem in heat transfer. Section 4.5 offers
some concluding remarks. Appendix D contains the proofs of the main results. The
material presented in this chapter can be also found in [260].
4.2
Theory
In this section we introduce optimal approximations of the posterior mean ùúáùëç|ùëå(ùëå)
and posterior covariance Œìùëç|ùëåof the QoI. Section 4.2.1 focuses on the approximation
of Œìùëç|ùëå, while the posterior mean approximation is addressed in Section 4.2.2. The
main results of this section are Theorem 4.1 and Theorem 4.2.
4.2.1
Approximation of the posterior covariance of the QoI
We first focus on approximating Œìùëç|ùëå. The cost of computing Œìùëç|ùëåaccording to (4.4)
is dominated by the solution of ùëùlinear systems with coefficient matrix Œì‚àí1
pos in order
to determine Œìpos ùí™‚ä§. In large-scale inverse problems only the action of the precision
matrix Œì‚àí1
pos on a vector is usually available; it is not reasonable to expect to have direct
factorizations of Œì‚àí1
pos, such as Cholesky decomposition. Thus, the solution of linear
systems with coefficient matrix Œì‚àí1
pos is often necessarily iterative (e.g., via Krylov
subspace methods for SPD matrices [128, 110]).
Moreover, the storage requirements
for Œìùëç|ùëåscale as ùëÇ(ùëù2). If the dimension of the QoI is inherently low, e.g., ùëù= ùëÇ(1),
then the use of direct formulas like (4.4) can be remarkably efficient. For instance, if
78

we are only interested in the average of ùëã, i.e., ùëç:= 1
ùëõ
‚àëÔ∏Ä
ùëñùëãùëñ, then the QoI is only
one-dimensional and computing the posterior covariance of the QoI amounts to solving
essentially a single linear system. As the dimension of the QoI increases, however,
direct formulas like (4.4) become increasingly impractical due to high computational
complexity and storage requirements. In many cases of interest, the dimension of the
QoI can be arbitrarily large. Consider the following simple example. If ùëãrepresents
a finite-dimensional approximation of a spatially distributed stochastic process (e.g.,
an unknown temperature field), then the QoI could be the restriction of this process to
a domain of interest. In this case, the QoI is also a finite-dimensional approximation
of a spatially distributed process, and its dimension can be arbitrarily high depending
on the chosen level of discretization. (We will revisit this example in Section 4.4.)
There is a clear need for new inference algorithms to efficiently tackle such problems.
Even though direct formulas like (4.4) can be intractable when the QoI is high-
dimensional, we saw in Chapter 3 that essential features of large-scale Bayesian inverse
problems bring additional structure to the Bayesian update: The prior distribution
might encode some kind of smoothness or correlation among the inversion parame-
ters. Observations are typically limited in number, indirect, corrupted by noise, and
related to the inversion parameters by the action of a forward operator that filters
out some information [261, 74]. As a result, data are usually informative, relative to
the prior, only about a low-dimensional subspace of the parameter space. That is,
the important differences between the prior and posterior distributions are confined
to a low-dimensional subspace. This source of low-dimensional structure is key to the
development of efficient Bayesian inversion algorithms [96, 74] and also plays a crucial
role when dealing with goal-oriented problems. The optimal approximation, ÃÇÔ∏ÄŒìpos, of
the posterior covariance of the parameters introduced in Chapter 3 is the starting
point for our analysis of goal-oriented inverse problems.
A na√Øve approximation
In this section we introduce an intuitive but suboptimal approximation of Œìùëç|ùëå,
which will provide insight and motivation for the structure of the forthcoming optimal
79

approximation. The reader interested exclusively in the optimal approximation may
jump directly to Section 4.2.1.
The combination of Theorem 3.1 with the direct formulas (4.4) suggests a first
approximation strategy for the posterior covariance of the QoI: just replace Œìpos in
(4.4) with the optimal approximation described by Theorem 3.1,
Œìùëç|ùëå‚âàÃÇÔ∏ÄŒìùëç|ùëå:= ùí™ÃÇÔ∏ÄŒìpos ùí™‚ä§= ùí™Œìpr ùí™‚ä§‚àíùí™ùêæùêæ‚ä§ùí™‚ä§,
(4.5)
where the low-rank update ùêæùêæ‚ä§is given by (3.11). Approximation (4.5) is already
a major computational improvement over the direct formulas (4.4). There is no need
to solve ùëùlinear systems; rather, we only need to compute the leading eigenpairs
of (ùêª, Œì‚àí1
pr ) with a matrix-free algorithm. The rank of the update depends on the
dimension of the parameter subspace that is most informed by the data.
Despite these favorable computational properties, the approximation (4.5) is still
not satisfactory as it does not explicitly account for the goal-oriented feature of the
problem: ÃÇÔ∏ÄŒìpos in (4.5) is the optimal approximation of the posterior covariance of
the parameters, but is by no means tailored to the QoI. The pencil (ùêª, Œì‚àí1
pr ) used
to compute the approximation ÃÇÔ∏ÄŒìpos does not include the goal-oriented operator ùí™.
In other words, the directions ( ÃÇÔ∏Äùë§ùëñ) defining the optimal prior-to-posterior update in
(3.11), though strongly data-informed, need not be relevant to the QoI. For instance,
some of the ( ÃÇÔ∏Äùë§ùëñ) could lie in the nullspace of the goal-oriented operator. Comput-
ing these eigenvectors would be an unnecessary waste of computational resources. Of
course, as the rank of the optimal prior-to-posterior update increases, the correspond-
ing approximation ÃÇÔ∏ÄŒìùëç|ùëåwill continue to improve until eventually Œìùëç|ùëå= ÃÇÔ∏ÄŒìùëç|ùëå. In
the worst case scenario, however, ÃÇÔ∏ÄŒìùëç|ùëåwill be a good approximation of Œìùëç|ùëåonly
as we start computing eigenpairs of (ùêª, Œì‚àí1
pr ) associated with the smallest nonzero
generalized eigenvalues. This is clearly unacceptable as the overall complexity of the
approximation algorithm would not depend on the nature of the goal-oriented op-
erator. Therefore, the approximation (4.5) cannot satisfy any reasonable optimality
statement in the spirit of Theorem 3.1 and calls for a proper modification.
80

An optimal approximation
The form of ÃÇÔ∏ÄŒìùëç|ùëåin (4.5) shows that the posterior covariance of the QoI can be written
as a low-rank update of the prior covariance. (Recall that the prior distribution of the
QoI is Gaussian, ùëç‚àºùí©(0, Œìùëç) with Œìùëç= ùí™Œìpr ùí™‚ä§.) This is once again consistent
with our intuition about the Bayesian update: the data will only inform certain
aspects of the QoI. Thus a structure-exploiting approximation class for Œìùëç|ùëåis given
by the set of positive definite matrices that can be written as rank‚Äìùëünegative-definite
updates of Œìùëç:
‚Ñ≥ùëç
ùëü= {Œìùëç‚àíùêæùêæ‚ä§‚âª0 : rank(ùêæ) ‚â§ùëü}.
(4.6)
Before introducing one of the main results of this chapter, we observe that ùëåand
ùëçare related by a linear model similar to (4.1). The following lemma clarifies this
relationship.
Lemma 4.1. A linear Gaussian model consistent with (4.4) is given by:
ùëå= ùê∫ùí™‚Ä† ùëç+ Œî,
(4.7)
where ùí™‚Ä† := Œìprùí™‚ä§Œì‚àí1
ùëç, ùëç‚àºùí©(0, Œìùëç) and Œî ‚àºùí©(0, ŒìŒî) are independent, and
ŒìŒî := Œìobs + ùê∫(Œìpr ‚àíŒìprùí™‚ä§Œì‚àí1
ùëçùí™Œìpr)ùê∫‚ä§.
The results of Chapter 3 can be extended to the goal-oriented case by applying
them to the reduced linear model in (4.7). The following theorem defines the optimal
approximation of Œìùëç|ùëåand is one of the main results of this chapter.
Theorem 4.1 (Optimal approximation of the posterior covariance of the QoI). Let
(ùúÜùëñ, ùëûùëñ) be the eigenpairs of:
(ùê∫Œìpr ùí™‚ä§Œì‚àí1
ùëçùí™Œìpr ùê∫‚ä§, Œìùëå)
(4.8)
with the ordering ùúÜùëñ‚â•ùúÜùëñ+1 > 0 and normalization ùëû‚ä§
ùëñùê∫Œìpr ùí™‚ä§Œì‚àí1
ùëçùí™Œìpr ùê∫‚ä§ùëûùëñ= 1,
where Œìùëå:= Œìobs + ùê∫Œìpr ùê∫‚ä§is the covariance matrix of the marginal distribution
81

of ùëå. Then, a minimizer ÃÉÔ∏ÄŒìùëç|ùëåof the Riemannian metric ùëë‚Ñõbetween Œìùëç|ùëåand an
element of ‚Ñ≥ùëç
ùëüis given by:
ÃÉÔ∏ÄŒìùëç|ùëå= Œìùëç‚àíùêæùêæ‚ä§,
ùêæùêæ‚ä§=
ùëü
‚àëÔ∏Å
ùëñ=1
ùúÜùëñÃÇÔ∏ÄùëûùëñÃÇÔ∏Äùëû‚ä§
ùëñ,
ÃÇÔ∏Äùëûùëñ:= ùí™Œìpr ùê∫‚ä§ùëûùëñ,
(4.9)
where the corresponding minimum distance is:
ùëë2
‚Ñõ(Œìùëç|ùëå, ÃÉÔ∏ÄŒìùëç|ùëå) = 1
2
‚àëÔ∏Å
ùëñ>ùëü
ln2( 1 ‚àíùúÜùëñ).
(4.10)
The optimal approximation in Theorem 4.1 yields the best approximation for any
given rank of the prior-to-posterior update and, most importantly, never requires the
full posterior covariance of the parameters. (This should be contrasted with [169].)
The directions (ùëûùëñ) that define the optimal update are the leading eigenvectors of
(ùê∫Œìpr ùí™‚ä§Œì‚àí1
ùëçùí™Œìpr ùê∫‚ä§, Œìùëå) and stem from a careful balance of all the ingredients
of the goal-oriented inverse problem: the forward model, measurement noise, prior
information, and ultimate goals. Incorporating ultimate goals reduces the intrinsic
dimensionality of the inverse problem: for any fixed approximation error, the rank
of the optimal update (4.9) can only be less than or equal to that of the suboptimal
approximation introduced in (4.5).
Computational remarks
If square roots of Œìpr and Œìobs are available, such that Œìpr = ùëÜpr ùëÜ‚ä§
pr and Œìobs =
ùëÜobs ùëÜ‚ä§
obs, then we can rewrite the pencil (4.8) in a more concise form as follows.
Corollary 4.1. Let ÃÇÔ∏Äùê∫:= ùëÜ‚àí1
obs ùê∫ùëÜpr, and let Œ† be an orthogonal projector onto the
range of ùëÜ‚ä§
pr ùí™‚ä§. Then the eigenvalues of
( ÃÇÔ∏Äùê∫Œ† ÃÇÔ∏Äùê∫‚ä§, ùêº+ ÃÇÔ∏Äùê∫ÃÇÔ∏Äùê∫‚ä§)
(4.11)
are the same as those of (4.8), and the eigenvectors of (4.11) can be mapped to the
eigenvectors of (4.8) with the transformation ùë§‚Ü¶‚ÜíùëÜ‚àí‚ä§
obs ùë§.
82

The proof of the corollary is straightforward once we note that Œ† can be written
as Œ† = ùëÜ‚ä§
pr ùí™‚ä§(ùí™ùëÜprùëÜ‚ä§
prùí™‚ä§)‚àí1 ùí™ùëÜpr = ùëÜ‚ä§
pr ùí™‚ä§Œì‚àí1
ùëçùí™ùëÜpr. Moreover, the action of the
projector Œ† on a vector ùë£can be computed efficiently since Œ†(ùë£) := ùëÜ‚ä§
pr ùí™‚ä§ùë•ls, where
ùë•ls is the least squares solution of the overdetermined linear system ùëÜ‚ä§
pr ùí™‚ä§ùë•ls = ùë£.
There is a variety of techniques for the solution of large-scale matrix-free least squares
problems (e.g., [208, 98, 54, 128, 190]).
We now focus our computational remarks on the analysis of the pencil in (4.11),
which is well suited for practical implementations of the approximation. To simplify
notation, let us rewrite (4.11) as (ùê¥, ùêµ), where ùê¥:= ÃÇÔ∏Äùê∫Œ† ÃÇÔ∏Äùê∫‚ä§and ùêµ:= ùêº+ ÃÇÔ∏Äùê∫ÃÇÔ∏Äùê∫‚ä§.
Finding the leading generalized eigenpairs of (4.11) requires the solution of a Her-
mitian generalized eigenvalue problem [13]. Unfortunately, it is not easy to reduce
(4.11) to a standard eigenvalue problem,1 as doing so would require the action of a
square root of ùêµor of ùêµ‚àí1. Nevertheless, there are a plethora of matrix-free algo-
rithms for large-scale generalized eigenvalue problems: generalized Lanczos iteration
[13, Section 5.5], randomized SVD‚Äìtype methods [241], manifold optimization algo-
rithms [1, 2, 14], the trace minimization algorithm [243, 153], and the inverse‚Äìfree
preconditioned Krylov subspace method [112], to name a few. These algorithms re-
quire the iterative solution of linear systems associated with ùêµ, in some cases to low
accuracy [243, 112]. Applying ùêµto a vector requires the evaluation of the forward
model, which may or may not be expensive (e.g., consider PDE based models [96]
versus image deblurring problems [145]). In practice, for a fixed dimension of the de-
sired eigenspace, algorithms for characterizing the eigenpairs of (ùê¥, ùêµ) lead to more
expensive computations than those for the pencil (ùêª, Œì‚àí1
pr ) used in the na√Øve approx-
imation of Section 4.2.1. However, the key point is that the optimal approximation
of Theorem 4.1 requires the characterization of lower dimensional eigenspaces for a
given accuracy.
Moreover, if we solve the generalized eigenvalue problem using a block Lanczos
iteration or a randomized method, then we can also exploit block Krylov methods to
1In contrast, this is often possible in the non-goal-oriented case when dealing with the pencil
(ùêª, Œì‚àí1
pr ), as the action of a square root of Œì‚àí1
pr , or of its inverse, is available in many cases of interest
(e.g., [174]).
83

solve the associated linear systems‚Äîcomprised of ùêµand multiple right-hand sides‚Äî
simultaneously [200, 240]. In particular, the convergence of Krylov methods for solv-
ing linear systems of the form ùêµùë•= ùëè, such as the conjugate gradient algorithm,
depends not only on the spectrum of ùêµbut also on the right-hand side ùëè[172, 12].
This dependence is especially important when the right-hand side has some structure
and is not entirely random: in our case, ùëèlies in the range of the possibly low-rank
operator ùê¥.
For instance, if ùëèis mostly contained in a low-dimensional invariant
subspace of ùêµ(whether associated with small or large eigenvalues), then the Krylov
solver will likely converge to an accurate solution in few steps. Conversely, it should
be noted that if the range of the operator Œ† in (4.11)‚Äîessentially the subspace of the
parameter space that is relevant to the QoI‚Äîhas non-negligible components along
every data-informed parameter direction (corresponding to the leading eigenspace of
(ùêª, Œì‚àí1
pr )), then it would be difficult to obtain an accurate approximation of Œìùëç|ùëå
without exploring the full data-informed subspace.
Even though the na√Øve approximation is suboptimal, it may be interesting from
a practical standpoint to assess its performance, since it is cheaper to compute for
a given approximation rank. The following lemma provides useful guidelines in this
direction.
Lemma 4.2 (Relationship between approximations). Let ÃÉÔ∏ÄŒìùëç|ùëå, ÃÇÔ∏ÄŒìùëç|ùëå‚àà‚Ñ≥ùëç
ùëübe,
respectively, the optimal and suboptimal approximations of Œìùëç|ùëåintroduced in (4.9)
and (4.5). Moreover, let ÃÇÔ∏ÄŒìpos ‚àà‚Ñ≥ùëübe the optimal approximation of Œìpos defined in
(3.11). Then
ùëë‚Ñõ( Œìùëç|ùëå, ÃÉÔ∏ÄŒìùëç|ùëå) ‚â§ùëë‚Ñõ( Œìùëç|ùëå, ÃÇÔ∏ÄŒìùëç|ùëå) ‚â§ùëë‚Ñõ( Œìpos , ÃÇÔ∏ÄŒìpos ).
(4.12)
Lemma 4.2 has several interesting consequences.
First of all, notice that it is
possible to bound the accuracy of the na√Øve approximation, ÃÇÔ∏ÄŒìùëç|ùëå= ùí™ÃÇÔ∏ÄŒìpos ùí™‚ä§, using
ùëë‚Ñõ( Œìpos , ÃÇÔ∏ÄŒìpos ). The latter distance can easily be bounded as a function of the gener-
alized eigenvalues of (ùêª, Œì‚àí1
pr ), as shown in (3.12). These are precisely the eigenvalues
computed by the na√Øve approximation. Thus, if the eigenvalues of (ùêª, Œì‚àí1
pr ) decay
84

sharply or, equivalently, if the distance ùëë‚Ñõ( Œìpos , ÃÇÔ∏ÄŒìpos ) can be made small with only
a low-dimensional (small ùëü) prior-to-posterior update, then Lemma 4.2 says that the
na√Øve approximation, albeit suboptimal, can yield a remarkably efficient approxima-
tion of Œìùëç|ùëå‚Äîwith strong accuracy guarantees in terms of ùëë‚Ñõ( Œìpos , ÃÇÔ∏ÄŒìpos). Intuitively,
if ÃÇÔ∏ÄŒìùëç|ùëåalready accounts for most of the data-informed directions in the parameter
space, then there is no major loss of accuracy in neglecting further directions of the
prior-to-posterior update, even if these directions are relevant to the QoI. In this sit-
uation, these additional directions would provide very limited information relative to
the prior and can be safely neglected.
On the other hand, if the eigenvalues of (ùêª, Œì‚àí1
pr ) do not decay as quickly, then
the bound provided in (4.12) becomes useless. This is not to say that the na√Øve ap-
proximation will necessarily perform poorly. (It is possible that the QoI depends only
on a few of the leading data-informed directions, such that the na√Øve approximation
performs well even for a low rank prior-to-posterior update.) But we cannot quantify
the accuracy of the na√Øve approximation unless we directly compute ùëë‚Ñõ(Œìùëç|ùëå, ÃÇÔ∏ÄŒìùëç|ùëå),
which in turn requires the solution of an expensive generalized eigenvalue problem
for each rank of the update. This is not feasible in practice. In such situations, we
should resort to the optimal approximation introduced in Theorem 4.1, which offers
a useful error bound as well as a concrete possibility for both computational and
storage savings.
Properties of the optimal covariance approximation
An important consequence of the optimal approximation of Œìùëç|ùëåwith respect to
the metric ùëë‚Ñõis optimality in distribution whenever the posterior mean of the QoI
is known. It follows from Lemma 3.1 that the minimizer of the Hellinger distance
(or the Kullback‚ÄìLeibler divergence) between the Gaussian posterior measure of the
QoI, ùúàùëç|ùëå:= ùí©(ùúáùëç|ùëå(ùëå), Œìùëç|ùëå), and the approximation ùí©(ùúáùëç|ùëå(ùëå), Œì) for a matrix
Œì ‚àà‚Ñ≥ùëç
ùëü, is given by the optimal approximation (4.9) defined in Theorem 4.1. In
particular, let Àúùúàùëç|ùëå:= ùí©(ùúáùëç|ùëå(ùëå), ÃÉÔ∏ÄŒìùëç|ùëå) be the measure that optimally approxi-
mates ùúàùëç|ùëå, where ÃÉÔ∏ÄŒìùëç|ùëåis defined in (4.9). Then it is easy to show that the Hellinger
85

distance between ùúàùëç|ùëåand the optimal approximation Àúùúàùëç|ùëåis given by:
ùëëHell(ùúàùëç|ùëå, Àúùúàùëç|ùëå) =
‚àöÔ∏É
1 ‚àí
‚àèÔ∏Å
ùëñ>ùëü
21/2(1 ‚àíùúÜùëñ)1/4
(2 ‚àíùúÜùëñ)1/2
(4.13)
where (ùúÜùëñ) are the generalized eigenvalues defined in Theorem 4.1 (e.g., [261, Appendix
A] or [209]).
The Hellinger distance can be used to bound the error of expectations of functions
of interest with respect to approximate measures [76].
That is, suppose that we
are interested in the posterior expectation Eùúàùëç|ùëå[ùëî] of some measurable function ùëî:
Rùëù‚ÜíR, with certain bounded moments with respect to the prior measure ùúàùëç:=
ùí©(0, Œìùëç), and suppose further that we can only evaluate integrals with respect to
the approximate measure Àúùúàùëç|ùëå. Then the error resulting from computing EÀúùúàùëç|ùëå[ùëî], as
opposed to Eùúàùëç|ùëå[ùëî], for a fixed realization of the data ùëå, can be bounded in terms of
the Hellinger distance between the two Gaussian measures using the following lemma,
which follows easily from [76, Lemma 7.14].
Lemma 4.3 (Convergence in expectation). Let ùëî: Rùëù‚ÜíR be a measurable function
with ùõΩ> 2 bounded moments with respect to the prior measure, i.e., Eùúàùëç[ |ùëî|ùõΩ] < ‚àû.
Then:
‚Éí‚Éí‚ÉíEùúàùëç|ùëå[ùëî] ‚àíEÀúùúàùëç|ùëå[ùëî]
‚Éí‚Éí‚Éí‚â§ùíû(ùëå, ùëî) ùëëHell(ùúàùëç|ùëå, Àúùúàùëç|ùëå),
(4.14)
where ùíû(ùëå, ùëî) := 2
‚àö
2
|Œìùëç|1/4
|Œìùëç|ùëå|1/4 exp
(Ô∏Å
1
2(ùõΩ‚àí2) ‚Äñùúáùëç|ùëå(ùëå)‚Äñ2
Œì‚àí1
ùëç
)Ô∏Å
Eùúàùëç[ |ùëî|ùõΩ]1/ùõΩand where
|ùê¥| denotes the determinant of the matrix ùê¥.
Notice that the constant ùíû(ùëå, ùëî) in (4.14) is independent of the approximating
measure Àúùúàùëç|ùëå. Convergence of the approximation in Hellinger distance thus implies
convergence of the expectation EÀúùúàùëç|ùëå[ùëî] to Eùúàùëç|ùëå[ùëî].
It is interesting to note that the optimal approximation of the posterior covari-
ance matrix of the QoI in Theorem 4.1 is always associated with a corresponding
approximation of the posterior covariance of the parameters, Œìpos.
The following
result clarifies the nature of this approximation.
86

Lemma 4.4 (Goal-oriented approximation of Œìpos). Let ÃÉÔ∏ÄŒìùëç|ùëåbe the minimizer of the
metric ùëë‚Ñõbetween Œìùëç|ùëåand an element of ‚Ñ≥ùëç
ùëüas given by (4.9). Then ÃÉÔ∏ÄŒìùëç|ùëåcan be
written as
ÃÉÔ∏ÄŒìùëç|ùëå= ùí™ÃÇÔ∏ÄŒì*
pos ùí™‚ä§,
ÃÇÔ∏ÄŒì*
pos = Œìpr ‚àí
ùëü
‚àëÔ∏Å
ùëñ=1
ùúÜùëñÃÉÔ∏ÄùëûùëñÃÉÔ∏Äùëû‚ä§
ùëñ,
ÃÉÔ∏Äùëûùëñ:= ùëÜpr Œ† ùëÜ‚ä§
pr ùê∫‚ä§ùëûùëñ,
(4.15)
where the vectors (ùëûùëñ) are defined in Theorem 4.1, ùëÜpr is a square root of the prior
covariance matrix such that Œìpr = ùëÜpr ùëÜ‚ä§
pr, Œ† is the orthogonal projector onto the
range of ùëÜ‚ä§
pr ùí™‚ä§, while ÃÇÔ∏ÄŒì*
pos satisfies
ÃÇÔ∏ÄŒì*
pos
‚àà
arg min
Œì ùëë‚Ñõ( Œìùëç|ùëå, ùí™Œì ùí™‚ä§)
(4.16)
s.t.
Œì ‚àà‚Ñ≥ùëü:= {Œì = Œìpr ‚àíùêæùêæ‚ä§‚âª0, rank(ùêæ) ‚â§ùëü}.
The matrix ÃÇÔ∏ÄŒì*
pos is an optimal goal-oriented approximation of Œìpos. This notion
of optimality is quite different from that in Theorem 3.1.
The prior-to-posterior
update directions, (ÃÉÔ∏Äùëûùëñ) in (4.15), have the intuitive interpretation of directions, in the
parameter space, that are most informed by the data, relative to the prior, and that
are relevant to the QoI. In particular, it is easy to see that the (ÃÉÔ∏Äùëûùëñ) are orthogonal to
the nullspace of the goal-oriented operator with respect to the inner product induced
by the prior precision, i.e.,
‚Ñé‚ä§Œì‚àí1
pr ÃÉÔ∏Äùëûùëñ= (ùí™‚Ñé)‚ä§Œì‚àí1
ùëçùí™Œìprùê∫‚ä§ùëûùëñ= 0,
‚àÄ‚Ñé‚ààNull(ùí™).
(4.17)
Note that even if ÃÉÔ∏ÄŒìùëç|ùëå= ùí™ÃÇÔ∏ÄŒì*
pos ùí™‚ä§is a good approximation of Œìùëç|ùëå, ÃÇÔ∏ÄŒì*
pos need not
be a good approximation of Œìpos.
Now we introduce a particularly simple factorization of the optimal approximation
ÃÉÔ∏ÄŒìùëç|ùëåfrom Theorem 4.1, as ÃÉÔ∏ÄŒìùëç|ùëå= ÃÉÔ∏ÄùëÜùëç|ùëåÃÉÔ∏ÄùëÜ‚ä§
ùëç|ùëåfor some matrix ÃÉÔ∏ÄùëÜùëç|ùëå. We can think
of ÃÉÔ∏ÄùëÜùëç|ùëåas a square root of ÃÉÔ∏ÄŒìùëç|ùëå, even though ÃÉÔ∏ÄùëÜùëç|ùëåneed not be a square matrix.
Obtaining the action of a square root of ÃÉÔ∏ÄŒìùëç|ùëåon a vector is an essential task if
our goal is to sample the distribution ùí©(ùúáùëç|ùëå(ùëå), ÃÉÔ∏ÄŒìùëç|ùëå) in truly high-dimensional
problems. The key requirement is that ÃÉÔ∏ÄùëÜùëç|ùëåbe easy to compute once we have the
87

optimal approximation ÃÉÔ∏ÄŒìùëç|ùëå. We have deferred the discussion of this topic until now
in order to exploit the results of Lemma 4.4 to obtain an explicit characterization of
ÃÉÔ∏ÄùëÜùëç|ùëå.
Lemma 4.5. Let (ùúÜùëñ, ùëûùëñ), ùëÜpr, and Œ† be defined as in Lemma 4.4. Then, a non-
symmetric square root, ÃÉÔ∏ÄùëÜùëç|ùëå, of ÃÉÔ∏ÄŒìùëç|ùëå, such that ÃÉÔ∏ÄŒìùëç|ùëå= ÃÉÔ∏ÄùëÜùëç|ùëåÃÉÔ∏ÄùëÜ‚ä§
ùëç|ùëå, is given by
ÃÉÔ∏ÄùëÜùëç|ùëå= ùí™ùëÜpr
(Ô∏É
ùëü
‚àëÔ∏Å
ùëñ=1
(
‚àöÔ∏Ä
1 ‚àíùúÜùëñ‚àí1) ¬Øùëûùëñ¬Øùëû‚ä§
ùëñ+ ùêº
)Ô∏É
,
¬Øùëûùëñ:= Œ† ùëÜ‚ä§
pr ùê∫‚ä§ùëûùëñ,
(4.18)
where ùêºis the identity matrix.
The virtue of this result is that it does not require an invertible square root of
Œìùëç= ùí™Œìpr ùí™‚ä§as one would expect from [261, Remark 2].
Note that it is easy
to apply ÃÉÔ∏ÄùëÜùëç|ùëåto a vector, which allows efficient sampling from ùí©(ùúáùëç|ùëå(ùëå), ÃÉÔ∏ÄŒìùëç|ùëå).
An interesting feature of ÃÉÔ∏ÄùëÜùëç|ùëå‚ààRùëù√óùëõis that it is a nonsquare matrix with ùëù< ùëõ.
This is certainly not an issue as long as ÃÉÔ∏ÄùëÜùëç|ùëåÃÉÔ∏ÄùëÜ‚ä§
ùëç|ùëå= ÃÉÔ∏ÄŒìùëç|ùëå. Notice also that (4.18)
contains a square root of the prior covariance matrix. The action of this matrix is
usually available in large-scale applications (e.g., [275, 85, 174, 265, 281]). However,
if the action of a square root of Œìpr is truly unavailable, then one can still sample
from ùí©(ùúáùëç|ùëå(ùëå), ÃÉÔ∏ÄŒìùëç|ùëå) by resorting to the action of the matrix ÃÉÔ∏ÄŒìùëç|ùëåalone (e.g.,
[59, 100, 211, 248]). It is straightforward to apply ÃÉÔ∏ÄŒìùëç|ùëåto a vector (see (4.9)).
4.2.2
Approximation of the posterior mean of the QoI
We conclude this theory section by introducing an optimal approximation of the
posterior mean of the QoI. The cost of computing
ùúáùëç|ùëå(ùëå) := ùí™ùúápos(ùëå) = ùí™Œìpos ùê∫‚ä§Œì‚àí1
obs ùëå
(4.19)
for a single realization of the data is usually dominated by the cost of solving a single
linear system associated with Œì‚àí1
pos to determine ùúápos(ùëå). This task can be efficiently
tackled with state-of-the-art matrix-free iterative solvers for symmetric linear systems
88

(e.g., [13, 128, 3]). If, however, one is interested in the fast computation of ùúáùëç|ùëå(ùëå) for
multiple realizations of the data, as we were in Chapter 3, then the situation is quite
different. Solving a linear system to compute ùúáùëç|ùëå(ùëå) each time a new measurement
is available might be infeasible in practical applications. If the dimension of the QoI
is small, say ùëù= ùëÇ(1), then there is an easy solution to this problem. One can just
precompute the matrix ùëÄ:= ùí™Œìpos ùê∫‚ä§Œì‚àí1
obs in an offline stage and then compute the
posterior mean of the QoI as ùúáùëç|ùëå(ùëå) = ùëÄùëåeach time a new realization of the data
becomes available. Yet the computational efficiency of this procedure breaks down
as the dimension of the QoI increases‚Äîfor instance, if the QoI is a finite-dimensional
approximation of some underlying function. In this case, the matrix ùëÄwould be large
and dense, and storing it could be quite inefficient. Moreover, performing a dense
matrix-vector product to compute ùúáùëç|ùëå(ùëå) = ùëÄùëåmight become more expensive
than solving, a single linear system associated with Œì‚àí1
pos.
Our goal is thus to characterize computationally efficient and statistically optimal
approximations of ùúáùëç|ùëå(ùëå) as a low-rank linear function of the data, i.e., ùúáùëç|ùëå(ùëå) ‚âà
ÃÉÔ∏Äùúáùëç|ùëå(ùëå) := ùê¥ùëåfor some low-rank matrix ùê¥. With such an ùê¥, computing ÃÉÔ∏Äùúáùëç|ùëå(ùëå)
for each new realization of the data would be computationally efficient. We define
optimality of the approximation with respect to the Bayes risk for squared-error loss
weighted by the posterior precision matrix of the QoI, i.e.,
‚Ñ¨(ùê¥) := E
[Ô∏Ç
‚Äñùê¥ùëå‚àíùëç‚Äñ2
Œì‚àí1
ùëç|ùëå
]Ô∏Ç
,
(4.20)
where ‚Ñ¨(ùê¥) denotes the Bayes risk associated with the matrix ùê¥, and where the
expectation is taken over the joint distribution of ùëçand ùëå. Minimizing the Bayes
risk (4.20) is equivalent to minimizing
E
[Ô∏Ç
‚Äñùúáùëç|ùëå(ùëå) ‚àíÃÉÔ∏Äùúáùëç|ùëå(ùëå)‚Äñ2
Œì‚àí1
ùëç|ùëå
]Ô∏Ç
(4.21)
over all approximations of the posterior mean of the form ÃÉÔ∏Äùúáùëç|ùëå(ùëå) = ùê¥ùëåfor some
low-rank matrix ùê¥. The Mahalanobis distance in (4.21) is precisely a Riemannian
metric of the form described in Section 3.2.2 and thus it is a natural way to assess
89

the quality of a posterior mean approximation. Notice that (4.21) is an average of
the squared Riemannian distance between ùúáùëç|ùëå(ùëå) and its approximation ÃÉÔ∏Äùúáùëç|ùëå(ùëå)
over the distribution of the data ùëå.
The following theorem characterizes the optimal approximation of ùúáùëç|ùëå(ùëå).
Theorem 4.2 (Optimal approximation of ùúáùëç|ùëå(ùëå)). Let (ùúÜùëñ, ùëûùëñ, ÃÇÔ∏Äùëûùëñ) be defined as in
Theorem 4.1 and consider the minimization of the following Bayes risk over the set
of low-rank matrices:
min
ùê¥
E
[Ô∏Ç
‚Äñùê¥ùëå‚àíùëç‚Äñ2
Œì‚àí1
ùëç|ùëå
]Ô∏Ç
,
s.t.
rank(ùê¥) ‚â§ùëü.
(4.22)
Then a minimizer of (4.22) is given by:
ùê¥* =
ùëü
‚àëÔ∏Å
ùëñ=1
ùúÜùëñÃÇÔ∏Äùëûùëñùëû‚ä§
ùëñ,
(4.23)
with minimum Bayes risk:
‚Ñ¨(ùê¥*) = E
[Ô∏Ç
‚Äñ ùê¥* ùëå‚àíùëç‚Äñ2
Œì‚àí1
ùëç|ùëå
]Ô∏Ç
=
‚àëÔ∏Å
ùëñ>ùëü
ùúÜùëñ
1 ‚àíùúÜùëñ
+ ‚Ñì,
(4.24)
where ‚Ñìis the dimension of the parameter space.
Note that (4.23) can be computed ‚Äúfor free‚Äù from the optimal approximation of
Œìùëç|ùëåintroduced in Theorem 4.1. Also, the optimal approximations of both the pos-
terior mean and the posterior covariance of the QoI become quite accurate as soon as
we start including generalized eigenvalues ùúÜ‚â™1 in the corresponding approximations
(see minimum loss (4.10) and Bayes risk (4.24)). This is perfectly consistent with our
previous analysis of Chapter 3.
4.3
Proof-of-concept example
Before investigating the numerical performance of our goal-oriented approximations,
we illustrate the theory with a simple proof-of-concept example.
We consider an
90

identity forward model ùê∫= ùêº, a diagonal observational noise precision Œì‚àí1
obs =
diag(‚Ñé1, . . . , ‚Ñéùëõ), and a diagonal prior covariance Œìpr = diag(ùúá1, . . . , ùúáùëõ), with ‚Ñéùëñ=
ùëõ‚àíùëñand ùúáùëñ= ùëñfor ùëñ= 1, . . . , ùëõ. We may think of this problem as denoising a signal
ùëã[261]. Figure 4-1 shows the normalized eigenvalues of Œì‚àí1
obs and Œìpr in blue and red,
respectively, for the case ùëõ= 30. The eigenvectors of both matrices correspond to the
canonical vectors in Rùëõ, i.e., ùëí1, . . . , ùëíùëõ. In this case, the data are most informative‚Äî
in absolute terms‚Äîalong directions ùëíùëñwith ùëñ‚â™ùëõ, since the observational noise
precision ‚Ñéùëñis a decreasing function of ùëñ. On the other hand, the prior variance is
large along ùëíùëñwhen ùëñ‚â´1, since ùúáùëñis an increasing function of ùëñ. Thus the prior is
more constraining where the data are more informative. The eigenpairs (ùõø2
ùëñ, ÃÇÔ∏Äùë§ùëñ) of
the pencil (ùêª, Œì‚àí1
pr ), defined in Theorem 3.1, are given by ùõø2
ùëñ= ‚Ñéùëñ¬∑ ùúáùëñ= (ùëõ‚àíùëñ) ¬∑ ùëñand
ÃÇÔ∏Äùë§ùëñ‚àùùëíùëñfor ùëñ= 1, . . . , ùëõ. (These ùõø2
ùëñare not sorted in decreasing order; for simplicity,
we retain the same index ùëñas in the problem definition.) From the relative magni-
tudes of (ùõø2
ùëñ)‚Äîillustrated by the green parabola in Figure 4-1‚Äîwe can identify the
parameter directions that are most informed by the data relative to the prior: they
correspond to ùëíùëñwith ùëñaround ùëõ/2 (the middle of the spectrum). These directions
define the optimal prior-to-posterior update of Theorem 3.1. Modes ùëíùëñwith ùëñ‚â™ùëõ/2
are strongly informed by the data in an absolute sense, but not relative to the prior;
thus their overall importance is limited. In the same way, modes ùëíùëñwith ùëñ‚â´ùëõ/2
are unimportant to the update since the posterior variance along these directions is
roughly equal to the prior variance (ùõø2
ùëñ‚â™1), even though both variances are relatively
large [261].
Now let the goal-oriented operator ùí™: Rùëõ‚ÜíRùëùbe defined as follows: ùí™ùë•=
(ùë•1, . . . , ùë•ùëù) for ùë•= (ùë•1, . . . , ùë•ùëõ) and ùëù= ùëõ/2. Simple algebra shows that the goal-
oriented eigenpairs (ùúÜùëñ, ùëûùëñ) of Theorem 4.1 are given by ùëûùëñ‚àùùëíùëñfor ùëñ= 1, . . . , ùëõ, and
ùúÜùëñ= 1/(1 + 1/(‚Ñéùëñùúáùëñ)) for ùëñ‚â§ùëùand ùúÜùëñ= 0 for ùëñ> ùëù. So that the eigenvalues ùõø2
ùëñ
and ùúÜùëñare comparable in terms of their associated covariance approximation errors‚Äî
see (3.12) and (4.10)‚Äîwe plot a nonlinear function of each ùúÜùëñin Figure 4-1, namely
^ùúÜùëñ= ùëì(ùúÜùëñ) for ùëì(ùë•) = 1/(1 ‚àíùë•). (Since ùëìis strictly increasing on [0, 1), the relative
importance of the (^ùúÜùëñ) is the same as that of the original (ùúÜùëñ).) The introduction of a
91

goal-oriented operator reveals directions that can be strongly informed by the data,
relative to the prior, but that are irrelevant to the QoI. These modes correspond to
(ùëíùëñ) for ùëñ> ùëù, and can be safely neglected when computing the Bayesian update
relevant to the QoI.
Of course, in the general case of non-diagonal operators (ùê∫, Œìobs, Œìpr), the direc-
tions ( ÃÇÔ∏Äùë§ùëñ) and (ùëûùëñ) need not coincide. The following numerical example will illustrate
this general situation.
index eigenvalue
0
5
10
15
20
25
30
normalized eigenvalues
0
0.2
0.4
0.6
0.8
1
Figure 4-1:
Normalized eigenvalues defined in the proof-of-concept example of Sec-
tion 4.3: in blue we show (‚Ñéùëñ/‚Ñémax), in red (ùúáùëñ/ùúámax), in green (ùõø2
ùëñ/ùõø2
max), and in
magenta (^ùúÜùëñ/^ùúÜmax), for ùëñ= 1, . . . , ùëõand ùëõ= 30. For any finite collection of eigenval-
ues (ùúéùëñ), we define ùúémax to be the maximum value over that collection. Since for ùëñ‚â§ùëù
and ùëù= 15, we have ^ùúÜùëñ= ùõø2
ùëñin this example, we shifted the magenta curve slightly
upwards to distinguish it from the green one.
4.4
Numerical examples
Now we numerically illustrate the performance of our approximations using a goal-
oriented inverse problem in heat transfer. In particular, we study the cooling of a
CPU by means of a heat sink. Our goal is to infer the (spatially inhomogeneous)
temperature of the CPU from noisy pointwise observations of temperature on the
heat sink. Figure 4-2 shows the problem setup: the three different layers of material
correspond, respectively, to the CPU (ùíü1), a thin silicone layer that connects the
CPU to the heat sink (ùíü2), and an aluminum fin (ùíü3). We denote by ùíüthe union
92

of these domains. Each ùíüùëñrepresents a two-dimensional cross section of material of
constant width ùëäalong the horizontal direction and a height ùêøùëñ. We assume that
no heat transfer happens along the third dimension; this is a common engineering
approximation [19]. Each material has a constant density ùúåùëñ, a constant specific heat
ùëêùëñand a constant thermal conductivity ùëòùëñ. The corresponding thermal diffusivities
ùõºùëñ= ùëòùëñ/ùúåùëñùëêùëñare shown in the table at the right of Figure 4-2. The time-dependent
temperature field in each domain is Œò(ùëñ) : ùíüùëñ√ó ùëá‚ÜíR, where ùëá= (0, ùë°end], for
ùëñ= 1, 2, 3. Jointly, these temperature fields are simply Œò : ùíü√ó ùëá‚ÜíR.
4.4.1
Forward, observational and prior models
The time evolution of each temperature field Œò(ùëñ) is described by a linear time-
dependent PDE of the form
ùúåùëñùëêùëñùúïùë°Œò(ùëñ) = ‚àá¬∑ (ùëòùëñ‚àáŒò(ùëñ)),
ùëñ= 1, . . . , 3,
(4.25)
where ùúïùë°denotes partial differentiation with respect to time. We assume no volumet-
ric heat production and use Fourier‚Äôs law for the heat flux [113]. Equations (4.25)
should be complemented with appropriate boundary and initial conditions to define
a well-posed forward problem. We use the independent variables ùë†1 and ùë†2 to denote,
respectively, the horizontal and vertical directions and let ùë†= (ùë†1, ùë†2). The point
ùë†= (0, 0) corresponds to the lower left corner of ùíü. At the lower boundary of ùíü1 we
impose a space- and time-dependent heat flux: ùëò1 ùúï‚ÉóùëõŒò(1) = ùëû(ùë†, ùë°) for ùë†‚ààùíü1,bottom,
where ‚Éóùëõrefers to the outward pointing normal and ùëûis a given nonconstant scalar
function in ùë†. At the interface between domains ùíüùëñand ùíüùëñ+1 we assume heat trans-
fer by conduction with no thermal contact resistance: ùëòùëñùúï‚ÉóùëõŒò(ùëñ) = ùëòùëñ+1 ùúï‚ÉóùëõŒò(ùëñ+1) and
Œò(ùëñ) = Œò(ùëñ+1) for ùë†‚ààinterface(ùíüùëñ, ùíüùëñ+1) and ùëñ= 1, 2. At the top, left, and right
boundaries of ùíü3, we assume heat transfer by convection with a fluid at constant
temperature Œò‚àû: ‚àíùëò3 ùúï‚ÉóùëõŒò(3) = ‚Ñéùëê(Œò(3) ‚àíŒò‚àû) for ùë†‚ààùíü3,top ‚à™ùíü3,left ‚à™ùíü3,right, where
‚Ñéùëêis a convective heat transfer coefficient. Finally, we impose adiabatic conditions
(no heat exchange) on the left and right boundaries of ùíü1 and ùíü2: ùúï‚ÉóùëõŒò(ùëñ) = 0 for
93

ùë†‚ààùíüùëñ,left ‚à™ùíüùëñ,right and ùëñ= 1, 2. The initial conditions are not specified here as they
are the objective of the forthcoming inverse problem.
We consider a finite element spatial approximation of the weak form of (4.25) by
means of linear elements on simplices [220]. We denote by Œò‚Ñé(ùë°) ‚ààRùëõthe collection
of temperature values at the finite element nodes on ùíüat time ùë°‚ààùëá. The function
Œò‚Ñésatisfies a system of ODEs of the form ùëÄùúïùë°Œò‚Ñé(ùë°) + ùê¥Œò‚Ñé(ùë°) = ùëì(ùë°), with ùë°‚ààùëá,
for a suitable mass matrix ùëÄ, stiffness matrix ùê¥, known time-dependent forcing term
ùëìand initial conditions Œò0‚Ñé:= Œò‚Ñé(ùë°= 0).
The initial conditions Œò0‚Ñéare unknown and must be estimated from local mea-
surements of the temperature field Œò at different locations in space and time. The
locations of the sensors ùë†1, . . . , ùë†ùëÅare shown as black dots in Figure 4-2. Observa-
tions are collected every Œîùë°time units for ùë°‚ààùëá. The first observation happens at
time ùë°= Œîùë°and we assume that there are ùëÄobservation times in total. We denote
measurements at time ùë°ùëñ= ùëñŒîùë°as ÃÇÔ∏Äùëåùëñ=
[Ô∏Ä
Œò(ùë†1, ùëñŒîùë°), . . . , Œò(ùë†ùëÅ, ùëñŒîùë°)
]Ô∏Ä
. We concatenate
the observations into a vector ÃÇÔ∏Äùëå= ( ÃÇÔ∏Äùëå1, . . . , ÃÇÔ∏ÄùëåùëÄ) ‚ààRùëë. The actual observations are
corrupted with additive Gaussian noise: ùëå= ÃÇÔ∏Äùëå+ ‚Ñ∞, where ‚Ñ∞‚àºùí©(0, ùúé2
obs ùêº) and ùêº
is the identity matrix. Notice that ÃÇÔ∏Äùëåis an affine function of Œò0‚Ñé. This relationship
can be made linear by a suitable redefinition of the data vector. Thus, we are lead to
a linear Gaussian inverse problem in standard form, ùëå= ùê∫Œò0‚Ñé+‚Ñ∞, where ùê∫defines
the forward operator, Œò0‚Ñé‚Ü¶‚ÜíÃÇÔ∏Äùëå, that can be evaluated implicitly by solving a heat
equation with no forcing term and initial conditions Œò0‚Ñéfor a time interval necessary
to collect the corresponding observations ÃÇÔ∏Äùëå.
We define a zero-mean Gaussian prior distribution2 on Œò0‚Ñéby modeling Œò0‚Ñéas a
discretized solution of a stochastic PDE of the form
ùõæ
(Ô∏Ä
ùúÖ2‚Ñê‚àí‚ñ≥
)Ô∏Ä
Œò(ùë†) = ùí≤(ùë†),
ùë†‚ààùíü,
(4.26)
where ùí≤is a white noise process, ùúÖis a positive scalar parameter, ‚ñ≥is the Laplacian
2There is no loss of generality in assuming zero prior mean. If we are given a statistical model of
the form ùëå= ùê∫Œò0‚Ñé+ ‚Ñ∞, where Œò0‚Ñé‚àºùí©(ùúápr, Œìpr) has a nonzero prior mean, then we can trivially
rewrite this model as ÃÇÔ∏Äùëå:= ùëå‚àíùê∫ùúápr = ùê∫(Œò0‚Ñé‚àíùúápr) + ‚Ñ∞for a modified data vector ÃÇÔ∏Äùëåand infer,
equivalently, a zero-prior-mean process Œò0‚Ñé‚àíùúápr ‚àºùí©(0, Œìpr).
94

operator and ‚Ñêis the identity operator. In particular, we exploit the explicit link
between Gaussian Markov random fields with the Mat√©rn covariance function and
solutions to stochastic PDEs as outlined in [174]. In this case, the action of a square
root of the prior covariance matrix on a vector is readily available as the solution of
an elliptic PDE on ùíü, and thus it is scalable to very large inverse problems [174].
In this example we use ‚Ñéùëê= 23.8 W/m2 K for the convective heat transfer co-
efficient between the aluminum fin and the external fluid (air), which has constant
temperature Œò‚àû= 283 K. The width of the domain ùíüin Figure 4-2 is ùêª= 2√ó10‚àí2 m.
The heat flux ùëû(ùë†, ùë°) is time-independent and nonnegative and can be written as the
superposition of two square impulse functions with zero background: one centered at
6 √ó 10‚àí3 m with width 8 √ó 10‚àí3 m and intensity 0.6 W/m2; and the other centered at
15√ó10‚àí3 m with width 4√ó10‚àí3 m and intensity 0.3 W/m2. Observations are collected
every Œîùë°= 5 √ó 10‚àí4 s for a total of ùëÄ= 100 measurements. We use ùúéobs = 1/2 as
the standard deviation of the observational noise. The prior parameters in (4.26) are
given by ùõæ= 1 √ó 104 and ùúÖ=
‚àö
8/ùúåpr with ùúåpr = ùêª/10. This choice of ùúÖdefines a
prior with correlation values near 1/10 at distance ùúåpr [174]. The original prior mean
is set to ùúápr = 318 K. However, we equivalently infer the zero-prior-mean process
Œò0‚Ñé‚àíùúápr as explained in the previous footnote.
4.4.2
Goal-oriented linear inverse problem
We now introduce the goal-oriented feature of the problem. As stated earlier, we are
only interested in the initial temperature distribution over the CPU (i.e., in ùíü1). Let
ùëçbe the restriction of Œò0‚Ñéto the domain of interest ùíü1. Clearly, there is a linear
map between ùëçand Œò0‚Ñé, i.e., ùëç= ùí™Œò0‚Ñéwith ùí™‚ààRùëù√óùëõand ùëù‚â™ùëõ. Thus, we have
a linear-Gaussian goal-oriented inverse problem as introduced in Section 4.2:
‚éß
‚é™
‚é®
‚é™
‚é©
ùëå= ùê∫Œò0‚Ñé+ ‚Ñ∞
ùëç= ùí™Œò0‚Ñé,
(4.27)
95

where both the marginal distribution of Œò0‚Ñéand the likelihood ùëå|Œò0‚Ñéare specified.
(In this example we denote the parameters by Œò0‚Ñérather than ùëã.) We choose a finite
element discretization of the temperature field such that Œò0‚Ñé‚ààR2400 and ùëç‚ààR370.
Our goal is to characterize optimal approximations of the posterior statistics of the
QoI, ùëç|ùëå, for a given set of observations (see Figure 4-3 (left)). In this case, comput-
ing the posterior distribution of the QoI using direct formulas like (4.4) is challenging
as the QoI is a finite-dimensional approximation to a distributed stochastic process,
Œò(0)|ùíü1, and can be arbitrarily high-dimensional depending on the chosen level of
discretization.
The configuration of this problem highlights a crucial aspect of dimensionality
reduction of goal-oriented inverse problems. Ideally we would place the sensors on
ùíü1 since we are interested in inferring the temperature field on the CPU. However,
due to geometrical constraints, we are forced to place our sensors on the heat sink
(ùíü3). As a result, observations are much more informative about the parameters
in ùíü3 than in ùíü1.
We see a hint of this in Figure 4-3 (right), which shows the
normalized difference between the prior and posterior variance of the parameters,
(Var(Œò0‚Ñé) ‚àíVar(Œò0‚Ñé|ùëå))/Var(Œò0‚Ñé). The prior variance is reduced the most around
the sensor locations in ùíü3, which makes intuitive sense as the data are increasingly
less informative as we move away from the sensors.
We first focus on the approximation of the posterior covariance of the QoI. If we use
the suboptimal approximation introduced in (4.5), then we have to pay a considerable
computational price as a result of the data being informative about directions in the
parameter space that are not relevant to the QoI. This issue is illustrated by the
numerical results in Figure 4-6.
To set the stage, we begin with the posterior covariance Œìpos of the parameters
Œò0‚Ñéand construct the optimal approximation ÃÇÔ∏ÄŒìpos = Œìpr ‚àíùêæùêæ‚ä§from Theorem
3.1.
Though this approximation is optimal for any given rank of the update, its
convergence in this problem is rather slow‚Äîas shown by the dotted blue line in Figure
4-6‚Äîbecause there are many data-informed directions in the parameter space. (Notice
the multitude of sensors on the heat sink in Figure 4-2, each yielding observations
96

at ùëÄsuccessive times.) If we now use ÃÇÔ∏ÄŒìpos to yield an approximation of the actual
posterior covariance of interest Œìùëç|ùëåby means of Œìùëç|ùëå‚âàÃÇÔ∏ÄŒìùëç|ùëå= ùí™ÃÇÔ∏ÄŒìpos ùí™‚ä§(i.e.,
the na√Øve approximation of (4.5)), then the convergence of this approximation is still
slow, as seen in green solid line of Figure 4-6. This slow convergence can be easily
explained. The optimal approximation ÃÇÔ∏ÄŒìpos of Œìpos accounts first for those directions
that are most informed by the data.
These directions correspond to modes with
features near the sensors in ùíü3 (see Figure 4-4), but they provide little information
about the parameters in the region of interest (ùíü1).
On the other hand, if we use the optimal approximation of Œìùëç|ùëådefined in The-
orem 4.1, then convergence is remarkably fast, as illustrated via the red solid line
in Figure 4-6. Now we only need to update Œìùëçalong a handful of directions‚Äîsay
twenty‚Äîto achieve a satisfactory approximation of Œìùëç|ùëå. The key to achieving such
fast convergence is to confine the inference to directions in the parameter space that
are most informed by the data, relative to the prior, and that are relevant to the
QoI. Moreover, these fundamental directions can be explicitly extracted from a goal-
oriented approximation of the posterior covariance of the parameters, as explained in
Lemma 4.4; three such directions are shown in Figure 4-5.
We note that Œìùëç|ùëåis by no means a low-rank matrix. (See its spectrum in Figure 4-
8 (left)). This situation is fairly typical when dealing with large-scale inverse problems
with non-smoothing priors (e.g., Gaussian fields with correlation function of Mat√©rn
type) and limited observations. In these situations, seeking an approximation of Œìùëç|ùëå
as a low-rank matrix would be inappropriate; that is, classic dimensionality reduction
techniques, e.g., Karhunen‚ÄìLo√®ve reduction [188, 167], are quite inefficient. Instead,
low-dimensional structure lies in the change from prior to posterior, due to the data
being informative, relative to the prior, only about a low-dimensional subspace of Rùëù.
This fact justifies the choice of the approximation class ‚Ñ≥ùëç
ùëüfor Œìùëç|ùëåin (4.6). The
efficiency of the approximation class ‚Ñ≥ùëç
ùëüis also evident from the sharp decay of the
red curve in Figure 4-6: only a handful of directions in the prior-to-posterior update
are needed for a good approximation of Œìùëç|ùëå.
The optimal approximation of the posterior mean of the QoI as a low-rank linear
97

function of the data, as introduced in Theorem 4.2, also converges very quickly as a
function of the rank of the approximation, as shown in Figure 4-7. Once a low-rank
approximation of the form (4.23) is available, then one can compute an accurate ap-
proximation of ùúáùëç|ùëå(ùëå) for each new realization of the data ùëåby simply performing
a low-rank (ùëü= 20 in this case) matrix-vector product.
4.4.3
A nonlinear QoI
We conclude this section by applying the approximation formulas introduced in this
chapter to a particular case of nonlinear goal-oriented inference. Suppose that we are
only interested in the posterior distribution of the maximum temperature over ùíü1
(see Figure 4-2). This is a useful QoI because the material properties of a sensitive
component (e.g., the CPU) might deteriorate above a certain critical temperature
(e.g., [38]). In this case, the QoI ÃÇÔ∏Äùëç:= maxùíü1 Œò0‚Ñéis a low-dimensional (in fact scalar-
valued) nonlinear function of the parameters. In general, let us write ÃÇÔ∏Äùëç= J (Œò0‚Ñé) for
some nonlinear function J : Rùëõ‚ÜíR. Then we can cast the nonlinear goal-oriented
Bayesian inverse problem as
‚éß
‚é™
‚é®
‚é™
‚é©
ùëå= ùê∫Œò0‚Ñé+ ‚Ñ∞
ÃÇÔ∏Äùëç= J (Œò0‚Ñé)
(4.28)
and try to characterize the posterior ÃÇÔ∏Äùëç|ùëåfor a particular realization of the data.
This problem is nontrivial, however, as ÃÇÔ∏Äùëç|ùëåis non-Gaussian and cannot easily be
characterized by just two moments. In the most general case, one needs to resort
to sampling techniques such as MCMC [129] to characterize ÃÇÔ∏Äùëç|ùëå, or perhaps some
deterministic alternative [196, 246, 83, 197]. Unfortunately, it is still not well under-
stood how to properly adapt these techniques to exploit ultimate goals and bypass
full inference of the parameters (see the offline‚Äìonline strategy of [170] for a related
effort in the context of goal-oriented nonlinear Bayesian inference). Though devel-
oping computationally efficient techniques to tackle general problems like (4.28) is of
fundamental importance, in this particular example we can adopt a much simpler,
98

yet effective, approach. Using the notation of this section, notice that the nonlinear
QoI ÃÇÔ∏Äùëçcan be written as ÃÇÔ∏Äùëç= ùëî(ùëç), where ùëçrepresents the inversion parameters in
the region of interest ùíü1, and where ùëî(ùë•) = maxùëñ(ùë•ùëñ) for all ùë•= (ùë•1, . . . , ùë•ùëù). Thus
we can rewrite (4.28) as:
‚éß
‚é™
‚é™
‚é™
‚é™
‚é™
‚é®
‚é™
‚é™
‚é™
‚é™
‚é™
‚é©
ùëå= ùê∫Œò0‚Ñé+ ‚Ñ∞
ùëç= ùí™Œò0‚Ñé
ÃÇÔ∏Äùëç= ùëî(ùëç).
(4.29)
Then we can approximate the Gaussian posterior distribution of ùëç|ùëåusing the goal-
oriented techniques presented in this chapter, and finally we can push forward the
latter distribution through the nonlinear operator ùëîto obtain a suitable approxima-
tion of ÃÇÔ∏Äùëç|ùëå. The nonlinear operator ùëîis never approximated in this process. That
is, we first compute the posterior mean and the optimal goal-oriented approximation
of the covariance of ùëç|ùëåusing the results of Theorem 4.1, then we sample from
the optimal approximating measure Àúùúàùëç|ùëå:= ùí©(ùúáùëç|ùëå(ùëå), ÃÉÔ∏ÄŒìùëç|ùëå) using the results of
Lemma 4.5, and finally we push forward these samples through ùëî: Rùëù‚ÜíR to ob-
tain approximate samples from the posterior distribution of the nonlinear QoI. We
can easily estimate the quality of these approximate posterior samples using bounds
like (4.14). Note, however, that a bound like (4.14) quantifies only the accuracy of
the posterior moments, and does not yield an explicit measure of distance between
the non-Gaussian posterior distribution of ÃÇÔ∏Äùëç|ùëåand its corresponding approximation.
Nevertheless, the plot on the right of Figure 4-8 shows that the resulting approxima-
tion of the density of ÃÇÔ∏Äùëç|ùëåis indeed quite good for this particular choice of nonlinear
operator ùëî.
4.5
Discussion
In this chapter, we developed statistically optimal and computationally efficient ap-
proximations of the posterior statistics of a quantity of interest (QoI) in a goal‚Äì
oriented linear‚ÄìGaussian inverse problem.
The posterior covariance of the QoI is
99

Œò‚àû
D1
D3
‚Éóq(t)
‚Éóq = 0
Sensors
‚Éóq = 0
D2
Material
ùõºùëñat 20 ‚àòC
Domain
‚Äî
m/s2
‚Äî
Copper
1.11 √ó 10‚àí4
ùíü1
Silicon
8.8 √ó 10‚àí5
ùíü2
Aluminum
8.42 √ó 10‚àí5
ùíü3
Figure 4-2: (left) CPU cooling problem. Inversion for the initial temperature field
on ùíü1 given noisy temperature measurements on an aluminum heat sink (ùíü3). The
figure shows the problem configuration, the locations of the sensors (black dots),
and the boundary conditions for the heat equation describing time evolution of the
temperature field on the domain ùíü:= ùíü1 ‚à™ùíü2 ‚à™ùíü3. (right) Material properties of
the different layers.
290
300
310
320
330
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
Figure 4-3: (left) Initial temperature field used to generate synthetic data according
to the problem configuration described in Section 4.4. This temperature field was
not drawn from the prior distribution of Œò0‚Ñé; instead, it corresponds to a finer dis-
cretization of the continuous stochastic process Œò evaluated at the initial time. (right)
Normalized difference between the prior and posterior variance of the parameters, i.e.,
(Var(Œò0‚Ñé)‚àíVar(Œò0‚Ñé|ùëå))/ Var(Œò0‚Ñé). The regions of greatest relative decrease of the
variance are localized around the sensor locations (black dots).
100

Figure 4-4: Three eigenvectors ( ÃÇÔ∏Äùë§ùëñ) of the matrix pencil (ùêª, Œì‚àí1
pr ) as defined in Theo-
rem 3.1: ÃÇÔ∏Äùë§1 (left), ÃÇÔ∏Äùë§6 (center), and ÃÇÔ∏Äùë§10 (right). These eigenvectors define the prior-
to-posterior update in the optimal approximation (3.11) of the posterior covariance
of the parameters Œìpos. Note that these leading eigenvectors have features near the
locations of the sensors in ùíü3. This is the region where the data are most informative
for the parameters, but not necessarily for the QoI.
Figure 4-5: Three vectors (ÃÉÔ∏Äùëûùëñ) defining the prior-to-posterior update in the optimal
goal-oriented approximation of Œìpos introduced in (4.15) (see Lemma 4.4). In partic-
ular, we show ÃÉÔ∏Äùëû1 (left), ÃÉÔ∏Äùëû3 (center), and ÃÉÔ∏Äùëû5 (right). One can interpret these vectors as
directions in the parameter space that are informed by the data, relative to the prior,
and that are relevant to the QoI. The relevant features of the (ÃÉÔ∏Äùëûùëñ) are concentrated
around the region of interest (ùíü1). These directions should be contrasted with the
modes in Figure 4-4, which are strongly informed by the data but, at the same time,
nearly irrelevant to the QoI.
101

rank of update
0
200
400
600
800
1000
error
10-10
10-5
100
dR(Œìpos, bŒìpos)
dR(ŒìZ|Y, eŒìZ|Y)
dR(ŒìZ|Y, O bŒìposO‚ä§)
rank of update
0
50
100
150
200
error
10-15
10-10
10-5
100
105
dR(Œìpos, bŒìpos)
dR(ŒìZ|Y, eŒìZ|Y)
dR(ŒìZ|Y, O bŒìposO‚ä§)
Figure 4-6:
(left) Convergence of the covariance approximations in the natural
geodesic distance over the manifold of SPD matrices (see Section 3.2.2). The blue
dotted line shows the distance between the covariance of the parameters Œò0‚Ñé|ùëå(i.e.,
Œìpos) and its optimal approximation ÃÇÔ∏ÄŒìpos = Œìpr ‚àíùêæùêæ‚ä§, as a function of the rank of
ùêæ(see Theorem 3.1). The red line shows the distance between Œìùëç|ùëåand its optimal
approximation introduced in Theorem 4.1, ÃÉÔ∏ÄŒìùëç|ùëå= Œìùëç‚àíùêæùêæ‚ä§, as a function of the
rank of ùêæ. Finally, the green line shows the distance between Œìùëç|ùëåand the subop-
timal approximation (4.5) obtained as ùí™ÃÇÔ∏ÄŒìpos ùí™‚ä§. (right) Detail of the figure on the
left, with both axes rescaled.
approximated as a low-rank negative update of the prior covariance of the QoI. Op-
timality holds with respect to the natural geodesic distance on the manifold of sym-
metric positive definite matrices. The posterior mean of the QoI is approximated as
a low-rank function of the data, and optimality follows from the minimization of the
Bayes risk for squared-error loss weighed by the posterior precision matrix of the QoI.
The minimization of this Bayes risk is associated with the minimization of a Rieman-
nian metric averaged over the distribution of the data. These optimal approximations
avoid computation of the full posterior distribution of the parameters and focus only
on directions in the parameter space that are informed by the data and that are rele-
vant to the QoI. These directions are obtained as the leading generalized eigenvectors
of a suitable matrix pencil, and reflect a balance among all the ingredients of the
goal‚Äìoriented inverse problem: prior information, the forward model, measurement
noise, and the ultimate goals.
An important avenue for future work is the extension of these optimality results
to the case of nonlinear forward operators. Here, we expect that interpreting the
102

rank approximation
0
10
20
35
50
75
100
150
error
10-6
10-4
10-2
100
102
 ¬µ  Z |  Y  
Figure 4-7: The solid curve shows the error associated with the optimal low-rank
approximation of the posterior mean of the QoI, ùúáùëç|ùëå(ùëå), given in Theorem 4.2. The
error is measured as the square root of E
[Ô∏Ç‚É¶‚É¶ùúáùëç|ùëå(ùëå) ‚àíùê¥*ùëå
‚É¶‚É¶2
Œì‚àí1
ùëç|ùëå
]Ô∏Ç
and is a function
of rank(ùê¥*). The top right corner shows ùúáùëç|ùëå(ùëå) for a particular realization of ùëå
(see Figure 4-3 (left)). The snapshots along the solid curve show the corresponding
approximation ùúáùëç|ùëå(ùëå) ‚âàùê¥* ùëåfor various ranks of ùê¥* and for the same realization
of ùëå. Notice that the approximation of the posterior mean of the QoI is already good
with rank(ùê¥*) = 20.
103

index i
0
50
100
150
200
250
300
350
eigenvalues
10-1
100
101
102
103
temperature
322
324
326
328
330
332
334
336
pdf
0
0.05
0.1
0.15
0.2
0.25
0.3
0.35
exact
approximate
Figure 4-8:
(left) Eigenvalues of Œìùëç|ùëå. For this problem configuration ùëç‚ààR370, so
the matrix Œìùëç|ùëåis not low-rank. (right) The blue solid curve shows a kernel density
estimate (KDE) of the exact posterior density of the nonlinear QoI ÃÇÔ∏Äùëç:= max(ùëç),
i.e., the density of ÃÇÔ∏Äùëç|ùëå, constructed from 1√ó106 samples. Notice that this density is
non-Gaussian. The red dotted curve shows a KDE constructed with 1 √ó 106 samples
from an approximation of ÃÇÔ∏Äùëç|ùëåobtained as follows: First we sample the approximate
measure Àúùúàùëç|ùëå:= ùí©(ùúáùëç|ùëå(ùëå), ÃÉÔ∏ÄŒìùëç|ùëå) obtained from an optimal approximation, ÃÉÔ∏ÄŒìùëç|ùëå,
of Œìùëç|ùëåas a 20‚Äìdimensional low rank update of Œìùëç(see Theorem 4.1). Then, we push
forward these samples through the nonlinear goal-oriented operator ùëî: Rùëù‚ÜíR. The
quality of the density approximation is already good for a rank‚Äì20 update. These
results are consistent with the theoretical bounds (4.14). (See, in particular, the error
curves in Figure 4-6.)
104

QoI posterior approximation as the result of composing the forward model with a
carefully chosen projection operator, as in [74], may be quite helpful. Relaxing the
Gaussianity assumptions on both the prior distribution and the measurement noise
are also important generalizations of the present work.
Moving forward, we are now ready to investigate a variety of notions of low-
dimensional structure in more general nonlinear non-Gaussian problems. This will
be the subject of the forthcoming Chapter 5 which will undertake a slightly more
general approach and address the characterization of non-Gaussian distributions in
high dimensions, including, of course, the case of posterior distributions arising in the
Bayesian approach to inverse problems.
105

106

Chapter 5
The structure of low-dimensional
couplings
5.1
Introduction
In this chapter we investigate possible sources of low-dimensional structure in non-
linear non-Gaussian Bayesian inverse problems. Our discussion, however, will not be
restricted to inference problems, but will rather address the more general task of inte-
gration with respect to a computationally intractable probability measure‚Äîcertainly
one of the fundamental challenges of modern statistical inference, particularly in the
Bayesian setting. Many measures of interest are in fact computationally intractable,
in the sense that Monte Carlo integration with independent samples and/or deter-
ministic integration via quadrature rules are not feasible.
Couplings‚Äîin particular, deterministic couplings‚Äîare a natural way to address
this challenge. A deterministic coupling between a ‚Äútractable‚Äù measure ùúàùúÇthat we
can easily simulate (e.g., a standard Gaussian) and an arbitrary target measure ùúàùúã
immediately renders the latter tractable. Such a coupling is induced by a map ùëá,
called a transport map, that satisfies ùúàùúÇ( ùëá‚àí1(‚Ñ¨)) = ùúàùúã(‚Ñ¨) for all measurable sets ‚Ñ¨
[271, 31]. The transport map allows any integral
‚à´Ô∏Ä
ùëîdùúàùúãover the target measure to
107

be rewritten as an integral over the reference measure, i.e.,
‚à´Ô∏Å
ùëîdùúàùúã=
‚à´Ô∏Å
ùëî‚àòùëádùúàùúÇ,
and thus enables the use of standard integration techniques for ùúàùúÇto provide an ele-
gant solution to the inference problem [191, 196, 272]. For instance, given a collection
of i.i.d. samples (ùëãùëñ)ùëñfrom the reference measure, we can simply evaluate the trans-
port map to obtain i.i.d. samples (ùëá(ùëãùëñ))ùëñfrom the target. The transport map ùëá
can be viewed as a transformation that moves particles: given a collection of samples
from ùúàùúÇ, ùëárearranges them in accordance with the new distribution ùúàùúã.
In this chapter we focus on absolutely continuous measures (ùúàùúÇ, ùúàùúã) on Rùëõ, for
which the existence of a deterministic coupling, and thus of a transport map ùëá:
Rùëõ‚ÜíRùëõ, is guaranteed [244]. Such a map, however, is seldom unique.
One of the key contributions of this chapter is to establish a link between the con-
ditional independence structure of the reference-target pair‚Äîthe so-called Markov
properties [161] of (ùúàùúÇ, ùúàùúã)‚Äîand the existence of special low-dimensional couplings.
These couplings are induced by transport maps that are (1) sparse or (2) decompos-
able. A sparse map consists of scalar-valued component functions that each depend
only on a few input variables, whereas a decomposable map factorizes as the exact
composition of finitely many functions of low effective dimension (i.e., ùëá= ùëá1‚àò¬∑ ¬∑ ¬∑‚àòùëá‚Ñì,
where each ùëáùëñdiffers from the identity map only along a subset of its components).
We also describe conditions on (ùúàùúÇ, ùúàùúã) that yield (3) low-rank maps, i.e., functions
that depart from the identity only on a linear subspace of the parameter space. These
properties, and their combinations, dramatically reduce the complexity of represent-
ing a transport map and can be deduced before the map is explicitly computed.
The utility of these results is twofold.
First, they make the construction of
couplings‚Äîand hence the complete characterization of complex probability distributions‚Äî
tractable for a large class of inference problems by leveraging several notions of low
dimensionality.
In fact, we will show how to systematically translate features of
the target distribution into low-dimensional properties of the map‚Äîthus providing a
108

powerful framework for harnessing structure of a typical inference problem in com-
putations. Second, they suggest new algorithmic approaches for important classes
of statistical models. For instance, our analysis of sparse triangular maps provides
a general framework for describing continuous and non-Gaussian Markov random
fields, and for exploiting the conditional independence structure of these fields in
computation. Our analysis of decomposable transport maps yields new variational
algorithms for sequential inference in nonlinear and non-Gaussian state space models.
These algorithms characterize the full Bayesian solution to the sequential inference
problem‚Äîi.e., the joint posterior distribution of all the states and parameters up to
the current assimilation time‚Äîby means of a decomposable transport map, which is
constructed (recursively) in a single forward pass using local operations only slightly
more complex than regular filtering.
These algorithms can be understood as the
natural generalization‚Äîto the non-Gaussian case‚Äîof the square-root Rauch-Tung-
Striebel Gaussian smoother [226, 210].
This chapter is organized as follows. Section 5.2 reviews the Knothe-Rosenblatt
rearrangement, a key coupling for our analysis, while Section 5.3 briefly recalls some
standard terminology for Markov random fields and graphical models.
The main
results are in Sections 5.4‚Äì5.6: Section 5.4 addresses the sparsity of triangular trans-
ports, while Section 5.5 introduces and develops the concept of decomposable trans-
port maps for general Markov networks. These two sections can be read independently
(even though the proofs of Section 5.5 rely on some of the results of Section 5.4). Sec-
tion 5.6 specializes the theory of Section 5.5 to state-space models, introducing new
variational algorithms for filtering, smoothing, and sequential parameter inference.
This section is largely self-contained and could be the focus of a reader primarily
interested in algorithmic aspects of sequential inference or data assimilation. Section
5.7 characterizes the notion of low-rank transports. Section 5.8 illustrates aspects of
the theory on two numerical examples: a stochastic volatility model with hyperpa-
rameters and a high-dimensional log-Gaussian Cox model with sparse observations. A
final discussion is presented in Section 5.9. Appendix B collects some technical details
on the Knothe-Rosenblatt rearrangement and its generalizations, while Appendix E
109

contains the proofs of the main results. The material presented in this chapter can
be also found in [259].
5.2
Triangular transport maps: a building block
A foundational transport for our analysis is the Knothe-Rosenblatt (KR) rearrange-
ment on Rùëõ[234, 154, 34]. For a pair of measures ùúàùúÇ, ùúàùúã‚ààM+(Rùëõ), with densities ùúÇ
and ùúã, respectively, the KR rearrangement is the unique monotone increasing lower
triangular measurable map that pushes forward ùúàùúÇto ùúàùúã, i.e., ùëá‚ôØùúàùúÇ= ùúàùúã[48, 34].
Here, monotonicity is with respect to the lexicographic order on Rùëõ, while uniqueness
is up to ùúàùúÇ-null sets [34]. A lower triangular map ùëá: Rùëõ‚ÜíRùëõis a multivariate
function whose ùëòth component depends only on the first ùëòinput variables, i.e.,
ùëá(ùë•) =
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
ùëá1(ùë•1)
ùëá2(ùë•1, ùë•2)
...
ùëáùëõ(ùë•1, ùë•2, . . . ùë•ùëõ)
‚é§
‚é•‚é•‚é•‚é•‚é•‚é•‚é¶
(5.1)
for some collection of functions (ùëáùëò) and for all ùë•= (ùë•1, . . . , ùë•ùëõ).
The distinction between lower, upper, or other more general forms of triangular
map is a matter of convention. We will revisit this important point in Section 5.5.
See Appendix B for a constructive definition of the KR rearrangement based on a
sequence of one-dimensional transports. In our hypothesis, the KR rearrangement is
always a bijection on Rùëõ, while each map
ùúâ‚Ü¶‚Üíùëáùëò(ùë•1, . . . , ùë•ùëò‚àí1, ùúâ)
(5.2)
is homeomorphic (continuous bijection with continuous inverse), strictly increasing,
and differentiable a.e. [244, 267]. Here, monotonicity with respect to the lexicographic
order is equivalent to each (5.2) being an increasing function. The resulting rearrange-
ment ùëáis far from being a diffeomorphism but is still regular enough to define a useful
110

change of variables, as the following lemma proven in [34] shows.
Lemma 5.1. If ùëáis a KR rearrangement pushing forward ùúàùúÇto ùúàùúã, then ùúàùúÇ-a.e.,
ùëá‚ôØùúã(ùë•) = ùúã(ùëá(ùë•)) det ‚àáùëá(ùë•) = ùúÇ(ùë•),
(5.3)
where det ‚àáùëá:= ‚àèÔ∏Äùëõ
ùëñ=1 ùúïùëòùëáùëòexists a.e., and where ùëá‚ôØùúãis the density of ùëá‚ôØùúàùúã.
In general, det ‚àáùëáin (5.3) is not the determinant of the Jacobian of ùëásince the
map may not be differentiable, in which case it would not be possible to define ‚àáùëá
in the classical sense; this is why det ‚àáùëáis redefined in the lemma. Nevertheless, it
is known that ùëáinherits the same regularity as ùúÇand ùúã, but not more [244, 34]. See
Appendix B for additional remarks on the regularity of the map.
An essential feature of the triangular transport map is its anisotropic dependence
on the input variables. That is, even though each component of the transport map
does not depend on all ùëõinputs, the map is still capable of coupling arbitrary prob-
ability distributions. Informally, we can think of the KR rearrangement as imposing
the sparsest possible structure that preserves generality of the coupling‚Äîin that the
rearrangement is guaranteed to exist for any ùúàùúÇ, ùúàùúã‚ààM+(Rùëõ). (In fact, the transport
can be defined under much weaker conditions [244].) In Section 5.5, we will show that
the anisotropy of the KR rearrangement is crucial to proving that certain ‚Äúcomplex‚Äù
(and generally non-triangular) transports can be factorized into compositions of a few
lower-dimensional triangular maps. Thus we can think of the KR rearrangement as
the fundamental building block of a more general class of non-triangular transports.
The KR rearrangement also enjoys many attractive computational features. As
shown in [196, 187], it can be characterized as the unique minimizer of the Kullback‚Äì
Leibler (KL) divergence ùíüKL( ùëá‚ôØùúàùúÇ|| ùúàùúã) over the cone ùíØ‚ñ≥of monotone increasing
triangular maps. From the perspective of function approximation, parameterizing a
monotone triangular map is straightforward: it suffices to write each component of
the map as
ùëáùëò(ùë•) = ùëéùëò(ùë•1, . . . , ùë•ùëò‚àí1) +
‚à´Ô∏Åùë•ùëò
0
exp (ùëèùëò(ùë•1, . . . , ùë•ùëò‚àí1, ùë°)) dùë°,
(5.4)
111

for some arbitrary functions ùëéùëò: Rùëò‚àí1 ‚ÜíR and ùëèùëò: Rùëò‚ÜíR [30, 221]. The resulting
transport map is always monotone and invertible. (In contrast, parameterizing gen-
eral classes of monotone non-triangular maps is a difficult task.) The minimization
of ùíüKL( ùëá‚ôØùúàùúÇ|| ùúàùúã) for a map in ùíØ‚ñ≥and for a pair of nonvanishing target (ùúã) and
reference (ùúÇ) densities can be rewritten as [196, 187]:
min
ùëá
‚àíE
[Ô∏É
log ùúã(ùëá(ùëã)) +
‚àëÔ∏Å
ùëò
log ùúïùëòùëáùëò(ùëã) ‚àílog ùúÇ(ùëã)
]Ô∏É
(5.5)
s.t.
ùëá‚ààùíØ‚ñ≥,
where the expectation is taken with respect to the reference measure‚Äîwhich is the
law of ùëã.
Two aspects of (5.5) are particularly important. First, for the purpose of opti-
mization, the target density can be replaced with its unnormalized version ¬Øùúã. (This
replacement is essential in Bayesian inference, where the posterior normalizing con-
stant is usually unknown.) Second, (5.5) can be treated as a stochastic program and
solved by means of sample-average approximation (SAA) [249] or stochastic approxi-
mation [158, 257, 36]. Recall that the reference measure is a degree of freedom of the
problem and is chosen precisely to make the integration in (5.5) feasible using, for
instance, quadrature, Monte Carlo, or quasi-Monte Carlo methods [79, 233, 84, 83].
Assuming some additional regularity for ùúã(e.g., at least differentiability) and us-
ing the monotone parameterization of (5.4), then (5.5) becomes an unconstrained and
differentiable optimization problem. In particular, we can use the gradient of log ùúã
to obtain an unbiased estimator for the gradient of (5.5) [130, 107, 8]. Alternatively,
if ‚àálog ùúãis unavailable, we can use the score method [108, 222] to produce an esti-
mator that is still unbiased, but with higher variance. For concreteness, consider the
realization of an i.i.d. sample (ùë•ùëñ)ùëÄ
ùëñ=1 from ùúàùúÇ. Then a SAA of (5.5) reads as:
min
ùëá
‚àí
ùëÄ
‚àëÔ∏Å
ùëñ=1
(Ô∏É
log ¬Øùúã(ùëá(ùë•ùëñ)) +
‚àëÔ∏Å
ùëò
log ùúïùëòùëáùëò(ùë•ùëñ) ‚àílog ùúÇ(ùë•ùëñ)
)Ô∏É
(5.6)
s.t.
ùëá‚ààùíØ‚ñ≥,
112

which is now amenable to deterministic optimization techniques. The numerical solu-
tion of (5.6) by means of an iterative optimization method (e.g., BFGS [276]) produces
a sequence of maps ÃÉÔ∏Äùëá1, ÃÉÔ∏Äùëá2, . . . that are increasingly better approximations of the KR
rearrangement, in the sense defined by (5.6). In particular, we can interpret ( ÃÉÔ∏Äùëáùëò)ùëòas
a discrete time flow that pushes forward the collection of reference samples, (ùë•ùëñ)ùëÄ
ùëñ=1,
to the target distribution.
See Figure 5-1 for a simple illustration.
As shown in
[196], the KL divergence ùíüKL( ÃÉÔ∏Äùëá‚ôØùúàùúÇ|| ùúàùúã) for an approximate map ÃÉÔ∏Äùëácan be efficiently
estimated as:
ùíüKL( ÃÉÔ∏Äùëá‚ôØùúàùúÇ|| ùúàùúã) ‚âà1
2Var
[Ô∏É
log ¬Øùúã( ÃÉÔ∏Äùëá(ùëã)) +
‚àëÔ∏Å
ùëò
log ùúïùëòÃÉÔ∏Äùëáùëò(ùëã) ‚àílog ùúÇ(ùëã)
]Ô∏É
,
(5.7)
up to second-order terms, in the limit of ùíüKL( ÃÉÔ∏Äùëá‚ôØùúàùúÇ|| ùúàùúã) ‚Üí0, even if the normal-
izing constant of ùúãis unknown. This convergence criterion is rather useful for any
variational inference method, and is usually not available for techniques like MCMC.
In the same way, [196] constructs efficient estimators for the normalizing constant
ùõΩ:= ¬Øùúã/ùúãas
^ùõΩ= exp E
[Ô∏É
log ¬Øùúã( ÃÉÔ∏Äùëá(ùëã)) +
‚àëÔ∏Å
ùëò
log ùúïùëòÃÉÔ∏Äùëáùëò(ùëã) ‚àílog ùúÇ(ùëã)
]Ô∏É
(5.8)
We refer the reader to [212, 187], and Section 6.2, for an alternative construction of
the transport map that is useful when only samples from the target measure are avail-
able. An interesting application of the latter construction is the problem of density
estimation [266] or Bayesian inference with intractable likelihoods [274, 184, 72]. In
this case, it turns out that the inverse transport ùëÜ= ùëá‚àí1 can be easily computed
via convex optimization [213]. (Notice that ùëÜis just an ordinary triangular transport
map that pushes forward ùúàùúãto ùúàùúÇ. The ‚Äúinverse‚Äù descriptor will help distinguish ùëÜ
from the map ùëáthat pushes forward the reference to the target distribution. We
refer to ùëáas the direct transport.) We can then invert ùëÜat ùë•‚ààRùëõto obtain the
evaluation of the direct transport ùëá(ùë•). Inverting a monotone triangular function
is a computationally trivial task since it requires the solution of a sequence of one-
113

dimensional root finding problems [212, 187]. In practice, one just needs to invert
(5.2) for ùëò= 1, . . . , ùëõ. It is also possible to compute the inverse transport from the
unnormalized target density, rather than from samples; here, it suffices to minimize
ùíüKL( ùúàùúÇ|| ùëÜ‚ôØùúàùúã) for ùëÜ‚ààùíØ‚ñ≥. The resulting variational problem is equivalent to (5.5)
with the identity ùëÜ= ùëá‚àí1. By symmetry of our formulation, ùëÜhas the same regu-
larity as ùëá. In particular, Lemma 5.1 holds for ùëÜas well, and gives a formula for the
pushforward density ùëá‚ôØùúÇas:
ùëá‚ôØùúÇ(ùëß) = ùúÇ(ùëÜ(ùëß)) det ‚àáùëÜ(ùëß) = ùúã(ùëß),
(5.9)
where det ‚àáùëÜ:= ‚àèÔ∏Äùëõ
ùëñ=1 ùúïùëòùëÜùëòexists a.e., and where ùëá‚ôØùúÇis the density of ùëá‚ôØùúàùúÇ. There
is a growing body of literature on the efficient numerical approximation of transport
maps (see, e.g., [30, 231, 178, 266, 196]). Essentially all of these approaches employ
numerical optimization to construct or realize the action of a map, and thus harness
optimization to enhance integration. Yet all these approaches face a fundamental
challenge: the transport map is a function from Rùëõonto itself, and in high dimensions
(i.e., for large ùëõ) the representation and approximation of such functions becomes
increasingly intractable. In the ensuing sections, on the other hand, we will show
that a large class of transport maps are in fact only superficially high-dimensional;
that is, they possess some hidden low-dimensional structure that can facilite their fast
and reliable computation. This low-dimensional structure is linked to the Markov
properties of the target measure, which we briefly review in the next section.
5.3
Markov networks
Let ùëç= (ùëç1, . . . , ùëçùëõ) be a collection of random variables with law ùúàùúãand density
ùúã. We can represent a list of conditional independences satisfied by ùëç‚Äîthe so-called
Markov properties‚Äîusing a simple undirected graph ùí¢= (ùí±, ‚Ñ∞), where each node
ùëò‚ààùí±is associated with a distinct random variable, ùëçùëò, and where the edges in ‚Ñ∞
encode a specific notion of probabilistic interaction among these random variables
114

3
0
3
3
0
3
6
9
12
15
18
ÃÉÔ∏Äùëá0
3
0
3
3
0
3
6
9
12
15
18
ÃÉÔ∏Äùëá1
3
0
3
3
0
3
6
9
12
15
18
ÃÉÔ∏Äùëá2
3
0
3
3
0
3
6
9
12
15
18
ÃÉÔ∏Äùëá3
Figure 5-1: Computation of a simple transport map in two dimensions: The leftmost
figure shows contours of the reference density ùúÇ, which is a standard Gaussian, and
of the target density ùúã, which is a banana-shaped distribution in the tails of ùúÇ. The
target distribution has a nonlinear dependence structure. The orange dots in the
leftmost figure correspond to 100 samples (ùë•ùëñ) from ùúÇand are used to make a sample-
average approximation of (5.5). We adopt the triangular monotone parameterization
of (5.4) for the candidate transport map, where the functions ùëéùëò, ùëèùëòare expanded
in a multivariate Hermite polynomial basis of total degree two [277]. The resulting
optimization problem is solved with a quasi-Newton method (BFGS). The ùëòth figure
from the left shows the pushforward of the original reference samples through the
approximate transport map, ÃÉÔ∏Äùëáùëò, after ùëòiterations of BFGS. The initial map ÃÉÔ∏Äùëá0 is
chosen to be the identity. The reference samples flow collectively towards the target
density and eventually settle on the support of ùúã, capturing its structure after just a
few iterations.
[156]. In particular, we say that ùëçis a Markov network‚Äîor a Markov random field
(MRF)‚Äîwith respect to ùí¢if for any triplet ùíú, ùíÆ, ‚Ñ¨of disjoint subsets of ùí±, where
ùíÆis a separator set for ùíúand ‚Ñ¨,1 the subcollections ùëçùíúand ùëç‚Ñ¨are conditionally
independent given ùëçùíÆ, i.e.,
ùëçùíú‚ä•‚ä•ùëç‚Ñ¨| ùëçùíÆ.
(5.10)
The measure ùúàùúãis said to satisfy the global Markov property, relative to ùí¢, if (5.10)
holds [161]. We can also say that ùúàùúãis globally Markov with respect to ùí¢. The
1 ùíÆis a separator set for ùíúand ‚Ñ¨if (1) ùíÆis disjoint from ùíúand ‚Ñ¨(2) Every path from ùõº‚ààùíú
to ùõΩ‚àà‚Ñ¨intersects ùíÆ. If ùíúand ‚Ñ¨are disconnected components of ùí¢, then ùíÆ= ‚àÖis a separator set
for ùíúand ‚Ñ¨.
115

corresponding graph is then called an independence map (I-map) for ùúàùúã[156].
Intuitively, a sparse graph represents a family of distributions that enjoy many
conditional independence properties. I-maps are in general not unique. Of particular
interest are minimal I-maps, i.e., the sparsest graphs compatible with the conditional
independence structure of ùúàùúã.
Conditional independence is associated with factorization properties of ùúã. For
instance, ùëçùíú‚ä•‚ä•ùëç‚Ñ¨| ùëçùíÆif and only if ùúãùëçùíú,ùëç‚Ñ¨|ùëçùíÆ= ùúãùëçùíú|ùëçùíÆùúãùëç‚Ñ¨|ùëçùíÆa.e. [161].
We
then say that ùúàùúãfactorizes according to some graph ùí¢if there exists a version of the
density of ùúàùúãsuch that
ùúã(ùëß) = 1
c
‚àèÔ∏Å
ùíû‚ààùíû
ùúìùíû(ùëßùíû),
(5.11)
for some nonnegative functions (ùúìùíû) called potentials, where ùíûis the set of maximal
cliques2 of ùí¢and c is a normalizing constant. It is immediate to show that if ùúàùúã
factorizes according to ùí¢, then ùúàùúãsatisfies the global Markov property relative to ùí¢
[161, Proposition 3.8]. The converse is true only under additional assumptions: for
instance, if ùúàùúãadmits a continuous and strictly positive density (see the Hammersley-
Clifford theorem [118, 161]).
A critical question then is how to characterize a suitable I-map for a given measure.
There are several answers. First of all, in many applications that involve probabilistic
modeling, the target distribution is defined in terms of its potentials, as in (5.11),
because this is just a more convenient way to specify a high-dimensional distribution
and to perform inference (or general probabilistic reasoning) with it [156]. Finding
a graph for which ùúàùúãfactorizes is then a trivial task. See Figure 5-4 (left) for an
example. Applications where this commonly holds range from spatial statistics and
image analysis to speech recognition [156, 236].
In Section 5.6, for example, we
focus exclusively on discrete-time Markov processes, where the Markov structure of
the problem is self-evident. In other settings, the graph is unknown and must be
estimated. When only samples from ùúàùúãare available, this is a question of model
learning, as described in [156, Part III]; see also [134, 189, 280, 173] for various
2 A clique is a fully connected subset of the vertices, whereas a maximal clique is a clique that is
not a strict subset of another clique.
116

applications.
In case of a known and smooth target density, we can characterize
pairwise conditional independence in terms of mixed second-order partial derivatives,
as shown by the following lemma.
Lemma 5.2 (Pairwise conditional independence). If ùëç‚àºùúàùúãfor a measure ùúàùúãwith
smooth and strictly positive density ùúã, we have:
ùëçùëñ‚ä•‚ä•ùëçùëó| ùëçùí±‚àñ(ùëñ,ùëó)
‚áê‚áí
ùúï2
ùëñ,ùëólog ùúã= 0 oùëõRùëõ.
(5.12)
Thus, if we can evaluate ùúãand its derivatives (up to a normalizing constant),
we can use Lemma 5.2 to assess pairwise conditional independence and to define a
minimal I-map for ùúàùúãas follows: add an edge between every pair of distinct nodes un-
less the corresponding random variables are conditionally independent [156, Theorem
4.5].
Regardless of the many ways to obtain an I-map, there is a fundamental connection
between Markov properties of a distribution and the existence of low-dimensional
transport maps. The rest of the chapter will elaborate precisely on this connection.
5.4
Sparsity of triangular transport maps
We begin our investigation of low dimensional structure by considering the notion
of sparse transport map. A sparse map is a multivariate function where each com-
ponent does not depend on all of its input variables. According to this definition, a
triangular transport is already sparse. In this section, however, we show that the KR
rearrangement can be even sparser, depending on the Markov structure of the target
distribution.
5.4.1
Sparsity bounds
Given a lower triangular function ùëá, we define its sparsity pattern, Iùëá, as the set of
all integer pairs (ùëó, ùëò), with ùëó< ùëò, such that the ùëòth component of the map does
not depend on the ùëóth input variable, i.e., Iùëá= {(ùëó, ùëò) : ùëó< ùëò, ùúïùëóùëáùëò= 0}. (We do
117

not include pairs ùëó> ùëòin the definition of Iùëásince, for a lower triangular function,
ùúïùëóùëáùëò= 0 for ùëó> ùëòby construction.)
Knowing the sparsity pattern of the KR rearrangement before computing the
actual transport has important computational implications.
For instance, in the
variational characterization of the transport described in (5.5), we can restrict the
feasible domain to the set of triangular maps with sparsity pattern given by Iùëá,
and still recover the desired KR rearrangement.
That is, if (ùëó, ùëò) ‚ààIùëá, we can
parameterize any candidate transport map by removing the dependence on the ùëóth
input variable from the ùëòth component of the map. Thus, analyzing the Markov
structure of the target distribution enables the representation and computation of
maps in possibly higher-dimensional settings.
The following theorem, which is the main result of this section, characterizes
bounds on the sparsity patterns of triangular transport maps given an I-map for the
target measure. In the statement of the theorem, we denote the direct transport by
ùëáand the inverse transport by ùëÜ= ùëá‚àí1 (see Section 5.2). The theorem suggests that
ùëÜand ùëácan have quite different sparsity patterns.3
Theorem 5.1 (Sparsity of Knothe‚ÄìRosenblatt rearrangements). Let ùëã‚àºùúàùúÇ, ùëç‚àº
ùúàùúãwith ùúàùúÇ, ùúàùúã‚ààM+(Rùëõ) and ùúàùúÇtensor product measure. Moreover, assume that ùúàùúã
is globally Markov with respect to ùí¢, and define, recursively, the sequence of graphs
(ùí¢ùëò)ùëõ
ùëò=1 as: (1) ùí¢ùëõ:= ùí¢and (2) for all 1 ‚â§ùëò< ùëõ, ùí¢ùëò‚àí1 is obtained from ùí¢ùëòby
removing node ùëòand by turning its neighborhood Nb(ùëò, ùí¢ùëò) into a clique. Then the
following hold:
1. If IùëÜis the sparsity pattern of the inverse transport map ùëÜ, then
ÃÇÔ∏ÄIùëÜ‚äÇIùëÜ,
(5.13)
where ÃÇÔ∏ÄIùëÜis the set of integer pairs (ùëó, ùëò) such that ùëó/‚ààNb(ùëò, ùí¢ùëò).
3 A note: as we already saw, the KR rearrangement is unique up to a set of measure zero.
Theorem 5.1 characterizes the sparsity pattern of a particular version of the map, the one given by
Definition B.2 in Appendix B. We will implicitly make this assumption throughout the chapter.
118

2. If Iùëáis the sparsity pattern of the direct transport map ùëá, then
ÃÇÔ∏ÄIùëá‚äÇIùëá,
(5.14)
where ÃÇÔ∏ÄIùëáis defined recursively as follows: for ùëò= 2, . . . , ùëõthe pair (ùëó, ùëò) ‚ààÃÇÔ∏ÄIùëá
if and only if (ùëó, ùëñ) ‚ààÃÇÔ∏ÄIùëáfor all ùëñ‚ààNb(ùëò, ùí¢ùëò).
3. The predicted sparsity pattern of ùëÜis always greater than or equal to that of ùëá,
i.e.,
ÃÇÔ∏ÄIùëá‚äÇÃÇÔ∏ÄIùëÜ.
(5.15)
Several remarks are in order. First, we emphasize the fact that Theorem 5.1 char-
acterizes sparsity patterns using only an I-map for ùúàùúã, without requiring any actual
computation of the transports. One only needs to perform simple graph operations
on ùí¢to build the sequence of graphs (ùí¢ùëò). See Figure 5-2 for an illustration of this
procedure, with the corresponding sparsity patterns in Figure 5-3. We refer to (ùí¢ùëò)
as the marginal graphs. In fact, the sequence (ùí¢ùëò) is precisely the set of intermediate
graphs produced by the variable elimination algorithm [156, Ch. 9], when marginal-
izing with elimination ordering (ùëõ, ùëõ‚àí1, . . . , 1). This should not be surprising as the
KR rearrangement is essentially a sequence of ordered marginalizations [271]. Also,
the hypothesis that ùúàùúÇis a tensor product measure is not restrictive since the reference
distribution is a degree of freedom of the problem.
We note that Theorem 5.1 does not provide the exact sparsity patterns of the
triangular transport maps; instead, (5.13) and (5.14) provide subsets of Iùëáand IùëÜ.
In other words, the actual transport maps might be sparser than predicted by the sets
ÃÇÔ∏ÄIùëÜand ÃÇÔ∏ÄIùëá‚Äîbut, crucially, they cannot be less sparse. Thus, we can think of Theorem
5.1 as providing bounds on the sparsity of triangular transports. An important fact
is that, without additional information on ùúàùúã, these bounds are sharp. That is, we
can always find a pair of measures (ùúàùúÇ, ùúàùúã) satisfying the hypotheses of Theorem 5.1
and such that the predicted and actual sparsity patterns coincide, i.e., ÃÇÔ∏ÄIùëá= Iùëáor
ÃÇÔ∏ÄIùëÜ= IùëÜ.
119

Part 3 of Theorem 5.1 shows that the predicted sparsity pattern of the inverse
KR rearrangement is always larger than or equal to that of the direct transport, i.e.,
ÃÇÔ∏ÄIùëá‚äÇÃÇÔ∏ÄIùëÜ. This does not mean that for every pair of measures (ùúàùúÇ, ùúàùúã), the inverse
triangular transport is always at least as sparse as the direct transport; in fact, it is
possible to provide simple counterexamples. However, this result does imply that if
we are only given an I-map for ùúàùúã, then parameterizing candidate inverse triangular
transports allows the imposition of more sparsity constraints than parameterizing
candidate direct transports. In general, sparser transports are easier to represent.
See Figure 5-4 (right) for a nontrivial example of sparsity patterns for a stochastic
volatility model.
Indeed, (5.15) hints at a typical trend: inverse transport maps tend to be sparser
(in many practical cases, much sparser) than their direct counterparts. Intuitively, the
sparsity of a direct transport is associated with marginal independence in ùëç, whereas
the inverse transport inherits sparsity from the conditional independence structure
of ùëç. The latter is a weaker condition than mutual independence; for instance, the
correlation length of a process modeled by a Markov random field may be much larger
than the typical neighborhood size [236, 32]. Thus, given a sparse I-map for the target
measure, it can be computationally advantageous to characterize an inverse transport
rather than a direct one, because the inverse transport can inherit a larger sparsity
pattern. Given an inverse triangular transport ùëÜ, we can then easily evaluate the
direct transport ùëá= ùëÜ‚àí1 at any point ùë•‚ààRùëõby inverting ùëÜpointwise, as described
in Section 5.2.
There is no need to have an explicit representation of the direct
transport as long as it can be implicitly defined through its inverse.
5.4.2
Connection to Gaussian Markov random fields
The reader familiar with Gaussian Markov random fields (GMRFs), might see links
between the preceding results and widespread approaches to the modeling of Gaussian
fields. In this section, we clarify the extent of these connections.
Many applications (e.g., image analysis, spatial statistics, time series) involve
modeling by means of high-dimensional Gaussian fields. Dealing with large and dense
120

3
2
5
1
4
ùí¢5
3
2
5
1
4
ùí¢4
3
2
5
1
4
ùí¢3
5
2
4
1
3
ùí¢2
Figure 5-2: Sequence of graphs (ùí¢ùëò) described in Theorem 5.1 for a target measure
in M+(R5) with I-map illustrated by the leftmost graph, ùí¢5. Notice that to generate
the graph ùí¢2, we remove node 3 from ùí¢3 and turn its neighborhood into a clique by
adding the edge (1, 2).
Pùëóùëò= ùúïùëóùëÜùëò
Pùëóùëò= ùúïùëóùëáùëò
Figure 5-3: Sparsity patterns predicted by Theorem 5.1 for the target measure an-
alyzed in Figure 5-2. We represent the sparsity patterns using a symbolic matrix
notation: the (ùëó, ùëò)-th entry of the matrix is not colored if the ùëòth component of the
map (ùëÜor ùëá) does not depend on the ùëóth input variable, or, equivalently, if (ùëó, ùëò) ‚ààÃÇÔ∏ÄIùëÜ
(resp. ÃÇÔ∏ÄIùëá) (5.13). (Since we are considering lower triangular transports, all entries
ùëó> ùëòare uncolored. Note also that ùëÜùëòand ùëáùëòare always functions of their ùëòth input
by strict monotonicity of the map.) The predicted sparsity pattern for the direct
transport in this example is ÃÇÔ∏ÄIùëá= ‚àÖ.
covariances, however, is often impractical; both storage and sampling of the Gaussian
field are problematic. The usual workaround is to replace or approximate the Gaussian
field with a sparse GMRF‚Äîi.e., a Gaussian Markov network that enforces locality in
the probabilistic interactions among the underlying random variables. The minimal
I-map for the GMRF is thus sparse, and so is the precision matrix Œõ of the field [236].
The covariance matrix is still in general dense, but dealing with the sparse precision
matrix is much easier. If ùêøùêø‚ä§is a (sparse) Cholesky decomposition of Œõ, then ùêø‚ä§
represents a linear triangular transport that pushes forward the joint distribution of
the GMRF, ùúàùúã= ùí©(0, Œõ‚àí1), to a standard normal, ùúàùúÇ= ùí©(0, I). The key point
is that for many Markov structures of interest, the Cholesky factor inherits sparsity
from the underlying graph, so that sampling from ùúàùúãcan be achieved at low cost as
121

ùëç0
ùëç1
ùëç2
ùëç3
ùëçùëÅ
ùúá
ùúë
Pùëóùëò= ùúïùëóùëÜùëò
Figure 5-4:
(left) Markov network for a stochastic volatility model in [149]. Blue
nodes represent the discrete-time latent log-volatility process (ùëçùëò)ùëÅ
ùëò=0, which obeys a
simple autoregressive model with hyperparameters ùúá, ùúë. The graph above is a min-
imal I-map for the posterior density described in Section 5.8.1, ùúãùúá,ùúë,ùëç0:ùëÅ|ùë¶0:ùëÅ, where
ùë¶0:ùëÅare some (fixed) observations. (right) The predicted sparsity pattern ÃÇÔ∏ÄIùëÜ(only
the top 6 √ó 6 block is shown) for the inverse transport corresponding to the model
on the left: the first column/row of the matrix refer jointly to all of the hyperpa-
rameters. Each component ùëÜùëòof the inverse transport can depend at most on four
input variables, namely ùúá, ùúë, ùëçùëò‚àí1, ùëçùëò, regardless of the overall dimension ùëÅof the
problem. In order to apply the results of Theorem 5.1, we must select an ordering of
the input variables; here, we used the ordering (ùúá, ùúë, ùëç0, . . . , ùëçùëÅ). Optimal orderings
are further discussed in Section 5.4.3.
follows: if ùëãis a sample from ùúàùúÇ, then we can obtain a sample ùëçfrom ùúàùúãsimply by
solving the sparse triangular linear system ùêø‚ä§ùëç= ùëã. There is no need to explicitly
represent or store the dense factor ùêø‚àí‚ä§, since we can implicitly represent its action
by inverting a sparse triangular function.
Now the connection with Section 5.4 is clear: ùêø‚ä§is an inverse triangular trans-
port,4 while ùêø‚àí‚ä§is a direct one.
Moreover, solving a triangular linear system is
just a particular instance of inverting a nonlinear triangular function by performing
a sequence of one-dimensional root-findings. Thus the developments of the previous
section, which consider arbitrary nonlinear maps, are a natural generalization‚Äîto
the non-Gaussian case‚Äîof modeling and sampling techniques for high-dimensional
GMRFs [236].
4Actually, this transport is upper rather than lower triangular. This distinction plays no role
in the following discussion, and the fact that a KR rearrangement is a lower triangular function is
merely a matter of convention. We will return to this important point in Section 5.5.
122

5.4.3
Ordering of triangular maps
The results of Theorem 5.1 suggest that the sparsity of a triangular transport map
depends on the ordering of the input variables. See Figure 5-5 for a simple illustration.
Indeed, the triangular transport itself depends anisotropically on the input variables
and requires the definition of a proper ordering. A natural approach is then to seek
the ordering that promotes the sparsest transport map possible.
Consider a pair of measures (ùúàùúÇ, ùúàùúã) that satisfies the hypotheses of Theorem
5.1. We associate an ordering of the input variables with a permutation ùúéon Nùëõ=
{1, . . . , ùëõ}, and define the reordered target measure ùúàùúé
ùúãas the pushforward of ùúàùúãby the
matrix ùëÑùúéthat represents the permutation ùúé. In particular, (ùëÑùúé)ùëñùëó= (ùëíùúé(ùëñ))ùëó, where
ùëíùëñis the ùëñth standard basis vector on Rùëõ. Moreover, if ùí¢is an I-map for ùúàùúã, then
we denote an I-map for ùúàùúé
ùúãby ùí¢ùúé. Notice that ùí¢ùúécan be derived from ùí¢simply by
relabeling its nodes according to the permutation ùúé. Then we can cast a variational
problem for the best ordering ùúé* as:
ùúé* ‚àà
arg maxùúé
|IùëÜ|
(5.16)
s.t.
ùëÜ‚ôØùúàùúé
ùúã= ùúàùúÇ
ùúé‚ààP(Nùëõ),
where ùëÜis the KR rearrangement that pushes forward the reordered target ùúàùúé
ùúãto ùúàùúÇ
and P(Nùëõ) is the set of permutations of Nùëõ. The goal is to maximize the cardinality
of the sparsity pattern of the inverse map, |IùëÜ|. We restrict our attention to the
sparsity of the inverse transport, since we know from Section 5.4 that the direct
transport tends to be dense, even for the most trivial Markov structures.
Ideally, we would like to determine a good ordering for the map before computing
the actual transport, and to use the resulting information about the sparsity pattern to
simplify the optimization problem for ùëÜ. However, evaluating the objective function
of (5.16) requires computing a different inverse transport for each permutation ùúé.
One possible way to relax (5.16) is to replace IùëÜwith the predicted sparsity pattern
ÃÇÔ∏ÄIùëÜintroduced in (5.13). The advantage of this approach is that the objective function
123

of the relaxed problem can now be evaluated in closed form without computing any
transport map, but rather by performing the simple sequence of graph operations on
ùí¢ùúédescribed by Theorem 5.1. The caveat is that, in general, ÃÇÔ∏ÄIùëÜ‚äÇIùëÜ, and thus
maximizing |ÃÇÔ∏ÄIùëÜ| amounts to seeking the tightest lower bound on the sparsity pattern
of the inverse transport. From the definition of ÃÇÔ∏ÄIùëÜ, it follows that the best ordering
ùúé* for the relaxed problem is one that introduces the fewest edges in the construction
of the marginal graphs ùí¢ùëõ, . . . , ùí¢1, whenever ùí¢ùëõ= ùí¢ùúé*. Thus, for a given I-map ùí¢,
we denote by F(ùúé; ùí¢) the fill-in produced by the ordering ùúé. That is, F(ùúé; ùí¢) is a set
containing all the edges introduced in the construction of the marginal graphs (ùí¢ùëò)
from ùí¢ùúé. A computationally feasible relaxation of (5.16) is then given by:
ùúé* ‚àà
arg minùúé
|F(ùúé; ùí¢)|
(5.17)
s.t.
ùúé‚ààP(Nùëõ) .
(5.17) is a standard problem in graph theory; it arises in a variety of practical settings,
including (most relatedly) finding the best elimination ordering for variable elimina-
tion in graphical models [156], or finding the permutation that minimizes the fill-in of
the Cholesky factor of a positive definite matrix [105, 240]. From an algorithmic point
of view, (5.17) is NP-complete [279]. This should not be surprising, as best‚Äìordering
problems are typically combinatorial in nature. Nevertheless, given its widespread
applicability, a host of effective polynomial-time heuristics for (5.17) have been devel-
oped in past years (e.g., min-fill or weighted-min-fill [156]). Most importantly, (5.17)
can be solved without ever touching the target measure (assuming, of course, that
an I-map ùí¢for ùúàùúãis known). As a result, the cost of finding a good ordering is
often negligible compared to the cost of characterizing a nonlinear transport map via
optimization.
124

3
2
5
1
4
ùí¢
Pùëñùëó= ùúïùëñùëÜùëó
5
2
3
1
4
ùí¢‚Ä≤
P‚Ä≤
ùëñùëó= ùúïùëñùëÜùëó
Figure 5-5:
Illustration of how a simple re-ordering of the input variables can change
the (predicted) sparsity pattern of the inverse map. On the left, ùí¢represents an I-
map for the target measure considered in Figure 5-2, with ordering (ùëç1, ùëç2, ùëç3, ùëç4, ùëç5),
together with its sparsity pattern ÃÇÔ∏ÄIùëÜ. (See Figure 5-3 for details on the ‚Äúmatrix‚Äù rep-
resentation of sparsity patterns.) On the right, ùí¢‚Ä≤ is an I-map for the same target
measure but with the ordering (ùëç1, ùëç2, ùëç5, ùëç4, ùëç3). The corresponding sparsity pat-
tern ÃÇÔ∏ÄIùëÜ‚Ä≤ is now the empty set.
5.5
Decomposability of transport maps
Thus far, we have investigated the sparsity of triangular transport maps and found
that inverse transports tend to inherit sparsity from the underlying Markov structure
of the target measure. Though direct triangular transports also inherit some sparsity
according to Theorem 5.1, they tend to be more dense.
This section shows that direct transports enjoy a different form of low-dimensional
structure: decomposability. A decomposable transport map is a function that can be
written as the composition of a finite number of low-dimensional maps, e.g., ùëá=
ùëá1 ‚àò¬∑ ¬∑ ¬∑ ‚àòùëá‚Ñìfor some integer ‚Ñì‚â•2. We use a very specific notion of low-dimensional
map, as follows.
Definition 5.1 (Low-dimensional map with respect to a set). A map ùëÄ: Rùëõ‚ÜíRùëõ
is low-dimensional with respect to a nonempty set ùíû‚äÇùí±‚âÉNùëõif
1. ùëÄùëò(ùë•) = ùë•ùëòfor ùëò‚ààùíû
2. ùúïùëóùëÄùëò= 0 for ùëó‚ààùíûand ùëò‚ààùí±‚àñùíû.
The effective dimension of ùëÄis the minimum cardinality |ùí±‚àñùíû| over all sets ùíûwith
respect to which ùëÄis low-dimensional.
125

In particular, up to a permutation of its components, we can rewrite ùëÄas:
ùëÄ(ùë•) =
‚é°
‚é£ùëÄ¬Øùíû(ùë•¬Øùíû)
ùë•ùíû
‚é§
‚é¶,
(5.18)
where ¬Øùíû= ùí±‚àñùíûdenotes the complement of ùíûin ùí±, and where for any map ùëÄand set
ùíú= {ùëé1, . . . , ùëéùëò}, ùëÄùíúdenotes the multivariate function ùë•‚Ü¶‚Üí(ùëÄùëé1(ùë•), . . . , ùëÄùëéùëò(ùë•))
obtained by stacking together the components of ùëÄwith index in ùíú. Thus ùëÄis the
trivial embedding of a | ¬Øùíû|-dimensional function into the identity map and has effec-
tive dimension bounded by | ¬Øùíû| < ùëõ. It is not surprising, then, that a decomposable
transport ùëá= ùëá1 ‚àò¬∑ ¬∑ ¬∑ ‚àòùëá‚Ñìshould be easier to represent than an ordinary map. A
perhaps less intuitive feature, however, is that the computation of a high-dimensional
decomposable transport can be broken down into multiple simpler steps, each associ-
ated with the computation of a low-dimensional map ùëáùëóthat accounts only for local
features of the target measure.
The forthcoming analysis will consider general, and hence possibly non-triangular,
transports. Thus its scope is much broader than that of Section 5.4, where we only
focused on the sparsity of triangular transports. Yet, we will show that triangular
maps are the building block of decomposable transports. The cornerstone of our anal-
ysis is Theorem 5.2, which characterizes the existence and structure of decomposable
transports given only the Markov structure of the underlying target measure.
Our discussion will proceed in two stages: first, we show how to identify direct
transports that decompose into two maps, i.e., ùëá= ùëá1 ‚àòùëá2, and then we explain
how to apply this result recursively to obtain a general decomposition of the form
ùëá= ùëá1 ‚àò¬∑ ¬∑ ¬∑ ‚àòùëá‚Ñì.
5.5.1
Preliminary notions
Before addressing the decomposability of transport maps, we need to introduce two
useful concepts: proper graph decompositions and generalized triangular functions.
The decomposition of a graph is a standard notion [161].
126

Definition 5.2 (Proper graph decomposition). Given a graph ùí¢= (ùí±, ‚Ñ∞), a triple
(ùíú, ùíÆ, ‚Ñ¨) of disjoint subsets of the vertex set ùí±forms a proper decomposition of ùí¢if
(1) ùí±= ùíú‚à™ùíÆ‚à™‚Ñ¨, (2) ùíúand ‚Ñ¨are nonempty, (3) ùíÆseparates ùíúfrom ‚Ñ¨, and (4) ùíÆ
is a clique.
See Figure 5-6 (top left) for an example of a decomposition. Clearly, not every
graph admits a proper decomposition; for instance, a fully connected graph does not
have a separator set for nonempty ùíúand ‚Ñ¨. The idea we will pursue here is that
graph decompositions lead to the existence of decomposable transports.
The notion of a generalized triangular function is perhaps less standard, but still
relatively straightforward:
Definition 5.3 (Generalized triangular function). A function ùëá: Rùëõ‚ÜíRùëõis said to
be generalized triangular, or simply ùúé-triangular, if there exists a permutation ùúéof Nùëõ
such that the ùúé(ùëò)th component of ùëádepends only on the variables ùë•ùúé(1), . . . , ùë•ùúé(ùëò),
i.e., ùëáùúé(ùëò)(ùë•) = ùëáùúé(ùëò)(ùë•ùúé(1), . . . , ùë•ùúé(ùëò)) for all ùë•= (ùë•1, . . . , ùë•ùëõ) and for all ùëò= 1, . . . , ùëõ.
We can think of a generalized triangular function as a map that is lower triangular
up to a permutation. In particular, if ùúéis the identity on Nùëõ, then a ùúé-triangular
function is simply a lower triangular map (see Section 5.2). To represent the permu-
tation ùúé, we use the notation ùúé({ùëñ1, . . . , ùëñùëò}) = {ùúé(ùëñ1), . . . , ùúé(ùëñùëò)} to denote an ordered
set that collects the action of the permutation on the elements (ùëñùëó). For example, if
ùëá: R4 ‚ÜíR4 is a ùúé-triangular map with ùúédefined as ùúé(N4) = {1, 4, 2, 3}, then ùëáwill
be of the form:
ùëá(ùë•) =
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
ùëá1(ùë•1)
ùëá2(ùë•1, ùë•4, ùë•2)
ùëá3(ùë•1, ùë•4, ùë•2, ùë•3)
ùëá4(ùë•1, ùë•4)
‚é§
‚é•‚é•‚é•‚é•‚é•‚é•‚é¶
(5.19)
for some collection (ùëáùëò). We regard each component ùëáùúé(ùëò) as a map Rùëò‚ÜíR. We say
that a ùúé-triangular function ùëáis monotone increasing if each component ùëáùëòis a mono-
tone increasing function of the input ùë•ùëò. Moreover, for any ùúàùúÇ, ùúàùúã‚ààM+(Rùëõ) and for
any permutation ùúéof Nùëõ, there exists a (ùúàùúÇ-unique) monotone increasing ùúé-triangular
127

map‚Äîwhich we call a ùúé-generalized KR rearrangement‚Äîthat pushes forward ùúàùúÇto
ùúàùúã. We give a constructive definition for a generalized KR rearrangement in Appendix
B.
A key property of a ùúé-generalized KR rearrangement is that it allows different
sparsity patterns to be engineered, depending on ùúé, in a map that is otherwise fully
general‚Äîin the sense of being able to couple arbitrary measures in M+(Rùëõ). This
feature will be essential to characterizing decomposable transport maps.
5.5.2
Decomposition and graph sparsification
We now characterize transports that decompose into a pair of low-dimensional maps,
as described in the following theorem. We formulate the theorem for a generic target
measure ùúàùëñ. Later we will apply the theorem recursively to a sequence (ùúàùëñ) of different
targets.
Theorem 5.2 (Decomposition of transport maps). Let ùëã‚àºùúàùúÇ, ùëçùëñ‚àºùúàùëñ, with
ùúàùúÇ, ùúàùëñ‚ààM+(Rùëõ) and ùúàùúÇtensor product measure. Denote by ùúÇ, ùúãùëña pair of nonvan-
ishing densities for ùúàùúÇand ùúàùúã, respectively, and assume that ùúàùëñfactorizes according to
a graph ùí¢ùëñ, which admits a proper decomposition (ùíú, ùíÆ, ‚Ñ¨). Then the following hold:
1. There exists a factorization of ùúãùëñof the form
ùúãùëñ(ùëß) = 1
c ùúìùíú‚à™ùíÆ(ùëßùíú‚à™ùíÆ) ùúìùíÆ‚à™‚Ñ¨(ùëßùíÆ‚à™‚Ñ¨),
(5.20)
where ùúìùíú‚à™ùíÆis strictly positive and integrable, with c =
‚à´Ô∏Ä
ùúìùíú‚à™ùíÆ.
2. For any factorization (5.20) and for any permutation ùúéof Nùëõwith
ùúé(ùëò) ‚àà
‚éß
‚é™
‚é™
‚é™
‚é™
‚é™
‚é®
‚é™
‚é™
‚é™
‚é™
‚é™
‚é©
ùíÆ
if ùëò= 1, . . . , |ùíÆ|
ùíú
if ùëò= |ùíÆ| + 1, . . . , |ùíú‚à™ùíÆ|
‚Ñ¨
otherwise,
(5.21)
there exists a nonempty family, Dùëñ, of decomposable transport maps ùëá= ùêøùëñ‚àòùëÖ
128

parameterized by ùëÖ‚ààRùëñsuch that each ùëá‚ààDùëñpushes forward ùúàùúÇto ùúàùëñand
where:
(a) ùêøùëñis a ùúé-generalized KR rearrangement that pushes forward ùúàùúÇto a mea-
sure with density ùúìùíú‚à™ùíÆ(ùëßùíú‚à™ùíÆ) ùúÇùëã‚Ñ¨(ùëß‚Ñ¨)/c and is low-dimensional with re-
spect to ‚Ñ¨.
(b) Rùëñis the set of maps Rùëõ‚ÜíRùëõthat are low-dimensional with respect to ùíú
and that push forward ùúàùúÇto the pullback ùêø‚ôØ
ùëñùúàùëñ‚ààM+(Rùëõ).
(c) If ùëçùëñ+1 ‚àºùêø‚ôØ
ùëñùúàùëñ, then ùëçùëñ+1
ùíú
‚ä•‚ä•ùëçùëñ+1
ùíÆ‚à™‚Ñ¨and ùëçùëñ+1
ùíú
= ùëãùíúin distribution.
(d) ùêø‚ôØ
ùëñùúàùëñfactorizes according to a graph ùí¢ùëñ+1 that can be derived from ùí¢ùëñas
follows:
‚Äì Remove any edge from ùí¢ùëñthat is incident to any node in ùíú.
‚Äì For any maximal clique ùíû‚äÇùíÆ‚à™‚Ñ¨with nonempty intersection ùíû‚à©ùíÆ,
let ùëóùíûbe the maximum integer ùëósuch that ùúé(ùëó) ‚ààùíû‚à©ùíÆand turn
ùíû‚à™{ùúé(1), . . . , ùúé(ùëóùíû)} into a clique.
We first look at the theorem for ùëñ= 1 and let ùúà1 := ùúàùúãand ùí¢1 := ùí¢, where
ùúàùúãdenotes our usual target measure with I-map ùí¢and where (ùíú, ùíÆ, ‚Ñ¨) denotes a
decomposition of ùí¢.
Among the infinitely many transport maps from ùúàùúÇto ùúàùúã, Theorem 5.2 identifies
a family of decomposable ones. The existence of these maps relies exclusively on the
Markov structure of ùúàùúã: we just require ùí¢to admit a (proper) decomposition.5
Each transport ùëá‚ààD1 pushes forward ùúàùúÇto ùúàùúãand is the composition of two
low-dimensional maps, i.e., ùëá= ùêø1 ‚àòùëÖfor a fixed ùêø1 defined in Theorem 5.2[Part
2a] and for some ùëÖ‚ààR1. (We also write D1 := ùêø1 ‚àòR1.6) The structure of these
low-dimensional maps is quite interesting. Up to a reordering of their components,
Theorem 5.2[Parts 2a and 2b] show that ùêø1 and ùëÖhave an intuitive complementary
5 To obtain a proper decomposition of ùí¢, one is free to add edges to ùí¢in order to turn the
separator set ùíÆinto a clique (see Definition 5.2); ùúàùúãstill factorizes according to any less sparse
version of ùí¢.
6 The notation here is intuitive: for a given ùëî: Rùëõ‚ÜíRùëõand for a given set of functions ‚Ñ±from
Rùëõto Rùëõ, ùëî‚àò‚Ñ±denotes the set of maps that can be written as ùëî‚àòùëìfor some ùëì‚àà‚Ñ±.
129

form:
ùêø1(ùë•) =
‚é°
‚é¢‚é¢‚é¢‚é£
ùêøùíú
1 (ùë•ùíÆ, ùë•ùíú)
ùêøùíÆ
1 (ùë•ùíÆ)
ùë•‚Ñ¨
‚é§
‚é•‚é•‚é•‚é¶,
ùëÖ(ùë•) =
‚é°
‚é¢‚é¢‚é¢‚é£
ùë•ùíú
ùëÖùíÆ(ùë•ùíÆ, ùë•‚Ñ¨)
ùëÖ‚Ñ¨(ùë•ùíÆ, ùë•‚Ñ¨)
‚é§
‚é•‚é•‚é•‚é¶.
(5.22)
(If ùíÆ= ‚àÖ, one can just remove ùêøùíÆ
1 and ùëÖùíÆfrom (5.22), and drop the dependence of
the remaining components on ùë•ùíÆ.) In particular, ùêø1 and ùëÖhave effective dimensions
bounded by |ùíú‚à™ùíÆ| and |ùíÆ‚à™‚Ñ¨| = |ùí±‚àñùíú|, respectively (see Definition 5.1). Even
though ùêø1 and ùëÖare low-dimensional maps, their composition is quite dense‚Äîin the
sense of Section 5.4‚Äîand is in general nontriangular:
ùëá(ùë•) = (ùêø1 ‚àòùëÖ)(ùë•) =
‚é°
‚é¢‚é¢‚é¢‚é£
ùêøùíú
1 ( ùëÖùíÆ(ùë•ùíÆ, ùë•‚Ñ¨), ùë•ùíú)
ùêøùíÆ
1 ( ùëÖùíÆ(ùë•ùíÆ, ùë•‚Ñ¨) )
ùëÖ‚Ñ¨(ùë•ùíÆ, ùë•‚Ñ¨)
‚é§
‚é•‚é•‚é•‚é¶,
(5.23)
and thus more difficult to represent and to work with. The key idea of decomposable
transports is that they can be represented implicitly through the composition of their
low-dimensional factors, similar to the way that direct transports can be represented
implicitly through their sparse inverses (Section 5.4).
The sparsity patterns of ùêø1 and ùëÖin (5.22) are needed for the theorem to hold.
In particular, ùêø1 must be a ùúé-triangular function with ùúéspecified by (5.21). Notice
that (5.21) does not prescribe an exact permutation, but just a few constraints on
a feasible ùúé. Intuitively, these constraints say that ùêø1 should be a function whose
components with indices in ùíÆdepend only on the variables in ùíÆ(whenever ùíÆÃ∏= ‚àÖ),
and whose components with indices in ùíúdepend only on the variables in ùíú‚à™ùíÆ.
Thus, there is usually some freedom in the choice of ùúé. Different permutations lead
to different families of decomposable transports, and can induce different sparsity
patterns in an I-map, ùí¢2, for ùêø‚ôØ
1 ùúàùúã(Theorem 5.2[Part 2d]).
Part 2d of the theorem shows how to derive a possible I-map ùí¢2‚Äînot necessarily
minimal‚Äîby performing a sequence of graph operations on ùí¢. There are two steps:
one that does not depend on ùúéand one that does. Let us focus first on the former:
130

the idea is to remove from ùí¢any edge that is incident to any node in ùíú, effectively
disconnecting ùíúfrom the rest of the graph. That is, if ùëç2 ‚àºùêø‚ôØ
1ùúàùúã, then, regardless
of ùúé, ùêø1 makes ùëç2
ùíúmarginally independent of ùëç2
ùíÆ‚à™‚Ñ¨by acting locally on ùí¢. And not
only that: ùêø1 also ensures that the marginals of ùúàùúÇand ùêø‚ôØ
1ùúàùúãagree along ùíú(see
Theorem 5.2[Part 2c]). Thus we should really interpret ùêø1 as the first step towards a
progressive transport of ùúàùúÇto ùúàùúã. ùêø1 is a local map: it can depend nontrivially only
upon variables in ùë•ùíú‚à™ùíÆ. Indeed, in the most general case, |ùíú‚à™ùíÆ| is the minimum
effective dimension of a low-dimensional map necessary to decouple ùíúfrom the rest
of the graph.
The more edges incident to ùíú, the higher-dimensional a transport
is needed.
This type of graph sparsification requires a peculiar ‚Äúblock triangular‚Äù
structure for ùêø1 as shown by (5.22): any ùúé-triangular function with ùúégiven by (5.21)
achieves this special structure. The second step of Part 2d shows that if ùíÆÃ∏= ‚àÖ, then
it might be necessary to add edges to the subgraph ùí¢ùíÆ‚à™‚Ñ¨, depending on ùúé.7 The
relevant aspect of ùúéfor this discussion is the definition of the permutation onto the
first |ùíÆ| integers. In general, there are |ùíÆ|! different permutations that could induce
different sparsity patterns in ùí¢2. We shall see that permutations that add the fewest
edges possible are of particular relevance.
5.5.3
Recursive decompositions
The sparsity of ùí¢2 is important because it affects the ‚Äúcomplexity‚Äù of the maps in
R1: each ùëÖ‚ààR1 pushes forward ùúàùúÇto ùêø‚ôØ
1 ùúàùúã. More specifically, by the previous
discussion, we can see how the role of each ùëÖ‚ààR1 is really only that of matching
the marginals of ùúàùúÇand ùêø‚ôØ
1 ùúàùúãalong ùí±‚àñùíú. A natural question then is whether we
can break this matching step into simpler tasks, or, in the language of this section,
whether R1 contains transports that are further decomposable. Intuitively, we are
seeking a finer-grained representation for some of the transports in R1. The following
lemma (for ùëñ= 1) provides a positive answer to this question as long as ùí±‚àñùíúis not
fully connected in ùí¢2. From now on, we denote (ùíú, ùíÆ, ‚Ñ¨) by (ùíú1, ùíÆ1, ‚Ñ¨1), since we
7 This is not always the case. For instance, if ùíÆis a subset of every maximal clique of ùí¢in ùíÆ‚à™‚Ñ¨
that has nonempty intersection with ùíÆ, then, by Theorem 5.2[Part 2d], no edges need to be added.
131

will be dealing with a sequence of different graph decompositions.
Lemma 5.3 (Recursive decompositions). Let ùúàùúÇ, ùúàùëñ, ùí¢ùëñbe defined as in the assump-
tions of Theorem 5.2 for a proper decomposition (ùíúùëñ, ùíÆùëñ, ‚Ñ¨ùëñ) of ùí¢ùëñ, while let ùí¢ùëñ+1 and
Dùëñ= ùêøùëñ‚àòRùëñbe the resulting graph (Part 2d) and family of decomposable transports,8
respectively. Then there are two possibilities:
1. ùíÆùëñ‚à™‚Ñ¨ùëñis not a clique in ùí¢ùëñ+1. In this case, it is possible to identify a proper
decomposition (ùíúùëñ+1, ùíÆùëñ+1, ‚Ñ¨ùëñ+1) of ùí¢ùëñ+1 for some ùíúùëñ+1 that is a strict superset
of ùíúùëñby (possibly) adding edges to ùí¢ùëñ+1 in order to turn ùíÆùëñ+1 into a clique.
Let Dùëñ+1 = ùêøùëñ+1 ‚àòRùëñ+1 be defined as in Theorem 5.2 for the pair of measures
ùúàùúÇ, ùúàùëñ+1 := ùêø‚ôØ
ùëñùúàùëñand (ùíúùëñ+1, ùíÆùëñ+1, ‚Ñ¨ùëñ+1). Then the following hold:
(a) Rùëñ‚äÉDùëñ+1 and ùêøùëñ‚àòRùëñ‚äÉùêøùëñ‚àòùêøùëñ+1 ‚àòRùëñ+1.
(b) ùêøùëñ+1 is low-dimensional with respect to ùíúùëñ‚à™‚Ñ¨ùëñ+1 and has effective dimen-
sion bounded by |(ùíúùëñ+1 ‚àñùíúùëñ) ‚à™ùíÆùëñ+1|.
(c) Each ùëÖ‚ààRùëñ+1 has effective dimension bounded by |ùí±‚àñùíúùëñ+1|.
2. ùíÆùëñ‚à™‚Ñ¨ùëñis a clique in ùí¢ùëñ+1. In this case, the decomposition of Part 1 does not
exist.
Lemma 5.3[Part 1] shows that if ùíÆ1 ‚à™‚Ñ¨1 is not fully connected in ùí¢2, then there
exists a proper decomposition (ùíú2, ùíÆ2, ‚Ñ¨2) of ùí¢2 (obtained, possibly, by adding edges
to ùí¢2 in ùí±‚àñùíú1) for which ùíú2 is a strict superset of ùíú1. One can then apply Theorem
5.2 for the pair ùúàùúÇ, ùúà2 = ùêø‚ôØ
1 ùúà1 and the decomposition (ùíú2, ùíÆ2, ‚Ñ¨2). As a result, Part
1a of the lemma shows that R1 contains a subset D2 = ùêø2 ‚àòR2 of decomposable
transport maps where both ùêø2 and each ùëÖ‚ààR2 are local transports on ùí±‚àñùíú1, i.e.,
they are both low-dimensional with respect to ùíú1. In particular, ùêø2 is responsible for
decoupling ùíú2 ‚àñùíú1 from the rest of the graph and for matching the marginals of ùúàùúÇ
and ùêø‚ôØ
2 ùêø‚ôØ
1 ùúàùúã= (ùêø1 ‚àòùêø2)‚ôØùúàùúãalong ùíú2 ‚àñùíú1. The effective dimension of ùêø2 is bounded
above by the size of the separator set ùíÆ2 plus the number of nodes in ùíú2 ‚àñùíú1 (Part 1b
8 Whenever we do not specify a permutation ùúéùëñor a factorization (5.20) in the definition of ùêøùëñ,
it means that the claim holds true for any feasible choice of these parameters.
132

of the lemma). The effective dimension of each ùëÖ‚ààR2 is bounded by the cardinality
of ùí±‚àñùíú2 and is, in the most general case, lower than that of the maps in R1 (Part
1c). Moreover, by Part 1a, D1 = ùêø1 ‚àòR1 ‚äÉùêø1 ‚àòùêø2 ‚àòR2, which means that among the
infinitely many decomposable transports that push forward ùúàùúÇto ùúàùúã, there exists at
least one that factorizes as the composition of three low-dimensional maps as opposed
to two, i.e., ùëá= ùêø1 ‚àòùêø2 ‚àòùëÖfor some ùëÖ‚ààR2. If, on the other hand, ùíÆ1 ‚à™‚Ñ¨1 is fully
connected in ùí¢2, then by Lemma 5.3[Part 2] we know that the decomposition of Part
1 does not exist. As a result, we cannot use Theorem 5.2 to prove the existence of
more finely decomposable transports in R1. In other words, if we want to match the
marginals of ùúàùúÇand ùêø‚ôØ
1 ùúàùúãalong ùí±‚àñùíú1 = ùíÆ1 ‚à™‚Ñ¨1, then we must do so in one shot,
using a single transport map; no more intermediate steps are feasible.
The main idea, then, is to apply Lemma 5.3[Part 1], recursively, ùëòtimes, where ùëò
is the first integer (possibly zero) for which ùíÆùëò+1 ‚à™‚Ñ¨ùëò+1 is a clique in ùí¢ùëò+2. After ùëò
iterations, the following inclusion must hold:
D1 = ùêø1 ‚àòR1 ‚äÉùêø1 ‚àò¬∑ ¬∑ ¬∑ ‚àòùêøùëò+1 ‚àòRùëò+1,
(5.24)
which shows that there exists a decomposable transport,
ùëá= ùêø1 ‚àò¬∑ ¬∑ ¬∑ ‚àòùêøùëò+1 ‚àòùëÖ,
(5.25)
for some ùëÖ‚ààRùëò+1, that pushes forward ùúàùúÇto ùúàùúã. (Note that we can apply Lemma
5.3[Part 1] only finitely many times since |ùí±‚àñùíúùëñ+1| is an integer function strictly
decreasing in ùëñand bounded away from zero.) Each ùêøùëñin (5.24) is a ùúéùëñ-triangular
map for some permutation ùúéùëñthat satisfies (5.25), and is low-dimensional with respect
to ùíúùëñ‚àí1 ‚à™‚Ñ¨ùëñ, i.e., for ùëñ> 1 and up to a permutation of its components,
ùêøùëñ(ùë•) =
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
ùë•ùíúùëñ‚àí1
ùêøùíúùëñ‚àñùíúùëñ‚àí1
ùëñ
(ùë•ùíÆùëñ, ùë•ùíúùëñ‚àñùíúùëñ‚àí1)
ùêøùíÆùëñ
ùëñ(ùë•ùíÆùëñ)
ùë•‚Ñ¨ùëñ
‚é§
‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é¶
.
(5.26)
133

The map ùëÖis low-dimensional with respect to ùíúùëò+1 and can also be chosen as a
generalized triangular function. Intuitively, we can think of ùêøùëñas decoupling nodes
in ùíúùëñ‚àñùíúùëñ‚àí1 from the rest of the graph in an I-map for (ùêø1 ‚àò¬∑ ¬∑ ¬∑ ‚àòùêøùëñ‚àí1)‚ôØùúàùúã. (Recall
that by Lemma 5.3 all the sets (ùíúùëñ) are nested, i.e., ùíú1 ‚äÇ¬∑ ¬∑ ¬∑ ‚äÇùíúùëò+1.) Figure 5-6
illustrates the mechanics underlying the recursive application of Lemma 5.3.
We emphasize that the existence and structure of (5.25) follow from simple graph
operations on ùí¢, and do not entail any actual computation with the target measure
ùúàùúã. Notice also that even if each map in the decomposition (5.24) is ùúé-triangular, the
resulting transport map ùëáneed not be triangular at all. In other words, we obtain
factorizations of general and possibly non-triangular transport maps in terms of low-
dimensional generalized triangular functions. In this sense, we can regard triangular
maps as a fundamental ‚Äúbuilding block‚Äù of a much larger class of transport maps.
Decomposable transports are clearly not unique.
In particular, there are two
factors that affect both the sparsity pattern and the number ùëòof composed maps in
the family ùêø1 ‚àò¬∑ ¬∑ ¬∑ ‚àòùêøùëò+1 ‚àòRùëò+1: the sequence of decompositions (ùíúùëñ, ùíÆùëñ, ‚Ñ¨ùëñ) and the
sequence of permutations (ùúéùëñ). Usually, there is a certain freedom in the choice of these
parameters, and each configuration might lead to a different family of decomposable
transports. Of course some families might be more desirable than others: ideally, we
would like the low-dimensional maps in the composition to have the smallest effective
dimension possible. Recall that by Lemma 5.3 the effective dimension of each ùêøùëñ
can be bounded above by |(ùíúùëñ‚àñùíúùëñ‚àí1) ‚à™ùíÆùëñ| (with the convention ùíú0 = ‚àÖ). Thus
we should intuitively choose a decomposition (ùíúùëñ, ùíÆùëñ, ‚Ñ¨ùëñ) of ùí¢ùëñand a permutation ùúéùëñ
for ùêøùëñthat minimize the cardinality of (ùíúùëñ‚àñùíúùëñ‚àí1) ‚à™ùíÆùëñ, and that, at the same time,
minimize the number of edges added from ùí¢ùëñto ùí¢ùëñ+1. In principle, we should also
account for the dimensions of all future maps in the recursion. In the most general
case, this graph theoretic question could be addressed using dynamic programming
[21]. In practice, however, we will often consider graphs for which a good sequence of
decompositions and permutations is rather obvious (see Section 5.6). For instance,
if the target distribution ùúàùúãfactorizes according to a tree ùí¢, then it is immediate
to show the existence of a decomposable transport ùëá= ùëá1 ‚àò¬∑ ¬∑ ¬∑ ‚àòùëáùëõ‚àí1 that pushes
134

forward ùúàùúÇto ùúàùúãand that factorizes as the composition of ùëõ‚àí1 low-dimensional
maps (ùëáùëñ)ùëõ‚àí1
ùëñ=1 , each associated to an edge of ùí¢: it suffices to consider a sequence of
decompositions (ùíúùëñ, ùíÆùëñ, ‚Ñ¨ùëñ) with ùíú1 ‚äÇùíú2 ‚äÇ¬∑ ¬∑ ¬∑ , where, for a given rooted version of
ùí¢, ùíúùëñ‚àñùíúùëñ‚àí1 consists of a single node ùëéùëñwith the largest depth in ùí¢ùí±‚àñùíúùëñ‚àí1, and where
ùíÆùëñcontains the unique parent of that node. Remarkably, each map ùëáùëñhas effective
dimension less than or equal to two, independent of ùëõ‚Äîthe size of the tree.
At this point, it might be natural to consider a junction tree decomposition of a
triangulation of ùí¢[156] as a convenient graphical tool to schedule the sequence of
decompositions (ùíúùëñ, ùíÆùëñ, ‚Ñ¨ùëñ) needed to apply Lemma 5.3 recursively. Decomposable
graphs are in fact ultimately chordal [161]. However, the situation might not be as
straightforward. The problem is that the clique structure of ùí¢ùëñ, an I-map for ùúàùëñ, can
be very different than that of ùí¢ùëñ+1, an I-map for ùêø‚ôØ
ùëñùúàùëñ; Theorem 5.2[Part 2d] shows
that ùí¢ùëñ+1 might contain larger maximal cliques than those in ùí¢ùëñ, even if ùí¢ùëñis chordal
(see Figure 5-6 for an example). Thus, working with a junction tree might require a
bit of extra care.
5.5.4
Computation of decomposable transports
Given the existence and structure of a decomposable transport like (5.25), what to
do with it? There are at least two possible ways of exploiting this type of informa-
tion. First, one could solve a variational problem like (5.5) and enforce an explicit
parameterization of the transport map as the composition ùëá= ùêø1 ‚àò¬∑ ¬∑ ¬∑ ‚àòùêøùëò+1 ‚àòùëÖ.
In this scenario, one need only parameterize the low-dimensional maps (ùêøùëñ, ùëÖ) and
optimize, jointly, over their composition. The advantage of this approach is that it
bypasses the parameterization of a single high-dimensional function, ùëá, altogether.
See the literature on normalizing flows [231] for possible computational ideas in this
direction.
An alternative‚Äîand perhaps more intriguing‚Äîpossibility is to compute the maps
(ùêøùëñ) sequentially by solving separate low-dimensional optimization problems‚Äîone for
each map ùêøùëñ. By Theorem 5.2[Part 2a] and Lemma 5.3, there exists a factorization
(5.20) of ùúãùëñ‚Äîa density of ùêø‚ôØ
ùëñ‚àí1 ùúàùëñ‚àí1‚Äîfor which ùêøùëñis a ùúéùëñ-generalized KR rearrangement
135

that pushes forward ùúàùúÇto a measure with density proportional to ùúìùíúùëñ‚à™ùíÆùëñùúÇùëã‚Ñ¨ùëñ, where
(ùíúùëñ, ùíÆùëñ, ‚Ñ¨ùëñ) is a decomposition of ùí¢ùëñand ùí¢ùëñis an I-map for ùúàùëñ. In general ùúìùíúùëñ‚à™ùíÆùëñ
depends on ùêøùëñ‚àí1, and so the maps (ùêøùëñ) must be computed sequentially.9 In essence,
decomposable transports break the inference task into smaller and possibly easier
steps.
Note that we could define ùêøùëñwith respect to any factorization (5.20) with ùúìùíúùëñ‚à™ùíÆùëñ
integrable: these different factorizations would lead to a family of decomposable trans-
ports with the same low-dimensional structure and sparsity patterns (as predicted by
Theorem 5.2). Thus, as long as we have access to a sequence of integrable factors
(ùúìùíúùëñ‚à™ùíÆùëñ), we can compute each map ùêøùëñindividually by solving a low-dimensional
optimization problem. (See Appendix B for computational remarks on generalized
triangular functions.) Intuitively, since by Lemma 5.3[Part 1b] ùêøùëñis low-dimensional
with respect to ùíúùëñ‚àí1 ‚à™‚Ñ¨ùëñ, we really only need to optimize for a portion of the map,
namely ùêøùíû
ùëñfor ùíû= (ùíúùëñ‚àñùíúùëñ‚àí1)‚à™ùíÆùëñ, which can be regarded effectively as a multivariate
map on R|ùíû|. In the same way, the map ùëÖcan be computed as any transport (pos-
sibly triangular) that pushes forward ùúàùúÇto ùêø‚ôØ
ùëò+1 ùúàùëò+1. Theorem 5.2[Part 2b] tells us
that once again we only need to optimize for a low-dimensional portion of the map,
namely, ùëÖùíÆùëò+1‚à™‚Ñ¨ùëò+1.
While it might be difficult to access a sequence of factorizations (5.20) for a general
problem, there are important applications, such as Bayesian filtering, smoothing, and
joint parameter/state estimation, where the sequential computation of the transports
(ùêøùëñ, ùëÖ) is always possible by construction. We discuss these applications in the next
section.
9 This is not always the case. For instance, given a rooted version of ùí¢and a pair of consecutive
depths (see the discussion at the end of Section 5.5.3), all the maps (ùêøùëñ) associated with edges
connecting nodes at these two depths can be computed in parallel.
136

ùíú1
‚Ñ¨1
ùíÆ1
2
3
1
6
4
5
ùíú1
‚Ñ¨1
ùíÆ1
2
3
1
6
4
5
ùíú2
‚Ñ¨2
ùíÆ2
2
3
1
6
4
5
ùíú2
‚Ñ¨2
ùíÆ2
2
3
1
6
4
5
Figure 5-6:
Sequence of graph decompositions associated with the recursive applica-
tion of Lemma 5.3. At (top left) is an I-map, ùí¢1, for ùúàùúã, with ùúàùúã‚ààM+(R6). We first
decompose this graph into (ùíú1, ùíÆ1, ‚Ñ¨1) as indicated, and apply Theorem 5.2 to the
pair ùúàùúÇ, ùúàùúã. To do so, we first need to add edge (2, 3) to ùí¢1 in order to turn (ùíú1, ùíÆ1, ‚Ñ¨1)
into a proper decomposition of ùí¢1 with a fully connected ùíÆ1.The resulting graph, ùí¢1
‚ãÜ,
is now chordal (in fact, a triangulation of ùí¢1 [161]), but still an I-map for ùúàùúã. The
first map ùêø1 is ùúé1-triangular with ùúé1(N6) = {2, 3, 1, 4, 5, 6} and it is low-dimensional
with respect to ‚Ñ¨1; The (top right) figure shows the I-map, ùí¢2, for ùêø‚ôØ
1 ùúàùúãas given
by Theorem 5.2[Part 2d]: as expected, ùíú1 is disconnected from ùíÆ1 ‚à™‚Ñ¨1; moreover,
a new maximal clique {2, 3, 4, 5} appears in ùí¢2. This new clique is larger than any
of the maximal cliques in ùí¢1
‚ãÜ, even though ùí¢1
‚ãÜis chordal. (Notice that ùúé1 is not the
permutation that adds the fewest edges possible in ùí¢2. An example of such ‚Äúbest‚Äù
permutation would be ùúé(N6) = {3, 2, 1, 4, 5, 6}.) Though Theorem 5.2 guarantees the
existence of a low-dimensional map ùëÖ‚ààR1 that pushes forward ùúàùúÇto ùêø‚ôØ
1 ùúàùúã, we in-
stead proceed recursively by applying Lemma 5.3[Part 1] for a proper decomposition,
(ùíú2, ùíÆ2, ‚Ñ¨2), of ùí¢2, where ùíú2 is a strict superset of ùíú1 (bottom left). The lemma
shows that R1 ‚äÉùêø2 ‚àòR2 for some ùúé2-triangular map ùêø2, which is low-dimensional
with respect to ùíú1 ‚à™‚Ñ¨2, and where each ùëÖ‚ààR2 pushes forward ùúàùúÇto (ùêø1 ‚àòùêø2)‚ôØùúàùúã.
Can we apply Lemma 5.3 one more time to characterize decomposable transports in
R2? The answer is no, as the I-map for (ùêø1 ‚àòùêø2)‚ôØùúàùúã(bottom right) consists of a single
clique in ùíÆ2 ‚à™‚Ñ¨2. Nevertheless, each ùëÖ‚ààR2 is still low-dimensional with respect
to ùíú2. Overall, we showed the existence of a transport map ùëá: R6 ‚ÜíR6 pushing
forward ùúàùúÇto ùúàùúãthat decomposes as ùëá= ùêø1 ‚àòùêø2 ‚àòùëÖ, where ùêø1, ùêø2, ùëÖare effectively
{3, 4, 3}-dimensional maps, respectively.
137

5.6
Sequential inference on state-space models: vari-
ational algorithms
In this section, we consider the problem of sequential Bayesian inference (or discrete-
time data assimilation [162, 230, 245]) for continuous, nonlinear, and non-Gaussian
state-space models.
Our goal is to specialize the theory developed in Section 5.5 to the solution of
Bayesian filtering and smoothing problems. The key result of this section is a new
variational algorithm for characterizing the full posterior distribution of the sequential
inference problem‚Äîe.g., not just a few filtering or smoothing marginals‚Äîvia recursive
lag‚Äì1 smoothing with transport maps. The proposed algorithm builds a decomposable
high-dimensional transport map in a single forward pass by solving a sequence of local
small-dimensional problems, without resorting to any backward pass on the state
space model (see Theorem 5.3). These results extend naturally to the case of joint
parameter and state estimation (see Section 5.6.4 and Theorem 5.4).
A state-space model consists of a pair of discrete-time stochastic processes (ùëçùëò, ùëåùëò)ùëò‚â•0
indexed by the time ùëò, where (ùëçùëò) is a latent Markov process of interest and where
(ùëåùëò) is the observed process. We can think of each ùëåùëòas a noisy and perhaps indi-
rect measurement of ùëçùëò. The Markov structure corresponding to the joint process
(ùëçùëò, ùëåùëò) is shown in Figure 5-7. The generalization of the results of this section to
the case of missing observations is straightforward and will not be addressed here (in
the interest of a concise formulation of our theorems).
We assume that we are given the transition densities ùúãùëçùëò+1|ùëçùëòfor all ùëò‚â•0,
sometimes referred to as the ‚Äúprior dynamic,‚Äù together with the marginal density
of the initial conditions ùúãùëç0. (For instance, the prior dynamic could stem from the
discretization of a continuous time stochastic differential equation [199].) We denote
by ùúãùëåùëò|ùëçùëòthe likelihood function, i.e., the density of ùëåùëògiven ùëçùëò, and assume that
ùëçùëòand ùëåùëòare random variables taking values on Rùëõand Rùëë, respectively. Moreover,
we denote by (ùë¶ùëò)ùëò‚â•0 a sequence of realizations of the observed process (ùëåùëò) that will
define the posterior distribution over the unobserved (hidden) states of the model,
138

and make the following regularity assumption in our theorems: ùúãùëç0:ùëò‚àí1,ùëå0:ùëò‚àí1 > 0 for
all ùëò‚â•1. (The existence of underlying fully supported measures will be left implicit
throughout the section for notational convenience.)
ùëç0
ùëç1
ùëç2
ùëç3
ùëçùëÅ
ùëå0
ùëå1
ùëå2
ùëå3
ùëåùëÅ
ùëã0
ùëã1
ùëã2
ùëã3
ùëãùëÅ
Figure 5-7:
(above) I-map for the joint process (ùëçùëò, ùëåùëò)ùëò‚â•0 defining the state-space
model. (below) I-map for the independent reference process (ùëãùëò)ùëò‚â•0 used in Theorem
5.3.
5.6.1
Smoothing and filtering: the full Bayesian solution
In typical applications of state-space modeling, the process (ùëåùëò) is only observed
sequentially, and thus the goal of inference is to characterize‚Äîsequentially in time
and via a recursive algorithm‚Äîthe joint distribution of the current and past states
given currently available measurements, i.e.,
ùúãùëç0:ùëò|ùë¶0:ùëò(ùëß0:ùëò) := ùúãùëç0:ùëò|ùëå0:ùëò(ùëß0:ùëò|ùë¶0:ùëò)
(5.27)
for all ùëò‚â•0. That is, we wish to characterize ùúãùëç0:ùëò|ùë¶0:ùëòbased on our knowledge of the
posterior distribution at the previous timestep, ùúãùëç0:ùëò‚àí1|ùë¶0:ùëò‚àí1, and with an effort that
is constant over time. We regard (5.27) as the full Bayesian solution to the sequential
inference problem [245].
Usually, the task of updating ùúãùëç0:ùëò‚àí1|ùë¶0:ùëò‚àí1 to yield ùúãùëç0:ùëò|ùë¶0:ùëòbecomes increasingly
challenging over time due to the widening inference horizon, making characterization
of the full Bayesian solution impractical for large ùëò[245]. Thus, two simplifications of
the sequential inference problem are frequently considered: filtering and smoothing
[245].
In filtering, we characterize ùúãùëçùëò|ùë¶0:ùëòfor all ùëò‚â•0, while in smoothing we
139

recursively update ùúãùëçùëó|ùë¶0:ùëòfor increasing ùëò> ùëó, where ùëçùëóis some past state of the
unobserved process. Both filtering and smoothing deliver particular low-dimensional
marginals of the full Bayesian solution to the inference problem, and hence are often
considered good candidates for numerical approximation [87, 245, 70].
The following theorem shows that characterizing the full Bayesian solution to
the sequential inference problem via a decomposable transport map is essentially no
harder than performing lag‚Äì1 smoothing, which, in turn, amounts to characterizing
ùúãùëçùëò‚àí1,ùëçùëò|ùë¶0:ùëòfor all ùëò‚â•0 (an operation only incrementally harder than regular filter-
ing). This result relies on the recursive application of the decomposition theorem for
couplings (Theorem 5.2) to the tree Markov structure of ùúãùëç0:ùëò|ùë¶0:ùëò. In what follows,
let (ùëãùëò)ùëò‚â•0 be an independent (reference) process with nonvanishing marginal den-
sities (ùúÇùëãùëò), with each ùëãùëòtaking values on Rùëõ. See Figure 5-7 for the corresponding
Markov network.
Theorem 5.3 (Decomposition theorem for state-space models). Let (Mùëñ)ùëñ‚â•0 be a
sequence of (ùúéùëñ)-generalized KR rearrangements on Rùëõ√ó Rùëõ, which are of the form
Mùëñ(ùë•ùëñ, ùë•ùëñ+1) =
‚é°
‚é£M0
ùëñ(ùë•ùëñ, ùë•ùëñ+1)
M1
ùëñ(ùë•ùëñ+1)
‚é§
‚é¶
(5.28)
for some ùúéùëñ, M0
ùëñ: Rùëõ√ó Rùëõ‚ÜíRùëõ, M1
ùëñ: Rùëõ‚ÜíRùëõ, and that are defined by the
recursion:
‚Äì M0 pushes forward ùúÇùëã0,ùëã1 to ùúã0 = ÃÉÔ∏Äùúã0/c0,
‚Äì Mùëñpushes forward ùúÇùëãùëñ,ùëãùëñ+1 to ùúãùëñ(ùëßùëñ, ùëßùëñ+1) = ùúÇùëãùëñ(ùëßùëñ) ÃÉÔ∏Äùúãùëñ(M1
ùëñ‚àí1(ùëßùëñ), ùëßùëñ+1)/cùëñ,
where cùëñis a normalizing constant and where (ÃÉÔ∏Äùúãùëñ)ùëñ‚â•0 are functions on Rùëõ√ó Rùëõgiven
by:
‚Äì ÃÉÔ∏Äùúã0(ùëß0, ùëß1) = ùúãùëç0,ùëç1(ùëß0, ùëß1) ùúãùëå0|ùëç0(ùë¶0|ùëß0) ùúãùëå1|ùëç1(ùë¶1|ùëß1),
‚Äì ÃÉÔ∏Äùúãùëñ(ùëßùëñ, ùëßùëñ+1) = ùúãùëçùëñ+1|ùëçùëñ(ùëßùëñ+1|ùëßùëñ) ùúãùëåùëñ+1|ùëçùëñ+1(ùë¶ùëñ+1|ùëßùëñ+1) for ùëñ‚â•1.
Then, for all ùëò‚â•0, the following hold:
140

1. The map M1
ùëòpushes forward ùúÇùëãùëò+1 to ùúãùëçùëò+1|ùë¶0:ùëò+1.
[filtering]
2. The map Mùëò, defined as (M1
ùëò‚àí1(ùë•) = ùë•for ùëò= 0)
Mùëò(ùë•ùëò, ùë•ùëò+1) =
‚é°
‚é£M1
ùëò‚àí1(M0
ùëò(ùë•ùëò, ùë•ùëò+1))
M1
ùëò(ùë•ùëò+1)
‚é§
‚é¶,
(5.29)
pushes forward ùúÇùëãùëò,ùëãùëò+1 to ùúãùëçùëò,ùëçùëò+1|ùë¶0:ùëò+1.
[lag‚Äì1 smoothing]
3. The composition of transport maps Tùëò= ùëá0 ‚àò¬∑ ¬∑ ¬∑ ‚àòùëáùëò, where each ùëáùëñis defined
as
ùëáùëñ(ùë•0, . . . , ùë•ùëò+1) =
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
ùë•0
...
ùë•ùëñ‚àí1
M0
ùëñ(ùë•ùëñ, ùë•ùëñ+1)
M1
ùëñ(ùë•ùëñ+1)
ùë•ùëñ+2
...
ùë•ùëò+1
‚é§
‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é¶
,
(5.30)
pushes forward ùúÇùëã0:ùëò+1 to ùúãùëç0:ùëò+1|ùë¶0:ùëò+1.
[full Bayesian solution]
4. The model evidence (marginal likelihood) is given by
ùúãùëå0:ùëò+1(ùë¶0:ùëò+1) =
ùëò
‚àèÔ∏Å
ùëñ=0
cùëñ.
(5.31)
Theorem 5.3 suggests a variational algorithm for smoothing and filtering a contin-
uous state-space model: compute the sequence of maps (Mùëñ), each of dimension 2ùëõ;
embed them into higher-dimensional identity maps to form (ùëáùëñ) according to (5.30);
then evaluate the composition Tùëò= ùëá0 ‚àò¬∑ ¬∑ ¬∑ ‚àòùëáùëòto sample directly from ùúãùëç0:ùëò+1|ùë¶0:ùëò+1
(i.e., the full Bayesian solution) and obtain information about any smoothing or fil-
tering distribution of interest.
Successive transports in the composition (Tùëò)ùëò‚â•0 are nested and thus ideal for
141

sequential assimilation: given Tùëò‚àí1, we can obtain Tùëòsimply by computing an ad-
ditional map Mùëòof dimension 2ùëõ‚Äîwith no need to recompute (Mùëñ)ùëñ<ùëò. This step
converts a transport map that samples ùúãùëç0:ùëò|ùë¶0:ùëòinto one that samples ùúãùëç0:ùëò+1|ùë¶0:ùëò+1.
This feature is quite remarkable since Mùëòis always a 2ùëõ-dimensional map, while
ùúãùëç0:ùëò+1|ùë¶0:ùëò+1 is a density on Rùëõ(ùëò+2)‚Äîa space whose dimension increases with time
ùëò. In fact, from the perspective of Section 5.5, Theorem 5.3 simply shows that each
ùúãùëç0:ùëò+1|ùë¶0:ùëò+1 can be represented via a decomposable transport Tùëò= ùëá0 ‚àò¬∑ ¬∑ ¬∑ ‚àòùëáùëò. The
sparsity pattern of each map Mùëñ, specified in (5.28), is necessary for Theorem 5.3 to
hold: Mùëñcannot be any transport map; it must be block upper triangular.
The proposed algorithm consists of a forward pass on the stace-space model‚Äî
wherein the sequence of transport maps (Mùëñ) are computed and stored‚Äîfollowed by
a backward pass where the composition Tùëò= ùëá0‚àò¬∑ ¬∑ ¬∑‚àòùëáùëòis evaluated deterministically
to sample ùúãùëç0:ùëò+1|ùë¶0:ùëò+1. This backward pass does not re-evaluate the potentials of the
state-space model (e.g., transition kernels or likelihoods) at earlier times, nor does
it perform any additional computation other than evaluating the maps (Mùëñ) in Tùëò.
(See [151, 87, 109] for alternative approximations of the forward‚Äìfiltering backward‚Äì
smoothing formulas.)
Though each map ùëáùëóis usually trivial to evaluate‚Äîe.g., the map might be pa-
rameterized in terms of polynomials [187] and differ from the identity along only 2ùëõ
components‚Äîit is true that the cost of evaluating Tùëògrows linearly with ùëò. This is
hardly surprising since ùúãùëç0:ùëò+1|ùë¶0:ùëò+1 is a density over spaces of increasing dimension.
A direct approximation of Tùëòis usually a bad idea since the map is high-dimensional
and dense (in the sense defined by Section 5.5); it is better to store Tùëòimplicitly
through the sequence of maps (Mùëñ)ùëò
ùëñ‚â•0, and sample smoothed trajectories by evalu-
ating Tùëòonly when it is needed. If we are only interested in a particular smoothing
marginal, e.g., ùúãùëç0|ùë¶0:ùëò+1 for all ùëò‚â•0, then we can define a general forward recursion
to sample ùúãùëç0|ùë¶0:ùëò+1 with a single transport map that is updated recursively over time,
rather than with a growing composition of maps‚Äîand thus with a cost independent
of ùëò. This construction is given in Section 5.6.5.
Also, it is important to emphasize that in order to assimilate a new measurement,
142

say ùë¶ùëò+1, we do not need to evaluate the full composition Tùëò‚àí1; we only need to com-
pute a low-dimensional map Mùëòwhose target density ùúãùëòdepends only on Mùëò‚àí1. The
previous maps (Mùëñ)ùëñ<ùëò‚àí1 are unnecessary at this stage. Thus the effort of assimilating
a new piece of data is constant in time‚Äîmodulo the complexity of each Mùëò.
The distribution ùúãùëç0:ùëò+1|ùë¶0:ùëò+1 is not represented via a collection of particles as
ùëò‚â•0 increases, but rather via a growing composition of low-dimensional transport
maps that yields fully supported approximations of ùúãùëç0:ùëò+1|ùë¶0:ùëò+1.
The maps are com-
puted via deterministic optimization: there are no importance sampling or resampling
steps. Intuitively, the optimization step for Mùëòmoves the particles on which the map
is evaluated, rather than reweighing them.
(Think of the assimilation step of an
ensemble Kalman filter [92, 93] as a particular example with linear maps.)
Part 1 of Theorem 5.3 shows that the lower subcomponent M1
ùëò: Rùëõ‚ÜíRùëõof the
map Mùëòcharacterizes the filtering distribution ùúãùëçùëò+1|ùë¶0:ùëò+1 for all ùëò‚â•0, while Part 2
shows that each Mùëòalso characterizes the lag‚Äì1 smoothing distribution ùúãùëçùëò,ùëçùëò+1|ùë¶0:ùëò+1
up to an invertible transformation of the marginal over ùëçùëò.
Thus, Theorem 5.3
implicitly suggests a deterministic algorithm for lag‚Äì1 smoothing that in fact fully
characterizes the posterior distribution of the nonlinear state-space model‚Äîmuch in
the same spirit as the Rauch-Tung-Striebel (RTS) smoothing algorithm for Gaussian
models. We clarify this connection in Section 5.6.2.
The maps (Mùëñ) must be approximated numerically, in general (see Section 5.2).
As a result, Monte Carlo estimators associated with the evaluation of Tùëò= ùëá0‚àò¬∑ ¬∑ ¬∑‚àòùëáùëò
are biased, although possibly with negligible variance, since it is trivial to evaluate the
map a large number of times. This bias is only due to the numerical approximation of
(Mùëñ), and not to the particular factorization properties of Tùëò. In practice, one might
either accept this bias or try to reduce it. The bias can be reduced in at least two
ways: (1) by enriching the parameterization of some (Mùëñ), and thus by increasing the
accuracy of these maps via optimization, or (2) by using the map-induced proposal
density (Tùëò)‚ôØùúÇùëã0:ùëò+1‚Äîi.e., the pushforward of a marginal of the reference process
through Tùëò‚Äîwithin the context of importance sampling (IS) or MCMC (see Section
143

5.8 ). For instance, the weight function
ùë§ùëò+1(ùë•) = ùúãùëç0:ùëò+1|ùë¶0:ùëò+1(ùë•)
(Tùëò)‚ôØùúÇùëã0:ùëò+1(ùë•)
(5.32)
is readily available, and can be used to yield consistent estimators with respect to the
smoothing distribution. However, the resulting weights cannot be computed recur-
sively in time, because even though the small dimensional maps Mùëòare computed
sequentially, the map-induced proposal (Tùëò)‚ôØùúÇùëã0:ùëò+1 changes entirely at every step.
In particle filters, the complexity of an approximation of the underlying distribu-
tion is given by the number ùëÅof particles. In the proposed variational approach,
the complexity of the approximation depends on the parameterization of each map
Mùëñ. There is no single parameter like ùëÅto describe the complexity of the latter,
though, broadly, it should depend on the number of degrees of freedom in the param-
eterization. In some cases, one might think of using the total order of a multivariate
polynomial expansion of each component of the map as a tuning parameter. But this
is far from general or practical in high dimensions. The virtue of a functional repre-
sentation of the transport map is the ability to carefully select the degrees of freedom
of the parameterization. For instance, we might model local interactions between dif-
ferent groups of input variables using different approximation orders or even different
sets of basis functions. This freedom should not be frightening, but rather should be
embraced as a great opportunity to exploit the structure of the particular problem at
hand. (See Chapter 6 for an example of this fact in the context of high-dimensional
filtering of spatio-temporal processes.)
As shown in (5.8), the computation of each Mùëñis also associated with an approx-
imation of the normalizing constant cùëñof its own target density, which then leads to
a one-pass approximation of the marginal likelihood using (5.31).
One last remark: the proof of Theorem 5.3 shows that the triangular structure
hypothesis for each Mùëñcan be relaxed provided that the underlying densities are
regular enough. The following corollary clarifies this point.
Corollary 5.1. The results of Theorem 5.3 still hold if we replace every KR rear-
144

rangement Mùëñwith a ‚Äúblock triangular‚Äù diffeomorphism of the form (5.28) that couples
the same distributions, provided that such regular transport maps exist.
Filtering and smoothing are of course very rich problems, and in this section we
have by no means attempted to be exhaustive. Rather, our goal was to highlight
some implications of decomposable transports on problems of sequential Bayesian
inference, in a general non-Gaussian setting. See Section 6 for additional topics in
high-dimensional filtering.
5.6.2
The linear Gaussian case: connection with the RTS smoother
In this section, we specialize the results of Theorem 5.3 to linear Gaussian state-space
models, and make explicit the connection with the RTS Gaussian smoother [226].
Consider a linear Gaussian state-space model defined by
ùëçùëò+1
=
ùêπùëòùëçùëò+ ùúÄùëò
(5.33)
ùëåùëò
=
ùêªùëòùëçùëò+ ùúâùëò
for all ùëò‚â•0, where ùúÄùëò‚àºùí©(0, ùëÑùëò), ùúâùëò‚àºùí©(0, ùëÖùëò), ùêπùëò‚ààRùëõ√óùëõ, ùêªùëò‚ààRùëë√óùëõ, and
ùëç0 ‚àºùí©(ùúá0, Œì0). Both ùúÄùëòand ùúâùëòare independent of ùëçùëò, while ùëÑùëò, ùëÖùëò, and Œì0 are
symmetric positive definite matrices for all ùëò‚â•0.
If we choose an independent reference process (ùëãùëò) with standard normal marginals,
i.e., ùúÇùëãùëò= ùí©(0, I), then the maps (Mùëò) of Theorem 5.3 can be chosen to be linear:
Mùëò(ùëßùëò, ùëßùëò+1) =
‚é°
‚é£ùê¥ùëò
ùêµùëò
0
ùê∂ùëò
‚é§
‚é¶
‚éß
‚é®
‚é©
ùëßùëò
ùëßùëò+1
‚é´
‚é¨
‚é≠+
‚éß
‚é®
‚é©
ùëéùëò
ùëêùëò
‚é´
‚é¨
‚é≠,
(5.34)
for some matrices ùê¥ùëò, ùêµùëò, ùê∂ùëò‚ààRùëõ√óùëõand ùëéùëò, ùëêùëò‚ààRùëõ. (Notice that in this case
Corollary 5.1 applies and the matrices ùê¥ùëò, ùêµùëòcan be full and not necessarily trian-
gular.) The following lemma gives a closed form expression for the maps (Mùëò) with
ùëò‚â•1. (M0 can be derived analogously with simple algebra.)
145

Lemma 5.4 (The linear Gaussian case). For ùëò‚â•1, the map Mùëòin (5.34) can be
defined as follows: if (ùëêùëò, ùê∂ùëò) is the output of a square-root Kalman filter at time ùëò
[28], i.e., if ùëêùëòand ùê∂ùëòare, respectively, the mean and square root of the covariance
of the filtering distribution ùúãùëçùëò+1|ùë¶0:ùëò+1, then one can set:
ùê¥ùëò
=
ùêΩ‚àí1/2
ùëò
(5.35)
ùêµùëò
=
‚àíùêΩ‚àí1
ùëò
ùëÉùëòùê∂ùëò
ùëéùëò
=
ùêΩ‚àí1
ùëò
ùëÉùëò(ùêπùëòùëêùëò‚àí1 ‚àíùëêùëò),
for ùêΩùëò:= I + ùê∂‚ä§
ùëò‚àí1 ùêπ‚ä§
ùëòùëÑ‚àí1
ùëòùêπùëòùê∂ùëò‚àí1 and ùëÉùëò= ‚àíùê∂‚ä§
ùëò‚àí1 ùêπ‚ä§
ùëòùëÑ‚àí1
ùëò.
The formulas in Lemma 5.4 can be interpreted as one possible implementation
of a square-root RTS smoother for Gaussian models: at each step ùëòof a forward
pass, the filtering estimates (ùëêùëò, ùê∂ùëò) are augmented with a collection (ùëéùëò, ùê¥ùëò, ùêµùëò)
of stored quantities, which can then be reused to sample the full Bayesian solution
(or particular smoothing marginals) whenever needed, and without ever touching
the state-space model again. In this sense, the algorithm proposed in Section 5.6.1
can be understood as the natural generalization‚Äîto the non-Gaussian case‚Äîof the
square-root RTS smoother.
5.6.3
Transport maps in filtering: examples in the literature
In this section we briefly review few approaches to filtering that rely explicitly on the
construction of couplings.
One of the earliest contributions using optimal transport in filtering is [228]. In
[228], the author proposes to construct an optimal transport plan between the empir-
ical approximation of the forecast distribution given by simulating the prior dynamic
and a corresponding empirical approximation of the filtering distribution obtained
by reweighing the forecast ensemble according to the likelihood. The cost function
used to define the optimal plan is given by the squared Euclidean distance. Thus,
[228] solves a discrete Kantorovich optimal transport problem (e.g., via the auction
146

algorithm [22]) instead of a continuous one for a transport map (cf. Section 5.6.1),
and then derives a linear update for the forecast ensemble from the optimal plan.
The linear update is chosen so that the mean of the transformed samples matches
the mean of the empirical approximation of the filtering distribution. Higher-order
schemes are also possible [80]. The resulting algorithm moves forecast particles much
in the same way as the EnKF does, but with important differences: (1) the analysis
ensemble is constrained to the convex hull of the forecast ensemble, and (2) under cer-
tain assumptions, the resulting filtering estimates are consistent. In [228], the explicit
construction of couplings is only used to update the forecast distribution, instead of
the previous filtering marginal (cf. Section 5.6.1). In some sense, the approach of
[228] is closer in spirit to our treatment of high-dimensional filtering by means of
local couplings (as presented in Chapter 6).
In Section 5.6.1 we considered filtering by means of block triangular Knothe-
Rosenblatt (KR) rearrangements. A recent work that relies on the notion of KR
rearrangement for the purpose of sequential inference is [126]. In [126] the authors
define a proposal for SMC (or MCMC) methods using the solution map of a dis-
cretized ordinary differential equation (ODE) whose drift term depends only on the
full conditionals of the target distribution, hence the name Gibbs flow. The inte-
gration of the flow requires only the evaluation of one-dimensional integrals, and the
action of the solution map defines implicitly a transport map, without the need to
parameterize explicitly the transformation. The authors show that that the proposed
flow is an approximation of a more general ODE, which can be thought of as the flow
analogue of the KR rearrangement. The approximation is obtained by replacing the
conditionals that usually define the KR rearrangement with full conditionals. Ap-
proaches that rely on the solution of different ODEs inspired by mass transportation
ideas are also available in the literature, e.g., [78, 278].
Another related methodology is certainly implicit sampling for particle filters [58].
We discuss implicit sampling in Section 6 of [187], together with many other ap-
proaches to inference that leverage the notion of transportation of probability mass
and the explicit construction of couplings.
147

In the next section, we look at the role of decomposable couplings for a slightly
more general problem: sequential parameter-state estimation in non-Gaussian state-
space models.
5.6.4
Sequential joint parameter and state estimation
In defining a state-space model, it is common to parameterize the transition densities
of the unobserved process or the likelihoods of the observables in terms of some
hyperparameters Œò.
The Markov structure of the resulting Bayesian hierarchical
model, conditioned on the data, is shown in Figure 5-8. The state-space model is
now fully specified in terms of the conditional densities (ùúãùëåùëò|ùëçùëò,Œò)ùëò‚â•0, (ùúãùëçùëò+1|ùëçùëò,Œò)ùëò‚â•0,
ùúãùëç0|Œò, and the marginal ùúãŒò. We assume that the hyperparameters Œò take values
on Rùëù, and that the following regularity conditions hold: ùúãŒò,ùëç0:ùëò‚àí1,ùëå0:ùëò‚àí1 > 0 for all
ùëò‚â•1.
Given such a parameterization, one often wishes to jointly infer the hidden states
and the hyperparameters of the model as observations of the process (ùëåùëò) become
available. That is, the goal of inference is to characterize, via a recursive algorithm,
the sequence of posterior distributions given by
ùúãŒò,ùëç0:ùëò|ùë¶0:ùëò(ùëßùúÉ, ùëß0:ùëò) := ùúãŒò,ùëç0:ùëò|ùëå0:ùëò(ùëßùúÉ, ùëß0:ùëò|ùë¶0:ùëò)
(5.36)
for all ùëò‚â•0 and for a sequence (ùë¶ùëò)ùëò‚â•0 of observations. The following theorem shows
that we can characterize (5.36) by computing a sequence of low-dimensional transport
maps in the same spirit as Theorem 5.3. In what follows, let (ùëãùëò) be an independent
process with marginals (ùúÇùëãùëò) as defined in Theorem 5.3 and let ùëãŒò be a random
variable on Rùëùthat is independent of (ùëãùëò) and with nonvanishing density ùúÇùëãŒò.
ùëç0
ùëç1
ùëç2
ùëç3
ùëçùëÅ
Œò
Figure 5-8:
I-map for ùúãŒò,ùëç0,...,ùëçùëÅ|ùë¶0,...,ùë¶ùëÅ, for any ùëÅ> 0.
148

Theorem 5.4 (Decomposition theorem for joint parameter and state estimation).
Let (Mùëñ)ùëñ‚â•0 be a sequence of (ùúéùëñ)-generalized KR rearrangements on Rùëù√ó Rùëõ√ó Rùëõ,
which are of the form
Mùëñ(ùë•ùúÉ, ùë•ùëñ, ùë•ùëñ+1) =
‚é°
‚é¢‚é¢‚é¢‚é£
MŒò
ùëñ(ùë•ùúÉ)
M0
ùëñ(ùë•ùúÉ, ùë•ùëñ, ùë•ùëñ+1)
M1
ùëñ(ùë•ùúÉ, ùë•ùëñ+1)
‚é§
‚é•‚é•‚é•‚é¶
(5.37)
for some ùúéùëñ, MŒò
ùëñ: Rùëù‚ÜíRùëù, M0
ùëñ: Rùëù√ó Rùëõ√ó Rùëõ‚ÜíRùëõ, M1
ùëñ: Rùëù√ó Rùëõ‚ÜíRùëõ, and
that are defined by the recursion:
‚Äì M0 pushes forward ùúÇùëãŒò,ùëã0,ùëã1 to ùúã0 = ÃÉÔ∏Äùúã0/c0,
‚Äì Mùëñpushes forward ùúÇùëãŒò,ùëãùëñ,ùëãùëñ+1 to
ùúãùëñ(ùëßùúÉ, ùëßùëñ, ùëßùëñ+1) = ùúÇùëãŒò,ùëãùëñ(ùëßùúÉ, ùëßùëñ) ÃÉÔ∏Äùúãùëñ(TŒò
ùëñ‚àí1(ùëßùúÉ), M1
ùëñ‚àí1(ùëßùúÉ, ùëßùëñ), ùëßùëñ+1)/cùëñ,
(5.38)
where cùëñis a normalizing constant, the map TŒò
ùëó:= MŒò
0 ‚àò¬∑ ¬∑ ¬∑ ‚àòMŒò
ùëófor all ùëó‚â•0, and
where (ÃÉÔ∏Äùúãùëñ)ùëñ‚â•0 are functions on Rùëù√ó Rùëõ√ó Rùëõgiven by:
‚Äì ÃÉÔ∏Äùúã0(ùëßùúÉ, ùëß0, ùëß1) = ùúãŒò,ùëç0,ùëç1(ùëßùúÉ, ùëß0, ùëß1) ùúãùëå0|ùëç0,Œò(ùë¶0|ùëß0, ùëßùúÉ) ùúãùëå1|ùëç1,Œò(ùë¶1|ùëß1, ùëßùúÉ),
‚Äì ÃÉÔ∏Äùúãùëñ(ùëßùúÉ, ùëßùëñ, ùëßùëñ+1) = ùúãùëçùëñ+1|ùëçùëñ,Œò(ùëßùëñ+1|ùëßùëñ, ùëßùúÉ) ùúãùëåùëñ+1|ùëçùëñ+1,Œò(ùë¶ùëñ+1|ùëßùëñ+1, ùëßùúÉ) for ùëñ‚â•1.
Then, for all ùëò‚â•0, the following hold:
1. The map ÃÉÔ∏Å
Mùëò, defined as
ÃÉÔ∏Å
Mùëò(ùë•ùúÉ, ùë•ùëò+1) =
‚é°
‚é£TŒò
ùëò(ùë•ùúÉ)
M1
ùëò(ùë•ùúÉ, ùë•ùëò+1)
‚é§
‚é¶,
(5.39)
pushes forward ùúÇùëãŒò,ùëãùëò+1 to ùúãŒò,ùëçùëò+1|ùë¶0,...,ùë¶ùëò+1.
[filtering]
2. The composition of transport maps Tùëò= ùëá0 ‚àò¬∑ ¬∑ ¬∑ ‚àòùëáùëò, where each ùëáùëñis defined
149

as
ùëáùëñ(ùë•ùúÉ, ùë•0, . . . , ùë•ùëò+1) =
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
MŒò
ùëñ(ùë•ùúÉ)
ùë•0
...
ùë•ùëñ‚àí1
M0
ùëñ(ùë•ùúÉ, ùë•ùëñ, ùë•ùëñ+1)
M1
ùëñ(ùë•ùúÉ, ùë•ùëñ+1)
ùë•ùëñ+2
...
ùë•ùëò+1
‚é§
‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é¶
,
(5.40)
pushes forward ùúÇùëãŒò,ùëã0,...,ùëãùëò+1 to ùúãŒò,ùëç0,...,ùëçùëò+1|ùë¶0,...,ùë¶ùëò+1.
[full Bayesian solution]
3. The model evidence (marginal likelihood) is given by (5.31).
Theorem 5.4 suggests a variational algorithm for the joint parameter and state
estimation problem that is similar to the one proposed in Theorem 5.3: compute the
sequence of maps (Mùëñ), each of dimension 2ùëõ+ùëù; embed them into higher-dimensional
identity maps to form (ùëáùëñ) according to (5.40); then evaluate the composition Tùëò=
ùëá0 ‚àò¬∑ ¬∑ ¬∑ ‚àòùëáùëòto sample directly from ùúãŒò,ùëç0:ùëò+1|ùë¶0:ùëò+1 (i.e., the full Bayesian solution).
Each map Mùëñis now of dimension twice that of the model state plus the dimension
of the hyperparameters.
This dimension is slightly higher than that of the maps
(Mùëñ) considered in Theorem 5.3, and should be regarded as the price to pay for
introducing hyperparameters in the state-space model and having to deal with the
Markov structure of Figure 5-8 as opposed to the tree structure of Figure 5-7. By
Theorem 5.4[Part 1], the composition of maps TŒò
ùëò= MŒò
0 ‚àò¬∑ ¬∑ ¬∑‚àòMŒò
ùëòprovides a recursive
characterization of the posterior distribution over the static parameters, ùúãŒò|ùë¶0:ùëò+1, for
all ùëò‚â•0. The latter is often the ultimate goal of inference [6]. In order to have
a sequential algorithm for parameter estimation, we also need to keep a running
approximation of TŒò
ùëòusing the recursion TŒò
ùëò= TŒò
ùëò‚àí1 ‚àòMŒò
ùëò‚Äîe.g., via regression‚Äîso
that the cost of evaluating TŒò
ùëòdoes not grow with ùëò.
Even in the joint parameter and state estimation case, only a single forward pass
150

with local computations is necessary to gather all the information from the state-space
model needed to sample the collection of posteriors (ùúãŒò, ùëç0:ùëò+1|ùë¶0:ùëò+1). Notice that the
accuracy of the variational procedure is only limited by the accuracy of each computed
map, and that the proposed approach does not prescribe an artificial dynamic for the
parameters [152, 177], or an a priori fixed-lag smoothing approximation [218]. Yet,
there is no rigorous proof that the performance of the proposed sequential algorithm
for parameter estimation does not deteriorate with time. Indeed, developing exact,
sequential, and online algorithms for parameter estimation in general non-Gaussian
state-space models is among the chief research challenges in SMC methods [136]. See
[55, 71, 82] for recent contributions in this direction and [148] for a review of SMC
approaches to Bayesian parameter inference. See also [91] for a hybrid approach that
combines elements of variational inference with particle filters.
We refer the reader to Section 5.8.1 for a numerical illustration of parameter
estimation with transport maps involving a stochastic volatility model.
5.6.5
Fixed-point smoothing
Consider again the problem of sequential inference in a state-space model without
static parameters (see Figure 5-7), and suppose that we are interested only in the
smoothing marginal ùúãùëç0|ùë¶0:ùëòfor all ùëò‚â•0; this is the fixed-point smoothing problem
[245].
In Section 5.6.1 we showed that computing a sequence of maps (Mùëñ)‚Äîeach of
dimension 2ùëõ‚Äîis sufficient to sample the joint distribution ùúãùëç0:ùëò+1|ùë¶0:ùëò+1 by evaluating
the composition Tùëò= ùëá0 ‚àò¬∑ ¬∑ ¬∑ ‚àòùëáùëò, where each ùëáùëñis a trivial embedding of Mùëñinto an
identity map. If we can sample ùúãùëç0:ùëò+1|ùë¶0:ùëò+1, then it is easy to obtain samples from the
marginal ùúãùëç0|ùë¶0:ùëò+1: in fact, it suffices to evaluate only the first ùëõcomponents of Tùëò,
which can be interpreted as a map from Rùëõ√ó(ùëò+2) to Rùëõ. To do so, however, we need
to evaluate ùëòmaps. A natural question then is whether it is possible to characterize
ùúãùëç0|ùë¶0:ùëò+1 via a single transport map that is updated recursively in time, as opposed
to a growing composition of maps.
Here we propose a solution‚Äîcertainly not the only possibility‚Äîbased on the the-
151

ory of Section 5.6.4. The idea is to treat ùëç0 as a static parameter, i.e., to set Œò := ùëç0
and apply the results of Theorem 5.4 to the Markov structure of Figure 5-9. The
resulting algorithm computes a sequence of maps (Mùëñ) of dimension 3ùëõ, i.e., three
times the state dimension, and keeps a running approximation of TŒò
ùëòvia the recursion
TŒò
ùëò= TŒò
ùëò‚àí1 ‚àòMŒò
ùëò, where each MŒò
ùëòis just a subcomponent of Mùëò. These maps (Mùëñ)
are higher-dimensional than those considered in Section 5.6.1, but they do yield the
desired result: each TŒò
ùëò: Rùëõ‚ÜíRùëõcharacterizes the smoothing marginal ùúãùëç0|ùë¶0:ùëò+1,
for all ùëò‚â•0, via a single transport map that is updated recursively in time with just
one forward pass (see Theorem 5.4[Part 1]).
ùëç1
ùëç2
ùëç3
ùëç4
ùëçùëÅ
ùëç0
Figure 5-9: I-map (certainly not minimal) for ùúãùëç0,ùëç1:ùëÅ|ùë¶0:ùëÅ, for any ùëÅ> 0. Orange
edges have been added compared to the tree structure of Figure 5-7.
5.7
Low-rank transport maps
In this section, we characterize yet another source of low-dimensional structure in
the representation of a transport map, based on the notion of low-rank functions.
Intuitively, a low-rank transport is a function that is low-dimensional up to a rotation
of the space, i.e., a function whose action is nontrivial only along a low-dimensional
subspace of the input space. We will see that this type of structure appears quite
naturally in the context of high-dimensional Bayesian inference problems (e.g., inverse
problems, spatial statistics) where the data may be informative only about a few
directions in the parameter space [261, 74].
5.7.1
General result
Let us start with a formal definition.
152

Definition 5.4 (Low-rank map). We say that ùëá: Rùëõ‚ÜíRùëõhas rank (at most) ùúÖ‚â§ùëõ,
if there exists a pair of rotation matrices (ùëÑ, ùëâ) on Rùëõsuch that for all ùë•‚ààRùëõ,
ùëá(ùë•) = (ùëÑ‚àòÃÇÔ∏Äùëá‚àòùëâ)(ùë•) = ùëà¬∑ ÃÇÔ∏Äùëá( ùëâùë•)
(5.41)
with ÃÇÔ∏Äùëá: Rùëõ‚ÜíRùëõof the form
ÃÇÔ∏Äùëá(ùë•) =
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
M(ùë•1, . . . , ùë•ùúÖ)
ùë•ùúÖ+1
...
ùë•ùëõ
‚é§
‚é•‚é•‚é•‚é•‚é•‚é•‚é¶
(5.42)
for some M : RùúÖ‚ÜíRùúÖ.
There is no universal definition for a low-rank nonlinear multivariate function;
here, we have opted for a simple definition that suits our purposes. One can certainly
imagine more general characterizations of low-rank maps based, for instance, on tensor
decompositions [115, 202].
The following theorem identifies a set of conditions on the reference‚Äìtarget pair
(ùúàùúÇ, ùúàùúã) that guarantee the existence of at least one low-rank transport. In what
follows, let ùúÇ, ùúãdenote the densities of ùúàùúÇand ùúàùúã, respectively.
Theorem 5.5. Let ùëã‚àºùúÇ= ùí©(0, I) and ùëç‚àºùúã. The following hold:
1. If there exists a rotation ùëÑof Rùëõsuch that the pullback ùúãùëÑ:= ùëÑ‚ôØùúãfactorizes
as
ùúãùëÑ(ùë•) = ùõø(ùë•1, . . . , ùë•ùúÖ) ùúÇùëãùúÖ+1,...,ùëãùëõ(ùë•ùúÖ+1, . . . , ùë•ùëõ)
(5.43)
for some positive density ùõø: RùúÖ‚ÜíR with ùúÖ> 0, then there is a low-rank
153

transport ùëáof the form
ùëá(ùë•) = (ùëÑ‚àòÃÇÔ∏Äùëá)(ùë•) = ùëÑ¬∑
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
M(ùë•1, . . . , ùë•ùúÖ)
ùë•ùúÖ+1
...
ùë•ùëõ
‚é§
‚é•‚é•‚é•‚é•‚é•‚é•‚é¶
,
(5.44)
such that ùëá‚ôØùúÇ= ùúã, where M can be any map that pushes forward ùúÇùëã1,...,ùëãùúÖto ùõø.
2. If, additionally, ùúãis smooth and there exists any smooth density ùúåwith finite
second central moment such that
‚à´Ô∏Ä
‚Äñ‚àálog ùúã(ùë•)‚Äñ2 ùúå(ùë•) dùë•< ‚àû, we can define
a matrix Cùúå‚ààRùëõ√óùëõas
(Cùúå)ùëñùëó=
‚à´Ô∏Å
ùúïùëñr(ùë•) ùúïùëór(ùë•) ùúå(ùë•) dùë•
(5.45)
for all ùëñ, ùëó= 1, . . . , ùëõ, with r := log(ùúã/ùúÇ). Then the rotation ùëÑof Part 1 exists
if and only if rank(Cùúå) ‚â§ùúÖ. In this case, a valid choice for ùëÑis ùëÑ= [ùëÑ1 | ùëÑ2],
where ùëÑ1 is a basis for the range of Cùúå, while ùëÑ2 is a basis for Rùëõ‚àñspan(ùëÑ1).
Theorem 5.5[Part 1] shows that if the ‚Äúdifference‚Äù between reference and tar-
get density lies in a ùúÖ-dimensional subspace of the parameter space‚Äînot necessarily
aligned with the coordinate axes‚Äîthen both densities can be coupled with a low-rank
map that is essentially ùúÖ-dimensional. The potentially nonlinear map ÃÇÔ∏Äùëáin (5.44) is
low-dimensional with respect to {ùúÖ+ 1, . . . , ùëõ}, according to Definition 5.1, and thus
its effective dimension is upper bounded by ùúÖ. An important question is then whether
there exists a rotation, ùëÑ, of Rùëõthat guarantees the factorization (5.43) of the pull-
back density ùëÑ‚ôØùúã. There are multiple answers to this question. One the one hand,
the existence and form of ùëÑmight be clear from the context (see the forthcoming
section 5.7.2 for a relevant example). In some other cases, however, they are not,
and one can rely on Theorem 5.5[Part 2] for a necessary and sufficient condition
for the existence of such rotation, provided that ùúãsatisfies some additional smooth-
ness assumptions. Part 2 of the theorem uses Cùúå‚Äîa particular average derivative
154

functional [242, 238, 67]‚Äîto certify the existence of a low-rank transport, whenever
rank(Cùúå) ‚â§ùúÖ.
From a computational point of view, Theorem 5.5 has important consequences:
the existence of a transport map that factorizes as (5.44) can be established before
computing the transport itself. For instance, we might just need to determine an
eigenvalue decomposition of Cùúå[68]. Thus, when solving a variational problem for a
transport map ùëá, we can impose the additional constraint ùëá= ùëÑ‚àòÃÇÔ∏Äùëáand optimize
over ÃÇÔ∏Äùëá. The advantage of this approach is that we only need to parameterize M‚Äîa
ùúÖ-dimensional map‚Äîinstead of looking for a coupling in the full space. Simple algebra
shows that ÃÇÔ∏Äùëácan be any transport map that pushes forward ùúÇto the pullback density
ùúãùëÑ= ùëÑ‚ôØùúã, including the KR rearrangement of Section 5.2. In particular, we can
formulate a variational problem like (5.5) for a map ÃÇÔ∏Äùëásuch that ÃÇÔ∏Äùëá‚ôØùúÇ= ùúãùëÑ. If we do
so, then (5.43) shows that we can approximate the integrand in the objective of (5.5)
as if it were a function on a ùúÖ-dimensional space as opposed to a function on Rùëõ.
Depending on the value of ùúÖ, we can then consider the use of integration techniques
with faster convergence rates than regular Monte Carlo (e.g., quasi-Monte Carlo or
cubature rules [84]).
The reference distribution in Theorem 5.5 is a standard normal. This is the first
time we make an explicit distributional assumption on ùúÇ(other than independence).
In fact, this hypothesis can be relaxed, but not too much. For the theorem to be useful
in general settings‚Äîe.g., for a general Cùúå‚ÄîùúÇ‚àòùëÑshould factorize as the product of its
marginals for all rotations ùëÑof Rùëõ(see the proof of Theorem 5.5). A nice argument
by Kac shows that the class of such densities consists precisely of isotropic Gaussians,
i.e., ùúÇ= ùí©(0, ùúé2I) for any ùúé‚ààR [142, 39]. In Theorem 5.5, we chose ùúé= 1.
5.7.2
Bayesian inference: local likelihood and the prior map
Given an arbitrary target density ùúã, there need not exist a rotation ùëÑthat satisfies
the hypothesis of Theorem 5.5[Part 1]. Nevertheless, in this section we show how, in
a rich class of Bayesian inference problems, it is possible to transform the posterior
distribution to fit the hypothesis of Theorem 5.5 with ùúÖ< ùëõ.
155

Let us first recall some notation for simple Bayesian inference problems.
We
denote by ùëçthe latent variables with prior density ùúãùëçand by ùëåthe observables
with likelihood function ùúãùëå|ùëç. Our target is the posterior distribution of ùëçgiven the
event {ùëå= ùë¶} for some fixed ùë¶, i.e., ùúã= ùúãùëç|ùë¶‚àùùúãùë¶|ùëçùúãùëç, where
ùúãùëç|ùë¶(ùëß) := ùúãùëç|ùëå(ùëß|ùë¶)
and
ùúãùë¶|ùëç(ùëß) := ùúãùëå|ùëç(ùë¶|ùëß).
(5.46)
Now assume that the likelihood function is localized, i.e., that ùúãùë¶|ùëçdepends only on
ùúÖ< ùëõlatent variables. This is a common situation in, e.g., spatial statistics with
point processes (see Section 5.8.2 for an example). Thus, up to a reordering of the
input variables, we can assume that the likelihood function is of the form
ùúãùë¶|ùëç(ùëß) = ùëî(ùëß1, . . . , ùëßùúÖ)
(5.47)
for some ùëî: RùúÖ‚ÜíR, so that ùúãùëç|ùë¶(ùëß) ‚àùùëî(ùëß1, . . . , ùëßùúÖ) ùúãùëç(ùëß). If ùúÇ= ùí©(0, I), then the
pair (ùúÇ, ùúã) need not satisfy the hypothesis of Theorem 5.5, but consider for a moment
a simple transformation of the target density. Let ùëápr : Rùëõ‚ÜíRùëõbe a (block) lower
triangular map of the form
ùëápr(ùë•) =
‚é°
‚é£ùëá0
pr(ùë•1:ùúÖ)
ùëá1
pr(ùë•1:ùúÖ, ùë•ùúÖ+1:ùëõ)
‚é§
‚é¶,
(5.48)
for some ùëá0
pr : RùúÖ‚ÜíRùúÖand ùëá1
pr : Rùëõ‚ÜíRùúÖ, that pushes forward ùúÇto ùúãùëç. Such a map
always exists, e.g., take a KR rearrangement, and notice that the pullback density
ùëá‚ôØ
prùúãcan be written as
ùëá‚ôØ
prùúã(ùëß) = ùëî
(Ô∏Ä
ùëá0
pr(ùëß1, . . . , ùëßùúÖ)
)Ô∏Ä
ùúÇ(ùëß).
(5.49)
The reference‚Äìtarget pair (ùúÇ, ùëá‚ôØ
prùúã) now does satisfy the hypothesis of Theorem 5.5[Part
1] with ùëÑequal to the identity matrix. Thus there exists a low-rank map ùëÑ‚àòÃÇÔ∏Äùëá, of
the form (5.44), that pushes forward ùúÇto ùëá‚ôØ
prùúã. At the same time, there should also
156

exist a transport map ùëáthat pushes forward ùúÇto ùúãand that decomposes10 as:
ùëá= ùëápr ‚àòùëÑ‚àòÃÇÔ∏Äùëá.
(5.50)
The maps ùëápr and ùëÑhave a simple structure: ùëÑis a rotation, while ùëápr‚Äîwhich we call
the prior map‚Äîpushes forward ùúÇto ùúãùëç. The prior map is usually simple to compute.
In many cases, it can be can be written down analytically [273]; alternatively, if we
can sample the prior distribution, then we can determine ùëápr as the solution of a
convex and separable optimization problem [212] that is independent of the data. In
fact, the only map in (5.50) which actually depends on ùëåis ÃÇÔ∏Äùëá. But ÃÇÔ∏Äùëáis essentially
a ùúÖ-dimensional map, and ùúÖdoes not depend on ùëõ‚Äîthe nominal dimension of the
inverse problem‚Äîbut only on the likelihood function ùúãùëå|ùëç.
A key ingredient of this result is the sparsity pattern of the prior map (5.48). In
this case, we could use a (block) lower triangular map because we assumed that the
observed variables had indices 1, . . . , ùúÖ. This is always possible modulo a permutation
of the latent variables.
A different sparsity pattern of ùëápr precludes, in general,
the existence of a simple transport like (5.50) (see Example E.1 in Appendix E).
Nevertheless, there is a practically relevant scenario where the sparsity pattern of the
prior map does not matter. Consider the case of a linear ùëápr that pushes forward
ùúÇ= ùí©(0, I) to a Gaussian prior ùúãùëç. Then, for a localized likelihood of the form
(5.47), it is immediate to verify the existence of a transport map that decomposes as
(5.50). This result is general, but it is particularly easy to grasp if we assume, for
a second, that the additional regularity assumptions of Theorem 5.5[Part 2] hold for
the density pair (ùúÇ, ùëá‚ôØ
pr ùúã). In this case, rank(Cùúå) ‚â§ùúÖ, independent of the sparsity
pattern of ùëápr, since the gradient of a linear map ùëápr is constant over Rùëõ.
10 The transport ùëáin (5.50) is not decomposable in the sense of Section 5.5.
157

5.7.3
Low-rank likelihood
There is another example of low-dimensional structure in the likelihood that guaran-
tees the existence of a low-rank transport. Assume that
ùúãùë¶|ùëç(ùëß) = ùëî( Œ†ùúÖùëß),
(5.51)
for some ùëî: Rùëõ‚ÜíR and a rank-ùúÖprojector Œ†ùúÖ, i.e., Œ†2
ùúÖ= Œ†ùúÖ.
(Notice that
we fall under the hypothesis of the previous section if we consider an orthogonal
projector onto the first ùúÖcoordinate directions. The forthcoming analysis, however,
is more general.) It turns out that the likelihood of a broad class of high-dimensional
Bayesian inverse problems [265] can be well approximated by (5.51) for ùúÖ‚â™ùëõ. This
is precisely what we saw in Chapter 3 for linear Gaussian problems [261], and what
is shown in [74, 73, 69] for nonlinear inverse problems. The intuition is simple: for
problems with high-dimensional latent fields (as arising from the discretization of a
distributed stochastic process) and limited indirect observations, the data will only
inform, relative to the prior, few linear combinations of the latent variables. As a
result, the prior-to-posterior update can be confined to a low-dimensional subspace
of the latent space.
Now let ùëÑbe any orthonormal matrix on Rùëõwith the following structure: ùëÑ=
[ùëÑ1 | ùëÑ2], where the columns of ùëÑ2 ‚ààRùëõ√ó(ùëõ‚àíùúÖ) form a basis for the nullspace of Œ†ùúÖ.
It is easy to verify that the pullback density ùëÑ‚ôØùúãcan now be written as
ùëÑ‚ôØùúã(ùëß) = ^ùëî(ùëß1, . . . , ùëßùúÖ) ùúãùëç(ùëÑùëß),
(5.52)
for some ^ùëî: RùúÖ‚ÜíR, and hence all the analysis of Section 5.7.2 for localized likeli-
hoods readily applies, if we interpret ùëÑ‚ôØùúãas the new posterior and ùúãùëç‚àòùëÑas the new
prior.
158

5.8
Numerical examples
We present two numerical examples: The former is a joint parameter and state es-
timation problem on a stochastic volatility model, which illustrates some aspects of
the theory developed in Section 5.6 for decomposable transports on non-Gaussian
state-space models. The latter example illustrates the role of low-rank transports in
a high-dimensional log-Gaussian Cox process with sparse observations, and demon-
strates some of the theory discussed in Section 5.7.
5.8.1
Stochastic volatility model with hyperparameters
Following [149, 237], we model the scalar log-volatility (ùëçùëò) of the return of a financial
asset at time ùëò= 0, . . . , ùëÅusing an autoregressive process of order one, which is
fully specified by ùëçùëò+1 = ùúá+ ùúë(ùëçùëò‚àíùúá) + ùúÄùëò, for all ùëò‚â•0, where ùúÄùëò‚àºùí©(0, 1)
is independent of ùëçùëò, ùëç0|ùúá, ùúë‚àºùí©(ùúá,
1
1‚àíùúë2), and where ùúëand ùúárepresent scalar
hyperparameters of the model. In particular, ùúá‚àºùí©(0, 1) and ùúë= 2 exp(ùúë‚ãÜ)/(1 +
exp(ùúë‚ãÜ)) ‚àí1 with ùúë‚ãÜ‚àºùí©(3, 1). We define Œò := (ùúá, ùúë). The process (ùëçùëò) and
parameters Œò are unobserved and must be estimated from an observed process (ùëåùëò),
which represents the mean return on holding the asset at time ùëò, ùëåùëò= ùúâùëòexp(1
2ùëçùëò),
where ùúâùëòis a standard normal random variable independent of ùëçùëò. As a dataset
(ùë¶ùëò)ùëÅ
ùëò=0, we use the ùëÅ+1 daily differences of the pound/dollar exchange rate starting
on 1 October 1981 with ùëÅ= 100 [237, 88].
Our goal is to sequentially characterize ùúãŒò,ùëç0:ùëò|ùë¶0:ùëò, for all ùëò= 0, . . . , ùëÅ, as ob-
servations of (ùëåùëò) become available. The Markov structure of ùúãŒò,ùëç0:ùëÅ|ùë¶0:ùëÅmatches
Figure 5-8. We solve the problem using the algorithm introduced in Section 5.6.4: we
compute a sequence, (Mùëó)ùëÅ‚àí1
ùëó=0 , of four-dimensional transport maps (ùëõ= dim(ùëçùëó) = 1
and ùëù= dim(Œò) = 2) according to their definition in Theorem 5.4 and using the vari-
ational form (5.5). All reference densities are standard Gaussians. Then, by Theorem
5.4[part 1], for any ùëò< ùëÅ, we can easily sample the filtering marginal ùúãùëçùëò+1|ùë¶0:ùëò+1 by
pushing forward a standard normal through the subcomponent M1
ùëòof Mùëò, and we can
also sample the posterior distribution over the static parameters ùúãŒò|ùë¶0:ùëò+1 by pushing
159

forward a standard normal through the map TŒò
ùëò. The map TŒò
ùëò= MŒò
0 ‚àò¬∑ ¬∑ ¬∑ ‚àòMŒò
ùëòis
updated sequentially over time (via regression) using the recursion TŒò
ùëò= TŒò
ùëò‚àí1 ‚àòMŒò
ùëò,
so that the cost of evaluating TŒò
ùëòdoes not increase with ùëò.
The resulting algo-
rithm for parameter estimation is thus sequential. Moreover if we want to sample
ùúãŒò,ùëç0:ùëò+1|ùëå0:ùëò+1‚Äîthe full Bayesian solution at time ùëò+ 1‚Äîwe simply need to embed
each Mùëóinto an identity map to form the transport ùëáùëó, for ùëó= 0, . . . , ùëò, and push for-
ward reference samples through the composition Tùëò= ùëá0 ‚àò¬∑ ¬∑ ¬∑ ‚àòùëáùëò(Theorem 5.4[part
2]).
Figure 5-10 shows the resulting filtering and smoothing marginals of the states
over time. Figure 5-11 collects the corresponding posterior marginals over the static
parameters, while Figure 5-12 (left) shows some trajectories drawn from the smooth-
ing distribution.
Figure 5-14 (left) illustrates the marginals of the posterior data
predictive together with the observed data (ùë¶ùëò), showing excellent coverage overall.
Our results rely on a numerical approximation of the desired transport maps.
Each component of Mùëòis parameterized via the monotone representation (5.4) using
linear combinations of tensorized radial basis functions, while the expectation in (5.5)
is approximated using tensorized Gauss quadrature rules. Both the number of basis
in the parameterization of the map and the quadrature order are decided adaptively
following the algorithm in [30]. The resulting minimization problems are sequentially
solved using the Newton-CG method [276].
This test case was run using a dedi-
cated software publicly available at http://transportmaps.mit.edu. The website
contains also additional details about possible parameterizations of the maps.
There are several ways to investigate the quality of these approximations. Fig-
ures 5-12 (right) and 5-13 compare the numerical approximation (via a decomposable
transport map) of the smoothing marginals of the states and the static parameters
to a ‚Äúreference‚Äù solution obtained via MCMC. The MCMC chain is run until it yields
105 effectively independent samples. The two solutions agree remarkably well and are
almost indistinguishable in most places. (Of course, MCMC in this context is not
a sequential algorithm; it requires that all the data (ùë¶ùëò)ùëÅ
ùëò=0 be available simultane-
ously.) An important fact is that the MCMC chain is generated using an independent
160

proposal [233] given by the pushforward of a standard Gaussian through the numer-
ical approximation of TùëÅ‚àí1 (denoted as ÃÉÔ∏ÄTùëÅ‚àí1). The resulting MCMC chain has an
acceptance rate slightly above 0.8, confirming the overall quality of the variational
approximation.
A second quality test can proceed as follows: since we use a standard Gaussian
reference distribution ùúàùúÇ, we expect the pullback of ùúãŒò,ùëç0:ùëÅ|ùë¶0:ùëÅthrough ÃÉÔ∏ÄTùëÅ‚àí1 to be
close to a standard Gaussian. Figure 5-14 (right) supports this claim by showing
a collection of random two-dimensional conditionals of the approximate pullback:
these ‚Äúslices‚Äù of the 103-dimensional (ùëÅ+1 states plus two hyperparameters) pullback
distribution are identical to a two-dimensional standard normal, as expected. The
fact that we can evaluate the approximate pullback density is one of the key features
of this variational approach to inference. Even more, we can use this approximate
pullback density in (5.7) to estimate the KL divergence between our target ùúàùúã(the full
Bayesian solution at time ùëÅ) and the approximating measure (ÃÉÔ∏ÄTùëÅ‚àí1)‚ôØùúàùúÇ. A numerical
realization of (5.7) yields ùíüKL( (ÃÉÔ∏ÄTùëÅ‚àí1)‚ôØùúàùúÇ|| ùúàùúã) ‚âà6.2√ó10‚àí2, which confirms the good
numerical approximation of ùúàùúã, a 103-dimensional target measure. For comparison,
we notice that the KL divergence from ùúàùúãto its Laplace approximation (a Gaussian
approximation at the mode) is approximately ‚âà22.6‚Äîconsiderably worse than what
is achieved through optimization of a nonlinear transport map.
5.8.2
Log-Gaussian Cox point process with sparse observa-
tions
We consider an inference problem in spatial statistics for a log-Gaussian Cox point
process on a square domain ùíü= [0, 1]2. This type of stochastic process is frequently
used to model spatially aggregated point patterns [193, 60, 237, 106].
Following
a configuration similar to [60, 193], we discretize ùíüinto a 64 √ó 64 uniform grid,
and denote by ùë†ùëñ‚ààùíüthe center of the ùëñth cell, for ùëñ= 1, . . . , ùëõ, with ùëõ= 642.
We consider a discrete stochastic process (ùëåùëñ)ùëõ
ùëñ=1, where ùëåùëñdenotes the number of
occurrences/points in the ùëñth cell.
(For instance, ùëåùëñcould denote the number of
161

0
20
40
60
80
100
time
2.5
2.0
1.5
1.0
0.5
0.0
0.5
1.0
1.5
0
20
40
60
80
100
time
2.5
2.0
1.5
1.0
0.5
0.0
Figure 5-10:
At each time ùëò, we illustrate the {5, 25, 40, 60, 75, 95}‚Äìpercentiles
(shaded regions) and the mean (solid line) of the numerical approximation of the
filtering distribution ùúãùëçùëò|ùë¶0:ùëò(left) and of the marginals ùúãùëçùëò|ùë¶0:ùëÅof the full smoothing
distribution (right), for ùëò= 0, . . . , ùëÅ.
0
20
40
60
80
100
time
1.5
1.0
0.5
0.0
0.5
1.0
0
20
40
60
80
100
time
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Figure 5-11:
At each time ùëò, we illustrate the {5, 25, 40, 60, 75, 95}‚Äìpercentiles
(shaded regions) and the mean (solid line) of the numerical approximation of the
posterior marginals over the static parameters: ùúãùúá|ùë¶0:ùëò(left) and ùúãùúë|ùë¶0:ùëò(right), for
ùëò= 0, . . . , ùëÅ.
162

0
20
40
60
80
100
time
2.5
2.0
1.5
1.0
0.5
0.0
0
20
40
60
80
100
time
2.5
2.0
1.5
1.0
0.5
0.0
Figure 5-12:
(left) The shaded regions and the red solid line have the same in-
terpretation as in Figure 5-10 (right), whereas the black solid lines correspond to
smoothed trajectories, i.e., independent realizations from the numerical approxima-
tion of ùúãùëç0:ùëÅ|ùë¶0:ùëÅ. (right) Comparison between the {5, 25, 75, 95}‚Äìpercentiles (dashed
lines) and the mean (solid line) of the numerical approximation of the smoothing
marginals ùúãùëçùëò|ùë¶0:ùëÅvia transport maps (red lines) versus a ‚Äúreference‚Äù solution obtained
via MCMC (black lines) with 105 effectively independent samples, for ùëò= 0, . . . , ùëÅ.
The two solutions are indistinguishable.
3
2
1
0
1
2
0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
1.6
œÄ¬µ|Y0 : N
Transport map
MCMC
0.5
0.0
0.5
1.0
0
1
2
3
4
5
œÄœÜ|Y0 : N
Transport Map
MCMC
Figure 5-13:
Comparison between the numerical approximation of the final-time
posterior marginals ùúãùúá|ùë¶0:ùëÅ(left) and ùúãùúë|ùë¶0:ùëÅ(right) via transport maps (solid lines)
versus a ‚Äúreference‚Äù solution obtained via MCMC (dashed lines) with 105 effectively
independent samples.
163

0
20
40
60
80
100
time
2.0
1.5
1.0
0.5
0.0
0.5
1.0
1.5
data
Figure 5-14:
(left) Shaded regions represent the {5, 25, 40, 60, 75, 95}‚Äìpercentiles of
the marginals of the posterior predictive distribution (conditioning on all the data),
along with black dots that represent the observed data (ùë¶ùëò)ùëÅ
ùëò=0. (right) Randomly
chosen two-dimensional conditionals of the pullback of ùúãŒò,ùëç0:ùëÅ|ùë¶0:ùëÅthrough the numer-
ical approximation of TùëÅ‚àí1. Since we use a standard normal reference distribution,
the numerical approximation of the transport map should be deemed satisfactory if
the corresponding pullback density is close to a standard normal, as it is here.
crimes in the ùëñth neighborhood during a certain period of time.) Each ùëåùëñis modeled
as a Poisson random variable with mean exp(ùëçùëñ)/ùëõ, where (ùëçùëñ) is a Gaussian process
with covariance
cov(ùëçùëñ, ùëçùëó) = ùúé2 exp
(Ô∏Ç
‚àí‚Äñùë†ùëñ‚àíùë†ùëó‚Äñ2
ùõΩ
)Ô∏Ç
,
(5.53)
and mean E[ùëçùëñ] = ùúá, for all ùëñ= 1, . . . , ùëõ. We consider the following values for the
parameters: ùõΩ= 7/10, ùúé= 1, and ùúá= 2 log 64. The (ùëåùëñ) are assumed conditionally
independent given the (latent) Gaussian field. For interpretability reasons, we also
define the intensity process (Œõùëñ)ùëõ
ùëñ=1 as Œõùëñ= exp(ùëçùëñ), for ùëñ= 1, . . . , ùëõ[106].
The goal of this problem is to infer the posterior distribution of the latent process
ùëç:= (ùëç1, . . . , ùëçùëõ) given few sparse realizations of (ùëåùëñ) at ùëë= 30 spatial locations
ùë†ùëò1, . . . , ùë†ùëòùëëshown in Figure 5-15 (top left). Thus, we set ùëå:= (ùëåùëò1, . . . , ùëåùëòùëë) and
denote by ùë¶‚ààRùëëa realization of ùëåobtained by sampling the latent Gaussian field
according to its marginal distribution (see Figure 5-15 (top left) for an illustration of
164

ùë¶). Our target distribution (ùúã) is then:
ùúãùëç|ùë¶(ùëß) := ùúãùëç|ùëå(ùëß|ùë¶).
(5.54)
We notice that the likelihood function is local, i.e., ùúãùë¶|ùëç(ùëß) = ùëî(ùëßùëò1, . . . , ùëßùëòùëë) for some
ùëî: Rùëë‚ÜíR, and thus all the discussion of Section 5.7.2 on low-rank transports
readily applies, provided that we perform a relabeling of the latent variables such
that ùëßùëò1, . . . , ùëßùëòùëëcorrespond to the first ùëëinputs, i.e., we can assume, without loss of
generality, that ùúãùë¶|ùëç(ùëß) = ùëî(ùëß1, . . . , ùëßùëë), where ùëçùëñnow refers to the latent process at
the ùëñth location ùë†ùëòùëñ, for ùëñ= 1, . . . , ùëë. Section 5.7.2 shows that there exists a transport
map ùëáthat pushes forward a standard normal reference, ùúÇ, to the target ùúãùëç|ùë¶, which
can be written as ùëá= ùëápr‚àòÃÇÔ∏Äùëá, where ùëápr is the so-called prior map, i.e., a transport that
pushes forward ùúÇto ùúãùëç‚Äîthe marginal distribution of ùëç‚Äîwith the sparsity pattern
in (5.48), and where ÃÇÔ∏Äùëáis essentially a ùëë-dimensional function. Two remarks are in
order: First, the prior map can be chosen as the KR rearrangement (lower triangular)
that couples ùúÇwith ùúãùëç= ùí©(ùúá, Œ£), where ùúá= (ùúá, . . . , ùúá) and Œ£ùëñùëó= cov(ùëçùëñ, ùëçùëó). In
particular, ùëápr is an affine function given by the Choleksy factor, ùêø, of Œ£, i.e.,
ùëápr(ùë•) = ùúá+ ùêøùë•=
‚éß
‚é®
‚é©
ùúá1
ùúá2
‚é´
‚é¨
‚é≠+
‚é°
‚é£ùêø11
0
ùêø12
ùêø22
‚é§
‚é¶ùë•,
(5.55)
where Œ£ = ùêøùêø‚ä§and ùêø11 ‚ààRùëë√óùëë. Second, the map ÃÇÔ∏Äùëápushes forward ùúÇto ùëá‚ôØ
pr ùúãùëç|ùë¶(ùëß) =
ùëî(ùúá1 + ùêø11 ùëß1:ùëë) ùúÇ(ùëß) and is of the form
ÃÇÔ∏Äùëá(ùë•) =
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
M(ùë•1, . . . , ùë•ùëë)
ùë•ùëë+1
...
ùë•ùëõ
‚é§
‚é•‚é•‚é•‚é•‚é•‚é•‚é¶
,
(5.56)
for some M : Rùëë‚ÜíRùëë.
The important point here is that M‚Äîreally the only
nonlinear map involved‚Äîcan be computed as the solution of a ùëë‚â™ùëõdimensional
165

problem, regardless of ùëõ, the nominal dimension of the latent field. That is, M can
be any map that pushes forward a standard normal to the density ùúâ‚Ü¶‚Üíùëî(ùúá1 + ùêø11 ùúâ)
on Rùëë‚Äîfor instance a KR rearrangement. This remark has important implications
since it allows us to get away with computing a nonlinear map on R30 as opposed to
a map on R4096.
The numerical results in Figures 5-16 follow precisely this approach by character-
izing ùëá= ùëápr‚àòÃÇÔ∏Äùëávia a numerical approximation of M (see Section 5.2). A comparison
versus a ‚Äúreference‚Äù solution obtained via MCMC shows almost identical low-order
posterior marginals (compare Figures 5-16 and 5-17). Lastly, a quantitative measure
of the quality of the approximation is given by (5.7): ùíüKL( ÃÉÔ∏Äùëá‚ôØùúàùúÇ|| ùúàùúã) ‚âà5.9 √ó 10‚àí2,
where ÃÉÔ∏Äùëádenotes the numerical approximation of ùëá, ultimately a transport map in
4096 dimensions.
166

6
7
8
9
10
0
1
2
3
4
5
6
0.00
0.32
0.64
0.96
1.28
1.60
1.92
2.24
2.56
√ó104
0
1
2
3
4
5
6
Figure 5-15:
Realization of the latent Gaussian field (ùëçùëñ) (top left), and of the cor-
responding intensity process (Œõùëñ) (top right), used to generate the Poisson data ùëå
for the inference problem. The centers of the circles represent the locations of the
observations, while the diameter of the circles is proportional to the magnitude of the
measurement ùë¶. (middle) Three different realizations from the numerical approxima-
tion (via transport maps) of the posterior distribution of the latent field, and of the
intensity process (bottom).
167

8
9
0.36
0.42
0.48
0.54
0.60
0.66
0.72
0.78
0.84
Figure 5-16: Mean (left) and standard deviation (right) of the numerical approxima-
tion (via transport maps) of the posterior distribution of the latent process.
8
9
0.36
0.42
0.48
0.54
0.60
0.66
0.72
0.78
0.84
Figure 5-17: Mean (left) and standard deviation (right) of a ‚Äúreference‚Äù approximation
of the posterior distribution of the latent process via MCMC with 105 effective sample
size. These posterior statistics are almost indistinguishable from those of Figure 5-16,
obtained via transport maps.
168

5.9
Discussion
This chapter has focused on the problem of coupling a pair (ùúàùúÇ, ùúàùúã) of absolutely
continuous measures on Rùëõ, for the purpose of sampling or integration, e.g., in
the context of non-Gaussian Bayesian inference, by leveraging key sources of low-
dimensional structure in the underlying distributions. If ùúàùúÇis a tractable measure
(e.g., an isotropic Gaussian) and ùúàùúãis an intractable measure of interest (e.g., a pos-
terior distribution), then a deterministic coupling enables principled approximations
of integrals via the identity
‚à´Ô∏Ä
ùëîdùúàùúã=
‚à´Ô∏Ä
ùëî‚àòùëádùúàùúÇ. In other words, a deterministic
coupling provides a simple way to simulate ùúàùúãby pushing forward samples from ùúàùúÇ
through a transport map ùëá. This idea, modulo some variations, has been exploited
in a variety of statistical applications‚Äîsome old, some new‚Äîincluding random num-
ber generation [185], Bayesian inference [196, 247, 231, 178, 187], the computation of
model evidence ratios [191], model learning and density estimation [266, 160, 5, 263],
non-Gaussian proposals for MCMC or importance sampling [213, 16, 195, 126, 201],
multiscale methods [214], and filtering [77, 58, 229, 228, 126], to name a few. Indeed
there are infinitely many ways to transport one measure to another [271] and as many
ways to compute one.
This chapter establishes an explicit link between the condititional independence
structure of (ùúàùúÇ, ùúàùúã) and the existence of low-dimensional couplings induced by trans-
port maps that are sparse, decomposable, and/or low-rank. These results can enhance
a wide array of numerical approaches to the transportation of measures, including
[196, 266, 231, 178, 30], and thus facilitate integration with respect to complex dis-
tributions in high dimensions. We briefly discuss our main results below.
Sparse transports
A sparse transport is a map whose components do not de-
pend on all input variables. Section 5.4 derives tight bounds on the sparsity pattern
of the Knothe‚ÄìRosenblatt (KR) rearrangement (a triangular transport map) based
solely on the Markov structure of ùúàùúã, provided that ùúàùúÇis a tensor product reference
measure (Theorem 5.1). This analysis shows that the inverse of the KR rearrange-
ment is the natural generalization to the non-Gaussian case of the Cholesky factor
169

of the precision matrix of a Gaussian MRF‚Äîin that both the inverse KR rearrange-
ment (a potentially nonlinear map) and the Cholesky factor (a linear map) have the
same sparsity pattern given target measures with the same Markov structure. Thus
the KR rearrangement can be used to extend well-known modeling and sampling
techniques for high-dimensional Gaussian MRFs [236] to non-Gaussian fields (Sec-
tion 5.4.2). Section 5.4 shows that sparsity is usually a feature of inverse transports,
while direct transports tend to be dense, even for the most trivial Markov struc-
tures. In fact, the sparsity of direct transports stems from marginal (rather than
conditional) independence‚Äîa property frequently exploited in localization schemes
for high-dimensional covariance estimation [103, 117].
Decomposable transports
A decomposable map is a function that can be written
as the composition of finitely many low-dimensional maps that are triangular up to
a permutation‚Äîi.e., ùëá= ùëá1 ‚àò¬∑ ¬∑ ¬∑ ‚àòùëá‚Ñì, where each ùëáùëñdiffers from the identity only
along a small subset of its components and is a generalized triangular function as
defined in Section 5.5. Theorem 5.2 shows that every target measure whose Markov
network admits a graph decomposition can be coupled with a tensor product (refer-
ence) measure via a decomposable map. Decomposable maps are important because
they are much easier to represent than arbitrary multivariate functions on Rùëõ. In
general, these maps are non-triangular, even though each map in the composition is
generalized triangular. In fact, we can think of the generalized KR rearrangement in
Appendix B as the fundamental ‚Äúbuilding block‚Äù of a much richer class of couplings:
the couplings induced by decomposable maps.
The notion of a decomposable map is different from the composition-of-maps
approaches advocated in the literature for the approximation of transport maps [266,
178, 231, 160].
In these approaches, very simple maps (ùëÄùëñ)ùëñ‚â•1 are composed in
growing number to define a transport map of increasing complexity, ùëÄ= ùëÄ1‚àò¬∑ ¬∑ ¬∑‚àòùëÄùëò.
The number of layers in ùëÄdepends on the desired accuracy of the transport and can
be arbitrarily large. On the other hand, a decomposable coupling is induced by a
special transport map that can be written exactly as the composition of finitely many
170

maps, ùëá= ùëá1 ‚àò¬∑ ¬∑ ¬∑ ‚àòùëá‚Ñì, where each ùëáùëñhas a specific sparsity pattern that makes it
low-dimensional. This definition does not specify a representation for ùëáùëñ. In fact, each
ùëáùëñcould itself be approximated by the composition of simple maps. The advantage of
targeting a decomposable transport, however, is the fact that the (ùëáùëñ) are guaranteed
to be low-dimensional.
Approximate Markov properties
Sparsity and decomposability of certain trans-
port maps are induced by the Markov properties of the target measure. A natural
question is: what happens when ùúàùúãsatisfies some Markov properties only approxi-
mately? In particular, let ùúàùúãbe Markov with respect to ùí¢, and assume that there
exists a measure ^ùúà‚ààM+(Rùëõ) which is Markov with respect to a graph ^ùí¢that is
sparser than ùí¢and such that ùíüKL( ^ùúà|| ùúàùúã) < ùúÄ, for some ùúÄ> 0. For small ùúÄ, we
would be tempted to use ^ùí¢to characterize couplings of (ùúàùúÇ, ùúàùúã) that are possibly
sparser or more decomposable than those associated with ùí¢. Concretely, if we are
interested in a triangular transport that pushes forward ùúàùúÇto ùúàùúã, we could minimize
ùíüKL( ùëá‚ôØùúàùúÇ|| ùúàùúã) over the set of maps that have the same sparsity pattern as the KR
rearrangement between ùúàùúÇand ^ùúà. Bounds on this sparsity pattern are given by The-
orem 5.1 using only graph operations on ^ùí¢; no explicit knowledge of ^ùúàis required.
Alternatively, if we are interested in decomposable transports that push forward ùúàùúÇto
ùúàùúã, we could minimize ùíüKL( ùëá‚ôØùúàùúÇ|| ùúàùúã) over the set of maps that factorize as any of
the decomposable transports between ùúàùúÇand ^ùúà. The shapes of these low-dimensional
factorizations are given by Theorem 5.2 using, once again, only graph operations on
^ùí¢.
Now let ^ùíØdenote the set of maps whose structure is constrained by ^ùí¢in terms
of sparsity or decomposability. It is easy to show that
min
ùëá‚àà^ùíØùíüKL( ùëá‚ôØùúàùúÇ|| ùúàùúã) < ùúÄ,
(5.57)
which means that the price of assuming that the coupling is either sparser or more
decomposable than it ought to be is just a small error in the approximation of ùúàùúã.
171

Of course, the pending question is whether ùúàùúãcan be well approximated by a
measure that satisfies additional Markov properties.
There is some work on this
topic, e.g., [141, 140, 51]‚Äîespecially in the case of Gaussian measures‚Äîbut a more
thorough investigation of the problem remains an open and important direction for
future work. Interestingly, the transport map framework also allows one to adaptively
discover information about low-dimensional couplings. For instance, one might start
with a very sparse transport map and then incrementally decrease the sparsity level
of the map until the resulting approximation of ùúàùúãbecomes satisfactory. The same
can be done for decomposable transports. See [30] for some details on this idea.
Filtering and smoothing
Section 5.5.4 shows how not only the representation,
but also the computation, of a decomposable map, ùëá= ùëá1 ‚àò¬∑ ¬∑ ¬∑ ‚àòùëá‚Ñì, can be broken
into a sequence of ‚Ñìsimpler steps, each associated with a low-dimensional optimiza-
tion problem whose solution yields ùëáùëñ. We give a concrete example of this idea for
filtering, smoothing, and sequential joint state‚Äìparameter inference in nonlinear and
non-Gaussian state-space models (Section 5.6). In this context, Theorems 5.3 and
5.4 introduce variational approaches for characterizing the full posterior distribution
of the sequential inference problem, essentially by performing only recursive lag‚Äì1
smoothing with transport maps. The proposed approaches consist of a single forward
pass on the state-space model, and generalize the square-root Rauch-Tung-Striebel
smoother to non-Gaussian models (see Section 5.6.2). In practice, we should think of
Theorems 5.3 and 5.4 as providing ‚Äúmeta-algorithms‚Äù within which all kinds of ap-
proximations can be introduced, e.g., linearizations of the forward model, restriction
to linear maps, and approximate flows [77, 178], to name a few. These approximations
are the workhorse of modern approaches to large-scale filtering, e.g., data assimilation
in geophysical applications [245, 93], and may play a key role in further instantiations
of the ‚Äúmeta-algorithms‚Äù proposed in Section 5.6. Of course, it would be desirable to
complement such variational approximations with a rigorous error analysis like the
one that is available for SMC methods (e.g., [70, 81, 251]). It is also important to re-
mark that one can always use functionals like (5.7) to estimate the quality of a given
172

approximate map, or use the map itself to build advanced proposals for sampling
techniques like MCMC [213].
Low-rank transports
A low-rank transport is a map that is low-dimensional up
to a rotation of the space, i.e., maps whose action is nontrivial only along a low-
dimensional subspace (Definition 5.4). This type of structure appears quite naturally
in certain high-dimensional Bayesian inference problems (e.g., inverse problems [265]
and spatial statistics) where the data may be informative only about a few linear
combinations of the latent parameters [261, 74]. This is essentially the same type of
structure that we exploited in Chapter 3 in the context of Gaussian problems. Low-
rank structure can be detected via certain average derivative functionals (Theorem
5.5), but cannot be deduced, in general, from the Markov structure of (ùúàùúÇ, ùúàùúã), an
important contrast with sparse and decomposable maps. An important future direc-
tion is to extend these results in the context of approximately low-rank structure as
we did in Chapter 3 and, more generally, as in [67, 282].
Further extensions
We envision many additional ways to extend the present work.
For instance, it would be interesting to investigate the low-dimensional structure of
deterministic couplings between pair of measures (ùúàùúÇ, ùúàùúã) that are not absolutely
continuous and that need not be defined on the same space Rùëõ. Such couplings are
usually induced by ‚Äúrandom‚Äù maps and can be particularly effective for approximating
multi-modal distributions; see the warp bridge transformations in [191, 272] for some
examples. Moreover, it would be nice to account for ultimate goals in the construction
of a nonlinear map (a natural generalization of what we did in Chapter 4).
173

174

Chapter 6
A class of nonlinear filters induced by
local couplings
6.1
Introduction
6.1.1
Problem setting
In this chapter we propose new nonlinear filtering algorithms that rely on our previous
study of low-dimensional deterministic couplings (see Chapter 5). We would like these
filtering algorithms to tackle models with:
1. High-dimensional and continuous state-space, which usually represents the dis-
cretization of a (spatially) distributed process
2. Challenging nonlinear dynamic (e.g., chaotic) governed by a possibly expensive
forward model, like a partial differential equation (PDE)
3. Intractable transition kernel, i.e., we assume that we can only simulate the prior
dynamic. We do not rely on any gradient information of the forward model.1
1 This assumption greatly extends the applicability of our techniques since both tangent and
adjoint codes are often times challenging to develop and maintain, and some forward models might
even fail to be differentiable in the first place. Of course, if the forward model were differentiable,
and if cheap gradients were available, then we would use this information and construct algorithms
that might look different than those proposed here.
175

4. Sparse measurement configuration, both in space and time, with likelihoods
that are mostly local and possibly non-Gaussian.
The algorithms that we propose can work in a much more general setting, but
they are specifically designed to exploit the structure mentioned above, which is fre-
quently encountered when filtering high-dimensional spatiotemporal processes [230]‚Äî
particularly in geophysical applications, such as atmosphere/ocean data assimilation
[33]. In addition, we are interested in situations where the ensemble size‚Äîand thus
the number of times we can simulate the model dynamic‚Äîmust remain relatively
small, perhaps much smaller than the dimension of the model state. State-of-the-art
results‚Äîin terms of tracking‚Äîfor these scenarios are currently obtained with local-
ized versions of the ensemble Kalman filter (EnKF) [93], even though recent advances
in particle filters are also promising [23, 227, 219]. We should remark that assess-
ing the accuracy of filtering algorithms in high dimensions is a challenging and open
problem. Ideally, we would compare filtering algorithms on their ability to approxi-
mate the filtering distribution. But the latter is often unavailable, because currently
there is no golden standard algorithm that can approximate the filtering distribution
arbitrarily well over long assimilation windows and with a reasonable computational
budget. The usual fallback is to measure filter accuracy in terms of tracking, e.g.,
using the mean square root error (RMSE) [182].
In the next paragraph, we describe our new filtering algorithm.
6.1.2
Approach
The idea that we pursue here is a generalization of the EnKF philosophy, and can
be synthesized as follows: we seek to transform the forecast ensemble into samples
from the filtering distribution by means of a sequence of local, low-dimensional, and
possibly nonlinear deterministic couplings (i.e., transport maps). The maps are local
in that they only act on a few components of the forecast ensemble at a time. Many
of these maps can be computed efficiently and in parallel via convex optimization be-
cause they are learned ‚Äúfrom samples‚Äù (see Section 6.2). Only few of these maps are
176

computed ‚Äúfrom densities‚Äù‚Äîfollowing the construction outlined in Section 5.2‚Äîand
can be extremely low-dimensional, depending on the local nature of each likelihood
function, i.e., regardless of the dimension of the parameter space (see Section 6.3).
Particles from the forecast distribution are progressively transformed into particles
from the filtering distribution, under the action of these maps. The forecast distri-
bution is only learned locally, whenever needed. In some sense, the tasks of first
estimating the forecast distribution and then sampling from the resulting filtering
distribution (a procedure common to many, but not all, EnKF algorithms), are now
entangled in a single process.
The use of local transformations implicitly approximates the projection of the
filtering distribution onto a manifold of sparse Markov random fields, not necessarily
Gaussian. The Markov assumption is a natural one: several spatially distributed
processes can be well approximated by sparse Markov random fields (MRFs) [236].
We envision the sparsity pattern of the MRFs in the manifold as a degree of freedom of
the problem‚Äîa regularization parameter‚Äî, but it could also be learned from samples
[156]. Each choice of the sparsity pattern corresponds to a sequence of local maps
that are needed to approximate the corresponding projection. As an intuitive rule
of thumb, the sparser the Markov structure, the lower dimensional the maps needed.
We will discuss this point in Section 6.3.2.
The projection step that we advocate has several benefits:
1. It regularizes the estimation of the filtering distribution. This is an important
step when dealing with high-dimensional problems and limited ensemble sizes.
Regularization in high-dimensions is needed, even when estimating a simple
covariance [26, 27], and allows to trade bias for variance of the estimators.
2. It enables the transformation from forecast to filtering distribution to be broken
into simpler, smaller dimensional steps that correspond to the computation of
local, low-dimensional, transport maps.
Since each map is low-dimensional,
one can inject local nonlinearities in the parameterization of each map without
worrying too much about the variance of an estimator for the coefficients of the
177

transformation. In general, this is very different from learning a single, high-
dimensional, and unstructured transport map. Of course, how much nonlinear
structure one can actually learn depends primarily on the size of the forecast
ensemble.
With very few particles, it is very hard to learn anything other
than a linear map. But the key point is that if one is willing to increase the
ensemble size, then the proposed framework offers a principled way to increase
the complexity of the forecast-to-filtering transformation, and thus to reduce
bias in the filtering estimation.
EnKF algorithms cannot do this: they are
limited by linear transformations. Larger ensemble sizes in EnKF algorithms
are not guaranteed to yield better inference results, and in fact, in most cases,
they simply will not.
An important point: one can recover many EnKF algorithms from the proposed
framework by restricting the attention exclusively to linear transformations, and by
neglecting approximately sparse Markov structure in the filtering distribution as a
form of regularization.
The rest of this chapter is organized as follows. In Section 6.2, we review the
construction of transport maps from samples via convex optimization. This will be
an essential construction for the proposed algorithm. In Section 6.3, we illustrate the
main idea and the mechanics of the new filtering algorithm on an abstract problem.
In Section 6.4, we investigate the performances of the proposed algorithm on the
Lorenz 96 model [180].
6.2
Transport maps from samples:
conditional simulation
We recall a construction first proposed by [213]. Assume that we have ùëÄsamples
(ùëßùëñ)ùëÄ
ùëñ=1 from a target distribution ùúã= ùúãùëç1,...,ùëçùëõon Rùëõ. Given a reference density
178

ùúÇ= ùúÇùëã1,...,ùëãùëõon Rùëõ, our goal is to compute a triangular transport map ùëÜ: Rùëõ‚ÜíRùëõ,
ùëÜ(ùë•) =
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
ùëÜ1(ùë•1)
ùëÜ2(ùë•1, ùë•2)
...
ùëÜùëõ(ùë•1, ùë•2, . . . ùë•ùëõ)
‚é§
‚é•‚é•‚é•‚é•‚é•‚é•‚é¶
,
(6.1)
that pushes forward the target to the reference density, i.e., ùëÜ‚ôØùúã= ùúÇ. As shown in
[213], one possibility is to minimize the K‚ÄìL divergence between ùëÜ‚ôØùúãand ùúÇover the
cone of monotone maps. The resulting optimization problem is convex and reads as
min
ùëÜ
‚àíE
[Ô∏É
log ùúÇ(ùëÜ(ùëç)) +
‚àëÔ∏Å
ùëò
log ùúïùëòùëÜùëò(ùëç)
]Ô∏É
(6.2)
s.t.
ùúïùëòùëÜùëò> 0 on Rùëõ,
ùëò= 1, . . . , ùëõ,
where the expectation is taken over the marginal distribution of ùëç‚àºùúã. See Section
5.2 for possible parameterizations of a monotone triangular map.
Given a map ùëÜ, we can easily sample from ùúãby inverting the map at samples
from ùúÇ. Inverting a triangular map is a computationally trivial task as it amounts
to a sequence of one-dimensional root findings (if the map were linear, then invert-
ing it would be equivalent to solve a triangular linear system by means of forward
substitution [110]).
If we choose a reference density that factorizes according to the product of its
marginals, i.e., if ùúÇùëã1,...,ùëãùëõ= ‚àèÔ∏Ä
ùëñùúÇùëãùëñ, then the optimization problem in (6.2) decouples
over the components of ùëÜ. That is, we can compute each component, ùëÜùëò, of the map
independently as follows:
min
ùëÜùëò
‚àíE
[Ô∏É
log ùúÇùëãùëò(ùëÜùëò(ùëç)) +
‚àëÔ∏Å
ùëò
log ùúïùëòùëÜùëò(ùëç)
]Ô∏É
(6.3)
s.t.
ùúïùëòùëÜùëò> 0 on Rùëõ,
179

for ùëò= 1, . . . , ùëõ. For instance, if ùúÇ= ùí©(0, I), then (6.3) reduces to
min
ùëÜùëò
E
[Ô∏É
1
2 (ùëÜùëò(ùëç))2 ‚àí
‚àëÔ∏Å
ùëò
log ùúïùëòùëÜùëò(ùëç)
]Ô∏É
(6.4)
s.t.
ùúïùëòùëÜùëò> 0 on Rùëõ,
and if we further use the samples (ùëßùëñ)ùëÄ
ùëñ=1 to approximate the expectation with respect
to ùúã, then the stochastic program can be written in a very simple form, i.e.,
min
ùëÜùëò
1
ùëÄ
ùëÄ
‚àëÔ∏Å
ùëñ=1
[Ô∏É
1
2 (ùëÜùëò(ùëßùëñ))2 ‚àí
‚àëÔ∏Å
ùëò
log ùúïùëòùëÜùëò(ùëßùëñ)
]Ô∏É
(6.5)
s.t.
ùúïùëòùëÜùëò> 0 on Rùëõ.
Now we make an important observation for the purpose of this chapter. Assume
that we want to sample from the conditional ùúãùëçùëò+1,...,ùëçùëõ|ùëç1,...,ùëçùëò( ¬∑ |ùúâ1, . . . , ùúâùëò) for some
ùëò‚â•1 and given ùúâ1:ùëò= (ùúâ1, . . . , ùúâùëò) ‚ààRùëò. By definition of the Knothe-Rosenblatt
rearrangement (see Appendix B), it suffices to consider the triangular map ùëÜùúâ1:ùëògiven
by
ùë•ùëò+1, . . . , ùë•ùëõ‚Ü¶‚Üí
‚é°
‚é¢‚é¢‚é¢‚é£
ùëÜùëò+1(ùúâ1:ùëò, ùë•ùëò+1)
...
ùëÜùëõ(ùúâ1:ùëò, ùë•ùëò+1, . . . , ùë•ùëõ)
‚é§
‚é•‚é•‚é•‚é¶.
(6.6)
It is easy to see that if ùúÇfactorizes as ùúÇ= ùúÇùëã1:ùëòùúÇùëãùëò+1:ùëõ, then ùëÜùúâ1:ùëòpushes forward
the desired conditional ùúãùëçùëò+1:ùëõ|ùëç1:ùëò( ¬∑ |ùúâ1:ùëò) to the reference marginal ùúÇùëãùëò+1:ùëõ. We thus
make a few remarks:
1. We can easily sample ùúãùëçùëò+1:ùëõ|ùëç1:ùëò( ¬∑ |ùúâ1:ùëò) by inverting the triangular map ùëÜùúâ1:ùëò
at samples from ùúÇùëãùëò+1:ùëõ
2. If we are only interested in ùëÜùúâ1:ùëò, then we really only need to compute (pos-
sibly in parallel) the components ùëÜùëò+1, . . . , ùëÜùëõof the inverse map via convex
optimization.
3. If the target density ùúãùëç1:ùëõhas a sparse Markov structure, then the map ùëÜ
inherits the sparsity pattern described in Section 5.4. A symmetric argument
180

also holds. By enforcing sparsity in the parameterization of each component of
ùëÜin (6.2), we can effectively ‚Äúproject‚Äù ùúãùëç1:ùëõ, according to the KL divergence,
onto a manifold of sparse Markov Random Fields, not necessarily Gaussian. In
this case, the result of the so-called M-projection [156] would be the pullback
density ùëÜ‚ôØùúÇ.
We are going to use extensively this construction in the proposed filtering algorithms.
6.3
Intuition for an abstract problem
We first shape our intuition on an abstract problem. We have a collection of random
variables, ùëç= (ùëç1, . . . , ùëçùëõ) and ùëå, with joint distribution ùúãùëç1:ùëõ,ùëå, and local likeli-
hood function, i.e., ùúãùëå|ùëç1:ùëõ= ùúãùëå|ùëç1, where we assume a local ordering of the state
variables so that the observed node is always the first one. (Here we only consider a
single measurement. We will generalize the procedure to multiple observations later
on in the section.)
We should think of ùëças a random variable on Rùëõdistributed according to the
forecast distribution at a certain time ùë°, whereas we can think of ùëåas a random
variable corresponding to the observed data at the same assimilation time. For in-
stance, if the state ùëçof the filtering problem corresponds to the spatial discretization
of a distributed process, then ùëçùëñmight denote the state at the ùëñth spatial location.
Assume that we also have ùëÄiid samples (ùëßùëñ)ùëÄ
ùëñ=1 from the marginal ùúãùëç1:ùëõ, i.e., each
ùëßùëñis a vector in Rùëõwith components
ùëßùëñ=
‚é°
‚é¢‚é¢‚é¢‚é£
ùëßùëñ
1
...
ùëßùëñ
ùëõ
‚é§
‚é•‚é•‚é•‚é¶
(6.7)
and also a sample from the forecast distribution. We assume that the only density
that we can evaluate in closed form is the local likelihood ùúãùëå|ùëç1. We remark that we
only have samples from ùúãùëç1:ùëõ, rather than the density itself.
181

Our goal is to generate approximate samples from the conditional ùúãùëç1:ùëõ|ùëågiven
some known realization of the data {ùëå= ùë¶}. We are thus in the usual framework
for the EnKF or the bootstrap particle filter.
A possible approach to this problem is to notice that the filtering distribution
ùúãùëç1:ùëõ|ùëåfactorizes as
ùúãùëç1:ùëõ|ùëå‚àùùúãùëç1|ùëåùúãùëç2:ùëõ|ùëç1,
(6.8)
where the latter term is independent of the data. This factorization is general as long
as the likelihood function is local and suggests the following sampling procedure:
1. Step 1 (local assimilation): sample from ùúãùëç1|ùëå.
2. Step 2 (propagation): given samples from ùúãùëç1|ùëå, sample from ùúãùëç2:ùëõ|ùëç1.
We address these two steps separately, since they are philosophically different.
Step 1 (local assimilation). First, we need to sample from ùúãùëç1|ùëå. One possibility
is certainly to assign likelihood weights to the forecast ensemble and then to perform a
resampling step, in the spirit of standard SMC methods [87]. This strategy, however,
could suffer when the marginal ùúãùëç1 and the likelihood ùúãùëå|ùëç1 are mutually singular
[56]. We propose to sample from an approximation of ùúãùëç1|ùëåby means of a local and
low-dimensional transport map. There are plenty of ways to compute this transport
map: for instance, one could adopt the parametric construction outlined in Section
5.2 or opt for a nonparametric approach, as in [178]. Either way, we need to estimate
the marginal density ùúãùëç1 in order to evaluate the local posterior
ùúãùëç1|ùëå‚àùùúãùëå|ùëç1 ùúãùëç1
(6.9)
in closed form, a prerequisite for the construction of the map. It is not hard to ap-
proximate the marginal ùúãùëç1 in low-dimensions since we already have forecast samples
(ùëßùëñ) at our disposal. One can use any preferred method: from kernel density esti-
mation (KDE) to implicit density estimation via inverse transport maps [266]. In
this chapter we opt for the latter strategy and seek an inverse map ùëÜ1 : R ‚ÜíR
that pushes forward ùúãùëç1 to a one dimensional reference density ùúÇùëã1, e.g., a standard
182

normal or any other simple distribution with a tail behavior similar to that of ùúãùëç1. In
particular, we adopt the construction outlined in Section 6.2 for a map from samples
and find a parametric approximation of a monotone ùëÜ1 via convex optimization. The
resulting approximation of ùúãùëç1 is given by the pullback density
ùúÇùëã1(ùëÜ1(ùë•)) dùëÜ1(ùë•)
dùë•
,
(6.10)
which is easy to evaluate and that yields a corresponding approximation of ùúãùëç1|ùëå. A
parametric approximation of ùúãùëç1 has the virtue of filling in the space (with probability
mass), and thus makes the ensuing conditioning on the data ‚Äúrobust‚Äù whenever ùúãùëç1
and ùúãùëå|ùëç1 are mutually singular.
Now we need to compute a transport map ùëá1 : R ‚ÜíR that samples the approxi-
mation of ùúãùëç1|ùëådescribed above. For instance, we might characterize the increasing
rearrangement on R via optimization or direct numerical integration [271]. Also the
computation of ùëá1 requires to specify a reference density. For simplicity, we consider
the same reference density ùúÇùëã1 used to compute ùëÜ1, but other choices are certainly
possible.
Given the pair of maps ùëÜ1 and ùëá1, we can easily generate approximate samples
(ùëéùëñ
1)ùëÄ
ùëñ=1 from ùúãùëç1|ùëåby setting
ùëéùëñ
1 = ùëá1(ùëÜ1(ùëßùëñ
1)),
(6.11)
for ùëñ= 1, . . . , ùëÄ. An important point: we do not sample ùúãùëç1|ùëåby pushing forward
samples from ùúÇùëã1 through ùëá1, but rather update each forecast particle directly using
the composition ùëá1 ‚àòùëÜ1. If the maps (ùëÜ1, ùëá1) were exact, then these two sampling
procedures would be equivalent. But if we only have a numerical approximation of
(ùëÜ1, ùëá1), then the sampling strategy that we advocate performs empirically better.
The underlying intuition is exactly the core philosophy in EnKF, which despite the
Gaussian approximation of the forecast distribution, generates approximate samples
from the filtering distribution‚Äîwith possibly non-Gaussian statistics‚Äîby pushing
forward forecast particles through a linear map.
183

Step 2 (propagation). We need to sample from ùúãùëç2:ùëõ|ùëç1 given the events {ùëç1 = ùëéùëñ
1}
for ùëñ= 1, . . . , ùëÄ. We call this step propagation, as we effectively need to propagate
information from the local filtering marginal ùúãùëç1|ùëåto every remaining state variable.
A first simple solution to this problem is to use the construction of Section 6.2 for
conditional simulation. That is, compute a triangular map2 ùëÜ: Rùëõ‚ÜíRùëõ,
ùëÜ(ùë•) =
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
ùëÜ1(ùë•1)
ùëÜ2(ùë•1, ùë•2)
...
ùëÜùëõ(ùë•1, ùë•2, . . . ùë•ùëõ)
‚é§
‚é•‚é•‚é•‚é•‚é•‚é•‚é¶
,
(6.12)
that pushes forward ùúãùëç1:ùëõ‚Äîavailable through samples‚Äîto a reference density that
factorizes as ùúÇ(ùë•) = ‚àèÔ∏Ä
ùëñùúÇùëãùëñ(ùë•ùëñ), e.g., a standard normal. (The first component of
(6.12) coincides with the map ùëÜ1 computed in the local assimilation step, assuming
that the corresponding reference densities match along the first marginal, as we do
here.) Then, for all ùúâ‚ààR, consider the restriction ùëÜùúâ: Rùëõ‚àí1 ‚ÜíRùëõ‚àí1 given by
ùë•ùëò+1, . . . , ùë•ùëõ‚Ü¶‚Üí
‚é°
‚é¢‚é¢‚é¢‚é£
ùëÜ2(ùúâ, ùë•2)
...
ùëÜùëõ(ùúâ, ùë•2, . . . , ùë•ùëõ)
‚é§
‚é•‚é•‚é•‚é¶,
(6.13)
and compute the ‚Äúmarginal‚Äù filtering samples (ùëéùëñ
2:ùëõ)ùëÄ
ùëñ=1‚Äîi.e., approximate samples
from ùúãùëç2:ùëõ|ùëå‚Äîas
ùëéùëñ
2:ùëõ= ùëÜ‚àí1
ùëéùëñ
1 ( ùëÜùëßùëñ
1( ùëßùëñ
2:ùëõ) ),
(6.14)
for ùëñ= 1, . . . , ùëÄ, where
ùëéùëñ
2:ùëõ=
‚é°
‚é¢‚é¢‚é¢‚é£
ùëéùëñ
2
...
ùëéùëñ
ùëõ
‚é§
‚é•‚é•‚é•‚é¶,
ùëßùëñ
2:ùëõ=
‚é°
‚é¢‚é¢‚é¢‚é£
ùëßùëñ
2
...
ùëßùëñ
ùëõ
‚é§
‚é•‚é•‚é•‚é¶,
(6.15)
2 Notice that we use a lower triangular function because we assigned a local ordering so that the
observed node is also the first one in the ordering. This construction is always possible.
184

and where (ùëéùëñ
1)ùëÄ
ùëñ=1 were obtained in the local assimilation step. The inversion of ùëÜùëéùëñ
1
in (6.14) is trivial since ùëÜùëéùëñ
1 is a triangular map: we just need to perform a sequence
of ordered one-dimensional root-findings (see [212] for additional details). Moreover,
each component of ùëÜcan be computed independently and in parallel using only fore-
cast samples.
Finally, by concatenating ùëéùëñ
1 and ùëéùëñ
2:ùëõ, we can obtain approximate
filtering samples as
ùëéùëñ=
‚é°
‚é¢‚é¢‚é¢‚é£
ùëéùëñ
1
...
ùëéùëñ
ùëõ
‚é§
‚é•‚é•‚é•‚é¶,
(6.16)
for ùëñ= 1, . . . , ùëÄ. The resulting filtering samples can be written as a (possibly non-
linear) function of the forecast ensemble. This transformation is given by the compo-
sition
ùíØ(ùë•) =
‚é°
‚é£ùëá1(ùë•1)
ùëÜ‚àí1
ùëá1(ùë•1)(ùë•2, . . . , ùë•ùëõ)
‚é§
‚é¶‚àòùëÜ(ùë•),
(6.17)
so that ùëéùëñ= ùíØ(ùëßùëñ) for all ùëñ.
In practice, one can then iterate this very same construction by assimilating a new
local observation at the same time ùë°, and by using (ùëéùëñ)ùëÄ
ùëñ=1 as the new ‚Äúprior‚Äù samples.
In fact, one has, for instance,
ùúãùëç1:ùëõ|ùëå1,ùëåùëò‚àùùúãùëåùëò|ùëçùëòùúãùëç1:ùëõ|ùëå1,
(6.18)
where we assumed that the likelihood for ùëåùëòis again local and only a function of the
corresponding variable ùëçùëò. When iterating the construction for ùëåùëò, it is important to
choose a new local ordering of the state variables so that ùëçùëòcorresponds to the first
element in the ordering. Once every measurement at time ùë°is assimilated, we can
propagate the filtering samples through the system dynamic to the next assimilation
time, e.g., ùë°+ Œîùë°obs.
In this section we presented an assimilation strategy that processes data sequen-
tially one at a time, but the proposed framework can be easily generalized to batches
of data. For instance, we can assimilate a pair of local measurements (ùëå1, ùëå2) follow-
185

ing the decomposition
ùúãùëç1:ùëõ|ùëå1,ùëå2 ‚àùùúãùëç1,ùëç2|ùëå1,ùëå2 ùúãùëç3:ùëõ|ùëç1,ùëç2
(6.19)
of the filtering distribution and repeating the construction above with minor modifi-
cations. The price to pay for the batch strategy is that now we need to solve a local
assimilation problem in two, rather than one, dimensions: not a big deal as long as
the dimension of the batches is limited.
In the next section we address the parameterization of the (possibly high-dimensional)
map ùëÜ.
6.3.1
Parameterizing a high dimensional inverse map
ùëÜis a multivariate function on Rùëõand so we must discuss the parameterization of its
components. A simple choice that would correspond to an EnKF algorithm is to let
each component ùëÜùëòto be a linear function of its inputs, i.e.,
ùëÜùëò(ùë•1, . . . , ùë•ùëò) = ùëê0 +
ùëò
‚àëÔ∏Å
ùëñ=1
ùëêùëñùë•ùëñ,
(6.20)
where (ùëê0, . . . , ùëêùëò) are ùëò+ 1 parameters to be found via optimization. But the frame-
work that we investigate here is far more general, and it allows to consider any
monotone parameterization of the map. For instance, let ùíúùëòbe a set of indeces (in-
cluding ùëò) that correspond to nodes that are spatial neighbors of ùëçùëò. Then we might
consider a parameterization of ùëÜùëò,
ùëÜùëò(ùë•1, . . . , ùë•ùëò) = ùëê0 +
‚àëÔ∏Å
ùëñ‚ààùíúùëê
ùëò
ùëêùëñùë•ùëñ+ Œ®(ùë•ùíúùëò),
(6.21)
where the nonlinearity is only restricted to variables in ùíúùëò. For instance, Œ® could be
a nonlinear function given by a multivariate polynomial chaos expansion of a certain
order [277]. In Section 6.4 we will give yet another example of scalable parameteri-
zation for ùëÜ. This flexibility in the parameterization of the map allows to introduce
186

local nonlinearities in a way that scales well with dimensions: transport maps offer
an opportunity to depart from linear EnKF updates in a scalable and principled way.
There is plenty of structure in the filtering problem that can be translated into low-
dimensional parameterizations for ùëÜ. As an example consider the decay of correlation
in the forecast distribution, and let ùëü> 0 be a localization radius, i.e., we assume that
if ùëë(ùëçùëñ, ùëçùëó) < ùëü, then ùëçùëñand ùëçùëóare marginally independent, where ùëë(¬∑, ¬∑) denotes a
distance function on the physical space. This localization radius is frequently used
in EnKF algorithms to localize covariance computations, and is an empirically tuned
parameter [150, 93].
Marginal independence is also reflected into sparsity of the
inverse map (see Section 5.4).
That is, from each component ùëÜùëò(ùë•1, . . . , ùë•ùëò) we
can drop the explicit dependence on variables ùë•ùëówith ùëë(ùëçùëó, ùëçùëò) < ùëü, and turn the
corresponding component into a function that is local in state space. Even more, if
ùëç1 is the observed node, then we can set every component ùëÜùëówith ùëë(ùëçùëó, ùëç1) < ùëüto
be the identity function,
ùëÜùëó(ùë•1, . . . , ùë•ùëó) = ùë•ùëó,
(6.22)
since the local observation will not update ‚Äúsignificantly‚Äù the forecast distribution
beyond the localization radius.
Thus, accounting for decay of correlations in the
forecast distribution can reduce dramatically the complexity of parameterizing ùëÜ.
An additional source of low-dimensional structure is related to conditional inde-
pendence. We address this point in the next section.
6.3.2
Introducing Markov assumptions
If the state of the filtering problem represents the discretization of a distributed pro-
cess (like the solution of a PDE), then in many cases it is true that the forecast
distribution can be well approximated by a sparse Markov Random Field (MRF).
For instance, this observation is at the core of modeling with sparse Gaussian MRFs
[236]. In this context, ‚Äúapproximated‚Äù is a key word since the filtering distribution
will not satisfy, in general, any Markov property, strictly speaking [156]. But consid-
ering approximate Markov structure has several important benefits that we (briefly)
187

describe in this section. A pioneering work at the intersection of filtering and sparse
MRFs is certainly [155], on which we build upon.
Sometimes it is possible to learn the approximate Markov structure of the forecast
distribution from samples [189, 156, 124], but in most cases, especially when dealing
with a limited ensemble size, one can simply impose the sparse Markov structure
when computing the map ùëÜ, as a solution of (6.2), in the propagation step. This
constraint can be easily enforced in terms of sparsity of ùëÜ, as explained in Section 5.4.
In particular, the sparsity of ùëÜis given by Theorem 5.1 by just looking at the graph
that represents the Markov structure. The resulting map implicitly approximates
the projection of the forecast distribution onto a manifold of sparse MRFs. If the
likelihoods are local (pointwise measurements), then the approximate forecast and
filtering distributions have the same Markov structure.
When dealing with spatially distributed processes, there are intuitive choices of
Markov structure. For instance, in 1-D we might want to consider chains or cycles,
in 2-D grids, and so on. In particular, we can regard the neighborhood size of the
graph as a (discrete) tuning parameter of the algorithm, in the same spirit as the
localization radius.
The sparsity of ùëÜinduced by approximate Markov properties should be added to
the sparsity of ùëÜinduced by the decay of correlations (and discussed in the previous
section). It is thus an additional form of regularization that increases the efficiency
of computations. As discussed in Section 5.4, the sparsity of ùëÜinduced by Markov
assumptions can be huge. For instance, if the underlying graphical model were a
cycle, then each component of ùëÜwould depend at most on three variables, regardless
of the dimension of the state.
So far, we showed how the components of ùëÜcan be sparse (due to correlation
and conditional independence), and how they can be computed independently and
in parallel via convex optimization.
In order to transform the forecast ensemble,
however, we need to invert ùëÜ, as explained in Section 6.3 (see equation 6.17). This
operation‚Äîwhich amounts to a sequence of 1-D root findings‚Äîis cheap, but requires
to evaluate the components of ùëÜin an orderly fashion (like in forward substitution).
188

A perhaps unintuitive consequence of the Markov assumption, is that the inversion of
ùëÜcan be parallelized, if we choose appropriate orderings for the triangular map. We
make this fact explicit for the simple Markov structure in Figure 6-1. The reader can
then simply extrapolate this strategy to more complex graphs. Given the ‚Äúspecial‚Äù
ordering of Figure 6-1, the map ùëÜhas the following sparsity pattern (Theorem 5.1):
ùëÜ(ùë•) =
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
ùëÜ1(ùë•1)
ùëÜ2(ùë•1, ùë•2)
ùëÜ3(ùë•1, ùë•2, ùë•3)
ùëÜ4(ùë•1, ùë•2,ùë•3, ùë•4)
ùëÜ5(ùë•1, ùë•2, ùë•3, ùë•4,ùë•5)
ùëÜ6(ùë•1,ùë•2, ùë•3, ùë•4, ùë•5,ùë•6)
ùëÜ7(ùë•1, ùë•2, ùë•3, ùë•4, ùë•5,ùë•6, ùë•7)
‚é§
‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é¶
,
(6.23)
which suggests a simple strategy to invert ùëÜat a point ùëß.
Let ùúâ= ùëÜ‚àí1(ùëß) and
consider the following steps:
1. Determine ùúâ1:3 = (ùúâ1, ùúâ2, ùúâ3) by inverting the submap
(ùë•1, ùë•2, ùë•3) ‚Ü¶‚Üí
‚é°
‚é¢‚é¢‚é¢‚é£
ùëÜ1(ùë•1)
ùëÜ2(ùë•1, ùë•2)
ùëÜ3(ùë•1, ùë•2, ùë•3)
‚é§
‚é•‚é•‚é•‚é¶
(6.24)
at ùëß1:3 = (ùëß1, ùëß2, ùëß3).
2. Determine ùúâ4:5 = (ùúâ4, ùúâ5) by inverting the submap
(ùë•4, ùë•5) ‚Ü¶‚Üí
‚é°
‚é£ùëÜ4(ùúâ1, ùë•2,ùúâ3, ùë•4)
ùëÜ5(ùúâ1, ùúâ2, ùë•3, ùë•4,ùë•5)
‚é§
‚é¶
(6.25)
at ùëß4:5 = (ùëß4, ùëß5).
189

3. Determine ùúâ6:7 = (ùúâ6, ùúâ7) by inverting the submap
(ùë•6, ùë•7) ‚Ü¶‚Üí
‚é°
‚é£ùëÜ6(ùë•1,ùúâ2, ùúâ3, ùë•4, ùë•5,ùë•6)
ùëÜ7(ùë•1, ùë•2, ùë•3, ùë•4, ùë•5,ùë•6, ùë•7)
‚é§
‚é¶
(6.26)
at ùëß6:7 = (ùëß6, ùëß7).
The key observation is that steps 2 and 3 can now be executed in parallel. What
is special about the ordering in Figure 6-1 is that the first variables in the ordering
correspond to (1) the observed node, and (2) a separator set for the graph. Clearly
this strategy can be applied to more general graphs, even recursively. For instance,
when dealing with a grid graph (in 2-D), one can choose a ring containing the observed
node as a separator set.
ùëç7
ùëç3
ùëç4
ùëç1
ùëç5
ùëç2
ùëç6
ùëå
Figure 6-1: Cycle graph used as a Markov structure for the filtering distribution in
the simple example of Section 6.3.2.
6.4
Numerical example: Lorenz 96
We demonstrate a particular version of the algorithm presented in Section 6.3 on a
challenging test case configuration for the Lorenz 96 model [180], which is intended to
reproduce coarse features of the mid-latitude atmosphere [182] and is frequently used
190

as a testbed for numerical weather prediction algorithms. The goal of this example is
to show how the use of local nonlinear couplings can outperform state-of-the-art EnKF
methodologies, even in terms of tracking (RMSE), which is a notoriously challenging
task [18]. We leave further numerical experiments to a forthcoming publication.
The test case configuration is taken from [18], which also develops its own nonlin-
ear filter based on Gaussian mixtures. The model dynamic is prescribed as a set of
deterministic nonlinear ODEs,
dùëçùëó
dùë°= (ùëçùëó+1 ‚àíùëçùëó‚àí2) ùëçùëó‚àí1 ‚àíùëçùëó+ ùêπ,
(6.27)
for ùëó= 1, . . . , 40, periodic boundary conditions, and ùêπ= 8 (chaotic regime). The
equations in (6.27) represent the spatial discretization of a one dimensional time
dependent PDE [230]. The state ùëç= (ùëç1, . . . , ùëç40) is 40-dimensional. We discretize
the dynamic in (6.27) using a Runge-Kutta 4 method with constant stepsize Œîùë°=
0.01. We observe the state locally in space and time according to the model
ùëåùëó= ùëçùëó+ ùúÄùëó,
(6.28)
for ùëó= 1, 3, 5, . . . , 39‚Äîi.e., we observe every other component‚Äîand ùúÄùëó‚àºùí©(0, 0.5).
The (ùúÄùëó)ùëóare mutually independent of each other and of the state. We make obser-
vations every Œîùë°obs = 0.4 time units, which corresponds to 40 discrete time steps,
and which is considered to be quite large for the purpose of estimation (e.g., highly
non-Gaussian statistics of the forecast distribution [18]).
We consider an estimation window of 2000 assimilation cycles. The reference solu-
tion and the observations are generated with the same model used for the assimilation,
following the usual ‚Äúidentical twin experiment‚Äù setting [230].
In the next section we describe the particular configuration of the filtering algo-
rithm used for the numerical experiments.
191

6.4.1
Configuration of the nonlinear filter
We use the same nonlinear filter induced by local couplings described in Section 6.3,
with the following choice of parameters, which is by no means optimal:
‚Ä¢ At each assimilation time, we process measurements sequentially one at a time.
‚Ä¢ We project the filtering distribution onto a manifold of non-Gaussian MRFs
with the Markov structure given by Figure 6-2, where each node is connected
with the 5 closest neighbors on the graph. See the discussion of Section 6.3.2
for more details. For comparison, Figure 6-3 shows the typically dense Markov
structure of the filtering distribution after a few assimilation cycles [156]. The
fact that the actual Markov structure is dense, does not mean that the filtering
distribution cannot be well-approximated by a sparse MRF [155]. The numerical
results in the next section support this claim.
‚Ä¢ Each component ùëÜùëòof the inverse map ùëÜused in the propagation step follows
the sparsity pattern predicted by Theorem 5.1 for the Markov structure of
Figure 6-2. Moreover, each ùëÜùëòis parameterized as follows:
ùëÜùëò(ùë•ùëó1, . . . , ùë•ùëó‚Ñì, ùë•ùëò) = ùúì(ùë•ùëó1) + ¬∑ ¬∑ ¬∑ + ùúì(ùë•ùëó‚Ñì) + ùúì(ùë•ùëò),
(6.29)
with
ùúì(ùë•) = ùëê0 + ùëê1 ùë•+
7
‚àëÔ∏Å
ùëñ=2
ùëêùëñexp(‚àí1
2ùúé2(ùë•‚àíùúâùëñ)2),
(6.30)
where (ùëê0, . . . , ùëê7) are optimization parameters‚Äîthese parameters are different
for each instance of ùúìin (6.29)‚Äî, (ùúâ2, . . . , ùúâ7) are the abscissas of a sixth or-
der Hermite quadrature rule on R, and where ùúé= 1.35. The choice of these
parameters is somewhat arbitrary and could be improved.
‚Ä¢ Each local optimization problem for ùëÜùëòis solved via Newton‚Äôs method.
‚Ä¢ The direct map ùëá1 in the local assimilation step (Section 6.3) is the increasing
rearranegement on R, computed via accurate numerical integration.
192

‚Ä¢ We use standard normal reference densities throughout the algorithm.
‚Ä¢ We also employ standard multiplicative inflation [182] with parameter ùëüinfl =
0.1.
We use the acronym LocNLF to denote the resulting nonlinear filtering algorithm.
If, in the LocNLF algorithm, we restrict every transport map to be linear, we obtain
an EnKF-type algorithm that we call LocLF. The only difference between these two
algorithms is the degree of nonlinearity of the local couplings.
6.4.2
Numerical results
Numerical results in terms of time-averaged RMSE (over 2000 assimilation cycles)
are shown in Table 6.1 for different filtering algorithms (EnKF, LocLF, LocNLF) and
different ensemble sizes (400 or 200). The values of RMSE for the EnKF algorithm
with 400 particles are those published in [18] for the same problem configuration as
the one considered here, but necessarily for a different data set. [18] uses a state-
of-the-art EnKF algorithm‚Äîwith an optimized tapering function [103]‚Äîin order to
establish a baseline measure of performance.
First of all, we notice that the values of RMSE for the EnKF in Tabe 6.1 are
almost identical to those of the LocLF algorithm (400 particles). Also the LocLF
can be thought of as a particular EnKF algorithm: one that uses a linear forecast-to-
filtering update, and that regularizes the covariance estimation by imposing sparsity
on its precision matrix [27], rather than tapering the covariance. Both the EnKF of
[18] and LocLF achieve similar performance on this challenging test case.
Second, and most importantly, we observe how the proposed nonlinear filtering
algorithm (LocNLF) is about 25% more accurate in every RMSE statistic (i.e.,
median, mean, variance) than any of its linear competitors, using either 400 or 200
particles. Figure 6-4 shows the excellent tracking performance of LocNLF over the
last 100 assimilation cycles.
The only difference between LocLF and LocNLF is in the nonlinearity of their
couplings. Thus, the 25% increase in accuracy of LocNLF over LocLF is entirely
193

due to the use of nonlinear transformations. This last observation reinforces the main
take-home message for this chapter: combining different sources of low-dimensional
structure (e.g., locality of observations, decay of correlations, approximately sparse
Markov structure) with local, low-dimensional, nonlinear couplings, produces nonlin-
ear filtering algorithms that can outperform state-of-the-art EnKF techniques, for the
same ensemble size, and at a marginal increase in computational cost.
Figure 6-2: Sparse Markov structure used by the nonlinear filter to project the filtering
distribution. Each node is connected with the 5 closest neighbors.
#particles: 400
#particles: 200
EnKF[18]
LocLF
LocNLF
LocLF
LocNLF
med RMSE
0.88
0.87
0.64
0.91
0.66
avg RMSE
0.97
0.99
0.74
1.02
0.79
var RMSE
0.12
0.1
0.06
0.1
0.09
Table 6.1: We report the median (med), the average (avg), and the variance (var) of
the time-averaged RMSE (over 2000 assimilation cycles) for different filtering algo-
rithms and number of particles. The proposed nonlinear filter (LocNLF) is ‚âà25%
more accurate in RMSE than a state-of-the-art EnKF algorithm.
194

Figure 6-3:
Actual dense Markov structure of a typical filtering distribution.
195

Figure 6-4: Comparison between the true signal (left) and the mean of the proposed
nonlinear filter (right), over the last 100 assimilation cycles. The overall agreement
between the two fields is excellent.
196

Appendix A
Rao‚Äôs metric between distributions
Let M = {ùúãùúÉ, ùúÉ‚ààŒò} be a parametric family of probability densities indexed by
ùúÉ= (ùúÉ1, . . . , ùúÉùëõ) ‚ààŒò [9]. Rao considered a quadratic differential form given by
dùë†2 =
‚àëÔ∏Å
ùëñ,ùëó
ùëîùëñùëó(ùúÉ) dùúÉùëñdùúÉùëó,
(A.1)
where ùëîùëñùëó(ùúÉ) = EùúãùúÉ[ ùúïùúÉùëñln ùúãùúÉùúïùúÉùëóln ùúãùúÉ] are the entries of the Fisher information ma-
trix, with EùúãùúÉdenoting integration with respect to ùúãùúÉ[95]. The Fisher information
matrix is a central object in mathematical statistics (e.g., the Cram√©r-Rao inequality
[223]). Intuitively, we can interpret (A.1) as the variance of the function that de-
scribes the first order relative difference between ùúãùúÉand a contiguous density, ùúãùúÉ+dùúÉ,
on M [224]. The definition of a quadratic form like (A.1) allows us to measure curves
on M .
Given a smooth curve ùõæ: [0, 1] ‚ÜíŒò ‚âÉM , we can define its length as
‚Ñì(ùõæ) :=
‚à´Ô∏Ä1
0 (‚àëÔ∏Ä
ùëñ,ùëóùëîùëñùëó(ùõæ(ùë°)) dùõæùëñdùõæùëó)1/2 dùë°[86]. Thus, Rao‚Äôs distance between a pair of
distributions on M is simply their geodesic distance, i.e., the length of the minimum
length curve joining these distributions [224]. The quadratic form defined by the
Fisher information matrix is invariant under regular reparameterizations of M [223].
Thus, this fundamental invariance is also shared by Rao‚Äôs distance which yields an
intrinsic way of comparing distributions on M . Of course, it is possible to consider
more general quadratic differential forms not based on the notion of Fisher infor-
mation. See [225] for various examples of differential metrics derived from entropy
197

functions or divergence measures between probability distributions. See [4] for a mod-
ern treatment of information geometry, the field at the intersection of statistics and
differential geometry.
198

Appendix B
Generalized Knothe-Rosenblatt
rearrangement
In this section we first review the classical notion of KR rearrangement [234, 154],
and then give a formal definition for a generalized KR rearrangement, i.e., a transport
map that is lower triangular up to a permutation. A disclaimer: these transports can
also be defined under weaker conditions than those considered here, at the expense,
however, of some useful regularity (e.g., see [34]).
The following definition introduces the one-dimensional version of the KR-rearrangement,
and it is key to extend the transport to higher dimensions.
Definition B.1 (Increasing rearrangement on R). Let ùúàùúÇ, ùúàùúã‚ààM+(R), and let ùêπ, ùê∫
be their respective cumulative distribution functions, i.e., ùêπ(ùë°) = ùúàùúÇ((‚àí‚àû, ùë°)) and
ùê∫(ùë°) = ùúàùúã((‚àí‚àû, ùë°)). Then the increasing rearrangement on R is given by ùëá= ùê∫‚àí1‚àòùêπ.
Under the hypothesis of Definition B.1, it is easy to see that both ùêπand ùê∫are
homeomorphisms, and that ùëáis a strictly increasing map that pushes forward ùúàùúÇto
ùúàùúã[244].
Definition B.2 (Knothe-Rosenblatt rearrangement). Given ùëã‚àºùúàùúÇ, ùëç‚àºùúàùúã, with
ùúàùúÇ, ùúàùúã‚ààM+(Rùëõ), and a pair ùúÇ, ùúãof strictly positive densities for ùúàùúÇand ùúàùúã, re-
spectively, the corresponding KR rearrangement is a triangular map ùëá: Rùëõ‚ÜíRùëõ
199

defined, recursively, as follows. For all ùë•1:ùëò‚àí1 ‚ààRùëò‚àí1, the map ùúâ‚Ü¶‚Üíùëáùëò(ùë•1:ùëò‚àí1, ùúâ)‚Äî
the restriction of the ùëòth component of ùëáonto its first ùëò‚àí1 inputs‚Äîis defined as
the increasing rearrangement on R that pushes forward ùúâ‚Ü¶‚ÜíùúÇùëãùëò|ùëã1:ùëò‚àí1(ùúâ|ùë•1:ùëò‚àí1) to
ùúâ‚Ü¶‚Üíùúãùëçùëò|ùëç1:ùëò‚àí1(ùúâ|ùëá1(ùë•1), . . . , ùëáùëò‚àí1(ùë•1:ùëò‚àí1)), where ùúÇùëãùëò|ùëã1:ùëò‚àí1 and ùúãùëçùëò|ùëç1:ùëò‚àí1 are condi-
tional densities defined as in (2.2).
Notice that for any measure ùúàin M+(Rùëõ) there always exists a strictly positive
version of its density. By considering such positive densities in Definition B.2, we can
define the KR rearrangement on the entire Rùëõ[34]. In fact, we should really think of
Definition B.2 as providing a possible version of the KR rearrangement (recall that
the increasing triangular transport is unique up to sets of measure zero [244]). Since
in this case ùúàùúãis equivalent to the Lebesgue measure (ùúàùúã(ùíú) =
‚à´Ô∏Ä
ùíúùúã(ùë•) ùúÜ(dùë•) = 0 ‚áí
ùúÜ(ùíú) = 0 if ùúã> 0 a.e.), the component (5.2) is also absolutely continuous on all
compact intervals [34, Lemma 2.4]. As a result, the rearrangement can be used to
define general change of variables as well as pullbacks and pushforwards with respect
to arbitrary densities, as shown by the following lemma adapted from [34].
Lemma B.1. Let ùëábe an increasing triangular bijection on Rùëõsuch that the functions
ùúâ‚Ü¶‚Üíùëáùëò(ùë•1, . . . , ùë•ùëò‚àí1, ùúâ)
(B.1)
are absolutely continuous on all compact intervals for a.e. (ùë•1, . . . , ùë•ùëò‚àí1) ‚ààRùëò‚àí1.
Then for any integrable function ùúô, it holds:
‚à´Ô∏Å
ùúô(ùë¶) dùë¶=
‚à´Ô∏Å
ùúô(ùëá(ùë•)) det ‚àáùëá(ùë•) dùë•,
(B.2)
where det ‚àáùëá:= ‚àèÔ∏Äùëõ
ùëò=1 ùúïùëòùëáùëò. In particular, if ùúàùúåis a measure on Rùëõwith density ùúå,
then we also have ùëá‚ôØùúàùúå‚â™ùúÜwith density (a.e.):
ùëá‚ôØùúå(ùë•) = ùúå(ùëá(ùë•)) det ‚àáùëá(ùë•).
(B.3)
The lemma can also be applied to the inverse KR rearrangement ùëá‚àí1 to show that
ùëá‚ôØùúàùúå‚â™ùúÜ, where the form of the corresponding pushforward density ùëá‚ôØùúåis given by
200

replacing ùëáwith ùëá‚àí1 in (B.3). We will use these results extensively in the proofs of
Appendix ??. Notice, however, that Lemma B.1 does not hold for a generic triangular
function: the map must be somewhat regular, in the sense specified by the lemma.
See [34] for an in depth discussion.
We now give a constructive definition for a generalized KR rearrangement.
Definition B.3 (Generalized Knothe-Rosenblatt rearrangement). Given ùëã‚àºùúàùúÇ,
ùëç‚àºùúàùúã, with ùúàùúÇ, ùúàùúã‚ààM+(Rùëõ), a pair ùúÇ, ùúãof strictly positive densities for ùúàùúÇand
ùúàùúã, respectively, and a permutation ùúéof Nùëõ, the corresponding ùúé-generalized KR
rearrangement is a ùúé-triangular map1 ùëá: Rùëõ‚ÜíRùëõdefined at any ùë•‚ààRùëõusing the
following recursion in ùëò. The map ùúâ‚Ü¶‚Üíùëáùúé(ùëò)(ùë•ùúé(1), . . . , ùë•ùúé(ùëò‚àí1), ùúâ) is defined as the
increasing rearrangement on R that pushes forward ùúâ‚Ü¶‚ÜíùúÇùëãùúé(ùëò)|ùëãùúé(1:ùëò‚àí1)(ùúâ|ùë•ùúé(1:ùëò‚àí1)) to
ùúâ‚Ü¶‚Üíùúãùëçùúé(ùëò)|ùëçùúé(1:ùëò‚àí1)(ùúâ|ùëáùúé(1)(ùë•ùúé(1)), . . . , ùëáùúé(ùëò‚àí1)(ùë•ùúé(1:ùëò‚àí1))),
(B.4)
where ùë•ùúé(1:ùëò‚àí1) = ùë•ùúé(1), . . . , ùë•ùúé(ùëò‚àí1).
Existence of a generalized KR rearrangement follows trivially from its definition.
Moreover, the transport map satisfies all the regularity properties discussed for the
classic KR rearrangement, including Lemmas 5.1 and B.1. Thus we will often cite
these two results when dealing with generalized KR rearrangements in our proofs.
The following lemma shows that the computation of a generalized KR rearrangement
is also essentially no different than the computation of a lower triangular transport
(and thus all the discussion of Section 5.2 readily applies).
Lemma B.2. Given ùúàùúÇ, ùúàùúã‚ààM+(Rùëõ), let ùëábe a ùúé-generalized KR rearrangement
that pushes forward ùúàùúÇto ùúàùúãfor some permutation ùúé. Then ùëá= ùëÑ‚ä§
ùúé‚àòùëá‚Ñì‚àòùëÑùúéa.e.,
where ùëÑùúé‚ààRùëõ√óùëõis a matrix representing the permutation, i.e., (ùëÑùúé)ùëñùëó= (ùëíùúé(ùëñ))ùëó,
and where ùëá‚Ñìis a (lower triangular) KR rearrangement that pushes forward (ùëÑùúé)‚ôØùúàùúÇ
to (ùëÑùúé)‚ôØùúàùúã.
Proof. If ùëá‚Ñìpushes forward (ùëÑùúé)‚ôØùúàùúÇto (ùëÑùúé)‚ôØùúàùúã, then ùúàùúÇ‚àòùëÑ‚ä§
ùúé‚àòùëá‚àí1
‚Ñì
= ùúàùúã‚àòùëÑ‚ä§
ùúé, and
so ùëá= ùëÑ‚ä§
ùúé‚àòùëá‚Ñì‚àòùëÑùúémust push forward ùúàùúÇto ùúàùúã. Moreover, notice that ùëáùúé(ùëò)(ùë•) =
1 See Definition 5.3.
201

ùëáùëò
‚Ñì(ùë•‚ä§ùëíùúé(1), . . . , ùë•‚ä§ùëíùúé(ùëò)), which shows that ùëáis a monotone increasing ùúé-generalized
triangular function (see Definition 5.3). The lemma then follows by ùúàùúÇ-uniqueness of
a KR rearrangement.
202

Appendix C
Proofs for Chapter 3
Here we collect the proofs and other technical results necessary to support the state-
ments made in Chapter 3.
We start with an auxiliary approximation result that plays an important role in our
analysis. Given a semi-positive definite diagonal matrix ùê∑, we seek an approximation
of ùê∑+ ùêºby a rank ùëüperturbation of the identity, ùëàùëà‚ä§+ ùêº, that minimizes a loss
function from the class ‚Ñídefined in (3.8).
The following lemma shows that the
optimal solution ÃÇÔ∏ÄùëàÃÇÔ∏Äùëà‚ä§is simply the best rank ùëüapproximation of the matrix ùê∑in
the Frobenius norm.
Lemma C.1 (Approximation lemma). Let ùê∑= diag{ùëë2
1, . . . , ùëë2
ùëõ}, with ùëë2
ùëñ‚â•ùëë2
ùëñ+1,
and ùêø‚àà‚Ñí. Define the functional ùí•: Rùëõ√óùëü‚ÜíR, as: ùí•(ùëà) = ùêø(ùëàùëà‚ä§+ ùêº, ùê∑+ ùêº) =
‚àëÔ∏Ä
ùëñùëì(ùúéùëñ), where (ùúéùëñ) are the generalized eigenvalues of the pencil (ùëàùëà‚ä§+ ùêº, ùê∑+ ùêº)
and ùëì‚ààùí∞. Then:
(i) There is a minimizer, ÃÇÔ∏Äùëà, of ùí•such that
ÃÇÔ∏ÄùëàÃÇÔ∏Äùëà‚ä§=
ùëü
‚àëÔ∏Å
ùëñ=1
ùëë2
ùëñùëíùëñùëí‚ä§
ùëñ.
(C.1)
where (ùëíùëñ) are the columns of the identity matrix.
(ii) If the first ùëüeigenvalues of ùê∑are distinct, then any minimizer of ùí•satisfies
(C.1).
203

Proof. The idea is to apply [165, Theorem 1.1] to the functional ùí•. To this end,
we notice that ùí•can be equivalently written as: ùí•(ùëà) = ùêπ‚àòùúåùëõ‚àòùëî(ùëà), where:
ùêπ: Rùëõ
+ ‚ÜíR is of the form ùêπ(ùë•) = ‚àëÔ∏Äùëõ
ùëñ=1 ùëì(ùë•ùëñ); ùúåùëõdenotes a function that maps an
ùëõ√ó ùëõSPD matrix ùê¥to its eigenvalues ùúé= (ùúéùëñ) (i.e., ùúåùëõ(ùê¥) = ùúéand since ùêπis a
symmetric function, the order of the eigenvalues is irrelevant); and the mapping ùëî
is given by: ùëî(ùëà) = (ùê∑+ ùêº)‚àí1/2(ùëàùëà‚ä§+ ùêº)(ùê∑+ ùêº)‚àí1/2, for all ùëà‚ààRùëõ√óùëü. Since the
function ùêπ‚àòùúåùëõsatisfies the hypotheses in [165, Theorem 1.1], ùêπ‚àòùúåùëõis differentiable
at the SPD matrix ùëãif and only if ùêπis differentiable at ùúåùëõ(ùëã), in which case
(ùêπ‚àòùúåùëõ)‚Ä≤(ùëã) = ùëçùëÜùúéùëç‚ä§, where
ùëÜùúé= diag[ ùêπ‚Ä≤(ùúåùëõ(ùëã)) ] = diag{ùëì‚Ä≤(ùúé1), . . . , ùëì‚Ä≤(ùúéùëõ)},
and ùëçis an orthogonal matrix such that ùëã= ùëçdiag[ ùúåùëõ(ùëã) ]ùëç‚ä§. Using the chain
rule, we obtain
ùúïùí•(ùëà)
ùúïùëàùëñùëó
= tr
(Ô∏Ç
ùëçùëÜùúéùëç‚ä§ùúïùëî(ùëà)
ùúïùëàùëñùëó
)Ô∏Ç
,
which leads to the following gradient of ùí•at ùëà:
ùí•‚Ä≤(ùëà) = 2(ùê∑+ ùêº)‚àí1/2ùëçùëÜùúé(ùê∑+ ùêº)‚àí1/2ùëç‚ä§ùëà= 2 ùëäùëÜùúéùëä‚ä§ùëà,
where the orthogonal matrix ùëçis such that the matrix ùëä= (ùê∑+ ùêº)‚àí1/2ùëçsatisfies
(ùëàùëà‚ä§+ ùêº)ùëä= (ùê∑+ ùêº)ùëäœíùúé
(C.2)
with œíùúé= diag(ùúé). Now we show that the functional ùí•is coercive. Let (ùëàùëò) be a
sequence of matrices such that ‚Äñùëàùëò‚Äñùêπ‚Üí‚àû. Hence, ùúéùëöùëéùë•(ùëî(ùëàùëò)) ‚Üí‚àûand so does
ùí•since:
ùí•(ùëàùëò) ‚â•ùëì(ùúéùëöùëéùë•(ùëî(ùëàùëò))) + (ùëõ‚àí1)ùëì(1)
and ùëì(ùë•) ‚Üí‚àûas ùë•‚Üí‚àû. Thus, ùí•is a differentiable coercive functional, and has a
204

global minimizer ÃÇÔ∏Äùëàwith zero gradient:
ùí•‚Ä≤(ÃÇÔ∏Äùëà) = 2ùëäùëÜùúéùëä‚ä§ÃÇÔ∏Äùëà= 0.
(C.3)
However, since ùëì‚ààùí∞, ùëì‚Ä≤(ùë•) = 0 iff ùë•= 1. It follows that condition (C.3) is equivalent
to
(ùêº‚àíœíùúé)ùëä‚ä§ÃÇÔ∏Äùëà= 0.
(C.4)
(C.2) and (C.4) give ÃÇÔ∏ÄùëàÃÇÔ∏Äùëà‚ä§‚àíùê∑= ùëä‚àí‚ä§œí‚àí1(œí ‚àíùêº)ùëä‚ä§, and right-multiplication by
ÃÇÔ∏ÄùëàÃÇÔ∏Äùëà‚ä§then yields:
ùê∑(ÃÇÔ∏ÄùëàÃÇÔ∏Äùëà‚ä§) = ( ÃÇÔ∏ÄùëàÃÇÔ∏Äùëà‚ä§)2.
(C.5)
In particular, if ùë¢is an eigenvector of ÃÇÔ∏ÄùëàÃÇÔ∏Äùëà‚ä§with nonzero eigenvalue ùõº, then ùë¢is an
eigenvector of ùê∑, ùê∑ùë¢= ùõºùë¢, and thus ùõº= ùëë2
ùëñ> 0 for some ùëñ. Thus, any solution of
(C.5) is such that:
ÃÇÔ∏ÄùëàÃÇÔ∏Äùëà‚ä§=
ùëüùëò
‚àëÔ∏Å
ùëñ=1
ùëë2
ùëòùëñùëíùëòùëñùëí‚ä§
ùëòùëñ,
(C.6)
for some subsequence (ùëò‚Ñì) of {1, . . . , ùëõ} and rank ùëüùëò‚â§ùëü. Notice that any ÃÇÔ∏Äùëàsatisfying
(C.5) is also a critical point according to (C.4). From (C.6) we also find that ùëî(ÃÇÔ∏Äùëà) is
a diagonal matrix,
ùëî(ÃÇÔ∏Äùëà) = (ùê∑+ ùêº)‚àí1
(Ô∏Éùëüùëò
‚àëÔ∏Å
ùëñ=1
ùëë2
ùëòùëñùëíùëòùëñùëí‚ä§
ùëòùëñ+ ùêº
)Ô∏É
.
The diagonal entries ùúéùëñ, which are the eigenvalues of ùëî(ÃÇÔ∏Äùëà), are given by ùúéùëñ= 1 if
ùëñ= ùëò‚Ñìfor some ‚Ñì‚â§ùëüùëò, or ùúéùëñ= 1/(1 + ùëë2
ùëñ) otherwise. In either case, we have 0 <
ùúéùëñ‚â§1 and the monotonicity of ùëìimplies that ùí•(ÃÇÔ∏Äùëà) is minimized by the subsequence
ùëò1 = 1, . . . , ùëòùëü= ùëü, and by the choice ùëüùëò= ùëü. This proves (C.1). It is clear that if the
first ùëüeigenvalues of ùê∑are distinct, then any minimizer of ùí•satisfies (C.1).
Most of the objective functions we consider have the same structure as the loss
function ùí•. Hence, the importance of Lemma C.1.
The next lemma shows that searching for a negative update of Œìpr is equivalent to
205

looking for a positive update of the prior precision matrix. In particular, the lemma
provides a bijection between the two approximation classes, ‚Ñ≥ùëüand ‚Ñ≥‚àí1
ùëü, defined
by (3.4) and (3.14). In what follows, ùëÜpr is any square root of the prior covariance
matrix such that Œìpr = ùëÜpr ùëÜ‚ä§
pr.
Lemma C.2 (Prior updates). For any negative semidefinite update of Œìpr, ÃÇÔ∏ÄŒìpos =
Œìpr ‚àíùêæùêæ‚ä§with ÃÇÔ∏ÄŒìpos ‚âª0, there is a matrix ùëà(of the same rank as ùêæ) such that
ÃÇÔ∏ÄŒìpos =
(Ô∏Ä
Œì‚àí1
pr + ùëàùëà‚ä§)Ô∏Ä‚àí1 . The converse is also true.
Proof. Let ùëçùê∑ùëç‚ä§= ùëÜ‚àí1
pr ùêæùêæ‚ä§ùëÜ‚àí‚ä§
pr , ùê∑= diag{ùëë2
ùëñ}, be a reduced SVD of ùëÜ‚àí1
pr ùêæùêæ‚ä§ùëÜ‚àí‚ä§
pr .
Since ÃÇÔ∏ÄŒìpos ‚âª0 by assumption, we must have ùëë2
ùëñ< 1 for all ùëñ, and we may thus define
ùëà= ùëÜ‚àí‚ä§
pr ùëçùê∑1/2(ùêº‚àíùê∑)‚àí1/2. By Woodbury‚Äôs identity:
(Ô∏Ä
Œì‚àí1
pr + ùëàùëà‚ä§)Ô∏Ä‚àí1
=
Œìpr ‚àíŒìprùëà
(Ô∏Ä
ùêº+ ùëà‚ä§Œì‚àí1
pr ùëà
)Ô∏Ä‚àí1 ùëà‚ä§Œìpr = Œìpr ‚àíùêæùêæ‚ä§= ÃÇÔ∏ÄŒìpos.
Conversely, given a matrix ùëà, we use again Woodbury‚Äôs identity to write ÃÇÔ∏ÄŒìpos as a
negative semidefinite update of Œìpr: ÃÇÔ∏ÄŒìpos = Œìpr ‚àíùêæùêæ‚ä§‚âª0.
Now we prove our main result on approximations of the posterior covariance ma-
trix.
Proof of Theorem 3.1. Given a loss function ùêø‚àà‚Ñí, our goal is to minimize:
ùêø(Œìpos, ÃÇÔ∏ÄŒìpos) =
‚àëÔ∏Å
ùëñ
ùëì(ùúéùëñ)
(C.7)
over ùêæ‚ààRùëõ√óùëüsubject to the constraint ÃÇÔ∏ÄŒìpos = Œìpr ‚àíùêæùêæ‚ä§‚âª0, where (ùúéùëñ) are
the generalized eigenvalues of the pencil (Œìpos, ÃÇÔ∏ÄŒìpos) and ùëìbelongs to the class ùí∞
defined by Eq. (3.9). We also write ùúéùëñ(Œìpos, ÃÇÔ∏ÄŒìpos) to specify the pencil corresponding
to the eigenvalues. By Lemma C.2, the optimization problem is equivalent to finding
a matrix, ùëà‚ààRùëõ√óùëü, that minimizes (C.7) subject to ÃÇÔ∏ÄŒì‚àí1
pos = Œì‚àí1
pr + ùëàùëà‚ä§. Observe
that (ùúéùëñ) are also the eigenvalues of the pencil (ÃÇÔ∏ÄŒì‚àí1
pos, Œì‚àí1
pos).
Let ùëäùê∑ùëä‚ä§= ùëÜ‚ä§
prùêªùëÜpr with ùê∑= diag{ùõø2
ùëñ}, be an SVD of ùëÜ‚ä§
prùêªùëÜpr. Then, by
206

the invariance properties of the generalized eigenvalues we have:
ùúéùëñ(ÃÇÔ∏ÄŒì‚àí1
pos, Œì‚àí1
pos)
=
ùúéùëñ( ùëä‚ä§ùëÜ‚ä§
pr ÃÇÔ∏ÄŒì‚àí1
pos ùëÜprùëä, ùëä‚ä§ùëÜ‚ä§
pr Œì‚àí1
pos ùëÜprùëä) = ùúéùëñ( ùëçùëç‚ä§+ ùêº, ùê∑+ ùêº),
where ùëç= ùëä‚ä§ùëÜ‚ä§
prùëà. Therefore, our goal reduces to finding a matrix, ùëç‚ààRùëõ√óùëü, that
minimizes (C.7) with (ùúéùëñ) being the generalized eigenvalues of the pencil ( ùëçùëç‚ä§+
ùêº, ùê∑+ ùêº). Applying Lemma C.1 leads to the simple solution: ùëçùëç‚ä§= ‚àëÔ∏Äùëü
ùëñ=1 ùõø2
ùëñùëíùëñùëí‚ä§
ùëñ,
where (ùëíùëñ) are the columns of the identity matrix. In particular, the solution is unique
if the first ùëüeigenvalues of ùëÜ‚ä§
prùêªùëÜpr are distinct. The corresponding approximation
ùëàùëà‚ä§is then
ùëàùëà‚ä§= ùëÜ‚àí‚ä§
pr ùëäùëçùëç‚ä§ùëä‚ä§ùëÜ‚àí1
pr =
ùëü
‚àëÔ∏Å
ùëñ=1
ùõø2
ùëñÃÉÔ∏Äùë§ùëñÃÉÔ∏Äùë§‚ä§
ùëñ,
(C.8)
where ÃÉÔ∏Äùë§ùëñ= ùëÜ‚àí‚ä§
pr ùë§ùëñand ùë§ùëñis the ùëñth column of ùëä. Woodbury‚Äôs identity gives the
corresponding negative update of Œìpr as:
ÃÇÔ∏ÄŒìpos = Œìpr ‚àíùêæùêæ‚ä§,
ùêæùêæ‚ä§=
ùëü
‚àëÔ∏Å
ùëñ=1
ùõø2
ùëñ
(Ô∏Ä
1 + ùõø2
ùëñ
)Ô∏Ä‚àí1 ÃÇÔ∏Äùë§ùëñÃÇÔ∏Äùë§ùëñ
‚ä§
(C.9)
with ÃÇÔ∏Äùë§ùëñ= ùëÜprùë§ùëñ. Now, it suffices to note that the couples (ùõø2
ùëñ, ÃÇÔ∏Äùë§ùëñ) defined here are
precisely the generalized eigenpairs of the pencil (ùêª, Œì‚àí1
pr ).
At optimality, ùúéùëñ= 1 for
ùëñ‚â§ùëüand ùúéùëñ= (1 + ùõø2
ùëñ)‚àí1 for ùëñ> ùëü, proving (3.12). ‚ñ°
Before proving Lemma 3.1, we recall that the Kullback-Leibler (K-L) divergence
and the Hellinger distance between two multivariate Gaussians, ùúà1 = ùí©(ùúá, Œ£1) and
ùúà2 = ùí©(ùúá, Œ£2), with the same mean and full rank covariance matrices are given,
respectively, by [209]:
ùê∑KL (ùúà1‚Äñ ùúà2) = 1
2
[Ô∏Ç
trace
(Ô∏Ä
Œ£‚àí1
2 Œ£1
)Ô∏Ä
‚àírank(Œ£1) ‚àíln
(Ô∏Çdet(Œ£1)
det(Œ£2)
)Ô∏Ç]Ô∏Ç
(C.10)
ùëëHell (ùúà1, ùúà2) =
‚àöÔ∏É
1 ‚àí|Œ£1|1/4 |Œ£2|1/4
|1
2Œ£1 + 1
2Œ£2|1/2.
(C.11)
Proof of Lemma 3.1. By (C.10), the K-L divergence between the posterior ùúàpos(ùëå)
207

and the Gaussian approximation ÃÇÔ∏Äùúàpos(ùëå) can be written in terms of the generalized
eigenvalues of the pencil (Œìpos, ÃÇÔ∏ÄŒìpos) as:
ùê∑KL (ùúàpos(ùëå)‚Äñ ÃÇÔ∏Äùúàpos(ùëå)) =
‚àëÔ∏Å
ùëñ
( ùúéùëñ‚àíln ùúéùëñ‚àí1 ) /2,
and since ùëì(ùë•) = (ùë•‚àíln ùë•‚àí1) /2 belongs to ùí∞, we see that the K-L divergence is
a loss function in the class ‚Ñídefined by (3.8). Hence, Theorem 3.1 applies and the
equivalence between the two approximations follows trivially. An analogous argument
holds for the Hellinger distance. The squared Hellinger distance between ùúàpos(ùëå) and
ÃÇÔ∏Äùúàpos(ùëå) can be written in terms of the generalized eigenvalues, (ùúéùëñ), of the pencil
(Œìpos, ÃÇÔ∏ÄŒìpos), as:
ùëëHell (ùúàpos(ùëå), ÃÇÔ∏Äùúàpos(ùëå))2 = 1 ‚àí2‚Ñì/2 ‚àèÔ∏Å
ùëñ
ùúé1/4
ùëñ
(1 + ùúéùëñ)‚àí1/2 .
(C.12)
where ‚Ñìis the dimension of the parameter space. Minimizing (C.12) is equivalent
to maximizing ‚àèÔ∏Ä
ùëñùúé1/4
ùëñ
(1 + ùúéùëñ)‚àí1/2, which in turn is equivalent to minimizing the
functional:
ùêø(Œìpos, ÃÇÔ∏ÄŒìpos) = ‚àí
‚àëÔ∏Å
ùëñ
ln( ùúé1/4
ùëñ
(1 + ùúéùëñ)‚àí1/2 ) =
‚àëÔ∏Å
ùëñ
ln( 2 + ùúéùëñ+ 1/ùúéùëñ)/4.
(C.13)
Since ùëì(ùë•) = ln( 2+ùë•+1/ùë•)/4 belongs to ùí∞, Theorem 3.1 can be applied once again.
‚ñ°
Proof of Corollary 3.1. The proofs of parts (i) and (ii) were already given in the
proof of Theorem 3.1. Part (iii) holds because,
(1 + ùõø2
ùëñ)Œìpos ÃÉÔ∏Äùë§ùëñ
=
(1 + ùõø2
ùëñ)(ùêª+ Œì‚àí1
pr )‚àí1ùëÜ‚àí‚ä§
pr ùë§ùëñ
=
(1 + ùõø2
ùëñ) ùëÜpr(ùëÜ‚ä§
prùêªùëÜpr + ùêº)‚àí1ùë§ùëñ= ùëÜpr ùë§ùëñ= Œìpr ÃÉÔ∏Äùë§ùëñ,
because ùë§ùëñis an eigenvector of ( ùëÜ‚ä§
prùêªùëÜpr + ùêº)‚àí1 with eigenvalue (1 + ùõø2
ùëñ)‚àí1 as shown
in the proof of Theorem 3.1. ‚ñ°
208

Now we turn to optimality results for approximations of the posterior mean. In
what follows, let ùëÜpr, ùëÜobs, ùëÜpos, and ùëÜùëåbe the matrix square roots of, respectively,
Œìpr, Œìobs, Œìpos, and Œìùëå:= Œìobs + ùê∫Œìpr ùê∫‚ä§such that Œì = ùëÜùëÜ‚ä§(i.e., possibly non-
symmetric square roots).
Equation (3.27) shows that, to minimize E( ‚Äñùê¥ùëå‚àíùëã‚Äñ2
Œì‚àí1
pos ) over ùê¥‚ààùíú, we
need only to minimize E( ‚Äñ ùê¥ùëå‚àíùúápos(ùëå) ‚Äñ2
Œì‚àí1
pos ).
Furthermore, since ùúápos(ùëå) =
Œìpos ùê∫‚ä§Œì‚àí1
obs ùëå, it follows that
E( ‚Äñ ùê¥ùëå‚àíùúápos(ùëå) ‚Äñ2
Œì‚àí1
pos ) = ‚Äñ ùëÜ‚àí1
pos (ùê¥‚àíŒìpos ùê∫‚ä§Œì‚àí1
obs) ùëÜùëå‚Äñ2
ùêπ,
(C.14)
We are therefore led to the following optimization problem:
min
ùê¥‚ààùíú‚Äñ ùëÜ‚àí1
posùê¥ùëÜùëå‚àíùëÜ‚ä§
posùê∫‚ä§Œì‚àí1
obs ùëÜùëå‚Äñùêπ.
(C.15)
The following result shows that an SVD of the matrix ùëÜÃÇÔ∏Ä
ùêª:= ùëÜ‚ä§
pr ùê∫‚ä§ùëÜ‚àí‚ä§
obs can be used
to obtain simple expressions for the square roots of Œìpos and Œìùëå.
Lemma C.3 (Square roots). Let ùëäùê∑ùëâ‚ä§be an SVD of ùëÜÃÇÔ∏Ä
ùêª= ùëÜ‚ä§
pr ùê∫‚ä§ùëÜ‚àí‚ä§
obs . Then:
ùëÜpos
=
ùëÜpr ùëä( ùêº+ ùê∑ùê∑‚ä§)‚àí1/2 ùëä‚ä§
(C.16)
ùëÜùëå
=
ùëÜobs ùëâ( ùêº+ ùê∑‚ä§ùê∑)1/2 ùëâ‚ä§
(C.17)
are square roots of Œìpos and Œìùëå.
Proof. We can rewrite Œìpos = ( ùê∫‚ä§Œì‚àí1
obsùê∫+ Œì‚àí1
pr )‚àí1 as
Œìpos
=
ùëÜpr ( ùëÜÃÇÔ∏Ä
ùêªùëÜ‚ä§
ÃÇÔ∏Ä
ùêª+ ùêº)‚àí1 ùëÜ‚ä§
pr = ùëÜpr ùëä( ùê∑ùê∑‚ä§+ ùêº)‚àí1ùëä‚ä§ùëÜ‚ä§
pr
=
[ ùëÜpr ùëä( ùê∑ùê∑‚ä§+ ùêº)‚àí1/2 ùëä‚ä§] [ ùëÜpr ùëä( ùê∑ùê∑‚ä§+ ùêº)‚àí1/2 ùëä‚ä§]‚ä§,
which proves (C.16). The proof of (C.17) follows similarly using: ùëÜ‚ä§
ÃÇÔ∏Ä
ùêªùëÜÃÇÔ∏Ä
ùêª= ùëÜ‚àí1
obsùê∫Œìpr ùê∫‚ä§Œì‚àí‚ä§
obs.
In the next two proofs we use (ùê∂)ùëüto denote a rank ùëüapproximation of the matrix
209

ùê∂in the Frobenius norm.
Proof of Theorem 4.2. By [102, Theorem 2.1], an optimal ùê¥‚ààùíúùëüis given by:
ùê¥= ùëÜpos
(Ô∏Ä
ùëÜ‚ä§
posùê∫‚ä§Œì‚àí1
obs ùëÜùëå
)Ô∏Ä
ùëüùëÜ‚àí1
ùëå.
(C.18)
Now, we need some computations to show that (C.18) is equivalent to (3.30). Using
(C.16) and (C.17) we find ùëÜ‚ä§
posùê∫‚ä§Œì‚àí1
obs ùëÜùëå= ùëä(ùêº+ùê∑ùê∑‚ä§)‚àí1/2ùê∑(ùêº+ùê∑‚ä§ùê∑)1/2 ùëâ‚ä§, and
therefore ( ùëÜ‚ä§
posùê∫‚ä§Œì‚àí1
obs ùëÜùëå)ùëü= ‚àëÔ∏Äùëü
ùëñ=1 ùõøùëñùë§ùëñùë£‚ä§
ùëñ, where ùë§ùëñis the ùëñth column of ùëä, ùë£ùëñis the
ùëñth column of ùëâ, and ùõøùëñis the ùëñth diagonal entry of ùê∑. Inserting this back into (C.18)
yields ùê¥= ‚àëÔ∏Ä
ùëñ‚â§ùëüùõøùëñ(1 + ùõø2
ùëñ)‚àí1ùëÜprùë§ùëñùë£‚ä§
ùëñùëÜ‚àí1
obs. Now it suffices to note that ÃÇÔ∏Äùë§ùëñ:= ùëÜprùë§ùëñis
a generalized eigenvector of (ùêª, Œì‚àí1
pr ), that ÃÇÔ∏Äùë£ùëñ:= ùëÜ‚àí‚ä§
obs ùë£ùëñis a generalized eigenvector of
(ùê∫Œìprùê∫‚ä§, Œìobs), and that (ùõø2
ùëñ) are also eigenvalues of (ùêª, Œì‚àí1
pr ).
The minimum Bayes
risk is a straightforward computation for the optimal estimator (3.30) using (C.14).
‚ñ°
Proof of Theorem 3.3. Given ùê¥‚ààÃÇÔ∏Ä
ùíúùëü, we can restate (C.15) as the problem of
finding a matrix ùêµ, of rank at most ùëü, that minimizes:
‚Äñ ùëÜ‚àí1
pos(Œìpr ‚àíŒìpos) ùê∫‚ä§Œì‚àí1
obs ùëÜùëå‚àíùëÜ‚àí1
posùêµ
(Ô∏Ä
ùê∫‚ä§Œì‚àí1
obs ùëÜùëå
)Ô∏Ä
‚Äñùêπ
(C.19)
such that ùê¥= (Œìpr ‚àíùêµ) ùê∫‚ä§Œì‚àí1
obs. By [102, Theorem 2.1], an optimal ùêµis given by:
ùêµ= ùëÜpos( ùëÜ‚àí1
pos (Œìpr ‚àíŒìpos) ùê∫‚ä§Œì‚àí1
obs ùëÜùëå)ùëü(ùê∫‚ä§Œì‚àí1
obs ùëÜùëå)‚Ä†
(C.20)
where ‚Ä† denotes the pseudo-inverse operator. A closer look at [102, Theorem 2.1]
reveals that another minimizer of (C.19), itself not necessarily of minimum Frobenius
norm, is given by:
ùêµ= ùëÜpos( ùëÜ‚àí1
pos (Œìpr ‚àíŒìpos) ùê∫‚ä§Œì‚àí1
obs ùëÜùëå)ùëü(ùëÜ‚ä§
prùê∫‚ä§Œì‚àí1
obs ùëÜùëå)‚Ä†ùëÜ‚ä§
pr.
(C.21)
210

By Lemma C.3,
ùëÜ‚ä§
prùê∫‚ä§Œì‚àí1
obs ùëÜùëå
=
ùëä[ ùê∑
(Ô∏Ä
ùêº+ ùê∑‚ä§ùê∑
)Ô∏Ä1/2 ]ùëâ‚ä§
ùëÜ‚àí1
pos Œìprùê∫‚ä§Œì‚àí1
obsùëÜùëå
=
ùëä[ (ùêº+ ùê∑ùê∑‚ä§)1/2ùê∑(ùêº+ ùê∑‚ä§ùê∑)1/2 ]ùëâ‚ä§
ùëÜ‚àí1
pos Œìposùê∫‚ä§Œì‚àí1
obsùëÜùëå
=
ùëä[ (ùêº+ ùê∑ùê∑‚ä§)‚àí1/2ùê∑(ùêº+ ùê∑‚ä§ùê∑)1/2 ]ùëâ‚ä§
and therefore (ùëÜ‚ä§
prùê∫‚ä§Œì‚àí1
obs ùëÜùëå)‚Ä† = ‚àëÔ∏Äùëû
ùëñ=1 ùõø‚àí1
ùëñ
(1 + ùõø2
ùëñ)‚àí1/2 ùë£ùëñùë§‚ä§
ùëñfor ùëû= rank(ùëÜÃÇÔ∏Ä
ùêª), whereas
( ùëÜ‚àí1
pos (Œìpr ‚àíŒìpos)ùê∫‚ä§Œì‚àí1
obsùëÜùëå)ùëü=
ùëü
‚àëÔ∏Å
ùëñ=1
ùõø3
ùëñùë§ùëñùë£‚ä§
ùëñ.
Inserting these expressions back into (C.21), we obtain:
ùêµ= ùëÜpr
(Ô∏É
ùëü
‚àëÔ∏Å
ùëñ=1
ùõø2
ùëñ
1 + ùõø2
ùëñ
ùë§ùëñùë§‚ä§
ùëñ
)Ô∏É
ùëÜ‚ä§
pr,
where ùë§ùëñis the ùëñth column of ùëä, ùë£ùëñis the ùëñth column of ùëâ, and ùõøùëñis the ùëñth diagonal
entry of ùê∑. Notice that (ùõø2
ùëñ, ÃÇÔ∏Äùë§ùëñ), with ÃÇÔ∏Äùë§ùëñ= ùëÜprùë§ùëñ, are the generalized eigenpairs
of (ùêª, Œì‚àí1
pr ) (cf. proof of Theorem 3.1). Hence, by Theorem 3.1, we recognize the
optimal approximation of Œìpos as ÃÇÔ∏ÄŒìpos = Œìpr ‚àíùêµ. Plugging this expression back into
(C.21) gives (3.33). The minimum Bayes risk in (ii) follows readily using the optimal
estimator given by (3.33) in (C.14). ‚ñ°
211

212

Appendix D
Proofs for Chapter 4
The following two Lemmas, D.1 and 4.1, will be used to prove Theorems 4.1 and 4.2.
We start with a result describing the relationships between different eigenpairs of the
Schur complements of a particular class of covariance matrices that arises in Bayesian
inverse problems.
Lemma D.1 (Eigenpairs of Schur complements). Let Œ£ ‚âª0 be a matrix partitioned
as
Œ£ =
‚éõ
‚éùùê¥
ùêµ
ùêµ‚ä§
ùê∂
‚éû
‚é†,
(D.1)
where ùê¥and ùê∂are square matrices and ùêµÃ∏= 0. Then, ùê¥,ùê∂, and the Schur comple-
ments, ùíÆ(ùê¥) := ùê∂‚àíùêµ‚ä§ùê¥‚àí1ùêµand ùíÆ(ùê∂) := ùê¥‚àíùêµùê∂‚àí1ùêµ‚ä§, are also SPD matrices.
Moreover:
1. If (ùõΩ, ùë§) is an eigenpair of (ùêµùê∂‚àí1ùêµ‚ä§, ùê¥), then ùõΩ< 1 and (1 ‚àíùõΩ, ùë§) is an
eigenpair of (ùíÆ(ùê∂), ùê¥). Furthermore, if ùõΩÃ∏= 0, then ((1 ‚àíùõΩ)‚àí1, ùêµ‚ä§ùë§) is an
eigenpair of (ùíÆ(ùê¥)‚àí1, ùê∂‚àí1).
2. If ùõΩÃ∏= 0 and (ùõΩ, ùë§) is an eigenpair of (ùêµùê∂‚àí1ùêµ‚ä§, ùê¥), then (ùõΩ(1 ‚àíùõΩ)‚àí1, ùêµ‚ä§ùë§) is
an eigenpair of (ùê∂‚àí1ùêµ‚ä§ùíÆ(ùê∂)‚àí1ùêµùê∂‚àí1, ùê∂‚àí1).
3. If ùë§1, . . . , ùë§ùëòare linearly independent eigenvectors of (ùêµùê∂‚àí1ùêµ‚ä§, ùê¥) with asso-
ciated eigenvalues ùõΩ1 ‚â•ùõΩ2 ‚â•¬∑ ¬∑ ¬∑ ‚â•ùõΩùëò> 0, then ùêµ‚ä§ùë§1, . . . , ùêµ‚ä§ùë§ùëòare linearly
213

independent. Moreover, if ùëò= rank(ùêµùê∂‚àí1ùêµ‚ä§), then there can be at most ùëò
linearly independent eigenvectors of (ùê∂‚àí1ùêµ‚ä§ùíÆ(ùê∂)‚àí1ùêµùê∂‚àí1, ùê∂‚àí1) associated with
strictly positive eigenvalues.
Proof. The fact that ùê¥, ùê∂, ùíÆ(ùê¥) and ùíÆ(ùê∂) are SPD matrices follows from [37]. (1)
From ùêµùê∂‚àí1ùêµ‚ä§ùë§= ùõΩùê¥ùë§we obtain ùíÆ(ùê∂)ùë§= (1 ‚àíùõΩ)ùê¥ùë§, which also implies that
ùõΩ< 1 as ùíÆ(ùê∂) ‚âª0 and ùê¥‚âª0. If ùõΩÃ∏= 0, then ùêµ‚ä§ùë§Ã∏= 0 and
ùíÆ(ùê¥)‚àí1ùêµ‚ä§ùë§
=
[ ùê∂‚àí1 + ùê∂‚àí1ùêµ‚ä§ùíÆ(ùê∂)‚àí1ùêµùê∂‚àí1 ] ùêµ‚ä§ùë§
=
[ ùê∂‚àí1ùêµ‚ä§+ ùê∂‚àí1ùêµ‚ä§ùíÆ(ùê∂)‚àí1(ùê¥‚àíùíÆ(ùê∂)) ]ùë§
=
(1 ‚àíùõΩ)‚àí1 ùê∂‚àí1ùêµ‚ä§ùë§.
where we used the Woodbury identity to rewrite ùíÆ(ùê¥)‚àí1. (2) It follows from (1)
that ((1 ‚àíùõΩ)‚àí1, ùêµ‚ä§ùë§) is an eigenpair of (ùíÆ(ùê¥)‚àí1, ùê∂‚àí1) and by ùíÆ(ùê¥)‚àí1 = ùê∂‚àí1 +
ùê∂‚àí1ùêµ‚ä§ùíÆ(ùê∂)‚àí1ùêµùê∂‚àí1 that (ùõΩ(1‚àíùõΩ)‚àí1, ùêµ‚ä§ùë§) is an eigenpair of (ùê∂‚àí1ùêµ‚ä§ùíÆ(ùê∂)‚àí1ùêµùê∂‚àí1, ùê∂‚àí1).
(3) If ‚àëÔ∏Äùëò
ùëó=1 ùëéùëóùêµ‚ä§ùë§ùëó= 0, then ùê¥‚àëÔ∏Äùëò
ùëó=1 ùõΩùëóùëéùëóùë§ùëó= 0, and therefore ‚àëÔ∏Äùëò
ùëó=1 ùõΩùëóùëéùëóùë§ùëó= 0
since ùê¥‚âª0, which leads to ùõΩùëóùëéùëó= 0 for ùëó= 1, . . . . , ùëòsince (ùë§ùëó) are linearly in-
dependent, and thus ùëéùëó= 0 for ùëó= 1, . . . . , ùëòsince ùõΩùëó> 0. Moreover, notice that
rank(ùê∂‚àí1ùêµ‚ä§ùíÆ(ùê∂)‚àí1ùêµùê∂‚àí1) = rank(ùêµ‚ä§ùíÆ(ùê∂)‚àí1ùêµ) = rank(ùêµùê∂‚àí1ùêµ‚ä§). Thus, there can
be at most rank(ùêµùê∂‚àí1ùêµ‚ä§) linearly independent eigenvectors of (ùê∂‚àí1ùêµ‚ä§ùíÆ(ùê∂)‚àí1ùêµùê∂‚àí1, ùê∂‚àí1)
with nonzero eigenvalues.
Proof of Lemma 4.1. Consider the identity ùëå= ùê∫ùëã+‚Ñ∞= ùê∫ùí™‚Ä† ùí™ùëã+ùê∫(ùêº‚àí
ùí™‚Ä† ùí™) ùëã+ ‚Ñ∞= ùê∫ùí™‚Ä† ùëç+ Œî, where ùí™‚Ä† := Œìprùí™‚ä§Œì‚àí1
ùëçand Œî := ùê∫(ùêº‚àíùí™‚Ä† ùí™) ùëã+ ‚Ñ∞.
A simple computation shows that E[(ùêº‚àíùí™‚Ä† ùí™)ùëãùëç‚ä§] = 0. Hence, (ùêº‚àíùí™‚Ä† ùí™)ùëãand
ùëçare uncorrelated, and, more importantly, independent since they are also jointly
Gaussian. It follows that Œî and ùëçare also independent since ‚Ñ∞was independent
of ùëãand ùëç= ùí™ùëã. In the hypothesis of zero prior mean, the mean of Œî is also
zero. Moreover, ŒìŒî = Var[ùê∫(ùêº‚àíùí™‚Ä† ùí™)ùëã] + Var[‚Ñ∞] since ùëãand ‚Ñ∞are independent.
Simple algebra leads to the particular form of ŒìŒî.
‚ñ°
214

We can now prove the main results of this chapter.
Proof of Theorem 4.1. By applying [261, Theorem 2.3] to the linear Gaussian
model defined in Lemma 4.1, we know that a minimizer, ÃÇÔ∏ÄŒìùëç|ùëå, of the geodesic dis-
tance, ùëë‚Ñõ, between Œìùëç|ùëåand an element of ‚Ñ≥ùëç
ùëüis given by: ÃÇÔ∏ÄŒìùëç|ùëå= Œìùëç‚àí‚àëÔ∏Äùëü
ùëñ=1 ùúÇ2
ùëñ(1+
ùúÇ2
ùëñ)‚àí1 ÃÇÔ∏ÄùëûùëñÃÇÔ∏Äùëû‚ä§
ùëñ, where (ùúÇ2
ùëñ, ÃÇÔ∏Äùëûùëñ) are the eigenpairs of (ùêªùëç, Œì‚àí1
ùëç), with the ordering ùúÇ2
ùëñ‚â•ùúÇ2
ùëñ+1,
the normalization ÃÇÔ∏Äùëû‚ä§
ùëñŒì‚àí1
ùëçÃÇÔ∏Äùëûùëñ= 1 and where ùêªùëç:= ùí™‚ä§
‚Ä† ùê∫‚ä§Œì‚àí1
Œî ùê∫ùí™‚Ä† is the Hessian of
the negative log‚Äìlikelihood ùëå|ùëç‚àºùí©(ùê∫ùí™‚Ä†, ŒìŒî). Moreover, [261, Theorem 2.3] im-
plies that the distance, at optimality, is given by ùëë2
‚Ñõ(ÃÇÔ∏ÄŒìùëç|ùëå, Œìùëç|ùëå) = ‚àëÔ∏Ä
ùëñ>ùëüln2( 1 + ùúÇ2
ùëñ)
and that the minimizer is unique if the first ùëüeigenvalues of (ùêªùëç, Œì‚àí1
ùëç) are distinct.
Now let (ùúÜùëñ, ùëûùëñ) be defined as in Theorem 4.1, with ùúÜùëñ> 0, and let Œ£ ‚âª0 be the
covariance matrix of the joint distribution of ùëåand ùëç, i.e.,
Œ£ =
‚éõ
‚éù
Œìùëå
ùê∫Œìpr ùí™‚ä§
ùí™Œìpr ùê∫‚ä§
Œìùëç
‚éû
‚é†.
(D.2)
By Lemma D.1[part 2] applied to (D.2), we know that ( ùúÜùëñ(1 ‚àíùúÜùëñ)‚àí1 , ùí™Œìpr ùê∫‚ä§ùëûùëñ)
are eigenpairs of (ùêªùëç, Œì‚àí1
ùëç).
Moreover, by Lemma D.1[part 3] we know that we
can always write a maximal set of linearly independent eigenvectors of (ùêªùëç, Œì‚àí1
ùëç),
associated with nonzero eigenvalues, as (ùí™Œìpr ùê∫‚ä§ùëûùëñ). Thus, since ùëì(ùúÜ) = ùúÜ(1 ‚àíùúÜ)‚àí1
is a decreasing function of ùúÜas ùúÜ‚Üì0, we must have ùúÇ2
ùëñ= ùúÜùëñ(1 ‚àíùúÜùëñ)‚àí1 and we can
assume, without loss of generality, that ÃÇÔ∏Äùëûùëñ= ùõºùí™Œìpr ùê∫‚ä§ùëûùëñfor some real ùõº> 0. Given
the normalizations ÃÇÔ∏Äùëû‚ä§Œì‚àí1
ùëçÃÇÔ∏Äùëû= 1 and ùëû‚ä§
ùëñ( ùê∫Œìpr ùí™‚ä§Œì‚àí1
ùëçùí™Œìpr ùê∫‚ä§)ùëûùëñ= 1, it follows that
ùõº= 1.
Simple algebra then leads to (4.9) and (4.10).
Notice, that ùúÜùëñ> 0 and
ùëì(ùúÜùëñ) > 0 imply ùúÜùëñ< 1. This property will be useful when proving Lemma 4.5.
‚ñ°
We now state a standard result that will be used in proving Lemma 4.2.
Theorem D.1 (Cauchy Interlacing Theorem (e.g., [157, 25])). Let ùê¥, ùêµ‚ààRùëõ√óùëõbe
symmetric matrices with ùêµ‚âª0 and let ùõæ1 ‚â•ùõæ2 ‚â•¬∑ ¬∑ ¬∑ ‚â•ùõæùëõbe the eigenvalues of
(ùê¥, ùêµ). For any ùëÉ‚ààRùëõ√óùëù, with ùëù‚â§ùëõand full column-rank, let ùúá1 ‚â•ùúá2 ‚â•¬∑ ¬∑ ¬∑ ‚â•ùúáùëù
215

be the eigenvalues of (ùëÉ‚ä§ùê¥ùëÉ, ùëÉ‚ä§ùêµùëÉ). Then:
ùõæùëò‚â•ùúáùëò‚â•ùõæùëõ‚àíùëù+ùëò,
ùëò= 1, . . . , ùëù.
(D.3)
Proof of Lemma 4.2. The first inequality in (4.12) follows from the optimality
statement of Theorem 4.1 since ÃÇÔ∏ÄŒìùëç|ùëå‚àà‚Ñ≥ùëç
ùëü. The second inequality in (4.12) follows
from the Cauchy interlacing theorem (see Theorem D.1). Let ùõæ1 ‚â•ùõæ2 ‚â•¬∑ ¬∑ ¬∑ ‚â•ùõæùëõ‚â•1
be the eigenvalues of ( ÃÇÔ∏ÄŒìpos , Œìpos ) and ùúá1 ‚â•ùúá2 ‚â•¬∑ ¬∑ ¬∑ ‚â•ùúáùëùbe the eigenvalues of
( ÃÇÔ∏ÄŒìùëç|ùëå, Œìùëç|ùëå) = ( ùí™ÃÇÔ∏ÄŒìpos ùí™‚ä§, ùí™Œìpos ùí™‚ä§), where ùí™is a full row-rank matrix. Then,
by Theorem D.1,
ùõæùëò‚â•ùúáùëò‚â•1,
ùëò= 1, . . . , ùëù.
(D.4)
In particular, since ln2(ùë•) is monotone increasing on ùë•> 1, we have:
ùëë‚Ñõ( Œìùëç|ùëå, ÃÇÔ∏ÄŒìùëç|ùëå) = 1
2
‚àëÔ∏Å
ùëò
ln2(ùúáùëò) ‚â§1
2
‚àëÔ∏Å
ùëò
ln2(ùõæùëò) ‚â§ùëë‚Ñõ( Œìpos , ÃÇÔ∏ÄŒìpos ),
(D.5)
where clearly ‚àëÔ∏Ä
ùëò>ùëùln2(ùõæùëò) ‚â•0.
‚ñ°
The following two lemmas will be used in proving Lemma 4.3.
Lemma D.2. If Œì1 ‚™∞Œì2 ‚âª0, then |Œì1| ‚â•|Œì2|
Proof. If Œì1 ‚™∞Œì2, then there exists a ùëÜ‚™∞0 such that Œì1 = Œì2+ùëÜ. Thus, |Œì1| |Œì2|‚àí1 =
|ùêº+ Œì‚àí1/2
2
ùëÜŒì‚àí1/2
2
| ‚â•1.
Lemma D.3. Let ùëã‚àºùí©(ùúá, Œ£) and ùëå‚àºùí©(0, Œì) with Œì ‚™∞Œ£ ‚âª0. Let ùëîbe a
measurable real-valued function such that
E[|ùëî|2+ùõº(ùëå)] < ‚àû
(D.6)
for some ùõº> 0. Then,
E[ùëî2(ùëã)] ‚â§|Œì|1/2
|Œ£|1/2 exp
(Ô∏Çùúá‚ä§Œì‚àí1ùúá
ùõº
)Ô∏Ç
E[|ùëî|2+ùõº(ùëå)]1/(1+ùõº/2)
(D.7)
216

Proof. Let ùëìùëãand ùëìùëåbe the densities of ùëãand ùëå, respectively, and ùëÄùëåthe moment
generating function of ùëå. Since we have Œ£‚àí1 = Œì‚àí1 + ùëÜfor some ùëÜ‚™∞0, it follows
that for all ùë•,
ùëìùëã(ùë•) ‚â§ùêæexp(ùúá‚ä§Œì‚àí1 ùë•) ùëìùëå(ùë•),
(D.8)
where ùêæ:= |Œì|1/2 |Œ£|‚àí1/2 exp(‚àíùúá‚ä§Œì‚àí1 ùúá/2). Now we use H√∂lder‚Äôs inequality, with
ùëù= 1 + ùõº/2 and ùëû= ùëù/(ùëù‚àí1) so that 1/ùëù+ 1/ùëû= 1, to obtain:
E[ùëî2(ùëã)]
‚â§
ùêæE[ùëî2(ùëå) exp(ùúá‚ä§Œì‚àí1 ùëå)]
‚â§
ùêæ(E[|ùëî|2ùëù(ùëå)])1/ùëù(E[exp(ùëûùúá‚ä§Œì‚àí1 ùëå)])1/ùëû
=
ùêæ(E[|ùëî|2ùëù(ùëå)])1/ùëùùëÄ1/ùëû
ùëå(ùëûŒì‚àí1ùúá)
=
ùêæ(E[|ùëî|2ùëù(ùëå)])1/ùëùexp(ùëûùúá‚ä§Œì‚àí1ùúá/2)
=
|Œì|1/2 |Œ£|‚àí1/2 (E[|ùëî|2ùëù(ùëå)])1/ùëùexp((ùëû‚àí1) ùúá‚ä§Œì‚àí1ùúá/2)
=
|Œì|1/2 |Œ£|‚àí1/2 exp(ùúá‚ä§Œì‚àí1ùúá/ùõº)(E[|ùëî|2+ùõº(ùëå)])1/(1+ùõº/2),
where we used the fact that ùëÄùëå(ùë°) = exp(ùë°‚ä§Œì ùë°/2) since ùëå‚àºùí©(0, Œì).
Proof of Lemma 4.3. By [76, Lemma 7.14] we have:
‚Éí‚Éí‚ÉíEùúàùëç|ùëå[ùëî] ‚àíEÀúùúàùëç|ùëå[ùëî]
‚Éí‚Éí‚Éí‚â§2
‚àöÔ∏É‚à´Ô∏Å
|ùëî|2 (ùúãùëç|ùëå+ Àúùúãùëç|ùëå) ùëëHell(ùúàùëç|ùëå, Àúùúàùëç|ùëå)
(D.9)
where ùúãùëç|ùëåand Àúùúãùëç|ùëåare, respectively, the densitites of ùúàùëç|ùëåand Àúùúàùëç|ùëåwith respect
to the Lebesgue measure. Now notice that Œìùëç|ùëå‚™ØŒìùëças well as ÃÉÔ∏ÄŒìùëç|ùëå‚™ØŒìùëç. Thus,
by Lemma D.3, we have:
Eùúàùëç|ùëå[|ùëî|2] + EÀúùúàùëç|ùëå[|ùëî|2] ‚â§2 |Œìùëç|1/2
|Œìùëç|ùëå|1/2 exp
(Ô∏Ç
1
ùõΩ‚àí2 ‚Äñùúáùëç|ùëå(ùëå)‚Äñ2
Œì‚àí1
ùëç
)Ô∏Ç
Eùúàùëç[|ùëî|ùõΩ]2/ùõΩ
(D.10)
where we used the fact that |ÃÉÔ∏ÄŒìùëç|ùëå| ‚â•|Œìùëç|ùëå| since ÃÉÔ∏ÄŒìùëç|ùëå‚™∞Œìùëç|ùëå(see Lemma D.2).
Thus, (4.14) follows from simple algebra.
‚ñ°
Lemma D.4. Let ùëÄ:= ùê¥(ùêº‚àíùêµùêµ‚ä§)ùê¥‚ä§‚âª0 for a pair of compatible matrices ùê¥, ùêµ,
217

and let ùëÉbe the orthogonal projector onto the range of ùê¥‚ä§. Then ùê∂:= ùêº‚àíùëÉùêµùêµ‚ä§ùëÉ‚âª
0 and ùëÄ= ùê¥ùê∂ùê¥‚ä§.
Proof. Since ùëÄ‚âª0, ùê¥‚ä§must be full column rank.
Thus, by definition, ùëÉ=
ùê¥‚ä§(ùê¥ùê¥‚ä§)‚àí1ùê¥and ùëÉùê¥‚ä§= ùêº= ùê¥ùëÉ. Hence ùëÄ= ùê¥ùê∂ùê¥‚ä§. Now let ùëÑ:= ùêº‚àíùëÉand
notice that ùëÉùëÑ= 0 and ùê∂ùëÑ= ùëÑ. Thus, for ùëßÃ∏= 0, ‚ü®ùê∂ùëß, ùëß‚ü©= ‚ü®ùê∂ùëÉùëß, ùëÉùëß‚ü©+‚ü®ùëÑùëß, ùëÑùëß‚ü©=
‚ü®ùê∂ùëÉùëß, ùëÉùëß‚ü©+‚ÄñùëÑùëß‚Äñ2. In particular, ‚ü®ùê∂ùëÉùëß, ùëÉùëß‚ü©= ‚ü®ùëÉùê∂ùëÉùëß, ùëß‚ü©= ‚ü®ùëÄ(ùê¥ùê¥‚ä§)‚àí1ùê¥ùëß, (ùê¥ùê¥‚ä§)‚àí1ùê¥ùëß‚ü©‚â•
0 and it is zero only if ùëÉùëß= 0, in which case ùëÑùëßÃ∏= 0 and ‚ÄñùëÑùëß‚Äñ > 0. Thus ùê∂‚âª0.
Proof of Lemma 4.4. We first need to show the equivalence ÃÉÔ∏ÄŒìùëç|ùëå‚â°ùí™ÃÇÔ∏ÄŒì*
pos ùí™‚ä§,
where ÃÇÔ∏ÄŒì*
pos is defined in (4.15).
Notice that ùí™ÃÇÔ∏ÄŒì*
pos ùí™‚ä§= Œìùëç‚àí‚àëÔ∏Ä
ùëñùúÜùëñùë£ùëñùë£‚ä§
ùëñ
with
ùë£ùëñ:= ùí™ùëÜpr Œ† ùëÜ‚ä§
pr ùê∫‚ä§ùëûùëñ= ùí™Œìpr ùê∫‚ä§ùëûùëñsince Œ† is a projector onto the rowspace of ùí™ùëÜpr.
The desired equivalence follows by comparison with (4.9). In particular, it follows
that ùí™ÃÇÔ∏ÄŒì*
pos ùí™‚ä§‚âª0. Thus, in order to show that ÃÇÔ∏ÄŒì*
pos is in the feasible set of (4.16) it
remains to prove that ÃÇÔ∏ÄŒì*
pos ‚àà‚Ñ≥ùëü. Clearly, it just suffices to show that ÃÇÔ∏ÄŒì*
pos ‚âª0. Notice
that ÃÉÔ∏ÄŒìùëç|ùëå= ùí™ùëÜpr(ùêº‚àíŒ†ùêµùêµ‚ä§Œ†)ùëÜ‚ä§
pr ùí™‚ä§‚âª0 where ùêµùêµ‚ä§:= ùëÜ‚ä§
pr ùê∫‚ä§‚àëÔ∏Äùëü
ùëñ=1 ùëûùëñùëû‚ä§
ùëñùê∫ùëÜpr.
Thus, we can apply Lemma D.4 with ùëÄ:= ÃÉÔ∏ÄŒìùëç|ùëå, ùê¥:= ùí™ùëÜpr, ùê∂:= ùêº‚àíŒ†ùêµùêµ‚ä§Œ†,
and get ùê∂‚âª0. In particular, this shows that ÃÇÔ∏ÄŒì*
pos = ùëÜpr ùê∂ùëÜ‚ä§
pr ‚âª0 and thus ÃÇÔ∏ÄŒì*
pos is
in the feasible set of (4.15). Optimality of ÃÇÔ∏ÄŒì*
pos then follows almost immediately. By
Theorem 4.1:
ùëë‚Ñõ(Œìùëç|ùëå, ùí™ÃÇÔ∏ÄŒì*
pos ùí™‚ä§) = ùëë‚Ñõ(Œìùëç|ùëå, ÃÉÔ∏ÄŒìùëç|ùëå) ‚â§ùëë‚Ñõ(Œìùëç|ùëå, ÃÉÔ∏ÄŒì)
‚àÄÃÉÔ∏ÄŒì ‚àà‚Ñ≥ùëç
ùëü.
(D.11)
In particular, we can consider ÃÉÔ∏ÄŒì of the form ÃÉÔ∏ÄŒì = ùí™Œì ùí™‚ä§for Œì ‚àà‚Ñ≥ùëü. Notice that
ÃÉÔ∏ÄŒì ‚âª0 since ùí™is assumed to be full row-rank. This shows optimality of ÃÇÔ∏ÄŒì*
pos according
to (4.16).
‚ñ°
Proof of Lemma 4.5. We first provide an explicit square root factorization of
ÃÇÔ∏ÄŒì*
pos, defined in (4.15) (Lemma 4.4), as ÃÇÔ∏ÄŒì*
pos = ÃÇÔ∏ÄùëÜ*
pos (ÃÇÔ∏ÄùëÜ*
pos)‚ä§for some matrix ÃÇÔ∏ÄùëÜ*
pos. We
claim that
ÃÇÔ∏ÄùëÜ*
pos = ùëÜpr
(Ô∏É
ùëü
‚àëÔ∏Å
ùëñ=1
(
‚àöÔ∏Ä
1 ‚àíùúÜùëñ‚àí1) ¬Øùëûùëñ¬Øùëû‚ä§
ùëñ+ ùêº
)Ô∏É
(D.12)
218

where ùêºis the identity matrix. and the (¬Øùëûùëñ) are defined in (4.18). First of all, notice
that (D.12) is well defined since 1 > ùúÜùëñ> 0 for all ùëñ= 1, . . . , ùëü(see the proof of
Theorem 4.1). One can verify that (D.12) is indeed a valid square root of ÃÇÔ∏ÄŒì*
pos. They
key observation is that ¬Øùëûùëñ= ùëÜ‚àí1
pr ÃÉÔ∏Äùëûùëñand that the vectors (ÃÉÔ∏Äùëûùëñ) are Œì‚àí1
pr -orthogonal, i.e.,
ÃÉÔ∏Äùëû‚ä§
ùëñŒì‚àí1
pr ÃÉÔ∏Äùëûùëó= ùõøùëñùëó. To see this, consider the following identities:
ÃÉÔ∏Äùëû‚ä§
ùëñŒì‚àí1
pr ÃÉÔ∏Äùëûùëó= ùëû‚ä§
ùëñùê∫ùëÜpr Œ† ùëÜ‚ä§
pr Œì‚àí1
pr ùëÜpr Œ† ùëÜ‚ä§
pr ùê∫‚ä§ùëûùëó= ùëû‚ä§
ùëñùê∫ùëÜpr Œ† ùëÜ‚ä§
pr ùê∫‚ä§ùëûùëó.
(D.13)
Since Œ† = ùëÜ‚ä§
pr ùí™‚ä§Œì‚àí1
ùëçùí™ùëÜpr, it must be that ÃÉÔ∏Äùëû‚ä§
ùëñŒì‚àí1
pr ÃÉÔ∏Äùëûùëó= ùëû‚ä§
ùëñùê∫Œìpr ùí™‚ä§Œì‚àí1
ùëçùí™Œìpr ùê∫‚ä§ùëûùëó=
ùõøùëñùëó, for the (ùëûùëñ) are the generalized eigenvectors of the pencil (4.8), properly normal-
ized. Now notice that ÃÉÔ∏ÄùëÜùëç|ùëå= ùí™ÃÇÔ∏ÄùëÜ*
pos and thus (4.15) implies that ÃÉÔ∏ÄùëÜùëç|ùëåÃÉÔ∏ÄùëÜ‚ä§
ùëç|ùëå= ÃÉÔ∏ÄŒìùëç|ùëå.
‚ñ°
Proof of Theorem 4.2. By applying [261, Theorem 4.1] to the linear Gaus-
sian model defined in Lemma 4.1, we know that a minimizer of (4.22) is given by:
ùê¥* = ‚àëÔ∏Äùëü
ùëñ=1 ùúÇùëñ(1 + ùúÇ2
ùëñ)‚àí1 ÃÇÔ∏ÄùëûùëñÃÇÔ∏Äùë£‚ä§
ùëñ, where (ùúÇ2
ùëñ, ÃÇÔ∏Äùëûùëñ) are eigenpairs of (ùêªùëç, Œì‚àí1
ùëç) with normal-
ization ÃÇÔ∏Äùëû‚ä§
ùëñŒì‚àí1
ùëçÃÇÔ∏Äùëûùëñ= 1, whereas (ÃÇÔ∏Äùë£ùëñ) are eigenvectors of (ùê∫ùí™‚Ä† Œìùëçùí™‚ä§
‚Ä† ùê∫‚ä§, ŒìŒî) with nor-
malization ÃÇÔ∏Äùë£‚ä§
ùëñŒìŒî ÃÇÔ∏Äùë£ùëñ= 1. Moreover, [261, Theorem 4.1] tells us that the Bayes risk as-
sociated with the minimizer ùê¥* can be written as: E[ ‚Äñ ùê¥* ùëå‚àíùëç‚Äñ2
Œì‚àí1
ùëç|ùëå] = ‚àëÔ∏Ä
ùëñ>ùëüùúÇ2
ùëñ+ùëõ,
where ùëõis the dimension of the parameter space. The fact that the vectors (ÃÇÔ∏Äùëûùëñ) can
be written as ÃÇÔ∏Äùëûùëñ= ùí™Œìpr ùê∫‚ä§ùëûùëñfor ùúÇ2
ùëñ> 0 was proved in Theorem 4.1.
Further-
more, in the proof of Theorem 4.1 we showed that ùúÇ2
ùëñ= ùúÜùëñ(1 ‚àíùúÜùëñ)‚àí1. Using the
latter expression we can rewrite the minimizer as ùê¥* = ‚àëÔ∏Äùëü
ùëñ=1
‚àöÔ∏Ä
ùúÜùëñ(1 ‚àíùúÜùëñ) ÃÇÔ∏ÄùëûùëñÃÇÔ∏Äùë£‚ä§
ùëñ. If
(ÃÇÔ∏Äùë£ùëñ) are eigenvectors of (ùê∫ùí™‚Ä† Œìùëçùí™‚ä§
‚Ä† ùê∫‚ä§, ŒìŒî), then they must also be eigenvectors of
(ùê∫Œìpr ùí™‚ä§Œì‚àí1
ùëçùí™Œìpr ùê∫‚ä§, Œìùëå). In particular, we can set ÃÇÔ∏Äùë£ùëñ= ùõºùëûùëñfor some real ùõº> 0.
Given the normalizations ùëû‚ä§
ùëñùê∫Œìpr ùí™‚ä§Œì‚àí1
ùëçùí™Œìpr ùê∫‚ä§ùëûùëñ= 1 and ÃÇÔ∏Äùë£‚ä§
ùëñŒìŒî ÃÇÔ∏Äùë£ùëñ= 1, it must
be ùõº= ùúÜ1/2
ùëñ
(1 ‚àíùúÜùëñ)‚àí1/2. Simple algebra then leads to (4.23).
‚ñ°
219

220

Appendix E
Proofs for Chapter 5
Proof of Lemma 5.2.
The general solution of ùúï2
ùëñ,ùëólog ùúã= 0 on Rùëõis given by
log ùúã(ùëß) = ùëî(ùëß1:ùëñ‚àí1, ùëßùëñ+1:ùëõ) + ‚Ñé(ùëß1:ùëó‚àí1, ùëßùëó+1:ùëõ) for some functions ùëî, ‚Ñé: Rùëõ‚àí1 ‚ÜíR.
Hence ùëçùëñ‚ä•‚ä•ùëçùëó| ùëçùí±‚àñ(ùëñ,ùëó) [161]. Conversely, if ùëçùëñ‚ä•‚ä•ùëçùëó| ùëçùí±‚àñ(ùëñ,ùëó), then ùúãmust factor as
ùúã= ùúãùëçùëñ|ùëçùí±‚àñ(ùëñ,ùëó)ùúãùëçùëó|ùëçùí±‚àñ(ùëñ,ùëó)ùúãùëçùí±‚àñ(ùëñ,ùëó),
(E.1)
so that ùúï2
ùëñ,ùëólog ùúã= 0 on Rùëõ.
‚ñ°
Proof of Theorem 5.1.
We begin with Part 1 of the theorem.
Let ùúÇ, ùúãbe a
pair of strictly positive densities for ùúàùúÇand ùúàùúã, respectively (these positive densities
exist since the measures are fully supported). Now consider a version of the KR
rearrangement, ùëÜ, that pushes forward ùúàùúãto ùúàùúÇas given by Definition B.2 for the
pair ùúÇ, ùúã(Appendix B). By definition, and for all ùëß1:ùëò‚àí1 ‚ààRùëò‚àí1, the map ùúâ‚Ü¶‚Üí
ùëÜùëò(ùëß1:ùëò‚àí1, ùúâ) is the monotone increasing rearrangement that pushes forward ùúâ‚Ü¶‚Üí
ùúãùëçùëò|ùëç1:ùëò‚àí1(ùúâ|ùëß1:ùëò‚àí1) to the marginal ùúÇùëãùëò(recall that ùúàùúÇis a tensor product measure).
Moreover, it follows easily from [161, Proposition 3.17], that each marginal ùúãùëç1:ùëò‚Äîor
better yet, the corresponding measure‚Äîis globally Markov with respect to ùí¢ùëò, and
that ùúãùëç1:ùëò(ùëß1:ùëò) ùúãùíû(ùëßùíû) = ùúãùëçùëò,ùëçùíû(ùëßùëò, ùëßùíû) ùúãùëç1:ùëò‚àí1(ùëß1:ùëò‚àí1), where ùíû:= Nb(ùëò, ùí¢ùëò), possibly
empty. Thus, the conditional ùúãùëçùëò|ùëç1:ùëò‚àí1(ùëßùëò|ùëß1:ùëò‚àí1) is constant along any input ùëßùëówith
ùëó/‚ààNb(ùëò, ùí¢ùëò). For any such ùëó, ùëÜùëòmust be constant along its ùëóth input, so that
(ùëó, ùëò) ‚ààÃÇÔ∏ÄIùëÜ.
221

Part 2 of the theorem follows similarly. Consider the KR rearrangement, ùëá, that
pushes forward ùúàùúÇto ùúàùúãas given by Definition B.2. For all ùë•1:ùëò‚àí1 ‚ààRùëò‚àí1, the map
ùúâ‚Ü¶‚Üíùëáùëò(ùë•1:ùëò‚àí1, ùúâ) is the monotone increasing rearrangement that pushes forward ùúÇùëãùëò
to
ùúâ‚Ü¶‚Üíùúãùëçùëò|ùëç1:ùëò‚àí1(ùúâ|ùëá1(ùë•1), . . . , ùëáùëò‚àí1(ùë•1:ùëò‚àí1)).
(E.2)
We already know that ùúãùëçùëò|ùëç1:ùëò‚àí1(ùëßùëò|ùëß1:ùëò‚àí1) can only depend (nontrivially) on ùëßùëòand
on ùëßùëófor ùëó‚ààNb(ùëò, ùí¢ùëò). Hence, if none of the components ùëáùëñ, with ùëñ‚ààNb(ùëò, ùí¢ùëò),
depends on the ùëóth input, then ùëáùëòis constant along its ùëóth input as well, so that
(ùëó, ùëò) ‚ààÃÇÔ∏ÄIùëá.
For Part 3, let (ùëó, ùëò) ‚ààÃÇÔ∏ÄIùëá. Then, by definition, (ùëó, ùëñ) ‚ààÃÇÔ∏ÄIùëáfor all ùëñ‚ààNb(ùëò, ùí¢ùëò),
which also implies that ùëó/‚ààNb(ùëò, ùí¢ùëò) since ùëóÃ∏= ùëñfor all (ùëó, ùëñ) ‚ààÃÇÔ∏ÄIùëá. Hence (ùëó, ùëò) ‚ààÃÇÔ∏ÄIùëÜ
and this shows the inclusion ÃÇÔ∏ÄIùëá‚äÇÃÇÔ∏ÄIùëÜ.
These arguments show that there exists at least a version of the KR rearrangement
that is exactly at least as sparse as predicted by the theorem.
‚ñ°
The following lemma specializes the results of Theorem 5.1[Part 2] to the case of
I-maps ùí¢with a disconnected component, and will be useful in the proofs of Section
5.5.
Lemma E.1. Let ùëã‚àºùúàùúÇ, ùëç‚àºùúàùúãwith ùúàùúÇ, ùúàùúã‚ààM+(Rùëõ) and ùúàùúÇtensor product
measure, and let ùúébe any permutation of Nùëõ. Moreover, assume that ùúàùúãis globally
Markov with respect to ùí¢= (ùí±, ‚Ñ∞), and assume that there exists a nonempty set ùíú‚äÇ
ùí±‚âÉNùëõsuch that ùëçùíú‚ä•‚ä•ùëçùí±‚àñùíúand ùëçùíú= ùëãùíúin distribution. Then the ùúé-generalized
KR rearrangement ùëágiven by Definition B.3 (for a pair ùúÇ, ùúãof nonvanishing densities
for ùúàùúÇand ùúàùúã, respectively) is low-dimensional with respect to ùíú, i.e.,
1. ùëáùëò(ùë•) = ùë•ùëòfor ùëò‚ààùíú
2. ùúïùëóùëáùëò= 0 for ùëó‚ààùíúand ùëò‚ààùí±‚àñùíú.
Proof. It suffices to prove the lemma for a lower triangular KR rearrangement; the
result for an arbitrary ùúéthen follows trivially. If ùíú= ùí±, then ùëáis simply the identity
map. Thus we assume that ùí±‚àñùíúis nonempty.
222

We begin with Part 1 of the lemma and use the results of Theorem 5.1[Part 2] to
characterize the sparsity of the rearrangement. Let ùëò‚ààùíúand notice that Nb(ùëò, ùí¢ùëò) =
‚àÖ, where ùí¢ùëòis the marginal graph defined in Theorem 5.1. Thus (ùëó, ùëò) ‚ààÃÇÔ∏ÄIùëá‚äÇIùëáfor
all ùëó= 1, . . . , ùëò‚àí1, so that ùëáùëò(ùë•) = ùë•ùëòfor all ùëò‚ààùíú.
Now let us focus on Part 2 and prove that (ùëó, ùëò) ‚ààÃÇÔ∏ÄIùëáfor all ùëó‚ààùíúand ùëò‚ààùí±‚àñùíú.
We proceed by contradiction. Assume that there exists some pair (ùëó, ùëò) ‚ààùíú√ó(ùí±‚àñùíú)
such that (ùëó, ùëò) /‚ààÃÇÔ∏ÄIùëá. In particular, let ùí¶be the set of ùëò‚ààùí±‚àñùíúfor which there
exists at least a ùëó‚ààùíúsuch that (ùëó, ùëò) /‚ààÃÇÔ∏ÄIùëá. Clearly ùí¶is nonempty and finite. Let
ùë†be the minimum integer in ùí¶, and let ùëó‚ààùíúbe a corresponding index for which
(ùëó, ùë†) /‚ààÃÇÔ∏ÄIùëá. In this case, by Theorem 5.1[Part 2], there must exist an ùëñ‚ààNb(ùë†, ùí¢ùë†)
such that (ùëó, ùëñ) /‚ààÃÇÔ∏ÄIùëá. Now there are two cases: either ùëñ‚ààùíú(for which we reach a
contradiction by part 1 of the lemma) or ùëñ‚ààùí±‚àñùíú. In the latter case, we also reach a
contradiction since ùëñ< ùë†and ùë†was defined as the smallest index for which (ùëó, ùë†) /‚ààÃÇÔ∏ÄIùëá
for some ùëó‚ààùíú.
Proof of Theorem 5.2. For notational convenience, we drop the subscript and
superscript ùëñfrom ùúàùëñ, ùúãùëñ, ùëçùëñ, and ùí¢ùëñ. Consider a factorization of ùúãof the form
ùúã(ùëß) = 1
c ùúìùíú‚à™ùíÆ(ùëßùíú‚à™ùíÆ) ùúìùíÆ‚à™‚Ñ¨(ùëßùíÆ‚à™‚Ñ¨),
(E.3)
where ùúìùíú‚à™ùíÆis strictly positive and integrable, with c =
‚à´Ô∏Ä
ùúìùíú‚à™ùíÆ< ‚àû. A factorization
like (E.3) always exist since ùúàfactorizes according to ùí¢‚Äîthus ùí¢is an I-map for
ùúà‚Äîand since (ùíú, ùíÆ, ‚Ñ¨) is a proper decomposition of ùí¢. For instance, one can set
ùúìùíú‚à™ùíÆ= ùúãùëçùíú‚à™ùíÆ, c = 1, and ùúìùíÆ‚à™‚Ñ¨= ùúãùëç‚Ñ¨|ùëçùíÆsince ùëçùíú‚ä•‚ä•ùëç‚Ñ¨|ùëçùíÆand since ùúãis a
nonvanishing density of ùúà. However, this is not the only possibility. See Section 5.6
for important examples where it is not convenient to assume that ùúìùíú‚à™ùíÆcorresponds
to a marginal of ùúã. This proves Part 1 of the theorem.
By [161, Proposition 3.16], we can rewrite ùúìùíÆ‚à™‚Ñ¨as:
ùúìùíÆ‚à™‚Ñ¨(ùëßùíÆ‚à™‚Ñ¨) =
‚àèÔ∏Å
ùíû‚ààùíûùíÆ‚à™‚Ñ¨
ùúìùíû(ùëßùíû)
(E.4)
223

for some nonvanishing functions (ùúìùíû), where ùíûùíÆ‚à™‚Ñ¨denotes the set of maximal cliques
of the subgraph ùí¢ùíÆ‚à™‚Ñ¨. Since ùíÆis a fully connected separator set (possibly empty) for
ùíúand ‚Ñ¨, the maximal cliques of ùí¢ùíÆ‚à™‚Ñ¨are precisely the maximal cliques of ùí¢that
are a subset of ùíÆ‚à™‚Ñ¨. We are going to use (E.4) shortly.
Define ÃÉÔ∏Äùúã: Rùëõ‚ÜíR as ÃÉÔ∏Äùúã(ùëß) = ùúìùíú‚à™ùíÆ(ùëßùíú‚à™ùíÆ) ùúÇùëã‚Ñ¨(ùëß‚Ñ¨)/c, and notice that ÃÉÔ∏Äùúãis a
nonvanishing probability density. Denote the corresponding measure by ÃÉÔ∏Äùúà‚ààM+(Rùëõ).
For an arbitrary permutation ùúéof Nùëõthat satisfies (5.21), let ùêøùëñbe the ùúé-generalized
KR rearrangement that pushes forward ùúàùúÇto ÃÉÔ∏Äùúàas given by Definition B.3 in Appendix
B. By Lemma E.1, ùêøùëñis low-dimensional with respect to ‚Ñ¨(Part 2a of the theorem).
To see this, let ÃÉÔ∏Äùëç‚àºÃÉÔ∏Äùúà, and notice that ÃÉÔ∏Äùëç‚Ñ¨‚ä•‚ä•ÃÉÔ∏Äùëçùíú‚à™ùíÆand ÃÉÔ∏Äùëç‚Ñ¨= ùëã‚Ñ¨in distribution.
By Lemma B.1, we can write a density of the pullback measure ùêø‚ôØ
ùëñùúàas:
ùêø‚ôØ
ùëñùúã
=
ùúã‚àòùêøùëñ| det ‚àáùêøùëñ|
(E.5)
=
(Ô∏Å
ùêø‚ôØ
ùëñÃÉÔ∏Äùúã
)Ô∏Å‚àèÔ∏Ä
ùíû‚ààùíûùíÆ‚à™‚Ñ¨ùúìùíû‚àòùêøùíû
ùëñ
ùúÇùëã‚Ñ¨
=
ùúÇùëãùíú‚à™ùíÆ
‚àèÔ∏Å
ùíû‚ààùíûùíÆ‚à™‚Ñ¨
ùúìùíû‚àòùêøùíû
ùëñ,
where we used the identity ùúã= ÃÉÔ∏ÄùúãùúìùíÆ‚à™‚Ñ¨/ùúÇùëã‚Ñ¨together with (E.4) and the fact that
ùêøùëò
ùëñ(ùë•) = ùë•ùëòfor ùëò‚àà‚Ñ¨(Part 2a), and where, for any ùíû= {ùëê1, . . . , ùëê‚Ñì} ‚ààùíûùíÆ‚à™‚Ñ¨with
ùúìùíû(ùëßùíû) = ùúìùíû(ùëßùëê1, . . . , ùëßùëê‚Ñì), ùêøùíû
ùëñis a map Rùëõ‚ÜíR‚Ñìgiven by ùë•‚Ü¶‚Üí(ùêøùëê1
ùëñ(ùë•), . . . , ùêøùëê‚Ñì
ùëñ(ùë•)).
If ùëç‚Ä≤ ‚àºùêø‚ôØ
ùëñùúà, then (E.5) shows that ùëç‚Ä≤
ùíú‚ä•‚ä•ùëç‚Ä≤
ùíÆ‚à™‚Ñ¨and that ùëç‚Ä≤
ùíú= ùëãùíúin distribution
(Part 2c of the theorem). Moreover, from the factorization in (E.5), we can easily
construct a graph for which ùêø‚ôØ
ùëñùúàfactorizes: it suffices to consider the scope of the
factors (ùúìùíû‚àòùêøùíû
ùëñ), i.e., the indices of the input variables that each ùúìùíû‚àòùêøùíû
ùëñcan depend
on. Recall that for a ùúé-triangular map, the ùúé(ùëò)th component can only depend on
the variables ùë•ùúé(1), . . . , ùë•ùúé(ùëò). For each ùíû‚ààùíûùíÆ‚à™‚Ñ¨there are two possibilites: Either
ùíû‚à©ùíÆ= ‚àÖ, in which case the scope of ùúìùíû‚àòùêøùíû
ùëñis simply ùíûsince ùêøùëò
ùëñ(ùë•) = ùë•ùëòfor
ùëò‚àà‚Ñ¨. Or ùíû‚à©ùíÆis nonempty, in which case let ùëóùíûbe the maximum integer ùëósuch
that ùúé(ùëó) ‚ààùíû‚à©ùíÆ, and notice that the scope of ùúìùíû‚àòùêøùíû
ùëñis simply ùíû‚à™{ùúé(1), . . . , ùúé(ùëóùíû)}.
Thus, we can modify ùí¢to obtain an I-map for ùêø‚ôØ
ùëñùúàas follows: (1) Remove any edge
224

that is incident to any node in ùíúbecause of Part 2c. (2) For every maximal clique
ùíûin ùí¢that is a subset of ùíÆ‚à™‚Ñ¨and that has nonempty intersection with ùíÆ, turn
ùíû‚à™{ùúé(1), . . . , ùúé(ùëóùíû)} into a clique. This proves Part 2d of the theorem.
Now let Rùëñbe the set of maps Rùëõ‚ÜíRùëõthat are low-dimensional with respect
to ùíúand that push forward ùúàùúÇto ùêø‚ôØ
ùëñùúà. Rùëñis nonempty. To see this, let ùëÖbe the
ùúé-generalized KR rearrangement that pushes forward ùúàùúÇto ùêø‚ôØ
ùëñùúà, for an arbitrary
permutation ùúé, as given by Definition B.3 (for the pair of nonvanishing densities ùúÇ
and ùêø‚ôØ
ùëñùúã). By Part 2c and Lemma E.1, ùëÖis low-dimensional with respect to ùíú. Thus
ùëÖ‚ààRùëñ(Part 2b of the theorem).
Let Dùëñ:= ùêøùëñ‚àòRùëñbe the set of maps that can be written as ùêøùëñ‚àòùëÖfor some ùëÖ‚ààRùëñ.
By construction, each ùëá‚ààDùëñpushes forward ùúàùúÇto ùúà(part 2 of the theorem).
‚ñ°
In the following corollary every symbol should be interpreted as in Theorem 5.2.
Corollary E.1. Given the hypothesis of Theorem 5.2, assume that there exists ùíú‚ä•‚äÇ
ùíúsuch that ùëçùëñ
ùíú‚ä•‚ä•‚ä•ùëçùëñ
ùí±‚àñùíú‚ä•and ùëçùëñ
ùíú‚ä•= ùëãùíú‚ä•in distribution.
Then ùêøùëñis low-
dimensional with respect to ùíú‚ä•‚à™‚Ñ¨, while each ùëá‚ààDùëñis low-dimensional with
respect to ùíú‚ä•.
Proof. By Theorem 5.2[Part 2a], ùêøùëñis low-dimensional with respect to ‚Ñ¨, while
Lemma E.1 shows that ùêøùëñis also low-dimensional with respect to ùíú‚ä•. Moreover,
notice that if ùíú‚ä•is nonempty, then for all ùëá= ùêøùëñ‚àòùëÖin Dùëñ, we have ùëáùëò(ùë•) = ùë•ùëò
for ùëò‚ààùíú‚ä•since ùêøùëò
ùëñ(ùë•) = ùë•ùëòand ùëÖùëò(ùë•) = ùë•ùëòfor ùëò‚ààùíú‚ä•(Theorem 5.2[Parts
2b]). Additionally, ùúïùëóùëáùëò= 0 for ùëó‚ààùíú‚ä•and ùëò‚ààùí±‚àñùíú‚ä•. To see this, notice that
ùëáùëò(ùë•) = ùêøùëò
ùëñ(ùëÖ(ùë•)) and that the following two facts hold: (1) The component ùêøùëò
ùëñ,
for ùëò‚ààùí±‚àñùíú‚ä•, does not depend on input variables whose index is in ùíú‚ä•since ùêøùëñ
is low-dimensional with respect to ùíú‚ä•; (2) The ‚Ñìth component of ùëÖwith ‚Ñì/‚ààùíú‚ä•
also does not depend on ùë•ùíú‚ä•since ùëÖis low-dimensional with respect to ùíú(Theorem
5.2[Parts 2b]). Hence, ùëámust be a low-dimensional map with respect to ùíú‚ä•.
Proof of Lemma 5.3. Let ùúàùúÇ, ùúàùëñ, ùúãùëñ, ùí¢ùëñ, Dùëñ, ùêøùëñ, Rùëñ, and ùí¢ùëñ+1 be defined as in Theo-
rem 5.2 for a proper decomposition (ùíúùëñ, ùíÆùëñ, ‚Ñ¨ùëñ) of ùí¢ùëñ, a permutation ùúéùëñthat satisfies
(5.21), and for any factorization (5.20) of ùúãùëñ.
225

We first want to prove that ùíÆùëñ‚à™‚Ñ¨ùëñis fully connected in ùí¢ùëñ+1 if and only if the
decomposition (ùíúùëñ+1, ùíÆùëñ+1, ‚Ñ¨ùëñ+1) of Part 1 does not exist.
Let us start with one
direction. Assume that a decomposition like the one in Part 1 does not exist, despite
the possibility to add edges to ùí¢ùëñ+1 in ùí±‚àñùíúùëñ. We want to show that in this case
ùíÆùëñ‚à™‚Ñ¨ùëñmust be a clique in ùí¢ùëñ+1. Since ‚Ñ¨ùëñis nonempty, there are two possibilities:
either |ùíÆùëñ‚à™‚Ñ¨ùëñ| = 1 or |ùíÆùëñ‚à™‚Ñ¨ùëñ| > 1. If |ùíÆùëñ‚à™‚Ñ¨ùëñ| = 1, then ùíÆùëñ‚à™‚Ñ¨ùëñconsists of a single
node and thus it is a trivial clique. If |ùíÆùëñ‚à™‚Ñ¨ùëñ| > 1, then ùíÆùëñ‚à™‚Ñ¨ùëñcontains at least two
nodes. In this case, let us proceed by contradiction and assume that ùíÆùëñ‚à™‚Ñ¨ùëñis not
fully connected in ùí¢ùëñ+1 = (ùí±, ‚Ñ∞ùëñ+1), i.e., there exist a pair of nodes ùõº, ùõΩ‚ààùíÆùëñ‚à™‚Ñ¨ùëñsuch
that (ùõº, ùõΩ) /‚àà‚Ñ∞ùëñ+1. Let ùíúùëñ+1 = ùíúùëñ‚à™{ùõº}, ‚Ñ¨ùëñ+1 = {ùõΩ}, and ùíÆùëñ+1 = (ùí±‚àñùíúùëñ+1) ‚àñ‚Ñ¨ùëñ+1.
Notice that (ùíúùëñ+1, ùíÆùëñ+1, ‚Ñ¨ùëñ+1) forms a partition of ùí±, with nonempty ùíúùëñ+1, ‚Ñ¨ùëñ+1 and
with ùíúùëñ+1 strict superset of ùíúùëñ. Moreover ùíÆùëñ+1 must be a separator set for ùíúùëñ+1 and
‚Ñ¨ùëñ+1 since (ùõº, ùõΩ) /‚àà‚Ñ∞ùëñ+1 and ùíúùëñis disconnected from ùíÆùëñ‚à™‚Ñ¨ùëñin ùí¢ùëñ+1 (Theorem 5.2[Part
2d]). Now there are two cases: If ùíÆùëñ+1 = ‚àÖ, then (ùíúùëñ+1, ùíÆùëñ+1, ‚Ñ¨ùëñ+1) is a decomposition
that satisfies Part 1 of the lemma (contradiction). If ùíÆùëñ+1 Ã∏= ‚àÖ, then we can always
add enough edges to ùí¢ùëñ+1 in ùíÆùëñ‚à™‚Ñ¨ùëñ‚äÉùíÆùëñ+1 in order to make ùíÆùëñ+1 fully connected.
Also in this case, the resulting decomposition (ùíúùëñ+1, ùíÆùëñ+1, ‚Ñ¨ùëñ+1) satisfies Part 1 of the
lemma and thus leads to a contradiction.
Now the reverse direction. Assume that ùíÆùëñ‚à™‚Ñ¨ùëñis a clique in ùí¢ùëñ+1. If |ùíÆùëñ‚à™‚Ñ¨ùëñ| = 1,
then the decomposition of Part 1 cannot exist since both ùíúùëñ+1 ‚àñùíúùëñand ‚Ñ¨ùëñ+1 should
be nonempty. Hence, let |ùíÆùëñ‚à™‚Ñ¨ùëñ| > 1 and proceed by contradiction. That is, let
(ùíúùëñ+1, ùíÆùëñ+1, ‚Ñ¨ùëñ+1) be a proper decomposition that satisfies Part 1 of the lemma. Notice
that this decomposition must have been achieved without adding any edge to ùí¢ùëñ+1 in
ùíÆùëñ‚à™‚Ñ¨ùëñsince this set is already fully connected. By hypothesis, there must exist ùõº, ùõΩ
such that ùõº‚ààùíúùëñ+1 ‚àñùíúùëñand ùõΩ‚àà‚Ñ¨ùëñ+1. However, both ùõºand ùõΩare also in ùíÆùëñ‚à™‚Ñ¨ùëñ,
and so they must be connected by an edge in ùí¢ùëñ+1. Hence, ùíÆùëñ+1 is not a separator set
for ùíúùëñ+1 and ‚Ñ¨ùëñ+1 (contradiction).
The latter result proves directly Part 2 of the lemma. Moreover, it shows that if
ùíÆùëñ‚à™‚Ñ¨ùëñis not a clique in ùí¢ùëñ+1, then there exists a proper decomposition (ùíúùëñ+1, ùíÆùëñ+1, ‚Ñ¨ùëñ+1)
of ùí¢ùëñ+1, where ùíúùëñ+1 is a strict superset of ùíúùëñ, obtained, possibly, by adding edges to
226

ùí¢ùëñ+1 in order to turn ùíÆùëñ+1 into a clique. Note that even if we add edges to ùí¢ùëñ+1, ùêø‚ôØ
ùëñùúàùëñ
still factorizes according to the resulting graph, which is then an I-map for ùêø‚ôØ
ùëñùúàùëñ.
Moreover we can really only add edges in ùí±‚àñùíúùëñsince ùíúùëñmust be a strict subset of
ùíúùëñ+1, and thus ùíúùëñremains disconnected from ùíÆùëñ‚à™‚Ñ¨ùëñin ùí¢ùëñ+1. Let Dùëñ+1, ùêøùëñ+1, Rùëñ+1
be defined as in Theorem 5.2 for the pair of measures ùúàùúÇ, ùúàùëñ+1 = ùêø‚ôØ
ùëñùúàùëñ, the decom-
position (ùíúùëñ+1, ùíÆùëñ+1, ‚Ñ¨ùëñ+1) of ùí¢ùëñ+1, a permutation ùúéùëñ+1 that satisfies (5.21), and for
any factorization (5.20) (note that ùêø‚ôØ
ùëñùúàùëñ‚ààM+(Rùëõ) by Theorem 5.2[Part 2b]). Fix
ùëá‚ààDùëñ+1. By Theorem 5.2[Part 2], ùëápushes forward ùúàùúÇto ùúàùëñ+1 = ùêø‚ôØ
ùëñùúàùëñ. Moreover, if
ùëçùëñ+1 ‚àºùêø‚ôØ
ùëñùúàùëñ, then by Theorem 5.2[Part 2c] we have ùëçùëñ+1
ùíúùëñ‚ä•‚ä•ùëçùëñ+1
ùíÆùëñ‚à™‚Ñ¨ùëñand ùëçùëñ+1
ùíúùëñ= ùëãùíúùëñ
in distribution. Then by Corollary E.1 it must also be that ùëáis low-dimensional with
respect to ùíúùëñ. Thus ùëá‚ààRùëñ, and this proves the inclusion Rùëñ‚äÉDùëñ+1.
Now fix any ùëá‚ààùêøùëñ‚àòùêøùëñ+1 ‚àòRùëñ+1 = ùêøùëñ‚àòDùëñ+1. It must be that ùëá= ùêøùëñ‚àòùëîfor
some ùëî‚ààDùëñ+1 ‚äÇRùëñ, so that ùëá‚ààùêøùëñ‚àòRùëñ, which shows the inclusion ùêøùëñ‚àòRùëñ‚äÉ
ùêøùëñ‚àòùêøùëñ+1 ‚àòRùëñ+1 (Part 1a of the lemma). By Corollary E.1, we have that ùêøùëñ+1 is low-
dimensional with respect to ùíúùëñ‚à™‚Ñ¨ùëñ+1, and so its effective dimension is bounded above
by |ùí±‚àñ(ùíúùëñ‚à™‚Ñ¨ùëñ+1)| = |(ùíúùëñ+1 ‚àñùíúùëñ)‚à™ùíÆùëñ+1| (Part 1b). Finally, by Theorem 5.2[Part 2b],
each ùëÖ‚ààRùëñ+1 is low-dimensional with respect to ùíúùëñ+1, and so its effective dimension
is bounded by |ùí±‚àñùíúùëñ+1| (Part 1c).
‚ñ°
Proof of Theorem 5.3. For the sake of clarity, we divide the proof in two parts:
First, we show that the maps (Mùëñ)ùëñ‚â•0 are well-defined. Then, we prove the remaining
claims of the theorem.
The maps (Mùëñ)ùëñ‚â•0 are well-defined as long as, for instance, we show that ùúãùëñis a
probability density for all ùëñ‚â•0, and as long as there exist permutations (ùúéùëñ) that
guarantee the block upper triangular structure of (5.28). As for the permutations, it
suffices to consider ùúé= ùúé1 = ùúé2 = ¬∑ ¬∑ ¬∑ with ùúé(N2ùëõ) = {2ùëõ, 2ùëõ‚àí1, . . . , 1}, i.e., upper
triangular maps. (If ùëõ> 1, then there is some freedom in the choice of ùúé.) As for the
targets (ùúãùëñ), we now show that ùúãùëñis a nonvanishing density and that the marginal
‚à´Ô∏Ä
ùúãùëñ(ùëßùëñ, ùëßùëñ+1) dùëßùëñ= ùúãùëçùëñ+1|ùë¶0:ùëñ+1, for all ùëñ‚â•0, using an induction argument over ùëñ. For
227

the base case (ùëñ= 0), just notice that
c0 =
‚à´Ô∏Å
ÃÉÔ∏Äùúã0(ùëß0, ùëß1) dùëß0:1 = ùúãùëå0,ùëå1(ùë¶0, ùë¶1) < ‚àû,
(E.6)
so that ùúã0 = ÃÉÔ∏Äùúã0/c0 > 0 is a valid density. Moreover, we have the desired marginal,
i.e.,
‚à´Ô∏Å
ùúã0(ùëß0, ùëß1) dùëß0 =
‚à´Ô∏Å
ùúãùëç0,ùëç1|ùëå0,ùëå1(ùëß0, ùëß1|ùë¶0, ùë¶1) dùëß0 = ùúãùëç1|ùëå0,ùëå1(ùëß1|ùë¶0, ùë¶1).
(E.7)
Now assume that ùúãùëñis a nonvanishing density and that the marginal
‚à´Ô∏Ä
ùúãùëñ(ùëßùëñ, ùëßùëñ+1) dùëßùëñ=
ùúãùëçùëñ+1|ùë¶0:ùëñ+1 for some ùëñ> 0. The map Mùëñis then well-defined. In particular, by def-
inition of KR rearrangement, the submap M1
ùëñpushes forward ùúÇùëãùëñ+1 to the marginal
‚à´Ô∏Ä
ùúãùëñ(ùëßùëñ, ùëßùëñ+1) dùëßùëñ. Moreover, by Lemma B.1, we have:
cùëñ+1
=
‚à´Ô∏Å
ùúÇùëãùëñ+1(ùëßùëñ+1) ÃÉÔ∏Äùúãùëñ+1(M1
ùëñ(ùëßùëñ+1), ùëßùëñ+2) dùëßùëñ+1:ùëñ+2
(E.8)
=
‚à´Ô∏Å
ùúãùëçùëñ+2,ùëåùëñ+2|ùëå0:ùëñ+1(ùëßùëñ+2, ùë¶ùëñ+2|ùë¶0:ùëñ+1) dùëßùëñ+2
=
ùúãùëåùëñ+2|ùëå0:ùëñ+1(ùë¶ùëñ+2|ùë¶0:ùëñ+1) < ‚àû,
where we used the change of variables ùë•ùëñ+1 = M1
ùëñ(ùëßùëñ+1) and the fact that (M1
ùëñ)‚ôØùúÇùëãùëñ+1 =
ùúãùëçùëñ+1|ùë¶0:ùëñ+1 (induction hypothesis). Thus ùúãùëñ+1 is a nonvanishing density and by (E.8)
we can easily verify that ùúãùëñ+1 has the desired marginal, i.e.,
‚à´Ô∏Ä
ùúãùëñ+1(ùëßùëñ+1, ùëßùëñ+2) dùëßùëñ+1 =
ùúãùëçùëñ+2|ùë¶0:ùëñ+2. This argument completes the induction step and shows that not only the
maps (Mùëñ)ùëñ‚â•0 are well-defined‚Äîtogether with the maps (ùëáùëñ)ùëñ‚â•0 in (5.30)‚Äîbut also
that (M1
ùëñ)‚ôØùúÇùëãùëñ+1 = ùúãùëçùëñ+1|ùë¶0:ùëñ+1 for all ùëñ‚â•0 (Part 1 of the theorem).
Now we move to Part 3 of the theorem and use another induction argument over
ùëò‚â•0. For the base case (ùëò= 0), notice that T0 = ùëá0 = M0, and that, by definition,
M0 pushes forward ùúÇùëã0,ùëã1 to ùúã0 = ùúãùëç0,ùëç1|ùë¶0,ùë¶1.
Assume that Tùëòpushes forward ùúÇùëã0:ùëò+1 to ùúãùëç0:ùëò+1|ùë¶0:ùëò+1 for some ùëò> 0 (Tùëòis well-
defined for all ùëòsince the maps (ùëáùëñ)ùëñ‚â•0 in (5.30) are also well-defined), and notice
228

that
ùúãùëç0:ùëò+2|ùë¶0:ùëò+2 = ùúãùëç0:ùëò+1|ùë¶0:ùëò+1
ùúãùë¶ùëò+2|ùëçùëò+2 ùúãùëçùëò+2|ùëçùëò+1
ùúãùë¶ùëò+2|ùë¶0:ùëò+1
= ùúãùëç0:ùëò+1|ùë¶0:ùëò+1
ÃÉÔ∏Äùúãùëò+1
cùëò+1
,
(E.9)
where we used (E.8) and the definition of the collection (ÃÉÔ∏Äùúãùëñ). Let Tùëò+1 = ùëá0‚àò¬∑ ¬∑ ¬∑‚àòùëáùëò+1
be defined as in Part 3 of the theorem, and observe that Tùëò+1 = ùê¥ùëò+1 ‚àòùëáùëò+1 with
ùê¥ùëò+1(ùë•0:ùëò+2) =
‚é°
‚é£Tùëò(ùë•0:ùëò+1)
ùë•ùëò+2
‚é§
‚é¶,
ùëáùëò+1(ùë•0:ùëò+2) =
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
ùë•0
...
ùë•ùëò
M0
ùëò+1(ùë•ùëò+1, ùë•ùëò+2)
M1
ùëò+1(ùë•ùëò+2)
‚é§
‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é¶
.
(E.10)
Thus the following hold:
T‚ôØ
ùëò+1 ùúãùëç0:ùëò+2|ùëå0:ùëò+2
=
ùëá‚ôØ
ùëò+1
(Ô∏Ç(Ô∏Å
T‚ôØ
ùëòùúãùëç0:ùëò+1|ùë¶0:ùëò+1
)Ô∏Åùúãùëò+1
ùúÇùëãùëò+1
)Ô∏Ç
(E.11)
=
ùëá‚ôØ
ùëò+1
(Ô∏Ä
ùúÇùëã0:ùëòùúãùëò+1)Ô∏Ä
=
ùúÇùëã0:ùëòM‚ôØ
ùëò+1 ùúãùëò+1 = ùúÇùëã0:ùëò+2,
where we used the fact that by Lemma B.1 (applied iteratively) it must be that
(ùê¥ùëò+1 ‚àòùëáùëò+1)‚ôØùúå= ùëá‚ôØ
ùëò+1 ùê¥‚ôØ
ùëò+1ùúåfor all densities ùúå. (Notice that ùê¥ùëò+1 is the composition
of functions which are trivial embeddings into the identity map of KR rearrangements
that couple pair of measures in M+(Rùëõ√ó Rùëõ), and thus each map in the composition
satisfies the hypothesis of Lemma B.1.) In particular, (Tùëò+1)‚ôØùúÇùëã0:ùëò+2 = ùúãùëç0:ùëò+2|ùë¶0:ùëò+2
(Part 3 of the theorem).
Now notice that each Tùëòcan also be written as
Tùëò(ùë•0:ùëò+1) =
‚é°
‚é£ùêµùëò(ùë•0:ùëò+1)
Mùëò(ùë•ùëò, ùë•ùëò+1)
‚é§
‚é¶
(E.12)
for a multivariate function ùêµùëò‚Äîwhose particular form is not relevant to this argument‚Äî
229

and for a map, Mùëò, defined in (5.29) as a function on Rùëõ√ó Rùëõ. Since (Tùëò)‚ôØùúÇùëã0:ùëò+1 =
ùúãùëç0:ùëò+1|ùë¶0:ùëò+1, the map Mùëòmust also push forward ùúÇùëãùëò,ùëãùëò+1 to the lag-1 smoothing
marginal ùúãùëçùëò,ùëçùëò+1|ùë¶0:ùëò+1. This proves Part 2 of the theorem.
For Part 4, just notice that
ùúãùëå0:ùëò+1(ùë¶0:ùëò+1) = ùúãùëå0,ùëå1(ùë¶0, ùë¶1)
ùëò
‚àèÔ∏Å
ùëñ=1
ùúãùëåùëñ+1|ùëå0:ùëñ(ùë¶ùëñ+1|ùë¶0:ùëñ) =
ùëò
‚àèÔ∏Å
ùëñ=0
cùëñ,
(E.13)
where we used both (E.6) and (E.8).
‚ñ°
Proof of Lemma 5.4. First a remark about notation: we denote by ùí©(ùë•; ùúá, Œ£) the
density (as a function of ùë•) of a Gaussian with mean ùúáand covariance Œ£.
Now let ùëò> 0 and notice that ùúãùëçùëò+1|ùëçùëò(ùëßùëò+1|ùëßùëò) = ùí©(ùëßùëò+1; ùêπùëòùëßùëò, ùëÑùëò), ùúãùëåùëò+1|ùëçùëò+1(ùë¶ùëò+1|ùëßùëò+1) =
ùí©(ùë¶ùëò+1; ùêªùëò+1 ùëßùëò+1, ùëÖùëò+1) and ùúÇùëãùëò(ùëßùëò) = ùí©(ùëßùëò; 0, I). By definition of the target ùúãùëò
in Theorem 5.3, we have:
ùúãùëò(ùëßùëò, ùëßùëò+1)
=
ùúÇùëãùëò(ùëßùëò) ùúãùëåùëò+1|ùëçùëò+1(ùë¶ùëò+1|ùëßùëò+1) ùúãùëçùëò+1|ùëçùëò(ùëßùëò+1|M1
ùëò‚àí1(ùëßùëò))
=
ùí©(ùëßùëò; 0, I) ùí©(ùë¶ùëò+1; ùêªùëò+1 ùëßùëò+1, ùëÖùëò+1)
ùí©(ùëßùëò+1; ùêπùëò(ùê∂ùëò‚àí1 ùëßùëò+ ùëêùëò‚àí1), ùëÑùëò)
‚àù
exp(‚àí1
2ùëß‚ä§ùêΩùëß+ ùëß‚ä§‚Ñé),
where ùëß= (ùëßùëò, ùëßùëò+1) ‚ààR2ùëõ, and where ùêΩ‚ààR2ùëõ√ó2ùëõ, ‚Ñé‚ààR2ùëõare defined as
ùêΩ=
‚é°
‚é£ùêΩ11
ùêΩ12
ùêΩ‚ä§
12
ùêΩ22
‚é§
‚é¶,
‚Ñé=
‚é°
‚é£‚Ñé1
‚Ñé2
‚é§
‚é¶,
(E.14)
with:
‚éß
‚é™
‚é™
‚é™
‚é™
‚é™
‚é™
‚é™
‚é™
‚é™
‚é™
‚é™
‚é™
‚é®
‚é™
‚é™
‚é™
‚é™
‚é™
‚é™
‚é™
‚é™
‚é™
‚é™
‚é™
‚é™
‚é©
ùêΩ11 = I + ùê∂‚ä§
ùëò‚àí1 ùêπ‚ä§
ùëòùëÑ‚àí1
ùëòùêπùëòùê∂ùëò‚àí1
ùêΩ12 = ‚àíùê∂‚ä§
ùëò‚àí1 ùêπ‚ä§
ùëòùëÑ‚àí1
ùëò
ùêΩ22 = ùëÑ‚àí1
ùëò
+ ùêª‚ä§
ùëò+1 ùëÖ‚àí1
ùëò+1 ùêªùëò+1
‚Ñé1 = ùêΩ12 ùêπùëòùëêùëò‚àí1
‚Ñé2 = ùëÑ‚àí1
ùëòùêπùëòùëêùëò‚àí1 + ùêª‚ä§
ùëò+1 ùëÖ‚àí1
ùëò+1 ùë¶ùëò+1.
(E.15)
230

In particular, we can rewrite ùúãùëòin information form [156] as ùúãùëò(ùëß) = ùí©‚àí1(ùëß; ‚Ñé, ùêΩ).
Moreover we know by Theorem 5.3[Part 1], that the submap M1
ùëò(ùëßùëò+1) = ùê∂ùëòùëßùëò+1+ùëêùëò
pushes forward ùúÇùëãùëò+1 to the filtering marginal ùúãùëçùëò+1|ùë¶0:ùëò+1. Hence (ùëêùëò, ùê∂ùëò) should
be, respectively, the mean and a square root of the covariance of ùúãùëçùëò+1|ùë¶0:ùëò+1‚Äîthus
the output of any square-root Kalman filter at time ùëò+ 1. Now we just need to
determine the submap M0
ùëò(ùëßùëò, ùëßùëò+1) = ùê¥ùëòùëßùëò+ ùêµùëòùëßùëò+1 + ùëéùëò. Given that Mùëòis a
block upper triangular function, the map ùëßùëò‚Ü¶‚ÜíM0
ùëò(ùëßùëò, ùëßùëò+1) should push forward
ùúÇùëãùëòto ùëßùëò‚Ü¶‚Üíùúãùëò
ùëçùëò|ùëçùëò+1(ùëßùëò|M1
ùëò(ùëßùëò+1)). Notice that ùúãùëò
ùëçùëò|ùëçùëò+1(ùëßùëò|ùëßùëò+1) = ùí©‚àí1(ùëßùëò; ‚Ñé1 ‚àí
ùêΩ12 ùëßùëò+1, ùêΩ11) = ùí©(ùëßùëò; ùêΩ‚àí1
11 (‚Ñé1 ‚àíùêΩ12 ùëßùëò+1), ùêΩ‚àí1
11 ).
Hence ùúãùëò
ùëçùëò|ùëçùëò+1(ùëßùëò|M1
ùëò(ùëßùëò+1)) =
ùí©(ùëßùëò; ùêΩ‚àí1
11 ùêΩ12(ùêπùëòùëêùëò‚àí1 ‚àíùê∂ùëòùëßùëò+1 ‚àíùëêùëò), ùêΩ‚àí1
11 ), and so:
M0
ùëò(ùëßùëò, ùëßùëò+1) = ùêΩ‚àí1
11 ùêΩ12(ùêπùëòùëêùëò‚àí1 ‚àíùê∂ùëòùëßùëò+1 ‚àíùëêùëò) + ùêΩ‚àí1/2
11
ùëßùëò.
(E.16)
Simple algebra then leads to (5.35).
‚ñ°
Proof of Theorem 5.4.
We use a very similar argument to Theorem 5.3.
We
first show that the maps (Mùëñ)ùëñ‚â•0 are well-defined. These maps are well-defined as
long as, for instance, we show that ùúãùëñis a probability density for all ùëñ‚â•0, and as
long as there exist permutations (ùúéùëñ) that guarantee the generalized block triangular
structure of (5.37). As for the permutations, it suffices to consider ùúé= ùúé1 = ùúé2 = ¬∑ ¬∑ ¬∑
with ùúé(Nùëù+2ùëõ) = {1, . . . , ùëù, ùëù+ 2ùëõ, ùëù+ 2ùëõ‚àí1, . . . , ùëù+ 1}. As for the targets (ùúãùëñ), we
now use a (complete) induction argument over ùëñto show that, for all ùëñ‚â•0, ùúãùëñis a
nonvanishing density and
‚à´Ô∏Ä
ùúãùëñ(ùëßùúÉ, ùëßùëñ, ùëßùëñ+1) dùëßùëñ= ùê¥‚ôØ
ùëñùúãŒò,ùëçùëñ+1|ùë¶0:ùëñ+1(ùëßùúÉ, ùëßùëñ+1) for a map
ùê¥ùëñdefined on Rùëù√ó Rùëõas
ùê¥ùëñ(ùë•ùúÉ, ùë•ùëñ+1) =
‚é°
‚é£TŒò
ùëñ‚àí1(ùë•ùúÉ)
ùë•ùëñ+1
‚é§
‚é¶,
(E.17)
with TŒò
ùëñ‚àí1(ùë•ùúÉ) = ùë•ùúÉif ùëñ= 0.
For the base case (ùëñ= 0), just notice that c0 = ùúãùëå0,ùëå1(ùë¶0, ùë¶1) < ‚àû, so that
231

ùúã0 = ÃÉÔ∏Äùúã0/c0 > 0 is a valid density. Moreover, we have the desired marginal, i.e.,
‚à´Ô∏Å
ùúã0(ùëßùúÉ, ùëß0, ùëß1) dùëß0 = ùúãŒò,ùëç1|ùëå0,ùëå1(ùëßùúÉ, ùëß1|ùë¶0, ùë¶1) = ùê¥‚ôØ
0 ùúãŒò,ùëç1|ùë¶0,ùë¶1(ùëßùúÉ, ùëß1),
(E.18)
since ùê¥0 is the identity map on Rùëù√ó Rùëõ.
Now assume that ùúãùëóis a nonvanish-
ing density for all ùëó‚â§ùëñ(complete induction) with ùëñ> 0, and that the marginal
‚à´Ô∏Ä
ùúãùëñ(ùëßùúÉ, ùëßùëñ, ùëßùëñ+1) dùëßùëñ= ùê¥‚ôØ
ùëñùúãŒò,ùëçùëñ+1|ùë¶0:ùëñ+1(ùëßùúÉ, ùëßùëñ+1).
Under this hypothesis, the maps
(Mùëó)ùëó‚â§ùëñare well-defined, and so are ùê¥ùëñ, ùê¥ùëñ+1 since TŒò
ùëñ
= MŒò
0 ‚àò¬∑ ¬∑ ¬∑ ‚àòMŒò
ùëñ. Before
checking the integrability of ùúãùëñ+1, notice that by definition of Mùëñ(a KR rearrange-
ment), the map ùêµùëñ, given by
ùêµùëñ(ùë•ùúÉ, ùë•ùëñ+1) =
‚é°
‚é£MŒò
ùëñ(ùë•ùúÉ)
M1
ùëñ(ùë•ùúÉ, ùë•ùëñ+1)
‚é§
‚é¶,
(E.19)
pushes forward ùúÇùëãŒò,ùëãùëñ+1 to the marginal
‚à´Ô∏Ä
ùúãùëñ(ùëßùúÉ, ùëßùëñ, ùëßùëñ+1) dùëßùëñ, which equals ùê¥‚ôØ
ùëñùúãŒò,ùëçùëñ+1|ùë¶0:ùëñ+1
(inductive hypothesis), i.e., (ùêµùëñ)‚ôØùúÇùëãŒò,ùëãùëñ+1 = ùê¥‚ôØ
ùëñùúãŒò,ùëçùëñ+1|ùë¶0:ùëñ+1. In particular, it must
also be that (ùê¥ùëñ‚àòùêµùëñ)‚ôØùúÇùëãŒò,ùëãùëñ+1 = ùúãŒò,ùëçùëñ+1|ùë¶0:ùëñ+1, where ùê¥ùëñ‚àòùêµùëñcorresponds precisely
to the map ÃÉÔ∏Å
Mùëñdefined in (5.39), so that (ÃÉÔ∏Å
Mùëñ)‚ôØùúÇùëãŒò,ùëãùëñ+1 = ùúãŒò,ùëçùëñ+1|ùë¶0:ùëñ+1. Now we can
prove that cùëñ+1 < ‚àûusing the following identities:
cùëñ+1
=
‚à´Ô∏Å
ùúÇùëãŒò,ùëãùëñ+1(ùëßùúÉ, ùëßùëñ+1)
(E.20)
ÃÉÔ∏Äùúãùëñ+1(TŒò
ùëñ(ùëßùúÉ), M1
ùëñ(ùëßùúÉ, ùëßùëñ+1), ùëßùëñ+2) dùëßùúÉdùëßùëñ+1:ùëñ+2
=
‚à´Ô∏Å
(ÃÉÔ∏Å
Mùëñ)‚ôØùúÇùëãŒò,ùëãùëñ+1(ùë•ùúÉ, ùë•ùëñ+1) ùúãùëçùëñ+2|ùëçùëñ+1,Œò(ùëßùëñ+2|ùë•ùëñ+1, ùë•ùúÉ)
ùúãùëåùëñ+2|ùëçùëñ+2,Œò(ùë¶ùëñ+2|ùëßùëñ+2, ùë•ùúÉ) dùë•ùúÉdùë•ùëñ+1 dùëßùëñ+2
=
‚à´Ô∏Å
ùúãŒò,ùëçùëñ+1|ùë¶0:ùëñ+1(ùë•ùúÉ, ùë•ùëñ+1)
(E.21)
ùúãùëçùëñ+2,ùëåùëñ+2|ùëçùëñ+1,Œò(ùëßùëñ+2, ùë¶ùëñ+2|ùë•ùëñ+1, ùë•ùúÉ) dùë•ùúÉdùë•ùëñ+1 dùëßùëñ+2
=
ùúãùëåùëñ+2|ùëå0:ùëñ+1(ùë¶ùëñ+2|ùë¶0:ùëñ+1) < ‚àû,
232

where we used the change of variables:
‚é°
‚é£ùë•ùúÉ
ùë•ùëñ+1
‚é§
‚é¶=
‚é°
‚é£TŒò
ùëñ(ùëßùúÉ)
M1
ùëñ(ùëßùúÉ, ùëßùëñ+1)
‚é§
‚é¶= ÃÉÔ∏Å
Mùëñ(ùëßùúÉ, ùëßùëñ+1),
(E.22)
and the fact that (ÃÉÔ∏Å
Mùëñ)‚ôØùúÇùëãŒò,ùëãùëñ+1 = ùúãŒò,ùëçùëñ+1|ùë¶0:ùëñ+1 (induction hypothesis). (The change
of variables in (E.22) is valid for the following reason: the map ÃÉÔ∏Å
Mùëñcan be factorized as
the composition of ùëñ+1 (generalized) triangular functions, all that fit the hypothesis of
Lemma B.1, so that (E.22) should really be interpreted as a sequence of ùëñ+1 change of
variables‚Äîeach associated with one map in the composition and justified by Lemma
B.1.) Therefore ùúãùëñ+1 is a nonvanishing density. Following the same derivations as in
(E.20), it is not hard to show that ùúãùëñ+1 has also the desired marginal, i.e.,
‚à´Ô∏Å
ùúãùëñ+1(ùëßùúÉ, ùëßùëñ+1, ùëßùëñ+2) dùëßùëñ+1 = ùê¥‚ôØ
ùëñ+1 ùúãŒò,ùëçùëñ+2|ùë¶0:ùëñ+2(ùëßùúÉ, ùëßùëñ+2).
(E.23)
This argument completes the induction step and shows that not only the maps
(Mùëñ)ùëñ‚â•0 are well-defined‚Äîtogether with the maps (ùëáùëñ)ùëñ‚â•0 in (5.40)‚Äîbut also that
(ÃÉÔ∏Å
M1
ùëñ)‚ôØùúÇùëãŒò,ùëãùëñ+1 = ùúãŒò,ùëçùëñ+1|ùë¶0:ùëñ+1 for all ùëñ‚â•0 (Part 1 of the theorem).
Now we prove Part 2 of the theorem using another induction argument on ùëò‚â•0.
For the base case (ùëò= 0), notice that T0 = ùëá0 = M0, and that, by definition, M0
pushes forward ùúÇùëãŒò,ùëã0,ùëã1 to ùúã0 = ùúãŒò,ùëç0,ùëç1|ùë¶0,ùë¶1. Assume that Tùëòpushes forward
ùúÇùëãŒò,ùëã0:ùëò+1 to ùúãŒò,ùëç0:ùëò+1|ùë¶0:ùëò+1 for some ùëò> 0 (Tùëòis well-defined for all ùëòsince the maps
(ùëáùëñ)ùëñ‚â•0 in (5.40) are also well-defined), and notice that
ùúãŒò,ùëç0:ùëò+2|ùë¶0:ùëò+2 = ùúãŒò,ùëç0:ùëò+1|ùë¶0:ùëò+1
ùúãùë¶ùëò+2|ùëçùëò+2,Œò ùúãùëçùëò+2|ùëçùëò+1,Œò
ùúãùë¶ùëò+2|ùë¶0:ùëò+1
= ùúãŒò,ùëç0:ùëò+1|ùë¶0:ùëò+1
ÃÉÔ∏Äùúãùëò+1
cùëò+1
,
where we used (E.20) and the definition of the collection (ÃÉÔ∏Äùúãùëñ). Let Tùëò+1 = ùëá0‚àò¬∑ ¬∑ ¬∑‚àòùëáùëò+1
233

be defined as in Part 2 of the theorem, and observe that Tùëò+1 = ùê∂ùëò+1 ‚àòùëáùëò+1 with
ùê∂ùëò+1(ùë•ùúÉ, ùë•0:ùëò+2) =
‚é°
‚é£Tùëò(ùë•ùúÉ, ùë•0:ùëò+1)
ùë•ùëò+2
‚é§
‚é¶,
ùëáùëò+1(ùë•ùúÉ, ùë•0:ùëò+2) =
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
MŒò
ùëò+1(ùë•ùúÉ)
ùë•0
...
ùë•ùëò
M0
ùëò+1(ùë•ùúÉ, ùë•ùëò+1, ùë•ùëò+2)
M1
ùëò+1(ùë•ùúÉ, ùë•ùëò+2)
‚é§
‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é•‚é¶
.
Thus the following hold:
T‚ôØ
ùëò+1 ùúãŒò,ùëç0:ùëò+2|ùë¶0:ùëò+2
=
ùëá‚ôØ
ùëò+1
(Ô∏Ç(Ô∏Å
T‚ôØ
ùëòùúãŒò,ùëç0:ùëò+1|ùë¶0:ùëò+1
)Ô∏Å
ùúãùëò+1
ùúÇùëãŒò,ùëãùëò+1
)Ô∏Ç
(E.24)
=
ùëá‚ôØ
ùëò+1
(Ô∏Ä
ùúÇùëã0:ùëòùúãùëò+1)Ô∏Ä
=
ùúÇùëã0:ùëòM‚ôØ
ùëò+1 ùúãùëò+1 = ùúÇùëãŒò,ùëã0:ùëò+2,
where we used the fact that by Lemma B.1 (applied iteratively) it must be that
(ùê∂ùëò+1 ‚àòùëáùëò+1)‚ôØùúå= ùëá‚ôØ
ùëò+1 ùê∂‚ôØ
ùëò+1ùúåfor all densities ùúå. (Notice that ùê∂ùëò+1 is the composition
of functions which are trivial embeddings into the identity map of KR rearrangements
that couple pair of measures in M+(Rùëù√óRùëõ√óRùëõ), and thus each map in the composi-
tion satisfies the hypothesis of Lemma B.1.) Thus (Tùëò+1)‚ôØùúÇùëãŒò,ùëã0:ùëò+2 = ùúãŒò,ùëç0:ùëò+2|ùë¶0:ùëò+2,
and this concludes the induction argument and the proof of Part 2 of the theorem.
The proof of Part 3 follows from c0 = ùúãùëå0,ùëå1(ùë¶0, ùë¶1), (E.20), and (E.13).
‚ñ°
Proof of Theorem 5.5. Let us start with Part 1 of the Theorem. Let M be a map
that pushes forward ùúÇùëã1,...,ùëãùúÖto ùõø(e.g., a KR rearrangement), and define a function
ÃÇÔ∏Äùëá: Rùëõ‚ÜíRùëõas follows:
ÃÇÔ∏Äùëá(ùë•) =
‚é°
‚é¢‚é¢‚é¢‚é¢‚é¢‚é¢‚é£
M(ùë•1, . . . , ùë•ùúÖ)
ùë•ùúÖ+1
...
ùë•ùëõ
‚é§
‚é•‚é•‚é•‚é•‚é•‚é•‚é¶
.
(E.25)
234

Clearly ÃÇÔ∏Äùëápushes forward ùúÇto ùúãùëÑ= ùëÑ‚ôØùúã, so that ùëá= ùëÑ‚àòÃÇÔ∏Äùëámust be a low-rank
transport that pushes forward ùúÇto ùúã.
Now Part 2. Assume that rank(Cùúå) ‚â§ùúÖ. Let Cùúå= ùëÑ1 Œ£1 ùëÑ‚ä§
1 be a (reduced)
eigendecomposition of Cùúåwith ùëÑ1 ‚ààRùëõ√óùúÖand Œ£1 = diag(ùúé1, . . . , ùúéùúÖ) with ùúéùëó‚â•
0. Such a decomposition always exists since rank(Cùúå) ‚â§ùúÖ. By (for instance) [67,
Proposition 2.3], there exists a function ùëì: RùúÖ‚ÜíR such that r(ùë•) = ùëì(ùëÑ‚ä§
1 ùë•) for all
ùë•‚ààRùëõ. Define a rotation ùëÑas ùëÑ= [ùëÑ1|ùëÑ2], where the columns of ùëÑ2 ‚ààRùëõ√ó(ùëõ‚àíùúÖ)
are any orthonormal basis of Rùëõ‚àñspan(ùëÑ1). Notice that the columns of ùëÑ1 span the
range of Cùúå, and that r(ùëÑùë•) = ùëî(ùë•1, . . . , ùë•ùúÖ) for some ùëî: RùúÖ‚ÜíR. In particular,
r ‚àòùëÑ= log ùúÇ‚àòùëÑ‚àílog ùúã‚àòùëÑ, where ùúÇ‚àòùëÑ= ùúÇsince ùúÇ= ùí©(0, I), and where
ùúã‚àòùëÑ= ùëÑ‚ôØùúã=: ùúãùëÑsince | det ùëÑ| = 1. Simple algebra then shows that:
ùúãùëÑ(ùë•) = ùõø(ùë•1, . . . , ùë•ùúÖ) ùúÇùëãùúÖ+1:ùëõ(ùë•ùúÖ+1:ùëõ),
(E.26)
where ùõø= ùúÇùëã1:ùúÖexp(‚àíùëî) = ùúãùëÑ/ùúÇùëãùúÖ+1:ùëõcorresponds to a smooth marginal of ùúãùëÑ. This
proves one direction. Now let us assume that there exists a rotation, ùëÑ, of Rùëõsuch
that the pullback ùúãùëÑ= ùëÑ‚ôØùúãfactorizes according to (5.44). In this case, ùúãùëÑ/ùúÇ= ùëû‚àòŒ†ùúÖ,
where ùëûis some real-valued function on Rùëõ, and where Œ†ùúÖis an orthogonal projector
onto the first ùúÖcoordinate directions. Hence ùúã/ùúÇ= ùëû‚àòŒ†ùúÖ‚àòùëÑ‚ä§, and
Cùúå= ùëÑŒ†ùúÖ
(Ô∏Ç‚à´Ô∏Å
‚àálog ùëû(Œ†ùúÖùëÑ‚ä§ùë•)‚àálog ùëû(Œ†ùúÖùëÑ‚ä§ùë•)‚ä§ùúå(ùë•) dùë•
)Ô∏Ç
Œ†ùúÖùëÑ‚ä§,
(E.27)
so that rank(Cùúå) ‚â§rank(Œ†ùúÖ) = ùúÖ.
‚ñ°
Example E.1. Let ùëç= (ùëç1, ùëç2) ‚àºùúãùëçbe a latent process on R2 with a non-Gaussian
prior density given by
ùúãùëç(ùë•) ‚àùexp(‚àí1
2(ùë•2 ‚àíùë•2
1)2 ‚àí1
2ùë•2
2),
(E.28)
and let ùëåbe a real-valued random variable which represents the observed process,
modeled by ùëå= ùëç1 + ‚Ñ∞, where ‚Ñ∞‚àºùí©(0, 1) is independent of ùëç1. The likelihood
235

function, ùúãùëå|ùëç, is of the form:
ùúãùëå|ùëç(ùë¶|ùë•) ‚àùexp(‚àí1
2(ùë¶‚àíùë•1)2),
(E.29)
whereas the posterior distribution for the event {ùëå= 0} is given by
ùúãùëç|ùëå(ùë•|0) ‚àùùúãùëå|ùëç(0|ùë•) ùúãùëç(ùë•).
(E.30)
For the purpose of this example, we regard (E.30) as the target density ùúãand consider
ùúÇ= ùí©(0, I) to be the reference density. If we apply Theorem 5.5[Part 2] to the density
pair (ùúÇ, ùúã) for ùúå= ùúÇ, we get
Cùúå=
‚é°
‚é£4(1 + 5!!)
0
0
1 + 3!!
‚é§
‚é¶,
(E.31)
which is clearly a full-rank matrix, and so no dimensionality reduction is possible in
this case (ùúÖ= ùëõ= 2). However, since the likelihood function is localized (see Section
5.7.3), we know that there exists a transport map that decomposes as (5.50), where ÃÇÔ∏Äùëá
is essentially a one-dimensional map (ùúÖ= 1), and where the prior map is the unique
monotone increasing and lower triangular rearrangement that pushes forward ùúÇto ùúãùëç.
The prior map was certainly key to achieve this type of dimensionality reduction. But
what if we had used a prior map with a different sparsity pattern? For instance, an
upper rather than a lower triangular map? Let ùëápr be the monotone increasing upper
triangular transport that pushes forward ùúÇto ùúãùëç. It is immediate to verify that ùëápr
is of the form
ùëápr(ùë•) =
‚é°
‚é£ùë•1 + ùë•2
2
ùë•2
‚é§
‚é¶,
(E.32)
and that
ùëá‚ôØ
pr ùúã(ùë•) = exp(‚àí1
2(ùë•1 + ùë•2
2)2) ùúÇ(ùë•).
(E.33)
236

If we now apply Theorem 5.5 to the density pair (ùúÇ, ùëá‚ôØ
pr ùúã) for ùúå= ùúÇ, we find that
Cùúå=
‚é°
‚é£1 + 3!!
0
0
4(1 + 5!!)
‚é§
‚é¶
(E.34)
is once again a full-rank matrix, so that no dimensionality reduction like (5.50) is
possible at this stage.
Thus, the sparsity pattern of the prior map can be really
important.
237

238

Bibliography
[1] P. A. Absil, C. G. Baker, and K. A. Gallivan, A truncated-CG style
method for symmetric generalized eigenvalue problems, J. Comput. Appl. Math,
189 (2006), pp. 274‚Äì285.
[2] P. A. Absil, C. G. Baker, K. A. Gallivan, and A. Sameh, Adaptive
model trust region methods for generalized eigenvalue problems, in International
Conference on Computational Science, Springer, 2005, pp. 33‚Äì41.
[3] V. Ak√ßelik, G. Biros, O. Ghattas, J. Hill, D. Keyes, and B. van
Bloemen Waanders, Parallel algorithms for PDE-constrained optimization,
Parallel Processing for Scientific Computing, 20 (2006), p. 291.
[4] S. Amari and H. Nagaoka, Methods of Information Geometry, vol. 191,
American Mathematical Soc., 2007.
[5] E. Anderes and M. Coram, A general spline representation for non-
parametric and semiparametric density estimates using diffeomorphisms,
arXiv:1205.5314, (2012).
[6] C. Andrieu, A. Doucet, and R. Holenstein, Particle markov chain monte
carlo methods, Journal of the Royal Statistical Society: Series B (Statistical
Methodology), 72 (2010), pp. 269‚Äì342.
[7] V. Arsigny, P. Fillard, X. Pennec, and N. Ayache, Geometric means
in a novel vector space structure on symmetric positive-definite matrices, SIAM
Journal on Matrix Analysis and Applications, 29 (2007), pp. 328‚Äì347.
239

[8] S. Asmussen and P. W. Glynn, Stochastic simulation: algorithms and anal-
ysis, vol. 57, Springer Science & Business Media, 2007.
[9] C. Atkinson and A. F. S. Mitchell, Rao‚Äôs distance measure, Sankhy¬Øa:
The Indian Journal of Statistics, Series A, 43 (1981), pp. 345‚Äì365.
[10] H. Auvinen, J. M. Bardsley, H. Haario, and T. Kauranne, Large-scale
Kalman filtering using the limited memory BFGS method, Electronic Transac-
tions on Numerical Analysis, 35 (2009), pp. 217‚Äì233.
[11]
, The variational Kalman filter and an efficient implementation using lim-
ited memory BFGS, International Journal for Numerical Methods in Fluids, 64
(2010), pp. 314‚Äì335.
[12] O. Axelsson and I. Kaporin, On the sublinear and superlinear rate of con-
vergence of conjugate gradient methods, Numerical Algorithms, 25 (2000), pp. 1‚Äì
22.
[13] Z. Bai, J. Demmel, J. Dongarra, A. Ruhe, and H. van der Vorst,
Templates for the Solution of Algebraic Eigenvalue Problems:
A Practical
Guide, vol. 11, SIAM, 2000.
[14] C. G. Baker, P. A. Absil, and K. A. Gallivan, An implicit Riemannian
trust-region method for the symmetric generalized eigenproblem, in International
Conference on Computational Science, Springer, 2006, pp. 210‚Äì217.
[15] A. Barachant, S. Bonnet, M. Congedo, and C. Jutten, Classification
of covariance matrices using a Riemannian-based kernel for BCI applications,
Neurocomputing, 112 (2013), pp. 172‚Äì178.
[16] J. M. Bardsley, A. Solonen, H. Haario, and M. Laine, Randomize-
then-Optimize: A method for sampling from posterior distributions in nonlinear
inverse problems, SIAM Journal on Scientific Computing, 36 (2014), pp. A1895‚Äì
A1910.
240

[17] R. Barrett, M. W. Berry, T. F. Chan, J. Demmel, J. Donato, J. Don-
garra, V. Eijkhout, R. Pozo, C. Romine, and H. Van der Vorst,
Templates for the Solution of Linear Systems: Building Blocks for Iterative
Methods, vol. 43, SIAM, 1994.
[18] T. Bengtsson, C. Snyder, and D. Nychka, Toward a nonlinear ensemble
filter for high-dimensional systems, Journal of Geophysical Research: Atmo-
spheres, 108 (2003).
[19] T. L. Bergman, F. P. Incropera, and A. S. Lavine, Fundamentals of
Heat and Mass Transfer, John Wiley & Sons, 2011.
[20] J. M. Bernardo and A. F. Smith, Bayesian theory, 2001.
[21] D. P. Bertsekas, Dynamic programming and optimal control, vol. 1, Athena
Scientific Belmont, MA, 1995.
[22] D. P. Bertsekas and D. A. Castanon, The auction algorithm for the trans-
portation problem, Annals of Operations Research, 20 (1989), pp. 67‚Äì96.
[23] A. Beskos, D. Crisan, A. Jasra, K. Kamatani, and Y. Zhou, A stable
particle filter in high-dimensions, arXiv:1412.3501, (2014).
[24] R. Bhatia, Positive Definite Matrices, Princeton University Press, 2009.
[25]
, Matrix Analysis, vol. 169, Springer Science & Business Media, 2013.
[26] P. J. Bickel and E. Levina, Covariance regularization by thresholding, The
Annals of Statistics, (2008), pp. 2577‚Äì2604.
[27]
, Regularized estimation of large covariance matrices, The Annals of Statis-
tics, (2008), pp. 199‚Äì227.
[28] G. J. Bierman, Factorization methods for discrete sequential estimation,
Courier Corporation, 2006.
241

[29] D. Bigoni, A. Spantini, and Y. Marzouk, Adaptive construction of mea-
sure transports for Bayesian inference, NIPS workshop on Approximate Infer-
ence, (2016).
[30] D. Bigoni, A. Spantini, and Y. Marzouk, On the computation of monotone
transports, In preparation, (2016).
[31] P. Billingsley, Probability and measure, John Wiley & Sons, 2008.
[32] A. Blake, P. Kohli, and C. Rother, Markov random fields for vision and
image processing, MIT Press, 2011.
[33] E. Blayo, M. Bocquet, E. Cosme, and L. F. Cugliandolo, Advanced
data assimilation for geosciences, 2014.
[34] V. I. Bogachev, A. V. Kolesnikov, and K. V. Medvedev, Triangular
transformations of measures, Sbornik: Mathematics, 196 (2005), p. 309.
[35] S. Bonnabel and R. Sepulchre, Riemannian metric and geometric mean
for positive semidefinite matrices of fixed rank, SIAM Journal on Matrix Anal-
ysis and Applications, 31 (2009), pp. 1055‚Äì1070.
[36] L. Bottou, F. E. Curtis, and J. Nocedal, Optimization methods for large-
scale machine learning, arXiv:1606.04838, (2016).
[37] S. Boyd and L. Vandenberghe, Convex Optimization, Cambridge Univer-
sity Press, 2004.
[38] C. M. Branco, R. Ritchie, and V. Sklenicka, Mechanical behaviour of
materials at high temperature, vol. 15, Springer Science & Business Media, 1996.
[39] W. Bryc, The normal distribution:
characterizations with applications,
vol. 100, Springer Science & Business Media, 2012.
[40] T. Bui-Thanh, C. Burstedde, O. Ghattas, J. Martin, G. Stadler,
and L. Wilcox, Extreme-scale UQ for Bayesian inverse problems governed
242

by PDEs, in Proceedings of the International Conference on High Performance
Computing, Networking, Storage and Analysis, IEEE Computer Society Press,
2012, p. 3.
[41] T. Bui-Thanh and O. Ghattas, Analysis of the Hessian for inverse scatter-
ing problems: I. Inverse shape scattering of acoustic waves, Inverse Problems,
28 (2012), p. 055001.
[42] T. Bui-Thanh, O. Ghattas, and D. Higdon, Adaptive Hessian-based non-
stationary Gaussian process response surface method for probability density ap-
proximation with application to Bayesian solution of large-scale inverse prob-
lems, SIAM Journal on Scientific Computing, 34 (2012), pp. A2837‚ÄìA2871.
[43] T. Bui-Thanh, O. Ghattas, J. Martin, and G. Stadler, A computa-
tional framework for infinite-dimensional Bayesian inverse problems part I: The
linearized case, with application to global seismic inversion, SIAM Journal on
Scientific Computing, 35 (2013), pp. A2494‚ÄìA2523.
[44] D. Calvetti, Preconditioned iterative methods for linear discrete ill-posed
problems from a Bayesian inversion perspective, Journal of computational and
applied mathematics, 198 (2007), pp. 378‚Äì395.
[45] D. Calvetti, B. Lewis, and L. Reichel, On the regularizing properties of
the GMRES method, Numerische Mathematik, 91 (2002), pp. 605‚Äì625.
[46] D. Calvetti, D. McGivney, and E. Somersalo, Left and right precondi-
tioning for electrical impedance tomography with structural information, Inverse
Problems, 28 (2012), p. 055015.
[47] D. Calvetti and E. Somersalo, Priorconditioners for linear systems, In-
verse problems, 21 (2005), p. 1397.
[48] G. Carlier, A. Galichon, and F. Santambrogio, From Knothe‚Äôs trans-
port to Brenier‚Äôs map and a continuation method for optimal transport, SIAM
Journal on Mathematical Analysis, 41 (2010), pp. 2554‚Äì2576.
243

[49] B. Carlin and T. Louis, Bayesian Methods for Data Analysis, Chapman &
Hall/CRC, 3rd ed., 2009.
[50] M.-H. Chen, Q.-M. Shao, and J. G. Ibrahim, Monte Carlo methods in
Bayesian computation, Springer Science & Business Media, 2012.
[51] D. Cheng, Y. Cheng, Y. Liu, R. Peng, and S. Teng, Efficient sam-
pling for Gaussian graphical models via spectral sparsification., in Conference
on Learning Theory, 2015, pp. 364‚Äì390.
[52] M. J. Choi, V. Chandrasekaran, D. M. Malioutov, J. K. Johnson,
and A. S. Willsky, Multiscale stochastic modeling for tractable inference and
data assimilation, Computer Methods in Applied Mechanics and Engineering,
197 (2008), pp. 3492‚Äì3515.
[53] M. J. Choi, V. Chandrasekaran, and A. S. Willsky, Gaussian mul-
tiresolution models: Exploiting sparse Markov and covariance structure, IEEE
Transactions on Signal Processing, 58 (2010), pp. 1012‚Äì1024.
[54] S. T. Choi, Iterative Methods for Singular Linear Equations and Least-Squares
Problems, PhD thesis, Stanford University, 2006.
[55] N. Chopin, P. E. Jacob, and O. Papaspiliopoulos, SMC2: an efficient
algorithm for sequential analysis of state space models, Journal of the Royal
Statistical Society: Series B (Statistical Methodology), 75 (2013), pp. 397‚Äì426.
[56] A. Chorin, M. Morzfeld, and X. Tu, Implicit particle filters for data as-
similation, Communications in Applied Mathematics and Computational Sci-
ence, 5 (2010), pp. 221‚Äì240.
[57] A. J. Chorin and P. Krause, Dimensional reduction for a Bayesian fil-
ter, Proceedings of the National Academy of Sciences of the United States of
America, 101 (2004), pp. 15013‚Äì15017.
244

[58] A. J. Chorin and X. Tu, Implicit sampling for particle filters, Proceedings
of the National Academy of Sciences, 106 (2009), pp. 17249‚Äì17254.
[59] E. Chow and Y. Saad, Preconditioned Krylov subspace methods for sampling
multivariate Gaussian distributions, SIAM Journal on Scientific Computing, 36
(2014), pp. A588‚ÄìA608.
[60] O. F. Christensen, G. O. Roberts, and J. S. Rosenthal, Scaling limits
for the transient phase of local Metropolis-Hastings algorithms, Journal of the
Royal Statistical Society. Series B: Statistical Methodology, 67 (2005), pp. 253‚Äì
268.
[61] R. Christensen, Plane Answers to Complex Questions: The Theory of Linear
Models, Springer-Verlag, 1987.
[62] J. Chung and M. Chung, Computing optimal low-rank matrix approxima-
tions for image processing, in Signals, Systems and Computers, 2013 Asilomar
Conference on, IEEE, 2013, pp. 670‚Äì674.
[63]
, An efficient approach for computing optimal low-rank regularized inverse
matrices, Inverse Problems, 30 (2014), p. 114009.
[64] J. Chung, M. Chung, and D. O‚ÄôLeary, Designing optimal spectral filters for
inverse problems, SIAM Journal on Scientific Computing, 33 (2011), pp. 3132‚Äì
3152.
[65]
, Optimal filters from calibration data for image deconvolution with data
acquisition error, Journal of Mathematical Imaging and Vision, 44 (2012),
pp. 366‚Äì374.
[66] J. Chung, M. Chung, and D. P. O‚ÄôLeary, Optimal regularized low rank
inverse approximation, Linear Algebra and its Applications, 468 (2015), pp. 260
‚Äì 269.
245

[67] P. G. Constantine, E. Dow, and Q. Wang, Active subspace methods in
theory and practice: Applications to kriging surfaces, SIAM Journal on Scientific
Computing, 36 (2014), pp. A1500‚ÄìA1524.
[68] P. G. Constantine and D. Gleich, Computing active subspaces with Monte
Carlo, arXiv:1408.0545, (2014).
[69] P. G. Constantine, C. Kent, and T. Bui-Thanh, Accelerating Markov
chain Monte Carlo with active subspaces, SIAM Journal on Scientific Comput-
ing, 38 (2016), pp. A2779‚ÄìA2805.
[70] D. Crisan and A. Doucet, A survey of convergence results on particle fil-
tering methods for practitioners, IEEE Transactions on signal processing, 50
(2002), pp. 736‚Äì746.
[71] D. Crisan and J. Miguez, Nested particle filters for online parameter esti-
mation in discrete-time state-space Markov models, arXiv:1308.1883, (2013).
[72] K. Csill√©ry, M. G. B. Blum, O. E. Gaggiotti, and O. Fran√ßois, Ap-
proximate Bayesian Computation (ABC) in practice, Trends in ecology & evo-
lution, 25 (2010), pp. 410‚Äì8.
[73] T. Cui, K. J. Law, and Y. M. Marzouk, Dimension-independent likelihood-
informed MCMC, Journal of Computational Physics, 304 (2016), pp. 109‚Äì137.
[74] T. Cui, J. Martin, Y. Marzouk, A. Solonen, and A. Spantini,
Likelihood-informed dimension reduction for nonlinear inverse problems, Inverse
Problems, 30 (2014), p. 114015.
[75] J. Cullum and W. Donath, A block Lanczos algorithm for computing the q
algebraically largest eigenvalues and a corresponding eigenspace of large, sparse,
real symmetric matrices, in Decision and Control including the 13th Symposium
on Adaptive Processes, 1974 IEEE Conference on, IEEE, 1974, pp. 505‚Äì509.
246

[76] M. Dashti and A. M. Stuart, The Bayesian approach to inverse problems,
arXiv:1302.6989, (2013).
[77] F. Daum and J. Huang, Particle flow for nonlinear filters with log-homotopy,
in SPIE Defense and Security Symposium, International Society for Optics and
Photonics, 2008, pp. 696918‚Äì696918.
[78] F. Daum and J. Huang, Particle flow and Monge-Kantorovich transport, in
Information Fusion (FUSION), 2012 15th International Conference on, IEEE,
2012, pp. 135‚Äì142.
[79] P. J. Davis and P. Rabinowitz, Methods of numerical integration, Courier
Corporation, 2007.
[80] J. de Wiljes, W. Acevedo, and S. Reich, Second-order accurate ensemble
transform particle filters, arXiv:1608.08179, (2016).
[81] P. Del Moral, Feynman-Kac formulae, in Feynman-Kac Formulae:
Ge-
nealogical and Interacting Particle Systems with Applications, Springer, 2004,
pp. 47‚Äì93.
[82] P. Del Moral, A. Jasra, and Y. Zhou, Biased online parameter inference
for state-space models, Methodology and Computing in Applied Probability, 19
(2017), pp. 727‚Äì749.
[83] J. Dick, R. N. Gantner, Q. T. L. Gia, and C. Schwab, Higher or-
der Quasi-Monte Carlo integration for Bayesian estimation, arXiv:1602.07363,
(2016).
[84] J. Dick, F. Y. Kuo, and I. H. Sloan, High-dimensional integration: the
Quasi-Monte Carlo way, Acta Numerica, 22 (2013), pp. 133‚Äì288.
[85] C. R. Dietrich and G. N. Newsam, Fast and exact simulation of stationary
Gaussian processes through circulant embedding of the covariance matrix, SIAM
Journal on Scientific Computing, 18 (1997), pp. 1088‚Äì1107.
247

[86] M. P. do Carmo, Riemannian Geometry, Birkh√§user, 1992.
[87] A. Doucet and A. M. Johansen, A tutorial on particle filtering and smooth-
ing: Fifteen years later, Handbook of nonlinear filtering, 12 (2009), p. 3.
[88] J. Durbin and S. J. Koopman, Time series analysis of non-Gaussian obser-
vations based on state-space models from both classical and Bayesian perspec-
tives, Journal of the Royal Statistical Society: Series B, 62 (2000), pp. 3‚Äì56.
[89] L. Dykes and L. Reichel, Simplified GSVD computations for the solution
of linear discrete ill-posed problems, Journal of Computational and Applied
Mathematics, 255 (2014), pp. 15‚Äì27.
[90] C. Eckart and G. Young, The approximation of one matrix by another of
lower rank, Psychometrika, 1 (1936), pp. 211‚Äì218.
[91] Y. B. Erol, Y. Wu, L. Li, and S. J. Russell, A nearly-black-box online al-
gorithm for joint parameter and state estimation in temporal models., in AAAI,
2017, pp. 1861‚Äì1869.
[92] G. Evensen, The ensemble Kalman filter: Theoretical formulation and prac-
tical implementation, Ocean dynamics, 53 (2003), pp. 343‚Äì367.
[93] G. Evensen, Data Assimilation, Springer, 2007.
[94] M. Fisher, Development of a simplified Kalman filter, European Centre for
Medium-Range Weather Forecasts, 1998.
[95] R. A. Fisher, On the mathematical foundations of theoretical statistics, Philo-
sophical Transactions of the Royal Society of London. Series A, 222 (1922),
pp. 309‚Äì368.
[96] H. Flath, L. Wilcox, V. Ak√ßelik, J. Hill, B. van Bloemen Waanders,
and O. Ghattas, Fast algorithms for Bayesian uncertainty quantification in
large-scale linear inverse problems based on low-rank partial Hessian approxi-
mations, SIAM Journal on Scientific Computing, 33 (2011), pp. 407‚Äì432.
248

[97] P. T. Fletcher, C. Lu, S. M. Pizer, and S. Joshi, Principal geodesic
analysis for the study of nonlinear statistics of shape, Medical Imaging, IEEE
Transactions on, 23 (2004), pp. 995‚Äì1005.
[98] D. C. Fong and M. Saunders, LSMR: An iterative algorithm for sparse
least-squares problems, SIAM Journal on Scientific Computing, 33 (2011),
pp. 2950‚Äì2971.
[99] W. F√∂rstner and B. Moonen, A metric for covariance matrices, in
Geodesy-The Challenge of the 3rd Millennium, Springer, 2003, pp. 299‚Äì309.
[100] C. Fox and A. Parker, Convergence in variance of Chebyshev accelerated
Gibbs samplers, SIAM Journal on Scientific Computing, 36 (2014), pp. A124‚Äì
A147.
[101] D. H. Fremlin, Measure theory, vol. 4, Torres Fremlin, 2000.
[102] S. Friedland and A. Torokhti, Generalized rank-constrained matrix ap-
proximations, SIAM Journal on Matrix Analysis and Applications, 29 (2007),
pp. 656‚Äì659.
[103] G. Gaspari and S. E. Cohn, Construction of correlation functions in two
and three dimensions, Quarterly Journal of the Royal Meteorological Society,
125 (1999), pp. 723‚Äì757.
[104] A. Gelman, J. B. Carlin, H. S. Stern, and D. B. Rubin, Bayesian Data
Analysis, Chapman and Hall, 2 ed., 2003.
[105] A. George and J. W. Liu, The evolution of the minimum degree ordering
algorithm, SIAM Review, 31 (1989), pp. 1‚Äì19.
[106] M. Girolami and B. Calderhead, Riemann manifold Langevin and Hamil-
tonian Monte Carlo methods, Journal of the Royal Statistical Society: Series B
(Statistical Methodology), 73 (2011), pp. 123‚Äì214.
249

[107] P. Glasserman, Gradient estimation via perturbation analysis, Springer Sci-
ence & Business Media, 1991.
[108] P. W. Glynn, Likelihood ratio gradient estimation for stochastic systems, Com-
munications of the ACM, 33 (1990), pp. 75‚Äì84.
[109] S. J. Godsill, A. Doucet, and M. West, Monte Carlo smoothing for non-
linear time series, Journal of the American Statistical Association, 99 (2004),
pp. 156‚Äì168.
[110] G. Golub and C. Van Loan, Matrix Computations, vol. 3, JHU Press, 2012.
[111] G. H. Golub and R. Underwood, The block Lanczos method for computing
eigenvalues, Mathematical software, 3 (1977), pp. 361‚Äì377.
[112] G. H. Golub and Q. Ye, An inverse free preconditioned Krylov subspace
method for symmetric generalized eigenvalue problems, SIAM Journal on Scien-
tific Computing, 24 (2002), pp. 312‚Äì334.
[113] G. Guglielmini and C. Pisoni, Elementi di trasmissione del calore, Veschi,
1990.
[114] H. Haario, M. Laine, M. Lehtinen, E. Saksman, and J. Tamminen,
Markov chain Monte Carlo methods for high dimensional inversion in remote
sensing, Journal of the Royal Statistical Society: Series B (Statistical Method-
ology), 66 (2004), pp. 591‚Äì607.
[115] W. Hackbusch, Tensor spaces and numerical tensor calculus, vol. 42, Springer
Science & Business Media, 2012.
[116] N. Halko, P. Martinsson, and J. Tropp, Finding structure with random-
ness: Probabilistic algorithms for constructing approximate matrix decomposi-
tions, SIAM Review, 53 (2011), pp. 217‚Äì288.
250

[117] T. M. Hamill, J. S. Whitaker, and C. Snyder, Distance-dependent fil-
tering of background error covariance estimates in an ensemble Kalman filter,
Monthly Weather Review, 129 (2001), pp. 2776‚Äì2790.
[118] J. M. Hammersley and P. Clifford, Markov fields on finite graphs and
lattices, (1971).
[119] M. Hanke, Conjugate gradient type methods for ill-posed problems, vol. 327,
CRC Press, 1995.
[120] M. Hanke and P. C. Hansen, Regularization methods for large-scale prob-
lems, Surv. Math. Ind, 3 (1993), pp. 253‚Äì315.
[121] P. C. Hansen, Regularization, GSVD and truncated GSVD, BIT Numerical
Mathematics, 29 (1989), pp. 491‚Äì504.
[122]
, Rank-Deficient and Discrete Ill-Posed Problems: Numerical Aspects of
Linear Inversion, vol. 4, SIAM, 1998.
[123] T. Hastie, R. Tibshirani, J. Friedman, and J. Franklin, The elements
of statistical learning: data mining, inference and prediction, The Mathematical
Intelligencer, 27 (2005), pp. 83‚Äì85.
[124] T. Hastie, R. Tibshirani, and M. Wainwright, Statistical learning with
sparsity: the lasso and generalizations, CRC press, 2015.
[125] J. Heikkinen, Statistical inversion theory in X-ray tomography, Master‚Äôs the-
sis, Lappeenranta University of Technology, Finland, 2008.
[126] J. Heng, A. Doucet, and Y. Pokern, Gibbs flow for approximate transport
with applications to Bayesian computation, arXiv:1509.08787, (2015).
[127] M. R. Hestenes, Pseudoinversus and conjugate gradients, Communications
of the ACM, 18 (1975), pp. 40‚Äì43.
[128] M. R. Hestenes and E. Stiefel, Methods of conjugate gradients for solving
linear systems, vol. 49, National Bureau of Standards Washington, DC, 1952.
251

[129] D. Higdon, C. S. Reese, J. D. Moulton, J. A. Vrugt, and C. Fox,
Posterior exploration for computationally intensive forward models, Handbook
of Markov chain Monte Carlo, CHAPMAN & HALL/CRC, (2011), pp. 401‚Äì418.
[130] Y. C. Ho and X. Cao, Perturbation analysis and optimization of queueing
networks, Journal of Optimization Theory and Applications, 40 (1983), pp. 559‚Äì
582.
[131] L. Homa, D. Calvetti, A. Hoover, and E. Somersalo, Bayesian pre-
conditioned CGLS for source separation in MEG time series, SIAM Journal on
Scientific Computing, 35 (2013), pp. B778‚ÄìB798.
[132] I. Horev, F. Yger, and M. Sugiyama, Geometry-aware principal compo-
nent analysis for symmetric positive definite matrices, in JMLR: Workshop and
Conference Proceedings, 2015.
[133] Y. Hua and W. Liu, Generalized Karhunen-Lo√®ve transform, IEEE Signal
Processing Letters, 5 (1998), pp. 141‚Äì142.
[134] A. Hyv√§rinen, Estimation of non-normalized statistical models by score
matching, Journal of Machine Learning Research, 6 (2005), pp. 695‚Äì709.
[135] T. Isaac, N. Petra, G. Stadler, and O. Ghattas, Scalable and efficient
algorithms for the propagation of uncertainty from data through inference to
prediction for large-scale problems, with application to flow of the antarctic ice
sheet, Journal of Computational Physics, 296 (2015), pp. 348‚Äì368.
[136] P. E. Jacob, Sequential Bayesian inference for implicit hidden Markov models
and current limitations, ESAIM: Proceedings and Surveys, 51 (2015), pp. 24‚Äì48.
[137] W. James and C. Stein, Estimation with quadratic loss, in Proceedings of
the Fourth Berkeley Symposium on Mathematical Statistics and Probability,
vol. 1, 1961, pp. 361‚Äì379.
252

[138] E. T. Jaynes, Probability theory: The logic of science, Cambridge University
Press, 2003.
[139] S. T. Jensen. Private communication, 1976. (As cited in Atkinson, Mitchell,
‚ÄúRao‚Äôs Distance Measure,‚Äù Sankhy¬Øa: The Indian Journal of Statistics, Series A,
43 (1981), pp. 345‚Äì365.).
[140] V. Jog and P. Loh, On model misspecification and KL separation for Gaus-
sian graphical models, in IEEE International Symposium on Information The-
ory, 2015, pp. 1174‚Äì1178.
[141] J. K. Johnson and A. S. Willsky, A recursive model-reduction method for
approximate inference in Gaussian Markov random fields, IEEE Transactions
on Image Processing, 17 (2008), pp. 70‚Äì83.
[142] M. Kac, On a characterization of the normal distribution, American Journal
of Mathematics, 61 (1939), pp. 726‚Äì728.
[143] W. Kahan and B. N. Parlett, How far should you go with the Lanczos
process., tech. rep., DTIC Document, 1978.
[144] J. Kaipio and E. Somersalo, Statistical and Computational Inverse Prob-
lems, vol. 160, Springer, 2005.
[145] J. Kaipio and E. Somersalo, Statistical inverse problems: Discretization,
model reduction and inverse crimes, Journal of Computational and Applied
Mathematics, 198 (2007), pp. 493‚Äì504.
[146] R. E. Kalman, A new approach to linear filtering and prediction problems,
Journal of Fluids Engineering, 82 (1960), pp. 35‚Äì45.
[147] S. Kaniel, Estimates for some computational techniques in linear algebra,
Mathematics of Computation, (1966), pp. 369‚Äì378.
253

[148] N. Kantas, A. Doucet, S. S. Singh, J. Maciejowski, and N. Chopin,
On particle methods for parameter estimation in state-space models, Statistical
Science, 30 (2015), pp. 328‚Äì351.
[149] S. Kim, N. Shephard, and S. Chib, Stochastic volatility: likelihood infer-
ence and comparison with ARCH models, The Review of Economic Studies, 65
(1998), pp. 361‚Äì393.
[150] P. Kirchgessner, L. Nerger, and A. Bunse-Gerstner, On the choice
of an optimal localization radius in ensemble Kalman filter methods, Monthly
Weather Review, 142 (2014), pp. 2165‚Äì2175.
[151] G. Kitagawa, Non-Gaussian state-space modeling of nonstationary time se-
ries, Journal of the American Statistical Association, 82 (1987), pp. 1032‚Äì1041.
[152]
, A self-organizing state-space model, Journal of the American Statistical
Association, (1998), pp. 1203‚Äì1215.
[153] A. Klinvex, F. Saied, and A. Sameh, Parallel implementations of the trace
minimization scheme TraceMIN for the sparse symmetric eigenvalue problem,
Computers & Mathematics with Applications, 65 (2013), pp. 460‚Äì468.
[154] H. Knothe et al., Contributions to the theory of convex bodies., The Michigan
Mathematical Journal, 4 (1957), pp. 39‚Äì52.
[155] D. Koller and R. Fratkina, Using learning for approximation in stochastic
processes., in ICML, 1998, pp. 287‚Äì295.
[156] D. Koller and N. Friedman, Probabilistic graphical models: principles and
techniques, MIT press, 2009.
[157] D. Kressner, M. M. Pandur, and M. Shao, An indefinite variant of
LOBPCG for definite matrix pencils, Numerical Algorithms, 66 (2014), pp. 681‚Äì
703.
254

[158] H. Kushner and G. G. Yin, Stochastic approximation and recursive algo-
rithms and applications, vol. 35, Springer Science & Business Media, 2003.
[159] C. Lanczos, An iteration method for the solution of the eigenvalue problem of
linear differential and integral operators, United States Governm. Press Office,
1950.
[160] V. Laparra, G. Camps-Valls, and J. Malo, Iterative Gaussianization:
from ICA to random rotations, IEEE transactions on neural networks, 22
(2011), pp. 537‚Äì549.
[161] S. L. Lauritzen, Graphical models, Oxford University Press, 1996.
[162] K. Law, A. Stuart, and K. Zygalakis, Data Assimilation: A Mathematical
Introduction, vol. 62, Springer, 2015.
[163] E. Lehmann and G. Casella, Theory of Point Estimation, Springer-Verlag,
1998.
[164] R. B. Lehoucq, D. C. Sorensen, and C. Yang, ARPACK users‚Äô guide: so-
lution of large-scale eigenvalue problems with implicitly restarted Arnoldi meth-
ods, vol. 6, SIAM, 1998.
[165] A. Lewis, Derivatives of spectral functions, Mathematics of Operations Re-
search, 21 (1996), pp. 576‚Äì588.
[166] W. Li and O. A. Cirpka, Efficient geostatistical inverse methods for struc-
tured and unstructured grids, Water Resources Research, 42 (2006), p. W06402.
[167]
, Efficient geostatistical inverse methods for structured and unstructured
grids, Water Resources Research, 42 (2006).
[168] C. Lieberman,
K. Fidkowski,
K. Willcox,
and B. van Bloe-
men Waanders, Hessian-based model reduction: large-scale inversion and
prediction, International Journal for Numerical Methods in Fluids, 71 (2013),
pp. 135‚Äì150.
255

[169] C. Lieberman and K. Willcox, Goal-oriented inference: approach, linear
theory, and application to advection diffusion, SIAM Journal on Scientific Com-
puting, 34 (2012), pp. A1880‚ÄìA1904.
[170]
, Nonlinear goal-oriented Bayesian inference: application to carbon capture
and storage, SIAM Journal on Scientific Computing, 36 (2014), pp. B427‚ÄìB449.
[171] C. Lieberman, K. Willcox, and O. Ghattas, Parameter and state model
reduction for large-scale statistical inverse problems, SIAM Journal on Scientific
Computing, 32 (2010), pp. 2523‚Äì2542.
[172] J. Liesen and P. Tich`y, Convergence analysis of Krylov subspace methods,
GAMM-Mitteilungen, 27 (2004), pp. 153‚Äì173.
[173] L. Lin, M. Drton, and A. Shojaie, High-dimensional inference of graphical
models using regularized score matching, arXiv:1507.00433, (2015).
[174] F. Lindgren, H. Rue, and J. Lindstr√∂m, An explicit link between Gaussian
fields and Gaussian Markov random fields: the stochastic partial differential
equation approach, Journal of the Royal Statistical Society: Series B, 73 (2011),
pp. 423‚Äì498.
[175] D. Lindley and A. Smith, Bayes estimates for the linear model, Journal of
the Royal Statistical Society. Series B, (1972), pp. 1‚Äì41.
[176] D. C. Liu and J. Nocedal, On the limited memory BFGS method for large
scale optimization, Mathematical programming, 45 (1989), pp. 503‚Äì528.
[177] J. Liu and M. West, Combined parameter and state estimation in simulation-
based filtering, in Sequential Monte Carlo methods in practice, Springer, 2001,
pp. 197‚Äì223.
[178] Q. Liu and D. Wang, Stein Variational Gradient Descent: A general purpose
Bayesian inference algorithm, in Advances in Neural Information Processing
Systems, 2016, pp. 2370‚Äì2378.
256

[179] Y. Liu, V. Chandrasekaran, A. Anandkumar, and A. S. Willsky,
Feedback message passing for inference in Gaussian graphical models, IEEE
Transactions on Signal Processing, 60 (2012), pp. 4135‚Äì4150.
[180] E. N. Lorenz, Predictability: A problem partly solved, in Proc. Seminar on
predictability, vol. 1, 1996.
[181] P. C. Mahalanobis, On the generalized distance in statistics, Proceedings of
the National Institute of Sciences (Calcutta), 2 (1936), pp. 49‚Äì55.
[182] A. J. Majda and J. Harlim, Filtering complex turbulent systems, Cambridge
University Press, 2012.
[183] A. J. Majda, D. Qi, and T. P. Sapsis, Blended particle filters for large-
dimensional chaotic dynamical systems, Proceedings of the National Academy
of Sciences, 111 (2014), pp. 7511‚Äì7516.
[184] J. M. Marin, P. Pudlo, C. P. Robert, and R. J. Ryder, Approxi-
mate Bayesian computational methods, Statistics and Computing, 22 (2012),
pp. 1167‚Äì1180.
[185] G. Marsaglia and W. W. Tsang, The Ziggurat method for generating ran-
dom variables, Journal of Statistical Software, 5 (2000), pp. 1‚Äì7.
[186] J. Martin, L. Wilcox, C. Burstedde, and O. Ghattas, A stochastic
Newton MCMC method for large-scale statistical inverse problems with appli-
cation to seismic inversion, SIAM Journal on Scientific Computing, 34 (2012),
pp. A1460‚ÄìA1487.
[187] Y. Marzouk, T. Moselhy, M. Parno, and A. Spantini, Sampling via
measure transport: An introduction, in Handbook of Uncertainty Quantifica-
tion, R. Ghanem, D. Higdon, and H. Owhadi, editors, Springer, 2016.
[188] Y. Marzouk and H. Najm, Dimensionality reduction and polynomial chaos
acceleration of Bayesian inference in inverse problems, Journal of Computa-
tional Physics, 228 (2009), pp. 1862‚Äì1902.
257

[189] N. Meinshausen and P. B√ºhlmann, High-dimensional graphs and variable
selection with the lasso, The annals of statistics, (2006), pp. 1436‚Äì1462.
[190] X. Meng, M. A. Saunders, and M. W. Mahoney, LSRN: A parallel it-
erative solver for strongly over-or underdetermined systems, SIAM Journal on
Scientific Computing, 36 (2014), pp. C95‚ÄìC118.
[191] X. Meng and S. Schilling, Warp bridge sampling, Journal of Computational
and Graphical Statistics, 11 (2002), pp. 552‚Äì586.
[192] M. Moakher and M. Z√©ra√Ø, The Riemannian geometry of the space of
positive-definite matrices and its application to the regularization of positive-
definite matrix-valued data, Journal of Mathematical Imaging and Vision, 40
(2011), pp. 171‚Äì187.
[193] J. M√∏ller, A. R. Syversveen, and R. Waagepetersen, Log Gaussian
Cox Processes, Scandinavian Journal of Statistics, 25 (1998), pp. 451‚Äì482.
[194] E. H. Moore, On the reciprocal of the general algebraic matrix, Bulletin of the
American Mathematical Society, 26, pp. 394‚Äì395.
[195] M. Morzfeld, X. Tu, J. Wilkening, and A. Chorin, Parameter es-
timation by implicit sampling, Communications in Applied Mathematics and
Computational Science, 10 (2015), pp. 205‚Äì225.
[196] T. Moselhy and Y. Marzouk, Bayesian inference with optimal maps, Jour-
nal of Computational Physics, 231 (2012), pp. 7815‚Äì7850.
[197] J. B. Nagel and B. Sudret, Spectral likelihood expansions for Bayesian
inference, Journal of Computational Physics, 309 (2016), pp. 267‚Äì294.
[198] J. Nocedal, Updating quasi-Newton matrices with limited storage, Mathemat-
ics of computation, 35 (1980), pp. 773‚Äì782.
[199] B. Oksendal, Stochastic differential equations: an introduction with applica-
tions, Springer Science & Business Media, 2013.
258

[200] D. P. O‚ÄôLeary, The block conjugate gradient algorithm and related methods,
Linear Algebra and its Applications, 29 (1980), pp. 293‚Äì322.
[201] D. S. Oliver, Metropolized randomized maximum likelihood for sampling from
multimodal distributions, arXiv:1507.08563, (2015).
[202] I. V. Oseledets, Tensor-train decomposition, SIAM Journal on Scientific
Computing, 33 (2011), pp. 2295‚Äì2317.
[203] C. C. Paige, The computation of eigenvalues and eigenvectors of very large
sparse matrices., PhD thesis, University of London, 1971.
[204]
, Computational variants of the Lanczos method for the eigenproblem, IMA
Journal of Applied Mathematics, 10 (1972), pp. 373‚Äì381.
[205]
, Error analysis of the Lanczos algorithm for tridiagonalizing a symmetric
matrix, IMA Journal of Applied Mathematics, 18 (1976), pp. 341‚Äì349.
[206] C. C. Paige and M. A. Saunders, Towards a generalized singular value
decomposition, SIAM Journal on Numerical Analysis, 18 (1981), pp. 398‚Äì405.
[207]
, Algorithm 583: LSQR: Sparse linear equations and least squares problems,
ACM Transactions on Mathematical Software (TOMS), 8 (1982), pp. 195‚Äì209.
[208]
, LSQR: An algorithm for sparse linear equations and sparse least squares,
ACM Transactions on Mathematical Software (TOMS), 8 (1982), pp. 43‚Äì71.
[209] L. Pardo, Statistical Inference Based on Divergence Measures, CRC Press,
2005.
[210] P. Park and T. Kailath, New square-root smoothing algorithms, IEEE
Transactions on Automatic Control, 41 (1996), pp. 727‚Äì732.
[211] A. Parker and C. Fox, Sampling Gaussian distributions in Krylov spaces
with conjugate gradients, SIAM Journal on Scientific Computing, 34 (2012),
pp. B312‚ÄìB334.
259

[212] M. Parno, Transport maps for accelerated Bayesian computation, PhD thesis,
Massachusetts Institute of Technology, 2015.
[213] M. Parno and Y. Marzouk, Transport map accelerated Markov chain Monte
Carlo, arXiv:1412.5492, (2014).
[214] M. Parno, T. Moselhy, and Y. Marzouk, A multiscale strategy for
Bayesian inference using transport maps, SIAM/ASA Journal on Uncertainty
Quantification, 4 (2016), pp. 1160‚Äì1190.
[215] X. Pennec, P. Fillard, and N. Ayache, A Riemannian framework for
tensor computing, International Journal of Computer Vision, 66 (2006), pp. 41‚Äì
66.
[216] R. Penrose, A generalized inverse for matrices, in Mathematical Proceedings
of the Cambridge Philosophical Society, vol. 51, Cambridge Univ Press, 1955,
pp. 406‚Äì413.
[217] E. A. Pnevmatikakis, K. R. Rad, J. Huggins, and L. Paninski, Fast
Kalman filtering and forward‚Äìbackward smoothing via a low-rank perturba-
tive approach, Journal of Computational and Graphical Statistics, 23 (2014),
pp. 316‚Äì339.
[218] N. G. Polson, J. R. Stroud, and P. M√ºller, Practical filtering with
sequential parameter learning, Journal of the Royal Statistical Society: Series
B, 70 (2008), pp. 413‚Äì428.
[219] J. Poterjoy, A localized particle filter for high-dimensional nonlinear systems,
Monthly Weather Review, 144 (2016), pp. 59‚Äì76.
[220] A. Quarteroni and A. Valli, Numerical Approximation of Partial Differ-
ential Equations, vol. 23, Springer Science & Business Media, 2008.
[221] J. Ramsay, Estimating smooth monotone functions, Journal of the Royal Sta-
tistical Society. Series B, Statistical Methodology, (1998), pp. 365‚Äì375.
260

[222] R. Ranganath, S. Gerrish, and D. M. Blei, Black box variational infer-
ence, in Artificial Intelligence and Statistics, 2014, pp. 814‚Äì822.
[223] C. R. Rao, Information and the accuracy attainable in the estimation of sta-
tistical parameters, Bulletin of the Calcutta Mathematical Society, 37 (1945),
pp. 81‚Äì89.
[224]
, On the distance between two populations, Sankhya, 9 (1949), pp. 246‚Äì248.
[225]
, Differential metrics in probability spaces, Differential geometry in statis-
tical inference, 10 (1987), pp. 217‚Äì240.
[226] H. E. Rauch, C. Striebel, and F. Tung, Maximum likelihood estimates of
linear dynamic systems, AIAA Journal, 3 (1965), pp. 1445‚Äì1450.
[227] P. Rebeschini, R. Van Handel, et al., Can local particle filters beat
the curse of dimensionality?, The Annals of Applied Probability, 25 (2015),
pp. 2809‚Äì2866.
[228] S. Reich, A nonparametric ensemble transform method for Bayesian inference,
SIAM Journal on Scientific Computing, 35 (2013), pp. A2013‚ÄìA2024.
[229] S. Reich and C. Cotter, Ensemble filter techniques for intermittent data
assimilation, Large Scale Inverse Problems. Computational Methods and Ap-
plications in the Earth Sciences, 13 (2013), pp. 91‚Äì134.
[230]
, Probabilistic Forecasting and Bayesian Data Assimilation, Cambridge
University Press, 2015.
[231] D. J. Rezende and S. Mohamed, Variational inference with normalizing
flows, arXiv:1505.05770, (2015).
[232] C. Robert, The Bayesian choice: from decision-theoretic foundations to com-
putational implementation, Springer Science & Business Media, 2007.
[233] C. Robert and G. Casella, Monte Carlo statistical methods, Springer Sci-
ence & Business Media, 2013.
261

[234] M. Rosenblatt, Remarks on a multivariate transformation, The annals of
mathematical statistics, (1952), pp. 470‚Äì472.
[235] W. Rudin, Real and complex analysis, Tata McGraw-Hill Education, 1987.
[236] H. Rue and L. Held, Gaussian Markov random fields: theory and applica-
tions, CRC Press, 2005.
[237] H. Rue, S. Martino, and N. Chopin, Approximate Bayesian inference for
latent Gaussian models by using integrated nested Laplace approximations, Jour-
nal of the Royal Statistical Society: Series B, 71 (2009), pp. 319‚Äì392.
[238] T. M. Russi, Uncertainty quantification with experimental data and complex
system models, PhD thesis, UC Berkeley, 2010.
[239] Y. Saad, On the rates of convergence of the Lanczos and the block-Lanczos
methods, SIAM Journal on Numerical Analysis, 17 (1980), pp. 687‚Äì706.
[240]
, Iterative Methods for Sparse Linear Systems, SIAM, 2003.
[241] A. K. Saibaba, J. Lee, and P. K. Kitanidis, Randomized algorithms
for generalized Hermitian eigenvalue problems with application to comput-
ing Karhunen‚ÄìLo√®ve expansion, Numerical Linear Algebra with Applications,
(2015).
[242] A. M. Samarov, Exploring regression structure using nonparametric func-
tional estimation, Journal of the American Statistical Association, 88 (1993),
pp. 836‚Äì847.
[243] A. H. Sameh and J. A. Wisniewski, A trace minimization algorithm for
the generalized eigenvalue problem, SIAM Journal on Numerical Analysis, 19
(1982), pp. 1243‚Äì1259.
[244] F. Santambrogio, Optimal Transport for Applied Mathematicians, vol. 87,
Springer, 2015.
262

[245] S. S√§rkk√§, Bayesian filtering and smoothing, no. 3, Cambridge University
Press, 2013.
[246] C. Schillings and C. Schwab, Sparse, adaptive Smolyak quadratures for
Bayesian inverse problems, Inverse Problems, 29 (2013), p. 065011.
[247]
, Scaling limits in computational Bayesian inversion, Research Report No.
2014-26, ETH-Z√ºrich, (2014).
[248] M. K. Schneider and A. S. Willsky, A Krylov subspace method for co-
variance approximation and simulation of random processes and fields, Multi-
dimensional Systems and Signal Processing, 14 (2003), pp. 295‚Äì318.
[249] A. Shapiro, Sample Average Approximation, Springer US, Boston, MA, 2013,
pp. 1350‚Äì1355.
[250] L. T. Skovgaard, A Riemannian geometry of the multivariate normal model,
Scandinavian Journal of Statistics, (1984), pp. 211‚Äì223.
[251] A. Smith, A. Doucet, N. de Freitas, and N. Gordon, Sequential Monte
Carlo methods in practice, Springer Science & Business Media, 2013.
[252] S. T. Smith, Covariance, subspace, and intrinsic Cram√©r-Rao bounds, Signal
Processing, IEEE Transactions on, 53 (2005), pp. 1610‚Äì1630.
[253] A. Solonen, T. Cui, J. Hakkarainen, and Y. Marzouk, On dimension
reduction in Gaussian filters, Inverse Problems, 32 (2016), p. 045003.
[254] A. Solonen, H. Haario, J. Hakkarainen, H. Auvinen, I. Amour, and
T. Kauranne, Variational ensemble Kalman filtering using limited memory
BFGS, Electronic Transactions on Numerical Analysis, 39 (2012), pp. 271‚Äì285.
[255] S. Sommer, F. Lauze, S. Hauberg, and M. Nielsen, Manifold valued
statistics, exact principal geodesic analysis and the effect of linear approxima-
tions, in Computer Vision‚ÄìECCV 2010, Springer, 2010, pp. 43‚Äì56.
263

[256] D. Sondermann, Best approximate solutions to matrix equations under rank
restrictions, Statistical Papers, 27 (1986), pp. 57‚Äì66.
[257] J. C. Spall, Introduction to stochastic search and optimization: estimation,
simulation, and control, vol. 65, John Wiley & Sons, 2005.
[258] A. Spantini, D. Bigoni, and Y. Marzouk, Variational inference via de-
composable transports: algorithms for Bayesian filtering and smoothing, NIPS
workshop on Approximate Inference, (2016).
[259]
, Inference via low-dimensional couplings, arXiv:1703.06131, (2017).
[260] A. Spantini, T. Cui, K. Willcox, L. Tenorio, and Y. M. Mar-
zouk, Goal-oriented optimal approximations of Bayesian linear inverse prob-
lems, SIAM Journal on Scientific Computing, in press (2017).
[261] A. Spantini, A. Solonen, T. Cui, J. Martin, L. Tenorio, and Y. Mar-
zouk, Optimal low-rank approximations of Bayesian linear inverse problems,
SIAM Journal on Scientific Computing, 37 (2015), pp. A2451‚ÄìA2487.
[262] M. Spivak, Calculus on manifolds, vol. 1, WA Benjamin New York, 1965.
[263] F. Stavropoulou and J. M√ºller, Parametrization of random vectors in
polynomial chaos expansions via optimal transportation, SIAM Journal on Sci-
entific Computing, 37 (2015), pp. A2535‚ÄìA2557.
[264] G. Stewart, The efficient generation of random orthogonal matrices with an
application to condition estimators, SIAM Journal on Numerical Analysis, 17
(1980), pp. 403‚Äì409.
[265] A. M. Stuart, Inverse problems: a Bayesian perspective, Acta Numerica, 19
(2010), pp. 451‚Äì559.
[266] E. Tabak and C. V. Turner, A family of nonparametric density estima-
tion algorithms, Communications on Pure and Applied Mathematics, 66 (2013),
pp. 145‚Äì164.
264

[267] T. Tao, An introduction to measure theory, vol. 126, American Mathematical
Soc., 2011.
[268] A. Tarantola, Inverse problem theory and methods for model parameter es-
timation, SIAM, 2005.
[269] A. Tikhonov, Solution of incorrectly formulated problems and the regulariza-
tion method, in Soviet Math. Dokl., vol. 5, 1963, pp. 1035‚Äì1038.
[270] C. F. Van Loan, Generalizing the singular value decomposition, SIAM Journal
on Numerical Analysis, 13 (1976), pp. 76‚Äì83.
[271] C. Villani, Optimal transport: old and new, vol. 338, Springer Science &
Business Media, 2008.
[272] L. Wang and X. Meng, Warp bridge sampling:
the next generation,
arXiv:1609.07690, (2016).
[273] Z. Wang, J. M. Bardsley, A. Solonen, T. Cui, and Y. M. Marzouk,
Bayesian inverse problems with ùëô1 priors: a Randomize-then-Optimize approach,
SIAM Journal on Scientific Computing, in press (2017).
[274] D. J. Wilkinson, Stochastic modelling for systems biology, CRC press, 2011.
[275] A. T. A. Wood and G. Chan, Simulation of stationary Gaussian processes
in [0, 1]ùëë, Journal of Computational and Graphical Statistics, 3 (1994), pp. 409‚Äì
432.
[276] S. J. Wright and J. Nocedal, Numerical Optimization, vol. 2, Springer
New York, 1999.
[277] D. Xiu, Numerical methods for stochastic computations: a spectral method ap-
proach, Princeton University Press, 2010.
[278] T. Yang, P. G. Mehta, and S. P. Meyn, Feedback particle filter, IEEE
transactions on Automatic control, 58 (2013), pp. 2465‚Äì2480.
265

[279] M. Yannakakis, Computing the minimum fill-in is NP-complete, SIAM Jour-
nal on Algebraic Discrete Methods, 2 (1981), pp. 77‚Äì79.
[280] M. Yuan and Y. Lin, Model selection and estimation in the Gaussian graph-
ical model, Biometrika, 94 (2007), pp. 19‚Äì35.
[281] Y. Yue and P. L. Speckman, Nonstationary spatial Gaussian Markov ran-
dom fields, Journal of Computational and Graphical Statistics, 19 (2010).
[282] O. Zham, Dimension reduction of nonlinear Bayesian inverse problem, In
preparation, (2017).
266

