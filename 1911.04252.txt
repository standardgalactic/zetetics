Self-training with Noisy Student improves ImageNet classiﬁcation
Qizhe Xie∗1, Minh-Thang Luong1, Eduard Hovy2, Quoc V. Le1
1Google Research, Brain Team, 2Carnegie Mellon University
{qizhex, thangluong, qvl}@google.com, hovy@cmu.edu
Abstract
We present Noisy Student Training, a semi-supervised
learning approach that works well even when labeled data
is abundant. Noisy Student Training achieves 88.4% top-
1 accuracy on ImageNet, which is 2.0% better than the
state-of-the-art model that requires 3.5B weakly labeled
Instagram images.
On robustness test sets, it improves
ImageNet-A top-1 accuracy from 61.0% to 83.7%, reduces
ImageNet-C mean corruption error from 45.7 to 28.3, and
reduces ImageNet-P mean ﬂip rate from 27.8 to 12.2.
Noisy Student Training extends the idea of self-training
and distillation with the use of equal-or-larger student mod-
els and noise added to the student during learning. On Im-
ageNet, we ﬁrst train an EfﬁcientNet model on labeled im-
ages and use it as a teacher to generate pseudo labels for
300M unlabeled images. We then train a larger Efﬁcient-
Net as a student model on the combination of labeled and
pseudo labeled images. We iterate this process by putting
back the student as the teacher. During the learning of the
student, we inject noise such as dropout, stochastic depth,
and data augmentation via RandAugment to the student so
that the student generalizes better than the teacher.1
1. Introduction
Deep learning has shown remarkable successes in image
recognition in recent years [45, 80, 75, 30, 83]. However
state-of-the-art (SOTA) vision models are still trained with
supervised learning which requires a large corpus of labeled
images to work well. By showing the models only labeled
images, we limit ourselves from making use of unlabeled
images available in much larger quantities to improve accu-
racy and robustness of SOTA models.
Here, we use unlabeled images to improve the SOTA Im-
ageNet accuracy and show that the accuracy gain has an out-
∗This work was conducted at Google.
1Models are available at https://github.com/tensorflow/
tpu/tree/master/models/official/efficientnet.
Code
is
available
at
https://github.com/google-research/
noisystudent.
sized impact on robustness (out-of-distribution generaliza-
tion). For this purpose, we use a much larger corpus of un-
labeled images, where a large fraction of images do not be-
long to ImageNet training set distribution (i.e., they do not
belong to any category in ImageNet). We train our model
with Noisy Student Training, a semi-supervised learning
approach, which has three main steps: (1) train a teacher
model on labeled images, (2) use the teacher to generate
pseudo labels on unlabeled images, and (3) train a student
model on the combination of labeled images and pseudo la-
beled images. We iterate this algorithm a few times by treat-
ing the student as a teacher to relabel the unlabeled data and
training a new student.
Noisy Student Training improves self-training and distil-
lation in two ways. First, it makes the student larger than, or
at least equal to, the teacher so the student can better learn
from a larger dataset. Second, it adds noise to the student so
the noised student is forced to learn harder from the pseudo
labels. To noise the student, we use input noise such as Ran-
dAugment data augmentation [18] and model noise such as
dropout [76] and stochastic depth [37] during training.
Using Noisy Student Training, together with 300M un-
labeled images, we improve EfﬁcientNet’s [83] ImageNet
top-1 accuracy to 88.4%. This accuracy is 2.0% better than
the previous SOTA results which requires 3.5B weakly la-
beled Instagram images. Not only our method improves
standard ImageNet accuracy, it also improves classiﬁca-
tion robustness on much harder test sets by large margins:
ImageNet-A [32] top-1 accuracy from 61.0% to 83.7%,
ImageNet-C [31] mean corruption error (mCE) from 45.7 to
28.3 and ImageNet-P [31] mean ﬂip rate (mFR) from 27.8
to 12.2. Our main results are shown in Table 1.
ImageNet
ImageNet-A
ImageNet-C
ImageNet-P
top-1 acc.
top-1 acc.
mCE
mFR
Prev. SOTA
86.4%
61.0%
45.7
27.8
Ours
88.4%
83.7%
28.3
12.2
Table 1: Summary of key results compared to previous
state-of-the-art models [86, 55]. Lower is better for mean
corruption error (mCE) and mean ﬂip rate (mFR).
1
arXiv:1911.04252v4  [cs.LG]  19 Jun 2020

2. Noisy Student Training
Algorithm 1 gives an overview of Noisy Student Train-
ing. The inputs to the algorithm are both labeled and unla-
beled images. We use the labeled images to train a teacher
model using the standard cross entropy loss. We then use
the teacher model to generate pseudo labels on unlabeled
images. The pseudo labels can be soft (a continuous dis-
tribution) or hard (a one-hot distribution). We then train
a student model which minimizes the combined cross en-
tropy loss on both labeled images and unlabeled images.
Finally, we iterate the process by putting back the student
as a teacher to generate new pseudo labels and train a new
student. The algorithm is also illustrated in Figure 1.
Require: Labeled images {(x1, y1), (x2, y2), ..., (xn, yn)} and
unlabeled images {˜x1, ˜x2, ..., ˜xm}.
1: Learn teacher model θt
∗which minimizes the cross entropy
loss on labeled images
1
n
n
X
i=1
ℓ(yi, f noised(xi, θt))
2: Use a normal (i.e., not noised) teacher model to generate soft
or hard pseudo labels for clean (i.e., not distorted) unlabeled
images
˜yi = f(˜xi, θt
∗), ∀i = 1, · · · , m
3: Learn an equal-or-larger student model θs
∗which minimizes
the cross entropy loss on labeled images and unlabeled
images with noise added to the student model
1
n
n
X
i=1
ℓ(yi, f noised(xi, θs)) + 1
m
m
X
i=1
ℓ(˜yi, f noised(˜xi, θs))
4: Iterative training: Use the student as a teacher and go back to
step 2.
Algorithm 1: Noisy Student Training.
Train equal-or-
larger student model 
with combined data 
and noise injected
Data augmentation
Stochastic depth
Dropout
…
…
steel arch bridge
canoe
Make the student a 
new teacher
Train teacher model 
with labeled data
Infer pseudo-labels 
on unlabeled data
Figure 1: Illustration of the Noisy Student Training. (All
shown images are from ImageNet.)
The algorithm is an improved version of self-training,
a method in semi-supervised learning (e.g., [71, 96]), and
distillation [33]. More discussions on how our method is
related to prior works are included in Section 5.
Our key improvements lie in adding noise to the stu-
dent and using student models that are not smaller than the
teacher. This makes our method different from Knowledge
Distillation [33] where 1) noise is often not used and 2) a
smaller student model is often used to be faster than the
teacher. One can think of our method as knowledge ex-
pansion in which we want the student to be better than the
teacher by giving the student model enough capacity and
difﬁcult environments in terms of noise to learn through.
Noising Student – When the student is deliberately
noised it is trained to be consistent to the teacher that is
not noised when it generates pseudo labels. In our exper-
iments, we use two types of noise: input noise and model
noise. For input noise, we use data augmentation with Ran-
dAugment [18]. For model noise, we use dropout [76] and
stochastic depth [37].
When applied to unlabeled data, noise has an important
beneﬁt of enforcing invariances in the decision function on
both labeled and unlabeled data. First, data augmentation
is an important noising method in Noisy Student Training
because it forces the student to ensure prediction con-
sistency across augmented versions of an image (similar
to UDA [91]).
Speciﬁcally, in our method, the teacher
produces high-quality pseudo labels by reading in clean im-
ages, while the student is required to reproduce those labels
with augmented images as input. For example, the student
must ensure that a translated version of an image should
have the same category as the original image. Second, when
dropout and stochastic depth function are used as noise, the
teacher behaves like an ensemble at inference time (when
it generates pseudo labels), whereas the student behaves
like a single model. In other words, the student is forced
to mimic a more powerful ensemble model. We present an
ablation study on the effects of noise in Section 4.1.
Other Techniques – Noisy Student Training also works
better with an additional trick: data ﬁltering and balancing,
similar to [91, 93]. Speciﬁcally, we ﬁlter images that the
teacher model has low conﬁdences on since they are usu-
ally out-of-domain images. To ensure that the distribution
of the unlabeled images match that of the training set, we
also need to balance the number of unlabeled images for
each class, as all classes in ImageNet have a similar number
of labeled images. For this purpose, we duplicate images
in classes where there are not enough images. For classes
where we have too many images, we take the images with
the highest conﬁdence.2
Finally, we emphasize that our method can be used with
2The beneﬁts of data balancing is signiﬁcant for small models while
less signiﬁcant for larger models. See Study #5 in Appendix A.2 for more
details.

soft or hard pseudo labels as both work well in our experi-
ments. Soft pseudo labels, in particular, work slightly better
for out-of-domain unlabeled data. Thus in the following, for
consistency, we report results with soft pseudo labels unless
otherwise indicated.
Comparisons with Existing SSL Methods.
Apart from
self-training, another important line of work in semi-
supervised learning [12, 103] is based on consistency train-
ing [5, 64, 47, 84, 56, 91, 8] and pseudo labeling [48, 39,
73, 1]. Although they have produced promising results, in
our preliminary experiments, methods based on consistency
regularization and pseudo labeling work less well on Ima-
geNet. Instead of using a teacher model trained on labeled
data to generate pseudo-labels, these methods do not have
a separate teacher model and use the model being trained
to generate pseudo-labels. In the early phase of training,
the model being trained has low accuracy and high en-
tropy, hence consistency training regularizes the model to-
wards high entropy predictions, and prevents it from achiev-
ing good accuracy. A common workaround is to use en-
tropy minimization, to ﬁlter examples with low conﬁdence
or to ramp up the consistency loss.
However, the addi-
tional hyperparameters introduced by the ramping up sched-
ule, conﬁdence-based ﬁltering and the entropy minimiza-
tion make them more difﬁcult to use at scale. The self-
training / teacher-student framework is better suited for Im-
ageNet because we can train a good teacher on ImageNet
using labeled data.
3. Experiments
In this section, we will ﬁrst describe our experiment de-
tails. We will then present our ImageNet results compared
with those of state-of-the-art models. Lastly, we demon-
strate the surprising improvements of our models on robust-
ness datasets (such as ImageNet-A, C and P) as well as un-
der adversarial attacks.
3.1. Experiment Details
Labeled dataset.
We conduct experiments on ImageNet
2012 ILSVRC challenge prediction task since it has been
considered one of the most heavily benchmarked datasets in
computer vision and that improvements on ImageNet trans-
fer to other datasets [44, 66].
Unlabeled dataset.
We obtain unlabeled images from the
JFT dataset [33, 15], which has around 300M images. Al-
though the images in the dataset have labels, we ignore the
labels and treat them as unlabeled data. We ﬁlter the Ima-
geNet validation set images from the dataset (see [58]).
We then perform data ﬁltering and balancing on this
corpus. First, we run an EfﬁcientNet-B0 trained on Ima-
geNet [83] over the JFT dataset [33, 15] to predict a label
for each image. We then select images that have conﬁdence
of the label higher than 0.3. For each class, we select at most
130K images that have the highest conﬁdence. Finally, for
classes that have less than 130K images, we duplicate some
images at random so that each class can have 130K images.
Hence the total number of images that we use for training a
student model is 130M (with some duplicated images). Due
to duplications, there are only 81M unique images among
these 130M images. We do not tune these hyperparameters
extensively since our method is highly robust to them.
To enable fair comparisons with our results, we also ex-
periment with a public dataset YFCC100M [85] and show
the results in Appendix A.4.
Architecture.
We use EfﬁcientNets [83] as our baseline
models because they provide better capacity for more data.
In our experiments, we also further scale up EfﬁcientNet-
B7 and obtain EfﬁcientNet-L2. EfﬁcientNet-L2 is wider
and deeper than EfﬁcientNet-B7 but uses a lower resolution,
which gives it more parameters to ﬁt a large number of unla-
beled images. Due to the large model size, the training time
of EfﬁcientNet-L2 is approximately ﬁve times the train-
ing time of EfﬁcientNet-B7. For more information about
EfﬁcientNet-L2, please refer to Table 8 in Appendix A.1.
Training details.
For labeled images, we use a batch size
of 2048 by default and reduce the batch size when we could
not ﬁt the model into the memory. We ﬁnd that using a batch
size of 512, 1024, and 2048 leads to the same performance.
We determine the number of training steps and the learning
rate schedule by the batch size for labeled images. Speciﬁ-
cally, we train the student model for 350 epochs for models
larger than EfﬁcientNet-B4, including EfﬁcientNet-L2 and
train smaller student models for 700 epochs. The learning
rate starts at 0.128 for labeled batch size 2048 and decays
by 0.97 every 2.4 epochs if trained for 350 epochs or every
4.8 epochs if trained for 700 epochs.
We use a large batch size for unlabeled images, espe-
cially for large models, to make full use of large quantities
of unlabeled images. Labeled images and unlabeled images
are concatenated together to compute the average cross en-
tropy loss. We apply the recently proposed technique to ﬁx
train-test resolution discrepancy [86] for EfﬁcientNet-L2.
We ﬁrst perform normal training with a smaller resolution
for 350 epochs. Then we ﬁnetune the model with a larger
resolution for 1.5 epochs on unaugmented labeled images.
Similar to [86], we ﬁx the shallow layers during ﬁnetuning.
Our largest model, EfﬁcientNet-L2, needs to be trained
for 6 days on a Cloud TPU v3 Pod, which has 2048 cores,
if the unlabeled batch size is 14x the labeled batch size.

Method
# Params
Extra Data
Top-1 Acc.
Top-5 Acc.
ResNet-50 [30]
26M
-
76.0%
93.0%
ResNet-152 [30]
60M
-
77.8%
93.8%
DenseNet-264 [36]
34M
-
77.9%
93.9%
Inception-v3 [81]
24M
-
78.8%
94.4%
Xception [15]
23M
-
79.0%
94.5%
Inception-v4 [79]
48M
-
80.0%
95.0%
Inception-resnet-v2 [79]
56M
-
80.1%
95.1%
ResNeXt-101 [92]
84M
-
80.9%
95.6%
PolyNet [100]
92M
-
81.3%
95.8%
SENet [35]
146M
-
82.7%
96.2%
NASNet-A [104]
89M
-
82.7%
96.2%
AmoebaNet-A [65]
87M
-
82.8%
96.1%
PNASNet [50]
86M
-
82.9%
96.2%
AmoebaNet-C [17]
155M
-
83.5%
96.5%
GPipe [38]
557M
-
84.3%
97.0%
EfﬁcientNet-B7 [83]
66M
-
85.0%
97.2%
EfﬁcientNet-L2 [83]
480M
-
85.5%
97.5%
ResNet-50 Billion-scale [93]
26M
3.5B images labeled with tags
81.2%
96.0%
ResNeXt-101 Billion-scale [93]
193M
84.8%
-
ResNeXt-101 WSL [55]
829M
85.4%
97.6%
FixRes ResNeXt-101 WSL [86]
829M
86.4%
98.0%
Big Transfer (BiT-L) [43]†
928M
300M weakly labeled images from JFT
87.5%
98.5%
Noisy Student Training (EfﬁcientNet-L2)
480M
300M unlabeled images from JFT
88.4%
98.7%
Table 2: Top-1 and Top-5 Accuracy of Noisy Student Training and previous state-of-the-art methods on ImageNet.
EfﬁcientNet-L2 with Noisy Student Training is the result of iterative training for multiple iterations by putting back the
student model as the new teacher. It has better tradeoff in terms of accuracy and model size compared to previous state-of-
the-art models. †: Big Transfer is a concurrent work that performs transfer learning from the JFT dataset.
Noise.
We use stochastic depth [37], dropout [76], and
RandAugment [18] to noise the student. The hyperparame-
ters for these noise functions are the same for EfﬁcientNet-
B7 and L2. In particular, we set the survival probability
in stochastic depth to 0.8 for the ﬁnal layer and follow the
linear decay rule for other layers. We apply dropout to the
ﬁnal layer with a dropout rate of 0.5. For RandAugment,
we apply two random operations with magnitude set to 27.
Iterative training.
The best model in our experiments is
a result of three iterations of putting back the student as the
new teacher. We ﬁrst trained an EfﬁcientNet-B7 on Ima-
geNet as the teacher model. Then by using the B7 model
as the teacher, we trained an EfﬁcientNet-L2 model with
the unlabeled batch size set to 14 times the labeled batch
size. Then, we trained a new EfﬁcientNet-L2 model with
the EfﬁcientNet-L2 model as the teacher. Lastly, we iter-
ated again and used an unlabeled batch size of 28 times the
labeled batch size. The detailed results of the three itera-
tions are available in Section 4.2.
3.2. ImageNet Results
We ﬁrst report the validation set accuracy on the Im-
ageNet 2012 ILSVRC challenge prediction task as com-
monly done in literature [45, 80, 30, 83] (see also [66]).
As shown in Table 2, EfﬁcientNet-L2 with Noisy Student
Training achieves 88.4% top-1 accuracy which is signiﬁ-
cantly better than the best reported accuracy on EfﬁcientNet
of 85.0%. The total gain of 3.4% comes from two sources:
by making the model larger (+0.5%) and by Noisy Student
Training (+2.9%). In other words, Noisy Student Training
makes a much larger impact on the accuracy than changing
the architecture.
Further, Noisy Student Training outperforms the state-
of-the-art accuracy of 86.4% by FixRes ResNeXt-101
WSL [55, 86] that requires 3.5 Billion Instagram images
labeled with tags. As a comparison, our method only re-
quires 300M unlabeled images, which is perhaps more easy
to collect. Our model is also approximately twice as small
in the number of parameters compared to FixRes ResNeXt-
101 WSL.

Model size study: Noisy Student Training for Efﬁcient-
Net B0-B7 without Iterative Training.
In addition to
improving state-of-the-art results, we conduct experiments
to verify if Noisy Student Training can beneﬁt other Efﬁ-
cienetNet models. In previous experiments, iterative train-
ing was used to optimize the accuracy of EfﬁcientNet-L2
but here we skip it as it is difﬁcult to use iterative train-
ing for many experiments. We vary the model size from
EfﬁcientNet-B0 to EfﬁcientNet-B7 [83] and use the same
model as both the teacher and the student. We apply Ran-
dAugment to all EfﬁcientNet baselines, leading to more
competitive baselines. We set the unlabeled batch size to
be three times the batch size of labeled images for all model
sizes except for EfﬁcientNet-B0. For EfﬁcientNet-B0, we
set the unlabeled batch size to be the same as the batch size
of labeled images. As shown in Figure 2, Noisy Student
Training leads to a consistent improvement of around 0.8%
for all model sizes. Overall, EfﬁcientNets with Noisy Stu-
dent Training provide a much better tradeoff between model
size and accuracy than prior works. The results also conﬁrm
that vision models can beneﬁt from Noisy Student Training
even without iterative training.
0
20
40
60
80
100
120
140
160
Number of Parameters (Millions)
74
76
78
80
82
84
86
ImageNet Top-1 Accuracy (%)
Model
Top-1 Acc.
EfﬁcientNet-B0
77.3%
Noisy Student Training (B0)
78.1%
EfﬁcientNet-B2
80.0%
Noisy Student Training (B2)
81.1%
EfﬁcientNet-B5
84.0%
Noisy Student Training (B5)
85.1%
EfﬁcientNet-B7
85.0%
Noisy Student Training (B7)
86.4%
B3
B4
B5
B6
Noisy Student Training (EfﬁcientNet-B7)
EfﬁcientNet-B7
ResNet-34
Inception-v2
NASNet-A
ResNet-50
DenseNet-201 ResNet-152
Xception
Inception-resnet-v2
ResNeXt-101
SENet
NASNet-A
AmoebaNet-A
AmoebaNet-C
Figure 2: Noisy Student Training leads to signiﬁcant im-
provements across all model sizes. We use the same archi-
tecture for the teacher and the student and do not perform
iterative training.
3.3. Robustness Results on ImageNet-A, ImageNet-
C and ImageNet-P
We evaluate the best model, that achieves 88.4% top-
1 accuracy, on three robustness test sets:
ImageNet-
A, ImageNet-C and ImageNet-P. ImageNet-C and P test
sets [31] include images with common corruptions and per-
turbations such as blurring, fogging, rotation and scaling.
ImageNet-A test set [32] consists of difﬁcult images that
cause signiﬁcant drops in accuracy to state-of-the-art mod-
els. These test sets are considered as “robustness” bench-
marks because the test images are either much harder, for
ImageNet-A, or the test images are different from the train-
ing images, for ImageNet-C and P.
Method
Top-1 Acc.
Top-5 Acc.
ResNet-101 [32]
4.7%
-
ResNeXt-101 [32] (32x4d)
5.9%
-
ResNet-152 [32]
6.1%
-
ResNeXt-101 [32] (64x4d)
7.3%
-
DPN-98 [32]
9.4%
-
ResNeXt-101+SE [32] (32x4d)
14.2%
-
ResNeXt-101 WSL [55, 59]
61.0%
-
EfﬁcientNet-L2
49.6%
78.6%
Noisy Student Training (L2)
83.7%
95.2%
Table 3: Robustness results on ImageNet-A.
Method
Res.
Top-1 Acc.
mCE
ResNet-50 [31]
224
39.0%
76.7
SIN [23]
224
45.2%
69.3
Patch Gaussian [51]
299
52.3%
60.4
ResNeXt-101 WSL [55, 59]
224
-
45.7
EfﬁcientNet-L2
224
62.6%
47.5
Noisy Student Training (L2)
224
76.5%
30.0
EfﬁcientNet-L2
299
66.6%
42.5
Noisy Student Training (L2)
299
77.8%
28.3
Table 4: Robustness results on ImageNet-C. mCE is the
weighted average of error rate on different corruptions, with
AlexNet’s error rate as a baseline (lower is better).
Method
Res.
Top-1 Acc.
mFR
ResNet-50 [31]
224
-
58.0
Low Pass Filter Pooling [99]
224
-
51.2
ResNeXt-101 WSL [55, 59]
224
-
27.8
EfﬁcientNet-L2
224
80.4%
27.2
Noisy Student Training (L2)
224
85.2%
14.2
EfﬁcientNet-L2
299
81.6%
23.7
Noisy Student Training (L2)
299
86.4%
12.2
Table 5: Robustness results on ImageNet-P, where images
are generated with a sequence of perturbations. mFR mea-
sures the model’s probability of ﬂipping predictions under
perturbations with AlexNet as a baseline (lower is better).
For ImageNet-C and ImageNet-P, we evaluate models on
two released versions with resolution 224x224 and 299x299
and resize images to the resolution EfﬁcientNet trained on.

lighthouse
sea lion
submarine
canoe
bullfrog
dragonfly
starfish
wreck
hummingbird bald eagle
basketball parking meter
(a) ImageNet-A
gown
ski
pill bottle
toaster
television
cannon
parking meter
vacuum
swing
mosquito net
electric ray
snow leopard
(b) ImageNet-C
racing car
fire engine
racing car
car wheel
medicine chest
plate rack
medicine chest
plate rack
refrigerator
plate rack
racing car
car wheel
(c) ImageNet-P
Figure 3: Selected images from robustness benchmarks ImageNet-A, C and P. Test images from ImageNet-C underwent
artiﬁcial transformations (also known as common corruptions) that cannot be found on the ImageNet training set. Test
images on ImageNet-P underwent different scales of perturbations. On ImageNet-A, C, EfﬁcientNet with Noisy Student
Tranining produces correct top-1 predictions (shown in bold black texts) and EfﬁcientNet without Noisy Student Training
produces incorrect top-1 predictions (shown in red texts). On ImageNet-P, EfﬁcientNet without Noisy Student Training ﬂips
predictions frequently.
As shown in Table 3, 4 and 5, Noisy Student Training yields
substantial gains on robustness datasets compared to the
previous state-of-the-art model ResNeXt-101 WSL [55, 59]
trained on 3.5B weakly labeled images. On ImageNet-A,
it improves the top-1 accuracy from 61.0% to 83.7%. On
ImageNet-C, it reduces mean corruption error (mCE) from
45.7 to 28.3. On ImageNet-P, it leads to a mean ﬂip rate
(mFR) of 14.2 if we use a resolution of 224x224 (direct
comparison) and 12.2 if we use a resolution of 299x299.3
These signiﬁcant gains in robustness in ImageNet-C and
ImageNet-P are surprising because our method was not de-
liberately optimized for robustness.4
3For EfﬁcientNet-L2, we use the model without ﬁnetuning with a larger
test time resolution, since a larger resolution results in a discrepancy with
the resolution of data and leads to degraded performance on ImageNet-C
and ImageNet-P.
4Note that both our model and ResNeXt-101 WSL use augmentations
that have a small overlap with corruptions in ImageNet-C, which might
result in better performance. Speciﬁcally, RandAugment includes aug-
mentation Brightness, Contrast and Sharpness. ResNeXt-101 WSL uses
augmentation of Brightness and Contrast.
Qualitative Analysis.
To intuitively understand the sig-
niﬁcant improvements on the three robustness benchmarks,
we show several images in Figure 3 where the predictions
of the standard model are incorrect while the predictions of
the model with Noisy Student Training are correct.
Figure 3a shows example images from ImageNet-A and
the predictions of our models. The model with Noisy Stu-
dent Training can successfully predict the correct labels of
these highly difﬁcult images. For example, without Noisy
Student Training, the model predicts bullfrog for the image
shown on the left of the second row, which might be re-
sulted from the black lotus leaf on the water. With Noisy
Student Training, the model correctly predicts dragonﬂy for
the image. At the top-left image, the model without Noisy
Student Training ignores the sea lions and mistakenly rec-
ognizes a buoy as a lighthouse, while the model with Noisy
Student Training can recognize the sea lions.
Figure 3b shows images from ImageNet-C and the cor-
responding predictions. As can be seen from the ﬁgure, our
model with Noisy Student Training makes correct predic-
tions for images under severe corruptions and perturbations
such as snow, motion blur and fog, while the model without

Noisy Student Training suffers greatly under these condi-
tions. The most interesting image is shown on the right of
the ﬁrst row. The swing in the picture is barely recognizable
by human while the model with Noisy Student Training still
makes the correct prediction.
Figure 3c shows images from ImageNet-P and the cor-
responding predictions. As can be seen, our model with
Noisy Student Training makes correct and consistent pre-
dictions as images undergone different perturbations while
the model without Noisy Student Training ﬂips predictions
frequently.
3.4. Adversarial Robustness Results
After testing our model’s robustness to common corrup-
tions and perturbations, we also study its performance on
adversarial perturbations. We evaluate our EfﬁcientNet-L2
models with and without Noisy Student Training against an
FGSM attack. This attack performs one gradient descent
step on the input image [25] with the update on each pixel
set to ϵ. As shown in Figure 4, Noisy Student Training leads
to very signiﬁcant improvements in accuracy even though
the model is not optimized for adversarial robustness. Un-
der a stronger attack PGD with 10 iterations [54], at ϵ = 16,
Noisy Student Training improves EfﬁcientNet-L2’s accu-
racy from 1.1% to 4.4%.
Note that these adversarial robustness results are not di-
rectly comparable to prior works since we use a large in-
put resolution of 800x800 and adversarial vulnerability can
scale with the input dimension [22, 25, 24, 74].
0
2
4
6
8
10
12
14
16
epsilon
60
65
70
75
80
85
ImageNet Top-1 Accuracy (%)
Noisy Student Training (L2)
EfficientNet-L2
Figure 4: Noisy Student Training improves adversarial ro-
bustness against an FGSM attack though the model is not
optimized for adversarial robustness. The accuracy is im-
proved by 11% at ϵ = 2 and gets better as ϵ gets larger.
4. Ablation Study
In this section, we study the importance of noise and it-
erative training and summarize the ablations for other com-
ponents of our method.
4.1. The Importance of Noise in Self-training
Since we use soft pseudo labels generated from the
teacher model, when the student is trained to be exactly the
same as the teacher model, the cross entropy loss on un-
labeled data would be zero and the training signal would
vanish. Hence, a question that naturally arises is why the
student can outperform the teacher with soft pseudo labels.
As stated earlier, we hypothesize that noising the student is
needed so that it does not merely learn the teacher’s knowl-
edge. We investigate the importance of noising in two sce-
narios with different amounts of unlabeled data and dif-
ferent teacher model accuracies. In both cases, we gradu-
ally remove augmentation, stochastic depth and dropout for
unlabeled images when training the student model, while
keeping them for labeled images. This way, we can isolate
the inﬂuence of noising on unlabeled images from the in-
ﬂuence of preventing overﬁtting for labeled images. In ad-
dition, we compare using a noised teacher and an unnoised
teacher to study if it is necessary to disable noise when gen-
erating pseudo labels.
Here, we show the evidence in Table 6, noise such as
stochastic depth, dropout and data augmentation plays an
important role in enabling the student model to perform bet-
ter than the teacher. The performance consistently drops
with noise function removed. However, in the case with
130M unlabeled images, when compared to the supervised
baseline, the performance is still improved to 84.3% from
84.0% with noise function removed. We hypothesize that
the improvement can be attributed to SGD, which intro-
duces stochasticity into the training process.
One might argue that the improvements from using noise
can be resulted from preventing overﬁtting the pseudo la-
bels on the unlabeled images. We verify that this is not
the case when we use 130M unlabeled images since the
model does not overﬁt the unlabeled set from the training
loss. While removing noise leads to a much lower train-
ing loss for labeled images, we observe that, for unlabeled
images, removing noise leads to a smaller drop in training
loss. This is probably because it is harder to overﬁt the large
unlabeled dataset.
Lastly, adding noise to the teacher model that generates
pseudo labels leads to lower accuracy, which shows the im-
portance of having a powerful unnoised teacher model.
4.2. A Study of Iterative Training
Here, we show the detailed effects of iterative training.
As mentioned in Section 3.1, we ﬁrst train an EfﬁcientNet-
B7 model on labeled data and then use it as the teacher to

Model / Unlabeled Set Size
1.3M
130M
EfﬁcientNet-B5
83.3%
84.0%
Noisy Student Training (B5)
83.9%
85.1%
student w/o Aug
83.6%
84.6%
student w/o Aug, SD, Dropout
83.2%
84.3%
teacher w. Aug, SD, Dropout
83.7%
84.4%
Table 6: Ablation study of noising. We use EfﬁcientNet-
B5 as the teacher model and study two cases with differ-
ent numbers of unlabeled images and different augmenta-
tions. For the experiment with 1.3M unlabeled images, we
use the standard augmentation including random translation
and ﬂipping for both the teacher and the student. For the ex-
periment with 130M unlabeled images, we use RandAug-
ment. Aug and SD denote data augmentation and stochastic
depth respectively. We remove the noise for unlabeled im-
ages while keeping them for labeled images. Here, iterative
training is not used and unlabeled batch size is set to be the
same as the labeled batch size to save training time.
train an EfﬁcientNet-L2 student model. Then, we iterate
this process by putting back the new student model as the
teacher model.
As shown in Table 7, the model performance improves
to 87.6% in the ﬁrst iteration and then to 88.1% in the sec-
ond iteration with the same hyperparameters (except using a
teacher model with better performance). These results indi-
cate that iterative training is effective in producing increas-
ingly better models. For the last iteration, we make use of a
larger ratio between unlabeled batch size and labeled batch
size to boost the ﬁnal performance to 88.4%.
Iteration
Model
Batch Size Ratio
Top-1 Acc.
1
EfﬁcientNet-L2
14:1
87.6%
2
EfﬁcientNet-L2
14:1
88.1%
3
EfﬁcientNet-L2
28:1
88.4%
Table 7: Iterative training improves the accuracy, where
batch size ratio denotes the ratio between unlabeled data
and labeled data.
4.3. Additional Ablation Study Summarization
We also study the importance of various design choices
of Noisy Student Training, hopefully offering a practical
guide for readers. With this purpose, we conduct 8 abla-
tion studies in Appendix A.2. The ﬁndings are summarized
as follows:
• Finding #1: Using a large teacher model with better
performance leads to better results.
• Finding #2: A large amount of unlabeled data is nec-
essary for better performance.
• Finding #3: Soft pseudo labels work better than hard
pseudo labels for out-of-domain data in certain cases.
• Finding #4: A large student model is important to en-
able the student to learn a more powerful model.
• Finding #5: Data balancing is useful for small mod-
els.
• Finding #6: Joint training on labeled data and unla-
beled data outperforms the pipeline that ﬁrst pretrains
with unlabeled data and then ﬁnetunes on labeled data.
• Finding #7: Using a large ratio between unlabeled
batch size and labeled batch size enables models to
train longer on unlabeled data to achieve a higher ac-
curacy.
• Finding #8:
Training the student from scratch is
sometimes better than initializing the student with the
teacher and the student initialized with the teacher still
requires a large number of training epochs to perform
well.
5. Related works
Self-training.
Our
work
is
based
on
self-training
(e.g., [71, 96, 68, 67]).
Self-training ﬁrst uses labeled
data to train a good teacher model, then use the teacher
model to label unlabeled data and ﬁnally use the labeled
data and unlabeled data to jointly train a student model. In
typical self-training with the teacher-student framework,
noise injection to the student is not used by default, or the
role of noise is not fully understood or justiﬁed. The main
difference between our work and prior works is that we
identify the importance of noise, and aggressively inject
noise to make the student better.
Self-training was previously used to improve ResNet-50
from 76.4% to 81.2% top-1 accuracy [93] which is still far
from the state-of-the-art accuracy. Yalniz et al. [93] also did
not show signiﬁcant improvements in terms of robustness
on ImageNet-A, C and P as we did. In terms of methodol-
ogy, they proposed to ﬁrst only train on unlabeled images
and then ﬁnetune their model on labeled images as the ﬁ-
nal stage. In Noisy Student Training, we combine these two
steps into one because it simpliﬁes the algorithm and leads
to better performance in our experiments.
Data Distillation [63], which ensembles predictions for
an image with different transformations to strengthen the
teacher, is the opposite of our approach of weakening the
student. Parthasarathi et al. [61] ﬁnd a small and fast speech
recognition model for deployment via knowledge distilla-
tion on unlabeled data. As noise is not used and the stu-
dent is also small, it is difﬁcult to make the student better
than teacher. The domain adaptation framework in [69] is
related but highly optimized for videos, e.g., prediction on

which frame to use in a video. The method in [101] en-
sembles predictions from multiple teacher models, which is
more expensive than our method.
Co-training [9] divides features into two disjoint parti-
tions and trains two models with the two sets of features
using labeled data. Their source of “noise” is the feature
partitioning such that two models do not always agree on
unlabeled data. Our method of injecting noise to the stu-
dent model also enables the teacher and the student to make
different predictions and is more suitable for ImageNet than
partitioning features.
Self-training / co-training has also been shown to work
well for a variety of other tasks including leveraging noisy
data [87], semantic segmentation [4], text classiﬁcation [40,
78]. Back translation and self-training have led to signiﬁ-
cant improvements in machine translation [72, 20, 28, 14,
90, 29].
Semi-supervised Learning.
Apart from self-training, an-
other important line of work in semi-supervised learn-
ing [12, 103] is based on consistency training [5, 64, 47, 84,
56, 52, 62, 13, 16, 60, 2, 49, 88, 91, 8, 98, 46, 7]. They con-
strain model predictions to be invariant to noise injected to
the input, hidden states or model parameters. As discussed
in Section 2, consistency regularization works less well on
ImageNet because consistency regularization uses a model
being trained to generate the pseudo-labels. In the early
phase of training, they regularize the model towards high
entropy predictions, and prevents it from achieving good
accuracy.
Works based on pseudo label [48, 39, 73, 1] are similar
to self-training, but also suffer the same problem with con-
sistency training, since they rely on a model being trained
instead of a converged model with high accuracy to gener-
ate pseudo labels. Finally, frameworks in semi-supervised
learning also include graph-based methods [102, 89, 94,
42], methods that make use of latent variables as target vari-
ables [41, 53, 95] and methods based on low-density sep-
aration [26, 70, 19], which might provide complementary
beneﬁts to our method.
Knowledge Distillation.
Our work is also related to
methods in Knowledge Distillation [10, 3, 33, 21, 6] via the
use of soft targets. The main use of knowledge distillation
is model compression by making the student model smaller.
The main difference between our method and knowledge
distillation is that knowledge distillation does not consider
unlabeled data and does not aim to improve the student
model.
Robustness.
A number of studies, e.g. [82, 31, 66, 27],
have shown that vision models lack robustness. Addressing
the lack of robustness has become an important research di-
rection in machine learning and computer vision in recent
years. Our study shows that using unlabeled data improves
accuracy and general robustness. Our ﬁnding is consistent
with arguments that using unlabeled data can improve ad-
versarial robustness [11, 77, 57, 97]. The main difference
between our work and these works is that they directly op-
timize adversarial robustness on unlabeled data, whereas
we show that Noisy Student Training improves robustness
greatly even without directly optimizing robustness.
6. Conclusion
Prior works on weakly-supervised learning required bil-
lions of weakly labeled data to improve state-of-the-art Im-
ageNet models. In this work, we showed that it is possible
to use unlabeled images to signiﬁcantly advance both ac-
curacy and robustness of state-of-the-art ImageNet models.
We found that self-training is a simple and effective algo-
rithm to leverage unlabeled data at scale. We improved it
by adding noise to the student, hence the name Noisy Stu-
dent Training, to learn beyond the teacher’s knowledge.
Our experiments showed that Noisy Student Training
and EfﬁcientNet can achieve an accuracy of 88.4% which
is 2.9% higher than without Noisy Student Training. This
result is also a new state-of-the-art and 2.0% better than the
previous best method that used an order of magnitude more
weakly labeled data [55, 86].
An important contribution of our work was to show that
Noisy Student Training boosts robustness in computer vi-
sion models. Our experiments showed that our model sig-
niﬁcantly improves performances on ImageNet-A, C and P.
Acknowledgement
We thank the Google Brain team, Zihang Dai, Jeff Dean,
Hieu Pham, Colin Raffel, Ilya Sutskever and Mingxing Tan
for insightful discussions, Cihang Xie, Dan Hendrycks and
A. Emin Orhan for robustness evaluation, Sergey Ioffe,
Guokun Lai, Jiquan Ngiam, Jiateng Xie and Adams Wei
Yu for feedbacks on the draft, Yanping Huang, Pankaj
Kanwar, Naveen Kumar, Sameer Kumar and Zak Stone
for great help with TPUs, Ekin Dogus Cubuk and Barret
Zoph for help with RandAugment, Tom Duerig, Victor
Gomes, Paul Haahr, Pandu Nayak, David Price, Janel
Thamkul, Elizabeth Trumbull, Jake Walker and Wenlei
Zhou for help with model releases, Yanan Bao, Zheyun
Feng and Daiyi Peng for help with the JFT dataset, Ola
Spyra and Olga Wichrowska for help with infrastructure.
References
[1] Eric Arazo, Diego Ortego, Paul Albert, Noel E O’Connor,
and Kevin McGuinness.
Pseudo-labeling and conﬁrma-
tion bias in deep semi-supervised learning. arXiv preprint
arXiv:1908.02983, 2019. 3, 9

[2] Ben Athiwaratkun, Marc Finzi, Pavel Izmailov, and An-
drew Gordon Wilson. There are many consistent explana-
tions of unlabeled data: Why you should average. In Inter-
national Conference on Learning Representations, 2018. 9
[3] Jimmy Ba and Rich Caruana. Do deep nets really need to
be deep?
In Advances in Neural Information Processing
Systems, pages 2654–2662, 2014. 9
[4] Yauhen Babakhin, Artsiom Sanakoyeu, and Hirotoshi Kita-
mura. Semi-supervised segmentation of salt bodies in seis-
mic images using an ensemble of convolutional neural net-
works. arXiv preprint arXiv:1904.04445, 2019. 9
[5] Philip Bachman, Ouais Alsharif, and Doina Precup. Learn-
ing with pseudo-ensembles. In Advances in Neural Infor-
mation Processing Systems, pages 3365–3373, 2014. 3, 9
[6] Anoop Korattikara Balan, Vivek Rathod, Kevin P Murphy,
and Max Welling. Bayesian dark knowledge. In Advances
in Neural Information Processing Systems, pages 3438–
3446, 2015. 9
[7] David Berthelot, Nicholas Carlini, Ekin D Cubuk, Alex
Kurakin, Kihyuk Sohn, Han Zhang, and Colin Raffel.
Remixmatch: Semi-supervised learning with distribution
alignment and augmentation anchoring.
arXiv preprint
arXiv:1911.09785, 2019. 9
[8] David Berthelot, Nicholas Carlini, Ian Goodfellow, Nicolas
Papernot, Avital Oliver, and Colin Raffel. Mixmatch: A
holistic approach to semi-supervised learning. In Advances
in Neural Information Processing Systems, 2019. 3, 9
[9] Avrim Blum and Tom Mitchell.
Combining labeled and
unlabeled data with co-training.
In Proceedings of the
eleventh annual conference on Computational learning the-
ory, pages 92–100. ACM, 1998. 9
[10] Cristian Bucilu, Rich Caruana, and Alexandru Niculescu-
Mizil. Model compression. In Proceedings of the 12th ACM
SIGKDD international conference on Knowledge discovery
and data mining, pages 535–541. ACM, 2006. 9
[11] Yair Carmon, Aditi Raghunathan, Ludwig Schmidt, Percy
Liang, and John C Duchi. Unlabeled data improves adver-
sarial robustness. arXiv preprint arXiv:1905.13736, 2019.
9
[12] Olivier Chapelle, Bernhard Scholkopf, and Alexander
Zien. Semi-supervised learning (chapelle, o. et al., eds.;
2006)[book reviews]. IEEE Transactions on Neural Net-
works, 20(3):542–542, 2009. 3, 9
[13] Yanbei Chen, Xiatian Zhu, and Shaogang Gong.
Semi-
supervised deep learning with memory.
In Proceedings
of the European Conference on Computer Vision (ECCV),
pages 268–283, 2018. 9
[14] Yong Cheng, Wei Xu, Zhongjun He, Wei He, Hua
Wu, Maosong Sun, and Yang Liu.
Semi-supervised
learning for neural machine translation.
arXiv preprint
arXiv:1606.04596, 2016. 9
[15] Franc¸ois Chollet. Xception: Deep learning with depthwise
separable convolutions. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition, pages
1251–1258, 2017. 3, 4
[16] Kevin Clark, Minh-Thang Luong, Christopher D Manning,
and Quoc V Le. Semi-supervised sequence modeling with
cross-view training. In Empirical Methods in Natural Lan-
guage Processing (EMNLP), 2018. 9
[17] Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Va-
sudevan, and Quoc V Le. AutoAugment: Learning aug-
mentation strategies from data. In Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition,
2018. 4
[18] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V
Le.
Randaugment: Practical data augmentation with no
separate search. arXiv preprint arXiv:1909.13719, 2019.
1, 2, 4
[19] Zihang Dai, Zhilin Yang, Fan Yang, William W Cohen, and
Ruslan R Salakhutdinov. Good semi-supervised learning
that requires a bad gan. In Advances in Neural Information
Processing Systems, pages 6510–6520, 2017. 9
[20] Sergey Edunov, Myle Ott, Michael Auli, and David Grang-
ier. Understanding back-translation at scale. In Proceed-
ings of the 2018 conference on Empirical methods in natu-
ral language processing, pages 489–500, 2018. 9
[21] Tommaso Furlanello, Zachary C Lipton, Michael Tschan-
nen, Laurent Itti, and Anima Anandkumar.
Born again
neural networks. In International Conference on Machine
Learning, 2018. 9
[22] Angus Galloway, Anna Golubeva, Thomas Tanay, Med-
hat Moussa, and Graham W Taylor.
Batch normaliza-
tion is a cause of adversarial vulnerability. arXiv preprint
arXiv:1905.02161, 2019. 7
[23] Robert Geirhos, Patricia Rubisch, Claudio Michaelis,
Matthias Bethge, Felix A Wichmann, and Wieland Bren-
del. ImageNet-trained CNNs are biased towards texture;
increasing shape bias improves accuracy and robustness.
In International Conference on Learning Representations,
2019. 5
[24] Justin Gilmer, Luke Metz, Fartash Faghri, Samuel S
Schoenholz, Maithra Raghu, Martin Wattenberg, and
Ian Goodfellow.
Adversarial spheres.
arXiv preprint
arXiv:1801.02774, 2018. 7
[25] Ian J Goodfellow, Jonathon Shlens, and Christian Szegedy.
Explaining and harnessing adversarial examples. In Inter-
national Conference on Learning Representations, 2015. 7
[26] Yves Grandvalet and Yoshua Bengio.
Semi-supervised
learning by entropy minimization. In Advances in neural
information processing systems, pages 529–536, 2005. 9
[27] Keren Gu, Brandon Yang, Jiquan Ngiam, Quoc Le, and
Jonathan Shlens.
Using videos to evaluate image model
robustness. In ICLR Workshop, 2019. 9
[28] Di He, Yingce Xia, Tao Qin, Liwei Wang, Nenghai Yu,
Tie-Yan Liu, and Wei-Ying Ma. Dual learning for machine
translation. In Advances in Neural Information Processing
Systems, pages 820–828, 2016. 9
[29] Junxian He, Jiatao Gu, Jiajun Shen, and Marc’Aurelio Ran-
zato. Revisiting self-training for neural sequence genera-
tion. arXiv preprint arXiv:1909.13788, 2019. 9
[30] Kaiming He, Xiangyu Zhang, Shaoqing Ren, and Jian Sun.
Deep residual learning for image recognition. In Proceed-
ings of the IEEE conference on computer vision and pattern
recognition, pages 770–778, 2016. 1, 4, 16

[31] Dan Hendrycks and Thomas G Dietterich. Benchmarking
neural network robustness to common corruptions and per-
turbations. In International Conference on Learning Rep-
resentations, 2019. 1, 5, 9, 17, 18
[32] Dan Hendrycks, Kevin Zhao, Steven Basart, Jacob Stein-
hardt, and Dawn Song. Natural adversarial examples. arXiv
preprint arXiv:1907.07174, 2019. 1, 5
[33] Geoffrey Hinton, Oriol Vinyals, and Jeff Dean.
Distill-
ing the knowledge in a neural network.
arXiv preprint
arXiv:1503.02531, 2015. 2, 3, 9
[34] Andrew G Howard. Some improvements on deep convo-
lutional neural network based image classiﬁcation. arXiv
preprint arXiv:1312.5402, 2013. 18
[35] Jie Hu, Li Shen, and Gang Sun. Squeeze-and-excitation
networks. In Proceedings of the IEEE conference on com-
puter vision and pattern recognition, pages 7132–7141,
2018. 4
[36] Gao Huang, Zhuang Liu, Laurens Van Der Maaten, and Kil-
ian Q Weinberger. Densely connected convolutional net-
works. In Proceedings of the IEEE conference on computer
vision and pattern recognition, pages 4700–4708, 2017. 4
[37] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kil-
ian Q Weinberger. Deep networks with stochastic depth. In
European conference on computer vision, pages 646–661.
Springer, 2016. 1, 2, 4
[38] Yanping Huang, Yonglong Cheng, Dehao Chen, Hy-
oukJoong Lee, Jiquan Ngiam, Quoc V Le, and Zhifeng
Chen. GPipe: Efﬁcient training of giant neural networks
using pipeline parallelism. In Advances in Neural Informa-
tion Processing Systems, 2019. 4
[39] Ahmet Iscen, Giorgos Tolias, Yannis Avrithis, and Ondrej
Chum. Label propagation for deep semi-supervised learn-
ing. In Proceedings of the IEEE Conference on Computer
Vision and Pattern Recognition, pages 5070–5079, 2019. 3,
9
[40] Giannis Karamanolakis, Daniel Hsu, and Luis Gravano.
Leveraging just a few keywords for ﬁne-grained aspect de-
tection through weakly supervised co-training. Empirical
Methods in Natural Language Processing (EMNLP), 2019.
9
[41] Durk P Kingma,
Shakir Mohamed,
Danilo Jimenez
Rezende, and Max Welling. Semi-supervised learning with
deep generative models. In Advances in neural information
processing systems, pages 3581–3589, 2014. 9
[42] Thomas N Kipf and Max Welling. Semi-supervised classi-
ﬁcation with graph convolutional networks. arXiv preprint
arXiv:1609.02907, 2016. 9
[43] Alexander Kolesnikov, Lucas Beyer, Xiaohua Zhai, Joan
Puigcerver, Jessica Yung, Sylvain Gelly, and Neil Houlsby.
Large scale learning of general visual representations for
transfer. arXiv preprint arXiv:1912.11370, 2019. 4
[44] Simon Kornblith, Jonathon Shlens, and Quoc V Le. Do bet-
ter imagenet models transfer better? In Proceedings of the
IEEE Conference on Computer Vision and Pattern Recog-
nition, pages 2661–2671, 2019. 3
[45] Alex Krizhevsky, Ilya Sutskever, and Geoffrey E Hinton.
Imagenet classiﬁcation with deep convolutional neural net-
works. In Advances in Neural Information Processing Sys-
tems, pages 1097–1105, 2012. 1, 4
[46] Guokun Lai, Barlas Oguz, and Veselin Stoyanov. Bridg-
ing the domain gap in cross-lingual document classiﬁca-
tion. arXiv preprint arXiv:1909.07009, 2019. 9
[47] Samuli Laine and Timo Aila.
Temporal ensembling for
semi-supervised learning. In International Conference on
Learning Representations, 2017. 3, 9
[48] Dong-Hyun Lee. Pseudo-label: The simple and efﬁcient
semi-supervised learning method for deep neural networks.
In Workshop on Challenges in Representation Learning,
ICML, volume 3, page 2, 2013. 3, 9
[49] Yingting Li, Lu Liu, and Robby T Tan.
Certainty-
driven consistency loss for semi-supervised learning. arXiv
preprint arXiv:1901.05657, 2019. 9
[50] Chenxi Liu, Barret Zoph, Maxim Neumann, Jonathon
Shlens, Wei Hua, Li-Jia Li, Li Fei-Fei, Alan Yuille,
Jonathan Huang, and Kevin Murphy.
Progressive neural
architecture search. In Proceedings of the European Con-
ference on Computer Vision (ECCV), pages 19–34, 2018.
4
[51] Raphael Gontijo Lopes, Dong Yin, Ben Poole, Justin
Gilmer, and Ekin D Cubuk. Improving robustness with-
out sacriﬁcing accuracy with patch gaussian augmentation.
arXiv preprint arXiv:1906.02611, 2019. 5
[52] Yucen Luo, Jun Zhu, Mengxi Li, Yong Ren, and Bo Zhang.
Smooth neighbors on teacher graphs for semi-supervised
learning. In Proceedings of the IEEE Conference on Com-
puter Vision and Pattern Recognition, pages 8896–8905,
2018. 9
[53] Lars
Maaløe,
Casper
Kaae
Sønderby,
Søren
Kaae
Sønderby, and Ole Winther.
Auxiliary deep generative
models. arXiv preprint arXiv:1602.05473, 2016. 9
[54] Aleksander Madry, Aleksandar Makelov, Ludwig Schmidt,
Dimitris Tsipras, and Adrian Vladu. Towards deep learning
models resistant to adversarial attacks. International Con-
ference on Learning Representations, 2018. 7
[55] Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan,
Kaiming
He,
Manohar
Paluri,
Yixuan
Li,
Ashwin
Bharambe, and Laurens van der Maaten. Exploring the lim-
its of weakly supervised pretraining. In Proceedings of the
European Conference on Computer Vision (ECCV), pages
181–196, 2018. 1, 4, 5, 6, 9, 18
[56] Takeru Miyato, Shin-ichi Maeda, Shin Ishii, and Masanori
Koyama.
Virtual adversarial training:
a regularization
method for supervised and semi-supervised learning. IEEE
transactions on pattern analysis and machine intelligence,
2018. 3, 9
[57] Amir Najaﬁ, Shin-ichi Maeda, Masanori Koyama, and
Takeru Miyato. Robustness to adversarial perturbations in
learning from incomplete data. In Advances in Neural In-
formation Processing Systems, 2019. 9
[58] Jiquan Ngiam, Daiyi Peng, Vijay Vasudevan, Simon Ko-
rnblith, Quoc V Le, and Ruoming Pang.
Domain adap-
tive transfer learning with specialist models. arXiv preprint
arXiv:1811.07056, 2018. 3
[59] A Emin Orhan. Robustness properties of facebook’s resnext
wsl models. arXiv preprint arXiv:1907.07640, 2019. 5, 6

[60] Sungrae Park, JunKeon Park, Su-Jin Shin, and Il-Chul
Moon.
Adversarial dropout for supervised and semi-
supervised learning. In Thirty-Second AAAI Conference on
Artiﬁcial Intelligence, 2018. 9
[61] Sree Hari Krishnan Parthasarathi and Nikko Strom.
Lessons from building acoustic models with a million hours
of speech. In IEEE International Conference on Acoustics,
Speech and Signal Processing (ICASSP), pages 6670–6674.
IEEE, 2019. 8
[62] Siyuan Qiao, Wei Shen, Zhishuai Zhang, Bo Wang, and
Alan Yuille. Deep co-training for semi-supervised image
recognition. In Proceedings of the European Conference
on Computer Vision (ECCV), pages 135–152, 2018. 9
[63] Ilija Radosavovic, Piotr Doll´ar, Ross Girshick, Georgia
Gkioxari, and Kaiming He.
Data distillation: Towards
omni-supervised learning. In Proceedings of the IEEE Con-
ference on Computer Vision and Pattern Recognition, pages
4119–4128, 2018. 8
[64] Antti Rasmus, Mathias Berglund, Mikko Honkala, Harri
Valpola, and Tapani Raiko. Semi-supervised learning with
ladder networks. In Advances in neural information pro-
cessing systems, pages 3546–3554, 2015. 3, 9
[65] Esteban Real, Alok Aggarwal, Yanping Huang, and Quoc V
Le. Regularized evolution for image classiﬁer architecture
search. In Proceedings of the AAAI Conference on Artiﬁcial
Intelligence, volume 33, pages 4780–4789, 2019. 4
[66] Benjamin Recht, Rebecca Roelofs, Ludwig Schmidt, and
Vaishaal Shankar. Do imagenet classiﬁers generalize to im-
agenet?
International Conference on Machine Learning,
2019. 3, 4, 9
[67] Ellen Riloff. Automatically generating extraction patterns
from untagged text. In Proceedings of the national confer-
ence on artiﬁcial intelligence, pages 1044–1049, 1996. 8
[68] Ellen Riloff and Janyce Wiebe. Learning extraction pat-
terns for subjective expressions. In Proceedings of the 2003
conference on Empirical methods in natural language pro-
cessing, pages 105–112, 2003. 8
[69] Aruni Roy Chowdhury, Prithvijit Chakrabarty, Ashish
Singh, SouYoung Jin, Huaizu Jiang, Liangliang Cao, and
Erik G. Learned-Miller. Automatic adaptation of object de-
tectors to new domains using self-training. In Proceedings
of the IEEE Conference on Computer Vision and Pattern
Recognition, 2019. 8
[70] Tim Salimans, Ian Goodfellow, Wojciech Zaremba, Vicki
Cheung, Alec Radford, and Xi Chen. Improved techniques
for training gans. In Advances in neural information pro-
cessing systems, pages 2234–2242, 2016. 9
[71] H Scudder. Probability of error of some adaptive pattern-
recognition machines. IEEE Transactions on Information
Theory, 11(3):363–371, 1965. 2, 8
[72] Rico Sennrich, Barry Haddow, and Alexandra Birch. Im-
proving neural machine translation models with monolin-
gual data. arXiv preprint arXiv:1511.06709, 2015. 9
[73] Weiwei Shi, Yihong Gong, Chris Ding, Zhiheng MaX-
iaoyu Tao, and Nanning Zheng.
Transductive semi-
supervised deep learning using min-max features. In Pro-
ceedings of the European Conference on Computer Vision
(ECCV), pages 299–315, 2018. 3, 9
[74] Carl-Johann Simon-Gabriel, Yann Ollivier, Leon Bottou,
Bernhard Sch¨olkopf, and David Lopez-Paz. First-order ad-
versarial vulnerability of neural networks and input dimen-
sion. In International Conference on Machine Learning,
pages 5809–5817, 2019. 7
[75] Karen Simonyan and Andrew Zisserman. Very deep convo-
lutional networks for large-scale image recognition. In In-
ternational Conference on Learning Representations, 2015.
1
[76] Nitish Srivastava, Geoffrey Hinton, Alex Krizhevsky, Ilya
Sutskever, and Ruslan Salakhutdinov. Dropout: a simple
way to prevent neural networks from overﬁtting. The jour-
nal of machine learning research, 15(1):1929–1958, 2014.
1, 2, 4
[77] Robert Stanforth, Alhussein Fawzi, Pushmeet Kohli, et al.
Are labels required for improving adversarial robustness?
arXiv preprint arXiv:1905.13725, 2019. 9
[78] Qianru Sun, Xinzhe Li, Yaoyao Liu, Shibao Zheng, Tat-
Seng Chua, and Bernt Schiele. Learning to self-train for
semi-supervised few-shot classiﬁcation.
arXiv preprint
arXiv:1906.00562, 2019. 9
[79] Christian Szegedy, Sergey Ioffe, Vincent Vanhoucke, and
Alexander A Alemi. Inception-v4, inception-resnet and the
impact of residual connections on learning. In Thirty-First
AAAI Conference on Artiﬁcial Intelligence, 2017. 4
[80] Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet,
Scott Reed, Dragomir Anguelov, Dumitru Erhan, Vincent
Vanhoucke, and Andrew Rabinovich. Going deeper with
convolutions. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 1–9, 2015.
1, 4, 18
[81] Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon
Shlens, and Zbigniew Wojna.
Rethinking the inception
architecture for computer vision.
In Proceedings of the
IEEE conference on computer vision and pattern recogni-
tion, pages 2818–2826, 2016. 4
[82] Christian Szegedy, Wojciech Zaremba, Ilya Sutskever, Joan
Bruna, Dumitru Erhan, Ian Goodfellow, and Rob Fergus.
Intriguing properties of neural networks.
arXiv preprint
arXiv:1312.6199, 2013. 9
[83] Mingxing Tan and Quoc V Le. EfﬁcientNet: Rethinking
model scaling for convolutional neural networks. In Inter-
national Conference on Machine Learning, 2019. 1, 3, 4,
5, 14
[84] Antti Tarvainen and Harri Valpola. Mean teachers are bet-
ter role models: Weight-averaged consistency targets im-
prove semi-supervised deep learning results. In Advances in
Neural Information Processing Systems, pages 1195–1204,
2017. 3, 9
[85] Bart Thomee, David A Shamma, Gerald Friedland, Ben-
jamin Elizalde, Karl Ni, Douglas Poland, Damian Borth,
and Li-Jia Li. Yfcc100m: The new data in multimedia re-
search. Communications of the ACM, 59(2):64–73, 2016.
3, 17
[86] Hugo Touvron, Andrea Vedaldi, Matthijs Douze, and Herv´e
J´egou. Fixing the train-test resolution discrepancy. arXiv
preprint arXiv:1906.06423, 2019. 1, 3, 4, 9

[87] Andreas Veit, Neil Alldrin, Gal Chechik, Ivan Krasin, Ab-
hinav Gupta, and Serge Belongie.
Learning from noisy
large-scale datasets with minimal supervision. In Proceed-
ings of the IEEE Conference on Computer Vision and Pat-
tern Recognition, pages 839–847, 2017. 9
[88] Vikas Verma, Alex Lamb, Juho Kannala, Yoshua Bengio,
and David Lopez-Paz. Interpolation consistency training
for semi-supervised learning. In Proceedings of the Twenty-
Eighth International Joint Conference on Artiﬁcial Intelli-
gence (IJCAI-19), 2019. 9
[89] Jason Weston, Fr´ed´eric Ratle, Hossein Mobahi, and Ronan
Collobert. Deep learning via semi-supervised embedding.
In Neural Networks: Tricks of the Trade, pages 639–655.
Springer, 2012. 9
[90] Lijun Wu, Yiren Wang, Yingce Xia, QIN Tao, Jianhuang
Lai, and Tie-Yan Liu.
Exploiting monolingual data at
scale for neural machine translation. In Proceedings of the
2019 Conference on Empirical Methods in Natural Lan-
guage Processing and the 9th International Joint Confer-
ence on Natural Language Processing (EMNLP-IJCNLP),
pages 4198–4207, 2019. 9
[91] Qizhe Xie, Zihang Dai, Eduard Hovy, Minh-Thang Luong,
and Quoc V Le. Unsupervised data augmentation for con-
sistency training. arXiv preprint arXiv:1904.12848, 2019.
2, 3, 9
[92] Saining Xie, Ross Girshick, Piotr Doll´ar, Zhuowen Tu, and
Kaiming He. Aggregated residual transformations for deep
neural networks. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 1492–1500,
2017. 4
[93] I. Zeki Yalniz, Herv’e J’egou, Kan Chen, Manohar Paluri,
and Dhruv Mahajan. Billion-scale semi-supervised learning
for image classiﬁcation. Arxiv 1905.00546, 2019. 2, 4, 8,
15
[94] Zhilin Yang, William W Cohen, and Ruslan Salakhutdi-
nov. Revisiting semi-supervised learning with graph em-
beddings. arXiv preprint arXiv:1603.08861, 2016. 9
[95] Zhilin Yang,
Junjie Hu,
Ruslan Salakhutdinov,
and
William W Cohen.
Semi-supervised qa with generative
domain-adaptive nets.
arXiv preprint arXiv:1702.02206,
2017. 9
[96] David Yarowsky. Unsupervised word sense disambiguation
rivaling supervised methods. In 33rd annual meeting of the
association for computational linguistics, pages 189–196,
1995. 2, 8
[97] Runtian Zhai, Tianle Cai, Di He, Chen Dan, Kun He, John
Hopcroft, and Liwei Wang. Adversarially robust general-
ization just requires more unlabeled data. arXiv preprint
arXiv:1906.00555, 2019. 9
[98] Xiaohua Zhai, Avital Oliver, Alexander Kolesnikov, and
Lucas Beyer. S4L: Self-supervised semi-supervised learn-
ing. In Proceedings of the IEEE international conference
on computer vision, 2019. 9
[99] Richard Zhang.
Making convolutional networks shift-
invariant again. In International Conference on Machine
Learning, 2019. 5
[100] Xingcheng Zhang, Zhizhong Li, Chen Change Loy, and
Dahua Lin. Polynet: A pursuit of structural diversity in
very deep networks. In Proceedings of the IEEE Confer-
ence on Computer Vision and Pattern Recognition, pages
718–726, 2017. 4
[101] Giulio Zhou, Subramanya Dulloor, David G Andersen, and
Michael Kaminsky. Edf: Ensemble, distill, and fuse for
easy video labeling.
arXiv preprint arXiv:1812.03626,
2018. 9
[102] Xiaojin Zhu, Zoubin Ghahramani, and John D Lafferty.
Semi-supervised learning using gaussian ﬁelds and har-
monic functions. In Proceedings of the 20th International
conference on Machine learning (ICML-03), pages 912–
919, 2003. 9
[103] Xiaojin Jerry Zhu. Semi-supervised learning literature sur-
vey.
Technical report, University of Wisconsin-Madison
Department of Computer Sciences, 2005. 3, 9
[104] Barret Zoph, Vijay Vasudevan, Jonathon Shlens, and
Quoc V Le. Learning transferable architectures for scal-
able image recognition. In Proceedings of the IEEE con-
ference on computer vision and pattern recognition, pages
8697–8710, 2018. 4

A. Experiments
A.1. Architecture Details
The architecture speciﬁcations of EfﬁcientNet-L2 are
listed in Table 8. We also list EfﬁcientNet-B7 as a refer-
ence. Scaling width and resolution by c leads to an increase
factor of c2 in training time and scaling depth by c leads to
an increase factor of c. The training time of EfﬁcientNet-L2
is around 5 times the training time of EfﬁcientNet-B7.
Architecture Name
w
d
Train Res.
Test Res.
# Params
EfﬁcientNet-B7
2.0
3.1
600
600
66M
EfﬁcientNet-L2
4.3
5.3
475
800
480M
Table 8: Architecture speciﬁcations for EfﬁcientNets used
in the paper. The width w and depth d are the scaling factors
that need to be contextualized in EfﬁcientNet [83]. Train
Res. and Test Res. denote training and testing resolutions
respectively.
A.2. Ablation Studies
In this section, we provide comprehensive studies of var-
ious components of our method. Since iterative training re-
sults in longer training time, we conduct ablation without it.
To further save training time, we reduce the training epochs
for small models from 700 to 350, starting from Study #4.
We also set the unlabeled batch size to be the same as the
labeled batch size for models smaller than EfﬁcientNet-B7
starting from Study #2.
Study #1: Teacher Model’s Capacity.
Here, we study if
using a larger and better teacher model would lead to better
results. We use our best model Noisy Student Training with
EfﬁcientNet-L2, that achieves a top-1 accuracy of 88.4%, to
teach student models with sizes ranging from EfﬁcientNet-
B0 to EfﬁcientNet-B7. We use the standard augmentation
instead of RandAugment on unlabeled data in this experi-
ment to give the student model more capacity. This setting
is in principle similar to distillation on unlabeled data.
The comparison is shown in Table 9. Using Noisy Stu-
dent Training (EfﬁcientNet-L2) as the teacher leads to an-
other 0.5% to 1.6% improvement on top of the improved
results by using the same model as the teacher. For exam-
ple, we can train a medium-sized model EfﬁcientNet-B4,
which has fewer parameters than ResNet-50, to an accuracy
of 85.3%. Therefore, using a large teacher model with bet-
ter performance leads to better results.
Study #2: Unlabeled Data Size.
Next, we conduct exper-
iments to understand the effects of using different amounts
of unlabeled data. We start with the 130M unlabeled im-
ages and gradually reduce the unlabeled set. We experiment
Model
# Params
Top-1 Acc.
Top-5 Acc.
EfﬁcientNet-B0
5.3M
77.3%
93.4%
Noisy Student Training (B0)
78.1%
94.2%
Noisy Student Training (B0, L2)
78.8%
94.5%
EfﬁcientNet-B1
7.8M
79.2%
94.4%
Noisy Student Training (B1)
80.2%
95.2%
Noisy Student Training (B1, L2)
81.5%
95.8%
EfﬁcientNet-B2
9.2M
80.0%
94.9%
Noisy Student Training (B2)
81.1%
95.5%
Noisy Student Training (B2, L2)
82.4%
96.3%
EfﬁcientNet-B3
12M
81.7%
95.7%
Noisy Student Training (B3)
82.5%
96.4%
Noisy Student Training (B3, L2)
84.1%
96.9%
EfﬁcientNet-B4
19M
83.2%
96.4%
Noisy Student Training (B4)
84.4%
97.0%
Noisy Student Training (B4, L2)
85.3%
97.5%
EfﬁcientNet-B5
30M
84.0%
96.8%
Noisy Student Training (B5)
85.1%
97.3%
Noisy Student Training (B5, L2)
86.1%
97.8%
EfﬁcientNet-B6
43M
84.5%
97.0%
Noisy Student Training (B6)
85.9%
97.6%
Noisy Student Training (B6, L2)
86.4%
97.9%
EfﬁcientNet-B7
66M
85.0%
97.2%
Noisy Student Training (B7)
86.4%
97.9%
Noisy Student Training (B7, L2)
86.9%
98.1%
Table 9: Using our best model with 88.4% accuracy as
the teacher (denoted as Noisy Student Training (X, L2))
leads to more improvements than using the same model as
the teacher (denoted as Noisy Student Training (X)). Mod-
els smaller than EfﬁcientNet-B5 are trained for 700 epochs
(better than training for 350 epochs as used in Study #4 to
Study #8). Models other than EfﬁcientNet-B0 uses an unla-
beled batch size of three times the labeled batch size, while
other ablation studies set the unlabeled batch size to be the
same as labeled batch size by default for models smaller
than B7.
with using
1
128, 1
64, 1
32, 1
16, 1
4 of the whole data by uniformly
sampling images from the the unlabeled set for simplicity,
though taking images with highest conﬁdence may lead to
better results. We use EfﬁcientNet-B4 as both the teacher
and the student.
As can be seen from Table 10, the performance stays
similar when we reduce the data to
1
16 of the whole data,5
which amounts to 8.1M images after duplicating. The per-
formance drops when we further reduce it. Hence, using a
large amount of unlabeled data leads to better performance.
Study #3: Hard Pseudo-Label vs. Soft Pseudo-Label on
Out-of-domain Data.
Unlike previous studies in semi-
supervised learning that use in-domain unlabeled data (e.g.,
5A larger model might beneﬁt from more data while a small model with
limited capacity can easily saturate.

Data
1/128
1/64
1/32
1/16
1/4
1
Top-1 Acc.
83.4%
83.3%
83.7%
83.9%
83.8%
84.0%
Table 10: Noisy Student Training’s performance improves
with more unlabeled data.
Models are trained for 700
epochs without iterative training.
The baseline model
achieves an accuracy of 83.2%.
CIFAR-10 images as unlabeled data for a small CIFAR-
10 training set), to improve ImageNet, we must use out-
of-domain unlabeled data. Here we compare hard pseudo-
label and soft pseudo-label for out-of-domain data. Since
a teacher model’s conﬁdence on an image can be a good
indicator of whether it is an out-of-domain image, we
consider the high-conﬁdence images as in-domain im-
ages and the low-conﬁdence images as out-of-domain im-
ages. We sample 1.3M images in each conﬁdence interval
[0.0, 0.1], [0.1, 0.2], · · · , [0.9, 1.0].
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
Conﬁdence Interval
76.0
76.5
77.0
77.5
78.0
ImageNet Top-1 Accuracy (%)
Noisy Student Training (soft pseudo label)
Noisy Student Training (hard pseudo label)
EfﬁcientNet-B0
Figure 5: Soft pseudo labels lead to better performance
for low conﬁdence data (out-of-domain data). Each dot at
p represents a Noisy Student Training model trained with
1.3M ImageNet labeled images and 1.3M unlabeled images
with conﬁdence scores in [p, p + 0.1].
We use EfﬁcientNet-B0 as both the teacher model and
the student model and compare using Noisy Student Train-
ing with soft pseudo labels and hard pseudo labels. The re-
sults are shown in Figure 5 with the following observations:
(1) Soft pseudo labels and hard pseudo labels can both lead
to signiﬁcant improvements with in-domain unlabeled im-
ages i.e., high-conﬁdence images. (2) With out-of-domain
unlabeled images, hard pseudo labels can hurt the perfor-
mance while soft pseudo labels lead to robust performance.
Note that we have also observed that using hard pseudo
labels can achieve as good results or slightly better results
when a larger teacher is employed. Hence, whether soft
pseudo labels or hard pseudo labels work better might need
to be determined on a case-by-case basis.
Study #4: Student Model’s Capacity.
Then, we inves-
tigate the effects of student models with different capaci-
ties. For teacher models, we use EfﬁcientNet-B0, B2 and
B4 trained on labeled data and EfﬁcientNet-B7 trained us-
ing Noisy Student Training. We compare using a student
model with the same size or with a larger size. The com-
parison is shown in Table 11. With the same teacher, using
a larger student model leads to consistently better perfor-
mance, showing that using a large student model is impor-
tant to enable the student to learn a more powerful model.
Teacher
Teacher Acc.
Student
Student Acc.
B0
77.3%
B0
77.9%
B1
79.5%
B2
80.0%
B2
80.7%
B3
82.0%
B4
83.2%
B4
84.0%
B5
84.7%
B7
86.9%
B7
86.9%
L2
87.2%
Table 11: Using a larger student model leads to better per-
formance. Student models are trained for 350 epochs in-
stead of 700 epochs without iterative training.
The B7
teacher with an accuracy of 86.9% is trained by Noisy Stu-
dent Training with multiple iterations using B7. The com-
parison between B7 and L2 as student models is not com-
pletely fair for L2, since we use an unlabeled batch size
of 3x the labeled batch size for training L2, which is not
as good as using an unlabeled batch size of 7x the labeled
batch size when training B7 (See Study #7 for more details).
Study #5: Data Balancing.
Here, we study the neces-
sity of keeping the unlabeled data balanced across cate-
gories. As a comparison, we use all unlabeled data that
has a conﬁdence score higher than 0.3. We present results
with EfﬁcientNet-B0 to B3 as the backbone models in Ta-
ble 12. Using data balancing leads to better performance for
small models EfﬁcientNet-B0 and B1. Interestingly, the gap
becomes smaller for larger models such as EfﬁcientNet-B2
and B3, which shows that more powerful models can learn
from unbalanced data effectively. To enable Noisy Student
Training to work well for all model sizes, we use data bal-
ancing by default.
Study #6: Joint Training.
In our algorithm, we train
the model with labeled images and pseudo-labeled images
jointly. Here, we also compare with an alternative approach
used by Yalniz et al. [93], which ﬁrst pretrains the model

Model
B0
B1
B2
B3
Supervised Learning
77.3%
79.2%
80.0%
81.7%
Noisy Student Training
77.9%
79.9%
80.7%
82.1%
w/o Data Balancing
77.6%
79.6%
80.6%
82.1%
Table 12: Data balancing leads to better results for small
models. Models are trained for 350 epochs instead of 700
epochs without iterative training.
on pseudo-labeled images and then ﬁnetunes it on labeled
images. For ﬁnetuning, we experiment with different steps
and take the best results. The comparison is shown in Table
13.
It is clear that joint training signiﬁcantly outperforms
pretraining + ﬁnetuning.
Note that pretraining only on
pseudo-labeled images leads to a much lower accuracy than
supervised learning only on labeled data, which suggests
that the distribution of unlabeled data is very different from
that of labeled data. In this case, joint training leads to a
better solution that ﬁts both types of data.
Model
B0
B1
B2
B3
Supervised Learning
77.3%
79.2%
80.0%
81.7%
Pretraining
72.6%
75.1%
75.9%
76.5%
Pretraining + Finetuning
77.5%
79.4%
80.3%
81.7%
Joint Training
77.9%
79.9%
80.7%
82.1%
Table 13: Joint training works better than pretraining and
ﬁnetuning. We vary the ﬁnetuning steps and report the best
results. Models are trained for 350 epochs instead of 700
epochs without iterative training.
Study #7: Ratio between Unlabeled Batch Size and La-
beled Batch Size.
Since we use 130M unlabeled images
and 1.3M labeled images, if the batch sizes for unlabeled
data and labeled data are the same, the model is trained on
unlabeled data only for one epoch every time it is trained
on labeled data for a hundred epochs. Ideally, we would
also like the model to be trained on unlabeled data for more
epochs by using a larger unlabeled batch size so that it can
ﬁt the unlabeled data better. Hence we study the importance
of the ratio between unlabeled batch size and labeled batch
size.
In this study, we try a medium-sized model EfﬁcientNet-
B4 as well as a larger model EfﬁcientNet-L2. We use mod-
els of the same size as both the teacher and the student. As
shown in Table 14, the larger model EfﬁcientNet-L2 bene-
ﬁts from a large ratio while the smaller model EfﬁcientNet-
B4 does not. Using a larger ratio between unlabeled batch
size and labeled batch size, leads to substantially better per-
formance for a large model.
Teacher (Acc.)
Batch Size Ratio
Top-1 Acc.
B4 (83.2)
1:1
84.0%
3:1
84.0%
L2 (87.0)
1:1
86.7%
3:1
87.4%
L2 (87.4)
3:1
87.4%
6:1
87.9%
Table 14: With a ﬁxed labeled batch size, a larger unlabeled
batch size leads to better performance for EfﬁcientNet-L2.
The Batch Size Ratio denotes the ratio between unlabeled
batch size and labeled batch size.
Study #8: Warm-starting the Student Model.
Lastly,
one might wonder if we should train the student model from
scratch when it can be initialized with a converged teacher
model with good accuracy. In this ablation, we ﬁrst train an
EfﬁcientNet-B0 model on ImageNet and use it to initialize
the student model. We vary the number of epochs for train-
ing the student and use the same exponential decay learning
rate schedule. Training starts at different learning rates so
that the learning rate is decayed to the same value in all
experiments. As shown in Table 15, the accuracy drops sig-
niﬁcantly when we reduce the training epoch from 350 to
70 and drops slightly when reduced to 280 or 140. Hence,
the student still needs to be trained for a large number of
epochs even with warm-starting.
Further, we also observe that a student initialized with
the teacher can sometimes be stuck in a local optimal. For
example, when we use EfﬁcientNet-B7 with an accuracy of
86.4% as the teacher, the student model initialized with the
teacher achieves an accuracy of 86.4% halfway through the
training but gets stuck there when trained for 210 epochs,
while a model trained from scratch achieves an accuracy of
86.9%. Hence, though we can save training time by warm-
staring, we train our model from scratch to ensure the best
performance.
Warm-start
Initializing student with teacher
No Init
Epoch
35
70
140
280
350
Top-1 Acc.
77.4%
77.5%
77.7%
77.8%
77.9%
Table 15: A student initialized with the teacher still requires
at least 140 epochs to perform well. The baseline model,
trained with labeled data only, has an accuracy of 77.3%.
A.3. Results with a Different Architecture and
Dataset
Results with ResNet-50.
To study whether other archi-
tectures can beneﬁt from Noisy Student Training, we con-
duct experiments with ResNet-50 [30]. We use the full Im-
ageNet as the labeled data and the 130M images from JFT

as the unlabeled data. We train a ResNet-50 model on Im-
ageNet and use it as our teacher model. We use RandAug-
ment with the magnitude set to 9 as the noise.
The results are shown in Table 16. Noisy Student Train-
ing leads to an improvement of 1.3% on the baseline model,
which shows that Noisy Student Training is effective for ar-
chitectures other than EfﬁcientNet.
Method
Top-1 Acc.
Top-5 Acc.
ResNet-50
77.6%
93.8%
Noisy Student Training (ResNet-50)
78.9%
94.3%
Table 16: Experiments on ResNet-50.
Results on SVHN.
We also evaluate Noisy Student Train-
ing on a smaller dataset SVHN. We use the core set with
73K images as the training set and the validation set. The
extra set with 531K images are used as the unlabeled set.
We use EfﬁcientNet-B0 with strides of the second and the
third blocks set to 1 so that the ﬁnal feature map is 4x4 when
the input image size is 32x32.
As shown in Table 17, Noisy Student Training improves
the baseline accuracy from 98.1% to 98.6% and outper-
forms the previous state-of-the-art results achieved by Ran-
dAugment with Wide-ResNet-28-10.
Method
Accuracy
RandAugment (WRN)
98.3%
RandAugment (EfﬁcientNet-B0)
98.1%
Noisy Student Training (B0)
98.6%
Table 17: Results on SVHN.
A.4. Results on YFCC100M
Since JFT is not a public dataset, we also experiment
with a public unlabeled dataset YFCC100M [85], so that re-
searchers can make fair comparisons with our results. Sim-
ilar to the setting in Section 3.2, we experiment with differ-
ent model sizes without iterative training. We use the same
model for both the teacher and the student. We also use
the same hyperparamters when using JFT and YFCC100M.
Similar to the case for JFT, we ﬁrst ﬁlter images from Ima-
geNet validation set. We then ﬁlter low conﬁdence images
according to B0’s prediction and only keep the top 130K
images for each class according to the top-1 predicted class.
The resulting set has 34M images since there are not enough
images for most classes. We then balance the dataset and in-
crease it to 130M images. As a comparison, before the data
balancing stage, there are 81M images in JFT.
As shown in Table 18, Noisy Student Training also leads
to signiﬁcant improvements using YFCC100M though it
Model
Dataset
Top-1 Acc.
Top-5 Acc.
EfﬁcientNet-B0
-
77.3%
93.4%
Noisy Student Training (B0)
YFCC
79.9%
95.0%
Noisy Student Training (B0)
JFT
78.1%
94.2%
EfﬁcientNet-B1
-
79.2%
94.4%
Noisy Student Training (B1)
YFCC
79.9%
95.0%
Noisy Student Training (B1)
JFT
80.2%
95.2%
EfﬁcientNet-B2
-
80.0%
94.9%
Noisy Student Training (B2)
YFCC
81.0%
95.6%
Noisy Student Training (B2)
JFT
81.1%
95.5%
EfﬁcientNet-B3
-
81.7%
95.7%
Noisy Student Training (B3)
YFCC
82.3%
96.2%
Noisy Student Training (B3)
JFT
82.5%
96.4%
EfﬁcientNet-B4
-
83.2%
96.4%
Noisy Student Training (B4)
YFCC
84.2%
96.9%
Noisy Student Training (B4)
JFT
84.4%
97.0%
EfﬁcientNet-B5
-
84.0%
96.8%
Noisy Student Training (B5)
YFCC
85.0%
97.2%
Noisy Student Training (B5)
JFT
85.1%
97.3%
EfﬁcientNet-B6
-
84.5%
97.0%
Noisy Student Training (B6)
YFCC
85.4%
97.5%
Noisy Student Training (B6)
JFT
85.6%
97.6%
EfﬁcientNet-B7
-
85.0%
97.2%
Noisy Student Training (B7)
YFCC
86.2%
97.9%
Noisy Student Training (B7)
JFT
86.4%
97.9%
Table 18: Results using YFCC100M and JFT as the unla-
beled dataset.
achieves better performance using JFT. The performance
difference is probably due to the dataset size difference.
A.5. Details of Robustness Benchmarks
Metrics.
For completeness, we provide brief descriptions
of metrics used in robustness benchmarks ImageNet-A,
ImageNet-C and ImageNet-P.
• ImageNet-A. The top-1 and top-5 accuracy are mea-
sured on the 200 classes that ImageNet-A includes.
The mapping from the 200 classes to the original Ima-
geNet classes are available online.6
• ImageNet-C. mCE (mean corruption error) is the
weighted average of error rate on different corruptions,
with AlexNet’s error rate as a baseline.
The score
is normalized by AlexNet’s error rate so that corrup-
tions with different difﬁculties lead to scores of a sim-
ilar scale. Please refer to [31] for details about mCE
and AlexNet’s error rate. The top-1 accuracy is simply
the average top-1 accuracy for all corruptions and all
severity degrees. The top-1 accuracy of prior methods
are computed from their reported corruption error on
each corruption.
6https://github.com/hendrycks/natural-adv-
examples/blob/master/eval.py

• ImageNet-P. Flip probability is the probability that the
model changes top-1 prediction for different pertur-
bations. mFR (mean ﬂip rate) is the weighted aver-
age of ﬂip probability on different perturbations, with
AlexNet’s ﬂip probability as a baseline. Please refer to
[31] for details about mFR and AlexNet’s ﬂip proba-
bility. The top-1 accuracy reported in this paper is the
average accuracy for all images included in ImageNet-
P.
On
Using
RandAugment
for
ImageNet-C
and
ImageNet-P.
Since Noisy Student Training leads to
signiﬁcant improvements on ImageNet-C and ImageNet-P,
we brieﬂy discuss the inﬂuence of RandAugment on
robustness results. First, note that our supervised baseline
EfﬁcientNet-L2 also uses RandAugment.
Noisy Student
Training leads to signiﬁcant improvements when compared
to the supervised baseline as shown in Table 4 and Table 5.
Second, the overlap between transformations of Ran-
dAugment and ImageNet-C, P is small. For completeness,
we list transformations in RandAugment and corruptions
and perturbations in ImageNet-C and ImageNet-P here:
• RandAugment transformations: AutoContrast, Equal-
ize, Invert, Rotate, Posterize, Solarize, Color, Con-
trast, Brightness, Sharpness, ShearX, ShearY, Trans-
lateX and TranslateY.
• Corruptions in ImageNet-C: Gaussian Noise, Shot
Noise, Impulse Noise, Defocus Blur, Frosted Glass
Blur, Motion Blur, Zoom Blur, Snow, Frost, Fog,
Brightness, Contrast, Elastic, Pixelate, JPEG.
• Perturbations in ImageNet-P: Gaussian Noise, Shot
Noise, Motion Blur, Zoom Blur, Snow, Brightness,
Translate, Rotate, Tilt, Scale.
The
main
overlap
between
RandAugment
and
ImageNet-C are Contrast,
Brightness and Sharpness.
Among them,
augmentation Contrast and Brightness
are also used in ResNeXt-101 WSL [55] and in vision
models that uses the Inception preprocessing [34, 80]. The
overlap between RandAugment and ImageNet-P includes
Brightness, Translate and Rotate.

