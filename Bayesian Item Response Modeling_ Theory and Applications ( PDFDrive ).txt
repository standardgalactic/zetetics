
Statistics for Social and Behavioral Sciences
Advisors:
S.E. Fienberg
W.J. van der Linden
For other titles published in this series, go to
http://www.springer.com/series/3463


 
Jean-Paul Fox 
Bayesian Item Response Modeling
Theory and Applications 

Springer New York Dordrecht Heidelberg London 
 
 
subject to proprietary rights. 
 
Printed on acid-free paper 
 
Springer is part of Springer Science+Business Media (www.springer.com) 
© Springer Science+Business Media, LLC 2010 
All rights reserved. This work may not be translated or copied in whole or in part without the 
written permission of the publisher (Springer Science+Business Media, LLC, 233 Spring Street, 
Use in connection with any form of information storage and retrieval, electronic adaptation, computer 
The use in this publication of trade names, trademarks, service marks, and similar terms, even if they 
are not identified as such, is not to be taken as an expression of opinion as to whether or not they are 
software, or by similar or dissimilar methodology now known or hereafter developed is forbidden. 
ISBN 978-1-4419-0741-7 
New York, NY 10013, USA), except for brief excerpts in connection with reviews or scholarly analysis. 
e-ISBN 978-1-4419-0742-4
DOI 10.1007/978-1-4419-0742-4 
Series Editors
Department of Statistics
Carnegie Mellon University
Pittsburgh, PA 15213-3890
USA
Wim J. van der Linden
CTB/McGraw-Hill
20 Ryan Ranch Road
Monterey, CA 93940
USA
Jean-Paul Fox 
University of Twente 
7500 AE Enschede 
Faculty of Behavioral Sciences 
Measurement, and Data Analysis
Department of Research Methodology,
The Netherlands
Library of Congress Control Number: 2010927930
Stephen E. Fienberg

To Jasmijn and Kae


Preface
The modeling of item response data is governed by item response theory, also
referred to as modern test theory. The ﬁeld of inquiry of item response theory
has become very large and shows the enormous progress that has been made.
The mainstream literature is focused on frequentist statistical methods for es-
timating model parameters and evaluating model ﬁt. However, the Bayesian
methodology has shown great potential, particularly for making further im-
provements in the statistical modeling process.
The Bayesian approach has two important features that make it attractive
for modeling item response data. First, it enables the possibility of incorporat-
ing nondata information beyond the observed responses into the analysis. The
Bayesian methodology is also very clear about how additional information can
be used. Second, the Bayesian approach comes with powerful simulation-based
estimation methods. These methods make it possible to handle all kinds of
priors and data-generating models.
One of my motives for writing this book is to give an introduction to
the Bayesian methodology for modeling and analyzing item response data. A
Bayesian counterpart is presented to the many popular item response theory
books (e.g., Baker and Kim 2004; De Boeck and Wilson, 2004; Hambleton and
Swaminathan, 1985; van der Linden and Hambleton, 1997) that are mainly
or completely focused on frequentist methods. The usefulness of the Bayesian
methodology is illustrated by discussing and applying a range of Bayesian
item response models.
Complex Assessments
The recognition of the complexity of the processes leading to responses on
test items stimulated the development of more realistic response models. The
ﬂexibility of the Bayesian modeling framework makes it particularly useful for
making proper adjustments when the response data violate common model
assumptions. Such violations might appear due to ﬂaws in the data collection
procedure or the complexity of the sample design.

VIII
Preface
Individuals’ performances are best measured under controlled conditions
such that other sources of variation are under control. An experimenter will
choose a test for measuring a construct consisting of a set of items that min-
imizes individual variation and emphasize diﬀerences between subjects. Vari-
ability between respondents due to factors other than the construct under
consideration is not desirable. Measurement models can become quite com-
plex when they are adjusted in such a way that all sources of variation are
taken into account. Flaws in experimental setups may engender the need for
more complex measurement models. The impact of context eﬀects on assess-
ment results may further complicate the modeling process. Context eﬀects
appear when items function diﬀerently due to factors like item positioning or
other material correlated with the item. Test data that violate assumptions
of common item response models require a more ﬂexible model that accounts
for the violations.
Other complexities may arise due to the fact that besides response in-
formation other kinds of information are known (e.g., individual or group
characteristics, response times, item characteristics). Diﬀerent sampling de-
signs can be used to sample respondents and/or items (e.g., adaptive item
sampling, multistage sampling, randomized response sampling, simple ran-
dom sampling, stratiﬁed sampling). Diﬀerent response formats (e.g., multiple
choice, binary, polytomous) and the presence of clusters of items may further
stimulate the use of a more complex measurement model.
Besides introducing the Bayesian methodology, my aim has been to write
a book to introduce Bayesian modeling of response data from complex as-
sessments. Often information beyond the observed response data is available
that can be used. The Bayesian modeling framework will prove to be very
ﬂexible, allowing simultaneous estimation of model parameters, computation
of complex statistics, and simultaneous hypothesis testing.
In the 1990s, Bayesian inference became feasible with the introduction of
Bayesian computational methods such as computer simulation and Monte
Carlo techniques. The development of powerful computational simulation
techniques induced a tremendous positive change in the applicability of
Bayesian methodology. This led to the development of more ﬂexible statistical
models for test theory but also diﬀerent strategies with respect to parame-
ter estimation and hypothesis testing. In this book, the Bayesian way of item
response modeling combined with the development of powerful numerical sim-
ulation techniques that led to a new research area in modern test theory is
outlined.
Outside the Scope of This Book
Designing tests and testing whether tests are suited for the intended purpose
are very complex subjects. Various questions need to be answered with respect
to the response format of the test, the purpose of the test, and the construction
of test materials, among others. The tests developed should also be reliable

Preface
IX
and valid; that is, consistently result in scores that reﬂect the construct level of
each respondent and measure what they are supposed to measure. Good tests
are discriminating in the sense that they show diﬀerences in the construct level
of respondents. There are a number of sources where this information is readily
available. For classical test theory, see, for example, Gulliksen (1950), and Lord
and Novick (1968), and for item response theory, see, for example, Lord and
Novick (1968) and Lord (1980). A manual of standards for the construction
and use of tests has been prepared by a joint committee of the American
Educational Research Association, American Psychological Association and
National Council of Measurement in Education (2000).
Overview
Statistical computations are necessary for applying the Bayesian methodol-
ogy, and some programming skills are needed. That is, some familiarity with a
statistical software package like R or S+ is needed to perform Bayesian anal-
ysis. On the one hand, this book aims to serve those who just want to apply
the models, and they can use the software implemented in R packages and
S+ programs (see Section 1.5). On the other hand, others may want to learn
via programming and/or implement codes by themselves to extend models or
adjust priors. For them, the mathematical details of the estimation procedures
are discussed in the book, and the computer codes are provided via a website
associated with the book. To understand the material, a basic background in
probability and statistics is needed, including some familiarity with matrix al-
gebra at the undergraduate level. The contents as well as the algorithms with
their implementations make this book self-contained. Hopefully, it will provide
an introduction to the essential features of Bayesian item response modeling
as well as a better understanding of more advanced topics. The contents, pro-
grams, and codes will hopefully help readers implement their own algorithms
and build their own set of tools for Bayesian item response modeling.
The book is organized as follows. In Chapter 1, the typical structure of
item response data and the common item response models are discussed. Ba-
sic elements of Bayesian response modeling are introduced together with the
basic building blocks for making Bayesian statistical inferences. WinBUGS is
used to illustrate the Bayesian modeling approach. Chapter 2 presents a hier-
archical modeling approach that supports the pooling of information, which
becomes important when typically limited information is observed about
many individuals. The Bayesian hierarchical modeling approach is outlined,
which has tremendous potential with the current developments in statisti-
cal computing. Before discussing various sampling-based estimation methods
for Bayesian item response models, which will be discussed in Chapter 4, in
Chapter 3 a more general introduction is given to sampling-based estimation
methods, testing hypotheses, and methods for model selection. Chapter 5 dis-
cusses methods for testing hypotheses and for model selection for the Bayesian
item response models described in Chapter 4.

X
Preface
In Chapters 6–9, more advanced item response models are discussed for
response data from complex assessments, response and response time data,
and responses from complex sampling designs. In Chapter 6, respondents are
assumed to be nested in groups (e.g., schools, countries). A hierarchical popu-
lation model for respondents is deﬁned to account for the within- and between-
group dependencies. In Chapter 7, models for relaxing common measurement
invariance restrictions are discussed. Chapter 8 introduces the multivariate
analysis of responses and response times for measuring the speed and accu-
racy of working. Chapter 9 introduces models for (randomized) response data
that are masked before they are observed to invite respondents to answer hon-
estly when asked sensitive questions. Several empirical examples are presented
to illustrate the methods and the usefulness of the Bayesian approach.
Acknowledgments
I would like to thank numerous people for their assistance and/or input dur-
ing the writing of this book. I acknowledge the input from collaborators on
earlier research projects that were addressed in the book. The cooperation of
Rinke Klein Entink, Cees Glas, Wim van der Linden, Martijn de Jong, and
Jan-Benedict Steenkamp has been greatly appreciated. I am indebted to Jim
Albert, who provided very helpful comments on earlier drafts. Cheryl Wyrick
has kindly provided data for a randomized response application. I thank my
colleagues at the Department of Research Methodology, Measurement, and
Data Analysis at the University of Twente. My colleagues Rinke Klein Entink
and Josine Verhagen read drafts of the book, and their suggestions and com-
ments led to its substantial improvement. I also thank John Kimmel for his
conﬁdence and assistance during the preparation of the book. The VIDI grant
of the Netherlands Organization for Scientiﬁc Research (NWO) supported the
writing of this book, for which I am most grateful.
Finally, I thank my wife, Miranda, for her support, encouragement, and
patience during the writing of this book.
University of Twente, Enschede
Jean-Paul Fox
March 2010

Contents
Preface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . VII
1
Introduction to Bayesian Response Modeling . . . . . . . . . . . . . .
1
1.1
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.1.1
Item Response Data Structures . . . . . . . . . . . . . . . . . . . . .
3
1.1.2
Latent Variables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5
1.2
Traditional Item Response Models . . . . . . . . . . . . . . . . . . . . . . . . .
6
1.2.1
Binary Item Response Models . . . . . . . . . . . . . . . . . . . . . . .
7
1.2.2
Polytomous Item Response Models . . . . . . . . . . . . . . . . . . 12
1.2.3
Multidimensional Item Response Models . . . . . . . . . . . . . 14
1.3
The Bayesian Approach . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15
1.3.1
Bayes’ Theorem . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 16
1.3.2
Posterior Inference . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 20
1.4
A Motivating Example Using WinBUGS . . . . . . . . . . . . . . . . . . . 21
1.4.1
Modeling Examinees’ Test Results . . . . . . . . . . . . . . . . . . . 21
1.5
Computation and Software . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 24
1.6
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 27
2
Bayesian Hierarchical Response Modeling . . . . . . . . . . . . . . . . . 31
2.1
Pooling Strength . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 31
2.2
From Beliefs to Prior Distributions . . . . . . . . . . . . . . . . . . . . . . . . 33
2.2.1
Improper Priors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 38
2.2.2
A Hierarchical Bayes Response Model . . . . . . . . . . . . . . . . 39
2.3
Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 42
2.4
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43
3
Basic Elements of Bayesian Statistics . . . . . . . . . . . . . . . . . . . . . . 45
3.1
Bayesian Computational Methods . . . . . . . . . . . . . . . . . . . . . . . . . 45
3.1.1
Markov Chain Monte Carlo Methods . . . . . . . . . . . . . . . . 46
3.2
Bayesian Hypothesis Testing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 51
3.2.1
Computing the Bayes Factor . . . . . . . . . . . . . . . . . . . . . . . . 54

XII
Contents
3.2.2
HPD Region Testing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 58
3.2.3
Bayesian Model Choice. . . . . . . . . . . . . . . . . . . . . . . . . . . . . 59
3.3
Discussion and Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . 61
3.4
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 62
4
Estimation of Bayesian Item Response Models . . . . . . . . . . . . . 67
4.1
Marginal Estimation and Integrals . . . . . . . . . . . . . . . . . . . . . . . . . 67
4.2
MCMC Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 71
4.3
Exploiting Data Augmentation Techniques. . . . . . . . . . . . . . . . . . 73
4.3.1
Latent Variables and Latent Responses . . . . . . . . . . . . . . . 74
4.3.2
Binary Data Augmentation . . . . . . . . . . . . . . . . . . . . . . . . . 75
4.3.3
TIMMS 2007: Dutch Sixth-Graders’
Math Achievement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 81
4.3.4
Ordinal Data Augmentation . . . . . . . . . . . . . . . . . . . . . . . . 83
4.4
Identiﬁcation of Item Response Models . . . . . . . . . . . . . . . . . . . . . 86
4.4.1
Data Augmentation and Identifying Assumptions . . . . . . 87
4.4.2
Rescaling and Priors with Identifying Restrictions . . . . . 88
4.5
Performance MCMC Schemes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 89
4.5.1
Item Parameter Recovery . . . . . . . . . . . . . . . . . . . . . . . . . . 89
4.5.2
Hierarchical Priors and Shrinkage . . . . . . . . . . . . . . . . . . . 92
4.6
European Social Survey: Measuring Political Interest . . . . . . . . . 95
4.7
Discussion and Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . 98
4.8
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 99
5
Assessment of Bayesian Item Response Models . . . . . . . . . . . . 107
5.1
Bayesian Model Investigation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107
5.2
Bayesian Residual Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 108
5.2.1
Bayesian Latent Residuals . . . . . . . . . . . . . . . . . . . . . . . . . . 109
5.2.2
Computation of Bayesian Latent Residuals . . . . . . . . . . . 109
5.2.3
Detection of Outliers. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 110
5.2.4
Residual Analysis: Dutch Primary School Math Test . . . 111
5.3
HPD Region Testing and Bayesian Residuals . . . . . . . . . . . . . . . . 112
5.3.1
Measuring Alcohol Dependence: Graded Response
Analysis. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 116
5.4
Predictive Assessment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 117
5.4.1
Prior Predictive Assessment . . . . . . . . . . . . . . . . . . . . . . . . 119
5.4.2
Posterior Predictive Assessment . . . . . . . . . . . . . . . . . . . . . 122
5.5
Illustrations of Predictive Assessment . . . . . . . . . . . . . . . . . . . . . . 126
5.5.1
The Observed Score Distribution . . . . . . . . . . . . . . . . . . . . 126
5.5.2
Detecting Testlet Eﬀects . . . . . . . . . . . . . . . . . . . . . . . . . . . 127
5.6
Model Comparison and Information Criteria . . . . . . . . . . . . . . . . 130
5.6.1
Dutch Math Data: Model Comparison . . . . . . . . . . . . . . . 131
5.7
Summary and Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 131
5.8
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133
5.9
Appendix: CAPS Questionnaire . . . . . . . . . . . . . . . . . . . . . . . . . . . 139

Contents
XIII
6
Multilevel Item Response Theory Models . . . . . . . . . . . . . . . . . . 141
6.1
Introduction: School Eﬀectiveness Research . . . . . . . . . . . . . . . . . 141
6.2
Nonlinear Mixed Eﬀects Models . . . . . . . . . . . . . . . . . . . . . . . . . . . 142
6.3
The Multilevel IRT Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 145
6.3.1
A Structural Multilevel Model . . . . . . . . . . . . . . . . . . . . . . 145
6.3.2
The Synthesis of IRT and Structural
Multilevel Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 148
6.4
Estimating Level-3 Residuals: School Eﬀects . . . . . . . . . . . . . . . . 153
6.5
Simultaneous Parameter Estimation of MLIRT . . . . . . . . . . . . . . 158
6.6
Applications of MLIRT Modeling . . . . . . . . . . . . . . . . . . . . . . . . . . 162
6.6.1
Dutch Primary School Mathematics Test . . . . . . . . . . . . . 162
6.6.2
PISA 2003: Dutch Math Data . . . . . . . . . . . . . . . . . . . . . . . 165
6.6.3
School Eﬀects in the West Bank: Covariate Error . . . . . . 172
6.6.4
MMSE: Individual Trajectories of
Cognitive Impairment . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 174
6.7
Summary and Further Reading . . . . . . . . . . . . . . . . . . . . . . . . . . . . 181
6.8
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 183
6.9
Appendix: The Expected School Eﬀect . . . . . . . . . . . . . . . . . . . . . 188
6.10 Appendix: Likelihood MLIRT Model . . . . . . . . . . . . . . . . . . . . . . . 190
7
Random Item Eﬀects Models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 193
7.1
Random Item Parameters . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 193
7.1.1
Measurement Invariance . . . . . . . . . . . . . . . . . . . . . . . . . . . . 194
7.1.2
Random Item Eﬀects Prior . . . . . . . . . . . . . . . . . . . . . . . . . 195
7.2
A Random Item Eﬀects Response Model . . . . . . . . . . . . . . . . . . . 198
7.2.1
Handling the Clustering of Respondents . . . . . . . . . . . . . . 203
7.2.2
Explaining Cross-national Variation . . . . . . . . . . . . . . . . . 203
7.2.3
The Likelihood for the Random Item Eﬀects Model . . . . 204
7.3
Identiﬁcation: Linkage Between Countries . . . . . . . . . . . . . . . . . . 205
7.3.1
Identiﬁcation Without (Designated) Anchor Items . . . . . 206
7.3.2
Concluding Remarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 208
7.4
MCMC: Handling Order Restrictions . . . . . . . . . . . . . . . . . . . . . . 209
7.4.1
Sampling Threshold Values via an M-H Algorithm . . . . . 209
7.4.2
Sampling Threshold Values via Gibbs Sampling . . . . . . . 211
7.4.3
Simultaneous Estimation via MCMC. . . . . . . . . . . . . . . . . 212
7.5
Tests for Invariance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 214
7.6
International Comparisons of Student Achievement . . . . . . . . . . 216
7.7
Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 221
7.8
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 222
8
Response Time Item Response Models . . . . . . . . . . . . . . . . . . . . 227
8.1
Mixed Multivariate Response Data . . . . . . . . . . . . . . . . . . . . . . . . 227
8.2
Measurement Models for Ability and Speed . . . . . . . . . . . . . . . . . 228
8.3
Joint Modeling of Responses and Response Times . . . . . . . . . . . 231
8.3.1
A Structural Multivariate Multilevel Model . . . . . . . . . . . 232

XIV
Contents
8.3.2
The RTIRT Likelihood Model . . . . . . . . . . . . . . . . . . . . . . . 234
8.4
RTIRT Model Prior Speciﬁcations
. . . . . . . . . . . . . . . . . . . . . . . . 235
8.4.1
Multivariate Prior Model for the Item Parameters . . . . . 235
8.4.2
Prior for ΣP with Identifying Restrictions . . . . . . . . . . . . 236
8.5
Exploring the Multivariate Normal Structure . . . . . . . . . . . . . . . 238
8.6
Model Selection Using the DIC . . . . . . . . . . . . . . . . . . . . . . . . . . . . 241
8.7
Model Fit via Residual Analysis . . . . . . . . . . . . . . . . . . . . . . . . . . . 242
8.8
Simultaneous Estimation of RTIRT . . . . . . . . . . . . . . . . . . . . . . . . 243
8.9
Natural World Assessment Test . . . . . . . . . . . . . . . . . . . . . . . . . . . 246
8.10 Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 248
8.11 Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 250
8.12 Appendix: DIC RTIRT Model . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 254
9
Randomized Item Response Models . . . . . . . . . . . . . . . . . . . . . . . 255
9.1
Surveys about Sensitive Topics . . . . . . . . . . . . . . . . . . . . . . . . . . . . 255
9.2
The Randomized Response Technique . . . . . . . . . . . . . . . . . . . . . . 256
9.2.1
Related and Unrelated Randomized
Response Designs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 257
9.3
Extending Randomized Response Models . . . . . . . . . . . . . . . . . . . 258
9.4
A Mixed Eﬀects Randomized Item Response Model . . . . . . . . . . 259
9.4.1
Individual Response Probabilities. . . . . . . . . . . . . . . . . . . . 259
9.4.2
A Structural Mixed Eﬀects Model . . . . . . . . . . . . . . . . . . . 261
9.5
Inferences from Randomized Item Response Data . . . . . . . . . . . . 262
9.5.1
MCMC Estimation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 265
9.5.2
Detecting Noncompliance Behavior . . . . . . . . . . . . . . . . . . 267
9.5.3
Testing for Fixed-Group Diﬀerences . . . . . . . . . . . . . . . . . 268
9.5.4
Model Choice and Fit . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 270
9.6
Simulation Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 272
9.6.1
Diﬀerent Randomized Response Sampling Designs . . . . . 272
9.6.2
Varying Randomized Response Design Properties . . . . . . 274
9.7
Cheating Behavior and Alcohol Dependence . . . . . . . . . . . . . . . . 275
9.7.1
Cheating Behavior at a Dutch University . . . . . . . . . . . . . 275
9.7.2
College Alcohol Problem Scale . . . . . . . . . . . . . . . . . . . . . . 279
9.8
Discussion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 284
9.9
Exercises . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 285
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 289
Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 309

1
Introduction to Bayesian Response Modeling
1.1 Introduction
In modern society, tests are used extensively in schools, industry, and gov-
ernment. Test results can be of value in counseling, treatment, and selection
of individuals. Tests can have a variety of functions, and often a broad clas-
siﬁcation is made in cognitive (tests as measures of ability) versus aﬀective
tests (tests designed to measure interest, attitudes, and other noncognitive
aspects).
The urge for testing increased in diﬀerent ﬁelds, and tests were used more
and more for various purposes, like evaluating the eﬃciency of educational sys-
tems and students’ learning progress, besides measuring persons and individ-
ual diﬀerences. Parallel to this development, an increasing public awareness of
the importance, limitations, and impact of testing led to much criticism. Both
stimulated the development of better tests and the improvement of statistical
methods for analyzing test scores. Dealing with common test problems such
as constructing tests and analyzing and interpreting results also encouraged
the development of modern test theory or item response theory (IRT). In the
second half of the twentieth century, item-based statistical models were used
for the measurement of subjective states like intelligence, arithmetic ability,
customer satisfaction, or neuroticism.
Originally, the item response models developed in the 1970s and 1980s were
mainly meant for analyzing item responses collected under standardized con-
ditions in real test situations. Assigning a score, determining its accuracy, and
comparing the results were the main targets. Today, these item response mod-
els are widely applied for their well-known scaling and measurement proper-
However, together with an increasing demand for testing, in the 1980s
and 1990s new possibilities in computer technology made it possible to collect
more data, improving the quality of data and the eﬃciency of data collec-
tion. The introduction of computer-assisted interviewing, Web-based surveys,
1
©
ties, supported by various commercial and noncommercial software packages,
and Behavioral Sciences, DOI 10.1007/978-1-4419-0742-4_1,
where diﬀerent types of response data require diﬀerent item response models.
J.-P. Fox, Bayesian Item Response Modeling: Theory and Applications, Statistics for Social
  Springer Science+Business Media, LLC 2010

2
1 Introduction to Bayesian Response Modeling
and statistical data centers, among other methods, has made the collection
of (response) data faster and more accurate. Nowadays, high-level data like
background data, data extracted from public registries, and administrative
data have become available via Web-enabled statistical databases. Typically,
in survey-based studies, more and more data are available, while the amount
of information is often limited at the individual level. Inferences are to be
made at the individual level and other ﬁner levels of aggregation, taking the
level of uncertainty into account.
Parallel to the availability of high-quality data, new questions arose that
were focused on addressing challenges such as complex response behavior (e.g.,
guessing, extreme responding), missingness and nonresponse, and complex
sampling designs. Data from large-scale assessments are often hierarchically
structured, where subjects are nested in groups, responses nested in subjects,
or items nested within units. The nesting leads to more complicated depen-
dency structures, with sources of variation at the diﬀerent levels of hierarchy.
The recognition of hierarchically structured data led to new challenges, like
accurately measuring subject diﬀerences and cross-level relationships when
accounting for nested sources of variation.
The increasing complexity of situations in which response data are col-
lected also posed new issues. For example, cross-national response observa-
tions are diﬃcult to interpret when the test characteristics are not invariant.
Cross-national diﬀerences can possibly be explained by social background dif-
ferences and measurement characteristic diﬀerences, but it is diﬃcult to iden-
tify the real eﬀects with a test that operates diﬀerently across countries. This
problem gets more complicated when socially desirable answers to sensitive
survey questions (e.g., about consumption of alcohol or use of illicit drugs)
are obtained where respondents intentionally distort or edit their item re-
sponses. Tests are often used as surveys such that the performance on the test
does not yield direct consequences for the respondent, with the eﬀect that the
amount of nonresponse increases signiﬁcantly. In more complex survey stud-
ies, basic item response model assumptions are often violated and threaten
the statistical inferences. One of the challenges is to account for respondent
heterogeneity, cross-classiﬁed hierarchical structures, and uncertainty at dif-
ferent hierarchical levels while at the same time making accurate inferences
at a disaggregate level.
To meet these challenges, a Bayesian approach to item response modeling
was started in the 1980s by Mislevy (1986), Rigdon and Tsutakawa (1983), and
Swaminathan and Giﬀord (1982, 1985), among others. The Bayesian model-
ing framework supports in a natural way extensions of common item response
models. The response model parameters are described via prior models at sep-
arate levels to account for diﬀerent sources of uncertainty, complex dependen-
cies, and other sources of information. This ﬂexibility in deﬁning prior models
for the item response model parameters is one of the strengths of Bayesian
modeling that makes it possible to handle for example more complex sampling
designs comprising complex dependency structures.

1.1 Introduction
3
In the 1980s, the conceptual elegance of the Bayesian approach had been
recognized, but major breakthroughs in computation were needed to make
a Bayesian modeling approach possible and attractive. Improved computa-
tional methods were needed to support a novel ﬂexible modeling approach
that, among other things, acts upon the discrete nature of response data and
handles relationships with higher-level data where standard distributional as-
sumptions do not apply. This breakthrough was accomplished with the intro-
duction of Markov chain Monte Carlo (MCMC) methods, which stimulated
in a profound way a Bayesian item response modeling approach. Since the
early 1990s, response modeling issues and problems of making inferences from
response data have been attacked in a completely Bayesian way without com-
putational obstacles. A key element was that the MCMC methods for simul-
taneous estimation remained straightforward as model complexity increased.
Speciﬁc problems related to the modeling of response data make certain
Bayesian methods very useful. However, before discussing the attractiveness
of Bayesian methods, typical characteristics of item response data and the use
of latent variables are discussed.
1.1.1 Item Response Data Structures
Response data can be characterized in diﬀerent ways, but a prominent feature
is that they come from respondents. The heterogeneity between respondents
is a typical source of variation in response data that needs to be accounted
for in a statistical response model. Generally, diﬀerences between respondents
are modeled via a probability distribution known as a respondents’ population
distribution, and inferences about respondents are always made with respect
to a population distribution, which will receive special attention throughout
this book.
Hierarchically Structured Data
In standard situations, respondents are assumed to be sampled independently
from each other. This standard sampling design is simple random sampling
with replacement from an inﬁnite population. In many situations, respondents
are clustered and the population of interest consists of subpopulations. The
observations are correlated within clusters and reﬂect that the clusters diﬀer in
certain ways. The observations are said to be hierarchically structured when
nested in clusters. Typically, response observations within each cluster are
not independently distributed, in contrast to (nonnested) observations from
diﬀerent clusters.
There are various examples of clustered (response) data. Longitudinal data
are hierarchically structured when subjects are measured repeatedly on the
same outcome at several points in time. When the number of measurements
and spacing of time points vary from subject to subject, the observations
are viewed as nested within subjects. A slightly more general term is repeated

4
1 Introduction to Bayesian Response Modeling
measurements, which refers to data on subjects measured repeatedly at diﬀer-
ent times or diﬀerent conditions. The term clustered data, which characterizes
the hierarchical structured nature of the data, is often used when observa-
tions are nested in geographical, political, or administrative units, or when
respondents are nested under an interviewer or within schools. In educational
research, response data are often doubly nested when observations are nested
within individuals and are in turn nested within organizations. Multivariate
data also contain a hierarchical structure since for each subject multiple out-
comes are measured that are nested within the subject.
There are diﬀerent terms used in the literature to characterize hierarchi-
cally structured data. The lowest level of the hierarchy is referred to as the
level-1, stage-1, micro-, or observational level. One higher level of the hierarchy
is referred to as level-2, stage-2, macro-, or cluster level. The terminology cho-
sen is that most appropriate to the context, and in the absence of a particular
context two levels of hierarchy are denoted by level 1 and level 2.
The heterogeneity between respondents is often of a complex nature, where
respondents (level 2) are nested in groups (level 3; e.g., schools, countries) and
responses (level 1) nested within individuals. Inferences have to be made at
diﬀerent levels of aggregation, and therefore a statistical model has to com-
prise the diﬀerent levels of analysis. The responses at the observational or
within-respondent level are explicitly modeled via a conditional likelihood
where typically conditional independence is assumed given a person parame-
ter. At a higher (hierarchical) level, a between-respondent model deﬁnes the
heterogeneity between respondents. Later on, it will be shown that hierarchi-
cally structured response data can be analyzed in a uniﬁed treatment of all
diﬀerent levels of analysis via a Bayesian modeling approach.
Response data are often sparse at the respondent level but are linked to
many respondents. This sparsity complicates an estimation procedure for ob-
taining reliable estimates of individual eﬀects. By borrowing strength from
the other individuals’ response data nested in the same group, improved esti-
mates of individual eﬀects can be obtained. In the same way, more accurate
estimates can be obtained at an aggregate level using the within-individual
data.
Response data are often integer-valued, where responses can be obtained
as correct or incorrect or are obtained on a ﬁve- or seven-point scale. The
lumpy nature of response data requires a special modeling approach since the
standard distributional assumptions do not apply.
Response data are often obtained in combination with other input vari-
ables. For example, response data are obtained from respondents together
with school information, and the object is to make joint inferences about
individual and school eﬀects given an outcome variable. In a Bayesian frame-
work, diﬀerent sources of information can be handled eﬃciently, accounting
for their level of uncertainty. It will be shown that the ﬂexibility of a Bayesian
modeling approach together with the powerful computational methods will
oﬀer an attractive set of tools for analyzing response data.

1.1 Introduction
5
1.1.2 Latent Variables
Various deﬁnitions of latent variables are given in the literature. In this book,
a latent variable is deﬁned as a random variable whose realizations cannot
be observed directly. It is obvious that a latent variable cannot be measured
directly or even in principle when it represents a hypothetical construct like
intelligence or motivation (e.g., Torgerson, 1958). An operational deﬁnition
states that the construct is related to the observable data. The relationship
is often deﬁned in such a way that item responses serve as indicators for
the measurement of the underlying construct. For example, common item
response models deﬁne a mathematical relationship between a person’s item
responses and a latent variable that represents the property of the person
that the items measure. In common situations, a latent variable appears as
a continuous random variable. It is also possible that a latent variable is
deﬁned to be categorical such that respondents are assigned to one of a set of
categories that may be ordered or unordered. Bartholomew and Knott (1999)
and Skrondal and Rabe-Hesketh (2004), among others, give a general overview
of latent variables and their uses in diﬀerent social science applications.
For various reasons, latent variables play an important role in the statis-
tical modeling of response data, especially in behavioral and social research.
First, as mentioned, the item responses are often assumed to be indicators
of an underlying construct or latent variable, and interest is focused on its
measurement. IRT deﬁnes a relationship between item responses and respon-
dents’ latent variable values. Second, the direct speciﬁcation of a joint dis-
tribution of the random observations is often extremely diﬃcult, and some
sort of summarization is needed to identify the interrelationships of the many
random observations. Latent variables can be used to deﬁne an underlying
structure to reduce the dimensionality of the data, and relationships can be
speciﬁed for a smaller set of variables. Third, discrete response outcomes are
often observed that can be regarded as a partial observation of an underly-
ing continuous variable. For example, it is often assumed that realizations
from a latent continuous variable are not observed but a censoring mecha-
nism produces discrete responses on a ﬁxed point scale. For binary responses,
a positive response is observed when an underlying continuous variable sur-
passes a threshold value, and a negative response is observed otherwise. The
latent continuous response formulation is very ﬂexible and can handle almost
all sorts of discrete responses, and it will be used extensively in subsequent
chapters. Then, other advantages of the latent response formulation will be
revealed.
In the following sections, some traditional item response models are re-
viewed from which extended Bayesian response models can be built. Then,
a general Bayesian response modeling framework is introduced that is used
throughout the rest of the book.

6
1 Introduction to Bayesian Response Modeling
1.2 Traditional Item Response Models
The literature on the development, description, and applications of item re-
sponse models for item-based tests is very rich and will not be repeated here.
Only a short overview of some popular item response models will be given,
including their assumptions. This introduction will also be used to introduce
the notation. The classic book of Lord and Novick (1968) is often cited as the
beginning of model-based statistical inference in educational and psycholog-
ical measurement. However, the development of item response models has a
longer history. A general and historical overview of item response theory can
be found in Baker and Kim (2004), Bock (1997), Embretson and Reise (2000),
and van der Linden and Hambleton (1997), among others. Item response mod-
els are sometimes introduced as an answer to shortcomings of classical test
theory (e.g., Hambleton, Swaminathan and Rogers, 1991; Thissen and Wainer,
2001).
IRT is concerned with the measurement of a hypothetical construct that
is latent and can only be measured indirectly via the measurement of other
manifest variables. This hypothetical construct is a latent variable and often
represents the ability, skill, or more generally a latent person characteristic
that the items measure. Throughout the entire book, the latent variable will
also be called an ability parameter as a generic name for the latent construct
that is measured by the items and will usually be denoted as θ. When the
latent variable refers to a person characteristic such as ability or proﬁciency,
it will also be called a person parameter.
Item response models have several desirable features. Most of these fea-
tures result from the fact that a common scale is deﬁned for the latent variable.
Item characteristic(s) and respondents’ characteristic(s) are both separately
parameterized within an item response model and are both invariant. This
means that the corresponding estimates are not test-dependent. Latent vari-
able estimates from diﬀerent sets of items measuring the same underlying con-
struct are comparable and diﬀer only due to measurement error. Estimates
of item characteristics from responses of diﬀerent samples of individuals from
the same population are comparable and diﬀer only due to sampling error.
There are two key assumptions involved in IRT. The ﬁrst assumption states
that a change in the latent variable leading to a change in the probability of
a speciﬁed response is completely described by the item characteristic curve
(ICC), item characteristic function, or trace line. This ICC speciﬁes how the
probability of an item response changes due to changes in the latent variable.
Diﬀerent mathematical forms of the item characteristic curves lead to diﬀerent
item response models. For dichotomous responses (correct or in agreement),
the probability of a success is modeled as a function of item and person pa-
rameters. The second assumption states that responses to a pair of items
are statistically independent when the underlying latent variable (the items
measure a unidimensional latent variable) is held constant. In that case, only
one (unidimensional) latent variable inﬂuences the item responses and local

1.2 Traditional Item Response Models
7
independence holds when the assumption of unidimensionality is true. The
assumption of local independence is easily generalized to a multidimensional
latent variable that states that responses to a pair of items are statistically
independent when the multidimensional latent variable is held constant.
A random vector of K responses is denoted as Yi, with observed values
yi = (yi1, yi2, . . . , yiK) of an individual indexed i with ability parameter θi.
Then the assumption of local independence can be stated as
P(yi | θi) = P(yi1 | θi)P(yi2 | θi) . . . P(yiK | θi) =
K
Y
k=1
P(yik | θi).
(1.1)
There is one latent variable underlying the observed responses when local
independence holds, and after conditioning on this latent variable the observed
responses are assumed to be independent. Therefore, the assumption of local
independence is also known as conditional independence.
There are two points of view on the meaning that (1.1) gives the condi-
tional probability that person i with ability θi will produce response pattern
yi (Holland, 1990; Molenaar, 1995). In the stochastic subject view, it is as-
sumed that subjects are stochastic in nature, which makes it meaningful to say
that a person with ability θi has a probability of producing a correct response.
The idea is that each person gives small response variations when confronting
the respondent with the same item over and over again and brainwashing the
person after each confrontation. Lord and Novick (1968) deﬁned a so-called
propensity distribution that describes similar variations in the total test scores
in classical test theory. Holland (1990) mentioned that the stochastic subject
view may suggest that there is no need to consider a population model for the
respondents (examinee population), but the eﬀect of the population will al-
ways be there (e.g., person and item parameters will always be estimated with
respect to a population). This leads to the other point of view, which is based
on the concept of sampling respondents from a population. In this so-called
random sampling view, each probability on the right-hand side of (1.1) is the
proportion of respondents with ability θi giving a correct response. This view-
point makes the population of respondents part of the probability model for
each response. The random sampling view for the meaning of the conditional
probability of a correct response is adopted. Throughout this book, speciﬁc
populations of respondents and items are included in the model since their
eﬀects cannot be ignored.
1.2.1 Binary Item Response Models
The Rasch Model
The Rasch model (Rasch, 1960), the one-parameter logistic response model,
is one of the simplest and the most widely used item response model. In the

8
1 Introduction to Bayesian Response Modeling
-4
-2
0
2
4
0.0
0.2
0.4
0.6
0.8
1.0
b=-1
b=0
b=1
Fig. 1.1. Item characteristic curves of the one-parameter IRT model corresponding
to three diﬃculty levels.
one-parameter response model, the probability of a correct response is given
by
P(Yik = 1 | θi, bk) =
exp(θi −bk)
1 + exp(θi −bk) =
 1 + exp(bk −θi)
−1
(1.2)
for individual i with ability level θi and item diﬃculty parameter bk. In Figure
1.1, three ICCs corresponding to Equation (1.2) are plotted with diﬀerent
item diﬃculties. Each ICC describes the item-speciﬁc relationship between the
ability level and the probability of a correct response. The diﬃculty parameter
bk is the point on the ability scale that corresponds to a probability of a correct
response of 1/2. An item is said to be easier when the probability of success is
higher in comparison with another item given the same ability level. In Figure
1.1, the plotted ICCs from the left to the right have increasing item diﬃculty
parameters. It can be seen that to maintain a probability of success of 1/2 on
each item one should increase its ability level from −1 to 1, starting from the
left ICC to the rightmost ICC. An important feature of ICCs corresponding
to the Rasch model is that the ICCs are parallel to one another. This means
that for these items an increase in ability leads to the same increase in the
probability of success. It is said that the items discriminate in the same way
between success probabilities for related ability levels.
Rasch (1960) presented the dependent variable as the log odds or logit of
passing an item, which equals the ability parameter minus the item diﬃculty

1.2 Traditional Item Response Models
9
parameter. The Rasch model has some desirable features. The probability
distribution is a member of the exponential family of distributions. As a result,
the Rasch model shares the nice mathematical and statistical properties of
exponential family models (see, e.g., Lehmann and Casella, 2003, pp. 23–32).
The structure of the Rasch model allows algebraic separation of the ability
and item parameters. Therefore, in the estimation of the item parameters, the
ability parameters can be eliminated through the use of conditional maximum
likelihood (CML) estimation. This can be achieved when the response space is
partitioned according to the raw sum scores, which are suﬃcient statistics for
the ability parameters. In the same way, the item scores are suﬃcient statistics
for the item diﬃculties.
It can be seen from Equation (1.2) that a response probability can be
increased by adding a constant to the ability parameter or subtracting this
constant from the item diﬃculty parameter. Both parameters are deﬁned in
the same metric, and the metric is only deﬁned up to a linear shift. This
identiﬁcation problem is solved by specifying the constraint in such a way
that the location of the metric is known. This is usually done by adding
the restriction that the sum of the diﬃculty parameters equals zero or by
restricting the mean of the scale to zero.
A limitation of the Rasch model is that all items are assumed to discrim-
inate between respondents in the same way and, as a result, items only diﬀer
in item diﬃculty. It is desirable from a practical point of view to param-
eterize item diﬃculties and item discriminations. Thissen (1982) developed
an estimation procedure (marginal maximum likelihood, MML) for the one-
parameter logistic model where all discrimination parameters are equal but
not restricted to be one.
Two-Parameter Model
In the two-parameter logistic model, a discrimination parameter is added to
the model, which leads to
P(Yik = 1 | θi, ak, bk) =
exp(akθi −bk)
1 + exp(akθi −bk) =
 1 + exp(bk −akθi)
−1. (1.3)
As a result, the item characteristic curve (ICC) has a slope parameter ak and
the items are no longer equally related to the ability parameter. In Figure
1.2, three ICCs for the two-parameter IRT model with the same diﬃculty
parameter (bk = 0) are plotted. The slope of each ICC is characterized by the
discrimination parameter ak.
The three ICCs have discrimination parameter values of 2, 1, and 1/2. The
ICC with ak = 2 has the steepest slope. The higher (lower) the discrimination
parameter, the (less) better the item is capable of discriminating between low
and high ability levels. Note that the item’s discrimination value is strongly
related to the item’s diﬃculty value. An item of high discrimination is only
useful in the area of the item’s diﬃculty level that corresponds to a certain

10
1 Introduction to Bayesian Response Modeling
-4
-2
0
2
0.0
0.2
0.4
0.6
0.8
1.0
-levels
Probability of a correct response
4
Fig. 1.2. Item characteristic curves of the two-parameter IRT model corresponding
to three discrimination levels and an equal level of diﬃculty.
region of the ability scale. In Figure 1.2, it can be seen that the item with the
steepest slope is useful in the region between −1 and 1 of the ability scale,
whereas the item with the ﬂattest ICC is useful in the region between −2 and
2.
There is no suﬃcient statistic for the ability parameters, and as a result
conditional maximum likelihood estimation is not possible. Bock and Lieber-
man (1970) and Bock and Aitkin (1981) developed an estimation procedure
based on MML for the two-parameter model. The item parameters are es-
timated from the marginal distribution by ﬁrst integrating over the ability
distribution and thus removing the ability parameters from the likelihood
function.
A probit version of the two-parameter model is deﬁned in the literature as
the normal ogive model (e.g., Lord and Novick, 1968, pp. 365–384) in which
the ICC is based on a cumulative normal distribution,
P(Yik = 1 | θi, ak, bk) = Φ(akθi −bk) =
Z akθi−bk
−∞
φ(z)dz,
(1.4)
where Φ(.) and φ(.) are the cumulative normal distribution function and the
normal density function,1 respectively. The logistic ICC and the normal ogive
1 Random variable Z is normally distributed with mean µ and variance σ2 when
its probability density function equals φ(z; µ, σ2) =
1
√
2πσ2 exp
  −1
2σ2 (z −µ)2
. The
standard normal density function is deﬁned by µ = 0 and σ = 1.

1.2 Traditional Item Response Models
11
ICC in Equations (1.3) and (1.4) closely resemble each other when the logistic
item parameter values are multiplied with a constant scaling factor d = 1.7.
Then, for diﬀerent values of the ability parameter, the response probabilities
of the two-parameter logistic and the normal ogive diﬀer in absolute value by
less than .01 (Hambleton et al., 1991, p. 15). The item parameters will also
be denoted by ξk, with ξk = (ak, bk)t.
The term akθi −bk in Equations (1.3) and (1.4) is often presented as
ak (θi −b∗
k). The b∗
k are deﬁned on the same scale as the latent variable. That
is, as in the Rasch model, the b∗
k is the point on the ability scale where an
examinee has a probability of success on the item of 1/2. The reparameter-
ization bk = ak · b∗
k relates both parameters with each other. In subsequent
chapters, it is shown that the term akθi −bk (without parentheses) will be
useful and the (estimated) diﬃculty parameters are easily transformed to an-
other scale. In Figure 1.2, the diﬃculty levels of the items are zero, and in
that case both parameterizations lead to the same diﬃculty level. The metric
of the ability parameters is known from item response data only up to a linear
transformation. The metric can be identiﬁed by ﬁxing a discrimination and
diﬃculty parameter or by adding constraints that the sum of item diﬃculties
and the product of item parameter values equals, for instance, zero and one,
respectively, or by ﬁxing the mean and variance of the population distribution
of ability parameters. Note that the choice of the identifying restrictions can
lead to numerical problems in the estimation of the parameters.
Three-Parameter Model
The two-parameter normal ogive model can be extended to allow for guessing
by introducing a nonzero lower asymptote for the ICC; that is,
P(Yik = 1 | θi, ak, bk, ck) = ck + (1 −ck)Φ(akθi −bk)
(1.5)
= Φ(akθi −bk) + ck (1 −Φ(akθi −bk)) , (1.6)
where ck is known as the guessing parameter of item k. The probability of
a correct response is given by a guessing parameter plus a second term rep-
resenting the probability of a correct response depending on item parameter
values and the ability level of respondent i. The logistic version becomes
P(Yik = 1 | θi, ak, bk, ck) = ck +
1 −ck
1 + exp(bk −akθi)
(1.7)
=
1
1 + exp(bk −akθi) +
ck
1 + exp(akθi −bk).
The item parameters of both models diﬀer by a constant scaling factor (see
also Section 4.3.2). When ck = 0, the three-parameter model resembles the
two-parameter model. For ck > 0, the interpretation of bk is changed. In the
three-parameter model, the proportion responding correctly at bk/ak equals
1/2 + ck, and in the two-parameter model bk/ak is the value of θi at which a
respondent has a probability of 1/2 of responding correctly.

12
1 Introduction to Bayesian Response Modeling
In Figure 1.3, three ICCs of the three-parameter model are plotted with
the same discrimination and diﬃculty level but with three diﬀerent levels of
guessing, low (.05), medium (.10), and high (.20). The height of the lower
asymptote is the guessing level of the item and corresponds to the probability
of success when guessing the response. It can be seen that for high-ability
respondents the eﬀect of guessing on the success probability is very small
since the three ICCs are almost identical at the higher end of the ability
scale.
-2
0
2
4
0.0
0.2
0.4
0.6
0.8
1.0
-levels
Probability of a correct response
-4
c=.05
c=.10
c=.20
Fig. 1.3. Item characteristic curves of the three-parameter IRT model corresponding
to three diﬀerent levels of guessing and an equal level of discrimination and diﬃculty.
1.2.2 Polytomous Item Response Models
Measurement items are often presented with multiple categories: rating scale
items such as Likert-type items, multiple-choice items where each response
category is scored separately, and items that assign partial credit for partially
correct answers, among others. Most polytomous models are based on ordered
polytomous items, which are those items where the response categories can
be ordered with respect to the ability parameter. Responses to ordered poly-
tomous items are also referred to as graded responses. Although polytomous
item response models contain more item parameters, more precise information
about the ability level can be obtained when more than two scoring categories

1.2 Traditional Item Response Models
13
are used. The measurement information will be reduced when dichotomizing
polytomous response data. Cohen (1983) showed an increase in statistical
information from polytomous IRT models in comparison with dichotomous
item response models. A general overview and a historical discussion of poly-
tomous item response models can be found in Ostini and Nering (2006) and
Embretson and Reise (2000).
In this section, two commonly used polytomous item response models are
presented for ordinal response data. The partial credit model (PCM; Masters,
1982) was developed for test items. It requires multiple steps, and partial
credit is assigned for completing each step. The probability of a response in a
particular category c (c = 1, . . . , Ck) of item k is deﬁned directly as
P(Yik = c | θi, κk) =
exp Pc
l=1(θi −κk,l)
PCk
r=1 (exp Pr
l=1(θi −κk,l))
,
where κk,l is the item step diﬃculty parameter and P1
l=1(θi −κk,l) ≡0.
The number of categories per item may diﬀer. The PCM model simpliﬁes to
the Rasch model for an item with only two categories. The item parameters
are not subject to an order constraint since each item parameter is deﬁned
locally with respect to two adjacent categories instead of taking into account
all categories simultaneously. Muraki (1992, 1993) developed the generalized
partial credit model that allows the items to have diﬀerent slope parameters.
In the PCM, the cumulative probabilities are not modeled directly but are
the result of summing the category response functions. In the graded response
model (Samejima, 1997), the cumulative probabilities are modeled directly.
The probability of scoring in a speciﬁc category is modeled by the probability
of responding in (or above) this category minus the probability of respond-
ing in (or above) the next category. Let Ck denote the number of response
categories of item k. Then there are Ck −1 thresholds between the response
options. The graded response model has the mathematical representation
P(Yik = c | θi, κk) = P(Yik ≥c −1 | θi, κk) −P(Yik ≥c | θi, κk)
(1.8)
=
Z ∞
κk,c−1
ψ (z; akθi) dz −
Z ∞
κk,c
ψ (z; akθi) dz
= Ψ (akθi −κk,c−1) −Ψ (akθi −κk,c)
=
exp(akθi −κk,c−1)
1 + exp(akθi −κk,c−1) −
exp(akθi −κk,c)
1 + exp(akθi −κk,c),
where ψ and Ψ are the logistic density2 and logistic cumulative distribution
function, respectively. The probability of scoring in or above the lowest cate-
gory is one and the probability of scoring above the highest category is zero.
2 Random variable Z is logistically distributed with mean µ and variance σ2π2/3
when its probability density function equals ψ(z; µ, σ2) =
exp((z−µ)/σ)
σ(1+exp((z−µ)/σ))2 . The
standard logistic density function is deﬁned by µ = 0 and σ = 1.

14
1 Introduction to Bayesian Response Modeling
Note that κk,c is the upper grade threshold parameter for category c. The
ordering of the response categories is displayed as −∞= κk,0 < κk,1 ≤
κk,2, . . . , < κk,Ck = ∞, where there are Ck categories.
The graded response model can also be written in cumulative normal re-
sponse probabilities; that is,
P(Yik = c | θi, κk) =
Z κk,c
κk,c−1
φ (z; akθi) dz
= Φ (κk,c −akθi) −Φ (κk,c−1 −akθi) ,
which is the normal ogive version of the graded response model. Note that
this formulation is comparable to the one in Equation (1.8) since the logistic
as well as the normal distribution is symmetric. The graded response model
has an order restriction on the threshold parameters in comparison with the
generalized partial credit model. However, the graded response model has
an underlying continuous response formulation that will prove to be very
useful for estimating and testing parameters. For example, in Chapter 7, the
underlying response formulation will be utilized for a more complex situation
where measurement characteristics are allowed to vary across nations.
The polytomous models are identiﬁed by ﬁxing the scale of the latent
ability parameter. This can be done by ﬁxing a threshold parameter and
in the case of the generalized partial credit model and the graded response
model a discrimination parameter or by ﬁxing the product of discrimination
parameters. In Section 4.4, the identiﬁcation issues are discussed in more
detail.
1.2.3 Multidimensional Item Response Models
Some test items require multiple abilities to obtain a correct response. That is,
more than one ability is measured by these items. The most common example
is a mathematical test item presented as a story that requires both mathe-
matical and verbal abilities to arrive at a correct score. Several assumptions
can be made. First, the probability of obtaining a correct response to a test
item is nondecreasing when increasing the level of the multiple abilities being
measured. This relates to the monotonicity assumption for unidimensional
item response models. Second, individual item responses are conditionally in-
dependent given the individual’s ability values, which is the assumption of
local independence. On the basis of these assumptions, the basic form of a
multidimensional item response model for binary response data is a direct
generalization of the unidimensional item response model. In this generaliza-
tion, each respondent is described by multiple person parameters rather than
a single scalar parameter, where the person parameters represent the multiple
abilities that are measured.
This extension to multiple dimensions of the logistic unidimensional two-
parameter model has the mathematical representation

1.3 The Bayesian Approach
15
P(Yik = 1 | θi, ak, bk) =
exp
 P
q akqθiq −bk

1 + exp
 P
q akqθiq −bk

=
exp
 at
kθi −bk

1 + exp
 at
kθi −bk
,
where respondent i has a vector of ability parameters θi with elements
θi1, . . . , θiQ. The elements of the discrimination matrix for item k, ak, can
be interpreted as the discriminating power of the item. The discriminating
level, akq, reﬂects the change in the probability of a correct response due to a
change in the corresponding ability level θiq. The dimensionality of the abil-
ity parameter can be increased to improve the ﬁt of the model (exploratory)
or to support theoretical relationships between the items and the dimensions
(conﬁrmatory).
Multidimensional item response models for binary response data were ﬁrst
explored by Lord (1980) and McDonald (1967). B´eguin and Glas (2001), and
Reckase (1985, 1997), among others, have further explored the utility of mul-
tidimensional item response models.
1.3 The Bayesian Approach
In the Bayesian approach, model parameters are random variables and have
prior distributions that reﬂect the uncertainty about the true values of the
parameters before observing the data. The item response models discussed
for the observed data describe the data-generating process as a function of
unknown parameters and are referred to as likelihood models. This is the
part of the model that presents the density of the data conditional on the
model parameters. Therefore, two modeling stages can be recognized: (1) the
speciﬁcation of a prior and (2) the speciﬁcation of a likelihood model. After
observing the data, the prior information is combined with the information
from the data and a posterior distribution is constructed. Bayesian inferences
are made conditional on the data, and inferences about parameters can be
made directly from their posterior densities.
The Role of Prior Information
Prior distributions of unknown model parameters are speciﬁed in such a way
that they capture our beliefs about the situation before seeing the data. The
Bayesian way of thinking is straightforward and simple. All kinds of infor-
mation are assessed in probability distributions. Background information or
context information is summarized in a prior distribution, and speciﬁc infor-
mation via observed data is modeled in a conditional probability distribution.
Objection to a Bayesian way of statistical inference is often based upon
the selection of a prior distribution that is regarded as being arbitrary and
subjective (see Gelman, 2008). The speciﬁcation of a prior is subjective since

16
1 Introduction to Bayesian Response Modeling
it presents the researcher’s thought or ideas about the prior information that
is available. In this context, the prior that captures the prior beliefs is the only
correct prior. The prior choice can be disputable but is not arbitrary because
it represents the researcher’s thought. In this light, other non-Bayesian statis-
tical methods are arbitrary since they are equally good and there is no formal
principle for choosing between them. Prior information can also be based on
observed data or relevant new information, or represent the opinion of an
expert, which will result in less objection to the subjective prior. It is also
possible to specify an objective prior that reﬂects complete ignorance about
possible parameter values. Objective Bayesian methodology is based upon
objective priors that can be used automatically and do not need subjective
input.
Incorporating prior information may improve the reliability of the statis-
tical inferences. The responses are obtained in a real setting, and sources of
information outside the data can be incorporated via a prior model. In such
situations where there is little data-based information, prior information can
improve the statistical inferences substantially. In high-dimensional problems,
priors can impose an additional structure in the high-dimensional parameter
spaces. Typically, hierarchical models are suitable for imposing priors that
incorporate a structure related to a speciﬁc model requirement. By imposing
a structure via priors, the computational burden is often reduced.
1.3.1 Bayes’ Theorem
Response data can be obtained via some statistical experiment where each
event or occurrence has a random or uncertain outcome. Let N observations
be denoted as y = (y1, . . . , yN), and assume that y is a numerical realization
of the random vector Y = (Y1, . . . , YN). The random vector Y has some
probability distribution. For simplicity, Y is a continuous or discrete random
vector with probability function p(y) for y ∈Y. This notation is slightly
sloppy since a continuous random variable has a probability density function
(pdf) and a discrete random variable a probability mass function (pmf). For
simplicity, the addition density or mass is often dropped. Formally, probability
distributions can be characterized by a probability density function, but the
terms distribution and density will be used interchangeably when not leading
to confusion.
Assume that response data are used to measure a latent variable θ that
represents person characteristics. The expression p(θ) represents the informa-
tion that is available a priori without knowledge of the response data. This
term p(θ) is called the prior distribution or simply the prior. It will often
indicate a population distribution of latent person characteristics that are un-
der study. Then, it provides information about the population from which
respondents for whom response data are available were randomly selected.
The term p(y | θ) represents the information about θ from the observed
response data. Considered as a function of the data, this is called the sampling

1.3 The Bayesian Approach
17
distribution of the data, and considered as a function of the parameters, it
is called the likelihood function. Interest is focused on the distribution of the
parameters θ given the observed data. This conditional distribution of θ given
the response data is
p(θ | y) = p(y | θ)p(θ)/p(y)
(1.9)
∝p(y | θ)p(θ),
(1.10)
where ∝denotes proportionality. The term p(θ | y) is the posterior density
of the parameter θ given prior beliefs and sample information. It provides
probability beliefs about the parameters from prior and response data infor-
mation. The denominator in (1.9) is called the marginal density of the data,
the marginal likelihood, or the integrated likelihood, and evaluating this ex-
pression is often a costly operation in computation time. When it suﬃces to
know the shape of the posterior p(θ | y), the unnormalized density function
can be used as in (1.10).
Equation (1.9) represents a mathematical result in probability theory and
is known as a statement of Bayes’ theorem (Bayes, 1763). The factorization in
(1.10) is a product of the likelihood, l(y; θ), and prior since usually l(y; θ) =
p(y | θ). This likelihood function contains all sample information regarding θ.
The likelihood principle states that two samples contain the same information
about θ when the likelihoods are proportional (Casella and Berger, 2002).
Bayesian inference adheres to the likelihood principle since all inferences are
based on the posterior density and the posterior depends on the data only via
the likelihood.
The joint posterior density p(y, θ) can be factorized as
p(y, θ) = p(θ | y)p(y)
= p(y | θ)p(θ).
Thus, the joint posterior density can be factorized as the marginal density of
the data and the posterior of θ, but also as the prior of θ and the likelihood of θ
given y. The joint posterior density p(y, θ) is also known as the unnormalized
posterior density function, which leads to the (normalized) posterior of θ when
divided by p(y).
The posterior density of the parameters, p(θ | y), is used for making infer-
ences. Bayesian computational methods make it possible to make inferences
without having to rely on asymptotic approximations. Response data are typ-
ically nonnormally distributed and together with small amounts of sample
information per parameter, particularly at the within-individual level, it is
precarious to rely on asymptotic approximations without showing them to be
accurate. Fully Bayesian methods provide a way to improve the precision of
the parameter estimates. The prior contributes additional information, and
the posterior estimate is based on the combined sources of information (like-
lihood and prior), which leads to greater precision. The inﬂuence of prior
information on the posterior estimates is illustrated in Section 1.4.1.

18
1 Introduction to Bayesian Response Modeling
Constructing the Posterior
As an illustration, assume that ﬁve dichotomous responses y = (1, 1, 0, 0, 0)t
were observed from a respondent with ability θ. The object is to estimate the
posterior density of the ability parameter. Assume that all items are of equal
diﬃculty, say zero. According to the probit version of the Rasch model, let
P(Yk = 1 | θ) = Φ(θ) deﬁne the probability of a correct response to item k.
It is believed priori that the respondent has a nonzero probability of giving
a correct answer and a nonzero probability of giving an incorrect answer.
Therefore, let θ be a priori uniformly distributed on the interval [−3, 3] such
that .001 < Φ(θ) < .998.
The likelihood function for θ equals
p (y | θ) = Φ (θ)2 (1 −Φ (θ))3 .
Multiplying the likelihood with the prior as in Equation (1.10), the posterior
density of θ is
p (θ | y) ∝Φ (θ)2 (1 −Φ (θ))3
for θ ∈[−3, 3]. The posterior mode, at which the posterior density is maxi-
mized, can be computed by taking the ﬁrst derivative of the logarithm of the
posterior, setting the expression equal to zero, and solving the equation for θ.
It follows that the posterior mode equals θm = Φ−1(2/5) ≈−.25.
Updating the Posterior
Bayes’ theorem can be seen as an updating rule where observed data are used
to translate the prior views into posterior beliefs. Assume that the posterior
density of θ is based on K item observations. The posterior can be expressed
as the product of likelihood times the prior. The response observations are
conditionally independent given θ, and it follows that the posterior can be
expressed as
p (θ | y1, y2, . . . , yK) ∝p (y1 | θ) p (y2 | θ) . . . p (yK | θ) p (θ)
∝p (θ | y1, y2, . . . , yK−1) p (yK | θ) .
The posterior density given all but the last observation is updated via the
likelihood of the last observation.
To illustrate the updating nature of Bayes’ theorem, consider 26 responses
to the Mini-Mental State Examination (MMSE) for measuring cognitive im-
pairment (the MMSE data will be described in Section 6.6.4). The object is to
update the posterior density of a respondent’s cognitive impairment, based on
previous knowledge, using the subsequent response observation. It is assumed
that in the population from which respondents are independently sampled the
levels of cognitive impairment are normally distributed, where a high (low)

1.3 The Bayesian Approach
19
θ value corresponds to mild (severe) cognitive impairment. A two-parameter
item response model deﬁnes the probability of a correct response given the
level of impairment, and the item parameters are assumed to be known. The
complete response pattern of a person consists of six incorrect responses (items
4, 12, 16, 17, 18, and 23).
The updated posterior densities are plotted in Figure 1.4. Without any
item observations, the standard normal prior reﬂects the a priori information
about the cognitive impairment (dotted line). The updated posterior densities
based on two (with symbol 2) and three (with symbol 3) items are shifted to
the right. The person’s cognitive impairment is less than expected a priori
since the items were answered correctly. A shift in the posterior means can be
detected, but the shapes of the posteriors are quite similar in correspondence
to the prior density. The fourth item was answered incorrectly. As a result,
the updated posterior (with symbol 4) is shifted to the left and the posterior
mean is negative. The posterior expectation about the person’s cognitive im-
pairment has changed dramatically due to an incorrect answer. Item ﬁve is
answered correctly, and the updated posterior shifts to the right. It can be seen
that when more than ﬁve item observations become available, the posterior
densities only become tighter, concentrated around the posterior mean.
-3
-2
-1
0
1
2
3
0.0
0.2
0.4
0.6
2
2
2
2
2
3
3
3
3
3
3
4
4
4
4
4
4
5
5
5
5
5
5
10
10
10
10
10
10
20
20
20
20
20
20
26
26
26
26
26
26
26
Posterior Density

Fig. 1.4. Updated posterior densities of a person’s cognitive impairment for 2–26
item observations.

20
1 Introduction to Bayesian Response Modeling
1.3.2 Posterior Inference
In item response modeling, the person and item parameters are often of in-
terest, and the objective of inferences is their posterior distributions. The
posterior information is most often summarized by reporting the posterior
mean and standard deviation.
Besides the prior density p(θ) for the person parameters θ, let the item
characteristics be parameterized by ξ and let p(ξ) represent the prior be-
liefs. The item characteristic parameters have an important role in response
modeling, and their prior density will receive special attention in this book.
According to Bayes’ theorem, the joint posterior density of the parameters of
interest can be stated as
p(θ, ξ | y) = p (y | θ, ξ) p(θ, ξ)/p(y)
= p (y | θ, ξ) p(θ)p(ξ)/p(y),
where the prior densities are assumed to be independent from each other.
Summarizing the complicated high-dimensional joint posterior density is very
diﬃcult since the posterior density has analytically intractable forms.
As a ﬁrst step, when interest is focused on θ, the item parameters need to
be marginalized out in the posterior density of interest. In Bayesian inference,
the nuisance parameters are eliminated while accounting for their uncertainty
simply by integrating the joint distribution over them. It follows that
p(θ | y) =
Z
p(y | θ, ξ)p(θ)p(ξ)/p(y) dξ
=
Z
p(θ, ξ | y) dξ.
(1.11)
From this point on, the range of integration will often be omitted from the ex-
pressions, as it will be speciﬁed implicitly by the diﬀerentials. Equation (1.11)
shows that the marginal posterior of interest is obtained by integrating out
the item parameters. In the same way, the marginal posterior of the item pa-
rameters is obtained by integrating out the person parameters. More integrals
need to be evaluated when the marginal posterior of a single component of,
for example, the vector of person parameters is required.
Summarizing the marginal posteriors remains diﬃcult since the mathe-
matical forms are not known. Simulation-based methods will be shown to be
capable of generating samples from the marginal posteriors. Subsequently, the
samples are used for purposes of statistical inference.
The powerful simulation-based estimation methods (MCMC) will be dis-
cussed in Chapter 3. Without diminishing the importance of the estimation
methods, attention is ﬁrst focused on the Bayesian way of thinking and mod-
eling. Until then, WinBUGS (Lunn, Thomas, Best and Spiegelhalter, 2000) is
used in the exercises and examples and it will be assumed that samples from
the posterior distributions (the common output of simulation-based estima-
tion methods) are available that can be used for making Bayesian inferences.

1.4 A Motivating Example Using WinBUGS
21
1.4 A Motivating Example Using WinBUGS
A simple example is given to illustrate the Bayesian modeling approach and
the corresponding Bayesian inference. This example is worked out using the
program WinBUGS (Lunn et al., 2000). The WinBUGS program is part of the
Bayesian inference using Gibbs sampling (BUGS) project and allows one to
put together Bayesian models and estimate simultaneously all model parame-
ters, where WinBUGS facilitates the implementation of the simulation-based
estimation method. Ntzoufras (2009) gives a thorough introduction to the
WinBUGS program and illustrates the many Bayesian modeling possibilities
via data examples.
1.4.1 Modeling Examinees’ Test Results
In the Netherlands, primary schools administer the Cito Test developed by
the National Institute for Educational Measurement (Cito) to get a reliable
measurement of what children have learned during eight years of primary
education. The scores on the test are used to advise children about the type
of secondary education to take.
A relatively small sample of N=200 grade eight students responding to
K=5 dichotomously scored mathematics items is considered. For the moment,
the nesting of students in schools is ignored, but it will be discussed in Section
6.6.1. It will be assumed that the ﬁve math items measure a unidimensional
ability in mathematics represented by θ, which is a continuous random variable
that assumes values on the real line.
The probability of a correct response by examinee i to item k is modeled
by a two-parameter item response model,
P (Yik = 1 | θi, ak, bk) = Φ (akθi −bk) ,
according to the normal ogive model in Equation (1.4). The response model
consists of N ability parameters and K discrimination and K diﬃculty pa-
rameters. The examinees are assumed to be sampled independently from a
population, and a normal prior density is speciﬁed for the ability parameters
with mean zero and variance one. This restriction identiﬁes the two-parameter
item response model and also deﬁnes a useful scale for interpreting estimated
ability values.
Prior densities for the item parameters will be thoroughly discussed in
Section 2.2. Here, a common normal prior is assumed for the discrimination
and diﬃculty parameters (e.g., Johnson and Albert, 1999),
ak ∼N
 µa, σ2
a

I(ak > 0),
bk ∼N
 µb, σ2
b

,
for k = 1, . . . , K. The discrimination parameter is restricted to be positive and
usually takes on values between 1/2 and 3, and the prior should discourage

22
1 Introduction to Bayesian Response Modeling
smaller or higher values. Diﬃculty parameters outside the interval [−4, 4] will
characterize the item as extremely easy or diﬃcult and will lead to all correct
or incorrect responses. The prior mean parameters are set to µa = 1 and
µb = 0, which indicates a moderate level of discrimination and average level
of diﬃculty. Both variance parameters are ﬁxed to one.
WinBUGS
The model is implemented in WinBUGS for a response data matrix of N
persons by K items. Each case i represents the responses of examinee i, and
each column k represents all responses to item k. In the model description, all
data points and parameters need to be speciﬁed. Therefore, the description
contains a loop over observations (variable name Y ), examinees (variable name
theta), and items (variable names a and b).
Listing 1.1. WinBUGS code: Two-parameter item response model.
model{
for
( i
in
1 :N){
for
( k
in
1 :K){
p [ i , k ] <−phi ( a [ k ] ∗theta [ i ]−b [ k ] )
Y[ i , k ]
˜ dbern(p [ i , k ] )
}
theta [ i ]
˜ dnorm( 0 , 1 )
}
for
( k
in
1 :K)
{
a [ k ]
˜ dnorm( 1 , 1 ) I ( 0 , )
b [ k ]
˜ dnorm( 0 , 1 )
}
}
The WinBUGS output contains sampled values from each parameter’s
marginal posterior density. Each marginal posterior density provides com-
plete information about the parameter. For Bayesian inference, the sampled
values are usually used to compute summary statistics of posterior densities
of parameters of interest. In Table 1.1, the marginal posterior density of each
item parameter (discrimination and diﬃculty) is summarized. The posterior
mean provides information on where most of the posterior density is located.
The reported posterior mean is the expected value of the item parameter
under the marginal posterior density. The posterior standard deviation and
quantiles provide information about the spread of the posterior. As measures
of spread, the posterior standard deviation and the 2.5% and 97.5% quantiles
of each marginal posterior are reported.
The reported posterior means (expected a posteriori) are usually used
as point estimates of the parameters. It follows that item ﬁve discriminates
poorly and item one highly discriminates examinees of diﬀerent ability. The
average estimated discrimination level is .90, which is slightly smaller than the
prior mean. The quantiles show that the posterior densities are nonsymmetric
and positively skewed (right tails are longer), which follows from the positivity
restriction on the discrimination parameter. The mean values are also higher

1.4 A Motivating Example Using WinBUGS
23
than the median values. For the diﬃculty parameter densities, the estimated
posterior means are all negative. This means that the items are too easy
since each item was answered correctly by more than 50% of the examinees
given a zero average population level of ability. The raw data show that the
proportions of correct responses of the ﬁve items are 56%, 73%, 54%, 71%, and
65%. Most of the students performed well on the test, which makes it more
diﬃcult to diﬀerentiate examinees. As shown, the items do not diﬀerentiate
well (four item discriminations are less than one) since the items are too easy.
Table 1.1. Item parameters’ posterior density information using WinBUGS.
Item Mean SD 2.5% Median 97.5%
Discrimination Parameter
1
1.54 .49
.82
1.45
2.75
2
.90 .25
.49
.87
1.47
3
.66 .18
.35
.65
1.05
4
.91 .24
.51
.88
1.43
5
.46 .15
.19
.45
.79
Diﬃculty Parameter
1
−.27 .17
−.65
−.26
.04
2
−.79 .15 −1.12
−.78
−.52
3
−.11 .11
−.33
−.12
.09
4
−.73 .15 −1.05
−.72
−.47
5
−.42 .10
−.63
−.42
−.23
The posterior means correspond with the posterior medians, which means
that the marginal posterior densities are approximately symmetric. However,
the mean prior diﬃculty level µb = 0 does not correspond with the estimated
average posterior diﬃculty of −.46. For items 2, 4, and 5, the 97.5% left-sided
posterior density interval does not contain the point zero. That is, the poste-
rior probability that the item diﬃculty is higher than zero is less than 2.5%,
which follows directly from the reported 97.5% quantile. This suggests that
there is a discrepancy between the prior information and the sample infor-
mation concerning the item diﬃculties. The posterior density is constructed
from the prior and sample information, where the prior parameters µb and σ2
b
deﬁne the prior weight.
To investigate the inﬂuence of this prior on the posterior, two cases will be
considered. In the ﬁrst case, the model is ﬁtted with a prior variance parameter
σ2
b = .1. This presents a stronger prior belief (a higher level of conﬁdence),
in comparison with σ2
b = 1, in a common item diﬃculty level of zero. In
the second case, the variance parameter is not ﬁxed but modeled via another
prior distribution, and an inverse gamma density is used to deﬁne a set of

24
1 Introduction to Bayesian Response Modeling
possible values. An inverse gamma prior with its parameters equal to .01 is
uninformative or vague about the variance parameter, so that inferences are
unaﬀected by information external to the data (provided that the variation is
supported by the data). Subsequently, the variance parameter σ2
b becomes a
model parameter that needs to be estimated.3
In Figure 1.5, the estimated posterior densities of the diﬃculty parameters
are plotted for each prior setting. The stronger belief in the diﬃculty’s prior
level where σ2
b = .1 leads to a shift of the posterior density to the right, towards
the prior mean. It follows that the prior’s variance parameter inﬂuences at
least the location of the posterior mean. Specifying prior parameters is diﬃcult
when not much is known beyond the data. By deﬁning a prior for the variance
parameter, instead of ﬁxing its value, the data are used to estimate the prior
variance. This approach is advisable when no prior information is available
to specify the variance. The location of each posterior mean is constructed
by combining sample and prior information, where the level of uncertainty
about the prior mean is estimated by the response data. The estimated prior
variance equals ˆσ2
b = .47 and, as a result, the corresponding posterior densities
of the item diﬃculty parameters (dotted lines) are located approximately in
the middle of the posterior densities with ﬁxed prior parameters.
It was shown that the prior parameters inﬂuence the posterior analysis,
and they require careful attention when making Bayesian inferences about
parameters for which not much is known beyond the data. A ﬂexible model-
ing framework was used that allows speciﬁc or noninformative prior settings,
which was illustrated by modeling the variance parameter of the prior for
the item diﬃculty parameters. This modeling framework will be explored
further to make accurate individual (item) parameter estimates when only
a few observations per respondent (item) are observed, to handle diﬀerent
sources of prior information, and to handle complex sampling designs, among
other things. The modeling framework needs to be accompanied by a pow-
erful estimation method that supports a realistic and practical way to make
Bayesian inferences. Both computational and modeling issues will receive at-
tention throughout the book.
1.5 Computation and Software
For the well-known item response models, various commercial and non-
commercial programs are available. It is to the credit of the pioneering work of
the researchers involved that today so many popular IRT programs are avail-
able. To give a short overview, BILOG-MG (Zimowski, Muraki, Mislevy and
Bock, 1996) allows the estimation of IRT parameters for multiple groups and
3 In WinBUGS, the variance parameter of a normal distribution is parameterized
in terms of the inverse variance (precision) such that, in the second case, the
precision parameter is modeled by a gamma prior.

1.5 Computation and Software
25
-1.5
-1.0
-0.5
0.0
0.5
0.0
0.5
1.0
1.5
2.0
-1.5
-1.0
-0.5
0.0
0.5
0.5
1.0
1.5
2.0
-1.5
-1.0
-0.5
0.0
0.5
0.5
1.0
1.5
2.0
2.5
-1.5
-1.0
-0.5
0.0
0.5
0.5
1.0
1.5
2.0
-1.5
-1.0
-0.5
0.0
0.5
0.5
1.0
1.5
2.0
2.5
b1
b2
b3
b4
b5
N(0,
)


N(0,1)
N(0,.1)
Fig. 1.5. Estimated posterior densities of the diﬃculty parameters for diﬀerent
prior choices.
enables detection of diﬀerential item functioning, among other uses. MULTI-
LOG (Thissen, 1991) can be used speciﬁcally to perform a multiple-category
IRT analysis for polytomous IRT models. PARSCALE (Muraki and Bock,
1997) is used for IRT scaling, item analysis, and scoring of rating scale data.
A popular noncommercial (Dutch) program is OPLM (Verhelst, Glas and Ver-
stralen, 1995); it can handle dichotomously or polychotomously scored items
using diﬀerent one-parameter models.
In the 1990s, the introduction of powerful simulation-based estimation
methods made it possible to employ Bayesian methods without the com-
mon computational constraints, which also posed new statistical model-
ing opportunities. Twenty years later, the Bayesian paradigm is available
to those with and without programming skills. The WinBUGS program

26
1 Introduction to Bayesian Response Modeling
(www.mrc-bsu.cam.ac.uk/bugs) makes it possible to apply Bayesian meth-
ods to the analysis and modeling of data. The WinBUGS site has online help
and contains lots of examples. Despite the many advantages of WinBUGS, for
many models discussed in this book, the program is often too slow or simply
does not work (e.g., Gelman and Hill, 2007). The main advantage is that it
is very ﬂexible in constructing models, but it can be slow, can get stuck with
large datasets, and cannot handle complex item response models.
The free R (R Development Core Team, 2010) and commercial S+ (TIBCO
Software, 2009) statistical programs are very popular in the Bayesian com-
munity. The programs contain methods for ﬁtting speciﬁc models, but the
popular higher programming languages of R and S+ allow one to program
any model. Obviously, more knowledge of the estimation algorithm is needed
in comparison with WinBUGS. Since the 1990s many R and S+ programs
have been made available via the Internet; speciﬁcally, so-called R packages
have been developed that allow one to construct programs that run within the
R software environment. A list of contributed R packages can be found on the
Comprehensive R Archive Network (CRAN; http://cran.r-project.org/).
Several Bayesian (response) models used in marketing and microeconometrics
applications are implemented in the bayesm package of Rossi, Allenby and
McCullogh (2005). Bayesian inference for a number of response models using
posterior simulation can be performed using the package MCMCpack. Gel-
man and Hill (2007) developed R programs for hierarchical models, including
a two-parameter item response model. Various R packages are regularly up-
dated and extended, and new contributions are frequently made, which makes
it impossible to give a complete list of R packages that supports the Bayesian
analysis of item response data.
Press (2003, pp. 169–171) listed references to popular Bayesian programs.
This includes the Matlab and Minitab programs of Johnson and Albert (1999)
for the analyses of ordinal data using Bayesian computational algorithms.
Computer Code Developed for This Book
Some models in this book can be handled by the programs mentioned, and
other models require a speciﬁc implementation. To be free from the restrictions
of other software programs, and to be completely ﬂexible in deﬁning diﬀerent
priors using diﬀerent computational methods and computing or evaluating
various statistics, I have programmed all models and methods in this book.
The programs run in the R and S+ environments.
The revolution in Bayesian computational methods led to programs that
needed days to come up with a solution. Large datasets, complex models with
poorly identiﬁed parameters, and poorly implemented methods increased the
computation time. In correspondence with Rossi et al. (2005, p. 7), the meth-
ods become impractical when more than a few hours of computing time is
needed using a common computer. Users usually are not willing to wait that

1.6 Exercises
27
long, especially when a simpliﬁed approach (e.g., by making additional as-
sumptions, ignoring some complicating issues) only takes a few minutes. Fur-
thermore, any statistical analysis requires ﬁtting diﬀerent models consisting
of diﬀerent priors and summarizing the inferences from diﬀerent perspectives.
Then, after evaluating the outcomes, model expansions with diﬀerent prior
information are considered, which is certainly impractical when each analysis
takes more than a few hours.
The developed programs discussed in this book are written in high- and
low-level languages to limit the computation time to around two hours. Several
programs are written in the R and S+ languages. The programs can be used for
the analysis of item response data but also serve as a basis for programming
more complex item response models. Changing pieces of code can be very
helpful in getting a better understanding of the substance and can be a ﬁrst
step in developing programming skills. Further, R packages and S+ programs
are developed that make use of a dynamic link library (dll), which is a shared
Microsoft Windows library. In Fortran (Intel Visual Fortran version 11 using
IMSL Numerical Library version 6), programs are written that can be called
within the R and S+ environment. The tools developed in Fortran are directly
accessible, as are their input and output, and they can be manipulated within
the statistical programs. To make the more complex models accessible for
practical use, a low-level language is needed, and in my experience it will
reduce the computation time roughly by a factor of ten.
Despite the increased CPU time and the increased size of available mem-
ory, the computational elements are important to make Bayesian inferences
possible in a reasonable amount of time. Computation plays an important part
in Bayesian statistical modeling, and to stress the importance of the compu-
tational methodology, the implemented algorithms are also described in this
book. Those who just want to apply the models can use the software, but it
also aims to serve those who want to implement and/or learn to develop and
implement algorithms by themselves.
The programs and the data for the examples in the book are available
on the World Wide Web at www.jean-paulfox.com, which contains more
supporting material.
1.6 Exercises
WinBUGS and Listing 1.1 can be used to obtain the sampled values from the
marginal posterior densities for making posterior inferences. The following
exercises are based on output from WinBUGS. When ﬁtting an item response
model in WinBUGS, run one chain of 10,000 MCMC iterations and use the
last 5,000 iterations.
1.1. In the example in Section 1.4, samples are obtained from each ability
posterior density p(θi | yi).

28
1 Introduction to Bayesian Response Modeling
(a) Graph the posterior density of the ability parameter of respondent i. Argue
that the plotted posterior is not necessarily a symmetric density although a
symmetric prior was assumed.
(b) Explain the summary statistics of θi reported by WinBUGS using the
posterior density plot of Exercise 1.1(a).
(c) Graph the posterior density of a respondent’s ability parameter that has
all items correct and one that has all items incorrect. Given that a standard
normal prior for the ability parameters was assumed, explain the direction of
the skewness of the plotted densities.
(d) For a respondent who scores perfect, will the skewness of the ability pos-
terior density increase or decrease when more items are administered?
1.2. The ability posterior density of examinee i is summarized.
(a) Argue that the posterior mean is often considered to be a good point
estimate of the ability parameter. Note that the posterior mean equals the
expected posterior ability and can be expressed as
E (θi | yi) =
Z
θip (θi | yi) dθi.
(b) Explain when the posterior mode might be considered as a point esti-
mate. Note that the posterior mode equals the posterior ability point θMAP
(maximum a posteriori) at which the posterior density is maximized,
θMAP = max
θi p (θi | yi) .
(c) Given the sampled values, show how the posterior mean can be estimated
and that the computation of the posterior mode is more complex.
(d) Argue that the posterior mean and variance can be used for adequately
summarizing a symmetric posterior density but that various central points
such as the mean, mode, and median, together with a region of high posterior
probability are needed to summarize a nonsymmetric density.
1.3. (continuation of Exercise 1.1) Consider the computed posterior means as
estimates of the ability parameters.
(a) Graph the density of the estimated abilities, and explain that this is the
estimated empirical population density or sample density.
(b) Explain that the empirical population density is expected to be positively
skewed where the right tail of the density is longer. (Note that the estimated
item diﬃculty parameters are all negative.)
(c) Compute the sample skewness of the empirical population density with
√
N PN
i=1
 θi −¯θ
3
PN
i=1
 θi −¯θ
23/2 ,
where ¯θ is the estimated mean ability. (Listing 1.2 provides code to compute
the sample skewness,)

1.6 Exercises
29
Listing 1.2. WinBUGS code: Computing the sample skewness.
for ( i
in
1 :N){
numerator [ i ] <−(1/N)∗pow( theta [ i ] −mean( theta [ ] ) , 3 )
}
skewness <−sum( numerator [ ] ) /(pow( sd ( theta [ ] ) , 3 ) )
(d) Does a skewed empirical population density indicate a model violation
since a normal population prior is assumed?
1.4. (continuation of Exercise 1.1) Each model parameter has a (posterior)
density function, which makes it possible to compute (posterior) probability
statements.
(a) Compute the prior probability that the ability of examinee i = 1 is below
the population average; that is,
P (θ1 < 0) =
Z 0
−∞
φ (x; µ = 0, σ = 1) dx.
(b) Compute the posterior probability that the ability of examinee i = 1 is
below the population average; that is,
P (θ1 < 0 | y) =
Z 0
−∞
p (θ1 | y) dθ1.
Use the WinBUGS code of Listing 1.3 or the sampled values from the posterior
density to compute the posterior probability since the analytical form of the
posterior density is unknown.
Listing 1.3. WinBUGS code: Computing the posterior probability of the event
θ1 < 0.
counting <−max( theta [ 1 ] , 0 )
p r o b a b i l i t y <−equals ( counting , 0 )
(c) In the same way, compute the prior and posterior probabilities that item
three appears to be more diﬃcult than item one.
1.5. Deﬁne priors for discrimination and diﬃculty parameters when additional
information is available.
(a) Deﬁne an item diﬃculty prior that reﬂects a known order of items by
diﬃculty. Explain how this inﬂuences the estimated item characteristic curves.
(b) Deﬁne an item discrimination prior that reﬂects a known order of items by
discrimination. Explain how this inﬂuences the estimated item characteristic
curves.
(c) Deﬁne a prior for the item parameters that reﬂects an ordering of items
by diﬃculty and discrimination.
(d) Deﬁne a prior for the item parameters such that it is expected a priori
that the more diﬃcult an item is, the better it will discriminate.

2
Bayesian Hierarchical Response Modeling
In the ﬁrst chapter, an introduction to Bayesian item response modeling was
given. The Bayesian methodology requires careful speciﬁcation of priors since
item response models contain many parameters, often of the same type. A
hierarchical modeling approach is introduced that supports the pooling of
information to improve the precision of the parameter estimates. The Bayesian
approach for handling response modeling issues is given, and speciﬁc Bayesian
elements related to response modeling problems will be emphasized. It will
be shown that the Bayesian paradigm engenders new ways of dealing with
measurement error, limited information about many individuals, clustered
response data, and diﬀerent sources of information.
2.1 Pooling Strength
In test situations, interest is focused on the within-individual and between-
individual response heterogeneities. The within-individual response hetero-
geneity provides information about test characteristics, individual response
behavior, and the individual level of ability, among other things. The between-
individual response heterogeneity provides information about test character-
istics, the relationship between ability and individual background informa-
tion, and clustering eﬀects of respondents, among other things. The within-
individual response diﬀerences are informative about the average character-
istic levels across items, whereas the between-individual response diﬀerences
are informative about each single item characteristic.
It is important to score respondents on a common scale, and a response
model is needed that is capable of generating individual-level parameter es-
timates and associated uncertainties. At the same time, a response model is
needed that is capable of producing item-level estimates and a characteriza-
tion of the corresponding uncertainties. The focus on within-individual and
between-individual diﬀerences corresponds with the interest in making in-
©
31
and Behavioral Sciences, DOI 10.1007/978-1-4419-0742-4_2,
J.-P. Fox, Bayesian Item Response Modeling: Theory and Applications, Statistics for Social
  Springer Science+Business Media, LLC 2010
ferences at disaggregate and aggregate levels. Bayesian hierarchical response

32
2 Bayesian Hierarchical Response Modeling
models will prove to be very useful for making inferences at diﬀerent hier-
archical levels and make it possible to construct posterior distributions of
individual-level parameters given that the data are sparse at the individual
level.
Although response data are traditionally collected at the individual level,
there is a tendency, speciﬁcally in large-scale survey studies, to collect (more)
background information at various aggregated levels. Obtaining data at dif-
ferent levels allows inferences to be made at diﬀerent levels. For example, re-
sponse data from students across schools make it possible to compare students
and schools with respect to the performances of the students. Background
information can be used for making correct comparisons and also for explain-
ing diﬀerences in performance at the student and school levels. The fact that
more data become available at diﬀerent levels creates modeling challenges and
provides opportunities for making inferences that exploit the heterogeneity.
The main challenge is to obtain accurate individual-level parameter estimates
given very limited individual-level information but a large amount of survey
respondents.
A common assumption is to regard the individual-level data as indepen-
dent conditional on individual-level parameters. In that case, the marginal
posterior of N respondents’ abilities in Equation (1.11) can be written as
p(θ1, . . . , θN | y) ∝
Z Y
i
p (yi | θi, ξ) p (θ1, . . . , θN | θP ) p (ξ) dξ
(2.1)
∝
Y
i
Z
p (yi | θi, ξ) p (θi | θP ) p (ξ) dξ,
where the proportionality sign is used since the normalizing constant is left
out of the equation.
The joint prior on the abilities is simpliﬁed by assuming that the abilities
are independent conditional on the hyperparameters θP and identically dis-
tributed. The distinction between a parameter and a hyperparameter is that
the sampling distribution of the (response) data is expressed directly condi-
tional on the former (Lindley and Smith, 1972). The parameter’s prior density
is parameterized by the hyperparameters.
The ﬁrst term on the right-hand side is the conditional likelihood. It fol-
lows that inferences for each respondent’s ability θi can be made indepen-
dently of all other respondents. Furthermore, the individual-level inferences
are based on the sample and prior information. Therefore, the posterior mean
is constructed from a combination of the prior mean and a likelihood-based
estimate. For example, when the common population parameters θP provide
detailed information about the value of θi, the posterior mean will be shrunk
more towards the prior mean. Note that such a shift towards the prior mean
(of item diﬃculty locations) was shown in the example of Section 1.4.
The amount of shrinkage is determined by the form of the prior and the
values of the hyperparameters. When the amount of within-individual infor-

2.2 From Beliefs to Prior Distributions
33
mation is relatively small, the posterior for the θi will reﬂect a high level of
shrinkage induced by the prior. The opposite is also true. The posterior for the
θi will reﬂect a low level of shrinkage induced by the prior when the amount
of within-individual information is high. The level of shrinkage induced by
the prior can be inferred from the data by constructing a second-stage prior
on the hyperparameters θP . This prior on the hyperparameters is sometimes
called a hyperprior (Berger, 1985). The sample information and the prior for
the hyperparameters will be used to make inferences about the θP . Then, the
level of shrinkage is determined by the information in the data (Exercise 3.3).
In a strictly hierarchical modeling approach, it is assumed that the re-
sponses are nested in individuals, individuals nested in groups, and so on.
Typically, response data are clustered in a cross-classiﬁed way where respon-
dent eﬀects and item eﬀects are present. To separate the eﬀects of individuals
and items, two cross-cutting hierarchies need to be speciﬁed. Therefore, as-
sume that the item-level response data are independent conditional on the
item-level parameters ξk. This independence assumption leads to a within-
item and between-item structure and deﬁnes the second hierarchy of the re-
sponse data. Analogous to the speciﬁcation of the joint posterior of abilities,
the joint posterior of the item parameters can be written as
p(ξ1, . . . , ξK | y) ∝
Z Y
k
p (yk | θ, ξk) p (ξ1, . . . , ξK | ξP ) p (θ) dθ (2.2)
∝
Y
k
Z
p (yk | θ, ξk) p (ξk | ξP ) p (θ) dθ,
where the prior for the item parameters is structured by assuming indepen-
dence conditional on the hyperparameters ξP . Subsequently, prior and sample
information are used to estimate the item eﬀects, where shrinkage eﬀects can
be inferred from the data by adding a second layer for the hyperparameters
ξP (see the example in Section 1.4).
2.2 From Beliefs to Prior Distributions
The Bayesian approach to response modeling starts with the speciﬁcation of
prior distributions. In general, there are two types of prior information. First,
when there is prior information about the values of the model parameters
from, for example, related datasets, this information can be used to construct
a case-speciﬁc prior. This type will be discussed in subsequent chapters when
additional data are available and case-speciﬁc information can be used to
construct priors.
Second, the prior information can come from mathematical properties.
Prior distributions can be classiﬁed into conjugate or nonconjugate priors. A
conjugate prior has the property that the posterior has the same algebraic
form as the prior. This has the advantage that the posterior has a known

34
2 Bayesian Hierarchical Response Modeling
analytical form, which simpliﬁes the statistical analysis. A nonconjugate prior
leads to a posterior that often has a complicated functional form, which makes
the statistical analysis mathematically more challenging. A conjugate prior
can easily reﬂect a likely range of parameter values, but remember that the
shape of the prior also has an impact on the posterior analysis. Lord (1986)
stressed the practical advantages of priors since, for example, the priors restrict
the parameter values to a plausible range. Although a range of priors can be
considered for item response models, only a limited number based on well-
known distributions are used in practice.
The hierarchical or multistage prior will prove to be very useful (e.g.,
Berger, 1985, Section 3.6; Lindley and Smith, 1972). The hierarchical prior
consists of a ﬁrst-stage prior for many parameters of the same type that are as-
sumed to be independent given hyperparameters. The hyperparameters have
their own prior at a second stage. The ﬁrst-stage cluster-speciﬁc parameters
(e.g., item and person parameters) can be related with an aggregated data
source at the cluster level. The similarity between the ﬁrst-stage parameters
and their possible relationship with aggregated prior information motivates
the hierarchical prior based on combining information to improve the preci-
sion of each ﬁrst-stage parameter estimate. The hierarchical prior improves
the estimation of the ﬁrst-stage parameters by pooling information (borrow-
ing strength) over clusters and by accounting for uncertainty in the hyperpa-
rameter estimates. Typically, there are many individuals but relatively little
response data on each individual. Pooling information over individuals ex-
ploits the assumed similarity between the individual parameters to improve
the individual parameter estimates. The hierarchical prior can be extended
to more than two stages, and such extensions will be discussed in subsequent
chapters.
A Hierarchical Prior for Item Parameters
In most cases, there is not much information about the values of the item pa-
rameters, and the response data are the source of information to distinguish
the item parameters from each other. Without a priori knowledge to distin-
guish the item parameters, it is reasonable to assume a common distribution
for them. In that case, the item parameters have a common population dis-
tribution and it is not possible to order or group the parameters. Typically,
the parameters are said to be exchangeable in their joint distribution, which
means that the joint probability of the item parameters is invariant with re-
spect to permutations of the indices. That is, it is assumed that the prior
information about the item parameters is exchangeable.
The exchangeability assumption is closely related to the concept of inde-
pendently and identically distributed but not the same. Independently and
identically distributed implies exchangeability, and exchangeable item param-
eters do have identical marginal distributions, but they are not necessarily
independent. The assumption of exchangeable item parameters is equivalent

2.2 From Beliefs to Prior Distributions
35
to the assumption of conditionally independent and identically distributed
item parameters given hyperparameters and a prior density on the hyperpa-
rameters.
An intuitive assumption of an item characteristic curve is that the higher
a respondent’s ability level the more likely it is that the respondent scores well
on the item. This so-called monotonicity assumption implies that P(Yik = 1 |
θi) is nondecreasing in θi, which is satisﬁed when the discrimination param-
eter is restricted to be positive. For example, in Bilog-MG (Zimowski et al.,
1996), a lognormal prior distribution can be speciﬁed to restrict a discrimina-
tion parameter to be positive. A normal prior is often used for each diﬃculty
parameter (e.g., Albert, 1992; Patz and Junker, 1999a). Exchangeability is
usually assumed such that the priors have the same hyperparameters in ad-
dition to the common form.
The assessment of the hyperparameters can be challenging since they can
have a substantial eﬀect on the item parameter estimates. The greater the
discrepancy between the sample-based information and the prior information,
the larger the amount of shrinkage towards the prior mean when keeping
other factors constant. A small prior variance leads to an informative prior
and greater shrinkage. A large prior variance implies a noninformative prior
and almost no shrinkage. The eﬀective use of a prior for item parameters de-
pends on the hyperparameter speciﬁcations, and that requires insight into the
test characteristics and respondents. Mislevy (1986) remarked that incorrectly
specifying the prior mean can result in biased item diﬃculty estimates. Below,
a hierarchical prior is deﬁned such that the hyperparameters are estimated
by the data, and the amount of shrinkage is inferred from the data. Then, the
appropriateness of the mean and variance of the prior is arranged using an
estimation procedure.
Albert (1992) and Patz and Junker (1999a), among others, suggested inde-
pendent prior distributions for the item parameters with ﬁxed hyperparameter
values: a (log)normal prior for the discrimination parameters and a normal
prior for the diﬃculty parameters. Johnson and Albert (1999) deﬁned a hi-
erarchical prior for the discrimination parameters: a normal density prior at
stage 1, and at stage 2 an inverse gamma prior for the variance (with hyper-
parameters equal to one) and a uniform prior for the mean. Bradlow, Wainer
and Wang (1999) and Kim, Cohen, Baker, Subkoviak and Leonard (1994) also
deﬁned independent hierarchical priors for the item parameters. Tsutakawa
and Lin (1986) proposed a bivariate prior for the item parameters induced
by the probability of correct responses to the items at diﬀerent ability levels.
This requires specifying the degree of belief about the probability of a correct
response to each item at two ability levels. Although their prior provides a way
to specify dependence among parameters within an item, it is often diﬃcult
to specify prior beliefs about success probabilities objectively.
A more straightforward realistic multivariate normal prior allows for
within-item characteristic dependencies; that is,

36
2 Bayesian Hierarchical Response Modeling
(ak, bk)t ∼N (µξ, Σξ) IAk(ak),
(2.3)
where Ak = {ak ∈R, ak > 0}. The truncated multivariate normal density in
Equation (2.3) is an exchangeable prior for item parameters ξk (k = 1, . . . , K).
The hyperparameters are modeled at a higher level since there is usually
little known about the mean item discrimination, mean item diﬃculty, and
variances. An inverse Wishart density, denoted as IW, with scale matrix Σ0
and degrees of freedom ν ≥2, is commonly used to specify Σξ.1 This is a
conjugated prior for the covariance matrix. Then, a normal prior is used to
specify the prior mean given the prior variance matrix Σξ. The joint prior
density for (µξ, Σξ) equals
Σξ ∼IW(ν, Σ0),
(2.4)
µξ | Σξ ∼N(µ0, Σξ/K0).
(2.5)
This joint prior density is known as a normal inverse Wishart density, where
K0 denotes the number of prior measurements (e.g., Gelman, Carlin, Stern
and Rubin, 1995). A proper noninformative prior is speciﬁed with µ0 = (1, 0)t,
ν = 2, a scale matrix Σ0 that is a minimally informative prior guess of Σξ,
and K0 a small number.
The hierarchical prior accounts for within-item dependencies and for un-
certainty of the prior’s parameters. The mean and variance of the normal
prior for the item parameters are modeled at a higher prior level and need to
be estimated from the data. As a result, the hierarchical prior gives rise to
shrinkage estimates of the item parameters, where the amount of shrinkage
is inferred from the data. An extreme and inﬁnite item parameter estimate
caused by the fact that the item is answered correctly or incorrectly by all
respondents is avoided due to shrinkage towards the mean of the prior.
When observing responses on a continuous scale that are assumed to be
normally distributed, the hierarchical prior presented is a conjugate prior (e.g.,
Mellenbergh, 1994b). In Section 4.2, for diﬀerent reasons, the discrete observed
response data will be augmented with normally distributed continuous data.
The hierarchical normal prior will be shown to be a conjugate prior with
respect to the normal likelihood of the augmented data, which will simplify
the procedure for making posterior inferences. A lognormal version of the
normal prior presented in Equation (2.3) will be discussed in Exercise 2.3 (see
also Exercise 4.3).
The following theoretical motivation provides more arguments for mod-
eling a covariance structure between item parameters. Let θl and θu be the
average ability levels of respondents sampled from low- and high-ability groups
with corresponding success probabilities Pk(θl) and Pk(θu) for item k, respec-
tively. Consider the diﬀerence between success probabilities, Pk(θu) −Pk(θl),
1 A q ×q random positive-deﬁnite symmetric matrix Σξ is distributed according to
an IW distribution with ν degrees of freedom and scale matrix Σ0 if its proba-
bility density function is proportional to |Σξ|−(ν+q+1)/2 exp

−tr

Σ0Σ−1
ξ

/2

.

2.2 From Beliefs to Prior Distributions
37
as an estimate of discrimination of item k. The higher this diﬀerence, the bet-
ter the item discriminates between respondents with low and high abilities.
Consider the mean success probability (Pk(θu) + Pk(θl))/2 as an estimate of
the item diﬃculty. A linear relationship between the item parameter estimates
is deﬁned as
(Pk(θu) + Pk(θl)) /2 = ρ (Pk(θu) −Pk(θl))
(2.6)
⇐⇒Pk(θu) + Pk(θl) = 2ρ (Pk(θu) −Pk(θl))
⇐⇒Pk(θu)
Pk(θl) = ρ + 1/2
ρ −1/2
for k = 1, . . . , K, and for any constant ρ > 1/2. A consistency in the ratio of
the mean success probabilities across the K items induces a covariance struc-
ture between the item parameters. In Equation (2.6), items of decreasing dif-
ﬁculty will discriminate better between the low- and high-ability respondents
for ρ > 1/2. If the success probabilities on the left-hand side of Equation (2.6)
are replaced by the failure probabilities, items of increasing diﬃculty will dis-
criminate better between the low- and high-ability respondents for ρ > 1/2.
The relationships show that a consistency in the ratio of group-speciﬁc re-
sponse probabilities across items can correspond with a common within-item
dependency structure.
In the three-parameter model, the guessing parameter, ck, is bounded
above by one and below by zero since it represents the probability that a
respondent correctly guessed the answer to item k. A convenient prior is the
beta density with parameters α and β that reﬂects α −1 prior successes and
β −1 prior failures (Swaminathan and Giﬀord, 1986; Zimowski et al., 1996).
Most often the guessing parameters are assumed to be a priori independently
and identically beta distributed (Exercise 2.4),
p (ck | α, β) = Γ (α + β)
Γ (α) Γ (β)cα−1
k
(1 −ck)β−1.
(2.7)
The normalizing constant contains the gamma function, which is deﬁned as
Γ(α) =
R ∞
0
xα−1e−xdx for α > 0 (Exercise 3.8). A hierarchical prior for the
guessing parameters is constructed by modeling the parameters of the beta
prior. Usually a uniform hyperprior is speciﬁed for the beta parameters.
Both priors in Equations (2.3) and (2.7) are nonconjugate for the observed-
data likelihood.2 In Chapter 4, it will be shown that a nonconjugate Bayesian
analysis is more complex and requires a diﬀerent method for estimating the
item parameters. An exchangeable hierarchical normal prior for the three item
parameters (ak, bk, ck) is possible via a reparameterization of the guessing
parameter. A straightforward way is to use an inverse normal transformation
2 In Section 4.2, it is shown that, for a speciﬁc data augmentation scheme, the
posterior of ck given augmented data is also a beta density, which makes the beta
prior in Equation (2.7) a conjugate prior.

38
2 Bayesian Hierarchical Response Modeling
function, ˜ck = Φ−1(ck), which ensures that ˜ck is deﬁned on the whole real line
(see Exercise 4.9).
The prior distribution for the threshold parameters in, for example, the
graded response model is usually chosen to be noninformative. Albert and
Chib (1993) deﬁned a uniform prior distribution for parameter κk,c but trun-
cated to the region {κkc ∈R, κk,c−1 < κk,c ≤κk,c+1} to take account of
the order constraints. Johnson and Albert (1999) argued that a uniform prior
for the thresholds assigns equal weight to all grades, although some grades
may be known to be rare. They proposed a more sophisticated prior where
the lower threshold equals the upper threshold when they correspond to an
unobserved grade. Other prior distributions for the threshold parameters will
be discussed in subsequent chapters.
A Hierarchical Prior for Person Parameters
A prior for person parameters assumes that the respondents represent a sam-
ple from a known population. In the case of a simple random sample, a subset
of individuals (a sample) are chosen from a larger set (a population) and each
individual is chosen randomly, where each individual has the same probability
of being chosen at any stage during the sampling process. The respondents are
assumed to be sampled independently from a large population; that is, the
person parameters are independently distributed from a normal population
distribution,
θi ∼N(µθ, σ2
θ),
(2.8)
for i = 1, . . . , N, where the mean and variance parameters are unknown and
are modeled at a higher level. A normal inverse gamma prior is the conjugate
prior for the normal distribution with unknown mean and variance. Therefore,
a joint hyperprior is speciﬁed as
σ2
θ ∼IG(g1, g2),
(2.9)
µθ | σ2
θ ∼N(µ0, σ2
θ/n0),
(2.10)
where g1 and g2 are the parameters of the inverse gamma density denoted
as IG and n0 presents the number of prior measurements.3 Other prior pop-
ulation distributions for more complex sampling designs will be thoroughly
discussed in later chapters.
2.2.1 Improper Priors
There have been attempts to construct noninformative priors that contain
no (or minimal) information about the parameters. The noninformative prior
3 Parameter σ is inverse gamma (IG) distributed with shape parameter g1 and
scale parameter g2 when its pdf is proportional to σ−(g1+1) exp (−g2/σ). The
inverse gamma distribution with ν degrees of freedom corresponds to an inverse
chi-square distribution when g1 = ν/2 and g2 = 1/2.

2.2 From Beliefs to Prior Distributions
39
does not favor possible values of the parameter over others. A prior for the
diﬃculty parameter that is completely determined or dominated by the like-
lihood is one that does not change much in the region where the likelihood
is appreciable and does not assume large values outside this region. A prior
with these properties is referred to as a locally uniform prior (Box and Tiao,
1973, p. 23). An exchangeable locally uniform prior for the diﬃculty param-
eters leads to improper independent priors p(bk) equal to a constant. When
the diﬃculty parameters have a locally uniform prior, meaning that they bear
no strong relationship to one another, they can be considered ﬁxed eﬀects
parameters. In that case, interest is focused on the estimation of the diﬃculty
parameters rather than the properties of their population distribution.
For example, a noninformative prior for the item diﬃculty parameter gives
equal weight to all possible values. That is, p (bk) = c, where c is a constant
greater than zero. As a result,
R
p (bk) dbk = ∞, and this noninformative
prior is an improper prior since it does not integrate to one. The value of c is
unimportant, and typically the noninformative prior density is p (bk) = 1.
Improper priors may lead to improper posteriors, which means that the
posterior is not a valid distribution and posterior moments have no mean-
ing (Exercise 6.8). Ghosh, Ghosh, Chen and Agresti (2000) showed that for
the one-parameter model the speciﬁcation of improper priors for person and
item parameters leads to an improper joint posterior. Improper priors are of-
ten conjugate and may lead to a straightforward simulation-based estimation
method. Such simulation-based methods make use of conditional distributions
of the model parameters that are available in closed form, but the marginal
posterior distributions are not. The conditional distributions can all be well
deﬁned and can be simulated from, yet the joint posterior can be improper
(Robert and Casella, 1999, pp. 328–332). Demonstrating the property of the
posterior is often impossible. The best way to avoid improper posteriors is
to use proper priors. Noninformative proper priors can also be used to spec-
ify less or no prior information, and they avoid the risk of getting improper
posteriors.
2.2.2 A Hierarchical Bayes Response Model
Summing up, the posterior density of interest is constructed from a response
model for the observed data and a hierarchical prior density. In a hierarchical
prior modeling approach, the prior parameters are modeled explicitly and
have hyperprior densities at the second stage of the prior.
Suppose posterior inferences are to be made about the item and per-
son parameters that require integration over the density functions of the
hyperparameters. The hyperparameters are denoted by θP =
 µθ, σ2
θ

and
ξP = (µξ, Σξ). The posterior density of interest can be expressed as

40
2 Bayesian Hierarchical Response Modeling
p (ξ, θ | y) ∝
Z Z
p (y | θ, ξ) p (θ, ξ | θP , ξP ) p (θP , ξP ) dξP dθP
∝
Z Z
p (y | θ, ξ) p (θ | θP ) p (ξ | ξP ) p (θP ) p (ξP ) dξP dθP
∝
Z Z Y
i,k

p (yik | θi, ξk) p (θi | θP ) p (ξk | ξP )

·
p (θP ) p (ξP ) dξP dθP .
(2.11)
The hierarchical prior model can be recognized in the ﬁrst equation, where
the (hyper)parameters of the prior also have a prior density. The item and
person parameters are assumed to be independent from each other, and the
corresponding hyperparameters are also assumed to be independent from each
other. This leads to the factorization in the second equation.
Typically, the observations are assumed to be conditionally and indepen-
dently distributed given item and person parameters. That is, the observations
are assumed to be clustered in a cross-classiﬁed way. Furthermore, the person
parameters as well as the item parameters are assumed to be independent
from one another. As a result, the joint posterior of the parameters of interest
can be expressed as a product of identically distributed observations given
person and item parameters, where the person and the item parameters are
identically distributed given common hyperparameters; see Equation (2.11).
The last factorization, Equation (2.11), illustrates the hierarchical model-
ing approach. The observations are modeled conditionally independent at the
ﬁrst stage given item and person parameters, p (yik | θi, ξk). This is the likeli-
hood part of the model which describes the distribution of the data given ﬁrst-
stage parameters. At the second stage, priors are speciﬁed for the ﬁrst-stage
parameters. The ﬁrst-stage priors consist of a prior describing the between-
individual heterogeneity, p (θi | θP ), and a prior describing the between-item
heterogeneity, p (ξk | ξP ). At the third stage, hyperpriors are deﬁned for the
parameters of the ﬁrst-stage priors.
Typically, the variability between individuals is modeled via a condition-
ally independent prior by conditioning on second-stage parameters. This al-
lows making inferences independently of other respondents, and the condi-
tional independence assumption simpliﬁes the joint prior for the numerous
person parameters. In the same way, the prior that describes the between-item
variability in item characteristics assumes independent item characteristics
given second-stage parameters. The second-stage parameters or hyperparam-
eters control the priors for the lower-level parameters. Then, a second-stage
prior for the hyperparameters, p (θP ) and p (ξP ), is deﬁned. The combination
of a likelihood and a hierarchical prior that consists of a ﬁrst-stage conditional
independent prior and a second-stage prior for the hyperparameters is called
a hierarchical Bayes model.
Inferences about the parameters of interest, the ﬁrst-stage parameters, are
based on information from the data and prior information. The contribution

2.2 From Beliefs to Prior Distributions
41
of the ﬁrst-stage prior to the posterior depends on the values of the prior
parameters. For example, the posterior means of the ﬁrst-stage parameters will
show an amount of shrinkage towards the prior mean depending on the values
of the hyperparameters. More shrinkage will occur when specifying a more
informative ﬁrst-stage prior. Assessment of the hyperparameters is diﬃcult,
and it is desirable to let the level of control of the ﬁrst-stage prior be driven
by information in the data. Therefore, the hyperparameters are modeled at
the second stage of the prior. As a result, the priors’ contribution to the
posterior is arranged by information in the data taking the hyperparameters’
uncertainty into account.
Posterior Computation
The integration problem quickly expands when computing marginal posterior
means for all parameters and when extending the prior and/or likelihood
model. For example, the computation of the ability posterior density requires
integration over the item parameter, item population, and ability population
densities. This leads to the computation of
p (θ | y) ∝
Z Z Z
p (y | θ, ξ) p (θ | θP ) p (θP )
p (ξ | ξP ) p (ξP ) dθP dξ dξP .
(2.12)
When assuming normal priors, the integrations might be performed via a
Gauss Hermite. Although other quadrature (numerical integration) methods
are available, a quadrature approach is limited to a certain range of integration
problems.
Obtaining a satisfactory approximation of the integration problem in
(2.12) via numerical integration is a complex task, and the available numer-
ical integration methods cannot handle integrals in dimensions greater than
four. It will be shown that in large-scale survey research, the computation of
properties of posteriors of interest is often further complicated due to the use
of complex parameter spaces (e.g., constrained parameter sets), complex sam-
pling models with intractable likelihoods (e.g., due to the presence of missing
data), an extremely large dataset, complex prior distributions, or a complex
inferential procedure.
In the next chapter, simulation-based Bayesian estimation methods are
discussed that are capable of estimating all model parameters simultaneously.
This powerful estimation method supports the hierarchical modeling frame-
work in a natural way. In subsequent chapters, it will be shown that all kinds
of model extensions can also be handled by the computational methods dis-
cussed.

42
2 Bayesian Hierarchical Response Modeling
2.3 Further Reading
Hierarchical response modeling makes up part of the popular hierarchical
modeling framework, which has a tremendous amount of literature. The hi-
erarchical models are used in many areas of statistics. For a more complete
overview with applications in the educational and social sciences, see, for
example, Goldstein (2003), Longford (1993), Raudenbush and Bryk (2002),
Skrondal and Rabe-Hesketh (2004), and Snijders and Bosker (1999).
Recently, the Bayesian formulation (Berger, 1985; Lindley and Smith,
1972) has received much attention. Congdon (2001), Gelman and Hill (2007),
and Rossi et al. (2005) show various examples of Bayesian hierarchical mod-
eling using WinBUGS and R. The Bayesian modeling approach accounts for
uncertainty in the variance parameters, which is particularly important when
the hierarchical variances are diﬃcult to estimate or to distinguish from zero
(see Carlin and Louis, 1996; Gelman et al., 1995). The Bayesian hierarchical
modeling approach for response data will be thoroughly discussed in subse-
quent chapters.
The Bayesian hierarchical item response modeling framework has been ad-
vocated by Mislevy (1986), Novick, Lewis and Jackson (1973), Swaminathan
and Giﬀord (1982, 1985), Tsutakawa and Lin (1986), and Tsutakawa and
Soltys (1988). Swaminathan and Giﬀord (1982) deﬁned a hierarchical Rasch
model with, at the second stage, normal priors for the ability and diﬃculty
parameters. At the third stage, parameters of the prior for the ability param-
eters were ﬁxed to identify the model. The mean and variance parameters
of the exchangeable prior for the diﬃculty parameters were assumed to be
uniformly and inverse chi-square distributed, respectively. For the hierarchi-
cal two-parameter model, a chi-square prior was additionally speciﬁed for the
discrimination parameters with ﬁxed parameters (Swaminathan and Giﬀord,
1985, 1986). Tsutakawa and Lin (1986) described a hierarchical prior for the
diﬃculty parameters and a standard normal prior for the ability parameters.
In the 1990s, the hierarchical item response modeling approach was picked
up by Kim et al. (1994) and Bradlow et al. (1999). After the millennium, the
hierarchical item response modeling approach became more popular. This
approach will be further pursued in Chapter 4, where several examples are
given to illustrate its usefulness.

2.4 Exercises
43
2.4 Exercises
The exercises can be made using WinBUGS and are focused on hierarchical
item response modeling. Diﬀerent response models are speciﬁed in WinBUGS,
and the output is used for making posterior inferences. The computational
aspects will be discussed in Chapter 3. When it is required, run one chain of
10,000 MCMC iterations and use the last 5,000 iterations.
2.1. Johnson and Albert (1999) consider mathematics placement test data
of freshmen at Bowling Green State University. The mathematics placement
test is designed to assess the math skill levels of the entering students and
recommend an appropriate ﬁrst college math class. The test form B consists
of 35 (dichotomously scored) multiple-choice items.
(a) Adjust the code in Listing 1.1 to ﬁt a two-parameter logistic response
model. Obtain item parameter estimates given the ﬁxed hyperparameter val-
ues.
(b) Explain that a less informative prior is deﬁned when increasing the prior
variance.
(c) Estimate the item parameters for diﬀerent item prior variances.
(d) Observe and explain the eﬀects of shrinkage by comparing the item pa-
rameter estimates.
(e) Evaluate the estimated posterior standard deviations of the item param-
eters. Do they change due to diﬀerent hyperparameter settings?
2.2. (continuation of Exercise 2.1) In this problem, attention is focused on
assessing the parameters of the diﬃculty and discrimination prior.
(a) Explain the hierarchical item prior speciﬁed in Listing 2.1.
Listing 2.1. WinBUGS code: Independent hierarchical prior for item parameters.
for ( k
in
1 :K){
a [ k ]
˜ dnorm(mu[ 1 ] , prec [ 1 ] ) I ( 0 , )
b [ k ]
˜ dnorm(mu[ 2 ] , prec [ 2 ] )
}
mu[ 1 ]
˜ dnorm( 1 , 1 . 0E−02)
mu[ 2 ]
˜ dnorm( 0 , 1 . 0E−02)
prec [ 1 ]
˜ dgamma( 1 , 1 )
sigma [ 1 ] <−1/prec [ 1 ]
prec [ 2 ]
˜ dgamma( 1 , 1 )
sigma [ 2 ] <−1/prec [ 2 ]
(b) Fit the two-parameter logistic response model with the independent hier-
archical prior for the item parameters. Investigate the inﬂuence of the second-
stage prior settings. (Use appropriate starting values for the hyperparame-
ters.)
(c) Plot the estimated posterior means of the discrimination parameters
against the diﬀerences between the estimated posterior means of Exercises

44
2 Bayesian Hierarchical Response Modeling
2.2(b) and 2.1(a), and explain the graph. Do the same for the diﬃculty pa-
rameters.
2.3. (continuation of Exercise 2.1) In this problem, the within-item charac-
teristic dependencies are investigated.
(a) Given the results from Exercise 2.2(b), plot the posterior means of the
diﬃculty parameters against the posterior means of the discrimination pa-
rameters and see whether there is a trend visible.
(b) A hierarchical prior for the item parameters is speciﬁed in Listing 2.2.
Explain that the discrimination parameters are positively restricted by a log
transformation.
Listing 2.2. WinBUGS code: Hierarchical prior for item parameters.
for
( k
in
1 :K)
{
item [ k , 1 : 2 ]
˜ dmnorm(mu[ 1 : 2 ] , prec [ 1 : 2 , 1 : 2 ] )
a [ k ] <−exp( item [ k , 1 ] )
b [ k ] <−item [ k , 2 ]
}
prec [ 1 : 2 , 1 : 2 ]
˜ dwish(S [ 1 : 2 , 1 : 2 ] , 4 )
Sigma [ 1 : 2 , 1 : 2 ] <−inverse ( prec [ 1 : 2 , 1 : 2 ] )
(c) Use the code in Listing 2.2 to deﬁne a two-parameter logistic response
model with the hierarchical prior for the item parameters, and ﬁt the model.
(Use appropriate starting values for the hyperparameters.)
(d) Evaluate the estimated posterior means of the within-item covariance
parameters, and explain the results.
2.4. (continuation of Exercise 2.1) In this problem, the three-parameter logis-
tic response model is used to study guessing behavior of students taking the
mathematics placement test.
(a) Adjust the code of Listing 1.1 to deﬁne a three-parameter response model
(Equation (1.5)) with a beta prior for the guessing parameters (Equation
(2.7)).
(b) The parameters α and β of the beta prior specify the mean and variance
of the density via
E (ck) =
α
α + β ,
Var (ck) =
αβ
(α + β)2(α + β + 1),
and the beta density is unimodal when both parameters are greater than one.
Specify the beta parameters such that a priori the mean guessing probability
is .25 (random guessing) with a standard deviation greater than .1. Fit the
three-parameter model, and evaluate the estimated posterior means of the
guessing parameters. Do they vary around the prior mean?
(c) Assume, at the second prior stage, that the parameters of the beta prior
are uniformly distributed on the interval [2, 100], and ﬁt the three-parameter
model. Does the beta prior dominate the ﬁnal results?

3
Basic Elements of Bayesian Statistics
A review of Bayesian estimation and testing methods is given that is not a
thorough overview but concentrates on some speciﬁc elements. First, simu-
lation-based methods for parameter estimation, like the Gibbs sampling and
the Metropolis-Hastings algorithms, from the general class of Markov chain
Monte Carlo algorithms, are discussed. Second, the Bayesian approach to
model selection and hypothesis testing is presented. The techniques and meth-
ods described in this chapter are needed to completely exploit the Bayesian
machinery for item response modeling.
3.1 Bayesian Computational Methods
The approach to the computational problems mentioned in Section 2.2.2 is
based on computer simulations that exploit the probabilistic properties of
the integrands and can handle the dimensionality of the problem. Numerical
integration methods are often based on analytical properties of the integrand
where less gain is to be expected, and they often rely on speciﬁc knowledge
of the distribution of interest. Furthermore, simulation-based methods are
appealing for two reasons. First, integration problems often involve probability
distributions in the integrand, which insinuates the use of simulation methods.
Second, the simulation methods are generally straightforward to implement
since they are often based on a few principles of simulation and the structure
of the problem.
The introduction of powerful simulation methods made Bayesian modeling
possible in various research ﬁelds covering a wide range of applications. The
posterior simulation methods make the posterior distributions accessible; that
is, the algorithms for posterior simulation can be used to obtain approximates
of posterior moments.
©
and Behavioral Sciences, DOI 10.1007/978-1-4419-0742-4_3,
45
J.-P. Fox, Bayesian Item Response Modeling: Theory and Applications, Statistics for Social
  Springer Science+Business Media, LLC 2010

46
3 Basic Elements of Bayesian Statistics
3.1.1 Markov Chain Monte Carlo Methods
Simulating values directly from the posterior of interest (direct sampling) is
often not possible. Intractable marginal posteriors and the dimensionality of
the problem lead to diﬃculties in obtaining directly simulated values from
the posteriors. In general, more sophisticated methods are needed. A class
of simulation methods known as Markov chain Monte Carlo (MCMC) build
sequences that converge in distribution to the posterior (target) distribution.
Then, sample averages are computed to estimate posterior expectations. Es-
sentially, MCMC is Monte Carlo integration using Markov chains. In Monte
Carlo integration, samples are drawn to perform the integration (Exercise 3.4).
In MCMC, the samples are drawn via a constructed Markov chain that has
the target distribution as its stationary distribution. The number of MCMC
methods is growing rapidly, and this overview is not meant to be exhaustive.
It is an overview of methods that have proven to be useful in the analysis
of item response data. For a more complete overview, see Chen, Shao and
Ibrahim (2000), Gilks, Richardson and Spiegelhalter (1995), and Robert and
Casella (1999), among others.
Gibbs Sampling
The most popular MCMC method is Gibbs sampling, its name originating
from a class of probability distributions for modeling spatial interactions and
spatial stochastic processes (e.g., Geman and Geman, 1984). It starts with
the partitioning or blocking of parameters or random vectors of interest in
subvectors θ = (θ1, . . . , θQ). The joint posterior density of the random vec-
tor equals p(θ | y), and this is also the target density. A transition process
from θ(m) to θ(m+1) is deﬁned by making draws at iteration m + 1 from the
conditional pdf of each subvector,
θ(m+1)
1
∼p

θ1 | θ(m)
2
, . . . , θ(m)
Q , y

θ(m+1)
2
∼p

θ2 | θ(m+1)
1
, θ(m)
3
, . . . , θ(m)
Q , y

...
θ(m+1)
Q
∼p

θQ | θ(m+1)
1
, . . . , θ(m+1)
Q−1 , y

.
The Gibbs sampler manages the transition process, and the form of the condi-
tional densities and the choice of blocking characterizes each sampler. Under
some regularity conditions, the MCMC chain has a stationary density equal
to p(θ | y). This means that θ(m) converges to θ in distribution for m →∞
(see, e.g., Tierney, 1994).
The regularity conditions can be summarized as follows. The MCMC chain
needs to satisfy three assumptions. First, the chain is irreducible; that is, it can
reach every nonempty set with positive probability. Each state in the sample

3.1 Bayesian Computational Methods
47
space can be reached from any other state by repeatedly sampling from the
conditionals as described. Second, the chain is aperiodic. It is not allowed to be
periodic such that it can move between states in regular periodic movements.
Third, the chain is positive recurrent. A chain is positive recurrent when an
initial value is distributed according to the target distribution and then all
subsequent sampled values are distributed according to it (see, e.g., Roberts,
1995).
Metropolis-Hastings
The Metropolis-Hastings (M-H) algorithm was developed by Metropolis,
Rosenbluth, Rosenbluth, Teller and Teller (1953) and generalized by Hast-
ings (1970). The M-H algorithm generalizes the Gibbs sampler since it oﬀers
a solution to the problem of sampling from a conditional distribution, from
which it is diﬃcult to sample directly. In a two-step procedure, the surface of
the posterior of interest is explored via a Markov chain. In the ﬁrst step, a
candidate is drawn from a proposal density that may be chosen to approxi-
mate the desired posterior density. In the second step, this draw is accepted
or rejected based on a speciﬁed acceptance criterion. That is, an acceptance
probability is constructed from the posterior density ratio to evaluate the can-
didate with the last accepted draw. Simply stated, a candidate with high (low)
posterior density is likely to be accepted (rejected). The acceptance probabil-
ity ensures that the algorithm generates samples from the target density. The
proposal density is allowed to depend on the current state θ(m).
Technically, at iteration m, a candidate, θ∗
q, is drawn from a proposal
density, q
 θq | θ(m)
q

, and the transition from θ(m)
q
to θ(m+1)
q
= θ∗
q is made if
u(m) ≤
p
 θ∗
q | y

/q
 θ∗
q | θ(m)
q

p
 θ(m)
q
| y

/q
 θ(m)
q
| θ∗q
,
(3.1)
where u(m) is the mth observation from random variable U, which is uni-
formly distributed on the interval [0, 1]. If the proposed value is not accepted,
the chain remains at its current state. The right-hand side of (3.1) is known as
the acceptance ratio. The algorithm is very simple, and the shape of the pro-
posal density is not restricted. However, there is a strong relationship between
the proposal distribution and the target distribution with respect to the rate
of convergence to the target distribution. Two special cases can be considered.
First, when the proposal density is symmetric, q
 θ∗
q | θ(m)
q

= q
 θ(m)
q
| θ∗
q

,
the terms cancel out in the ratio deﬁned in (3.1) and the algorithm of Metropo-
lis et al. (1953) is obtained. Second, when the proposal density is not depen-
dent on former values, q
 θq | θ(m)
q

= q
 θq

, the Metropolis independent
chain (Tierney, 1994) is obtained.
It is eﬃcient to have an acceptance ratio close to one, but the proposed
values should also cover the entire range of probable values under the tar-
get distribution. That is, proposals are to be drawn from the entire region

48
3 Basic Elements of Bayesian Statistics
where p(θ | y) is appreciable. In most applications, it is eﬃcient to have a
proposal distribution with slightly heavier tails than the target distribution.
However, when the proposal distribution is too diﬀuse, many proposed values
are rejected and convergence to the target distribution is very slow. When the
proposal distribution has less heavy tails, the proposed values can be restricted
to a subregion of the entire sample space where p(θ | y) is appreciable, leading
not only to slow convergence but even biased results. This ﬁne-tuning of the
proposal distribution is very important since it highly aﬀects the convergence
of the algorithm. In high-dimensional problems, it is often helpful to explore
the eﬀects of diﬀerent proposals and their parameter choices. This is useful for
establishing fast convergence but also for rapid mixing of the chain through
the parameter space.
The M-H algorithm can be used within a Gibbs sampler, also denoted
as M-H within Gibbs. This retains the idea of sequential sampling but M-H
steps are used on some variables rather than attempting to sample from the
exact conditional distribution, which might not be feasible. This way, a hybrid
algorithm is deﬁned where the sampled values converge in distribution to the
target.
The M-H algorithm is useful when a proposal distribution can be found
from which it is easy to simulate that leads to accepted values and an accept-
able convergence rate and does not complicate computation of the acceptance
ratio. Note that the target density is only present in the algorithm via the
acceptance ratio; therefore, it suﬃces to evaluate p(.) in the acceptance ratio
(3.1) up to a normalizing constant.
Issues in MCMC
The iterative nature of an MCMC algorithm presents two problems. First, it
must be decided when the algorithm has reached convergence after a ﬁnite
number of iterations. Second, the MCMC samples drawn are dependent in
the sense that values close to one another in the sequence are more alike than
values that are far apart. The latter leads to problems in estimating the error
variance of an MCMC estimate.
Assessing convergence can be done via a single chain or multiple chains.
The subject of whether inferences are to be made from one long chain or
multiple chains has received much attention (e.g, Brooks and Gelman, 1998;
Cowles and Carlin, 1996; Gelman, 1995; Geyer, 1992; Raftery, 1995). A single
chain can be more precise since fewer burn-in iterations are discarded and
multiple runs that are too short cannot be used for inference. The use of
parallel chains reduces the variability and dependency on the initial values.
With parallel chains it is often easier to establish convergence by comparing
quantities of diﬀerent chains. When the cost of running multiple chains is not
that expensive using multiple computers or parallel processors, it is the most
simple and most reliable way to go.

3.1 Bayesian Computational Methods
49
Single Chain Analysis
The number of iterations before a chain converges to the stationary distribu-
tion (burn-in period) depends on several factors, such as the starting values
or starting distribution, amount of missing data, identiﬁcation rules, imple-
mentation strategies, and rate of convergence. Various methods have been
proposed to establish the burn-in time. Computing theoretical upper bounds
on burn-in time is diﬃcult for relatively simple problems (e.g., Roberts and
Tweedie, 1999; Rosenthal, 1995). The usefulness of these methods is rather
limited in the case of more complex models due to the computational burden
that is involved. Therefore, most users apply the more easy to use conver-
gence diagnostics to sampler output. An overview of MCMC diagnostics can
be found in Cowles and Carlin (1996).
The most popular way is to plot the iterates of the parameters from the
simulation runs and monitor trends. The plot is used to detect abnormal or
nonstationary behavior of the chain and investigate the convergence of the
empirical average and the mixing of the chain by looking for trends. Plots
of successive draws are so-called trace plots or time-series plots, and they
may be misleading when the chain is slowly mixing. The autocorrelations be-
tween successive iterates can be monitored to investigate their relationships
(Equation 3.3). A slow convergence to stationarity often corresponds to highly
correlated samples. Note that trace and autocorrelation plots can only pro-
vide indications of convergence. After establishing convergence with respect
to some model parameters, it is still possible that the algorithm has not con-
verged with respect to some components or functions of model parameters
that were not examined. It is also possible that the algorithm has an inade-
quate opportunity to visit some part of the parameter space due to multiple
modes and oddly shaped posteriors.
The BOA (Smith, 2010) and CODA (Best, Cowles and Vines, 2010) soft-
ware for diagnosing convergence of the distribution of the iterates to the sta-
tionary distribution include most common methods (Gelman and Rubin, 1992;
Geweke, 1992; Heidelberg and Welch, 1983; Raftery and Lewis, 1992).
The next step is establishing convergence of posterior summaries like the
marginal mean and variance. Let θ(m), m = 1, . . . , M, denote the output from
one simulation run. This sample can be used to estimate the posterior mean.
That is,
ˆθM =
M
X
m=1
θ(m).
M
(3.2)
approximates the posterior mean given dependent samples from the poste-
rior. The central limit theorem ensures that ˆθM converges to the (existing)
expected value of θ under the target distribution if M →∞(Tierney, 1994).
Furthermore, the same holds for any real-valued function of θ. The popula-
tion density can be estimated via a histogram of the sampled values. Although
most summaries that are appropriate for an independent random sample are

50
3 Basic Elements of Bayesian Statistics
appropriate for a dependent sample, the variance of the sample average in
Equation (3.2) is not the sample variance
1
M −1
X
m

θ(m) −ˆθM
2
when the sampled values are highly correlated. A variety of methods have
been proposed to estimate the variance of an MCMC estimate. Geyer (1992)
proposed several time-series methods for estimating the variance. Moreover,
Chen et al. (2000) reviewed several approaches. Estimating the variance from
a single chain is also possible by resampling from the sampled values to obtain
an approximately independent subsample. This procedure is not very eﬃcient,
and when M iterations are made it is better to summarize the results based
on the M iterates since the average of M iterates is more precise than the
average of a related subsample (e.g., MacEachern and Berliner, 1994).
The sample autocorrelation at lag h is deﬁned as
rh =
PM
m=h+1

θ(m) −ˆθM
 
θ(m−h) −ˆθM

PM
m=h+1

θ(m) −ˆθM
2
.
(3.3)
The correlations between adjacent iterates of an MCMC sequence can be mon-
itored using the sample autocorrelation estimates of diﬀerent orders, where
the iterates from the burn-in period are ignored.
Multiple Chain Analysis
A popular quantitative convergence diagnostic is the method of Gelman and
Rubin (1992). In that case, R chains of length M are simulated and iterate m
from run r is denoted as θ(r,m). The idea is to establish convergence by com-
paring the between-run variation, σb, with the (average) within-run variation,
σw, where
ˆσb =
M
R −1
X
r
 ˆθrM −ˆθM
2,
ˆσw =
1
R(M −1)
X
r
X
m
 θ(r,m) −ˆθrM
2.
Here, ˆθrM denotes the sample average of M iterates from run r and ˆθM the
sample average across the R runs. An estimate of the variance of the sample
average is obtained via a weighted average of σw and σb and equals
ˆσ = M −1
M
ˆσw + ˆσb
M .
This estimate is an unbiased variance estimate of the posterior mean of θ
when the sampled values are obtained from the target distribution.

3.2 Bayesian Hypothesis Testing
51
Gelman and Rubin (1992) considered the variance ratio term ˆσ/σw (scale
reduction factor) as an MCMC convergence diagnostic. For a large value of the
scale reduction factor and overdispersed starting values relative to the target
distribution, it is concluded that the between-run variation is much larger than
the within-run variation, and the between-run variation can be decreased or
the within-run variation can be increased by further simulations. When the
scale reduction factor is close to one, it is concluded that the simulated values
are close to being distributed according to the target distribution. At that
point, the within-run variance dominates the between-run variance, and it is
argued that the chains are no longer inﬂuenced by their starting values and
have traversed all of the target distribution.
The procedure has some limitations. First, the procedure relies on a nor-
mal approximation to the posterior, which might be inadequate. Second, it
can be diﬃcult to ﬁnd starting values or a starting distribution that is indeed
overdispersed with respect to the target distribution. Knowledge of the tar-
get distribution is needed to verify that the starting values are overdispersed.
Third, running multiple chains is very ineﬃcient when from each run a sub-
stantial number of iterations are discarded. For example, if one compares 10
single runs of 100 iterations with a single long run of 1,000 iterations, then
disregarding the ﬁrst 50 draws eliminates 500 draws from the multiple chains
and only 50 draws of the single long chain. Moreover, the last 900 simulated
values of the single long run are probably closer to the target distribution
than any of the values from the multiple chains.
A beneﬁcial eﬀect of running multiple chains is that it enables the com-
putation of a variance estimate. The variance term of any statistic computed
from output of a single run can be estimated when this single run is replicated
R times. The approach is essentially univariate, but a multivariate extension
has been proposed by Brooks and Gelman (1998).
3.2 Bayesian Hypothesis Testing
Bayesian inference involves specifying a hypothesis and collecting evidence
that supports or does not support the statistical hypothesis. The amount
of evidence can be used to specify the degree of belief in a hypothesis in
probabilistic terms. With enough evidence, the probability of supporting the
hypothesis can become very high or low. Hypotheses with a high degree of
belief in probabilistic terms are accepted as true, and those with low belief
are rejected as false.
The problem of comparing alternative hypotheses has received a lot of at-
tention and remains a subject of much discussion. A conﬂict in the classical
approach is that a frequentist’s one-sided hypothesis test uses a probability
(p-value) computed on all possible observations for a ﬁxed parameter value,
although the hypothesis is about a parameter value being restricted to a cer-
tain interval. Furthermore, the probability that the observations belong to

52
3 Basic Elements of Bayesian Statistics
a certain class is calculated using the null distribution. That is, attention is
focused not only on the observations that are made but also on others that
might have been made. Jeﬀreys (1961) remarked that in this sense the use of
a p-value implies that a hypothesis can be rejected because of a lack of pre-
dicted observations that have not occurred. Press (2003, pp. 220–224) gives
a complete overview of problems with frequentist methods for testing. These
peculiarities are avoided in a Bayesian approach, but other issues arise. The
probabilities of various hypotheses are computed given the data. However,
an exact hypothesis has zero probability since a continuous prior assigns zero
probability to any exact value. Furthermore, prior choices can highly inﬂuence
the estimation of posterior probabilities of the hypotheses and the associated
decision.
In this section, a short overview of Bayesian hypothesis testing is given.
Several situations will be looked at: one-sided and two-sided hypotheses, point
null or precise hypotheses, and the Bayes factor. A substantial amount of
literature can be considered for a more thorough treatment of hypothesis
testing from a Bayesian point of view (see, e.g., Berger and Delampady, 1987;
Berger and Selke, 1987; Jeﬀreys, 1961; Lee, 2004; Lindley, 1965; Zellner, 1971).
Let H0 represent a hypothesis, θ ∈Θ0, called a null hypothesis with
prior probability π0 and H1, θ ∈Θ1, the alternative hypothesis with prior
probability π1 = 1 −π0 such that Θ0 ∪Θ1 = Θ and Θ0 ∩Θ1 = ∅. The
Bayesian framework allows researchers to place probabilities on competing
hypotheses.
Let p(y | H0) denote the posterior density of y under the null hypothe-
sis. The posterior density p(y) is the marginal density of the data under all
mutually exclusive hypotheses. P(H0 | y) is called the posterior probability
of H0 given the data. This probability will be large when it is likely that the
observations are made when the null hypothesis under consideration is true.
The prior probabilities on the hypotheses are updated via Bayes’ theorem
given the observed data, which leads to the posterior probabilities of the hy-
potheses. For example, the posterior probability of the null hypothesis can be
expressed as
P(H0 | y) =
π0p(y | H0)
π0p(y | H0) + π1p(y | H1)
= π0
R
p(y | θ, H0)p(θ | H0)dθ
π0p(y | H0) + π1p(y | H1)
=
π0
R
θ∈Θ0 p(y | θ)p(θ) dθ
π0
R
θ∈Θ0 p(y | θ)p(θ)dθ + π1
R
θ∈Θ1 p(y | θ)p(θ) dθ , (3.4)
where it is assumed that the conditional prior p(θ | H0) and p(θ | H1) equal
p(θ) restricted to θ ∈Θ0 and θ ∈Θ1, respectively.
For a one-sided hypothesis test, Equation (3.4) can be reduced to the
posterior probability of the null hypothesis being true. Therefore, deﬁne g(θ)
as the unnormalized prior,

3.2 Bayesian Hypothesis Testing
53
p(θ) =
g(θ)
R
θ∈Θ0 g(θ) dθ
=
g(θ)
P
 θ ∈Θ0
.
The prior probability of the null hypothesis equals P(H0) = π0 = P(θ ∈Θ0).
As a result,
P
 H0 | y

=
R
θ∈Θ0 p(y | θ)g(θ) dθ
R
θ∈Θ0 p(y | θ)g(θ) dθ +
R
θ∈Θ1 p(y | θ)g(θ) dθ
=
R
θ∈Θ0 p(θ | y)p(y) dθ
R
θ∈Θ p(y | θ)g(θ) dθ
=
Z
θ∈Θ0
p(θ | y) dθ.
The null hypothesis is rejected when this posterior probability is less than
the signiﬁcance level. A one-sided hypothesis can be tested by computing its
posterior probability using the posterior distribution of θ. In the same way, a
Type I error (reject when H0 is true) and a Type II error (do not reject when
H0 is false) can be calculated given the posterior distribution of θ.
Multiple hypothesis testing can be done in a similar manner by assign-
ing prior probabilities to each hypothesis and calculating the corresponding
posterior probabilities (see, e.g., Zellner, 1997, pp. 364–382).
A point null hypothesis can be stated as θ = θ0 versus H1 θ ̸= θ0. In
practice, the hypothesis that θ = θ0 is never exactly the case due to, for
example, minor biases in the experiment. Therefore, a precise hypothesis is
often better represented as H0 : θ ∈Θ0 where Θ0 = (θ0 −ϵ, θ0 + ϵ) versus
H1:θ ̸∈Θ0 where ϵ is close to zero. This null hypothesis is approximated by
the point null hypothesis when the posterior probabilities are close.
A continuous prior density cannot be used since it assigns zero prior (and
posterior) probability to θ0. Therefore, a positive prior probability π0 is given
to θ0 and a continuous prior π1p(θ) is deﬁned for all θ ̸= θ0 where π1 = 1−π0.
Subsequently, the posterior probability of the null hypothesis equals
P(H0 | y) =
π0p(y | θ0)
π0p(y | θ0) + π1
R
θ̸=θ0 p(y | θ)p(θ) dθ
=
π0p(y | θ0)
π0p(y | θ0) + (1 −π0)p1(y)
(3.5)
= π0p(y | θ0)
p(y)
.
(3.6)
The Bayes factor (see, e.g., Berger, 1985; Jeﬀreys, 1961; Kass and Raftery,
1995) is deﬁned as the ratio of the posterior probabilities of the null and
alternative hypotheses over the ratio of prior probabilities of the null and

54
3 Basic Elements of Bayesian Statistics
alternative hypotheses. The Bayes factor in favor of the null can be constructed
from prior and posterior odds on H0 against H1,
P(H0 | y)
P(H1 | y) = p(y | H0)
p(y | H1)
P(H0)
P(H1),
and the Bayes factor equals
BF = P (H0 | y)
P (H1 | y)
P(H0)
P(H1) = p (y | H0)
p (y | H1).
In this respect, Equation (3.5) can be presented in such a way that it contains
the Bayes factor. Then, the posterior probability of the null hypothesis can
be found from its prior probability and the Bayes factor,
P(H0 | y) =
π0p(y | θ0)
π0p(y | θ0) + (1 −π0)p1(y)
=

1 + 1 −π0
π0
p1(y)
p(y | θ0)
−1
=

1 + 1 −π0
π0
BF −1
−1
.
The Bayes factor summarizes the evidence provided by the data. A value of
BF = 2 means that the null hypothesis is supported by the data two times
as much as the alternative hypothesis. Values of the Bayes factor are often
evaluated on the log10 scale (Jeﬀreys, 1961), and the evidence against the null
is called decisive if log10(BF) is less than 1/2.
The Bayes factor is usually inﬂuenced by prior information that is neither
vague nor diﬀuse, but hypothesis testing via a Bayes factor is only possible
using proper priors (integrate to unity). Modiﬁcations of the Bayes factor have
been proposed to accommodate it for improper priors. Most of them are based
on training data. Examples are the partial Bayes factor, intrinsic Bayes factor
of Berger and Pericchi (1996), and fractional Bayes factor of O’Hagan (1995).
Furthermore, a sensitivity analysis is needed to investigate inﬂuences of prior
choices on the outcomes of the Bayes factor. Sinharay and Stern (2002) showed
that diﬀerent prior distributions for the model parameters lead to diﬀerent
Bayes factors and a sensitivity analysis is needed before conclusions can be
drawn.
3.2.1 Computing the Bayes Factor
The computation of the Bayes factor requires numerical methods for eval-
uating the integrals. Typically, when an implemented MCMC algorithm is
available, methods for computing the Bayes factor are based on the available
posterior draws. Diﬀerent identities exist that express the Bayes factor as an
expectation of quantities such that available draws from the posteriors can be
used. Here, a few techniques will be described. Kass and Raftery (1995) and
Rossi et al. (2005), among others, reviewed and compared diﬀerent methods.

3.2 Bayesian Hypothesis Testing
55
Importance Sampling
Importance sampling is often used to estimate the value of integrals using ap-
proximate samples from complex high-dimensional distributions. The impor-
tance sampling method is especially useful in the computation of normalizing
constants; for example, in the computation of marginal likelihoods, the Bayes
factor, and likelihood inference. For a complete overview, see, for example,
Geweke (2005), Robert and Casella (1999), and references therein.
Under a hypothesis, say H0, the marginal likelihood of the data can be
expressed as
p (y | H0) =
Z
Rθ
p(y | θ)p(θ) dθ
=
R
Rθ p(y | θ)w(θ)q(θ)dθ
R
Rθ w(θ)q(θ) dθ
,
(3.7)
where w(θ) = p(θ)/q(θ) are known as importance ratios or importance
weights. The denominator in (3.7) appears when w(θ) is only known up to a
normalizing constant. Here, q(.) is the importance sampling density function
from which samples can be drawn. The integral is evaluated using samples
θ(m) from the importance sampling density:
ˆp (y | H0) = M −1 X
m
p

y | θ(m)
w

θ(m)
.
(3.8)
When the importance sampling function is known up to a constant, the es-
timate on the right-hand side of (3.8) is divided by P
m w(θ(m))/M to take
account of the normalizing constant. The choice of the importance sampling
function highly inﬂuences the accuracy of the outcome. Poor results are ob-
tained when the importance ratios are small with high probability and large
with low probability. This occurs when the integrand has wide tails com-
pared with the importance sampling density. To compute the Bayes factor,
the marginal likelihood needs to be estimated under both hypotheses, each
with a separate importance density.
Newton and Raftery (1994) proposed a speciﬁc importance sampling
function to evaluate the marginal likelihood of the data. They suggested
q(θ) = p(θ | y) since samples θ(m) from this posterior are often easily obtained
via an MCMC sampler. As a result,
ˆp(y) =
"
1
M
X
m
1
p
 y | θ(m)
#−1
(3.9)
is considered to be an estimate of the marginal likelihood of the data. This
harmonic mean of likelihood values is easily computed given sampled values
but can be unstable since the inverse likelihood does not have a ﬁnite variance.

56
3 Basic Elements of Bayesian Statistics
Using Identities and MCMC Output
Gelfand and Dey (1994) used an identity for estimating the marginal likelihood
that leads to
p(y)−1 =
Z
Rθ
f(θ)
p(y | θ)p(θ)p(θ | y) dθ
=
lim
M→∞M −1 X
m
f
 θ(m)
p
 y | θ(m)
p
 θ(m),
(3.10)
where f(θ) is a proper function having support contained in Rθ. In the case
where f(θ) = p(θ), the right-hand-side term in (3.10) resembles the right-
hand-side term in (3.9). A stable estimate of the marginal likelihood can be
obtained via a stability function f(.). Geweke (2005) showed how to construct
a probability density function f(.), a truncated multivariate normal density,
from the generated samples θ(m).
The bridge sampling estimator of Meng and Wong (1996) is based on draws
from the posterior and an importance density function q(θ). They provide an
identity that contains an arbitrary function f(θ) that can be used to estimate
the marginal likelihood,
ˆp (y) = M −1
q
P f
 θ(mq)
p
 θ(mq) | y

M −1 P f
 θ(m)
q
 θ(m) | y
 ,
where θ(mq) mq = 1, . . . , Mq are drawn from the importance density q(θ) and
θ(m) are drawn from the posterior density. The method of Gelfand and Dey
(1994) and importance sampling are special cases of bridge sampling where
the function f(.) equals the importance density function or the unnormalized
posterior, respectively.
For nested hypotheses, a useful identity exists that simpliﬁes the compu-
tation of the Bayes factor. Let θ = θ0 be the null hypothesis H0 and θ ̸= θ0
the alternative hypothesis H1. It follows that
p (y | ξ, H0) = p (y | θ = θ0, ξ, H1)
due to the nested nature of the hypotheses. Furthermore, suppose that
p (ξ | H0) = p (ξ | θ = θ0, H1) .
(3.11)
Then, it can be proven (see Exercise 6.3) that the Bayes factor equals
BF = p (θ = θ0 | y, H1)
p (θ = θ0 | H1)
(3.12)
=
lim
M→∞M −1 X
m
p
 θ = θ0 | ξ(m), y, H1

p (θ = θ0 | H1)
,

3.2 Bayesian Hypothesis Testing
57
where ξ(m) are sampled from p (ξ | y, H1). Equation (3.12) is referred to as
the Savage-Dickey density ratio. It is a widely used tool for estimating the
Bayes factor, but the priors under both hypotheses need to be conditionally
linked as in Equation (3.11). Verdinelli and Wasserman (1995) proposed a
generalization when the priors do not satisfy Equation (3.11).
Bayes Factor for Item Response Models
Response data provide information about the characteristics of the items and
the respondents in terms of abilities that are measured. Selecting more re-
spondents will improve the item parameter estimates but will automatically
increase the dimensionality of the ability parameter space. In the same way,
adding more items will improve the precision of the ability estimates but
will increase the dimensionality of the item parameter space. In both ways,
sampling more respondents and/or increasing the item set will enlarge the di-
mensionality of the parameter space. Typically, the complexity of the problem
of computing the marginal likelihood is getting more complex when observ-
ing more data. The unnormalized posterior density function that needs to be
integrated increases in dimension when observing more response data.
The discrete nature of the response data leads to a nonlinear likelihood
model. The joint and marginal posterior distributions of the item and person
parameters have unknown analytical properties in a high-dimensional param-
eter space. This highly complicates the computation of a marginal likelihood.
An asymptotic method for computing the Bayes factor is based on the
assumption that the unnormalized posterior converges to a normal distribu-
tion when increasing the number of observations. Then, a Taylor expansion of
the unnormalized posterior around the posterior mode is considered to be an
asymptotic approximation of the marginal likelihood. The Bayesian informa-
tion criterion (BIC; Schwarz, 1978) is based on an asymptotic approximation
to the marginal likelihood. Subsequently, twice the log of the Bayes factor is
approximately equal to the diﬀerence in the BIC. That is, the ∆BIC approx-
imates 2 log BF (see, e.g., Raftery, 1995) and is deﬁned as
∆BIC = 2 log
 
p(y | ˆθ, H0)
p(y | ˆθ, H1)
!
−(p2 −p1) log n,
(3.13)
where p1 and p2 are the number of parameters under H0 and H1, respectively,
n the number of observations, and ˆθ the value that maximizes p(y | θ).
However, the asymptotic approximation does not hold for item response
models where the number of model parameters increases with the number
of observations. Typically, the asymptotic approximation only holds when
increasing the number of observations while holding the dimension of the
parameter space constant. In response modeling, the BIC is known to be
extremely imprecise and is not recommended for estimating a Bayes factor.

58
3 Basic Elements of Bayesian Statistics
Another problem is that the within-individual and between-respondent
modeling structure leads to a hierarchical model that complicates the speciﬁ-
cation of the number of free parameters. For example, a tight prior constrains
the model and a ﬂat prior corresponds to a more ﬂexible model. The implied
prior structure is often needed to handle the high-dimensional parameter space
but often it is not clear in what way the prior reduces the number of free pa-
rameters.
Accurate computation of the Bayes factor for complex nonlinear response
models with an interest in respondent-level and item-level parameters is very
diﬃcult. The posterior distributions that comprehend information at diﬀerent
levels are high-dimensional objects and are not members of a known class of
distributions. This complicates the computation of the marginal likelihood,
which requires integrating out the parameters in the joint posterior density.
Below, for the class of response models where the Bayes factor is diﬃcult
to compute or where improper priors are used, alternative ways of testing
hypotheses are explored.
3.2.2 HPD Region Testing
Box and Tiao (1973) developed a procedure for Bayesian testing of signiﬁ-
cance that allows the use of diﬀuse priors. In this situation, interest is focused
on a speciﬁc null hypothesis. Typical examples are testing the equality of
population means and testing whether a regression coeﬃcient equals zero. In
both cases, the prior distribution under the null hypothesis that θ = θ0 is
ﬂat such that it is assumed that values near θ0 are as likely as θ = θ0. The
procedure is based on a Bayesian conﬁdence interval for a signiﬁcance level α
given the (unimodal) posterior density p(θ | y). Then, the null hypothesis is
rejected when θ0 falls outside this interval. The procedure is based on the fact
that the density function for θ can be used to investigate whether the region
where θ = θ0 has high posterior density. If the region where θ = θ0 has low
posterior density, then this value for θ is not likely and the null hypothesis
is rejected. This way of testing hypotheses is also known as highest posterior
density (HPD) interval testing or, in a multidimensional setting, HPD region
testing.
The interval method of testing hypotheses is based on the construction
of a Bayesian conﬁdence interval. In the literature, a Bayesian conﬁdence
interval (e.g., Box and Tiao, 1973; Lindley, 1965) is usually referred to as
a credible set or a credible interval. A credible set for a scalar parameter θ
(continuous valued) presents the set of values such that θ lies within this set
with probability 1−α. The posterior density is used to quantify the probability
that θ lies in a credible set. That is, a 100(1−α)% credible interval is a subset
C ⊂Θ for θ such that
P(C | y) =
Z
C
p(θ | y) dθ = 1 −α.
(3.14)

3.2 Bayesian Hypothesis Testing
59
An important defect of such a credible set is that it does not specify whether
values of θ within the set are more probable than values outside the set. It is
preferable to choose a credible set of values with the highest posterior density.
Therefore, an extension of the credible set is the highest posterior density
(HPD) interval that covers at least a probability of 1 −α and contains the
most likely values of θ,
P(C | y) ≥1 −α,
(3.15)
and for θ1 ∈C, θ2 ̸∈C it follows that p(θ1 | y) ≥p(θ2 | y). The posterior
density of every point inside the HPD interval is greater than that for every
point outside the interval.
For a multidimensional parameter θ, the credible set is deﬁned as a credible
region rather than a credible interval. In the same way, the HPD region has
the property that it has the smallest possible volume in the parameter space of
θ. For a symmetric unimodal posterior distribution, the HPD region equals an
equal-tail credible set that is deﬁned by taking the α/2 and 1 −α/2 quantiles
of the posterior distribution as the credible region with probability 1−α. Note
that a speciﬁc point θ0 is inside the HPD region if and only if
P
 p(θ | y) > p(θ0 | y) | y

≤1 −α,
(3.16)
where p(θ | y) is treated as a random variable. The expression (3.16) becomes
especially useful when it is possible to evaluate the probability content of
the distribution directly. For the HPD region method of hypothesis testing,
Box and Tiao (1973) speciﬁed an event directly or in terms of some mono-
tonic function in such a way that the probability that this event belongs to a
particular (HPD) region is easily computed.
Berger and Delampady (1987) argued that concentrating only on inter-
vals is not correct since it ignores the special nature of θ0. A speciﬁc point
outside an interval often has a likelihood that is not much smaller than the
average likelihood of θ within the region. Then, there is not much support
for rejecting the null hypothesis. Credible regions often correspond to diﬀuse
prior distributions and are not appropriate in the case of a single special value.
The Bayes factor is used to judge whether a speciﬁc point is supported by the
data, speciﬁcally when H0 is precise. However, the main advantage of credible
or HPD regions is that they display the actual gap between θ and θ0. Be-
sides the Bayes factor, HPD regions remain important since they indicate the
magnitude of the possible discrepancy. Furthermore, HPD regions may prove
worthwhile in situations where prior knowledge is vague, where the Bayes fac-
tor is diﬃcult to compute, or where the Bayes factor is highly inﬂuenced by
prior choices.
3.2.3 Bayesian Model Choice
The problem of model choice diﬀers from that of testing since it holds more
inferential goals and more demanding computational tasks than mere testing.

60
3 Basic Elements of Bayesian Statistics
The Bayes factor can still be used for model selection problems but depends
on prior densities for the parameters. Other criteria such as the BIC and
deviance information criterion (DIC; Spiegelhalter, Best, Carlin and van der
Linde, 2002) do not have to make a reference to prior densities for model
parameters. These selection methods become particularly interesting when
computation of the Bayes factor is (almost) impossible. Both methods are
based on a measure of ﬁt and some penalty function based on the number
of free parameters for the complexity of the model. A bias–variance trade-oﬀ
exists between these two quantities since a more complex model often leads
to a better ﬁt but a less complex model involves more accurate estimation.
In a setting where there are two competing models denoted as M1 and
M2 that have respective parameters θ1 and θ2, the object is to select the
best model for the observed data y. When prior probabilities πi (i = 1, 2) are
speciﬁed for both models, the posterior probability of model Mi equals
P(Mi | y) =
πi
R
Θi p(y | θi)p(θi) dθi
P
j=1,2 πj
R
Θj p(y | θj)p(θj) dθj
,
according to Bayes’ theorem. Subsequently, the model with the highest poste-
rior density is selected. This procedure is easily extended to more models. The
Bayes factor involves the computation of possibly high-dimensional and in-
tractable integrals. Furthermore, the Bayes factor for model selection depends
on prior choices, and the sensitivity to them should be investigated.
The BIC deﬁnes the change in the conditional density of the data of model
M2 to that of model M1 plus a penalty term for the extra parameters in
model M2. This diﬀerence in size between models can be seen by considering
model M2 to be the saturated or full model and M1 the reduced model.
Subsequently, if the BIC in (3.13) is negative, model M1 is preferred by the
data, and if the BIC is positive, model M2 ﬁts the data better.
The BIC is a rough but straightforward penalized likelihood ratio method
that is easy to compute and enables the direct comparison of nonnested mod-
els. However, diﬃculties may arise in specifying the diﬀerence in the number of
parameters between models and/or the number of observations. For example,
the eﬀective number of parameters in a hierarchical model is often diﬃcult to
calculate. Although the nominal number of parameters follows directly from
the likelihood, the prior distribution imposes additional restrictions on the pa-
rameter space and reduces its eﬀective dimension. In a random eﬀects model,
the eﬀective number of parameters depends strongly on the higher-level vari-
ance parameters. When the variance of the random eﬀects approaches zero,
all random eﬀects are equal and the model reduces to a simple linear model
with one mean parameter. But when the variance goes to inﬁnity, the number
of free parameters approaches the number of random eﬀects.
Spiegelhalter et al. (2002) proposed the deviance information criterion
(DIC) for model comparison when the number of parameters is not clearly
deﬁned. The DIC is deﬁned as the sum of a deviance measure and a penalty

3.3 Discussion and Further Reading
61
term for the eﬀective number of parameters based on a measure of model
complexity described below. The deviance deﬁned as D(θ) = −2 log p(y |
θ) + 2 log p(y) is not a suitable discriminating measure between models since
it will always prefer the higher-dimensional models. Therefore, a penalizing
term based on the estimated number of eﬀective parameters is added to correct
for the bias of the deviance towards higher-dimensional models. This term
estimates the number of eﬀective model parameters and equals
pD = E
 −2 log p(y | θ)

+ 2 log p
 y | ˆθ

= D(θ) −D
 ˆθ

,
where the expectation is taken with respect to the pdf of θ given the data and
ˆθ is an estimate of θ. Here, D(θ) is the posterior mean deviance and D
 ˆθ

the estimated deviance given a posterior estimate of θ. Subsequently, the DIC
is deﬁned as
DIC = D(θ) +

D(θ) −D
 ˆθ

= D(θ) + pD
= D
 ˆθ

+ 2pD.
Given a closed form of D(θ), the posterior mean of the deviance can be esti-
mated given simulated values of D(θ) using, for example, MCMC. The term
D
 ˆθ

is approximated by plugging in an estimate for θ. Note that for model
comparisons it can be assumed that the standardizing factor p(y) equals one
such that D(θ) = −2 log p(y | θ). Finally, the best model is associated with
the smallest DIC value.
3.3 Discussion and Further Reading
Bayesian modeling is focused on a likelihood function and priors. They are
the building blocks, and statistical inferences are based on the product that
deﬁnes the posterior of the unknown model parameters. Those interested in
a complete introduction are referred to Berger (1985), Bernardo and Smith
(1994), Box and Tiao (1973), Gelman et al. (1995), Press (2003), or Zellner
(1971).
The computation of high-dimensional posterior densities, and summarizing
information from them, is possible mainly through the use of MCMC simula-
tion algorithms. MCMC methods are particularly suited for the hierarchical
response models, including cross-classiﬁed hierarchies. The hierarchical models
are built on conditional distributions and therefore give rise to the construc-
tion of an MCMC sampler (e.g., Gelman and Hill, 2007; Leonard and Hsu,
1999; Rossi et al., 2005). For example, the problem of sampling individual-
level parameters is simpliﬁed by conditioning on the higher-level parameters.

62
3 Basic Elements of Bayesian Statistics
The iterative nature of MCMC methods makes it possible to estimate simul-
taneously the parameters of complex hierarchical models for cross-classiﬁed
response data.
More complex models can be constructed by adding layers to the hierarchy,
which can be handled by MCMC methods since they just lead to additional
sampling steps. A variety of posterior simulation methods have been developed
since the 1990s, and a more complete overview can be found in Geweke (2005)
and Robert and Casella (1999). In a shortcut approximation of a fully Bayesian
analysis of hierarchical models, higher-level parameters are estimated using
the data and then plugged into the lower levels. An overview of this empirical
Bayes estimation can be found in Carlin and Louis (1996) and Morris (1983).
There is extensive literature about the Bayesian approach to hypothesis
testing. An introduction is given by Berger (1985), Jeﬀreys (1961), and Press
(2003). An introduction from a decision-making point of view can be found in
DeGroot (1970) and Zellner (1971). In Chapter 5, more attention will be given
to model choice and assessment, and prior and posterior predictive methods
are also considered.
3.4 Exercises
The exercises provide insight and background information for various statis-
tical results that will be used further on.
3.1. A Bayesian analysis of the normal mean is considered. Let the observa-
tions Yi (i = 1, . . . , n) be independently normally distributed with mean θ
and variance σ2.
(a) Show that the likelihood function is given by
p
 y1, . . . , yn | θ, σ2
=
 2πσ2−n/2 exp

−1
2σ2

S + n(ˆθ −θ)2
,
where ˆθ = P yi/n and S = P
i
 yi −ˆθ
2.
(b) Assume a standard noninformative prior p (θ, σ) ∝σ−1, and show that
the posterior density p(θ | y, σ2) is a normal density with mean ˆθ and variance
σ2/n.
3.2. (continuation of Exercise 3.1) The posterior density of θ is constructed
from normally distributed sample information Y and a normal prior with
mean θ0 and variance σ2
0 (see, e.g., Box and Tiao, 1973; Zellner, 1971).
(a) Derive the posterior density of θ given y, σ2 from
p(θ | y, σ2) ∝p(y | θ, σ2)p(θ)
∝exp
 
−1
2σ2
X
i
(yi −θ)2
!
exp

−1
2σ2
0
(θ −θ0)2

∝exp

−1
2σ2 n(θ −ˆθ)2 −
1
2σ2
0
(θ −θ0)2

(3.17)

3.4 Exercises
63
by factorizing (σ2
0 + σ2/n)/(2σ2
0σ2/n) in (3.17).
(b) Show that Y and θ are distributed as
 Y
θ

∼N
 1nθ0
θ0

,
σ2In + σ2
0Jn σ2
01n
σ2
01t
n
σ2
0

where 1n is a vector of ones of length n, In the unity matrix of dimension n,
and Jn a matrix of ones of dimension n.
(c) Show via (a) and (b) that the posterior expectation of θ equals
E(θ | y) =
ˆθn/σ2 + θ0/σ2
0
n/σ2 + 1/σ2
0
(3.18)
and
E(θ | y) = θ0 + σ2
01t
n(σ2In + σ2
0Jn)−1(y −θ0),
(3.19)
respectively, and that (3.18) equals (3.19) using the identity
 σ2In + σ2
0Jn
−1 = σ−2

In −
σ2
0
σ2 + nσ2
0
Jn

.
(3.20)
3.3. The posterior expectation in Equation (3.18) is referred to as the Bayes
estimate of θ, where ˆθ is the least squares estimate (Lindley and Smith, 1972).
(a) Show that the Bayes estimate is biased in contrast to the least squares
estimate.
(b) Argue that the Bayes estimate is a weighted average, and explain the inﬂu-
ence of the variances and the number of observations. (Notice that the Bayes
estimate is particularly useful when the least squares estimate is unreliable.)
(c) Explain that the amount of shrinkage can be inferred from the data when
deﬁning priors for the variance parameters.
3.4. Let the posterior density of a linear regression parameter β depend on
observations y and explanatory observations x. Assume that samples can be
drawn from the posterior p (β | y, x).
(a) Explanatory values x are observed with error, and the measurement error
model is deﬁned by Xi ∼N(µ, 1). Explain that the marginal posterior of β
can be expressed as
p (β | y) =
Z
p (β | y, x) p (x | µ) dx.
(b) Monte Carlo integration allows the calculation of integrals using random
draws. Show how independent samples x(m) can be used to estimate the pos-
terior expected value E (β | y).
(c) Assume µ ∼N(0, 1), and show that draws µ(m) ∼p(µ) and X(m) | µ(m) ∼
p(x | µ) can be used to estimate the posterior expected value of β.
(d) In a Gibbs sampling scheme, draws are made from the full condition-
als. Argue that, depending on the prior choices, a Gibbs sampling algorithm
requires fewer iterations to obtain an accurate estimate.

64
3 Basic Elements of Bayesian Statistics
3.5. Assume that realizations of a normally distributed random variable are
never observed due to a censoring mechanism. Let Zi ∼N(θ, 1) denote the
(unobserved) underlying random variable and let Yi denote the censored ran-
dom variable.
(a) Assume that Yi = 1 if Zi > 0 and Yi = 0 if Zi ≤0, and show that
P (Yi = 1) = P (Zi > 0) = Φ (θ) .
(b) Show that the likelihood of the observed data can be expressed as
p (y | θ) =
Y
i
Φ(θ)yi (1 −Φ(θ))1−yi
=
Y
i
Z
p (zi, yi | θ) dzi.
(c) Explain that it is much easier to work with the complete data (y, z) for
making inferences concerning θ. (This motivates a Gibbs sampling algorithm
that includes the simulation of latent data z; see Chapter 4.)
3.6. Assume n observations (y1, . . . , yn) from a normally distributed random
variable, Y ∼N(θ, 1).
(a) Show that the posterior density of θ is normal with mean P
i yi/n = y
and variance 1/n given an improper prior p (θ) ∝1.
(b) Let U be uniformly distributed on the interval (0, 1), U ∼U(0,1). Making
use of the probability integral transformation,1 show that a draw θ∗from the
posterior density can be obtained via
θ∗= y +
1
√nΦ−1 (U) ,
where Φ−1(.) is the inverse cumulative normal distribution function.
(c) Given a standard normal prior, show how to draw θ∗from the posterior
density.
3.7. (continuation of Exercise 3.6) Assume that θ is uniformly distributed on
the interval (θ0, θ1).
(a) Let f(θ) = √n(θ−y). Show that the posterior probability of θ ≤θ∗equals
P (θ ≤θ∗| y) = Φ (f(θ∗)) −Φ (f(θ0))
Φ (f(θ1)) −Φ (f(θ0)).
(b) Let U be uniformly distributed on (0, 1). Show that a posterior draw θ∗
can be obtained via
1 Let Y have a continuous cumulative distribution function F(Y ), and deﬁne ran-
dom variable U as U = F(Y ). Then U is uniformly distributed on (0, 1) such that
P(U ≤u) = u for 0 < u < 1 (Casella and Berger, 2002).

3.4 Exercises
65
θ∗= y +
1
√nΦ−1 Φ(f(θ1)) −Φ(f(θ0))

U + Φ(f(θ0))

,
which is known as the inverse sampling method (Ripley, 1987).
(c) Assume that the observations are truncated to the region Ry = {yi : y0 <
yi < y1, i = 1, . . . , n}. Then, the conditional density of yi is given by
p (yi | θ) =
φ (yi; θ)
R y1
y0 φ (y; θ) dy .
Show that the parameter of interest θ is also present in the normalizing con-
stant of p(yi | θ), which hinders an inverse sampling method for simulating
values θ∗.
(d) Show that the cumulative probability of Yi ≤yi given θ equals
P(Yi ≤yi | θ) = Φ(yi −θ) −Φ(y0 −θ)
Φ(y1 −θ) −Φ(y0 −θ).
(e) A nontruncated random variable Zi (i = 1, . . . , n) can be deﬁned via an
inverse normal transformation,
Zi = θ + Φ−1  ˜Ui

,
where ˜Ui = P(Yi ≤yi | θ). Show how the augmented Zi can be used to draw
a value from the posterior density of θ.
3.8. Bayesian inferences and decision making under uncertainty can be done
via posterior probabilities of the hypotheses. There is uncertainty about θ
where under H0 it is assumed that θ ≤θ0 and under H1 it is assumed that
θ > θ0. A loss function L(a, θ) can be deﬁned for each action a,
L(a, θ) =



θ0 −θ accept H1, H0 true
θ −θ0 accept H0, H1 true
0
otherwise,
where the loss of an incorrect decision depends on the severity of the mistake.
(a) Show that the posterior expected loss with respect to the posterior density
p(θ | y) equals
E(L(a, θ) | y) = E(θ0 −θ | θ ≤θ0) + E(θ −θ0 | θ > θ0).
(b) Let Y denote the number of successes from n Bernoulli trials such that
Y ∼B(n, θ), and assume a beta prior for θ, denoted as Be(α, β). Show that the
marginal posterior density of θ is Be(α′, β′) with α′ = y+α and β′ = n−y+β.
(c) Derive that the posterior expected loss equals
E(L(a, θ) | y) = θ0
 2P(θ0 | y) −1

−
Z θ0
0
θα′(1 −θ)β′−1 dθ/B(α′, β′)
+
Z 1
θ0
θα′(1 −θ)β′−1 dθ/B(α′, β′),

66
3 Basic Elements of Bayesian Statistics
where B(α′, β′) is the normalizing constant and P(θ0 | y) the cumulative
posterior distribution function; that is,
B(α′, β′) =
Z 1
0
θα′−1(1 −θ)β′−1 dθ.
P(θ0 | y) =
Z θ0
0
θα′−1(1 −θ)β′−1 dθ/B(α′, β′).
3.9. (continuation of Exercise 3.1) Interest is focused on deﬁning an HPD
interval for the variance parameter.
(a) Show that the joint posterior density of (θ, σ2) given the noninformative
prior can be expressed as
p
 θ, σ2 | y

∝
 σ2−(n/2+1) exp

−1
2σ2

S + n(ˆθ −θ)2
.
(b) Verify that the marginal posterior density function of σ2 can be expressed
as
p
 σ2 | y

= p
 θ, σ2 | y

p (θ | σ2, y)
∝
 σ2−(n+1)/2 exp
 −S
2σ2

.
Show that σ2 is inverse chi-square distributed with ν = n −1 degrees of
freedom and scale parameter νs2, where s2 = S/ν.
(c) Derive the limits of a (1 −α) HPD interval for σ2.
(d) An HPD interval is not invariant under a nonlinear transformation of the
parameters. Show that the endpoints of the HPD interval of Exercise 3.9(c)
are not proportional to the endpoints of an HPD interval of σ.
(e) Assume that an MCMC sample σ2(m) (m = 1, . . . , M) is obtained from the
(unimodal) marginal posterior density of p(σ2 | y). Construct an estimator of
the (1 −α) credible interval based on the ordered MCMC sample.
(f) Show how to compute a (1−α) HPD interval using the order statistics es-
timator. Note that an HPD interval is that credible interval with the smallest
width. (Chen and Shao, 1999, proved convergence results of the order statis-
tics estimator for unimodal posteriors and gave a conjecture for multimodal
posteriors.)

4
Estimation of Bayesian Item Response Models
The general form of a Bayesian item response model consists of a probability
model for the responses, prior distributions for the model parameters, and
possibly prior distributions for the hyperparameters. An overview of Bayesian
procedures for simultaneous estimation is given in which MCMC estimation
methods are emphasized. Interest is focused on simultaneous estimation of
marginal posterior densities of item and person parameters.
4.1 Marginal Estimation and Integrals
The estimation of parameters is often characterized as the problem of estimat-
ing structural parameters in the presence of incidental or nuisance parameters
(e.g., Anderson, 1980; Ghosh, 1995). Some of the model parameters are of in-
terest and others are not especially interesting and can be treated as nuisance
parameters. The distinction between structural parameters and nuisance pa-
rameters is made with respect to the parameters that are to be estimated.
The object is to obtain the marginal posterior density of the structural model
parameters by integrating the joint posterior density of all parameters over
the density of the nuisance parameters. Once the marginal posterior density
of the parameters of interest is calculated, posterior probability statements,
intervals, the posterior mean, and the posterior mode can be derived relative
to the marginal posterior density. The marginal posterior density of the struc-
tural parameters summarizes all posterior information and is regarded as a
basis for Bayesian inference regarding the structural parameters.
A diﬀerent approach proposed by Swaminathan and Giﬀord (1982, 1985,
1986), avoids the integration problem. The maximizers of the joint posterior
distribution of nuisance and structural parameters are considered to be the
Bayesian estimates. A Newton-Raphson procedure is used to obtain the es-
timates that maximize the joint posterior. The joint posterior estimates are
considered to be inferior to the marginal posterior estimates. First, the joint
posterior estimates have to be based on informative priors that impose limits
©
and Behavioral Sciences, DOI 10.1007/978-1-4419-0742-4_4,
67
J.-P. Fox, Bayesian Item Response Modeling: Theory and Applications, Statistics for Social
  Springer Science+Business Media, LLC 2010

68
4 Estimation of Bayesian Item Response Models
on the range of values. Otherwise, the estimates may drift out of bounds. Sec-
ond, O’Hagan (1976) provided numerical evidence that marginal estimates are
more accurate than the nonmarginal estimates, especially when the nuisance
parameters are poorly determined in the joint posterior. Third, Mislevy (1986)
argued that marginal estimates are also preferred on the basis of asymptotic
behavior. In this case, the marginal posterior mode estimates correspond with
the Bayes modal estimates that are asymptotically normally distributed under
some regularity conditions. However, when the number of nuisance parameters
increases to inﬁnity and the number of structural parameters is held constant,
the regularity conditions are not always satisﬁed (Neyman and Scott, 1948).
As a result, asymptotic normality of the joint posterior mode estimator does
not hold, but it may hold for the marginal posterior mode estimator depending
on the prior distributions speciﬁed. Kim et al. (1994) performed a simulation
study to compare the marginal posterior estimates with the joint posterior es-
timates using simulated data generated under diﬀerent conditions. They found
smaller root mean square diﬀerences and bias for the marginal posterior item
parameter estimates, especially in the case of small samples and short tests.
In the literature, several procedures have been proposed to handle the
numerical integration problem for obtaining marginal likelihood or marginal
posterior estimates. Bock and Lieberman (1970) estimated the item parame-
ters on the assumption that the person parameters are normally distributed
and used Gauss-Hermite quadrature to perform the integration, where the
person parameters are treated as nuisance parameters. The main computa-
tional problem was the size of the information matrix that needs to be inverted
in the Newton-Raphson iterations to obtain the marginal estimates. Bock and
Aitkin (1981), Mislevy (1984), and Thissen (1982) explored an approach based
on the EM algorithm (Dempster, Laird and Rubin, 1977). In this approach,
they assumed independent items, independent respondents, and independence
between items and persons. Then, an EM solution can be obtained item by
item. That is, there is one log-likelihood per item, involving only the parame-
ters for that item. Interest is focused on estimating the item parameters, and
the person parameters are treated as nuisance parameters. The distribution
of person parameters is approximated by a discrete distribution covering a
ﬁnite set of levels that can be recognized as the set of quadrature points. The
term typically maximized that leads to marginal maximum likelihood item
parameter estimates equals
max l(y; ξ) =
Z
l(y; ξ, θ)p(θ) dθ,
(4.1)
where the left-hand side is the marginal likelihood function. The derivative
of the marginal log-likelihood that needs to be maximized is approximated
using suﬃcient statistics. The suﬃcient statistics, deﬁned at each individ-
ual latent variable level deﬁning so-called pseudo-data, are (1) the number
of respondents expected to have a certain latent variable level and (2) the
number of respondents with this level expected to correspond correctly. They

4.1 Marginal Estimation and Integrals
69
are updated in the E-step of the EM algorithm given estimates for the item
parameters. In the M-step, item parameter estimates are obtained given the
updated pseudo-data. Both steps are repeated until a convergence criterion is
satisﬁed. Note here that the assumption of normally distributed person pa-
rameters refers to a population model or to subjective uncertainty about the
person parameters and was not supposed to deﬁne a prior distribution.
Mislevy (1986) implemented this EM procedure of Bock and Aitkin (1981)
for the binary one-, two-, and three-parameter models including a hierarchical
prior model for the item and person parameters (see also Lord, 1986). Poste-
rior mode estimates (MAP, maximum a posteriori) were obtained instead of
marginal maximum likelihood estimates via the Bock and Aitkin procedure.
The posterior modes (MAP) are usually used as estimates for the parame-
ters of interest since they are easier to compute, although they are not invari-
ant with respect to marginalization. The mean of the posterior (EAP, expected
a posteriori) is invariant with respect to marginalization but is more diﬃcult
to compute when no closed-form solution is available. However, joint Bayes
modal estimates are obtained for item and population (hyper)parameters since
only the person parameters are integrated out in the joint posterior and the
marginalized posterior still depends on the hyperparameters. Subsequently,
the person parameters can be estimated by plugging in the marginal posterior
mode estimates of the item parameters in the relevant conditional posterior
distribution (Bock and Aitkin, 1981; Mislevy, 1986). In this approach, the
uncertainty in the item and population parameter estimates is ignored.
For large samples, the estimated item parameters are approximately nor-
mally distributed with the inverse of the Fisher information matrix as the
covariance matrix. Since the item parameters are usually unknown, the Fisher
information matrix is diﬃcult to compute and the posterior variances are ap-
proximated by computing the observed information matrix, deﬁned as the
negative second derivative of the log-likelihood function evaluated at the esti-
mated parameter values (e.g., Mislevy, 1986; Tsutakawa, 1984). A more gen-
eral overview of IRT estimation procedures, including speciﬁc EM implemen-
tations, can be found for example in Baker and Kim (2004) and van der Linden
and Hambleton (1997).
According to Chapter 2, the marginal posterior of interest is proportional
to
p
 ξ | y

∝
Z Z Z
p (y | θ, ξ) p (θ | θP ) p (ξ | ξP )
p (ξP ) p (θP ) dθ dθP dξP .
(4.2)
In a hierarchical Bayes approach, inferences about the item parameters are to
be made from their marginal posterior but the computation of the marginal
posterior requires high-dimensional numerical integration. Typically, the in-
tegrand in Equation (4.2) resembles the (unnormalized) joint posterior that
contains all information about the unknown parameters. Subsequently, the

70
4 Estimation of Bayesian Item Response Models
marginal posterior of a single parameter is obtained by integrating over the
distribution of the other parameters. Posterior summaries of the marginal
density such as the posterior mean and the posterior standard deviation also
require evaluating integrals.
In an empirical Bayes approach, the parameters in the highest level of the
hierarchy are estimated using the data (Morris, 1983). That is, the compu-
tation of marginal population parameter estimates requires integration over
person and item parameter distribution. It seems appealing to marginalize
over person and item parameters to obtain the marginal posterior mode esti-
mates of the hyperparameters, say
 bθP , bξP

. An empirical Bayes estimate of
the item parameters can be obtained by computing marginal posterior esti-
mates of the population parameters and the modal item parameter estimates
in the marginalized conditional density given the population parameter esti-
mates, p
 ξ | y, bθP , bξP

.
The empirical Bayes estimates may not diﬀer much from the hierarchi-
cal Bayes estimates when the number of respondents is large, leading to
nearly symmetric posterior densities and stable parameter estimates. Empir-
ical Bayes procedures are usually developed to approximate the hierarchical
Bayes estimates that are obtained via high-dimensional integration. However,
Berger (1985) noted that an empirical Bayes procedure can diﬀer substantially
from a hierarchical Bayes procedure since the hyperparameter estimation error
is ignored. The empirical Bayes procedure fails to indicate how to incorporate
the hyperparameter estimation error. The errors are automatically incorpo-
rated in the hierarchical Bayes analysis.
To perform a hierarchical Bayes approach, (potentially) high-dimensional
integrals need to be evaluated. Therefore, computational methods are needed
that support the computation of complicated marginal posterior densities and
posterior summaries. In Section 4.2, it will be shown that MCMC methods en-
able the computation of high-dimensional integrals and facilitate the superior
hierarchical Bayes modeling approach.
In conclusion, several remarks can be made concerning advantages of hi-
erarchical Bayes estimates over marginal maximum likelihood estimates.
•
In the situation where information is available from previous administra-
tions or reasonable prior distributions can be constructed given the current
data, Bayesian estimates will be superior to marginal maximum likelihood
estimates. The Bayesian approach enables the use of additional informa-
tion for estimating the parameters.
•
The Bayesian procedure uses prior and sample information to estimate a
parameter, and the estimate will have a smaller standard error than the
standard error corresponding to a marginal maximum likelihood estimate.
This is only an advantage when reasonable prior information is available.
•
A Bayes estimate is based on a combination of prior and sample informa-
tion that results in an estimate that relies on the response pattern but also
on individual characteristics and/or group-speciﬁc information. For exam-

4.2 MCMC Estimation
71
ple, a respondent’s ability estimate might shrink towards a group mean in
an upward (downward) direction when the respondent is a member of a
high (low) ability group. Mislevy (1986) extended an item response model
with prior population models such that observations provide information
about the person parameters but also contribute information about the
population to which they belong. Knowledge of the populations is used to
improve the estimation of the person parameters.
•
The use of prior information has the advantage that the parameter es-
timates (item and person) are restricted to a plausible range of values
and avoid estimates that drift out of range. Person and item parameter
estimates can be obtained for perfect and imperfect response patterns.
•
A Bayesian estimation procedure is more appropriate for moderate and
smaller sample sizes since it does not rely on large-sample asymptotic
results like the marginal maximum likelihood procedure. For very large
sample sizes, Bayesian and marginal maximum likelihood estimates tend
to be similar, the posterior becomes dominated by the likelihood, and the
inﬂuence of the prior becomes negligible.
•
Most statistical inferences follow easily from the estimated posterior dis-
tribution of the structural parameters. For instance, one can obtain pa-
rameter estimates and related credible sets. This is in contrast with the
marginal maximum likelihood procedure, where obtaining estimates and
related conﬁdence intervals are two diﬀerent (computational) problems.
4.2 MCMC Estimation
In the 1990s, several MCMC implementations were developed for logistic
and normal ogive item response models. The developed simulation-based al-
gorithms can be grouped using two MCMC sampling methods: Metropolis-
Hastings (M-H) and Gibbs sampling. The Gibbs sampling method was used
by Albert (1992) for the two-parameter normal ogive model (see also Albert
and Chib, 1993). The Gibbs sampling implementation of Albert (1992) is char-
acterized by the fact that all full conditionals can be obtained in closed form
and it is possible to directly sample from them given augmented data. This
approach has been extended in several directions, and several expansions will
be considered in subsequent chapters.
Combining Gibbs sampling with M-H sampling leads to an M-H within
Gibbs algorithm that turns out to be a powerful technique for obtaining sam-
ples from the target distribution (see Chib and Greenberg, 1995, for a general
description). Patz and Junker (1999a, 1999b) proposed an M-H within Gibbs
sampler for several logistic response models. Their M-H within Gibbs scheme
is extended by allowing within-item dependencies and treating the hyperpa-
rameters as unknown model parameters.
In each M-H step, the so-called candidate-generating density equals a nor-
mal proposal density with its mean at the current state. Then, the acceptance

72
4 Estimation of Bayesian Item Response Models
ratio is deﬁned by a posterior probability ratio since the proposal density is
symmetric. For the two-parameter logistic response model (Equation (1.3)) a
hierarchical prior for the item parameters is deﬁned as
˜ξk = (log ak, bk)t ∼N (µξ, Σξ) ,
(4.3)
with the second-stage prior for the hyperparameters according to Equations
(2.4) and (2.5). An exchangeable hierarchical prior is deﬁned for θi accord-
ing to Equations (2.8)–(2.10). This leads to the following M-H within Gibbs
scheme.
MCMC Scheme 1 (Two-Parameter Logistic Model)
1. Sample θ∗
i ∼N(θ(m)
i
, ϕθ) and Ui ∼U[0,1], and set θ(m+1)
i
= θ∗
i when
ui ≤
p
 yk | θ∗
i , ξ(m)
p

θ∗
i | µ(m)
θ
, σ2(m)
θ

p

yk | θ(m)
i
, ξ(m)

p

θ(m)
i
| µ(m)
θ
, σ2(m)
θ

and otherwise set θ(m+1)
i
= θ(m)
i
for i = 1, . . . , n.
2. Sample ˜ξ
∗
k ∼N

˜ξ
(m)
k
, ϕξ

and Uk ∼U[0,1], and set ξ(m+1)
k
= ξ∗
k when
uk ≤
p

yi | ξ∗
k, θ(m+1)
p

˜ξ
∗
k | µ(m)
ξ
, Σ(m)
ξ

p

yi | ξ(m)
k
, θ(m+1)

p

˜ξ
(m)
k
| µ(m)
ξ
, Σ(m)
ξ

and otherwise set ξ(m+1)
k
= ξ(m)
k
for k = 1, . . . , K.
3. Sample hyperparameter values (µθ, σ2
θ, µξ, Σξ) from their full condition-
als
a) Sample µ(m+1)
θ
, σ2(m+1)
θ
from the conditional densities
µθ | σ2(m)
θ
, θ(m+1) ∼N

n0
n + n0
µ0 +
n
n + n0
θ,
σ2
θ
n + n0

,
σ2
θ | θ(m+1) ∼IG

g1 + n
2 , σ2
n

,
where σ2
n = g2 + (n −1)s2/2 +
nn0
2(n+n0)(θ −µ0)2 (see Exercise 4.2 for
details).
b) Sample µ(m+1)
ξ
, Σ(m+1)
ξ
from the conditional densities
µξ | Σ(m)
ξ
, ˜ξ
(m+1) ∼N

K0
K0 + K µ0 +
K
K0 + K ξ,
Σξ
K + K0

,
Σξ | ˜ξ
(m+1) ∼IW (K + ν, Σ∗) ,
where Σ∗= Σ0 + KS +
KK0
K+K0
 ξ −µ0
  ξ −µ0
t (see Exercise 4.3
for details).

4.3 Exploiting Data Augmentation Techniques
73
The advantage of an M-H algorithm is the fact that an arbitrary condi-
tional proposal density can lead to simulating values from the posterior target
density. But at the same time, the drawback of an M-H algorithm is that the
choice of the proposal density greatly aﬀects the rate of convergence. If the
proposal leads to a high acceptance rate, the Markov chain takes small steps,
resulting in slow convergence. If it leads to a low acceptance rate, the Markov
chain converges slowly because most of the proposed candidates are rejected.
Establishing good convergence in a more complex M-H scheme, including sev-
eral proposals, can be quite diﬃcult. In some cases, an acceptable convergence
rate can be obtained by adjusting the variance of the proposal density (Gel-
man, Meng and Stern, 1996).
MCMC scheme 1 for estimating the parameters of the two-parameter lo-
gistic model is easily extended to the three-parameter logistic model. Since
the posterior density of the guessing parameter is less peaked, it is diﬃcult to
sample values from its conditional density, via a proposal density, around the
posterior mean (Patz and Junker, 1999b). That is, a lot of the sampled val-
ues are located in the tails of the posterior, resulting in an unstable estimate
of the guessing parameter. Moreover, the convergence of the M-H algorithm
becomes more complicated since the variance of the proposal density of the
guessing parameter also has to be identiﬁed.
4.3 Exploiting Data Augmentation Techniques
A Gibbs sampling implementation has two advantages over an M-H imple-
mentation. First, a Gibbs sampler avoids the speciﬁcation of a proposal dis-
tribution and possibly related convergence problems; that is, an M-H method
often needs tuning to obtain a reliable and eﬃcient algorithm. Second, a Gibbs
sampler, in contrast to an M-H algorithm, is more easily implemented since
it consists of sampling from well-known distributions. The use of a Gibbs
sampler implies limitations on the choice of the posterior distribution since it
requires direct sampling from it. This can be averted by data augmentation
with a completion construction also known as the auxiliary variable method.
In the method of auxiliary variables, realizations from a complicated dis-
tribution can be obtained by augmenting the variables of interest by one or
more additional variables such that the full conditionals are tractable and easy
to simulate from. The construction of sampling algorithms via the introduc-
tion of (unobserved) augmented data received much attention since it resulted
in both simple and fast algorithms (e.g., Higdon, 1998; Meng and van Dyk,
1999; Neal, 1997). Tanner and Wong (1987) introduced augmented variables
to improve the speed of convergence and showed that their data augmentation
scheme made a sampling procedure feasible and simple. It will be shown that
these developments also proved to be useful for constructing eﬃcient MCMC
schemes for item response models.

74
4 Estimation of Bayesian Item Response Models
The data augmentation algorithm is based on the introduction of so-called
augmented data, denoted as Z, that are linked to the observed data via a
many-to-one mapping such that f(Z) = Y , so inﬁnitely many values of Z will
give the same value of Y . Equivalent inferences about a model parameter θ
are to be made when using a model for the observed data, p(y | θ), or when
using a model for the augmented data, p(z | θ), since
p (y | θ) =
Z
f(z)=y
p (z | θ) dz.
(4.4)
A proper augmentation scheme obeys the restriction that the distribution of
the observed data is implied by the distribution of the augmented data. The
augmented data are introduced for computational purposes and should not
alter the analysis model. Appropriate augmented data are useful mainly when
sampling from both p (z | y, θ) and p (θ | z) becomes easier or feasible and sam-
pling directly from p (θ | y) is diﬃcult. It is possible that via data augmenta-
tion the Markov chains mix more quickly and thus reduce the computation
time (van Dyk and Meng, 2001). Further on, the many-to-one mapping that
speciﬁes the range of integration will often be omitted from the expression, as
it will be speciﬁed implicitly by the data augmentation scheme.
An augmented variable will also be referred to as a latent or auxiliary
variable. Latent responses are also referred to as augmented data when the
observed data are seen as indicators of underlying continuous responses.
4.3.1 Latent Variables and Latent Responses
The auxiliary or latent variable approach has several important advantages.
First, the approach is very ﬂexible and can handle almost all sorts of discrete
responses. Typically, the likelihood of the observed response data has a com-
plex structure but the likelihood of the augmented (latent) data has a known
distribution with convenient mathematical properties. Second, conjugate pri-
ors, where the posterior has the same algebraic form as the prior, can be more
easily deﬁned for the likelihood of the latent response data, which has a known
distributional form, than for the likelihood of the observed data. Third, the
augmented variable approach facilitates easy formulation of a Gibbs sampling
algorithm based on data augmentation. It will turn out that by augmenting
with a latent continuous variable, conditional distributions can be deﬁned
based on augmented data, from which samples are easily drawn. Fourth, the
conditional posterior given augmented data has a known distributional form
such that conditional probability statements can be directly evaluated for
making posterior inferences. The likelihood of the augmented response data
is much more easily evaluated than the likelihood of the observed data and
can be used to compare models. Fifth, item response models are not identiﬁed
such that a diﬀerent set of parameter values leads to the same distribution of
the response data. This identiﬁcation problem is often better understood in

4.3 Exploiting Data Augmentation Techniques
75
a latent variable formulation. Then, the introduced latent response variable
becomes a dependent variable and its scale needs to be ﬁxed to identify the
model. This viewpoint provides a direct interpretation of the identiﬁcation
problem. A proper (conjugate) prior can be deﬁned that identiﬁes the poste-
rior although the likelihood is not identiﬁed. It will be shown that the scale
of the latent responses can also be ﬁxed in each MCMC iteration without
imposing exact restrictions on the model parameters.
The likelihood p(y | θ, ξ) speciﬁes the distribution of the observed re-
sponses given levels of item and person parameters. Below, it will be shown
that the likelihood of the augmented response data will be consistent with
the conditional likelihood for the observed response data. Subsequently, var-
ious latent variable formulations will be given to handle probit and logistic
response models for binary and polytomous response data.
4.3.2 Binary Data Augmentation
Torgerson (1958) showed that the normal ogive item characteristic curve that
relates the level of ability to the probability of a correct response (see Figure
1.2) can also be presented by the level of ability and a normal deviate Zik
that corresponds to the probability of a correct response. In Figure 4.1, levels
of ability, θi, are plotted against deviate values Zik = akθi −bk (right vertical
axis). When the deviate values are considered to be normal deviate values, the
levels of ability plotted against the corresponding success probabilities (left
vertical axis) represent the normal ogive ICC.
The two-parameter logistic model can be written as P (Yik = 1 | θi, ξk)
= Ψ(akθi −bk), where Ψ(.) is the standard logistic cumulative distribution
function with mean zero and variance π2/3. This model can be written in
terms of the log odd or the logit of the probabilities:
log

P (Yik = 1 | θi, ξk)
1 −P (Yik = 1 | θi, ξk)

= akθi −bk.
It follows that the logistic model deﬁnes a linear relationship between the
logits and the parameters; that is, the model is linear in terms of the logits.
When the deviate values in Figure 4.1 are considered to be logistic deviate
or logit values, the levels of ability plotted against the corresponding success
probabilities represent the logistic ICC. In Figure 4.1, a logistic scale factor
of d = 1.7 is used such that a close agreement is obtained between the logistic
and normal ogive ICCs (see Exercise 4.6).
Several parameterizations are discussed in Torgerson (1958) to describe
the straight-line ICC of Figure 4.1. One of the parameterizations stems from
Tucker (1952), who described the straight line in Figure 4.1 as Zik = akθi−bk,
where ak and bk correspond to the slope and the intercept, respectively, and
they are sometimes referred to as Tucker’s item parameters (e.g., Torgerson,
1958). This parameterization is also pursued here.

76
4 Estimation of Bayesian Item Response Models
Logistic ICC
Normal Ogive ICC
-levels
Fig. 4.1. Normal ogive and logistic ICCs with respect to the deviate values and the
corresponding success probabilities.
Let the latent variable Zik determine the performance of respondent i on
item k. The respondent answers the item correctly if Zik > 0 (Yik = 1) and
incorrectly if Zik ≤0 (Yik = 0). Then, the response probability that person i
answers item k correctly is deﬁned by
P (Yik = 1 | θi, ξk) = P(Zik > 0 | θi, ξk)
=
Z ∞
0
φ(z; akθi −bk) dz = Φ(akθi −bk)
(4.5)
=
Z ∞
0
ψ(z; d(akθi −bk)) dz = Ψ(d(akθi −bk)), (4.6)
where φ(·) and ψ(·), as deﬁned in Chapter 1, denote the normal and logistic
density functions, respectively, and d the logistic scale factor.
Torgerson (1958, pp. 386–388) and Lord and Novick (1968, pp. 358–394)
deﬁned three conditions leading to a two-parameter item response model. The
ﬁrst two conditions state that the regression of this latent variable Zik on the
latent ability parameter θi is linear and with the same error variance for all
θi. The third condition states that a normal ogive model is deﬁned when
the conditional distribution of Zik given θi is normal (Equation (4.5)) and a
logistic model is deﬁned when the conditional distribution is logistic (Equation
(4.6)). It can be concluded that for the normal ogive model the deviates
are normally distributed, corresponding to a linear relationship between the
ability levels and the normal deviates, and for the logistic model the deviates

4.3 Exploiting Data Augmentation Techniques
77
are logistically distributed, corresponding to a linear relationship between the
ability levels and the logistic deviates or logits.
Albert (1992) (see also Albert and Chib, 1993) constructed an MCMC al-
gorithm using the latent variable Zik (normal deviate) as an auxiliary variable
for estimating the two-parameter normal ogive model. A data augmentation
scheme for diﬀerent logistic item response models was developed by Maris and
Maris (2002) that used the logistically distributed latent variable Zik. The
augmented data are deﬁned in such a way that each full conditional becomes
an indicator function with bounds speciﬁed by the other parameter values.
As a result, the sampling of the parameters is easy; however, the sampled val-
ues are highly correlated due to this incorporated dependency structure. The
samples cannot be drawn freely from the target distribution but are restricted
to a subspace speciﬁed by the other parameter values.
Here, an augmented data scheme will be deﬁned that can handle unidi-
mensional logistic as well as normal ogive response models and will lead to a
simple and fast M-H within Gibbs algorithm. Let L(0, 1) and N(0, 1) denote
the standard logistic and the standard normal density functions, respectively.1
Two diﬀerent approaches can be followed for augmenting data. The ﬁrst ap-
proach is based on deﬁning a deviate as Zik = akθi −bk. Now, augmented
data are conditionally distributed as
Zik | Yik, θi, ξk ∼
 L(d(akθi −bk), 1)
N(akθi −bk, 1),
(4.7)
where Yik is the indicator that Zik is positive and d = 1.7 (see Exercise 4.6
for more details about the logistic scale factor d). This approach for the nor-
mal deviate corresponds with the procedure of Albert (1992). In the second
approach, augmented data are standard normally or standard logistically dis-
tributed but truncated in such a way that the distribution of the augmented
data implies the distribution of the observed data. In this case, the augmented
data are conditionally distributed as
˜Zik | Yik, θi, ξk ∼
 L(0, 1)
N(0, 1),
(4.8)
where Yik is the indicator that assumes a value one if ˜Zik > d(bk −akθi)
and zero otherwise (d = 1.7 and d = 1 for the two-parameter logistic and
normal ogive response models, respectively). The auxiliary variables Zik and
˜Zik as deﬁned in Equations (4.7) and (4.8) are related to each other; that is,
Zik = ˜Zik + d(akθi −bk) such that Zik is normally (logistically) distributed
with mean d(akθi−bk) and variance one (π2/3) when ˜Zik is standard normally
(logistically) distributed.
Given the deﬁned augmentation schemes, say Zik is normally distributed
(Equation (4.7)) and ˜Zik logistically distributed (Equation (4.8)), it follows
1 It follows that Z ∼L(0, 1) corresponds with p(z) = ψ(z; 0, 1) and Z ∼N(0, 1)
corresponds with p(z) = φ(z; 0, 1).

78
4 Estimation of Bayesian Item Response Models
that the distribution of the observed data is implied by the distribution of the
augmented data,
P (Yik = 1 | θi, ξ) = P

˜Zik > d(bk −akθi) | θi, ξk

=
Z ∞
d(bk−akθi)
ψ (x) dx = Ψ(d(akθi −bk))
(4.9)
= P (Zik > 0 | θi, ξk)
=
Z ∞
0
φ (x; akθi −bk) dx = Φ (akθi −bk) .
(4.10)
It can be seen from Equation (4.9) that the deﬁnition of the conditional logis-
tically distributed auxiliary variable ˜Zik leads to the probability of a correct
response under the two-parameter logistic model. From Equation (4.10) it
follows that the deﬁnition of the conditional normally distributed auxiliary
variable Zik leads to the probability of a correct response under the two-
parameter normal ogive model. As a result, the choice of the measurement
model is deﬁned in this data augmentation step. The augmentation step de-
ﬁnes a probit or logit analysis.
The parameters of the complete-data likelihood can be separated from
the distribution of the latent response data. As an example, the conditional
posterior density of the augmented data, ˜Z, can be written as
p (˜z | y, θ, ξ) ∝
Y
i,k
h
I(0,∞)(˜zik + d(akθi −bk))yik ·
I(−∞,0)(˜zik + d(akθi −bk))1−yik
i
p(˜zik),
(4.11)
where p(˜zik) is the standard logistic or normal density function. Now, the con-
ditional posterior density of the ability parameter can be constructed from the
complete-data likelihood and its prior (see also Exercise 4.4(e)). The restric-
tion on the ˜zik can be expressed as a restriction on the θi. Therefore, deﬁne
the set Aik = {θi ∈R; θi > (dbk −˜zik)/dak} with the complement set Ac
ik. It
follows that the full conditional posterior density of θi can be expressed as
p (θi | y, ˜z, ξ, θP ) ∝
Y
k
h
IAik(θi)I(Yik = 1) + IAc
ik(θi)
I(Yik = 0)
i
p(θi | µθ, σ2
θ)
∝
Y
k|yik=1
IAik(θi)
Y
k|yik=0
IAc
ik(θi)p(θi | µθ, σ2
θ)
∝I∪k|yik=1Aik(θi)I∩k|yik=0Ac
ik(θi)p(θi | µθ, σ2
θ). (4.12)
The set indicator function of the union (intersection) of sets is the maximum
(minimum) function of the indicator functions. Therefore, deﬁne

4.3 Exploiting Data Augmentation Techniques
79
∆l = max
k|yik=1(dbk −˜zik)/dak,
(4.13)
∆u =
min
k|yik=0(dbk −˜zik)/dak,
(4.14)
and it follows that Equation (4.12) can be written as
p
 θi | y, ˜z, ξ, µθ, σ2
θ

∝I(∆l,∆u)(θi)p(θi | µθ, σ2
θ).
(4.15)
The conditional posterior can be recognized as a truncated prior that is condi-
tionally independent of the distribution of the augmented data. The following
MCMC scheme can be deﬁned for estimating the parameters of logistic and
normal ogive models.
MCMC Scheme 2 (Two-Parameter Model)
1. Sample augmented data ˜z(m+1) according to Equation (4.8) for a logistic
or normal ogive model.
2. Let the prior for θi be deﬁned by Equation (2.8). According to (4.15),
sample θ(m+1)
i
from
θi | y, ˜z(m+1), ξ(m), µ(m)
θ
, σ2(m)
θ
∼N
 µθ, σ2
θ

IRθ(θi),
(4.16)
where Rθ = {θ ∈R, ∆l < θ < ∆u} with (∆l, ∆u) deﬁned in Equations
(4.13) and (4.14), respectively.
3. Let the prior for ξk be deﬁned by Equation (2.3) with known values for
µξ and Σξ. Let x =
 dθ(m+1), −d1n

, and sample ξ(m+1)
k
from the con-
ditional density
ξk | z(m+1)
k
, θ(m+1), µ(m)
ξ
, Σ(m)
ξ
∼N
 µ∗
ξ, Ωξ

IAk(ak),
(4.17)
where
Ω−1
ξ
= ϕ−1  xtx

+ Σ−1
ξ ,
(4.18)
µ∗
ξ = Ωξ

xtzk + µξΣ−1
ξ

,
(4.19)
with Zik = ˜Zik + d (akθi −bk) and ϕ = 1 or ϕ = π2/3 if ˜Zk is normally
or logistically distributed, respectively. The density in Equation (4.17)
is used to generate candidates evaluated in an M-H step when ˜Zk is
logistically distributed.
4. Sample values of prior parameters µ(m+1)
θ
, σ2(m+1)
θ
according to step 3
in MCMC scheme 1.
In step 3 of scheme 2, the standard logistic density function is approxi-
mated by a normal density function with variance π2/3 and an M-H step is
used to correct any deﬁciencies in the approximation since the tail of the lo-
gistic density function is somewhat longer. Candidate values drawn from the

80
4 Estimation of Bayesian Item Response Models
density in Equation (4.17) are almost always accepted since both densities
(normal and logistic) are closely comparable (see, for example, Figure 4.1).
A sampling step for the hyperparameters (µξ, Σξ) is complicated due to
the positivity restriction on the discrimination parameters (see Exercise 4.4).
The lognormal prior for the item parameters in Equation (4.3) is a nonconju-
gate prior to the augmented data likelihood. Then, an M-H step is required
to draw samples from their full conditional, but the hyperparameters can be
sampled according to step 3 of MCMC scheme 1.
MCMC scheme 2 can be extended to sample the parameters of the three-
parameter model. This is done by introducing another auxiliary variable, de-
noted as Sik, which equals one when person i knows the correct answer to
item k and equals zero otherwise (B´eguin and Glas, 2001). Note that, ac-
cording to Equation (1.6) or (1.7), a correct response can be obtained when
person i knows and gives a correct response but also when person i does not
know the correct response but gives it by guessing correctly. In this light, the
conditional probability that respondent i knows the correct answer to item k
given a correct response equals
P (Sik = 1 | Yik = 1, θi, ξk) =
Φ (akθi −bk)
Φ (akθi −bk) + ck (1 −Φ (akθi −bk)). (4.20)
In the same way, the conditional probability that respondent i does not know
the correct answer to item k given a correct response equals
P (Sik = 0 | Yik = 1, θi, ξk) =
ck (1 −Φ (akθi −bk))
Φ (akθi −bk) + ck (1 −Φ (akθi −bk)). (4.21)
The value of Sik equals zero with probability one when an incorrect response
is given by respondent i to item k. Note that the normal ogive response prob-
abilities can be replaced by logistic response probabilities in Equations (4.20)
and (4.21). The following conditional sampling scheme can be adopted. Set
Sik equal to zero when Yik = 0. When Yik = 1, sample Sik from a Bernoulli
distribution with a success probability deﬁned by Equation (4.20). Per item
k, the number of correct responses via guessing is binomially distributed with
parameters ns = P
i|Sik=0 I (Yik = 1) and success probability ck. This like-
lihood combined with the conjugate beta prior in Equation (2.7) leads to a
beta posterior density,
ck | s, y ∼Be (α + ns, β + ˜sk −ns) ,
(4.22)
where ˜sk denotes the number of persons who do not know the correct answer
to item k and guess the response. A comparable implementation is given by
Sahu (2002), who showed a better performance of the Gibbs sampler over
the M-H algorithm in terms of the eﬀective sample size. The sampling of
augmented data S and parameters c is easily integrated in MCMC scheme 2,
but note that augmented data, item, and person parameters are to be sampled
conditionally on the values of S.

4.3 Exploiting Data Augmentation Techniques
81
4.3.3 TIMMS 2007: Dutch Sixth-Graders’ Math Achievement
The Netherlands was one of the participating countries in TIMSS 2007 (Trends
in International Mathematics and Science Study), a large-scale international
assessment of math and science achievement conducted on a 4-year cycle that
started in 1995. The TIMMS 2007 data can be found at http://timss.bc.
edu/timss2007/idb_ug.html (January 2010).
The math achievement of Dutch pupils in group 6 (ﬁrst class of the upper
level in primary education, equivalent to the fourth school year after kinder-
garten) was assessed. For illustration purposes, eight math items of the content
domain number stored in booklets 1 and 14 were considered.
A two-parameter response model was used for analyzing the item response
data. A standard normal prior was deﬁned for the ability parameters for iden-
tiﬁcation purposes (see Section 4.4.2). Independence between the discrimina-
tion and diﬃculty parameters and a common normal distribution for both
sets of parameters was assumed such that
ak ∼N
 µa, σ2
a

,
(4.23)
bk ∼N
 µb, σ2
b

.
(4.24)
The hyperparameters µa and µb were normally distributed with means one
and zero, respectively, and a large variance. The hyperparameters σ2
a and σ2
b
both had an inverse gamma prior with a small value for the shape and scale
parameters.
Normally distributed augmented data were sampled to ﬁt a normal ogive
response model. A popular MCMC scheme was used that corresponds closely
to Albert’s (1992) scheme:
1. Sample augmented data according to Equation (4.7).
2. Sample ability parameters as speciﬁed in Exercise 4.5(a).
3. Sample the item parameters (see Exercise 4.5(d)).
4. Sample the hyperparameters (see Exercise 4.5(e)).
Starting values were generated from the priors with µa = 1 and µb = 0.
The MCMC algorithm was run for 10,000 iterations, and the ﬁrst 2,000 iter-
ations were considered as a burn-in. Imputed data were used for the missing
observations. The imputation procedure is described in Exercise 4.10(e).
In Table 4.1, the item parameter estimates and the hyperparameter es-
timates are given. It can be seen that the items discriminate quite well and
there is not much variation in discrimination values. The variation in diﬃculty
values is much higher (around 1.1). Items one, three, and four appear to be
much more diﬃcult than items ﬁve, seven, and eight. This follows from the
fact that items one, three, and four were answered correctly by only 15% to
20% of the pupils and items ﬁve, seven, and eight were answered correctly by
70% to 90% of the pupils.

82
4 Estimation of Bayesian Item Response Models
Table 4.1. TIMMS 2007: Item parameter estimates of eight math items of the
content domain number.
Discrimination
Diﬃculty
Item Mean
SD
Mean
SD
1
.864
.132
1.072
.114
2
.616
.101
.499
.075
3
.920
.141
1.029
.117
4
.875
.143
1.377
.140
5
.544
.098
−.615
.074
6
.760
.114
−.003
.074
7
.924
.142
−.942
.108
8
.759
.128
−1.433
.130
µa
.786
.102
µb
.119
.378
σ2
a
.063
.046
σ2
b
1.113
.732
The MCMC output was used to compute the posterior probability of an-
swering the diﬃcult item 4 incorrectly and the easy item 8 correctly. This
follows from
P (Yi4 = 0, Yi8 = 1 | y) =
Z
P (Yi4 = 0 | θi, y) P (Yi8 = 1 | θi, y) p (θi | y) dθi
≈M −1 X
m

1 −Φ

a(m)
4
θ(m)
i
−b(m)
4

Φ

a(m)
8
θ(m)
i
−b(m)
8

for M MCMC samples
 a(m), θ(m), b(m)
from the joint posterior density.
High- and low-ability pupils have a low posterior probability of up to .40.
As expected, average-ability pupils have a high posterior probability of up to
.80. Answering item 4 correctly and item 8 incorrectly is very unlikely for all
pupils. This is reﬂected by the computed posterior probabilities, which vary
from .001 to .008 for pupils with diﬀerent levels of ability.
It was investigated whether the boys performed better on the eight items
than the girls. Therefore, a ﬁxed factor was deﬁned with values equal to one
for the boys and equal to zero for the girls. This ﬁxed factor was used to deﬁne
a mean diﬀerence in ability between boys and girls. That is, the prior for the
ability parameters had a ﬁxed mean of zero for the girls and an unknown
mean, µθ, for the boys.2 The estimated posterior population mean of the
boys’ abilities equalled .282, with a posterior standard deviation of .115, and
it was concluded that the boys signiﬁcantly outperformed the girls.
2 The boys and girls respond to a common set of (anchor) items, and the scale
of the girls (reference) group is identiﬁed, which suﬃces for identiﬁcation (see
Section 7.3).

4.3 Exploiting Data Augmentation Techniques
83
4.3.4 Ordinal Data Augmentation
Albert and Chib (1993) deﬁned a Gibbs sampler for the univariate ordinal
probit response model. They made the transition to polytomous scored items
by deﬁning the polytomous response Yik as an indicator of Zik falling into one
of the response categories deﬁned by threshold parameters κ. In this case, an
auxiliary variable Zik is deﬁned as
Zik | Yik = c, θi, κk, ak ∼N (akθi, 1) I (κk,c−1 < Zik ≤κk,c) .
(4.25)
The ordering of the response categories is displayed as
κk,0 < κk,1 ≤κk,2 ≤. . . < κk,Ck,
(4.26)
where there are Ck categories, the number of categories per item may diﬀer,
and κk,0 = −∞and κk,Ck = ∞. The probability that an individual with
ability θi obtains a grade c ≥1 or gives a response falling into category c on
item k is deﬁned by (see also Equation (1.8))
P (Yik = c | θi, ak, κk) = Φ (akθi −κk,c−1) −Φ (akθi −κk,c)
= Φ (κk,c −akθi) −Φ (κk,c−1 −akθi)
= P (κk,c−1 < Zik ≤κk,c | θi, ak, κk)
=
Z κk,c
κk,c−1
φ(z; akθi)dz.
(4.27)
This shows that (4.25) qualiﬁes as a data augmentation scheme.
Simulating ability and discrimination parameter values is done in a simi-
lar way as for the two-parameter normal ogive model. There has been some
discussion in the literature about the best way to sample the threshold pa-
rameter values (e.g., Chen et al., 2000; Cowles, 1996; Fox, 2005b; Lee and
Zhu, 2000; Shi and Lee, 1998; Song and Lee, 2001). One way is to to derive
the full conditional distribution using a conjugate prior that takes the order
constraint in (4.26) into account. Deﬁne a uniformly distributed variable Uik
over [0, 1] such that
Uik ≤P (Zik ≤κk,c | y, κk, θi, ak) I (i ∈A1) ,
(4.28)
Uik > P (Zik > κk,c | y, κk, θi, ak) I (i ∈A2) ,
(4.29)
for the set A1 = {i : Yik = c} and A2 = {i : Yik = c+1}. Accordingly, the full
conditional distribution of κk,c is uniform using a (diﬀuse) prior with equal
probability for each possible parameter value,
κk,c | z, θ, κk(−c), ak ∼U (∆l, ∆h) ,
(4.30)
where ∆l = max
 maxi|Yik=c zik, κk,c−1

, ∆h = min
 mini|Yik=c+1 zik, κk,c+1

,
and κk(−c) is the set of threshold parameters for item k without κk,c.

84
4 Estimation of Bayesian Item Response Models
This implementation (Albert and Chib, 1993) is straightforward since it
consists of sampling from normal and uniform densities, but the convergence
is very slow when the number of respondents is larger than 50 (see Chen
et al., 2000, p. 37; Cowles, 1996). The interval in Equation (4.30) becomes
narrow when increasing the number of respondents. As a result, the sampled
threshold values diﬀer slightly across iterations also due to the fact that the
iterates are highly correlated. A slow convergence of the threshold parameters
may also lead to a slow convergence of the other model parameters.
The convergence is accelerated by sampling threshold parameters via an
M-H step (Cowles, 1996). This requires constructing a suitable proposal den-
sity. A new candidate is generated from a truncated normal density,
κ∗
k,c ∼N

κ(m)
k,c , σ2
mh

I

κ∗
k,c−1 < κ∗
k,c < κ(m)
k,c+1

,
(4.31)
where κ(m)
k,c is the value of κk,c in the mth iteration of the sampler.
Chen et al. (2000) argued that the proposal density often is not spread out
enough and, as a result, it will not generate candidate threshold values from
the entire parameter space with nonzero posterior probability. Furthermore,
the variance parameter of the proposal density is diﬃcult to specify. The vari-
ance of the proposal density, σ2
mh, must be speciﬁed appropriately to establish
an eﬃcient algorithm. Nandram and Chen (1996) deﬁned a reparameteriza-
tion that avoids the use of a truncated proposal for univariate response data.
This approach is also interesting for multivariate ordinal data, in particular
from an item-based test with three response options (see Exercise 4.8).
To avoid specifying any tuning parameters, an adaptive proposal is de-
ﬁned. The variance of this proposal density is adjusted within the sampling
procedure. This ﬁne-tuning of the proposal results in a good and eﬃcient con-
vergence of the algorithm without detailed prior information regarding the
variance of the proposal. Speciﬁcally, say after each 50th iteration the accep-
tance rate regarding the threshold parameters is evaluated. If the acceptance
rate is low, a high percentage of the sampled new candidates were rejected
and the variance σ2
mh is too high. The other way around, if the acceptance
rate is high, a high percentage of the sampled new candidates were accepted
and the variance σ2
mh is too low.
In both situations, the variance is adjusted in the right direction. This
way, a sequence of proposals is generated automatically that includes the
ﬁne-tuning of the proposal’s variance parameter. In an exploratory phase, the
proposal is calibrated to obtain an M-H step with an acceptable convergence
rate. Gelman et al. (1996) recommended an acceptance rate close to 50% for
one- or two-dimensional models.
A general MCMC scheme is presented for the graded response model. For
ease of notation, the adjustment factor d is dropped, where d = 1.7 and d = 1
for the logistic and normal ogive graded response models, respectively.

4.3 Exploiting Data Augmentation Techniques
85
MCMC Scheme 3 (Graded Response Model)
1. Sample augmented data z(m+1) for a logistic or normal ogive graded re-
sponse model,
Zik | Yik, θ(m)
i
, ξ(m)
k
∼
 L(akθi, 1)
N(akθi, 1),
(4.32)
where Yik = c if κk,c−1 < Zik ≤κk,c.
2. Assume a hierarchical prior for θi (Equation (2.8)). Sample θ(m+1)
i
from
θi | z(m+1)
i
, a(m), µ(m)
θ
, σ2(m)
θ
∼N
 Σθ
 atzi + µθ/σ2
θ

, Σθ

,
(4.33)
where Σ−1
θ
= ata/ϕ + σ−2
θ . Candidates are generated and evaluated in
an M-H step when Zi is logistically distributed.
3. Draw candidates κ∗
k from the proposal density in Equation (4.31). Sample
Uk ∼U(0, 1), and set κ(m+1)
k,c
= κ∗
k,c for c = 1, . . . , Ck −1 when
uk ≤min

Y
i
F

akθi −κ∗
k,yik−1

−F

akθi −κ∗
k,yik

F (akθi −κk,yik−1) −F (akθi −κk,yik)
Ck−1
Y
c=1
Φ

κk,c+1−κk,c
σmh

−Φ
 κ∗
k,c−1−κk,c
σmh

Φ
 κ∗
k,c+1−κ∗
k,c
σmh

−Φ
 κk,c−1−κ∗
k,c
σmh
, 1

,
(4.34)
where F denotes the corresponding cumulative logistic or normal distri-
bution function.
4. Assume a positively truncated normal prior for ak,
ak | µa, σ2
a ∼N
 µa, σ2
a

IAk (ak) ,
where Ak = {ak ∈R, ak > 0}. Let x = θ(m+1), and sample a(m+1)
k
from
ak | z(m+1)
k
, θ(m+1), µ(m)
a
, σ2(m)
a
∼N (µ∗
a, Ωa) IAk(ak),
(4.35)
where
Ω−1
a
= ϕ−1  xtx

+ σ−2
a ,
(4.36)
µ∗
a = Ωa
 xtzk + µaσ−2
a

.
(4.37)
Candidates are generated when Zk is logistically distributed and evaluated
in an M-H step.
5. Sample values of hyperparameters
 µθ, σ2
θ

from their full conditionals;
see step 3 in MCMC scheme 1.
In steps 2, 3, and 4, ϕ = 1 or ϕ = π2/3 if Zi is normally distributed or
logistically distributed, respectively. In step 3, the candidate threshold pa-
rameters per item are evaluated and a variance parameter σ2
mh per item is

86
4 Estimation of Bayesian Item Response Models
also calibrated. The ﬁrst term of the acceptance probability in Equation (4.34)
represents the ratio of posterior probabilities of the candidate threshold values
to the threshold values from the last iteration. The second term in Equation
(4.34) accounts for the nonsymmetric proposal density, which might favor
some parameter values over others (see Equation (3.1)). This diﬀerence lies
in the normalization constant of the proposal density according to Equation
(4.31).
The positivity restriction on the discrimination parameters hinders a direct
sampling approach for the hyperparameters µa and σ2
a. Alternative sampling
approaches are discussed in Exercises 3.7 and 4.4 and in Chapter 7.
4.4 Identiﬁcation of Item Response Models
It was shown that the observed response data can be viewed as realizations
of an underlying latent variable. Assume the two-parameter likelihood model
for augmented response data according to Equation (4.7), which is given by
Zik = akθi −bk + ϵik,
(4.38)
where ϵik are independent and standard normally or standard logistically dis-
tributed. This item response model is overparameterized since it has more
parameters than can be estimated from the data. For example, for any con-
stant c, eθi = akθi + c and ebk = bk + c give the same probability of a correct
response since eθi −ebk = akθi −bk. That is, both parameterizations lead to the
same success probability,
P (Yik = 1 | θi, ξk) = P (Zik > 0 | θi, ξk)
= P (akθi −bk + ϵik > 0)
= P

eθi −ebk + ϵik > 0

= P

Zik > 0 | eθi,ebk

= P

Yik = 1 | eθi,ebk

.
In the same way, it can be shown that for any constant c, eθi = θi/c and
eak = akc give the same probability of a correct response.
The metric (location and scale) of the person parameters is only known up
to a linear transformation (Lord and Novick, 1968, p. 366). In the literature,
several types of restrictions are proposed to anchor the metric. The location
can be identiﬁed by ﬁxing the ability level of a speciﬁc person, a so-called stan-
dard person (Rasch, 1960), or by ﬁxing the mean population level of ability
to zero. Items are often applied to diﬀerent sets of individuals, and therefore
it can be more convenient to put a constraint on the diﬃculty parameters.
This is done by selecting one item as the standard item and ﬁxing the item

4.4 Identiﬁcation of Item Response Models
87
diﬃculty parameter to a speciﬁc value, most often zero, or restricting the sum
of item diﬃculty parameters to zero. If necessary, the scale of the metric is
identiﬁed by ﬁxing the population variance of the abilities. The scale can also
be identiﬁed by restricting the discrimination parameter of a so-called stan-
dard item to a speciﬁc value, most often one, or restricting the product of
discrimination parameters to one.
Bayesian item response models usually are not identiﬁed by restricting a
single parameter since this complicates the speciﬁcation of priors and condi-
tional posteriors in MCMC algorithms. Furthermore, if the restricted single
parameter is poorly identiﬁed, the standard errors of the estimated parame-
ters go up. In an informal way, it will be shown that Bayesian item response
models can be identiﬁed (1) by imposing restrictions on the hyperparameters
or (2) via a (standard) scale transformation in the estimation procedure.
4.4.1 Data Augmentation and Identifying Assumptions
The introduction of an underlying latent variable Zik already induces an iden-
tiﬁcation problem. Three identifying restrictions were introduced for the two-
parameter likelihood model for augmented response data (Equation (4.38)).
First, in general, the latent response data can be linked to the observed re-
sponse data in such a way that Zik > κ if Yik = 1 and Zik ≤κ if Yik = 0, where
κ is the cutpoint. In Equation (4.7), this cutpoint or threshold parameter is
restricted to zero. For an unrestricted threshold parameter, the probability of
a correct response is given by
P (Yik = 1 | θi) = P (Zik > κ | θi)
= P (akθi −bk + ϵik > κ | θi)
= P (ϵik > (bk + κ) −akθi | θi) .
It can be seen that a change in the threshold parameter can always be com-
pensated for by a corresponding change in the diﬃculty parameter, and the
model is unidentiﬁed.
Second, the conditional mean of the error term is restricted to zero, E(ϵik |
θi) = 0. Assume that E (ϵik | θi) = µϵ. Then
P (Yik = 1 | θi) = P (ϵik > bk −akθi | θi)
= F (akθi −(bk −µϵ) | θi) ,
where F is a standard cumulative (normal or logistic) distribution function.
In the same way as above, a change in µϵ can be compensated for by a similar
change in bk, and the model is not identiﬁed.
Third, the conditional variance of the residuals is restricted to one,
Var(ϵik | θi) = 1 in the probit model and Var(ϵik | θi) = π2/3 in the logit
model. Assume that the variance is parameterized as ϕ. Then

88
4 Estimation of Bayesian Item Response Models
P (Yik = 1 | θi) = P (ϵik > bk −akθi | θi)
= P (ϵik > ak (b∗
k −θi) | θi)
= F
 ak
√ϕ (θi −b∗
k) | θi

,
and the model is not identiﬁed since a change in the discrimination parameter
can be compensated for by a similar change in √ϕ.
4.4.2 Rescaling and Priors with Identifying Restrictions
The two-parameter likelihood model for the augmented responses is identi-
ﬁed due to the three assumptions mentioned given that the metric of θi is
known. The metric of person parameters is usually unknown, and additional
restrictions are needed to identify the model. One approach is to identify the
metric by ﬁxing the (hyper)parameters of the prior for the person parameters.
A common assumption is to set µθ = 0 and σ2
θ = 1 such that the person pa-
rameters are standard normally distributed. Then, the two-parameter model
for the latent response is given by
Zik = akϵθ −bk + ϵik,
where ϵθ and ϵik are independent standard normally distributed. The random
term ϵθ describes the between-individual heterogeneity and ϵik the within-
individual residual variation.
In a diﬀerent approach, the hyperparameters are freely speciﬁed and the
model is identiﬁed by establishing a metric in each MCMC iteration. That
is, the sampled person parameter values are rescaled to an a priori speci-
ﬁed metric. The metric of sampled values θ(m)
i
is easily changed via a linear
transformation. When ˜θ(m)
i
= σθθ(m)
i
+ µθ, it follows that
E

˜θ(m)
i

= E

σθθ(m)
i
+ µθ

= σθE

θ(m)
i

+ µθ,
Var

˜θ(m)
i

= Var

σθθ(m)
i
+ µθ

= σ2
θ Var

θ(m)
i

,
and when θ(m)
i
is sampled from f
 θ(m)
i
| y

, then ˜θ(m)
i
is sampled from
f
 (˜θ(m)
i
−µθ)/σθ | y

/σθ. It follows that each sampled value from the full
conditional posterior in iteration m, say θ(m)
i
, can be linearly transformed to
have, for example, mean zero and variance one. In subsequent sampling steps
but in the same iteration of the MCMC algorithm, the same rescaled vector
is used. As a result, the identiﬁcation problem is solved since the sampled
person parameters have a ﬁxed metric.
It is also possible to rescale the sample of item parameter values in each
MCMC iteration. The sampled diﬃculty values can be rescaled such that, for
example, their sum equals zero. Deﬁne µb = P
k b(m)
k
/K and transform the

4.5 Performance MCMC Schemes
89
sample at iteration m as ˜b(m)
k
= b(m)
k
−µb. This scale transformation sets the
location. In the same way, the sampled discrimination values can be rescaled
such that, for example, their product equals one. Let σa = Q
k a(m)
k
, and
transform the sample at iteration m as ˜a(m)
k
= a(m)
k
(1/σa)1/K. This restriction
sets the scale of the metric. In the same iteration, the rescaled sample of item
parameter values is used to sample the person parameters.
This identiﬁcation procedure has the advantage that complicated response
models with multistage priors can be identiﬁed via simple transformations of
samples drawn at each MCMC iteration. Furthermore, it is diﬃcult to identify
a complex model by ﬁxing parameters of a multistage prior since the location
and scale are not directly parameterized (e.g., Chapters 6 and 7).
This technique corresponds closely to the identiﬁcation procedure in Bilog-
MG (Zimowski et al., 1996). The EM algorithm implemented uses Gauss-
Hermite quadrature to integrate over the ability distribution. Initially the
quadrature nodes and weights are based on an approximation to the prior
ability distribution. In the ﬁrst iteration, the metric of the estimated item
parameters obtained in the M-step of the algorithm is identiﬁed by the metric
of the prior ability distribution. In the next E-step, the posterior ability distri-
bution is used to recompute new quadrature weights. The recomputed weights
are adjusted to have a mean of zero and a variance of one. Thus, the metric
of the estimated item parameters from the next M-step is identiﬁed by the
rescaled posterior ability distribution, which has a location parameter of zero
and a scale parameter of one. In each EM cycle, the recomputed quadrature
weights based on the posterior ability distribution are rescaled to identify the
model. In a similar way, in each iteration of the MCMC algorithm, the ability
values drawn from the posterior ability distribution are rescaled to identify
the model. Baker and Kim (2004, Chapter 6) give more details about how the
identiﬁcation problem is handled in the EM procedure.
4.5 Performance MCMC Schemes
4.5.1 Item Parameter Recovery
The performance of MCMC scheme 2 was compared with the performance of
the WinBUGS (Lunn et al., 2000) and the Bilog-MG (Zimowski et al., 1996)
programs. The convergence properties of the MCMC output of scheme 2 and
WinBUGS were compared, and the accuracy of the item parameter estimates
from the three programs was investigated. Data were generated under the two-
parameter logistic model for K=10 items and N=1,000 persons. The prior
model consists of the hierarchical prior for the item parameters (Equations
(2.3)–(2.5)) and the hierarchical prior in (2.8) for the ability parameters. The
parameters of the ability prior were ﬁxed to mean zero and variance one
to identify the model. Note that it is not possible in WinBUGS to ﬁx the

90
4 Estimation of Bayesian Item Response Models
mean and variance in each iteration since identiﬁcation restrictions have to
be implemented in the model.
Three separate runs of 20,000 iterations were performed using MCMC
scheme 2 (implemented in Fortran) and WinBUGS to investigate the sensi-
tivity of the starting values. For each run, starting values were generated from
the priors.
In Figure 4.2, the ﬁrst 10,000 MCMC iterates corresponding to the param-
eters of item 4 generated from scheme 2 of three diﬀerent runs are plotted. In
each iteration and each run, a running mean was computed that presents the
mean of sampled values up to that iteration number. The three diﬀerent lines
in each plot present the running means as a function of the MCMC iterates
drawn for the three diﬀerent runs. It can be seen that within 1,000 iterations
the running means from the diﬀerent runs almost coincide. Similar plots were
obtained for the other item parameters.
The ﬁrst-order autocorrelation or serial correlation between the MCMC
iterates was calculated for each MCMC chain, where the iterates from the
burn-in period were ignored (see Equation 3.3). The averaged ﬁrst-order au-
tocorrelation was .827 for the sampled discrimination parameter values and
.643 for the sampled diﬃculty parameter values. It takes less time to ex-
plore the entire posterior density of the diﬃculty parameter since the MCMC
sample corresponding to the diﬃculty parameter exhibited less ﬁrst-order au-
tocorrelation. A high ﬁrst-order autocorrelation indicates that longer runs are
necessary to assure that all areas of the posterior density are reached. Fur-
thermore, if the level of autocorrelation is high, a trace plot will be a poor
diagnostic for convergence.
In Figure 4.3, the MCMC iterates of the parameters of item four are plot-
ted from three diﬀerent runs in WinBUGS. The running means of the three
diﬀerent runs diﬀer greatly, and it takes at least 5,000 iterations for the run-
ning means to be of comparable size. After 5,000 iterations, further samples
just slightly inﬂuence the calculation of the mean. Comparable plots were
obtained using the samples of the other item parameters. It follows that in
WinBUGS the starting values have a much higher impact on the values drawn
in comparison with the values drawn from scheme 2. The averaged ﬁrst-order
autocorrelations of both MCMC samples from WinBUGS are comparable and
indicate that a more eﬃcient sample is obtained from scheme 2.
In Table 4.2, Geweke’s and Gelman and Rubin’s convergence diagnostic
values and diﬀerent levels of autocorrelations are given for the MCMC sam-
ples of all item parameters from WinBUGS and scheme 2. The convergence
diagnostic of Geweke (1992) compares the means of the sampled values at
two diﬀerent stages of the sequence. The diﬀerence between the two means is
assumed to be asymptotically normally distributed, and a diﬀerence that is
located in the tail of the distribution provides evidence against convergence.
The corresponding p-values in Table 4.2 show that the MCMC samples of dis-
crimination parameter 7 and diﬃculty parameters 4, 7, and 8 may not have
reached convergence when using a signiﬁcance level of 5%. Gelman and Ru-

4.5 Performance MCMC Schemes
91
Iteration
Iteration
Discrimination parameter item 4: Mean = .882, SD = .077, Lag 1 acf = .827.
4
4
Difficulty parameter item 4: Mean = -.680, SD = .053, Lag 1 acf = .643.
Fig. 4.2. MCMC iterates from three parallel chains of the discrimination and diﬃ-
culty parameters from item 4 using scheme 2.
bin’s convergence diagnostic shows no indication of nonconvergence for any
of the MCMC chains. This scale reduction factor should be close to one if
the sampler is close to the target distribution. For the discrimination param-
eter, comparable estimated autocorrelations within chains were found with
both programs. After approximately a lag of 50, the iterations within a chain
are independent. For the diﬃculty parameter, WinBUGS produces stronger
within-chain autocorrelated samples when looking at the estimated autocor-
relation of lags 1 and 5. Cross-correlations indicate the amount of correlation
between the parameter values drawn, and high correlations lead to slow con-
vergence. The MCMC samples did not exhibit high cross-correlations.
Finally, the parameter estimates and standard deviations from the dif-
ferent programs are reported in Table 4.3. Despite the autocorrelations, all
sampled values obtained after a burn-in period of 5,000 iterations were used
for posterior summarization. Bilog-MG assumes a lognormal and a normal
prior for each discrimination and each diﬃculty parameter, respectively. This
program computes MAP (maximum a posteriori, posterior mode) estimates
for the item parameters. In Table 4.3, it can be seen that the estimated values
do not diﬀer much from the EAP estimates that are computed via WinBUGS
and MCMC scheme 2, and most diﬀerences occur only in the second or third

92
4 Estimation of Bayesian Item Response Models
Iteration
Iteration
4
4
Difficulty parameter item 4: Mean = -.690, SD = .059, Lag 1 acf = .802.
Discrimination parameter item 4: Mean = .908, SD = .080, Lag 1 acf = .836.
Fig. 4.3. MCMC iterates from three parallel chains of the discrimination and diﬃ-
culty parameters from item 4 using WinBUGS.
decimal places. Baker and Kim (2004, Chapter 12) and Kim (2001), among
others, compared the Gibbs sampling method of Albert (1992) with Bilog-
MG and a predecessor of WinBUGS, and they obtained comparable item and
ability estimates for diﬀerent datasets. From the present study, it follows that
MCMC scheme 2 produces a more eﬃcient MCMC sample and uses less com-
putation time (in Fortran). Inspection of the trace plots also showed that the
choice of starting values hardly inﬂuences the run of the chain in contrast to
the sequences produced by WinBUGS.
4.5.2 Hierarchical Priors and Shrinkage
Item response data were generated according to a normal ogive model with
a standard normal prior for the ability parameters and the hierarchical prior
for the item parameters (Equations (2.3)–(2.5)). A within-item correlation
structure of .30 was assumed. Data were simulated for 100 persons responding
to 50 items and 1,000 persons responding to 10 items.
The item parameters were obtained for each dataset using the normal
ogive model. Two diﬀerent priors for the item parameters were used: ﬁrst, the
hierarchical prior, where the hyperparameters were estimated from the data

4.5 Performance MCMC Schemes
93
Table 4.2. Convergence properties of the Gibbs sampling chains from scheme 2 and
the MCMC chains from WinBUGS.
MCMC scheme 2
WinBUGS
Geweke
Autocorrelation
G&R Geweke
Autocorrelation
G&R
Item p-value Lag 1 Lag 5 Lag 50 SRF
p-value Lag 1 Lag 5 Lag 50 SRF
Discrimination Parameter
1
.590
.892
.603 −.001 1.000
.386
.844
.444
.036 1.001
2
.526
.866
.528
.000 1.000
.146
.836
.402
.029 1.003
3
.868
.923
.713
.081 0.999
.371
.885
.579 −.019 1.000
4
.105
.827
.453 −.028 1.000
.303
.836
.441
.000 1.002
5
.078
.794
.406 −.007 1.000
.837
.835
.441 −.017 1.001
6
.211
.722
.255
.016 1.000
.768
.797
.347
.005 1.004
7
.927
.777
.328 −.017 1.000
.995
.797
.308
.024 1.001
8
.500
.858
.506 −.014 0.999
.899
.819
.388 −.005 1.000
9
.065
.848
.520
.012 1.001
.247
.848
.477 −.005 1.001
10
.108
.793
.350
.020 1.001
.754
.821
.397 −.017 1.000
Diﬃculty Parameter
1
.933
.645
.187
.071 1.000
.174
.833
.505
.039 1.001
2
.391
.603
.151
.046 1.000
.869
.809
.432
.045 1.001
3
.928
.776
.448
.318 1.000
.245
.860
.552
.038 1.000
4
.416
.643
.214
.100 1.000
.008
.802
.431
.041 1.000
5
.117
.646
.222
.116 1.000
.877
.810
.435
.026 1.001
6
.085
.501
.058
.015 1.000
.947
.760
.327
.030 1.002
7
.054
.487
.037
.000 1.000
.014
.760
.335
.034 1.000
8
.096
.609
.137
.063 1.000
.096
.803
.426
.032 1.000
9
.143
.703
.288
.150 1.000
.414
.829
.473
.046 1.003
10
.058
.588
.145
.050 1.000
.108
.809
.415
.043 1.000
that handle the amount of shrinkage in the posterior means, and second, an
uninformative independent normal prior for the item parameters with a large
variance, where the discrimination parameters were restricted to be positive.
Thus, in the second case, the hyperparameters were ﬁxed a priori, which means
that the amount of shrinkage was also settled a priori.
Diﬀerent runs of 20,000 iterations were performed using MCMC scheme
2. The ﬁrst 1,000 iterations were considered as a burn-in period. The sampled
item parameter values under the diﬀerent priors were used to compute mean
squared errors (MSEs). In Figure 4.4, the estimated MSEs of the item param-
eters for two diﬀerent prior structures are plotted for the dataset of 50 items
and 100 persons. It can be seen that estimated MSEs of the discrimination
and diﬃculty parameters corresponding to the independent prior are in most
cases greater than the MSE estimates corresponding to the hierarchical prior.
The accuracy of each item parameter estimate was improved using a hierarchi-
cal prior by borrowing information from the other item parameter estimates,

94
4 Estimation of Bayesian Item Response Models
Table 4.3. Item parameter estimates of the two-parameter logistic response model.
MCMC 2
Bilog-MG
WinBUGS
Item Simulated
Mean
SD
Mean
SD
Mean
SD
Discrimination Parameter
1
1.150
1.167
.096
1.132
.101
1.168
.110
2
1.034
1.067
.086
1.021
.092
1.061
.094
3
1.372
1.273
.114
1.233
.113
1.264
.114
4
.873
.882
.077
.869
.081
.908
.080
5
.856
.792
.070
.776
.073
.799
.072
6
.629
.623
.060
.605
.058
.640
.059
7
.890
.792
.067
.752
.068
.792
.074
8
1.057
1.061
.084
1.020
.093
1.074
.089
9
1.087
.936
.081
.922
.083
.951
.087
10
.838
.836
.070
.816
.075
.836
.071
Diﬃculty Parameter
1
−.505
−.477
.063 −.485
.067 −.489
.068
2
−.376
−.390
.060 −.388
.061 −.391
.060
3
.796
.815
.067
.825
.081
.800
.076
4
−.560
−.680
.063 −.685
.063 −.690
.059
5
.768
.755
.063
.756
.062
.740
.062
6
.314
.419
.044
.416
.049
.409
.048
7
.163
.156
.044
.154
.050
.152
.051
8
−.519
−.444
.051 −.445
.062 −.454
.058
9
.807
.811
.058
.822
.069
.806
.067
10
.471
.528
.049
.526
.058
.513
.056
where the amount of shrinkage was inferred from the data. Furthermore, the
within-item dependency improved the item parameter estimates. The inde-
pendent prior induced almost no shrinkage, and the item parameter estimates
were based only on the 100 item responses.
In Figure 4.5, the MSEs are plotted for the dataset of 10 items and 1,000
persons. The estimated diﬀerences in MSEs corresponding to the indepen-
dent and hierarchical priors are very small and not systematic. The estimated
shrinkage eﬀects are very small for both priors. The accuracy of the item
parameter estimates was substantially improved by increasing the number
of respondents, and the prior inﬂuence became very small. Furthermore, a
less accurate estimate of the within-item covariance structure was obtained
by decreasing the number of items. It can be concluded that the hierarchical
prior is useful for relatively small datasets when prior information signiﬁcantly
inﬂuences the item parameter estimates.
Assume the object was to estimate the person parameters. Then, a hi-
erarchical prior for the person parameters supports shrinkage eﬀects on the

4.6 European Social Survey: Measuring Political Interest
95
MSEs Discrimination Parameters
MSEs Difficulty Parameters
Item
Item
Independent Item Prior
Hierarchical Item Prior
Fig. 4.4. MSEs of item parameter estimates using an independent item prior and
a hierarchical item prior with N=100 and K=50.
person parameter estimates. There are many persons with 10 responses, and
improved person parameter estimates will be obtained by pooling information
from other respondents. This is in comparison with the dataset of 50 items,
where accurate person estimates are obtained from the data and the prior
inﬂuence is small.
4.6 European Social Survey: Measuring Political Interest
The European Social Survey (ESS; http://www.europeansocialsurvey.
org) has taken place every two years starting in 2001 and covers over 30
countries. The objective of the ESS is to gather data about attitudes, at-
tributes, and behavior patterns; measure and explain people’s social values,
cultural norms, and behavior patterns, and explore the way in which they
diﬀer within and between nations and the direction and speed at which they
are changing.
Attention is focused on three items measuring political interest from the
main questionnaire of 2006 on the block political engagement. The three items
are: (1) How interested would you say you are in politics (very, quite, hardly,
or not at all interested)? (2) How often does politics seem so complicated that
you can’t really understand what is going on (never, seldom, occasionally,
regularly, frequently)? (3) Do you think that you could take an active role

96
4 Estimation of Bayesian Item Response Models
MSEs Discrimination Parameters
MSEs Difficulty Parameters
Item
Item
Independent Item Prior
Hierarchical Item Prior
Fig. 4.5. MSEs of item parameter estimates using an independent item prior and
a hierarchical item prior with N=1,000 and K=10.
in a group involved with political issues (deﬁnitely, probably, not sure either
way, probably not, deﬁnitely not)?
The answers from the 2,364 Dutch respondents were analyzed with the
graded response model. The latent variable, θ, presents the political interest
and has a prior density according to Equations (2.8)–(2.10). The threshold
parameters are order restricted a priori and uniformly distributed, and the
discrimination parameters have a common positively truncated normal prior.
MCMC scheme 3 was run for 10,000 iterations, where the ﬁrst 1,000 iterations
were regarded as a burn-in period.
In Figure 4.6, for each item, the ﬁrst 1,000 MCMC iterates of the threshold
parameters are plotted. The threshold parameters are ordered, which follows
from the stepwise pattern, where the ﬁrst threshold parameter separates the
ﬁrst two response options. The starting values were noninformative ordered
integer values beginning at zero, and the proposal density variance was set
at .05. It can be seen that item 1 has three threshold parameters (four re-
sponse categories) and items 2 and 3 four threshold parameters each (ﬁve
response categories). Although an M-H algorithm was used to sample values,
the iterates of each threshold parameter converged very quickly to the highest
posterior density region.
The estimated discrimination parameters are, respectively, .942 (.05),
1.040 (.06), and .974 (.05), where the posterior standard deviations are given in
parentheses. In Figure 4.7, the estimated category response curves are plotted

4.6 European Social Survey: Measuring Political Interest
97
1,000
-3
-2
-1
0
1
2
3
-3
-2
-1
0
1
2
3
-3
-2
-1
0
1
2
3
1,000
1,000
Iteration
Item 1
Item 2
Item 3
1,000
1,000
1,000
1,000
1,000
1,000
1,000
1,000
1,000
1,000
Iteration
Iteration
Fig. 4.6. ESS 2006: For each item, the ﬁrst 1,000 MCMC iterates of the threshold
parameters.
for three political items. Each category response curve represents the proba-
bility, denoted as Pkc(θ), of scoring in the particular category (c = 1, . . . , Ck)
conditional on the level of θ. For the population mean level of θ = 0, category
2 has the highest response probability for item 1 and category 3 for items 2
and 3. The estimated discrimination parameters are comparable, which means
that the category response curves are similarly shaped (see Figure 4.7). There-
fore, the category response curves diﬀerentiate across items in a similar way
among levels of the latent variable.
In Figure 4.7, for each item, the density of the latent responses (see Equa-
tion (4.32)) is plotted (dotted lines). The corresponding estimated threshold
parameters plotted show the range of latent response values that lead to a re-
sponse in a certain category. It follows that only a few respondents selected the
ﬁrst and last response categories of item 3 such that only a few respondents
are absolutely certain about playing an active role in a political group.
Posterior probability statements can be estimated using the sampled val-
ues. For example, the posterior probability of a Dutch person with below
average interest in politics who ﬁnds politics never or rarely too complicated
to understand equals

98
4 Estimation of Bayesian Item Response Models
-3
-2
-1
0
1
2
3
0.0
0.2
0.4
0.6
0.8
1.0
-3
-2
-1
0
1
2
3
0.0
0.2
0.4
0.6
0.8
1.0
-3
-2
-1
0
1
2
3
0.0
0.2
0.4
0.6
0.8
1.0
Item 1
Item 2
Item 3
11
12
13
21
22
23
24
31
32
33
34
1
2
3
4
1
2
3
4
1
2
3
4
5
1
2
3
4
5
1
2
3
4
5
1
2
3
4
5



P ( )
1c 
P ( )
2c 
P ( )
3c 
Fig. 4.7. ESS 2006: Category response curves of the three items under the graded
response model.
P (Yrep ≤2 | y) =
Z ∞
0
P (Z ≤κ2,3 −a2θ | θ) p(θ | y)dθ
=
Z ∞
0
Φ (κ2,3 −a2θ) p(θ | y)dθ
≈1/M
X
m
Φ

κ2,3 −a2θ(m)
= .101,
(4.39)
where θ(m) > 0 (m = 1, . . . , M) is an MCMC sample from the marginal
posterior density. The posterior probability is estimated given estimated item
parameters. It is also possible to estimate the marginal posterior probability by
integrating over the item parameter density using the MCMC samples. Note
that in this case the normal distribution of the underlying latent response is
used to compute the posterior probability. The observation yrep is a (posterior)
predictive observation under the model given the observed data y. In Chapter
5, a more complete treatment of predictions based on posterior predictive
densities is given.
4.7 Discussion and Further Reading
Bayesian estimation methods in the area of item response modeling started
with applications of ability estimation under the assumption of known item

4.8 Exercises
99
parameters. Bayes estimates of ability parameters were obtained by Birn-
baum (1969) and Owen (1975). In the 1980s, Bayesian estimation methods
were mainly focused on ﬁnding the posterior modes of both item and person
parameters and relied heavily on numerical routines such as Gauss-Hermite
quadrature and Newton-Raphson. The pioneering work of Mislevy (1986),
Swaminathan and Giﬀord (1982, 1985, 1986), Tsutakawa (1984), Tsutakawa
and Lin (1986), and Tsutakawa and Soltys (1988) stimulated the Bayesian ap-
proach to item response modeling and reﬂects the discussion of the marginal
estimation approach versus the joint estimation approach. Baker and Kim
(2004, Chapter 7) give an overview of Bayesian parameter estimation meth-
ods for item response models that were developed in the 1980s.
The feasibility of the Bayesian approach was often questioned due to the
computational burden. This turned around in the 1990s. With the introduc-
tion of MCMC, the Bayesian methods became popular. In the 1990s, item
response modeling applications using the MCMC methodology appeared (Al-
bert, 1992; Bradlow et al., 1999; Patz and Junker, 1999a). The behavior of
MCMC methods, parameter recovery, the computation time, and the conver-
gence properties were investigated by Baker and Kim (2004), Kim (2001), and
Patz and Junker (1999a), among others. As the complexity increases, MCMC
methods become more attractive since they are based on simulations instead
of exact numerical methods, and MCMC methods become particularly useful
when data are sparse and/or asymptotic theory is unlikely to hold. In subse-
quent chapters, it will be shown that the MCMC schemes discussed are easily
extended to more complex response models.
MCMC estimation methods for item response models have become in-
creasingly common in diﬀerent research areas, and several applications will
be presented. Nowadays, diﬀerent MCMC implementations are proposed for
various item response models, and only a few are mentioned here besides
the MCMC schemes discussed. There are several MCMC implementations for
estimating the parameters of item response models for polytomous response
data. Patz and Junker (1999a), among others, described an implementation of
an M-H within Gibbs scheme for the generalized partial credit model (GPC;
Muraki, 1992). Johnson and Albert (1999) generalized the M-H within Gibbs
scheme of Cowles (1996) for the ordinal probit model to a scheme for the
normal ogive graded response model. B´eguin and Glas (2001) developed an
MCMC algorithm for multidimensional item response models. Rupp, Dey and
Zumbo (2004) give a short overview of item response modeling applications
using MCMC estimation methods.
4.8 Exercises
4.1. According to (4.3), assume a lognormal prior for a discrimination param-
eter, log ak ∼N(µa, σ2
a).
(a) Show that the density function of ak equals

100
4 Estimation of Bayesian Item Response Models
p(ak) =
1
akσa
√
2π exp
 −1
2σ2a
 log ak −µa
2

.
(b) Let ak = exp
 ˜ak

. Then the moments of ak can be obtained via the
moment-generating function of a normal density, which is
E(exp(t˜ak)) = exp(tµa + tσ2
a/2),
(4.40)
and the kth moment is equal to the kth derivative of the right-hand side of
(4.40) evaluated at t = 0. Show that the mean and variance of the lognormally
distributed variable ak equal
E(ak) = exp
 µa + σ2
a/2

,
(4.41)
Var(ak) = exp
 2µa + σ2
a
  exp
 σ2
a

−1

.
(4.42)
(c) Derive equations for the prior parameters
 µa, σ2
a

from (4.41) and (4.42)
that can be used to assign prior parameter values for speciﬁc values of E(ak)
and Var(ak).
(d) Show how to generate values a(m)
k
with mean 1 and variance .5 when
ak = exp
 ˜ak

and ˜ak ∼N
 µa, σ2
a

.
4.2. According to Equations (2.8)–(2.10), let g1 = n0/2 and g2 = n0σ2
0/2. It
can be shown that the conditional density p(µθ, σ2
θ | θ) is a normal inverse
gamma.
(a) Show that the prior p(µθ, σ2
θ) and the conditional density p(θ | µθ, σ2
θ) are
proportional to, respectively,
 σ2
θ
−(
n0+1
2
+1) exp

−1
2σ2
θ
 n0σ2
0 + n0(µθ −µ0)2
(4.43)
and
 σ2
θ
−n
2 exp

−1
2σ2
θ
 (n −1)s2 + n(θ −µθ)2
,
(4.44)
where ¯θ = P
i θi/n and s2 = P
i(θi −µθ)2/(n −1).
(b) Combine the mean structures of Equations (4.43) and (4.44) and show
that they can be rewritten as
n0(µθ −µ0)2 + n(µθ −θ)2 = (n + n0) (µθ −µn)2 +
nn0
n + n0
(θ −µ0)2,(4.45)
where µn =
n
n+n0 θ +
n0
n+n0 µ0.
(c) Derive the parameters of the normal density p(µθ | σ2
θ, θ).
(d) Collect terms of interest from (4.43)–(4.45), and derive the parameters of
the inverse gamma density p(σ2
θ | θ).

4.8 Exercises
101
4.3. Assume that ˜ξk = (log(ak), bk)t and ˜ξk ∼N (µξ, Σξ), and let (2.4) and
(2.5) specify the hyperprior densities.
(a) Show via matrix algebra that the conditional density
p

˜ξ | µξ, Σξ

∝|Σξ|−K/2 exp
 
−1
2
X
k

˜ξk −µξ
t
Σ−1
ξ

˜ξk −µξ
!
can be factorized as
|Σξ|−K/2 exp
K
2

−tr Σ−1
ξ S −
 ξ −µξ
t Σ−1
ξ
 ξ −µξ

,
(4.46)
where S = P
k(˜ξk −ξ)(˜ξk −ξ)t/K and ξ = P
k ˜ξk/K.
(b) Show that the hyperprior density of (µξ, Σξ) can be written as
p (µξ, Σξ | µ0, Σ0, K0, ν) ∝|Σξ|−( ν+2
2
+1) exp
1
2

−tr

Σ0Σ−1
ξ

−K0 (µξ −µ0)t Σ−1
ξ (µξ −µ0)

.
(4.47)
(c) Derive the parameters of the normal density p(µξ | Σξ, ˜ξ) by collecting
terms from (4.46) and (4.47).
(d) Given that p(µξ, Σξ | ˜ξ) = p(µξ | Σξ, ˜ξ)p(Σξ | ˜ξ), derive the parameters
of the inverse Wishart density p(Σξ | ˜ξ).
4.4. The prior for the item parameters ξk is normal with mean µξ = (µa, µb)
and variance Σξ, and assume a normal prior for µξ with mean µ0 and variance
Σ0.
(a) Assume that Σξ is known. Show that the density p (µξ | ξ, µ0, Σ0) is
normal with mean
µ∗=

Σ−1
0
+ KΣ−1
ξ
−1 
KξΣ−1
ξ
+ µ0Σ−1
0

and variance
Σ∗=

Σ−1
0
+ KΣ−1
ξ
−1
,
where ξ = K−1 P
k ξk.
(b) Assume that each ak is restricted to the set Ak = {ak ∈R, ak > 0}
and ak | µa, σ2
a ∼N(µa, σ2
a)I(ak ∈Ak). Show that the conditional density of
µξ | ξ, µ0, Σ0 is proportional to
p (µξ | ξ, µ0, Σ0) ∝
exp

−1
2 (µξ −µ∗)t Σ∗(µξ −µ∗)

Q
k Φ (µa/σa)
(4.48)
when ak ∈Ak and zero when ak ∈Ac
k.

102
4 Estimation of Bayesian Item Response Models
(c) Extend MCMC scheme 2 with an M-H step to draw samples from the
density in Equation (4.48), where the parameter of interest is also located in
the normalizing constant.
(d) Assume the lognormal prior in Equation (4.3) for the item parameters,
with a normal inverse Wishart prior for the hyperparameters. Adjust MCMC
scheme 2 by deﬁning sampling steps for the item parameters and the hyper-
parameters.
(e) Related to (d), Gibbs sampling steps can be constructed to sample the
item parameters. Show how the diﬃculty parameters can be sampled from a
normal density via
p (bk | ˜zk, θ, ak, µξ, Σξ) ∝p (˜zk | ξk, θ) p (bk | ak, µξ, Σξ) .
Show that ˜ak = log(ak) can be sampled from
˜ak | ˜zk, bk, µξ, Σξ ∼N
 µ˜a|b, Σ˜a|b

I(∆l,∆u)(˜ak),
where µ˜a|b and Σ˜a|b are the prior parameters from the prior of ak given bk,
and where
∆l = max

max
i|Yik=0,θi<0 f(˜zik, θi, bk),
max
i|Yik=1,θi>0 f(˜zik, θi, bk)

,
∆u = min

min
i|Yik=0,θi>0 f(˜zik, θi, bk),
min
i|Yik=1,θi<0 f(˜zik, θi, bk)

,
with f(˜zik, θi, bk) = log

bk−˜zik
θi

.
4.5. Consider the data augmentation scheme deﬁned in Equation (4.7), a nor-
mal prior for the ability parameters (Equation (2.8)), and common normal
priors for the item parameters (Equation (4.23) and (4.24)). Sampling steps
are derived that will make up Albert’s MCMC scheme (Albert, 1992).
(a) Assume normally distributed augmented data. Show that the full condi-
tional of θi is normal with mean
 ata + σ−2
θ
−1  at (zi + b) + µθ/σ2
θ

(4.49)
and variance
 ata + σ−2
θ
−1 .
(b) The model is identiﬁed by ﬁxing prior parameters µθ = 0 and σ2
θ = 1.
Show that the conditional expected value of θi equals
E (θi | zi, ξ) =
 ata
−1 at (zi + b) −
1
ata + 1
 ata
−1 at (zi + b) .
(4.50)
(c) Explain that the EAP of θi in (4.50) has the form of a shrinkage estimator.

4.8 Exercises
103
(d) Deﬁne the sampling step for the item parameters by specifying the full
conditional density.
(e) Deﬁne sampling steps for the parameters of the priors for the item pa-
rameters using normal priors for the means and inverse gamma priors for the
variances.
4.6. The logistic response model is very close to the normal ogive response
model. The parameters of both models are deﬁned on the same scale when
using an adjustment factor d such that
p (Yi = 1 | θi, a, b) = Φ (aθi −b) =
exp (d (aθi −b))
1 + exp (d (aθi −b)).
If the adjustment factor is considered to be a parameter, a sampling scheme
can be deﬁned to estimate its value. Therefore, simulate data under the nor-
mal ogive model, given values for (a, b, θ), by deﬁning uniformly distributed
variables Ui ∈(0, 1) (i = 1, . . . , N) in such a way that Yi = 1 when
Ui < Φ (aθi −b)
and zero otherwise.
(a) Show that the conditional density of Ui is given by
Ui | y, d ∼



U

0,
exp(d(aθi−b))
1+exp(d(aθi−b))

if Yi = 1
U

exp(d(aθi−b))
1+exp(d(aθi−b)), 1

if Yi = 0.
(4.51)
(b) Show that the conditional density in Equation (4.51) induces the restric-
tions
d ≥log

ui
1 −ui

ηi if i ∈A1,
d < log

ui
1 −ui

ηi if i ∈A2,
where the sets A1 and A2 equal
A1 = {i : (Yi = 1) ∪(ηi > 0) ∩(Yi = 0) ∪(ηi < 0)} ,
A2 = {i : (Yi = 1) ∪(ηi < 0) ∩(Yi = 0) ∪(ηi > 0)} ,
where ηi = aθi −b.
(c) Given a noninformative prior p(d) ∝1, deduce the conditional posterior
d | u, y ∼U

max
i∈A1 log

ui
1 −ui

ηi, min
i∈A2 log

ui
1 −ui

ηi

.
(d) Deﬁne a Gibbs sampling scheme for estimating the adjustment factor d.
(e) Explain how sample size N inﬂuences the convergence properties of the
algorithm. (MCMC scheme 2 can be extended such that the adjustment factor
becomes a nuisance parameter instead of using a ﬁxed value of 1.7; see Lord,
1980, pp. 12–14).

104
4 Estimation of Bayesian Item Response Models
4.7. Deﬁne a Gibbs sampler for the partial credit model based on data aug-
mentation. Attention is focused on a Gibbs sampling step for the threshold
parameter κk,c.
(a) Derive the conditional distribution of κk in terms of cumulative probabil-
ities, where
πik(c) = P (Yik ≤c) =
Pc
s=0 exp Ps
l=0 (θi −κk,l)
PCK
r=0 exp Pr
l=0 (θi −κk,l)
.
(b) Derive the conditional distribution of κk given realizations of a random
variable Uik ∼U[0,1], where
πik(c) = P
 
Uik ≤
Pc
s=0 exp Ps
l=0 (θi −κk,l)
PCK
r=0 exp Pr
l=0 (θi −κk,l)
!
.
(c) Show that the conditional distribution of Uik is given by
Uik | θ, κ, Yik = c ∼U (πik(c −1), πik(c)) .
(d) Show that the augmented data can be used to deﬁne a restriction on the
parameter space of κk,c; that is,
κk,c ≤
c−1
X
l=0
(θi −κk,l) + θi −log


uik
1 −uik


Ck
X
r̸=c
exp
r
X
l=0
(θi −κk,l)
−1
uik
c−1
X
s̸=c
exp
s
X
l=0
(θi −κk,l)



if Yik = c,
(4.52)
κk,c >
c−1
X
l=0
(θi −κk,l) + θi −log


uik
1 −uik


Ck
X
r̸=c
exp
r
X
l=0
(θi −κk,l)
−1
uik
c−1
X
s=0
exp
s
X
l=0
(θi −κk,l)
!#
if Yik = c + 1.
(4.53)
(e) Show that the full conditional of κk,c equals
κk,c | u, y, θ, κk(−c) ∼U(h1, h2)p(κk,c),
where
h1 =
max
i|Yik=c+1 ∆l,
h2 = min
i|Yik=c ∆u,
where ∆l and ∆u equal the right-hand sides of (4.53) and (4.52), respectively.

4.8 Exercises
105
(f) Consider the logistic graded response model and the prior in Equation
(2.8). In the same way as above, deﬁne a uniformly distributed random vari-
able and show that the M-H step 2 of MCMC scheme 3 can be replaced by
the Gibbs sampling step
θi | y, a(m), κ(m), µ(m)
θ
, σ2(m)
θ
∼N
 µθ, σ2
θ

IRθ(θi)
for Rθ = {θi ∈R, ∆l < θi < ∆u}, where
∆l = max
c|k∈A

log
1 −uk
uk

+ κk,c

/ak,
∆u = min
c|k∈A

log
1 −uk
uk

+ κk,c−1

/ak,
and A = {k : Yik = c}.
4.8. The object is to sample threshold parameters. Deﬁne δ = 1/κk,Ck−1 for
Ck −1 > 0.
(a) Show that the normally distributed augmented data deﬁned in Equation
(4.25) when parameterized as Z∗
ik = Zikδ are distributed as
Z∗
k | δ, ak, θ ∼N
 akθδ2, δ2
.
(b) Show how to sample threshold parameter values κk,c from their conditional
distributions, where δ functions as a variance parameter.
(c) The reparameterization eliminates all unknown threshold parameters for
item k when Ck = 3 and κk,1 is ﬁxed to zero to identify the model. Show how
to sample the threshold parameters κk′1 for k′ ̸= k by using the cumulative
probabilities P (Z∗
k′ ≤κk′1) and P (Z∗
k′ > κk′1).
4.9. A hierarchical normal prior for the item parameters ξk = (ak, bk, ˜ck)t is
deﬁned as
(ak, bk, ˜ck)t ∼N (µξ, Σξ) IAk(ak),
where Ak = {ak ∈R, ak > 0} and ˜ck = Φ−1 (ck). Assume a normal inverse
Wishart distribution for the hyperparameters.
(a) Show that the transformation ˜ck = Φ−1 (ck) is monotone, where ck and
˜ck have density functions p(.) and g(.) with support sets
Rck = {ck; p (ck) > 0},
R˜ck = {˜ck; ˜ck = Φ−1 (ck) for ck ∈Rck},
respectively.
(b) Show that the conditional distribution of ˜ck given ak, bk, µξ, Σξ is contin-
uous on the support set R˜c and that Φ−1 (ck) has a continuous derivative on
R˜c.

106
4 Estimation of Bayesian Item Response Models
(c) Derive the conditional prior of ck given ak, bk, µξ, Σξ via
p (ck | ak, bk, µξ, Σξ) = g
 Φ−1 (ck) | ak, bk, µξ, Σξ
 dΦ−1 (ck)
dck
.
(4.54)
(d) Deﬁne the acceptance probability of a candidate value c∗
k from a proposal
Be(α, β) distribution given the prior for ck, deﬁned in Equation (4.54), and
binomially distributed augmented data Sk, as deﬁned above Equation (4.22).
4.10. An observed item response is denoted as yobs and a missing item re-
sponse as ymis. Assume that data are missing at random (MAR) such that
the probability of a value being missing depends only on observed values and
not missing values (Rubin, 1976).3
(a) The observed-data posterior distribution of θ can be related to the
complete-data posterior distribution:
p (θ | yobs) =
Z
p (θ | ymis, yobs) p (ymis | yobs) dymis.
Explain how the posterior mean can be estimated using draws of Ymis from
p (ymis | yobs).
(b) Use the item response model for analysis to draw missing values. Verify
that the probability function of Ymis given yobs can be written as
p (ymis | yobs) =
Z
p (ymis | θ) p (θ | yobs) dθ.
(c) Consider MCMC scheme 1, and deﬁne an additional sampling step for
generating realizations of Ymis given yobs.
(d) Consider MCMC scheme 2, where a data augmentation step is used. Verify
that the probability function of Ymis given yobs can be written as
p (ymis | yobs) =
Z Z
p (ymis | z) p (z | θ, yobs) p (θ | yobs) dθdz,
where z is an augmented item response.
(e) The following data augmentation scheme for MCMC scheme 2 is deﬁned:
Zik | θi, ak, bk, Yik ∼



N (akθi −bk, 1) I (Zik ≤0) if Yik = 0
N (akθi −bk, 1) I (Zik > 0) if Yik = 1
N (akθi −bk, 1)
if Yik is missing.
Show how to compute the expected value of a missing observation.
(f) Assume that the (conditional) expected level of ability is given by Equation
(4.49), and that the data augmentation scheme in (e) is used. Explain the
eﬀect of imputing missing responses on the estimated posterior mean level of
ability.
3 The missing-data mechanism is ignorable by assuming that the parameters of the
item response model for the observed data and the parameters of the missingness
mechanism are distinct (Rubin, 1987).

5
Assessment of Bayesian Item Response Models
The underlying assumptions of Bayesian item response models have to be
examined to ensure their credibility and that meaningful inferences can be
made. A set of tools will be discussed for testing model assumptions and
hypotheses. This set of tools includes methods based on Bayesian residuals and
predictive diagnostic checks. It will be shown that related computations can
be done during an MCMC estimation procedure or afterwards using MCMC
output.
5.1 Bayesian Model Investigation
A very powerful approach for model assessment is based on predictive as-
sessment. The idea is to generate predictive data from the model, and the
replicated data are compared with the observed data. Replicated datasets are
compared with observed data with respect to features of interest, which are
captured in a summary statistic. Diﬀerent summary statistics of the data can
be considered to investigate whether the model is able to describe the char-
acteristics of the observed data. Summary statistics or discrepancy measures
that are functions of the data only are also called test statistics. Discrep-
ancy measures are chosen to detect systematic diﬀerences between model and
data. It is possible to test various speciﬁc assumptions of a model due to the
enormous ﬂexibility in choosing discrepancy functions.
Two diﬀerent sampling procedures for simulating replicated data will be
considered. Data can be simulated from a ﬁtted model and will be referred to
as posterior predictive data since values are drawn conditional on the observed
data. Data can also be sampled from a model without conditioning on observed
data and will be referred to as prior predictive data. Prior predictive model
checks will be used to test whether the model is appropriate for describing the
data without needing posterior simulations. At this point, diﬀerent priors can
be considered to construct a complete model that yields sensible implications
for observables. An MCMC algorithm can be developed for a complete model,
©
and Behavioral Sciences, DOI 10.1007/978-1-4419-0742-4_5,
107
J.-P. Fox, Bayesian Item Response Modeling: Theory and Applications, Statistics for Social
  Springer Science+Business Media, LLC 2010

108
5 Assessment of Bayesian Item Response Models
and posterior predictive data can be used to detect inconsistencies between
model and observed data and to better understand the implications of the
model for observed data.
Although several predictive checks are explored to test the ﬁt of an item
response model, in this chapter only a brief overview is given since there are
numerous possibilities and to date no deﬁnitive predictive tests exist in this
area. Model ﬁtting is an active area of current research, new developments
are evolving quickly, and it seems that this will be a topic of further research.
Besides predictive assessment, a residual analysis is a common statistical
tool for model validation. Residuals are easily obtained as by-products of an
MCMC algorithm and will be used to investigate the ﬁt of the model. When
diﬀerent models are ﬁtted to the same data, they can be compared using a
summary measure of ﬁt. The DIC discussed in Section 3.2.3 will be used to
compare diﬀerent models for observed item response data.
5.2 Bayesian Residual Analysis
Bayesian residuals, also referred to as residuals, are viewed as random pa-
rameters with unknown values. The residuals need to be estimated from the
data together with their uncertainties. Summary statistics of the posterior
distributions of the residuals need to be calculated, and posterior probabil-
ity statements can be made about the values of the realized errors (Box and
Tiao, 1973; Zellner, 1971). For example, assume the normal ogive item re-
sponse model for analyzing binary data. In that case, a residual is deﬁned
as Rik = Yik −Φ (akθi −bk). The marginal posterior density of the residual,
p (rik | yik), is a continuous-valued posterior, and the posterior mean is used
to estimate the residual value.
In a standard residual analysis, residuals are usually transformed such that
they approximately follow a normal distribution. In the case of discrete ob-
servations, such transformations result in poor approximations by the normal
distribution. In a Bayesian residual analysis, attention can be focused on the
posterior distribution of each residual, which can be estimated via MCMC.
Let (θ(m)
i
, a(m)
k
, b(m)
k
) denote an MCMC sample from their joint posterior
distribution. It follows that sampled values from the residual’s posterior dis-
tribution corresponding to observation ik are given by
R(m)
ik
= Yik −Φ

a(m)
k
θ(m)
i
−b(m)
k

.
(5.1)
Although the Bayesian residuals are easily estimated within an MCMC
scheme, the posterior variances of the residuals diﬀer and the residuals’ pos-
terior densities are not directly comparable.

5.2 Bayesian Residual Analysis
109
5.2.1 Bayesian Latent Residuals
Albert and Chib (1995) and Johnson and Albert (1999) introduced Bayesian
latent residuals as an alternative to the Bayesian residuals. In Section 4.3,
various data augmentation schemes were introduced such that a regression
of an augmented variable Zik on the latent ability θi is linear and with the
same error variance for all θi. The diﬀerence between the latent response, Zik,
and the expected response is deﬁned as a Bayesian latent residual. Speciﬁ-
cally, according to the augmentation scheme in Section 4.3.2, the Bayesian
latent residual is deﬁned as the diﬀerence between the augmented normally
distributed Zik (Equation (4.7)) and the expected mean akθi −bk. Note that
each latent residual is standard normally distributed due to the identifying as-
sumptions associated with the data augmentation scheme (see Section 4.4.1).
5.2.2 Computation of Bayesian Latent Residuals
According to Equation (4.38), the Bayesian latent residual corresponding to
binary observations Yik is deﬁned as
εik = Zik −akθi + bk.
(5.2)
From the deﬁnition of the augmented data, it follows that, given ξk and θi, the
Bayesian latent residual εik is standard normally or logistically distributed.
For polytomous response data,
εik = Zik −akθi,
(5.3)
where Zik is deﬁned according to Equation (4.25). Both Bayesian latent resid-
uals (Equations (5.2) and (5.3)) can be estimated as an average of computed
residual values in each MCMC iteration.
A more eﬃcient estimator is based on the conditional expectation given
a suﬃcient statistic, which is called a Rao-Blackwellized estimator (Gelfand
and Smith, 1990). When it is possible to draw independent samples, a Rao-
Blackwellized estimator that is based on averaging conditional expectations
instead of the original parameter (the empirical estimator) of interest can pro-
duce a large variance reduction and be more eﬃcient. The variance reduction
is not guaranteed when the estimator is based on samples that are drawn
dependently using a Gibbs sampler. However, Liu, Wong and Kong (1994)
proved that the Rao-Blackwellized estimator is better than the empirical es-
timator for data augmentation schemes.
The conditional expectation of a Bayesian latent residual is derived by
integrating out the augmented response data. For binary response data (see
Exercise 5.1), if Yik = 1,
E (εik | Yik = 1, θij, ξk) =
Z ∞
0
εik
p (zik, Yik = 1 | θi, ξk)
P (Yik = 1 | θi, ξk)
dzik
= φ (bk −akθi)
Φ (akθi −bk),
(5.4)

110
5 Assessment of Bayesian Item Response Models
where φ(.) is the standard normal density function. For Yik = 0,
E (εik | Yik = 0, θi, ξk) = −φ (bk −akθi)
Φ (bk −akθi) .
(5.5)
For ordinal response data using Equations (4.27) and (5.3), the conditional
expectation of a Bayesian latent residual given Yik = c equals
E (εik | Yik = c, θi, ak) = φ (κk,c−1 −akθi) −φ (κk,c −akθi)
Φ (κk,c −akθi) −Φ (κk,c−1 −akθi).
(5.6)
Some elementary calculations have to be done to ﬁnd expressions for the
posterior variances of the residuals (Exercise 5.1). Note that in a comparable
way Rao-Blackwellized estimates can be obtained for logistically distributed
latent residuals.
5.2.3 Detection of Outliers
The posterior distribution of a Bayesian latent residual can be used to calcu-
late the posterior probability that the corresponding observation is an outlier.
An observation is considered to be outlying if the posterior distribution of
the corresponding residual is located far from its mean. Following Albert and
Chib (1995), Chaloner and Brant (1988), and Zellner (1971), yik is an outlier
if the absolute value of the residual is greater than some prespeciﬁed value q
times the standard deviation. That is, observation yik is marked as an outlier
if P (|εik| > q | yik) is large. The probability that an observation exceeds a
prespeciﬁed value is called the outlying probability.
A Rao-Blackwellized estimate of the conditional probability that the ab-
solute value of latent residual εik exceeds a value q given yik can be derived.
That is, if Yik = 1,
P (|εik| > q | Yik = 1, θi, ak, bk) =
Φ (−q)
Φ (akθij −bk)
(5.7)
for q > −(akθi −bk), and if Yik = 0,
P (|εik| > q | Yik = 0, θi, ak, bk) =
Φ (−q)
1 −Φ (akθi −bk)
(5.8)
for q > akθi −bk. The expressions can be used to estimate the outlying
probabilities of the estimated Bayesian latent residuals given sampled values
of the model parameters (see Exercise 5.2 for details).
It is also possible to ﬁnd the value q such that the outlying probability of
an observation assumes a given percentage, say ν. Therefore, in every MCMC
iteration, q must be solved in the equation P (|εik| > q | yik) = ν/100. The
mean of these values is an estimate of the unique root, that is, the q-percent
value, or the probability that zik will deviate from its mean by more than q.

5.2 Bayesian Residual Analysis
111
The choice of q is quite arbitrary, but if the model under consideration is
required to describe the data, then q = 2 might be used to ﬁnd observations
that are not well described by the data. There is reason for concern if more
than 5% of the residuals have a high posterior probability of being greater
than two standard deviations.
Notice that other complex posterior probabilities can be computed with
an MCMC algorithm by keeping track of all the possible outcomes of the
relevant probability statement. However, this method has the drawback that
a lot of iterations are necessary to get a reliable estimate. It could be possible,
for example, that in the case of multiple outliers a test for a single outlier
does not detect one outlier in the presence of another outlier. This so-called
masking occurs when two outlying probabilities related to observations ik and
sk do not indicate any outliers but the joint posterior probability
P (|εik| > q, |εsk| > q | y)
(5.9)
shows that yik and ysk are both outliers. This joint probability can be esti-
mated by counting the events where both absolute values of the residuals are
greater than q times the standard deviation divided by the total number of
iterations.
5.2.4 Residual Analysis: Dutch Primary School Mathematics Test
Item responses from 2,156 grade eight students, unequally spread over 97
schools, to 18 dichotomously scored mathematics items taken from the ex-
amination upon leaving school developed by the National Institute for Edu-
cational Measurement are considered (Doolaard, 1999). For the moment, the
nesting of students in schools is ignored, but it will be discussed in Section
6.6.1.
A two-parameter normal ogive model was used as the measurement model,
with a standard normal prior for the ability parameters and with the hierar-
chical prior in Equation (4.3) for the item parameters. In Listing 5.1, Rao-
Blackwellized estimates of the latent residuals are speciﬁed. The expressions
for the latent residual estimates are not dependent on the augmented vari-
able and can be implemented in other model formulations such as Listing 1.1
(Exercise 5.3).
Listing 5.1. WinBUGS code: Estimating Bayesian latent residuals.
for
( i
in
1 :N){
for
( k
in
1 :K){
eta [ i , k ] <−a [ k ] ∗theta [ i ] −b [ k ]
r e s i d n [ i , k ]
<−0.3989∗exp( −.5∗(pow(−eta [ i , k ] , 2 ) ) )
r e s i d u a l [ i , k ] <−( r e s i d n [ i , k ] /phi ( eta [ i , k ] ) ) ∗Y[ i , k ] +
(−r e s i d n [ i , k ] /phi(−eta [ i , k ] ) ) ∗(1−Y[ i , k ] )
}
}

112
5 Assessment of Bayesian Item Response Models
In Figure 5.1, the marginal posterior densities of Bayesian latent residuals
and Bayesian residuals corresponding to the same 25 randomly selected an-
swers to item 17 are plotted (lower and upper plots, respectively). The order of
the posterior means is the same in both plots. It can be seen that the marginal
posterior distributions of the Bayesian residuals are deﬁned on (−1, 0) if the
observation corresponding to item 17 equals zero and on (0, 1) otherwise. The
posterior mean of a Bayesian latent residual is positive (negative) when the
answer is correct (incorrect). It is more diﬃcult to assess the extremeness of
the marginal posterior densities of the Bayesian residuals since they are dif-
ferent and deﬁned on diﬀerent domains. Subsequently, it is diﬃcult to identify
outliers from these marginal posterior distributions.
The outlying probabilities were computed for q = 0 using Equations (5.7)
and (5.8) (see also Exercise 5.2). In Figure 5.1, the four smallest posterior
means of the Bayesian latent residuals are signiﬁcantly smaller than zero when
using a 10% signiﬁcance level. For q = 1, the outlying probability of a Bayesian
latent residual is .982, and the corresponding response pattern showed that
all items were scored correct except item 17, although this item was answered
correctly by 88% of the students.
5.3 HPD Region Testing and Bayesian Residuals
As explained in Section 3.2.2, hypotheses can be tested using the posterior
density of the parameters of interest. The posterior density is used to test
whether or not a speciﬁc point lies inside or outside an HPD region. According
to the usual form of a hypothesis that a parameter value or a function of
parameter values is zero, the HPD interval is used to test if the parameter
value diﬀers signiﬁcantly from zero.
Item and Person Fit
By means of a person-ﬁt statistic, the ﬁt of a score pattern can be determined
given that the item response model holds. Investigation of the person ﬁt may
provide information about the response behavior of a person. There have been
many statistics proposed to evaluate the ﬁt of persons’ response patterns. Glas
and Meijer (2003) and Meijer and Sijtsma (2001) have given an overview of
person-ﬁt statistics for various item response models.
Here, the idea is to evaluate whether a set of Bayesian latent residuals
corresponding to a speciﬁc response pattern is extreme under the item re-
sponse model. Assume the data augmentation scheme in Equation (4.25) for
the graded response model. The (person-ﬁt) statistic Qp,i in (5.10) is deﬁned
for a set of Bayesian latent residuals (as deﬁned in Equation (5.3)) belonging
to the response pattern of a person indexed i as
Qp,i (Zi) =
X
k
(Zik −akθi)2 =
X
k
ϵ2
ik,
(5.10)

5.3 HPD Region Testing and Bayesian Residuals
113
Bayesian latent residuals
Bayesian residuals
Fig. 5.1. Posterior densities of Bayesian latent residuals and Bayesian residuals
corresponding to item 17.
where the dependence on item and person parameters is ignored. Each latent
residual is standard normally distributed, and the sum of the K squared
Bayesian latent residuals is chi-square distributed with K degrees of freedom.1
This reference distribution is used to quantify the extremeness of the sum of
squared latent residuals. As a result, a corresponding marginal posterior p-
value (tail-area probability) is deﬁned as
p0(Qp,i) =
Z
P
 χ2
K > Qp (zi)

p (zi | yi) dzi,
(5.11)
and a small tail-area probability indicates that the estimated set of estimated
latent residuals is extreme under the item response model for individual i.
Each tail-area probability is easily computed within an MCMC scheme. In
each iteration, after convergence, the conditional tail-area probability is com-
1 Let Z1, . . . , ZK be independent normal random variables. The random variable
χ2
K = Z2
1 +Z2
2 +· · ·+Z2
K has a chi-square distribution with K degrees of freedom.

114
5 Assessment of Bayesian Item Response Models
puted and the mean of the conditional tail-area probabilities is an estimate of
the corresponding marginal tail-area probability.
In the same way, an item ﬁt statistic is deﬁned to assess the ﬁt of an item
characteristic function under the graded response model. For each item, the
corresponding Bayesian latent residuals are explored and interest is focused
on assessing whether the sum of squared estimated Bayesian latent residuals
is signiﬁcantly large. This would indicate a poor ﬁt of the item characteristic
curve. The item ﬁt statistic is deﬁned as
Qitem,k (Zk) =
X
i
(Zik −akθi)2 ,
(5.12)
and the corresponding marginal tail-area probability equals
p0(Qitem,k) =
Z
P
 χ2
N > Qitem,k (zk)

p (zk | yk) dzk,
(5.13)
where the dependence on person and item parameters is suppressed.
Detecting Discriminating Items
In an item analysis, interest is often focused on the discriminating power of the
items. The more eﬀectively an item diﬀerentiates between persons of higher
and lower ability, the more useful that item is as an instrument for separating
individuals. It is of interest to detect the most discriminating items but also
to test whether items discriminate diﬀerently.
The one-sided hypothesis that the discriminating power of an item is above
a mean level, say one, is tested by computing the posterior probability
P (ak ≥1 | y) =
Z ∞
1
p (ak | y) dak.
(5.14)
The posterior probability can be computed via an MCMC sample of discrim-
inating values from the marginal posterior distribution.
The marginal posterior probability in Equation (5.14) can also be com-
puted via a data augmentation scheme. Assume a conditional normally dis-
tributed discrimination parameter according to Equation (4.35). The marginal
posterior probability is expressed as
P (ak ≥1 | y) =
Z ∞
1
Z
p (ak | z) p (z | y) dzdak,
which follows from properties of the augmented data scheme (see Exercise 5.5).
The conditional distribution given augmented data is known, which makes it
possible to derive a closed-form expression,
P
 ak ≥1 | z, θ, µa, σ2
a

=
Φ

Ω−1/2
a
(µ∗
a −1)

Φ

Ω−1/2
a
µ∗a

,
(5.15)

5.3 HPD Region Testing and Bayesian Residuals
115
where Ωa and µ∗
a are deﬁned in Equations (4.36) and (4.37), respectively. As
a result, the posterior probability of ak > 1 can be accurately estimated using
MCMC output also when the event is unlikely to occur.
The hypothesis that a discrimination parameter, or a set of discrimination
parameters, equals a prespeciﬁed value is more diﬃcult to test. The approach
taken is to compute the posterior probability that a prespeciﬁed value a0
k
is contained in the 1 −α HPD region. According to Equation (3.16), the
parameter value a0
k is included in the HPD interval if and only if
P
 p (ak | y) ≥p
 a0
k | y

| y

≤1 −α.
(5.16)
It is concluded that there is evidence that ak diﬀers signiﬁcantly from a0
k when
the probability that the point a0
k is included is greater than 1−α. In that case,
the HPD interval needs to be stretched out to include a speciﬁc point that
is unlikely to be covered. However, the corresponding marginal posterior dis-
tribution is unknown, which makes the procedure slightly more complicated.
The idea is to condition on, among other things, augmented data, such that a
closed-form expression of the conditional posterior probability that a prespec-
iﬁed value is covered by a 1 −α HPD interval is obtained. The conditional
posterior probability is easily computed in each MCMC iteration, and the
average is an estimate of the corresponding marginal posterior probability.
Consider the conditional posterior distribution of ak in Equation (4.35),
which is normal with mean µa∗and variance Ωa. The conditional posterior
density p(ak | zk) is a monotonically decreasing function of
Qa (ak) = Ω−1
a
(ak −µ∗
a)2 ,
(5.17)
and the conditional joint posterior density p(a | z) is a monotonically decreas-
ing function of
Qa (a1, . . . , aK) =
X
k
Ω−1
a
(ak −µ∗
a)2 ,
(5.18)
where, for notational convenience, the conditioning on the other parameters
is suppressed. A large value of Qa indicates that the point is not likely to be
covered by the HPD interval. The Qa is chi-square distributed when ignoring
the positivity restriction on the discrimination parameters. In that case, it
follows that a speciﬁc point is included when
P
 Qa
 a0
k | y

| y

= P
 Qa (ak | y) ≤Qa
 a0
k | y

| y

=
Z
P
 Qa (ak | z) ≤Qa
 a0
k | z

| z

p (z | y) dz
=
Z
P
 χ2
1 ≤Qa
 a0
k | z

| z

p (z | y) dz
≤1 −α,
(5.19)
and a general point is included when

116
5 Assessment of Bayesian Item Response Models
P
 Qa
 a0 | y

| y

= P
 Qa (a | y) ≤Qa
 a0 | y

| y

=
Z
P
 Qa (a | z) ≤Qa
 a0 | z

| z

p (z | y) dz
=
Z
P
 χ2
K ≤Qa
 a0 | z

| z

p (z | y) dz
≤1 −α.
(5.20)
In each MCMC iteration, a conditional p-value is calculated given aug-
mented data using the property that Qa is chi-square distributed. The average
p-value is considered to be an estimate of the marginal p-value. Note that the
p-values related to (5.19) and (5.20) refer to the probability of whether a point
is included within an HPD interval. It is not based on the evaluation of a cri-
terion using predictive or replicated data under the model. Model assessment
using predictive data will be discussed in Section 5.4.
Common misinterpretations of the p-value are (1) that it would specify
the posterior probability of the null hypothesis being true and (2) that a
small p-value provides strong evidence against the null hypothesis. The p-value
measures the surprise in the data and provides information to further develop
or elaborate the model. At best, the p-value provides a rough indication that
there is a certain mismatch between the model and the observed data. The
hypothesis being tested is only a part of the actual test that is performed.
This is one major point of criticism since with this kind of hypothesis testing
it is not possible to address the question of interest directly. With respect
to the second misinterpretation, Berger and Selke (1987) provided several
examples showing that a small p-value does not necessarily mean that there
is strong evidence against the null. Berger and Delampady (1987) and Berger
and Selke (1987), among others, generally recommended the use of the Bayes
factor when speciﬁc alternative hypotheses are available.
5.3.1 Measuring Alcohol Dependence: Graded Response Analysis
The College Alcohol Problem Scale (CAPS; O’Hare, 1997) was developed to
serve as an initial screening instrument for students cited with a ﬁrst oﬀense for
violating their university’s rules concerning underage drinking. The items com-
prising the CAPS scale covered socioemotional problems (hangovers, memory
loss, nervousness, depression) and community problems (drove under the in-
ﬂuence, engaged in activities related to illegal drugs, problems with the law).
Due to the high prevalence of alcohol abuse among college students, it is im-
portant that practitioners in student health services or counseling be able to
identify students with drinking problems.
In 2002, 351 students from four colleges and universities in the state of
North Carolina (Elon University, Guilford Technical Community College, Uni-
versity of North Carolina, Wake Forest University) were asked to respond
to a questionnaire with 13 items from the CAPS instrument, with response

5.4 Predictive Assessment
117
categories on a ﬁve-point scale (1=never/almost never to 5=almost always).
The CAPS questionnaire is given in Section 5.9. It is assumed that a unidi-
mensional latent variable representing alcohol dependence, denoted as θ, was
measured by the items, where a higher level indicated that a participant was
more likely to have a drinking problem.
The normal ogive graded response model was used to measure the individ-
ual alcohol-dependence levels. The ﬁt of individual response patterns, items,
and levels of item discrimination were examined.
For the CAPS data, the Qp,i statistic was evaluated for each person, and
3.4% of the response patterns can be characterized as improbable (aberrant)
when using a signiﬁcance level of 5%. This leads to the conclusion that the
number of person misﬁts is relatively small. Two examples of the assessed
aberrant response behavior are (1) a person answered “often” to items 5
(“Spent too much money on drugs”) and 10 (“Drove under the inﬂuence”)
and “never” to all other items, and (2) a person answered “never” to items 5
(“Spent too much money on drugs”) and 8 (“Caused others to criticize your
behavior”) and “often” to all other items. This suggests that these respondents
were not willing to provide truthful answers and therefore gave inconsistent
answers. The estimated observed score distribution is right-skewed, which in-
dicates that a lot of respondents scored very low, and it might be possible that
several respondents did not give honest answers due to the sensitive nature of
the questions. More attention will be paid to this issue in Chapter 9.
From the 95% HPD intervals in Table 5.1 it follows that all discrimination
parameters are located far away from zero. In the last column of Table 5.1, the
tail-area probabilities, related to Equation (5.19), are given under the heading
p0(Qa), corresponding to the null hypothesis ak = 1 (k = 1, . . . , K). It can
be seen that items 8 and 11 are highly discriminating between persons with
discrimination values signiﬁcantly diﬀerent from one (10% signiﬁcance level).
The tail-area probability of all discrimination parameters being equal to one
(related to Equation (5.20)) is .030. It is concluded that there is no evidence
that all CAPS items discriminate identically.
The Qitem,k ﬁt statistic was computed for each item. In the next to last
column of Table 5.1, the tail-area probabilities are given for each item, and it
can be concluded that the items ﬁt the data. That is, the estimated sum of
squared latent residuals cannot be considered extreme under the graded item
response model.
5.4 Predictive Assessment
A standard statistical tool for model checking is based on a discrepancy mea-
sure (Gelman et al., 1996) or (departure) statistic (Bayarri and Berger, 2000)
to investigate the compatibility of the model with the data. This discrepancy
measure is chosen in such a way that large values indicate less compatibility.

118
5 Assessment of Bayesian Item Response Models
Table 5.1. CAPS: Discrimination parameter estimates of the normal ogive graded
response model.
Item Mean SD
HPD
p0(Qitem) p0(Qa)
1
.645 .075
[.507, .796]
.224
.054
2
.727 .086
[.570, .902]
.511
.134
3 1.017 .101
[.824, 1.206]
.529
.598
4
.848 .113
[.629, 1.073]
.492
.390
5
.838 .124
[.578, 1.063]
.378
.407
6
.874 .099
[.682, 1.058]
.489
.448
7 1.212 .175
[.892, 1.555]
.612
.316
8 1.361 .145 [1.099, 1.647]
.374
.095
9
.916 .084
[.749, 1.074]
.342
.562
10 1.123 .113
[.886, 1.349]
.571
.436
11 1.395 .141 [1.144, 1.678]
.652
.055
12 1.257 .102 [1.064, 1.468]
.267
.164
13 1.229 .142
[.942, 1.503]
.535
.339
The reference distribution of the discrepancy measure is used to measure the
extremeness of the observed discrepancy.
Consider an item response model M with parameters (θ, ξ). Under the
null hypothesis, it is assumed that the response data are conditionally dis-
tributed as p(y | θ, ξ) and the unknown parameters have a prior distribution
p (θ, ξ), without having a speciﬁc alternative hypothesis. Interest is focused
on a statistic, denoted as T(y), to investigate the compatibility of model M
with the observed data yobs. Subsequently, a tail-area probability or p-value
can be computed:
p0(yobs) = P (T(Y) ≥T(yobs) | M) .
(5.21)
The p-value can be computed in diﬀerent ways when the parameters of the
null model are unknown. In a prior predictive approach, the computation of
p0 is done with respect to the marginal distribution of Y (Box, 1980). In a
posterior predictive approach, the computation is done with respect to the
posterior predictive distribution of Y given yobs (e.g., Rubin, 1984). Below,
both approaches will be explored, and the advantages and disadvantages of
both procedures will be discussed.
The predictive diagnostic checks utilized via p-values are speciﬁcally im-
portant when no fully speciﬁed alternative model is available. It is certainly
not unusual that alternative models are not available and that checking the ﬁt
of the posited model needs to be done without the immediate availability of
an alternative. This corresponds with the general beliefs about p-values (e.g.,
Bayarri and Berger, 2000; Gelman et al., 1996; Meng, 1994) that they are
particularly interesting for investigating the compatibility of the model with

5.4 Predictive Assessment
119
the data. That is, a set of T statistics can be used to test the incompatibility
of the model with the data without needing speciﬁc alternatives.
5.4.1 Prior Predictive Assessment
Box (1980) recommended the use of the marginal predictive distribution for
computing a p-value as deﬁned in (5.21). Given that an assumed model M
with parameters (θ, ξ) is true, all possible data samples y that could occur
are distributed as
p(y | M) =
Z Z
p(y | θ, ξ, M)p(θ, ξ | M)dθdξ.
(5.22)
The left-hand side of (5.22) can be recognized as the marginal distribution
of the data but can also be denoted as the prior predictive distribution. The
prior predictive distribution has the same interpretation as the prior distri-
bution. It does not depend on previous observations (prior distribution) and
speciﬁes the distribution of a quantity that is observable (predictive distri-
bution). Equation (5.22) is the predictive distribution of Y that is needed
to measure the surprise in the data. Box (1980) speciﬁed checking functions
of the form T(y) to investigate certain features in the data that are seldom
extreme, if the model is true. The extremeness of each feature is measured
by reference to p(T(y)). This way, diagnostic checks of parametric as well as
residual features of the model can be explored.
An overall natural predictive check for model M is deﬁned by T(y) =
p(y)−1 since high values of T(y), and small values of p(y), suggest that it is
unlikely that the data are observed under the null model. The corresponding
p-value equals
p0(yobs) = P

p (y | M)−1 ≥p (yobs | M)−1
,
(5.23)
where yobs denotes the observed data. The prior predictive tail-area proba-
bility p0 is an indication of the credibility of the model. A (very) low p0 value
indicates that it is unlikely that the observed data yobs have occurred under
model M.
The prior predictive distribution of the discrepancy measure is diﬃcult to
obtain in closed form when it depends on unknown nuisance parameters. In
that case, a forward simulation method can be used to estimate the prior pre-
dictive probability. The forward simulation method is easy to perform since
it only requires sampling parameter values from the prior distributions and
sampling observables from their conditional distribution given the sampled
parameter values. Given population parameters θP and ξP , a forward simu-
lator is deﬁned by
ξ(m) ∼p (ξ | ξP ) ,
θ(m) ∼p (θ | θP ) ,
y(m) ∼p

y | θ(m), ξ(m)
,

120
5 Assessment of Bayesian Item Response Models
where the last step deﬁnes the sampling of response data from the prior pre-
dictive density (according to Equation (5.22)).
For example, let T(yobs) denote the sample skewness of the observed sum
scores (see Exercise 1.3). A highly skewed distribution of observed sum scores
may point out that a normal prior for the ability parameters is not appropriate
and other population priors might be considered. It might also indicate that a
test is too diﬃcult (easy) for the sampled respondents, leading to excessively
low (high) scores, and diﬀerent priors for the item diﬃculty parameters might
be considered. The prior distribution of the statistic T(y) reveals whether the
observed value T(yobs) is an extreme observation under the model. The sample
skewness is computed for each draw of the prior predictive distribution. The
fraction of draws leading to a value higher than T(yobs) is an estimate of the
corresponding prior predictive p-value.
Considering data augmentation scheme (4.7), the prior predictive distri-
bution of the observed data, Equation (5.22), can be written as,
p(y | M) =
Z Z Z
p(z | θ, ξ, M)p(θ, ξ | M)dzdθdξ.
Then, a statistic can be deﬁned based on the augmented data T(zobs). The
following steps can be added to the forward simulator:
z(m)
obs ∼p

z | yobs, θ(m), ξ(m)
,
z(m) ∼p

z | y(m), θ(m), ξ(m)
.
Samples z(m)
obs , from the ﬁrst step, are needed to estimate the value of the
(observed) statistic, and in a similar way draws z(m), from the second step,
are needed to estimate the prior predictive p-value.
Again a discrepancy measure is deﬁned to contrast the sample and prior
information about the person parameter and to check their compatibility us-
ing terms of the posterior density. Assume the item parameters are known.
Consider a normal prior for the person parameters, p
 θi | µθ, σ2
θ

. In Exer-
cise 5.4, the terms in the exponent of the unnormalized posterior density of
the person parameter are derived. Based on that expansion, the discrepancy
measure
T (zi | θP ) = (K −1)s2
i +

ˆθi −µθ
2
P
k a−2
k
+ σ2
θ
(5.24)
is deﬁned, where s2
i and ˆθi are deﬁned in Equation (5.38) and (5.39), re-
spectively. The corresponding prior predictive p-value for respondent i given
augmented data is deﬁned as
p0(zi,obs) = P
 χ2
K ≥T (zi,obs | θP ) | yi,obs

.
(5.25)

5.4 Predictive Assessment
121
The prior predictive p-value p0(yi,obs) is computed by averaging over prior
predictive p-values based on draws z(m)
i,obs.
The second term in (5.24) is focused on the discrepancy between the data-
dependent least squares estimate and the prior estimate of θi. A predictive
check can be deﬁned to test the hypothesis that the sample and prior informa-
tion are not in conﬂict with each other with respect to the mean. Note that
the posterior distribution of θi combines the sample and prior information
and it is used to construct a shrinkage estimate of θi (Exercise 5.4), which is
a weighted combination of the (least squares) sample and prior estimate of
θi. A large discrepancy between the sample and prior estimate of θi leads to
a severely biased shrinkage estimate since it comprises two diﬀerent thoughts
about the true value of θi. Therefore, deﬁne the function
T

ˆθi −µθ | θP

=

ˆθi −µθ
2
P
k a−2
k
+ σ2
θ
,
(5.26)
and the compatibility of the least squares estimate and the prior estimate is
checked via
P

χ2
1 ≥T

ˆθi −µθ | θP

.
(5.27)
Theil (1963) proposed a comparable test and denoted it as a comparabil-
ity statistic. When the tail-area probability is small, it is concluded that the
model is discredited by the data and that the posterior distribution of θi is
not (closely) centered at the prior mean µθ. Note that the predictive check
in Equation (5.27) can be extended to test simultaneously the compatibility
of the sample and prior information with respect to the mean across all re-
spondents. Similarly, the compatibility between prior and sample information
with respect to the item parameters can be tested.
A general concern about the prior predictive test is its dependence on
the prior distributions. An excellent model with very poor prior distributions
may become suspicious during prior predictive checks. The computation of
prior predictive probabilities requires large sets of sampled values when the
prior distributions are proper but (very) vague. Also, the prior predictive
distribution is improper when using improper prior distributions, and for that
reason the prior predictive checks are restricted to proper prior distributions.
On the other hand, the dependency on the prior distributions makes the
prior predictive checks particularly interesting for testing the compatibility
between prior and sample information. A whole set of statistics may reveal
the ﬁt of the model without having to estimate any parameter, and they
provide speciﬁc insights into the discrepancy between the model and the prior
beliefs. When the prior predictive assessment reveals deﬁciencies, it depends
on the investigator’s belief in the prior distributions if the next step consists
of altering the model or the prior speciﬁcations or both. Speciﬁc tests may
provide useful clues for how to alter the model and/or priors.

122
5 Assessment of Bayesian Item Response Models
5.4.2 Posterior Predictive Assessment
Geisser (1975), Guttman (1967), and Rubin (1984), among others, have fo-
cused on posterior predictive checks where the posterior predictive distribu-
tion of a statistic T(Y) is used to test whether the observed statistic’s value,
T(yobs), appears to be typical. The computation of a p-value is done using
the posterior predictive distribution of the predictive or replicated data Yrep.
In correspondence with the prior predictive distribution of the data, Equation
(5.22), the posterior predictive distribution is deﬁned as
p (yrep | y, M) =
Z Z
p (yrep | y, θ, ξ, M) p (θ, ξ | y, M) dθdξ
=
Z Z
p (yrep | θ, ξ, M) p (θ, ξ | y, M) dθdξ,
(5.28)
where the replicated or predictive data are independent of the observed data
given the ability parameters due to the assumption of local independence. The
distribution in (5.28) is a posterior predictive distribution since it concerns
the distribution of future observables (predictive) and it conditions on ob-
served values (posterior). In a posterior predictive assessment, the posterior
predictive p-value (5.21) is computed using the posterior predictive density
deﬁned in (5.28).
Meng (1994) introduced an extension of the posterior predictive model
check by allowing a statistic to depend on unknown model parameters. For
example, the statistic in Equation (5.21) can be extended to depend on θ,
which leads to a p-value
p0(yobs) = P (T(yrep, θ) ≥T(yobs, θ) | M) ,
(5.29)
where the probability is taken over the joint posterior distribution of (yrep, θ)
under the null hypothesis.
There are two interpretations of the posterior predictive check with a
parameter-dependent discrepancy measure. First, it allows one to measure
directly the discrepancy between sample and unknown primary parameters
θ, where the sampling distribution of the discrepancy measure may depend
on unknown nuisance parameters. Second, the posterior predictive p-value in
(5.29) can be computed for a reference distribution given primary parameters
leading to p0(yobs, θ). In the second stage, the expected value of the p-value
is computed with respect to the marginal posterior distribution of θ, leading
to the p-value deﬁned in (5.29). As a result, the uncertainty in the primary
parameters is taken into account in the computation of the p-value. This un-
certainty is generated by the marginal distribution of the primary parameters.
Consider a data augmentation scheme based on the replicated response
data. In that case, the posterior predictive density is deﬁned as
p (yrep | y) =
Z Z Z
p (zrep | yrep, θ, ξ) p (yrep | θ, ξ) p (θ, ξ | y) dzrepdθdξ.

5.4 Predictive Assessment
123
A forward simulation method can be deﬁned to obtain replicated data samples
from the posterior predictive distribution. The forward simulation method for
generating posterior predictive data requires sampling parameter values from
the posterior distributions and sampling observables from their conditional
distribution given the sampled parameter values. Given population parameters
θP and ξP , a forward simulator is deﬁned by
ξ(m) ∼p (ξ | yobs, ξP ) ,
θ(m) ∼p (θ | yobs, θP ) ,
y(m)
rep ∼p

yrep | θ(m), ξ(m)
,
z(m)
rep ∼p

zrep | y(m)
rep , θ(m), ξ(m)
.
In the ﬁrst two steps, parameter values are sampled from their marginal poste-
rior distributions, which are the common output of an MCMC algorithm. The
posterior predictive tests are easily evaluated within an MCMC algorithm.
The sampling distribution of a discrepancy measure is usually unknown, and
the MCMC algorithm is routinely used to derive a posterior predictive p-value;
that is, the posterior predictive distribution is used to compute the probability
that the discrepancy value obtained is extreme with respect to the observed
value.
A common problem with the computation of posterior predictive p-values
is its double use of the observed data, ﬁrst to compute the posterior distribu-
tion of the model parameters for determining the posterior predictive distri-
bution and second to compute the posterior predictive p-value. This double
use of the data leads to unnatural behavior of the p-value. Bayarri and Berger
(1999, 2000) showed in several examples that the distribution of the p-value
is not uniformly distributed on [0, 1] under the null. This complicates the in-
terpretation and comparison of posterior predictive p-values. The literature
shows two diﬀerent ways of dealing with this problem.
One school of thought treats a posterior predictive p-value as statistical
evidence for model misﬁt instead of using it for testing a speciﬁc null hypothe-
sis (Gelman et al., 1996; Meng, 1994; Stern, 2000). It accepts the conservative
behavior (more closely concentrated around .5) of the posterior predictive p-
value (e.g., Bayarri and Berger, 2000; Sinharay and Stern, 2002). In this light,
the posterior predictive model checks are viewed as diagnostic measures to
evaluate model ﬁt, in which the p-value is considered to be a useful summary.
Attention is also focused on displaying diagnostic diﬀerences graphically. Stern
(2000) noted that the posterior predictive p-value remains the conditional pos-
terior probability of the event T(Y) ≥T(yobs) and therefore provides useful
model checking information with or without an (asymptotic) null distribution.
The other school is focused on establishing p-values on a calibrated scale
(Aitkin, 1997; Bayarri and Berger, 2000). Bayarri and Berger (2000) proposed
the conditional and partial predictive p-values, which are deﬁned in such a way
that the data are not used twice. However, posterior predictive model checking

124
5 Assessment of Bayesian Item Response Models
is easy to perform via simulation-based techniques, but these proposed p-
values are often diﬃcult to compute. The recommended conditional posterior
p-value is based upon the knowledge of a suﬃcient statistic such that the
predictive distribution of a statistic is free from unknown model parameters,
which may be hard to obtain.
Overview of Posterior Predictive Model Checks
Several posterior predictive checks have been developed for testing the ﬁt of
item response models. Statistics were developed to test the assumption of lo-
cal independence (e.g., Hoijtink, 2001; Levy, 2006; Sinharay, 2005), person
ﬁt (Glas and Meijer, 2003), item ﬁt (e.g., Sinharay, 2006), and diﬀerential
item functioning (Hoijtink, 2001), among other things. The advantages of the
proposed posterior predictive checks mentioned are (1) that the theoretical
sampling distribution of the statistic does not need to be derived and (2) the
discrepancy measures may depend on unknown parameters, and the associ-
ated uncertainty is explicitly taken into account in the computation of the
p-value. Note that, in the frequentist framework, ﬁt statistics that account for
uncertainty in estimated item and ability parameters are complex, and several
studies have shown that the theoretical sampling distribution of a statistic is
aﬀected due to plugged-in estimates depending on the degree of uncertainty
(Molenaar and Hoijtink, 1990; Snijders, 2001; Stone and Hansen, 2000).
The assumption of local independence is often tested by investigating the
assumption of weak local independence, which is a necessary but not a suﬃ-
cient condition for (strong) local independence. The assumption of weak local
independence can be stated as Cov(Yik, Yik′ | θi) = 0, which implies that the
item responses to items k and k′ (k ̸= k′) are independent conditional on θi.
The covariance between responses of item pairs can be used as a discrepancy
measure to test local independence,
Cov(Yk, Yk′) =
X
i
 Yik −Y ik
  Yik′ −Y ik′
/N
= (n11n00 −n10n01) /N 2,
(5.30)
where, for example, n11 is the number of subjects responding correctly to
both items. Hoijtink (2001) proposed a discrepancy measure based on squared
conditional covariances given the respondent’s rest score Rkk′,
X
Rkk′
√nRkk′ (Cov (Yik, Yik′ | Rkk′))2 ,
where nRkk′ equals the number of respondents with rest score Rkk′. Note
that the sum of covariances is approximately zero if local independence holds.
Edwards (1963) showed that the cross-product ratio or odds ratio can be taken
as a measure of association when interest is focused on paired attributes with
no ﬁxed marginal totals. That is,

5.4 Predictive Assessment
125
ORkk′ = n11/n10
n01/n00
= n11n00
n10n01
.
(5.31)
The odds ratio is invariant under the interchange of rows and columns and
under row and column multiplications. Other measures of association and
other properties of the odds ratio can be found in Bishop, Fienberg and Hol-
land (1975). Sinharay, Johnson and Stern (2006) found the odds ratio to be a
useful discrepancy measure for testing the assumption of local independence.
Levy (2006) used the odds ratio to detect violations of local independence
but with a speciﬁc interest in detecting a violation of the unidimensionality
assumption due to the multidimensional nature of the observed data. Viola-
tions of local independence may be caused by assessment phenomena such as
diﬀerential item functioning, testlet eﬀects, and rater eﬀects, and they can
be framed in terms of multidimensionality (e.g., Mellenbergh, 1994a; Stout,
Habing, Douglas, Kim, Roussos and Zhang, 1996; Yen, 1993).
Person-ﬁt research is focused on determining the ﬁt of individual response
patterns. Response patterns can be regarded as unexpected, for example due
to cheating, guessing, or plodding. Several person-ﬁt statistics have been pro-
posed for parametric and nonparametric item response models (e.g., Emons,
Sijtsma and Meijer, 2005; Meijer and Sijtsma, 2001; Meijer, 2003). Glas and
Meijer (2003) used most common person-ﬁt tests as discrepancy measures
and investigated the properties of the associated posterior predictive person-
ﬁt tests for binary response data. The various tests were used to detect the
rate of guessing, item disclosure, and violations of local independence. With a
simulation study, they showed that the detection rates for guessing and item
disclosure were higher than for violations against local independence and con-
cluded, that even for small sample sizes, accurate Type I error values were
obtained. Across conditions, a discrepancy test from Tatsuoka (1984) had the
highest power. This discrepancy measure takes on a large positive value for
response patterns with correct answers to diﬃcult items and incorrect answers
to easy items. The nonstandardized version of the indices is given by
X
k
(Pik(θi) −Yik) (Pik(θi) −Yi/K) ,
(5.32)
where Pik(θi) is the probability of a correct response to item k of a person in-
dexed i and Yi the number of correct responses. The theoretical distribution of
Tatsuoka’s standardized statistic is unknown, and often a normal distribution
is assumed. As said, within the posterior predictive framework, the theoretical
sampling distribution of the statistic does not need to be known explicitly.
Sinharay (2006) showed that the item-ﬁt indices of Orlando and Thissen
(2000, 2003) are promising discrepancy measures for unidimensional item re-
sponse models for binary data. The ﬁt indices are based on comparisons be-
tween the observed and expected proportions of correct scores for diﬀerent
examinee groups. Within a simulation study, it was shown that they have
acceptable Type I error rates and considerable power for moderate and large
sample sizes.

126
5 Assessment of Bayesian Item Response Models
Two additional posterior predictive checks are based on weighted and un-
weighted mean squares. B´eguin and Glas (2001) proposed a discrepancy mea-
sure for overall model ﬁt to compare the observed score distribution with the
posterior predictive score distribution. The discrepancy measure compares the
expected, E(nr), and observed numbers of correct scores, nr; that is,
X2 =
X
r
nr −E(nr)
E(nr)
2
,
(5.33)
where r = 0, 1, . . . , K. It is known that X2 is not chi-square distributed, and
the posterior predictive p-value is computed using samples from the poste-
rior predictive distribution. Wright (1977) and Masters and Wright (1997)
proposed so-called outﬁt and inﬁt statistics without knowing the exact dis-
tributional properties. The outﬁt statistic is the unweighted mean squares of
standardized residuals, where each residual is deﬁned as the diﬀerence be-
tween the observed and the expected response to an item. The inﬁt statistic
is the weighted mean squares, where the weights are deﬁned by the residual
variances. This statistic can be computed for each person and each item. Both
statistics summarize the variation between the observed response patterns and
the expected response patterns under the model. The inﬁt statistic is less sen-
sitive to outliers since the inﬂuence of outliers is reduced by the weighting
factor. The distributional properties of both statistics do need to be known
explicitly when using them as posterior predictive checks (see Exercise 5.6).
5.5 Illustrations of Predictive Assessment
5.5.1 The Observed Score Distribution
Consider the CAPS response data in Section 5.3.1. The ﬁt of the graded
response model is investigated by comparing the observed score distribution
with the posterior predictive score distribution. The discrepancy measure in
Equation (5.33) is used to evaluate the extremeness of the observed score
distribution where r ranges from r = 13 to r = 65 (13 items with response
categories ranging from 1 to 5). The posterior predictive p-value equals
p0 (yobs) = P
 X2 (yrep) ≥X2 (yobs)

.
The corresponding posterior predictive p-value equals .039, which indicates
that the observed score distribution is more extreme than expected under
the model. This follows from the fact that the observed scores indicate a
highly skewed (to the right) observed score distribution. A lot of respondents
score relatively low, with 25% scoring between 13 and 14.5 (the ﬁrst quartile)
and 25% scoring between 23 and 52 (third quartile), with the median equal
to 19. The discrepancy measure can also be computed for each single score
level, which leads to a posterior predictive p-value for each of them. The

5.5 Illustrations of Predictive Assessment
127
computations can be done in the same way. As expected, the p-values indicate
that the observed score levels at the lower and higher ends are more extreme
than expected under the model.
5.5.2 Detecting Testlet Eﬀects
A Short Introduction to Testlet Eﬀects
Rosenbaum (1988) argued that the assumption of local independence might be
violated when there is a dependence within bundles of items that explicitly
share material. This can be, for example, a reading passage. Respondents
that have diﬃculties with certain words or sentences in the passage have
greater diﬃculty with the entire bundle of items associated with the passage.
It is also possible that items exhibit dependence without explicitly sharing
material. Rosenbaum (1988) proved that the assumption of unidimensionality
may hold between item bundles given the loss of local independence within
the item bundles. As a result, the assumption of unidimensionality between
item bundles holds when the assumption of conditional independence between
item bundles holds. A unidimensional item response model is still appropriate
when using the item bundle as the unit of measurement.
Bradlow et al. (1999) introduced a testlet parameter to model the addi-
tional dependence between items within the same item bundle, also called a
testlet (for a more general overview of testlet models, see Wainer, Bradlow and
Wang, 2007). Let lk indicate the testlet of item k and l indicate a testlet, and
assume that there are L testlets in total (l, lk ∈1, . . . , L). Then, let parameter
υi,l denote a person-speciﬁc testlet eﬀect that is independent of person pa-
rameters and item parameters. The testlet parameter, υi,l, is considered to be
a normally distributed random eﬀects parameter with mean zero and variance
σ2
υl. This speciﬁcation allows for a testlet-speciﬁc variance parameter, and the
testlet parameters are assumed to be independent from each other.
The random eﬀects (testlet) parameter models the additional dependence
between the individual responses to items within a testlet. To see this, assume
binary response data, Yik, that are augmented by normally distributed latent
continuous response data, Zik, with mean akθi −bk −υi,lk and variance one,
and Yik is the indicator that Zik is positive. The probability that person i
responds correctly to item k is deﬁned by
P (Yik = 1 | θi, ξk, υi,lk) = P (Zik > 0 | θi, ξk, υi,lk)
=
Z ∞
0
φ (x; akθi −bk −υi,lk) dx
= Φ (akθi −bk −υi,lk) .
(5.34)
Thus the normal ogive model (see Equation (4.5)) is adjusted with a random
eﬀects parameter to account for a testlet eﬀect. The sign of this testlet pa-
rameter leads to a higher (negative sign) or a lower (positive sign) success

128
5 Assessment of Bayesian Item Response Models
probability. A respondent that scores well (poorly) on items in a particular
testlet has higher (lower) success probabilities than the respondent’s ability
level would suggest, and this additional eﬀect is implemented by a negative
(positive) testlet parameter value. Now, for ﬁxed item parameter values and
k ̸= k′, it follows that
Cov (Zik, Zik′ | ξk, ξk′) = Cov (akθi, ak′θi) + Cov
 υi,lk, υi,lk′

=
 akak′σ2
θ + σ2
υlk if lk = lk′
akak′σ2
θ
if lk ̸= lk′
(5.35)
due to the various assumptions of independence. It can be concluded that
when the items k and k′ belong to the same testlet the additional dependence
between the corresponding observations is captured by the testlet parameters.
An MCMC algorithm for estimating the two-parameter item response model
with testlet parameters can be constructed from MCMC scheme 2. This re-
quires adding the steps for sampling testlet and testlet variance parameters
from their conditional posterior distributions and adjusting a few original
steps (see Exercise 5.7).
Simulation Study
Data are generated according to a normal ogive testlet model, Equation (5.34),
with N = 1,000 and K = 30. The items are grouped by six testlets, each
with ﬁve items. A common variance is assumed for the testlet parameters,
σ2
υl = .20. An exchangeable hierarchical prior density is speciﬁed for the item
parameters,
(log ak, bk)t ∼N (µξ, Σξ) ,
Σξ ∼IW(2, Σ0),
µξ | Σξ ∼N(0, Σξ/2),
and an exchangeable hierarchical prior for the testlet parameters
υi,l ∼N(0, σ2
υl),
σ2
υl ∼IG (3, 1) .
The parameters of a two-parameter normal ogive model are estimated,
given the data generated via the normal ogive testlet model, using MCMC
scheme 2. Violations of local independence are to be expected due to the addi-
tional dependence between individual responses to items in the same testlet.
The odds ratio (Equation (5.31)) is used as a predictive check to test for
violations of local independence. Replicated data are sampled from the prior
predictive distribution (Equation (5.22)) and the posterior predictive distribu-
tion (Equation (5.28)) under the two-parameter normal ogive model. The odds

5.5 Illustrations of Predictive Assessment
129
10
20
30
40
50
60
Testlets
0.0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
P-values
Prior Predictive
-value
P
Posterior Predictive
-value
P
Testlet 1
Testlet 4
Testlet 5
Testlet 6
Testlet 3
Testlet 2
Fig. 5.2. Detecting violations of local independence: Prior and posterior predictive
p-values related to item pairs where both items are nested in the same testlet.
ratio is evaluated as a prior predictive check by computing the prior predic-
tive p-value for each item pair using replicated data from the prior predictive
distribution of the data. A posterior predictive p-value is also computed for
each item pair using replicated data from the posterior predictive distribution
given the observed data.
In Figure 5.2, the estimated prior and posterior predictive p-values are
plotted for item pairs where violations of local independence are to be ex-
pected. More speciﬁcally, the ﬁrst ﬁve items are nested in the ﬁrst testlet.
Then, a total of 10 item-pairs are of interest: item 1 with 2, 3, 4, and 5; item
2 with 3, 4, and 5; item 3 with 4 and 5; and item 4 with 5. The estimated
p-values are plotted in the same order. Each testlet consists of ﬁve items, and
p-values are computed for the 10 item pairs.
It can be seen that each plotted prior predictive p-value is smaller than the
corresponding posterior predictive p-value. This follows from the fact that, in
general, the posterior predictive p-value is conservative and tends to 0.5 (for
more details, see Robins, van der Vaart and Ventura, 2000). Subsequently,
they are not exactly uniformly distributed, which makes it more diﬃcult to say
whether they are extreme or not. The prior predictive p-values are uniformly
distributed and show more power in detecting violations of local independence.
With a signiﬁcance level of .05, 85% of the prior predictive p-values show
a signiﬁcant violation of local independence. For the posterior predictive p-
values, 72% of the p-values detect a signiﬁcant violation of local independence.

130
5 Assessment of Bayesian Item Response Models
The p-values corresponding to item-pairs of items nested in diﬀerent testlets
should not show a signiﬁcant violation of local independence. For this case,
99.7% of the posterior predictive p-values and 99.5% of the prior predictive
p-values did not indicate a violation of local independence when violations
were not to be expected. It can be concluded that the false-alarm rates are
approximately the same but the detection rates diﬀer (when violations of local
independence are expected).
5.6 Model Comparison and Information Criteria
To compare diﬀerent item response models ﬁtted to the same data, the DIC
can be used as a summary measure of ﬁt. A general discussion of the DIC is
given in Section 3.2.3.
The DIC of the two-parameter normal ogive model is described in more
detail. Consider the deviance function D (θ, ξ), which is based on the log-
likelihood of the data given item and person parameters. This deviance func-
tion can be stated as
D (θ, ξ) = −2 log p (y | θ, ξ)
= −2 log
Y
i,k
h
Φ (akθi −bk)yik (1 −Φ (akθi −bk))1−yiki
= −2
X
i,k
h
yik log Φ (ηik) + (1 −yik) log (1 −Φ (ηik))
i
,
where ηik = akθi −bk.
Assume MCMC samples θ(m) and ξ(m) from the marginal posterior den-
sities. Then, the posterior mean of the deviance is estimated by
D (θ, ξ) ≈
X
m
D

θ(m), ξ(m).
M.
The deviance of the posterior means is estimated by substituting estimated
posterior means for the item and person parameters. The penalty term pD,
the eﬀective number of parameters, is easily evaluated as the posterior mean
deviance minus the deviance of the posterior means.
The log-likelihood is known explicitly, which makes computing the marginal
likelihood of the data easier when using MCMC samples (see Section 3.2.1).
Subsequently, a Bayes factor can be used for model comparison. Spiegelhalter
et al. (2002) argued that the Bayes factor and the DIC have diﬀerent purposes.
The Bayes factor summarizes how well priors have predicted the observed
data, whereas the DIC summarizes how well the posterior density predicts fu-
ture data generated by the same process as that which generated the obtained
data. The DIC has a posterior predictive approach towards model selection,
whereas the Bayes factor has a prior predictive approach. Nonetheless, in the
next example, both methods are used to compare diﬀerent response models
ﬁtted to the same data.

5.7 Summary and Conclusions
131
5.6.1 Dutch Math Data: Model Comparison
The use of the DIC is illustrated by comparing the ﬁts of diﬀerent response
models. Therefore, the Dutch primary school math test data are considered
(see Section 5.2.4). The ﬁts of four item response models are investigated,
where M1 and M2 denote the one-parameter and two-parameter normal ogive
models, respectively. Let M3 and M4 denote the one-parameter and two-
parameter logistic models, respectively. For all models, a standard normal
prior is speciﬁed for the ability parameters. For the two-parameter models, a
hierarchical prior is assumed for the item parameters. For the one-parameter
models, a normal inverse gamma prior for the diﬃculty parameters is assumed.
Table 5.2 shows the posterior mean deviance, the deviance of the poste-
rior means, the eﬀective number of parameters, the DIC, and the marginal
log-likelihood for each model. It follows from the estimated DICs that the
two-parameter normal ogive model ﬁts the data best. Furthermore, the two-
parameter models are preferred over the one-parameter models, and the nor-
mal ogive models over the logistic models.
When looking at the estimated posterior mean deviances, it is remarkable
that the one-parameter normal ogive model has the smallest posterior mean
deviance. The corresponding DIC is relatively high due to the fact that the
estimated eﬀective number of parameters is considerably greater than that of
the two-parameter models. In this case, the improvement of ﬁt of the two-
parameter models is enhanced by the reduction in the eﬀective number of pa-
rameters and the DIC reduced. For the one-parameter models, the restriction
on the item discrimination parameters leads to less pooled ability parameter
estimates and a high number of eﬀective parameters. That is, the ability pa-
rameter estimates are less pooled towards the prior mean in comparison with
the estimates under the two-parameter model. The more complex modeling
structure of the two-parameter model leads to a decrease in the number of
eﬀective parameters, ability estimates are shrunk towards the prior mean, and
variability in the item discriminations is supported.
For each model, the estimated marginal log-likelihood is given in the last
column of Table 5.2. A Bayes factor is used for model comparison. It follows
that the two-parameter normal ogive model M2 is preferred over the one-
parameter model M1 since
ˆ
BF = exp((−18,529) −(−18,532)) = exp(3). In
the same way, it can be veriﬁed that the Bayes factor supports the conclusions
that were made via the DIC.
5.7 Summary and Conclusions
The topic of model ﬁtting is an active area of research with many diverse new
methods. It is not possible to summarize all these methods in one chapter.
In this chapter, attention was focused on Bayesian residual analysis, predic-
tive assessment, and model comparison. Speciﬁcally, posterior predictive as-
sessment constitutes a testing framework with many possibilities, but more

132
5 Assessment of Bayesian Item Response Models
Table 5.2. Dutch math test: A model comparison.
Model
D(θ, ξ) D(ˆθ, ˆξ)
pD
DIC
log p (y | M)
Normal Ogive M1 37,062 35,168 1,894 38,956
−18,532
M2 37,058 35,340 1,718 38,776
−18,529
Logistic
M3 37,104 35,220 1,884 38,989
−18,553
M4 37,081 35,366 1,715 38,797
−18,539
research is needed to fully explore the possibilities of parameter-dependent
discrepancy functions with known or unknown sampling distributions for as-
sessing the ﬁt of item response models.
Model criticism and selection is often focused on assessing the adequacy
of a model in predicting the outcome of individual data points and summa-
rizing the ﬁt of the model as a whole. Carlin and Louis (1996), Gelfand, Dey
and Chang (1992), and others advocated the use of cross-validation where
the ﬁtted value of an observation (or set of observations) is evaluated given
all data except the observed value (set of observations). The posterior mean
and variance are computed in relation to the conditional predictive distribu-
tion, also called the conditional predictive ordinate, which is the likelihood of
each point given the rest of the data. Data points with low conditional pre-
dictive ordinates are not ﬁtted well by the model. The conditional predictive
distribution is often unknown and must be evaluated analytically, which can
be time-consuming. An illustration of evaluating the ﬁt of an item response
model via cross-validation can be found in Fox (2005b).
The general ﬁt of a model can also be examined via a model comparison
approach. The adequacy of a baseline item response model can be evaluated
by comparing it with competing models. However, as argued by Embretson
and Reise (2000, p. 246), in straightforward item response analyses, the re-
searcher’s choices are already restricted to the standard models. They also
remarked that, given suﬃcient response (unidimensional) data, the diﬀerences
between competing models are probably very small. In subsequent chapters,
more complex item response models will be discussed and more attention will
be paid to the possibilities of evaluating a baseline model via Bayesian model
choice methods as summarized in Section 3.2.3. In contrast to this, Bayesian
item response modeling involves more issues than just choosing a likelihood
model as in the frequentist framework. Prior and hyperprior distributions need
to be speciﬁed, together with a likelihood model for the sample data, and they
constitute the model. This complicates the Bayesian model assessment. Prior
distributions may inﬂuence the results, and it is important to detect conﬂicts
between prior and sample information. Also, the ﬁt of a model can be im-
proved by adjusting the prior distributions, a topic that will also be discussed
in later chapters.

5.8 Exercises
133
Finally, there is no standard procedure for evaluating the ﬁt, and the
variety of tests do not lead to a deﬁnite conclusion that a model does or
does not ﬁt the data. The outcomes of diﬀerent test procedures need to be
combined to come to a judgment about the ﬁt of the model, and it is up to
the researcher to make a proper judgment.
5.8 Exercises
5.1. Consider data augmentation scheme 4.7, and the object is to construct
a Rao-Blackwellized estimator for the Bayesian latent residual in Equation
(5.2).
(a) Verify that
P (Yik = 1 | θi, ξk) = P (Zik > 0 | θi, ξk) = Φ (akθi −bk) .
(b) For Yik = 1, verify that the conditional expected value of εik given θi and
ξk equals
E (εik | Yik = 1, θi, ξk) =
Z ∞
0
εikp (zik | θi, ξk, Yik = 1) dzik
=
Z ∞
0
εik
p (zik | θi, ξk)
P (Yik = 1 | θi, ξk)dzik,
and derive the result in Equation (5.4).
(c) For Yik = 0, show that the conditional expected value of εik given θi and
ξk is given by Equation (5.5).
(d) Derive a closed expression for the conditional variance of εik; that is,
Var (εik | yik, θi, ξk) =
Z ∞
0
(εik −ε(yik))2 p (zik | θi, ξk, yik) dzik,
where ε(yik) = E (εik | θi, ξk, yik). For Yik = 1, show that the conditional
variance equals
1 −φ (−ηik)
Φ (ηik)

ηik + φ(−ηik)
Φ(ηik)

,
and, for Yik = 0, show that the conditional variance equals
1 −
φ (−ηik)
1 −Φ (ηik)
 φ(−ηik)
1 −Φ(ηik) −ηik

,
where ηik = akθi −bk.
(f) Construct a Rao-Blackwellized estimate of the variance of the point esti-
mate deﬁned in Equations (5.4) and (5.5).

134
5 Assessment of Bayesian Item Response Models
5.2. The object is to ﬁnd Rao-Blackwellized estimates of outlying probabili-
ties, as deﬁned in Equations (5.7) and (5.8), by integrating out the augmented
response data.
(a) Let Zik be normally distributed according to Equation (4.7). Derive the
conditional outlying probability of observation ik; that is, show that
P (|εik| > q | Yik, θi, ak, bk) = P (q < εik ≤−q | θi, ak, bk)
P (Yik | θi, ak, bk)
=
(
(1−Φ(q))
Φ(akθi−bk) if Yik = 1
(1−Φ(q))
Φ(bk−akθi) if Yik = 0,
(5.36)
with the restriction that q > bk −akθi when Yik = 1 and q > akθi −bk when
Yik = 0.
(b) Let Zik be logistically distributed according to Equation (4.7). Show that
the outlying probability of observation ik equals
P (|εik| > q | Yik, θi, ak, bk) =
(
(1−Ψ(q))
Ψ(akθi−bk) if Yik = 1
(1−Ψ(q))
Ψ(bk−akθi) if Yik = 0,
(5.37)
with the restriction that q > bk −akθi when Yik = 1 and q > akθi −bk when
Yik = 0.
5.3. Consider the examinees’ test result data in Section 1.4. An implementa-
tion of the normal ogive model via a latent variable speciﬁcation is shown in
Listing 5.2 based on the data augmentation step in Equation (4.7).
Listing 5.2. WinBUGS code: Latent variable speciﬁcation of the normal ogive item
response model.
model{
for ( i
in
1 :N){
for ( k
in
1 :K){
eta [ i , k ] <−a [ k ] ∗theta [ i ]−b [ k ]
Z [ i , k ]
˜ dnorm( eta [ i , k ] , 1 )
P[ i , k ] <−step (Z [ i , k ] )
Y[ i , k ]
˜ dbern(P[ i , k ] )
}
theta [ i ]
˜ dnorm( 0 , 1 )
}
}
(a) Estimate the Bayesian residuals and plot them against the ﬁtted proba-
bilities. Indicate observations that can be marked as outliers.
(b) Estimate the Bayesian latent residuals simply as by-products of the
MCMC algorithm and plot them against the ﬁtted probabilities. Explain the
diﬀerences with the plot of (a).
(c) Compute Rao-Blackwellized estimates of the Bayesian latent residuals,
Equations (5.4) and (5.5). Plot the estimated Bayesian latent residuals against
the estimates of (b) and explain the diﬀerences.

5.8 Exercises
135
(d) Compute Rao-Blackwellized estimates of the outlying probabilities and
compare the potential outliers detected with the results from (a).
5.4. The purpose is to derive the prior predictive checks in Equations (5.25)
and (5.27). Assume normally distributed augmented response data, Zik, with
mean akθi−bk and variance one. The augmented response data given observed
binary responses are conditionally distributed according to Equation (4.7).
(a) Consider the augmented data likelihood p (zi | θ, ξk), expand the exponent
with ak(ˆθi −ˆθi), and show that the likelihood is proportional to
p (zi | θ, ξk) ∝exp
"
−1
2
 
(K −1)s2
i +
X
k
a2
k

ˆθi −θi
2
!#
,
where
ˆθi =
X
k
a−2
k
X
k
akzik,
(5.38)
s2
i = (K −1)−1 X
k

zik −

akˆθi −bk
2
.
(5.39)
(b) Prove the following identity by completing the squares
ϕ1(θ −θ1)2 + ϕ2(θ −θ2)2 = (ϕ1 + ϕ2)(θ −θ∗)2 + (θ1 −θ2)2
ϕ−1
1
+ ϕ−1
2
,
where
θ∗= (ϕ1θ1 + ϕ2θ2)
ϕ1 + ϕ2
.
(c) Assume a normal prior for θi (Equation (2.8)). Show that the unnormalized
posterior p(zi, θi | ξk, θP ) = p(zi | θi, ξk)p(θi | θP ) is proportional to
exp

−1
2


(K −1)s2
i +
 X
k
a2
k + σ−2
θ
!
(θi −θ∗
i )2 +

ˆθi −µθ
2
P
k a−2
k
+ σ2
θ




,
where
θ∗
i =
P
k a2
kˆθi + µθ/σ2
θ
P
k a2
k + σ−2
θ
.
(d) Derive the conditional posterior distribution of θi using the result of (c).
(e) Derive the conditional predictive distribution of Zi given (ξk, θP ) via
p (zi | ξk, θP ) = p (zi, θi | ξk, θP )
p (θi | zi, ξk, θP ).

136
5 Assessment of Bayesian Item Response Models
(f) Derive the distribution of
T

ˆθi −µθ | ξk, θP

=

ˆθi −µθ
2
P
k a−2
k
+ σ2
θ
.
5.5. Let observed ordinal data be described by a graded response model
with prior distributions as deﬁned in MCMC scheme 3. Assume normally
distributed augmented data according to Equation (4.32).
(a) Prove that
P (ak ≥1 | y) =
Z ∞
1
Z
p (ak | z) p (z | y) dzdak
=
Z ∞
1
p (ak | y) dak.
(b) Derive the closed-form expression given by Equation (5.15), and show how
MCMC output can be used to estimate the corresponding marginal probabil-
ity.
(c) Consider the quantity Qa(ak) in Equation (5.17) with conditional poste-
rior density p (Qa | z). Show that the posterior probability of Qa (ak | y) ≥
Qa
 a0
k | y

can be expressed as
1 −P
 Qa
 a0
k | y

| y

=
Z Z ∞
Qa(a0
k)
p(Qa | z)p(z | y)dQadz,
suppressing the conditioning on the other parameters.
(d) Show how MCMC output can be used to estimate the posterior probability
with and without the fact that the Qa(ak) is chi-square distributed given
augmented data z.
5.6. Binary response data are modeled with the normal ogive item response
model. Assume the item parameters are known. The outﬁt person statistic of
Wright (1977) is given by
T(Yi, θi) = K−1 X
k
 
Yik −Pik(θi)
p
(Pik(θi)(1 −Pik(θi)))
!2
.
(5.40)
(a) Consider the outﬁt statistic T(Yi, θi) as a discrepancy measure, and deﬁne
the corresponding posterior predictive p-value.
(b) Show that the outﬁt person statistic based on Bayesian latent residuals
given normally distributed augmented data Zi, Equation (4.7), is given by
T(Zi, θi) = K−1 X
k
(Zik −(akθi −bk))2 .
(c) Substantiate that the conditional posterior predictive p-value given Zi and
θi equals

5.8 Exercises
137
p0 (Zi, θi) = P
 χ2
K ≥K T (Zi, θi) | Zi, θi

,
(5.41)
and show how to compute the posterior predictive p-value using MCMC out-
put.
(d) In the same way as in (b) and (c), derive the outﬁt item statistic based
on Bayesian latent residuals.
(e) Argue that the posterior predictive tests via Bayesian residuals and
Bayesian latent residuals are diﬀerent tests and will lead to diﬀerent out-
comes.
5.7. Assume that response data are distributed according to the normal ogive
testlet model, Equation (5.34). The object is to develop an MCMC scheme for
estimating the model parameters by extending MCMC scheme 2 with steps
for sampling testlet parameters υi,l and σ2
υl.
(a) Show that the full conditional distribution of υi,l is normal with mean and
variance
E
 υi,l | zi, ξ, θi, σ2
υl

= −P
k:lk=l (zik −(akθi −bk))
nl + σ−2
υl
,
Var
 υi,l | zi, ξ, θi, σ2
υl

=
 nl + σ−2
υl
−1 ,
where nl = P
k I (lk = l) is the number of items in testlet l.
(b) Given an inverse gamma prior IG(g1, g2) for σ2
υl, show that the full condi-
tional distribution of σ2
υl is an inverse gamma with shape parameter g1 + n/2
and scale parameter g2 + P
i υ2
i,l/2.
(c) Show how to adjust MCMC scheme 2 to obtain a scheme for estimating
the normal ogive testlet model.
5.8. A Bayesian residual can be deﬁned for polytomous item response data
as the diﬀerence between the observed and the expected responses. Following
Masters and Wright (1997), deﬁne the expected response and variance as
µrik = E (Yik | θi, ξk) =
Ck
X
c=1
cP (Yik = c | θi, ξk) ,
σ2
rik = Var (Yik | θi, ξk) =
Ck
X
c=1
(c −µrik)2 P (Yik = c | θi, ξk) ,
and, subsequently, the standardized Bayesian residual as
Rik = (Yik −µrik) /σrik.
(a) Reason that the sum of squared standardized Bayesian residuals Qk(y) =
P
k R2
ik and Qi(y) = P
i R2
ik can be used as a measure to detect item and
person misﬁts, respectively.
(b) Deﬁne posterior predictive checks using the parameter-dependent discrep-
ancy functions in (a).

138
5 Assessment of Bayesian Item Response Models
(c) Compare the posterior predictive tests constructed with the posterior pre-
dictive tests in Equations (5.11) and (5.13) and explain the diﬀerences.
(d) Let rk(θ, y) denote the vector of residuals for item k given θ; that is,
rk (θ, y) =
Z
rk(ξ, θ, y)p (ξ | y) dξ.
Let σrk,k′ denote the covariance between two vectors of residuals correspond-
ing to items k and k′. Construct a test for local independence based on the
covariance term σrk,k′ , and show how to use an MCMC algorithm to perform
the computations.
5.9. The examinees’ test result data in Section 1.4 are examined. Posterior
predictive tests are performed in WinBUGS to detect person misﬁt.
(a) Extend Listing 1.1 with code from Listing 5.3 to simulate posterior pre-
dictive response data. Argue that posterior predictive data are sampled.
Listing 5.3. WinBUGS code: Simulating posterior predictive data.
theta . ppf <−cut ( theta )
a . ppf <−cut ( a )
b . ppf <−cut (b)
for ( i
in
1 :N){
for ( k
in
1 :K){
P[ i , k ] <−phi ( a . ppf [ k ] ∗theta . ppf [ i ]−b . ppf [ k ] )
rep [ i , k ]
˜ dbern(P[ i , k ] )
}
}
(b) Implement and evaluate Wright’s outﬁt person statistic (Equation (5.40)).
(c) Explain detected misﬁts by examining the corresponding response pat-
terns.
(d) Implement and evaluate Tatsuoka’s nonstandardized person statistic
(Equation (5.32)).
(e) Evaluate both statistics under the two-parameter logistic model, and com-
pare the results with (c) and (d).

5.9 Appendix: CAPS Questionnaire
139
5.9 Appendix: CAPS Questionnaire
How often (almost always (5), often (4), sometimes (3), seldom (2), almost
never (1)) have you had any of the following problems over the past year as a
result of drinking too much alcohol?
1. Feeling sad, blue, or depressed.
2. Nervousness or irritability.
3. Hurt another person emotionally.
4. Family problems related to your drinking.
5. Spent too much money on drugs.
6. Badly aﬀected friendship or relationship.
7. Hurt another person physically.
8. Caused others to criticize your behavior.
9. Nausea or vomiting.
10. Drove under the inﬂuence.
11. Spent too much money on alcohol.
12. Feeling tired or hung over.
13. Illegal activities associated with drug use.

6
Multilevel Item Response Theory Models
The item response data structure is hierarchical since item responses are
nested within respondents. Often respondents are also grouped into larger
units and variables are available that characterize the respondents and the
higher-level units. An item response modeling framework is discussed that
includes a multilevel population model for the respondents and takes such a
hierarchical data structure into account. An important application area is in
education, where response observations are grouped in students and students
grouped in schools. Several school eﬀectiveness research studies are discussed.
The hierarchical item response model is extended in several directions to han-
dle latent explanatory variables, model latent individual growth, and identify
clusters of respondents.
6.1 Introduction: School Eﬀectiveness Research
School eﬀectiveness research is concerned with exploring diﬀerences within
and between schools. The objective is to investigate the relationship between
explanatory and outcome factors. This involves choosing an outcome variable,
such as student’s ability, and studying diﬀerences among schools after adjust-
ing for relevant background variables. Interest is focused on the relative size of
school diﬀerences and the factors that explain these diﬀerences and inﬂuence
student learning.
Typically, in school eﬀectiveness research, students are nested in class-
generally acceptable statistical model in the assessment of school eﬀective-
ness therefore requires the deployment of multilevel analysis techniques. A
multilevel model takes the hierarchical structure into account, and variance
components are modeled at each sampling level. As a result, homogeneity of
results of individual pupils in the same school is accounted for since pupils in
the same school share common experiences. Speciﬁcally, a multilevel model
©
141
and Behavioral Sciences, DOI 10.1007/978-1-4419-0742-4_6,
J.-P. Fox, Bayesian Item Response Modeling: Theory and Applications, Statistics for Social
  Springer Science+Business Media, LLC 2010
rooms, classrooms in schools, schools within school systems, and so on. A

142
6 Multilevel Item Response Theory Models
describes relationships between one or more “outcome” variables (examina-
tion results, attitudes), school and teacher characteristics (teacher’s attitude,
ﬁnancial resources, class size), and student characteristics (achievements, so-
cial background). In the study of school eﬀectiveness research, multilevel mod-
eling has become so conspicuous since it allows for the analysis of individual
and group-level eﬀects and cross-level interactions.
The appropriateness of multilevel models in the assessment of school eﬀec-
tiveness was shown by Aitkin and Longford (1986). Since that time, most of
the research has focused on multilevel modeling of hierarchically structured
educational data and the assessment of relevant input and output indicators
(e.g., Goldstein, 2003; Longford, 1993; Raudenbush and Bryk, 2002).
6.2 Nonlinear Mixed Eﬀects Models
Two approaches for analyzing variables from diﬀerent levels at one single
level have been criticized. The ﬁrst disaggregates all higher-order variables
to the individual level. That is, data from higher levels are assigned to a
much larger number of units at level 1. All disaggregated values are assumed
to be independent of each other, which is a misspeciﬁcation that threatens
the validity of the inferences. In the second approach, observations at level 1
are aggregated to the higher level. As a result, all within-group information
is lost. Relations between aggregated variables can be much stronger and
diﬀerent from the relations between nonaggregated variables. Snijders and
Bosker (1999) give a complete overview of potential (statistical) errors when
the clustered structure of the data is ignored.
Statistical models under a broad variety of names have been developed that
can handle the diﬀerent levels of the data. The models capture the between-
and within-subject variances by modeling the data in two stages. At the ﬁrst
stage, a regression function is speciﬁed for the observations for each subject.
Each subject has its own regression function. The same covariates are used
across subjects, but the regression coeﬃcients are allowed to vary. At the
second stage, the regression coeﬃcients that are allowed to vary are considered
to be random outcome variables. These random outcome variables are referred
to as random (regression) eﬀects or random coeﬃcients. This explains the
term random coeﬃcient models that is often used in sociological research and
econometrics (De Leeuw and Kreft, 1986; Longford, 1993). However, there are
a variety of names in the literature to describe versions of the same model. In
educational and sociological research, the name multilevel model is often used
(Goldstein, 2003; Snijders and Bosker, 1999). In biometric research, mixed
eﬀects model or random eﬀects model are common terms (Laird and Ware,
1982; Hedeker and Gibbons, 2006; Longford, 1987). Other common names
are variance component models (Dempster, Rubin and Tsutakawa, 1981) and
hierarchical linear models (Raudenbush and Bryk, 2002).

6.2 Nonlinear Mixed Eﬀects Models
143
In a nonlinear mixed eﬀects model, the related regression function de-
pends nonlinearly on ﬁxed and random eﬀects parameters. This is in contrast
to a linear mixed eﬀects model, where the regression function is a linear com-
bination of ﬁxed and random eﬀects parameters. The integrated likelihood
function of the nonlinear mixed eﬀects model does not have a closed-form
expression, which complicates the estimation algorithms. Nonlinear (mixed
eﬀects) models are referred to as generalized linear (mixed eﬀects) models
when the observations are distributed according to an exponential family dis-
tribution and when the predictor can be expressed as a linear term.
Typical for mixed eﬀects regression models is the incorporation of ran-
dom eﬀects parameter(s) in regression models that account for dependencies
between level-1 observations. The correlation structure of the level-1 observa-
tions is described by the random eﬀects parameters, and the observations are
assumed to be conditionally independent given the random eﬀects parame-
ters. The random eﬀects parameters are assumed to be distributed according
to a common population distribution.
In this light, an item response model can be recognized as a nonlinear
mixed eﬀects model. A characteristic element of item response data is that the
observations from each respondent are not mutually independent. The indi-
vidual’s responses share a common underlying ability parameter and therefore
the responses are said to be dependent. That is, the item responses are nested
within subjects, which leads to a two-level structure. Note that the local in-
dependence assumption states that the observations are (only) conditionally
independent given the ability parameter.
In the nonlinear mixed eﬀects framework, the two-parameter model is
stated as
h (E (Yik | θi, bk)) = αkθi −bk
(6.1)
where h(.) is the so-called link function and equals the probit function Φ(.)−1
or the logit function Ψ(.)−1. It can be seen that the success probabilities are
transformed such that (1) they are linearly related to the term akθi −bk and
(2) the transformed success probabilities are mapped from the unit interval
onto the whole real line. Given ability parameters, the model is a member of
the class of generalized linear models (McCullagh and Nelder, 1989).
Let the ability parameters be normally distributed with unknown mean
and variance. Then, a nonlinear mixed eﬀects model is deﬁned where the cor-
relation structure of the (level-1) item responses is described by the random
person parameters. Note that the model in terms of the underlying latent
variable (Equation (4.7)) can also be recognized as a nonlinear mixed eﬀects
model. The right-hand side of Equation (4.38) contains a product of param-
eters, and the model is therefore not within the class of generalized linear
mixed eﬀects models. The Rasch model can be recognized as a generalized
linear random intercept model.
Liu and Hedeker (2006) showed that within this framework the parameters
can be estimated via marginal maximum likelihood estimation. The likelihood

144
6 Multilevel Item Response Theory Models
equations are solved via multidimensional Gauss-Hermite quadrature. Liu and
Hedeker mentioned that three levels of random eﬀects can be handled and that
the number of level-3 random eﬀects is limited to three or four when Gauss-
Hermite quadrature is used to numerically integrate over the distribution of
random eﬀects. The nonlinear mixed eﬀects modeling framework, Equation
(6.1), requires speciﬁc priors for the item parameters, and this limits its use-
fulness in more complex settings.
An intraclass correlation coeﬃcient is easily computed as the proportion
of (unexplained) variance that is between subjects, also called the between-
subjects eﬀect. Consider the unobserved continuous response vector Zk for
item k, where the level-1 variance is restricted to be one and the level-
2 variance equals a2
k which leads to an intraclass correlation coeﬃcient of
ρI = a2
k/(1 + a2
k). A highly discriminating item leads to a high proportion of
variance between subjects. This makes sense since a steep ICC leads to more
diverse responses in comparison with a ﬂat ICC, which leads to more similar
responses from respondents of diﬀerent ability levels. That is, for various abil-
ity levels, the variation in success probabilities is higher for highly discriminat-
ing items (see Figure 1.2), and more variation is explained between subjects,
than for lowly discriminating items. Note that ρI is a correlation coeﬃcient
since it deﬁnes the correlation between two random continuous responses of
the same subject. Other deﬁnitions of the intraclass correlation coeﬃcient are
possible (Commenges and Jacqmin, 1994) and will lead to somewhat diﬀerent
results (Snijders and Bosker, 1999, p. 224).
The level-1 variance is ﬁxed, which causes the nonlinear mixed eﬀects
model to act diﬀerently in comparison with the linear mixed eﬀects model. In
the linear case, the level-1 variance will reduce when incorporating explana-
tory variables in the model, but this cannot happen in this nonlinear model.
Snijders and Bosker (1999) noted that in this particular case the variance of
the random eﬀects and the estimated regression eﬀects will tend to become
larger when adding a level-1 explanatory variable.
This generalized linear mixed eﬀects model representation (which includes
Rasch-type models) has some advantages. In some ways, standard Rasch-
IRT models can be extended within this modeling framework, and standard
software for generalized linear mixed eﬀects models can be used. Current
software packages using a likelihood-based estimation method for (nonlinear)
mixed eﬀects models include GLLAMM (generalized linear latent and mixed
models; Skrondal and Rabe-Hesketh, 2004), HLM (hierarchical linear models;
Raudenbush, Bryk, Cheong and Congdon, 2000), MIXOR (mixed eﬀect ordi-
nal regression; Hedeker and Gibbons, 1996), MIXNO (mixed eﬀect nominal
logistic regression; Hedeker, 1999), MLwiN (Goldstein et al., 1998), Mplus
(Muth´en and Muth´en, 1998), NLME library in S+ (nonlinear mixed eﬀects
model; Pinheiro and Bates, 2000), and SAS PROC NLMIXED. The software
package MLwiN also includes a Bayesian estimation method.
Adams, Wilson and Wu (1997) deﬁned a Rasch model with a population
model for the ability parameters that describes the between-subject variation

6.3 The Multilevel IRT Model
145
using explanatory information at the level of subjects as a nonlinear mixed
eﬀects logistic model. Kamata (2001), Maier (2001), and Raudenbush and
Sampson (1999), among others, deﬁned multilevel formulations of the Rasch
model within this framework to study relationships between background infor-
mation and levels of ability and to examine whether items function diﬀerently
across groups of respondents. Rijmen, Tuerlinckx, De Boeck and Kuppens
(2003) give an overview of item response models that are covered within the
general class of nonlinear mixed eﬀects models.
The framework of the generalized linear mixed eﬀects model is too re-
strictive for the class of models that will be studied in this and subsequent
chapters. First, it can only handle one-parameter (Rasch) models since the
extended two- and three-parameter models are not within this class. Second,
the class of models does not allow free prior choices for the model parameters.
Simply stated, more complex priors may lead to a model that is not within this
class. Moreover, MCMC methods make it possible to estimate simultaneously
parameters of more complex models. Several MCMC algorithms have been
developed for parameter estimation of nonlinear mixed eﬀects models (e.g.,
Albert and Chib, 1993; Karim and Zeger, 1992; Zeger and Karim, 1991).
6.3 The Multilevel IRT Model
6.3.1 A Structural Multilevel Model
The student’s ability is considered to be an outcome variable of the multi-
level regression model. This outcome variable is not directly observable but is
known to be a latent variable. The multilevel model including a latent variable
with a known parametric distribution is called a structural multilevel model.
The multilevel model describes the structure of the individual abilities. Before
engaging in measuring the latent variable, assume for the moment that the
abilities can be directly observed.
The population of level-2 clusters, say schools, are indexed j = 1, . . . , J.
The students at level 1 are nested in schools and indexed i = 1, . . . , nj. Let
level-1 student-speciﬁc covariates be denoted by xij = (x0ij, x1ij, . . . , xQij)t,
where x0ij usually equals one. In general, the level-1 model is represented by
θij = β0j + . . . + βqjxqij + . . . + βQjxQij + eij,
(6.2)
where the errors are independently and identically distributed with mean zero
and variance σ2
θ. The regression parameters are allowed to vary across schools.
Level-2 school-speciﬁc covariates are denoted by wqj = (w0qj, w1qj, . . . , wSqj)t,
where w0qj typically equals one. The random regression coeﬃcients deﬁned in
Equation (6.2) are considered to be outcomes in the linear regression at level
2,
βqj = γq0 + . . . + γqswsqj + . . . + γqSwSqj + uqj,
(6.3)

146
6 Multilevel Item Response Theory Models
for q = 0, . . . , Q, and where level-2 error terms, uj, are multivariate normally
distributed with mean zero and covariance matrix T. The elements of T are
denoted by τ 2
qq′ for q, q′ = 0, . . . , Q.
In the formulation above, the coeﬃcients of all level-1 predictors are
treated as random; that is, as varying across level-2 units. In certain ap-
plications, it can be desirable to constrain the eﬀects of one or more of the
predictors to be identical across level-2 units. In that case, the corresponding
regression eﬀect is considered to be a ﬁxed eﬀect.
The multilevel model in Equations (6.2) and (6.3) is more easily explained
without explanatory variables. Therefore, consider the empty structural mul-
tilevel model,
θij = β0j + eij,
(6.4)
β0j = γ00 + u0j,
(6.5)
where θij is modeled by a school-speciﬁc intercept β0j and the within-school
level-1 error term eij. The school-speciﬁc intercept is broken down into a
general mean γ00 and the deviation, u0j, of the school-speciﬁc intercept from
the general mean. The schools in a sample are thought to be representative of
a larger population of schools, and the school-speciﬁc intercepts are treated as
random eﬀects. Most common is to assume a normal distribution. Therefore,
the error term at level 2, u0j, is assumed to be normally distributed with mean
zero and variance τ 2
00. The error term at level 1 is normally distributed with
mean zero and variance σ2
θ.
It is typically assumed that any two abilities within the same school are
correlated because they share the same random component, and two abilities
related to diﬀerent schools are uncorrelated. It follows that
Cov (θij, θi′,j′) = Cov (β0j + eij, β0j′ + ei′j′)
= Cov (u0j + eij, u0j′ + ei′j′)
= Cov (eij, ei′j′) + Cov (u0j, u0j′)
=



σ2
θ + τ 2
00 for i = i′, j = j′
τ 2
00
for i ̸= i′, j = j′
0
for j ̸= j′,
where it is assumed that the school-speciﬁc intercepts are uncorrelated, and
level-1 and level-2 residuals are uncorrelated. The covariance can be expressed
as a correlation coeﬃcient, which leads to the intraclass correlation coeﬃcient
ρI =
Cov (θij, θi′j′)
p
Var (θij)
p
Var (θi′j′)
=
τ 2
00
τ 2
00 + σ2
θ
,
when i ̸= i′, and j = j′. This correlation coeﬃcient presents the proportion
of variance in individual abilities that is attributable to schools.

6.3 The Multilevel IRT Model
147
The empty multilevel model is a linear mixed eﬀects model since the model
can be presented in a ﬁxed and a random part in addition to the level-1
residual eij. The total random part is composed of level-1 and level-2 residuals;
that is,
θij =
γ00
|{z}
ﬁxed part
+
u0j + eij
|
{z
}
random part
.
(6.6)
The variance components of the random part are regarded as between-school
and within-school variances. Therefore, the model is also referred to as a
variance components model.
The structural multilevel model with explanatory variables can also be
presented as a linear mixed eﬀects model. Therefore, stack the row vectors xij
in the matrix xj that represents the explanatory information of individuals
in school j. The matrix wj represents explanatory information on school j,
and it contains the stacked vectors wqj (q = 0, . . . , Q). A matrix of covariates
will also be referred to as a design matrix without a direct reference to an
experimental setting. The level-1 part of the model can be written as


θ1j
...
θnjj

=


1 x11j . . . xQ1j
...
...
...
...
1 x1njj · · · xQnjj




β0j
...
βQj

+


e0j
...
enjj


and the level-2 part as


β0j
β1j
...
βQj

=


wt
0j
0
. . .
0
0
wt
1j . . .
...
...
...
...
...
0
0
· · · wt
Qj




γ00
...
...
γQS


+


u0j
u1j
...
uQj

.
In matrix notation, the model is written as
θj = xjβj + ej,
(6.7)
βj = wjγ + uj.
(6.8)
Now, the single matrix equation for school j becomes
θj = xj (wjγ + uj) + ej
= xjwjγ + xjuj + ej
= ˜xjγ + xjuj + ej,
(6.9)
where ej ∼N
 0, σ2
θInj

and uj ∼N (0, T). The ﬁxed eﬀects design ma-
trix ˜xj contains student-level, school-level, and the product of student- and
school-level covariates that represent cross-level interactions. Such a cross-
level interaction refers to the modiﬁcation of the eﬀect of a level-1 variable

148
6 Multilevel Item Response Theory Models
by characteristics of the level-2 school to which an individual belongs (or vice
versa). The term cross-level eﬀect is used to denote a main eﬀect of a level-2
variable on the level-1 outcome as well as the modiﬁcation of the eﬀect of a
level-1 variable by a level-2 variable.
Finally, the (conditional) covariance structure of θj can be written as
Var (θj | xj, wj) = xjTxt
j + σ2
θInj.
The conditional covariance matrix of θj is modeled in terms of a component
that includes the random school eﬀects and a component that includes the
within-school error structure. The ﬁrst component deals with the heterogene-
ity in the population of schools, and the second component posits an error
structure that is the same for all schools.
This two-level framework is easily generalized to more levels (e.g., Gold-
stein, 2003; Raudenbush and Bryk, 2002).
6.3.2 The Synthesis of IRT and Structural Multilevel Models
The success of the structural multilevel model (Equations (6.2) and (6.3))
with a latent outcome variable depends partly on the appropriate handling
of measurement issues regarding this latent dependent variable. In practice,
often error prone measures are used, and ignoring measurement error in the
estimated multilevel outcome variable can lead to biased structural multi-
level parameter estimates. Since each student can be presented only a limited
number of items, inference about his or her ability is subject to considerable
uncertainty. This also includes response error due to the unreliability of the
measurement instrument, and moreover, human response behavior is stochas-
tic in nature.
In the present approach, the idea is to integrate an item response model for
measuring the individual abilities with a structural multilevel model that ex-
plains diﬀerences at diﬀerent levels of ability. The measurement model deﬁnes
the relationship between the ability and the corresponding observed response
data. The structural multilevel model describes the hierarchical structure of
individuals in the population.
The observations are considered to be nested within the individuals and
individuals nested in schools. At level 1, measurement error is deﬁned as being
associated with the individual ability. At level 2, diﬀerences in ability among
individuals within the same school are modeled given student-level character-
istics. At level 3, the variation across schools is modeled given background
information at the school level. In Figure 6.1, a path diagram of this multi-
level IRT (MLIRT) model is given. Three boxes are plotted, where the box
indexed item is plotted in the box indexed individual, which is plotted in the
box indexed school to illustrate the nesting of item observations in individuals
and individuals in schools. The ellipse represents the item response model for
measuring the multilevel outcome variable θij. The two boxes indexed indi-
vidual and school constitute the structural multilevel model for θij. It can be

6.3 The Multilevel IRT Model
149
School j
Individual i
qij
xk
xij
bj
wj
yijk
Item k
Fig. 6.1. Path diagram for the MLIRT model.
seen that there are three levels of uncertainty; (1) at the level of observations,
(2) at the student level, and (3) at the school level. The explanatory informa-
tion xij and wj at the individual and school levels explain variability in the
latent abilities within schools and the random regression eﬀects across schools,
respectively. As noted, the observations are nested within the students, and
this is depicted as a box within the individual box. An ellipse is drawn to
stress the nonlinear structure and the diﬀerent meaning of the measurement
part of the model. The item parameters are not inﬂuenced by the nested
structure and are placed outside the boxes. The structural multilevel part of
the model is focused on explaining variation via covariates and hierarchical
levels, where the measurement part of the model, depicted as an ellipse, is
focused on quantifying the measurement error corresponding to the nonlinear
relationship between the discrete observations and the abilities.
The likelihood of the MLIRT model is obtained by integrating over the
random eﬀects distribution. This likelihood equation can be derived by fol-
lowing the structure given in Figure 6.1. Let y be the matrix of observed
response data. Then the likelihood is a product of the likelihood for the J
groups. Taking the nested structure into account and integrating over the
random eﬀects distributions, it follows that
p
 y | ξ, σ2
θ, γ, T

=
J
Y
j=1


Z 

nj
Y
i=1|j
Z
p (yij | θij, ξk) p
 θij | xij, βj, σ2
θ

dθij


p (βj | wj, γ, T)
#
dβj.
(6.10)

150
6 Multilevel Item Response Theory Models
The three levels of the model in Figure 6.1 can also be recognized from the like-
lihood in Equation (6.10). The distribution of the observations, p(yij | θij, ξk),
represents the item response model at the lowest level. The second level is rep-
resented by the conditional distribution of the ability parameters given the
random school eﬀects parameters, p
 θij | xij, βj, σ2
θ

, and the highest level is
described by the distribution of the random school eﬀects, p (βj | wj, γ, T).
The MLIRT modeling framework has additional advantages. First, the
structural multilevel model parameters are estimated from the item response
data without having to condition on estimated person parameters. In “tra-
ditional” multilevel studies, estimated ability parameters considered to be
outcome variables are treated as known. That is, they are assumed to be
measured without an error. In the ﬁrst stage, the abilities are estimated given
a set of item responses using an item response model or by simply counting the
number of correct responses. In the second stage, relationships between the
estimated outcome variable and observed student variables and other group
characteristics are analyzed using multilevel analysis techniques. Such a two-
stage estimation procedure can cause serious underestimation of the standard
errors of the model parameters due to the fact that some parameters are held
ﬁxed at values estimated from the data. Ignoring the uncertainty regarding
the abilities within the model may lead to biased parameter estimates, and
the statistical inference may be misleading (see also Section 6.6.3). In conclu-
sion, this MLIRT framework leads to a proper treatment of the measurement
error associated with the ability parameter.
Second, the MLIRT modeling framework allows the incorporation of ex-
planatory variables at diﬀerent levels of hierarchy. The inclusion of explana-
tory information can be important in various situations. Mislevy and Sheehan
(1989) described diﬀerent cases of using collateral information about exami-
nees. A particular case is concerned with using collateral information to sample
examinees and assign items to them. In this case, inconsistent item parameter
estimates will be obtained when the parameters are estimated via integration
over the examinees’ population distribution and ignoring the collateral infor-
mation. This follows from the fact that the missing explanatory information
about the examinees cannot be seen as missing at random (MAR); the prob-
ability of the observed pattern of missingness is not the same for all values of
the missing variables. A striking example is given in targeted testing, where
examinees of diﬀerent grade levels obtain diﬀerent test items that are selected
on the basis of the collateral information. In this case, the item parameter
estimates will be inconsistent when the explanatory information is ignored.
Third, another related advantage of the model is that it can handle in-
complete data in a very ﬂexible way. To ease the notation, the index K is not
given a subscript i, but variation across individuals in terms of the number
of completed items is allowed. That is, there are no restrictions on the num-
ber of observations per individual, and it is possible that there are only a few
common items across individuals. The number of individuals may diﬀer across
schools. Each school j may vary in the number of respondents since n carries

6.3 The Multilevel IRT Model
151
the subscript j. There are no restrictions on the number of students per school.
This model feature follows in a straightforward way from properties of item
response and multilevel models. Note that this ﬂexibility assumes that the
missing data are missing completely at random, also known as MCAR (Ru-
bin, 1976). That is, item responses and subjects are missing for completely
random reasons. The missing-data mechanism that characterizes the reasons
for the missingness is known to be MCAR and constitutes patterns of missing
data that are independent of both the observed data and the missing data.
Fourth, the MLIRT modeling framework provides shrinkage estimators
that are based on an eﬃcient combination of the response data and the collat-
eral information. In general, a shrinkage estimator is biased, but a reduction in
mean squared error can be achieved when this estimator has a smaller variance
in comparison with an unbiased estimator. The shrinkage estimator of the abil-
ity parameter illustrates the combined use of response data and student-level
collateral information. Therefore, consider the two-parameter response (pro-
bit) model formulation (Equation (4.38)) and the structural multilevel model
for the ability parameters (Equations (6.2) and (6.3)). The latent response
data Zij and the ability parameter θij are bivariate normally distributed.
That is,
Zij
θij

= N
 aθij −b
xt
ijβj

,
σ2
θaat + IK σ2
θa
σ2
θat
σ2
θ

.
(6.11)
From a property of the bivariate normal distribution,1 it follows that the
conditional posterior expectation of ability can be expressed as (suppressing
the conditioning for notational convenience)
E (θij) = xt
ijβj + σ2
θat  IK + σ2
θaat−1  zij −
 axt
ijβj −b

= xt
ijβj + σ2
θat

IK −
aat
σ−2
θ
+ ata
  (zij + b) −axt
ijβj

= xt
ijβj −
ata
σ−2
θ
+ ataxt
ijβj + at (zij + b)
σ−2
θ
+ ata
= σ−2
θ xt
ijβj
σ−2
θ
+ ata + at (zij + b)
σ−2
θ
+ ata
=
 σ−2
θ
+ ata
−1  ata
 ˆθij + σ−2
θ xt
ijβj

,
(6.12)
=
ata
σ−2
θ
+ ata
ˆθij +

1 −
ata
σ−2
θ
+ ata

xt
ijβj,
(6.13)
1 Let (X1, X2) be bivariate normally distributed with means (µ1, µ2) and variances
(σ2
x1, σ2
x2) and correlation ρ. The conditional distribution of X1 given X2 = x2 is
normal with mean µx1 + ρσ−2
x2 (x2 −µx2) and variance σ2
x1 −ρ2σ−2
x2 (Anderson,
2003, pp. 33–36).

152
6 Multilevel Item Response Theory Models
where ˆθij = (ata)−1 at (zij + b) and represents the least squares estimate for
θij given the latent response data and item parameters. In the second step for
deriving the expression in (6.13), a speciﬁc expression for the so-called Schur
complement of σ2
θaat + IK of the covariance matrix in (6.11) is used. In this
case, the inverse of the Schur complement is given by
 A + λvvt−1 = A−1 −A−1vvtA−1
1/λ + vtA−1v;
(6.14)
see Searle et al. (1992, p. 453).
The posterior mean of the corresponding posterior distribution is consid-
ered to be an estimate of the ability parameter. In this light, the result in
Equation (6.13) is viewed as a shrinkage estimate of the ability parameter,
which consists of a linear combination of two weighted estimates, the ordinary
least squares estimate ˆθij and, given an estimate of random regression eﬀect
βj, the (prior) level-2 estimate xt
ijβj. The weights are proportional to the
variance of the ordinary least squares estimate and the variance σ2
θ. When
reducing the number of items (level 1), the variance of the ˆθij increases and
ata decreases, and the posterior mean moves towards the level-2 mean. When
increasing the number of (informative) items, the least squares estimate is
less corrected by the level-2 mean. The shrinkage estimate can be viewed as
a compromise between the within-subject estimate that is based on the indi-
vidual’s item response data and the between-subject estimate that is based
on the information from the structural population model. It can be concluded
that the use of the examinees’ explanatory information may lead to a more
accurate estimate of the ability parameters.
Finally, in the same way, the use of explanatory information may lead to
more accurate item parameter estimates. In that case, the posterior distribu-
tion of the ability parameters is based on item response data and collateral
information. Both provide information about the examinees’ ability param-
eters. Subsequently, a more accurate marginal posterior density of the item
parameters can be obtained when integrating over the more informative pos-
terior density of ability parameters that is based on collateral and response
information. Note that the marginal posterior density of the item parameters
can be expressed as
p (ξk | y, x) =
Z
p (ξk | y, θ) p
 θ; β, σ2
θ, x

dθ
(6.15)
=
Z p (y | ξk, θ) p(ξk)p
 θ; β, σ2
θ, x

p(y)
dθ
=
Z
p (y | ξk, θ) p(ξk)p
 θ; β, σ2
θ, x

p(y)
dθ
=
Z
p (y | ξk, θ) p(ξk)p
 θ | y; β, σ2
θ, x

p (y | θ)
dθ

6.4 Estimating Level-3 Residuals: School Eﬀects
153
=
Z p (y | ξk, θ) p(ξk)
p (y | θ)
p
 θ | y; β, σ2
θ, x

dθ
=
Z
p (ξk | y, θ) p
 θ | y; β, σ2
θ, x

dθ,
(6.16)
where the semicolon notation is used to stress that the subsequent parameters
are assumed to be known.
In Equation (6.15), the integration is performed with respect to the prior
density of the ability parameters using the collateral information. After some
transformations, it follows that the marginal posterior density of the item pa-
rameters can also be obtained via integration using the conditional posterior
density of the ability parameters, Equation (6.16). As a result, the marginal
posterior density of the item parameters, which is obtained via prior or pos-
terior integration, might be improved by using collateral information.
Note that the use of an item response model, in contrast to an observed
(sum) score, also has some advantages. First, the structural multilevel model
with a latent outcome variable measured with an item response model has
the advantage that latent rather than observed scores are used as dependent
variables, which oﬀers the possibility of separating the inﬂuences of item dif-
ﬁculty and ability level and modeling response variation and measurement
error. Second, contrary to observed scores, latent scores are test-independent,
which oﬀers the possibility of using results from diﬀerent tests in one analysis
where the parameters of the item response model and the multilevel model
can be estimated concurrently.
6.4 Estimating Level-3 Residuals: School Eﬀects
There is increasing interest in the accountability of educational institutions.
Research has focused on measuring the “quality” of schools and making quan-
titative comparisons between schools. So-called performance indicators can be
used to judge school eﬀectiveness and determine what measures can be taken
for improvement, obtains knowledge about the relative size of school diﬀer-
ences, and determine the extent to which other indicators may explain dif-
ferences. Within this context, attention has focused on contextual diﬀerences
and the appropriate speciﬁcation of a statistical model.
Aitkin and Longford (1986), among others, modeled school eﬀects as
higher-level residuals and showed interest in the actual realized values to com-
pare schools and in the distributional properties of the residuals. For example,
a signiﬁcant school-level variance indicates the existence of diﬀerences among
schools. This method for comparing schools is not without criticism. Obvi-
ously, the residuals behave like noise, which complicates the inferences. The
random error term may contain a contextual part that should be excluded
when the purpose is assessing the eﬀectiveness of a school. A comprehensive
review of school eﬀectiveness research can be found in Scheerens (1992).

154
6 Multilevel Item Response Theory Models
Although several student outcome measures need to be used, for a full
understanding of school eﬀectiveness research, one kind of measure is used and
attention is focused on the reliability of this outcome measure. In the present
approach, diﬀerent sources of variation are taken into account when estimating
the school eﬀects: within-subject, between-subject or within-school, between-
school variability.
For illustration, the contribution of a school, the so-called school eﬀect, to
the abilities of its students is analyzed by a multilevel model, Equations (6.4)
and (6.5). The MLIRT model can be presented as one equation by entering the
multilevel expressions in the normal latent response formulation of Equation
(4.38). Then, the stacked individual response vectors, each related to K items,
of the nj persons in school j, denoted as Zj, can be represented as


Z1j1
...
Z1jK
...
ZnjjK


=


a1γ00 −b1 + a1u0j + a1e1j + ϵ1j1
...
aKγ00 −bK + aKu0j + aKe1j + ϵ1jK
...
aKγ00 −bK + aKu0j + aKenjj + ϵnjjK


,
(6.17)
where three independent random error terms can be recognized. The ϵijk rep-
resents the random deviation between the Zijk and the mean term akθij −bk.
This within-individual residual variation is ﬁxed at one due to an identiﬁ-
cation restriction (Section 4.4). The eij represents the between-individual or
within-school residual variation given the school eﬀect and reﬂects the vari-
ation around the mean γ00 + u0j. The third residual term, u0j, represents
the random school eﬀect. The normally distributed random error terms are
independent and have a mean of zero. The within-school variance is denoted
by σ2
θ and the between-school variance by τ 2.
The equations for the nj times K latent observations Zj can be presented
in matrix notation by utilizing a summing vector 1nj = (1, . . . , 1)t of nj
elements of one. A matrix of ones is denoted as Jnj, which equals 1nj1t
nj.
Subsequently, the vector 1nj ⊗a is the stacked vector of nj vectors a =
(a1, . . . , aK)t, where matrix operation ⊗is known as the direct product or
Kronecker product. Now, Equation (6.17) can be presented as
Zj = 1nj ⊗(aγ00 −b) + 1nj ⊗au0j + ej ⊗a + ϵj.
(6.18)
Interest is focused on estimating the random error term u0j via the pos-
terior expectation of u0j. Therefore, consider the multivariate normally dis-
tributed latent response data and the normally distributed random eﬀects,
(Zj, θj, u0j)t. The nested structure of observations within individuals and
of individuals within schools becomes apparent from the variance-covariance
structure. Via Equation (6.18), it follows that, suppressing the conditioning,

6.4 Estimating Level-3 Residuals: School Eﬀects
155
Var (Zj) = Var
 1nj ⊗a

u0j + Var (ej ⊗a) + Var (ϵj)
=
 1nj ⊗a

τ 2  1nj ⊗a
t + Inj ⊗σ2
θaat + I(nj·K)
= Jnj ⊗
 τ 2aat
+ Inj ⊗
 σ2
θaat + IK

.
The covariance structure of the observations Zj consists of three terms. The
ﬁrst term reﬂects the covariance structure induced by the nesting of individu-
als in schools. The second term reﬂects the nesting of observations within indi-
viduals. The discrimination parameters function as weights in such a way that
high (low) discriminating parameter values cause a strong (weak) covariance
structure in individual observations. The last term is a unity matrix. In the
same way, the entire dispersion matrix of the random variables, (Zj, θj, u0j)t,
equals


Jnj ⊗
 τ 2aat
+ Inj ⊗
 σ2
θaat + IK

| Jnj ⊗τ 2a + Inj ⊗σ2
θa | 1nj ⊗τ 2a
Jnj ⊗τ 2at + Inj ⊗σ2
θat
|
σ2
θInj + τ 2Jnj
|
1njτ 2
1t
nj ⊗τ 2at
|
1t
njτ 2
|
τ 2

.
For the moment, assume that the student abilities are measured exactly.
Interest is often focused on the population of schools; in particular, the vari-
ability of the school eﬀects. Here, attention is focused on the size of the
school eﬀects given the individual abilities. The abilities and random school
eﬀects are bivariate normally distributed. The conditional expected value of
the school eﬀects given the abilities, suppressing the conditioning on other
model parameters, equals
E (u0j | θj) = E (u0j) + Cov (u0j, θj) (Var (θj))−1 (θj −E(θj))
= 1t
njτ 2  τ 2Jnj + σ2
θInj
−1  θj −1njγ00

= njτ 2
σ2
θ
 θj −γ00

−njτ 2
σ2
θ

njτ 2
σ2
θ + njτ 2
  θj −γ00

=
njτ 2
σ2
θ + njτ 2
 θj −γ00

=
 θj −γ00

−
σ2
θ
σ2
θ + njτ 2
 θj −γ00

,
(6.19)
using that
 τ 2Jnj + σ2
θInj
−1 = σ−2
θ

Inj −
τ 2
σ2
θ + njτ 2 Jnj

.
(6.20)
In Equation (6.19), the conditional expected posterior value is expressed in a
shrinkage representation. The expected posterior value is a shrinkage estimate
of the school eﬀect. In estimating the school eﬀect, u0j, the overall mean, in
this case zero, is biased, but the unbiased estimate θj −γ00 has a larger
variance. For the shrinkage estimate, Equation (6.19), it follows that when θj

156
6 Multilevel Item Response Theory Models
exceeds γ00, the expected school eﬀect is less than θj −γ00, whereas for θj less
than γ00, the expected school eﬀect exceeds θj −γ00. But the expected school
eﬀect is only corrected by a fraction of θj −γ00 and not by θj −γ00 itself. For
large within-school variances and small numbers of students per school, the
shrinkage estimate is much more eﬃcient than the overall mean or θj −γ00.
In this particular case, the unbiased within-sample mean has a large variance.
The fraction deﬁned in Equation (6.19) is a combination of within-school
variance and between-school variance; that is, the empty multilevel model
takes these diﬀerent sources of variation into account. The within-subject vari-
ance is the additional source of variation that is taken into account. Therefore,
the conditional expected posterior value of u0j given the latent response data
is considered to be an estimate of the school eﬀect. The conditional expected
posterior value of u0j can be derived in a comparable way. From Appendix
6.9, it follows that
E (u0j | zj) =

1t
nj ⊗τ 2at  Jnj ⊗τ 2aat
+
 Inj ⊗
 σ2
θaat + IK
−1
·
 zj −
  1nj ⊗a

γ00 −1nj ⊗b

(6.21)
=

njτ 2
((ata)−1 + σ2
θ) + njτ 2

ˆu0j
= ˆu0j −

(ata)−1 + σ2
θ
((ata)−1 + σ2
θ) + njτ 2

ˆu0j,
(6.22)
where
ˆu0j = (ata)−1at (zj −(aγ00 −b))
and zj = Pnj
i=1 zij/nj.
The estimator is a weighted sum of the mean of weighted latent responses
of the students of a school indexed j and the overall population mean, which
in this case is zero. The shrinkage factor that determines the movement of this
least squares estimator to the overall mean consists of the between-school and
within-school variances and the inner product of the discrimination parame-
ters, called measurement variance. Relatively high discrimination parameters
indicate that the abilities of the examinees can be distinguished quite well
from each other given their response patterns. When the information regard-
ing the abilities of the students is relatively high, the estimated school eﬀect
shrinks less towards the overall mean. In that case, schools can be better
distinguished from each other with respect to the estimated school eﬀects.
Relatively low discrimination parameters move the estimate of the school ef-
fect towards the overall mean since the estimated abilities of the students
cannot be distinguished accurately from each other.
The least squares estimator ˆu0j is constructed as the mean over all nj in-
dividual least squares estimators. The variance of the least squares estimator
equals (ata)−1, and this measurement variance is inﬂuenced by the number of

6.4 Estimating Level-3 Residuals: School Eﬀects
157
items and the values of the discrimination parameters. The expected posterior
mean shrinks towards the overall mean when decreasing the between-school
variance, τ 2, or increasing the within-school variance, σ2
θ, and/or the mea-
surement variance (ata)−1.
The shrinkage factor in Equation (6.19) equals the shrinkage factor in
Equation (6.22) when the inner product of discrimination parameters equals
zero such that the measurement variance is ignored. In Figure 6.2, both shrink-
age factors are plotted for various conditions. The discrimination parameter
values equal one such that the inner product of discrimination values equals
K. Population parameter γ00 equals zero. The (horizontal) x-axis presents the
number of items from three to ten and the number of respondents in school j
from 30 to 100. The shrinkage factor of Equation (6.22), plotted as a straight
line and labeled random, decreases when the number of items or the number
of individuals increases. The shrinkage factor of Equation (6.19), plotted as
a broken line and labeled ﬁxed, decreases when the number of individuals
increases.
For relatively small within- and between-school variances, the upper ﬁgure
in Figure 6.2 shows that the measurement variance has a large eﬀect on the
value of the shrinkage factor. The variance of the least squares estimator
increases due to the small number of items and as a result the expected
school eﬀect is shrunk towards zero. When ignoring the measurement variance,
the corresponding shrinkage factor is small due to the small values for τ 2
and σ2
θ, which leads to a relatively high estimated school eﬀect. Ignoring the
measurement variance leads to higher school eﬀects in absolute values and
a sharper distinction between schools. Furthermore, covariates that explain
variation in school eﬀects are more likely to be signiﬁcant when ignoring the
measurement variance. Note that the small values of τ 2 and σ2
θ correspond
to the realistic situation where additional background information explains a
large part of the within-school and between-school variations. The middle and
lower ﬁgures show the shrinkage factors for increasing values of the within- and
between-school variations. It follows that the measurement variance part has
a smaller impact on the shrinkage factor when the other sources of variance
increase. In that case, the straight lines and the broken lines are closer to each
other but diﬀerences remain visible.
The plotted shrinkage factors are based on the true known values of other
model parameters, which are usually unknown. Ignoring measurement vari-
ance leads to higher estimated school eﬀects in absolute values. This will lead
to a biased between-school variance estimate since the schools appear to dif-
fer more with respect to the estimated school eﬀects. The degree of shrinkage
depends on unknown model parameters, and they need to be estimated from
the data as well. Ignoring the measurement variance inﬂuences the estimates
of the school eﬀects and the other model parameters.

158
6 Multilevel Item Response Theory Models
0.00
0.05
0.10
0.15
0.20
0.00
0.05
0.10
0.15
0.20
0.25
0.00
0.02
0.04
0.06
0.08
0.10


2
2
=.05,
=.05 (Random)


2
2
=.1,
=.1 (Random)


2
2
=.1,
=.1 (Fixed)


2
2
=.5,
=.1 (Random)


2
2
=.5,
=.5 (Random)


2
2
=.5,
=.1 (Fixed)


2
2
=.5,
=.5 (Fixed)


2
2
=.5,
=.1 (Random)


2
2
=1,
=.5 (Random)


2
2
=.5,
=.1 (Fixed)


2
2
=1,
=.5 (Fixed)
K=10
n =100
j
K=9
n =90
j
K=8
n =80
j
K=7
n =70
j
K=6
n =60
j
K=5
n =50
j
K=4
n =40
j
K=3
n =30
j
K=10
n =100
j
K=9
n =90
j
K=8
n =80
j
K=7
n =70
j
K=6
n =60
j
K=5
n =50
j
K=4
n =40
j
K=3
n =30
j
K=10
n =100
j
K=9
n =90
j
K=8
n =80
j
K=7
n =70
j
K=6
n =60
j
K=5
n =50
j
K=4
n =40
j
K=3
n =30
j
Fig. 6.2. Shrinkage factors of the conditional expected (residual) school eﬀect U0j.
6.5 Simultaneous Parameter Estimation of MLIRT
A Markov chain Monte Carlo (MCMC) method is constructed to estimate
all model parameters simultaneously. Assume the structural multilevel model
in Equations (6.2) and (6.3) in combination with a two-parameter model for
binary discrete item observations or a graded response model for ordinal re-
sponse data.
Assume the following prior distributions for the structural multilevel model
parameters. The ﬁxed eﬀects, γ, are assumed to have an independent normal
prior, with mean zero and variance σγ, and the hyperparameter σγ equals a
large number to specify a noninformative prior. The prior for the covariance
matrix T is taken to be an inverse Wishart density,
p (T | nq, ST ) ∝|T|−(nq+Q+2)/2 exp
 −tr
 ST T−1
/2

,
with, for example, unity matrix ST and nq (nq ≥Q + 1) equal to a small
number to specify a proper diﬀuse prior. The conventional prior for σ2
θ is the
inverse gamma with parameters g1 and g2 with density function
p
 σ2
θ

∝
 σ2
θ
−(g1+1) exp

−g2
σ2
θ

.
A proper noninformative prior is speciﬁed with g1 = 1 and a small value
for g2. The following MCMC scheme for the MLIRT model contains slightly
modiﬁed steps of schemes 2 and 3.

6.5 Simultaneous Parameter Estimation of MLIRT
159
MCMC Scheme 4 (MLIRT)
A1) Binary response data
1. Sample augmented data z(m+1) according to Equation (4.7).
2. Assume an exchangeable multivariate normal prior for ξk with mean
µξ and variance Σξ. Let H =
 dθ(m), −d1n

, and sample ξ(m+1)
k
from the conditional density
ξk | z(m+1)
k
, θ(m), µ(m)
ξ
, Σ(m)
ξ
∼N
 µ∗
ξ, Ωξ

,
(6.23)
where
Ω−1
ξ
= ϕ−1  HtH

+ Σ−1
ξ ,
µ∗
ξ = Ωξ

Htzk + µξΣ−1
ξ

,
with ϕ = 1 if Zk is normally distributed and ϕ = π2/3 if Zk is
logistically distributed. The density in Equation (6.23) is used to gen-
erate candidates evaluated in an M-H step when Zk is logistically dis-
tributed.
3. Sample µ(m+1)
ξ
, Σ(m+1)
ξ
from the conditional densities
µξ | Σ(m)
ξ
, ξ(m+1) ∼N

K0
K0 + K µ0 +
K
K0 + K ξ, Σξ/(K + K0)

Σξ | ξ(m+1) ∼IW (K + ν, Σ∗) ,
where Σ∗= Σ0 + KS +
KK0
K+K0
 ξ −µ0
  ξ −µ0
t.
A2) Polytomous response data
1. Sample augmented data z(m+1) given θ(m), ξ(m) according to Equation
(4.32).
2. Sample threshold parameter values κ(m+1) given z(m+1), θ(m), a(m)
according to step 3 of MCMC scheme 3.
3. For each k, sample parameters a(m+1)
k
given z(m+1)
k
, θ(m), µ(m)
a
, σ2(m)
a
according to step 4 of MCMC scheme 3.
B) Sample the multilevel parameter values
4. For each i and j, sample θij from the conditional normal density
θij | z(m+1)
ij
, ξ(m+1), β(m)
j
, σ2(m)
θ
∼N (µθ, Ωθ) ,
(6.24)
where
µθ = Ωθ
 ata
 ˆθij + σ−2
θ xt
ijβj

,
(6.25)
Ωθ =
 ata + σ−2
θ
−1 .
(6.26)
The density in (6.24) becomes a proposal density when the latent re-
sponses are logistically distributed.

160
6 Multilevel Item Response Theory Models
5. For each j, sample β(m+1)
j
from the full conditional
βj | θ(m+1)
j
, σ2(m)
θ
, T(m), γ(m) ∼N (µβ, Ωβ) ,
(6.27)
where
µβ = Ωβ

Σ−1
j
ˆβj + T−1wjγ

,
Ωβ =
 Σ−1
j
+ T−1−1 ,
with Σj = σ2
θ
 xt
jxj
−1 and ˆβj =
 xt
jxj
−1 xt
jθj.
6. Sample γ(m+1) from the full conditional
γ | β(m+1), T(m), σγ ∼N (µγ, Ωγ) ,
where
µγ = Ωγ
X
j
wt
jT−1βj,
Ωγ =
 X
j
wt
jT−1wj + Iνσ−1
γ
!−1
,
where unity matrix Iν is of dimension ν = (Q + 1)(S + 1).
7. Sample σ2(m+1)
θ
given θ(m+1) and β(m+1) from an inverse gamma
density with shape parameter g1 + P
j nj/2 and scale parameter
X
j
(θj −xjβj)t (θj −xjβj) /2 + g2.
8. Sample T(m+1) given γ(m+1), β(m+1) from an inverse Wishart density
with nq + J degrees of freedom and scale matrix
X
j
(βj −wjγ) (βj −wjγ)t + ST .
The MCMC scheme of Fox and Glas (2001) only considers binary response
data and the normal ogive item response model. In this scheme, logistic and
normal ogive item response models are considered and more advanced prior
distributions for the item parameters are used, as discussed in Chapter 2.
MCMC scheme 4 is easily extended to handle a structural multilevel model
with more than two levels. Restricting the mean and variance of the latent
ability scale is a common way of identifying the MLIRT model. This can be
done during the MCMC estimation procedure by transforming each drawn
vector of ability values in such a way that the mean and variance of the
scaled vector correspond with the a priori speciﬁed mean and variance. This

6.5 Simultaneous Parameter Estimation of MLIRT
161
procedure corresponds to ﬁxing the mean and variance of the conditional
posterior distribution of the latent variable. The MLIRT model can also be
identiﬁed via restrictions on the item parameters (see Section 4.4).
The log of the complete-data likelihood of the MLIRT model can be pre-
sented as
log p
 y, θ, β | ξ, γ, σ2
θ, T

=
X
i,j
 X
k
log p (yijk | θij, ξk)
|
{z
}
Item response part
+
log p
 θij | βj, σ2
θ

+ log p (βj | γ, T)
!
.
|
{z
}
Structural multilevel part
This complete-data log-likelihood of the MLIRT model consists of two parts,
a part following from the item response model and a part following from the
structural multilevel model. The idea is that model changes in the multilevel
part can be tested conditional on the item response part such that relatively
small changes in the log-likelihood of the multilevel part can be detected. In
practice, MLIRT models with diﬀerent structural multilevel parts and equiv-
alent item response parts are often compared with each other. Therefore,
attention is focused on the multilevel part, and the likelihood of interest is
deﬁned as
p
 θ | γ, σ2
θ, T

= p
 θ | β, σ2
θ

p (β | γ, T)
p (β | θ, γ, σ2
θ, T)
(6.28)
=
Y
j
 2πσ2
θ
−nj/2 |Ωβ|1/2|T|−1/2 exp
 −1
2σ2
θ

θj −xj ˜βj
t

θj −xj ˜βj

−1
2

˜βj −wjγ
t
T−1 
˜βj −wjγ

, (6.29)
where ˜βj is normally distributed with mean µβ and variance Ωβ according
to Equation (6.27). Equation (6.28) holds for all βj and in particular when
βj = ˜βj (Dempster et al., 1981). The log-likelihood term of Equation (6.29)
can be used as a deviance term for model comparison; that is,
D
 γ, σ2
θ, T; θ

= N log
 2πσ2
θ

−J log |Ωβ| + J log |T| + 1
σ2
θ

θ −x˜β
t

θ −x˜β

+
X
j

˜βj −wjγ
t
T−1 
˜βj −wjγ

.
(6.30)
The Bayesian information criterion (BIC) can be computed using the de-
viance deﬁned in Equation (6.30) since the number of parameters is speciﬁed
directly. The number of model parameters is needed to deﬁne the penalty

162
6 Multilevel Item Response Theory Models
term in the BIC. When the deviance term contains random eﬀects parame-
ters, a deviance information criterion (DIC) can be computed using output
from MCMC scheme 4. The DIC consists of two terms. One term is the de-
viance of the multilevel part, which is evaluated at the posterior means. The
other term is the expected posterior deviance, where the expectation is taken
with respect to the posterior distribution of the structural multilevel model
parameters.
The DIC of the structural multilevel part conditions on the latent depen-
dent variable, which can be integrated out via its marginal posterior distri-
bution. That is, for both terms of the DIC, the posterior expectation can be
computed with respect to the marginal posterior density of θ.
An integrated likelihood of an MLIRT model is derived in Appendix 6.10.
The integrated likelihood in Equation (6.44) can be used for model comparison
without having to condition on random eﬀects parameter estimates. When a
deviance term is deﬁned based on the likelihood in Equation (6.44), the DIC
based on this deviance is a posterior expected DIC where the expectation is
taken with respect to the marginal posterior densities of θ and β.
6.6 Applications of MLIRT Modeling
The applications in Sections 6.6.1, 6.6.2, and 6.6.3 focus on school eﬀective-
ness research with an interest in the development of the knowledge and skill
of individual students in relation to school characteristics. Data are analyzed
at the individual level, and it is assumed that classrooms, schools, and experi-
mental interventions have an eﬀect on all students exposed to them. In school
or teacher eﬀectiveness research, both levels of the structural multilevel model
are of importance because the objects of interest are schools and teachers as
well as students. There is interest in the eﬀect on student learning of the
organizational structure of the school, characteristics of a teacher, and char-
acteristics of the student. In Section 6.6.3, the MLIRT model is extended by
incorporating latent explanatory variables. The application is focused on esti-
mating the eﬀect of educational leadership (school level) on math achievement
(student level) after adjusting for student characteristics. In the application
in Section 6.6.4, longitudinal item response data are analyzed where responses
are nested within measurement occasions that are nested within subjects. A
growth mixture modeling approach will be pursued to estimate individual
trajectories of cognitive impairment for patients and controls.
6.6.1 Dutch Primary School Mathematics Test
In Section 5.2.4, test data for students leaving primary school were analyzed
with a two-parameter item response model that ignored the nesting of the stu-
dents in schools and the background information of students and schools. The
2,156 grade eight students are unequally spread over 97 schools and responded

6.6 Applications of MLIRT Modeling
163
to dichotomously scored mathematics items taken from the examination leav-
ing school developed by Cito (Netherlands national institute for educational
measurement). The 97 schools were fairly representative of all Dutch primary
schools. Of the 97 schools sampled, 72 schools regularly participated in the
examination leaving primary school. These are denoted as Cito schools, and
the remaining 25 schools are denoted as the non-Cito schools. A school-level
indicator variable (End) equaled one if the school participated in the test
leaving primary school and zero if this was not the case.
Several student-intake characteristics were measured: socioeconomic sta-
tus (SES), standardized scores on a nonverbal intelligence test (ISI-NV), and
gender. The standardized SES scores were based on four indicators: the edu-
cation and occupation level of both parents (if present). Gender was coded as
zero for males and one for females.
The data were analyzed with an MLIRT model consisting of a normal
ogive item response model at level 1 for measuring students’ math abilities.
A structural multilevel model deﬁned the level-2 (student-level) and level-3
(school-level) parts. The MLIRT analysis was compared with a linear multi-
level analysis using observed sum scores, the sum of the number of correct
answers, as a measurement for the math abilities.
Interest was focused on (1) the eﬀect of schools’ participation in the exam-
ination leaving primary school on students’ math abilities after adjusting for
individual diﬀerences and (2) diﬀerences in parameter estimates when using
sum scores compared with using an item response model for measuring math
achievement. Therefore, the structural multilevel model M1
θij = β0j + β1jISIij + β2jSESij + β3jFemaleij + eij,
β0j = γ00 + γ01Endj + u0j,
β1j = γ10,
β2j = γ20,
β3j = γ30,
was considered, where eij ∼N
 0, σ2
θ

and u0j ∼N
 0, τ 2
00

.
In Table 6.1, the estimates of the parameters issued from MCMC scheme 4
are given. Under the heading Linear Multilevel Model, parameter estimates of
an empty multilevel model and multilevel model M1 are given using standard-
ized observed sum scores as the outcome variable. Under the heading MLIRT
Model, parameter estimates of MLIRT models are given with the same struc-
tural multilevel part as the linear multilevel models. The MLIRT models were
identiﬁed by ﬁxing the mean and variance of the vector of sampled math
abilities to zero and one, respectively, in each MCMC iteration. This way, all
model parameter estimates are directly comparable. The reported standard
deviations and HPD regions are the posterior standard deviations and the
95% highest posterior density intervals, respectively.
The grouping of students in schools explained a lot of the variance in
math ability. A higher proportion of the variance is explained at the school

164
6 Multilevel Item Response Theory Models
level according to the MLIRT analysis in comparison with the linear multi-
level analysis. It followed that 28% of the individual variance according to
the MLIRT analysis and 21% according to the linear multilevel analysis is
explained at the school level. The estimated posterior standard deviations are
larger in the MLIRT analysis since the measurement error in the dependent
variable is taken into account. The posterior standard deviations from the
MLIRT analysis provide a more reliable basis for testing, for example, the
importance of predictor variables, since the inaccuracy of the estimated math
achievements is taken into account.
From the parameter estimates of model M1 it follows that conditioned on
SES, ISI, and Female, Cito schools performed better than non-Cito schools.
That is, a signiﬁcant positive eﬀect on the students’ math abilities is found
in schools that participate on a regular basis in the central test leaving pri-
mary school. The general mean ability, γ00, of the students attending non-Cito
schools is signiﬁcantly diﬀerent from zero, and the positive signiﬁcant value
of γ01 indicates a positive eﬀect on the students’ math abilities due to the
school’s regular participation in the central exam leaving school. The positive
eﬀects of the predictors ISI and SES indicate that a student with a higher
score (socioeconomic status or nonverbal intelligence test) performed better
on the math test than a student with a lower score. The eﬀect of Female is
also signiﬁcant and negative, meaning that boys outperformed girls on the
math test.
The estimated predictor eﬀects of the MLIRT analysis are higher in abso-
lute value than the estimated eﬀects of the linear multilevel analysis. There-
fore, diﬀerences between students’ math abilities and the explanatory eﬀects
of individual and school characteristics both become more apparent. The mea-
surement of math abilities based on the two-parameter item response model
resulted in a sharper distinction between students’ outcomes than the ob-
served sum scores. This is caused by the fact that the response patterns con-
tain more information about the students’ math abilities than the sum scores.
The sum score is often used for unidimensional ability estimation, and under
the Rasch model the sum score is statistically suﬃcient in estimating the true
ability (Lord, 1980). However, the sum scores are biased estimates of ability
when the items have diﬀerent discrimination values. Errors in the sum scores
are ignored, and they attenuate the eﬀects of the predictors towards zero.
The variance explained due to grouping is considerably lower in the linear
multilevel analysis. The proportion of variance explained by the explanatory
variables in the MLIRT analysis is 38% at the student level and 39% at the
school level and in the linear multilevel analysis 29% and 46%, respectively.
For each model, the log-likelihood of the multilevel model part is computed
given the outcome variable (for the linear multilevel models) or by integrating
over the density of the latent outcome variable (for the MLIRT models). The
DICs of the linear multilevel models and the MLIRT models show that in both
cases model M1 is preferred. The dependent variable of the linear multilevel
model is diﬀerent from that of the MLIRT model, but they are scaled the same

6.6 Applications of MLIRT Modeling
165
way and are both considered estimates of the true ability values. However,
the standardizing factors, the marginal posterior distributions p(θ) and p(y)
(where y are the observed sum scores), were excluded from the deviance terms,
which complicates a direct comparison of the linear multilevel and MLIRT
models using the estimated DICs. The reduction in DIC values is independent
of the standardizing factor.
It can be seen that this reduction of 1,048 from the MLIRT analyses is
much larger than the reduction of 736 from the linear multilevel analyses. This
indicates that there is more statistical evidence to prefer MLIRT model M1
over the empty MLIRT model in comparison with preferring linear multilevel
model M1 over the empty linear multilevel model. This conclusion is sup-
ported by the fact that the (ﬁxed) eﬀects (in absolute value) of the student-
and the school-level variables are higher. The estimated eﬀective number of
parameters is smaller for the linear multilevel models. This indicates that the
random intercepts have shrunk more towards the prior mean and schools ap-
pear to be more similar in their eﬀects on student achievement in comparison
with the results from the MLIRT analyses.
6.6.2 PISA 2003: Dutch Math Data
The Programme for International Student Assessment (PISA) launched by
the Organisation for Economic Co-operation and Development (OECD) is
conducted to assess student performance and collect data on student and
institutional factors that can explain diﬀerences in performance. The PISA
2003 results can be found in OECD (Organisation for Economic Co-operation
and Development) (2004), and the PISA 2003 data can be found at http:
//pisa2003.acer.edu.au/downloads.php (February 2010).
In 2003, 41 countries participated, and the survey covered mathematics
(the main focus in 2003), reading, science, and problem solving. Attention is
focused on the mathematics (literacy) abilities of 15-year-old Dutch students.
Student performance in mathematics is measured via 85 items. Students were
given credit for each item that they answered with an acceptable response.
In this example, the item responses were coded as zero (incorrect) or one
(correct).
In PISA 2003, each student was given a test booklet with clusters of items.
Each mathematics item appeared in the same number of test booklets. This
(linked) incomplete design makes it possible to construct a scale of mathe-
matical performance using item response theory where each student has a
score on this scale representing his or her estimated ability. Variations in stu-
dent ability within the Netherlands are investigated using various background
variables. A total of 3,829 students across 150 schools were questioned.
Multiple Imputation
The multiple imputation technique (Rubin, 1987, Little and Rubin, 2002) can
be used to handle the uncertainty regarding the ability parameter estimates.

166
6 Multilevel Item Response Theory Models
Table 6.1. Parameter estimates of linear multilevel and MLIRT models using explanatory information at the student and school
levels.
Linear Multilevel Model
MLIRT Model
Empty Model
Model M1
Empty Model
Model M1
Mean
SD
Mean SD
HPD
Mean
SD
Mean SD
HPD
Fixed Eﬀects
γ00 Intercept
−.058
.052
−.312 .080 [−.474, −.164]
−.064
.060
−.326 .096 [−.525, −.146]
Student variables
γ10 ISI-NV
.412 .017
[.377, .446]
.459 .020
[.420, .496]
γ20 SES
.217 .019
[.180, .254]
.248 .021
[.205, .290]
γ30 Female
−.160 .035 [−.226, −.095]
−.181 .038 [−.257, −.107]
School variables
γ01 End
.464 .090
[.285, .641]
.489 .108
[.276, .699]
Random Eﬀects
Within schools
σ2
θ Residual variance
.813
.029
.578 .019
[.542, .612]
.766
.026
.472 .019
[.436, .510]
Between schools
τ 2
00 Intercept
.216
.039
.117 .022
[.076, .162]
.299
.051
.183 .031
[.124, .244]
Information Criteria
−2 log-likelihood
5587.94
4852.30
6027.29
4978.83
DIC (pD)
5752.15(82.11)
5016.28(81.99)
6122.08(94.79)
5074.47(95.65)

6.6 Applications of MLIRT Modeling
167
When using plausible values, the standard errors of the ability estimates can
be taken into account when estimating the other multilevel model parameters.
In PISA 2003, the ability parameters were replaced by simulated values.
The so-called plausible values were drawn randomly from the posterior distri-
bution of the ability parameters given the response patterns and explanatory
information. The posterior density from which plausible values were sampled
is given by
p
 θ | y; ξ, β, σ2
θ

∝
Z
p (y | θ; ξ) p
 θ; β, σ2
θ

dθ,
(6.31)
where the item parameters and parameters of the population’s ability distribu-
tion are known. A subsample of respondents from diﬀerent countries, referred
to as the international calibration sample, were used to estimate the inter-
national item parameters. The estimated international item parameter values
were assumed to hold in each country. The explanatory variables in the pos-
terior distribution of abilities are known as conditioning variables, and ﬁve
variables were used: booklet number, gender, mother’s occupation, father’s
occupation, and school mean mathematics score.
For each student, ﬁve plausible values for the mathematical literacy do-
main were drawn randomly from the posterior density in Equation (6.31).
This posterior density does not depend on the missing-data indicator (in this
particular case, all the ability values are missing), which indicates that the
multiple imputations are appropriate under the assumption of ignorability.
Together with the fact that the multiple imputations are independent sam-
ples, Schafer (1997) characterized such plausible values as Bayesianly proper
based on Rubin’s (1987) deﬁnition for proper multiple imputations.
Once the plausible values are obtained, the complete data can be analyzed
in a second phase. An important advantage of multiple imputation inference
is the temporal separation of handling the missing-data problem and ana-
lyzing the complete data. The missing-data problem is conﬁned entirely to
the ﬁrst phase. One good set of plausible values can solve the missing-data
problem for many future analyses. On the other hand, the two phases, impu-
tation and analysis, are distinct, and this may lead to inconsistencies when
the imputation model and the model for analysis are based on diﬀerent as-
sumptions. When the model for analysis assumes more than the imputation
model, then correct inferences are made when the additional assumption of
the analyst’s model is true. The inferences derived from the imputed data will
be valid, but the plausible values may contain additional uncertainty since
the imputation model is more general than the model for analysis. When the
imputation model is more restrictive, valid inferences are made when the ad-
ditional restriction is true. In the case where the additional assumption of the
imputation model is false, incorrect inferences are made. Plausible values that
are drawn from an erroneous model can lead to erroneous conclusions.
The imputation model in Equation 6.31 has some potential weaknesses
(see, e.g., Goldstein, Bonnet and Rocher, 2007). The item parameters and

168
6 Multilevel Item Response Theory Models
population parameters are assumed to be known, and therefore the plausible
values drawn do not reﬂect any uncertainty with respect to the corresponding
estimated values. Estimated international item parameters instead of nation-
speciﬁc item parameters were used in the imputation model. The diﬀerence is
that the international item parameters are based on the restrictive assump-
tion that all items function equally across nations, whereas nation-speciﬁc
item parameters are assumed to function equally within that nation. It is not
likely that all international item characteristics apply to each individual in
the cross-national PISA survey. It may be possible that items function dif-
ferently across groups when, for example, cross-national and/or cross-cultural
response heterogeneity is present. A more realistic assumption is to assume
that within each nation some of the item characteristics may deviate from
the international item characteristics. This topic will be further discussed in
Chapter 7. Finally, the abilities were assumed to be independent given the
conditioning variables. However, the students were nested in schools, and the
performances from students of the same school are likely to be correlated since
they share common experiences and have the same teachers and teaching pro-
grams. It is likely that, given the conditioning variables, the drawn students’
plausible values are still correlated.
The reported plausible values from the PISA study were used to construct
ﬁve complete-data sets that were analyzed with a linear multilevel model.
The posterior moments of the model parameters can be estimated from a rel-
atively small number of draws since the complete-data posterior is based on
multivariate normality given that the fraction of missing data is not too large
(Little and Rubin, 2002). The complete-data inferences are combined to esti-
mate the structural multilevel model parameters. That is, for each parameter
of interest, the posterior mean and variance are estimated from the complete-
data posterior distribution. The multiple imputations are used to average over
the missing ability values and to approximate the posterior mean and vari-
ance of the marginal posterior distribution. This way, the posterior mean and
variance of the ﬁxed eﬀects parameters γ given M = 5 plausible vectors θ(m)
(m = 1, . . . , 5) are estimated by
E
 γ | y; ξ, β, σ2
= E
 E (γ | y, θ) | y; ξ, β, σ2
=
Z Z
γp (γ | y, θ) p
 θ | y; ξ, β, σ2
dγdθ
≈M −1 X
m
Z
γp

γ | y, θ(m)
dγ
≈M −1 X
m
E

γ | y, θ(m)
= γ
and, given
 ξ, β, σ2
,

6.6 Applications of MLIRT Modeling
169
Var (γ | y) = E (Var (γ | y, θ) | y) + Var (E (γ | y, θ) | y)
≈M −1 X
m
Var

γ | y, θ(m)
+
1
M −1
X
m
(ˆγm −γ) (ˆγm −γ)t
≈V γ + B,
where ˆγm = E
 γ | y, θ(m)
, V γ denotes within-imputation variance (the av-
erage of the complete-data variance estimates), and B denotes the between-
imputation variance (the variance of the complete-data point estimates).
When M is small, an improved approximation of the posterior variance is ob-
tained by V γ + (1 + M −1)B. The fraction of missing information is estimated
by the ratio of estimated between-imputation variance to total variance; that
is,
(1 + M −1)B
V γ + (1 + M −1)B .
The MLIRT model was used to analyze the Dutch item response data
(which were also used for generating the plausible values) given background
information at the student and school levels. The MLIRT model consisted of a
two-parameter item response model and a structural multilevel model for the
ability parameters. In the MLIRT model analysis, nation-speciﬁc item param-
eters were estimated simultaneously with the other MLIRT model parameters
using MCMC scheme 4. In contrast to the multiple-imputation method, the
uncertainty of the item parameter estimates is taken into account in estimat-
ing the other model parameters. For each vector of plausible values, the linear
multilevel parameters were also estimated with MCMC using the structural
multilevel part of scheme 4.
In Table 6.2, the empty structural multilevel parameter estimates are pre-
sented under the MLIRT model and the linear multilevel model. The vectors
of plausible values were standardized and the MLIRT model was identiﬁed
by standardizing the ability scale.2 This makes the results under both models
directly comparable. It follows that the parameter estimates and standard de-
viations of both empty model analyses are quite similar. The plausible values
generated contain the uncertainty of the ability estimates, and valid inferences
can be made from the linear empty multilevel analysis using multiple impu-
tations. The estimated intraclass correlation coeﬃcient is around 59%, which
is the proportion of variation in ability estimates explained by the grouping
of students in schools. This proportion is high and above the OECD average.
Goldstein (2004) also found a huge diﬀerence in explained variation by school
across countries, with many countries at 50% or more. It is not clear why such
high intraclass correlation coeﬃcients are found for the PISA data.
2 In PISA 2003, the Dutch overall performance in mathematics was measured on
a scale with mean 542 and standard deviation of around 92, whereas the metric
for all participating countries had a mean of 500 and standard deviation of 100.

170
6 Multilevel Item Response Theory Models
To investigate diﬀerences in performance between schools and the eﬀects of
student-level and school-level factors on a student’s ability, several background
characteristics were incorporated in the multilevel model. According to the
PISA 2003 study, the following student characteristics explained variation in
performance: gender, place of birth (Netherlands or foreign), language (Dutch
or speaks a foreign language most of the time), and index of economic, social,
and cultural status. The school’s mean index of economic, social and cultural
status is used as an explanatory variable for the random intercept. In Table
6.2, the results of the linear multilevel analysis using plausible values and the
MLIRT analysis are reported under model M1.
It can be seen that the estimated standard deviations are similar. The
estimated (level-3) eﬀect, γ01, is slightly larger under the MLIRT model. The
plausible values were generated without taking the nested structure of the
data into account, which might explain this underestimated school eﬀect. In
the MLIRT analysis, the ability parameters are re-estimated with a multilevel
part that includes covariates that also accounts for the nested structure of the
observed data. The posterior mean estimates of the (level-2) ﬁxed eﬀects are
comparable. Under the linear multilevel analysis, the explanatory variables
explain 6.9% of the student-level variance and 18.6% of the school-level vari-
ance, and under the MLIRT analysis, 6.4% of the student-level variance and
19.6% of the school-level variance.
From the MLIRT analysis it follows that the male students perform slightly
better than the females. The native speakers also perform better than non-
native speakers with a migrant background, taking account of socioeconomic
diﬀerences between students and between schools. It can be concluded that
students from more advantaged socioeconomic backgrounds generally perform
better.
Only the diﬀerence in DICs for the MLIRT and the linear multilevel models
can be compared since the deviance term does not contain the standardizing
factor. The DICs are based on the log-likelihood of the multilevel model. The
diﬀerences in DIC values are comparable: 279.01 for the MLIRT analysis and
282.15 for the linear multilevel analysis. This indicates that both modeling
techniques lead to a comparable amount of statistical evidence for selecting
model M1 over the empty model. The log-likelihood and DIC estimates under
the linear multilevel model are the averaged estimates since single estimates
were obtained for each vector of plausible values. In the MLIRT analysis, the
ability parameter is integrated out using its marginal posterior distribution,
and in the linear multilevel analysis the integration is approximated by taking
the average over the outcomes based on ﬁve vectors of plausible values. This is
a rough approximation and leads to lower DIC estimates and higher estimates
of the number of eﬀective parameters. In this case, more draws of plausible
values could drastically improve the estimates. Note that under the linear
multilevel analysis, the averaged log-likelihood and DIC estimates were based
on the same set of plausible values, so these diﬀerences in DIC values remain
interesting.

6.6 Applications of MLIRT Modeling
171
Table 6.2. PISA (Dutch) 2003: Parameter estimates of the linear multilevel model with multiple imputations and the MLIRT model
using explanatory information at the student and school levels.
Linear Multilevel Model
MLIRT Model
Empty Model
Model M1
Empty Model
Model M1
Mean
SD
Mean SD
HPD
Mean
SD
Mean SD
HPD
Fixed Eﬀects
γ00 Intercept
−.039
.065
.022 .063
[−.103, .142] −.037
.065
.017 .062
[−.103, , 139]
Student variables
γ10 Student is female
−.166 .024 [−.208 −.126]
−.163 .026 [−.216, −.112]
γ20 Student is foreign born
−.281 .050 [−.371, −.196]
−.275 .055 [−.384, −.168]
γ30 Student speaks foreign language
most of the time
−.226 .047 [−.317 −.137]
−.226 .058 [−.342, −.116]
γ40 Index of economic, social, and
cultural status
.143 .015
[.115, .169]
.152 .017
[.119, .184]
School variables
γ01 Mean index of economic, social,
and cultural status
.359 .156
[.048, .654]
.391 .156
[.084, .701]
Random Eﬀects
Within schools
σ2
θ Residual variance
.418
.019
.389 .013
[.371, .407]
.425
.013
.398 .012
[.374, .422]
Between schools
τ 2
00 Intercept
.609
.075
.496 .061
[.384, .620]
.606
.074
.487 .061
[.375, .610]
Information Criteria
−2 log-likelihood
8337.44
8045.33
8451.60
8171.71
DIC (pD)
8643.02(152.79)
8360.87(157.77) 8659.83(104.12)
8380.82(104.55)

172
6 Multilevel Item Response Theory Models
6.6.3 School Eﬀects in the West Bank: Covariate Error
Up to now, the MLIRT models presented have deﬁned a nonlinear relationship
between response variables and explanatory variables, where the explanatory
variables were assumed to be measured without an error. However, often it is
not possible to measure all relevant explanatory variables accurately. Assess-
ing the measurement errors is important since errors in explanatory variables
can bias the association between the response and explanatory variables. Mea-
surement errors in one covariate can also bias the link between a response vari-
able and other covariates that are measured without an error (Fuller, 1991).
A comprehensive discussion of linear and nonlinear measurement error mod-
els can be found in Fuller (1987) and Carroll, Ruppert and Stefanski (1995),
respectively.
Assume that a true or exact predictor is a latent variable such that its re-
alizations cannot be observed directly. Let the latent explanatory variable be
measured by items whose observations can be observed. In the measurement
error literature, this operationally deﬁned true explanatory variable is often
referred to as a gold standard (Carroll et al., 1995, p. 11), which refers to the
best way of measuring the true latent variable in practice. In this application,
educational leadership is considered to be a latent explanatory variable at the
school level. Scheerens, Glas and Thomas (2003) deﬁned educational leader-
ship as one of the ﬁve process indicators of school functioning, and its impact
on student achievement is often investigated in school eﬀectiveness research.
The MLIRT model is extended to deal with the error in the measurement
of educational leadership, which is commonly ignored in school eﬀectiveness
research, and to demonstrate the use of latent explanatory variables.
School Leadership and Math Achievement
The eﬀect of school leadership on student math achievement was investigated
using data from a school eﬀectiveness study in the West Bank (Shalabi, 2002).
A stratiﬁed sample of 119 schools ensured that all school types and all geo-
graphical districts of the West Bank were represented. The average number of
students per class was 28, with a minimum of 10 and a maximum of 46 stu-
dents. A total of 3,384 grade seven students were randomly selected from the
selected schools. The math achievement of grade seven students was measured
using a math test consisting of 50 dichotomously scored items (correct/incor-
rect). A test of 25 ﬁve-point Likert items was taken by teachers and school
principals to measure educational leadership. The ﬁrst three response cate-
gories were collapsed since a minimal response was observed in the two lowest
categories.
A structural multilevel model was used to deﬁne a relationship between
educational leadership ζ and math achievement θ and to account for the fact
that students were nested in schools. The MLIRT model consists of two mea-
surement models: a two-parameter normal ogive model for measuring math

6.6 Applications of MLIRT Modeling
173
achievement using the students’ item responses and a graded response model
for measuring educational leadership using teachers’ item responses. Let Z
denote the latent responses to the math items (Equation (4.7)) and V the
latent responses to the educational leadership items (Equation (4.25)). In the
latent response formulation, the MLIRT model can be stated as
Zijk = akθij −bk + ϵijk (student level)
Vjk = λkζj + νjk
(school level)

Measurement part
θij = β0j + eij
(student level)
β0j = γ00 + γ01ζj + u0j (school level)

Structural part
where the error terms in the measurement part are independently standard
normally distributed and the error terms in the structural multilevel part are
independently normally distributed with mean zero and variance σ2
θ and τ 2,
respectively. For the two-parameter response model, a truncated multivari-
ate normal prior was used for the discrimination and diﬃculty parameters
(Chapter 2). For the graded response model, the threshold parameters were
assumed to be order-restricted and uniformly distributed, and the discrimina-
tion parameters were assumed to have a common positively truncated normal
prior.
The MLIRT model can be identiﬁed by ﬁxing the scale of each latent
factor. The scale of the latent factors math and educational leadership were
identiﬁed by restricting the mean to zero and the variance to one. Note that
this MLIRT model allows for mixed response types (binary responses to the
math items and ordinal responses to the educational leadership items). Fox
and Glas (2003) deﬁned an MCMC scheme for simultaneously estimating all
parameters. This MCMC scheme is an extension of scheme 4, and it includes
the sampling of latent explanatory variable values.
Table 6.3 gives the structural model parameter estimates of an empty
MLIRT model and an MLIRT model, denoted as M1, with level-2 variable
female (male=0, female=1) and level-3 latent variable educational leadership.
It follows that about 49% of the variance in student achievement is explained
by school diﬀerences. The between-school variance is relatively high for a
developing country. In developing countries, around 40% of the variance in
student achievement is accounted for by schools (Scheerens et al., 2003).
From the estimates of model M1 it follows that female students performed
better than male students. There are only 24 schools with male and female
students (48 schools have only male students and 47 schools have only female
students). As a result, the level-1 predictor female only reduced the between-
school variation and not the within-school variation.
The factor educational leadership has a signiﬁcant positive eﬀect on stu-
dent math achievement after adjusting for gender diﬀerences. This is not sur-
prising given that the deﬁned contents of the educational leadership concept
were among other things setting periodic exams, identifying talented students,
and making sure that students acquire basic skills.

174
6 Multilevel Item Response Theory Models
Table 6.3. School eﬀectiveness in the West Bank: Parameter estimates of the
MLIRT model with latent explanatory variable educational leadership.
Empty Model
Model M1
Mean
SD
Mean SD
HPD
Fixed Eﬀects
γ00 Intercept
.003
.066
−.079 .072 [−.225, .058]
Student variable
γ10 Female
.169 .065
[.040, .292]
School variable
γ01 Educational leadership
.167 .066
[.036, .294]
Random Eﬀects
Within schools
σ2
θ Residual variance
.515
.014
.514 .015
[.487, .544]
Between schools
τ 2
00 Intercept
.500
.069
.456 .062
[.344, .581]
Information Criteria
−2 log-likelihood
14570.11
14329.75
6.6.4 MMSE: Individual Trajectories of Cognitive Impairment
The Mini-Mental State Examination (MMSE) is a widely used screening in-
strument for the detection of cognitive impairment. The test was ﬁrst de-
scribed by Folstein, Folstein and McHugh (1975) as a method for grading the
cognitive state. Originally it was used as a brief objective assessment of ﬁve
cognitive concepts (orientation, attention, registration, recall, and language).
Mood or thought disorders are not assessed, and therefore it was called mini.
The beneﬁts of the MMSE are its brevity and that it covers many diﬀerent
domains. In general, the MMSE is a brief screening test that quantitatively
assesses the level of cognitive impairment and for repeated measurements as-
sesses cognitive changes occurring over time.
The MMSE has gained increasing popularity for measuring neurobehav-
ioral deﬁcits and the cognitive abilities of elderly patients. Folstein et al. (1975)
suggested the presence of dementia in persons with scores of less than 23 out of
30 and at least eight years of education. Depending on the sample, a ceiling ef-
fect might be present when a patient scores perfect who is often well-educated
but meets criteria for dementia. Tombaugh (1992) reviewed the psychomet-
ric properties and utilities of the MMSE. The main outcome was that the
MMSE possessed moderate to high reliability coeﬃcients, was shown to be
sensitive to cognitive impairments in persons suﬀering from Alzheimer’s dis-
ease, and reﬂected cognitive decline, which is typical for dementia patients.
On the other hand, criticisms included (1) that the MMSE showed limited

6.6 Applications of MLIRT Modeling
175
ability to discriminate people not demented from people with mild dementia,
(2) language items were too easy to identify mild language deﬁcits, and (3)
the MMSE scores were aﬀected by age, education, and cultural background
but not gender. Schulz-Larsen, Kreiner and Lomholt (2007a, 2007b) explored
the properties of the MMSE using a mixture Rasch model to detect item char-
acteristic diﬀerences across groups (Rost, 1990; Rost and von Davier, 1995).
The MMSE items were expected to act diﬀerently in the two groups of elderly
with and without cognitive impairments.
MMSE data from a study of the OPTIMA3 cohort (Oxford Project to
Investigate Memory and Ageing) are analyzed. The data cover 668 partici-
pants, who were questioned on diﬀerent measurement occasions. On several
occasions, 26 discrete MMSE item responses (scores for item 15 have been di-
chotomized) and background information were observed. Let observation Yijk
denote the response to item k of subject i on measurement occasion j. For
each occasion j, observed MMSE responses of subject i are used to estimate
the cognitive impairment θij using a two-parameter item response model. Low
scores are assumed to correspond with severe cognitive impairment.
The participants in the naturalistic follow-up study can be classiﬁed as
patients (who are suﬀering from dementia) and controls, where some convert
from controls to patients. Most of the patients show increasing cognitive im-
pairment over time, which is characteristic of dementia and is seen as a risk
factor for Alzheimer’s disease. In Table 6.4, the demographic information of
the study participants is given, including a distribution of the averaged sum
scores across measurement occasions. It can be seen that 302 participants
score on average at least 24 items correct and do not suﬀer from cognitive
impairment. Therefore, the distribution of sum scores is likely to be a mixture
of a patient’s and a control’s sum score distributions.
Figure 6.3 presents a so-called spaghetti plot of some participants’ observed
sum scores. For each participant selected, the observed sum scores are plotted
and connected by the follow-up time in years. The baseline time point zero
corresponds to the ﬁrst time a participant is confronted with the MMSE.
The darker lines represent participants that show an increase in cognitive
impairments and the lighter dotted lines represent participants who show no
variations in cognitive impairment over time. The plot suggests that there is a
general decline in sum scores for a group of patients besides the considerable
individual heterogeneity and no decline for a group of controls.
This illustrates that the classiﬁcation of subjects as patients or controls is
important since subjects in one group do not suﬀer from cognitive impairment,
whereas subjects from the other group suﬀer from mild or severe cognitive im-
pairment. Class membership may have diﬀerent consequences and antecedents
in diﬀerent classes, which makes it important to distinguish among subjects in
the diﬀerent groups. It is to be expected that the average trajectories of cog-
3 Data were supported by the NIHR Oxford Comprehensive Biomedical Research
Centre.

176
6 Multilevel Item Response Theory Models
Table 6.4. Demographic information for the study participants.
Participants (N = 668)
Gender
Male
329
Female
339
Age
Start Mean
50–59
55
41
60–69
195
149
70–79
323
315
80–89
149
215
90–100
9
14
Average Sum Score
24–26
302
22–23
66
20–21
47
18–19
61
15–17
66
< 14
126
nitive impairment in the two groups are diﬀerent and that there are diﬀerent
individual variations around the average trajectories.
Mixture MLIRT Modeling
The response data (level 1) are nested within measurement occasions (level
2), which are nested within subjects (level 3). An item response model is de-
ﬁned at level 1 for the measurement of occasion-speciﬁc impaired cognitive
functioning. At level 2, the measured cognitive impairments are nested within
subjects. This level accounts for the longitudinal character of the observed
data. At level 3, heterogeneity among subjects is captured via random eﬀects.
A (three-level) MLIRT model is deﬁned to account for the nested structure of
the data. The MLIRT model can handle the varying number of measurements
across participants, and it can handle complicated factors such as measure-
ments not measured on the same time points and measurements not uniformly
distributed across subjects.
To identify a group of patients and a group of controls, to model depen-
dencies between the response outcomes, and to allow for diﬀerent individual
trajectories of cognitive impairment across the groups, a latent class model
(e.g., Goodman, 1974; Lazarsfeld and Henry, 1968) or mixture model (e.g.,
McLachlan and Peel, 2000) is deﬁned. Therefore, the structural part of the
MLIRT model equals a two-component mixture model. That is, the popula-
tion distribution of cognitive impairments is deﬁned to be a mixture of two

6.6 Applications of MLIRT Modeling
177
0
2
4
6
Follow-up Time (years)
0
5
10
15
20
25
MMSE Sum Scores
Fig. 6.3. Spaghetti plot of some participants’ observed MMSE sum scores.
components presenting the population distributions of cognitive impairments
of patients and controls.
Let the distribution of cognitive impairments in a population of elderly
reﬂect the mixture of patients and controls in the population and be given by
p (θij | Ω) =
2
X
g=1
πigp (θij | Ωg) ,
where Ωg are the structural multilevel parameters of group g = 1, 2.
Let indicator or class membership variable Gi indicate if subject i suf-
fers from cognitive impairment (Gi = 1) and is labeled patient or does not
(Gi = 2) and is labeled control. Each indicator variable Gi is distributed un-
conditionally as Bernoulli with success probability πi1, and a conjugate beta
prior is speciﬁed for the success probability. Then, the conditional posterior
probability that subject i is classiﬁed as a patient equals
P (Gi = 1 | yi, θi, Ω1, πi) =
πi1
Qni
j=1 p (yij | θij) p (θij | Ω1)
P
g=1,2 πig
Qni
j=1 p (yij | θij) p (θij | Ωg).
The introduction of the class membership variables Gi supports the compu-
tation of the posterior probability that a participant is classiﬁed as a patient

178
6 Multilevel Item Response Theory Models
given item response data and the computation of the proportion of partici-
pants with cognitive impairment.
Diebolt and Robert (1994) introduced a Gibbs sampling algorithm for
the estimation of normal mixtures with a prespeciﬁed number of mixture
components. In their MCMC implementation, class membership variables Gi
are considered to be missing data and in each MCMC iteration samples for the
missing data are generated. The other model parameters, including the mixing
proportions, are sampled given a realization of class memberships. MCMC
scheme 4 can be extended with a step for sampling class memberships and a
step for sampling the mixing proportions. The other sampling steps remain
the same but may depend on the class membership variables.
Identiﬁability
The items are assumed to function in the same way for controls and patients.
It is also assumed that the items function in the same way across measurement
occasions. That is, the MMSE items are assumed to be time-invariant and in-
variant across the two latent classes. The invariance assumptions are needed
to ensure that the estimated occasion-speciﬁc cognitive impairments are mea-
sured on a common scale across latent classes. Given class memberships, the
latent scale is identiﬁed by ﬁxing its mean and variance.
A ﬁnite mixture model is not identiﬁed since the distribution of the data is
unchanged if the class membership labels are permuted. In the two-component
mixture, the labels for patients and controls can be switched in an MCMC
iteration, which leads to a nonidentiﬁed solution. To avoid any ambiguity, the
mixture components can be identiﬁed by ﬁxing the means of the components
to be in nondecreasing order. This way, the mean cognitive impairment of
the patients is restricted to be smaller than the mean cognitive impairment
of the controls. It is also possible to identify the mixture model by specifying
an order restriction on the mixing proportions. The mixture MLIRT model is
identiﬁed by ﬁxing the mean and variance of the latent scale and specifying
an order restriction on the mixture component means.
OPTIMA Cohort: Modeling Individual Trajectories
Interest will be focused on the part that models the individual trajectories
of cognitive impairment. First, consider a mixture MLIRT model M0 where
the (occasion-speciﬁc) cognitive impairments are nested within subjects that
are measured via a two-parameter item response model. The within-subject
residual variation and the between-subject variation in cognitive impairment
are assumed to be common across classes. This leads to the two-component
mixture density of cognitive impairments
p
 θij | γ00, τ 2
00, σ2
= πi1φ
 γ00,1 + ui0, σ2
+ πi2φ
 γ00,2 + ui0, σ2
,

6.6 Applications of MLIRT Modeling
179
where ui0 ∼N
 0, τ 2
00

. This structural mixture part enables the computation
of subject-speciﬁc class probabilities and average class means of cognitive
impairment.
Table 6.5 gives the parameter estimates of the two-component mixture part
of M0. It can be seen that the mean cognitive impairment of the patients is
much smaller than that of the controls. The within-subject residual varia-
tion is smaller than the between-subject variation in cognitive impairment,
and around 63% of the variation in cognitive impairment is attributable to
subjects.
In model M0, it is assumed that the between-subject variation is com-
mon across groups. However, more variation in cognitive impairment is to be
expected in the patient group. The patients suﬀer from diﬀerent degrees of
cognitive impairment, whereas the controls display almost no decrease in cog-
nitive functioning across measurement occasions, and less variation between
subjects is to be expected. The follow-up times are included to model the rate
of change in cognitive impairment across time. It is not likely that the rate
of change is similar for all subjects. Therefore, the individual time trends are
treated as random eﬀects. The random eﬀects of follow-up time on cognitive
impairment are allowed to vary across subjects and groups. This means that
the average eﬀect of follow-up times (Fup) may diﬀer across groups and that
the individual variations across the averages may diﬀer.
This is included in model M1, with a two-component mixture density of
cognitive impairment given by
p
 θij | γ, T, σ2
= πi1φ
 µij,1, σ2
+ πi2φ
 µij,2, σ2
,
(6.32)
where
µij,g = βi0,g + βi1,gFupij,
βi0,g = γ00,g + ui0,g,
βi1,g = γ10,g + ui1,g,
and ui,g ∼N (0, Tg), with Tg a diagonal matrix with elements τ 2
00,g and τ 2
11,g
for g = 1, 2.
In Equation (6.32), a growth mixture model is presented where individ-
ual trajectories of cognitive impairment are modeled. The random intercept
parameters deﬁne the starting point at the subject’s ﬁrst measurement occa-
sion, and the random slope parameters deﬁne the degree of change in cognitive
impairment over time. The ﬁxed eﬀects parameters deﬁne the average initial
population level and average population trend.
In Table 6.5, the average intercept values diﬀer, and it can be seen that the
average score of the controls on the ﬁrst measurement is much higher. The
initial scores of cognitive impairment are comparable for the controls, but
there is considerable heterogeneity in the initial scores of the patients. The
average population slope is around zero for the controls. This means that their

180
6 Multilevel Item Response Theory Models
Table 6.5. MMSE: Parameter estimates of mixture MLIRT model M0 and M1.
Mixture MLIRT M1
Mixture MLIRT M2
Patients
Controls
Patients
Controls
Mean SD
Mean SD
Mean SD
Mean SD
Fixed Eﬀects
γ00 Intercept
−.998 .037
.689 .030 −.332 .037
.913 .012
Time variables
γ10 Follow-up time
−.274 .013 −.007 .004
Random Eﬀects
Within individual
σ2
θ Residual variance
.133 .003
.133 .003
.043 .001
.043 .001
Between individual
τ 2
00 Intercept
.211 .015
.211 .015
.471 .038
.016 .003
τ 2
11 Follow-up time
.047 .004
.002 .000
level of cognitive impairment remains constant across time. A negative aver-
age trend of −.274 was estimated for the patients. The estimated population
variation for the slope equals .047. Thus, there is considerable heterogeneity
in the patients’ change in cognitive impairment across time.
In Figure 6.4, the estimated individual random eﬀects are plotted for the
subjects of both groups. The circles are estimates of participants (38% of
the subjects) that are classiﬁed as controls with 95% reliability. The squares
are the estimates of participants (53% of the subjects) that are classiﬁed as
patients with 95% reliability. The triangles are the estimates of participants
(9% of the subjects) that could not be classiﬁed with 95% reliability. In most
cases, they are controls, with high initial values, who have converted.
It can be seen that there is almost no between-subject variation in initial
scores (horizontal distance) and individual trends (vertical distance) for the
controls. This group of subjects shows almost no decline over time. The pa-
tients show a lot of variation in initial scores and trends, where some have
high initial scores but show severe cognitive impairment over time (lower
right-hand corner). Patients with below average initial scores show mild to
severe cognitive impairment over time. Patients with low scores on the ﬁrst
measurement occasion show severe cognitive impairment over time.
A few subjects (upper-middle part) scored below average on the ﬁrst mea-
surement occasion but improved their performance over time. In Figure 6.3,
it can be seen that some subjects display an improvement in sum scores.
These participants have positive individual trend eﬀects and low starting val-
ues. They are classiﬁed as patients since their initial scores diﬀer too much
from the average initial level of the controls. Other subjects with initial scores

6.7 Summary and Further Reading
181
around zero showing a relatively small positive or negative improvement over
time are also classiﬁed as patients. In a subsequent analysis, a third latent
class could be introduced to capture subjects that show an improvement in
their performance over time.
-2
-1
0
1
2
Initial Score
-1.0
-0.8
-0.6
-0.4
-0.2
0.0
0.2
Individual Trend
Patients
Controls
Not Classified
Fig. 6.4. Estimated random eﬀects of initial levels and linear trends of cognitive
impairment.
6.7 Summary and Further Reading
A grouping of respondents in clusters leads to dependencies between students
due to the fact that a group of individuals share common interests, beliefs,
and behaviors. The item response modeling framework is extended with a
multilevel population distribution for the ability parameter to account for such
a hierarchical data structure. The MLIRT model discussed can be viewed as
a nonlinear mixed eﬀects model with a linear multilevel model for the ability
parameter that has a nonlinear relationship with the observed multivariate
discrete response data. The class of generalized linear mixed eﬀects models
contains the Rasch version of the MLIRT model but with speciﬁc restrictions
on the prior distributions.
Fox (2001, 2003) discussed MCMC algorithms for simultaneously estimat-
ing all MLIRT model parameters. Several applications of MLIRT modeling

182
6 Multilevel Item Response Theory Models
are discussed in Fox (2004) and Fox and Glas (2003). More applications and
details about the MLIRT software for the statistical package R can be found
in Fox (2007). The assessment of MLIRT models is discussed in Fox (2005a).
The MLIRT model supports an improved estimate of a single ability using
the fact that similar estimates exist for other individuals. This also applies for
the estimation of eﬀects within individual groups. In the educational data ex-
amples, separate equations were obtained for the males and females, where the
estimated gender eﬀect was based on the weighted information across schools.
The estimation of a school-speciﬁc gender eﬀect can be based on weighted in-
formation across schools and the information from that school. This feature of
the multilevel modeling approach becomes particularly interesting when data
are sparse for the males or females and when male or female students are not
present in some schools (e.g., Braun, Jones, Rubin and Thayer, 1983). Besides
the advantage of the borrowing strength principle, the MLIRT model allows
the speciﬁcation of cross-level interactions where variables at one level aﬀect
relationships at another level. In the educational examples, item discriminat-
ing eﬀects can be evaluated for males and females.
Two other popular item response models also account for an additional
nesting of item responses: the testlet model (Section 5.5.2) and the hierar-
chical rater model. The hierarchical rater model for multiple ratings of test
items combines information from multiple raters to assess student perfor-
mance. Patz, Junker, Johnson and Mariano (2002) developed a hierarchical
rater model that accounts for the nested structure of the data where item re-
sponses are nested in examinees and raters (see also Verhelst and Verstralen,
2001). Conditional independence between raters’ ratings is assumed by condi-
tioning on an ideal rating, and conditional independence between ideal ratings
is assumed by conditioning on the examinees’ abilities.
There is extensive literature about multilevel models for continuous and
discrete outcome data. A more thorough overview of multilevel modeling is
given by Goldstein (2003), Hedeker and Gibbons (2006), Raudenbush and
Bryk (2002), and Snijders and Bosker (1999), among others. A fully Bayesian
multilevel analysis was ﬁrst presented by Hill (1965) and Tiao and Tan (1965).
A seminal contribution on Bayesian estimation of linear multilevel models
was given by Lindley and Smith (1972). Other important work on Bayesian
multilevel analysis includes Efron and Morris (1975), Dempster et al. (1981),
and Gelfand and Smith (1990). Gelman and Hill (2007) demonstrate the use
of WinBUGS and R for ﬁtting multilevel models.
The MLIRT model can be extended with latent explanatory variables at
diﬀerent hierarchical levels as shown in Section 6.6.3. Latent explanatory vari-
ables function in the same way as manifest explanatory variables, and (cross-
level) interaction eﬀects can also be deﬁned for latent covariates. An item
response modeling framework is described for unobserved continuous covari-
ates. A mixture modeling approach can be followed to handle discrete latent
covariates (Kuha, 1997). More about latent variable modeling for handling
measurement error in latent covariates can be found in, among others, Fox

6.8 Exercises
183
and Glas (2003), Muth´en (1992), Raudenbush and Bryk (2002), and Skrondal
and Rabe-Hesketh (2004).
The mixture MLIRT model described in Section 6.6.4 contains a two-
component mixture at level 3 for the grouping of subjects as patients or con-
trols. The classiﬁcation of multiple item observations within a measurement
occasion and multiple measurement occasions within subjects is directly ob-
served. Vermunt (2008) describes various latent class models for hierarchical
data where mixtures are deﬁned at diﬀerent levels of hierarchy. Applications
of (general) growth mixture models for longitudinal data are described by
Muth´en and Shedden (1999) and Muth´en (2001), among others. Bayesian ap-
plications of mixture models can be found in Diebolt and Robert (1994), and
Gelman and King (1990), and see also Gelman et al. (1995) and McLachlan
and Peel (2000) and references therein.
6.8 Exercises
6.1. Assume an MLIRT model consisting of a two-parameter measurement
model for the latent response data (Equation (4.38)) and an empty multilevel
model for the ability parameters (Equations (6.4) and (6.5)).
(a) Show that the covariance structure of two latent responses to item k for
ﬁxed item parameters can be expressed as
Cov (Zijk, Zi′j′k′) =



a2
k
 σ2
θ + τ 2
00

+ ς for i = i′, j = j′, k = k′
a2
kτ 2
00
for i ̸= i′, j = j′, k = k′
0
for j ̸= j′,
where ς = 1 or ς = π2/3 when the latent response data are normally or
logistically distributed, respectively.
(b) Show that the correlation between latent responses to the same item is
given by
ρI =
τ 2
00
(σ2
θ+τ 2
00)+ςa−2
k
for i ̸= i′, j = j′, k = k′,
and interpret the result.
(c) The covariance structure of two latent responses to diﬀerent items for ﬁxed
item parameters can be expressed as
Cov (Zijk, Zi′j′k′) =



ak
 σ2
θ + τ 2
00

ak′ + ς for i = i′, j = j′, k ̸= k′
akτ 2
00ak′
for i ̸= i′, j = j′, k ̸= k′
0
for j ̸= j′.
(d) Show that the correlation between latent responses to diﬀerent items is
given by
ρI =
τ 2
00
(σ2
θ+τ 2
00)+a−1
k ςa−1
k′
for i ̸= i′, j = j′, k ̸= k′,
and interpret the result.

184
6 Multilevel Item Response Theory Models
6.2. Consider an empty multilevel model for the ability parameter (Equations
(6.4) and (6.5)).
(a) Argue that the value of τ 2
00 inﬂuences the relationship between the β0j.
(b) Substantiate the eﬀect of normal prior p
 β0j | γ00, τ 2
00 →∞

on the ex-
pected school eﬀect in Equation (6.19).
(c) Consider the prior p (β0j) ∝c, and derive the expected school eﬀect given
the abilities. (Box and Tiao, 1973, p. 379, called this locally uniform prior a
ﬁxed eﬀect prior.)
6.3. Consider an unrestricted MLIRT model M. Let MLIRT model M0 be
a restricted version in which ﬁxed eﬀects γ equal γ0. Assume a priori that
P (M) = P (M0). Let Ωdenote the set of MLIRT model parameters exclud-
ing γ.
(a) Show that the Bayes factor comparing M0 with M has the form
BF =
R
Ωp (y | Ω, M0) p (Ω| M0) dΩ
R
Ω
R
γ p (y | Ω, γ, M) p (Ω, γ | M) dγdΩ.
(6.33)
(b) Show that the denominator of Equation (6.33) can be expressed as
p (y | M) = p (y | Ω, γ0, M) p (Ω, γ0 | M)
p (Ω, γ0 | y, M)
.
(c) Show that the numerator of Equation (6.33) can be expanded as
p (y | M0) = p (γ0 | y, M)
Z
Ω
p (y | Ω, M0) p (Ω| M0) ·
p (Ω| y, γ0, M)
p (Ω, γ0 | y, M)dΩ.
(d) Use the results in (b) and (c) to express the Bayes factor as
BF = p (γ0 | y, M)
p (γ0 | M) E
 p (Ω| M0)
p (Ω| γ0, M) | y

,
(6.34)
where the expectation is with respect to p (Ω| y, γ0, M).
(e) Show that the Bayes factor in (d) equals the ratio
BF = p (γ0 | y, M)
p (γ0 | M)
(6.35)
when
p (Ω| γ = γ0, M) = p (Ω| M0) .
(f) Describe a method for estimating the ratio in Equation (6.35) using MCMC
scheme 4. (The special form of the Bayes factor in Equation (6.35) is from
Dickey, 1971; see also Equation (3.12). The generalized form in Equation
(6.34) is from Verdinelli and Wasserman, 1995.)

6.8 Exercises
185
6.4. (continuation of Exercise 6.1) Consider the problem of making predictions
with an MLIRT model.
(a) Discuss a strategy for generating predictive data yrep from respondents in
the dataset given y using draws from MCMC scheme 4.
(b) Discuss a strategy for generating predictive data yrep from respondents
in the dataset given y using a forward simulator (see Section 5.4.2).
(c) Let θsj denote the ability of an unsampled respondent s in a sampled
group j. Show that
 θsj
θj

∼N
xt
sjwjγ
xjwjγ

,
xt
sjTxsj + σ2
θ
xt
sjTxt
j
xjTxsj
xjTxt
j + σ2
θInj

.
(d) Describe a method for predicting θsj given estimates of the multilevel
model parameters (θj, γ, σ2
θ, T). (This prediction technique can be recognized
as the multilevel prediction rule of Afshartous and De Leeuw, 2005.)
(e) Discuss a method for predicting a response pattern of an unsampled re-
spondent s in a sampled group j given item parameters and using the result
in (d).
6.5. A convenient form of the posterior density of βj is to be derived given
p (βj | wj, γ, T) as the prior density (Equation (6.8)) and likelihood function
p
 θj | xj, βj, σ2
θ

(Equation (6.7)) with σ2
θ = 1.
(a) Show that the terms within the exponent of the posterior density can be
expressed as
D = (βj −wjγ)t T−1 (βj −wjγ) + (θj −xjβj)t (θj −xjβj)
= (βj −wjγ)t T−1 (βj −wjγ) +

βj −ˆβj
t
xt
jxj

βj −ˆβj

+

θj −xj ˆβj
t 
θj −xj ˆβj

,
(6.36)
where ˆβj =
 xt
jxj
−1 xt
jθj.
(b) Let the function D (βj) contain the terms of (6.36) that include βj. Set
the ﬁrst derivative of D (βj) to zero,
dD (βj)
dβj
= 0.
Solve this equation and show that ˜βj minimizes D (βj) where
˜βj =
 T−1 + xtx
−1 
T−1wjγ + xtxˆβj

.
(c) Show that

186
6 Multilevel Item Response Theory Models
D (βj) =

βj −˜βj
t  T−1 + xt
jxj
 
βj −˜βj

+ˆβ
t
jxtxˆβ
t
j + (wjγ)t T−1wjγ −˜β
t
j
 T−1 + xt
jxj
 ˜βj
=

βj −˜βj
t  T−1 + xt
jxj
 
βj −˜βj

+

ˆβj −˜βj
t
xt
jxj

ˆβj −˜βj

+

wjγ −˜βj
t
T−1 
wjγ −˜βj

.
(d) Derive the conditional posterior density of βj from (c).
6.6. Consider the conditional density of θij as deﬁned in Equation (6.24).
(a) Suppress the conditioning on (ξ, β, σθ) and show that the posterior prob-
ability of θl ≤θij ≤θu can be expressed as
P (θl ≤θij ≤θu | yij) =
Z 
Φ
θu −µθ
√Ωθ

−
Φ
θl −µθ
√Ωθ

p (zij | yij) dzij,
(6.37)
where µθ and Ωθ are deﬁned in Equations (6.25) and (6.26), respectively.
(b) For known values of θu and θl, show how to compute the posterior prob-
ability in (6.37) using MCMC scheme 4.
(c) Let θ(m)
ij
(m = 1, . . . , M) be draws from the marginal posterior distribution
of θij. Show how to estimate the posterior probability in (6.37) using the drawn
values. Does this estimate diﬀer from the estimate of (b)?
6.7. A credible region for the multivariate parameter β is deﬁned as
P (Cβ | y) =
Z
Cβ
p (β | y) dβ,
where Cβ does not need to be an interval. Assume that β(m)
q
(m = 1, . . . , M; q =
1, . . . , Q) is an MCMC sample from the (unimodal) marginal posterior distri-
bution.
(a) For Q = 1, an order statistics estimate of a (1 −α) credible interval
is
 β(x), β(x+(1−α)M)

, where β(x) is the xth smallest value in the ordered
sequence of β(m). Show how to estimate an equal-tailed interval and an HPD
interval using the order statistics estimator.
(b) For Q > 1, show how to estimate a credible region for β by estimating
the proportion of samples that fall simultaneously in all univariate credible
intervals.
(c) Argue that the bounds are conservative and that the estimated credible
region is restricted to be hyper-rectangular.

6.8 Exercises
187
6.8. Investigate the properness of a noninformative prior (see Section 2.2.1).
Assume a two-parameter response model with item parameters (a, b). Let
F denote a normal or logistic cumulative distribution function. A standard
normal prior is deﬁned for θi and an improper prior for the item parameters,
p (a, b) ∝QK
k=1 p(ak)p(bk) = QK
k=1 I(ak > 0).
(a) Show that the density of the observed data given item parameters equals
p (yi | a, b) =
Z ∞
−∞
K
Y
k=1
F (ηik)yik (1 −F (ηik))1−yik φ (θi) dθi,
where ηik = akθi −bk.
(b) Derive the following upper bound and lower bound: if Yi1 = 1,
p (yi | a, b) ≥F (−b1)
Z ∞
0
K
Y
k=2
F (ηik)yik (1 −F (ηik))1−yik φ (θi) dθi,
and if Yi1 = 0,
p (yi | a, b) ≥(1 −F (−b1))
Z 0
−∞
K
Y
k=2
F (ηik)yik (1 −F (ηik))1−yik φ (θi) dθi.
(c) When the responses to item 1 are all correct or all incorrect, prove that
Z ∞
0
N
Y
i=1
p (yi1 | a1, b1) p(a1)da1 ≥∞.
(d) What can be said about the improperness of the posterior densities
p (a, b | y) and p (a, b, θ | y)?
6.9. Consider the PISA 2003 data discussed in Section 6.6.2. Use the MLIRT
software (Fox, 2007) to explore relationships between background information
and students’ math abilities.
(a) Estimate the mean, standard deviation, and 95% HPD interval to sum-
marize the posterior density of the intraclass correlation coeﬃcient.
(b) Use the variable school’s mean index of economic, social, and cultural
status to explain variation in the mean school levels of math ability. Compute
the proportional reduction in variance at the school level.
(c) Estimate the main eﬀect of gender on math ability, and investigate whether
the eﬀect varies across schools, conditional on the school’s mean index of
economic, social, and cultural status.
(d) Deﬁne and estimate a cross-level interaction term to investigate whether
there are gender diﬀerences in the eﬀect of a school’s mean index of economic,
social, and cultural status on math ability.
(e) Deﬁne and estimate a level-2 interaction term to investigate whether there
are gender diﬀerences in the eﬀect of the index of economic, social, and cultural
status on math ability.

188
6 Multilevel Item Response Theory Models
6.9 Appendix: The Expected School Eﬀect
In Equation (6.22), a simpliﬁed expression for the conditional expected school
eﬀect, E (u0j | zj), is given without conditioning on other model parameters
for notational convenience. This expression is obtained by analytically per-
forming an inverse problem that is contained in the general expression of the
expected school eﬀect. This inverse problem equals
D−1 =
 Jnj ⊗τ 2aat
+
 Inj ⊗
 σ2
θaat + IK
−1 .
The expression for the inverse of a Schur complement, Equation (6.14), can
be used to obtain the inverted matrix. It follows that
D−1 = A−1 −A−1  1nj ⊗a
  1nj ⊗a
t A−1
τ −2 +
 1nj ⊗a
t A−1  1nj ⊗a
,
where
A−1 =

Inj ⊗
 σ2
θaat + IK
−1
= Inj ⊗

IK −
aat
σ−2
θ
+ ata

,
again using the matrix inverse expression of Equation (6.14).
The expression for D−1 is plugged into Equation (6.21). This leads to
E (u0j | zj) =

1t
nj ⊗τ 2at
D−1  zj −
  1nj ⊗a

γ00 −1nj ⊗b

=

1t
nj ⊗τ 2at
D−1˜zj.
(6.38)
Some tedious calculations need to be done to obtain the expression in Equation
(6.22). The matrix operations can be separated into three parts. The ﬁrst part
equals

1t
nj ⊗atτ 2
A−1˜zj =

1t
nj ⊗atτ 2
˜zj −



1t
nj ⊗atτ 2
aat
σ−2
θ
+ ata

˜zj
= τ 2

σ−2
θ
σ−2
θ
+ ata
 
1t
nj ⊗at
˜zj,
the second

1t
nj ⊗atτ 2
A−1  1nj ⊗a
  1nj ⊗a
t A−1˜zj =
 njτ 2ata
 
σ−2
θ
σ−2
θ
+ ata
2 
1t
nj ⊗at
˜zj,

6.9 Appendix: The Expected School Eﬀect
189
and the third
τ −2 +
 1nj ⊗a
t A−1  1nj ⊗a

= τ −2 + njata

σ−2
θ
σ−2
θ
+ ata

.
The three results are plugged into Equation (6.38), which leads to
E (u0j | zj) = τ 2

σ−2
θ
σ−2
θ
+ ata
  1nj ⊗a
t ˜zj −
njτ 2ata

σ−2
θ
σ−2
θ
+ata
2
τ −2 + njata

σ−2
θ
σ−2
θ
+ata


1t
nj ⊗at
˜zj
=

τ 2σ−2
θ
σ−2
θ
+ ata


1 −
njata

τ 2σ−2
θ
σ−2
θ
+ata

1 + njata

τ 2σ−2
θ
σ−2
θ
+ata




1t
nj ⊗at
˜zj
=


τ 2
σ2
θ+(ata)−1
1 +
njτ 2
σ2
θ+(ata)−1



1t
nj ⊗
 ata
−1 at
˜zj
=
 
njτ 2
σ2
θ + (ata)−1 + njτ 2
! 
1t
nj ⊗
 ata
−1 at
˜zj/nj
=
 
njτ 2
σ2
θ + (ata)−1 + njτ 2
!
ˆu0j,
where ˆu0j is the mean least squares estimate of u0j given the individual obser-
vations. This follows from Equation (6.18) when treating the within-individual
and within-school error components as one normally distributed error com-
ponent. That is,
ˆu0j =
 1nj ⊗a
t  1nj ⊗a
−1  1nj ⊗a
t ˜zj
=

1t
nj1nj ⊗ata
−1  1nj ⊗a
t ˜zj
=
X
i
1
nj
 ata
−1 at˜zij
=
X
i
1
nj
 ata
−1 at (zij −(aγ00 −b))
=
 ata
−1 at (zj −(aγ00 −b)) .

190
6 Multilevel Item Response Theory Models
6.10 Appendix: Likelihood MLIRT Model
An expression is derived for the MLIRT integrated likelihood considering a
two-parameter normal ogive model and the structural multilevel model as de-
ﬁned in Equations (6.2) and (6.3). Integrated likelihoods, where the likelihood
is marginalized with respect to the random eﬀect parameters, of other MLIRT
models can be derived in a similar way.
The idea is to obtain an expression of the likelihood for the augmented
data z and estimate the likelihood for the observed data via MCMC. The
likelihood of the observed data can be expressed as the integrated augmented
data likelihood; that is,
p
 y | γ, ξ, σ2
θ, T

=
Z
p
 z | γ, ξ, σ2
θ, T

dz.
Let Λ =
 γ, ξ, σ2
θ, T

. Interest is focused on the augmented data likelihood,
p (z | Λ) = p (z, θ, β | Λ)
p (θ, β | z, Λ)
= p
 z, θ | β, ξ, σ2
θ

p (θ | z, β, Λ)
p (β | γ, T)
p (β | z, Λ)
= p (z | β, Λ) p (β | γ, T)
p (β | z, Λ) .
(6.39)
First, the conditional augmented data likelihood is derived given the random
eﬀects parameters βj (the ﬁrst term on the right-hand side of (6.39)). Second,
the integrated augmented data likelihood is derived by dividing the conditional
posterior density of z, β given Λ by the conditional posterior density of β given
(z, Λ).
The ﬁrst step gives
p (z | β, Λ) =
Y
j
(2π)−Knj/2
Ωθ
σ2
θ
nj/2
exp
 
−1
2
X
i,k

(zijk + bk) −ak˜θij
2
!
exp
 
−1
2σ2
θ

˜θj −xjβj
t 
˜θj −xjβj
!
,
(6.40)
where ˜θij is normally distributed with mean µθ and variance Ωθ according to
Equations (6.25) and (6.26), respectively.
The second step requires the conditional density p(β | z, Λ). Therefore,
consider the MLIRT model presentation
Zj = θj ⊗a −1nj ⊗b + ϵj
= (xjβj + ej) ⊗a −1nj ⊗b + ϵj
= (xj (wjγ + uj)) ⊗a −1nj ⊗b + ej ⊗a + ϵj
= xjwjγ ⊗a −1nj ⊗b
|
{z
}
ﬁxed part
+ xjuj ⊗a + ej ⊗a + ϵj
|
{z
}
random part
,
(6.41)

6.10 Appendix: Likelihood MLIRT Model
191
where Zj = (Z1j1, . . . , Z1jK, . . . , ZnjjK)t is the stacked vector of nj individ-
ual augmented response vectors. The joint distribution of (Zj, βj)t given Λ
is multivariate normal, and from Equation (6.41) the covariance matrix of
(Zj, βj) can be obtained as
xjTxt
j ⊗aat + Inj ⊗
 σ2
θaat + IK

| xjT ⊗a
Txt
j ⊗at
|
T

.
Subsequently, the conditional distribution of βj | zj, Λ is normal with mean
˜βj = E (βj | zj, Λ) = wjγ +
 Txt
j ⊗at
D−1 ·
 zj −
 xjwjγ ⊗a −1nj ⊗b

(6.42)
and variance
Σ ˜βj = Var (βj | zj, Λ) = T +
 Txt
j ⊗at
D−1 (xjT ⊗a) ,
(6.43)
where D = xjTxt
j ⊗aat + Inj ⊗
 σ2
θaat + IK

.
The likelihood of the MLIRT model is obtained by performing the second
step using the conditional normal distribution of βj | zj, Λ. It follows that
p (zj | Λ) = p(zj | βj, Λ)p(βj | γ, T)
p(βj | zj, Λ)
(6.44)
= (2π)−Knj/2
Ωθ
σ2
θ
nj/2
|T|−1/2|Σ ˜βj|1/2 exp

−S

˜θj, ˜βj

/2

,
where
S

˜θj, ˜βj

=
X
i,k

(zijk + bk) −ak˜θij
2
+ σ−2
θ

˜θj −xj ˜βj
t 
˜θj −xj ˜βj

+

˜βj −wjγ
t
T−1 
˜βj −wjγ

(6.45)
and ˜θij is conditionally normally distributed with mean µθ (Equation (6.25))
and variance Ωθ (Equation (6.26)) for βj = ˜βj.

7
Random Item Eﬀects Models
Cluster-speciﬁc item eﬀects parameters are introduced that are assumed to
vary over clusters of respondents. The modeling of cluster-speciﬁc item param-
eters relaxes the assumptions of measurement invariance. Item characteristic
diﬀerences are simply allowed, and it is not necessary to classify items as be-
ing invariant or noninvariant. Tests and estimation methods are discussed for
item response models with random item eﬀects parameters.
7.1 Random Item Parameters
Thus far in this book, attention has been focused on structural models for the
person parameters for which the data are strictly hierarchically structured. A
typical example was given in Chapter 6, where a set of item responses belongs
to one student who belongs to one speciﬁc school. The additional nesting of the
responses in items leads to a cross-classiﬁed data structure. In general, an item
response model describes the relationship between responses and abilities,
with each observation deﬁned by the cross-classiﬁcation of persons and items.
The parameters of the item response model can then vary over persons and
items. Here, attention is focused on the item side of the model.
Prior beliefs about the item parameters need to be speciﬁed, and they are
presented in a prior probability distribution. The prior distribution can be
constructed from subjective beliefs or objective data-based information. The
general prior structure given by Equation (2.3) deﬁnes an exchangeable prior
model since there is often no prior knowledge about diﬀerences in item char-
acteristics. This prior distribution already deﬁnes random item parameters.
It makes sense to handle the item parameters as random without needing
sampling arguments. The prior structure, Equations (2.4) and (2.5), reduces
the parameter space to two dimensions, which may improve the item param-
eter estimates. A prior distribution for the item parameters also makes it
possible to easily integrate prior information from experts and background
information, handle hierarchical item structures, and express measurement
©
and Behavioral Sciences, DOI 10.1007/978-1-4419-0742-4_7,
193
J.-P. Fox, Bayesian Item Response Modeling: Theory and Applications, Statistics for Social
  Springer Science+Business Media, LLC 2010

194
7 Random Item Eﬀects Models
uncertainty. For example, the linear-logistic test model (LLTM) assumes a
perfect linear decomposition of the item diﬃculty parameter using item co-
variates. This unrealistic assumption is easily adjusted by adding a random
error term at the item level such that the item diﬃculty parameter is treated as
random (e.g., Janssen, Schepers and Peres, 2004; Klein Entink, Kuhn, Hornke
and Fox, 2009b).
In the psychometrics literature, other applications are described that mo-
tivate the deﬁnition of random item parameters. Albers, Does, Imbos and
Janssen (1989) described an application of a Rasch model with random item
and person parameters. In their longitudinal study, students are obligated to
participate in a progress test four times a year for a period of six years. Each
progress test consisted of items that are randomly selected from the same con-
stant item bank. The sampling process of items induces a sampling variance
that has to be taken into account. In general, in domain-referenced testing,
a test is assembled for each person from an item bank. The item sampling
design induces random item characteristics.
Glas and van der Linden (2003) considered the application of item cloning.
In this procedure, items are generated by a computer algorithm given a parent
item (e.g., item shell or item template). Although the item cloning techniques
are still improving, item characteristics of cloned items show within-parent
variation. The item responses are not independent conditional on the ability
parameters since items from the same parent may be statistically related. The
problem of conditional independence is tackled by allowing random variabil-
ity between cloned item characteristics conditional on the parent item, which
induces random item parameters. Another related example of random item
parameters was given by Janssen, Tuerlinckx, Meulders and De Boeck (2000).
They described a criterion referenced test measuring several achievement tar-
gets. They assumed that items’ characteristics are correlated when the items
are nested within the same criterion, leading to within-criterion dependency.
A hierarchical item structure was deﬁned for the random item parameters.
De Jong, Steenkamp and Fox (2007), and De Jong, Steenkamp, Fox and
Baumgartner (2008) introduced a random item modeling approach for survey-
based marketing research. They introduced random item eﬀects to accommo-
date cross-national diﬀerences in item characteristics. In this case, although a
ﬁxed test is used, the item characteristics can typically be regarded as random
to generalize the inferences to some population of countries. In this chapter,
the natural extension to random item parameters for cross-national survey
data will be pursued further.
7.1.1 Measurement Invariance
International comparative survey studies such as the Programma for Interna-
tional Student Assessment (PISA) and the Third International Mathematics
and Science Study (TIMMS) are focused on international comparisons of stu-
dent achievement. Assessing comparability of the test scores across countries,

7.1 Random Item Parameters
195
cultures, and diﬀerent educational systems is a well-known complex problem.
The main issue is that the measurement instrument has to exhibit adequate
cross-national equivalence. This means that the calibration of the measure-
ment instrument remains invariant across populations (nations, countries) of
examinees. Meaningful comparisons can be made when measurement invari-
ance holds.
There are several types of invariance that have to be dealt with as a pre-
requisite to conducting comparisons across countries (e.g., Meredith and Mill-
sap, 1992; Steenkamp and Baumgartner, 1998). A lack of conﬁgural invariance
conveys that the latent variable being measured has some degree of diﬀeren-
tial meaningfulness across countries. The construct being measured may vary
across countries. The strength of the relationship between the underlying la-
tent variable and the items may vary across countries, which indicates a lack
of metric invariance. If an item satisﬁes the requirement of metric invariance,
scores on that item can still be biased. Cross-national diﬀerences in the means
of the observed item scores can be invoked by diﬀerences in the means of
the underlying construct. Scalar invariance refers to the consistency between
cross-national diﬀerences in latent means and cross-national diﬀerences in ob-
served means. A thorough discussion of these and other types of invariance in
a conﬁrmatory factor-analytic framework is given by Vandenberg and Lance
(2000).
In an item response theory framework, the item response function for an
invariant item is identical across populations. That is, for binary response
data, the discrimination and diﬃculty parameter of a measurement-invariant
item are assumed to be equal across populations. Mellenbergh (1989) explicitly
deﬁned a measurement-invariant item, or an unbiased item, as the property
that the item’s characteristic function does not depend on group membership,
P (Yik = 1 | G, θi) = P (Yik = 1 | θi) ,
(7.1)
where variable G characterizes the population respondent i belongs to. When
Equation (7.1) does not hold, the response probability depends on group mem-
bership given the level of the latent variable. Such an item behaves diﬀerently
across populations and is said to show diﬀerential item functioning (DIF).
Item properties that concern the distribution of the latent variable may still
vary across populations, even for a measurement-invariant item.
7.1.2 Random Item Eﬀects Prior
Assume that an item is not invariant but observations from a group are con-
ditionally independent given the level of the latent variable and group-speciﬁc
item characteristics. The group-speciﬁc item parameters diﬀer across groups,
but the diﬀerences are considered to be random measurement errors. There-
fore, item characteristics are modeled as random item eﬀects parameters to
establish conditional independence, where the random part captures random
error due to diﬀerential item functioning.

196
7 Random Item Eﬀects Models
In Chapter 2, an exchangeable hierarchical prior is deﬁned that allows
for a within-item correlation structure without a priori knowledge to predict
how the characteristics of item k will diﬀer from the characteristics of item
k′. Then, a multivariate normal prior density for the item parameters was
speciﬁed,
(ak, bk)t ∼N (µξ, Σξ) IAk(ak),
(7.2)
with prior parameters
Σξ ∼IW(ν, Σ0),
µξ | Σξ ∼N(µ0, Σξ/K0),
where Ak = {ak ∈R, ak > 0}. This level of the hierarchical prior presents
a model for the invariant item characteristics. It is assumed that the item
parameters (ak, bk) are invariant across populations. They will be referred
to as “international item characteristics”. In present cross-national survey
research, attention is focused on obtaining the values of the international
item characteristics, and they are used to obtain individual and/or country
scores based on the assumptions of measurement invariance.
It is not likely that all international item characteristics apply to each
individual in a cross-national survey. A more realistic assumption is to assume
that for each nation some of the item characteristics may deviate from the
international item characteristics. That is, the nation-speciﬁc item parameter
values are inﬂuenced by the international item characteristics and the nation-
speciﬁc characteristics (e.g., observed responses, response behavior). Note that
these kinds of diﬀerences can be explained if they are caused by, for example,
speciﬁc background diﬀerences (e.g., culture or gender diﬀerences). It is also
possible to allow for cross-classiﬁed diﬀerences in item characteristics when, for
example, cross-national and cross-cultural response heterogeneity is present.
In most cross-national research, this kind of heterogeneity is not allowed since
the conventional aim is to establish measurement invariance based on the
assumption that measurement instruments can be developed in such a way
that they contain items that display the same statistical properties in each
nation.
Subsequently, a random item eﬀects structure is deﬁned to allow for ran-
dom error variation in the item’s functioning across nations. That is,
˜ξkj =

˜akj,˜bkj
t
∼N

(ak, bk)t , Σ˜ξ

(7.3)
for j = 1, . . . , J. The covariance matrix may vary across items. An indepen-
dent random eﬀects structure is deﬁned when Σ˜ξ is a diagonal matrix with
diagonal elements σ2
ak and σ2
bk. Treating the nation-speciﬁc item parameters
as random eﬀects induces a natural covariance structure:
Cov

˜ξkj, ˜ξk′j′

=



Σξ + Σ˜ξ k = k′, j = j′
Σξ
k = k′, j ̸= j′
0
k ̸= k′.

7.1 Random Item Parameters
197
In general, noninvariant nation-speciﬁc item characteristics are deﬁned but
are nested within the international item characteristics.
The random item eﬀects prior can be generalized to deﬁne random item
threshold parameters for polytomous items. In the graded response model,
deﬁned in Equation (1.8) (see also Section 4.6), the threshold parameter κk,c
divided by the item discrimination parameter ak represents the latent vari-
able value necessary to respond above category c with probability .5. The
threshold parameters can be considered invariant across countries and will be
referred to as the international item threshold parameters. For each item k,
an exchangeable hierarchical prior is constructed to allow for random error
variation in the country-speciﬁc threshold parameters across countries. An ex-
changeable proper hierarchical prior is speciﬁed for the international threshold
parameters. For c = 1, . . . , Ck −1,
p (κk,c) ∝IA (κk,c) ,
where A = {κk,c ∈R, κk,0 < . . . < κk,c < . . . < κk,Ck} with κk,0 = −∞and
κk,Ck = ∞. Now, country-speciﬁc random threshold eﬀects are deﬁned as
˜κkj,c = κk,c + ϵκkj,c
(7.4)
for j = 1, . . . , J and c = 1, . . . , Ck−1, where ϵκkj,c ∼N
 0, σ2
κk

. The country-
speciﬁc threshold eﬀects have an order restriction,
−∞= ˜κkj,0 < . . . < ˜κkj,c < . . . < ˜κkj,Ck = ∞.
The error term is a country-speciﬁc deviation that is independently normally
distributed given the value of the international threshold parameter. The
item-speciﬁc variance σ2
κk controls the amount of variation in the nation-
speciﬁc threshold parameters. Note that the order of the country-speciﬁc
threshold parameters does not lead directly to an order of the international
threshold parameters. However, the order of the conditional expected country-
speciﬁc threshold parameters orders the international threshold parameters.
The residual random threshold parameters are independently normally dis-
tributed with mean zero given κk. It follows that
E
 κk,1 + ϵκkj,1 | κk

< E
 κk,2 + ϵκkj,2 | κk

and, subsequently, κk,1 < κk,2.
Analogous to the introduction of random threshold eﬀects, an exchange-
able hierarchical prior for the discrimination parameters that allows for ran-
dom variation across countries is deﬁned as
˜akj | ak ∼N
 ak, σ2
ak

,
(7.5)
ak ∼N
 µa, σ2
a

IAk(ak),
(7.6)
where Ak = {ak ∈R, ak > 0}. Note that the prior level-1 and level-2 variances
only diﬀer with respect to a single index. However, the variance terms have

198
7 Random Item Eﬀects Models
completely diﬀerent meanings. The prior variance at level 2 represents the
a priori uncertainty about the value of ak, and the prior variance at level
1 represents the cross-national variation in item discrimination. The level-1
variance parameter is of speciﬁc interest since it can be used to investigate
the assumption of measurement invariance for item k.
In random eﬀects modeling, interest is usually focused on the eﬀect of
grouping the observations where the level-2 variance is informative about the
between-group variability. In this case, nation-speciﬁc item characteristics are
clustered within items. The between-item variability (at level 2) often is not
of particular interest, and items usually discriminate diﬀerently (see Section
5.3). Here, interest is focused on the between-country variability (at level 1)
since it presents the variation in country-speciﬁc item parameters.
To complete the full Bayesian modeling approach, proper conjugate priors
are speciﬁed for the variance and covariance prior parameters. An inverse
Wishart distribution is speciﬁed for Σ˜ξ with degrees of freedom n˜ξ ≥2 and
scale matrix S˜ξ. An exchangeable inverse gamma distribution is set for σ2
ak,
and a normal inverse gamma prior is set for the parameters (µa, σ2
a).
7.2 A Random Item Eﬀects Response Model
Binary Response Data
An observation Yijk refers to an answer of respondent i in nation j to item k.
Taking account of random item eﬀects, the item response model (level 1) for
binary data reads as
P

Yijk = 1 | θi, ˜ξjk

=



Φ

˜akjθi −˜bkj

Ψ

d

˜akjθi −˜bkj

,
(7.7)
where for the moment the nesting of respondents in countries is ignored. The
random item eﬀects parameters are assumed to be normally distributed. The
logistic item response model with normally distributed random item eﬀects
leads to a complex MCMC scheme. Moreover, in contrast to the logistic item
response model, it will be shown that normally distributed random item eﬀects
ﬁt naturally into the normal ogive response model, which leads to readily
interpretable parameters.
The normal ogive response model in Equation (7.7) describes the prob-
ability of a correct response given a person’s ability and group-speciﬁc item
characteristics. The distribution of the random item eﬀects is introduced in
Equation (7.3). Assume an independent random item eﬀects structure. Then
a combined or integrated likelihood model can be stated as
P (Yijk = 1 | θi, ξk) = Φ
 akθi −bk −ϵbkj + ϵakjθi

,
(7.8)

7.2 A Random Item Eﬀects Response Model
199
where the random item eﬀects ϵbkj and ϵakj are normally distributed with
mean zero and variance σ2
bk and σ2
ak, respectively. Conditional on a person’s
ability, in addition to the mean structure, there are two sources of variation:
the unexplained variation in diﬃculties and discriminations across countries
for item k.
Observations from respondents from the same country to item k are as-
sumed to be correlated. Let this correlation structure be modeled via the
random item diﬃculty parameters. Let ˜Zijk denote a standard normally dis-
tributed underlying response variable. The conditional probability of Yijk = 1
given θi and ξk can be expressed as the expected conditional success proba-
bility,
P (Yijk = 1 | θi, ξk) = E

Φ

akθi −˜bkj

(7.9)
= E

P

˜Zijk ≤akθi −˜bkj | ˜bkj

= P

˜Zijk ≤akθi −bk −ϵbkj

= P

˜Zijk + ϵbkj ≤akθi −bk

= Φ

akθi −bk
q
1 + σ2
bk


= Φ (a∗
kθi −b∗
k) ,
(7.10)
where the third identity holds since the expected value of the conditional prob-
ability is the unconditional probability. It follows that the combined model,
Equation (7.10), is again a normal ogive item response model, which moti-
vates the use of the normal ogive formulation. Note that the international
item parameters from Equation (7.10) are smaller than the true international
item parameters due to ignoring cross-national variation in item diﬃculties
since
a∗
k = ak/
q
1 + σ2
bk,
b∗
k = bk/
q
1 + σ2
bk.
Latent continuous responses are deﬁned that underlie the discrete observed
response data. Therefore, let Zijk denote the latent response of respondent i
in country j to item k and
Zijk | Yijk, θi, ˜ξkj ∼N

˜akjθi −˜bkj, 1

,
(7.11)
where Yijk is the indicator that Zijk is positive. In this way, augmented data
are deﬁned according to Equation (4.7) given country-speciﬁc item parame-
ters. Assume that θi ∼N
 µθ, σ2
θ

. Then the combined likelihood model for
the latent response data can be written as

200
7 Random Item Eﬀects Models
Zijk = ˜akjθi −˜bkj + ϵijk
= ˜akj (µθ + σθϵθ) −
 bk + ϵbkj

+ ϵijk
= akµθ −bk
|
{z
}
ﬁxed part
+ akσθϵθ + ϵakjµθ + ϵakjσθϵθ −ϵbkj + ϵijk
|
{z
}
random part
,
(7.12)
where ϵθ and ϵijk are standard normally distributed. The ﬁxed part is the
general mean latent response to item k in the population. Besides the mea-
surement error part peculiar to Zijk, the random part contains four additional
random terms. Three sources of variability are expected: among persons, and
among discrimination and diﬃculty parameters across countries for item k.
The term ϵbkj denotes the random eﬀects error of the item diﬃculties since
it is assumed that the diﬃculty of the item varies across countries. The term
akσθϵθ contains the random eﬀects error of the persons and accounts for the
between-person variation in ability. The random eﬀects error of the discrim-
inations times the mean ability, ϵakjµθ, accounts for the residual variation
across countries in the association between the mean ability and the latent
response. Finally, the term ϵakjσθϵθ accounts for the residual variation across
countries in the association between the between-person variation in ability
and the latent response.
The normal ogive response model with random item eﬀects partitions the
total variance in the outcome into a within-country part and a between-
country part. The within-country variance is further partitioned into error
variance between individuals and error variance between an individual’s ob-
servations. The between-country variance is induced by the random eﬀects
item parameters. The conditional covariance between two latent observations
to item k can be expressed as (Exercise 7.5)
Cov (Zijk, Zi′j′k) = Cov

˜akjθi −˜bkj + ϵijk, ˜akj′θi′ −˜bkj′ + ϵi′j′k

= Cov (˜akjθi, ˜akj′θi′) + Cov (bkj, bkj′) + Cov (ϵijk, ϵi′j′k)
=



σ2
akσ2
θ + σ2
akµ2
θ + a2
kσ2
θ + σ2
bk + 1 i = i′, j = j′
σ2
akµ2
θ + σ2
bk
i ̸= i′, j = j′
0
j ̸= j′.
An intraclass correlation coeﬃcient can be deﬁned that presents the propor-
tion of variance in the latent outcomes that is attributable to countries,
ρξk =
σ2
akµ2
θ + σ2
bk
σ2akσ2
θ + σ2akµ2
θ + a2
kσ2
θ + σ2
bk + 1.
(7.13)
Polytomous Response Data
Observed ordinal response data are stored in a matrix y, and an observation
yijk refers to an answer of respondent i in nation j to item k in category

7.2 A Random Item Eﬀects Response Model
201
c = 1, . . . , Ck. Taking account of random item eﬀects, Equations (7.4) and
(7.5), the level-1 item response model reads as
P

Yijk = c | θi, ˜ξkj

=
 Φ (˜akjθi −˜κkj,c−1) −Φ (˜akjθi −˜κkj,c)
Ψ (˜akjθi −˜κkj,c−1) −Ψ (˜akjθi −˜κkj,c) , (7.14)
where ˜ξkj = (˜akj, ˜κkj). The latent (auxiliary) variable formulation for the
graded response model was introduced in Chapter 4, where normally dis-
tributed latent response data were deﬁned; see Equation (4.25).
The introduction of a normally distributed underlying variable allows for
easily interpretable parameters and easy formulation of an MCMC algorithm,
and it increases the ﬂexibility for making diﬀerent model adjustments such
as incorporating explanatory variables at the level of items and/or persons.
Therefore, attention is focused on the normal ogive or probit version of the
graded response model. Here, in the same way, normally distributed aug-
mented data are deﬁned as
Zijk | Yijk = c, θi, ˜ξkj ∼N (˜akjθi, 1) I (˜κkj,c−1 ≤Zijk ≤˜κkj,c) .
(7.15)
Subsequently, a level-1 response model in terms of a standard normally dis-
tributed underlying variable ˜Zijk can be deﬁned as
P

Yijk = c | θi, ˜ξkj

= P

˜akjθi −˜κkj,c ≤˜Zijk ≤˜akjθi −˜κkj,c−1

= P

˜κkj,c−1 −˜akjθi ≤˜Zijk ≤˜κkj,c −˜akjθi

= Pijk(c) −Pijk(c −1).
Cumulative or threshold models such as the graded response model are
often deﬁned by modeling the conditional cumulative response probability.
The probability Pijk(c) presents the probability of responding in or below
category c given item and person parameters. The conditional cumulative
response probability can be given in terms of the observable variable Yijk and
the unobservable underlying variable ˜Zijk since
P

Yijk ≤c | θi, ˜ξkj

= P

˜Zijk ≤˜κkj,c −˜akjθi | θi, ˜ξkj

.
Subsequently, a linear structure on the underlying latent variable ˜Zijk has the
form
˜Zijk = ˜κkj,c −˜akjθi + ϵijk
= κk,c −akθi + ϵκkj,c −ϵakjθi + ϵijk,
(7.16)
where ˜Zijk ≤˜κkj,c −˜akjθi if Yijk ≤c and ˜Zijk > ˜κkj,c −˜akjθi if Yijk > c.
The combined cumulative response model in terms of the underlying con-
tinuous response data can be recognized as a linear mixed eﬀects model with

202
7 Random Item Eﬀects Models
normally distributed random eﬀects. The term ϵκkj,c presents the unexplained
random eﬀects error of upper grade thresholds across countries for category c
of item k. The random eﬀect consisting of the unexplained random discrimina-
tion eﬀect times the ability, ϵakjθi, accounts for the variation across countries
in the association between ability and the latent responses to item k. Note
that θi is a random eﬀects parameter that accounts for heterogeneity among
persons.
In the linear latent response model, Equation (7.16), the latent response is
modeled as a function of ﬁxed item eﬀects, (ak, κk,c), and random (country-
speciﬁc) item eﬀects (ϵakj, ϵκkj,c). It can be shown that the implied marginal
response model, by integrating out the random item eﬀects, is again a normal
ogive graded response model.
Therefore, assume random country-speciﬁc shifting of thresholds but in-
variant discrimination parameters such that σ2
ak = 0. The expected condi-
tional cumulative success probability, where the expectation is taken with
respect to the distribution of the random threshold parameters, can be ex-
pressed as
P (Yijk ≤c | θi, ξk) = E (Φ (akθi −˜κkj,c−1))
= E

P

˜Zijk ≤akθi −˜κkj,c−1 | ˜κkj,c−1

= P

˜Zijk ≤akθi −κk,c−1 −ϵκkj,c−1

= P

˜Zijk + ϵκkj,c−1 ≤akθi −κk,c−1

= Φ

akθi −κk,c−1
q 1 + σ2κk



= Φ
 a∗
kθi −κ∗
k,c−1

.
Subsequently, the expected conditional success probability can be expressed
as
P (Yijk = c | θi, ξk) = Φ
 a∗
kθi −κ∗
k,c−1

−Φ
 a∗
kθi −κ∗
k,c

.
(7.17)
It follows that the marginal model is again a normal ogive graded response
model. The logistic version of the graded response model does not have this
property which is a reason for using the normal ogive graded response model.
Furthermore, the international item parameters from Equation (7.17) are typi-
cally smaller than the true parameters, and they are attenuated when ignoring
cross-national variation in item thresholds (σκk > 0) since
a∗
k = ak/
q
1 + σ2κk,
κ∗
k,c = κk,c/
q
1 + σ2κk.

7.2 A Random Item Eﬀects Response Model
203
7.2.1 Handling the Clustering of Respondents
Up to now, the nesting of respondents in countries has been ignored. How-
ever, the clustering of respondents can be properly modeled via a structural
multilevel model, as presented in Chapter 6. Therefore, consider the empty
multilevel model
θij = β0j + eij,
(7.18)
β0j = γ00 + u0j,
(7.19)
where θij is modeled by a country-speciﬁc intercept β0j and the within-country
level-1 error term eij. The level-1 error variance can also be speciﬁed to be
country-speciﬁc, which means that the errors are independent normally dis-
tributed with mean zero and variance σ2
θj given the country-speciﬁc intercept.
This corresponds to relaxing the assumption of the factor variance invariance.
The conditional variability of the latent variable is assumed to be diﬀerent
across countries given β0j. However, the error term at level 2 is assumed to
be independent normally distributed with mean zero and variance τ 2
00, which
means that the covariance structure is assumed to be common across coun-
tries.
7.2.2 Explaining Cross-national Variation
The responses in each nation j are conditionally independent given the person
and country-speciﬁc item parameters. Therefore, the usual local independence
assumption holds when conditioning on the random eﬀects. The posterior
density of the nation-speciﬁc item parameters can be expressed as
p

˜ξkj | yjk, θj, ξk, Σ˜ξ

∝
nj
Y
i=1
p

yijk | θij, ˜ξkj

p

˜ξkj | ξk, Σ˜ξ

.
Response observations and prior information will be used to make in-
ferences about the nation-speciﬁc item parameters. The international item
parameters determine the prior mean of the nation-speciﬁc item parameters.
The posterior mean of the nation-speciﬁc item parameters is also determined
by the within-country information. In a full Bayes approach, shrinkage esti-
mates of the nation-speciﬁc item parameters are obtained where the amount
of shrinkage is driven by information in the data and the level of shrinkage
depends on the heterogeneity among the country-speciﬁc item parameters.
There is more shrinkage towards the common (international) item character-
istics when less observed country-speciﬁc information is available or when the
country-speciﬁc item parameters are tightly distributed around the interna-
tional item parameters.
The variation in country-speciﬁc item parameters can be explained by
cross-national explanatory information. Let matrix v contain information that

204
7 Random Item Eﬀects Models
explains heterogeneity in item characteristics across countries. The random
item eﬀects model in Equation (7.3) can be extended to handle such covariate
information as
˜akj = ak + vkjδa + ϵakj,
(7.20)
˜bkj = bk + vkjδb + ϵbkj,
(7.21)
where the errors are multivariate normally distributed with mean zero and
variance Σ˜ξ. The covariate information to explain variation in cross-national
item characteristics can diﬀer between item parameters. The same extension
can be made for the random item eﬀects model for polytomous data. Sub-
sequently, to explain heterogeneity among respondents, within-nation and
between-nation covariate information can be used, as explained in Chapter
6, Equations (6.7) and (6.8). Note that in Chapter 6 the index j refers to a
school but here it refers to a country.
7.2.3 The Likelihood for the Random Item Eﬀects Model
Let Ωrepresent the common model parameters such that Ω=
 σ2
θ, T,γ,δ,
ξ,Σ˜ξ

. The likelihood model for the observed data contains random eﬀects
at the item and person levels. The random eﬀects models are used to average
the conditional likelihood, which leads to an unconditional likelihood of only
the common parameters. For that reason, the random eﬀects models are con-
sidered part of the likelihood. Then, the likelihood of the common parameters
is given by
p (y | Ω) =
Z Z Z 

J
Y
j=1
" nj
Y
i=1
" K
Y
k=1
p

yijk | θij, ˜ξkj

p

˜ξkj | vkj, δ, ξk, Σ˜ξ

d˜ξkj
#
p
 θij | xij, βj, σ2
θ

dθij
#
p (βj | wj, γ, T)
#
dβj.
(7.22)
The likelihood of the common model parameters consists of three levels. At
the ﬁrst level, the observations are distributed according to an item response
model with country-speciﬁc random item eﬀects and random person eﬀects.
At the second level, the distribution of the random eﬀects is speciﬁed. At
this level, the covariate information, stored in matrices x and v, explains
heterogeneity among respondents in country j and among item characteristics
across countries, respectively. The third level describes the distribution of the
random country eﬀects that account for the between-country variation, and
covariate w is used to explain this variation.
The modeling framework related to the likelihood in Equation (7.22) is de-
picted in Figure 7.1. The ellipse describes the nonlinear item response model.
The multilevel structure on the person parameters was already discussed in

7.3 Identiﬁcation: Linkage Between Countries
205
Country j
Individual i
qij
xkj
xij
bj
wj
yijk
Item k
xk
vkj
~
Fig. 7.1. Path diagram for a random item eﬀects model.
Chapter 6. In this case, level-2 and level-3 random eﬀects models describe
the within-country and between-country variations of persons, respectively.
The item observations are nested within the individuals, and the individuals
are nested within countries. At level 2, random item eﬀects parameters are
introduced that account for the variation in item characteristics across coun-
tries. The explanatory information vkj explains between-country variability
in characteristics of item k. The hierarchical structure of the model is depicted
in boxes. There is a strict hierarchical structure, where item observations are
nested within individuals that are nested in countries. Another hierarchical
structure of the model is the nesting of item observations in countries, lead-
ing to country-speciﬁc item characteristics. This is illustrated by allowing the
item box to overlap the country box outside the individual box.
7.3 Identiﬁcation: Linkage Between Countries
Meaningful comparisons across countries can be made when the individuals
are measured on the same scale. When measurement invariance is present,
the levels of the latent variable are measured on a common scale and cross-
national diﬀerences can be meaningfully analyzed, and this will lead to valid
interpretations. When the measurement scales diﬀer across countries, cross-
national diﬀerences in mean or variance levels are diﬃcult to interpret since
they do not reﬂect the true diﬀerences between countries.
As stated in Section 4.4, the scale of the latent variable needs to be iden-
tiﬁed. It was shown that it is possible to identify the scale via restrictions on
item parameters or to ﬁx the mean and if necessary the variance of the distri-
bution of the latent variable. When restricting the scale in each country, the
scale of the latent variable is identiﬁed, but it is not common across countries.
Thus, additional restrictions are needed to link the countries.

206
7 Random Item Eﬀects Models
Therefore, traditional practice in the analysis of cross-national data is to
establish measurement invariance. The items satisfy the assumption of mea-
surement invariance when their characteristics do not vary across nations. The
item parameters are said to be invariant. In that case, persons with identical
scores on the latent variable also have the same item scores. It is not necessary
that all items exhibit measurement invariance. Comparing respondents on a
common scale across countries is technically possible with at least one invari-
ant item. This invariant item can be used as an anchor item to establish a
common scale across countries. The other items are allowed to function diﬀer-
ently but should be related to the common scale. Among others, Steenkamp
and Baumgartner (1998) noted that at least two invariant items are needed
when one needs to test the assumption of measurement invariance. This pro-
cedure has several limitations. First, testing items for measurement invariance
is an exploratory post-hoc procedure that is prone to capitalization on chance.
Second, when there are only a few invariant items, the usual tests for diﬀeren-
tial item functioning may identify invariant items as exhibiting noninvariance
since the model also tries to ﬁt the other noninvariant items (Holland and
Wainer, 1993). Third, the existence of two invariant items is not realistic
when the number of items is small and the number of countries is high. Then,
measurement invariance may only hold for subgroups of countries.
7.3.1 Identiﬁcation Without (Designated) Anchor Items
The random item eﬀects model cannot be identiﬁed via marker or anchor items
when all country-speciﬁc item parameters are modeled as random item param-
eters. Obviously, the random item parameters do not satisfy the assumption
of measurement invariance. However, without identifying restrictions the scale
of the latent variable is not identiﬁed. The object is to identify the random
item eﬀects model in such a way that a common latent scale is established
across countries.
The combined random item eﬀects model as stated in Equation (7.12) can
be identiﬁed by ﬁxing the scale of the latent variable. For example, for µθ = 0
and σθ = 1, the model in Equation (7.12) can be recognized as a mixed
eﬀects model with random item eﬀects and standard normally distributed
random person eﬀects; that is, random item diﬃculty eﬀects and random
item discrimination eﬀects on the association between the (identiﬁed) random
person eﬀects and the latent responses.
First, assume that the nesting of respondents in countries is modeled via
an empty multilevel model on the person parameters (Equations (6.4) and
(6.5)), and assume invariant item discrimination. Then, the conditional suc-
cess probability given country-speciﬁc diﬃculty parameters and latent country
means is given by

7.3 Identiﬁcation: Linkage Between Countries
207
P

Yijk = 1 | β0j,˜bkj

= E

Φ

θij −˜bkj

= E

P

˜Zijk ≤θij −˜bkj | θij,˜bkj

= P

˜Zijk ≤β0j + eij −˜bkj | β0j,˜bkj

= Φ
 
β0j −˜bkj
p
1 + σ2
θ
!
,
where the random person eﬀects eij are independently normally distributed
and the random country eﬀects and random item diﬃculty eﬀects are inde-
pendently distributed as β0j ∼N(γ00, τ 2) and ˜bkj ∼N(bk, σ2
bk), respectively.
The model is not identiﬁed by ﬁxing the mean of the latent scale by setting
γ00 = 0 or by restricting the sum of the international diﬃculty parameters,
P
k bk = 0. There is an indeterminacy between the country-speciﬁc latent
mean and the location of the country-speciﬁc item diﬃculties. A common shift
in the country-speciﬁc item diﬃculties can be compensated for by a similar
shift in the country-speciﬁc latent mean. Therefore, in each country j, the lo-
cation of the random item diﬃculties is restricted by setting P
k ˜bkj = 0. This
way, a common shift in random item diﬃculties in country j is not allowed.
Instead, a comparable shift in the country-speciﬁc latent mean is captured.
Subsequently, the location of the latent variable is identiﬁed in each country
j, and therefore the general location of the common scale is identiﬁed. Note
that, despite the identifying restriction, a common shift in country-speciﬁc
item diﬃculties is possible when covariate information is available that ex-
plains it. In that case, the location of the country-speciﬁc item diﬃculties is
restricted conditional on the item predictor eﬀects δb (see Equation (7.21)).
Second, assume random item eﬀects parameters but still assume factor
variance invariance such that σ2
θj = σ2
θ for each j. Condition only on the
random eﬀects for notational convenience. Then the conditional success prob-
ability can be expressed as
P

Yijk = 1 | β0j,˜bkj

= E

E

Φ

˜akjθij −˜bkj | θij, ˜ξkj

= E

P

˜Zijk ≤˜akj (β0j + eij) −˜bkj | β0j, ˜ξkj

= P

˜Zijk ≤ak (β0j + eij) + ϵakj (β0j + eij) −˜bkj | β0j,˜bkj

. (7.23)
Besides the restriction on the country-speciﬁc diﬃculty parameters, there is
still an indeterminacy in the location of the international item discrimination
parameters and the latent person errors eij (reﬂected in the term ak(β0j+eij)).
A common shift in the international discrimination parameters can be com-
pensated for by a shift in the latent person errors. Therefore, it is necessary
to ﬁx the variance of the latent variable (for example, via rescaling; see Sec-
tion 4.4.2) or restrict the international discrimination parameters by setting
Q
k ak = 1.

208
7 Random Item Eﬀects Models
Third, also assume factor variance noninvariance and allow country-
speciﬁc variances of the latent variable. As a result, a restriction on the total
variance of the latent variable leads to an unidentiﬁed model. This follows
from the fact that a common shift of all item discrimination eﬀects ϵakj can
be compensated for by an equal shift of the country-speciﬁc errors eij in coun-
try j. Note that the indeterminacy in the term ϵakj (β0j + eij) of Equation
(7.23) is apparent. This identiﬁcation problem can be solved by restricting
the country-speciﬁc item discriminations such that Q
k ˜akj = 1 for each j.
In each country, a common shift in the random item discrimination eﬀects
is not allowed and will be captured by a shift in the country-speciﬁc vari-
ance of the latent variable. In the same way as for the random item diﬃculty
eﬀects, a common shift is allowed when such an eﬀect can be explained by
country-speciﬁc information.
The generalization to polytomous items is straightforward. For the most
general case, the country-speciﬁc discrimination parameters are restricted
via Q
k ˜akj = 1 for each j. A common shift in the country-speciﬁc thresh-
old parameters can be compensated by a comparable negative shift in the
country-speciﬁc latent mean. This indeterminacy is neutralized by disal-
lowing a common shift in the country-speciﬁc threshold parameters and,
in that case, forcing a comparable shift in the latent country mean. Let
κj,2 = P
k ˜κkj,2/K denote the mean threshold value across items in country j.
The rescaled threshold parameters are deﬁned as ˜κkj,c −κj,2 for k = 1, . . . , K,
and c = 1, . . . , Ck −1, and they have a ﬁxed location. It is easily seen that
this location does not change when adding the same constant to all country-
speciﬁc threshold parameters. Rescaling the random threshold parameters in
each country also identiﬁes the general location of the scale.
7.3.2 Concluding Remarks
This modeling framework has several important advantages. The person pa-
rameters can be estimated on a common scale given noninvariant items. Iden-
tiﬁcation of the model does not depend on the presence of marker items. That
is, measurement invariance assumptions are not needed to meaningfully com-
pare item scores across countries. The model based on measurement invariance
is nested within the model for measurement noninvariance. As a result, evi-
dence of measurement invariance can be obtained by comparing both models.
Deviations in item characteristics across countries can be considered measure-
ment errors. Diﬀerences can be explained by explanatory information, but
without the proper covariates they are considered to be measurement errors.
The proposed modeling approach accounts for the (un)explained heterogene-
ity in response probabilities. For example, if the level of stylistic response
is measured, diﬀerences between item characteristics across nations may be
controlled by adjusting for it.
Shrinkage estimates of the nation-speciﬁc item parameters are based on
the observations from that nation and the international item characteristics,

7.4 MCMC: Handling Order Restrictions
209
which are based on all respondents’ observations. The shrinkage towards the
international item parameters enhances the stability of the country-speciﬁc
item parameter estimates.
The random item eﬀects modeling approach reduces the number of eﬀec-
tive model parameters in comparison with modeling nation-speciﬁc item pa-
rameters as ﬁxed eﬀects, which will rapidly increase the number of parameters
with a growing number of nations and response options. Invariance hypotheses
can be tested by evaluating diﬀerent models via an information criterion. The
Bayes factor can also be used for testing a null hypothesis concerning mea-
surement invariance, which can be computed via the sampled values obtained
from estimating the parameters of the most general model.
7.4 MCMC: Handling Order Restrictions
An MCMC scheme will be presented for the general random item eﬀects model
that allows measurement-noninvariant item parameters and country-speciﬁc
distributional properties of the latent variable. In a fully Bayesian hierarchi-
cal modeling approach, each sampling step is characterized by a conditional
independence assumption. This strategy leads to a straightforward implemen-
tation of the diﬀerent sampling steps. However, speciﬁc attention has to be
paid to the steps for drawing international and nation-speciﬁc threshold pa-
rameters, which are complicated due to order restrictions.
7.4.1 Sampling Threshold Values via an M-H Algorithm
Order restrictions complicate a direct sampling procedure and an M-H proce-
dure is proposed to obtain samples from the full conditional. In the same way
as in Chapter 4, adaptive M-H steps are deﬁned to avoid specifying any tuning
parameters. An M-H sampling procedure is discussed for the nation-speciﬁc
and international threshold parameters and the threshold variance parameter
σ2
κk (Exercise 7.6).
Nation-Speciﬁc Threshold Parameters
The adaptive proposal distribution is based on the information of the last
iteration, denoted as m, and item k’s candidate threshold values should follow
the order restriction. Therefore, generate candidate nation-speciﬁc threshold
values from
˜κ∗
kj,c ∼N

˜κ(m)
kj,c, σ2
mh

I

˜κ∗
kj,c−1 < ˜κ∗
kj,c < ˜κ(m)
kj,c+1

(7.24)
for c = 1, . . . , Ck −1, where ˜κ(m)
kj,c is the value of ˜κkj,c in iteration m. The
variance of the proposal distribution is adjusted to improve the eﬃciency of
the MCMC algorithm as described in Section 4.2.

210
7 Random Item Eﬀects Models
The conditional posterior density of threshold parameters ˜κkj is given by
p
 ˜κkj | θij, κk, σ2
κk, y

∝
Y
i|j
p (yijk | θij, ˜κkj)
Ck−1
Y
c=1
p
 ˜κkj,c | κk,c, σ2
κk

,
where
p
 ˜κkj,c | κk,c, σ2
κk

=
φ
 ˜κkj,c; κk,c, σ2
κk

R ˜κkj,c+1
˜κkj,c−1 φ
 ˜κkj,c; κk,c, σ2κk
.
(7.25)
Note that the part of the normalizing constant that is attributable to the
truncation cannot be ignored since it includes the threshold parameters of
interest. An acceptance ratio can be deﬁned to evaluate a proposed candidate
˜κ∗
kj with the current state ˜κ(m)
kj . The acceptance ratio R is deﬁned as
R = p
 ˜κ∗
kj | θij, κk, σ2
κk, y

p

˜κ(m)
kj
| θij, κk, σ2κk, y

Ck−1
Y
c=1
Φ

˜κ(m)
kj,c+1−˜κ(m)
kj,c
σmh

−Φ

˜κ∗
kj,c−1−˜κ(m)
kj,c
σmh

Φ
 ˜κ∗
kj,c+1−˜κ∗
kj,c
σmh

−Φ

˜κ(m)
kj,c−1−˜κ∗
kj,c
σmh
 .
At iteration m + 1, the transition ˜κ(m+1)
kj
= ˜κ∗
kj is made with probability
min(1, R). The second term of the acceptance ratio accounts for the diﬀerence
in the normalizing constant of the proposal densities.
International Threshold Parameters
In the same way, an M-H step can be deﬁned for the international threshold
parameters. The conditional posterior density of the international threshold
parameter is deﬁned by
p
 κk,c | ˜κk,c, σ2
κk

∝
Y
j
p
 ˜κkj,c | κk,c, σ2
κk

,
which can be recognized as the product of truncated normal prior densities
according to Equation (7.25). Deﬁne a proposal distribution as
κ∗
k,c ∼N

κ(m)
k,c , σ2
mh

I

κ∗
k,c−1 < κ∗
k,c < κ(m)
k,c+1

for c = 1, . . . , Ck −1. Now, an acceptance ratio can be deﬁned to evaluate the
posterior densities of the states κ∗
k and κ(m)
k
while taking account of diﬀerences
in the proposal densities.

7.4 MCMC: Handling Order Restrictions
211
7.4.2 Sampling Threshold Values via Gibbs Sampling
The nation-speciﬁc threshold parameters have a truncated normal prior ac-
cording to Equation (7.4). The object is to deﬁne a corresponding set of
threshold parameters that have a nontruncated normal prior. This new set
of threshold parameters has to comply with the prior model for the nation-
speciﬁc threshold parameters. This is achieved by restricting the cumulative
prior probabilities of the transformed set of parameters to equal the cumula-
tive prior probabilities of the nation-speciﬁc threshold parameters. Then, the
transformed set of threshold parameters can be used to sample directly from
the full conditionals of the other prior parameters.
Consider the normal distribution of a nation-speciﬁc threshold parameter
truncated to the interval A = (˜κkj,c−1, ˜κkj,c+1)
˜κkj,c = νkj,cIA (νkj,c) ,
where
IA (νkj,c) =
 1 νkj,c ∈A
0 otherwise,
where νkj,c ∼N
 κk,c, σ2
κk

. Now, parameter νkj,c can be recognized as the
nontruncated version of ˜κkj,c. Deﬁne the cumulative normal distribution func-
tion F(νkj,c) = Φ ((νkj,c −κk,c)/σκk).
The cumulative distribution function of ˜κkj,c follows from the cumulative
distribution function of νkj,c. That is, the cumulative probability p of values
less than or equal to ˜κkj,c can be expressed as
p = G (˜κkj,c) =
F (˜κkj,c) −F (˜κkj,c−1)
F (˜κkj,c+1) −F (˜κkj,c−1).
(7.26)
In compliance with the prior model for ˜κkj,c, the value of νkj,c that satisﬁes
the equation G (˜κkj,c) = F (νkj,c) is considered to be the corresponding unique
value of νkj,c, where F is the nontruncated version of G. This corresponding
value of νkj,c can be computed via the inverse cumulative distribution function
method. Given probability p from Equation (7.26), the value of νkj,c is given
by
νkj,c = F −1 (G (˜κkj,c)) = F −1 (p) = κk,c + σκkΦ−1 (p)
= κk,c + σκkΦ−1
 F (˜κkj,c) −F (˜κkj,c−1)
F (˜κkj,c+1) −F (˜κkj,c−1)

.
(7.27)
This procedure can be repeated for each item k, j, and c.
Now, consider the factorization
p
 νk,c, κk,c, σ2
κk | ˜κk,c

∝p
 ˜κk,c | νk,c, κk,c, σ2
κk

p
 νk,c, κk,c, σ2
κk

∝p (˜κk,c | νk,c) p
 νk,c, κk,c, σ2
κk

∝p
 νk,c | κk,c, σ2
κk

p (κk,c) p
 σ2
κk

,

212
7 Random Item Eﬀects Models
where the conditional density p (˜κk,c | νk,c) equals one due to the relationship
in Equation (7.27). As a result, inferences for κk,c and σ2
κk can be based on
νk. That is, the nontruncated normally distributed threshold parameters νk
can be used to sample international threshold parameters and a threshold
variance parameter directly (see MCMC scheme 5).
7.4.3 Simultaneous Estimation via MCMC
MCMC Scheme 5
A1) Binary response data
1. Sample augmented data z(m+1) according to Equation (7.11).
2. For each k and j, sample ˜ξkj from the conditional distribution
˜ξkj | z(m+1)
kj
, θ(m)
j
, ξ(m)
k
, Σ(m)
˜ξ
∼N

µ∗
˜ξ, Ω˜ξ

,
where
Ω−1
˜ξ
= HtH + Σ−1
˜ξ ,
(7.28)
µ∗
˜ξ = Ω˜ξ

Htzkj + Σ−1
˜ξ ξk

,
(7.29)
and H =
 θj, −1nj

.
3. For each k, sample ξk from the conditional distribution
ξk | ˜ξ
(m+1)
k
, Σ(m)
˜ξ
, µ(m)
ξ
, Σ(m)
ξ
∼N
 µ∗
ξ, Ωξ

IAk (ak) ,
where
Ω−1
ξ
= JΣ−1
˜ξ
+ Σ−1
ξ ,
µ∗
ξ = Ωξ

JΣ−1
˜ξ ˆξk + Σ−1
ξ µξ

,
with ˆξk = P
j ˜ξkj/J.
4. Sample Σ(m+1)
˜ξ
given ˜ξ
(m+1)
k
and ξ(m+1)
k
from an inverse Wishart
distribution with J + n0 degrees of freedom and scale matrix
X
j

˜ξkj −ξk
 
˜ξkj −ξk
t
+ S0.
5. Sample µ(m+1)
ξ
, Σ(m+1)
ξ
via M-H accounting for the positivity restric-
tion on the international item discriminations.
A2) Polytomous response data
1. Sample augmented data z(m+1) according to Equation (7.15).

7.4 MCMC: Handling Order Restrictions
213
2. For each k and j, sample ˜a(m+1)
kj
from the conditional distribution
˜akj | z(m+1)
jk
, θ(m)
j
, a(m)
k
, σ2(m)
ak
∼N (µ∗
˜a, Ω˜a) ,
where
Ω−1
˜a
= θt
jθj + σ−2
ak ,
µ∗
˜a = Ω˜a
 θt
jzjk + akσ−2
ak

.
3. For each k, sample discrimination parameters a(m+1)
k
from the con-
ditional distribution
ak | ˜a(m+1)
k
, σ2(m)
ak
, µa, σ2
a ∼N (µ∗
a, Ωa) IAk (ak) ,
where
Ω−1
a
= Jσ−2
ak + σ−2
a
µ∗
a = Ωa
 X
j
˜akj/σ2
ak + µa/σ2
a
!
.
4. For each k, sample σ2(m+1)
ak
from an inverse gamma density with shape
parameter g1 and scale parameter P
j (˜akj −ak)2 /2 + g2.
5. For each k and j, sample ˜κ∗
kj,c for c = 1, . . . , Ck −1 from the pro-
posal distribution; see Equation (7.24). The candidate nation-speciﬁc
threshold parameters are evaluated for each item-country combination.
Sample Ukj ∼U (0, 1), and set ˜κ(m+1)
kj
= ˜κ∗
kj when Ukj ≤min(1, R)
where the acceptance ratio R is deﬁned below Equation (7.25).
6. Compute ν(m+1) given ˜κ(m+1) according to Equation (7.27). For each
k and c = 1, . . . , Ck −1, sample κ(m+1)
k,c
from the truncated conditional
posterior distribution,
κk,c | ν(m+1)
k,c
, σ2(m)
κk
∼N
 νk,c, σ2
κk/J

IA (κk,c) ,
where νk,c = P
j νkj,c/J and A = (κk,c−1, κk,c+1).
7. Sample σ2(m+1)
κk
from an inverse gamma density with shape parameter
g1 + J(Ck −1)/2 and scale parameter
X
j
X
c
(νkj,c −κk,c)2.
2 + g2.
B) Sample multilevel parameter values
1. For each i and j, sample θ(m+1)
ij
given z(m+1)
ij
, ξ(m+1), β(m)
j
, and
σ2(m)
θj
according to step 4 in MCMC scheme 4.

214
7 Random Item Eﬀects Models
2. For each j, sample β(m+1)
j
given θ(m+1)
j
, σ2(m)
θj
, T(m), and γ(m) ac-
cording to step 5 in MCMC scheme 4.
3. Sample γ(m+1)
j
given β(m+1)
j
, T(m), and σγ according to step 6 in
MCMC scheme 4.
4. For each j, sample σ2(m+1)
θj
given θ(m+1)
j
and β(m+1)
j
from an inverse
gamma density with shape parameter g1 + nj/2 and scale parameter
g2 + (θj −xjβj)t (θj −xjβj) /2.
5. Sample T(m+1) given β(m+1) and γ(m+1) according to step 8 in
MCMC scheme 4.
7.5 Tests for Invariance
Measurement invariance and factor variance invariance can be tested via an
information criterion. Therefore, an expression is needed for the likelihood of
Λ =
 γ, ξ, σ2
θ, T, Σ˜ξ

, the parameters of interest. An expression is derived
for the random item eﬀects likelihood for binary data. The likelihood for
polytomous response data can be derived in a similar way. In correspondence
with the computation of the MLIRT likelihood in Section 6.10, an expression
for the augmented likelihood is derived and via MCMC the likelihood for the
observed data is estimated. Express the likelihood of the observed data as the
integrated augmented data likelihood:
p

y | γ, ξ, σ2
θ, T, Σ˜ξ

=
Z
p

z | γ, ξ, σ2
θ, T, Σ˜ξ

dz
=
Z p
 z, ˜ξ
∗| θ, β, Λ

p
 ˜ξ
∗| z, θ, β, Λ

p
 z, θ, β | Λ

p
 θ, β | z, Λ
dz.
(7.30)
For notational convenience, assume that ˜ξkj is normally distributed with
mean µ∗
˜θ from Equation (7.29) and variance Ω˜ξ from Equation (7.28). Then,
for ˜ξ
∗
kj = ˜ξkj, the augmented data likelihood, under the integral sign in Equa-
tion (7.30), can be expressed as
p (z | Λ) =
Y
j
(2π)−Knj/2 |Σ˜ξ|K/2  Ωθ/σ2
θ
nj/2 |T|−1/2 ·
|Σ ˜βj|1/2 exp

−S

˜θj, ˜βj

/2

,
where
S

˜θj, ˜βj

=
X
i,k

(zijk + ˜bkj) −˜akj ˜θij
2
+
X
k

˜ξkj −ξk
t
Σ−1
˜ξ

˜ξkj −ξk

+ σ−2
θj

˜θj −xj ˜βj
t 
˜θj −xj ˜βj

+

˜βj −wjγ
t
T−1 
˜βj −wjγ

,

7.5 Tests for Invariance
215
using the expression for the augmented data likelihood in Equation (6.44).
Note that the residual sum of squares is partitioned. The second term on
the right-hand side presents the contribution of cross-national diﬀerences in
item characteristics to the total residual sum of squares. It can be seen that
the residual sum of squares of the MLIRT model in Equation (6.45), is nested
within the residual sum of squares of the random item eﬀects model. It follows
that the MLIRT model can be compared with the random item eﬀects model
via a DIC or BIC to test for full measurement invariance. When the MLIRT
model is preferred, the items exhibit measurement invariance. Other restricted
models can be deﬁned to test for speciﬁc measurement invariance assumptions.
Invariance tests based on HPD regions can be constructed from the pos-
terior distributions of the most general random item eﬀects model. This ap-
proach has the advantage that assumptions of measurement invariance and
factor variance invariance can be tested by estimating the general random
item eﬀects model. To illustrate this approach, consider the conditional dis-
tribution of the nation-speciﬁc latent variable variances,
p
 σ2
θ1, . . . , σ2
θJ | θ, β

∝
Y
j

σ2
θj
−((g1+nj)/2+1)
exp
 
−s2
j + g2
2σ2
θj
!
∝
Y
j

σ2
θj
−(n′
j/2+1)
exp
 
−n′
js′2
j
2σ2
θj
!
,
where
s′2
j =
P
i|j
 θij −xt
ijβj
2 + g2
nj + g1
,
using an inverse gamma prior with shape and scale parameters g1/2 and g2/2,
respectively. Deﬁne (J−1) linear independent contrasts ∆j = log σ2
θj −log σ2
θJ.
The point ∆0 = 0 corresponds to the event that σ2
θ1 = . . . = σ2
θJ. The point
∆0 = 0 is included in a (1 −α) HPD region if and only if
P (p (∆| θ, β) > p (∆0 | θ, β) | θ, β) < 1 −α.
Box and Tiao (1973, pp. 133–136) showed that, for nj →∞, this probability
statement is approximately equal to the probability statement
P

χ2
J−1 ≤−
J
X
j=1
nj
 log s′2
j −log s′2

< 1 −α,
(7.31)
where s′2 is the weighted average variance across nations. The marginal pos-
terior probability that the point ∆0 is included in the HPD region is com-
puted by integrating out the parameters θ and β using MCMC samples from
their joint posterior. That is, the conditional posterior probability in Equation

216
7 Random Item Eﬀects Models
(7.31) is evaluated in each MCMC iteration and the averaged posterior prob-
ability is considered to be an estimate of the marginal posterior probability.
To test for measurement invariance, consider the conditional posterior dis-
tribution of the nation-speciﬁc item parameters,
p

˜ξk | zkj, θj, Σ˜ξ, ξk

∝exp

−1
2
X
j

˜ξkj −µ∗
˜ξ
t
Ω−1
˜ξ

˜ξkj −µ∗
˜ξ


,
where µ∗
˜ξ and Ω−1
˜ξ
are deﬁned in Equations (7.29) and (7.28), respectively.
The term within the exponent is chi-square distributed with 2J degrees of
freedom. It follows that the event that ˜ξkj = ξk for each j, corresponding to
the assumption of measurement invariance, is included in the (1 −α) HPD
region if and only if
P

χ2
2J ≤
X
j

ξk −µ∗
˜ξ
t
Ω−1
˜ξ

ξk −µ∗
˜ξ


< 1 −α.
In the same way as above, the marginal posterior probability is estimated via
MCMC. The procedure can be extended to test measurement invariances of
a set of items simultaneously, to test scalar and metric invariances separately,
or to test measurement-invariant properties of polytomous scored items.
7.6 International Comparisons of Student Achievement
To illustrate the random item eﬀects modeling approach, data from the Pro-
gramma of International Student Assessment (PISA) survey of mathematics
performance were analyzed. In 2003, four subject domains were tested (mathe-
matics, reading, science, and problem solving). There were 13 clusters, includ-
ing seven mathematics clusters and two clusters in each of the other domains.
According to the test design, 13 test booklets were deﬁned that consisted of
four clusters. Each cluster appeared exactly once in each of the four possible
positions within a test booklet. The test items were distributed across 13 test
booklets in a rotated test design (a balanced incomplete test design) such
that each test item appeared in four test booklets. This type of test design
ensured a wide coverage of content while at the same time keeping the indi-
vidual testing burden low. Sampled students were randomly assigned one of
the test booklets. Not all test booklets assessed the same domains, however,
mathematical literacy was assessed in all test booklets.
Mathematics was the major domain, and 85 items were used to assess stu-
dent achievement across seven clusters and 13 test booklets. Each test booklet
was designed to be of approximately equal diﬃculty and equivalent content
coverage. Due to the balanced design, item parameter estimates are not in-
ﬂuenced by a booklet eﬀect. The diﬀerent locations of domains within each

7.6 International Comparisons of Student Achievement
217
booklet were expected to lead to booklet inﬂuences. Therefore, in PISA 2003,
an international test booklet eﬀect was incorporated into the measurement
model to correct for the eﬀect of item location. Although a booklet eﬀect pa-
rameter can be incorporated in the random item eﬀects model, the response
data from eight mathematics items from booklet 1 were analyzed to avoid
any booklet eﬀects. A total of 9,796 students were sampled at random from
40 participating countries (excluding Liechtenstein, with 28 students).
The main problem consists of ensuring comparability of test scores across
countries, cultures, and educational systems. In the present modeling ap-
proach, item and person eﬀects are estimated simultaneously, allowing for
between-country diﬀerences of item characteristics and student achievement.
Therefore, consider the random item eﬀects model
P

Yijk = 1 | θij, ˜ξk

= Φ

˜akjθij −˜bkj

,
(7.32)

˜akj,˜bkj
t
= (ak, bk)t +
 ϵakj, ϵbkj
t ,
θij = β0j + eij,
where eij ∼N
 0, σ2
θj

, β0j ∼N
 γ00, τ 2
, and
 ϵakj, ϵbkj
t ∼N
 0, Σ˜ξ

.
This model will be referred to as model M4 and allows for measurement-
noninvariant items and cross-national heterogeneity in latent means and vari-
ances. Model M4 is identiﬁed by restricting the sum of the nation-speciﬁc
item diﬃculties to zero in each country and by restricting the product of
nation-speciﬁc item discriminations to one in each country.
In Table 7.1, three models are considered that are nested within the gen-
eral model M4 (Equation (7.32)). Model M0 is an MLIRT model with an
empty structural multilevel model. Model M0 considers that all items ex-
hibit measurement invariance. Cross-national diﬀerences in latent means are
allowed but diﬀerences in latent variances are not. The model was identiﬁed
by restricting the mean and variance of the latent scale to be zero and one,
respectively. Model M1 allows for country-speciﬁc item parameters but disre-
gards cross-national heterogeneity in the latent variable and assumes a stan-
dard normal prior for it. Model M1 is identiﬁed since the scale of the latent
variable is restricted with mean zero and variance one. Model M2 allows for
nation-speciﬁc item parameters and cross-national diﬀerences in latent means.
The model was identiﬁed by restricting the sum of the nation-speciﬁc item
diﬃculties to be zero in each country and the variance of the latent variable
to be one.
In model M1, the cross-national heterogeneity in the latent variable is ig-
nored but cross-national diﬀerences in item characteristics are allowed. It can
be seen that the estimated international item parameters resemble the esti-
mated international item parameters of model M0. However, there are large
cross-national variations in item discrimination and diﬃculty. The estimated
item-speciﬁc posterior standard deviations are given under the labels σak and
σbk. For example, the estimated country-speciﬁc diﬃculty parameters of the

218
7 Random Item Eﬀects Models
Table 7.1. PISA 2003: Exploring cross-national item variation.
Invariant
Noninvariant Items
Model M0
Model M1
Model M2
Item
Mean SD
Mean SD σak
Mean SD σak p0(ak)
Discrimination Parameter
1
.808 .027
.727 .032 .077
.817 .037 .089
.860
2
1.060 .035
1.076 .064 .284
1.099 .055 .230
.999
3
.729 .024
.611 .029 .068
.717 .029 .066
.567
4
.688 .023
.617 .027 .082
.694 .032 .115
.971
5
.555 .020
.533 .027 .091
.582 .029 .117
.997
6
.367 .025
.343 .036 .102
.402 .044 .160
.999
7
.694 .026
.606 .032 .074
.691 .035 .097
.926
8
.660 .025
.637 .035 .093
.684 .034 .112
.971
Item
Mean SD
Mean SD σbk
Mean SD σbk p0(bk)
Diﬃculty Parameter
1
−.586 .016
−.581 .066 .384
−.590 .030 .133
.999
2
.185 .016
.172 .075 .452
.194 .036 .115
.963
3
−.040 .014
−.040 .061 .378
−.039 .030 .106
.992
4
−.359 .015
−.377 .058 .342
−.359 .026 .105
.976
5
−.018 .013
−.026 .040 .242
−.024 .022 .082
.727
6
−1.510 .023 −1.549 .046 .241 −1.549 .027 .100
.909
7
−.780 .017
−.785 .058 .343
−.785 .026 .098
.954
8
−.942 .018
−.966 .045 .262
−.957 .022 .082
.756
Structural Part
Fixed
γ00
.010 .083
.000
-
.734 .083
Random
σ2
θ
.791 .013
1.000
-
.791 .014
τ 2
00
.269 .063
.270 .063
Information Criteria
−2log L
95727.5
102322.1
94681.0
DIC (pD)
100395.1(4667)
98960.2(4279)
ﬁrst three items are for Turkey .104, .643, and .572, for the United King-
dom −.742, .153, and −.189, and for the Netherlands −.691, .011, and −.275.
Model M0 treats all items as measurement-invariant but captures the hetero-
geneity across respondents, and it follows that around 25% of the variation in
estimated ability is explained by the grouping of students into countries.
The variation in item diﬃculties is unrealistically large since it includes the
cross-national variation in latent means, which is not explicitly modeled. This

7.6 International Comparisons of Student Achievement
219
follows directly from the estimates of model M2. The reported estimates of the
international item parameters were transformed to a standard scale to allow
a direct comparison with the international item parameter estimates of the
other models. The estimated within-country and between-country variabilities
in student achievement correspond to the estimated values of model M0.
Under model M2, conditional on the comparable cross-national variation in
the latent means, a substantial amount of cross-national variation in item
diﬃculties was found. For model M2, the country-speciﬁc diﬃculty estimates
of items 1–3 of Turkey, .087, .809, and .544, the United Kingdom, −.116, 1.029,
and .396, and the Netherlands, .033, 1.052, and .405, diﬀer substantially.
The joint hypothesis of full measurement invariance is tested by comparing
the estimated DIC value of model M2 with that of model M0. It follows that
full measurement invariance is not supported. Under the label p0(ak), the
posterior probability content of the HPD interval is given, which just includes
the event ˜akj = ak for j = 1, . . . , J for each item k. Thus, with 95% conﬁdence
it is concluded that the discriminations of items 1, 3, and 7 are invariant. In
the same way, invariance of item diﬃculty does hold for items 5, 6, and 8 with
95% conﬁdence.
The estimated cross-national variations in item discrimination of models
M1 and M2 are comparable since both models assume factor variance invari-
ance. However, cross-national variations in item discrimination may include
cross-national variations in the factor variance. Model M3 assumes noninvari-
ant factor means and variances and measurement-invariant items. In Table
7.2, the international item parameter estimates of models M3 and M4 are
given. In the same way as for the item parameter estimates of model M2, the
international item parameter estimates of model M4 were transformed to a
standard scale.
The estimated international item parameters of model M3 and their stan-
dard deviations are comparable with the estimated values of model M0. How-
ever, the population distributions of achievement diﬀer since model M3 does
not assume factor variance invariance. In Table 7.2, the general within-country
factor variance, denoted as σ2
θ, is presented, which is the mean within-country
variation. It follows that this mean level corresponds to the estimated variance
component of model M0. Under model M3, the estimated between-country
variation in mean achievement is slightly lower, which leads to a general in-
traclass correlation coeﬃcient of .21. A country-speciﬁc intraclass correlation
coeﬃcient can be computed given the country-speciﬁc residual factor variance
via ˆτ 2
00/(ˆτ 2
00 + ˆσ2
θj). The intraclass correlation coeﬃcient varies across coun-
tries from .160 (Turkey) to .296 (Iceland). The estimated intraclass correlation
coeﬃcients for the Netherlands and the United Kingdom are .194 and .180,
respectively. The range of estimated intraclass correlation coeﬃcients shows
that substantial diﬀerences are found over countries in their eﬀects on student
achievement. From the estimated DIC values of models M0 and M3, it can
also be concluded that the assumption of invariant factor variance does not
hold when assuming measurement-invariant items.

220
7 Random Item Eﬀects Models
Table 7.2. PISA 2003: Exploring cross-national item variation and factor variation.
Invariant Items
Noninvariant Items
Model M3
Model M4
Item
Mean
SD
Mean SD σak p0(ak)
Discrimination Parameter
1
.803
.027
.732 .045 .085
.436
2
1.068
.034
1.020 .125 .123
.820
3
.729
.025
.631 .038 .070
.177
4
.689
.023
.628 .036 .075
.299
5
.556
.020
.529 .032 .075
.391
6
.366
.025
.328 .063 .056
.093
7
.692
.026
.621 .039 .072
.218
8
.662
.025
.611 .043 .071
.258
Item
Mean
SD
Mean SD σbk p0(bk)
Diﬃculty Parameter
1
−.586
.016
−.588 .032 .158
.999
2
.182
.016
.185 .092 .167
.999
3
−.042
.014
−.004 .042 .118
.999
4
−.359
.015
−.367 .027 .126
.999
5
−.018
.013
−.023 .034 .105
.988
6
−1.510
.023
−1.519 .086 .127
.999
7
−.780
.017
−.785 .034 .116
.999
8
−.942
.018
−.956 .039 .092
.932
Structural Part
Fixed
γ00
.010
.073
.496 .051
Random
σ2
θ
.797
.023
.370 .011
τ 2
00
.218
.056
.104 .025
Information Criteria
−2log L
94331.2
87277.4
DIC (pD) 99984.3(5653.1)
91593.5(4316.1)
In comparison with model M3, the estimated international item param-
eters are comparable, but the estimated posterior standard deviations are
higher for model M4. This additional uncertainty is caused by the cross-
national variation in the item parameters. For model M4, the estimated cross-
national variation in discrimination parameters is slightly smaller than that
of model M2. This reduction is caused by allowing noninvariant factor vari-
ances. A country-speciﬁc common shift in variation in item discrimination is

7.7 Discussion
221
not allowed and is recognized as a shift in the country-speciﬁc factor variance.
The diﬀerence in estimated DIC values indicates that the hypothesis of factor
variance invariance is rejected given noninvariant items.
From the posterior probability contents of the HPD intervals that just
include the international item parameter values it follows that all item dis-
criminations are invariant given noninvariant factor variances. Furthermore,
with 95% conﬁdence, it is concluded that only the diﬃculty of item 8 is in-
variant.
The general model M4 allowed country-speciﬁc diﬀerences but retained a
common measurement scale. It was shown that country diﬀerences are present
in the item characteristics and in the students’ population distribution. In
PISA 2003, items are assumed to be invariant, but the present analysis shows
that full measurement invariance is not supported by the data. Ignoring the
multilevel structure in the population distribution leads to large cross-national
variation in item characteristics. That is, the corresponding estimated item
characteristic variation contains the between-country variation in students’
abilities, which complicates the statistical inferences and parameter interpre-
tations. The diﬀerent sources of variation are recognized in model M4, which
makes it possible to detect measurement and structural diﬀerences without
making unrealistic simplifying assumptions.
Making meaningful comparisons across countries is a complex issue that
requires a ﬂexible model that allows for diﬀerent levels of variation, includ-
ing cross-national diﬀerences. In this light, Goldstein (2004) argued that a
multilevel modeling approach is necessary and that country diﬀerences are in-
cluded rather than eliminated in favor of a common measurement scale. This
emphasizes the usefulness of the random item eﬀects approach that supports a
simultaneous multilevel analysis of response data from an international com-
parative survey study, accounting for cross-national diﬀerences and leading to
a common measurement scale.
7.7 Discussion
In many test settings, a test is considered to be ﬁxed and is used for measuring
students’ abilities. From this point of view, inferences are made with respect
to the test that is used without the objective of generalizing the inferences
to some population of tests. The test or item parameters are treated as ﬁxed
eﬀects. A generalization from the sample of respondents to the population
of respondents is preferred, and the person parameters are typically treated
as random eﬀects parameters. In this chapter, it is assumed that the item
characteristics of the test vary over clusters of respondents. The item param-
eters are modeled as random item eﬀects such that inferences about the item
characteristics can be generalized to the population of clusters.
The MLIRT model introduced in Chapter 6 is extended with a random
item eﬀects structure. The random item eﬀects vary over a grouping structure,

222
7 Random Item Eﬀects Models
which can be induced by a grouping of respondents (e.g., countries, schools)
or a grouping of items (e.g., item cloning, item bank). The grouping struc-
ture of the student population may diﬀer from the grouping structure of the
hierarchical item population. In that case, a cross-classiﬁed random eﬀects
model can handle the more complex data structure in which responses are
cross-classiﬁed by two grouping structures (e.g., Raudenbush and Bryk, 2002,
Chapter 12).
In this chapter, a typical grouping structure is deﬁned such that there
are two sources of between-country variation. At the level of persons, the
respondents are nested in countries and there is variation across countries
in latent means and variances. At the level of items, there is cross-country
variation in item characteristics and the country-speciﬁc characteristics are
nested in the international item characteristics. The doubly nested structure
leads to a complex identiﬁcation problem that is solved by preventing common
shifts of the country-speciﬁc item characteristics. Background information can
be used to explain the between-country variability.
The MCMC algorithm can also handle polytomous response data, leading
to random threshold eﬀects. De Jong et al. (2007) present an application of
consumers’ susceptibility to normative inﬂuence given cross-national polyto-
mous response data. The model allows for cross-national random threshold
eﬀects besides cross-national diﬀerences in scale usage. Fahrmeir and Tutz
(2001) and Tutz and Hennevogl (1996) extended the ordinal cumulative model
with random threshold eﬀects that may vary over clusters or subjects. In
that case, the preference for response categories can vary across individu-
als. De Jong and Steenkamp (2009) incorporated a mixture distribution for
the random item eﬀects parameters to allow latent class diﬀerences. In this
approach, unexplained cross-national heterogeneity in random item character-
istics is partly captured by latent item class diﬀerences. Soares, Gon¸calves and
Gamerman (2009) used a mixture modeling approach to classify an item as
measurement-invariant (anchor item) or that exhibits diﬀerential item func-
tioning (DIF item). The invariant as well as the noninvariant item charac-
teristics are all identiﬁed and estimated simultaneously with the other model
parameters.
7.8 Exercises
7.1. Rasch (1960) assumed that comparisons between persons are invariant
over items and comparisons between items are invariant over persons (e.g.,
Embretson and Reise, 2000).
(a) For the one-parameter logistic response model, show that the diﬀerence
in log odds for an item k of two persons indexed i and i′ can be expressed as
f (P (Yik) , P (Yi′k)) = ln
P (Yik)
1 −P (Yik) −ln
P (Yi′k)
1 −P (Yi′k)
= (θi −bk) −(θi′ −bk),

7.8 Exercises
223
where P (Yik) = P (Yik = 1 | θi, bk).
(b) Argue that the comparison in (a) between the two persons is independent
of the item k that is used (invariant-person comparison).
(c) Show that the diﬀerence in log odds for a person i of two items indexed k
and k′ can be expressed as
f (P (Yik) , P (Yik′)) = (θi −bk) −(θi −bk′).
(d) Argue that the comparison between the two items in (c) is independent
of the person i that is used to compare them (invariant-item comparison).
7.2. (continuation of Exercise 7.1) Consider the logistic one-parameter re-
sponse model for hierarchically structured response data.
(a) Assume that the respondents are nested in groups. Show that the diﬀerence
in log odds of two persons to item k can be expressed as
f (P (Yijk) , P (Yi′j′k)) = (θij −bk) −(θi′j′ −bk) ,
where θij ∼N
 β0j, σ2
θ

.
(b) Explain that the diﬀerence in (a) is independent of the item and that it
consists of a between-group as well as a between-individual diﬀerence in log
odds.
(c) Assume random item diﬃculty parameters for the noninvariant items.
Show in the same way that the diﬀerence in log odds of two noninvariant
items for a person represents a between-group as well as a between-individual
part.
7.3. To improve understanding of the identiﬁcation issue, assume that re-
sponse data from one respondent per country were observed. A random item
eﬀects model for the latent response data can be stated as
Zjk = β0j −˜bkj + ϵjk,
where β0j ∼N(γ00, τ 2), ˜bkj ∼N(bk, σ2
bk), and ϵjk ∼N(0, 1).
(a) Show that a common shift in the item diﬃculties of country j can be
counterbalanced by a common negative shift in the country mean β0j.
(b) Is it possible to identify this random item eﬀects model by ﬁxing one latent
country mean, which would be the standard country?
(c) Explain how the model can be identiﬁed via an anchor item.
7.4. Consider the random item eﬀects model in Equation (7.7), but assume
ﬁxed item discriminations, ˜akj = 1, for each j and k. Let θi ∼N (0, 1) and
˜bkj be normally distributed with mean zero and standard deviation σbk = .15.
Use Figure 7.2 to answer the following questions.
(a) For θi = 0, explain and determine the vertical distance between the item
characteristic curves when bh and bl are the upper and lower limits of the 95%
density interval of diﬃculty parameter ˜bkj.

224
7 Random Item Eﬀects Models
-3
-2
-1
0
1
2
3
0.0
0.2
0.4
0.6
0.8
1.0
0.9
0.6
0.3
0.0
-0.3
-0.6
-0.9

bh
bl
bh
bl
Fig. 7.2. Item characteristic curves for diﬀerent diﬃculty values from the random
eﬀects distribution (right vertical axis). The probability of a correct response (left
vertical axis) is given as a function of the ability parameter (horizontal axis) for
item diﬃculty values from the random diﬃculty distribution (right vertical axis).
(b) For a success probability of .5, explain and determine the horizontal dif-
ference in item characteristic curves.
(c) For a constant ability level, does a linear change in item diﬃculty lead to
a linear change in the probability of success?
(d) The corresponding densities of probabilities are graphed to the right of
the left vertical axis. Explain the shift in means of the densities.
(e) A population average item characteristic curve can be deﬁned as the ex-
pected success probability as a function of the ability, where the expectation is
taken over the density of the random diﬃculty parameter; see Equation (7.10).
Why will the average success probabilities be shrunk towards .5? Outline the
population average characteristic curve.
7.5. Consider the combined random item eﬀects model for binary response
data as stated in Equation (7.12).
(a) Show that the total variance of Zijk equals
Var (Zijk) = σ2
akσ2
θ + σ2
akµ2
θ + a2
kσ2
θ + σ2
bk + 1.
Use the iterated conditional variance identity to handle the product of two
normally distributed variables:
Var (˜akjθi) = E (Var (˜akjθi | θi)) + Var (E (˜akjθi | θi)) .

7.8 Exercises
225
(b) Derive the intraclass correlation coeﬃcient given in Equation (7.13).
(c) The combined random item eﬀects model is extended with indepen-
dent normally distributed priors for the (international) item parameters,
ak ∼N
 µa, σ2
a

and bk ∼N
 µb, σ2
b

. Show that the total variance of Zijk
equals
Var (Zijk) = σ2
aσ2
θ + σ2
aµ2
θ + µ2
aσ2
θ + σ2
akσ2
θ + σ2
akµ2
θ + σ2
bk + σ2
b + 1.
(d) Derive three diﬀerent kinds of intraclass correlation coeﬃcients: (1) the
correlation between latent responses to item k of diﬀerent respondents from
the same country, (2) the correlation between latent responses to item k from
diﬀerent respondents from diﬀerent counties, and (3) the correlation between
latent responses to diﬀerent items from the same respondent.
7.6. Implement an M-H step for the variance parameter σ2
κk using an inverse
gamma prior. The conditional posterior density of the threshold variance pa-
rameter σ2
κk can be stated as
p
 σ2
κk | ˜κk, κk

∝
Y
j
p
 ˜κkj | κk, σ2
κk

p
 σ2
κk

.
(a) Show that the full conditional posterior distribution does not reduce to
standard form due to order constraints.
(b) Deﬁne a suitable proposal distribution for generating candidates.
(c) Deﬁne the corresponding acceptance ratio, and summarize the M-H step
in an algorithmic form.

8
Response Time Item Response Models
Response times and responses can be collected via computer adaptive testing
or computer-assisted questioning. Inferences about test takers and test items
can therefore be based on the response time and response accuracy informa-
tion. Response times and responses are used to measure a respondent’s speed
of working and ability using a multivariate hierarchical item response model.
A multivariate multilevel structural population model is deﬁned for the per-
son parameters to explain individual and group diﬀerences given background
information. An application is presented that illustrates novel features of the
model.
8.1 Mixed Multivariate Response Data
Nowadays, response times (RTs) are easily collected via computer adaptive
testing or computer-assisted questioning. The RTs can be a valuable source
of information on test takers and test items. The RT information can help
to improve routine operations in testing such as item calibration, test design,
detection of cheating, and adaptive item selection.
The collection of multiple item responses and RTs leads to a set of mixed
multivariate response data since the individual item responses are often ob-
served on an ordinal scale, whereas the RTs are observed on a continuous
scale. The observed responses are imperfect indicators of a respondent’s abil-
ity. When measuring a construct such as ability, attention is focused on the
accuracy of the test results. The observed RTs are indicators of a respondent’s
speed of working, and speed is considered to be a diﬀerent construct. As a
result, mixed responses are used to measure the two constructs ability and
speed.
Although response speed and response accuracy measure diﬀerent con-
©
227
J.-P. Fox, Bayesian Item Response Modeling: Theory and Applications, Statistics for Social
and Behavioral Sciences, DOI 10.1007/978-1-4419-0742-4_8,
structs (Schnipke and Scrams, 2002, and references therein), the reaction-time
  Springer Science+Business Media, LLC 2010
research in psychology indicates that there is a relationship between response

228
8 Response Time Item Response Models
speed and response accuracy (Luce, 1986). This relationship is often charac-
terized as a speed–accuracy trade-oﬀ. A person can decide to work faster, but
this will lead to a lower accuracy. The trade-oﬀis considered to be a within-
person relationship: a respondent controls the speed of working and accepts
the related level of accuracy. It will be assumed that each respondent chooses
a ﬁxed level of speed, which is related to a ﬁxed accuracy.
A hierarchical measurement model was proposed by van der Linden (2007)
to model RTs and dichotomous responses simultaneously that accounts for
diﬀerent levels of dependency. The diﬀerent stages of the model capture the
dependency structure of observations nested within persons at the observa-
tional level and the relationship between speed and ability at the individual
level. Klein Entink, Fox and van der Linden (2009a), and Fox, Klein Entink
and van der Linden (2007) extended the model for measuring accuracy and
speed (1) to allow time-discriminating items, (2) to handle individual and/or
group characteristics, and (3) to handle the nesting of individuals in groups.
This extension has a multivariate multilevel structural population model for
the ability and the speed parameters that can be considered a multivariate
extension of the structural part of the MLIRT model of Chapter 6. In this
chapter, the complete modeling framework will be discussed, and an extension
is made to handle polytomous response data.
The RTs and responses have been modeled from diﬀerent viewpoints.
Scheiblechner (1979) and Maris (1993) explicitly modeled RTs separately from
the responses. They both focused on uncomplicated cognitive tasks, where it
is assumed that most items would be solved correctly were there enough time,
and excluded accuracy scores. Among others, Roskam (1997) and Verhelst,
Verstralen and Jansen (1997) developed a regular item response model with
time parameters added to model RTs and responses. Thissen (1983) devel-
oped a timed-testing model for RTs that contains a response accuracy term.
This leads to a log-linear relationship between response speed and response
accuracy. Schnipke and Scrams (2002) give an overview of RT models for test
items, and van der Linden (2007) discusses the main diﬀerences between a
multilevel modeling perspective for RTs and single-level RT models for test
items.
8.2 Measurement Models for Ability and Speed
An item-based test is used to measure ability as an underlying construct that
cannot be observed directly. In general, item response models (as discussed in
Chapter 4) are adopted to make inferences from the multivariate item response
data since they describe the probability of a correct response to an item as a
function of ability and item characteristics. The notation remains the same:
the observation of item k (k = 1, . . . , K) of person i (i = 1, . . . , nj) in group
j (j = 1, . . . , J) is denoted as yijk.

8.2 Measurement Models for Ability and Speed
229
Various families of distributions have been employed in psychometric ap-
plications to model RT data; e.g., Poisson, gamma, Weibull, and lognormal
distributions (see Maris, 1993; Schnipke and Scrams, 1997; Thissen, 1983; van
Breukelen, 2005; van der Linden, 2006). Here, the log of the RTs is assumed
to be normally distributed. Note that RT distributions are skewed to the right
since RTs have a natural lower bound at zero. In Section 8.5, it will be shown
that RTs combined with latent continuous item responses can be treated in a
multivariate model, which simpliﬁes the statistical inferences.
Let Tijk denote the log-RT of person i in group j on item k. Then, Tijk is
normally distributed, with a mean depending on the speed at which the person
works denoted as ζij. Speed is assumed to be the underlying construct that is
measured via the item RTs. It is assumed that persons work with a constant
speed during a test. A higher speed of working leads to lower RTs, which is
recognized by deﬁning a negative linear relationship between the log-RTs and
the speed parameter.
Items have diﬀerent time intensities, which means that they diﬀer in the
time that is required to obtain their solution at a constant level of speed. The
time intensity of item k is given by λk. A higher λk indicates that item k is
expected to consume more time. In Figure 8.1, the item characteristic curves
(ICCs) of two dichotomous items are given, where the left y-axis presents the
probability of a correct response as a function of the ability parameter (x-axis).
The items have diﬀerent levels of diﬃculty since their success probabilities
diﬀer at a constant level of ability. An increase in ability leads to an increase
in the success probability. The increase is the same for both items since the
discrimination parameters are the same. Now, let the person parameter on the
x-axis denote the speed of working. For both items, the expected RT is given
(on the right y-axis) as a function of the speed parameter. The so-called RT
characteristic curve (RTCC) shows that the RTs decrease with speed. It can
be seen that at a constant speed the RTs of both items diﬀer, which indicates
that the items have diﬀerent time intensities. The upper RTCC corresponds
to the most time-intensive item. Furthermore, an increase in the speed leads
to a similar decrease in RTs.
The eﬀect of changing the level of speed may vary across items. Items
may diﬀer with respect to the decrease in RT when increasing the level of
speed. Therefore, an item characteristic time-discrimination parameter ϕk
is introduced. In Figure 8.2, ICCs and RTCCs of two items are given with
diﬀerent discrimination and time-discrimination parameters, respectively. It
can be seen that the diﬀerence in expected RTs between persons working at
diﬀerent speed levels is less for the lower time-discriminating item. The same
holds for the ICCs. The less discriminating item shows a smaller diﬀerence
in success probabilities for persons with diﬀerent abilities. The diﬃculty and
time-intensity parameters are equal for both items (b = 0, λ = 4).
The model for measuring a person’s speed level from the RTs resembles the
two-parameter item response model for measuring ability. This measurement
model also has two item characteristic parameters, the time intensity and

230
8 Response Time Item Response Models
.4
.2
0
.6
.8
1.0
100
50
0
150
200
250
-1.5
-1.0
-0.5
0.0
0.5
1.0
1.5
ICC (b=0)
ICC (b=1)
RTCC ( = )
 
RTCC ( = )
 
Person Parameter
Fig. 8.1. Item characteristic curves (ICCs) and response time characteristic curves
(RTCCs) for two items with equal discrimination parameters.
100
0
200
300
400
.4
.2
0
.6
.8
-1.5
-1.0
-0.5
0.0
0.5
1.0
1.5
1.0
ICC (a=.8)
ICC (a=1.2)
RTCC ( =.8)

RTCC ( =1.2)

Person Parameter
Fig. 8.2. Item characteristic curves (ICCs) and response time characteristic curves
(RTCCs) for two items with equal diﬃculty and time-intensity parameters.
the time-discrimination parameter, and a (unidimensional) underlying latent
variable. The latent variable is also nonlinearly related to the observations.
The correspondence is closest for continuous observed responses; for these,

8.3 Joint Modeling of Responses and Response Times
231
see, for instance, Mellenbergh (1994b) and Shi and Lee (1998). The model is
referred to as the item response time model and is given by
Tijk = λk −ϕkζij + ϵζijk,
(8.1)
where ϵζijk ∼N
 0, ω2
k

. Notice that the interpretation of the model parame-
ters in Equation (8.1) results in a diﬀerent location of the minus sign compared
with the two-parameter item response model. In Figures 8.1 and 8.2, the ex-
pected RTs are plotted, which are nonlinearly (exponentially) related to the
speed parameter. The expected log-RTs are linearly related to the speed pa-
rameter; see Equation (8.1). Then, the corresponding log-RT characteristic
curve is a straight line.
A person’s observed RTs are assumed to be independent given the level of
speed. This conditional independence assumption can be stated as
p (tij | ζij) =
Y
k
p (tijk | ζij) .
(8.2)
In a similar way, the item responses of a person are conditionally independent
given the level of ability; see Equation (1.1). The two conditional independence
assumptions induce a third conditional independence assumption. A person’s
item response and item response time are conditionally independent given the
level of ability and speed. As a result, the following factorizations can be made
p (yij, tij | θij, ζij) =
Y
k
p (yijk, tijk | θij, ζij)
=
Y
k
p (yijk | θij) p (tijk | ζij)
= p (yij | θij)
Y
k
p (tijk | ζij)
= p (yij | θij) p (tij | ζij) .
(8.3)
In the second step, the induced conditional independence assumption is used.
In the third and fourth steps, the conditional independence assumptions of the
item response model and the item response time model are used, respectively.
8.3 Joint Modeling of Responses and Response Times
The total variation in observed RTs and responses can be partitioned into
variations due to (i) response variation in item RTs and item responses, (ii) the
sampling of persons and items, and (iii) the sampling of groups. The diﬀerent
sources that contribute to the variation between item responses and item RTs
are modeled via random eﬀects. The three observed levels of hierarchy lead
to diﬀerent stages of the model.

232
8 Response Time Item Response Models
Response variation is modeled via an item response model and an item
response time model, and they deﬁne level 1 of the joint model. The measure-
ment models deﬁne a probabilistic relationship between the RTs and responses
and the underlying person parameters. The normal ogive item response model
is used since the underlying continuous responses (Equation (4.7)) with the
corresponding log-RTs are multivariate normally distributed. Then, a multi-
variate normal joint measurement model can be constructed for the ability
and speed parameters, which will simplify the statistical inferences.
8.3.1 A Structural Multivariate Multilevel Model
At level 2, the ability and speed parameters are considered as outcome vari-
ables of a multivariate multilevel model. They are modeled to have a multi-
variate normal distribution. This allows the speciﬁcation of a within-person
correlation structure for the ability and speed parameters. Between-person
diﬀerences in ability and speed can be modeled by covariates x. Subsequently,
group diﬀerences between the latent outcome variables are explained as a
function of group-level covariates w at a third level. The higher-level regres-
sion structure for the latent person parameters makes it possible to partition
their total variance into within-group and between-group components. This
way, inferences can be made about the person parameters for diﬀerent groups
simultaneously given the explanatory variables.
The latent outcomes (speed and ability) of respondent i in group j are lin-
early related to xij =
 xt
1ij ⊕xt
2ij

. The regression eﬀects, βj = vec (β1j, β2j)
are allowed to vary across groups where the matrix operation vec creates a
column vector by stacking all columns under each other. Now, the structural
multivariate multilevel model can be expressed as
[θij, ζij] =

xt
1ij, xt
2ij

[β1j, β2j] +

eθij, eζij

,
(8.4)
[β1j, β2j] =

wt
1j, wt
2j

[γ1, γ2] + [u1j, u2j] .
(8.5)
The regression coeﬃcients are speciﬁed as random but can be restricted to
be common to all groups. The error terms at each level are assumed to be
multivariate normally distributed, and the level-2 error terms are assumed to
be independent of the level-3 error terms.
The structural multivariate outcome variables can be stacked in a vector.
Let Ωij = (θij, ζij)t. It follows that
Ωij = xijβj + eij,
(8.6)
βj = wjγ + uj,
(8.7)
where wj = (w1j ⊕w2j) and eij =
 eθij, eζij
t is multivariate normally dis-
tributed with mean zero and covariance matrix ΣP . The covariance matrix
ΣP equals
ΣP =
σ2
θ ρ
ρ σ2
ζ

.

8.3 Joint Modeling of Responses and Response Times
233
The covariance parameter ρ represents the within-person dependency be-
tween speed and ability, and the person parameters speed and ability are
independent for ρ = 0. This independency is conditional on the structure of
the item parameters since such a dependency can also be modeled via a co-
variance structure on the item parameters (Klein Entink et al., 2009b). When
ρ is positive, persons who work faster on average are expected to have above-
average abilities. The group-level error term, uj, is assumed to be multivariate
normally distributed with mean zero and covariance matrix V. More stable
covariance parameter estimates can be obtained by restricting this covariance
matrix to be block-diagonal with diagonal matrices V1 and V2. In this case,
the random eﬀects in the regression of θ on x1 are allowed to correlate but
are independent of those in the regression of ζ on x2.
Let Ωj be a single vector that contains the vector of individual abilities and
the vector of speed levels stacked on top of each other. Then, the structural
multilevel model for group j becomes
Ωj = xjβj + ej
= xjwjγ + xjuj + ej,
(8.8)
where xj = (x1j ⊕x2j) and ej ∼N
 0, ΣP ⊗Inj

. Note that the structural
multilevel part in Equation (8.8) is closely related to the structural multilevel
part of the MLIRT model (Equation (6.9)) but the latter assumes indepen-
dence between the latent outcomes given the random group eﬀects.
Marginalizing over the random regression eﬀects in Equation (8.8), the
conditional covariance structure of Ωj becomes
Cov (Ωj | xj, wj) = xjVxt
j + ΣP ⊗Inj.
(8.9)
The covariance structure consists of a component that deals with the between-
group heterogeneity and the second component represents the within-person
covariance structure. This within-person structure is assumed to be common
across individuals and groups.
The structural parts in Equations (8.6) and (8.7) are referred to as level-2
and level-3 models, respectively. The structural component of the model allows
a simultaneous regression analysis of all person parameters on explanatory
variables at the individual and group levels while taking into account the
dependencies between the individuals within each group. As a result, among
other things, conclusions can be drawn as to the size of the eﬀects of the
explanatory variables on the test takers’ ability and speed as well as the
correlation between these person parameters. Note that hypotheses on these
eﬀects can be tested simultaneously.
The model can be used for various analyses. First, the analysis might fo-
cus on the item parameters; more speciﬁcally, the relationships between the
characteristics of the items in the domain covered by the test. For example,
the correlation between the time intensity and diﬃculty parameters of the
items can be investigated. Second, the analysis could be about the structural

234
8 Response Time Item Response Models
relationships between explanatory information at the individual and/or group
levels and the test takers’ ability and speed. For example, the variance com-
ponents of the structural model might help in exploring the partitioning of
the variance of the speed parameters across the diﬀerent levels of analysis.
Third, interest might be in the random eﬀects in the model (e.g., to identify
atypical individuals or groups with respect to their ability or speed).
8.3.2 The RTIRT Likelihood Model
The likelihood part of the model consists of three stages. At level 1, the simul-
taneous measurement framework for responses and RTs is deﬁned. At level 2,
a structural multivariate multilevel model is deﬁned for both underlying latent
variables. The structural multivariate multilevel model consists of two stages.
The three stages will be referred to as the likelihood part of the RTIRT (re-
sponse time item response theory) model. A more comprehensive name could
also stress the integration of both measurement models for measuring speed
and ability with the structural multivariate multilevel model that explains
diﬀerences at the diﬀerent levels of ability and speed.
In Figure 8.3, a path diagram of the likelihood part of the RTIRT model
for responses and RTs is given. The two ellipses represent the measurement
models for the multivariate outcome variables θij and ζij. Note that the mea-
surement models have diﬀerent functional forms. In this case, a lognormal
model is used for the continuous RTs (upper ellipse) and a normal ogive
model is used for the discrete responses (lower ellipse). The item parameters
of both measurement models, Λ1k and Λ2k, are allowed to correlate to capture
within-item correlations.
Three boxes are plotted, where the box indexed as items is plotted in
the box individual, which is plotted in the box group to illustrate the nested
structure of the data. The individual and group boxes constitute the struc-
tural multivariate multilevel model for Ωij. In this structural part of the
model, the latent variables are allowed to correlate to model unexplained
within-individual correlation between ability and speed. It follows that it is
possible to construct an underlying within-item and/or within-person corre-
lation structure to deal with the correlations between RTs and responses.
There are three levels of uncertainty: (1) at the observation level, (2) at
the individual level, and (3) at the group level. In this multivariate case,
the measurement models quantify measurement error corresponding to the
nonlinear relationship between responses and the individual abilities, and RTs
and the individual speed levels. The explanatory information xij and wj at the
individual and group levels explains simultaneously variability in the ability
and speed levels within and between groups.
The likelihood of the RTIRT model can be obtained by integrating over
the random eﬀects. It follows that

8.4 RTIRT Model Prior Speciﬁcations
235
Group j
Individual i
qij
L1k
xij
bj
wj
Yijk
Item k
zij
L2k
Tijk
Fig. 8.3. Path diagram for the RTIRT model.
p (y, t | a, b, ϕ, λ, ΣP , V) =
Y
j
Z "Z Z Y
i|j
p (yij | θij, a, b) p (tij | ζij, ϕ, λ) ·
p (θij, ζij | xij, βj, ΣP ) dθijdζij
#
p (βj | wj, γ, V) dβj.
(8.10)
At the observation level, the product of distributions of the observations fol-
lows from the conditional independence assumptions; see Equation (8.3). This
represents the measurement part of the model. The other levels describe the
population distributions of the person parameters and random group eﬀects.
8.4 RTIRT Model Prior Speciﬁcations
Prior distributions are to be speciﬁed for the ﬁxed eﬀect parameters γ, the
item parameters, and the covariance parameters (ΣP , V). A multivariate nor-
mal prior distribution with mean γ0 and covariance matrix Σγ is a conjugate
prior for the ﬁxed eﬀects parameters. Attention is focused on a prior for ΣP
with a model identifying restriction. Furthermore, an item prior structure is
deﬁned for the measurement models that allows for within-item correlations.
8.4.1 Multivariate Prior Model for the Item Parameters
The exchangeable prior distribution for the item parameters of both mea-
surement models is speciﬁed such that, for each item k, the vector Λk =
(ak, bk, ϕk, λk) is assumed to follow a multivariate normal distribution with

236
8 Response Time Item Response Models
mean vector µI = (µa, µb, µϕ, µλ) and covariance matrix ΣI. The assumption
introduces a common within-item correlation structure. For example, it may
be expected that easy items require less time to be solved than more diﬃ-
cult items. If so, the time intensity parameter correlates positively with the
item diﬃculty parameter. The guessing parameter of the response model has
no analogous parameter in the item response time model (since there is no
guessing aspect for the RTs). Therefore, it does not serve a purpose to in-
clude it in this multivariate model, and an independent prior for the guessing
parameter is speciﬁed.
8.4.2 Prior for ΣP with Identifying Restrictions
The model is identiﬁed by ﬁxing the metric of the two latent person parameters
ability and speed. The metric of the ability parameter is identiﬁed by ﬁxing
the mean and the variance. This is done by ﬁxing the general mean to zero,
µθ = 0, and the variance, σ2
θ, to one. The RTs have a natural unit. However,
there is an indeterminacy in the mean (variance) that is determined via the
item intensity (time-discrimination) parameters and the mean (variance) of
the latent speed parameter. The variance of the metric is identiﬁed via the
restriction Q ϕk = 1. The mean of the metric is identiﬁed by ﬁxing the general
mean to zero, µζ = 0.
Within an MCMC algorithm, both general means are easily restricted
to zero by transforming in each iteration the sampled vector of ability and
speed values in such a way that each transformed sample has a mean of zero.
Subsequently, a prior is chosen such that σ2
θ = 1 with probability one, and
the covariance matrix of the latent person parameters (θij, ζij)t equals
ΣP =
 1 ρ
ρ σ2
ζ

.
(8.11)
Note that a multivariate probit model is identiﬁed by ﬁxing the diagonal ele-
ments of the covariance matrix (Chib and Greenberg, 1995) but that, because
of the special nature of the RTs, in the current case only one element of ΣP
has to be ﬁxed.
In general, there are two issues when restricting a covariance matrix. First,
deﬁning proper priors for a restricted covariance matrix is rather diﬃcult. For
example, for the conjugate inverse Wishart prior, there is no choice of param-
eter values that reﬂects a restriction on the variance of the ability parameter
such as that above. For the multinomial probit model, McCulloch, Polson
and Rossi (2000) tackled this problem by specifying proper diﬀuse priors for
the unidentiﬁed parameters and reporting the marginal posterior distribu-
tions of the identiﬁed parameters. However, it is hard to specify prior beliefs
about unidentiﬁed parameters. Second, for a Gibbs sampler, sampling from
a restricted covariance matrix requires extra attention. Chib and Greenberg
(1995) deﬁned individual priors on the free covariance parameters but, as a

8.4 RTIRT Model Prior Speciﬁcations
237
result, the augmented data had to be sampled from a special truncated region
and the values of the free covariance parameter could only be sampled us-
ing an M-H step. However, such steps involve the speciﬁcation of an eﬀective
proposal density with tuning parameters that can only be ﬁxed through a
cumbersome process.
A general approach for sampling from a restricted covariance matrix can
be found in Browne (2006), but it also is based on an M-H algorithm. For
completeness, there is an alternative approach. Barnard, McCullogh and Meng
(2000) formulated a prior directly for the identiﬁed parameters. In order to do
so, they factored the covariance matrix into a diagonal matrix with standard
deviations and a correlation matrix and speciﬁed an informative prior for
the latter. This prior was then incorporated into a Griddy-Gibbs sampler.
However, such algorithms can be slow and require the choice of a grid size
and boundaries. Boscardin and Zhang (2004) followed a comparable approach
but used a parameter-extended M-H algorithm for sampling values from the
conditional distribution of the correlation matrix.
In the present approach, the conditional distribution of ζij given θij has a
normal density,
ζij | θij, βj, ρ, σ2
ζ ∼N
 xt
2ijβ2j + ρ(θij −xt
1ijβ1j), ˜σ2
ζ

,
where ˜σ2
ζ = σ2
ζ −ρ2. Parameter ρ can be viewed as the regression parameter
in a normal regression problem of ζij on θij with variance ˜σ2
ζ. Specifying
conjugate priors for these parameters,
ρ ∼N
 ρ0, σ2
ρ

,
˜σ2
ζ ∼IG (g1, g2) ,
the full conditional posterior densities become
ρ | θ, ζ, β ∼N

∆

ρ0σ−2
ρ
+ ˜σ−2
ζ
(θ −x1β1)t (ζ −x2β2)

, ∆

, (8.12)
˜σ2
ζ | θ, ζ, β, ρ ∼IG
 g1 + N/2, g2 + ΞtΞ/2

,
(8.13)
where
∆−1 = ˜σ−2
ζ
(θ −x1β1)t (θ −x1β1) + σ−2
ρ ,
Ξ = (ζ −x2β2) −ρ (θ −x1β1) .
When implementing an MCMC sampler, random draws of the elements of
covariance matrix ΣP in Equation (8.11) are constructed from the samples
drawn from (8.12) and (8.13).
The (symmetric) covariance matrix constructed from the sampled values
is positive deﬁnite. The determinant of ΣP equals |ΣP | = σ2
ζ −ρ2 = ˜σ2
ζ and
˜σ2
ζ > 0, and it follows that the determinant |ΣP | > 0. The latter is suﬃcient
to guarantee that symmetric matrix ΣP is positive deﬁnite and that it has an
inverse.

238
8 Response Time Item Response Models
The key element of the present approach is the speciﬁcation of a proper
prior distribution for the covariance matrix with one ﬁxed diagonal element
and the construction of random draws from the matrix of the corresponding
conditional posterior distribution. The draws will show more autocorrelation
due to this new parameterization. This implies that more MCMC iterations
are needed to cover the support of the posterior distribution adequately, a
measure that only involves a (linear) increase in the running time of the
sampler. On the other hand, convergence of the algorithm is easily established
without having to specify any tuning parameter.
8.5 Exploring the Multivariate Normal Structure
A linear relationship is established between the new augmented variable Zijk
and the ability parameter via data augmentation. For binary response data,
the augmentation step is deﬁned in Equation (4.7), and for polytomous re-
sponse data, it is deﬁned in Equation (4.25). Note that an additional sampling
step is introduced when using the three-parameter item response model. The
normally distributed augmented response data and the log-RTs are consid-
ered to be outcome variables. The dependent level-1 measurements are lin-
early related to the latent person parameters and can be analyzed jointly as
multivariate data.
Statistical inferences can be made from the complete data using the fac-
torization
p
 y, t, z, s | Λ, c, γ, ΣP , V

= p
 y | z, s

p(s|c)p
 z, t | Λ, γ, ΣP , V

(8.14)
where s denotes the augmented data for knowing or not knowing the correct
answer. Interest is focused on exploring the relationships between ability and
speed. Therefore, the term on the far right-hand side of (8.14) will be explored
in more detail. This likelihood can be taken to be that of a normal multivariate
multilevel model,
p
 z, t | Λ, γ, ΣP , V

=
Z Z Z
p
 z | θ, a, b

p
 t | ζ, ϕ, λ

p
 ζ, θ | β, ΣP

p
 β | γ, V

dθdζdβ.
(8.15)
The ﬁrst two factors in this decomposition occur because of the independence
of the responses and RTs given the latent person parameters. The last two
factors represent the structural multivariate multilevel part of the model.
The augmented response data and log-RTs are stacked in one vector to
explore simultaneously the within-item and between-item relations for ability
and speed. For binary response data, the vector of augmented data Zij =
(Zij1, . . . , ZijK)t plus the vector of diﬃculty parameters, b, and the similar
vector of log-RTs Tij = (Tij1, . . . , TijK)t minus the vector of time intensity

8.5 Exploring the Multivariate Normal Structure
239
parameters, λ, are stacked in a vector Z∗
ij. Then, both measurement models
can be presented as a linear regression structure,
Z∗
ij = (a ⊕−ϕ) (θij, ζij)t + ϵij
= HP Ωij + ϵij,
(8.16)
where ϵij ∼N
 0, IK ⊕IKω

. A similar linear relationship can be found for
polytomous data using the augmentation step in Equation (4.25), and in that
case, Z∗
ij = vec (Zij, Tij −λ).
Inferences from this multivariate model are simpliﬁed by taking advantage
of some of the properties of the multivariate normal distribution. For exam-
ple, assume for a moment that the item parameters are known, and deﬁne
Z∗
ij =vec(˜Zij, ˜Tij) = vec(Zij + b, Tij −λ). Level 1 and level 2 of the model
can then be represented by the following multivariate structure:


θij
ζij
˜Zij
˜Tij

∼N






xt
1ijβ1j
xt
2ijβ2j
aθij
−ϕζij

,


σ2
θ
ρ
σ2
θat
−ρϕt
ρ
σ2
ζ
ρat
−σ2
ζϕt
aσ2
θ
aρ
aσ2
θat + IK
−aρϕt
−ϕρ −ϕσ2
ζ
−ϕρat
ϕσ2
ζϕt + ωIK





.
(8.17)
This representation provides insight into the complex correlational structure
hidden in the data and entails several possible inferences. It also helps us to de-
rive some of the conditional posterior distributions for the MCMC sampling
algorithm (e.g., the conditional posterior distributions of the latent person
parameters given the augmented data). For a general treatment of the deriva-
tion of conditional distributions from multivariate normal distributions, see,
for instance, Searle et al. (1992).
Parameter ρ, which controls the covariance between the θs and ζs, plays
an important role in the model. It can be considered the bridge between the
separate measurement models for ability and speed. Therefore, its role within
the hierarchical structure will be explored in more detail.
The conditional covariance between the latent response and log-RT on
item k is given by
Cov (Zijk, Tijk | ρ, Λk) = Cov
 akθij −bk + ϵθijk, −ϕkζij + λk + ϵζijk

= Cov
 akθij + ϵθijk, −ϕkζij + ϵζijk

= Cov (akθij, −ϕkζij)
= −ak Cov (θij, ζij) ϕk
= −akρϕk,
due to independence between the residuals as well as between the residuals
and the person parameters. Since ak and ϕk are positive, the latent response
and log-RT correlate negatively when ρ is positive. So, in spite of conditional
independence between the responses and log-RTs given the person parameters,
their correlation is negative.

240
8 Response Time Item Response Models
Due to properties of the multivariate normal distribution, the conditional
distribution of θij given ζij is also normal:
θij | ζij, βj, σ2
θ, σ2
ζ, ρ ∼N

xt
1ijβ1j + ρσ−2
ζ (ζij −xt
2ijβ2j), σ2
θ −ρ2σ−2
ζ

.
A greater covariance ρ between the person parameters gives a greater reduc-
tion of the conditional variance of θij given ζij. The expression also shows that
the amount of information about θij in ζij depends both on the precision of
measuring the speed parameter and its correlation with the ability parameter.
It can be seen that, for ρ > 0, the prior expected value of θij is assumed to
be higher when working at a higher speed.
From Equation (8.17), it also follows that for ρ = 0 the conditional poste-
rior expectation of θij given ζij and the response data reduce to
E
 θij | β1j, ˜zij, σ2
θ, a, b

=
 ata + σ−2
θ
−1 at˜zij + σ−2
θ xt
ijβ1j

.
(8.18)
This expression can be recognized as the precision-weighted mean of the pre-
dictions of θij from the (augmented) response data and from the linear re-
gression of θ on x; see Equation (6.12).
In Equation (8.17), in addition to the responses and RTs, the random
test takers were the only extra source of heterogeneity. But another level of
heterogeneity was added in (8.8), where the test takers were assumed to be
nested within groups and the regression eﬀects were allowed to vary randomly
across them. Because of these random eﬀects and correlations, the marginal
covariances between the measurements change.
Several comments can be made with respect to the structural multivariate
multilevel part of the model:
•
In Equation (8.17), a special structure (compound symmetry) for the co-
variance matrix of the residuals at the level of individuals was shown to
exist. This structure may lead to more eﬃcient inferences. For a general
discussion of possible parameterizations and estimation methods for multi-
variate random eﬀects structures, see, for instance, Harville (1977), Rabe-
Hesketh and Skrondal (2001), and Reinsel (1983).
•
Linear multivariate three-level structures for continuous responses are dis-
cussed by Goldstein (2003) and Snijders and Bosker (1999), among oth-
ers. As already indicated, the covariance structure of the level-3 random
regression eﬀects is assumed to be block-diagonal. This means that the
parameters in the regression of θ on x are conditionally independent of
those in the regression of ζ on x. It is possible to allow these parameters
to correlate, but this option is unattractive when the dimension of the
covariance matrix becomes large. Typically, the covariance matrix is then
poorly estimated (Laird and Ware, 1982).
•
For the same reason, the covariance matrix in the multivariate normal prior
of the ﬁxed eﬀects in Equation (8.5) is assumed to be block-diagonal. The
Bayesian approach allows us to specify diﬀerent levels of prior information
about this matrix.

8.6 Model Selection Using the DIC
241
8.6 Model Selection Using the DIC
As shown in Section 3.2.3, a well-known criterion of model selection based on
a deviance ﬁt measure is the Bayesian information criterion (BIC; Schwarz,
1978). This criterion depends on the eﬀective number of parameters in the
model as a measure of model complexity, which is often diﬃcult to calculate
for hierarchical models. Although the nominal number of parameters follows
directly from the likelihood, the prior distribution imposes additional restric-
tions on the parameter space and reduces its eﬀective dimension.
A DIC can be formulated for choosing between models that diﬀer in the
ﬁxed and/or random parts of the multivariate model without having to specify
the number of model parameters. Interest is focused on the likelihood of the
structural parameters in the model. Therefore, the multivariate random eﬀects
model in Equation (8.16) is considered. Using the factorization in Equation
(8.14), the standardized deviance is
D(Ω) =
X
ij
 z∗
ij −HP Ωij
tC−1 z∗
ij −HP Ωij

,
(8.19)
where C = (IK ⊕IKω). When comparing models, it can be assumed without
loss of generality that p
 y, t

= 1 for all models. This deviance term is based
on the complete data. Subsequently, the posterior expectation of the DIC over
the augmented data will be taken. Then, the DIC of interest is deﬁned as
DIC =
Z 
DIC | z

p(z|y)dz
=
Z 
D( ¯Ω) + 2pD

p(z | y)dz
= Ez

D( ¯Ω) + 2pD | y

,
(8.20)
where ¯Ωequals the posterior mean and pD is the eﬀective number of pa-
rameters given the augmented data. The latter can be shown to be equal to
the mean deviance minus the deviance of the mean. A similar procedure was
proposed for mixture models by DeIorio and Robert (2002).
A DIC will be derived for the complete-data likelihood with the random
eﬀects integrated out. The speciﬁc interest in the underlying structure of Ω
becomes apparent in this expression. The corresponding penalty term in the
DIC reﬂects the eﬀective number of parameters related to the multivariate
structure on the person parameters. Furthermore, this DIC has the advantage
that it only requires estimates of ﬁxed eﬀects and variance parameters and
not estimates of any random eﬀects parameters. The variances, covariances,
and item parameters are considered nuisance parameters, and their values are
assumed to be known. In Appendix 8.12, it is shown that the expression for
pD equals

242
8 Response Time Item Response Models
pD = EΩ

D(Ω) | z∗
−D(E(Ω| z∗))
=
X
ij
tr
h HP xijwjΣγwt
jxt
ijHt
P + HP xijVxt
ijHt
P
+HP ΣP Ht
P + C
−1 HP xijwjΣγwt
jxt
ijHt
P
+HP xijVxt
ijHt
P + HP ΣP Ht
P
i
,
(8.21)
where tr(·) denotes the trace function (i.e., the sum of the diagonal elements).
The expectation is taken with respect to the posterior distribution of Ωgiven
the ﬁxed eﬀects and covariance parameters.
The ﬁrst term of the DIC, D(E(Ωij | z∗
ij)), requires an expression of the
marginal expectation of Ωij given the complete data; that is,
E
 Ωij | z∗
ij

= E
 E
 Ωij | z∗
ij, βj

| z∗
ij

=
 Σ−1
e
+ Σ−1
u
−1 
Σ−1
e
ˆΩij + Σ−1
u xijwjγ

,
(8.22)
where Σ−1
u
=
 xijVxt
ij + ΣP
−1 and Σ−1
e
= Ht
P (IK ⊕IKω)−1 HP (see Ex-
ercise (8.6)).
The terms in Equations (8.21) and (8.22) can be estimated as a by-product
of the MCMC algorithm. DICs of nested models are computed by restricting
one or more variance parameters in (8.21) to zero. Usually the variance pa-
rameters are unknown. Then the DIC has to be integrated over their marginal
distribution, too. In fact, the correct Bayesian approach would be to integrate
the joint posterior over the nuisance parameters to obtain the marginal pos-
terior of interest. However, this approach is not possible since no closed-form
expression of the DIC can be obtained for this marginal posterior. The DIC
does not account for the unknown variances, and the term in (8.21) reﬂects
the eﬀective number of parameters of the model without the additional vari-
ability in the posterior because of the unknown covariance parameters. The
more general case with unknown covariance parameters is complex, and no
simple correction seems available. Vaida and Blanchard (2005) showed that,
for a mixture model, the correction for unknown covariance parameters is
negligible asymptotically. So, it seems safe to assume that their eﬀect on the
estimate of the eﬀective number of parameters only becomes apparent when
the covariance parameters are estimated less precisely.
8.7 Model Fit via Residual Analysis
The ﬁt of the measurement models to response and RT data can be assessed
through residual analysis. The residual analysis for responses was described
in Section 5.2, where it was based on Bayesian latent residuals. The observed
RTs are continuous-valued, which simpliﬁes a residual analysis.

8.8 Simultaneous Estimation of RTIRT
243
The actual observation tijk is evaluated under the posterior predictive
density. That is, the probability of observing a value smaller than tijk can be
estimated by
P (Tijk < tijk | y, t) ≈
X
m
Φ

tijk | ζ(m)
ij
, ϕ(m)
k
, λ(m)
k
.
M
(8.23)
given m = 1, . . . , M iterations of the MCMC algorithm. If the item response
time model holds, the marginal cumulative posterior probabilities follow a
uniform distribution according to the probability integral transformation the-
orem (see Exercise 3.6). By comparing the estimated moments with the true
moments of the uniform distribution, it is checked whether equally spaced in-
tervals contain equal numbers of estimated probabilities. The implicit smooth-
ing in the empirical cumulative distributions can also be plotted against the
identity line. Cumulative posterior probabilities that show departures from
uniformity indicate that corresponding observations are unlikely under the
model.
In a similar way, the augmented responses are evaluated under the poste-
rior predictive density. The probability of observing a latent response under
the model equals
P (Zijk < zijk | y) ≈
X
m
Φ

z(m)
ijk | a(m)
k
, b(m)
k
, θ(m)
ij
.
M.
(8.24)
The ﬁt of the two-parameter item response model for the augmented data
can be evaluated by checking whether these cumulative posterior probabilities
follow a uniform distribution.
In general, the (posterior) predictive tests of Chapter 5 can also be used
to evaluate the ﬁt of the RTIRT model. Speciﬁc posterior predictive tests
can be developed to test typical assumptions of the model. In Klein Entink
et al. (2009b), several posterior predictive tests were used to test the ﬁt of
the RTIRT model. Exercise 8.2 is focused on evaluating the ﬁt of the item
response time model.
8.8 Simultaneous Estimation of RTIRT
The MCMC scheme for simultaneously estimating all parameters consists
of a data augmentation step as described in Section 8.5. In the augmenta-
tion step, continuous-valued responses are generated to obtain a multivariate
normal model for the augmented responses and log-RTs. This multivariate
structure was explored in Section 8.5. A similar multivariate structure can be
constructed for the item parameters.
Therefore, let Zk and the vector of log-RTs (Tk) to item k be stacked in
a vector Z∗
k. Deﬁne a covariate matrix HI = (θ, −1N) ⊕(−ζ, 1N). Then a
linear regression structure for the item parameters can be presented as

244
8 Response Time Item Response Models
Z∗
k = HI (ak, bk, ϕk, λk)t + ϵk
= HIΛk + ϵk,
(8.25)
where ϵk ∼N (0, IN ⊕INωk). This multivariate model for the complete data
facilitates the sampling of the item parameters from their conditional distri-
butions.
MCMC Scheme 6 (RTIRT)
A1) Sample the item parameters
1. Binary response data. The item parameters are the coeﬃcients of the
regression of Z∗
k on HI in Equation (8.25). Combined with the prior
in Section 8.4.1, the conditional posterior density is a multivariate
normal with parameters
E (Λk | z∗
k, Ω, ΣI) =
 Σ−1
I
+ Ht
IC−1
I HI
−1  Ht
IC−1
I z∗
k + µIΣ−1
I

,
Var (Λk | z∗
k, Ω, ΣI) =
 Σ−1
I
+ Ht
IC−1
I HI
−1 ,
respectively, where CI = IN ⊕INωk.
2. Polytomous response data; see Exercise 8.4.
A2) Sample the person parameters
The multivariate distribution of the person parameters and the augmented
data equals
Ωij
z∗
ij

∼N

xijβj
HP xijβj

,

ΣP
ΣP Ht
P
HP ΣP HP ΣP Ht
P + CP

,
where CP = IK ⊕IKω and HP = (a ⊕−ϕ). The conditional posterior
of the person parameters is a multivariate normal,
Ωij | z∗(m+1)
ij
, Σ(m)
P
, β(m)
j
∼N (µΩ, ΣΩ) ,
with
µΩ= xijβj + HP ΣP (HP ΣP Ht
P + CP )−1  z∗
ij −HP xijβj

,
ΣΩ= ΣP −ΣP Ht
P
 HP ΣP Ht
P + CP
−1 HP ΣP .
A3) Sample the remaining measurement parameters
1. For binary response data, the hyperparameters (µI, ΣI) are sampled
from a normal inverse Wishart density. Given values for (K0, µ0) and
(ν, Σ0), sample µ(m+1)
I
and Σ(m+1)
I
from
µI | Σ(m)
I
, Λ(m+1) ∼N

K0
K0 + K µ0 +
K
K0 + K Λ, ΣI/(K + K0)

ΣI | Λ(m+1) ∼IW (K + ν, Σ∗) ,
where

8.8 Simultaneous Estimation of RTIRT
245
Σ∗= Σ0 + KS +
K0K
K0 + K
 Λ −µ0
  Λ −µ0
t ,
S = K−1 X
k
 Λk −Λ
  Λk −Λ
t ,
Λ = K−1 X
k
Λk.
In the case of polytomous response data, see Exercise 8.4.
2. For each item k, sample the residual variance ω2(m+1)
k
given ϕ(m+1)
k
,
λ(m+1)
k
, ζ(m+1), and t from an inverse gamma distribution with pa-
rameter g1 + N/2 and scale parameter
g2 + 1
2
X
i,j
(tijk −(λk −ϕkζij))2 .
B)
Sampling of structural model parameters
1. For each j, sample β(m+1)
j
given Ω(m+1)
j
, Σ(m)
P
and V(m), γ(m) from
a multivariate normal with mean
µβ = wjγ + xjV
 xjVxt
j + ΣP
−1 (Ωj −xjwjγ)
and variance
Σβ = V −Vxt
j
 xjVxt
j + ΣP
−1 xjV.
2. Assume that γ is a priori multivariate normally distributed with mean
zero and covariance matrix Σγ. Sample ﬁxed coeﬃcients γ(m+1) from
the full conditional
γ | β(m+1), V(m), Σγ ∼N (µγ, Ξγ) ,
where
µγ = Ξγ
X
j
wt
jV−1βj,
Ξγ =
X
j
wt
jV−1wj + Σ−1
γ
−1
.
3. Sample V(m+1) from an inverse Wishart density with n0 + J degrees
of freedom and scale matrix
J
X
j=1

βj −wjγ

βj −wjγ
t
+ V0,
where n0 ≥2.

246
8 Response Time Item Response Models
4. Sample Σ(m+1)
P
given Ω(m+1) and β(m+1) as described in Section
8.4.2.
This scheme is easily extended to handle a three-parameter measurement
model. Then, an additional augmentation step is introduced according to
Equations (4.20) and (4.21). Subsequently, guessing parameters are sampled
according to Equation (4.22).
8.9 Natural World Assessment Test
A computerized version of the Natural World Assessment Test (NAW-8) was
taken by 388 second year students who answered 65 items. The test takers were
required to participate in the educational assessment. This low-stakes test is
used to assess the quantitative and scientiﬁc reasoning proﬁciencies of college
students. Besides the test results, the test taker’s SAT score, gender (GE), a
self-reported measure of citizenship (CS), and a self-reported measure of test
importance (TI) were available. Citizenship was a measure of a test taker’s
willingness to help the university collect its assessment data. Test importance
was a self-reported measure of how important the test was for the test taker.
In this example, the relationship between ability and speed is explored and
the extent to which individual background information explains variability in
ability and speed of working is evaluated. The three-parameter item response
model is used as the measurement model for the item responses.
The RTIRT model was estimated with 20,000 iterations of the Gibbs sam-
pler, and the ﬁrst 10,000 iterations were discarded as the burn-in. Several
posterior predictive checks were carried out to evaluate the ﬁt of the model.
The odds ratio (Equation (5.31)) was used to test for violations of local in-
dependence. This test indicated that less than 4% of the possible item com-
binations showed a signiﬁcant between-item dependency. Furthermore, the
replicated response patterns under the posterior distribution matched the ob-
served data quite well using the discrepancy measure in Equation (5.33). From
the posterior residual check (Equation (8.23)), the item response time model
described the data well.
The model with ﬁxed time-discrimination parameters (ϕ = 1) was com-
pared with the model with unrestricted time-discrimination parameters (ϕ ̸=
1). To compare the models, the DIC was computed with Σγ and V equal to
zero for both models. The estimated DIC’s were 85,780 and 84,831 for the
restricted and the unrestricted RTIRT models, respectively. The estimated
time-discrimination parameters varied from .25 to 1.65. It was concluded that
the items discriminated diﬀerently between test takers of diﬀerent working
speeds. This means that the rate of change in log-RT relative to changes in
working speed varied over items.
Speed tests are characterized by the fact that the time limit is strict.
The goal is to measure how quickly test takers can answer items. On speed

8.9 Natural World Assessment Test
247
tests, some respondents may not ﬁnish all the items. Others may rapidly
mark answers without thorough thought. Therefore, the time limit provokes
diﬀerent types of response behaviors that often violate the RTIRT’s stationary
speed assumption. A test taker running out of time will show a deviation of
this assumption towards the end of the test. Such a deviation can be detected
by estimating the outlying probabilities of the standardized residuals ϵζijk/ωk.
The probability that the standardized residual exceeds some prespeciﬁed value
q equals
P
 ϵζijk/ωk
 > q | tijk, ϕk, λk, ζij

= 2Φ (−q; λk −ϕkζijk) .
(8.26)
For q = 2, less than 5% of the residuals have a high posterior probability of be-
ing greater than two standard deviations. Therefore, there were no indications
of speededness for this test.
Table 8.1 gives the estimated covariance components and correlations be-
tween the measurement parameters at level 1. The correlation between the
person parameters was estimated to be −.81. This strong negative depen-
dence indicates that higher-ability candidates are working at a lower speed.
This might indicate that students who took their time were serious about the
test and that students who worked fast were not doing their best to solve the
items. Although care has to be taken to draw strong conclusions, the response
times might reveal that the measurement instrument is not valid since some
students were not taking the test seriously.
The within-item covariance estimates contain information about the com-
mon covariance item structure. The estimated item characteristic correlations
are given under the heading “Cor”. The diﬃculty of an item is positively cor-
related with the time intensity of an item. This is in line with the common
assumption that the more diﬃcult tasks require more time to be solved. The
more diﬃcult items and the more time-intensive items, which are positively
correlated, have higher time-discrimination parameters. Thus, the more time-
discriminating items can be recognized as the more diﬃcult (time-intensive)
items. The more diﬃcult items discriminate less between test takers with
diﬀerent abilities. It follows that the more diﬃcult test items can better dis-
criminate test takers with diﬀerent speed levels than test takers with diﬀerent
ability levels.
The individual background information was used to explain simultaneously
variability in the ability and speed levels. The full RTIRT model contains all
covariate information for explaining variability in both latent variables. In Ta-
ble 8.2, the parameter estimates of the full RTIRT model are given. It follows
that male students perform equally well on the test with respect to accuracy
and speed. Students that were more willing to help the university in collect-
ing assessment data (citizenship) were not working faster or more accurately.
The SAT scores explain a signiﬁcant amount of variation in the ability levels.
However, the SAT scores did not explain a signiﬁcant amount of variability
in the speed levels. This means that the student’s speed of working is not re-
lated to the SAT score. Finally, a positive relationship of test importance with

248
8 Response Time Item Response Models
Table 8.1. NWA-8: Covariance components and correlation estimates.
Variance Components
Mean SD Cor.
Person Covariance Matrix ΣP
(Ability)
σ2
θ
1.00
- 1.00
ρ
−.37 .02 −.81
(Speed)
σ2
ζ
.21 .02 1.00
Item Covariance Matrix ΣI
(Discrimination)
Σ11
.03 .01 1.00
Σ12 −.03 .02 −.29
Σ13
.03 .01
.58
Σ14
.01 .02
.14
(Diﬃculty)
Σ22
.39 .08 1.00
Σ23
.06 .03
.35
Σ24
.07 .05
.20
(Time discrimination) Σ33
.08 .02 1.00
Σ34
.08 .03
.47
(Time intensity)
Σ44
.34 .06 1.00
ability was expected. Students who found the test important were expected
to try harder and spend more time per item to receive a higher grade. The
signiﬁcant negative relationship of test importance with speed supports this
hypothesis; that is, students who found the test important worked with less
speed than students who did not. Test importance is also positively correlated
with ability.
8.10 Discussion
A multivariate hierarchical item response measurement framework was devel-
oped for measuring ability and speed of working given item responses and
RTs, respectively. The RTs and responses are assumed to be conditionally
independent given the levels of speed and ability. The latent variables ability
and speed may correlate, which means that speed of working can inﬂuence the
accuracy of the results. This trade-oﬀbetween speed and accuracy often de-
scribes a negative correlation between the speed and accuracy levels at which
a person can operate. At a higher level of the RTIRT model, a multivariate
multilevel structural model is deﬁned for the person variables. At this stage,
the latent correlation structure is deﬁned and explanatory information can
be incorporated into the model to explain between-individual (within-group)
and between-group diﬀerences in speed and ability levels. Klein Entink et al.
(2009b) showed how to model dependencies between item parameters by mod-
eling a population model for the item parameters. In this population model,

8.10 Discussion
249
Table 8.2. NWA-8: Explaining variance in ability and speed levels.
Full RTIRT
Restricted RTIRT
Mean SD
HPD
Mean SD
HPD
Fixed Eﬀects
Ability
γ00 Intercept
.00
-
.00
-
γ01 SAT score
.58 .06
[.46, .70]
.60 .07 [0.47, 0.73]
γ02 Test importance
.65 .06
[.52, .78]
.61 .07 [0.49, 0.74]
γ03 Male
.14 .12
[−.09, .37]
γ04 Citizenship
−.08 .06
[−.20, .04]
Speed
γ10 Intercept
.00
-
.00
-
γ11 SAT score
−.02 .02
[−.07, .02]
γ12 Test importance
−.22 .02 [−.27, −.17]
−.22 .02 [−.27, −.18]
γ13 Male
−.02 .05
[−.11, .08]
γ14 Citizenship
−.01 .02
[−.06, .04]
Residual Variance
σ2
θ Ability
1.00
-
1.00
-
σ2
ζ Speed
.21 .02
[.18, .24]
.21 .02
[.18, .24]
ρ
Covariance
−.37 .02 [−.42, −.33]
−.37 .02 [−.42, −.33]
Information Criteria
−2log-likelihood
51020.32
51001.09
DIC (pD)
52570.3(775.0)
52551.1(775.0)
content-speciﬁc information about items can be related to the item diﬃculty
and intensity.
The modeling framework discussed for analyzing log-RTs and responses
belongs to the class of multivariate nonlinear mixed eﬀects models. This class
of models has not received much attention in the literature. For the class of
multivariate linear mixed eﬀects models, Reinsel (1982) derived closed-form
estimates given a balanced design and completely observed data. Schafer and
Yucel (2002) discussed the multivariate linear case where multiple responses
are ignorably missing. Pinheiro and Bates (2000, Chapter 7) discussed multi-
level nonlinear mixed eﬀects models, with multiple nested factors, for univari-
ate data including computational methods for maximum likelihood estimation
that are implemented in the nlme library of S+ and R (R Development Core
Team, 2010). A multivariate extension of the multilevel nonlinear mixed ef-
fects models was proposed by Hall and Clutter (2004). They iteratively ﬁtted a
sequence of linear mixed eﬀects models derived from ﬁrst-order Taylor expan-
sions to the nonlinear model. In this approach, the true likelihood is considered
to be the ﬁrst-order approximate likelihood, and approximate maximum likeli-

250
8 Response Time Item Response Models
hood estimates are obtained. Standard errors, likelihood ratio tests, and model
selection criteria are based on the approximate likelihood. The MCMC algo-
rithm developed can simultaneously estimate the model parameters without
having to approximate the joint posterior distribution. Extensions to handle
ignorable missing item responses and unbalanced designs can be made in a
straightforward way.
For a long time, experimental and cognitive psychologists have used RTs
to test theories. The pioneering work of the Dutch psychologist F. C. Don-
ders (Donders, 1868) on measurement of mental processing times dates from
the middle of the 19th century. Recently, RTs have been used in test theory
(and survey research), especially since they are easily collected in computer
adaptive (or assisted) testing. The introduction of RTs into test theory led to
various applications. Attention has been focused on simultaneously measuring
speed and ability (van der Linden, 2007) and using background information
as collateral information (Klein Entink et al., 2009a). RTs can also be used to
diagnose response problems (e.g., van der Linden and Krimpen-Stoop, 2003).
Items that induce guessing behavior can be detected from the RTs when,
for example, time-intensive (diﬃcult) items take less time and are followed
by incorrect answers than less time-intensive (easy) items followed by correct
answers.
In test construction, item selection is based on the information functions
of the items. For example, in a computer adaptive test (CAT), the selection of
a new item is based on the estimated ability of the person given the observed
response data and the information functions of the items that were not pre-
sented. The object is to minimize the set of items that are needed to measure
a test taker’s ability up to a speciﬁed accuracy. The RTs provide an additional
source of data that can be used to improve the accuracy of the ability estimate
(Klein Entink, 2009). Subsequently, the use of RTs as an additional source of
information can improve the (minimal) subset of items that is obtained via a
selection criterion based on response data. Furthermore, a diﬀerent selection
criterion may include an item’s time characteristics such as intensity and time
discrimination. The selected subset of items based on this criterion can reduce
the length of the test in time in comparison with a random selection of items
(van der Linden, 2008).
8.11 Exercises
8.1. Consider 12 RTs and item responses of 356 students to rule-based items
for measuring speed of working and ﬁgural reasoning ability, respectively
(Klein Entink et al., 2009b). The log-RTs are modeled by the response time
model in Equation (8.1) with time discriminations equal to one.
(a) Deﬁne priors for the parameters of the response time model, and complete
the implementation given in Listing 8.1.

8.11 Exercises
251
Listing 8.1. WinBUGS code: Response time measurement model.
model{
for
( i
in
1 :N)
{
for
( k
in
1 :K)
{
T[ i , k ]
˜ dnorm( lambda [ k ] −zeta [ i ] , precomega [ k ] )
T[ i , k ] <−log (RT[ i , k ] )
}
zeta [ i ]
˜ dnorm(0 , preczeta )
}
for
( k
in
1 :K)
{
lambda [ k ]
˜ dnorm(mu, preclambda )
}
(b) Estimate the model using WinBUGS. Check that the posterior mean esti-
mates of the time-intensity parameters correspond closely with the observed
item means.
(c) Estimate the intraclass correlation coeﬃcient
σ2
ζ
σ2
ζ+ω2
k per item, and explain
the results.
(d) Verify that the marginal probability of observing a log-RT less than tik
on item k equals
P (Tik < tik) = E (P (λk −ζi + ϵζik < tik) | ζi)
= Φ

tik −λk
q
σ2
ζ + ω2
k

.
(e) For each item, compute the marginal probability of observing a log-RT
less than the average time intensity.
8.2. To evaluate the ﬁt of the response time model, item- and person-ﬁt statis-
tics based on Bayesian residuals can be deﬁned.
(a) Find the posterior density of a Bayesian residual deﬁned as
ϵζik = Tik −(λk −ζi) .
(b) Explain that a person-ﬁt statistic is deﬁned by the function Qp,i,
Qp,i (Ti) =
X
k
ϵζik
ωk
2
,
with corresponding tail-area probability
p0 (Qp,i) = P
 χ2
K > Qp,i (Ti)

.
(c) In the same way, deﬁne an item-ﬁt statistic and the tail-area probability
based on the Bayesian residuals deﬁned in (a).
(d) Use the result from Exercise 8.1(d) to deﬁne the tail-area probability

252
8 Response Time Item Response Models
P

Z > Tik −λk
q
σ2
ζ + ω2
k

,
where Z is standard normally distributed, and explain that it can be used to
investigate the compatibility of the model with the observed log-RTs.
(e) (continuation of Exercise 8.1) Evaluate the ﬁt of the response time model
by evaluating the tail-area probabilities in WinBUGS.
8.3. (continuation of Exercise 8.1) Students are grouped by the item response
(correct/incorrect). Attention is focused on diﬀerential item response time
functioning for the groups of respondents.
(a) Complete Listing 8.2 with priors for the parameters. Investigate whether
the time-intensity parameters diﬀer across the groups.
Listing 8.2. WinBUGS code: Diﬀerential response time measurement model.
model{
for
( i
in
1 :N)
{
for
( k
in
1 :K)
{
T[ i , k ]
˜ dnorm( lambda [ class [ i , k ] , k ] −zeta [ i ] , precomega [ k ] )
T[ i , k ] <−log (RT[ i , k ] )
class [ i , k ] <−Y[ i , k ] + 1
}
zeta [ i ]
˜ dnorm(0 , preczeta )
}
for
( k
in
1 :K)
{
lambda [ 1 , k ]
˜ dnorm(mu1, preclambda1 )
lambda [ 2 , k ]
˜ dnorm(mu2, preclambda2 )
}
}
(b) Deﬁne a univariate normal mixture density for the speed of working at
item k,
p (ζik) = I(Yik = 0)φ
 ζik; 0, σ2
ζ

+ I(Yik = 1)φ
 ζik; µk, σ2
ζ

.
Explain that the mixture density allows individual variation in speed of work-
ing across items.
(c) Explain that the response time model in Exercise 8.1 with a prior mixture
density for the speed parameter is identiﬁed.
(d) Implement the mixture density in Listing 8.1, and investigate whether the
speed of working diﬀers across items and across correct and incorrect answers.
8.4. Assume a normal ogive graded response model (Equation (4.25)) for the
observed responses. Let κk denote the set of threshold parameters for item k
with the order restriction
−∞< κk1 ≤κk2 ≤. . . ≤κk,Ck−1 < ∞,
(8.27)
where there are Ck categories.

8.11 Exercises
253
(a) State the advantages and disadvantages of a multivariate normal prior
distribution for the item parameters (ak, κk, ϕk, λk) with mean µI and co-
variance matrix ΣI, where κk obey the order restriction in (8.27).
(b) Show that a conditional normal prior p (κk | ak, ϕk, λk, µI, ΣI) can be
derived from the multivariate normal prior.
(c) An M-H step for drawing values from the conditional posterior distribution
p (κk | yk, θ, ak, ϕk, λk, µI, ΣI) can be deﬁned. Use the proposal distribution
in Equation (4.31) for drawing candidate values. Deﬁne the acceptance ratio
(Equation (3.1)) for evaluating the proposed values.
(d) Show that the normalizing constant of the conditional distribution of
p (ak, κk, ϕk, λk | µI, ΣI) contains a term P (κk ∈Ak | µI, ΣI), where Ak =

κk ∈RCk−1, −∞< κk1 ≤κk2 ≤. . . ≤κk,Ck−1 < ∞
	
. Why does this com-
plicate direct sampling from the conditional posterior distribution of µI?
(e) The hyperparameter µI given ΣI has a normal distribution with mean
parameter µ0 and scale parameter ΣI/K0. Deﬁne an M-H step for sampling
values µI from the full conditional distribution.
8.5. Consider the Amsterdam Chess Test data from players responding to
computerized chess problems to measure their chess-playing proﬁciency (van
der Maas and Wagenmakers, 2005). For each of the 40 tasks, the response
accuracy and the RT were measured.
(a) Fit the RTIRT model to the data using the R-package CIRT (Fox et al.,
2007).
(b) Plot the estimated ability against the speed of playing, and explain the re-
lationship. How does the trend relate to the estimated covariance components
of matrix ΣP ?
(c) Test whether the mean speed level for correct responses diﬀers from the
mean speed level for the incorrect responses to item 7. Are players responding
correctly to item 7 solving the items faster than those who respond to it
incorrectly?
8.6. The likelihood model is given by Equation (8.16), and the prior structure
is deﬁned in Equations (8.6) and (8.7).
(a) Show that a (marginal) prior model for Ωij is given by
Ωij | γ, ΣP , V ∼N
 xijwjγ, xijVxt
ij + ΣP

.
(b) Derive the conditional distribution of Ωij given the complete data, item
parameters, ﬁxed eﬀects, and covariance parameters. Show that the expecta-
tion equals the term in Equation (8.22).

254
8 Response Time Item Response Models
8.12 Appendix: DIC RTIRT Model
An expression for the number of eﬀective parameters is constructed in which
the item parameters are considered to be nuisance parameters. Instead of
computing the posterior mean deviance minus the deviance of the posterior
mean of Ωij, an expected penalty term is derived where the expectation is
taken with respect to the posterior density of Ωij given the complete data
and covariance parameters. That is,
pD = D(Ω) −D( ¯Ω)
(8.28)
= EΩ

D(Ω) | z∗
−D(E(Ω| z∗))
= EΩ
"X
ij
 z∗
ij −HP Ωij
tC−1 z∗
ij −HP Ωij

#
−D(E(Ω| z∗))
= tr
"X
ij
EΩ
 z∗
ij −HP Ωij
 z∗
ij −HP Ωij
tC−1
#
−
tr
"X
ij
 z∗
ij −HP E(Ω| z∗)
 z∗
ij −HP E(Ω| z∗)
tC−1
#
=
X
ij
tr
h
EΩ
 z∗
ij −HP Ωij
 z∗
ij −HP Ωij
tC−1−
 z∗
ij −HP E(Ω| z∗)
 z∗
ij −HP E(Ω| z∗)
tC−1i
=
X
ij
tr

C−1 Var(ϵij | z∗
ij)

(8.29)
=
X
ij
tr

C−1  Var(ϵij) −Cov(ϵij, z∗
ij) Var(z∗
ij)−1 Cov(ϵij, z∗
ij)

=
X
ij
tr

Var(z∗
ij)−1 Var
 HP Ωij

(8.30)
=
X
ij
tr
h HP xijwjΣγwt
jxt
ijHt
P + HP xijVxt
ijHt
P + HP ΣP Ht
P + C
−1
 HP xijwjΣγwt
jxt
ijHt
P + HP xijVxt
ijHt
P + HP ΣP Ht
P
i
.
The terms in (8.29) can be recognized as the posterior variances of the residu-
als, whereas those in (8.30) follow from the fact that, because of independence,
the variance of z∗
ij equals the sum of the variance of HP Ωij and ϵij.

9
Randomized Item Response Models
Item responses can be masked before they are observed via a randomized
response mechanism. This technique is used to protect individuals and im-
prove their willingness to answer truthfully. Various traditional randomized
response sampling techniques are discussed and extended to a multivariate
setting. So-called randomized item response models will be introduced for
analyzing multivariate randomized response data. This class of models can
also be extended to handle explanatory information at diﬀerent hierarchical
levels. The models discussed are particularly suitable for analyzing sensitive
individual characteristics and their relationships to background variables.
9.1 Surveys about Sensitive Topics
The collection of data through surveys is subject to two main sources of error.
First, there is sampling error due to the fact that a sample is studied instead
of the entire corresponding population. Sampling error can be reduced by in-
creasing the sample size, by improving the eﬃciency of the sampling design,
or by improving the estimation procedure to improve the accuracy of the pop-
ulation estimates. A second type of error can be characterized as nonsampling
error, which covers the systematic errors. Bias in survey data can be caused
by response bias, which is the diﬀerence between the true response accord-
ing to the respondent’s true beliefs and the expected observed response over
repeated measurements. There are two well-known sources of response bias,
nonresponse (the refusal to respond) and typical response behavior, which
leads to underreporting or overreporting such that the answers are systemat-
ically lower or higher, respectively.
Surveys on highly personal and sensitive issues may lead to response be-
havior that manifests as answering refusals and false responses, making in-
ferences diﬃcult. Respondents may attempt to distort answers to avoid em-
barrassment and mask their socially undesirable answers. Apart from social
desirability, certain questions are inherently oﬀensive because they invade the
©  Springer Science+Business Media, LLC 2010
and Behavioral Sciences, DOI 10.1007/978-1-4419-0742-4_9,
255
J -P  Fox, Bayesian Item Response Modeling: Theory and Applications, Statistics for Social
.
.

256
9 Randomized Item Response Models
respondent’s privacy. In that case it is not the answer that is sensitive but
the question. The sensitivity of the questions also depends on respondents’
concerns about disclosing information to the interviewer and third parties. In
general, asking sensitive questions leads to a reduction in overall and item
response rates and response accuracy (percentage of truthful answers).
In this light, it is to be expected that the item nonresponse increases with
the question sensitivity, but this relationship is not so evident (Tourangeau
and Yan, 2007). Nevertheless, there is considerable evidence that the data
collection method inﬂuences the answers and that the procedure of self-
administration increases the respondents’ willingness to respond truthfully
to sensitive questions (e.g., Richman, Kiesler, Weisband and Drasgow, 1999;
Tourangeau, Rips and Rasinski, 2000; Tourangeau and Yan, 2007). The main
variety of self-administration methods are based on promises of conﬁdentiality
and can be characterized as a direct way of obtaining sensitive information.
Obtaining valid and reliable information depends on the cooperation of the
respondents, and the willingness of the respondents depends on the conﬁden-
tiality of their responses.
9.2 The Randomized Response Technique
An indirect way of asking sensitive questions such that the respondents’ an-
swers do not reveal anything deﬁnite was developed by Warner (1965). Warner
(1965) developed a data collection procedure, the randomized response (RR)
technique, that allows researchers to obtain sensitive information while guar-
anteeing privacy to respondents. In this procedure, a respondent randomly
selects one of two cards with statements of the form: I have the sensitive char-
acteristic and (2) I do not have the sensitive characteristic. A randomization
device is used to choose one of the cards (e.g., tossing a die or using a spinner).
The randomization is performed by the interviewee, and the interviewer is not
permitted to observe the outcome of the randomization.
The respondent is protected since the interviewer will not know which
question is being answered. The interviewee responds to a question selected
by the randomization device, and the interviewer only knows the response.
The respondent’s privacy or anonymity is well protected because only the
respondent knows which question was answered. In several empirical studies
it has been shown that respondents are more willing to provide honest answers
with this technique because their answers do not reveal any information about
themselves. Fox and Tracy (1986), Lensvelt-Mulders, Hox, van der Heijden and
Maas (2005), and Tracy and Fox (1981), among others, showed via several
studies that the randomized response method leads to less biased estimates
of the sensitive characteristic in comparison with estimates obtained from
traditional survey methods.

9.2 The Randomized Response Technique
257
9.2.1 Related and Unrelated Randomized Response Designs
The model of Warner (1965) for dichotomous responses is meant for estimating
a population proportion, π, that represents some sensitive characteristic. The
respondent answers “True” or “False” without revealing which question was
selected by the randomizing device. Let p1 denote the probability that question
(1) will be selected by the randomizing device. Then, the probability that the
respondent indexed i gives a positive response equals
P(Yi = 1) = p1π + (1 −π)(1 −p1).
(9.1)
In this model, both questions or statements deal with the sensitive topic.
Greenberg, Abul-Ela, Simmons and Horvitz (1969) proposed a diﬀerent
design, in which the sensitive question is paired with an innocuous question.
In this unrelated question design, the second question is not related and com-
pletely innocuous, and the probability of a positive response is known. In
this perspective, Warner’s model is also referred to as the related randomized
response design since a sensitive question is paired with its converse.
The unrelated question can also be built into the randomizing device. In
that case, two probabilities are speciﬁed by the randomizing device: proba-
bility p1 that the respondent has to answer the sensitive question and the
conditional probability p2 of a positive response in case a forced response has
to be given (Edgell, Himmelfarb and Duchan, 1982). The probability of a
positive response for respondent i can be stated as
P(Yi = 1) = p1π + (1 −p1)p2.
(9.2)
The extension to multiple, say c = 1, . . . , C, response categories is easily
made. Let π(c) denote the proportion of respondents scoring in category c, and
the randomization device determines if the item is to be answered honestly
with probability p1 or a forced response is scored with probability 1 −p1. If a
forced response is given, it is scored in category c with conditional probability
p2(c). Then, the probability of observing a score in category c equals
P(Yi = c) = p1π(c) + (1 −p1)p2(c).
(9.3)
Note that it is assumed that the response probability of scoring in category c
for the nonsensitive unrelated question is known a priori. This is more eﬃcient
since it reduces the sampling variability. Furthermore, it is quite easy to deﬁne
unrelated neutral questions whose response probabilities are known in advance
(e.g., Greenberg et al., 1969).
In a setting where the response probabilities of the unrelated question are
unknown, two independent distinct samples, say A1 and A2, are needed from
the population where two independent randomization devices are employed.
Let the randomization devices be such that p1
1 is the probability that the
respondent has to answer the sensitive question in A1 and p2
1 the probability

258
9 Randomized Item Response Models
in A2. Subsequently, given group membership, the probability of scoring in
category c equals
P(Yi = c) =
 π(c)p1
1 +
 1 −p1
1

p2(c) if i ∈A1
π(c)p2
1 +
 1 −p2
1

p2(c) if i ∈A2.
(9.4)
When selecting p1
1 close to p2
1, the point estimates of π(c) and p2(c) may be
unstable and greater than unity (Greenberg et al., 1969).
9.3 Extending Randomized Response Models
The traditional randomized response techniques are meant for univariate data,
and the main goal is to obtain an estimate of the proportion of respondents
having the sensitive characteristic. Theoretical details about the estimation
of π can be found for example, in Warner (1965) and Greenberg et al. (1969).
In some cases, additional information at the individual level is available that
can be related to the probability of a positive response. Maddala (1983) and
Scheers and Dayton (1988) incorporated explanatory variables in the random-
ized response model. The additional information can be used to reduce the
standard errors and establish a relationship between the covariate information
and the population proportion with the sensitive characteristic.
RR data can be hierarchically structured, and there is interest in group
diﬀerences regarding some sensitive characteristic. One could think of an ap-
plication where it is of interest to know if cheating behavior diﬀers across
faculties or if social security fraud is more likely to appear in groups with
certain characteristics. The classical (e.g., related and unrelated) randomized
response models do not allow a hierarchical data analysis. Statistical methods
for hierarchically structured data, for example analysis of variance (ANOVA)
or multilevel analysis, cannot be applied since the individual responses are
randomized.
A major drawback of classical randomized response models is that only
aggregate-level inferences can be obtained; that is, estimates of popula-
tion proportions and related conﬁdence intervals. It is not possible to make
individual-level inferences, and this prevents insights into the possible deter-
minants and consequences of the sensitive construct under study. This corre-
sponds to the fact that related and unrelated randomized response methods
were originally developed for analyzing univariate data.
In this chapter, the randomized response technique will be extended to
a multivariate setting with the purpose of measuring an underlying sensitive
construct. The motivation is that the quality of the measurements is improved
by the randomized response technique such that a more reliable estimate of
the sensitive individual characteristic can be obtained. Covariates concerning
individual or group characteristics can be taken into account in the estima-
tion of the individual sensitive attitudes. It is also possible, the other way

9.4 A Mixed Eﬀects Randomized Item Response Model
259
around, to explore characteristics that can be held responsible for individ-
ual diﬀerences in (sensitive) attitudes. The diﬀerential inﬂuence of individual
and group characteristics on sensitive individual characteristics can be more
accurately investigated.
9.4 A Mixed Eﬀects Randomized Item Response Model
Assume that there are J groups and nj individuals nested within each group.
Let Yijk denote the randomized response for an individual, indexed ij, to an
item indexed k. Subsequently, eYijk denotes the latent (true) item response
in the randomized response design. Note that this latent response is never
observed directly since each individual response is randomized and only the
randomized response is observed.
9.4.1 Individual Response Probabilities
The general randomized response model
P (Yijk = c | πijk) = ˜p1πijk(c) + ˜p2
= f (πijk(c); ˜p1, ˜p2)
(9.5)
includes the related, the unrelated, and the forced randomized response mod-
els, where for the related (Warner’s) model ˜p1 = (2p1 −1) and ˜p2 = (1 −p1),
for the unrelated question model ˜p1 = p1 and ˜p2 = (1−p2)πu
ijk(c), and for the
forced response model ˜p1 = p1 and ˜p2 = (1 −p1)p2(c) according to Equations
(9.1) and (9.3). The πijk(c) is the response probability of person i in group
j scoring a latent response eYijk = c and πu
ijk(c) the probability of the la-
tent response in category c to the unrelated nonsensitive question. Although
the unrelated question method ﬁts within this framework, this randomized
response design will not be pursued further. Note that ˜p1 and ˜p2 are known
speciﬁc characteristics of the randomizing device that is used in the random-
ized response survey.
Univariate Response Data
At the second stage, the latent (true) responses are modeled. Note that,
strictly speaking, from the hierarchical point of view, the latent response data
are also modeled at stage one. Suppose that eYijk takes on value zero or one
with probability 1−πijk and πijk, respectively. Then, let Zijk be a continuous
latent variable such that eYijk = 1 if Zijk is positive and eYijk = 0 if Zijk is
negative. A probit model is deﬁned for the latent response as
πijk = P

eYijk = 1

= P(Zijk > 0),
(9.6)

260
9 Randomized Item Response Models
where Zijk is standard normally distributed. A logistic response function can
be assumed, and then Zijk is standard logistically distributed.
For polytomous ordinal data, other polytomous responses can be handled
in a similar way, where the latent response eYijk denotes a categorical outcome
and Zijk the underlying latent score such that the probability of individual ij
scoring in category c equals
πijk(c) = P
 eYijk = c | κ

= Φ (zijk −κk,c−1) −Φ (zijk −κk,c) ,
(9.7)
or replace Φ(.) with
Ψ (zijk −κk,c) =
1
1 + exp[−(zijk −κk,c)],
where κ are the threshold parameters such that κk,r > κk,s whenever r > s,
with κk,0 = −∞and κk,C = ∞.
Multivariate Response Data
It will be assumed that the items are composed to measure some underlying
sensitive construct. For multivariate observed RR data, the second modeling
stage consists of an item response model for deﬁning the relationship between
the observed randomized item responses and the underlying latent sensitive
characteristic. The latent categorical outcome, eYijk, represents the item re-
sponse of person ij on item k with latent ability or attitude parameter θij.
For dichotomous item responses, a two-parameter item response model is
used for specifying the relation between the level of a latent variable and the
probability of a particular item response. That is,
πijk = P

eYijk = 1 | θij, ak, bk

= Φ (akθij −bk) .
(9.8)
For polytomous item responses, the probability that an individual obtains
a grade c on item k is deﬁned by a graded response model
πijk = P

eYijk = c | θij, ak, κk

= Φ (akθij −κk,c−1) −Φ (akθij −κk,c)
(9.9)
where the boundaries between the response categories are represented by an
ordered vector of thresholds κ. For the logistic item response model, replace
Φ(.) with Ψ(.). Note that these item response models correspond to the (like-
lihood) models in Chapter 4. The diﬀerence is that the item response models
in this chapter are meant for relating, at the second stage, the underlying
sensitive construct to the latent item responses.

9.4 A Mixed Eﬀects Randomized Item Response Model
261
9.4.2 A Structural Mixed Eﬀects Model
At the third stage, the latent continuous response in Equation (9.6) can be
modeled as a function of incidence matrices xij and wij,
Zijk = wt
ijγ + xt
ijβj + eijk,
(9.10)
where xij is the design vector for the random eﬀects. The Q-dimensional
vector βj contains the random eﬀects, and their distribution is assumed to be
multivariate normal with mean zero and covariance matrix T. Furthermore, γ
is an S-dimensional vector of ﬁxed eﬀects, and the residuals eijk have mutually
independent normal distributions with mean zero and variance one. There is
independence between random eﬀects of diﬀerent groups, and the random
eﬀects are independent of the residuals eijk.
For the multivariate case, at the third stage, the latent sensitive construct
θij can be modeled using characteristics at the individual or group level. At
this third stage, eﬀects of group-level variables on the individual’s binary or
ordinal true response may vary across groups. The mixed eﬀects model for
the vector θj of length nj can be written as
θj = wjγ + xjβj + ej,
(9.11)
where γ is the vector of ﬁxed eﬀects and βj the vector of random regres-
sion eﬀects. The prior distributional assumptions of the random eﬀects and
residuals are given by
βj ∼N (0, T) ,
ej ∼N
 0, σ2
θInj

.
This mixed eﬀects model can be presented as a multilevel model. A parti-
tioning is made in a level-1 model,
θij = wt(1)
ij γ(1) + xt(1)
ij βj + eij,
(9.12)
and a level-2 model,
βj = wt(2)
j
γ(2) + uj,
where w(1)
ij
and w(2)
j
are the ﬁxed level-1 and ﬁxed level-2 covariates, respec-
tively. An overview of structural multilevel models is given in Chapter 6.
The combination of a randomized response model for the observed RR data
(stage 1), an individual response model for latent item response data (stage
2), and a structural mixed eﬀects model for the latent (sensitive) construct
(stage 3) constitutes a mixed eﬀects randomized item response theory (RIRT)
model.
Fox (2005c) proposed an RIRT model for analyzing multivariate binary RR
data. In a Bayesian framework, the model was applied in a study on cheat-
ing behavior of students nested in studies at a Dutch university. B¨ockenholt

262
9 Randomized Item Response Models
and van der Heijden (2007) introduced an RIRT model for binary RR data
in a frequentist framework with a speciﬁc interest in measuring noncompli-
ance. Fox and Wyrick (2008) developed an RIRT model that simultaneously
handles binary and polytomous RR data motivated by a study for measuring
alcohol dependence among students. De Jong, Pieters and Fox (2010) used
an RIRT model in an empirical application to consumers’ desires for adult
entertainment.
In Figure 9.1, a path diagram for the RIRT model is given. This path
diagram is closely related to the MLIRT model discussed in Chapter 6. It can
be seen that in comparison with Figure 6.1 an ellipse is added. This ellipse
depicts the relationship between the observed randomized item response and
the latent item response given parameters ˜p. This relationship is speciﬁed
by the randomized response sampling design, where parameters ˜p deﬁne the
characteristics of the randomizing device. The structural part of the model
is used to explain between-school and within-school variability in the latent
variable. The item response measurement part, also depicted as an ellipse,
relates the latent variable to the latent discrete responses. Besides the three
levels of uncertainty that are modeled in the item response measurement part
and the structural mixed eﬀects part, an additional level of uncertainty is
added due to the randomized response sampling design.
The mixed eﬀects randomized item response model can be identiﬁed in
the same way as in the MLIRT model (see Chapter 6). The scale of the la-
tent variable can be identiﬁed via restrictions on the item parameters or by
restricting the mean and variance of the latent variable. Both ways of identi-
fying the model lead to the same results but on a diﬀerent scale, depending
on the types of restrictions.
9.5 Inferences from Randomized Item Response Data
A general probabilistic relationship is deﬁned between the randomized item
response data and the latent item response data. Another variable deﬁned
is Hijk = 1 when for respondent ij the randomizing device determines that
item k is to be answered truthfully and Hijk = 0 otherwise. The conditional
probability that individual ij scores a latent response in category c′ given an
observed randomized response in category c can be stated as
P

eYijk = c′ | Yijk = c

=
P

eYijk = c′, Yijk = c

P (Yijk = c)
=
P
l∈(0,1) P

eYijk = c′, Yijk = c | Hijk = l

P (Hijk = l)
P
l∈(0,1) P (Yijk = c | Hijk = l) P (Hijk = l)
,(9.13)
where c, c′ = {0, 1} for binary response data or c, c′ = {1, 2, . . . , Ck} for poly-
tomous response data.

9.5 Inferences from Randomized Item Response Data
263
School j
Individual i
ij
xij
j
wj
p
k
Item k
yijk
yijk
Fig. 9.1. Path diagram for the mixed eﬀects randomized item response model.
The conditional probability of a latent response given an observed ran-
domized response can be derived from Equation (9.13) for diﬀerent random-
ized response sampling designs and for binary as well as polytomous item
response data. To illustrate, consider the forced randomized response design
for polytomous data. The denominator in Equation (9.13) equals the marginal
probability of observing a response in category c. This observed response can
be a latent response or a forced response. That is, the respondent, indexed
ij, is instructed by the randomizing device to give a true response or a forced
response. The ﬁrst and second events take place with probabilities p1πijk(c)
and (1−p1)p2(c), respectively. Let the numerator in Equation (9.13) be equal
to the simultaneous probability of the event that a response is observed in
category c and that individual ij gives a latent response in category c. With
probability πijk(c), a latent response is given that is observed with probability
p1 or a forced response is observed with probability (1−p1)p2(c). Both events
take place with probability πijk(p1 + (1 −p1)p2(c)). As a result, the fraction
gives the conditional probability of a latent response in category c for item
k of individual ij given an observed randomized response in category c. The
other cases can be derived in the same way.
As a result, the conditional probability of a latent polytomous response
given an observed polytomous response using the forced randomized response
sampling design can be deﬁned:
P

eYijk = c′ | Yijk = c

=
( πijk(c′)(p1+(1−p1)p2(c))
πijk(c)p1+(1−p1)p2(c)
if c = c′
πijk(c′)(1−p1)p2(c)
πijk(c)p1+(1−p1)p2(c)
if c ̸= c′.
(9.14)

264
9 Randomized Item Response Models
Assume the structural model deﬁned in Equation (9.10). Then the likeli-
hood part of the RIRT model can be stated as
p
 y | ξ, σ2
θ, γ, T

=
J
Y
j=1


Z 

nj
Y
i=1|j
Z
X
˜yij∈˜
Yij
p
 yij | ˜yij

p
 ˜yij | θij, ξk

p
 θij | xij, wj, βj, γ, σ2
θ

dθij
#
p (βj | T)
#
dβj,
(9.15)
where ˜Yij is the set of all possible latent response vectors. The various levels
of the RIRT model (Figure 9.1) can also be recognized from the likelihood in
Equation (9.15).
The distribution of the randomized item response data given latent item
response data is deﬁned by the randomized response sampling design, p (y | ˜y).
The distribution of the latent item responses, p(˜yij | θij, ξk), represents
the item response model at the lowest level of the RIRT model. The sec-
ond level is represented by the conditional distribution of the latent variable,
p
 θij | xij, wj, βj, γ, σ2
θ

, and at the third level, the distribution of the ran-
dom group eﬀects is given, p (βj | T). Note that the randomized response
model is positioned at the ﬁrst level of the RIRT model since the latent item
observations are masked before they are observed.
Before an MCMC implementation is discussed, a short overview of the
prior distributions is given. The joint prior density of
 ξ, γ, σ2
θ, T

can be
factorized as
p
 ξ, γ, σ2
θ, T

= p(ξ)p(γ)p
 σ2
θ

p(T).
For the item parameters, noninformative proper priors for the discrimination
and diﬃculty parameters are used, although other (conjugated) priors can
be used as well; see Chapter 4. The prior for item parameters in the graded
response model is speciﬁed as
p(ξ) ∝
Y
k
φ
 ak; µa, σ2
a

I(ak > 0)I(κk,1, . . . , κk,Ck ∈A),
(9.16)
subject to the condition κk,0 < κk,1 < . . . < κk,Ck with κk,0 = −∞and
κk,Ck = ∞. Furthermore, A is a suﬃciently large bounded interval.
According to the speciﬁcation of the linear mixed eﬀects model, the latent
variable is assumed to have a normal distribution with mean wγ + xβ and
variance σ2
θ. The ﬁxed eﬀects, γ, are assumed to have independent normal
priors, with mean zero and variance σγ, and the hyperparameter σγ equals a
large number that reﬂects a noninformative prior.
The prior for the covariance matrix T is taken to be an inverse Wishart
density
p (T | nq, S) ∝|T|−(nq+Q+1)/2 exp

−1
2 tr
 ST−1

9.5 Inferences from Randomized Item Response Data
265
with unity matrix S and hyperparameter nq ≥Q equal to a small number
to specify a diﬀuse proper prior. The conventional prior for σ2
θ is the inverse
gamma with prior parameters g1 and g2.
9.5.1 MCMC Estimation
The joint posterior distribution, combining the likelihood in Equation (9.15)
with the speciﬁed prior distributions, is intractable analytically, but MCMC
methods such as the Gibbs sampler and the M-H algorithm can be used to
draw samples. An MCMC scheme will be presented for binary as well as poly-
tomous response data observed via the related or forced randomized response
sampling design. The MCMC scheme consists of three parts. In part A, the la-
tent (discrete) response data are sampled given the observed RR data. In part
B, latent continuous responses are sampled given the latent discrete responses.
Finally, in part C, the parameters are sampled given the latent continuous re-
sponses. Let π(m)
ijk (c) denote the probability of a latent response in category
c according to a normal ogive or logistic item response model given latent
variable and item parameter values sampled at iteration m.
MCMC Scheme 7 (RIRT)
A1) Binary response data
1. The related-question design. Via Equation (9.13),
eYijk | Yijk = 1, π(m)
ijk ∼B

λ =
p1πijk
p1πijk + (1 −p1)(1 −πijk)

,
eYijk | Yijk = 0, π(m)
ijk ∼B

λ =
(1 −p1)πijk
p1(1 −πijk) + (1 −p1)πijk

,
where λ deﬁnes the success probability of the Bernoulli distribution.
2. The unrelated-question design. The latent item responses are Bernoulli
distributed,
eYijk | Yijk = 1, π(m)
ijk ∼B

λ = πijk (p1 + p2(1 −p1))
p1πijk + p2(1 −p1)

,
eYijk | Yijk = 0, π(m)
ijk ∼B
 
λ =
πijk(1 −p1)(1 −p2)
1 −
 p1πijk + p2(1 −p1)

!
.
A2) Polytomous response data
For the unrelated-question design, latent variable eYijk given Yijk = c and
π(m)
ijk is multinomially distributed with cell probabilities
∆(c) = πijk(c′)p1I(c = c′) + πijk(c′)(1 −p1)p2(c)
πijk(c)p1 + (1 −p1)p2(c)
.

266
9 Randomized Item Response Models
B1) Binary latent response data
Sample augmented data z(m+1) and item parameters ξ(m+1) given latent
response data ey(m+1) and θ(m) according to step A1 in MCMC scheme
4 with y(m+1) = ey(m+1).
B2) Polytomous latent response data
Sample augmented data z(m+1) and item parameters ξ(m+1) given latent
response data ey(m+1) and θ(m) according to step A2 in MCMC scheme
4 with y(m+1) = ey(m+1).
C)
Sample structural mixed eﬀects model parameters
1. Given normally distributed latent continuous responses, the full con-
ditional density of θ equals
θij | z(m+1)
ij
, ξ(m+1), β(m)
j
, γ(m), σ2(m)
θ
∼N (µθ, Ωθ) ,
(9.17)
where
µθ = Ωθ
 ata
 bθij + (wt
ijγ + xt
ijβj)/σ2
θ

,
(9.18)
Ωθ =
 ata + σ−2
θ
−1 ,
(9.19)
and bθij is the least squares estimate following from the regression of
zij + b on a for binary data and zij on a for polytomous data. This
full conditional is used as a proposal density in an M-H algorithm
when the continuous latent responses are logistically distributed.
2. The random regression eﬀects βj are conditionally multivariate nor-
mally distributed,
βj | θ(m+1)
j
, γ(m), σ2(m)
θ
, T(m) ∼N
 Dxt
j(θj −wjγ)/σ2
θ, D

,
where
D−1 =
 xt
jxj/σ2
θ + T−1
.
3. The ﬁxed eﬀects are conditionally multivariate normally distributed,
γ | θ(m+1), β(m+1), σ2(m)
θ
, T(m) ∼N (µγ, Ωγ) ,
where
µγ = Ωγwt(θ −xβ),
Ωγ =
 wtw + σ−1
γ σ2
θIS
−1 .
4. Variance parameter σ2
θ is conditionally distributed as inverse gamma
with parameter g1 +N/2 and scale parameter g2 +P
i|j(θij −(wijγ +
xijβj))2/2.
5. Covariance matrix T is conditionally distributed as inverse Wishart
with degrees of freedom nq + J and scale parameter S + P
j βjβt
j.

9.5 Inferences from Randomized Item Response Data
267
9.5.2 Detecting Noncompliance Behavior
Although the randomized response design protects the privacy of respondents,
some of them may not be convinced and/or do not trust the privacy protection.
Respondents may not follow the randomization scheme and always answer
the least stigmatizing category regardless of the question asked. This behav-
ior is called cheating (Clark and Desharnais, 1998) or noncompliance (e.g,
B¨ockenholt and van der Heijden, 2007; Cruyﬀ, van den Hout, van der Heijden
and B¨ockenholt, 2007; van den Hout and Klugkist, 2009). This noncompliance
behavior is characterized as consistently selecting the least self-incriminating
response categories.
Clark and Desharnais (1998) developed a method for estimating the mag-
nitude of the noncompliance behavior using two sample groups that are con-
fronted with diﬀerent randomized response designs. B¨ockenholt and van der
Heijden (2007) developed a latent two-class model where one group consists
of respondents that follow the randomized response design instructions and a
second group of respondents do not follow the instructions and show noncom-
pliance behavior. The latent class approach opens the possibility of handling
diﬀerent types of noncompliance behavior induced by diﬀerent randomized
response designs.
The RIRT model can be extended to handle noncompliance response be-
havior via a latent class structure. In the most straightforward way, assume
that noncompliance response behavior is characterized by consistently re-
sponding to the least stigmatizing option (say zero in the case of binary data or
one in the case of polytomous data). Respondents exhibiting noncompliance
behavior answer with probability one the least self-incriminating response,
and their response patterns do not provide any information about the items’
characteristics.
A binary latent class variable Gijk is deﬁned such that Gijk = 1 when
respondent ij answers item k according to self-protective response behavior
(noncompliance) and Gijk = 0 when respondent ij answers item k according
to the randomized response mechanism. Let νij = P (Gijk = 1) denote the
conditional probability that respondent ij answers question k in the least
self-incriminating way. For binary response data, it follows that
P (Yijk = 0) = (1 −νij) P (Yijk = 0 | ξk, θij) + νijI (Yijk = 0)
= (1 −νij) f (1 −πijk; ˜p1, ˜p2) + νijI (Yijk = 0) ,
(9.20)
where the ﬁrst category is considered to be the least incriminating response
and πijk is the success probability of respondent ij in the compliance class
(Equation (9.8)) given a randomized response design (Equation (9.5)).
The full model is a mixture model consisting of an RIRT model for the
compliance class and a model for the noncompliance class. The mixing weight
parameter νij combines the two models. This mixture model can be recog-
nized from Equation (9.20), where the observed response is distributed with

268
9 Randomized Item Response Models
probability (1 −νij) according to the RIRT model and with probability νij
according to the noncompliance response model. When there is no noncom-
pliance response behavior, the mixture model resembles the RIRT model.
Inferences concerning the parameters of interest in the RIRT model are
to be based on the response data of the respondents in the compliance class.
This requires knowledge of the status of the latent class variable Gijk. The
conditional distribution of the latent class variable Gijk is Bernoulli; that is,
Gijk | νij, yij, πij ∼B

νijI (Yijk = 0)
νijI (Yijk = 0) + (1 −νij)f (1 −πijk)

, (9.21)
where the success probability speciﬁes the conditional probability that respon-
dent ij answers in the least self-incriminating way to item k. Assume a beta
prior with parameters α and β for the mixing weight νij. The full conditional
posterior of νij is beta; that is,
νij | gij ∼Be (α + ng, β + K −ng) ,
(9.22)
where ng = P
k I(Gijk = 1). MCMC scheme 7 is easily extended to handle
noncompliance behavior by adding the sampling steps in Equations (9.21)
and (9.22). Subsequently, in part A, latent discrete response data are sampled
for observed responses that stem from the RIRT model. In that case, the
corresponding latent class variables are zero. The other sampling steps remain
the same.
The prior distribution of the mixing weights can be made more informative
by adding related background information on the respondents. This might
also decrease the uncertainty in the estimation of the mixing weights and
other model parameters. When interest is focused on the mixing weights, a
structural model can be deﬁned on the parameters ν to explain individual and
group diﬀerences and to investigate relationships to background variables.
9.5.3 Testing for Fixed-Group Diﬀerences
In the structural mixed eﬀects model, Equation (9.12), a grouping structure
is modeled via random eﬀects parameters. The groups are assumed to be
sampled from a population, and interest is focused on the characteristics of
this population distribution. A diﬀerent analysis is required when inferences
are to be made about those groups that are in the sample.
Assume that a grouping structure is deﬁned by indicator variables that are
stored in a design matrix w and that matrix wtw is nonsingular. Attention is
focused on the grouping structure in the latent variable θ. A linear hypoth-
esis is considered where the expectation of a θ is a known linear function of
unknown parameters γ, E(θ) = wγ.
Interest is in a point null hypothesis of the form γ = γ0 and an alternative
hypothesis γ ̸= γ0. Therefore, an HPD region is constructed from the condi-
tional posterior distribution, and the null hypothesis is rejected if and only if

9.5 Inferences from Randomized Item Response Data
269
γ0 is outside the HPD region. A p-value p0 can be deﬁned as one minus the
content of the HPD region that just covers γ0, which equals
p0 = 1 −P [p (γ | θ) > p (γ0 | θ) | θ] .
(9.23)
The null hypothesis is rejected when the p-value is less than or equal to a
signiﬁcance level α.
For the moment, the factor eﬀects γ and the log of the within-group vari-
ance, log σ2
θ, are assumed to have independent prior distributions that are
uniform over the real line. The joint posterior distribution of γ and σ2
θ given
θ is
p
 γ, σ2
θ | θ

∝σ−n−2
θ
exp
 −1
2σ2
θ
(θ −wγ)t (θ −wγ)

∝σ−n−2
θ
exp
 −1
2σ2
θ

(γ −ˆγ)t wtw (γ −ˆγ) + S2
,
where ˆγ = (wtw)−1wtθ and S2 is the residual sum of squares. Integrating
with respect to σ2
θ leads to (see Box and Tiao, 1973, Chapter 2)
p (γ | θ) ∝
"
1 + (γ −ˆγ)t wtw (γ −ˆγ)
S2
#−n
2
∝

1 + S2
s(γ)/(n −s)
S2/(n −s)
−n
2
.
(9.24)
Lindley (1965, Chapter 8) and Box and Tiao (1973) showed that the
marginal posterior in Equation (9.24) is a monotonically decreasing func-
tion of the S2
s(γ) and S2
s(γ) = c deﬁnes a contour of the distribution
in the s-dimensional space of γ. Furthermore, they show that the term
(S2
r(γ)/r)/(S2/(n −s)) is F-distributed with (r, n −s) degrees of freedom
for any r ≤s. That is, the contours of the marginal distribution of any subset
r of the ﬁxed factor eﬀects γ are deﬁned by an F-distribution and will deﬁne
a conﬁdence region.
The conditional posterior probability that γ0 is not included in the HPD
region (Equation (9.23)) depends on the usually unknown parameter θ. The
corresponding marginal posterior probability that a certain point is included
can be computed via MCMC. The marginal posterior probability that an r-
dimensional subset γ0 is not included in a (1 −α) HPD region is speciﬁed
by
α ≥1 −P [p (γ | y) > p (γ0 | y) | y]
≥
Z
1 −P

F(r, n −s) ≤S2
r(γ0)/r
S2/(n −s)
 θ

p (θ | y) dθ
≥
Z
p0 (θ) p (θ | y) dθ ≈
X
m
p0

θ(m).
M,
(9.25)

270
9 Randomized Item Response Models
where θ(m) is an MCMC sample from the marginal posterior distribution.
The conditional posterior probability is easily computed using the fact that
the quantity is F-distributed and the marginal posterior probability is ob-
tained using the MCMC sample. That is, the marginal posterior probability
of interest is obtained by integrating over the parameter θ using an MCMC
sample from the marginal posterior.
9.5.4 Model Choice and Fit
The assumptions of the RIRT model can be evaluated via a Bayesian residual
analysis (see Chapter 5). The Bayesian residual Rijk = Yijk −P
c c·f(πijk(c))
is the diﬀerence between the observed randomized response and the expected
response. These residuals provide information about the ﬁt at the level of
the randomized response model. The residuals can be estimated in MCMC
scheme 7. At another level of the model, latent residuals can be deﬁned as
eRijk = eYijk −P
c c·πijk(c), which represents the diﬀerence between the latent
response and the expected latent response. The latent residuals can also be
estimated from MCMC scheme 7.
The ﬁt of the items and the persons can be tested via two functions of
sums of squared residuals,
Qk (y) =
N
X
i=1
R2
ik,
Qi (y) =
K
X
k=1
R2
ik.
Both functions can be deﬁned in the same way for the latent residuals. Sub-
sequently, the functions can be used as discrepancy measures in a posterior
predictive check to evaluate the ﬁt; that is,
P (Qi (yrep) ≥Qi (y) | y) =
X
yrep∈Yrep
I (Qi (yrep) ≥Qi (y)) p (yrep | y) ,
where Yrep is the set of all possible randomized response vectors of yrep. The
extremeness of the realized discrepancy is evaluated via replicated data under
the model. Relatively high function values correspond to a poor ﬁt of the item
or person using the marginal distribution of the replicated data to quantify
the extremeness of the realized discrepancy.
A test of local independence can be performed by evaluating the con-
ditional covariance between residuals concerning two items given the per-
son parameters. Let Rk(˜y, θ) denote the vector of residuals of item k given
the latent response data and the latent person parameters. The covari-
ance between two vectors of item residuals k and k′ is denoted as σk,k′ =
Cov (Rk(˜y, θ), Rk′(˜y, θ)). The assumption of local independence is violated

9.5 Inferences from Randomized Item Response Data
271
with signiﬁcance level α when the point σk,k′ = 0 is not included in the
(1 −α) HPD region; that is, when the posterior probability
p0 = P (p (σk,k′ | y) > p (σk,k′ = 0 | y) | y)
(9.26)
=
X
˜y∈˜
Y
P (p (σk,k′ | ˜y) > p (σk,k′ = 0 | ˜y) | ˜y) p (˜y | y)
=
X
˜y∈˜
Y
Z
P (p (σk,k′ | θ, ˜y) > p (σk,k′ = 0 | θ, ˜y) | ˜y) p (θ | ˜y) dθp (˜y | y)
is greater than 1 −α. This posterior probability can be computed via MCMC
scheme 7.
A DIC can be computed to compare models with each other but requires
an analytical expression for the likelihood. In a similar way as in deriving
an expression for the likelihood of the MLIRT model in Appendix 6.10, an
augmented likelihood expression is derived for the augmented data ˜y such
that the likelihood is expressed as an integrated augmented likelihood. Let
the parameters of interest be deﬁned as Λ = (γ, ξ, σ2
θ, T). Then
p (y | Λ) =
X
˜y∈˜
Y
p (y | ˜y) p (˜y | Λ)
=
Z X
˜y∈˜
Y
p (y | ˜y) p (˜y | z) p (z | Λ) dz
=
Z Z X
˜y∈˜
Y
p (y | ˜y) p (˜y | z) p (z, θ | Λ)
p (θ | z, Λ)dzdθ
=
Z Z Z
p (y | z) p (z, θ | β, Λ)
p (θ | z, β, Λ)
p (β | T)
p (β | z, Λ)dzdθdβ. (9.27)
As deﬁned in MCMC scheme 7, there are two types of augmented data.
The observed randomized responses are augmented with latent (discrete) re-
sponses. The discrete latent responses are augmented with continuous latent
responses denoted as z. The likelihood part p (y | ˜y) presents the ﬁt of the
randomized response model, and the part p (˜y | z) is equal to one since the
values of z completely specify the values of ˜y. These likelihood parts do not
contain the parameters of interest and are not interesting for comparing dif-
ferent RIRT models since they are similar with respect to the randomized
response model part. A DIC for comparing RIRT models that diﬀer in the
structural part and share the same randomized response part is constructed
from the other likelihood terms in Equation (9.27). The corresponding log-
likelihood term, log p (z | Λ), can be expressed as
log p (z | Λ) = 1
2
X
j
h
−Knj log(2π) + nj log (Ωθ) −nj log
 σ2
θ

−log |T|
+ log
Σ ˜βj
 −S

˜θj, ˜βj
i
,
(9.28)

272
9 Randomized Item Response Models
where
S

˜θj, ˜βj

=
X
i,k

zijk −

ak˜θij −bk
2
+ σ−2
θ

˜θj −wjγ −xj ˜βj
t
·

˜θj −wjγ −xj ˜βj

+ ˜β
t
jT−1˜βj.
(9.29)
Furthermore, each ˜θij is independently normally distributed with mean µθ
(Equation (9.18)) and variance Ωθ (Equation (9.19)) with βj = ˜βj. The
˜βj = E (βj | zj, Λ) and Σ ˜βj are deﬁned in Equations (6.42) (with mean wjγ)
and (6.43), respectively.
The deviance for model comparison is deﬁned as D (Λ) = −2 log p (z | Λ),
and the DIC is deﬁned as
DIC =
X
˜y∈˜
Y
Z
[DIC | z] p (z | ˜y) dz p (˜y | y)
=
X
˜y∈˜
Y
Z h
D

bΛ

+ 2pD | z
i
p (z | ˜y) dz p (˜y | y)
= Ez,˜y
h
D

bΛ

+ 2pD | y
i
,
(9.30)
where bΛ is the posterior mean of Λ. MCMC scheme 7 can be used to estimate
the DIC using the likelihood expression in Equation (9.28).
9.6 Simulation Study
In the ﬁrst study, univariate randomized item response data were simulated
under diﬀerent randomized response designs. In the second study, hierarchi-
cally structured multivariate randomized item response data were simulated
under diﬀerent randomizing device characteristics. Interest was focused on
the diﬀerential eﬀects of diﬀerent randomized response sampling designs or
diﬀerent design properties on the structural model parameter estimates.
9.6.1 Diﬀerent Randomized Response Sampling Designs
A total of N binary latent responses, ˜y, divided at random across ten groups,
were generated according to the (nonlinear) mixed eﬀects model
P

˜Yij = 1

= πij = Φ (γ0 + u0j + xiγ1) ,
(9.31)
where u0j ∼N(0, τ 2). The values of vector x were simulated from a nor-
mal distribution with mean zero and standard deviation 1/2. Randomized
response data, y, were generated via Equation (9.5) with p1 = 4/5 according

9.6 Simulation Study
273
Table 9.1. Re-estimating mixed eﬀects model parameters given latent or random-
ized responses.
Latent Response
Warner
Forced
N
Par. True
Mean
SD
Mean SD Mean SD
1000
γ0
.00
−.06
.16
−.06 .16
−.07 .16
γ1 2.00
1.94
.17
1.81 .26
1.80 .20
τ
.25
.29
.18
.20 .16
.24 .16
5000
γ0
.00
−.12
.15
−.11 .16
−.12 .16
γ1 2.00
2.01
.07
2.01 .12
1.97 .09
τ
.25
.23
.14
.22 .15
.24 .15
to Warner’s model (related response design) and with p1 = 4/5 and p2 = 2/3
according to the forced response model (unrelated response design).
MCMC algorithm 7 was run for 20,000 iterations, convergence was ob-
tained after 5,000 iterations, and the cumulative averages of sampled param-
eter values resembled the true parameter values. Table 9.1 presents the true
values, posterior mean estimates, and standard deviations given the latent
response vector ˜y, labeled Latent Response, for the mixed eﬀects Warner
model, labeled Warner, and for the mixed eﬀects forced response model, la-
beled Forced. It is apparent that the point estimates resemble the true values
for a sample size of N=5,000 and the estimates are close to the true values
for a sample size of N=1,000. The Warner model has the largest estimated
standard deviations with respect to parameter γ1, which was also found by
Scheers and Dayton (1988).
The proportion, π, of positive responses was estimated using M MCMC
sampled values of latent response data ˜y(m),
ˆπ =
1
MN
X
m
X
i,j
˜y(m)
ij
.
For N=5,000, the simulated proportion equals .464. The estimated proportion
under the mixed eﬀects Warner model equals .464 with standard deviation
.007, and equals .465 with standard deviation .004 under the mixed eﬀects
forced response model. These point estimates resemble the estimated propor-
tion using Warner’s model, ˆπ = .462 with standard deviation .010, and using
the forced response model, ˆπ = .464 with standard deviation .008.
For N=1,000, the simulated proportion equals .485 and the estimated pro-
portions under the mixed eﬀects Warner model and the mixed eﬀects forced
response model equal .488 (.014) and .480 (.009), respectively, where the stan-
dard deviations are given in parentheses. The point estimates under Warner’s
model and the forced response model equal .490 (.023) and .481 (.019), re-
spectively. For both sample sizes, it can be concluded that the estimated
proportions are comparable but that there is a reduction in sampling error

274
9 Randomized Item Response Models
since the mixed eﬀects model accounts for the grouping of individuals and
utilizes the covariate information.
9.6.2 Varying Randomized Response Design Properties
Estimates of mixed eﬀects model parameters are compared given directly ob-
served and RR data for diﬀerent randomizing proportions. The probability
distribution of a latent behavior parameter, θij, has the same form for each
individual, but the parameters of that distribution vary over 20 groups. A
level-2 model describes a linear relationship between the behaviors of N =
1,000 respondents and explanatory variables x and w, and a level-3 model
represents the distribution of the random eﬀect parameters
θij = xt
ijβj + wijγ(1) + eij,
β0j = γ(2)
0
+ u0j,
β1j = γ(2)
1
+ u1j,
where eij ∼N
 0, σ2
, uj ∼N (0, T), and the random eﬀects are independent
of the residuals.
The ﬁrst column of x consists of ones, and the second column and the vec-
tor w contain values generated from a normal distribution with mean zero and
standard deviation .30. Latent responses to 10 items per subject, each with
three ordinal response categories, were simulated using a graded response
model. The latent responses were randomized via the unrelated-question de-
sign with p2 = 1/3. The probability that the latent response matched the
observed response (e.g., a truthful response was demanded), p1, was consid-
ered to be one (which resembles a direct response), .80, and .60. A total of 100
datasets were analyzed for each value of p1. Discrimination parameter values
were sampled from a lognormal distribution, ak ∼log N(exp(1), 1/4). Thresh-
old parameters, κk1 and κk2, were sampled from a normal distribution with
means −1/2 and 1/2 (taking order restrictions into account), respectively, and
variance 1/4.
For identiﬁcation of the model, the mean and variance of the latent behav-
ior variable were scaled to the true simulated mean and variance, respectively.
For each dataset, the graded response model parameters and the mixed ef-
fects model parameters were estimated simultaneously using 50,000 posterior
draws. The burn-in period consisted of 5,000 iterations. Table 9.2 presents,
for each model parameter, the true setup value, the average of the means,
and standard deviations over the MCMC samples corresponding to the 100
datasets.
It can be seen that there is close agreement between the true and the av-
erage estimated means. For each model parameter, the average of posterior
standard deviations resembled the standard deviation in the 100 estimated
posterior means. Note that even for p1 = .60, which means that 40% of the

9.7 Cheating Behavior and Alcohol Dependence
275
Table 9.2. Generating values, means, and standard errors of recovered values.
Direct Response
Forced
p1 = 1
p1 = .80
p1 = .60
Parameter
True
Mean
SD
Mean SD Mean SD
Fixed Eﬀects
γ(1)
.5
.50
.03
.50 .04
.49 .04
γ(2)
0
0
.03
.11
.02 .11
−.08 .10
γ(2)
1
0
.04
.11
.04 .12
.08 .10
Random Eﬀects
σ2
e
1
1.00
.05
1.00 .05
.98 .05
τ00
.3
.36
.08
.36 .09
.31 .08
τ11
.2
.20
.05
.20 .06
.20 .06
τ01
0
.13
.04
.14 .05
.00 .04
responses were forced responses, the estimated values resemble the true sim-
ulated values. The standard deviations of the mixed eﬀect parameter esti-
mates were not increasing, due to the incorporation of a randomized response
sampling design. Furthermore, additional variance in the item parameter es-
timates, due to the randomized response sampling design, did not result in
biased estimates.
9.7 Cheating Behavior and Alcohol Dependence
Data from two randomized response research studies were analyzed. In the ﬁrst
study, attention was focused on measuring cheating behavior of students at
a Dutch university. The questionnaire consists of binary items. In the second
study, attention was focused on measuring alcohol dependence of students
of diﬀerent colleges and universities in North Carolina. This questionnaire
consists of polytomous items and was described in Section 5.3.1.
9.7.1 Cheating Behavior at a Dutch University
Detecting fraud is hard, and educational organizations often are not willing
to expend the eﬀort required to get to the bottom of cheating cases. On
the other hand, student cheating and plagiarism is becoming an important
problem with the rising number of ways to cheat on exams. The introduction
of mobile phones and handheld computers has led to high-tech cheating with
Web-equipped cell phones or handheld organizers. In 2002, a study was done
to assess cheating behavior of students at a university in the Netherlands. The
main targets were to investigate the number of students committing fraud, the
reasons, and the diﬀerent ways that students are cheating on exams.

276
9 Randomized Item Response Models
Students received an email in which they were asked to participate in the
study. The forced randomized response method was explained in the email
to gain the respondents’ conﬁdence, so that they were willing to participate
and to answer the questions truthfully. A website was developed containing
36 questions concerning cheating on exams and assignments. Note that types
of cheating or academic dishonesty are measured by diﬀerent items to capture
the whole range of cheating. The 36 questionnaire items can be found in Fox
and Meijer (2008). Each item was a statement, and respondents were asked
whether they agreed or disagreed with it. When a student visited the website,
an on-Web dice server rolled two dice before a question could be answered.
The student answered “yes” when the sum of the outcomes equaled 2, 3, or
4, answered “no” when the sum equaled 11 or 12, or otherwise answered the
sensitive question. In practice, a forced response was automatically given since
it is known that some respondents ﬁnd it diﬃcult to lie (Fox and Tracy, 1986).
As a result, the forced response technique was implemented with p1 = 3/4
and p2 = 2/3.
Data were available from 349 students (229 male students and 120 female
students) from one of seven main disciplines: computer science (CS), educa-
tional science and technology (EST), philosophy of science (PS), mechanical
engineering (ME), public administration and technology (PAT), science and
technology (ST), and applied communication sciences (ACS). Within these
seven disciplines, a stratiﬁed sample of students was drawn such that diﬀer-
ent majors were represented in proportion to their total number of students.
In the cheating literature, several variables, such as personal attributes,
background characteristics, or situational factors, have been shown to be re-
lated to cheating behavior. For example, earlier cheating research focused on
estimating population proportions of cheating across locations (small/large
university), school types (private/public), or individual diﬀerences (male/fe-
male) (see, for example, Davis, Grover, Becker and McGregor, 1992). In the
present study, background information was collected with respect to age, gen-
der, year of major, number of hours per week spent on the major (less than
10 hours, 10–20 hours, 20–30 hours, 30–40 hours, and more than 40 hours),
residence (on campus, in the city, with their parents), and lifestyle (active or
passive). Respondents were guaranteed conﬁdentiality, and the questionnaires
were ﬁlled in anonymously.
Diﬀerences in Attitudes
In this study, the three ﬁxed factors, gender, major, and year of major, were
considered as student characteristics that are likely to explain diﬀerences in
cheating behavior. A design matrix w was constructed that represent diﬀerent
grouping structures via indicator variables. The latent attitude parameter was
assumed to be normally distributed with mean wγ and standard deviation σθ.
This structural part is the third stage of the RIRT model. The other stages of

9.7 Cheating Behavior and Alcohol Dependence
277
the model are deﬁned in Equations (9.5) and (9.8), where the discrimination
parameters were ﬁxed to one.
The RIRT model with the ﬁxed factors was estimated given the random-
ized responses to the 36 items. The model was identiﬁed by ﬁxing the mean
of the scale of the attitude parameter to zero. The MCMC procedure con-
tained 50,000 iterations and a burn-in period of 5,000 iterations. Convergence
of the MCMC chain was checked using the standard convergence diagnostics
from the BOA program. Plots of the runs and the diagnostic tests suggested
a convergence of the MCMC chain.
In Table 9.3, the number of observations per group and the parameter
estimates, including the estimated factor eﬀects, are given. It can be seen that
the estimated mean attitude for the males is slightly below zero. This means
that female students cheat slightly more than male students. There is variation
in attitudes across the seven majors, where computer science students hardly
cheat and applied communication science students often cheat. It is apparent
that freshmen cheated less than others. Third-year students and those in the
sixth-year and above were more inclined to cheat.
Let γ denote the eﬀect of being male. Then, using Equation (9.25), the tail-
area probability of one minus the marginal posterior probability that γ = 0
(no gender diﬀerences) is just included in the HPD region was computed.
This tail-area probability corresponds to the null hypothesis that there are no
gender diﬀerences in attitudes. The eﬀect of gender is not signiﬁcant with a
signiﬁcance level of α = .01. In the same way, it is concluded that there are
no signiﬁcant diﬀerences in levels of cheating across years in college.
Interpretation of the relationship between cheating and year of study is
diﬃcult because many characteristics (such as motivation, age, and experi-
ence) change as students progress through the grade levels. For example, it is
known that cheating is negatively correlated with age. Finally, it is mentioned
that attitudes towards cheating diﬀer signiﬁcantly across majors. The largest
diﬀerence was found between CS and ACS students, with ACS students more
inclined to cheat than CS students.
Item Level Analysis
To illustrate the advantages of the item response modeling approach, Figure
9.2 presents the ICCs of six items about diﬀerent ways of cheating. The ICCs
reveal that the items have diﬀerent threshold values.
The ICCs of item 10 (“use of crib notes during an exam”) and item 31
(“lying to postpone a deadline”) have the lowest thresholds and thus represent
the most popular ways of cheating of the six selected items (they are also the
most popular methods among all 36 items). In contrast, the ICCs of item 2
(“received information using signals or sign language during an exam”) and
item 7 (“obtaining information outside the classroom or examination area”)
have higher thresholds and are thus less popular. Note that here the term
popular refers to a preference of those respondents who are likely to cheat and

278
9 Randomized Item Response Models
Table 9.3. Cheating study: Parameter estimates of the RIRT model with ﬁxed
factors gender, major, and year of major.
RIRT Model
nj
Mean
SD
HPD
p0
Fixed Eﬀects
γ(2) (Intercept)
−.020
.123
[−.266, .217]
Student variables
Gender
.015
γ(1)
1
(Male)
229 −.020
.163
[−.342, .294]
Major
.001
γ(1)
2
(CS)
50 −.438
.184
[−.821, −.090]
γ(1)
3
(PAT)
53
.180
.157
[−.144, .469]
γ(1)
4
(ACS)
53
.448
.161
[.129, .762]
γ(1)
5
(ST)
46 −.045
.176
[−.401, .288]
γ(1)
6
(EST)
66
.157
.159
[−.159, .466]
γ(1)
7
(ME)
49 −.157
.173
[−.498, .168]
γ(1)
8
(PS)
32 −.141
.211
[−.565, .259]
Year of Major
.030
γ(1)
9
(First)
52 −.661
.192 [−1.063, −.298]
γ(1)
10 (Second)
73
.026
.140
[−.250, .299]
γ(1)
11 (Third)
66
.235
.137
[−.041, .495]
γ(1)
12 (Fourth)
61
.085
.145
[−.210, .360]
γ(1)
13 (Fifth)
45
.078
.167
[−.268, .390]
γ(1)
14 (>Fifth)
52
.237
.160
[−.083, .546]
Random Eﬀects
σ2
θ Residual
.857
.077
[.710, 1.007]
does not refer to a general preference among the population of respondents.
Item 34 (“submitted coursework from others without their knowledge”) and
item 12 (“looking at others’ work with their knowledge during an exam”)
have the same item characteristics. Most interesting, however, is that from
inspecting the ICCs (Figure 9.2) it can be concluded that using signals is
a popular method for students with high cheating attitudes. The analysis
showed that students who use signals are likely to be frequent cheaters.
In conclusion, not “everyone’s doing it,” but about 25% of the students
admitted they have cheated on an exam. The analysis with the RIRT model
revealed that cheating behavior varied across majors and that computer sci-

9.7 Cheating Behavior and Alcohol Dependence
279
-3
-2
-1
0
1
2
3
Attitude parameter 
Attitude parameter 
0.0
0.2
0.4
0.6
0.8
1.0
Probability positive latent response
Probability positive latent response
Item 7
Item 7
Item 2
Item 2
Item 34
Item 34
Item 12
Item 12
Item 10
Item 10
Item 31
Item 31
Item 2: Using signals
Item 7: Obtaining information outside the classroom
Item 10: Using a crib note
Item 12: Looking at others’ work
Item 31: Lying to postpone a deadline
Item 34: Submitting others’ work as your own
Item 2: Using signals
Item 7: Obtaining information outside the classroom
Item 10: Using a crib note
Item 12: Looking at others’ work
Item 31: Lying to postpone a deadline
Item 34: Submitting others’ work as your own
Fig. 9.2. Cheating study: Item characteristic functions for ways of cheating.
ence students are less likely to cheat. More details about the RIRT model
cheating analysis can be found in Fox (2005c) and Fox and Meijer (2008).
9.7.2 College Alcohol Problem Scale
In Section 5.3.1, the response data of 351 students answering 13 items from
the CAPS questionnaire (Appendix 5.9) were analyzed. The responses of the
351 students were obtained via direct questioning. The CAPS questions can
be marked as sensitive since they will elicit answers that are socially unac-
ceptable. Although conﬁdentiality was promised, misreporting was expected
due to socially desirable response behavior.
Whether the randomized response technique improved the quality of the
self-reports was investigated. Therefore, each class of respondents (5–10 par-
ticipants) was randomly assigned to either the direct questioning (DQ) or
the randomized response technique condition. Random assignment at the in-
dividual level was not logistically feasible. The 351 students assigned to the
DQ condition, denoted as the DQ group, were instructed to answer the ques-
tionnaire as they normally would. They served as the study’s control group.

280
9 Randomized Item Response Models
Table 9.4. CAPS: Gender and ethnicity demographics.
Total
DQ Group
RR Group
Count
(%)
(%)
Gender
Female
502
65
62
Male
291
35
38
Ethnicity
Asian
17
3
1
White
655
81
84
Black
93
12
11
Other
28
4
4
Note: DQ Group = direct questioning condition,
RR Group = randomized response condition.
The 442 students in the RR condition, denoted as the RR group, received a
spinner to assist them in completing the questionnaire. For each CAPS item,
the participant spun the spinner, and wherever the arrow landed determined
whether the item was to be answered honestly or dictated the answer choice
to be recorded by the participant. The spinner was developed such that 60%
of the area was comprised of answer honestly space and 40% of the area was
divided into equal sections to represent the ﬁve possible answer choices. Each
answer choice was given 8% of the area of the circle, 4% in two diﬀerent places
on the circle. The respondents from the DQ group and the RR group were
assumed to be selected from the same population.
In total, 793 student participants from four local colleges and universities,
Elon University (N=495), Guilford Technical Community College (N=66),
University of North Carolina (N=166), and Wake Forest University (N=66),
voluntarily responded to a questionnaire with 16 items in 2002. Three items
of this questionnaire asked participants about their age, gender, and ethnicity
(demographic information). In Table 9.4, the demographic (gender and eth-
nicity) percentages are given, and it follows that they are similar for the DQ
group and RR group.
A unidimensional latent variable representing alcohol dependence, denoted
as θ, was measured by the items, where a higher level indicated that a par-
ticipant was more likely to have a drinking problem. A forced randomized
response sampling design was implemented with p1 = .60 and p2(c) = .20
(c = 1, . . . , 5). All response data, observed via direct questioning and via the
randomized response technique, were used to measure the latent behaviors
(alcohol dependence) of the respondents on a common scale using the graded
response model. This results in the level-1 part of the RIRT model,
P(Yijk = c | θij, ak, κk) =
 p1πijk(c) + (1 −p1)p2(c) i ∈RR
πijk(c)
i ∈DQ,
(9.32)

9.7 Cheating Behavior and Alcohol Dependence
281
where
πijk(c) = Φ (akθij −κk,c−1) −Φ (akθij −κk,c) .
An indicator variable w1ij was deﬁned that equaled one if respondent i at
university j was assigned to the RR group and zero otherwise. Other observed
explanatory information (gender and ethnicity) was also stored in the matrix
w via eﬀect coding such that the intercept equaled the (unweighted) grand
mean. This led to the structural mixed eﬀects model for the latent variable,
θij = γ(2) + β0j + wt
ijγ(1) + eij,
(9.33)
where eij ∼N
 0, σ2
θ

and β0j ∼N
 0, τ 2
.
It was assumed that the item response functions were the same across
groups, that is, the response probabilities given the alcohol dependence level
did not depend on group membership (e.g., DQ group and RR group). The
general intercept, γ(2), in Equation (9.33) presents the general mean level
of the males of diﬀerent ethnic origin in the DQ group and parameter γ(1)
1
denotes the contribution of the treatment eﬀect, that is, being questioned
via the randomized response technique. A signiﬁcant positive treatment eﬀect
was expected, which means that the randomized response technique induced
respondents to answer the items more truthfully.
The model was identiﬁed by ﬁxing the mean and variance of the scale of
the latent variable to zero and one, respectively. The MCMC algorithm was
used to estimate all parameters simultaneously, using 50,000 iterations with
a burn-in period of 5,000 iterations. The estimated value of the parameter
corresponding to the RR indicator variable is .232 and signiﬁcantly diﬀerent
from zero while controlling for other population diﬀerences. This estimate
indicates that the RR group scored signiﬁcantly higher in comparison with
the DQ group on the standardized alcohol dependence scale. It is concluded
that the randomized response technique led to an improved willingness of
students to answer truthfully.
Fixed Versus Random Eﬀects
The structural relationships between students’ alcohol dependence and ob-
served background variables were estimated using the observations from the
RR group since those students were more likely to give honest answers. Be-
sides a mixed eﬀects model, an alternative ﬁxed eﬀects model was estimated
where interest was focused on students of the four selected colleges/universi-
ties that took part in the experiment and not on the underlying population.
In a similar way, the clustering of students in colleges/universities was repre-
sented using dummy variables. The corresponding structural model contained
only ﬁxed eﬀects.
In Table 9.5, the parameter estimates are given for both models. The es-
timates of the mean and posterior standard deviation of the random eﬀects

282
9 Randomized Item Response Models
(college/university) are given under the label Mixed Eﬀects Model. The esti-
mated variance of the random eﬀects, τ 2, indicates that alcohol dependence of
students varies across colleges/universities. However, the corresponding esti-
mated posterior standard deviation is too large to make substantial inferences.
The ﬁxed eﬀects estimates show a slightly stronger variation in alcohol de-
pendence across colleges/universities. From the DICs it follows that the ﬁxed
eﬀects RIRT model ﬁts the data better than the mixed eﬀects RIRT model.
Furthermore, it follows that male students scored signiﬁcantly higher in
comparison with female students. That is, male students are more likely to ex-
perience alcohol-related problems. There are inequalities in reporting alcohol-
related problems across ethnic groups, and it turns out that the mean score
of black students is much lower than that of other ethnic groups. The mean
score of students from Guilford Technical Community College is higher than
the other college/university means. The results indicate that gender, ethnic-
ity, and college/university are associated with alcohol-related problems. From
Equation (9.25), it followed that both null hypotheses, γ(1)
3
= γ(1)
4
= γ(1)
5
=
γ(1)
6
and γ(1)
7
= γ(1)
8
= γ(1)
9
= γ(1)
10
are rejected with α = .05. It can be
concluded that there is a main eﬀect for ethnicity and college/university.
Explaining Item Response Variation
The latent category response probabilities for item 10 (“drove under the in-
ﬂuence”) were investigated for respondents in the RR group. Each respondent
i had a probability πik(c) of giving a latent response in category c. Interest
was focused on comparing the latent category response probabilities of item
10 across gender and colleges/universities.
The latent response model of the ﬁxed eﬀects RIRT model was expanded
for item 10 and the factors gender and college/university,
Zik = akθi + wt
iγ + ϵik
(9.34)
for k = 10, were included, where ϵik is standard normally distributed and w is
a design matrix that records the factor information via eﬀect coding. In this
case, diﬀerences in latent category response probabilities are explained by a
random person eﬀect and ﬁxed factors gender and college/university. Note
that in the structural model of Equation (9.33) diﬀerences in the individual
alcohol dependence levels are explained by the ﬁxed factors. In that case, the
ﬁxed factor eﬀects are assumed to be the same across items.
In Figure 9.3, the estimated latent category response probabilities per
factor level are given from the ﬁxed eﬀects RIRT analysis that includes the
speciﬁc latent response model for item 10 (Equation (9.34)). Visual inspection
shows that males have lower response probabilities for the category “never”
and higher response probabilities for the other categories. The estimated cate-
gory response probabilities also diﬀer visually across levels of factor college/u-
niversity. It follows that students from Guilford Technical Community College

9.7 Cheating Behavior and Alcohol Dependence
283
Table 9.5. CAPS: Parameter estimates of a mixed and ﬁxed eﬀects RIRT model
using the RR data.
Mixed Eﬀects Model
Fixed Eﬀects Model
Mean
SD
HPD
Mean
SD
HPD
Fixed Eﬀects
γ(2) (Intercept)
.118
.445
[−.780, .969]
.140
.157
[−.179, .442]
Student variables
γ(1)
1
(Female)
−.261
.102 [−.465, −.065]
−.264
.109 [−.483, −.052]
Ethnicity
γ(1)
2
(Asian)
−.180
.293
[−.758, .375]
−.198
.324
[−.854, .412]
γ(1)
3
(White)
.089
.118
[−.141, .321]
.085
.141
[−.195, .357]
γ(1)
4
(Black)
−.474
.178 [−.837, −.137]
−.465
.186 [−.824, −.095]
γ(1)
5
(Other)
.543
.247
[.079, 1.027]
.587
.240
[.092, 1.050]
School variables
University
γ(1)
6
(Elon)
.188
.127
[−.073, .417]
.041
.092
[−.144, .217]
γ(1)
7
(UNCG)
−.148
.137
[−.439, .105]
−.288
.122 [−.529, −.054]
γ(1)
8
(Wake Forest) −.014
.159
[−.370, .262]
−.150
.154
[−.452, .149]
γ(1)
9
(Guilford)
.474
.149
[.143, .722]
.406
.161
[.080, .712]
Random Eﬀects
Within schools
σ2
θ Residual
.913
.066
[.787, 1.049]
.912
.065
[.789, 1.038]
Between schools
τ 2Intercept
.721
.773
[.106, 1.861]
Information Criteria
−2 log-likelihood
11768.90
11666.75
DIC
17979.75
17907.82
are more likely to drive under the inﬂuence. The ﬁxed factor eﬀects γ in Equa-
tion (9.34) were tested in a similar way as in Equation (9.25). It was concluded
that the factor levels were not signiﬁcantly diﬀerent from zero. This means
that the factors explained signiﬁcant structural diﬀerences in the individual
levels of alcohol dependence (see Table 9.5) but no signiﬁcant additional item-
speciﬁc diﬀerences in the latent category response probabilities of item 10.

284
9 Randomized Item Response Models
0.0
0.2
0.4
0.6
0.8
1.0
Elon
NC
WF
GTC
Elon
NC
WF
GTC
Female
Male
Drove under the influence (item 10)
Elon = Elon University, NC = University of North Carolina,
WF = Wake Forest University, GTC = Guilford Technical Community College
Fig. 9.3. The estimated latent category response probabilities of item 10 across
gender and colleges/universities.
9.8 Discussion
An RIRT model is developed for analyzing binary or polytomous hierarchically
structured randomized response data. The proposed RIRT model is capable of
treating a variety of special problems in a uniﬁed framework. The statistical
inference of RR data is improved and/or expanded by assuming an individ-
ual response model that speciﬁes the relation between the randomized item
response data and an (individual) underlying behavior (multivariate response
data) or an individual true response probability (univariate response data).
Although the (latent) responses are masked and only randomized responses
are observed, it is possible to compute (1) individual estimates of a sensi-
tive characteristic, (2) relationships between background variables and the
sensitive characteristic, and (3) item characteristics. It is also possible to es-
timate simultaneously respondent and item eﬀects when observing a mixture
of binary/polytomous randomized response and direct-questioning data.
When a researcher believes that distorted results will be obtained because
of the sensitive nature of the research, the randomized response methodol-
ogy reduces the distortions caused by nonresponses or inaccurate responses.
On the other hand, the randomized response technique has limitations. The
simulation studies showed that the model parameters can be accurately es-
timated given direct-questioning and/or randomized response observations.

9.9 Exercises
285
However, the randomized response technique requires larger sample sizes to
obtain parameter estimates with the same precision as those obtained via di-
rect questioning. In the unrelated-question design, there is an eﬃciency loss
due to observing responses to the unrelated question. This loss of eﬃciency
can be improved using relevant prior information. There is also additional
time needed to administer and explain the procedures to respondents. Be-
sides, tabulating and calculating statistics is more diﬃcult, which increases
the cost of using the procedure. In this area, Moshagen (2008) proposed a new
administrative procedure for multiple items by introducing a particular dis-
tribution scheme for the outcomes of a randomization device. In this design,
multiple item outcomes are masked without needing multiple randomization
processes.
Respondents in a randomized response study are to be informed about
the levels of information that can and cannot be obtained from their answers.
Most often, via instructions that are given before starting the survey, it is
said that it is impossible to know the true (latent) answers to the questions
because the outcome of the randomization device is only visible to the respon-
dent. Furthermore, it is said that it is possible that individual scores on some
latent scale can be inferred. It is important to inform respondents that the
randomized response design masks the observed item scores. This will improve
the willingness of the respondents to cooperate and provide truthful answers.
However, from an ethical point of view, it is important to inform respondents
about the level of inference that can be made. It would be interesting to inves-
tigate the inferences that respondents make based on the instructions. In this
light, the method is powerful for respondents who are fully informed about
the potential risks of participation in the survey and are still encouraged to
provide truthful answers.
9.9 Exercises
9.1. The object is to explore gender diﬀerences in responses to item 10 of the
cheating study in Section 9.7.1.
(a) Deﬁne priors for the parameters of a univariate randomized response model
with ﬁxed factor gender, and complete the implementation given in Listing
9.1.
Listing 9.1. WinBUGS code: Univariate randomized item response model.
model {
for
( i
in
1 :N)
{
Y10 [ i ]
˜ dbern( pr [ i ] )
pr [ i ] <−p1∗p [ i ] + (1−p1 )∗p2
p [ i ] <−phi ( theta [ i ] )
theta [ i ]
˜ dnorm(mu[ i ] , 1 )
mu[ i ]
<−b0 + b1∗Male [ i ]
}
}

286
9 Randomized Item Response Models
(b) Estimate the posterior mean eﬀect of being male, test whether there are
signiﬁcant gender diﬀerences, and state your conclusions.
(c) Estimate the true average success probability for the males and females.
(c) Do exercises (b) and (c) for diﬀerent (incorrect) design probabilities p1 and
p2, and evaluate the inﬂuence of using incorrect randomized response design
parameters.
9.2. Consider the randomized response data of the cheating study in Sec-
tion 9.7.1. Use a one-parameter item response model to analyze the data and
identify the model by restricting the mean of the scale.
(a) Implement the (one-parameter) randomized item response model in Win-
BUGS and estimate the parameters.
(b) Use the MLIRT program (Fox, 2007) to estimate the parameters by indi-
cating that randomized item-responses were observed.
(c) Assume that the (randomized) observations were directly observed and ﬁt
the one-parameter item response model. Explain that the items have lower
thresholds when assuming directly observed item responses.
9.3. (continuation of Exercise 9.2) Consider the outﬁt person statistic in Equa-
tion (5.40) for detecting noncompliance behavior and misﬁt of item response
patterns.
(a) Implement a posterior predictive check in WinBUGS using the outﬁt per-
son statistic at the level of the randomized item responses,
T (Yi, θi) = K−1 X
k
 
Yik −Pik (θi)
p
Pik (θi) (1 −Pik (θi))
!2
,
where Pik (θi) = p1Φ (θi −bk) + (1 −p1) p2.
(b) Using the posterior predictive check deﬁned in (a), compute the proportion
of respondents with extreme response patterns under the model using a 5%
signiﬁcance level.
(c) Implement a posterior predictive check using the outﬁt person statistic at
the level of the latent item responses,
T

˜Yi, θi

= K−1 X
k
 
˜Yik −Pik (θi)
p
Pik (θi) (1 −Pik (θi))
!2
,
with Pik (θi) = Φ (θi −bk).
(d) Using the posterior predictive check deﬁned in (c), compute the proportion
of respondents with extreme response patterns under the model using a 5%
signiﬁcance level.
(e) Compare the estimated proportions of (b) and (d), and argue that person
misﬁts are likely to be caused by noncompliance behavior.
9.4. Consider observed randomized sum scores, denoted as Y , each consist-
ing of K randomized item responses. Assume a constant individual response

9.9 Exercises
287
probability or response rate, denoted as θ, for the K binary items. The object
is to make posterior inferences about θ.
(a) Show that an observed randomized sum score is binomially distributed
with success probability
L (θ) = P (Yk = 1) = p1θ + (1 −p1) p2
when the forced randomized response technique is used.
(b) Assume a beta prior with parameters α and β for the success probability
L(θ). Show that the posterior density of L(θ) equals
p (L (θ) | y) =
Γ (α + β + K)
Γ (α + y) Γ (β + K −y)L (θ)α+y−1 (1 −L (θ))β+K−y−1 ,
which is a beta density; see Exercise 3.8(b).
(c) Show that the posterior expected response rate equals
E (θ | y) =
α + y
p1 (α + β + K) −(1 −p1) p2/p1
using L (θ) as a linear transformation function with an inverse function.
(d) Derive a 95% conﬁdence interval for θ using that
β′
α′
L (θ)
1 −L (θ) ∼F2α′,2β′
and that L (θ) is a posteriori beta distributed with parameters α′ = α+y and
β′ = β + K −y according to (b).
9.5. The CAPS data (see Sections 5.3.1 and 9.7.2) that are obtained via di-
rect questioning (DQ condition) and the randomized response technique (RR
condition) can be analyzed simultaneously.
(a) Use the MLIRT program to ﬁt an RIRT model (Equation (9.32)) with a
main eﬀect of RR treatment on the alcohol dependence and invariant CAPS
items.
(b) Investigate whether respondents of the RR group score higher on the
alcohol dependence scale than respondents of the DQ group.
(c) Assume characteristic diﬀerences of the CAPS items across the groups (RR
group and DQ group). Assume a standard normal population distribution for
alcohol dependence and only one invariant item. Use the MLIRT program to
ﬁt the RIRT model (Equation (9.32)).
(d) Compare the results of (a) and (c), and state your conclusion(s).
9.6. (continuation of Exercise 9.5) Investigate diﬀerences in alcohol depen-
dence among students using RR treatment and gender as independent ex-
planatory variables. Assume invariant CAPS items across groups.
(a) Estimate the RIRT model with RR treatment and gender (female coded as
one) as independent explanatory categorical variables. Interpret the estimated
main eﬀects.

288
9 Randomized Item Response Models
(b) Construct an interaction variable by multiplying the RR treatment vari-
able with the gender variable.
(c) Estimate the additional eﬀect of being female in the RR group conditional
on the main eﬀects of RR treatment and gender. Investigate whether there is
an RR treatment eﬀect on alcohol dependence only for females .
9.7. The observed number of randomized responses per category of the CAPS
response data are modeled, where nic refers to the number of randomized
observations in category c of respondent i over K items.
(a) Given a constant response rate of person i per category c, denoted as θic,
show that the observations are multinomially distributed such that
p (ni1, . . . , niC | θi1, . . . , θiC) =
K!
Q
c nic!
Y
c
L (θic)nic
for C response categories, where L (θic) = p1θic + (1 −p1) p2(c).
(b) Deﬁne a conjugated Dirichlet prior for L (θi),
p (L (θi) | α) = Γ (P
c αc)
Q
c Γ (αc)
Y
c
L (θi)αc−1 ,
where Γ(.) is the gamma function (Equation (2.7)). Show that the posterior
of L (θi) is also a Dirichlet distribution.
(c) Show that the posterior mean of θic equals
E (θic | α, ni) =
αc + nic
p1
P
c (αc + nic) −(1 −p1) p2(c)/p1
in a similar way as in Exercise 9.4.

References
Adams, R. J., Wilson, M., and Wu, M. (1997). Multilevel item response mod-
els: An approach to errors in variables regression. Journal of Educational
and Behavioral Statistics, 22, 47–76.
Afshartous, D. and De Leeuw, J. (2005).
Prediction in multilevel models.
Journal of Educational and Behavioral Statistics, 30, 109–139.
Aitkin, M. (1997). The calibration of p-values, posterior Bayes factors and
the AIC from the posterior distribution of the likelihood. Statistics and
Computing, 7, 253–261.
Aitkin, M. and Longford, N. (1986). Statistical modelling issues in school
eﬀectiveness studies.
Journal of the Royal Statistical Society, Series A,
149, 1–43.
Albers, W., Does, R. J. M. M., Imbos, T., and Janssen, M. P. E. (1989). A
stochastic growth model applied to repeated tests of academic knowledge.
Psychometrika, 54, 451–466.
Albert, J. H. (1992). Bayesian estimation of normal ogive item response curves
using Gibbs sampling. Journal of Educational Statistics, 17, 251–269.
Albert, J. H. and Chib, S. (1993). Bayesian analysis for binary and polychoto-
mous response data. Journal of the American Statistical Association, 88,
669–679.
Albert, J. H. and Chib, S. (1995). Bayesian residual analysis for binary re-
sponse regression models. Biometrika, 82, 747–769.
Anderson, E. B. (1980). Discrete Statistical Models with Social Science Ap-
plications. Amsterdam: North-Holland.
Anderson, T. W. (2003). An Introduction to Multivariate Statistical Analysis.
Hoboken, NJ: Wiley.
American Educational Research Association, American Psychological Associ-
ation, and National Council of Measurement in Education (2000). Standards
for Educational and Psychological Testing 1999, 2nd ed. Washington, DC:
AERA.
Baker, F. B. and Kim, S.-H. (2004). Item Response Theory: Parameter Esti-
mation Techniques, 2nd ed. New York: Marcel Dekker.

290
References
Barnard, J., McCullogh, R. E., and Meng, X.-L. (2000). Modeling covariance
matrices in terms of standard deviations and correlations, with application
to shrinkage. Statistica Sinica, 10, 1281–1311.
Bartholomew, D. J. and Knott, M. (1999). Latent Variable Models and Factor
Analysis, 2nd ed. London: Arnold.
Bayarri, M. J. and Berger, J. O. (1999). Quantifying surprise in the data.
In J. M. Bernardo, J. O. Berger, A. P. Dawid, and A. F. M. Smith (Eds.),
Bayesian Statistics 6 (pp. 53–82). New York: Oxford University Press.
Bayarri, M. J. and Berger, J. O. (2000). P values for composite null models.
Journal of the American Statistical Association, 95, 1127–1142.
Bayes, T. (1763).
An essay towards solving a problem in the doctrine of
chances. Philosophical Transactions of the Royal Society, 53, 370–418.
B´eguin, A. A. and Glas, C. A. W. (2001). MCMC estimation and some model-
ﬁt analysis of multidimensional IRT models. Psychometrika, 66, 541–561.
Berger, J. O. (1985). Statistical Decision Theory and Bayesian Analysis, 2nd
ed. New York: Springer.
Berger, J. O. and Delampady, M. (1987). Testing precise hypotheses. Statis-
tical Science, 2, 317–335.
Berger, J. O. and Pericchi, L. R. (1996). The intrinsic Bayes factor for linear
models. In J. M. Bernardo, J. O. Berger, A. P. Dawid, and A. F. M. Smith
(Eds.), Bayesian Statistics 5 (pp. 25–44). New York: Oxford University
Press.
Berger, J. O. and Selke, T. (1987). Testing a point null hypothesis: The irrec-
oncilability of P values and evidence. Journal of the American Statistical
Association, 82, 112–122.
Bernardo, J. M. and Smith, A. F. M. (1994). Bayesian Theory. New York:
Wiley.
Best, N. G., Cowles, M. K., and Vines, K. (2010). CODA: Convergence diag-
nosis and output analysis software for Gibbs sampling output, version 0.5-1
[computer software and manual]. Retrieved from http://www.mrc-bsu.
cam.ac.uk/bugs/classic/coda04/readme.shtml.
Birnbaum, A. (1969). Statistical theory for logistic mental test models with
a prior distribution of ability. Journal of Mathematical Psychology, 6, 258–
276.
Bishop, Y. M. M., Fienberg, S. E., and Holland, P. W. (1975). Discrete Multi-
variate Analysis: Theory and Practice. Cambridge, MA: The Massachusetts
Institute of Technology.
Bock, R. D. (1997).
A brief history of item response theory. Educational
Measurement: Issues and Practices, 16, 21–33.
Bock, R. D. and Aitkin, M. (1981). Marginal maximum likelihood estimation
of item parameters: Application of an EM algorithm. Psychometrika, 46,
443–459.
Bock, R. D. and Lieberman, M. (1970). Fitting a response model for n di-
chotomously scored items. Psychometrika, 35, 179–197.

References
291
B¨ockenholt, U. and van der Heijden, P. G. M. (2007).
Item randomized-
response models for measuring noncompliance: Risk-return perceptions, so-
cial inﬂuences, and self-protective responses. Psychometrika, 72, 245–262.
Boscardin, W. J. and Zhang, X. (2004). Modeling the covariance and correla-
tion matrix of repeated measures. In A. Gelman and X.-L. Meng (Eds.), Ap-
plied Bayesian Modeling and Causal Inference from Incomplete-Data Per-
spectives (pp. 215–226). New York: Wiley.
Box, G. E. P. (1980). Sampling and Bayes’ inference in scientiﬁc modelling
and robustness.
Journal of the Royal Statistical Society, Series A, 143,
383–430.
Box, G. E. P. and Tiao, G. C. (1973). Bayesian Inference in Statistical Anal-
ysis. Reading, MA: Addison-Wesley.
Bradlow, E. T., Wainer, H., and Wang, X. (1999). A Bayesian random eﬀects
model for testlets. Psychometrika, 64, 153–168.
Braun, H. I., Jones, D. H., Rubin, D. B., and Thayer, D. T. (1983). Empirical
Bayes estimation of coeﬃcients in the general linear model from data of
deﬁcient rank. Psychometrika, 48, 171–181.
Brooks, S. P. and Gelman, A. (1998). General methods for monitoring con-
vergence of iterative simulations. Journal of Computational and Graphical
Statistics, 7, 434–455.
Browne, W. J. (2006). MCMC algorithms for constrained variance matrices.
Computational Statistics and Data Analysis, 50, 1655–1677.
Carlin, B. P. and Louis, T. A. (1996). Bayes and Empirical Bayes Methods
for Data Analysis. London: Chapman and Hall.
Carroll, R. J., Ruppert, D., and Stefanski, L. A. (1995). Measurement Error
in Nonlinear Models. London: Chapman and Hall.
Casella, G. and Berger, R. L. (2002). Statistical Inference, 2nd ed. Paciﬁc
Grove, CA: Duxbury Thomson Learning.
Chaloner, K. and Brant, R. (1988). A Bayesian approach to outlier detection
and residual analysis. Biometrika, 75, 651–659.
Chen, M.-H. and Shao, Q.-M. (1999). Monte Carlo estimation of Bayesian
credible and HPD intervals. Journal of Computational and Graphical Statis-
tics, 8, 69–92.
Chen, M.-H., Shao, Q.-M., and Ibrahim, J. G. (2000). Monte Carlo Methods
in Bayesian Computation. New York: Springer.
Chib, S. and Greenberg, E. (1995). Understanding the Metropolis-Hastings
algorithm. The American Statistician, 49, 327–335.
Clark, S. J. and Desharnais, R. A. (1998). Honest answers to embarrassing
questions: Detecting cheating in the randomized response model. Psycho-
logical Methods, 3, 160–168.
Cohen, J. (1983). The cost of dichotomization. Applied Psychological Mea-
surement, 7, 249–253.
Commenges, D. and Jacqmin, H. (1994). The intraclass correlation coeﬃcient:
Distribution-free deﬁnition and test. Biometrics, 50, 517–526.
Congdon, P. (2001). Bayesian Statistical Modelling. Chichester: Wiley.

292
References
Cowles, M. K. (1996). Accelerating Monte Carlo Markov Chain convergence
for cumulative-link generalized linear models. Statistics and Computing, 6,
101–111.
Cowles, M. K. and Carlin, B. P. (1996). Markov chain Monte Carlo conver-
gence diagnostics: A comparative review. Jounal of the American Statistical
Association, 91, 883–904.
Cruyﬀ, M. J. L. F., van den Hout, A., van der Heijden, P. G. M., and
B¨ockenholt, U. (2007). Log-linear randomized-response models taking self-
protective response behavior into account. Sociological Methods and Re-
search, 36, 266–282.
Davis, S. F., Grover, C. A., Becker, A. H., and McGregor, L. N. (1992). Aca-
demic dishonesty: Prevalence, determinants, techniques, and punishments.
Teaching of Psychology, 19, 16–20.
De Boeck, P. and Wilson, M. (2004). Explanatory Item Response Models: A
Generalized Linear and Nonlinear Approach. New York: Springer.
De Jong, M. G., Pieters, R., and Fox, J.-P. (2010). Reducing social desir-
ability bias through item randomized response: An application to measure
underreported desires. Journal of Marketing Research, 47, 14–27.
De Jong, M. G. and Steenkamp, J. B. E. M. (2009). Finite mixture multilevel
multidimensional ordinal IRT models for large scale cross-cultural research.
Psychometrika, (online).
De Jong, M. G., Steenkamp, J. B. E. M., and Fox, J.-P. (2007). Relaxing cross-
national measurement invariance using a hierarchical IRT model. Journal
of Consumer Research, 34, 260–278.
De Jong, M. G., Steenkamp, J. B. E. M., Fox, J.-P., and Baumgartner, H.
(2008). Using item response theory to measure extreme response style in
marketing research: A global investigation. Journal of Marketing Research,
45, 104–115.
De Leeuw, J. and Kreft, I. G. G. (1986).
Random coeﬃcient models for
multilevel analysis. Journal of Educational and Behavioral Statistics, 11,
57–85.
DeGroot, M. H. (1970). Optimal Statistical Decisions. New York: Wiley.
DeIorio, M. and Robert, C. P. (2002). Discussion of Spiegelhalter et al. Journal
of the Royal Statistical Society, Series B, 64, 629–630.
Dempster, A. P., Laird, N. M., and Rubin, D. B. (1977). Maximum likelihood
from incomplete data via the EM algorithm (with discussion). Journal of
the Royal Statistical Society, Series B, 39, 1–38.
Dempster, A. P., Rubin, D. B., and Tsutakawa, R. K. (1981). Estimation in
covariance components models. Journal of the American Statistical Asso-
ciation, 76, 341–353.
Dickey, J. (1971). The weighted likelihood ratio, linear hypotheses on normal
location parameters. The Annals of Statistics, 42, 204–223.
Diebolt, J. and Robert, C. P. (1994). Estimation of ﬁnite mixture distributions
through Bayesian sampling. Journal of the Royal Statistical Society, Series
B, 56, 363–375.

References
293
Donders, F. C. (1868). Over de snelheid van psychische processen [On the
speed of mental processes]. Onderzoekingen gedaan in het Physiologisch
Laboratorium der Utrechtsche Hoogeschool, 1868–1869, Tweede reeks, II,
92–120.
Doolaard, S. (1999). Schools in Change or Schools in Chains? PhD disserta-
tion, University of Twente.
Edgell, S. E., Himmelfarb, S., and Duchan, K. L. (1982). Validity of forced
responses in a randomized response model. Sociological Methods and Re-
search, 11, 89–100.
Edwards, A. W. F. (1963). The measure of association in a 2x2 table. Journal
of the Royal Statistical Society, Series A, 126, 109–114.
Efron, B. and Morris, C. (1975). Data analysis using Stein’s estimator and
its generalization. Journal of the American Statistical Association, 70, 311–
319.
Embretson, S. E. and Reise, S. P. (2000). Item Response Theory for Psychol-
ogists. Mahwah, NJ: Lawrence Erlbaum.
Emons, W. H. M., Sijtsma, K., and Meijer, R. R. (2005). Global, local, and
graphical person-ﬁt analysis using person-response functions. Psychological
Methods, 10, 101–119.
Fahrmeir, L. and Tutz, G. (2001). Multivariate Statistical Modelling Based on
Generalized Linear Models, 2nd ed. New York: Springer.
Folstein, M. F., Folstein, S. E., and McHugh, P. R. (1975). Mini-mental/state:
A practical method for grading the cognitive state of patients for the clini-
cian. Journal of Psychiatric Research, 12, 189–198.
Fox, J. A. and Tracy, P. E. (1986).
Randomized Response: A Method for
Sensitive Surveys. Beverly Hills, CA: Sage.
Fox, J.-P. (2001). Multilevel IRT: A Bayesian Perspective on Estimating Pa-
rameters and Testing Statistical Hypotheses. PhD dissertation, University
of Twente, Faculty of Behavioural Sciences.
Fox, J.-P. (2003). Stochastic EM for estimating the parameters of a multilevel
IRT model. British Journal of Mathematical and Statistical Psychology, 56,
65–81.
Fox, J.-P. (2004). Applications of multilevel IRT modeling. School Eﬀective-
ness and School Improvement, 15, 261–280.
Fox, J.-P. (2005a). Multilevel IRT model assessment. In A. van der Ark,
M. A. Croon, and K. Sijtsma (Eds.), New Developments in Categorical Data
Analysis for the Social and Behavioral Sciences (pp. 227–252). Mahwah, NJ:
Lawrence Erlbaum.
Fox, J.-P. (2005b). Multilevel IRT using dichotomous and polytomous items.
British Journal of Mathematical and Statistical Psychology, 58, 145–172.
Fox, J.-P. (2005c).
Randomized item response theory models.
Journal of
Educational and Behavioral Statistics, 30, 189–212.
Fox, J.-P. (2007). Multilevel IRT modeling in practice. Journal of Statistical
Software, 20, Issue 5.

294
References
Fox, J.-P. and Glas, C. A. W. (2001). Bayesian estimation of a multilevel IRT
model using Gibbs sampling. Psychometrika, 66, 271–288.
Fox, J.-P. and Glas, C. A. W. (2003). Bayesian modeling of measurement
error in predictor variables using item response theory. Psychometrika, 68,
169–191.
Fox, J.-P., Klein Entink, R. E., and van der Linden, W. J. (2007). Modeling
of responses and response times with the package cirt. Journal of Statistical
Software, 20, Issue 7.
Fox, J.-P. and Meijer, R. R. (2008). Using item response theory to obtain in-
dividual information from randomized response data: An application using
cheating data. Journal of Applied Psychological Measurement, 32, 595–610.
Fox, J.-P. and Wyrick, C. (2008). A mixed eﬀects randomized item response
model. Journal of Educational and Behavioral Statistics, 33, 389–415.
Fuller, W. A. (1987). Measurement Error Models. New York: Wiley.
Fuller, W. A. (1991). Regression estimation in the presence of measurement
error. In P. P. Biemer, R. M. Groves, L. E. Lyberg, and N. A. Mathiowetz
(Eds.), Measurement Errors in Surveys (pp. 617–635). New York: Wiley.
Geisser, S. (1975). The predictive sample reuse method with applications.
Journal of the American Statistical Association, 70, 320–328.
Gelfand, A. E. and Dey, D. K. (1994). Bayesian model choice: Asymptotics
and exact calculations. Journal of the Royal Statistical Society, Series B,
56, 501–514.
Gelfand, A. E., Dey, D. K., and Chang, H. (1992). Model determination using
predictive distributions with implementation via sampling based methods
(with discussion).
In J. M. Bernardo, J. O. Berger, A. P. Dawid, and
A. F. M. Smith (Eds.), Bayesian Statistics 4 (pp. 147–167). Oxford: Oxford
University Press.
Gelfand, A. E. and Smith, A. F. M. (1990). Sampling-based approaches to
calculating marginal densities. Journal of the American Statistical Associ-
ation, 85, 398–409.
Gelman, A. (1995). Inference and monitoring convergence. In W. R. Gilks,
S. Richardson, and D. J. Spiegelhalter (Eds.), Markov Chain Monte Carlo
in Practice (pp. 131–143). London: Chapman and Hall.
Gelman, A. (2008). Objections to Bayesian statistics. Bayesian Analysis, 3,
445–450.
Gelman, A., Carlin, J. B., Stern, H. S., and Rubin, D. B. (1995). Bayesian
Data Analysis. London: Chapman and Hall.
Gelman, A. and Hill, J. (2007). Data Analysis Using Regression and Multi-
level/Hierarchical Models. Cambridge: Cambridge University Press.
Gelman, A. and King, D. G. (1990). Estimating the electoral consequences of
legislative redirecting. Journal of the American Statistical Association, 85,
274–282.
Gelman, A., Meng, X.-L., and Stern, H. S. (1996). Posterior predictive as-
sessment of model ﬁtness via realized discrepancies. Statistica Sinica, 6,
733–807.

References
295
Gelman, A. and Rubin, D. B. (1992). Inference from iterative simulation using
multiple sequences. Statistical Science, 7, 457–472.
Geman, S. and Geman, D. (1984). Stochastic relaxation, Gibbs distribution,
and the Bayesian restoration of images.
IEEE Transactions on Pattern
Analysis and Machine Intelligence, 6, 721–741.
Geweke, J. (1992). Evaluating the accuracy of sampling-based approaches
to calculating posterior moments. In J. M. Bernardo, J. O. Berger, A. P.
Dawid, and A. F. M. Smith (Eds.), Bayesian Statistics 4 (pp. 169–193).
Oxford: Oxford University Press.
Geweke, J. (2005).
Contemporary Bayesian Econometrics and Statistics.
Hoboken, NJ: Wiley.
Geyer, C. J. (1992). Practical Markov chain Monte Carlo. Statistical Science,
4, 473–483.
Ghosh, M. (1995). Inconsistent maximum likelihood estimators for the Rasch
model. Statistics and Probability Letters, 23, 165–170.
Ghosh, M., Ghosh, A., Chen, M.-H., and Agresti, A. (2000). Noninforma-
tive priors for one-parameter item response models. Journal of Statistical
Planning and Inference, 88, 99–115.
Gilks, W. R., Richardson, S., and Spiegelhalter, D. J. (1995). Markov Chain
Monte Carlo in Practice. London: Chapman and Hall.
Glas, C. A. W. and Meijer, R. R. (2003). A Bayesian approach to person ﬁt
analysis in item response theory models. Applied Psychological Measure-
ment, 27, 217–233.
Glas, C. A. W. and van der Linden, W. J. (2003). Computerized adaptive
testing with item cloning. Applied Psychological Measurement, 27, 247–261.
Goldstein, H. (2003). Multilevel Statistical Models, 3rd ed. London: Hodder
Arnold.
Goldstein, H. (2004). International comparisons of student attainment: Some
issues arising from the PISA study. Assessment in Education, 11, 319–330.
Goldstein, H., Bonnet, G., and Rocher, T. (2007). Multilevel structural equa-
tion models for the analysis of comparative data on educational perfor-
mance. Journal of Educational and Behavioral Statistics, 32, 252–286.
Goldstein, H., Rasbash, J., Plewis, I., Draper, D., Browne, W., Yang, M.,
Woodhouse, G., and Healy, M. (1998). A User’s Guide to MLwiN. London:
Multilevel Models Project, Institute of Education.
Goodman, L. A. (1974).
Exploratory latent structure analysis using both
identiﬁable and unidentiﬁable models. Biometrika, 61, 215–231.
Greenberg, B. G., Abul-Ela, A., Simmons, W. R., and Horvitz, D. G. (1969).
The unrelated question randomized response model: Theoretical framework.
The American Statistician, 64, 520–539.
Gulliksen, H. O. (1950). Theory of Mental Tests. New York: Wiley.
Guttman, I. (1967). The use of the concept of a future observation in goodness-
of-ﬁt problems. Journal of the Royal Statistical Society, Series B, 29, 83–
100.

296
References
Hall, D. B. and Clutter, M. (2004). Multivariate multilevel nonlinear mixed
eﬀects models for timber yield predictions. Biometrics, 60, 16–24.
Hambleton, R. K. and Swaminathan, H. (1985). Item Response Theory: Prin-
ciples and Applications. Boston: Kluwer NijhoﬀPublishing.
Hambleton, R. K., Swaminathan, H., and Rogers, H. J. (1991). Fundamentals
of Item Response Theory. London: Sage.
Harville, D. A. (1977). Maximum likelihood approaches to variance compo-
nent estimation and related problems. Journal of the American Statistical
Association, 72, 320–340.
Hastings, W. K. (1970). Monte Carlo sampling methods using Markov chains
and their applications. Biometrika, 57, 97–109.
Hedeker, D. R. (1999). MIXNO: A computer program for mixed-eﬀects nom-
inal logistic regression. Journal of Statistical Software, 4, 1–92.
Hedeker, D. R. and Gibbons, R. D. (1996). MIXOR: A computer program
for mixed-eﬀects ordinal probit and logistic regression analysis. Computer
Methods and Programs in Biomedicine, 49, 157–176.
Hedeker, D. R. and Gibbons, R. D. (2006). Longitudinal Data Analysis. Hobo-
ken, NJ: Wiley.
Heidelberg, P. and Welch, P. (1983). Simulation run length control in the
presence of an initial transient. Operations Research, 31, 1109–1144.
Higdon, D. M. (1998). Auxiliary variable methods for Markov Chain Monte
Carlo with applications. Journal of the American Statistical Society, 93,
585–595.
Hill, B. M. (1965). Inference about variance components in the one-way model.
Journal of the American Statistical Society, 60, 806–825.
Hoijtink, H. (2001). Conditional independence and diﬀerential item function-
ing in the two-parameter logistic model. In A. Boomsma, M. A. J. van
Duijn, and T. A. B. Snijders (Eds.), Essays on Item Response Theory (pp.
109–129). New York: Springer.
Holland, P. W. (1990). On the sampling theory foundations of item response
theory models. Psychometrika, 55, 577–601.
Holland, P. W. and Wainer, H. (1993). Diﬀerential Item Functioning. Hills-
dale, NJ: Erlbaum.
Janssen, R., Schepers, J., and Peres, D. (2004). Models with item and item
group predictors. In P. de Boeck and M. Wilson (Eds.), Explanatory Item
Response Models: A Generalized Linear and Nonlinear Approach (pp. 189–
212). New York: Springer.
Janssen, R., Tuerlinckx, F., Meulders, M., and De Boeck, P. (2000). A hier-
archical IRT model for criterion-referenced measurement. Journal of Edu-
cational and Behavioral Statistics, 25, 285–306.
Jeﬀreys, H. J. (1961). Theory of Probability. New York: Oxford University
Press.
Johnson, V. E. and Albert, J. H. (1999). Ordinal Data Modeling. New York:
Springer.

References
297
Kamata, A. (2001). Item analysis by the hierarchical generalized linear model.
Journal of Educational Measurement, 38, 79–93.
Karim, M. R. and Zeger, S. L. (1992). Generalized linear models with random
eﬀects; salamander mating revisited. Biometrics, 48, 631–644.
Kass, R. E. and Raftery, A. E. (1995). Bayes factors. Journal of the American
Statistical Association, 90, 773–795.
Kim, S.-H. (2001). An evaluation of a Markov chain Monte Carlo method for
the Rasch model. Applied Psychological Measurement, 25, 163–176.
Kim, S.-H., Cohen, A. S., Baker, F. B., Subkoviak, M. J., and Leonard, T.
(1994). An investigation of hierarchical Bayes procedures in item response
theory. Psychometrika, 59, 405–421.
Klein Entink, R. H. (2009). Statistical Models for Responses and Response
Times.
PhD dissertation, University of Twente, Faculty of Behavioural
Sciences.
Klein Entink, R. H., Fox, J.-P., and van der Linden, W. J. (2009a). A multi-
variate multilevel approach to the modeling of accuracy and speed of test
takers. Psychometrika, 74, 21–48.
Klein Entink, R. H., Kuhn, J.-T., Hornke, L. F., and Fox, J.-P. (2009b).
Evaluating cognitive theory: A joint modeling approach using responses
and response times. Psychological Methods, 14, 54–75.
Kuha, J. (1997).
Estimation by data augmentation in regression models
with continuous and discrete covariates measured with error.
Statistics
in Medicine, 16, 189–201.
Laird, N. M. and Ware, J. H. (1982). Random-eﬀects models for longitudinal
data. Biometrics, 38, 963–974.
Lazarsfeld, P. F. and Henry, N. W. (1968). Latent Structure Analysis. New
York: Houghton Miﬄin.
Lee, P. M. (2004). Bayesian Statistics: An Introduction, 3rd ed. New York:
Wiley.
Lee, S.-Y. and Zhu, H.-T. (2000). Statistical analysis of non-linear equation
models with continuous and polytomous data. British Journal of Mathe-
matical and Statistical Psychology, 53, 209–232.
Lehmann, E. L. and Casella, G. (2003). Theory of Point Estimation, 2nd ed.
New York: Springer.
Lensvelt-Mulders, G. J. L. M., Hox, J. J., van der Heijden, P. G. M., and Maas,
C. J. M. (2005). Meta-analysis of randomized response research: Thirty-ﬁve
years of validation. Sociological Methods and Research, 33, 319–348.
Leonard, T. and Hsu, J. S. J. (1999). Bayesian Methods: An Analysis for
Statisticians and Interdisciplinary Researchers.
Cambridge: Cambridge
University Press.
Levy, R. (2006). Posterior Predictive Model Checking for Multidimensional-
ity in Item Response Theory and Bayesian Networks. PhD dissertation,
University of Maryland.

298
References
Lindley, D. V. (1965). An Introduction to Probability and Statistics from a
Bayesian Viewpoint (Parts 1 and 2). Cambridge: Cambridge University
Press.
Lindley, D. V. and Smith, A. F. M. (1972). Bayes estimates for the linear
model. Journal of the Royal Statistical Society, Series B, 34, 1–41.
Little, R. J. A. and Rubin, D. A. (2002). Statistical Analysis with Missing
Data, 2nd ed. Hoboken, NJ: Wiley.
Liu, J. S., Wong, W. H., and Kong, A. (1994). Covariance structure of the
Gibbs sampler with applications to the comparisons of estimators and aug-
mentation schemes. Biometrika, 81, 27–40.
Liu, L. C. and Hedeker, D. (2006).
A mixed-eﬀects regression model for
longitudinal multivariate ordinal data. Biometrics, 62, 261–268.
Longford, N. T. (1987).
A fast scoring algorithm for maximum likeli-
hood estimation in unbalanced mixed models with nested random eﬀects.
Biometrika, 74, 817–827.
Longford, N. T. (1993). Random Coeﬃcient Models. New York: Oxford Uni-
versity Press.
Lord, F. M. (1980). Applications of Item Response Theory to Practical Testing
Problems. Hillsdale, NJ: Lawrence Erlbaum.
Lord, F. M. (1986). Maximum likelihood and Bayesian parameter estimation
in item response theory. Journal of Educational Measurement, 23, 157–162.
Lord, F. M. and Novick, M. R. (1968). Statistical Theories of Mental Test
Scores. Reading, MA: Addison-Wesley.
Luce, R. D. (1986).
Response Times: Their Role in Inferring Elementary
Mental Organization. New York: Oxford University Press.
Lunn, D., Thomas, A., Best, N., and Spiegelhalter, D. (2000). WinBUGS
– a Bayesian modelling framework: Concepts, structure, and extensibility
[computer software]. Statistics and Computing, 10, 325–337.
MacEachern, S. N. and Berliner, L. M. (1994). Subsampling the Gibbs sam-
pler. The American Statistician, 48, 188–190.
Maddala, G. S. (1983).
Limited-Dependent and Qualitative Variables in
Econometrics. Cambridge: Cambridge University Press.
Maier, K. S. (2001). A Rasch hierarchical measurement model. Journal of
Educational and Behavioral Statistics, 26, 307–330.
Maris, E. (1993). Additive and multiplicative models for gamma distributed
random variables, and their application as psychometric models for response
times. Psychometrika, 58, 445–469.
Maris, G. and Maris, E. (2002). A MCMC-method for models with continuous
latent responses. Psychometrika, 67, 335–350.
Masters, G. M. (1982). A Rasch model for partial credit scoring. Psychome-
trika, 47, 149–174.
Masters, G. N. and Wright, B. D. (1997). The partial credit model. In W. J.
van der Linden and R. K. Hambleton (Eds.), Handbook of Modern Item
Response Theory (pp. 101–121). New York: Springer.

References
299
McCullagh, P. and Nelder, J. A. (1989). Generalized Linear Models, 2nd ed.
New York: Chapman and Hall.
McCulloch, R. E., Polson, N. G., and Rossi, P. E. (2000). A Bayesian analysis
of the multinomial probit model with fully identiﬁed parameters. Journal
of Econometrics, 99, 173–193.
McDonald, R. P. (1967). Nonlinear Factor Analysis (Psychometric Society
Monograph No. 15). Richmond, VA: William Byrd Press.
McLachlan, G. J. and Peel, D. (2000). Finite Mixture Models. New York:
Wiley.
Meijer, R. R. (2003). Diagnosing item score patterns on a test using item
response theory-based person-ﬁt statistics. Psychological Methods, 8, 72–
87.
Meijer, R. R. and Sijtsma, K. (2001). Methodology review: Evaluating person
ﬁt. Applied Psychological Measurement, 25, 107–135.
Mellenbergh, G. J. (1989). Item bias and item response theory. International
Journal of Educational Research, 13, 127–143.
Mellenbergh, G. J. (1994a). Generalized linear item response theory. Psycho-
logical Bulletin, 115, 300–307.
Mellenbergh, G. J. (1994b). A unidimensional latent trait model for continu-
ous item responses. Multivariate Behavioral Research, 29, 223–236.
Meng, X.-L. (1994). Posterior predictive p-values. The Annals of Statistics,
22, 1142–1160.
Meng, X.-L. and van Dyk, D. A. (1999). Seeking eﬃcient data augmentation
schemes via conditional and marginal augmentation. Biometrika, 86, 301–
320.
Meng, X.-L. and Wong, W. H. (1996). Simulating ratios of normalizing con-
stants via a simple identity: A theoretical exploration. Statistica Sinica, 6,
831–860.
Meredith, W. and Millsap, R. E. (1992). On the misuse of manifest variables
in the detection of measurement bias. Psychometrika, 57, 289–311.
Metropolis, N., Rosenbluth, A. W., Rosenbluth, M. N., Teller, A. H., and
Teller, E. (1953). Equations of state calculations by fast computing ma-
chines. The Journal of Chemical Physics, 21, 1087–1092.
Mislevy, R. J. (1984). Estimating latent distributions. Psychometrika, 49,
359–381.
Mislevy, R. J. (1986). Bayes model estimation in item response models. Psy-
chometrika, 51, 177–195.
Mislevy, R. J. and Sheehan, K. M. (1989). The role of collateral information
about examinees in item parameter estimation. Psychometrika, 54, 661–
679.
Molenaar, I. W. (1995). Some background for item response theory and the
Rasch model. In G. H. Fisher and I. W. Molenaar (Eds.), Rasch Models:
Foundations, Recent Developments and Applications (pp. 3–14). New York:
Springer.

300
References
Molenaar, I. W. and Hoijtink, H. (1990).
The many null distributions of
person ﬁt indices. Psychometrika, 55, 75–106.
Morris, C. N. (1983). Parametric empirical Bayes inference: Theory and ap-
plications. Journal of the American Statistical Association, 78, 47–55.
Moshagen, M. (2008). Multinomial Randomized Response Models. Phd dis-
sertation, Heinrich-Heine-Universit¨at Dusseld¨orf, Mathematisch Naturwis-
senschaftlichen Fakult¨at.
Muraki, E. (1992). A generalized partial credit model: Application of an EM
algorithm. Applied Psychological Measurement, 16, 159–176.
Muraki, E. (1993).
Information functions of the generalized partial credit
model. Applied Psychological Measurement, 17, 351–363.
Muraki, E. and Bock, R. D. (1997). PARSCALE: IRT Based Test Scoring
and Item Analysis for Graded Items and Rating Scales [computer software].
Chicago, IL: Scientiﬁc Software International.
Muth´en, B. O. (1992). Latent variable modeling in epidemiology. Alcohol
Health and Research World, 16, 286–292.
Muth´en, B. O. (2001).
Latent variable mixture modeling.
In G. A. Mar-
coulides and R. E. Schumacker (Eds.), New Developments and Techniques
in Structural Equation Modeling (pp. 1–33). New York: Lawrence Erlbaum
Associates.
Muth´en, B. O. and Shedden, K. (1999). Finite mixture modeling with mixture
outcomes using the EM algorithm. Biometrics, 55, 463–469.
Muth´en, L. K. and Muth´en, B. O. (1998). Mplus: The Comprehensive Mod-
eling Program for Applied Researchers [computer software]. Los Angeles,
CA: Muth´en and Muth´en.
Nandram, B. and Chen, M.-H. (1996). Reparameterizing the generalized lin-
ear model to accelerate Gibbs sampler convergence. Journal of Statistical
Computation and Simulation, 54, 129–144.
Neal, R. M. (1997). Markov Chain Monte Carlo methods based on ‘slicing’
the density function. Technical Report No. 9722, University of Toronto,
Department of Statistics.
Newton, M. A. and Raftery, A. E. (1994). Approximate Bayesian inference
with the weighted likelihood bootstrap.
Journal of the Royal Statistical
Society, Series B, 56, 3–48.
Neyman, J. and Scott, E. L. (1948). Consistent estimates based on partially
consistent observations. Econometrika, 16, 1–32.
Novick, M. R., Lewis, C., and Jackson, P. H. (1973). The estimation of pro-
portions in m groups. Psychometrika, 38, 19–46.
Ntzoufras, I. (2009).
Bayesian Modeling Using WinBUGS.
Hoboken, NJ:
Wiley.
OECD (Organisation for Economic Co-operation and Development) (2004).
Learning From Tomorrow’s World. First Results from PISA 2003. Paris:
OECD.
O’Hagan, A. (1976). On posterior joint and marginal modes. Biometrika, 63,
329–333.

References
301
O’Hagan, A. (1995). Fractional Bayes factors for model comparison (with
discussion). Journal of the Royal Statistical Society, Series B, 57, 99–138.
O’Hare, T. M. (1997). Measuring problem drinkers in ﬁrst time oﬀenders:
Development and validation of the college alcohol problem scale (CAPS).
Journal of Substance Abuse Treatment, 14, 383–387.
Orlando, M. and Thissen, D. (2000).
Likelihood-based item-ﬁt indices for
dichotomous response theory models. Applied Psychological Measurement,
24, 50–64.
Orlando, M. and Thissen, D. (2003). Further investigation of the performance
of s −x2: An item ﬁt index for use with dichotomous item response theory
models. Applied Psychological Measurement, 27, 289–298.
Ostini, R. and Nering, M. L. (2006). Polytomous Item Response Theory Mod-
els. Thousand Oaks, CA: Sage.
Owen, R. (1975). A Bayesian sequential procedure for quantal response in
the context of adaptive mental testing. Journal of the American Statistical
Association, 70, 351–356.
Patz, R. J. and Junker, B. W. (1999a). Applications and extensions of MCMC
in IRT: Multiple item types, missing data, and rated responses. Journal of
Educational and Behavioral Statistics, 24, 342–366.
Patz, R. J. and Junker, B. W. (1999b). A straightforward approach to Markov
chain Monte Carlo methods for item response models. Journal of Educa-
tional and Behavioral Statistics, 24, 146–178.
Patz, R. J., Junker, B. W., Johnson, M. S., and Mariano, L. T. (2002). The
hierarchical rater model for rated test items and its application to large-
scale educational assessment data. Journal of Educational and Behavioral
Statistics, 27, 341–384.
Pinheiro, J. C. and Bates, D. M. (2000). Mixed-Eﬀects Models in S and S-Plus.
New York: Springer.
Press, S. J. (2003). Subjective and Objective Bayesian Statistics. Hoboken,
NJ: Wiley.
R Development Core Team (2010).
R: A Language and Environment for
Statistical Computing. Vienna: R Foundation for Statistical Computing.
Rabe-Hesketh, S. and Skrondal, A. (2001). Parameterization of multivariate
random eﬀects models for categorical data. Biometrics, 57, 1256–1264.
Raftery, A. E. (1995). Bayesian model selection in social research. Social
Methodology, 25, 111–163.
Raftery, A. L. and Lewis, S. (1992). Comment: One long run with diagnos-
tics: Implementation strategies for Markov chain Monte Carlo. Statistical
Science, 7, 493–497.
Rasch, G. (1960). Probabilistic Models for some Intelligence Tests and At-
tainment Tests. Copenhagen: Danish Institute for Educational Research.
Raudenbush, S. W. and Bryk, A. S. (2002). Hierarchical Linear Models: Ap-
plications and Data Analysis Methods, 2nd ed. Thousand Oaks, CA: Sage.

302
References
Raudenbush, S. W., Bryk, A. S., Cheong, Y. F., and Congdon, R. T. (2000).
HLM 5: Hierarchical Linear and Nonlinear Modeling.
Lincolnwood, IL:
Scientiﬁc Software International.
Raudenbush, S. W. and Sampson, R. J. (1999). Ecometrics: Toward a science
of assessing ecological settings, with application to the systematic social
observation of neighborhoods. Sociological Methodology, 29, 1–41.
Reckase, M. D. (1985). The diﬃculty of test items that measure more than
one ability. Applied Psychological Measurement, 9, 401–412.
Reckase, M. D. (1997). The past and future of multidimensional item response
theory. Applied Psychological Measurement, 21, 25–36.
Reinsel, G. (1982). Multivariate repeated-measurement or growth curve mod-
els with multivariate random-eﬀects covariance structure. Journal of the
American Statistical Association, 77, 190–195.
Reinsel, G. (1983). Some results on multivariate autoregressive index models.
Biometrika, 70, 145–156.
Richman, W. L., Kiesler, S., Weisband, S., and Drasgow, F. (1999). A meta-
analytic study of social desirability distortion in computer-administered
questionnaires, traditional questionnaires, and interviews. Journal of Ap-
plied Psychology, 84, 754–775.
Rigdon, S. E. and Tsutakawa, R. K. (1983). Parameter estimation in latent
trait models. Psychometrika, 48, 567–574.
Rijmen, F., Tuerlinckx, F., De Boeck, P., and Kuppens, P. (2003). A nonlinear
mixed model framework for item response theory. Psychological Methods,
8, 185–205.
Ripley, B. D. (1987). Stochastic Simulation. New York: Wiley.
Robert, C. P. and Casella, G. (1999). Monte Carlo Statistical Methods. New
York: Springer.
Roberts, G. O. (1995). Markov chain concepts related to sampling algorithms.
In W. R. Gilks, S. Richardson, and D. J. Spiegelhalter (Eds.), Markov Chain
Monte Carlo in Practice (pp. 45–57). London: Chapman and Hall.
Roberts, G. O. and Tweedie, R. L. (1999). Bounds on regeneration times
and convergence rates for Markov chains. Stochastic Processes and Their
Applications, 80, 211–229; Correction 91, 337–338.
Robins, J. M., van der Vaart, A. W., and Ventura, V. (2000). Asymptotic
distribution of P values in composite null models: Rejoinder. Journal of the
American Statistical Association, 95, 1171–1172.
Rosenbaum, P. R. (1988). Items bundles. Psychometrika, 53, 349–359.
Rosenthal, J. S. (1995). Minorization conditions and convergence rates for
Markov chain Monte Carlo. Journal of the American Statistical Association,
90, 558–566; Correction 91, 1136.
Roskam, E. E. (1997). Models for speed and time-limit tests. In W. J. van der
Linden and R. K. Hambleton (Eds.), Handbook of Modern Item Response
Theory (pp. 187–208). New York: Springer.
Rossi, P. E., Allenby, G. M., and McCullogh, R. E. (2005). Bayesian Statistics
and Marketing. Chichester: Wiley.

References
303
Rost, J. (1990). Rasch models in latent classes: An integration of two ap-
proaches to item analysis. Applied Psychological Measurement, 14, 271–282.
Rost, J. and von Davier, M. (1995).
Mixture distribution Rasch models.
In G. Fischer and I. Molenaar (Eds.), Rasch Models: Foundations, Recent
Developments and Applications (pp. 257–268). New York: Springer.
Rubin, D. B. (1976). Inference and missing data. Biometrika, 63, 581–592.
Rubin, D. B. (1984). Bayesianly justiﬁable and relevant frequency calculations
for the applied statistician. The Annals of Statistics, 12, 1151–1172.
Rubin, D. B. (1987). Multiple Imputation for Nonresponse in Surveys. New
York: Wiley.
Rupp, A. A., Dey, D. K., and Zumbo, B. D. (2004). To Bayes or not to Bayes,
from whether to when: Applications of Bayesian methodology to modeling.
Structural Equation Modeling, 11, 424–451.
Sahu, S. K. (2002). Bayesian estimation and model choice in item response
models. Journal of Statistical Computation and Simulation, 72, 217–232.
Samejima, F. (1997). The graded response model. In W. J. van der Linden
and R. K. Hambleton (Eds.), Handbook of Modern Item Response Theory
(pp. 85–100). New York: Springer.
Schafer, J. L. (1997).
Analysis of Incomplete Multivariate Data.
London:
Chapman and Hall.
Schafer, J. L. and Yucel, R. M. (2002). Computational strategies for multi-
variate linear mixed-eﬀects models with missing values. Journal of Compu-
tational and Graphical Statistics, 11, 437–457.
Scheerens, J. (1992). Eﬀective Schooling: Research, Theory and Practice. Lon-
don: Cassell.
Scheerens, J., Glas, C. A. W., and Thomas, S. M. (2003). Educational Eval-
uation, Assessment, and Monitoring. Lisse: Swets and Zeitlinger.
Scheers, N. J. and Dayton, C. (1988). Covariate randomized response model.
Journal of the American Statistical Association, 83, 969–974.
Scheiblechner, H. (1979). Speciﬁcally objective stochastic latency mechanisms.
Journal of Mathematical Psychology, 19, 18–38.
Schnipke, D. L. and Scrams, D. J. (1997).
Modeling item response times
with a two-state mixture model: A new method for measuring speededness.
Journal of Educational Measurement, 34, 213–232.
Schnipke, D. L. and Scrams, D. J. (2002). Exploring issues of examinee be-
havior: Insights gained from response-time analyses. In C. N. Mills, M. T.
Potenza, J. J. Fremer, and W. C. Ward (Eds.), Computer-Based Testing:
Building the Foundation for Future Assessments (pp. 237–266). Mahwah,
NJ: Lawrence Erlbaum.
Schulz-Larsen, K., Kreiner, S., and Lomholt, R. K. (2007a).
Mini-mental
status examination: A short form of MMSE was as accurate in predicting
dementia. Journal of Clinical Epidemiology, 60, 260–267.
Schulz-Larsen, K., Kreiner, S., and Lomholt, R. K. (2007b).
Mini-mental
status examination: Mixed Rasch model item analysis derived two diﬀerent

304
References
cognitive dimensions of the MMSE. Journal of Clinical Epidemiology, 60,
268–279.
Schwarz, G. (1978). Estimating the dimension of a model. The Annals of
Statistics, 6, 461–464.
Searle, S. R., Casella, G., and McCulloch, C. E. (1992). Variance Components.
New York: Wiley.
Shalabi, F. (2002). Eﬀective Schooling in the West Bank. PhD dissertation,
University of Twente.
Shi, J.-Q. and Lee, S.-Y. (1998). Bayesian sampling-based approach for factor
analysis models with continuous and polytomous data. British Journal of
Mathematical and Statistical Psychology, 51, 233–252.
Sinharay, S. (2005).
Assessing ﬁt of unidimensional item response theory
models using a Bayesian approach. Journal of Educational Measurement,
42, 375–394.
Sinharay, S. (2006). Bayesian item ﬁt analysis for unidimensional item re-
sponse theory models. British Journal of Mathematical and Statistical Psy-
chology, 59, 429–449.
Sinharay, S., Johnson, M. S., and Stern, H. S. (2006). Posterior predictive
assessment of item response theory models. Applied Psychological Measure-
ment, 30, 298–321.
Sinharay, S. and Stern, H. S. (2002). On the sensitivity of Bayes factors to
the prior distributions. The American Statistician, 56, 196–201.
Skrondal, A. and Rabe-Hesketh, S. (2004). Generalized Latent Variable Mod-
eling: Multilevel, Longitudinal, and Structural Equation Models. London:
Chapman and Hall.
Smith, B. (2010).
BOA: Bayesian Output Analysis Program, version
1.1.5 [computer software and manual].
Retrieved from http://www.
public-health.uiowa.edu/boa/.
Snijders, T. A. B. (2001). Asymptotic null distribution of person ﬁt statistics
with estimated person parameter. Psychometrika, 66, 331–342.
Snijders, T. A. B. and Bosker, R. J. (1999). Multilevel Analysis: An Introduc-
tion to Basic and Advanced Multilevel Modeling. London: Sage.
Soares, T. M., Gon¸calves, F. B., and Gamerman, D. (2009). An integrated
Bayesian model for DIF analysis. Journal of Educational and Behavioral
Statistics, 34, 348–377.
Song, X.-Y. and Lee, S.-Y. (2001). Bayesian estimation and test for factor
analysis model with continuous and polytomous data in several populations.
British Journal of Mathematical and Statistical Psychology, 54, 237–263.
Spiegelhalter, D. J., Best, N. G., Carlin, B. P., and van der Linde, A. (2002).
Bayesian measures of model complexity and ﬁt. Journal of the Royal Sta-
tistical Society, Series B, 64, 583–639.
Steenkamp, J. B. E. M. and Baumgartner, H. (1998).
Assessing measure-
ment invariance in cross-national consumer research. Journal of Consumer
Research, 25, 78–90.

References
305
Stern, H. S. (2000). Asymptotic distribution of P values in composite null
models: Comment.
Journal of the American Statistical Association, 95,
1157–1159.
Stone, C. A. and Hansen, M. A. (2000). The eﬀect of errors in estimating abil-
ity on goodness-of-ﬁt tests for IRT models. Educational and Psychological
Measurement, 60, 974–991.
Stout, W., Habing, B., Douglas, J., Kim, H. R., Roussos, L., and Zhang, J.
(1996).
Conditional covariance-based nonparametric multidimensionality
assessment. Applied Psychological Measurement, 20, 331–354.
Swaminathan, H. and Giﬀord, J. A. (1982). Bayesian estimation in the Rasch
model. Journal of Educational Statistics, 7, 175–192.
Swaminathan, H. and Giﬀord, J. A. (1985). Bayesian estimation in the two-
parameter logistic model. Psychometrika, 50, 349–364.
Swaminathan, H. and Giﬀord, J. A. (1986). Bayesian estimation in the three-
parameter logistic model. Psychometrika, 51, 589–601.
Tanner, M. A. and Wong, W. H. (1987). The calculation of posterior distri-
butions by data augmentation. Journal of the American Statistical Associ-
ation, 82, 528–540.
Tatsuoka, K. K. (1984). Caution indices based on item response theory. Psy-
chometrika, 49, 95–110.
Theil, H. (1963). On the use of incomplete prior information in regression
analysis. Journal of the American Statistical Association, 58, 401–414.
Thissen, D. (1982). Marginal maximum likelihood estimation for the one-
parameter logistic model. Psychometrika, 47, 175–186.
Thissen, D. (1983). Timed testing: An approach using item response theory.
In D. J. Weiss (Ed.), New Horizons in Testing: Latent Trait Test Theory
and Computerized Adaptive Testing (pp. 179–203). New York: Academic
Press.
Thissen, D. (1991). MULTILOG: Multiple Category Item Analysis and Test
Scoring Using Item Response Theory [computer software].
Chicago, IL:
Scientiﬁc Software International.
Thissen, D. and Wainer, H. (2001). Test Scoring. Mahwah, NJ: Lawrence
Erlbaum.
Tiao, G. C. and Tan, W. Y. (1965). Bayesian analysis of random-eﬀects models
in the analysis of variance. I: Posterior distribution of variance components.
Biometrika, 52, 37–53.
TIBCO Software (2009). TIBCO Spotﬁre S+ 8.1: Programmer’s Guide and
Computer Program [computer software]. TIBCO Software Inc.
Tierney, L. (1994). Markov chains for exploring posterior distributions. The
Annals of Statistics, 22, 1701–1762.
Tombaugh, T. N. (1992). The mini-mental state examination: A comprehen-
sive review. The Journal of the American Geriatrics Society, 40, 922–935.
Torgerson, W. S. (1958). Theory and Methods of Scaling. New York: Wiley.
Tourangeau, R., Rips, L., and Rasinski, K. (2000). The Psychology of Survey
Response. Cambridge: Cambridge University Press.

306
References
Tourangeau, R. and Yan, T. (2007). Sensitive questions in surveys. Psycho-
logical Bulletin, 133, 859–883.
Tracy, P. E. and Fox, J. A. (1981). The validity of the randomized response
for sensitive measurements. American Sociological Review, 46, 187–200.
Tsutakawa, R. K. (1984). Estimation of two-parameter logistic item response
curves. Journal of Educational Statistics, 9, 263–276.
Tsutakawa, R. K. and Lin, H. Y. (1986). Bayesian estimation of item response
curves. Psychometrika, 51, 251–267.
Tsutakawa, R. K. and Soltys, M. J. (1988). Approximation for Bayesian ability
estimation. Journal of Educational Statistics, 13, 117–130.
Tucker, L. R. (1952). A level of proﬁciency scale for a unidimensional skill.
American Psychologist, 7, 408 (Abstract).
Tutz, G. and Hennevogl, W. (1996).
Random eﬀects in ordinal regression
models. Computational Statistics and Data Analysis, 22, 537–557.
Vaida, F. and Blanchard, S. (2005). Conditional Akaike information for mixed-
eﬀects models. Biometrika, 92, 351–370.
van Breukelen, G. J. P. (2005).
Psychometric modeling of response speed
and accuracy with mixed and conditional regression. Psychometrika, 70,
359–376.
van den Hout, A. and Klugkist, I. (2009). Accounting for non-compliance
in the analysis of randomized response data. Australian and New Zealand
Journal of Statistics, 51, 353–372.
van der Linden, W. J. (2006). A lognormal model for response times on test
items. Journal of Educational and Behavioral Statistics, 31, 181–204.
van der Linden, W. J. (2007). A hierarchical framework for modeling speed
and accuracy on test items. Psychometrika, 72, 287–308.
van der Linden, W. J. (2008).
Using response times for item selection in
adaptive testing. Journal of Educational and Behavioral Statistics, 33, 5–
20.
van der Linden, W. J. and Hambleton, R. K. (1997). Handbook of Modern
Item Response Theory. New York: Springer.
van der Linden, W. J. and Krimpen-Stoop, E. M. L. A. (2003). Using response
times to detect aberrant responses in computerized adaptive testing. Psy-
chometrika, 68, 251–265.
van der Maas, H. L. J. and Wagenmakers, E.-J. (2005).
A psychometric
analysis of chess expertise. The American Journal of Psychology, 118, 29–
60.
van Dyk, D. A. and Meng, X.-L. (2001).
The art of data augmentation.
Journal of Computational and Graphical Statistics, 10, 1–50.
Vandenberg, R. J. and Lance, C. E. (2000). A review and synthesis of the
measurement invariance literature: Suggestions, practices, and recommen-
dations for organizational research. Organizational Research Methods, 3,
4–70.

References
307
Verdinelli, I. and Wasserman, L. (1995). Computing Bayes factors using a
generalization of the Savage-Dickey density ratio. Journal of the American
Statistical Association, 90, 614–618.
Verhelst, N. D., Glas, C. A. W., and Verstralen, H. H. F. M. (1995). OPLM:
One Parameter Logistic Model [computer software]. Arnhem: Cito.
Verhelst, N. D. and Verstralen, H. H. F. M. (2001). An IRT model for multiple
raters. In A. Boomsma, M. A. J. van Duijn, and T. A. B. Snijders (Eds.),
Essays on Item Response Theory (pp. 89–106). New York: Springer.
Verhelst, N. D., Verstralen, H. H. F. M., and Jansen, M. G. H. (1997). A
logistic model for time-limit tests.
In W. J. van der Linden and R. K.
Hambleton (Eds.), Handbook of Modern Item Response Theory (pp. 169–
185). New York: Springer.
Vermunt, J. K. (2008). Latent class and ﬁnite mixture models for multilevel
data sets. Statistical Methods in Medical Research, 17, 33–51.
Wainer, H., Bradlow, E. T., and Wang, X. (2007). Testlet Response Theory
and Its Applications. Cambridge: Cambridge University Press.
Warner, S. L. (1965). Randomized response: A survey technique for eliminat-
ing evasive answer bias. Journal of the American Statistical Association,
60, 63–69.
Wright, B. D. (1977). Misunderstanding the Rasch model. Journal of Educa-
tional Measurement, 14, 219–225.
Yen, W. M. (1993). Scaling performance assessments: Strategies for managing
local item dependence. Journal of Educational Measurement, 30, 187–213.
Zeger, S. L. and Karim, M. R. (1991). Generalized linear models with random
eﬀects; A Gibbs sampling approach. Journal of the American Statistical
Association, 86, 79–86.
Zellner, A. (1971). An Introduction to Bayesian Inference in Econometrics.
New York: Wiley.
Zellner, A. (1997). Bayesian Analysis in Econometrics and Statistics: The
Zellner View and Papers. Cheltenham: Edward Elgar.
Zimowski, M. F., Muraki, E., Mislevy, R. J., and Bock, R. D. (1996). BILOG-
MG: Multiple-Group IRT Analysis and Test Maintenance for Binary Items
[computer software]. Chicago, IL: Scientiﬁc Software International.

Index
Aggregated levels, 32
anchor item, 206
attenuation, 164, 202
augmented data
continuous, 73
discrete, 271
auxiliary variable, 74
auxiliary variable method, 73
Background information, 32
Bayes factor, 53–58
computing, 54–57
bridge sampling, 56
importance sampling, 55
Savage-Dickey density ratio, 56, 184
Bayes model, 39
Bayes’ theorem, 16, 17
updating rule, 18
Bayesian estimation, 45–51
Bayesian hierarchical response model,
32
Bayesian inference, 15
Bayesian information criterion, 57, 60
Bayesian latent residuals, 109
Bayesian output analysis, 49
Bayesian residual, 108
beta binomial model, 286
between-item structure, 33
bias–variance trade-oﬀ, 60
BIC, see Bayesian information criterion
binary response, 14
BOA, see Bayesian output analysis
booklet, 165
booklet eﬀect, 216
borrowing strength, 34
CODA, see convergence diagnostics
and output analysis
common scale, 208
conditional distribution, 17
conditional independence, 7
conditional maximum likelihood, 9
conditioning variables, 167
conﬁdence interval, 58–59
credible interval, 58
credible region, 59
highest posterior density interval, 59,
66
HPD region, 59
multivariate, 186
convergence diagnostics and output
analysis, 49
covariate measurement error, 172–173
criterion referenced test, 194
cross-level interaction, 148
cross-national surveys, 196
cumulative response probability, 201
Data augmentation, 73–86, 109
discrete, 262
identiﬁcation, 87
ordinal, 83
proper, 74
scheme, 77
deviance, 60, 161
deviance information criterion, 60–61,
130

310
Index
model selection, 241
DIC, see deviance information criterion
DIF, see diﬀerential item functioning
diﬀerential item functioning, 195
diﬃculty parameter, 8
prior, see prior
Dirichlet multinomial model, 288
discrimination parameter, 9
prior, see prior
distribution
Bernoulli, 80, 177
beta, 37, 44
Dirichlet, 288
F, 269
inverse chi-square, 38fn
inverse cumulative normal, 64
inverse gamma, 38fn, 158
inverse Wishart, 36fn, 158, 198
logistic, 13fn, 76
cumulative, 75
truncated, 134
lognormal, 99, 229
multinomial, 288
normal, 10fn, 76
bivariate, 151
cumulative, 10
inverse, 64
multivariate, 35, 233
truncated, 65
normal inverse gamma, 100, 198
normal inverse Wishart, 101, 198
uniform, 65
EAP, see posterior
empirical Bayes, 70
exchangeability, 34, 35
Factor variance invariance, see
measurement invariance
ﬁnite mixture model, see mixture model
ﬁrst-stage prior, 34
ﬁxed eﬀect
prior, 184
full conditional, 71, 73
fully Bayesian analysis, 17, 62
Generalized linear mixed eﬀects model,
144
software, 144
generalized linear model, 143
Gibbs sampling, see Markov chain
Monte Carlo
growth mixture model, 179
guessing parameter, 11
prior, see prior
Heterogeneity
between-individual, 31
residual variation, 88
between-subject, 178, 179
cross-national, 203
within-individual, 31
residual variation, 88
within-subject, 178, 179
hierarchical Bayes model, 39, 40, 70
hierarchical rater model, 182
hierarchical response modeling, 31–33,
42
Bayes model, 39
between-individual, 40
between-item, 33
ﬁrst-stage, 40
pooling information, 31
second-stage, 40
within-item, 33, 36, 44
higher-level data, 4
HPD, see conﬁdence interval
HPD testing, see hypothesis testing
hyperparameter, 32
hyperprior, 32
hypothesis testing, 51–54
frequentist, 51
HPD, 58–59, 112
item ﬁt, 112
outﬁt, 137
nested hypothesis, 56
p-value, 116
person ﬁt, 112
outﬁt, 136
point null, 53
precise, 53
Identiﬁcation, see item response models
anchor item, 206
linkage, 205
incomplete design, 165
individual trajectories, 176
integrated likelihood, 17

Index
311
intraclass correlation coeﬃcient, 144
country-speciﬁc, 219
inverse sampling, 65
item bank, 194
item characteristic curve, 6
item cloning, 194
item ﬁt, see hypothesis testing
item level, 33
item parameters
group-speciﬁc, 195
international, 167, 196
nation-speciﬁc, 168
order restrictions, 209
random, 196
time-invariant, 178
item response models, 6–15
graded response model, 14
identiﬁcation, 9, 86–89
invariance, 222
linear-logistic test model, 194
MCMC estimation, 71–86
multidimensional response model,
14–15
one-parameter logistic model, 7
partial credit model, 13–14, 104
Rasch model, 7–9
software, 24–27
three-parameter model, 11–12, 44
two-parameter model, 9–11
normal ogive, 10, 75
item response time model, 229–231
conditional independence, 231
predictive assessment, 242
residual analysis, 242
Joint hyperprior, 38
joint posterior, 17
joint prior, 32
Latent explanatory variable, 172
gold standard, 172
latent variable, 5, 74
level-1 observations, 143
level-1 residual, 146
level-1 variance, 144
likelihood function, 16
link function, 143
local independence, 7
lower-level data, 4
MAP, see posterior
marginal estimation, 67
marginal likelihood, 17
marginal maximum likelihood, 9, 68,
143
marginal posterior, 20, 22
Markov chain Monte Carlo, 45–51
acceptance rate, 47
autocorrelation, 49
burn-in, 48
convergence, 48–51, 90
diagnostics, 50
software, 49
Gibbs sampling, 46
M-H within Gibbs, 71
Metropolis-Hastings, 47
adaptive, 84
tuning, 84
multiple-chain, 50
single-chain, 49
trace plots, 49
MCMC, see Markov chain Monte Carlo
measurement error, 164
measurement invariance, 194–195
conﬁgural, 195
factor variance, 203
metric, 195
scalar, 195
test, 214
measurement occasion, 176
metric, 11, 86
mixed eﬀects model, 261
structural, 261
mixture MLIRT model, see multilevel
IRT model
mixture model, 177, 268
class membership, 178
growth, 179
identiﬁcation, 178
two-component, 177
monotonicity assumption, 35
multilevel IRT model, 145–153
applications, 162–181
BIC, 161
DIC, 162, 170
intraclass correlation coeﬃcient, 169
likelihood, 161, 190
MCMC, 158
mixed response types, 173

312
Index
mixture, 176–178
MLIRT, 148
predictions, 185
school eﬀect, 188
shrinkage, 151
multilevel model, 145–148
DIC, 170
empty, 146
intraclass correlation coeﬃcient, 146
linear, 163
structural, 145
multiple imputation, 165–169
between-imputation variance, 169
model, 167
plausible values, 167
within-imputation variance, 169
multivariate multilevel model, 232–234
multivariate nonlinear mixed eﬀects
models, 249
Nested models, 242
noncompliance, 267
nonlinear mixed eﬀects model, 142, 181
ﬁxed eﬀects, 142
link function, 143
mixed eﬀects, 142
random eﬀects, 142
two-parameter model, 143
nonnested models, 60
nonsampling error, 255
nonsensitive question, 259
normal approximation, 51
normalizing constant, 32
nuisance parameters, 20
numerical integration, 41, 45–51
EM algorithm, 69
Gauss-Hermite quadrature, 68
high-dimensional, 70
Monte Carlo, 63
Newton-Raphson, 68
Objective prior, 16, 35
odds ratio, 124
ordinal response, 13
outcome, 4
outcome variable, 4
P-value, 52, 113
marginal posterior, 114
person ﬁt, see hypothesis testing
PISA, 165, 216
plausible values, see multiple imputation
pooling information, 34
pooling strength, 31
posterior, 17
computation, 41
EAP, expected a posteriori, 69
MAP, maximum a posteriori, 69
mean, 22, 49
median, 22
mode, 18, 69
predictive distribution, 122
probability, 29
summarizing, 20, 27–29
unnormalized, 17
posterior density, 17
predictive assessment, 117–130
posterior, 122–126
prior, 119–121
prior, 16
conjugate, 33
ﬁrst-stage, 34
hierarchical, 92
hierarchical normal, 36
hyperparameter, 32
hyperprior, 32
identiﬁcation, 236
improper, 38–39, 187
informative, 35, 36
item parameters, 21, 29, 33–38, 43
exchangeable, 34
hierarchical, 34, 39, 43, 72, 105
lognormal, 99
multistage, 34
multivariate, 235
random, 196
locally uniform, 184
nonconjugate, 33
noninformative, 35, 38
objective, 193
person parameters
hierarchical, 38
population, 38
predictive distribution, 119
random item eﬀects, 197
second-stage, 39
subjective, 193
threshold parameter, 38

Index
313
unnormalized, 52
prior density, 16, 20
prior information, 33
probit model, see normal ogive model
proportionality sign, 32
proposal density, 47
Quadrature, 41
quantile, 23
Random eﬀects, 60, 127, 142
random item eﬀects, 194, 195
random item eﬀects model, 198–200
random item parameters, 193
random threshold eﬀects, 197
randomized item response model,
259–262
DIC, 271
identiﬁcation, 262
randomized response, see response data
randomized response design
related, 257
unrelated, 257
randomized response model, 259
randomized response technique, 256–258
Rao-Blackwellized estimate, 109
Rasch model, see item response models
residual analysis, 108, 109
Bayesian, 109
item response models, 109
latent, 270
dichotomous, 109
polytomous, 270
outlier, 110, 112
outlying probability, 110, 112
response accuracy, 228
response data, 3
clustered, 3
complete, 78
cross-classiﬁed, 33, 193
hierarchical, 3
latent, 74
longitudinal, 175
missing, 106
MAR, 106, 150
MCAR, 151
mixed multivariate, 227
multivariate, 260
ordered categories, 83
randomized, 256
dichotomous, 260
forced, 257
hierarchical, 258
multivariate, 258
polytomous, 260
response speed, 228
response time characteristic curve, 229
response time item response model, 234
response times, 227
RIRT, see randomized item response
model
RTIRT, see response time item response
model
Sample information, 33
sampling distribution, 16
sampling error, 255
Savage-Dickey density ratio, see Bayes
factor
school eﬀectiveness, 141
school level, 32
sensitive characteristic, 256
sensitive items, 255
sensitive question, 259
shrinkage, 32, 63
simulation-based estimation methods,
see Markov chain Monte Carlo
structural parameter, 67
student level, 32
stylistic responding, 208
subjective prior, 16
survey, 255
Target density, 46
test booklet, 216
testlet, 127
testlet response model, 127–130, 137
threshold parameter, 14, 197
country-speciﬁc, 197
international, 197
time discrimination, 229
time intensity, 229
time trend, 179
TIMMS, 81
true response, 259
latent, 259
WinBUGS, 21
within-item structure, 33

