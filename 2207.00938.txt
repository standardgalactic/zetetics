JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
1
Interpretable by Design: Learning Predictors by
Composing Interpretable Queries
Aditya Chattopadhyay, Stewart Slocum, Benjamin D. Haeffele,
Ren´e Vidal, Fellow, IEEE and Donald Geman, Life Senior Member, IEEE
Abstract—There is a growing concern about typically opaque decision-making with high-performance machine learning algorithms.
Providing an explanation of the reasoning process in domain-speciﬁc terms can be crucial for adoption in risk-sensitive domains such
as healthcare. We argue that machine learning algorithms should be interpretable by design and that the language in which these
interpretations are expressed should be domain- and task-dependent. Consequently, we base our model’s prediction on a family of
user-deﬁned and task-speciﬁc binary functions of the data, each having a clear interpretation to the end-user. We then minimize the
expected number of queries needed for accurate prediction on any given input. As the solution is generally intractable, following prior
work, we choose the queries sequentially based on information gain. However, in contrast to previous work, we need not assume the
queries are conditionally independent. Instead, we leverage a stochastic generative model (VAE) and an MCMC algorithm (Unadjusted
Langevin) to select the most informative query about the input based on previous query-answers. This enables the online
determination of a query chain of whatever depth is required to resolve prediction ambiguities. Finally, experiments on vision and NLP
tasks demonstrate the efﬁcacy of our approach and its superiority over post-hoc explanations.
Index Terms—Explainable AI, Interpretable ML, Computer Vision, Generative Models, Information Theory
!
1
INTRODUCTION
I
N recent years, interpreting large machine learning mod-
els has emerged as a major priority, particularly for
transparency in making decisions or predictions that impact
human lives [1], [2], [3]. In such domains, understanding
how a prediction is made may be as important as achieving
high predictive accuracy. For example, medical regulatory
agencies have recently emphasized the need for computa-
tional algorithms used in diagnosing, predicting a progno-
sis, or suggesting treatment for a disease, to explain why a
particular decision was made [4], [5].
On the other hand, it is widely believed that there exists
a fundamental trade-off in machine learning between inter-
pretability and predictive performance [6], [7], [8], [9], [10].
Simple models like decision trees and linear classiﬁers are
often regarded as interpretable1 but at the cost of potentially
reduced accuracy compared with larger black box models
such as deep neural networks. As a result, considerable ef-
fort has been given to developing methods that provide post-
hoc explanations of black box model predictions, i.e., given a
prediction from a (ﬁxed) model provide additional annota-
tion or elaboration to explain how the prediction was made.
As a concrete example, for image classiﬁcation problems,
one common family of post-hoc explanation methods pro-
duces attribution maps which seek to estimate the regions
of the image that are most important for prediction. This is
typically approached by attempting to capture the effect
or sensitivity of perturbations to the input (or intermediate
•
The authors are with the Mathematical Institute for Data Science and the
Center for Imaging Science of The Johns Hopkins University, MD, 21218.
E-mail: {achatto1, sslocum3, bhaeffele, rvidal, geman}@jhu.edu
1. Although later in the paper we will discuss situations in which
even these simple models need not be interpretable.
features) on the model output [11], [12], [13], [14], [15], [16],
[17], [18]. However, post-hoc analysis has been critiqued for
a variety of issues [2], [19], [20], [21], [22], [23] (see also §2)
and often fails to provide explanations in terms of concepts
that are intuitive or interpretable for humans [24].
This naturally leads to the question of what an ideal
explanation of a model prediction would entail; however,
this is potentially highly task-dependent both in terms of the
task itself as well as what the user seeks to obtain from
an explanation. For instance, a model for image classiﬁca-
tion is often considered interpretable if its decision can be
explained in terms of patterns occurring in salient parts of
the image [25] (e.g., the image is a car because there are
wheels, a windshield, and doors), whereas in a medical task
explanations in terms of causality and mechanism could be
desired (e.g., the patient’s chest pain and shortness of breath
is likely not a pulmonary embolism because the blood D-
dimer level is low, suggesting thrombosis is unlikely). Note
that some words or patterns may be domain-dependent and
therefore not interpretable to non-experts, and hence what
is interpretable ultimately depends on the end user, namely
the person who is trying to understand or deconstruct the
decision made by the algorithm [26].
In addition to this task-dependent nature of model inter-
pretation, there are several other desirable intuitive aspects
of interpretable decisions that one can observe. The ﬁrst is
that meaningful interpretations are often compositional and
can be constructed and explained from a set of elementary
units [27]. For instance, words, parts of an image, or domain-
speciﬁc concepts [28], [29], [30] could all be a suitable basis
to form an explanation of a model’s prediction depend-
ing on the task. Moveover, the basic principle that simple
and concise explanations are preferred (i.e., Occam’s razor)
suggests that interpretablity is enhanced when an explana-
arXiv:2207.00938v2  [cs.CV]  25 Nov 2022

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
2
tion can be composed from the smallest number of these
elementary units as possible. Finally, we would like this
explanation to be sufﬁcient for describing model predictions,
meaning that there should be no external variables affecting
the prediction that are not accounted for by the explanation.
Inspired by these desirable properties, we propose a
framework for learning predictors that are interpretable by
design. The proposed framework is based on composing a
subset of user-deﬁned concepts, i.e., functions of the input
data which we refer to as queries, to arrive at the ﬁnal
prediction. Possible choices for the set of queries Q based
on the style of interpretation that is desired include:
1) Salient image parts: For vision problems, if one is
interested in explanations in terms of salient image
regions then this can be easily accomplished in our
framework by deﬁning the query set to be a collection
of small patches (or even single pixels) within an image.
This can be thought of as a generalization of the pixel-
wise explanations generated by attribution maps.
2) Concept-based explanations: In domains such as med-
ical diagnosis or species identiﬁcation, the user might
prefer explanations in terms of concepts identiﬁed by
the community to be relevant for the task. For instance,
a “Crow” is determined by the shape of the beak,
color of the feathers, etc. In our framework, by simply
choosing a query for each such concept, the user can
easily obtain concept-based explanations (see Fig. 1(b)).
3) Visual scene interpretation: In visual scene under-
standing, one seeks a rich semantic description of a
scene by accumulating the answers to queries about the
existence of objects and relationships, perhaps generat-
ing a scene graph [31]. One can design a query set Q by
instantiating these queries with trained classiﬁers. The
answers to chosen queries in this context would serve
as a semantic interpretation of the scene.
4) Deep neuron-based explanations: The above three
examples are query sets based on domain knowledge.
Recent techniques [30], [32], [33] have shown the ability
of different neurons in a trained deep network to act
as concept detectors. These are learnt from data by
solving auxiliary tasks without any explicit supervisory
signal. One could then design a Q in which each
query corresponds to the activation level of a speciﬁc
concept neuron. Such a query set will be useful for
tasks in which it is difﬁcult to specify interpretable
functions/queries beforehand.
Given a user-speciﬁed set of queries Q, our framework
makes its prediction by selecting a short sequence of queries
such that the sequence of query-answer pairs provides a
complete explanation for the prediction. More speciﬁcally,
the selection of queries is done by ﬁrst learning a generative
model for the joint distribution of queries and output labels
and then using this model to select the “most informative”
queries for a given input. The ﬁnal prediction is made using
the Maximum A Posteriori (MAP) estimate of the output
given these query-answer pairs. Fig. 1(a) gives an illustra-
tion of our proposed framework, where the task is to predict
the bird species in an image and the queries are based on
color, texture and shape attributes of birds. We argue that
the sequence of query-answer pairs provides a meaningful
Input image 𝑥!"#
Green Jay with 
99% probability
Predicted bird species
Ask a sequence of interpretable queries about 𝑥!"#
𝑞$. Has shape perching-like?
Yes
𝑞%.
Has bill shape all-purpose? 
Yes
𝑞&.
Has belly color yellow? 
Yes
𝑞'.
Has upperparts color yellow?
No
𝑞(.
Has throat color yellow?
No
𝑞).
Has breast color black? 
Yes
𝑞*.
Has belly color olive? 
Yes
(a)
(b)
What is 𝑞!?
𝑞& 𝑥!"# = No
𝑞& 𝑥!"# = Yes
Fig. 1. (a) An illustration of our proposed learning framework. The
prediction of a bird species is explained through a short sequence of
interpretable queries, (q1, q2, ..., q7), derived from a user-deﬁned query
set of domain-speciﬁc attribute for birds. (b) Interpretable queries.
Each query in this case corresponds to a well-deﬁned bird attribute. For
instance, q3 asks “Does the bird have belly color yellow?”. We visualize
some example images which evaluate to “Yes” and observe that all of
them correspond to birds with a yellow belly. Similarly, all images which
evaluate to “No” corresponds to birds which do not have a yellow belly.
explanation to the user that captures the subjective nature
of interpretability depending on the task at hand, and that
is, by construction, compositional, concise and sufﬁcient.
At ﬁrst glance, one might think that classical decision
trees [34], [35] based on Q could also produce interpretable
decisions by design. However, the classical approach to
determining decision tree branching rules based on the
empirical distribution of the data is prone to over-ﬁtting due
to data fragmentation. Whereas random forests [36], [37] are
often much more competitive than classical decision trees
in accuracy [38], [39], [40], they sacriﬁce interpretability,
the very property we want to hardwire into our decision
algorithm. Similarly, the accuracy of a single tree can be
improved by using deep networks to learn queries directly
from data, as in Neural Decision Trees (NDTs) [41]. How-
ever, the opaqueness of the interpretation of these learnt
queries makes the explanation of the ﬁnal output, in terms
of logical operations on the queries at the internal nodes,
unintelligible. Figure 2 illustrates this with an example.
In this paper we make the following contributions;
• We propose a novel framework for prediction that is
interpretable by design. We allow the end-user to specify
a set Q of queries about input X and formulate learning
as the problem of selecting a minimal set of queries

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
3
Input image 𝑥!"#
𝑑$
𝑑%
𝑑&
𝑑'
𝑑(
𝑑)
𝑑*
𝑙$
𝑙%
𝑙&
𝑙'
𝑙(
𝑙)
𝑙*
𝑙+
Green Jay with 
99% probability
Predicted Green Jay 
because d! x"#$ <
0.5, d% x"#$ ≥0.5
and d& x"#$ < 0.5
(a)
What is 𝑑$?
𝑑! 𝑥"#$ ≥0.5
𝑑! 𝑥"#$ < 0.5
(b)
Fig. 2. The interpretability of an explanation depends on how inter-
pretable the queries are. (a) An illustration of a Deep Neural decision
tree [41] trained on the CUB-2011 dataset of bird images. The bold
path denotes the trajectory the input image xobs takes through the tree.
Each di corresponds to an internal node of the tree and is a black-box
function/query learnt from data. Each li denotes a leaf and computes
the ﬁnal classiﬁcation for xobs. The prediction can be explained as a
conjunction of internal node functions, but is it really interpretable? (b)
Example images that get routed to the left sub-tree (d1 ≥0.5) and right
sub-tree (d1 < 0.5) of the root node. Notice that the interpretation of
d1 is not clear from these examples. Compare this to Fig. 1 where the
semantics of each query is unambiguous to the end-user.
from Q whose answers are sufﬁcient for predicting
output Y . We formulate this query selection problem as
an optimization problem over strategies that minimize
the number of queries needed on average to predict
Y from X. A prediction for Y is then made based
on the selected query-answer pairs, which provide an
explanation for the prediction that is by construction
interpretable. The set of selected query-answer pairs
can be viewed as a code for the input. However, a major
difference between our framework and coding theory
is that, due to the constraint of interpretability, Q is
a vanishingly small collection of the functions of X,
whereas coding theory typically considers Q to be all
possible binary functions of X.
• Since computing the exact solution to our optimization
problem is computationally challenging, we propose to
greedily select a minimal set of queries by using the
Information Pursuit (IP) algorithm [42]. IP sequentially
selects queries in order of maximum information gain
until enough evidence is gathered from the query-
answer pairs to predict Y . This sequence of query-
answer pairs serves as the explanation for predicting Y
from X. To ameliorate the computational challenge of
computing information gain for high-dimensional in-
put and query spaces, prior work [42] had assumed that
query answers were conditionally independent given
Y , an assumption that is largely inadequate for most
prediction tasks we encounter in practice. In this paper,
we propose a latent variable graphical model for the
joint distribution of queries and outputs, p(Q(X), Y ),
and learn the required distributions using Variational
Autoencoders (VAEs). We then use the Unadjusted
Langevin Algorithm (ULA) to generate samples re-
quired to carry out IP. This gives us a tractable algo-
rithm for any task and query set. To the best of our
knowledge, ours is the ﬁrst implementation of IP that
uses deep generative models and does not assume that
query answers are conditionally independent given Y .
• Finally, we demonstrate the utility of our framework on
various vision and NLP tasks. In binary image classiﬁ-
cation using MNIST, Fashion-MNIST & KMNIST, and
bird species identiﬁcation using CUB-200, we observe
that IP ﬁnds succinct explanations which are highly
predictive of the class label. We also show, across var-
ious datasets, that the explanations generated by our
method are shorter and more predictive of the class la-
bel than state-of-the-art post-hoc explanation methods
like Integrated Gradients and DeepSHAP.
2
RELATED WORK
Methods for interpretable deep learning can be separated
into those that seek to explain existing models (post-hoc
methods) and those that build models that are interpretable
by design. Because they do not negatively impact perfor-
mance and are convenient to use, post-hoc explanations
have been the more popular approach, and include a great
diversity of methods.
Saliency maps estimate the contribution of each feature
through ﬁrst-order derivatives [11], [12], [16], [17], [43].
Linear perturbation-based methods like LIME [44] train a
linear model to locally approximate a deep network around
a particular input, and use the coefﬁcients of this model to
estimate the contribution of each feature to the prediction.
Another popular set of methods use game-theoretic Shapley
values as attribution scores, estimating feature contributions
by generating predictions on randomly sampled subsets
of the input [45]. We provide quantitative comparisons
between IP and these methods in Section 5.1.2. Recently,
there has been interest in concept-based analogues of these
methods that leverage similar approaches to measure the
sensitivity of a prediction to high-level, human-friendly
concepts as opposed to raw features [46], [47], [48].
Despite certain advantages, what all the above post-
hoc methods have in common is that they come with little
guarantee that the explanations they produce actually reﬂect
how the model works [2]. Indeed, several recent studies [18],
[19], [20], [21], [22] call into question the veracity of these
explanations towards the trained model. Adebayo et al. [19]
show that several popular attribution methods act similar
to edge detectors and are insensitive to the parameters of
the model they attempt to explain! Yang et al. [20] ﬁnd that
these methods often produce false-positive explanations,
assigning importance to features that are irrelevant to the
prediction of the model. It is also possible to adversarially
manipulate post-hoc explanations to hide any spurious bi-
ases the trained model might have picked up from data [23].
Interpretability by design. These issues have motivated
recent work on deep learning models which are interpretable
by design, i.e., constrained to produce explanations that
are faithful to the underlying model, albeit with varying
conceptions of “faithfulness”. Several of these models are
constructed so they behave similarly to or can be well-
approximated by a classically interpretable model, such as
a linear classiﬁer [49], [50] or a decision tree [51]. This
allows for an approximately faithful explanation in raw
feature space. In a similar vein, Pillai & Pirsiavash [52] ﬁx

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
4
a post-hoc explanation method (e.g. Grad-CAM [16]), and
regularize a model to generate consistent explanations with
the chosen post-hoc method. However, our method does
not just behave like a fully interpretable model or generate
approximately faithful explanations, but rather it produces
explanations that are guaranteed to be faithful and fully
explain a given prediction.
Another approach to building interpretable models by
design is to generate explanations in terms of high-level,
interpretable concepts rather than in raw feature space,
often by applying a linear classiﬁer to a ﬁnal latent space of
concepts [25], [49], [53]. However these concepts are learned
from data, and may not align with the key concepts iden-
tiﬁed by the user. For example, Prototypical Part Networks
[25] take standard convolutional architectures and insert a
“prototype layer” before the ﬁnal linear layer, learning a
ﬁxed number of visual concepts that are used to represent
the input. This allows the network to explain a prediction in
terms of these “prototype” concepts. Since these prototypes
are learned embeddings, there is no guarantee that their
interpretation will coincide with the user’s requirements.
Furthermore, these explanations may require a very large
number of concepts, while in contrast, we seek minimal-
length explanations to preserve interpretability.
Attention-based models are another popular family of
models that are sometimes considered interpretable by de-
sign [54], [55]. However, attention is only a small part of the
overall computation and can be easily manipulated to hide
model biases [56]. Moreover, the attention coefﬁcients are
not necessarily a sufﬁcient statistic for the model prediction.
Perhaps most similar to our work are Concept Bottleneck
Networks [24], which ﬁrst predict an intermediate set of
human-speciﬁed concepts c and then use c to predict the
ﬁnal class label. Nevertheless, the learnt mapping from con-
cepts to labels is still a black-box. To remedy this, the authors
suggest using a linear layer for this mapping but this can
be limiting since linearity is often an unrealistic assump-
tion [27]. In contrast, our framework makes no linearity
assumptions about the ﬁnal classiﬁer and the classiﬁcation
is explainable as a sequence of interpretable query-answer
pairs obtained about the input (see Fig. 1(a)).
Neural networks and decision trees. Unlike the above
methods, which can be thought of as deep interpretable
linear classiﬁers, our method can be described as a deep
decision tree that branches on responses to an interpretable
query set. Spanning decades, there has been a variety of
work building decision trees from trained neural networks
[29], [57], [58], [59] and using neural networks within nodes
of decision trees [41], [60], [61], [62]. Our work differs from
these in three important aspects. First, rather than allowing
arbitrary splits, we branch on responses to an interpretable
query set. Second, instead of using empirical estimates of
information gains based on training data (which inevitably
encounter data-fragmentation [63] and hence overﬁtting),
or using heuristics like agglomerative clustering on deep
representations [29], we calculate information gain from a
generative model, leading to strong generalization. Third,
for a given input, say xobs, we use a generative model to
compute the queries along the branch traversed by xobs in
an online manner. The entire tree is never constructed. This
allows for much very deep terminal nodes when necessary
to resolve ambiguities in prediction. As an example, for
the task of topic classiﬁcation using the HuffPost dataset
(§5.0.3), our framework asks about 199 queries (on average)
before identifying the topic. Such large depths are impossi-
ble in standard decision trees due to memory limitations.
Information bottleneck and minimal sufﬁcient statis-
tics. The problem of ﬁnding minimal-length, task-sufﬁcient
codes is not new. For example, the information bottleneck
method [64] seeks a minimum-length encoding for X that
is (approximately) sufﬁcient to solve task Y . Our concept
of description length differs in that we constrain the code
to consist of interpretable query functions rather than all
functions of the input, as in the information bottleneck
and classical information theory. Indeed, arbitrary subsets
of the input space (e.g. images) are overwhelmingly not
interpretable to humans.
Sequential active testing and hard attention. The infor-
mation pursuit (IP) algorithm we use was introduced in
[42] under the name ”active testing,” which sequentially
observes parts of an input (rather than the whole input
at once), using mutual information to determine ”where
to look next,” which is calculated online using on a scene
model. Sequentially guiding the selection of partial obser-
vations has also been independently explored in Bayesian
experimental design [65]. Subsequent works in these two
areas include many ingredients of our approach (e.g. gen-
erative models [31], [66] and MCMC algorithms [67]). Of
particular interest is the work of Branson et al. [68] which
used the CUB dataset to identify bird species by sequentially
asking pose and attribute queries to a human user. They
employ IP to generate the query sequence based on answers
provided by the user, much like our experiments in §5.0.2.
However, for the sake of tractability, all the above works
assume that query answers are independent conditioned on
Y . We do not. Rather, to the best of our knowledge, ours
is the ﬁrst implementation of the IP algorithm that uses
deep generative models and only assumes that queries are
independent given Y and some latent variable Z. This greatly
improves performance, as we show in §5.
The strategy of inference through sequential observa-
tions of the input has been recently re-branded in the
deep learning community as Hard Attention [69], [70], [71].
However, high variance in gradient estimates and scalability
issues have prevented widespread adoption. In the future,
we wish to explore how our work could inform more
principled and better-performing reward functions for Hard
Attention models.
Visual question answering. Although it may appear that
our work is also related to the Visual Question Answering
(VQA) literature [72], [73], [74], [75], [76], [77], we note that
our work addresses a very different problem. VQA focuses
on training deep networks for answering a large set of
questions about a visual scene. In contrast, our framework
is concerned with selecting a small number of queries to ask
about a given image to solve a task, say classiﬁcation. As we
move on to more complex tasks, an interesting avenue for
future work would involve using VQA systems to supply
answers to the queries used in our framework. However,

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
5
this would require signiﬁcantly more complex generative
models that the ones considered here.
3
LEARNING INTERPRETABLE PREDICTORS
BY
COMPOSING QUERIES VIA INFORMATION PURSUIT
Let X and Y be the input data and the corresponding
output/hypothesis, both random variables assuming values
in X and Y respectively. In supervised learning, we seek to
infer Y from X using a ﬁnite set of samples drawn from
the joint distribution pXY (x, y).2 As motivated in Section 1,
useful explanations for prediction should be task-dependent,
compositional, concise and sufﬁcient. We capture such proper-
ties through a suitably rich set Q of binary functions q(x),
or queries, whose answers {q(x)}q∈Q collectively determine
the task Y . More precisely, a query set Q is sufﬁcient for Y if
p(y | x) = p(y | {x′ ∈X : q(x′) = q(x) ∀q ∈Q}).
(1)
In other words, Q is sufﬁcient for Y if whenever two inputs
x and x′ have identical answers for all queries in Q, their
corresponding posteriors are equal, i.e., p(y | x) = p(y | x′).
Given a ﬁxed query set Q, how do we compose queries
into meaningful representations that are predictive of Y ?
We answer this by ﬁrst formally deﬁning an explanation
strategy π and then formulating the task of composing
queries as an optimization problem.
Explanation strategies based on composing queries. An ex-
planation strategy, or just strategy, is a function, π : K∗→Q,
where K∗is the set of all ﬁnite-length sequences generated
using elements from the set K = {(q, q(x)) | q ∈Q, x ∈X}
of query-answer pairs. We require that Q contains a special
query, qST OP , which signals the strategy to stop asking
queries and output explπ
Q(x), the set of query-answer pairs
asked before qST OP . More formally, a strategy π is recur-
sively deﬁned as follows; given input sample xobs
1) q1 = π(∅). The ﬁrst query is independent of xobs.
2) qk+1 = π({qi, qi(xobs)}1:k). All subsequent queries de-
pend on the query-answer pairs observed so far for xobs.
3) If qL+1 = qST OP terminate, and return
explπ
Q(xobs) := {qi, qi(xobs)}1:L.
(2)
Notice that each qi depends on xobs, but we drop this
dependency in the notation for brevity. We call the number
of pre-STOP queries for a particular xobs as the expla-
nations’ description length and denote it by tπ(xobs) :=
|explπ
Q(xobs)|. Computing a strategy on xobs is thus akin
to traversing down the branch of a decision tree dictated
by xobs. Each internal node encountered along this branch
computes the query proposed by the strategy based on the
path (query-answer pairs) observed so far.
Notice also that we restrict out attention to sequential
strategies so that the resulting explanations satisfy the
property of being preﬁx-free.3 This means that explanations
generated for predictions made on an input signal x1 cannot
be a sub-part for explanations generated for predictions
on a different input signal x2; otherwise, the explanation
2. We denote random variables by capital letters and their realizations
with small letters.
3. The term preﬁx-free comes from the literature on instantaneous
codes in information theory.
Strategy (𝜋)
q1(xobs)
q1
xobs
;
Strategy (𝜋)
{qi, qi(xobs)}1:L
qST OP
expl⇡
Q(xobs) :
arg max
y2Y
p(y | expl⇡
Q(x))
ypred
iter. 1
iter. L+1
Asking queries for input xobs
Prediction for input xobs
Strategy (𝜋)
{q1, q1(xobs)}
q2(xobs)
q2
iter. 2
xobs
qi 2 Q
Queries from a 
user-defined 
query set
expl⇡
Q(xobs) := {q1, q1(xobs), . . .
. . . , qL, qL(xobs)}
Explanations are 
conjunctions of 
query-answer 
pairs gleaned 
from 𝑥!"#
Fig. 3. Schematic view of the overall framework for quantifying explana-
tions for predicting y from xobs. For details see Sec. 3.
procedure is ambiguous because a terminal node carrying
one label could be an internal node of a continuation leading
to a different label. Sequential strategies generate preﬁx-free
explanations by design. For non-sequential strategies, which
are just functions mapping an input X to a set of queries in
Q, it is not clear how to effectively encode the constraint of
generating preﬁx-free explanations.
Concise and approximately sufﬁcient strategies. In ma-
chine learning, we are often interested in solving a task
approximately rather than exactly. Let Q be sufﬁcient for Y ,
choose a distance-like metric d on probability distributions
and let ϵ > 0. We propose the following optimization
problem to efﬁciently compose queries for prediction,
min
π
EX
|explπ
Q(X)|
 =: Hϵ
Q(X; Y )
(3)
s.t. EX[d
 p(Y | X), p(Y | explπ
Q(X)
] ≤ϵ (ϵ-Sufﬁciency),
where the minimum is taken over all strategies π. The
solution π∗to (3) provides a criterion for an optimal strategy
for the task of inferring Y approximately from X. The min-
imal expected description length objective, Hϵ
Q(X; Y ), ensures
the conciseness of the explanations, while the constraint
ensures approximate sufﬁciency of the explanation. The
“metric” d on distributions could be KL-divergence, total
variation, Wasserstein distance, etc. The hyper-parameter ϵ
controls how approximate the explanations are. The poste-
rior p(y | explπ
Q(xobs)) should be interpreted as the condi-
tional probability of y given the event
[xobs]π,Q := {x ∈X | explπ
Q(x) = explπ
Q(xobs)}.
(4)
explπ
Q(X) can also be interpreted as a random variable
which maps input X to its equivalence class [X]π,Q.
The ﬁnal prediction/inference for the input xobs is then
taken to be the usual MAP estimator, namely
ypred = arg max
y∈Y
p(y | explπ
Q(xobs)).
(5)
The sequence of query-answers streams obtained by π on
xobs serves as the explanation for ypred. One could also mon-
itor the posterior over the labels Y evolving as successive
queries get asked to gain more insight into the strategy’s
decision-making process. Fig. 3 illustrates the overall frame-
work in detail.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
6
Information Pursuit: a greedy approximation. Unfortu-
nately, solving (3) is known to be NP-Complete and hence
generally intractable [78]. As an approximate solution to (3)
we propose to use a greedy algorithm called Information
Pursuit (IP). IP was introduced by Geman & Jedynak in 1996
[42] as a model-based, online construction of a single but
deep branch. The IP strategy, that is, π = IP, is recursively
deﬁned as follows,
q1 = IP(∅) = arg max
q∈Q
I(q(X); Y )
(6)
qk+1 = IP({qi, qi(xobs)}1:k)) = arg max
q∈Q
I(q(X); Y | SIP
k (xobs))
where I denotes mutual information and SIP
k (xobs) cor-
responds to the event {x
∈
X
|
{qi, qi(xobs)}1:k
=
{qi, qi(x)}1:k}. Ties in choosing qk+1 are broken arbitrarily
if the maximum is not unique.
The algorithm stops when there are no more informative
queries left in Q, that is, it satisﬁes the following condition:
qL+1 = qST OP
if max
q∈Q I(q(X); Y | SIP
m(xobs)) ≤ϵ
∀m ∈{L, L + 1, ..., L + T},
(7)
where hyper-parameter T
>
0 is chosen via cross-
validation. This termination criteria corresponds to taking
the distance-like metric d in (3) as the KL-divergence be-
tween the two distributions. Further details about the rela-
tion between this termination criteria and the ϵ-Sufﬁciency
constraint in (3) are provided in Appendix A.3. For tasks in
which Y is a function of X, a common scenario in many
supervised learning problems, we use a simpler alternative,
qL+1 = qST OP
if arg max
y∈Y
p(y | SIP
m(xobs)) ≥1 −ϵ
∀m ∈{L, L + 1, ..., L + T}.
(8)
The key distinction between the information gain criteria
used in standard decision tree induction and IP is that the
former uses the empirical distributions to compute (6) while
the latter is based on generative models (as we will see
in Section 4). The use of generative models guards against
data fragmentation [63] and thus allows for asking longer
sequences of queries without grossly over-ﬁtting.
How does IP compare to the optimal strategy π∗? We begin
by characterizing the constraint in (3) in terms of mutual
information, the quantity that drives IP.
Proposition 1. Let Sπ
k (X) be a random variable where any
realization Sπ
k (xobs), xobs ∈X , denotes the event
Sπ
k (xobs) := {x′ ∈X | {qi, qi(xobs)}1:k = {qi, qi(x′)}1:k},
where qi is the ith query selected by π for input xobs. Here we use
the convention that Sπ
0 (X) = Ω(the entire sample space) and
Sπ
l (X) = Sπ
tπ(X)(X) ∀l > tπ(X). If Q is ﬁnite4 and d is taken
to be the KL-divergence, then objective (3) can be rewritten as
Hϵ
Q(X; Y ) := min
π
EX
|explπ
Q(X)|

s.t.
τ π
X
k=1
I(Y ; Sπ
k (X) | Sπ
k−1(X)) ≥I(X; Y ) −ϵ,
(9)
4. The assumption of Q being a ﬁnite set is benign. Many interested
applications can be addressed with a ﬁnite Q as we show in our
experiments.
where τ π = max{tπ(x) : x ∈X} and tπ(X) is deﬁned as the
number of queries selected by π for input X until qST OP .
See Appendix A.1 for a detailed proof. The objective in
(9) can be alternatively stated as,
max
π
τ π
X
k=1
I(Y ; Sπ
k (X) | Sπ
k−1(X))
s.t. EX
|explπ
Q(X)|
 ≤γ,
(10)
where γ > 0 is a user-deﬁned hyper-parameter. From (10) it
is clear that the optimal strategy π∗would ask a sequence of
queries about X that would maximize the cumulative sum
of the mutual information each additional query provides
about Y , conditioned on the history of query-answers ob-
served so far, subject to a constraint on the average number
of queries that can be asked. As stated before, solving for π∗
is infeasible but a greedy approximation that makes locally
optimal choices is much more amenable.
Suppose that one has been given the answers to k queries
about a given input, the locally optimal choice would then
be to ask the most informative query about Y conditioned
on the history of these k query-answers observed. This
greedy choice at each stage gives rise to the IP strategy.
Obtaining approximation guarantees for IP is still an open
problem; however in the special case where Q is taken to be
the set of all possible binary functions of X, it is possible to
show that IP asks at most 1 query more than π∗on average.
More formally, we have the following result, whose proof
can be found in Appendix A.2.
Proposition 2. Let Y be discrete. Let ˜HQ(X; Y ) be the expected
description length obtained by the IP strategy. If H(Y |X) = 0
and Q is the set of all possible binary functions of X such that
H(q(X) | Y ) = 0 ∀q ∈Q, then
H(Y ) ≤˜HQ(X; Y ) ≤H(Y ) + 1
(11)
Having posed the problem of ﬁnding explanations as an
optimization problem and proposed a greedy approxima-
tion to solving it, in the next section we propose a tractable
implementation of IP based on deep generative models.
4
INFORMATION
PURSUIT
USING
VARIATIONAL
AUTOENCODERS AND UNADJUSTED LANGEVIN
IP requires probabilistic models relating query-answers and
data to compute the required mutual information terms in
(6). Speciﬁcally, computing qk+1 in (6) (for any iteration
number k) requires computing the mutual information be-
tween q(X) and Y given the history SIP
k (xobs) till time k. As
histories become longer, we quickly run out of samples in
our dataset which belong to the event SIP
k (xobs). As a result,
non-parametric sample-based methods to estimate mutual
information (such as [79]) would be impractical. In this
section, we propose a model-based approach to address this
challenge for a general supervised learning task and query
set Q. In §5 we adapt this model to the speciﬁc cases where
Q is taken to be image patches or task-based concepts.
Information Pursuit Generative Model. To make learning
tractable, we introduce latent variables Z to account for all

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
7
the dependencies between different query-answers, and we
posit the following factorization of Q(X), Y, Z
pQ(X)ZY (Q(x), z, y)
(12)
=
Y
q∈Q
pq(X)|ZY (q(x) | z, y)pY (y)pZ(z),
where Q(X) = {q(X) : q ∈Q}, and z and q(x) denote
realizations of Z and q(X) respectively. In other words,
we assume that the query-answers are conditionally in-
dependent given the label y and a latent vector z. The
independence assumption in (12) shows up ubiquitously in
many machine learning applications, such as the following.
1) q(X) as object presence indicators evaluated at
non-overlapping windows: Let Q be a set of non-
overlapping windows in the image X with q(X) being
a random variable indicating the presence of an object
at the qth location. The correlation between the qs is
entirely due to latent image generating factors Z, such
as lighting, camera position, scene layout, and texture
along with the scene description signal Y .
2) q(X) as snippets of speech utterances: A common
assumption in speech recognition tasks is that the audio
frame features (q(X)) are conditionally independent
given latent phonemes Z (which is often modeled as
a Hidden Markov Model).
The latent space Z is often a lower-dimensional space
compared to the original high-dimensional X. We learn
Z from data in an unsupervised manner using variational
inference. Speciﬁcally, we parameterize the distributions
{pω(q(x) | z, y) ∀q ∈Q} with a Decoder Network with
shared weights ω. These weights are learned using stochas-
tic Variational Bayes [80] by introducing an approximate
posterior distribution p′
φ(z | y, Q(x)) parameterized by
another neural network with weights φ called the Encoder
Network and priors pY (y) and pZ(z). More speciﬁcally, the
parameters φ and ω are learned by maximizing the Evidence
Lower BOund (ELBO) objective. Appendix A.7 gives more
details on this optimization procedure. The learned Decoder
Network pω∗(q(x) | z, y) is then used as a plug-in estimate
for the true distribution pq(X)|ZY (q(x) | z, y), which is in
turn used to estimate (12).
Implementing IP using the generative model. Once the
Decoder Network has been learned using variational infer-
ence, the ﬁrst query q1 = IP(∅) is the one that maximizes
the mutual information with Y as per (6). The mutual infor-
mation term for any query q is completely determined by
p(q(x), y), which is obtained by numerically marginalizing
the nuisances Z from (12) using Monte Carlo integration. In
particular, we carry out the following computation ∀q ∈Q,
pq(X)Y (q(x), y) =
Z
z
pQ(X)ZY (Q(x), z, y)dz
=
Z
z
pq(X)|ZY (q(x) | z, y)pY (y)pZ(z)dz
≈1
N
N
X
i=1
pω∗(q(x) | y, z(i))pY (y)
=: ˜p(q(x), y)).
(13)
In the last approximation, pω∗(q(x) | y, z(i)) is the distribu-
tion obtained using the trained decoder network. N is the
number of i.i.d. samples drawn and zi ∼pZ(z). We then
estimate mutual information numerically via the following
formula,
I(Y ; q(X)) =
X
q(x),y
˜p(q(x), y) log ˜p(q(x), y)
˜p(q(x))˜p(y).
(14)
The computation of subsequent queries qk+1 requires
the mutual information conditioned on observed history
SIP
k (xobs), which can be calculated from the distribution
p(q(x), y | SIP
k (xobs))
(15)
=
Z
p(q(x), z, y | SIP
k (xobs))dz
=
Z
p(q(x) | z, y, SIP
k (xobs))p(z | y, SIP
k (xobs))p(y | SIP
k (xobs))dz
=
Z
p(q(x) | z, y)p(z | y, SIP
k (xobs))p(y | SIP
k (xobs))dz.
The ﬁrst equality is an application of the law of total
probability. The last equality appeals to the assumption that
{q(X), q ∈Q} are conditionally independent given Y, Z
(12).
To estimate the right-hand side of (15) via Monte Carlo
integration, one needs to sample zi ∼p
 z | y, SIP
k (xobs)

and compute
p(q(x), y | SIP
k (xobs)) ≈˜p(q(x), y | SIP
k (xobs))
(16)
:= 1
N
N
X
i=1
pω∗(q(x)|z(i), y)p(y|SIP
k (xobs)),
where the term p(y | SIP
k (xobs)) is estimated recursively via
the Bayes’ theorem. This computation is as follows,
p(y | SIP
k (x)) ∝p(y, SIP
k (x))
= p(qk(x), y, SIP
k−1(x))
∝p(qk(x) | y, SIP
k−1(x))p(y | SIP
k−1(x))
(17)
SIP
0 (x) = ∅(since no evidence via queries has been gathered
from x yet) and so p(y | SIP
0 (x)) = pY (y). The posterior p(y |
SIP
k (x)) is obtained by normalizing the last equation in (17)
such that P
y p(y|SIP
k (x)) = 1. This recursive updating of the
posterior is similar to the posterior updates used in Bayesian
sequential ﬁltering [81]. The term p(qk(x) | y, SIP
k−1(x)) is
estimated using (16).
Having estimated p(q(x), y | SIP
k (xobs)), we then nu-
merically compute the mutual information between query-
answer q(X) and Y given history for every q ∈Q via the
formula
I(Y ; q(X) | SIP
k (xobs)) =
(18)
X
q(x),y
˜p(q(x), y|SIP
k (xobs)) log
˜p(q(x), y | SIP
k (xobs))
˜p(q(x)|SIP
k (xobs))˜p(y|SIP
k (xobs)).
Estimating
p
 z | y, SIP
k (xobs)

with
the
Unadjusted
Langevin Algorithm. Next we describe how to sample
from this posterior p(z | y, SIP
k (xobs)) using the Unadjusted
Langevin Algorithm (ULA). ULA is an iterative algorithm
used to approximately sample from any distribution with a
density known only up to a normalizing factor. It has been
successfully applied to many high-dimensional Bayesian

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
8
inference problems [82], [83], [84]. Given an initialization
z(0), ULA proceeds by
z(i+1) = z(i) + η∇U(z(i)) +
p
2ηζ(i+1).
(19)
Here (ζ(i))i≥1 ∼N(0, I) and η is the step-size. Asymptoti-
cally, the chain (z(i))i≥1 converges to a stationary distribu-
tion that is “approximately” equal to a measure with density
∝eU(z) [85].
For IP, we need samples from p(z | y, SIP
k (xobs). This
is achieved by initializing z(0) using the last iterate of the
ULA chain used to simulate p(z | y, SIP
k−1(xobs).5 We then
run ULA for N iterations by recursively applying (19) with
U(z) := log p(z, SIP
k (xobs)|y) = log p(SIP
k (xobs)|z, y)p(z)p(y).
The number of steps N is chosen to be sufﬁciently large
to ensure the ULA chain converges “approximately” to the
desired z ∼p(z | y, SIP
k (xobs)). We use the trained decoder
network Qk
i=1 pω(qi(x) | z, y), with qi being the ith query
asked by IP for input x, as a proxy for p
 SIP
k (xobs)
 | z, y).
We then obtain stochastic approximations of (15) by time
averaging the iterates,
p

q(x), y|SIP
k (xobs)

≈1
N
N
X
i=1
pω

q(x)|z(i), y

p

y|SIP
k (xobs)

,
(20)
where (z(i))1:N are the iterates obtained using the ULA
chain whose stationary distribution is “approximately”
p
 z | y, SIP
k (xobs)

.
Algorithmic complexity for IP. For any given input x, the
per-iteration cost of the IP algorithm is O(N+|Q|m)6, where
|Q| is the total number of queries, N is the number of
ULA iterations, and m is cardinality of the product sample
space q(X) × Y . For simplicity we assume that the output
hypothesis Y and query-answers q(X) are ﬁnite-valued and
also that the number of values query answers can take is the
same. However, our framework can handle more general
cases. See Appendix A.6 for more details.
5
EXPERIMENTS
In this section, we empirically evaluate the effectiveness
of our method. We begin by analyzing the explanations
provided by IP for classifying individual input data, in
terms of words, symbols, or patterns (the queries). We ﬁnd
in each case that IP discovers concise explanations which
are amenable to human interpretation. We then perform
quantitative comparisons which show that (i) IP explana-
tions are more faithful to the underlying model than existing
attribution methods; and (ii) the predictive accuracy of our
method using a given query set is competitive with black-
box models trained on features provided by the same set.
5.0.1
Binary Image Classiﬁcation with Patch Queries
Task and query set. We start with the simple task of binary
image classiﬁcation. We consider three popular datasets –
MNIST [86], Fashion-MNIST [87] and KMNIST [88]. We
choose a threshold for binarizing these datasets since they
5. z(0) ∼N(0, I) for the ﬁrst iteration of IP.
6. In this computation we have assumed, for simplicity, a unit cost for
any operation that was computed in a batch concurrently on a GPU.
are originally grayscale. We choose the query set Q as the
set of all w × w overlapping patch locations in the image.
The answer q(X) for any q ∈Q is the w2 pixel intensities
observed at the patch indexed by location q. This choice of
Q reﬂects the user’s desire to know which parts of the input
image are most informative for a particular prediction, a
common practice for explainability in vision tasks [25]. We
conduct experiments for multiple values of w and conclude
that w = 3 provides a good trade-off between the required
number of queries and the interpretability of each query.
Note that when w > 1 the factorization in (12) that we
use to model p(Q(x), y, z) and compute mutual informa-
tion no longer holds as the overlapping queries q(X) are
now causally related (and therefore dependent even when
conditioned on Z, making them unable to be modeled by
a VAE). So instead of training a VAE to directly model
the query set p(Q(x) | y, z), we train a VAE to model
the pixel distribution p(x | y, z), and then compute the
probability distribution over the patch query p(q(x) | z, y)
as the product of the probabilities of all pixels in that patch.7
IP in action. Fig. 4(a) illustrates the decision-making process
of IP using 3 × 3 patch queries on an image xobs of a 6
from the MNIST test set. The ﬁrst query is near the center of
the image; recall from (6) that this choice is independent of
the particular input image and represents the patch whose
pixel intensities have maximum mutual information with
Y (the class label). The updated posterior, p
 Y | SIP
1 (xobs)

,
concentrates most of its mass on the digit “1”, perhaps
because most of the other digits do not commonly have a
vertical piece of stroke at the center patch. However, the next
query (about three pixels below the center patch) reveals
a horizontal stroke and the posterior mass over the labels
immediately shifts to {2, 3, 6, 8}. The next two queries are
well-suited to discerning between these four possibilities
and we see that after asking 4 questions, IP is more than
90% conﬁdent that the image is a 6. Such rich explanations
in terms of querying informative patches based on what is
observed so far and seeing how the belief p
 Y | SIP
k (xobs)

of the model evolves over time is missing from post-hoc at-
tribution methods which output static importance scores for
every pixel towards the black-box model’s ﬁnal prediction.
Explanation length vs. task complexity. Fig. 6 shows that IP
requires an average of 5.2, 12.9 and 14.5 queries of size 3×3
to predict the label with 99% conﬁdence (ϵ = 0.01 in (8))
on MNIST, KMNIST and FashionMNIST, respectively. This
reﬂects the intuition that more complex tasks require longer
explanations. For reference, state-of-the-art deep networks
on these datasets obtain test accuracies in order MNIST ≥
KMNIST ≥FashionMNIST (see last row in Table 1).
Effect of patch size on interpretability. We also run IP
on MNIST with patch sizes of 1 × 1 (single pixels), 2 × 2,
3 × 3, and 4 × 4. We observed that IP terminates at 99%
conﬁdence after 21.1, 9.6, 5.2, and 4.6 queries on average,
respectively. While this suggests that larger patches lead to
shorter explanations, we note that explanations with larger
patches use more pixels (e.g. on MNIST, IP uses 21.1 pixels
7. Since the patches overlap in our query set, when computing the
conditional probability of a patch query given history we only consider
the probability of the pixels in the patch that have not yet been observed
in our history.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
9
p
!
Y | SIP
k (xobs)
"
p
!
Y | SIP
k (xobs)
"
Fig. 4. (a) IP on MNIST. The top row displays the test image with red boxes denoting the current queried patch and blue boxes denoting previous
patches. The second row shows the revealed portion of the image that IP gets to use at each query. The ﬁnal row shows the model’s estimated
posteriors at each query, beginning at a nearly uniform prior before converging on the true digit “6” after 4 queries. (b) IP on CUB Bird Species
Classiﬁcation. On the left we show the input image and on the right we have a heatmap of the estimated class probabilities at each iteration. We
only show the top 10 most probable classes out of the 200. To the right, we display the queries asked at each iteration, with red indicating a “no”
response and green a “yes” response. (c) IP on HuffPost News. We show the input news item and a heatmap depicting the evolution of topic
probabilities as IP asks queries and gathers answers. Words colored in red are absent from the sentence while words in green are present. For our
visualization, we compute the KL divergence between each successive posterior and plot only the top 20 queries that led to the greatest change in
posterior class probabilities.
on average for 1 × 1 patches and 54.7 pixels on average for
4 × 4 patches). That being said, very small patch queries are
hard to interpret (see Fig. 5) and very large patch queries are
also hard to interpret since each patch contains many image
features. Overall, we found that 3 × 3 patches represented
the right trade-off between interpretability in terms of edge
patterns and minimality of the explanations. Speciﬁcally,
single pixels are not very interpretable to humans but the
explanations generated are more efﬁcient in terms of number
of pixels needed to predict the label. On the other extreme,
using the entire image as a query is not interesting from
an interpretability point of view since it does not help us
understand which parts of the image are salient for predic-
tion. We refer the reader to Appendix B.3.1 for additional
patch size examples and quantitative analysis.
5.0.2
Concept-Based Queries
Task and query set. What if the end-user is interested in a
more semantic explanation of the decision process in terms
of high-level concepts? This can be easily incorporated into
our framework by choosing an appropriate query set Q.
k=0
k=1
k=2
k=3
k=4
k=6
k=21
0123456789
0.0
0.5
1.0
p(Y
SIP
k ((xobs)))
0123456789
0123456789
0123456789
0123456789
0123456789
0123456789
Fig. 5. IP with 1 × 1 patches on MNIST. Through the ﬁrst 6 iterations,
IP asks queries in the same center vertical region as in Fig. 4(a) (which
uses 3×3 queries), outlining the distinctive loop in the bottom of the “6”.
However, reaching 99% conﬁdence requires a total of 21 1 × 1 queries
as opposed to just 4 3 × 3 ones. For conciseness, we show only the
6 queries that led to the greatest KL divergence between successive
posterior class probabilities.
As an example we consider the challenging task of bird
species classiﬁcation on the Caltech-UCSD Birds-200-2011
(CUB) dataset [89]. The dataset contains images of 200

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
10
different species of birds. Each image is annotated with
312 binary attributes representing high-level concepts, such
as the colour and shape of the beak, wings, and head.
Unfortunately, these attribute annotations are very noisy. We
follow [24] in deciding attribute labels by majority voting.
For example, if more than 50% of images in a class have
black wings, then we set all images in that class to have
black wings. We construct Q by choosing a query for asking
the presence/absence of each of these 312 binary attributes.
Unfortunately, attribute annotations are not available at test
time. To remedy this, we train a CNN (see [24] for details) to
answer each query using the training set annotations, which
is then used to answer queries at test time. Subsequently,
we learn a VAE to model the joint distribution of query-
answers supplied by this CNN (instead of the ground truth
annotations) and Y , so our generative model can account
for any estimation errors incurred by the CNN. Finally, we
carry out IP as explained in §4.
IP in action. Consider the image of a Great Crested Flycatcher
in Fig. 4(b). IP proceeds by asking most informative queries
about various bird attributes progressively making the pos-
terior over the species labels more and more peaked. After 5
queries, IP has gathered that the image is of a bird that has
a perching-like shape, all-purpose beak and yellow belly,
but does not have a yellow throat nor yellow upperparts.
This results in a posterior concentrated on just 4 species that
exhibit these characteristics. IP then proceeds to discount
Green Jay and Scott Oriole which have black breasts with
query 6. Likewise, Tropical Kingbirds have grayish back and
is segregated from Great Crested Flycatchers which have buff-
coloured backs with query 7. Finally after 9 queries, IP is
99% conﬁdent about the current class. Such concept-based
explanations are more accessible to non-experts, especially
on ﬁne-grained classiﬁcation datasets, which typically re-
quire domain expertise. On average IP takes 14.7 queries to
classify a given bird image with ϵ = 0.007 as the stopping
criteria (See (7)).
5.0.3
Word-based Queries
Task and query set. Our framework can also be successfully
applied to other domains like NLP. As an example we
consider the task of topic identiﬁcation from newspaper ex-
tended headlines (headline + short description ﬁeld) using
the the Hufﬁngton Post News Category Dataset [90]. We
adopt a simple query set that consists of binary queries
probing the existence of words in the extended headline.
The words are chosen from a pre-deﬁned vocabulary ob-
tained by stemming all words in the HuffPost dataset and
choosing the top-1,000 according to their tf-idf scores [91].
We process the dataset to merge redundant categories (such
as Style & Beauty and Beauty & Style), remove semantically
ambiguous, HuffPost-speciﬁc categories (e.g. Impact or Fifty)
and remove categories with few samples, arriving at 10 ﬁnal
categories (see Appendix B.1).
IP in action. Fig. 4(c) shows an example run of IP on the
HuffPost dataset. Note that positive responses to queries are
very sparse, since each extended headline only contains 8.6
words on average out of the 1,000 in the vocabulary. As a re-
sult, IP asks 125 queries before termination. As discussed in
§2, such long decision paths would be impossible in decision
trees due to data fragmentation and memory limitations.
For clarity of presentation we only show the 20 queries
with the greatest impact on the estimated posterior (as
measured by KL-divergence from previous posterior). Upon
reaching the ﬁrst positive query “eat”, the probability mass
concentrates on the categories Food & Drink and Wellness
with little mass on Travel. However, as the queries about the
existence of “citi”, “visit”, “york”, and “bar” in the extended
headline come back positive, the model becomes more and
more conﬁdent that “Travel” is the correct class. IP requires
about 199.3 queries on average to predict the topic of the
extended headline with ϵ = 10−3 as the stopping criteria
(See (7)). Additional details on the HuffPost query set are in
Appendix B.1.
Further examples of IP performing inference on all tasks
can be found in Appendix B.3.
5.1
Quantitative Evaluation
5.1.1
Classiﬁcation Accuracy
We compare the classiﬁcation accuracy of our model’s pre-
diction based on the query-answers gathered by IP until
termination with several other baseline models. For each of
the models considered, we ﬁrst give a brief description and
then comment on their performance with respect to IP. All
the results are summarized in Table 1.
DECISION TREE refers to standard classiﬁcation trees
learnt using the popular CART algorithm [34]. In the In-
troduction, we mentioned that classical decision trees learnt
using Q to supply the node splitting functions will be
intepretable by construction but are not competitive with
state-of-the-art methods. This is illustrated in our results in
Table 1. Across all datasets, IP obtains superior performance
since it is based on an underlying generative model (VAE)
and only computes the branch of the tree traversed by the
input data in an online manner, thus it is not shackled by
data fragmentation and memory limitations.
MAP USING Q refers to the Maximum A Posteriori
estimate obtained using the posterior distribution over the
labels given the answers to all the queries in Q (for a given
input). Recall, IP asks queries until the stopping criteria is
reached (Equation (7) & Equation (8)). Naturally, there is
a trade-off between the length of the explanations and the
predictive performance observed. If we ask all the queries
then the resulting explanations of length |Q| might be too
long to be desirable. The results for IP reported in Table 1
use different dataset-speciﬁc stopping criteria according to
the elbow in their respective accuracy vs. explanation length
curves (see Fig. 6). On the binary image datasets, (MNIST,
KMNIST, and FashionMNIST) IP obtains an accuracy within
3% of the best achievable upon seeing all the query-answers
with only about 2% of the total queries in Q. Similarly for
the CUB and Huffpost datasets, IP achieves about the same
accuracy as MAP USING Q but asks less than 5% and 20%
of total possible queries respectively.
BLACK-BOX USING Q refers to the best performing deep
network model we get by training on features supplied
by evaluating all q ∈Q on input data from the various
training datasets. For the binary image datasets, this is just
a 4-layer CNN with ReLU activations. For CUB we use
the results reported by the sequential model in [24]. For

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
11
0
5
10
15
...
676
Average Explanation Length
0.00
0.25
0.50
0.75
1.00
Test Accuracy
0
5
10
15
...
310
Average Explanation Length
0
50 100 150 200 250 ... 1000
Average Explanation Length
MNIST
KMNIST
FashionMNIST
CUB
HuffPost News
Fig. 6. Trade-off between predictive performance and explanation length Different points along the curves correspond to different values of ϵ
as the stopping criteria (7) is varied. The colored dotted vertical line in each plot indicates the avg. explanation length v/s test accuracy at the ϵ
value used as the stopping criteria for reporting results for the IP strategy in this work. For each plot, the x-axis ranges from 0 to the size of the
query set, |Q|, chosen for that task.
HuffPost, we found a single hidden layer with ReLU non-
linearity give the best performance. Further architectural
and training details are in Appendix B.2. In Table 1 we show
that across all datasets, the predictive performance obtained
by MAP USING Q is on par with the best performance we
obtained using black-box expressive non-interpretable net-
works BLACK-BOX USING Q. Thus, our generative models,
which form the backbone for IP, are competitive with state-
of-the-art prediction methods.
BLACK-BOX refers to the best performing black-box
model on these datasets in terms of classiﬁcation accuracy as
reported in literature; to the best of our knowledge. In Table
1, we see a performance gap in each dataset when compared
with MAP USING Q which uses an interpretable query set.
This is expected since explainability can be viewed as an
additional constraint on learning. For example, on Fash-
ionMNIST we see an almost 8.5% relative fall in accuracy
due to binarization. This is because it is harder to decipher
between some classes like shirts and pullovers at the binary
level. On the other hand, binary patches are easily inter-
pretable as edges, foregrounds and backgrounds. Similarly,
there is a relative drop of accuracy of about 17% for the
HuffPost dataset since our queries regarding the existence of
different words ignore their arrangement in sentences. Thus
we lose crucial contextual information used by state-of-the-
art transformer models [92]. Ideally, we would like query
sets to be easily interpretable, lead to short explanations and
be sufﬁcient to solve the task. Finding such query sets is
nontrivial and will be explored in future work.
TABLE 1
Classiﬁcation accuracy of our model (Information Pursuit) relative
to baselines on different test sets. See 5.1.1 for details on each
model.
Model
MNIST
KMNIST
Fashion
CUB
HuffPost
INFORMATION PURSUIT
96.78%
91.02%
85.60%
76.73%
71.21%
DECISION TREE [34]
90.23%
78.00%
80.80%
68.80%
63.00%
MAP USING Q
99.05%
94.25%
87.56%
76.80%
71.72%
BLACK-BOX USING Q
99.15%
95.10%
88.43%
76.30%
71.48%
BLACK-BOX
99.83% [93]
98.83% [88]
96.70% [94]
82.70% [24]
86.45% 8
8. We ﬁne-tuned a Bert Large Uncased Transformer model [92] with
the last layer replaced with a linear one. See Appendix B.2.3 for details.
5.1.2
Comparison to current attribution methods
At ﬁrst glance, it might seem that using attribution meth-
ods/saliency maps can provide the same insights as to
which parts of the image or more generally which queries
in Q were most inﬂuential in a decision made by a black-
box model trained on input features supplied by all the
query-answers. However, the unreliability of these methods
in being faithful to the model they try to explain brings
their utility into question [19], [20], [22]. We conjecture that
this is because current attribution methods are not designed
to generate explanations that are sufﬁcient statistics of the
model’s prediction. We illustrate this with a simple experi-
ment using our binary image classiﬁcation datasets.
For each input image x, we compute the corresponding
attribution map e(x) for the model’s predicted class using
two popular attribution methods, Integrated gradients (IG)
[95] and DeepSHAP [45]. We then compute the L most
important 3 × 3 patches, where L is the number of patches
queried by IP for that particular input image. For comput-
ing the attribution/importance of a patch we average the
attributions of all the pixels in that patch (following [20]).
We proceed as follows: (i) Given e(x), compute the patch
with maximum attribution and add these pixels to our ex-
planation, (ii) Zero-out the attributions of all the pixels in the
previously selected patch and repeat step (i) until L patches
are selected. The ﬁnal explanation consists of L possibly
overlapping patches. Now, we evaluate the sufﬁciency of
the generated explanation for the model’s prediction by esti-
mating the MAP accuracy of the posterior over labels given
the intensities in the patches included in this explanation.
This is done via a VAE trained to learn the joint distribution
over image pixels and class labels. We experiment with both
the raw attribution scores returned by IG and DeepSHAP
and also the absolute values of the attribution scores for
e(x). The results are reported in Table 2. In almost all cases
(with the exception of DeepSHAP on FashionMNIST), IP
generates explanations that are more predictive of the class
label than popular attribution methods.
6
CONCLUSION
We have presented a step towards building trustwor-
thy interpretable machine learning models that respect
the domain- and user-dependent nature of interpretabil-
ity. We address this by composing user-deﬁned, inter-

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
12
TABLE 2
MAP accuracy of explanations generated by Information Pursuit
(IP) v/s other attribution methods. IP explanations (in almost all
cases) achieve a higher classiﬁcation accuracy than explanations of the
same length generated using baseline attribution methods. The
(absolute) method refers to explanations generated using absolute
values of the attribution map scores. On MNIST and KMNIST, IP
explanations achieve a 10% and 2.38% relative improvement
respectively over the best performing baseline method. On
FashionMNIST, IP explanations are second best with a relative
decrease of about 3.12% from the best performing baseline.
Explanation Method
MNIST
KMNIST
Fashion-MNIST
INFORMATION PURSUIT
96.78%
91.02%
85.60%
IG
78.48%
84.87%
78.49%
IG (ABSOLUTE)
70.39%
84.72%
64.95%
DEEPSHAP
87.98%
88.90%
88.36%
DEEPSHAP (ABSOLUTE)
84.80%
84.56%
84.35%
pretable queries into concise explanations. Furthermore,
unlike many contemporary attempts at explainability, our
method is not post-hoc, but is interpretable by design and
guaranteed to produce faithful explanations. We formulate
a tractable approach to implement this framework through
deep generative models, MCMC algorithms, and the infor-
mation pursuit algorithm. Finally, we demonstrate the effec-
tiveness of our method across various vision and language
tasks at generating concise explanations describing the un-
derlying reasoning process behind the prediction. Future
work will be aimed at extending the proposed framework
to more complex tasks beyond classiﬁcation such as scene
parsing, image captioning, and sentiment analysis.
ACKNOWLEDGMENTS
The authors thank Mar´ıa P´erez Ortiz and John Shawe-Taylor
for their contributions to the design of the experiments
on document classiﬁcation presented in Section 5.0.3. This
research was supported by the Army Research Ofﬁce under
the Multidisciplinary University Research Initiative contract
W911NF-17-1-0304 and by the NSF grant 2031985.
REFERENCES
[1]
D. Gunning, M. Steﬁk, J. Choi, T. Miller, S. Stumpf, and G.-Z. Yang,
“Xai—explainable artiﬁcial intelligence,” Science Robotics, vol. 4,
no. 37, p. eaay7120, 2019.
[2]
C. Rudin, “Stop explaining black box machine learning models
for high stakes decisions and use interpretable models instead,”
Nature Machine Intelligence, vol. 1, no. 5, pp. 206–215, 2019.
[3]
W. J. Murdoch, C. Singh, K. Kumbier, R. Abbasi-Asl, and B. Yu,
“Interpretable machine learning: deﬁnitions, methods, and appli-
cations,” arXiv preprint arXiv:1901.04592, 2019.
[4]
European Commission, “Building trust in human-centric artiﬁcial
intelligence,” Communication from the Commission to the European
Parliament, the Council, the European Economic and Social Committee
and the Committee of the Regions, vol. 168, 2019.
[5]
United States Food and Drug Administration, “Virtual pub-
lic workshop - transparency of artiﬁcial intelligence/machine
learning-enabled medical devices,” Transcript: https://www.fda.
gov/media/154423/download, Oct. 14, 2021.
[6]
U. Johansson, C. S¨onstr¨od, U. Norinder, and H. Bostr¨om, “Trade-
off between accuracy and interpretability for predictive in silico
modeling,” Future medicinal chemistry, vol. 3, no. 6, pp. 647–663,
2011.
[7]
J. Wanner, L.-V. Herm, K. Heinrich, and C. Janiesch, “Stop or-
dering machine learning algorithms by their explainability! an
empirical investigation of the tradeoff between performance and
explainability,” in Conference on e-Business, e-Services and e-Society.
Springer, 2021, pp. 245–258.
[8]
F. K. Doˇsilovi´c, M. Brˇci´c, and N. Hlupi´c, “Explainable artiﬁcial
intelligence: A survey,” in 2018 41st International convention on
information and communication technology, electronics and microelec-
tronics (MIPRO).
IEEE, 2018, pp. 0210–0215.
[9]
A. B. Arrieta, N. D´ıaz-Rodr´ıguez, J. Del Ser, A. Bennetot, S. Tabik,
A. Barbado, S. Garc´ıa, S. Gil-L´opez, D. Molina, R. Benjamins et al.,
“Explainable artiﬁcial intelligence (xai): Concepts, taxonomies,
opportunities and challenges toward responsible ai,” Information
fusion, vol. 58, pp. 82–115, 2020.
[10] D. Gunning and D. Aha, “Darpa’s explainable artiﬁcial intelli-
gence (xai) program,” AI magazine, vol. 40, no. 2, pp. 44–58, 2019.
[11] D. Baehrens, T. Schroeter, S. Harmeling, M. Kawanabe, K. Hansen,
and K.-R. M¨uller, “How to explain individual classiﬁcation deci-
sions,” The Journal of Machine Learning Research, vol. 11, pp. 1803–
1831, 2010.
[12] K. Simonyan, A. Vedaldi, and A. Zisserman, “Deep inside con-
volutional networks: Visualising image classiﬁcation models and
saliency maps,” arXiv preprint arXiv:1312.6034, 2013.
[13] S. Kolek, D. A. Nguyen, R. Levie, J. Bruna, and G. Kutyniok, “A
rate-distortion framework for explaining black-box model deci-
sions,” in International Workshop on Extending Explainable AI Beyond
Deep Models and Classiﬁers.
Springer, 2022, pp. 91–115.
[14] A. Shrikumar, P. Greenside, and A. Kundaje, “Learning important
features through propagating activation differences,” in Interna-
tional conference on machine learning.
PMLR, 2017, pp. 3145–3153.
[15] M. D. Zeiler and R. Fergus, “Visualizing and understanding
convolutional networks,” in European conference on computer vision.
Springer, 2014, pp. 818–833.
[16] R. R. Selvaraju, M. Cogswell, A. Das, R. Vedantam, D. Parikh, and
D. Batra, “Grad-cam: Visual explanations from deep networks via
gradient-based localization,” in Proceedings of the IEEE international
conference on computer vision, 2017, pp. 618–626.
[17] D. Smilkov, N. Thorat, B. Kim, F. Vi´egas, and M. Wattenberg,
“Smoothgrad: removing noise by adding noise,” arXiv preprint
arXiv:1706.03825, 2017.
[18] A. Subramanya, V. Pillai, and H. Pirsiavash, “Fooling network in-
terpretation in image classiﬁcation,” in Proceedings of the IEEE/CVF
International Conference on Computer Vision, 2019, pp. 2020–2029.
[19] J. Adebayo, J. Gilmer, M. Muelly, I. Goodfellow, M. Hardt, and
B. Kim, “Sanity checks for saliency maps,” Advances in neural
information processing systems, vol. 31, 2018.
[20] M. Yang and B. Kim, “Benchmarking attribution methods with
relative feature importance,” arXiv preprint arXiv:1907.09701, 2019.
[21] P.-J. Kindermans, S. Hooker, J. Adebayo, M. Alber, K. T. Sch¨utt,
S. D¨ahne, D. Erhan, and B. Kim, “The (un) reliability of saliency
methods,” in Explainable AI: Interpreting, Explaining and Visualizing
Deep Learning.
Springer, 2019, pp. 267–280.
[22] H. Shah, P. Jain, and P. Netrapalli, “Do input gradients highlight
discriminative features?” Advances in Neural Information Processing
Systems, vol. 34, 2021.
[23] D. Slack, S. Hilgard, E. Jia, S. Singh, and H. Lakkaraju, “Fooling
lime and shap: Adversarial attacks on post hoc explanation meth-
ods,” in Proceedings of the AAAI/ACM Conference on AI, Ethics, and
Society, 2020, pp. 180–186.
[24] P. W. Koh, T. Nguyen, Y. S. Tang, S. Mussmann, E. Pierson,
B. Kim, and P. Liang, “Concept bottleneck models,” in International
Conference on Machine Learning.
PMLR, 2020, pp. 5338–5348.
[25] C. Chen, O. Li, D. Tao, A. Barnett, C. Rudin, and J. K. Su, “This
looks like that: deep learning for interpretable image recognition,”
Advances in neural information processing systems, vol. 32, 2019.
[26] C. Rudin, C. Chen, Z. Chen, H. Huang, L. Semenova, and
C. Zhong, “Interpretable machine learning: Fundamental princi-
ples and 10 grand challenges,” Statistics Surveys, vol. 16, pp. 1–85,
2022.
[27] T. M. Janssen and B. H. Partee, “Compositionality,” in Handbook of
logic and language.
Elsevier, 1997, pp. 417–473.
[28] H. Lakkaraju, S. H. Bach, and J. Leskovec, “Interpretable decision
sets: A joint framework for description and prediction,” in Proceed-
ings of the 22nd ACM SIGKDD international conference on knowledge
discovery and data mining, 2016, pp. 1675–1684.
[29] A. Wan, L. Dunlap, D. Ho, J. Yin, S. Lee, S. Petryk, S. A. Bargal,
and J. E. Gonzalez, “{NBDT}: Neural-backed decision tree,” in

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
13
International Conference on Learning Representations, 2021. [Online].
Available: https://openreview.net/forum?id=mCLVeEpplNE
[30] J. Mu and J. Andreas, “Compositional explanations of neurons,”
Advances in Neural Information Processing Systems, vol. 33, pp.
17 153–17 163, 2020.
[31] E. Jahangiri, E. Yoruk, R. Vidal, L. Younes, and D. Geman, “In-
formation pursuit: A bayesian framework for sequential scene
parsing,” arXiv preprint arXiv:1701.02343, 2017.
[32] D. Bau, B. Zhou, A. Khosla, A. Oliva, and A. Torralba, “Network
dissection: Quantifying interpretability of deep visual representa-
tions,” in Proceedings of the IEEE conference on computer vision and
pattern recognition, 2017, pp. 6541–6549.
[33] E. Hernandez, S. Schwettmann, D. Bau, T. Bagashvili, A. Torralba,
and J. Andreas, “Natural language descriptions of deep visual
features,” arXiv preprint arXiv:2201.11114, 2022.
[34] L. Breiman, J. H. Friedman, R. A. Olshen, and C. J. Stone, Classiﬁ-
cation and regression trees.
Routledge, 2017.
[35] J. R. Quinlan, “Induction of decision trees,” Machine learning,
vol. 1, no. 1, pp. 81–106, 1986.
[36] Y. Amit and D. Geman, “Shape quantization and recognition with
randomized trees,” Neural computation, vol. 9, no. 7, pp. 1545–1588,
1997.
[37] L. Breiman, “Random forests,” Machine learning, vol. 45, no. 1, pp.
5–32, 2001.
[38] R. Caruana and A. Niculescu-Mizil, “An empirical comparison
of supervised learning algorithms,” in Proceedings of the 23rd
international conference on Machine learning, 2006, pp. 161–168.
[39] M. Fern´andez-Delgado, E. Cernadas, S. Barro, and D. Amorim,
“Do we need hundreds of classiﬁers to solve real world classiﬁ-
cation problems?” The journal of machine learning research, vol. 15,
no. 1, pp. 3133–3181, 2014.
[40] H. Xu, K. A. Kinfu, W. LeVine, S. Panda, J. Dey, M. Ainsworth,
Y.-C. Peng, M. Kusmanov, F. Engert, C. M. White et al., “When are
deep networks really better than decision forests at small sample
sizes, and how?” arXiv preprint arXiv:2108.13637, 2021.
[41] P. Kontschieder, M. Fiterau, A. Criminisi, and S. R. Bulo, “Deep
neural decision forests,” in Proceedings of the IEEE international
conference on computer vision, 2015, pp. 1467–1475.
[42] D. Geman and B. Jedynak, “An active testing model for tracking
roads in satellite images,” IEEE Transactions on Pattern Analysis and
Machine Intelligence, vol. 18, no. 1, pp. 1–14, 1996.
[43] A. Chattopadhay, A. Sarkar, P. Howlader, and V. N. Balasubra-
manian, “Grad-cam++: Generalized gradient-based visual expla-
nations for deep convolutional networks,” in 2018 IEEE winter
conference on applications of computer vision (WACV).
IEEE, 2018,
pp. 839–847.
[44] M. T. Ribeiro, S. Singh, and C. Guestrin, “” why should i trust
you?” explaining the predictions of any classiﬁer,” in Proceedings
of the 22nd ACM SIGKDD international conference on knowledge
discovery and data mining, 2016, pp. 1135–1144.
[45] S. M. Lundberg and S.-I. Lee, “A uniﬁed approach to interpret-
ing model predictions,” Advances in neural information processing
systems, vol. 30, 2017.
[46] B. Kim, M. Wattenberg, J. Gilmer, C. Cai, J. Wexler, F. Viegas et al.,
“Interpretability beyond feature attribution: Quantitative testing
with concept activation vectors (tcav),” in International conference
on machine learning.
PMLR, 2018, pp. 2668–2677.
[47] B. Zhou, Y. Sun, D. Bau, and A. Torralba, “Interpretable basis de-
composition for visual explanation,” in Proceedings of the European
Conference on Computer Vision (ECCV), 2018, pp. 119–134.
[48] C.-K. Yeh, B. Kim, S. Arik, C.-L. Li, T. Pﬁster, and P. Ravikumar,
“On completeness-aware concept-based explanations in deep neu-
ral networks,” Advances in Neural Information Processing Systems,
vol. 33, pp. 20 554–20 565, 2020.
[49] D. Alvarez Melis and T. Jaakkola, “Towards robust interpretability
with self-explaining neural networks,” Advances in neural informa-
tion processing systems, vol. 31, 2018.
[50] M. Bohle, M. Fritz, and B. Schiele, “Convolutional dynamic align-
ment networks for interpretable classiﬁcations,” in Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
2021, pp. 10 029–10 038.
[51] M. Wu, S. Parbhoo, M. C. Hughes, V. Roth, and F. Doshi-Velez,
“Optimizing for interpretability in deep neural networks with tree
regularization,” Journal of Artiﬁcial Intelligence Research, vol. 72, pp.
1–37, 2021.
[52] V. Pillai and H. Pirsiavash, “Explainable models with consistent
interpretations,” UMBC Student Collection, 2021.
[53] Z. Chen, Y. Bei, and C. Rudin, “Concept whitening for inter-
pretable image recognition,” Nature Machine Intelligence, vol. 2,
no. 12, pp. 772–782, 2020.
[54] M. De-Arteaga, A. Romanov, H. Wallach, J. Chayes, C. Borgs,
A. Chouldechova, S. Geyik, K. Kenthapadi, and A. T. Kalai,
“Bias in bios: A case study of semantic representation bias in a
high-stakes setting,” in proceedings of the Conference on Fairness,
Accountability, and Transparency, 2019, pp. 120–128.
[55] A. Galassi, M. Lippi, and P. Torroni, “Attention in natural language
processing,” IEEE Transactions on Neural Networks and Learning
Systems, vol. 32, no. 10, pp. 4291–4308, 2020.
[56] D. Pruthi, M. Gupta, B. Dhingra, G. Neubig, and Z. C. Lipton,
“Learning to deceive with attention-based explanations,” arXiv
preprint arXiv:1909.07913, 2019.
[57] M. Craven and J. Shavlik, “Extracting tree-structured representa-
tions of trained networks,” Advances in neural information processing
systems, vol. 8, 1995.
[58] D. Dancey, D. A. McLean, and Z. A. Bandar, “Decision tree extrac-
tion from trained neural networks,” in Proceedings of the Nineteenth
Conference on Artiﬁcial Intelligence.
American Association for
Artiﬁcial Intelligence, 2004.
[59] N. Frosst and G. Hinton, “Distilling a neural network into a soft
decision tree,” arXiv preprint arXiv:1711.09784, 2017.
[60] A. Roy and S. Todorovic, “Monocular depth estimation using
neural regression forest,” in Proceedings of the IEEE conference on
computer vision and pattern recognition, 2016, pp. 5506–5514.
[61] U. C. Bic¸ici, C. Keskin, and L. Akarun, “Conditional information
gain networks,” in 2018 24th International Conference on Pattern
Recognition (ICPR).
IEEE, 2018, pp. 1390–1395.
[62] V. N. Murthy, V. Singh, T. Chen, R. Manmatha, and D. Comaniciu,
“Deep decision network for multi-class image classiﬁcation,” in
Proceedings of the IEEE conference on computer vision and pattern
recognition, 2016, pp. 2240–2248.
[63] R. Vilalta, G. Blix, and L. Rendell, “Global data analysis and the
fragmentation problem in decision tree induction,” in European
Conference on Machine Learning.
Springer, 1997, pp. 312–326.
[64] N. Tishby, F. C. Pereira, and W. Bialek, “The information bottleneck
method,” arXiv preprint physics/0004057, 2000.
[65] K. Chaloner and I. Verdinelli, “Bayesian experimental design: A
review,” Statistical Science, pp. 273–304, 1995.
[66] R. Sznitman and B. Jedynak, “Active testing for face detection
and localization,” IEEE Transactions on Pattern Analysis and Machine
Intelligence, vol. 32, no. 10, pp. 1914–1920, 2010.
[67] M. Cuturi, O. Teboul, Q. Berthet, A. Doucet, and J.-P. Vert, “Noisy
adaptive group testing using bayesian sequential experimental
design,” arXiv preprint arXiv:2004.12508, 2020.
[68] S. Branson, G. Van Horn, C. Wah, P. Perona, and S. Belongie, “The
ignorant led by the blind: A hybrid human–machine vision system
for ﬁne-grained categorization,” International Journal of Computer
Vision, vol. 108, no. 1, pp. 3–29, 2014.
[69] V. Mnih, N. Heess, A. Graves et al., “Recurrent models of visual
attention,” in Advances in neural information processing systems,
2014, pp. 2204–2212.
[70] G. Elsayed, S. Kornblith, and Q. V. Le, “Saccader: improving
accuracy of hard attention models for vision,” in Advances in
Neural Information Processing Systems, 2019, pp. 702–714.
[71] M. Li, S. S. Ge, and T. H. Lee, “Glance and glimpse network:
A stochastic attention model driven by class saliency,” in Asian
Conference on Computer Vision.
Springer, 2016, pp. 572–587.
[72] H. Li, P. Wang, C. Shen, and A. v. d. Hengel, “Visual question an-
swering as reading comprehension,” in Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, 2019, pp.
6319–6328.
[73] M. Malinowski, M. Rohrbach, and M. Fritz, “Ask your neurons:
A deep learning approach to visual question answering,” Interna-
tional Journal of Computer Vision, vol. 125, no. 1, pp. 110–135, 2017.
[74] J. Mao, C. Gan, P. Kohli, J. B. Tenenbaum, and J. Wu, “The neuro-
symbolic concept learner: Interpreting scenes, words, and sen-
tences from natural supervision,” arXiv preprint arXiv:1904.12584,
2019.
[75] K. J. Shih, S. Singh, and D. Hoiem, “Where to look: Focus regions
for visual question answering,” in Proceedings of the IEEE conference
on computer vision and pattern recognition, 2016, pp. 4613–4621.
[76] J. Lu, J. Yang, D. Batra, and D. Parikh, “Hierarchical question-
image co-attention for visual question answering,” Advances in
neural information processing systems, vol. 29, 2016.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
14
[77] J. Andreas, M. Rohrbach, T. Darrell, and D. Klein, “Neural module
networks,” in Proceedings of the IEEE conference on computer vision
and pattern recognition, 2016, pp. 39–48.
[78] H. Laurent and R. L. Rivest, “Constructing optimal binary decision
trees is np-complete,” Information processing letters, vol. 5, no. 1, pp.
15–17, 1976.
[79] M. I. Belghazi, A. Baratin, S. Rajeswar, S. Ozair, Y. Bengio,
A. Courville, and R. D. Hjelm, “Mine: mutual information neural
estimation,” arXiv preprint arXiv:1801.04062, 2018.
[80] D. P. Kingma and M. Welling, “Auto-encoding variational bayes,”
arXiv preprint arXiv:1312.6114, 2013.
[81] A. Doucet, A. M. Johansen et al., “A tutorial on particle ﬁltering
and smoothing: Fifteen years later,” Handbook of nonlinear ﬁltering,
vol. 12, no. 656-704, p. 3, 2009.
[82] A. Jalal, S. Karmalkar, A. Dimakis, and E. Price, “Instance-optimal
compressed sensing via posterior sampling,” Proceedings of Ma-
chine Learning Research, vol. 139, 2021.
[83] E. Nijkamp, M. Hill, T. Han, S.-C. Zhu, and Y. N. Wu, “On the
anatomy of mcmc-based maximum likelihood learning of energy-
based models,” in Proceedings of the AAAI Conference on Artiﬁcial
Intelligence, vol. 34, no. 04, 2020, pp. 5272–5280.
[84] A. Durmus and E. Moulines, “High-dimensional bayesian infer-
ence via the unadjusted langevin algorithm,” Bernoulli, vol. 25,
no. 4A, pp. 2854–2882, 2019.
[85] M. Welling and Y. W. Teh, “Bayesian learning via stochastic
gradient langevin dynamics,” in Proceedings of the 28th international
conference on machine learning (ICML-11).
Citeseer, 2011, pp. 681–
688.
[86] Y. LeCun, L. Bottou, Y. Bengio, and P. Haffner, “Gradient-based
learning applied to document recognition,” Proceedings of the IEEE,
vol. 86, no. 11, pp. 2278–2324, 1998.
[87] H. Xiao, K. Rasul, and R. Vollgraf, “Fashion-mnist: a novel image
dataset for benchmarking machine learning algorithms,” arXiv
preprint arXiv:1708.07747, 2017.
[88] T. Clanuwat, M. Bober-Irizar, A. Kitamoto, A. Lamb, K. Yamamoto,
and D. Ha, “Deep learning for classical japanese literature,” arXiv
preprint arXiv:1812.01718, 2018.
[89] C. Wah, S. Branson, P. Welinder, P. Perona, and S. Belongie, “The
caltech-ucsd birds-200-2011 dataset,” 2011.
[90] R. Misra, “News category dataset,” 06 2018.
[91] M. Lavin, “Analyzing documents with tf-idf,” 2019.
[92] J. Devlin, M.-W. Chang, K. Lee, and K. Toutanova, “Bert: Pre-
training of deep bidirectional transformers for language under-
standing,” arXiv preprint arXiv:1810.04805, 2018.
[93] J. Hu, L. Shen, and G. Sun, “Squeeze-and-excitation networks,”
in Proceedings of the IEEE conference on computer vision and pattern
recognition, 2018, pp. 7132–7141.
[94] H. Xiao, K. Rasul, and R. Vollgraf, “Fashion-mnist: A mnist-like
fashion product database,” in GitHub, 2017.
[95] M. Sundararajan, A. Taly, and Q. Yan, “Axiomatic attribution for
deep networks,” in International conference on machine learning.
PMLR, 2017, pp. 3319–3328.
Aditya Chattopadhyay is a PhD student in the
Computer Science Department, Johns Hopkins
University. He received the Bachelor of Technol-
ogy degree in Computer Science and Master of
Science by Research degree in Computational
Natural Sciences from the International Institute
of Information Technology, Hyderabad in 2016
and 2018 respectively. His research interests
include explainable AI, probabilistic graphical
models and Bayesian inference.
Stewart Slocum received his BS in computer
science and applied mathematics from Johns
Hopkins University in 2021. His research inter-
ests center on principled deep learning methods
with performance and robustness guarantees.
Benjamin Haeffele is an Associate Research
Scientist in the Mathematical Institute for Data
Science at Johns Hopkins University. His re-
search interests involve developing theory and
algorithms for processing high-dimensional data
at the intersection of machine learning, optimiza-
tion, and computer vision. In addition to basic
research in data science he also works on a vari-
ety of applications in medicine, microscopy, and
computational imaging. He received his Ph.D. in
Biomedical Engineering at Johns Hopkins Uni-
versity in 2015 and his B.S. in Electrical Engineering from the Georgia
Institute of Technology in 2006.
Ren´e Vidal received his B.S. degree in Electrical
Engineering (valedictorian) from the Pontiﬁcia
Universidad Cat´olica de Chile in 1997 and his
M.S. and Ph.D. degrees in Electrical Engineering
and Computer Science from the University of
California at Berkeley in 2000 and 2003, respec-
tively. He is currently the Director of the Mathe-
matical Institute for Data Science (MINDS) and
the Hershel L. Seder Professor of Department of
Biomedical Engineering at The Johns Hopkins
University, where he has been since 2004. He
is co-author of the book “Generalized Principal Component Analysis”
(Springer 2016), co-editor of the book “Dynamical Vision” (Springer
2006) and co-author of over 300 articles in machine learning, computer
vision, signal and image processing, biomedical image analysis, hybrid
systems, robotics and control. He is or has been Associate Editor
in Chief of the IEEE Transactions on Pattern Analysis and Machine
Intelligence and Computer Vision and Image Understanding, Associate
Editor or Guest Editor of Medical Image Analysis, the IEEE Transac-
tions on Pattern Analysis and Machine Intelligence, the SIAM Journal
on Imaging Sciences, Computer Vision and Image Understanding, the
Journal of Mathematical Imaging and Vision, the International Journal
on Computer Vision and Signal Processing Magazine. He has received
numerous awards for his work, including the 2021 Edward J. McCluskey
Technical Achievement Award, the 2016 D’Alembert Faculty Fellowship,
the 2012 IAPR J.K. Aggarwal Prize, the 2009 ONR Young Investigator
Award, the 2009 Sloan Research Fellowship and the 2005 NSF CA-
REER Award. He is a Fellow of the IEEE, Fellow of IAPR, Fellow of
AIMBE, and a member of the ACM and SIAM.
Donald Geman (Life Senior Member, IEEE) re-
ceived the B.A. degree in literature from the
University of Illinois and the Ph.D. degree in
mathematics from Northwestern University. He
was a Distinguished Professor with the Uni-
versity of Massachusetts until 2001, when he
joined the Department of Applied Mathematics
and Statistics, Johns Hopkins University, where
he is currently a member of the Center for Imag-
ing Science and the Institute for Computational
Medicine. His current research interests include
statistical learning, computer vision, and computational biology. He is a
member of the National Academy of Sciences and a fellow of the IMS
and SIAM.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
15
APPENDIX A
In proofs of propositions and lemmas we rewrite the statement (un-numbered) for convenience.
A.1
Characterizing the optimal strategy π∗
In this subsection we characterize the optimal strategy for any such arbitrary query set chosen by the user.
Proposition. Assuming Q is ﬁnite and when d is taken to be the KL-divergence then objective (3) can be rewritten as,
Hϵ
Q(X; Y ) := min
π
EX
|explπ
Q(X)|

(21)
s.t.
τ π
X
k=1
I(Y ; Sπ
k (X) | Sπ
k−1(X)) ≥I(X; Y ) −ϵ
where, τ π = max{tπ(x) : x ∈X} and tπ(X) is deﬁned as the number of queries selected by π for input X until qST OP . We deﬁne
Sπ
k (X) as a random variable where any realization Sπ
k (xobs), xobs ∈X , denotes the event
Sπ
k (xobs) := {x′ ∈X | {qi, qi(xobs)}1:k = {qi, qi(x′)}1:k},
where qi is the ith query selected by π for input xobs. Here we use the convention that Sπ
0 (X) = Ω(the entire sample space) and
Sπ
l (X) = Sπ
tπ(X)(X) ∀l > tπ(X).
Proof. We begin by reformulating our sufﬁciency constraint in (3) in terms of entropy by taking the “distance” between
probability distributions as the KL divergence.9 The ϵ-Sufﬁciency constraint can then be rewritten as,
ϵ ≥EX[KL
 p(Y | X), p(Y | explπ
Q(X)
]
= EX
"
EY
"
log
p(Y | X)
p(Y | explπ
Q(X)) | X
##
= EX

EY

log p(Y | X)
P(Y )
| X

+ EX
"
EY
"
log
p(Y )
P(Y | explπ
Q(X)) | X
##
= I(X; Y ) −I(explπ
Q(X); Y )
= H(Y | explπ
Q(X)) −H(Y | X)
In the third equality we multiplied the term inside the log by the identity P (Y )
P (Y ) = 1. The ﬁfth inequality is by deﬁnition of
mutual information. Thus, we can rewrite (3) as,
Hϵ
Q(X; Y ) := min
π
EX
|explπ
Q(X)|

(22)
s.t. H(Y | explπ
Q(X)) −H(Y | X) ≤ϵ
(ϵ-Sufﬁciency)
Let’s deﬁne tπ(X) as the number of queries selected by π for input X until qST OP . Deﬁne τ π = max{tπ(X) : X ∈X}.
For the purpose of analysis we can vacuously modify π such that for any given xobs ∈X , π asks a ﬁxed τ π number of
queries by ﬁlling the remaining τ π −tπ(xobs) queries with qST OP . An immediate consequence of this modiﬁcation is that
Sπ
l (X) = Sπ
tπ(X)(X) ∀l > tπ(X).
We will now show that the sufﬁciency criteria H(Y | explπ
Q(X)) −H(Y | X) can be rewritten as a sum of successive
mutual information terms.
H(Y | explπ
Q(X)) −H(Y | X)
= (H(Y ) −H(Y | X)) −(H(Y ) −H(Y | explπ
Q(X)))
= I(X; Y ) −(H(Y ) −H(Y | Sπ
τ (X))
(23)
The third equality uses the fact that H(Y | explπ
Q(X)) = H(Y | Sπ
τ (X)) since the ∀x ∈X, Sπ
τ (x) = Sπ
tπ(x)(x).
We can now write H(Y ) −H(Y | Sπ
τ (X)) as a telescoping series,
H(Y ) −H(Y | Sπ
τ (X) = H(Y ) −H(Y | Sπ
1 (X)) + H(Y | Sπ
1 (X)) −H(Y | Sπ
1 (X), Sπ
2 (X))
+ H(Y | Sπ
1 (X), Sπ
2 (X)) . . . + H(Y | Sπ
1 (X), Sπ
2 (X), . . . , Sπ
τ−1(X)) −H(Y | Sπ
τ (X))
= I(Y ; Sπ
1 (X)) + I(Y ; Sπ
2 (X) | Sπ
1 (X))) + . . . + I(Y ; Sπ
τ (X) | Sπ
τ−1(X))
(24)
The last equality is obtained by noticing that
9. In favour of a clearer exposition, we abuse notation here and use p(Y | explπ
Q(X)) to denote P(Y | Sπ
tπ(X)(X)). In reality, explπ
Q(x) refers
to the sequence of query-answer pairs chosen for x as deﬁned in (2) whereas Sπ
tπ(X=x)(X = x) refers to the event which is the set of all possible
data-points that agree on the ﬁrst tπ(x) query-answers observed for x.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
16
1) H(Y | Sπ
1 (X), . . . , Sπ
k−1(X)) −H(Y | Sπ
1 (X), . . . , Sπ
k (X)) = I(Y ; Sπ
k (X) | Sπ
k−1(X)),
2) H(Y | Sπ
τ (X)) = H(Y | Sπ
1 (X), Sπ
2 (X), . . . , Sπ
τ (X)) since the events {Sπ
k (x)}1:τ are nested ∀x ∈X .
Putting it all together we can rewrite (22) as,
Hϵ
Q(X; Y ) := min
π
EX
|explπ
Q(X)|

(25)
s.t. I(X; Y ) −
τ π
X
k=1
I(Y ; Sπ
k (X) | Sπ
k−1(X)) ≤ϵ
A.2
Approximation guarantees for IP
In Proposition 2 we prove that the IP strategy comes within 1 bit of H(Y ) (the entropy of Y ) under the assumption that
one has access to all possible binary functions of X, that are also binary functions of Y , as queries. Given such a query set,
it is well-known that the optimal strategy π∗is given by the Huffman Code for Y which is also within 1 bit of H(Y ). Thus,
the result is immediate that IP asks at most 1 query more than π∗on average. We restate Proposition 2 below for ease.
Proposition. Let Y be discrete. Let ˜HQ(X; Y ) be the expected description length obtained by the IP strategy. If H(Y |X) = 0 and
Q is the set of all possible binary functions of X such that H(q(X) | Y ) = 0 ∀q ∈Q, then H(Y ) ≤˜HQ(X; Y ) ≤H(Y ) + 1.
We make two remarks before turning to the proof.
Remark 1: We have observed data X, a categorical discrete r.v. Y , and binary queries {q(X), q ∈Q} from which Y can be
estimated. In fact, let’s suppose that Y is determined by X, say Y = f(X); equivalently, H(Y |X) = 0. We assume that Y
represents some high-level, possibly semantic, interpretation of X which can only be seen through the eyes of the queries
q ∈Q. Ideally, we would like to be able to query the membership of Y in any subset of Y (the set of possible values of
Y ); this is the information we would have in coding Y . We will say that the query IY ∈D is realizable for some D ⊂Y if
the query IX∈f −1(D) is in Q, i.e., we can test for Y ∈D by one of our observable data queries. In general, not all subsets
of Y can be associated with attributes or features of X, e.g., “Napolean” or “Dead” in “20 Questions”, or “black beak” in
bird species classiﬁcation. If there was a query q(X) for every subset D of values of Y , then our theorem says that mean
number of queries needed to determine the state of Y with IP is bounded below by H(Y ) and above by H(Y ) + 1.
Remark 2: The sequence of queries q1, q2, ... generated by the IP algorithm for a particular data point can be seen
as one branch, root to leaf, of a decision tree constructed by the standard machine learning strategy based on
successive reduction of uncertainty about Y as measured by mutual information: q1 = arg maxq∈Q I(q(X); Y ), qk+1 =
arg maxq∈Q I(q(X); Y |SIP
k (x0)) where the SIP
k (x0) is the event that for the ﬁrst k questions the answers agree with those
for x0. We stop as soon as Y is determined. Whereas a decision tree accommodates all x simultaneously, the questions
along the branch depends on having a particular, ﬁxed data point. But the learning problem in the branch version (“active
testing”) is exponentially simpler.
Proof. The lower bound H(Y ) ≤˜HQ(X; Y ) comes from Shannon’s source coding theorem for stochastic source Y .
Now for the upper bound, since I(q(X); Y | SIP
k (x0)) = H(q(X)|SIP
k (x0)) −H(q(X)|Y, SIP
k (x0)) and since Y determines
q(Y ) and hence also q(X), the second entropy term is zero (since given H(q(X) | Y ) = 0). So our problem is maximize
the conditional entropy of the binary random variable q(X) given SIP
k (x0). So the IP algorithm is clearly just “divide and
conquer”:
q1 = arg max
q∈Q
H(q(X)),
qk+1 = arg max
q∈Q
H(q(X)|SIP
k (x0)).
Equivalently, since entropy of a binary random variable ρ is maximized when P(ρ) = 1
2,
qk+1 = arg min
q∈Q |P(q(X) = 1|SIP
k (x0)) −1
2|.
Let Yk be the set of “active hypotheses” after k queries (denoted as Ak), namely those y with positive posterior
probability: P(Y = y|SIP
k (x0)) > 0. Indeed,
P(Y = y|SIP
k (x0)) =
P(SIP
k (x0)|Y = y)p(y)
P
y P(SIP
k (x0)|Y = y)p(y)
=
1Ykp(k)
P
y∈Ak p(y)
since
P(SIP
k (x0)|Y = y) =
 1,
if y ∈Ak
0,
y /∈Ak

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
17
In particular, the classes in the active set have the same relative weights as at the outset. In summary:
p(y|SIP
k (x0)) =
 p(y)/ P
Ak p(l), y ∈Ak
0,
otherwise
The key observation to prove the theorem is that if a hypothesis y generates the same answers to the ﬁrst m or more
questions as y0, and hence is active at step m, then its prior likelihood p(y) is at most 2−(m−1), m = 1, 2, . . .. This is
intuitively clear: if y has the same answer as y0 on the ﬁrst question, and p(y0) > 1
2, then only one question is needed and
the active set is empty at step two; if q1(y) = q1(y0) and q2(y) = q2(y0) and p(y0) > 1
4, then only two question are needed
and the active set is empty at step three, etc.
Finally, since C, the code length, takes values in the non-negative integers {0, 1, . . . , }:
˜HQ(X; Y )
:=
E[C]
=
∞
X
m=1
P(C ≥m)
≤
∞
X
m=1
P(p(Y ) < 2−(m−1))
=
∞
X
m=1
X
y:p(y)<2−(m−1)
p(y)
=
X
y∈Y
∞
X
m=1
1{p(y)<2−(m−1)}p(k)
=
X
y∈Y
p(k)(1 −log p(k))
=
H(Y ) + 1
A.3
Termination Criteria for IP
We would ﬁrst analyze the termination criteria for the exact case, that is, ϵ = 0 in (3), and then move on to the more general
case.
Termination Criteria when ϵ = 0 Ideally for a given input xobs, we would like to terminate (IP outputs qST OP ) after L
steps if
p(y | xobs) = p(y | x′) ∀x′ ∈SIP
L (xobs), y ∈Y
(26)
Recall, SIP
L (xobs) = {x′ ∈X | {qi, qi(x′)}1:L = {qi, qi(xobs)}1:L}. In other words, its the event consisting of all x′ ∈X
which share the ﬁrst L query-answer pairs with xobs.
If (26) holds for all xobs ∈X , the it is easy to see that this is equivalent to the sufﬁciency constraint in the case ϵ = 0,
p(y | x) = p(y | explIP
Q(x)) ∀(x, y) × (X × Y)
where p(y | explIP
Q(x)) := p(y | SIP
tIP(xobs)(xobs)) ∀(x, y) ∈(X × Y) and tIP(x) is the number of iterations IP takes on input x
before termination.
Unfortunately, detecting (26) is difﬁcult in practice. Instead we have the following lemma which justiﬁes our stopping
criteria for IP.
Lemma 1. For a given input xobs if event SIP
L (xobs) (after asking L queries) satisﬁes the condition speciﬁed by (26) then for all
subsequent queries qm, m ≥L, maxq∈Q I(q(X); Y |SIP
m(xobs)) = 0.
Refer to Appendix A.4 for a proof.
Inspired from Lemma 1 we formulate an optimistic stopping criteria as,
L = inf{k ∈{1, 2, ..., |Q|} : max
q∈Q I(q(X); Y |SIP
m(xobs)) = 0 ∀m ≥k, m ≤|Q|}
(27)
Evaluating (27) would be computationally costly since it would involve processing all the queries for every input x. We
employ a more practically amenable criteria
qL+1 = qST OP
if
max
q∈Q I(q(X); Y |SIP
m(xobs)) = 0 ∀m ∈{L, L + 1, ..., L + T}
(28)
T > 0 is a hyper-parameter chosen via cross-validation. Note, it is possible that there does not exist any informative
query in one iteration, but upon choosing a question there suddenly appears informative queries in the next iteration.
For example, consider the XOR problem. X ∈R2 and Y ∈{0, 1}. Let Q be the set to two axis-aligned half-spaces. Both

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
18
half-spaces have zero mutual information with Y . However, upon choosing any one as q1, the other half-space is suddenly
informative about Y . Equation (28) ensures that we do not stop prematurely.
Termination Criteria for general ϵ when d is taken as the KL-divergence For a general ϵ > 0 we would like IP to terminate
such that on average,
EX[KL
 p(Y | X), p(Y | explIP
Q(X)
] ≤ϵ
(29)
Detecting this is difﬁcult in practice since IP is an online algorithm and only computes query-answers for a given input
x. So it is not possible to know apriori when to terminate such that in expectation the KL divergence would be less than ϵ.
Instead we opt for the stronger requirement that,
KL
 p(Y | x), p(Y | explIP
Q(x)
 ≤ϵ
∀x ∈X.
(30)
It is easy to see that (30) implies (29).
As before, p(y | explIP
Q(x)) := p(y | SIP
tIP(xobs)(xobs)) ∀(x, y) ∈(X × Y) and tIP(x) is the number of iterations IP takes on
input x before termination. We have the following lemma (analogous to the ϵ = 0 case).
Lemma 2. We make the following assumptions:
1) Y is a countable set (recall Y ∈Y).
2) For any xobs ∈X and x1, x2 ∈SIP
tIP(xobs)(xobs), we have p(Y | x1) and p(Y | x2) have the same support.
Then, for given input xobs if event SIP
tIP(xobs)(xobs) (after asking tIP(xobs) queries) satisﬁes the condition speciﬁed by (30) then for all
subsequent queries qm, m ≥tIP(xobs), maxq∈Q I(q(X); Y |SIP
m(xobs)) ≤ϵ′, where ϵ′ = Cϵ for some constant C > 0.
Refer to Appendix A.5 for a proof. The assumption of Y being countable is typical for supervised learning (the scenario
considered in this paper) where the set of labels is often ﬁnite. The second assumption intuitively means that P(y | x2) =
0 =⇒P(y | x1) = 0 for any y ∈Y. This is a reasonable assumption since we envision practical scenarios in which ϵ is
close to 0 and thus different inputs which share the same query-answers until termination by IP are expected to have very
“similar” posteriors.10
Inspired from Lemma 2 we formulate an optimistic stopping criteria ∀xobs ∈X as,
tIP(xobs) = inf{k ∈{1, 2, ..., |Q|} : max
q∈Q I(q(X); Y |SIP
m(xobs)) ≤ϵ′ ∀m ≥k, m ≤|Q|}
(31)
Evaluating (31) would be computationally costly since it would involve processing all the queries for every input xobs. We
employ a more practically amenable criteria
qtIP(xobs)+1 = qST OP
if
max
q∈Q I(q(X); Y |SIP
m(xobs)) ≤ϵ′ ∀m ∈{L, L + 1, ..., L + T}
(32)
T > 0 is a hyper-parameter chosen via cross-validation.
A.4
Proof of Lemma 1
Proof. Recall each query q partitions the set X and SIP
L (xobs) is the event {x′ ∈X | {qi, qi(xobs)}1:L = {qi, qi(x′)}1:L}. It is
easy to see that if SIP
L (x) satisﬁes the condition speciﬁed by (26) then
P(y | SIP
m(xobs)) = P(y | x′) ∀x′ ∈SIP
m(xobs) ∀m ≥L, ∀q ∈Q
(33)
This is because subsequent query-answers partition a set in which all the data points have the same posterior distributions.
Now, ∀q ∈Q, ∀a ∈Range(q), y ∈Y
p(q(X) = a, y|SIP
m(xobs)) = p(q(X) = a | SIP
m(xobs))p(y | q(X) = a, SIP
m(xobs))
(34)
(34) is just an application of the chain rule of probability. The randomness in q(X) is entirely due to the randomness in X.
For any a ∈Range(q), y ∈Y
p(y | q(X) = a, SIP
m(xobs)) =
X
x′
p(y, X = x′ | a, SIP
m(xobs))
=
X
x′
p(y | X = x′, a, SIP
m(xobs))p(X = x′ | a, SIP
m(xobs))
=
X
x′
p(y | X = x′)p(X = x′ | a, SIP
m(xobs))
= p(y | SIP
m(xobs))
X
x′
p(X = x′ | a, SIP
m(xobs))
= p(y | SIP
m(xobs))
(35)
10. We refer to the distribution p(y | x) for any x ∈X as the posterior distribution of x.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
19
The ﬁrst equality is an application of the law of total probability, third due to conditional independence of the history
and the hypothesis given X = x′ (assumption) and the fourth by invoking ((33)).
Substituting (35) in (34) we obtain Y ⊥⊥q(X) | SIP
m(xobs)) ∀m ≥L, q ∈Q. This implies that for all subsequent queries
qm, m > L, maxq∈Q I(q(X); Y |SIP
m(xobs))) = 0. Hence, Proved.
A.5
Proof of Lemma 2
Proof. Condition (30) implies bounded KL divergence between inputs on which IP has identical query-answer
trajectories Recall SIP
tIP(xobs))(xobs)) is the event {x′ ∈X | {qi, qi(xobs))}1:tIP(xobs)) = {qi, qi(x′)}1:tIP(xobs))}. If SIP
tIP(xobs))(xobs))
satisﬁes (30) then using Pinsker’s inequality we conclude,
δ(p(Y | xobs)), p(Y | SIP
tIP(xobs))(xobs))) ≤
r ϵ
2
(36)
Here δ is the total variational distance between the two distributions. Since δ is a metric we conclude for any x1, x2 ∈
SIP
tIP(xobs))(xobs)),
δ(p(Y | x1), p(Y | x2)) ≤δ(p(Y | x1), p(Y | SIP
tIP(xobs)(xobs)) + δ(p(Y | x2), p(Y | SIP
tIP(xobs)(xobs))
≤
√
2ϵ
(37)
Since Y is countable, deﬁne η = min{p(y | ˆx) : y ∈Y, p(y | ˆx)) > 0, ˆx ∈X}. Then, by the reverse Pinsker’s inequality
we conclude,
KL(p(Y | x1), p(Y | x2))) ≤ϵ
η =: ϵ′
∀x1, x2 ∈SIP
tIP(xobs)(xobs)
(38)
Note, the above upper bound holds since by assumption 2, p(Y | x1) and p(Y | x2)) have the same support.
Bounded KL divergence between inputs implies subsequent queries have mutual information bounded by ϵ For any
subsequent query q ∈Q that IP asks about input xobs we have ∀x ∈SIP
tIP(xobs)(xobs),
KL

p(Y | x), p(Y | SIP
tIP(xobs)+1(xobs))

=
X
Y
p(Y | x) log p(Y | x) −
X
Y
p(Y | x) log p(Y | SIP
tIP(xobs)+1(xobs))
(39)
where, SIP
tIP(xobs)+1(xobs) := SIP
tIP(xobs)(xobs) ∩{x′ ∈X : q(x′) = q(x)}. For brevity, we denote SIP
tIP(xobs)+1(xobs) as B,
X
Y
p(Y | x) log p(Y | B) =
X
Y
p(Y | x) log
" X
x′∈B
p(Y | x′, B)P(x′ | B)
#
=
X
Y
p(Y | x) log
" X
x′∈B
p(Y | x′)P(x′ | B)
#
≥
X
Y
p(Y | x)
X
x′∈B
p(x′ | B) log p(Y | x′)
=
X
x′∈B
p(x′ | B)
X
Y
p(Y | x) log p(Y | x′)
(40)
In the third inequality Jensen’s inequality was used. Substituting (40) in (39),
KL (p(Y | x)||p(Y | B)) ≤
X
x′∈B
p(x′ | B)
X
Y
p(Y | x) log p(Y | x)
p(Y | x′)
≤ϵ′ X
x′
p(x′ | B)
= ϵ′
(41)
In the second inequality we substituted from (38) since x, x′ ∈B ⊆SIP
tIP(xobs)(xobs). In the third equality we used the identity
P
x′∈B p(x′ | B) = 1,
It is easy to see that (41) holds for all x′ ∈SIP
tIP(xobs)+1(xobs) and thus I(X; Y | SIP
tIP(xobs)+1(xobs)) ≤ϵ
Since Y →X →q(X) we can apply the data-processing inequality to obtain,
I(q(X); Y | SIP
tIP(xobs)+1(xobs)) ≤I(X; Y | SIP
tIP(xobs)+1(xobs)) ≤ϵ′
∀q ∈Q.
This implies that for all subsequent queries qm, m > tIP(xobs), maxq∈Q I(q(X); Y |SIP
m(xobs)) ≤ϵ. Hence, Proved.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
20
A.6
Complexity of the Information Pursuit Algorithm
For any given input x, the per-iteration cost of the IP algorithm is O(N + |Q|m), where |Q| is the total number of queries,
N is the number of ULA iterations, and m is cardinality of the product sample space q(X) × Y . For simplicity we assume
that the output hypothesis Y and query-answers q(X) are ﬁnite-valued and also that the number of values query answers
can take is the same but our framework can handle more general cases.
More speciﬁcally, to compute qk+1 = arg maxq∈Q I(q(X); Y | SIP
k (xobs)). We ﬁrst run ULA for N iterations to get
samples from p(z | y, SIP
k (x)) which are then used to estimate the distribution p(q(x), y | SIP
k (x)) (using (15)) for every query
q ∈Q and every possible query-answer hypothesis, (q(x), y), pair. This incurs a cost of O(N +|Q|m). We then numerically
compute the mutual information between query-answer q(X) and Y given history for every q ∈Q as described in (18).
This has a computational complexity O(|Q|m). Finally, we search over all q ∈Q to ﬁnd the query with maximum mutual
information (refer (6)).
It is possible to reduce N by using advanced MCMC sampling methods which converge faster to the required
distribution p(z | y, SIP
k (x))). The linear cost of searching of all queries can also be reduced by making further assumptions
about the structure of the query set. For example, we conjecture that this cost can be reduced to log |Q| using hierarchical
query sets where answers to queries would depend upon to answers to queries higher up in the hierarchy. Note that
if the query answers q(X) and Y are continuous random variables then we would need to resort to sampling to
construct stochastic estimates of the mutual information between q(X) and Y instead of carrying our explicit numerical
computations. We would explore these directions in future work.
A.7
Network Architectures and Training Procedure
Here we describe the architectures and training procedures for the β-VAEs used to calculate mutual information as
described in Section 4.
A.7.1
Architectures
Conv 32
Conv 64
MaxPool
Conv 128
Conv 256
MaxPool
Image X
28x28x3
FC 256
Label Y
10x1
Global Avg Pool
FC 2048
FC 100
FC 2048
FC 100
Concat
μ
Σ
FC 256
Label Y
10x1
Concat
FC 256
ConvTranspose 256
kernel 4
ConvTranspose 256
kernel 2, stride 2
ConvTranspose 128
ConvTranspose 64
ConvTranspose 64
kernel 2, stride 2
ConvTranspose 64
kernel 2, stride 2
ConvTranspose 32
ConvTranspose 3
Generated Image
28x28x3
Encoder
Decoder
Latent Z
100x1
Latent Z ~ N(μ, Σ)
100x1
Fig. 7. Binary Image VAE
Binary Image Classiﬁcation In the encoder, all convolutional layers use kernel size 3 and stride 1, and max pool layers use a
pooling window of size 2. In the decoder all transposed convolutions use kernel size 3 and stride 1 unless otherwise noted.
In both the encoder and decoder, all non-pooling layers are followed by a BatchNorm layer and LeakyReLU activation
(with slope -0.3) except for the ﬁnal encoder layer (no nonlinearities) and the ﬁnal decoder layer (sigmoid activation).

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
21
FC 512
FC 128
Concepts C
312x1
Label Y
200x1
FC 100
FC 100
Concat
μ
Σ
Label Y
200x1
Concat
FC 128
FC 512
Generated Concepts
312x1
Z ~ N(μ, Σ)
100x1
Latent Z
100x1
Encoder
Decoder
FC 512
FC 128
Bag-of-Words X
1000x1
Label Y
10x1
FC 100
FC 100
Concat
μ
Σ
Label Y
10x1
Concat
FC 128
FC 512
Generated Words X
1000x1
Z ~ N(μ, Σ)
100x1
Latent Z
100x1
Encoder
Decoder
(a) CUB Concepts VAE
(b) HuffPost News Headlines VAE
Fig. 8. CUB Concepts and HuffPost News Headlines VAEs
CUB Bird Species Classiﬁcation and HuffPost News Headline Classiﬁcation For CUB and HuffPost News, we use
essentially the same VAE architecture, only designed to handle different-sized inputs X and one-hot labels Y . All layers
are followed by a BatchNorm layer and ReLU activation except for the ﬁnal encoder layer (no nonlinearities) and the ﬁnal
decoder layer (sigmoid activation).
A.7.2
Training
The β-VAE was trained by optimizing the Evidence Lower BOund (ELBO) objective
max
ω,φ ELBO(ω, φ) =
n
X
i=1
h
Epφ(z|y(i),x(i))[log pω(x(i) | z, y(i))] −βDKL(pφ(z | y(i), x(i))∥p(z))
i
(42)
where pφ(z | y(i), x(i)) denotes the encoder and pω(x(i) | z, y(i)) the decoder. The prior over latents p(z) is taken to be
standard Gaussian.
For all binary image datasets, we trained our VAE for 200 epochs using Adam with learning rate 0.001 and the β-VAE
parameter β = 5.0. We also trained the CUB VAE for 200 epochs using Adam with learning rate 0.001 but with β = 1.0.
Finally, we trained the HuffPost News VAE for 100 epochs using Adam with learning rate 0.0002 and β = 1.0.
APPENDIX B
B.1
HuffPost News Task and Query Set
The Hufﬁngton Post News Category dataset consists of 200,853 articles published by the Hufﬁngton Post between 2012
and 2018. Each datapoint contains the article “headline”, a “short description” (a one to two-sentence-long continuation
of the headline), and a label for the category/section it was published under in the newspaper. We concatenate the article
headline and short description to form one extended headline. Additionally, many of the 41 category labels are redundant
(due to changes in how newspaper sections were named over the years), are semantically ambiguous and HuffPost-speciﬁc
(e.g. “Impact”, “Fifty”, “Worldpost”), or have very few articles. Therefore, we combine category labels for sections with
equivalent names (e.g. “Arts & Culture” with “Culture & Arts”), remove ambiguous HuffPost-speciﬁc categories, and then
keep only the 10 most frequent categories to ensure that each category has an adequate number of samples. This leaves us
with a ﬁnal dataset of size 132,508 and category labels “Entertainment”, “Politics”, “Queer Voices”, “Business”, “Travel”,
“Parenting”, “Style & Beauty”, “Food & Drink”, “Home & Living”, and “Wellness”.
B.2
Comparison Models
B.2.1
MAP using Q
For IP, we use ULA to sample from p(z | y, SIP
k (x)), but now that we have access to the all the query answers, Q(x), we
can improve performance by making use of the VAE’s encoder instead. Following equation 16, we draw many samples
from the encoder p(z | Q(x), y) and then decode these samples to estimate the VAE’s posterior distribution p(y|Q(x)).
For each problem, we set the number of samples to be the same as what we draw during each iteration of IP (12,000 for
binary image tasks, 12,000 for CUB, 10,000 for HuffPost News). We expect the accuracy of this model to serve as an upper
bound for what we can achieve with IP given that it uses all queries. Our task-speciﬁc VAE architectures can be found in
Appendix A.7.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
22
B.2.2
Black-Box Using Q
We also compare to non-interpretable supervised models which receive all queries Q(x) as input and try to predict the
associated label y. This allows a comparison between the accuracy of the posterior of our generative model and traditional
supervised approaches on the chosen interpretable query set.
For the binary image datasets, we use a simple CNN where all convolutional layers use kernel size 3 × 3 and a stride
of 1, all max pooling uses a kernel size of 2. For CUB and HuffPost News we use simple MLPs.
TABLE 3
Binary Image CNN Architecture
Layer
Input Size/Channels
Output Size/Channels
Nonlinearity
Convolution
3
32
BatchNorm + ReLU
Convolution
32
64
BatchNorm + ReLU + MaxPool
Convolution
64
128
BatchNorm + ReLU
Convolution
128
256
BatchNorm + ReLU + MaxPool + Global Avg Pool
Fully-connected
256
2048
BatchNorm + ReLU
Fully-connected
2048
10
Sigmoid
TABLE 4
CUB Attributes MLP Architecture
Layer
Input Size/Channels
Output Size/Channels
Nonlinearity
Fully-connected
312
100
ReLU
Fully-connected
100
25
ReLU
Fully-connected
25
200
Sigmoid
TABLE 5
HuffPost Bag-of-Words MLP Architecture
Layer
Input Size/Channels
Output Size/Channels
Nonlinearity
Fully-connected
1000
100
ReLU
Fully-connected
100
25
ReLU
Fully-connected
25
10
Sigmoid
B.2.3
Black-Box
Since we ourselves pre-processed the cleaned 10-class version of HuffPost News, there are no reported accuracies in
the literature to compare IP with. Therefore, as a strong black-box baseline, we ﬁne-tune a pre-trained Bert Large
Uncased Transformer model [92] with an additional dropout layer (with dropout probability 0.3) and randomly initialized
fully-connected layer. Our implementation is publicly available at https://www.kaggle.com/code/stewyslocum/news-
classiﬁcation-using-bert.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
23
B.3
Additional Example Runs
B.3.1
IP with Various Patch Scales
For binary image classiﬁcation, we also experimented with patch queries of sizes other than 3 × 3, from single pixel 1 × 1
queries up to 4 × 4 patches.
k=0
k=1
k=2
k=3
k=4
k=5
k=6
k=7
0123456789
0.0
0.5
1.0
p(Y
SIP
k ((xobs)))
0123456789
0123456789
0123456789
0123456789
0123456789
0123456789
0123456789
k=0
k=1
k=2
k=3
k=4
0123456789
0.0
0.5
1.0
p(Y
SIP
k ((xobs)))
0123456789
0123456789
0123456789
0123456789
k=0
k=1
k=2
k=3
k=4
0123456789
0.0
0.5
1.0
p(Y
SIP
k ((xobs)))
0123456789
0123456789
0123456789
0123456789
(a) IP with 1x1 patch queries
(b) IP with 2x2 patch queries
(c) IP with 3x3 patch queries
(d) IP with 4x4 patch queries
k=0
k=1
k=2
k=4
k=5
k=6
k=7
k=11
k=20
0123456789
0.0
0.5
1.0
p(Y
SIP
k ((xobs)))
0123456789
0123456789
0123456789
0123456789
0123456789
0123456789
0123456789
0123456789
Fig. 9. IP on MNIST with different sized patch queries. In each subﬁgure, the top row displays the test image with red boxes denoting the current
queried patch and blue boxes denoting previous patches. The second row shows the revealed portion of the image that IP gets to use at each
query. The ﬁnal row shows the model’s estimated posteriors at each query. For conciseness in (a), we only display the 8 iterations of IP with highest
KL divergence between successive posteriors (i.e. the most inﬂuential iterations). Observe that at all patch sizes, the queries chosen by IP cover
roughly the same parts of the image, illustrating the importance of this region during classiﬁcation. Reaching the stopping criteria of 99% posterior
conﬁdence takes 20 queries with 1 × 1 patches, 7 queries with 2 × 2 patches, 4 queries with 3 × 3 patches, and 4 queries with 4 × 4 patches.
TABLE 6
Number of queries and pixels gleaned by IP (until termination) using query sets of different patch scales on MNIST.
Patch Size
1 × 1
2 × 2
3 × 3
4 × 4
# Queries
21.1
9.6
5.2
4.6
# Pixels
21.1
32.8
44.7
54.7
% Pixels
2.7
4.2
5.7
6.9
While 1 × 1 patch queries use the smallest total number of pixels and most similarly resemble existing post-hoc
attribution maps, they lead to a large number of scattered queries that are hard to interpret individually. On the other
extreme, as the patch size grows larger, the number of total queries decreases, but queries becomes harder to interpret
since each patch would contain many image features. On the MNIST dataset, we found 3 × 3 patches to be a sweet spot
where explanations tended to be very short, but were also at a level of granularity where each patch could be individually
interpreted as a single edge or stroke. Remarkably, at all chosen patch scales, only a very small fraction (2-7%) of the image
needs to be revealed to classify the images with high conﬁdence.

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
24
B.3.2
IP for Binary Image Classiﬁcation
Now we provide additional example runs of IP for each of the three binary image classiﬁcation datasets.
As in Figure 4 in the main paper, in each plot, the top row displays the test image with red boxes denoting the current
queried patch and blue boxes denoting previous patches. The second row shows the revealed portion of the image that IP
gets to use at each query. The ﬁnal row shows the model’s estimated posteriors at each query.
k=0
k=1
k=2
k=3
0123456789
0.0
0.5
1.0
p(Y
SIP
k ((xobs)))
0123456789
0123456789
0123456789
(a) Each query reveals the patch with maximum mutual information with Y , conditioned on query history. This is initially independent
of the particular image and asks for the pixel intensities in the center patch (see k = 1 in row 1). After the ﬁrst query reveals that the
center patch is all black, the posterior concentrates on “0” and “7”. After observing a white corner in the bottom left (which would
be black for a “7”), the model becomes conﬁdent that the image is a “0”, and even more so after one ﬁnal query when it reaches
termination.
k=0
k=1
k=2
k=3
0123456789
0.0
0.5
1.0
p(Y
SIP
k ((xobs)))
0123456789
0123456789
0123456789
(b) The ﬁrst query reveals a vertical white stroke in the center of the image, leading to a concentration of the posterior on “1”. In the
next two queries, IP determines that there is a single long vertical stroke center stroke taking up the entire height of the image, and so
it reaches a 99% conﬁdence of the image being a “1”.
k=0
k=1
k=2
k=3
k=4
k=5
k=6
k=7
0123456789
0.0
0.5
1.0
p(Y
SIP
k ((xobs)))
0123456789
0123456789
0123456789
0123456789
0123456789
0123456789
0123456789
(c) As in the top left example, the ﬁrst query is all black, and the posterior mass shifts onto “0”. Because the answer to this ﬁrst query
is identical as in the top left example, the second query chosen, in the bottom left of the image, is also the same. The response to this
query is also a white corner as in the top left example, and so the posterior continues to concentrate on “0”, and the third query is also
in the same left area of the image. However, this third query reveals a black patch, indicating that the image might be a “5” instead
of a “0”. In the remaining four queries, IP discovers other portions of the “5” digit and ﬁnally arrives at the right answer with high
conﬁdence.
Fig. 10. Additional Examples of IP on MNIST

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
25
Recall that in order to improve performance on the KMNIST and FashionMNIST datasets (at the expense of asking
a few more queries) we modiﬁed IP’s termination criteria to include a stability condition: terminate when the original
criterion (maxY p(Y |SIP
k (x)) ≥0.99) is true for 5 queries in a row.
す
おき
つなはまやれを
ki
su
tsu
na
ha
ma
ya
re
wo
o
(a) KMNIST is a 10-class dataset of handwritten Japanese Hiragana characters. To assist the reader in understanding the examples
below, we display each typed character along with its romanized name.
k=0
k=1
k=2
k=3
k=4
k=5
k=6
k=7
k=11
okisutsunahamayarewo
0.0
0.5
1.0
p(Y
SIP
k ((xobs)))
okisutsunahamayarewo
okisutsunahamayarewo
okisutsunahamayarewo
okisutsunahamayarewo
okisutsunahamayarewo
okisutsunahamayarewo
okisutsunahamayarewo
okisutsunahamayarewo
(b) For each image, IP selects the ﬁrst query to be in the middle right of the image, where several characters are likely to have a stroke.
Upon ﬁnding none, IP rules out “o”, “tsu”, and “ya” but otherwise distributes probability mass rather equally. On the second query,
IP discovers a closed loop in the bottom of the character, a clear sign of “na” and “ma”, which increase the most. The discovery of the
double crosses in the remaining queries concentrate the posterior on “ma” until termination. For conciseness, we only display the 8
iterations of IP with highest KL divergence between successive posteriors (i.e. the most inﬂuential iterations).
k=0
k=1
k=2
k=3
k=4
k=5
k=6
k=8
okisutsunahamayarewo
0.0
0.5
1.0
p(Y
SIP
k ((xobs)))
okisutsunahamayarewo
okisutsunahamayarewo
okisutsunahamayarewo
okisutsunahamayarewo
okisutsunahamayarewo
okisutsunahamayarewo
okisutsunahamayarewo
(c) In the ﬁrst query, IP discovers a left edge, hinting at the presence of a large loop on the right side of the image, features of “o” and
“tsu”. The second query is likely intended to disambiguate these two characters as “o” contains white strokes in this region. Upon
ﬁnding a black patch here, IP is already conﬁdent, and reaches 99% conﬁdence in just four queries.
Fig. 11. Additional Examples of IP on KMNIST

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
26
k=0
k=1
k=2
k=3
k=4
k=5
k=6
k=7
k=10
okisutsunahamayarewo
0.0
0.5
1.0
p(Y
SIP
k ((xobs)))
okisutsunahamayarewo
okisutsunahamayarewo
okisutsunahamayarewo
okisutsunahamayarewo
okisutsunahamayarewo
okisutsunahamayarewo
okisutsunahamayarewo
okisutsunahamayarewo
(d) The ﬁrst query reveals a right edge, suggesting a slightly smaller loop on the right side of the image, a feature shared by several
characters. However, most of these queries have a busy center region except for “tsu”, whose probability increases after the second query
reveals a black patch. The next two queries outline the shape of the loop which is very large, a distinctive characteristic of “tsu”.
Fig. 11. Additional Examples of IP on KMNIST

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
27
k=0
k=1
k=2
k=3
k=4
k=5
k=6
k=8
k=13
T-shirt/top
Trouser
Pullover
Dress
Coat
Sandal
Shirt
Sneaker
Bag
Ankle boot
0.0
0.5
1.0
p(Y
SIP
k ((xobs)))
T-shirt/top
Trouser
Pullover
Dress
Coat
Sandal
Shirt
Sneaker
Bag
Ankle boot
T-shirt/top
Trouser
Pullover
Dress
Coat
Sandal
Shirt
Sneaker
Bag
Ankle boot
T-shirt/top
Trouser
Pullover
Dress
Coat
Sandal
Shirt
Sneaker
Bag
Ankle boot
T-shirt/top
Trouser
Pullover
Dress
Coat
Sandal
Shirt
Sneaker
Bag
Ankle boot
T-shirt/top
Trouser
Pullover
Dress
Coat
Sandal
Shirt
Sneaker
Bag
Ankle boot
T-shirt/top
Trouser
Pullover
Dress
Coat
Sandal
Shirt
Sneaker
Bag
Ankle boot
T-shirt/top
Trouser
Pullover
Dress
Coat
Sandal
Shirt
Sneaker
Bag
Ankle boot
T-shirt/top
Trouser
Pullover
Dress
Coat
Sandal
Shirt
Sneaker
Bag
Ankle boot
(a) On this dataset, IP always selects the ﬁrst query to be the patch in the top center, which being all black in this case rules out the
possibility of the object being a type of pant or upper body garment, which would take up the entire height of the image. Over the next
several queries, IP focuses on queries that would allow it to distinguish types of shoes from each other, in particular ﬁnding the shoe to
have a high top and a small heel, eventually causing the posterior to concentrate on the correct “Ankle Boot” category. For conciseness,
we only display the 8 iterations of IP with highest KL divergence between successive posteriors (i.e. the most inﬂuential iterations).
k=0
k=1
k=2
k=3
k=4
k=5
k=6
k=8
k=16
T-shirt/top
Trouser
Pullover
Dress
Coat
Sandal
Shirt
Sneaker
Bag
Ankle boot
0.0
0.5
1.0
p(Y
SIP
k ((xobs)))
T-shirt/top
Trouser
Pullover
Dress
Coat
Sandal
Shirt
Sneaker
Bag
Ankle boot
T-shirt/top
Trouser
Pullover
Dress
Coat
Sandal
Shirt
Sneaker
Bag
Ankle boot
T-shirt/top
Trouser
Pullover
Dress
Coat
Sandal
Shirt
Sneaker
Bag
Ankle boot
T-shirt/top
Trouser
Pullover
Dress
Coat
Sandal
Shirt
Sneaker
Bag
Ankle boot
T-shirt/top
Trouser
Pullover
Dress
Coat
Sandal
Shirt
Sneaker
Bag
Ankle boot
T-shirt/top
Trouser
Pullover
Dress
Coat
Sandal
Shirt
Sneaker
Bag
Ankle boot
T-shirt/top
Trouser
Pullover
Dress
Coat
Sandal
Shirt
Sneaker
Bag
Ankle boot
T-shirt/top
Trouser
Pullover
Dress
Coat
Sandal
Shirt
Sneaker
Bag
Ankle boot
(b) The ﬁrst query detects a white corner in the top center of the image, which hints at the presence of a collar, causing the posterior
mass to move to the “Coat” and “Shirt” categories. Determining between these two categories is relatively difﬁcult however, especially
with binary images. But in general, coats tend to be bulkier than shirts. Therefore, after ﬁnding a white patch in iteration k = 2, the
probability of “Coat” slightly increases, but as more partially black queries along the outside of the shirt are revealed and the slimmer
outline of the shirt comes into view, the posterior converges on the correct category of “Shirt”.
k=0
k=1
k=2
k=3
k=4
k=5
k=7
T-shirt/top
Trouser
Pullover
Dress
Coat
Sandal
Shirt
Sneaker
Bag
Ankle boot
0.0
0.5
1.0
p(Y
SIP
k ((xobs)))
T-shirt/top
Trouser
Pullover
Dress
Coat
Sandal
Shirt
Sneaker
Bag
Ankle boot
T-shirt/top
Trouser
Pullover
Dress
Coat
Sandal
Shirt
Sneaker
Bag
Ankle boot
T-shirt/top
Trouser
Pullover
Dress
Coat
Sandal
Shirt
Sneaker
Bag
Ankle boot
T-shirt/top
Trouser
Pullover
Dress
Coat
Sandal
Shirt
Sneaker
Bag
Ankle boot
T-shirt/top
Trouser
Pullover
Dress
Coat
Sandal
Shirt
Sneaker
Bag
Ankle boot
T-shirt/top
Trouser
Pullover
Dress
Coat
Sandal
Shirt
Sneaker
Bag
Ankle boot
(c) Detecting an all white patch in the top center, IP rules out the possibility of the image being some type of shoe. The second query in
the lower left returns a black patch, suggesting that the image is also not an upper body garment, which would take up the width of the
image. In query k = 3, IP queries the center bottom of the image, discovering the space in between the two legs of the trouser, causing
the posterior probability of “Trouser” to jump to nearly 100%, which remains stable over the last few queries.
Fig. 12. Additional Examples of IP on FashionMNIST

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
28
B.3.3
IP for Bird Species Classiﬁcation
As in the main text, for each plot, on the left, we show the input image and on the right we have a heatmap of the estimated
class probabilities per iteration. For readability, we only show the top 10 most probable classes out of the 200. To the right,
we display the queries asked at each iteration, with red indicating a “no” response and green a “yes” response.
Black-footed Albatross
Mallard
Black-footed Albatross
Sooty Albatross
Mangrove Cuckoo
Geococcyx
Gadwall
White-breasted Kingfisher
Northern Flicker
Brown Creeper
Nighthawk
1. shape::perching-like?
2. primary_color::black?
3. nape_color::white?
4. bill_length::about_the_same_as_head?
5. size::medium_(9_-_16_in)?
6. breast_color::brown?
7. under_tail_color::grey?
8. underparts_color::grey?
9. back_color::grey?
10. bill_shape::hooked_seabird?
p(Y
SIP
k ((xobs)))
0.0
0.2
0.4
0.6
0.8
1.0
(a) The ﬁrst few queries narrow down the potential species from 200 down to a small number. Since we only show the 10 most probable
classes, the ﬁrst few queries increase the probability of all shown classes. The ﬁrst queries that distinguish among these classes concern
bill length (which rules out the short-billed Nighthawk) and size (which rule out the smaller Mangrove Cuckoo, Geococcyx, Kingﬁsher,
Northern Flicker, and Brown Creeper). The remaining birds are quite similar, all medium-sized brown water birds. The next two color
queries suggest that the bird in question is a Black-footed Albatross, which is conﬁrmed by the answers to the next few queries, which
all match up with the characteristics of that bird.
Groove-billed Ani
Common Raven
Brandt Cormorant
Bronzed Cowbird
Groove-billed Ani
Shiny Cowbird
Brewer Blackbird
Fish Crow
American Crow
Pelagic Cormorant
Pigeon Guillemot
1. shape::perching-like?
2. primary_color::black?
3. throat_color::black?
4. head_pattern::plain?
5. bill_length::about_the_same_as_head?
6. bill_shape::specialized?
7. bill_shape::all-purpose?
8. eye_color::red?
9. size::small_(5_-_9_in)?
p(Y
SIP
k ((xobs)))
0.0
0.2
0.4
0.6
0.8
1.0
(b) Again, the ﬁrst few queries narrow down the likely species into the top 10 displayed classes. In just two queries (bill length and
bill shape), IP distinguishes among these similar-looking, black, plain-headed birds that are hard for non-expert humans to differentiate
between. Again, the last few queries serve to conﬁrm the posterior prediction that the bird is a Groove-billed Ani.
Ringed Kingfisher
Brown Pelican
Pomarine Jaeger
Ringed Kingfisher
Forsters Tern
Least Tern
Artic Tern
Elegant Tern
Caspian Tern
Common Tern
Heermann Gull
1. shape::perching-like?
2. primary_color::black?
3. nape_color::white?
4. crown_color::white?
5. upper_tail_color::white?
6. shape::long-legged-like?
7. bill_shape::hooked_seabird?
8. primary_color::grey?
9. breast_color::white?
10. bill_shape::dagger?
11. crown_color::black?
p(Y
SIP
k ((xobs)))
0.0
0.2
0.4
0.6
0.8
1.0
(c) After establishing the top few most probable classes, IP converges on the class Ringed Kingﬁsher after just 7 queries. The last four
queries simply serve to increase its conﬁdence in its prediction.
Fig. 13. Additional Examples of IP on CUB Bird Species Identiﬁcation

JOURNAL OF LATEX CLASS FILES, VOL. 14, NO. 8, AUGUST 2015
29
B.3.4
IP for HuffPost News Headline Category Classiﬁcation
Entertainment
Politics
Queer Voices
Business
Travel
Parenting
Style & Beauty
Food & Drink
Home & Living
Wellness
1. trump
2. photo
3. gay
4. travel
5. parent
6. kid
7. recip
8. fashion
9. republican
10. mom
11. food
13. health
17. babi
19. style
20. life
23. children
60. trailer
70. new
72. market
0.0
0.2
0.4
0.6
0.8
1.0
Category: Entertainment
Short Description: Margot Robbie goes for 
the gold as the infamous ice skater.
Headline: 'I, Tonya' Looks Like A Winner In 
Slick New Trailer
Category: Style & Beauty
Short Description:PHOTO: See the rest of 
Kim's maternity gear: Want more? Be sure to 
check out HuffPost Style on Twitter, Facebook, 
Tumblr",
Headline: Kim Kardashian's Flip Flops, 
Dorothy Perkins Dress Is A Cute Change 
(PHOTO)
Entertainment
Politics
Queer Voices
Business
Travel
Parenting
Style & Beauty
Food & Drink
Home & Living
Wellness
1. trump
2. photo
3. home
4. recip
5. style
7. check
8. twitter
9. want
0.0
0.2
0.4
0.6
0.8
1.0
Category: Queer Voices
Short Description: "I don’t know if it 
changed me, but it changed my life."
Headline: Last Words: Ginger Minj Reflects 
On ‘RuPaul’s All Stars Drag Race’
Entertainment
Politics
Queer Voices
Business
Travel
Parenting
Style & Beauty
Food & Drink
Home & Living
Wellness
1. trump
2. photo
3. gay
4. travel
5. parent
6. kid
7. recip
8. fashion
10. food
12. babi
20. life
21. children
26. live
32. star
62. race
131. chang
208. word
229. reflect
234. clear
0.0
0.2
0.4
0.6
0.8
1.0
(a)
(b)
(c)
Fig. 14. Additional Examples of IP on HuffPost News Headline Classiﬁcation. As before, when more than 20 queries were asked, we only
display the 20 queries that led to greatest KL divergence between successive posteriors. (a) Because of the sparse structure of natural language, it
typically takes a signiﬁcant number of queries before the ﬁrst word that is present in the sentence is found. Until this point, no query is particularly
informative, and the posterior distribution remains mostly unchanged from the prior. However, at query 60, IP asks the word “trailer”, which is present
in the extended headline. Naturally, the posterior shifts heavily towards “Entertainment”, and a few queries later IP reaches its termination criteria.
Analyzing this run, we can say that IP reached its decision primarily because of the presence of the word “trailer”, leading us to say that this is
a reasonable and trustworthy prediction. (b) This is an example of a relatively short explanation as IP happens to discover words present in the
sentence after just two queries. Initially, the presence of “photo” causes the categories “Travel”, “Home & Living”, and “Style & Beauty” to become
more probable. Several words later however, the word “style” is found, which is very strongly associated with the “Style & Beauty” category. (c)
The posterior remains mostly unchanged until IP discovers the word “life”, which reasonably, shifts probability mass onto the “Wellness” category.
However, several queries later, “star” is found to be present, which shifts the posterior away from “Wellness” onto “Entertainment” and “Queer
Voices”. After discovering several more words that are present, “race”, “change”, “word”, “reﬂect”, the posterior progressively converges on “Queer
Voices”, but still with relatively high uncertainty, likely because it never came across identifying words such as “drag”, which was not present in the
vocabulary.

