A family of algorithms for approximate Bayesian inference
by
Thomas P Minka
Submitted to the Department of Electrical Engineering and Computer Science
in partial fulfillment of the requirements for the degree of
Doctor of Philosophy in Computer Science and Engineering
at the
MASSACHUSETTS INSTITUTE OF TECHNOLOGY
January 2001
© Massachusetts InstitutLof Technology 2001. All rights reserved.
A uthor ...............
Department of Electrical Engineering and Computer Science
January 12, 2001
Certified by........................
Rosalind Picard
Associate Professor of Media Arts and Sciences
Thesis Supervisor
Accepted by ..............
Arthur C. Smith
Chairman, Department Committee on Graduate Students
BARKER
MASSACHUSETTS INSTITUTE
OF TECHNOLOGY
APR 2 42001
LIBRARIES

A family of algorithms for approximate Bayesian inference
by
Thomas P Minka
Submitted to the Department of Electrical Engineering and Computer Science
on January 12, 2001, in partial fulfillment of the
requirements for the degree of
Doctor of Philosophy in Computer Science and Engineering
Abstract
One of the major obstacles to using Bayesian methods for pattern recognition has been
its computational expense. This thesis presents an approximation technique that can per-
form Bayesian inference faster and more accurately than previously possible. This method,
"Expectation Propagation," unifies and generalizes two previous techniques: 
assumed-
density filtering, an extension of the Kalman filter, and loopy belief propagation, an ex-
tension of belief propagation in Bayesian networks. The unification shows how both of
these algorithms can be viewed as approximating the true posterior distribution with a
simpler distribution, which is close in the sense of KL-divergence. Expectation Propagation
exploits the best of both algorithms: the generality of assumed-density filtering and the
accuracy of loopy belief propagation.
Loopy belief propagation, because it propagates exact belief states, is useful for lim-
ited types of belief networks, such as purely discrete networks. Expectation Propagation
approximates the belief states with expectations, such as means and variances, giving it
much wider scope. 
Expectation Propagation also extends belief propagation in the op-
posite direction-propagating richer belief states which incorporate correlations between
variables.
This framework is demonstrated in a variety of statistical models using synthetic and
real-world data. On Gaussian mixture problems, Expectation Propagation is found, for the
same amount of computation, to be convincingly better than rival approximation techniques:
Monte Carlo, Laplace's method, and variational Bayes. For pattern recognition, Expecta-
tion Propagation provides an algorithm for training Bayes Point Machine classifiers that is
faster and more accurate than any previously known. The resulting classifiers outperform
Support Vector Machines on several standard datasets, in addition to having a comparable
training time. Expectation Propagation can also be used to choose an appropriate feature
set for classification, via Bayesian model selection.
Thesis Supervisor: Rosalind Picard
Title: Associate Professor of Media Arts and Sciences
2

Acknowledgments
I wish to thank Rosalind Picard for giving me the freedom to satisfy my curiosity, not
only in this project but many others before it. When I started as a graduate student six
years ago, she said her job was to help me to get the most out of MIT. That she has done.
For comments on drafts of this thesis, I thank Rosalind Picard, Yuan Qi, and my readers,
Tommi Jaakkola and Trevor Darrell. This work was generously supported by members of
the Things That Think and Digital Life Consortia at the MIT Media Lab.
I wish to thank Kris Popat for inducing me to go to graduate school in the first place and
encouraging me to use probability theory for solving problems in machine learning. Thanks
also to Andrew McCallum for hosting me at Justsystem Pittsburgh Research Center, where
key groundwork for this thesis was performed.
It has been a pleasure to exchange ideas with everyone at the Vision and Modeling
group at the MIT Media Lab, especially Martin Szummer, Flavia Sparacino, Ali Rahimi,
and Yuan Qi. Thanks to Sumit Basu for providing his code for the billiard algorithm.
3

Contents
1 
Introduction 
5
2 Methods of numerical integration 
8
3 
Expectation Propagation 
13
3.1 
Assumed-density filtering 
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
13
3.2 
Expectation Propagation . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
17
3.2.1 
The clutter problem . . . . . . . . . . . . . . . . . . . . . . . . . . .
21
3.2.2 
Results and comparisons . . . . . . . . . . . . . . . . . . . . . . . . .
22
3.3 
Another example: Mixture weights . . . . . . . . . . . . . . . . . . . . . . .
27
3.3.1 
A D F . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
27
3.3.2 
E P . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
28
3.3.3 
A simpler method 
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
29
3.3.4 
Results and comparisons . . . . . . . . . . . . . . . . . . . . . . . . .
30
4 Disconnected approximations of belief networks 
31
4.1 
Belief propagation 
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
31
4.2 
Extensions to belief propagation 
. . . . . . . . . . . . . . . . . . . . . . . .
35
4.2.1 
Grouping terms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
36
4.2.2 
Partially disconnected approximations . . . . . . . . . . . . . . . . .
37
4.2.3 
Combining both extensions . . . . . . . . . . . . . . . . . . . . . . .
40
4.2.4 
Future work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
41
5 
Classification using the Bayes Point 
42
5.1 
The Bayes Point Machine . . . . . . . . . . . . . . . . . . . . . . . . . . . .
42
5.2 
Training via ADF . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
44
5.3 
Training via EP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
45
5.4 
EP with kernels . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
48
5.5 
Results on synthetic data . . . . . . . . . . . . . . . . . . . . . . . . . . . .
50
5.6 
Results on real data . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
56
5.7 
Model selection . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
62
5.8 
A better billiard algorithm . . . . . . . . . . . . . . . . . . . . . . . . . . . .
65
6 
Summary and future work 
68
4

Chapter 1
Introduction
The dominant computational task in Bayesian inference is numerical integration. 
New
methods for fast and accurate integration are therefore very important and can have great
impact. This dissertation presents a new deterministic approximation framework, Expecta-
tion Propagation, which achieves higher accuracy than existing integration algorithms with
similar computational cost.
The general scenario of Bayesian inference is that there is some observed data and some
unobserved quantity of interest. Inferences about the unknown x are based on its posterior
distribution given the observed D:
p(xID) = p(x, D) 
p(x, D)
p(D) 
f- p(x, D)dx
For example, we may want to know the posterior mean and variance of the unknown, which
are integrals over the posterior:
E[xlD] = 
xp(xID)dx = fxxp(xD)dx 
(1.2)
fxp(x,D)dx
[X2 
f 
x2p(x, D)dx
E[x2 |D] = 
X2p(x|D)dx = fx 
x 
(1.3)
fx p(x, Ddx
var(xID) 
= E[x 2 ID] - E[xID]2  
(1.4)
In real situations, there are many unknowns, not just the one we are interested in. In
Bayesian inference, these must be marginalized out of the joint distribution, which involves
yet more integrals:
p,,ID p(x, y, z, D)
Thus numerical integration goes hand in hand with practical Bayesian inference.
Numerical integration algorithms can be principally divided into deterministic vs. non-
deterministic methods. Deterministic methods try to approximate the integrand with some-
thing whose integral is known exactly. They work from properties of the integrand like its
maxima and curvature. Nondeterministic methods sample the integrand at random points
to get a stochastic estimate of the integral. This approach is more general since it works for
almost any integrand, but it also requires a great deal more computation than deterministic
approximation.
5

The proposed method, Expectation Propagation, is a deterministic approximation
method. It is an extension to assumed-density filtering (ADF), (Maybeck, 1982; Lauritzen,
1992; Bernardo & Giron, 1988; Stephens, 1997; Boyen & Koller, 1998b; Barber & Sollich,
1999; Opper & Winther, 1999; Frey et al., 2000) a one-pass, sequential method for comput-
ing an approximate posterior distribution. In ADF, observations are processed one by one,
updating the posterior distribution which is then approximated before processing the next
observation. For example, we might replace the exact one-step posterior with a Gaussian
having the same mean and same variance (Maybeck, 1982; Lauritzen, 1992; Barber & Sol-
lich, 1999; Opper & Winther, 1999). Or we might replace a posterior over many variables
with one that renders the variables independent (Boyen & Koller, 1998b) or approximates
them as Markovian (Frey et al., 2000). In each case, the approximate posterior is found by
minimizing KL-divergence, which amounts to preserving a specific set of posterior expecta-
tions. Note that the statistical model in question need not be a time series model, and the
processing order of observations need not correspond with time of arrival. The weakness of
ADF stems from its sequential nature: information that is discarded early on may turn out
to be important later. Even if ADF is augmented with a backward pass (Boyen & Koller,
1998a), this information cannot be recovered. ADF is also sensitive to observation ordering,
which is undesirable in a batch context.
Expectation Propagation (EP) extends ADF to incorporate iterative refinement of the
approximations, by making additional passes. The information from later observations re-
fines the choices made earlier, so that the most important information is retained. When the
refinement converges, the resulting posterior is independent of ordering and more accurate.
Iterative refinement has previously been used in conjunction with sampling (Koller et al.,
1999) and extended Kalman filtering (Shachter, 1990). 
Expectation Propagation differs
by applying this idea to the deterministic approximation of general distributions, not just
Gaussian distributions as in Shachter (1990). EP is more expensive than ADF by only a
constant factor-the number of refinement passes (typically 4 or 5). As shown in chapter 3,
the accuracy of EP is significantly better than ADF as well as rival approximation methods:
Monte Carlo, Laplace's method, and variational Bayes.
Thinking in this framework has a number of benefits. For example, in belief networks
with loops it is known that approximate marginal distributions can be obtained by iterating
the belief propagation recursions, a process known as loopy belief propagation (Frey &
MacKay, 1997; Murphy et al., 1999). 
As described in chapter 4, it turns out that this
heuristic procedure is a special case of Expectation Propagation, where the approximate
posterior is a completely disconnected network with no other constraints on functional form.
In other words, loopy belief propagation is a direct generalization of the ADF algorithm of
Boyen & Koller (1998b).
Expectation Propagation can therefore be seen as a way of generalizing loopy belief
propagation-to less restrictive approximations that are not completely disconnected and to
useful constraints on functional form such as multivariate Gaussian. Partially disconnected
approximations are useful for improving accuracy, just as in variational methods (Jordan
et al., 1999), while constraints on functional form are useful for reducing computation.
Instead of propagating exact belief states, which may be intractable, EP only needs to
propagate expectations relevant to the chosen approximating distribution (e.g. means and
variances). Hence the name "Expectation Propagation."
Expectation Propagation also has connections to statistical physics techniques. Yedidia
et al. (2000) have shown that belief propagation tries to minimize an approximate free en-
ergy on discrete networks. This is known as the TAP approach in statistical physics. Opper
6

& Winther (2000a) have generalized the TAP approach to networks with mixed continuous
and discrete nodes via the cavity method, and applied it to Gaussian process classifiers
(Opper & Winther, 2000c). Chapter 5 shows that Opper & Winther's algorithm, which
has excellent performance, is a special case of Expectation Propagation where the posterior
approximation is Gaussian. In this sense, Expectation Propagation as a generalization of
belief propagation parallels the cavity method as a generalization of TAP.
Expectation Propagation, as a general framework for approximate Bayesian inference,
can be applied to many real-world tasks. Chapter 5 demonstrates its use for the general
task of discriminating objects into classes. Classification via Bayesian averaging has long
been popular in the theoretical community, but difficult to implement in a computationally
competitive way. An excellent example of this is the Bayes Point Machine (Rujan, 1997;
Herbrich et al., 1999). With Expectation Propagation, the advantages of Bayesian averaging
can be achieved at less expense than previously possible.
In short:
" Chapter 2 reviews the prior work in approximate Bayesian inference, excluding ADF.
" Chapter 3 reviews ADF and introduces Expectation Propagation. Both are illustrated
on simple statistical models.
* Chapter 4 addresses belief networks, describing how loopy belief propagation is a
special case of Expectation Propagation and how belief propagation may be extended.
" Chapter 5 applies Expectation Propagation to the Bayes Point Machine classifier.
" Chapter 6 summarizes the results and offers suggestions for future work on Expecta-
tion Propagation.
7

Chapter 2
Methods of numerical integration
This chapter describes prior work in approximate Bayesian inference. The outline is:
1. Numerical quadrature
2. Monte Carlo methods: importance sampling, Gibbs sampling
3. Laplace's method
4. Variational bound on the integral using Jensen's inequality
5. Variational bound on the integrand; variational Bayes
6. Statistical physics methods
7. Sequential filtering
The classical approach to numerical integration is quadrature (Davis & Rabinowitz,
1984). 
In this deterministic approach, the integrand is evaluated at several locations
('knots') and a function is constructed that interpolates these values. The interpolant is
chosen from a simple family that can be integrated analytically, e.g. polynomials or splines.
The integral of the interpolant approximates the desired integral, and with enough knots
the approximation can be made arbitrarily accurate. By using a set of predefined knots
X1,..., X, 
the interpolation and integration procedure can be compiled down to a simple
summation of weighted function values:
I= 
f(x)dx 
(2.1)
n
S= 
w(xi) 
(2.2)
i=:1
where the weights wi are known. This method is excellent with integrands that are simple,
i.e. that are easy to interpolate. In one dimension, quadrature is nearly unbeatable. But
in high dimensions it is infeasible, because of the vast number of knots required to get a
good interpolant of a complex function. In Bayesian inference problems, the integrands are
mostly zero except in small regions, i.e. they are sparse, which makes the situation even
worse-most of the knots will be wasted. Newer deterministic techniques try to avoid these
problems by exploiting more properties of the integrand than just its value at given points.
Another approach is nondeterminism.
8

Nondeterministic, i.e. Monte Carlo, methods do not try to interpolate or otherwise
approximate the integrand at all-they merely appeal to the law of large numbers. In the
simplest approach, we sample knots uniformly in the region of integration A. The expected
value E[f(x)] under such a sampling must be the desired integral I divided by the area of
A. Hence the estimate
I 
f(xi) 
(2.3)
i=1
will converge to the true value, given enough samples, and this happens independent of
dimensionality and independent of the complexity of f. Thus while Monte Carlo is rather
inefficient in low dimensions, requiring thousands of knots when quadrature would only need
around 10, it is often the only feasible method in high dimensions. In Bayesian inference
problems, the sparsity of the integrand can be addressed by the technique of importance
sampling. Instead of sampling uniformly in A, we sample from a proposal distribution p(x)
that matches the shape of If I as well as possible. Then the estimate
S1 n f (X.)
1 = 
f-- ) 
(2.4)
n i=1 p(xi)
also converges to I, and much faster than (2.3) would. Importance sampling also has the
advantage of working for infinite regions A. Various enhancements to the basic importance
sampling procedure are possible (Ventura, 2000). With importance sampling, the difficulties
of numerical integration are replaced by the difficulties of sampling from a complex distribu-
tion. Good proposal distributions may be hard to sample from. In this case, one can apply
Markov Chain Monte Carlo methods (Neal, 1993; Liu, 1999), such as Metropolis sampling
and Gibbs sampling. In these methods, we generate samples that are approximately from
p(x), and then apply (2.4) as before. For these methods, careful monitoring and restarting
is required to ensure that the samples adequately represent p(x). The Billiard algorithm for
Bayes Point Machines, discussed in chapter 5, is a particularly clever Markov Chain Monte
Carlo algorithm.
To learn about a function, we can compute its value at a large number of knots, but
we could also compute a large number of derivatives at a single knot. This is basic idea of
Taylor expansion. By finite differences, a given number of knots translates into an equivalent
number of derivatives, so theoretically this approach has no advantage over quadrature. But
for Bayesian inference problems it has certain conceptual advantages. Since the integrand is
sparse, it makes sense to focus on one area where the action is. Interpolation is also simpler
since we just match derivatives. The most popular application of this idea in statistics is
Laplace's method (Kass & Raftery, 1993), where we expand log(f) about its mode:
1
log(f(x)) 
log(f(k)) + g (x - i) + -(x -
k) T A(x - i) 
(2.5)
2
g 
d log(f(x)) 
(2.6)
dx 
R
d' log f (X)
H 
d 
x) 
(2.7)
dxdxT.
Because k is the mode, g = 0 and we get
f(x) 
f(k) exp( (x - i) TH(x - k)) 
(2.8)
2
I= 
f(x)dx x f(k)(27r)rows(H)/
2 I-H 1/2  
(2.9)
JA 
A-(29
9

-
Exact
-- Laplace
W 
W
(a) 
(b)
F- 
Exact]
-- EP
w
(C)
Figure 2-1: Deterministic methods for integration try to approximate the integrand. (a)
Laplace's method uses a Gaussian that has the correct curvature at the mode. It produces
approximations that are too local. (b) In one type of variational bound, the integrand is
bounded everywhere. It produces approximations that are too inaccurate. (c) The proposed
method, Expectation Propagation, tries to minimize KL-divergence, a global measure of
deviation. It produces the most accurate integrals.
What Laplace's method does is approximate f by a scaled Gaussian density that matches
the value, first derivative, and second derivatives of f at xc. Figure 2-1 shows an example.
The drawback of this method is that it is difficult to use higher order derivatives, resulting
in an approximation that is limited in its accuracy and scope.
Variational bounding is a deterministic approach more global than Laplace's method.
We start by introducing an arbitrary function q(x):
I =- 
q 
f(x) 
dx 
(2.10)
X 
q(x)
Jensen's inequality for convex functions says that (in the case of logarithm)
log 
q(x)g(x)dx 
;> 
Iq(x)logg(x)dx 
(2.11)
/ 
=
if 
I 
j~d 
1 
2.2
X
10
' --
Exact
-- Bound

Combining (2.10) and (2.11) gives the following bound on I:
I> exp 
q(x) log f{Xdx 
(2.13)
( 
Xq(x)
This of course requires that f(x) is positive, a condition that is satisfied for most (but not
all) integrals in Bayesian inference. We are free to choose q(x) to get the tightest bound,
which corresponds to maximizing the right hand side of (2.13). Note that this is equivalent
to minimizing the (reversed) KL-divergence
D(q 
f) = 
q(x) log 
dx 
(2.14)
over q subject to the constraint (2.12). If q(x) is unconstrained, the maximum is achieved
at q(x) = f(x) and the bound matches the original integral. To achieve a simplification
in the integral, we must constrain q(x) in some way. Typically, q(x) is constrained to be
Gaussian (Hinton & van Camp, 1993; Barber & Bishop, 1997; Seeger, 1999) but mixture
distributions have also been suggested (Jaakkola & Jordan, 1999c). Having a lower bound
is useful since it provides a firm guarantee about the true value of the integral 
something
none of the other methods can do. However it tends to be very computational and is feasible
in a limited number of cases. Jensen's inequality takes advantage of the fact that log(f(x))
is often easy to integrate when f(x) is not. But this is not always true. For example,
in a mixture problem such as discussed in chapter 3, f is a product of sums, which does
not simplify under a logarithm. And in chapter 5, the likelihood is a step function which
reaches zero. The step function could be softened into a sigmoid to make the Jensen bound
well-defined, but we could not expect a good fit to result.
A less accurate but simpler way to obtain a variational bound is to bound the integrand
and then integrate the bound:
f(x) 
g(x) for all x 
(2.15)
I 
> jg(x)dx 
(2.16)
Figure 2-1 shows an example. Unlike the previous variational method, this approach can
be used in mixture problems. This approach was used explicitly by Jaakkola & Jordan
(1999a; 1999b) and implicitly by Waterhouse et al. (1995); Attias (1999); Ghahramani &
Beal (1999), under the name "variational Bayes." The implicit approach introduces hidden
variables to define a bound. Start by writing f(x) in terms of h(x, y):
f(x) = j h(x, y)dy 
(2.17)
Apply the Jensen bound to get
I 
= 
h(x, y)dydx 
(2.18)
> 
exp (j 
q(x, y) log{xy 
dydx) 
(2.19)
At this point, we constrain q(x, y) to factor into separate functions for x and for y:
q(x, y) = q,(x)q,(y) 
(2.20)
11

with no other constraints on functional form. 
The q, and qy functions are iteratively
optimized to maximize the value of the bound. To see that this is equivalent to (2.16), note
that for any qy we can solve analytically for the optimal qx, which is
qx(x) 
g(x) 
(2.21)
fx g(x)dx
where g(x) 
= 
exp (jqy(y)log 
( 
)dy) 
(2.22)
When we substitute this qx, the bound becomes
I 
> jg(x)dx 
(2.23)
Regardless of which approach we use-implicit or explicit-we can optimize the bound via
the EM algorithm. This is described in Minka (2000c).
Besides variational bounds, there are other integration techniques that are inspired by
mean-field statistical physics. 
Most of these are extensions of TAP, such as the cavity
method (Opper & Winther, 2000a), Bethe approximation (Yedidia, 2000), and Plefka ex-
pansion (Kappen & Wiegerinck, 2000). So far they have been applied to regular structures
such as the Boltzmann machine and rarely to general probabilistic models, where it is less
obvious how to apply them. An exception to this is the paper by Opper & Winther (2000c).
The algorithm they derive is new and accurate, yet coincides with the framework proposed
in this thesis. It will be interesting to see what other algorithms come out of this mean-field
line of research.
For approximating an integral, we thus find ourselves in the following position. We have
methods that work well for simple functions in low dimensions (quadrature) and complex
functions in high dimensions (Monte Carlo). We have methods that are simple and fast but
inaccurate (Laplace's method, variational Bayes). We have methods that apply in special
cases (Jensen bound, TAP). What is missing is a general and accurate deterministic method
in high dimensions, at least for simple functions. That is what this thesis provides.
The approach taken by this thesis continues the path started by the extended Kalman
filter (EKF). The EKF is a sequential method developed for inference in dynamical systems
with nonlinear dynamics. It is not a general method for integration. But there are several
variations on the EKF (Maybeck, 1982) that have promise. One is sequential Monte Carlo,
a family of nondeterministic techniques (Liu & Chen, 2000; Carpenter et al., 1999). Another
is the assumed-density filter, a general deterministic method that approximately minimizes
the KL-divergence D(f I g) between f(x) and its approximation g(x), as shown in figure 2-
1. The next chapter discusses the assumed-density filter and its extension to Expectation
Propagation.
12

Chapter 3
Expectation Propagation
This chapter describes recursive approximation techniques that try to minimize the KL-
divergence between the true posterior and the approximation. Assumed-density filtering
is a fast sequential method for this purpose. Expectation Propagation is introduced as
an extension of assumed-density filtering to batch situations. It has higher accuracy than
assumed-density filtering and other comparable methods for approximate inference.
3.1 
Assumed-density filtering
This section reviews the idea of assumed-density filtering (ADF), to lay groundwork for
Expectation Propagation. Assumed-density filtering is a general technique for computing
approximate posteriors in Bayesian networks and other statistical models. ADF has been in-
dependently proposed in the statistics (Lauritzen, 1992; Bernardo & Giron, 1988; Stephens,
1997), artificial intelligence (Boyen & Koller, 1998b; Opper & Winther, 1999; Barber & Sol-
lich, 1999; Frey et al., 2000), and control literature (Kushner & Budhiraja, 2000; Maybeck,
1982). "Assumed-density filtering" is the name used in control; other names include "on-
line Bayesian learning," "moment matching," and "weak marginalization." 
ADF applies
when we have postulated a joint distribution p(D, 9) where D has been observed and 9 is
hidden. We would like to know the posterior over 9, p(91D), as well as the probability of
the observed data (or evidence for the model), p(D). The former is useful for estimation
while the latter is useful for model selection.
For example, suppose we have observations from a Gaussian distribution embedded in
a sea of unrelated clutter, so that the observation density is a mixture of two Gaussians:
p(x|O) = 
(1 - w)A(x; 9, I) + wiV(x; 0, 101) 
(3.1)
1 
1T
.A(x; m, V) 
= 
1 
2 exp(--(x - m)TV- 1(x -
M)) 
(3.2)
127rV /2 
2
The first component contains the parameter of interest, while the other component describes
clutter. The constant w is the known ratio of clutter. Let the d-dimensional vector 9 have
a Gaussian prior distribution:
p(O) 
~A(0, 
100dI) 
(3.3)
The joint distribution of 9 and n independent observations D = {x 1, ... , xn} is therefore:
p(D,9) 
= 
p(9)JJp(x;i9) 
(3.4)
13

q(0) 
p(0)
t1 (9) 
t2(0) 
t3(0) 
t4(0) 
4(0)
qnew(O
Figure 3-1: Assumed-density filtering computes an exact one-step posterior P(9) and then
approximates it to get qnew(9).
To apply ADF, we first factor the joint distribution p(D, 0) into a product of simple
terms:
p(D, 0) = l t(0) 
(3.5)
There are many ways to do this. As a rule of thumb, fewer terms are better, since it entails
fewer approximations. 
However, we need each term to be simple enough to propagate
expectations through. In the mixture example, we can use the factoring into n + 1 terms
implied by (3.4):
to(9) 
= 
p(O) 
(3.6)
1(O) 
= p(xi9) 
(3.7)
For a general Bayesian network, we would use the factoring into conditional probability
tables: Hlnodes y p(Y pa(Y)), where pa(Y) is the parents of node Y in the graph.
The second step is to choose a parametric approximating distribution. It is essential that
the distribution be in the exponential family, so that only a fixed number of expectations (the
sufficient statistics) need to be propagated. Usually the nature and domain of 9 constrains
the distribution enough that there is no choice left to make. In the clutter problem, a
spherical Gaussian distribution is appropriate. The approximate posterior is therefore
q(9) ~ V(mo, veI) 
(3.8)
Finally, we sequence through and incorporate the terms ti into the approximate poste-
rior. At each step we move from an old q(9) to a new q(9), as shown in figure 3-1. (For
notational simplicity, we drop the dependence of q(9) on i.) Initialize with q(9) = 1. Incor-
porating the prior term is trivial, with no approximation needed. To incorporate the next
term 4(9), take the exact posterior
p(-) = 
q(3.9)
fo t1(O)q(O)dO
and minimize the KL-divergence D(fi(9)||qe"w()) subject to the constraint that q"'e(0) is
a Gaussian distribution. Zeroing the gradient with respect to (mo, vo) gives the conditions
new 
= 
P(9)9 dO 
(3.10)
v~ew±(mew)T(Mnew) 
= 
j 1
(9)9Td9(1)
Vonewd + (M m0e 
0T(M") 
() 
dO 
(3.11)
or in other words, expectation constraints:
Ecnew[]= E [O] 
(3.12)
Eqnew[9T9] 
E4[T9] 
(3.13)
14

We see that the spherical Gaussian distribution is characterized by the expectations
(E[9], E[OTO]). 
For other members of the exponential family, we will get constraints on
different expectations, as illustrated in section 3.3.
To compute these expectations, it is helpful to exploit the following relations:
Z(mO, vO) 
= 
f t()q(O) dO
0o
(3.14)
Vm log Z(mO, va)
E[0]
EgOTO] - Ep [O]T E[0]
/0 
t(O) 
-exp(- 
1 (9 - m)T( 
-- mo)) dO 
(3.15)
= (27rvo)d/ 2  
2vo
_1 
(0 -_ 
)_ 
() 
1
= L (9m0) 
t(/) 
exp(- -( 
- ma)T(O -
mO)) dq3.16)
Z 
ve 
(2irva)d/ 
2va
EP[O] 
mo
- O 
E[ 
m(3.17)
mO + VOVm log Z(mO, vo) 
(3.18)
= 
vad - vo(V Vm - 2Vv log Z(mo, vo)) 
(3.19)
These relations are a property of the Gaussian distribution and hold for any t(O). In the
clutter problem, we have
Z(mO, vO)
r
Vm log Z(mO, vO)
VV log Z(mO, vo)
VmVm -
2Vv log Z(mo, vo)
An estimate of the probability
ply accumulate the normalization
overall normalization for the poste
p(|jD)p(D).
The final ADF algorithm is:
= 
(1 - w)Af(x; mo, (vo + 1)1) + wIV(x; 0, 101)
(1 - w)A((x; m, (vO + 1)I)
(1 - w)AF(x; mo, (vo + 1)1) + w.A(x; 0, 101)
x -m
= 
r
Va + 1
rd 
r(x -
mO)T(x -
mO)
= -
+
2(vo + 1) 
2(vo + 1)2
rd 
(x - ma)T(x - mO)
VO + I 
(v + 1)2
(3.20)
(3.21)
(3.22)
(3.23)
(3.24)
of the data, p(D), is a trivial byproduct of ADF. Sim-
factors Zi(mo, vo) produced by each update, to get an
rior. This normalizer estimates p(D) because p(D, 0) =
1. Initialize mo = 0, va = 100 (the prior). Initialize s = 1 (the scale factor).
2. For each data point xi, update (mo, ve, s) according to
new
Vew
= 
mO + vOrii -
VO + 1
V 22(X, 
- ma)T(x, -
mO)
= 
vo - r 
2 
+ ri(l - ri) 
d
Va + 1 
d(vo+1)2
new 
= 
s x Zj(mo, VO)
(3.25)
(3.26)
(3.27)
This algorithm can be understood in an intuitive way: for each data point we compute its
probability r of not being clutter, make a soft update to our estimate of 9 (ma), and change
our confidence in the estimate (vo). However, it is clear that this algorithm will depend on
15

the order in which data is processed, because the clutter probability depends on the current
estimate of 9.
This algorithm was derived using a spherical Gaussian approximating distribution. If
we use a richer set of distributions like diagonal or full-covariance Gaussians, we can get
a better result, since more expectations will be preserved. Unlike fitting models to data,
there is no overfitting problem when fitting approximations to posteriors. The only penalty
is computational cost.
Figure 3-2 shows the output of this algorithm using three random orderings of the same
data. Synthetic data was generated using w = 0.5 and 9 = 2. 
No theory is available
for how ADF varies with ordering, but some empirical observations can be made. The
error increases whenever similar data points are processed together. Processing the data
in sorted order is especially bad. This is because the algorithm overcommits to one part
of input space, only to realize that there is also data somewhere else. The approximate
mean and especially the variance will suffer because of this. So one approach to improve
ADF is to find an ordering that best reflects the natural variation in the data, if this can
be quantified somehow. Another approach is to modify ADF to eliminate the dependence
on ordering, which is described in the next section.
-
-
Exact
ADF
-1 
0 
1 
2 
3 
4 
5
0
Figure 3-2: The approximate posterior resulting from ADF, using three different orderings
of the same data. The posterior is scaled by the evidence estimate, p(D).
16

q(9) 
q(O) 
q(0)
01((0) 
i2 (0) 
t3 (0) 
t4 (0) 
t(0) 
01(0) 
i2 (0) 
6(0) 
i4 (0) 
i5 (0)
II
il (0 
2 (0 
3 (0 
40 
t5(0) 
i (0 
2 (0 
3 (0 
4 (0 
5(0
qnew(0) 
q ew(0)
Figure 3-3: An alternative view of ADF as approximating each term and then computing
qnew(0) exactly. EP refines each term in the context of all other terms.
3.2 
Expectation Propagation
This section describes the Expectation Propagation algorithm and demonstrates its use
on the clutter problem. 
Expectation Propagation is based on a novel interpretation of
assumed-density filtering. Normally we think of ADF as treating each observation term ti
exactly and then approximating the posterior that includes ti. But we can also think of it
as first approximating ti with some ii and then using an exact posterior with ii (figure 3-3).
This interpretation is always possible because we can define the approximate term ii to be
the ratio of the new posterior to the old posterior times a constant:
j(0) = Zqnew() 
(3.28)
q(O)
Multiplying this approximate term by q(9) gives q"' (9), as desired. An important property
is that if the approximate posterior is Gaussian, i.e. an exponentiated quadratic in 9, then
the equivalent term approximation, which is a ratio of these, will also be an exponentiated
quadratic. If the approximate posterior is any distribution in the exponential family, then
the term approximations will have the same functional form as that distribution.
The algorithm of the previous section can thus be interpreted as sequentially comput-
ing a Gaussian approximation i4(9) to every observation term ti(O), then combining these
approximations analytically to get a Gaussian posterior on 9. Under this perspective, the
approximations do not have any required order-the ordering only determined how we made
the approximations. We are free to go back and refine the approximations, in any order.
From this idea we get Expectation Propagation.
Consider how this new interpretation applies to the previous section. From the ratio of
spherical Gaussians, we get the following term approximation:
Zj(mo, vo) 
= 
ti(O)q()d9 
(3.29)
i(9) 
Z (mo,vO) qe() 
(3.30)
q(9)( )d/2 
1
- Z~( 
O, ) 
) 
exp( 
(- M 0 
)T(O 
mnew))
ziM,/ 
e 
2V new0
exp( 1 (9 -
me)T(9 -
mO)) 
(3.31)
2vo
Now define (mi, vi) according to
V-1 
1 
ew)-l -
V_1 
(3.32)
17

v(Vyew)-lmye 
-
viv- me 
(3.33)
= 
mO + (v; + vo)vl(m Cw -
mO) 
(3.34)
to get the simplification
i() 
Zi(mo, vo) 
V +V 
exp(-1 (0 - mi)T(o - mi)
1
exp( 
(M - mO)T(m, - me)) 
(3.35)
2(vi + vo)
Zi(M, O) 
-,(O; mi, V;I) 
(3.36)
A (mj; mo, (v; + vo)I)
By construction, multiplying q(9) by this approximation gives exactly the ADF update,
with the proper scale factor. The Ar notation is here used formally as shorthand, not to
imply a proper density. The approximation is an exponentiated quadratic in 0, which has
the form of a Gaussian with mean mi and variance vI, times a scale factor. What makes
this approximation different from a Gaussian is that the variance vi may be negative or
infinite, which for ti is perfectly fine: negative variance corresponds to a function that
curves upward and infinite variance corresponds to a constant.
Figure 3-4 illustrates ti(9) and i(9) for the clutter problem. The exact term t;(O) =
p(x;i9) is a Gaussian in 9, raised by a constant. The approximate term ifi() 
is an expo-
nentiated quadratic. The current posterior q(9) determines where the approximation will
be accurate. When q(9) is narrow, i.e. vo is small, then it(9) is accurate only in a narrow
range of 9 values, determined by mo. When vo is large, then i(9) tries to approximate 1(O)
more broadly, and mg is less important.
18

M = 3, vo = 0.1
-5 
0 
5 
10
m- 
0, v= 
1
.
*. 
....
5 
0 
5
M = 0, vO= 10
0
0
-5
5
0
I 
.
I
5 
0 
5
m = 3, vO = 1
5 
0 
5
9
mo 3, vo 
10
10 
-5
0
0
0
10
5
Figure 3-4: The approximate term i(9) as a function of 0, plotted versus the exact term
t(9) and the current posterior q(9). The current posterior controls where the approximation
will be accurate.
19
I 
.
I
MO = 0, VO = 0.1
-
0
-
-

So far we have just rewritten the ADF updates in a different way. To get EP, we
refine the term variables (mi, v;) based on all of the other approximations. The general EP
algorithm is:
1. Initialize the term approximations tz
2. Compute the posterior for 9 from the product of ii:
q ne() 
= 
~ (0) 
(3.37)
f Hi ii(O)dO
3. Until all ii converge:
(a) Choose a ii to refine
(b) Remove i from the posterior to get an 'old' posterior q(O), by dividing and
normalizing:
q(9) Oc 
( 
(3.38)
(For notational simplicity, we drop the dependence of q(O) on i. However q"ew(O)
is not a function of i.)
(c) Compute the posterior qne"(0) and normalizing factor Zi from q(O) and ti(O) via
ADF.
(d) Set
qnew () 
(3.39)
q(9)
4. Use the normalizing constant of q"f"(0) (from (3.37)) as an approximation to p(D):
p(D) ~7iT(0)dO 
(3.40)
At convergence, the result will be independent of processing order, as desired. 
In this
algorithm, we have used division to remove ii from the posterior. 
Step 3b can also be
performed without division, by accumulating all terms except for ii:
(X 
1c ij 
i(0) 
(3.41)
bsit
but division is usually more efficient.
20

3.2.1 
The clutter problem
For the clutter problem of the previous section, the EP algorithm is:
1. The term approximations have the form
ti (0) = si exp(- 
( 
-
mz)T(9 -
mi))
2vi
Initialize the prior term to itself:
VO 
100 
(
s 
= 
(27rvO)-d/2 
(
Initialize the data terms to 1:
3.43)
3.44)
3.45)
Mi 
= 
0
Si 
= 
1
2. mnew = movew = VO
3. Until all (mi, vi, si) converge (changes are less than 10-4):
loop i = 1, ... , n:
(a) Remove ii from the posterior to get an 'old' posterior:
V-1
V0
= 
(Vnew)
1 -
V;- 1
= 
ve(vew)~lmew -
vev 1 mi = m-ew + vovne (mnew - mi) (3.50)
(b) Recompute (mnew, vnew, Zj) from (mo, vo) (this is the same as ADF):
new
new
VO
(1 - w)Ar(x,; mo, (vo + 1)1)
(1 - w)AF(x ; mo, (vo + 1)1) + wAf(xi; 0, 101)
2(X. -
mO)T(x. -
mO)
- ri) OZ 
I
d(vo+ 1)2
0 + 1)1) + wA/(xi; 0, 101)
= 
mo + vori Xi-
VO + 1
+ 1
= 
vo - ri- 
+ ri(1
VO + I
Z 
= 
(1 - w)(xi; me, (v
(3.51)
(3.52)
(3.53)
(3.54)
(c) Update ii:
.-1
V.
= 
(V new)-l -
V- 1
ri
V\ + 1+
- r(1 -
r)X_ 
-
MO)
d(vo + 1)2
M 
= 
m + (vi + vo)v~1(m new -
me)
me + (vi + vo)r XiMO
Ve + 1
Zi
(2rv,)d/ 2A[(m; me, (vi + vo)I)
- v0  
(3.56)
(3.57)
(3.58)
(3.59)
21
(3.42)
vi 
= 
00 
(3.46)
(3.47)
(3.48)
(3.49)
(3.55)
S i

4. Compute the normalizing constant:
(m ew)Tmnew 
mT
B = 
n ew 
m0 
(3.60)
V0 
2 
vi
p(D) 
~ 
i 
i(O)dO 
(3.61)
= 
(2 WVoe0)d/2 exp(B/2) 
si 
(3.62)
i=O
Because the term approximations start at 1, the result after one pass through the data is
identical to ADF. On later passes, the refinement may sometimes fail due to a negative
value for v. 
This happens when many of the vi are negative and we wish to refine a term
with positive v-. In the subtraction step (3.49), we subtract a positive value from Vnew and
are left with something negative. From this Zi does not exist and the algorithm fails.
Another problem is that the term approximations may oscillate without ever converging.
This tends to occur in conjunction with negative vi's, which suggests a simple work-around:
force all vi > 0 by relaxing some of the expectation constraints. Whenever a vi would become
negative, make it large (108) instead and set Oe = vo (because VneW -
(v- 1 +v. 
)-1). This
trick does provide convergence, but leads to inaccurate posteriors since we are effectively
ignoring some of the data. When EP does converge with negative vi's, the result is always
better than having forced vi > 0. So a better solution would be to use a refinement algorithm
with better convergence properties-see the discussion at the end of section 4.1.
3.2.2 
Results and comparisons
This section evaluates ADF and EP on the clutter problem and compares them to four other
algorithms for approximate inference: 
Laplace's method, variational Bayes, importance
sampling (specifically likelihood-weighted sampling), and Gibbs sampling.
In Laplace's method, we first run EM to get a MAP estimate of 9. Then we construct a
Gaussian approximation to the posterior by using the curvature of the posterior at 9. This
curvature is (Minka, 2000c)
H = Vr 
log p(O1D) 
= 
-
r1I + 
r1 (1 - ri)(xi -
)(xz -
)T 
(3.63)
(1 - w)K(x; 9, I)
(1 - w)V(x; 9, I) + wV(x; 0, 101)
The Gaussian approximation and normalizing constant are
p(O|D) 
.AJ(9, -H- 1 ) 
(3.65)
p(D) 
p(D,9)(2r)d/2|-HI-1/ 2  
(3.66)
For variational Bayes, we lower bound the joint distribution by bounding each data term
(Minka, 2000c):
(1 - w)A (x; 0, I) 
ll 
(W 
(x; 0, 10I)i 
(367
Xxi 0) 
i, 
qi2
22

The variational parameters qij are optimized to give the tightest bound. Given the bounds,
the posterior for 9 is Gaussian:
KJ = S ij 
(3.68)
1
j 
-K 
qig xi 
(3.69)
Sj 
= 
qij (x, - X3)(X, -3 .)T 
(3.70)
V 
= 
K 
(3.71)
100
mO 
= 
VOK 1k 1  
(3.72)
p(9jD) 
.Af(mo,VO) 
(3.73)
1 
1
p(D) 
I 
2(Xi; 
0,1 /K 1 + 100) exp(--tr(Si)) 
(3.74)
(2 7r)d(K-1)/2Kd/ 
2
1 
1
1 d2)V(x2; 0, 101/K2) exp(- -tr(S2/10)) 
(3.75)
(2 7r1 0)d(K2-1)/2K/ 2  
2
q i j 
q i 
2( 
3 .7 6 )
qi) 
qi2
For importance sampling, or more specifically likelihood-weighted sampling, we draw
samples {1, ... , Os} from p(9) and approximate the integrals via
1S
p(D) 
S 
p(D I 0) 
(3.77)
J: -p( D|6;)
E[91D] 
Z= 
pD 
(3.78)
_1 p(D10i)
For Gibbs sampling, we introduce hidden variables ci which indicate whether xi is clutter
or not. Given a choice for 9, we sample ci from a Bernoulli distribution with mean ri. Then
given ci, we form an exact Gaussian posterior for 9 and sample a new 9. The average of the
O's that arise from this process is our estimate of the posterior mean.
The three deterministic algorithms, EP, Laplace, and VB all obtain approximate pos-
teriors close to the true one. The sampling algorithms only estimate specific integrals. To
compare the algorithms quantitatively, we compute the absolute difference between the es-
timated and exact evidence and the estimated and exact posterior mean. Figure 3-5 shows
the results on a typical run with data size n = 20 and n = 200. It plots the accuracy vs. cost
of the algorithms on the two integrals. Accuracy is measured by absolute difference from
the true integral. Cost is measured by the number of floating point operations (FLOPS) in
Matlab, via Matlab's flops function. This is better than using CPU time because FLOPS
ignores interpretation overhead.
The exact shape of these curves, especially the EP curve, should not be taken too
seriously since the convergence properties of EP, including whether it converges at all, can
be substantially affected by the particular dataset and data ordering. The Laplace curve is
generated by applying (3.66) to intermediate values of 9, even though the gradient may not
be zero.
23

Evidence
Importance
VB
Laplace 
~'-
EP 
'pr
0 2 
10 3 
10 4 
10 5 
1
FLOPS
Data size n = 20
Posterior mean
103 
104
FLOPS
n = 20
10 99
10 200
10-201
10 -202
10-203
10 
204
10 20
10 3
100
10-1
10-2
10-3
10 -
0
LWi
-_ 
10-
10 -
10 
106 
103
Evidence
104 
10
FLOPS
n = 200
Posterior mean
104 
10
FLOPS
n = 200
106 
10
106 
10
Figure 3-5: Cost vs. accuracy curves for expectation propagation (EP), Laplace's method,
variational Bayes (VB), importance sampling, and Gibbs sampling on the clutter problem
with w = 0.5 and 9 = 2. Each 'x' is one iteration of EP. ADF is the first 'x'.
24
10 17
0- [
10-19
0 
-20
E10
-21
10
10-
Importance
VB
Laplace
EP
10-23
1
10 0
10
0-2
10-1
W
-4
10
-5
10
10-6 L
102
Importance
Laplace 
- VB
Gibbs
EP
lmportanc6
Gibbs
VB
Laplace
EFP
1
6"
1
1

ADF is equivalent to the first iteration of EP (the first 'x' on the curve). It performs sig-
nificantly worse than variational Bayes and Laplace's method, which are offline algorithms.
Yet by refining the ADF approximations, we move from last place to first. The worth of an
algorithm is not always what it seems.
It is interesting to see how the different properties of the algorithms manifest themselves.
EP, Laplace, and VB all try to approximate the posterior with a Gaussian, and their
performance depends on how well this assumption holds. With more data, the posterior
becomes more Gaussian so they all improve. Sampling methods, by contrast, assume very
little about the posterior and consequently cannot exploit the fact that it is becoming more
Gaussian.
However, this disadvantage turns into an advantage when the posterior has a complex
shape. Figure 3-6 shows a run with n = 20 where the true posterior has three distinct
modes. EP crashes on this data, due to negative variances. Restricted EP, which forces
all vi > 0, does converge but to a poor result that captures only a single mode. Laplace's
method and variational Bayes are even worse. For all three algorithms, the error in the
posterior mean increases with more iterations, as they focus on a single mode. Sampling
methods, by contrast, are only slightly challenged by this posterior.
25

0.35
0.2
U)
0~
-
Exact
-
EP
.... 
VB
-
Laplace
-
:- 
-
7
6
5
C6-
Z4
3
2
1
0
10
10
theta
(a)
Posterior mean
Laplace
VB EP 
Gibbs
Importance
103 
104
FLOPS
(c)
10-24
10-25
0
w
10-26
10-271
105 
106 
102
-5
0
theta
(b)
5
10
Evidence
103 
104
FLOPS
(d)
105 
106
Figure 3-6: A complex posterior in the clutter problem. (a) Exact posterior vs. approx-
imations obtained by Restricted EP, Laplace's method, and variational Bayes. (b) Exact
posterior vs. approximation obtained by Gibbs sampling and complete conditional density
averaging (Gelfand & Smith, 1990). (c,d) Cost vs. accuracy. EP only converged when
restricted to positive vi.
26
x 10-26
8 r---
0.15-
0.1 -
0.05 -
-6 
-4 
-2 
0 
2 
4 
6 
-10
Exact
-
-
Gibbs
'--
0
2
w
Laplace
VB
EP
Impm rtance
10 2
10 2
0.3-
0.25 -
10-1

3.3 
Another example: Mixture weights
This section demonstrates how the structure of ADF and EP are preserved with a differ-
ent approximating distribution-a Dirichlet distribution. Let the probabilistic model be a
mixture of K known densities, p1(x), ... , pK(x), with unknown mixing weights w:
p(xIw) 
Z wkpk(x) 
(3.79)
k
ZWk 
= 
1 
(3.80)
k
p(w) 
= 
1 
(3.81)
p(D, w) 
= p(w) 
p(x dw) 
(3.82)
We want to compute the posterior p(wID) and the evidence p(D). ADF was applied to this
problem by Bernardo & Giron (1988) and Stephens (1997).
Break the joint distribution p(D, w) into n + 1 terms as given by (3.82). The parameter
vector w must lie in the simplex Ek wk = 1, so a Dirichlet distribution is an appropriate
approximation to the posterior. The approximate posterior will have the form
q(w) 
F(Zk ak) 
W 
1 
(3.83)
Ilk F(ak) 
k
3.3.1 
ADF
For ADF, we sequence through and incorporate the terms ti into the approximate posterior.
The prior is a Dirichlet distribution, so we initialize with that. To incorporate a data term
ti(w), take the exact posterior
ti(w)q(w)
f, ti(w)q(w)dw
and minimize the KL-divergence D(P(w)lq new(w)) subject to the constraint that qnew(w)
is a Dirichlet distribution. Zeroing the gradient with respect to a e 
gives the conditions
(k = i ..., K)
new(ae) 
-
I(Z 
a7We) 
= 
J 
(w) log(wk)dw 
(3.85)
d log F(a) 
(3.86)
where WL(a) 
= 
da.6
da
As usual, these are expectation constraints:
Enew[log(wk)] 
= 
E [log(wk)] 
k = 1, ..., K 
(3.87)
The Dirichlet distribution is characterized by the expectations E[log(wk)].
How do we solve for anew? Given the values E[ 1 log(wk)], Newton's method is very effec-
tive, and EM-like algorithms are also possible (Minka, 2000b). To compute the expectations,
use integration by parts to find
Z(a) = 
t(w)q(w)dw 
(3.88)
EP[log(wk)] = 
'(ak)- 
T(J aJ)+VlogZ(a) 
(3.89)
.
27

This is a property of the Dirichlet
problem, we have
1(w)
Z(a)
Vak log Z(a)
distribution, valid for any t(w). In the mixture weight
= 
>pk(x)Wk 
(3.90)
k
= 
IPk(x)Eq[Wk] = Ek Pk(x)ak 
(3.91)
k 
k ak
=Z~a
3  
/Z(a) 
(3.92)
Zaj 
(E.- aj)2
Pk(X) 
1
Zj p1 (x)a.-E 
Za.-
So the final ADF algorithm is
1. Initialize a = 1 (the prior). Initialize s = 1 (the scale factor).
2. For each data point x, in turn, solve the equations (k = 1 
K)
new(ae) 
-
(anew) 
= 
-
ak) 
-
P(ag) 
-
P(Xi) 
_
3 .
-j 
pjp(xi)aj 
E j a.
to get ane". Update s via
s new 
s xZ 
(a) = s  
pi(xi)a.
T.- a.,
(3.93)
(3.94)
(3.95)
Bernardo & Giron (1988) and Stephens (1997) show that this algorithm performs better
than simpler rules like Directed Decision and Probabilistic Teacher. With a little extra
work we can get an EP algorithm that is even better.
3.3.2 
EP
Divide the new posterior by the old to get a term approximation parameterized by b:
=Z 
qnew(O)
q(O)
Z 
(Eka ew) HkF(ak) 
1l bk
17k F(ae') F(Ek ak) 
k
where bk 
ak"e 
-
ak
The EP algorithm is:
1. The term approximations have the form
.(O) = si 
W bi"k
k
(3.96)
(3.97)
(3.98)
(3.99)
Initialize the prior term to itself:
bo
s8
= 
0
= 1
Initialize the data terms to 1:
(3.100)
(3.101)
b 
-
0
s 
= 
1
28
(3.102)
(3.103)
(0)

2. anew = 1 + EZ=Obi= 1
3. Until all (bi, si) converge:
loop i = 1, ... , n:
(a) Remove ii from the posterior to get an 'old' posterior:
a 
= 
anew - bi 
(3.104)
(b) Recompute anew from a by solving (k = 1,..., K)
new(ae) 
- hI(L a"') 
= 
T(ak) -
T (I: a)+ 
-
P 
(3.105)
p (x )aj 
a.
(c) Update i;:
b 
= 
anew - a 
(3.106)
= 
Fa (Zk anew) Hk F(ak)
SHZ 
F.agj) 
Z(Zk 
ka) 
(3.107)
fUk F(anew) T ( E 
ak)
4. Compute the normalizing constant:
p(D) 
Hk(anew)n
p(D) 
F 
aw) 
si 
(3.108)
3.3.3 
A simpler method
The complexity of this algorithm results from preserving the expectations E[log(wk)]. 
Cow-
ell et al. (1996) suggest instead preserving only the first two moments of the posterior dis-
tribution, i.e. the expectations (E[wk], Ek E[wk]). Preserving these expectations will not
minimize KL-divergence, but allows a closed-form update. Their proposed ADF update is:
Mik 
= 
E[wk] 
(3.109)
ak 
Pk(Xi) + 1:3 p3-(xi)a3-
(3.110)
Z (a) E a. 
1+ E3 a.
m(2) 
= E [w] 
(3.111)
(ak + 1)ak 
2pk(xi) +E p3(xi)aj 
(3.112)
Z (a)(1 + I, a3)(I, a.-) 
2 + 1 3 a.
ne 
k(M k -
-M (2))
anew 
) Mik 
(3.113)
k 
(~2) 
_M2
I~k (ink 
-ik)
This update can be incorporated into EP by replacing (3.105). The benefit of this change
is determined empirically in the next section.
29

4
X 10
6,
2
0
6
4
2
n
x 10~4
0.2 
0.4 
0.6 
0.8 
1
w
0 
0.2 
0.4 
0.6 
0.8
w
(a)
10-46
0
1048 [
Evidence
1
10
FLOPS
(b)
Figure 3-7: (a) Exact posterior vs. approximations, scaled by evidence p(D). The varia-
tional Bayes approximation is a lower bound while the others are not. The approximation
from EP2 is very similar to EP. (b) Cost vs. accuracy in approximating p(D). ADF is the
first 'x' on the EP curve. EP2 uses the fast update of Cowell et al.
3.3.4 
Results and comparisons
EP is compared against Laplace's method and variational Bayes. For Laplace's method,
we use a softmax parameterization of w (MacKay, 1998), which makes the approximate
posterior a logistic-normal distribution (the result of passing a Gaussian random variable
through the logistic function) (Aitchison & Shen, 1980). For variational Bayes, we use a
Jensen bound as before, to get a Dirichlet approximation (Minka, 2000c).
Figure 3-7 shows a typical example with w = [0.5, 0.5], n = 50. The component densities
were two closely spaced Gaussians:
(3.114)
(3.115)
P2(x) 
~ 
A(1, 3)
p2(X) 
~ AI(1, 3)
The accuracy of the three methods at estimating p(D) can be seen immediately from the
different posterior approximations. EP, using either the exact update or the fast update,
is the most accurate. Even ADF, the first iteration of EP, is very accurate. The success
of the fast update illustrates that it is sometimes good to consider criteria other than KL-
divergence.
30
-
Exact
-
- Laplace
- -- .VB
Q
0~
0
IExact
-
-
-
-EP
NI-
10 
1 
.- 
VB
Laplace
EP2 
EP
4
0-49
10 4
10 5

Chapter 4
Disconnected approximations of
belief networks
This chapter describes how Expectation Propagation can be used to approximate a belief
network by a simpler network with fewer edges. 
Such networks are called disconnected
approximations.
4.1 
Belief propagation
Belief propagation is normally considered an algorithm for exact marginalization in tree-
structured graphs. But it can also be applied to graphs with loops, to get approximate
marginals (Frey & MacKay, 1997; Murphy et al., 1999). In this section, it is shown that
belief propagation applied in this heuristic way is actually a special case of Expectation
Propagation. Loopy belief propagation arises when the approximating network in EP is
required to be completely disconnected (all variables independent). This is interesting be-
cause loopy belief propagation is normally considered distinct from techniques that compute
approximate models (but see Rusmevichientong & Roy (1999) for a similar interpretation).
The equivalence also suggests how to improve both algorithms.
First consider the ADF case, where a completely disconnected approximation was stud-
ied by Boyen & Koller (1998b). 
Let the hidden variables be x1 , ... , XK and collect the
observed variables into D = {Y1, ..., YN}. A completely disconnected distribution for x has
q2(x 2 )
qi(x 1 )
(X1,x2,X3,X4,X5) 
q3X3)
q5(x5 ) 
q4 (x 4)
Figure 4-1: Loopy belief propagation corresponds to approximating a complex Bayesian
network by a disconnected network
31

the form (figure 4-1)
K
q(x) =1 
q k(xk) 
(4.1)
k=1
To incorporate a term t;(x), take the exact posterior
x) = 
(x)q(x) 
(4.2)
and minimize the KL-divergence D(f(x)J|q"e(x)) subject to the constraint that q"ew(x) is
completely disconnected. Zeroing the gradient with respect to q"e' gives the conditions
qke"(xk) 
= 1(xk)= >P(x) 
k = 1, ... , K 
(4.3)
x\xk
i.e. the marginals of q"'e 
and P must match. As usual, these are expectation constraints:
Eqnew [6(xk -
V)] 
= 
E [(Xk - v)] 
k = 1,..., K, all values of v 
(4.4)
From this we arrive at the algorithm of Boyen & Koller (1998b):
1. Initialize qk(Xk) =1 and s = 1
2. For each term t,(x) in turn, set q11W to the kth marginal of P:
q"e'(xk) 
= 
P (x) 
(4.5)
X\xk
= 
Y ti(x)q(x) 
(4.6)
X\xk
where Zi 
= 
t(x)q(x) 
(4.7)
x
Update s via
S new 
s x Z' 
(4.8)
While it appears that we have circumvented the exponential family, in practice we must
still use it. The marginals of P are feasibly computed and stored only if P is in the exponential
family, which means q(x) and tQx) must also be in the family. The most common use of
belief propagation is for discrete x where P is a multinomial distribution.
Now we turn this into an EP algorithm. From the ratio qn6W/q, we see that the approx-
imate terms 14x) are completely disconnected. The EP algorithm is thus:
1. The term approximations have the form
(x) = 
igik(xk) 
(4.9)
k
Initialize Qi(x) = 1.
2. Compute the posterior for x from the product of ii:
qk 
(xk) 
Cx ]jiik(Xk) 
(4.10)
32

3. Until all ii converge:
(a) Choose a ii to refine
(b) Remove ii from the posterior. For all k:
qk(xk) 
OC 
(Xk) 
(4.11)
tik(Xk)
Oc 
fJk(Xk) 
(4.12)
jii
(c) Recompute q"ew(x) from q(x) as in ADF.
new 
1
qk '(Xk) 
= 
- E t,-(x)q(x) 
(4.13)
Zi= 
1 t(x)q(x) 
(4.14)
(d) Set
itk(Xk) 
= 
qk 
(Xk) 
-
ZX\xk tZ(x 
(4.15)
qk(xk) 
qk(xk)
= Z t(x) J7 qj (x,-) 
(4.16)
x\xk ~
4. Compute the normalizing constant:
p(D) 
~ 
( 1 
ik(Xk) 
(4.17)
k 
Xk 
i
To make this equivalent to belief propagation (see e.g. Murphy et al. (1999)), the original
terms t, should correspond to the conditional probability tables of a directed belief network.
That is, we should break the joint distribution p(D, x) into
p(D, x) = f p(xkIpa(xk)) fIp(y Ipa(yj)) 
(4.18)
k 
a
where pa(X) is the set of parents of node X. The network has observed nodes yj and hidden
nodes Xk. The parents of an observed node might be hidden, and vice versa. The quantities
in EP now have the following interpretations:
" qkew(xk) is the belief state of node Xk, i.e. the product of all messages into xk.
* The 'old' posterior qk(xk) for a particular term i is a partial belief state, i.e. the
product of messages into xk except for those originating from term i.
" When i $ k, the function iik(xk) is the message that node i (either hidden or observed)
sends to its parent Xk in belief propagation. For example, suppose node i is hidden
and ti(x) = p(x;lpa(xi)). The other parents send qj(xj), and the child combines them
with qj(xj) to get (figure 4-2(a))
tik(y) = Sp(xpa(x;))q,(x) 
r 
qj(x 3 ) 
(4.19)
x\xk 
other parents j
33

q(xq)
k 
xX
tii
q(x%) 
X
tik(xk) 
x 
|xXxJ)qi(xi)qj(xj) 
(Xi=Z) 
p(x |xk,x )qk(xk)qj(x I
(a) 
(b)
Figure 4-2: The messages in belief propagation correspond to term approximations in EP
* When node i is hidden, the function i;j(xi) is a combination of messages sent to node
i from its parents in belief propagation. 
Each parent sends qj(xJ), and the child
combines them according to (figure 4-2(b))
i;i(xi) = 
E 
p(xi|pa(xi)) 
qj(xj) 
(4.20)
pa(x ) 
parents '
Unlike the traditional derivation of belief propagation in terms of A and 7r messages, this
derivation is symmetric with respect to parents and children. In fact, it is the form used in in
factor graphs (Kschischang et al., 2000). All of the nodes that participate in a conditional
probability table p(Xjpa(X)) send messages to each other based on their partial belief
states.
This algorithm applies generally to any decomposition into terms t(x), not just that
given by (4.18). For example, equivalence also holds for undirected networks, if the terms ti
are the clique potentials-compare to the equations in Yedidia et al. (2000). See section 4.2
for an example with an undirected network.
The remaining difference between belief propagation and EP is that belief propagation
does not specify the order in which messages are sent. In the EP algorithm above, each
node must exchange messages with all of its parents at each step. However, we can relax
this condition by considering a simple variation of EP, where we do not minimize KL-
divergence completely at each step, but only partially. Specifically, we only update one of
the tik functions each time. With this variation, we can pass messages in any order. This
reordering will not change the result at convergence.
This equivalence shows that there is a close connection between Boyen & Koller's al-
gorithm and loopy belief propagation. Boyen & Koller's algorithm for dynamic Bayesian
networks is simply one pass of loopy belief propagation where each term ti is the prod-
uct of multiple conditional probability tables-all of the tables for a given timeslice. This
equivalence was also noticed by Murphy & Weiss (2000), though described in terms of
modifications to the graph.
The equivalence also tells us two things about EP:
* Even though EP is derived as an approximation, in some cases the final expectations
may be exact. We know this because the marginals resulting from belief propagation
are exact on tree-structured networks. But for other EP algorithms we do not yet
know how to predict when this will happen.
34

* EP may not converge for some models and/or approximating densities. We know this
because belief propagation does not always converge on graphs with loops.
Unlike the clutter problem of section 3.2.1, there is no obvious constraint like positive
variance that we can use to ensure convergence. The convergence of EP can be helped in two
ways. One way is to improve the quality of the approximation. For example, Yedidia et al.
(2000) have shown that clustering nodes in a belief network can encourage belief propagation
to converge (as well as improve its accuracy). Clustering corresponds to an approximating
density that is partially disconnected, such as q(x) = q(x1, x 2 )q(X 3 , x 4 ). As described in
section 4.2.1, we can also cluster some of the terms ti(x) to get fewer approximations. Or
we can use a Markov chain approximation, as described in section 4.2.2.
A second way to improve convergence is to devise a better iteration scheme. Analogous
to belief propagation, EP can be derived by minimizing a free energy-like objective. The
expression is similar to that of Yedidia et al. (2000) but involving expectation constraints.
Different EP iterations can be found by applying different optimization techniques to this
objective. (The same applies to belief propagation.) This technique will be described more
fully in a separate paper.
4.2 
Extensions to belief propagation
This section shows how the flexibility of EP allows us to extend loopy belief propagation
for higher accuracy. In ADF/EP there are two decisions to make: how to factor the joint
distribution p(x, D) into terms ti(x) and what approximating family q(x) to apply. As a
running example, consider the undirected network in figure 4-3. The joint distribution of
all nodes is proportional to a product of pairwise potentials, one for each edge:
p(x 1, x 2, x 3, X4 , xS, X6 ) oc h(xi, x 2)h(x 2, X3)h(X 3 , x4 )h(x 1 , x 4)h(x 4, x 5)h(xs, X6 )h(x 3 , X6 )
(4.21)
We want to approximate this distribution with a simpler one.
e 
4 -
3
X5  
X6
Figure 4-3: An undirected belief network
35

4.2.1 
Grouping terms
Belief propagation arises when the EP terms ti(x) correspond to the edges of the graph,
that is ti(x) = h(x1,x
2), t 2(x) = h(X 2,X 3), etc. Instead of breaking the joint into seven
factors, we could break it into only two factors, say
t1(x1,x
2,x 3,x 4) 
= 
h(x1,x 2)h(x 2, X3)h(x 3 ,x 4)h(x,x
4 )
12(x 3 , x4 , X5 , X6 ) 
= 
h(x 4, X5)h(xs, X6)h(x 3 , X6 )
(4.22)
(4.23)
as illustrated in figure 4-4. Let the
as before.
approximating family q(x) be completely disconnected
t1
t 2
X5
X6
Figure 4-4: Belief propagation can be improved by factoring the joint distribution into
subgraphs instead of individual edges
It is sufficient to consider the ADF updates. Start with a uniform
(assuming all variables are binary):
1
q(x) =
The first step of ADF will be
qnew(x
new
q2  
(X
qgewx
qa"ew(x
Z 
= 
t1(x1, x2,x3, X4)q(x)
1
x) 
= 
t(x1, x 2, x 3 , x 4)q(x)
)= 
(i) 
= 
2 11(x1 , x 2, X3 , x 4)q(x)
2) 
= 
g 
3 t1 (x1 , x 2 , x 3 , x 4 )q(x)
x\T2
5)
6)
= 
q5(x 5)
= 
q6(X6)
(unchanged)
(unchanged)
distribution on x
(4.24)
(4.25)
(4.26)
(4.27)
(4.28)
(4.29)
(4.30)
(4.31)
36

The second step of ADF will be
Z 
= 
t2(3, x4, X5, X6)q(x) 
(4.32)
AX)= 
+t 2 (x 3 , X4, X5 , X6 )q(x) 
(4.33)
q"'w(x) 
= 
qi(x 1 ) 
(unchanged) 
(4.34)
q2"'e(x2) 
= 
q2 (X 2) 
(unchanged) 
(4.35)
q3e(X
3 ) 
= 
1 : t 2 (X 3 , X4 , X5 , X6)q(x) 
(4.36)
x\r 3
(4.37)
In the usual way, these ADF updates can be turned into an EP algorithm, which will be
more accurate than belief propagation. However, it will involve significantly more work,
since each message requires marginalizing three variables (the number of variables in each
term, minus one) instead of marginalizing just one variable.
4.2.2 
Partially disconnected approximations
Besides the factoring of the joint into individual edges, belief propagation arises when the
approximating network is completely disconnected: q(x) = qi(x1)q2(X2) - - -. Instead of
going to this extreme, we can use a Markov chain approximation, as shown in figure 4-5:
q(x) = q(x 1 )q(X 2|X1 )q(x 3|X2)q(X 4 |x3 )q(X 5 lx
4)q(X 6jx 5 ) 
(4.38)
Let the terms t,(x) be the individual edges, as in belief propagation. Partially disconnected
approximations have previously been used in ADF by Frey et al. (2000) and in the context
of variational bounds by Ghahramani & Jordan (1997), Barber & Wiegerinck (1998), and
references therein.
£1 
£2
X4 
£3
Figure 4-5: Belief propagation can also be improved by using a Markov chain approximation
instead of a completely disconnected approximation
Note that we could equally well parameterize the Markov chain as
q(x) = q(xlx
2)q(X 2 |X 3)q(X 3)q(X4lX 3)q(X5 lx4)q(X 6 lX 5 ) 
(4.39)
37

Before, x, was the root; now x 3 is the root. Given a distribution in one parameterization,
it is straightforward to convert to another parameterization. This will be called changing
the root.
In ADF, we incorporate edges h(xi, xj) from the original graph one at a time. We take
the exact posterior
(x) -
t(x)q(x) 
(4.40)
Ex t,-(x)q(x)
and minimize the KL-divergence D(f(x)llqne.(x)) subject to the constraint that qe"(x)
has the form (4.38). Zeroing the gradient with respect to qne, gives the conditions
q"W(xi) 
= 
p(x 1) 
(4.41)
qne(x
2 x1) 
= P(x 2 |x 1 ) 
(4.42)
qne(x
3 1x2 ) 
= 
P(x 31x 2 ) 
(4.43)
These directly generalize the marginal constraints (4.3) for belief propagation.
The approximation we have chosen preserves all edges in the graph except two. The
preserved edges can be incorporated into q(x) without approximation:
q(x) cx h(xi, x 2 )h(x 2 , X3)h(x 3 , x 4 )h(x 4 , x5)h(X5 , X6 ) 
(4.44)
Now we process the edge h(x 1, x4). The marginals of the exact posterior are
Z 
= 
h(x1, x4)q(x) 
(4.45)
X
-
E 
h(xi, x4)q(x
1 )q(x
2|x 1 )q(X 3jx2)q(x
4 |x 3) 
(4.46)
X1 X2,X3,X4
A(xi) 
I T 
h(x 1, x 4)q(x) 
(4.47)
E 
h(x1, x4 )q(xi)q(x 2|x1)q(x 3 lx 2)q(x 4|x 3 ) 
(4.48)
X2 X3 ,X4
P(x 2 ,x 1 ) 
-
1 
h(x,x
4)q(x) 
(4.49)
x\(ri,X2 )
-
Z E h(x 1, x4)q(x
1 )q(x 2 |1x)q(x 3 lX 2)q(x 4|x 3 ) 
(4.50)
a3 ,
4
P(x 2 lx1) 
= 
P(X2,X 1) 
(4.51)
(same for x 3 and x 4 )
P(x 51x4 ) 
= 
q(x 5 lx
4 ) 
(4.52)
P(x 6 |x 5) = 
q(x 6 |x 5) 
(4.53)
There is a simple pattern going on here. Adding the edge (x 1 , x 4 ) creates a loop in P,
involving the nodes x 1 , ... , x4 , which include the root. The conditional distributions for
nodes outside the loop do not change. The marginal for a node inside the loop only involves
summing over the rest of the loop, and similarly for pairwise marginals.
This leads us to a general update rule:
38

1. To incorporate an edge h(xa, Xb), first change the root to Xa.
2. Let XL be the set of nodes on the loop created by adding (Xa, Xb).
Z = 
h(xa,xb)q(XL) 
(4.54)
XL
For nodes x, on the loop:
P(Si) 
h(xa, Xb)q(xL) 
(4.55)
XL \xi
For edges (xi, x3 ) on the loop:
Axi, Xj) 
= 
z 
h(xa, xb)q(xL) 
(4-56)
XL\(Xi,xj)
q"e(xiIx3) = 
P(xi, X3)/P(x) 
(4.57)
For a large loop, these sums can be computed efficiently via forward-backward recur-
sions.
3. For edges (xi, xJ) not on the loop:
q"e(xilx,) = q(xilx, ) 
(4.58)
Returning to the example, after processing h(xi, x 4) the pairwise marginal for (x 3, x 4)
under q(x) will be
q(x 3 ,x 4) 
1 3 h(x1,,r 4)h(x,x
2)h(x 2 ,x 3)h(x 3 ,x 4 ) 
(4.59)
X1 ,X2
This is a step forward from section 4.2.1, where only the marginals q(x 3) and q(x 4 ) were
retained between terms. After processing the second edge, h(x 3 , X6 ), the pairwise marginal
will be
q"e(x
3 , X 4 ) 
c< 
q(X 3 , x 4 ) E 
h(x 4, x 5 )h(x 5 , X6 )h(x 3 , X6 ) 
(4.60)
15,X6
= 
(1 
2 h(x,, x4 )h(x1,x 2)h(x 2, X3)h(X3,x
4)
(X1,X2
( E 
h(x 4 , X5 )h(x 5 , X6)h(x 3, X6) 
(4.61)
This is an interesting result: it is the exact marginal for (x 3 , x 4 ) in the original graph!
The same is true for q"'e(x
4, X5) and q"e'(x 5 , X6 ). This is after processing both edges
with ADF. If we use EP to re-process the original edge, then the exact q"ew(x 3 , x 4 ) will be
used to recompute q"ew(x1, x 2 ) and q"e'(X2 , x 3 ), which will also be exact. Thus by moving
from a completely disconnected approximation to a Markov chain, we move from having
approximate marginals to having exact marginals throughout the entire chain. Obviously
this is a special property of the graph and the approximation we have chosen, and we
would not expect it in general. But it demonstrates how effective partially disconnected
approximations can be.
39

4.2.3 
Combining both extensions
The extensions described in the last two sections can be combined in the same algorithm.
Consider the two-term grouping
= 
h(x1, x 2 )h(x 2 , x 3)h(x 3, X4 )h(x1, X4)
t 2(X 3 , x 4 , X5 , X6 ) 
= 
h(x 4,x 5 )h(x 5 ,X6 )h(x 3,X6 )
along with the clustered approximation
q(x) = q(X1, X2)q(X3, X4)q(X5, X6)
as shown in figure 4-6.
* 
tl
* 
t 2
Figure 4-6: We can group edges in addition to using a partially disconnected approximation
Starting from a uniform q(x), the first step of ADF will be
Z 
= 
t (X1, X2, £3, x 4)q(x)
= 
Y 
t 1 (x 1 , X2, £3, X4)q(x1, X2)q(X3, X4)
1i ,2,T3,X4
p3(x)
qe w(X1,X
2 )
qne (x
3 ,x 4 )
new w(£s£ 6 )
I
= 
+1(X1, 
X2, X3, X4 )q(x)
q(£i, Xr2)
= 
t1(X1, X2, X3, X4)q(X3, X4)
X3 7X 4
q(X3, X4 )
_~ ~ 
t(2 
4) 
1(X1, X2, X3, X4)q(x1, X2)
=X ,X2
q= 
5 
6
The second step will be
Z 
= 
t 2(X 3 , £4, £5, £6)q(x)
= 
Z 
t 2 (X 3 , X4, X5, £6)q(X3, X4)q(£ 5 , £6)
X3,X4,E5,X6
40
t1 (Xi, X2, X3, X4)
(4.62)
(4.63)
(4.64)
(4.65)
(4.66)
(4.67)
(4.68)
(4.69)
(4.70)
(4.71)
(4.72)

I
(x) 
= 
t 2 (x 3 , x 4 , x5 , X6)q(x) 
(4.73)
q"'e(xi,x 2) 
q(x 1 , x 2) 
(4.74)
qnew (X3, 4) 
q(x 3, x 4) Z t 2(X3 , X4, X5 , x6 )q(X 5 , X6 ) 
(4.75)
qnew(x 5,x
6 ) 
= 
q(x 5 X6 ) E 
t 2 (x 3 , x 4 , x 5 , x 6)q(x 3 , x 4 ) 
(4.76)
£3 ,X4
Once again, q"ew(x 3 , x 4 ) is the exact pairwise marginal in the original graph. So is
q"W(X 5 , X 6 ), and after one refinement pass q e(x1, x 2 ) will also be exact. In this case, it is
clear why the results are exact: the algorithm is equivalent to running belief propagation on
a clustered graph where the pairs (x 1, x 2), (X3 , x 4 ), and (X 5 , X6 ) are merged into super-nodes.
Interestingly, if we did not group terms, and used individual edges, the algorithm would
be identical to belief propagation on the remaining edges, and we would have gained little
from making the clustered approximation. This is because adding an edge to q does not
create a loop in P. EP approximates each edge by a ratio qfneW/q. If the edge joins two
disconnected clusters in q, then there is no way for EP to represent the correlation induced
by the edge.
4.2.4 
Future work
This section showed that improvements to belief propagation were possible if we grouped
terms, used a partially disconnected approximation, or both. For a general graph, it would
be useful to know what combination of grouping and disconnection gives the highest ac-
curacy for the least cost, i.e. which is most efficient. Determining the cost of a given
grouping/disconnection scheme is straightforward, but estimating the accuracy is not. For
example, as described in the last subsection, it is not a good idea to break the graph into
disconnected subgraphs, unless terms are grouped appropriately. Similar complexities arise
when choosing the right approximating structure for variational methods (Jordan et al.,
1999). This is a useful area for future work.
41

Chapter 5
Classification using the Bayes Point
This chapter applies Expectation Propagation to inference in the Bayes Point Machine.
The Bayes Point Machine is a Bayesian approach to linear classification that competes
with the popular but non-Bayesian Support Vector Machine (SVM). Bayesian averaging
of linear classifiers has been proven both theoretically and empirically optimal in terms of
generalization performance (Watkin, 1993; Bouten et al., 1995; Buhot et al., 1997). But
an efficient algorithm has remained elusive. The Bayes Point Machine approximates the
Bayesian average by choosing one 'average' classifier, the Bayes Point (Watkin, 1993; Rujan,
1997; Herbrich et al., 1999). Computing the Bayes Point is a simpler, but still very difficult
task.
This chapter shows that Expectation Propagation, using a full-covariance Gaussian ap-
proximation to the posterior, provides an accurate estimate of the Bayes Point-an ap-
proximation to the full Bayesian average. The algorithm turns out to be identical to what
Opper & Winther (2000c) derived by statistical physics methods. However, EP uses a dif-
ferent optimization scheme that is faster and does not require a stepsize parameter. EP
also provides an estimate of the evidence p(D), which is useful for feature selection but not
provided by Opper & Winther.
5.1 
The Bayes Point Machine
The Bayes Point Machine (BPM) is a Bayesian approach to linear classification. A linear
classifier classifies a point x according to y = sign(w Tx) for some parameter vector w (the
two classes are y 
±1). Given a training set D = {(x1, yi), ... , (x,, y,)}, the likelihood for
w can be written
p(Djw) 
JJ p(y;|xZ, w) = l 0(yZwTxZ) 
(5.1)
0(z) 
= 
0 
if z < 0 
(5.2)
This is a slight abuse of notation since p(Djw) is only a distribution for y, not x. This
likelihood is 1 if w is a perfect separator and 0 otherwise. A simple way to achieve linear
separation is to progressively enlarge the set of features, e.g. by computing squares and third
powers, until the data is separable in the new feature space. This has the effect of producing
a nonlinear decision boundary in the original measurement space. Feature expansion can
be implemented efficiently using the 'kernel trick', where we rewrite the algorithm in terms
42

* 
4 
..
x1 
x1
(a) 
(b)
Figure 5-1: Two different classification data sets, with label information removed. The
Bayes Point Machine assumes that, without labels, we cannot infer where the boundary
lies. In (b), one might be inclined to think that the horizontal axis determines the decision.
But the horizontal axis could easily be something irrelevant, like the time of day the datum
was collected. Only domain knowledge can determine the validity of the assumption.
of inner products and perform the feature expansion implicitly via the inner product. This
technique will be described more fully later.
Because it is conditional on xi, this model assumes that the distribution of the x's is
unrelated to the true decision boundary. That is, if we were given the x's without any
labels, we would have no information about where the boundary lies. The validity of this
assumption must come from domain knowledge-it cannot be determined from the data
itself (see figure 5-1). The Support Vector Machine (SVM), for example, does not seem to
make this assumption; Tong k Koller (2000) show that the SVM follows from assuming
that the x's follow a Gaussian mixture distribution in each class. In addition, some workers
have used the SVM with unlabeled data (Bennett & Demiriz, 1998), while the BPM, by
design, cannot. The advantage of assuming independence in the BPM is that we avoid
stronger assumptions about the nature of the dependence, and thus obtain a more general
algorithm. Experiments show it is quite robust (section 5.6).
A refinement to the basic model is to admit the possibility of errors in labeling or
measurement. A labeling error rate of c can be modeled using (Opper & Winther, 2000b)
p(ylx,w) 
= 
c(1 - O(ywTx)) + (1 - C)O(ywTx) 
(5.3)
C + (1 - 2c)O(ywTx) 
(5.4)
Under this model, the likelihood p(Dlw) for a given w will be cr(l - e)nr, where r is the
number of errors on the training set. Only the number of errors is important, not where
they are. This way of modeling errors can automatically reject outliers, because it doesn't
care how far an error is from the boundary. Other approaches based on adding 'slack'
to the boundary only allow errors near the boundary and are sensitive to outliers. These
approaches can be simulated by using a smooth likelihood function such as
p(ylx,w) 
= 
0i(ywTx) 
(5.5)
where 0 is the cumulative distribution function for a Gaussian. The tails of this function
fall like exp(-x
2 ), so it provides 'quadratic slack'. 
The logistic function could also be
43

used; it falls like exp(-x), providing 'linear slack'. By changing 0 to one of these smooth
functions, the labeling error model (5.3) can be combined with slack, with no computational
difficulties. However, this chapter focuses on (5.3) without slack.
To complete our Bayesian model, we need to specify a prior on w. Since the magnitude
of w is irrelevant for classification, some authors have restricted w to the unit sphere in
d dimensions, and have given w a uniform distribution on the sphere (Rujan, 1997). This
distribution is nice because it is fair to all decision boundaries, but it is not very convenient
to work with. A more general non-informative prior is an average over spheres. Let r = ||w|i
be the magnitude of w. Given r, let w be uniformly distributed on the sphere of radius
r. This prior is non-informative for any r. It is also non-informative if r is unknown with
distribution p(r). For example, p(r) could be uniform from 0 to 1, allowing w to live in the
unit ball instead of on the unit sphere. By choosing p(r) appropriately, we can also give w
a spherical Gaussian distribution:
p(w) ~ A(0, I) 
(5.6)
It is a property of the spherical Gaussian distribution that, conditional on any r, w is
uniformly distributed on the sphere of radius r, so no particular decision boundary is favored
by this prior.
Given this model, the optimal way to classify a new data point x is to use the predictive
distribution for y given the training data:
p(yIx, D) = J p(yIx, w)p(w ID)dw 
(5.7)
However, this requires solving an integral each time. A practical substitute is to use a single
value of w which approximates the predictive distribution as well as possible. Watkin (1993)
and Rujan (1997) have defined the 'Bayes point' as the single best w. However, this point
is difficult to find. Thus, following Rujan (1997), we use the posterior mean for w, E[wiD].
Following convention, we will be sloppy and also call this the 'Bayes point'. Our strategy
will be to make a Gaussian approximation to the posterior and use its mean as the estimated
Bayes point. Of course, if we really wanted to, we could also use EP to solve the integral
(5.7) for each test point. The normalizing constant of the posterior is also useful because,
as an estimate of p(D), it allows us to choose among different models (see section 5.7).
5.2 
Training via ADF
Start with the ADF updates. ADF was previously applied to the Bayes Point Machine
by Csato et al. (1999). Divide the joint distribution p(D, w) into n terms, one for each
data point. Since y and x always appear multiplied together, the formulas will drop y and
assume that xi is already scaled by yi. Let the approximate posterior have the form
q(w) ~N(m., V.) 
(5.8)
From minimizing the KL-divergence we get the expectation constraints
Eqnew[w] 
= 
E1 [w] 
(5.9)
Eqnew[wwT] 
= 
E [wwT] 
(5.10)
44

To compute these expectations, use the following relations (obtained from integration-by-
parts):
Z(mw, Vw)
Ep [w]
Ep[wwT] - Ep[w]Ep[w]T
= 
i(w)q(w) dw
= mW + VwVm log Z(m., Vw)
-
V_ - Vw(VmVm - 2Vv log Z(mW, V ))V
These hold for any t(w). For the Bayes Point Machine, we have (combining yx into x):
t(w)
O(z)
= 
c + (1 - 2c)O(wT x)
= LZI(z; 0, 1)dz
Z 
=x
V/xTVwx
Z(mw, Vw)
a
Vm log Z(mW, Vw)
VV log Z(m., Vw)
VmVm - 2V log Z(mw, V,)
= 
E+ (1 - 2c)O(z)
1 
(1 - 2c)F(z; 0, 1)
v/xTVwx 
e + (1 - 2c)#(z)
= 
ax
-
1 amI x
2 xTV~x
T
a2 XXT + 
mV WX 
T
= axxT+ 
xx
a(mw + Vwax) Tx 
T
xx
-xTVwx
The ADF algorithm is thus:
1. Initialize mw = 0, V_ = I (the prior). Initialize s = 1 (the scale factor).
2. For each data point xi, update (mw, VW, s) according to
= 
W + Vwaixi
=VW 
-
(VwLxi)(
aITM new
2xT 
) (Vwx.)T
x TVW.
(5.23)
(5.24)
(5.25)
The final mw is the estimate of w used for classification. Note that this algorithm does not
require matrix inversion. It is the same algorithm obtained by Csato et al. (1999), except
they expressed it in terms of kernel inner products, which makes it more complex.
5.3 
Training via EP
Now we turn the ADF algorithm into an EP algorithm. By dividing two consecutive Gaus-
sian posteriors, we find that the term approximations are also Gaussian, parameterized by
m, and Vi:
q new (w)
q(w)
(5.26)
45
(5.11)
(5.12)
(5.13)
(5.14)
(5.15)
(5.16)
(5.17)
(5.18)
(5.19)
(5.20)
(5.21)
(5.22)
mW
Vnew
w
8 new 
=s 
xZj(mwVw)
1(w)

m V+ 
V(W; m, VO) 
(5.27)
'V(mi; m ..; Vi + Vw)
where V- 1  
= 
(V"'e)-l -V- 
(5.28)
= Vi(Vnew)~lmMew - v;v;~ 
(5.29)
mW + (Vi + Vw)V-1(mwew -
mw) 
(5.30)
To simplify this, combine it with the ADF updates (5.23,5.24) to get (mi, Vi) directly from
(Mrn, 
V'):
aix T 
new 
-
Vi 
= 
V 
xx. 
- Vw 
(5.31)
m 
= 
m + (Vi + Vw)ajxt 
(5.32)
The matrix O'4TrnW xZxT has one nonzero eigenvalue, which means V- will have one finite
eigenvalue, in the direction of x-. This makes sense because term i only constrains the
projection of w along xi. This special structure allows us to represent Vi with a scalar, vi:
V. 1  
= -
-1 ixT 
(5.33)
x[V.x. 
= 
v. 
(5.34)
From (5.31), we know that
xTVzx, 
= 
- xT Vwx. 
(5.35)
I 
axTmnew
t mw
which means the update for vi is
V= 
xVx 
-
) 
(5.36)
Another consequence of this special structure is that instead of the full vector mi, we only
need the projection mTxi, which can be stored as a scalar mi.
46

Now we can write an efficient EP algorithm:
1. 
Except for the prior, the term approximations 
Initialize them to 1:
have the form
1Vi 
= 
o(5.38)
i (w) = si exp(-y(w'xi - m2)2) 
(5.37) 
m; 
= 
0 (5.39)
Si 
= 
1 (5.40)
2. m new = 0, Vew =I (the prior)
3. Until all (mi, vi, st) converge (changes are less than 10-4):
loop i = 1, ... , n:
(a) Remove ii from the posterior to get an 'old' posterior:
Vw 
= 
((VWew)- 
- v' xixT) 1  
(5.41)
= vew + (V ewX)(v, -- XTVneX)l(Vew X ) T  
(5.42)
MW 
= 
mmW + V1 V 
m ,e 
-
mZ) 
(5.43)
= 
m new + (Vwxi)v. l(xTm new - mj) 
(5.44)
These expressions involving Vw can be computed efficiently:
vi
Vx, 
= (V 
x)- 
XTVwX 
(5.45)
XT~~iI 
-
x1)_1x
x 
(1x) 
=i(5.46)
xTVnmewx; 
Vi
(b) Recompute (mnew, Vnew, Zi) from (mw,Vw), as in ADF.
(c) 
Update it:
V, 
= 
xTVT 
x( 
-
(5.47)
m 
= 
xTmW + (v. + xTVwxi)ai 
(5.48)
= 
Z 
V 
exp( (m - mw)T(VI + Vw)
1(mi -
mw))(5.49)
= 
Zi 
1+ v- 
xTVxexp( 
1 
eWai) 
(5.50)
4. Compute the normalizing constant:
B 
= 
(mnew)T (Vew)~ 
new) -
(5.51)
vi
p(D) 
~ 
VWe|/ 
exp(B/2) 
si 
(5.52)
i=1
This algorithm processes each data point in O(d 2 ) time. Assuming the number of iterations
is constant, which seems to be true in practice, computing the Bayes point therefore takes
O(nd2 ) time. Computing the normalizing constant at the end requires O(d 3 ) time.
47

5.4 
EP with kernels
To use the kernel trick, we need to rewrite EP in terms of inner products. Following the
notation of Opper & Winther (2000c), define
A- 
= xTVVx, 
(V\ is the V, when term i is left out) 
(5.53)
Ci. 
= xfxj 
(5.54)
A 
= diag(vi, ... , v,) 
(5.55)
h 
= 
xTmeW 
(5.56)
hV= 
xTm\ 
(5.57)
From the definition of qnew(w) as the product of approximate terms it(w), we know that
ve 
+ 
(I+ XA-XT 
(5.58)
m new 
Vnew 
m x 
(5.59)
3 
V
xTmnew 
= 
(XT Vnew Xm 
(5.60)
i 
vi
From the matrix inversion lemma we therefore have
(C + A)- 
= 
A- 1 - A-' XTVneZXA-1 
(5.61)
XTVnewX 
= 
A -
A(C + A)-A 
(5.62)
= (C- 1 + A-)-
1  
(5.63)
In this way, we can compute xTVnewxj for any (i, j) using only C and A. The EP algorithm
becomes:
1. Initialize vi = oomi = 0,si = 1.
2. Initialize hi = 0, Ai = Cit.
3. Until all (mi, vi, si) converge:
loop i = 1, ... , n:
(a) Remove i from the posterior to get an 'old' posterior:
0Z 
h + Aiv7 1(h- - mi) 
(5.64)
(b) Recompute (part of) the new posterior, as in ADF:
z 
= 
(5.65)
Z 
= 
c + (1 - 2c)#(z) 
(5.66)
1 
(1 - 2c)V(z; 0, 1) 
(5.67)
-
h~±~(5.68)
V1A 
c + (I - 2c)#(z)
hi 
= 
0' 
+ A-ga- 
(5.68)
48

(c) Set
Vi 
= 
A 
-
(5.69)
mi 
= 
h 
+ (v- + Ai)a = hi + viaj 
(5.70)
Si 
= 
Zi 
1 + v-i A, exp(Z) 
(5.71)
(d) Now that A is updated, finish recomputing the new posterior:
A 
= 
(C- 1 + A- 1)~1  
(5.72)
For all i:
hT 
= 
Ajg m- 
(from (5.60)) 
(5.73)
1 
1 
-
A- 
(from (5.46)) 
(5.74)
(Ai 
vi
4. Compute the normalizing constant:
B 
= 
Am 
(5.75)
ti 
V 
vi
IA|11/2
p(D) 
~ 
1/2 exp(B/2) 
s 
(5.76)
IC + Al 
i1
The determinant expression in (5.76) follows from (5.58):
IVewI 
= 
I + XA-XT 
= I+ XTXA-1 
(5.77)
= 
I+ CA-' 
C JAI 
(5.78)
1 
|C + A l
In step (5.72), it would appear that we need to invert a matrix, taking 0(n') time, for each
data point. However, since only one vi changes each time, A can be updated incrementally
in O(n 2) time. Initialize A = C and then update with
Anew 
= 
(C-1 + A- 
+ AA1) 
(5.79)
ajT
= 
A- 
2  
(5.80)
6 + aii
where 5 
= 
(jew 
-
1d) 
(5.81)
Assuming a constant number of iterations, the algorithm thus runs in 0(n') 
time, plus the
time to compute the inner product matrix C. This is a great improvement over O(nd2 ) if
the dimensionality is very large, e.g. if the feature set is artificially expanded.
To show the equivalence with Opper & Winther's algorithm, we make the following
observations:
49

* At convergence, (5.70) will hold for all i, so that
Z jxh 
x 
+ E a 
> 
(5.82)
>Zyj 
axj 
(5.84)
Vj 
V31
w3 
j
h. = 
-(x x ) = 
Ci-a -
(5.85)
This is the equation Opper & Winther use instead of (5.73).
* The update (5.74) for Ai can be written
A. 
I1 
1 )-
-
(5.86)
[i-o 
(C + A)-']Zt 
vi
-
(v* -
V? (C + A11 )(v- [(C + A)-])-1 
(5.87)
-Vi 
(5.88)
[(C + A)-']
This is the update Opper & Winther use instead of (5.74).
9 EP computes a normalizing constant for the posterior, while Opper & Winther do
not.
* All other updates are identical to those in Opper & Winther (2000c), though carried
out in a different order. Reordering the updates does not matter as long as both
algorithms converge.
The input to the algorithm is simply the matrix C, which can be computed using an
arbitrary inner product function C(xi, x-) instead of xfxj. However, in that case we need
to keep yi and xi separate:
CiJ = yiyjC(xi,x 3 ) 
(5.89)
Using a nonlinear inner product function is an efficient way to use an expanded feature space,
since we don't have to represent each expanded data point. We only have to compute the
inner product between two expanded data points, which is a scalar function of the original
data.
The output of the algorithm is the aj, which implicitly represent mn,, through (5.84).
With these we classify a new data point according to
f(x) = sign(xT m"') = sign( 
a y C(xx)) 
(5.90)
5.5 
Results on synthetic data
Figure 5-2(a) demonstrates the Bayes point classifier vs. the SVM classifier on 3 training
points. Besides the two dimensions shown here, each point had a third dimension set at
50

1. This provides a 'bias' coefficient w 3 so that the decision boundary doesn't have to pass
through (0, 0). Each classifier is set to zero label noise (c = 0 and zero slack). The Bayes
point classifier approximates a vote between all linear separators, ranging from an angle of
0' to 1350. The Bayes point chooses an angle in the middle of this range. SVM chooses the
decision boundary to maximize 'margin', the distance to the nearest data point. This makes
it ignore the topmost data point. In fact, the Bayes point and SVM would coincide if that
point were removed. Adding the point reduces the set of linear separators and Bayesian
inference must take this into account.
Figure 5-3 plots the situation in parameter space. For viewing convenience, we restrict w
to the unit sphere. The exact Bayes point was computed by likelihood-weighted sampling,
i.e. sampling a random w on the sphere and discarding those which are not separators.
EP provides an accurate estimate of the posterior mean m, and posterior covariance V,.
Compare:
0.366 
0.0551 
-0.1251 
[0.358 
0.0589 
-0.118 1
Exact V, = 
0.0551 
0.533 
-0.073 
EP VW = 
0.0589 
0.542 
-0.0809
--0.125 
-0.073 
0.164 .
L-0.118 
-0.0809 
0.163 .
Figure 5-2(b) plots cost vs. error for EP versus three other algorithms for estimating the
Bayes point: the billiard algorithm of Herbrich et al. (1999), the TAP algorithm of Opper
& Winther (2000c), and the mean-field (MF) algorithm of Opper & Winther (2000c). The
error is measured by Euclidean distance to the exact solution found by importance sampling.
The error in using the SVM solution is also plotted for reference. Its unusually long running
time is due to Matlab's quadprog solver. TAP and MF were slower to converge than EP,
even with a large initial step size of 0.5. As expected, EP and TAP converge to the same
solution.
The billiard algorithm is a Monte Carlo method that bounces a ball inside the region
defined by hyperplane constraints. It must be initialized with a linear separator and the
SVM was used for that purpose. 
Since there are many other ways one could initialize
the billiard, which may be much cheaper, the initialization step was not counted against
the billiard's FLOP count (otherwise the billiard curve would be right of the SVM point).
The error axis must also be interpreted carefully. While it is tempting to use the lower
envelope of the curve as the measure of error, it is actually more accurate to use the upper
envelope, since this is all that the algorithm can consistently achieve as samples accumulate.
Nevertheless it is clear that EP is much faster and more accurate.
Laplace's method and variational Bayes do not work at all on this problem. Laplace
fails because the derivatives of the likelihood are not informative: they are either zero or
infinity. Variational Bayes fails for c = 0 because no Gaussian can lower bound the likelihood
(Gaussians never reach zero). Even with e > 0, a Gaussian bound must be quite narrow so
we can't expect competitive performance.
Herbrich & Graepel (2000) point out that the SVM is sensitive to scaling an input
vector, i.e. if we replace x 1 by 2x 1 then the solution will change. They argue theoretically
and empirically that data vectors should therefore be normalized to unit length before SVM
training. This is quite a shock since no mention of this was made in the original SVM papers.
If the data in figure 5-2, including the bias dimension, is normalized before training, then
the SVM solution tilts slightly toward the BPM solution, supporting Herbrich & Graepel's
suggestion. The Bayes Point Machine, by contrast, does not require any normalization,
because the data likelihood (5.1) is correctly invariant to scaling an input vector. The
solution found by EP is also invariant.
51

SVM
Bayes
0
oX
103 
104
FLOPS
105 
10
Figure 5-2: (left) Bayes point machine vs. Support Vector Machine on a simple data set.
The Bayes point more closely approximates a vote between all linear separators of the data.
(right) Cost vs. error in estimating the posterior mean. ADF is the first 'x' on the EP
curve.
es-
Figure 5-3: (left) The same problem viewed in parameter space (the unit sphere). Each
data point imposes a linear constraint, giving an odd-shaped region on the sphere. The
Bayes point is the centroid of the region. The EP estimate is indistinguishable from the
exact Bayes point on this plot. (right) The Gaussian posterior approximation obtained by
EP, rendered as a one standard deviation isoprobability ellipsoid.
52
3 
4
10 0
10
2w0-
SVM
Billiard
\ 
F
EP 
TAP
1
3
0~
1041
102

0
Billiard
EP 
TAP
102
102 
103 
104 
105 
106
FLOPS
Figure 5-4: Cost vs. error for the 3-point problem with one point replicated 10 times
The linear classification problem is special in that some points can have large influence
on the solution while adjacent points have zero influence. This is because the constraints
imposed by the latter points may be redundant given the constraints already imposed by
other points. This property allows the support vector machine to focus on a small number
of points, the support vectors, when finding its solution. Unfortunately, this property can
cause havoc with EP. EP uses Gaussians to smooth out the hard constraints and cannot
always recognize when a point is redundant and should be ignored. 
The result is that
clusters of points can degrade the EP estimate. Figure 5-4 shows what happens to figure 5-
2(b) when the point at (1, 0) is replicated 10 times. As expected, the cluster of points counts
more than it should, making the EP boundary move slightly away from it. The accuracy of
EP, TAP, and MF degrades significantly, while SVM and Billiard are unchanged. On the
bright side, the error cannot be made arbitrarily high-using more than 10 replications, or
replicating the other points, does not degrade the accuracy any further. One work-around
for this behavior would be to reduce the dataset in advance, e.g. to just the support vectors.
This idea has not been tested yet.
Figure 5-5 compares the EP solution to the exact posterior mean when noisy labels
are allowed (E = 0.1). Theoretically, this integral is harder since we have to consider the
entire parameter space, not just perfect separators. EP can handle it with no additional
cost. The solution comes out vertical because it is a compromise between the small set
of perfect separators at 135' and the larger set of boundaries between 0' and 1350 which
have one error. (There is also a set of boundaries past 135' which also have one error.) On
this dataset, a similar result can be achieved with the SVM if we set the slack parameter
C = 2, though this would not necessarily be true on another dataset. Winther (1998) has
given extensive evaluations of EP with c = 0 and c > 0 on synthetic datasets, showing that
it achieves the theoretical performance of Bayesian averaging. Those experiments are not
repeated here.
Figure 5-6 demonstrates EP with a nonlinear kernel, contrasting it to the SVM solution
53
SVM
MF
100

2
1.5- 
x
1- 
0
0.5- 
0
0 
0 
x 
x
-0.5
-1
-1 
-0.5 
0 
0.5 
1 
1.5 
2
Figure 5-5: The EP solution has good agreement with the exact posterior mean when noisy
labels are allowed (c = 0.1)
with the same kernel. The kernel was Gaussian:
1
C(x.,x-) = exp(- 
(xi -
x )T(x, -
x)) 
(5.91)
with width parameter a- 
0.2 and a = 0.5. The feature expansion equivalent to this kernel
has an infinite number of features. The SVM is quite sensitive to the choice of a as well
as the idiosyncrasies of the dataset. This is related to the sparsity of the SVM solution:
when the boundary depends on only a few data points, i.e. a few Gaussian kernels, small
changes in those kernels have a larger effect on the boundary. Similar results obtain if we
use a polynomial kernel:
C(xj, xJ) = (xTxj + 1)P 
(5.92)
For large p, the boundaries are almost identical to those for a wide Gaussian kernel.
54
EP 
Exact

0~ 
0
0 
0
o 
0
0
o 
0
0
-
- BPM
-
SVM
x
x 
x
x
x X
x
x
narrow Gaussian kernel
BPM
-
SVM
0 
x
0 
x
o 
Qui
x
0
x
wide Gaussian kernel
Figure 5-6: Classification boundaries resulting from Support Vector Machine training vs.
Bayes Point Machine training with EP. Both used a Gaussian kernel with the same width
parameter. (left) Narrow width. The SVM tends to put extra bumps and kinks in the
boundary. (right) Wider width. The SVM chooses an unusual solution in order to maximize
margin in the expanded space. The BPM boundary is non-sparse (all ai > 0) but smoother.
It is hardly affected by changing the kernel width. Billiard gives results similar to EP.
55

5.6 
Results on real data
For higher dimensional problems, it is more difficult to compute the exact Bayes point and
make cost vs. accuracy comparisons. So instead we'll measure cost vs. test performance
on benchmark problems.
Figure 5-7 compares EP, Billiard, and SVM on discriminating handwritten digit '3'
from '5'. Each data point xi is an 8 x 8 binary image (64 dimensions). The dataset was
randomly split 40 times into a (small) training set of 70 points and a test set of 430 points.
All algorithms were linear and set for zero noise. Billiard was run for 2000 iterations, which
is far more computation than EP or SVM used. Nevertheless, EP is best.
Figures 5-8 and 5-9 show the same type of comparison on four datasets from the UCI
repository (Blake & Merz, 1998). Each dataset was randomly split 40 times into a training
set and test set, in the ratio 60%:40%. In each trial, the features were normalized to have
zero mean and unit variance in the training set. The classifiers used zero label noise and a
Gaussian kernel with o- = 3. Billiard was run for 500 iterations. The thyroid dataset was
made into a binary classification problem by merging the different classes into normal vs.
abnormal. Except for sonar, EP and Billiard beat the SVM a majority of the time, and
in all cases EP performed similarly to Billiard. However, the running time for Billiard is
significantly higher, since it must be initialized at the SVM solution. Figure 5-10 shows cost
vs. test error curves on some typical runs. The SVM using quadprog is particularly slow
on thyroid. The unusual success of the SVM on sonar was also reported by Herbrich et al.
(1999), and may be related to the different assumptions made by the SVM (see section 5.1).
To give an idea of the actual running time in Matlab, figure 5-11 plots the training time
vs. training set size for 40 trials. As expected, the time scales as n 3. Extrapolating from
this, it would take an hour for 1,000 data points and 6 weeks for 10,000 data points. Finding
a way to speed up EP, similar to the way that quadratic programming can be sped up for
the SVM, will be essential for large data sets.
56

70 
70
65- 
65-
60- 
60-
55- 
55-
w 
w
50- 
50-
45- 
45-
40- 
40-
35 
35
30 
40 
50 
60 
70 
80 
30 
40 
50 
60 
70
SVM 
Billiard
Figure 5-7: Test error of SVM vs. EP (left) and Billiard vs. EP (right) on digit classification,
over 40 random train/test splits. Each point is (test error for SVM/Billiard, test error for
EP). Points under the line correspond to trials where EP had a lower test error. EP beat
SVM 34 times and beat Billiard 28 times, despite being the fastest of all three algorithms
of this dataset.
57
Test errors
Test errors

Heart: 13 features, 162 training points
Test errors
0.2 
0.25
SVM
0.32
0.31
0.28
0.26
0.24
0- 
0.22
0.2
0.18
0.16
0.14
0.12
0.3
0.15
0.2 
0.25
Billiard
Thyroid: 5 features, 129 training points
Test errors
0 
0.05
SVM
0.15
0.1
L
U 
00
0
0.1 
0.15 
-0.
0 
0.05
Billiard
0.1 
0.15
Figure 5-8: 
Test error rates over 40 train/test splits.
SVM/Billiard, test error for EP). Points under the line
had a lower test error. For running times, see figure 5-10.
Each point is (test error for
correspond to trials where EP
58
Test errors
0.28
0.26
0.24
Ct
W 0.22
0.2
0.18
0.16
0.14
0.15
0.3
Test errors
0.15
0.1
L
0.05
0
-0.05
0.32
0.3
05
)5

Ionosphere: 34 features, 211 training points
Test errors
0.05 
0.1 
0.15 
0.2
SVM
Sonar: 61 feature
Test errors
0.05
0.1 
0.15
SVM
0.2
0.2
0.18
0.16
0.14
L 
0.12
0.11
0.05
0.1
Billiard
0.15
s, 124 training points
Test errors
0.22
0.2-
0.18-
0.16
o. 0.14
0.12[
0.1
0.08
0.06
0.05
0.1 
0.15
Billiard
Figure 5-9: 
Test error rates over 40 train/test splits. 
Each point is (test error for
SVM/Billiard, test error for EP) for one trial. Points under the line correspond to tri-
als where EP had a lower test error. For running times, see figure 5-10.
59
0.22
0.2
0.18
0.16
0.14
0.12
Test errors
0wi
0.1
0.08
0.06
0.04
0.22
0.2
0.2
0.18
0.16
0.14
0.12
C-
wU
0.11
0.08
0.06
0.04
0.2
0.08
0.06
0.04

Heart
Billiard
SVM
EP
0 7 
108 
1
FLOPS
Ionosphere
109
Thyroid
0
(D
0.2 -
0.18-
0.16-
0.14-
0.12-
0.1 -
0.08-
0.06-
0.04-
0.02
0.3
0.29
0.28
0.27
0.26
0.25
0.24
0.23
0.22
0.21
1
0
a)
Co
a)I-
10 10
0.28-
0.26-
0.24-
0.22-
0.2-
0.18
0.16
0.14 -
106
0 
108 
109 
10
FLOPS
Sonar
10
FLOPS
Figure 5-10: Cost vs. test error on a typical
FLOPS
trial, for each dataset
60
0
14
A2 
')
SVM
EP 
Billiard
10
0.22-
0.2-
0.18-
0.16-
0.14
0.12
0.1
0.08 -
10 7
SVM
0
Billiard
EP
Billiard
SVM 
EP
0
0 9
03
10 8
10 7
10

200 
400
Training set size
600 
800
Figure 5-11: Actual training time
sizes. It nearly perfectly follows n3
is not included.
in Matlab for EP with kernels, for 40 different dataset
growth. The time to compute the inner product matrix
61
25-
20-
E15
CD
E
0)
C
'E 10
F-
5
0L0

5.7 
Model selection
This section reports results on Bayesian model selection using the estimated evidence p(D).
This is a unique feature of EP because the billiard algorithm cannot provide an estimate
of p(D). By maximizing the evidence, we can select the appropriate feature set, feature
kernel, or noise parameter c. 
For background on Bayesian model selection, see MacKay
(1995); Kass & Raftery (1993); Minka (2000a). A popular approach to model selection for
the SVM is to minimize 
, where R is the radius of the smallest ball containing the training
set (measured in the mapped feature space) and M is the margin (Cristianini et al., 1998).
Another approach is to minimize the span bound given by thresholding the a's (Chapelle
& Vapnik, 1999; Chapelle et al., 2000):
S = 
O(a, - [C-1]) 
(5.93)
support vectors 2
These three criteria, evidence, margin, and span, can lead to quite different results.
Figure 5-12 shows a synthetic training set classified according to distance from the origin,
i.e. the true decision boundary is a circle. Three different decision boundaries result from
training an SVM with three different kernels (the decision boundaries from EP are nearly
identical). All of these boundaries achieve zero error so we need to invoke some other rule
besides training error to select among them. Over all Gaussian kernels, the margin criterion
is minimized by taking a = 0.1. This produces a decision boundary with spurious kinks
and bumps. The span bound is minimized by an even narrower kernel: a = 0.011, where
the bound is 0. By contrast, the best Gaussian kernel according to evidence has a = 0.9,
providing a much smoother and more realistic boundary. The quadratic kernel, which is
the closest to the true boundary, has even greater evidence yet less margin.
In the noise-free case, the Bayesian evidence p(D) has a natural interpretation. It is
the fraction of perfect separators out of all representable classifiers. If a particular set of
features has a large percentage of perfect separators, then we intuitively should have more
faith in that feature set. This is what Bayesian model selection does, and in choosing the
Bayes point we follow the same basic argument. Maximizing the margin does not necessarily
behave this way.
The next example repeats the feature selection experiment of Weston et al. (2000);
Chapelle et al. (2000). 
Synthetic data with six features was created by first sampling
y = ±1 with equal probability and then setting
X1 
= A (y, 1) 
(5.94)
X2 
= 
K(2y, 1) 
(5.95)
X3 
= 
Ar(3y, 1) 
(5.96)
X4 
= 
K(0,1) 
(5.97)
X 
=5(0,1) 
(5.98)
X6 
.= (0,1) 
(5.99)
With probability 0.3, features Xi-x
3 were swapped with features x4-X6 . Thus all six features
are relevant to some degree. 
To these six relevant features are appended 14 irrelevant
features with distribution V(0, 20). The test is to see whether we can reject the 14 irrelevant
features. In Chapelle et al. (2000), the algorithm was forced to keep only two features. Here
we use a more rigorous test: features are added one by one, starting with the relevant ones,
62

x 
x
xx 
\0
7 
0
x
./060 
0 
C
00
x0 
00
& 
0 0
0 
0
0o 0
x 00
'0 
0
x
x
x
x
x 
x
x
x
x
0
x
x
x
X 
X
0- 0
) i
xx
K
K
x
x 
~
0 
x
x 
/
x 
000 
0
x 
xx/O 
0 
0 
0
0 
0 
O\
6' o~ 
oo
x 
x
0 0 
/ 
x
0i 
00
xx 6
xx
x 
0 0
V 
00
x 
~ 
~ \ 
xx
X,
x
x x
x
Gaussian o, = 0.1
x
x
x 
x
x
x x
x
x
Gaussian a = 0.9
x
x
x
7 
0"' 
1
x
00 
0 
0 
0
x 
x 
0 
0 
0
0 
0
& 
0000
00
.0 
0
x 1 00
,0 00
S000
x
x
x
\X x
0
x
x
x
x
,1
x 
0
x 
x 
x
x
x x
x
Kernel
a = 0.1
r = 0.9
x 
quadratic
13
284
1300
span
5
5
6
log(p(D))
-40
-18
-17
x
x
x
Quadratic
Figure 5-12: A dataset classified with three different kernels. The margin criterion R 2 /M
2
prefers the a = 0.1 solution, while the Bayesian evidence p(D) prefers the quadratic solution.
The span bound prefers a Gaussian with tiny or = 0.011 (not shown). The true boundary
is a circle.
63
x
x
x
x
,
,
,

35 
7
30-
25-
20-
15-
10' 
'5
0 
5
10
Number of features
6-
55-
4
.0
cz 3
2
1-
0
15 
20 
0 
5
10
Number of features
15 
20
-7
-8
-9
-10
-11
-12
-13
-14
-15
0
5
10
Number of features
15 
20
Figure 5-13: Feature selection curves for the margin bound, span bound, and Bayesian
evidence. The Bayesian evidence has a clear peak at six, the correct answer. The other
criteria, which are to be minimized, do not perform as well. The margin bound has only a
local minimum at six, and the span bound only a local minimum at five.
and the algorithm must decide when to stop. Figure 5-13 shows the curves for the margin
bound, span bound, and Bayesian evidence. Both the SVM and BPM used a linear kernel
with zero slack. The Bayesian evidence is clearly a better measure of relevance.
64
0
C
C>
a,
0
7
35

Figure 5-14: (a) Conventional billiard algorithms allow the ball to escape. They project
the collision point onto the surface of the sphere, but preserve the rebound velocity (dashed
line). 
(b) The new algorithm works inside the sphere, using the sphere's surface as an
additional wall.
5.8 
A better billiard algorithm
A major drawback of current billiard algorithms is that they do not really operate on
the sphere. The billiard ball always follows a straight line trajectory until it hits a wall,
at which point it is projected back onto the sphere (figure 5-14(a)). Unfortunately, this
means the ball can sometimes leave the sphere without ever hitting a wall. On the 3-point
problem in figure 5-3, this happens on 50% of the bounces. Rujan (1997) recovers from this
by restarting the billiard at its initial point, with a random velocity. Unfortunately, this
introduces severe bias in the estimate, by overcounting the initial point. Herbrich et al.
(1999) have a better solution: leave the ball where it is, but choose a new random velocity.
This has less bias, but doesn't resemble a billiard anymore.
The problem of escaping billiard balls can be avoided entirely if we simply redefine
the pool. Remember that the unit sphere was an arbitrary choice; we can use any prior
distribution that assigns equal probability to all w vectors of a given length. So let's use the
inside of the unit sphere, as shown in figure 5-14(b). The sphere's surface is an additional
wall that prevents escape. The new algorithm is:
1. Determine the next collision. From position w and velocity v, compute the flight time
to each wall and take the minimum. The distance from wall xi is
di= wTX 
(5.100)
xTx.
and the velocity normal to xi is
Vi= 
Tx 
(5.101)
so the flight time is
( 
L, if vi
r"i = 
V. 
(5.102)
x0 
otherwise
The flight time to the surface of the sphere satisfies
(w + vr)T(w + vr) = 
1 
(5.103)
65

7 
V(wTV)2 + vTv(1 -
wTw) -
WTV
T 
-=T 
(5.104)
VTV
which is always positive and finite.
2. Update the position of the ball, the total distance traveled (s), and the center of mass
estimate (M).
Wew 
= 
W + VTmin 
(5.105)
ZflEneWW1 
(5.105)
z= 
||""- 
w|l 
(5.106)
Mnew 
-
ZM+ 
Z 
w 
+ W 
(5.107)
ms 
z 
s~z 
2
snew= 
s + z 
(5.108)
3. Update the velocity. If xi was hit,
V new = v - 2vi 
xt'(5.109)
xxi
If the surface of the sphere was hit,
Vnew 
= V -
2wnew(Wnew)TV 
(5.110)
The length of v is unchanged by these updates, so if we initialize vTv = 1, it will stay
that way throughout the algorithm.
The algorithm can also be made use a kernel inner product, as in Herbrich et al. (1999).
With this algorithm, we can run one billiard trajectory, without ever restarting. However,
that is not a good idea, because the pool is not perfectly ergodic. By periodically randomiz-
ing the velocity of the ball, we can achieve a more ergodic sampling. Periodic randomization
is important in all Markov chain Monte Carlo schemes, including Gibbs sampling. For the
billiard, randomization at every 100 bounces seems to work well.
Figure 5-15 compares the three algorithms on the 3-point problem of section 5.5. Each
was run for 100,000 iterations, using a comparable number of flops. Because of its bias,
Rujan's algorithm is never more accurate than 10-1 in Euclidean distance to the true value.
Herbrich's algorithm does better but also seems to level off at 102. The new algorithm,
with randomization every 100 bounces, converges nicely. These characteristics are typical
and hold across many different runs on this dataset.
66

100
10-1 
Rujan
10
0
1-2
10
10-3
102 
1064 
10
FLOPS
Figure 5-15: Cost vs. error of conventional billiard algorithms vs. the new algorithm.
67

Chapter 6
Summary and future work
This thesis has developed a family of algorithms for approximate inference, based on ex-
tending assumed-density filtering (ADF) to use iterative refinement. This extension removes
all order dependence from ADF and increases its accuracy. The family includes loopy be-
lief propagation in Bayesian networks, and extends it to allow approximate messages (for
reduced computation) as well as more elaborate messages (for increased accuracy). For in-
ference in the Bayes Point Machine, it includes the algorithm of Opper & Winther (2000c)
and provides an alternative to the costly billiard algorithm.
The accuracy of the Expectation Propagation family was demonstrated on a variety of
examples, with most emphasis on the Bayes Point Machine:
Section 3.2.2 For the clutter problem in one dimension, EP was (in well-behaved cases) 10
times more accurate than its nearest competitor, Laplace's method, both in estimating
the posterior mean and the normalizing constant p(D). In other cases, when the true
posterior was strongly multimodal, EP did not converge at all, while Restricted EP
and other deterministic methods lagged behind Monte Carlo. The computational cost
of EP is slightly higher than Laplace's method but much less than Monte Carlo.
Section 3.3.4 For marginalizing the mixing weight in a mixture density of two Gaussians,
EP was again 10 times more accurate than Laplace's method, its nearest competitor.
With modifications inspired by Cowell et al. (1996), EP is also faster than Laplace's
method. In this problem, the posterior is always unimodal (in fact log-convex), which
may explain why EP always converged.
Section 5.5 On a toy dataset where the Bayes Point Machine is very different from the
Support Vector Machine, EP delivers high accuracy at a fraction of the cost of its
competitor, the billiard algorithm. The accuracy of EP degrades when points are
clustered together, but even then it still beats the billiard algorithm with respect
to cost. On separable problems, the posterior is unimodal and EP seems to always
converge.
Section 5.5 EP is also accurate in situations that the billiard algorithm cannot handle,
namely when there is label noise. This was demonstrated on a toy dataset; more de-
tailed experiments, showing that EP achieves theoretical bounds, are given by Winther
(1998), since his algorithm is equivalent to EP.
Section 5.6 On benchmark datasets with around 150 points and 5-60 features, EP achieves
test set error rates consistent with the billiard algorithm. Beating the SVM on 4 out
68

of 5 datasets, EP realizes in practice the theoretical gains expected with a Bayesian
approach.
Many opportunities for future work are available, both within the framework of EP as
well as beyond it. First, how effective is EP for other statistical models? For example, is it
useful for inference in coupled hidden Markov models, sigmoid belief networks (Barber &
Sollich, 1999), and dynamic trees (Storkey, 2000)? Is EP effective for Bayesian parameter
estimation of classification models beyond linear classifiers, e.g. hidden Markov models?
Is any deterministic method effective for nonparametric models such as Dirichlet processes
(Rasmussen, 1999)?
Second, can we anticipate how EP will perform? Can EP provide an estimate of its
error? (The experiments in this paper always used the ADF initialization. Are the fixed
points of EP unique, or does initialization matter? What causes EP to diverge? 
Can
it be made to always converge? Can EP be made to give reasonable approximations to
multimodal posteriors?
Can EP be made to run faster, especially for the Bayes Point Machine? This thesis
compared EP to the original quadratic programming implementation of the Support Vector
Machine. But much faster, specialized algorithms for the SVM are now available due to
intense research. Can we exploit special properties of the Bayes Point Machine to speed up
EP? For example, can we represent the A matrix in a sparse fashion? Can we identify the
terms that need to be refined? Having an error estimate for EP could facilitate this.
Is it optimal to minimize KL-divergence at each step, or should we use some other
criterion? The modified update inspired by Cowell et al. (1996) shows that we may obtain
computational speedups with other criteria. But improvements in accuracy may also be
possible, if we replace the greedy KL criterion with something that incorporates lookahead.
For example, if we know the posterior has heavy tails and we are approximating it with a
Gaussian, then we may choose to use a Gaussian with inflated variance, to better account
for the tails.
The entire difference between EP and ADF is that EP makes better choices for the
approximate terms ti. But both algorithms base their choices on the 'old' posterior q(x),
which is assumed to be accurate. If the true posterior is multimodal, then q(x) cannot
possibly be accurate. 
Could we propagate additional information about q(x) to use in
choosing Ii?
Can iterative refinement be applied to other filtering algorithms? For example, some
variants of ADF do not use the exponential family. Cowell et al. (1996) approximate a
mixture posterior with a smaller mixture after processing each data point; see also the
"generalized pseudo-Bayes" algorithms in Murphy (1998). Frey et al. (2000) approximate
the discrete posterior with an arbitrary tree-structured distribution. Conventional EP is
not practical with these kinds of approximations because the equivalent terms t,(x)=
q new(x)/q(x) do not simplify.
Yedidia et al. (2000) give a generalization of belief propagation that is different from
the generalization afforded by EP. Their algorithm propagates exact marginals and pairwise
marginals, which gives it high accuracy but also limits its practicality. Can it be extended
to allow approximate messages as in EP? Is there a common parent of their algorithm and
EP?
Bayesian inference is a fertile area for developing numerical integration algorithms, espe-
cially deterministic ones, because of the rich problem structure and prior knowledge about
the functions being integrated. There is also relatively little research on it, compared to
69

optimization for example. Many researchers are swayed enough by this imbalance to use
inferior techniques, such as maximum likelihood estimation, because they only involve op-
timization. As argued in this thesis, it doesn't have to be that way. And I anticipate
that there are even better integration algorithms out there, waiting for someone to discover
them.
70

Bibliography
Aitchison, J., & Shen, S. M. (1980). Logistic-normal distributions: Some properties and
uses. Biometrika, 67, 261-272.
Attias, H. (1999). Inferring parameters and structure of latent variable models by
variational Bayes. Uncertainty in Artificial Intelligence.
http://www.gatsby.ucl.ac.uk/~hagai/papers. html.
Barber, D., & Bishop, C. (1997). Ensemble learning for multi-layer networks. NIPS 10.
MIT Press. http://www.mbfys.kun.nl/~davidb/papers/kl-mlp-nipslO.html.
Barber, D., & Sollich, P. (1999). Gaussian fields for approximate inference in layered
sigmoid belief networks. NIPS 12.
Barber, D., & Wiegerinck, W. (1998). Tractable variational structures for approximating
graphical models. NIPS 11.
ftp: //ftp .mbfys. kun. nl/snn/pub/reports/Barber. nips98. ps. Z.
Bennett, K., & Demiriz, A. (1998). Semi-supervised support vector machines. NIPS 11.
http://www.rpi. edu/~bennek/s3vm.ps.
Bernardo, J. M., & Giron, F. J. (1988). A Bayesian analysis of simple mixture problems.
Bayesian Statistics 3 (pp. 67-78).
Blake, C. L., & Merz, C. J. (1998). UCI repository of machine learning databases.
http://www.ics.uci.edu/ mlearn/MLRepository.html. Irvine, CA: University of
California, Department of Information and Computer Science.
Bouten, M., Schietse, J., & den Broeck, C. V. (1995). Gradient descent learning in
perceptrons - a review of its possibilities. Physical Review E, 52, 1958-1967.
Boyen, X., & Koller, D. (1998a). Approximate learning of dynamic models. NIPS 11.
http: //robotics .Stanford.EDU/~koller/papers/nips98.html.
Boyen, X., & Koller, D. (1998b). Tractable inference for complex stochastic processes.
Uncertainty in AL http://robotics.Stanford.EDU/~koller/papers/uai98.html.
Buhot, A., Moreno, J., & Gordon, M. (1997). Finite size scaling of the Bayesian
perceptron. Physical Review E, 55, 7434-7440.
Carpenter, J., Clifford, P., & Fearnhead, P. (1999). Building robust simulation-based
filters for evolving data sets (Technical Report). Department of Statistics, Oxford
University. http : //citeseer.nj .nec .com/carpenter99building .html.
71

Chapelle, 0., Vapnik, V., Bousquet, 0., & Mukherjee, S. (2000). Choosing kernel
parameters for support vector machines. http: //www. ens-lyon. fr/ochapell/.
Chapelle, 0., & Vapnik, V. N. (1999). Model selection for support vector machines. NIPS
12. http://www.ens-lyon.fr/-ochapell/.
Cowell, R. G., Dawid, A. P., & Sebastiani, P. (1996). A comparison of sequential learning
methods for incomplete data. Bayesian Statistics 5.
http://www.ucl.ac.uk/Stats/research/psfiles/135.zip.
Cristianini, N., Campbell, C., & Shawe-Taylor, J. (1998). Dynamically adapting kernels in
support vector machines. NIPS 11.
Csato, L., Fokue, E., Opper, M., Schottky, B., & Winther, 0. (1999). Efficient approaches
to Gaussian process classification. NIPS 12.
http: //www . ncrg. aston. ac . uk/cgi-bin/travail .pl?trnuber=NCRG/99/025.
Davis, P. J., & Rabinowitz, P. (1984). Methods of numerical integration. Academic Press.
2nd edition.
Frey, B. J., & MacKay, D. J. (1997). A revolution: Belief propagation in graphs with
cycles. NIPS 10. http://www.cs.toronto.edu/~mackay/rev.ps.gz.
Frey, B. J., Patrascu, R., Jaakkola, T., & Moran, J. (2000). Sequentially fitting inclusive
trees for inference in noisy-OR networks. NIPS 13.
Gelfand, A. E., & Smith, A. F. M. (1990). Sampling-based approaches to calculating
marginal densities. J American Stat Assoc, 85, 398-409.
Ghahramani, Z., & Beal, M. J. (1999). Variational inference for Bayesian mixtures of
factor analysers. NIPS 12.
http://www.gatsby.ucl . ac .uk/~zoubin/papers/nips99 
.ps . gz.
Ghahramani, Z., & Jordan, M. I. (1997). Factorial hidden Markov models. Machine
Learning, 29, 245-273. http://citeseer.nj .nec.com/13664.html.
Herbrich, R., & Graepel, T. (2000). A PAC-Bayesian margin bound for linear classifiers:
Why SVMs work. NIPS 13.
http: //www. research. microsoft. com/users/rherb/abstract-HerGraeOOb. htm.
Herbrich, R., Graepel, T., & Campbell, C. (1999). Bayes point machines: Estimating the
Bayes point in kernel space. IJCAI Workshop Support Vector Machines (pp. 23-27).
http: //stat . cs .tu-berlin. de/ ralfh/ abstract -HerGraeCamp99a .html.
Hinton, G., & van Camp, D. (1993). Keeping neural networks simple by minimizing the
description length of the weights. Sixth ACM conference on Computational Learning
Theory. http: //citeseer.nj .nec. com/hinton93keeping.html.
Jaakkola, T. S., & Jordan, M. I. (1999a). Variational probabilistic inference and the
QMR-DT network. Journal of Artificial Intelligence Research, 10, 291-322.
http: //www . cs .berkeley 
.edu/-jordan/papers/varqmr .ps .Z.
72

Jaakkola, T. S., & Jordan, M. I. (1999b). Bayesian parameter estimation via variational
methods. Statistics and Computing, to appear.
http://www.cs.berkeley.edu/~jordan/papers/variational-bayes.ps 
.Z.
Jaakkola, T. S., & Jordan, M. I. (1999c). Improving the mean field approximation via the
use of mixture distributions. In Learning in graphical models. MIT Press.
http://www.cs.berkeley.edu/~jordan/papers/mixture-mean-f ield.ps.Z.
Jordan, M. I., Ghahramani, Z., Jaakkola, T. S., & Saul, L. K. (1999). An introduction to
variational methods for graphical models. Machine Learning, 37, 183-233.
http://citeseer.nj.nec.com/jordan98introduction.html.
Kappen, H., & Wiegerinck, W. (2000). Second order approximations for probability
models. NIPS 13. ftp://ftp .mbfys .kun.nl/snn/pub/reports/Kappen-NIPS2000 .ps.
Kass, R. E., & Raftery, A. E. (1993). Bayes factors and model uncertainty (Technical
Report 254). University of Washington.
http: //www. stat. 
washington. edu/tech. reports/tr254. ps.
Koller, D., Lerner, U., & Angelov, D. (1999). A general algorithm for approximate
inference and its application to hybrid Bayes nets. Uncertainty in AI (pp. 324-333).
http://robotics.Stanford.EDU/~koller/papers/uai99kla.html.
Kschischang, F. R., Frey, B. J., & Loeliger, H.-A. (2000). Factor graphs and the
sum-product algorithm. IEEE Trans Info Theory, to appear.
http://www.comm. toronto. edu/f rank/f actor/.
Kushner, H. J., & Budhiraja, A. S. (2000). A nonlinear filtering algorithm based on an
approximation of the conditional distribution. IEEE Trans Automatic Control, 45,
580-585.
Lauritzen, S. L. (1992). Propagation of probabilities, means and variances in mixed
graphical association models. J American Statistical Association, 87, 1098-1108.
Liu, J. S. (1999). Markov chain Monte Carlo and related topics (Technical Report).
Department of Statistics, Stanford University.
http: //www-stat .stanford. edu/~jliu/TechRept/99.html.
Liu, J. S., & Chen, R. (2000). Mixture Kalman filters. J Royal Statistical Society B, to
appear. http: //www-stat .stanford.edu/~jliu/TechRept/00.html.
MacKay, D. J. C. (1995). Probable networks and plausible predictions -
a review of
practical Bayesian methods for supervised neural networks. Network: Computation in
Neural Systems, 6, 469-505.
http: //wol.ra.phy. cam. ac .uk/mackay/abstracts/network.html.
MacKay, D. J. C. (1998). Choice of basis for Laplace approximation. Machine Learning,
33. http://wol.ra.phy.cam.ac.uk/mackay/abstracts/laplace.html.
Maybeck, P. S. (1982). Stochastic models, estimation and control, chapter 12.7. Academic
Press.
73

Minka, T. P. (2000a). Automatic choice of dimensionality for PCA. NIPS 13.
f tp: //whit echapel.media.mit .edu/pub/tech-reports/TR-5 14-ABSTRACT .html.
Minka, T. P. (2000b). Estimating a Dirichlet distribution.
http: //vismod.www.media.mit.edu/~tpminka/papers/dirichlet.html.
Minka, T. P. (2000c). Variational Bayes for mixture models: Reversing EM.
http://vismod.www.media.mit. edu/~tpminka/papers/rem.html.
Murphy, K. (1998). Learning switching Kalman filter models (Technical Report
CSD-98-990). Compaq Cambridge Research Lab.
http: //www. cs .berkeley 
. edu/~murphyk/Papers/skf .ps . gz.
Murphy, K., & Weiss, Y. (2000). The Factored Frontier algorithm for approximate
inference in DBNs (Technical Report). UC Berkeley.
http: //www. cs .berkeley 
.edu/~murphyk/Papers/ff .ps . gz.
Murphy, K., Weiss, Y., & Jordan, M. (1999). Loopy-belief propagation for approximate
inference: An empirical study. Uncertainty in AL
http://www.cs.berkeley.edu/~murphyk/Papers/loopy.ps.gz.
Neal, R. M. (1993). Probabilistic inference using Markov chain Monte Carlo methods
(Technical Report CRG-TR-93-1). Dept. of Computer Science, University of Toronto.
http: //www. cs. toronto. edu/~radford/review. abstract.html.
Opper, M., & Winther, 0. (1999). A Bayesian approach to on-line learning. On-Line
Learning in Neural Networks. Cambridge University Press.
http://www.ncrg.aston.ac.uk/cgi-bin/tr-avail.pl?trnumber=NCRG/99/029.
Opper, M., & Winther, 0. (2000a). Adaptive TAP equations. Advanced Mean Field
Methods - Theory and Practice. MIT Press.
http://nimis.thep.lu.se/tf2/staff/winther/opper.adaptive.ps.Z.
Opper, M., & Winther, 0. (2000b). Gaussian Processes and SVM: Mean field results and
leave-one-out. Advances in Large Margin Classifiers. MIT Press.
http: //nimis.thep. lu. se/tf2/staff/winther/opper. largemargin.ps. Z.
Opper, M., & Winther, 0. (2000c). Gaussian processes for classification: Mean field
algorithms. Neural Computation.
http: //nimis .thep . lu. se/tf2/staf f /winther/opper neuralcomp.ps .Z.
Rasmussen, C. E. (1999). The infinite Gaussian mixture model. NIPS 12.
http ://bayes . imm. dtu. dk/pub/inf .mix .nips .99. abs . html.
Rujan, P. (1997). Perceptron learning by playing billiards. Neural Computation, 9,
99-122. http://saturn.neuro.uni-oldenburg.de/-rujan/.
Rusmevichientong, P., & Roy, B. V. (1999). An analysis of turbo decoding with gaussian
densities. NIPS 12.
Seeger, M. (1999). Bayesian model selection for support vector machines, Gaussian
processes and other kernel classifiers. NIPS 12.
http: //www. dai. ed. ac. uk/homes/seeger/papers/nips-paper. ps. gz.
74

Shachter, R. (1990). A linear approximation method for probabilistic inference.
Uncertainty in AL
Stephens, M. (1997). Bayesian methods for mixtures of normal distributions. Doctoral
dissertation, Oxford University.
http://www.stats.ox.ac.uk/~stephens/papers.html.
Storkey, A. (2000). Dynamic trees: A structured variational method giving efficient
propagation rules. UAI'00. http://anc.ed.ac.uk/~amos/uail43.ps.
Tong, S., & Koller, D. (2000). Restricted Bayes optimal classifiers. Proceedings of the 17th
National Conference on Artificial Intelligence (AAAI).
http: //robotics. stanf ord. edu/~koller/papers/aaaiO0tk.html.
Ventura, V. (2000). Double importance sampling (Technical Report). Dept of Statistics,
Carnegie Mellon University.
http: //www. stat . cmu. edu/cmu-stats/tr/tr694/tr694. html.
Waterhouse, S., MacKay, D., & Robinson, T. (1995). Bayesian methods for mixtures of
experts. NIPS 8.
Watkin, T. (1993). Optimal learning with a neural network. Europhysics Letters, 21,
871-876.
Weston, J., Mukherjee, S., Chapelle, 0., Pontil, M., Poggio, T., & Vapnik, V. (2000).
Feature selection for SVMs. NIPS 13. http: //www. ens-lyon.fr/~ochapell/.
Winther, 0. (1998). Bayesian mean field algorithms for neural networks and Gaussian
processes. Doctoral dissertation, University of Copenhagen.
http: //nimis. thep. lu. se/tf2/staff/winther/thesis. ps. Z.
Yedidia, J. S. (2000). An idiosyncratic journey beyond mean field theory (Technical
Report TR2000-27). MERL. http://www.merl. com/reports/TR2000-27/index.html.
Yedidia, J. S., Freeman, W. T., & Weiss, Y. (2000). Generalized belief propagation
(Technical Report TR2000-26). MERL.
http://www.merl. com/report s/TR2000-26/ index.html.
75

