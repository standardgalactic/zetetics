DiffWire: Inductive Graph Rewiring via the LovÃ¡sz Bound
Adrian Arnaiz-Rodriguez
ELLIS Alicante
adrian@ellisalicante.org
Ahmed Begga
University of Alicante
Francisco Escolano
ELLIS Alicante
sco@ellisalicante.org
Nuria Oliver
ELLIS Alicante
nuria@ellisalicante.org
Abstract
Graph Neural Networks (GNNs) have been shown to achieve competitive results
to tackle graph-related tasks, such as node and graph classification, link prediction
and node and graph clustering in a variety of domains. Most GNNs use a message
passing framework and hence are called MPNNs. Despite their promising results,
MPNNs have been reported to suffer from over-smoothing, over-squashing and
under-reaching. Graph rewiring and graph pooling have been proposed in the
literature as solutions to address these limitations. However, most state-of-the-art
graph rewiring methods fail to preserve the global topology of the graph, are
neither differentiable nor inductive, and require the tuning of hyper-parameters.
In this paper, we propose DIFFWIRE, a novel framework for graph rewiring in
MPNNs that is principled, fully differentiable and parameter-free by leveraging
the LovÃ¡sz bound. The proposed approach provides a unified theory for graph
rewiring by proposing two new, complementary layers in MPNNs: CT-LAYER, a
layer that learns the commute times and uses them as a relevance function for edge
re-weighting; and GAP-LAYER, a layer to optimize the spectral gap, depending on
the nature of the network and the task at hand. We empirically validate the value of
each of these layers separately with benchmark datasets for graph classification.
We also perform preliminary studies on the use of CT-LAYER for homophilic and
heterophilic node classification tasks. DIFFWIRE brings together the learnability
of commute times to related definitions of curvature, opening the door to creating
more expressive MPNNs.
1
Introduction
Graph Neural Networks (GNNs) [1, 2] are a class of deep learning models applied to graph structured
data. They have been shown to achieve state-of-the-art results in many graph-related tasks, such as
node and graph classification [3, 4], link prediction [5] and node and graph clustering [6, 7], and in a
variety of domains, including image or molecular structure classification, recommender systems and
social influence prediction [8].
Most GNNs use a message passing framework and thus are referred to as Message Passing Neural
Networks (MPNNs) [4] . In these networks, every node in each layer receives a message from its
adjacent neighbors. All the incoming messages at each node are then aggregated and used to update
the nodeâ€™s representation via a learnable non-linear function â€“which is typically implemented by
means of a neural network. The final node representations (called node embeddings) are used to
perform the graph-related task at hand (e.g. graph classification). MPNNs are extensible, simple and
have proven to yield competitive empirical results. Examples of MPNNs include GCN [3], GAT [9],
GATv2 [10], GIN [11] and GraphSAGE [12]. However, they typically use transductive learning, i.e.
the model observes both the training and testing data during the training phase, which might limit
their applicability to graph classification tasks.
A. Arnaiz-Rodriguez et al., DiffWire: Inductive Graph Rewiring via the LovÃ¡sz Bound. Proceedings of the First
Learning on Graphs Conference (LoG 2022), PMLR 198, Virtual Event, December 9â€“12, 2022.

DiffWire: Inductive Graph Rewiring via the LovÃ¡sz Bound
However, MPNNs also have important limitations due to the inherent complexity of graphs. Despite
such complexity, the literature has reported best results when MPNNs have a small number of layers,
because networks with many layers tend to suffer from over-smoothing [13] and over-squashing [14].
However, this models fail to capture information that depends on the entire structure of the graph [15]
and prevent the information flow to reach distant nodes. This phenomenon is called under-reaching
[16] and occurs when the MPNNâ€™s depth is smaller than the graphâ€™s diameter.
Over-smoothing [8, 17â€“19] takes place when the embeddings of nodes that belong to different classes
become indistinguishable. It tends to occur in MPNNs with many layers that are used to tackle short-
range tasks, i.e. tasks where a nodeâ€™s correct prediction mostly depends on its local neighborhood.
Given this local dependency, it makes intuitive sense that adding layers to the network would not
help the networkâ€™s performance.
Conversely, long-range tasks require as many layers in the network as the range of the interaction
between the nodes. However, as the number of layers in the network increases, the number of
nodes feeding into each of the nodeâ€™s receptive field also increases exponentially, leading to over-
squashing [14, 20]: the information flowing from the receptive field composed of many nodes is
compressed in fixed-length node vectors, and hence the graph fails to correctly propagate the messages
coming from distant nodes. Thus, over-squashing emerges due to the distortion of information flowing
from distant nodes due to graph bottlenecks that emerge when the number of k-hop neighbors grows
exponentially with k.
Graph pooling and graph rewiring have been proposed in the literature as solutions to address these
limitations [14]. Given that the main infrastructure for message passing in MPNNs are the edges
in the graph, and given that many of these edges might be noisy or inadequate for the downstream
task [21], graph rewiring aims to identify such edges and edit them.
Many graph rewiring methods rely on edge sampling strategies: first, the edges are assigned new
weights according to a relevance function and then they are re-sampled according to the new weights
to retain the most relevant edges (i.e. those with larger weights). Edge relevance might be computed
in different ways, including randomly [22], based on similarity [23] or on the edgeâ€™s curvature [20].
Due to the diversity of possible graphs and tasks to be performed with those graphs, optimal graph
rewiring should include a variety of strategies that are suited not only to the task at hand but also to
the nature and structure of the graph.
Motivation.
State-of-the-art edge sampling strategies have three significant limitations. First,
most of the proposed methods fail to preserve the global topology of the graph. Second, most
graph rewiring methods are neither differentiable nor inductive [20]. Third, relevance functions that
depend on a diffusion measure (typically in the spectral domain) are not parameter-free, which adds
a layer of complexity in the models. In this paper, we address these three limitations.
Contributions and Outline.
The main contribution of this work is to propose a theoretical frame-
work called DIFFWIRE for graph rewiring in GNNs that is principled, differentiable, inductive, and
parameter-free by leveraging the LovÃ¡sz bound [15] given by Eq. 1. This bound is a mathematical
expression of the relationship between the commute times (effective resistance distance) and the
networkâ€™s spectral gap. Given an unseen test graph, DIFFWIRE predicts the optimal graph structure
for the task at hand without any parameter tuning. Given the recently reported connection between
commute times and curvature [24], and between curvature and the spectral gap [20], the proposed
framework provides a unified theory linking these concepts. Our aim is to leverage diffusion and
curvature theories to propose a new approach for graph rewiring that preserves the graphâ€™s structure.
We first propose using the CT as a relevance function for edge re-weighting. Moreover, we develop a
differentiable, parameter-free layer in the GNN (CT-LAYER) to learn the CT. Second, we propose an
alternative graph rewiring approach by adding a layer in the network (GAP-LAYER) that optimizes
the spectral gap according to the nature of the network and the task at hand. Finally, we empirically
validate the proposed layers with state-of-the-art benchmark datasets in a graph classification task.
We test our approach on a graph classification task to emphasize the inductive nature of DIFFWIRE:
the layers in the GNN (CT-LAYER or GAP-LAYER) are trained to predict the CTs embedding or
minimize the spectral gap for unseen graphs, respectively. This approach gives a great advantage
when compared to SoTA methods that require optimizing the parameters of the models for each graph.
CT-LAYER and GAP-LAYER learn the weights during training to predict the optimal changes in the
2

DiffWire: Inductive Graph Rewiring via the LovÃ¡sz Bound
topology of any unseen graph in test time. Finally, we also perform preliminary node classification
experiments in heterophilic and homophilic graphs using CT-LAYER.
The paper is organized as follows: Section 2 provides a summary of the most relevant related literature.
Our core technical contribution is described in Section 3, followed by our experimental evaluation
and discussion in Section 4. Finally, Section 5 is devoted to conclusions and an outline of our future
lines of research.
2
Related Work
In this section we provide an overview of the most relevant works that have been proposed in the
literature to tackle the challenges of over-smoothing, over-squashing and under-reaching in MPNNs
by means of graph rewiring and pooling.
Graph Rewiring in MPNNs. Rewiring is a process of changing the graphâ€™s structure to control the
information flow and hence improve the ability of the network to perform the task at hand (e.g. node
or graph classification, link prediction...). Several approaches have been proposed in the literature for
graph rewiring, such as connectivity diffusion [25] or evolution [20], adding new bridge-nodes [26]
and multi-hop filters [27], and neighborhood [12], node [28] and edge [22] sampling.
Edge sampling methods sample the graphâ€™s edges based on their weights or relevance, which
might be computed in different ways. Rong et al. [22] show that randomly dropping edges during
training improves the performance of GNNs. Klicpera et al. [25], define edge relevance according
to the coefficients of a parameterized diffusion process over the graph and then edges are selected
using a threshold rule. For Kazi et al. [23], edge relevance is given by the similarity between the
nodesâ€™ attributes. In addition, a reinforcement learning process rewards edges leading to a correct
classification and penalizes the rest.
Edge sampling-based rewiring has been proposed to tackle over-smoothing and over-squashing in
MPNNs. Over-smoothing may be relieved by removing inter-class edges [29]. However, this strategy
is only valid when the graph is homophilic, i.e. connected nodes tend to share similar attributes.
Otherwise, removing these edges could lead to over-squashing [20] if their removal obstructs the
message passing between distant nodes belonging to the same class (heterophily). Increasing the
size of the bottlenecks of the graph via rewiring has been shown to improve node classification
performance in heterophilic graphs, but not in homophilic graphs [20]. Recently, Topping et al. [20]
propose an edge relevance function given by the edge curvature to mitigate over-squashing. They
identify the bottleneck of the graph by computing the Ricci curvature of the edges. Next, they remove
edges with high curvature and add edges around minimal curvature edges.
Graph Structure Learning (GSL). GSL methods [30] aim to learn an optimized graph structure and
its corresponding representations at the same time. DIFFWIRE could be seen from the perspective of
GSL: CT-LAYER, as a metric-based, neural approach, and GAP-LAYER, as a direct-neural approach
to optimize the structure of the graph to the task at hand.
Graph Pooling. Pooling layers simplify the original graph by compressing it into a smaller graph
or a vector via pooling operators, which range from simple [31] to more sophisticated approaches,
such as DiffPool [32] and MinCut pool [33]. Although graph pooling methods do not consider the
edge representations, there is a clear relationship between pooling methods and rewiring since both
of them try to quantify the flow of information through the graphâ€™s bottleneck.
Positional Encodings (PEs) A Positional Encoding is a feature that describes the global or local
position of the nodes in the graph. These features are related to random walk measures and the
Laplacianâ€™s eigenvectors [34]. Commute Times embeddings (CTEs) may be considered an expressive
form of PEs due to their spectral properties, i.e. their relation with the shortest path, the spectral gap
or Cheeger constant. Velingker et al. [35] recently proposed use the CTEs as PE or commute times
(CT) as edge feature. They pre-compute the CTEs and CT and add it as node or edge features to
improve the structural expressiveness of the GNN. PEs are typically pre-computed and then used to
build more expressive graph architectures, either by concatenating them to the node features or by
building transformer models [36, 37]. Our work is related to PEs as CT-LAYER learns the original
PEs from the input X and the adjacency matrix A instead of pre-computing and potentially modifying
them, as previous works do [35â€“38]. Thus, CT-LAYER may be seen as a method to automatically
learn the PEs for graph rewiring.
3

DiffWire: Inductive Graph Rewiring via the LovÃ¡sz Bound
Figure 1: DIFFWIRE. Left: Original graph from COLLAB (test set). Center: Rewired graph after
CT-LAYER. Right: Rewired graph after GAP-LAYER. Colors indicate the strength of the edges.
3
Proposed Approach: DIFFWIRE for Inductive Graph Rewiring
DIFFWIRE provides a unified theory for graph rewiring by proposing two new, complementary layers
in MPNNs: first, CT-LAYER, a layer that learns the commute times and uses them as a relevance
function for edge re-weighting; and second, GAP-LAYER, a layer to optimize the spectral gap,
depending on the nature of the network and the task at hand.
In this section, we present the theoretical foundations for the definitions of CT-LAYER and GAP-
LAYER. First, we introduce the bound that our approach is based on: The LovÃ¡sz bound. Table 3 in
A.1 summarizes the notation used in the paper.
3.1
The LovÃ¡sz Bound
The LovÃ¡sz bound, given by Eq. 1, was derived by LovÃ¡sz in [15] as a means of linking the spectrum
governing a random walk in an undirected graph G = (V, E) with the hitting time Huv between any
two nodes u and v of the graph. Huv is the expected number of steps needed to reach (or hit) v from
u; Hvu is defined analogously. The sum of both hitting times between the two nodes, v and u, is the
commute time CTuv = Huv + Hvu. Thus, CTuv is the expected number of steps needed to hit v
from u and go back to u. According to the LovÃ¡sz bound:

1
vol(G)CTuv âˆ’
 1
du
+ 1
dv
 â‰¤1
Î»â€²
2
2
dmin
(1)
where Î»â€²
2 â‰¥0 is the spectral gap, i.e. the first non-zero eigenvalue of L = I âˆ’Dâˆ’1/2ADâˆ’1/2
(normalized Laplacian [39], where D is the degree matrix and A, the adjacency matrix); vol(G) is
the volume of the graph (sum of degrees); du and dv are the degrees of nodes u and v, respectively;
and dmin is the minimum degree of the graph.
The term CTuv/vol(G) in Eq. 1 is referred to as the effective resistance, Ruv, between nodes u and
v. The bound states that the effective resistance between two nodes in the graph converges to or
diverges from (1/du + 1/dv), depending on whether the graphâ€™s spectral gap diverges from or tends
to zero. The larger the spectral gap, the closer CTuv/vol(G) will be to
1
du +
1
dv and hence the less
informative the commute times will be.
We propose two novel GNNs layers based on each side of the inequality in Eq. 1: CT-LAYER, focuses
on the left-hand side, and GAP-LAYER, on the right-hand side. The use of each layer depends on
the nature of the network and the task at hand. In a graph classification task (our focus), CT-LAYER
is expected to yield good results when the graphâ€™s spectral gap is small; conversely, GAP-LAYER
would be the layer of choice in graphs with large spectral gap.
The LovÃ¡sz bound was later refined by von Luxburg et al. [40]. App. A.2.2 presents this bound along
with its relationship with Ruv as a global measure of node similarity. Once we have defined both
sides of the LovÃ¡sz bound, we proceed to describe their implications for graph rewiring.
4

DiffWire: Inductive Graph Rewiring via the LovÃ¡sz Bound
3.2
CT-LAYER: Commute Times for Graph Rewiring
We focus first on the left-hand side of the LovÃ¡sz bound which concerns the effective resistances
CTuv/vol(G) = Ruv (or commute times)1 between any two nodes in the graph.
Spectral Sparsification leads to Commute Times.
Graph sparsification in undirected graphs
may be formulated as finding a graph H = (V, Eâ€²) that is spectrally similar to the original graph
G = (V, E) with Eâ€² âŠ‚E. Thus, the spectra of their Laplacians, LG and LH should be similar.
Theorem 1 (Spielman and Srivastava [41]). Let Sparsify(G, q) â€“> Gâ€™ be a sampling algorithm of
graph G = (V, E), where edges e âˆˆE are sampled with probability q âˆRe (proportional to the
effective resistance). For n = |V | sufficiently large and 1/âˆšn < Ïµ â‰¤1, O(n log n/Ïµ2) samples are
needed to satisfy âˆ€x âˆˆRn : (1 âˆ’Ïµ)xT LGx â‰¤xT LGâ€²x â‰¤(1 + Ïµ)xT LGx , with probability â‰¥1/2.
The above theorem has a simple explanation in terms of Dirichlet energies, E(x). The Laplacian
L = D âˆ’A â‰½0, i.e. it is positive semi-definite (all its eigenvalues are non-negative). Then,
if we consider x : V â†’R as a real-valued function of the n nodes of G = (V, E), we have
that E(x) := xT LGx = P
e=(u,v)âˆˆE(xu âˆ’xv)2 â‰¥0 for any x. In particular, the eigenvectors
f := {fi : Lfi = Î»ifi} are the set of special functions that minimize the energies E(fi), i.e. they are
the mutually orthogonal and normalized functions with the minimal variabilities achievable by the
topology of G. Therefore, any minimal variability of Gâ€² is bounded by (1 Â± Ïµ) times that of G if we
sample enough edges with probability q âˆRe. In addition, Î»i = E(fi)
f T
i fi .
This first result implies that edge sampling based on commute times is a principled way to rewire a
graph while preserving its original structure and it is bounded by the Dirichlet energies. Next, we
present what a commute times embedding is and how it can be spectrally computed.
Commute Times Embedding (CTE).
The choice of effective resistances in Theorem 1 is explained
by the fact that Ruv can be computed from Ruv = (euâˆ’ev)T L+(euâˆ’ev), where eu is the unit vector
with a unit value at u and zero elsewhere. L+ = P
iâ‰¥2 Î»âˆ’1
i fif T
i , where fi, Î»i are the eigenvectors
and eigenvalues of L, is the pseudo-inverse or Greenâ€™s function of G = (V, E) if it is connected. The
Greenâ€™s function leads to envision Ruv (and therefore CTuv) as metrics relating pairs of nodes of G.
As a result, the CTE will preserve the commute times distance in a Euclidean space. Note that this
latent space of the nodes can not only be described spectrally but also in a parameter free-manner,
which is not the case for other spectral embeddings, such as heat kernel or diffusion maps as they rely
on a time parameter t. More precisely, the embedding matrix Z whose columns contain the nodesâ€™
commute times embeddings is spectrally given by:
Z :=
p
vol(G)Î›âˆ’1/2FT =
p
vol(G)Î›â€²âˆ’1/2GT Dâˆ’1/2
(2)
where Î› is the diagonal matrix of the unnormalized Laplacian L eigenvalues and F is the matrix of
their associated eigenvectors. Similarly, Î›â€² contains the eigenvalues of the normalized Laplacian L
and G the eigenvectors. We have F = GDâˆ’1/2 or fi = giDâˆ’1/2, where D is the degree matrix.
Finally, the commute times are given by the Euclidean distances between the embeddings CTuv =
âˆ¥zu âˆ’zvâˆ¥2. The spectral calculation of commute times distances is given by:
Ruv = CTuv
vol(G) = âˆ¥zu âˆ’zvâˆ¥2
vol(G)
=
n
X
i=2
1
Î»i
(fi(u) âˆ’fi(v))2 =
n
X
i=2
1
Î»â€²
i
gi(u)
âˆšdu
âˆ’gi(v)
âˆšdv
2
(3)
Commute Times as an Optimization Problem.
In this section, we demonstrate how the CTs may
be computed as an optimization problem by means of a differentiable layer in a GNN. Constraining
neighboring nodes to have a similar embedding leads to
Z = arg min
ZT Z=I
P
u,v âˆ¥zu âˆ’zvâˆ¥2Auv
P
u,v Z2uvdu
=
P
(u,v)âˆˆE âˆ¥zu âˆ’zvâˆ¥2
P
u,v Z2uvdu
= Tr[ZT LZ]
Tr[ZT DZ] ,
(4)
which reveals that CTs embeddings result from a Laplacian regularization down-weighted by the
degree. As a result, frontier nodes or hubs â€“i.e. nodes with inter-community edgesâ€“ which tend to
1We use commute times and effective resistances interchangeably as per their use in the literature
5

DiffWire: Inductive Graph Rewiring via the LovÃ¡sz Bound
have larger degrees than those lying inside their respective communities will be embedded far away
from their neighbors, increasing the distance between communities. Note that the above quotient of
traces formulation is easily differentiable and different from Tr[ ZT LZ
ZT DZ] proposed in [42].
With the above elements we define CT-LAYER, the first rewiring layer proposed in this paper. See
Figure 2 for a graphical representation of the layer.
Definition 1 (CT-Layer). Given the matrix XnÃ—F encoding the features of the nodes after any
message passing (MP) layer, ZnÃ—O(n) = tanh(MLP(X)) learns the association X â†’Z while Z is
optimized according to the loss LCT = T r[ZT LZ]
T r[ZT DZ] +

ZT Z
âˆ¥ZT Zâˆ¥F âˆ’In

F . This results in the following
resistance diffusion TCT = R(Z) âŠ™A, i.e. the Hadamard product between the resistance distance
and the adjacency matrix, providing as input to the subsequent MP layer a learnt convolution matrix.
We set R(Z) to the pairwise Euclidean distances of the node embeddings in Z divided by vol(G).
Thus, CT-LAYER learns the CTs and rewires an input graph according to them: the edges with
maximal resistance will tend to be the most important edges so as to preserve the topology of the
graph.
ğ¿ğ¶ğ‘‡= ğ‘‡ğ‘Ÿ[ğ™ğ“ğ‹ğ™]
ğ‘‡ğ‘Ÿ[ğ™ğ“ğƒğ™] +
ğ™ğ“ğ™
ğ™ğ“ğ™ğ¹
âˆ’ğˆğ‘
ğ¹
Pool - tanh
ğ—
A
ğ™âˆˆâ„ğ‘›Ã—ğ‘‚(ğ‘›)
ğ“ğ‚ğ“âˆˆâ„ğ‘›Ã—ğ‘›= cdist(ğ™)
ğ‘£ğ‘œğ‘™(ğº) âŠ™A
ğ“ğ‚ğ“
Figure 2: Detailed depiction of CT-LAYER, where cdist refers to the matrix of pairwise Euclidean
distances between the node embeddings in Z.
Below, we present the relationship between the CTs and the graphâ€™s bottleneck and curvature.
TCT and Graph Bottlenecks.
Beyond the principled sparsification of TCT (Theorem 1), this
layer rewires the graph G = (E, V ) in such a way that edges with maximal resistance will tend to be
the most critical to preserve the topology of the graph. More precisely, although P
eâˆˆE Re = n âˆ’1,
the bulk of the resistance distribution will be located at graph bottlenecks, if they exist. Otherwise,
their magnitude is upper-bounded and the distribution becomes more uniform.
Graph bottlenecks are controlled by the graphâ€™s conductance or Cheeger constant, hG = minSâŠ†V hS,
where: hS =
|âˆ‚S|
min(vol(S),vol( Â¯S)), âˆ‚S = {e = (u, v) : u âˆˆS, v âˆˆÂ¯S} and vol(S) = P
uâˆˆS du.
The interplay between the graphâ€™s conductance and effective resistances is given by:
Theorem 2 (Alev et al. [43]). Given a graph G = (V, E), a subset S âŠ†V with vol(S) â‰¤vol(G)/2,
hS â‰¥
c
vol(S)1/2âˆ’Ïµ â‡â‡’|âˆ‚S| â‰¥c Â· vol(S)1/2âˆ’Ïµ,
(5)
for some constant c and Ïµ âˆˆ[0, 1/2]. Then, Ruv â‰¤

1
d2Ïµ
u +
1
d2Ïµ
v

Â·
1
ÏµÂ·c2 for any pair u, v.
According to this theorem, the larger the graphâ€™s bottleneck, the tighter the bound on Ruv are.
Moreover, max(Ruv) â‰¤1/h2
S, i.e., the resistance is bounded by the square of the bottleneck.
This bound partially explains the rewiring of the graph in Figure 1-center. As seen in the Figure 1-
center, rewiring using CT-LAYER sparsifies the graph and assigns larger weights to the edges located
in the graphâ€™s bottleneck. The interplay between Theorem 2 and Theorem 1 is described in App. A.1.
Recent work has proposed using curvature for graph rewiring. We outline below the relationship
between CTs and curvature.
Effective Resistances and Curvature.
Topping et al. [20] propose an approach for graph rewiring,
where the relevance function is given by the Ricci curvature. However, this measure is non-
differentiable. More recent definitions of curvature [24] have been formulated based on resistance
6

DiffWire: Inductive Graph Rewiring via the LovÃ¡sz Bound
distances that would be differentiable using our approach. The resistance curvature of an edge
e = (u, v) is Îºuv := 2(pu + pv)/Ruv where pu := 1 âˆ’1
2
P
uâˆ¼w Ruv is the nodeâ€™s curvature.
Relevant properties of the edge resistance curvature are discussed in App. A.1.3, along with a related
Theorem proposed in Devriendt and Lambiotte [24].
3.3
GAP-LAYER: Spectral Gap Optimization for Graph Rewiring
The right-hand side of the LovÃ¡sz bound in Eq. 1 relies on the graphâ€™s spectral gap Î»â€²
2, such that the
larger the spectral gap, the closer the commute times would be to their non-informative regime. Note
that the spectral gap is typically large in commonly observed graphs â€“such as communities in social
networks which may be bridged by many edges [44]â€“ and, hence, in these cases it would be desirable
to rewire the adjacency matrix A so that Î»â€²
2 is minimized.
In this section, we explain how to rewire the graphâ€™s adjacency matrix A to minimize the spectral gap.
We propose using the gradient of Î»2 wrt each component of ËœA. Then, we can compute these gradient
either using Laplacians (L, with Fiedler Î»2) or normalized Laplacians (L, with Fiedler Î»â€²
2). We also
present an approximation of the Fiedler vectors needed to compute those gradients, and propose
computing them as a GNN Layer called the GAP-LAYER. A detailed schematic of GAP-LAYER is
shown in Figure 3.
Rewiring using a Ratio-cut (Rcut) Approximation. We propose to rewire the adjacency matrix, A,
so that Î»2 is minimized. We consider a matrix ËœA close to A that satisfies ËœLf2 = Î»2f2, where f2 is
the solution to the ratio-cut relaxation [45]. Following [46], the gradient of Î»2 wrt each component
of ËœA is given by
âˆ‡ËœAÎ»2 := Tr
h
(âˆ‡ËœLÎ»2)T Â· âˆ‡ËœAËœL
i
= diag(f2f T
2 )11T âˆ’f2f T
2
(6)
where 1 is the vector of n ones; and [âˆ‡ËœAÎ»2]ij is the gradient of Î»2 wrt ËœAuv. The driving force of
this gradient relies on the correlation f2f T
2 . Using this gradient to minimize Î»2 results in breaking
the graphâ€™s bottleneck while preserving simultaneously the inter-cluster structure. We delve into this
matter in App. A.2.
Rewiring using a Normalized-cut (Ncut) Approximation.
Similarly, considering now Î»â€²
2 for
rewiring leads to
âˆ‡ËœAÎ»â€²
2 := Tr
h
(âˆ‡Ëœ
LÎ»2)T Â· âˆ‡ËœA ËœL
i
=
dâ€² n
gT
2 ËœAT ËœDâˆ’1/2g2
o
1T + dâ€² n
gT
2 ËœA ËœDâˆ’1/2g2
o
1T
+
ËœDâˆ’1/2g2gT
2 ËœDâˆ’1/2
(7)
where dâ€² is a n Ã— 1 vector including derivatives of degree wrt adjacency and related terms. This
gradient relies on the Fiedler vector g2 (the solution to the normalized-cut relaxation), and on the
incoming and outgoing one-hop random walks. This approximation breaks the bottleneck while
preserving the global topology of the graph (Figure 1-left). Proof and details are included in App. A.2.
We present next an approximation of the Fiedler vector, followed by a proposed new layer in the
GNN called the GAP-LAYER to learn how to minimize the spectral gap of the graph.
Approximating the Fiedler vector.
Given that g2 = ËœD1/2f2, we can obtain the normalized-cut
gradient in terms of f2. From [17] we have that
f2(u) =

+1/âˆšn
if u belongs to the first cluster
âˆ’1/âˆšn
if u belongs to the second cluster + O
log n
n

(8)
Definition 2 (GAP-Layer). Given the matrix XnÃ—F encoding the features of the nodes after any
message passing (MP) layer, SnÃ—2 = Softmax(MLP(X)) learns the association X â†’S while S is
optimized according to the loss LCut = âˆ’T r[ST AS]
T r[ST DS] +

ST S
âˆ¥ST Sâˆ¥F âˆ’In
âˆš
2

F . Then the Fiedler vector
f2 is approximated by appyling a softmaxed version of Eq. 8 and considering the loss LF iedler =
âˆ¥ËœA âˆ’Aâˆ¥F + Î±(Î»âˆ—
2)2, where Î»âˆ—
2 = Î»2 if we use the ratio-cut approximation (and gradient) and
Î»âˆ—
2 = Î»â€²
2 if we use the normalized-cut approximation and gradient. This returns ËœA and the GAP
diffusion TGAP = ËœA(S) âŠ™A results from minimizing LGAP := LCut + LF iedler.
7

DiffWire: Inductive Graph Rewiring via the LovÃ¡sz Bound
MLP - Ïƒ
ğ—
A
ğ’âˆˆâ„ğ‘›Ã—2
à·©AâŠ™A
ğ¿ğ‘ğ‘¢ğ‘¡= ğ‘‡ğ‘Ÿ[ğ’ğ“ğ‹ğ’]
ğ‘‡ğ‘Ÿ[ğ’ğ“ğƒğ’] +
ğ’ğ“ğ’
ğ’ğ“ğ’ğ¹
âˆ’ğˆğ‘
2
ğ¹
ğ“ğ†ğ€ğ
ğŸ2(ğ’)
Î»ğŸ= â„°ğŸ2
âˆ‡à·©ğ€ğ¿ğ¹ğ‘–ğ‘’ğ‘‘ğ‘™ğ‘’ğ‘Ÿ
à·©ğ€= ğ€âˆ’ğœ‡Ã— âˆ‡à·©ğ€Î»2
ğ¿ğ‘“ğ‘–ğ‘’ğ‘‘ğ‘™ğ‘’ğ‘Ÿ=
à·©ğ€âˆ’A ğ¹+ Î±(Î»2)2
âˆ‡à·©ğ€Î»2 = 2 à·©ğ€âˆ’ğ€+ (diag ğŸ2ğŸ2
ğ‘‡ğŸğŸğ‘‡âˆ’ğŸ2ğŸ2
ğ‘‡) Ã— ğœ†2
Figure 3: GAP-LAYER (Rcut). For GAP-LAYER (Ncut), substitute âˆ‡ËœAÎ»2 by Eq. 7
4
Experiments and Discussion
4.1
Graph Classification
In this section, we study the properties and performance of CT-LAYER and GAP-LAYER in a graph
classification task with several benchmark datasets. To illustrate the merits of our approach, we
compare CT-LAYER and GAP-LAYER with 3 state-of-the-art diffusion and curvature-based graph
rewiring methods. Note that the aim of the evaluation is to shed light on the properties of both layers
and illustrate their inductive performance, not to perform a benchmark comparison with all previously
proposed graph rewiring methods.
LINEAR
CONV
MINCUT
CONV
READOUT
MLP
X
X
X
A
X 
à·¡A
X
X
à·¡Y
(a) MINCUT baseline
X
LINEAR
CONV
MINCUT
CONV
READOUT
MLP
X
X
A
X 
à·¡A
X
X
à·¡Y
REWIRING
T
(b) CT-LAYER or GAP-LAYER
Figure 4: GNN models used in the experiments. Left: MinCut Baseline model. Right: CT-LAYER
or GAP-LAYER models, depending on what method is used for rewiring.
Baselines:. The first baseline architecture is based on MINCUT Pool [33] and it is shown in Figure 4a.
It is the base GNN that we use for graph classification without rewiring. MINCUT Pool layer learns
(AnÃ—n, XnÃ—F ) â†’(Aâ€²kÃ—k, XkÃ—F ), being k < n the new number of node clusters. The first baseline
strategy using graph rewiring is k-NN graphs [47], where weights of the edges are computed based
on feature similarity. The next two baselines are graph rewiring methods that belong to the same
family of methods as DIFFWIRE, i.e. methods based on diffusion and curvature, namely DIGL
(PPR) [25] and SDRF [20]. DIGL is a diffusion-based preprocessing method within the family of
metric-based GSL approaches. We set the teleporting probability Î± = 0.001 and Ïµ is set to keep the
same average degree for each graph. Once preprocessed with DIGL, the graphs are provided as input
to the MinCut Pool (Baseline1) arquitecture. The third baseline model is SDRF, which performs
curvature-based rewiring. SDRF is also a preprocessing method which has 3 parameters that are
highly graph-dependent. We set these parameters to Ï„ = 20 and C+ = 0 for all experiments as per
[20]. The number of iterations is estimated dynamically according to 0.7 âˆ—|V | for each graph.
Both DIGL and SDRF aim to preserve the global topology of the graph but require optimizing their
parameters for each input graph via hyper-parameter search. In a graph classification task, this search
is O(n3) per graph. Details about the parameter tuning in these methods can be found in App. A.3.3.
To shed light on the performance and properties of CT-LAYER and GAP-LAYER, we add the
corresponding layer in between Linear(X)
âˆ—âˆ’â†’Conv1(A, X). We build 3 different models: CT-
LAYER, GAP-LAYER (Rcut), GAP-LAYER (Ncut), depending on the layer used. For CT-LAYER,
we learn TCT which is used as a convolution matrix afterwards. For GAP-LAYER, we learn TGAP
either using the Rcut or the Ncut approximations. A schematic of the architectures is shown in
Figure 4b and in App. A.3.2.
As shown in Table 1, we use in our experiments common benchmark datasets for graph classification.
We select datasets both with features and featureless, in which case we use the degree as the node
features. These datasets are diverse regarding the topology of their networks: REDDIT-B, IMDB-B
8

DiffWire: Inductive Graph Rewiring via the LovÃ¡sz Bound
Table 1: Experimental results on common graph classification benchmarks. Red denotes the best
model row-wise and Blue marks the runner-up. â€˜*â€™ means degree as node feature.
MinCutPool
k-NN
DIGL
SDRF
CT-LAYER
GAP-LAYER (R)
GAP-LAYER (N)
REDDIT-B*
66.53Â±4.4
64.40Â±3.8
76.02Â±4.3
65.3Â±7.7
78.45Â±4.5
77.63Â±4.9
76.00Â±5.3
IMDB-B*
60.75Â±7.0
55.20Â±4.3
59.35Â±7.7
59.2Â±6.9
69.84Â±4.6
69.93Â±3.3
68.80Â±3.1
COLLAB*
58.00Â±6.2
58.33Â±11
57.51Â±5.9
56.60Â±10
69.87Â±2.4
64.47Â±4.0
65.89Â±4.9
MUTAG
84.21Â±6.3
87.58Â±4.1
85.00Â±5.6
82.4Â±6.8
87.58Â±4.4
86.90Â±4.0
86.90Â±4.0
PROTEINS
74.84Â±2.3
76.76Â±2.5
74.49Â±2.8
74.4Â±2.7
75.38Â±2.9
75.03Â±3.0
75.34Â±2.1
SBM*
53.00Â±9.9
50.00Â±0.0
56.93Â±12
54.1Â±7.1
81.40Â±11
90.80Â±7.0
92.26Â±2.9
ErdÃ¶s-RÃ©nyi*
81.86Â±6.2
63.40Â±3.9
81.93Â±6.3
73.6Â±9.1
79.06Â±9.8
79.26Â±10
82.26Â±3.2
and COLLAB contain truncate scale-free graphs (social networks), whereas MUTAG and PROTEINS
contain graphs from biology or chemistry. In addition, we use two synthetic datasets with 2 classes:
ErdÃ¶s-RÃ©nyi with p1 âˆˆ[0.3, 0.5] and p2 âˆˆ[0.4, 0.8] and Stochastic block model (SBM) with
parameters p1 = 0.8, p2 = 0.5, q1 âˆˆ[0.1, 0.15] and q2 âˆˆ[0.01, 0.1]. More details about the datasets
in App. A.3.1. In addition, Table 1 reports average accuracies and standard deviation on 10 random
data splits, using 85/15 stratified train-test split, training during 60 epochs and reporting the results of
the last epoch for each random run. We use Pytorch Geometric [48] and the code is available in a
public repository2.
The experiments support our hypothesis that rewiring based on CT-LAYER and GAP-LAYER
improves the performance of the baselines on graph classification. Since both layers are differentiable,
they learn how to inductively rewire unseen graphs. The improvements are significant in graphs where
social components arise (REDDITB, IMDBB, COLLAB), i.e. graphs with small world properties and
power-law degree distributions with a topology based on hubs and authorities. These are graphs
where bottlenecks arise easily and our approach is able to properly rewire the graphs. However, the
improvements observed in planar or grid networks (MUTAG and PROTEINS) are more limited: the
bottleneck does not seem to be critical for the graph classification task.
Moreover, CT-LAYER and GAP-LAYER perform better in graphs with featureless nodes than graphs
with node features because it is able to leverage the information encoded in the topology of the
graphs. Note that in attribute-based graphs, the weights of the attributes typically overwrite the
graphâ€™s structure in the classification task, whereas in graphs without node features, the information
is encoded in the graphâ€™s structure. Thus, k-NN rewiring outperforms every other rewiring method in
graph classification where graphs has node features.
App. A.3.4 contains an in-depth analysis of the comparison between the spectral node CT embeddings
(CTEs) given by Equation 2, and the learned node CTEs as predicted by CT-LAYER. We find that
the CTEs that are learned in CT-LAYER are able to better preserve the original topology of the
graph while shifting the distribution of the effective resistances of the edges towards an asymmetric
distribution where few edges have very large weights and a majority of edges have low weights.
In addition, App. A.3.4 also includes the analysis of the graphs latent space of the readout layer
produced by each model. Finally, we analyze the performance of the proposed layers in graphs
with different structural properties in App. A.3.6. We analyze the correlation between accuracy, the
graphâ€™s assortativity, and the graphâ€™s bottleneck (Î»2).
CT-LAYER vs GAP-LAYER. The datasets explored in this paper are characterized by mild bottle-
necks from the perspective of the LovÃ¡sz bound. For completion, we have included two synthetic
datasets (Stochastic Block Model and ErdÃ¶s-RÃ©nyi) where the LovÃ¡sz bound is very restrictive.
As a result, CT-LAYER is outperformed by GAP-LAYER in SBM. Note that the results on the
synthetic datasets suffer from large variability. As a general rule of thumb, the smaller the graphâ€™s
bottleneck (defined as the ratio between the number of inter-community edges and the number
of intra-community edges), the more useful the CT-LAYER is because the rewired graph will be
sparsified in the communities but will preserve the edges in the gap. Conversely, the larger the
bottleneck, the more useful the GAP-Layer is.
2https://github.com/AdrianArnaiz/DiffWire
9

DiffWire: Inductive Graph Rewiring via the LovÃ¡sz Bound
4.2
Node Classification using CT-LAYER
CT-LAYER and GAP-LAYER are mainly designed to perform graph classification tasks. However,
we identify two potential areas to apply CT-LAYER for node classification.
First, the new TCT diffusion matrix learned by CT-LAYER gives more importance to edges that
connect different communities, i.e., edges that connect distant nodes in the graph. This behaviour
of CT-LAYER is aligned to solve long-range and heterophilic node classification tasks using fewer
number of layers and thus avoiding under-reaching, over-smoothing and over-squashing.
Second, there is an increasingly interest in the community in using PEs in the nodes to develope
more expressive GNN. PEs tend to help in node classification in homophilic graphs, as nearby nodes
will be assigned similar PEs. However, the main limitation is that PEs are usually pre-computed
before the GNN training due to their high computational cost. CT-LAYER provides a solution to this
problem, as it learns to predict the commute times embedding (Z) of a given graph (see Figure 2
and definition 1). Hence, CT-LAYER is able to learn and predict PEs from X and A inside a GNN
without needing to pre-compute them.
We empirically validate CT-LAYER in a node classification task on benchmark homophilic (Cora,
Pubmed and Citeseer) and heterophilic (Cornell, Actor and Wisconsin) graphs. The results are
depicted in Table 2 comparing three models: (1) the baseline model consists of a 1-layer-GCN; (2)
model 1 is a 1-layer-GCN where the CTEs are concatenated to the node features as PEs (X âˆ¥Z);
(3) Finally, model 2 is a 1-layer-GCN where TCT is used as a diffusion matrix (A = TCT). More
details can be found in App. A.3.5.
As seen in the Table, the proposed models outperform the baseline GCN model: using CTEs as
features (model 1) yields competitive results in homophilic graphs whereas using TCT as a matrix
for message passing (model 2) performs well in heterophilic graphs. Note that in our experiments
the CTEs are learned by CT-LAYER instead of being pre-computed. A promising direction of future
work would be to explore how to combine these two approaches (model 1 and model 2) to leverage
the best of each of the methods on a wide range of graphs for node classification tasks.
Table 2: Results in node classification
Dataset
GCN (baseline)
model 1:
model 2:
X âˆ¥Z
A = TCT
Homophily
Cora
82.01Â±0.8
83.66Â±0.6
67.96Â±0.8
81.0%
Pubmed
81.61Â±0.3
86.07Â±0.1
68.19Â±0.7
80.0%
Citeser
70.81Â±0.5
72.26Â±0.5
66.71Â±0.6
73.6%
Cornell
59.19Â±3.5
58.02Â±3.7
69.04Â±2.2
30.5%
Actor
29.59Â±0.4
29.35Â±0.4
31.98Â±0.3
21.9%
Wisconsin
68.05Â±6.2
69.25Â±5.1
79.05Â±2.1
19.6%
5
Conclusion and Future Work
In this paper, we have proposed DIFFWIRE, a unified framework for graph rewiring that links the
two components of the LovÃ¡sz bound: CTs and the spectral gap. We have presented two novel, fully
differentiable and inductive rewiring layers: CT-LAYER and GAP-LAYER. We have empirically
evaluated these layers on benchmark datasets for graph classification with competitive results when
compared to SoTA baselines, specially in graphs where the the nodes have no attributes and have
small-world properties. We have also performed preliminary experiments in a node classification
task, showing that using the CT Embeddings and the CT distances benefit GNN architectures in
homophilic and heterophilic graphs, respectively.
In future work, we plan to test the proposed approach in other graph-related tasks and intend to apply
DIFFWIRE to large-scale graphs and real-world applications, particularly in social networks, which
have unique topology, statistics and direct implications in society.
10

DiffWire: Inductive Graph Rewiring via the LovÃ¡sz Bound
6
Acknowledgments
A. Arnaiz-Rodriguez and N. Oliver are supported by a nominal grant received at the ELLIS Unit
Alicante Foundation from the Regional Government of Valencia in Spain (Convenio Singular signed
with Generalitat Valenciana, Conselleria dâ€™InnovaciÃ³, Universitats, CiÃ¨ncia i Societat Digital, Direc-
ciÃ³n General para el Avance de la Sociedad Digital). A. Arnaiz-Rodriguez is also funded by a grant
by the Banc Sabadell Foundation. F. Escolano is funded by the project RTI2018-096223-B-I00 of the
Spanish Government.
References
[1] Marco Gori, Gabriele Monfardini, and Franco Scarselli. A new model for learning in graph domains. In
Proceedings. 2005 IEEE international joint conference on neural networks, volume 2, pages 729â€“734,
2005. URL https://ieeexplore.ieee.org/document/1555942. 1
[2] Franco Scarselli, Marco Gori, Ah Chung Tsoi, Markus Hagenbuchner, and Gabriele Monfardini. The
graph neural network model. IEEE transactions on neural networks, 20(1):61â€“80, 2008. URL https:
//ieeexplore.ieee.org/document/4700287. 1
[3] Thomas N. Kipf and Max Welling. Semi-supervised classification with graph convolutional networks. In
International Conference on Learning Representations (ICLR), 2017. URL https://openreview.net/forum?
id=SJU4ayYgl. 1
[4] Justin Gilmer, Samuel S. Schoenholz, Patrick F. Riley, Oriol Vinyals, and George E. Dahl. Neural message
passing for quantum chemistry. In Proceedings of the 34th International Conference on Machine Learning,
ICML, page 1263â€“1272, 2017. 1
[5] Thomas N Kipf and Max Welling. Variational graph auto-encoders. In NeurIPS Workshop on Bayesian
Deep Learning, 2016. URL http://bayesiandeeplearning.org/2016/papers/BDL_16.pdf. 1
[6] Shaosheng Cao, Wei Lu, and Qiongkai Xu. Deep neural networks for learning graph representations. In
Proceedings of the AAAI Conference on Artificial Intelligence, volume 30, 2016. URL https://ojs.aaai.
org/index.php/AAAI/article/view/10179. 1
[7] Fei Tian, Bin Gao, Qing Cui, Enhong Chen, and Tie-Yan Liu. Learning deep representations for graph
clustering. In Proceedings of the AAAI Conference on Artificial Intelligence, 2014. URL https://ojs.aaai.
org/index.php/AAAI/article/view/8916. 1
[8] Zonghan Wu, Shirui Pan, Fengwen Chen, Guodong Long, Chengqi Zhang, and Philip S. Yu. A comprehen-
sive survey on graph neural networks. IEEE Transactions on Neural Networks and Learning Systems, 32
(1):4â€“24, 2021. URL https://ieeexplore.ieee.org/document/9046288. 1, 2
[9] Petar VeliË‡ckoviÂ´c, Guillem Cucurull, Arantxa Casanova, Adriana Romero, Pietro LiÃ², and Yoshua Bengio.
Graph Attention Networks. International Conference on Learning Representations, 2018. URL https:
//openreview.net/forum?id=rJXMpikCZ. 1
[10] Shaked Brody, Uri Alon, and Eran Yahav. How attentive are graph attention networks? In International
Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=F72ximsx7C1. 1
[11] Keyulu Xu, Weihua Hu, Jure Leskovec, and Stefanie Jegelka. How powerful are graph neural networks?
In International Conference on Learning Representations, 2019. URL https://openreview.net/forum?id=
ryGs6iA5Km. 1
[12] Will Hamilton, Zhitao Ying, and Jure Leskovec. Inductive representation learning on large graphs. In
Advances in Neural Information Processing Systems, 2017. URL https://proceedings.neurips.cc/paper/
2017/file/5dd9db5e033da9c6fb5ba83c7a7ebea9-Paper.pdf. 1, 3
[13] Qimai Li, Zhichao Han, and Xiao-Ming Wu. Deeper insights into graph convolutional networks for
semi-supervised learning. In Proceedings of the Thirty-Second AAAI Conference on Artificial Intelligence,
2018. URL https://ojs.aaai.org/index.php/AAAI/article/view/11604. 2
[14] Uri Alon and Eran Yahav. On the bottleneck of graph neural networks and its practical implications.
In International Conference on Learning Representations, 2021. URL https://openreview.net/forum?id=
i80OPhOCVH2. 2
[15] LÃ¡szlÃ³ LovÃ¡sz. Random walks on graphs. Combinatorics, Paul erdos is eighty, 2(1-46):4, 1993. URL
https://web.cs.elte.hu/~lovasz/erdos.pdf. 2, 4
[16] Pablo BarcelÃ³, Egor V. Kostylev, Mikael Monet, Jorge PÃ©rez, Juan Reutter, and Juan Pablo Silva. The
logical expressiveness of graph neural networks. In International Conference on Learning Representations,
2020. URL https://openreview.net/forum?id=r1lZ7AEKvB. 2
11

DiffWire: Inductive Graph Rewiring via the LovÃ¡sz Bound
[17] NT Hoang, Takanori Maehara, and Tsuyoshi Murata. Revisiting graph neural networks: Graph filtering
perspective. In 25th International Conference on Pattern Recognition (ICPR), pages 8376â€“8383, 2021.
URL https://ieeexplore.ieee.org/document/9412278. 2, 7, 19
[18] Kenta Oono and Taiji Suzuki. Graph neural networks exponentially lose expressive power for node
classification. In International Conference on Learning Representations, 2020. URL https://openreview.
net/forum?id=S1ldO2EFPr.
[19] Jie Zhou, Ganqu Cui, Zhengyan Zhang, Cheng Yang, Zhiyuan Liu, and Maosong Sun. Graph neural
networks: A review of methods and applications. CoRR, abs/1812.08434, 2018. URL http://arxiv.org/
abs/1812.08434. 2
[20] Jake Topping, Francesco Di Giovanni, Benjamin Paul Chamberlain, Xiaowen Dong, and Michael M.
Bronstein. Understanding over-squashing and bottlenecks on graphs via curvature. In International
Conference on Learning Representations, 2022. URL https://openreview.net/forum?id=7UmjRGzp-A. 2, 3, 6,
8, 18, 23
[21] Petar VeliË‡ckoviÂ´c. Message passing all the way up. In ICLR 2022 Workshop on Geometrical and Topological
Representation Learning, 2022. URL https://openreview.net/forum?id=Bc8GiEZkTe5. 2
[22] Yu Rong, Wenbing Huang, Tingyang Xu, and Junzhou Huang. Dropedge: Towards deep graph convolu-
tional networks on node classification. In International Conference on Learning Representations, 2020.
URL https://openreview.net/forum?id=Hkx1qkrKPr. 2, 3
[23] Anees Kazi, Luca Cosmo, Seyed-Ahmad Ahmadi, Nassir Navab, and Michael Bronstein. Differentiable
graph module (dgm) for graph convolutional networks. IEEE Transactions on Pattern Analysis and
Machine Intelligence, pages 1â€“1, 2022. URL https://ieeexplore.ieee.org/document/9763421. 2, 3
[24] Karel Devriendt and Renaud Lambiotte. Discrete curvature on graphs from the effective resistance. arXiv
preprint arXiv:2201.06385, 2022. doi: 10.48550/ARXIV.2201.06385. URL https://arxiv.org/abs/2201.
06385. 2, 6, 7, 18
[25] Johannes Klicpera, Stefan WeiÃŸenberger, and Stephan GÃ¼nnemann. Diffusion improves graph learning. In
Advances in Neural Information Processing Systems, 2019. URL https://proceedings.neurips.cc/paper/
2019/file/23c894276a2c5a16470e6a31f4618d73-Paper.pdf. 3, 8, 23
[26] Peter W Battaglia, Jessica B Hamrick, Victor Bapst, Alvaro Sanchez-Gonzalez, Vinicius Zambaldi,
Mateusz Malinowski, Andrea Tacchetti, David Raposo, Adam Santoro, Ryan Faulkner, et al. Relational
inductive biases, deep learning, and graph networks. arXiv preprint arXiv:1806.01261, 2018. URL
https://arxiv.org/abs/1806.01261. 3
[27] Fabrizio Frasca, Emanuele Rossi, Davide Eynard, Benjamin Chamberlain, Michael Bronstein, and Federico
Monti. Sign: Scalable inception graph neural networks. In ICML 2020 Workshop on Graph Representation
Learning and Beyond, 2020. URL https://grlplus.github.io/papers/77.pdf. 3
[28] PÃ¡l AndrÃ¡s Papp, Karolis Martinkus, Lukas Faber, and Roger Wattenhofer. DropGNN: Random dropouts
increase the expressiveness of graph neural networks. In Advances in Neural Information Processing
Systems, 2021. URL https://openreview.net/forum?id=fpQojkIV5q8. 3
[29] Deli Chen, Yankai Lin, Wei Li, Peng Li, Jie Zhou, and Xu Sun. Measuring and relieving the over-
smoothing problem for graph neural networks from the topological view. Proceedings of the AAAI
Conference on Artificial Intelligence, 34(04):3438â€“3445, Apr. 2020. doi: 10.1609/aaai.v34i04.5747. URL
https://ojs.aaai.org/index.php/AAAI/article/view/5747. 3
[30] Yanqiao Zhu, Weizhi Xu, Jinghao Zhang, Yuanqi Du, Jieyu Zhang, Qiang Liu, Carl Yang, and Shu
Wu. A survey on graph structure learning: Progress and opportunities. arXiv PrePrint, 2021. URL
https://arxiv.org/abs/2103.03036. 3
[31] Diego Mesquita, Amauri Souza, and Samuel Kaski. Rethinking pooling in graph neural networks. In
Advances in Neural Information Processing Systems, 2020. URL https://proceedings.neurips.cc/paper/
2020/file/1764183ef03fc7324eb58c3842bd9a57-Paper.pdf. 3
[32] Zhitao Ying, Jiaxuan You, Christopher Morris, Xiang Ren, Will Hamilton, and Jure Leskovec.
Hierarchical graph representation learning with differentiable pooling.
In Advances in Neu-
ral Information Processing Systems, 2018.
URL https://proceedings.neurips.cc/paper/2018/file/
e77dbaf6759253c7c6d0efc5690369c7-Paper.pdf. 3
[33] Filippo Maria Bianchi, Daniele Grattarola, and Cesare Alippi. Spectral clustering with graph neural
networks for graph pooling. In Proceedings of the 37th International Conference on Machine Learning,
2020. URL https://proceedings.mlr.press/v119/bianchi20a.html. 3, 8
[34] Ladislav RampÃ¡Å¡ek, Mikhail Galkin, Vijay Prakash Dwivedi, Anh Tuan Luu, Guy Wolf, and Dominique
Beaini. Recipe for a General, Powerful, Scalable Graph Transformer. arXiv:2205.12454, 2022. URL
https://arxiv.org/pdf/2205.12454.pdf. 3
12

DiffWire: Inductive Graph Rewiring via the LovÃ¡sz Bound
[35] Ameya Velingker, Ali Kemal Sinop, Ira Ktena, Petar VeliË‡ckoviÂ´c, and Sreenivas Gollapudi. Affinity-aware
graph networks. arXiv preprint arXiv:2206.11941, 2022. URL https://arxiv.org/pdf/2206.11941.pdf. 3,
23, 25
[36] Vijay Prakash Dwivedi and Xavier Bresson. A generalization of transformer networks to graphs. AAAI
Workshop on Deep Learning on Graphs: Methods and Applications, 2021. URL https://arxiv.org/pdf/
2012.09699.pdf. 3, 23
[37] Derek Lim, Joshua David Robinson, Lingxiao Zhao, Tess Smidt, Suvrit Sra, Haggai Maron, and Stefanie
Jegelka. Sign and basis invariant networks for spectral graph representation learning. In ICLR 2022
Workshop on Geometrical and Topological Representation Learning, 2022. URL https://openreview.net/
forum?id=BlM64by6gc. 3
[38] Pan Li, Yanbang Wang, Hongwei Wang, and Jure Leskovec.
Distance encoding: Design prov-
ably more powerful neural networks for graph representation learning.
Advances in Neural
Information Processing Systems, 33, 2020.
URL https://proceedings.neurips.cc/paper/2020/file/
2f73168bf3656f697507752ec592c437-Paper.pdf. 3
[39] Fan RK Chung. Spectral Graph Theory. American Mathematical Society, 1997. URL https://www.
bibsonomy.org/bibtex/295ef10b5a69a03d8507240b6cf410f8a/folke. 4
[40] Ulrike von Luxburg, Agnes Radl, and Matthias Hein. Hitting and commute times in large random
neighborhood graphs. Journal of Machine Learning Research, 15(52):1751â€“1798, 2014. URL http:
//jmlr.org/papers/v15/vonluxburg14a.html. 4, 20
[41] Daniel A. Spielman and Nikhil Srivastava. Graph sparsification by effective resistances. SIAM Journal on
Computing, 40(6):1913â€“1926, 2011. doi: 10.1137/080734029. URL https://doi.org/10.1137/080734029. 5
[42] Huaijun Qiu and Edwin R. Hancock. Clustering and embedding using commute times. IEEE Transactions
on Pattern Analysis and Machine Intelligence, 29(11):1873â€“1890, 2007. doi: 10.1109/TPAMI.2007.1103.
URL https://ieeexplore.ieee.org/document/4302755. 6
[43] Vedat Levi Alev, Nima Anari, Lap Chi Lau, and Shayan Oveis Gharan. Graph Clustering using Effective
Resistance. In 9th Innovations in Theoretical Computer Science Conference (ITCS 2018), volume 94, pages
1â€“16, 2018. doi: 10.4230/LIPIcs.ITCS.2018.41. URL http://drops.dagstuhl.de/opus/volltexte/2018/8369.
6, 17, 24
[44] Emmanuel Abbe. Community detection and stochastic block models: Recent developments. Journal of
Machine Learning Research, 18(177):1â€“86, 2018. URL http://jmlr.org/papers/v18/16-480.html. 7, 20
[45] Thomas BÃ¼hler and Matthias Hein. Spectral clustering based on the graph p-laplacian. In Proceedings of the
26th Annual International Conference on Machine Learning, ICML â€™09, page 81â€“88, New York, NY, USA,
2009. Association for Computing Machinery. ISBN 9781605585161. doi: 10.1145/1553374.1553385.
URL https://doi.org/10.1145/1553374.1553385. 7, 20
[46] Jian Kang and Hanghang Tong. N2n: Network derivative mining. In Proceedings of the 28th ACM
International Conference on Information and Knowledge Management, CIKM â€™19, page 861â€“870, New
York, NY, USA, 2019. Association for Computing Machinery. ISBN 9781450369763. doi: 10.1145/
3357384.3357910. URL https://doi.org/10.1145/3357384.3357910. 7, 19
[47] Franco P Preparata and Michael I Shamos. Computational geometry: an introduction. Springer Science &
Business Media, 2012. URL http://www.cs.kent.edu/~dragan/CG/CG-Book.pdf. 8
[48] Matthias Fey and Jan E. Lenssen. Fast graph representation learning with PyTorch Geometric. In ICLR
Workshop on Representation Learning on Graphs and Manifolds, 2019. 9
[49] Joshua Batson, Daniel A. Spielman, Nikhil Srivastava, and Shang-Hua Teng. Spectral sparsification
of graphs: Theory and algorithms. Commun. ACM, 56(8):87â€“94, aug 2013. ISSN 0001-0782. doi:
10.1145/2492007.2492029. URL https://doi.org/10.1145/2492007.2492029. 16
[50] Morteza Alamgir and Ulrike Luxburg. Phase transition in the family of p-resistances. In Advances
in Neural Information Processing Systems, 2011. URL https://proceedings.neurips.cc/paper/2011/file/
07cdfd23373b17c6b337251c22b7ea57-Paper.pdf. 20
[51] Morteza Alamgir and Ulrike Luxburg. Phase transition in the family of p-resistances. In Advances in
Neural Information Processing Systems, volume 24, 2011. URL https://proceedings.neurips.cc/paper/
2011/file/07cdfd23373b17c6b337251c22b7ea57-Paper.pdf. 20
[52] Gregory Berkolaiko, James B Kennedy, Pavel Kurasov, and Delio Mugnolo. Edge connectivity and the
spectral gap of combinatorial and quantum graphs. Journal of Physics A: Mathematical and Theoretical,
50(36):365201, 2017. URL https://doi.org/10.1088/1751-8121/aa8125. 20
[53] Zoran StaniÂ´c. Graphs with small spectral gap. Electronic Journal of Linear Algebra, 26:28, 2013. URL
https://journals.uwyo.edu/index.php/ela/article/view/1259. 20
[54] Douglas J Klein and Milan RandiÂ´c. Resistance distance. Journal of Mathematical Chemistry, 12(1):81â€“95,
1993. URL https://doi.org/10.1007/BF01164627. 24
13

DiffWire: Inductive Graph Rewiring via the LovÃ¡sz Bound
A
Appendix
In Appendix A we include a Table with the notation used in the paper and we provide an analysis of
the diffusion and its relationship with curvature. In Appendix B, we study in detail GAP-LAYER and
the implications of the proposed spectral gradients. Appendix C reports statistics and characteristics of
the datasets used in the experimental section, provides more information about the experiments results,
describes additional experimental results, and includes a summary of the computing infrastructure
used in our experiments.
Table 3: Notation.
Symbol
Description
G = (V, E)
Graph = (Nodes, Edges)
A
Adjacency matrix: A âˆˆRnÃ—n
X
Feature matrix: X âˆˆRnÃ—F
v
Node v âˆˆV or u âˆˆV
e
Edge e âˆˆE
x
Features of node v: x âˆˆX
n
Number of nodes: n = |V |
F
Number of features
D
Degree diagonal matrix where dv in Dvv
dv
Degree of node v
vol(G)
Sum of the degrees of the graph vol(G) = Tr[D]
L
Laplacian: L = D âˆ’A
B
Signed edge-vertex incidence matrix
be
Incidence vector: Row vector of B, with be=(u,v) = (eu âˆ’ev)
ve
Projected incidence vector: ve = L+/2be
Î“
Ratio Î“ = 1+Ïµ
1âˆ’Ïµ
E
Dirichlet Energy wrt L: E(x) := xT Lx
L
Normalized Laplacian: L = I âˆ’Dâˆ’1/2ADâˆ’1/2
Î›
Eigenvalue matrix of L
Î›â€²
Eigenvalue matrix of L
Î»i
i-th eigenvalue of L
Î»2
Second eigenvalue of L: Spectral gap
Î»â€²
i
i-th eigenvalue of L
Î»â€²
2
Second eigenvalue of L: Spectral gap
F
Matrix of eigenvectors of L
G
Matrix of eigenvectors of L
fi
i eigenvector of L
f2
Second eigenvector of L: Fiedler vector
gi
i eigenvector of L
g2
Second eigenvector of L: Fiedler vector
ËœA
New Adjacency matrix
Eâ€²
New edges
Huv
Hitting time between u and v
CTuv
Commute time: CTuv = Huv + Hvu
Ruv
Effective resistance: Ruv = CTuv/vol(G)
Z
Matrix of commute times embeddings for all nodes in G
zu
Commute times embedding of node u
TCT
Resistance diffusion or commute times diffusion
R(Z)
Pairwise Euclidean distance of embedding Z divided by vol(G)
S
Cluster assignment matrix: S âˆˆRnÃ—2
TGAP
GAP diffusion
eu
Unit vector with unit value at u and 0 elsewhere
âˆ‡Ëœ
AÎ»2
Gradient of Î»2 wrt ËœA
[âˆ‡Ëœ
AÎ»2]ij
Gradient of Î»2 wrt ËœAuv
pu
Node curvature: pu := 1 âˆ’1
2
P
uâˆ¼w Ruv
Îºuv
Edge curvature: Îºuv := 2(pu + pv)/Ruv
âˆ¥
Concatenation
14

DiffWire: Inductive Graph Rewiring via the LovÃ¡sz Bound
A.1
Appendix A: CT-LAYER
A.1.1
Notation
The Table 3 summarizes the notation used in the paper.
A.1.2
Analysis of Commute Times rewiring
First, we provide an answer to the following question:
Is resistance diffusion via TCT a principled way of preserving the Cheeger constant?
We answer the question above by linking Theorems 1 and 2 in the paper with the LovÃ¡sz bound.
The outline of our explanation follows three steps.
â€¢ Proposition 1: Theorem 1 (Sparsification) provides a principled way to bias the adjacency
matrix so that the edges with the largest weights in the rewired graph correspond to the edges in
graphâ€™s bottleneck.
â€¢ Proposition 2: Theorem 2 (Cheeger vs Resistance) can be used to demonstrate that increasing
the effective resistance leads to a mild reduction of the Cheeger constant.
â€¢ Proposition 3: (Conclusion) The effectiveness of the above theorems to contain the Cheeger
constant is constrained by the LovÃ¡sz bound.
Next, we provide a thorough explanation of each of the propositions above.
Proposition 1 (Biasing). Let Gâ€™ = Sparsify(G, q) be a sampling algorithm of graph G = (V, E),
where edges e âˆˆE are sampled with probability q âˆRe (proportional to the effective resistance).
This choice is necessary to retain the global structure of G, i.e., to satisfy
âˆ€x âˆˆRn : (1 âˆ’Ïµ)xT LGx â‰¤xT LGâ€²x â‰¤(1 + Ïµ)xT LGx ,
(9)
with probability at least 1/2 by sampling O(n log n/Ïµ2) edges , with 1/âˆšn < Ïµ â‰¤1, instead of
O(m), where m = |E|. In addition, this choice biases the uniform distribution in favor of critical
edges in the graph.
Proof. We start by expressing the Laplacian L in terms of the edge-vertex incidence matrix BmÃ—e:
Beu =
(
1
if u is the head of e
âˆ’1
if u is the tail of e
0
otherwise .
(10)
where edges in undirected graphs are counted once, i.e. e = (u, v) = (v, u). Then, we have
L = BT B = P
e bebT
e , where be is a row vector (incidence vector) of B, with be=(u,v) = (euâˆ’ev).
In addition, the Dirichlet energies can be expressed as norms:
E(x) = xT Lx = xT BT Bx = âˆ¥Bxâˆ¥2
2 =
X
e=(u,v)âˆˆE
(xu âˆ’xv)2 .
(11)
As a result, the effective resistance Re between the two nodes of an edge e = (u, v) can be defined as
Re = (eu âˆ’ev)T L+(eu âˆ’ev) = bT
e L+be
(12)
Next, we reformulate the spectral constraints in Eq. 9, i.e. (1 âˆ’Ïµ)LG â‰¼LGâ€² â‰¼(1 + Ïµ)LG as
LG â‰¼LGâ€² â‰¼Î“LG , Î“ = 1 + Ïµ
1 âˆ’Ïµ .
(13)
This simplifies the analysis, since the above expression can be interpreted as follows: the Dirichlet
energies of LGâ€² are lower-bounded by those of LG and upper-bounded by Î“ times the energies of
LG. Considering that the energies define hyper-ellipsoids, the hyper-ellipsoid associated with LGâ€² is
between the hyper-ellipsoids of LG and Î“ times the LG.
The hyper-ellipsoid analogy provides a framework to proof that the inclusion relationships are
preserved under scaling: MLGM â‰¼MLGâ€²M â‰¼MÎ“LGM where M can be a matrix. In this case,
if we set M := (L+
G)1/2 = L+/2
G
we have:
L+/2
G
LGL+/2
G
â‰¼L+/2
G
LGâ€²L+/2
G
â‰¼L+/2
G
Î“L+/2
G
,
(14)
15

DiffWire: Inductive Graph Rewiring via the LovÃ¡sz Bound
which leads to
In â‰¼L+/2
G
LGâ€²L+/2
G
â‰¼Î“In .
(15)
We seek a Laplacian LGâ€² satisfying the similarity constraints in Eq. 13. Since Eâ€² âŠ‚E, i.e. we want
to remove structurally irrelevant edges, we can design LGâ€² in terms of considering all the edges E:
LGâ€² := BT
GBG =
X
e
sebebT
e
(16)
and let the similarity constraint define the sampling weights and the choice of e (setting se â‰¥0
propertly). More precisely:
In â‰¼L+/2
G
X
e
bebT
e L+/2
G
â‰¼Î“In .
(17)
Then if we define ve := L+/2
G
be as the projected incidence vector, we have
In â‰¼
X
e
sevevT
e â‰¼Î“In .
(18)
Consequently, a spectral sparsifier must find se â‰¥0 so that the above similarity constraint is satisfied.
Since there are m edges in E, se must be zero for most of the edges. But, what are the best candidates
to retain? Interestingly, the similarity constraint provides the answer. From Eq. 12 we have
vT
e ve = âˆ¥veâˆ¥2 = âˆ¥L+/2
G
beâˆ¥2
2 = bT
e L+
Gbe = Re .
(19)
This result explains why sampling the edges with probability q âˆRe leads to a ranking of m edges
of G = (V, E) such that edges with large Re = âˆ¥veâˆ¥2 are preferred3.
Algorithm 1 implements a deterministic greedy version of Sparsify(G, q), where we build incremen-
tally Eâ€² âŠ‚E by creating a budget of decreasing resistances Re1 â‰¥Re2 â‰¥. . . â‰¥ReO(n log n/Ïµ2).
Note that this rewiring strategy preserves the spectral similarities of the graphs, i.e. the global
structure of G = (V, E) is captured by Gâ€² = (V, Eâ€²).
Moreover, the maximum Re in each graph determines an upper bound on the Cheeger constant and
hence an upper bound on the size of the graphâ€™s bottleneck, as per the following proposition.
Algorithm 1: GREEDYSparsify
Input
:G = (V, E),Ïµ âˆˆ(1/âˆšn, 1], n = |V | .
Output :Gâ€² = (V, Eâ€²) with Eâ€² âŠ‚E such that |Eâ€²| = O(n log n/Ïµ2).
L â†List({ve : e âˆˆE})
Q â†Sort(L, descending, criterion=âˆ¥veâˆ¥2)
â–·Sort candidate edges by descending Resistance
Eâ€² â†âˆ…
I â†0nÃ—n
repeat
ve â†pop(Q)
â–·Remove the head of the queue
I â†I + vevT
e
if I â‰¼Î“In then
Eâ€² â†Eâ€² âˆª{e}
â–·Update the current budget of edges
else
return Gâ€² = (V, Eâ€²)
until Q = âˆ…
Proposition 2 (Resistance Diameter). Let Gâ€™ = Sparsify(G, q) be a sampling algorithm of graph
G = (V, E), where edges e âˆˆE are sampled with probability q âˆRe (proportional to the effective
resistance). Consider the resistance diameter Rdiam := maxu,v Ruv. Then, for the pair of (u, v)
3Although some of the elements of this section are derived from [49], we note that the Nikhil Srivastavaâ€™s
lectures at The Simons Institute (2014) are by far more clarifying.
16

DiffWire: Inductive Graph Rewiring via the LovÃ¡sz Bound
does exist an edge e = (u, v) âˆˆEâ€² in Gâ€² = (V, Eâ€²) such that Re = Rdiam. A a result the Cheeger
constant of G hG is upper-bounded as follows:
hG â‰¤
Î±Ïµ
âˆšRdiam Â· Ïµvol(S)Ïµâˆ’1/2,
(20)
with 0 < Ïµ < 1/2 and du â‰¥1/Î± for all u âˆˆV .
Proof. The fact that the maximum resistance Rdiam is located in an edge is derived from two
observations: a) Resistance is upper bounded by the shortest-path distance; and b) edges with
maximal resistance are prioritized in (Proposition 1).
Theorem 2 states that any attempt to increase the graphâ€™s bottleneck in a multiplicative way (i.e.
multiplying it by a constant c â‰¥0) results in decreasing the effective resistances as follows:
Ruv â‰¤
 1
d2Ïµ
u
+ 1
d2Ïµ
v

Â·
1
Ïµ Â· c2
(21)
with Ïµ âˆˆ[0, 1/2]. This equation is called the resistance bound. Therefore, a multiplicative increase of
the bottleneck leads to a quadratic decrease of the resistances.
Following Corollary 2 of [43], we obtain an upper bound of any hS, i.e. the Cheeger constant for
S âŠ†V with vol(S) â‰¤vol(G)/2 â€“ by defining c properly. In particular we are seeking a value of c
that would lead to a contradiction, which is obtained by setting
c =
v
u
u
t

1
d2Ïµ
uâˆ—+
1
d2Ïµ
vâˆ—

Rdiam Â· Ïµ
,
(22)
where (uâˆ—, vâˆ—) is a pair of nodes with maximal resistance, i.e. Ruâˆ—vâˆ—= Rdiam.
Consider now any other pair of nodes (s, t) with Rst < Rdiam. Following Theorem 2, if the
bottleneck of hS is multiplied by c, we should have
Rst â‰¤
 1
d2Ïµ
s
+ 1
d2Ïµ
s

Â·
1
Ïµ Â· c2 =
 1
d2Ïµ
s
+ 1
d2Ïµ
s

Â·
Rdiam

1
d2Ïµ
uâˆ—+
1
d2Ïµ
vâˆ—
 .
(23)
However, since Rdiam â‰¤

1
d2Ïµ
uâˆ—+
1
d2Ïµ
vâˆ—

we have that Rst can satisfy
Rst >
 1
d2Ïµ
s
+ 1
d2Ïµ
s

Â·
1
Ïµ Â· c2
(24)
which is a contradiction and enables
hS â‰¤
c
vol(S)1/2âˆ’Ïµ â‡â‡’|âˆ‚S| â‰¤c Â· vol(S)1/2âˆ’Ïµ.
(25)
Using c as defined in Eq. 22 and du â‰¥1/Î± we obtain
c =
v
u
u
t

1
d2Ïµ
uâˆ—+
1
d2Ïµ
vâˆ—

Rdiam Â· Ïµ
â‰¤
r
Î±Ïµ
Rdiam Â· Ïµ â‰¤
Î±Ïµ
âˆšRdiam Â· Ïµ .
(26)
Therefore,
hS â‰¤
c
vol(S)1/2âˆ’Ïµ â‰¤
Î±Ïµ
âˆšRdiamÂ·Ïµ
vol(S)1/2âˆ’Ïµ =
Î±Ïµ
âˆšRdiam Â· Ïµ Â· vol(S)Ïµâˆ’1/2.
(27)
As a result, the Cheeger constant of G = (V, E) is mildly reduced (by the square root of the maximal
resistance).
Proposition 3 (Conclusion). Let (uâˆ—, vâˆ—) be a pair of nodes (may be not unique) in G = (V, E)
with maximal resistance, i.e. Ruâˆ—vâˆ—= Rdiam. Then, the Cheeger constant hG relies on the ratio
between the maximal resistance Rdiam and its uninformative approximation

1
dâˆ—u +
1
dâˆ—v

. The closer
this ratio is to the unit, the easier it is to contain the Cheeger constant.
17

DiffWire: Inductive Graph Rewiring via the LovÃ¡sz Bound
Figure 5: Left: Original graph with nodes colored as Louvain communities. Middle: TCT learnt by
CT-LAYER with edges colors as node importance [0,1]. Right: Node and edge curvature: TCT using
pu := 1 âˆ’1
2
P
uâˆ¼w TCT
uv and Îºuv := 2(pu + pv)/TCT
uv
with edge an node curvatures as color. Graph from Reddit-B dataset.
Proof. The referred ratio above is the ratio leading to a proper c in Proposition 2. This is consistent
with a LovÃ¡sz regime where the spectral gap Î»â€²
2 has a moderate value. However, for regimes with
very small spectral gaps, i.e. Î»â€²
2 â†’0, according to the LovÃ¡sz bound, Rdiam â‰«

1
dâˆ—u +
1
dâˆ—v

and
hence the Cheeger constant provided by Proposition 2 will tend to zero.
We conclude that we can always find an moderate upper bound for the Cheeger constant of G =
(V, E), provided that the regime of the LovÃ¡sz bound is also moderate. Therefore, as the global
properties of G = (V, E) are captured by Gâ€² = (V, Eâ€²), a moderate Cheeger constant, when
achievable, also controls the bottlenecks in Gâ€² = (V, Eâ€²).
Our methodology has focused on first exploring the properties of the commute times / effective
resistances in G = (V, E). Next, we have leveraged the spectral similarity to reason about the
properties â€“particularly the Cheeger constantâ€“ of G = (V, Eâ€²). In sum, we conclude that resistance
diffusion via TCT is a principled way of preserving the Cheeger constant of G = (V, E).
A.1.3
Resistance-based Curvatures
We refer to recent work by Devriendt and Lambiotte [24] to complement the contributions of Topping
et al. [20] regarding the use of curvature to rewire the edges in a graph.
Theorem 3 (Devriendt and Lambiotte [24]). The edge resistance curvature has the following prop-
erties: (1) It is bounded by (4 âˆ’du âˆ’dv) â‰¤Îºuv â‰¤2/Ruv, with equality in the lower bound iff
all incident edges to u and v are cut links; (2) It is upper-bounded by the Ollivier-Ricci curvature
ÎºOR
uv
â‰¥Îºuv, with equality if (u, v) is a cut link; and (3) Forman-Ricci curvature is bounded as
follows: ÎºF R
uv /Ruv â‰¤Îºuv with equality in the bound if the edge is a cut link.
The new definition of curvature given in [20] is related to the resistance distance and thus it is
learnable with the proposed framework (CT-LAYER). Actually, the Balanced-Forman curvature
(Definition 1 in [20]) relies on the uniformative approximation of the resistance distance.
Figure 5 illustrates the relationship between effective resistances / commute times and curvature on
an exemplary graph from the COLLAB dataset.
As seen in the Figure, effective resistances prioritize the edges connecting outer nodes with hubs
or central nodes, while the intra-community connections are de-prioritized. This observation is
consistent with the aforementioned theoretical explanations about preserving the bottleneck while
breaking the intra-cluster structure. In addition, we also observe that the original edges between hubs
have been deleted o have been extremely down-weighted.
18

DiffWire: Inductive Graph Rewiring via the LovÃ¡sz Bound
Regarding curvature, hubs or central nodes have the lowest node curvature (this curvature increases
with the number of nodes in a cluster/community). Edge curvatures, which rely on node curvatures,
depend on the long-term neighborhoods of the connecting nodes. In general, edge curvatures can be
seen as a smoothed version â€“since they integrate node curvaturesâ€“ of the inverse of the resistance
distances.
We observe that edges linking nodes of a given community with hubs tend to have similar edge-
curvature values. However, edges linking nodes of different communities with hubs have different
edge curvatures (Figure 5-right). This is due to the different number of nodes belonging to each
community, and to their different average degree inside their respective communities (property 1 of
Theorem 3).
Finally, note that the range of edge curvatures is larger than that of resistance distances. The sparsifier
transforms a uniform distribution of the edge weights into a less entropic one: in the example of
Figure 5 we observe a power-law distribution of edge resistances. As a result, Îºuv := 2(pu+pv)/TCT
uv
becomes very large on average (edges with infinite curvature are not shown in the plot) and a log
scale is needed to appreciate the differences between edge resistances and edge curvatures.
A.2
Appendix B: GAP-LAYER
A.2.1
Spectral Gradients
The proposed GAP-LAYER relies on gradients wrt the Laplacian eigenvalues, and particularly the
spectral gap (Î»2 for L and Î»â€²
2 wrt L). Although the GAP-LAYER inductively rewires the adjacency
matrix A so that Î»2 is minimized, the gradients derived in this section may also be applied for gap
maximization.
Note that while our cost function LF iedler = âˆ¥ËœA âˆ’Aâˆ¥F + Î±(Î»âˆ—
2)2, with Î»âˆ—
2 âˆˆ{Î»2, Î»â€²
2}, relies on
an eigenvalue, we do not compute it explicitly, as its computation has a complexity of O(n3) and
would need to be computed in every learning iteration. Instead, we learn an approximation of Î»2â€™s
eigenvector f2 and use its Dirchlet energy E(f2) to approximate the eigenvalue. In addition, since
g2 = D1/2f2, we first approximate g2 and then approximate Î»â€²
2 from E(g2).
Gradients of the Ratio-cut Approximation.
Let A be the adjacency matrix of G = (V, E); and
ËœA, a matrix similar to the original adjacency but with minimal Î»2. Then, the gradient of Î»2 wrt each
component of ËœA is given by
âˆ‡ËœAÎ»2 := Tr
h
(âˆ‡ËœLÎ»2)T Â· âˆ‡ËœAËœL
i
= diag(f2f T
2 )11T âˆ’f2f T
2 ,
(28)
where 1 is the vector of n ones; and [âˆ‡ËœAÎ»2]ij is the gradient of Î»2 wrt ËœAuv. The above formula is
an instance of the network derivative mining mining approach [46]. In this framework, Î»2 is seen
as a function of ËœA and âˆ‡ËœAÎ»2, the gradient of Î»2 wrt ËœA, comes from the chain rule of the matrix
derivative Tr
h
(âˆ‡ËœLÎ»2)T Â· âˆ‡ËœAËœL
i
. More precisely,
âˆ‡ËœLÎ»2 := âˆ‚Î»2
âˆ‚ËœL
= f2f T
2 ,
(29)
is a matrix relying on an outer product (correlation). In the proposed GAP-LAYER, since f2 is
approximated by:
f2(u) =

+1/âˆšn
if u belongs to the first cluster
âˆ’1/âˆšn
if u belongs to the second cluster ,
(30)
i.e. we discard the O

log n
n

from Eq. 30 (the non-liniarities conjectured in [17]) in order to simplify
the analysis. After reordering the entries of f2 for the sake of clarity, f2f T
2 is the following block
matrix:
f2f T
2 =

1/n
âˆ’1/n
âˆ’1/n
1/n

whose diagonal matrix is diag(f2f T
2 ) =

1/n
0
0
1/n

(31)
Then, we have
âˆ‡ËœAÎ»2 =

1/n
1/n
1/n
1/n

âˆ’

1/n
âˆ’1/n
âˆ’1/n
1/n

=

0
2/n
2/n
0

(32)
19

DiffWire: Inductive Graph Rewiring via the LovÃ¡sz Bound
which explains the results in Figure 1-left: edges linking nodes belonging to the same cluster remain
unchanged whereas inter-cluster edges have a gradient of 2/n. This provides a simple explanation
for TGAP = ËœA(S) âŠ™A. The additional masking added by the adjacency matrix ensures that we do
not create new links.
Gradients Normalized-cut Approximation.
Similarly, using Î»â€²
2 for graph rewiring leads to the
following complex expression:
âˆ‡ËœAÎ»â€²
2 := Tr
h
(âˆ‡Ëœ
LÎ»2)T Â· âˆ‡ËœA ËœL
i
=
dâ€² n
gT
2 ËœAT ËœDâˆ’1/2g2
o
1T + dâ€² n
gT
2 ËœA ËœDâˆ’1/2g2
o
1T
+
ËœDâˆ’1/2g2gT
2 ËœDâˆ’1/2 .
(33)
However, since g2 = D1/2f2 and f2 = Dâˆ’1/2g2, the gradient may be simplified as follows:
âˆ‡ËœAÎ»â€²
2 := Tr
h
(âˆ‡Ëœ
LÎ»2)T Â· âˆ‡ËœA ËœL
i
=
dâ€² n
f T
2 ËœD1/2 ËœAT f2
o
1T + dâ€² n
f T
2 ËœD1/2 ËœAf2
o
1T
+
ËœDâˆ’1/2f2f T
2 ËœDâˆ’1/2 .
(34)
In addition, considering symmetry for the undirected graph case, we obtain:
âˆ‡ËœAÎ»â€²
2 := Tr
h
(âˆ‡Ëœ
LÎ»2)T Â· âˆ‡ËœA ËœL
i
=
2dâ€² n
f T
2 ËœD1/2 ËœAf2
o
1T + ËœDâˆ’1/2f2f T
2 ËœDâˆ’1/2 .
(35)
where dâ€² is a n Ã— 1 negative vector including derivatives of degree wrt adjacency and related terms.
The obtained gradient is composed of two terms.
The first term contains the matrix ËœD1/2 ËœA which is the adjacency matrix weighted by the square root
of the degree; f T
2 ËœD1/2 ËœAf2 is a quadratic form (similar to a Dirichlet energy for the Laplacian) which
approximates an eigenvalue of ËœD1/2 ËœA. We plan to further analyze the properties of this term in
future work.
The second term, ËœDâˆ’1/2f2f T
2 ËœDâˆ’1/2, downweights the correlation term for the Ratio-cut case f2f T
2
by the degrees as in the normalized Laplacian. This results in a normalization of the Fiedler vector:
âˆ’1/n becomes âˆ’âˆšdudv/n at the uv entry and similarly for 1/n, i.e. each entry contains the average
degree assortativity.
A.2.2
Beyond the LovÃ¡sz Bound: the von Luxburg et al. bound
The LovÃ¡sz bound was later refined by von Luxburg et al. [40] via a new, tighter bound which replaces
dmin by d2
min in Eq. 1. Given that Î»â€²
2 âˆˆ(0, 2], as the number of nodes in the graph (n = |V |) and
the average degree increase, then Ruv â‰ˆ1/du + 1/dv. This is likely to happen in certain types of
graphs, such as Gaussian similarity-graphs â€“graphs where two nodes are linked if the neg-exponential
of the distances between the respective features of the nodes is large enough; Ïµ-graphs â€“graphs where
the Euclidean distances between the features in the nodes are â‰¤Ïµ; and kâˆ’NN graphs with large k wrt
n. The authors report a linear collapse of Ruv with the density of the graph in scale-free networks,
such as social network graphs, whereas a faster collapse of Ruv has been reported in community
graphs â€“congruent graphs with Stochastic Block Models (SBMs) [44].
Given the importance of the effective resistance, Ruv, as a global measure of node similarity, the
von Luxburg et al.â€™s refinement motivated the development of robust effective resistances, mostly in
the form of pâˆ’resistances given by Rp
uv = arg minf{P
eâˆˆE re|fe|p}, where f is a unit-flow injected
in u and recovered in v; and re = 1/we with we being the edgeâ€™s weight [50]. For p = 1, Rp
uv
corresponds to the shortest path; p = 2 results in the effective resistance; and p â†’âˆleads to
the inverse of the unweighted u-v-mincut4. Note that the optimal p value depends on the type of
graph [50] and pâˆ’resistances may be studied from the perspective of pâˆ’Laplacians [45, 51].
While Ruv could be unbounded by minimizing the spectral gap Î»â€²
2, this approach has received little
attention in the literature of mathematical characterization of graphs with small spectral gaps [52][53],
i.e., instead of tackling the daunting problem of explicitly minimizing the gap, researchers in this
field have preferred to find graphs with small spectral gaps.
4The link between CTs and mincuts is leveraged in the paper as an essential element of our approach.
20

DiffWire: Inductive Graph Rewiring via the LovÃ¡sz Bound
A.3
Appendix C: Experiments
In this section, we provide details about the graphs contained in each of the datasets used in our
experiments, a detailed clarification about architectures and experiments, and, finally, report additional
experimental results.
A.3.1
Datasets Statistics
Table 4 depicts the number of nodes, edges, average degree, assortativity, number of triangles,
transitivity and clustering coefficients (mean and standard deviation) of all the graphs contained in
each of the benchmark datasets used in our experiments. As seen in the Table, the datasets are very
diverse in their characteristics. In addition, we use two synthetic datasets with 2 classes: ErdÃ¶s-RÃ©nyi
with p1 âˆˆ[0.3, 0.5] and p2 âˆˆ[0.4, 0.8] and Stochastic block model (SBM) with parameters p1 = 0.8,
p2 = 0.5, q1 âˆˆ[0.1, 0.15] and q2 âˆˆ[0.01, 0.1].
Table 4: Dataset statistics. Parenthesis in Assortativity column denotes number of complete graphs
(assortativity is undefined).
Nodes
Egdes
AVG Degree
Triangles
Transitivity
Clustering
Assortativity
REDDIT-B
429.6 Â±554
497.7 Â±622
2.33 Â±0.3
24 Â±41
0.01 Â±0.02
0.04 Â±0.06
-0.364 Â±0.17 (0)
IMDB-B
19.7 Â±10
96.5 Â±105
8.88 Â±5.0
391 Â±868
0.77 Â±0.15
0.94 Â±0.03
-0.135 Â±0.16 (139)
COLLAB
74.5 Â±62
2457 Â±6438
37.36 Â±44
12Ã—104 Â±48Ã—104
0.76 Â±0.21
0.89 Â±0.08
-0.033 Â±0.24 (680)
MUTAG
2.2 Â±0.1
19.8 Â±5.6
2.18 Â±0.1
0.00 Â±0.0
0.00 Â±0.00
0.00 Â±0.00
-0.279 Â±0.17 (0)
PROTEINS
39.1 Â±45.8
72.8 Â±84.6
3.73 Â±0.4
27.4 Â±30
0.48 Â±0.20
0.51 Â±0.23
-0.065 Â±0.2 (13)
In addition, Figure 6 depicts the histograms of the assortativity for all the graphs in each of the
eight datasets used in our experiments. As shown in Table 4 assortativity is undefined in complete
graphs (constant degree, all degrees are the same). Assortativity is defined as the normalized degree
correlation. If the graph is complete, then both correlation and its variance is 0, so assortativity will
be 0/0.
(a) REDDIT
(b) IMDB-BINARY
(c) COLLAB
(d) MUTAG
(e) PROTEINS
Figure 6: Histogram of the Assortativity of all the graphs in each of the datasets.
In addition, Figure 7 depicts the histograms of the average node degrees for all the graphs in each of
the eight datasets used in our experiments. The datasets are also very diverse in terms of topology,
corresponding to social networks, biochemical networks and meshes.
21

DiffWire: Inductive Graph Rewiring via the LovÃ¡sz Bound
(a) REDDIT
(b) IMDB-BINARY
(c) COLLAB
(d) MUTAG
(e) PROTEINS
Figure 7: Degree histogram of the average degree of all the graphs in each of the datasets.
A.3.2
Graph Classification GNN Architectures
Figure 8 shows the specific GNN architectures used in the experiments explained in section 4 in the
manuscript. Although the specific calculation of TGAP and TCT are given in Theorems 2 and 1, we
also provide a couple of pictures for a better intuition.
LINEAR
CONV
MINCUT
CONV
READOUT
MLP
X
X
X
A
X 
à·¡A
X
X
à·¡Y
(a) MINCUT baseline
X
LINEAR
CONV
MINCUT
CONV
READOUT
MLP
X
X
A
X 
à·¡A
X
X
à·¡Y
GAP-Layer
Tg
(b) GAP-LAYER
X
LINEAR
CONV
MINCUT
CONV
READOUT
MLP
X
X
A
X 
à·¡A
X
X
à·¡Y
CT-Layer
Tct
(c) CT-LAYER
Figure 8: Diagrams of the GNNs used in the experiments.
22

DiffWire: Inductive Graph Rewiring via the LovÃ¡sz Bound
A.3.3
Training Parameters
The value of the hyperparameters used in the experiments are the ones by default in the code
repository 5. We report average accuracies and standard deviation on 10 random iterations, using
different 85/15 train-test stratified split (we do not perform hyperparameter search), training during
60 epochs and reporting the results of the last epoch for each random run. We have used an Adam
optimizer, with a learning rate of 5e âˆ’4 and weight decay of 1e âˆ’4. In addition, the batch size
used for the experiments are shown in Table 5. Regarding the synthetic datasets, the parameters are:
ErdÃ¶s-RÃ©nyi with p1 âˆˆ[0.3, 0.5] and p2 âˆˆ[0.4, 0.8] and Stochastic block model (SBM) p1 = 0.8,
p2 = 0.5, q1 âˆˆ[0.1, 0.15] and q2 âˆˆ[0.01, 0.1].
Table 5: Dataset Batch size
Batch
Dataset size
REDDIT-BINARY
64
1000
IMDB-BINARY
64
2000
COLLAB
64
5000
MUTAG
32
188
PROTEINS
64
1113
SBM
32
1000
ErdÃ¶s-RÃ©nyi
32
1000
For the k-nn graph baseline, we choose k such that the main degree of the original graph is maintained,
i.e. k equal to average degree. Our experiments also use 2 preprocessing methods DIGL and SDRF.
Unlike our proposed methods, both SDRF [20] and DIGL [25] use a set of hyperparamerters to
optimize for each specific graph, because both are also not inductive. This approach could be
manageable for the task of node classification, where you only have one graph. However, when it
comes to graph classification, the number of graphs are huge (5) and it is nor computationally feasible
optimize parameters for each specific graph. For DIGL, we use a fixed Î± = 0.001 and Ïµ based on
keeping the same average degree for each graph, i.e., we use a different dynamically chosen Ïµ for
each graph in each dataset which maintain the same number of edges as the original graph. In the
case of SDRF, the parameters define how stochastic the edge addition is (Ï„), the graph edit distance
upper bound (number of iterations) and optional Ricci upper-bound above which an edge will be
removed each iteration (C+). We set the parameters Ï„ = 20 (the edge added is always near the edge
of lower curvature), C+ = 0 (to force one edge is removed every iteration), and number of iterations
dynamic according to 0.7 âˆ—|V |. Thus, we maintain the same number of edges in the new graph
(Ï„ = 20 and C+ = 0), i.e., same average degree, and we keep the graph distance to the original
bounded by 0.7 âˆ—|V |.
A.3.4
Latent Space Analysis
In this section, we analyze the two latent spaces produced by the models.
â€¢ First, we compare the CT Embedding computed spectrally (Z in equation 2) with the CT
Embedding predicted by our CT-LAYER (Z in definition 1) for a given graph, where each point
is a node in the graph.
â€¢ Second, we compare the graph readout output for every model defined in the experiments
(Figure 4) where each point is a graph in the dataset.
Spectral CT Embedding vs CT Embeddings Learned by CT-LAYER .
The well-known em-
beddings based on the Laplacian positional encodings (PE) are typically computed beforehand and
appended to the input vector X as additional features [35, 36]. This task requires an expensive
computation O(n3) (see equation 2). Conversely, we propose a GNN Layer that learns how to predict
the CT embeddings (CTEs) for unseen graphs (definition 1 and Figure 2) with a loss function that
optimizes such CTEs. Note that we do not explicitly use the CTE features (PE) for the nodes, but we
use the CTs as a new diffusion matrix for message passing (given by TCT in Definition 1). Note that
we could also use Z as positional encodings in the node features, such that CT-LAYER may be seen
as a novel approach to learn Positonal Encodings.
5https://github.com/AdrianArnaiz/DiffWire
23

DiffWire: Inductive Graph Rewiring via the LovÃ¡sz Bound
In this section, we perform a comparative analysis between the spectral commute times embeddings
(spectral CTEs, Z in equation 2) and the CTEs that are predicted by our CT-LAYER (Z in definition 1).
As seen in Figure 9 (top), both embeddings respect the original topology of the graph, but they differ
due to (1) orthogonality restrictions, and more interestingly to (2) the simplification of the original
spectral loss function in Alev et al. [43]: the spectral CTEs minimize the trace of a quotient, which
involves computing an inverse, whereas the CTEs learned in CT-LAYER minimize the quotient of
two traces which is computationally simpler (see LCT loss in Definition 1). Two important properties
of the first term in Definition 1 are: (1) the learned embedding Z has minimal Dirichlet energy
(numerator) and (2) large degree nodes will be separated (denominator). Figure 9 (top) illustrates
how the CTEs that are learned in CT-LAYER are able to better preserve the original topology of the
graph (note how the nodes are more compactly embedded when compared to the spectral CTEs).
Figure 9 (bottom) depicts a histogram of the effective resistances or commute times (CTs) (see
Section 3.2 in the paper) of the edges according to CT-LAYER or the spectral CTEs. The histogram is
computed from the upper triangle of the TCT matrix defined in Definition 1. Note that the larger the
effective resistance of an edge, the more important that edge will be considered (and hence the lower
the probability of being removed [54]). We observe how in the histogram of CTEs that are learned
in CT-LAYER there is a â€˜small clubâ€™ of edges with very large values and a large number of edges
with low values yielding a power-law-like profile. However, the histogram of the effective resistances
computed by the spectral CTEs exhibits a profile similar to a Gaussian distribution. From this result,
we conclude that the use of LCT in the learning process of the CT-LAYER shifts the distribution of
the effective resistances of the edges towards an asymmetric distribution where few edges have very
large weights and a majority of edges have low weights.
CT-Layer CTE
Spectral CTE
0.00
0.01
0.02
0.03
0
20
40
60
80
100
120
CT-Layer CT Dist histogram
0.003
0.004
0.005
0.006
0
20
40
60
Spectral CT Dist histogram
5
10
15
20
25
30
Node Degree
Figure 9: Top: CT embeddings predicted by CT-LAYER (left) and spectral CT embeddinggs (right).
Bottom: Histogram of normalized effective resistances (i.e., CT distances or upper triangle in TCT)
computed from the above CT embeddings. Middle: original graph from the COLLAB dataset. Colors
correspond to node degree. CT-LAYER CTEs reduced from 75 to 32 dimensions using Johnson-
Lindenstrauss. Finally, both CTEs reduced from 32 to 2 dimensions using T-SNE.
Graph Readout Latent Space Analysis.
To delve into the analysis of the latent spaces produced
by our layers and model, we also inspect the latent space produced by the models (Figure 4) that use
MINCUTPOOL (Figure 8a), GAP-LAYER (Figure 8b) and CT-LAYER (Figure 8c). Each point is a
graph in the dataset, corresponding to the graph embedding of the readout layer. We plot the output
of the readout layer for each model, and then perform dimensionality reduction with TSNE.
Observing the latent space of the REDDIT-BINARY dataset (Figure 10), CT-LAYER creates a disperse
yet structured latent space for the embeddings of the graphs. This topology in latent spaces show that
this method is able to capture different topological details. The main reason is the expressiveness of
the commute times as a distance metric when performing rewiring, which has been shown to be a
24

DiffWire: Inductive Graph Rewiring via the LovÃ¡sz Bound
optimal metric to measure node structural similarity. In addition, GAP-LAYER creates a latent space
where, although the 2 classes are also separable, the embeddings are more compressed, due to a more
aggressive â€“yet still informativeâ€“ change in topology. This change in topology is due to the change in
bottleneck size that GAP-LAYER applies to the graph. Finally, MINCUT creates a more squeezed
and compressed embedding, where both classes lie in the same spaces and most of the graphs have
collapsed representations, due to the limited expressiveness of this architecture.
(a) CT-LAYER
(b) MinCut
(c) GAP-LAYER
Figure 10: REDDIT embeddings produced by GAP-LAYER (Ncut) CT-LAYER and MINCUT.
A.3.5
Architectures and Details of Node Classification Experiments
The application of our framework for a node classification task entails several considerations. First,
this first implementation of our method works with dense A and X matrices, whereas node classifica-
tion typically uses sparse representations of the edges. Thus, the implementation of our proposed
layers is not straightforward for sparse graph representations. We are planning to work on the sparse
version of this method in future work.
Note that we have chosen benchmark datasets that are manageable with our dense implementation.
In addition, we have chosen a basic baseline with 1 GCN layer to show the ability of the approaches
to avoid under-reaching, over-smoothing and over-squashing.
The baseline GCN is a 1-layer-GCN, and the 2 compared models are:
â€¢ 1 CT-LAYER for calculating Z followed by 1 GCN Layer using A for message passing and
X âˆ¥Z as features. This approach is a combination of Velingker et al. [35] and our method. See
Figure 11c.
â€¢ 1 CT-LAYER for calculating TCT followed by 1 GCN Layer using that TCT for message
passing and X as features. See Figure 11b.
X
GCN
A
à·¡Y
(a) GCN baseline
X
GCN
A
à·¡Y
CT-Layer
ğ“ğœğ­
X
(b) A = TCT
X
GCN
A
à·¡Y
CT-Layer
ğ™
A
(c) X âˆ¥Z
Figure 11: Diagrams of the GNNs used in the experiments for node classification.
A promising direction of future work would be to explore how to combine both approaches to leverage
the best of each of the methods on a wide range of graphs for node classification tasks. In addition,
using this learnable CT distance for modulating message passing in more sophisticated ways is
planned for future work.
25

DiffWire: Inductive Graph Rewiring via the LovÃ¡sz Bound
A.3.6
Analysis of Correlation between Structural Properties and CT-LAYER Performance
To analyze the performance of our model in graphs with different structural properties, we analyze the
correlation between accuracy, the graphâ€™s assortativity, and the graphâ€™s bottleneck (Î»2) in COLLAB
and REDDIT datasets. If the error is consistent along all levels of accuracy and gaps, the layer can
generalize along different graph topologies.
As seen in Figure 14, Figure 12 (middle), and Figure 13 (middle), we do not identify any correlation
or systematic pattern between graph classification accuracy, assortativity, and bottleneck with CT-
LAYER-based rewiring, since the proportion of wrong and correct predictions are regular for all levels
of assortativity and bottleneck size.
In addition, note that while there is a systematic error of the model over-predicting class 0 in the
COLLAB dataset (see Figure 12), this behavior is not explained by assortativity or bottleneck size,
but by the unbalanced number of graphs in each class.
0.5
0.0
0.5
1.0
assortativity
0
20
40
60
80
100
Assortativity histograms
Label
0
1
2
0.5
0.0
0.5
1.0
assortativity
0
20
40
60
80
100
Correct prediction
False
True
0.5
0.0
0.5
1.0
assortativity
0
20
40
60
80
100
Predicted
0
1
2
0.00
0.25
0.50
0.75
1.00
bottleneck
0
20
40
60
80
100
2 histograms
Label
0
1
2
0.00
0.25
0.50
0.75
1.00
bottleneck
0
20
40
60
80
100
Correct prediction
False
True
0.00
0.25
0.50
0.75
1.00
bottleneck
0
20
40
60
80
100
Predicted
0
1
2
0
1
2
Predicted
0
1
2
Label
0.74
0.11
0.15
0.23
0.77
0
0.29
0.046
0.67
COLLAB
Figure 12: Analysis of assortativity, bottleneck and accuracy for COLLAB dataset. Top: Histograms
of assortativity. Bottom: Histograms of bottleneck size (Î»2). Both are grouped by actual label of the
graph (left), by correct or wrong predictions (middle) and by predicted label (right).
1.00
0.75
0.50
0.25
0.00
assortativity
0
10
20
30
Assortativity histograms
Label
0
1
1.00
0.75
0.50
0.25
0.00
assortativity
0
10
20
30
Correct prediction
False
True
1.00
0.75
0.50
0.25
0.00
assortativity
0
10
20
30
Predicted
0
1
0.00
0.25
0.50
0.75
1.00
bottleneck
0
50
100
150
200
250
2 histograms
Label
0
1
0.00
0.25
0.50
0.75
1.00
bottleneck
0
50
100
150
200
250
Correct prediction
False
True
0.00
0.25
0.50
0.75
1.00
bottleneck
0
50
100
150
200
250
Predicted
0
1
0
1
Predicted
0
1
Label
0.91
0.095
0.2
0.81
REDDIT-BINARY
Figure 13: Analysis of assortativity, bottleneck and accuracy for REDDIT-B dataset. Top: Histograms
of assortativity. Bottom: Histograms of bottleneck size (Î»2). Both are grouped by actual label of the
graph (left), by correct or wrong predictions (middle) and by predicted label (right).
26

DiffWire: Inductive Graph Rewiring via the LovÃ¡sz Bound
0.5
0.0
0.5
1.0
1.5
Assortativity
0.25
0.00
0.25
0.50
0.75
1.00
1.25
Bottleneck 
2
Correct prediction
False
True
(a) COLLAB
1.00
0.75
0.50
0.25
0.00
Assortativity
0.0
0.2
0.4
0.6
0.8
1.0
Bottleneck 
2
Correct Prediction
False
True
(b) REDDIT-B
Figure 14: Correlation between assortativity, Î»2 and accuracy for CT-LAYER. Histograms shows
that the proportion of correct and wrong predictions are regular for all levels of assortativity (x axis)
and bottleneck size (y axis). For the sake of clarity, these visualizations, a and b, are the combination
of the 2 histograms in the middle column of Figure 12 and Figure 13 respectively.
A.3.7
Computing Infrastructure
Table 6 summarizes the computing infrastructure used in our experiments.
Table 6: Computing infrastructure.
Component
Details
GPU
2x A100-SXM4-40GB
RAM
1 TiB
CPU
255x AMD 7742 64-Core @ 2.25 GHz
OS
Ubuntu 20.04.4 LTS
27

