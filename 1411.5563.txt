Spacetimes with Semantics (I)
Notes on Theory and Formalism
Mark Burgess
November 21, 2014
Abstract
Relationships between objects constitute our notion of space. When these relationships change we
interpret this as the passage of time. Observer interpretations are essential to the way we understand
these relationships. Hence observer semantics are an integral part of what we mean by spacetime.
Semantics make up the essential difference in how one describes and uses the concept of space in
physics, chemistry, biology and technology. In these notes, I have tried to assemble what seems to be a
set of natural, and pragmatic, considerations about discrete, ﬁnite spacetimes, to unify descriptions of
these areas.
It reviews familiar notions of spacetime, and brings them together into a less familiar framework
of promise theory (autonomous agents), in order to illuminate the goal of encoding the semantics of
observers into a description of spacetime itself. Autonomous agents provide an exacting atomic and
local model for ﬁnite spacetime, which quickly reveals the issues of incomplete information and non-
locality. From this we should be able to reconstruct all other notions of spacetime.
The aim of this exercise is to apply related tools and ideas to an initial uniﬁcation of real and
artiﬁcial spaces, e.g. databases and information webs with natural spacetime. By reconstructing these
spaces from autonomous agents, we may better understand naming and coordinatization of semantic
spaces, from crowds and swarms to datacentres and libraries, as well as the fundamental arena of natural
science.
Contents
1
Introduction
3
2
Models of spacetime
4
2.1
Elements with names . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5
2.2
Models of space . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6
2.3
Models of time
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
2.4
Scaling of descriptions - renormalization . . . . . . . . . . . . . . . . . . . . . . . . . .
8
2.5
Models of adjacency, communication and linkage . . . . . . . . . . . . . . . . . . . . .
9
3
Mathematical structures of spacetime
10
3.1
Sets and Topology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
10
3.1.1
Vectors between elements and direction . . . . . . . . . . . . . . . . . . . . . .
11
3.1.2
Coorinates, bases, matroids and independent dimensions . . . . . . . . . . . . .
12
3.2
Categories and algebras . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
13
3.2.1
Operators and total functions . . . . . . . . . . . . . . . . . . . . . . . . . . . .
14
3.2.2
Diagrams as categories . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
14
3.3
Vector spaces . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
15
1
arXiv:1411.5563v1  [cs.MA]  20 Nov 2014

3.3.1
Group lattice . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
15
3.3.2
Naming of points on a vector space: coordinate systems and bases . . . . . . . .
16
3.3.3
Rank, linear independence and dimensionality
. . . . . . . . . . . . . . . . . .
16
3.3.4
Tensors, order and rank . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
17
3.3.5
Distance and the inner product . . . . . . . . . . . . . . . . . . . . . . . . . . .
17
3.3.6
Matrices and transformations . . . . . . . . . . . . . . . . . . . . . . . . . . . .
17
3.3.7
Derivatives and vectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
18
3.3.8
Boundaries, subspaces and immersion of structures . . . . . . . . . . . . . . . .
18
3.4
Manifolds and Minkowski spacetime . . . . . . . . . . . . . . . . . . . . . . . . . . . .
19
3.5
Graphs or networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
20
3.5.1
Adjacency and matrix representation . . . . . . . . . . . . . . . . . . . . . . . .
20
3.5.2
Strongly connected components . . . . . . . . . . . . . . . . . . . . . . . . . .
21
3.5.3
Functions f(x) on a graph . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
22
3.5.4
Distance on a graph (hops) . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
22
3.5.5
Linear independence, matroids and spanning trees
. . . . . . . . . . . . . . . .
23
3.5.6
Scale transformations a graph
. . . . . . . . . . . . . . . . . . . . . . . . . . .
24
3.5.7
Derivatives and vectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
24
3.5.8
Lattices from irregular graphs . . . . . . . . . . . . . . . . . . . . . . . . . . .
24
3.6
Symbolic grammars as spatial models . . . . . . . . . . . . . . . . . . . . . . . . . . .
25
3.6.1
Dimensionality and topology in languages . . . . . . . . . . . . . . . . . . . . .
25
3.6.2
Automata as classiﬁers of grammatical structure
. . . . . . . . . . . . . . . . .
25
3.6.3
Naming of elements in a grammar . . . . . . . . . . . . . . . . . . . . . . . . .
26
3.7
Bigraphs: nested structures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
26
3.7.1
Bare graphs, forests and links
. . . . . . . . . . . . . . . . . . . . . . . . . . .
27
3.7.2
Boundaries and interfaces on a bigraph
. . . . . . . . . . . . . . . . . . . . . .
28
3.7.3
Signatures and sorts for semantic typing of spatial structures . . . . . . . . . . .
29
3.7.4
Composition of bigraph operators . . . . . . . . . . . . . . . . . . . . . . . . .
30
3.7.5
Time and motion in bigraphs . . . . . . . . . . . . . . . . . . . . . . . . . . . .
31
3.7.6
Derivatives and vectors . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
32
3.8
Processes algebras, time and motion . . . . . . . . . . . . . . . . . . . . . . . . . . . .
32
4
Spacetime semantics
32
5
Promise theory - autonomous local observer semantics
34
5.1
Basics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
34
5.2
Agent names, identiﬁers and namespaces
. . . . . . . . . . . . . . . . . . . . . . . . .
36
5.3
Reconstructing adjacency, local and non-local space . . . . . . . . . . . . . . . . . . . .
37
5.4
Continuity, direction, and bases . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
38
5.5
Symmetry, short and long range order . . . . . . . . . . . . . . . . . . . . . . . . . . .
39
5.6
Material (scalar) properties as singularities and cliques
. . . . . . . . . . . . . . . . . .
40
5.7
Spacetime (vector) promises and quasi-transitivity . . . . . . . . . . . . . . . . . . . . .
42
5.8
Fields and potentials
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
43
5.9
Boundaries and holes . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
44
5.10 Containment within regions and boundaries . . . . . . . . . . . . . . . . . . . . . . . .
45
5.11 Local, global, and proper time (What counts as a clock?) . . . . . . . . . . . . . . . . .
46
5.11.1 Concurrency, simultaneity, and timelines
. . . . . . . . . . . . . . . . . . . . .
46
5.11.2 Shared (non-local) timeline example . . . . . . . . . . . . . . . . . . . . . . . .
47
5.12 Motion, speed and acceleration in agent space . . . . . . . . . . . . . . . . . . . . . . .
50
5.12.1 Foreword: is there a difference between space and matter that ﬁlls it?
. . . . . .
50
2

5.12.2 Motion of the ﬁrst kind (gaseous)
. . . . . . . . . . . . . . . . . . . . . . . . .
50
5.12.3 Motion of the second kind (solid state conduction)
. . . . . . . . . . . . . . . .
52
5.12.4 Motion of the third kind (solid state conduction)
. . . . . . . . . . . . . . . . .
53
5.12.5 Measuring speed, velocity and transport properties . . . . . . . . . . . . . . . .
54
5.13 Growth and death of agent based spacetime . . . . . . . . . . . . . . . . . . . . . . . .
56
6
Semantic (Knowledge) spaces
57
6.1
Modelling concepts and their relationships . . . . . . . . . . . . . . . . . . . . . . . . .
57
6.2
Coordinate systems for knowledge spaces . . . . . . . . . . . . . . . . . . . . . . . . .
58
6.3
Semantic distance . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
59
6.4
The autonomous agent view of a knowledge map
. . . . . . . . . . . . . . . . . . . . .
61
6.5
Semantic agencies: things . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
62
6.6
Promising knowledge as a typed, attributed, or material space . . . . . . . . . . . . . . .
63
6.7
Indices, meta-data, and code-books for semantic spaces . . . . . . . . . . . . . . . . . .
64
6.8
World lines, processes, stories, events, and reasoning . . . . . . . . . . . . . . . . . . .
65
6.9
The semantics and dynamics of identity and context . . . . . . . . . . . . . . . . . . . .
67
6.10 Low-entropy functional spaces, and componentization
. . . . . . . . . . . . . . . . . .
68
6.11 High-entropy load-sharing spaces and de-coordinatization
. . . . . . . . . . . . . . . .
70
6.12 Coordinatizing multi-phase space (a ubiquitous Internet of Things) . . . . . . . . . . . .
74
6.13 Proper time, branching, and many worlds
. . . . . . . . . . . . . . . . . . . . . . . . .
75
7
Closing remarks
76
A Graph bases and coordinatized dimensions
79
A.1
Example 1 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
79
A.2
Example2 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
81
A.3
Example 3 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
81
A.4
Artiﬁciality of dimensions
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
82
1
Introduction
Descriptions of how things are arranged, and how they change, give us our notions of space and time.
Between any two identiﬁable objects, we may identify various kinds of relationship. Of these many
relationships, spatial and temporal relationships have a special meaning to us. We identify the idea of
location with the elements of a space and direction with the relationships between them.
Traditionally, theories of the world treat spacetime as a smooth background latticework on which the
things within it (matter and energy) move. In other words, spacetime is treated as something separate
from matter and energy, a kind of measuring apparatus into which we embed processes, replete with
coordinate labels. However, in the physics and chemistry of materials, the notion of place within a ma-
terial is not distinguished from the materials themselves, as it would be a burden to do so. Moreover,
our representation of space in other cases comes from measuring it along side a set of states we arrange
for the purpose. Thus the perception of distance requires an elapsed time, so they two cannot be sepa-
rated. Indeed, measurement implies the ability of an observer to compare different notions of distance by
signalling over an elapsed time, so what we are missing is a more general observer view of spacetime.
One problem with current models of space and time is that they are inconsistent with respect to
locality. Space and time are laid out with global properties and structure a priori, only then does one
make corrections the account for locality instead of the other way around. This is because observers are
already considered to be looking at the macroscopic scale in a smooth continuum approximation. When
spacetime is considered as arising from fundamental local interactions at the smallest scales, the picture
3

becomes different, and familiar notions have to emerge by deﬁning a continuum limit. Indeed, at this
level, there seems to be a fundamental non-locality to information that is unavoidable.
Einstein was the ﬁrst to illustrate how observer locality has unexpected consequences, once one took
seriously the relationship between space and time. In mainstream information systems there are even
more pronounced effects from variations in the ‘speed’ at which communication can connect different
bodies. This viewpoint has held dominion for three centuries, but there is something missing, especially
from a practical perspective. It is is viewpoint that leads to problems even in physics (cf. Zeno’s paradox,
the inﬁnities of quantum ﬁeld theory, for example). Moreover, it is not a useful parameterization of
dynamical systems for the information sciences.
Setting aside whether it might have fundamental signiﬁcance, or merely traditionalist convenience,
we need an intentional theory of spacetime that includes functional aspects of the world we live in, i.e.
a way to bring the observer and semantics into the picture. The fact that this is missing from conven-
tional descriptions means that we cannot understand all aspects of relativity that stem from situational
differences of observers.
Until recently, science has eschewed the notion that agency (that quality which adds subjectivity and
intent) should be a part of a description of the world. This has led to manifest contradictions and paradoxes
in relativity and quantum mechanics. Starting with Einstein, we have been forced to take seriously the
role of the observer and allow for differences between their perceptions.
The idea of connectivity is not unique to the physical sciences, but plays a major role in informa-
tion systems too. There, spatial relationships and topologies are often functional and imply semantics.
Semantics and intentions are subjective and require agency, so objectifying the notion of spacetime will
never offer a satisfactory description for information science. If one starts with an agent view of the
world, the idea of an absolute spacetime seems untenable or at best artiﬁcial. Thus we need a version
of relativity that takes into account how autonomous agents learn information about the world. This has
some simple but profound implications for the resulting view of space and time, which can no longer be
seen as independent concepts.
Promise Theory already exists to address questions like these; but, one of the pressing questions we
face (which happens to be of great practical importance in information systems) is how do we scale from
small clusters of promises to large fabrics, spaces, or even manifolds.
A descriptions of even an objective spacetime is potentially a highly technical topic that has ﬁlled
lifetimes with mathematics, so we must have modest ambitions here. I shall try to sketch only an outline
of how such a picture of spatial relationships works in this paper. No reference will be made to string
or brane theories of spacetime, which suffer from the same defects as Euclidean space. Details can be
studied in more depth elsewhere and by others.
2
Models of spacetime
Let us begin with the idea that space and time are observed quantities. We take the existence of ‘ele-
ments’ (points, regions, agents) of location as given, though we shall not always elaborate on their nature.
Changes in the state of these elements can include changes in their relative relationships. Relative po-
sition is one such relationship. As states change, we mark out different versions of these relationships,
which is how we measure the progression of time.
Deﬁnition 1 (Spatial element) An element of a spacetime is a member in a set, which expresses the
property of location. It can be named, and satisﬁes all the properties of set theory.
Assumption 1 (Adjacency represents space) Space is the expression of a connectivity of its base con-
stituent elements.
4

Intersections
Edges
Symmetry
Generators
Promises
Agents
Group
Elements
Vertices
Set
Set
Elements
Objects
Adjacency Relationships
Figure 1: Spatial relationships between elements
We often speak of points for elements, but I’ll reserve that for manifolds or Euclidean space and inﬁnites-
imal dots that form a continuum. There is no need for that to be the nature of space (see for instance a
larger point element in ﬁg. 8).
Assumption 2 (Clocks measure time) Time measures and is measured by change. Any changing system
may be regarded as a clock that marks the passage of time. Without distinguishable change there is no
time.
Physicists are used to stydying large systems over long times compared to Caesium transitions, so one
many take for granted the ability to have a seemingly independent clock. This cannot be taken for granted
in a ﬁnite system of states. From the assumption of clocks, we can immediately say:
Lemma 1 If the variety of states available in a system is ﬁnite, then only a ﬁnite number of times can be
measured. Hence time is ﬁnite in a ﬁnite space.
If we ignore what we think we know about spacetime from experience, these basic assumptions
already contain must insight. Space is about structural patterns. How much structure do we need to be
able to represent patterns? What is the medium of such a pattern in empty space? If space is discrete and
ﬁnite, then it possesses a ﬁnite lifetime, which may or may not proceed as an ordered sequence of states.
The next step must be to elucidate the meaning of space using well known formalisms.
There are different approaches to describing space. In the Euclidean view, one underlines the notion
of expanse. Then there is the container view in which everything is placed within some kind of bounded
region and one element can be inside another (nested like Russian dolls). These views are complementary
but quite different. They may be related through vector formulae such as Gauss’s law or the divergence
theorem, etc.
The role of semantics now enters swiftly into the discussion. How we choose to parameterize spatial
relationships is an observer choice that requires intent (e.g. do we choose Cartesian or polar coordi-
nates for the naming convention?), thus we need to unify intent with description to understand spatial
relationships.
2.1
Elements with names
How we label the elements and collections of elements in a space is an important issue. If elements cannot
be identiﬁed, then they do not really exist to an observer’s universe. This does not necessarily imply that
every element must be individually distinguishable: there may be symmetries between elements which are
5

interchangeable. Collective names (categories) may be used for such redundant parts. These symmetries
in turn affect an observer’s ability to tell time.
Names may further have scope, or a domain of validity. In computer science one speaks of private
and public names. In vector spaces, with metric scales, we commonly use Cartesian vector coordinates,
based on tuples, to identify elements (points in that instance). Thus a name might also be a vector. In
general any property that can pinpoint an element may be called its name, whether an identifying mark, a
proper string, or a coordinate vector.
Any element might have more than one name, and there is no particular reason to assume that names
will remain constant. We need to understand the consequences of these considerations.
Sets of elements may also collectively have names, representing common properties. The term scope,
namespace, container, class, and even group are used for this. Some of these names have speciﬁc techni-
cal meanings. Each represents a set of neighbouring elements, or a ‘patch’ of space in which an element’s
name has validity. The functional role of an element can easily be used as a name, even if that changes,
making a connection with semantics.
There is a long tradition of naming elements in hierarchical namespaces, called taxonomies. The
coordinates of a name within a hierarchy are given by paths through the graph from some reference
root element, through each branch to the speciﬁc element. Tree-like hierarchy is not a necessity, but
in general one needs a collection of spanning sets to give every element ownership to a named scope,
even if those scopes overlap. The naming of elements into independent classes or regions leads to the
concept of dimensionality. This is very familiar to us in the case of a locally Euclidean vector space,
but understanding the nature of dimensions is quite hard for topologies such as graphs. As we shall see,
especially in connection with graphs, dimension is something an observer can choose to perceive in a
discrete spacetime. This issue is about concepts like orthogonality of vectors or independence of sets,
scopes or namespaces, as well as adjacency. In this regard, we shall meet the additional generalized
concepts of linear independence, matroids and spanning trees, to mention a few.
• How do we talk about where things are, and how those relationships change?
• A difference of two locations (two names) is considered to be both a vector and a derivative on a
topological space. This is how we deﬁne rate of change or gradient.
• If all of the names for an element are removed, then it effectively disappears from the universe of
things as an independent.
• The limits of a space are effectively the size of the list of names that it contains, whether they be
coordinates or other labels. Thus we have the notion of boundedness as the dimension of a set |S|.
2.2
Models of space
Our notion of extent begins with sets of elements and nearest neighbours within those sets. The concept
of a neighbourhood is the basis of topology. We understand extent as transitivity. i.e. if {A, B, C} are
elements of a partially ordered set (poset), where we consider the A is a nearest neighbour of and B is
a nearest neighbour of C, then in order to go from A to C, we must go through B. In general, to get
from one element to another, we might need to make several hops from element to element. This might
seem trivial (certainly commonplace), but there is no reason why this has to be the case. One could also
imaging directly teleporting to any element from any other with equal cost i.e. distance. This would
imply a ‘complete graph’ structure. Conversely, discreteness of elements invalidates assumptions about
distance like Pythagorean relationships (see ﬁg. 2). Thus the assumption of transitivity might turn out to
be wrong, but that will be the assumption for the remainder of this paper.
From the mathematics of sets, one may add a succession of reﬁnements with increasing amounts of
structure. Some key concepts include:
6

Figure 2: We are used to assuming that a triangle exists as straight lines in smooth Euclidean space, but
in a ﬁnite, discrete space there are no diagonal paths.
• Groupoid or Magma: a set with a closed operation.
• Semi-group: a magma whose operation is associative (a directed graph).
• Monoid: a semi-group with an identity element (a directed graph).
• Group: a monoid with an inverse (an undirected graph).
• Lattice: a tiling formed by the generators of a discrete group.
Our traditional notions of spacetime usually begin with groups and the regular lattices they generate.
From there, we invoke a continuum hypothesis, approximating a large countable set by a non-countable
distribution of values to obtain a smooth manifold (see below). Here we shall end up with a more primitive
(elementary) view.
2.3
Models of time
Time, for any observer, is measured by clocks. A clock is a mechanism which changes some measurable
phenomenon at a rate which has to be assumed locally constant. In principle there is only one clock, which
is the state of the entire universe; however, not all agents can observe the entire clock, thus observers
partition off their own set of distinguishable states which they use to measure their perception of change.
Measurable phenomena from which we might build clocks include those that require a change of
spatial position (e.g. a rotary clock), but could also include spin, charge and other ‘quantum numbers’.
Any ﬁnite (countable) state machine will do as a clock.
More importantly, every distinguishable spatial motion behaves as a clock, thus one cannot deﬁne
space and time and speed independently. They are dependent concepts. The best one can do is to ﬁx one
of the values and work from there. In Einsteinian relativity, one ﬁxes the speed of light.
We assume that the passage of time is monotonic, but this is merely an assumption and we shall see
that consistency requires us to allow time to run forwards and backwards in ﬁnite systems.
In a ﬁnite system of states, time is inseparably bound to those states, including states of position. In
order to make a clock that can measure time, we need to distinguish observable states. A change in the
states is a change in time, as there are no other measurable qualities. If a state change is repeated, by
cycle or reversal, then time also repeats, as measured by this clock.
Proposition 1 In a ﬁnite system, which acts as its own clock, a change of state cannot happen indepen-
dently of a change of time, since that change of state (the movement of the clock) is what we mean by
7

proper time. The number of possible times is equal to the number of distinct states. The measurable
velocities are 0 < dx/dt < ∞.
For the latter, if space measures proper time and a unit change of space is therefore a unit change of time,
then there is only a single speed at which all motion takes place: ∆x = ∆t = ∆x/∆t = 1. Any different
speed can only come about by choosing to measure ∆x, ∆t differently. See section 5.12.
The notion of proper time here is essentially what computer science refers to as a version of an
information source. Each ‘spatial hypersurface’ containing a unique microstate is a ‘version’, and a
single position in time. If the microstate is reversed to an earlier conﬁguration, then it is indistinguishable
from that time, and effectively the system is reverted to that time. This can only seem like a strange idea
to an observer who has an independent clock1.
If we compose spaces with more states, then all of the state conﬁgurations contribute to possible times
in the system. As the state space grows, time can extend combinatorially as the number of distinguishable
conﬁgurations. Thus time is linked to the level of entanglement of the different elements forming the
space. The connectivity of space must be linked to the extent of possible times. However, necessarily
in a space with a countable number of elements, time is also countable and ﬁnite. This follows from the
closure of the space under whatever operation relates neighbouring elements. Topology is therefore the
origin of time.
A note is in order here about the traditions of physics, where the complete state of a system is thought
to be canonically deﬁned by two quantities: position and velocity. In a state model, where time is mea-
sured by space itself, there cannot be an independent notion of velocity, since there is no independent
clock; thus a phase-space description has to be an emergent model from an underlying state model in
the view of this work. Velocity is replaced by a knowledge of transitions between states. However, we
cannot consider this to be a measure of time, as this will not change what observers see, only the sequence
with which they potentially see outcome. There is, surely, additional information in transitions that is not
understood in this explanation, but it cannot change the perception of time experienced by observers, who
are part of the states themselves.
2.4
Scaling of descriptions - renormalization
There is every reason to expect that the fundamental nature of a spacetime changes at different scales,
so eventually we shall need to deﬁne what we mean by scale. The different mathematical structures
represented in this paper have quite different properties in this respect.
Figure 3: Regular lattice scaling - the dimensionality is constant at all scales due to the group structure.
1The reason we don’t think of this in physics is that we do not measure the proper time of the universe. Space is too big.
However, in a closed system, external states are irrelevant except to an outside observer with access to them. Our familiar notion of
time must be understood as a continuum hypothesis of a ﬁnite state system.
8

Deﬁning scale is more complicated than it sounds in a general space. In Euclidean spaces that map to
Rn, scaling is a trivial group of multiplicative factors because the structure of space is self-similar under
different multiplicative maps, even when they are anisotropic or inhomogeneous. Euclidean spaces and
lattices have scale invariant behaviour down the minimum cutoff distance (see ﬁg 3). However, graphs
which are not regular lattices have no such regular scaling properties. The number of nearest neighbours
at one scale and one location might be very different than at a different scale or location.
Spacetimes are expected to be inhomogeneous and non-scale-invariant. Several approaches to space-
time already account for this. One thinks of the Regge calculus [1], curved Riemann manifolds [2],
fuzzy sets and topologies [3], and also studies of large scale networks (the Internet, the World Wide Web,
etc) [4]. The meaning of scaling in a graph has been deﬁned through Strongly Connected Components
(SCC) in [4] (see ﬁg. 4).
For a general set topology, we may take a collection of independent subsets σi (sets where each subset
has unique elements) and form a single set by their union:
S = ∪iσi
(1)
This coarse-grains (or renormalizes) a high resolution picture (small scale) into a lower resolution element
(large scale). This view of scale is sufﬁcient for the present paper. The matter of scaling is highly
technical.
Dimensionality is another area one struggles to deﬁne uniquely. In a non-isotropic, inhomogeneous
space, like a graph, dimensionality can vary from place to place and from scale to scale. In other words the
ability to translate position from element to element, depends on the scale at which we deﬁne translation
and how far we intend to go. This is a very different picture of space than the one we learn in school.
Figure 4: Scaling in a graph. At a large scale the arrangement of points seems 1 dimensional. A local
observer at a smaller grain size experiences more degrees of freedom at each point, suggesting a higher
dimensionality. The dimension of spacetime is only one aspect that can change under a scale transforma-
tion.
2.5
Models of adjacency, communication and linkage
How can we model spacetime without the prejudice of earlier ideas intruding too much? The most
primitive concept is that of locations that can be occupied by certain information (i.e. properties), and
their adjacencies or neighbourhoods. Within a neighbourhood, elements may be considered adjacent in
a number of ways: either by a string-like communication channel (like a graph), or in the manner of
intersecting open regions (see ﬁg. 6), as in algebraic topology. In either case, the connectivity implied by
adjacency leads to an implicit graph structure. Whether that graph structure is real or merely a map (like
a coordinatization) might not be possible to say.
9

As an idealization, Shannon’s model of source, sink and channel is particularly useful here, as it
uniﬁes the idea of set intersection with that of an graph-like adjacency channel [5].
Source
Receiver
S
R
S
R
U
Figure 5: Shannon’s communication channel uniﬁes the overlap of sent and received message domains
with a simple graphical view of what adjacency means in terms of information. Adjacency is a channel
by which information can be shared.
Any source element may be considered a transmitter or a receiver. If there is a communication channel
between them, we may consider them to be adjacent. A communication channel could be thought of as
a signalling channel, as is the case in communication networks, or the virtual particle interactions of
Feynman diagrams. The nature of a signal from one to another may be assumed discrete and symbolic,
as in Shannon’s model of the discrete channel [5].
3
Mathematical structures of spacetime
Let us now review how these concepts emerge in a number of well known mathematical formalisms for
space. This helps to compare and contrast the different ideas, preparing the way for an autonomous agent
viewpoint which will be a basis for all other notions.
3.1
Sets and Topology
The most basic mathematical notion of a space comes from elementary topology. Topology deals with sets
of elements, often called ‘points’. These elements are grouped into sets which may overlap of intersect
(see ﬁg. 6), and thus cover space. Sets of points allows us to talk about three important properties of
spatial extent: continuity, connectedness and limiting convergence or path ordering. A topology is then a
set of points with neighbourhoods that satisfy certain axioms.
At such an elementary level, it is not possible to assign unambiguous meanings to basic elements and
concepts (this is one reason why we need a semantic theory of space). The ‘size’ of an element (e.g. a
‘point’) is not deﬁned in this description; we imagine points as semantic primitives, and say that questions
of the size or extent of points are meaningless. They may or may not have internal structure; but, if they
do, it is irrelevant to the notion of the space they form. Points are merely ‘agents’ that exude the property
of location.
A neighbourhood of any element p in a set S, is a subset N of S that incorporates an open set U
including p: p ∈N ⊆S. We may equivalently say that p is in the interior of the neighbourhood N.
An open set is one that does not include any of its boundary points, for some notion of a boundary. It is
10

s
s
s
s
s
s
s
s
0
1
2
3
4
5
6
7
start
end
Figure 6: Overlapping patches form a notion of connected spatial regions
usually the generalization of the open interval in the real numbers R1, but this is concept is meant to be
used ﬂexibly. For a manifold, a neighbourhood is essentially an open ball (see ﬁg. 7)2.
Figure 7: Neighbourhoods and open sets
Three concepts are particularly important:
• Continuity of functions, which means that small changes of the space lead to small changes of the
function.
• Connectedness, which implies that a space is not a union of disjoint open sets.
• Compactness generalizes the notion of closure of the set (including boundary). If formed from
bounded sets, the the points are naturally close together in some sense. If unbounded, all points are
deﬁned to lie within some ﬁxed ‘distance’ of one another. Compactness is also a ﬂexible concept,
depending on the notion of boundedness. Clearly there is no clear notion of distance either at this
stage, so it is a heuristic. For example [0, ∞) is not compact since inﬁnity is unbounded.
3.1.1
Vectors between elements and direction
Directionality is normally associated with vectors, which in turn are normally associated with Euclidean-
Cartesian vector spaces, but we are free to deﬁne the idea of a vector as a direction by associating any
pair of elements in any space. For instance, an elephant vector is shown in ﬁg. 8 from a set of different
elephants.
Unit vectors are always deﬁned from adjacent named elements. The elements of space we consider
adjacent must be deﬁned at a given scale. If there is internal structure within an element, this can be
ignored. In a set S with adjacent elements si ∈S, a vector is some function of the two elements
⃗s(ij) = f(si, sj).
2The concept of an open set or open ball in algebraic topology is also subject to some ambiguity, which ultimately arises from
our expectation about points in space having no size. Such a ball represents a unit element of space.
11

If two elements are not adjacent, we may still write a vector as two points, consisting of potentially
several transits (often called hops), but we should be careful of the issue depicted in ﬁg. 2 of assuming
that vectors represent straight line routes from one element to another. We might call such a vector a path
or a route, i.e. a string of transitions belonging to the graph of adjacencies. We are used to being able
to assume the structure of intermediate points, by the homogeneity, isotropy and apparent continuity of a
Euclidean spacetime lattice. No such assumption can be made about general spacetimes.
1
2
3
4
5
Figure 8: A vector is any ordered pair of similar elements in a space
3.1.2
Coorinates, bases, matroids and independent dimensions
In Euclidean space we have a clear notion of dimensionality from everyday convention. However, if we
are constructing space from sets of elements, dimensionality is by no means given. If we are given a set of
bricks, we may arrange them into a line, a plane or a three dimensional block at will. Thus, dimensionality
is a matter of intent, i.e. semantics, as we build from the bottom up (see appendix A).
Our understanding of dimensionality of space is based on independence of vectors, or what is called
a basis or spanning set. Note that this is coordinate semantics, hence dimensionality will emerge to be a
choice rather than a deﬁnite reality that ultimately is limited only by the maximum number of elements
in the base set.
The analogue of a basis for sets is called a matroid. A matroid captures and generalizes the notion of
linear independence familiar in vector spaces (see below). There are several independent ways to deﬁne
matroids. Given a ﬁnite set E, one may choose a family of subsets I of E, called the independent sets.
Just as independent vectors don’t need to be orthogonal, the independent sets may overlap as long as
each subset in I contains points unique to itself. This mimics the idea that two vectors can be independent
as long as they are not collinear. One can exchange points between the independent sets in a matroid, just
as one may alter the angle between two vectors.
A matroid is thus a pair (E, I) of a set of elements and a spanning set of subset patches. For a ﬁnite set
E, there is a limit to have many sets can be formed containing unique elements and without overlapping
completely. The rank of a matroid is the number of independent sets. As set is maximal if it becomes
dependent by adding any element of E. Clearly the rank cannot exceed the size of base set E. Just as
we may organize bricks into several different dimensional arrangements, so the dimensionality of a set is
deﬁned by a speciﬁc matroid basis. This non-uniqueness of dimension is unfamiliar from our ideas about
vector spaces.
An element of the rank r dimensional set is not a single element of E, but a tuple of elements from
each set in I that become associated through this spanning basis. Thus, the dimensionality of a set is
a matter of choice, provided there are sufﬁcient base elements to construct tuples, with each coordinate
component represented by an element from a named independent set.
12

E
Figure 9: A matroid of rank 5 on E. The dotted subset is not independent of the others, as it is spanned
by the intersection of two independent sets.
3.2
Categories and algebras
Category Theory is a very general way of classifying structures and arrangements. A category comprises
a collection of things (objects) and arrows (morphisms) f which behave as functions. Arrows map from
a domain to a co-domain via a function
f : A →B
(2)
g : B →C
(3)
h : C →D
(4)
Under composition, the arrows must be associative
h ◦(g ◦f) = (h ◦g) ◦f
(5)
There is operator ordering from right to left. There are right and left identity arrows, for any arrow f:
idB ◦f = f
and
f ◦idA = f
(6)
Categories have names, e.g. the category of sets is called Set, when arrows represent total functions. A
monoid M is a set with a structure in its domain, that satisﬁes
(x · y) · z = x · (y · z),
∀x, y, z ∈M
(7)
and has identity e ∈M. It is a homomorphism (it is also a semigroup that has an identity. A partially
ordered set or poset (P, ≤P ) has a relation ≤which preserves ordering, i.e. can represent monotonic
functions.
1. The category 0 has no objects and no arrows.
2. The category 1 has one object and one arrow (the identity).
3. The category 2 has two objects, two identity arrows and one arrow from one object to the other
(see ﬁg. 10)
4. The category 3 has three objects, three identity arrows and two arrows between the objects (see ﬁg.
11)
13

A
B
id
id A
B
f
Figure 10: Category with two objects 2
A
B
id
id A
B
f
C
g
h
Figure 11: Category with three objects
3.2.1
Operators and total functions
In connecting discrete objects together we shall frequently have use for the notion of an operator (such as
in matrix multiplication and map composition) as a total function.
An inner product of mappings may be deﬁned if we have operators (functions) O1 and O2, where O1
maps from a domain D to co-domain I, and O2 maps from domain I to co-domain C:
O1 : D →I
(8)
O2 : I →C
(9)
Then these two operator functions may be joined into a single total function so that their product maps
directly from the domain D to co-domain C, eliminating all reference to the intermediate range I:
O2 ◦O1 : I →K,
(10)
We ﬁnd examples of this kind of product in tuple spaces of matrices, as well as in graphs and the more
complex bi-graphs. The inner product is possible when transformations are compatible with one another,
by being smoothly joinable across an intermediate domain.
An inner product can be performed on any two operations, but there is a special signiﬁcance to auto-
morphic functions that take one domain onto an image of itself, such as with ‘square matrices’. These are
particularly important for representing global symmetries of spacetime.
3.2.2
Diagrams as categories
Diagrams are graphs with functionally labelled edges, and are categories if each object is labelled, and
the directed edges are consistently labelled with edges f, g, h, . . ., etc, with domains and co-domains.
Diagrams that form categories should not be confused with diagrams of categories.
A diagram is said to commute if all the paths from one object to another are equivalent and equal In
ﬁgure 12, the diagram commutes if the compositions of morphisms from domain A to co-domain D are
equal, i.e. f ◦g = f ′◦g′, despite the fact that the two functional routes pass through different intermediate
co-domains B and C.
An object is initial (a source) if it has exactly one arrow to a neighbour, and it is ﬁnal (a sink) if it
receives one arrow from a neighbour.
14

C
D
A
B
f
g
f’
g’
Figure 12: Diagrams and commutativity. This commutes if f ◦g = f ′ ◦g′
3.3
Vector spaces
The notion of a vector space is what we normally associate with Euclidean space of the natural world,
with tuples pn of coordinates (x, y, z, ...). A vector is essentially deﬁned by two points ⃗v = (p1, p2).
This is a vector rather than merely a tuple space because the key structural properties lie in the transitions
between the coordinates (the vectors) rather than the points themselves.
A vector space has the algebraic structure of a group, and satisﬁes group axioms for two kinds of
operation: + and product. If V is a vector space, containing vectors, then it satisﬁes the following axiom
under combination:
1. Closure: ⃗u + ⃗v ∈V,
∀⃗u,⃗v ∈V .
2. Associativity: ⃗u + (⃗v + ⃗w) = (⃗u + ⃗v) + ⃗w.
3. Identity exists: ⃗u +⃗0 = ⃗u.
4. Inverse exists: ⃗u + (−⃗u) = ⃗0.
5. Commutation: ⃗u + ⃗v = ⃗v + ⃗u.
Under scaling by a ﬁeld λ, the vectors satisfy:
1. Closure: λ⃗u ∈V,
∀λ, ∀⃗u ∈V
2. Associativity: λ(⃗u + ⃗v) = λ⃗u + λ⃗v, and (λ + µ)⃗u = λ⃗u + µ⃗u.
3. Identity exists: 1 ⃗u = ⃗u.
There is no formal restriction on the commutativity of scalar multipliers, but this is generally assumed as
given. The associativity under + implies that we can enumerate the inverse:
(1 + (−1))⃗u = ⃗0
(−1)⃗u = −⃗u.
(11)
3.3.1
Group lattice
Vector spaces are special in that they have an assumed group structure, which we take very much for
granted. This is sometimes called a lattice structure. A lattice is an ordered, regular tiling of vectors
translations that leads to a simple tuple structure. For many it is hard to set aside this intuitive notion of
dimensionality, as it models the world as we experience it as humans at the macroscopic scale.
Every point is homogeneous with respect to the options for translating along a vector to a new point.
Homogeneity of a space implies uniform properties as one follows a single direction. Isotropy implies
the appearance of uniform properties in all directions around a point of observation.
15

3.3.2
Naming of points on a vector space: coordinate systems and bases
Points in an n-dimensional vector space are named numerically in tuples. A coordinate tuple is often
written as a column vector3, here shown decomposed as a linear combination of orthonormal basis vec-
tors:
⃗v =




1
2
0
7



= 1




1
0
0
0



+ 2




0
1
0
0



+ 0




0
0
1
0



+ 7




0
0
0
1




(12)
This has the structure
⃗v = (λ1, λ2, . . . , λn) =
n
X
i
λi⃗ei
(13)
where ⃗ei are called basis vectors, which satisfy the orthonormality property, and ⃗λ is the tuple of vector
components measured relative to that basis.
⃗ei ◦⃗ej = δij
(14)
where δij is the Kronecker delta (δ = 1 iff i = j, else zero).
3.3.3
Rank, linear independence and dimensionality
A Euclidean space (or generally a manifold) of dimension n is formed by the outer product of independent
sets isomorphic to the real line R1, thus Rn = R1 ⊗R1 ⊗. . . ⊗R1. From the generalized viewpoint of
matroids, or coordinate bases, these real-line sets are the independent sets of the matroid, and a point in
the n-dimensional space is really an association of n-elements from the n spanning sets.
Normally, one expresses this in the following way. A vector space is said to be n-dimensional if
there exist n linearly independent vectors ⃗ei (basis vectors or generators), which are pairs of tuples that
originate from an arbitrary point (called the origin), and point directly to each independent set. Linear
independence means that there is no non-zero value of λi such that
n
X
i=1
λi⃗ei = 0,
(15)
⇒λi = 0,
∀i
(16)
We are so familiar with the idea of Euclidean dimension that the dimensions of other structures are
often deﬁned implicitly by immersion into a Euclidean space of some dimension. The smallest number of
Euclidean dimensions required to embed something then becomes a deﬁnition of dimensionality. How-
ever, this might be interesting in some geometrical sense, but it is not a true measure of the number of
degrees of freedom available to an element at some location (see appendix A.4).
In group theory, the algebras which generate the equational structure of the group may also be drawn
as vectors known as the roots or weights of the algebra in a given dimension. The rank of the algebra is
the number of independent generators, however there might be more members in a tuple which represent
the algebra, with some that are not independent.
3A tuple is sometime loosely called a vector, where it is assumed to be measured relative to the origin or tuple composed of all
zeros.
16

3.3.4
Tensors, order and rank
Tensors are indexed arrays of values that transform correctly between different coordinate bases. Vectors
are tensors of order 1. Matrices have order 2. Objects of higher dimension can also be formed:
Ti
(tuple)
(17)
Tij
(matrix)
(18)
Tijk...mn
(higher order)
(19)
Rank is distinct from order. Rank refers (as with coordinate spans) to the number of independent sets
composing the object. A matrix (which has order 2) has rank one if it can be written as an outer product
of two non-zero rank-1 vectors:
Tij = viwj
(20)
The rank r is the smallest number of such outer products that can be summed to produce the tensor.
Tij =
r
X
n=1
v(n)
i
w(n)
j
(21)
3.3.5
Distance and the inner product
Vectors have inner products that reduce a matrix of dimension l × n and n × m and produces an element
of dimension l × m. The inner product of two vectors ⃗a,⃗b with components ai, ab relative to some basis
⃗e with components ei is:
a ◦b =
X
i
aibi = Lab
(22)
This has a geometric interpretation as a relative length. If⃗a = ⃗b, then this reduces to Pythagoras’ result for
the square length of the vector. For orthogonal vectors it vanishes, providing the orthonormality condition
in equation 14. In all other cases, this inner product gives a scalar result that is the scaled projection of
one vector along the other, somewhat like a geometric average of their extent. This is an example of an
inner product as described in equation 10.
Fig. 2 shows how we could potentially deﬁne distance by different measures in a vector space. If
the space is continuous, we may deﬁne effective distance by Pythagoras’ theorem. If there is not direct
connection along the hypotenuse of the triangle, then the discrete distance is found by summing the bond
lengths of every atom along the edge however. In a discrete space, there is no way to take the direct
Pythagorean route, but we might still interpret this as an ‘average’ measure of the effective distance.
3.3.6
Matrices and transformations
One of the ﬁrst places for the appearance of semantics in spatial models arises with the introduction of
structure through transformations, usually symmetries. Variable observability, with transformations to
map between them, implies a relativity for local observers.
For a matrix with components Mij, i = 1 . . . Rows(M), and j = 1 . . . Columns(M). In a square
matrix Rows(M) = Columns(M). Square matrices are a necessary condition for closure under multipli-
cation.
Matrix multiplication is an inner product (see section 3.2.1), deﬁned by M ◦N = P
j MijNjk, which
is possible if the number of rows in the ﬁrst is equal to the number of columns of the second matrix.
17

⃗X =





X1
X2
...
Xn




.
(23)
In refs. [6–8], a bounded n-dimensional Euclidean model was used to model the memory of a computer
system.
As large as current systems may be, they remain ﬁnite and can be modelled by ﬁnite vectors. We
deﬁne relative operators for individual parameters in a state as
 a
b
c
d

◦
 A
B

=
 aA + bB
cA + dB

(24)
 a
b
c
d

◦
 A
B
C
D

=
 aA + bC
aB + bD
cA + dC
cB + dD

(25)
Matrix multiplication is associative, and may or may not be commutative (path dependent).
3.3.7
Derivatives and vectors
A vector is a difference of tuple elements. If T is a generator of a translation, we may write:
⃗v = p2 −p1 = (T −1)p1
(26)
A derivative is a measure of how quickly a function changes with respect to a constrain parameter. It
measures differences. In Euclidean space, we have the classic Newton-Leibniz deﬁnition of a derivative
df
dx

x
= lim
dx→0
f(x + dx) −f(x)
dx
(27)
This can be written mode simply in terms of atomic succession operator + like this:
dfx = fx+ −fx
(28)
The length scale implied by the difference between x and x+ is now irrelevant as it is assumed to be the
nearest neighbour spacing. On a lattice, the smallest distance between neighbours has to be considered
constant, as there is no objective way to measure it from inside the lattice.
Thus unless we can deﬁne distance in a simple way, we cannot measure rate of change, or derivative.
Ultimately, these deﬁnitions all become mutually intertwined, more self-consistent web of ideas than a
hierarchy of axioms4.
3.3.8
Boundaries, subspaces and immersion of structures
We may embed subspaces hi of lower dimension into a Euclidean space Rn by immersion. These are
often called hypersurfaces and may be deﬁned in the form of additional constraints of the spanning set of
independent vectors:
χ(⃗v) = 0
(29)
4This too kind of self-consistent web will become important and explicable in the context of semantic networks below.
18

By adding such additional constraints with such an equation (analogous to (16)), we may reduce the
number of independent vectors leading to a smaller solution set for ⃗v with reduced dimensionality. For
example, a two dimensional sphere S2 (the surface of a three dimensional ball) may be embedded within
R3 by direct immersion. Moreover, by ﬁnding equivalence classes of a group generator, a vector space
can be decomposed into subspaces or non-overlapping hypersurfaces. This property of embedding is
order
Figure 13: Hyperplane (subspace) decomposition of a space
often used to deﬁne the dimensions of elements who dimensionality is harder to understand (like graphs).
3.4
Manifolds and Minkowski spacetime
If we try to add an independent description of time to a Euclidean space, then we must introduce a
coordinate t. Thus one immediately has the velocity of one observer with respect to an element as ⃗v = d⃗x
dt .
Again, the velocity of everything except massless signals depends on the way one chooses to measure
space. The constancy of the speed of light seems to indicate that this is a primitive form of transmission,
as one expects in a discrete spacetime.
In Newtonian mechanics the Galilean group generates the transformations that relate different ob-
server speeds and directions. Taking into account Einstein’s considerations, based on our knowledge that
the speed of light and other massless signals in a vacuum appears constant, one ﬁnds that the Poincar´e
group generates the correct transformations in a 4 dimensional representation (see section 3.4).
A manifold is the generalization of a locally Euclidean space, whose global structure might possess
curvature. The surface of a sphere is an example. Pythagoras rule, and sum of angles in a triangle do not
object Euclidean rules.
In modern formulations of Einstein’s special relativity, one treats the constancy of signal propagation
(the speed of light) as a constant, which we’ve established is entirely natural in a discrete spacetime.
Expressing this as an invariant inner product on Euclidean space R3
X
i
dxidxi = c2dt2
(30)
allows a four dimensional space to be deﬁned with inner product
ds2 ≡(−cdt, ⃗dx) ◦(cdt, ⃗dx)T
=
0
(31)
ds2 = −c2dt2 +
X
i
dxidxi
=
0
(32)
This is the spacetime generalization of Pythagoras’ theorem, but unlike Pythagoras which is a spherical
constraint this is a rescaling of spatial elements with respect to elements of time. It is sometime written
19

symmetrically using an imaginary time coordinate idt, or one may introduce a metric tensor, analogous
to the Kronecker delta:
ηµν
=
diag(−1, 1, 1, 1),
(33)
dxµη ν
µ dxν
≡
(cdt, ⃗dx) ◦(cdt, ⃗dx)T = 0
(34)
where µ, ν = 0, 1, 2, 3 and the zeroth dimension is now time scaled by the speed of light. This metric
tensor is useful as it generalizes to curved spacetimes used in general relativity and other problems of
curvilinear coordinates. Alas, that is a large topic that goes beyond the scope of this paper.
The symmetry group structure that makes this 4-dimensional extension of Euclidean space behave
like local observer transformations in Minkowski spacetime is generated by the Poincar´e group transfor-
mations.
Notice that, by extending time into the very fabric of a spatial coordinate description, we have made
a fundamental choice to embed a clock into the coordinate system itself. The Poincar´e transformations
describe apparent changes in the clock rates for different observers because, the assumption of a ﬁxed
universal measuring scale is incompatible with the constancy of the speed of light. Formally, the negative
sign for time in ds2 makes this an equation of constraint, which means that spacetime is a constrained
surface. This is a reasonable starting point on which to build a model of spacetime, but if one looks more
carefully from a local-observer viewpoint, it is a simpliﬁcation of the issues. It homogenizes all observers
to have the same idea of time and space with minor adjustments. In other words, it imposes uniform
semantics on to spacetime.
3.5
Graphs or networks
The ﬁrst discrete notion of a space that generalizes lattices, and has minimal initial semantics, may be
found in graph theory. A graph G = (V, E) is a collection of nodes or vertices V and edges E between
nodes. The space is the tuple of vertices, and their structure and dimensionality are determined from the
topology of the edges. A graph is said to be acyclic if it contains no loops. A forest is a collection of
possibly disconnected tree-like or acyclic graphs.
In a general graph, there is no implied regularity of structure as there is in the tiling patterns of a group
lattice. Graphs may be directed (with arrows marked in one direction) or undirected (without arrows) in
which case both directions are implied (see the example in ﬁg. 14).
The familiar concepts of dimensionality and direction begin to unravel when we turn to graphs. On the
other hand, graphs help us to see the issues of space more clearly than any other representation, because
we are forced to separate our notion of dimension and direction (as deﬁned by a basis) from our ideas of
adjacency, change and connectivity.
3.5.1
Adjacency and matrix representation
A graph, may be represented as an ordered tuple of nodes or vertices. The connectivity (adjacency) of
nodes in the graph may be either directed or undirected. In either case, the entire graph may be represented
by its adjacency matrix A, whose rows and columns completely describe the graph. For example ﬁg. 14)
has adjacency matrix
Aij =




0
1
1
0
0
0
1
0
1
1
0
0
0
0
0
1




(35)
where i, j run over the rows and columns, i.e. the ordered vertices.
20

1
2
3
4
f=4
f=1
f=7
f=0
Figure 14: A four vertex graph G4, with a function f(G).
The matrix element Arc of the adjacency matrix A may be thought of as the generator of a translation,
or a degree of freedom Ar→c. That is, Arc = 1 if r points to c. The symbol λ is used as an unspeciﬁed
eigenvalue of a matrix. ρ is reserved for the principal eigenvalue, or spectral radius of a strongly connected
graph. A graph which has repeated eigenvalues in its spectrum is said to be degenerate.
The in-degrees of the nodes, i.e. the numbers of links that point to each node, is denoted by the vector
⃗kin; the out-degree vector, or number of outgoing links per node, is denoted ⃗kout. Recalling that Arc
implies r = row and c = column, then in a directed graph, the row-sum of the adjacency matrix is the
out-degree of the node whose row is summed:
X
c
Arc = kout
r
.
(36)
Similarly, the column sum is the in-degree of the node:
X
r
Arc = kin
c .
(37)
A square, leading-diagonal sub-matrix of A is written arc, with r, c now running over a limited set of
values. A non-square, off-diagonal sub-matrix of A is written ℓrc.
3.5.2
Strongly connected components
The concept of an ‘SCC’ is useful in describing regions of a directed graph [4].
Deﬁnition 2 (SCC - Strongly Connected Component) Let G be a directed graph. A strongly connected
component of G is a maximal subgraph, g, in which there exists a directed path from every node in g to
every other node in g, by some route.
It is implicit in this deﬁnition that a path follows the direction of the arrows (links). The set of SCCs
is uniquely determined by the graph, and every node belongs to one and only one SCC. The number of
SCCs for a graph with N nodes may be as large as N for a directed acyclic graph or DAG, or as small as
one. In the latter case we simply say that the graph is strongly connected (SC).
The weaker property of being connected simply requires that every node in a connected subgraph has
some path to every other, disregarding the direction of the links (ie, treating them as undirected). We will
discuss only connected graphs in this paper.
More important is the notion of a complete subgraph, or Complete Connected Component (CCC).
21

Deﬁnition 3 (CCC - Completely Connected Component) Let G be a directed graph. A strongly con-
nected component of G is a maximal subgraph, g, in which there exists a non-directed path from every
node in g to every other node in g, by direct adjacency.
The Perron-Frobenius theorem addresses the property of reducibility in a graph [9, 10]. Each irre-
ducible region can be associated with an SCC, for the following reason. A real N × N matrix M, with
non-negative entries (such as an adjacency matrix), is said to be irreducible if every element, labelled as
a row-column pair (r, c), is greater than zero in some ﬁnite power of the matrix, i.e., for every pair (r, c),
there is a p such that (M p)rc > 0. The adjacency matrix A of a strongly connected graph is irreducible,
since (Ap)rc is just the number of paths from r to c of length p. Conversely, if a graph is not strongly
connected, then it is reducible.
3.5.3
Functions f(x) on a graph
A graph may be represented in a functional form by grouping the vertices into an n-tuple. The naming of
vertices is accomplished by treating them as a poset, i.e. by simply numbering them uniquely. This is a
global operation. Indeed, graphs do not support the concept of locality even in disconnected components.
The values in the tuple the represent the values of a function on the vertices, analogous to f(x):
f(G4) =




1
7
0
4




(38)
The edges of the graph may or may not have a realization, i.e. a speciﬁc interpretation. Any asso-
ciation will do to relate to elements of the graph. Graphs are formed by many structures. In statistical
analyses, correlations matrices lead to graphical structure for example. Analysis of their properties has
considerable importance to revealing trends and clustering properties in the space of correlation distance.
In such a case, the row and column elements are not 1 or 0, but a value of the correlation function, acting
as a relative weight or distance in the graph.
The edges of the graph may also be viewed as generators of a transformation from a tuple of names for
the vertices onto its image. Since the tuple is a representation of the space, the operation of the adjacency
matrix acts to generate a rotation of the space in the direction of the arrows.
A : V →V
(39)
The ﬁxed points of this map lead to an eigenvalue distribution over the graph, whose principal eigenfunc-
tion describes the relative centrality of the nodes (see [4,11]).
Self-loops (like identity elements of a category) are important for global stability of automorphic func-
tions on graphs, as they avoid singularities by allowing ‘pumping’ of functional value at a source node,
and ‘orbiting’ at sink nodes. Without such nodes directed graphs that point to sinks act as singularities
that suck up all distribution value.
3.5.4
Distance on a graph (hops)
Apart from the weighting of links on the edges of the graph, one can measure the distance between any
two nodes as the length of the path one follows from one node to another, in ‘hops’, by summing the
values of Arc along the path.
22

1
3
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
2
path 1
origin
Figure 15: A graph and the tuples measured along its spanning tree.
3.5.5
Linear independence, matroids and spanning trees
A notion of linear independence does not exist for a graph, but we may use the matroid concept to deﬁne
dimensionality of a region in terms of sets of edges.
Suppose we take the base set E to be set of edges in a graph G = (V, E). Then we may form a
matroid basis of given rank. The individual edges may be considered independent sets provided they
form a forest, i.e. provided there are no cycles in the graph. This is simply the deﬁnition of a spanning
tree in network parlance.
If there are any loops, the extra edge simply short-circuits two nodes so that there are two routes to
the same location, implying that they are dependent. Each independent edge has to take us somewhere
new in order to be an independent edge (see appendix A.1).
The dimensionality of a graph is sometimes deﬁned by the immersion of the graph into a Euclidean
space. Another deﬁnition involves the number of independent colours needed to label adjacent nodes
without the same colour being directly adjacent. However, if we follow the pattern of basing dimension-
ality on matroid rank, and the association of tuples spanned by the independent sets then the interpretation
of dimension is clear. The tuples of an n-dimensional graph are spanned by the edges from some origin
node or tree root along the unique paths of a spanning tree, of rank or length n.
The independent sets are thus rank r unique paths pk from some origin to the leaves in a spanning
tree for the graph. These constitute the independent directions in the graph. Then a tuple has the form
(p1, p2, . . . , pr), where the path ⃗pk = {vp} is the set of SCC effective vertices that lie along it. This
means we must label all vectors between non-adjacent neighbours by the independent path by which we
intend to traverse the graph order to label their independent role, e.g. ⃗ep1
1n. This is the equivalent of
specifying a basis vector with a ﬁxed tuple label in a lattice. For leaf nodes, this is unique.
This is only one coordinatization of the graph. Just as one may have Cartesian or polar coordinates in
Euclidean space, so there are many options for parameterizing a graph as a multi-dimensional object. See
appendix A for an discussion of graph dimensionality with examples. The key ﬁnding is that, unlike the
case of a regular lattice, dimensionality and adjacency by symmetry generator are independent concepts
– they are not interchangeable as they are in a group lattice.
Graphs that exhibit no special symmetries are fundamentally bounded structures. Since the different
paths form strings composed of different numbers of elements, spacetime is bounded anisotropically, i.e.
the size of a graph space is different along its different directions. Moreover, these properties depend
on the speciﬁc choice of root node or origin, so space is also inhomogeneous. In a directed graph, there
is moreover the curious property of singularities in space into which one may move but never return, or
from which something can emerge but not be absorbed. This is reminiscent of the idea of monopole and
23

charges in physics, or singleton objects in information technology.
The length of the paths is important in deciding the range over which the tuple values run (since
one SCC might container more vertices than another), as well as the number of vertices in any strongly
connected components along the path. In ﬁg. 15, nodes 7,8,9 all belong equivalently to a single SCC
along the path to node 10. There is thus a local symmetry transformation that implies hidden degrees of
freedom at this stage of the path.
The tuple coordinates label symmetrical elements within SCCs. So the extent of tuple coordinates
depend on how many different nodes we can reach in the strongly connected region along a given path.
Movement within the CCCs behaves like ‘hidden dimensions’ embedded within the larger spanning set.
3.5.6
Scale transformations a graph
Scaling in a graph is not a simple matter of multiplicative renormalization of measures. Graphs are not
self-similar in general, and thus such a scaling has no meaning. Figure 4 shows what we mean by scaling
in a graph. In general, we have seen that the coordinatization of a graph depends mainly on the edges in
between completely connected regions.
At a given scale, we may form a coarse graining of the graph by constructing the strongly connected
components up to a ﬁxed number of nodes n, or horizon. One ends up with new single elements in place
of CCCs, linked by a new matroid, bounded by a given number of hops. This is a renormalization of the
graph of level n.
Repeating the process, one may perform further renormalizations, until the graph is a forest. This
forest yields the large scale connectivity of the graph.
3.5.7
Derivatives and vectors
A vector on a graph is simply a pair of vertices (i, j), where i, j ∈V . It is natural, though not necessary,
to restrict vectors to only directly connected vertices, so that valid vectors are those for which there is
a non-zero element in the adjacency matrix. If we consider vectors linking nodes that are not directly
connected, implying a path or a route composed of multiple hops, a vector might not exist or if it does it
might not be unique.
The partial derivative of a function deﬁned on the tuple of vertices, may be deﬁned along nearest
neighbour edge k, at node i, as
d⃗fk

i = fi+ −fi,
(40)
where + is the succession operator along k.
3.5.8
Lattices from irregular graphs
Our familiar notions of spacetime involve tiled lattices, so when we think of graphs as spatial, they should
scale up they have to lead to familiar topologies.
Like a lattice, we can expect there to be a connection between neighbouring points in the tuple space
spanning a general graph. In a lattice we generally expect that (1,0,0) and (2,0,0) are adjacent and con-
nected unless there is a boundary. In a graph, we cannot assume that the neighbouring point exists to a
uniform value, as a particular direction can suddenly end in a cul de sac. However, (1,0,0) and (0,1,0)
might actually be the same point. Paths are not orthogonal.
Question: What requirements need to be imposed to ensure large scale lattice structure from local
graphs?
24

3.6
Symbolic grammars as spatial models
Information sciences are principally about the recognition and manipulation of discrete sequential string-
like patterns called languages. Languages are formed from alphabets of symbols. A symbol is a spatial
pattern, or may be representative of one. This allows us to compress spatial representations, deﬁne the
notions of semantic index and code book (see section 6.7).
The syntax of any language is the set of all allowed strings of symbols. It can be modelled by a
general theory of its structure, called a grammar. Grammatical methods assume that arriving data form a
sequence of digital symbols (called an alphabet) and have a structure that describes an essentially hierar-
chical coding stream. The meaning of the data is understood by parsing this structure to determine what
information is being conveyed. The leads us to the well-known Chomsky hierarchy of transformational
grammars (see, for instance, [12]).
Language is clearly something closely associated with semantics, so a pseudo symbolic structure for
space is of considerable signiﬁcance to our interpretation of its meaning. Documents are nothing if not
spaces of structured symbols which are assigned explicit meaning.
Because of their regularity and conduciveness to formalization, computer science has seized upon the
idea of grammars and automata to describe processes. Symbolic logics are used to describe everything
from computer programs and language ( [13]) to biological mechanisms that describe processes like the
vertebrate immune response ( [14]). Readers are referred to texts like [12,15] for authoritative discussions.
For a cultural discussion of some depth see [16].
3.6.1
Dimensionality and topology in languages
Languages are usually written explicitly in a one-dimensional representation, in the sense that they are
viewed as sequences of symbols. However, context-free grammars support the embedding of parenthetic
sections. However, we are familiar with the idea that a sequential book can embed structures like chapters
and footnotes. Such ‘digressions’ belong in a different dimension to the main thread of a storyline.
One only has to think about the storage of information on a computer to understand how dimension-
ality is an issue of interpretation. Imagine a two dimensional table, encoded as a one dimensional stream
of bytes read from a three dimensional array of disks. What appears to be a connected stream is generally
stored in very disjoint locations within a storage array. What information science refers to as a virtual
view of data is simply a redeﬁnition of spacetime semantics.
3.6.2
Automata as classiﬁers of grammatical structure
A grammar is a multi-dimensional structure encoded as a linear stream of symbols taken from an alphabet
Σ. The complexity of patterns in a language may be classiﬁed according to the level of sophistication
required to compute their structures. Chomsky deﬁned a four-level hierarchy of languages called the
Chomsky hierarchy of transformational grammars that corresponds precisely to four classes of automata
capable of parsing them. Each level in the hierarchy incorporates the lower levels: that is, anything that
can be computed by a machine at the lowest level can also be computed by a machine at the next highest
level.
State machine
Language class
Finite Automata
Regular Languages
Push-down Automata
Context-free Languages
Non-deterministic Linear Bounded Automata
Context-sensitive Languages
Turing Machines
Recursively Enumerable Languages
State machines are therefore important ways of recognizing input, and thus play an essential part in
human-computer systems.
25

Time progresses in an automaton or state machine by the arrival of each new symbol in a language
string, even a blank one. Thus the automaton that parses a language is its own clock. Space and time are
explicity matched one to one.
3.6.3
Naming of elements in a grammar
A grammatical structure may be used to span a space, such as a document. Its regions form a matroid,
(like a spanning tree in the case of a hierarchical grammar) of strictly non-overlapping elements.
The atomic elements of such a space are the symbols of the language alphabet themselves, and but
there may be larger regions of repeated patterns that form regions or coordinate patches, and could be
aggregated in a ‘rescaling’. Indeed, during data compression it is the replacement of extended patterns of
atomic symbols with single ‘meta-symbols’ that reduces the size of the space. Take the following string
of symbols from the alphabet: Σ = {A, B, G, N, Y, Z, ), (}.
XXX(YZA(BG))ANXXX(A)...
We notice a repeated pattern ‘XXX’ which could be aggregated into a region as a rescaled element,
analogous to an SCC, or an independent set.
Regular expressions (or patterns of an embedded regular language) can, for example, represent larger
strings or ‘words’ in a language which repeat and are attached signiﬁcance. This is a common way of
parsing documents in information technology.
Another parenthetic language which used extended symbols for begin and end parenthesis is the
Hypertext Markup Language used in the World Wide Web. The semantics of each region are encoded
into the name for the relevant parenthesis.
<html>
<body>
<h1>My title</h1>
This is some body of text, where the context has no interpretation
to the HTML language, it is a human language embedded in a machine
language.
</body>
</html>
3.7
Bigraphs: nested structures
Graphs have many important qualities, but they are elementary structures. Let us now consider a gen-
eralization of graphs with intentional semantics, using some concepts from category theory. This work
was pioneers by Milner in an attempt to seek a language for describing spatial arrangements in computer
science [17].
Bigraphs are a composite model of space that add to graph theory a speciﬁc notion of semantics in
the arrangements of things. Rather than discussing boundaries, it talks about interfaces, imagining that
arrangements or fragments of bigraphs will be composed into larger ones. Bigraphs, are thus a model for
machinery.
Milner used a topological categorical model of discrete containers to mark out regions of agency.
This is a non-Euclidean view. Bigraphs describe places (somewhat analogous to the primitive points in a
topological space) and the links between them. The difference is that Milner’s points may contain internal
structure, including holes to be ﬁlled in with other bigraphs, like pluggable modules. This assumes the
notion of an embedding of the structures described within some kind of topological space.
26

3.7.1
Bare graphs, forests and links
Bigraphs merge several points of view in order to add interpretation to a regular graph. A ‘bare’ bigraph
˘G may be seen in ﬁg 16. It contains the basic relationships between elements. vertices vi, some of which
are inside one another, and edges ei. The edges are not labelled in the ﬁgure. In standard graph theory,
v
v
v
v5
4
v2
1
0
3v
1
2
J
J
Figure 16: A bare bigraph
edges are always point to point adjacencies; however Milner follows his own convention here and treats
edges as ‘communication buses’ that can have junctions. Formally this makes bigraphs bipartite graphs.
For clarity, I shall mark the junctions Jℓwith an X symbol on the edge, to indicate the presence of a
junction agent (see ﬁg. 16).
From the single picture in ﬁg. 16, we may infer two views. The ﬁrst is called the forest graph of
˘G (see ﬁg 17). A forest is a treelike structure without the constraint of connectedness. The forest view
v
v
1
2
v3
4
v5
v
v0
Figure 17: A forest view of the bigraph
shows the hierarchical relationships between the vertices, from the largest at the top to the smallest at the
bottom. It indicates the boundaries of containment membranes through which a link must pass in order
to connect vertices. Here large implies the ability to contain small (though size is implicit, since bigraphs
have no internal notion of size). It is typically disjoint unless all vertices are encompassed by a single
root. The hierarchy represents a notion of boundary, as in Gauss’ law: that which can emanate from a
vertex depends on that which is inside it.
The second derived view is the link graph, which illustrates the channels of connectivity (perhaps
communication) between the vertices. Note that without the junction markers J1, J2, these link graphs
look unlike traditional graphs of graph theory.
A bigraph ‘face’ is thus associated with two different sets of description G = ⟨N, {X}⟩, where N is
a natural number representing the countable vertices, and X is a set of links.
27

v0
1v
v3
v2
v4
v5
Figure 18: A link view of the bigraph
3.7.2
Boundaries and interfaces on a bigraph
The forest graph indicates containment relationships, while the link graph indicates connectedness. It is
also possible to denote open ended channels as ‘doorways’ in and out of the graph as interfaces (or simply
‘faces’ in Milner’s language). This will allow bigraphs to be thought of as representing the structure of
pieces of active machinery, like mathematical operators, and further be composed as fragments. This will
be analogous to having matrices in a vector space.
A complete bigraph interface is a mapping from an inner or input face to an outer or output face. We
write the face in angle brackets as a transformation from inner to outer faces:
inner
→
outer
(41)
⟨sites, inlinks⟩
→
⟨regions, outlinks⟩
(42)
For example, in ﬁg. 19, we see a mapping from one inner site to two outer facing regions, and two
incoming links mapping to a single outgoing link:
⟨1, 2⟩→⟨2, 1⟩
(43)
Figure 19: A interface mapping one interior site to two regions, and two input links to one outgoing:
⟨1, 2⟩→⟨2, 1⟩
Site-region interfaces are drawn as meta-locations around the bare bigraph, using boxes (see ﬁg 20).
The outer boxes are regions (sometimes called roots) of the graph are labelled ri, and represent distinct
places. The sites, labelled si, are like holes into which other regions could be plugged. Sites therefore
take on the guise of receptors, as in biology, or sockets as in engineering. Regions are thus the outer face
of elements, while sites are the inner face. They are the closest representation of Euclidean space in this
description.
28

s
s
s
r
r
1
2
3
2
1
Figure 20: A place-oriented interface, with regions and sites
Links can also form interfaces, ready to connect with the connectors at the boundary of other places.
Link interfaces are edges that extrude from the outmost region of a bigraph and ‘wait to be completed’
(like an operator hungry to act on its operand). Conventionally one draws incoming interfaces beneath a
bigraph (as inputs) labelled yj and outgoing interfaces (outputs) above the graph, labelled xi (see ﬁgure
21).
{y }
j
{x }i
Incoming
Outgoing
Inner−face
Outer−face
Figure 21: A link oriented interface
Combining all the features discussed leads to a general bigraph, such as that shown in ﬁg. 22.
r1
0r
v0
v1
v2
v3
y0
y1
y2
x0
x1
0r
r1
s1
v2
v0
v1
s0
s2
v3
s0
s2
s1
Figure 22: A fully featured bigraph, mapping inner to outer faces: ⟨3, {x0, x1}⟩→⟨2, {y0, y1, y2}⟩
3.7.3
Signatures and sorts for semantic typing of spatial structures
The addition of sorts for bigraph vertices is one way of using an algebraic concept to endow them with
elementary semantics. Type and arity with respect to links, for each vertex, provides an operator view
of vertices, making them functional or agent-like, i.e. classifying them with named roles. We’ll see the
same notion of typed agency in promise theory.
The typing of vertices, including their ‘arity’ with respect to edges speciﬁed a functional signature for
the bigraph machinery. In order to arrive at a signature, we have to classify or name the nodes by role.
29

Milner refers to these names as controls.
Signature = {K : 2, L : 0, M : 1}
(44)
For example, the type K might represent a building, and M might represent a fenced area. L might
1
2
J
J
K
K
M
K
L
M
Figure 23: A link oriented interface
represent a table. Or, K might be an oscillator, L an ampliﬁer, and M a coil.
If there is a sufﬁciently detailed typology of controls and their arities, then a signature acts like a kind
of role inventory for a bigraph: a description of the basic functions of its machinery. It will not normally
be sufﬁcient to tell the purpose of the machinery, since that depends on how it is combined with other
bigraphs, however it offers a table of elements for a chemistry of composition, at a molecular level.
3.7.4
Composition of bigraph operators
The composition of bigraphs can be carried out in a number of different ways. The simplest composition
is to simply place two graphs alongside one another without connection. This is called juxtaposition, and
is denoted G1 ⊗G2. It is considered to be a tensor product.
A proper inner product (see equation 10), analogous to a matrix product in linear algebra may be
carried out if the outgoing face of one bigraph matches the incoming face of the other, making them
compatible. A bigraph of arity 2 is like a matrix, and arity 1 is like a tuple. For example, suppose we have
faces:
I = ⟨a, {x}⟩
(45)
J = ⟨b, {y}⟩
(46)
K = ⟨c, {z}⟩
(47)
and bigraphs
G1 = A : I →J
(48)
G2 = B : J →K
(49)
Then we may write
C = B ◦A = G2 ◦G1 : I →K,
(50)
which is the analogue of matrix multiplication Cik = P
j AijBjk. Two bigraphs are said to be incompat-
ible if they cannot be composed, written G1#G2. This process is illustrated in ﬁgure 24, where we see
how the regions of G1 are slotted into the sites of G2, and the outgoing links match up to the incoming
links (as labelled):
Other kinds of combination are possible, where there is an absence of sites. In ﬁg. 25, one deﬁnes
G2|G1 to mean the connection of links and the merging of the regions from the two graphs into a single
region, proving a ‘semi-proper’ inner product.
Similarly, the same operation can be carried out without merging the regions of the graph, as shown
in ﬁg 26, in an ‘improper’ inner product..
30

x
y
y
z
x
y
z
L
L
K
K
=
Figure 24: The composition of two bigraphs by proper inner product (nesting)
x
y
y
z
=
K
L
M
K
L
M
x
y
z
Figure 25: A link composition of two bigraphs merging places, by semi-proper inner product
3.7.5
Time and motion in bigraphs
We have established from the nature of composition that a bigraph is analogous to a matrix in linear
algebra, i.e. it transforms other compatible bigraphs that it operates on with its faces. Such vectors are
not described by Milner, but clearly they exist as bigraphs with only incoming faces (sinks), or transverse
vectors with only out-going faces (sources).
In linear algebra, motion of vectors by translation and rotation occurs by acting on vectors with the
matrices, hence there is an analogous notion of change of motion enacted by the interpretation of bigraphs
as machines that enact change.
Milner’s own idea of bigraphs is that the layout of this machinery itself is what changes over time,
and such changes require a separate kind of transformation known as a reaction rule. The analogy here
to linear algebra is that of a group transformation of a matrix operator.
Time is enacted by reaction rules in Milner’s view, but it may also be marked by changes carried out by
the machinery (analogous to matrix multiplication) as the number of stages of transformation, or number
of intermediate faces (co-domains) through which the machinery passes. His language and formulation
seem anchored to interaction machinery - biological models, and electrical circuits. Spatial containers
have interfaces, somewhat like matrices/tensors, that express dimensionality at the joins between regions.
Bigraphs are important because the theoretical formulation is rigorous and well developed, in spite
if its very particular vision. In particular, its notion of containment semantics as a dual viewpoint to
continuity and translational invariance, is an inspired break from the dynamical traditions of physics,
where symmetry and continuity are the starting points.
The weakness of bigraphs lies in the fact that all changes that can be represented are semantic changes.
This limits what we can say about a environment where dynamics would be a more appropriate formula-
tion of change.
31

x
y
y
z
=
K
L
M
K
L
M
x
y
z
Figure 26: A link composition of bigraphs without merging places, by improper inner-product
3.7.6
Derivatives and vectors
A derivative is a rather strange object for a bigraph. It can still be deﬁned as the difference between two
bigraphs, though it is unclear to what end one would need this concept. Bi-graphs are not a dynamical
model, they are a semantic model. What might a rate of change mean under these circumstances? The
answer is likely connected to the idea of versions and proper time, to be discussed later in these notes.
3.8
Processes algebras, time and motion
In computer science there is a description of ﬁnite system behaviour expressed in terms of labelled state
transitions, or process algebras. A process consists of agents which undergo transitions.
A transition that happens externally might trigger an input of a value x into input i, then the process
continues according to the deﬁnition of P
i(x).P
(51)
Conversely, a process agent P outputs a value on port o() y and continues according to the deﬁnition of
P.
o(y).P
(52)
Transitions can also occur internally or non-deterministically (outside the scope of modelled process) like
‘noise’ on a Shannon channel:
τ.P
(53)
Algebras can be quite expressive at rendering scope and transitions (time).
The iteration implied in the equations is solved, much like a propagation solved for a differential
equation, by tracing the intermediate states.
4
Spacetime semantics
A brief summary of some of the lessons learnt from the foregoing descriptions of spacetime is warranted.
The foregoing reviews can be summarized in a few points.
• Space is a set of adjacencies between basic elements.
• Time is counted by distinguishable changes of state.
• Spacetime plays the role of a measuring system for transitions that can be represented in two ways:
32

– As coordinates (names).
– As transitions (change).
• A set of independent transition matrices form a basis for the allowed changes (i.e. vectors) in a
space. The superposition of these generators of allowed change leads to a global transition matrix,
or adjacency graph, which represents regular symmetries across patches of space.
• Distance, extent and change are somehow equivalent things, all related to transitions. Our common
notions of distance are based on a very special case of homogeneous lattices.
• Dimension of a space is a semantic issue: the naming of locations (coordinatization) and the as-
sociation of a tuple of D base elements with a single location is a matter of convention, indepen-
dent of the connectivity. For a graph, we can deﬁne a heuristic topological dimension at every
point D = 1
2kout, but tuple dimensionality of vertex sets leads to ‘strangely’ connected coordinate
spaces.
• Spaces may be composed with inner (◦) and outer (⊗) products, for joining and exfoliating respec-
tively.
• Different scales in space imply different levels of coarse graining of the base elements. Only regular
lattices have scale invariant multiplicative renormalizations.
• Time is measured by change in the states available to an observer. Proper time encompasses all
states and is therefore not independent from space. If we partition a system so that only part of it
represents the clock for measuring time, complicated relativity issues ensue: change can occur and
be observed in one part, while time stands still in another.
• The structural compositions, or grammars of spacetimes are algorithmic in their representations
and complexity.
• In several models there are two classes of object. Just as in physics one has spacetime and the
matter one puts into it, so in bi-graphs there are places and agents with their connections that one
puts into them. By contrast graphs normally only have a single class of vertex to represent both.
We shall return to this in section 5.12.
• Finally, from the appendix, we see that the total amount of information in a spatial structure is the
same no matter now many dimensions we choose for the spanning representation.
Our received notion of dimension is closely tied to the idea of tuple coordinates, in which one has
a unique name for each component value in the tuple in each direction. Coordinate names are typically
numbered values when we deal with lattices, but they could also be labels like Internet Protocol addresses,
or proper names in an arrangement of books such as a library, or a semantic network.
Of the foregoing cases, graph theory seems to stand alone in placing all of the issues on the table
plainly. It becomes a natural starting point to explain the semantics of spacetime. Unlike completely
regular lattices, general graphs force us to see the difference between issues we can take for granted in
manifolds. The percolation of large scale motion through graphs ought to be compatible with a three
dimensional Euclidean view of the world at large distances.
Category theory haunts the fringes of these descriptions, offering us a raw functional viewpoint, but
is has few insights into the structural nature of spacetime that are not better described in terms of speciﬁc
transition systems (semi-groups and greater). Its main purpose is to state the regularity of the structures
and patterns in a rigorous way, but it is a viewpoint that offers little in the way of insight into semantics.
Bigraphs show a contrasting view of space, somewhat like a grammar, with a relatively signiﬁcant
amount of semantic content. What is missing from bigraphs is the semantics of autonomy. Is a bigraph
33

an autonomous device that can be a source of its own behaviour, or is it merely a cog in a machine? The
signature typing suggests that the former is the case, but the composition of bigraphs offers no notion of
whether one bigraph would be able to ‘allow’ or ‘disallow’ another to be composed with it, or whether it
would reject it. Thus the concepts of autonomous decision-making and a local observer view seem to be
missing. This is a weakness, as there is no room for reasoning or individual agency.
5
Promise theory - autonomous local observer semantics
Let us now turn to the main purpose of this essay, which is to understand spacetime from the viewpoint of
autonomous agents. The principal motivation for this is technological, but it is also fascinating to reﬂect
on what the conclusions might mean for the natural world5.
The need to represent artiﬁcial and conceptual spaces, especially in technology, motivates the study of
non-uniform observer interpretations about the relationships between elements of space. Rather than this
being less fundamental than the foregoing models of natural spacetime, such artiﬁcial constructions turn
out to have more features, and thus it requires more work to recreate aspects that are taken for granted in
physics. Such a study might offer some insight into the assumptions we take for granted about physical
spacetime, as well as provide a framework for more fully understanding artiﬁcial constructions within the
information sciences.
Promise theory builds relational structures that look superﬁcially like graphs or categories. However,
a promise is not a function, nor a transport channel, so it cannot be computed or traversed. A promise does
however require the notion of communication, and hence adjacency. Promises allow us to discuss and
label the nature of relationships between elements in a space in diverse ways, as well as address concepts
such as permission to move from one to another. We shall see that the notion of autonomy makes the idea
of motion harder to understand altogether.
5.1
Basics
One begins with the idea of autonomous agents that interact through the promises they make to one
another [18,19]6. Agent is the term used for the fundamental elements in Promise Theory; these will be
the elements of spacetime too. Agents have no observable internal structure, as with the foregoing models,
however, we allow them to exhibit agency or intent, either fundamentally or by proxy; this means that an
observer would interpret behaviour as being intentional, as observers always impose semantics on what
they observe.
An agent is autonomous in the sense that it controls its own behaviour, it promises are made by itself,
and it cannot be forced to comply with an imposition; however, it can promise to comply voluntarily with
impositions from external sources.
Promise Theory [18, 19] is an elemental framework for expressing intended behaviour graphically
between autonomous parts. This makes it analogous to atomic theory, as a framework of elemental
5In particular, it seems to be impossible to reproduce ideas of spacetime without the existence of fundamental global (non-local)
interactions that maintain symmetries. These are often seen as an embarrassment in physics, where Einsteinian relativity suggests
that they cannot exist. However, it could be that Einsteinian relativity is merely the effective face of those remaining observables
that manifest themselves on our observable large scale.
6Physicists might feel confused by the notion of spacetime elements with arbitrary semantics, represented as a facsimile of
human promises, but this is no more or less mysterious than different ﬂavours of charge we usually attribute to point-like objects,
which are good examples of promises made by (or equivalently the semantics of) elementary objects in the natural world, at least
according to mainstream thinking. Physics has inconsistently turned up its nose at the admission of semantics in world descriptions.
Ascribing intent to inanimate objects in generally frowned upon; thus one would not say ‘A promises to move towards B’. However,
inconsistently we do say ‘A attracts B’ rather than ‘A pulls B towards it’. Both have technically ‘intentional’ semantics (merely
de-personalizing the choice of words does not change that), so the arbitrary line of acceptability might just as well be removed.
34

building blocks with which to recreate systems of greater sophistication. Observability cannot be taken for
granted in promise theory; like statistical and quantum theories, it is a theory of incomplete information.
We write a promise from Promiser to Promisee, with body b as follows:
Promiser
b
−→Promisee.
and we denote an imposition by
Imposer
b
Imposee.
Promises come in two polarities, denoted with a ± signs, as below. The + sign gives assertion semantics:
x1
+b
−−→
x2 (I will give b)
(54)
while the −sign gives projection semantics:
x1
−b
−−→
x2 (I will accept b)
(55)
where xi denote autonomous agents. A promise to give or provide a behavior b is denoted by a body
+b; a promise to accept something is denoted −b (or sometimes U(b), meaning use-b). Similarly, an
imposition on an agent to give something would have body +b, while an imposition to accept something
has a body −b. In general, intent is not transmitted from one agent to another unless it is both + promised
and accepted with a −. Such neutral bindings are the exchange symmetry.
A promise model thus consists of a graph of vertices (agents), and edges (either promises or impo-
sitions) used to communicate intentions. Agents publish their intentions and agents in scope of those
promises may or may not choose to pay attention. In that sense, it forms a chemistry of intent [20], with
no particular manifesto, other than to decompose systems into the set of necessary and sufﬁcient promises
to model intended behavior.
A promise binding deﬁnes a voluntary constraint on agents. The perceived strength of that binding
is a value judgement made by each individual agent in scope of the promises. If an agent offers b1 and
another agent accepts b2, the possible overlap b1∩b2 is called the effective action of the promise.
For example, A promises B ‘to give an apple’. This does not imply that B will accept the apple. B
might then promise A to ‘accept an apple’. Now both are in a position to conclude that there is a non-zero
probability that an apple will be transferred from A to B at some time in the future. If the promise is to
continuously transfer apples, then the timing is less ambiguous. Thus a promise binding is the basis for
interaction, and this must also include adjacency.
The constraints implied by the scope of observability for agents complicates this. Consider an ex-
change of promised behaviour, in which one agent offers an amount b1 of something, and the recipient
promises in return to accept an amount b2 of the promised offer.
π1 : x1
+b1
−−→
σ1
x2
(56)
π2 : x2
−b2
−−→
σ2
x1
(57)
Then any agent in scope σ1 of promise π1, will perceive that the level of promised cooperation between
x1 and x2 is likely b1. An agent in scope σ2 of promise π2, will perceive that the level of promised
cooperation between x1 and x2 is likely b2. Finally, an agent in scope σ1 ∩σ2 of both promises π1 and
π2, will perceive that the level of promised cooperation between x1 and x2 is likely b1 ∩b2. The relativity
of observations can lead to peculiar behaviours, contrary to expectation. Ultimately every agent makes
decisions based on the information it has.
35

If a promise with body S is provided subject to the provision of a pre-requisite promise π, then
the provision of the pre-requisite by an assistant is acceptable if and only if the principal promiser also
promises to acquire the service π from an assistant (promise labelled −X):
xT
+b(π)
−−−−→x1, x1
S|b(π)
−−−−→x2
x1
−b(π)
−−−−→x2
)
∼xT
+b(π)
−−−−→x1, x1
S−→x2
(58)
The relativity of observers will be key to understanding the semantics of spacetimes. Intent, being an
interpretation offered by an observer, brings with it a variety of anthropomorphisms, like trust and level
of belief which are equally important to science (witness the Bayesian interpretation of statistical obser-
vations for instance). This should not be considered a problem; it is merely the reﬂection of a received
interpretation by local observers. Similarly, promise theory, like statistics and quantum mechanics, is a
theory of incomplete information. The promise formalism is described in [19].
5.2
Agent names, identiﬁers and namespaces
Agents may or may not be identiﬁable to one another. In order to be identiﬁed by another agent, each
agent has to promise to be visible or identiﬁable. The property of observability might be an interpretation
of the property of reﬂecting light, or of sending messages. This is an example of what is meant by
semantics.
1. Agents may or may not promise their name or identity. If no name is promised, then only nearest
(adjacent) neighbours can attribute information to them, by virtue the agent’s own labelling.
2. Observers may promise to accept, and hence associate a unique identity with each agent they can
observe directly or indirectly. That identity is local to the observer.
Promise theory makes the concept of a namespace unambiguously something that belongs to an observer
alone.
Assume that an agent x0 can distinguish its nearest neighbours xN by labelling its adjacencies inter-
nally N = 1 . . . n (see ﬁg. 27). If neighbouring agents are completely interchangeable, with respect to
what can be observed by x0, then x0 can label the adjacency channels but cannot know if agents exchange
places along those channels. If agents are not directly adjacent, but make similar promises in different
A
A
A1
2
same
same
same
1
2
1
2
3
Figure 27: Agents may be distinguished by having distinguishable promises, by observer labelling, or by
knowing the path between them and the observer (if trusted).
locations, they are potentially distinguishable by information about the path along which information
passes. This in turn assumes that information about the path is passed along the path by neighbouring
agents and that there is a chain of trust. Trust in information is usually taken for granted in natural science
(with exceptions in quantum mechanics), but it is by no means assured in a techno-social network7.
7A difference here with the foregoing models of spacetime, is that the naming of agents cannot be assumed constant or even
unique over an extended region. We might wish to say something like: every element in a set must have a unique name. In Promise
Theory however this is an imposition or obligation, and cannot be assumed.
36

As autonomous elements, agents might not have agreed to coordinate with every other to ensure
unique distinguishable identities (a non-local property). Even if they have, these might not be a promises
they are able to keep. We must renormalize our notions of what can be taken for granted.
5.3
Reconstructing adjacency, local and non-local space
Promising implies the ability to communicate messages. We assume that adjacency and the ability to
exchange information are synonymous. After all, space is merely a reﬂection of the ability to observe and
transport information8. This might seem peculiar from the viewpoint of absolute spacetime, but distance
is just one of many possible associations between agents that change with perception, circumstance and
individual capability. We must begin by rekindling the notion of distance from the more primitive concept
of adjacency.
Assumption 3 (Promised adjacency) A promise of adjacency can be made by any agent to any other,
allowing them the promiser to offer information to promisee.
This is the same assumption by which we build topologies from sets. To complete any kind of information
exchange, we need a match an imposition (+) or a promise to send with a promise to use (-).
Not all promises are about spacetime structure: being blue or having positive electric charge does not
qualify as an spatial promise, for instance, as it doesn’t tell us about neighbouring points. Many promises
offer membership in some group of similar agents, thus could be non-local properties (like charge and
mass), but do not explain any relativity or connectivity between them.
Being close to, being able to see or hear a neighbour, being able to point to, or even being attracted to,
are examples of adjacency, as they name a speciﬁc target. Thus an adjacency promise is more than mere
continuity9.
Deﬁnition 4 (Adjacency promise) An adjacency promise is a promise that relates an agent xi to another
speciﬁc unique agent xj (i ̸= j), and may give a local interpretation to a relative orientation i.e. direction
between the two.
Agents making an adjacency promises to more than one agent cannot simply be exchanged for one
another without changing the linkage. Thus adjacency is a form of order (see section 5.5). Let us now
examine how many primitive promises are needed to bind adjacent points in a spacetime.
Deﬁnition 5 (Adjacency promise binding) A bundle of bilateral promises, analogous to a contract,
binds an agent xn with another agent xn+1, promising a channel between them.
xn promises that xn+1 may transmit (+) directed inﬂuence to it. xn+1 promises to use (-) xn’s offer.
xn+1 promises that xn may transmit (+) directed inﬂuence to it. xn promises to use xn+1’s offer (-).
xn
+accept msg
−−−−−−−−→
xn+1
(59)
xn+1
+accept msg
−−−−−−−−→
xn
(60)
xn
−accept msg
−−−−−−−−→
xn+1
(61)
xn+1
−accept msg
−−−−−−−−→
xn
(62)
Notice that Newton’s third law is not automatically guaranteed in Promise Theory: that which is given
is not necessarily received; hence conservation of promised properties is not guaranteed, it must be doc-
umented with explicit promises just like charge. In this respect, familiar dynamical concepts of the
8See also the theorem of consistent knowledge propagation [19].
9Database normalization rules (ﬁrst normal form) [7, 21] are promises of regularity of form (internal structure on spacetime
elements that are tables) but not just any promise tells us about how to traverse from one place to another.
37

continuum are puzzling from a discrete information perspective. Neither mass nor velocity are easy to
incorporate.
By the duality rules in promise theory [19], we can interpret the acceptance of an accept-message
promise like a promise to send messages (though the anthropomorphism makes that sound stronger than
necessary for a natural spacetime). Accepting a message is the same as accepting the presence of the
agent. We might say that the agent promises to avail itself of the opportunity to send messages directly
to its neighbour, cementing this relationship. The symmetry between A and B makes the adjacency
relationship into an undirected graph.
The scope of any promise defaults to the two agents involved in the adjacency relationship, but it could
also extend beyond them, allowing others to observe the relative positioning of points, allowing in turn
the coordination of distributed behaviours. In semantic structures like swarms (ﬂocks of birds, or shoals
of ﬁsh), nearest neighbour observations are sufﬁcient to maintain the coherence of the emergent cohesion,
suggesting that a spacetime formed from autonomous agents with nearest neighbour interactions could
be sufﬁciently stable without long range interactions.
A promise network may thus be partitioned into two parts: the graph of promise bindings referring to
adjacency and communication between the agents, and all other promises that use the former to expand
their scope. If we think of the classical concept of matter living within spacetime, then we reduce this to
bundles of ‘scalar’ promises (see below) that associate with a location by virtue of being anchored to an
agent that has position as a result it its adjacency promises. Thus matter is simply spacetime with special
properties, explaining how matter (quite literally) occupies space.
5.4
Continuity, direction, and bases
Nearest neighbour adjacency is a difﬁcult enough concept to understand in an autonomous framework;
continuity across regions is even harder. Why, for example, would agents align themselves with a reduced
symmetry such as a lattice?
Deﬁnition 6 (Spatial continuity) Continuity is understood to mean that, if a direction exists at a certain
location xi, it continues to exist in the local neighbourhood around it.
Suppose we wanted a concept of travelling North. How can this be understood from an agent per-
spective? The concept of North-ness is non-local, and uniform over a wide region. In order to image
continuing in the same direction, we also need to know about the continuity of directionality. Direction
too is thus a non-local concept. When we speak of direction, we mean something that goes beyond who
are the agents closest to us. Any agent can promise to bind to a certain number of other neighbours,
calling its adjacencies to them with the same name (say North, South, etc), but why would the next agent
continue this behaviour? How does each agent calibrate these in a standard way?
As established for graphs, membership in a basis set is a semantic convention used by observers.
It cannot be imposed. A point in a space need not promise its role in a coordinate basis, because that
information is only meaningful to an observer, and could simply be ignored by the observer.
An agent can promise to be adjacent to another agent, but to propose its own classiﬁcation as a member
of some basis would be to impose information onto others from a different viewpoint. By autonomy, each
agent is free to classify another agent as a member of an independent set within a matroid that spans the
world it can observe.
The dimensionality of spacetime, perceived by any observer belongs to the rank of the matroid it
chooses to apply to the agents it can observe. The consequence of this is that spacetime can have any
dimension that is compatible with the adjacencies of the observer. Indeed, the notion of dimensionality
experienced by the elements of a promised space is different for every agent, at every point. The ob-
server with n outgoing adjacencies may regard each independent adjacency as a potential basis vector or
direction.
38

Assumption 4 (Matroids are observables) Every autonomous agent decides its own set of independent
sets to span a space. Hence direction is a local observer view, as noted for graph theory.
Consider an ordered sequence of agents xi that are mutually adjacent. An agent xi recognizes a
direction µ if it promises adjacency (+adjµ) along a locally understood direction µ to a subsequent
neighbour xi+1, and it promises to accept adjacency (−adjµ) with a previous neighbour xi−1:
xi
+adjµ
−−−−→
xi+1
(63)
xi
−adjµ
−−−−→
xi−1
(64)
xi
+adjµ
−−−−→
xi−1
(65)
xi
−adjµ
−−−−→
xi+1
(66)
for all xi. Or in shorthand:
xi
±adjµ
−−−−→
xi±1, ∀xi.
(67)
We shall need to say what happens at edges where we run out of xi (see section 5.9). These promises
are local but require long range homogeneity between the agents, i.e. the condition ∀xi is a non-local
constraint. It is equivalent to promises by every agent to conform to these promises:
xi
C(adjµ)
−−−−−→
xj, ∀i, j
(68)
The issue is not the numbering i = 1 . . . N of the agents, as this may be freely redeﬁned. Any local agent
will bind to another exclusively and the ordering can easily emerge by self-organization, however, the
notion that all of the agents or spacetime points would coordinate with long range order in keeping these
promises does beg an explanation.
A multi-dimensional interpretation of the different spanning sets, does not really add further difﬁcul-
ties, but emphasizes further the non-local cooperation in terms of promise homogeneity. If we choose a 3
dimensional basis with coordinate names (x, y, z),
(xi, yj, zk)
±adjµ
−−−−→(xi±1, yj, zk)
(69)
(xi, yj, zk)
±adjµ
−−−−→(xi, yj±1, zk)
(70)
(xi, yj, zk)
±adjµ
−−−−→(xi, yj, zk±1)
(71)
The directional names belong to the local agent’s coordinate basis, and the non-local cooperation is
assumed homogeneous. This leads to the possibility of a collection or misaligned, non-oriented agents
self-organizing into a crystal lattice. In this way, a space can acquire long-range order with only local,
autonomous promises, provided they are homogeneous over a sufﬁcient region.
5.5
Symmetry, short and long range order
Adjacency (vector) promise bindings are exactly analogous to chemical bonds, with the addition of se-
mantic types. A graph of autonomous agents with only without adjacency bindings has a state of maximal
symmetry; we may call it ‘gaseous’ or ‘disordered’, by analogy. If agents promise adjacency promises in
a uniform and homogeneous way, one may speak of long-range order as in a ‘solid’. It is natural to de-
scribe such a space with a latttice coordinate system (e.g. Cartesian), like that of a crystal. For something
we take for granted, this is quite non-trivial.
39

If we are considering spacetimes of technological origin, e.g. computing infrastructure, then we
readily see a mixture of these two states in everyday life. Mobile phones, pads and computers migrate
without ﬁxed adjacency on a background of more permanent ﬁxtures: servers, disks, network switches,
and other ‘boxes’. Thus, we should be prepared to view spacetimes of mixed phase within a single picture.
The self-organization of autonomous agents is quite analogous to a phase transition by local inter-
action in matter. Indeed, it should be clear by now that in a discrete spacetime, elements of space must
behave and interact according to the same essential mechanisms as elements of matter, causing one to
wonder if there should be a distinction at all.
A spacetime with a graph structure thus exists in phases analogous to gaseous (disordered) with short-
lived adjacencies, or crystalline with long-lived adjacencies (ordered). The disordered state is said to be
a symmetrical state, and a crystalline order is a broken or reduced symmetry, which is usually related to
breakdown to arbitrary symmetry operations, by selection of a subset that appears to bring long range
order. A lattice exhibits long range order, while a ﬂuid, replete with vortices and ﬂows, might exhibit
only short range (nearest neighbour) order, for instance.
For a physical world, the remarkable point here is this: if regular spacetime structure requires long
range order with underlying global symmetry, what is the origin of such order? Why should all points
make the same promises? Perhaps they don’t.
These notions are usually ready-built into most descriptions of spacetime, and as such taken for
granted, but in a graphical view of the world we are forced to confront how this arises, as we cannot
separate symmetry breaking from the structure of spacetime itself. How one breaks the autonomous sym-
metry of spacetime is thus the ultimate question for understanding its structure. The Internet does not
possess a natural long-range order, for example, which is a hindrance to the creation of a global network
addressing scheme.
Long range order can exist for many kinds of promises, not just adjacency promises, but only adja-
cency promises lead to connectivity. Similarly, there are many ways of breaking a symmetry. One is to
build a function on top of spacetime with a monotonic gradient which serves as the generator of a notion
of consistent direction. This is how chemotaxis works in cellular biology for example. If one takes cells
to be autonomous agents and considers bio-space, one could form a directional basis for self-organizing
adjacency provided each cell could homogeneously promise to measure the functional gradient, as in
foetal morphology.
As much as one tries to build the concepts of regular space from a local observer perspective, one
never quite escapes the notion of non-local symmetries at work in creating long range structure. Like it or
not, in a discrete spacetime, we are forced to confront the idea of phases and the possibility of transitions
between them. In promise theory, this amounts to understanding why more than a single agent with no
direct adjacency should make the same promises. In this respect, the model is like that of a cellular
automation with an undetermined topology.
5.6
Material (scalar) properties as singularities and cliques
Consider now how to represent point-like properties in agent space. There are two possible local repre-
sentations of the assertion that agent A1 is blue. In the ﬁrst case:
A1
+I am Blue
−−−−−−−→
A2
(72)
A2
−I am Blue
−−−−−−−→
A1
(73)
A1 merely asserts a property of itself to an observer A2. The observer A2 can take or leave the promise of
blueness, thus it must promise to use or accept the assertion, (it might be colour blind, for instance, and
hence effectively not promise to use the information). Notice here that the conceptual world of blueness
lives entirely within the body of the promise, and does not affect the type of objects between which
40

promises are made. In this representation, concepts thus live in a parallel world that does not intersect
with the space formed by the agents Ai themselves.
Any agent might make the same promise, and a priori there is be no objective calibrated standard for
blueness. Different agents might interpret this promise differently too.
For the second representation, consider the merger of the physical and conceptual worlds, by intro-
ducing special agents for material properties concerned:
ABlue
+Blue
−−−−→
A1
(74)
A1
−Blue
−−−−→
ABlue
(75)
A special kind of agent, whose function it is to label things as blue, now promises this quality as if
providing the property as a single point of service. Association with this source of blueness is what gives
A1 the property. To be blue, all A1 has to do is promise to promise to use the service.
Multiple agents may now in two ways: either by coordinating their deﬁnitions individually in a peer-
to-peer clique,
{Ai}
±blue,C(blue)
−−−−−−−−−→{Aj}, ∀i, j
(76)
or by using a deﬁnitive calibration source,
ABlue
+Blue
−−−−→
{Ai}
(77)
{Ai}
−Blue
−−−−→
ABlue
(78)
which then acts as a kind of hub for the property of blueness (see ﬁg. 28).
1
2
3
4
1
2
3
4
Blue
+You are Blue
− Yes, I’ll buy that
Figure 28: Global symmetries - calibrating a property, either by equilibrating with a single source (singu-
larity), or everyone individually (clique).
Notice that, since each property has at least one single vertex associated uniquely with it (the source
for that property), the set of links emanating from it is automatically an independent set. Each property
or type of promise that refers to an intrinsic quality is in fact a basis vector, belonging to a matroid (see
ﬁg. 29). What is interesting is that, unlike a vector space, this vector ends at a singularity, like a charge
radiating lines of force.
The scalar promises bear an obvious resemblance to the use of ‘tags’ or ‘keywords’ to label informa-
tion documents and inventory items in databases. They act as orthogonal dimensions to the matroid that
spans agents in the ordered phase of a spacetime.
41

R
G
B
(1,0,0)
(2,0,0)
(3,0,1)
(4,1,0)
(5,2,0)
Figure 29: Matroid basis for global properties with three property hubs adding three components to the
coordinate tuples.
5.7
Spacetime (vector) promises and quasi-transitivity
Scalar promises imbue elements of spacetime with intrinsic properties; vector promises describe cumula-
tive relationships between them10. Vector promises are, in principle, interpretable as one of the following
cases:
• A1 can inﬂuence A2 (causation)
• A1 is connected to A2 (topology)
• A1 is part of A2 (containment)
e.g.
A1
Causes
−−−−→
A2
(79)
A1
Precedes/Follows
−−−−−−−−−−−→
A2
(80)
A1
Aﬀects
−−−−→
A2
(81)
A1
Is a special case of
−−−−−−−−−−−−→
A2
(82)
A1
Generalizes
−−−−−−−→
A2
(83)
Adjacency too may be considered quasi-transitive, for while A next to B next to C does not imply
that A is next to C, if we reinterpret adjacency only slightly as connectivity, we can make it true, e.g. πL:
A is to the left of B is to the left of C.
πadj →







πconnected
πL,R
πN,S,E,W
π±µ
(84)
10In the language of chemistry, scalar promises are like atomic isotopes, while vectors are their interatomic bonds, forming
molecules and crystals.
42

Clearly if A causes B and B causes C, there is a sense in which one might (at least in some circum-
stances) interpret that A causes C, hence there is a kind of transitivity. Mathematically, there is exists a
generator of a translational symmetry which can be repeated more than once to bring about a sense of
continuity of motion. In functional terms, these relational maps have arity 2 and span a single coordinate
direction. Such relationships generate a spacetime, analogous to a vector space.
Next there are container models, analogous to bi-graphical positioning:
A1
Is contained by
−−−−−−−−−→
A2
(85)
A1
Is found within
−−−−−−−−−→
A2
(86)
A1
Is part of
−−−−−−→
A2
(87)
A1
Is eaten by
−−−−−−−→
A2
(88)
These generate forest graph relations. Thus we have a way of incorporating both types of spatial semantics
in the promise framework, and we can link translation and containment through their quasi-transitive
nature. These promise types provide a notion of spatial continuity.
Promises that cannot be made into a succession of symmetrical translations belong to the singular
properties discussed in section 5.6. They represent promises about self, rather than about relationships
to others (in functional terms they have arity 1). Such expressions may be formulated and interpreted in
two ways, depending on who or what are the recipients of the relationship: as promises, or as general
associations between topics in a topic map.
5.8
Fields and potentials
The classical continuum notion of a ﬁeld or potential is now seen to be a functional representation of the
split between a common underlay of vector adjacency promises (spacetime), and a set of local material
promises (the potential) at each spacetime element.
Figure 30: In a continuum approximation, the idea of a potential ﬁeld φ(x) (as in electrodynamics) whose
value φ is speciﬁed at each location on top of a spacetime x, appears as a separation of scalar promises
on top of the common denominator of adjacency promises.
This construction is very similar to Schwinger’s formulation of quantum ﬁelds (see ﬁgure 30), except
that we have autonomous agents for spacetime elements, and the promise of ﬁeld or material properties, at
some location, belongs to each spacetime agent. The promise to measure ‘particle events’ might be kept in
43

a particular location (to be assessed by an observer). This shows how promises capture the ﬁeld idea in a
discrete way, and emphasize the symbolic semantics of particles alongside dynamical properties [22,23].
5.9
Boundaries and holes
The interruption of vector continuity is what we mean by a boundary. Boundaries explicitly break sym-
metries and seed the formation of structure by anchoring symmetry generators to some ﬁxed points.
The simplest notion of a boundary is the absence of a promise of adjacency. For autonomous agents,
this can have two possible directions. Suppose one names agents in some sequence; at position n,
xn
∅−→xn+1
(89)
An agent might be ready to accept messages from a neighbour, but there is no neighbour to quench it,
xn+1
+accept presence
−−−−−−−−−−−→
∅
(90)
(91)
or no promise given to keep such a promise.
xn+1
+accept presence
−−−−−−−−−−−→
xn
(92)
xn
−∅
−−→
xn+1
(93)
We can summarize different cases:
Deﬁnition 7 (Continuity boundary) If an agent xi does not promise +adjµ to any other agent may be
said to be part of a µ-transmission boundary.
Deﬁnition 8 (Observation boundary/event horizon) If an agent xi does not promise −adjµ to any
other agent may be said to be part of a µ-observation boundary, or event horizon.
Boundaries can thus be semi-permeable membranes. These are quite common in biology. Boundaries
can be localized or extended (see ﬁg. 31). Their perceived extent depends on observer semantics, or
coordinatization. The absence of an adjacency along a direction labelled µ between agents A and B may
be called a µ-boundary, even though there is still a path from A to B via C, in a direction ν.
B
A
C
µ
ν
Figure 31: A boundary or a partition in a graph may be the absence of an adjacency.
Boundaries are usually considered to be the discontinuation of a certain degree of freedom or direc-
tion. The default state in a network of autonomous agents is boundary.
Deﬁnition 9 (Edge boundary) If an agent xi does promises ±adjµ to an agent that does not exist, we
may say that xi belongs to the µ edge of the space.
44

Deﬁnition 10 (Material boundary) The edge of a vector region consisting of those agents that uniformly
promises material property X.
Thus we attribute a variety of semantics to boundaries:
• The absence of an agent adjacency (an edge of space or a crystal vacancy).
• An agent that selectively refuses a promise to one or more agents (a semantic barrier, e.g. a ﬁrewall,
passport control, etc).
For example:
• The edge of space itself
• The edge of a property e.g. blue, table
• The edge of an organization e.g. ﬁrewall ‘DMZ’, freemason
5.10
Containment within regions and boundaries
How shall we represent the idea of one object being inside another in a world of autonomous agents?
Agents are atomic, and one atom cannot be inside another. The clue to this lies is viewing containment
as a bulk material property. We start by deﬁning membership in regions or cliques.
A compound agent, denoted {Ai}, with role R is the set of agents that mutually promise to belong to
set R. Membership in a group, role of property follows the discussion in section 5.6. Containment and
overlap may now be deﬁned with reference to ﬁg. 32
A
{A }
sub
{A }
1
{A }
3
2
+R
−R
Figure 32: Motion of the ﬁrst kind: extrinsic motion in an untyped spacetime
Deﬁnition 11 (Containment promise) Compound agent {A1} is R-inside (or R-contained by) com-
pound agent {A2} iff
{A1}
−R
−−→{A2}
(94)
{A2}
+R
−−→{A1}
(95)
The promise of +R represents membership in region R, which deﬁnes a compound or ‘super-agent’, that
is a coarser grain of space than the component agents.
Deﬁnition 12 (Spatial overlap promise) Compound agent {A1} R-overlaps with compound agent {A2}
iff
{Asub} ⊆{A3}
−R
−−→{A2}
(96)
{A2}
+R
−−→{Asub}
(97)
45

When we say one region is inside another, it is sometimes convenient to describe the boundary of the
region rather than the region’s name. There is a straightforward relationship between these, given that a
region is simply a material boundary in which there exist adjacent agents, some of which promise −R
(inside the region’s boundary) and some of which do not (outside the region).
The same criterion of membership applies. Using the container as a label for the type membership is
the complement of using the type itself, since deﬁning an edge requires us to specify edge with respect to
which property.
Compound agents inside one another or adjacent to one another can all be represented as regular
tuple coordinates, by deﬁning matroids as indicated here. An incidental example of this may be seen in
the construction of data network addresses, like IP addresses, VLAN numbers, etc [24].
5.11
Local, global, and proper time (What counts as a clock?)
To the eye of an all-seeing ‘godlike’ observer, time is simply a series of spacelike hypersurfaces. Any
change of the properties of one of these represents a new time. We refer to this total picture as proper time.
Each moment of proper time represents a single version of all the promised attributes of the universe. In
an artiﬁcial system where one has the ability to observe everything ‘instantaneously’, one could always
deﬁne a proper time accordingly. Each universe is its own clock.
Locally, agents do not have access to this complete instantaneous overview. They have information
horizons, and information travels at a ﬁnite speed. They can only observe what can be transmitted to
them, and what they have promised to receive, up to the limitations of their own faculties. For local
time, agents can choose any convenient set of states within themselves, to represent their clock. This is a
convention that might vary to place to place11.
To keep matters simple, the clocks by which agents measure their own passage of time are best kept
sufﬁciently local as to not be affected by the need to cooperate with neighbouring agents, and the ﬁnite
speed of communications that entails. In practice, we assume that clocks are internal and of zero size12.
Another possibility is to build a co-clock with a neighbour, in the manner of a dialogue or handshake.
A local clock is one whose state counters are all contained internally by an agent A. Thus we exclude
adjacency bindings from the measurement of local time. This is not without controversy since time for
a super-agent necessarily includes the bonds between atomic agents. Moreover, as we’ll see, the ability
to measure derivatives requires agents to be able to remember at least two or three points, which could
require the cooperation of other agents. Constructing a shared clock goes beyond the current paper.
Agents must be able to measure space and time in order to cooperate and evaluate one anothers’
keeping of promises. In a discrete spacetime, this is a lot more difﬁcult to understand than if space and
time were separate, since both velocity and acceleration are themselves only understandable from the
viewpoint of a continuous independent spacetime.
5.11.1
Concurrency, simultaneity, and timelines
A clock tick is an observable change of state, and a timeline is a sequential ordering of such changes,
observed by some agent. Every agent is free to form its own timeline, within its own world, because
intentions arise from each agent individually (the intent to see, and the intent to interpret). Any agent that
11Many of our assumptions about time come from the doctrine that time exists independently of space. In Maxwell-Einstein
relativity, the constancy of the speed of light in all frames warns of the falsity of this assumption, but one never quite lets go of time
as an independent, yet coupled quantity. On a macroscopic level, it is possible to arrange that illusion by partitioning information.
However, at a microscopic level, one cannot measure or experience anything at all without causing change. This information,
causation, spatial conﬁguration, and time are all tightly interwoven.
12In the special theory of relativity, the locality of clocks is glossed over. What is, in the twins paradox, the clocks of both twins
were always two halves of a single clock, or it was so large as to span the distance travelled by one of the twins?
46

accepts intentions from another can promise to use its intended order, or not. That is the ﬁrst level of
possible distortion, as it is essentially a ‘voluntary’ or autonomous act.
If multiple agents communicate, then each neighbour interaction can add distortions of its own. More-
over, when there are multiple routes for information to take through the web of adjacencies, different
routes might lead to different distortions. In physics we assume that information is passed with integrity,
only delayed predictably in time. This is mainly because there is no way to verify if it isn’t. However,
this is too strong an assumption for general transmission of information in intentional structures.
An agent can only tell if one event is intended to follow another if it is promised the causal sequence
in the form of a dependency. This is what conditional promises do:
A
b1|b2
−−−→A′
(98)
A dependency promise of this kind offers non-local (non-Markov) information. It starts to build a journal
of events, or a timeline. Here the agent A promises that the intention described in body b1 must follow the
intended outcome of b2. If, on the other hand, A′ receives promise or their outcomes in a certain order,
without documented dependency:
A
b1
−→A′
(99)
A
b2
−→A′
(100)
then A′ cannot be certain of their intended order. Regardless of what order the messages arrive in, without
the conditional promise dependency, A′ must view b1 and b2 to be concurrent (both as outcome events,
and promised processes). This is an important deﬁnition of concurrency.
The question of a whether one event happens before another is not really a meaningful one in any
spacetime, because until an event has been observed there is no causal connection between one part of
space and another; the real mystery is why changes or events happen at all. If space is gaseous and time
is space, it implies that time must also be non-ordered. However, if there is a channel for communication,
then there it is possible to place some limits on the order of transitions, by sharing information on trust13.
Agents observer limited views of the world around them. The horizon of the observability deﬁnes
their clocks, and hence their notion of time. Often observers measure time based on internal states, which
are only available to them, so every agent must experience time individually. This is the essence of
relativity. It is well known that different agents, distributed in space, can experience changes in different
ways, and even disagree about the order of certain events.
The issue of measurement takes on an even greater importance when considering observer semantics,
because an event only acquires meaning once it has been noticed and interpreted. Signals might lie latent
until observers choose to ‘process them’, e.g. such as in queueing systems.
If we are interested in intentional behaviour, then the issue of causation is more about what an observer
chooses to see than what it is potentially capable of seeing. This might come as a slap in the face to those
who think that special relativity (i.e. the effects brought on by the ﬁnite speed of communication) already
makes matters hard enough.
5.11.2
Shared (non-local) timeline example
Expecting distributed consensus between individual observers is a weak strategy for any observer. One
should expect diversity, as this is a promise more likely to be kept. Consider an example.
Suppose four agents A, B, C and D need to try to come to promise each other a decision about when
to synchronize their activities. If we think in terms of impositions or command sequences, the following
might happen:
13This is the essence of Lamport clocks or vector clocks in the literature of distributed computing.
47

1. A suggests Wednesday to B,C,D.
2. D and B agree on Tuesday in private.
3. D and C then agree that Thursday is better, also in private.
4. A talks to B and C, but cannot reach D to determine which conclusion it reached,
In a classical view of time, resolving this seems like a trivial matter. Each agent check the times of the
various conversations according to some global clock, and the last decision wins. That viewpoint view is
problematic on several levels however.
First of all, without D to conﬁrm the order in which its conversations with B and C occurred, A only
has the word of B and C that their clocks were synchronized and that they were telling the truth.
A
B
C
A
B
C
D
D
Figure 33: Comprehending intended time
A is neither able to ﬁnd out what the others have decided, not able to agree with them by ‘signing off’
on the proposal. To solve this, more information is needed by the agents. The promise theory principles
of autonomous agents show where the problem lies very quickly. To see why, let’s ﬂip the perspective
from agents telling one another when to meet to agents telling one another when they can promise to
meet. Now they are only talking about themselves, and what they know.
1. A promises it can meet on Wednesday to B,C,D.
π1 : A
Wed
−−−→{B, C, D}
(101)
2. D and B promise each other than they can both meet on Tuesday.
π2 : {B, D}
Tue
−−→{B, D}
(102)
3. D and C promise each other that they can both meet on Thursday.
π2 : {C, D}
Thu
−−→{C, D}
(103)
4. A is made aware of the promises by B and C, but cannot reach D.
π2 : {B, D}
Tue
−−→
A
{B, D}
(104)
π2 : {C, D}
Thu
−−→
A
{C, D}
(105)
48

Each agent knows only what promises it has been exposed to. So A knows it has said Wednesday, and
B,C, and D know this too. A also knows that B has said Tuesday and that C has said Thursday, but
doesn’t know what D has promised, because the two promises involving D are concurrent according to
its information.
There is no problem with any of this. Each agent could autonomously meet at the promised time, and
they would keep their promises. The problem is only that the intention was that they should all synchro-
nize at the same time. That intention was a group intention, and somehow it has to be communicated
to each agent individually, which is just as hard as deciding when to meet for dinner. Each agent has to
promise (+) its intent, and accept (−) the intent of the others to coordinate.
Let’s assume that the promise to meet one another (above) implies that A,B,C, and D all should meet
at the same time, and that each agent understands this. If we look again at the promises above, we see
that no one realizes that there is a problem except for D. To agree with B and C, it had to promise to
accept the promise by B to meet on Tuesday, and accept to meet C on Thursday. These two promises are
incompatible. So D knows there is a problem, and it is responsible to accepting these (its own actions).
So promise-theoretically, D should not accept both of these options, and either B or C would not end up
knowing D’s promise.
To know whether there is a solution, a God’s eye view observation of the agents (as we the readers
are) only need to ask: do the intentions of the four agents overlap at any time (see the right hand side of
the ﬁgure)? We can see that they don’t, but the autonomous agents are not privy that information. The
solution to the problem thus needs the agents to be less restrictive in their behaviour and to interact back
and forth to exchange the information about suitable overlapping time. This process amounts to the way
one would go about solving a game-theoretic decision by Nash equilibrium.
With only partial information, progress can still be made if the agents trust one another to inform
about previous communications.
1. π1: A promises it can meet on Wednesday or Friday to B,C,D.
π1 : A
Wed
−−−→{B, C, D}
(106)
2. π2: D and B promise each other than they can both meet on Tuesday or Friday, given knowledge
of π1.
π2 : {B, D}
Tue,Fri|π1
−−−−−−→{B, D}
(107)
3. π3: D and C promise each other that they can both meet on Tuesday or Thursday, given knowledge
of 1 and 2.
π3 : {C, D}
Tue,Thu|π1,π2
−−−−−−−−−→{C, D}
(108)
4. π4: A is made aware of the promises by B and C, but cannot reach D.
π2 : {B, D}
Tue,Fri|π1
−−−−−−→
A
{B, D}
(109)
π2 : {C, D}
Tue,Thu|π1,π2
−−−−−−−−−→
A
{C, D}
(110)
Now A does not need to reach D as long as it trusts C to relay the history of interactions. It can now
see that the promise π2 was made after π1, and that π3 was made after π2. Thus A can surmise that the
agents have not agreed on a time, but that B, C, D’s promises overlap on Tuesday. It could now alter its
promise to reach a consensus or equilibrium.
49

The notion of time, in a world of autonomous agents, becomes nothing more than a emergent post-hoc
narrative that summarizes the dependencies between interacting agencies. To put it another way, we can
draw a ﬂowchart based on what we think happens in a distributed system, perhaps even in a number of
different ways, from different observer perspectives, and with potentially different interpretations.
5.12
Motion, speed and acceleration in agent space
Behaviour consists of exhibiting certain promised attributes; this can be assessed by an agent acting as
an observer. Motion, accordingly, can be measured as a change in the location of promised attributes14.
This can now be deﬁned in a few different ways, some of which amount to cellular automata, in which
the agents are cells, and others are re-wirings of the fabric15. In a space of autonomous agents, even the
familiar concepts of uniform motion in a straight line are non-trivial.
5.12.1
Foreword: is there a difference between space and matter that ﬁlls it?
A question that becomes relevant when we approach the matter of spacetime semantics is whether there
is a basic difference between empty space and something material that ﬁlls it?
Imagine a blank storage array, which becomes ﬁlled with data. Is the absence of an intended content
fundamentally different from the promise of an unspeciﬁed value? Is a tissue of stem cells different from
cells that have been given material semantics by expressing differentiated types? Both have DNA; they
merely express different promises. Is empty space merely an agent without a promise to behave like
matter? There is at least the possibility that matter is simply the breakdown of indistinguishability in
space.
This question arises naturally in the possible descriptions of motion in a discrete agent-based space-
time. Three distinct models of motion make sense from the perspective of promise theory. These might
seem excessive from a physical viewpoint, but all are in fact common in the artiﬁcial spaces of technology,
as well as in material science.
In the ﬁrst case, there is only a single kind of agent in a gaseous state. In the second, there is a
two-phase model with a solid spatial lattice and material properties bonded loosely to them at certain
locations. In other words, the position of matter is by bonding to an element of space. Finally, in the third
model, there is only a single kind of agent, but the physical properties promised as matter can bind to a
speciﬁc agent and be transferred from one to another.
Technologically, we have need for all three models of motion. The ﬁrst relates to ad hoc mobile agents,
the second to base-station attachment of mobile devices, and the third to ﬁxed (virtual) infrastructure. It
is also fascinating to speculate as to the meaning of these processes in nature. Certainly, these alternatives
exist within material structures. The question remains open as to whether spacetime itself is constructed
in the same way.
5.12.2
Motion of the ﬁrst kind (gaseous)
The ﬁrst case deals with a homogeneous collection of agents that can move by swapping places, within
an ordered graph of adjacencies (see ﬁg. 34). This becomes increasingly complex in a multi-dimensional
lattice, so we’ll restrict this to one dimension only to understand its properties.
In order to be able to form a new adjacency, an agent must know about the existence of the agent
to which is wants to bind, and vice versa. This can be assumed for nearest neighbours only. However,
14Attribute information might well exist without being promised, but in that case it is unobservable.
15Motion is a change in the relative adjacencies between bundles of properties. This can happen by treating agents as containers
for the promised attributes and redeﬁning free-ﬂoating agent relationships (as in a swarm), or by dividing agents into ﬁxed sign-post
and mobile traveller type agents, i.e. building a rigid agent scaffolding on which free ﬂoating agents can move by binding and
re-binding to an infrastructure network (somewhat like the way mobile phones attach to different cells in a cellular network).
50

it turns out that exchanging places16 also requires knowledge of next-nearest neighbours. This would
seem to involve multiple messages back and forth to discover one another. This in turn seems to create
a bootstrap problem for spacetime. How can spacetime form structure without such structure existing
to begin with? However, as long as agents promise to relay information about their adjacencies to their
neighbours, this can be handled in a purely local manner.
A second, but related problem, is how to form a coordinate system in a gaseous phase. If one cannot
use the sequential, monotonic nature of integer labels, then coordinate names become ad hoc and lose the
extrapolative power of a pattern.
Consider a simple one dimensional model with adjacencies to the left and right of each agent (see ﬁg.
34). We denote a trial agent that has a non-zero velocity from left to right by Ai (Ai = B in the ﬁgure).
Ai+1 is to the right of Ai, and Ai−1 is to the left of Ai. The agent must be able to distinguish left from
right.
A
B
C
D
A
B
C
D
Figure 34: Motion of the ﬁrst kind: intrinsic motion in an untyped spacetime.
In ﬁg. 34 we see that, in order for an agent to move one place the right, four agents’ promises need to
be coordinated. This is a non-local phenomenon, since Ai = B and Ai+2 = D are initially non-adjacent.
We may denote the bodies for adjacency promises by ±adjL,R. In addition we denote the property of
momentum to the left or right by +pL,R.
We need to introduce a value function on the agents’ promises to allow one combination of promises
to be preferred over another π1 > π2. Two promises will be considered incompatible when we write
π1#π2. If two promises are incompatible and the ﬁrst is preferred, i.e. π1 > #π2 then we may assume a
promise transition in which π is withdrawn in favour of π1.
Motion from left to right occurs with the following promises, for all i:
1. Ai
+pR
−−−→∗, i.e. we assume an agent has the property of right momentum, which is promised to all
observers.
2. Ai
±Ai±1
−−−−→Ai±1, i.e. all agents mutually promise their neighbours to relay information about their
neighbours to them, both ±.
3. pR > #−adjR the acceptance of a right-adjacency (from the left) is incompatible with the attribute
of right momentum. i.e. an agent with right momentum would immediately withdraw its promise
of right adjacency. This allows B to drop its adjacency to A in the ﬁgure.
4. If the adjacency with right neighbour Ai goes away, try the next neighbour Ai+1:
Ai−1
+adjR|¬−adjR
−−−−−−−−−→Ai+1
This allows A to connect with C in the diagram. Note this requires a memory of the next neigh-
bour’s identity, which means that spacetime has to store more than information about attributes.
16This is essentially a bubble sort algorithm
51

5. To join B with D and reverse links BC
An agent Ai with right momentum +pR might prefer to bind to Ai+2 and relabel its offer of right
adjacency to Ai+1 as acceptance of a right adjacent −adjR from Ai+1.
6. To drop CD, an agent might prefer to use a combined promise of momentum and adjacency from
B, than pure adjacency from C, i.e. pR, +adjR > # + adjR.
7. To ﬂip the use-right-adjacency of CB to a give right adjacency offer CB, this might be preferred
if the −adjR is no longer received from D.
8. Finally, all the left adjacencies also need to be connected along with the right adjacencies in a
similar manner.
This is a lot of work to move an agent one position in a lattice. It seems unnecessarily Byzantine, in
violation of Occam’s razor. The valuation preference, for instance, favours promises from farther away
than nearby agents. This also seems contrary to physical experience.
This algorithm does the job with only locally passed information but it has loose ends, as well as the
unsatisfactory need for extended memory. Moreover, in higher dimensions it becomes even messier. It
does not seem viable as a method for motion, though possible. Loose ends include adjacency promises
that are not withdrawn, leading to non-simple connectivity. This can be dealt with by introducing condi-
tional equilibrium promises: I will make you a promise only if you accept it.
A
+adjR|−adjR
−−−−−−−−→A′
(111)
A
+adjL|−adjL
−−−−−−−−→A′
(112)
In this construction, the promises are invalid unless the recipient is listening so the momentum can break
the symmetry of the equilibrium [20]. To view a ‘particle’ as moving in this spacetime, one would have
a spacetime element promise the particle’s properties.
In physics, one assumes that motion is an intrinsic behaviour of bodies, but here we see that it is a co-
operative non-local behaviour. This might be unavoidable in any description. This requires no non-locally
transmitted knowledge (i.e. the bootstrapping is self-consistent). However, it is far from satisfactory. Per-
haps one might also view such multiple connectivity as a form of tunnelling akin to entanglement, in the
sense of quantum mechanics [25].
What this exercise in promise theory reveals is the logical need for memory of non-local information
in order to bootstrap motion. Whether spacetime is artiﬁcial or not, this structural information conundrum
is unavoidable.
5.12.3
Motion of the second kind (solid state conduction)
In the ﬁrst approach to motion, there is only a single kind of agent, which suggests great simplicity, but
this leads to a highly complex set of behaviours to explain motion. A second kind of motion may be
explained by separating agents into two classes, which we may refer to as spatial skeleton agents Si,
which account for the ordered structure of spacetime, and a second kind of agents which promise non-
ordered material properties Mj. Adjacency {M}
±adj
−−−→{S} accounts for the location of ‘matter’ within
‘space’, and matter becomes effectively a container for material promises.
Motion within such a model then consists of re-binding material agents at new spacetime locations
(see ﬁg. 35, with S1 = A, S2 = B).
withdraw
M1
±adj
−−−→
S1
(113)
make
M1
±adj
−−−→
S2
(114)
52

A
B
C
D
A
B
C
D
Figure 35: Motion of the second kind: extrinsic motion on a typed spacetime.
This is the way mobile phones attach to different cell base-stations, by re-basing or re-homing of a satellite
agent around a ﬁxed cell location. It is also the model of location used by bigraphs. Locations are ﬁxed
seeds around which material agents accrete or migrate. Since the property of velocity belongs to the
material agent, it must be an autonomous promise to bind to a new site.
Just as before, agents carrying a momentum need to know of the next location binding point. That
information has to be relayed between the spatial agents, just as in the previous section. With two classes
of agent, new questions arise: how many Mj can bind to the same Si? This discussion goes beyond what
there is room for here (in physics this relates to the particle classes called bosons, fermions and anyons).
The same issues will persist as we generalize the kinds of agents to many classes in section 6
In this model, motion is still not a behaviour that is purely intrinsic to an agent; it is non-local, as
the existence of non-adjacent points is not knowable without information by the skeletal agents being
exchanged between A and B. However, in this case only two skeletal agents and one mobile agent need
to be involved to transport the mobile agent. This is much simpler than motion of the ﬁrst kind.
Although this model recreates the old idea of absolute spacetime, or ‘the æther’, it seems preferable
than the ﬁrst model.
Notice how other forms of spatial belonging fall into the same category of binding to a seed. Member-
ship on a person in a club or organization can be viewed in the same light. Then the adjacency promise
takes on the semantics of ‘is a member of’ or ‘is employed by’, etc. Another way to form a group or
‘role’ as it is called in promise theory is for all agents to promise all other agents in a clique to have the
same promises. This is a de-localized alternative to the localized binding envisaged here.
5.12.4
Motion of the third kind (solid state conduction)
To remove the notion of an explicit æther, one could avoid distinguishing a separate class of agent, and
keep all agents the same. In technology, this is what is known as a peer to peer network. Movement of
promises between agents is the third and perhaps simplest possibility. It is essentially the same as motion
of the second kind, without a second type of agent but with two classes of promise.
A
B
C
D
A
B
C
D
Figure 36: Motion of the third kind: extrinsic motion of promises on a non-typed spacetime background.
53

Motion now consists of transferring promises from one location to the next. This brings new assump-
tions of global homogeneity: now every agent in space must be capable of keeping every kind of promise.
There can be no specialization.
The promises to do this are straightforward, and the continuing need for global cooperation with
local information in this picture looks very like Schwinger’s construction of quantum ﬁelds [22,23], and
Feynman’s equivalent graphical approach [26,27].
5.12.5
Measuring speed, velocity and transport properties
What is the meaning of speed and acceleration of transitions for a ﬁnite state machine? Speed is deﬁned
as the translation of a measurable quantity over a deﬁnite time interval. While philosophically interesting
in its own right, this has practical consequences for the transport of material promises from place to place
in a cooperative system, so it is worth spending a few words on.
The three kinds of motion described above do not give us an automatic notion of speed. Motion of the
second and third kinds are representative of what we observe classically in materials. Motion of the ﬁrst
kind is akin to gaseous collisions, quantum tunnelling and topological effects. In those phenomena, time
is always part of the independent backdrop. In a space of autonomous agents, that assumption is invalid.
Consider then how we can deﬁne speed and acceleration, the ﬁxtures of classical mechanics on which
we base much of our idea of reality, in an autonomous agent space.
• How long does a transition take? No agent can measure a transition faster than its own clock ticks,
so transfer of information is limited by each participant’s local ability to experience time. Events
which happen faster than a clock tick of a receiver appear concurrent or simultaneous.
• So how do we even measure the concept of velocity in a state machine that makes transitions, where
those transitions are the very deﬁnition of time? Clearly it is not possible without promises and
trust in the version of reality they express.
Suppose now we try to measure speed and acceleration, as one would in an experiment at our macro-
scopic scale. If we can measure speed, then we can also measure acceleration, by measuring successive
speeds and comparing for a change. In continuous spacetime, we can take many things for granted; as
we’ll see, in a discrete network, we cannot. It’s not important which model of motion we choose. For
the sake of argument, let’s take motion of the third kind in which promises move (like electrons skipping
through an atomic lattice).
The conclusion is quickly seen by the following argument: clocks to measure time have to be built
from space, so a change in spatial conﬁguration implies a change in time. However, a change in time
from other states which do not alter position in a particular direction, so a change in time does not imply a
change in space. This implies that a change in speed must always be less than or equal to some maximum
value:
∆x
⇒
∆t
(115)
∆t
̸⇒
∆x
(116)
⇒∆x
∆t
≤
vmax.
(117)
Lemma 2 (Universal speed limit) In any discrete transition system that measures its own time, there is
a maximum speed (which we can deﬁne as unity).
Consider just two nearest neighbours in isolation. Our only measure of distance is in adjacency hops,
so the distance between neighbours is always 1 unit. Similarly, the time it takes for a transition can always
be deﬁned to be 1, since an observation leads to a change of state and hence a tick of the clock. Actually,
54

astute readers will note that the agents cannot measure the ‘time’ in any other sense than this. There is
no information about when a signal started that is available. Transitions are considered instantaneous.
However, the arrival of the event leads to a change, which in turn counts as a tick of the clock so that
change must be 1.
Regardless of what any external observer might see or think, the local observers always see informa-
tion travel at a constant speed of
vneighbour = 1
1
hops/transition
(118)
This is straightforward because the observer and the locations are the same. By this reasoning, an observer
would never be able to observe any transport at a speed different from 117. It suggests that all signals must
travel at the same speed, and that there can never be any acceleration. However, if we extend to paths that
go beyond nearest neighbours, then the state space grows and it becomes possible to measure differences
of effective speed on average, because of the incompleteness of information available to observers.
To extend the model to a greater distance, look at ﬁgure 37. Suppose an observer O observes a
promised property π at (x1, t1)O according to its own measurements, and later observes that π has moved
to (x2, t2)O. Then, according to our ordinary understanding of speed, it can compute the speed as the
ratio of distance travelled per unit time: by:
vO = x2 −x1
t2 −t1
.
(119)
Since there are more agents involved now, it is possible that other interactions might have caused O′s
clock to advance while π is travelling, so we are freed from the idea that the time for a transition must be
a single tick.
This assumes that we are numbering the coordinates and times in an ordered sequence. This assumes
either non-local cooperation between agents, or swarm-like cooperation. If the coordinatization of space
and time by O is not monotonic, this arithmetic difference of values means nothing. Thus the semantics
must be policed by O alone. In a gaseous state, this is a risky assumption, but let’s imagine that the
relative positions of the agents are frozen for now to keep things as simple as possible.
(x,t)
(x’,t’)
Figure 37: Motion observed by a third party.
Assuming that the transfer takes place, and O makes its measurements, the ﬁrst issue is that O has no
direct knowledge of how many intermediary agents might lie between x1 and x2, without foreknowledge.
This is hard to imagine given that we normally assume a regular, predictable Cartesian coordinate lattice.
Without that safety net, simply knowing the names of the end points of an interval does not uniquely
17This is analogous to the constancy of the speed of light in electrodynamics: speed depends only on universal constants, in the
frame of the observer. However, this wave equation result implicitly assumes the uniformity of spacetime.
55

determine a path or history. This is analogous to the matter of geodesics in curved spacetimes. The only
measure of distance we have is nearest neighbour adjacency: ‘hops’.
So O has no knowledge that π travels monotonically from x1 to x2: it might take a non-direct path,
or it might actually reverse course sometimes and even oscillate between two agents for a while before
continuing. All the while, O’s restricted clock might be ticking (or even reversing). What speed would
be inferred then?
Lemma 3 (Speed can only be inferred) The speed of transport of an observable property cannot be
measured with certainty by any observer, since this would require it to impose a promise to report on
behalf of all agents along the path taken by the observable property. This would violate the autonomy of
the agents.
Even if none of this possible weirdness happens, the promise that π moved from x1 and has reached
x2 has to be communicated by passing through a number of intermediate agents, with all the same con-
cerns above. Thus there is ample opportunity to count additional ticks of O’s clock. Since the act of
measurement opens it to state which becomes part of its experiential clock, this suggests that O’s time
can never run backwards as long as it is interacting with other agents.
So, O’s clock is ticking at a rate which it experiences as ﬁxed. And we seem to have discovered that
the speed of any object observed by O
vO ≤1.
(120)
Now that speed is no longer ﬁxed, there is a possibility to measure accelerations too. The question of how
this relates to forces must be left for another time (on the reader’s clock).
The ability to carry out this experiment actually requires us to assume a knowledge of the structure of
spacetime in advance. Clearly, this is already a problem. What makes this uncertain is that an agent (or
compound super-agent) is basically free to measure time according to any clock it wants to. If the clock
is not entirely internal, then it has to deal with the fact that speed is an illusion of the local passage of
time. There are only transitions, which take unit time (by deﬁnition) to complete. The details go beyond
the scope of these notes.
The bottom line is that an agent can only rely on what it observes directly. Promises made by other
agents help it to form a model of the external world, but that is subject to uncertainty, and requires
extensive cooperation between agents to be able to trust18.
5.13
Growth and death of agent based spacetime
Usually, in a spacetime, one considers the number of places to be constant. Cosmology, biology and
technology are exceptions to this. In a discrete spacetime, as in a cell colony, there can be spawning of
new agents (mitosis, meiosis), and both intentional termination (apoptosis) and unintended termination
(necrosis).
In technology, the cosmos of infrastructure is constantly being upturned, growing and shrinking, like a
rich ecosystem. Injection of agents, representing either matter or space, has to be handled in a symmetrical
way: both involve either an increase or decrease in the number of discrete agents. How agents are imbued
with properties is beyond the scope of this work. Semantically, this is only important if the agents make
promises to other agents. The number of agents in a ‘noble gas’ of non-interacting agents is actually quite
uninteresting, except perhaps from a global resource perspective19.
18In distributed knowledge models, like Paxos and Raft, etc, the so-called guarantees of consensus about local knowledge between
agents rests entirely on the non-local assumption that all agents follow the same set of rules without error.
19Sources and sinks are requires to explain the addition of information into a closed space; however, the entire question of
conservation of quantities is beyond the scope of this discussion about spacetime, and it does not seem to be simply related to the
idea of semantics.
56

The expansion and contraction of spacetime, through the birth and death of agents, is mostly about
the keeping or not keeping of promises.
• Is there a cost to introducing/removing a promise?
• Is a new agent addressable (recognition)?
• Is it unique (naming/entropy promise)?
6
Semantic (Knowledge) spaces
The ﬁnal chapter of these notes addresses the goal of unifying dynamics and semantics into a single
description at the spacetime level. The motivation for this begins with technology, but the implications
are much wider. Indeed, I’ve already used examples from chemistry, biology, and physics. The goal
for this section is only to sketch out the mathematical preliminaries, using promise theory to unify the
semantics and dynamics.
A few simple semantic aspects of spacetimes have already shown up in this discussion so far. A further
compelling reason to study the spacetimes with more sophisticated semantics is to model databases,
semantic webs, or knowledge maps, in which relationships and adjacencies have diverse interpretations.
Databases are ubiquitous and knowledge maps are used for artiﬁcial reasoning (e.g. Bayesian and neural
networks) as well as the creation of expert systems; even ‘smart’ public infrastructure (computing clusters
and clouds) and human crowds (e.g. a typing pool, or a classroom) may be considered phases of a
semantic space, along side the materials of the physical world. The utility of this is that is brings a uniﬁed
framework of description that can cross interfaces and disciplines, allowing us to understand the scaling
behaviour of semantics.
Semantics are called ‘meta-data’ in information technology. Another term for a knowledge space
is an ‘index’ for another space. Confusingly, an entire spacetime can be a semantic representation of
something, and that semantic space might also contain its own index, or semantic sub-space. This nesting
is how we can begin to understand scaling.
Paths through semantic spaces form what we think of as processes, stories or narratives. The concept
of a resource, or valuable thing, is a semantic one. Clearly, semantics are so ordinary that we neglect their
proper consideration, taking them for granted.
Knowledge spaces extend the foregoing un-categorized spaces by not merely having structural adja-
cency and dimensionality, like direction etc, but also types or ‘ﬂavours’ of adjacency. By examining the
semantics of such generalized spacetimes, there is an opportunity to better understand the challenges and
potential solutions that present themselves in technology, and perhaps even in fundamental physics too.
We must ask: what kind of a space is a knowledge space? Conversely: in what sense can any other
kind of space share the characteristics of and be interpretable as a knowledge map? Would this help in
the design of shops, warehouses or museums, designed for organizing things by the promises they make?
6.1
Modelling concepts and their relationships
Knowledge modelling introduces abstractions, such as the notion of concepts, and the relationships be-
tween them. Such ideas are mysterious without a framework like promise theory to help formalize them.
Using it, we can de-personalize these building blocks, and understand the issues without too much ado20.
There are two main questions to answer:
• How do we encode a concept within a spacetime?
20Clearly, linguistics and psychology have deﬁnitions for these things, but they are not satisfactory from the viewpoint of formal-
ization for engineering purpose.
57

• How do we relate concepts to one another, using adjacencies?
Ironically, to understand knowledge better, we need to understand its local scaling properties. Approx-
imation, i.e. de-sensitization to speciﬁc location into regions of space, is one of the most signiﬁcant
precepts for modelling at the conceptual level. Concepts, after all, are always described relative to a con-
text of other concepts, things and ideas. This suggests that the application of the compound super-agent
to encoding semantics will be important.
Deﬁnition 13 (Context) Promise theoretically, the context of an agent is the collection of agents in its
neighbourhood, that inﬂuence its semantics from the viewpoint of an observer.
This rough description will be the model of context for explaining how names become concepts. From it,
we can obtain trains of thought, patterns of usage, and so on, by linking agents together.
Deﬁnition 14 (Concept) A concept is an agent, in a semantic space, which has been labelled by an
observer to have a non-numerical coordinate name, and a non-empty context.
A concept agent may be adjacent to a number of exemplary agents, in which case the concept agent links
together a class of other agents. A concept is therefore a source or sink, in the graph theoretical sense,
that binds together exemplars or occurrences into a super-agent, i.e. it mediates a containment promise
(see section 5.10), aggregating agents into a generalization or umbrella class.
Context
Concept
Figure 38: A concept’s signiﬁcance comes from its name and its context together. Without a context, a
concept makes an ambiguous promise.
A similar deﬁnition was given in [28,29]. When does a name or coordinate label become a context?
The aim of this section is to see how this comes about from the promise theory of the foregoing sections.
6.2
Coordinate systems for knowledge spaces
One of the important problems in knowledge management is how to ﬁnd concepts within a repository
space or map. How we coordinatize a space, i.e. name its locations, is the key to the addressability of its
resources: to ﬁnding things and reasoning about them.
The lack of an intuitive coordinate system for concepts and their relationships makes ﬁnding objects
into a brute force search; though indexing is a strategy that can help to reduce the cost of a search. Usually
58

the burden is placed on the observer to have signiﬁcant knowledge of the global topology of a knowledge
space in order to locate elements. For humans, this learning takes a lifetime.
The traditional approach to knowledge structures was to model them as tree-like taxonomies, reﬁning
names into ever smaller categories. This was particularly popular during the reductionist phase of science
of the 1800s. Database theory later came up with the notion of relational tables, in which one has many
similar data records with a ﬁxed template that are interconnected via abstract references. Later still,
hyper-linked structures like Topic Maps and RDF were invented to describe information webs. All of
these structures can be accounted for within the scope of promise modelling. Spacetime points now
become data records or some other kind of aggregated data, linked by adjacencies that are both promised
by the agents themselves and possibly reinterpreted by observers21. Even a mundane data network can be
considered to be a semantic web, in which devices are linked together by physical and logical connections.
Elements of space can promise storage, computing capabilities, memory, and so on.
If we coordinatize something, we encode memory of the adjacencies in a non-local region of space-
time, at the same time as encoding structural semantics. A space is not a Markov process, unless it is in
the gaseous state.
• Tree structures represent branching processes, which suggest reﬁnement of an idea, or the reasoning
about an idea. The tree forms a history, tracing the roots of particular outcomes. If the tree ﬂairs
out, we move to more possibilities of ever lower signiﬁcance (deduction). If the branches converge
to a root, it represents the identiﬁcation of greater meaning from hints of evidence (induction or
abduction).
• Cartesian tuples suggest a regular lattice structure, based of the generators of a translational sym-
metry, or transitivity.
• Networks have a hub and spoke architecture which suggests radial, or graph-like adjacency. This
is a web or ecosystem, emphasizing cooperative relational signiﬁcance.
Knowledge space agents have two roles, in the promise theory sense, to explain their signiﬁcance:
the concepts or tokens of meaning, and the exemplars or occurrences of those concepts. In one common
interpretation of knowledge maps, the conceptual map is considered to be an associative index of the
exemplars.
Since a concept might be composed of a number of others, a scaled understanding, in which the local
coding of concepts is separated from associations between them, emerges naturally from the autonomous
agent model. The link between these two parts is a set of associations called an index. Moreover, because
concepts (e.g. animal) are singular generalizations for exemplars, which are multitudinous, we would not
expect concept space to have anything like a simple lattice structure. More likely, it would be a sparse,
ad hoc graph. Occurrences (e.g. dog, cat), are many and share promised (material) characteristics in
common.
6.3
Semantic distance
The value of semantics lies in the ability to aggregate similar things, more than to decompose things
into separate classes. Branching creates diversity, but aggregation creates uniqueness (see section 6.9).
Branching into more categories is a common modelling viewpoint, but it leads to intentional inhomogene-
ity, whereas aggregation of observations into categories is natural empirically, and leads to continuity and
homogeneous meanings.
21Recently the idea that the brain itself might organize itself physically as a semantic space have been proposed [30], though the
evidence for this is weak. I speculate that the reason is that we do no know how to code concepts in the brain in terms of neuron
coordinates (indeed we do not know how the brain coordinatizes concepts at all).
59

It is natural to keep related things close together in some sense; or, conversely, we might attribute
closeness to mean relatedness. This duality is why the concept of a semantic space proves useful.
Conceptual closeness (aggregation) leads to increased stability of semantics or spacetime regions by
membership. Thus the formation of concepts, as agents which bind several exemplary agents, is more
valuable than the individual exemplars, because the concept is more likely to be ‘remembered’ through
the bindings to others. The association with neuron connections is surley not accidental.
This vaue in aggregation also suggests one way of localizing or coordinatizing knowledge agents,
around sign-posts called concepts. Localization itself has semantics, as distance can be promised in
different ways. Two obvious candidates are:
• Semantic distance: the abstract distance between concepts, as measured by the number of transfor-
mations of hops to get from one concept to another in some representation of the knowledge.
• Occurrence distance: the sum of adjacencies from one occurrence of knowledge to another, as
measured in a spacetime that contains both.
Ultimately, it is up to observers to evaluate the closeness of knowledge elements in their own model. In
section 3.6 we looked at document spaces, where chapter and section numbers could be used to span a
document as vector references. The alphabetical index pointing to a linear page numbering is also a simple
(scalar) standard that is helpful in locating strings in a document. It has been replaced by hyperlinks in
groups of non-linear documents. But what about the space of all documents?
Imagine looking for a book in a shop or library. If you happen to know the coordinate system by
which the books are stored then ﬁnding a book will be quicker than searching through all the rows (as
long as you promise to use the same coordinate categorization as the provider of the index). Libraries
invented the Dewey decimal system of numbering for books (later the universal decimal system) as a
coordinatization of book categories. It tries to make similar things close together, and separate dissimilar
things, by mapping a subject ontology to a relative positive in space.
A typed or categorized matroid to cover such a graph must have duplicate coverings for each type
of link relationship. A numbering of things or topics within ‘subject categories’, ‘promise roles’ or
‘contexts’ depending on nomenclature of choice assigns a number to agents , e.g. occurrences of a word
or phrase in a text, or a particular kind of device in a network. We know such objects by the term ‘index’.
The coordinates are alphabetically ordered, and map to linear page references.
Using the semantic spatial elements,
Deﬁnition 15 (Semantic coordinate system) A tuple numbering of elements in a material network, spanned
by a matroid of the form (scalars, vectors, ...)
The vector components are covariant in the tensor sense. The scalar elements are obviously invariants22.
The ﬁnal matter context for concepts has to be dealt with, as semantics are context dependent. Intu-
itively we expect that observer semantics must be related to use (-) promises.
Recall the example of blueness in section 5.6. What if there were multiple occurrences of a property
or topic, e.g. the name ‘blue’ in different contexts and with different meanings (homonyms)? Suppose
we were then searching for ‘blue’, how could we distinguish different meanings, represented by different
sources? The classical answer in taxonomy is that each blue occurs in a different type or category, as
part of some tree classiﬁcation of containers for concepts. However, we know that matroids are not
non-overlapping taxonomies in general, they can overlap. A more general answer could be that each
one has a different independent set for each different source (see section 6.3). This generates a different
tuple member. Thus we can always form a non-trivial tuple space from a set of singular properties. This
22Notice how there is a simple correspondence between semantic elements with their attached scalar promise networks and the
notion of tables in relational databases. Note also that the homogenization of scalar spacelike components amounts to normalization
of a database in ﬁrst normal form.
60

means that we may unify spacelike properties and singular properties of point-like locations in a regular
coordinate system.
6.4
The autonomous agent view of a knowledge map
Let’s invoke promise theory to explore some of the basic properties of a knowledge map (see ﬁg. 39)23.
From the introduction, we have the idea that a knowledge map has to solve two main problems:
1. How to represent and encode concepts, as well as their exemplars, in spacetime structures.
2. How to relate concepts together to form context and narratives, through adjacency.
A knowledge map is often an abstraction built by an observer as a representation of a real world, but one
may also interpret a real world as a knowledge space in its own right. One could consider a virtual reality
facsimile of a city to be a knowledge map of it. Alternatively, one might consider an encyclopædia index
to be a knowledge map. The important feature is that the entire map has a maintainer, and each maintainer
may create their own map as a valid representation.
has brother
Cat
Fred
Mark
Sally
once owned
called (at 10pm yesterday)
Figure 39: Knowledge maps or semantic webs express typed relationships. Notice how this form mixes
an idea of time into the relationships, so this cannot be a pure representation of spacetime.
Deﬁnition 16 (Knowledge map and maintainer) A knowledge map is a collection of agencies, called
topics or things, which each promise identifying names, and possibly a number of other semantic inter-
relationships. The promised links, or adjacencies, are called associations. The map belongs in its entirety
to an independent agency, called the maintainer.
Because topics or things have intended meanings, we may view them as intentional agents, in the
sense of promise theory. Each associative link asserts an intended property in the manner of an obligation
or imposition to accept; each observer is free to re-interpret this according to its own convention or world
view.
One of the interpretations of autonomous agents is that they exhibit voluntary cooperation. There is
no real sense in which knowledge relationships can be considered ‘voluntary’ in a knowledge structure;
however, we can still think of the relationships as being autonomous, signalling their independence and
local signiﬁcance. Topics in a pre-designed knowledge map can’t refuse to bind with one another24, they
23An overview of knowledge maps and their challenges was given in reference [31]. In that review, the cost of globally obliged
order was evaluated against that of a purely local approach built up from promises.
24This is a curious oversight in data modelling, since all real physical and programmatic systems exhibit the properties of access
control, and the right to refuse an assertion made by another. We choose instead to assume that such matters are irrelevant to the
semantics. This is another example of classic assumption of absolute or necessary space.
61

are imposed by the authority of a designer. However, as knowledge is passed on and assimilated by
merging the maps of different observers into shared map, one still has a choice about whether to connect
two things that assert their connectedness.
6.5
Semantic agencies: things
In promise theory, intentional agents only differ in the promises they make. A priori, they are similar ele-
ments, like stem cells, or empty boxes waiting to be ﬁlled. They are both the promisers and the observers
of characteristics. This models reality better than the assumption of authoritative design, encompassing
it as an option.
Let’s consider ﬁrst how some simple semantic relationships can be modelled as promises, and then
try to contrast this with a database viewpoint. Imagine an agent describing its properties to an observer.
The promising agent’s probably semantic type is hinted in parentheses:
A1(pixel?)
Has colour RGB
−−−−−−−−−−−→
A2
(121)
A1(Swimmer?)
Swims breast stroke
−−−−−−−−−−−−−→
A2
(122)
A1(Book?)
Is a work of ﬁction
−−−−−−−−−−−−→
A2
(123)
A1(Resistor?)
Has resistance 100Ω±5%
−−−−−−−−−−−−−−−−→
A2
(124)
A1(Girder?)
Is made of high grade steel
−−−−−−−−−−−−−−−−−→
A2
(125)
The words in parentheses are possible interpretations of the agent A1 in each case. Nothing demands that
we interpret the agents A1 in this way. Indeed, on receiving promises, it is the purview of A2 to decide
the nature of A1 by observation alone. It is as if the agents are playing a game of charades, and A1 asks:
this is what I promise, what am I?
So who or what is A2? Here, it is some other agent with no particular type. A1 and A2 are symmetrical
comparable things in a promise model. To use an biological analogy, agents are like generic stem-cells
whose characters are differentiated only by the promises they make.
Now compare this promise structure to knowledge relationships in a database of normalized objects.
For example, a hypertext web has the form:
Thing(A1)
Has colour
−−−−−−−⇁
Thing(RGB)
(126)
Thing(A1)
Swims
−−−−⇁
Thing(breast stroke)
(127)
Thing(A1)
Is a work of
−−−−−−−−⇁
Thing(ﬁction)
(128)
Thing(A1)
Has resistance
−−−−−−−−−⇁
Thing(100Ω± 5%)
(129)
Thing(A1)
Is made of
−−−−−−−⇁
Thing(high grade steel)
(130)
The ‘things’ on the right hand side of these arrows are primitive in comparison to intentional agents; they
are simply abstract entities, like information records in a database. The left and right hand sides of the
relationships here are very different kinds of objects, so traversing the knowledge relationship is not like
a simple adjacency, one ends up in a completely different world: from say a world of pixels to a world of
colours. Once we arrive there, if we kept going, where might we end up? What is related to colour? The
answer is clearly very many things, so knowledge relationships tend to be nexuses of connectivity rather
than mere adjacency.
62

6.6
Promising knowledge as a typed, attributed, or material space
To make a spacetime, with both scalar (material) attributes and vector (adjacency) bindings, from the
atomic building blocks of autonomous agents, it is a simple issue of deﬁning the basic elements in a
semantic space:
Deﬁnition 17 (Semantic element (topic)) A semantic element or topic T tuple ⟨Ai, {πscalar j, . . .}⟩con-
sisting of a single autonomous agent, and an optional number of scalar material promises.
A semantic element is thus an autonomous agent from promise theory, surrounded by a halo of scalar
promises that imbue it with certain semantics (see ﬁg. 40). This can be compared to the ﬁeld concept in
Scalar
Scalar
Scalar
Scalar
Scalar
Scalar
A
A
A
Vector
Vector
Vector
Thing /element
Thing/element
Thing/element
Figure 40: Elements of semantic space may be viewed as autonomous agents surrounded by a cloud of
scalar promises, joined into a semantic web by vector promise bindings. Compare this to ﬁgure 30.
ﬁgure 30.
Familiarity is a valuation ranking that an agent can promise to encode Bayesian-style learning into a
knowledge map. This indicates the general utility of the promise model.
For example, to use the biological analogy, the autonomous agent acts as a stem cell, while the scalar
promises correspond to a genome that is expressed by the cell and interpreted semantically, as well as
a ripeness level that ranks its familiarity dynamically. This indicates that we may view a biological
organism (or indeed any functional environment) as a semantic network too.
The associations between the semantic elements may also be deﬁned in terms of promise bindings (I
assume that there is no need to model permission to associate).
Deﬁnition 18 (Semantic association) An uni-directional association between semantic elements T1, T2
is a function f(±b), in a promise binding between the agent component of T, denoted A(T)
A(T1)
+f(b)
−−−−→A(T2)
(131)
A(T2)
−f(b)
−−−−→A(T1)
(132)
The inverse association is
A(T2)
+f(b)−1
−−−−−→A(T1)
(133)
A(T1)
−f(b)−1
−−−−−→A(T2)
(134)
where b is one of the three types of promise body described in section 5.7 (causation, topology and
containment). e.g. f(b) is ‘eats’, and f(b)−1 is ‘is eaten by’.
63

A vector promise which can be mapped to a ± vector promise binding, whose body is a function of
Semantic typing initially seems to destroy any notion of simple translational symmetry. A topic map
could not have long range order, and no obvious notion of continuity, because it does not deal with
instances (occurrences, exemplars) of the concepts directly. Instead it ties them together through the
aggregate concepts. This is a centralized hub network model.
Thus gives semantic knowledge maps like taxonomies, topic maps and RDF their bi-partite, hierar-
chical structure. Locally, one is motivated to restore some translational symmetry however, as this makes
reasoning simpler. In the work with Alva Couch, we proposed re-interpretation of associations for causal
inference as one approach (see section 5.7).
6.7
Indices, meta-data, and code-books for semantic spaces
An index is a map of space, organized semantically. It associates locations within a knowledge space, with
a short identifying name representing the semantics associated with those locations. It may be viewed as
a collection of additional promises made about bulk regions (like pages instead of chapters) so that low
resolution skimming can be used to speed up brute force a search.
An index can only work if there is already a coordinate system by which locations can be referenced,
ordered in some predictable monotonic lattice.
Deﬁnition 19 (Index map) An index is a semantically structured map. It associates knowledge items
with coordinates of elements in a knowledge space. Any structure that maps knowledge items to locations
may be considered an index.
Although we think of an index as a physically represented code book, an index could also be an algo-
rithm, which implements the mapping by computation. Hashing and tree-sorting algorithms satisfy these
qualities.
In a book, topics are (usually) listed alphabetically for quick lookup. Alphabetization is a cheap
hashing function that allows the user to skip obviously irrelevant entries, and search for a matching
name. Coordinates are then provided by page number. In electronic indices, references can be hypertext
references to speciﬁc documents or tagged regions within a document model.
A code book index is usually encoded in a small region of a spacetime. Finding something in a
book with an index is efﬁcient only if we can ignore what is written on the pages as we ﬂick through
them in order to ﬁnd the right page. It is tempting to think that one could simply jump directly to a
location if one knows its reference, rather than traversing all the intermediate locations, but that depends
on the assumption of whatever underlying coordinate system has been implemented to traverse it. In other
words, we can perform a code book compression of the full space into a single coordinate (like G¨odel
numbering), but this does not in itself tell us about the efﬁciency of ﬁnding the coordinate.
An efﬁcient coordinate system, from an indexing perspective, is one that minimizes the number of
points one has to traverse in order to identify the destination.
Deﬁnition 20 (Index compression) An index may be called efﬁcient if the cost of using or locating an
item in the index added to the cost of ﬁnding the location from the coordinates is less than the time needed
to locate the knowledge item directly in the knowledge space.
Categorization of information, such as a taxonomy, could be considered a form of indexing, but
conceptually, taxonomies are often based on the idea of sub-dividing categories so as to separate things
into exclusive containers. (As discussed in section 6.9, this is more likely to be a successful strategy for
associations than concepts.) This leads to exponential growth in the number of outcomes. Aggregation
of objects (e.g. alphabetically or by conceptual generalization) is usually a more efﬁcient way of ending
up with fewer categories to search.
64

Coordinate systems that exploit the natural structure to navigate a space can offer short cuts to locating
elements of the space, but only if we can reduce the information content being addressed. A discussion
of caching in semantic networks is also forthcoming, but beyond the current scope25.
6.8
World lines, processes, stories, events, and reasoning
Much of human thinking revolves around our perception of timelines, i.e. narratives, in which we also
sew together events and attribute the ordered sequence itself a meaning, as a collective entity.
We need not speculate why we have this predilection, but it plays a large role in the way we organize
semantics. In Einsteinian relativity, we have come to refer to the story of a material body as it moves
through spacetime as a world line (see ﬁgure 41). It represents the journal or history
space
time−like
path / trajectory
space−like
time / version
world line
Figure 41: World lines may be space-like (stories) or time-like (histories).
For example, when we design human jobs, experiences, and even spaces for human enjoyment, we
try to craft a storyline or narrative that is either practical or emotionally appealing. Often a sequence ﬁts
this requirement, because of our fondness of stories, and the desire to know how they end.
There might be a sequence of interactions between a mobile (gaseous) agent, like a customer, and
a number of other ﬁxed agents that alter the state of the mobile agent, by interaction with the services
provided by the ﬁxed stations. This is the arrangement in an assembly line, or the converse of a bee
pollenating ﬂowers. Semantics and narratives are at the root of our basic attitudes to information.
For example, when we check into an airport: the airport staff follow you from the outer airport zone
check-in desks, and then follow you through to the boarding gate, where they see you on your way. The
same staff carry out both stages, as the information from both contexts is shared, and the experience is
humanly satisfying. They hand over to security personnel for a screening, and to baggage personnel for
luggage transport. There are thus three agents working alongside from start to ﬁnish: boarding, baggage
and security. The same staff follow the passengers throughout their whole process.
Maintaining constant agency throughout an entire process has the advantage of continuity, not requir-
ing information to be communicated to another agent thus wasting time. It is also satisfying for both
25To discsuss a cache as a kind of index, one must also deal with semantic replication of indentity, see section 6.9.
65

customer and staff. One could organize things differently, however. Different teams could handle the
different specializations. Inside the baggage super-agent, there is a chain of intermediaries. The security
is focused at a single entry point, and walled off by a boundary.
Check−in
Boarding
Paperwork
time
H1
File paper work
Identity check
Welcome
Baggage check
Boarding card
Verify inventory
Boarding card
H2
agency
Figure 42: Following a job narrative thread along the timeline (longitudinally) or staging component-wise
(transversely), with hand-offs to different agents H1 and H2.
Check-in and boarding are two different skills and could also be handled by different agents, but then
both stages would have to promise to collaborate and pass on information. These two decompositions of
a timeline could be referred to as follows (see ﬁgures 42 and 43):
• Continuous threading involves a single agent whose role adjusts to enact the different phases of the
whole process from start to ﬁnish, forming a time-like world-line.
• Discrete staging involves breaking a story into different specialized agents for different phases of
the whole, forming a space-like story.
Welcome
Baggage check
Identity check
Boarding card
Boarding card
File paper work
time
(Longitudinal thread)
Verify inventory
(Transverse stages)
Figure 43: Three agents could work longitudinally (threading), or transversely (staging).
If we stage a timeline, by handing off to different agents, we need interfaces between them, as each
agent really has its own world and timeline with local information26. This is the approach modelled by
bi-graphs. Reasoning semantics are exposed in staging, and hidden in threading. In order to unravel
reasoning as a storyline, we use the rewriting logic of section 5.7 to create a quasi-transitive chain [29,
31,32].
26In programming, one speaks of APIs or Application Programmer Interfaces in between the stages. These are like function calls.
66

Deﬁnition 21 (Story or narrative) An ordered collection of elements belonging to a connected semantic
space, joined by associations with a quasi-transitive interpretation.
This, in turn, can be converted into a sequence of promises. Consider the following story example:
(COMPUTER X) signals (ERROR 13)
(ERROR 13) stands for (DISK FAULT)
(DISK FAULT) can be caused by (LOSS OF POWER)
(LOSS OF POWER) can be caused by (DIESEL GENERATOR)
(DIESEL GENERATOR) is manufactured by (ACME GENERATOR COMPANY)
The agent names are generic agents with scalar promises, and the bold face associations are adjacencies.
To generate stories, one now only needs to follow vector links from a particular initial position, as if
solving a difference equation.
Inference rules are relabellings of associations and pathways that any agent can make, usually about
the quasi-transitive vector promises. Every agent assessing a promised association is free to re-interpret
a semantic assertion [31].
An agent that makes many scalar (material) promises deﬁnes a semantic element type. An agent
which makes many vector adjacency promises forms part of a story structure.
6.9
The semantics and dynamics of identity and context
The goal of promise theory, and indeed semantic spacetime, is to unify the description of dynamics with
semantics. The concept of identity lies at the heart of this uniﬁcation. The promise of identity comprises
two aspects:
1. A distinguishing name or form27 (semantics).
2. A local prominence, relative to its neighbourhood or context (dynamics).
The semantics of identity associate symbolic labels, with singular dynamical structures (singularities,
ﬁxed points, hubs, delta distributions, etc); thus identities are where dynamics and semantics meet. In a
sense, semantics are associated with the opposite of translation invariance and connectedness: they attach
to things that break maximal symmetry, and become identiﬁable. Consequently, we would not expect
complex semantics to be associated with long range effects.
In physics, there is a lot of attention given to invariances and symmetries. This leads to a meta-
knowledge of uniform expectation, without complex semantics. A lack of change means it becomes easy
to predict a trend. However, all the ‘interesting’ phenomena, semantically, occur where symmetries break
down.
To claim to know something is a familiarity value judgement about information. Familiarity increases
the prominence of a location in a semantic space by repeatedly visiting and reinforcing it (see ﬁgure 44).
It mimics the training of memory structures in brains and machine learning methods. This is a stigmergic
cooperation between agents. As a result, knowledge is associated with signals and structures that stand
out against a background noise because they have grown in our subjective valuations.
The signiﬁcance of localization explains why we build monuments, such as obelisks and towers, that
stand out against their backgrounds. The extreme low entropy is attributed with the cultural meaning of
order, authority, or control. Logos are singular symbols imbued with speciﬁc meaning, like ‘brands’, that
stand out against pages of words and or images.
So how do we explain the importance of stories? Pathways are also singular structures, like semantic
world lines, that connect concepts. Semantically meaningful paths are relatively rare within the collec-
tion of all paths. The fact that we are able to infer reasonable stories from the many associations possible
27A name need not be a linguistic token, but a name in the promise theory sense, see [19].
67

Figure 44: The ranking of importance or familiarity is a promisable attribute which can be reinforced at
each spacetime location by frequency of visitation. It plays a major role in identity, as familiar things
‘loom large’ in the space.
between concepts suggests that the (dynamically based) transitivities generalized in section 5.7 are auto-
matically evaluated when recognizing a concept. So it is not concepts that are hierarchically evaluated,
but associations. This is fascinating, though beyond the scope of this work.
The concept of informational entropy is descriptive here. A symbol or structure with high entropy
represents a highly ﬂat distribution of much information, but little meaning. Indeed, noise has the great-
est information content of all, just the lowest signiﬁcance. Conversely, low entropy represents little
information, but we attribute to this great signiﬁcance: it is something that stands up (like a Dirac delta
distribution) anomalously against the background of a statistical average. Thus the association between
information and meaning is an inverse one.
The singularity of concepts makes them sound fragile. Indeed, singular items are often considered
fragile, because they are ‘single points of failure’ or ‘bottlenecks’. Destroying one would inﬂict irrepara-
ble change on the connectedness of a semantic space. The way concepts become robust is by associating
with many example occurrences, and related.
If relationships were only between concepts and not the exemplary occurrences, then the concept hubs
would still bring fragility to a knowledge map. One of the advantages of a spacetime perspective is in
being able to redesign them so that associations can also be made directly between the exemplars.
For a cluster of agents representing a concept, the complete ‘identity’ of the concept lies in the sum
of associations as much as any one of the agents in the cluster. This is reminiscent of organizational
membership and containment too, as in bi-graphs. In other words, its context deﬁnes its meaning as
much as the promises it makes, through its bindings. The more interconnected it is, the more robust a hub
of semantics it becomes. As long as the associations are ﬁxed, the meaning is constant. If none of the
contextual bindings is preserved, an agent essentially has a new identity.
6.10
Low-entropy functional spaces, and componentization
Spacetimes, in which each agent is of unique and special signiﬁcance, represent specialization networks.
One could characterize them as a molecular phase, rather than gaseous or crystalline phase of atomic
agents, because there is local structure with global variation (see ﬁgure 45).
The entropy of agents’ promise distribution is low in this kind of structure, as most agents stand out
68

uniquely in a special role. A low spatial entropy network consists of mostly unique exemplars, possi-
bly indexed or organized by general concepts. They contrast with bulk materials of indistinguishable
network
user
software
ruby
java
network−library
storage
math−library
drawing−library
memory
CPU
C
power
system
settings
Figure 45: A semantic spacetime of low entropy is an ecosystem of functional roles.
resources (see section 6.11). Thus a low entropy network has strong semantics, but is fragile dynami-
cally, as each element’s uniqueness makes it a possible point of fracture in the integrity of the whole.
Adjacency combined with semantic uniqueness implies dependency. For this reason, low entropy implies
‘meaningful’ as well as dynamically fragile or potentially unstable.
Human organizations, shopping malls, organizational maps, electronic circuitry, and computer ﬂowchart
algorithms are all examples of low entropy structures, because a change at a single spatial element would
alter the perceived semantics of its observable purpose measurably. See ﬁgure 45.
Databases also typically have low entropy, because each data record has unique content. The coordi-
natization of databases is interesting as it blends dynamical and semantic strategies. A simple numbering
of elements using an integer primary key is simple and reliable, but has no interpretation. One there-
fore imposes a tabular matroid that has coordinates based not only on an integer number, but also the
individual promises of content being represented: e.g.
(key, name, address, occupation)
(135)
Some guiding principles for modelling and coordinatizing semantic data have been developed in
the guise of the so-called normal forms [21, 33], which attempt to avoid the many-worlds problem of
inconsistent branches. Normal forms constrain the similarity and redundancy of coded information within
spatial elements of the database (tables or records). The normal forms are thus entropy constraining
patterns.
A database or space is in the ﬁrst normal form if all of the elements have the same size, shape and
make the same type of promises. Thus, one effectively ignores the uniqueness of the data (or the bodies
of the promises), paying attention to the similarity of form only, like the template shown in (135). This is
similar to the idea of indexing, but without a separate map.
A ﬁrst normal form might include repeated patterns, which are orthogonal degrees of freedom form-
ing a natural subspace, like say the addresses of persons. Semantically, one would like to extract any
redundancy to make a single point of change, because as a human tool, we don’t want to have to update
the same information in more than one place. This could lead to inconsistencies. Information inside one
agent is in a separate ‘world’ to that of another, meaning that there is no causal connection between the
information. To assure the causality of a change of address, one could factor it out as a dependency.
69

Name 1
Name 1
Name 2
Address
Address
Name 2
Address
Figure 46: Normalization is factoring out a common dependence. It increases semantic focus at the
expense of dynamical fragility.
For instance, several people can live at the same address, so one could extract the address as an
independent class of agents (role) so that the promise that a person lives at an address is independent of
the address’s details. This makes it more reliable and potentially less work to make a change of address
to one or more agents.
There are several more extended normal forms, which attempt to go even further and separate out
hidden dependences. From a spacetime perspective, normal forms therefore turn agents into super-agent
clusters. From a promise theory perspective, they encourage the separation of agency, and the avoidance
of diverging worlds, at the expense of increased complexity. As the number of agencies is increased, the
number of promises required to bind them in association grows as the square of the number.
The many worlds problem can easily be overstated. It is fundamentally about managing causality
during change, and reliance on dependency helps to make consistency inevitable. This is particularly
important because database change was originally expected to be entered by human operators, prone
to errors during repetitive tasks. In data warehousing, conversely, duplication of information is not a
problem, as the database is populated by machinery from an independent source. This duplication can be
done without human error. Similarly, in database replication for backup and disaster recovery, duplication
is actually an intended strategy because of the fragility problem discussed above.
The many worlds problem haunts simple cloning of data, nonetheless, by re-introducing the possi-
bility that inconsistent promises might be observed by agents working concurrently with the process of
duplication. Since a cloning is a change of promises in space, it is also a change of time, so duplication
cannot be an atomic operation, without active concealment. At best copying can be gestated in isolation,
typically by mutex locking or temporary partitioning of space, so that time is perceived as stopped for
observers for the duration of the process. See also the branching discussion in section 6.13.
6.11
High-entropy load-sharing spaces and de-coordinatization
Repetitive network structures are found in many redundant systems, including biological tissues, physical
materials, and artiﬁcial communications networks. Unlike the low entropy ecosystems, the high entropy
regularity serves an effective equivalence relation, based on low individual signiﬁcance. With low signif-
icance comes higher resilience. No point in such a network has any particular identity, and thus the items
are interchangeable, and the spatial structure is resilient to point disruptions.
The point, in practical terms, is that we don’t need to identity speciﬁc agents; ﬁnding one that is
‘good enough’ is sufﬁcient to quench a need. This return to translational invariance and high entropy
distributions suggests that coordinates are of less signiﬁcance. It might even be seen as a goal, from a
technology perspective, to do away with them altogether, even though humans habitually name things we
work with, as part of forming a working relationship with them.
It is tempting to think that algorithms of information science, like hashing functions, sorting trees, and
so on, might remove the need for coordinates by using the essence of their structure to locate particular
regions.
70

• Hashing functions assign a unique integer to every unique data pattern.
• Sorting trees (like a coin counter) let data percolate left or right depending on a criterion (e.g. size,
alphabetic value, etc).
However, these algorithms merely transduce semantic names into spatial processes faithfully. They only
work if there is a coordinate system to use as a pointer. They shift part of the coordinate problem into the
names themselves, like a code book (which is, of course, itself an index). A token string gets converted
into a numbered location by mapping using a total function, somewhat like assigning hotel visitors room
numbers. This is not an elimination of coordinates, but rather a spatial representation of semantics via
a transformation function. It is a form of possibly irregular auto-indexing (see work on Batcher-Banyan
networks [34]).
One might completely eliminate identity from a spacetime, and still interact with it. It would be like
interacting with empty space; some material promises are needed to grasp onto. To eliminate all identity
would be to eliminate all information, and hence all structure from spacetime, leaving only boundary
conditions, which are a form of identity. There must be a source of information somewhere to associate
material promises with speciﬁc locations, data with records, examples with concepts, else how could they
come together (e.g. the name of a body entering the featureless space)?
The question becomes about how semantics are interpreted from dynamical patterns. A few possibil-
ities could be explored:
• Spacetime is fundamentally ordered as a lattice with long range order, or hierarchy. Then a small
tuple with coordinate numbering sufﬁces to impose a map of locations. Within such a regime, we
need to place ‘tenants’ at locations:
– We use the data in the body of a promise to compute and impose the location (spatially
ordered). In this case the information lies in the coding (name) of the change events which
describe time. The source of this information is unknown, but it can be observed.
– We stack items ‘ﬁrst come ﬁrst served’ (FCFS) (time ordered imposition).
• Spacetime is disordered, and something is mixing the gaseous mixture so that interaction might be
perceived as a random collision. Now the source of the mixing information is unknown.
Figure 47: A dispatcher is an indexing code book for mapping promises to locations.
For example, consider a biological tissue as a homogeneous, high entropy space of cellular agents.
Blood, hormones and other biochemical signals diffuse through the tissue with a chance of absorption,
71

to distribute the interaction load across the bulk. In order for this to work, batches of incoming promises
(e.g. blood) have to be ﬁnely grained so that they can be shared and bind to individual receptors.
Another example of load assignment could be queueing lanes in a supermarket or airport check-in
counter. Servers are chosen by customers on the basis of visual feedback of who is free, or by using
a single incoming funnel queue and dispatch process. The agent selecting locations for the incoming
customers promises to show queue lengths to customers, which in turn promise to use the information
to self-organize. The decision can be represented by a dispatcher, which represents the algorithm for the
decision (see ﬁgure 47). In technology one often uses a ‘load balancer’ dispatch agent as a specialist
semantic component. This breaks the symmetry of the uniform load-bearing space with a singular point.
It has high semantic value, so it is easily understood, but it has low dynamical integrity so it is a bottleneck
and single point of failure.
A similar idea is a round-robin dispatcher, which counts the state of the next server to allocate, modulo
the total number of servers. A round-robin algorithm is a trivial hashing function that always indexes new
arrivals to the next queue, like going around a clock. In either case, this is not a Markov process. It
requires memory to implement: memory which must be encoded into space too. Thus, the spatial world
is used as the memory for coordinating the process.
These examples are forms of collective stigmergic cooperation, in which the shared state of the pro-
cess is written or encoded into the environment itself as part of the total state. Ants and other insects
use this approach to cooperate, with pheromone trails. The partitioning of regions of space for different
purposes becomes part of the general functional semantics of the space. The processes described require
memory and a minimal identity to cooperate, whether to maintain an order (as in FCFS) or a stigmergic
trail.
An example of a technology, with a more sophisticated spacetime of medium-to-high entropy, and
which uses physical structure to navigate, comes from the world of datacentres and resources. Network
architectures are designed to hard-code navigational properties (analogous to spanning trees). However,
they are designed with functional semantics that ignore their symmetries as spaces. When drawing de-
signs, the ﬂatness of a printed page leads engineers to draw networking structures in two dimensions,
whereas a natural coordinatization would be three dimensional.
Consider the tree-like structure in ﬁgure 48, which has been used in some cluster communications
networks. In two dimensions, it appears to be a slightly muddled tree. However, if we re-draw it in
N
E
S
W
C
3
C4
C
2
C
1
N
E
S
W
Figure 48: A Body Centred Cubic (BCC) lattice. Regularity allows us to exploit symmetries.
three dimensions, it turns out to be a very well known structure in material science: a body-centred cubic
lattice, similar (but not quite identical) to that of Iron (see Figure 49).
Another example is the non-blocking networks, as invented for telephony, which are now equally
important in modern datacentres, because they create regular load-sharing patterns that are extensible.
Consider the pattern in ﬁgure 50.
72

C
C
N
E
S
W
1
2
Figure 49: The BCC lattice in three dimensions.
S
S
S
S
S
LR
LL
RL
RR
S
L
R
P
P
P
P
LL
LR
RL
RR
A
L
A
A
A
A
A
A
A
L
L
L
L
L
L
L
LL
LL
LL
LL
LR
LR
LR
LR
RL
RL
RL
RL
RR
RR
RR
RRR
L
R
L
R
L
R
L
R
L
R
L
R
L
L
R
Figure 50: A simple 2x2 redundant Clos non-blocking network or fat tree network. Half the top links to
the right hand nodes of the second level are missing for clarity.
The same argument applies here. The inability to choose a natural spacetime-motivated coordinate
system, in three dimensions, leads to the complexity of real and current datacentre designs. Racks and
servers are mounted in a three dimensional cubic lattice structure, because this is how we design humans
spaces, but the network devices are connected in a three-dimensional tree-like form (Figure 50), with a
radial symmetry. The amount of criss-crossed and folded cabling required to perform this unholy union
leads to mind-boggling cabling complexity inside datacentres today.
Could this be avoided? The simpler cubic lattice suggests that we might have our cake and eat it. In
fact, the more robust Clos network can also be redrawn in a simpler geometry. If one looks at Figure
50, any mathematician would immediately notice a regular radial symmetry. Indeed, if we begin to
unravel the topology by re-drawing and un-twisting the connections, in three dimensions, a surprising
thing happens.
The ﬁrst step is shown in Figure 51. Instead of thinking about the network as a hierarchy from top
to bottom, and use the symmetry of the structure to avoid crossing cables. The remaining cables at top
and bottom, which seem to cross, do so because we are projecting a three dimensional structure into two
dimensions. If we bend the middle layers into a torus (doughnut) by rotating them 90 degrees and arrange
the outgoing connections, then we can unfold the entire structure as a toroidal geometry as shown in
Figure 52.
73

LL
A
R
LLL
R
LR
S
SRR
S
S
LL
RL
P
P
P
P
LL
LR
RL
RR
ALL
LL
L
L
L
A
A
A
A
A
A
L
L
L
L
L
L
Figure 51: Re-drawing the 2x2 Clos network, still in two dimensions.
The advantage of a radial design is that all nodes can reach on another by line-of-sight connections, or
perhaps with a simple mirror to reﬂect back inside the inner annulus, thus avoiding the need for expensive
folding of ﬁbre-optic waveguides (which need replacing as they grow brittle). Lasers could connect
separate units directly. Perhaps such datacentres will be built in the future.
If the leaf nodes are functionally distinguishable (low entropy), the only interesting identity to assign
names to is the port number, or leaf address, but for practical fault ﬁnding it might be useful to encode
part of the path or story for a host. A triplet such as (Spine, rack, node) would serve both needs.
If leaves could be interchangeable (high entropy), then no coordinate labels at all would be strictly
necessary, just as one has no need to label the atoms in a sheet of metal. The technological challenge
then is to distribute the load without imposing a technology based on an unnatural and unnecessary set of
identities. Self-routing fabrics, are one example of this [34].
6.12
Coordinatizing multi-phase space (a ubiquitous Internet of Things)
If some agents are ﬁxed and some are mobile or ﬂuid in their adjacencies, this presents a conundrum
for the spanning sets. There are two choices: either one ﬁxes the coordinates of the gaseous agents and
de-couples the naming from the adjacency, or one redeﬁnes the coordinate matroids continuously along
with the ﬂow of the mobile agents.
There does not seem to be a precedent for this kind of labelling. It would seem natural to put mobile
agents in an entirely orthogonal set of dimensions so that their wanderings in the other dimensions can
take place without affecting their numbering in the gaseous phase. This wandering from ﬁxed location to
ﬁxed location is sometimes called ‘homing’. The mobile agents enter orbit around their home worlds. If
conﬁgurations are distinguishable in the gaseous phase, then this generates a passing of time, in the sense
already described. This can only happen by changing their internal (scalar) properties.
A practical consequence of the spatial labelling, for knowledge spaces, lies in how we ﬁnd agents
quickly. Without any symmetry to guide the ﬁnding of locations, searching a space is a case of exploratory
mapping.
A map, directory or index is area of space, which promises to associate names with coordinates. To
be useful, it should be signiﬁcantly smaller than the space it maps, so that searching the map or index is
less costly than searching the space itself.
74

Figure 52: The Clos network can now be unfolded into a radial geometry with line-of-sight connections
that could be maintained by direct ﬁbre-free laser optics.
indexi = (namei, coordinatei)
(136)
Indices are only effective it the list of signiﬁcant names is less than or equal to the list of spatial
locations. In the worst case, any space may be considered an index for itself. This is what we mean by
coordinates. The more we aggregate structures, and assign them non-unique names, the more we can
compress spatial structure into coarse-grained locations.
In a two-phase space, the only thing that has to change is the addition of extra dimensions that do not
possess long range order.
6.13
Proper time, branching, and many worlds
The voluntary partitioning of space into semantically independent regions, whether by boundary or vol-
untary abstention, is one of the tools we use most often in technology, and nature itself has found this to
be a stable strategy in dynamical processes.
Branching processes are processes in which a space partitions into two or more disjoint sets, which
then live out causally separate histories. Sometimes, the separate entities might continue to interact as
with one another, or they might part company forever, recombine, or remain weakly coupled. Examples
include:
• Cell mitosis (cell division) in biology,
• Process forking or cloning in computer operating systems.
• Worms and simple life-forms subjected to the guillotine.
• Code version branching in software engineering
• Involuntary loss of connectivity in a network due to breakage.
75

Once separated, the version history of each region is a separate bundle of world lines which can evolve in
independent ways, with their own private space and time. They become separate worlds, in the Leibniz,
Kripke, Everett sense.
Nothing guarantees that each world timeline will be unique. Coincidentally, two branches might
actually end up in the same state, in which case they form indistinguishable worlds, but this is unknowable
to any branch28. Two versions that are indistinguishable in the same timeline represent the same time.
Once separated, each region is its own proper time clock. This is true whether it occurs by voluntarily
limiting its perception into non-overlapping regions, or by physical separation. Thus each agent measures
time according to the information it receives.
As observers’ worlds diverge, the space the see has to grow in order to maintain time in their branched
world. In the ultimate case where space completely partitions into a scenario of singular agents each in
their own world, time must stop in each branch, because the branching reduces the number of states that
can represent change. A perfect static equilibrium between agents would arise. Similarly, when a promise
has been kept, and no change of state can be registered due to convergence, there are no changes of state
by which to experience time. Thus promised process narratives are the black holes of semantic space:
singularities from which timelines come to a static equilibrium (an operator ﬁxed point).
There is an simple relationship between many worlds and the breakage of networks into partitions that
cannot communicate29, though these breakages lead to merging of worlds with associated collisions of
intent. The same thing happens in software versioning systems. Software versions are not truly separate
worlds, merely embedded channels within a larger world which contains independent clocks. As swim-
ming lanes, embedded in a larger space, they are just voluntarily longitudinal super agents, observable by
external agencies. This is why they can be indexed and why versions can be separated from clock time.
Knowledge spaces, which are facsimiles of other processes, change connections more dynamically
than spaces with few semantics, as they encode memories. They are often fed continuously with new
information, as representation of an independent, external process. Each change or addition leads to a
new version of the knowledge, or a new proper time in the knowledge spacetime, but time is not usually
a measure of interest, except to know that it occurs. Knowledge spaces are indices, if only of themselves.
The branching phenomenon can also happen at the meta-level of the maintainer of the knowledge
space. If one information base is cloned and then each part develops in isolation, they will evolve away
from one another, as different species.
7
Closing remarks
This now quite lengthy review of ideas about spacetime attempts to compare and contrast differing views,
focusing on interpretational semantics. Although, many pages were needed to sketch out these ideas, they
only scratch the surface of application to the worlds we inhabit and create.
The concepts of space and time used in these notes might seem irritatingly contrary to convention
from the standstead of physics, yet they are in fact very ordinary, and seem entirely natural for closed
discrete systems. Quite possibly, one could construct a continuum limit, for large number, to obtain
results of the kind we are more used to in natural science. If that is the case, intentionality must disappear
in the continuum limit.
Some of the key ideas were:
• Agency plays a role in interpreting spacetime.
• Semantics (identity and its dynamical prominence) play several roles in mapping out space and
time.
28There is no reason to suppose the usual science ﬁctional account in which many worlds branches all represent distinct realities.
29The divergence of these worlds would also be interpreted as inconsistencies, in the sense of Paxos or the CAP conjecture.
76

• Dynamics (magnitude and semantic interpretation) are heavily constrained by semantics of ob-
servers.
• Coordinate systems (matroids) can be introduced for all topologies, to interpret space.
• A discrete space with non-trivial semantics must be able to exist in different phases (gas, molecular,
and solid).
• the ﬁdelity of transmitted information cannot be guaranteed in a space with non-trivial semantics.
• The ability to deﬁne motion and speed is not to be taken for granted in a discrete transition system.
• Branching into causally independent worlds has to be taken seriously both in natural science, and
especially in artiﬁcial spaces.
With this review of concepts for reference, one may now proceed to apply the ideas more rigorously
to the study of intentional spaces, such as information systems for the modern world.
Acknowledgement
I am grateful to Paul Borrill for interesting discussions on the nature of time.
References
[1] T. Regge. General relativity without coordinates. Nuovo Cimento, 19(3):558571, 1961.
[2] S. Weinberg. Gravitation and cosmology: principles and applications of the general theory of
relativity. Wiley, 1972.
[3] D.H. Foster. Fuzzy topological groups. J. Math. Analysis and Applications, 67(2):549–564, 1979.
[4] J. Bjelland, M. Burgess, G. Canright, and K. Eng-Monsen. Eigenvectors of directed graphs and
importance scores: dominance, t-rank, and sink remedies. Data Mining and Knowledge Discovery,
20(1):98–151, 2010.
[5] C.E. Shannon and W. Weaver. The mathematical theory of communication. University of Illinois
Press, Urbana, 1949.
[6] M. Burgess and A. Couch. On system rollback and totalized ﬁelds: An algebraic approach to system
change. J. Log. Algebr. Program., 80(8):427–443, 2011.
[7] Mark Burgess.
Analytical Network and System Administration — Managing Human-Computer
Systems. J. Wiley & Sons, Chichester, 2004.
[8] M. Burgess. On the theory of system administration. Science of Computer Programming, 49:1,
2003.
[9] R.S. Varga. Matrix Iterative Analysis. Prentice Hall, Englewood Cliffs, New Jersey, 1962.
[10] H. Minc. Nonnegative Matrices. Wiley Interscience, New York, 1987.
[11] M. Burgess and G. Canright. Scaling behaviour of peer conﬁguration in logically ad hoc networks.
IEEE eTransactions on Network and Service Management, 1:1, 2004.
77

[12] H. Lewis and C. Papadimitriou. Elements of the Theory of Computation, Second edition. Prentice
Hall, New York, 1997.
[13] Logic. Mathematical logic around the world. http:///www.uni-bonn.de/logic/world.html.
[14] N.K. Jerne. The generative grammar of the immune system. Nobel lecture, 1964.
[15] D. Watt. Programming language syntax and semantics. Prentice Hall, New York, 1991.
[16] D. Hofstadter. G¨odel, Escher, Bach: an eternal golden braid. Penguin books., Middlesex, England,
1979/1981.
[17] R. Milner. The space and motion of communicating agents. Cambridge, 2009.
[18] Mark Burgess. An approach to understanding policy based on autonomy and voluntary coopera-
tion. In IFIP/IEEE 16th international workshop on distributed systems operations and management
(DSOM), in LNCS 3775, pages 97–108, 2005.
[19] J.A. Bergstra and M. Burgess. Promise Theory: Principles and Applications. χtAxis Press, 2014.
[20] Mark Burgess. In Search of Certainty - The Science of Our Information Infrastructure. χtAxis
Press, November 2013.
[21] C.J. Date. Introduction to Database Systems (7th edition). Addison Wesley, Reading, MA, 1999.
[22] J. Schwinger. Theory of quantized ﬁelds i. Physical Review, 82:914, 1951.
[23] J. Schwinger. Theory of quantized ﬁelds ii. Physical Review, 91:713, 1953.
[24] Paul Borrill, Mark Burgess, Todd Craw, and Mike Dvorkin. A promise theory perspective on data
networks. CoRR, abs/1405.2627, 2014.
[25] S. Weinberg. Lectures on Quantum Mechanics. Cambridge University Press, 2012.
[26] R.P. Feynamn. Space-time approach to quantum electrodynamics. Physical Review, 76:769, 1949.
[27] F.J. Dyson. The radiation theories of tomonaga, schwinger and feynman. Physical Review, 75:486,
1949.
[28] K. Erk. Supporting inferences in semantic space: representing words as regions. In Proceedings of
the 8th Conference on Computational Semantics, pages 104–115, 2009.
[29] Mark Burgess.
Knowledge management and promises.
Lecture Notes on Computer Science,
5637:95–107, 2009.
[30] AlexanderG. Huth, Shinji Nishimoto, AnT. Vu, and JackL. Gallant. A continuous semantic space
describes the representation of thousands of object and action categories across the human brain.
Neuron, 76(6):1210 – 1224, 2012.
[31] M. Burgess.
New Research on Knowledge Management Models and Methods., chapter What’s
wrong with knowledge management? The emergence of ontology. Number ISBN 979-953-307-
226-4. InTech, 2012.
[32] A. Couch and M. Burgess. Compass and direction in topic maps. (Oslo University College preprint),
2009.
78

[33] M. Burgess. Analytical Network and System Administration — Managing Human-Computer Sys-
tems. J. Wiley & Sons, Chichester, 2004.
[34] M.J. Narasimha. The batcher-banyan self-routing network: universality and simpliﬁcation. Com-
munications, IEEE Transactions on, 36(10):1175–1178, Oct 1988.
A
Graph bases and coordinatized dimensions
It is instructive to see examples of how to specify the dimensional elements in a graph, using the notion
of matroids or independent sets. This puts the description of graphs on a par with that of lattices and
manifolds.
Consider the graph shown in ﬁg. 53. We may use this as a simple example. The graph has a self loop
and a tree structure.
5
4
3
2
1
Figure 53: A small graph - but how many dimensions does it have?
The adjacency matrix for this graph is:
Aij =






1
1
0
0
0
1
0
1
0
0
0
1
0
1
1
0
0
1
0
0
0
0
1
0
0






(137)
This can be decomposed into a number of generators of spanning sets. If we choose rank r, then
Aij =
r
X
a=1
Ia.
(138)
A.1
Example 1
For example consider the matroid basis shown in ﬁg. 54. This may be written as independent link sets as
a linear decomposition of the adjacency matrix with three independent sets to span the space.
A = I1 + I2 + I3
(139)
Aij =






1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0






+






0
1
2
0
0
0
1
2
0
1
2
0
0
0
1
2
0
1
0
0
0
1
0
0
0
0
0
0
0






+






0
1
2
0
0
0
1
2
0
1
2
0
0
0
1
2
0
0
0
0
0
0
0
1
0
0
0
1
0






(140)
79

5
4
3
2
1
I
I
I1
3
2
Figure 54: A 3 dimensional spanning basis.
The fractional factors show that the basis is not orthogonal (i.e. disjoint). The spanning sets do overlap.
Interpreting the spanning sets as the imposed dimensionality, and grouping related points together, we
obtain tuples.
v1
=
(1, 1, 1)
(141)
v2
=
(0, 1, 1)
(142)
v3
=
(0, 2, 2)
(143)
v4
=
(0, 3, 0)
(144)
v5
=
(0, 0, 3)
(145)
As the regions overlap and contain multiple points, The three points lying along directions I2 and I3 are
labelled simply with coordinates 1,2,3 as consecutive elements. This ordering is arbitrary, of course, but
can be motivated by the action of the adjacency sets as translation generators to generate the ordering.
The names of the node in the graph need not match (they belong to a global namespace).
Notice also that I wrote a value of 0 in the spirit of a linear interpretation, meaning ‘does not depend
on this direction’, but this is an arbitrary choice of ordinal that symbolizes that we are beyond the edge
of the space in this direction. One could easily write b for ‘boundary’ or ‘nw’ for nowhere. I like the way
this enforced dimensionalization shows how the appropriateness of the concept of dimensionality is more
closely related to regular symmetry than to connectivity or the availability of a sense of direction. Put
another way, the semantics of direction may be freely chosen, but not without the cost of some weirdness
relative to a regular lattice world.
If we add an edge to the graph as in ﬁg. 55, so that we create a non-trivial cycle, then nothing changes
in the coordinates: all the nodes as still in the same place. All we’ve done is to change the underlying
topology. This shows that coordinates and topology are separate descriptions that do not necessarily
5
4
3
2
1
I
I
I1
3
2
Figure 55: Adding an edge to make a cycle does not change the basis, as loops cannot be included.
reﬂect one another, except in cases of assumed symmetry.
80

A.2
Example2
Next consider ﬁg. 56 This may be written as independent link sets as a linear decomposition of the
5
4
3
2
1
I1
I2
4I
3I
Figure 56: A 4 dimensional spanning basis.
adjacency matrix:
A = I1 + I2 + I3 + I4
(146)
Aij =






1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0






+






0
1
0
0
0
1
0
1
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
0






+






0
0
0
0
0
0
0
0
0
0
0
0
0
1
0
0
0
1
0
0
0
0
0
0
0






+






1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
1
0
0
0
1
0






(147)
In this basis of dimension 4 (which we observe subtly is labelled by links not nodes), we write tuples for
the vertices v1 . . . v5 as:
v1
=
(1, 1, 0, 0)
(148)
v2
=
(0, 1, 1, 0)
(149)
v3
=
(0, 1, 1, 1)
(150)
v5
=
(0, 0, 1, 0)
(151)
v5
=
(0, 0, 0, 1)
(152)
A.3
Example 3
In ﬁg. 57 we see an alternative three dimensional matroid basis.
5
4
3
2
1
2I
1I
3I
Figure 57: Alternative 3 dimensional spanning basis.
81

Aij =






1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0






+






0
1
0
0
0
1
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0
0






+






0
0
0
0
0
0
0
1
0
0
0
1
0
1
0
0
0
1
0
1
0
0
0
1
0






(153)
Interpreting the spanning sets again as the imposed dimensionality, and grouping related points together,
we obtain tuples.
v1
=
(1, 0, 0)
(154)
v2
=
(0, 1, 1)
(155)
v3
=
(0, 0, 2)
(156)
v4
=
(0, 0, 3)
(157)
v5
=
(0, 0, 4)
(158)
Four of the vertices now exist inside the direction I3, whereas only one vertex is in each of I1 and I2.
A.4
Artiﬁciality of dimensions
It is instructive to take an extreme case of a linear graph and reinterpret it in a three dimensional basis. To
build sufﬁcient rank, we then need at least four vertices (see ﬁg. 58).
1
1
2
3
4
2
4
3
Figure 58: A line bent into three dimensions
Taking each link as a separate orthogonal direction, we obtain the coordinates of the four points to be:
v1
=
(1, 0, 0)
(159)
v2
=
(1, 1, 0)
(160)
v3
=
(0, 1, 1)
(161)
v4
=
(0, 0, 1)
(162)
(163)
Viewing these in a pseudo 3d lattice shows that the constraints simply turn this into a bent string. It is in
the nature of graphs to be bounded, though this one is rather extreme. What’s important to see is that the
degrees of freedom are no longer related to the dimensionality, but rather to the connectivity constraints.
With such coordinatizations, the computation of distance no more convenient. There is no simple
Pythagoras formula, or line of sight distance, because we cannot interpolate the existence of smooth
continuous expanse.
82

