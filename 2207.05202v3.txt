Version December 23, 2022
Preprint typeset using LATEX style openjournal v. 09/06/15
THE COSMIC GRAPH: OPTIMAL INFORMATION EXTRACTION FROM LARGE-SCALE STRUCTURE
USING CATALOGUES
T. Lucas Makinen
Imperial Centre for Inference and Cosmology (ICIC) & Astrophysics Group, Imperial College London, Blackett Laboratory, Prince
Consort Road, London SW7 2AZ, United Kingdom and
Harvard & Smithsonian Center for Astrophysics, Observatory Building E, 60 Garden St, Cambridge, MA 02138, United States
Tom Charnock
Freelance consultant in statistical modelling
Pablo Lemos
Department of Physics and Astronomy, University of Sussex, Brighton, BN1 9QH, UK and
University College London, Gower St, London, UK
Natalia Porqueres & Alan Heavens
Imperial Centre for Inference and Cosmology (ICIC) & Astrophysics Group, Imperial College London, Blackett Laboratory, Prince
Consort Road, London SW7 2AZ, United Kingdom
Benjamin D. Wandelt
Sorbonne Universit´e, CNRS, UMR 7095, Institut d’Astrophysique de Paris, 98 bis boulevard Arago, 75014 Paris, France and
Center for Computational Astrophysics, Flatiron Institute, 162 5th Avenue, New York, NY 10010, USA
Version December 23, 2022
ABSTRACT
We present an implicit likelihood approach to quantifying cosmological information over discrete
catalogue data, assembled as graphs. To do so, we explore cosmological parameter constraints using
mock dark matter halo catalogues. We employ Information Maximising Neural Networks (IMNNs) to
quantify Fisher information extraction as a function of graph representation. We a) demonstrate the
high sensitivity of modular graph structure to the underlying cosmology in the noise-free limit, b) show
that graph neural network summaries automatically combine mass and clustering information through
comparisons to traditional statistics, c) demonstrate that networks can still extract information when
catalogues are subject to noisy survey cuts, and d) illustrate how nonlinear IMNN summaries can
be used as asymptotically optimal compressed statistics for Bayesian simulation-based inference. We
reduce the area of joint Ωm, σ8 parameter constraints with small (∼100 object) halo catalogues by a
factor of 42 over the two-point correlation function, and demonstrate that the networks automatically
combine mass and clustering information. This work utilizes a new IMNN implementation over graph
data in Jax, which can take advantage of either numerical or auto-diﬀerentiability. We also show that
graph IMNNs successfully compress simulations away from the ﬁducial model at which the network is
ﬁtted, indicating a promising alternative to n-point statistics in catalogue simulation-based analyses.
Subject headings: cosmology, large-scale structure, statistical methods, machine learning, graph net-
works, galaxy surveys
1. INTRODUCTION
Modern cosmological analyses typically focus on ob-
taining theory and parameter constraints from com-
pressed summary statistics obtained from ﬁeld data such
as the Cosmic Microwave Background or weak lensing
mass-maps (Tegmark et al. 1997; Alsing & Wandelt 2018;
Jeﬀrey et al. 2020).
Recently, ﬁeld-level analyses like
Porqueres et al. (2021); Leclercq & Heavens (2021), al-
though computationally expensive, have made it possible
to sample the full ﬁeld at the pixel level to ensure all sur-
vey information is accounted for in posterior construction
for cosmological parameters.
However, the data collected by telescopes are often in-
Electronic address: l.makinen21@imperial.ac.uk
stantly compressed into discrete catalogues of sources,
like galaxies and their underlying dark mater halos, or
cosmic voids (Sutter et al. 2012; Kreisch et al. 2021).
The typical approach taken to analyse galaxy cluster
data is to “paint” identiﬁed sources onto a grid and per-
form luminosity peak counts in high-density regions as
a tracer for underlying dark matter. Analyses of these
catalogues usually focus on 2-point information, either
in real or Fourier space.
However, these statistics are
only suﬃcient when the underlying ﬁeld is Gaussian,
which is not the case for late-time cosmic web struc-
tures. Finding a statistic with which to capture more
of this information is an active area of research. Exist-
ing methods include the three-point correlation function
(the bispectrum in Fourier space, e.g. Philcox & Ivanov
(2022)), Minkowski functionals (Petri et al. 2013), the
arXiv:2207.05202v3  [astro-ph.CO]  22 Dec 2022

2
1D probability distribution function (Uhlemann et al.
2020), marked power spectra (Massara et al. 2022), min-
imum spanning trees (Barrow et al. 1985; Naidoo et al.
2019, 2022), and ﬁeld-level sampling (Jasche & Wan-
delt 2013; Ramanah et al. 2019; Porqueres et al. 2021;
Leclercq & Heavens 2021; Leclercq 2015; Jasche et al.
2015).
However, truncating analyses to power or bis-
pectra almost certainly discards information, especially
for highly non-Gaussian ﬁelds, while ﬁeld-level methods
quickly become computationally expensive with increas-
ing survey volume. Likewise, void cosmology constructs
correlation functions from void positions and redshifts
(Hamaus et al. 2015) to capture under-dense regions in
structure formation.
This sort of analysis usually dis-
cards morphological features of voids, such as void el-
lipticity, resulting in a loss of information that could be
relevant to the underlying cosmological model (Biswas
et al. 2010; Lavaux & Wandelt 2010; Xu et al. 2019).
Graphs provide a natural way to describe the nonlinear
aspects of large-scale structure (LSS). Dark matter ha-
los and their galaxy clusters can be attributed to nodes
(vertices), while ﬁlaments are traced by smaller halos and
edges connecting neighbouring edges. In this represen-
tation, clustering under gravity can be translated into
higher connectivity or number of edges. Higher order n-
point functions can be computed eﬃciently for clusters,
while avoiding the cost of computing extraneous connec-
tions across voids. Graph representation of LSS promises
a more modular approach to information quantiﬁcation,
and compliments the existing body of literature. Mini-
mum spanning trees (MSTs) have been used in cosmolog-
ical analyses since Barrow et al. (1985), and subsequent
studies have investigated using binned halo graph fea-
tures from simulations as cosmological probes (Bhavsar
& Ling 1988; van de Weygaert et al. 1992; Krzewina
& Saslaw 1996; Ueda & Itoh 1997; Coles et al. 1998;
Adami & Mazure 1999; Colberg 2007; Alpaslan et al.
2014; Beuret et al. 2017; Libeskind et al. 2018; Bonnaire
et al. 2020, 2022). More recently, Naidoo et al. (2022,
2019) use the minimum spanning tree (MST) computed
from the Quijote simulations to compute the cosmologi-
cal information by binning branch and shape features of
the MST computed over the simulation suite. Yang & Yu
(2022) illustrate graph-based approaches for modelling
small-scale halo clustering in cosmological simulations.
The advent of deep learning in cosmology has made
massive data generation and analysis more tractable.
Many studies have investigated neural techniques for
point estimate cosmological parameter extraction from
cosmological ﬁelds via regression networks trained on
simulation-parameter pairs (Pan et al. 2020; Ravan-
bakhsh et al. 2017; Kwon et al. 2020; Prelogovi´c et al.
2021; Fluri et al. 2019, 2018; Matilla et al. 2020; Ribli
et al. 2018; Gillet et al. 2019), ﬁeld reconstruction (Dai
& Seljak 2022; Jamieson et al. 2022), foreground removal
emulation (Makinen et al. 2020; Jeﬀrey et al. 2022),
or cosmological parameters from graphs (Villanueva-
Domingo & Villaescusa-Navarro 2022) with squared loss.
As reviewed in Villaescusa-Navarro et al. (2020a), these
techniques can estimate the posterior mean of parame-
ters (see also Jeﬀrey & Wandelt (2020)). This implies
they require simulations drawn from a prior, speciﬁed at
the time of training, not just near the parameters favored
by the data. This adds to the variability that needs to
be ﬁt by the network.
We take a diﬀerent approach: we consider halo cat-
alogue graphs as our dataset and use Information Max-
imising Neural Networks (IMNNs) to measure the Fisher
information contained in these graphs. IMNNs are neu-
ral networks that compress data to informative nonlin-
ear summaries, trained on simulations around a ﬁducial
model to maximise the Fisher information (Charnock
et al. 2018; Makinen et al. 2021), where, for the purposes
of compression and forecasting only, the summary statis-
tics are assumed to have a Gaussian sampling distribu-
tion. Neural networks can make use of all available data
simultaneously, even saturating known ﬁeld-level likeli-
hoods (Makinen et al. 2021). This approach enables us
to use asymptotically optimal nonlinear statistics (Alsing
& Wandelt 2018; Charnock et al. 2018) to then compute
summaries and estimate maximum likelihood parameters
and perform eﬃcient implicit likelihood inference over a
prior.
We combine this framework with a graph neural net-
work (GNN) architecture. GNNs are well-suited to dis-
crete and variable-length problems such as molecular
classiﬁcation, weather forecasting, and even physics (re)-
discovery with symbolic regression (Lemos et al. 2022;
Cranmer et al. 2020), (see Battaglia et al. (2018) for a
complete review).
Recent studies have made use of IMNNs for cosmology
(Makinen et al. 2021; Fluri et al. 2021; Fluri et al. 2022),
and highly non-Gaussian problems, such as galaxy type
identiﬁcation from multiband images (Livet et al. 2021).
However, previous implementations relied on computing
Fisher statistics for data with a ﬁxed input size. Here,
using GNNs, we extend the framework to a much more
general class of problems. We will refer to graph IMNNs
as gIMNNs.
We show how gIMNN summaries from catalogue
graphs compares to traditional cosmological techniques
with respect to information extraction using the Quijote
halo catalogues (Villaescusa-Navarro et al. 2020b). We
illustrate that by encoding physical symmetries and more
descriptive graph attributes in the IMNN framework, we
can extract more information from limited catalogues
than traditional 2-point statistics.
The study is organised as follows: We present a graph
description of large-scale structure in Section 2, followed
by a review of the IMNN framework in the context of
graph data in Section 3.
In Section 4 we present our
halo catalogue graph and GNN architectures and our
main ﬁndings: We ﬁrst investigate information as a func-
tion of increasing GNN depth and graph connectivity
on both invariant and non-invariant graphs, and show
that gIMNNs consistently extract more information than
the 2-pt function. We next show that decorating graph
nodes with mass further increases information extrac-
tion. Third, we explore the information stored in graph
cardinality (the number of nodes or objects and edges
connecting them) in the context of the halo mass func-
tion. Next, we proceed to a more realistic case in which
catalogue construction is subject to various levels of un-
certainty in the halo mass determination. In Section 7
we conclude by showing how trained gIMNN summaries
can be used as optimal compressors in simulation-based
inference density estimation. We include supplementary
descriptions of graph assembly and network generaliza-

3
tion in Appendix B.
2. LARGE-SCALE STRUCTURE AS A GRAPH
Fig. 1.— Dark matter halo graph representation of large-scale
structure, constructed from a single Quijote simulation.
The
largest halos (grey) with a mass Mi > 1.5 × 1015 M⊙trace the
largest physical scales, here shown coloured by the log of the
halo’s mass, and connected to all neighbours within a radius of
rconnect = 200 Mpc. Smaller halo masses Mi > 1.1 × 1015 M⊙
(light grey) trace smaller scale clustering. The box encases a cos-
mological comoving volume of (1Gpc)3.
Graphs provide a natural language with which to de-
scribe the cosmic web. Dark matter halos are attributed
to nodes (vertices), while ﬁlaments are traced by smaller
halos and edges, illustrated in Figure 1.
In this rep-
resentation, clustering under gravitational interactions
can be translated into higher edge cardinality (number of
edges). Higher order n-point functions can be computed
eﬃciently for clusters, while avoiding the cost of comput-
ing extraneous connections across voids. Void catalogues
(where edges would correspond to the walls separating
the voids) can likewise be assembled into the dual of a
halo graph. Graph construction also allows arbitrary ex-
tra information, such as the halo masses or peculiar ve-
locities, for which the underlying sampling distribution is
not known, to be combined in a nonlinear fashion in the
form of node or edge labels, unlike (marked) correlation
functions.
2.1. Graph Notation
We deﬁne a graph explicitly as a tuple G = (u, V, E),
following the notation in Battaglia et al. (2018).
The
u is a global attribute of the graph, i.e.
a label or
global parameter value.
V
= {vi}i=1:N v is the set
of graph nodes, with cardinality N v.
The edge set
E = {(ek, rk, sk)}rk=i,k=1:Ne, indexed by k = 1 : N e,
is comprised of vectors ek of cardinality N e, which may
be directed, connected via receiving and sending indices
between nodes, rk and sk. Senders and receivers can be
equivalently parameterized by an adjacency matrix Aij
in which i and j index sender and receiver nodes, respec-
tively. Each node, indexed by i = 1 : N v, has a set of
edges, E′
i = {(e′
k, rk, sk)}rk=i,k=1:Ne, connected to it via
a subset of senders and receivers. The full set of nodes is
deﬁned as V = {vi}i=1:N v, where each node vi is a vector
of features. In a physical system of particles, one might
represent V as a set of individual particles’ attributes,
like mass, position, and velocity, with edges expressing
interactions, such as forces, between particles. A global
attribute of a graph might be a classiﬁcation label, such
as in molecule or cluster classiﬁcation (Satorras et al.
2021; Kipf & Welling 2016).
Careful data representa-
tion on graphs can vastly simplify physical problems via
inductive biases and symmetry capture (see e.g. Lemos
et al. 2022; Cranmer et al. 2020; Battaglia et al. 2018).
2.2. Halo Graphs
We
deﬁne
a
dark
matter
halo
graph
G
=
(u, V halo, Ehalo), constructed from a catalogue for a sin-
gle realisation of the universe. We can equivalently deﬁne
its dual, H = (u, V void, Evoid), from a void catalogue.
Note that if we assign global cosmological parameters to
u, G and H share this property. Hereafter we will focus
on graphs from halo catalogues.
The graph framework allows the cardinality of a cos-
mological graph’s nodes and edges to vary as a function
of cosmological or survey parameters, reﬂecting the often
strong dependence of the abundance of clusters on cos-
mological parameters. When assembling a graph from a
halo catalogue, we choose to vary two physical param-
eters: a mass cut, Mcut, and a linking radius, rconnect.
A halo i with a mass above Mcut is connected to a halo
j if the absolute distance between halos i and j is less
than rconnect, i.e. |dij| < rconnect. We display the same
catalogue at two mass cuts in Figure 1. A conservative
Mcut = 1.5×1015M⊙(dark points) contains the heaviest
halos and traces the largest scales, while smaller masses
(Mcut = 1.1×1015M⊙, light points) trace smaller scales.
Each graph is connected by rconnect = 200 Mpc.
Graphs can be assembled from halo catalogues in one of
two ways: as non-invariant or as invariant graphs. Non-
invariant graphs have positions, p, as node labels, setting
vi = pi, with edges labelled as the relative distances
between halos, dij. This graph is not invariant under
translations and rotations, as the node values are pinned
to the underlying simulation grid. Invariant graphs have
only relative positional information, all of which is stored
in the edges. The cosmological models that we wish to
constrain are invariant to rigid Euclidean group rotations
and translations of the large-scale structure. In this work
we include both representations for completeness.
2.2.1. Node features
In the invariant representation, graph nodes are ‘dec-
orated’ with either an indicator vi = vi = 1 in the un-
decorated case or the halo’s scalar mass, vi = vi = Mi.
In the non-invariant case, nodes are also decorated with
position vi = (Mi, pi). We describe graph construction
and padding details in Appendix B.
2.2.2. Edge features
To construct invariant graphs, we impose translational
symmetry by attributing functions of relative positions
between halos on the edges. We compute the vector sep-
arations dij = pi −pj between all halos and do not

4
link halos directly if |dij| > rconnect. For rotational in-
variance, we adopt Villanueva-Domingo & Villaescusa-
Navarro (2022)’s notation and ﬁrst compute the unit vec-
tors sij = dij/|dij| and ni = (pi −¯p)/|pi −¯p| where ¯p is
the centroid (or reference halo position). We then com-
pute the direction cosines aij = ni · nj and bij = ni · sij.
The normalized edge features for invariant halo graphs
are then
eij = [|dij|/rconnect, aij, bij] ,
(1)
whilst for non-invariant graphs, eij = |dij|/rconnect.
2.2.3. Global features
A halo graph’s global features can be any quantity that
describes the global properties of the system, in this case
conﬁguration or cosmology parameters. In a regression
case, one might wish to label each halo graph simula-
tion with a set of cosmological or hydrodynamical pa-
rameters, as done in Villanueva-Domingo & Villaescusa-
Navarro (2022), and ﬁt a neural network to minimise
some distance measure between the network output and
these parameters. Here, global properties will be arbi-
trary nonlinear summaries of cosmology, learned in an
unsupervised manner as a function of the graph’s at-
tributes using information maximising neural networks.
3. INFORMATION MAXIMISING NEURAL NETWORKS
The graph framework allows for a modular study of
the cosmological information embedded in large-scale
structure. We next review IMNNs as a tool for infor-
mation extraction, as well as optimal compression for
graphs assembled from cosmological surveys. The IMNN
framework is presented in full in Charnock et al. (2018)
with developmental updates discussed in Makinen et al.
(2021), but we review the formalism here for complete-
ness and introduce new aspects to the technique. The
sharper the peak of an informative likelihood function
L(d|θ) for some ﬁxed data d with nd data points and
nθ parameters at a given value of θ, the more informa-
tive θ is about the data. The Fisher information matrix
describes how much information d contains about the
parameters, and is given as the second moment of the
score of the likelihood
Fαβ =
Z
dd L(d|θ)∂ln L(d|θ)
∂θα
∂ln L(d|θ)
∂θβ
,
(2)
and can be written as
Fαβ = −
 ∂2 ln L
∂θα∂θβ
 
θ=θfid
,
(3)
evaluated at some ﬁxed ﬁducial parameters.
A large
Fisher information for a set of data indicates that the
data is very informative about the model parameters
attributed to it.
Fisher forecasting for a given model
is made possible by the information inequality and the
Cram´er-Rao bound (Cram´er 1946; Rao 1945), which
states that the minimum variance of the value of an es-
timator θ is given by
⟨(θα −⟨θα⟩)(θβ −⟨θβ⟩)⟩≥F−1
αβ.
(4)
We will write the compression as a function f : d →x.
For large datasets, data compression is essential for infer-
ence to avoid the curse of dimensionality. The MOPED
formalism (Heavens et al. 2000) gives optimal score com-
pression for cases where the likelihood and sampling dis-
tributions are exactly Gaussian.
IMNNs are neural networks that perform data com-
pression and compute the Fisher information of a data
set. Such compression is possible even if the data like-
lihood is unknown or intractable, simply based on hav-
ing simulations of the data at a given ﬁducial parameter
point and local information about how the parameters
change the data distribution. It can be shown (Wandelt,
2022, in preparation) that the optimality of the IMNN
summaries holds for any unknown or intractable data
likelihood even though the IMNN maximizes Fisher in-
formation assuming the parameter-independent covari-
ance form of the Gaussian likelihood for the IMNN sum-
maries
−2 ln L(x|d) = (x −µf(θ))T C−1
f (x −µf(θ))
(5)
where
µf(θ) = 1
ns
ns
X
i=1
xs
i
(6)
is the mean of the compressed summaries xs
i, with
{xs
i|i ∈[1, ns]}’ and we assume a parameter-independent
covariance matrix. Here i indexes the random initiali-
sation of ns simulations, and the superscript s denotes
quantities derived from simulations, unlike quantities
without the superscript s which are derived from actual
observations. The summaries are obtained via simula-
tion of data ds
i = ds
i(θ, i) via the compression scheme
f : ds
i →xs
i. The covariance of the summaries is com-
puted from the data as well:
(Cf)αβ =
1
ns −1
ns
X
i=1
(xs
i −µf)α(xs
i −µf)β.
(7)
Note that this covariance is assumed to be independent
of the parameters, which, whilst not strictly true, is en-
forced by regularisation during the ﬁtting of the IMNN.
A Fisher matrix can then be computed from the likeli-
hood in equation (5):
Fαβ = tr[µT
f,αC−1
f µf,β],
(8)
where we introduce the notation y,α ≡∂y/∂θα for par-
tial derivatives with respect to parameters. If the com-
pression function f is a neural network parameterized
by layer weights wℓand biases bℓ(with ℓthe layer in-
dex), the summaries (and respective mean and covari-
ance) then become functions of these new parameters
x(θ) →x(θ, wℓ, bℓ). To evaluate equation (8) for a neu-
ral compression, we must compute
µf,α =
∂
∂θα
1
ns
ns
X
i=1
xs ﬁd
i
.
(9)
One way of computing the derivatives of the summary
means with respect to the parameters is to deﬁne a ﬁnite
diﬀerence gradient dataset by altering simulation ﬁducial
values by a small amount, yielding
 ∂ˆµi
∂θα
s ﬁd
≈1
ns
ns
X
i=1
xs ﬁd+
i
−xs ﬁd−
i
∆θ+
α −∆θ−
α
.
(10)

5
F!"
edge update
node update
u
globals
C#
𝑙= 1, … 𝑛$
𝑑%
$ '
𝜃( = {𝜃)*+, 𝜃±}
𝝁#,!
density 0ield simulator
graph assembly
GNN
+ noise 
survey cut
interaction network blocks
aggregation
× 𝑁!"#
x$
Fig. 2.— Cartoon of the graph-based information maximising neural network scheme. For each i = 1, . . . ns dark matter ﬁeld realization,
halo catalogues are computed from a density ﬁeld and assembled into a connected graph. A GNN block then computes edge and node
updates (green) outlined in Section 3.1, pooling graph attributes to compute global summaries, x = u.
This process is repeated for
simulations at the ﬁducial parameter values, as well as at θ± for numerical derivative calculation via equation (10). The output of the
IMNN is the Fisher information matrix, computed via equation (8). The network is trained via gradient back-propagation, with the det F
and Cf contributing to the scalar loss function.
To prevent extra information being extracted from ac-
cidental correlation in limited sized data sets, reported
statistics need to be computed on a validation set of sim-
ulations, which is unlikely to share the same accidental
correlations as the ﬁxed training set. An alternative ex-
plored in Makinen et al. (2021) is to calculate the adjoint
gradient of the simulations as well as the derivatives of
the network parameters with respect to the simulations:
µf,α = 1
ns
ns
X
i=1
 ∂x
∂θα
s ﬁd
i
= 1
ns
ns
X
i=1
nd
X
k=1
∂xs ﬁd
i
∂dk
∂ds ﬁd
i
∂θα
.
(11)
If the gradient of the simulations can be computed ef-
ﬁciently, this technique for computing the compression
Fisher information eliminates the need for hyperparam-
eter tuning of the ﬁnite diﬀerence derivative size, ∆θα.
The network is trained to maximise the logarithm of
the determinant of the Fisher information, computed via
equation (8). As described in Charnock et al. and Livet
et al., the Fisher information is invariant to nonsingu-
lar linear transformations of the summaries. To remove
this ambiguity, a term driving covariance to the identity
matrix is added
ΛC = 1
2

||(Cf −1)||2
F +

(C−1
f
−1)


2
F

,
(12)
where ||A||F ≡
√
tr AAT denotes the Frobenius norm.
This yields the loss function
Λ = −ln det F + rΛCΛC,
(13)
with regularization parameter
rΛC =
λΛC
ΛC + exp(−αΛC),
(14)
where λ and α are user-deﬁned parameters. When the
covariance is far from identity, the rΛC function is large
and the optimization focuses on bringing the covariance
and its inverse back to identity. The network is trained
until the Fisher information stops increasing for a pre-
determined number of iterations.
We stress that the
value of F reported as an information metric, however, is
the one computed via Eq. 8, computed over a validation
set of simulations in the case of a ﬁnite set of data.
To summarise the IMNN algorithm we take the fol-
lowing steps every training epoch to optimise the Fisher
information:
i) compress simulations at the ﬁducial model with dif-
ferent random seeds to the network. Calculate the
covariance of these summaries using equation (7).
ii) compress simulations generated at perturbed ﬁdu-
cial parameter values, θ± to produce x±. Calculate
the derivatives xf,α with equation (10).
iii) Calculate the Fisher matrix (Eq. (8)). Pass the
Fisher and covariance matrices to the loss function.
Update neural network weights using gradient de-
scent such that det F increases.
3.1. Graph Neural Networks
A graph neural network (GNN) block typically consists
of three update functions, φ = (φu, φv, φe), and three ag-
gregation functions, ρ = (ρu, ρv, ρe), applied sequentially
to a graph tuple G = (u, V, E). A single graph block ℓis
comprised of several update steps to its elements:
1. Edge update: Each edge is parameterized by a func-
tion φℓ+1
e
which takes as inputs its connected nodes,
previous value, and graph global properties and
yields another edge:
eℓ+1
ij
= φℓ+1
e
(vℓ
i, vℓ
j, eℓ
ijuℓ),
(15)
where vℓ
i and vℓ
j are sender and receiver nodes in-
dexed by (sk, rk).
2. Node update: Each node is then parameterized by
a function φℓ+1
v
and outputs a new node:
vℓ+1
i
= φℓ+1
v
 ρe→v(Eℓ+1
i
), vℓ
i, uℓ
,
(16)
Here a permutation-invariant aggregation opera-
tion ρe→v(Eℓ+1
i
) pools the neighbourhood of edges

6
Eℓ+1
i
connected to node i into a ﬁxed-sized vector
to feed into the update function.
3. Global update: The global features of the graph are
then updated with a function φℓ+1
u
:
uℓ+1 = φu  ρe→v(Eℓ+1), ρv→u(V ℓ+1), uℓ
,
(17)
where the graph’s edge (Eℓ+1) and node (V ℓ+1)
sets are pooled into ﬁxed-sized vectors for the
global update.
The order of operations of these updates is ﬂexible, but
usually applied in the order displayed above, and in the
GNN block in Fig. 2. This framework allows φ functions
to be arbitrarily parameterized as neural networks with
nonlinear activation functions. Aggregation functions ρ
must be allowed to take a variable number of arguments,
so are usually chosen to be permutation-invariant opera-
tors such as the mean, summation, or maximum (Bron-
stein et al. 2021). Stacking ℓ= 1 : N int GNN blocks
allows node information to be propagated to and from
neighbours N int degrees away, where int refers to inter-
actions. In this work all GNN blocks operate over the
entire graph. However, one could also devise surrogate
GNN blocks that operate on small scales and then pass
information up to larger scales via an aggregation func-
tion ρsmall→large, such that one GNN network is not re-
sponsible for operating on nodes of all scales in a densely-
populated graph. We detail our speciﬁc implementation
and architecture in Section 4.1.2.
The GNN framework is readily incorporated into the
IMNN formalism, since the details of the neural network
architecture only serve to better capture how the data
changes with the parameters. Instead of predicting an
output graph or class label, as in Battaglia et al. (2018),
our ﬁnal global update φu outputs IMNN summaries,
x = uℓ=Nint. This new aspect to the IMNN formalism
is the ability to operate over variable-length data inputs,
rendering the cardinality of input graphs, ndata = N v
and N e informative features of the data. A stochastic
system might yield a diﬀerent number of discrete parti-
cles for diﬀerent parameters, meaning the number of data
becomes a descriptor of the statistical model. This allows
for a study of information I = 1
2 ln det F as a function of
N v and enables much more ﬂexible data modelling.
4. COSMOLOGICAL PARAMETER INFERENCE WITH
HALO CATALOGUES
Here we consider applications of our graph IMNNs on
realistic cosmological problems. Even future astronomi-
cal studies will not be able to image complete dark mat-
ter overdensity ﬁelds of large-scale structure. However,
discrete galaxy and void catalogues can be assembled
as tracers of structure. Diﬀerent LSS realizations from
stochastic initial conditions will have diﬀerent numbers
of halos and voids, posing a problem for usual ﬁxed-size
neural networks, (which are themselves problematic for
inference unless treated correctly). We explore informa-
tion extraction as a function of graph connectivity in the
context of two-parameter inference for the matter den-
sity parameter, Ωm, as well as σ8, the r.m.s. ﬂuctuation
of density perturbations at the 8 h−1 Mpc scale. Both
Ωm and σ8 parameterize the distribution of matter in
cosmological simulations, so the graph topology should
be sensitive to changes in parameters.
connected graph
2pt function
+ interaction steps
+ node decoration
𝑁pt function
4ield −level
improved information extraction
Fig. 3.— Information as a function of large-scale structure data
representation. Cosmological parameter information extraction ef-
ﬁciency increases as information is added and propagated through-
out the halo graph with increased connections and message-passing
steps. The second and third graph show message propagation from
the top left node after Nint = 2 update steps.
The more descriptive a graph is, the more information
one intuitively expects to extract. We demonstrate this
trend by ﬁrst considering undecorated graphs, annotated
with just positions of and relative distances between ha-
los. We show that information extraction eﬃciency in-
creases as graph connectivity increases. We then show
that information increases further when halo masses are
included as node features.
The closest existing statistics to this representation
are n-point correlation and mass functions, and we show
how information increases beyond the 2-point correlation
function with the same catalogue as graph connectivity
is increased, as illustrated in the cartoon in Figure 3.
4.1. Halo Catalogues
Here we describe the simulated catalogues that are
used for training and validation.The Quijote Halo cat-
alogues are assembled from 3D overdensity ﬁelds at the
present day (z = 0) using the Friends of Friends (FoF)
algorithm (Davis et al. 1985). Attributes computed by
the ﬁnder are halo masses Mi, positions pi, and velocities
vi. Each full simulation yields a catalogue of ∼400,000
halos on average. Here we restrict our analysis to mass
and clustering information in an eﬀort to compare our
method to known statistics.
4.1.1. Graph inputs assembly
We initially connect graphs of a manageable size by
varying two hyperparameters. We ﬁrst make a minimum
mass cut Mcut to be considered in the catalogue. Nodes
are then connected to one another within a Euclidean
distance rconnect. We initially explore the noise-free limit
with known masses and ﬁx Mcut = 1.5 × 1015M⊙to
assess the pure information limit of the catalogues. We
add noise to the analysis in Section 6. This cut yields
N v ∈[70, 140] halos per catalogue.
We visualize two
graphs in Appendix B, ﬁgure 2.
We then assemble the truncated catalogues into non-
invariant and invariant graphs, as outlined in Section
2.2. We initialize each graph’s global property with a
tuple uℓ=0 = (arcsinh N v, arcsinh N e) summarising the
cardinality of the graphs.
As described in Battaglia
et al. (2018); Lemos et al. (2022); Villanueva-Domingo &
Villaescusa-Navarro (2022), imposing symmetries in data
representation can improve GNN training, since the net-
work can focus on learning relevant correlations to the
problem, as opposed to re-learning symmetry. We test
this notion in the context of information extraction.

7
4.1.2. Graph neural network architecture
We choose to parameterize our GNN functions with
simple fully-connected networks.
Each φ function is
a dense network with two layers of 50 hidden neurons
and gelu activations (Hendrycks & Gimpel 2016). We
built a custom aggregation function akin to that found
in Villanueva-Domingo & Villaescusa-Navarro (2022), in
which mean, max, sum, and variance are computed over
node and edge attributes and then concatenated, since
it is not known a priori which function is most useful
for information extraction. To aggregate e.g. the set of
edges Ei in a neighbourhood around node i we compute:
M
j∈Ei
eij =

max
j∈Ei eij,
X
j∈Ei
eij,
P
j∈Ei eij
P
j∈Ei


(18)
We additionally modify these operators with a trainable
arcsinh layer e.g. for edge-to-node aggregation:
ρℓ+1
e→v(Eℓ
i ) = a arcsinh

b
M
j∈Eℓ
i
eℓ
ij + c

+ d,
(19)
where (a, b, c, d) are scalar learnable parameters initial-
ized as (1, 1, 0, 0) to ensure numerical stability for gradi-
ent calculation. All networks are trained with an Adam
optimizer with a learning rate set to 0.0001 and coupling
parameters λ = 10 and α = 0.95.
We construct our
graphs and GNNs using the jraph (Godwin et al. 2020)
and Flax (Heek et al. 2020) libraries, which are both
Jax-compatible.
We train our gIMNNs by splitting the Quijote sim-
ulations into equally-sized training and validation sets.
Gradient descent is performed on training data, while
reported compression statistics (Fisher information) are
computed for the validation set using equation (8). Both
training and validation sets comprise of ns = 500 ﬁdu-
cial simulations at θﬁd = (Ωm, σ8) = (0.3175, 0.834) and
nd = 250 seed-matched derivative simulations perturbed
by δθ = (0.01, 0.015), yielding nd × 2 × 2 = 1000 simula-
tions (see Villaescusa-Navarro et al. (2020b) for details).
Training on the loss deﬁned in Eq 13 is performed until a
patience criterion is met, in this case, when the training
Fisher information stops increasing signiﬁcantly for 1000
epochs.
4.2. Undecorated Graphs vs. n-point Statistics
We ﬁrst consider an undecorated graph representa-
tion of halo catalogues without descriptive node features.
Drawing more edges between nodes increases the con-
nectivity of the graph, allowing information from a sin-
gle node to reach more distant neighbours. Undecorated
graphs of increasing connectivity are analogous to tradi-
tional n-point statistics computed for galaxy catalogues.
3-point statistics for example consider triangular group-
ings of galaxies, and generally oﬀer tighter constraints
from large-scale structure data than the 2PCF, as shown
in (Hahn et al. 2020). We additionally explore invariant
and non-invariant graph structures, outlined in Section
2.2.
4.2.1. Comparison to 2-point correlation information
As a benchmark for our analysis, we also compare the
information content obtained from the 2-point correla-
tion function (2PCF), ξ(r), of our small halo catalogues,
the real-space equivalent to the Quijote power spectrum
computed in Villaescusa-Navarro et al. (2020b). For a
statistic Q = ξ(r), the Fisher information is given by
(Tegmark et al. 1997):
Fij = 1
2tr
(
C−1
" 
∂Q
∂θi
∂Q
∂θj
T !
+
 
∂Q
∂θi
T ∂Q
∂θj
!#)
,
(20)
where C is estimated from simulations at the ﬁducial and
the derivatives are approximated numerically via
∂Q
∂θi
≈Q(θ+
i ) −Q(θ−
i )
θ+
i −θ−
i
(21)
We use the full suite of 500 derivative and 1000 ﬁdu-
cial halo catalogue simulations to compute Eq 20, and
crucially make the same conservative mass cut. We bin
distances into 10 ﬁxed bins between 0 and
3√
3 Gpc3,
yielding a covariance matrix of size 1002. For the 2PCF
we obtain a Fisher information of 2.28×106, or Shannon
entropy of 7.319 nats.
4.2.2. Increasing graph connectivity
Here we construct graphs of varying edge cardinality
by varying a physical connection parameter, rconnect ∈
{100, 200, 300} Mpc, yielding graphs with average edge
number ⟨N e⟩∈{27, 150, 437} respectively. We also com-
pare information as a function of increasing GNN inter-
action blocks, N int, for all rconnect, for both non-invariant
and invariantly-structured halo graphs. Network archi-
tecture is identical for each rconnect value, and initialized
by the same random seed.
Results. We display information extraction as a func-
tion of graph connectivity in Figure 4, computed for the
validation simulation set. We also display the catalogue’s
2PCF information (dashed line) as a benchmark.
In-
variant graphs (top row) train more smoothly than non-
invariant graphs (bottom row) since the network does
not have to learn relationships from position values on
the nodes. A single GNN block struggles to extract infor-
mation with rconnect = 100 Mpc in the non-invariant rep-
resentation (lower left), but plateaus at ≈2.8×107 for all
other conﬁgurations. This behavior is likely because in
most cases the network is both descriptive enough and is
able to capture patterns at much larger scales by attend-
ing to halos higher degrees away. The common saturation
value across multiple network and connectivity combina-
tions indicates that undecorated graphs typically contain
det FIMNN/ det F2PCF ≈10 −15 times more information
than the 2PCF, regardless of connectedness, provided a
descriptive enough network can extract it.
The graph
representation improves marginal constraints in Ωm by a
factor of ≈2 and in σ8 by a factor of ≈11, displayed in
Figure 5.
We initially hypothesized that increasing network com-
plexity would increase the information extraction. How-
ever, we found that we obtain essentially equivalent in-
formation for any combination of rconnect and N int. It
is clear that for this small number of halos the informa-

8
Fig. 4.— Information extraction comparison for undecorated invariant (top row) and non-invariant (bottom row) halo graphs over
validation simulations. Columns indicate how many graph network update steps are performed, and colours indicate graphs assembled
with diﬀerent physical connection radii. The information obtained from the 2PCF computed from the same catalogues with the same mass
cut is shown as a dashed line. For suﬃciently descriptive networks (Nint > 1), invariantly-assembled graphs train faster, but all graphs
connected for r > 100 Mpc saturate at det F ≈2.8 × 107, or ≈15 times higher than the 2PCF. Jagged dips in validation curves indicate
points of restarting network training after patience criteria were met.
tion is easily saturated at any level, but with more halos
in the catalogue, as discussed in Section 8, hierarchical
clustering at the graph or network level might pull out
more information from e.g. smaller mass scales. This
exploration is reserved for a future work.
Since all suﬃciently-connected representations obtain
the same information, we proceed in our experiments
with invariant graphs connected with rconnect = 200 Mpc
and N int = 2, since this combination resulted in the
smoothest and fastest training (4 minutes) on a single
NVIDIA-v100 GPU.
4.3. Decorated Graphs: Incorporating Halo Mass
We next decorate each halo node with the correspond-
ing (noise-free) halo mass. We widen the network’s hid-
den dimension to 64 and train both decorated and undec-
orated graphs with the same patience settings. Training
was restarted for each three times after plateau to ensure
saturation. The same network architecture is able to ex-
tract 2.3 times more information when decorated with
masses, corresponding to 42 times more information than
the 2PCF. We display the corresponding Fisher ellipses
in Figure 5, along with isomass lines of the halo mass
function, described in Section 5.1.
We also decorated
graphs with peculiar velocities with slight improvement
in information extraction but restricted our analysis to
mass and clustering for interpretability.
5. MASS CUT INFORMATION
We next investigate how much information is contained
in the mass cut, Mcut which determines halo number
N v. We compare two graph assemblies: one with a ﬁxed
number of the most massive halos N v
ﬁxed = 105, i.e. the
average number of halos across variable sized halo cata-
logues with ﬁxed mass cut Mcut = 1.5×1015M⊙, and an-
other where the cardinality is allowed to vary with Mcut.
For each case we compare decorated and undecorated
graphs, and the network (epistemic) and data sampling
(aleatoric) errors associated with each representation. To
catalogue Nv
graph assembly
ln det F
vary network
vary data
without mass
5.03 ± 0.47
5.98 ± 1.06
ﬁxed
with mass
12.43 ± 1.44
12.39 ± 0.22
2PCF
9.74
without mass
17.89 ± 0.33
17.66 ± 0.27
variable
with mass
17.40 ± 0.57
17.85 ± 0.12
2PCF
14.19
TABLE 1
Comparison of extracted information from ﬁxed- and variable-
length catalogues.
Information was extracted with rconnect =
200 Mpc, and networks with Nint = 2 across 5 identical initial-
isations. We display each conﬁguration’s best Fisher information
and the means and standard deviations over 5 network initializa-
tions (second-to-last column) and 5 diﬀerent train/validation set
splits (last column).
estimate network variability we train ﬁve gIMNNs with
N int = 2 with diﬀerent initialization of network param-
eters, whilst ﬁxing the training and validation sets, dis-
playing the best Fisher obtained as well as the mean and
standard deviation of ln det F over the ﬁve runs. For data
sampling uncertainty we ﬁx the gIMNN weight values on
initialization and train on ﬁve diﬀerent randomised train-
validation equal-sized splits of the available simulations.
For both data and network error cases, the same ﬁve
random seeds and network architecture is used across all

9
data conﬁgurations.
Results. We display results in Table 1. Fixing cata-
logue size eliminates halo number as a useful feature to
the network, evidenced by much lower information yields.
Without mass decoration there is less information in the
graph data so the data can be ﬁt by more possible func-
tions by the network, so the variability of the Fisher as a
function of the data sampling is increased over the dec-
orated case. However, the network has to ﬁt a simpler
compression since there are fewer relevant features with-
out mass, so the variability in possible network weights
decreases, compared to the decorated case. Fixed-length
graphs do not exceed the 2PCF information until anno-
tated with mass information.
When catalogues are allowed to vary with a physical
mass cut, much more information can be extracted from
both decorated and undecorated graphs. Including mass
information on the nodes again increases the variability
incurred across diﬀerent network initializations, but de-
creases the aleatoric uncertainty since we better describe
the likelihood with more information.
5.1. Comparison to the Halo Mass Function
The results of Section 5 indicate that catalogue in-
formation extraction is extremely sensitive to a physical
mass cut. This behaviour is akin to constraints obtained
using halo mass cumulative distribution functions (Reed
et al. 2006; Uhlemann et al. 2020; Artis et al. 2021).
The halo number density function is dn/dM, deﬁned
as the number of halos of mass M per unit volume per
unit interval in M, equivalently parameterized using the
smoothed r.m.s. linear overdensity of the density ﬁeld,
σ(M), via the halo mass function (HMF), f(σ). The frac-
tion of mass in collapsed halos per unit interval ln σ−1
obeys
Z ∞
−∞
f(σ)d ln σ−1 = 1,
(22)
and is related to the halo number density function via
dn
dM = ρo
M
d ln σ−1
dM
f(σ),
(23)
where ρo is the mean mass density of the universe. The
form of f(σ; θ) can be related analytically to cosmologi-
cal parameters, such as in Press & Schechter (1974), or
approximated using simulations (Reed et al. 2006).
We
compare
gIMNN
Fisher
constraints
to
iso-
mass contours of the integrated Press-Schechter HMF,
fPS(σ; Ωm, σ8), integrated from a ﬁxed Mcut as a func-
tion of cosmological parameters in Figure 5. We use the
HMF since this quantity incorporates both halo number
and mass information. See Appendix A for a detailed
comparison of dn/dM and f(σ) functions.
We utilize
hmf calc (Murray et al. 2013; Murray 2014) for the cal-
culation. The HMF and the corresponding halo number
density at ﬁxed M = Mcut determines a
relatively narrow locus in the (Ωm, σ8) plane. The clus-
tering information, traced by the 2PCF Fisher, (which is
accessible by the network) serves to lift this degeneracy.
The network Fishers are nearly parallel to the HMF iso-
mass contours, but are not degenerate in the direction
of the 2PCF Fisher’s major axes. Decorating nodes with
masses also induces a slight rotation towards the isomass
contours, since the network has more detailed mass in-
formation to work with. This result indicates that the
Fig. 5.— Fisher matrix comparison for decorated (black) and
undecorated (green) graphs and 2PCF (dashed) computed for the
same catalogue, plotted over contours of the Press-Schechter halo
mass function (grey), where the numbers indicate the integrated
mass density of halos from Mcut to inﬁnity, as a function of Ωm
and σ8. The HMF contours indicate the information provided by a
ﬁxed physical mass cut, orthogonal to the positional information in
the 2PFC. The IMNN Fisher ellipses follow these contours, but are
not degenerate in the direction of the 2PCF, indicating that the
IMNN has learned to use both mass and clustering information.
Graphs annotated with halo mass tighten constraints by a factor
of 2.3 over undecorated graphs, and by a factor of 42 over 2PCF
information.
network has automatically learned to extract information
from both halo clustering and mass information.
6. WORKING WITH NOISY CATALOGUES
We next add observational noise and catalogue cuts
on-the-ﬂy during gIMNN training to mimic survey as-
sembly with imperfect observations. Before a graph is
constructed from a halo catalogue and fed to the network
in training, halo masses are subjected to white noise with
ﬁxed variance,
ˆmi = mi + N(0, σ2
noise),
(24)
where σnoise = AnoiseMcut. Observed halos that fall be-
low Mcut are then trimmed from the graph to mimic
real catalogue cuts in the presence of noisy mass esti-
mates. This noise model reﬂects uncertainty in the halo
ﬁnder or galaxy catalogue builder. Smaller masses close
to Mcut are more likely to be cut due to mass underes-
timation, similar to low-brightness clusters in sky sur-
veys. We choose Mcut = 1.5 × 1015M⊙and train identi-
cal networks with diﬀerent amplitudes of on-the-ﬂy noise;
Anoise ∈{0.05, 0.1, 0.2}.
Results.
We display validation curves over training
epoch in Figure 6. Increasing catalogue noise results in
higher variance per epoch in the computed Fisher statis-
tics, as well as a slightly lower information plateau. This
can be interpreted as higher noise obscuring small mass
scales in the information extraction. This eﬀect is illus-
trated via inﬂated Fisher constraints in Figure 7.

10
Fig. 6.— Validation curves for noisy masses. Smaller noise vari-
ance (darker curves) results in smaller per-epoch variance in det F
and slightly more information extraction. Information leakage oc-
curs with higher noise variance since smaller scales are poorly re-
solved and trimmed from the catalogue.
As the noise level increases the low-end masses have
more variance when drawn on-the-ﬂy so more halos are
projected out of the catalogue because they fall below
Mcut, and so this information cannot be encapsulated
in the compressed summaries. Increasing the noise am-
plitude to 20% of Mcut inﬂates constraints in Ωm.
In
the high-noise limit, halo positions dominate det F, in-
dicated by the relatively unchanged, position-dependent
σ8 constraints. As noise decreases and masses are bet-
ter known, the Fisher exhibits the same rotation seen in
Section 5.1 along the isomass HMF lines. This eﬀect is
discussed in detail in Appendix A.
Despite inﬂating constraints, showing the network
large numbers of on-the-ﬂy noise realizations during
training can harden the network to the negative eﬀects
of limited training data and therefore provide smoother
training whilst still bein g able to extract information
at a similar level to noise-free catalogues. However, the
model of these noisy masses must be accurate to the noise
model expected for the real data otherwise the on-the-ﬂy
simulations do not provide hardening of the summaries
in the correct way and may even project out informative
data correlations.
7. APPLICATION TO IMPLICIT LIKELIHOOD INFERENCE
The IMNN framework is both an information quan-
tiﬁcation scheme as well as an asymptotically optimal
compression mechanism for implicit likelihood inference.
The global network summaries used to compute Fisher
statistics can also be used as proxies for the cosmologi-
cal parameters via a score estimate (Alsing & Wandelt
2018; Charnock et al. 2018) using the IMNN Fisher and
covariance:
ˆθα = F−1
αβ
∂µi
∂θβ
C−1
ij (xj(w, d) −µj).)
(25)
These summaries are not explicit predictions for cosmo-
logical parameters, although they are pseudo-maximum
likelihood estimates for the parameters in the region
asymptotically close to the ﬁducial cosmological param-
eter values. Instead, we suggest using these values as in-
formative summaries in Approximate Bayesian Compu-
tation (ABC) or density estimation schemes, as demon-
strated in Makinen et al. (2021) and Charnock et al.
(2018).
Figure 8 shows a spread of these summaries
Fig. 7.— Fisher constraints for diﬀerent noise models, plotted
over lines of the Press-Schechter halo mass function integrated from
Mcut (grey), where the numbers indicate the integrated mass den-
sity of halos from Mcut to inﬁnity, as a function of Ωm and σ8.
Higher σnoise (lighter curves) cause low-mass halos to drop out,
inﬂating constraints in Ωm, but constraints in σ8 remain relatively
unchanged since this parameter is largely position-dependent.
over 200 test (non-seed matched with cosmic variance)
θ± Quijote datasets. The network is able to distinguish
halo graphs simulated at diﬀerent parameter values (on
the level of the parameter degeneracy), rendering these
gIMNN summaries usable in accept-reject simulation-
based inference (SBI) schemes. We additionally discuss
network generalization to other data in Appendix B.
8. DISCUSSION & CONCLUSION
In this study, we explored cosmological information ex-
traction from halo catalogues assembled as graphs. We
ﬁrst introduced graphs as a general language for describ-
ing large-scale structure formation.
We showed that
nonlinear summaries from suﬃciently expressive graph
neural networks far exceed the information contained in
traditional 2-point statistics, using the ≈100 most mas-
sive halos. We illustrated that decorating graphs with
mass information increases the information yield, as well
as decreases training-validation set sampling variance
(aleatoric error). Finally, we showed that summaries pro-
duced by the network are readily usable for simulation-
based inference.
We also explored cosmological information as a func-
tion of graph construction. We showed that a signiﬁcant
amount of information is contained in the variable car-
dinality of the graph, N v, i.e. the number of halos in
a catalogue, and related this feature to the halo mass
function formalism. This test demonstrated a distinct
advantage in graph representation: allowing data vector
size to vary with cosmology, combining both positional
and mass information automatically into just two statis-
tics.
Next we demonstrated that gIMNN training can be
made robust to noise with little information loss in a

11
Fig. 8.— gIMNN score summaries for test derivative datasets
(θ± with cosmic variance. A trained network can distinguish be-
tween simulations generated at diﬀerent parameter values, indi-
cated by the diﬀerent distributions in summary space. These score
summaries are not explicit predictions for cosmological parame-
ters; they should be considered as informative summaries of the
cosmological parameters that can be used for ABC or density esti-
mation for building cosmological posteriors. In general and on av-
erage, simulations generated at lower Ωm and σ8 values have lower
pseudo-maximum likelihood estimated values for these parameters
and vice versa, however, they should not be taken as predictions
of the cosmological parameters.
more realistic setting where halo masses are estimates
with measurement error.
We also explored network (epistemic) and data sam-
pling (aleatoric) error in graph representation.
We
showed that a combined compression of masses and po-
sitional information decreased data variability, meaning
training and validation graphs become more descriptive
of their underlying likelihood with decoration, even in a
ﬁxed-length scenario.
The results of this work hold several implications for
cosmological parameter estimation and study of large-
scale structure.
The graph framework presented here
enables further modular study of nonlinear statistics
that combine attributes of mass functions and correla-
tion functions, at a fraction of the computational cost
of bispectrum or trispectrum calculation. Cosmological
parameter constraints from void catalogues, here the du-
als to halo graphs, might elucidate complementary con-
straints to those obtained here (Kreisch et al. 2021).
Coulton et al. (2022) and Jung et al. (2022) recently in-
vestigated bispectrum statistics from n-body primordial
nongaussianity (pNG) simulations in the Quijote suite to
investigate propagation of pNG to late-time cosmologi-
cal structure.
A complementary study using gIMNNs
might pick up signatures not isolated in the bispectrum
framework. The gIMNN formalism could also be used
to summarise and understand better the halo merger
and clustering statistics as a function of scale or graph
feature, such as clique number.
Hierarchical aggrega-
tion schemes, as described in Section 3.1, could make
the gIMNN scheme tractable for aggregating catalogued
galaxies into subhalo graphs, and further into a global
graph over the largest scales. Doing so would scale this
analysis to full-sized galaxy catalogues.
Follow-up study is warranted on much larger cata-
logues and with more parameters, with a view to ready-
ing this framework for applications to inference from
detailed physical simulations and, ultimately, realistic
galaxy surveys.
9. CODE AVAILABILITY
The code used for this analysis is available at https://
github.com/tlmakinen/cosmicGraphs. Full documen-
tation for the IMNN software is available at https://
www.aquila-consortium.org/doc/imnn/index.html.
T.L.M acknowledges the Imperial College London
President’s Scholarship fund for support of this study,
as well as the great discussions with Stephon Alexan-
der, David Spergel, and Doug Finkbeiner that inspired
this work. B.D.W. acknowledges support by the ANR
BIG4 project, grant ANR-16-CE23-0002 of the French
Agence Nationale de la Recherche; and the Labex ILP
(reference ANR-10-LABX-63) part of the Idex SUPER,
and received ﬁnancial state aid managed by the Agence
Nationale de la Recherche, as part of the programme
Investissements d’avenir under the reference ANR-11-
IDEX-0004-02. This work was done within the Aquila
Consortium and the Learning the Universe Collabora-
tion. The Flatiron Institute is supported by the Simons
Foundation.
REFERENCES
Adami, C., & Mazure, A. 1999, Astron. Astrophys. Suppl. Ser.,
134, 393, doi: 10.1051/aas:1999145
Alpaslan, M., Robotham, A. S. G., Driver, S., et al. 2014,
MNRAS, 438, 177, doi: 10.1093/mnras/stt2136
Alsing, J., & Wandelt, B. 2018, MNRAS, 476, L60,
doi: 10.1093/mnrasl/sly029
Artis, E., Melin, J.-B., Bartlett, J. G., & Murray, C. 2021,
Astronomy & Astrophysics, 649, A47,
doi: 10.1051/0004-6361/202140293
Barrow, J. D., Bhavsar, S. P., & Sonoda, D. H. 1985, Monthly
Notices of the Royal Astronomical Society, 216, 17,
doi: 10.1093/mnras/216.1.17
Battaglia, P. W., Hamrick, J. B., Bapst, V., et al. 2018,
Relational inductive biases, deep learning, and graph networks.
https://arxiv.org/abs/1806.01261
Beuret, M., Billot, N., Cambr´esy, L., et al. 2017, A&A, 597,
A114, doi: 10.1051/0004-6361/201629199
Bhavsar, S. P., & Ling, E. N. 1988, PASP, 100, 1314,
doi: 10.1086/132325
Biswas, R., Alizadeh, E., & Wandelt, B. D. 2010, Phys. Rev. D,
82, 023002, doi: 10.1103/PhysRevD.82.023002
Bonnaire, T., Aghanim, N., Decelle, A., & Douspis, M. 2020,
Astronomy & Astrophysics, 637, A18,
doi: 10.1051/0004-6361/201936859
Bonnaire, T., Aghanim, N., Kuruvilla, J., & Decelle, A. 2022,
Astronomy & Astrophysics, 661, A146,
doi: 10.1051/0004-6361/202142852
Bronstein, M. M., Bruna, J., Cohen, T., & Veliˇckovi´c, P. 2021,
Geometric Deep Learning: Grids, Groups, Graphs, Geodesics,
and Gauges, arXiv, doi: 10.48550/ARXIV.2104.13478
Charnock, T., Lavaux, G., & Wandelt, B. D. 2018, Physical
Review D, 97, doi: 10.1103/physrevd.97.083004
Colberg, J. M. 2007, Monthly Notices of the Royal Astronomical
Society, 375, 337, doi: 10.1111/j.1365-2966.2006.11312.x

12
Coles, P., Pearson, R. C., Borgani, S., Plionis, M., & Moscardini,
L. 1998, MNRAS, 294, 245,
doi: 10.1046/j.1365-8711.1998.01147.x
Coulton, W. R., Villaescusa-Navarro, F., Jamieson, D., et al.
2022, Quijote-PNG: Simulations of primordial non-Gaussianity
and the information content of the matter ﬁeld power spectrum
and bispectrum, arXiv, doi: 10.48550/ARXIV.2206.01619
Cram´er, H. 1946, Mathematical methods of statistics, by Harald
Cramer, .. (The University Press)
Cranmer, M., Sanchez-Gonzalez, A., Battaglia, P., et al. 2020,
Discovering Symbolic Models from Deep Learning with
Inductive Biases. https://arxiv.org/abs/2006.11287
Dai, B., & Seljak, U. 2022, arXiv e-prints, arXiv:2202.05282.
https://arxiv.org/abs/2202.05282
Davis, M., Efstathiou, G., Frenk, C. S., & White, S. D. M. 1985,
ApJ, 292, 371, doi: 10.1086/163168
Fluri, J., Kacprzak, T., Lucchi, A., et al. 2019, Physical Review
D, 100, doi: 10.1103/physrevd.100.063514
Fluri, J., Kacprzak, T., Lucchi, A., et al. 2022, arXiv e-prints,
arXiv:2201.07771. https://arxiv.org/abs/2201.07771
Fluri, J., Kacprzak, T., Refregier, A., et al. 2018, Physical Review
D, 98, doi: 10.1103/physrevd.98.123518
Fluri, J., Kacprzak, T., Refregier, A., Lucchi, A., & Hofmann, T.
2021, Physical Review D, 104,
doi: 10.1103/physrevd.104.123526
Gillet, N., Mesinger, A., Greig, B., Liu, A., & Ucci, G. 2019,
Monthly Notices of the Royal Astronomical Society,
doi: 10.1093/mnras/stz010
Godwin, J., Keck, T., Battaglia, P., et al. 2020, Jraph: A library
for graph neural networks in jax., 0.0.1.dev.
http://github.com/deepmind/jraph
Hahn, C., Villaescusa-Navarro, F., Castorina, E., & Scoccimarro,
R. 2020, Journal of Cosmology and Astroparticle Physics, 2020,
040, doi: 10.1088/1475-7516/2020/03/040
Hamaus, N., Sutter, P., Lavaux, G., & Wandelt, B. D. 2015,
Journal of Cosmology and Astroparticle Physics, 2015,
036–036, doi: 10.1088/1475-7516/2015/11/036
Heavens, A. F., Jimenez, R., & Lahav, O. 2000, Monthly Notices
of the Royal Astronomical Society, 317, 965–972,
doi: 10.1046/j.1365-8711.2000.03692.x
Heek, J., Levskaya, A., Oliver, A., et al. 2020, Flax: A neural
network library and ecosystem for JAX, 0.5.2.
http://github.com/google/flax
Hendrycks, D., & Gimpel, K. 2016, Gaussian Error Linear Units
(GELUs), arXiv, doi: 10.48550/ARXIV.1606.08415
Jamieson, D., Li, Y., de Oliveira, R. A., et al. 2022, Field Level
Neural Network Emulator for Cosmological N-body
Simulations, arXiv, doi: 10.48550/ARXIV.2206.04594
Jasche, J., Leclercq, F., & Wandelt, B. 2015, Journal of
Cosmology and Astroparticle Physics, 2015, 036,
doi: 10.1088/1475-7516/2015/01/036
Jasche, J., & Wandelt, B. D. 2013, MNRAS, 432, 894,
doi: 10.1093/mnras/stt449
Jeﬀrey, N., Alsing, J., & Lanusse, F. 2020, Monthly Notices of the
Royal Astronomical Society, 501, 954,
doi: 10.1093/mnras/staa3594
Jeﬀrey, N., Boulanger, F., Wandelt, B. D., et al. 2022, MNRAS,
510, L1, doi: 10.1093/mnrasl/slab120
Jeﬀrey, N., & Wandelt, B. D. 2020, Solving high-dimensional
parameter inference: marginal posterior densities & Moment
Networks. https://arxiv.org/abs/2011.05991
Jung, G., Karagiannis, D., Liguori, M., et al. 2022, Quijote-PNG:
Quasi-maximum likelihood estimation of Primordial
Non-Gaussianity in the non-linear dark matter density ﬁeld,
arXiv, doi: 10.48550/ARXIV.2206.01624
Kipf, T. N., & Welling, M. 2016, Semi-Supervised Classiﬁcation
with Graph Convolutional Networks, arXiv,
doi: 10.48550/ARXIV.1609.02907
Kreisch, C. D., Pisani, A., Villaescusa-Navarro, F., et al. 2021,
The GIGANTES dataset: precision cosmology from voids in the
machine learning era, arXiv, doi: 10.48550/ARXIV.2107.02304
Krzewina, L. G., & Saslaw, W. C. 1996, MNRAS, 278, 869,
doi: 10.1093/mnras/278.3.869
Kwon, Y., Hong, S. E., & Park, I. 2020, Journal of the Korean
Physical Society, 77, 49–59, doi: 10.3938/jkps.77.49
Lavaux, G., & Wandelt, B. D. 2010, MNRAS, 403, 1392,
doi: 10.1111/j.1365-2966.2010.16197.x
Leclercq, F. 2015, Bayesian large-scale structure inference and
cosmic web analysis, arXiv, doi: 10.48550/ARXIV.1512.04985
Leclercq, F., & Heavens, A. 2021, On the accuracy and precision
of correlation functions and ﬁeld-level inference in cosmology.
https://arxiv.org/abs/2103.04158
Lemos, P., Jeﬀrey, N., Cranmer, M., Ho, S., & Battaglia, P. 2022,
Rediscovering orbital mechanics with machine learning.
https://arxiv.org/abs/2202.02306
Libeskind, N. I., van de Weygaert, R., Cautun, M., et al. 2018,
MNRAS, 473, 1195, doi: 10.1093/mnras/stx1976
Livet, F., Charnock, T., Borgne, D. L., & de Lapparent, V. 2021,
Catalog-free modeling of galaxy types in deep images: Massive
dimensional reduction with neural networks.
https://arxiv.org/abs/2102.01086
Makinen, T. L., Charnock, T., Alsing, J., & Wandelt, B. D. 2021,
Journal of Cosmology and Astroparticle Physics, 2021, 049,
doi: 10.1088/1475-7516/2021/11/049
Makinen, T. L., Lancaster, L., Villaescusa-Navarro, F., et al.
2020, deep21: a Deep Learning Method for 21cm Foreground
Removal. https://arxiv.org/abs/2010.15843
Massara, E., Villaescusa-Navarro, F., Hahn, C., et al. 2022,
Cosmological Information in the Marked Power Spectrum of
the Galaxy Field, arXiv, doi: 10.48550/ARXIV.2206.01709
Matilla, J. M. Z., Sharma, M., Hsu, D., & Haiman, Z. 2020,
Physical Review D, 102, doi: 10.1103/physrevd.102.123506
Murray, S. 2014, HMF: Halo Mass Function calculator,
Astrophysics Source Code Library, record ascl:1412.006.
http://ascl.net/1412.006
Murray, S. G., Power, C., & Robotham, A. S. G. 2013, Astronomy
and Computing, 3, 23, doi: 10.1016/j.ascom.2013.11.001
Naidoo, K., Massara, E., & Lahav, O. 2022, Monthly Notices of
the Royal Astronomical Society, 513, 3596,
doi: 10.1093/mnras/stac1138
Naidoo, K., Whiteway, L., Massara, E., et al. 2019, Monthly
Notices of the Royal Astronomical Society, 491, 1709,
doi: 10.1093/mnras/stz3075
Pan, S., Liu, M., Forero-Romero, J., et al. 2020, Cosmological
parameter estimation from large-scale structure deep learning.
https://arxiv.org/abs/1908.10590
Petri, A., Haiman, Z., Hui, L., May, M., & Kratochvil, J. M. 2013,
Phys. Rev. D, 88, 123002, doi: 10.1103/PhysRevD.88.123002
Philcox, O. H., & Ivanov, M. M. 2022, Physical Review D, 105,
doi: 10.1103/physrevd.105.043517
Porqueres, N., Heavens, A., Mortlock, D., & Lavaux, G. 2021,
Monthly Notices of the Royal Astronomical Society, 509,
3194–3202, doi: 10.1093/mnras/stab3234
Prelogovi´c, D., Mesinger, A., Murray, S., Fiameni, G., & Gillet,
N. 2021, Machine learning galaxy properties from 21 cm
lightcones: impact of network architectures and signal
contamination. https://arxiv.org/abs/2107.00018
Press, W. H., & Schechter, P. 1974, ApJ, 187, 425,
doi: 10.1086/152650
Ramanah, D. K., Lavaux, G., Jasche, J., & Wandelt, B. D. 2019,
A&A, 621, A69, doi: 10.1051/0004-6361/201834117
Rao, C. R. 1945, Bulletin of the Calcutta Mathematical Society,
37, 81–89
Ravanbakhsh, S., Oliva, J., Fromenteau, S., et al. 2017,
Estimating Cosmological Parameters from the Dark Matter
Distribution. https://arxiv.org/abs/1711.02033
Reed, D. S., Bower, R., Frenk, C. S., Jenkins, A., & Theuns, T.
2006, Monthly Notices of the Royal Astronomical Society, 374,
2, doi: 10.1111/j.1365-2966.2006.11204.x
Ribli, D., ´Armin Pataki, B., & Csabai, I. 2018, An improved
cosmological parameter inference scheme motivated by deep
learning. https://arxiv.org/abs/1806.05995
Satorras, V. G., Hoogeboom, E., Fuchs, F. B., Posner, I., &
Welling, M. 2021, E(n) Equivariant Normalizing Flows, arXiv,
doi: 10.48550/ARXIV.2105.09016
Sutter, P. M., Lavaux, G., Wandelt, B. D., & Weinberg, D. H.
2012, ApJ, 761, 44, doi: 10.1088/0004-637X/761/1/44
Tegmark, M., Taylor, A. N., & Heavens, A. F. 1997, The
Astrophysical Journal, 480, 22–35, doi: 10.1086/303939
Ueda, H., & Itoh, M. 1997, Publications of the Astronomical
Society of Japan, 49, 131, doi: 10.1093/pasj/49.2.131
Uhlemann, C., Friedrich, O., Villaescusa-Navarro, F., Banerjee,
A., & Codis, S. 2020, MNRAS, 495, 4006,
doi: 10.1093/mnras/staa1155

Uhlemann, C., Friedrich, O., Villaescusa-Navarro, F., Banerjee,
A., & Codis, S. 2020, Monthly Notices of the Royal
Astronomical Society, 495, 4006, doi: 10.1093/mnras/staa1155
van de Weygaert, R., Jones, B. J., & Mart´ınez, V. J. 1992,
Physics Letters A, 169, 145,
doi: https://doi.org/10.1016/0375-9601(92)90584-9
Villaescusa-Navarro, F., Wandelt, B. D., Angl´es-Alc´azar, D., et al.
2020a, Neural networks as optimal estimators to marginalize
over baryonic eﬀects. https://arxiv.org/abs/2011.05992
Villaescusa-Navarro, F., Hahn, C., Massara, E., et al. 2020b, The
Astrophysical Journal Supplement Series, 250, 2,
doi: 10.3847/1538-4365/ab9d82
Villanueva-Domingo, P., & Villaescusa-Navarro, F. 2022,
Learning cosmology and clustering with cosmic graphs, arXiv,
doi: 10.48550/ARXIV.2204.13713
Xu, X., Cisewski-Kehe, J., Green, S., & Nagai, D. 2019,
Astronomy and Computing, 27, 34–52,
doi: 10.1016/j.ascom.2019.02.003
Yang, D., & Yu, H.-B. 2022, A graph model for the clustering of
dark matter halos, arXiv, doi: 10.48550/ARXIV.2206.05578
APPENDIX
Fig. 9.— Comparison of undecorated graph (green ellipse) and
decorated graph (dark ellipse) gIMNN Fishers, plotted over inte-
grated halo number density (dashed green lines) and mass fraction
(dark solid lines) functions from ﬁxed Mcut.
Decorating graphs
with mass information induces a slight rotation away from dn/dM
and towards f(σ) contours, since this quantity incorporates both
mass and halo number information.
COMPARING HALO MASS AND NUMBER DENSITY
FUNCTIONS
Here we discuss the diﬀerence between the halo num-
ber density function and halo mass function in the con-
text of gIMNN information extraction. The halo number
density function,
dn
dM = ρo
M
d ln σ−1(M)
dM
f(σ),
(A1)
and the halo mass function,
f(σ) = M
ρo
dn(M)
d ln(σ−1(M)),
(A2)
within the Press-Schechter formalism (Press & Schechter
1974). Integrating the halo number density from a ﬁxed
mass Mcut yields the number of halos with a mass above
this threshold:
N(Mi > Mcut) =
Z ∞
Mcut
dn
dM dM,
(A3)
which in our case is the node cardinality of a halo graph,
N v. By contrast, integrating the halo mass function from
Mcut yields the fraction of total mass residing in col-
lapsed halos of mass above Mcut:
F(Mi > Mcut) =
Z ∞
Mcut
f(σ)dM,
(A4)

13
which incorporates both halo number N v and mass in-
formation above Mcut.
We compare these two integrated quantities to undec-
orated and decorated gIMNN Fisher constraints in Fig-
ure 9. In the undecorated case (green ellipse), the net-
work has explicit access to halo number and clustering
information, resulting in contours more closely aligned
with integrated dn/dM (dashed green lines).
By con-
trast, when the graph nodes are annotated with mass
labels (black ellipse), the network has explicit access to
a combination of halo number, mass, and clustering in-
formation. This results in a slight rotation towards the
integrated HMF f(σ) contours (dark solid lines), since
this quantity reﬂects the addition of mass fraction infor-
mation. This eﬀect is also illustrated in Figure 7. As
discussed in Section 6, as noise level decreases, the net-
work has access to sharper mass information, inducing a
rotation towards the integrated HMF line.
DETAILS OF GRAPH ASSEMBLY IN JAX
Here we detail Jax-compatible graph assembly. Jax is
a pseudo-compiled language, meaning arrays must have a
pre-determined ﬁxed length before sent to a GPU device
for operations like gradient descent.
We navigate this
constraint by padding graph features by pre-determined
ﬁxed values, and masking features to assess information
content in a modular fashion.
Fig. 10.— Mass distribution after added noise, σnoise = 0.2Mcut
(black) and simulated halo ﬁnder cuts (teal) for a single ﬁducial
simulation for masses larger than 1.1 × 1015 M⊙.
The orange
dashed lines indicates the minimum mass considered by the “sur-
vey” cutoﬀ, Mcut = 1.5 × 1015 M⊙.
The Quijote catalogues are assembled into graphs by
ﬁrst making a mass cut on a larger selection of halos.
Masses are then padded with a pre-determined padding
value, generally chosen to be max(N v) + 10.
These
dummy halos are assigned a very large mass value and
a position outside of the 1Gpc3 box. A distance matrix
is then computed for both halo and dummy halo nodes,
along with sender-receiver indexes, (sk, rk). In the in-
variant graph case we also compute relative angles be-
tween all halos, outlined in Section 2.2.2. Connections
|dij| > rconnect and dij = 0 (self-edges) are then re-
moved and replaced by a large padding value larger than
rconnect. We compute N e by the number of distances that
ﬁt these criteria. Distance values and sender-receiver ar-
rays are then sorted smallest to largest and slotted into
padded arrays of length max(N e) + 10, where padded
edge values are then replaced with zeros. We encoun-
tered gradient stability issues when padding was made
too large for e.g. smaller edge sets, since more padding
means the network is asked to operate on extra non-
informative features.
This sorting arrangement is ad-
vantageous since small distances (local connections) are
always included in the halo graph’s edges, even if the
padding container is chosen to be too small for all edges
for a given rconnect value. What this means is that net-
works trained on a small rconnect can generalize reason-
ably well to datasets constructed with larger connection
criteria.
Adding Noise.
When constructing noisy catalogues
on-the-ﬂy, we ﬁrst truncate catalogues with a smaller
Mcut = 1.1 × 1015M⊙to include more halos in the true
catalog. Every training epoch, a new realisation of obser-
vational noise is then added to the masses according to
Section 6, after which halos below Mcut = 1.5 × 1015M⊙
are discarded, illustrated in Figure 10. The graph edge
attributes are then computed for the remaining halos as
described above.
For the noise scheme chosen in this
work, this results in approximately equal numbers of ha-
los being discarded above and below the mass cut line,
yet increases the uncertainty in the informative HMF
number count, which inﬂates constraints in Ωm.
DETAILS OF THE GNN ARCHITECTURE
Masking Graph Features.
To conduct information
extraction tests with and without node decoration, the
GNN architecture must remain ﬁxed between decorated
and undecorated cases. To mask a graph node or edge
feature, an indicator 1 is assigned to the array in place
of numerical value. These uninformative features are fed
through the network but do not contribute to the infor-
mation extraction since the same operation is performed
across ﬁducial and derivative datasets, yet ensure a fair
comparison of information as a function of catalogue data
features since network architectures (hidden and output
size dimensions) could be kept ﬁxed with the same ini-
tialized weights.
This paper was built using the Open Journal of As-
trophysics LATEX template. The OJA is a journal which
provides fast and easy peer review for new papers in the
astro-ph section of the arXiv, making the reviewing pro-
cess simpler for authors and referees alike. Learn more
at http://astro.theoj.org.

