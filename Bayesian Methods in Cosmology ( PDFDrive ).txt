
This page intentionally left blank

BAY E S I A N M E T H O D S I N
C O S M O L O G Y
In recent years, cosmologists have advanced from largely qualitative models of
the Universe to precision modelling using Bayesian methods in order to determine
the properties of the Universe to high accuracy. This timely book is the only com-
prehensive introduction to the use of Bayesian methods in cosmological studies,
and is an essential reference for graduate students and researchers in cosmology,
astrophysics and applied statistics.
The ﬁrst part of the book focuses on methodology, setting the basic foundations
and giving a detailed description of techniques. It covers topics including the
estimation of parameters, Bayesian model comparison, and separation of signals.
The second part explores a diverse range of applications, from the detection of
astronomical sources (including through gravitational waves), to cosmic microwave
background analysis and the quantiﬁcation and classiﬁcation of galaxy properties.
Contributions from 24 highly regarded cosmologists and statisticians make this an
authoritative guide to the subject.
MICHAEL P. HOBSON is Reader in Astrophysics and Cosmology at the Cavendish
Laboratory, University of Cambridge, where he researches theoretical and observa-
tional cosmology, Bayesian statistical methods, gravitation and theoretical optics.
ANDREW H. JAFFE is Professor of Astrophysics and Cosmology at Imperial
College, London, and a member of the Planck Surveyor Satellite collaboration,
which will create the highest-resolution and most sensitive maps of the cosmic
microwave background ever produced.
ANDREW R. LIDDLE is Professor of Astrophysics at the University of Sussex. He
is the author of over 150 journal articles and four books on cosmology, covering
topics from early Universe theory to modelling astrophysical data.
PIA MUKHERJEE is a Postdoctoral Research Fellow in the Astronomy Centre at the
University of Sussex, specializing in constraining cosmological models, including
dark energy models, from observational data.
DAVID PARKINSON is a Postdoctoral Research Fellow in the Astronomy Centre at
the University of Sussex, working in the areas of cosmology and the early Universe.


BAYESIAN METHODS
IN COSMOLOGY
M I C H A E L P. H O B S O N
Cavendish Laboratory, University of Cambridge
A N D R E W H. JA F F E
Imperial College, London
A N D R E W R. L I D D L E
University of Sussex
P I A M U K H E R J E E
University of Sussex
DAV I D PA R K I N S O N
University of Sussex

CAMBRIDGE UNIVERSITY PRESS
Cambridge, New York, Melbourne, Madrid, Cape Town, Singapore,
São Paulo, Delhi, Dubai, Tokyo
Cambridge University Press
The Edinburgh Building, Cambridge CB2 8RU, UK
First published in print format
ISBN-13    978-0-521-88794-6
ISBN-13    978-0-511-77015-9
© Cambridge University Press 2010
2009
Information on this title: www.cambridge.org/9780521887946
This publication is in copyright. Subject to statutory exception and to the 
provision of relevant collective licensing agreements, no reproduction of any part
may take place without the written permission of Cambridge University Press.
Cambridge University Press has no responsibility for the persistence or accuracy 
of urls for external or third-party internet websites referred to in this publication, 
and does not guarantee that any content on such websites is, or will remain, 
accurate or appropriate.
Published in the United States of America by Cambridge University Press, New York
www.cambridge.org
eBook (NetLibrary)
Hardback

Contents
List of contributors
page ix
Preface
xi
Part I
Methods
1
1
Foundations and algorithms
3
John Skilling
1.1
Rational inference
3
1.2
Foundations
4
1.3
Inference
11
1.4
Algorithms
20
1.5
Concluding remarks
32
2
Simple applications of Bayesian methods
36
D. S. Sivia and S. G. Rawlings
2.1
Introduction
36
2.2
Essentials of modern cosmology
37
2.3
Theorists and pre-processed data
41
2.4
Experimentalists and raw measurements
49
2.5
Concluding remarks
54
3
Parameter estimation using Monte Carlo sampling
57
Antony Lewis and Sarah Bridle
3.1
Why do sampling?
57
3.2
How do I get the samples?
59
3.3
Have I taken enough samples yet?
69
3.4
What do I do with the samples?
70
3.5
Conclusions
77
v

vi
Contents
4
Model selection and multi-model inference
79
Andrew R. Liddle, Pia Mukherjee and David Parkinson
4.1
Introduction
79
4.2
Levels of Bayesian inference
80
4.3
The Bayesian framework
82
4.4
Computing the Bayesian evidence
87
4.5
Interpretational scales
89
4.6
Applications
90
4.7
Conclusions
96
5
Bayesian experimental design and model selection forecasting
99
Roberto Trotta, Martin Kunz, Pia Mukherjee and David Parkinson
5.1
Introduction
99
5.2
Predicting the effectiveness of future experiments
100
5.3
Experiment optimization for error reduction
106
5.4
Experiment optimization for model selection
115
5.5
Predicting the outcome of model selection
120
5.6
Summary
124
6
Signal separation in cosmology
126
M. P. Hobson, M. A. J. Ashdown and V. Stolyarov
6.1
Model of the data
127
6.2
The hidden, visible and data spaces
128
6.3
Parameterization of the hidden space
129
6.4
Choice of data space
133
6.5
Applying Bayes’ theorem
137
6.6
Non-blind signal separation
140
6.7
(Semi-)blind signal separation
151
Part II
Applications
165
7
Bayesian source extraction
167
M. P. Hobson, Grac¸a Rocha and Richard S. Savage
7.1
Traditional approaches
168
7.2
The Bayesian approach
170
7.3
Variable-source-number models
175
7.4
Fixed-source-number models
178
7.5
Single-source models
178
7.6
Conclusions
191

Contents
vii
8
Flux measurement
193
Daniel Mortlock
8.1
Introduction
193
8.2
Photometric measurements
193
8.3
Classical ﬂux estimation
196
8.4
The source population
199
8.5
Bayesian ﬂux inference
201
8.6
The faintest sources
204
8.7
Practical ﬂux measurement
209
9
Gravitational wave astronomy
213
Neil Cornish
9.1
A new spectrum
213
9.2
Gravitational wave data analysis
214
9.3
The Bayesian approach
220
10
Bayesian analysis of cosmic microwave background data
229
Andrew H. Jaffe
10.1
Introduction
229
10.2
The CMB as a hierarchical model
231
10.3
Polarization
240
10.4
Complications
242
10.5
Conclusions
243
11
Bayesian multilevel modelling of cosmological populations
245
Thomas J. Loredo and Martin A. Hendry
11.1
Introduction
245
11.2
Galaxy distance indicators
247
11.3
Multilevel models
252
11.4
Future directions
261
12
A Bayesian approach to galaxy evolution studies
265
Stefano Andreon
12.1
Discovery space
265
12.2
Average versus maximum likelihood
266
12.3
Priors and Malmquist/Eddington bias
268
12.4
Small samples
270
12.5
Measuring a width in the presence of a contaminating population 272
12.6
Fitting a trend in the presence of outliers
275
12.7
What is the number returned by tests such as χ2, KS, etc.?
280
12.8
Summary
281

viii
Contents
13
Photometric redshift estimation: methods and applications
283
Ofer Lahav, Filipe B. Abdalla and Manda Banerji
13.1
Introduction
283
13.2
Template methods
285
13.3
Bayesian methods and non-colour priors
286
13.4
Training methods and neural networks
287
13.5
Errors on photo-z
289
13.6
Optimal ﬁlters
290
13.7
Comparison of photo-z codes
290
13.8
The role of spectroscopic datasets
292
13.9
Synergy with cosmological probes
294
13.10 Discussion
296
Index
299

Contributors
Filipe B. Abdalla
Department of Physics and Astronomy,
University College London,
Gower Street,
London WC1E 6BT, UK
Stefano Andreon
INAF–Osservatorio Astronomico di
Brera via Brera 28, 20121 Milano, Italy
M. A. J. Ashdown
Astrophysics Group, Cavendish
Laboratory, JJ Thomson Avenue,
Cambridge CB3 0HE, UK
Manda Banerji
Department of Physics and Astronomy,
University College London,
Gower Street, London WC1E 6BT, UK
Sarah Bridle
Department of Physics and Astronomy,
University College London,
Gower Street, London WC1E 6BT, UK
Neil Cornish
Department of Physics, Montana
State University, Bozeman,
MT 597717, USA
Martin A. Hendry
Department of Physics and Astronomy,
University of Glasgow,
Glasgow G12 8QQ, UK
M. P. Hobson
Astrophysics Group, Cavendish
Laboratory, JJ Thomson Avenue,
Cambridge CB3 0HE, UK
Andrew H. Jaffe
Astrophysics Group, Imperial College
London, Blackett Laboratory,
London SW7 2AZ, UK
Martin Kunz
Astronomy Centre, University of
Sussex, Brighton BN1 9QH, UK
Ofer Lahav
Department of Physics and Astronomy,
University College London, Gower Street,
London WC1E 6BT, UK
Antony Lewis
Institute of Astronomy and Kavli
Institute for Cosmology,
Madingley Road,
Cambridge CB3 0HA, UK
ix

x
List of contributors
Andrew R. Liddle
Astronomy Centre, University of
Sussex, Brighton BN1 9QH, UK
Thomas J. Loredo
Department of Astronomy,
Cornell University, Ithaca,
NY 14853, USA
Daniel Mortlock
Astrophysics Group, Imperial College
London, Blackett Laboratory, London
SW7 2AZ, UK
Pia Mukherjee
Astronomy Centre, University of
Sussex, Brighton BN1 9QH, UK
David Parkinson
Astronomy Centre, University of
Sussex, Brighton BN1 9QH, UK
Steve Rawlings
Astrophysics, Department of Physics,
Oxford University, Keble Road, Oxford
OX1 3RH, UK
Grac¸a Rocha
California Institute of Technology,
1200 East California Boulevard,
Pasadena, CA 91125, USA
Richard S. Savage
Astronomy Centre, University
of Sussex, Brighton BN1 9QH, UK,
and Systems Biology Centre, University
of Warwick, Coventry CV4 7AL, UK
D. S. Sivia
St John’s College, St. Giles,
Oxford OX1 3JP, UK
John Skilling
Maximum Entropy Data Consultants
Ltd, Kenmare, County Kerry, Ireland
V. Stolyarov
Astrophysics Group, Cavendish
Laboratory, JJ Thomson Avenue,
Cambridge CB3 0HE, UK
Roberto Trotta
Astrophysics, Department of Physics,
Oxford University, Keble Road,
Oxford OX1 3RH, UK
and Astrophysics Group, Imperial
College London, Blackett Laboratory,
London SW7 2AZ, UK

Preface
A revolution is underway in cosmology, with largely qualitative models of the
Universe being replaced with precision modelling and the determination of
Universe’s properties to high accuracy. The revolution is driven by three distinct
elements – the development of sophisticated cosmological models and the abil-
ity to extract accurate predictions from them, the acquisition of large and precise
observational datasets constraining those models, and the deployment of advanced
statistical techniques to extract the best possible constraints from those data.
This book focuses on the last of these. In their approach to analyzing datasets,
cosmologists for the most part lie resolutely within the Bayesian methodology for
scientiﬁc inference. This approach is characterized by the assignment of proba-
bilities to all quantities of interest, which are then manipulated by a set of rules,
amongst which Bayes’ theorem plays a central role. Those probabilities are con-
stantly updated in response to new observational data, and at any given instant
provide a snapshot of the best current understanding. Full deployment of Bayesian
inference has only recently come within the abilities of high-performance
computing.
Despite the prevalence of Bayesian methods in the cosmology literature, there is
no single source which collects together both a description of the main Bayesian
methods and a range of illustrative applications to cosmological problems. That, of
course, is the aim of this volume. Its seeds grew from a small conference ‘Bayesian
Methods in Cosmology’, held at the University of Sussex in June 2006 and attended
by around 60 people, at which many cosmological applications of Bayesian meth-
ods were discussed. CUP editor Vince Higgs, who attended the conference, saw
the need for a comprehensive volume covering these topics, and suggested that we
put together an edited volume of articles. And here it is!
The book is divided into two part. The ﬁrst part, ‘Methods’, concentrates on the
formalism, methods and algorithms, with only limited illustrative examples. The
focus is very much on those aspects that have proven valuable in cosmological
xi

xii
Preface
studies, complementing the much more complete treatments of Bayesian inference
given in the excellent books by MacKay (2003; see page 35), Gregory (2005; see
page 97), and Sivia and Skilling (2006; see page 35), all of which we recommend
to the interested reader. The second part, ‘Applications’, studies a wide range of
cosmological applications in detail. Many of the codes used in these applications
are publicly available.

Part I
Methods


1
Foundations and algorithms
John Skilling
Why and how – simply – that’s what this chapter is about.
1.1 Rational inference
Rational inference is important. By helping us to understand our world, it gives us
the predictive power that underlies our technical civilization. We would not func-
tion without it. Even so, rational inference only tells us how to think. It does not
tell us what to think. For that, we still need the combination of creativity, insight,
artistry and experience that we call intelligence.
In science, perhaps especially in branches such as cosmology, now coming of
age, we invent models designed to make sense of data we have collected. It is no
accident that these models are formalized in mathematics. Mathematics is far and
away our most developed logical language, in which half a page of algebra can
make connections and predictions way beyond the precision of informal thought.
Indeed, one can hold the view that frameworks of logical connections are, by def-
inition, mathematics. Even here, though, we do not ﬁnd absolute truth. We have
conditional implication: ‘If axiom, then theorem’ or, equivalently, ‘If not theorem,
then not axiom’. Neither do we ﬁnd absolute truth in science.
Our question in science is not ‘Is this hypothetical model true?’, but ‘Is this
model better than the alternatives?’. We could not recognize absolute truth even
if we stumbled across it, for how could we tell? Conversely, we cannot recognize
absolute falsity. If we believe dogmatically enough in a particular view, then no
amount of contradictory data will convince us otherwise, if only because the data
could be dismissed as evidence of conspiracy to deceive. Yet even a determined
sceptic might be sufﬁciently charitable to acknowledge that a model with demon-
strable ability to predict future effects could have practical value.
3

4
Foundations and algorithms
Let us, then, avoid the philosophical mineﬁelds of belief and truth, and pay at-
tention to what we really need, which is predictive ability. We anticipate the Sun
will rise tomorrow, not just because it always has done so far, but because this is
predicted by models of stellar structure and planetary dynamics, which accord so
well with such a variety of data that perceived failure of the Sun to rise might more
likely be hallucination.
Rational assessment of different models is the central subject of Bayesian meth-
ods, so called after Revd Thomas Bayes, the eighteenth century clergyman gener-
ally associated with the beginnings of formal probability theory. We will ﬁnd that
probability calculus is forced upon us as the only method which lets us learn from
data irrespective of their order – surely a required symmetry. We will also discover
how to use it properly, with the aid of modern computers and algorithms. Inference
was held back for a century by technical inability to do the required sums, but that
sad era has closed.
1.2 Foundations
Suppose we are given a choice of basic models, a = apple, b = banana, c =
cherry, which purport to explain some data. Such models can be combined, so that
apple-OR-banana, written a ∨b, is also meaningful. Data that excluded cherries
would, in fact, bring us down from the original apple-OR-banana-OR-cherry
combination to just that choice. With n basic models, there are 2n possible combi-
nations. They form the elements of a lattice, ranging from the absurdity in which
none of the models is allowed, up to the provisional truism in which all of them
remain allowed. In inference, we need to be able to navigate these possibilities as
we reﬁne our knowledge.
a ∨b ∨c
a ∨b
a ∨c
b ∨c
a
b
c
∅


Q
Q
Q


Q
Q
Q


Q
Q
Q


Q
Q
Q
The absurdity ∅is introduced merely because analysis is cleaner with it than with-
out it, rather as 0 is often included with the positive integers.
The three core concepts of measure, information and probability all have wider
scope than inference alone. They apply to lattices in general, whether or not the
lattice ﬁlls out all 2n possibilities. By exposing just the foundation that we need,
and no more, we can allow wider application, as well as clarifying the basis so that

1.2 Foundations
5
alternative formulations of these concepts become even less plausible than they
may have been before.
1.2.1 Lattices
The critical idea we need is ‘partial ordering’. We always have “=”: every element
equals itself, x = x. Sometimes, we have “<”, as in x < y, meaning that y includes
x. In inference, we say that apple is included in apple-OR-banana, because the
scope of the latter is wider and includes all of the former, but we would not try to
include apple within banana. We don’t need that particular motivation, though. All
we need is “<” in the abstract. Ordering is to be transitive,
x < y and y < z implies x < z,
(1.1)
otherwise it would not make sense.
The other idea we need is ‘least upper bound’. The upper bounds to elements x
and y are those elements at or including both x and y. If there is a least such bound,
we write it as x ∨y and call it the least upper bound:
 x ≤x ∨y
y ≤x ∨y

, and x ∨y ≤u for all u obeying
 x ≤u
y ≤u

.
(1.2)
In inference, the unique least upper bound x ∨y is that element including all the
components of x and y, but no more. There, the existence of least upper bound is
obvious.
Technically, a lattice is a partially ordered set with least upper bound, so that “<”
and “∨” are deﬁned. Any pair of elements x and y also has lower bounds, being all
those elements at or beneath both. There is a unique greatest lower bound, written
x ∧y. (If there were alternatives u and v, then u ∨v could be ambiguously x or
y, contradicting uniqueness of their least upper bound.) Mathematicians (Klain &
Rota 1997) traditionally deﬁne a lattice in terms of ∨and ∧, but our use of <
and ∨is equivalent, and (with =) underlies their traditional axioms of reﬂexivity,
antisymmetry, transitivity, idempotency, commutativity, associativity and absorp-
tion. Of these, the associativity property
(x ∨y) ∨z = x ∨(y ∨z)
(1.3)
is of particular importance to us.
What we now seek is a numerical valuation v(x) on our lattice of models, so that
we can rank the possibilities. Remarkably, there is only one way of conforming to
lattice structure, and this leads us to measure theory, thence to information and
probability. Though modernized following Knuth (2003), the approach dates back
to Cox (1946, 1961).

6
Foundations and algorithms
1.2.2 Measure
Addition
For a start, we want valuations to conform to “≤”, so we require
x ≤y
=⇒
v(x) ≤v(y).
(1.4)
Moreover, whatever our valuations were originally, we can shift them to give a
standard value 0 to the ubiquitous absurdity ∅, so that the range of value becomes
0 = v(∅) ≤v(x).
We next assume that if x and y are disjoint, so that x ∧y = ∅and they have
nothing in common, then the valuation v(x ∨y) should depend only on v(x) and
v(y). Write this relationship as a binary operation ⊕,
v(x ∨y) = v(x) ⊕v(y) when x ∧y = ∅.
(1.5)
To conform with associativity (1.3), we require

v(x) ⊕v(y)

⊕v(z) = v(x) ⊕

v(y) ⊕v(z)

.
(1.6)
This has to hold for arbitrary values v(x), v(y), v(z), and the associativity theorem
(Azc´el 2003) then tells us that there must be some invertable function F of our
valuations v such that
F

v(x ∨y)

= F

v(x)

+ F

v(y)

.
(1.7)
That being the case, we are free to discard the original valuations v and use m =
F(v) instead, for which ∨is simple addition: for disjoint x and y we have the sum
rule
m(x ∨y) = m(x) + m(y)
(1.8)
In other words, valuation can without loss of generality be taken to be what math-
ematicians call a ‘measure’. They traditionally deﬁne measures from the outset as
additive over inﬁnite sets, but offer little justiﬁcation. Mathematicians just do it.
Physicists want to know why. Here we see that there’s no alternative, and although
we start ﬁnite we can extend to arbitrarily many elements; it is the same struc-
ture. This is why measure theory works – it is because of associativity – and we
physicists don’t have to worry about the inﬁnite.
Assignment
As for actual numerical values, we can build them upwards by addition – except
for foundation elements that are not equal to any least upper bound of different
elements. Those values alone cannot be determined by the sum rule.

1.2 Foundations
7
Thus, in the inference example, we can value an apple, a banana and a cherry
arbitrarily, but there is a scale on which combinations add. On that scale, if an ap-
ple costs 3c/ and a cherry costs 4c/, then their combination costs 3 + 4 = 7c/, not
32 + 42 = 25 or other non-linear construction. Associativity underlies money. Per-
sonal assignments may be on a different scale. In economics, for example, personal
beneﬁt is sometimes held to be logarithmic in money, m = log($), to reﬂect the
asymmetry between devastating downside risk and comforting upside reward. On
that scale, money combines non-linearly, as log $(x ∨y) = log $(x) + log $(y).
That’s permitted, the point being that there is a scale on which one’s numbers add.
Thus quantiﬁcation is intrinsically linear – because of associativity.
In inference, ∨behaves as logical OR and ∧as logical AND, obeying the extra
property of distributivity:
(x OR y) AND z = (x AND z) OR (y AND z) ,
(x AND y) OR z = (x OR z) AND (y OR z) .
(1.9)
Equivalently, they behave as set union and set intersection of the foundation ele-
ments, which can therefore be assigned arbitrary values. In other applications, ∨
and ∧might not be distributive, and the foundation assignments become restricted
by the non-equality of combinations that would otherwise be identical. But their
calculus would still be additive.
Multiplication
As well as by addition, measures can also combine by multiplication. Here, we
consider a direct product of lattices. For example, one lattice might have playing-
card foundation elements (♠, ♥, ♣, ♦) while the other has music-key foundations
(♭, ♮, ♯). The direct-product lattice treats both together, here with 12 foundation
elements like ♥× ♮and 212 elements overall:
♠×♭♥×♭♣×♭♦×♭
♠×♮♥×♮♣×♮♦×♮
♠×♯♥×♯♣×♯♦×♯
=
♠
♥
♣
♦
×
♭
♮
♯
We now assume that the measure m(x×y) should depend only on m(x) and m(y).
Write this relationship as a binary operation
m(x × y) = m(x) ⊗m(y).
(1.10)
Now the direct-product operator is associative, (x × y) × z = x × (y × z),

8
Foundations and algorithms
""
""
""
""
""
""
""
""
""
""
=
""
""
""
=
"
""
"
""
"
""
"
""
"
""
"
""
""
""
so

m(x) ⊗m(y)

⊗m(z) = m(x) ⊗

m(y) ⊗m(z)

.
(1.11)
This has to hold for arbitrary values m(x), m(y), m(z), and the associativity theo-
rem then tells us that there must be some invertable function Φ of the measures m
such that
Φ(m(x × y)) = Φ(m(x)) + Φ(m(y)).
(1.12)
We cannot now re-grade to Φ(m) and ignore m because we have already ﬁxed
the behaviour of m to be additive. What we can do is require consistency with
that behaviour by requiring the sum rule (1.8) to hold for composite elements,
m(x × t) + m(y × t) = m((x ∨y) × t) for any t. The context theorem (Knuth
and Skilling, in preparation) then shows that Φ has to be logarithmic, so that
m(x × y) = m(x) m(y).
(1.13)
While ⊕is addition, ⊗is multiplication. Combination is intrinsically multiplicative
– because of associativity. There is no alternative.
Commutativity
Technically, we have not used the commutative property x ∨y = y ∨x of a lattice.
However, the sum rule automatically generates values that are equal, v(x ∨y) =
v(y ∨x). So real valuations cannot capture non-commutative behaviour. Quantum
mechanics is an example, where states lack ordering so do not form a lattice, and
the calculus is complex. Inference is not an example. There, apple-OR-banana is
the same as banana-OR-apple so ∨is commutative for us, and we are allowed to
use the real values that we need.
1.2.3 Information
Different measures can legitimately be assigned to the same foundation elements,
as when different individuals value apples, bananas and cherries differently. The
difference between source measure μ and destination measure m can be quantiﬁed,
consistently with lattice structure, as ‘information’ H(m | μ).
One way of deriving the form of H is as a variational potential, in which desti-
nation m is obtained at the extremal (minimum, actually) of H, subject to whatever
constraints require the change from μ. Suppose the playing-card example above has

1.2 Foundations
9
source measure μ, with destination m obtained by some constraint on card suits.
Independently, the music-key example has source measure ν, with destination n
obtained by some constraint on music keys.
Equivalently, we must be able to analyze the problems jointly. Measures multi-
ply, so the joint element ‘card suit i and music key j’ has source measure μiνj and
destination minj. The latter is to be obtained at the extremal of H(minj | μiνj),
under one constraint acting on i and another on j. Temporarily suppressing the
ﬁxed source μν, the variational equation for the destination measure is
H′(minj) = λ1(i) + λ2(j),
(1.14)
where the λ’s are the Lagrange multipliers of the i and j constraints. Writing x =
mi and y = nj, and differentiating ∂2/∂x∂y, the right-hand side is annihilated,
leaving
xyH′′′(xy) + H′′(xy) = 0,
(1.15)
whose solution is
H(z) = A −Bz + Cz log z.
(1.16)
Setting C = 1 an as arbitrary scale (positive to ensure a minimum), B = 1 to place
that minimum correctly at m = μ, and A = μ to make the minimum zero, we
reach (Skilling 1988)
H(m | μ) = μ −m + m log m
μ
(1.17)
This obeys (1.14), so the potential we seek exists, and is required to be of this
unique form. The difference between measures plays a deep rˆole in Bayesian
analysis.
1.2.4 Probability
Acquiring data involves a reduction of possibilities. Some outcomes that might have
happened, did not. In terms of the lattice of possibilities, the all-encompassing top
element moves down. To deal with this, we seek a bi-valuation p(x | t), in which
the context t of model x can shrink. Within any ﬁxed context, p is to be a measure,
being non-negative and obeying the sum rule. But we want to change the numbers
when the context changes.
To ﬁnd the dependence on context, take ordered elements x ≤y ≤z ≤t. As
before, we require conformity with lattice ordering, here
x ≤y ≤z
=⇒
p(x | z) ≤p(x | y)
(1.18)

10
Foundations and algorithms
so that a wider context dilutes the numerical value. Ordering such as x ≤z can be
carried out in two steps, x ≤y and y ≤z. Our bi-valuation should conform to this,
meaning that we require a “⊙” operator combining the two steps into one:
p(x | z) = p(x | y) ⊙p(y | z).
(1.19)
Extending this to three steps and considering passage p(x | t) from x to t, via y
and z, gives another associativity relationship,

p(x | y) ⊙p(y | z)

⊙p(z | t) = p(x | y) ⊙

p(y | z) ⊙p(z | t)

,
(1.20)
representing (((x ≤y) ≤z) ≤t) = (x ≤(y ≤(z ≤t))). As before, this induces
some invertable function Φ of our valuations p such that
Φ

p(x | z)

= Φ

p(x | y)

+ Φ

p(y | z)

.
(1.21)
Again, we require consistency with the sum rule p(x ∨y | t) = p(x | t) + p(y | t)
for arbitrary context t. A variant of the context theorem (Knuth and Skilling, in
preparation) then shows that Φ has to be logarithmic as before, so ⊙was multipli-
cation. Speciﬁcally, we recognize p as probability, hereafter “pr”,
0 = pr(∅) ≤pr(x) ≤pr(t) = 1
Range
pr(x ∨y) = pr(x) + pr(y)
Sum rule for disjoint x, y
pr(x ∧y) = pr(x | y) pr(y)
Product rule
⎫
⎬
⎭
∥t (1.22)
(The “ ∥t ” notation means that all probabilities are conditional on t, and avoids
proliferation of “ | t ” without introducing ambiguity.)
Just as measure theory was forced for valuations, so probability theory is forced
for bi-valuations. We need not be distracted by claimed alternatives because they
conﬂict with very general requirements. It is all very simple. There’s only this one
calculus for numerical bi-valuations on a lattice. If, say, we seek a calculus for
conditional beliefs, then this has to be it. But the calculus itself is abstract and
motive-free. We don’t have to subscribe to an undeﬁned idea like ‘belief’ in order
to use it. In fact, the reverse holds. It is probability, with its deﬁned properties, that
would underpin belief, not the other way round.
Most simply of all, probability calculus can be subsumed in the single deﬁnition
of probability as a ratio deﬁnition of measures:
pr(x | t) = m(x ∧t)
m(t)
(1.23)
This is the original discredited frequentist deﬁnition, as the ratio of number of
successes to number of trials, now retrieved at an abstract level, which bypasses
the catastrophic difﬁculties of literal frequentism when faced with isolated non-
reproducible situations. The calculus of probability is no more than the calculus of
proportions.

1.3 Inference
11
1.3 Inference
Henceforward, in accordance with traditional accounts, we take all foundation ele-
ments to be disjoint, and work in terms of these. The OR operator ∨can be replaced
by the summation to which it reduces, while the AND operator ∧can be written as
the traditional comma. It is also usual to use I for context, and allow the discrete
choice x to be continuous θ. The rules of probability calculus then reduce to
pr(θ) ≥0
Positivity
	
pr(θ) dθ = 1
Sum rule
pr(φ, θ) = pr(φ | θ) pr(θ)
Product rule
⎫
⎪
⎬
⎪
⎭
∥I
(1.24)
1.3.1 Bayes’ theorem
In inference, we need to consider both parameter(s) θ and data D, all in the over-
arching context I of all possibilities we are currently considering. By the product
law, the joint probability of model and data factorizes:
pr(θ) pr(D | θ)
= pr(θ, D) =
pr(D) pr(θ | D)
∥I
Prior × Likelihood
=
Joint
=
Evidence × Posterior
π(θ) L(θ)
= · · · · · · =
E P(θ)
Inputs
=====⇒
Outputs
(1.25)
On the left lies the prior probability π(θ) = pr(θ | I), representing how we orig-
inally distributed the parameters’ unit mass of probability. This assignment has
provoked legendary argumentation, and we discuss it below. Also on the left is the
likelihood L(θ) = pr(D | θ), representing the probability distribution of the data
for each allowed input θ. This is less controversial. The instrument acquiring the
data can usually be calibrated with known inputs θ to ﬁnd how often it produces
speciﬁc outputs D, which effectively ﬁxes the likelihood to any desired precision.
If there remain any unknown calibration parameters in the likelihood, they can be
incorporated in θ as extra parameters to be determined, leading to extra computa-
tion but no difﬁculty of principle.
On the far right is the posterior P(θ) = pr(θ | D, I), representing our inferred
distribution of probability among the models, after using the data. The difference
between prior and posterior is the information (1.17)
H(P | π) =

P(θ) log

P(θ)/π(θ)

dθ
(1.26)
gleaned about θ. Also on the right is the evidence E = pr(D | I), representing
how well our original assignments managed to predict the data. E is also known
as ‘prior predictive’ (how it is often used), ‘marginal likelihood’ (how it is often

12
Foundations and algorithms
computed), and various similar terms. However, there ought to be a simple moniker
(what it is) for this key quantity in Bayesian analysis, and ‘evidence’ (not to be
confused with dataset) is that name (MacKay 2003).
Of course, the terminology is for convenience only. It is not hard and fast. A pos-
terior to a ﬁrst analyst may become a prior to a second with new data. Evidence
values become likelihoods if the context is widened, so that I becomes merely
a provisional model within a wider analysis, and so on. There is really just one
quantity, probability.
The two outputs, evidence and posterior, can be disentangled by noting that the
posterior, being a probability, sums to 1. Here, then, is the complete calculus of
inference:

π(θ) dθ = 1
Prior
E =

π(θ) L(θ) dθ
Evidence
P(θ) = π(θ) L(θ)
E
Bayes’ theorem
(1.27)
Bayes’ theorem shows how the prior is modulated into posterior through the likeli-
hood/evidence ratio. The same ratio L/E = P/π shows up in the information H,
alternatively known as the negative entropy.
1.3.2 Prior probability
Before using the data, we need to assign a distribution of prior probability. Prob-
ability calculus tells us how to manipulate probability values, but not what they
should be in the ﬁrst place. Neither does the world tell us. The only restriction is
that, by the sum rule, all the possibilities must add to 1. Beyond that, we are free to
invent any model we want. In that sense, anything goes. It is a challenge. However,
the world does give its opinion through data. Better hypotheses predict the data
better, through having high values of evidence. That and that alone is what we get,
and it is all we need for understanding and for technology. The quest for certainty
is mistaken and naive.
Guidelines have been developed for assigning priors, but beware the dangers!
The ﬁrst step is to decide on a model – which parameters are to be used to predict
the data? The range of these parameters deﬁnes what is known as the ‘hypothesis
space’, over which unit mass of prior probability is to be distributed.

1.3 Inference
13
Informal
No matter how sophisticated the methodology, there is in the end no escape from
an informal assessment of what is judged reasonable in the light of whatever back-
ground knowledge is available. Your author proceeds by contemplating perhaps ten
points, each representing 10% of the prior, and assigning plausible θ to them. This
introspection gives a rough range and indication of shape for the prior, which is
then assigned some algebraic form conforming to these. Instead of putting prior
mass onto θ, this procedure puts θ onto prior mass, which seems more sympathetic
to the basic equations. It is also more sympathetic to the computational require-
ments, because the prior is uniform by deﬁnition when prior mass is the underly-
ing coordinate. Either way, the prior doesn’t have to be ‘right’ in some undeﬁnable
sense – it just has to be reasonable.
For a location parameter, an informal centre c and width w might suggest a
Cauchy distribution,
π(θ) = w/3.14159 . . .
(θ −c)2 + w2 ,
(−∞< θ < ∞),
(1.28)
which comfortably tolerates quite wide excursions from the guessed centre if the
data demand them. Note that we don’t need to interpret c and w as moments, and
indeed we may be wiser not to. Probability calculus requires normalization, but
not the existence of mean and standard deviation. To some extent, moments are a
holdover from the days of manual paper-and-pencil calculation.
A necessarily positive intensity parameter of plausible magnitude a might be
assigned either a truncated Cauchy or an exponential distribution,
π(θ) = a−1e−θ/a ,
(θ > 0) .
(1.29)
If magnitude was accompanied by width, then a Gamma distribution
π(θ) = θ−1+μe−θ/λ
Γ(μ)λμ
,
(θ > 0)
(1.30)
might be appropriate. Whatever the choice, the prior has to be normalized because
it is a probability.
Symmetry
Sometimes, our knowledge of some or all of the states is invariant to exchange.
The classic example is a six-sided die, for which θ can be 1 or 2 or 3 or 4 or 5 or 6.
Given this knowledge and nothing more, we can only assign equal probability to
each: π(1) = π(2) = · · · = π(6) = 1
6. If we did anything else, say π(1) > π(2),
then we could exchange the labels 1 and 2 and reach a different assignment with
π(2) > π(1) on the basis of a null change to our prior knowledge. So the same state

14
Foundations and algorithms
of knowledge would be coded two different ways, which is unlikely to be helpful.
Prior assignments should conform to any symmetry in our prior knowledge. This
does not mean that the object being investigated need be symmetric. Indeed, data
may well tell us it is not.
Symmetry arguments can be over-played. Here, the classic example is a loca-
tion parameter θ for which the hypothesis space is unbounded, −∞< θ < ∞.
Given this, and nothing more, one’s prior knowledge would be invariant to offset
of origin, implying π(θ) = constant. After acquiring data, the posterior distribution
P = πL/E would be independent of whatever constant was chosen. P would be
proportional to the likelihood, which would plausibly prohibit inﬁnite values. With
non-zero constant, the prior would become un-normalized (the dreaded ‘improper
prior’), but otherwise all might be well.
Actually, no. The posterior is the lesser half of Bayesian inference. The evi-
dence comes ﬁrst. As the allowed range W of θ increases indeﬁnitely, the prior
π = 1/W decreases indeﬁnitely, and so does the evidence. This means that the
model with W →∞loses by an inﬁnite factor when compared with any prior
that includes even the slightest knowledge of the expected range. In the limit, the
posterior becomes P(θ) = 0×L(θ) / 0, and total ignorance is seen to be total stu-
pidity. Moreover, the improper prior extending arbitrarily far fails the sanity check
of informal assessment. Are you really almost certain that |θ| > 10100?
Approximate invariance to small offsets of origin suggests that the prior should
be smooth, but that’s as far as the argument should go.
Maximum entropy
Sometimes, informal background knowledge is accompanied by ‘testable’ con-
straints in the form of known means ⟨Q⟩=
	
Q(θ)p(θ) dθ. Here, the informal
prior π can be modiﬁed to a revised p by minimizing the information H(p | π).
‘Entropy’ is just the traditional word for the negative of information, so that max-
imum entropy just means minimum information. Note that maximum entropy is
a method of assignment, not inference. It assigns a single p to be used in later
inference. It does not infer a probabilistic distribution of plausible p’s.
The variational equation is
δ
 
p(θ) log p(θ)
π(θ) dθ + λ0

p(θ) dθ + λ1

Q(θ)p(θ) dθ + · · ·

= 0, (1.31)
where the ﬁrst term is H, λ0 is the Lagrange multiplier for normalization, λ1 is the
multiplier for the constraint, and so on for as many constraints as are given. The
solution is
p(θ) = π(θ) exp(−1 −λ0 −λ1Q(θ) −· · · ),
(1.32)

1.3 Inference
15
where the λ’s ﬁt the constraints to their required values.
One standard example is a location parameter subject to ﬁrst and second
moments:
μ =

θ p(θ) dθ ,
μ2 + σ2 =

θ2p(θ) dθ.
(1.33)
The original π becomes modiﬁed by a Gaussian,
p(θ) = π(θ) exp(−1 −λ0 −λ1θ −λ2θ2).
(1.34)
If the original informal knowledge was weaker than the new constraints, so that
π was effectively constant within a few σ of μ, the Gaussian modiﬁcation would
dominate, leading to the standard normal (or Gaussian) distribution:
p(θ) = exp

−(θ −μ)2/2σ2
√
2πσ2
,
(π = 3.14159 . . .) .
(1.35)
Another standard example is an intensity parameter subject to mean value μ.
Here, the original π becomes modiﬁed by an exponential,
p(θ) = π(θ) exp(−1 −λ0 −λ1θ) ,
(θ > 0) .
(1.36)
Again, if the original knowledge was appropriately weaker than the new con-
straints, so that π was effectively constant for small or moderate θ, this result be-
comes of standard exponential form:
p(θ) = μ−1 exp(−θ/μ) ,
(θ > 0) .
(1.37)
On the other hand, if the original knowledge was weak but different, perhaps
effectively uniform over θ2 so that π ∝θ, the result
p(θ) = (θ/μ2) exp(−θ/μ) ,
(θ > 0)
(1.38)
would be different too. Maximum entropy reﬁnes prior knowledge, but does not
replace it.
Continuous problems
Here, we want to infer a measure mi deﬁned over so many states i that we may as
well use continuum notation m(x). Such a model might represent a spectrum or
image, whose intensity is distributed across frequency or spatial coordinate(s) x. In
practice, x is digitized into cells i, with the continuum limit merely meaning that
macroscopic results stop changing when the digitization gets arbitrarily ﬁne.
We commonly want cell boundaries to be invisible. This means that, in the ab-
sence of data saying otherwise, we have the same expectation for the intensity
accumulated in a domain Δx whether the domain is treated as a whole, or as the

16
Foundations and algorithms
sum of two or more subdivisions. In symbols, with cell k = i ∪j decomposed into
i and j (known as ‘stick-breaking’), we require
π(mk) =

δ(mk −mi −mj) π(mi, mj) dmi dmj
(1.39)
so that the intensities add correctly as mk = mi + mj. Additionally, we often
suppose that each cell is to behave independently,
π(mi, mj) = π(mi)π(mj),
(1.40)
so that there is no prior expectation of internal correlation.
These conditions are actually quite restrictive on the form of prior. As an exam-
ple of a prior that does not work, take the candidate π(mk) = δ(mk−1)+δ(mk−2)
with two-point support (1 and 2) for the values. This cannot be subdivided at all,
let alone inﬁnitely. In any subdivision into symmetric halves, each half would need
at least two points of support, because giving each only one would be insufﬁcient.
But the combination would then cover at least three points, which is too many:
QED. Another prior that does not work is π(m) ∝exp(−H(m)), proposed in
the hope that maximum entropy assignment of a single m might be promoted to a
distribution of m’s. That cannot be subdivided either.
In technical parlance, a prior that behaves consistently on all scales right down
to the inﬁnitesimal limit is called a ‘process’, and the property of such consistency
is called ‘inﬁnite divisibility’ (Steutel 1979). One prior that does work, common in
physics, is the Poisson process. Each small cell i is usually empty, but has small
probability λi of receiving a quantum, and negligible chance of more than one.
By construction, this works in the inﬁnitesimal limit. Occupancies r (usually 0,
occasionally 1, negligibly more) of small cells are distributed as
π(r1, r2, . . . , rn) =
n

i=1

(1 −λi)δ(ri −0) + λiδ(ri −1)

.
(1.41)
If the quanta are allowed to have individually variable intensity, say exponential
π(m | quantum) = e−m
(1.42)
in suitable units, the intensity pattern among the small cells is
π(m1, m2, . . . , mn) =
n

i=1

(1 −λi)δ(mi) + λie−mi
,
(1.43)
with each small cell having a small chance of holding a macroscopic intensity. If
micro-cells are combined, their Poisson rates λ add, so that λ(x) is itself a measure
on x. In fact, there is an exact macroscopic formula,
π(m) = e−λ
δ(m) + e−m
λ/m I1(2
√
λm)

,
(1.44)

1.3 Inference
17
Poisson
0
5
10
0
1
2
Gamma
0
5
10
0
1
2
Fig. 1.1. (Left) Sample of Poisson process averaging 10 spikes of mean intensity 1. (Right)
Sample of Gamma process of same mean and variance.
for the Poisson model. It is parameterized by the unit of quantum intensity (here 1)
and by the production measure λ, and is not too difﬁcult to program in terms of its
constituent quanta.
Another prior that works, more popular among statisticians, is the Gamma
process:
π(m1, m2, . . . , mn) =
n

i=1
m−1+λi
i
Γ(λi) e−mi,
(1.45)
where, as before, λ(x) is a measure over x. Although the formula is arguably sim-
pler algebraically, it is less interpretable and more expensive to program because
every micro-cell enters the prior (instead of a limited number of quanta). Not that
random samples look very different. As calculated at high resolution, both give
spiky results (Figure 1.1). The difference is that the Gamma process produces a lot
of extremely low-level grass which the Poisson process cuts away. If the Gamma
measure m is normalized, the formula reduces to the Dirichlet process (Ferguson
1973)
π(p1, p2, . . . , pn) = δ

1 − p

Γ
 λ

n

i=1
p−1+λi
i
Γ(λi)
(1.46)
for inferring a probability distribution p.
Geometry
When two measures m and m + δm are close, their information H becomes ap-
proximately symmetric,
H(m + δm | m) ≈
n

j=1
(δmj)2
mj
,
(1.47)

18
Foundations and algorithms
and behaves as a distance-squared in parameter space, here digitized to n points
for notational clarity, and with coordinates written as contravariant superscripts
because the space is about to become Riemannian. The metric describing this dis-
tance is diagonal,
gjk =
 1/mj
if j = k,
0
otherwise,
(1.48)
and, locally, H = (ds)2 =  gjk dmj dmk. Measures within a small ﬁxed dis-
tance ϵ of m ﬁll an ellipsoid of ‘radius’ ϵ and volume proportional to (det g)−1/2.
By supposing that ϵ-ellipsoids all contain the same mass, regardless of central lo-
cation, the m’s induce their own natural density proportional to (det g)1/2. In the
absence of any better guidance, one might try to use this as the prior on m.
For example, suppose parameter space has just two intensities (a 2-cell image,
perhaps), over which we seek a prior π(m1, m2). The proposal is
(det g)1/2 =

det
 1/m1
0
0
1/m2

=
1
√
m1m2 .
(1.49)
An immediate objection is that this expression is not normalizable, leading to an
improper prior for m. However, the components of m could represent proportions
p and 1−p of some ﬁxed total, and the proposed prior for p (the shape of the 2-cell
image) would then be
π(p) = 1/3.14159 . . .

p(1 −p)
,
(1.50)
which is normalized, and might well be acceptable.
The generalization to a larger number n of proportions p is a Dirichlet distri-
bution (1.46), but with all indices λ equal to 1
2. This would not be acceptable for
inference about an image digitized to arbitrarily many cells. The reason is that
macroscopic structure is washed out. For example, the total proportion in any n/2
cells is almost certain to be very close to the uniform, featureless 1
2. Dirichlet in-
dices λ need to be a measure, thereby getting individually smaller as cells are
subdivided. The geometrical indices of 1
2 do not get smaller. Here, the geometric
proposal fails to be inﬁnitely divisible.
The geometrical formulation can be generalized to a parameterized subspace
mi = m(i | θ) restricted to the range of r parameters θ1, . . . , θr. The information
between neighbours becomes
H(θ+dθ | θ) =
n

i=1
(dmi)2
mi
=
n

i=1
1
mi

r

j=1
∂mi
∂θj dθj
r

k=1
∂mi
∂θk dθk
, (1.51)

1.3 Inference
19
in which the coefﬁcient of dθj dθk, namely
gjk =
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
n

i=1
1
m(i | θ)
∂m(i | θ)
∂θj
∂m(i | θ)
∂θk
(discrete)

dx
m(x | θ)
∂m(x | θ)
∂θj
∂m(x | θ)
∂θk
(continuous),
(1.52)
deﬁnes the ‘Fisher metric’ with respect to changes in θ. Thus, if the parameters θ
are being used to deﬁne a measure or probability distribution m, that relationship
induces a metric g, and thence a ‘Fisher density’ (det g)1/2 which might be usable
as a prior on θ. After all, this density is derived from the coordinate-invariant H,
so represents a coordinate-invariant mass. That sounds attractive.
There has even been a proposal to automate the whole process by taking the
induced measure m(x | θ) to be the likelihood function L(x | θ) that characterizes
the experiment (er, which experiment that we haven’t built yet?) that eventually
observes data x = D. However, that idea doesn’t work. A sufﬁcient reason is
that the prior on θ would then depend on the entire form of likelihood function,
integrated over all anticipated x. After the data were found to be D and D alone,
posterior inference would still depend on all the other data values that might have
been observed but were not. That’s a recipe for disaster, because with the falsity in
play one can ‘prove’ anything.
Even discarding the likelihood idea, the geometric proposal can fail to be nor-
malized. As a simple example, take the family
p(x | θ) = e−θ cos x
2πI0(θ)
(1.53)
of maximum entropy probability distributions on the unit circle 0 ≤x < 2π,
parameterized by θ determined by a constraint on the mean ⟨cos x⟩. Their Fisher
density evaluates to
(det g)1/2 =

d2 log I0 / dθ2,
(1.54)
which behaves acceptably for small θ but not for large, where its asymptotic | θ |−1
behaviour cannot be normalized. If used as a prior for θ, it would put all its mass
at inﬁnite θ, as if x = 0 and x = π were the only two points on the circle that
mattered.
Geometry leaves no place for prior knowledge or judgment, so it represents deep
ignorance. Yet that ignorance can easily be too extreme, and contradict reasonable
expectation. The fact is that we never are as completely ignorant as the geomet-
rical approach supposes. A geometrical prior can be acceptable, but one should
always use the sanity check of informal assessment, and over-ride the proposal if
necessary.

20
Foundations and algorithms
The general need to employ informal assignment, or at least assessment, indi-
cates that the long search for general ‘objective’ priors is doomed. Inference is
inherently subjective, through its dependence on incompletely formalized back-
ground knowledge. Like it or not, that’s the way it is. The world assesses our priors
through their evidence values, requiring us to question and observe. It does not give
divine instruction.
1.4 Algorithms
Practical Bayesian inference requires us to compute the evidence E for the current
model. If E is outclassed by alternative models, there is no point in proceeding
further. Otherwise, and insofar as the model is reasonable, it is worth proceeding to
compute the posterior for more detailed inference about parameters and properties.
But the primary task is to calculate E:
E =

L(θ)π(θ) dθ.
(1.55)
The also-useful H = (
	
L(θ) log L(θ) π(θ) dθ−log E) / E can be accumulated in
parallel. Actually, the computer has less work to do if part of the likelihood can be
transferred analytically to the prior, so that E =
	
(L/w)(wπ) dθ for some useful
weighting function w. This trick is known as ‘importance sampling’, and can be
borne in mind.
Unfortunately, there is a general difﬁculty. In problems of even moderate com-
plexity, θ involves more than just a few parameters, and we are faced with a high-
dimensional integral. More fundamentally, if θ is encoded into ν bits of computer
memory, the evaluation of E involves a sum over 2ν possibilities. If storage ν is
considered as in class aleph-null, then possibilities 2ν are in class aleph-one. (Be-
yond that again is the domain of possible questions, in 22ν class aleph-two (Knuth
2005).)
We do not have aleph-one resources, so cannot evaluate E by deﬁnitive exhaus-
tive summation. In practice, we can only explore a limited number of points θ. This
means that the estimation of E is itself an inference problem, as opposed to clas-
sical numerical analysis. The result will be uncertain, and we ought to know what
that uncertainty is.
Moreover, the information H scales with the size of the problem, and is the log-
arithm of the prior-to-posterior compression ratio. In other words, the bulk of the
posterior occupies an exponentially small fraction O(e−H) of the prior domain.
That means it is hard to ﬁnd. It also means that, unable to do better, we are forced
towards random ‘Monte Carlo’ sampling methods to ﬁnd our points θ. There are
two basic approaches to this, nested sampling and simulated annealing, each with

1.4 Algorithms
21
strengths and weaknesses. Each has a kernel program that codes the general strat-
egy of evaluation, to which is attached an exploration program involving the like-
lihood function appropriate to a particular application. The kernels are really very
simple. Only the speciﬁc exploration code needs to reﬂect the complexity of an
awkward likelihood.
1.4.1 Nested sampling
Nested sampling (Skilling 2004b; Sivia & Skilling 2006) is a twenty-ﬁrst-century
algorithm, already being used in cosmology (Mukherjee, Parkinson & Liddle
2006).
Theory
We can think of ordering the 2ν states by decreasing likelihood value, accumulating
prior mass as we do so. As a technicality to ensure unambiguous ordering when L
values coincide, we can append a ‘key’ κ(θ) to the least signiﬁcant bits of the
computer’s coding of L, chosen so that key values don’t repeat and all L’s become
different. This ordering deﬁnes a strictly decreasing function
L(X) = ‘enclosing likelihood’ function of prior mass X,
(1.56)
with its strictly decreasing inverse (Figure 1.2)
X(L) =

L(θ)>L
π(θ) dθ = ‘enclosed prior’ of likelihood L.
(1.57)
Here your author adopts the computer-science practice of overloading function
names according to argument type; L(θ) and L(X) are technically different func-
tions, but the same likelihood.
Our task is to ﬁnd the value of what is now a one-dimensional sum along the
states, ordered-by-likelihood as they now are, over unit prior mass:
E =
 1
0
L(X) dX.
(1.58)
The integral for E is now very well behaved (Figure 1.2), with a decreasing positive
integrand L over unit range of X. But we only have resources for a limited number
of θ, from which we must infer the evidence and consequential properties. Let
these states be θ1, θ2, . . . , θn with Li = L(θi) and Xi = X(Li), and let them be
ordered as
0 < Xn < · · · < X2 < X1 < X0 = 1.
(1.59)

22
Foundations and algorithms
Area Z
Enclosed prior mass X
Enclosing likelihood L
0
X3
X2
X1
X0=1
L1
L2
L3
q1
q2
q3
Parameter space of q
Fig. 1.2. Nested likelihood contours sort to enclosed prior mass X.
In any interval, L is constrained by the end-points, Li+1 ≥L(X) ≥Li in
Xi+1 ≤X ≤Xi, so E is not far from its trapezoid-rule quadrature estimate
E =
n

i=1
Xi−1 −Xi+1
2
Li ,
(L0 = 0) .
(1.60)
Around this is a O(n−1) quadrature error, and what will become an exponentially
small O(Xn) truncation error. Progress is systematically inward within the nested
likelihood contours, hence the name nested sampling.
Kernel
If, in practice, we could assign X and compute its L(X), the evaluation of E would
reduce to the above elementary numerical analysis. But we cannot. What we can
do is assign X at random, as follows. Run the algorithm with N ‘particles’, where
N can be as low as 1 though larger N gives improved accuracy and power. Each
particle has known location θ, and known likelihood L(θ). At iterate i, suppose that
all N particles are uniformly distributed in X, though constrained within X < Xi,
equivalently L > Li. At the initial iterate i = 0, this just means that the N particles
are independent samples from the prior, with X0 = 1 being no constraint at all
because all the prior mass is accessible.
Let Li be the outermost (lowest) of the N likelihood values current at iterate
i −1. The N −1 other particles are uniformly distributed over the prior, except that
they are now known to have yet larger likelihood L > Li, equivalently X < Xi.

1.4 Algorithms
23
Iterate i –1
Iterate i
old
new
Fig. 1.3. One nested sampling iterate with N = 4 particles.
For iterate i, discard this outermost particle and replace it with a new one within
this same constraint L > Li.
pr(θ) ∝π(θ) in L(θ) > Li
(1.61)
(How to do this is discussed below.) We again have N particles uniformly dis-
tributed in X, but now they are constrained within X < Xi, equivalently L > Li,
and this completes an iterate (Figure 1.3).
Uncertainty
If we could actually do the θ →X ordering, we would know exactly what X was,
but we don’t. We are stuck with the partial knowledge that Xi was the highest of
N values, each uniformly distributed between 0 and Xi−1. This means that we can
claim no more than
Xi = tiXi−1 where pr(ti) = NtN−1
i
in 0 < ti < 1,
(1.62)
NtN−1 being the distribution of the highest of N random numbers less than 1.
Equivalently,
log Xi = log Xi−1 −τi/N where τ = −N log t has pr(τ) = e−τ in τ > 0.
(1.63)
However, we do know Li = L(θi) deﬁnitively.
The upshot is that we have a method which generates a correctly ordered
sequence of (L, X) pairs, with L known deﬁnitively and X known statistically.
This sufﬁces to infer E statistically. Take 100 or so independent guesses about
what the compression sequence of τ’s might have been by sampling each from

24
Foundations and algorithms
(1.63). We know no better. Each sequence deﬁnes its corresponding X’s, which
can be substituted in (1.60) to yield an estimate of E, preferably presented as
log E. These 100 estimates are samples from pr(log E), so they deﬁne the re-
quired result – conveniently summarized as mean and standard deviation. The re-
sult is uncertain, and we do know what that uncertainty is. Nested sampling is well
formulated.
As iterates proceed, the inward trend is geometrical. After r iterates, T = τ1 +
τ2 + · · ·+ τr = −N log Xr is distributed as pr(T) = T r−1e−T /(r −1)!; basically
T = r ± √r. The bulk of the posterior, where most of the evidence integral is
found, is near compression factor X = e−H, so it is reached after about NH ±
√
NH iterates, and usually left behind quite soon after. Even though the posterior
volume is small, it is reached in linear time because of the geometrical nature of
the algorithm. Hence nested sampling is feasible. The computational cost scales
as size-squared, one factor being due to the O(H) number of steps and the other
factor being the intrinsic cost of an iterate.
If the minor overhead of taking 100 samples of the τ’s is deemed too awkward, it
is often enough to set −N log Xr to its mean value r, thereby ﬁxing Xr = e−r/N,
and then evaluate a single central E from (1.60). This is subject to uncertainty
±

H/N through the ±
√
NH uncertainty in number of iterates, each of which
compresses by about 1/N in the logarithm.
Exploring the prior
Nested sampling requires a particle to explore randomly with respect to the prior,
within a hard constraint on likelihood value. Such exploration is easiest to program
if carried out in coordinates over which the prior is ﬂat. Your author goes further,
and places a d-parameter problem in the unit hypercube ρ ∈(0, 1)d within which
the prior is ﬂat, pr(ρ) = 1. To ensure that all sample points are a-priori-equivalent,
and to avoid possible singularity at the edges, the controlling random variables
can conveniently be 32-bit unsigned integers u ∈[0, 232 −1], with wraparound
topology. In terms of these, ρi = 2−32(u + 1
2), which stays far enough away from
0 and 1 for functions like logarithm to avoid singularity. With 4 billion allowed
values, the 32 bits give enough choice for practical purposes, and unsigned integers
are automatically wraparound in most computers.
The desired parameters θ are then appropriate functions of ρ. For example,
an intensity distributed as pr(θ) ∝exp(−θ/q) would be constructed as θ =
−q log ρ. A Cauchy distribution pr(θ) ∝1/(θ2 + w2) would be constructed as
θ = w tan((ρ −1
2)π), and so on. This ρ-to-θ approach is opposite to many presen-
tations, which start with θ and consider ρ, if at all, as the cumulant
	 θ pr(θ′) dθ′.
However, ρ-to-θ is more in keeping with the philosophy of deﬁning the prior by
placing typical θ on top of prior mass ρ, and it is easier to program too.

1.4 Algorithms
25
Xplore
(a)
Constrained Xplore
(b)
Fig. 1.4. Xplore explores the prior (a), constrained by likelihood (b).
Sampling from the prior alone is easy – for each coordinate, just call a random-
number generator for a 32-bit integer u, scale it to ρ and transform to θ. Moving
within the prior is also easy. To perturb a coordinate u within some limit k, gener-
ate a random integer between −k and k, and add it to u. If this is done for only one,
or a few, of the coordinates, this exploration strategy is ‘Gibbs sampling’ (Gelman
et al. 1995). If it is done for all coordinates, the exploration can take any direc-
tion. One of your author’s favourite tricks (Skilling 2004a) is to raster through
the d-dimensional 232-cube with a space-ﬁlling Hilbert curve, so that each point
is labelled by a 32d-bit integer in a way that substantially preserves neighbour-
hood relationships. Exploration is then 1-dimensional, with movement coded by
extended-precision addition and subtraction. Whatever strategy is adopted, we sup-
pose that we have an algorithm ‘Xplore’ that can explore the prior (Figure 1.4a).
What matters is that Xplore is symmetric. The probability of reaching v from
u should equal the probability of reaching u from v:
pr(v | u) = pr(u | v)
(1.64)
– a condition known as ‘detailed balance’. In the very long term ergodic limit,
this would ensure equal numbers of transitions between u and v when u and v are
equally populated. This uniform equilibrium state models the uniform prior that
we seek. Of course, we never wait that long, and our random-number generator
is likely to be a deﬁnitive algorithm, so discussion of ergodicity and equilibration
are really just metaphor. The rationale is that our predictive belief about the oc-
cupancies in our single system equilibrates in the same way that an ensemble of
very many systems would. Probability and relative frequency obey the same laws,
so we can use frequentist arguments as metaphor without believing in frequentism.

26
Foundations and algorithms
Regarding the random-number generator, we agree to be ignorant of its inner work-
ings, so that it is (to us) unpredictable except in terms of probability.
Exploring the constraint
Finding a point within nested sampling’s hard constraint on likelihood would be-
come very difﬁcult if carried out directly, because the constraint volume shrinks to
become exponentially small. In practice, we are forced to start with some location
obeying the constraint, and re-equilibrate it sufﬁciently to obtain the effectively
new point that we seek. Using past history is technically known as a Markov chain,
so our algorithms are ‘Markov chain Monte Carlo’ (MCMC).
At each iterate, there are N points already available, N −1 of them in the interior
plus the about-to-be-discarded outer point on the constraint boundary. Any of these
can be used as a source location to be re-equilibrated to the destination, though it
seems preferable (if N > 1) to start at one of the survivors within the strict interior.
So the ﬁrst step is to replace the discarded outer point with a copy of one of the
others.
The next step is to equilibrate the new point, which can be done with the same
Xplore algorithm as before – but with the restriction (Figure 1.4b) that any tran-
sition to a prohibited destination is rejected, leaving the source in place.
Accept θ if and only if L(θ) > Lconstraint
(1.65)
Transitions between interior points remain in detailed balance, transitions from
interior to exterior are blocked, and no transitions can start from the exterior, so
the scheme is fully balanced and equilibrates towards the constrained prior, as re-
quired. As always in MCMC, the step-size of a proposed perturbation has to be
large enough to make useful progress in ‘forgetting’ the source location, but not so
large that too many expensive transitions are rejected.
It is the user’s responsibility to acquire a new location, a task that can become
more difﬁcult if the enclosing likelihood has awkward shape in high dimension.
Note, though, that copying has the very useful side effect that any point becoming
stuck in a local false likelihood maximum will ﬁnd itself trapped as the likelihood
constraint becomes more severe, and when it becomes the outer point it gets dis-
carded. This means that false maxima can be left behind without need for Xplore
to propose an unlikely transition from false to favoured location.
Posterior
At the end of a nested-sampling run, it has accumulated a sequence of samples θ
with known likelihoods L(θ). Perhaps 100 guesses of the corresponding sequence
of X’s have been computed, each of which has given its evidence estimate (1.60)

1.4 Algorithms
27
and thereby tried to weight θi by wi = 1
2( 
Xi−1 −
Xi+1)Li/ E. The average wi of
these weights gives the best estimate of the posterior as a weighted sum
P(θ) =

wiδ(θ −θi)
(1.66)
of the samples. Already randomly selected around the likelihood contours, this list
gives the most faithful posterior distribution that is available. It can be used for
estimating any property Q(θ).
The effective number N of independent samples is given by the information
content of the weights through
log N = −

wi log wi
(1.67)
and is usually close to N ≈Ne. If this is too few to sample the posterior ade-
quately, then typical θ taken from the sequence can be used to seed fuller explo-
ration, using the Metropolis–Hastings balancing described next.
1.4.2 Simulated annealing
Theory
Nested sampling took a direct view of the enclosing likelihood function L(X).
The traditional method of simulated annealing (Kirkpatrick, Gelatt & Vecchi 1983)
takes an indirect view, introducing the likelihood gradually through a fractional
power β. At β, the annealed likelihood is Lβ, the evidence is E(β) =
	
Lβ dX,
and the posterior is LβdX / E(β). At the start, β = 0 switches the likelihood off,
leaving just the prior with E(0) = 1. At the ﬁnish, β = 1 gives the likelihood its
proper weight, with required evidence E = E(1). The term ‘simulated annealing’
marks a conscious analogy with thermal physics, with L = exp(−energy) and
β = 1/temperature. The identity
d log E
dβ
= dE/dβ
E
=
	
Lβ log L dX
	
Lβ dX
≡⟨log L⟩β
(1.68)
expresses the E differential in terms of the current log-likelihood average, and
integrates to the ‘thermodynamic integration’ formula
log E =
 1
0
⟨log L⟩β dβ.
(1.69)
Kernel
Simulated annealing evaluates E with an ordered sequence of coolness,
0 = β−1 = β0 ≤β1 ≤β2 ≤· · · ≤βn = βn+1 = · · · = 1,
(1.70)

28
Foundations and algorithms
Area log Z
1
0
< log L>
Estimate log Z
1
0
log L
Coolness b
<
Coolness b
<
Fig. 1.5. Thermodynamic integral (left) and trapezoid estimate (right).
known as the ‘annealing schedule’ and locates a particle at each station accord-
ing to
pr(θ) ∝Lβ(θ)π(θ)
(1.71)
Equivalently, Xi is taken from Lβi(X).
Either of the two quadrature forms,
log E−=
n

i=0
(βi+1 −βi) log Li ,
log E+ =
n

i=0
(βi −βi−1) log Li, (1.72)
could be used as an estimate of log E, with the central trapezoid estimate
log E = 1
2(log E−+ log E+) =
n

i=0
Δβi log Li ,
Δβi = βi+1 −βi−1
2
,
(1.73)
preferred. As the annealing schedule is made denser, these estimates plausibly con-
verge to the correct log E, as the sum approaches the integral and the statistical
variation of the terms averages out (Figure 1.5).
Uncertainty
The uncertainty is estimated by taking ξ = log X as the abscissa and observing
that, at coolness β, ξ is sampled from
pr(ξ) dξ ∝Lβ(X)dX = Lβ(X)X dξ = eβλ+ξ dξ,
(1.74)
where λ = log L. The maximum (if there is one!) occurs where dξ/dλ = −β.
Around this, second-order expansion of the exponent yields a Gaussian of variance

1.4 Algorithms
29
var(ξ) =

−d2(βλ + ξ)
dξ2
−1
=

−β d2λ
dξ2
−1
= β2 dλ
dβ ,
(1.75)
where the last expression is obtained by eliminating dξ. This is the variability to
be expected for a sample obtained at coolness β, which needs to be mirrored in the
uncertainty of our inference from that sample. Since dξ = −β dλ,
var(λ) = var(ξ)
β2
= dλ
dβ ≈Δλ
Δβ ,
(1.76)
and the variance of the estimate (1.73) of log E = (Δβ)λ becomes
var(log E) =

(Δβi)2var(λi) =

Δβi Δλi = log E+ −log E−.
(1.77)
So the uncertainty variance that we seek is very simply estimated by the difference
of the two quadrature forms (1.72). In Figure 1.5, this difference is shaded dark for
positive contributions, and light for negative contributions.
Schedule
For minimum uncertainty, the annealing schedule should be chosen so that the
terms in (1.77) balance, with Δβi ∝1/

var(λi). Performance of the algorithm
is only weakly dependent on the schedule details, and it sufﬁces to estimate the
variance from the spread of λ’s in recent iterates. As it happens, β2var(λ) = R/2,
where R is the locally effective dimensionality of the likelihood function – an exact
relation if L(θ) is Gaussian (aka rank-R multivariate normal) within a ﬂat prior,
and an informative diagnostic in general.
As with most statistical methods, accuracy is improved by √N by taking N
times more samples. Accordingly, the preferred annealing schedule is to cool at
each step by something like
Δβi =
1
N

var(λ)
,
(1.78)
in which the variance of λ derives from about 10N previous values. Here, the factor
10 guards against accidental or local coalescence of values leading to a wrongly
enhanced cooling step. Setting the schedule is something of a black art.
Exploration
Again, it is necessary to use the Markov chain idea and start from a known location
when seeking a new point. This is another black art, involving choice between the
most recent point and some previous point that might afford better escape from
a false likelihood maximum. Theory does not give ﬁrm guidance here. Once a

30
Foundations and algorithms
source has been selected, equilibration to the new destination can be performed
by modulating Xplore’s exploration of the prior according to the Metropolis–
Hastings criterion:
Accept θproposed if and only if Lβ(θproposed)
Lβ(θsource)
> t ∼Uniform(0, 1)
(1.79)
When a transition links two points u and v having likelihoods L(u) < L(v),
passage from less likely u to more likely v is unimpeded, whereas the reverse
is restricted by the average factor Lβ(u)/Lβ(v). At equilibrium, u is thus under-
populated relative to v, consistent with occupation density proportional to Lβ, as
required by simulated annealing.
free
Less likely u
−−−−−−−−−−−−−−−−−−→
←−−−−−
More likely v
restricted
Posterior
To explore the posterior after cooling to β = 1, all one need do is stop cooling by
holding β ﬁxed. The Xplore procedure modulated by Metropolis–Hastings will
allow a particle to diffuse faithfully to the posterior for as long as desired.
1.4.3 Comparison
The relative costs (measured by number of steps) and uncertainties of nested sam-
pling and simulated annealing, programmed using the same N for an R-dimensional
Gaussian (rank-R multivariate normal) likelihood, are roughly
#steps(nested)
#steps(anneal) ≈0.2
√
R ,
var log E(nested)
var log E(anneal) ≈2.0
√
R
(1.80)
for large R. At least for this simplest of applications, nested sampling is slower
and less accurate than simulated annealing. It is less accurate because its estimate
of X is subject to Brownian-motion accumulation of uncertainty, whereas sim-
ulated annealing’s schedule of β is known. It is slower because its particles are
constrained by the hard likelihood constraint to occupy a range Δ log X ∼1,
whereas simulated annealing’s soft constraint allows the particles to spread out
over Δ log X ∼√R (Figure 1.6).
On the other hand, nested sampling has the advantage of being more robust. It
systematically tracks inwards, generating a complete map of the likelihood func-
tion L(X). Nothing gets lost. Simulated annealing depends on tracking the maxi-
mum of β log L + log X as the inverse slope β increases. That requires log L to be

1.4 Algorithms
31
Nested sampling
log X
O(1)
Simulated annealing
log X
log L
log L
O(R1/2)
slope = –1/b
Fig. 1.6. N = 4 samples, in nested sampling and simulated annealing.
log X
log L
A
B
C
D
E
F
G
H
I
Nest
?
A
n
n
e
a
l
Fig. 1.7. Nested sampling and simulated annealing for a partly convex likelihood function.
a concave (⌢) function of log X. Many simple problems, such as the archetypical
Gaussian, are of this gratifyingly simple form.
Other problems, speciﬁcally those involving phase changes, are not of this form.
In fact, many of today’s difﬁcult problems do involve phase changes. Simulated
annealing is unable to deal with these. Consider the likelihood function plotted in
Figure 1.7. Nested sampling would start near A at log X = 0 and move systemati-
cally inwards along log X, plotting out the entire L(X) and enabling computation
of the evidence.
Simulated annealing would start the same, but move inwards by tracking the
tangent slope along ABC... At or before the point of inﬂection D, though, particles
will discover the unstable convex (⌣) region DEF in which the curvature is the
wrong way round. In seeking to increase their likelihood-volume β log L + log X,
the particles will crash inward towards HI. .. If that situation were recognized, and

32
Foundations and algorithms
log X
log X
log L
D
E
F
Water
log L
Steam
D
E
F
Fig. 1.8. Annealing is unable to distinguish very different problems!
if it were deemed to be overshoot, annealing could be reversed to track IHG...
However, the convex region remains unstable and at or before F particles would
crash back out to B or beyond. During a crash, the evidence value is lost, and with
it knowledge of the relative importance of the outer phase (steam?) near B and the
inner phase (water?) near H.
Relating the phases demands connecting them across the convex region, which
simulated annealing can only do if allowed exponential resources. For example,
Figure 1.8 shows likelihood functions for which the annealing integrands ⟨log L⟩β
– computed by inevitably local exploration – behave identically with coolness β.
The functions differ only in their inaccessible convex region DEF, but that differ-
ence sufﬁces to change log E enough to reverse the dominant phases. To put it
mildly, that could be important.
Simulated annealing could warn of impending catastrophe because the relevant
curvature,
−d 2 log L
d log X 2 = β2 var(λ) = R
2 ,
(1.81)
is already being tracked in order to ﬁx the cooling schedule. The warning signal
would be a dangerously small effective dimension R as the coolness lost control
of the likelihood. The estimated dimension could even be negative, because the
formula (1.77) does not have positive-deﬁnite form. But simulated annealing could
not avoid the catastrophe. Avoidance requires a more robust strategy, for which
nested sampling is available.
1.5 Concluding remarks
The foundations of Bayesian inference are solid. Recent work has deepened our un-
derstanding of the measure/information/probability nexus, without lending support

1.5 Concluding remarks
33
to any alternative fuzzy or quantum logics. Quite the reverse. The applications have
been widened, but not the calculus. The message is that generalizing the uniquely
correct is a bad idea.
Another message is that Bayesian analysis is prescriptive as to method, but not
as to content. Within a model, we are required to assign a prior probability distri-
bution before we can reach posterior conclusions, but that assignment is essentially
a matter of educated intelligence.
Nature judges models through our observations, which yield evidence values
allowing us to discriminate. This ‘evidence’ is the prime output of Bayesian infer-
ence, and precedes any evaluation of posterior distribution. However, it is not on
an absolute scale. There is no magic level at which a hypothetical model can be
‘rejected with 95% conﬁdence’ in old-fashioned ‘orthodox’ style. All we can do
is compare models (A and B, say) through their evidence ratios known as Bayes
factors,
BayesFactor(A, B) = EA
EB
.
(1.82)
We may feel that model A is unsatisfactory, but we are stuck with it until we can do
better. We cannot stand outside our current horizon, in some mythic state of holistic
totality. On the contrary, in problems of signiﬁcant size we often don’t manage to
ﬁt the data well enough to satisfy an orthodox χ2 signiﬁcance test. Some stray
effect that we haven’t thought about or bothered with often upsets the ﬁne detail of
a ﬁt, but that doesn’t make us reject the model. The point about a good model is
not that it explains all of a large dataset, but that it explains most of it better than
the alternatives. We get nothing more than that. The adult view of inference is that
we need nothing more.
In practice, unfortunately, the stray effect that we haven’t thought about can
corrupt the parameter values that we seek, so that we reach wrong values with
over-stated precision. In principle, we should take the effort to model the stray
effect. Often, though, we just allow ourselves to scale up the experimental noise
until the data are more-or-less satisﬁed by the model we are actually interested
in. The scale parameter is just another number to be determined in the analysis
and, unsurprisingly, that factor tends to make χ2 very acceptable. Real life often
demands such compromise with perfection.
Computing evidence values requires good algorithms, which nowadays usually
means some variant of Markov chain Monte Carlo. Before good algorithms were
available, Bayesian inference was desperately handicapped, but now we can do
it properly. Presented and compared here are two central methods: nested sam-
pling for evaluating E =
	 1
0 L dX and simulated annealing for evaluating log E =
	 1
0 ⟨log L⟩β dβ. Of the two, nested sampling is more direct and robust.

34
Foundations and algorithms
For easy problems, simulated annealing is likely to be somewhat quicker and
more accurate, although nested sampling can increase its accuracy by any factor
√N by using N times more resources. For difﬁcult problems, simulated annealing
can fail completely, unless given impractical exponential resources. So, if compu-
tational resources are not a constraint, nested sampling may be preferred. When
properly formulated, both methods yield estimates of the uncertainty that accom-
panies their Monte Carlo sampling, and arrive at
log E = estimate ± uncertainty,
(1.83)
which usually represents a nice single-humped probability distribution. Your au-
thor recommends, as a courtesy to future workers who may have alternative mod-
els, that presentations of Bayesian results should always be accompanied by this
sort of statement of the evidence, including the relevant inverse-data units.
The uncertainties matter. Suppose models A and B yield
loge
EA
(km/s)−200 = −1200 ± 30 ,
loge
EB
(km/s)−200 = −1260 ± 40 , (1.84)
when explaining 200 velocity measurements. Naive interpretation might indicate
that model A was preferred by the hugely convincing Bayes factor e60. Actually,
though, the difference of 60 ± 50 is only a 1.2-sigma effect, from which we prop-
erly conclude that
pr(A better than B) = 0.885 .
(1.85)
Yes, A is preferred, but only at an odds ratio of 8:1, not e60:1. There’s a 12% chance
that the algorithm’s random sampling led to the wrong conclusion, and that’s what
matters. More computation would be needed to gain the accuracy needed to settle
the issue. Even then, we are likely to be content with uncertainties considerably
above ±1 in the logarithm, corresponding to orders of magnitude of uncertainty in
E if we were foolish enough to try to undo the logarithm.
Parasitic upon the computation of log E is the generation of random samples of θ
which deﬁne the posterior P(θ) and any desired properties. With nested sampling,
the E computation itself yields a weighted sequence of θ ﬁt for purpose. With
simulated annealing, a separate exploration phase needs to follow the ‘burn-in’
cooling phase. Either way, the posterior is available. But, contrary to orthodox
presentation, the evidence comes ﬁrst.
References
Acz´el, J. (2003). In G. J. Erickson and Y. Zhai, eds., AIP Conf. Proc., 707, 195.
Cox, R. T. (1946). Am. J. Phys., 14, 1.
Cox, R. T. (1961). The Algebra of Probable Inference. Baltimore, MD: Johns Hopkins
Press.

1.5 Concluding remarks
35
Ferguson, T. S. (1973). Ann. Stat., 1, 209.
Gelman, A., Carlin, J. B., Stern, J. S. and Rubin, D. B. (1995). Bayesian Data Analysis.
London: Chapman and Hall, Chapter 11.
Kirkpatrick, S., Gelatt, C. D. and Vecchi, M. P. (1983). Science, 220, 671.
Klain, D. A. and Rota, G.-C. (1997). Introduction to Geometric Probability. Cambridge:
Cambridge University Press.
Knuth, K. H. (2003). In G. J. Erickson and Y. Zhai, eds., AIP Conf. Proc., 707, 204.
Knuth, K. H. (2005). Neurocomputing, 67, 245.
Knuth, K. H. and Skilling, J., in preparation.
MacKay, D. J. C. (2003). Information Theory, Inference, and Learning Algorithms.
Cambridge: Cambridge University Press.
Mukherjee, P., Parkinson, D. and Liddle, A. R. (2006). Astrophys. J. Lett., 638, L51.
Sivia, D. S. and Skilling, J. (2006). Data Analysis: A Bayesian Tutorial, 2nd edn. Oxford:
Oxford University Press, Chapter 9.
Skilling, J. (1988). In G. J. Erickson and C. R. Smith, eds., Maximum-Entropy and
Bayesian Methods in Science and Engineering. Dordrecht: Kluwer, p. 173.
Skilling, J. (2004a). In G. J. Erickson and Y. Zhai, eds., AIP Conf. Proc., 707, 381, 388.
Skilling, J. (2004b). In R. Fischer, R. Preuss and U. von Toussaint, eds., AIP Conf. Proc.,
735, 395.
Steutel, F. W. (1979). Scand. J. Stat., 6, 57.

2
Simple applications of Bayesian methods
D. S. Sivia and S. G. Rawlings
Having seen how the need for rational inference leads to the Bayesian approach
for data analysis, we illustrate its use with a couple of simpliﬁed cosmological
examples. While real problems require analytical approximations or Monte Carlo
computation for the sums to be evaluated, toy ones can be made simple enough to
be done with brute force. The latter are helpful for learning the basic principles of
Bayesian analysis, which can otherwise become confused with the details of the
practical algorithm used to implement them.
2.1 Introduction
In science, as in everyday life, we are constantly faced with the task of having to
draw inferences from incomplete and imperfect information. Laplace (1812, 1814),
perhaps more than anybody, developed probability theory as a tool for reasoning
quantitatively in such situations where arguments cannot be made with certainty;
in his view, it was ‘nothing but common sense reduced to calculation’. Although
this approach to probability theory lost favour soon after his death, giving way to
a frequency interpretation and the related birth of statistics (Jaynes 2003), it has
experienced a renaissance since the late twentieth century. This has been driven,
in practical terms, by the rapid evolution of computer hardware and the advent of
larger-scale problems. Theoretical progress has also been made with the discov-
ery of new rationales (Skilling 2010), but most scientists are drawn to Laplace’s
viewpoint instinctively.
In the past few years, several introductory texts have become available on the
Bayesian (or Laplacian) approach to data analysis written from the perspective
of the physical sciences (Sivia 1996; MacKay 2003; Gregory 2005). This chapter
aims to fulﬁll a similar tutorial need in cosmology. To set the scene, a brief orien-
tation on the pertinent astrophysics is given in Section 2.2. This is followed by two
36

2.2 Essentials of modern cosmology
37
examples, in Sections 2.3 and 2.4, chosen to illustrate some of the issues that might
be faced by a theorist and an experimentalist respectively, simpliﬁed to highlight
the principles underlying the analysis. The chapter concludes with a summary and
discussion of the salient points in Section 2.5.
2.2 Essentials of modern cosmology
On suitably large scales, and to some level of approximation, the Universe obeys
the cosmological principle of isotropy and homogeneity: it looks the same in every
direction, and would appear so to (hypothetical) fundamental observers (FOs) in
other galaxies. Together with its expansion, as measured by the Hubble parameter
H = ˙a/a, where a is the separation of two FOs, this implies a remarkable rela-
tionship between the local geometry of the Universe and the energy densities of its
contents. Adopting Einstein’s general relativity, this is encapsulated succinctly in
Friedmann’s equation, which describes the temporal, or redshift z, dependence of
H on all the Ω parameters deﬁned at the current epoch (z =0) and H0:
H2 = H2
0

Ωk(1+z)2+ ΩM(1+z)3+ ΩDE(1+z)3+3w+ ΩR(1+z)4
.
(2.1)
The local geometry is determined by Ωk =−k c2/H2
0a2
0, with k =−1 , 0 , 1 repre-
senting the hyperbolic, Euclidean and spherical cases respectively; the hyperbolic
and spherical geometries have a constant curvature, associated with the length a0,
and Ωk and k have opposite signs. The contents of the Universe are denoted by
a set of density parameters Ωj = 8π Gρj/3H2
0, where j = M , DE , R and G is
Newton’s gravitational constant. Matter, baryonic plus cold dark matter, has total
density ρM; the mysterious dark energy has an equation-of-state parameter (mod-
elled as a constant) w = pDE/ρDE c2, where pDE and ρDE are its pressure and
density respectively; and relativistic material, photons and hot dark matter, has to-
tal density ρR. The connection between the local geometry and the contents of the
Universe becomes clear when Eq. (2.1) is evaluated at z =0; namely,
Ωk + ΩM + ΩDE + ΩR = 1 .
(2.2)
2.2.1 Standard rulers and candles
The most straightforward way of probing the geometry and contents of the Universe
is with a ‘standard (cosmic) ruler’. Its ﬁxed size means that the apparent length ob-
served at various redshifts gives direct information on everything inﬂuencing the
separation of light rays as they propagate through an expanding and, perhaps, in-
herently curved Universe. If the standard ruler is of an intrinsic co-moving size

38
Simple applications of Bayesian methods
s, and oriented perpendicular to the line-of-sight, then its transverse angular size,
θobs, at a redshift of z is given by
θobs =
s
DM
,
(2.3)
in radians, where
DM =
⎧
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎩
c
H0
1
√Ωk sinh
√Ωk
H0DC
c

for k=−1, Ωk >0 ,
DC
for k=
0, Ωk =0 ,
c
H0
1
√
|Ωk| sin

|Ωk| H0DC
c

for k=+1, Ωk <0 ,
(2.4)
and DC is the integral of c/H with respect to redshift from 0 to z. Neglecting the
contribution from relativistic material, which is reasonable when dealing with the
Universe after recombination,
DC =
c
H0
z

0

Ωk(1+ζ)2+ ΩM(1+ζ)3+ ΩDE(1+ζ)3+3w−1/2
dζ .
(2.5)
Note that, regardless of the effects of the local geometry, any sources of energy den-
sity close to the (ruler’s) light rays curve space-time in accordance with Einstein’s
general relativity, so that Eqs. (2.3), (2.4) and (2.5) are only true along an average
path; detailed gravitational lensing calculations are needed for any speciﬁc line-of-
sight.
The quantity DC, the co-moving distance, represents the current radial separa-
tion between the observer and the standard ruler, assuming that neither has a sig-
niﬁcant ‘peculiar’ velocity. This can be thought of in two ways: either, the sum of
distances measured simultaneously by a chain of FOs between the observer and the
ruler at the epoch corresponding to z =0; or, the line integral of the distance trav-
elled by a photon moving from the ruler to the observer, where each element of the
integral is weighted by (1+z) to account for the cosmic expansion since the pho-
ton passed that point (FO) along its path. We see from Eq. (2.3) that a measurement
of θobs and a ‘knowledge’ of s together determine DM and hence, via Eqs. (2.4)
and (2.5), have the potential to constrain the cosmological parameters strongly.
The search for a suitable cosmic ruler, or something of ‘known’ physical length,
has been in progress since the birth of observational cosmology. It came to dra-
matic fruition around the year 2000, when cosmic microwave background (CMB)
experiments reached sufﬁcient sensitivity and ﬁdelity to map out the power in
the anisotropies of the CMB as a function of angular scale. A distinctive peak
in this distribution, at an angular scale just below 1 degree, and its harmonics at
smaller angles were seen, just as had been predicted by theoretical cosmologists.

2.2 Essentials of modern cosmology
39
These peaks reﬂect the so-called baryon acoustic oscillations (BAOs). The relevant
physics here is that when the Universe is young and hot, the photon–baryon ﬂuid
is prevented from freely falling into dark-matter potential wells (and down from
potential hills) by radiation pressure, resulting in acoustic oscillations, or sound
waves. These oscillations cease when the expansion of the Universe causes the tem-
perature to drop sufﬁciently for the electrons and atoms to combine, and thus for
the photons to be released, and ultimately be observed as the CMB, with physical
compressions and rarefactions corresponding to hot and cold spots on the CMB.
Since these processes occur around a known universal temperature (∼3000 K at
z ∼1100), and hence, for a particular choice of cosmological parameters, at an
equivalent age of the Universe (∼4 × 105 yr), and because the speed of sound in
a plasma is ﬁnite (∼
<c) then no compressions (or rarefactions) can form on phys-
ical scales larger than ∼130 kpc. Allowing for cosmic expansion from redshift
z ∼1100, this yields a standard ruler of co-moving size s ≈150 Mpc in the
form of the fundamental BAO, a scale that is also seen in the separation of the
harmonics.
The best current measurements of θobs from BAOs seen in projection in the CMB
are around 0.8◦(Hinshaw et al. 2007), whence Eq. (2.3) implies DM ∼11 Gpc.
With reasonable values of ΩM = 0.3 and H0 = 70 km s−1 Mpc−1, and Eqs. (2.4)
and (2.5), this rules out ΩDE = 0, as the implied curvature would then predict
θobs ≈0.4◦for s ≈150 Mpc. The CMB data suggest that the geometry is close to
spatially ﬂat (k=0) and ΩDE ≈0.7.
The BAO standard ruler has now also been detected in large galaxy redshift sur-
veys (Percival et al. 2007) at low redshift (z ∼0.3), showing that, on large scales,
galaxies trace the matter distribution seen at much earlier times in the CMB. The
details of this link comprise the complex subject of ‘galaxy bias’ (e.g., Dekel &
Lahav 1999). Such methods also probe the ruler scale both perpendicular and par-
allel to the line-of-sight, with the latter type of observation giving, for a ruler at
redshift z of length ΔDC/Δz, a direct measurement of c/H(z). The cosmological
principle requires identical results for rulers parallel and perpendicular to the line-
of-sight, the idea behind the cosmological test of Alcock and Paczynski (1979), and
which on relatively small scales yields corrections for ‘velocity–space distortion’ –
this effect is caused by the gravity-driven peculiar velocities of galaxies, and pro-
vides crucial independent information on galaxy bias.
Somewhat remarkably, these CMB- and galaxy-based experiments were not the
ﬁrst to provide evidence for dark energy. Ostriker and Steinhardt (1995) argued that
the preponderance of data (such as direct and indirect measurements of ΩM ∼
< 1;
the relatively low Hubble Constant; and the overall level of CMB anisotropy)
argued for a ﬂat Universe with non-zero cosmological constant. Jaffe (1996) applied
Bayesian model selection (Chapter 4) to a subset of this data and reached a similar

40
Simple applications of Bayesian methods
conclusion. Most importantly, astronomers studying distant type-Ia supernovae
(e.g., Perlmutter et al. 1999) had previously noted that they were too dim, or too far
away at a given z, if treated as good ‘standard candles’ in either a ﬂat ΩM =1 or a
curved ΩM =0.3 (ΩDE =0) cosmology, meaning that a ﬂat ΩM =0.3 (ΩDE =0.7)
cosmology is preferred. A standard candle assumes that the intrinsic luminosity of
an object is ‘known’, or in practice calibratable, and provides a measurement of the
luminosity distance DL = (1+z) DC.
It is important to appreciate the different nature of the constraint that supernovae
provide on the content and geometry of the Universe. Current supernovae standard
candle measurements typically probe the fairly local (z ∼
< 1) Universe where, for
w < −1/3, the same DC is obtained from positively correlated values of ΩM and
ΩDE (to see this either substitute Ωk = 1 −ΩM −ΩDE into Eq. (2.5), or note
equivalently that the time derivative of the Friedmann equation yields an expres-
sion in which dark energy provides acceleration, gravity provides deceleration and
the effects of curvature differentiate away). On the other hand, the CMB standard
ruler method probes such a high redshift that DC ≫c/H0 so, from Eq. (2.4), angu-
lar size measurements become dominated by curvature (and hence ΩM+ΩDE), and
the same DM results from negatively correlated values of ΩM and ΩDE. The com-
bination of the different constraints imposed on the parameters, such as ΩM and
ΩDE, by a variety of experiments is a good way of breaking degeneracies inherent
in a single type of measurement. Note that much of the decorrelating power comes
from observations at different redshifts because: (i) the (1+z) scalings of Eq. (2.1)
changes the mix of components with redshift; and (ii) Eq. (2.4) shows the strong
effects of curvature on photon paths originating at large distances (DC ≫c/H0).
Note also that at a given redshift there is no difference between a standard candle
method and a standard ruler method applied to a ruler parallel to the line-of-sight
as both probe DC and hence, via Eq. (2.5), H(z).
In practice, both the measurements and the inference related to the size and
luminosities of standard rulers and candles are fraught with difﬁculties. Even for a
‘clean’ BAO ruler, the physics determining how the length scale gets imprinted on
the CMB and in the galaxy distributions is sufﬁciently complicated that theoretical
cosmologists should always worry about subtle model dependencies in the absolute
ruler length. Experimental cosmologists must also measure angular wavelengths
of ‘oscillatory’ features against non-trivial backgrounds in the presence of noise
which may have its own preferred angular scales.
2.2.2 Motivation
The discussion of cosmology above is brief, but designed to motivate the simple
examples used in Sections 2.3 and 2.4; more comprehensive treatments can be

2.3 Theorists and pre-processed data
41
found in standard text books (e.g., Peacock 1999). Our intention was to choose
instructive illustrations that can be related to potentially fundamental experiments
over the coming decades.
Cosmologists expect that ‘standard ruler’ experiments will eventually provide
decoupled observations of the key parameters. For simplicity, we will set most
at ﬁducial values in Section 2.3 and focus on w and Ωk. Detection of w ̸= −1,
presumably then mappable as a time and space varying quantity, would indicate
that dark energy is not simply Einstein’s cosmological constant and would dictate
the need for new physics; there is already much speculation (e.g., Peebles & Ratra
2003). Similarly, Ωk ≫10−5 would imply much more power on super-Hubble-
length scales than is predicted by cosmic inﬂation, perhaps casting doubt on this
popular theory for the early Universe, or perhaps supporting fascinating anthropic
arguments about the origin of dark energy (Knox 2006).
It should be noted that modern cosmology features other fundamental cosmolog-
ical parameters, such as the amplitude and shape of the primordial power spectrum
of scalar perturbations and the ratio of tensor-to-scalar perturbations. Although
their measurement is crucial to a full understanding of our Universe, they will
not be considered further here, and we shall also largely ignore many critical
‘nuisance’ parameters, such as galaxy bias and the optical depth to the epoch of
recombination.
2.3 Theorists and pre-processed data
From the discussion of Section 2.2, and with reference to Eq. (2.1) in particular,
our theoretical model of cosmology is deﬁned by the estimates of the parameters
Ωk, ΩM, ΩDE, ΩR and w. In the Bayesian context, our state of knowledge about
their values is encapsulated in the conditional probability density function (PDF)
prob(Ωk, ΩM, ΩDE, ΩR, w|Data, I), where I represents all the information and
assumptions relevant to the problem other than the data at hand. Since this is a
ﬁve-dimensional entity, it is awkward to display graphically. To aid visualization,
let us focus on just two of the cosmological parameters, Ωk and w, and consider
prob(Ωk, w|Data, I). This PDF is related to the former through marginalization,
or an integration over the omitted parameters,
prob(Ωk, w|Data, I) =
(2.6)

prob(Ωk, ΩM, ΩDE, ΩR, w|Data, I) dΩM dΩDE dΩR,
and indicates what we can infer about Ωk and w when the uncertainties in ΩM,
ΩDE and ΩR are taken into account. To keep this example simple, let us avoid

42
Simple applications of Bayesian methods
any complications related to the evaluation of the triple integral by rewriting the
integrand as
prob(Ωk, w|ΩM, ΩDE, ΩR, Data, I) × prob(ΩM, ΩDE, ΩR|Data, I) ,
using the product rule of probability, and removing uncertainty about the values of
ΩM, ΩDE and ΩR with the assignment
prob(ΩM, ΩDE, ΩR|Data, I) = prob(ΩM, ΩDE, ΩR|I)
= δ(ΩM−0.3) δ(ΩR) δ(1−Ωk+ΩM+ΩDE+ΩR) .
Whereas the third δ-function follows from the theoretical constraint of Eq. (2.2),
the other two represent very strong assumptions imposed on this analysis through
the conditioning information I. The marginalization of Eq. (2.6) then yields
prob(Ωk, w|Data, I)
= prob(Ωk, w|ΩM = 0.3, ΩDE = 0.7 −Ωk, ΩR = 0, Data, I) ,
as expected, where ΩM and ΩR are ﬁxed at their ﬁducial values.
Suppose we learn that CMB observations indicate an angular size of θobs =0.8◦,
to within 1%, at z ∼1100 for a cosmic ruler of co-moving size s ≈150 Mpc; what
can we infer about the values of Ωk and w in the light of this datum? This is deﬁned
by prob(Ωk, w|Data, I), of course, which can be related to other PDFs through
Bayes’ theorem:
prob(Ωk, w|Data, I) = prob(Data|Ωk, w, I) × prob(Ωk, w|I)
prob(Data|I)
,
(2.7)
where ΩM = 0.3, ΩDE = 0.7−Ωk and ΩR = 0 have been subsumed into the gen-
eral conditioning symbol I to reduce algebraic cluttering. The various terms in
Bayes’ theorem are often given speciﬁc names. The left-hand side is the posterior
PDF that we seek, and encodes our state of knowledge about the values of Ωk and
w after the analysis of the current data. By contrast, prob(Ωk, w|I) is called the
prior PDF since it represents our ignorance (or knowledge) regarding them before a
consideration of the new measurements. The experimental results enter the analysis
through the likelihood function, prob(Data|Ωk, w, I), which quantiﬁes the signif-
icance of the mismatch between the data and their theoretically predicted values
from a given Ωk and w. The denominator, prob(Data|I), is an uninteresting nor-
malization constant from the perspective of an inference about Ωk and w given a
speciﬁc I,

2.3 Theorists and pre-processed data
43
prob(Data|I) =

prob(Data, Ωk, w|I) dΩk dw
(2.8)
=

prob(Data|Ωk, w, I) prob(Ωk, w|I) dΩk dw ,
which ensures that the integral of the posterior PDF with respect to Ωk and w in
Eq. (2.7) is unity, but it plays a central role when the relative merit of alternative
analysis assumptions needs to be assessed. For this reason, scientists sometimes
refer to it as the ‘evidence’ for a model; conventionally trained (Bayesian) statis-
ticians know it by a variety of other names, such as the prior predictive and the
marginal likelihood.
Although the nomenclature above is used widely, and intended to be helpful, it
can be misconstrued since people are led to regard the prior as being subjective,
whereas the likelihood is seen as objective. In reality, every term is just a condi-
tional PDF and, as such, on a par with each other. The power of Bayes’ theorem
comes from its ability to turn things around with respect to the conditioning state-
ment and, thereby, allow us to express the PDF of interest in terms of others that
we are more comfortable in assigning; indeed, the latter dictates how the rules of
probability are used in general. Since all probabilities quantify a state of knowledge
from the Bayesian point of view, and so are conditional on the information at hand,
the assignment of a PDF, whether it be a prior or a likelihood function, depends on
a careful consideration of the relevant contents of I (including assumptions).
So, what should we assign for prob(Ωk, w|I)? That depends on what is known
about Ωk and w prior to the analysis of the current data. If the answer is ‘not much’,
then this ought to be reﬂected with a very broad PDF; a sharply peaked one would
indicate that we already had a pretty good idea about the values of Ωk and w.
Since a PDF that is invariant over a suitably large range is both simple and broad,
we could legislate that gross ignorance be encoded through the assignment of a
uniform prior; subtle information about the nature of the parameters can then be
incorporated through a judicious choice of the exact hypothesis space in which the
calculation is done. For example, a uniform probability in x makes a slightly dif-
ferent assumption about this parameter than invariance with respect to log(x). The
former PDF, which remains unchanged if the origin is translated, is suitable if ab-
solute differences in x are deemed signiﬁcant; the latter, which implicitly assumes
that x is positive and is invariant under a change of units, is appropriate if relative
changes (Δx/x) are considered important. With this in mind, the deﬁnitions of Ωk
and w, and the roles they play in Eq. (2.1), suggest that it would be reasonable to
have a prior which was uniform in log(|Ωk|) and w; to be properly normalized,
however, ﬁnite ranges of validity also need to be speciﬁed. Thus,
prob(Ωk, w|I) = prob(Ωk|I) × prob(w|I) ,
(2.9)

44
Simple applications of Bayesian methods
which assumes logical independence in our knowledge of Ωk and w as far as I is
concerned, with
prob(Ωk|I) =
⎧
⎪
⎪
⎨
⎪
⎪
⎩
γ
for k=0, Ωk =0 ,
A (1−γ)
|Ωk|
for k=±1, α < |Ωk|<β ,
0
otherwise ,
(2.10)
where A = [ 2 ln(β/α) ]−1 and 1/ |Ωk| transforms to invariance with respect to
log(|Ωk|), and
prob(w|I) =

B
for wmin < w < wmax ,
0
otherwise ,
(2.11)
where B = (wmax−wmin)−1. The choice of the bounds, α, β, wmin and wmax,
and the probability γ that Ωk = 0, are dependent on I as well. Based only on the
three distinct possibilities for k, we might set γ = 1/3; α represents the smallest
value of |Ωk| considered to be signiﬁcantly different from zero, whereas β may be
estimated from the fact that a non-zero Ωk would have been obvious by now if its
magnitude had been large enough. Similar arguments are required for the step-size
to straddle w=−1.
What should we assign for the likelihood function, prob(Data|Ωk, w, I)? If the
measurement of the angular size of a standard ruler at redshift z yields the datum
D, which is ascribed a fractional uncertainty ϵ, then most people would be inclined
to say that
prob(D|Ωk, w, I) =
1
σ
√
2π exp

−

D −θ(z)
2
2 σ2

,
(2.12)
where σ = ϵD and θ(z) is the value of θobs at z predicted by Eqs. (2.3)–(2.5)
for the given Ωk and w ; a knowledge of s, H0, c and ϵ, as well as ΩM = 0.3,
ΩR = 0 and ΩDE = 0.7−Ωk, is assumed in I. Although the Gaussian PDF is
ubiquitous, it is important to remember that Eq. (2.12) is merely an attempt to
quantify the information inherent in I. According to Jaynes’ principle of maximum
entropy (MaxEnt), it is best regarded as the PDF which satisﬁes constraints on the
mean and variance of the mismatch between a measurement and its prediction,

D −θ(z)
 
= 0
and
!
D −θ(z)
2"
= ϵD ,
but is otherwise maximally non-committal about D (Jaynes 2003). Does the avail-
able datum correspond to expectations of this kind? Perhaps the statement of the
uncertainty in fractional terms should be interpreted as

2.3 Theorists and pre-processed data
45
#$ D −θ(z)
D
%2&
= ϵ ,
whence MaxEnt would deem a log-normal PDF (Sivia & Skilling 2006) more ap-
propriate? Such questions rarely have clear-cut answers. This indicates how the
perceived objectivity of the likelihood over a prior is largely an illusion, resulting
from familiarity, and it is really down to the analysis assumptions and background
information in both cases. A toy problem, known as ‘Peelle’s pertinent puzzle’, is
used to illustrate this point explicitly in Sivia (2006).
Having assigned the PDFs of Eqs. (2.9)–(2.12), the ingredients required for the
calculation of the posterior, and the evidence of the related model, are in place. Our
inference about Ωk and w is appreciated most easily through the computation and
graphical display of prob(Ωk, w|D, I) on a regular two-dimensional grid. The dig-
itization of the w-axis is straightforward: with N points spanning the prior range,
wj = wmin +
$wmax−wmin
N −1
%
(j−1) ,
(2.13)
for j =1, 2, . . . , N. To accommodate a uniform prior in log(|Ωk|) for k=±1, and
to handle the case of k =0 correctly, it is best to let the index i for the Ωk-axis run
from −M to +M:
(Ωk)i =
⎧
⎪
⎨
⎪
⎩
Υi
for k=−1, i=1, 2, . . . , M ,
0
for k= 0, i=0 ,
−Υi
for k=+1, i=−M, . . . , −2, −1 ,
(2.14)
where
Υi = α
β
α
|i|−1
M−1
.
(2.15)
Taking M and N to be sufﬁciently large that integrals with respect to Ωk and w
are approximated well by a simple summation over the discrete mesh, the prior of
Eqs. (2.9)–(2.11) is equivalent to
prob(i, j |I) =

γ
N
for i=0 ,
1−γ
2MN
otherwise .
(2.16)
In conjunction with the corresponding likelihood, it is computationally prudent to
evaluate the logarithmic sum
Lij = ln

prob(D|i, j, I)

+ ln

prob(i, j |I)

,
(2.17)

46
Simple applications of Bayesian methods
where prob(D|i, j, I) is the value of Eq. (2.12) when Ωk = (Ωk)i and w = wj on
the two-dimensional grid of points deﬁned by i and j. If the greatest value of Lij
is Lmax, then the posterior PDF and the evidence for the analysis model can be
ascertained safely through
prob(i, j |D, I) = eLij−Lmax
Z
,
(2.18)
where
Z =
N

j=1
M

i=−M
eLij−Lmax = prob(D|I) × e−Lmax.
(2.19)
Let us see a few examples.
In accordance with the simplifying assumptions made above, we will remove
uncertainty about s, H0, ΩM and ΩR in the illustrations by ﬁxing them at their
ﬁducial values (s=150 Mpc, H0 =70 km s−1 Mpc−1, ΩM =0.3 and ΩR =0) and
take them, and the relevant z, to be known exactly. Given the datum D = 0.629◦
to within 1% at a redshift of 1000, which is θobs from Eqs. (2.3)–(2.5) for Ωk = 0
and w=−1, the resultant posterior PDF is shown with a greyscale and contours in
Figure 2.1(a); the related prior had α =10−6, β =10−1, γ =1/3, wmin =−3 and
wmax =0, and the computation was done on a grid with M =N =200. While this
two-dimensional function captures all that can be inferred simultaneously about the
values of Ωk and w in the light of D and I, our principal interest may just be in one
of them. The associated information is conveyed through the marginal posteriors,
prob(w|D, I) ≡prob(j |D, I) =
M

i=−M
prob(i, j |D, I) ,
(2.20)
and the equivalent expression for prob(Ωk|D, I), which are plotted in Figure 2.1(c)
and (d). Also shown, with a dotted line, are the marginals corresponding to a frac-
tional error of 0.2%; to aid comparison, the PDFs have been scaled vertically to
have unit maxima. Although the ﬁve-fold improvement in the quality of the mea-
surement is reﬂected in the greater reliability of the estimate of w, there is no
signiﬁcant gain for Ωk! Indeed, the insensitivity of Ωk (between α and β) to the
datum (for either ϵ) can also be seen from the fact that the posterior probability of
a ﬂat universal geometry,
prob(k=0|D, I) =
N

j=1
prob(i=0, j |D, I) ,
(2.21)
changes imperceptibly from the prior value of a third. (We note that if we had
deﬁnite prior knowledge of w, for example, if we knew it to have the cosmological
constant value w = −1, we would be able to measure Ωk somewhat better, as
expected from the discussion of Section 2.2.)

2.3 Theorists and pre-processed data
47
Fig. 2.1. (a) The posterior PDF, prob(Ωk, w|D, I), of Eqs. (2.13)–(2.19) for D=0.629◦
at z = 1000 and ϵ = 1%; the darker regions indicate areas of higher probability, and the
vertical dashed line marks the discontinuity at k = 0. (b) The corresponding posterior for
a uniform prior in Ωk, between ±0.1. (c) The marginal posterior PDF for Ωk resulting
from (a), scaled to a maximum of unity (k̸=0); the dotted line shows the equivalent curve
for ϵ = 0.2%. (d) The same as (c) but for w, with an additional dashed line indicating the
marginal yielded by (b).
Posterior PDFs are frequently speciﬁed with the means, standard deviations
and covariances of their parameters. While this is often the only practical way
of summarizing the inference, its usefulness is limited when the posterior is not
approximated well by a multivariate Gaussian. It would paint an incomplete and
potentially misleading picture in the case of Figure 2.1, for example, even allowing
for the unusual δ-function at Ωk = 0 in (a), as non-linear correlations cannot be
captured with a covariance (or a Fisher) matrix.
If the preceding numerical experiment is repeated for a datum corresponding to
low redshift, D = 2.60◦at z = 1 with ϵ = 1%, then the resultant posterior PDF for
Ωk and w will be as shown in Figure 2.2. The fractional error on a measurement
of this type is currently closer to 10%, so that prob(Ωk, w|D, I) would be about
ten times more spread out in practice. The marginal posterior PDF for w is also
plotted in Figure 2.2, with the case of ϵ = 10% indicated with a dotted line; to aid

48
Simple applications of Bayesian methods
Fig. 2.2. (a) The posterior PDF for Ωk and w, with D=2.60◦at z =1 and ϵ=1%. (b) The
marginal for w, with the dotted line being for ϵ=10%.
comparison, both prob(w|D, I) have been scaled vertically to have unit maxima.
The marginal posterior for Ωk has not been displayed since it is very broad and
shows little change from the prior.
What would be gained inferentially if both high- and low-redshift data were
available? We can assess this by combining the two measurements that have just
been considered:
Data ≡
'
D1 =0.629◦(z1 =1000, ϵ1) , D2 =2.60◦(z2 =1, ϵ2)
(
.
The analysis proceeds as before, except that the previous datum D is replaced by
the vector Data, with the likelihood function
prob(Data|Ωk, w, I) =
ND

q=1
prob(Dq|Ωk, w, I) ,
(2.22)
where q =1, 2, . . . , ND and prob(Dq|Ωk, w, I) is given by Eq. (2.12) with z =zq,
D = Dq and σ = ϵq Dq; ND = 2 in our case. The decomposition of the likelihood
into a product of the probabilities for the individual data assumes independence,
implicit in I, where the noise in one measurement is not correlated with that of
any other. The resultant posterior PDF, prob(Ωk, w|Data, I), with ϵ1 = ϵ2 = 1%
is shown in Figure 2.3(a). It resembles Figure 2.2(a) more than Figure 2.1(a), be-
cause D2 imposes a much tighter constraint on w; this can also be seen for the
corresponding marginals for w in Figures 2.1(d), 2.2(b) and 2.3(b). The two data
contain complementary information on Ωk for |Ωk| ∼
> 10−3, which gives rise to a
rounded fall off at the extremities. With ϵ2 = 10%, Figure 2.3(a) becomes almost
identical to Figure 2.1(a). Incidentally, the posterior probability of a ﬂat universal
geometry rises from the prior value of 1/3 to 39.3% with ϵ1 = ϵ2 = 1% but only
increases to 35.3% when ϵ2 =10%.
The number of unknowns in this example was intentionally kept to just two,
to aid visualization and understanding, by imposing very strong constraints in the

2.4 Experimentalists and raw measurements
49
Fig. 2.3. (a) The resultant posterior PDF, prob(Ωk, w|Data, I), when the two measure-
ments that yield Figs. 2.1(a) and 2.2(a) separately are used together. (b) The corresponding
marginal for w (ϵ1 = ϵ2 = 1%), scaled to a maximum of unity, with the case of ϵ2 = 10%
shown with a dotted line.
conditioning information. Uncertainties with regard to s, z, H0, ΩM, and so on,
need to be taken into account through marginalization, as in Eq. (2.6), with the
assignment of appropriate PDFs to quantify our limited knowledge of their values:
prob(Data|Ωk, w, I) =

· · ·

prob(Data, w, s, . . .|Ωk, w, I) ds. . .,
where the integrand can be decomposed into a product of the previous likelihood
function, prob(Data|Ωk, w, s, . . . , I), where s, . . . are given (formerly subsumed
in I), and the prior PDF, prob(s, . . .|I), which will not have δ-function character
in general. The marginalization results in a broadening of the posterior for Ωk and
w. It rapidly becomes impractical to deal with such problems through an explicit
computation of the probabilities on a regular grid in the space of many parameters,
and so we are forced to turn to analytical approximations and Monte Carlo methods
to carry out the calculations; the latter are, therefore, the focus of much of this
volume.
2.4 Experimentalists and raw measurements
The previous section was intended to be a simple illustration of data analysis from
the perspective of a theoretical cosmologist. In reality, θobs is not an actual mea-
surement but the distillation of the pertinent information contained in raw observa-
tions. We now model this situation in an elementary way to highlight some of the
issues involved in the intermediate step. The example is designed to bear a pass-
ing resemblance to problems associated with measuring BAOs in the galaxy power
spectrum, where the gravitationally-driven non-linear growth of structures boosts
the power preferentially on small scales; the noise can be quite a complicated func-
tion of scale, with the precise details depending on the speciﬁc experiment. Indeed,

50
Simple applications of Bayesian methods
in practice the angle θobs is not measured directly at all, but rather the cosmolog-
ical parameters’ impact on the galaxy power spectrum or correlation function is
calculated, which is in turn compared to the observed data in a similar fashion to
the case for the CMB power spectrum discussed in various contexts elsewhere in
this volume.
For simplicity, suppose that the data are a cosine function of 1/θ, of unit ampli-
tude and zero phase, superimposed upon a quadratic background:
F(θ) = cos
2πθobs
θ

+ b
θ2 ,
(2.23)
where the wavelength, θ −1
obs, is of interest but there is one nuisance parameter, b.
Given a set of measurements, {d (θl)} for l = 1, 2, . . . , Nd , such as those in
Figure 2.4(a), what can we say about θobs?
This type of problem is usually addressed by a least-squares ﬁt of the model to
the measurements, whereby the best estimates of θobs and b are taken to be the
values which minimize the quadratic mismatch statistic
χ2 =
Nd

l=1

F(θl) −d (θl)
2 .
(2.24)
The procedure can be interpreted in Bayesian terms as representing the maximum
of the posterior PDF for θobs and b with a uniform prior (in θobs and b), and a
likelihood function which assumes that every datum is subject to independent and
additive Gaussian noise of the same standard deviation, σ:
prob({d (θl)}|θobs, b, σ, I) =

σ
√
2π
−Nd exp
$
−χ2
2 σ2
%
,
(2.25)
which basically follows from Eqs. (2.12) and (2.22). Although σ has no impact
on the optimal θobs and b, its value does affect our assessment of their reliability.
Without a good estimate of the noise level, σ has to be marginalized out just like
any other nuisance parameter:
prob({d (θl)}|θobs, b, I) =

prob({d (θl)}|θobs, b, σ, I) prob(σ|I) dσ
≈

2πe/Nd
−Nd/2
ln(σmax/σmin)

χ2−Nd/2 ,
(2.26)
where the prior, prob(σ|I), has been taken to be uniform with respect to log(σ),
over a large but ﬁnite range, and the integral simpliﬁed under the assumption that
Nd ≫1. The marginal posterior PDF for θobs that results from Eqs. (2.23), (2.24)
and (2.26), given the data of Figure 2.4(a) and a uniform prior for θobs and b, is
shown in Figure 2.4(b).

2.4 Experimentalists and raw measurements
51
Fig. 2.4. (a) Measurements pertaining to Eq. (2.23), with Gaussian noise of uniform
level σ. (b) The corresponding marginal posterior PDF for θobs, with a uniform prior and
a likelihood given by Eqs. (2.23), (2.24) and (2.26).
How should the inference of θobs from the observations {d (θl)} be propagated
to the analysis of the preceding section? Well, our state of knowledge about Ωk
and w given the ‘real’ data is encapsulated in the PDF prob(Ωk, w|{d (θl)} , I).
This was equated with prob(Ωk, w|D, I), as per Figure 2.1, on the understanding
that D (and ϵ) contained the same information about θobs as {d (θl)}. For exam-
ple, θobs = 0.997◦± 0.005◦constitutes a useful summary of the inference from
the data of Figure 2.4(a) because the posterior of Figure 2.4(b) is described well
by a Gaussian. The formal link can made through Bayes’ theorem of Eq. (2.7),
with Data interpreted as {d (θl)}, by using marginalization and the product rule of
probability to express the likelihood function as
prob({d (θl)}|Ωk, w, I) =

prob({d (θl)}|θobs, I) prob(θobs|Ωk, w, I) dθobs,
where the conditioning on Ωk and w has been dropped from the PDF in which θobs
is given (without uncertainty), since it is multiplied by another that is non-zero only
when they are consistent:
prob(θobs|Ωk, w, I) = δ(Θ(Ωk, w)−θobs) ,
where Θ(Ωk, w) is the value of θobs predicted by Eqs. (2.3)–(2.5) for the given Ωk
and w; a knowledge of z, s, and so on, is assumed in I. The integration over the
δ-functions conﬁrms the intuitive result that the relevant likelihood in the inference
of Ωk and w is prob({d (θl)}|Θ(Ωk, w), I).
The data in Figure 2.4(a) were analyzed on the basis that they were subject to
independent and additive Gaussian noise of constant magnitude. This was known
to be the case since they were generated in a computer under those conditions
(with σ = 0.25); reassuringly, the angular wavelength recovered was in line with

52
Simple applications of Bayesian methods
Fig. 2.5. (a) Measurements with Gaussian noise and a few spurious zeros. (b) Marginal
posterior PDFs, scaled to unit maxima, corresponding to noise models h1 (dotted), h2
(dashed) and h3 (solid), explained in the text.
the θobs =1 used for the simulation. In real life this would only be an assumption,
not least because part of the expected mismatch between the measurements and
predictions would stem from an inadequacy of the functional model used to ﬁt the
structure in the data, such as Eq. (2.23), and we must never forget that the inference
is conditional upon it.
Consider the measurements in Figure 2.5(a), where each datum d (θl) is accom-
panied by an estimate of its error bar, σl. Although the previous cosine function
and quadratic background are visible, so too are glitches where the signal drops to
zero occasionally. If the data were analyzed blindly using least squares, with the
deﬁnition of Eq. (2.24) generalized to the sum of the squares of the normalized
residuals, χ2 = R2
l where
Rl = F(θl) −d (θl)
σl
,
(2.27)
then the resultant marginal posterior PDF, prob(θobs|{d (θl)} , h1, I) where h1
explicitly denotes the corresponding noise hypothesis, would be as shown by the
dotted line in Figure 2.5(b). It is not very surprising that the simulation value of
θobs = 1 lies well outside the ‘credible’ region, since h1 makes no allowance for
‘spurious measurements’. While the validity of h1 would be questionable due to
the least-squares ﬁt to the data being too poor ( χ2
min ≫Nd), it is down to us to
come up with a better alternative.
A common course of action involves the postulate, h2, that all the stated error-
bars ought to be scaled by an unknown positive constant, σ, so that σl →σ σl;
it can be regarded as a ‘safety valve’ designed to compensate crudely for unac-
counted sources of uncertainty. A marginalization over σ leads to the likelihood
function of Eq. (2.26), with χ2 generalized as before and ln(σmax/σmin) multi-
plied by σ1 σ2 . . . σNd, and gives rise to the dashed posterior PDF in Figure 2.5(b),

2.4 Experimentalists and raw measurements
53
prob(θobs|{d (θl)} , h2, I). Even though the ‘optimal’ estimate of θobs remains un-
changed, the posterior conditional on h2 is sufﬁciently broad to encompass the
simulation value within the realm of plausibility.
Noise models that intrinsically offer protection against erratic data have likeli-
hood functions which fall off much more slowly in the tails, when Rl ≫1, than a
Gaussian. For example,
prob({d (θl)}|{σl} , θobs, b, h3, I) =
Nd

l=1
1
σl
√
2π
)
1 −e−R2
l /2
R2
l
*
,
(2.28)
derived in Chapter 8 of Sivia & Skilling (2006), which assumes that the σl repre-
sent only the lower bounds for the (Gaussian) error bars to be associated with the
respective measurements. The posterior PDF for θobs conditional on h3 is shown
with a solid line in Figure 2.5(b). Since the data of Figure 2.5(a) were generated
in a computer, we know that, of the three noise models considered, h3 yields the
inference which is most faithful to the underlying reality. Could this have been
surmised without being privy to the simulation program?
The support for a proposition, be it a model for the object of interest or the
noise properties of the related data, is quantiﬁed by its posterior probability. In
the context of h1, h2 and h3 above, we need to evaluate prob(hr|{d (θl)} , I) for
r = 1, 2, 3. Using Bayes’ theorem, this can be expressed in terms of the prior and
evidence probabilities for hr,
prob(hr|{d (θl)} , I) = prob({d (θl)}|hr, I) × prob(hr|I)
prob({d (θl)}|I)
,
(2.29)
where, for a total of NR noise models deemed reasonable given I,
prob({d (θl)}|I) =
NR

r=1
prob({d (θl)}|hr, I) prob(hr|I) .
(2.30)
Although we might assign prob(hr|I)=1/NR in the absence of cogent informa-
tion, only a few hr possibilities are considered in practice. This means that most
of the evidence values needed for the normalization factor of Eq. (2.30) are not
available, and even NR is usually left unspeciﬁed. We can still ascertain the rela-
tive merit of alternative hr possibilities, however, as the denominator of Eq. (2.29)
cancels out when calculating
prob(hr|{d (θl)} , I)
prob(hr′|{d (θl)} , I) = prob(hr|I)
prob(hr′|I) × prob({d (θl)}|hr, I)
prob({d (θl)}|hr′, I) ,
so that the posterior odds are equal to the ratio of the evidences for r and r′ to within
the prior odds. While an evidence value is meaningless in isolation, it is crucial for
a comparison with alternatives. For the case of h1, h2 and h3 in Figure 2.5(b), the

54
Simple applications of Bayesian methods
Fig. 2.6. (a) Noisy measurements that are free from spurious glitches. (b) Marginal poste-
rior PDFs, scaled to unit maxima, corresponding to noise models h1 (dotted), h2 (dashed)
and h3 (solid), explained in the text.
natural logarithm of their evidences are −333.0, −64.5 (σmin = 0.01 and σmax =
100) and −45.2 respectively; for equal priors, this makes h3 favoured by a factor
of 108 over h2 and 10125 with respect to h1.
If the previous analysis is repeated for the data of Figure 2.6(a), which do not
suffer from spurious zeros, the corresponding (logarithmic) evidence values for h1,
h2 and h3 are −8.4, −10.6 and −24.1. As shown in Figure 2.6(b), the posterior
PDFs conditional on h1 and h2 are virtually identical; the probabilistic evidence
prefers the former by a small margin (101) since the latter has an extra degree of
freedom (σ ̸= 1) that is not warranted by the data. The posterior for θobs subject
to h3 is about twice as broad as the other two, which is not surprising given its
pessimistic stance on the error bars; the evidence now weighs against it by a factor
of 107 relative to h1.
Incidentally, the evidence returns a non-committal verdict on the alternative
choice of priors for Ωk in Figure 2.1. Logarithmic values of 5.9 for (a) and 5.6
for (b) give a posterior ratio of essentially unity based on the single datum.
2.5 Concluding remarks
The deﬁning feature of the Bayesian approach to data analysis is not the use of
Bayes’ theorem or priors per se, but the interpretation of probability as something
that quantiﬁes a state of knowledge (or ignorance). The corollary of this viewpoint
is that all analyses are understood to be conditional on the information, simpliﬁca-
tions and assumptions that go into them. The loss of absoluteness may be uncom-
fortable, but it is honest.
A Bayesian analysis can seem troublesome, as even the simplest case involves
awkward questions before a calculation can be performed: How is the hypothesis

2.5 Concluding remarks
55
to be formulated? What are the alternatives? What is known a priori? How are the
modelling and measurement uncertainties characterized? . . . No wonder a conven-
tional least-squares procedure, with its associated measure of goodness-of-ﬁt, is
so appealing! On reﬂection, however, the apparent objectivity of orthodox meth-
ods starts to look illusory and the difﬁcult issues appear relevant. For the data
of Figure 2.5(a), for example, a much higher than expected value of χ2 warns
us that something is awry; but what? Is the description of Eq. (2.23) inadequate,
or is the implicit assumption of Eq. (2.25) inappropriate? Without trying speciﬁc
alternatives, it is hard to tell. An acceptance that the problem might lie with the
noise model raises some interesting points: (i) the likelihood function is never
really ‘known’; (ii) the introduction of noise uncertainties, such as those leading
to Eqs. (2.26) and (2.28), makes goodness-of-ﬁt tests impossible in general; (iii)
since the hypothesis and data appear on the wrong sides of the probability con-
ditioning symbol for an inference, the use of the likelihood cannot be justiﬁed
logically without acknowledging the role of the prior.
As Steve Gull once remarked, data analysis is simply a dialogue with the data.
The awkward theoretical issues that come to the fore when trying to implement
the Bayesian approach are akin to us asking a question only to hear the response,
‘Well, that depends on what you mean by ...’, requiring qualiﬁcation. Likewise,
on receiving an unsatisfactory answer we might be moved to ask another question
by saying, ‘No, what I meant to ask was ...’ It is a dynamic process, just like a
conversation.
In the 1980s, a straw poll of working cosmologists would probably have re-
vealed a strong belief in the then ‘preferred’ cosmological parameters: ΩM = 1,
ΩDE = 0 and Ωk = 0. Approaching 2010, we ﬁnd ourselves driven, largely by ob-
servational evidence and statistical inference, towards a radically new consensus:
ΩM ∼0.3, ΩDE ∼0.7, w∼−1 and |Ωk|∼10−5. Bayesian methods of analysis will
undoubtedly play a major role during the next change in beliefs, potentially aiding
a paradigm shift in the way we view the Universe.
References
Alcock, C. and Paczynski B. (1979). Nature, 281, 358.
Dekel, A. and Lahav, O. (1999). Astrophys. J., 520, 24.
Gregory, P. (2005). Bayesian Logical Data Analysis for the Physical Sciences. Cambridge:
Cambridge University Press.
Hinshaw, G. et al. (2007). Astrophys. J. Supp., 170, 288.
Jaffe, A. J. (1996). Astrophys. J., 471, 24.
Jaynes, E. T. (2003). In G. L. Bretthorst, ed., Probability Theory: The Logic of Science.
Cambridge: Cambridge University Press.
Knox, L. (2006). Phys. Rev. D, 73, 023503.
de Laplace, P. S. (1812). Th´eorie Analytique des Probabilit´es. Paris: Courcier Imprimeur.

56
Simple applications of Bayesian methods
de Laplace, P. S. (1814). Essai Philosophique sur les Probabilit´es. Paris: Courcier
Imprimeur.
MacKay, D. J. C. (2003). Information Theory, Inference, and Learning Algorithms.
Cambridge: Cambridge University Press.
Ostriker, J. P. and Steinhardt, P. J. (1995). Nature, 377, 600.
Peacock, J. A. (1999). Cosmological Physics. Cambridge: Cambridge University Press.
Peebles, P. J. E. and Ratra, B. (2003). Rev. Mod. Phys., 75, 559.
Percival, W. J. et al. (2007). Mon. Not. Roy. Astron. Soc., 381, 1053.
Perlmutter, S. et al. (1999). Astrophys. J., 517, 565.
Sivia, D. S. and Skilling, J. (2006). Data Analysis: A Bayesian Tutorial. Oxford: Oxford
University Press.
Sivia, D. S. (2006). In P. Ciarlini et al., eds., Advanced Mathematical and Computational
Tools in Metrology VII. Singapore: World Scientiﬁc Press, p. 108.
Skilling, J. (2010). Chapter 1 in this volume.

3
Parameter estimation using Monte Carlo sampling
Antony Lewis and Sarah Bridle
In this chapter we assume that you have a method for calculating the likelihood
of some data from a parameterized model. Using some prior on the parameters,
Bayes’ theorem then gives the probability of the parameters given the data and
model. A common goal in cosmology is then to ﬁnd estimates of the parameters
and their error bars. This is relatively simple when the number of parameters is
small, but when there are more than about ﬁve parameters it is often useful to use a
sampling method. Therefore in this chapter we focus mainly on ﬁnding parameter
uncertainties using Monte Carlo methods.
3.1 Why do sampling?
We suppose that you have (i) some data, d, (ii) a model, M, (iii) a set θ of unknown
parameters of the model, and (iv) a method for calculating the probability of these
parameters from the data P(θ|d, M). For convenience we mostly shall leave the
dependence on d and M implicit, and thus write P(θ) = P(θ|d, M).
An example we will consider throughout this chapter is the estimation of cosmo-
logical parameters from cosmic microwave background (CMB) data. For example,
you could consider that (i) the data is the CMB power spectrum, (ii) the cosmolog-
ical model is a Big Bang inﬂation model with cold dark matter and a cosmological
constant, and (iii) the unknown parameters are cosmological parameters such as
the matter density and expansion rate of the Universe. We discuss methods for
obtaining the probability of the parameters from this data in Section 3.2.
From the posterior probability distribution we can answer statistical questions
about functions of the parameters, P(f(θ)|P(θ)). For example, the expected value
of a function of parameters f(θ) is given exactly by
⟨f(θ)⟩=

dθ f(θ)P(θ).
(3.1)
57

58
Parameter estimation using Monte Carlo sampling
In the simplest case, we could use this to ﬁnd the expectation value of a cosmolog-
ical parameter f(θ) = θ = Ωm, conditional on all the other cosmological param-
eters being known; this would require calculating the one-dimensional probability
distribution P(Ωm) as a function of Ωm and performing the one-dimensional in-
tegration above. In the more realistic case where we do not know the values of
the other cosmological parameters, then the posterior must be evaluated in the full
multidimensional parameter space to perform the above integral for the expecta-
tion value of Ωm. This full posterior probability distribution could also be used
to obtain the expectation value of other parameters which are not included in the
original parameter set, such as Ωmh, by carrying out the above integral.
In principle, this integral can be calculated numerically to machine precision (or
to the precision with which we can calculate P(θ)). However, if there are n param-
eters then the posterior is an n-dimensional scalar-valued function. The simplest
computational method would sum over an evenly spaced grid in parameter space.
This grid needs to cover a large enough volume of parameter space to capture all
the regions in which f(θ)P(θ) is signiﬁcantly non-zero, with some width wi in
each direction. The grid resolution will depend on the smoothness of the function
and probability distribution, with some resolution Δi required in each dimension.
Therefore the number of grid points is +n
1(wi/Δi) ∼(w1/Δ1)n, where the latter
applies if all dimensions contain a similar amount of structure. Note the exponential
scaling with the number of dimensions, which makes direct integration numerically
prohibitive in large dimensions.
Instead we can try to radically compress P(θ) into a small manageable collection
of numbers by sampling. The probability of taking a sample from a given position
in parameter space θ is proportional to the probability P(θ) at that position in
parameter space. A set of ns samples {θi} speciﬁes ns positions in parameter space
and therefore consists of n × ns numbers. The number density of samples should
then be proportional to the probability distribution itself.
From the set of samples {θi} we can then try to infer properties of the full distri-
bution. So instead of directly calculating P(f(θ)|P(θ)) we now instead calculate
P(f(θ)|{θi}). For general properties of f(θ) this is difﬁcult; however, if we are
primarily interested in a set of expectation values the central limit theorem comes
to the rescue. As long as the distribution of f(θi) has ﬁnite variance, the central
limit theorem states that the distribution over different sets of samples of any sum
of numbers 
i f(θi) will tend to a normal distribution for large numbers of inde-
pendent samples. For example, from {θi} we can estimate ⟨f(θ)⟩using
ˆEf ≡1
ns
ns

i=1
f(θi).
(3.2)

3.2 How do I get the samples?
59
The expected value of the ˆEf estimator is just the true expectation value, ⟨ˆEf⟩=
⟨f(θ)⟩. For large numbers of samples the central limit theorem tells us that P( ˆEf)
tends to the normal distribution N(⟨f(θ)⟩, σ2
E), where σE = σf/√ns and σ2
f is the
true variance of f(θ). This is true even if the distribution of f(θ) is not Gaussian,
so we can reliably infer expectation values from samples even for general distribu-
tions.
Compressing the posterior distribution to a set of samples is very convenient
when calculating low-dimensional statistical properties of the full distribution, such
as the posterior expectation value of parameters or some function of them. Expecta-
tion values can be calculated very quickly from a ﬁxed number ns ≫1 of samples
independent of the dimensionality of θ. This is useful since often the time to gen-
erate a ﬁxed number of independent samples has a low scaling with dimension.
Sampling therefore potentially saves an exponentially large fraction of the time
when computing expectation values compared to integrating the posterior over the
full n-dimensional parameter space.
3.2 How do I get the samples?
Compressing the posterior into samples is a useful trick. But given some distribu-
tion P(θ), how do we actually generate the samples?
3.2.1 Direct sampling methods
We ﬁrst describe two sampling methods that work well in low-dimensional param-
eter spaces: ‘inverse transform sampling’ and ‘rejection sampling’. Both methods
are ‘direct’, in the sense that they generate exactly independent samples, rather than
more complicated methods that often draw independent samples only asymptoti-
cally.
‘Inverse transform sampling’ is generally applicable for one-dimensional prob-
ability distributions P(θ) and is sometimes more efﬁcient than rejection sampling,
because all computed samples are used. We consider the simplest case, which uses
the fact that it is easy to sample uniformly from values of the cumulative distribu-
tion Q(θ), a monotonic function ranging from zero to one,
Q(θ) =
 θ
−∞
dθ′P(θ′).
(3.3)
Given a sample xi from the uniform distribution between zero and one, we can
convert it into a sample θi from P(θ) by ﬁnding θi for which Q(θi) = xi.
This method may be inefﬁcient, especially if Q(θ) or its inverse cannot be com-
puted analytically. As a simple example that works well, consider sampling from

60
Parameter estimation using Monte Carlo sampling
the normalized positive exponential distribution P(θ) = e−θ/2/2 for 0 ≤θ < ∞,
which has a cumulative distribution Q(θ) = 1 −e−θ/2: generate uniform sam-
ples xi (with 0 ≤xi < 1) and then compute the sample parameter value θi =
−2 ln(1 −xi).
‘Rejection sampling’ is sometimes more efﬁcient if a suitable ‘umbrella’ or
‘envelope’ distribution U(θ) can be found that is easy to sample from, where
U(θ) ≥P(θ) for all points in parameter space θ (where U can be scaled so that this
is true and hence is not normalized). Samples from U will occur with a density that
differs from P by a factor ∝U(θ)/P(θ). Rejection sampling works by generating
a sample θ from U(θ), and keeping it with probability P(θ)/U(θ) or rejecting it
otherwise. Non-rejected samples will then be independent samples from P(θ).
A simple example is sampling uniformly within the unit circle. Given a standard
random number generator for the interval [−1, +1], we can easily generate a point
(x, y) that lies within an enclosing square, −1 ≤x ≤1, −1 ≤y ≤1 (the umbrella
distribution). To generate a sample from within the unit circle we then simply keep
the point if r2 = x2 + y2 < 1, otherwise we reject it and try generating another
point. The method will generate a valid point a fraction π/4 of the time, so rejection
sampling is a fairly efﬁcient sampling method in this case.
Alternatively, we may wish to sample from a probability distribution P(θ) which
is a function of a single parameter θ, and normalized to have a maximum value of
unity, for example P(Ωm) = exp(−0.5(Ωm −0.258)2/0.032). It is helpful to
deﬁne a range of our parameter, e.g., 0 < Ωm < 0.5. We could then deﬁne the
umbrella distribution U to be unity within the allowed parameter range and zero
outside. Samples from U are obtained by drawing a trial sample θt, which is simply
a uniform random number from the parameter range. We now keep the trial sample
at a rate proportional to P(θt). This can be achieved by generating another random
number for each trial sample, which is uniform in the range zero to unity. The trial
sample is kept if the random number is less than P(θt); otherwise it is removed
from the ﬁnal list.
An efﬁcient way to sample from a Gaussian distribution is to use a combination
of rejection sampling and inverse transform sampling, the Box–Muller algorithm.
The idea is to draw a sample uniformly from within the unit circle (xci, yci) and
then scale it to produce a sample from a 2-D Gaussian distribution (xGi, yGi).
Samples from the unit circle have a uniform distribution in polar angle φci, but a
probability distribution in the radial rci direction P(rci) ∝rci. Therefore Rci ≡
r2
ci is uniformly distributed between zero and one, following the rules for map-
ping probability distributions when changing variables. A point from a 2-D Gaus-
sian distribution is also uniformly distributed in angle φGi but has P(rGi, φGi) ∝
rGie−r2
Gi/2, and therefore P(RGi) ∝e−RGi/2. A sample from the unit circle can
be transformed into a sample from a 2-D Gaussian using the inverse transform

3.2 How do I get the samples?
61
sampling result we derived earlier, RGi = −2 ln(Rci) (where Rci = 1 −xi since
0 < Rci ≤1 and 0 ≤xi < 1). The new components are thus (xGi, yGi) =
(xci, yci)
,
−2 ln(r2
ci)/rci. Each component is an independent sample from a 1-D
unit Gaussian distribution, giving two independent Gaussian random numbers of
unit variance.
Samples from Gaussian distributions are often useful in cosmological applica-
tions, and a general n-dimensional distribution can easily be sampled from, if its
covariance is known, by using the Box–Muller algorithm to sample in each inde-
pendent eigenvector. More efﬁcient methods for generating Gaussian variates, such
as the Ziggurat algorithm,1 are also based on rejection sampling. These methods
essentially cover the distribution in a series of rectangular boxes (that are easy to
sample from, and edges can be computed analytically), so that the probability of
rejection is very low. For a comparison of methods for sampling from a Gaussian
distribution, see Thomas et al. (2007).
3.2.2 Problems with large dimensions
Rejection sampling and inverse transform sampling often work well in low dimen-
sions. However, as the number of dimensions increases we have to be careful. As
an example consider trying to sample uniformly from within a unit n-sphere, gen-
eralizing the method described before for sampling uniformly within a unit circle.
Rejection sampling will certainly work in principle: we just generate n uniform
numbers xi ∈[−1, +1] and test whether r2 = n
i=1 x2
i < 1. The problem comes
when we calculate the acceptance probability, given by the ratio of the volume of
a unit n-sphere to the volume of an n-cube of size 2. The cube has volume 2n, and
the volume of the sphere is
Vn =

dΩn
 ∞
0
dr rn−1 = 1
n

dΩn.
(3.4)
The surface area of a unit n-sphere
	
dΩn can be calculated readily from known
properties of Gaussian distributions:

dnθ e−θ2/2
(2π)n/2 = (2π)−n/2

dΩn
 ∞
0
dr rn−1e−r2/2 = 1
(3.5)
=⇒

dΩn = 2πn/2
Γ(n/2),
(3.6)
so that
Vn =
2πn/2
nΓ(n/2)
=⇒
Vn
2n =
√π
2
n
1
Γ(1 + n/2).
(3.7)
1 See, e.g., http://en.wikipedia.org/wiki/Ziggurat algorithm.

62
Parameter estimation using Monte Carlo sampling
As n becomes large this ratio becomes exponentially small: for n = 2 the accep-
tance probability is ﬁne at π/4, but for n = 10 it is only ∼0.025 and for n = 100
it is a prohibitive ∼10−70. Clearly rejection sampling from an n-cube is not a good
way to sample uniformly from a unit n-sphere when n is large. In this case we
can use the symmetries of the distribution to ﬁnd a better method. For example,
we could generate a vector θ in a random direction by generating n Gaussian ran-
dom numbers, generate a unit vector θ/|θ|, then re-scale so that P(r) ∝rn−1 for
r ≤0 < 1. This alternative method only scales linearly with the number of di-
mensions, but it is only possible because of the spherical symmetry of the target
distribution.
For a general distribution it will not normally be possible to ﬁnd a fast direct sam-
pling method. Rejection sampling in high dimensions either requires an umbrella
distribution that is very close to the true distribution, or it requires an exponentially
large number of tries, and is therefore not usually possible. The problem is that in
high dimensions most of the probability tends to lie in an exponentially tiny frac-
tion of the space; if the umbrella distribution is a factor of w broader than the target
in all dimensions, the relevant volume ratio that determines the acceptance proba-
bility is ∼1/wn. For this reason, sampling methods for high-dimensional problems
usually use some kind of local sampling method, setting up a random walk through
parameter space so that only regions with fairly high probability are explored.
3.2.3 Markov chain sampling
The most common methods for sampling from general distributions in high dimen-
sions are based on Markov chains. The idea is to construct a rule for choosing a
sequence of points in parameter space such that after a long time the probability of
the current position being θi is proportional to P(θi). If the rule for moving from
θi to θi+1 depends only on θi (and not on the previous history), then the sequence
of points is called a Markov chain. Some of these points may later be considered
‘samples’ if certain conditions are met (see below).
We can deﬁne a transition probability T(θi, θi+1) that determines the probability
of the chain moving to θi+1 given that it is currently at position θi. In order for the
chain to have the correct asymptotic distribution, the probability of arriving at a
given point must be equal to the probability at that point,
P(θj) =

dθiP(θi)T(θi, θj).
(3.8)
It can be seen by substitution that one way to ensure this is to use a (normalized)
transition probability satisfying detailed balance
P(θi+1)T(θi+1, θi) = P(θi)T(θi, θi+1),
(3.9)

3.2 How do I get the samples?
63
so that ‘the probability of being here and going there is the same as the proba-
bility of being there and coming here’. There are numerous ways to choose T so
that detailed balance holds, so we shall discuss only some of the most popular.
The only general constraint on T is that every point in the parameter space must
be accessible within a ﬁnite number of transitions. Excellent references available
online include MacKay (2003) and Neal (1993), and there are also several books:
Gilks, Richardson and Spiegelhalter (1996), Liu (2001), Gelman et al. (2003), and
Gamerman and Lopes (2006).
It important to emphasize that although asymptotically the position at any given
time follows the correct distribution, P(θi+1) and P(θi) are certainly not indepen-
dent. Running a chain until it equilibrates and then stopping generally gives one
independent sample from the distribution. To generate more than one independent
sample it would then be necessary to wait for the chain to move about parameter
space until it has lost all memory of the previous independent sample. The efﬁ-
ciency of the method depends on how long this wait is; i.e., how efﬁciently the
chain moves about parameter space.
3.2.4 Metropolis–Hastings algorithm
The Metropolis–Hastings algorithm (Metropolis et al. 1953; Hastings 1970) is one
of the simplest and most popular ways to set up a Markov chain satisfying detailed
balance. It uses an arbitrary proposal density distribution q(θi, θi+1) to propose
a new point θi+1 given the chain is currently at θi. We can choose a proposal
density that is easy to sample from. The proposed new point is then accepted with
probability
α(θi, θi+1) = min

1, P(θi+1)q(θi+1, θi)
P(θi)q(θi, θi+1)

.
(3.10)
The total transition probability is therefore
T(θi, θi+1) = α(θi, θi+1)q(θi, θi+1).
(3.11)
It is straightforward to show by substitution that this satisﬁes the detailed balance
condition. Note that when the proposal is rejected, θi+1 = θi, so there are two (or
more) samples at exactly the same position. Equivalently these can be combined
into a ‘weighted sample’ with weight two (see the later section on importance
sampling). Of course these samples are not independent.
Often it is convenient to choose a symmetric proposal density, q(θi+1, θi) =
q(θi, θi+1), in which case the proposal density cancels out of the acceptance prob-
ability. This is the original Metropolis algorithm, which says ‘move to the new
position if the probability there is higher than at the current position, otherwise

64
Parameter estimation using Monte Carlo sampling
0
100
200
300
400
500
600
700
800
900
1000
chain step
log likelihood
Fig. 3.1. The log-likelihood value of the ﬁrst 1000 points in a Markov chain sampling a
typical CMB posterior. The likelihood grows rapidly initially until the chain equilibrates
around good parameter values. Horizontal lines correspond to more than one sample at
exactly the same parameter values following rejection of a proposed move.
move to the new position with a probability equal to the ratio of the probabilities at
the new and current positions’. Following convention, we will refer to Metropolis
and Metropolis–Hastings as Metropolis methods hereafter. The likelihood values
at the start of a typical chain run are shown in Figure 3.1.
Note that it is important that the proposal distribution q is only a function of
the current position, otherwise the chain is not Markov; in particular, q cannot be
chosen based on details of the past history of the chain. However, during an initial
‘burn-in’ phase where samples are not kept, it is not necessary for the chain to be
Markov as long as it is Markov when samples are being collected. It is therefore
often useful to use the early movements of the chain to reﬁne the proposal density.
After this burn-in phase the proposal density must be ﬁxed, or it must at least only
be updated in such a way that it asymptotically converges to a ﬁxed distribution.
At best, the time to produce a ﬁxed number of independent samples using
Metropolis sampling methods can scale linearly with the number of dimensions.
For example, to sample from an n-dimensional Gaussian P(θ) ∝e−θ2/2 we could
propose a change to each component of θ using a Gaussian of width 1/√n. The

3.2 How do I get the samples?
65
acceptance probability is then O(1) (in fact, a width of ∼2.4/√n is optimal in
this particular case, with acceptance rate ∼0.234 (Gelman, Roberts & Gilks 1996;
Roberts, Gelman & Gilks 1997)). In each component, a random walk with step-size
1/√n will take ∼n steps to cover the unit width of the distribution. So we can ex-
pect to generate an independent sample about every O(n) steps of the chain. If this
is the case, Metropolis sampling to compress the posterior into a set of samples
can be very efﬁcient for moderate numbers of dimensions. In realistic situations
the scaling will be worse, both because a priori we don’t know the best proposal
distribution, and because the distribution may have a more complicated shape.
The choice of proposal density can have a large effect on how the algorithm
performs in practice. In general, it is best to have a proposal density that is of sim-
ilar shape to the posterior, since this ensures that large changes are proposed to
parameters along the degeneracy directions. Fortunately, with cosmological data,
we have a reasonable idea of what the posterior might look like, and so choosing
a sensible proposal density is often not difﬁcult. However, it is a limitation of the
method that it is not at all robust to bad choices of proposal density: if the distri-
bution is too narrow it takes a long time to random walk around the space; if it
is too broad then the acceptance probability becomes very low. For the method to
perform well, the acceptance probability should be around unity. In many cases
a good choice of proposal density gives an acceptance rate in the range 0.2−0.3
(Gelman, Roberts & Gilks 1996; Roberts, Gelman & Gilks 1997; Dunkley et al.
2005).
One common way to choose the proposal density is to make a preliminary short
chain with some simple proposal, calculate the covariance matrix from the samples
generated, and then use the (possibly scaled) covariance matrix for a Gaussian pro-
posal distribution in a new chain. This procedure can be iterated until the sampling
seems to be working efﬁciently (see Section 3.3). Samples from the preliminary
runs are discarded. Alternatively, when new data become available, there are often
old data that can be used to generate quite a good initial proposal covariance. In
addition, there is considerable freedom in how the covariance is used; for exam-
ple, using a Gaussian proposal may not be a good idea if there is a danger that
the covariance is inaccurate. Also proposing changes in all directions at once (e.g.,
using an n-D Gaussian) may not be as good as making a series of proposals in
lower dimensions. For further discussion of these issues see, e.g., Lewis and Bridle
(unpublished).
As mentioned above, the method works poorly if the proposal density does not
allow the region of high likelihood to be explored efﬁciently. In particular, if two
parameters are strongly correlated, proposals which vary a single parameter may
almost always be rejected, even though a large change to some combination of
the parameters would be allowed by exploration along the degeneracy direction.

66
Parameter estimation using Monte Carlo sampling
Using the covariance matrix for a Gaussian proposal distribution allows linear
degeneracies to be explored efﬁciently. However, if the distribution is banana-
shaped (or worse), exploration can still be very slow. For this reason, when tight
degeneracies are known in advance, it is a very good idea to change variables so
that the parameters are as independent as possible.
In the context of cosmology this is a common situation; for example, we know
that the CMB data constrains the angular diameter distance to the last scattering
surface very well, but that various non-linear combinations of curvature, dark en-
ergy and Hubble parameters can all give the same distance. Posterior constraints
on these parameters therefore tend to be tightly correlated. By changing variables
to the well-constrained combination (i.e., the angular diameter distance) and other
combinations, a much more weakly correlated set of parameters can be obtained,
greatly improving the convergence properties of the Markov chain (Kosowsky,
Milosavljevic & Jimenez 2002). Note that when changing variables the prior should
also be modiﬁed by the corresponding Jacobian, though in practice people often
simply re-deﬁne their priors to be ﬂat in the decorrelated variables.
3.2.5 Other sampling methods
One distinct disadvantage of the Metropolis method is that the efﬁciency depends
strongly on the choice of the proposal density. A poor choice can mean that the
algorithm is hopelessly slow. If we don’t know very much about the target distri-
bution, it might therefore be nice to have a sampling method that is more robust,
being able to explore the full width of the distribution efﬁciently, even if an initial
guess is very wrong. One general method is called slice sampling (Neal 2003); this
uses a ‘stepping-out’ and ‘stepping in’ process to automatically adjust the proposal
density width at each step. There is however some cost to doing this, and for this
reason the method has so far been little used in cosmological parameter estima-
tion, where often we have a very good idea about the distribution and Metropolis
methods can be made to work efﬁciently.
One particularly simple choice for the Metropolis–Hastings proposal density is
to use the conditional distributions, assuming they can easily be sampled from;
e.g., propose changes to x by drawing from P(x|y) and propose changes to y
by drawing from P(y|x). By substituting into the acceptance probability we see
that these proposals are always accepted. This sampling method that alternately
samples from the conditional distributions is called Gibbs sampling (Geman &
Geman 1984). It can be very efﬁcient if x and y are not too correlated and work
even in very high dimensions. However, if the parameters are strongly correlated it
can take many Gibbs steps to random walk in a diagonal direction by taking many
steps parallel to the axes.

3.2 How do I get the samples?
67
One of the main problems of the Metropolis method is that the exploration of
highly correlated directions can be quadratically slow due to the random walk be-
haviour. If possible, it would be much better to use a partly directed walk, so that
the chain could traverse the distribution much more quickly. One such method is
called Hamiltonian sampling, or the hybrid Monte Carlo algorithm. This method
is useful when derivatives of the likelihood can be calculated easily, and works in
analogy to classical Newtonian mechanics by giving the chain position a ‘momen-
tum’ vector, so that the current state has some ‘memory’ of the direction that it
came from encoded in this momentum. For a detailed discussion of the method
and scaling with dimension see, e.g., Neal (1993). The method can be applied both
when the derivatives can be calculated analytically, and when the derivatives can
be calculated approximately; see, e.g., Hajian (2007), and Taylor, Ashdown and
Hobson (2007) for potential applications in the cosmological context.
3.2.6 Thermodynamic and ﬂat-histogram methods
As discussed in this book’s opening chapter, many sampling methods are primar-
ily solutions to the question of how to calculate the probability marginalized over
all parameters within a model (evidence). This is particularly true in solid state
physics where the equivalent quantity is the free energy of the system, a quan-
tity of physical importance. There is a substantial literature giving sophisticated
sampling methods to calculate the free energy in a variety of different problems.
Chapter 1 brieﬂy described nested sampling and thermodynamic integration. An-
other class of methods are the ‘ﬂat-histogram’ methods (e.g., multi-canonical (Berg
2000; Gubernatis & Hatano 2000) and Wang–Landau (Wang & Landau 2001a &
2001b) sampling), which aim to generate approximately equal numbers of samples
for each log-range in likelihood (in the solid state context, equal numbers of sam-
ples at each energy). Like nested sampling and thermodynamic integration, these
methods generate samples far into the tails of the distribution and probe a much
wider range of likelihood values than direct Metropolis sampling (which generates
samples only where the likelihood is high). They are therefore potentially of use
both for parameter estimation, and answering questions about the tails of the dis-
tribution that may not be easy to answer accurately from a set of samples directly
from the posterior, such as high-signiﬁcance conﬁdence limits.
If the target distribution is multi-modal, sampling methods that probe more of
the distribution are much more likely to ﬁnd the multiple modes than purely lo-
cal methods making a random walk in the posterior. When asking questions about
the extreme tails of the distribution, or sampling complicated multi-modal or awk-
wardly shaped distributions, using samples from an evidence-estimation method
can therefore be a good idea, even if the evidence value is not actually required.

68
Parameter estimation using Monte Carlo sampling
A detailed discussion is beyond the scope of this chapter, but most of the methods
can produce weighted samples from which parameters of interest can be calculated
straightforwardly (see the importance sampling section below and Chapter 1).
3.2.7 Baby and toy
As a ﬁnal example of a clever sampling algorithm, we discuss brieﬂy one method
for sampling even when the likelihood cannot be calculated easily at a single point.
We assume the likelihood distribution can be factored as L(θ|d) = f(d, θ)/Z(θ)
into a part f(d, θ) that depends on the data d and parameters θ which is easy
to calculate, and some normalizing factor Z(θ) that is very hard to compute. In
addition, it must be possible to simulate fake data y from L; this has a probability
L(θ|y). There is also a prior Pr(θ) that must be applied to calculate the posterior
probability from which we wish to sample. For example, the likelihood may be
calculable to within a normalizing constant from a predicted mean vector dm(θ)
and a covariance matrix C as f(d, θ) = exp(−(d−dm(θ))TC−1(d−dm(θ))/2)
and Z(θ) is a normalization that depends on hard-to-compute determinants.
The baby and toy algorithm (Murray, Ghahramani & MacKay 2006) may be
executed as follows. Generate some typical fake data y from the model at θi+1 and
calculate f(θi, y)/f(θi+1, y), which is a one-shot importance sampling estimator
for the hard-to-compute Z(θi)/Z(θi+1) ratio (see Eq. (3.16) below). Then ask:
Does the real data prefer θi+1 relative to θi more than the typical fake data prefers
θi+1 relative to θi? If it does, i.e., f(d, θi+1)/f(d, θi) > f(y, θi+1)/f(y, θi), then
θi+1 is likely to be preferred and the move is looking promising, pending taking
into account priors and transition ratios. We therefore use a Metropolis–Hastings
method, where the acceptance ratio is
α(y, θi, θi+1) = min

1, f(d, θi+1)Pr(θi+1)q(θi+1, θi)
f(d, θi)Pr(θi)q(θi, θi+1)
f(y, θi)
f(y, θi+1)

.
(3.12)
The total transition probability is then
T(θi, θi+1) =

dyf(y, θi+1)
Z(θi+1) α(y, θi, θi+1)q(θi, θi+1).
(3.13)
We can easily check that detailed balance is satisﬁed for the target distribution
Pr(θ)f(d, θ)/Z(θ), so the sampling method is valid. The sampling method is some-
times called ‘baby and toy’ or the ‘single variable exchange algorithm’. General-
izations of the algorithm could use a more reﬁned estimator of the Z(θi)/Z(θi+1)
ratio.

3.3 Have I taken enough samples yet?
69
3.3 Have I taken enough samples yet?
A general problem with Markov chain based sampling methods is that the points
in the chain are correlated. Only asymptotically is one position of the chain guar-
anteed to be an independent sample from the target distribution. For example, near
the beginning of a chain there will generally be a steady progression of the chain
likelihoods towards higher values as the chain walks towards the high-likelihood
region. How do we know when it has got there and, once there, how long does it
take to generate another independent sample?
Unfortunately there is no general sufﬁcient test for whether a set of samples are
independent. In particular, if the distribution is multi-modal it is quite possible for
the chain to walk directly towards one mode and then appear to be producing inde-
pendent samples, when in fact the other modes are not being explored at all (i.e.,
we have no independent samples). Having said that, it is quite easy to write down
a string of tests that are (on average over realizations) necessary for the samples to
be independent. We just outline a few here, and you should remember that they are
only necessary but not sufﬁcient; for this reason it is often a good idea to look at
more than one test.
Often the easiest way to tell if a chain is producing samples from the correct
distribution is to produce multiple chains and compare results. Any difference be-
tween chains gives a measure of the statistical error in the quantity being estimated.
If we are interested in some function f(θ), we can estimate f from each chain and
calculate its variance. For the estimates of f from a single chain to be reliable we
want the rms between the chains to be much smaller than the posterior uncertainty
on f.
The ‘Gelman and Rubin’ ratio statistic R = (variance between chains)/(mean
variance within the chains) is often quoted as a measure of convergence, where
we want R −1 ∼0 for the Monte Carlo estimators potentially to be reliable
(Gelman & Rubin 1992; Gelman, Roberts & Gilks 1996). R is most often quoted
for the means of the parameters, but it may also be used for any quantity of interest
(for example, higher conﬁdence limits). If there are many variables, the between-
chain covariance can be diagonalized and R quoted for the worst eigenvector, al-
lowing the statistic to show up poor convergence in correlated directions. Since
chains will take a while to ‘burn in’ as they converge on the high-likelihood region,
the Gelman–Rubin statistic is usually quoted for the last halves of the chains. If
R −1 ∼0, quantities estimated from the samples might then be reliable if the ﬁrst
half of each chain is discarded.
Convergence can also be assessed from a single chain. One way is simply to
split the chain up and then apply a Gelman and Rubin test to each chunk. We can
also calculate the correlations between chain parameters as a function of distance

70
Parameter estimation using Monte Carlo sampling
along the chain. This allows an estimate of the correlation length, the distance rc
at which the correlation falls to 1/e. A high value for the correlation length for
one parameter indicates that the chain is not exploring that direction efﬁciently.
The correlation length can also be used to give an upper limit to the number of
independent samples we might have, nstep/rc. Clearly, for reliable results we need
nstep ≫rc for all the parameters.
A similar idea is used by the Raftery and Lewis (1992) convergence test for per-
centile limits. This calculates roughly how much the chain must be thinned in order
for occurrences of parameter values above a given percentile to have a Markovian
or independent distribution. The method gives an estimate of the distance between
independent samples, similar to the correlation length.
3.4 What do I do with the samples?
We now discuss how to calculate some quantities of interest from your samples,
and how to combine your original dataset with other information without having to
resample. We continue with the example of parameter estimation from the WMAP
CMB data. You can download sets of samples for various cosmological models
yourself.2
3.4.1 Parameter constraints
The samples give you a full multidimensional map of your probability distribution,
since the number density of samples is proportional to the probability. However, it
is difﬁcult to display this in more than three dimensions. The expectation values
of parameters can be calculated easily, as discussed in Section 3.1. Equation (3.2)
shows that the desired quantity can be calculated for each sample, and the average
taken. In the simplest case, we are interested in the expectation value of one of our
parameters in θ. In this case we just take the mean of the sample positions for that
parameter, and can ignore all the other parameters.
Traditional probability analyses often start by ﬁnding the best-ﬁt point in
parameter space. However such high-dimensional statistical properties cannot be
calculated accurately from the samples.3 If the probability value has been stored
with each sample position, then the sample with the highest probability can be
identiﬁed. There may be very few samples close to the best-ﬁt point, and these
will be scattered along the directions of degeneracy. Therefore, due to the random
2 http://lambda.gsfc.nasa.gov/product/map/current/parameters.cfm.
3 The maximum of an n-dimensional unit Gaussian is at the origin, but the region r < ϵ ≪1 only contains a
fraction ∝ϵn of the probability – in high dimensions there will almost certainly be no samples within r < 1
of the origin, where r is the radius from the origin.

3.4 What do I do with the samples?
71
nature of sampling, the sample with the highest probability will move around from
sampling realization to realization.
The original probability density can be estimated from the samples to within a
normalization constant because the number density of samples is proportional to
the probability. For example, f(θ) could be one in some area of interest around
θp and zero otherwise; ˆEf then has expectation proportional to P(θp). Note that
if using just the number density of samples, the normalization has been lost, so
we can only easily estimate ratios of probability densities at different points. Also,
the estimate will only be accurate to within O(1/√ns) of the posterior range of
P. If P(θp) is small relative to the peak, then a very large number of samples
would be required to estimate it to within a small fractional error. Nonetheless,
the probability density can be estimated easily where the probability is large, and
density plots of samples are a useful way to show the shape of the distribution in
the most likely region.
It is also difﬁcult to calculate conditional probability distributions from samples.
For example, we may wish to calculate the probability of the Hubble constant from
WMAP, conditional on all other cosmological parameters being ﬁxed, e.g., with
Ωm = 0.3. We would therefore need to keep only the samples for which Ωm = 0.3.
However, there will most likely be no samples with Ωm = 0.300000 precisely.
Therefore we could instead decide to use a narrow prior 0.29 < Ωm < 0.31 to
achieve a similar effect. Again there may be few samples in this range, so the
answer will be correspondingly noisy.
Fortunately it is extremely easy to calculate marginalized probability distribu-
tions. For example, a histogram of sample values for a single parameter is pro-
portional to the probability distribution marginalized over all other parameters that
were sampled. Similarly, a plot of sample positions for two parameters shows the
probability density marginalized over all other parameters. The points in the left-
hand panel of Figure 3.2 show this for the WMAP ΛCDM samples using the nat-
ural parameters for the CMB, in which the contours are relatively uncorrelated.
We can estimate the underlying probability distribution from the sample posi-
tions, and thus draw lines enclosing 68 and 95 per cent of the probability. The
simplest method is to bin the samples on a grid (2-D histogram). This will be
noisy due to the ﬁnite number of samples, therefore, if the probability distribu-
tion is believed to be slowly varying, then the histogram could be smoothed to
produce a more realistic set of contours. This is shown by the solid lines in
Figure 3.2.
Probability distributions of new parameters can often be calculated easily. The
right-hand panel of Figure 3.2 shows the values of the more conventional cosmo-
logical parameters H0 and Ωm. For each sample, the values of H0 and Ωm have
been calculated from the natural CMB parameters. Since these parameters are more

72
Parameter estimation using Monte Carlo sampling
0.0105
0.0104
0.0103
0.08
0.09
0.1
0.11
W mh 2
0.12
0.13
0.14
q
80
75
70
1
0.8
0.6
0.4
0.2
0
65
0.2
0.25
0.3
0.35
H0
Wm
Wb h2
Fig. 3.2. Samples generated from the WMAP 5-year data. Samples are plotted correspond-
ing to two of the parameters, and the solid lines enclose approximately 68% and 95% of
the samples (and hence probability). The dashed lines show the probability after impor-
tance sampling using a prior of H0 = 72 ± 1 km s−1 Mpc−1. The left-hand plot shows the
natural CMB parameters in which the samples were taken. The right-hand plot shows the
transformation to more conventional parameters, which are more correlated and therefore
not good parameters to use when sampling.
correlated it would be more difﬁcult to sample along the degeneracy directions
in parameter space using these. It is also possible to illustrate a third dimension,
for example, by colour coding the samples according to the value of a third para-
meter. Note that if a ﬂat prior was assumed on the natural CMB parameters when
calculating the samples, then this is still present. It would be necessary to apply
a correction to replace this with, e.g., a ﬂat prior on H0 and Ωm, as discussed
below.
3.4.2 Importance sampling
We have already discussed how generating a set of samples from a distribution
P(θ) can be an efﬁcient way to represent the information in the distribution. From
these samples expectations of functions of the parameters can be calculated easily.
But what happens if we want to test how the results change when we change P(θ)?
For example, we might want to change our choice of prior slightly, update the
distribution due to the arrival of more data, or perhaps test robustness by removing
parts of the data.
If the new distribution P ′(θ) is not too dissimilar to P(θ) we can ‘importance
sample’ the original samples from P(θ) without having to produce a new set of
samples from P ′(θ). The idea is essentially the same as rejection sampling, ex-
cept that instead of actually rejecting samples we weight them in proportion to
the probability ratios. This effectively gives a collection of non-integer weighted

3.4 What do I do with the samples?
73
samples for computing Monte Carlo estimates. For example, the expected value of
a function f(θ) under P ′(θ) is given by
⟨f(θ)⟩P ′ =

dθP ′(θ)f(θ) =

dθP ′(θ)
P(θ) P(θ)f(θ)
=
-P ′(θ)
P(θ) f(θ)
.
P
.
(3.14)
Given a set {θi} of ns samples from P(θ) a Monte Carlo estimate is therefore
⟨f(θ)⟩P ′ ≈1
ns
ns

i=1
P ′(θi)
P(θi) f(θi) = 1
ns
ns

i=1
wif(θi).
(3.15)
For this to work it is essential that P(θ) > 0, where P ′(θ) ̸= 0, and in practice
the weights wi ≡P ′(θ)/P(θ) should not be very large. The method works best
when {wi} ∼const as we then have the same number of importance samples as
we started with. If P ′(θ) is zero over some fraction F of P(θ), then the number of
samples will be reduced by at least a fraction F. If there is a signiﬁcant mismatch
between the distributions it is therefore necessary to start with many more samples
from P(θ) than would be required if sampling directly from P ′(θ).
If the distributions are not normalized, so that
	
dθP(θ) = Z, the ratio of the
normalizing constants can be estimated using
Z′
Z =
-P(θ)′
P(θ)
.
P
≈1
ns
ns

i=1
P ′(θi)
P(θi) = 1
ns
ns

i=1
wi,
(3.16)
and hence
⟨f(θ)⟩P ′ ≈
ns
i=1 wif(θi)
ns
i=1 wi
.
(3.17)
This is the obvious generalization of the normal Monte Carlo estimator to weighted
samples. The estimator for the normalization in Eq. (3.16) is precisely what you
would calculate when performing Monte Carlo integration of P ′(θ) (and P(θ) is
chosen to be analytically integrable so that you know Z).
Importance sampling is illustrated by the dashed lines in Figure 3.2. We have
taken the samples shown by the circles, and applied importance sample weights
using a Gaussian centred on H0 = 72 km s−1 Mpc−1 of unit variance. Ideally, this
kind of information would come from another dataset (although in practice current
data yield much larger uncertainties on the Hubble constant).
Given a new likelihood function we only need to do ns likelihood calculations
to re-weight our samples for the new posterior distribution; importance sampling
is very fast. However, like rejection sampling, it faces serious problems in high di-
mensions. It is only possible to importance sample between two high-dimensional

74
Parameter estimation using Monte Carlo sampling
distributions if the distributions are very similar in most dimensions so that the
effective number of samples 
i wi/max({wi}) ≫1.
Combining consistent cosmological datasets is one instance where importance
sampling can be useful. Often, any one dataset constrains only one or two com-
binations of parameters at all well. Combining one dataset with another therefore
usually only shrinks the distribution in one or two dimensions. If this is the case,
importance sampling can work reliably. It can also be useful for rapidly checking
how small changes in the likelihood propagate into the parameters, for instance,
small changes to the observational noise model or different methods for correcting
for systematics. Note, however, that if there is any inconsistency in the datasets,
importance sampling can be disastrous: if P(θ) ∼0 but P ′(θ) is signiﬁcant, there
are going to be almost no samples to importance weight and the effective number
of weighted samples will be tiny.
Note that unlike direct sampling, where the likelihood values are not actually
used to construct the estimators of the expectation values, to use importance sam-
pling the likelihood values are used. So before importance sampling the compres-
sion is into a set of parameter values and the (potentially un-normalized) posterior
probability at each point {θi, P(θi)}. In most sampling methods, the probability
values are calculated in order to do the sampling, but in some cases calculating the
probability values may take much more work than just sampling.
3.4.3 Inference from simulation
A common situation in observations is that we have a huge stream of data d with
approximately Gaussian noise, and we are interested in measuring some linear
function of d that encapsulates the relevant physics, say, a = Md. For exam-
ple, a may be just the large-scale modes of the data or the principal components.
Typically M is a rectangular matrix because a is much smaller than d. Since the
process is linear, Gaussian noise on d gives Gaussian noise on a with some co-
variance. If M is a large complicated matrix, computing the noise covariance of a
directly may be very hard, since calculating N = ⟨aaT⟩= M⟨ddT⟩MT requires
calculating the full d-covariance and then performing large matrix multiplications.
Instead, it is often easy to simulate multiple pure-noise data streams di. Each sim-
ulation gives one sample of a in its noise distribution. From a set of Monte Carlo
simulations of the noise, we can generate an estimator for the noise covariance,
ˆN = 1
ns
ns

i=1
aiaT
i .
(3.18)
This is an instance of applying parameter estimation to the parameters of cosmo-
logical observations rather than cosmological parameters themselves. Of course,

3.4 What do I do with the samples?
75
having estimated the noise we might then want to use its likelihood to calculate a
posterior for other parameters of more direct interest. This estimator is a sufﬁcient
statistic for the likelihood of the true noise covariance N, since for Gaussian noise,
L(N| ˆN) ∝exp(−
i aT
i N−1ai/2)
|2πN|ns/2
∝e−nsTr( ˆ
NN−1)/2
|2πN|ns/2
.
(3.19)
This is called an inverted Wishart distribution (for details see, e.g., Gupta and Nagar
(1999)). At least ns = n samples are required for ˆN to even be invertible (because
we need at least one sample per eigenvalue). For the likelihood to be normalizable,
we need ns > 2n. In general, the expected value (for ns > 2n + 2 and a ﬂat prior
on N) is E(N| ˆN) = ˆNns/(ns−2n−2), so the the actual covariance is likely to be
somewhat larger than the maximum likelihood estimate ˆN. The inverse is however
unbiased, E(N −1| ˆN) =
ˆN −1. The central limit theorem means that L(N| ˆN)
can be approximated by a multivariate Gaussian distribution for large numbers of
samples.
Either the full distribution of N should be used and integrated out, or we must
have ns ≫2n in order to safely assume that N ∼ˆN. If we are calculating the
noise N in order to estimate a signal covariance component,
S ∼
1
nobs
nobs

i=1
aiaT
i −N
accuracy can be critical. The fractional uncertainty on N from simulations is
∼

2/ns, so we need ns ≫(N/S)2 simulations to distinguish S from noise sam-
pling uncertainty. If the signal to noise is low, a great many simulations may be
required if the uncertainty in N is to be a small contribution to the posterior un-
certainty in S. For further discussion and possible ways to improve convergence in
a cosmological context, see, e.g., Hartlap, Simon and Schneider (2007) and Pope
and Szapudi (2007).
3.4.4 Model selection as parameter estimation
If we have a set of cosmological models {Mi}, each potentially with different
sets of parameters Sm = {θm,i}, there are a variety of different questions that
we can ask. For example, the straight parameter estimation question asks: ‘Given
the model M, what are the parameters?’ But we could also ask, ‘Which model
is correct?’, or more quantitatively, ‘What are the posterior odds on each model
being correct given the data?’. All of these questions can be recast in terms of
some larger parameter estimation framework. For instance, we could introduce a
discrete model index parameter h, so h = 1 corresponds to model M1. Adding
this ‘hyperparameter’ to all of the parameters of all the models gives an enlarged

76
Parameter estimation using Monte Carlo sampling
set of parameters that completely describes all the possibilities, {h, {Sm}}. To for-
mulate this correctly as a posterior probability problem it is necessary to include
the relative priors on each of the models. Doing parameter estimation on this en-
larged set would give us the posterior distribution of h and thus tell us about our
posterior belief in the different models. We could also compute conditional distri-
butions; for example, P(S1|h = 1) would tell us the distribution of the parameters
of model 1 given model 1 is correct. Our posterior odds of model 1 compared with
model 2 would just be P(h = 1)/P(h = 2), where the distributions have all the
other parameters integrated out. If all models are considered equally likely a priori
(same prior weight to each model) then this corresponds to the ratio of ‘evidence’,
discussed further in later chapters.
Casting model selection problems as parameter estimation can be particularly
useful when there is a large overlap between models, for example, when many pa-
rameters are common between the models. This is a frequent situation in cosmol-
ogy; for instance, M2 may just be a special case of M1 where one of the parameters
is ﬁxed at a particular value. As a concrete example, M2(θ) may correspond to a
ﬂat universe model where the curvature parameter ΩK = 0, and M1 has a free
curvature parameter M1(ΩK, θ), so that M2(θ) = M1(0, θ). In this case, know-
ing P(θ, ΩK|M1) is sufﬁcient to also know P(θ|M2). But can we work out the
posterior odds P(M2)/P(M1)? The general model selection approach would be
to work out P(M1) and P(M2) separately by integrating out all the other param-
eters, which may be a very time consuming process, and P(M1) will of course
depend on the prior chosen over ΩK. Instead, we could view M2 as a special case
of M1, so that for a separable prior4
P(M2|d)
P(M1|d) = P(d|M2)
P(d|M1)
Pr(M2)
Pr(M1)
(3.20)
=
P(d|M1, ΩK = 0)
	
dΩKP(d|M1, ΩK)Pr(ΩK|M1)
Pr(M2)
Pr(M1).
(3.21)
This result depends only on the un-normalized marginalized density P(d|ΩK) and
the priors. So from a parameter estimation run in M1, calculating the marginalized
likelihood P(d|ΩK) (e.g., estimated from the sample density as a function of ΩK,
where sampling is done with ﬂat prior on ΩK), we can easily work out the posterior
model odds given a particular choice of prior Pr(ΩK|M1). In fact, in this case,
simply plotting P(d|ΩK) is generally much more informative that computing the
odds for a particular choice of prior. In particular, if P(d|ΩK) peaks at ΩK = 0,
it is clear that for any choice of prior P(M2|d)/P(M1|d) > 0, so the restricted
model will be favoured.
4 This is essentially the same as the Savage–Dickey density ratio, see, e.g., Trotta (2007).

3.5 Conclusions
77
In the above example we can of course only estimate P(ΩK = 0|d) by as-
suming P(ΩK|d) is a smooth function, and the estimate might be quite noisy.
If this is an issue, as it may be in other cases, we could run our parameter es-
timation process in the enlarged parameter space with the model index parameter
h ∈(1, 2). In practice, this would mean that the sampling method would have to ar-
range to give some samples on the ﬁxed hypersurface ΩK = 0, but this is often not
difﬁcult.
Even in more general cases where the differences between models are more
radical, it may still be helpful to treat model selection as a parameter estimation
problem. For instance, sampling to compute the relevant posterior model odds ra-
tio may be much easier than computing each evidence value separately. In some
cases, the model index parameter h can also usefully be extended into a continuous
parameter. This can mean that its posterior distribution may give useful additional
information, and it may also improve sampling efﬁciency in some instances. For
example, M1 might use a likelihood with a simple noise model, and M2 might be
a likelihood using an alternative model that includes an extra contribution from
some noise source that was neglected in M1. By allowing the amplitude of this
extra noise to be a free parameter A, we can pose the model selection problem of
M2 versus M1 as a question about the ratio P(A)/P(0). In this case, if we orig-
inally thought that A = 1 in M2, but by plotting P(A) we see that P(A) peaks
at A = 2, this is giving us useful information that we should reconsider our noise
model.
3.5 Conclusions
We described the computational difﬁculties of mapping out probability distribu-
tions in multidimensional space and showed how sampling is often a good way to
obtain parameter estimates and error bars. We outlined a range of sampling tech-
niques, and emphasized the various potential problems and the importance of con-
vergence tests when using Markov chain results. When the underlying probability
distribution changes, importance sampling techniques can be used to generate a
new set of weighted samples very quickly.
Samples from a distribution are a useful resource for answering many questions
that could only be calculated directly from the likelihood function at much greater
computational cost. Monte Carlo simulation samples are also often a useful way
to learn about the low-dimensional statistical properties of high-dimensional dis-
tributions. In simple cases, model selection problems can also usefully be viewed
as simple parameter estimation problems.

78
Parameter estimation using Monte Carlo sampling
References
Berg, B. A. (2000). Fields Inst. Commun., 26, 1.
Dunkley, J., Bucher, M., Ferreira, P. G., Moodley, K. and Skordis, C. (2005). Mon. Not.
Roy. Astron. Soc., 356, 925.
Gamerman, D. and Lopes, H. F. (2006). Markov Chain Monte Carlo: Stochastic Simulation
for Bayesian Inference. London: Chapman and Hall/CRC.
Gelman, A. and Rubin, D. B. (1992). Stat. Sci., 7, 457.
Gelman, A., Carlin, J. B., Stern, H. S. and Rubin, D. B. (2003). Bayesian Data Analysis.
London: Chapman and Hall/CRC.
Gelman, A., Roberts, G. O. and Gilks W. R. (1996). In J. M. Bernado et al., eds., Bayesian
Statistics 5. Oxford: Oxford University Press, p. 599.
Geman, S. and Geman, D. (1984). IEEE Trans. Pattern Anal. Mach. Intell., 6, 721.
Gilks, W. R., Richardson, S. and Spiegelhalter, D. J. (1996). Markov Chain Monte Carlo in
Practice. London: Chapman and Hall/CRC.
Gubernatis, J. E. (2000). Comput. Sci. Eng., 2, 95.
Gupta, A. K. and Nagar, D. K. (1999). Matrix Variate Distributions. London: Chapman &
Hall.
Hajian, A. (2007). Phys. Rev. D, 75, 083525.
Hartlap, J., Simon, P. and Schneider P. (2007). Astron. Astrophys., 464, 399.
Hastings, W. K. (1970). Biometrika, 57, 97.
Kosowsky, A., Milosavljevic, M. and Jimenez, R. (2002). Phys. Rev. D, 66, 063007.
Lewis, A. and Bridle S., CosmoMC Notes, unpublished, available at http://cosmologist.
info/notes/cosmomc.ps.gz.
Liu, J. S. (2001). Monte Carlo Strategies in Scientiﬁc Computing. New York: Springer.
MacKay, D. J. C. (2003). Information Theory, Inference and Learning Algorithms.
Cambridge: Cambridge University Press; http://www.inference.phy.cam.ac.uk/mackay/
itila/book.html.
Metropolis, N., Rosenbluth, A. W., Rosenbluth, M. N., Teller, A. H. and Teller, E. (1953).
J. Chem. Phys., 21, 1087.
Murray, I., Ghahramani, Z. and MacKay, D. J. C. (2006). Proceedings of the 22nd Annual
Conference on Uncertainty in Artiﬁcial Intelligence (UAI-06). Arlington, VA: AUAI
Press.
Neal, R. M. (1993). Probabilistic inference using Markov chain Monte Carlo methods,
available at http://cosmologist.info/Neal93.
Neal, R. M. (2003). Ann. Stat., 31, 705.
Pope, A. C. and Szapudi, I. (2007). arXiv:0711.2509 [astro-ph].
Raftery, A. E. and Lewis, S. M. (1992). In J. M. Bernado, ed., Bayesian Statistics. Oxford:
Oxford University Press, p. 765.
Roberts, G. O., Gelman, A. and Gilks, W. R. (1997). Ann. Appl. Probab., 7, 110.
Taylor, J. F., Ashdown, M. A. J. and Hobson, M. P. (2007). arXiv:0708.2989 [astro-ph].
Thomas, D. B., Luk, W., Leong, P. H. W. and Villasenor, J. D. (2007). ACM Comput. Surv.,
39, 4, 11.1.
Trotta, R. (2007). Mon. Not. Roy. Astron. Soc., 378, 72.
Wang, F. and Landau, D. P. (2001a). Phys. Rev. Lett., 86, 2050.
Wang, F. and Landau, D. P. (2001b). Phys. Rev. E, 64, 5, 056101.

4
Model selection and multi-model inference
Andrew R. Liddle, Pia Mukherjee and David Parkinson
4.1 Introduction
One of the principal aims of cosmology is to identify the correct cosmological
model, able to explain the available high-quality data. Determining the best model
is a two-stage process. First, we must identify the set of parameters that we will
allow to vary in seeking to ﬁt the observations. As part of this process we need also
to ﬁx the allowable (prior) ranges that these parameters might take, most generally
by providing a probability density function in the N-dimensional parameter space.
This combination of parameter set and prior distribution is what we will call a
model, and it should make calculable predictions for the quantities we are going
to measure. Having chosen the model, the second stage is to determine, from the
observations, the ranges of values of the parameters which are compatible with
the data. This second step, parameter estimation, is described in the cosmological
context by Lewis and Bridle in Chapter 3 of this volume. In this article, we shall
concentrate on the choice of model.
Typically, there is not a single model that we wish to ﬁt to the data. Rather, the
aim of obtaining the data is to choose between competing models, where different
physical processes may be responsible for the observed outcome. This is the statis-
tical problem of model comparison, or model selection. This is readily carried out
by extending the Bayesian parameter estimation framework so that we assign prob-
abilities to models, as well as to parameter values within those models. The model
probabilities can be consistently updated, as usual, by applying Bayes’ theorem.
It is necessary to tackle the issue of cosmological model selection head-on,
because of the large number of parameters that one could conceive of including
within a cosmological model. It is currently well-accepted that the best cosmo-
logical data – a compilation of cosmic microwave background (CMB) anisotropy,
79

80
Model selection and multi-model inference
Table 4.1. Parameter constraints of the Standard Cosmological Model,
reproduced from Spergel et al. (2007, WMAP collaboration) with some additional
rounding. The values quoted are mean values and 68 per cent conﬁdence
intervals. All columns assume the ΛCDM cosmology with a power-law initial
spectrum, no tensors, spatial ﬂatness, and a cosmological constant as dark
energy. Three different data combinations are shown to highlight the extent to
which this choice matters. The parameters are Ωmh2 (physical matter density),
Ωbh2 (physical baryon density), h (Hubble parameter), n (density perturbation
spectral index), τ (optical depth to last-scattering surface), and σ8 (density
perturbation amplitude).
WMAP alone
WMAP + 2dF
WMAP + all
Ωmh2
0.128 ± 0.008
0.126 ± 0.005
0.132 ± 0.004
Ωbh2
0.0223 ± 0.0007
0.0222 ± 0.0007
0.0219 ± 0.0007
h
0.73 ± 0.03
0.73 ± 0.02
0.704+0.015
−0.016
n
0.958 ± 0.016
0.948 ± 0.015
0.947 ± 0.015
τ
0.089 ± 0.030
0.083 ± 0.028
0.073+0.027
−0.028
σ8
0.76 ± 0.05
0.74 ± 0.04
0.78 ± 0.03
galaxy clustering, and supernova luminosity distance data – can well ﬁt with a
rather small number of parameters, indeed just 6 or perhaps even 5, as listed in
Table 4.1.
However, the list of candidate parameters which might be required by future
data is very large indeed. Table 4.2 lists an (incomplete) set of parameters that
have already been considered, and there are upwards of twenty of these. One can-
not simply include all possible parameters in ﬁts to data, as each one will bring
new parameter degeneracies that weaken the constraints on the basic parameters
above, until we ﬁnd that we haven’t actually constrained those parameters at all,
which surely contradicts common sense. We require a consistent scheme to discard
parameters whose inclusion is not motivated by the data to hand. We will argue
here for a fully Bayesian methodology, rather than an ad hoc method such as a
chi-squared per degree of freedom approach.
4.2 Levels of Bayesian inference
The most familiar application of Bayesian methods is to estimate the allowed pa-
rameter values of a model. Having decided what the correct model is, we want to
know what values of its parameters are consistent with observational data. This
can be done, for instance, by sampling the posterior probability distributions using
Markov chain Monte Carlo methods, as described in Chapter 3. This approach is
widespread in cosmology.

4.2 Levels of Bayesian inference
81
Table 4.2. Candidate parameters: those that might be relevant for cosmological
observations, but for which there is presently no convincing evidence requiring
them. They are listed so as to take the value zero in the base cosmological model.
Those above the line are parameters of the background homogeneous cosmology,
and those below describe the perturbations. Of the latter set, the ﬁrst ﬁve refer to
adiabatic perturbations, the next three to tensor perturbations, and the remainder
to isocurvature perturbations. This table is taken from
Liddle (2004).
Ωk
spatial curvature
Nν −3.04
effective number of neutrino species (CMBFAST deﬁnition)
mνi
neutrino mass for species ‘i’
[or more complex neutrino properties]
mdm
(warm) dark matter mass
w + 1
dark energy equation of state
dw/dz
redshift dependence of w
[or more complex parameterization of dark energy evolution]
c2
S −1
effects of dark energy sound speed
1/rtop
topological identiﬁcation scale
[or more complex parameterization of non-trivial topology]
dα/dz
redshift dependence of the ﬁne structure constant
dG/dz
redshift dependence of the gravitational constant
dn/d ln k
running of the scalar spectral index
kcut
large-scale cut-off in the spectrum
Afeature
amplitude of spectral feature (peak, dip or step) . . .
kfeature
. . . and its scale
[or adiabatic power spectrum amplitude parameterized in N bins]
fNL
quadratic contribution to primordial non-Gaussianity
[or more complex parameterization of non-Gaussianity]
r
tensor-to-scalar ratio
r + 8nT
violation of the inﬂationary consistency equation
dnT/d ln k
running of the tensor spectral index
PS
CDM isocurvature perturbation S . . .
nS
. . . and its spectral index . . .
PSR
. . . and its correlation with adiabatic perturbations . . .
nSR −nS
. . . and the spectral index of that correlation
[or more complicated multi-component isocurvature perturbation]
Gμ
cosmic string component of perturbations
However, usually we do not in fact know what the correct model is. Instead, a
common data analysis goal is to decide which parameters need to be included in
order to explain the data. That is, we want to know the physical effects to which
our data are sensitive, in the hope of relating those effects to fundamental physics.
So, we have to acknowledge the possibility of more than one model. This brings

82
Model selection and multi-model inference
us to the second level of Bayesian inference, model selection/comparison. Given a
dataset, we want to know what the best model is. This is achieved by computing
the posterior model probability, which is obtained from the model likelihood, better
known in cosmology circles as the Bayesian evidence E. Having ﬁgured out which
the best model is, we can then do parameter estimation within that model in order
to ﬁnd the acceptable parameter ranges.
Even this, however, may not be the last word, because we might well ﬁnd that
there isn’t a single best model (indeed, the authors’ current experience is that there
never is). Instead, two or more models may all have non-negligible posterior prob-
abilities. Yet we still might want to know how probable the parameter values are.
This can be achieved using multi-model inference, which is the process of con-
sistently tracking and combining probabilities at both model and parameter level.
Parameter probability distributions can be obtained using Bayesian model averag-
ing (Hoeting et al. 1999), where the posteriors in each model are added together
weighted by the model probabilities.1
4.3 The Bayesian framework
The full multi-model inference framework can be built up directly from Bayes
theorem:
P(B|A) = P(A|B)P(B)
P(A)
.
(4.1)
Here A and B could be anything at all, but we will take A to be the set of data
D, and B to be the parameter values θ (where θ is the N-dimensional vector of
parameters being varied in the model under consideration), hence writing
P(θ|D) = P(D|θ)P(θ)
P(D)
.
(4.2)
As rehearsed in earlier articles in this volume, one of our objectives is to use
this equation to obtain the posterior probability of the parameters given the data,
P(θ|D). This is achieved by computing the likelihood P(D|θ). The result depends
on our prior belief on the probability of θ, P(θ), before the data came along, which
is often taken to be ﬂat.
In parameter estimation the normalizing factor P(D) is irrelevant and commonly
ignored, but it turns out to be crucial for model selection. Let us now explicitly
1 This is somewhat analogous to quantum mechanics, with the parameter following a superposition of probabil-
ity distributions (possibly including delta-functions if the parameter has a ﬁxed value in some models). Future
observations may collapse this probability into fewer viable models, perhaps only one.

4.3 The Bayesian framework
83
acknowledge that our probabilities are conditional not just on the data but on our
assumed model M, writing
P(θ|D, M) = P(D|θ, M)P(θ|M)
P(D|M)
.
(4.3)
The denominator, the probability of the data given the model, is by deﬁnition the
model likelihood, also known as the Bayesian evidence. It is useful because it ap-
pears in yet another rewriting of Bayes theorem, this time as
P(M|D) = P(D|M)P(M)
P(D)
.
(4.4)
The left-hand side is the posterior model probability (i.e., the probability of the
model given the data), which is just what we want for model selection. To deter-
mine it, we need to compute the Bayesian evidence, and we need to specify the
prior model probability. It is a common convention to take the prior model proba-
bilities to be equal (the model equivalent of a ﬂat parameter prior), but this is by no
means essential.
To obtain an expression for the evidence, consider Eq. (4.3) integrated over all θ.
Presuming we have been careful to keep our probabilities normalized, the left-hand
side integrates to unity, while the evidence on the denominator is independent of θ
and comes out of the integral. Hence
P(D|M) =

P(D|θ, M)P(θ|M)dθ
(4.5)
or, more colloquially,
Evidence =

(Likelihood × Prior) dθ
(4.6)
In words, the evidence is the average likelihood of the parameters averaged over
the parameter prior. For the distribution of parameter values you thought reasonable
before the data came along, what was the average value of the likelihood?
The Bayesian evidence rewards model predictiveness. For a model to be pre-
dictive, observational quantities derived from it should not depend very strongly
on the model parameters. That being the case, if it ﬁts the actual data well for a
particular choice of parameters, it can be expected to ﬁt fairly well across a signif-
icant fraction of its prior parameter range, leading to a high average likelihood. An
unpredictive model, by contrast, might ﬁt the actual data well in some part of its
parameter space, but because other regions of parameter space make very different
predictions it will ﬁt poorly there, pulling the average down. Finally, a model, pre-
dictive or otherwise, that cannot ﬁt the data well anywhere in its parameter space
will necessarily get a poor evidence.

84
Model selection and multi-model inference
Often predictiveness is closely related to model simplicity; typically the fewer
parameters a model has, the more limited the variety of predictions it can make.
Consequently, model selection is often portrayed as tensioning goodness of ﬁt
against the number of model parameters, the latter being thought of as an imple-
mentation of Ockham’s razor. However the connection between predictiveness and
simplicity is not always a tight one. Consider for example a situation where the
predictions turn out to have negligible dependence on one of the parameters (or a
degenerate combination of parameters). This is telling us that our observations lack
the sensitivity to tell us anything about that parameter (or parameter combination).
The likelihood will be ﬂat in that parameter direction and it will factorize out of the
evidence integral, leaving it unchanged. Hence the evidence will not penalize the
extra parameter in this case, because it does not change the model predictiveness.
The ratio of the evidences of two models M0 and M1 is known as the Bayes
factor (Kass & Raftery 1995):
B01 ≡E0
E1
,
(4.7)
which updates the prior model probability ratio to the posterior one. Some calcula-
tional methods determine the Bayes factor of two models directly. Usual conven-
tion is to specify the logarithms of the evidence and Bayes factor.
4.3.1 Priors
Bayesian inference requires that the prior probabilities be speciﬁed, giving the state
of knowledge before the data was acquired to test the hypothesis. These priors are
to be chosen, and different people may well not agree on them – priors are where
physical intuition comes into play. In our view, one shouldn’t hope to identify a
single ‘right’ prior. Rather, one should test how robust the conclusions are under
reasonable variation of the priors. Eventually, sufﬁciently good data will overturn
incorrect choices of prior.2
Parameter priors are relatively uncontroversial, though different researchers may
certainly have different opinions as to which choice is the most appropriate. This
is because they have to be stated explicitly in order for any useful calculation to
be possible, for instance, that of the posterior parameter probability distribution.
Model prior probabilities are a little more subtle, because they are not required for
computation of the evidence, and results are commonly expressed under the (often
implicit) assumption that the prior model probabilities are equal. However, in many
cases there may be very good reasons to take the prior model probabilities to be
2 ‘Incorrect’ here means that one’s original belief concerning a parameter’s value turns out not to match the
actual value.

4.3 The Bayesian framework
85
different, for instance, if one model has a strong physical underpinning and is well
supported by other data not being considered, while the other is not. Further, within
the common Bayesian philosophy of constant updating of probabilities in response
to emerging data, one may have inherited unequal prior model probabilities from an
earlier comparison with independent data (though admittedly in cosmology usual
practice is to reﬁt from scratch with a large combined dataset each time a new piece
of data emerges).
In any event, one always ought to consider how robust the conclusions be-
ing drawn are to variations in the model prior probabilities. This can usefully be
thought of as a decomposition into prior theoretical prejudice, modulated by the
evidence which tells us how the new data has changed our previous view. People
may disagree on the original position, but should all agree on the extent to which
the data has modiﬁed it. If our original prejudice were weighted in favour of one
model, we might well still end up favouring that model overall, even if the evidence
from the data was leaning in the opposite direction.
4.3.2 Information and complexity
In this section we have already spoken about how simplicity and predictiveness are
not trivially related. Often, parameters can be added to a model that the data can
make no predictions about at all, leaving the evidence unchanged. For example, if
we added the price of ﬁsh at Billingsgate market as a parameter to our cosmological
model, it would change none of the predictions for the cosmological datasets, and
so its likelihood would be completely ﬂat. If it was included in a model selection
analysis, it would have the evidence unchanged, no matter how large a prior was
placed on it.
This example may seem slightly absurd, but consider the case of including the
galaxy bias as a free parameter in an analysis that only used CMB data. There
are also the cases of parameters that we cannot measure well now, but may be
measurable in the future, such as the tensor to scalar ratio r, or wa = dw/da, the
CPL parameter for the evolution of the equation of state of the dark energy with
redshift.
In this case we ask a slightly different question than the standard model selec-
tion question. Instead of ‘Do we need these parameters?’, we can ask, ‘Have we
measured these parameters?’ In this case, the information of a model is deﬁned as
a distance (deﬁned by the Kullback–Leibler divergence) between the source mea-
sure (the prior) and the destination measure (the posterior), given by the equation
(taken from Chapter 1 by Skilling)
H =

P(θ) log[P(θ|D)/P(θ)]dθ .
(4.8)

86
Model selection and multi-model inference
Here the information H is (minus) the logarithm of the compression ratio, i.e., the
fraction of prior space that contains the bulk of the posterior volume.3 In this way
it represents how much we learn about the model by analysing the data. If we in-
troduce a new parameter, the prior space will expand, but if this new parameter is
well measured, then the ratio of prior to posterior will be larger and we gain more
information. If, on the other hand, the new parameter is useless at making predic-
tions, then the prior to posterior ratio will be unity, and the log of this ratio will be
zero, meaning no new information is learnt. Usually H ≃N log(signal/noise),
where N is the number of parameters.
A similar quantity is the complexity, introduced by Spiegelhalter et al. (2002)
and applied to cosmology by Kunz, Trotta and Parkinson (2006). It is deﬁned as
C = −2

DKL(p, π) −/
DKL

,
(4.9)
where DKL is once again the Kullback–Leibler divergence. The ﬁrst term in the
brackets is the gain in information under the model, i.e., H as deﬁned above. The
second term /
DKL is an estimate for the maximum information gain we can expect
under the model. By taking the difference between these quantities (with some
suitable multiplying factor), we estimate the number of parameters being measured
by the data under the model.
We can use Eq. (4.8) and Bayes’ theorem to rewrite Eq. (4.9) as
C = −2

P(θ|d) log P(D|θ) + 2 log P(D|ˆθ).
(4.10)
Deﬁning an effective χ2 through the likelihood as P(D|θ) ∝exp(−χ2/2) (any
constant factors drop out of the difference of the logarithms in Eq. 4.10) we can
write the effective number of parameters as
C = χ2(θ) −χ2(ˆθ),
(4.11)
where the mean is taken over the posterior PDF. This quantity can be computed
fairly easily from a Markov chain Monte Carlo (MCMC) run, which is nowadays
widely used to perform the parameter inference step of the analysis.
When using the complexity, one must be aware that it is not really a Bayesian
quantity. The point estimator for the maximum information gain, /
DKL, suffers
from the fact that it is only well deﬁned in the best case, where the posterior is well
described by a multidimensional Gaussian. The information, on the other hand, is
both well deﬁned and well behaved.
Both quantities, the information H and the complexity C, encode information
about how well the analysis has done at measuring the different parameters of the
3 Since it involves an averaging over the prior space, it can be computed simultaneously with the evidence.

4.4 Computing the Bayesian evidence
87
model, on scales set by the prior. As such they are measures of the predictiveness
of the model, independent of how well the model ﬁts the data. They can be used
in combination with the evidence to conﬁrm that, in the case where many models
have very similar evidence values, the best model really is the simplest.
4.4 Computing the Bayesian evidence
As we have seen, the problem of computing the evidence is that of carrying out a
multidimensional integral, which is of course a standard numerical problem. In this
case, however, the problem is made difﬁcult for two reasons. One is that the integral
is commonly extremely sharply peaked, with the location of the peak not known in
advance. The second is that the evaluations of the likelihood, if requiring inclusion
of cosmic microwave anisotropy or galaxy clustering data, are time-consuming; the
typical requirement of several seconds per evaluation limits the number of points
at which the integrand can be sampled to perhaps 105 or 106, even with multipro-
cessor capability.
There are a range of methods available, which fall into three broad classes:
• General Monte Carlo methods (which would become exact in the limit of arbi-
trarily many likelihood evaluation).
• Monte Carlo methods holding in restricted circumstances (but still potentially
arbitrarily accurate in those circumstances).
• Approximate methods.
We consider these in turn.
4.4.1 General Monte Carlo methods
Monte Carlo methods invoke a strategy for random sampling of the likelihood in
a way that allows an approximation to the integral to be made. To date, three such
methods have been applied in cosmology:
Nested sampling: This algorithm was devised by Skilling (2006), and is described
in more detail in Chapter 1 in this volume. The method populates the
prior with a number of randomly chosen points, and then uses replacement
to ‘walk’ the points to the high-likelihood region. We have implemented
this algorithm for cosmology in the publicly-available code COSMONEST
(Parkinson et al. 2006, www.cosmonest.org) and it is the method used to
obtain the results described in this chapter. A more sophisticated sampling
strategy, known as clustered nested sampling, has been described by Shaw,
Bridges and Hobson (2007), and Feroz and Hobson (2008) .

88
Model selection and multi-model inference
Thermodynamic integration: A standard technique in the statistical community,
which is also described in Skilling’s chapter, and sometimes called simu-
lated annealing, this method probes the distribution by ‘heating’ a Monte
Carlo Markov chain in order to probe the entire prior volume. It is some-
what less versatile than nested sampling, and so far has proven less fast
in cosmological applications, though this may be due to lack of proper
optimization.
VEGAS: This is a multidimensional integration package popular with particle
physicists, and deployed in cosmology by Serra et al. (2007). It has been
less widely tested than other methods but is promising both in terms of
speed and accuracy.
In addition to these three methods, there are other techniques in the statistics
literature that have not yet been used in cosmology. An example is parallel tem-
pering, described in Gregory (2005), where Markov chains at different tempera-
tures are run in parallel and are permitted to swap chain elements under certain
conditions.
4.4.2 Restricted Monte Carlo methods
The most important method here is the Savage–Dickey density ratio, introduced
into cosmological usage by Trotta (2007). It can be used where one model is nested
within another (i.e., it is obtained from the larger model by ﬁxing one or more
parameters to ﬁxed values), provided that the prior distribution of those parameters
obeys the factorization condition,4
π1(θ, ψ|M1) = π0(θ|M1)˜π(ψ|M0),
(4.12)
where π indicates the priors, θ is the vector of parameters varied in both models,
and ψ the parameters varied in model M1 but held ﬁxed in M0. This factorization
condition holds in many cosmological cases, though certainly not all (and may be
broken by prior boundaries even with apparently uniform priors).
It can then be shown that the evidence ratio between the two models is given by
evaluating the marginalized posterior parameter distribution at the special parame-
ter value(s) ψ = ψ∗:
B01 = P(ψ|D, M1)
π1(ψ|M1)
0000
ψ=ψ∗
.
(4.13)
This then allows the evidence ratio to be estimated from the posterior parame-
ter distribution of ψ, which typically has few enough dimensions that the cor-
rect normalization can be obtained, for instance, in a Markov chain. The general
4 A slightly weaker condition is actually required, but usually priors that satisfy it will be in this form.

4.5 Interpretational scales
89
problem of normalizing the full posterior is the same as computing the evidence;
the advantage here is that only the normalization of the marginalized posterior is
needed.
The main problem with this method, however, is that if the nested model is
disfavoured by the data, there may be few Markov chain samples in the vicinity
of the nested model, meaning that the estimate becomes noisy. The method is thus
better suited to cases where the simpler model will be supported by the data than
to those where it is not. It may be possible to redeem this situation using Markov
chains run at higher temperatures, but this possibility has not been formulated.
4.4.3 Approximate methods
Approximate methods can be used to estimate the evidence. For instance, one could
ﬁt a multivariate Gaussian to the likelihood in the vicinity of its maximum (the
so-called Laplace approximation), and then use this ﬁt to evaluate the integral. Un-
derstanding the accuracy of such an approximation may be challenging, however.
Related to this method is an approximate model selection statistic known as the
Bayesian Information Criterion (BIC), deﬁned by (Schwarz 1978; Liddle 2004)
BIC = −2 ln Lmax + k ln N,
(4.14)
where Lmax is the maximum likelihood achievable within the model, k the number
of parameters, and N the number of data points. The difference in BIC between
models estimates the log-evidence difference, but only under highly restricted cir-
cumstances (Gaussian likelihood well contained within the prior, identically and
independently distributed data points) that are unlikely to hold in practice. The
BIC should therefore be avoided unless it really is impossible properly to evaluate
the evidence itself.
Outside the Bayesian framework there are a number of other possible model
selection tools, most notably the Akaike Information Criterion (Akaike 1974) and
the principles of Minimum Message/Description Length (Wallace 2005), but as
this book is about Bayesian methods we do not need to discuss these further
here.
4.5 Interpretational scales
The standard scale used in interpreting the evidence is the Jeffreys scale, which
appeared in Appendix B of his famous 1961 textbook (we have omitted some of
his ﬁner levels of distinction).

90
Model selection and multi-model inference
Jeffreys
This volume
Δ ln E < 1
Not worth more than a bare mention.
1 < Δ ln E < 2.5
Signiﬁcant.
Weak.
2.5 < Δ ln E < 5
Strong to very strong.
Signiﬁcant.
5 < Δ ln E
Decisive.
Strong.
While the verbal descriptions he gives are useful, we have found that they may
be a little too strong if there is signiﬁcant uncertainty over the suitable choice of
priors, as is typical in cosmological problems. For a result to prove robust under a
range of reasonable priors the ratio must be stronger. Accordingly, we propose the
alternative verbal descriptions in the rightmost column.
In practice, we ﬁnd the divisions at 2.5 (corresponding to posterior odds of about
13:1) and 5 (corresponding to posterior odds of about 150:1) the most useful. Gen-
erally speaking, a difference of less than one is not very interesting, while differ-
ences above one are worth paying attention to.
The Jeffreys scale is commonly described, as above, as applying to the evidence
ratios, but should more properly be applied to the ratio of posterior model proba-
bilities. Common convention sets the prior model probabilities equal, so that the
evidence and posterior model probability ratios coincide, but one should always
consider the possible effect of varying that assumption. In many cosmological con-
texts, there are good physical reasons to favour one model over another before the
data are considered, though this may be hard to quantify. Indeed, in extreme cases
this can allow theories to be considered as favoured even in the absence of any
observational evidence, as has long been the case with superstring theory.
4.6 Applications
Cosmological applications of Bayesian model selection and multi-model inference
fall into two broad categories:
• Application to observational data to obtain model and parameter probabilities.
• Forecasting of the ability of future experiments to answer model selection ques-
tions. This second category also includes survey optimization using model
selection methods.
4.6.1 Applications to real data
A key goal of cosmology is to establish which models, or sets of parameters, to
investigate. At the time of writing the standard cosmological model consists of a
Universe around 14 billion years old, composed of a mixture of baryonic matter and

4.6 Applications
91
cold dark matter, whose rate of expansion is being accelerated by a mysterious dark
energy, and whose structure was seeded at early time by a near scale-invariant spec-
trum of Gaussian, adiabatic, superhorizon perturbations. This model is described
by the parameters given in Table 4.1, and supported by a range of observations led
by cosmic microwave anisotropies from the WMAP satellite.
Some of the most interesting questions concern the properties of those primor-
dial perturbations, which one hopes to link to physical processes taking place in the
early Universe, such as cosmological inﬂation. Of especial interest is the spectral
index of the primordial power spectrum, whose deviation from the scale-invariant
value would indicate dynamical processes taking place. There are also other, more
radical models for the primordial perturbations, such as isocurvature perturbations,
and causal seed models such as cosmic strings. Another major sector of model
uncertainty is in the modelling of dark energy, where a straightforward cosmolog-
ical constant is to be set against various models for dynamical dark energy and/or
modiﬁed gravity.
The spectral tilt
The favoured mechanism for generating the primordial perturbations is inﬂation,
which can accommodate a range of values for the spectral tilt parameter (n −1 ≡
d ln PR/d ln k where PR is the spectrum of curvature perturbations) under var-
ious conditions. However, the Harrison–Zel’dovich model makes the simpler as-
sumption of a scale-invariant spectrum (n = 1), and this model predates inﬂation.
Using the WMAP1 data, the HZ model achieved the largest evidence by virtue
of having one less parameter than a model with n (Mukherjee, Parkinson & Lid-
dle 2006). The datasets used were the CMB TT and TE anisotropy power spec-
trum data from the WMAP experiment (Bennett et al. 2003; Spergel et al. 2003;
Kogut et al. 2003), together with higher l CMB temperature power spectrum data
from VSA (Dickinson et al. 2004), CBI (Pearson et al. 2003) and ACBAR (Kuo
et al. 2004), matter power spectrum data from SDSS (Tegmark et al. 2004) and
2dFGRS (Percival et al. 2001), and supernovae apparent magnitude–redshift data
from the High-z Supernovae Search Team (Riess et al. 2004). The priors used are
stated in Table 4.3. The Bayes factor between the HZ and n models was found to
be −0.58, slightly favouring the simpler HZ model, but with no signiﬁcance.
The three-year WMAP3 data (Spergel et al. 2007; Hinshaw et al. 2007; Page
et al. 2007), however, for the ﬁrst time suggested a preference for a tilted spec-
trum, i.e., a scale-dependent primordial power spectrum with lesser power on small
scales. Parameter constraints from WMAP3 alone indicated n = 0.958 ± 0.016,
nearly three-sigma away from unity. A model selection analysis by Parkinson,
Mukherjee and Liddle (2006), and by Trotta (2007) indicated that WMAP3 data
on its own showed little evidence for n deviating from one, but with external data

92
Model selection and multi-model inference
Table 4.3. Priors for the different parameters, as discussed in the text.
Parameter
Type
Min.
Max.
Ωbh2
Uniform
0.018
0.032
Ωcdmh2
Uniform
0.04
0.16
Θ
Uniform
0.98
1.1
τ
Uniform
0
0.5
ln(As × 1010)
Uniform
2.6
4.2
n
Uniform
0.8
1.2
r
Uniform
0.0
1.0
r
Logarithmic
e−80
1
from smaller scale CMB experiments and LSS experiments (the same as the other
datasets for the WMAP1 analysis, with the addition of the Boomerang dataset by
Jones et al. 2006) the log-evidence for the n model went up to almost 2, which is a
substantial increase. The priors are the same as in the WMAP1 case. These results
are shown in Table 4.4.
However, slow-roll inﬂation models predict not only a spectral tilt but also a non-
zero tensor-to-scalar ratio r, and a proper model selection motivated by inﬂationary
predictions should contrast its predictions regarding n and r to a simpler (n = 1,
r = 0 Harrison–Zel’dovich) model. r can take values between 0 and 1 (case 1),
or, because its order is entirely unknown, linked to the energy scale of inﬂation,
one can use a logarithmic prior for r which allows inﬂation to occur at the very
small electroweak scale, giving rise to an r of about 10−34 (case 2). Thus, under
the logarithmic prior, the allowed range for ln r is from −80 to 0. Most of this
range, though, corresponds to an observationally negligible r, and hence this prior
generates an evidence that is close to the n-only model, whereas in case 1, the n–r
model is disfavoured over the simplest HZ model, as seen from the table.
A last point to note is that measurements of n are especially prone to a number
of systematic uncertainties as of now. Assumptions made during foreground (in
particular, point source) subtraction, corrections for other small-scale effects such
as the SZ effect, and reionization, all affect the estimate of n, urging caution in
arriving at a verdict about n just yet.
Dark energy
Another very interesting application of model selection is in making deductions
regarding the nature of dark energy. This was studied in detail by Liddle et al.
(2006). Here the simplest model to consider is the cosmological constant, corre-
sponding to the energy of the vacuum, with a constant equation of state w of −1.

4.6 Applications
93
Table 4.4. Evidence ratios for the different models and different datasets, as
discussed in the text.
Datasets
Model
ln B10
WMAP1 + all
HZ
0.0
n
−0.58 ± 0.12
WMAP3 only
HZ
0.0
n
0.34 ± 0.26
WMAP3 + all
HZ
0.0
n
1.99 ± 0.26
n + r (uniform prior)
−1.45 ± 0.45
n + r (log prior)
1.90 ± 0.24
Table 4.5. Evidence ratios for the different dark energy models.
Model
ln B10
Prob
I: ΛCDM
0.0
63%
II: −1 ≤w ≤−0.33
−1.3 ± 0.1
17%
III: −2 ≤w ≤−0.33
−1.8 ± 0.1
10%
IV: −2 ≤w0 ≤−0.33, −1.33 ≤wa ≤1.33
−2.0 ± 0.1
9%
V: −1 ≤w(a) ≤1 for 0 ≤z ≤2
−4.1 ± 0.1
1%
A model with a different constant value of w is possible, though not well mo-
tivated theoretically. Two-parameter models of the equation of state dark energy
are better motivated; these, for instance, describe the evolution of w with epoch
or scale factor a as w(a) = w0 + wa(1 −a). Then there is the issue of whether
to allow w to cross the so-called phantom divide of −1 in either of these mod-
els/parameterizations. Results for various cases are shown in Table 4.5.
Overall, the simplest model continues to be a good ﬁt to the data and is rewarded
by the evidence for its predictiveness, and thus remains the favoured model. Other
models have smaller evidence, though none of them are decisively ruled out. But
the two-parameter models, considered more natural, are quite substantially dis-
favoured. Leaving all the models on the table, and converting the Bayes factor into
probabilities, results in the probabilities shown in the ﬁnal column of Table 4.5.
Probabilities of 77%, 18% and 5% are obtained if we consider each parameteriza-
tion as referring to a model and, within each parameterization, marginalize over the
different prior choices we consider viable. This would avoid penalizing the ΛCDM

94
Model selection and multi-model inference
Fig. 4.1. The parameter likelihood distributions of w0 and wa after Bayesian model aver-
aging over models I, II and V (reproduced from Liddle et al. 2006).
model for having only one prior choice. Or we could disallow the crossing of the
phantom divide and then models I, II, and V results in probabilities of 78%, 21%
and 1%, respectively. This is to say that given model uncertainties the indicated
probabilities can change somewhat, but ΛCDM remains preferred by current data.
Future data will be quite signiﬁcantly more constraining of dark energy param-
eters. This is often taken to mean that they have a high chance of detecting dark
energy evolution, because in evaluating future surveys one assumes that a w0, wa
model is the true model, ignoring other models, and evaluates a parameter esti-
mation Figure of Merit (FoM) based on how much the allowed parameter space
in these parameters would shrink given a future survey. However, a model se-
lection analysis of current, relatively poor quality, data already indicates that the
ΛCDM is the favoured model, and if it is indeed the true model there is no dark
energy evolution to detect. In that event, the merit of future experiments will lie
in favouring in the ΛCDM model decisively against its alternatives. A model se-
lection based Figure of Merit (as discussed in Mukherjee et al. 2006) must then
be used to rank future surveys based on their ability to do this, as parameter es-
timation based FoMs cannot be used to represent positive support for a simpler
model.
One ﬁnal point to note in this context is that when multiple models are on the
table, Bayesian model averaging, consisting of superimposing the parameter con-
straints within each model, weighted by the model’s probability, should be used to
arrive at parameter constraints that thus include model uncertainty. For dark energy,
this results in distributions for w0 and wa that are already pretty tightly constrained
about −1 and 0, respectively, their values in a ΛCDM model, as seen in Figure 4.1.

4.6 Applications
95
Other applications
One question of interest is the nature of primordial perturbations, which would
reﬂect in turn the nature of the mechanisms that could have produced such pertur-
bations. Adiabatic (or density) perturbations are most naturally produced, but there
are, for instance, multi-ﬁeld scenarios of inﬂation which would lead to isocurvature
(or entropy) perturbations. The observable signature of such a component is a shift
in the peaks in the CMB spectrum. The issue of constraining models that contain
a variety of isocurvatures modes has been considered in Beltr`an et al. (2005) and
Trotta (2007), who considered various priors on the modes. These models are cur-
rently disfavoured. There is only a small chance that CMB polarization data, which
will improve dramatically with Planck, can affect this result.
Similarly, one can consider models that contain a subdominant contribution
from cosmic strings. Cosmic strings are theoretically motivated by supersymmet-
ric and grand-uniﬁed theories, and actively source additional anisotropies in the
CMB affecting the resultant temperature and polarization spectra. After perform-
ing detailed simulations of the string to arrive at ﬁrm predictions, Bevis et al. (2008)
do not ﬁnd signiﬁcant evidence for strings at present.
Another question of interest is whether our Universe has a non-trivial topol-
ogy. A variety of non-trivial topologies (here it is hard to exhaust all possibilities)
were considered by Niarchou and Jaffe (2007), and Niarchou et al. (2004). The
observable signatures in this case are multipole space correlations in the CMB. No
evidence was found for a non-trivial topology.
The Bayesian evidence has also been used to, for example: argue for the pres-
ence of residual foreground contamination in the CMB by Eriksen et al. (2007);
determine the optimal level of regularization in component separation of multi-
frequency CMB observations (Hobson et al. 1998; Stolyarov et al. 2002); de-
termine the optimal relative weighting of cosmological datasets in joint analyses
(Hobson, Bridle & Lahav 2002); determine the level of multual consistency of
cosmological datasets (Marshall, Rajguru & Slosar 2006); determine the prob-
ability that localized features in CMB maps correspond to Sunyaev–Zel’dovich
clusters (Hobson & McLachlan 2003) and extragalactic point sources (Carvalho,
Rocha & Hobson 2009); determine whether a cold spot seen in the CMB could
be due to a cosmic texture (Cruz et al. 2007); investigate whether the WMAP
CMB data is well described by a Bianchi model (Bridges et al. 2007) and in-
vestigate the form of the primordial perturbation spectrum (Bridges, Lasenby &
Hobson 2006).
Hence model selection tools have been used to address a variety of questions
that we were unable to address robustly before. Future better-quality data will bring
further interesting opportunities for applying such tools.

96
Model selection and multi-model inference
4.6.2 Forecasting and survey design
In addition to making inferences from actual observational data, model selection
tools are well adapted to understanding the capabilities of proposed future experi-
ments, where they complement and extend parameter estimation techniques, such
as Fisher matrix forecasting of expected parameter errors.
Parameter estimation offers a fairly limited view of experimental prospects,
based as it is on the assumption that we already know the correct model and are
interested only in determining its parameters. It also encourages the incorrect (or
at least non-Bayesian) view that ﬁxing the parameters of a larger model to speciﬁc
values is the same as considering a more restricted model (from a Bayesian view-
point, the two are distinguished by the different model predictiveness). Moreover,
parameter estimation methods are at best restricted to the case where one model is
embedded as a special case of the other, and offers no guidance on how an exper-
iment might distinguish between two competing alternatives. For instance, if we
imagine a future experiment that manages to rule out the pure cosmological con-
stant as the model of dark energy, one might wish to know whether it would then be
able to say whether quintessence or modiﬁed gravity is the preferred explanation.
Once we allow multiple models, the questions we ask of future experiments
become much broader. We can ask how well two models can be distinguished,
under the assumption that either one is the true model, and also as a function of the
true values of model parameters. For instance, we could ask how far the values of
dark energy parameters have to be from the cosmological constant limit before a
model comparison would be able to rule out the latter model.
Model selection forecasting for cosmology was ﬁrst discussed by Trotta (2007)
in an implementation predicting the probability of future experiments yielding dif-
ferent model selection outcomes (based on present knowledge), and by Mukherjee
et al. (2006) in an implementation studying the dependence of model selection
outcome on true model parameters. The two approaches were combined in Pahud
et al. (2006) and Liddle et al. (2006), the latter also employing Bayesian model
averaging.
Model selection forecasting, and its further extension to Bayesian experimental
design, are described in the article by Trotta et al. in Chapter 5 of this volume.
4.7 Conclusions
There is a growing body of literature in the use of Bayesian model selection to make
inferences about cosmological models. Model selection allows one to pose and
answer questions which are not available within the parameter estimation frame-
work alone, for instance, in comparing two models where one is not embedded
within the other. As in Bayesian parameter estimation, these developments are

4.7 Conclusions
97
driven by the recent availability of powerful calculational algorithms and sufﬁcient
computing power to deploy them. In our view, developing a robust understand-
ing of cosmological models requires the use of model-level Bayesian inference,
both in analyzing the viability of models in light of existing data, and in designing
forthcoming experiments to maximize their impact.
References
Akaike, H. (1974). IEEE Trans. Automat. Contr., 19, 716.
Beltr´an, M., Garcia-Bellido, J., Lesgourgues, J., Liddle, A. R. and Slosar, A. (2005). Phys.
Rev. D, 71, 063532.
Bevis, N., Hindmarsh, M., Kunz, M. and Urrestilla, J. (2008). Phys. Rev. Lett., 100, 021301.
Bridges, M., Lasenby, A. N. and Hobson, M. P. (2006). Mon. Not. Roy. Astron. Soc., 369,
1123.
Bridges, M., McEwen, J. D., Lasenby, A. N. and Hobson, M. P. (2007). Mon. Not. Roy.
Astron. Soc., 377, 1473.
Carvalho, P., Rocha, G. and Hobson, M. P. (2009). Mon. Not. Roy. Astron. Soc., 393, 681.
Cruz, M., Turok, N., Vielva, P., Martinez-Gonzalez, M. and Hobson, M. (2007). Science,
318, 1612.
Dickinson, C. et al. (2004). Mon. Not. Roy. Astron. Soc., 353, 732.
Eriksen, H. K., Banday, A. J., Gorski, K. M., Hansen, F. K. and Lilje, P. B. (2007). Astro-
phys. J., 660, L81.
Feroz, F. and Hobson, M. P. (2008). Mon. Not. Roy. Astron. Soc., 384, 449.
Gregory, P. (2005). Bayesian Logical Data Analysis for the Physical Sciences. Cambridge:
Cambridge University Press.
Hinshaw, G. et al. (2007). Astrophys. J. Supp., 170, 288.
Hobson, M. P., Bridle, S. L. and Lahav, O. (2002). Mon. Not. Roy. Astron. Soc., 335, 377.
Hobson, M. P., Jones, A. W., Lasenby, A. N. and Bouchet, F. R. (1998). Mon. Not. Roy.
Astron. Soc., 300, 1.
Hobson, M. P. and McLachlan, C. (2003). Mon. Not. Roy. Astron. Soc., 338, 765.
Hoeting, J. A., Madigan, D., Raftery, A. E. and Volinsky, C. T. (1999). Stat. Sci., 14, 382.
Jeffreys, H. (1961). Theory of Probability, 3rd edn. Oxford: Oxford University Press.
Jones, W. C. (2006). Astrophys. J., 647, 823.
Kass, R. E. and Raftery, A. E. (1995). J. Am. Stat. Assoc., 90, 773.
Kogut, A. et al. (2003). Astrophys. J. Supp., 148, 161.
Kunz, M., Trotta, R. and Parkinson, D. (2006). Phys. Rev. D, 74, 023503.
Kuo, C. J. et al. (2004). Astrophys. J., 600, 32.
Liddle, A. R., 2004, Mon. Not. Roy. Astron. Soc., 351, L49
Liddle, A. R., Mukherjee, P., Parkinson, D. and Wang, Y. (2006). Phys. Rev. D, 74, 123506.
Marshall, P., Rajguru, N. and Slosar, A. (2006). Phys. Rev. D, 73, 7302.
Mukherjee, P., Parkinson, D., Corasaniti, P. S., Liddle, A. R. and Kunz, M. (2006). Mon.
Not. Roy. Astron. Soc., 369, 1725.
Mukherjee, P., Parkinson, D. and Liddle, A. R. (2006). Astrophys. J. Lett., 638, L51.
Niarchou, A. and Jaffe A. (2007). Phys. Rev. Lett., 99, 081302.
Niarchou, A., Jaffe, A. and Pogosian, L. (2004). Phys. Rev. D, 69, 063515.
Page, L. et al. (2007). Astrophys. J. Supp., 170, 355.
Pahud, C., Liddle, A. R., Mukherjee, P. and Parkinson, D. (2006). Phys. Rev. D, 73, 123524.
Parkinson, D., Mukherjee, P. and Liddle, A. R. (2006). Phys. Rev. D, 73, 123523.
Pearson, T. J. et al. (2003). Astrophys. J., 591, 556.

98
Model selection and multi-model inference
Percival, W. J. et al. (2001). Mon. Not. Roy. Astron. Soc., 327, 1297.
Riess, A. G. et al. (2004). Astrophys. J., 607, 665.
Schwarz, G. (1978). Ann. Stat., 5, 461.
Serra, P., Heavens, A. and Melchiorri, A. (2007). Mon. Not. Roy. Astron. Soc., 379, 1169.
Shaw, J. R., Bridges, M. and Hobson, M. P. (2007). Mon. Not. Roy. Astron. Soc., 378, 1365.
Skilling, J. (2006). Bayesian Analysis, 1, 833.
Spergel, D. N. et al. (2003). Astrophys. J. Supp., 148, 175.
Spergel, D. N. et al. (2007). Astrophys. J. Supp., 170, 377.
Spiegelhalter, D. J. et al. (2002). J. R. Stat. Soc. B, 64, 583.
Stolyarov, V., Hobson, M. P., Ashdown, M. A. J. and Lasenby, A. N. (2002). Mon. Not. Roy.
Astron. Soc., 336, 97.
Tegmark, M. et al. (2004). Astrophys. J., 606, 702.
Trotta, R. (2007). Mon. Not. Roy. Astron. Soc., 378, 72.
Wallace, C. S. (2005). Statistical and Inductive Inference by Minimum Message Length.
Berlin: Springer.

5
Bayesian experimental design and model selection
forecasting
Roberto Trotta, Martin Kunz, Pia Mukherjee and David Parkinson
5.1 Introduction
Common applications of Bayesian methods in cosmology involve the computation
of model probabilities and of posterior probability distributions for the parameters
of those models. However, Bayesian statistics is not limited to applications based
on existing data, but can equally well handle questions about expectations for the
future performance of planned experiments, based on our current knowledge.
This is an important topic, especially with a number of future cosmology experi-
ments and surveys currently being planned. To give a taste, they include: large-scale
optical surveys such as Pan-STARRS (Panoramic Survey Telescope and Rapid Re-
sponse System), DES (the Dark Energy Survey) and LSST (Large Synoptic Survey
Telescope), massive spectroscopic surveys such as WFMOS (Wide-Field Fibre-
fed Multi-Object Spectrograph), satellite missions such as JDEM (the Joint Dark
Energy Explorer) and EUCLID, continental-sized radio telescopes such as SKA
(the Square Kilometer Array) and future cosmic microwave background experi-
ments such as B-Pol searching for primordial gravitational waves. As the amount
of available resources is limited, the question of how to optimize them in order
to obtain the greatest possible science return, given present knowledge, will be of
increasing importance.
In this chapter we address the issue of experimental forecasting and optimiza-
tion, starting with the general aspects and a simple example. We then discuss the
so-called Fisher Matrix approach, which allows one to compute forecasts rapidly,
before looking at a real-world application. Finally, we cover forecasts of model
comparison outcomes and model selection Figures of Merit. Throughout, the the-
ory is illustrated with examples of cosmological applications.
99

100
Bayesian experimental design and model selection forecasting
5.2 Predicting the effectiveness of future experiments
The optimization of experiments can be usefully considered from the point of view
of decision theory. First, one needs to deﬁne a quantity of interest, generally called
the Figure of Merit (FoM), associated with the proposed experiment and its science
return. The choice of this ﬁgure of merit is a priori free, and it clearly depends on
the question that is being asked. In some cases it may be obvious – if we try to
forecast the outcome of ﬁnancial strategies, we are probably trying to maximize
the expected proﬁt. For cosmology the choice is less obvious, and we will discuss a
few possibilities further below. We call the FoM a utility U (if we want to maximize
it), or loss L (in which case minimization would seem preferable). The task is to
determine which experiment maximizes the utility, subject to external constraints
and – crucially – to the uncertainty about the true nature of the Universe.
5.2.1 Utility, expected utility and optimization
Let us assume that we have a choice of experiments e that we can build, and that
within a given model M, the physical system under consideration is described by
a set of parameters θ. We assume that other experiments o have been performed,
so that our present knowledge of the system before we build the new experiment
is described by the posterior1 P(θ|o). The quantity of interest to us is the utility
U(θ, e, o), which in general depends on what we have observed so far (the data o),
on the assumed values for the parameters θ, and on the characteristics of the future
experiment, e. From the utility we can build the expected utility,
E[U|e, o] =

dˆθ U(ˆθ, e, o)P(ˆθ|o).
(5.1)
This expression should be interpreted as follows: If the Universe is correctly de-
scribed by the set of parameters ˆθ (that we call ﬁducial parameters) and we build
the experiment e, then we can compute the utility for that experiment, U(ˆθ, e, o).
However, our knowledge of the Universe is limited, and it is described by the cur-
rent posterior distribution P(ˆθ|o). Averaging the utility over the posterior accounts
for present uncertainty as to the value of the parameters. If, as is usually the case in
cosmology, there is also uncertainty as to the underlying true model, then one has
to average over possible models, as well:
E[U|e, o] =

i
P(M(i)|o)

dˆθ(i)U(ˆθ(i), e, o)P(ˆθ(i)|o, M(i)),
(5.2)
where ˆθ(i) represents the set of parameters under model M(i). The aim is to se-
lect the experiment e that maximizes the expected utility in Eq. (5.2). Notice that
1 If e is the ﬁrst experiment of its kind then P(θ|o) becomes the prior distribution for the parameters.

5.2 Predicting the effectiveness of future experiments
101
E[U|e, o] takes into full account current model and parameter uncertainty, and
therefore maximizing it is a way of ‘hedging’ over current lack of information
as to the true model for the Universe (see Loredo (2004) and Bassett (2005) for
other examples).
5.2.2 Choosing the best experiment – an example
The above ideas can be illustrated in the following toy example. Suppose we want
to measure a single parameter θ, which is the slope of a linear function of x > 0.
In cosmology, we might think of x as being the redshift and θ the local Hubble
constant. This simple linear model can be written as
y = θx + ϵ,
(5.3)
where ϵ is a noise term. The experiment o has been performed by measuring the
quantity y at two locations, x0, x1, resulting in the data y0, y1. We assume that
the two measurements are uncorrelated, and that the noise from experiment o is
described by a Gaussian of 0 mean and variance σ2. Then one can show that the
likelihood as a function of the parameter θ is given by
L(o|θ) = Lmaxexp
$
−1
2(θ −θ0)2L
%
,
(5.4)
where θ0 = (x0y0 + x1y1)/(x2
0 + x2
1) is the maximum likelihood value for the
parameter and the inverse variance L is given by L = (x2
0 + x2
1)/σ2. The posterior
from o follows by applying Bayes’ theorem. If we take a prior centred around 0 for
θ and of unit variance, then the posterior P(θ|o) is again a Gaussian, with mean
given by ¯θ = (x0y0 + x1y1)/(σ2 + x2
0 + x2
1) and inverse variance (denoted by F)
given by the sum of the prior and likelihood inverse variances, i.e., F = 1 + L.
We now have a choice as to how to design a new experiment e in the light of what
we have learned from o. The noise of the future measurement, τ, might depend in a
known way on the location on the x-axis, for example, because measurements fur-
ther out along x come at a greater experimental cost and hence their noise increases
as some function of x. For a known function τ(xf), the optimization problem for e
is then reduced to the choice of location xf that will yield the best science return.
This is described in terms of the utility function we choose to adopt. Since we are
trying to determine θ as best we can, it is sensible to use as utility function the
inverse of the posterior variance of θ, denoted by F. Recalling that the error on θ
is given by 1/
√
F, maximizing the utility function means minimizing the posterior
error on the parameter of interest.
Let us now compute the utility function from e, U(ˆθ, e, o) = F. By repeating
the reasoning above, we obtain that the posterior for θ from the future experiment

102
Bayesian experimental design and model selection forecasting
e is again a Gaussian distribution. Its inverse variance (our utility function) is
given by
F(xf) = 1 + L +
x2
f
τ 2(xf).
(5.5)
Because this is a linear model, the posterior inverse variance does not depend on
the assumed ﬁducial value for the parameter, ˆθ. Then the expected utility is
E[U|o, e] =

dˆθP(ˆθ|o)F(xf) = F(xf),
(5.6)
where the second equality derives from the facts that our utility function does not
depend on ˆθ for this particular model and that the posterior from current data,
P(ˆθ|o), integrates to unity. Therefore, in this case, maximization of the expected
utility is equivalent to maximization of the utility itself.
If the experiment e has a noise that is approximately constant as a function of the
x-axis location, i.e., for τ 2(xf) = τ 2, then E[U|o, e] = 1 + L + x2
f/τ 2, which is
maximized if we take xf as large as we can, compatibly with our experimental set-
up. This is simply expressing the fact that we should use the longest possible lever
arm when trying to measure a slope, an eminently sensible result. If the future
measurement is correlated with the previous ones, the expected utility becomes
E[U|o, e] = 1 + XtC−1X, where X = (x0, x1, xf) and C is the data correlation
matrix (which itself might depend on xf). Then the maximization of the expected
utility reduces to the choice of xf that maximizes XtC−1X.
Consider now the case where we could build a more accurate experiment, a,
which could measure y with noise τ⋆≪τ, but only if the signal falls around
a certain value, y⋆. In other words, y⋆is a ‘sweet spot’ where experiment a can
deliver a very precise measurement. We can model this situation by writing for the
noise of a,
τ 2
a = τ 2
⋆exp (y −y⋆)2
2Δ2
.
(5.7)
We consider the parameters y⋆, τ⋆, Δ as ﬁxed quantities, but in principle one could
imagine optimizing over them as well. Here, we are interested in ﬁnding the op-
timal location xf for a new measurement and in comparing the performance of a
with the experiment considered above, e.
In building a, the problem is that we do not know precisely to which value xf
on the x-axis the sweet spot y⋆corresponds, because of our ﬁnite precision on θ.
If we wanted simply to minimize the noise τa, we would pick xf in such a way
that y = ¯θxf = y⋆, hence xf = y⋆/¯θ, recalling that ¯θ is the posterior mean
for the slope from the current data. However, this is ignoring the lesson drawn
from consideration of the utility, Eq. (5.5), which told us that the quantity one has

5.2 Predicting the effectiveness of future experiments
103
to maximize is actually x2
f/τa(xf)2, in order to maximize the lever arm of the
measurement. Now the utility depends on what we assume for θ, since y = θxf.
If we take the current posterior mean as the ﬁducial value for the slope (i.e., using
y = ¯θxf) and we maximize the utility Eq. (5.5) with the noise given by (5.7), we
get for the optimal location,
xf = y⋆
¯θ
1 +

1 + 8Δ2/y2⋆
2
.
(5.8)
This is shifted to a larger value than if we had simply minimized the noise, exploit-
ing the fact that the lever arm of the new datum is increased if xf is pushed to a
larger value, while not degrading the performance of the experiment too much by
staying within a width ∼Δ of the sweet spot.
Finally, we can compute the expected utility, which accounts for the fact that
there is uncertainty around the relation y = ¯θxf, since ¯θ is known only with accu-
racy σ from present data. From the ﬁrst equality in Eq. (5.6) we get
E[U|o, a] = 1 + L +
x2
f
τ 2⋆
exp
$
−(y⋆−¯θxf)2
2(Δ2 + Δy2)
%
Δ

Δ2 + Δy2 ,
(5.9)
where we have deﬁned Δy ≡Σxf, where Σ ≡1/
√
F is the posterior error on
θ from the current data. Hence Δy represents the uncertainty in y at the location
x = xf stemming from our present uncertainty on the value of the slope. If Δ2 ≫
Δy2, then our uncertainty on the slope is much less than the width of the sweet spot,
Δ, and the expected utility (5.9) reduces to the utility assuming ¯θ as the ﬁducial
value, leading again to Eq. (5.8). However, the opposite case Δ2 ≪Δy2 describes
a sweet spot that is much narrower than our current uncertainty on its location.
Then the expected utility is maximized by the choice
xf = y⋆
¯θ
1 +

1 −4Σ2/¯θ2
2Σ2/¯θ2
.
(5.10)
Notice that xf is real only for |¯θ/Σ| > 2, i.e., if the slope from current data is
measured to better accuracy than two sigma. If this is not the case, inspection of
Eq. (5.9) reveals that E[U|o, a] is a monotonically increasing function of xf (see
the example in the lower panel of Figure 5.1), which is therefore maximized by
taking xf as large as possible, even though this means carrying out an experiment
with very poor accuracy, as τ 2
a →∞for xf →∞. The interpretation of this result
is that designing an experiment that exploits a sweet spot is only feasible if our
current uncertainty on the parameter is small enough compared with the width of
the sweet spot window of opportunity.

104
Bayesian experimental design and model selection forecasting
4
3
2
1
0
3
2
1
0
0
1
2
Experiment a
Expected utility
Experiment e
x
y
3
xmax
x*
x1
x0
4
3
2
1
00
1
2
Experiment a
Expected utility
Experiment e
x
y
3
0
0.2
0.4
0.6
0.8
1
xmax
x*
x1
x0
Fig. 5.1. Optimization of a future measurement using the expected utility. The choice is
between a more accurate experiment a, with a ‘sweet spot’ given by the vertical dashed
band, and a less accurate experiment e that has constant noise across the whole x range.
The diagonal/dashed line is the true value of the slope while the diagonal/triangle shaped
dashed region is the 68.3% region as inferred from the two measurements (data points
at x0, x1). The dashed/thick curve is the expected utility of a while the solid/thick curve
is for experiment e (arbitrary units on the right vertical axis). The ratio of the maxima
of the expected utilities determines which experiment is better. For the case in the top
panel, experiment a is to be preferred, while in the bottom panel, experiment e is the better
choice.

5.2 Predicting the effectiveness of future experiments
105
It is instructive to examine the numerical values of a concrete case. Let us con-
sider the situation where data have been obtained at x0 = 0.5 and x1 = 1.0 with
noise σ = 0.1, when the true underlying slope is θ = 1.0. For the data realization
shown in the upper panel of Figure 5.1, the posterior from experiment o has mean
¯θ = 1.04 and standard deviation Σ = 0.09. The x-range of future measurements is
limited by xf < xmax = 3.0. Experiment e consists of collecting a further datum
at larger x with the same noise as o, hence τ = σ = 0.1. From the considera-
tions above, we know that the utility for e is maximized by choosing the largest
possible lever arm, hence by carrying out the measurement at xf = xmax. For ex-
periment a, we assume that τ⋆= σ/5, y⋆= 1.5 and Δ = 0.1 in Eq. (5.7), hence
the width of the sweet spot is comparable to the present uncertainty in its location
(for Δy ≈Σy⋆/¯θ = 0.128). If we knew the exact location of the sweet spot, by
targeting it with experiment a (i.e., by choosing xf = x⋆= 1.5), the measurement
of θ would improve by a factor
RU ≡max(U(a))
max(U(e)) =
1 + L + x2
⋆/τ 2
⋆
1 + L + x2max/τ 2 = 5.2
(5.11)
with respect to what could be obtained with experiment e. However, since there
is uncertainty on the location of the sweet spot, the relevant ratio to decide which
experiment is best is the one between the maxima of the expected utilities, i.e.,
REU ≡max(E[U|o, a])
max(E[U|o, e]) = 3.3.
(5.12)
The expected utility for a (Eq. (5.9)) is plotted in the upper panel of Figure 5.1,
and it is maximized by the choice xf = 1.46. For comparison, the expected utility
for e peaks at xmax. It is clear that in this case experiment a is the better choice,
since REU > 1. For the case shown in the lower panel of Figure 5.1, however,
present-day uncertainty is much larger than the width of the sweet spot (here we
have chosen Δ = 0.05, σ = 0.5, τ = 0.1 and all other quantities as before).
Experiment a is still much more powerful than e, as at the sweet spot it would
deliver an improvement by a factor RU = 6.2 over what e can obtain at xf = xmax.
However, the expected utility for a (plotted in the lower panel of Figure 5.1) also
peaks at the largest possible lever arm, as the location of the sweet spot cannot be
reliably determined given the uncertainty in the slope. Indeed, the estimate of x⋆
from the current posterior mean for the slope is x⋆= 1.35, where the noise of a
is degraded by a factor of ∼90 with respect to its best performance, which occurs
at x⋆= 1.5. In this case, the ratio of the expected utilities at their maximum is
REU = 0.65, indicating that experiment e ought to be preferred.
Many other elements can be incorporated in the utility function as appropri-
ate. For example, in the above toy problem we might want to add a dimension

106
Bayesian experimental design and model selection forecasting
of ‘risk’ in the utility, describing the probability that the experiment will not de-
liver an improvement over present-day knowledge. In this case, strategy e has a
risk which is close to 0, as its accuracy is constant as a function of xf and the re-
sulting datum will improve current constraints. Experiment a might instead have a
non-negligible risk factor, stemming from the fact that it might miss its sweet spot
altogether and hence deliver a very noisy measurement that will not add to current
data. Furthermore, every real measurement costs time and money, and this sets up
a tension between the desired outcome and the resources available to achieve that
outcome. Therefore, a real-world experiment optimization has to cope with com-
plicated boundary conditions in high-dimensional parameter spaces, describing for
example the accuracy of a future measurement given a ﬁxed observational time for
different observation strategies. This is a highly non-trivial computational prob-
lem, and in the next section we introduce the Fisher matrix approach which allows
a much faster evaluation of the quantities involved. Then we look in more detail
at a speciﬁc example, the optimization of the Wide-ﬁeld Fibre-fed Multi-Object
Spectroscopic (WFMOS) survey.
5.3 Experiment optimization for error reduction
The conceptually straightforward approach to evaluating the expected utility is to
construct virtual experiments, by simulating data D from a proposed experiment
e, assuming a ﬁducial value for the parameters of interest, ˆθ. The procedure is
then repeated for all possible values of the ﬁducial parameters and each outcome
is weighted by the corresponding current posterior probability (both within and
across models, as in Eq. (5.2)). The utility is computed in each case and the ex-
perimental choice that maximizes it is selected. In most cases, however, this pro-
cedure is computationally prohibitive. A massive shortcut in computational time
is provided by the Fisher matrix method, which gives a lower bound (the Cram´er-
Rao bound) to the accuracy with which the parameters can be measured using a
quadratic estimator. Here we employ it as a simple approximation to the likeli-
hood for a future experiment, allowing one to assess the performance of a future
measurement without time-consuming generation of virtual realizations of data.
5.3.1 Fisher matrix error forecast
Consider the likelihood function for a future experiment with experimental pa-
rameters e, L(θ|e) ≡P(Dˆθ|θ, e), where Dˆθ are simulated data from the future
experiment assuming that the true parameter values are ˆθ (within a given model).
We can Taylor expand the log-likelihood around its maximum-likelihood value.
By deﬁnition, at an extremum the ﬁrst derivatives vanish, and the shape of the
log-likelihood in parameter space is approximated by the Hessian H,

5.3 Experiment optimization for error reduction
107
ln L(θ|e) ≈ln L(θML) + 1
2

ij
(θi −θML
i
)tHij(θj −θML
j
),
(5.13)
where
Hij ≡∂2 ln L
∂θi∂θj
⏐⏐⏐
θML,
(5.14)
and the derivatives are evaluated at the maximum-likelihood point. By taking
the expectation of Eq. (5.13) with respect to many data realizations, we can
replace the maximum-likelihood value with the ﬁducial value, ˆθ, as the maximum-
likelihood estimate is unbiased, i.e., ⟨θML⟩= ˆθ. We then deﬁne the Fisher infor-
mation matrix as the expectation value of the Hessian,
Fij ≡⟨Hij⟩.
(5.15)
The inverse of the Fisher matrix, F −1, is an estimate of the covariance matrix
for the parameters, and it describes how fast the log-likelihood falls (on average)
around the maximum likelihood value. (Kendall & Stuart 1977; Tegmark et al.
1997). In general, the derivatives depend on where in parameter space we take
them (except for the simple case of linear models), hence it is clear that F is a
function of the ﬁducial parameters.
Once we have the Fisher matrix, we can give estimates for the accuracy on the
parameters from a future measurement. Interpreting the likelihood in a Bayesian
sense, and taking the prior as a Gaussian with inverse covariance matrix (i.e., Fisher
matrix) Π, then the above approximation for the likelihood means that the posterior
will also be a Gaussian with Fisher matrix given by F = F + Π. Furthermore, if
the likelihood is much more informative than the prior, the posterior Fisher matrix
is approximately equal to the likelihood Fisher matrix, F ≈F. If we are only
interested in a subset of the parameters, then we can marginalize easily over the
others: computing the Gaussian integral over the unwanted parameters is the same
as inverting the Fisher matrix, dropping the rows and columns corresponding to
those parameters (keeping only the rows and columns containing the parameters of
interest) and inverting the smaller matrix back. The result is the marginalized Fisher
matrix ¯F. For example, the one-sigma error for parameter i, marginalized over all
other parameters, is simply given by σi =

(F−1)ii. It is also straightforward
to combine constraints from different independent experiments: multiplication of
their likelihoods just leads to the sum of the corresponding Fisher matrices.
It remains to compute the Fisher matrix for the future experiment. This can
be done analytically for the case where the likelihood function is approximately
Gaussian in the data, which is a good approximation for many applications of

108
Bayesian experimental design and model selection forecasting
interest. We can write for the log-likelihood
−2 ln L = ln |C| + (D −μ)tC−1(D −μ),
(5.16)
where D are the (simulated) data that would be observed by the experiment and, in
general, both the mean μ and covariance matrix C may depend on the parameters θ
we are trying to estimate. The expectation value of the data corresponds to the
true mean, ⟨D⟩= μ, and similarly the expectation value of the data matrix Δ ≡
(D −μ)t(D −μ) is equal to the true covariance, ⟨Δ⟩= C. Then it can be shown
(see, e.g., Tegmark et al. 1997) that the Fisher matrix is given by
Fij = 1
2tr

AiAj + C−1⟨Δ,ij⟩

,
(5.17)
where Ai ≡C−1C,i and the comma denotes a derivative with respect to the para-
meters, for example C,i ≡∂C/∂θi. The fact that this expression depends only on
expectation values, and not on the particular data realization, means that the Fisher
matrix can be computed from knowledge of the noise properties of the experiment
without having to go through the step of actually generating any simulated data.
The speciﬁc form of the Fisher matrix then becomes a function of the type of
observable being considered and of the experimental parameters e.
Explicit expressions for the Fisher matrix for cosmological observables can be
found in Tegmark et al. (1997) for cosmic microwave background data, in Tegmark
(1997) for the matter power spectrum from galaxy redshift surveys (applied to
baryonic acoustic oscillations in Seo & Eisenstein 2003) and in Hu and Jain (2004)
for weak lensing. A useful summary of Fisher matrix technology is given in the
Dark Energy Task Force report (Albrecht et al. 2006).
5.3.2 Utility functions for error minimization
Using the Fisher matrix technology sketched above, we can compute very quickly
what the expected posterior error on the parameters will be (or at least a lower
bound) for a given ﬁducial choice of cosmological parameters. From the Fisher
matrix F (its dependence on the ﬁducial parameters is understood), we can deﬁne
several interesting utility functions associated with the experiment. Some popular
choices are:
• The determinant of the Fisher matrix, |F| (often called D-optimality), which is
inversely proportional to the square of the parameter volume enclosed by the
posterior. This is close to the Dark Energy Task Force (Albrecht et al. 2006)
Figure of Merit, [σ(wa) × σ(wp)]−1, which is in fact

| ¯FDE| where ¯FDE is
the marginalized 2 × 2 Fisher matrix for the dark energy parameters w0 and wa
(deﬁned in Eq. (5.20) below).

5.3 Experiment optimization for error reduction
109
• The logarithm of the determinant, ln |F|. For a ﬁxed ﬁducial model it does not
matter if we maximize the determinant or its logarithm, but it may be important
when averaging over ﬁducial models when computing the expected utility.
• The trace of the Fisher matrix, trF, or its logarithm: this is proportional to the
sum of the variances, and is often called A-optimality.
• The information gain H from performing the experiment (also often called Kull-
back–Leibler divergence),
H(ˆθ, e, o) =

P(θ|ˆθ, e, o) ln P(θ|ˆθ, e, o)
P(θ|o)
dθ
(5.18)
= 1
2

ln |F| −ln |Π| −tr[1 −ΠF−1]

,
(5.19)
where the prior P(θ|o) has Fisher matrix Π and P(θ|ˆθ, e, o) is the posterior
from future data assuming the ﬁducial parameters ˆθ, with the Fisher matrix given
by F.
In many cases, D-optimality is a good indicator of the overall size of the error
across all parameter space. But it does not tell us whether we are dealing with a
roughly spherical error region, or with a very narrow and elongated one with the
same volume, which is the hallmark of strongly degenerate directions in param-
eter space. A-optimality, on the other hand, will prefer a spherical error region,
but may not necessarily select the one with the smallest volume. The informa-
tion gain represents a compromise between these two possibilities and also has a
direct information-theoretical interpretation. Additionally, we may be able to toler-
ate larger errors in some parameters than in others, depending on the science goals
of the experiment. All these considerations have to enter the choice of the utility
function. Below we discuss a somewhat different case, where we do not try to min-
imize error bars but are instead interested in model comparison questions, leading
to a new class of utility functions, discussed in Section 5.4.
We now have a framework at our disposal for computing the utility at a given
ﬁducial point in parameter space, ˆθ. We now turn to discussing an application of
this procedure to a speciﬁc real-world example, which tries to assess quantitatively
the expected performance of future experiments in terms of their ability to reduce
the error on certain cosmological parameters as a function of experimental para-
meters e.
5.3.3 Application to cosmology: optimization of the WFMOS survey
We consider an experiment for measuring the parameters of the mysterious dark
energy based on the WFMOS design (Glazebrook et al. 2005; Bassett, Nichol &
Eisenstein 2005), following Parkinson et al. (2007). WFMOS measures the

110
Bayesian experimental design and model selection forecasting
Table 5.1. Survey parameters in each regime (high- and low-redshift
observations) for the WFMOS optimization problem.
Survey parameter
Symbol
Survey time
τ
Area covered
A
Number of redshift bins
nbin
Midpoint of ith redshift bin
zi
Half-width of ith redshift bin
dzi
expansion history of the Universe through geometrical measurements of standard
rulers, in this case Baryon Acoustic Oscillations (BAO) frozen into the large-scale
structure of the Universe at early times. To measure the BAO, it is necessary to
conduct a survey of the positions and redshifts of millions of galaxies over large
volumes of the Universe.
The questions of experimental design are now much more complicated. Instead
of just two different experiments (as in the example above) we have a vast ensemble
of possible experimental conﬁgurations to consider. We can ask questions such as:
• What are the optimal redshifts for BAO observations?
• What is the best combination of areal coverage and target density?
• What type of galaxies are the best targets?
To answer these questions, each different survey geometry s is deﬁned in terms
of survey parameters that completely specify the survey. The survey is split into ob-
servational regimes (high- and low-redshift observations), and then for each regime
we deﬁne τ (survey time), A (survey area), nbin (number of redshift bins), zi (me-
dian redshift of the ith redshift bin), and dzi (half-width of the ith redshift bin). All
these parameters are listed in Table 5.1.
In selecting a survey, we are also constrained by the technical characteristics of
the proposed instrumentation. These form constraint parameters that limit the pos-
sible range of values of the survey parameters. Here we only consider a WFMOS-
like experiment, whose important properties are summarized in Table 5.2. These
include the ﬁeld of view of the telescope (FoV), which gives the amount of area that
can be observed per telescope pointing, and the number of ﬁbres (nﬁbres), which
limits the number of objects that can be observed simultaneously. The total observ-
ing time serves as a good example of a constraint parameter. Since we treat these
two redshift regimes separately, they have separate areas and exposure times, so
the sums of the two observing times must equal this total.
The baryon acoustic oscillations offer two different measurements of the cos-
mological parameters, depending on which direction they are measured in. The

5.3 Experiment optimization for error reduction
111
Table 5.2. Constraint parameters for the WFMOS survey.
Constraint parameter
Value
nﬁbres
3000
FoV
1.5o diameter
Aperture
8 m
Fibre diameter
1′′
Overhead time
10 minutes
Minimum exposure time
15 minutes
Maximum exposure time
10 hours
Total observing time
1500 hours
oscillations tangential to our line of sight allow us to measure the angular-diameter
distance to that redshift, dA(z), while measurement of the radial modes allows us to
measure the Hubble expansion rate H(z) directly. The angular-diameter distance
in a ﬂat universe containing only matter and dark energy is given by
dA(z) =
c
(1 + z)
 z
0
dz′
H(z′) ,
where H(z′) is the Hubble function given by
H(z′) = H0

Ωm(1 + z′)3 + ΩDEf(z′)
1/2 ,
where we have deﬁned
f(z′) ≡exp
 z′
0
3(1 + w(x))
1 + x
dx .
Thus both the angular-diameter distance and the Hubble rate depend on the amount
of matter (Ωm) and dark energy (ΩDE) in the Universe, and on how w(z), the
equation of state of dark energy, evolves with redshift. In a ﬂat universe, ΩDE =
1−Ωm, which leaves only H0, Ωm and w(z) to be measured. For our optimization,
we employ the following parameterization for the equation of state (Chevalier &
Polarski 2001; Linder 2003):
w(a) = w0 + wa(1 −a) ,
(5.20)
where a is the scale factor of the Universe (normalized to unity today), a = 1/
(1 + z). We therefore wish to optimize the WFMOS survey so as to obtain the best
constraints on w0 and wa, while marginalizing over H0 and Ωm. For these last two
parameters, we recast them as Ωmh2 and Ωm (where h = H0/100 km s−1 Mpc−1),
and we further assume a ﬂat universe.
For the utility (or FoM) appearing in Eq. (5.1) we use
U(e, ˆθ) = ln |F|,
(5.21)

112
Bayesian experimental design and model selection forecasting
(see, e.g., Bassett 2005; Bassett, Parkinson & Nichol 2005), the logarithm of the
determinant of the posterior Fisher matrix for the parameters (see Section 5.3),
which we estimate using the method outlined in Glazebrook and Blake (2005).
Since |F| is inversely proportional to the square of the volume of the posterior
ellipsoid, a larger value for our utility function corresponds to smaller errors on the
parameters of interest.
The computational problem of searching through the survey parameter space (S)
is that the available volume of possible surveys is very large. Therefore, we use a
Monte Carlo Markov chain (MCMC) procedure to ﬁnd the maximal value of the
FoM, and the survey conﬁguration associated with it. As there may be a number of
degenerate minima in this parameter space (i.e., surveys with very different conﬁg-
urations but similar FoMs), we use the simulated annealing algorithm (Kirkpatrick
et al. 1983; Cerny 1985; also described in Chapter 1) to cool and heat the chains
generated by the MCMC process.
The optimization proceeds as follows for a ﬁxed choice of ﬁducial parame-
ters ˆθ, which in our case correspond to the cosmological constant model, i.e.,
ˆw0 = −1, ˆwa = 0:
(i) Select a test survey conﬁguration (s) of survey parameters (area coverage,
exposure time, redshifts etc.) from the experimental parameter space (S).
(ii) Estimate the number density of galaxies that will be observed by this test
survey using luminosity functions.
(iii) Estimate the error on dA(z) and H(z) using scaling relations given in Blake
et al. (2006).
(iv) Calculate the Fisher matrix of parameters, using distance data plus other
external constraints that will be available from future experiments.
(v) Use the Fisher matrix to calculate the Figure of Merit for survey s, as given
by Eq. (5.21).
(vi) Repeat steps 1 through 5, conducting an MCMC search over the survey
conﬁguration parameter space S, attempting to maximize the utility.
We ﬁrst focus on ﬁnding the optimal trade-off between exposure time and areal
coverage for different galaxy populations. At low redshift, galaxies are observed
either by line emission (active, star-forming ‘blue’ galaxies) or by continuum emis-
sion (passive, ‘red’ galaxies). At high redshift, the LBGs (Lyman Break Galax-
ies) can be observed using either method, but we assume line emission only in
this case. We ﬁx the two redshift bins at 0.5 < z < 1.3 for ‘low redshift’ (or
zlow = 0.9 and dzlow = 0.4) and 2.5 < z < 3.5 for ‘high redshift’ (zhigh = 3.0
and dzhigh = 0.5) and the total time spent observing in each regime, τlow = 800 hr
and τhigh = 700 hr. This leaves us with only two free survey parameters in each

5.3 Experiment optimization for error reduction
113
FoM
arealow
0
1000
2000
3000
0
2
4
areahigh
0
1000
2000
3000
0
2
4
FoM
exposure timelow
0
0
200
400
600
2
4
exposure timehigh
0
200
400
600
0
2
4
Fig. 5.2. The distribution of survey parameters by FoM for the area (in square degrees,
top panels) and exposure time (in minutes, bottom panels) in the low (z ∼1) and high
(z ∼3) redshift bins. The shaded regions delimit surveys compatible with experimen-
tal constraints. The darker shade is for surveys observing line-emission galaxies at low
redshift, and the lighter shade for continuum emission. Both cases observe line-emission
galaxies at high redshift. Line emission galaxies appear to be a superior target in terms of
the FoM used here.
redshift bin, namely the area and exposure time for each pointing. Note that since
the total observing time is ﬁxed, the exposure time and area are linked through the
number of repeat pointings to each ﬁeld of view. This parameter is selected by an
algorithm that makes best use of the available number of ﬁbres.
The results shown in Figure 5.2 indicate that at low redshift, the ‘blue’ galaxy
population is a signiﬁcantly better choice than the ‘red’ population, as the best
FoM for surveys targeting ‘blue’ galaxies is a factor of 3.2 ≃ln(25) higher. This
corresponds to a factor of
√
25 ≃5 improvement in the area of the error ellipse in
the w0–wa parameter space. This advantage is due to the speed at which redshifts
can be obtained for these ‘blue’ galaxies (from the O[II] emission line), which
allows the survey to cover more area per unit time compared with targeting ‘red’
galaxies (even though they are more biased). On the other hand, the ﬂatness of
the FoM for the high-redshift bin (both as a function of area and exposure time)
indicates that these parameters do not have a large impact on the FoM. It is also

114
Bayesian experimental design and model selection forecasting
found that observing either line or continuum emission galaxies in the high-redshift
bin does not improve the FoM, no matter what type of galaxies are observed at low
redshift. This is due to the small energy density of the dark energy at high redshift,
hence to the smaller constraining power of the observations in this regime.
Although it is possible to optimize simultaneously for all the survey parameters
in Table 5.1, it is perhaps more interesting to consider the possibility of optimizing
some of the constraint parameters instead. Constraint parameters are built in at an
instrument level, and therefore cannot be considered as parameters determining the
survey strategy – rather, the optimization of constraint parameters must be carried
out before the instrument is even designed. Some constraint parameters will be
unbounded by the FoM in one direction. For example, a survey that runs for 5 years
will always do better than a survey that lasts for only 3 years. However, while the
FoM increases as the total survey time is increased, it may quickly asymptote to
some maximum as a function of other constraint parameters, such as for example
the number of ﬁbres in the instrument.
We investigate the optimal number of ﬁbres by ﬁnding the best Figure of Merit
for a survey with a single bin at low redshift as a function of the number of ﬁbres,
while all other survey parameters are allowed to vary. We also test the dependence
on our models of the galaxy number counts, by including pessimistic (and opti-
mistic) models which translate into roughly a 50% deﬁcit (surplus) of galaxies.
Here we assume that the ﬁbres are able to access any part of the ﬁeld of view.
The results can be seen in Figure 5.3. Looking at the left panel, the optimal
value of the number of ﬁbres for a single low-redshift bin is found to be equal
to or greater than around 10 000 when sampling ‘blue’ galaxies and ≥750 for
‘red’ galaxies. At this point the galaxies are well sampled (including the effects
of bias) so that shot noise is negligible. Neither strategy is affected much by the
change in number counts because the amount of excess/deﬁcit in number density
is small.
This optimal number of 10 000 ﬁbres for line-emission galaxies may seem rather
large, especially as the instrument being considered so far in the design proposal
only has 3000 ﬁbres. Furthermore, a detailed analysis shows that the optimal sur-
vey conﬁguration for 10 000 ﬁbres is only actually observing around 6000 galaxies,
with the remaining targets possessing emission-line ﬂuxes that fall below the min-
imum S/N level and produce failed redshifts. Hence such a conﬁguration wastes
roughly 40% of the instrument capacity. We might try to ﬁnd the most efﬁcient use
of ﬁbres by plotting the FoM per ﬁbre as a function of ﬁbre number, as shown on
the right-hand plot of Figure 5.3. For line-emission galaxies (blue band), the most
efﬁcient number is about 2000, but this has a lower FoM (5.0, compared with 5.7
for 10 000 ﬁbres). A happy medium would be around 3000–4000, where the FoM
is higher, but the efﬁciency is still reasonable.

5.4 Experiment optimization for model selection
115
Fig. 5.3. The behaviour of the FoM of the best survey as the total number of spectroscopic
ﬁbres (N) changes for line emission (solid curve) and continuum (dashed) galaxies for a
single bin at low redshift. We also include the effect of changing the number count models
of the galaxies (‘blue galaxies’ band for line-emission galaxies and ‘red galaxies’ band
for continuum-emission galaxies). Changes in the number count models leave the optimal
value largely unchanged. On the right we plot the FoM−ln(N) against the number of ﬁbres
to ﬁnd the most efﬁcient use of the instrument. For line-emission (blue) galaxies, the most
efﬁcient number of ﬁbres is around 2000.
5.4 Experiment optimization for model selection
In cosmology, often the question is less about the precise value of a given param-
eter, and more about the probability of models. For example, we are more inter-
ested in the question whether space is ﬂat or not, and whether the dark energy is
a cosmological constant or not. These are questions of model selection rather than
parameter constraints. In many cases we may want to optimize our experiment to-
wards deciding that issue rather than to deliver minimal error bars. This section is
dedicated to the issue of experimental optimization in view of answering model
selection questions.
5.4.1 Quantifying experimental capabilities using Bayes factors
Let us recall that the prime tool for Bayesian model comparison is the Bayes factor
B, giving the change in relative plausibility for two models in the light of the data
(as discussed in detail in Chapter 4). A Bayes factor plot chalks out the region in
parameter space over which a simulated future survey can convincingly rule out
one model in favour of the other (Mukherjee et al. 2006). The aim is to design
the experiment so as to maximize the region where the models in question can
be clearly distinguished. In a later section, we consider instead the problem of
obtaining probabilities for the different verdicts, i.e., the problem of Bayes factor
forecast.

116
Bayesian experimental design and model selection forecasting
We consider here the case of nested models, i.e., we assume that the parameters
of the more complex model (M1) can be written as θ = (φ, ψ) with the simpler
model (M0) obtained by setting the extra parameter(s) ψ = 0. Then the procedure
for producing a Bayes factor plot is as follows:
(i) Select an experimental conﬁguration, e.
(ii) Fix the common parameters in both models to a ﬁducial value, ˆφ, usually
the current posterior mean. In principle, one should marginalize over them,
as well, but if the future error on the parameters of interest ψ does not de-
pend strongly on the assumed value for φ, this approximation is sufﬁcient.
(iii) Consider possible values for ψ that cover the entire theoretically motivated
range for them under M1.
(iv) For each ﬁducial choice, ψ = ˆψ generate simulated data with the proper-
ties expected of experiment e. When employing a Fisher matrix approach,
simulated data are further averaged over realizations.
(v) From the simulated data D, compute the evidences for models M0 and
M1. Build the Bayes factor B01 as the ratio of the evidences, B01 ≡
P(D|M0)/P(D|M1).
(vi) Plot ln B01 as a function of ˆψ.
(vii) Deﬁne a suitable model comparison utility function; for example, the vol-
ume in ψ parameter space where ln B01 exceeds a certain threshold, cor-
responding to the region where e will be able to discriminate strongly
between the two models (see next section for an example).
For the nested models considered here, a computationally economic method to
compute Bayes factors is the Savage–Dickey density ratio (SDDR; see Trotta
(2007a) and references therein). Under mild conditions for the priors, the Bayes
factor between M0 and M1 is given by
B01 = P(ψ|D, M1)
P(ψ|M1)
0000
ψ=0
,
(5.22)
i.e., the evaluation of the Bayes factor of two nested models only requires the prop-
erly normalized value of the marginal posterior for the extended model at ψ = 0
(the value predicted under the simpler model). We can then estimate the Bayes
factor from a future observations from the predicted Fisher matrix alone (see, e.g.,
Kunz et al. 2006; Heavens et al. 2007), as we can approximate the posterior as
a Gaussian with mean given by the ﬁducial parameters ˆψ and Fisher matrix F,

5.4 Experiment optimization for model selection
117
obtained as explained in Section 5.3. Then using Eq. (5.22), the Bayes factor in
favour of the simpler model that one would obtain by performing the experiment is
ln B01( ˆψ, e, o) = 1
2 ln | ¯F|
|¯Π| −1
2
ˆψt ¯F ˆψ,
(5.23)
where ¯Π and ¯F are the marginalized prior and posterior Fisher matrices for the pa-
rameters ψ, respectively. Recalling that ln B01 > (<) 0 favours the simpler (more
complex) model, the ﬁrst term on the right-hand side represents the ‘Occam’s ra-
zor’ effect, which penalizes models with a large volume of wasted parameter space,
i.e., those for which the parameter space volume | ¯F|−1/2 that survives after arrival
of the data is much smaller than the initially available parameter space under the
model prior, |¯Π|−1/2. The second term, on the contrary, disfavours the simpler
model if ˆψt ¯F ˆψ ≫0, i.e., if the ﬁducial value of the parameters ˆψ is far away from
the predicted value ψ = 0 under the simpler model.
The use of the Bayes factor plots to quantify experimental capabilities is quite
distinct, both philosophically and operationally, from the use of parameter error
forecasts we considered above. In fact, many experiments (e.g., dark energy exper-
iments) are motivated principally by model selection questions (e.g., does the dark
energy density evolve with time?) and so their performance should be quantiﬁed
by their ability to answer such questions. Bayesian model selection accords special
status to the ψ = 0 model as being a well-motivated lower-dimensional model,
which in Bayesian terms is rewarded for its predictiveness in having a smaller
prior volume. Parameter estimation analyses do not recognize a special status for
such models. Furthermore, model selection criteria provide a more conservative
threshold than signiﬁcance tests for the inclusion of new parameters in a model, as
discussed in Trotta (2008) and Gordon and Trotta (2007). Another feature is that
model selection analyses can also accrue positive support for the simpler model,
whereas signiﬁcance tests can only fail to reject the simpler model. Finally, we
notice that in Bayes factor plots, the data are simulated at each point of the param-
eter space of the more complex model and then compared with the simpler model,
whereas parameter error forecasts are plotted around only selected ﬁducial refer-
ence values for the parameters (often just one). In particular, in the latter case the
data are usually simulated for a model that people hope to exclude (i.e., the simpler
model) rather than the true model which would allow that exclusion.
Set against these advantages, the only disadvantages of the Bayes factor method
are that it is computationally more demanding, and that its conceptual framework
has yet to become as familiar as that of parameter estimation.

118
Bayesian experimental design and model selection forecasting
−2
−1.5
−1
−0.5
−1
−0.5
0
0.5
1
w0
wa
SNLS SN−Ia
−2
−1.5
−1
−0.5
−1
−0.5
0
0.5
1
w0
wa
WFMOS BAO
Fig. 5.4. Bayes factor forecasts for the SNLS supernovae survey and the WFMOS BAO
survey. Contours are shown for ln B01 = 0, −2.5 and −5 (from the inside out). They
delineate regions where the experiment will gather positive evidence in favour of a cos-
mological constant (ln B01 > 0), or on the contrary moderate (−5.0 < ln B01 < −2.5)
or strong (ln B01 < −5.0) evidence in favour of evolving dark energy. An independent
measurement of Ωm to ±0.01 accuracy is assumed.
5.4.2 Application: dark energy vs. a cosmological constant
A relevant example from cosmology is again the dark energy equation of state.
Parameterizing the equation of state as in Eq. (5.20), one would like to determine
which future survey will be able to distinguish more strongly between a model
with (w0 = −1, wa = 0) (i.e., dark energy in the form of a cosmological constant,
or the ΛCDM model), and one in which both these parameters need to be varied
in order to ﬁt the data better (dynamical dark energy, Liddle et al. 2006). In this
section we give examples of Bayes factor plots in the w0−wa space and construct
model selection based utility functions with respect to which future surveys can be
optimized.
As an example, consider the Bayes factor plots shown in Figure 5.4, based on
Mukherjee et al. (2006). The innermost contoured region is where the evidence of
the ΛCDM model is greater than that of the evolving dark energy model (ln B01 >
0). The outer contours show ln B01 = −2.5 and −5, levels that suggest moderate
and strong evidences in favour of the evolving dark energy model, respectively. As
with parameter estimation contours, the smaller the contours the more powerful the
experiment is. Since the logarithms of the Bayes factors are additive, if more than
one of these surveys is realized, or if there are two independent parts to a survey,
their Bayes factor plots can be added together to give a net Bayes factor plot. Flat
priors in the ranges −2 < w0 < −0.333 and −1.333 < wa < 1.333 have been
used for the model comparison, and we comment on the prior dependence further
below.

5.4 Experiment optimization for model selection
119
Table 5.3. Two experimental Figures of Merit based on model selection
capabilities: the inverse of the area in the ˆw0– ˆwa plane where ln B01 exceeds
−2.5, and the value of ln B01 at ˆw0 = −1 and ˆwa = 0 (cosmological constant
model). A larger value for the former means that the experiment has a better
model discrimination capability, i.e., it will not confuse an evolving dark energy
model for ΛCDM. The second FoM is called ‘MaxEv’ and it measures the
maximum evidence with which the experiment would support ΛCDM were it the
true model. Both FoMs are additive between surveys and for independent probes
of dark energy within the same survey. The value in parenthesis is normalized to
SNLS.
Experiment
Inverse area
MaxEv
Type of observation
SNLS
2.0 (1.0)
3.7 (1.0)
Supernovae (ground)
SNAP
2.9 (1.4)
4.5 (1.2)
Supernovae (space)
JEDI (SN)
5.3 (2.6)
5.0 (1.3)
Supernovae (space)
ALPACA
12.5 (6.2)
6.1 (1.6)
Supernovae (ground)
WFMOS
3.8 (1.9)
4.8 (1.3)
BAO (ground)
JEDI (BAO)
25.0 (12.5)
6.0 (1.6)
BAO (space)
In addition to plotting Bayes factor contours, one can further compress the in-
formation on how powerful an experiment is by computing the inverse of the area,
within a particular contour level, to give a single number summarizing its utility.
Table 5.3 summarizes these inverse areas, expressed in coordinate units, for six ex-
periments considered in Mukherjee et al. (2006), showing the inverse of the area
where ln B01 exceeds −2.5. Note that this region corresponds to the parameter area
in which an experiment cannot strongly exclude ΛCDM (i.e., regions where the
experiment can confuse the models) and hence a small area corresponds to more
discriminative experiments, hence to a larger utility.
We can also consider the ability of the experiments to rule out the evolving dark
energy model in favour of ΛCDM, rather than the opposite which we have focused
on thus far. Unlike parameter estimation methods, Bayesian model selection can
offer positive support in favour of the simpler model. Because the simpler model is
nested within the dark energy model, it can never ﬁt the data better, but it can beneﬁt
from the Occam’s razor effect of its smaller parameter space. All one needs to do is
read off the Bayes factor for the case where the ﬁducial model is ΛCDM. Table 5.3
shows ln B01 at ˆw0 = −1 and ˆwa = 0, i.e., when ΛCDM is the true model. We
call this FoM ‘MaxEv’, for ‘Maximum Evidence’. We ﬁnd that this value is above
2.5 for all surveys, and above 5 for several of them. Thus many of the surveys are
capable of accumulating strong evidence supporting ΛCDM over evolving dark

120
Bayesian experimental design and model selection forecasting
energy. Thus the MaxEv can be seen as another utility function quantifying the
power of experiments. Note that this utility is also additive.
Clearly, the absolute values of the two FoMs above do depend on the chosen
prior range. While the choice adopted here is somewhat arbitrary, if the prior space
were reduced for instance by a factor of 2, that would decrease ln B01 by at most
ln 2 ≃0.69, and this would not signiﬁcantly affect our contours or conclusions,
which are based on differences in ln B01 of 2.5 and 5. As a consequence, the inverse
area FoM would not be greatly affected. The MaxEv FoM is more sensitive to the
prior ranges chosen for the dark energy parameters, which control the Occam’s
razor effect. However, the relative strength of surveys, as given by the ratio of their
FoMs, is largely unaffected by prior changes. It is important to remember that these
results also retain some parameterization dependence – dark energy may turn out to
be something entirely different from the parameterization adopted here, a problem
common to more traditional uncertainty forecasts.
5.5 Predicting the outcome of model selection
The Bayes factor plots show the predicted outcomes of a future experiment as a
function of the ﬁducial parameters. A further step is to predict a probability distri-
bution for the future value of the Bayes factor itself – in other words, to derive a
predictive distribution over the outcome of a future model comparison, accounting
for both the current parameters and model uncertainty.
5.5.1 Predictive distributions
Assume for simplicity that we are considering only two models (the generalization
to an arbitrary number of nested models is simple), with model 0 nested within
model 1. Then the predictive distribution for the value of the posterior mean for
the parameters ¯θ from a future experiment e given the current data o is
P(¯θ|o, e) =

i
P(¯θ|o, e, M(i))P(M(i)|o)
(5.24)
=

i
P(M(i)|o)

P(¯θ|ˆθ(i), e, M(i))P(ˆθ(i)|o, M(i))dˆθ(i).
Here P(¯θ|ˆθ(i), e, M(i)) is the probability distribution for the posterior mean of θ
assuming (ˆθ(i), M(i)) are the true parameters and true model (the superscripts (i)
designate different model choices and the corresponding parameter sets) and for
a given choice of experimental parameters e for the future observation. P(ˆθ(i)|o,
M(i)) is the current posterior under model M(i), while different models are aver-
aged over with relative weight given by the outcome of the current model

5.5 Predicting the outcome of model selection
121
comparison result, P(M(i)|o). On the right-hand side, the posterior for the fu-
ture experiment can be estimated by using the Fisher matrix forecast technique
explained in Section 5.3. In words, Eq. (5.24) is a probability distribution for a
future observation e to measure a value ¯θ averaging over present-day uncertainty.
Let us consider a simple example, where the predictive distribution can be writ-
ten down analytically under some simplifying assumptions, but which is still rel-
evant for many real-world cases. As before, we consider a more complex model
M1 with free parameters θ = (φ, ψ) and a simpler, nested model M0 which is
obtained by setting the extra parameter ψ = 0. We further assume that the likeli-
hood Fisher matrix F does not depend on φ – in other words, that future errors do
not depend on the location in the subspace of parameters common to both models.
Then one can marginalize over the common parameters φ and therefore we can
assume without loss of generality that M1 has only one free parameter (ψ) com-
pared with model M0 with no free parameters. We take a Gaussian prior on the
extra parameter, ψ, centred around 0 and of unity standard deviation. The present-
day likelihood is a Gaussian of mean μ and variance σ2 (both are understood
to be expressed in units of the prior width and are thus dimensionless). Finally,
the marginal posterior for ψ from the future experiment can be computed using
Fisher matrix techniques. It follows that the distribution of the posterior mean, ¯ψ,
is a Gaussian centred around the ﬁducial parameter value ˆψ and has a variance
τ 2 (which will itself depend on the experimental parameters e). Then plugging
this into Eq. (5.24) and integrating over the value for the ﬁducial parameter ˆψ we
obtain
P( ¯ψ|o, e) ∝1
τσ exp

−
¯ψ2
2τ 2 −μ2
2σ2

+
1
(τ 2 + σ2 + τ 2σ2)1/2 exp

−1
2
( ¯ψ −μ)2 + σ2 ¯ψ2 + τ 2μ2
τ 2 + σ2 + τ 2σ2

,
(5.25)
where we have dropped irrelevant constants and assumed that the two models have
equal prior probability, P(M0) = P(M1) = 1/2. Equation (5.25) gives the prob-
ability of obtaining a value ¯ψ from a future measurement of precision τ, conditional
on the present accuracy σ around a measured value μ. The ﬁrst term on the right-
hand side stems from the simpler model. If either measurement ﬁnds a strong dis-
crepancy with the predictions of the simpler model that ¯ψ = 0 (i.e., for μ2/σ2 ≫1
or ¯ψ2/τ 2 ≫1), then this term will be negligible in the model averaging, because
the exponential →0. The second term represents the prediction assuming that the
more complex model is true. If the future experiment is considerably better than
the current one (i.e., for τ ≪σ < 1), then this term becomes small if the two
experiments give discordant outcomes (i.e., for ( ¯ψ −μ)/σ2 ≫1). Notice that in

122
Bayesian experimental design and model selection forecasting
the scenario τ ≪σ, the spread in the predictive distribution is dominated by σ, i.e.,
the accuracy of our prediction is limited by the extent of our present uncertainty, σ,
and not by how well the future experiment will be able to do (τ). This is very dif-
ferent from the result one would get by simply considering the usual Fisher matrix
forecast around a single ﬁducial point in parameter space, which does not take into
account the current uncertainty at all.
5.5.2 Predictive posterior odds distribution
We now wish to use the predictive distribution for the parameters to derive a prob-
ability distribution for the Bayes factor that a future experiment can obtain. This
is done as follows. For every value of the ﬁducial parameter ˆψ, one can derive the
posterior distribution that the future experiment would obtain if that was the true
value of the parameters, as we did above. To each posterior corresponds a value
of the Bayes factor that one would derive from such a measurement, as given in
Eq. (5.23). Each of such values of ln B01 has a weight given by the current poste-
rior probability that ˆψ is the correct value of the parameters, averaged over possible
choices of models, as well. If we have a series of MCMC samples drawn from the
current posterior for each model, then the distribution of samples is automatically
proportional to the probability of ˆψ being correct within that model. We only need
to weight the samples from each model by the corresponding model probability
obtained from the present-day model comparison. Then producing a histogram of
the values of ln B01 obtained from the samples gives a probability distribution for
the Bayes factor that will be obtained by the future experiment e. This procedure
has been introduced in Trotta (2007b), where it was called ‘Predictive Posterior
Odds Distribution’, or PPOD for short.
Going back to the one-dimensional example of the previous section, we can
write down the Bayes factor corresponding to a choice of ﬁducial parameters ˆψ, cf.
Eq. (5.23), as
ln B01( ˆψ, e, o) = 1
2 ln 1 + τ 2
τ 2
−1
2
ˆψ2 1 + τ 2
τ 2
.
(5.26)
Notice that setting ˆψ = 0 (corresponding to the future observation measuring the
predicted value of ψ under M0) the above expression gives the maximum odds
in favour of the simpler model one can hope to gather from a future measurement
of accuracy τ. This is the MaxEv utility introduced in Section 5.4.2, giving the
strength of evidence that an experiment can gather in favour of the simpler model.
5.5.3 Application: spectral index from the Planck satellite
As an example of the application of the PPOD technique, we consider the pre-
dictive probability for the Planck satellite to gather signiﬁcant evidence against a

5.5 Predicting the outcome of model selection
123
Fig. 5.5. Predictive distribution for the Planck satellite nominal mission, conditional on
current knowledge, showing the probability distribution of obtaining a certain value of the
spectral tilt, nS. The bump at nS = 1 corresponds to the probability associated with the
HZ model, obtained by model averaging. The nearly identical curves show the numerical
result from the MCMC chains and form the Gaussian approximation, Eq. (5.25). In the
inset, the four shaded areas, reading from left to right, delimit regions where the Bayes
factor from Planck will deliver strong evidence in favour of nS ̸= 1 (ln B01 < −5.0,
this region extending to all smaller nS values), moderate evidence for nS ̸= 1 (−5.0 ≤
ln B01 ≤−2.5), weak evidence for nS ̸= 1 (−2.5 ≤ln B01 ≤0.0) or favour nS = 1
(ln B01 > 0).
scale-invariant spectral index of perturbations, nS ̸= 1 (further details are given in
Trotta 2007b). Thus we have a more complex model M1 with a non-scale invari-
ant index and a simpler, nested model M0 predicting nS −1 = 0 (the so-called
‘Harrison–Zel’dovich’ (HZ) model). For the more complex model, considerations
of slow-roll inﬂation lead us to believe that deviations from scale invariance cannot
be too large, otherwise slow roll would break down. This can be approximately
quantiﬁed by selecting a Gaussian prior for nS under M1 centred around nS = 1
and with standard deviation ΔnS = 0.2. Current data then give a model compari-
son result that favours M1 with odds of order 10:1, or equivalently ln B01 ∼−2.5
(Trotta 2007a; Pahud et al. 2007; Kunz et al. 2006). This result seems to indi-
cate a moderate preference for a non-scale invariant spectrum, although it still falls
short of the ‘strong’ evidence threshold. The question is then to predict what model
selection result the Planck satellite will obtain from its higher quality measure-
ments of the CMB temperature and polarization power spectra.
Figure 5.5 shows the predictive probability distribution for the Planck satel-
lite to measure a certain value of nS. The two curves compare the full numerical

124
Bayesian experimental design and model selection forecasting
Table 5.4. Prediction for the probability of Planck to obtain different levels of
evidence in a model comparison of nS = 1 (HZ) against an
inﬂationary-motivated model with nS ̸= 1.
Integrated probability
Bayes factor range
Interpretation of Bayes factor
0.928
ln B01 < −5
Strong evidence against HZ
0.005
−5 < ln B01 < −2.5
Moderate evidence against HZ
0.006
−2.5 < ln B01 < 0
Weak evidence against HZ
0.061
ln B01 > 0
Evidence for HZ
computation using Eq. (5.24) and an MCMC chain and the approximation in
Eq. (5.25). The approximation is extremely good in this case, because the cur-
rent posterior is close to Gaussian, and the future errors forecast for Planck vary
only very mildly over the range of parameter space singled out by the present pos-
terior, hence assuming a constant τ as in Eq. (5.25) is justiﬁed. The bump in the
predictive distribution around nS = 1 corresponds to the probability associated
with the HZ model. We can then obtain the PPOD numerically and integrate it to
get the probability of the model comparison result from future data. This is given
in Table 5.4 as the probability for Planck to obtain different levels of evidence. We
can conclude that there is a very high probability (about 93%) that Planck will not
only conﬁrm but even strengthen the current model selection outcome favouring
nS ̸= 1, by obtaining evidence above the ‘strong’ threshold. On the contrary, there
is only about 6% of probability that future data will overturn the current verdict,
by favouring the HZ model instead (ln B01 > 0). An analysis of the dependence of
PPOD on the prior width for nS shows that these results are robust for physically
plausible choices of prior (Trotta 2007b).
5.6 Summary
We have seen that Bayesian methods offer powerful tools for the prediction of the
outcome of future experiments. The science return of a future measurement can be
encapsulated in utility functions that can be targeted at both error reduction and
model comparison questions. By exploring the dependence of the utility function
on experimental parameters and maximizing it, we can design measurements that
are optimally suited to the science goal being pursued.
The technique of model averaging can be employed to ‘hedge’ our predictions
against current parameters and model uncertainties that are thereby fully accounted
for in the optimization procedure. We can also produce probability distributions for
the outcome of a future observation and the ensuing model comparison, thereby

5.6 Summary
125
allowing one to assess proposed experiments in terms of their ability to answer
model selection questions.
The exploration of those techniques is only just beginning in cosmology and
many exciting developments will take place, given the necessity of optimizing our
science return from a limited amount of resources.
References
Albrecht, A. et al. (2006). Report of the Dark Energy Task Force, available as
astro-ph/0609591.
Bassett, B. A., 2005, Phys. Rev. D, 71, 083517.
Bassett, B. A., Nichol, R. C. and Eisenstein, D. J. (2005). Astron. Geophys., 46 (5), 5.26.
Bassett, B. A., Parkinson, D. and Nichol, R. C. (2005). Astrophys. J. 626, L1.
Blake, C., Parkinson, D., Bassett, B., Glazebrook, K., Kunz, M. and Nichol, R. C. (2006).
Mon. Not. Roy. Astron. Soc., 365, 255.
Cerny, V. (1985). J. Opt. Theory Appl., 45:1, 41.
Chevalier, M. and Polarski, D. (2001). Int. J. Mod. Phys. D, 10, 213.
Glazebrook, K. and Blake, C. (2005). Astrophys. J., 631, 1.
Glazebrook, K., Eisenstein, D., Dey, A. and Nichol, B. (2005). White paper to the Dark
Energy Task Force, available as astro-ph/0507457.
Gordon, C. and Trotta, R. (2007). Mon. Not. Roy. Astron. Soc., 382, 1859.
Heavens, A. F., Kitching, T. D. and Verde, L. (2007). Mon. Not. Roy. Astron. Soc., 380,
1029.
Hu, W. and Jain, B. (2004). Phys. Rev. D, 70, 043009.
Kendall, M. G. and Stuart, A. (1977). The Advanced Theory of Statistics, 4th edn. London
and High Wycombe: Grifﬁn & Co.
Kirkpatrick, S., Gelatt Jr., C. D. and Vecchi, M. P. (1983). Science, 220, 671.
Kunz, M., Trotta, R. and Parkinson, D. (2006). Phys. Rev. D, 74, 023503.
Liddle, A. R., Mukherjee, P., Parkinson, D. and Wang, Y. (2006). Phys. Rev. D, 74, 123506.
Linder, E. V. (2003). Phys. Rev. Lett., 90, 091301.
Loredo, T. J. (2003). AIP Conf. Proc., 707, 330, available as astro-ph/0409386.
Mukherjee, P., Parkinson, D., Corasaniti, P. S., Liddle, A. R. and Kunz, M. (2006). Mon.
Not. Roy. Astron. Soc., 369, 1725.
Pahud, C., Liddle, A. R., Mukherjee, P. and Parkinson, D. (2007). Mon. Not. Roy. Astron.
Soc., 381, 489.
Parkinson, D., Blake, C., Kunz, M., Bassett, B. A., Nichol, R. C. and Glazebrook, K.
(2007). Mon. Not. Roy. Astron. Soc., 377, 185.
Seo, H.-J. and Eisenstein, D. (2003). Astrophys. J., 598, 720.
Tegmark, M. (1997). Phys. Rev. Lett., 79, 3806.
Tegmark, M., Taylor, A. and Heavens, A. (1997). Astrophys. J., 480, 22.
Trotta, R. (2007a). Mon. Not. Roy. Astron. Soc., 378, 72.
Trotta, R. (2007b). Mon. Not. Roy. Astron. Soc., 378, 819.
Trotta, R. (2008). Contemp. Phys., 49, 2, 71.

6
Signal separation in cosmology
M. P. Hobson, M. A. J. Ashdown and V. Stolyarov
Signal separation is a common task in cosmological data analysis. The basic prob-
lem is simple to state: a number of signals are mixed together in some manner,
either known or unknown, to produce some observed data. The object of signal
separation is to infer the underlying signals given the observations.
A large number of techniques have been developed to attack this problem. The
approaches adopted depend most crucially on the assumptions made regarding the
nature of the signals and how they are mixed. Often methods are split into two
broad classes: so-called blind and non-blind methods. Non-blind methods can be
applied in cases where we know how the signals were mixed. Conversely, blind
methods assume no knowledge of how the signals were mixed, and rely on as-
sumptions about the statistical properties of the signals to make the separation.
There are some techniques that straddle the two classes, which we shall refer to
as ‘semi-blind’ methods. They assume partial knowledge of how the signals are
mixed, or that the mixing properties of some signals are known and those of others
are not.
There is a large literature in the ﬁeld of signal processing about signal separation,
using Bayesian techniques or otherwise. For any cosmological signal separation
problem, it is almost always the case that someone has already attempted to solve
an analogous problem in the signal processing literature. Readers who encounter a
problem of this type, which is not already addressed in the cosmological literature,
are encouraged to look further aﬁeld for existing solutions.
In this chapter, we shall describe the application of Bayesian methods to one par-
ticular cosmological signal separation problem, the separation of multi-wavelength
observations into their cosmological or astrophysical components. The techniques
described here are quite general, and can be applied to other signal separation prob-
lems as they stand or with only minor modiﬁcations. Throughout this chapter, we
will assume that the data are measurements only of the total intensity of the sky.
126

6.1 Model of the data
127
It is straightforward, in principle, to extend the methods presented to the case in
which one also measures polarization, but we will not discuss this generalization.
6.1 Model of the data
The ﬁrst step towards performing a Bayesian signal separation is to deﬁne a model
for the data. Focusing on our astrophysical application, we assume that (part of)
the sky is observed in a number of frequency channels, Nν, and that the sky signal
at each frequency is a linear combination of Nc components of emission. We also
assume that the data are convolved by a telescope beam (point spread function),
which may be different in each channel, and that the instrumental noise is additive.
Thus, the general model for the data in channel ν observed in the direction x is
dν(x) = W(x)
2
Bν(|x −x′|)
Nc

c=1
Fνc(x′) sc(x′) dx′ + nν(x)
3
,
(6.1)
where sc(x) is the spatial emission from component c (at some reference frequency
ν0) and nν is the noise in channel ν. The coefﬁcients Fνc(x) determine how the
components are mixed to make the data; they form the components of the mixing
matrix. They may depend on other parameters and be a function of position (as
indicated), although they are often assumed to be spatially invariant. The beam
Bν(r) in channel ν is assumed not to change with position on the sky. Furthermore,
we assume that it is circularly symmetric about its peak.1 The window function
W(x) represents the region of the data maps to be analyzed: it may equal unity
everywhere; it may be zero in masked regions (or regions missing data) and unity
elsewhere; or a more general (apodized) form for the window function may be
used. We assume here that the same window function is applied to all frequency
channels. Finally, it is worth noting that if the noise term nν(x) in Eq. (6.1) is set
to zero, the resulting expression is sometimes termed the predicted data, noiseless
data or data model, and is often denoted by mν(x); we shall use this notation in
Section 6.5.1.
We note that, in attempting to infer the components sc(x) from the data deﬁned
in Eq. (6.1), one is not only performing a signal separation, but also a simultaneous
deconvolution of the instrumental beams. Some methods do attempt this joint task,
but many others do not perform the deconvolution. For the latter class of methods,
it is usual to convolve all the channel maps to a common resolution (usually set
1 It is possible to extend the methods we discuss below to asymmetric or spatially varying beams, but they
become signiﬁcantly more complicated and so we shall not discuss them here.

128
Signal separation in cosmology
by the largest of the observing beams). Thus, assuming that any window function
used is applied after the convolution, the data then become
¯dν(x) = W(x)
2
B(|x −x′|)
) Nc

c=1
Fνc(x′) sc(x′) + nν(x)
*
dx′
3
≡W(x)
 ¯Fνc(x)¯sc(x) + ¯nν(x)

,
(6.2)
where B(r) is the same for all channels ν, and a bar over a variable indicates the
spatial ‘averaging’ resulting from convolution to a common resolution. In the ﬁnal
expression, it should be noted that there is some freedom in deﬁning ¯Fνc(x) and
¯sc(x), but this is not usually a problem. The signal separation task is then to infer
the convolved components ¯sc(x) from the data. Once this has been completed,
an attempt may be made to deconvolve (partially) the resulting component maps,
although this is rarely performed. As above, setting the noise term in Eq. (6.2) to
zero results in the predicted data, which we will denote by ¯mν(x).
In performing a signal separation, different quantities in Eqs. (6.1) or (6.2) may
be (assumed to be) known. The window function W(x) is typically always known,
and we will generally assume that the beam Bν(r) at each frequency and the sta-
tistical properties of the noise nν(x), and hence those of ¯nν(x), are also known.
Typically, the instrumental noise is well described by a Gaussian process with a
known covariance matrix (although we will consider the case in which the noise
covariance matrix is not known in Section 6.7.2). These assumptions are usually
valid, but can be relaxed at the cost of introducing additional complications that we
will not discuss here. The key differences between signal separation methods there-
fore result from the assumptions made regarding the mixing matrix Fνc(x) and
the statistical properties of the components sc(x) in Eq. (6.1), or their convolved
counterparts in Eq. (6.2). The nature of these assumptions typically depends on the
parameterization chosen to represent the mixing matrix and the component ﬁelds.
6.2 The hidden, visible and data spaces
The next steps in performing a Bayesian signal separation – steps that are often
taken for granted – are (i) to choose an appropriate parameterization of the prob-
lem, which may differ from the ‘natural’ parameterization used in the deﬁnition of
the model; and (ii) to decide in what domain to analyze the data. Adapting slightly
the nomenclature of Gull and Skilling (1990), this means deﬁning three spaces: the
‘hidden’ space H, the ‘visible’ space V and the ‘data’ space D. For our pur-
poses, the visible space V may be taken as consisting of the parameters appearing
explicitly in the data model Eq. (6.1), namely the mixing matrix elements Fνc and

6.3 Parameterization of the hidden space
129
component amplitudes sc in each pixel xp (of the reconstructed maps), or their
convolved counterparts in Eq. (6.2). The parameters deﬁning the hidden space H
are those in terms of which the Bayesian inference problem is actually formulated,
and may differ from the ‘visible’ parameters in the model. It is the values of the hid-
den parameters over which the corresponding posterior distribution is maximized
or explored (usually) via sampling. Any priors adopted are also usually imposed
directly on the hidden-space parameters. Finally, the choice of D determines in
what space the model predictions are compared with the data, and hence the form
of the likelihood function. The data space is usually chosen as the ‘natural’ space
in which the data are taken, and hence the instrumental noise properties are sim-
plest. Nonetheless, other choices are possible and can lead to useful algorithmic
simpliﬁcations. The possible choices of the hidden and data spaces are discussed
below.
6.3 Parameterization of the hidden space
We now turn to the speciﬁcation of the hidden space H for our astrophysical signal
separation problem, which requires us to deﬁne the parameterizations of the mixing
matrix and the component ﬁelds.
6.3.1 Mixing matrix
Let us ﬁrst consider the parameterization of the mixing matrix, which divides into
three main areas: (i) position (in)dependence; (ii) modelling of spectral behaviour;
and (iii) level of prior knowledge. These are discussed in turn below.
It is clear that the mixing matrix must depend on position to some extent,
particularly for diffuse Galactic emission, such as synchrotron, free–free and dust
emission, and several methods allow for some (large-scale) spatial variation.
Nonetheless, it is not uncommon to assume that the mixing matrix is indepen-
dent of position, and hence that the spectral behaviour of the components does not
vary across the region of sky being analyzed. This assumption cannot be strictly
true, but may be a reasonable approximation when analyzing small sky patches
separately.
The parameterization of the spectral behaviour of the components can be deﬁned
in a large number of ways. In the most general case, the parameters are taken
simply to be the mixing matrix components Fνc themselves (which can vary with
position). The resulting proliferation of parameters can be unhelpful, however, and
one often adopts some parameterized physical model for the spectral behaviour of
each component. In this case Fνc(x) = fc(ν; φc(x)), where the function fc deﬁnes

130
Signal separation in cosmology
the physical model for the spectral behaviour of component c and depends on the
parameters φc (which may be position dependent). A commonly adopted form is
fc(ν; φc(x)) = g(ν, ν0)
 ν
ν0
βc(x)
,
(6.3)
where the only parameter for each component is the (in general, spatially depen-
dent) spectral index βc. The quantity ν0 is the reference frequency at which fc is
unity, and g(ν, ν0) is a given function (often just equal to unity). Whatever the pre-
cise form chosen for the spectral behaviour, we will denote the spectral parameters
for all the components collectively by φ(x), so that Fνc = Fνc(φ(x)).
Depending on whether the mixing matrix is parameterized in terms of the
matrix elements themselves or in terms of a set of physically motivated spectral
parameters, the full parameter set Φ associated with the mixing matrix is either
Φ = {Fνc(xp)} or Φ = {φ(xp)}, where p counts pixels (from 1 to Npix) in the
reconstructed component maps. However, since one expects the spectral behaviour
of components to change more slowly with position than the pixel scale, it is often
better instead to divide the sky into a set of larger ‘patches’ and assume that mix-
ing matrix parameters are identical for all pixels lying within each patch. This can
signiﬁcantly reduce the number of parameters in Φ.
The level of prior knowledge assumed about the mixing matrix is central to
the distinction between non-blind, semi-blind and blind signal separation methods.
Non-blind methods assume the mixing matrix is known, or equivalently the values
of all the spectral parameters. Semi-blind and blind methods relax this assumption
to varying degrees. The usual (somewhat arbitrary) distinction made in the liter-
ature between semi-blind and blind methods is that the former parameterize the
mixing matrix with some (physical) spectral parameters φ, whereas the latter use
the mixing matrix elements Fνc themselves as the parameters. In each case, there is
a wide range of possibilities for the constraints applied to either set of parameters.
One might, for example, ﬁx the values of a subset of the parameters, link parame-
ters in some way, or restrict the ranges that parameters might take. In all cases, one
can quantify the wide range of possibilities by simply deﬁning the prior probability
assigned to the parameter set Φ (see Section 6.5.1). Finally, it should be noted that,
in semi-blind or blind methods, the number of components Nc can either be as-
sumed (which is most common) or allowed to vary and hence be determined from
the data; how the latter may be achieved is discussed in Section 6.7.4.
6.3.2 Component ﬁelds
We now turn to the parameterization of the components ﬁelds sc(x) themselves.
The assumed statistical properties of the components sc(x) vary considerably

6.3 Parameterization of the hidden space
131
–500
500
0
0.06
25.0
25.0
2.05
2500
–0.25
0.25
0.02
5.0
Fig. 6.1. All-sky realizations of the six main physical components contributing to the sky
emission at Planck wavelengths. The components are primordial CMB, kinetic and thermal
SZ effects from clusters and Galactic dust, free–free and synchrotron emission. Each map
is deﬁned in the HEALPIX pixelization scheme with Nside = 2048, which corresponds
to ∼50 × 106 pixels of size 1.7 arcmin. Each map is plotted at 300 GHz in units of μK.
(Reproduced from Stolyarov et al. 2002.)
between different signal separation methods. In general, each component sc(x)
may be a statistically inhomogeneous, non-Gaussian ﬁeld with an inﬁnite number
of non-vanishing N-point correlation functions; some examples of all-sky com-
ponent ﬁelds are shown in Figure 6.1. It is not possible, however, to model the
statistical properties of sc(x) in such a general way, and so some assumptions or
approximations are necessary. The nature of these assumptions typically depends
on the parameterization chosen to represent the component ﬁelds.
For later convenience, we will denote the parameter set associated with the com-
ponent ﬁeld c by the ‘signal’ vector sc, and the full signal parameter set by the
concatenated (and then possibly re-ordered) vector s = {sc}. The most natural

132
Signal separation in cosmology
parameterization of the cth component ﬁeld is sc = {sc(xp)}, so that the pth ele-
ment of sc is simply the amplitude (at the reference frequency ν0) of the pth pixel
in the component ﬁeld c; in this case, the hidden space and visible space parameters
describing the component ﬁelds are identical. An obvious alternative is instead to
take sc = {4sc(kp)}, in which case the pth element of sc is the amplitude of the pth
Fourier mode in the Fast Fourier Transform (FFT) of the (pixelized) component
ﬁeld (when working on the full sky, one instead performs a spherical harmonic ex-
pansion, so sc contains the discrete set of spherical harmonic coefﬁcients aℓ,m). In
fact, it is often more convenient to construct sc so that it is real, rather than com-
plex. One way to achieve this is simply to ﬁll the ﬁrst half of sc with the real parts
of the mode amplitudes 4sc(kp) and the second half with their imaginary counter-
parts. For a real sky, 4s∗
c(k) = 4sc(−k) and so the (maximum required) length of
the vector sc is again equal to the number of pixels in the real-space component
map. One may alternatively choose sc to contain the coefﬁcients of the expansion
of the component ﬁeld in some more general basis, such as a wavelet basis (see,
e.g., Maisinger, Hobson & Lasenby 2004 and references therein), although we will
not pursue this further here. It should be noted that it is not necessary to make the
same choice of parameterization for every component c, although we will assume
throughout that the vector sc is real.
Since the length of the signal parameter vector sc is typically rather large, one
is usually limited to describing any assumed a-priori statistical properties of sc in
terms only of its covariance matrix Sc = ⟨scsT
c ⟩. Moreover, it is usually neces-
sary to take this covariance matrix to be diagonal. These computational restrictions
result in natural advantages and disadvantages to each of the parameterizations
discussed above. In the pixel-based parameterization, one can include expected
pixel-by-pixel rms values and hence it is straightforward to accommodate spatial
variations in these quantities. One cannot, however, include any anticipated inter-
pixel (i.e., spatial) correlations. In the Fourier-based parameterization, one can in-
clude the expected mode-by-mode rms values in the form of a power spectrum,
thereby accommodating spatial correlations, but one cannot easily include spatial
variations in the statistical properties. Wavelet-based parameterizations lie some-
where between the pixel and Fourier bases, and so allow some inclusion of spatial
correlation information, while retaining spatial dependence.
In the construction of the full covariance matrix S = ⟨ssT⟩, one is typically able
to include correlations between the corresponding elements of sc and sc′, where c′
denotes another component (at least when the two components share the same pa-
rameterization). Thus, depending on the parameterization chosen, one can include
either pixel-by-pixel or mode-by-mode inter-component correlations.
Just as there is considerable freedom in the assumed level of prior knowledge
regarding the mixing matrix parameters Φ, the assumed prior knowledge of the

6.4 Choice of data space
133
covariance matrix S can vary signiﬁcantly. Some methods assume complete knowl-
edge of S in advance. Others assume knowledge of some subset of the elements of
this covariance matrix, make links between the values of various elements, or con-
strain the range of values they might take. All these cases can be quantiﬁed by the
imposition of an appropriate prior on the covariance matrix elements. It should be
noted, however, that the (unﬁxed) elements of the covariance matrix then become
additional parameters in the hidden space.
6.3.3 Linear and non-linear parameters
Bringing together our discussion of the hidden space, we see that, in general, it
is parameterized by the mixing matrix parameters Φ, the component ﬁelds signal
parameters s (in some domain) and the elements of the corresponding a-priori co-
variance matrix S of the component ﬁelds. We shall denote all these hidden-space
parameters collectively by h = {Φ, s, S}, which we assume always to be real.
It is worth noting that the signal parameters s are typically related via a linear
transformation to the pixel amplitudes of the component ﬁelds. Since the model
Eq. (6.1) or Eq. (6.2) is linear in the latter quantities, then it is also linear in the
parameters s. In general, however, there is no such restriction on the spectral pa-
rameters Φ. If one is using the parameterization Φ = {Fνc(x)}, then these param-
eters again enter the model linearly, but if Φ = {φ(x)} the spectral parameters
may enter the model in a non-linear manner. Linear and non-linear parameters can
be treated differently when we make inferences about the components (see, e.g.,
Section 6.7.1). Finally, we note that the model Eq. (6.1) or Eq. (6.2) depends only
indirectly on the parameters S through the signal vector s.
6.4 Choice of data space
Once the parameterization of the hidden space has been deﬁned, it still remains to
specify the data space D, in which the model predictions are compared with the
observations. As mentioned above, the data space is usually taken to be the space
in which the observations are made, and hence in which the statistical properties of
the instrumental noise are simplest, although this is not necessary.
Mirroring our discussion of the parameterization of the component ﬁelds in
Section 6.3.2, we will denote the parameter set associated with the data channel
ν by the ‘data’ vector dν, and the full data parameter set by the concatenated (and
possibly re-ordered) vector d = {dν}. Since most cosmological observations are
made with single-dish telescopes, it is most natural to choose the pixel domain as
the data space, in which case dν = {dν(xp)}; examples of all-sky data maps in
the lowest eight frequency channels of the Planck satellite are shown in Figure 6.2.

134
Signal separation in cosmology
–0.022
–0.11
–0.3
–0.207
0.23
–0.05
5.0
–0.341
0.38
0.3
0.11
0.19
–0.19
–0.045
0.045
0.022
Fig. 6.2. All-sky data maps in the lowest eight frequency channels of the Planck satellite:
30, 44, 70, 100, 143, 217, 353 and 545 GHz. The maps are constructed using the component
maps shown in Figure 6.1, with the simple assumption that the spectral dependence of each
component is not spatially varying. (Reproduced from Stolyarov et al. 2002.)
Once again, however, another possibility is to take dν = {4dν(kp)} or, better, a real
vector constructed from the complex Fourier mode amplitudes (see Section 6.3.2).
Further choices of the elements of dν include the coefﬁcients of the expansion of
the data map in some other, more general basis set; wavelets are again a popular

6.4 Choice of data space
135
option, but we will not discuss them further. It should be noted that the choice
of data space is independent of the choice of parameterization for the component
ﬁelds s. For example, it is not necessary for the data and the component ﬁelds both
to be deﬁned either in the pixel domain or the Fourier domain, although this can be
advantageous in some circumstances, as outlined below.
The advantages and disadvantages of each choice for the data space centre
around the resulting statistical properties of the corresponding ‘noise’ vector n =
{nν}, deﬁned in a similar manner to the data vector d. Throughout this chapter,
we will make the reasonably accurate assumption that the (pixel-domain) instru-
mental noise in each data channel is well described by a multivariate Gaussian
distribution with a known covariance matrix. Since the domain chosen as the data
space is usually linearly related to the pixel domain, the statistical properties of the
corresponding noise vector n are also well described by a multivariate Gaussian
distribution with known covariance matrix N = ⟨nnT⟩.
For all analyses except those at very low resolution, the typical length of n means
it is necessary to assume that N is diagonal. Hence, if the data space is the pixel
domain, one must assume that the noise is not correlated between pixels. If one is
working instead in the Fourier or spherical harmonic domain, it is assumed that the
noise is not correlated between modes. Both approximations can cause problems,
as we discuss below.
6.4.1 Pixel-domain data space
If the data space D is the pixel domain, the above restriction means that (except
for very low resolution analyses) one must assume that the noise is not correlated
between pixels. This assumption may not be too severe if the data are analyzed at
their original spatial resolution, as in Eq. (6.1). If one performs a convolution as in
Eq. (6.2), however, then the approximation can be a poor one. Nonetheless, the ad-
vantage of working in the pixel domain is that it is straightforward to accommodate
non-stationary noise and data windows (or cuts).
If one additionally chooses the component ﬁelds s to be deﬁned in the pixel
domain, then it is also straightforward to accommodate a spatially varying mix-
ing matrix, which is very common in practice. Moreover, if one simply ignores
any spatial correlations in the components and noise ﬁelds (either intrinsic or in-
duced by convolving all the data channels to the same spatial resolution), then one
can perform a pixel-by-pixel Bayesian signal separation based on the data model
Eq. (6.2), in which the separation is performed entirely separately for each pixel
(see, e.g., Section 6.7.1). Assuming for simplicity that any applied window takes
only the values zero or unity, then at each pixel x
=
xp separately in
the region with W(x) = 1, one obtains the much lower dimensionality inference

136
Signal separation in cosmology
problem
d = F(Φ)s + n,
(6.4)
in which the data vector d now has components dν (for ν = 1, . . . , Nν) at that
pixel, and similar for n; the signal vector has components sc (for c = 1, . . . , Nc)
at that pixel. Note that F(Φ) can also be deﬁned on a pixel-by-pixel basis, thus
allowing one to model spatially varying spectral behaviour. This pixel-by-pixel
approach thus replaces a single Bayesian inference problem with a great many
parameters by a large number of individual inference problems, each containing
just a few parameters, which is typically far quicker to analyze.
6.4.2 Fourier-domain data space
If one chooses the Fourier domain as the data space, one faces a complementary
set of difﬁculties. Although it is now straightforward to accommodate noise cor-
relations between pixels, it is more difﬁcult to take account of statistically inho-
mogeneous noise and data cuts. The Fourier transform of the data model Eq. (6.1)
reads
4dν(k) =

dk′ 5
W(k −k′)
2
4Bν(k′)
Nc

c=1

dk′′ 4Fνc(k′ −k′′) 4sc(k′′) + 4nν(k′)
3
,
(6.5)
where k = |k|. From the above expression, we ﬁrst see the consequences of any
data window. Even if the original noise ﬁelds and component ﬁelds are statistically
homogeneous, so that there are no inter-mode correlations, the presence of the
window induces correlations between the Fourier modes of the data. Moreover, we
see that if the mixing matrix is spatially varying, an additional set of correlations
are induced between the modes of the component ﬁelds. Computationally, we note
that in going from Eq. (6.1) to Eq. (6.5) we have replaced the beam convolutions
with a simple multiplication, but also replaced the multiplications by the window
and mixing matrix by corresponding convolutions.
It is only when further assumptions are made that choosing the Fourier domain
as the data space yields any advantages. If there is no window (or cut) applied
to the data and the mixing matrix may be taken as independent of position, the
corresponding Fourier transforms in Eq. (6.5) reduce to delta functions and so the
expression for the Fourier data reduces to
4dν(k) = 4Bν(k)
Nc

c=1
Fνc4sc(k) + 4nν(k).
(6.6)

6.5 Applying Bayes’ theorem
137
It is important to note that Eq. (6.6) is satisﬁed at each Fourier mode k indepen-
dently, irrespective of whether the component ﬁelds and the noise ﬁelds are statis-
tically homogeneous. If both sets of ﬁelds are, in fact, statistically homogeneous,
or one simply assumes this to be true, then their respective covariance matrices
are diagonal in Fourier space. Thus by choosing also to represent the component
ﬁelds in Fourier space, the entire inference problem can be analyzed mode-by-
mode. Hence, at each Fourier mode k = kp separately, one has the much smaller
inference problem
d = R(Φ)s + n,
(6.7)
where the data vector d now has components 4dν (for ν = 1, . . . , Nν) at that Fourier
mode, and similar for n; the signal vector has components 4sc (for c = 1, . . . , Nc)
at that mode. The matrix R has components Rνc(Φ) = 4Bν(kp)Fνc(Φ), where
Fνc(Φ) is the same for each mode (since the mixing matrix is assumed to be
spatially invariant). In a similar manner to the pixel-by-pixel analysis mentioned
above, this replaces a single Bayesian inference problem with many parameters by
a large number of inference problems of low dimensionality, which is usually far
less computationally demanding.
6.5 Applying Bayes’ theorem
Irrespective of the particular forms chosen for the hidden parameters h = {Φ, s, S}
and the data vector d, Bayesian inference methods provide a consistent approach
to the estimation of the parameters in our model (or hypothesis) H from the data.
Bayes’ theorem states that
Pr(h|d, H) = Pr(d| h, H) Pr(h|H)
Pr(d|H)
,
(6.8)
where Pr(h|d, H) ≡P(h) is the posterior probability distribution of the param-
eters, Pr(d|h, H) ≡L(h) is the likelihood, Pr(h|H) ≡π(h) is the prior, and
Pr(d|H) ≡E is the Bayesian evidence.
In parameter estimation, the normalizing evidence factor is usually ignored,
since it is independent of the parameters h. This (un-normalized) posterior con-
stitutes the complete Bayesian inference of the parameter values. Inferences are
usually obtained either by taking samples from the (un-normalized) posterior us-
ing Markov chain Monte Carlo (MCMC) methods, or by locating its maximum
(or maxima) using numerical optimization routines and approximating the shape
around the peak(s) by a multivariate Gaussian.

138
Signal separation in cosmology
In contrast to parameter estimation problems, in model selection the evidence
takes the central role and is simply the factor required to normalize the posterior
over h:
E =

L(h)π(h) dDh,
(6.9)
where D is the dimensionality of the parameter space. As the average of the likeli-
hood over the prior, the evidence is larger for a model if more of its parameter space
is likely and smaller for a model with large areas in its parameter space having low
likelihood values, even if the likelihood function is very highly peaked. Thus, the
evidence automatically implements Occam’s razor: a simpler theory with compact
parameter space will have a larger evidence than a more complicated one, unless
the latter is signiﬁcantly better at explaining the data. The question of model se-
lection between two models H0 and H1 can then be decided by comparing their
respective posterior probabilities given the observed dataset d, as follows:
Pr(H1|d)
Pr(H0|d) = Pr(d|H1) Pr(H1)
Pr(d|H0) Pr(H0) = E1
E2
Pr(H1)
Pr(H0),
(6.10)
where Pr(H1)/ Pr(H0) is the a-priori probability ratio for the two models, which
can often be set to unity but occasionally requires further consideration. Unfortu-
nately, evaluation of the multidimensional integral Eq. (6.9) is a challenging numer-
ical task, but sampling-based methods do exist, such as thermodynamic integration
(see, e.g., ´O’Ruanaidh & Fitzgerald 1996) or, better, nested sampling (Chapter 1
of this volume; Skilling 2004b; Sivia & Skilling 2006; Feroz & Hobson 2008;
Feroz, Hobson & Bridges 2008). Alternatively, a fast approximate method for evi-
dence evaluation is to model the posterior as a multivariate Gaussian centred at its
peak(s) (see, e.g., Hobson, Bridle & Lahav 2002).
6.5.1 Deﬁning the posterior distribution
Let us ﬁrst consider the likelihood L(h) ≡Pr(d|h, H) in Eq. (6.8). We will as-
sume that the statistical properties of the instrumental noise vector n (in whatever
domain is chosen as the data space) are well described by a multivariate Gaussian
distribution with known (usually diagonal) covariance matrix N = ⟨nnT⟩. In this
case, the likelihood takes the form
L(h) = exp{−1
2[d −m(Φ, s)]TN−1[d −m(Φ, s)]}
(2π)NpixNν/2|N|1/2
,
(6.11)

6.5 Applying Bayes’ theorem
139
where m is the data model (also called the predicted or noise-free data) deﬁned
by m = limn→0 d. Note that the likelihood, in fact, depends only on the sets
of parameters Φ and s. Since we are assuming the noise covariance matrix N is
known, it will not vary as a function of the hidden space parameters h, and so we
can neglect the unimportant denominator in Eq. (6.11). Since it is more convenient
computationally to work with the log-likelihood, we have
ln L(h) ∝−1
2[d −m(Φ, s)]TN−1[d −m(Φ, s)] ≡−1
2χ2(h),
(6.12)
where we have ignored an unimportant arbitrary constant and identiﬁed the simple
χ2 misﬁt function.
Once the likelihood has been deﬁned, all that remains is to specify the prior
π(h) ≡Pr(h|H) on the hidden parameters h = {Φ, s, S} for the model H. The
speciﬁcation of this prior is key to the differences between various signal sepa-
ration methods used in cosmology. In general, it is reasonable to assume that the
mixing matrix parameters Φ are independent of the signal vector s and its a-priori
covariance matrix S. Thus, we may write
π(h) = π(Φ)π(s, S) = π(Φ)π(s|S)π(S).
(6.13)
A wide range of possibilities are available for the factors π(Φ) and π(S) above.
The priors on the mixing matrix parameters are typically taken to be uniform in
some range, or delta functions ﬁxing some of the parameters to pre-deﬁned values.
Note that assuming the mixing matrix to be fully known (i.e., in non-blind methods)
is equivalent to setting π(Φ) = δ(Φ −Φ0), where Φ0 are the pre-deﬁned values
of the mixing matrix parameters. This approach is identical to conditioning on the
values of Φ = Φ0.
The prior π(S) must reﬂect the fact that the individual covariance matrices Sc
for each component are usually assumed to be diagonal, but inter-component cor-
relations at a given pixel or Fourier mode are usually included in the full covariance
matrix S. The possibilities range from uniform or Jeffreys priors on the elements of
S to ﬁxing part or all of the covariance matrix. The last option is formally achieved
by setting π(S) = δ(S −S0), which is equivalent to conditioning on S = S0.
Once the prior on S has been speciﬁed, the prior π(s|S) is straightforward to
deﬁne. The most common choice is to assume a priori that the signal vector s is
well described by a multivariate Gaussian with the covariance matrix S, in which
case,
π(s|S) = exp(−1
2sTS−1s)
(2π)NpixNc/2|S|1/2 .
(6.14)

140
Signal separation in cosmology
It is worth noting that it is sometimes useful instead to deﬁne a set of a-priori
uncorrelated variables u such that s = Lu, where L is the lower triangular matrix
in the Cholesky decomposition S = LLT of the covariance matrix. In this case,
since ⟨uuT⟩is the identity matrix a-priori, one has
⟨ssT⟩= ⟨LhhTLT⟩= L⟨hhT⟩LT = LLT = S,
(6.15)
as required. It can occasionally be useful to work in terms of these a-priori uncor-
related variables, especially when using entropic priors (see Section 6.6.2). In such
cases, the hidden-space parameters are instead taken as h = {Φ, u, S}, so that
s →u in the expressions (6.11)–(6.13), remembering that s = Lu. The prior on
the uncorrelated variables is then usually written as
π(u|S) ∝exp
2
p
f(up)
3
,
(6.16)
where f(up) is some given function and p counts the elements in u. If the com-
ponent ﬁelds are assumed to obey Gaussian statistics, then the prior Eq. (6.14) is
recovered by setting f(up) = −u2
p/2 to obtain
π(u|S) ∝exp(−1
2uTu).
(6.17)
6.6 Non-blind signal separation
First, we concentrate on non-blind signal separation where we assume the mix-
ing matrix parameters Φ to be completely speciﬁed. Moreover, we shall assume
throughout that (at least some approximation to) the signal covariance matrix S is
also given in advance. This may seem perverse, as we then require advance knowl-
edge of the full covariance structure of the physical components that we are trying
to reconstruct. Nonetheless, it is possible to extract some information concern-
ing the power spectra of the various components, and correlations between them,
either from pre-existing observations or by performing an initial approximate sep-
aration using, for example, the singular value decomposition (SVD) algorithm (see
Bouchet et al. 1997; Bouchet & Gispert 1999). This information can then be used
to construct an approximate signal covariance matrix. Indeed, the SMICA method
discussed in Section 6.7.2 provides a ‘pre-processing’ step that determines (some
of) both the mixing matrix parameters Φ and the signal covariance matrix S. The
non-blind methods presented below can then be considered merely as the ﬁnal
stage of a larger (semi-)blind approach.

6.6 Non-blind signal separation
141
6.6.1 Wiener ﬁltering
The Wiener ﬁlter (WF) is the minimum mean squared error (MMSE) optimal
stationary linear ﬁlter (Kolmogorov 1939; Levinson 1947; Wiener 1943) for
convolved images with additive noise. Calculation of the Wiener ﬁlter requires
the assumption that the signal and noise processes are second-order stationary (in
the random process sense). Wiener ﬁltering was extended to multi-frequency and
multi-resolution microwave data in Tegmark and Efstathiou (1996); Bouchet et al.
(1997) and Bouchet and Gispert (1999). Below we will derive the WF in a Bayesian
context following Hobson et al. (1998).
WF posterior
In applying the WF it is usual to take the data and the component ﬁelds to be
represented in the Fourier domain (although this is not necessary). Moreover, one
assumes (at least formally) that no window (or cut) has been applied to the data,
the mixing matrix components are independent of position and ﬁxed, and that each
component ﬁeld and the noise ﬁelds in each channel are statistically homogeneous.
Thus, the data model is that given in Eq. (6.7), with the further simpliﬁcation that
the mixing matrix parameters are ﬁxed, Φ = Φ0. Thus, at each Fourier mode k
separately, one has the small inference problem
d = Rs + n,
(6.18)
where the matrix R has the ﬁxed components Rνc = 4Bν(kp)Fνc(Φ0), where Fνc
is the same for each mode (since the mixing matrix is assumed to be spatially
invariant). Thus, the χ2 misﬁt statistic deﬁned in Eq. (6.12) is a function of s only
and can be written as
χ2(s) = (d −Rs)TN−1(d −Rs).
(6.19)
In addition, it is assumed that the signal covariance matrix is also given in ad-
vance, so S = S0. Thus, the only free parameters in the hidden space are h = s.
Finally, the key assumption in the WF is that the emission from each of the physi-
cal components can be modelled by a Gaussian random ﬁeld. Thus, the prior on s
follows from Eq. (6.14), namely,
π(s|S0) ∝exp(−1
2sTS−1
0 s).
(6.20)
Combining Eqs. (6.19) and (6.20) we thus ﬁnd that the posterior distribution
P(s) ≡Pr(s|d, Φ0, S0, H) on s is given by
P(s) ∝exp

−1
2χ2(s) −1
2sTS−1
0 s

.
(6.21)

142
Signal separation in cosmology
Completing the square for s in the exponential (see Zaroubi et al. 1995), it is
straightforward to show that the posterior probability is also a multivariate
Gaussian of the form
P(s) ∝exp

−1
2(s −ˆs)TC−1(s −ˆs)

,
(6.22)
which has its maximum value at the estimate ˆs of the signal vector and where C is
the covariance matrix of the reconstruction errors.
Optimal values and error estimates
The estimate ˆs of the signal vector in Eq. (6.22) reads
ˆs =

S−1
0
+ RTN−1R
−1 RTN−1d ≡Wd,
(6.23)
where we have deﬁned W, the Wiener matrix. We thus recover the standard Wiener
ﬁlter. This optimal linear ﬁlter is usually derived by choosing the elements of W
such that they minimize the variances of the resulting reconstruction errors (mini-
mum mean squared error, or MMSE ﬁlter). From Eq. (6.23) we see that at a given
Fourier mode, we may calculate the estimator ˆs that maximizes the posterior prob-
ability simply by multiplying the data vector d by the Wiener matrix W. The
assignment of errors on the Wiener ﬁlter reconstruction is also straightforward and
the covariance matrix of the reconstruction errors C in Eq. (6.23) is given by
C ≡⟨(s −ˆs)(s −ˆs)T⟩=

S−1
0
+ RTN−1R
−1 .
(6.24)
Alternatively, one may adopt an iterative approach and obtain the Wiener ﬁl-
ter solution by direct numerical maximization of the posterior Eq. (6.21). Writing
P(s) ∝exp[−FWF(s)], this is equivalent to minimizing the function
FWF(s) = 1
2χ2(s) + 1
2sTS−1
0 s,
(6.25)
where we have retained the factors of one-half for later convenience. Similarly, the
covariance matrix C of the reconstruction residuals can be obtained by evaluat-
ing the curvature (or Hessian) matrix H = ∇s∇s ln P(s) = −∇s∇sFWF(s) at
the peak s = ˆs of the posterior, and then setting C = (−H)−1. Since the poste-
rior Eq. (6.21) is in fact a multivariate Gaussian in this case, no approximation is
involved.
It should be noted that the linear nature of the Wiener ﬁlter and the simple propa-
gation of errors are both direct consequences of assuming that the spatial templates
we wish to reconstruct are well described by Gaussian random ﬁelds with a known
covariance structure.
Finally, the reconstructed maps of the sky emission due to each physical compo-
nent are obtained by inverse Fourier transformation of the estimated signal vectors
ˆs at each Fourier mode. Since this operation is linear, the errors on these maps may

6.6 Non-blind signal separation
143
therefore be deduced straightforwardly from the above error covariance matrix.
Several applications are given in Bouchet et al. (1997); an example of component
separation of simulated Planck data is shown in Hobson et al. (1998).
6.6.2 Harmonic-space maximum-entropy method
The sky emission from most astrophysical components is known to be neither
Gaussian nor stationary. This is particularly true for the kinetic and thermal
SZ effects, but the Galactic dust and free–free emissions also appear quite
non-Gaussian. The maximum-entropy method (MEM) uses non-Gaussian entropic
priors that are based on information-theoretic considerations. The concept of en-
tropy in information theory was ﬁrst introduced by Shannon (1948).
The application of the MEM to cosmological signal separation was ﬁrst dis-
cussed in Hobson et al. (1998). In this approach, many of the assumptions made
are the same as those outlined above for the WF. In particular, the data and the
component ﬁelds are represented in the Fourier domain (although this is not nec-
essary). One assumes (at least formally) that no window (or cut) has been applied
to the data, the mixing matrix components are independent of position and ﬁxed,
and that each component ﬁeld and the noise ﬁelds in each channel are statistically
homogeneous. Thus, the data model is precisely that assumed in Eq. (6.18) for the
WF, in which the data are analyzed mode-by-mode. In addition, it is again assumed
that the signal covariance matrix is also given in advance, so S = S0.
Harmonic-space MEM posterior
As discussed in Hobson et al. (1998), however, one of the fundamental axioms
of the MEM is that it should not itself introduce correlations between individual
elements of the image. It is therefore necessary to work in terms of the a-priori
uncorrelated variables u described in Section 6.5.1, which are then the only free
parameters in the hidden space, i.e., h = u. Since s = Lu, the χ2 misﬁt function
in Eq. (6.19) becomes
χ2(u) = (d −RLu)TN−1(d −RLu).
(6.26)
It may be shown (Skilling 1989; Gull & Skilling 1990; Hobson & Lasenby 1998)
that, in the absence of any other information, the prior on the variables u should
take the form of Eq. (6.16) with f(up) = αS(up, μp), where the dimensional
constant α depends on the scaling of the problem and may be considered as a
regularizing parameter, and the cross-entropy is deﬁned by
S(up, μp) = ψp −2μp −up ln
ψp + up
2μp

.
(6.27)

144
Signal separation in cosmology
In this expression ψp = (u2
p + 4μ2
p)1/2 and μp determines the width of the cross-
entropy function as a function of up. We note that S(up, μp) takes its maximum
value of zero at up = 0, independent of the value of μp. Writing S(u, μ) =

p S(up, μp), the prior thus becomes
π(u|S0, α) ∝exp[αS(u, μ)].
(6.28)
One can show (Maisinger et al. 2004) that an appropriate choice is to set μp propor-
tional to the expected rms of up. Since each up is constructed to have unit variance,
it is convenient (see below) to set μp = 1
2 for all p.
Optimal values and error estimates
Combining Eqs. (6.26) and (6.28), we learn that the posterior probability P(u|α) ≡
Pr(u|d, Φ0, S0, α, H) on u is given by
P(u|α) ∝exp

−1
2χ2(u) + αS(u, μ)

.
(6.29)
Since the cross-entropy is a non-linear function of u, one must locate the maximum
u = ˆu of the posterior by numerical iterative optimization. Writing P(u|α) ∝
exp[−FMEM(u; α)], this is equivalent to minimizing the function
FMEM(u; α) = 1
2χ2(u) −αS(u, μ).
(6.30)
This task can be performed with a variety of numerical optimization methods,
such as downhill simplex, conjugate gradient or variable metric methods in multi-
dimensions (Press et al. 1997). The corresponding estimate of the signal vector, for
the given value of α used in Eq. (6.30), is then given by ˆs(α) = Lˆu(α).
The covariance matrix on the reconstruction residuals can be estimated by ﬁrst
calculating the Hessian matrix H = ∇u∇u ln P(u) = −∇u∇uFMEM(u; α) at
the peak u = ˆu of the posterior. The required covariance matrix is then given
approximately by
C = ⟨(s −ˆs)(s −ˆs)T⟩≈−LH−1LT.
(6.31)
We note that this procedure is equivalent to making a Gaussian approximation to
the posterior Eq. (6.29) about its maximum u = ˆu. As for the WF, the recon-
structed map of the sky emission due to each physical component is obtained by
inverse Fourier transformation of the estimated signal vectors ˆs at each Fourier
mode. Since this operation is linear, the errors on these maps may therefore be
deduced straightforwardly from the above error covariance matrix.
Relationship between the MEM and WF
It is worth noting that the WF solution discussed in the previous section can also be
expressed in terms of the set of a-priori uncorrelated variables u, and this highlights

6.6 Non-blind signal separation
145
the relationship between the harmonic-space MEM and WF approaches. In terms
of u, the WF prior Eq. (6.20) takes the simple form given in Eq. (6.17). Thus, the
corresponding WF posterior then reads P(u) ∝exp[−FWF(u)], where
FWF(u) = 1
2χ2(u) + 1
2uTu,
(6.32)
and χ2(u) is given by Eq. (6.26). The connection with the MEM is now clariﬁed
by expanding the expression Eq. (6.27) for the cross-entropy around its peak at
up = 0 as a power series in up, which reads
S(up, μp) = −u2
p
4μp
+ O(u4
p).
(6.33)
Since we have taken μp = 1
2 for all p, we thus ﬁnd that the functions FMEM(u; α)
and FWF(u), given in Eqs. (6.30) and (6.32) respectively, are equal to second-order
in u, provided α = 1. Thus, the WF can be regarded as the quadratic approximation
to the MEM in this case.
Determination of the regularization constant
So far, we have made no mention of how to choose the parameter α, which de-
termines the amount of regularization on the reconstruction. It is clear that mini-
mizing χ2 only (by setting α = 0) would lead to closer agreement with the data,
and thus to noise ﬁtting. On the other hand, maximizing the entropy alone (by set-
ting α = ∞) would lead to the reconstruction ˆu = 0. Indeed, for every choice of
α there is an estimated vector ˆu(α) (and hence ˆs(α)) corresponding to the mini-
mum of FMEM(u; α) for that particular choice. The estimates ˆu(α) vary along a
trade-off curve (sometimes called the maximum-entropy trajectory) as α is varied.
There are several methods for assigning an optimal value for α. As we have
shown above, setting α = 1 recovers the WF to second order, and choosing this
ﬁxed value is often a reasonable approach. Nonetheless, in general, it is preferable
to determine the appropriate value of α (and hence the level of regularization)
from the data themselves. In early MEM applications, the ‘optimal’ value α = ˆα
was chosen such that at the estimate ˆu(ˆα) the value of χ2 equalled its expectation
value, i.e., χ2 = Nd, where Nd is the number of data values. However, it can
be shown (Titterington 1985) that this choice leads to systematic under-ﬁtting of
the data. Therefore, an alternative method has been proposed (Gull 1989; Gull &
Skilling 1990) in which a value for α is obtained within the Bayesian framework
itself. Treating α as an additional (nuisance) parameter, one would ideally remove
the dependence of the posterior on this parameter by marginalizing over α. Thus
(omitting the other conditional parameters for brevity), one has
Pr(u|d) =

Pr(u|d, α) Pr(α|d) dα.
(6.34)

146
Signal separation in cosmology
This integral is usually too computationally demanding to perform, and so one in-
stead chooses an optimal value α = ˆα by maximizing Pr(α|d) ∝Pr(d|α) Pr(α).
Moreover, since Pr(d|α) should overwhelm any prior on α, one in fact performs a
maximization with respect to α of the evidence
Pr(d|α) =

Pr(d|u) Pr(u|α) du =

L(u)π(u|α) du ∝

P(u|α) du,
(6.35)
where P(u|α) is the MEM posterior given in Eq. (6.29). This integral is usually
performed by making a Gaussian approximation to the posterior about its peak
ˆu(α). Differentiating the resulting expression with respect to α leads to an implicit
analytic expression for α, which may be solved numerically to yield the optimal
value α = ˆα (Gull & Skilling 1990; Hobson et al. 1998).
It is worth noting that an equivalent procedure can be used within the WF ap-
proach. In other words, one could replace the WF objective function FWF(u),
given in Eq. (6.32), by
FWF(u; α) = 1
2χ2(u) + 1
2αuTu,
(6.36)
and determine the optimal value of the regularization constant α in an analogous
way to that presented above.
Iterative updating of the signal covariance matrix
Aside from determining the optimal level of regularization on the solution,
Hobson et al. (1998) found that the above approach for determining the regular-
ization constant α allowed one to update iteratively the assumed signal covariance
matrix S0. In this process, one uses the covariance matrix of the estimated signals
as the input S0 to a new separation step, and repeats until convergence is obtained.
This mitigates, to some extent, the reliance of the MEM approach on prior knowl-
edge of the signal covariance matrix. This method was further demonstrated in
Stolyarov et al. (2002). Figure 6.3 shows examples obtained in this way of all-
sky reconstructions of the components in Figure 6.1, obtained by analyzing the
frequency channel data shown in Figure 6.2. The MEM approach should be con-
trasted with the standard WF, which is well known to suppress power in high-k
Fourier modes, where the signal-to-noise ratio becomes small. As a result, if the
covariance matrix of the reconstructed signals is used as the input S0 to a subse-
quent WF separation, the solution gradually tends to zero as more iterations are
performed.
Accommodation of spatially varying noise and spectral parameters
We conclude our discussion of the harmonic-space MEM separation with a brief
account of how one can accommodate (at least approximately), the common

6.6 Non-blind signal separation
147
–500
–1
0
–0.25
0
–0.1
0.25
2500
5.0
500
25.0
25.0
Fig. 6.3. All-sky reconstructions obtained using the harmonic-space MEM of the compo-
nents in Figure 6.1, derived by analyzing the frequency channel data shown in Figure 6.2.
(Reproduced from Stolyarov et al. 2002.)
complications of spatially varying noise properties and spectral parameters. We
note that although these partial ﬁxes have been developed in the context of the
MEM (Stolyarov et al. 2005), they can equally well be applied to the WF separa-
tion method, which also neglects coupling between different k-modes.
Let us ﬁrst consider the case of anisotropic instrument noise, which we represent
as a non-stationary Gaussian random process in pixel space. This leads to correla-
tions between different Fourier modes of the noise ﬁeld. The elements of the noise
covariance matrix in Fourier space are deﬁned as
[N(k, k′)]νν′ ≡⟨4nν(k)4nν′(k′)⟩,
(6.37)
which are then, in general, non-zero for k ̸= k (and for ν ̸= ν′, if there are
correlations between the noise ﬁelds in different frequency channels). Stolyarov

148
Signal separation in cosmology
et al. (2005) advocate setting the elements of the inverse noise covariance matrix
appearing in the χ2 misﬁt function Eq. (6.26) directly, such that at each Fourier
mode k one has
[N −1]νν′ = [N−1(k, k)]νν′.
(6.38)
Note that this is not, in general, equivalent simply to setting [N]νν′ = [N(k, k)]νν′
and then inverting the resulting matrix. The reason for using the deﬁnition
Eq. (6.38) is that it allows for the straightforward (approximate) treatment of cut-
sky data, particularly when the noise is uncorrelated between pixels. One can con-
sider the missing areas in a cut-sky map as an extreme case of anisotropic noise
in which the noise rms of the pixels in the cut is formally inﬁnite. This leads to
elements of the noise covariance matrix that are also formally inﬁnite, which can
cause computational problems in the analysis. By using the assignment Eq. (6.38),
however, a formally inﬁnite noise rms in the cut is easily accommodated: it cor-
responds simply to omitting the pixels in the cut from the calculation. One is, in
effect, performing a separation over the entire sky patch, but not constraining the
solution in the cut in any way.
We now turn to the accommodation of (weakly) spatially varying spectral pa-
rameters. In general, the sky emission from the signal component c in observing
channel ν is
sc(x; ν) = sc(x)Fνc(φc(x)),
(6.39)
where (as previously) sc(x) denotes the signal at some reference frequency ν = ν0.
The mixing matrix is, in general, a function of position and we have assumed it is
parameterized in terms of a set of physically motivated spectral parameters φc(x).
As noted in Section 6.4.2, however, the choice of the Fourier domain as the data
space (as used in WF and harmonic-space MEM) requires one to assume a ﬁxed
mixing matrix if one is to avoid a considerable additional computational burden.
Nonetheless, Stolyarov et al. (2005) propose a (partial) solution to this problem
by performing a Taylor expansion in the spectral parameters around their assumed
mean values ¯φc. Thus, one writes
sc(x; ν) = sc(x)
⎡
⎣Fνc(¯φc) +

j
Δφc,j(x) ∂Fνc
∂φc,j
0000
φc,j=¯φc,j
+ · · ·
⎤
⎦,
(6.40)
where j labels the spectral parameters in φc and Δφc,j(x) = φc,j(x) −¯φc,j. It is
now possible to use the standard formalism to estimate several separate (but highly
correlated) ﬁelds, namely sc(x) and sc(x)Δφc,j(x) (for all j), from which one can
then obtain sc(x) and Δφc,j(x) separately. If necessary, second- and higher-order
terms in the Taylor expansion can be employed to obtain an increasingly accurate

6.6 Non-blind signal separation
149
approximation to sc(x, ν). One must remember, however, that one is typically con-
strained such that the total number of ﬁelds to be estimated does not exceed the
number of frequency channels at which observations are made.
6.6.3 Mixed-space maximum-entropy method
As our ﬁnal non-blind signal separation technique, we brieﬂy consider the mixed-
space MEM proposed by Barreiro et al. (2004), which circumvents many of the
problems associated with wholly Fourier-domain algorithms such as WF and
harmonic-space MEM, but at the cost of considerably larger computational require-
ments. In fact, in this approach, one does ultimately estimate (spatially constant)
spectral parameters, so it does go beyond a simple non-blind analysis, but it is best
considered here, alongside non-blind techniques.
In this method, one again begins by assuming ﬁxed, spatially constant values
for the mixing matrix parameters, Φ = Φ0. One also assumes that the covariance
matrix of the signals is known, S = S0, and works in terms of a-priori uncorre-
lated variables. Thus, once again, the only free parameters (initially) in the hidden
space are h = u, and these are represented in the Fourier domain, as above. More-
over, the prior on these variables is again taken as the entropic form Eq. (6.28),
namely,
π(u|S0, α) ∝exp[αS(u, μ)],
(6.41)
with μp = 1
2 for all p, although the WF quadratic approximation to the entropy is
another possibility.
The key difference in the mixed-space MEM is that the data space is now chosen
to be the pixel domain, rather than the Fourier domain. Thus, the χ2 misﬁt statistic
in Eq. (6.26) is replaced by
χ2(u; Φ) = (d −F−1RLu)TN−1(d −F−1RLu),
(6.42)
where N is now the noise covariance matrix in the pixel domain, and F−1 denotes
a (real) inverse Fourier transform matrix (for data deﬁned on the celestial sphere,
the Fourier transform would be replaced by a spherical harmonic transform). For
later convenience, we have also made explicit the dependence of χ2 on the spectral
parameters Φ.
By choosing the data space to be the pixel domain, anisotropic noise and sky-
cuts can easily be accommodated in the analysis (although inter-pixel noise corre-
lations are harder to incorporate). One could, in principle, also allow the mixing
matrix parameters to vary with position, but this was not considered by Barreiro
et al. (2004). On the other hand, by still representing the component ﬁelds in the

150
Signal separation in cosmology
Fourier domain, one preserves the advantage of easily including a-priori (cross)
power spectrum information in the algorithm.
Combining Eqs. (6.41) and (6.42), we see that the posterior again has the form
Eq. (6.29) and its maximum is located by numerical minimization of Eq. (6.30),
namely
FMEM(u; Φ, α) = 1
2χ2(u; Φ) −αS(u, μ),
(6.43)
but with χ2 now given by Eq. (6.42). Since the reconstruction is obtained in Fourier
space, but the misﬁt to the observed data is calculated in pixel space, one is not able
to perform a mode-by-mode optimization, and coupling between Fourier modes is
now taken into account. Unfortunately, the computational price one has to pay
is a single minimization for all the variables u simultaneously. Moreover, each
evaluation of the objective function (and its derivatives) requires inverse Fourier
transforms to be performed. The method is thus much slower than harmonic-space
MEM, and is only applicable to low-resolution signal separation problems;
Barreiro et al. (2004) apply the method to COBE data.
For a given value of α, the covariance matrix on the reconstruction residuals is
again estimated as in Eq. (6.31), although this matrix is now much larger. The op-
timal value of the regularization parameter α is again determined using the method
outlined in the previous section, but it is a more computationally demanding cal-
culation in this case. Details are presented in Barreiro et al. (2004), but it is worth
mentioning here that the use of optimal regularization again allows for the signal
covariance matrix S0 of the components to be updated iteratively.
Thus, for any given set of the (assumed spatially constant) spectral parameters
Φ, one iterates the signal covariance matrix to convergence, determining the opti-
mal value ˆα of the regularization parameter at each iteration, to obtain an estimated
signal separation ˆs(ˆα) = Lˆu(ˆα). The estimation of the spectral parameters is then
performed by repeating this entire procedure with different sets of spectral param-
eter values with the aim of minimizing an empirically deﬁned selection function.
In Barreiro et al. (2004), this function is a weighted linear combination of: the
χ2 value of the estimate ˆs(ˆα), its entropy, the cross-correlations between the est-
imates of the CMB and Galactic components and the dispersion of the estimated
CMB component. The best set of weights to use in the linear combination were
determined empirically from the analysis of simulated datasets.
A related, but more Bayesian, approach to the estimation of the spectral param-
eters (not explored by Barreiro et al.) would be to maximize the posterior distri-
bution Pr(Φ|d, ˆα) ∝Pr(d|Φ, ˆα) Pr(Φ), where the evidence factor Pr(d|Φ, ˆα) is
given by
Pr(d|Φ, ˆα) =

Pr(d|u, Φ) Pr(u|ˆα) du ∝

P(u|ˆα, Φ) du.
(6.44)

6.7 (Semi-)blind signal separation
151
In this expression, P(u|ˆα, Φ) ∝exp[−FMEM(u; ˆα, Φ)] is the MEM posterior
given the current set of spectral parameters, and the integral may be evaluated by
making a Gaussian approximation, as discussed in Section 6.6.2.
6.7 (Semi-)blind signal separation
In this section, we move on to consider methods for both semi-blind and blind
signal separation methods. As mentioned in Section 6.3.1, the usual (somewhat
arbitrary) distinction between semi-blind and blind methods is that the former pa-
rameterize the mixing matrix with some (physical) spectral parameters φ, whereas
the latter use the mixing matrix elements Fνc themselves as the parameters. Here
we combine the discussion of these classes of methods, since they are so similar. In
each case, there is a wide range of possibilities for the assumed (partial) knowledge
of parameters or imposed constraints, which may be quantiﬁed by simply deﬁning
the prior probability assigned to the parameter set Φ (see Section 6.5.1). More-
over, we begin by assuming the number of components Nc to be ﬁxed in advance,
although we will relax this condition in Section 6.7.4.
6.7.1 Pixel-domain parameter estimation
A direct approach to cosmological signal separation, ﬁrst advocated by Brandt
et al. (1994), is to treat it as a traditional parameter estimation problem in the pixel
domain; this approach has since been reﬁned by Eriksen et al. (2006) and further
extended by Dunkley et al. (2009).
In this approach, one works with data convolved to a common resolution, as in
Eq. (6.2), and takes both the data space and the parameterization of the component
ﬁelds to be in the pixel domain. It is further assumed that the window function
W(x) in Eq. (6.2) is either unity or zero, and one simply ignores all the pixels in
which it is zero. Thus, in general, the full data vector containing the values of all
the pixels in the unmasked region in all the frequency channels is given by
d = F(Φ)s + n,
(6.45)
where we have omitted the overbars used in Eq. (6.2) for the sake of notational
simplicity. Expressing this in component form, we have
dνp =
Nc

c=1
Fνc(Φ)scp + nνp,
(6.46)
where p labels pixels. Typically the mixing matrix is expressed in terms of a set of
physical parameters Φ = {φ(x)} which may, in general, depend on position (see
Section 6.3.1).

152
Signal separation in cosmology
Recall that our full hidden space of parameters is usually h = {Φ, s, S}. In this
semi-blind separation method, one usually attempts to make an inference on the
mixing matrix parameters Φ and the signal amplitudes s simultaneously, but not
on the signal covariance matrix S (although the general methodology does allow
for this). The assumed likelihood has the usual Gaussian form Eq. (6.11) and the
prior has the general form Eq. (6.13).
In the opinion of the current authors, the best methodology for setting the prior
Eq. (6.13) is ﬁrst to write π(s, S) = π(s|S)π(S). If one does not wish to infer S,
then one can simply ﬁx it to some known value S0, so that π(S) = δ(S −S0), and
then assume the corresponding prior on the signal amplitudes to have the Gaussian
form Eq. (6.14), namely,
π(s|S0) =
exp(−1
2sTS−1
0 s)
(2π)NpixNc/2|S0|1/2 .
(6.47)
As outlined below, however, neither Eriksen et al. nor Dunkley et al. follow this
prescription. The prior on the (physical) mixing matrix parameters π(Φ) will de-
pend on the form of the model chosen, so we will not specify it further here.
Combining the priors and likelihood using Bayes’ theorem, we obtain the joint
posterior distribution P(s, Φ) ≡Pr(s, Φ|d, S0, h), which is given by
P(s, Φ) = L(s, Φ)π(s|S0)π(Φ)
E
.
(6.48)
In general, this posterior has a large number of parameters and we must ﬁnd an
efﬁcient way of exploring it. We shall now look at how we can use this posterior
distribution to make inferences under a number of assumptions about the properties
of the signals and noise. As discussed in Section 6.3.3, the model m for the data
that enters into the likelihood function is typically linear in the signal amplitudes
s, but non-linear in the physical mixing matrix parameters Φ.
Uncorrelated signals and noise
As mentioned in Section 6.4.1, when both the data and component ﬁelds are des-
cribed in the pixel domain, the simplest approach is to ignore any spatial correla-
tions in the components and noise ﬁelds (as advocated by Eriksen et al. 2006). This
allows one to perform a pixel-by-pixel signal separation, where at each pixel one
has the much lower dimensionality inference problem given by Eq. (6.4).
Formally, this assumption means that the likelihood can be factorized L =
+Npix
p=1 Lp into the individual likelihoods for each pixel. Thus, for the pth pixel,
one has
Lp(sp, Φp) = exp{−1
2[dp −mp]TN−1
p [dp −mp]}
(2π)Nν/2|Np|1/2
,
(6.49)

6.7 (Semi-)blind signal separation
153
where each vector and matrix contains only the relevant values corresponding to
that pixel. Similarly, one could factorize the prior π(s|S0) on the signal amplitudes,
such that for the pth pixel,
π(sp|Sp0) =
exp(−1
2sT
p S−1
0p sp)
(2π)Nc/2|S0p|1/2 ,
(6.50)
where S0p could, in general, vary between pixels. We note that since both Np
and S0p do not depend on the parameters sp and Φp to be inferred, then the
denominators in Eqs. (6.49) and (6.50) can be safely ignored. In fact, Eriksen
et al. do not impose a prior of the form Eq. (6.50). Rather, they make no men-
tion of S0p and simply impose a uniform positivity prior on the signal ampli-
tudes (over a sufﬁciently wide range), such that the posterior is proportional to the
likelihood.
Let us now turn to the prior on the (physical) mixing matrix parameters Φ. In
principle, one could allow for these spectral parameters to vary from pixel to pixel
by considering a separate Φp vector at each pixel. In practice, however, one ex-
pects (hopes) that spatial variations in spectral parameters will occur only on much
larger scales. To take account of this, and thereby reduce the number of free pa-
rameters in the model, Eriksen et al. propose a two-stage hierarchical approach to
the signal separation, as follows. In the ﬁrst stage, the mapped region is divided
into large ‘pixels’ (or patches), in each of which the spectral parameters ΦP (and
the signal amplitudes sP ) are allowed to vary independently (where P labels large
pixels); Eriksen et al. adopt a wide uniform prior on each spectral parameter, but
this is easily modiﬁed. For each P, one then obtains estimates ΦP of the spec-
tral parameters by maximizing or (Gibbs) sampling from the corresponding pos-
terior; alternatively, via sampling one can obtain a full one-dimensional marginal
distribution for each of the spectral parameters ΦP (note that, in general, these
are non-linear parameters). In the second stage, the mapped region is divided into
small pixels. For each small pixel p ∈P, the spectral parameters are either taken
to be Φp = ΦP or their values are sampled from the one-dimensional marginals of
the ΦP found in the ﬁrst stage. Once spectral parameters are ﬁxed, estimates of
the signal amplitudes sp are obtained by maximizing or (Gibbs) sampling from the
corresponding conditional posterior for that (small) pixel. We note that, in general,
the signal amplitudes are linear parameters of the model, and so analytical max-
imization of the posterior and error estimation are often possible, although care
must be paid to positivity priors on the signal amplitudes. Figure 6.4 shows the
results of the above technique obtained by Eriksen et al. when applied to simulated
WMAP observations.

154
Signal separation in cosmology
Reconstruction
Residual
Estimated RMS error
CMB
Synch
Free–free
Dust
0 mk
100 mk
–1 mk
1 mk
0 mk
1 mk
0 mk
500 mk –20 mk
20 mk
0 mk
20 mk
0 mk
1000 mk –20 mk
20 mk
0 mk
20 mk
–200 mk
200 mk –10 mk
10 mk
0 mk
10 mk
Fig. 6.4. Results obtained using the pixel-by-pixel separation method applied to all-sky
simulated multi-frequency WMAP data smoothed to a common resolution of a 1-degree
FHWM Gaussian beam. (Reproduced from Eriksen et al. 2006.)
Correlated signals and noise
It is clear that, in reality, there will exist signal and noise correlations between
pixels, both intrinsic and induced by the convolution of the separate frequency
channels to a common resolution. Dunkley et al. (2009) therefore extended the
pixel-by-pixel approach outlined above to include noise correlations between pix-
els; thus the corresponding likelihood has the general form of Eq. (6.11). This
means that one must estimate all the components of the full signal vector s (and
spectral parameters vector Φ) simultaneously, which presents a far greater com-
putational challenge than performing a pixel-by-pixel analysis, and hence lim-
its the applicability of this approach to data convolved to a very low common
resolution.
As above, Dunkley et al. apply only a uniform positivity prior to the signal
amplitudes s. Similarly, they deﬁne larger ‘pixels’ (or patches) over which the
spectral parameters are assumed constant, i.e., Φp = ΦP for all p ∈P, and adopt
a mixture of physically motivated Gaussian and uniform priors on the ΦP . To

6.7 (Semi-)blind signal separation
155
explore the large joint (s, ΦP ) parameter space, Dunkley et al. have to employ a
Metropolis-within-Gibbs sampling procedure to achieve a reasonably efﬁciency.
6.7.2 Independent component analysis (ICA)
An alternative approach to (semi-)blind signal separation is to make some assump-
tions about the statistical properties of the signals. In particular, a common assump-
tion is that the signals are statistically independent, which leads to the important
class of methods known collectively as Independent Component Analysis (ICA).
In general, the assumption of statistical independence is much stronger than as-
suming the signals are merely uncorrelated; the latter leads to the Principal Com-
ponent Analysis (PCA) and related methods. It is worth noting, however, that for
Gaussian processes the notions of statistical independence and lack of correlation
coincide.
Once one has assumed the signals to be statistically independent, there are many
methods, both Bayesian and non-Bayesian, for performing a signal separation. In-
deed, several techniques for cosmological signal separation have been proposed
that are based on ICA and its variations, including FastICA, ensemble learning ICA
and spectral matching ICA (Maino et al. 2002, 2003; Donzelli et al. 2006; Stivoli
et al. 2006), and Correlated Component Analysis (CCA; Bonaldi et al. 2006, 2007).
We illustrate the ideas behind ICA with an example based on spectral matching
ICA (SMICA), which may be naturally considered in a Bayesian framework.
SMICA data model
The main idea underlying SMICA is to separate the signals on the basis of their
different power spectra. Indeed, since SMICA models the component as Gaussian
random ﬁelds, their power spectra and cross power spectra provide a complete
statistical description. Moreover, the assumption of Gaussianity means that statis-
tical independence between components is equivalent to them being uncorrelated.
In fact, as we shall see, SMICA can allow for correlations between components,
so overall the method’s name is rather misleading! As mentioned in Section 6.6,
SMICA may be considered as a ‘pre-processing’ step where (some of ) both the
mixing matrix parameters Φ and the signal covariance matrix S are determined,
which can then be used as input to non-blind separation methods such as the Wiener
ﬁlter and the harmonic-space MEM. In fact, SMICA can also be used to estimate
the noise covariance matrix N, if it is not known in advance.
Like the Wiener ﬁlter and harmonic-space MEM, SMICA typically adopts a
Fourier-domain data space, as discussed in Section 6.6 (although this is not neces-
sary). Similarly, one assumes (at least formally) that no window (or cut) has been
applied to the data, that the component ﬁelds and the noise ﬁelds in each channel

156
Signal separation in cosmology
are statistically homogeneous, and that the spectral parameters are independent of
position. Hence, at each Fourier mode k = kp separately, one has the data model
Eq. (6.7). In fact, although again not necessary, SMICA usually further assumes
that the data in each frequency channel has been convolved to a common resolu-
tion. Since the effective observing beam is thus the same in each channel, its effect
can be absorbed into the signals s, which now represent the convolved components.
Hence, at each Fourier mode k = kp separately, one has the effective data model,
dp = F(Φ)sp + np,
(6.51)
in which the mixing matrix F is the same for all modes (since the spectral be-
haviour is assumed to be spatially invariant), but we do not know it in advance.
In its original form, SMICA parameterized spectral behaviour directly in terms of
the elements of the mixing matrix, such that Φ = {Fνc}. It is straightforward,
however, to use some physically motivated spectral parameterization Φ = φ (see
Section 6.3.1).
SMICA likelihood
At each mode k = kp, the signals are assumed to be drawn from a Gaussian
distribution of zero mean and covariance
⟨spsp′⟩= δpp′Sp.
(6.52)
Since we have assumed that the signals are independent of one another (and hence
uncorrelated), the matrix Sp is diagonal, and we denote its cth diagonal element
as Spc. Similarly, the noise is assumed drawn from a Gaussian distribution of zero
mean and covariance
⟨npnp′⟩= δpp′Np.
(6.53)
We can also assume that the noise covariance matrix is diagonal, since the noise
is unlikely to be correlated between the instrumental channels. The νth diagonal
element of the matrix is denoted as Npν.
Combining the above, we ﬁnd that the covariance matrix of the data dp is
⟨dpdp′⟩= ⟨(Fsp + np)(Fsp′ + np′)T⟩= δpp′(FSpFT + Np) ≡δpp′Dp, (6.54)
where we have assumed that there is no correlation between the signals and the
noise. We can use this to write the likelihood L(Sp, Np, F) ≡Pr(dp|Sp, Np, F)
of the data in mode p given the signal covariance, noise covariance and mixing
matrix,
L(Sp, Np, F) = exp(−1
2dT
p D−1
p dp)
(2π)Nν/2|Dp|1/2 ,
(6.55)

6.7 (Semi-)blind signal separation
157
where the dependence on the parameters is concealed in the data covariance matrix.
Note that we are allowing, in general, for the noise covariance matrix also to be
estimated, since this is straightforward within the SMICA approach; it is, however,
simple to condition on a known noise covariance matrix if desired. In what follows,
it is more convenient to work with the log-likelihood,
ln L(Sp, Np, F) = −1
2(dT
p D−1
p dp + ln |Dp|) + const.,
(6.56)
where we may ignore the irrelevant additive constant. The full log-likelihood
ln L(S, N, F) ≡ln Pr(d|S, N, F) for this problem is simply the sum of the log-
likelihoods of the individual modes
ln L(S, N, F) = −1
2

p
(dT
p D−1
p dp + ln |Dp|).
(6.57)
Note that the log-likelihood does not depend on the amplitudes of the signals, only
their variance. By eliminating the signal variance, we have reduced the dimension
of the hidden parameter space.
We further reduce the dimension of the parameter space by binning the signal
and noise covariance matrices. We deﬁne a number of regions in Fourier space,
r = 1, . . . , Nr, where the signal and noise covariance matrices (and therefore the
data covariance matrix) take the same value, that is,
Sp = Sr
(6.58)
Np = Nr
(6.59)
Dp = Dr = FSrFT + Nr
(6.60)
for all modes p that fall within region r (p ∈r). We denote the number of modes
that fall within region r as nr. The usual procedure is to deﬁne the regions to be
annuli in Fourier space centred on the origin, so that the they correspond to bins in
the power spectrum. Thus the likelihood becomes
ln L(S, N, F) = 1
2
Nr

r=1

p∈r
(dT
p D−1
r dp + ln |Dr|)
(6.61)
= 1
2
Nr

r=1

p∈r
{Tr (dpdT
p D−1
r ) + ln |Dr|},
(6.62)
where in Eq. (6.62) we have used a matrix identity to rearrange the term involving
dp. Having compressed the hidden parameter space, we can also compress the data
space by deﬁning the ‘measured’ covariance matrix of the data in region r to be
4Dr = 1
nr

p∈r
dpdT
p .
(6.63)

158
Signal separation in cosmology
Substituting this into the likelihood, we obtain
ln L(S, N, F) = −1
2
Nr

r=1
nr{Tr ( 4DrD−1
r ) + ln |Dr|}.
(6.64)
The problem has been transformed into one where the hidden space consists of
the signal power spectra Src, the noise power spectra Nrν and the mixing matrix
F, which enter the likelihood through the data covariance matrices
Dr = Dr(Src, Nrν, F),
(6.65)
and the data space consists of the measured covariance matrices Eq. (6.63). The
measured covariance matrices contain NrN2
ν data which are used to constrain the
parameters. There are NrNc signal power spectrum parameters, NrNν noise power
spectrum parameters and NcNν mixing matrix parameters.
The SMICA likelihood Eq. (6.64) has a number of degeneracies which we must
be aware of when trying to make an inference about the sources. Since this is a blind
method, the ordering of the sources is not set in advance. We may exchange any
two sources, by exchanging their power spectra and the corresponding columns
of the mixing matrix, and leave the likelihood unchanged. We may also change
the sign of the elements of the mixing matrix for any component and leave the
likelihood unchanged. There is one further degeneracy between the elements of
the mixing matrix and the power spectrum: for any component, we may multiply
its elements of the mixing matrix by a factor λ, and multiply its power spectrum
by λ−2 and leave the likelihood unchanged. Some of these degeneracies can be
avoided by imposing appropriate priors on the parameters, as discussed below.
SMICA priors
The signal and noise power spectra are positive quantities, so the priors must reﬂect
this fact. If we do not know the scale of the power spectra parameters, then it
is appropriate to use the Jeffreys prior for them. If, instead, we have some prior
knowledge of how large the spectra parameters are expected to be, we could use an
exponential prior or a top-hat prior. In the latter case we would set the minimum
value allowed by the prior to be zero and the maximum value to be something
larger than the anticipated value of the power spectrum.
Some of the degeneracies in the likelihood can be avoided by careful choice of
the priors on the elements of the mixing matrix. In order to avoid the scaling de-
generacy between the power spectrum of a signal and the corresponding elements
of the mixing matrix, we can ﬁx the sum of the squares of the elements to be unity,
π(Fνc) = δ
) Nν

ν=1
F 2
νc −1
*
.
(6.66)

6.7 (Semi-)blind signal separation
159
To ﬁx the degeneracy in the sign of the mixing matrix elements for a component, we
can additionally constrain one of the elements to be positive or negative, depend-
ing on our knowledge of the components. The degeneracy involving the arbitrary
ordering of the components cannot be solved unless we know what some of the
components will be. This is addressed in Section 6.7.3.
SMICA posterior
Now we have the deﬁned the likelihood and the prior distributions, we can use
Bayes’ theorem to ﬁnd the posterior distribution P(S, N, F) ≡Pr(S, N, F|d)
of the parameters in the usual manner. This distribution is typically multi-modal,
however, and so exploring it can be a challenge. The most common approach is
to use numerical optimization to maximize the posterior, but one must be careful
to avoid the degeneracies outlined above. Once the maximum a posteriori point is
found, the uncertainties in the parameters can be found by using the curvature of
the log-posterior, which can be evaluated either analytically or numerically. Since
we have relatively few parameters, it may be possible to sample from the posterior
using MCMC. If we chose the species of MCMC algorithm carefully, it could be
used to explore the degeneracies between the parameters.
Once we have found the best-ﬁt parameters from the posterior, we may wish
to ﬁnd a physical interpretation of the results. This can be challenging since there
is much freedom in the model. We imposed priors on the parameters to ﬁx de-
generacies and to be able to perform the inference efﬁciently. These priors do not
necessarily correspond to physical constraints on the sources, and so we can relax
them when attempting to match the results to our physical understanding of the
problem. We can, for example, change the sign of the mixing matrix parameters
to match a known spectrum. The results of applying the SMICA approach to the
analysis of simulated Planck data similar to that plotted in Figure 6.2 are shown in
Figure 6.5.
Often the physical sources underlying the data are not statistically independent,
even though we have assumed that in order to be able to make the separation. If
this is the case, we can make linear combinations of the sources and their spectra
in the search for a physical interpretation. Alternatively, one can introduce off-
diagonal terms in the signal covariance matrices Sr, thereby allowing for explicit
correlations between components. One must be careful, however, not to introduce
too many additional free parameters into the model.
6.7.3 Correlated component analysis (CCA)
We can make a connection between blind separation and semi-blind separation by
considering an alternative parameterization of the likelihood in SMICA. Instead of

160
Signal separation in cosmology
8000
CMB input map power spectrum
Estimated CMB power spectrum
6000
4000
2000
l(l+1)Cl/2π (mK2)
0
–2000
0
1000
2000
3000
1.0000
0.1000
0.0100
0.0010
0.0001
Frequency (GHz)
CMB
Thermal SZ
DUST
Intensity (MJy/m2/sr)
1000
100
Fig. 6.5. Results obtained in applying the SMICA approach to all-sky simulated multi-
frequency Planck similar to that shown in Figure 6.2. Top: the estimated CMB power spec-
trum (reproduced from Patanchon 2003). Bottom: the recovered mixing matrix elements
for some components (diamonds) overplotted on the input frequency spectrum (solid lines).
(Reproduced from Snoussi et al. 2002.)
using the elements of the mixing matrix as our parameters, we could instead use a
physical parameterization. This provides a more prescriptive model that can break
the degeneracies in the likelihood. Since the physical sources are not necessarily
statistically independent, we can also again relax the requirement that the signal

6.7 (Semi-)blind signal separation
161
covariance matrices Sr are diagonal, although we must still be wary of having too
many degrees of freedom in the model to constrain using the data.
A very similar approach is adopted in correlated component analysis (CCA;
Bonaldi et al. 2006, 2007), except that CCA is formulated in the pixel domain.
In CCA, the signals are modelled by their covariance matrices evaluated between
points at a number of displacements in the pixel domain. The data consist of the
covariance matrices of the observed images evaluated at the same displacements.
Since this method is modelling the correlation structure of the signals, we can see
that it is the pixel-space equivalent of SMICA which is doing the same thing in
Fourier space. The list of displacements is therefore equivalent to the spectral bins
of SMICA, and we may formulate the likelihood in an identical manner. The phys-
ical parameterization of the mixing matrix allows the assumption of independence
to be relaxed, so the signals may be correlated, hence the name of the method.
6.7.4 Determining the optimal number of components
So far, we have assumed the number Nc of physical components is known. In
general, however, it is desirable to infer Nc from the data themselves. The theo-
retically most desirable approach is to include Nc as an additional variable in the
inference problem, on which one could then place a prior π(Nc), such as a Pois-
son distribution with mean equal to the expected number of physical components.
It is clear, however, that a crucial complication inherent to this approach is that
the length of the (hidden space) parameter vector h = (s, S, Φ, Nc) is variable,
since it depends on the unknown value Nc. If one were to explore the posterior
distribution using MCMC methods, for example, the proposal distribution in the
Metropolis–Hastings algorithm must be able to propose moves between spaces of
differing dimension. In this case, the detailed balance conditions must be carefully
considered (Green 1994; Phillips & Smith 1995), and can lead to algorithmic com-
plications, but reliable implementations of variable-dimension samplers do exist,
such as the BAYESYS sampler (Skilling 2004a).
The high computational cost of the above approach results largely from needing
to sample from a parameter space of variable dimension. One therefore usually
resorts to the ‘poor man’s’ approach for achieving virtually the same result ‘by
hand’, which is algorithmically much simpler and usually computationally faster.
Instead of allowing Nc to vary during the sampling process, one instead considers
a series of models HNs, each with a ﬁxed number of components Nc, where Nc
goes from Nmin to Nmax. For each such model, one can then sample from (or ﬁnd
the maxima of) a posterior deﬁned in a parameter space of ﬁxed length to obtain
parameter estimates (and errors) for the Nc components. Of equal importance, one
can use any of the standard methods to obtain an estimate of the evidence ENc

162
Signal separation in cosmology
for the model containing Nc components. One can then apply Bayesian model
selection to determine the favoured number of sources by ﬁnding the model that
maximizes ENcπ(Nc).
References
Barreiro, R. B., Hobson, M. P., Banday, A. J., Lasenby, A. N., Stolyarov V., Vielva P. and
G´orski, K. M. (2004). Mon. Not. Roy. Astron. Soc., 351, 515.
Bonaldi, A., Bedini, L., Salerno, E., Baccigalupi, C. and De Zotti, G. (2006). Mon. Not.
Roy. Astron. Soc., 373, 271.
Bonaldi, A. et al. (2007). Mon. Not. Roy. Astron. Soc., 382, 1791.
Bouchet, F. R., Gispert, R., Boulanger, F., Puget, J. L. (1997). In F. R. Bouchet,
R. Gispert, B. Guideroni and J. Tran Thanh Van, eds., Proceedings of the 16th
Moriond Astrophysics Meeting, Les Arcs, France. Gif-sur-Yvette: Editions Fronti`ere,
p. 481.
Bouchet, F. R. and Gispert, R. (1999). New Astron., 4, 443.
Brandt, W. N., Lawrence, C. R., Readhead, A. C. S., Pakianathan, J. N. and Fiola T. M.
(1994). Astrophys. J., 424, 1.
Donzelli, S. et al. (2006). Mon. Not. Roy. Astron. Soc., 369, 441.
Dunkley, J. et al. (2009). Astrophys. J. Supp., 180, 306.
Eriksen, H. K. et al. (2006). Astrophys. J., 641, 665.
Feroz, F. and Hobson, M. P. (2008). Mon. Not. Roy. Astron. Soc., 384, 449.
Feroz, F., Hobson, M. P. and Bridges, M. (2008). Mon. Not. Roy. Astron. Soc., submitted
(arXiv0809.3437).
Green, P. J. (1994). J. R. Stat. Soc., 56, 589.
Gull, S. F. (1989). In J. Skilling, ed., Maximum Entropy and Bayesian Methods. Dordrecht:
Kluwer, p. 53.
Gull, S. F. and Skilling J. (1990). The MEMSYS5 User’s Manual. Maximum Entropy Data
Consultants Ltd, Royston, UK.
Hobson, M. P., Bridle, S. L. and Lahav, O. (2002). Mon. Not. Roy. Astron. Soc., 335, 377.
Hobson, M. P., Jones, A. W., Lasenby, A. N. and Bouchet, F. R. (1998). Mon. Not. Roy.
Astron. Soc., 300, 1.
Hobson, M. P. and Lasenby, A. N. (1998). Mon. Not. Roy. Astron. Soc., 298, 905.
Kolmogorov, A. (1939). C. R. Acad. Sci., 208, 2043.
Levinson, N. (1947). J. Math. Phys., 25, 261.
Maino, D. et al. (2002). Mon. Not. Roy. Astron. Soc., 334, 53.
Maino, D., Banday, A. J., Baccigalupi, C., Perrotta, F. and G´orski, K. M. (2003). Mon. Not.
Roy. Astron. Soc., 344, 544.
Maisinger, K., Hobson, M. P. and Lasenby, A. N. (2004). Mon. Not. Roy. Astron. Soc., 347,
339.
O’Ruanaidh, J. J. K. and Fitzgerald, W. J. (1996). Numerical Bayesian Methods Applied to
Signal Processing. New York: Springer-Verlag.
Patanchon, G. (2003). New Astron. Rev., 47, 871.
Phillips, D. B. and Smith, A. F. M. (1995). In W. R. Gilks, S. Richardson and D. J.
Spiegelhalter, eds., Markov Chain Monte Carlo in Practice. London: Chapman
& Hall.
Press, W. H., Teukolsky, S., Vetterling, W. T. and Flannery, B. P. (1997). Numerical Recipes
in C. Cambridge: Cambridge University Press, p. 394.
Shannon, C. (1948). Bell System Tech. J., 27, 379.

6.7 (Semi-)blind signal separation
163
Sivia, D. S. and Skilling, J. (2006). Data Analysis: A Bayesian Tutorial. Oxford: Oxford
University Press.
Skilling, J. (1989). In J. Skilling, ed., Maximum Entropy and Bayesian Methods. Dordrecht:
Kluwer, p. 45.
Skilling, J. (2004a). AIP Conf. Proc., 707, 388.
Skilling, J. (2004b). AIP Conf. Proc., 735, 395.
Snoussi, H., Patanchon, G., Macias-P´erez, J. F., Mohammad-Djafari, A. and Delabrouille,
J. (2002). AIP Conf. Proc., 617, 125.
Stivoli, F., Baccigalupi, C., Maino, D. and Stompor, R. (2006). Mon. Not. Roy. Astron. Soc.,
372, 615.
Stolyarov, V., Hobson, M. P., Ashdown, M. A. J. and Lasenby A. N. (2002). Mon. Not. Roy.
Astron. Soc., 336, 97.
Stolyarov, V., Hobson, M. P., Lasenby, A. N. and Barreiro, R. B. (2005). Mon. Not. Roy.
Astron. Soc., 357, 145.
Tegmark, M. and Efstathiou, G. (1996). Mon. Not. Roy. Astron. Soc., 281, 1297.
Titterington, D. M. (1985). Astron. Astrophys., 144, 381.
Wiener, N. (1949). Extrapolation, Interpolation and Smoothing of Stationary Time Series,
with Engineering Applications. Cambridge, MA: MIT Press.
Zaroubi, S., Hoffman, Y., Fisher, K. B. and Lahav, O. (1995). Astrophys. J., 449, 446.


Part II
Applications


7
Bayesian source extraction
M. P. Hobson, Grac¸a Rocha and Richard S. Savage
Source extraction is a generic problem in modern observational astrophysics and
cosmology. Indeed, one of the major challenges in the analysis of astronomical
observations is to identify and characterize a localized signal immersed in some
general background. Typical one-dimensional examples include the extraction of
point or extended sources from time-ordered scan data or the detection of absorp-
tion or emission lines in quasar spectra. In two dimensions, one often wishes to
detect point or extended sources in astrophysical images that are dominated either
by instrumental noise or contaminating diffuse emission. Similarly, in three di-
mensions, one might wish to detect galaxy clusters in large-scale structure surveys.
Moreover, the ability to perform source extraction with reliable, automated meth-
ods has become vital with the advent of modern large-area surveys too large to be
inspected in detail ‘by eye’. Indeed, much of the science derived from the study
of astronomical sources, or from the background in which they are immersed, pro-
ceeds directly from accurate source extraction.
In extracting sources from astronomical data, we typically face a number of
challenges. Firstly, there is instrumental noise. Nonetheless, it is often possible to
obtain an accurate statistical characterization of the instrumental noise, which can
then be used to compensate for its effects to some extent. More problematic are
any so-called ‘backgrounds’ to the observation. These can be astrophysical or cos-
mological in origin, such as Galactic emission, cosmological backgrounds, faint
source confusion, or even simply emission from parts of the telescope itself. These
backgrounds are often much harder to characterize and often constitute an in-depth
study in themselves. A prime example is provided by high-resolution observations
of the cosmic microwave background (CMB). In addition to the CMB emission,
which varies on a characteristic scale of order ∼10 arcmin, one is often interested
in detecting emission from discrete objects such as extragalactic ‘point’ (i.e., beam-
shaped) sources or the Sunyaev–Zel’dovich (SZ) effect in galaxy clusters, which
167

168
Bayesian source extraction
have characteristic scales similar to that of the primordial CMB emission. More-
over, the rms of the instrumental noise in CMB observations is often comparable to
the amplitude of the discrete sources. We may also have to contend with systematic
effects such as glitches that can be caused by cosmic ray hits on the detectors of
space telescopes.
In this chapter, we will focus on the important speciﬁc example of extracting
discrete sources from a diffuse background in a two-dimensional astronomical im-
age, although our discussion will, in general, be applicable to datasets of arbitrary
dimensionality.
7.1 Traditional approaches
Several excellent, general-purpose packages exist for performing source extrac-
tion from two-dimensional images, such as DAOﬁnd (Stetson 1992) and SExtrac-
tor (Bertin & Arnouts 1996). In both cases, it is assumed that the background is
smoothly varying, with a characteristic length scale much larger than that of the dis-
crete source being sought. For example, SExtractor approximates the background
emission by a low-order polynomial, which is subtracted from the image. Object
detection is then performed by ﬁnding sets of connected pixels above some given
threshold.
Nonetheless, when one has some prior knowledge of the properties of the
sources, backgrounds or instrumental noise, it is clearly advantageous to make use
of this information. Over the years, a number of methods have been devised that
use various sets of information to perform ‘optimal’ source extraction (subject to
certain assumptions). Many such techniques are based on the concept of ﬁltering
the data to enhance, relative to the background, the signal due to sources with a
certain set of characteristics.
In these approaches, one typically applies a linear ﬁlter to the original image
d(x) and then analyses the resulting ﬁltered ﬁeld,
df(x) =

ψ(x −y)d(y) d2y.
(7.1)
This process is usually performed by Fourier transforming the image d(x) to ob-
tain ˜d(k), multiplying by some ﬁlter function ˜ψ(k) and inverse Fourier transform-
ing. The form of the ﬁlter function clearly determines which Fourier modes are
suppressed and by what factor. In the simplest cases ˜ψ(k) may set to zero the am-
plitudes of all k-modes with |k| above (or below) some critical value kc, and one
obtains a simple low-pass (or high-pass) Fourier ﬁlter. Alternatively, one may re-
tain only some speciﬁc set of Fourier modes by setting to zero all modes with |k|
lying outside some range kmin to kmax (see, for example, Chiang et al. 2002).

7.1 Traditional approaches
169
More generally, suppose one is interested in detecting sources with some given
spatial template t(x) (normalized for convenience to unit peak amplitude). If the
original image contains Ns sources at positions Xi with amplitudes Ai, together
with contributions from backgrounds and additive instrumental noise, we may
write
d(x) ≡s(x) + b(x) + n(x) =
Ns

i=1
Ait(x −Xi) + b(x) + n(x),
(7.2)
where s(x) is the signal of interest, b(x) is the (diffuse) ‘background’ (or sum
of backgrounds) in which the sources are embedded, and n(x) is the instrumental
noise. In many cases, the background b(x) is well modelled as a (sum of ) stochastic
process(es), in which case it is common to consider the ‘generalized noise’ g(x) ≡
b(x) + n(x), which is deﬁned as all contributions to the image aside from the
sources of interest.
In the latter case, it is well known (see, e.g., Haehnelt & Tegmark 1996) that,
under the assumption that g(x) is a statistically homogeneous random ﬁeld, one
can derive a ‘matched ﬁlter’ (MF) ψ(x) such that the ﬁltered ﬁeld Eq. (7.1) has
the following properties: (i) df(Xi) is an unbiassed estimator of Ai; and (ii) the
variance of the ﬁltered generalized noise ﬁeld gf(x) is minimized. Various exten-
sions to the MF have been proposed, in which further constraints on the ﬁltered
ﬁeld are imposed but, in each case, one may consider the ﬁltering process as ‘op-
timally’ boosting (in a linear sense and subject to certain constraints) the signal
from discrete sources, with a given spatial template, and simultaneously suppress-
ing emission from the background.
Most notable amongst the extensions to the MF is the scale-adaptive ﬁlter (SAF;
Sanz, Herranz & Martinez-Gonzalez 2001), for which one additionally requires
that (iii) the expected value of the ﬁltered ﬁeld at a source position, when consid-
ered as a function of the (unknown) spatial extent R of the source, has an extremum
at some value R = R0 that can be related to the source’s true spatial extent. Thus
the drop in gain of the SAF relative to the MF that results from imposing the ad-
ditional condition (iii) is offset by the ability to determine the scale of the source,
if this is unknown (see McEwen, Hobson and Lasenby (2006) for a full discus-
sion). The SAF is thus particularly useful if the set of sources in the map do not all
have the same spatial extent. In the case of the SZ effect, for example, one might
model all clusters as having the same functional form of the template, but with an
unknown ‘core radius’ that differs from one cluster to another. If one uses the MF
in this case, one needs to repeat the ﬁltering process at a number of spatial scales
to obtain several ﬁltered ﬁelds, each of which would optimally boost sources with
that scale (Herranz et al. 2002a). Closely related to the SAF are the wavelet ﬁlters

170
Bayesian source extraction
(see, e.g., Lopez-Caniego et al. 2005; Barreiro et al. 2003). More recently, Makovoz
and Marleau (2005) have derived a ﬁlter of this type using the Bayesian formal-
ism, thus allowing for the explicit inclusion of prior knowledge. It is also possible
through the construction of directional ﬁlters (see, e.g., McEwen et al. 2008) to
accommodate sources that are not circularly symmetric and have orientations that
vary from one source to another.
Finally, it is unnecessary to restrict oneself to analyzing a single astronomical
image at a particular observing frequency. In many cases, images at different fre-
quencies may be available. Once again, the SZ effect provides a good example.
CMB satellite missions, such as the Planck experiment, provide high-sensitivity,
high-resolution observations of the whole sky at a number of different observing
frequencies. Owing to the distinctive frequency dependence of the thermal SZ ef-
fect, it is better to use the maps at all the observed frequencies simultaneously
when attempting to detect and characterize thermal SZ clusters hidden in the emis-
sion from other astrophysical components. The generalization of the above ﬁlter
techniques to multi-frequency data is straightforward, and leads to the concept of
multi-ﬁlters (Herranz et al. 2002b). We also note that an alternative approach to
this problem, which relies only on the well-known frequency dependencies of the
thermal SZ and CMB emission, has been proposed by Diego et al. (2002).
7.2 The Bayesian approach
The traditional approaches outlined above have been shown to produce good re-
sults, but the ﬁltering process is only optimal among the rather limited class of
linear ﬁlters and is logically separated from the subsequent source detection step
performed on the ﬁltered map(s). Moreover, the above approaches are not, in gen-
eral, sufﬁciently ﬂexible to take full account of the statistical characteristics of the
(generalized) noise background, particularly when it is non-Gaussian. It is impor-
tant, for example, to be able to take proper account of the Poissonian nature of
many forms of data, most notably low-photon-count CCD output from X-ray ob-
servations. Also, the traditional techniques do not allow for the straightforward
inclusion of prior information on the sources of interest, either in terms of their
physical structure or their abundance.
As a result, Hobson and McLachlan (2003; hereinafter HM03) introduced a
Bayesian approach to the detection and characterization of discrete sources in a
diffuse background, which allows for the inclusion of all pertinent information; the
general framework we present below follows this closely. As in the ﬁltering tech-
niques, the method assumes a parameterized form for the sources, but the optimal
values of these parameters, and their associated errors, are obtained in a single step
by evaluating their full posterior distribution. In principle, any statistical form for

7.2 The Bayesian approach
171
the (generalized) noise can be accommodated by deﬁning an appropriate likelihood
function; indeed the Bayesian methodology has also been applied in the Poisson
noise regime (see, e.g., Guglielmetti et al. 2004). If available, one can also place
physical priors on the parameters deﬁning a source and on the number of sources
present. Moreover, as discussed below, one can decide whether each peak found
in the posterior distribution corresponds to a real source or a background ﬂuctua-
tion by performing a Bayesian model comparison using (an approximation to) the
evidence. The approach therefore represents the theoretically optimal method for
performing parameterized source extraction.
7.2.1 Discrete sources in a background
To keep our discussion as general as possible, let us denote the totality of our
available data by the vector d. This may represent the pixel values in a single
image, or in a collection of images, such as a multi-frequency dataset. Equally,
d could represent the Fourier coefﬁcients of the image(s), or coefﬁcients in some
other basis, but we will concentrate here on the pixel basis for simplicity.
We ﬁrst consider the contribution to the data of the discrete sources of inter-
est. Let us suppose that we wish to detect and characterize some set of (two-
dimensional) discrete sources, each of which is described by a template τ(x; p). In
the analysis of single-frequency maps, one could take the template to be the (pix-
elized) shape of the source after convolution with the beam. For multi-frequency
data, however, it is more natural to treat the intrinsic source shape and the beam pro-
ﬁles separately, in which case the template is taken to describe the former (perhaps
at a speciﬁc observing frequency, if the intrinsic shape of the source is frequency
dependent).
The template is deﬁned in terms of a set of parameters p that might typically
denote (collectively) the position (X, Y ) of the source, its amplitude A and some
measure R of its spatial extent. A particular example is the circularly symmetric
Gaussian-shaped source deﬁned by
τ(x; p) = A exp
$
−(x −X)2 + (y −Y )2
2R2
%
,
(7.3)
so that p = {X, Y, A, R}. In what follows, we will consider only circularly sym-
metric sources, but our general approach accommodates templates that are, for ex-
ample, elongated in one direction, at the expense of increasing the dimensionality
of the parameter space.1
1 Such an elongation can be caused by asymmetry of the beam, source or both. If our model assumes a circularly
symmetric source but an asymmetric beam, the elongation will be constant across the map, but not if the
sources are intrinsically asymmetric. In the latter case, one thus has to introduce the source position angle as
a parameter, in addition to the elongation.

172
Bayesian source extraction
If Ns sources of interest are present in the map and the contribution of each
source to the data is additive, and so too is the noise, we may write
d =
Ns

k=1
s(pk) + b(q) + n(r),
(7.4)
where the ‘signal vector’ s(pk) denotes the (pixelized) contribution to the data
vector from the kth discrete source, which includes, for example, the effect of the
observing beam(s). The vector b(q) denotes the (pixelized) background contribu-
tion, which may itself be characterized in terms of a set of (potentially unknown)
parameters q; again b must include the effect of the observing beam(s). The vector
n is the (pixelized) instrumental noise, the statistical description of which may be
characterized by a number of (potentially unknown) parameters r. As mentioned
above, it is often useful to combine the background and instrumental noise contri-
butions into the generalized noise vector g(q, r). It is also convenient occasionally
to denote the total contribution to the data by all the sources simply by s(p), where
p denotes collectively the source parameters {p1, p2, . . . , pNs}.
Clearly, we wish to use the data d to place constraints on the values of the un-
known source parameters Ns and pk (k = 1, . . . , Ns). If the values of any back-
ground and noise parameters q and r are unknown, then these are usually nuisance
parameters and should ideally be marginalized over. In some cases, however, one
may wish to estimate the values of (a subset of) these parameters as well.2 In any
case, we will denote the full parameter space by Θ = {Ns, p, q, r}.
7.2.2 Bayesian inference
Bayesian inference methods provide a consistent approach to the estimation of a
set of parameters Θ in a model (or hypothesis) H for the data d. Bayes’ theorem
states that
Pr(Θ|d, H) = Pr(d| Θ, H) Pr(Θ|H)
Pr(d|H)
,
(7.5)
where Pr(Θ|d, H) ≡P(Θ) is the posterior probability distribution of the para-
meters, Pr(d|Θ, H) ≡L(Θ) is the likelihood, Pr(Θ|H) ≡π(Θ) is the prior, and
Pr(d|H) ≡E is the Bayesian evidence.
In parameter estimation, the normalizing evidence factor is usually ignored,
since it is independent of the parameters Θ. This (un-normalized) posterior con-
stitutes the complete Bayesian inference of the parameter values. Inferences are
2 It should be remembered, however, that our general expression Eq. (7.4) already assumes additive noise. This
is often true, but an important case for which Eq. (7.4) is not valid is when the data are Poisson distributed;
this is discussed further in Section 7.2.3.

7.2 The Bayesian approach
173
usually obtained either by taking samples from the (un-normalized) posterior us-
ing Markov chain Monte Carlo (MCMC) methods, or by locating its maximum
(or maxima) and approximating the shape around the peak(s) by a multivariate
Gaussian.
In contrast to parameter estimation problems, in model selection the evidence
takes the central role and is simply the factor required to normalize the posterior
over Θ:
E =

L(Θ)π(Θ)dDΘ,
(7.6)
where D is the dimensionality of the parameter space. As the average of the likeli-
hood over the prior, the evidence is larger for a model if more of its parameter space
is likely and smaller for a model with large areas in its parameter space having low
likelihood values, even if the likelihood function is very highly peaked. Thus, the
evidence automatically implements Occam’s razor: a simpler theory with compact
parameter space will have a larger evidence than a more complicated one, unless
the latter is signiﬁcantly better at explaining the data. The question of model se-
lection between two models H0 and H1 can then be decided by comparing their
respective posterior probabilities given the observed dataset d, as follows:
Pr(H1|d)
Pr(H0|d) = Pr(d|H1) Pr(H1)
Pr(d|H0) Pr(H0) = E1
E2
Pr(H1)
Pr(H0),
(7.7)
where Pr(H1)/ Pr(H0) is the a-priori probability ratio for the two models, which
can often be set to unity but occasionally requires further consideration. Unfortu-
nately, evaluation of the multidimensional integral (7.6) is a challenging numeri-
cal task, but sampling-based methods do exist, such as thermodynamic integration
(see, e.g., ´O’Ruanaidh & Fitzgerald 1996) or, better, nested sampling (Skilling
2004a; Sivia & Skilling 2006). Moreover, a fast approximate method for evidence
evaluation is to model the posterior as a multivariate Gaussian centred at its peak(s)
(see, e.g., Hobson, Bridle & Lahav 2002).
7.2.3 Deﬁning the posterior distribution
For any given model H of the source signal s(p), background b(q) and noise
n(r), one can write down a likelihood function L(Θ) ≡Pr(d|Θ, H). As a typical
example, suppose that the background is deterministic (rather than stochastic) and
the instrumental noise is a (zero mean) statistically homogeneous Gaussian random
ﬁeld with covariance matrix N = ⟨nnT⟩. In this case, the likelihood function takes
the form (including the parameter dependencies explicitly)

174
Bayesian source extraction
L(Θ) =
exp
:
−1
2[d −s(p) −b(q)]TN−1(r)[d −s(p) −b(q)]
;
(2π)Npix/2|N(r)|1/2
.
(7.8)
If instead the background is also a (zero mean) statistically homogeneous Gaussian
random ﬁeld, then so too is the generalized noise g = b + n. Denoting its covari-
ance matrix by G = ⟨ggT⟩, the likelihood function can then be written
L(Θ) =
exp
:
−1
2[d −s(p)]TG−1(q, r)[d −s(p)]
;
(2π)Npix/2|G(q, r)|1/2
.
(7.9)
Another important example is Poisson-distributed data, such as those obtained us-
ing photon counting CCDs. As mentioned in Section 7.2.1, the ansatz (7.4) is not
valid in this case, since the noise is not additive. Nonetheless, the likelihood can
still be deﬁned. For example, in the case where the background is taken as deter-
ministic, this reads
L(Θ) =
Npix

i=1
e−λiλdi
i
di!
,
(7.10)
where di (an integer) is the ith element of the data vector and we have deﬁned
λi ≡si(p) + bi(q). Note that for Poisson data the ‘noise parameters’ r are absent.
Since the (log-)likelihood is usually the computationally most expensive quan-
tity to calculate, it is worth noting a simple method for reducing this burden consid-
erably in the (frequently occurring) case that the background and noise parameters
q and r are known. Consider, for example, the case Eq. (7.9), for which the log-
likelihood function now reads
ln L(p) = c −1
2[d −s(p)]TG−1[d −s(p)],
(7.11)
where c is an unimportant constant. Following Carvalho, Rocha and Hobson
(2008), we can rewrite the log-likelihood as
ln L(p) = c′ −1
2s(p)TG−1s(p) + dTG−1s(p),
(7.12)
where c′ = c−1
2dTG−1d is again independent of the parameters p. The advantage
of this formulation is that the part of the log-likelihood dependent on the parame-
ters p consists only of products involving the data and the signal. Since the signal
from a (set of) discrete source(s) with parameters p is only (signiﬁcantly) non-
zero in a limited region centred on the putative position(s), one need only calculate
the quadratic forms in Eq. (7.12) over a very limited number of pixels, whereas
the quadratic form in the standard version Eq. (7.11) must be calculated over all

7.3 Variable-source-number models
175
the pixels in the image. Clearly, similar computational savings can be made for
likelihoods of the form Eqs. (7.8) and (7.10).
Once the likelihood has been deﬁned, all that remains is to specify the prior
π(Θ) ≡Pr(Θ|H) on the parameters for the model H. For most applications,
it is natural to assume that the parameters p, q and r, characterizing the signal,
background and noise respectively, are independent. Moreover, it is also reasonable
to assume that the number of sources Ns and the parameters pk for each source are
mutually independent, so that
π(Θ) = π(Ns)π(p1)π(p2) . . . π(pNs)π(q)π(r).
(7.13)
As mentioned above, the parameters pk that characterize the kth source will typi-
cally consist of its position Xk and Yk, amplitude Ak and spatial extent Rk, and the
priors imposed on these parameters will generally depend on the application. For
example, one might impose uniform priors on Xk and Yk within the borders of the
image, whereas the priors on Ak and Rk may be provided by some physical model
of the sources one wishes to detect. Similarly, one may impose a prior on the num-
ber of unknown sources Ns, which is clearly a discrete parameter. For example, if
the sources of interest are not clustered on the sky and have a mean number density
μ per image area, then one would set
π(Ns) = e−μμNs
Ns!
.
(7.14)
It is worth noting here that in source extraction problems the posterior distribu-
tion is typically an extremely complicated function of the parameters Θ, possessing
many modes and elongated degeneracies. One therefore requires robust methods to
explore (or maximize) the posterior distribution reliably.
7.3 Variable-source-number models
The theoretically most desirable approach is to attempt to detect and characterize
all the sources in the image simultaneously by sampling from the (un-normalized)
posterior distribution. In other words, the Bayesian purist would attempt to infer
simultaneously the full set of parameters Θ ≡(Ns, p1, p2, . . . , pNs, q, r). In par-
ticular, this allows one straightforwardly to include prior information regarding the
number of sources expected in the image.
It is clear, however, that a crucial complication inherent to this approach is that
the length of the parameter vector Θ is variable, since it depends on the unknown
value Ns. If one were to explore the posterior distribution using MCMC methods,
for example, the proposal distribution in the Metropolis–Hastings algorithm must
be able to propose moves between spaces of differing dimension. In this case, the

176
Bayesian source extraction
Fig. 7.1. The toy problem discussed in HM03. The 200 × 200 pixel test image (left panel)
contains eight discrete Gaussian-shaped sources of varying widths and amplitudes. The
corresponding data map (right panel) has independent Gaussian pixel noise added with an
rms of 2 units.
detailed balance conditions must be carefully considered (Green 1994; Phillips &
Smith 1995), and can lead to algorithmic complications, but reliable implemen-
tations of variable-dimension samplers do exist, such as the BAYESYS sampler
(Skilling 2004b).
Nonetheless, further complications occur that are particular to the source extrac-
tion problem. Most notable is the source-swapping degeneracy. If one is sampling
from a parameter space with Ns > 1, then one can swap the values of the para-
meters pk and pk′ of any two sources without affecting the value of the posterior.
One must therefore take care when assigning the source parameter vectors in each
output sample to particular sources identiﬁed in the image. Another practical difﬁ-
culty results if the prior π(Ns) on the number of sources remains non-zero at large
values of Ns, since then the size of the corresponding parameter space to be sam-
pled becomes very large. Consequently, the algorithm can be slow to burn in and
typically requires a lot of CPU time.
This variable-source-number approach has been illustrated by application to a
simple toy source extraction problem by HM03. In Figure 7.1, the left panel shows
a 200×200 pixel test image containing eight Gaussian sources deﬁned by Eq. (7.3),
with different values of the parameters Xk, Yk Ak and Rk (k = 1, . . . , 8). The X
and Y position coordinates are drawn independently from the uniform distribution
U(0, 200), whereas the amplitude A and size R of each source are drawn inde-
pendently from the uniform distributions U(0.5, 1) and U(5, 10), respectively. The
right panel of Figure 7.1 shows the corresponding data map, which has indepen-
dent (‘white’) Gaussian pixel noise added, with an rms of 2 units. This corresponds

7.3 Variable-source-number models
177
Number of samples
500
1000
1500
10
6
0
2
4
8
12
Number of objects
Fig. 7.2. Left: histogram of the number of post burn-in samples obtained in each subspace
corresponding to a different number of sources Ns. Right: the samples obtained for Ns = 7,
projected into the (X, Y ) subspace; the ellipses indicate the samples used in the calculation
of the properties of the sources.
to a signal-to-noise ratio of 0.25–0.5 as compared with the peak emission in each
source; we note that no sources are visible to the naked eye.
In the analysis of this toy problem, we have no unknown background parameters
q or noise parameters r. On the number of sources Ns, we assume the Poisson
prior Eq. (7.14) with a mean of μ = 4, which is purposely chosen to be some-
what smaller than the actual number of sources Ns = 8. Since the Poisson prior
imposes no upper limit on the possible number of sources, the overall parameter
space under consideration is formally the countably inﬁnite union of subspaces
Θ = <∞
Ns=0 ΘNs, where ΘNs = {p1, . . . , pNs} denotes the 4Ns-dimensional
space corresponding to the model with Ns sources. The parameters of the kth
source are pk = {Xk, Yk, Ak, Rk}. The priors on Xk and Yk (k = 1, . . . , Ns)
are (correctly) assumed to be U(0, 200), whereas the priors for Ak and Rk are
taken as U(0, 2) and U(3, 12), respectively.
The BAYESYS sampler was used, running ten interacting Markov chains, to ob-
tain 5000 post burn-in samples from the variable-length parameter space Θ. The
left panel of Figure 7.2 shows a histogram of the number of samples obtained
in each subspace of different dimension, showing the most favoured number of
sources is Ns = 7. One is free to use the 5000 post burn-in samples in a variety of
ways to place limits on the parameters Θ. For illustration only, in the right panel
of Figure 7.2 we plot the samples obtained for the case Ns = 7, projected into the
(X, Y ) subspace. We see that there exist seven main areas in which the samples
are concentrated, which we highlight with ellipses. Comparison with Figure 7.1

178
Bayesian source extraction
(left panel) shows that each of these areas corresponds to a real source. The mean
and standard deviation of the parameters {Xk, Yk, Ak, Rk} (k = 1, . . . , 7) for
each detected source were calculated from the samples contained in each ellipse,
and were found to recover the input parameters to reasonable accuracy. It must be
noted, however, that the two overlapping sources in Figure 7.1 have been confused,
and yield a single detected ‘source’.
Clearly more optimal strategies exist for using the samples to characterize the
sources and distinguish between real and spurious detections. We shall not pursue
them further here, however, owing to the rather computationally intensive nature
of the above approach; the above analysis required ∼17 hours of CPU time on a
standard desktop.
7.4 Fixed-source-number models
The high computational cost of the above approach results largely from needing to
sample from a parameter space of variable dimension. Fortunately, there exists a
‘poor man’s’ approach for achieving virtually the same result ‘by hand’, which is
algorithmically much simpler and usually computationally faster.
Instead of allowing Ns to vary during the sampling process, one instead consid-
ers a series of models HNs, each with a ﬁxed number of sources Ns, where Ns goes
from (say) zero to some predeﬁned maximum value Nmax. For each such model,
one can then sample from (or ﬁnd the maxima of ) a posterior deﬁned in a parameter
space of ﬁxed length to obtain parameter estimates (and errors) for the Ns sources.
Of equal importance, one can use any of the standard methods to obtain an esti-
mate of the evidence ENs for the model containing Ns sources. One can then apply
Bayesian model selection to determine the favoured number of sources by ﬁnding
the model that maximizes ENsπ(Ns) (see, e.g., Marshall 2006). It should be noted,
however, that, for Ns > 1, this approach still exhibits the source-swapping degen-
eracy mentioned above. Moreover, the computational cost is still typically rather
large, albeit less than the variable-source-number approach.
7.5 Single-source models
A very special case of the ﬁxed-source-number approach is simply to set Ns = 1
throughout the analysis; this is equivalent to imposing the prior π(Ns) = 1 if
Ns = 1 and zero otherwise. Since the model consists of just a single source,
the full source parameter is p = {X, Y, A, R} (say), which is ﬁxed and only
4-dimensional. Although one has ﬁxed Ns = 1, it is important to understand
that this does not restrict one to detecting just a single source in the map. Indeed,
by modelling the data in this way, one would expect the posterior distribution to

7.5 Single-source models
179
Fig. 7.3. The 2-dimensional conditional log-posterior distributions in the (X, Y ) subspace
for the toy problem illustrated in Figure 7.1, where the model contains a single source
parameterized by p = {X, Y, A, R}. The amplitude A and size R are conditioned at A =
0.75, R = 5 (left panel) and A = 0.75, R = 10 (right panel).
possess numerous local maxima in the 4-dimensional parameter space, some of
which correspond to the location in this space of sources present in the image. This
is illustrated in Figure 7.3, where we see that maxima do indeed occur at positions
corresponding to each of the input sources in Figure 7.1, but there exist numerous
other maxima that do not coincide with the position of a real source, but instead
occur because the background noise in some areas has ‘conspired’ to give the im-
pression that a source might be present.
HM03 show that the vastly simpliﬁed approach of setting Ns = 1 is both fast
and reliable when the sources of interest are spatially well separated (but can en-
counter problems when sources overlap). In what follows, we will assume for sim-
plicity that the background and noise parameters q and r are known, and so we
are interested only in determining the source parameters p. The process of source
extraction then reduces to locating the local maxima of the posterior distribution in
the parameter space p; we consider various strategies for performing this task in
the following subsections, and for assigning uncertainties to the derived parameter
values associated with each posterior peak.
First, however, we discuss how one may decide whether a detected maximum
corresponds to a real source by evaluating the evidence associated with that peak
for two competing models for the data. The precise deﬁnition of the models H0
and H1 can vary, but we shall adopt here a formulation in which each model is
parameterized by the same parameters, for example p = {X, Y, A, R}, but with
different priors.

180
Bayesian source extraction
In this case the evidence for each model is
Ei =

L(p)πi(p)dp,
(7.15)
where the likelihood function L(p) is the same for both models. Although not
required by the method, we will assume for illustration that for each model the
prior is separable, so that for i = 0, 1,
πi(p) = πi(X)πi(A)πi(R),
(7.16)
where X = (X, Y ) is the vector position of the source.
It is convenient to consider some general spatial region S in the image, and take
our two models for the data to be
H0 = ‘there is no source with its centre lying in the region S’,
H1 = ‘there is one source with its centre lying in the region S’.
In this case, the priors on the source position in the two models are uniform:
π0(X) = π1(X) ≡π(X) = 1/|S| if X ∈S, and zero otherwise, where |S| is
the area of the region S.3 We also assume the prior on R is the same for both mod-
els, so π0(R) = π1(R) ≡π(R), but the priors on A are substantially different.
Guided by the forms for H0 and H1 given above, we take π0(A) = δ(A) (which
forces A = 0) and π1(A) to be an appropriate prior probability distribution on the
amplitude of the sources of interest, which is non-zero in some range [0, Amax].
One could, of course, consider alternative deﬁnitions of these hypotheses, such as
setting H0: A ≤Alim and H1: A > Alim, where Alim is some (non-zero) cut-
off value below which one is not interested in the identiﬁed source. We shall not,
however, pursue this further here.
Assuming the above priors, the evidence for each model can be written
Ei(S) =

L(X, A, R) π(X)πi(A)π(R) dX dA dR ≡1
|S|

S
¯Pi(X) dX,
(7.17)
where we have written explicitly the dependence of the evidence on the chosen spa-
tial region S and we have deﬁned the (un-normalized) two-dimensional marginal
posterior
¯Pi(X) =

πi(A)π(R)L(X, A, R) dA dR.
(7.18)
3 Non-uniform priors on the source position are often useful and can be easily accommodated with little modiﬁ-
cation of our discussion. For example, one might impose a Gaussian prior on X centred at the putative source
position with a width indicative of one’s uncertainty in its location.

7.5 Single-source models
181
In particular, using π0(A) = δ(A), the evidence for model H0 is then simply
E0 = 1
|S|

S
L0 dX = L0,
(7.19)
since L0 ≡L(X, A = 0, R) is, in fact, independent of the source parameters and
the priors are normalized. Note that E0 is independent of S.
So far we have not addressed the prior ratio Pr(H1)/ Pr(H0) in Eq. (7.7). Al-
though this factor is often set to unity in model selection problems, one must be
more careful in the current setting. For the sake of illustration and simplicity, let us
assume that the sources we seek are randomly distributed in spatial position, i.e.,
they are not clustered. In that case, if μS is the (in general, non-integer) expected
number of sources (centred) in a region of size |S|, then the probability of there
being N sources in such a region is Poisson distributed:
Pr(N|μS) = e−μSμN
S
N!
.
(7.20)
Thus, bearing in mind the above deﬁnitions of H0 and H1, we have
Pr(H1)
Pr(H0) = μS.
(7.21)
Hence, the key equation (7.7) for model selection becomes
ρ ≡Pr(H1|D)
Pr(H0|D) = μS
E1(S)
L0
= μ
	
S ¯P1(X) dX
L0
,
(7.22)
where μ = μS/|S| is the expected number of sources (centres) per unit area. One
accepts the posterior peak as corresponding to a real source if ρ > 1 and rejects
it otherwise; Carvalho et al. (2008) call this the ‘symmetric loss’ criterion, since it
implies that a rejected real source is as undesirable as an accepted spurious one.
The only conceptual issue remaining is the choice of the region S, and various
alternatives are possible. If one takes S to be the entire map, then the corresponding
‘global’ evidence E1(S) has contributions from the entire multi-modal posterior
distribution. With this choice of S, the model H1 essentially becomes ‘there is one
source somewhere in the map’. In source detection, however, one is usually more
interested in whether an individual posterior peak is associated with a real source.
One therefore wishes to evaluate separately the ‘local’ evidence associated with
each posterior peak (of interest). Thus, for each posterior peak, S is taken to be
a region (just) enclosing it entirely (to a good approximation) in the X subspace.
Another choice for S, explored further in Section 7.5.5, is to set it equal to the
region contained in a single pixel of interest. One can even take S to be a single
point, so that π(X) = δ(X −X0), thereby ﬁxing a priori the (centre) position of
the source.

182
Bayesian source extraction
It is worth mentioning that, for the last two choices of S, one must exercise some
caution. If one knows absolutely that, if a source is present, then it must be centred
in some pixel or at some precise location, then one can use the ratio ρ in Eq. (7.22)
as described above. This is not usually the case, however. More often, one might,
for example, ﬁx the source position X0 to lie ‘close’ to where one supposes a true
source might be located, in order to save on computation by reducing the dimen-
sionality of the parameter space. In this case, to decide if a source is really present,
one should still calculate the ‘local’ evidence E1 associated with the entire cor-
responding posterior peak. Nonetheless, an available approximation to the model
selection ratio (7.22) in this case is given by
ρ = μ
	
peak ¯P1(X) dX
L0
≈μA
¯P1(X0)
L0
≈fs
¯P1(X0)
L0
,
(7.23)
where A is the ‘typical’ projected area of a source of interest and hence fs is the ex-
pected fraction of the image covered by such sources, assuming no sources overlap
(see Cruz et al. 2007).
Although the approach outlined above for Bayesian source detection and vali-
dation is conceptually straightforward, the practical task of locating the multiple
peaks of the posterior distribution, estimating uncertainties on derived parameter
values and evaluating the required evidences for model selection can be extremely
computationally demanding. In the remainder of this section, we therefore con-
sider some approaches to these problems, roughly in order of increasing theoreti-
cal desirability and, typically, computational complexity. Our ﬁnal Section 7.5.5
presents an alternative, pixel-by-pixel, method that takes a somewhat different
approach.
7.5.1 Analytic source extraction and the matched ﬁlter
It is worth noting that, in a particular special case, the approach outlined above
leads to the matched ﬁlter. Consider the case in which the non-source contribution
to the image can be considered as a generalized noise with covariance matrix G =
⟨ggT⟩(again assuming the parameters q and r characterizing the generalized noise
are known). Also let us write the template Eq. (7.3) as
τ(x; p) = At(x −X; R),
(7.24)
where A, X and R are, respectively, the amplitude, vector position and spatial
extent of the source, and t(x; R) is the spatial proﬁle of a unit amplitude reference
source of size R, centred on the origin. Then the signal vector in the likelihood
Eq. (7.9) is simply s(p) = At(X, R), where the vector t has components ti =
t(xi −X; R), in which xi is the position of the centre of the ith pixel in the image.

7.5 Single-source models
183
Substituting this expression into the log-likelihood, assuming uniform priors on
the source parameters in some (wide) range, differentiating the log-posterior with
respect to A and setting the result to zero yields an analytic estimate for the source
amplitude,
A(X, R) =
tT(X, R) G−1d
tT(X, R) G−1t(X, R).
(7.25)
Note that, under the assumption of statistical homogeneity, the denominator in the
above expression does not in fact depend on the source position X, and thus need
only be evaluated once (for a source at the origin, say) for any given value of R.
More importantly, the estimator Eq. (7.25) is precisely the ﬁltered ﬁeld produced
by the standard matched ﬁlter, in which one assumes a value for R. Moreover, the
corresponding log-posterior can be written:
ln P(X, A, R) = constant + 1
2α(R)[ A(X, R)]2.
(7.26)
Thus, for a given value of R, peaks in the ﬁltered ﬁeld correspond to peaks on
the log-posterior considered as a function of X. If the sources sought all have the
same size R and this is known in advance, then the local maxima of the now 3-
dimensional posterior in the space (X, Y, A) are all identiﬁed by this method. This
scenario corresponds, for example, to point sources observed by an experiment
with a known, invariant beam proﬁle.
In this approach, uncertainties in the (source) parameter values derived from
each posterior peak and the associated ‘local’ evidences required for source vali-
dation can be obtained simply by evaluating the curvature (or Hessian) matrix at
each maximum and using it to construct a Gaussian approximation to each poste-
rior peak (see, e.g., Hobson & McLachlan 2003; Carvalho et al. 2008), although
this is rarely performed in matched ﬁlter analyses.
7.5.2 Iterative source extraction: local maximization
In the matched ﬁlter method, when the source size R is not known in advance, or
the sources have different sizes, one typically adopts the rather ad hoc approach of
ﬁltering the ﬁeld at several predetermined values of R and combining the results
in some way to obtain estimates of the parameters p = {X, Y, A, R} for each
identiﬁed source.
Carvalho et al. (2008) adopt a different strategy based on locating the ‘substan-
tial’ local maxima of the posterior using multiple downhill minimizations. They
show that the resulting POWELLSNAKES algorithm outperforms linear ﬁlters, while
retaining their speed. The algorithm is technically quite complicated, but the basic
approach can be summarized as follows:

184
Bayesian source extraction
• First, a pre-ﬁltering step is applied in which the original image is replaced by the
ﬁeld Eq. (7.25), where R is taken equal to the mid-point of its prior range. The
advantage of this step is that, even in the presence of sources of different sizes,
the ﬁltered ﬁeld often has pronounced local maxima near many real sources.
• In the next step, one launches a large number (typically a few hundred) of
downhill minimizations in the two-dimensional (X, Y ) subspace, using the
Powell algorithm (Press et al. 1994). The (X, Y ) starting points for the
‘snakes’ are dithered randomly about a uniform rectangular grid, with the to-
tal number of snakes equal to the ratio of the total number of pixels to the
smallest anticipated spatial extent of a source (in pixel units). The Powell min-
imization algorithm is also modiﬁed to include a ‘jump procedure’, inspired by
quantum-mechanical barrier tunnelling, designed to avoid ‘small’ local max-
ima in the posterior, such as those resulting from the background and noise.
The purpose of the 2-dimensional minimization step is simply to obtain a
good estimate of the (X, Y ) positions of the sources, although one may then use
Eq. (7.25) also to obtain an approximate estimate of the putative source
amplitude.
• In the ﬁnal step, one performs multiple Powell minimizations (with no jump
procedure) in the full 4-dimensional space (X, Y, A, R), starting each mini-
mization from the X, Y, A values found in the previous step (and setting the
initial value of R to the mid-point of its prior range). The posterior peak lo-
cated at the end of each minimization is then either accepted as a real source
using an evidence-based criterion (see below) or rejected. If accepted, the best-
ﬁt source template is subtracted from the map before the next minimization is
launched.
As in the previous subsection, uncertainties in the (source) parameter values
derived from each posterior peak and the associated ‘local’ evidences required
for source validation are obtained by constructing a Gaussian approximation to
each located posterior maximum by evaluating its Hessian matrix. Nonetheless,
Carvalho et al. (2008) point out that the estimation of evidences via the Gaussian
approximation does not take into account the fact that the prior might abruptly
truncate a posterior peak before it falls (largely) to zero, and hence may lead to
an overestimate of the evidence. They go on to suggest ways in which this effect
might be mitigated. In particular, they compute an upper bound on the quality of
a detection, and show that a quantity termed the ‘normalized integrated signal-to-
noise ratio’ (NISNR) plays an important role in establishing a limit on the source
amplitudes that can be reliably detected. They also consider the pixel-based prior
mentioned in Section 7.5, where the region S is chosen as the pixel containing the
relevant posterior peak.

7.5 Single-source models
185
Fig. 7.4. The 500 samples from the posterior obtained on the third iteration of the
MCCLEAN algorithm when applied to the toy problem in Figure 7.1. The samples are
projected into the 2-dimensional subspaces (X, Y ) (left panel) and (A, R) (right panel).
7.5.3 Iterative source extraction: global maximization
Rather than performing iterative source extraction based on local maximization,
HM03 originally advocated locating the global maximum of the posterior at each
iteration, subtracting the corresponding source if accepted as real based on the
evidence criterion, and repeating the process until the ﬁrst rejected detection. This
approach has a stronger theoretical motivation than those presented above, since
it concentrates at each stage on the most prominent source remaining in the map,
but it is far more computationally expensive. HM03 suggest two different ways of
performing this iterative approach.
• In the ﬁrst method, called MCCLEAN, the posterior peak centred on the global
maximum is located and explored using MCMC sampling. HM03 found that
this could be achieved relatively straightforwardly by using just a few interact-
ing MCMC chains and employing a relatively fast annealing schedule during
the burn-in phase. The latter allows the chains initially to sample from remote
regions of the posterior and locate the global posterior peak, before sampling
from it during the post burn-in phase. Note that if two (or more) of the dominant
posterior peaks are well explored by the MCMC chains, then one simply char-
acterizes and subtracts more than one source at that iteration. HM03 applied this
method to the toy problem illustrated in Figure 7.1, using the BAYESYS sam-
pler with ﬁve chains and, at each iteration, taking 500 post burn-in samples. For
illustration, the samples taken during the third iteration are shown in Figure 7.4.
These samples can then be used to determine best-ﬁt values and uncertainties for
the source parameters. For each putative source, the evidence value required to

186
Bayesian source extraction
assess whether to accept it as real is calculated by performing a thermodynamic
integration during the burn-in period. In the toy model, the algorithm stopped
after identifying six sources (one being a combination of two overlapping real
sources), but required ∼40 minutes of CPU time on a desktop computer.
• In the second method, called MAXCLEAN, the global maximum of the poste-
rior at each iteration is located using a simulated annealing downhill simplex
optimizer (Press et al. 1994), with an appropriate choice of annealing sched-
ule. The Hessian matrix at the maximum is then calculated to obtain a Gaussian
approximation to the shape of the posterior peak, which is used to obtain uncer-
tainties on the derived parameter values and to estimate evidences for the source
validation step. When applied to the toy problem illustrated in Figure 7.1, the
results were similar to those obtained using the MCCLEAN method, identifying
the same six sources before terminating. In this approach, however, the entire
analysis required only ∼8 minutes of CPU time.
7.5.4 Simultaneous source extraction
The iterative approach outlined in the previous subsection was in fact originally
born out of necessity, since the computation required to sample reliably from the
full multi-modal posterior (see Figure 7.3) is prohibitive using standard MCMC
methods based on the Metropolis–Hastings algorithm. Indeed, straightforward im-
plementations of this approach ﬁnd it very difﬁcult, in general, to transition be-
tween widely separated, narrow modes of a posterior, and can easily give spurious
results. A state-of-the-art MCMC sampler, such as BAYESYS, which employs mul-
tiple interacting chains and compound proposals, can cope with such distributions,
but requires over 10 hours of CPU time to explore the full posterior adequately
when applied to the toy problem illustrated in Figure 7.1.
Fortunately, this problem has recently been overcome by the multi-modal nested
sampling algorithm of Feroz and Hobson (2008), which can efﬁciently produce
posterior samples and calculate global and local evidences for posteriors that are
highly multi-modal. It is therefore ideally suited to performing Bayesian source
detection and validation. The technique combines the basic nested sampling ap-
proach of Skilling (2004a) with an automatic clustering algorithm, and builds on
the work of Mukherjee, Parkinson and Liddle (2006) and Shaw, Bridges and
Hobson (2007). When applied to the toy problem, this method produces the sam-
ples shown in Figure 7.5, when projected into the (X, Y ) subspace. Not only has
the sampler clearly explored the full posterior distribution, but it has also auto-
matically identiﬁed which samples belong to each posterior peak. In total, 11 pos-
terior peaks are located, seven of which were correctly identiﬁed as real sources
by evaluation of their local evidence (although one peak again corresponds to the

7.5 Single-source models
187
0
20
40
60
80
100
120
140
160
180
200
0
20
40
60
80
100
120
140
160
180
200
y
x
Fig. 7.5. The samples obtained from the toy model posterior using the multi-modal nested
sampling algorithm, projected into the (X, Y ) subspace. The different tints correspond to
different posterior peaks.
combination of two overlapping real sources). The remaining four peaks were cor-
rectly rejected as sources using the local evidence criterion. The entire analysis
took ∼5 minutes of CPU time.
7.5.5 Pixel-by-pixel source extraction
We conclude our summary of Bayesian source extraction within the single-source
model with a method based on that suggested by Savage and Oliver (2007), which
takes a rather different approach to those outlined above. In this approach, one
retains the classical source extraction methodology of dividing the process into two
distinct stages: source detection and source photometry. This is computationally
much quicker than the single, combined approach used thus far, although at the
cost of losing some theoretical rigour. We take the opportunity here to make some
straightforward generalizations to the basic method originally presented.
In the source detection step, the key idea is to consider in turn each pixel in the
map separately. Suppose one is currently concerned with the kth pixel at position
xk. One ﬁrst reduces the computational burden by taking the corresponding data
vector d to be a small portion of the map centred on this pixel, rather than the whole
map. Clearly the size of this patch should be chosen by considering the range of

188
Bayesian source extraction
spatial sizes of sources of interest. Following Eq. (7.4) the (reduced) data vector is
then considered to be of the form
d = s(p) + b(q) + n(r),
(7.27)
where, as we have assumed throughout this section, the signal vector is assumed
to result from just a single source (i.e., Ns = 1). In the current approach, how-
ever, one additionally assumes that X = xk, i.e., that the source is centred in the
current pixel. Moreover, one assumes a value for the source size R, typically the
mid-point of its allowed range (note that no assumption is necessary in the case
where all the sources have the same, known size, for example when one is consid-
ering populations of point-like sources, such as distant galaxies or stars). Thus the
only remaining source parameter to be determined is the amplitude A. To simplify
matters further, the background is assumed to be uniform across the patch, so the
only unknown parameter q is the overall background level B. It is also assumed
that the noise parameters r are known and the noise is statistically homogeneous
with a covariance matrix N. Hence the model contains only the two free parameters
A and B.
One then considers two models for the data:
H0 = ‘there is no source centred in the kth pixel’,
H1 = ‘there is one source centred in the kth pixel’,
where, as above, the former model is simply obtained from the latter by setting
A = 0. Thus the log-posteriors for H0 and H1 read
ln P0(B) = c −1
2(d −B1)TN−1(d −B1) + ln π(B),
(7.28)
ln P1(A, B) = c −1
2(d −At −B1)TN−1(d −At −B1)
+ ln π(A) + ln π(B),
(7.29)
where c is an unimportant constant, 1 is the vector with all elements equal to unity,
and the vector t has components ti = t(xi −xk) and corresponds to a unit ampli-
tude reference source centred in the kth pixel.
To simplify subsequent results, it is useful to deﬁne the quantities
α ≡tTN−1t,
β ≡1TN−11,
γ ≡tTN−1d,
δ ≡1TN−1d,
ϵ ≡tTN−11,
(7.30)
where it is worth noting that the quantities α, γ and ϵ must be calculated separately
for each pixel k, whereas β and δ need only be calculated once. It is then easy to
show that the optimal value for B in model H0 must satisfy
β ˆB0 −∂ln π(B)
∂B
0000 ˆB0
= δ.
(7.31)

7.5 Single-source models
189
Approximating the posterior peak as a Gaussian, one ﬁnds that the rms uncertainty
in ˆB0 is
σ ˆB0 =
)
β −∂2 ln π(B)
∂B2
0000 ˆB0
*−1/2
(7.32)
and the corresponding estimate of the log-evidence is
ln E0 = 1
2 ln(2π) + ln σ ˆB0 + c −1
2dTN−1d −1
2 ˆB2
0β + ˆB0δ + ln π( ˆB0). (7.33)
Similarly, for the model H1, the optimal values of A and B satisfy
α ˆA1 −∂ln π(A)
∂A
0000 ˆ
A1
+ ϵ ˆB1 = γ,
(7.34)
ϵ ˆA1 + β ˆB1 −∂ln π(B)
∂B
0000 ˆB1
= δ.
(7.35)
The Hessian matrix at the posterior peak reads
H =
⎛
⎜
⎜
⎝
−α + ∂2 ln π(A)
∂A2
0000 ˆ
A1
−ϵ
−ϵ
−β + ∂2 ln π(B)
∂B2
0000 ˆB1
⎞
⎟
⎟
⎠
(7.36)
and hence deﬁnes a Gaussian approximation leading to the log-evidence estimate
ln E1 = ln(2π) −1
2 ln | −H| + c −1
2dTN−1d −1
2 ˆA2
1α + ˆA1γ −ˆA1 ˆB1ϵ
+ ˆB1δ −1
2 ˆB2
1β + ln π( ˆA1) + ln π( ˆB1).
(7.37)
Detection of the sources then proceeds by comparing the models H0 and H1 at
each pixel in the map. At each pixel, the logarithm of the model selection ratio
Eq. (7.22) is
ln ρ = ln E1 −ln E0 + ln⟨n⟩
= 1
2 ln(2π) −1
2 ln | −H| −ln σ ˆB0 −1
2 ˆA2
1α + ˆA1γ
−ˆA1 ˆB1ϵ + ( ˆB1 −ˆB0)δ −1
2( ˆB2
1 −ˆB2
0)β
+ ln π( ˆA1) + ln π( ˆB1) −ln π( ˆB0) + ln⟨n⟩,
(7.38)
where we have used the fact that Pr(H1)/ Pr(H0) = ⟨n⟩, i.e., the (in general,
non-integer) expectation value of the number of sources per pixel (which is likely
to be ≪1); in the second equality we note that the quantities c and dTN−1d have
cancelled out of the ﬁnal expression. By cycling through the pixels in turn, one
creates a ‘map’ of ln ρ. Each peak in this map with ln ρ > 0 is identiﬁed as the
position of a real source, with the corresponding ˆA1 giving its ﬂux. An illustration
of the process is shown in Figure 7.6, assuming uniform priors on A and B.

190
Bayesian source extraction
Fig. 7.6. Input map (left) and ln ρ map (right) produced by the pixel-by-pixel source
detection method.
If all the sources have the same known size (such as point sources in a map with
a ﬁxed beam), the source detection step above can, in fact, be the complete analy-
sis. In more general problems, however, the detection step is followed by a source
photometry step. For each source identiﬁed in the detection step, an MCMC anal-
ysis is performed in the full source parameter space {X, Y, A, R}, but with narrow
priors imposed on X and Y , centred on the identiﬁed source position. Thermody-
namic integration can then be used to calculate a more accurate evidence value to
decide whether to accept the source as real.
The motivation for the two-stage approach adopted in this method is speed of
analysis, which can be very important in the early stages of modern, large astro-
nomical surveys (when the production of early results and metrics of survey depth,
and so on, are desirable as quickly as possible). To this end, it is noted that be-
cause of the simplicity of models we use here (i.e., single sources, centred on pixel
positions), one can implement the above calculations using Fast Fourier Trans-
forms, making the method virtually as fast (and having the same algorithmic de-
pendence on data size) as even the simplest implementations of the matched ﬁlter.
In addition, the requirement for a fast, analytic solution still leaves some freedom
in the choice of prior on the source amplitude A and background B. In particular,
one can choose a prior on A of the form
π(A) ∝An,
(7.39)
which includes the following special cases: n = 0 is a uniform prior; n = −1 is
a (non-informative) Jeffreys prior; and n< −1 is a good description of observed
(power-law) source number counts, which is useful when, for example, one is ex-
tracting distant galaxies from survey data.

7.6 Conclusions
191
7.6 Conclusions
We have presented the Bayesian approach to the detection and characterization
of discrete sources in a diffuse background, which allows for the inclusion of all
pertinent prior information available. The method assumes a parameterized form
for the sources, and the optimal values of these parameters, and their associated
errors, are usually obtained in a single step by evaluating their full posterior dis-
tribution. In principle, any statistical form for the (generalized) noise can be ac-
commodated by deﬁning an appropriate likelihood function. If available, one can
also place physical priors on the parameters deﬁning a source and on the number of
sources present. Moreover, one can decide whether each peak found in the posterior
distribution corresponds to a real source or a background ﬂuctuation by perform-
ing a Bayesian model comparison using (an approximation to) the evidence. The
approach therefore represents the theoretically optimal method for performing
parameterized source extraction.
Acknowledgments
We thank Farhan Feroz for useful discussions and for providing Figure 7.5.
References
Barreiro, R. B., Sanz, J. L., Herranz, D. and Martinez-Gonzalez, E. (2003). Mon. Not. Roy.
Astron. Soc., 342, 119.
Bertin, E. and Arnouts, S. (1996). Astron. Astrophys. Supp., 117, 393.
Carvalho, P., Rocha, G. and Hobson, M. P. (2008). Mon. Not. Roy. Astron. Soc., submitted.
Chiang, L.-Y., Jorgensen, H. E., Naselsky, I. P., Naselsky, P. D., Novikov, I. D. and
Christensen, P. R. (2002). Mon. Not. Roy. Astron. Soc., 335, 1054.
Cruz, M., Turok, N., Vielva, P., Martinez-Gonzalez, E. and Hobson, M. P. (2007). Science,
318, 1612.
Diego, J. M., Vielva, P., Martinez-Gonzalez, E., Silk, J. and Sanz, J. L. (2002). Mon. Not.
Roy. Astron. Soc., 336, 1351.
Feroz, F. and Hobson, M. P. (2008). Mon. Not. Roy. Astron. Soc., 384, 449.
Green, P. J. (1994). J. R. Stat. Soc., 56, 589.
Guglielmetti, F., Fischer, R., Voges, W., Boese, G. and Dose, V. (2004). In V. Schoenfelder,
G. Lichti and C. Winkler, eds., 5th INTEGRAL Workshop. ESA SP-552.
Haehnelt, M. G. and Tegmark, M. (1996). Mon. Not. Roy. Astron. Soc., 279, 545.
Herranz, D., Sanz, J. L., Barreiro, R. B. and Martinez-Gonzalez, E. (2002a). Astrophys. J.,
580, 610.
Herranz, D., Sanz, J. L., Hobson, M. P., Barreiro, R. B., Diego, J. M., Martinez-Gonzalez,
E. and Lasenby, A. N. (2002b). Mon. Not. Roy. Astron. Soc., 336, 1057.
Hobson, M. P., Bridle, S. L. and Lahav, O. (2002). Mon. Not. Roy. Astron. Soc., 335, 377.
Hobson, M. P. and McLachlan, C. (2003). Mon. Not. Roy. Astron. Soc., 338, 765.
L´opez-Caniego, M., Herranz, D., Barreiro, R. B. and Sanz, J. L. (2005). Mon. Not. Roy.
Astron. Soc., 359, 993.
Makovoz, D. and Marleau, F. R. (2005). Proc. Astron. Soc. Pac., 117, 1113.

192
Bayesian source extraction
Marshall, P. J. (2006). Mon. Not. Roy. Astron. Soc., 372, 1289.
McEwen, J. D., Hobson, M. P. and Lasenby, A. N. (2008). IEEE Trans. Signal Process., 56,
3813, available as astro-ph/0612688.
Mukherjee, P., Parkinson, D. and Liddle, A. R. (2006). Astrophys. J. Lett., 638, L51.
´O’Ruanaidh, J. J. K. and Fitzgerald, W. J. (1996). Numerical Bayesian Methods Applied to
Signal Processing. New York: Springer-Verlag.
Phillips, D. B. and Smith, A. F. M. (1995). In W. R. Gilks, S. Richardson and D. J.
Spiegelhalter, eds., Markov Chain Monte Carlo in Practice. London: Chapman &
Hall.
Press, W. H., Teukolsky, S. A., Vetterling, W. T. and Flannery, B. P. (1994). Numerical
Recipes in Fortran. Cambridge: Cambridge University Press.
Sanz, J. L., Herranz, D. and Martinez-Gonzalez, E. (2001). Astrophys. J., 552, 484.
Savage, R. and Oliver, S. (2007). Astrophys. J., 661, 1339.
Shaw, R., Bridges, M. and Hobson, M. P. (2007). Mon. Not. Roy. Astron. Soc., 378, 1365.
Sivia, D. S. and Skilling, J. (2006). Data Analysis: A Bayesian Tutorial. Cambridge:
Cambridge University Press.
Skilling, J. (2004a). In E. Erickson, J. T. Rychert and C. R. Smith, eds., AIP Conf. Proc.,
735, 395.
Skilling, J. (2004b). BayeSys and MassInf, online at http://www.inference.phy.cam.
ac.uk/bayesys/
Stetson, P. B. (1992). User’s Manual for Daophot II, online at http://www.star.bris.
ac.uk/∼mbt/daophot.

8
Flux measurement
Daniel Mortlock
8.1 Introduction
The measurement of the ﬂux of an astronomical source is a classic parameter esti-
mation problem in which the quantity of interest must be inferred from noisy data.
The Bayesian methods described in this book provide a clear, unambiguous, self-
consistent and optimal method for answering this type of question, but the vast
majority of ﬂux measurements are made by applying a heuristic classical estima-
tor to the data. This raises some immediate questions: Why is the estimator-based
approach adopted in most cases? How do the resultant ﬂux measurements differ?
What is the relationship between the two techniques?
To answer these questions ﬁrst requires an understanding of the astronomical
measurement process itself (Section 8.2), which leads very naturally to the def-
inition of the standard ﬂux estimator (Section 8.3). Using a model for the source
population (Section 8.4) as a prior, it is also possible to apply Bayesian inference to
the problem (Section 8.5), although care is required to avoid some potential incon-
sistencies (Section 8.6). Even with the full Bayesian result in hand, however, the
existence of databases containing billions of classically estimated ﬂuxes and errors
leads to a number of practical considerations which argue against simply reporting
posterior distributions for astronomical ﬂuxes (Section 8.7).
8.2 Photometric measurements
How is the ﬂux of an astronomical source measured? ‘With great difﬁculty’ is one
possible answer, especially given a history of photographic plates, dipole antennae,
microdensitometers and other arcane equipment. A more pertinent answer would
be ‘with technology’, as almost all astronomical measurements now rely on some
combination of large telescopes and highly efﬁcient electronic detectors. Many
193

194
Flux measurement
modern detectors (e.g., optical or X-ray charge-coupled devices) are close to 100%
efﬁcient, and even the registration of individual photons is a mature technique (see,
e.g., Law et al. 2006; Murphy et al. 2008). As photon emission is usually an in-
dependent stochastic process, an immediate implication is that the probability1 of
registering Nγ source photons is Poisson distributed. Hence
P(Nγ| ¯Nγ) =
¯NNγ
γ
e−¯
Nγ
Γ(Nγ + 1),
(8.1)
where the average number of photons expected, ¯Nγ, depends not only on the
source’s ﬂux, but also on the details of the observation. For a source of ﬂux F
observed for integration time Tobs using a telescope of collecting area A,
¯Nγ = F TobsA
¯Eγ
,
(8.2)
where ¯Eγ is the average energy2 of the photons. Thus, for example, an average of
¯Nγ ≃103 photons would be registered from a typical redshift ∼0.1 galaxy in a
routine 1-minute observation on the 2.5 m diameter telescope used for the Sloan
Digital Sky Survey (SDSS; York et al. 2000), whereas even a 100-hour integration
on a faint active galactic nucleus (AGN) with the 0.04 m2 Chandra X-Ray Obser-
vatory might have ¯Nγ ≃5 (cf. the example in Chapter 12).
There are, of course, a great many real-world complexities (e.g., atmospheric ab-
sorption, non-ideal detectors and terrestrial interference) which combine to make
accurate photometry such a difﬁcult task, but most of these effects simply modify
the conversion between ﬂux and counts. Thus it is useful to combine these factors
with the basic observational parameters described above to give a single calibration
constant, C, which relates the expected counts to the ﬂux by C = F/ ¯Nγ, and re-
duces to C = ¯Eγ/TobsA in the ideal case. The count distribution given in Eq. (8.1)
can then be rewritten as
P(Nγ|F) = (F/C)Nγ e−F/C
Γ(Nγ + 1)
,
(8.3)
where just one photon would be expected from a source of ﬂux C in this
observation.
1 Here, ‘probability’ is used strictly in the logical sense adopted by, e.g., Jaynes (2005). Hence all expressions
of the form P(A|B) should be interpreted as the degree to which (the truth of) proposition B implies the
truth of proposition A. As such P(A|B) is not a mathematical function in the usual sense, although if A and
B are mathematical in nature then the more formal P(x = x0|y = y0) is generally replaced by the less
cumbersome, if potentially ambiguous, shorthand P(x|y).
2 The spectral response, or passband, of an instrument is characterized in terms of energy in X-ray and gamma-
ray astronomy, by wavelength in optical and infrared astronomy, and by frequency in radio and microwave
astronomy. Further, it is almost always the ﬂux density averaged over a ﬁnite energy range that is obtained in
actual astrophysical measurements. Some treatments of ﬂux estimation (e.g., Murdoch et al. 1973) work in
terms of ﬂux density; however, this complication is irrelevant to the statistical aspects of the problem, and so
only total ﬂuxes are considered here.

8.2 Photometric measurements
195
Implicit in the above results is that the ﬁeld of view of the measurement, of
angular area Ωobs, is greater than that of the observational point-spread function.3
If this were not the case then an ‘aperture correction’ would be required, but a more
important implication of a ﬁnite aperture size is that source photons might not be
the only contribution to the data.
Unfortunately, the majority of astronomical measurements includes contami-
nation from actual astrophysical backgrounds (e.g., the cosmic microwave back-
ground in the submillimetre), terrestrial foregrounds (e.g., sky-glow in the near-
infrared) or instrumental imperfections (e.g., the read-out noise from solid state
detectors or thermal noise in microwave bolometers). These physical processes
might differ greatly, but the effect is the same: a reasonably uniform but noisy con-
tribution to the data which must be accounted for when measuring a source’s ﬂux.
Expressed in ﬂux units, the raw data take the form
d = CNγ + b,
(8.4)
where b is the total background in the aperture (as opposed to the background
surface brightness).
The presence of a smooth, perfectly known, background would not affect ﬂux
measurements, but any uncertainty in the value of b will result in increased errors.
Instrumental noise, intrinsic variations across the background (e.g., due to clouds
in the atmosphere or Poisson ﬂuctuations in astrophysical backgrounds) and con-
tamination by other sources in the ﬁeld can all lead to an error in the estimation
of b. It might seem these processes should be treated separately, even if only at
the level of formalism, but the differences between the nature of these processes
are largely immaterial, and it is only the state of knowledge that matters, some-
thing which is particularly clear within the Bayesian framework. If the background
signal is estimated (by whatever means) to be ˆb with uncertainty σb, its sampling
distribution is taken to be
P(ˆb|data) =
1
(2π)1/2σb
exp
⎡
⎣−1
2
)ˆb −b
σb
*2⎤
⎦,
(8.5)
where ‘data’ refers to all the information used to determine the background level.
The assumption of Gaussianity may be justiﬁed either because of the central limit
theorem or due to maximum entropy principles (e.g., Jaynes 2003).
With the stochastic nature of the observation process fully described by the sam-
pling distributions given in Eqs. (8.3) and (8.5), the task of ﬂux measurement is,
then, to use these relationships to constrain a source’s ﬂux from whatever data
3 In optical and infrared observations this is characterized by the seeing, usually deﬁned as the full-width at
half-maximum of the point-spread function of the instrument. In the microwave and radio regime the term
‘beam’ is usually used for the same quantity.

196
Flux measurement
have been obtained. Two very different approaches are considered here: the con-
struction of a classical unbiased estimator (Section 8.3); and the calculation of the
full Bayesian posterior (Section 8.5). Despite the latter method being the main fo-
cus of this book, it is the former which is treated ﬁrst, partly because it has been
used to generate almost all reported astronomical ﬂux measurements, but mainly
because it is simpler.
8.3 Classical ﬂux estimation
Having made a photometric observation of a source (as described in Section 8.2),
the most common method of ﬂux measurement is to construct an unbiased estima-
tor simply by subtracting the measured background, ˆb, from the data, d, to give
ˆF = d −ˆb = CNγ −(ˆb −b).
(8.6)
Aside from being a reasonable estimator of the ﬂux, ˆF can also function as a more
intuitive representation of the data, being a sufﬁcient statistic provided that ˆb is
known. However, it is also immediately clear from Eq. (8.6) that ˆF can be negative
if d < ˆb, one of several shortcomings of the estimator-based approach to ﬂux
estimation that are discussed at the end of this section.
The sampling distribution of ˆF can be found by combining Eqs. (8.1) and (8.5)
to give
P( ˆF|F) =
∞

Nγ=0
 ∞
−∞
P(Nγ|F) P(ˆb|data) δ
:
ˆF −

CNγ −(ˆb −b)
;
dˆb,
=
∞

Nγ=0
(F/C)Nγe−F/C
Γ(Nγ + 1)
1
(2π)1/2σb
exp
⎡
⎣−1
2
) ˆF −CNγ
σb
*2⎤
⎦.
(8.7)
The sum effectively marginalizes over the unknown background, although it is
revealing to note that the sampling distribution is independent of its actual (or esti-
mated) value. Whilst Eq. (8.7) cannot be simpliﬁed in general, one of two approxi-
mations that result in analytic forms for P( ˆF|F) can be adopted in almost all cases
of practical interest.
In X-ray astronomy, for example, there is often no signiﬁcant background, which
is equivalent to it being perfectly known [i.e., σb = 0 and P(ˆb|data) = δ(ˆb −b)],
in which case
P( ˆF|F) = (F/C) ˆF/CeF/C
Γ( ˆF/C + 1)
.
(8.8)
Flux estimation in this regime is discussed in more detail in Chapter 12.

8.3 Classical ﬂux estimation
197
It is more common – and almost always the case in optical and infrared astron-
omy – that the background is appreciable but the source photon counts are high
as well, ¯Nγ ≫1. In this case it is legitimate to take the continuum limit in which
the discrete photon distribution given in Eq. (8.3) becomes a Gaussian. The sum in
Eq. (8.7) then simpliﬁes to
P( ˆF|F) ≃
 ∞
−∞
1
(2π)1/2(CF)1/2 exp
$
−1
2
(F ′ −F)2
CF
%
×
1
(2π)1/2σb
exp
2
−1
2
( ˆF −F ′)2
σ2
b
3
dF ′
=
1
(2π)1/2(σ2
b + CF)1/2 exp
2
−1
2
( ˆF −F)2
σ2
b + CF
3
,
(8.9)
where CF = C2 F/C is the Poisson variance, as exactly one photon is expected
from a source of ﬂux C in such an observation. The simple Gaussian form of
the sampling distribution makes clear the separate inﬂuences of the Poisson and
background terms, and is used as the basis for all the results that follow (and, even
when applied in the low-Nγ limit, produces the same mean and variance as the full
expression in Eq. (8.7)).
The most important, if obvious, implication of Eq. (8.9) is that ˆF is an unbiased
estimator of the source ﬂux, with an expectation value over noise realizations of
⟨ˆF⟩= F
(8.10)
and a variance of
⟨( ˆF −F)2⟩= σ2
b + CF.
(8.11)
Whilst care was taken to derive these results rigorously, the same expression for
the variance could have been arrived at using more heuristic arguments, by adding
the variances from the source and background terms in quadrature. Often one
of the two terms in Eq. (8.11) dominates: the low σ2
b case appropriate to some
X-ray observations was already mentioned, while the other extreme, characterized
by ⟨( ˆF −F)2⟩≃σ2
b, is appropriate to all but the brightest sources in most optical
or infrared surveys.
It is not possible, however, to calculate ⟨( ˆF −F)2⟩from the data, as the source’s
actual ﬂux is unknown. The commonly used estimate for the measurement error is,
instead, obtained by simply replacing F with | ˆF| in Eq. (8.11) to obtain
Δ ˆF =

σ2
b + C| ˆF|
1/2
.
(8.12)

198
Flux measurement
Fig. 8.1. Scatter plots of the estimated error, Δ ˆF, as a function of the estimated ﬂux, ˆF,
from sources taken from the SDSS (York et al. 2000) in (a) and the UKIRT Infrared Deep
Sky Survey (UKIDSS; Lawrence et al. 2000) in (b). Both the estimated ﬂuxes and errors
are normalized to the background uncertainty, σb, and the dashed lines show the expected
form Δ ˆF = (σ2
b + C| ˆF|)1/2 of the data model adopted here (Eq. 8.12). The SDSS data
model deviates from this in two ways: there is a scatter from deblended close pairs of
sources; and a minimum magnitude error (i.e. Δ ˆF ∝ˆF for bright sources) is applied to
account for overall calibration uncertainties.
The relative inﬂuences of the constant and ﬂux-dependent contributions to Δ ˆF are
shown in Figure 8.1. Whilst Δ ˆF ≃⟨( ˆF −F)2⟩1/2 for bright sources, the necessity
of using | ˆF| in calculating the error term is a serious shortcoming close to the
detection limit (e.g., Lupton et al. 1999).
This problem is just one of the limitations of an estimator-based approach to
parameter inference. No method which ignores prior information can do more than
summarize the properties of the data, including even the most rigorous frequentist
conﬁdence limits which account for the measurement process but ignore the under-
lying source population. Whilst it is true by construction that, for a source of ﬂux
F, ˆF will be drawn from the sampling distribution given in Eq. (8.7), the apparent
implication that the true ﬂux has an ‘effective posterior’,
P(F| ˆF) =
1
(2π)1/2(σ2
b + C| ˆF|)1/2 exp
2
−1
2
(F −ˆF)2
σ2
b + C| ˆF|
3
,
(8.13)
does not necessarily follow. Even though this distribution is not explicitly implied
by the estimator-based approach to ﬂux measurement, it is the natural distribution
in F to construct from ˆF and Δ ˆF, and even what many non-Bayesians might think
of subconsciously when presented with a measurement and its error. Moreover, an
effective posterior like this facilitates comparison with the Bayesian results derived

8.4 The source population
199
in Section 8.5, although these also require the explicit inclusion of prior informa-
tion. In the case of ﬂux measurement, this comes from the astronomical source
population, which is described in Section 8.4.
8.4 The source population
Astronomical sources, from the faintest sub-stellar objects to the most luminous
galaxies and quasars, are seen at distances from tens to billions of light years,
resulting in observed ﬂuxes that span more than ten orders of magnitude. Fortu-
nately, the incredible variety of evolving astronomical populations is irrelevant to
the problem of ﬂux estimation, and it is only the distribution of source ﬂuxes that
is important for an observer. Under the assumption that the sources are spatially
uncorrelated (e.g., Tsutomu & Takako 2004) this can be characterized by dΣ/dF,
the differential number of sources of ﬂux F per steradian on the sky.
The form of dΣ/dF is largely determined by geometry: the quadratic depen-
dence of the volume of a spherical shell on its radius ensures that distant sources
are more common than those nearby, and hence that there are many more sources
of low ﬂux than high ﬂux, independent of their intrinsic luminosity distributions.
In an unevolving, eternal, transparent and spatially ﬂat Universe, the source num-
ber counts are determined purely by this effect, yielding the classic result that
dΣ/dF ∝F −5/2 (e.g., Peebles 1993). In reality, cosmological evolution (of extra-
Galactic populations) and the ﬁnite size of the Galaxy (for stars) combine to ensure
that most faint source populations actually have decidedly ﬂatter slopes than this
(e.g., Hopkins et al. 1998; Beckwith et al. 2006). Not that this result is surprising:
the darkness of the night sky (the subject of Olbers’ paradox) can only be recon-
ciled with power-law source counts if they rise less steeply than F −3/2 for low
F. Further, the ﬁnite volume of the observable Universe means that the number
counts must drop to zero below some minimum ﬂux, Fmin, corresponding roughly
to the faintest source at the greatest possible distance. The actual value of Fmin
is somewhat uncertain but, critically, is considerably fainter than current observa-
tional limits in all passbands. At optical wavelengths, for example, the Hubble Ultra
Deep Field (HUDF; Beckwith et al. 2006) observations have revealed that galaxies
have a still-rising surface density of Σ ≃106 deg−2 at the survey’s detection limit,
as shown in Figure 8.2.
Despite the considerable complexity of, and uncertainty about, astronomical
source populations, most of the relevant properties of the combined ﬂux distri-
bution can be captured by a simple bounded power law of the form
dΣ
dF = Θ(F −Fmin)(α −1)Σ∗
F∗
 F
F∗
−α
,
(8.14)

200
Flux measurement
Fig. 8.2. Observed optical source counts taken from the Automatic Plate Measuring
(APM) Galaxy Survey (Maddox et al. 1990; triangles), the Jones et al. (1991) sample
(squares), and the HUDF (Beckwith et al. 2006; circles). The dashed lines illustrate the
degree to which a simple power-law model can ﬁt the observed counts over ﬂux ranges of
>∼100. The slightly arcane abscissa is chosen to approximately match standard astronom-
ical magnitudes, m; by way of comparison, the faintest stars visible to the naked eye have
m ≃5.
where Θ(x) is the Heaviside step function, α is the logarithmic slope, and Fmin
is the low-ﬂux cut-off. Provided that α > 1 and F∗≥Fmin, the corresponding
cumulative distribution is
Σ(> F) = Σ∗
$max(F, Fmin)
F∗
%−(α−1)
.
(8.15)
This gives the normalization condition that there are Σ∗sources per steradian
brighter than F∗on average.
Aside from being a reasonable approximation to the observed source counts over
a large range of ﬂuxes (as shown in Figure 8.2), the use of a power-law model also
facilitates comparison with a number of previous statistical studies of ﬂux mea-
surement (e.g., Murdoch et al. 1973; Condon 1974). In most cases, however, the
low-ﬂux cut-off has been treated only qualitatively, if at all. Whilst it is possible
to pursue a Bayesian approach using the improper prior that results from taking

8.5 Bayesian ﬂux inference
201
Fmin →0 (e.g., Hogg & Turner 1998), it is preferable to work with a correctly
normalized posterior, as in Section 8.5. Moreover, the value of Fmin plays a sur-
prisingly important role in the problem of ﬂux estimation, as shown in Section 8.6.
8.5 Bayesian ﬂux inference
Given a full statistical description of the observation process (Section 8.2) and a
model for the source population (Section 8.4), Bayesian inference is the obvious
method of using the available data to estimate a source’s ﬂux, F. The resultant
posterior incorporates all the available information in a self-consistent manner and
thus gives the tightest legitimate parameter constraints. Using the classical ﬂux
estimator, ˆF, deﬁned in Eq. (8.6), as a convenient data statistic, the posterior is
given by
P(F| ˆF) =
π(F) L( ˆF|F)
	 ∞
0 π(F ′) L( ˆF|F ′) dF ′ .
(8.16)
In this expression π(F) = P(F|source) is the prior, L( ˆF|F) = P( ˆF|F, source) is
the likelihood, and the evidence integral in the denominator ensures that P(F| ˆF) =
P(F| ˆF, source) is a correctly normalized probability distribution in F. The explicit
use of ‘source’ emphasizes that all these results are conditional on there actually be-
ing a single source in the aperture, an issue that is considered further in Section 8.6.
The prior is obtained by applying unit normalization to the source number
counts, dΣ/dF, to give
π(F) =
dΣ/dF
	 ∞
0 dΣ/dF ′ dF ′
= Θ(F −Fmin)α −1
Fmin
 F
Fmin
−α
,
(8.17)
where the second expression assumes the power-law form given in Eq. (8.14), pro-
vided that Fmin > 0 and α > 1.
The use of ˆF as a data surrogate means that the likelihood is identical to the
sampling distribution given in Eq. (8.9). This immediately gives
L( ˆF|F) =
1
(2π)1/2(σ2
b + CF)1/2 exp
2
−1
2
( ˆF −F)2
σ2
b + CF
3
,
(8.18)
although it is worth noting that whilst this is a simple normalized Gaussian distri-
bution in ˆF, it is a considerably more complicated function of F.

202
Flux measurement
Multiplying the prior from Eq. (8.17) with the likelihood in Eq. (8.18) then gives
the posterior for the source ﬂux as
P(F| ˆF) ∝Θ(F −Fmin)
dΣ/dF
(σ2
b + CF)1/2 exp
2
−1
2
(F −ˆF)2
σ2
b + CF
3
∝Θ(F −Fmin)
F −α
(σ2
b + CF)1/2 exp
2
−1
2
(F −ˆF)2
σ2
b + CF
3
,
(8.19)
where the second expression again assumes the power-law source counts. Note that
the posterior is only given up to a normalization constant, which must be computed
numerically.
Having calculated the full posterior, the Bayesian inference task is complete; all
the available information about this source’s ﬂux is summarized in this one exp-
ression. Examples of this distribution are shown in Figure 8.3, revealing a variety
of distinct forms which can best be understood by examining a number of special
cases.
Before looking at the inﬂuence of the source population, it is illustrative to ex-
amine a uniform prior (i.e., α = 0, with the added assumption that there is some
high maximum ﬂux to normalize the prior). Under this assumption, the posterior
in Eq. (8.19) simpliﬁes to
P(F| ˆF) ∝Θ(F −Fmin)
1
(σ2
b + CF)1/2 exp
2
−1
2
(F −ˆF)2
σ2
b + CF
3
,
(8.20)
examples of which are shown in Figure 8.3(a) and (b). Although unrealistic for
astronomical sources, this brings the Bayesian result closer to the ‘effective poste-
rior’ deﬁned in Eq. (8.13). The two distributions would be identical if not for the
ﬂux-dependent Poisson term, which causes the peak of the Bayesian posterior to
be skewed towards lower ﬂuxes and results in a longer tail at high ﬂuxes. Thus,
even in a hypothetical universe with sources drawn from a ﬂat ﬂux distribution, the
Bayesian posterior and the classical ﬂux estimator give very different results.
A more realistic special case than a uniform ﬂux distribution is the situation
where there is no signiﬁcant background, astrophysical or otherwise, and hence no
uncertainty in its (non-)subtraction. With σb = 0, the posterior given in Eq. (8.19)
simpliﬁes to (cf. Chapter 12)
P(F| ˆF) ∝Θ(F −Fmin)F −(α+1/2) exp
2
−1
2
(F −ˆF)2
CF
3
.
(8.21)
As can be seen from Figure 8.3(c) and (d), the peak of this posterior, which is not
Gaussian or even symmetric, is skewed to lower ﬂuxes than ˆF, due to the sharply

8.5 Bayesian ﬂux inference
203
Fig. 8.3. Un-normalized posterior probability distributions, P ′(F| ˆF), of the ﬂux, F,
scaled by the classical estimated noise, Δ ˆF (as this is always non-zero, unlike σb or C).
In each panel the solid line shows the Bayesian posterior and the dashed line shows the
‘effective posterior’ derived from the classical ﬂux estimator.

204
Flux measurement
rising prior. In classical ﬂux estimation, the increased number of fainter sources
scattered bright is a well-known bias (Eddington 1913) which has to be accounted
for post hoc, rather than being a natural part of the formalism. A corollary of this is
that any natural estimator (e.g., the mean or the median) that might be constructed
from the skewed Bayesian posterior would, in a classical sense, be ‘biased’. This
is often cited as a ﬂaw in the Bayesian approach to parameter estimation, but as
the bias is only deﬁned in terms of a long-run frequency of identical trials, it is
inappropriate to apply it to a single measurement. Another important aspect of the
posterior given in Eq. (8.21) is that the huge prior probability at F ≃Fmin is
overwhelmed by the even more extreme drop in the likelihood resulting from the
ﬂux dependence of the variance. Hence the posterior does not depend on Fmin,
provided it is well below both C and ˆF. Combined with the fact that the posterior
is normalizable, this is a sufﬁcient condition for taking the limit Fmin →0.4 Given
the lack of knowledge about Fmin, this is a desirable state of affairs, although this
result relies on perfect knowledge of the background.
Turning ﬁnally to the most general problem, with σb > 0 and steep source counts
with α ≃5/2, it is immediately clear from Figure 8.3(e) and (f) that the situation
is more complicated than in either of the special cases considered above. That the
form of the peak at F ≃ˆF is skewed by both the signal-dependent error term and
the prevalence of fainter sources is as expected; but the rising source counts and
the background noise combine to have the far more drastic effect of producing a
dominant second peak in the posterior at F = Fmin. For faint sources with ˆF ≃
σb the posterior increases monotonically to Fmin, which reﬂects the fact that the
posterior is merely a perturbation of the prior, with the data only providing an upper
limit on the ﬂux. However, the existence of a second peak at F = Fmin even for
bright, well-measured sources – including the example given in Figure 8.3(f), even
though the peak is not visible – is counter-intuitive at best. Can it really be the case
that most apparently bright objects are really just undetectable faint sources that
have been subject to extreme positive noise ﬂuctuations? In more concrete terms,
can it really be true that, upon glancing up into the night sky and noticing a faint
star, it is at all likely to be one of the distant galaxies of the type seen in the HUDF?
The answer to this question, and the resolution of this apparent paradox, requires
a more careful treatment of the faintest sources, which is the subject of the next
section.
8.6 The faintest sources
The seemingly straightforward Bayesian analysis described in Section 8.5 seems
to imply that the numerous faint sources in the Universe can combine with back-
4 Note that taking the limit at this stage is a rigorous mathematical operation, unlike taking Fmin →0 initially
and adopting an improper prior in the hope that the resultant posterior will be normalizable. The common
success of the latter approach hides the fact that, as emphasized by Jaynes (1991), taking the limit of a ratio
and evaluating the ratio of limits are not equivalent processes.

8.6 The faintest sources
205
ground noise to dominate the numbers of bright detections. A corollary of this
would be that many apparently luminous objects are actually orders of magnitude
fainter than they appear. At very least this is counter-intuitive and, if correct, would
imply that most astrophysical knowledge is severely compromised. Before dis-
carding several centuries’ worth of hard-won understanding, however, it is worth
re-examining the assumptions and subsequent reasoning which led to the double-
peaked posterior of Eq. (8.19).
An obvious starting point is the source population that provides the prior for the
ﬂux measurement. It is always tempting to assume a simple mathematical form
like the power law introduced in Eq. (8.14), but is it valid to apply it over such a
large range of ﬂuxes? In short, yes, as can be seen from Figure 8.2, which shows
measured source counts rising steadily over ten orders of magnitude in ﬂux. Given
the empirical existence of such faint sources, the low-ﬂux peak in Eq. (8.19) is
clearly not caused by an unrealistic model of the source population.
Another common source of error in statistical analyses is the unwitting con-
tradiction of a hidden assumption, such as that implicit in the ﬁrst sentence of
Section 8.2, that it is a single source under consideration. That would be uncon-
tentious if it were possible to travel to the object in question and investigate it in
isolation; but astronomy is an observational, not an experimental, science, and so
sources can only be observed projected along a line-of-sight onto the plane of the
sky. In the context of photometric measurements, a source can only be consid-
ered isolated if there are no others within the observational aperture introduced in
Section 8.2.
This assumption is very obviously broken in some situations (e.g., low angu-
lar resolution gamma-ray observations, or near-infrared imaging of the dense star
ﬁelds near the Galactic centre) where the average angular separation between de-
tectable sources is comparable to, or less than, the angular resolution of the in-
strument. In such crowded ﬁelds, such as in Figure 8.4(a), the model must include
the possibility of multiple sources in the aperture, and blindly applying a single
source model would result in over estimating the ﬂux. Of course, the classical es-
timator described in Section 8.3 would be similarly biased; no statistical method
can produce correct results if the underlying model is wrong. Reliable photometry
in crowded ﬁelds is possible, but only by using more complicated image analysis
algorithms than are considered here (e.g., Stetson 1987; Irwin 1990).
Whether or not a ﬁeld is crowded can be judged directly from an image, if avail-
able, but this can also be determined directly from the source counts. A conser-
vative ‘rule of thumb’ in image analysis is that a ﬁeld is crowded if there is, on
average, more than one detectable source per ∼30 beam areas (e.g., Wall & Jenkins
2003). In the context of the power-law model given in Eq. (8.14) this reduces to
Σ(> Fdet) >∼1/30, where Fdet is the ﬂux of the faintest individually detectable
source. As expected, this depends on both the source counts and the observational

206
Flux measurement
Fig. 8.4. Examples of a crowded ﬁeld (left) and non-crowded ﬁeld (right) observed in the
near-infrared by UKIDSS (Lawrence et al. 2007).
parameters, and so the possibility of crowding must be assessed on a case-by-case
basis.
As important as crowding is, it is not the explanation for the low-ﬂux peak in the
posterior given in Eq. (8.19). In the context of the rather extreme example invoked
at the end of Section 8.5, bright, naked eye stars are clearly well separated (even for
an observer who, like the author, is short-sighted). This situation is also illustrated
in Figure 8.4(b), which emphasizes that any ﬂux measurement of the central target
source is unlikely to be completely unaffected by any detectable sources nearby. If
crowding of bright sources is not the problem, then, what about the much higher
surface density of fainter undetectable sources? They are so numerous that there
are inevitably many per beam in any astronomical observation, more than satisfy-
ing the above crowding criterion; but, rather than being identiﬁable as discrete, if
possibly overlapping, sources, they are so dense on the sky that the net result is
essentially that of a noisy background. Termed source confusion (as distinct from
source crowding at detectable ﬂuxes), this phenomenon has now been observed in
most passbands, although it will probably always be most famous as the cause of
the discrepancy between the source counts inferred by two early radio surveys that
were attempting to test the Big Bang hypothesis (see, e.g., Wall & Jenkins 2003).
This dispute was settled when Scheuer (1957) developed a method for calculat-
ing the distribution of total faint source ﬂux, Ffaint,tot, in an aperture of solid angle
Ωobs. The distribution is analytic for power-law source counts (Condon 1974), but
simpliﬁes further in the confusion limit relevant to this problem. The expected
number of faint sources in any aperture is huge, but subject to Poisson ﬂuctuations;
the result is that the distribution of Ffaint,tot tends to a Gaussian. More importantly,

8.6 The faintest sources
207
there are so many undetectable sources that they combine to mimic a uniform, if
noisy, background.
The presence of an additional noisy background would inevitably increase ﬂux
uncertainties, but the fact that the background contribution is estimated from the
plentiful data ought to ensure that it does not induce any systematic bias. In-
deed, the standard ﬂux estimator is insensitive to the faint source population for
this reason, but a problem does arise in the Bayesian approach because these
sources are accounted for initially in the estimated background and then again in
the prior which extends to ﬂuxes well below the detection limit. The measured
background, ˆb, implicitly includes all contributions, and cannot distinguish these
faint sources from other putative contaminants. To then admit the possibility that a
detected, isolated source might actually be one of the background sources whose
ﬂux has been absorbed into ˆb is a clear contradiction, and it is this double-counting
which is, ﬁnally, the source of the spurious low-ﬂux peak in the posterior given in
Eq. (8.19).
To obtain a self-consistent Bayesian ﬂux estimate, the prior should be modiﬁed
in such a way that the numerous ultra-faint sources that have already been ac-
counted for during background subtraction do not enter into the calculation of the
posterior. For non-crowded ﬁelds this can be accomplished by truncating the pos-
terior at the confusion ﬂux, Fconfusion, above which only ∼1 source is expected in
the aperture. In the case of power-law source counts this is given from Eq. (8.15) as
Fconfusion ≃F∗(ΩobsΣ∗)1/(α−1). The correct posterior for well-detected sources
is thus obtained by modifying Eq. (8.19) to give
P(F| ˆF) ∝Θ(F −Fconfusion)
dΣ/dF
(σ2
b + CF)1/2 exp
2
−1
2
(F −ˆF)2
σ2
b + CF
3
∝Θ(F −Fconfusion)
F −α
(σ2
b + CF)1/2 exp
2
−1
2
(F −ˆF)2
σ2
b + CF
3
,
(8.22)
where, as before, the second expression assumes power-law source counts. This
form is also what would have been expected from qualitative arguments (Hogg &
Turner 1998): a posterior with a single peak near F ≃ˆF that both the rising source
counts and Poisson noise skew towards lower ﬂux.
For well-detected sources, the absence of the low-ﬂux peak does not depend on
the exact value of Fconfusion adopted, as long as it falls in the trough between the
two peaks of the posterior shown in Figure 8.5. Critically, in non-crowded ﬁelds,
Fconfusion is comfortably below the detection limit, Fdet, and so this condition is
met by deﬁnition. The main effect of truncating the prior at Fconfusion is simply to
remove the anomalous low-ﬂux peak in the posterior, leaving only the likelihood-
dominated peak at F ≃ˆF.

208
Flux measurement
Fig. 8.5. Schematic diagram showing the various ﬂuxes that enter into the problem of
aperture ﬂux measurement: the minimum ﬂux of the population, Fmin; the confusion ﬂux,
Fconfusion; the detection limit, Fdet; and the classical estimated ﬂux, ˆF. The solid line is
the (un-normalized) posterior truncated at Fconfusion; the dashed line is the naive posterior
that extends all the way down to Fmin and is dominated by the spurious low-ﬂux peak.
Note that both axes are logarithmic.
For fainter sources (with F ≃σb), however, there may not be any trough in
the posterior, with the result that the truncated distribution still rises to Fconfusion
without any signiﬁcant peak at F ≃ˆF. This result immediately makes sense once
it is realized that such a ‘source’ may not even be real; if the data are consistent with
a three- or four-‘sigma’ deviation in the noise then this prior, which is conditional
on there actually being a source there in the ﬁrst place, is not necessarily valid. The
posterior could then be understood in terms of the data merely applying an upper
limit to the power-law prior. In this situation, the identiﬁcation and classiﬁcation
of the source are not separable – something that is always true in principle (e.g.,
Hobson & McLachlan 2003), even if performing the two tasks independently is
generally reasonable in practice. The validity of the truncated posterior in this low-
ﬂux case is less clear-cut, but such difﬁculties are a direct consequence of the non-
Bayesian treatment of the background described in Section 8.2. This could be seen
as just punishment for violating Bayesian principles, by subtracting the background
rather than modelling it, even if ﬁtting for all the contributions to real astronomical
datasets is impractical at best (e.g., Irwin 1990).

8.7 Practical ﬂux measurement
209
Whatever the appropriate prior, the full Bayesian answer to any parameter in-
ference problem is the posterior distribution, in this case P(F| ˆF), as given in
Eq. (8.22). Even with the statistical problem solved, however, there are a number
of practical issues that must also be considered when working with, or generating,
catalogues of ﬂux measurements.
8.7 Practical ﬂux measurement
With the correct handling of undetectable faint sources it is possible to obtain a
well-deﬁned Bayesian posterior for the ﬂux of a source by combining prior knowl-
edge about the population with the data from a photometric observation (Sec-
tion 8.5 and Section 8.6). In practice, however, it is often necessary to characterize a
source’s ﬂux and uncertainty with a few numbers (e.g., in a catalogue or database).
For the fairly simple posteriors given in Eq. (8.22), the mean and variance of the
posterior contain most of the information, although these can differ signiﬁcantly
from the classically estimated mean, ˆF, and variance, Δ ˆF, deﬁned in Section 8.3.
Nonetheless, it is inevitably ˆF and Δ ˆF which are reported in survey databases,
and, without reprocessing the raw data, it might seem there is no option but to
‘make do’ with these quantities, maybe constructing the effective posterior de-
ﬁned in Eq. (8.13) to at least encode this information in a probabilistic form. How-
ever, it should also be possible to reconstruct likelihoods from the catalogued ﬂux
estimates and, given that the likelihood represents the most complete statistical
description of the data, it could be argued that this is a necessary condition if a
database is to live up to its name.
Clearly it is possible to ensure this is true in future surveys, but what about
the billions of sources that are already catalogued? For bright sources, speciﬁcally
those for which the background uncertainty can be neglected, ˆF and Δ ˆF are suf-
ﬁcient to reconstruct the likelihood by setting σb = 0 and C = (Δ ˆF)2/ ˆF in
Eq. (8.18). Similarly, if the Poisson contribution to the error from ﬁnite photon
counts is negligible (as is the case for most faint sources), the likelihood can be
obtained from ˆF and Δ ˆF by setting σb = Δ ˆF and C = 0 in Eq. (8.18). It is also
possible to solve the more general problem of reconstructing the variance term
σ2
b + CF in Eq. (8.18), either by directly obtaining the background noise, σb, and
the ﬂux calibration, C, or, failing that, by utilizing the detection limit of the survey,
Fdet, as introduced in Section 8.6.
The ﬂux limit is deﬁned in many different ways, but mostly it is a signal-to-noise
based criterion, in which a source must have ˆF/Δ ˆF >∼(S/N)min to be reliably
detected. Working with this deﬁnition, Eqs. (8.6) and (8.12) can then be combined

210
Flux measurement
to give
C = Fdet
(Δ ˆF/Fdet)2/(S/N)2
min −1
ˆF/Fdet/(S/N)2
min −1
(8.23)
and
σb =
Fdet/ ˆF
(S/N)2
min
Fdet/ ˆF −(Δ ˆF/ ˆF)2
1 −Fdet/ ˆF/(S/N)2
min
.
(8.24)
Inserting these expressions into Eq. (8.18) then gives the full likelihood for a source
from the estimated quantities ˆF and Δ ˆF and the survey parameters Fdet and
(S/N)min. For different types of surveys an equivalent calculation should yield
the desired quantities, the key point being that it must be possible to reconstruct
the likelihood in order to extract all the information contained in the data.
It is possible to take this approach one step further by reconstructing not just the
likelihood, but the posterior, from the catalogued data. Indeed, Hogg and Turner
(1998) advocate that their maximum posterior corrections, which account for the
slope of the source counts, ‘should in principle be applied to all ﬂux measurements
in a ﬂux-limited sample’. Although Hogg and Turner (1998) only treat the low-
ﬂux peak in the posterior qualitatively, their corrections are the same as would be
obtained from Eq. (8.22), so why apply them only ‘in principle’? And why has this
suggestion largely been ignored (but see Laird et al. 2009), with survey databases
still containing only classical ﬂux estimates?
The immediate reaction of many Bayesians would be to put this down to a mis-
guided faith in classical methods, but there are actually several good reasons why
posteriors are not, and should not, be used in survey catalogues and databases. As
appealing an idea as that might be in principle, it is ﬂawed in practice because
there is no deﬁnitive ﬂux prior that can be applied to a source that will always be
appropriate. A generic model like the power law used in Section 8.4 is probably ad-
equate for a newly discovered source about which little else is known, but as more
and more information is obtained (e.g., through follow-up spectroscopy or a proper
motion measurement) the prior – or at least external – information will evolve. Sim-
ilarly, a good measurement of a source’s ﬂux in one passband is likely to imply a
very tight prior given by the previously observed distribution of ﬂux ratios between
the two bands (Hogg & Turner 1998): the range of X-ray hardness ratios or optical
colours is much smaller than the full range of ﬂuxes allowed by the prior given in
Eq. (8.17). Essentially, it is impossible to combine measurements given only the
corresponding posterior distributions; there is no general way of disentangling the
information provided by each measurement from whatever priors have been used.
The same problem could arise in the example given in Chapter 11, in which the
source population is being modelled: a Bayesian approach to this problem requires

8.7 Practical ﬂux measurement
211
the likelihood for each source, not the posterior obtained given some model for the
population.
It might seem strange to be advocating against the use of prior information in a
book on Bayesian methods, but this is the difference between the optimal recording
of data and attempting to answer scientiﬁc questions from that data. The prior infor-
mation is critical to any questions about the properties of the real world for which
the data do not completely determine the answer. But with so many different pos-
sible constraints available, what prior knowledge should be applied? The answer,
inevitably, is all of it; whatever relevant information is available should be utilized.
And if more information becomes available subsequently then it is perfectly legit-
imate to apply that as well. As noted by Jaynes (1991), ‘every Bayesian problem
is open-ended: no matter how much analysis you have completed, this only sug-
gests still other kinds of prior information that you might have had, and therefore
still more interesting calculations that need to be done, to get still deeper insight
into the problem’. While the likelihood for a given measurement should be ﬁxed,
other measurements – or insights – will always occur. One of the great beauties
of Bayesian inference is that this evolution of knowledge is naturally incorporated
into what is, essentially, a formalism for learning.
References
Beckwith, S. V. W. et al. (2006). Astron. J., 132, 1729.
Condon, J. J. (1974). Astrophys. J., 188, 279.
Eddington, A. S. (1913). Mon. Not. Roy. Astron. Soc., 73, 359.
Hobson, M. P. and McLachlan C. I. (2003). Mon. Not. Roy. Astron. Soc., 338, 765.
Hogg, D. W. and Turner, E. L. (1998). Proc. Astron. Soc. Pac., 110, 727.
Hopkins, A. M., Mobasher, B., Cram, L. and Rowan-Robinson, M. (1998). Mon. Not. Roy.
Astron. Soc., 296, 839.
Irwin, M. J. (1990). Mon. Not. Roy. Astron. Soc., 214, 575.
Jaynes, E. T. (1991). In W. T. Grandy and L. Schick, eds., Proceedings of the Tenth Annual
MAXENT Workshop. Dordrecht: Kluwer Academic Press.
Jaynes, E. T. (2003). Probability Theory: The Logic of Science. Cambridge: Cambridge
University Press.
Jones, L. R., Fong, R., Shanks, T., Ellis, R. S. and Peterson, B. A. (1991). Mon. Not. Roy.
Astron. Soc., 249, 481.
Laird, E. S. et al. (2009). Astrophys. J. Supp., 180, 102.
Law, N. M., Mackay, C. D. and Baldwin, J. E. (2006). Astron. Astrophys., 446, 739.
Lawrence, A. et al. (2007). Mon. Not. Roy. Astron. Soc., 381, 1400.
Lupton, R. H., Gunn, J. E. and Szalay, A. S. (1999). Astron. J., 118, L1406.
Maddox, S. J., Efstathiou, G., Sutherland, W. J. and Loveday, J. (1990). Mon. Not. Roy.
Astron. Soc., 243, 692.
Murdoch, H. S., Crawford, D. F. and Jauncey, D. L. (1973). Astrophys. J., 183, 1.
Murphy, T. W. et al. (2008). Proc. Astron. Soc. Pac., 120, 20.
Peebles, P. J. E. (1993). The Principles of Physical Cosmology. Princeton, NJ: Princeton
University Press.

212
Flux measurement
Scheuer, P. A. G. (1957). Proc. Camb. Phil. Soc., 53, 764.
Stetson, P. B. (1987). Proc. Astron. Soc. Pac., 99, 191.
Tsutomu, T. and Takako, T. I. (2004). Astrophys. J., 604, 40.
Wall, J. V. and Jenkins, C. R. (2003). Practical Statistics for Astronomers. Cambridge:
Cambridge University Press.
York, D. G. et al. (2000). Astron. J., 120, 1579.

9
Gravitational wave astronomy
Neil Cornish
9.1 A new spectrum
Just a century ago, our view of the cosmos was limited to the single window
provided by optical telescopes. In the intervening years, new windows opened as
cosmic rays, neutrinos, radio waves, X-rays, microwaves and gamma rays were
enlisted in our quest to understand the Universe. These new messengers have rev-
olutionized astronomy and profoundly altered our perception of the cosmos.
Despite these impressive advances, almost everything we know about the Uni-
verse beyond our own galaxy comes from observing light of various energies.
Hopefully this will soon change as a new spectrum is opened by the direct de-
tection of gravitational waves. Just as our sense of hearing complements our sense
of sight, gravitational wave astronomy can extend and enrich the picture provided
by electromagnetic astronomy (Hughes 2003).
Astrophysical and cosmological sources of gravitational waves are expected to
produce signals that span over twenty decades in frequency, ranging from primor-
dial signals with frequencies as low as 10−18 Hz, to supernovae explosions that
reach frequencies of 104 Hz. A suite of detection techniques have been proposed
to detect these signals, from acoustic (bar) detectors for narrow band detection
of high-frequency waves, through to polarization maps of the cosmic microwave
background radiation to look for imprints of the lowest frequency waves. Pul-
sar timing arrays are a promising technique for detecting waves in the 10−8 Hz
range, and there is an outside chance that this technique might yield the ﬁrst direct
detection. Most attention in gravitational wave astronomy is focused on ground
and space-based interferometric detectors, which together will span the frequency
range between 10−5 and 104 Hz. The ﬁrst generation of ground-based detectors,
GEO, TAMA, LIGO and Virgo, have completed their initial science runs,
213

214
Gravitational wave astronomy
including a full-year triple coincidence run at design sensitivity for the US-based
LIGO detectors. Planning for the joint ESA–NASA space-based LISA mission
is well advanced, and development will proceed rapidly once funding is made
available.
The science potential for gravitational wave astronomy is as wide as its spectral
range and as varied as the detector technologies. Any asymmetric movement of
mass/energy will produce gravitational waves, but the extreme stiffness of space-
time limits the detectable phenomena to those that are either extremely violent or
nearby. The most promising sources involve large amounts of high-density mate-
rial moving at close to the speed of light, such as occurs in tight binary systems
composed of two compact stellar remnants, or core collapse supernovae. The ﬁnal
inspiral and merger of two black holes of the type found at our Galactic centre
could be seen out to z = 10 or greater by the LISA observatory, while stellar rem-
nant black-hole binaries could be seen out to z = 1 or greater by Advanced LIGO.
These observations can be used to probe fundamental physics by testing general
relativity in the dynamical, strong ﬁeld regime, and to study star and galaxy forma-
tion throughout cosmic history.
For a more in-depth introduction to gravitational wave astronomy, see the re-
cent review articles by Hughes (Hughes 2003) and Camp and Cornish (2004), and
the overviews produced by LIGO (LIGO Scientiﬁc Collaboration 2007) and LISA
(LISA International Science Team 2007) science collaborations.
9.2 Gravitational wave data analysis
The development of practical methods for applying Bayesian inference to scientiﬁc
data largely predates the development of gravitational wave data analysis but, until
recently, traditional frequentist approaches have held sway. The historical reasons
for this choice appear to be a combination of the inertia of existing paradigms, and
a highly inﬂuential paper by Thorne (1987) that set the direction for gravitational
wave data analysis. Thorne’s paper described the technique of Wiener matched
ﬁltering for constructing a frequentist detection statistic. Today, the majority of pa-
pers that describe the analysis of data from gravitational wave detectors employ
frequentist techniques and, where applicable, the matched ﬁltering statistic sug-
gested by Thorne.
Given this state of affairs it is rather surprising to ﬁnd that many of the seminal
papers (Finn 1992; Finn & Chernoff 1993; Flanagan & Hughes 1998a,b;
Anderson et al. 2001) on gravitational wave data analysis employ Bayesian prob-
ability theory. These analyses are generally of a hybrid type, where Bayesian rea-
soning is used to motivate the form of a particular frequentist statistic. This hybrid
style of analysis is particularly evident in recent studies of stochastic backgrounds

9.2 Gravitational wave data analysis
215
(Allen et al. 2003), unmodeled bursts (Searle et al. 2008) and methods for setting
upper limits (Biswas et al. 2007).
A short time after Thorne’s article appeared, Davis (1989) suggested that Bayes
estimators be used in gravitational wave data analysis, but he also noted that the
computational cost may be prohibitive. The possibility of using Bayesian infer-
ence outside the conﬁnes of abstract theoretical analyses did not take root for an-
other decade. The turning point came when Christensen and Meyer (1998) applied
MCMC techniques to simulated gravitational wave data. This pioneering work was
followed by a trickle, and later a ﬂood, of papers that explored the use of practical
Bayesian inference techniques in a vast array of gravitational wave data analysis
problems. At the present time there is a wealth of data to analyze from the world-
wide network of gravitational wave detectors, and MCMC pipelines are being de-
veloped to perform follow-ups to the triggers generated by the frequentist analyses.
The application of Bayesian inference to parameter estimation is now fairly well
accepted within the gravitational wave community, but there is currently little en-
thusiasm for taking a Bayesian approach to the detection problem.
Even without a ﬁrst detection, gravitational wave astronomy has grown into a
large ﬁeld that involves many types of detectors and many different source classes.
Here we will focus our attention on laser interferometers, and sources for which
theoretical waveforms have been derived, though most of the discussion also ap-
plies to acoustic detectors, pulsar timing arrays, and deterministic yet unmodelled
sources.
9.2.1 The traditional approach
The output of an idealized gravitational wave detector can be described by a time
series s(t) that is a linear combination of possible signal(s) h(t) and noise n(t):
s(t) = h(t) + n(t) .
(9.1)
For much of the discussion to follow it is more convenient to work in the Fourier
domain where
˜s(f) =
 ∞
−∞
e2πifts(t)dt .
(9.2)
In what follows we will simplify our notation by writing s(f) for the output of the
Fourier transform.
For an ideal detector the response h(t) to a gravitational wave signal with polar-
ization components h+(t) and h×(t) is linear, and described by
h(t) = F +(t; θ, φ, ψ, τ)h+(t) + F ×(t; θ, φ, ψ, τ)h×(t).
(9.3)

216
Gravitational wave astronomy
Here F + and F × are the antenna beam patterns of the detector, which depend on
the sky location (θ, φ) and polarization angle ψ of the source in some ﬁxed refer-
ence system, and the orientation of the detector in that reference system at time t.
When more than one detector is available, or the signal duration is long compared
with the orbital timescale of the detector, the response must also include the time
delay τ between the wave striking the detector and the wave arriving at some ﬁxed
reference location (such as the solar barycenter). Additional complications arise
when the wavelength is comparable to, or shorter than, the size of the detector, as
then the antenna beam functions have to be replaced by linear time delay operators.
The output of our ideal detector includes instrument noise of the stationary and
Gaussian variety favoured by theorists. Such noise is fully speciﬁed by the expec-
tation values:
⟨n(f)⟩= 0
⟨n(f)n∗(f′)⟩= T
2 δff′Sn(f) ,
(9.4)
where Sn(f) is the one-sided noise spectral density and T is the observation time.
The statistical properties of the noise motivate a natural inner product on the vector
space of signals, deﬁned by
(a|b) = 2
 ∞
0
a(f)b∗(f) + a∗(f)b(f)
Sn(f)
df .
(9.5)
For several important classes of gravitational wave signal there exist analytic
theoretical models for the gravitational waveforms that can be used to construct
parameterized models h(⃗λ) that predict the instrument response to a source de-
scribed by parameters ⃗λ. Examples include the quasi-circular inspiral of two spin-
ning black holes (D = dim(⃗λ) = 15), and the post-merger ringdown of a single
distorted black hole (D = 6). Standard matched-ﬁlter searches for modelled sig-
nals employ the Wiener ﬁlter statistic
ρ(⃗λ) = (s|ˆh(⃗λ)) ,
(9.6)
where the ﬁlter functions are scaled to unit norm: (ˆh|ˆh) = 1. The ﬁlter is optimal
in the sense that it maximizes the ratio of signal power to noise power for signals
described by h(⃗λ) in the presence of stationary Gaussian noise. When a signal h(⃗λ)
is present in the data, the expectation value of ρ is equal to the optimal (amplitude)
signal-to-noise ratio (SNR) of the signal:
⟨ρ⟩= SNR =
,
(h(⃗λ)|h(⃗λ)) .
(9.7)

9.2 Gravitational wave data analysis
217
1e-24
1e-23
1e-22
1e-21
100
1000
hf (Hz-1/2)
hf (Hz-1/2)
hf (Hz-1/2)
hf (Hz-1/2)
f (Hz)
LHO
SNR = 22
1e-24
1e-23
1e-22
1e-21
100
1000
f (Hz)
LLO
SNR = 24
1e-24
1e-23
1e-22
1e-21
100
1000
f (Hz)
Virgo
SNR = 23
1e-24
1e-23
1e-22
1e-21
100
1000
f (Hz)
GEO
SNR = 10
Fig. 9.1. Simulated strain spectral densities for a binary black-hole inspiral signal inci-
dent on the LIGO Hanford (LHO), LIGO Livingston (LLO), Virgo and GEO network of
detectors. The dashed line corresponds to the design level instrument noise for the ﬁrst
generation of detectors. The lower line in each panel is the raw strain, and the upper line is
the effective strain after matched ﬁltering. The single detector SNR is listed in each panel.
The simulated signal is for a pair of black holes with masses M of 10M⊙and 5M⊙and
spin magnitudes S/M 2 of 0.7 and 0.5, at a luminosity distance of 10 Mpc.
The utility of the matched ﬁltering approach can be seen by considering the contri-
bution to the SNR from a logarithmic frequency interval with central frequency f:
d SNR2
d ln f
= 2
Sh(f)
Sn(f)

N(f) .
(9.8)
Here Sh(f)/Sn(f) is the raw (power) signal-to-noise ratio, and N(f) = fT is
the number of wave cycles observed. By coherently matching the waveform over
many cycles, the raw signal-to-noise is enhanced by N(f), and it becomes possible
to detect signals that are buried deep below the instrument noise. The utility of the
matched ﬁltering approach is illustrated in Figure 9.1.
The search for the best-ﬁt signal parameters is performed by laying out a uni-
formly spaced template grid and ﬁltering the signal against each template (Owen
1996). The grid spacing is then determined by using the overlap (or match)

218
Gravitational wave astronomy
function M(⃗θ,⃗λ) = (ˆh(⃗θ)|ˆh(⃗λ)), which deﬁnes a natural Riemannian metric gij(⃗λ)
on the space of signals:
M(⃗λ + Δ⃗λ,⃗λ) = 1 −gij(⃗λ)ΔλiΔλj + · · · ,
(9.9)
where
gij(⃗λ) = −1
2
∂2M
∂λi∂λj .
(9.10)
The choice of grid spacing involves a trade-off between computational cost and
signal coverage. Adopting a minimum match, Mmin, between templates sets the
allowed fractional loss in signal-to-noise, and determines the number of templates
needed to cover the search space. For a hypercubic lattice, the number of temp-
lates is given by N = V/ΔV , where V =
	
dDλ√g is the volume of the parameter
space and ΔV = (2

(1 −Mmin)/D)D is the volume covered by each template.
At ﬁrst sight it is not obvious how a matched-ﬁlter analysis is related to the meth-
ods used in Bayesian inference. The relationship becomes less mysterious when
optimal matched ﬁltering is recast in terms of maximum likelihood estimation. To
establish this link, consider the probability that the noise in a given frequency bin
is equal to n(f). For data s0(f) with no gravitational wave signal present, the con-
ditional probability of observing n(f) is, for Gaussian instrument noise,
pr(s0|n) =
1
2πTSn(f) exp

−2n(f)n(f)∗
TSn(f)

.
(9.11)
Now, the conditional probability of measuring s(f) when a signal h(f) is present,
pr(s|h), is equal to the conditional probability of measuring s0 = s −h assuming
that no signal is present, pr(s0|n). Thus,
pr(s|h) =
1
2πTSn(f) exp

−2(s(f) −h(f))(s∗(f) −h∗(f))
TSn(f)

.
(9.12)
Combining the contributions from all the frequency bins yields the likelihood
pr(s|h) = pr(s|⃗λ) = C exp

−(s −h|s −h)
2

.
(9.13)
The normalization constant C ∼exp(−
	
log Sn(f)df) is often ignored since it
does not depend on the signal h(⃗λ). The generalization to a network of detec-
tors is straightforward: simply multiply together the likelihoods for each detector.
To evaluate the maximum likelihood, ﬁrst maximize with respect to the template
amplitude A = (h|h)1/2, which yields the solution AML = (s|ˆh) = ρ(⃗λ) and the
partial maximization

pr(s|⃗λ)

maxA
= C′ exp ρ2(⃗λ)
2
.
(9.14)

9.2 Gravitational wave data analysis
219
Maximizing the likelihood with respect to the remaining signal parameters is then
equivalent to maximizing the matched-ﬁlter statistic ρ(⃗λ).
In the frequentist setting, the detection problem is usually addressed by applying
the Neyman–Pearson criterion, which seeks to minimize the false dismissal prob-
ability for a given false alarm probability. Here false dismissal corresponds to the
conclusion that h = 0 when in fact a signal is present, and false alarm corresponds
to the conclusion that h ̸= 0 when no signal is present. For Gaussian noise, the
likelihood ratio
Λ = pr(s|h)
pr(s|0) = exp

−(s|h) + 1
2(h|h)

(9.15)
provides an optimal decision statistic for the Neyman–Pearson test. The space of
possible observations is partitioned by setting a threshold Λ0 > 0 that yields the
desired false alarm probability. In practical applications, where the noise is neither
stationary nor Gaussian, the likelihood ratio Λ, or the closely related matched-
ﬁlter statistic ρ, are adopted as a detection statistic, and Monte Carlo simulations
are used to estimate the false alarm and false dismissal probabilities. The false dis-
missal probability is estimated using hardware and software injections of simulated
gravitational wave signals. Estimating the false alarm probability is more difﬁcult
since there is no way to know in advance if a signal is present in the data or not, as
it is impossible to shield the detector from gravitational waves. The closest alter-
native is to artiﬁcially time-shift the data between detectors in a network, thereby
destroying the coherent response to a gravitational wave, while leaving the noise
largely unaffected.
While waiting for the ﬁrst detection, gravitational wave astronomers amuse
themselves by predicting what can be learned from observing various sources of
gravitational waves. A key element of these investigations are estimates of how
accurately the source parameters can be recovered. These estimates employ the
Gaussian approximation to the likelihood in the neighbourhood of maximum like-
lihood:
pr(s|⃗λML + Δ⃗λ) =

det
 Γ
2π
1/2
e−1
2ΓijΔλiΔλj
,
(9.16)
where
Γij = −
!∂2 log p(s|⃗λ)
∂λi∂λj
"00000
ML
= (h,i|h,j) .
(9.17)
At this level of approximation, the parameter error variance–covariance matrix is
given by
⟨ΔλiΔλj⟩= Γ−1
ij .
(9.18)

220
Gravitational wave astronomy
In the gravitational wave literature the matrix Γij is generally referred to as the
Fisher Information Matrix, even though this terminology is only strictly correct if
the priors are uniform.
9.3 The Bayesian approach
The availability of fast computers and the development of efﬁcient algorithms for
mechanizing Bayesian inference has transformed data analysis in many ﬁelds. The
ﬂedgling ﬁeld of gravitational wave astronomy has been slow to adopt these new
techniques, in part because of history, and in part because of several technical chal-
lenges that will be described shortly. At present there is no end-to-end Bayesian
inference pipeline for the analysis of data collected by the current generation of
gravitational wave observatories, and it may be many years before such a pipeline
is developed. The situation is more promising for the future space-based detector
LISA, where Bayesian methods have taken root early in the data analysis devel-
opment effort, and portions of a comprehensive Bayesian inference pipeline have
been implemented and tested on simulated data. In the near term, the main ap-
plication of Bayesian analysis will be to follow up detection candidates from the
standard search pipelines. The goal is to improve upon the point estimate for the
signal parameters that comes from a matched-ﬁlter or maximum likelihood search,
by providing full posterior distributions. In other words, the Bayesian contribution
will be to put ‘error bars’ on the parameters.
Two technical challenges have hindered the adoption of practical Bayesian infer-
ence in gravitational wave astronomy. The ﬁrst is the complexity of the detectors,
which makes it difﬁcult to come up with an appropriate form for the likelihood
function, and the second is the tight concentration of the posterior weight within a
high-dimensional search space (in other words, the signals are hard to ﬁnd).
The likelihood function quoted in Eq. (9.13) is deﬁcient in several respects: it
considers just the gravitational wave data channel, it assumes that the noise is sta-
tionary and Gaussian, and it assumes that the instrument response is linear. The
state of the art gravitational wave interferometers at the LIGO and Virgo sites are
complex machines with many thousands of components, controlled by dozens of
non-linear feedback loops, and covered with sensors that record tens of thousands
of channels of data in addition to the ‘gravitational wave’ channel that monitors the
differential motion of the test masses. It is known that certain disturbances recorded
in the monitoring channels can correlate with disturbances in the gravitational wave
channel, and this should be taken into account in the likelihood function. However,
despite heroic efforts by the detector characterization teams, the full spectrum of
cross couplings is far from understood, and a multi-channel likelihood function
remains a distant dream. Instead, a list of data quality ﬂags are produced that tag

9.3 The Bayesian approach
221
sections of data with descriptions of possible problems and a numerical assessment
of their severity. The highest-level ﬂags are used to veto sections of data, while the
lower-level ﬂags are considered when following up potential detections. The in-
struments also exhibit various glitches and excursions that are not ﬂagged by any
monitoring channel, and the overall noise spectrum changes over time. The mea-
surement process is further complicated by the feedback loop that is used to keep
the interferometers locked: the length of the arms is adjusted so that the laser light
from the two arms interferes destructively at the so-called anti-symmetric port. If
a passing gravitational wave, or some other disturbance, acts to change the relative
length of the two arms, forces are applied to the test masses (mirrors) so as to keep
the anti-symmetric port dark. Thus, the gravitational wave signal is encoded in the
response of the non-linear control loop that keeps the interferometers in lock. The
response of the control loop is continuously measured by applying sinusoidally
varying forces to the test masses at various frequencies. The amplitude of these
calibration lines are used to determine the transfer function for the length sensing
control loop, and from this the strain h(t) is reconstructed. This procedure leads to
a non-linear coupling of the gravitational wave signal and the instrument noise, and
the instrument noise with itself, which violates two of the key assumptions used to
derive the simple form for the likelihood in Eq. (9.13).
Despite all these difﬁculties, something very close to the expression in (9.13) can
provide a good approximation to the likelihood function. The ﬁrst modiﬁcation is
to introduce a Heaviside step function that vetoes data segments tagged by the data
quality ﬂags. The second modiﬁcation is to treat the noise level and spectral shape
as unknowns to be determined from the data, and to further allow these quantities
to depend on time (either by working with shorter segments of data or using some
other time-frequency decomposition such as wavelets). The ﬁnal modiﬁcation is
to allow for a non-Gaussian tail in the noise distribution. One method for doing
this is to model the noise as the sum of two Gaussians of different weight and
variance. A non-Gaussian tail in the instrument noise will result in the noise ﬁt
identifying a second component with larger variance. On the other hand, if the
noise is Gaussian, and the number of parameters in the noise ﬁt is itself a parameter,
the second component will be discarded. It has been shown (Allen et al. 2003) that
these modiﬁcations lead to a robust frequentist statistic that is near optimal for
weak signals buried in non-Gaussian noise.
The second technical challenge relates to the extreme concentration of the poste-
rior mass. Deﬁne ΔV to be the smallest volume of parameter space to encompass
95% of the posterior weight, and deﬁne V to be the total volume of the parame-
ter space. In many cases, the ratio ΔV/V is tiny – akin to ﬁnding a cork bobbing
about somewhere in the Paciﬁc Ocean – and the volume ΔV is often spread over
several disjoint regions surrounding secondary maxima of the posterior distribution

222
Gravitational wave astronomy
function. The smallness of this ratio is reﬂected in the large number of templates
needed to perform a frequentist analysis, and the long burn-in times for an MCMC
style analysis. A variety of techniques can be used to help mitigate this problem,
such as hierarchical searches, adaptive grid reﬁnement, simulated annealing and
parallel tempering, but the bottom line is that the signals are hard to ﬁnd. Much
of the current research on Bayesian inference in gravitational wave astronomy is
devoted to developing strategies for minimizing the burn-in time of MCMC style
analyses.
9.3.1 Parameter estimation
In the happy event of a gravitational wave detection it will be desirable to quote
more than just the best-ﬁt values for the source parameters. Electromagnetic as-
tronomers will want to know the shape and extent of the ‘error ellipse’ that de-
scribes the uncertainty in the sky location, and astrophysicists will want to know the
level of uncertainty in quantities such as the component masses and spins. When
a grid search is used to make the detection there will be some crude information
available from the distribution of likelihood values on surrounding grid points, but
the searches typically maximize, rather than marginalize, over quantities such as
time of arrival and gravitational wave phase, and prior information is not accounted
for. A far more informative analysis of the parameter distributions can be derived
using standard MCMC techniques.
Metropolis and Hastings provided a simple recipe for performing MCMC
analyses that requires just three ingredients: likelihood, prior and proposal. The
basic form for the likelihood function (9.13) and its generalizations have already
been discussed. Appropriate priors for the signal parameters are fairly well under-
stood for most source types, and are arrived at using a combination of astrophysi-
cal modelling and electromagnetic observations. For example, distant sources are
expected to be uniformly distributed on the sky, and the initial orientation of a bi-
nary’s orbital plane should be uniformly distributed in the cosine of the inclination
angle. The distribution of other parameters, such as the mass M and spin S of a
black hole, are not as well understood, but theory does provide constraints, such as
S/M 2 ≤1.
The ﬁnal ingredient in the MCMC recipe – the form of the proposal distribution –
is where the MCMC practitioner gets to add a creative touch. In principle, any
non-trivial proposal distribution will yield a Markov chain with stationary distri-
bution equal to the posterior distribution function being sought. In practice, poorly
chosen proposal distributions will lead to chains that take longer than the age of the
Universe to reach stationarity. Proposal distributions that closely approximate the
posterior distribution yield well-mixed chains that rapidly approach stationarity,

9.3 The Bayesian approach
223
but constructing such distributions seems to imply advance knowledge of the end
result. The MCMC literature describes many strategies for choosing proposal dis-
tributions, and several of these strategies have been applied in the context of
gravitational wave parameter estimation (Christensen & Meyer 2001; Cornish &
Crowder 2005; Cornish & Porter 2006; Rover, Meyer & Christensen 2007). The
main methods studied so far are delayed rejection, Gaussian approximation, and
parallel tempering.
It has been found that a hybrid algorithm that uses parallel tempering and a
Gaussian approximation to the posterior as the proposal distribution delivers good
all-round performance. The method assumes that the dominant modes of the poste-
rior distribution function have been identiﬁed by the original search algorithm. The
Hessian of the log posterior, Hij, is then computed at each posterior mode, either
by direct numerical computation, or by using the Fisher matrix approximation of
(9.17). The proposal distribution is then written as
q(⃗x|⃗y) = q(⃗x) =

n
Ane−(⃗x −⃗λn)·
↔
Hn ·(⃗x −⃗λn)/2,
(9.19)
where ⃗λn is the position of the nth mode and
↔
Hn is the Hessian at this mode. The
amplitudes An describe the relative weight of each mode, and are scaled to yield
a properly normalized probability distribution. A distribution of this type can be
drawn from by ﬁrst randomly selecting the mode n (suitably weighted), then mak-
ing Gaussian draws along each eigendirection of the Hessian, with variances given
by the respective eigenvalues. The transitions between modes is aided by the tech-
nique of parallel tempering, whereby several chains with different ‘temperatures’ T
are run in parallel, and Metropolis–Hastings transitions can occur between chains.
The likelihood function for a chain with temperature T is given by [pr(s|⃗λ)]1/T ,
and the width of the peaks in the proposal distribution are similarly widened by re-
placing Hij →Hij/T. While only the samples from the T = 1 chain can be used
to construct the posterior, the higher-temperature chains help facilitate mixing and
transitions between modes. There is always the danger that a particular choice of
proposal distribution will restrict the movement of the chain, and with insufﬁcient
samples, produce a posterior distribution that follows the proposal distribution. The
parallel tempering technique helps to alleviate this tendency, as does the use of a
mixture of proposal distributions. For example, ten jumps drawn from Eq. (9.19)
might be followed by one jump drawn from a proposal distribution of the form
q(⃗x|⃗y) = ⃗y + δ⃗y ,
(9.20)
where the ith component of δ⃗y is drawn from a uniform distribution of width ϵi.
Figure 9.2 shows the marginalized posterior distributions for the 15 parameters

224
Gravitational wave astronomy
Fig. 9.2. Marginalized posterior distributions for the 15 parameters that describe the
LIGO–Virgo–GEO response to a binary system of spinning black holes. The MCMC
derived posterior distributions (the noisy curves with a higher peak) are compared with
the predictions of the Fisher information matrix (the smoother, Gaussian curve). The ver-
tical dashed line indicates the injected values for the source parameters.
that describe the response of the LIGO–Virgo–GEO network to the inspiral signal
from a pair of spinning black holes (shown in Figure 9.1). The simulation employs
synthetic stationary and Gaussian instrument noise and uses the MCMC algorithm
described above.
9.3.2 Search strategies
As described in Section 9.2.1, the standard technique for searching for signals of
a known type employs matched ﬁltering against a uniformly spaced bank of tem-
plates. It is often possible to reduce the size of the search space by analytically
maximizing over some of the signal parameters, such as time of arrival, distance
to the source and the initial phase of the waveform. The computational cost of the
search can be reduced further by adopting a hierarchical approach that starts with
a coarse grid, then follows up with a ﬁner grid in the region surrounding templates
that gave an SNR above some threshold. An alternative hierarchical approach is
to start with some subset of the available data – for example, some fraction of the
total observation time or bandwidth – and progressively incorporate more of the
data.
While hierarchical grid searches work well for simple signals and low-dimen-
sional search spaces, the computational cost can become prohibitive when the com-
plexity of the signal or the dimensionality of the search space becomes large. Some
of the most challenging examples are the signals from pairs of spinning black holes,
the combined signals from tens of millions of Galactic white dwarf binaries, and
signals from extreme or intermediate mass ratio inspirals (EMRIs or IMRIs). While

9.3 The Bayesian approach
225
various hierarchical procedures have been proposed to deal with these sources, the
most successful algorithms developed to date abandon the template grid approach
and employ MCMC inspired search strategies.
The gridless search techniques that have been developed in gravitational wave
astronomy are extensions of the methods used to speed up the ‘burn-in’ phase of
MCMC posterior studies. Since the samples from the burn-in portion of the chain
are discarded, the usual rules concerning reversibility and hysteresis can be ig-
nored. Some of the more effective algorithms (Cornish & Porter 2007; Crowder &
Cornish 2007) combine deterministic hill-climbing moves with stochastic
Metropolis–Hastings sampling and simulated annealing. These hybrid algorithms
are generally superior to purely deterministic approaches, such as the Nelder–Mead
SIMPLEX (amoeba) algorithm, which tend to get stuck at local maxima, and they
also out-perform standard random walk Metropolis–Hastings algorithms due to
their superior hill-climbing abilities.
These directed-stochastic searches share many features in common with genetic
algorithms (Crowder, Cornish & Reddinger 2006), and their success relies on the
posterior landscape offering ‘partial credit’. If the landscape resembles a ﬂat plain
with a single sharp spike at the posterior mode, then a directed-stochastic search
would take longer to locate the mode than a methodical grid search. On the other
hand, if the posterior landscape more closely resembled Colorado, where the east-
ern plains gradually rise up until they reach the foothills of the Rocky Moun-
tains, then a directed-stochastic search would quickly home in on the posterior
mode, while a simple grid search would waste time out in the great plains. An
intelligently designed hierarchical grid search could also exploit the large-scale
structure of the posterior to achieve comparable performance, but the advantage
of the directed-stochastic searches is that they arrive at a near optimal strategy
automatically.
9.3.3 Model selection
One of the most exciting areas that is currently undergoing rapid development in
gravitational wave astronomy is the application of Bayesian model selection to
important questions, such as ‘Do the data contain a gravitational wave signal?’
For the detection question the models are: Model 0 – ‘only noise is present’ and
Model 1 – ‘there is a gravitational wave signal and noise present’. As mentioned
in Section 9.2.1, the traditional approach to the detection question employs the
Neyman–Pearson test, calibrated with extensive Monte Carlo simulations of
injected signals and scrambled data.
Bayesian inference provides a well-deﬁned answer to the detection question.
Bayes’ theorem states that the posterior probability of model Mi, pr(Mi|s), is

226
Gravitational wave astronomy
given in terms of the prior probability of the model, pr(Mi), and the marginal
likelihood or evidence for Mi, pr(s|Mi), by
pr(Mi|s) = pr(Mi)pr(s|Mi)
pr(s)
.
(9.21)
The normalization factor pr(s) is unimportant here since we are only interested in
the relative probability, or odds ratio of model Mi against model Mj:
Oij = pr(Mi|s)
pr(Mj|s) = pr(Mi)
pr(Mj)
pr(s|Mi)
pr(s|Mj) = Pij Bij .
(9.22)
The odds ratio is given by the product of the prior belief, Pij = pr(Mi)/pr(Mj),
and the Bayes factor Bij = pr(s|Mi)/pr(s|Mj). The Bayes factor is a measure of
how the data have informed our degree of belief in the two models.
The evaluation of the model evidence involves an integral of the likelihood
weighted by the priors on the model parameters ⃗λ:
pr(Mi|s) =

d⃗λ pr(⃗λ|Mi) pr(s|⃗λ, Mi) .
(9.23)
In most cases of interest in gravitational wave astronomy, the parameter space
dimension is large, and a direct numerical evaluation of the evidence integral is
impractical. One exception is the search for gravitational wave signals associated
with neutron star spin glitches. When treated as a power spectrum search, the glitch
signal depends on just three parameters: amplitude, decay time, and characteristic
frequency, and the evidence can be computed by brute force (Clark et al. 2008).
Very recently the powerful technique of nested sampling (Skilling 2006) has been
used to compute the model evidence for simulated black-hole inspiral signals em-
bedded in synthetic noise (Veitch & Vecchio 2008). It is hoped that this technique
will eventually be implemented as part of the detection follow-up procedure for
the LIGO–Virgo–GEO network. Careful thought will have to be given to the as-
signment of prior belief in this implementation, as the events being considered will
have been pre-selected as promising detection candidates.
The Reverse Jump Markov chain Monte Carlo (RJMCMC) (Green 1995) algo-
rithm provides an alternative computational framework for performing Bayesian
model selection that neatly side-steps the need to compute the model evidence.
The RJMCMC approach extends the usual MCMC technique to include transitions
between models. The Bayes factor Bij is then given by the ratio of the time the
chain spends exploring model Mi to the time spent exploring model Mj. The ad-
vantage of this approach is that it combines parameter estimation and model selec-
tion within a single framework. The RJMCMC approach is particularly well suited
to the analysis of data from the future space-based LISA detector, where one has
to contend with a Galactic foreground produced by tens of millions of white dwarf

9.3 The Bayesian approach
227
 0.1
1
 10
 100
 1000
0
1
2
3
4
5
6
7
B10
SNR
Fig. 9.3. Bayes factor as a function of signal-to-noise for model 0, pure instrument noise,
and model 1, instrument noise and the signal from a white dwarf binary with frequency
∼3 mHz. The three lines correspond to different noise realizations, with Bayes factors that
cross the detection threshold for different signal-to-noise ratios. This illustrates that the
detectability of a signal depends on the detailed structure of the noise, in addition to the
relative strength of the signal and noise.
binaries in our galaxy. To avoid biasing the analysis of the more interesting extra-
galactic signals from massive black holes and extreme mass ratio inspirals it will be
necessary to simultaneously solve for all the signals present in the data. Since the
number and nature of the resolvable signals is not known a priori, model selection
will take centre stage in LISA data analysis. While MCMC techniques have been
employed to isolate the signals from ∼20 000 white dwarf binaries in simulated
data and to explore the joint posterior (Crowder & Cornish 2007), the model selec-
tion in these large-scale simulations has relied on approximations to the evidence.
The RJMCMC technique has been applied to the simpler model selection problem
of deciding if a single white dwarf binary is better described as monochromatic
or chirping (Cornish & Littenberg 2007), and there have been some pilot studies
that have looked at determining the number of white dwarf signals present in sim-
ulated data. The ﬁrst of these studies considered the signals from two overlapping
white dwarf binaries, and compared models with 1, 2 or 3 signals present (Stroeer,
Gair & Vecchio 2006). This work has recently been extended to include the de-
tection problem, where simulated data with 0 or 1 white dwarf signals present is
used to compare models with 0 or 1 signals present (Littenberg & Cornish 2009).
Figure 9.3 shows how the Bayes factor B10 varies with the signal-to-noise of a

228
Gravitational wave astronomy
simulated white dwarf signal. The noise model employed eight parameters, while
the signal plus noise model requires an additional eight parameters to describe the
signal from a chirping white dwarf binary. Work is currently in progress to apply
the RJMCMC approach to larger numbers of white dwarf signals, and to the binary
inspiral detection problem for ground-based detectors.
References
Allen, B., Creighton, J. D. E., Flanagan, E. E. and Romano, J. D. (2003). Phys. Rev. D, 67,
122002.
Anderson, W. G., Brady, P. R., Creighton, J. D. E. and Flanagan, E. E. (2001). Phys. Rev. D,
63, 042003.
Biswas, R., Brady, P. R., Creighton, J. D. E. and Fairhurst, S. (2007). arXiv:0710.0465
[gr-qc].
Camp, J. B. and Cornish, N. J. (2004). Annu. Rev. Nucl. Part. Sci., 54, 525.
Christensen, N. and Meyer, R. (1998). Phys. Rev. D, 58, 082001.
Christensen, N. and Meyer, R. (2001). Phys. Rev. D, 64, 022001.
Clark, J., Heng, I. S., Pitkin, M. and Woan, G. (2008). J. Phys. Conf. Ser., 122, 012035.
Cornish, N. J. and Crowder, J. (2005). Phys. Rev. D, 72, 043005.
Cornish, N. J. and Littenberg, T. B. (2007). Phys. Rev. D, 76, 083006.
Cornish, N. J. and Porter, E. K. (2006). Class. Quant. Grav., 23, S761.
Cornish, N. J. and Porter, E. K. (2007). Phys. Rev. D, 75, 021301.
Crowder, J. and Cornish, N. J. (2007). Phys. Rev. D, 75, 043008.
Crowder, J., Cornish, N. J. and Reddinger, L. (2006). Phys. Rev. D, 73, 063011.
Davis, M. H. A. (1989). In B. F. Schutz, ed., Gravitational Wave Data Analysis. Dordrecht:
Kluwer, p. 73.
Finn, L. S. (1992). Phys. Rev. D, 46, 5236.
Finn, L. S. and Chernoff, D. F. (1993). Phys. Rev. D, 47, 2198.
Flanagan, E. E. and Hughes, S. A. (1998a). Phys. Rev. D, 57, 4535.
Flanagan, E. E. and Hughes, S. A. (1998b). Phys. Rev. D, 57, 4566.
Green, P. J., (1995), Biometrika, 82, 711.
Hughes, S. A. (2003). Ann. Phys., 303, 142.
LIGO Scientiﬁc Collaboration (2007). arXiv:0711.3041 [gr-qc].
LISA International Science Team (2007). http://www.srl.caltech.edu/lisa/documents/
lisasciencecase.pdf.
Littenberg, T. B. and Cornish, N. J. (2009). arXiv:0902.0368.
Owen, B. J. (1996). Phys. Rev. D, 53, 6749.
Rover, C., Meyer, R. and Christensen, N. (2007). Phys. Rev. D, 75, 062004.
Searle, A. C., Sutton, P. J., Tinto, M. and Woan, G. (2008). Class. Quant. Grav., 25, 114038.
Skilling, J. (2006). Bayesian Analysis, 1, 833.
Stroeer, A., Gair, J. and Vecchio, A. (2006). AIP Conf. Proc., 873, 444.
Thorne, K. S. (1987). In S. W. Hawking and W. Israel, eds., 300 Years of Gravitation.
Cambridge: Cambridge University Press.
Veitch, J. and Vecchio, A. (2008). Phys. Rev. D, 78, 022001.

10
Bayesian analysis of cosmic microwave
background data
Andrew H. Jaffe
10.1 Introduction
At redshifts z ∼
> 1000, with temperatures higher than those equivalent to the
recombination energy of hydrogen (1 Ry = 13.6 eV), there were sufﬁcient ener-
getic photons in the Universe to maintain a very high ionization fraction among
the atomic species (75% hydrogen with the remainder almost entirely helium). In
such an ionized plasma, the mean free path of photons to Thomson scattering off of
the electrons is very short compared with the Hubble scale, and the Universe is ef-
fectively opaque. As the Universe cools, the photons no longer possess enough
energy to maintain the high ionization fraction, and the protons and electrons
very rapidly combine into neutral hydrogen gas.1 This neutral gas no longer scat-
ters the photons, and hence they decouple, streaming freely through the Universe
thereafter, and redshifting until they are observed today as the cosmic microwave
background (CMB), with an average temperature T0 = 2.725 ± 0.001 K (Fixsen
et al. 1996). Because the state of the plasma was not completely uniform, we ob-
serve a slightly different temperature of the gas in different directions, with an rms
deviation ⟨(ΔT/T0)2⟩1/2 ∼10−5.
Observationally, as predicted by the inﬂationary paradigm for their generation,
these temperature ﬂuctuations appear to be well described as a statistically
isotropic Gaussian ﬁeld on the sky (reﬂecting an underlying statistically isotropic
1 In fact this occurs not at kT ≃13.6 eV, but rather at the lower temperature kT ≃0.3 eV, as there are
roughly 109 photons per baryon in the Universe and thus there are sufﬁcient energetic photons in the tail of
the Bose–Einstein distribution to maintain a high ionization fraction.
229

230
Bayesian analysis of cosmic microwave background data
three-dimensional density ﬁeld, a perturbation around the homogeneous and
isotropic Friedmann–Robertson–Walker background):
-ΔT
T0
(ˆx)ΔT
T0
(ˆy)
.
= C(arccos ˆx · ˆy),
(10.1)
where C(θ) is the correlation function at angular distance θ. Isotropy is embod-
ied in the requirement that this correlation function only depends upon θ and not
the angles themselves. The small fractional perturbation amplitude means that the
evolution of the ﬂuctuations is well described by linear equations, which in turn
implies that an initially Gaussian distribution is subsequently maintained.
It is more convenient to work in the spherical equivalent of Fourier space, the
spherical harmonic domain:
T(ˆn) =
∞

ℓ=2
+ℓ

m=−ℓ
aℓmYℓm(ˆn),
(10.2)
which has diagonal correlations given by
⟨aℓma∗
ℓ′m′⟩= Cℓδℓℓ′δmm′,
(10.3)
where Cℓis the power spectrum, related to the correlation function by
C(θ) =

ℓ
2ℓ+ 1
4π
CℓPℓ(cos θ),
(10.4)
or
Cℓ= 4π

d(cos θ)Pℓ(cos θ)C(θ) .
(10.5)
Note that the monopole (ℓ= 0, corresponding to the average temperature) and
dipole (ℓ= 1, dominated by the doppler shift pattern induced by our motion with
respect to the rest frame of the CMB) are generated by different physics and there-
fore not included in the sums above, which correspond to the primordial perturba-
tion signal.
For a speciﬁc set of cosmological parameters, the power spectrum Cℓ– which
is all we need to provide a complete statistical description of Gaussian cosmolog-
ical perturbations – is completely determined, and can be calculated by solving
the coupled linearized Einstein–Boltzmann equations for the distribution of mat-
ter particles, photons and dark matter in the expanding Universe. This has been
implemented in the popular and publicly available CMBFAST2 and CAMB3 codes.
Ultimately, then, we wish to invert this, and recover these cosmological parameters
from noisy data of the CMB sky.
2 http://www.cmbfast.org
3 http://camb.info

10.2 The CMB as a hierarchical model
231
We can also observe the polarization of the CMB radiation, which can conﬁrm
the cosmological measurements from the temperature ﬁeld and moreover holds
the promise of providing a measurement of an early background of gravitational
radiation – predicted by the theory of cosmic inﬂation (Starobinsky 1979; Guth
1981) – and whose effect can in principle be disentangled from other cosmological
perturbations. In the ﬁrst part of this chapter, we concentrate on the temperature
(intensity) of the CMB, and defer discussion of polarization, which is formally
similar but brings added complications, to Section 10.3.
CMB data is therefore an ideal case study for Bayesian analysis (for early dis-
cussions, see Borrill 1999 and Bond et al. 1999). For the simplest theories there
really is no information in the CMB beyond the isotropic, two-point correlation
function: we can write down the appropriate prior distributions right away. (To put
this in a non-Bayesian way with which many physicists will be comfortable, the
CMB ‘really is’ described by a Gaussian distribution.)
10.2 The CMB as a hierarchical model
Raw CMB data usually come to us in the form of a set of timestreams, the read-
out of multiple detectors observing the sky, always smoothed with some sort of
experimental beam (known as the point-spread function or response function in
other areas of astronomy and physics) and combined with instrumental noise. From
these data, we usually perform the following steps:
(i) estimating a map from the timestream(s);
(ii) estimating a power spectrum from the map; and
(iii) estimating the cosmological parameters from the power spectrum.
(In addition, there may be additional steps to deal with real-world contamina-
tion: we may have to separate out instrumental ‘systematic’ effects and astrophys-
ical foreground emission which we will discuss brieﬂy in Section 10.4.1.) This is
known in statistical parlance as a ‘hierarchical model’, but in fact nearly any sci-
entiﬁc data analysis involves these sorts of steps, although very often the earlier
parts of the process are known as ‘data processing’ and the later as ‘science’ or
‘analysis’ or ‘interpretation’. The relative simplicity of the data and the underlying
theoretical models serve to make the procedure more transparent for the CMB. As
we will see, we really can perform these steps in this order, and ignore the prod-
ucts of the earlier steps: the power spectrum depends on the map and not the raw
timestream; the cosmological parameters depend on the power spectrum and not
the map or the timestream. In the following we will make these steps more explicit
and explore some of the algorithms that have been developed to perform the needed
calculations.

232
Bayesian analysis of cosmic microwave background data
10.2.1 CMB data
A CMB instrument observes the sky temperature Tν(ˆx) at frequency ν and position
ˆx. But in fact any real instrument has a ﬁnite beam and bandwidth as well as noise,
so a more realistic model for the observing process is
d(f, t) =

dν wf(ν)

d2ˆxBt(ˆx) Tν(ˆx) + n(f, t),
(10.6)
where wf(ν) deﬁnes an observing ﬁlter (usually labelled as centred around
frequency channel f) and Bt(ˆx) deﬁnes the experimental beam (usually centred
around some position labelled p = p(t)), and the noise in that channel at that
time is n(f, t). The CMB itself has a ﬂat spectrum in thermodynamic temperature,
Tν = T, so from now on we ignore this effect (although it enters into the disentan-
glement of astrophysical foregrounds from the cosmological signal). In practice,
we assume that Bt(ˆx) is only a function of the distance between the nominal de-
tector pointing at time t and the location ˆx, so the integral turns into an isotropic
convolution, which in turn means that the spherical harmonic components of the
convolved quantity can simply be written as aℓmBℓwhere aℓm are the components
of T(ˆx) and Bℓof the beam function, dependent only upon ℓfor an isotropic beam.
In a more simpliﬁed form, the data can therefore be described by the model
di =

p
AipTp + ni .
(10.7)
Here, di is the data gathered at a particular detector (f) and time (t), together
denoted i = (f, t). The label p denotes pixels on the sky (i.e., speciﬁc locations on
the sky, smoothed by the beam function) and Tp gives the temperature in that pixel.
The matrix Aip gives the action of the detector at a particular time: for a scanning
experiment, Aip = 1 when observing pixel p with detector/time i, and 0 otherwise.
For a differencing experiment such as WMAP or COBE/DMR, Aip will be ±1 for
the two pixels being differenced. In these cases, we take the underlying sky to be
already smoothed by the beam and ignore the effects of pixelization (we address
these shortcomings brieﬂy below in Section 10.2.2). Finally, the noise contribution
is given by ni. We will occasionally also use matrix notation:
d = AT + n .
(10.8)
For now, we will ignore any other contributions to the data, such as foregrounds or
instrumental systematics (see Section 10.4.1).
To apply Bayes’ theorem, we need to determine the appropriate parameteriza-
tion of the quantities on the right-hand side and priors to quantify our knowledge
of them. At the outset, we will take the noise contribution to be described by a

10.2 The CMB as a hierarchical model
233
Gaussian distribution with zero mean and covariance matrix Nij:
⟨ni⟩= 0,
⟨ninj⟩= Nij .
(10.9)
We will discuss how the noise covariance matrix is estimated below.
10.2.2 From detectors to maps
First, we wish to recover the sky signal, Tp, from the timestream(s) di. Given our
Gaussian model for the noise, we can write the likelihood function as
P(di|Tp, Nij) =
1
|2πN|1/2 exp
$
−1
2(d −AT)TN−1(d −AT)
%
.
(10.10)
Up to a constant factor, this can be written by completing the square in the expo-
nential as
P(di|Tp, Nij) ∝
1
|2πCN|1/2 exp
$
−1
2(T −ˆT)TCN −1(T −ˆT)
%
,
(10.11)
with
ˆT = (ATN−1A)−1ATN−1d
(10.12)
and
CN = (ATN−1A)−1.
(10.13)
If we assume a uniform (and therefore un-normalized) prior on Tp, Eq. (10.11)
just gives the posterior distribution for Tp, distributed as a Gaussian with mean
ˆTp and covariance matrix CNpp′. (This is in fact the correct posterior in the limit
of an inﬁnitely dispersed but correctly normalized prior.) This is also exactly the
generalized least-squares (GLS) solution for Tp given data di in the presence of
correlated noise.
We will see, however, that we do not in fact need to use a prior for this step
to be useful; since the data only enter the likelihood through ˆTp, the latter is a
sufﬁcient statistic (along with its covariance matrix) and we can choose to work
with it instead of the raw timestream. The map ˆTp can be seen as simply a form
of data compression, although in practice it is a very useful intermediate step to
visualize the data.
Note one special case of this formula. If the noise is white with variance σ2
w, we
have Ntt′ = σ2
wδtt′ and
ˆTp = 1
Np

t∈p
dt
(white noise)
(10.14)

234
Bayesian analysis of cosmic microwave background data
and
CNpp′ = σ2
w
Np
δpp′,
(10.15)
where the sum is over all time samples for which pixel p is observed and Np is
the number of such samples. Of course, this is just the obvious average and ‘root-
N’ noise reduction we expect. In this trivial but unphysical case, the map can be
calculated in O(Np) steps. (In fact, in this case the scaling is best expressed as
O(Nt), where Nt is the number of time samples.)
The cost of solving the full GLS equations, however, is naively O(N3
p ), owing
to the calculation of the inverse matrix, Eq. (10.13). However, to calculate the map
itself we need actually only solve
C−1
N ˆT = ATN−1d
(10.16)
for ˆT. This can usually be done iteratively with preconditioned conjugate-gradient
methods in which each iteration has cost O(N 2
p ) (and the number of iterations is
always much less than Np). The appropriate preconditioner (deﬁned as something
that can be used as an approximate inverse) is usually very simple: we just use the
inverse of the white-noise case, Eq. (10.15) (e.g., Ashdown et al. 2007a,b, 2009
and references therein).
Note that, irrespective of the preconditioner, the calculation of the full noise
matrix CN does scale as (or nearly) O(N3
p ): each of the O(N2
p ) entries in the
symmetric matrix requires O(Np) operations. For an arbitrary scanning strategy, it
is very difﬁcult to reduce these scalings, but, as we shall see next, there are cases
with sufﬁcient symmetry to simplify the needed calculations considerably.
Destripers
In practice, we often do not have to solve the full GLS equations when we take
the speciﬁcs of the noise structure and the scan strategy into account. We will
consider the Planck satellite as a case study (Planck Consortium 2005). Planck ob-
serves the sky by rotating at a rate of approximately one rpm, with its detectors
pointed at approximately 85◦from the rotation axis. After one hour, that axis it-
self is repointed by about 2.5 arcmin to its normal, so the rotation axes trace out
a great circle over the course of approximately seven months. The noise is well
described by low-frequency correlated noise with power spectrum 1/fα, α ≃1
(‘1/f noise’) along with white noise (constant power spectrum). The frequency
at which the two have equal contributions is known as the knee frequency, fknee.
As long as fknee ∼
< (60 sec)−1, we can model the noise as a constant offset plus a
high-frequency component along each one-minute circle. This enables some sig-
niﬁcant simpliﬁcations (see, for example, Delabrouille 1998; Stompor & White
2004; Ashdown et al. 2007a,b, 2009).

10.2 The CMB as a hierarchical model
235
We denote a single rotation of the satellite as a circle, and the collection of
consecutive circles between repointings as a ring. First, we consider the recovery of
the signal along a single ring. In fact, this is just the standard mapmaking problem,
restricted to the ring. This gives a map ˆTj = T(ˆxj) + nj, where j labels the
phase along the ring and nj gives the noise contribution. However, in the simplest
destriping implementations, we now make a considerable simplifying assumption:
we assume that the noise has the very simple model nj = α + wj, where α is a
constant (for each ring) and wj is an uncorrelated noise contribution, ⟨wjwj′⟩=
δjj′σ2/Nj, where Nj gives the number of hits in phase-bin j. We then combine
the individual maps by a least-squares solution for the α for each ring (in fact,
we marginalize over them). More complicated versions of the destriping algorithm
allow for more complicated low-frequency noise templates as well as correlated
noise at high frequencies.
Although the description here has been in terms of a scanning strategy like that
of Planck, in fact it can be applied to any strategy in which discrete areas of sky are
observed for times that are long compared with the inverse of the knee frequency
(Stompor et al. 2002).
Deconvolution mapmaking
So far, we have made a signiﬁcant approximation in our formalism. In going from
the more complicated model of Eq. (10.6) to the simpliﬁed Eq. (10.7) that is ac-
tually solved by the mapmakers discussed so far, we assume that the underlying
signal we are trying to reconstruct is constant across each pixel, so that all mea-
surements within a given pixel are of the same quantity. We further assume that this
quantity is equal to the isotropic convolution of the beam function with the signal,
represented by Bℓaℓm in harmonic space. In practice, neither of these obtain: every
measurement is of a slightly different quantity. Formally, we could estimate the
underlying signal smoothed with a beam of our choosing at more ﬁnely grained
pixels, but this is only practical for highly symmetric scanning strategies, as dis-
cussed in Armitage and Wandelt (2004) and Armitage-Caplan and Wandelt (2008);
this technique has not yet been implemented for a realistic experimental set-up.
Noise marginalization
We have thus far assumed that we know the noise correlation matrix Nij, or its
Fourier transform, the noise power spectrum N(f), a priori. In practice, this noise
power spectrum is usually estimated using a number of simplifying assumptions.
First, we assume that the noise in a single detector is piecewise stationary. That is,
the noise in different ‘stationary chunks’ is taken to be completely uncorrelated,
and that within such a chunk the noise covariance is taken to be only a function
of the interval between samples. Hence, within a chunk, Nij = N(|ti −tj|). This

236
Bayesian analysis of cosmic microwave background data
further enables us to consider as a more fundamental quantity the noise power
spectrum ˜N(f), the Fourier transform of the noise covariance function, N(t),
which we further approximate by the discrete Fast Fourier Transform (which does
use the unphysical assumption of periodicity). In practice, the noise power spec-
trum is determined by an iterative process (Ferreira & Jaffe 2000) in which esti-
mates of the signal power, described in the following section, are subtracted from
the data timeline, after which the smoothed periodogram of the resulting estimated
noise timeline is used as the power spectrum estimate. In most cases, the noise is
not marginalized over, as would be most appropriately Bayesian, but just ﬁxed at
its maximum after this iteration procedure.
Wiener ﬁlters
If instead of a uniform prior on the signal we assume a Gaussian, we still get a
Gaussian posterior, now depending upon the data, the noise power spectrum, and
the signal prior we have chosen. The obvious choice for a signal prior, expressing
our theoretical idea that the signal is isotropic and distributed as a Gaussian, is
expressed most simply in spherical harmonics as
P(aℓm|Cℓ) =
1
√2πCℓ
exp
2
−1
2
|aℓm|2
Cℓ
3
;
(10.17)
that is, as a zero-mean Gaussian with variance ⟨|aℓm|2⟩= Cℓ. This translates to a
zero-mean Gaussian in pixel space, P(T|Cℓ), with correlation matrix
CSpp′ =

ℓ
2ℓ+ 1
4π
CℓPℓ(cos θpp′)B2
ℓ,
(10.18)
where Pℓ(cos θpp′) are the Legendre polynomials evaluated at the cosine of the
angular distance between pixels p and p′, and Bℓgives the spherical transform of
the beam (a more complicated expression can be written for an asymmetric beam).
We combine this prior with the likelihood of Eq. (10.11) by completing the square
to ﬁnd
P(T|d, N, Cℓ) =
1
|2πCW |1/2 exp
$
−1
2(T −W ˆT)TC−1
W (T −W ˆT)
%
,
(10.19)
where the posterior maximum is at
W ˆT = CS(CS + CN)−1 ˆT
(10.20)
with covariance
CW = CS(CS + CN)−1CN .
(10.21)

10.2 The CMB as a hierarchical model
237
This is the Wiener ﬁlter, and this calculation shows exactly when it is applicable in
a Bayesian setting: if both the signal and noise contributions are assigned Gaussian
distributions. Hence, we do not expect it to recover the signal correctly when the
Gaussian is inappropriate (as for foreground astrophysical emission), and we un-
derstand that the usual Wiener ‘oversmoothing’ occurs due to the relaxation of the
Gaussian posterior to the zero-mean prior in the absence of data.
10.2.3 From maps to power spectra
We have just seen the effect of assuming a Gaussian prior on the posterior dis-
tribution of the sky signal Tp, resulting in the Wiener ﬁlter estimate for the map.
Instead, we can marginalize over the signal and determine the likelihood for param-
eters that enter into the Gaussian prior for the signal: the power spectrum values,
Cℓ. The easiest way to see this is
P(Cℓ| ˆT) =

dT P(Cℓ, T| ˆT) ∝

dT P(T|Cℓ)P(Cℓ)P(ˆT|T, Cℓ), (10.22)
where in the proportionality we use Bayes’ theorem and write the joint prior as
P(T, Cℓ) = P(Cℓ)P(T|Cℓ). Hence, the likelihood is just
P( ˆT|Cℓ) =

dTP(T|Cℓ)P( ˆT|T, Cℓ)
(10.23)
∝
1
|2π(CN + CS)|1/2 exp
$
−1
2
ˆTT(CN + CS)−1 ˆT)
%
,
where we can do the integrals after substituting in the distributions from the pre-
vious section. That is, the map ˆTp can be thought of as a sum of signal and noise
contributions, ˆTp = Tp + np, distributed as a multivariate Gaussian with covari-
ance given by the sum of signal (CS) and noise (CN) covariances, which is exactly
what we would have written down to begin with, of course, but reminds us of the
Bayesian formalism’s self-consistency.
Note that in the simplest case of an all-sky experiment with uniform white
noise of pixel variance σ2, power spectrum estimation is trivial. In this case, we
can perform all calculations in the spherical-harmonic basis, transforming from
ˆTp to ˆaℓm = aℓm + nℓm, where now the noise contribution is uncorrelated with
⟨nℓmnℓ′m′⟩= σ2δℓℓ′δmm′ so
P(ˆaℓm|Cℓ) =

ℓ
1
|2π(Cℓ+ σ2)|1/2 exp
2
−1
2
|ˆaℓm|2
Cℓ+ σ2
3
.
(10.24)

238
Bayesian analysis of cosmic microwave background data
In this case, the properties of the distribution as a function of Cℓare straightfor-
ward; it is maximized at the expected
ˆCℓ= |ˆaℓm|2 −σ2,
where
|ˆaℓm|2 =
1
2ℓ+ 1
ℓ

m=−ℓ
|ˆaℓm|2
(10.25)
gives the average ‘observed’ spectrum at each ℓ.
Whereas the likelihood function for the map, Eq. (10.11), is a Gaussian distri-
bution with an analytic form for the mean and variance, the shape of Eq. (10.23)
as a function of the Cℓcan be quite complicated. In general it does not depend on
a small number of sufﬁcient statistics: the likelihood function should be calculated
in detail. Luckily, a number of approximations have been developed to understand
and explore the structure of the likelihood function.
Of course, we can simply evaluate the likelihood directly as a function of Cℓ(as
in, e.g., the COBE/DMR analysis of Gorski et al. 1996), but from the expressions
above, each evaluation takes O(N 3
p ) operations (the determinant is actually the
most difﬁcult to speed up in this case), and this must be multiplied by the number
of evaluations.
As a ﬁrst step, we can try to ﬁnd the maximum value of the likelihood and some
measurement of the width of the function around the peak. This is usually done by
something like Newton–Raphson iteration to ﬁnd the maximum, accompanied by
the evaluation of the second derivative (Hessian) matrix as a proxy for the width
of the peak. [Sometimes, the Fisher matrix, which is the ensemble average Hessian
for a ﬁxed Cℓ, is used as well, but it is not considerably less expensive to evaluate
(Bond, Jaffe & Knox 1998; Tegmark 1997)]. Unfortunately, each step in the itera-
tion naively scales as O(N3
p ), with considerably difﬁculty of signiﬁcant speed-up,
except in highly symmetric cases.
As an alternative to direct manipulation of the likelihood function, we can
instead ﬁnd a method to draw samples from it. The most obvious Metropolis–
Hastings sampling methods would themselves require an evaluation of the like-
lihood function for each sample, making them prohibitively expensive. Instead,
Gibbs samplers are often used, which go back to the joint distribution P(T, Cℓ| ˆT)
and alternately sample from the two conditional distributions P(T|Cℓ, ˆT) and
P(Cℓ|T, ˆT). In this case, the former distribution is simply that of our Wiener
ﬁlter, Eq. (10.19), and the latter is an inverse Gamma distribution (or an inverse
Wishart distribution if we consider polarization data, to be discussed below)
(Eriksen et al. 2004; Larson et al. 2007). Alternatively, a Hamiltonian Monte Carlo
method can be used to sample directly from the joint distribution P(T, Cℓ| ˆT)
(Taylor, Ashdown & Hobson 2008).

10.2 The CMB as a hierarchical model
239
As is discussed in Part I of this volume, from such samples we can easily con-
struct any desired mean values of our estimated Cℓ. Similarly, although they do not
characterize the full shape of the likelihood, the maximum likelihood provides a
measurement of the spectrum, and the curvature a measurement of the errors about
this peak. However, we emphasize that to use these numbers for any further cal-
culations (as described in the following section), the Bayesian program requires
the full shape of the likelihood function. Until recently, the quality of CMB data
was such that various analytical ansatzen were of sufﬁcient quality to model the
functional shape (e.g., Bond, Jaffe & Knox 2000; Hamimeche & Lewis 2008).
Current (and certainly future) experiments will need more precision. Various pro-
posals have been advanced, ranging from the direct evaluation of the likelihood, to
the ﬁtting of analytical forms to Monte Carlo samples, to the use of a Blackwell–
Rao estimate of the likelihood function itself (e.g., Chu et al. 2005). Unfortunately,
all of these methods have problems, requiring considerable resources especially
when extended to higher multipoles.
Indeed, at high ℓall of these methods are computationally prohibitive to even
calculate the means and/or maxima, and we are forced to rely on frequentist or
hybrid methods, such as those based on the MASTER formalism (Hivon et al. 2002).
MASTER calculates so-called ‘pseudo-aℓm’ components using a brute-force spheri-
cal apodization, and then calculates the linear relationship between the underlying
power spectrum and the ensemble-average ‘pseudo-Cℓ’, deﬁned in the naive way
as the average of the squared pseudo-aℓm at each ℓ. FASTER (Myers et al. 2003) is
a particularly interesting wrinkle on this method, as it uses the MASTER calculation
to then perform a hybrid Bayesian calculation, assuming that the conditional like-
lihoods at different ℓare independent. In a Bayesian/maximum-entropy sense, this
approximation is equivalent to removing information in a controlled way, but not
introducing any new assumptions.
For the simple case of an all-sky experiment with uniform white noise between
pixels (i.e., white noise in the timestream with the same number of observations
per pixel), the frequentist methods give answers equivalent to the Bayesian tech-
nique (Eq. 10.25): the frequentist mean gives the maximum likelihood and the vari-
ance gives the curvature. We can further reproduce the likelihood shape by using
descriptions such as those discussed above (Bond, Jaffe & Knox 2000; Hamimeche
& Lewis 2008). However, it must be stressed that for more complicated (i.e. realis-
tic) experiments, there is no reason to expect this Bayesian/frequentist correspon-
dence to hold in detail.
10.2.4 From spectra to cosmological parameters
We now wish to measure the cosmological parameters, such as the Hubble constant
H0, the matter density Ωm, the primordial spectral tilt of scalar perturbations ns,

240
Bayesian analysis of cosmic microwave background data
etc., which we will collectively label θ. For standard cosmological models, the
parameters uniquely determine the power spectrum, Cℓ= Cℓ(θ), so the likelihood
function can be written
P(θ|d) =

dCℓP(θ, Cℓ|d) ∝

dCℓP(Cℓ|θ)P(θ)P(d|Cℓ)
(10.26)
=

dCℓδ[Cℓ−Cℓ(θ)]P(θ)P(d|Cℓ)
(10.27)
= P(θ)P[d|Cℓ= Cℓ(θ)] .
(10.28)
Again, we do not need to assume a separate prior on the power spectrum. Unlike
the previous step, however, we see that the likelihood function does not have a
simple form as a function of Cℓ, and cannot trivially be reduced to a function of
a small number of sufﬁcient statistics, such as the maximum and curvature of the
Cℓlikelihood. Hence, in order to measure the cosmological parameters we need
to do one of the following. First, we can calculate the likelihood function directly,
but as we have noted this scales as O(N3
p ) and so is prohibitively expensive in
many cases. Second, we can try to model the function with some small number of
parameters in addition to the aforementioned maximum and curvature. Finally, we
can instead draw samples from this distribution which can be used in a variety of
ways. This latter method, using Markov chain Monte Carlo and related techniques
such as nested sampling, is described in Part I of this volume.
10.3 Polarization
Until this point, we have assumed that our experiment measures only the tempera-
ture (equivalently, the intensity) of the CMB radiation. For much of the last decade,
however, attention has been concentrated upon the measurement of the polariza-
tion of the CMB, and the majority of recent experiments are polarimeters as well
as thermometers. Polarization is produced as the Universe transitions from being
ionized to neutral, at which point individual electrons will emit polarized radiation
aligned with any surrounding quadrupole radiation ﬁeld. This polarized CMB will
probe the scalar mode of the metric – the gravitational potential – which dominates
the temperature, and also the tensor mode, corresponding to gravitational waves.
These tensor modes are thought to be produced by (and possibly only by) metric
perturbations from an early epoch of inﬂation. Moreover, they can in principle be
separated from the scalar contribution due to their different geometrical proper-
ties. Polarization is represented by a headless two-dimensional vector ﬁeld on the
spherical sky. The simplest representation of polarization on the sky is in terms
of its components (the pixelized values of the Stokes parameters Qp and Up), but
these depend on the coordinate system used.

10.3 Polarization
241
For such experiments, the data become somewhat more complicated:
di =

p
Aip [Tp + Qp cos(2αi) + Up sin(2αi)] + ni,
(10.29)
where now αi gives the polarization angle of the detector, and now we wish to
reconstruct not just the temperature Yi but the Qi and Ui components as well (ref-
erenced to the direction of the North Galactic Pole as in the WMAP analysis of
Page et al. (2007) and elsewhere). Formally, this equation still has the same linear
form, d = BS + n, where now S = (T, Q, U), and the operator B is considerably
more complicated, containing the angle-dependent terms from Eq. (10.29). Note
that data from a single time sample is used to reconstruct all of the three compo-
nents of S seen at that time. This has several repercussions. First, a larger number
of observations of a given pixel are required to reliably reconstruct each compo-
nent. Second, this inevitably induces noise correlations between the T, Q and U
components at the pixel which must be taken into account.
The Q and U components represent a headless vector ﬁeld on the sphere, which
itself can be decomposed into so-called ‘E’ and ‘B’ components (as they are de-
noted in the CMB community). By analogy with electromagnetism, the E com-
ponent is curl-free, and the B component divergence-free (for suitable spherical
deﬁnitions of curl and divergence). Speciﬁcally, a polarization ﬁeld can be writ-
ten by the extension of Eq. (10.2) to a polarization ﬁeld as, in the notation of
Zaldarriaga and Seljak (1997),
Qp ± iUp =

ℓm
±2aℓm ±2Yℓm(ˆxp),
(10.30)
where ±2Yℓm(ˆxp) are the spin-2 spherical harmonics (related by derivatives to the
usual Yℓm). We can then deﬁne the scalar (pseudo-scalar) ﬁeld corresponding to E
(B) as
aE
ℓm = −(+2aℓm + −2aℓm)
and
aB
ℓm = i(+2aℓm −−2aℓm)
(10.31)
and then we can ﬁnally deﬁne the power spectra in analogy to Eq. (10.3):
⟨aA
ℓmaA′
ℓ′m′⟩= CAA′
ℓ
δℓℓ′δmm′ ,
(10.32)
where (A, A′) ∈(T, E, B) (in most theories CEB
ℓ
= CTB
ℓ
≡0). Equivalently,
the Q and U maps themselves have correlation functions that are linear combina-
tions of these spectra. (Kamionkowski, Kosowsky & Stebbins 1997; Zaldarriaga &
Seljak 1997; Tegmark & de Oliveira-Costa 2001).
Power spectra and E/B separation
For a full-sky noise-free experiment this could in principle be inverted to recover
maps of the polarization components corresponding to the E and B ﬁelds, but

242
Bayesian analysis of cosmic microwave background data
boundary effects in a realistic experiment make this impossible to do unambigu-
ously. In fact, the fully Bayesian technique does not involve any E/B reconstruc-
tion or separation, but just starts from the Q and U maps and calculates the distri-
bution of power spectra given these maps by techniques completely equivalent to
those given in 10.2.3. This is straightforward but computationally expensive, and
of course E and B maps are useful diagnostic tools which are not natural products
of this technique (although the equivalent of a Wiener ﬁlter can also be applied to
effectively separate the components).
Recently (Smith 2006; Smith & Zaldarriaga 2007) there has been an effort to
directly estimate the scalar and pseudo-scalar components on the sky (rather than in
harmonic space). This is a somewhat more local operation than the reconstruction
of the E and B polarization ﬁelds, but at the expense of requiring a numerical
derivative (always dangerous with noisy data) or signiﬁcant apodization in order to
use harmonic techniques.
10.4 Complications
10.4.1 Foregrounds and systematics
So far, we have assumed an idealized CMB experiment uncontaminated by astro-
physical foregrounds or instrumental effects (beyond the instrumental noise whose
behaviour is fully speciﬁed by the Nij correlation matrix). Instrumental effects are
often correlated with other pieces of ‘housekeeping’ data, such as the temperature
of the telescope, or the rotation angle of the polarizing hardware. These instru-
mental effects can often be added into our mapmaking procedure as templates in
a similar manner to the offsets in the destriping methods discussed above (e.g.,
Stompor et al. 2002; Johnson et al. 2007).
Astrophysical foregrounds are ﬁxed on the sky, but correlated between frequency
channels. There is rarely enough information to unambiguously reconstruct the
foregrounds and different methods have been proposed to take into account differ-
ent kinds and amounts of prior information. In some cases, template methods can
be used for a fully Bayesian treatment, but other ‘non-linear’ methods have been
developed that ﬁt less easily into the Bayesian paradigm, which requires knowl-
edge of the full posterior distribution of the foreground-cleaned map, and in prac-
tice requires that this distribution be a multivariate Gaussian with a simple noise
correlation structure (Hobson, Ashdown & Stolyarov 2009).
10.4.2 Non-Gaussianity
However, there is one major complication that has not yet proven amenable to
Bayesian analysis: non-Gaussianity. In practice, we do expect to see small

10.5 Conclusions
243
departures from Gaussianity, parameterized by non-zero higher correlation func-
tions, the multivariate equivalent of the skewness, kurtosis, etc. A Gaussian distri-
bution is the unambiguous distribution to assign when only the two-point function
is known, but no such assignment exists for the more general case. (Indeed, the
Gaussian distribution arises from numerous lines of argument: it is the only distri-
bution with zero higher-order correlations; it is the maximum-entropy distribution
for a known two-point function; it arises naturally via the central limit theorem in
many realistic situations.) Non-Gaussian distributions must be assigned in cases
based on speciﬁc physical or mathematical models. But such models do not in
general exist; instead only speciﬁc moments of the required distribution can be
calculated.
All of this means that, although some non-Gaussian distributions have been pro-
posed (e.g., Contaldi et al. 2000; Rocha et al. 2001), they have not been applied to
CMB data in practice; frequentist techniques have dominated the analysis so far.
10.5 Conclusions
We have seen that the problem of CMB data analysis is an ideal case study for
Bayesian techniques in cosmology. For the standard model of small perturbations
and Gaussian initial conditions, we can calculate in sequence the map from a set of
timestreams, the power spectrum from the map, and the cosmological parameters
from the spectra. At least in some cases, these techniques can cope with contami-
nation from instrumental and astrophysical sources. Some of these steps are com-
putationally expensive, and a fully Bayesian solution has not been implemented for
datasets as large as that expected from the Planck satellite.
References
Armitage, C. and Wandelt, B. (2004). Phys. Rev. D, 70, 123007.
Armitage-Caplan, C. and Wandelt, B. (2008). arXiv:0807.4179.
Ashdown, M. et al. (2007a). Astron. Astrophys., 467, 761.
Ashdown, M. et al. (2007b). Astron. Astrophys., 471, 361.
Ashdown, M. et al. (2009). Astron. Astrophys., 493, 753.
Bond, J. R., Crittenden, R., Jaffe, A. H. and Knox, L. E. (1999). Comput. Sci. Eng., 1:2, 21.
Bond, J. R., Jaffe, A. H. and Knox, L. E. (1998). Phys. Rev. D, 57, 2117.
Bond, J. R., Jaffe, A. H. and Knox, L. E. (2000). Astrophys. J., 533, 19.
Borrill, J. (1999). arXiv:astro-ph/9911389.
Chu, M. et al. (2005). Phys. Rev. D, 71, 103002.
Contaldi, C. R., Ferreira, P. G., Magueijo, J. and Gorski, K. M. (2000). Astrophys. J.,
534, 25.
Delabrouille, J., Gorski, K. M. and Hivon, E. (1998). Mon. Not. Roy. Astron. Soc.,
298, 445.
Eriksen, E. K. et al. (2004). Astrophys. J. Supp., 155, 227.

244
Bayesian analysis of cosmic microwave background data
Ferreira, P. G. and Jaffe, A. H. (2000). Mon. Not. Roy. Astron. Soc., 312, 89.
Fixsen, D. et al. (1996). Astrophys. J., 473, 576.
Gorski, K. M. et al. (1996). Astrophys. J. Lett., 464, L11.
Guth, A. H. (1981). Phys. Rev. D, 23, 347.
Hamimeche, S. and Lewis, A. (2008). Phys. Rev. D, 77, 103013.
Hivon, E. et al. (2002). Astrophys. J., 567, 2.
Hobson, M. P., Ashdown, M. A. J. and Stolyarov, V. (2010). Chapter 6 in this volume.
Johnson, B. et al. (2007). Astrophys. J., 665, 42.
Kamionkowski, M., Kosowsky, A. and Stebbins, A. (1997). Phys. Rev. D, 55, 7368.
Larson, D. et al. (2007). Astrophys. J., 656, 653.
Myers, S. T. et al. (2003). Astrophys. J., 591, 575.
Page, L. et al. (2007). Astrophys. J. Supp., 170, 335.
Planck Consortium (2005). Planck: The Scientiﬁc Programme. ESA-SCI(2005)1.
Rocha, G. et al. (2001). Phys. Rev. D, 64, 63512.
Smith, K. (2006). New Astron. Rev., 50, 1025.
Smith, K. and Zaldarriaga, M. (2007). Phys. Rev. D, 76, 43001.
Starobinsky, A. (1979). JETP Lett., 30, 682.
Stompor, R. et al. (2002). Phys. Rev. D, 65, 022003.
Stompor, R. and White, M. (2004). Astron. Astrophys., 419, 783.
Taylor, J. F., Ashdown, M. A. J. and Hobson, M. P. (2008). Mon. Not. Roy. Astron. Soc.,
389, 1284.
Tegmark, M. (1997). Phys. Rev. D, 55, 5895.
Tegmark, M. and de Oliveira-Costa, A. (2001). Phys. Rev. D, 64, 063001.
Zaldarriaga, M. and Seljak, U. (1997). Phys. Rev. D, 55, 1830.

11
Bayesian multilevel modelling of cosmological
populations
Thomas J. Loredo and Martin A. Hendry
11.1 Introduction
Surveying the Universe is the ultimate remote sensing problem. Inferring the
intrinsic properties of the galaxy population, via analysis of survey-generated cat-
alogues, is a major challenge for twenty-ﬁrst century cosmology, but this chal-
lenge must be met without any prospect of measuring these properties in situ. Thus,
for example, our knowledge of the intrinsic luminosity and spatial distribution of
galaxies is ﬁltered by imperfect distance information and by observational selec-
tion effects, issues which have come to be known generically in the literature as
‘Malmquist bias’.1 Figure 11.1 shows schematically how such effects may distort
our inferences about the underlying population since, in general, these must be
derived from a noisy, sparse and truncated sample of galaxies.
There is a long (and mostly honourable!) tradition in the astronomical literature
of attempts to cast such remote surveying problems within a rigorous statistical
framework. Indeed, it is interesting to note that seminal examples from the early
twentieth century (Eddington 1913, 1940; Malmquist 1920, 1922) display, at least
with hindsight, hints of a Bayesian formulation long before the recent renaissance
of Bayesian methods in astronomy. Unfortunately, space does not permit us to
review in detail that early literature, nor many of the more recent papers which
evolved from it. A more thorough discussion of the literature on statistical analysis
of survey data can be found in, e.g., Hendry and Simmons (1995), Strauss and
Willick (1995), Teerikorpi (1997) and Loredo (2007).
1 Although ‘Malmquist’ is the most prevalent appellation, the literature also uses other terms – including
‘Eddington–Malmquist’ and ‘Lutz–Kelker’ – to denote biases arising in astronomical surveys from distance
indicator scatter and observational selection. There is also an unfortunate history in the cosmology literature
of the same term being used to mean substantially different things by different authors. For a more detailed
account of the meaning, use and abuse of bias terminology see, e.g., Hendry and Simmons (1995), Strauss
and Willick (1995) and Teerikorpi (1997).
245

246
Bayesian multilevel modelling of cosmological populations
Observables
Measurements
Catalogue
Population, q
Selection
Observation
Mapping, φ
= point
= likelihood
L
r
F
z
Indicator scatter and
transformation bias
Measurement
error
Truncation and
censoring
Fig. 11.1. Schematic depiction of the survey process. A population with a distribution of
source parameters, S (e.g., luminosity and distance), implies, via a mapping φ, a distribu-
tion of observables, O (e.g., ﬂux and redshift). Observation introduces measurement error;
selection criteria truncate or censor the catalogued population.
The analysis of survey catalogues can have a number of scientiﬁc goals. For
example, the objective may be to compare the underlying galaxy luminosity dis-
tribution with the predictions of different galaxy formation models. In this case
the galaxy distances (which we must infer as an intermediate step towards estimat-
ing their luminosities) are, in Bayesian parlance, nuisance parameters. On the other
hand, the goal may be to infer the distances of the surveyed galaxies, in order to test
models of galaxy clustering and/or constrain parameters of the underlying cosmol-
ogy. In this case it is the inferred galaxy luminosities which may be thought of as
nuisance parameters. The related inference problem for galaxy ﬂuxes of individual
objects is discussed by Mortlock in Chapter 8.
In the literature of the past 20 years the second case, that of inferring galaxy
distances, has proven to be fertile territory for the development and application of
Bayesian methods. This is particularly true with regard to redshift-independent dis-
tance indicators, i.e., indicators whose behaviour is independent of the underlying
cosmological model (and which may thus be straightforwardly used to constrain
parameters of that model). A likely reason for this is that redshift-independent dis-
tance indicators suffer from large intrinsic scatter, with distance uncertainties to
individual galaxies typically in the range 5% to 30%. A consequence is that care
must be taken when incorporating prior distance information, since in this setting
ﬁnal inferences can be signiﬁcantly inﬂuenced by the prior, a point we discuss
further in Section 11.2.
Another hallmark of the literature on extracting information from galaxy surveys
has been the recognition by some authors (e.g., Hendry and Simmons 1995; Loredo
2007) that the task of identifying ‘optimal’ (e.g., in the sense of unbiased and/or
minimum mean square error) estimators of galaxy distance and luminosity will
generally not have a unique solution, but will depend on the context in which the
inferred galaxy distances or luminosities are to be used.

11.2 Galaxy distance indicators
247
In Section 11.3 we discuss how Bayesian multilevel models provide a natural
and powerful framework in which to formulate and implement optimal analyses
of surveys, incorporating prior information and carefully accounting for selection
effects and source uncertainties. To motivate this approach, and to establish con-
text, in Section 11.2 we survey key Bayesian elements of current ‘state of the art’
methodology for analysis of survey data, focussing on use of redshift-independent
distance indicators, and highlighting examples which have previously hinted at a
multilevel approach.2
11.2 Galaxy distance indicators
While future astrometric space missions (e.g., GAIA and SIM) offer some prospect
of applying trigonometric methods over cosmological scales, in large part the mea-
surement of galaxy distances relies on more astrophysical, and therefore less pre-
cise, methods. Perhaps the most obvious of these methods is the cosmological red-
shift. To derive a distance measure from it requires the use of a cosmological model.
But if one’s goal is to use galaxy distance estimates to probe the parameters of the
cosmological model, the safest path is to identify galaxy distance indicators whose
properties are independent of redshift.
Almost all redshift-independent ‘distance’ indicators are really luminosity or
size indicators: one uses the indicator to estimate the intrinsic luminosity or size
of a galaxy (or some object therein); combined with a measurement of its apparent
brightness or apparent size, one may estimate distance via the inverse-square law
or the angle–distance relation (or their cosmological generalizations). For simplic-
ity, we consider here only the case of luminosity indicators, although very sim-
ilar statistical considerations apply to other types of indicator. We can estimate
the luminosity either by assuming a constant, ﬁducial value (the so-called ‘stan-
dard candle’ assumption) or, better, by exploiting correlations between luminos-
ity and some other intrinsic, but directly measurable, physical characteristic(s) of
the source. (The latter approach is often referred to as a ‘standardizable candle’.)
Examples of these correlations include: the Tully–Fisher relation for spiral galax-
ies; the period–luminosity relation for Cepheid variables and the luminosity–light
curve shape relation for type-Ia supernovae. The correlations may be motivated
by theory, but they must be calibrated empirically, e.g., using nearby galaxies at
known distances (and hence of known luminosity). Their scatter, reﬂecting the in-
trinsic spread in luminosity of the sources, renders distance indicators susceptible
2 The state of the art here is a formalism developed more than a decade ago, suggesting that this has been
a neglected research ﬁeld for some years. In large part this has been due to limitations on the quality and
quantity of indicator-based survey data available; this is a situation which is improving dramatically, with the
latest galaxy surveys such as 6dF and SDSS, and should continue to do so.

248
Bayesian multilevel modelling of cosmological populations
to observational selection biases since one cannot observe arbitrarily faint sources
to arbitrary distances.
Since the late 1980s, several authors have investigated the statistical properties
of redshift-independent galaxy distance indicators, with the goal of placing their
use in cosmology on a more rigorous statistical footing. A signiﬁcant step forward
in this regard came with the work of Willick (1994), which brought much needed
clarity to the discussion by explicitly making a distinction between the tasks of
calibrating a distance indicator and applying it to a galaxy survey to infer distance
information. We now brieﬂy summarize the formalism presented in Willick (1994)
and adopted in subsequent papers.
11.2.1 The calibration problem
Our starting point is the joint probability distribution for a single galaxy’s distance,
r, apparent magnitude, m, and some third observable correlated with luminosity
which, following Willick, we denote by η and refer to as the ‘line width’ parameter.
As a concrete example, consider the Tully–Fisher relation, for which we expect the
intrinsic relation between absolute magnitude and η to be linear, i.e., M = aη + b,
where the coefﬁcients a and b must be calibrated empirically. As noted earlier,
an analysis may have various goals: a and b may simply be nuisance parameters,
necessary to estimate galaxy distances; alternatively, they may be important target
parameters in their own right.
Thus, for the joint distribution describing the properties of galaxies within a
particular survey catalogue, we have
p(r, m, η) ∝r2n(r) S(m, η) ψ(m|η) φ(η),
(11.1)
where n(r) and φ(η) denote the marginal distribution of distance and line width re-
spectively for the galaxy population, ψ(m|η) denotes the conditional distribution of
apparent magnitude at a given line width (which depends on unknown parameters,
e.g., a and b), and S(m, η) denotes the observational selection effects (assumed
here, for simplicity, not to depend on distance or direction).
Consider ﬁrst the calibration of the distance indicator, which might reasonably
be carried out, e.g., using a galaxy cluster, so the set of calibrators are all at the
same distance.3 In this case it is natural to work with the conditional distribution
of m at given η and r (i.e., the result from using Eq. (11.1) as a prior in Bayes’
theorem, with a ‘likelihood’ corresponding to precise measurement of η and r).
Then the indicator coefﬁcients a and b can be interpreted as the slope and zero-
point of a linear regression of absolute magnitude on η; this case is known as the
3 Willick also considers the calibrators at a range of true distances; this case lends itself well to a Bayesian
multilevel formulation, as we discuss in Section 11.3.

11.2 Galaxy distance indicators
249
‘direct’ indicator relation. Thus
P(m|η, r) =
S(m, η) ψ(m|η)
	 ∞
−∞S(m, η) ψ(m|η)dm.
(11.2)
Notice that, as expected, the marginal distributions of distance and line width drop
out. The presence of the observational selection effects will bias the determination
of the indicator coefﬁcients a and b obtained via simple linear regression; however,
Willick (1994) proposed an iterative scheme to overcome this problem and showed
that it works well for realistic mock galaxy data.
A popular variant on the above approach is to use the so-called ‘inverse’ relation,
i.e., (in Willick’s notation) η0(M) = a′M + b′, where the inverse coefﬁcients a′
and b′ again must be determined empirically (and again may be regarded either as
nuisance parameters or target parameters). This relation is most directly expressed
by the conditional distribution of η at a given r and m (corresponding to given M,
since we are assuming all the calibrators lie at the same distance), namely
p(η|m, r) =
S(m, η) Ψ(η|m)
	 ∞
−∞S(m, η) ψ(η|m)dη,
(11.3)
where Ψ denotes the conditional distribution of line width at given apparent magni-
tude. In this case, explicit dependence on the galaxy luminosity function drops out
of our expression. Moreover, one can see that if the selection effects depend only
on apparent magnitude and not on line width, then a straightforward linear regres-
sion of η on M will yield unbiased estimates of the indicator coefﬁcients a′ and
b′. This appealing property of the inverse indicator relation had been recognized in
principle much earlier by Schechter (1980) and was also placed on a rigorous sta-
tistical footing around the same time as Willick by Hendry and Simmons (1994).
However, the successful calibration of a galaxy distance indicator is only the ﬁrst
part of the story.
11.2.2 The estimation problem
Suppose one has used the relations above to calibrate a distance indicator accu-
rately and precisely (e.g., a and b are now precisely known). Now we seek to use
the indicator in settings where there is no direct measurement of r; we must in-
fer r from measurements of m and η. We can calculate a predicted galaxy dis-
tance, d, in the obvious way by combining the observed apparent magnitude of the
galaxy with its estimated absolute magnitude inferred (via our indicator relation)
from its observed line width. Moreover, since d = d(m, η), it is straightforward
to compute the joint distribution, p(r, d), of true and estimated galaxy distance,
and further to determine the conditional distribution of r given d. For the direct

250
Bayesian multilevel modelling of cosmological populations
indicator, we obtain
p(r|d) =
r2n(r) exp

−[ln r/d]2
2Δ2

	 ∞
0 r2n(r) exp

−[ln r/d]2
2Δ2

dr
,
(11.4)
where Δ is a constant, proportional to the (here assumed Gaussian) scatter in the
direct indicator relation, i.e., the dispersion of the conditional distribution of abso-
lute magnitude at given line width. For the inverse indicator, on the other hand, we
obtain
p(r|d) =
r2n(r)s(r) exp

−[ln r/d]2
2Δ2

	 ∞
0 r2n(r)s(r) exp

−[ln r/d]2
2Δ2

dr
,
(11.5)
where s(r) is an integral over the galaxy luminosity function weighted by the se-
lection effects, and expresses the probability that a galaxy at true distance r would
be observable in the survey. This term is often referred to as the selection function
for r.
The interpretation of Eqs. (11.4) and (11.5) within the framework of Bayesian
inference is clear. We can think of p(r|d) as representing the posterior distribution
of true distance r, given some observed data d (i.e., the estimated distance, from
our indicator). Moreover the difference between the two expressions can then be
interpreted in terms of the adoption of different prior information for r: for the
direct indicator, the prior information is the true distance distribution n(r), while
for the inverse relation the prior is the product of n(r) and the selection function.
The classical Malmquist bias is manifest when we take the conditional expecta-
tion of r given d, using Eqs. (11.4) and (11.5). For both direct and inverse indicators
we ﬁnd that in general E(r|d) ̸= d. However, we can correct our ‘raw’ distance
indicator d, deﬁning dcorr which satisﬁes
E(r|dcorr) = dcorr,
(11.6)
with the correction term referred to as a ‘Malmquist correction’. Note, however,
that the Malmquist correction depends explicitly on the true distance distribu-
tion n(r), which in general will be unknown. Lynden-Bell et al. (1988) computed
homogeneous Malmquist corrections, assuming that the underlying spatial distri-
bution of galaxies is uniform, in which case
E(r|d) = d exp
7
2Δ2

≃d

1 + 7
2Δ2

≡dcorr.
(11.7)
We should not be surprised that the correction is always positive in this case; since
we are assuming homogeneity we are saying that the distance indicator scatter
is more likely to scatter galaxies downwards from greater true distances, simply

11.2 Galaxy distance indicators
251
because there are more galaxies at larger r due to the rapid growth of the volume
element with r. Note, however, that for the inverse indicator the assumption of a
uniform prior is not appropriate: even if n(r) were constant, the selection function
s(r) clearly will not be.
The more realistic case is, of course, where the intrinsic distribution of distance
is not uniform. In this case the adoption of a suitable prior for n(r) leads to a so-
called inhomogeneous Malmquist correction. For the direct indicator, the source of
the prior information could be, for example, the underlying density ﬁeld of galax-
ies reconstructed from an external source, e.g., an all-sky redshift survey (Hudson
1994; Strauss & Willick 1995; Freudling et al. 1995; Erdogdu et al. 2006). The
Malmquist corrections will only be valid in this case, however, provided that the
external galaxy survey traces the same underlying population as the galaxies to
which the distance indicator is being applied.
In an important paper, Landy and Szalay (1992) proposed an interesting alter-
native approach, whereby the marginal distribution of raw distances might provide
a suitable estimate of the prior true distance distribution. Crucially, this method
should not be applied using the direct indicator since the marginal distribution of
raw distances provides a poor estimate of n(r). On the other hand, it does provide
a reasonable proxy for the distribution of true distances for ‘observable’ galaxies;
i.e., the product of n(r) and s(r). Thus, it is probably well suited to use with the
inverse indicator.
Landy and Szalay’s approach, although not rigorously derived, has several at-
tractive features. It offers a method of deﬁning inhomogeneous Malmquist correc-
tions that adapts to spatial inhomogeneity, without requiring external assumptions
or prior information about n(r) from other galaxy surveys. Indeed, it appears to
be an approximation to a hierarchical Bayesian procedure, as we discuss further in
Section 11.3.
11.2.3 Applications of galaxy distance indicators
Why might we regard Malmquist-corrected distance indicators, which satisfy
Eq. (11.6), as optimal estimators in the ﬁrst place? The answer lies largely in the
uses to which they have been put. Since the late 1980s, redshift-independent dis-
tance indicators have often been used to measure galaxy peculiar velocities, namely
the motions, over and above the Hubble expansion, induced by the net gravitational
attraction of the matter distribution around them. Methods of analyzing peculiar
velocities generally involve ﬁrst binning and grouping galaxies together based on
their estimated distance. By requiring that on average the true distance of each
galaxy be equal to its estimated distance, one aims to ensure that on average the
correct radial peculiar velocity will be ascribed to each galaxy’s apparent position.

252
Bayesian multilevel modelling of cosmological populations
In the 1990s, astronomers developed a number of sophisticated methods to
compare observed and predicted galaxy peculiar velocities, the latter the result
of reconstructing the density and peculiar velocity ﬁeld from position and red-
shift data from an all-sky redshift survey. This reconstruction requires a model for
galaxy biasing; i.e., a description of how the distribution of luminous galaxies and
dark matter are related. By comparing observed and predicted peculiar velocities
one can constrain parameters of the galaxy biasing model.
From a Bayesian perspective, probably the most notable of these comparison
methods was VELMOD (Willick & Strauss 1998). This assumed a simple linear
relation between the galaxy and matter density ﬁelds and computed a posterior
for the linear bias parameter, marginalized over the nuisance parameters of the
distance indicator relation. In its explicit modelling of the galaxy distance uncer-
tainties, en route to estimating the linear bias parameter, VELMOD shares features
with the multilevel Bayesian model approach which we now describe.
11.3 Multilevel models
The issues motivating the astronomical developments just surveyed are hardly
unique to astronomy. Statisticians have addressed similar issues in applications
spanning many disciplines. Although none of the resulting methods is an ‘exact ﬁt’
to an astronomical survey problem, the body of literature offers numerous insights
that should inspire signiﬁcant advances in Bayesian methodology for astronomical
surveys.
A recurring theme of much of the relevant literature is the use of multilevel mod-
els (MLMs), a relatively recent term for a rich framework that underlies several
important statistical innovations of the latter twentieth century, including empir-
ical and hierarchical Bayes methods, random effects and latent variable models,
shrinkage estimation, and ridge regression. MLMs start with a ﬁrst-level proba-
bility model for the measurements of parameters for each of many objects (e.g.,
sources in a survey). The second level assigns a shared prior distribution to the
ﬁrst-level parameters (e.g., a population-level distribution for source properties);
this distribution may itself have unknown parameters, dubbed hyperparameters.
The second level leads to probabilistic dependence among the ﬁrst-level param-
eters that implements a pooling of information that can improve the accuracy of
inferences; one says the estimates ‘borrow strength’ from each other. Other levels
may be added, e.g., to describe relationships between groups of objects.
We here focus on Bayesian treatment of MLMs, though multilevel modelling is
an area where there has been signiﬁcant cross-fertilization between Bayesian and
frequentist approaches. We begin by describing a very simple MLM – the normal–
normal MLM – highlighting a feature of MLM point estimates, shrinkage, that has

11.3 Multilevel models
253
connections to classic astronomical approaches for correcting for survey biases.
We use this as a stepping stone to a more thoroughgoing Bayesian approach that
moves beyond point estimates and corrections. To date, this approach has been
implemented only in fairly simple astronomical settings; we end by highlighting
directions for future research.
11.3.1 Adjusting source estimates: shrinkage
Suppose we have survey data for a population of sources that we will model as
having a log-normal luminosity function, so the population distribution of source
absolute magnitudes, M, is a normal distribution with location M0 and scale (stan-
dard deviation) τ – these are the hyperparameters. As a simple starting point, we
will suppose τ is known (τ = 0.5 mag, say), and we denote the population distri-
bution by f(M|M0). For now, we also assume there are no selection effects. At the
population level, our goal is to infer M0.
The survey produces data, Di, for each source; we will suppose these lead to in-
dependent, Gaussian-shaped, likelihood functions for each source’s unknown true
absolute magnitude Mi, with maximum likelihood estimates (MLEs) ˆ
Mi and un-
certainties (standard deviations) σi. We denote these source likelihoods as
ℓi(Mi) ≡p(Di|Mi) = N(Mi| ˆ
Mi, σ2
i ), with N(·|μ, σ2) the normal distribution
with mean μ and variance σ2. The Mi are the ﬁrst-level parameters. The survey
catalogue consists of a table of the ˆ
Mi estimates and their uncertainties. For sim-
plicity, we assume equal uncertainties, σi = σ = 0.3 mag.
Let D ≡{Di} and M ≡{Mi} denote the collections of data and source pa-
rameters. The likelihood function is L(M0, M) ≡p(D|M0, M) = p(D|M) =
+
i ℓi(Mi); that is, it does not depend on M0 because the probabilities for the
source data, Di, are fully determined (and independent) if M is speciﬁed. Thus the
joint MLEs for the source parameters are just the independent MLEs: ˆM = { ˆ
Mi}.
But in a Bayesian calculation, estimates are determined by the posterior, not the
likelihood. If M0 is known, the joint posterior for the source parameters, condi-
tional on M0, is given by
π(M|D, M0) ∝

i
f(Mi)ℓi(Mi),
(11.8)
with the population density appearing as a prior factor for each source. Point esti-
mates may be found from this conditional posterior, e.g., by ﬁnding the mode or
posterior mean for M. It is important to note that if we increase the amount of sur-
vey data by increasing N, such Bayesian estimates will not converge to the MLEs,
because additional prior factors enter with each new source. That is, we are not
in the common, simpler setting of a ﬁxed number of parameters, with additional

254
Bayesian multilevel modelling of cosmological populations
data providing likelihood factors that eventually overwhelm a single prior factor.
The presence of source uncertainties implies that each new source adds a new pa-
rameter, so differences between Bayesian and likelihood estimates persist. This
is evident in the Malmquist corrections described above. The only way for these
Bayesian estimates to converge to MLEs is to add follow-up data for each source,
i.e., to make all of the ℓi(Mi) functions narrower.
Considering the population parameter M0 to be unknown, with prior distribution
π(M0), the joint posterior for all the unknowns is given by
π(M0, M|D) ∝π(M0)

i
f(Mi|M0)ℓi(Mi).
(11.9)
If our goal is to estimate the source parameters, we account for M0 uncertainty by
marginalizing over M0, giving the source parameter marginal posterior, π(M|D) =
	
dM0 π(M0, M|D). If instead our goal is to infer the population density, we cal-
culate the marginal posterior for M0, π(M0|D) =
	
dM π(M0, M|D). In this
simple normal–normal MLM, these integrals can be done analytically (we adopt a
ﬂat prior for M0, the hyperprior).
The top panel of Figure 11.2 shows a population distribution with M0 = −21;
the circles on the line just below it indicate the true Mi values of a sample of
N = 30 sources. Below that, diamonds indicate the MLEs for the sources, ˆ
Mi,
for one realization of measurement error; a line segment connects each estimate
with its true parent value. The MLEs are intuitively appealing estimates; they also
have several appealing frequentist properties, considering an ensemble of many re-
alizations of the measurement errors. For example, normal MLEs are unbiased (in
fact, they are the best linear unbiased estimators), and they are invariant to trans-
lation in M. But considered as an ensemble, they are overdispersed with respect
to the population distribution; this is visually evident in the ﬁgure. Intuitively, the
MLEs here can be viewed as samples from a modiﬁed population density that is
the convolution of the true population density with the error density (though we
note that this convolution interpretation does not generalize to more complicated
settings).
As alternatives to the MLEs, consider Bayesian estimates in two different sce-
narios. First, suppose we knew M0 = −21 a priori. Using the (known) popula-
tion distribution as the prior for each Mi produces posteriors that remain indepen-
dent and normal, but with means, ˜
Mi, shifted from the MLEs toward M0. Deﬁne
b ≡σ2/(σ2 + τ 2); then the posterior means (and modes) are given by
˜
Mi = (1 −b) · ˆ
Mi + b · M0,
(11.10)
and the variance for each estimate is reduced to (1 −b)σ2 rather than just σ2.
The squares on the third line below the panel in Figure 11.2 show these estimates.

11.3 Multilevel models
255
Fig. 11.2. Shrinkage in a simple normal–normal model. Top panel shows population dis-
tribution. ‘True’ axis shows Mi values of 30 samples. Remaining axes show estimates
from measurements with σ = 0.3 normal error: MLEs, conditional (on the true mean), and
empirical/hierarchical Bayes estimates.
They all move toward M0, and thus toward each other. One says that the ensemble
of estimates ‘shrinks toward M0’; this phenomenon is called shrinkage. They are
labeled ‘Cond’ in the ﬁgure to indicate that we conditioned on M0.
As an ensemble, the shrunken estimates look much more like the true values than
the MLEs. These estimates are biased and are no longer invariant, but even from a
frequentist perspective they may be deemed better than the MLEs: despite the bias,
the shrunken estimates are, on the average (over error realizations), closer to the
true values than the MLEs – i.e., they have smaller mean squared error (MSE) – as
long as N > 2. Stein discovered this effect around 1960 and, after a decade or two
of sorting out its subtleties, the use of deliberately (and carefully) biased estimators
for joint estimation of related quantities is now widespread in statistics (frequentist
and Bayesian), and considered one of the key innovations of late-twentieth-century
statistics.
We have used strong prior information here: precise knowledge of M0. But what
if we did not know M0 a priori? The empirical Bayes (EB) approach ‘plugs in’ an
ad hoc estimate of M0 and uses the resulting prior. The obvious estimator here is

256
Bayesian multilevel modelling of cosmological populations
¯
M, the average of the MLEs, whose position is indicated by the thick vertical line
in the ﬁgure. Using the resulting prior produces the circle estimates on the bottom
axis, still shrunken, but towards ¯
M rather than M0. Equation (11.10) again gives
the estimates, if we replace M0 with ¯
M. Note that since ¯
M depends on all of the
MLEs, the EB estimates are no longer independent.
From a fully Bayesian point of view, plugging in ¯
M for M0 is unjustiﬁed; M0
is unknown, so we should consider it a parameter, assign its prior distribution,
and marginalize over it, an approach called hierarchical Bayes (HB). The resulting
estimates are identical to the EB estimates in this problem (though they need not
be in general); however, the uncertainties in the HB estimates are somewhat larger
than those produced in an EB calculation, reﬂecting uncertainty in the shrinkage
point. We have motivated EB and HB shrinkage estimates via Bayesian arguments,
but the frequentist advantages of shrinkage in the conditional case still hold: despite
their bias, these estimates have smaller MSE than MLEs. In addition, due to their
accounting for M0 uncertainty, conﬁdence intervals based on the HB procedure
have more accurate frequentist coverage than EB intervals.
The conditional shrinkage estimates are the normal–normal model counterparts
to homogeneous Malmquist and Lutz–Kelker corrected estimates (the latter do not
so obviously ‘shrink’ because the prior in astronomical settings is much broader
than a normal distribution). On the one hand, this is a source of comfort, in that
some of the statistical beneﬁts of shrinkage are presumably shared by the astro-
nomical methods. However, the correspondence is also a source of caution and
concern. Decades of study have revealed shrinkage to be a subtle phenomenon,
with snares for the unwary. We highlight just a few key developments here; entries
to the large literature in this area include Carlin and Louis (2000, 2009), Carlin
et al. (2006), and Browne and Draper (2006).
Most obviously, as noted in Section 11.2.2, the homogeneous density assump-
tion of the classic corrections is seldom justiﬁable; in reality, the population density
is inhomogeneous and unknown. That is, conditional shrinkage is not appropriate;
something along the lines of the EB or HB approaches is in order. This must be
done with some care: it is known that shrinkage may not improve estimates, and
may even worsen them, if done in a way that does not reﬂect the true distribution
of ﬁrst-level parameters. The EB approach – using a ‘plug-in’ estimate of hyper-
parameters to specify the population hyperprior – has appealing simplicity; the
Landy–Szalay approach probably has an approximate EB justiﬁcation. But it is
known that EB approaches tend to underestimate ﬁnal uncertainties (due to ignor-
ing hyperparameter uncertainty). HB estimates can offer improvements, but with
computational costs, and with other challenges noted below.
More subtly, shrinkage must be tuned, not only to the underlying population dis-
tribution, but also to the inferential goal. The shrinkage estimates just described

11.3 Multilevel models
257
do indeed reduce the MSE of the collection of source absolute magnitudes. But if
one uses the shrunken point estimates to infer the population distribution, it turns
out the point estimates are underdispersed and the distribution may be poorly es-
timated. If one instead seeks from the beginning point estimates that are optimal
for estimating the population distribution (via a decision-theoretic calculation), a
different shrinkage prescription is appropriate. Similarly, it is evident from the seg-
ments connecting the true Mi’s with their MLEs that the ranks of the sources are
shufﬂed, and shrinkage has not corrected it. In some settings, shrinkage estimates
that improve rank estimates have been identiﬁed; they differ from those optimal for
individual parameter or distribution estimates. There is a kind of ‘complementar-
ity’ in relying on point estimates for subsequent inferences; estimates optimal for
some questions may be misleading for other questions (Louis 1984).
A main source of these complications is the inadequacy of point estimates as
summaries of a correlated, high-dimensional posterior distribution. This motivates
a more thoroughly Bayesian treatment in the spirit of HB, relying on marginaliza-
tion over uncertain parameters rather than use of point estimates. We pursue this
approach below.
But before doing so, given the Bayesian focus of this volume, some comments
at a conceptual level are appropriate here. The second level of our MLM here de-
scribes the population with a continuous density. A frequentist interpretation of
this density is problematic. The volume accessible to a survey, indeed the volume
within the horizon, contains a ﬁnite number of sources (galaxies, cluster, quasars,
etc.). Repeating a survey will produce catalogues that largely contain the same
sources (some sources near the survey detection limit may differ from one repeti-
tion to the next); the population density cannot be interpreted in terms of frequen-
tist variability. At the ﬁrst level of the MLM, measurement errors may differ among
repetitions, but if the ﬁrst-level uncertainties are the result of indicator scatter, it-
self a population-level phenomenon, ﬁrst-level results will also be the same across
repeated surveys.
From the Bayesian point of view, the population density describes uncertainty
about population members, not variability in repeated sampling; its introduction
and speciﬁcation should be motivated by epistemological considerations. One way
to formally motivate it is as a mechanism to introduce dependence among esti-
mates. That is, we expect that learning the properties of many sources of a par-
ticular type should help us predict the properties of as-yet unmeasured sources of
that type; this is what it means to consider the sources to comprise a population.
Consistency requires that, once we obtain measurements of new sources, we can-
not ignore the prior information provided by measurements of other sources that
we would have used in the absence of the data. The resulting dependences in the
joint posterior pools information, leading to shrinkage.

258
Bayesian multilevel modelling of cosmological populations
More formally still, we might justify introducing a population density by requir-
ing the joint prior distribution for a set of source properties to be exchangeable, that
is, invariant to permuting the labels of sources. For example, in the setting above,
p(M1, M2, . . . , MN|θ) should take the same functional form if we permute the or-
der of the {Mi}. This appears to be both a natural and a weak assumption; it is in
the spirit of the common ‘iid’ assumption in that the marginals for each source are
identical but, by allowing dependence, it sets the stage for sharing of information
across the population. Surprisingly, exchangeability itself implies the MLM struc-
ture: the (continuous) de Finetti exchangeability theorem implies that any such
exchangeable distribution can be written as a density-weighted mixture of identi-
cal, conditionally-independent distributions, i.e., as an MLM. In the setting here,
the theorem says one may write p(M1, M2, . . . , MN) =
	
dθ μ(θ) +
i f(Mi|θ),
where μ(θ) deﬁnes a unit-normed measure over the form of the independent densi-
ties f(·|θ). The theorem is a purely mathematical result providing a representation
for symmetric functions, but in a Bayesian context it motivates introducing a con-
tinuous population density, playing the role of f(·), with a hyperprior playing the
role of μ.4
11.3.2 Poisson point process multilevel models
While the MLM set-up above has the essential ingredients needed for us to move
beyond point estimate-based population modelling, we need to generalize it in two
ways to meet the needs of astronomical survey analysis. First, the analysis above
took the catalogue size, N, as given. In an astronomical survey, N is instead de-
termined by the population density and the volume surveyed; it is thus informative
about the population. Second, astronomical surveys suffer from selection effects,
most typically in the form of truncation in a ‘blanket’ survey (where sources may
be missed due to detection criteria), or censoring in a targeted follow-up survey
(where sources known to exist may have unmeasureable properties due to limited
sensitivity). We discuss the truncation case here.
To allow catalogue size to be informative about the population density, we model
the population with an inhomogeneous Poisson point process, characterized by an
intensity measure rather than a probability distribution. Let O denote the observ-
able source parameters; the Poisson point process assumption implies there is an
intensity measure, μ(O) that, when known, allows us to write the probability for
there being a source with O in the interval [O, O + dO] as μ(O)dO, to leading
order in O. It also presumes that this probability is independent of whether a source
4 Rigorously, the theorem requires that the judgement of exchangeability apply for any selection of a ﬁnite set of
Mi’s from an inﬁnite set. If there is a ﬁnite limit to N, the integral representation may not be able to represent
some possible exchangeable distributions, though the restriction is minor if the limit is large. See Diaconis
and Freedman (1980) for details.

11.3 Multilevel models
259
is found in any other (distinct) interval (provided we know the intensity μ(·); i.e.,
this is conditional independence). Usually we will not know the intensity, e.g., it
may depend on parameters, θ, whose values are uncertain, which we indicate by
writing μ(O; θ).
To account for truncation, we introduce a survey detection efﬁciency, ϵ(O),
specifying the probability that a source with parameters O will be detected. Al-
though we take ϵ(O) as given in what follows, it is worth noting (especially for
non-astronomer readers) that calculating the ϵ that characterizes a particular survey
is often a very difﬁcult task, requiring both careful measurement and calibration,
and often extensive Monte Carlo simulation. Not all surveys provide an accurate
detection efﬁciency, yet it is necessary for what follows. (In some cases it may be
possible to partially infer ϵ from the available survey data; we do not cover this
challenging task here.)
Finally, we highlight a point made in passing above: from the Bayesian point
of view, a survey source catalogue should not be viewed as providing estimates
of source properties, but rather as summary statistics specifying source likelihood
functions, ℓi(Oi) = p(Di|Oi), where Di denotes the data for source i. This change
in viewpoint has far-reaching implications. It can enable more accurate account-
ing of source uncertainties, e.g., by reporting a likelihood parameterization more
complex than the traditional ‘best-ﬁt ± uncertainty’, such as a parameterization de-
scribing possible likelihood skewness (as might be important near detection limits).
It also opens the door to the use of marginal detections or upper limits in censored
surveys, by averaging over source uncertainties with likelihoods that are far from
normal, possibly peaking at zero source ﬂux. This is discussed further in Chapter 8.
With these ingredients in hand, one can calculate the truncated Poisson point
process counterpart to the MLM joint posterior density for the population and
source parameters of Eq. (11.9) (see Loredo & Wasserman 1995 or Loredo 2004
for derivations):
π(θ, {Oi}|D) ∝π(θ) exp
$
−

dO ϵ(O)μ(O; θ)
% N

i=1
ℓi(Oi)μ(Oi; θ).
(11.11)
Marginal posteriors for θ or {Oi} may be calculated as was done above, though ob-
viously the calculations can be challenging for astrophysically interesting models.
Our own applications to date have been to situations with parametric population
models, where O was one dimensional (magnitudes of trans-Neptunian objects
(TNOs); Petit et al. 2007 and references therein), or three dimensional (ﬂuxes and
directions of gamma-ray bursts; Loredo & Wasserman 1998a,b). In these cases, the
N integrals over Oi were done by quadrature.

260
Bayesian multilevel modelling of cosmological populations
Fig. 11.3. Scatter plots of posterior modes for population parameters, using data simulated
from a rolling power law, from a Bayesian analysis marginalizing over source parameters
(circles), and a maximum likelihood analysis using best-ﬁt source estimates (+ symbols).
Left panel is for simulated surveys of N = 100 sources; right panel is for N = 1000.
As a simple example focusing on one aspect of the MLM approach, the value of
marginalizingoversourceuncertainty,consideramagnitudesurvey(i.e.,the‘number
counts’ setting) where O = m, the apparent magnitude of a source. Suppose the
source population has a rolling power-law distribution of ﬂuxes, so we may write
the magnitude distribution as μ(m) = A × 10[α(m−23)+α′(m−23)2], where A is
the density per unit magnitude at m = 23, and α and α′ give the slope of the
number–magnitude distribution, and its rate of change with m, at m = 23. We sim-
ulated sources from this distribution, and simulated detections and measurements
for a very simple survey performing source detection and measurement via photon
counting. The survey parameters were chosen so that the dimmest detected sources
have magnitude uncertainties ∼0.15 magnitudes. We analyzed data from many
simulated surveys, all of a population with α = 0.7 and α′ = −0.05 (values that
describesomeTNOdata),andestimatedαandα′ byﬁndingthemodeofthemarginal
posterior for these parameters, marginalizing Eq. (11.11) over all the mi and the
amplitude, A. The resulting estimates, for surveys of size N = 100, are plotted as the
open circles in the left panel of Figure 11.3; the cross-hair shows the true parameter
values.Wealsocalculatedmaximumlikelihoodestimates,i.e.,usingonlythebest-ﬁt
mi estimates (not marginalizing). These are plotted as ‘+’ symbols in the ﬁgure.
The Bayesian estimates are distributed roughly symmetrically about the truth; the
MLEs are clearly biased toward large α and small α′, though the uncertainties are
large enough that the estimates are sometimes accurate.
The right panel shows results from the same calculation, but with N = 1000.
The Bayesian estimates have converged closer to the truth. In contrast, the MLEs
have converged away from the truth. This example highlights the value of marginal-
ization, particularly in settings with measurement error. In such settings, each new
source adds parameters to the problem; one is not in the ﬁxed parameter dimension

11.4 Future directions
261
setting in which our statistical intuitions are trained. As a result, the effects of
source uncertainties do not ‘average out’; instead, it becomes more rather than less
important to carefully account for them as survey size grows. This aspect of pop-
ulation modelling has been repeatedly rediscovered by astronomers. The earliest
discovery we know of is Eddington’s treatment of what has become known as Ed-
dington bias (Eddington 1940). A recent rediscovery in a cosmological context is
contemporary as we write: Sheth’s work on the effects of photometric redshift er-
rors on modelling galaxy and quasar populations (Sheth 2007). Sheth advocates an
ad hoc deconvolution algorithm; we think Bayesian multilevel modelling offers a
vastly more ﬂexible and accurate framework for addressing such problems.
11.4 Future directions
The fully Bayesian MLM approach just described has so far been implemented
only for fairly simple models of modest-sized surveys (though the challenging
VELMOD analysis of Tully–Fisher data by Willick and Strauss (1998), mentioned
in subsection 11.2.3, includes many of the key elements). Future research must
explore applications of increased complexity in three different dimensions: survey
size, source parameter dimension, and population model complexity.
An obvious need is development of numerical algorithms appropriate for mul-
tivariate observables and large surveys, perhaps invoking approximations or us-
ing Monte Carlo methods for source parameter marginalization. With respect to
multivariate observables, it will be insightful to work out the detailed connec-
tions between the MLM approach and other methods, such as those surveyed in
Section 11.2. For example, when the observables are ﬂux and a distance indicator,
an analogue to the direct indicator method should ‘fall out’ of an MLM calculation
when the source likelihoods provide precise estimates of the indicators.
Implementing Bayesian MLM with more complex population models will also
demand development of clever algorithms. But more subtle and interesting chal-
lenges arise as model complexity increases. These challenges arise because of the
‘softening’ of the impact of source measurements on inferences, due to uncertain-
ties. We saw above that this ‘changes the rules’ in the sense of causing violations
of naive intuition about uncertainties averaging out as N grows. But this is only
one of several issues complicating life with multilevel models.
Some of these issues mimic problems associated with non-parametric modelling,
and this is no accident. Though a common informal ‘deﬁnition’ of a non-parametric
model is a model with an inﬁnite number of parameters, a more insightful deﬁni-
tion is a model in which the effective dimension of the parameter space can grow
with sample size. In fact, the ‘many normals’ problem – essentially the normal–
normal MLM, with the actual dimension growing linearly with sample size – is

262
Bayesian multilevel modelling of cosmological populations
sometimes used as a surrogate for more complex non-parametric models in theo-
retical analyses (Wasserman 2005).
A prime issue which non-parametric modellers must face is assessing how pri-
ors over large-dimensional spaces may inﬂuence inferences. Similar concerns arise
for MLMs. For example, for estimating the mean and standard deviation of a nor-
mal distribution using precise measurements, common default priors are ﬂat for
the mean and log-ﬂat for the standard deviation. We saw above that a ﬂat prior
for M0 in the normal–normal MLM produced sensible inferences. But had we
considered the population standard deviation τ to be unknown, we would have dis-
covered that a log-ﬂat τ prior leads to an improper (un-normalizable) posterior. In-
stead, priors ﬂat in τ or τ 2 (among others) are advocated, with various justiﬁcations
(sometimes including the good frequentist performance of the resulting estimates;
see, e.g., Berger et al. 2005; Gelman 2006). This indicates that as astronomers in-
crease the complexity of MLMs for surveys, care must be taken with priors; statis-
ticians have helpful insights to offer here. It also suggests that model checking,
in the spirit of goodness-of-ﬁt tests, is important for MLMs. Their rich structure
makes conventional model checking methods inappropriate, but there is recent re-
search on model checking methods tailored to MLMs (e.g., Sinharay & Stern 2003;
Bayarri & Castellanos 2007).
An alluring direction for increasing model complexity is to make the popula-
tion model itself truly non-parametric. One motivation comes from existing non-
parametric methods designed to ﬂexibly account for selection effects, such as the
C−method of Lynden-Bell (1971) or the stepwise maximum likelihood method
(Efstathiou et al. 1988). These methods ignore source uncertainties; ﬁnding coun-
terparts in the MLM framework promises to broaden applicability of such ap-
proaches, and unify them with approaches relying on Malmquist-style corrections
(insofar as MLMs have shrinkage ‘built in’). But the fact that parametric MLMs
already have some of the issues of non-parametric modelling suggests that non-
parametric multilevel modelling will be tricky, requiring even more care with as-
sessing robustness to priors. Fortunately, there are successful examples in the statis-
tics literature to build upon (e.g., M¨uller & Quintana 2004).
Sensitivity to priors can make newcomers to Bayesian methods consider re-
treating to frequentist territory. But there is little solace there; the subtleties of
complex multilevel Bayesian modelling reﬂect genuine complexity in the task of
modelling surveys, complexity that has frequentist implications. For example, the
best-studied methods for non-parametric analysis of survey data with uncertainties
(yet to be extended to include truncation) rely on deconvolution. Though the meth-
ods are straightforward, it is known that the resulting population estimates have
discouragingly slow rates of convergence (often only logarithmic in N, as opposed
to the
√
N we are accustomed to for parametric inference without measurement

11.4 Future directions
263
errors). In fact, leading developers of such methods have recently turned to
Bayesian methods, where careful attention to structure in the prior can lead to
methods with improved performance (e.g., Berry et al. 2002).
We think development of semi-parametric population models for use in MLM
analyses of survey data may be an especially fruitful research direction. For ex-
ample, we envision non-parametric modelling of the density distribution of galax-
ies, combined with parametric modelling of the luminosity function (e.g., by a
Schechter function or a mixture of a few Schechter functions), as a promising ap-
proach, allowing adaptivity to complex spatial structure, but hopefully providing
good convergence rates for learning the luminosity function. But there are several
challenges to conquer on the path from the current state of the art to such a goal.
Acknowledgments
We are grateful to many collaborators and colleagues who have contributed to
our understanding of survey biases and population modelling, especially David
Chernoff, Woncheol Jang, John Simmons and Ira Wasserman. We also gratefully
acknowledge the Statistical and Applied Mathematical Sciences Institute (SAMSI),
whose 2006 Astrostatistics Program assembled astronomers and statisticians to dis-
cuss these issues. Loredo was partly supported by NSF grant AST-0507589 and by
NASA grants NAG5-12082 and NNG06GH84G for work reported here.
References
Bayarri, M. J. and Castellanos, M. E. (2007). Stat. Sci., 22, 322.
Berger, J. O., Strawderman, W. and Tang, D. (2005). Ann. Stat., 33, 606.
Berry, S. M., Carroll, R. J. and Ruppert, D. (2002). JASA, 97, 160.
Browne, W. J. and Draper, D. (2006). Bayesian Analysis, 1, 473.
Carlin, B. P. and Louis, T. A. (2000). Bayes and Empirical Bayes Methods for Data
Analysis. London: Chapman & Hall/CRC.
Carlin, B. P. and Louis, T. A. (2009). Bayesian Methods for Data Analysis. London:
Chapman & Hall/CRC.
Carlin, B. P., Clark, J. S. and Gelfand, A. E. (2006). In J. Clark and A. E. Gelfand, eds.,
Hierarchical Modelling for the Environmental Sciences. Oxford: Oxford University
Press, p. 3.
Diaconis, P. and Freedman, D. (1980). Ann. Prob., 8, 745.
Eddington, A. S. (1913). Mon. Not. Roy. Astron. Soc., 73, 359.
Eddington, A. S. (1940). Mon. Not. Roy. Astron. Soc., 100, 354.
Efstathiou, G., Ellis, R. S. and Peterson, B. A. (1988). Mon. Not. Roy. Astron. Soc.,
232, 431.
Erdogdu, P. et al. (2006). Mon. Not. Roy. Astron. Soc., 373, 45.
Freudling, W. et al. (1995). Astron. J., 110, 920.
Gelman, A. (2006). Bayesian Analysis, 1, 515.
Hendry, M. A. and Simmons, J. F. L. (1994). Astrophys. J., 435, 515.

264
Bayesian multilevel modelling of cosmological populations
Hendry, M. A. and Simmons, J. F. L. (1995). Vistas Astron., 39, 297.
Hudson, M. J. (1994). Mon. Not. Roy. Astron. Soc., 266, 468.
Landy, S. D. and Szalay, A. S. (1992). Astrophys. J. Lett., 391, 494.
Loredo, T. J. (2004). In R. Fischer, R. Preuss and U. von Toussaint, eds., AIP Conf. Proc.,
735, 195.
Loredo, T. J. (2007). ASP Conf. Ser., 371, 121.
Loredo, T. J. and Wasserman, I. M. (1995). Astrophys. J. Supp., 96, 261.
Loredo, T. J. and Wasserman, I. M. (1998a). Astrophys. J., 502, 75.
Loredo, T. J. and Wasserman, I. M. (1998b). Astrophys. J., 502, 108.
Louis, T. A. (1984). JASA, 79, 393.
Lynden-Bell, D. (1971). Mon. Not. Roy. Astron. Soc., 155, 95.
Lynden-Bell, D. et al. (1988). Astrophys. J., 326, 19.
Malmquist, K. G. (1920). Medd. Lund Astron. Obs., Ser. 2, No. 22, 359.
Malmquist, K. G. (1922). Medd. Lund, Ser. I, 100, 1.
M¨uller, P. and Quintana, F. A. (2004). Stat. Sci., 19, 95.
Petit, J.-M., Kavelaars, J. J., Gladman, B. J. and Loredo, T. J. (2007). In A. Barucci,
H. Boehnhardt, D. Cruikshank and A. Morbidelli, eds., Kuiper Belt. Tucson, AZ:
University of Arizona Press and Lunar and Planetary Institute (in press).
Schechter, P. L. (1980). Astron. J., 85, 801.
Sheth, R. K. (2007). Mon. Not. Roy. Astron. Soc., 378, 709.
Sinharay, S. and Stern, H. S. (2003). J. Stat. Plan. Infer., 111, 209.
Strauss, M. A. and Willick, J. A. (1995). Phys. Rep., 261, 271.
Teerikorpi, P. (1997). Annu. Rev. Astron. Astrophys., 35, 101.
Wasserman, L. (2005). All of Nonparametric Statistics. New York: Springer-Verlag.
Willick, J. A. (1994). Astrophys. J. Supp., 92, 1.
Willick, J. A. and Strauss, M. A. (1998). Astrophys. J., 507, 64.

12
A Bayesian approach to galaxy evolution studies
Stefano Andreon
12.1 Discovery space
We astronomers mostly work in the ‘discovery space’, the region where effects
are statistically signiﬁcant at less than three-sigma or near boundaries in data or
parameter space. Working in the discovery space is a normal astronomical activ-
ity; few published results are initially found at large conﬁdence. This can lead
to anomalous results, e.g., positive deﬁnite quantities (such as mass, fractions, star
formation rates, dispersions, etc.) are sometimes found to be negative or, more gen-
erally, quantities are sometimes found at unphysical values (completeness larger
than 100%, V/Vmax > 1 or fractions larger than 1, for example). Working in the
discovery space is typical of frontier-line research because almost every signiﬁcant
result reaches this status after having appeared ﬁrst in the discovery space, and be-
cause a good determination of known effects or trends usually triggers searches for
ﬁner, harder to detect, effects, mostly falling once more in the discovery space.
Many of us are very conﬁdent that commonly used statistical tools work prop-
erly in the situations in which we use them. Unfortunately, in the discovery space,
and sometimes outside it, we should not take this for granted, as shown below with
a few examples. We cannot avoid working in this grey region, because to move
our results into the statistically signiﬁcant area we often need a larger or better
sample. In order to obtain this, we ﬁrst need to convince the community (and the
telescope time allocation committees) that an effect is probably there, by working
in the discovery space. Furthermore, awaiting a larger sample leaves the unappeal-
ing possibility that someone else may publish our result and they, not us, will be
credited for the discovery. Of course, this assumes that a larger sample exists and
is accessible, which is not always the case. There is just one Universe (and sky), al-
ready fully observed in the microwave. A group formed by 10 galaxies has no more
265

266
A Bayesian approach to galaxy evolution studies
than 10 galaxies available to obtain its velocity dispersion. Gamma-ray bursts (and
any transient event) may not be long-lived enough to allow us to collect enough
photons to place our target measurement, say polarization, in the statistically sig-
niﬁcant area. Working in the discovery space region is therefore an essential part
of astronomers’ work.
Standard tools may fail (especially if misused) in many ways. In the next two
sections we will show some examples of failure in two idealized experiments, and
we will show that the Bayesian approach does not suffer from these failures. In
the ﬁrst example, we show that the maximum of the likelihood (the best ﬁt) may
not be a good estimate of the true value: averaging the likelihood is preferable to
maximizing it. The second example shows that sometimes the observed value is
biased, and highlights the bad things that may occur when the prior is ignored.
These two examples are very simple as compared to true problems, and they have
been chosen so as to make obvious that the best-ﬁt value or observed value may
be bad or biased. The third example shows that when the sample size is small,
even simple operations on data, such as performing an average or ﬁtting it with a
function, is a potentially risky operation (and something that fails even in a simple
case is unlikely to work correctly in a complex one). In Section 12.5 we consider
two realistic examples, showing failures of standard methods that are more difﬁ-
cult to spot, but are of the same nature as the easily spotted failures – all of them
come from contradicting axioms of probabilities. In the ﬁrst example, we want
to measure the width of a distribution in the presence of a contaminating popula-
tion. A mixture modelling of inhomogeneous Poisson processes easily solves this
problem. In the second example, we wish to ﬁt a trend in the presence of a contam-
inating population and we will use a mixture of regressions. We ﬁnally conclude
the chapter by showing that the Bayesian approach allows us to properly under-
stand the number returned by tests like Kolmogorov–Smirnov, χ2, etc., named in
the statistical jargon as the p-value.
12.2 Average versus maximum likelihood
Maximum likelihood estimates (called ‘best ﬁt’ by astronomers) are one of the
most widely used tools in astronomy and it is usually taken for granted that maxi-
mizing the likelihood (or minimizing χ2 = −2 ln L) will always give the correct
result.
Mixture distributions naturally arise in astronomy when data come from two
populations, in such cases where
(a) a signal is superposed on a background,
(b) there are interlopers in the sample,

12.2 Average versus maximum likelihood
267
Fig. 12.1. Left panel: An example of a mixture distribution in two dimensions. Readers
may consider that we displayed the spatial distribution of photons coming from two as-
tronomical extended sources as observed by a perfect instrument (no background, perfect
angular resolution, etc.), or the distribution of two galaxy populations in a two-dimensional
parameter space, or whatever else. Right panel: true (input) values, maximum likelihood
estimate (MLE) and posterior (Bayes) probability distributions for the example in the left
panel.
(c) there are two distinct galaxy populations (by colour, morphology, dynamical
properties, etc.),
(d) an image of the sky is taken using an instrument with a ﬁeld of view large
enough to accommodate more than one source, or
(e) a galaxy spectrum is observed in which we note the presence of two stellar
populations.
Let us consider the simple case of a mixture (sum) of two Gaussians:
p(yi|μ1, σ1, μ2, σ2, λ) = λN(yi|μ1, σ2
1) + (1 −λ)N(yi|μ2, σ2
2),
(12.1)
where (μj, σj) j = 1, 2 are the location (or centre) and scale (or width) of the two
Gaussians
N(yi|μj, σ2
j ) =
1
√
2πσj
e
−
(yi−μj)2
2σ2
j
,
(12.2)
λ and 1 −λ are the proportions of the two components, and yi is the ith datum.
Figure 12.1 shows a two-dimensional example.
We want to determine the locations and scales of the two Gaussians with the data
at hand. The likelihood of independently and identically distributed data is given

268
A Bayesian approach to galaxy evolution studies
by the product, over the data yi, of the terms in Eq. (12.1):
p(y|μ1, σ1, μ2, σ2, λ) =

i
p(yi|μ1, σ1, μ2, σ2, λ).
(12.3)
We usually maximize the likelihood in current problems, blindly assuming that
the maximum likelihood values are good estimates of the true value. Here, though,
the parameters that maximize the likelihood above (the best ﬁt) are not near their
true value (e.g., those used to draw the points in Figure 12.1), but occur when
μj = yi and σj →0. In fact, the likelihood goes to inﬁnity as σj →0, because the
ith term of Eq. (12.1) diverges and the remaining ones take a ﬁnite, non-zero, value.
The problem is a general characteristic of mixtures of probability distributions
of non-ﬁxed variance, not only of Gaussians. The problem does not disappear ‘in
the long run’, i.e., by obtaining a sufﬁciently large sample (that we don’t have,
or which time allocation committees are reluctant to allocate, or perhaps does not
exist). On the contrary, our chances of failure increase with sample size, because
there is an increasing number of values for which the likelihood goes to inﬁnity,
one per datum.
Therefore, maximizing the likelihood, even for unlimited data, does not return
the size of two astronomical sources, or the velocity dispersion of a cluster in the
presence of interlopers, or many other quantities in the presence of two popula-
tions or signals (or an interesting and an uninteresting population or signal). Even
worse, there is no ‘warning bell’, i.e., something that signals that something is
going wrong, until an inﬁnity is found when maximizing the likelihood. In real
applications, as those described in Sections 12.4 and 12.5, nothing as bad as an
inﬁnity appears, and thus there is no ‘warning bell’ signalling that something is
going wrong.
The Bayesian approach is not affected by such problematics: it never instructs us
to maximize any unknown parameter, because the sum axiom of probability tells
us to sum (or integrate) over unknown quantities, so that their effect is averaged
over all plausible values.
The right panel of Figure 12.1 shows the posterior distribution for the data shown
in the left panel, adopting a constant prior up to very large values (the precise
values are irrelevant for this parameter estimation problem). The posterior is well
behaved and it is centred on the input value. The likelihood, instead, has hundreds
of inﬁnities, one per datum, all at σ = 0.
12.3 Priors and Malmquist/Eddington bias
Number counts are steep. It is well known to astronomers that the true value, μ,
of the source intensity differs from the measured counts, n, of the source when n

12.3 Priors and Malmquist/Eddington bias
269
is small: even in the presence of symmetric errors, an object with n counts more
probably comes from the numerous population of objects having μ < n than the
rare population having μ > n. Therefore, objects with n counts have, likely, μ < n
(e.g., Eddington 1913). A similar effect arises for parallaxes, star counts, velocity
dispersions and any noisy determination of a quantity concerning an object drawn
from a population that shows an important numerical change over the range in-
cluded by the error on the measurand1 (on μ, in our example). Restated in the
statistical jargon, in parameter estimation problems where the prior, p(μ), has a
large change in the μ range where the likelihood is slowly varying, the prior cannot
be neglected.
As a quantitative example, tailored around the X-Bootes survey (Kenter et al.
2005), we consider a Poisson process p(n|μ) = μne−μ/n!, with rate μ and a
power-law prior of logarithmic slope α, p(μ) = μ−α (the latter is called number
counts by astronomers). Having observed four photons (i.e., n = 4), the maxi-
mum likelihood estimate of the source rate is ˆμ = 4, but astronomers know by
experience that this value is wrong: with better data we ﬁnd, most of the time,
that actually μ < 4. Assuming a Euclidean slope α = 2.5 (the observed value
of the slope of number counts at the rate of interest), the posterior p(n|μ)p(μ) is
∝μne−μμ−2.5/n!. The posterior mean (or, in astronomical terms, the Eddington-
corrected value) of the source rate is 2.5 photons, 40% less than the originally
observed value. See Figure 12.2 for detail.
The same holds true, as mentioned, for many noisy quantities, such as the deter-
mination of a velocity dispersion with just a few velocities or the estimate of the
cluster richness N200 (see Andreon & Hurn 2009 for details on the latter).
What is known in astronomy as Malmquist or Eddington bias is the manifes-
tation of the important role of the prior when measurements are imprecise, i.e.,
the fact that a correct inference proceeds along the Bayes’ theorem. The prior (the
correction for Malmquist bias) moves the result away from the observed value, or
the maximum likelihood estimate, and brings it near the true value. The example
shows that priors (the fact that there are many more fainter systems than bright
ones) cannot be ignored in inferences, if one does not wish to be wrong most of
the time. Priors are speciﬁc to the Bayesian approach and, usually, non-Bayesian
methods consider them something to be avoided.
It is therefore apparent that prior-free and maximum likelihood methods are in
trouble.
1 The measurand is the parameter being quantiﬁed. It usually differs from the outcome of the measurement
because of the noise.

270
A Bayesian approach to galaxy evolution studies
Fig. 12.2. Posterior distribution (solid curve) of the source ﬂux, having observed four pho-
tons and knowing that source number counts have a Euclidean distribution (dashed curve).
The maximum likelihood estimate (MLE) is also reported.
12.4 Small samples
Astronomers are often faced with computing an average by combining a small
number of estimates, or ﬁtting a trend or a function from just a few data points.
We almost always start with a few measurements of an interesting quantity, say
the rate at which a galaxy mass increases. In most of the cases, the measurand
may be parameterized in several ways. For example, if the aim is to measure the
relative evolution of luminous (L) and faint (F) red galaxies, a central topic in
galaxy evolutionary studies, should we study L/F, F/L, or does the chosen pa-
rameterization not matter? Both parameterizations have been adopted in recent
astronomical papers (and the author, in Andreon (2008), took a third, different,
parameterization!). Speciﬁc star formation rates (sSFR) and e-folding times, τ,
are approximatively reciprocal measures (long e-folding times correspond to small
sSFR). To the author’s knowledge, none of the proposed parameterizations (e.g.,
L/F vs. F/L or sSFR vs. τ) has a special status. Unfortunately, when the sample
size is small, results obtained via commonly used formulae (e.g., weighted aver-
age, best ﬁt, etc.) depend on the adopted parametric form. For example, an average
value, computed by a weighted sum, or a ﬁt performed by minimizing the χ2, has
a special meaning, because the result depends on which parameterization is being
adopted.

12.4 Small samples
271
As an example, let us consider just two data points, (f/l)1 = 3 ± 0.9 and
(f/l)2 = 0.3333 ± 0.1. The error weighted average ⟨f/l⟩is 0.37. The reciprocal
values ((l/f)i = 1/(f/l)i; 0.3333 ± 0.1 and 3 ± 0.9) have an error weighted aver-
age equal again to 0.37, quite different from the reciprocal of ⟨f/l⟩, 1/0.37 = 2.7.
Therefore ⟨f/l⟩̸= 1/⟨l/f⟩, and they differ by much more than their error (obvi-
ously, the error on the mean is smaller than the error on any data point). At ﬁrst
sight, by choosing the appropriate parameterization, the astronomer may select the
number he wants, a situation surely not recommended by the scientiﬁc method.
Similar problems are present with two data points differing by just one-sigma, or
in general with small samples.
One may argue that in the above case the number of points is so small that
no one will likely make an average of them. However, one almost always starts
by averaging two or three values, or looks for trends, or ﬁts a function using a
number of data points only slightly exceeding the number of parameters. Often,
the few points are the result of a large observational effort (e.g., obtained through
analyzing thousands of galaxies, as in the example above) and it is very hard to
assemble a larger sample. Thus the average of a few numbers is almost all we can
do. For example, how many estimations of the SFR at z ∼6 exist? Should we
not combine the very few available in some way to take proﬁt of all them? Small
sample problems are often ‘hidden’ in large samples: even large surveys, such as
the SDSS, 2dF, VVDS and CNOC2 surveys including tens or hundreds of thousand
of galaxies, estimate galaxy densities using sub-samples equivalent to just one to
ten galaxies. Finally, how many of us have checked, before performing a ﬁt or an
average, whether the sample size is large enough to be insensitive to the chosen
parameterization?
The described problem originates from the freedom, in the frequentist
paradigm, of choosing an estimator of the measurand (⟨f/l⟩or 1/⟨l/f⟩for ex-
ample). All estimators (satisfying certain conditions) will converge on the true
value of the estimand in the long run, but without any assurance that such a regime
is reached with the sample size in hand. Until this regime is reached, different
estimators will return different numbers. Bayesian methods do not present this
shortcoming, because they already hold with n = 2 and do not pass through the
intermediate and non-unique step of building an estimator of the measurand.
This example shows that frequentist methods return the value of an estimator
of the measurand, not the measurand itself or its probability distribution. While
these differences are easy to appreciate in our example, it is not always so with
real cases dealing with dispersion, slope or intrinsic scatter, discussed in the next
sections.

272
A Bayesian approach to galaxy evolution studies
12.5 Measuring a width in the presence of a contaminating population
We now focus on how to measure the scale (dispersion) of a distribution (say, of
velocities v), knowing that the sample is contaminated by the presence of inter-
lopers, but without the knowledge of which object is an interloper. The main idea
is not to identify or de-weight interlopers in the scale estimate, but to account for
them statistically, precisely as astronomers do with photons when estimating the
ﬂux of a source in the presence of a background.
We assume that data come from two populations: background galaxies, whose
distribution is assumed to be a homogeneous (i.e., the intensity is independent of
v) Poisson random process, and cluster galaxies, whose distribution is assumed to
be a Poisson process whose intensity is Gaussian-distributed in v, i.e.,
I(vi|...) = NclusN(vi|vclus, σ2
vi + σ2
clus) + Nbkg
Δv ,
(12.4)
where Δv is the velocity range over which velocities are considered (say, ±5000
km s−1 from the cluster preliminary velocity centre), σvi is the velocity error, Nclus
and Nbkg are the number of cluster and background galaxies, and vclus and σclus
are our most interesting quantities: the cluster redshift and velocity dispersion.
Simple algebra shows that the likelihood of independently and identically dis-
tributed data, p(v|I(v)), is
p(v|I(v)) ∝

i
I(vi|...) e−
	
v I(v|...).
(12.5)
Combined with prior probability distributions for the parameters, this likelihood
function yields, via Bayes’ theorem, the posterior distribution for the function
parameters, given the data. Uniform priors, zeroed at unphysical values of the
parameters, are often adequate for the samples available. Markov chain Monte
Carlo with a Metropolis sampler (Metropolis et al. 1953) may be used to sam-
ple the posterior. The chain provides a sampling of the posterior that directly gives
credible intervals for whatever quantity, either for the parameters or any derived
quantity: for an interval at the desired credible level it is simply a matter of taking
the interval that includes the relevant percentage of the samplings.
Most literature estimates of cluster velocity dispersions are, instead, based on
the family of estimators presented by Beers, Flynn and Gebhardt (1990), often
called ‘robust’. We now compare the performances of the ‘robust’ method and the
Bayesian approach.
Let us consider a simulated ‘cluster’ having σv = 1000 km s−1 composed of
500 galaxies with Gaussian-distributed velocities, superposed over a background of
500 uniformly distributed (in velocity) interlopers, within ±3000 km s−1. Note that
within 1000 km s−1 from the cluster centre there are on average 500 × 0.68 = 340

12.5 Measuring a width in the presence of a contaminating population
273
Fig. 12.3. Perturbed velocity distribution (solid line), given by Eq. (12.6), and a Gaussian
with identical ﬁrst two moments (dashed line). The former is used to generate hypothetic
data, the latter is assumed to estimate σv.
members and 500/3 = 83 background galaxies, i.e., the contamination is here just
20%. The large sample size has been adopted to let the data speak for themselves.
Applying the methods of Beers et al. (1990) yields ˆσv = 1400 km s−1, which is
an excessively large estimate of σv (and hence of mass). The posterior mean is
940 ± 85 km s−1 which is closer to the ‘true’ (input) value. This simulation shows
the presence of a systematic bias in the Beers et al. estimator, even for a large
sample. Actually, the bias is independent of the sample size, provided the relative
fraction of cluster and interlopers is maintained.
We now acknowledge that in real-world experiments, we do not precisely know
the model from which data are drawn (e.g., Is the velocity distribution perfectly
Gaussian? Does it have more power in the wings or is it slightly tilted?). Let us
therefore suppose that cluster substructure perturbs the velocity distribution, which
we now assume to be described by
p(v) ∝ev/1000 
1 + e2.75v/1000−1
,
(12.6)
depicted in Figure 12.3 (solid line). The function has ﬁrst and second moments
(mean and dispersion) equal to −460 and 1130 km s−1, respectively, excess kurto-
sis and a non-zero skewness. We simulate 1000 (virtual) clusters of 25 members
each (and no interlopers) drawn from the distribution above (Eq. 12.6), but we
compute the velocity dispersion using Eq. (12.4), i.e., with a likelihood function

274
A Bayesian approach to galaxy evolution studies
0.03
0.02
p(quoted error)
0.01
0
0
100
200
300
‘robust’
Bayes
true Bayes
true robust
quoted error (km s–1)
400
500
0.002
0.0015
p(s)
0.001
0
0
0.0005
2500
‘robust’
Bayes
input
mean
(km s–1)
s
Fig. 12.4. Left panel: Comparison between the distributions of the quoted error (his-
tograms) by ‘robust’ (biweight estimator of scale) and by our Bayesian method for 1000
simulations of 25 galaxies uncontaminated by background. The ‘robust’ error estimate is
noiser (the histogram is wider) and somewhat biased, because the histogram is mostly
on the left of the true error (given by the standard deviation of returned velocity disper-
sions). Bayesian errors (posterior standard deviation) are less biased and show a lower
scatter. Right panel: The true (input) value of the velocity dispersion, and the histogram of
recovered values by the biweight estimator of scale (right histogram) and by our Bayesian
method (left histogram) for 1000 simulations of a sample of 25 galaxies, 50% contami-
nated by background. In the presence of a background, the ‘robust’ estimate of the velocity
dispersion is biased.
appropriate for members drawn from a Gaussian, to make our study more re-
alistic. The average of the determined posterior means is 1140 km s−1 (vs. the
1130 km s−1 input value) with a standard deviation of 185 km s−1. The uncertainty
(posterior standard deviation), averaged over simulations, is 163 km s−1, close (as
it should be) to the scatter of the posterior means. The uncertainty has a negligi-
ble scatter, 18 km s−1, indicating the low noise level of each individual uncertainty
determination. The uncertainty of the dispersion error is four times better with a
Bayesian estimation than using the methods of Beers et al., displaying a scatter
of 70 km s−1, and returning uncertainties as small as 73 km s−1 and as large as
865 km s−1 for data that are supposed to give a unique, ﬁxed, value of uncertainty
(see the left panel of Figure 12.4).
As a more difﬁcult situation, we now consider a sample drawn, as before, from
a distribution different from the one used for the analysis, but furthermore ∼50%
contaminated by interlopers and consisting of half as many members: 13 galax-
ies are drawn from the distribution above (Eq. 12.6), superposed with a back-
ground of 12 galaxies uniformly drawn from ±5000 km s−1. Within 1130 km s−1
(i.e., 1σv) from the centre the average contamination is about 20%. The average
of the obtained posterior means is 1160 km s−1 (vs. the 1130 km s−1 input value).

12.6 Fitting a trend in the presence of outliers
275
The average uncertainty is 390 km s−1, with a low (80 km s−1) scatter. The bi-
weight estimator returns, on average, a strongly biased estimate, 2135 km s−1, see
Figure 12.4.
As mentioned, mixtures often arise in astronomy and our Eqs. (12.4) and (12.5)
hold equally for any Poisson signal superposed on a background, such as the distri-
bution of galaxies in colour or the spatial distribution of X-ray photons, or galax-
ies, or whatever. We just need to re-name variables with names appropriate to the
measurand and, eventually, consider a more complex model, for example if the
background distribution is not uniform. In fact, the solution illustrated in this sec-
tion has been developed to measure the X-ray core radius of a cluster of galaxies
barely detected (Andreon et al. 2008) and later used to measure the cluster velocity
dispersion.
12.6 Fitting a trend in the presence of outliers
Now we consider an apparently different problem: we observed some quantities x
and y and we want to estimate some parameters describing how these two quanti-
ties vary as a function of each other. In astronomy, examples of these regressions
are the Tully–Fisher, Faber–Jackson, and colour–magnitude relations, the funda-
mental plane, cluster scaling relations, and the Ghirlanda relation (for gamma-ray
bursts). Many articles present their own method for the determination of these
parameters (direct, inverse, orthogonal, bivariate correlated error and intrinsic scat-
ter, measurement errors and intrinsic scatter ﬁt). The Bayesian approach allows a
simple solution, even in the difﬁcult case of a linear ﬁt in the presence of het-
eroscedastic (i.e., of different magnitude) errors on both variables and an intrinsic
scatter (i.e., not accounted for by experimental errors), and censored or truncated
data. In such a case, and for an ignorable data collection process (see below) and
for variables having names appropriate for the colour–magnitude relation, slope
a, intercept c, intrinsic scatter σintr of the colour–magnitude relation, and Gaus-
sian photometric errors, the likelihood is a Gaussian (e.g., D’Agostini 2003, 2005;
Gelman et al. 2004):
p(mi, coli|a, c, σintr, no bkg) ∝N(coli|a mi + c, σ2
intr + σ2
coli + a2σ2
mi), (12.7)
where σmi and σcoli are the errors on magnitude and colour of the ith galaxy.
The solution is quite intuitive: the colour–magnitude relation has a width given by
the sum in quadrature of the intrinsic scatter, colour errors and magnitude errors
propagated on the colour (via the slope a). In spite of the solution’s simplicity,
many pages are spent in journals trying to decide which approximate procedure
(usually far more complicated than the equation above) should be used in which
cases, all of which can be shown to be approximations of Eq. (12.7). Of course, a
change of variable names makes the result useful for whatever scaling relation.

276
A Bayesian approach to galaxy evolution studies
Fig. 12.5. Three simulated datasets of 25 objects (points), true trends from which data are
generated (solid line), recovered trend by BCES (dotted line) and mean (Bayesian) model
(dashed line). In our 1000 simulations, BCES results worse than those shown in the central
and right panels occur in about ten per cent of the cases (see text for details).
To avoid recourse to maths, let us perform numerical simulations and compare
the Bayesian approach and the state-of-the-art non-Bayesian astronomical method,
BCES (Akritas & Bershady 1996). BCES accounts for intrinsic scatter and for
heteroscedastic errors. We considered a sample of 25 objects obeying a linear trend
of slope a = 5 with an intrinsic dispersion σintr = 1. The data have Gaussian
errors, σx = 1 and σy = 0.4. In detail, the true x values have been drawn from
a Gaussian having στ = 1 centred on μτ = 0. The true y values are given by
y = 5x (i.e., c = 0 in Eq. (12.7)). Observed x values and y values are computed
by adding to each true x and y some noise (a Gaussian variate with σx = 1 and
σy = 0.4, respectively). Because of the intrinsic scatter, y is perturbed by adding
a Gaussian variate with σintr = 1. Figure 12.5 shows three simulated datasets.
Qualitatively, these plots look similar to, or better than, many LX −σv relations
seen in astronomical journals. We produced 1000 simulations of 25 data points. For
each simulation we compute the slope and slope error as determined by BCES. We
also compute the slope posterior mean and standard deviation, assuming uniform
priors for all parameters but for the slope a, for which we take a uniform prior on
the angle α (a = tan α). Since in our problem σx is comparable to the x range
and the x distribution is far from being uniform (in the statistical jargon ‘the data
collection process is not ignorable’), the likelihood continues to be described by
a Gaussian, as Eq. (12.7), but in a two-dimensional (y, x) space. Performing the

12.6 Fitting a trend in the presence of outliers
277
0.5
0.4
0.3
0.6
0.4
0.2
0
0.2
0.1
0 –10
0
Estimated slope
10
20
30
0
quoted slope error
10
20
30
40
50
p(slope)
p(quoted slope error)
Bayes
true Bayes
true BCES
BCES
BCES
input
Bayes
Fig. 12.6. Comparison between BCES and the Bayesian approach for our linear regression
problem. We considered 1000 simulations of a sample of 25 objects uncontaminated by
interlopers. Left panel: BCES sometimes returns badly wrong slope estimates: the BCES
histogram distribution shows outliers, cumulated at −10 and 30. Right panel: True value of
the slope error (vertical arrows), as measured by the scatter of returned slope minus input
slope, and distribution of quoted errors (histograms). BCES is overly optimistic about the
quality of its error; the large majority of the error estimates are small when the scatter
between input and output slope is large. Furthermore, BCES displays a large scatter in the
returned error, for data samples supposed to give identical values of uncertainty.
algebra associated with the matrix product gives a Gaussian N(μ, σ2) with para-
meters μ and σ2:
σ2 = (a2σ2
τ + σ2
intr + σ2
y)(σ2
τ + σ2
x) −a2σ4
τ,
(12.8)
μ = (σ2
τ + σ2
x)(yi −c −aμτ)2 −2aσ2
τ(yi −c −aμτ)(xi −μτ)
+ (a2σ2
τ + σ2
intr + σ2
y)(xi −μτ)2.
(12.9)
As στ →∞, the likelihood converges to Eq. (12.7), i.e., Eq. (12.7) is an approxi-
mation of the present equation. Of course, the parameters used to produce the data
(στ, μτ, σintr, a and c) are assumed to be unknown in both analyses.
The left panel of Figure 12.6 shows that both the BCES method and the Bayesian
approach return slopes whose distribution is centred on the input value, at least for
our setting. However, BCES sometimes returns slopes very different from the in-
put one (study the histogram wings, in particular around −10, 30 where we have
cumulated more extreme values). The Bayesian approach does not show such catas-
trophic failures. The right panel of Figure 12.6 shows the distribution of the quoted
errors. The important thing here is not how large (or small) the error claimed by a
method is, i.e., the location of the plotted histogram, but the veracity of the claimed
error, i.e., whether the quoted error distribution is located near or far from the
true error (vertical arrow). The true error is computed as the scatter between the

278
A Bayesian approach to galaxy evolution studies
returned slope and the input slope. On average, BCES optimistically estimates er-
rors by a large factor, mainly because in ten per cent of the cases it presents a catas-
trophical failure. The Bayesian method performs better in this respect: the quoted
slope uncertainty is equal to the scatter between the input and output slopes, as it
should be. Second, BCES displays a large scatter in the quoted slope error, for data
samples supposed to give similar values of uncertainty.
To summarize, although BCES is not systematically in error, in ten per cent
of our simulations, BCES returns badly wrong slopes with badly underestimated
errors. In a real application, true values are unknown, and in such a case there is no
way of knowing whether the BCES result is one of the frequently good values or
a bad one. The Bayesian method better performs because it is better behaved, and
there are no such catastrophic failures.
As formulated above, the problem does not account for our everyday experience:
real samples are contaminated by interlopers, i.e., objects unrelated to the ones we
are interested in. Now our model will be a mixture of two regressions, one carry-
ing the signal (the cluster colour–magnitude relation) and the other describing the
background (galaxies, objects on the line of sight), with the usual difﬁculty that we
do not know which galaxy belongs to the cluster and which one is simply projected
along the cluster line of sight. A real case (the cluster Abell 1185, from Andreon
et al. 2006a) is shown in Figure 12.7. The distribution of background galaxies in
the m, col space is not uniform, and therefore the background is modelled by an
inhomogeneous process, B(mi, coli|m, col). Therefore, the likelihood of the ith
galaxy, p(mi, coli|a, c, σintr), is given by the mixture of two distributions:
p(mi, coli|a, c, σintr, α, M∗φ∗) = ΩjB(mi, coli|m, col)
(12.10)
+ δcΩjN(coli|a mi + c, σ2
intr + σ2
coli + a2σ2
mi)S(mi|α, M∗φ∗).
In this equation we considered the usual case, where we have a control ﬁeld
in the form of data from a sky region uncontaminated by the cluster contribution.
In such a case, δc = 1 for cluster datasets, δc = 0 for the other datasets, Ωj is
the studied solid angle. Otherwise, it is just a matter of replacing δc with a radial
proﬁle. S is the usual Schechter (1976) function, with α, M∗and φ∗parameters,
that describes the luminosity function of galaxies. We have also assumed that the
data collection model is ignorable, for mathematical convenience.
As in Eq. (12.5), the likelihood of independently and identically distributed data
is given by the product, over the data mi, coli of the individual likelihood terms.
As shown there, the likelihood includes an integral term, given by the integral of
the model over the values ranges. The integral should be performed on the appro-
priate colour and magnitude ranges (those accessible to the data) and it is equal
to the expected number of galaxies. This term disfavours models that predict a

12.6 Fitting a trend in the presence of outliers
279
Fig. 12.7. Left panels: Colour–magnitude diagrams for background galaxies (upper panel)
and cluster+background galaxies (lower panel). These are true data for the cluster Abell
1185, presented in Andreon et al. (2006a). The solid line is the mean colour–magnitude
relation of cluster galaxies computed as described in the text. The shaded region marks
the 68% highest posterior interval. Right panel: Posterior probability distribution of the
colour–magnitude intrinsic scatter. The jagged nature of the distribution is due to the ﬁnite
lengh of the MCMC chain. A Gaussian with ﬁrst two moments matching the distribution
is overplotted to guide the eye.
number of galaxies very different from the observed one. If errors on m (mag) are
not negligible, S in Eq. (12.10) should be replaced by the convolution between
the Schechter function and the error function. The inference proceeds as usual,
by choosing a prior, computing the posterior, and summarizing the result of the
computation above with a few numbers, those of scientiﬁc interest.
The problem of determining an intrinsic scatter around a linear trend in the
presence of outliers or a background population is so difﬁcult that, to our best
knowledge, there are no non-Bayesian solutions to be compared with our Bayesian
approach. We cannot, therefore, simulate some data and compare the performances
of different methods because of the lack of a contender.
Had we (mis)used BCES, then the obtained slope of the colour–magnitude rela-
tion shown in Figure 12.7 would be completely wrong, and equal to the one of the
background population, outnumbering, by a factor of four, the cluster population.
This occurs because BCES is not built to be robust against a contaminating popu-
lation. An ad hoc solution often used in astronomical papers is to remove the slope
dependency by subtracting off an expected one (e.g., one observed at z = 0, assum-
ing no slope evolution), measuring the scatter using ‘robust’ methods, as described

280
A Bayesian approach to galaxy evolution studies
in the previous section, and ﬁnally quadratically subtracting colour errors from the
measured scatter, following Stanford et al. (1998). This procedure often leads to
intrinsic scatter with 68% error bars extending to negative values, including some
examples in Stanford et al. (1998). We have already discussed the shortcomings of
using the ‘robust’ estimate of the scatter. The Bayesian method does not require ad
hoc methods, does not make assumptions on the trend slope, and always returns
positive intrinsic dispersions, as in the case of Abell 1185 shown in Figure 12.7.
Readers interested in fractions fb bounded in the [0, 1] range, or hardness ratios,
(H −S)/(H + S) bounded in the physical range for data contaminated by a back-
ground, may consult Andreon et al. (2006b) and D’Agostini (2004). The hardness
ratio has the same mathematical properties of 1 −2fb, i.e., one minus twice the
(blue) fraction discussed in these two works.
12.7 What is the number returned by tests such as χ2, KS, etc.?
Many articles measure the ‘probability of rejecting the null hypothesis’ using some
statistical tests, for example, Kolmogorov–Smirnov’s, Kendall’s, Spearman’s rank
correlation, F-, Student’s t-, Wilcoxon rank, and χ2 tests. Many of us have noted
oddities with the numbers (called p-values) returned by them: by taking two sta-
tistical tests we sometimes found widely different ‘probabilities’, e.g., 0.001 and
0.861. How can this be the case, since the desired result is a single unique value?
In our example, which one is the good probability, the one rejecting the null or the
other one? The mere existence of a variety of tests, as opposed to a single one, is
an indication that no test always gives the desired number. Actually, p-values are
not the probability of the hypothesis, which is the desired probability. They are the
probability of observing more discrepant values of the chosen statistic for hypo-
thetical data drawn from the null hypothesis, that is, the probability of rejecting the
null hypothesis when it is true. There is nothing strange that two different statistics
(measures, say, of height and width) of data drawn under the null hypothesis takes
different values.
The difference between the p-values and probability of the hypothesis can be
better understood with an astronomical example: the detection of faint sources. In
such a case, the null hypothesis to reject is ‘no source is there’. Let I0 be the ﬂux
measured at the target position. A usual way to compute the detection conﬁdence
is by measuring how frequently one observes larger values, >I0, under the null,
i.e., in areas free from sources: p(>I0|background). The p-value is, precisely, the
measured frequency. For many famous tests, like those mentioned at the start of
the section, the probability distribution of the test statistic is analytically known
and there is no need of further data (the background) in order to compute the dis-
tribution of the test statistics.

12.8 Summary
281
Let us suppose we have found a p-value of 0.003, i.e., that measurements free
of sources gives p(>I0|background) = 0.003. Does this mean that the target is
real at one minus the p-value conﬁdence, p(source|I0) = 1 −0.003 = 0.997,
i.e., is real at 99.7% conﬁdence? Certainly not. Qualitatively, if sources ﬁll a small
portion of the sky, there is a lot of sky left to the background. Then, statistical
ﬂuctuations of the background, even rare ones, may overwhelm the number of true
sources. In such a case, only a very tiny fraction of detections are true, not 99.7%
as the p-value leads us to believe. More quantitatively: let x be the portion of sky
occupied by sources (when observed in the same observational set-up that gives a
p-value of 0.003), and N the number of independent beams in the sky. Note that
x is the probability a priori that there is a source in a beam. Then, xN beams are
occupied by sources and (1 −x)N are not. Assuming a 100% detection efﬁciency,
xN are true sources detected, and 0.003(1 −x)N will be instead false positive
detections. Thus, there will be xN + 0.003(1 −x)N detections, but among them
only xN are true sources. The probability of being a true source, p(source|I0),
is given by the fraction of true detected sources over total number of detections:
x/(x + 0.003(1 −x)). If x = 0.07%, a value appropriate for typical Chandra
exposures, then sources believed to be detected at 99.7% conﬁdence (or better, with
a p-value of 0.003) have only a 19% probability of being real. Adopting instead a
5% p-value, we end up with a catalogue composed of entries that are junk 99 times
out of 100, instead of being true sources 95% of the time, as the p-value suggests.
Only in fortunate cases (appropriate values of x) will one have similar numbers for
the p-value and the probability of rejecting the null hypothesis. Therefore, these
two probabilities are conceptually different and take different values.
As shown in the example, the desired probability does depend on the a priori
probability of the (null) hypothesis (x in the example). However, virtually all non-
Bayesian astronomical papers compute p-values but call them ‘the probability of
rejecting the null hypothesis’. For example, in testing the reality of a trend, the
Spearman rank correlation test is often used, and the one minus the p-value is
quoted as the probability of rejecting the null (‘no trend’) hypothesis. Such a prac-
tice ignores the essential role played by the a priori probability of the competing
hypothesis, which, in principle, may convert a ‘95%’ conﬁdent result into an incon-
clusive result, as in our example. The Bayesian approach is based on probabilities
for the hypothesis – it cannot ignore them – and in our example Bayes’ theorem
takes a form very similar to the one we have used to evaluate the desired probability.
12.8 Summary
The Bayesian approach solves some difﬁculties encountered with other procedures.
It works in the regime of typical researcher activity: when looked-for effects are

282
A Bayesian approach to galaxy evolution studies
marginally signiﬁcant, or near boundaries, such as when the small intrinsic dis-
persion of the colour–magnitude relation is to be determined, or when there is no
agreement among astronomers how to set up the right procedure, as for the re-
gression problem in the absence of a background. It also offers a solution when
no other is there, as in the case of the ﬁt of a trend in the presence of a contam-
inating population. It works when otherwise obtained results are unsatisfactory,
as for velocity dispersions or for cases when other procedures return unphysical
values. The Bayesian approach already includes corrections for biases, as for the
Eddington bias. The ultimate reason for its good performance is highlighted in two
idealized cases at the start of this chapter: (a) it obeys the sum axiom of probability
and thus averages (marginalizes) over unknown quantities, instead of maximizing
the value of some ad hoc estimators; and (b) it performs inferences following the
Bayes’ theorem instead of considering priors as something to be avoided. Finally,
the Bayesian approach clariﬁes what other methods are actually computing, for
example, the meaning of the number returned by Kolmogorov–Smirnov, χ2, and
Wilcoxon rank tests.
Let us conclude this chapter by remembering that the scientiﬁc method suggests
preference for a procedure known to work over one whose reliability is uncertain.
References
Akritas, M. G. and Bershady, M. A. (1996). Astrophys. J., 470, 706.
Andreon, S. (2008). Mon. Not. Roy. Astron. Soc., 386, 1045.
Andreon, S. and Harn, M. (2009). Mon. Not. Roy. Astron. Soc., in press.
Andreon, S., Cuillandre, J.-C., Puddu, E. and Mellier, Y. (2006a). Mon. Not. Roy. Astron.
Soc., 372, 60.
Andreon, S., Quintana, H., Tajer, M., Galaz, G. and Surdej, J. (2006b). Mon. Not. Roy.
Astron. Soc., 365, 915.
Andreon, S., De Propris, R., Puddu, E., Giordano, L. and Quintana, H. (2008). Mon. Not.
Roy. Astron. Soc., 383, 102.
Beers, T., Flynn, K. and Gebhardt, K. (1990). Astron. J., 100, 32.
D’Agostini, G. (2003). Bayesian Reasoning in Data Analysi: A Critical Introduction.
Singapore: World Scientiﬁc Publishing.
D’Agostini, G. (2004). arXiv:physics/0412069.
D’Agostini, G. (2005). arXiv:physics/0511182.
Eddington, A. S. (1913). Mon. Not. Roy. Astron. Soc., 73, 359.
Gelman, A., Carlin, J., Stern, H. and Rubin, D. (2004). Bayesian Data Analysis. London:
Chapman & Hall/CRC.
Kenter, A. et al. (2005). Astrophys. J. Supp., 161, 9.
Metropolis, N., Rosenbluth, A., Rosenbluth, M., Teller, A. and Teller, E. (1953). J. Chem.
Phys., 21, 1087.
Schechter, P. (1976). Astrophys. J., 203, 297.
Stanford, S. A., Eisenhardt, P. R. and Dickinson, M. (1998). Astrophys. J., 492, 461.

13
Photometric redshift estimation: methods and
applications
Ofer Lahav, Filipe B. Abdalla and Manda Banerji
13.1 Introduction
Estimating the distance to an astronomical object is a fundamental problem in as-
tronomy. Distances to galaxies are usually estimated from their redshifts:
z = λobs
λem
−1 ,
(13.1)
where λobs and λem are the observed and emitted wavelengths. Ideally the redshift
is derived from a high-resolution spectrum. The alternative is to use the photo-
metric redshift technique. Using the colours of a galaxy observed in a selection of
medium- or broad-band ﬁlters, we can gain a crude approximation of the galaxy’s
spectral energy distribution (SED), from which its redshift and spectral type may
be found. In Figure 13.1 we show how the colours of a galaxy change with redshift,
and the major features that can be picked up in each of the bands.
The technique is very efﬁcient compared with spectroscopic redshifts, since the
signal-to-noise in broad-band ﬁlters is much greater than the signal-to-noise in a
dispersed spectrum. Furthermore, a whole ﬁeld of galaxies may be imaged at once,
while spectroscopy is limited to individual galaxies or those that can be positioned
on slits or ﬁbres. However, photometric redshifts are only approximate at best and
are sometimes subject to large errors. For many applications though, large sample
sizes are more important than precise redshifts and photometric redshifts may be
used to good effect.
Photometric redshifts date back to Baum (1962). For reviews see Weymann et al.
(1999) and Koo (1999). The methods have been used extensively in recent years on
the ultra-deep and well-calibrated Hubble Deep Field observations (e.g., Connolly,
Szalay & Brunner 1998).
283

284
Photometric redshift estimation: methods and applications
Fig. 13.1. The SED of an elliptical galaxy at redshift z = 0 (solid line) and the same
galaxy at z = 3 (dashed line) as seen through a set of broad-band optical (griz) and near-
infrared (JHKs) ﬁlters. The Balmer break at 4000 ˚A moves from the g-band at z = 0 to
the near-infrared bands as the galaxy is redshifted to z = 3. (Figure courtesy of Eduardo
Cypriano.)
The photo-z approach has regained popularity with the planning of wide-ﬁeld
deep imaging surveys, e.g., SDSS1, CFHTLS2, KIDS, Subaru Hyper-Suprime Cam,
Pan-STARRS3, the Dark Energy Survey4 (combined with the VISTA Hemisphere
Survey5), LSST6, DUNE/Euclid7, SNAP8, PAU9 and Skymapper10. The accuracy
with which photometric redshifts can be calculated for these surveys is crucial for
the determination of cosmological parameters such as the dark energy equation of
state.
1 http://www.sdss.org
2 http://www.cfht.hawaii.edu/Science/CFHLS
3 http://pan-starrs.ifa.hawaii.edu/public/
4 https://www.darkenergysurvey.org
5 http://www.ast.cam.ac.uk/∼rgm/vhs
6 http://www.lsst.org
7 http://www.dune-mission.net
8 http://snap.lbl.gov/
9 http://www/ice.csic.es/pau/
10 http://www/mso.anu.edu.au/skymapper/

13.2 Template methods
285
For a statistician, photo-z estimation is an inverse problem: going from the
colour space to the redshift, type and other galaxy properties. There are two com-
mon approaches to the problem: (1) template methods (e.g., HYPERZ) and (2)
training methods (e.g., ANNZ) as described below. We note that training methods
can also be used on synthetic data, so they are not necessarily ‘empirical’.
We also note that there could be different deliverables required from the photo-z
estimation technique, e.g., a catalogue where a photo-z and associated error is given
per galaxy, or, as required for certain applications, just the redshift distribution
N(z). A ﬁnal comment is that different techniques have been developed for the
photo-z for ﬁeld galaxies and cluster galaxies. The focus of this review is on ﬁeld
galaxies.
13.2 Template methods
The most basic approach is the template-ﬁtting technique. A commonly used pack-
age is HYPERZ (Bolzonella et al. 2000). This involves compiling a library of
template spectra, either theoretical SEDs from population synthesis models (e.g.,
GISSEL, Bruzual & Charlot 1993, BC hereafter) or empirical SEDs (e.g., Coleman,
Wu & Weedman 1980, CWW hereafter). The expected ﬂux through each survey
ﬁlter is calculated for each template SED on a grid of redshifts, with corrections
for the interstellar medium, intergalactic medium and Galactic extinction where
necessary. A redshift and spectral type are estimated for each observed galaxy by
minimizing
χ2
with
respect
to
redshift,
z,
and
spectral
type,
SED,
where
χ2(z, SED) =

i
fi −α(z, SED)ti(z, SED)
σi
2
,
(13.2)
fi is the observed ﬂux in ﬁlter i, σi is the error in fi, ti(z, SED) is the ﬂux in ﬁlter
i for the template SED at redshift z. α(z, SED), the scaling factor normalizing the
template to the observed ﬂux, is determined by minimizing Eq. (13.2) with respect
to α, giving
α(z, SED) =

i
fiti(z,SED)
σ2
i

i
ti(z,SED)2
σ2
i
.
(13.3)
The template-ﬁtting photometric redshift technique makes use of the available
and reasonably detailed knowledge of galaxy SEDs, and in principle may be used
reliably even for populations of galaxies for which there are few or no spectro-
scopically conﬁrmed redshifts. However, crucial to its success is the compilation
of a library of accurate and representative template SEDs. Empirical templates are

286
Photometric redshift estimation: methods and applications
typically derived from nearby bright galaxies, which may not be truly representa-
tive of high-redshift galaxies. Conversely, while theoretical SEDs can cover a large
range of star-formation histories, metallicities, dust extinction models, etc., not all
combinations of these parameters (at any particular redshift) are realistic, and the
ad hoc inclusion of superﬂuous templates increases the potential for errors when
using observations with noisy photometry.
13.3 Bayesian methods and non-colour priors
An extension of the above likelihood (χ2) approach is to incorporate priors, using
the Bayesian framework. Benitez (2000) formulated the problem as follows. The
probability of a galaxy with colour C and magnitude m having a redshift z is
p(z|C, m) = p(z|m)p(C|z)
p(C)
∝p(z|m)p(C|z),
(13.4)
where the term p(C|z) is the conventional redshift likelihood employed, e.g., by
HYPERZ, and p(C) is just a normalization. The new important ingredient is
p(z|m), which brings in the prior knowledge of the magnitude–redshift distribu-
tion. With the aid of the extra information (prior), this approach is effective in
avoiding catastrophic errors of placing a galaxy at a wrong and unrealistic redshift.
As already mentioned, photo-z estimation can be viewed as mapping from colour
space to a galaxy’s redshift. In fact, Jain, Connolly and Takada (2007) have pro-
posed a method called ‘colour tomography’. The idea is to use limited colour in-
formation from two or three bands to derive a coarse redshift distribution N(z),
thus bypassing the need for a photo-z per galaxy.
Colour information is not the only input that can be used by a photometric red-
shift method. Structural properties of a galaxy such as its size (e.g., Firth, Lahav &
Somerville 2003), concentration indices (e.g., Oyaizu et al. 2008) and radial light
proﬁles (e.g., Wray & Gunn 2008) have previously been used as inputs. These are
useful as the size, for example, is a function of distance and therefore can be used
to get redshift information. We also note that galaxy magnitudes can be measured
using different aperture sizes. This will result in different noise levels in the galaxy
photometry and therefore colour gradients.
Recently, a new idea has been proposed – to use the galaxy surface brightness
as a prior (e.g., Kurtz et al. 2007; Stabenau, Connolly & Jain 2008). As the surface
brightness dimming is proportional to (1 + z)−4 in any conventional cosmology, it
can serve as a natural prior. Indeed, using this prior eliminates a large fraction of
outliers.

13.4 Training methods and neural networks
287
13.4 Training methods and neural networks
Another approach can be used when one has a sufﬁciently large (e.g., ∼100–1000,
depending on the redshift range) and representative sub-sample with spectroscopic
redshifts. Then one can ﬁt a polynomial or other function mapping the photomet-
ric data to the known redshifts and use this to estimate redshifts for the remain-
der of the sample with unknown redshifts (e.g., Connolly et al. 1995; Brunner,
Szalay & Connolly 2000; Sowards-Emmerd et al. 2000). With this approach, er-
rors in the estimated redshifts may also be calculated analytically or via Monte
Carlo simulations.
An extension of the latter approach is to use artiﬁcial neural networks (ANNs).
ANNs have been used before in astronomy for, amongst other applications, galaxy
morphological classiﬁcation (e.g., Naim et al. 1995; Lahav et al. 1996), morpho-
logical star/galaxy separation (e.g., Bertin & Arnouts 1996; Andreon et al. 2000)
and stellar spectral classiﬁcation (e.g., Bailer-Jones, Irwin & von Hippel 1998).
Essentially, an ANN takes a set of inputs (e.g., logarithms of ﬂuxes – i.e., magni-
tudes – in different ﬁlters) for each object, applies some non-linear function, and
outputs a value (e.g., the estimated redshift). The ANN is ﬁrst trained, i.e., the co-
efﬁcients (weights) of the function are optimized, by using a training set where
the desired output is known. The ANN may then be used on any number of other
objects with similar inputs (i.e., magnitudes in the same ﬁlter set) but unknown
outputs (i.e., redshifts).
An ANN comprises a set of input nodes, one or more output nodes, and one
or more hidden layers each containing a number of nodes (Figure 13.2); see, e.g.,
Bishop (1995) for more details. For example, 5:2:1 takes 5 inputs (colours), has
2 nodes in a single hidden layer and gives a single output (redshift). The nodes
are connected and each connection carries a weight, which together comprise the
vector of coefﬁcients w that are to be optimized. The input parameters for each
object are represented by the vector x (e.g., the magnitudes in a set of ﬁlters).
Given a training set of inputs xk and desired outputs zk (e.g., the redshift), the
ANN is optimized by minimizing the cost function
E = 1
2

k
[zk −F(w, xk)]2.
(13.5)
The function F(w, xk) is given by the network. A function gp is deﬁned at each
node p, taking as its argument,
up =

j
wjxj,
(13.6)
where the sum is over the input nodes to p. These functions are typically taken (in
analogy to biological neurons) to be sigmoid functions such as gp(up) = 1/[1 +

288
Photometric redshift estimation: methods and applications
Fig. 13.2. A schematic diagram of an ANN with input nodes taking, for example, magni-
tudes mi = −2.5 log10 fi in various ﬁlters, a single hidden layer, and a single output node
giving, for example, redshift z. The architecture is n:p:1 in the notation used in this chapter.
Each connecting line carries a weight wj. The bias node allows for an additive constant in
the network function deﬁned at each node. More complex nets can have additional hidden
layers. (Illustration from Collister & Lahav 2004.)
exp(−up)], which is used here. An extra input node – the bias node – is automati-
cally included to allow for additive constants in these functions. The combination
of these functions over all the network nodes makes up the function F. The algo-
rithm takes as its input a network architecture, a training set and a random seed to
initiate the weight vector w, and uses an iterative quasi-Newton method (see, e.g.,
Bishop 1995) to minimize the cost function. To ensure that the weights are regular-
ized (i.e., they do not become too large), an extra quadratic cost term is commonly
added to Eq. (13.5). After each training iteration, the cost function is evaluated
on a separate validation set. After a chosen number of training iterations, training
terminates and the ﬁnal weights chosen for the ANN are those from the iteration
at which the cost function is minimal on the validation set. This is useful to avoid
over-ﬁtting to the training set if the training set is small. The Neural Network ap-
proach (‘ANNZ’) was implemented by Firth, Lahav and Somerville (2003), and by
Collister and Lahav (2004).

13.5 Errors on photo-z
289
While choosing a template library that is representative is a source of concern for
the template-ﬁtting method, ANNs automatically ﬁt the true range of galaxy SEDs.
Another potential advantage of ANNs relative to the template-ﬁtting method is that
the weights applied to each ﬁlter may be more optimal than simple χ2 weighting.
In addition, one can also feed in other observational inputs, such as image size
or surface brightness, morphology and concentration parameters, where such data
are available. We note that a single input node with a magnitude input can give
redshift information, simply because the ﬂux of a galaxy is inversely proportional
to the distance squared.
Can ANNs be viewed as Bayesian? If the training set is ‘faithful’ then the ANN
learns from a dataset very similar to the target distribution. If the training set is
not representative then the method may result in biases (see below). See Bishop
(1995) and Lahav et al. (1996) and references therein for the interpretation of
neural networks in the Bayesian framework.
13.5 Errors on photo-z
Each method described above has a way of obtaining error estimates which re-
ﬂects the degree of conﬁdence with which one can rely on the photometric redshift
obtained.
In template methods the χ2 can be computed for each potential redshift value.
This produces a probability distribution function and the errors are obtained from
the points where the probability falls one-sigma away from the best-ﬁt value. As
other techniques use different methods for photometric redshift estimation they
have to obtain error estimates in a different way. For example, in a Neural Network
method, the error estimate is obtained through a chain rule that relates errors in the
original magnitudes to errors on the ﬁnal redshift (Collister & Lahav 2004). The
estimated magnitude errors can then be used to obtain a redshift error estimate.
Recently a more general method based on nearest neighbours has been devised
and can be applied to any sample with a training set independently of the photo-z
estimator used. This method, described in Oyaizu et al. (2007), uses the differences
between the spectroscopic redshift and the photometric redshift of the galaxies
in a training set as a function of the magnitudes. An error estimate is obtained
by a nearest neighbour interpolation of this training data in magnitude space and
produces very good results.
The error, or more generally the probability distribution function attached to
each redshift, is a very important quantity. For example, galaxies with large er-
rors in the photo-z can be removed (‘clipped’) from the sample before using them
for any kind of cosmological analysis. Depending on the science question be-
ing addressed, there will be a trade-off between the amount of clipping applied

290
Photometric redshift estimation: methods and applications
to remove undesired outliers, and the number of galaxies required to keep the
shot-noise levels low. For an illustration of the clipping procedure as applied to
weak lensing and baryon acoustic oscillation studies, see Abdalla et al. (2008a)
and Banerji et al. (2008), respectively.
13.6 Optimal ﬁlters
The performance of any photo-z method depends directly on the input colours and
therefore the number of ﬁlters, their wavelength range (response function) and the
exposure time in each. The choice of optimal ﬁlters is very much dependent on the
science one wants to achieve. Naively speaking, dividing a single ﬁlter into two
with equal light falling in each will always seem more attractive from a photomet-
ric redshift perspective due to the additional colour information obtained compared
with the single-ﬁlter case. However, if one considers the noise associated with read-
ing more CCDs, as well as the extra telescope time needed to collect photons with
two different ﬁlters, then there is an optimal number of ﬁlters which would deliver
the science required. This optimization has been done internally by the DES team
and it was found that for galaxies between redshift 0 and 1.5, an optimal number of
ﬁlters is 4 to 5, with a larger fraction of time spent on the redder ﬁlters to take ac-
count of the extra sky noise present here. Furthermore, if any near-infrared (NIR)
bands are included, the number of NIR bands makes little difference to the ﬁnal
redshift accuracy provided the total integration time is conserved.
Similar conclusions were reached using simulations for deeper surveys in the
NIR, such as the proposed Euclid satellite (Abdalla et al. 2008a). However, these
general arguments break down when the required science becomes more speciﬁc.
For example, if one is interested in high-redshift quasar studies, the Y band be-
comes much more important, as the Lyman break is bracketed by that band at a
redshift of 6. Alternatively, if one is interested in features along the line of sight,
such as baryonic acoustic oscillations (BAOs), then a much larger number of ﬁlters
would be needed, as ﬁlters as wide as 1000 ˚A cannot obtain the resolution to get
the accuracy in redshift needed to see the BAOs along the line of sight. On the
other hand, if low-redshift objects are of interest, then the 4000 ˚A break has to be
bracketed by a u-band ﬁlter and bluer bands become important.
13.7 Comparison of photo-z codes
It is interesting to see how different photometric redshift estimation methods com-
pare in evaluating photo-z’s for the same dataset. There are various publicly avail-
able software packages currently available for photo-z estimation. These are listed
in Table 13.1. In addition to these, many other methods have been used extensively
in the literature, but have not as yet been made public.

13.7 Comparison of photo-z codes
291
Table 13.1. Publicly available software packages for photo-z estimation.
Code
Authors/Web link
Method
HYPERZ
Bolzonella et al. 2000
Likelihood
http://webast.ast.obs-mip.fr/hyperz/
BPZ
Benitez 2000
Bayesian priors
http://acs.pha.jhu.edu/∼txitxo/bpzdoc.html
ANNZ
Collister & Lahav 2004
Neural networks
http://zuserver2.star.ucl.ac.uk/∼lahav/annz.html
IMPZ
Babbedge et al. 2004
Template
http://astro.ic.ac.uk/∼tsb1/Impzlite/ImpZlite.html
ZEBRA
Feldmann et al. 2006
Bayesian,
www.exp-astro.phys.ethz.ch/ZEBRA
Hybrid
KCORRECT
Blanton & Roweis 2007
Model templates
http://cosmo.nyu.edu/blanton/kcorrect/
Le Phare
Arnouts & Ilbert
Template
http://www.oamp.fr/people/arnouts/LE PHARE.html
EAZY
Brammer et al. 2008
Template
http://www.astro.yale.edu/eazy/
We now analyze the result of running four different photo-z algorithms on the
same data sample of ∼5400 luminous red galaxies from the 2dF and SDSS LRG
and QSO (2SLAQ) survey. We use a training set method, ANNZ, a template maxi-
mum likelihood method, HYPERZ with both synthetic (BC) and empirical (CWW)
templates, and a Bayesian method ZEBRA which, in addition to its use of Bayesian
priors, can be thought of as a hybrid template and empirical code. This is because
a training set of galaxies (with available spectroscopic redshifts) are used to re-
pair the templates until a suitable match is obtained between the spectroscopic and
photometric redshifts.
We can easily compare the results from the different codes, as the spectroscopic
redshifts are available for all these objects. As an example, a plot of the spectro-
scopic redshift versus the photometric redshift obtained using the neural network
code, ANNZ, is shown in Figure 13.3. In a similar way, ANNZ was used to create
a catalogue of LRGs with a million photometric redshifts (MegaZ-LRG; Collister
et al. 2007).
In Figure 13.4 we plot the one-sigma scatter on the spectroscopic redshift in
each photo-z bin as a function of the photometric redshift. We also plot the bias in
each photometric redshift bin, deﬁned simply as the mean difference between the
photometric and spectroscopic redshifts in each bin. The training method ANNZ is
almost free from bias due to the presence of a complete and representative train-
ing set across the entire redshift range. However, the one-sigma scatter is higher
for this method in the lowest and highest photo-z bins due to a lack of sufﬁcient

292
Photometric redshift estimation: methods and applications
0.4
0.45
0.5
0.55
0.6
0.65
0.7
0.4
0.45
0.5
0.55
0.6
0.65
0.7
Photometric redshift
Spectroscopic redshift
Fig. 13.3. Photometric redshift versus spectroscopic redshift for 5482 luminous red
galaxies from the 2SLAQ survey. The photometric redshifts were derived using ANNZ.
(From Abdalla et al. 2008b)
numbers of training galaxies in these bins. The template method gives very differ-
ent results based on whether empirical or synthetic templates are used. The syn-
thetic templates seem to produce much better results in this case. The Bayesian
method suffers from a large bias and scatter in the highest photometric redshift
bins, but performs slightly better than the maximum likelihood method across the
entire redshift range.
It is obvious that each photo-z method has its advantages and disadvantages,
depending on the dataset and redshift regime in which it is being applied. Other
empirical methods, such as support vector machines (e.g., Wadadekar 2005) and
kernel regression (e.g., Wang et al. 2007), have also been used for photometric
redshift estimation, but to our knowledge are yet to be made available as public
codes.
13.8 The role of spectroscopic datasets
First, we should distinguish between spectroscopic sets for training methods (e.g.,
ANNZ) and spectroscopic data for calibration of the photometric redshift
distribution obtained by either template or training sets. Typically, a larger spectro-
scopic set is needed for calibration than for training. We also note that one needs

13.8 The role of spectroscopic datasets
293
0.3
0.4
0.5
0.6
0.7
0.8
0.9
0
0.02
0.04
0.06
0.08
0.1
0.12
0.14
0.16
0.18
Photometric redshift
1σ scatter around mean spec−z
Training − ANNz
Template − HYPERZ BC
Template − HYPERZ CWW
Bayesian, Hybrid − zEBRA
0.3
0.4
0.5
0.6
0.7
0.8
0.9
−0.4
−0.3
−0.2
−0.1
0
0.1
0.2
0.3
Photometric redshift
Bias
Training − ANNz
Template − HYPERZ BC
Template − HYPERZ CWW
Bayesian, Hybrid − zEBRA
Fig. 13.4. The one-sigma scatter on the mean spectroscopic redshift in each photometric
redshift bin (upper panel) and the bias (lower panel) as a function of photometric red-
shift obtained after running three different codes on a sample of 5482 luminous red galax-
ies from the 2SLAQ survey. The three types of codes compared are the training method
ANNZ, the maximum likelihood template method HYPERZ, using both synthetic (BC) and
empirical (CWW) templates, and the Bayesian hybrid method ZEBRA that uses a training
set of galaxies with known spectroscopic redshifts to optimize templates. (From Abdalla
et al. 2008b.)

294
Photometric redshift estimation: methods and applications
to consider whether photo-z’s are required for every single galaxy (e.g., to produce
a catalogue) or whether an N(z) distribution will sufﬁce, as for weak lensing and
baryon acoustic oscillation studies.
Let us consider three scenarios. Firstly, when no spectroscopic datasets are avail-
able, one has to use either the template method to derive a photo-z, or the training
methods with synthetic data, based on a model for N(z) derived from additional
observational constraints. However, photometric redshifts derived in this way will
probably be prone to biases.
Secondly, if there is a perfect spectroscopic dataset, which is a random subset of
the photometric data, and provided it is large enough (104−105 spectra), we can
use it both to get a photo-z catalogue via the training method and to calibrate N(z)
for cosmological probes.
Thirdly, if there is an incomplete spectroscopic dataset that is not representative
of the photometric data, but the incompleteness is known as a function of magni-
tude and redshift, a quantitative approach can be used. For a detailed study of such
biases in the photo-z estimate, see Banerji et al. (2008). One approach is to use a
weighting scheme to reconstruct the photo-z N(z) given the incomplete spectra,
therefore generating a photo-z catalogue as in Lima et al. (2008). We note that if
the spectroscopic training set covers only a small fraction of the sky, it may suffer
‘cosmic variance’ and may generate biases in the testing sets.
Another way to derive N(z) is with the aid of clustering information. The idea
is to measure the angular cross-correlation between galaxies with photometric red-
shifts and galaxies with spectroscopic redshifts. This, together with certain as-
sumptions about clustering and biasing, can yield more accurate N(z) (Newman
2008).
13.9 Synergy with cosmological probes
The main utility of photometric redshifts is in their application to cosmological
probes, such as weak lensing tomography and baryon acoustic oscillations. In weak
lensing tomography, the idea is to slice a lensing survey into photometric redshift
bins and to analyze the cosmic shear of high signal-to-noise galaxy images in each
photo-z bin, e.g., as discussed by Ma et al. (2006), Amara and Refregier (2007),
and Abdalla et al. (2008a). Uncertainties in the photometric redshifts will therefore
propagate into errors in the cosmological parameters.
A simple back-of-the-envelope calculation helps to explain the link between
the dark energy parameters, the photometric redshift performance and the num-
ber of spectroscopic redshifts. Consider a distribution of sources selected from a
photometric redshift bin that results in a more complicated (e.g., Gaussian)

13.9 Synergy with cosmological probes
295
distribution with respect to true (spectroscopic) redshift, with mean redshift ¯z and
variance:
μ2 =

(zspec −¯z)2 
.
(13.7)
Assuming Poisson statistics we can predict the variance in the mean redshift ¯z
given Nspec spectroscopic redshifts associated with that photo-z bin:
(δz)2 ≡rms2(¯z) = μ2
Ns
.
(13.8)
We can now crudely model the uncertainty in deriving the constant dark energy
parameter w = P/ρ from weak lensing, if the only uncertainty is due to photo-z
errors:
0000
δw
w
0000 ≈5δz
¯z = 5
¯z
C μ2
Ns
.
(13.9)
The pre-factor of approximately 5 can be estimated from detailed modelling of
the weak lensing power spectrum. It can be justiﬁed qualitatively by examining
the sensitivity of cosmological distance and the linear growth to variations in w
(Peacock et al. 2006).
For example, for a desired fractional error of 1% on w, ¯z = 1 and μ2 = 0.06
(derived from mock catalogues and ANNZ averaged over a range of proposed Eu-
clid optical and NIR bands) we ﬁnd that Ns ≈15 000 in each bin or, for say 10
bins, a total of 150 000 spectroscopic redshifts are required.
Ideally, we would like to know the probability distribution with redshift, pi(z),
exactly from a spectroscopic survey, where the redshift is derived from the spec-
trum of each galaxy. In reality, deep wide-ﬁeld surveys will only provide us with
multi-band imaging data that will allow us to derive photometric redshifts based
on templates and/or spectroscopic training sets. We can relate the probabilities for
the true redshift zspec and the photometric redshift zphot by the Bayesian rule of
conditional probability:
p(zspec, zphot) = p(zspec|zphot)p(zphot) = p(zphot|zspec)p(zspec).
(13.10)
Consider now a sharp cut in a photo-z bin i, i.e., we select only those galaxies in
the range zphot(i) < zphot < zphot(i+1). We can write the probability for the true
redshift distribution resulting from the photo-z slice as
pi(zspec) =
 zphot(i+1)
zphot(i)
p(zphot, zspec) dzphot.
(13.11)
Typically pi(zspec) would have a wide spread, not in the form of a Gaussian, as
asymmetric tails are present due to the photo-z catastrophic errors.

296
Photometric redshift estimation: methods and applications
We can parameterize pi(zspec) directly based on the projection of the photo-z
scatter diagram pi(zspec, zphot), derived based on a spectroscopic training set or
mock catalogues. The uncertainties in the shapes of the probability distribution
function also have to be taken into account, due to the ﬁnite number Ns of the
spectroscopic redshifts per bin in the calibration set. One can then marginalize
over both of these uncertainties. The dependence on the number of spectroscopic
redshifts is really an indirect expression of the scatter in ¯z and in μ2.
Consider the speciﬁc example of the dark energy equation of state, commonly
written as w(a) = w0 + (1 −a)wa. One can also deﬁne ap, the pivot value at
which the uncertainty in the constant part w(ap) is minimal. Accordingly, we deﬁne
wp = w0 + (1 −ap)wa. Armed with this parameterization we can now apply the
standard Fisher matrix formalism, and deﬁne the Figure of Merit (FoM) as
FoM =
1
δwpδwa
,
(13.12)
where δwp and δwp are the 68% errors on wp and wa.11
One can then explore how the FoM varies subject to photo-z performance. For
example, Abdalla et al. (2008a) have shown that the FoM for weak lensing can be
improved by factors of 1.5–1.7 by adding NIR to the optical Euclid ﬁlter. Banerji
et al. (2008) have illustrated how the addition of NIR colours from VISTA can
improve the measurements of the galaxy power spectrum.
13.10 Discussion
Photo-z estimation is a growing ﬁeld, which is of great importance for the next gen-
eration of wide-ﬁeld imaging surveys. Bayesian ingredients in some of the methods
have proven useful in reducing the number of outliers. The recent applications to
cosmological studies have illustrated that the relevant question is not ‘What is the
best photo-z one can estimate?’ but rather ‘Is the photo-z accuracy sufﬁcient to
achieve a certain science goal?’
Future challenges are:
• The development of hybrid template/training methods.
• Finding reliable methods for assigning probability distribution functions and di-
agnostics of errors.
• Developing methods for incorporating incomplete spectroscopic calibration sets.
• Seeking new approaches for accurate photo-z for clusters of galaxies and hosts
of supernovae type Ia.
11 This is the FoM as deﬁned by the Dark Energy Task Force (DETF) in Albrecht et al. (2006).

13.10 Discussion
297
Acknowledgements
We are grateful to Adam Amara, Peter Capak, Adrian Collister, Carlos Cunha,
Eduardo Cypriano, Andrew Firth, Josh Frieman, Simon Lilly, Marcos Lima, Huan
Lin, Hiroaki Oyaizu, Jason Rhodes and other members of the DES and Euclid
photo-z working groups for their contribution to the work presented here and for
helpful discussions.
References
Abdalla, F. B., Amara, A., Capak, O., Cypriano, E. S., Lahav, O. and Rhodes, J. (2008a).
Mon. Not. Roy. Astron. Soc., 387, 969.
Abdalla, F. B., Banerji, M., Lahav, O. and Rashkov, V. (2008b). arXiv:0812.3831; Mon.
Not. Roy. Astron. Soc., in press.
Albrecht, A. et al. (2006). eprint arXiv:astro-ph/0609591.
Amara, A. and Refregier, A. (2007). Mon. Not. Roy. Astron. Soc., 381, 1018.
Andreon, S., Gargiulo, G., Longo, G., Tagliaferri, R. and Capuano, N. (2000). Mon. Not.
Roy. Astron. Soc., 319, 700.
Babbedge, T. S. R. et al. (2004). Mon. Not. Roy. Astron. Soc., 353, 654.
Bailer-Jones, C. A. L., Irwin, M. and von Hippel, T. (1998). Mon. Not. Roy. Astron. Soc.,
298, 361.
Banerji, M., Abdalla, F. B., Lahav, O. and Lin, H. (2008). Mon. Not. Roy. Astron. Soc., 386,
1219.
Baum, W. A. (1962). Problems of Extragalactic Research. IAU Symposium No. 15, 390.
Benitez, N. (2000). Astrophys. J., 536, 571.
Bertin, E. and Arnouts, S. (1996). Astron. Astrophys. Supp., 117, 393.
Bishop, C. M. (1995). Neural Networks for Pattern Recognition. Oxford: Oxford
University Press.
Blanton, M. R. and Roweis, S. (2007). Astron. J., 133, 734.
Bolzonella, M., Miralles, J.-M. and Pell´o, R. (2000). Astron. Astrophys., 363, 476.
Brammer, G. B., van Dokkum, P. G. and Coppi, P. (2008). Astrophys. J., 686, 1503.
Brunner, R. J., Szalay, A. S. and Connolly, A. J. (2000). Astrophys. J., 541, 527.
Bruzual, A. G. and Charlot, S. (1993). Astrophys. J., 405, 538.
Coleman, G. D., Wu, C.-C. and Weedman, D. W. (1980). Astrophys. J. Supp., 43, 393.
Collister, A. A. and Lahav, O. (2004). Proc. Astron. Soc. Pac., 116, 345.
Collister, A. A. et al. (2007). Mon. Not. Roy. Astron. Soc., 375, 68.
Connolly, A. J., Csabai, I., Szalay, A. S., Koo, D. C., Kron, R. G. and Munn, J. A. (1995).
Astron. J., 110, 2655.
Connolly, A. J., Szalay, A. S. and Brunner, R. J. (1998). Astrophys. J., 499, L125.
Feldmann, R. et al. (2006). Mon. Not. Roy. Astron. Soc., 372, 565.
Firth, A. E., Lahav, O. and Somerville, R. S. (2003). Mon. Not. Roy. Astron. Soc., 339,
1195.
Jain, B., Connolly, A. and Takada, M. (2007). J. Cosmol. Astropart. Phys., 03, 013J.
Koo, D. C. (1999). In R. J. Weymann, L. J. Storrie-Lombardi, M. Sawicki and R. Brunner,
eds., Photometric Redshifts and High-Redshift Galaxies. ASP Conference Series. Vol.
191. San Francisco: Astronomical Society of the Paciﬁc.
Kurtz, M. J., Geller, M. J., Fabricant, D. G. and Wyatt, W. F. (2007). Astron. J., 134, 1360.
Lahav, O., Naim, A., Sodr´e Jr., L. and Storrie-Lombardi, M. C. (1996). Mon. Not. Roy.
Astron. Soc., 283, 207.

298
Photometric redshift estimation: methods and applications
Lima, M., Cunha, C. E., Oyaizu, H., Frieman, J., Lin, H. and Sheldon, E. S. (2008). Mon.
Not. Roy. Astron. Soc., 390, 118.
Ma, Z., Hu, W. and Huterer, D. (2006). Astrophys. J., 636, 21.
Naim, A., Lahav, O., Sodr´e Jr., L., Storrie-Lombardi, M. C. (1995). Mon. Not. Roy. Astron.
Soc., 275, 567.
Newman, J. (2008). arXiv:0805.1409.
Oyaizu, H., Lima, M., Cunha, C. E., Lin, H. and Frieman, J. (2007). arXiv:0711.0962.
Oyaizu, H., Lima, M., Cunha, C. E., Lin, H., Frieman, J., Sheldon, E. S. (2008). Astrophys.
J., 674, 768.
Peacock, J. A. et al. (2006). astro-ph/0610906.
Sowards-Emmerd, D., Smith, J. A., McKay, T. A., Sheldon, E., Tucker, D. L. and Castander
F. J. (2000). Astron. J., 119, 2598.
Stabenau, H. F., Connolly, A. and Jain, B. (2008). Mon. Not. Roy. Astron. Soc., 387, 1215.
Wadadekar, Y. (2005). Proc. Astron. Soc. Pac., 117, 79.
Wang, D., Zhang, Y. X., Liu, C. and Zhao, Y. H. (2007). Mon. Not. Roy. Astron. Soc., 382,
1601.
Weymann, R. J., Storrie-Lombardi, L., Sawicki, M. and Brunner, R. J., eds. (1999).
Photometric Redshifts and High-Redshift Galaxies. ASP Conference Series. Vol. 191.
San Francisco: Astronomical Society of the Paciﬁc.
Wray, J. J. and Gunn, J. E. (2008). Astrophys. J., 678, 144.

Index
A-optimality 109
absurdity 4
Akaike information criterion (AIC) 89
Alcock–Paczynski test 39
algorithms 20
ANNs see artiﬁcial neural networks
annealing schedule 28
annealing, simulated see simulated annealing
applications 36
artiﬁcial neural networks (ANNs) 287
associativity theorem 6
astrometric space missions 247
baby and toy sampling 68
BAO see baryon acoustic oscillation
baryon acoustic oscillation (BAO) 39, 110, 290
Bayes factor 33, 84, 115
Bayes, Rev. Thomas 4
Bayes, theorem 11
Bayesian evidence see evidence (Bayesian)
Bayesian ﬂux inference 201
Bayesian information criterion (BIC) 89
Bayesian model averaging 82, 94
BAYESYS sampler 176
BCES 276
beam 127, 231
deconvolution 127
belief 4
best ﬁt 266
bias
Eddington 204, 268
Eddington–Malmquist 245, 268
galaxy 39
Lutz–Kelker 245
Malmquist 245, 250, 268
bi-valuation 9
black hole inspiral 214
Blackwell–Rao estimate 239
blind signal separation 151
Box–Muller algorithm 60
broad-band ﬁlters 283
Brownian motion 30
burn-in 64
CAMB 230
Cauchy distribution 13
censoring 258
Cepheid variables 247
chi-squared test 280
Cholesky decomposition 140
cluster scaling relations 275
CMB see cosmic microwave background
CMBFAST 230
COBE/DMR 232
colour–magnitude relation 275
commutativity 8
comoving distance 38
complexity (Bayesian) 85
component ﬁelds 130
context theorem 8
contributors ix
correlated component analysis (CCA) 159
correlation function 230
correlation length 70
cosmic microwave background (CMB) 38, 57, 229
cosmic strings 95
cosmological constant 92
cosmological populations 245
cosmology 37
COSMONEST 87
covariance matrix 107
Cram´er-Rao bound 106
crowded ﬁeld 205
cumulant 24
D-optimality 108
DAOﬁnd 168
dark energy 92, 284
Dark Energy Task Force 108
data space 128, 133
de Finetti exchangeability theorem 258
deconvolution 127
mapmaking 235
delayed rejection 223
density parameter Ω 37
destripers 234
detailed balance 25, 62
299

300
Index
direct product 7
Dirichlet process 17
discovery space 265
distance 283
comoving 38
distribution
Cauchy 13
exponential 13
Gamma 13
Gaussian 15
normal see Gaussian distribution
domain
Fourier 135
pixel 133
E and B polarization 241
Eddington bias 204, 268
Eddington–Malmquist bias 245, 268
Einstein–Boltzmann equations 230
emission components 127
empirical Bayes approach 255
enclosed prior 21
entropy
maximum 14, 44
negative 12
ergodic limit 25
estimator 196, 271
optimal 246
evidence (Bayesian) 11, 20, 67, 82, 137
expectation value 58
expected utility 100
experimental design 99
exponential distribution 13
extended sources 167
F-test 280
Faber–Jackson relation 275
faintest sources 204
fastICA 155
ﬁducial parameters 100
Figure of Merit (FoM) 94, 100, 296
model selection 119
Fisher density 19
Fisher matrix 106
Fisher metric 19
ﬂat-histogram sampling 67
ﬂux measurement 193
Bayesian 201
practical 209
FoM see Figure of Merit
forecasting 96
model selection 99
foregrounds 242
foundations 3
Fourier domain 135
free energy 67
frequentism 10, 25
Friedmann equation 37
fundamental observers 37
fundamental plane 275
fuzzy logic 33
galaxy bias 39, 252
galaxy clustering 246
galaxy evolution 265
galaxy luminosity 246
galaxy morphology 287
Gamma distribution 13
Gamma process 17
Gaussian distribution 15
Gelman–Rubin statistic 69
general relativity 37
GEO 213
geometry (of the Universe) 37
Ghirlanda relation 275
Gibbs sampling 25, 66, 238
gravitational wave astronomy 213
gravitational waves 213
Hamiltonian sampling 67, 238
Harrison–Zel’dovich model 91, 123
Hessian 106
hidden layers 287
hidden space 128
hierarchical Bayes approach 256
hierarchical model 231, 252
Hilbert curve 25
Hubble parameter H
37
Hubble Ultra Deep Field 199
hyperparameter 75, 252
hyperprior 254
HYPERZ 285
hypothesis space 12
ICA see independent component analysis
importance sampling 20, 72
improper prior 14
independent component analysis (ICA) 155
inference 3, 11
multi-model 79
inﬁnite divisibility 16
inﬂation 231
information 85, 11, 85
minimum 14
instrument noise 127
intensity measure 258
interferometric detectors 213
inverse relation 249
inverse transform sampling 59
iterative source extraction 183
Jeffreys prior 190
Jeffreys scale 89
Kendall’s test 280
kernel regression 292
knee frequency 234
Kolmogorov–Smirnov test 280
Kullback–Leibler divergence 85, 109
kurtosis 243
Laplace approximation 89
latent variable models 252

Index
301
lattice 5
least upper bound 5
LIGO 213
likelihood 11, 82
linear ﬁlter 168
LISA 214
logic 33
loss 100
luminosity indicators 247
luminosity–light curve shape relation 247
luminous red galaxies 291
Lutz–Kelker bias 245
magnitude–redshift relation 286
Malmquist bias 245, 250, 268
Malmquist correction 250
inhomogeneous 251
mapmaking, deconvolution 235
maps 233
marginal likelihood 11
Markov chain 26
Markov chain Monte Carlo 26
Reverse Jump (RJMCMC) 226
Markov chain sampling 62
mass, prior 13, 21
MASTER formalism 239
matched ﬁlter (MF) 169, 182, 214
maximum entropy 14, 44
method (MEM) 143
maximum likelihood 266
MCMC see Markov chain Monte Carlo
measure 6
MEM see maximum entropy method
Metropolis–Hastings
algorithm 63, 222, 238
criterion 30
minimum information 14
minimum message/description length 89
mixing matrix 127, 129
mixture distributions 266
model 79
model averaging 82
model comparison see model selection
model likelihood 82
model selection 79, 225
as parameter estimation 75
Figure of Merit 119
modiﬁed gravity 96
Monte Carlo
Markov chain 26
methods 87
sampling 57
morphological classiﬁcation 287
multilevel model (MLM) 245, 252
multi-canonical sampling 67
multi-model inference 79, 82
multi-wavelength data 126
negative entropy 12
nested sampling 21, 87
clustered 87
neural networks 287
Neyman–Pearson criterion 219
noise
marginalization 235
spatially varying 146
non-blind signal separation 140
non-Gaussianity 242
non-parametric modelling 261
normal distribution see Gaussian distribution
null hypothesis 280
number counts 268
objective prior 20
Olbers’ paradox 199
optimal estimator 246
optimality 108
optimization 100
outliers 275
p-values 280
parallax 269
parallel tempering 88, 223
parameter constraints 70
parameter estimation 57
parameters
effective number of 86
ﬁducial 100
partial ordering 5
passband 194
PCA see principal component analysis
peculiar velocities 251
Peelle’s pertinent puzzle 45
period–luminosity relation 247
photometric measurements 193
photometric redshift 283
errors 289
software packages 291
pixel-by-pixel source extraction 187
pixel domain 133
Planck satellite 170, 234
point sources 167
point spread function 127
Poisson process 16
polarization 231, 240
E and B components 241
populations 245
positivity 11
posterior (probability) 11, 26
distribution 137
posterior model probability 83
POWELL SNAKES algorithm 183
power spectrum 230
practical ﬂux measurement 209
predictive ability 4
predictive distributions 120
Predictive Posterior Odds Distribution (PPOD) 122
principal component analysis (PCA) 155
prior mass 13, 21
prior predictive 11
prior (probability) 11, 12, 84
enclosed 21

302
Index
prior (probability) (cont.)
improper 14
Jeffreys 190
model 84
objective 20
prior model probability 83
probability (Pr) 9, 10
posterior 11
prior 11
probability theory 4
process 16
Dirichlet 17
Gamma 17
Poisson 16
product, direct 7
product rule 11
proportion 10
proposal density 63
pseudo-Cℓ
239
pulsar timing arrays 213
quantum logic 33
quintessence 96
red galaxies 270
redshift 247, 283
photometric see photometric redshift
rejection sampling 59
Reverse Jump Markov chain Monte Carlo
(RJMCMC) 226
ridge regression 252
Riemannian space 18
sampling
baby and toy 68
direct 59
doubly intractable distributions 68
ﬂat-histogram 67
Gibbs 25, 66, 238
Hamiltonian 67, 238
importance 20, 72
inverse transform 59
Markov chain 62
Metropolis–Hastings see Metropolis–Hastings
algorithm
Monte Carlo 57
multi-canonical 67
nested 21, 87
rejection 59
slice 66
Wang–Landau 67
Savage–Dickey density ratio 76, 88, 116
scale-adaptive ﬁlter (SAF) 169
SED see spectral energy distribution
selection effects 258
semi-blind signal separation 151
semi-parametric population models 263
SExtractor 168
shrinkage estimation 252, 253
signal separation 126
blind 151
non-blind 140
semi-blind 151
signal vector 172
signiﬁcance test 33
simulated annealing 27
simultaneous source extraction 186
singular value decomposition 140
skewness 243
sky signal 127
slice sampling 66
small samples 270
source extraction 167
analytic 182
iterative 183
pixel-by-pixel 187
simultaneous 186
source likelihood function 259
source population 199
sources
extended 167
faintest 204
point 167
spatially varying noise 146
Spearman’s rank correlation test 280
speciﬁc star formation rates 270
spectral energy distribution 283
spectral matching ICA (SMICA) 155
spectral response 194
spectral tilt n 91
spectrum 283
standard candle 37, 247
Standard Cosmological Model 80
standard ruler 37
standardizable candle 247
star counts 269
star formation rates 270
stick-breaking 16
Stokes parameters 240
Student’s t-test 280
sufﬁcient statistic 233
sum rule 11
Sunyaev–Zel’dovich effect 167
supernova
explosions 213
type Ia 40, 247
support vector machines 292
survey catalogues 245
survey design 96, 99
symmetry 13
systematics 242
SZ effect see Sunyaev–Zel’dovich effect
TAMA 213
template methods 285
tensor-to-scalar ratio r 92
terrestrial foregrounds 195
theorem
associativity 6
Bayes’ 11
thermodynamic integration 27, 67, 88
tilt see spectral tilt

Index
303
training methods 285, 287
transition probability 62
transitivity 5
trapezoid rule 22
truth 3
Tully–Fisher relation 247, 275
type-Ia supernova 40
unbiased estimator 196
utility 100
valuation 5
variational potential 8
VEGAS 88
VELMOD 252
velocity dispersion 269
velocity-space distortion 39
Virgo 213
visible space 128
Wang–Landau sampling 67
wavelet 132
weak lensing 290
wide-ﬁeld deep imaging surveys 284
Wide-ﬁeld Fibre-fed Multi-Object Spectrograph
(WFMOS) 106
Wiener ﬁlter 141, 214, 236
Wiener matrix 142
Wilcoxon rank test 280
Wilkinson Microwave Anisotropy Probe
(WMAP) 80, 232
ZEBRA 291
Ziggurat algorithm 61

