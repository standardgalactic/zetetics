GPT is becoming a Turing machine:
Here are some ways to program it
Ana Jojic 1 Zhen Wang 2 Nebojsa Jojic 3
Abstract
We demonstrate that, through appropriate prompt-
ing, GPT-3 family of models can be triggered
to perform iterative behaviours necessary to ex-
ecute (rather than just write or recall) programs
that involve loops, including several popular al-
gorithms found in computer science curricula or
software developer interviews. We trigger execu-
tion and description of iterations by regimenting
self-attention (IRSA) in one (or a combination)
of three ways: 1) Using strong repetitive structure
in an example of an execution path of a target
program for one particular input, 2) Prompting
with fragments of execution paths, and 3) Explic-
itly forbidding (skipping) self-attention to parts
of the generated text. On a dynamic program exe-
cution, IRSA leads to larger accuracy gains than
replacing the model with the much more power-
ful GPT-4. IRSA has promising applications in
education, as the prompts and responses resemble
student assignments in data structures and algo-
rithms classes. Our ﬁndings hold implications
for evaluating LLMs, which typically target the
in-context learning: We show that prompts that
may not even cover one full task example can
trigger algorithmic behaviour, allowing solving
problems previously thought of as hard for LLMs,
such as logical puzzles. Consequently, prompt
design plays an even more critical role in LLM
performance than previously recognized.
1. Introduction
Large language models (LLMs) (Brown et al., 2020; Rae
et al., 2021; Chowdhery et al., 2022; OpenAI, 2023) are
trained on large amounts of text data, which typically in-
1Fred Hutchinson Cancer Research Center, Seattle, WA, USA
2University of California San Diego, La Jolla, CA, USA, and
Mohamed bin Zayed University of Artiﬁcial Intelligence, Mas-
dar City, Abu Dhabi 3Microsoft Research, Redmond, WA, USA.
Correspondence to: Nebojsa Jojic <jojic@microsoft.com>.
clude descriptions of procedures and even computer pro-
grams (Chen et al., 2021). They have demonstrated a sur-
prisingly high competency in retrieving knowledge from the
training data and generalizing it to new, slightly different sit-
uations. The models are typically evaluated on “in-context
learning” tasks, i.e., zero- and few-shot prompting, with
results implying that these models compress iterative rea-
soning into a savant-like ability to directly reach correct
conclusions without a disciplined step-by-step process (Wei
et al., 2022; Kojima et al., 2022). It is difﬁcult to understand
if these abilities are simply due to a high similarity with the
training data, or if they are evidence of the ever-increasing
generalization.
In practice, however, even in simple scenarios where the
justiﬁcation of answers to a given question requires a cou-
ple of reasoning steps, providing those steps in the prompt
for a few examples improves the accuracy of LLMs. Early
such approaches include (Shwartz et al., 2020; Zelikman
et al., 2022; Nye et al., 2021), while more general Chain-
of-Thought (CoT) prompting methods include (Wei et al.,
2022; Wang et al., 2022b; Zhou et al., 2022; Creswell et al.,
2022; Wang et al., 2022a; Liu et al., 2022; Kojima et al.,
2022; Li et al., 2022b). This implies that despite the mas-
sive number of parameters and the self-attention to all pre-
vious tokens, current LLMs are unlikely to solve problems
that require many (or iterated) reasoning steps in a direct,
savant-like manner. In designing new benchmarks, the NLP
community has been targeting more complex tasks where
humans would not only need detailed reasoning to justify
their answer, but need it to reach the conclusions in the
ﬁrst place. Several tasks, such as logical deduction and
logical grid puzzles in BIG-bench Lite (Srivastava et al.,
2022), require constraint satisfaction propagation to solve,
and in-context learning of these problems is typically poor.
LLMs excite us with apparent emergence of such savant
abilities elsewhere, as evidenced by GitHub Copilot usage
statistics (Peng et al., 2023), where nearly 50% of code is
auto-generated by Codex (Chen et al., 2021). But Copilot
is a human-in-the-loop application, and standalone systems
like AlphaCode (Li et al., 2022a) and Codex fall short com-
pared to expert programmers, mostly because they guess the
answer but have no ability to execute, track state, and debug
programs (apart from anecdotal evidence, e.g. Fig. 3.7 in
arXiv:2303.14310v1  [cs.CL]  25 Mar 2023

Programming in GPT
(Bubeck et al., 2023); also see Section 5).
LLMs simply generate tokens in order, each based on many
(ideally all) previous tokens in the sequence, whether these
tokens were part of the prompt or were just generated by the
LLM itself. Thus, the self-attention could allow an LLM
to use all previously generated tokens as the scratchpad for
tracking reasoning steps, states, etc1. Such a use of gener-
ated tokens would resemble a classical Turing Machine with
its memory tape (Turing, 1936). In principle, a non-trivial
recurrent transformer model with inﬁnite attention could
be Turing-complete and capable of executing arbitrary rou-
tines, as long as the attention mechanism can be controlled
stringently enough. But, even in relatively simple settings,
LLMs appear to resist strict controls, e.g., slight changes
in prompts can yield dramatically different responses (Liu
et al., 2021; Malkin et al., 2022; Shi et al., 2023), because
many recurrent patterns in the training data are encoded into
a single model, and learned patterns overlap and vary in
the context size. Thus it is easy to mislead with a prompt
that has accidental alphabetical or numerical ordering, or
happens to deal only with one type of object, etc (Zhao et al.,
2021; Lu et al., 2022; Min et al., 2022).
In this paper, we study much stricter attention controls that
instruct LLMs to unroll the reasoning steps over a long pro-
cedure with the initially undetermined length, decide when
the solution was found, and output the result. A simple ex-
ample is given in Prompt 1, where a prompt describing the
state evolution of the Bubble Sort algorithm on one example
turns GPT-3 into a Bubble Sorter that outputs not only the
sorted sequence but other details of the state, including the
number of element swaps performed (response in Prompt
2). While the latest LLMs can sort lists and even recall the
Bubble Sort algorithm in multiple programming languages,
inferring the number of swaps, as a measure of sequence
disorder, is one type of difﬁcult reasoning task that is hard to
solve in a savant manner. This example alone immediately
points to important consequences discussed in Section 5.
First, LLMs can (or soon will be able to) execute arbitrary
code and thus can have applications beyond Copilot in soft-
ware engineering and education (Gao et al., 2022; Parisi
et al., 2022; Schick et al., 2023; Mialon et al., 2023). More
pressingly, our ﬁndings point out the potentially problem-
atic issue of in-context learning evaluation on LLMs: if
prompts can combine typical natural language instructions
with algorithmic iterative reasoning, then abilities of LLMs
in zero- and few-shot learning with CoT reasoning are vastly
underestimated, and their comparisons are deceptive. The
sensitivity of the performance to prompt design may even be
ampliﬁed by the iterative reasoning triggered by the prompt,
which will then beg the question: If one LLM beats the
1This is likely to be one of the reasons for the increased perfor-
mance of CoT prompting.
other on a task, is it simply because we have not found the
right prompt for the second model? For example, Prompt
3 increases the performance of GPT-3 family on logical
deduction puzzles (Section 2.1) from 32% to 76% (Table 1,
Section 2.1).
While examples in Prompts 1 and 3 may already be sufﬁcient
to make these points, we show that these are not one-off
examples by presenting more detailed results on a wider
range of algorithms taught in computer science curricula
and used to test software engineers in coding interviews,
including string manipulations, dynamic programming, and
stack operations. In Section 2, we discuss how we were
able to trigger the execution (rather than just recollection)
of algorithms consistently, across varied inputs, through
deliberate control of self-attention, a technique we refer
to as Iteration by Regimenting Self-Attention (IRSA). The
basic way to achieve this, demonstrated in Prompts 1, 3, 7,
8, and 9, is through highly structured prompting with an
example of an execution path for one example. We also
investigate prompting that combines multiple fragments of
execution paths in Section 2.2, and a strategy of skipping
parts of generated text when performing self-attention in
Section 2.3. In Section 3, we show how one may use these
ideas to further design interpreter/compiler prompts that
can translate an algorithm in a high-level programming
language into an IRSA prompt that GPT-3 can execute.
Section 4 contains numerical results on several datasets
from BIG-bench and tasks of our own-making, and ﬁnally,
the discussion in Section 5 covers some lessons learned
on how hard it is to “program” in GPT as “machine code,”
as well as possible practical consequences of our ﬁndings.
The discussion also includes an experiment with GPT-4
(OpenAI, 2023) on a well known dynamic programming
task showing that even the latest (at the moment) member
in the family cannot consistently execute code without more
careful prompting in IRSA style.
2. Iteration by Regimenting Self Attention
(IRSA)
Prompts 1, 3, 7, 8, and 9 show the basic IRSA. In each
of these examples, a single prompt is provided for a task,
which, when combined with a new instance of the task, trig-
ger the execution of an iterative algorithm. The algorithms
are single loop (Prompts 8 and 9) or double loop (Prompts
1, 7, and 3), and may have a known or unknown number of
iterations until termination.
Crucially, the prompts show all state changes and explain
each change before it occurs. Although the explanation is
colloquial, the structure of it is both rigid and repetitive,
strictly regimenting the attention to the rules (corresponding
to program instructions) and state changes. In all these
examples, this strategy hardens the attention sufﬁciently to

Programming in GPT
Prompt 1. Bubble Sort: The prompt describes iterative state evolution, including counting swaps, and making the
deterimination when to stop. Playground link (use with 0 temperature)
Problem: 2, 3, 1, 5
EXECUTION
Prep
Length of the list: 4
Number of consecutive pairs: 3
a=[2 3 1 5]
set n_swaps=0
EndPrep
Iteration:
set swap_flag=false. The state is:
State: a=[2 3 1 5], n_swaps=0, swap_flag=false EndState
Pair a[1,2] = [2 3] Check if 2<3. Is it true? Yes.
Because of that, we leave state as is
State: a=[2 3 1 5], n_swaps=0, swap_flag=false
Pair a[2,3]= [3
1] Check if 3<1.
Is it true? No.
Thus, we swap_flag=true, increase n_swaps by one,
and in the latest a=[2 3 1 5] swap 3 and 1 to get into state:
State: a=[2 1 3 5], n_swaps=1, swap_flag=true EndState
Pair a[3,4]= [3 5]
Check if 3<5. Is it true? Yes.
Because of that, we leave state as is
State: a=[2 1 3 5], n_swaps=1, swap_flag=true EndState
swap_flag is true, so do another iteration
Iteration:
set swap_flag=false. The state is:
State: a=[2 1 3 5], n_swaps=1, swap_flag=false EndState
Pair a[1,2] = [2 1] Check if 2<1. Is it true? No.
Thus, we set swap_flag=true, increase n_swaps by one,
and in the latest a=[2, 1, 3, 5] swap 2 and 1 to get into state:
State: a=[1 2 3 5], n_swaps=2, swap_flag=true EndState
Pair a[2,3] = [2 3] Check if 2<3. Is it true? Yes.
Because of that, we leave state as is
State: a=[1 2 3 5], n_swaps=2, swap_flag=true EndState
Pair a[3,4] = [3 5] Check if 3<5. Is it true? Yes.
Because of that, we leave state as is
State: a=[1 2 3 5], n_swaps=2, swap_flag=true EndState
swap_flag is true, so do another iteration
Iteration:
set swap_flag=false. The state is:
State: a=[1 2 3 5], n_swaps=2, swap_flag=false EndState
Pair a[1,2] = [1 2] Check if 1<2. Is it true? Yes.
Because of that, we leave state as is
State: a=[1 2 3 5], n_swaps=2, swap_flag=false EndState
Pair a[2,3] = [2 3] Check if 2<3. Is it true? Yes.
Because of that, we leave state as is
State: a=[1 2 3 5], n_swaps=2, swap_flag=false EndState
Pair a[3,4] = [3 5] Check if 3<5. Is it true? Yes.
Because of that, we leave state as is
State: a=[1 2 3 5], n_swaps=2, swap_flag=false EndState
swap_flag is false, so stop the iteration
Final List: 1, 2, 3, 5
Number of swaps: 2
END OF EXECUTION
Problem: 0, 3, 8, 5, 6
EXECUTION

Programming in GPT
Prompt 2. Response (shaded green) to Prompt 1, whose ending part is shown.
...
State: a=[1 2 3 5], n_swaps=2, swap_flag=false EndState
swap_flag is false, so stop the iteration
Final List: 1, 2, 3, 5
Number of swaps: 2
END OF EXECUTION
Problem: 0, 3, 8, 5, 6
EXECUTION
Prep
Length of the list: 5
Number of consecutive pairs: 4
a=[0 3 8 5 6]
set n_swaps=0
EndPrep
Iteration:
set swap_flag=false. The state is:
State: a=[0 3 8 5 6], n_swaps=0, swap_flag=false EndState
Pair a[1,2] = [0 3] Check if 0<3. Is it true? Yes.
Because of that, we leave state as is
State: a=[0 3 8 5 6], n_swaps=0, swap_flag=false EndState
Pair a[2,3] = [3 8] Check if 3<8. Is it true? Yes.
Because of that, we leave state as is
State: a=[0 3 8 5 6], n_swaps=0, swap_flag=false EndState
Pair a[3,4] = [8 5] Check if 8<5. Is it true? No.
Thus, we set swap_flag=true, increase n_swaps by one,
and in the latest a=[0 3 8 5 6] swap 8 and 5 to get into state:
State: a=[0 3 5 8 6], n_swaps=1, swap_flag=true EndState
Pair a[4,5] = [8 6] Check if 8<6. Is it true? No.
Thus, we set swap_flag=true, increase n_swaps by one,
and in the latest a=[0 3 5 8 6] swap 8 and 6 to get into state:
State: a=[0 3 5 6 8], n_swaps=2, swap_flag=true EndState
swap_flag is true, so do another iteration
Iteration:
set swap_flag=false. The state is:
State: a=[0 3 5 6 8], n_swaps=2, swap_flag=false EndState
Pair a[1,2] = [0 3] Check if 0<3. Is it true? Yes.
Because of that, we leave state as is
State: a=[0 3 5 6 8], n_swaps=2, swap_flag=false EndState
Pair a[2,3] = [3 5] Check if 3<5. Is it true? Yes.
Because of that, we leave state as is
State: a=[0 3 5 6 8], n_swaps=2, swap_flag=false EndState
Pair a[3,4] = [5 6] Check if 5<6. Is it true? Yes.
Because of that, we leave state as is
State: a=[0 3 5 6 8], n_swaps=2, swap_flag=false EndState
Pair a[4,5] = [6 8] Check if 6<8. Is it true? Yes.
Because of that, we leave state as is
State: a=[0 3 5 6 8], n_swaps=2, swap_flag=false EndState
swap_flag is false, so stop the iteration
Final List: 0, 3, 5, 6, 8
Number of swaps: 2
END OF EXECUTION

Programming in GPT
facilitate disciplined procedural reasoning, while leaving
non-regimented content open to interpretation. For example,
Prompt 1 shows how a sequence of 4 integers can be sorted
in some detail, but the same prompt can also be used to
sort characters alphabetically or animals by size, and the
procedure typically works for both shorter and longer lists.
More on this in Section 2.1.
These prompts could be thought of as an instance of Chain-
of-Thought prompting (Wei et al., 2022; Wang et al., 2022b;
Zhou et al., 2022; Creswell et al., 2022; Wang et al., 2022a;
Liu et al., 2022; Kojima et al., 2022). However, a signiﬁcant
distinction lies in the number of reasoning steps, which is
limited and ﬁxed in usual CoT applications. In contrast,
the algorithms explored here require an unspeciﬁed number
of iterations required to complete the execution, and may
even incorporate double loops. The prompt contains the
condition for declaring the end of execution. Interestingly,
we even found that if that declaration involves the word END,
the text versions of GPT models will often stop after they
produce that token, while the code-tuned versions tend to
continue on, inventing a new problem to solve and solving
it until they run out of tokens; Including END as the stop
word in calls to the GPT API will terminate both.
2.1. Applying IRSA to reason over logical puzzles
We focus mostly on standard algorithms in order to evaluate
how close GPT-3 is to being Turing-complete, but in this
section, we pay special attention to a task that involves rea-
soning, rather than program execution. This type of task is
commonly used in the NLP community for testing LLMs
(Srivastava et al., 2022), where both natural language under-
standing and logical deduction are needed. The BIG-bench
Logical Deduction task requires inference of the ordering of
several objects given their pairwise relationships described
in natural language (e.g., a robin is standing on a branch to
the right of a raven, but a sparrow is the left-most). Despite
the low number of objects (e.g., ﬁve) in these puzzles, LLMs
struggle to solve them in zero- or few-shot settings, much
like how human solvers typically cannot just see the correct
answer instantly, and instead require pencil and paper to
manage a search strategy.
An LLM capable of solving such problems with only a few
examples – or even a prompt that breaks down the strategy
into necessary steps in the Chain-of-Thought style – would
indeed demonstrate savant qualities. (As of the date of this
writing, the logical deduction task for ﬁve objects is not
solved by LLMs without external search/reasoning/infer-
ence algorithms, such as ThinkSum (Ozturkler et al., 2022)).
However, Prompt 3 can be used to solve 76% of these puz-
zles. The ﬁrst part of this prompt has a CoT structure that
translates the problem into a canonical form, isolates ob-
ject names, provides the number of objects, and enumerates
statements about object pairs. Then a scoring mechanism is
decided on, and object scores are assigned variable names.
This procedure could continue till the problem is written
in an integer programming or constraint satisfaction form
and then sent to an external mechanism for resolution. In
(Ozturkler et al., 2022), a similar strategy was pursued to
parse the puzzles into inequalities that an LLM can evaluate:
Through new calls, token probabilities are used to compute
the likelihood of a certain item order, so that the external
reasoning step is reduced to probabilistic inference over all
possible orders.
IRSA-inducing Prompt 3, however, does not rely on exter-
nal mechanisms. Instead, it provides parsing instructions
based on GPT-3’s English understanding, and describes it-
erative reasoning in a highly structured manner (similar to
Prompt 1). This approach expects that the self-attention
to the generated text in the response will be drawn to the
strong patterns in the prompt, enabling it to continue as
many iterations as needed to discover scores for the items
that satisfy the ordering clues. Thus the prompt combines
natural language instructions and a description of an exe-
cution path of a constraint satisfaction algorithm. (In fact,
the algorithm is buggy, as discussed in Section 4.2, but still
works most of the time).
2.2. Fragmented prompting
In this section, we introduce an alternative way to trigger
the iterations through fragmented prompting. An example is
given in Prompt 4, which differs signiﬁcantly from Prompt
1 in how it enables iterations of the Bubble Sort algorithm:
• Complete state speciﬁcation. In contrast to Prompt 1
where iterative behaviour is induced indirectly through
worked-out examples of multiple full loops, Prompt 4
explicitly deﬁnes the state-to-state transitions that cause
new iterations for different cases. For that to be possible,
the state in Prompt 4 includes the iterator i.
• Fragmentation. Prompt 4 does not fully cover the entire
execution path of any single example. Instead, it follows
the ﬁrst three state changes2 for the sequence 2, 3, 1, 5,
and then stops in the middle of a sentence. Then it shows
6 additional fragments of execution paths for different
problems. Each fragment illustrates a single state change.
Interestingly, fragmented prompting can also trigger it-
erative behaviour, where the language model accurately
executes the algorithm on a given input and outputs
END OF EXECUTION when the termination condition (no
new updates on the sequence) is met. Viewing this prompt
as an instance of in-context learning, it is challenging to
classify it in usual terms. It goes beyond 0-shot learning as
it contains explanations speciﬁc to the algorithmic sorting
2The full execution path in this style is shown in Prompt 7.

Programming in GPT
Prompt 3. A prompt for solving logical deduction puzzles from BIG-bench. Playground link (use with 0 temperature)
PUZZLE: The following objects need to be ordered. obj1 is the biggest. obj2 is smaller than obj3.
obj1 is bigger than obj2.
QUESTION: Which object is the biggest?
START
Parsing step:
Items: obj1, obj2, obj3
Numbe of items: 3
Statement 1: obj1 is the biggest.
Statement 2: obj2 is smaller than obj3.
Statement 3: obj1 is bigger than obj2.
Scoring identification step:
Scores will refer to size.
Since we have 3 items, let’s assume that the biggest gets a score
of 3 pounds
and the smallest gets the score of 1 pound.
Translation step:
Available variable names: x, y, z, a, b, c
Map item scores of ’obj1’, ’obj2’, ’obj3’ to variable names x, y, z
obj1 score is x; obj2 score is y; obj3 is z;
Statement 1: ’x’ is the biggest.
Statement 2: ’y’ is smaller than ’z’.
Statement 3:
’x’ is bigger than ’y’.
Initialization step:
Words used to qualify the realtionsips: smaller, bigger, biggest
Orientation step:
the biggest: refers to the score of 3
smaller: refers to smaller score
bigger:
refers to larger score
Initialize so that all scores are
different numbers between 1 and 3
Score_assignment_A:
x=2, y=3, z=1
Iterative reasoning
Iteration 1:
update_flag=false
Statement 1:
’x’ is the biggest, meaning: x should be 3
In Score_assignment_A, x is 2
x is not what it should be, so we need to make a change, so we set update_flag=true and we need to make a swap.
In the statement there is only one variable and it is
x. We need
to find another. We want x to be 3,
but we see that in Score_assignment_A that 3 is assigned to y, so we swap
values of x and y to make
Score_assignment_B:
x=3, y=2, z=1
Statement 2: ’y’ is smaller than ’z’, meaning: y<z
In Score_assignment_B, y is 2 and z is 1,
so y<z maps to 2<1
2<1 is false, so we need to make a change, so we set update_flag=true and we
need
ot make a swap.
In the statement there are two variables and those are y and z so we swap in Score_assignment_B to make
Score_assignment_C:
x=3, y=1, z=2
Statement 3: ’ x’ is bigger than ’y’, meaning x>y
In Score_assignment_C, x is 3 and y is 1,
so x>y maps to 3>1
3>1 is true, so we don’t need to make a change.
End of iteration. Since update_flag is true, we need more iterations.
Iteration 2:
update_flag=false
Statement 1:
’x’ is the biggest, meaning: x=3
In Score_assignment_C, x is 3,
so x=3 maps to 3=3
3=3
is true, so we don’t need to make a change.
Statement 2: ’y’ is smaller than z, meaning: y<z
In Score_assignment_C, y is 1 and z is 2, so y<z maps to 1<2
1<2 is true, so we don’t need to make a change.
Statement 3: ’x’ is bigger than y, meaning x>y
In Score_assignment_C, x is 3 and y is 1,
so x>y maps to 3>1
3>1 is true, so we don’t need to make a change.
End of iteration. Since update_flag is false, we have finished all iterations and found the correct order.
The correct score assignment is the last (Score_assignment_C):
x=3, y=1, z=2
Reverse translation step:
Map items ’obj1’, ’obj2’, ’obj3’ to variable names x, y, z
so we replace x by obj1, y by obj2, and z by obj3 to get size scores:
obj1 has the score 3; obj2 has the score 1; obj3 has the score 2
Question: Which object is the biggest?
Answer: obj1
Sorting all by score starting with obj1:
with score 3, obj1
with score 2, obj3
with score 1, obj2
END
PUZZLE: On a shelf, there are five books: a gray book, a red book, a purple book, a blue book, and a black book.
The red book is to the right of the gray book. The black book is to the left of the blue book.
The blue book is to the left of the gray book. The purple book is the second from the right.
QUESTION: Which is leftmost?
START

Programming in GPT
Prompt 4. Fragmented prompt: The ﬁrst few Bubble Sort state transitions follow the execution path for one problem, but
the path is not completed. Instead, state transitions involving different sequences at different execution points are provided.
Initial part of the response is marked green. Skip attention: The part of the response up to the last state is not needed to
continue the generation. Only the prompt, the last state bracketed by <state> and </state>, and the text after it are
necessary to generate the next token. Both single execution path prompts and fragmented prompts can have self-attention
instructed to always skip to last generated state, as marked with Attend. Playground link (use with 0 temperature)
Attend
Problem: 2, 3, 1, 5
EXECUTION
Length of the list: L=4
Number of pairs: P=3
a=[2 3 1 5]
set n_swaps=0. set i=P=3. set swap_flag=true.
<state> a=[2 3 1 5] i=3 P=3 n_swaps=0 swap_flag=true </state>
Since i=3 and P=3, i and P are equal, so this iteration is done, but swap_flag is true,
so we need another iteration
Iteration:
set swap_flag=false.
set i=0. The state is:
<state> a=[2 3 1 5] i=0 P=3 n_swaps=0 swap_flag=false </state>
Since i=0 and P=3, these two are different, so we continue
a[i]=a[0]=2 a[i+1]=a[1]=3
Because 2<3 is true we keep state as is and move on by increasing i
<state> a=[2 3 1 5] i=1 P=3 n_swaps=0 swap_flag=false </state>
Since i=1 and P=3, these two are different, so we continue
a[i]=a[1]=3 a[i+1]=a[2]=1
Because 3<1 is false we set swap_flag=true,increase n_swaps by one, and in a=[2 3 1 5] swap 3 and 1,
and increase i, and keep P as is to get
<state> a=[2 1 3 5] i=2 P=3 n_swaps=1 swap_flag=true </state>
Since i=2 and
<state> a=[6 5 8 9 1 2] i=2 P=5 n_swaps=5 swap_flag=false </state>
Since i=2 and P=5 i and P are different, so we continue
a[i]=a[2]=8 a[i+1]=a[3]=9
Because 8<9 is true we we keep state as is and move on by increasing i
<state> a=[6 5 8 9 1 2] i=3 P=5 n_swaps=5 swap_flag=false </state>
<state> a=[9 1] i=0 P=1 n_swaps=2 swap_flag=true
</state>
Since i=0 and P=1 i and P are different, so we continue
a[i]=a[0]=9 a[i+1]=a[1]=1
Because 9<1 is false we set swap_flag=true,increase n_swaps by one, and in a=[9 1] swap 9 and 1
and increase i, and keep P as is to get
<state> a=[1 9] i=1 P=1 n_swaps=3 swap_flag=true
</state>
<state> a=[6 7 3 5] i=3 P=3 n_swaps=7 swap_flag=false </state>
Since i=3 and P=3 i and P are equal, so this iteration is done, swap_flag is false, so stop the iteration
Final List: 6, 7, 3, 5
Number of swaps: 7
END OF EXECUTION
<state> a=[3 5 6 8] i=3 P=3 n_swaps=1 swap_flag=true
</state>
Since i=3 and P=3 i and P are equal, so this iteration is done, but swap_flag is true,
so we need another iteration
Iteration:
sset swap_flag=false.
set i=0. The state is:
<state> a=[3 5 6 8] i=0 P=3 n_swaps=1 swap_flag=false </state>
<state> a=[2 8 1 3 5 7 4] i=1 P=6 n_swaps=5 swap_flag=false </state>
Since i=1 and P=6 i and P are different, so we continue
a[i]=a[1]=8 a[i+1]=a[2]=1
Because 8<1 is false we set swap_flag=true,increase n_swaps by one, and in a=[2 8 1 3 5 7 4] swap 8 and 1
and increase i, and keep P as is to get
<state> a=[2 1 8 3 5 7 4] i=2 P=6 n_swaps=6 swap_flag=true
</state>
<state> a=[4 8] i=0 P=1 n_swaps=7 swap_flag=true
</state>
Since i=0 and P=1 i and P are different, so we continue
a[i]=a[0]=4 a[i+1]=a[1]=8
Because 4<8 is true we we keep state as is and move on by increasing i
<state> a=[4 8] i=1 P=1 n_swaps=7 swap_flag=true
</state>
Problem: 3, 1, 8, 9, 6
EXECUTION
Don’t attend
Length of the list: L=5
Number of pairs: P=4
a=[3 1 8 9 6]
set n_swaps=0. set i=P=4. set swap_flag=true.
<state> a=[3 1 8 9 6] i=4 P=4 n_swaps=0 swap_flag=true </state>
Since i=4 and P=4 i and P are equal, so this iteration is done, but swap_flag is true,
so we need another iteration
Iteration:
set swap_flag=false.
set i=0. The state is:
Attend
<state> a=[3 1 8 9 6] i=0 P=4 n_swaps=0 swap_flag=false </state>
Since i=

Programming in GPT
task. Yet, as opposed to what the few-shot CoT prompt-
ing might do, it does not work out any single example of
array sorting. Instead, it provides fragments of patterns
that can be stitched together to execute the algorithm (and
GPT-3 CODE-DAVINCI-002 does execute it correctly for
new inputs).
The potential advantage of such fragmented prompting is
that the prompt can be shorter and include a greater vari-
ety of situations that may be encountered in new problems
without going through multiple examples (or, as in this case,
without going through any single example of an execution
path from start to ﬁnish). A potential disadvantage is that
the language model may get confused by the fragmentation
and start hallucinating independent fragments itself. In this
case, we managed to avoid that by having the ﬁrst fragment
starting from the start of execution, going through several
state transitions, and ending mid-sentence. Because of this,
when a new problem is given, the language model starts
running the execution path from the beginning, and later
refers to various cases in the prompt for guidance on how to
proceed. The <state>*</state> structure encourages
the model to refer to the previous state it just generated
when creating the follow-up text.
2.3. Skip attention
Prompt 4 also illustrates the idea of attention skipping.
Whether we are using a fragmented prompt or a single-
execution prompt, if the state in the <state>*</state>
structure is complete, the attention mechanism can effec-
tively generate the next token without attending to all the
generated text. It is sufﬁcient to attend to the prompt and
the text generated after and including the last state.
Depending on the implementation, such skipping can of-
fer several advantages. If the skipping is implemented on
the server side, akin to how OpenAI provides stop word
functionality in its API, then skipping unnecessary atten-
tion saves computation: The state of the model at the end
of the prompt is cached and used to continue processing
from the latest generated <state> marker, ignoring the
text generated in-between. Skip-to-state can also be imple-
mented on the client side, iteratively updating the original
prompt by concatenating the latest <state>*</state>
structure to the original prompt and calling the generative
model with </state> as a stop sequence (We did the
latter in our experiments). In both cases, the skip-to-state
strategy should increase the number of tokens that can be
generated, as self-attention, which grows linearly with the
generated text, is the primary cause for the token limitations.
Skip-to-state strategy keeps the self-attention cost constant.
As IRSA requires the unrolling of potentially long iterative
algorithms, these savings are important. For example, run-
ning a dynamic program that keeps track of 2D matrices is
only practical in this manner. (See also (Schuurmans, 2023)
on an external memory approach to dealing with limited
attention length. Here we deal with it by skipping parts of
generated text, instead). Another advantage of skip-to-state
attention is that by only attending to the necessary informa-
tion, the generative model is less likely to get confused by
accidental patterns created in its own generated text. (See
more on this in Section 5 and Figure 1.)
3. GPT as a machine language: Prompting to
interpret/compile a program
Previous examples show that GPT-3 can be prompted to
perform iterative reasoning and program execution by pro-
viding it with execution paths or multiple path fragments,
which implies that GPT-3 is close to being Turing-complete.
On the other hand, a general-purpose computer can also
execute algorithms that convert the text of a program into its
machine code. Indeed, we can design prompts with instruc-
tions on how to turn code in some language into execution
paths that can then be used in prompting.
An example is shown in Prompt 5, where several exam-
ples of hypothetical syntax for transforming states are
given, including setting values of variables and matrices,
printing them, a single loop program execution, and the
detailed_max function that breaks down steps and ex-
plains them. Then, the double loop dynamic programming
algorithm for ﬁnding the longest common subsequence
(LCS) is also presented in this new language. This prompt
successfully triggers the correct execution of the algorithm,
complete with detailed explanations and state transitions
(green shaded in Prompt 6). This can then be used as a
prompt to execute the LCS algorithm on arbitrary inputs
(Section 4.3). We should note that GPT-3 is still sensitive to
small alterations in text, and Prompt 5 does not always lead
to good interpretations of the algorithm. The performance
may depend on accidental deceptive patterns and inconsis-
tencies in the prompt, as well as the input. Nevertheless,
once the output has been veriﬁed as correct, the Prompt 5
together with the response in Prompt 6 became the prompt
– IRSA ’machine code’ for GPT — to execute (mostly cor-
rectly) the LCS algorithm for new inputs, as long as they
are appended in the same format:
LCS:
Input: <seq1> <seq2> End of input
LCS Prep:
4. Results
We evaluated two versions of iteration by regimenting self
attention (IRSA):
• Basic IRSA: Prompting with highly structured single exe-
cution path examples (Table 1). Although similar to CoT
prompting, there are notable differences. CoT prompts

Programming in GPT
typically provide multiple steps of reasoning shown for a
few examples and have the LLM perform the same steps
on a new example. Conversely, IRSA prompts are de-
signed to trigger iterative reasoning that is repeated until
the stop condition is reached and the solution is found.
Furthermore, the execution path example for each task
is deliberately chosen to be out-of-distribution (e.g., the
Bubble Sort prompt features a worked-out example of sort-
ing a four-number sequence in just three passes, while the
dataset consists of ﬁve-number sequences requiring 2 to 5
iterations and up to 20 state transitions, with varying com-
plexity across problems). Thus in terms of information
they provide, these prompts can be seen as somewhere
between single-shot and zero-shot prompts.
• Skip-to-state IRSA: Prompting as above, but with ad-
ditional forced attention skipping. In this approach, the
LLM is forced to attend only to the prompt and the last
generated state as it iterates through the input to ﬁnd the
solution (as illustrated at the end of Prompt 4). Our ex-
periments (Table 2) also evaluated fragmented prompts,
where the prompt does not consist of a single complete
execution path for an example, but instead shows several
state-to-state transitions for different inputs. Such prompts
help generalize the instructions, even though they further
deviate from the single-shot prompts, as there is no single
example worked out till the end.
Baselines. To make fair comparisons and avoid unnecessary
recomputation, we reused existing baselines from Srivas-
tava et al. (2022) wherever possible, denoted by an asterisk
(*) (especially considering that these baselines typically
perform close to random guessing on certain tasks). We
reused these datasets and baselines for the following tasks:
Logical deduction, Balanced parenthesis, and Longest com-
mon subsequences for long sequences. We created our own
datasets and ran baselines for the following tasks: Bubble
sort, Longest substring without repeating characters, and
Longest common subsequence for short sequences. We in-
clude the best result from Srivastava et al. (2022) for the
GPT family, as our experiments were mainly conducted
using GPT-3. Our baselines included zero or few (5) shot
prompting with or without relevant code added to the de-
scription of the task in the prompt (e.g. Prompt 12). Few
shot baselines were made with 5 different random choices
of examples to be included in the prompt.
Table 1 summarizes the success of the basic IRSA in com-
parison to the best-performing baselines. Table 2 provides a
breakdown of individual baseline performance and evaluates
the IRSA using skip attention and fragmented prompting.
The ’Guessing’ strategy refers to picking the most frequently
correct answer for a given task as a guess for each problem
in the task, which is different from truly random guessing.
In-context learning with multiple examples could prime the
answers to pick the most frequently seen answer, even when
no understanding of the problem happens. Thus, our ’Guess-
ing’ strategy captures the task difﬁculty more accurately.
Models. We have brieﬂy experimented with different mem-
bers of the GPT-3 family, but decided to run complete ex-
periments with CODE-DAVINCI-002 for two reasons: TEXT-
DAVINICI-002 and 003 often produced qualitatively similar
results, and experimentation with the lightweight CODE-
DAVINCI-002 was easier due to better combination of token
quota and availability. Having been tuned on code, this
model may have slight advantages over models tuned for
other more natural language-targeted tasks. Nevertheless, as
we show in the experiments and discuss in Section 5, with-
out IRSA, CODE-DAVINCI-002 cannot solve the problems
we discuss here, even when it can generate the code that
could. To induce iterative reasoning in LLMs, it appears
that attention needs to be highly regimented through strong
structure, and possibly additional attention control, such as
the skip-to-state strategy we described in Section 2.3. This
also applies to GPT-4 (OpenAI, 2023), which, at the time
of this writing, has just become available for testing. In the
Discussion (Section 5.3), we show that prompting GPT-4
with straight-forward Prompts 13, 14, 15 does not match the
performance of IRSA in GPT-3.
4.1. Datasets
Bubble sort. We created a dataset of 100 random non-
repeating digit sequences of length 5. For each sequence, we
ran the bubble sort algorithm to establish the total number of
element swaps it requires. The task is to predict the number
of swaps for a given sequence.
Longest substring without repeating characters. A clas-
sical coding interview question: Given a string of letters,
ﬁnd the longest contiguous substring such that no letter ap-
pears more than once. We created a dataset of 100 random
strings of length 7, and for each found the length of the
longest subsequence without repeating characters. The task
is to predict that length for a given sequence.
Logical deduction (Srivastava et al., 2022). We include
this task (Section 2.1) in experiments to emphasize the broad
importance of triggering iteration in LLMs responses. En-
abling LLMs to execute iterative algorithms through effec-
tive prompting could help solve numerous reasoning prob-
lems. To illustrate this, we consider this task that involves
solving a puzzle about an order of items/objects/persons,
such as books on the shelf, birds on a branch, cars, golfers,
etc., given several clues, such as “minivan is more expensive
than the car”, or “the robin is to the left of the ﬁnch.” We
focus on a subtask involving 5 items, with varying sets of
items and the types of ordering across the puzzles. While
in-context learning with LLMs consistently solves less than
35% of puzzles, a recent combination of GPT-3 and proba-
bilistic reasoning (Ozturkler et al., 2022) was able to solve

Programming in GPT
Task
IRSA
Baseline
Guessing
Bubble sort
- Prompt 1
0.74
0.27
0.23
- Prompt 7
1.00
0.27
0.23
Longest substring
1.00
0.60
0.59
Logical deduction
0.76
0.32∗
0.2
Parentheses
0.96
0.56∗
0.5
Table 1. Iteration through Regimented Self-Attention (IRSA) com-
pared with standard in-context learning baselines, and with the
strategy of always guessing the most frequent answer. (*) denotes
the best result for GPT-3 from the BIG-bench (Srivastava et al.,
2022) .
77% of the puzzles. We reach a similar performance through
IRSA, without an additional external reasoning mechanism.
Valid parentheses (Srivastava et al., 2022). The task is
the ﬁrst of the two in the cs-algorithms challenge in BIG-
bench. The goal is to evaluate LLMs ability to perform
reasoning equivalent to the classical stack manipulations
needed to verify if a sequence of parentheses of different
types is balanced or not. LLMs (including GPT) tend to do
about the same as chance (50%), except for PaLM with 3
shots, which gets around 75% accuracy.
Longest common subsequence (long) (Srivastava et al.,
2022). The second task in BIG-bench cs-algorithms in-
volves solving the classical dynamic programming problem.
Deﬁning a subsequence of a sequence to be a sequence of
symbols one could get by skipping arbitrary stretches in the
original sequence, the task is to ﬁnd the length of the longest
subsequence common to two given sequences. LLMs do
not do much better than chance on this task (∼10%).
Longest common subsequence (short). We created this
dataset in the same manner as the above one from the BIG-
bench, but with the constraint on the sequence lengths, lim-
iting them to a maximum of 6 characters. This allows us
to evaluate IRSA on more cases, ensuring it does not run
out-of-memory (tokens) in generation3.
4.2. Basic IRSA results
The basic IRSA involves a single prompt for a given cate-
gory of problems (or algorithms). The prompt contains a
single execution path for one problem, deliberately to be
slightly out of the data distribution, such as a shorter prob-
lem than those found in the datasets. The results are sum-
marized in Table 1. For Bubble Sort evaluations, we show
the results using both Prompt 1, and Prompt 7. The latter
is a single execution path for the same problem (2, 3, 1, 5),
but in the style of Fragmented Prompt 4 by continuing the
execution path initiated by Prompt 4, without incorporating
3Buble sort, Longest substring, and Longest common subse-
quence (short) datasets: https://github.com/anajojic/gpt-coding
fragments from other paths. The former had an accuracy of
74% for inferring the numbers of swaps necessary to sort
different sequences, while the latter achieved 100%. Note
that while the execution path for the example 2, 3, 1, 5 re-
quires three iterations of the outer loop and three iterations
in each inner loop, the dataset contains sequences of length
5 and thus requires four iterations in the inner loop and a
variable number of iterations of the outside loop – anywhere
from 2 to 5 – and yet the model can execute the correct
number of iterations based on the stoppage criterion (that in
the inner loop, no changes were made to the sequence).
For the logical deduction puzzles, we used the Prompt 3.
Note that the logic of the iterative reasoning there is faulty as
it may enter an inﬁnite loop. When that happens, the genera-
tion runs out of tokens and we simply used the answer after
the 4th iteration in evaluation. Further discussion in Section
5 suggests the potential for creating more effective prompts.
Nevertheless, with this prompt to induce iterative reason-
ing, we still reach the state-of-the-art results, comparable
only with (Ozturkler et al., 2022), which uses an external
reasoning mechanism in conjunction with prompting.
To solve the longest substring without repeating characters
problems, we developed Prompt 8 based on the 1-index
version of the following single-pass algorithm. Interestingly,
this algorithm trades computation for memory by creating
one variable per unique letter in the sequence for storing
the location where the letter was last seen in the sequence
during the pass (last_ind):
# s contains the given string
last_ind = {}
m_len = 0
# window start
st_ind = 0
for i in range(0, len(s)):
if s[i] in last_ind:
st_ind=max(st_ind,last_ind[s[i]]+1)
# Update result if window is longer
m_len = max(m_len, i-st_ind + 1)
# Update last index of the character
last_ind[s[i]] = i
return m_len
To address the parentheses problem, we used the single exe-
cution path that demonstrates the needed stack operations
for determining whether the sequence is balanced or not.
The beginning and the end are shown in Prompt 9. For
brevity, we have omitted certain portions represented by
ellipses. Note that creating long prompts is made easier by
GPT’s autocompletion capabilities, i.e., by starting with a

Programming in GPT
Classical prompting
Bubble sort
LCS-S
LCS-L
0-shot
0.20
0.09
0.14∗
0-shot + code
0.20
0.11
-
few shot
0.25±0.05
0.07±0.01
0.16∗
few shot + code
0.23±0.03
0.06±0.02
-
Guessing
0.23
0.44
0.10
IRSA skip-to-state
single path
0.95
0.93
0.28
7 fragments
0.99±0.02
-
-
13 fragments
0.97±0.03
-
-
19 fragments
0.99±0.02
-
-
25 fragments
0.97±0.03
-
-
Table 2. IRSA with forced skipping of everything but the latest
state in generated text for Bubble Sort and Longest Common
Subsequence. We also evaluate fragmented prompting in Bubble
sort, where prompts contain multiple state-to-state fragments of
execution paths for different problems. (*) denotes the best result
for the GPT family from the BIG-bench (Srivastava et al., 2022)
description of a few steps and asking the model to complete
it. Then wherever we want the prompt to differ from the
model’s guess, we erase the generated text from that point
and continue typing our correction/instruction and try to au-
tocomplete again. (See also Section 5, as well as Section 3).
But interestingly, as discussed in Section 2.2 on fragmented
prompting, parts of the execution paths can be omitted: In
fact, the Prompt 9 as is, with the ellipsis instead of 10 steps
in the algorithm, still achieves 91% accuracy!
4.3. Skip-to-state attention results
The dynamic programming solution to the longest com-
mon subsequence (LCS) problem involves a large state that
includes a (M + 1) × (N + 1) matrix representing the so-
lution for all preﬁxes of the two sequences of lengths M
and N. Without skip-to-state attention (Section 2.3), the
API calls run out of tokens before reaching the end for all
but the shortest problems. As elaborated in Section 3 we
used Prompt 5 to compile an execution path in Prompt 6,
and then used both of them to induce IRSA on LCS short
(LCS-S) and LCS long (LCS-L) problems. Even with skip
attention, the state was too large to ﬁt the token limit for
most of the problems in LCS-L from BIG-bench. Yet, IRSA
with skip attention still beats the state-of-the-art signiﬁcantly
(Table 2). On shorter problems LCS-S, where the state was
small enough to allow IRSA with skip attention to make all
state-to-state transition without running out of tokens, the
performance was a respectable 93%. Note that even GPT-4,
without IRSA, cannot reach this accuracy (Section 5.3).
We also tested fragmented prompting of Bubble Sort ex-
ecution, as shown in Table 2. Interestingly, state-to-state
skipping slightly hurts the performance of the single execu-
tion path prompt that achieved 100% accuracy when used
without skipping (Table 1). Nonetheless, the fragmented
prompts did well with skip attention. For each selected num-
ber of fragments – 7, 13, 19, 25 – at least one of ﬁve ran-
domly generated prompts achieved 100% accuracy. These
prompts followed the format in Prompt 4, starting with the
few state transitions from the beginning for the sequence
[2, 3, 1, 5] and then listing additional 6, 12, 18, or 24 frag-
ments. Bubble Sort has 6 different transitions, and listing
one, two, three, or four of each type, with a random se-
quence in the state, allows fully balanced instruction, which
leads to a slightly better performance than having a com-
pletely randomly chosen execution path fragment (data not
shown). These six basic transitions, illustrated in Prompt
4, involve two ways of ending an iteration depending on
the swap ﬂag and four ways of changing the state: two pos-
sibilities for inequality being true or not, combined with
two possible previous values of the swap ﬂag. As we could
have expected, using an ensemble approach with multiple
prompts can yield 100% accuracy, even if the individual
prompts fall short: The cryptic sensitivity to accidentally
hidden (at least to our eyes) patterns in the prompts means
that different prompts fail for different test cases. Thus, ev-
ery one of the fragmented prompt collections yields 100%
when used as an ensemble.
5. Discussion
Iteration by Regimenting Self-Attention (IRSA) is a tech-
nique for triggering code execution in GPT-3 models. Note
that the goal is different from the goal of Alphacode (Li
et al., 2022a) and Copilot (Chen et al., 2021; Peng et al.,
2023), which are meant to write the code, without neces-
sarily understanding what it outputs. While there are in-
deed examples of rather impressive code generation and
even, anecdotally, execution path generation using minimal
prompting in the latest Codex and GPT-3 models, the lack
of control in current LLMs prevents the consistent achieve-
ment of these feats with precision, which is why the code
generation applications involve humans in the loop. For
instance, as illustrated in zero-shot bubble sort code Prompt
11, when relying on Codex alone to attempt code execution,
the generated samples are intuitively close to the correct so-
lution, but a bit off, preventing correct execution. IRSA, on
the other hand, can produce consistently accurate outputs.
In algorithm design, trading computation for memory use
is a recurrent idea. IRSA as a technique for LLM inference
can be seen in a similar light: We could train a bigger model
on more data, with attention spanning deeper into the past
tokens, hoping that it could answer a simple yet computa-
tionally complex query in just a couple of tokens directly;
or we can devise a prompting strategy instructing a smaller
LLM to use its token stream as a memory tape, allowing it

Programming in GPT
to reach similar functionality with increased token usage.
By triggering and controlling iterative behaviour, we can, in
principle, execute arbitrary algorithms, which further raises
interesting questions: What are the consequences of LLMs
becoming Turing-complete? And how difﬁcult is it to pro-
gram via IRSA? Will larger GPT models become capable
of executing programs correctly without IRSA? Based on
our experience in designing the prompts we showed here,
we speculate on these three questions in this section.
5.1. Possible consequences
(Teaching) Coding. The integration of LLMs’ code genera-
tion capabilities with IRSA leads to innovative applications
in code generation. Some of it is implied in the interpreter/-
compiler Prompt 5, which instructs GPT how to interpret
and execute code. Following these ideas, exploring program
veriﬁcation and automatic debugging could be a promising
direction. Another obvious application of IRSA is in com-
puter science education, where we often expect students to
execute programs on paper to determine what the state will
be at some point during the execution. Furthermore, IRSA
may also point to new ways of programming by example.
Adversarial applications.
Any time a computational
medium is Turing-complete, a variety of malicious uses may
become possible, such as creating and executing malware,
exploiting system vulnerabilities, conducting cryptographic
attacks, causing resource exhaustion, etc. Thus we should
be aware of the double-edged sword with the increased
versatility and computational power of GPT models.
In-context learning and LLM evaluation.
Prompting
with IRSA must be considered a zero- or one-shot learn-
ing technique, analogous to chain-of-thought prompting.
If, via IRSA, LLMs can be disciplined with a regimented
prompt to execute arbitrary algorithms involving (double)
loops, they may be able to solve arbitrary problems NLP
researchers can compose, incorporating natural language
understanding and iterative reasoning like belief propaga-
tion, constraint satisfaction, search, etc. This renders many
of the hard BIG-bench tasks easier than they initially appear,
as already suggested by (Suzgun et al., 2022) using classical
CoT prompting. Many CoT results can be further improved
with IRSA (as logical deductions with Prompt 3).
However, triggering such iterative behaviour may still be
hampered by the same sensitivity of in-context learning to
accidental misleading patterns, already observed in classical
prompting (Lu et al., 2022; Zhao et al., 2021), where there
may exist a “fantastical” crafting of the prompt that signiﬁ-
cantly improves the accuracy of the task. In fact, iterative
reasoning may further amplify the fantastical choices. Thus,
if one LLM successfully solves a hard logical reasoning task
using a suitable prompt while another does not, this might
imply that the optimal prompt has not yet been found. In
fact, it would not be surprising if better prompts are even-
tually found that enable the LLM we used here (GPT-3,
CODE-DAVINCI-002) to solve all tasks with 100% accuracy.
Thus, evaluating LLMs on their in-context learning abilities
is of questionable value: Some of the hard tasks in BIG-
bench may be better suited to evaluating the skills of prompt
engineers rather than the LLMs themselves.
Hybrid models – LLMs as translators.
If LLMs are
Turing-complete and can transform problems described in
natural language into algorithmically solvable programs, the
decision to let them execute the program or not becomes a
practical matter of computational cost. With the apparent
magic of savant-like guessing gone, it is much more prac-
tical to run the algorithms on a classical computer, an ap-
proach taken by, for example, (Ozturkler et al., 2022) where
the external computational mechanism performs probabilis-
tic inference, or (Khot et al., 2022) that involves external
control ﬂows, and many other recent published and unpub-
lished experiments combining LLMs with external calls
and tools (Parisi et al., 2022; Gao et al., 2022; Yao et al.,
2022; Press et al., 2022; Schick et al., 2023; Paranjape et al.,
2023). Such hybrid models could separate the higher level
reasoning “System 2” – to use an analogy with models of
human cognitive processes (Tversky & Kahneman, 1974;
Kahneman, 2011) – from the lower-level “knee-jerk reac-
tion” reasoning “System 1”, however savant-like it might be.
In such systems, LLMs can dramatically improve traditional
artiﬁcial intelligence algorithms simply by translating the
problems into an appropriate form: see Prompt 10 where
the logical deduction task is solved by creating a call to the
Solve command in Wolfram language (Mathematica) for
an example. The artiﬁcial intelligence community is increas-
ingly interested in researching such systems, e.g., (Bengio,
2017; Goyal & Bengio, 2020), and the developer commu-
nity is already developing and deploying hybrid language
models (Bing-ChatGPT integration, for instance).
Self-attention control in training and inference. To para-
phrase an old adage on parenting, researchers have spent a
lot of effort teaching GPTs to pay attention to everything
in the text, and now IRSA is an attempt to stop it from at-
tending to everything. We accomplish it both by drawing
attention with a strong repetitive structure and by brute force
through skip attention (Section 2.3). More ﬂexible ways of
determining what the model should attend to may be needed
both in model building and inference.
5.2. Pitfalls of programming in GPT-3
Prompts we experimented with induce single loop or dou-
ble loop program execution. Generally, controlling double
loop algorithms, such as Bubble Sort and Longest Common
Subsequence, is more challenging. The difﬁculty lies not
in understanding the double loop logic, but rather in the

Programming in GPT
increased probability of running into some of the problems
described below. These problems are not always obvious,
but can result in a wide range of accuracies achieved by
seemingly similar prompts. For example, the two prompt
designs for Bubble Sort both worked surprisingly well, but
showed a big gap in performance between them (74% and
100%). Here are some tips for attempting IRSA.
Keep a complete state. While it is often possible to in-
struct by analogy without fully accounting for all decisions,
keeping the full state (i.e., showing it repeatedly after each
transition) is usually preferable. For example, Prompt 4 con-
tains the iterator variable in the state, while Prompt 1 does
not. Not only does keeping full state help regiment the atten-
tion, but it makes fragmented prompting and skip-to-state
attention possible.
Explain why before the instruction, not after. LLMs
are autoregressive, which makes them easier to prompt
in order: from left to right. Thus, instead of instructing
with ‘We now swap 4 and 2 because 2<4’, we
instruct with:
Because 4<2 is false we swap 4 and 2
Then later in generation, e.g., ‘Becasue 5<3 is’ will
trigger generation of token false and it, in turn, will trig-
ger generation of ‘we swap’, and so on.
Avoid unnecessary variation, follow strong structure.
We used the term regimenting attention in the naming of the
technique to emphasize that strong structure is even more
important in IRSA than in other prompting applications. It
is usually crucial to order the variables in the state always
in the same order, utilize the same keywords to designate
the state, use the same language to explain the transitions,
and ensure consistent capitalization, punctuation, and even
spacing/tabulation. We experimented with several variants
of the Bubble Sort prompt, and even when using the same
worked-out example, the accuracy can vary dramatically.
Generate as much of the prompt with LLM itself. One
way to create such a strong structure is to let the model
continue the prompt we are designing after every few lines
(going back to correct the incorrectly generated continua-
tion). The model is more likely to stay faithful to the pattern
human started than the human is (with spacing, typos, and so
on). Because of this, using the interpreter/compiler Prompt
5 to create an LCS execution path to serve as a prompt is a
safer way of generating an IRSA-inducing prompt (as long
as we verify that the exemplary execution path is correct).
Overlapping patterns can be problematic. When gener-
ating the next token, an LLM has to balance many inﬂuences
of patterns both in the prompt and the so-far generated text.
For example, in the LCS algorithm execution Prompt 6, the
model has to balance the long-range self-attention when de-
2
4
6
8
10
12
14
-4
-3
-2
-1
0
1
2
3
4
Figure 1. The difference between GPT Codex log probabilities of
tokens true and false after ‘Because 2<1 is’, which was
preceded by a long context of variable length (x-axis). The context
contains between 1 and 15 lines of text comparing number 2 with
randomly chosen larger numbers and declaring, e.g., Because
2<6 is true ... We show the band between the maximum
and minimum log odds over 20 trials, as well as the mean of the
difference. When the preceding context does not have too many
comparisons of 2 with larger numbers, the model overwhelmingly
prefers the correct evaluation false, but when the context is
longer than 7 statements, the model usually prefers true.
ciding the next token after C[1,1]= with the short-range
inﬂuences, which make the token 1 most likely after two
1s in a row regardless of the longer context. At times,
short-range inﬂuences prevail and cause an incorrect execu-
tion. But, long-range self-attention can also inappropriately
overrule correct short-range reasoning. For instance, when
generating based on the Bubble Sort Prompt 4, the model
generates repetitive text that includes many statements of the
form ‘Because n<m is true/false ...,’ which
can create strong pattern overruling local evaluation of the
next inequality.
To demonstrate that, we evaluated the
likelihood of the next token after ‘Because 2<1 is’
for different lengths of context preceding this text. The
context had between 1 and 15 lines of text in the form
‘Because 2<m is true we ...’ with m ∈[3..9]
randomly chosen, e.g.
Because 2<3 is true we ...
Because 2<7 is true we ...
Because 2<5 is true we ...
Because 2<1 is
As we show in Fig 1, although the preceding context is
correct when evaluating the inequalities, the log odds of
an incorrect evaluation of 2<1 increase by over six or-
ders of magnitude with the length of this context. The
longer this context is, the more it reinforces the pattern
‘Because 2< ... true’: If 2 was smaller than a vari-
ety of numbers, then it is smaller than 1, too! Furthermore,

Programming in GPT
there is a large variation due to the random selection of m
in the examples in the context, indicating a variety of other
patterns that drive the generation (The ﬁgure shows the band
between the maximum and minimum log odds over 20 runs).
For the contexts of length 7 the odds of picking true over
false become roughly even. IRSA can drive probabilities
to be so taut that rerunning the same API call with zero
temperature can sometimes return a different result (The
code behind the API presumably always adds a very small
constant to log probabilities before sampling). Skip-to-state
strategy in Section 2.3 is thus less sensitive to patterns that
result from program execution.
This fragility further emphasizes the difﬁculty in evaluating
LLMs on in-context learning tasks: Improving accuracy
may simply be a matter of spending more time designing
a prompt (becoming a GPT whisperer). Still, getting GPT
to execute the algorithms studied here was not excessively
hard, and it may even become easier on newer models.
5.3. And what about GPT-4?
A recent qualitative analysis of GPT-4 abilities (Bubeck
et al., 2023) includes one example of detailed execution of
a Python program for one input (in their Fig. 3.7). The LCS
algorithm is well-known, so would the newer and better
GPT-4 model execute it correctly and consistently across
different inputs? In Prompt 13, we show a prompt that
simply asks GPT-4 to show the LCS algorithm, execute
it, and report the result. On our LCS-S dataset, using this
prompt design and sampling with zero temperature, GPT-
4 gets the correct answer 49% of the times, just slightly
better than the ’Guessing’ baseline (Table 1). An alternative
prompt shown in Prompt 14, asks for intermediate steps
of execution to be shown before the answer is generated,
moving the prompting strategy closer to IRSA. This prompt
can be thought of as a version of Prompt 5, but lighter and
more straightforward, expecting GPT-4 to be able to show
program execution without strict speciﬁcations. This prompt
leads to the accuracy of 69% on LCS-S, still behind IRSA
result with codex (93%, Table 2). To illustrate why this
may be, in Prompt 15 we show the same prompt asking for
intermediate steps, but for a different input. The inputs in
Prompts 14 and 15 were processed differently, even though
everything else in the prompts was the same, and API calls
were made with zero temperature. In one case, only the
initial and end states of the “dp” matrix are shown, while in
the other, several steps (but not all!) are shown. Therefore, it
seems that GPT-4 is still hard to control without regimenting
self-attention more strictly.
6. Conclusion
We demonstrated that GPT-3 can be triggered to execute
iterative algorithms, including double loops with variable
termination conditions, by careful prompt design, which
emphasizes structure in an example of an execution path
of the desired algorithm, or in a collection of fragments of
such paths. A variety of algorithms we were able to execute
indicate that the GPT family is already close to being Turing-
complete, i.e., not only capable of recalling or composing
(hallucinating) code, but actually simulating its execution.
This has several consequences. For example, if LLMs are
Turing Machines (in addition to being natural language
translators and analyzers), their evaluation probably needs
to be rethought, esp. in cases where models are expected to
make inferences for which we have algorithms, because in-
context learning would cover prompts designed to execute
them (Section 5).
References
Bengio, Y.
The consciousness prior.
arXiv preprint
arXiv:1709.08568, 2017.
Brown, T., Mann, B., Ryder, N., Subbiah, M., Kaplan, J. D.,
Dhariwal, P., Neelakantan, A., Shyam, P., Sastry, G.,
Askell, A., Agarwal, S., Herbert-Voss, A., Krueger, G.,
Henighan, T., Child, R., Ramesh, A., Ziegler, D., Wu, J.,
Winter, C., Hesse, C., Chen, M., Sigler, E., Litwin, M.,
Gray, S., Chess, B., Clark, J., Berner, C., McCandlish,
S., Radford, A., Sutskever, I., and Amodei, D. Language
models are few-shot learners. Neural Information Pro-
cessing Systems (NeurIPS), 2020.
Bubeck, S., Chandrasekaran, V., Eldan, R., Gehrke, J.,
Horvitz, E., Kamar, E., Lee, P., Lee, Y. T., Li, Y., Lund-
berg, S., Nori, H., Palangi, H., Ribeiro, M. T., and Zhang,
Y. Sparks of artiﬁcial general intelligence: Early experi-
ments with gpt-4, 2023.
Chen, M., Tworek, J., Jun, H., Yuan, Q., Pinto, H. P. d. O.,
Kaplan, J., Edwards, H., Burda, Y., Joseph, N., Brockman,
G., Ray, A., Puri, R., Krueger, G., Petrov, M., Khlaaf,
H., Sastry, G., Mishkin, P., Chan, B., Gray, S., Ryder, N.,
Pavlov, M., Power, A., Kaiser, L., Bavarian, M., Winter,
C., Tillet, P., Such, F. P., Cummings, D., Plappert, M.,
Chantzis, F., Barnes, E., Herbert-Voss, A., Guss, W. H.,
Nichol, A., Paino, A., Tezak, N., Tang, J., Babuschkin, I.,
Balaji, S., Jain, S., Saunders, W., Hesse, C., Carr, A. N.,
Leike, J., Achiam, J., Misra, V., Morikawa, E., Radford,
A., Knight, M., Brundage, M., Murati, M., Mayer, K.,
Welinder, P., McGrew, B., Amodei, D., McCandlish, S.,
Sutskever, I., and Zaremba, W. Evaluating large language
models trained on code, 2021. URL https://arxiv.
org/abs/2107.03374.
Chowdhery, A., Narang, S., Devlin, J., Bosma, M., Mishra,
G., Roberts, A., Barham, P., Chung, H. W., Sutton, C.,
Gehrmann, S., et al. PaLM: Scaling language modeling
with pathways. arXiv preprint arXiv:2204.02311, 2022.

Programming in GPT
Creswell, A., Shanahan, M., and Higgins, I. Selection-
inference:
Exploiting large language models for
interpretable
logical
reasoning.
arXiv
preprint
arXiv:2205.09712, 2022.
Gao, L., Madaan, A., Zhou, S., Alon, U., Liu, P., Yang, Y.,
Callan, J., and Neubig, G. Pal: Program-aided language
models. arXiv preprint arXiv:2211.10435, 2022.
Goyal, A. and Bengio, Y. Inductive biases for deep learning
of human cognition. arXiv preprint arXiv:2011.15091,
2020.
Kahneman, D. Thinking, fast and slow. Macmillan, 2011.
Khot, T., Trivedi, H., Finlayson, M., Fu, Y., Richardson, K.,
Clark, P., and Sabharwal, A. Decomposed prompting:
A modular approach for solving complex tasks. arXiv
preprint arXiv:2210.02406, 2022.
Kojima, T., Gu, S. S., Reid, M., Matsuo, Y., and Iwasawa,
Y. Large language models are zero-shot reasoners. arXiv
preprint arXiv:2205.11916, 2022.
Li, Y., Choi, D., Chung, J., Kushman, N., Schrittwieser, J.,
Leblond, R., Eccles, T., Keeling, J., Gimeno, F., Dal Lago,
A., et al. Competition-level code generation with alpha-
code. Science, 378(6624):1092–1097, 2022a.
Li, Y., Lin, Z., Zhang, S., Fu, Q., Chen, B., Lou, J.-G.,
and Chen, W. On the advance of making language mod-
els better reasoners. arXiv preprint arXiv:2206.02336,
2022b.
Liu, J., Shen, D., Zhang, Y., Dolan, B., Carin, L., and Chen,
W. What makes good in-context examples for gpt-3?
arXiv preprint arXiv:2101.06804, 2021.
Liu, Z., Patwary, M., Prenger, R., Prabhumoye, S., Ping, W.,
Shoeybi, M., and Catanzaro, B. Multi-stage prompting
for knowledgeable dialogue generation. In Findings of the
Association for Computational Linguistics: ACL 2022,
pp. 1317–1337, Dublin, Ireland, May 2022. Association
for Computational Linguistics. doi: 10.18653/v1/2022.
ﬁndings-acl.104.
URL https://aclanthology.
org/2022.findings-acl.104.
Lu, Y., Bartolo, M., Moore, A., Riedel, S., and Stene-
torp, P.
Fantastically ordered prompts and where to
ﬁnd them: Overcoming few-shot prompt order sensi-
tivity.
In Proceedings of the 60th Annual Meeting
of the Association for Computational Linguistics (Vol-
ume 1: Long Papers), pp. 8086–8098, Dublin, Ireland,
May 2022. Association for Computational Linguistics.
doi: 10.18653/v1/2022.acl-long.556.
URL https:
//aclanthology.org/2022.acl-long.556.
Malkin, N., Wang, Z., and Jojic, N.
Coherence boost-
ing: When your pretrained language model is not paying
enough attention. In Proceedings of the 60th Annual
Meeting of the Association for Computational Linguistics
(Volume 1: Long Papers), pp. 8214–8236, 2022.
Mialon, G., Dess`ı, R., Lomeli, M., Nalmpantis, C., Pa-
sunuru, R., Raileanu, R., Rozi`ere, B., Schick, T., Dwivedi-
Yu, J., Celikyilmaz, A., et al. Augmented language mod-
els: a survey. arXiv preprint arXiv:2302.07842, 2023.
Min, S., Lyu, X., Holtzman, A., Artetxe, M., Lewis, M.,
Hajishirzi, H., and Zettlemoyer, L. Rethinking the role of
demonstrations: What makes in-context learning work?
arXiv preprint arXiv:2202.12837, 2022.
Nye, M., Andreassen, A. J., Gur-Ari, G., Michalewski, H.,
Austin, J., Bieber, D., Dohan, D., Lewkowycz, A., Bosma,
M., Luan, D., et al. Show your work: Scratchpads for
intermediate computation with language models. arXiv
preprint arXiv:2112.00114, 2021.
OpenAI. Gpt-4 technical report, 2023.
Ozturkler, B., Malkin, N., Wang, Z., and Jojic, N. Thinksum:
Probabilistic reasoning over sets using large language
models, 2022.
URL https://arxiv.org/abs/
2210.01293.
Paranjape, B., Lundberg, S., Singh, S., Hajishirzi, H., Zettle-
moyer, L., and Ribeiro, M. T. Art: Automatic multi-step
reasoning and tool-use for large language models. arXiv
preprint arXiv:2303.09014, 2023.
Parisi, A., Zhao, Y., and Fiedel, N. Talm: Tool augmented
language models.
arXiv preprint arXiv:2205.12255,
2022.
Peng, S., Kalliamvakou, E., Cihon, P., and Demirer, M. The
impact of ai on developer productivity: Evidence from
github copilot, 2023. URL https://arxiv.org/
abs/2302.06590.
Press, O., Zhang, M., Min, S., Schmidt, L., Smith, N. A.,
and Lewis, M.
Measuring and narrowing the com-
positionality gap in language models. arXiv preprint
arXiv:2210.03350, 2022.
Rae, J. W., Borgeaud, S., Cai, T., Millican, K., Hoffmann, J.,
Song, F., Aslanides, J., Henderson, S., Ring, R., Young,
S., et al.
Scaling language models: Methods, analy-
sis & insights from training Gopher.
arXiv preprint
arXiv:2112.11446, 2021.
Schick, T., Dwivedi-Yu, J., Dess`ı, R., Raileanu, R., Lomeli,
M., Zettlemoyer, L., Cancedda, N., and Scialom, T. Tool-
former: Language models can teach themselves to use
tools. arXiv preprint arXiv:2302.04761, 2023.

Programming in GPT
Schuurmans, D.
Memory augmented large language
models are computationally universal. arXiv preprint
arXiv:2301.04589, 2023.
Shi, F., Chen, X., Misra, K., Scales, N., Dohan, D., Chi, E.,
Sch¨arli, N., and Zhou, D. Large language models can
be easily distracted by irrelevant context. arXiv preprint
arXiv:2302.00093, 2023.
Shwartz, V., West, P., Le Bras, R., Bhagavatula, C., and
Choi, Y. Unsupervised commonsense question answer-
ing with self-talk. In Proceedings of the 2020 Confer-
ence on Empirical Methods in Natural Language Pro-
cessing (EMNLP), pp. 4615–4629, Online, November
2020. Association for Computational Linguistics. doi:
10.18653/v1/2020.emnlp-main.373. URL https://
aclanthology.org/2020.emnlp-main.373.
Srivastava, A., Rastogi, A., Rao, A., Shoeb, A. A. M., Abid,
A., Fisch, A., Brown, A. R., Santoro, A., Gupta, A.,
Garriga-Alonso, A., et al. Beyond the imitation game:
Quantifying and extrapolating the capabilities of language
models. arXiv preprint arXiv:2206.04615, 2022.
Suzgun, M., Scales, N., Sch¨arli, N., Gehrmann, S., Tay,
Y., Chung, H. W., Chowdhery, A., Le, Q. V., Chi,
E. H., Zhou, D., et al. Challenging big-bench tasks and
whether chain-of-thought can solve them. arXiv preprint
arXiv:2210.09261, 2022.
Turing, A. M.
On computable numbers, with an ap-
plication to the Entscheidungsproblem.
Proceedings
of the London Mathematical Society, 2(42):230–265,
1936.
URL http://www.cs.helsinki.fi/u/
gionis/cc05/OnComputableNumbers.pdf.
Tversky, A. and Kahneman, D. Judgment under uncertainty:
Heuristics and biases: Biases in judgments reveal some
heuristics of thinking under uncertainty. Science, 185
(4157):1124–1131, 1974.
Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E., and
Zhou, D. Rationale-augmented ensembles in language
models. arXiv preprint arXiv:2207.00747, 2022a.
Wang, X., Wei, J., Schuurmans, D., Le, Q., Chi, E.,
and Zhou, D.
Self-consistency improves chain of
thought reasoning in language models. arXiv preprint
arXiv:2203.11171, 2022b.
Wei, J., Wang, X., Schuurmans, D., Bosma, M., Chi, E.,
Le, Q., and Zhou, D. Chain of thought prompting elic-
its reasoning in large language models. arXiv preprint
arXiv:2201.11903, 2022.
Yao, S., Zhao, J., Yu, D., Du, N., Shafran, I., Narasimhan,
K., and Cao, Y. React: Synergizing reasoning and acting
in language models. arXiv preprint arXiv:2210.03629,
2022.
Zelikman, E., Wu, Y., and Goodman, N. D. STaR: Boot-
strapping reasoning with reasoning.
arXiv preprint
arXiv:2203.14465, 2022.
Zhao, Z., Wallace, E., Feng, S., Klein, D., and Singh, S.
Calibrate before use: Improving few-shot performance
of language models.
In International Conference on
Machine Learning, pp. 12697–12706. PMLR, 2021.
Zhou, D., Sch¨arli, N., Hou, L., Wei, J., Scales, N., Wang,
X., Schuurmans, D., Bousquet, O., Le, Q., and Chi, E.
Least-to-most prompting enables complex reasoning in
large language models. arXiv preprint arXiv:2205.10625,
2022.

Programming in GPT
Prompt 5. Interpreter/Compiler prompt that can execute
LCS algorithm. Playground link (use 0 temperature)
Matrix C contains values, e.g:
C[0,0]=1 C[0,1]=6 C[0,2]=11 C[0,3]=16 C[0,4]=21
C[1,0]=2 C[1,1]=7 C[1,2]=12 C[1,3]=17 C[1,4]=22
To query a value:
Show(C[1,2])
<state> C[1,2]=12 </state>
Query an undefined variable
Show(a)
<state> a=0 </state>
To set a value:
C[0,1]:=8
C[0,1] was 6. Now C[0,1]=8.
a:=5
a was 0. Now a=5.
To query multiple variables:
Show(a, C[0:1,0:4])
<state>
a=5
C[0,0]=1 C[0,1]=8 C[0,2]=11 C[0,3]=16 C[0,4]=21
C[1,0]=2 C[1,1]=7 C[1,2]=12 C[1,3]=17 C[1,4]=22
</state>
Program:
N:=1
for i from 0 to N
C[i,i]=-3
Execute:
i:=0
C[0,0]:=-3
i:=1
C[1,1]:=-3
Finished with i=N, so done with i loop
Show(C)
<state>
C[0,0]=-3 C[0,1]=8 C[0,2]=11 C[0,3]=16 C[0,4]=21
C[1,0]=2 C[1,1]=-3 C[1,2]=12 C[1,3]=17 C[1,4]=22
</state>
Program:
if a<C[0,4]
C[0,0]:=5
else
C[0,1]:=2
Execute:
Check if a<C[0,1]?
a is 5 C[0,1] is 8 Is 5<8?...
... Yes. C[0,0]:=5
Done
Show(C[0,0])
<state> C[0,0]=5 </state>
Program:
C[0,2]:=detailed_max(C[0,3],C[0,4])
Execute:
C[0,3] is 16, C[0,4] is 21. C[0,2] is the greater of...
...them. C[0,2]:=21
Done
LCS:
Input: TA ATA End of input
LCS Prep:
a[1]=T a[2]=A
b[1]=A b[2]=T b[3]=A
M=2 N=3
LCS program:
Show(a,b,M,N)
for i from 1 to M
for j from 1 to N
if a[i]==b[j]
C[i,j]:=C[i-1,j-1]+1
else
C[i,j]:=detailed_max(C[i,j-1],C[i-1,j])
Show(i, j, M, N, C[0:i,0:N])
Show(’END’)
Execute:
Prompt 6. In response to Prompt 5, the model returns the
correct LCS execution path shown here
...
Execute:
<state>
a[1]=T a[2]=A
b[1]=A b[2]=T b[3]=A
M=2 N=3
</state>
i:=1
j:=1
Check if a[1]==b[1]?
a[1] is T b[1] is A Is T==A?...
... No. C[1,1]:=detailed_max(C[1,0],C[0,1])
... C[1,0] is 0, C[0,1] is 0. C[1,1] is the greater of
...them. C[1,1]:=0
<state>
i=1 j=1 M=2 N=3
C[0,0]=0 C[0,1]=0 C[0,2]=0 C[0,3]=0
C[1,0]=0 C[1,1]=0 C[1,2]=0 C[1,3]=0
</state>
j:=2
Check if a[1]==b[2]?
a[1] is T b[2] is T Is T==T?...
... Yes. C[1,2]:=C[0,1]+1
... C[0,1] is 0. C[1,2]:=1
<state>
i=1 j=2 M=2 N=3
C[0,0]=0 C[0,1]=0 C[0,2]=0 C[0,3]=0
C[1,0]=0 C[1,1]=0 C[1,2]=1 C[1,3]=0
</state>
j:=3
Check if a[1]==b[3]?
a[1] is T b[3] is A Is T==A?...
... No. C[1,3]:=detailed_max(C[1,2],C[0,3])
... C[1,2] is 1, C[0,3] is 0. C[1,3] is the greater of
...them. C[1,3]:=1
<state>
i=1 j=3 M=2 N=3
C[0,0]=0 C[0,1]=0 C[0,2]=0 C[0,3]=0
C[1,0]=0 C[1,1]=0 C[1,2]=1 C[1,3]=1
</state>
i:=2
j:=1
Check if a[2]==b[1]?
a[2] is A b[1] is A Is A==A?...
... Yes. C[2,1]:=C[1,0]+1
... C[1,0] is 0. C[2,1]:=1
<state>
i=2 j=1 M=2 N=3
C[0,0]=0 C[0,1]=0 C[0,2]=0 C[0,3]=0
C[1,0]=0 C[1,1]=0 C[1,2]=1 C[1,3]=1
C[2,0]=0 C[2,1]=1 C[2,2]=0 C[2,3]=0
</state>
j:=2
Check if a[2]==b[2]?
a[2] is A b[2] is T Is A==T?...
... No. C[2,2]:=detailed_max(C[2,1],C[1,2])
... C[2,1] is 1, C[1,2] is 1. C[2,2] is the greater of
...them. C[2,2]:=1
<state>
i=2 j=2 M=2 N=3
C[0,0]=0 C[0,1]=0 C[0,2]=0 C[0,3]=0
C[1,0]=0 C[1,1]=0 C[1,2]=1 C[1,3]=1
C[2,0]=0 C[2,1]=1 C[2,2]=1 C[2,3]=0
</state>
j:=3
Check if a[2]==b[3]?
a[2] is A b[3] is A Is A==A?...
... Yes. C[2,3]:=C[1,2]+1
... C[1,2] is 1. C[2,3]:=2
<state>
i=2 j=3 M=2 N=3
C[0,0]=0 C[0,1]=0 C[0,2]=0 C[0,3]=0
C[1,0]=0 C[1,1]=0 C[1,2]=1 C[1,3]=1
C[2,0]=0 C[2,1]=1 C[2,2]=1 C[2,3]=2
</state>
<state>
END
</state>

Programming in GPT
Prompt 7. Full single path execution path prompt using the state description as in the fragmented Prompt 4. Playground
link (use with 0 temperature)
Problem: 2, 3, 1, 5
EXECUTION
Length of the list: L=4
Number of pairs: P=3
a=[2 3 1 5]
set n_swaps=0. set i=P=3. set swap_flag=true.
<state> a=[2 3 1 5] i=3 P=3 n_swaps=0 swap_flag=true </state>
Since i=3 and P=3, i and P are equal, so this iteration is done, but swap_flag is true,
so we need another iteration
Iteration:
set swap_flag=false.
set i=0. The state is:
<state> a=[2 3 1 5] i=0 P=3 n_swaps=0 swap_flag=false </state>
Since i=0 and P=3, these two are different, so we continue
a[i]=a[0]=2 a[i+1]=a[1]=3
Because 2<3 is true we keep state as is and move on by increasing i
<state> a=[2 3 1 5] i=1 P=3 n_swaps=0 swap_flag=false </state>
Since i=1 and P=3, these two are different, so we continue
a[i]=a[1]=3 a[i+1]=a[2]=1
Because 3<1 is false we set swap_flag=true,increase n_swaps by one, and in a=[2 3 1 5] swap 3 and 1,
and increase i, and keep P as is to get
<state> a=[2 1 3 5] i=2 P=3 n_swaps=1 swap_flag=true </state>
Since i=2 and P=3, these two are different, so we continue
a[i]=a[2]=3 a[i+1]=a[3]=5
Because 3<5 is true we keep state as is and move on by increasing i
<state> a=[2 1 3 5] i=3 P=3 n_swaps=1 swap_flag=true </state>
Since i=3 and P=3, these two are equal, so this iteration is done, but swap_flag is true,
so we need another iteration
Iteration:
set swap_flag=false.
set i=0. The state is:
<state> a=[2 1 3 5] i=0 P=3 n_swaps=1 swap_flag=false </state>
Since i=0 and P=3, these two are different, so we continue
a[i]=a[0]=2 a[i+1]=a[1]=1
Because 2<1 is false we set swap_flag=true,increase n_swaps by one, and in a=[2 1 3 5] swap 2 and 1,
and increase i, and keep P as is to get
<state> a=[1 2 3 5] i=1 P=3 n_swaps=2 swap_flag=true </state>
Since i=1 and P=3, these two are different, so we continue
a[i]=a[1]=2 a[i+1]=a[2]=3
Because 2<3 is true we keep state as is and move on by increasing i
<state> a=[1 2 3 5] i=2 P=3 n_swaps=2 swap_flag=true </state>
Since i=2 and P=3, these two are different, so we continue
a[i]=a[2]=3 a[i+1]=a[3]=5
Because 3<5 is true we keep state as is and move on by increasing i
<state> a=[1 2 3 5] i=3 P=3 n_swaps=2 swap_flag=true </state>
Since i=3 and P=3, these two are equal, so this iteration is done, but swap_flag is true,
so we need another iteration
Iteration:
set swap_flag=false.
set i=0. The state is:
<state> a=[1 2 3 5] i=0 P=3 n_swaps=2 swap_flag=false </state>
Since i=0 and P=3, these two are different, so we continue
a[i]=a[0]=1 a[i+1]=a[1]=2
Because 1<2 is true we keep state as is and move on by increasing i
<state> a=[1 2 3 5] i=1 P=3 n_swaps=2 swap_flag=false </state>
Since i=1 and P=3, these two are different, so we continue
a[i]=a[1]=2 a[i+1]=a[2]=3
Because 2<3 is true we keep state as is and move on by increasing i
<state> a=[1 2 3 5] i=2 P=3 n_swaps=2 swap_flag=false </state>
Since i=2 and P=3, these two are different, so we continue
a[i]=a[2]=3 a[i+1]=a[3]=5
Because 3<5 is true we keep state as is and move on by increasing i
<state> a=[1 2 3 5] i=3 P=3 n_swaps=2 swap_flag=false </state>
Since i=3 and P=3, these two are equal, so this iteration is done, but swap_flag is false, so we are done
Final List: 1, 2, 3, 5
Number of swaps: 2
END OF EXECUTION
Problem: 3, 6, 8, 2, 7
EXECUTION

Programming in GPT
Prompt 8. Prompt that triggers execution of the search for
the longest substring without repeating characters. Play-
ground link (use 0 temperature)
Input: s = c, b, c, a, b, b
START
Unique letters: a, b, c
Define variables last_a=0, last_b=0, last_c=0
Length of sequence s:
L=6
Because L is 6, the needed number of iterations is 6
set st_ind=1
st m_len=0
set i=1
Iteration 1:
s(1) is c, so use last_c
last_c is 0, so
nothing to do here.
max(m_len,
i-st_ind+1) is max(0, 1-1+1) which is...
...max(0,1)=1, so we set m_len=1
since i is 1, and the letter is c, set last_c=1
increase i by one
i=2, st_ind=1, m_len=1, last_a=0, last_b=0, last_c=1
End of iteration 1. But we need to do 6 iterations,...
...so we do another one
Iteration 2:
s(2) is b, so use last_b
last_b is 0, so
nothing to do here.
max(m_len,
i-st_ind+1) is max(1, 2-1+1) which is...
...max(1, 2)=2, so we set m_len=2
since i is 2, and the letter is b, set last_b=2
increase i by one
i=3, st_ind=1, m_len=2, last_a=0, last_b=2, last_c=1
End of iteration 2.
But we need to do 6 iterations,...
...so we do another one
Iteration 3:
s(3) is c, so use last_c
last_c is greater than 0, so we reason...
...max(st_ind, last_c+1) is max(1, 2)=2...
...so we set st_ind=2
max(m_len,
i-st_ind+1) is max(2, 3-2+1) which is...
...max(2, 2)=2, so we set m_len=2
since i is 3, and the letter s(3) is c, set last_c=3
increase i by one
i=4, st_ind=2, m_len=2, last_a=0, last_b=2, last_c=3
End of iteration 2. But we need to do 6 iterations,...
...so we do another one
Iteration 4:
s(4) is a, so use last_a
last_a is 0, so
nothing to do here.
max(m_len,
i-st_ind+1) is max(2, 4-2+1) which is...
...max(2, 3)=3, so we set m_len=3
since i is 4, and the letter s(4) is a, set last_a=4
increase i by one
i=5, st_ind=2, m_len=3, last_a=4, last_b=2, last_c=3
End of iteration 4. But we need to do 6 iterations,...
...so we do another one
Iteration 5:
s(5) is b, so use last_b
last_b is greater than 0, so we reason...
...max(st_ind, last_b+1) is max(2, 2+1) which is...
...max(2, 3)=3 so we set st_ind=3
max(m_len,
i-st_ind+1) is max(3, 5-3+1) which is...
...max(3, 3)=3, so we set m_len=3
since i is 5, and the letter s(5) is b, set last_b=5
increase i by one
i=6, st_ind=3, m_len=3, last_a=4, last_b=5, last_c=3
End of iteration 5. But we need to do 6 iterations,...
...so we do another one
Iteration 6:
s(6) is b, so use last_b
last_b is greater than 0, so we reason...
...max(st_ind, last_b+1) is max(3, 5+1) which is...
...max(3, 6)=6 so we set st_ind=6
max(m_len,
i-st_ind+1) is max(3, 6-6+1) which is...
...max(3, 1)=3, so we set m_len=3
since i is 6, and the letter s(6) is b, set last_b=6
increase i by one
i=7, st_ind=6, m_len=3, last_a=4, last_b=6, last_c=3
End of iteration 6. We needed to do 6 iterations,...
...so we are done
The solution is: m_len=3
END
Input: s = p, w, w, k, e, p, z
START
Prompt 9. Prompt that triggers evaluation of parentheses
using a stack. Full prompt in playground, and Prompt as
here (with 10 steps missing) in playground. Meant to be
used with 0 temperature.
input: ) [ { } ] ( { } ) [ ( { } ) ] } {
input wriritten as a sequence of symbols:
s= ’)’, ’[’, ’{’, ’}’, ’]’, ’(’, ’{’,’}’, ’)’, ’[’,
’(’, ’{’, ’}’, ’)’,’]’, ’}’, ’{’
length(s)= 17
stack is initialized as empty
i=0
there is nothing in stack, so push s(0)= ’)’ on stack
stack= )
are the last two symbols an open and a closed
parenthesis of the same type? No. Stack stays same.
i=1
we push s(1)=’[’ on the stack
stack= ) [
are the last two symbols an open and a closed
parenthesis of the same type? No. Stack stays same.
i=2
we push s(2)=’{’ to the stack
stack= ) [ {
are the last two symbols an open and a closed
parenthesis of the same type? No. Stack stays same.
i=3
we push s(3)=’}’ to the stack
stack= ) [ { }
are the last two symbols an open and a closed
parenthesis of the same type? Yes, they are { },
opening then closing.
We pop the last two symbols from the stack.
stack= ) [
i=4
we push s(4)=’]’ to the stack
stack= ) [ ]
are the last two symbols an open and a closed
parenthesis of the same type? Yes, they are [ ],
opening then closing.
We pop the last two symbols from the stack
stack= )
i=5
we push s(5)=’(’ to the stack
stack= ) (
...
i=15
we push s(15)=’}’ to the stack
stack= ) }
are the last two symbols an open and a closed
parenthesis of the same type? No. Stack stays same.
i=16
we push s(16)=’{’ to the stack
stack= ) } {
are the last two symbols an open and a closed
parenthesis of the same type? No. Stack stays same.
i=17
we have reached the end of the input string.
If the stack has some parenthesis left in it,
the sequence is invalid, otherwise,
if the stack is empty, it is valid.
Sequence is: invalid
END
input:

Programming in GPT
Prompt 10. A prompt (white background) for translating logical deduction puzzles from BIG-bench into a Wolfram
language (Mathematica) call. The response from GPT-3 Codex is shown green-shaded. Playground link (use 0 temperature)
Puzzle: The following objects need to be ordered. obj1 is the biggest. obj2 is smaller than obj3.
obj1 is bigger than obj2.
Question: Which object is the biggest?
Parsing step:
Items: obj1, obj2, obj3
Numbe of items: 3
Statement 1: obj1 is the biggest.
Statement 2: obj2 is smaller than obj3.
Statement 3: obj1 is bigger than obj2.
Scoring identification step:
Scores will refer to size.
Since we have 3 items, let’s assume that the biggest gets a score
of 3 pounds and the smallest gets
the score of 1 pound.
Translation step:
Available variable names: x, y, z, a, b, c
Map item scores of ’obj1’, ’obj2’, ’obj3’ to variable names x, y, z
obj1 score is x; obj2 score is y; obj3 is z;
Statement 1: ’x’ is the biggest.
Statement 2: ’y’ is smaller than ’z’.
Statement 3:
’x’ is bigger than ’y’.
Domain constraints: 1<=x<=3, 1<=y<=3, 1<=z<=3
Statemets as a set of inequalities or equalities: x==3, y<z, x>y
All varaibles are different: x!=y, x!=z, y!=z
Call to WolframAlpha:
Solve[{1<=x<=3, 1<=y<=3, 1<=z<=3, x==3, y<z, x>y, x!=y, x!=z, y!=z }, {x, y, z}, Integers]
END
Puzzle: A fruit stand sells five fruits: kiwis, pears, peaches, loquats, and apples.
The loquats are more expensive than the kiwis. The pears are more expensive than the peaches.
The apples are more expensive than the loquats. The apples are less expensive than the peaches.
Question: Which is the most expensive?
Parsing step:
Items: kiwis, pears, peaches, loquats, apples
Numbe of items: 5
Statement 1: The loquats are more expensive than the kiwis.
Statement 2: The pears are more expensive than the peaches.
Statement 3: The apples are more expensive than the loquats.
Statement 4: The apples are less expensive than the peaches.
Scoring identification step:
Scores will refer to price.
Since we have 5 items, let’s assume that the most expensive gets a score
of 5 dollars and the least expensive
gets the score of 1 dollar.
Translation step:
Available variable names: x, y, z, a, b, c
Map item scores of ’kiwis’, ’pears’, ’peaches’, ’loquats’, ’apples’ to variable names x, y, z, a, b
kiwis score is x; pears score is y; peaches is z; loquats is a; apples is b;
Statement 1: ’a’ is more expensive than ’x’.
Statement 2: ’y’ is more expensive than ’z’.
Statement 3:
’b’ is more expensive than ’a’.
Statement 4:
’b’ is less expensive than ’z’.
Domain constraints: 1<=x<=5, 1<=y<=5, 1<=z<=5, 1<=a<=5, 1<=b<=5
Statemets as a set of inequalities or equalities: a>x, y>z, b>a, b<z
All varaibles are different: x!=y, x!=z, x!=a, x!=b, y!=z, y!=a, y!=b, z!=a, z!=b, a!=b
Call to WolframAlpha:
Solve[{1<=x<=5, 1<=y<=5, 1<=z<=5, 1<=a<=5, 1<=b<=5, a>x, y>z, b>a, b<z, x!=y, x!=z, x!=a, x!=b, y!=z, y!=a,
y!=b, z!=a, z!=b, a!=b}, {x, y, z, a, b}, Integers]

Programming in GPT
Prompt 11. A zero-shot code prompt (white background) which, when temperature is carefully chosen may trigger code
execution (shaded green) by Codex. Although the sequence gets sorted, the execution goes awry after that. Importantly,
the number of swaps is not tracked, even though the code contains appropriate instructions. In contrast, IRSA prompts
consistently and repeatedly trigger correct program execution. See also Prompts 14 and 15 for examples of inconsistent
behaviour of GPT-4 with zero-shot code prompting.
# Here is the code for Bubble Sort algorithm that keeps track of the number of needed swaps:
def bubbleSort(arr):
n = len(arr)
# initiate the number of swaps to 0
n_swaps=0;
# Traverse through all array elements
for i in range(n):
# Last i elements are already in place
for j in range(0, n-i-1):
# traverse the array from 0 to n-i-1
# Swap if the element found is greater
# than the next element
if arr[j] > arr[j+1] :
arr[j], arr[j+1] = arr[j+1], arr[j]
n_swaps = n_swaps+1
Problem: 8, 7, 6, 4, 9
EXECUTION:
i=0
j=0
8 > 7
7, 8, 6, 4, 9
j=1
8 > 6
7, 6, 8, 4, 9
j=2
8 > 4
7, 6, 4, 8, 9
j=3
8 > 9
7, 6, 4, 8, 9
j=4
i=1
j=0
7 > 6
6, 7, 4, 8, 9
j=1
7 > 4
6, 4, 7, 8, 9
j=2
7 > 8
6, 4, 7, 8, 9
j=3
i=2
j=0
6 > 4
4, 6, 7, 8, 9
j=1
6 > 7
4, 6, 7, 8, 9
j=2
i=3
j=0
4 > 6
4, 6, 7, 8, 9
j=1
i=4
j=0
i=5
j=0
i=6
j=0
i=7
j=0
i=8
...

Programming in GPT
Prompt 12. An example of a baseline prompt design. Prompts with and without the code, shaded in gray, were used.
Zero shot prompts omitted the examples, also shaded in gray. Other than the (buggy) algorithm we adopted for the logical
deduction puzzle, the algorithms we tested are known to Codex, and we based the codes used in the prompts on the ones
generated by Codex itself.
# Python3 program to find the length
# of the longest substring
# without repeating characters in string s
# the maximum length of such a substring will be returned in m_len
def longestUniqueSubsttr(s):
# last index of every character
last_idx = {}
m_len = 0
# starting index of current
# window to calculate m_len
start_idx = 0
for i in range(0, len(s)):
# Find the last index of str[i]
# Update start_idx (starting index of current window)
# as maximum of current value of start_idx and last
# index plus 1
if s[i] in last_idx:
start_idx = max(start_idx, last_idx[s[i]] + 1)
# Update result if we get a larger window
m_len = max(m_len, i-start_idx + 1)
# Update last index of current char.
last_idx[s[i]] = i
return m_len
What would the algorithm above, which looks for the longest substring with non repeating characters
compute for a given problem? Use this format:
Input: s = ...
START
The solution is: m_len= ...
END
Input: s = a, a, c, a, d, b, b
START
The solution is: m_len=4
END
Input: s = b, b, a, a, d, b, b
START
The solution is: m_len=3
END
Input: s = c, a, d, c, b, c, a
START
The solution is: m_len=4
END
Input: s = a, c, b, a, c, d, d
START
The solution is: m_len=4
END
Input: s = b, b, d, d, a, d, b
START
The solution is: m_len=3
END
Input: s = a, c, d, c, a, c, a
START

Programming in GPT
Prompt 13. Given the non-shaded text as a prompt, GPT-4 (0 temperature) returns the green shaded response.
We need to compute the longest common subsequence for two sequences
s1=bccba
s2=ccaa
using the dynamic programming algorithm. Show the python code for the algorithm, and then execute it. Finally,
output the length of the longest common subsequence bracketed with <answer> and </answer>.
Here’s the Python code for the dynamic programming algorithm to find the longest common subsequence:
‘‘‘python
def longest_common_subsequence(s1, s2):
m = len(s1)
n = len(s2)
dp = [[0] * (n + 1) for _ in range(m + 1)]
for i in range(1, m + 1):
for j in range(1, n + 1):
if s1[i - 1] == s2[j - 1]:
dp[i][j] = dp[i - 1][j - 1] + 1
else:
dp[i][j] = max(dp[i - 1][j], dp[i][j - 1])
return dp[m][n]
s1 = "bccba"
s2 = "ccaa"
lcs_length = longest_common_subsequence(s1, s2)
print(f"<answer>{lcs_length}</answer>")
‘‘‘
Now, let us execute the code and find the length of the longest common subsequence:
‘‘‘python
s1 = "bccba"
s2 = "ccaa"
lcs_length = longest_common_subsequence(s1, s2)
print(f"<answer>{lcs_length}</answer>")
‘‘‘
Output:
‘‘‘
<answer>3</answer>
‘‘‘

Programming in GPT
Prompt 14. Given the non-shaded text as a prompt, GPT-4 (0 temperature) returns the green shaded response. The prompt
attempts to trigger tracking steps of program execution, but only the initial and ﬁnal matrix are shown.
We need to compute the longest common subsequence for two sequences
s1=bccba
s2=ccaa
using the dynamic programming algorithm. Show the python code for the algorithm, and then write down its
execution with intermediate steps. Finally, output the length of the longest common subsequence bracketed
with <answer> and </answer>.
Here’s the Python code for the dynamic programming algorithm to find the longest common subsequence:
‘‘‘python
def longest_common_subsequence(s1, s2):
m = len(s1)
n = len(s2)
dp = [[0] * (n + 1) for _ in range(m + 1)]
for i in range(1, m + 1):
for j in range(1, n + 1):
if s1[i - 1] == s2[j - 1]:
dp[i][j] = dp[i - 1][j - 1] + 1
else:
dp[i][j] = max(dp[i - 1][j], dp[i][j - 1])
return dp[m][n]
s1 = "bccba"
s2 = "ccaa"
print(longest_common_subsequence(s1, s2))
‘‘‘
Now let us s go through the execution with intermediate steps:
1. Initialize the ’dp’ table with dimensions (m+1) x (n+1), where m and n are the lengths of s1 and s2,
respectively. The table will look like this:
0 0 0 0 0
0 0 0 0 0
0 0 0 0 0
0 0 0 0 0
0 0 0 0 0
0 0 0 0 0
2. Iterate through the table, comparing characters from s1 and s2. Update the table based on the dynamic
programming formula:
0 0 0 0 0
0 0 1 1 1
0 1 1 1 1
0 1 2 2 2
0 1 2 2 2
0 1 2 2 3
3. The length of the longest common subsequence is in the bottom-right cell of the table, which is 3.
So, the length of the longest common subsequence is <answer>3</answer>.

Programming in GPT
Prompt 15. Given the same non-shaded text as in Prompt 14 as a prompt, just for a different input, GPT-4 (0 temperature)
returns the green shaded response that this time does list more (but not all!) intermediate steps. This style of prompting
GPT-4 reaches accuracy of 69% on LCS-S, compared to 93% achieved with IRSA in GPT-3.
We need to compute the longest common subsequence for two sequences
s1=aaca
s2=abab
using the dynamic programming algorithm. Show the python code for the algorithm, and then write down its
execution with intermediate steps. Finally, output the length of the longest common subsequence bracketed
with <answer> and </answer>.
Here is the Python code for the dynamic programming algorithm to compute the longest common subsequence:
‘‘‘python
def longest_common_subsequence(s1, s2):
m = len(s1)
n = len(s2)
dp = [[0]*(n+1) for _ in range(m+1)]
for i in range(m+1):
for j in range(n+1):
if i == 0 or j == 0:
dp[i][j] = 0
elif s1[i-1] == s2[j-1]:
dp[i][j] = dp[i-1][j-1] + 1
else:
dp[i][j] = max(dp[i-1][j], dp[i][j-1])
return dp[m][n]
s1 = "aaca"
s2 = "abab"
result = longest_common_subsequence(s1, s2)
print(result)
‘‘‘
Here is the execution with intermediate steps:
1. Initialize the ’dp’ table with dimensions (m+1) x (n+1). For the given sequences, it will look like:
0 0 0 0 0
0
0 0 0 0 0
0
0 0 0 0 0
0
0 0 0 0 0
0
0 0 0 0 0
2. Fill in the ’dp’ table by iterating through each cell and apply the algorithm rules:
For i, j = 1, 1: s1[i-1] != s2[j-1], so dp[i][j] = max(dp[i-1][j], dp[i][j-1]) = max(0, 0) = 0
0 0 0 0 0
0
0 0 0 0 0
0
0 0 0 0 0
0
0 0 0 0 0
0
0 0 0 0 0
For i, j = 1, 2: s1[i-1] != s2[j-1], so dp[i][j] = max(dp[i-1][j], dp[i][j-1]) = max(0, 0) = 0
0 0 0 0 0
0
0 0 0 0 0
0
0 0 0 0 0
0
0 0 0 0 0
0
0 0 0 0 0
For i, j = 1, 3: s1[i-1] == s2[j-1], so dp[i][j] = dp[i-1][j-1] + 1 = 0 + 1 = 1
0 0 0 0 0
0
0 0 0 0 0
0
0 0 1 0 0
0
0 0 0 0 0
0
0 0 0 0 0
Continue filling in the table with the same rules:
0 0 0 0 0
0
0 0 0 0 0
0
0 0 1 0 0
0
0 1 1 1 0
0
0 1 1 2 0
3. The value in the bottom-right cell contains the length of the longest common subsequence:
<answer>2</answer>

