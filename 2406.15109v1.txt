Brain-Like Language Processing via a
Shallow Untrained Multihead Attention Network
Badr AlKhamissi1
Greta Tuckute2
Antoine Bosselut∗,1
Martin Schrimpf∗,1
1EPFL
2MIT
Abstract
Large Language Models (LLMs) have been shown to be effective models of the
human language system, with some models predicting most explainable variance
of brain activity in current datasets. Even in untrained models, the representations
induced by architectural priors can exhibit reasonable alignment to brain data. In
this work, we investigate the key architectural components driving the surprising
alignment of untrained models. To estimate LLM-to-brain similarity, we first select
language-selective units within an LLM, similar to how neuroscientists identify the
language network in the human brain. We then benchmark the brain alignment of
these LLM units across five different brain recording datasets. By isolating critical
components of the Transformer architecture, we identify tokenization strategy
and multihead attention as the two major components driving brain alignment. A
simple form of recurrence further improves alignment. We further demonstrate
this quantitative brain alignment of our model by reproducing landmark studies
in the language neuroscience field, showing that localized model units – just like
language voxels measured empirically in the human brain – discriminate more
reliably between lexical than syntactic differences, and exhibit similar response
profiles under the same experimental conditions. Finally, we demonstrate the utility
of our model’s representations for language modeling, achieving improved sample
and parameter efficiency over comparable architectures. Our model’s estimates of
surprisal sets a new state-of-the-art in the behavioral alignment to human reading
times. Taken together, we propose a highly brain- and behaviorally-aligned model
that conceptualizes the human language system as an untrained shallow feature
encoder, with structural priors, combined with a trained decoder to achieve efficient
and performant language processing.1
1
Introduction
Deciphering the brain’s algorithms underlying our ability to process language and communicate is
a core goal in neuroscience. Our ability to process language is supported by the human language
system (HLS), a set of left-lateralized frontotemporal regions in the brain (Binder et al., 1997; Bates
et al., 2003; Gorno-Tempini et al., 2004; Price, 2010; Fedorenko, 2014; Hagoort, 2019, Figure 1)
that respond robustly and selectively to linguistic input (Fedorenko et al., 2024). Driven by recent
advances in machine learning, large language models (LLMs) trained via next-word prediction
on large corpora of text, are now a particularly promising model family to capture the internal
processes of the HLS. When exposed to the same text stimuli (e.g., sentences or narratives) as human
participants during neuroimaging and electrophysiology sessions, certain LLMs predict most of the
∗Equal Supervision
1Code and data available at https://github.com/bkhmsi/brain-language-suma
Preprint. Under review.
arXiv:2406.15109v1  [cs.CL]  21 Jun 2024

THE DOG CHASED THE CAT ALL DAY LONG
BOKER DESH HE THE DRILES LER CICE FRISTY'S 
Sentence:
Non-Words:
Language Network Localization
Benchmarking
Extract Top-K 
Language Selective Activations
Pereira2018
(fMRI, Reading)
Blank2014
(fMRI, Listening)
Fedorenko2016
(ECoG, Reading)
Proposed Model
Trained 
Transformer Blocks
Recurrence
via
Shared
Weights
Untrained
Multihead
Attention
Language
Input Text
Tokenize Text
Localize Lang Units
Brain Responses (Blank et al. 2014)
Model Unit Activations
Tuckute2024
(fMRI, Reading)
Sentence
Non-Words
Contrast
Extract Top-K 
Language Selective Activations
Sentence
Non-Words
Contrast
Method: Fedorenko et al. (2010)
Our Method
Flatten
Flatten
Model Localized Language
Representations
Brain Localized Language
Representations
...
...
Stimuli
Similarity
Score
Stimuli
Localizing Language Selective Units from the Brain and Models 
Wehbe2014
(fMRI, Reading)
Figure 1: Evaluating Model Alignment to the Human Language System. (Left) Localization: We
select the top-k language selective units in models and brain recordings by contrasting the difference
in unit activations between sentences and lists of non-words, following Fedorenko et al. (2010).
(Center) Benchmarking: Across five different neural datasets, we measure the alignment between
language selective units in models and the human brain. Each model’s score is the mean of linear
predictivity scores for each of the five datasets. We built our model using the first three benchmarks
for validation and additionally report scores on two held-out benchmarks. (Right) Proposed Model:
We conceptualize language processing in the human brain as an untrained feature encoder (SUMA)
providing representations to a downstream trainable decoder that produces language output.
variance of human neural responses relative to the estimated reliability of the datasets (Schrimpf
et al., 2021; Caucheteux and King, 2022; Pasquiou et al., 2022). One surprising observation is that
untrained models exhibit internal representations that are about half as brain-like as those of their
trained counterparts (Schrimpf et al., 2021; Pasquiou et al., 2022). In this work, we investigate the
reasons underlying the high brain alignment of untrained models, and, more broadly, isolate the
critical model components that enable LLMs to capture human responses to language.
To isolate the components driving model-to-brain alignment, we incrementally construct a single
Transformer block from the ground up, beginning with static embeddings for each word and progress-
ing to a complete single-block architecture. At each stage of this process, we measure brain alignment
across three datasets (Pereira et al., 2018; Fedorenko et al., 2016; Blank et al., 2014) to guide our de-
sign choices and subsequently evaluate our final model on two held-out datasets (Tuckute et al., 2024;
Wehbe et al., 2014). Notably, we find that even without any training, the representations encoded by
architectural priors alone are highly aligned to brain data. We end-up with a simple untrained model
that is able to explain most variance in current brain recording benchmarks. Beyond quantitative
predictions, responses from this shallow untrained multihead attention architecture (SUMA) exhibit
similar response profiles to those shown empirically in studies on the HLS (Fedorenko et al., 2010,
2012b; Shain et al., 2024a), such as being more sensitive to lexical over syntactic differences. To test
language production capabilities, we investigate whether this untrained model can serve as a good
base for language modeling by feeding SUMA outputs to a trainable downstream decoder model.
This combined model outperforms comparable trained architectures in terms of sample-efficiency
and parameter-efficient perplexity and achieves state-of-the-art behavioral alignment on a benchmark
(Futrell et al., 2018) predicting human reading times. Taken together, our simple untrained model
yields representations that are consistent with the human language system and capable of supporting
downstream language production. Mapped to language processing in the brain, the human language
system might thus serve as a simple generic feature encoder that provides representations for a
downstream decoder.
2
Preliminaries & Related Work
A Primer on Language in the Human Brain
The Human Language System (HLS) is a set of
brain regions supporting language that are functionally defined by their increased activity to language
inputs over perceptually matched controls (e.g. lists of non-words) (Fedorenko et al., 2010, Section
3). These regions are typically lateralized to left-hemisphere frontal and temporal areas and exhibit
2

Table 1: Datasets Used for Evaluating Model Alignment. Neuroimaging datasets were collected
via either functional magnetic resonance imaging (fMRI) or electrocorticography (ECoG). Text stimuli
range from short sentences (FEDORENKO2016, TUCKUTE2024) to paragraphs (PEREIRA2018) and
entire stories (BLANK2014, WEHBE2014, FUTRELL2018). FUTRELL2018 is a behavioral dataset.
Dataset
Modality
Stimulus Example
PEREIRA2018
fMRI (Reading)
Accordions produce sound with bellows ...
BLANK2014
fMRI (Listening)
A clear and joyous day it was and out on the wide ...
FEDORENKO2016
ECoG (Reading)
‘ALEX’, ‘WAS’, ‘TIRED’, ‘SO’, ‘HE’, ‘TOOK’, ...
TUCKUTE2024
fMRI (Reading)
The judge spoke, breaking the silence.
WEHBE2014
fMRI (Reading)
Harry had never believed he would meet a boy ...
FUTRELL2018
Reading Times
A clear and joyous day it was and out on the wide ...
remarkable selectivity for language processing compared to various non-linguistic inputs and tasks,
such as music perception (Fedorenko et al., 2012a; Chen et al., 2023) or arithmetic computation
(Fedorenko et al., 2011) (for review, see (Fedorenko et al., 2024)). These language regions show only
weak responses when participants comprehend or articulate meaningless non-words (Fedorenko et al.,
2010; Hu et al., 2023). This selectivity profile is supported by extensive neuroimaging research and
further corroborated by behavioral evidence from aphasia studies: when brain damage is confined
to language areas, individuals lose their linguistic abilities while retaining other skills, such as
mathematics (Benn et al., 2013; Varley et al., 2005), general reasoning (Varley and Siegal, 2000), and
theory of mind (Siegal and Varley, 2006).
Model-to-Brain Alignment
Previous studies have shown that the internal representations of certain
artificial neural networks closely resemble those in the brain. This alignment was initially observed
in the domain of vision (Yamins et al., 2014; Cichy et al., 2016; Schrimpf et al., 2018, 2020; Cadena
et al., 2019; Kubilius et al., 2019) and has more recently been extended to language processing
(Schrimpf et al., 2021; Caucheteux and King, 2022; Goldstein et al., 2022; Kauf et al., 2023; Hosseini
et al., 2024; Aw et al., 2023; Tuckute et al., 2024), and auditory processing (Kell et al., 2018; Tuckute
et al., 2023; Koumura et al., 2023). Importantly, most of these models have been trained on large
quantities of data. Within the domain of language, we here show that this is not necessarily required
(see also Hosseini et al. (2024)).
Untrained Models
Recent work has shown that an untrained convolutional network can yield high
brain alignment to recordings in the visual ventral stream without the need for experience-dependent
training (Geiger et al., 2022; Kazemian et al., 2024). Other works have investigated the inductive
biases in different architectures and initializations in models of visual processing (Cichy et al.,
2016; Cadena et al., 2019; Geiger et al., 2022), speech perception (Millet and King, 2021; Tuckute
et al., 2023), and language (Schrimpf et al., 2021; Pasquiou et al., 2022), highlighting that randomly
initialized networks are not random functions (Teney et al., 2024). We are not aware of any studies
that thoroughly investigates the alignment of untrained models in language.
Spurious Correlations
There have been concerns in recent studies about what aspects are captured
by both trained and untrained language models when measuring brain alignment (Kauf et al., 2023;
Antonello et al., 2024; Feghhi et al., 2024). Therefore, to ensure our results are meaningful, we
validate our findings by measuring alignment under several control conditions. Specifically, we
observe a decrease in alignment on some datasets as we alter the input from the original sentence
used in the neuroimaging study to the same sequence of tokens but shuffled and finally to a random
sentence with the same sequence length. We performed these analyses across different metrics and
datasets, and selected linear predictivity as our primary metric since it was the only one that best
behaved as expected under the control conditions. We additionally replicated the control experiments
done by Feghhi et al. (2024) on the PEREIRA2018 dataset and observe that simple properties such
as sentence length and sentence position only predict a small portion of the variance relative to the
models we tested (Appendix B.3).
3

3
Localization of the Language Network
The Human Language System (HLS) is defined functionally which means that units are chosen
according to a ‘localizer’ experiment (Saxe et al., 2006). Specifically, the HLS is the set of neural
units (e.g., voxels/electrodes) that are more selective to sentences over a perceptually-matched control
condition (Fedorenko et al., 2010). When selecting units from artificial models for comparison
against HLS units, previous work selected output units from an entire Transformer block based on
brain alignment scores (Schrimpf et al., 2021). However, LLMs learn diverse concepts and behaviors
during their considerable pretraining, not all of which are necessarily related to language processing,
e.g., storage of knowledge (AlKhamissi et al., 2022) in the MLP layers (Geva et al., 2021) and
ability to perform complex reasoning (Huang and Chang, 2023). Therefore, we here characterize
the language units in artificial neural networks using functional localization as is already standard
in neuroscience. This approach comes with the advantage of comparability across different models
since we can choose a fixed set of units which are localized independently of the critical experiment
or modality.
Specifically, we present a set of sentences and strings of non-words of the same sequence length
to each model, obtaining activations for each stimulus from units in the model at the output of
different architectural components. We then define the model language system as the top-k units
(with k = 4096 for all models to keep feature sizes comparable) that maximize the difference
between activations to sentences and strings of non-words, measured using positive t-values with
a Welch’s t-test (Figure 1; see Appendix D for the effect of different values for k and Appendix E
for a visualization of the selected units for the pretrained and untrained versions of GPT2-XL and
LLAMA-2-7B). This localization method selects a distributed set of units across the entire network,
rather than restricting the representations to a single layer as done in prior work. We examine unit
activations after 4 components in each Transformer block: (1) input layer normalization, (2) multihead
self-attention, (3) post-attention layer normalization, and (4) the MLP (see Figure 2(a) for Transformer
architecture). For instance, for a model like LLAMA-2-7B (Touvron et al., 2023) which consists
of 32 Transformer blocks and a hidden dimension of 4096, we consider 32 × 4 × 4096 = 524, 288
units, from which we select 4096 as the model’s language system.
4
Benchmarks
Datasets
The neuroimaging datasets used in this work can be categorized along three dimensions:
the imaging modality, the context length, and the modality through which the language stimulus
was presented to human participants (auditory or visual). The imaging modalities we consider are
functional magnetic resonance imaging (fMRI), which measures brain activity by detecting changes
associated with blood oxygenation, and electrocorticography (ECoG) which records electrical activity
via electrodes in direct contact with the brain. Table 1 provides an overview of all datasets in this study.
To investigate the robustness of our results, we evaluate the same neural datasets used by Schrimpf
et al. (2021), and also test on the WEHBE2014 dataset (Wehbe et al., 2014) and the distributionally-
diverse TUCKUTE2024 dataset (Tuckute et al., 2024).1 The datasets used to guide our design choices
are from Pereira et al. (2018); Blank et al. (2014); Fedorenko et al. (2016), each covering different
imaging modalities, context lengths, and stimulus presentation modality. We only consider neural
units (electrodes, voxels, or regions) associated with the brain’s language system that were localized
by their respective authors using the method described in Section 3. However, in WEHBE2014,
functional localization was not performed – to approximate the language regions, we extract the
top-10% voxels from each anatomically defined language region according to a probabilistic atlas for
the human language system (Lipkin et al., 2022). In an additional analysis, we investigate alignment
with language behavior using the Futrell et al. (2018) dataset, which consists of self-paced per-word
human reading times. See Appendix A for details of each dataset.
Metrics
Following standard practice in measuring brain alignment, we train a ridge regression
model to predict brain activity from model representations, using the same input stimuli presented to
human participants in neuroimaging studies (Toneva and Wehbe, 2019; Tuckute et al., 2024). We
then measure the Pearson correlation between the predicted brain activations and the actual brain
activations of human participants on a held-out set. This process is repeated over 10 cross-validation
1Both datasets were only used until all design choices were fixed.
4

Multihead 
Attention
(+ Positional Encoding)
Tokens
LayerNorm
MLP
LayerNorm
M
Recurrence
via
Shared
Weights
A
T
LN1
LN2
Cross-Subject Consistency
(a)
(b)
(d)
(c)
Figure 2:
Isolating Critical Components of the Transformer Architecture. All models are
untrained, i.e., representations are driven by architectural priors alone. Brain alignment is evaluated
via ridge regression using each model’s top 4096 language-selective units (Figure 1) on the three
validation datasets. The green dashed line indicates the estimated data reliability (cross-subject
consistency). Each experiment is repeated 5 times with different model seed initializations indicated
by the error bars. (a) Transformer block with components labeled for the ablation study in (b).
The blue dashed box indicates our final model SUMA. (b) Multihead Attention and tokenization
strategy drives brain alignment for SUMA. Brain alignment of a single block model with different
architectural ablations (labels as in (a)). Attention ( A ) is implemented with 512 attention heads.
The virtual depth of the model is two layers. Representations are taken in response to the last token.
Mean implies taking the average of all tokens. PosEnc implies using positional encoding. (c)
Increasing the number of attention heads increases brain alignment. Base architecture is two layers
of T+LN1+A . (d) Recurrent application of weights increases brain alignment. Base architecture
is T+LN1+A . Virtual depth is increased by unrolling the same set of weights multiple times in the
depth dimension (a simple form of recurrence). “A” refers to adaptive depth relative to the number of
tokens measured using ceiling( # of tokens
256
).
splits, and we report the average (mean) Pearson correlation as our final result. We refer to this
metric as Linear Predictivity. Appendix B shows results for non-parametric metrics: Centered
Kernel Alignment (CKA; Kornblith et al., 2019) and Representational Dissimilarity Matrices (RDM;
Kriegeskorte et al., 2008).
Estimation of Cross-Subject Consistency
To estimate the reliability of our datasets and the noise
inherent in brain recordings, we compute each benchmark’s cross-subject consistency—referred to as
the noise ceiling in previous work (Schrimpf et al., 2021). For benchmarks with a Linear Predictivity
metric, we estimate the consistency by predicting the brain activity of one held-out subject from all
other subjects using 10 cross-validation splits for all subjects. However, for TUCKUTE2024 we use
the theoretical estimate provided by Tuckute et al. (2024). To aggregate across metrics, the model
score (Pearson r for Linear Predictivity) on each benchmark is normalized with the cross-subject
consistency estimate (normalized score = max(0,raw score)
consistency
) and we report the final score for each
model as the average across all considered benchmarks. Otherwise, the raw brain alignment refers to
the average Pearson r across datasets without normalization.
5
Brain-Like Representations with Shallow Untrained Multihead Attention
We here investigate which architectural components drive the alignment of (untrained) transformer-
based models to the human language system, and synthesize our findings into a simple untrained
architecture.
5.1
Isolating Critical Components of the Transformer Architecture
We conduct a comprehensive ablation study of the architectural components in a single untrained
LLAMA Transformer block to identify the key elements driving the alignment of (untrained) models
with the HLS. Figure 2(a) illustrates a Transformer block with the different components which we
evaluate on its brain alignment in Figure 2(b-d).
5

Table 2: Efficient Brain Alignment on the five datasets for SUMA and different architectures,
pretrained/untrained, with the number of FLOPs as a proxy for simplicity and efficiency. Each
untrained model was evaluated using 5 random initializations, we here report the average. Fed2016
refers to the FEDORENKO2016 dataset.
Model (MFLOPs)
Pereira2018 Blank2014 Fed2016 Tuckute2024 Wehbe2014 Average
GPT2-Small (170)
0.38/0.16
0.10/0.05 0.27/0.27
0.29/0.21
0.11/0.05
0.23/0.15
GPT2-Med (604)
0.38/0.16
0.10/0.04 0.29/0.26
0.37/0.19
0.11/0.05
0.25/0.14
GPT2-Large (1,420)
0.39/0.16
0.09/0.05 0.30/0.25
0.32/0.21
0.08/0.04
0.23/0.14
GPT2-XL (2,950)
0.34/0.15
0.04/0.04 0.27/0.25
0.34/0.23
0.04/0.04
0.21/0.15
LLaMA-2-7B (12,950)
0.32/0.32
0.01/0.24 0.22/0.34
0.34/0.13
0.02/0.15
0.18/0.24
LLaMA-2-13B (25,380)
0.41/0.28
0.04/0.14 0.26/0.32
0.34/0.17
0.06/0.09
0.22/0.20
SUMA (268)
- / 0.43
- / 0.44
- / 0.34
- / 0.19
- / 0.21
- / 0.32
Token Aggregation via Multihead Attention
The brain alignment of the last token vector alone,
even when coupled with an MLP, is close to zero (Figure 2(b), first two bars). Aggregating tokens
through an attention mechanism significantly increases the brain alignment (third bar), with higher
alignment than even trained models (Table 2). Other architectural changes such as adding positional
encoding or adding an MLP to the attention do not improve the brain alignment over the simple
Token-LayerNorm-Attention model. We further find that encoding token frequency using a
BPE tokenizer results in higher brain alignment than a simple word-based tokenization scheme.
We hypothesized that the aggregation of token vectors is a critical mechanism underlying high
alignment of models. Indeed we find that the simple mixing of tokens via a mean operation achieves
brain alignment close to that of the attention architecture (gray bar). Increased diverse aggregation
via more attention heads, i.e. multiple association mappings between different tokens in the context,
improves brain alignment considerably (Figure 2(c)).
Recurrent Application of Shared Weights
Since increased token mixing via multiple attention
heads improved brain alignment, we explored whether further increasing mixing via the repeated
application of the attention mechanism would yield further improvements. In neuroscience terms, this
is typically referred to as recurrence, wherein representations are processed multiple times through
the same neurons. In ML terms, we unroll the same set of modules multiple times with shared
weights. This approach increases the computational depth while maintaining the same number of
parameters, in similar spirit to (Dehghani et al., 2018; Lan et al., 2019). In Figure 2(d) we tested
different numbers of unrolling steps and found that passing the output hidden states of the first pass
to the same module again (i.e., two passes) leads to the best brain alignment.
SUMA
To summarize, we identified token aggregation via repeated multihead attention as key
architectural components that drive the high brain alignment of untrained models. We combine these
building blocks into an architecture that we term SUMA: tokens are processed through LayerNorm
and Multihead Attention with two layers with shared weights (a simple form of recurrence).
This model is much simpler than most other architectures, both in terms of its components but also in
terms of its (untrained) parameter count. We quantify this simplicity as the number of floating point
operations (FLOPs) and observe that SUMA provides highly efficient brain alignment (Table 2). As
a sanity check that SUMA’s brain alignment results are not a byproduct of spurious features, we
tested the brain alignment for different control conditions (see Appendix B.3).
When developing our SUMA model, we used the aggregate across three datasets to guide our design
decisions. To measure distributional robustness, we evaluate on two other datasets: the WEHBE2014
dataset where participants read a chapter from Harry Potter and the Sorcerer’s Stone, and on the
distributionally-diverse TUCKUTE2024 dataset consisting of brain responses to 1000 linguistically
diverse single sentences. Although SUMA still provides an efficient trade-off between FLOPs and
brain alignment, we observe a drop in alignment (Table 2) on the TUCKUTE2024 dataset compared
to pretrained models. However, SUMA achieves the highest alignment score on the other held-out
WEHBE2014 dataset.
6

THE SPEECH THAT THE 
POLITICIAN PREPARED 
WAS TOO LONG FOR 
THE MEETING
S: Sentences
IN BECAUSE NEW 
ROBBERY SOON EVERY 
ANGRY DIRECTIONS 
TRACY MORNING
W: Unconnected
Words
AFTER THE BONTER 
MELIVERED THE 
PERLEN HE MESSED TO 
WEER ON COLMITION
J: Jabberwocky
Sentences
WAS DURING CUSARIST 
FICK PRELL PRONT THE 
POME VILLPA AND 
WORNETIST SHE
N: Unconnected
Non-Words
+ | +
+ | -
 - | +
 - | -
Lexical | Syntax
Lang Selective
BPE
Random Sampling
BPE
Lang Selective
Word-Based
Localization:
Tokenizer:
BPE
BPE
Word-Based
Localization:
Tokenizer:
GPT2-XL GPT2-Small
GPT2-XL GPT2-Small
GPT2-XL GPT2-Small
GPT2-XL GPT2-Small
(a)
(b)
(c)
(d)
(e)
(f)
(g)
Lang Selective
Random Sampling
Lang Selective
Multivariate Representational Pattern Analysis
Univariate Condition-Level Responses
Figure 3: Language Models Exhibit Similar Response Profiles as the HLS. Brain (green) and
model (blue) responses for Univariate Condition-Level Responses (Top Row) and Multivariate
Representational Pattern Analysis (Bottom Row). Each untrained model plot is the average across
5 different random model initializations. The error bars are across the different initializations and
conditions. (a) Examples of the four experimental conditions used in this analyses with the ‘+/-’
signs denoting whether the condition contains lexical or syntactic information, respectively. (b)
Brain responses to the four conditions; data from (Shain et al., 2024a). (c) SUMA responses to
the four conditions. Control experiments show the effect of unit localization (“Lang Selective” vs
“Random Sampling”) and tokenization (“BPE” vs “Word-Based”). (d) The same univariate analysis
for GPT2-XL and GPT2-SMALL models. (e-g) Same as (b-d) but for the multivariate analysis
(Section 5.2). Brain data from extracted from reported results in Fedorenko et al. (2012b).
5.2
Language Models Exhibit Similar Response Profiles as the HLS
To further validate SUMA’s alignment with the human language system, we investigate whether
it can replicate landmark neuroscience studies that qualitatively describe the response profiles in
the HLS. Specifically, we closely follow the analyses in Fedorenko et al. (2010) and Fedorenko
et al. (2012b), using the same set of experimental conditions which are widely used in neuroimag-
ing studies examining lexical and syntactic processing. The four conditions are: (1) Sentences ,
denoted as S, are well-formed sentences containing both lexical and syntactic information. (2)
Unconnected Words , denoted as W, are scrambled sentences containing lexical but not syntactic
information. (3) Jabberwocky Sentences , denoted as J, where content words are replaced by
pronounceable non-words (such as “pront”, or “blay”), thus containing syntactic but not lexical infor-
mation. (4) Unconnected Non-Words , denoted as N, which are scrambled Jabberwocky sentences
containing neither lexical nor syntactic information. Examples of the four experimental conditions
are shown in Figure 3(a). Note that we use a disjoint set of Sentences and Non-Words for the
original functional localization (Section 3).
Models Predict Univariate Condition-Level Responses
In this analysis, we record the responses
of the localized units from SUMA when presented with stimuli from each of the four experimental
conditions. Figure 3(b) displays empirical fMRI responses from the HLS in green (data from Shain
et al. (2024a)). The brain’s language regions are highly sensitive to linguistic structure: responses to
S are numerically higher than all other conditions, while responses to both W and J are higher than
those to N, which contains neither structure nor meaning. Our SUMA model replicates these results
(Figure 3(c)). We find that localization and tokenization are critical model components: using a BPE
tokenizer and recording responses from the localized units show a similar response profile with as the
brain recordings. However, when we randomly sample the same number of units from the model, or
use a word-based tokenization scheme the resulting profile is no longer consistent with brain data.
We observe similar response profiles in other architectures such as GPT2-XL and GPT2-SMALL
(Figure 3(d)). We explore the effect of the number of localized units on this analysis in Appendix C.
7

Models Predict Multivariate Representational Patterns
We follow the Multivoxel Pattern Anal-
ysis (Haxby et al., 2001) conducted in Fedorenko et al. (2012b), where the authors demonstrate that
the brain’s language regions can more reliably discriminate between conditions that differ along the
lexical dimension (S vs J and W vs N) than those differing along the syntactic dimension (S vs W
and J vs N). To illustrate this analysis, consider the S vs J case: Each condition is divided into two
sets (i.e., (S1, S2) and (J1, J2)). We then calculate the Pearson correlation between the localized unit
responses to S1 and S2 (within-correlation) and the correlation between responses to S1 and J2 and
S2 and J1 (in-between correlation). The difference between the within-correlation and the in-between
correlation (we take the average across both in-between comparisons following (Fedorenko et al.,
2012b)) in this case reflects sensitivity to lexical information, as lexical information is present in S
and absent in J, while both contain syntactic information. Following Fedorenko et al. (2012b), all r
values are Fisher transformed. Figure 3(e) compares the difference in correlation between the pairs of
conditions that differ by lexical and syntactic information, showing that language voxels in the brain
are more sensitive to lexical information. Repeating the same analysis in models, we again find that
our SUMA model replicates the neuroscience observations – but only when using a BPE tokenizer
(Figure 3(f)). We observe a similar pattern in other language models (Figure 3(g)) and report more
results in Appendix C.
6
Language Modeling with Untrained Representations
After finding that the representations in certain untrained models are highly aligned to the HLS—both
quantitatively and also qualitatively—we asked whether they could then also support downstream
language production. We appended a trainable decoder to several untrained models to test if there is a
benefit to using the representations encoded by them.
6.1
Localized Untrained Units Improve Language Modeling
Language Modeling Setup
We train 6 models using causal language modeling. Two of these
models, SUMA+1 and SUMA+2, consist of a frozen untrained SUMA that uses a single forward
pass (see Figure 2) with an additional one or two trainable LLAMA Transformer blocks, respectively.
For each SUMA+N model, we test two variants: one that takes the localized units as input, and
another that takes the output of the final SUMA layer as input. Additionally, we train two control
models that directly take the untrained tokens as input. We refer to these models as BASELINE-333M
and BASELINE-536M, and the numbers correspond to the number of trainable parameters in the
decoder Transformer block(s) and the language model head. All models are trained on WIKITEXT-
103 (Merity et al., 2016) for 5 epochs, with a batch-size of 128, a context-window size of 512 tokens,
and an initial learning rate of 5e−3 when training a single block (5e−4 when training two blocks) that
anneals to zero throughout training. See Appendix F for more details about the training setup. We do
not train the token embeddings for the 6 models to keep them comparable as otherwise we will have
to perform localization at every optimization step since the token embeddings will keep changing.
Results
Table 3 compares the WIKITEXT-103 test perplexity and the behavioral score (described
in the next section) of the different models considered in this work, along with various GPT2 and
LLAMA-2 models for reference.2 We report the floating point operations per second as a proxy
for model complexity. We find that localized units in the untrained SUMA model provide effective
representations for language modeling: Passing the randomly initialized tokens through SUMA and
then extracting the localized units distributed across the network as input significantly improves
perplexity and sample efficiency. This method performs better than taking the representation from the
last layer or passing the tokens directly as input (Table 3). This improvement is especially pronounced
when appending a single trainable Transformer layer; increasing the number of parameters by adding
another block reduces this gap (Figure 4 (b)). In all cases, localizing units in SUMA as input not
only achieves better final perplexity than the control models with similar number of parameters, but
already reaches a lower validation loss early during training.
2For GPT2, we use the results reported in Radford et al. (2019), which we closely reproduced, and for
LLAMA-2, we evaluated the models ourselves using a context-window size of 512 tokens.
8

Table 3:
Language Modeling Perplexity and Behavioral Alignment Results.
PPL denotes
the WIKITEXT-103 test perplexity results, Behavior denotes the alignment of model surprisal to
human reading times in the FUTRELL2018 benchmark. Params denotes the number of trainable
parameters. GPT2 models use a different tokenizer, making results not directly comparable. The
best result within each block is highlighted in bold.
Model
Input Repr (Trained)
Params
MFLOPs (↓)
PPL (↓)
Behavior (↑)
BASELINE-333M
Embeddings (✗)
333M
667
60.26
0.446
SUMA+1
Final Layer (✗)
333M
801
30.48
0.452
SUMA+1
Localized Units (✗)
333M
801
22.17
0.451
BASELINE-536M
Embeddings (✗)
536M
1070
20.22
0.442
SUMA+2
Final Layer (✗)
536M
1210
20.81
0.449
SUMA+2
Localized Units (✗)
536M
1210
16.51
0.453
LLAMA-2-7B
Embeddings (✓)
7B
13220
12.33
0.243
LLAMA-2-13B
Embeddings (✓)
13B
25700
16.29
0.221
GPT2-SMALL
Embeddings (✓)
117M
247
37.50
0.367
GPT2-MEDIUM
Embeddings (✓)
345M
707
26.37
0.350
GPT2-LARGE
Embeddings (✓)
762M
1550
22.05
0.331
GPT2-XL
Embeddings (✓)
1.5B
3110
17.48
0.318
(a)
(c)
(b)
Figure 4: Localized Untrained Units Provide Representations Suitable for Language Modeling.
(a-b) The WIKITEXT-103 validation loss of SUMA variants and control models during training,
when training (a) one Transformer block, and (b) two Transformer blocks. We train two variants of
SUMA-based models: one that uses the output of the localized units as the input representation for
the downstream decoder, and another that uses the final representation (Final Repr) of the untrained
model as input. The baseline model refers to passing the static tokens directly to the decoder without
any intermediate architecture. (c) Behavioral alignment to human reading times in FUTRELL2018 as
a function of model complexity and efficiency measured by the number of FLOPs.
6.2
State-of-the-art Human Reading Times Prediction
Training decoder layers on top of our untrained models allows us to evaluate how similar model logit
surprisal ratings are to human behavior. This enables us to test whether the entire proposed architecture
(Figure 1) is reasonably consistent with the HLS’s outputs beyond its internal representations.
Behavioral Benchmark
We use the FUTRELL2018 benchmark, which has been widely used in
previous research to measure linguistic behavior (Futrell et al., 2018; Schrimpf et al., 2021; Aw et al.,
2023). This dataset consists of self-paced reading times for naturalistic story materials from 180
participants. Per-word reading times provide a measure of incremental comprehension difficulty, a
cornerstone of psycholinguistic research for testing theories of sentence comprehension (Gibson,
1998; Smith and Levy, 2013). We measure alignment by calculating the Pearson correlation between
a model’s cross-entropy loss for a specific token in the sequence and the average human per-word
reading time. The loss for words that comprise of multiple tokens are added together before computing
the correlation.
9

Results
Figure 4(c) shows the behavioral score as a function of the model’s complexity (exact
numbers in Table 3). On reference GPT2 and LLAMA-2 models, we surprisingly observe that
increasing the complexity reduces the behavioral score (see also Oh and Schuler (2023); Shain et al.
(2024b). Our shallow models all surpass the pretrained models by a large margin, with the most
complex SUMA+2 achieving a new state-of-the-art on this human reading times benchmark.
7
Discussion
Tokenization & Multihead Attention
Our findings identify token frequency and aggregation
via multihead attention or a simple mean operation as critical components driving brain alignment
(Section 5). Utilizing a Byte Pair Encoding (BPE) tokenizer enhances brain alignment by effectively
encoding token frequency, breaking down infrequent words into more tokens (Kida et al., 1999;
Sennrich et al., 2016). This frequency-based tokenization scheme allow the model to more accurately
capture linguistic nuances, aligning well with the sensitivity of humans to lexical frequency in
language comprehension (Rayner and Duffy, 1986; Singh et al., 2016). Further aggregating these
tokens through a multihead attention mechanism then constructs diverse representations between
each token pair within the context. The improved alignment from an increased number of attention
heads is most likely due to greater expressivity achieved by multiple heads, enabling the model to
presumably capture an array of context-dependent relations.
Encoding via Architectural Priors Aid Language Decoding
We found that passing localized
units as token representations to downstream trainable modules leads to significantly better language
modeling performance (Section 6). Through structural priors inherent in the architecture, the
representations are potentially constrained to a space from which it is easier to generalize. In
neuroscience terms, this approach follows an encoder-decoder setup where the untrained SUMA
encoder provides generic representations that the downstream trainable decoder can then make
effective use of. Interestingly, our combined models are significantly more aligned to human reading
times than even much larger pretrained models. Similar observations have been reported by Steuer
et al. (2023), Oh and Schuler (2023), and Shain et al. (2024b).
A Need for Better Brain Benchmarks
Although we are excited about the strong brain alignment
of our model, we do not believe it to be a perfect model of the human language system. First, while
cross-subject consistency estimates typically attempt to compute an “upper bound” or ceiling of
how well the best possible model could perform, this estimate is routinely surpassed by the best
models (Table 2). We believe the data consistency is under-estimated by trying to predict noisy
data (target subject) from other noisy data (held-out subject pool; whereas model predictors are not
noisy). We hope future brain recordings of language will include more stimulus repetitions to increase
this signal-to-noise (SNR) ratio and allow more accurate ceiling estimates. Second, we observed
inconsistencies between different datasets (Table 2) and especially different metrics (Appendix B).
Third, the FEDORENKO2016 dataset show high alignment even when a random sequence of tokens of
the same length is passed to untrained models. This appears to be because most stimuli in this dataset
consist of unique words, making them indistinguishable from the original sentence in untrained
models. Highlighting the importance of linguistic diversity in the stimuli set presented to participants
in neuroimaging studies. Finally, it is important to note that our final model architecture is based
on the aggregate score across multiple datasets, so it may not represent the best possible alignment
achievable with a shallow untrained model on individual datasets. Despite a (perpetual) wish for data
with higher SNR, we find that simple models such as single-token only or the sentence length and
position do not necessarily yield high alignment scores, making us confident in ’s utility.
8
Conclusion
By identifying the key components that contribute to neural alignment between language models
and the human language system, we demonstrate that a shallow untrained multihead attention
network can achieve significant alignment with the language regions in the brain. We first perform
functional localization to identify language-selective units within a language model, analogous to
how neuroscientists localize the language system in the human brain, and benchmark model-to-
brain alignment across five different brain recordings. We identify token aggregation via repeated
application of the multihead attention mechanism combined with the tokenization strategy that
10

implicitly encodes token frequency, enhance the expressivity and effectiveness of the model. We
further demonstrate that those localized units exhibit similar response profiles as shown in landmark
studies in language neuroscience (Fedorenko et al., 2024). Our findings suggest that the human
language system might be simpler than previously thought, potentially following a hierarchical
structure similar to the processing of vision (DiCarlo et al., 2012) and speech (Gwilliams et al.,
2024). This conceptualization aligns with our model’s ability to support downstream language
production by acting as a feature encoder that provides suitable representations for a downstream
decoder. Furthermore, our work underscores the need for datasets with higher signal-to-noise-ratio,
and consistent metrics to accurately benchmark and evaluate model-to-brain alignment (see Appendix
B). Together, these efforts offer a new perspective on LLMs in the context of brain alignment,
emphasizing that achieving high alignment does not necessarily require large-scale or highly complex
models.
Acknowledgments and Disclosure of Funding
We would like to thank Abdülkadir Gökce, Yingtian Tang, Ben Lonnqvist, Hannes Mehrer, Muham-
mad ElNokrashy, Eghbal Hosseni, Anna Ivanova, and Ev Fedorenko for their valuable feedback and
insightful discussions. We also thank Syrielle Montariol for her feedback on the final manuscript.
References
AlKhamissi, B., Li, M., Celikyilmaz, A., Diab, M. T., and Ghazvininejad, M. (2022). A review on
language models as knowledge bases. ArXiv, abs/2204.06031.
Antonello, R., Vaidya, A., and Huth, A. (2024). Scaling laws for language encoding models in fmri.
Advances in Neural Information Processing Systems, 36.
Aw, K. L., Montariol, S., AlKhamissi, B., Schrimpf, M., and Bosselut, A. (2023). Instruction-tuning
aligns llms to the human brain.
Bates, E., Wilson, S. M., Saygin, A. P., Dick, F., Sereno, M. I., Knight, R. T., and Dronkers, N. F.
(2003). Voxel-based lesion–symptom mapping. Nature Neuroscience, 6(5):448–450.
Benn, Y., Wilkinson, I. D., Zheng, Y., Kadosh, K. C., Romanowski, C. A., Siegal, M., and Varley,
R. (2013). Differentiating core and co-opted mechanisms in calculation: The neuroimaging of
calculation in aphasia. Brain and Cognition, 82(3):254–264.
Binder, J. R., Frost, J. A., Hammeke, T. A., Cox, R. W., Rao, S. M., and Prieto, T. (1997). Human brain
language areas identified by functional magnetic resonance imaging. The Journal of Neuroscience,
17(1):353–362.
Blank, I., Kanwisher, N., and Fedorenko, E. (2014). A functional dissociation between language
and multiple-demand systems revealed in patterns of BOLD signal fluctuations. Journal of
Neurophysiology, 112(5):1105–1118.
Cadena, S. A., Denfield, G. H., Walker, E. Y., Gatys, L. A., Tolias, A. S., Bethge, M., and Ecker,
A. S. (2019). Deep convolutional models improve predictions of macaque v1 responses to natural
images. PLoS computational biology, 15(4):e1006897.
Caucheteux, C. and King, J.-R. (2022). Brains and algorithms partially converge in natural language
processing. Communications biology, 5(1):134.
Chen, X., Affourtit, J., Ryskin, R., Regev, T. I., Norman-Haignere, S., Jouravlev, O., Malik-Moraleda,
S., Kean, H., Varley, R., and Fedorenko, E. (2023). The human language system, including its
inferior frontal component in “broca’s area,” does not support music perception. Cerebral Cortex,
33(12):7904–7929.
Cichy, R. M., Khosla, A., Pantazis, D., Torralba, A., and Oliva, A. (2016). Comparison of deep
neural networks to spatio-temporal cortical dynamics of human visual object recognition reveals
hierarchical correspondence. Scientific reports, 6(1):27755.
11

Dehghani, M., Gouws, S., Vinyals, O., Uszkoreit, J., and Kaiser, Ł. (2018). Universal transformers.
arXiv preprint arXiv:1807.03819.
DiCarlo, J., Zoccolan, D., and Rust, N. (2012). How does the brain solve visual object recognition?
Neuron, 73(3):415–434.
Fedorenko, E. (2014). The role of domain-general cognitive control in language comprehension.
Frontiers in Psychology, 5.
Fedorenko, E., Behr, M. K., and Kanwisher, N. (2011). Functional specificity for high-level linguistic
processing in the human brain. Proceedings of the National Academy of Sciences, 108(39):16428–
16433.
Fedorenko, E., Hsieh, P.-J., Nieto-Castanon, A., Whitfield-Gabrieli, S. L., and Kanwisher, N. G.
(2010). New method for fmri investigations of language: defining rois functionally in individual
subjects. Journal of neurophysiology, 104 2:1177–94.
Fedorenko, E., Ivanova, A. A., and Regev, T. I. (2024). The language network as a natural kind within
the broader landscape of the human brain. Nature Reviews Neuroscience, 25(5):289–312.
Fedorenko, E., McDermott, J. H., Norman-Haignere, S., and Kanwisher, N. (2012a). Sensitivity to
musical structure in the human brain. Journal of Neurophysiology, 108(12):3289–3300.
Fedorenko, E., Nieto-Castañon, A., and Kanwisher, N. (2012b). Lexical and syntactic representations
in the brain: An fmri investigation with multi-voxel pattern analyses. Neuropsychologia, 50(4):499–
513. Multivoxel pattern analysis and cognitive theories.
Fedorenko, E., Scott, T. L., Brunner, P., Coon, W. G., Pritchett, B., Schalk, G., and Kanwisher, N.
(2016). Neural correlate of the construction of sentence meaning. Proceedings of the National
Academy of Sciences, 113(41):E6256–E6262.
Feghhi, E., Hadidi, N., Song, B., Blank, I. A., and Kao, J. C. (2024). What are large language models
mapping to in the brain? a case against over-reliance on brain scores.
Futrell, R., Gibson, E., Tily, H. J., Blank, I., Vishnevetsky, A., Piantadosi, S., and Fedorenko, E.
(2018). The natural stories corpus. In Proceedings of the Eleventh International Conference
on Language Resources and Evaluation (LREC 2018), Miyazaki, Japan. European Language
Resources Association (ELRA).
Geiger, F., Schrimpf, M., Marques, T., and DiCarlo, J. J. (2022). Wiring up vision: Minimizing su-
pervised synaptic updates needed to produce a primate ventral stream. In International Conference
on Learning Representations 2022 Spotlight.
Geva, M., Schuster, R., Berant, J., and Levy, O. (2021). Transformer feed-forward layers are key-
value memories. In Moens, M.-F., Huang, X., Specia, L., and Yih, S. W.-t., editors, Proceedings of
the 2021 Conference on Empirical Methods in Natural Language Processing, pages 5484–5495,
Online and Punta Cana, Dominican Republic. Association for Computational Linguistics.
Gibson, E. (1998). Linguistic complexity: locality of syntactic dependencies. Cognition, 68(1):1–76.
Goldstein, A., Zada, Z., Buchnik, E., Schain, M., Price, A., Aubrey, B., Nastase, S. A., Feder, A.,
Emanuel, D., Cohen, A., Jansen, A., Gazula, H., Choe, G., Rao, A., Kim, C., Casto, C., Fanda, L.,
Doyle, W., Friedman, D., Dugan, P., Melloni, L., Reichart, R., Devore, S., Flinker, A., Hasenfratz,
L., Levy, O., Hassidim, A., Brenner, M., Matias, Y., Norman, K. A., Devinsky, O., and Hasson, U.
(2022). Shared computational principles for language processing in humans and deep language
models. Nature Neuroscience, 25(3):369–380.
Gorno-Tempini, M. L., Dronkers, N. F., Rankin, K. P., Ogar, J. M., Phengrasamy, L., Rosen, H. J.,
Johnson, J. K., Weiner, M. W., and Miller, B. L. (2004). Cognition and anatomy in three variants
of primary progressive aphasia. Annals of Neurology, 55(3):335–346.
Gwilliams, L., Marantz, A., Poeppel, D., and King, J.-R. (2024). Hierarchical dynamic coding
coordinates speech comprehension in the brain. bioRxiv.
12

Hagoort, P. (2019).
The neurobiology of language beyond single-word processing.
Science,
366(6461):55–58.
Harvey, S. E., Larsen, B. W., and Williams, A. H. (2023). Duality of bures and shape distances with
implications for comparing neural representations. In UniReps: the First Workshop on Unifying
Representations in Neural Models.
Haxby, J. V., Gobbini, M. I., Furey, M. L., Ishai, A., Schouten, J. L., and Pietrini, P. (2001).
Distributed and overlapping representations of faces and objects in ventral temporal cortex. Science,
293(5539):2425–2430.
Hosseini, E. A., Schrimpf, M., Zhang, Y., Bowman, S., Zaslavsky, N., and Fedorenko, E. (2024).
Artificial neural network language models predict human brain responses to language even after a
developmentally realistic amount of training. Neurobiology of Language, pages 1–21.
Hu, J., Small, H., Kean, H., Takahashi, A., Zekelman, L., Kleinman, D., Ryan, E., Nieto-Castañón, A.,
Ferreira, V., and Fedorenko, E. (2023). Precision fmri reveals that the language-selective network
supports both phrase-structure building and lexical access during language production. Cerebral
Cortex, 33(8):4384–4404.
Huang, J. and Chang, K. C.-C. (2023). Towards reasoning in large language models: A survey. In
Rogers, A., Boyd-Graber, J., and Okazaki, N., editors, Findings of the Association for Computa-
tional Linguistics: ACL 2023, pages 1049–1065, Toronto, Canada. Association for Computational
Linguistics.
Kauf, C., Tuckute, G., Levy, R., Andreas, J., and Fedorenko, E. (2023). Lexical-Semantic Content,
Not Syntactic Structure, Is the Main Contributor to ANN-Brain Similarity of fMRI Responses in
the Language Network. Neurobiology of Language, pages 1–36.
Kazemian, A., Elmoznino, E., and Bonner, M. F. (2024). Convolutional architectures are cortex-
aligned de novo.
Kell, A. J., Yamins, D. L., Shook, E. N., Norman-Haignere, S. V., and McDermott, J. H. (2018). A
task-optimized neural network replicates human auditory behavior, predicts brain responses, and
reveals a cortical processing hierarchy. Neuron, 98(3):630–644.
Kida, T., Fukamachi, S., Takeda, M., Shinohara, A., Shinohara, T., and Arikawa, S. (1999). Byte pair
encoding: a text compression scheme that accelerates pattern matching.
Kornblith, S., Norouzi, M., Lee, H., and Hinton, G. (2019). Similarity of neural network representa-
tions revisited. In International conference on machine learning, pages 3519–3529. PMLR.
Koumura, T., Terashima, H., and Furukawa, S. (2023). Human-like modulation sensitivity emerging
through optimization to natural sound recognition. Journal of Neuroscience, 43(21):3876–3894.
Kriegeskorte, N., Mur, M., and Bandettini, P. (2008). Representational similarity analysis - connecting
the branches of systems neuroscience. Frontiers in Systems Neuroscience, 2.
Kubilius, J., Schrimpf, M., Kar, K., Rajalingham, R., Hong, H., Majaj, N., Issa, E., Bashivan, P.,
Prescott-Roy, J., Schmidt, K., Nayebi, A., Bear, D., Yamins, D. L., and DiCarlo, J. J. (2019). Brain-
like object recognition with high-performing shallow recurrent anns. In Wallach, H., Larochelle, H.,
Beygelzimer, A., d'Alché-Buc, F., Fox, E., and Garnett, R., editors, Advances in Neural Information
Processing Systems, volume 32. Curran Associates, Inc.
Lan, Z., Chen, M., Goodman, S., Gimpel, K., Sharma, P., and Soricut, R. (2019). Albert: A lite bert
for self-supervised learning of language representations. ArXiv, abs/1909.11942.
Lipkin, B., Tuckute, G., Affourtit, J., Small, H., Mineroff, Z., Kean, H., Jouravlev, O., Rakocevic,
L., Pritchett, B., Siegelman, M., Hoeflin, C., Pongos, A., Blank, I. A., Struhl, M. K., Ivanova, A.,
Shannon, S., Sathe, A., Hoffmann, M., Nieto-Castañón, A., and Fedorenko, E. (2022). Probabilistic
atlas for the language network based on precision fmri data from>800 individuals. Scientific Data,
9(1).
Merity, S., Xiong, C., Bradbury, J., and Socher, R. (2016). Pointer sentinel mixture models.
13

Millet, J. and King, J.-R. (2021). Inductive biases, pretraining and fine-tuning jointly account for
brain responses to speech. ArXiv, abs/2103.01032.
Oh, B.-D. and Schuler, W. (2023). Why does surprisal from larger transformer-based language models
provide a poorer fit to human reading times? Transactions of the Association for Computational
Linguistics, 11:336–350.
Pasquiou, A., Lakretz, Y., Hale, J., Thirion, B., and Pallier, C. (2022). Neural language models are
not born equal to fit brain data, but training helps.
Pereira, F., Lou, B., Pritchett, B., Ritter, S., Gershman, S. J., Kanwisher, N., Botvinick, M., and
Fedorenko, E. (2018). Toward a universal decoder of linguistic meaning from brain activation.
Nature Communications, 9(1):963.
Price, C. J. (2010). The anatomy of language: a review of 100 fmri studies published in 2009. Annals
of the New York Academy of Sciences, 1191(1):62–88.
Radford, A., Wu, J., Child, R., Luan, D., Amodei, D., and Sutskever, I. (2019). Language models are
unsupervised multitask learners.
Rayner, K. and Duffy, S. A. (1986). Lexical complexity and fixation times in reading: Effects of
word frequency, verb complexity, and lexical ambiguity. Memory & cognition, 14(3):191–201.
Saxe, R., Brett, M., and Kanwisher, N. (2006). Divide and conquer: a defense of functional localizers.
Neuroimage, 30(4):1088–1096.
Schrimpf, M., Blank, I. A., Tuckute, G., Kauf, C., Hosseini, E. A., Kanwisher, N., Tenenbaum, J. B.,
and Fedorenko, E. (2021). The neural architecture of language: Integrative modeling converges on
predictive processing. Proceedings of the National Academy of Sciences, 118(45):e2105646118.
Schrimpf, M., Kubilius, J., Hong, H., Majaj, N. J., Rajalingham, R., Issa, E. B., Kar, K., Bashivan,
P., Prescott-Roy, J., Geiger, F., Schmidt, K., Yamins, D. L. K., and DiCarlo, J. J. (2018). Brain-
Score: Which Artificial Neural Network for Object Recognition is most Brain-Like? preprint,
Neuroscience.
Schrimpf, M., Kubilius, J., Lee, M. J., Ratan Murty, N. A., Ajemian, R., and DiCarlo, J. J. (2020).
Integrative benchmarking to advance neurally mechanistic models of human intelligence. Neuron,
108(3):413–423.
Sennrich, R., Haddow, B., and Birch, A. (2016). Neural machine translation of rare words with
subword units. In Erk, K. and Smith, N. A., editors, Proceedings of the 54th Annual Meeting of
the Association for Computational Linguistics (Volume 1: Long Papers), pages 1715–1725, Berlin,
Germany. Association for Computational Linguistics.
Shain, C., Kean, H., Casto, C., Lipkin, B., Affourtit, J., Siegelman, M., Mollica, F., and Fedorenko,
E. (2024a). Distributed Sensitivity to Syntax and Semantics throughout the Language Network.
Journal of Cognitive Neuroscience, pages 1–43.
Shain, C., Meister, C., Pimentel, T., Cotterell, R., and Levy, R. (2024b). Large-scale evidence for
logarithmic effects of word predictability on reading time. Proceedings of the National Academy
of Sciences, 121(10):e2307876121.
Siegal, M. and Varley, R. (2006). Aphasia, language, and theory of mind. Social Neuroscience,
1(3–4):167–174.
Singh, A. D., Mehta, P., Husain, S., and Rajakrishnan, R. (2016). Quantifying sentence complexity
based on eye-tracking measures. In CL4LC@COLING 2016.
Smith, N. J. and Levy, R. (2013). The effect of word predictability on reading time is logarithmic.
Cognition, 128(3):302–319.
Steuer, J., Mosbach, M., and Klakow, D. (2023). Large gpt-like models are bad babies: A closer look
at the relationship between linguistic competence and psycholinguistic measures. arXiv preprint
arXiv:2311.04547.
14

Teney, D., Nicolicioiu, A., Hartmann, V., and Abbasnejad, E. (2024). Neural redshift: Random
networks are not random functions.
Toneva, M. and Wehbe, L. (2019). Interpreting and improving natural-language processing (in
machines) with natural language-processing (in the brain). In Neural Information Processing
Systems.
Touvron, H., Martin, L., Stone, K. R., Albert, P., Almahairi, A., Babaei, Y., Bashlykov, N., Batra, S.,
Bhargava, P., Bhosale, S., Bikel, D. M., Blecher, L., Ferrer, C. C., Chen, M., Cucurull, G., Esiobu,
D., Fernandes, J., Fu, J., Fu, W., Fuller, B., Gao, C., Goswami, V., Goyal, N., Hartshorn, A. S.,
Hosseini, S., Hou, R., Inan, H., Kardas, M., Kerkez, V., Khabsa, M., Kloumann, I. M., Korenev,
A. V., Koura, P. S., Lachaux, M.-A., Lavril, T., Lee, J., Liskovich, D., Lu, Y., Mao, Y., Martinet, X.,
Mihaylov, T., Mishra, P., Molybog, I., Nie, Y., Poulton, A., Reizenstein, J., Rungta, R., Saladi, K.,
Schelten, A., Silva, R., Smith, E. M., Subramanian, R., Tan, X., Tang, B., Taylor, R., Williams, A.,
Kuan, J. X., Xu, P., Yan, Z., Zarov, I., Zhang, Y., Fan, A., Kambadur, M., Narang, S., Rodriguez,
A., Stojnic, R., Edunov, S., and Scialom, T. (2023). Llama 2: Open foundation and fine-tuned chat
models. ArXiv, abs/2307.09288.
Tuckute, G., Feather, J., Boebinger, D., and McDermott, J. H. (2023). Many but not all deep neural
network audio models capture brain responses and exhibit correspondence between model stages
and brain regions. PLOS Biology, 21(12):1–70.
Tuckute, G., Sathe, A., Srikant, S., Taliaferro, M., Wang, M., Schrimpf, M., Kay, K., and Fedorenko,
E. (2024). Driving and suppressing the human language network using large language models.
Nature Human Behaviour, pages 1–18.
Varley, R. and Siegal, M. (2000). Evidence for cognition without grammar from causal reasoning
and ‘theory of mind’ in an agrammatic aphasic patient. Current Biology, 10(12):723–726.
Varley, R. A., Klessinger, N. J. C., Romanowski, C. A. J., and Siegal, M. (2005). Agrammatic but
numerate. Proceedings of the National Academy of Sciences, 102(9):3519–3524.
Wehbe, L., Murphy, B., Talukdar, P., Fyshe, A., Ramdas, A., and Mitchell, T. (2014). Simultaneously
uncovering the patterns of brain regions involved in different story reading subprocesses. PLOS
ONE, 9(11):1–19.
Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R.,
Funtowicz, M., and Brew, J. (2019). Huggingface’s transformers: State-of-the-art natural language
processing. ArXiv, abs/1910.03771.
Yamins, D. L., Hong, H., Cadieu, C. F., Solomon, E. A., Seibert, D., and DiCarlo, J. J. (2014).
Performance-optimized hierarchical models predict neural responses in higher visual cortex.
Proceedings of the national academy of sciences, 111(23):8619–8624.
15

Appendix
A
Neuroimaging & Behavioral Datasets
A.1
Neuroimaging Datasets
Pereira et al. (2018)
This dataset consists of fMRI activations (blood-oxygen-level-dependent;
BOLD responses) recorded as participants read short passages presented one sentence at a time for
4 s. The dataset is composed of two distinct experiments: one with 9 subjects presented with 384
sentences, and another with 6 subjects presented with 243 sentences each. The passages in each
experiment spanned 24 different topics. The results reported for this dataset are the average alignment
across both experiments after normalizing with their respective cross-subject consistency estimates.
Blank et al. (2014)
This dataset also involves fMRI signals but recorded from only 12 functional
regions of interest (fROI) instead of the higher resolution signal used by Pereira et al. (2018). The
data was collected from 5 participants as they listened to 8 long naturalistic stories that were adapted
from existing fairy tales and short stories (Futrell et al., 2018). Each story was approximately 5
minutes long, averaging up to 165 sentences, providing a much longer context length than the other
neuroimaging datasets except WEHBE2014. When measuring brain alignment we use the input
stimuli of the last 32 TRs as the model’s context.
Fedorenko et al. (2016)
This dataset captures ECoG signals from 5 participants as they read
8-word-long sentences presented one word at a time for 450 or 700 ms. Following Schrimpf et al.
(2021) we select the 52/80 sentences that were presented to all participants.
Tuckute et al. (2024)
This dataset is used to measure distributional robustness of our model, and is
tested only after confirming all our design choices based on the preceding datasets. In this dataset,
5 participants read 1000 6-word sentences presented on one sentence at a time for 2 s. BOLD
responses from voxels in the language network were averaged within each participant and then across
participants to yield an overall average language network response to each sentence. The stimuli used
span a large part of the linguistic space, enabling model-brain comparisons across a wide range of
single sentences. Sentence presentation order was randomized across participants. The averaging
of sentences across participants effectively minimizes the effect of temporal autocorrelation/context
in this dataset. In combination with the diversity in linguistic materials, this dataset presents a
particularly challenging dataset for model evaluation.
Wehbe et al. (2014)
We similarly evaluate on this dataset after confirming our design choices
on the first three datasets. The fMRI data was collected from 8 native English speakers while they
read Chapter 9 of “Harry Potter and the Sorcerer’s Stone.” Participants were familiar with the Harry
Potter series, and were reminded of the events leading up to Chapter 9 before the experiment. They
read the chapter using Rapid Serial Visual Presentation (RSVP), with each word displayed for 0.5
seconds, over 45 minutes, covering 5,000 words. Since functional localization was not performed by
the authors, we conduct the the localization ourselves for measuring brain alignment by extracting
the top-10% voxels from each anatomically defined language region from both the left and right
hemisphere using a probabilistic atlas of the human language system (Lipkin et al., 2022). We exclude
the angular gyrus since it has been found to consistently respond differently than other regions in
the language system despite other similarities such as responding more strongly to sentences than
unconnected non-words (Fedorenko et al., 2024).
A.2
Cross-Subject Consistency Estimates
Table 4 shows the cross-subject consistency estimates (known as noise-ceiling in previous work)
that we compute to normalize the raw correlation before aggregating the results across datasets and
metrics. The method in which we compute those estimates is described in the main paper for the
linear predictivity metric in Section 4 and for the non-parameteric metrics in Appendix B.1. The
TUCKUTE2024 dataset can only work with a regression-based metric since it contains only one value
for each stimulus that we attempt to predict.
16

Table 4: Cross-Subject Consistency Estimates used in this work for the three metrics considered.
PER2018 is the PEREIRA2018 dataset which consists of two separate experiments {384, 243} as
described in Appendix A.1 each with its own estimate. FED2016 is the FEDORENKO2016 dataset.
PER2018.384 PER2018.243 FED2016 BLANK2014 TUCKUTE2024 WEHBE2014
Linear
0.3430
0.2826
0.2013
0.1621
0.5592
0.2134
CKA
0.1247
0.1128
0.1547
0.0319
-
-
RDM
0.0808
0.0802
0.0894
0.0201
-
-
A.3
Behavioral Datasets
Futrell et al. (2018)
This dataset consists of self-paced reading times for each word from 180
participants. The stimuli include 10 stories from the Natural Stories Corpus (Futrell et al., 2018),
similar to BLANK2014. Each participant read between 5 and all 10 stories.
B
Brain Alignment Using Additional Metrics
B.1
Metrics: Centered Kernel Alignment & Representational Dissimilarity Matrices
We report here the results using additional metrics. Specifically, we use Centered Kernel Alignment
(Kornblith et al., 2019) and Representational Dissimilarity Matrices (Kriegeskorte et al., 2008).
Unlike training a linear regression, both of those metrics are non-parameteric and therefore do not
require any additional training. However, they exhibit unusual behavior when presenting the model
with control stimuli, such as random input, as shown in Figure 7 and discussed in Section B.3, which
is the reason why we opted for Linear Predictivity as our primary metric.
Centered Kernel Alignment (CKA)
Kornblith et al. (2019) introduced CKA as a substitute for
Canonical Correlation Analysis (CCA) to assess the similarity between neural network representations.
Unlike linear predictivity, it is a non-parameteric metric and therefore does not require any additional
training. CKA is particularly effective with high-dimensional representations, and its reliability in
identifying correspondences between representations in networks trained from different initializations
(Kornblith et al., 2019).
Representational Dissimilarity Matrices (RDM)
Kriegeskorte et al. (2008) introduced RDMs as
a solution to the challenge of integrating brain-activity measurements, behavioral observations, and
computational models in systems neuroscience. RDMs are part of a broader analytical framework
referred to as representational similarity analysis (RSA). In practical terms, to compute the dissimi-
larity matrix for an N-dimensional network’s responses to M different stimuli, an M×M matrix of
distances between all pairs of evoked responses is generated for both brain activity and the language
model’s activations Harvey et al. (2023). The correlation between these two matrices is then used as
a measure of brain alignment.
Estimating Cross-Subject Consistency
For both CKA and RDM, we compute the similarity
between two halves of subjects over all possible combinations, instead of predicting brain activity of
one subject using all other subjects as we did for Linear Predictivity. This approach yields a higher
estimate, which we believe more accurately reflects the true performance ceilings, as many of the
models we tested exceeded the previously estimated ceiling.
B.2
Results: Isolating Critical Components of the Transformer Architecture
We here report the same results as in Figure 2 but as aggregate on the three validation datasets and
the three metrics. The aggregate result is averaged across the 9 benchmarks after being normalized
relative to each cross-subject consistency estimate. See Figure 10 for more results on untrained
models and their trained counterparts.
Architectural Ablation
Figure 5(b) shows those results in the architectural ablation study. We can
see the same effect shown before that highlights the importance of token aggregation either through
17

Multihead 
Attention
(+ Positional Encoding)
Tokens
LayerNorm
MLP
LayerNorm
M
Recurrence
via
Shared
Weights
A
T
LN1
LN2
Cross-Subject Consistency
(a)
(b)
(d)
(c)
Figure 5:
Isolating Critical Components of the Transformer Architecture. Similar plots as
Figure 2 but aggregating the results across the three metrics on the three validation datasets instead of
only using Linear Predictivity. Each experiment is repeated 5 across different random initializations.
(a) Transformer block with components labeled for the ablation study in (b). The blue dashed box
indicates our final model SUMA. (b) Brain alignment of a two-layer SUMA with shared weights on
different architectural ablations and 512 attention heads. We use the model representation of the last
token, except for T+Mean . Labels refer to (a). (c) Increasing the number of attention heads increases
brain alignment. We use here the T+LN+A architecture with a depth of two. (d) Brain alignment of
the T+LN+A model as a function of the number of unrolling steps. Adaptive refers to adaptive depth
relative to the number of tokens.
Figure 6: Sentence Length and Position Does Not Explain All Variance Predicted By SUMA
We here replicate the experiment Feghhi et al. (2024) that claims sentence length (SL) and sentence
position (SP) can account for all variance expalined by untrained models on PEREIRA2018, which is
not the case with our method.
taking a simple mean or via the multihead attention mechanism. However, in the case when using the
three metrics, the best model is when using the full Transformer block architecture without positional
encoding, although by just a small margin compared to the T+LN+A , which we used as the SUMA
architecture.
Multihead Attention
Figure 5(c) shows a similar to the conclusions we reached with the linear
metric, we here see the same effect that increasing the number of attention heads increases brain
alignment, reaching a plateau at around 512 attention heads for a dimensionality of 4096. This is
using the SUMA architecture described in the main paper.
Increasing Depth via Shared Weights
Figure 5(d) shows a similar trend as well for the effect of
unrolling the SUMA model L times in the depth dimension. Interestingly increasing the effective
depth to 2 also results in the best alignment for those datasets.
B.3
Results: Brain Alignment Controls on Each Metric
We opted for Linear Predictivity as our primary metric since CKA and RDM exhibited unexpected
behavior when given certain control sentences as input. Specifically, we compare the raw brain
alignment of the original sentence under a given metric to the following conditions: to control
for word order, we shuffle the tokens randomly at each point we measure alignment; to control
for sequence length, we randomly sample the same number of tokens as in the original context
from the model’s vocabulary. Figures 7, 8 and 9 shows the raw brain alignment for each metric
and dataset independently for SUMA, untrained GPT2-SMALL and pretrained GPT2-SMALL
respectively. For SUMA, CKA significantly surpasses the cross-subject consistency estimate and
shows an opposite trend to what is expected, with a completely random sequence of tokens yielding
the highest alignment in some cases. RDM similarly exhibits unexpected responses on some datasets.
18

Untrained GPT2-SMALL shows a similar trend. However, for pretrained models like GPT2-SMALL,
CKA behaves as expected except for FEDORENKO2016, while RDM continues showing unexpected
trends for pretrained models as well. Linear Predictivity was the only metric that consistently behaved
as expected across all models when presented with control conditions. Moreover, we replicated the
same experiment reported in Feghhi et al. (2024) that claims sentence length and sentence position
accounts for all variance predicted by untrained GPT2-XL for the PEREIRA2018 dataset, and find
that this is not the case for SUMA (see Figure 6).
Figure 7: SUMA Brain Alignment with Respect to Control Conditions: The raw alignment
(Pearson correlation scores) for three metrics and three datasets are reported separately based on
the input condition. Each plot includes bars in the following order: Original indicates that the
sentence is passed correctly without modifications. Shuffle means the tokens in each sequence are
shuffled. Random (same length) means uniformly sampling the same number of tokens from the
entire vocabulary and passing them to the model. Results from PEREIRA2018 are divided into its
two respective experiments. Error bars show the variance across five different random seeds.
19

Figure 8: Untrained GPT2 Brain Alignment with Respect to Control Conditions: The raw
alignment (Pearson correlation scores) for three metrics and three datasets are reported separately
based on the input condition. Each plot includes bars in the following order: Original indicates that
the sentence is passed correctly without modifications. Shuffle means the tokens in each sequence
are shuffled. Random (same length) means uniformly sampling the same number of tokens from the
entire vocabulary and passing them to the model. Results from PEREIRA2018 are divided into its
two respective experiments. Error bars show the variance across five different random seeds.
Figure 9: Pretrained GPT2 Brain Alignment with Respect to Control Conditions: The raw
alignment (Pearson correlation scores) for three metrics and three datasets are reported separately
based on the input condition similar to Figure 8.
20

B.4
Results: Pretrained Models and Untrained Counterparts
Figure 10 shows the normalized brain alignment across the three metrics and three datasets for
pretrained models and their untrained counterparts. The untrained models are evaluated 5 times with
different initializations, and are initialized according to the default HuggingFace implementation
(Wolf et al., 2019). Interestingly, on those benchmarks, the untrained LLAMA-2-7B achieves higher
alignment than its pretrained counterpart, while a model such as untrained GPT2-XL underperforms
significantly compared to the pretrained GPT2-XL.
(a) Linear
(c) RDM
(b) CKA
(d) Aggregate
Figure 10: Brain Alignment Results For Different Metrics Brain alignment for pretrained models
and their untrained counteparts aggregated across the three validation datasets for: (a) Linear, (b)
CKA, (c) RDM, and (d) Aggregate, with SUMA there as reference. MFLOPs—plotted in log-scale—
is used as a proxy for the model’s complexity.
21

THE SPEECH THAT THE 
POLITICIAN PREPARED 
WAS TOO LONG FOR 
THE MEETING
S: Sentences
IN BECAUSE NEW 
ROBBERY SOON EVERY 
ANGRY DIRECTIONS 
TRACY MORNING
W: Unconnected
Words
AFTER THE BONTER 
MELIVERED THE 
PERLEN HE MESSED TO 
WEER ON COLMITION
J: Jabberwocky
Sentences
WAS DURING CUSARIST 
FICK PRELL PRONT THE 
POME VILLPA AND 
WORNETIST SHE
N: Unconnected
Non-Words
+ | +
+ | -
 - | +
 - | -
Lexical | Syntax
Lang Selective
BPE
Random Sampling
BPE
Lang Selective
Word-Based
Localization:
Tokenizer:
BPE
BPE
Word-Based
Localization:
Tokenizer:
(a)
(b)
(c)
(d)
(e)
(f)
(g)
Lang Selective
Random Sampling
Lang Selective
Multivariate Representational Pattern Analysis
Univariate Condition-Level Responses
Lang Selective
BPE
Random Sampling
BPE
Localization:
Tokenizer:
Lang Selective
BPE
Random Sampling
BPE
Localization:
Tokenizer:
Figure 11: Language Models Exhibit Similar Response Profiles as the HLS. This is a similar
to Figure 3 but analyzes the localization and tokenization effect on untrained and pretrained GPT2-
XL. Brain (green) and model (blue) responses for Univariate Condition-Level Responses (Top
Row) and Multivariate Representational Pattern Analysis (Bottom Row). Each untrained model
plot is the average across 5 different random model initializations. The error bars are across the
different initializations and conditions. (a) Examples of the four experimental conditions used in this
analyses with the ‘+/-’ signs denoting whether the condition contains lexical or syntactic information,
respectively. (b) Brain responses to the four conditions; data from (Shain et al., 2024a). (c) Untrained
GPT2-XL responses to the four conditions. Control experiments show the effect of unit localization
(“Lang Selective” vs “Random Sampling”) and tokenization (“BPE” vs “Word-Based”). (d) The
same univariate analysis but for pretrained GPT2-XL. (e-g) Same as (b-d) but for the multivariate
analysis (Section 5.2). Brain data from extracted from reported results in Fedorenko et al. (2012b).
C
Univariate and Multivariate Representational Analyses
Figure 11 shows the same analyses described in Section 5 but focuses on responses from untrained
and pretrained GPT2-XL. Similar to SUMA, Figure 11(c) shows that the localized units in an
untrained GPT2-XL exhibit a similar response profile to the human language system when presented
with same set of conditions using a BPE tokenizer, whereas randomly sampling the same number of
units or using a word-based tokenization strategy does not yield a similar profile. Figure 11(d) shows
the same univariate analysis but for pretrained GPT2-XL. Note that GPT2-XL was trained using
BPE, therefore we do not evaluate the pretrained version on the word-based tokenization strategy.
Figure 11(f) shows a similar pattern but for the multivariate representational analysis, where the
localized units of untrained GPT2-XL is able to distinguish more reliably between inputs that
differ along the lexical dimension than the syntactic dimension similar to what has been shown
in neuroscience studies (Fedorenko et al., 2012b). This is only true when using BPE and for the
localized set of units. Interestingly, for pretrained GPT2-XL it does not seem to matter whether we
localize or randomly sample the same number of units to exhibit a similar response profile to the
brain.
D
Effect of the Number of Localized Units
D.1
Effect on Brain Alignment
Figure 12 shows the effect of localizing the top-k units for different values of k. The results are
presented for each metric and dataset separately using both pretrained and untrained GPT2-SMALL.
The untrained result is the average across 5 different initializations for each k. The Linear Predictivity
metric (first column) shows an interesting trend where the untrained models are more or less consistent
as we increase the value of k from 32 to 4096, whereas is the pretrained version increases more
noticeably for the three datasets. The CKA metric (second column) on the other hand displays a
consistent increase in alignment as we increase the number of localized units for both pretrained and
22

Figure 12:
Pretrained and Untrained GPT2-Small Brain Alignment As a Function of the
Number of Localized Units Here, we vary the number of top-k units localized and measure brain
alignment across three validation datasets (rows) and three metrics (columns). The untrained model
is evaluated with 5 different initializations for each k, and the average alignment is reported along
with the variance, representing the 95% confidence interval.
untrained models demonstrating that dimensionality size does affect the raw correlation significantly.
Finally, RDM (third column) shows a different trend for the FEDORENKO2016 dataset where the
untrained models only benefit from an increase in dimensionality size while pretrained does not, but
for the other datasets the effect is much less pronounced. Note that when experimenting with linear
regression without regularization the effect of the number of units was mich more significant with
each benchmark (metric x dataset) combination behaving differently. Future work should focus on
developing a method that selects the number of units based on each model’s size for example and the
dataset in question, since each dataset measures different number of voxels/regions.
D.2
Effect on Univariate and Multivariate Representational Analyses
Figure 13 repeats the same analyses conducted in Section 5.2 but for different values of k ∈
{16, 32, 64, ..., 4096} using pretrained and untrained GPT2-XL. For the pretrained model, we
observe a response profile similar to what has been observed in the human language system across
all values of k. Even when localizing only 16 units, the extracted representations exhibit similar
23

(a) 
(b) 
(d) 
(c) 
Figure 13: Univariate Condition-Level Responses and Multivariate Representational Pattern
Analysis using Pretrained and Untrained GPT2-XL Here, we observe the effect of localizing
different number of units on the univariate (a,b) and multivariate (c,d) representational analyses
described in Section 5.2. We use 5 different initializations for the untrained models (b,d) for each
value of k. Similar to Figure 3 the error bars shows the variance across the stimuli and models
evaluated.
selectivity for different conditions and can more reliably differentiate between lexical and non-lexical
inputs compared to syntactic and non-syntactic inputs. However, for the untrained models, a large
number of localized units is necessary to identify patterns akin to those found in the brain.
E
Localized Units
Figure 14 shows a visualization of the top 4096 language selective units for LLAMA-2-7B and
GPT2-XL. The vertical axes illustrates the depth, where every 4 ticks correspond to the components
considered in a single Transformer block in the order in which they appear in the architecture.
For instance, the first row in each matrix correspond to the output activations of the first layer
normalization in the first Transformer block, this is then followed by the output of the multihead
attention, then the second layer normalization then finally the output of the MLP. This sequence is
repeated equal L times where L is the number of Transformer blocks in the corresponding architecture.
LLAMA-2-7B has 32 blocks whereas GPT2-XL has 48 blocks, corresponding to 128 and 192 rows
respectively. The horizontal axis is the hidden state dimension, which is 1600 for GPT2-XL and
4096 for LLAMA-2-7B.
The language-selective units in the pretrained models appear to be more structured, where most of
the units are at the last layers. The untrained ones on the other hand are more distributed around the
network with little apparent structure.
F
Training Setup
We train 6 different models on wikiText-103-v1 available on HuggingFace datasets.3 Each model
was trained for 5 epochs with an effective batch-size of 128. We used a context-window size of 512
tokens. The learning rate started with a warm-up for 500 optimization steps to 5e−3 when training
one Transformer block, and 5e−4 when training two blocks, then decayed linearly over the course
of training. Model evaluation on the validation set was done every 1000 steps, and we took the
checkpoint with the lowest validation loss.
3https://huggingface.co/datasets/wikitext
24

Pretrained
LLaMA-2-7B
GPT2-XL
Untrained
Figure 14: Language Units Visualization. Visualization of the top 4096 language selective units for
the trained and an untrained versions of LLAMA-2-7B (top) and GPT2-XL (bottom). The depth is
the vertical axis and goes from top to bottom, while the width is the horizontal axis.
G
Experiments Compute Resources
We used NVIDIA TITAN X GPUs to evaluate the brain alignment of the GPT2 model family as well
as for the shallow untrained architectures like SUMA. For the larger models such as those belonging
to the LLAMA-2 family we used NVIDIA A100 with 80 GBs of memory. The latter GPU was also
used for training all the language modeling experiments.
25

