Energy-based Out-of-distribution Detection
Weitang Liu
Department of Computer Science and Engineering
University of California, San Diego
La Jolla, CA 92093, USA
wel022@ucsd.edu
Xiaoyun Wang
Department of Computer Science
University of California, Davis
Davis, CA 95616, USA
xiywang@ucdavis.edu
John D. Owens
Department of Electrical and Computer Engineering
University of California, Davis
Davis, CA 95616, USA
jowens@ece.ucdavis.edu
Yixuan Li
Department of Computer Sciences
University of Wisconsin-Madison
Madison, WI 53703, USA
sharonli@cs.wisc.edu
Abstract
Determining whether inputs are out-of-distribution (OOD) is an essential building
block for safely deploying machine learning models in the open world. However,
previous methods relying on the softmax conﬁdence score suffer from overconﬁ-
dent posterior distributions for OOD data. We propose a uniﬁed framework for
OOD detection that uses an energy score. We show that energy scores better distin-
guish in- and out-of-distribution samples than the traditional approach using the
softmax scores. Unlike softmax conﬁdence scores, energy scores are theoretically
aligned with the probability density of the inputs and are less susceptible to the
overconﬁdence issue. Within this framework, energy can be ﬂexibly used as a
scoring function for any pre-trained neural classiﬁer as well as a trainable cost
function to shape the energy surface explicitly for OOD detection. On a CIFAR-10
pre-trained WideResNet, using the energy score reduces the average FPR (at TPR
95%) by 18.03% compared to the softmax conﬁdence score. With energy-based
training, our method outperforms the state-of-the-art on common benchmarks.
1
Introduction
The real world is open and full of unknowns, presenting signiﬁcant challenges for machine learning
models that must reliably handle diverse inputs. Out-of-distribution (OOD) uncertainty arises when
a machine learning model sees an input that differs from its training data, and thus should not be
predicted by the model. Determining whether inputs are out-of-distribution is an essential problem
for deploying ML in safety-critical applications such as rare disease identiﬁcation. A plethora of
recent research has studied the issue of out-of-distribution detection [2,3,13–16,20,23–25,28].
Previous approaches rely on the softmax conﬁdence score to safeguard against OOD inputs [13].
An input with a low softmax conﬁdence score is classiﬁed as OOD. However, neural networks can
produce arbitrarily high softmax conﬁdence for inputs far away from the training data [31]. Such a
failure mode occurs since the softmax posterior distribution can have a label-overﬁtted output space,
which makes the softmax conﬁdence score suboptimal for OOD detection.
In this paper, we propose to detect OOD inputs using an energy score, and provide both mathematical
insights and empirical evidence that the energy score is superior to both a softmax-based score and
generative-based methods. The energy-based model [21] maps each input to a single scalar that is
lower for observed data and higher for unobserved ones. We show that the energy score is desirable
34th Conference on Neural Information Processing Systems (NeurIPS 2020), Vancouver, Canada.
arXiv:2010.03759v4  [cs.LG]  26 Apr 2021

for OOD detection since it is theoretically aligned with the probability density of the input—samples
with higher energies can be interpreted as data with a lower likelihood of occurrence. In contrast,
we show mathematically that the softmax conﬁdence score is a biased scoring function that is not
aligned with the density of the inputs and hence is not suitable for OOD detection.
Importantly, the energy score can be derived from a purely discriminative classiﬁcation model without
relying on a density estimator explicitly, and therefore circumvents the difﬁcult optimization process
in training generative models. This is in contrast with JEM [11], which derives the likelihood score
log p(x) from a generative modeling perspective. JEM’s objective can be intractable and unstable
to optimize in practice, as it requires the estimation of the normalized densities over the entire
input space to maximize the likelihood. Moreover, while JEM only utilizes in-distribution data, our
framework allows exploiting both the in-distribution and the auxiliary outlier data to shape the energy
gap ﬂexibly between the training and OOD data, a learning method that is much more effective than
JEM or Outlier Exposure [14].
Contributions. We propose a uniﬁed framework using an energy score for OOD detection.1 We show
that one can ﬂexibly use energy as both a scoring function for any pre-trained neural classiﬁer (without
re-training), and a trainable cost function to ﬁne-tune the classiﬁcation model. We demonstrate the
effectiveness of energy function for OOD detection for both use cases.
• At inference time, we show that energy can conveniently replace softmax conﬁdence for any
pre-trained neural network. We show that the energy score outperforms the softmax conﬁ-
dence score [13] on common OOD evaluation benchmarks. For example, on WideResNet,
the energy score reduces the average FPR (at 95% TPR) by 18.03% on CIFAR-10 compared
to using the softmax conﬁdence score. Existing approaches using pre-trained models may
have several hyperparameters to be tuned and sometimes require additional data. In contrast,
the energy score is a parameter-free measure, which is easy to use and implement, and in
many cases, achieves comparable or even better performance.
• At training time, we propose an energy-bounded learning objective to ﬁne-tune the network.
The learning process shapes the energy surface to assign low energy values to the in-
distribution data and higher energy values to OOD training data. Speciﬁcally, we regularize
the energy using two square hinge loss terms, which explicitly create the energy gap
between in- and out-of-distribution training data. We show that the energy ﬁne-tuned model
outperforms the previous state-of-the-art method evaluated on six OOD datasets. Compared
to the softmax-based ﬁne-tuning approach [14], our method reduces the average FPR (at
95% TPR) by 10.55% on CIFAR-100. This ﬁne-tuning leads to improved OOD detection
performance while maintaining similar classiﬁcation accuracy on in-distribution data.
The rest of the paper is organized as follows. Section 2 provides the background of energy-based
models. In Section 3, we present our method of using energy score for OOD detection, and experi-
mental results in Section 4. Section 5 provides an comprehensive literature review on OOD detection
and energy-based learning. We conclude in Section 6, with discussion on broader impact in Section 7.
2
Background: Energy-based Models
The essence of the energy-based model (EBM) [21] is to build a function E(x) : RD →R that maps
each point x of an input space to a single, non-probabilistic scalar called the energy. A collection of
energy values could be turned into a probability density p(x) through the Gibbs distribution:
p(y | x) =
e−E(x,y)/T
R
y′ e−E(x,y′)/T = e−E(x,y)/T
e−E(x)/T ,
(1)
where the denominator
R
y′ e−E(x,y′)/T is called the partition function, which marginalizes over y,
and T is the temperature parameter. The Helmholtz free energy E(x) of a given data point x ∈RD
can be expressed as the negative of the log partition function:
E(x) = −T · log
Z
y′ e−E(x,y′)/T
(2)
1Our code is publicly available to facilitate reproducible research: https://github.com/wetliu/
energy_ood.
2

CNN
Energy Function
Negative Energy
Frequency
threshold τ
in-distribution
out-of-distribution
f (x;θ)
E(x; f )
x
Figure 1: Energy-based out-of-distribution detection framework. The energy can be used as a scoring function
for any pre-trained neural network (without re-training), or used as a trainable cost function to ﬁne-tune the
classiﬁcation model. During inference time, for a given input x, the energy score E(x; f) is calculated for a
neural network f(x). The OOD detector will classify the input as OOD if the negative energy score is smaller
than the threshold value.
Energy Function The energy-based model has an inherent connection with modern machine learning,
especially discriminative models. To see this, we consider a discriminative neural classiﬁer f(x) :
RD →RK, which maps an input x ∈RD to K real-valued numbers known as logits. These logits
are used to derive a categorical distribution using the softmax function:
p(y | x) =
efy(x)/T
PK
i efi(x)/T ,
(3)
where fy(x) indicates the yth index of f(x), i.e., the logit corresponding to the yth class label.
By connecting Eq. 1 and Eq. 3, we can deﬁne an energy for a given input (x, y) as E(x, y) = −fy(x).
More importantly, without changing the parameterization of the neural network f(x), we can express
the free energy function E(x; f) over x ∈RD in terms of the denominator of the softmax activation:
E(x; f) = −T · log
K
X
i
efi(x)/T .
(4)
3
Energy-based Out-of-distribution Detection
We propose a uniﬁed framework using an energy score for OOD detection, where the differences
of energies between in- and out-of-distribution allow effective differentiation. The energy score
mitigates a critical problem of softmax conﬁdence with arbitrarily high values for OOD examples [12].
In the following, we ﬁrst describe using energy as an OOD score for pre-trained models, and the
connection between the energy and softmax scores (Section 3.1). We then describe how to use energy
as a trainable cost function for model ﬁne-tuning (Section 3.3).
3.1
Energy as Inference-time OOD Score
Out-of-distribution detection is a binary classiﬁcation problem that relies on a score to differentiate
between in- and out-of-distribution examples. A scoring function should produce values that are
distinguishable between in- and out-of-distribution. A natural choice is to use the density function of
the data pin(x) and consider examples with low likelihood to be OOD. However, previous research
showed that density function estimated through deep generative models cannot be reliably used for
OOD detection [29].
To mitigate the challenge, we resort to the energy function derived from a discriminative model for
OOD detection. A model trained with negative log-likelihood (NLL) loss will push down energy
for in-distribution data point [21]. To see this, we can express the negative log-likelihood loss for a
3

model trained on in-distribution data (x, y) ∼P in:
Lnll = E(x,y)∼P in
 −log
efy(x)/T
PK
j=1 efj(x)/T

.
(5)
By deﬁning the energy E(x, y) = −fy(x), the NLL loss can be rewritten as:
Lnll = E(x,y)∼P in
  1
T · E(x, y) + log
K
X
j=1
e−E(x,j)/T 
.
(6)
The ﬁrst term pushes down the energy of the ground truth answer y. The second contrastive term
can be interpreted as the Free Energy (log partition function) of the ensemble of energies. The
contrastive term causes the energy of the ground truth answer y to be pulled down, whereas the
energies of all the other labels to be pulled up. This can be seen in the expression of the gradient for a
single example:
∂Lnll(x, y; θ)
∂θ
= 1
T
∂E(x, y)
∂θ
−1
T
K
X
j=1
∂E(x, y)
∂θ
e−E(x,y)/T
PK
j=1 e−E(x,j)/T
= 1
T
  ∂E(x, y)
∂θ
(1 −p(Y = y)|x)
|
{z
}
↓energy push down for y
−
X
j̸=y
∂E(x, j)
∂θ
p(Y = j|x)
|
{z
}
↑energy pull up for other labels

.
Moreover, the energy score E(x; f) deﬁned in Eq. 4 is a smooth approximation of −fy(x) = E(x, y),
which is dominated by the ground truth label y among all labels. Therefore, the NLL loss overall
pushes down the energy E(x; f) of in-distribution data.
Given the characteristics of energy, we propose using the energy function E(x; f) in Eq. 4 for OOD
detection:
g(x; τ, f) =
0
if −E(x; f) ≤τ,
1
if −E(x; f) > τ,
(7)
where τ is the energy threshold. Examples with higher energies are considered as OOD inputs and
vice versa. In practice, we choose the threshold using in-distribution data so that a high fraction
of inputs are correctly classiﬁed by the OOD detector g(x). Here we use negative energy scores,
−E(x; f), to align with the conventional deﬁnition where positive (in-distribution) samples have
higher scores. The energy score is non-probabilistic, which can be conveniently calculated via the
logsumexp operator.
3.2
Energy Score vs. Softmax Score
Our method can be used as a simple and effective replacement for the softmax conﬁdence score [13]
for any pre-trained neural network. To see this, we ﬁrst derive a mathematical connection between
the energy score and the softmax conﬁdence score:
max
y
p(y | x) = max
y
efy(x)
P
i efi(x) =
ef max(x)
P
i efi(x)
=
1
P
i efi(x)−f max(x)
=⇒log max
y
p(y | x) = E(x; f(x) −f max(x))
= E(x; f)
| {z }
↓for in-dist x
+ f max(x)
| {z }
↑for in-dist x
,
4

C1
C2
C3
C4
C5
C6
C7
C8
C9 C10
Class Label
0.0
0.2
0.4
0.6
0.8
1.0
1.2
1.4
probability
in-distribution sample
out-distribution sample
(a) softmax scores 1.0 vs. 0.99
C1
C2
C3
C4
C5
C6
C7
C8
C9 C10
Class Label
−4
−2
0
2
4
6
8
10
logits
in-distribution sample
out-distribution sample
(b) negative energy scores: 11.19 vs. 7.11
Figure 2: (a) Softmax and (b) logit outputs of two samples calculated on a CIFAR-10 pre-trained WideResNet.
The out-of-distribution sample is from SVHN. For (a), the softmax conﬁdence scores are 1.0 and 0.99 for the in-
and out-of-distribution examples. In contrast, the energy scores calculated from logit are E(xin) = −11.19,
E(xout) = −7.11. While softmax conﬁdence scores are almost identical for in- and out-distribution samples,
energy scores provide more meaningful information with which to differentiate them.
when T = 1. This reveals that the log of the softmax conﬁdence score is in fact equivalent to a special
case of the free energy score, where all the logits are shifted by their maximum logit value. Since
f max(x) tends to be higher and E(x; f) tends to be lower for in-distribution data, the shifting results
in a biased scoring function that is no longer suitable for OOD detection. As a result, the softmax
conﬁdence score is less able to reliably distinguish in- and out-of-distribution examples.
To illustrate with a real example, Figure 2 shows one example from the SVHN dataset (OOD) and
another example from the in-distribution data CIFAR-10. While their softmax conﬁdence scores
are almost identical (1.0 vs 0.99), the negative energy scores are more distinguishable (11.19 vs.
7.11). Thus, working in the original logit space (energy score) instead of the shifted logit space
(softmax score) yields more useful information for each sample. We show in our experimental results
in Section 4.2 that energy score is a superior metric for OOD detection than the softmax score.
3.3
Energy-bounded Learning for OOD Detection
While energy score can be useful for a pre-trained neural network, the energy gap between in- and
out-of-distribution might not always be optimal for differentiation. Therefore, we also propose an
energy-bounded learning objective, where the neural network is ﬁne-tuned to explicitly create an
energy gap by assigning lower energies to the in-distribution data, and higher energies to the OOD
data. The learning process allows greater ﬂexibility in contrastively shaping the energy surface,
resulting in more distinguishable in- and out-of-distribution data. Speciﬁcally, our energy-based
classiﬁer is trained using the following objective:
min
θ
E(x,y)∼Dtrain
in [−log Fy(x)] + λ · Lenergy
(8)
where F(x) is the softmax output of the classiﬁcation model and Dtrain
in
is the in-distribution train-
ing data. The overall training objective combines the standard cross-entropy loss, along with a
regularization loss deﬁned in terms of energy:
Lenergy = E(xin,y)∼Dtrain
in (max(0, E(xin) −min))2
(9)
+ Exout∼Dtrain
out (max(0, mout −E(xout)))2
(10)
where Dtrain
out is the unlabeled auxiliary OOD training data [40]. In particular, we regularize the energy
using two squared hinge loss terms2 with separate margin hyperparameters min and mout. In one term,
the model penalizes in-distribution samples that produce energy higher than the speciﬁed margin
parameter min. Similarly, in another term, the model penalizes the out-of-distribution samples with
2We also explored using a hinge loss such as max(0, E(xin) −E(xout) + m) through a single constant
margin parameter m. While the difference between E(xin) and E(xout) can be stable, their values do not
stabilize. Optimization is more ﬂexible and the training process is more stable with two separate hinge loss
terms.
5

0.0
0.2
0.4
0.6
0.8
1.0
Softmax score (pretrained)
0
5
10
15
20
25
Frequency
in-distribution (CIFAR-10)
out-of-distribution (SVHN)
(a) FPR95: 48.49
2.5
5.0
7.5 10.0 12.5 15.0 17.5 20.0
Energy score (pretrained)
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
Frequency
in-distribution (CIFAR-10)
out-of-distribution (SVHN)
(b) FPR95: 35.59
0.0
0.2
0.4
0.6
0.8
1.0
Softmax score ﬁne-tuning
0
2
4
6
8
10
Frequency
in-distribution (CIFAR-10)
out-of-distribution (SVHN)
(c) FPR95: 4.36
0
10
20
30
40
Energy score ﬁne-tuning
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
Frequency
in-distribution (CIFAR-10)
out-of-distribution (SVHN)
(d) FPR95: 1.04
Figure 3: (a & b) Distribution of softmax scores vs. energy scores from pre-trained WideResNet. We contrast
the score distribution from ﬁne-tuned models using Outlier Exposure [14] (c) and our energy-bounded learning
(d). We use negative energy scores for (b & d) to align with the convention that positive (in-distribution) samples
have higher scores. Using energy score leads to an overall smoother distribution (b & d), and is less susceptible
to the spiky distribution that softmax exhibits for in-distribution data (a & c).
energy lower than the margin parameter mout. In other words, the loss function penalizes the samples
with energy E(x) ∈[min, mout]. Once the model is ﬁne-tuned, the downstream OOD detection is
similar to our description in Section 3.1.
4
Experimental Results
In this section, we describe our experimental setup (Section 4.1) and demonstrate the effectiveness of
our method on a wide range of OOD evaluation benchmarks. We also conduct an ablation analysis
that leads to an improved understanding of our approach (Section 4.2).
4.1
Setup
In-distribution Datasets We use the SVHN [30], CIFAR-10 [19], and CIFAR-100 [19] datasets as
in-distribution data. We use the standard split, and denote the training and test set by Dtrain
in
and Dtest
in ,
respectively.
Out-of-distribution Datasets For the OOD test dataset Dtest
out , we use six common benchmarks:
Textures [5], SVHN [30], Places365 [51], LSUN-Crop [48], LSUN-Resize [48], and iSUN [47].
The pixel values of all the images are normalized through z-normalization in which the parameters are
dependent on the network type. For the auxiliary outlier dataset, we use 80 Million Tiny Images [40],
which is a large-scale, diverse dataset scraped from the web. We remove all examples in this dataset
that appear in CIFAR-10 and CIFAR-100.
Evaluation Metrics We measure the following metrics: (1) the false positive rate (FPR95) of OOD
examples when true positive rate of in-distribution examples is at 95%; (2) the area under the receiver
operating characteristic curve (AUROC); and (3) the area under the precision-recall curve (AUPR).
6

OOD
FPR95
AUROC
AUPR
ﬁne-tune?
dataset
Dtest
in
Dtest
out
↓
↑
↑
WideResNet
CIFAR-10

Softmax score [13] / Energy score (ours)
iSUN
56.03 / 33.68
89.83 / 92.62
97.74 / 98.27
Places365
59.48 / 40.14
88.20 / 89.89
97.10 / 97.30
Texture
59.28 / 52.79
88.50 / 85.22
97.16 / 95.41
SVHN
48.49 / 35.59
91.89 / 90.96
98.27 / 97.64
LSUN-Crop
30.80 / 8.26
95.65 / 98.35
99.13 / 99.66
LSUN-Resize
52.15 / 27.58
91.37 / 94.24
98.12 / 98.67
average
51.04 / 33.01
90.90 / 91.88
97.92 / 97.83
WideResNet
CIFAR-10

OE ﬁne-tune [14] / Energy ﬁne-tune (ours)
iSUN
6.32 / 1.60
98.85 / 99.33
99.77 / 99.87
Places365
19.07 / 9.00
96.16 / 97.48
99.06 / 99.35
Texture
12.94 / 5.34
97.73 / 98.56
99.52 / 99.68
SVHN
4.36 / 1.04
98.63 / 99.41
99.74 / 99.89
LSUN-Crop
2.89 / 1.67
99.49 / 99.32
99.90 / 99.86
LSUN-Resize
5.59 / 1.25
98.94 / 99.39
99.79 / 99.88
average
8.53 / 3.32
98.30 / 98.92
99.63 / 99.75
Table 1: OOD detection performance comparison using softmax-based vs. energy-based approaches. We use
WideResNet [49] to train on the in-distribution dataset CIFAR-10. We show results for both using the pretrained
model (top) and applying ﬁne-tuning (bottom). All values are percentages. ↑indicates larger values are better,
and ↓indicates smaller values are better. Bold numbers are superior results.
Training Details We use WideResNet [49] to train the image classiﬁcation models. For energy
ﬁne-tuning, the weight λ of Lenergy is 0.1. We use the same training setting as in Hendryks et al. [14],
where the number of epochs is 10, the initial learning rate is 0.001 with cosine decay [26], and
the batch size is 128 for in-distribution data and 256 for unlabeled OOD training data. We use the
validation set as in Hendrycks et al. [14] to determine the hyperparameters: min is chosen from
{−3, −5, −7}, and mout is chosen from {−15, −19, −23, −27} that minimize FPR95. The ranges
of min and mout can be chosen around the mean of energy scores from a pre-trained model for in- and
out-of-distribution samples respectively. We provide the optimal margin parameters in Appendix B.
4.2
Results
Does energy-based OOD detection work better than the softmax-based approach? We begin
by assessing the improvement of energy score over the softmax score. Table 1 contains a detailed
comparison for CIFAR-10. For inference-time OOD detection (without ﬁne-tuning), we compare
with the softmax conﬁdence score baseline [13]. We show that using energy score reduces the
average FPR95 by 18.03% compared to the baseline on CIFAR-10. Additional results on SVHN as
in-distribution data are provided in Table 6, where we show the energy score consistently outperforms
the softmax score by 8.69% (FPR95).
We also consider energy ﬁne-tuning and compare with Outlier Exposure (OE) [14], which regularizes
the softmax probabilities to be uniform distribution for outlier training data. For both approaches,
we ﬁne-tune on the same data and use the same training conﬁgurations in terms of learning rate and
batch size. Our energy ﬁne-tuned model reduces the FPR95 by 5.20% on CIFAR-10 compared to
OE. The improvement is more pronounced on complex datasets such as CIFAR-100, where we show
a 10.55% improvement over OE.
To gain further insights, we compare the energy score distribution for in- and out-of-distribution data.
Figure 3 compares the energy and softmax score histogram distributions, derived from pre-trained
as well as ﬁne-tuned networks. The energy scores calculated from a pre-trained network on both
training and OOD data naturally form smooth distributions (see Figure 3(b)). In contrast, softmax
scores for both in- and out-of-distribution data concentrate on high values, as shown in Figure 3(a).
Overall our experiments show that using energy makes the scores more distinguishable between in-
and out-of-distributions, and as a result, enables more effective OOD detection.
How does our approach compare to competitive OOD detection methods? In Table 2, we
compare our work against discriminative OOD detection methods that are competitive in literature.
All the numbers reported are averaged over six OOD test datasets. We provide detailed results
7

Dtest
in
Method
FPR95
AUROC
AUPR
In-dist
Test Error
↓
↑
↑
↓
CIFAR-10
(WideResNet)
Softmax score [13]
51.04
90.90
97.92
5.16
Energy score (ours)
33.01
91.88
97.83
5.16
ODIN [24]
35.71
91.09
97.62
5.16
Mahalanobis [23]
37.08
93.27
98.49
5.16
OE [14]
8.53
98.30
99.63
5.32
Energy ﬁne-tuning (ours)
3.32
98.92
99.75
4.87
CIFAR-100
(WideResNet)
Softmax score [13]
80.41
75.53
93.93
24.04
Energy score (ours)
73.60
79.56
94.87
24.04
ODIN [24]
74.64
77.43
94.23
24.04
Mahalanobis [23]
54.04
84.12
95.88
24.04
OE [14]
58.10
85.19
96.40
24.30
Energy ﬁne-tuning (ours)
47.55
88.46
97.10
24.58
Table 2: Comparison with discriminative-based OOD detection methods. ↑indicates larger values are better,
and ↓indicates smaller values are better. All values are percentages and are averaged over the six OOD test
datasets described in section 4.1. Bold numbers are superior results. Detailed results for each OOD test dataset
can be found in Appendix A.
for each dataset in Appendix A. We note that existing approaches using a pre-trained model have
hyperparameters that need to be tuned, sometimes with the help of additional data and a classiﬁer to
be trained (such as Mahalanobis [23]). In contrast, using an energy score on a pre-trained network
is parameter-free, easy to use and deploy, and in many cases, achieves comparable or even better
performance than ODIN [24].
In Table 3, we also compare with state-of-the-art hybrid models that incorporated generative model-
ing [8,11,18]. These approaches are stronger baselines than pure generative-modeling-based OOD
detection methods [4,29,34], due to the use of labeling information during training. In both cases
(with and without ﬁne-tuning), our energy-based method outperforms hybrid models.
How does temperature scaling affect the energy-based OOD detector? Previous work ODIN [24]
showed both empirically and theoretically that temperature scaling improves out-of-distribution
detection. Inspired by this, we also evaluate how the temperature parameter T affects the performance
of our energy-based detector. Applying a temperature T > 1 rescales the logit vector f(x) by 1/T.
Figure 4 in Appendix A shows how the FPR95 changes as we increase the temperature from T = 1
to T = 1000. Interestingly, using larger T leads to more uniformly distributed predictions and makes
the energy scores less distinguishable between in- and out-of-distribution examples. Our result means
that the energy score can be used parameter-free by simply setting T = 1.
How do the margin parameters affect the performance? Figure 4(b) shows how the performance
of energy ﬁne-tuning (measured by FPR) changes with different margin parameters of min and
mout on WideResNet. Overall the method is not very sensitive to mout in the range chosen. As
expected, imposing too small of an energy margin min for in-distribution data may lead to difﬁculty
in optimization and degradation in performance.
Does energy ﬁne-tuning affect the classiﬁcation accuracy of the neural network?
For the
inference-time use case, our method does not change the parameters of the pre-trained neural
network f(x) and preserves its accuracy. For energy ﬁne-tuned models, we compare classiﬁcation
accuracy of f(x) with other methods in Table 2. When trained on WideResNet with CIFAR-10 as
in-distribution, our energy ﬁne-tuned model achieves a test error of 4.98% on CIFAR-10, compared
to the OE ﬁne-tuned model’s 5.32% and the pre-trained model’s 5.16%. Overall this ﬁne-tuning
leads to improved OOD detection performance while maintaining almost comparable classiﬁcation
accuracy on in-distribution data.
5
Related Work
Out-of-distribution uncertainty for pre-trained models The softmax conﬁdence score has become
a common baseline for OOD detection [13]. A theoretical investigation [12] shows that neural
networks with ReLU activation can produce arbitrarily high softmax conﬁdence for OOD inputs.
DeVries and Taylor [6] propose to learn the conﬁdence score by attaching an auxiliary branch to a pre-
8

Dtest
in
Method
pre-trained?
SVHN
CIFAR-100
CelebA
CIFAR-10
Class-conditional Glow [18]

0.64
0.65
0.54
IGEBM [8]

0.43
0.54
0.69
JEM-softmax [11]

0.89
0.87
0.79
JEM-likelihood [11]

0.67
0.67
0.75
Energy score (ours)

0.91
0.87
0.78
Energy ﬁne-tuning (ours)

0.99
0.94
1.00
Table 3: Comparison with generative-based models for OOD detection. Values are AUROC.
trained classiﬁer and deriving an OOD score. However, previous methods are either computationally
expensive or require tuning many hyper-parameters. In contrast, in our work, the energy score can be
used as a parameter-free measurement, which is easy to use in an OOD-agnostic setting.
Out-of-distribution detection with model ﬁne-tuning While it is impossible to anticipate the
exact OOD test distribution, previous methods have explored using artiﬁcially synthesized data
from GANs [22] or unlabeled data [14] as auxiliary OOD training data. Auxiliary data allows the
model to be explicitly regularized through ﬁne-tuning, producing lower conﬁdence on anomalous
examples [2, 9, 27, 28, 38]. A loss function is used to force the predictive distribution of OOD
samples toward uniform distribution [14,22]. Recently, Mohseni et al. [28] explore training by adding
additional background classes for OOD score. Chen et al. [3] propose informative outlier mining by
selectively training on auxiliary OOD data that induces uncertain OOD scores, which improves the
OOD detection performance on both clean and perturbed adversarial OOD inputs. In our work, we
instead regularize the network to produce higher energy on anomalous inputs. Our approach does not
alter the semantic class space and can be used both with and without auxiliary OOD data.
Generative Modeling Based Out-of-distribution Detection. Generative models [7,17,35,39,41]
can be alternative approaches for detecting OOD examples, as they directly estimate the in-distribution
density and can declare a test sample to be out-of-distribution if it lies in the low-density regions.
However, as shown by Nalisnick et al. [29], deep generative models can assign a high likelihood
to out-of-distribution data. Deep generative models can be more effective for out-of-distribution
detection using improved metrics [4], including the likelihood ratio [34, 37]. Though our work
is based on discriminative classiﬁcation models, we show that energy scores can be theoretically
interpreted from a data density perspective. More importantly, generative-based models can be
prohibitively challenging to train and optimize, especially on large and complex datasets. In contrast,
our method relies on a discriminative classiﬁer, which can be much easier to optimize using standard
SGD. Our method therefore inherits the merits of generative-based approaches, while circumventing
the difﬁcult optimization process in training generative models.
Energy-based learning Energy-based machine learning models date back to Boltzmann machines [1,
36], networks of units with an energy deﬁned for the overall network. Energy-based learning [21,
32, 33] provides a uniﬁed framework for many probabilistic and non-probabilistic approaches to
learning. Recent work [50] also demonstrated using energy functions to train GANs [10], where
the discriminator uses energy values to differentiate between real and generated images. Xie et
al. [43] ﬁrst showed that a generative random ﬁeld model can be derived from a discriminative neural
networks. In subsequent works, Xie et al. [42,44–46] explored using EBMs for video generation and
3D shape pattern generation. While Grathwohl et al. [11] explored using JEM for OOD detection,
their optimization objective estimates the joint distribution p(x, y) from a generative perspective; they
use standard probabilistic scores in downstream OOD detection. In contrast, our training objective is
purely discriminative, and we show that non-probabilistic energy scores can be directly used as a
scoring function for OOD detection. Moreover, JEM requires estimating the normalized densities,
which can be challenging and unstable to compute. In contrast, our formulation does not require
proper normalization and allows greater ﬂexibility in optimization. Perhaps most importantly, our
training objective directly optimizes for the energy gap between in- and out-of-distribution, which
ﬁts naturally with the proposed OOD detector that relies on energy score.
6
Conclusion and Outlook
In this work, we propose an energy-based framework for out-of-distribution detection. We show
that energy score is a simple and promising replacement of the softmax conﬁdence score. The key
idea is to use a non-probabilistic energy function that attributes lower values to in-distribution data
and higher values to out-of-distribution data. Unlike softmax conﬁdence scores, the energy scores
9

are provably aligned with the density of inputs, and as a result, yield substantially improved OOD
detection performance. For future work, we would like to explore using energy-based OOD detection
beyond image classiﬁcation tasks. Our approach can be valuable to other machine learning tasks such
as active learning. We hope future research will increase the attention toward a broader view of OOD
uncertainty estimation from an energy-based perspective.
7
Broader Impact
Our project aims to improve the dependability and trustworthiness of modern machine learning
models. This stands to beneﬁt a wide range of ﬁelds and societal activities. We believe out-of-
distribution uncertainty estimation is an increasingly critical component of systems that range from
consumer and business applications (e.g., digital content understanding) to transportation (e.g., driver
assistance systems and autonomous vehicles), and to health care (e.g., rare disease identiﬁcation).
Through this work and by releasing our code, we hope to provide machine learning researchers a new
methodological perspective and offer machine learning practitioners an easy-to-use tool that renders
safety against anomalies in the open world. While we do not anticipate any negative consequences to
our work, we hope to continue to improve and build on our framework in future work.
Acknowledgement
The research at UC Davis was supported by an NVIDIA gift and their donation of a DGX Station.
Research at UW-Madison is partially supported by the Ofﬁce of the Vice Chancellor for Research
and Graduate Education with funding from the Wisconsin Alumni Research Foundation (WARF).
References
[1] David H. Ackley, Geoffrey E. Hinton, and Terrence J. Sejnowski. A learning algorithm for
Boltzmann machines. Cognitive Science, 9(1):147–169, 1985.
[2] Petra Bevandi´c, Ivan Krešo, Marin Orši´c, and Siniša Šegvi´c. Discriminative out-of-distribution
detection for semantic segmentation. arXiv preprint arXiv:1808.07703, 2018.
[3] Jiefeng Chen, Yixuan Li, Xi Wu, Yingyu Liang, and Somesh Jha.
Informative outlier
matters: Robustifying out-of-distribution detection using outlier mining.
arXiv preprint
arXiv:2006.15207, 2020.
[4] Hyunsun Choi and Eric Jang. WAIC, but why? Generative ensembles for robust anomaly
detection. arXiv preprint arXiv:1810.01392, 2018.
[5] Mircea Cimpoi, Subhransu Maji, Iasonas Kokkinos, Sammy Mohamed, and Andrea Vedaldi.
Describing textures in the wild. In Proceedings of the IEEE Conference on Computer Vision
and Pattern Recognition, pages 3606–3613, 2014.
[6] Terrance DeVries and Graham W Taylor. Learning conﬁdence for out-of-distribution detection
in neural networks. arXiv preprint arXiv:1802.04865, 2018.
[7] Laurent Dinh, Jascha Sohl-Dickstein, and Samy Bengio. Density estimation using real NVP.
arXiv preprint arXiv:1605.08803, 2016.
[8] Yilun Du and Igor Mordatch. Implicit generation and generalization in energy-based models.
arXiv preprint arXiv:1903.08689, 2019.
[9] Yonatan Geifman and Ran El-Yaniv. SelectiveNet: A deep neural network with an integrated
reject option. arXiv preprint arXiv:1901.09192, 2019.
[10] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil
Ozair, Aaron Courville, and Yoshua Bengio. Generative adversarial nets. In Advances in Neural
Information Processing Systems, pages 2672–2680, 2014.
[11] Will Grathwohl, Kuan-Chieh Wang, Joern-Henrik Jacobsen, David Duvenaud, Mohammad
Norouzi, and Kevin Swersky. Your classiﬁer is secretly an energy based model and you should
treat it like one. In International Conference on Learning Representations, 2020.
[12] Matthias Hein, Maksym Andriushchenko, and Julian Bitterwolf. Why ReLU networks yield
high-conﬁdence predictions far away from the training data and how to mitigate the problem.
10

In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pages
41–50, 2019.
[13] Dan Hendrycks and Kevin Gimpel. A baseline for detecting misclassiﬁed and out-of-distribution
examples in neural networks. arXiv preprint arXiv:1610.02136, 2016.
[14] Dan Hendrycks, Mantas Mazeika, and Thomas Dietterich. Deep anomaly detection with outlier
exposure. In International Conference on Learning Representations, 2019.
[15] Yen-Chang Hsu, Yilin Shen, Hongxia Jin, and Zsolt Kira. Generalized ODIN: Detecting
out-of-distribution image without learning from out-of-distribution data. In Proceedings of
the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 10951–10960,
2020.
[16] Rui Huang and Yixuan Li. Mos: Towards scaling out-of-distribution detection for large semantic
space. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition
(CVPR), 2021.
[17] Diederik P. Kingma and Max Welling. Auto-encoding variational Bayes. arXiv preprint
arXiv:1312.6114, 2013.
[18] Durk P. Kingma and Prafulla Dhariwal. Glow: Generative ﬂow with invertible 1x1 convolutions.
In Advances in Neural Information Processing Systems, pages 10215–10224, 2018.
[19] Alex Krizhevsky. Learning multiple layers of features from tiny images. Master’s thesis,
University of Toronto, Department of Computer Science, 2009.
[20] Balaji Lakshminarayanan, Alexander Pritzel, and Charles Blundell. Simple and scalable
predictive uncertainty estimation using deep ensembles. In Advances in Neural Information
Processing Systems, pages 6402–6413, 2017.
[21] Yann LeCun, Sumit Chopra, Raia Hadsell, Marc’Aurelio Ranzato, and Fu-Jie Huang. A tutorial
on energy-based learning. In G. Bakir, T. Hofman, B. Schölkopf, A. Smola, and B. Taskar,
editors, Predicting Structured Data. MIT Press, 2006.
[22] Kimin Lee, Honglak Lee, Kibok Lee, and Jinwoo Shin. Training conﬁdence-calibrated classiﬁers
for detecting out-of-distribution samples. arXiv preprint arXiv:1711.09325, 2017.
[23] Kimin Lee, Kibok Lee, Honglak Lee, and Jinwoo Shin. A simple uniﬁed framework for
detecting out-of-distribution samples and adversarial attacks. In Advances in Neural Information
Processing Systems, pages 7167–7177, 2018.
[24] Shiyu Liang, Yixuan Li, and Rayadurgam Srikant.
Enhancing the reliability of out-of-
distribution image detection in neural networks. In 6th International Conference on Learning
Representations, ICLR 2018, 2018.
[25] Ziqian Lin, Sreya Dutta, and Yixuan Li. Mood: Multi-level out-of-distribution detection.
Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
2021.
[26] Ilya Loshchilov and Frank Hutter. SGDR: Stochastic gradient descent with warm restarts. arXiv
preprint arXiv:1608.03983, 2016.
[27] Andrey Malinin and Mark Gales. Predictive uncertainty estimation via prior networks. In
Advances in Neural Information Processing Systems, pages 7047–7058, 2018.
[28] Sina Mohseni, Mandar Pitale, JBS Yadawa, and Zhangyang Wang. Self-supervised learning for
generalizable out-of-distribution detection. Proceedings of the AAAI Conference on Artiﬁcial
Intelligence, 34(04):5216–5223, April 2020.
[29] Eric Nalisnick, Akihiro Matsukawa, Yee Whye Teh, Dilan Gorur, and Balaji Lakshminarayanan.
Do deep generative models know what they don’t know? arXiv preprint arXiv:1810.09136,
2018.
[30] Yuval Netzer, Tao Wang, Adam Coates, Alessandro Bissacco, Bo Wu, and Andrew Y. Ng.
Reading digits in natural images with unsupervised feature learning. In NIPS Workshop on
Deep Learning and Unsupervised Feature Learning, 2011.
[31] Anh Nguyen, Jason Yosinski, and Jeff Clune. Deep neural networks are easily fooled: High
conﬁdence predictions for unrecognizable images. In Proceedings of the IEEE Conference on
Computer Vision and Pattern Recognition, pages 427–436, 2015.
11

[32] Marc’Aurelio Ranzato, Christopher Poultney, Sumit Chopra, and Yann LeCun. Efﬁcient learning
of sparse representations with an energy-based model. In Advances in Neural Information
Processing Systems, pages 1137–1144, 2007.
[33] Marc’Aurelio Ranzato, Y-Lan Boureau, Sumit Chopra, and Yann LeCun. A uniﬁed energy-
based framework for unsupervised learning. In Artiﬁcial Intelligence and Statistics, pages
371–379, 2007.
[34] Jie Ren, Peter J Liu, Emily Fertig, Jasper Snoek, Ryan Poplin, Mark Depristo, Joshua Dillon,
and Balaji Lakshminarayanan. Likelihood ratios for out-of-distribution detection. In Advances
in Neural Information Processing Systems, pages 14680–14691, 2019.
[35] Danilo Jimenez Rezende, Shakir Mohamed, and Daan Wierstra. Stochastic backpropagation
and approximate inference in deep generative models. arXiv preprint arXiv:1401.4082, 2014.
[36] Ruslan Salakhutdinov and Hugo Larochelle. Efﬁcient learning of deep Boltzmann machines. In
Proceedings of the Thirteenth International Conference on Artiﬁcial Intelligence and Statistics,
pages 693–700, 2010.
[37] Joan Serrà, David Álvarez, Vicenç Gómez, Olga Slizovskaia, José F. Núñez, and Jordi Luque.
Input complexity and out-of-distribution detection with likelihood-based generative models. In
International Conference on Learning Representations, 2020.
[38] Akshayvarun Subramanya, Suraj Srinivas, and R. Venkatesh Babu. Conﬁdence estimation in
deep neural networks via density modelling. arXiv preprint arXiv:1707.07013, 2017.
[39] Esteban G Tabak and Cristina V Turner. A family of nonparametric density estimation algo-
rithms. Communications on Pure and Applied Mathematics, 66(2):145–164, 2013.
[40] Antonio Torralba, Rob Fergus, and William T. Freeman. 80 million tiny images: A large data
set for nonparametric object and scene recognition. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 30(11):1958–1970, 2008.
[41] Aaron Van den Oord, Nal Kalchbrenner, Lasse Espeholt, Oriol Vinyals, Alex Graves, and Koray
Kavukcuoglu. Conditional image generation with PixelCNN decoders. In Advances in Neural
Information Processing Systems, pages 4790–4798, 2016.
[42] Jianwen Xie, Yang Lu, Ruiqi Gao, Song-Chun Zhu, and Ying Nian Wu. Cooperative training
of descriptor and generator networks. IEEE transactions on pattern analysis and machine
intelligence, 42(1):27–45, 2018.
[43] Jianwen Xie, Yang Lu, Song-Chun Zhu, and Yingnian Wu. A theory of generative convnet. In
International Conference on Machine Learning, pages 2635–2644, 2016.
[44] Jianwen Xie, Zilong Zheng, Ruiqi Gao, Wenguan Wang, Song-Chun Zhu, and Ying Nian Wu.
Learning descriptor networks for 3d shape synthesis and analysis. In Proceedings of the IEEE
conference on computer vision and pattern recognition, pages 8629–8638, 2018.
[45] Jianwen Xie, Song-Chun Zhu, and Ying Nian Wu. Synthesizing dynamic patterns by spatial-
temporal generative convnet. In Proceedings of the ieee conference on computer vision and
pattern recognition, pages 7093–7101, 2017.
[46] Jianwen Xie, Song-Chun Zhu, and Ying Nian Wu. Learning energy-based spatial-temporal
generative convnets for dynamic patterns. IEEE transactions on pattern analysis and machine
intelligence, 2019.
[47] Pingmei Xu, Krista A Ehinger, Yinda Zhang, Adam Finkelstein, Sanjeev R. Kulkarni, and
Jianxiong Xiao. TurkerGaze: Crowdsourcing saliency with webcam based eye tracking. arXiv
preprint arXiv:1504.06755, 2015.
[48] Fisher Yu, Ari Seff, Yinda Zhang, Shuran Song, Thomas Funkhouser, and Jianxiong Xiao.
LSUN: Construction of a large-scale image dataset using deep learning with humans in the loop.
arXiv preprint arXiv:1506.03365, 2015.
[49] Sergey Zagoruyko and Nikos Komodakis.
Wide residual networks.
arXiv preprint
arXiv:1605.07146, 2016.
[50] Junbo Zhao, Michael Mathieu, and Yann LeCun. Energy-based generative adversarial networks.
In 5th International Conference on Learning Representations, ICLR 2017.
12

[51] Bolei Zhou, Agata Lapedriza, Aditya Khosla, Aude Oliva, and Antonio Torralba. Places: A
10 million image database for scene recognition. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 40(6):1452–1464, 2017.
13

Supplementary Material:
Energy-based Out-of-distribution Detection
A
Detailed Experimental Results
We report the performance of OOD detectors on each of the six OOD test datasets in Table 4
(CIFAR-10) and Table 5 (CIFAR-100).
Dataset
Dtest
out
FPR95
AUROC
AUPR
↓
↑
↑
Texture
Softmax score [13]
59.28
88.50
97.16
Energy score (ours)
52.79
85.22
95.41
ODIN [24]
49.12
84.97
95.28
Mahalanobis [23]
15.00
97.33
99.41
OE [14]
12.94
97.73
99.52
Energy ﬁne-tuning (ours)
5.34
98.56
99.68
SVHN
Softmax score [13]
48.49
91.89
98.27
Energy score (ours)
35.59
90.96
97.64
ODIN [24]
33.55
91.96
98.00
Mahalanobis [23]
12.89
97.62
99.47
OE [14]
4.36
98.63
99.74
Energy ﬁne-tuning (ours)
1.04
99.41
99.89
Places365
Softmax score [13]
59.48
88.20
97.10
Energy score (ours)
40.14
89.89
97.30
ODIN [24]
57.40
84.49
95.82
Mahalanobis [23]
68.57
84.61
96.20
OE [14]
19.07
96.16
99.06
Energy ﬁne-tuning (ours)
9.00
97.48
99.35
LSUN-C
Softmax score [13]
30.80
95.65
99.13
Energy score (ours)
8.26
98.35
99.66
ODIN [24]
15.52
97.04
99.33
Mahalanobis [23]
39.22
94.15
98.81
OE [14]
2.89
99.49
99.90
Energy ﬁne-tuning (ours)
1.67
99.32
99.86
LSUN
Resize
Softmax score [13]
52.15
91.37
98.12
Energy score (ours)
27.58
94.24
98.67
ODIN [24]
26.62
94.57
98.77
Mahalanobis [23]
42.62
93.23
98.60
OE [14]
5.59
98.94
99.79
Energy ﬁne-tuning (ours)
1.25
99.39
99.88
iSUN
Softmax score [13]
56.03
89.83
97.74
Energy score (ours)
33.68
92.62
98.27
ODIN [24]
32.05
93.50
98.54
Mahalanobis [23]
44.18
92.66
98.45
OE [14]
6.32
98.85
99.77
Energy ﬁne-tuning (ours)
1.60
99.33
99.87
Table 4: OOD Detection performance of CIFAR-10 as in-distribution for each OOD test dataset. The Maha-
lanobis score is calculated using the features of the second-to-last layer. Bold numbers are superior results.
B
Details of Experiments
Software and Hardware. We run all experiments with PyTorch and NVIDIA Tesla V100 DGXS
GPUs.
Number of Evaluation Runs. We ﬁne-tune the models once with a ﬁxed random seed. Following
OE [14], reported performance for each OOD dataset is averaged over 10 random batches of samples.
Average Runtime On a single GPU, the running time for energy ﬁne-tuning is around 6 minutes;
each training epoch takes 34 seconds. The evaluation time for all six OOD datasets is approximately
4 minutes.
14

Dataset
Dtest
out
FPR95
AUROC
AUPR
↓
↑
↑
Texture
Softmax score [13]
83.29
73.34
92.89
Energy score (ours)
79.41
76.28
93.63
ODIN [24]
79.27
73.45
92.75
Mahalanobis [23]
39.39
90.57
97.74
OE [14]
61.11
84.56
96.19
Energy ﬁne-tuning (ours)
57.01
87.40
96.95
SVHN
Softmax score [13]
84.59
71.44
92.93
Energy score (ours)
85.82
73.99
93.65
ODIN [24]
84.66
67.26
91.38
Mahalanobis [23]
57.52
86.01
96.68
OE [14]
65.91
86.66
97.09
Energy ﬁne-tuning (ours)
28.97
95.40
99.05
Places365
Softmax score [13]
82.84
73.78
93.29
Energy score (ours)
80.56
75.44
93.45
ODIN [24]
87.88
71.63
92.56
Mahalanobis [23]
88.83
67.87
90.71
OE [14]
57.92
85.78
96.56
Energy ﬁne-tuning (ours)
51.23
89.71
97.63
LSUN-C
Softmax score [13]
66.54
83.79
96.35
Energy score (ours)
35.32
93.53
98.62
ODIN [24]
55.55
87.73
97.22
Mahalanobis [23]
91.18
69.69
92.27
OE [14]
21.92
95.81
99.08
Energy ﬁne-tuning (ours)
16.04
96.97
99.34
LSUN
Resize
Softmax score [13]
82.42
75.38
94.06
Energy score (ours)
79.47
79.23
94.96
ODIN [24]
71.96
81.82
95.65
Mahalanobis [23]
21.23
96.00
99.13
OE [14]
69.36
79.71
94.92
Energy ﬁne-tuning (ours)
64.83
81.95
95.25
iSUN
Softmax score [13]
82.80
75.46
94.06
Energy score (ours)
81.04
78.91
94.91
ODIN [24]
68.51
82.69
95.80
Mahalanobis [23]
26.10
94.58
98.72
OE [14]
72.39
78.61
94.58
Energy ﬁne-tuning (ours)
67.23
79.36
94.37
Table 5: OOD Detection performance of CIFAR-100 as in-distribution for each speciﬁc dataset. The Maha-
lanobis scores are calculated from the features of the second-to-last layer. Bold numbers are superior results.
Energy Bound Parameters The optimal min is −23 for CIFAR-10 and −27 for CIFAR-100. The
optimal mout is −5 for both CIFAR-10 and CIFAR-100.
15

OOD
FPR95
AUROC
AUPR
ﬁne-tune?
dataset
Dtest
in
Dtest
out
↓
↑
↑
WideResNet
SVHN

Softmax score [13] / Energy score (ours)
iSUN
17.63 / 8.30
97.27 / 98.26
99.47 / 99.66
Places365
19.26 / 9.55
97.02 / 98.15
99.40 / 99.63
Texture
24.32 / 17.92
95.64 / 96.17
98.96 / 99.00
CIFAR-10
18.77 / 9.13
97.10 / 98.23
99.43 / 99.65
LSUN-Crop
31.60 / 26.02
94.40 / 94.59
98.79 / 98.75
LSUN-Resize
23.57 / 12.03
96.55 / 97.69
99.32 / 99.54
average
22.52 / 13.83
96.33 / 97.18
99.23 / 99.37
WideResNet
SVHN

OE ﬁne-tune [14] / Energy ﬁne-tune (ours)
iSUN
0.56 / 0.01
99.82 / 99.99
99.96 / 100.00
Places365
2.65 / 0.36
99.43 / 99.88
99.89 / 99.97
Texture
7.29 / 3.89
98.60 / 99.20
99.69 / 99.82
CIFAR-10
2.14 / 0.17
99.50 / 99.90
99.90 / 99.98
LSUN-Crop
10.93 / 10.26
97.96 / 97.82
99.56 / 99.46
LSUN-Resize
0.63 / 0.00
99.82 / 99.99
99.96 / 100.00
average
4.03 / 2.45
99.19 / 99.46
99.83 / 99.87
Table 6: OOD detection performance comparison using softmax-based vs. energy-based approaches. We use
WideResNet [49] to train on the in-distribution dataset SVHN with its training set only. We show results for
both using the pretrained model (top) and applying ﬁne-tuning (bottom). All values are percentages. ↑indicates
larger values are better, and ↓indicates smaller values are better. Bold numbers are superior results.
100
101
102
103
Temperature
10
20
30
40
50
60
70
FPR95 of OOD data
(a) Effect of temperature T
-10
-30
-100
-300
-1000Energy bound min for in-dist data
3.2
10
100
FPR95 of OOD data
mout = -3
mout = -5
mout = -7
(b) Effect of margin parameters
Figure 4: (a) We show the effect of T on a CIFAR-10 pre-trained WideResNet. The FPR (at 95% TPR)
increases with larger T. (b) Effect of margin parameters min and mout during energy ﬁne-tuning (WideResNet).
The x-axes are on a log scale.
16

