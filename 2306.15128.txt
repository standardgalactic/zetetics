MIMIC: Masked Image Modeling
with Image Correspondences
Kalyani Marathe1,2∗Mahtab Bigverdi1,2∗Nishat Khan1
Tuhin Kundu
Aniruddha Kembhavi2
Linda G. Shapiro1
Ranjay Krishna1,2
1University of Washington, 2Allen Institute for Artificial Intelligence
{kmarathe,mahtab,nkhan51,shapiro,ranjay}@cs.washington.edu, anik@allenai.org
Abstract
Many pixelwise dense prediction tasks—depth estimation and semantic segmenta-
tion—in computer vision today rely on pretrained image representations. Therefore,
curating effective pretraining datasets is vital. Unfortunately, the effective pre-
training datasets are those with multi-view scenes and have only been curated
using annotated 3D meshes, point clouds, and camera parameters from simulated
environments. We propose a dataset-curation mechanism that does not require any
annotations. We mine two datasets: MIMIC-1M with 1.3M and MIMIC-3M with
3.1M multi-view image pairs from open-sourced video datasets and from synthetic
3D environments. We train multiple self-supervised models with different masked
image modeling objectives to showcase the following findings: Representations
trained on MIMIC-3M outperform those mined using annotations on multiple
downstream tasks, including depth estimation, semantic segmentation, surface nor-
mals, and pose estimation. They also outperform representations that are frozen and
when downstream training data is limited to few-shot. Larger dataset (MIMIC-3M)
significantly improves performance, which is promising since our curation method
can arbitrarily scale to produce even larger datasets. MIMIC code, dataset, and
pretrained models are open-sourced at https://github.com/RAIVNLab/MIMIC.
1
Introduction
Today, dense vision tasks—depth prediction, semantic segmentation, surface normals, and pose
estimation—rely on pretrained representations [24, 2]. Naturally, self-supervised learning lends itself
as a potential solution. Despite the impressive performance on object recognition and other high-level
tasks, self-supervised representations for dense prediction tasks have not yet fully delivered [49]. The
representations trained on object-centric datasets such as ImageNet-1K [18] do not transfer well to
dense prediction datasets such as NYUv2 [43] and ADE20K [58] which contain indoor and outdoor
scenes. Moreover, the contrastive objectives that are often used on these object-centric datasets utilize
augmentations that do not preserve geometric pixel-wise information [14, 15, 11]. In response, the
general purpose representation learning method—masked image modeling and specifically masked
autoencoders (MAE)—has become a popular default self-supervised mechanism for such tasks [24].
Unfortunately, recent findings suggest that the patch representations learned by these MAE are devoid
of sufficient local information for tasks like depth estimation [49].
In response, we ask the following question: What information is necessary to learn useful represen-
tations for dense vision tasks? We find a potential answer in cognitive science: 3D understanding
of the physical world is one of the first visual skills emergent in infants; it plays a critical role in
the development of other skills, like depth estimation, understanding surfaces, occlusions, etc [26].
∗The authors contribute equally to this work.
Preprint. Under review.
arXiv:2306.15128v2  [cs.CV]  28 Jun 2023

Figure 1: We introduce a data-curation method that generates multi-view image datasets for self-
supervised learning. Our method identifies potential data sources, including indoor scenes/people/ob-
jects videos, 3D indoor environments, outdoor street views, and stereo pairs to identify potential
multiview images. Next, we use traditional computer vision methods such as SIFT keypoint detection
and homography transformation to locate corresponding patches. Finally, we filter pairs based on a
threshold for significant overlap, ensuring a substantial percentage of pixels match between a pair.
Icons have been designed using images from Flaticon.com
Scientists hypothesize that 3D understanding emerges from infants learning the relationship between
changes in visual stimuli in response to their self-motion, i.e. 3D awareness emerges by learning
correspondences between appearances as the infant’s vantage point changes [35].
Very recently, a machine learning paper proposed a variant of masked image modeling, named
cross-view completion (CroCo), which uses an objective that operationalizes learning representations
in response to changes in self-motion [49]. CroCo uses a pair of multi-view images to reconstruct a
masked view using the second view as support. Unfortunately, CroCo is a data-hungry objective. Its
MULTIVIEW-HABITAT dataset of 1.8M multi-view images was curated using a method that requires
ground truth 3D meshes to be annotated. Although CroCo shows promise, the lack of datasets
with 3D annotations is a severe limitation, preventing its objective from scaling. If one could mine
large-scale multi-view datasets, perhaps dense vision tasks could enjoy the success that the field of
natural language processing has welcomed due to the availability of large-scale pretraining text [7].
In this work, we contribute MIMIC: a data-curation method for developing multi-view datasets that
scale. Our method doesn’t require any 3D meshes and can generate multi-view datasets from unan-
notated videos and 3D simulated environments. We leverage classical computer vision techniques,
such as SIFT keypoint detection [33], RANSAC [21], homography estimation [23], etc. to extract
correspondences between frames in open-sourced unannotated videos (see Figure 1). In other words,
MIMIC produces a pretraining dataset for masked image modeling using image correspondences.
We mine two datasets: MIMIC-1M and MIMIC-3M, and show that they effectively train use-
ful self-supervised (MAE and CroCo) representations when compared to MULTIVIEW-HABITAT.
Our experiments show the following: First, representations from MIMIC-3M outperform those
trained using MULTIVIEW-HABITAT on multiple downstream tasks: depth estimation (NYUv2 [34]),
semantic segmentation (ADE20K [58]), surface normals (Taskonomy [56]), and pose estimation
(MSCOCO [31]). Second, using CroCo, we outperform both when representations are frozen as
well as when the encoders are fine-tuned end-to-end for individual downstream tasks. Third, larger
pretraining datasets (MIMIC-3M > MIMIC-1M) significantly improve performance, which is
promising since our curation method can arbitrarily scale to produce even larger datasets. Fourth,
performance on downstream tasks improves with more pretraining epochs. Fifth, we consistently
perform better in few-shot experiments. Sixth, since we don’t make any assumptions about the videos
we mine from, we can mine from object-centric videos and perform better on ImageNet-1K[18] linear
probing. Finally, the decoder that is usually discarded after masked image modeling produces better
quality reconstructions when trained on MIMIC-3M.
2
Related work
Our work enables self-supervised learning for dense vision tasks. Self-supervised learning in vision
can be divided into two types: instance discrimination objectives, in which two augmented views of
an image are encouraged to have the same representations, and masked image modeling, in which an
image is reconstructed from an incomplete version of itself.
Instance discrimination objectives. Amongst the instance discrimination objectives, DeepClus-
ter [8] and SwAV [10] discriminate between the image clusters; Moco [25] and SimCLR [13] use
contrastive learning; BYOL [22] and DINO [11] use interactions between a teacher and student model;
2

VICReg [4] avoids representation collapse by introducing regularization terms. These representations,
unfortunately, perform quite poorly on dense vision tasks [49].
Some objectives are specifically designed to incentivize denser representations [5]. PixPro [53]
and DenseCL [48] use pixel-level contrastive losses; DetCon [27] uses contrastive detections to
encourage object-level representations; InsLoc [55] uses instance localization; LOCA [9] learns by
predicting the patch positions of the query view in the reference view; CP2 [47] distinguishes between
foreground and background; ReSim [51] learns representations by maximizing the similarity between
the two sub-regions from the two augmented views. Most of these approaches however are primarily
pretrained on object-centric datasets, such as ImageNet-1K [18]. Moreover, dense prediction tasks
are usually used in applications such as indoor navigation and autonomous driving, which feature
indoor and outdoor scenes; object-centric representations typically do not perform well.
Masked image modeling. Amongst masked image modeling, BEiT [3] proposes the pretext task
of recovering the visual tokens from a corrupted image, MAE [24] learns by masking patches of an
image and inpainting the masked patches; MultiMAE extends MAE to a multi-task formulation [2].
Their approach uses pseudo labels extracted using supervised models such as DPT [38] pre-trained
on Omnidata [20] for depth and Mask2Former [16] on the MSCOCO [31] for semantic segmentation;
hence, MultiMAE is not fully self-supervised. CroCo [49] uses cross-view completion and ingests
multi-view images. Their data curation method, though, uses 3D metadata and meshes of synthetic
3D environments; their data is also not publicly available. By contrast, MIMIC neither needs any
pseudo labels extracted using supervised methods nor it needs any 3D meshes, point clouds or camera
parameters for dataset curation.
Downstream task. Several pretext task methods have improved downstream performances on
semantic segmentation, depth estimation, and other dense prediction tasks that require reasoning
about the 3D structure. Showing improvements for semantic segmentation, CP2 [47] copy pastes the
foreground of an image onto two different backgrounds and learns representations by distinguishing
the foreground and the background of the reference image; LOCA [9] learns by predicting the patch
positions of the query view in the reference view. To improve object detection, DetCo [52] learns
high-quality representations for object detection via multi-level supervision and contrastive learning
between the image and the local patches; InsLoc [55] learns by predicting the instance category of
the images composed of a foreground image pasted on a different background. To improve depth
prediction, CroCo [49] proposes cross-view completion.
Data curation for large scale visual learning Large-scale image datasets have incredibly accelerated
progress in visual learning. ImageNet-1K [18], with 1.2M images annotated by crowdsourcing led
to several breakthroughs and is still a standard dataset used for pretraining vision models. Visual
Genome [29] and LAION-5B [42], which connect language and vision, have paved the way similarly
for vision-language modeling. However, the efforts so far have been focused on high-level semantic
tasks like classification, and large-scale pretraining datasets for dense prediction tasks are not available
publicly. To address this challenge we propose a methodology for curating multi-view datasets using
videos and 3D environments.
3
MIMIC: Curating multi-view image dataset for dense vision tasks
Although CroCo [49] recently used MULTIVIEW-HABITAT, a multi-view dataset, this dataset is not
yet publicly available. Moreover, their dataset generation process requires 3D mesh, point cloud,
depth, or camera pose information for each scene. This requirement restricts the data sources that can
be employed to curate a multi-view dataset. Regrettably, there exists no such large-scale publicly
available dataset. To address this gap, we design MIMIC.
MIMIC can curate multi-view image datasets without any requirements on the data sources. It works
by cleverly combining traditional computer vision methods (Figure 1). The only mechanism our
curation process requires is a sampling mechanism (I1, I2) ∼g(S), where S is some data source
from which g(·) samples two images I1 and I2. For example, S can be a video from which g(·)
samples two image frames. Or S can be a synthetic 3D environment from which g(·) navigates to
random spatial locations and samples two random image renderings of the scene.
Identifying data sources. With no restrictions on data sources, we generate our MIMIC dataset
from both real as well as synthetic data sources. Due to ethical considerations, we do not extract
3

more from real videos and focus on open source video datasets. Therefore we extract also from
synthetic environments and only use their annotations to generate videos of trajectories. We use
DeMoN, ScanNet, ArkitScenes, Objectron, CO3D, Mannequin, and 3DStreetView as real data
sources. DeMoN [46] is a dataset containing stereo image pairs. ScanNet [17] and ArkitScenes [6]
contain videos from indoor environments. Objectron [1] and CO3D [39] are collections of videos
containing objects. Mannequin [30] provides a video dataset featuring individuals engaged in the
mannequin challenge. 3DStreetView [57] offers a collection of street images from multiple urban
areas.
Synthetic sources include 3D indoor scenes from HM3D [36], Gibson [50] and Matterport [12]
datasets using the Habitat simulator [41].
For these synthetic environments, we initialize an
agent randomly in the 3D environment and design g(·) to move the agent in random steps
and directions. For each scene, the agent moves to numerous locations and captures various
views. The total number of pairs sampled differs based on their navigatable area of each syn-
thetic environment.
All our data sources with their distributions are visualized in Figure 2.
Figure 2: Distribution of Data Sources (%). Real data sources, includ-
ing DeMoN, ScanNet, ArkitScenes, Objectron, CO3D, Mannequin, and
3DStreetView, contribute to 32% of MIMIC. The remaining portion
consists of synthetic sources, namely HM3D, Gibson, and Matterport.
Mining potential pairs.
The primary characteristic
of the image pairs in our
dataset resides in their abil-
ity to capture the same
scene or object from vary-
ing viewpoints while ex-
hibiting a substantial degree
of overlap. The dataset is
designed to strike a balance:
the overlap is not exces-
sively large to the point of
containing identical images,
rendering the pre-training
task trivial; nor is it ex-
cessively small, resulting in
disjoint image pairs that of-
fer limited utility, making
the task only self-completion.
In each video or scene, many image pairs can be generated. However, our focus is on selecting a
limited number of pairs that are more likely to meet our desired condition of having sufficient overlap.
Nonetheless, not all of these candidate pairs may ultimately be chosen. For instance, when dealing
with video data, a practical strategy involves creating a list of frames at regular time intervals, which
depends on the video’s speed. By selecting consecutive frames from this list, potential pairs are
generated. Conversely, collecting potential pairs in 3D scenes such as HM3D [36] or Gibson [50]
presents greater challenges. Therefore, inspired by CroCo [49], we employ the habitat simulator [41]
to capture comprehensive environment views. The agent undergoes random rotations and movements,
exploring the scene from various perspectives. By capturing images during these random walks, we
generate potential pairs for further analysis. The selection process involves identifying the best pair
based on a specified overlap range (50% to 70%), ensuring the inclusion of high-quality pairs with
diverse viewpoints. However, our approach doesn’t rely on additional information such as meshed
representations or camera parameters. Instead, we solely utilize the available images.
Matching and measuring overlap. Given a potential image pair capturing a scene, we employ the
widely recognized Scale-Invariant Feature Transform (SIFT) [33] algorithm to localize key points
in both images. After obtaining the key points and descriptors, we apply a brute-force matching
technique to establish correspondences between the key points in the first image and those in the
second image. We further utilize these matches to estimate a homography matrix [23], leveraging
the RANSAC (Random Sample Consensus) [21] algorithm to handle outliers effectively. We then
partition each image into non-overlapping patches of size 16×16. For each patch in the first image,
we conduct a search for the corresponding patch in the second image by randomly sampling a set
of points within the target patch. Matching these sampled points with their correspondences in the
4

second image allows us to identify the patch that exhibits the highest number of corresponding points,
fulfilling our objective.
Furthermore, we assess the degree of overlap by counting the number of patches in the first image
whose matches fall within the bounds of the second image. The fraction of these patches over all
patches provides a quantitative measure of the overlap between the images in a pair.
Additionally, it is noteworthy that information from the corresponding patches is made available as
metadata for each image pair. Moreover, in the data generation process, the patch size can be adjusted
to cater to different research purposes or specific requirements.
Filtering out degenerate matches. In our approach, the selection of image pairs is guided by the
objective of capturing shared 3D information while mitigating redundancy. As mentioned earlier,
the desired pairs consist of images that depict the same objects or scenes from different perspectives.
This characteristic enables the learning model to acquire valuable insights about the underlying 3D
structure. However, it is crucial to avoid including pairs where one image is a zoomed-in version of
the other, as such pairs provide limited additional information.
To address this concern, we modify the overlap metric used in the pair selection process. Specifically,
we incorporate a criterion that prevents the inclusion of patches from the first image that have exact
correspondences in the second image. Therefore, in the counting, we consider all patches that have
the same corresponding patch in the second image as a single entity. This refinement in the pair
selection process improves the overall quality of the dataset.
Overall statistics. The initial version of our dataset, MIMIC-1M, comprises a total of 1, 316, 199
image pairs, each capturing different scenes or objects from varying viewpoints. Among these pairs,
761, 751 are sourced from HM3D [36], 305, 197 from Gibson [50], 29, 658 from Matterport [12] ,
114, 729 from Mannequin [30], 22, 184 from DeMoN [46], 36, 433 from ScanNet [17], and 46, 250
from Objectron [1].
Building upon the scalability of our data curation approach, we expand the dataset to create a
second version, MIMIC-3M, to contain a total of 3, 163, 333 image pairs. This expansion involves
augmenting the HM3D [36] dataset with an additional 699, 322 pairs, the Gibson [50] dataset
with 351, 828 pairs, and the inclusion of new datasets such as ArkitScenes [6] with 81, 189 pairs,
CO3D [39] with 133, 482 pairs, and 3dStreetViews [57] with 579, 310 pairs. By incorporating these
new datasets, we further enrich the diversity and quantity of image pairs available in our dataset.
4
Training with MIMIC
To measure the effectiveness of MIMIC, we pretrain self-supervised models on it and evaluate the
utility of the learnt representations on downstream dense prediction tasks. We compare against
existing pretraining dataset alternatives.
4.1
Pretraining
We use two masked image modeling objectives: Masked Autoencoders [24] and CroCo [49]. We
use a ViT-B/16[19] as a backbone for all our experiments with input images sizes of 224 × 224.
We train our models on 8 RTX A6000 GPUs for 200 epochs with a warmup of 20 epochs. We
use a base learning rate of 1.5 × 10−4 and an AdamW [32] optimizer with a cosine learning rate
schedule, a weight decay of 0.05, and an effective batch size of 4096. We evaluate these pretrained
representations on a series of downstream dense prediction tasks.
MAE [49] pretraining.: MAE [24] masks out a large portion (75%) of the input patches of an
image and uses an asymmetric encoder-decoder architecture to reconstruct the masked-out pixels.
Specifically, it uses a ViT-based encoder to extract the latent representations of the masked view.
Then it pads the output with the masked tokens and feeds the latent patch embeddings to a lightweight
decoder. The decoder’s output reconstruction is optimized with an L2 loss. The reconstruction pixel
targets are normalized by computing the mean and standard deviation of the image patches.
CroCo [49] pretraining. CroCo [49] reconstructs a masked image input similar to MAE but supports
the reconstruction process through an unmasked second support view. CroCo curates a pretraining
dataset from the Habitat simulator [41] such that the two views do not have too many or too few
5

Table 1: CroCo pretrained with MIMIC-3M outperforms on NYUv2 depth estimation referred to
as (depth.est), ADE20K semantic segmentation (sem.seg), Taskonomy surface normal prediction
(surf.norm), and MSCOCO pose estimation (pos.est.) tasks.
Model
Frozen
Dataset
NYUv2
ADE20K
Taskonomy
MSCOCO
depth.est.
sem.seg.
surf.norm.
pos.est.
δ1 (↑)
mIOU (↑)
L1 (↓)
AP (↑)
AR (↑)
MAE
ImageNet-1K
79.60
46.10
59.20
74.90
80.40
MAE
✓
MV-HABITAT
-
-
-
-
-
MAE
✓
MIMIC-3M
80.65
29.05
68.97
-
-
MAE
MV-HABITAT
79.00
40.30
59.76
-
-
MAE
MIMIC-3M
85.32
40.54
58.72
69.13
75.22
CroCo
✓
MV-HABITAT
85.20
-
64.58
-
-
CroCo
✓
MIMIC-3M
85.81
30.25
61.7
-
-
CroCo
MV-HABITAT
85.60
40.60
54.13
66.50
73.2
CroCo
MIMIC-3M
91.79
42.18
53.02
72.8
78.4
corresponding pixels. We refer to it as MULTIVIEW-HABITAT. CroCo masks 90% of the first
image. CroCo uses Siamese ViT encoders with shared weights to encode each view. The decoding
cross-attends over the second view while reconstructing the first masked view.
4.2
Downstream tasks, datasets, and evaluation metrics
Depth estimation. We use the NYUv2 [34], a standard dataset used for measuring progress in depth
estimation. It consists of 795 training and 654 test images of indoor scenes. We report the δ1 metric -
which computes the percent of the pixels with error max(
ypi
ygi ,
ygi
ypi ) less than 1.25, where ypi is the
depth prediction and ygi is the ground truth of the ith pixel of an image.
Semantic Segmentation. We use ADE20K [58]. It consists of 20, 210 training images and 150
semantic categories. We report the mIOU which quantifies the percentage overlap between the
predicted and the ground truth predictions.
Surface normal. is a regression task that aims to estimate the orientation of a 3D surface. We use a
subset of Taskonomy [56] with 800 training images, 200 validation images, and 54, 514 test images.
We report the L1 loss value on the test set.
Classification. We use ImageNet-1K[18] classification. It contains 1.28M training images and 50k
validation images. We fix the encoder, run linear probing on the validation set and report accuracy.
Pose estimation. We use MSCOCO [31] for training and report average precision (AP) and average
recall (AR) on the validation set. Specifically, we adopt ViTPose-B [54] with the MAE encoder and
classic decoder [54].
4.3
Baseline datasets
We compare MIMIC with: ImageNet-1K [40] and MULTIVIEW-HABITAT [49].
ImageNet-1K[18]. ImageNet-1K is widely used large-scale dataset with 1.2M training images
associated with one of the 1K categories. The dataset was curated via crowdsourcing and primarily
contains object centric images with animals, plants, everyday objects, and instruments.
MULTIVIEW-HABITAT. Second, we compare against models trained on MULTIVIEW-HABITAT.
The dataset comprises synthetic renderings of indoor scenes collected using the 3D meshes available
in the Habitat simulator and is derived from the HM3D [37], ScanNet [17], Replica [44] and
ReplicaCAD [45]. This dataset is not available publicly. So, we compare against the released models
trained on this dataset [49].
6

50
100
150
200
Number of epochs
82
84
86
88
90
92
delta 1 
NYUv2 dataset
finetuned
frozen
50
100
150
200
Number of epochs
28
30
32
34
36
38
40
42
mIOU
ADE20K dataset
finetuned
frozen
(a)
100
101
102
103
Training samples seen
30
40
50
60
70
80
delta 1 
NYUv2 dataset
Multiview-Habitat
MIMIC-3M
102
103
104
Training samples seen
5
10
15
20
25
30
mIOU
ADE20K dataset
Multiview-Habitat
MIMIC-3M
(b)
Figure 3: (a) CroCo [49] pretrained on MIMIC shows an increasing trend with the number of training
epochs. The figure on the left shows the trends for the fine-tuned and frozen versions of the encoder
on NYUv2 depth estimation. The figure on the right shows the trend on the ADE20K dataset. (b)
CroCo pretrained on MIMIC-3M achieves better few shot performance on CroCo pretrained on
MULTIVIEW-HABITAT. The figure on the left shows the few shot performance on the NYUv2 dataset
and the figure on the right shows the few shot performance on ADE20K (semantic segmentation).
5
Experiments
Our experiments highlight the following key findings: First, we observe that our representations
outperform MULTIVIEW-HABITAT on depth estimation (NYUv2), semantic segmentation (ADE20K),
and surface normal (Taskonomy). Second, these results hold for CroCo regardless of whether the
representations are kept frozen and also when the image encoder is fine-tuned (§ 5.1). Third, larger
pretraining datasets (MIMIC-3M > MIMIC-1M) significantly improve performance, which is
promising since our curation method can arbitrarily scale to produce even larger datasets. Fourth,
for both depth estimation and semantic segmentation, we find that more pretraining continues to
lead to performance gains (§ 5.2). Fifth, our performance benefits hold as we vary the number
of few-shot fine-tuning data points available for both depth estimation and semantic segmentation
(§ 5.4). Sixth, we see improvements on ImageNet-1K linear probing classification compared to the
MULTIVIEW-HABITAT dataset, whose collection required 3D meshes (§ 5.5). Finally, we find that
our model produces higher-quality reconstructions using the pretraining decoder (§ 5.6).
5.1
MIMIC-3M outperforms multiple downstream tasks
Even though MIMIC-3M was generated with fewer assumptions than MULTIVIEW-HABITAT,
representations pretrained on MIMIC-3M perform better on multiple tasks (Table 1). When fine-
tuned for depth estimation, semantic segmentation, surface normals, and pose estimation, our
representations learned using both MAE and CroCo perform better. In fact, CroCo when trained on
MIMIC-3M leads to the state-of-the-art δ1 of NYUv2 depth using self-supervised methods. It also
achieves an mIOU of 42.18 on ADE20K, an L1 loss of 53.2 on Taskonomy surface normals, and an
average precision of 72.8 and an average recall of 78.4 on MSCOCO pose estimation.
To understand the quality of the learned representations using our MIMIC-3M, we freeze the
transformer backbone pretrained using MIMIC-3M and compare it with the CroCo [49] trained on
MULTIVIEW-HABITAT. MIMIC-3M outperforms MULTIVIEW-HABITAT and improves NYUv2
depth δ1 by 0.61 and reduces the L1 loss on Taskonomy surface normals by 2.87 points with a frozen
transformer. Existing work [49] did not report the frozen semantic segmentation or pose estimation
values, preventing us from comparing.
5.2
Pretraining on MIMIC improves downstream performance with training epochs
As we train for more training steps, the performance of both MIMIC-1M and MIMIC-3M improves
on the downstream tasks such as depth estimation and semantic segmentation (Figure 3(a)). This
trend holds regardless of whether the representations are fine-tuned or kept frozen.
7

Table 2: (a) MIMIC-3M shows improvements over MIMIC-1M. (b) MIMIC-3M outperforms
MULTIVIEW-HABITAT [49] on the classification task.
(a)
Dataset
Frozen
NYUv2(↑)
ADE20K(↑)
Taskonomy(↓)
depth δ1
sem.seg. mIOU
surf.norm. L1
MIMIC-1M
✓
82.67
27.47
67.23
MIMIC-3M
✓
85.81
30.25
61.70
MIMIC-1M
89.46
38.45
57.12
MIMIC-3M
91.79
42.18
53.02
(b)
Model
Pretraining Dataset
IN-1K (↑)
% accuracy
MAE
ImageNet-1K
67.45
CroCo
MV-HABITAT [49]
37.00
CroCo
MIMIC-3M
39.64
MAE
MV-HABITAT [49]
32.50
MAE
MIMIC-3M
39.86
Table 3: MIMIC-3M achieves better FID score and reduces the reconstruction loss on 500 test
images from the Gibson dataset compared to MULTIVIEW-HABITAT
Model
Dataset
Reconst. loss (↓)
FID score (↓)
CroCo
MV-HABITAT [49]
0.357
85.77
CroCo
MIMIC-3M
0.292
73.12
5.3
Scaling up MIMIC leads to performance gains
We study the scaling trends of MIMIC by varying the data size. We experiment with two scales:
the first MIMIC-1M with 1.3M image pairs and the second MIMIC-3M with 3.1M image pairs.
We train CroCo [49] with these two training sets and evaluate the performance on depth estimation,
semantic segmentation, and surface normals prediction tasks. Table 2(a) shows the downstream
performance on depth (NYUv2), semantic segmentation (ADE20K), and surface normals(Taskonomy)
tasks with data scaling. We observe consistent improvements on NYUv2 [34], ADE20K [58], and
Taskonomy [56]. Specifically, with fine-tuning, MIMIC-3M improved δ1 by 2.33 points, mIOU on
ADE20K by 3.72 points, and L1 loss by 4.1 points.
5.4
MIMIC-3M representations outperform with limited downstream data
We measure the label efficiency of the learned representations trained on MIMIC-3M by evaluating
its few-shot performance on NYUv2 depth esimation and ADE20k semantic segmentation. We freeze
the image encoder and fine-tune the task-specific decoders by varying the number of training images.
We run each k-shot finetuning at least 5 times and report the mean and the standard deviation of the
runs. Overall the representations trained on our MIMIC-3M show better labeling efficiency than
those trained used MULTIVIEW-HABITAT (Figure 3(b)).
5.5
MIMIC-3M improves linear probing accuracy on ImageNet-1K
Thus far, we have focused on dense vision tasks. To understand the potential of MIMIC for the
high-level classification tasks, we evaluate MAE [24] and CroCo [49] pretrained with MIMIC-3M
on ImageNet-1K [18]. MIMIC-3M outperforms MULTIVIEW-HABITAT on MAE by 7.36% and on
CroCo by 2.64%. We hypothesize that these improvements come from the real and object-centric
data from Objectron [1] included in MIMIC-3M. Naturally, performance is still much lower than
MAE pretrained on ImageNet-1K, which is expected since the pretraining data is in-domain.
5.6
Pretraining on MIMIC-3M improves FID score and reduces the reconstruction error
We analyze the quality of the reconstructions trained on MIMIC-3M versus MULTIVIEW-HABITAT.
We use FID scores [28], which indicate how realistic the reconstructions are and the reconstruction
error (L2 loss) in the original masked image modeling objective. We sample a test set of 500 images
from the Gibson dataset. We ensure that these images are sampled from the scenes that are not a part
of the MULTIVIEW-HABITAT [49] and MIMIC-3M. They are synthetic images for fair comparisons,
which MULTIVIEW-HABITAT only has synthetic images. We mask 90% of each test image and then
compare the quality of the reconstructions (Table 3). Our analysis shows that CroCo trained on
MIMIC-3M improves the FID by 12.65 points and reduces the reconstruction loss on the test set.
8

6
Discussion
In this work, we present MIMIC, an approach to curating large-scale datasets for self-supervised
pretraining, geared towards dense vision tasks. We discuss the limitations and safety considerations
regarding our dataset, and lay out opportunities for future work.
Limitations. There are several limitations of our work. First, we train our models with a limited
computing budget and hence are undertrained. Our trends show that more pretraining continues to
improve performance. Second, we pretrain CroCo on MIMIC-3M using a fixed-sized architecture
ViT-B/16; model scaling experiments are outside the scope of our resources. Lastly, our curated
dataset primarily consists of static objects and does not involve dynamic scenes with moving objects.
Moreover, MIMIC-3M has a limited amount of object-centric data, and its suitability for object-
related tasks is unknown.
Safety and ethical considerations. While our method uses publicly available datasets for data
curation, we acknowledge that the algorithm can be scaled up to scrape videos in the wild. We are
aware of the safety, privacy, and ethical issues caused by models trained on large-scale datasets and
the amplification of the social and racial biases these models may result in. We do not endorse using
our methodology for applications other than academic research purposes and recommend the use of
face blurring and NSFW filtering before deploying models trained using MIMIC.
Future work. We would ideally like to study scaling trends with respect to ViT architecture, design
methodologies to mine dynamic videos where epipolar geometric constraints do not apply, design
new objectives for pretraining on image pairs curated using MIMIC, and evaluate representations on
more diverse tasks. The flexibility of MIMIC makes it suitable for further scaling it up to even larger
pretraining datasets.
Acknowledgements
This research is sponsored by grant from Amazon Technologies, Inc. as part of the Amazon-UW
Science HUB. We thank Michael Wolf and Ariel Gordon for helpful discussions, Saygin Seyfioglu
for helpful feedback, Sadjyot Gangolli for help with data, and Mitchell Wortsman for help with
large-scale pretraining. We also thank the UW-IT team: Stephen Spencer, Nam Pho, and Matt Jay.
References
[1] Adel Ahmadyan, Liangkai Zhang, Artsiom Ablavatski, Jianing Wei, and Matthias Grundmann. Objectron:
A large scale dataset of object-centric videos in the wild with pose annotations. In Proceedings of the
IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), pages 7822–7831, June
2021.
[2] Roman Bachmann, David Mizrahi, Andrei Atanov, and Amir Zamir. Multimae: Multi-modal multi-task
masked autoencoders. In Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel,
October 23–27, 2022, Proceedings, Part XXXVII, pages 348–367. Springer, 2022.
[3] Hangbo Bao, Li Dong, Songhao Piao, and Furu Wei. Beit: Bert pre-training of image transformers. arXiv
preprint arXiv:2106.08254, 2021.
[4] Adrien Bardes, Jean Ponce, and Yann LeCun. Vicreg: Variance-invariance-covariance regularization for
self-supervised learning. arXiv preprint arXiv:2105.04906, 2021.
[5] Adrien Bardes, Jean Ponce, and Yann LeCun. Vicregl: Self-supervised learning of local visual features.
arXiv preprint arXiv:2210.01571, 2022.
[6] Gilad Baruch, Zhuoyuan Chen, Afshin Dehghan, Tal Dimry, Yuri Feigin, Peter Fu, Thomas Gebauer,
Brandon Joffe, Daniel Kurz, Arik Schwartz, and Elad Shulman. ARKitscenes - a diverse real-world
dataset for 3d indoor scene understanding using mobile RGB-d data. In Thirty-fifth Conference on Neural
Information Processing Systems Datasets and Benchmarks Track (Round 1), 2021.
[7] Tom Brown, Benjamin Mann, Nick Ryder, Melanie Subbiah, Jared D Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda Askell, et al. Language models are few-shot learners.
Advances in neural information processing systems, 33:1877–1901, 2020.
9

[8] Mathilde Caron, Piotr Bojanowski, Armand Joulin, and Matthijs Douze. Deep clustering for unsupervised
learning of visual features. In Proceedings of the European conference on computer vision (ECCV), pages
132–149, 2018.
[9] Mathilde Caron, Neil Houlsby, and Cordelia Schmid. Location-aware self-supervised transformers. arXiv
preprint arXiv:2212.02400, 2022.
[10] Mathilde Caron, Ishan Misra, Julien Mairal, Priya Goyal, Piotr Bojanowski, and Armand Joulin. Unsu-
pervised learning of visual features by contrasting cluster assignments. Advances in neural information
processing systems, 33:9912–9924, 2020.
[11] Mathilde Caron, Hugo Touvron, Ishan Misra, Hervé Jégou, Julien Mairal, Piotr Bojanowski, and Armand
Joulin. Emerging properties in self-supervised vision transformers. In Proceedings of the IEEE/CVF
international conference on computer vision, pages 9650–9660, 2021.
[12] Angel Chang, Angela Dai, Thomas Funkhouser, Maciej Halber, Matthias Niessner, Manolis Savva, Shuran
Song, Andy Zeng, and Yinda Zhang. Matterport3d: Learning from rgb-d data in indoor environments.
International Conference on 3D Vision (3DV), 2017.
[13] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Geoffrey Hinton. A simple framework for
contrastive learning of visual representations. In International conference on machine learning, pages
1597–1607. PMLR, 2020.
[14] Xinlei Chen and Kaiming He. Exploring simple siamese representation learning. In Proceedings of the
IEEE/CVF conference on computer vision and pattern recognition, pages 15750–15758, 2021.
[15] Xinlei Chen, Saining Xie, and Kaiming He.
An empirical study of training self-supervised vision
transformers. In Proceedings of the IEEE/CVF International Conference on Computer Vision, pages
9640–9649, 2021.
[16] Bowen Cheng, Ishan Misra, Alexander G Schwing, Alexander Kirillov, and Rohit Girdhar. Masked-
attention mask transformer for universal image segmentation. In Proceedings of the IEEE/CVF Conference
on Computer Vision and Pattern Recognition, pages 1290–1299, 2022.
[17] Angela Dai, Angel X Chang, Manolis Savva, Maciej Halber, Thomas Funkhouser, and Matthias Nießner.
Scannet: Richly-annotated 3d reconstructions of indoor scenes. In Proceedings of the IEEE conference on
computer vision and pattern recognition, pages 5828–5839, 2017.
[18] Jia Deng, Wei Dong, Richard Socher, Li-Jia Li, Kai Li, and Li Fei-Fei. Imagenet: A large-scale hierarchical
image database. In 2009 IEEE conference on computer vision and pattern recognition, pages 248–255.
Ieee, 2009.
[19] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. An image is worth
16x16 words: Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929, 2020.
[20] Ainaz Eftekhar, Alexander Sax, Jitendra Malik, and Amir Zamir. Omnidata: A scalable pipeline for
making multi-task mid-level vision datasets from 3d scans. In Proceedings of the IEEE/CVF International
Conference on Computer Vision, pages 10786–10796, 2021.
[21] Martin A Fischler and Robert C Bolles. Random sample consensus: a paradigm for model fitting with
applications to image analysis and automated cartography. Communications of the ACM, 24(6):381–395,
1981.
[22] Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena Buchatskaya,
Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, et al. Bootstrap your
own latent-a new approach to self-supervised learning. Advances in neural information processing systems,
33:21271–21284, 2020.
[23] Richard Hartley and Andrew Zisserman. Multiple view geometry in computer vision. Cambridge university
press, 2003.
[24] Kaiming He, Xinlei Chen, Saining Xie, Yanghao Li, Piotr Dollár, and Ross Girshick. Masked autoencoders
are scalable vision learners. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern
Recognition, pages 16000–16009, 2022.
[25] Kaiming He, Haoqi Fan, Yuxin Wu, Saining Xie, and Ross Girshick. Momentum contrast for unsupervised
visual representation learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern
recognition, pages 9729–9738, 2020.
10

[26] Richard Held and Alan Hein. Movement-produced stimulation in the development of visually guided
behavior. Journal of comparative and physiological psychology, 56(5):872, 1963.
[27] Olivier J Hénaff, Skanda Koppula, Jean-Baptiste Alayrac, Aaron Van den Oord, Oriol Vinyals, and
Joao Carreira. Efficient visual pretraining with contrastive detection. In Proceedings of the IEEE/CVF
International Conference on Computer Vision, pages 10086–10096, 2021.
[28] Martin Heusel, Hubert Ramsauer, Thomas Unterthiner, Bernhard Nessler, and Sepp Hochreiter. Gans
trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information
processing systems, 30, 2017.
[29] Ranjay Krishna, Yuke Zhu, Oliver Groth, Justin Johnson, Kenji Hata, Joshua Kravitz, Stephanie Chen,
Yannis Kalantidis, Li-Jia Li, David A Shamma, et al. Visual genome: Connecting language and vision
using crowdsourced dense image annotations. International journal of computer vision, 123:32–73, 2017.
[30] Zhengqi Li, Tali Dekel, Forrester Cole, Richard Tucker, Noah Snavely, Ce Liu, and William T Freeman.
Learning the depths of moving people by watching frozen people. In Proceedings of the IEEE/CVF
conference on computer vision and pattern recognition, pages 4521–4530, 2019.
[31] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár,
and C Lawrence Zitnick. Microsoft coco: Common objects in context. In Computer Vision–ECCV 2014:
13th European Conference, Zurich, Switzerland, September 6-12, 2014, Proceedings, Part V 13, pages
740–755. Springer, 2014.
[32] Ilya Loshchilov and Frank Hutter.
Decoupled weight decay regularization.
arXiv preprint
arXiv:1711.05101, 2017.
[33] David G Lowe. Distinctive image features from scale-invariant keypoints. International journal of
computer vision, 60:91–110, 2004.
[34] Pushmeet Kohli Nathan Silberman, Derek Hoiem and Rob Fergus. Indoor segmentation and support
inference from rgbd images. In ECCV, 2012.
[35] Nancy Rader, Mary Bausano, and John E Richards. On the nature of the visual-cliff-avoidance response in
human infants. Child development, pages 61–68, 1980.
[36] Santhosh K Ramakrishnan, Aaron Gokaslan, Erik Wijmans, Oleksandr Maksymets, Alex Clegg, John
Turner, Eric Undersander, Wojciech Galuba, Andrew Westbury, Angel X Chang, et al. Habitat-matterport
3d dataset (hm3d): 1000 large-scale 3d environments for embodied ai. arXiv preprint arXiv:2109.08238,
2021.
[37] Santhosh Kumar Ramakrishnan, Aaron Gokaslan, Erik Wijmans, Oleksandr Maksymets, Alexander Clegg,
John M Turner, Eric Undersander, Wojciech Galuba, Andrew Westbury, Angel X Chang, Manolis Savva,
Yili Zhao, and Dhruv Batra. Habitat-matterport 3d dataset (HM3d): 1000 large-scale 3d environments
for embodied AI. In Thirty-fifth Conference on Neural Information Processing Systems Datasets and
Benchmarks Track, 2021.
[38] René Ranftl, Alexey Bochkovskiy, and Vladlen Koltun. Vision transformers for dense prediction. In
Proceedings of the IEEE/CVF International Conference on Computer Vision, pages 12179–12188, 2021.
[39] Jeremy Reizenstein, Roman Shapovalov, Philipp Henzler, Luca Sbordone, Patrick Labatut, and David
Novotny. Common objects in 3d: Large-scale learning and evaluation of real-life 3d category reconstruction.
In International Conference on Computer Vision, 2021.
[40] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang,
Andrej Karpathy, Aditya Khosla, Michael Bernstein, et al. Imagenet large scale visual recognition challenge.
International journal of computer vision, 115:211–252, 2015.
[41] Manolis Savva, Abhishek Kadian, Oleksandr Maksymets, Yili Zhao, Erik Wijmans, Bhavana Jain, Julian
Straub, Jia Liu, Vladlen Koltun, Jitendra Malik, Devi Parikh, and Dhruv Batra. Habitat: A platform for
embodied ai research. In Proceedings of the IEEE/CVF International Conference on Computer Vision
(ICCV), October 2019.
[42] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti,
Theo Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. Laion-5b: An open large-scale
dataset for training next generation image-text models. arXiv preprint arXiv:2210.08402, 2022.
[43] Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob Fergus. Indoor segmentation and support
inference from rgbd images. In European Conference on Computer Vision, 2012.
11

[44] Julian Straub, Thomas Whelan, Lingni Ma, Yufan Chen, Erik Wijmans, Simon Green, Jakob J. Engel, Raul
Mur-Artal, Carl Ren, Shobhit Verma, Anton Clarkson, Mingfei Yan, Brian Budge, Yajie Yan, Xiaqing Pan,
June Yon, Yuyang Zou, Kimberly Leon, Nigel Carter, Jesus Briales, Tyler Gillingham, Elias Mueggler,
Luis Pesqueira, Manolis Savva, Dhruv Batra, Hauke M. Strasdat, Renzo De Nardi, Michael Goesele,
Steven Lovegrove, and Richard Newcombe. The Replica dataset: A digital replica of indoor spaces. arXiv
preprint arXiv:1906.05797, 2019.
[45] Andrew Szot, Alexander Clegg, Eric Undersander, Erik Wijmans, Yili Zhao, John Turner, Noah Maestre,
Mustafa Mukadam, Devendra Singh Chaplot, Oleksandr Maksymets, et al. Habitat 2.0: Training home
assistants to rearrange their habitat. Advances in Neural Information Processing Systems, 34:251–266,
2021.
[46] Benjamin Ummenhofer, Huizhong Zhou, Jonas Uhrig, Nikolaus Mayer, Eddy Ilg, Alexey Dosovitskiy, and
Thomas Brox. Demon: Depth and motion network for learning monocular stereo. In Proceedings of the
IEEE conference on computer vision and pattern recognition, pages 5038–5047, 2017.
[47] Feng Wang, Huiyu Wang, Chen Wei, Alan Yuille, and Wei Shen. Cp 2: Copy-paste contrastive pretraining
for semantic segmentation. In Computer Vision–ECCV 2022: 17th European Conference, Tel Aviv, Israel,
October 23–27, 2022, Proceedings, Part XXX, pages 499–515. Springer, 2022.
[48] Xinlong Wang, Rufeng Zhang, Chunhua Shen, Tao Kong, and Lei Li. Dense contrastive learning for
self-supervised visual pre-training. In Proceedings of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition, pages 3024–3033, 2021.
[49] Philippe Weinzaepfel, Vincent Leroy, Thomas Lucas, Romain Brégier, Yohann Cabon, Vaibhav Arora,
Leonid Antsfeld, Boris Chidlovskii, Gabriela Csurka, and Jérôme Revaud. Croco: Self-supervised
pre-training for 3d vision tasks by cross-view completion. arXiv preprint arXiv:2210.10716, 2022.
[50] Fei Xia, Amir R Zamir, Zhiyang He, Alexander Sax, Jitendra Malik, and Silvio Savarese. Gibson env:
Real-world perception for embodied agents. In Proceedings of the IEEE conference on computer vision
and pattern recognition, pages 9068–9079, 2018.
[51] Tete Xiao, Colorado J Reed, Xiaolong Wang, Kurt Keutzer, and Trevor Darrell. Region similarity
representation learning. In Proceedings of the IEEE/CVF International Conference on Computer Vision,
pages 10539–10548, 2021.
[52] Enze Xie, Jian Ding, Wenhai Wang, Xiaohang Zhan, Hang Xu, Peize Sun, Zhenguo Li, and Ping Luo. Detco:
Unsupervised contrastive learning for object detection. In Proceedings of the IEEE/CVF International
Conference on Computer Vision (ICCV), pages 8392–8401, October 2021.
[53] Zhenda Xie, Yutong Lin, Zheng Zhang, Yue Cao, Stephen Lin, and Han Hu. Propagate yourself: Exploring
pixel-level consistency for unsupervised visual representation learning. In Proceedings of the IEEE/CVF
Conference on Computer Vision and Pattern Recognition, pages 16684–16693, 2021.
[54] Yufei Xu, Jing Zhang, Qiming Zhang, and Dacheng Tao. Vitpose: Simple vision transformer baselines for
human pose estimation. arXiv preprint arXiv:2204.12484, 2022.
[55] Ceyuan Yang, Zhirong Wu, Bolei Zhou, and Stephen Lin. Instance localization for self-supervised detection
pretraining. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition,
pages 3987–3996, 2021.
[56] Amir R. Zamir, Alexander Sax, William B. Shen, Leonidas J. Guibas, Jitendra Malik, and Silvio Savarese.
Taskonomy: Disentangling task transfer learning. In IEEE Conference on Computer Vision and Pattern
Recognition (CVPR). IEEE, 2018.
[57] Amir R Zamir, Tilman Wekel, Pulkit Agrawal, Colin Wei, Jitendra Malik, and Silvio Savarese. Generic
3d representation via pose estimation and matching. In Computer Vision–ECCV 2016: 14th European
Conference, Amsterdam, The Netherlands, October 11-14, 2016, Proceedings, Part III 14, pages 535–553.
Springer, 2016.
[58] Bolei Zhou, Hang Zhao, Xavier Puig, Tete Xiao, Sanja Fidler, Adela Barriuso, and Antonio Torralba.
Semantic understanding of scenes through the ade20k dataset. International Journal of Computer Vision,
127:302–321, 2019.
12

Appendix
7
Dataset, Resources, Assets
7.1
Dataset usage
The code and instructions to download, access, and use MIMIC-3M can be found at [link]. The
primary use case of this dataset is to train a 3D-aware ViT in a self-supervised manner. The dataset is
currently hosted on the UW CSE drive and we will update the URLs in the repository in case there
are any changes to the hosting plans.
7.2
Compute Resources
As mentioned in Section 4.1 (Pretraining) we train CroCo [49] for 200 epochs, each epoch taking
about 1 hour 40 minutes using 8 NVIDIA RTX A6000 GPUs. The cost for one training run is about
111 GPU days.
7.3
Assets
We provide the details of the dataset and code licenses used in our study in Table4. We bear all
responsibility in case of violation of rights. Our code is primarily based on MAE [25], MultiMAE [2]
and CroCo [49] and we have provided the license information here: LICENSE
Table 4: List of the assets and licenses
Asset
License
Pretraining datasets
HM3D [36]
[link]
Gibson [50]
[link]
3DStreetView [57]
[link]
CO3D [39]
[link]
Mannequin [30]
[link]
ArkitScenes [6]
[link]
Objectron [1]
[link]
ScanNet [17]
[link]
Matterport [12]
[link]
DeMoN [46]
[link]
Downstream datasets
ImageNet-1K [18]
[link]
NYUv2 [34]
[link]
ADE20K [58]
[link]
Taskonomy [56]
[link]
MSCOCO [31]
[link]
Code/Pretrained models
MAE [24]
[link]
CroCo [49]
[link]
MultiMAE [2]
[link]
8
Data curation details
8.1
Details on mining potential pairs
We utilized different data types within our datasets, including videos, 3D scenes, and street views.
Consequently, the process of mining potential pairs for each data type varied. For street views [57],
we adopted a strategy where we grouped images based on their target id (images that have the same
target id in their name, show the same physical point in their center). Subsequently, among all
possible combinations of images in a group, we selected the pair with minimal overlap ranging from
50% to 70%.
When dealing with video data, a practical approach involved creating a list of frames at regular time
intervals, determined by the speed of the video. Then, we generated pairs of consecutive frames
from this list. In cases where substantial overlap between consecutive frames was observed, we
specifically chose the second consecutive frame and evaluated its overlap with the preceding frame.
13

We implemented this step to ensure that the selected frame pair exhibits an appropriate level of
dissimilarity and minimized redundancy.
To tackle the challenges associated with handling 3D scenes, we employed the habitat simulator [41]
to sample locations within the navigable area of the scene. We initialized an agent with a random
sensor height and rotated it eight times at 45◦intervals, capturing a comprehensive view of the
surroundings to form the first list of eight images. Subsequently, we sampled a random rotation
degree from multiples of 60◦(excluding 180◦and 360◦), and rotated the agent accordingly before
moving in the current direction for a random step ranging from 0.5 to 1 meter. We repeated the
process of rotating eight times at 45◦intervals, capturing the second list of eight images. Likewise,
we randomly rotated and moved the agent to generate the third list of eight images. From these lists,
we selected an optimal pair (img1, img2) from a pool of 8 × 16 potential pairs. img1 belonged to
the first list, while img2 was chosen from the combined pool of the second and third lists, with a
minimal overlap ranging from 50% to 70%, if applicable.
The selection of a 45◦rotation aimed to capture a comprehensive view of the environment while
minimizing redundancy. Furthermore, the choice of rotation degrees as multiples of 60◦prevented
capturing images in directions already covered by those obtained with the 45◦rotation, effectively
avoiding the capture of zoomed-in versions of previously acquired images.
8.2
Details on measuring the overlap
Given a pair of images or views from a scene (we call it a potential pair), we checked whether these
two are sufficiently overlapped during the six steps. If they had enough overlap, we saved this pair
along with other metadata for the next phase, which was the model pretraining. The six steps are
listed below:
Keypoint localization using SIFT [33]. We used SIFT (Scale-Invariant Feature Transform) as a
feature detector to localize the two views’ key points separately. SIFT has been shown to perform
well compared to other traditional methods. Figure 4a provides an example pair with key points.
(a)
(b)
Figure 4: (a) A pair of images with SIFT key points. (b) Matching key points of images with a brute
force matcher.
Brute force matching. Having obtained both key point features and their descriptors from the
previous step, we performed a brute-force matching process to match the key points in the first view
(source points) with the key points in the second view (destination points). We present matches
between two views in Figure 4b.
Finding homography transformation [23]. We leveraged the homography [23] matrix to translate
the transformation among the views with provided source and destination points matches from the
previous step. However, we know the found transformation is not thoroughly accurate and free of
errors. Therefore, to overcome this issue, we used RANSAC [21] to conclude with better estimations
of the transformation. As a result, only some of the matches was categorized as inliers. Inlier matches
are shown in Figure 5a
Creating non-overlapping patches. After finding the homography matrix, we divided each view
into non-overlapping patches (16 × 16 here) and matched patches from view 1 to view 2, see Figure
5b.
14

(a)
(b)
Figure 5: (a) Inlier matches after finding the homography matrix. (b) Dividing each image to
non-overlapping patches.
(a)
(b)
Figure 6: (a) Sampling random points from a patch in the first view. (b) Blue points are the
corresponding points of the green points in the second view.
Obtaining the patch correpondences To find a corresponding patch in the second view for a
particular patch in the first view, we performed the following steps: 1. Randomly sampled a suitable
number of points within the specific patch in the first view (e.g., 100 points). In Figure 6a, random
green points are sampled within the green patch of the first view. 2. Applied the homography matrix
H to the sampled points to determine their corresponding positions in the second view. 3. Determined
the patch number in which each corresponding point falls, such as patch(x = 17, y = 0) = 1. 4.
Identified the patch that contains the maximum number of corresponding points as the match for
the specific patch in the first image. In Figure 6b, the blue points represent the positions of the
(a)
(b)
Figure 7: (a) The green patch from the view 1 is matched with the blue patch in view 2. (b) Two
views with their matching patches (matching patches have the same color).
15

Figure 8: Visualizations of the patchwise correspondences (matching patches have the same color).
corresponding points in the second view that fall within nearby patches. It can be observed that the
majority of the blue points cluster within a specific patch, which is marked as the matched patch for
the green patch. This match is illustrated in Figure 7a.
Measuring the visual overlap We repeated the procedure from the previous step for all patches in
the first view to determine their matches in the second view. We computed the count of patches in
the first view that have a matching patch within the boundaries of the second view, provided that the
matching patch has not been previously matched with another patch from the first view. Then, we
divided this count by the total number of patches, serving as a metric to measure the overlap.
To ensure a comprehensive evaluation, we performed the mentioned algorithm both for finding
overlap(view1, view2) and its inverse, overlap(view2, view1). We chose the minimum value
between these two overlap metrics as the final overlap measure.
Subsequently, we retained pairs with an overlap ranging from 50% to 75% along with corresponding
patches information. Figure 7b showcases all patches from the first view that have their matches
falling within the second view. Additionally, Figure 8 provides an illustrative example of a retained
pair of images from each dataset, along with their corresponding patches.
9
Downstream tasks
9.1
Finetuning details
In Table 5 , we provide the details of the hyperparameters used for finetuning CroCo [49] pretrained
on MIMIC-3M on NYUv2 [34], ADE20K [58], Taskonomy [56], MSCOCO [31].
16

Table 5: Hyperparameters used for fine-tuning NYUv2 depth, ADE20K semantic segmentation,
Taskonomy surface normals, and MSCOCO pose estimation
Hyperparameter
NYUv2(depth)
ADE20K(sem.seg.)
Taxonomy (surf.norm.)
MSCOCO(pos.est.)
Optimizer
AdamW
AdamW
AdamW
AdamW
Learning rate
0.0001
0.0005
0.0003
0.0005
Layer-wise lr decay
0.75
0.75
0.75
0.75
Weight decay
0.0003
0.05
0.05
0.1
Adam β
(0.9, 0.999)
(0.9, 0.999)
(0.9, 0.999)
(0.9, 0.999)
Batch size
64
16
8
512
Learning rate schedule.
Cosine decay
Cosine decay
Cosine decay
Linear Decay
Training epochs
2000
64
100
210
Warmup learning rate
-
0.000001
0.000001
0.001
Warmup epochs
100
1
5
500
Input resolution
256 × 256
512 × 512
384 × 384
224 × 224
Augmentation
ColorJitter, RandomCrop
HorizontalFlip, ColorJitter
-
TopDownAffine
Drop path
0.0
0.1
0.1
0.30
9.2
Visualizations of the fine-tuned models
In this section, we provide the visualizations of the depth maps, semantic segmentation masks, surface
normal predictions, and pose regression outputs after finetuning CroCo pretrained using MIMIC-3M.
For finetuning NYUv2 for depth, ADE20K for semantic segmentation, and Taskonomy for surface
normals, we followed MultiMAE [2] and used the settings from 9.1. For finetuning on MS COCO
we used ViTPose [54].
Depth Estimation. Figure 9 shows the input RGB file, predicted depth maps, and ground truth depth
maps from the validation set after finetuning on NYUv2.
Semantic Segmentation. Figure 10 shows the RGB images, predicted semantic segmentations, and
the ground truth labels from the ADE20K validation set after finetuning.
Surface Normals. Figure 11 shows predicted surface normals from the Taskonomy test set after
finetuning.
Pose estimation. Figure 12 shows the predicted keypoints from MS COCO validation set after
finetuning.
10
Details on the reconstructions experiment
In this study, we collected 500 test image pairs from the Gibson dataset to ensure a fair evaluation
process. We made a careful selection to exclude scenes present in the MIMIC-3M dataset, and
confirmed that the MULTIVIEW-HABITAT dataset did not include Gibson scenes. Following this, we
employed a random masking approach on the target image, utilizing the same masking matrix for
inputs of both the model trained on MIMIC3M and the one trained on MV-Habitat. The purpose
of this consistent masking procedure was to enable a comparative assessment of the reconstruction
performance on equivalent image patches. Then, each model separately reconstructed the masked
target view using the reference view. For the overall reconstruction loss, we got the average over 500
test pairs, which reconstruction loss for each pair was an average of l2 loss over masked pixels. See
reconstruction examples of both models in Figure 13.
17

Figure 9: Visualizations of the depth maps
18

Figure 10: Visualizations of the segmentation maps
19

Figure 11: Visualizations of the surface normal predictions
20

Figure 12: Visualizations of the pose estimation
21

Figure 13: Visualizations of the reconstructions
22

