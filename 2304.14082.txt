Published as a conference paper at ICLR 2023
JAXPRUNER
A CONCISE LIBRARY FOR SPARSITY RESEARCH
Joo Hyung Lee
Wonpyo Park
Nicole Mitchell
Jonathan Pilault
Johan Obando-Ceron
Han-Byul Kim
Namhoon Lee
Elias Frantar
Yun Long
Amir Yazdanbakhsh
Shivani Agrawal
Suvinay Subramanian
Xin Wang
Sheng-Chun Kao
Xingyao Zhang
Trevor Gale
Aart Bik
Woohyun Han
Milen Ferev
Zhonglin Han
Hong-Seok Kim
Yann Dauphin
Karolina Dziugaite
Pablo Samuel Castro
Utku Evci
Google Research
ABSTRACT
This paper introduces JaxPruner, an open-source JAX-based pruning and sparse
training library for machine learning research. JaxPruner aims to accelerate re-
search on sparse neural networks by providing concise implementations of popular
pruning and sparse training algorithms with minimal memory and latency over-
head. Algorithms implemented in JaxPruner use a common API and work seam-
lessly with the popular optimization library Optax, which, in turn, enables easy
integration with existing JAX based libraries. We demonstrate this ease of inte-
gration by providing examples in four different codebases: Scenic, t5x, Dopamine
and FedJAX and provide baseline experiments on popular benchmarks. Jaxpruner
is hosted at github.com/google-research/jaxpruner.
1
WHY A NEW SPARSITY LIBRARY IN JAX?
Sparsity is an active research area for achieving better efﬁciency in deep learning. However, utilizing
sparsity and realizing its potential in real life requires a closer collaboration between hardware,
software and algorithms research. Such collaborations often require a ﬂexible library to enable
rapid prototyping of ideas and evaluation on a variety of ever-changing benchmarks.
Sparsity in neural networks can appear in activations (data-dependent) or in parameters (data-
independent). In JaxPruner we mainly focus on parameter sparsity. This is because previous re-
search has highlighted its potential for achieving better performance than dense models with same
parameter count (Kalchbrenner et al., 2018; Li et al., 2020). Furthermore, the data-independent
nature of parameter sparsity makes it a good candidate for hardware acceleration. In this work we
focus on two strategies for achieving parameter sparsity: (1) pruning which aims to obtain sparse
networks starting from dense networks for inference efﬁciency and (2) sparse training which aims
to train sparse networks from scratch, thus reducing training cost, too.
1
arXiv:2304.14082v1  [cs.LG]  27 Apr 2023

Published as a conference paper at ICLR 2023
Over the last few years, JAX (Bradbury et al., 2018) has seen increasing adoption by the research
community (Godwin et al., 2020; Heek et al., 2020; Ro et al., 2021; Tang et al., 2022). The key
difference between JAX and other popular deep learning frameworks like PyTorch and Tensorﬂow
is the clear separation between functions and state. This makes function transformations like taking
gradients, hessian calculations or vectorization relatively easy, thus reducing the time required for
implementing complex ideas (Babuschkin et al., 2020). Similarly, having the entire state of a func-
tion in one place makes it easy to modify. As we will shortly discuss, these features also ease the
implementation of common subroutines across many pruning and sparse training algorithms.
Though implementations of individual algorithms (global magnitude pruning (Kao, 2022) and sparse
training with N:M sparsity and quantization (Lew et al., 2022)) exist, there is no comprehensive
library for sparsity research in JAX. This motivated us to develop JaxPruner. In what follows,
we discuss key design principles of JaxPruner (Section 2), provide a short overview of the library
(Section 3) and share our preliminary results bench-marking common pruning and sparse training
algorithms in (Section 4). We conclude with our plans for the future versions.
2
OUR TENETS: FAST INTEGRATION, RESEARCH FIRST AND MINIMAL
OVERHEAD
We want JaxPruner to facilitate sparsity research and help us answer important questions like
“Which sparsity pattern achieves a desired trade-off between accuracy and performance?”, “Is it
possible to train sparse networks without training a large dense model ﬁrst?”, and others. We were
guided by three tenets when designing the library in order to achieve these goals:
Fast Integration
Research in Machine Learning (ML) is fast paced. Combined with the huge
variety of ML applications, this result in a high number of ever-changing codebases. At the same
time, adaptability of new research ideas is highly correlated with their ease of use. Therefore, we
aimed to reduce friction for those integrating JaxPruner into existing codebases. JaxPruner uses
the popular Optax optimization library (Babuschkin et al., 2020) to achieve this, requiring minimal
changes when integrating with existing libraries. State variables (i.e. masks, counters) needed for
pruning and sparse training algorithms are stored together with the optimization state, which makes
parallelization and checkpointing easy.
Research First
Often research projects require running multiple algorithms and baselines, and so
they beneﬁt greatly from rapid prototyping. JaxPruner achieves this by committing to a generic API
shared among different algorithms, which in return makes it extremely easy to switch between differ-
ent algorithms. We provide implementations for common baselines and aim to make our algorithms
easy to modify. Furthermore it is extremely easy to switch between common sparsity structures
(unstructured, N:M, Block). A quick overview of such features are discussed in next section.
Minimal Overhead
There are growing number of options for accelerating sparsity in neural net-
works (e.g. N:M sparsity (Mishra et al., 2021), CPU-acceleration (Elsen et al., 2020), activation
sparsity (Gale et al., 2022)). However, integration with existing frameworks is often lacking, mak-
ing these advances relatively difﬁcult to use, especially in research. Given our main goal of facilitat-
ing research, JaxPruner follows the tradition of using binary masks for introducing sparsity, which
introduces some additional operations and requires additional storage for the masks. We aim to
minimize this overhead. Building on this initial version, we hope to introduce further optimizations
as software and hardware support matures over time. As a ﬁrst step towards this goal, we provide
integration with the jax.experimental.sparse1 module, which converts pruned parameters
into sparse representations and reduces the memory footprint signiﬁcantly.
3
QUICK OVERVIEW
The initial version of JaxPruner consists of about 1000 lines of code (+850 lines of tests), organized
into six modules. We provide interactive Python notebooks and integration with popular research
1https://jax.readthedocs.io/en/latest/jax.experimental.sparse.html
2

Published as a conference paper at ICLR 2023
libraries to facilitate adoption. Here we give a short overview of the JaxPruner API and list its key
features.
Figure 1: Visualization of a training loop. (left)
common training loop (right) JaxPruner wraps
the existing Optax transformations to store and
update variables like masks needed for pruning.
Additional operations (PRE/POST-OP) are added
to modify parameters at different points.
Optax Integration
State of the art pruning
algorithms often require iterative adjustments
to the sparsity pattern used. Such iterative ap-
proaches are stateful, i.e., they require some
additional variables like masks, counters, ini-
tial values, etc. This is similar to common op-
timization algorithms like Adam (Kingma &
Ba, 2014) and Momentum SGD, which require
their optimization state to be handled through-
out training. Most codebases in JAX achieve
this through Optax, which bundles all the dif-
ferent kinds of state-variables as a parameter
tree. A simpliﬁed diagram of a neural network
training loop which in Jax is given in Figure 1.
At every step of the training, parameters and
optimizer state is transformed using the gradi-
ents calculated through back-propagation. Op-
tax UPDATE FN is used to transform the gradi-
ents and the optimizer state. Finally resulting
gradients are added to the parameters.
One of the main novelties of JaxPruner is that
it exploits the observation that most iterative
pruning and sparse training algorithms can be thought of as special kinds of optimizers, which con-
ﬁne parameters into a sparse sub-domain. This key observation motivates us to use Optax gradient
transformations to implement our algorithms. This approach reduces boiler-plate code (like check-
pointing, passing variables around) required to integrate JaxPruner to existing codebases. Below
we give an example usage of JaxPruner inside an existing training loop and visualize these changes
in Fig. 1.
1 import jaxpruner
2
3 tx, params = _existing_code()
4 pruner = jaxpruner.MagnitudePruning(...) # Line 1: Create pruner.
5 tx = pruner.wrap_optax(tx) # Line 2: Wrap optimizer.
6
7 opt_state = tx.init(param)
8 # Line 3: [Optional] modifies weights temporarily for the forward pass.
9 forward_params = pruner.post_gradient_update(forward_params, opt_state)
10 new_params, new_opt_state = _training_step(tx, opt_state, forward_params)
11 # Line 4: Apply masks to parameters.
12 new_params = pruner.post_gradient_update(new_params, new_opt_state)
One-shot Pruning
Most iterative pruning algorithms can be converted into one-shot pruning al-
gorithms and vice-versa. Similarly, one can use pruning algorithms independently of a training
loop. In order to address such use cases we include the instant_sparsify method in our API.
instant_sparsify support variable collections and individual JAX arrays. Below we give an
example.
1 pruner = jaxpruner.MagnitudePruning(...) # Line 1: Create pruner.
2 X_pruned = pruner.instant_sparsify(X) # Line 2: Prune parameters.
BaseUpdater
Most pruning or sparse training algorithms share the following routines: (1) initial-
ize masks (2) apply masks and (3) update masks. This motivates us to unify common pruning and
sparse training operations under a single stateless class: jaxpruner.BaseUpdater. BaseUp-
dater implements most of the API functions (like wrap_optax, and instant_sparsify) in
a modular way such that different pruning algorithms can be implemented by overwriting only few
3

Published as a conference paper at ICLR 2023
functions. This makes implementation of common pruning or sparse training algorithms relatively
easy, reducing friction when trying new ideas. BaseUpdater class is also highly customizable, in
what follows we present three ways of controlling the behaviour of our algorithms.
Custom Sparsity Distributions
In the pruning literature, it is common to apply custom sparsities
to some of the layers, sometimes keeping them dense. We provide some common sparsity distri-
butions like uniform and erk under sparsity_distributions. These distributions can be
customized further by passing a mapping between parameters and sparsity values. Alternatively
users can deﬁne their own distribution functions and pass them to BaseUpdater directly.
Update Schedules
Most pruning algorithms work during training, but differ in how frequently
they increase the sparsity toward a target level and when they apply masks to parameters.
Similarly, sparse training algorithms often require changes in the sparsity pattern at differ-
ent frequencies.
We provide some common scheduling modules for such mask updates under
sparsity_schedules. Similar to sparsity distributions, users can deﬁne their custom schedules
easily and pass them to the existing algorithms.
Structured Sparsity
Despite exciting developments (Elsen et al., 2020; Gale et al., 2020), the
challenge of accelerating sparse neural networks remains. Using more regular sparsity patterns like
block (Gray et al., 2017; Mao et al., 2017; Narang et al., 2017; Li et al., 2016) and N:M sparsity
(Hubara et al., 2021; Mishra et al., 2021; Zhou et al., 2021; Sun et al., 2021) makes acceleration eas-
ier, however such patterns often sacriﬁce performance compared to unstructured sparsity. Reducing
this performance gap through ﬁnding better sparsity structures and algorithms is an active area of
research. Therefore, we make the sparsity type customizable through custom top-k functions. This
allows us to implement algorithms that can work with different sparsity types seamlessly:
1 pruner = jaxpruner.MagnitudePruning(sparsity_type=jaxpruner.NbyM(2,4))
2 X_pruned = pruner.instant_sparsify(X)
Other Features
We use uint8 types for storing masks2 to reduce memory footprint of our algo-
rithms. We also provide an example that converts masked dense parameters of a pruned ViT-B/16
model to sparse BCOO format and run the model with signiﬁcantly lower memory footprint using
jax.experimental.sparse.
4
BASELINES
We integrate our library with Scenic (Dehghani et al., 2022), T5x (Roberts et al., 2022), Dopamine
(Castro et al., 2018) and FedJAX (Ro et al., 2021). Often this process requires changing only a few
lines of code in the training loop as shown in the previous section. Our uniﬁed API enables easy
experimentation with a wide variety of algorithms. In this report we share results using the following
representative set of baseline algorithms:
1. Gradual Pruning with random (RAND), saliency (SAL) (Molchanov et al., 2016), weight
magnitude (MAG) (Zhu & Gupta, 2018) criteria. We also implement global pruning with
weight magnitude criterion (MAG-G).
2. Straight Through Estimator with weight magnitude criteria (STE). In sparse training
with straight through gradients (Bengio et al., 2013), parameters are projected into a sparse
sub-space before the forward pass. Then gradients are calculated for all parameters and
applied to the original set of dense parameters. STE is often applied using a ﬁxed sparsity
from the start of the training. In our experiments, however, we use the polynomial schedule
used by the gradual pruning algorithms, as we observed this to give better results.
3. Sparse Training including static sparse training (STATIC) and dynamic sparse training
with random (SET) (Mocanu et al., 2018) and gradient based (RIGL) (Evci et al., 2020)
growth.
2We also support using bits for storage, but due to slow-down during conversion and lack of native support
in jax we decided not to use them.
4

Published as a conference paper at ICLR 2023
Dense
Rand
Mag
Sal
Mag-G
STE
Static
SET
RigL
ResNet-50
76.67
70.19
75.53
74.93
75.49
73.54
71.34
74.57
74.75
ViT-B16
74.04
69.76
72.89
72.80
73.60
74.21
64.61
70.98
71.58
ViT-B16+
74.84
73.43
75.73
75.95
75.65
76.13
70.17
75.62
75.64
Fed. MNIST
86.21
83.53
85.74
85.60
86.01
86.16
83.33
84.20
84.64
T5-Base (↓)
2.58
3.29
2.95
3.52
5.44
2.71
3.17
3.13
3.12
DQN
2588
1435
2123
-
2322
-
1156
1723
1535
Table 1: Results with some of the algorithms implemented in JaxPruner. We group algorithms that
require storage or compute proportional to dense training in the middle. We report the validation
accuracy for the image classiﬁcation experiments (ﬁrst horizontal group). For T5-Base, we report
perplexity on C4 validation split (lower is better). DQN experiments report average returns on
MsPacman.
We run pruning and sparse training algorithms in the following settings: (1) ImageNet (Russakovsky
et al., 2015) image classiﬁcation using the ViT-B/16 (Dosovitskiy et al., 2021) and ResNet-50 archi-
tectures, (2) C4 pretraining the T5-Base encoder-decoder transformer architecture (Vaswani et al.,
2017; Raffel et al., 2020) (3) a DQN agent with a convolutional backbone trained on MsPacman
Atari game (4) Federated EMNIST (Cohen et al., 2017) character recognition using a CNN with
dropout (Reddi et al., 2020). We share our results in Table 1 and discuss experimental details and
results below.
4.1
IMAGE CLASSIFICATION
We apply JaxPruner algorithms to train 80% sparse ViT-B/16 and ResNet-50 architectures. Our
goal in these experiments is not to get state-of art results. Instead, we aim to provide some baseline
results using different training recipes and architectures. For all experiments, we use the default
hyper-parameters provided by the Scenic library.
ResNet-50 is a popular architecture in sparsity literature (Blalock et al., 2020; Hoeﬂer et al., 2021).
We train 80% sparse ResNet-50 models on ImageNet to reproduce previous results reported in the
literature (Gale et al., 2019; Evci et al., 2020). We use uniform sparsity across layers and leave
the ﬁrst convolutional layer dense as recommended by Gale et al. (2019). Though most results
match previous work, we observe a signiﬁcant improvement for the accuracy achieved by the SET
algorithm compared to the implementation done in (Evci et al., 2020).
We use ERK sparsity distribution (Mocanu et al., 2018; Evci et al., 2020) in our ViT experiments.
Sparse vision transformers achieve better generalization even for the shorter, 90 epoch, training
runs (ViT-B16). STE obtains the best results and exceeds the baseline performance by 0.2%. In-
terestingly, when we increase the number of training epochs to 300 (ViT-B16+), this gap widens
and sparse ViT-B/16 trained with STE obtains 1.2% higher accuracy, despite having worse (higher)
training loss. Dynamic sparse training methods (RigL and SET) performs poorly in shorter training
runs, however with extended training, they achieve almost 5% higher accuracy and exceed the dense
baseline.
4.2
FEDERATED LEARNING AND JAXPRUNER
Given the compute and communication constraints of the federated learning setting, pruning and
sparse training are critical mechanisms to explore. FedJAX (Ro et al., 2021) supports federated
learning research through JAX-based federated algorithm design and simulation, and can easily
be integrated with JaxPruner to explore how to leverage sparse training in federated learning. In
this paper we benchmark server-side pruning by using JaxPruner algorithms to change the server
optimization step.
We test the effect of various pruning algorithms on the federated EMNIST character recognition
benchmark (Cohen et al., 2017), using the model architecture and task setup presented in Reddi
et al. (2020). Pruning is applied on the server model on each round of training speciﬁed by the
pruning schedule, before being broadcast to a sampled selection of clients to continue training.
Each experiment is run for 1000 federated rounds, in which 50 clients are sampled and complete
5

Published as a conference paper at ICLR 2023
a single epoch of training on their data using a batch size of 32. For optimizers, we use SGD on
clients and Adam on server.
All pruning algorithms are conﬁgured with a target sparsity of 80%, the ERK distribution, an update
frequency of every 10 rounds, and an update end step of 750 federated rounds. The sparse training
algorithms (Static, RigL and SET) are conﬁgured to start updating in the ﬁrst federated round, while
the gradual pruning and straight through estimator algorithms are conﬁgured to begin pruning at
round 250. All results reported are the average ﬁnal accuracy on the evaluation dataset across ﬁve
random trials. We ﬁnd STE to perform best of the gradual pruning methods and RigL to outperform
the other sparse training methods tested.
4.3
LANGUAGE MODELING
We also build a JaxPruner integration with the t5x library (Roberts et al., 2022), which opens access
to a suite of Transformer-based (Vaswani et al., 2017) Language Models (LMs). In this section, we
apply JaxPruner algorithms to a T5 encoder-decoder LM model (Raffel et al., 2020). Similar to
experiments in Section 4.1, we prune 80% of the weights (5x compression) of our LM architecture.
We train from scratch a T5-base (220M parameter) model to predict missing words within a cor-
rupted span of text on the C4 dataset3 with the Adam optimizer (Kingma & Ba, 2015). We report
the per token cross-entropy loss on the validation split in Table 1. Our results show large differences
in performance across the pruning algorithms. As in our ViT vision experiments and federated learn-
ing experiments, STE outperforms other pruning algorithm and is within 5% of the dense baseline
performance.
4.4
DEEP REINFORCEMENT LEARNING ON ATARI
Dopamine (Castro et al., 2018) and Acme (Hoffman et al., 2020) provide stable and comprehensive
implementations for various Deep RL algorithms in JAX. We integrate JaxPruner with Dopamine
in our initial version as it has been used in the past for sparsity research and benchmarking various
sparse training and pruning methods (Graesser et al., 2022).
Dopamine framework includes DQN (Mnih et al., 2015), Rainbow (Hessel et al., 2018), and other
distributional deep RL agents like Quantile Regression for Distributional RL (QR-DQN) (Dabney
et al., 2018a) and Implicit Quantile Networks (IQN) (Dabney et al., 2018b). Though it is possible to
run any of the Atari games (Bellemare et al., 2012) and agents, we choose MsPacman and DQN for
our initial experiments.
We use the default hyper-parameter values provided in Dopamine library together with the CNN
architecture used in original DQN paper (Mnih et al., 2015). We apply sparsity to the existing model
using the ERK distributions and at 98% target sparsity. We ran our experiments for 40M frames, 5
independent seeds and report the average returns calculated over 125000 environment steps at the
end of the training.
5
RELATED WORK
JAX
(Bradbury et al., 2018) is a recently introduced Python library for high-performance machine
learning research. With Autograd (Baydin et al., 2018), JAX can automatically differentiate native
Python and Numpy functions such as loops and branches in both forward and backward modes.
Also, JAX supports just-in-time compilation of NumPy programs on multiple GPUs or TPUs in par-
allel with XLA (Sabne, 2020). Following the functional programming paradigm all transformations
in JAX work on pure functions and thus can be composed together in an arbitrary order. Since these
features can dramatically facilitate machine learning research, the community has started adopting
JAX to develop new research frameworks in recent years, including for example Optax (Babuschkin
et al., 2020), FedJAX (Ro et al., 2021), Flax (Heek et al., 2020), JaxOpt (Blondel et al., 2021) just
to name a few.
3https://www.tensorﬂow.org/datasets/catalog/c4
6

Published as a conference paper at ICLR 2023
Hardware
There are numerous efforts towards hardware and software support for sparsity. An
example of hardware acceleration for inference is 2:4 ﬁne-grained structured sparsity that was in-
troduced by the Nvidia A100 GPU (Pool et al., 2021; Pool & Yu, 2021). When each contiguous
block of four elements contains two zeros (viz. 50% sparsity), a low-overhead compression be-
comes possible which stores the non-zero values together with 2-bit indices. The hardware supports
this compressed format by only operating on the nonzero values during the computation.
Software
A promising direction for developing sparse software was pioneered for sparse linear
algebra in the MT1 compiler (Bik, 1996) and generalized to sparse tensor algebra in the Tensor Al-
gebra Compiler (Kjolstad et al., 2017; Chou et al., 2018; Kjolstad et al., 2019). In these approaches,
sparsity is treated as a property of tensors, not a tedious implementation detail, and a compiler auto-
matically generates sparse code from a ‘dense’ deﬁnition of the computation where the programmer
merely adds sparsity annotations to the tensor operands. A single description of a computation can
be mapped to a wide range of sparse implementations, each tailored to speciﬁc sparsity properties.
These ideas gave rise to, for example, sparse tensor support in the MLIR compiler-infrastructure (Bik
et al., 2022) and proposed sparse extensions to JAX (Bradbury et al., 2018).
Sparse linear algebra binary libraries such as MKL (Wang et al., 2014) and cuSPARSE (Naumov
et al., 2010) implement sparse basic linear algebra subroutines for a small set of sparse data types.
Generic libraries like Eigen (Guennebaud et al., 2010) and CUSP (Dalton et al., 2014) allow writ-
ing math-like expressions for a wider choice of data types. The GraphBLAS (Kepner et al., 2015)
standard speciﬁes a core set of general sparse matrix-based graph operations over arbitrary semi-
rings. Many libraries implement this standard (Buluc¸ & Gilbert, 2011; Anderson et al., 2016; Yang
et al., 2019; Davis, 2019) for CPUs and GPUs. Libraries such as Sputnik (Gale et al., 2020), cuS-
PARSELt (NVIDIA, 2021), and LIBXSMM (Heinecke et al., 2016) add new kernels and data types
speciﬁc to deep learning, but still with limited portability. MegaBlocks (Gale et al., 2022) is a
framework of efﬁcient Mixture-of-Experts training on GPUs.
Sparsity
has witnessed a resurgence of research interests over the last few years, with many ex-
citing developments and variations of standard sparsity approaches. See Hoeﬂer et al. (2021); Liu
& Wang (2023) for comparisons between various sparsity methods and a comprehensive analysis.
Despite progress made, there is a need for sparsity libraries, benchmarks, and evaluation protocols.
Gale et al. (2019) compared few popular algorithms across different domains and architectures.
Similarly (Blalock et al., 2020) provided an extensive report on benchmarks used in pruning papers.
One of the key existing libraries, OpenLTH (Frankle, 2019), is primarily focused on easing research
related to the Lottery Ticket Hypothesis (Frankle & Carbin, 2018), and facilitates implementation
of magnitude-based pruning methods as well as computation of related metrics. Other mask-based
pruning libraries in PyTorch include (Paganini, 2021). Alternatively, Ivanov et al. (2022) focuses
on providing acceleration in PyTorch through sparse matrix representations and operations. Many
of these libraries have been used by many published papers. We hope our library would facilitate
research in a similar way.
ACKNOWLEDGMENTS
We would like to thank Hugo Larochelle, Jo¨elle Barral, Laura Graesser and the greater Google
Resaerch team for their feedback and support. We also thank Mathieu Blondel, Yingtao Tian and
Mostafa Dehghani for sharing their experience with open-sourcing.
REFERENCES
Michael J Anderson, Narayanan Sundaram, Nadathur Satish, Md Mostofa Ali Patwary, Theodore L
Willke, and Pradeep Dubey. Graphpad: Optimized graph primitives for parallel and distributed
platforms. In 2016 IEEE International Parallel and Distributed Processing Symposium (IPDPS),
pp. 313–322. IEEE, 2016.
Igor Babuschkin, Kate Baumli, Alison Bell, Surya Bhupatiraju, Jake Bruce, Peter Buchlovsky,
David Budden, Trevor Cai, Aidan Clark, Ivo Danihelka, Claudio Fantacci, Jonathan Godwin,
Chris Jones, Ross Hemsley, Tom Hennigan, Matteo Hessel, Shaobo Hou, Steven Kapturowski,
Thomas Keck, Iurii Kemaev, Michael King, Markus Kunesch, Lena Martens, Hamza Merzic,
7

Published as a conference paper at ICLR 2023
Vladimir Mikulik, Tamara Norman, John Quan, George Papamakarios, Roman Ring, Francisco
Ruiz, Alvaro Sanchez, Rosalia Schneider, Eren Sezener, Stephen Spencer, Srivatsan Srinivasan,
Luyu Wang, Wojciech Stokowiec, and Fabio Viola. The DeepMind JAX Ecosystem, 2020. URL
http://github.com/deepmind.
Atilim Gunes Baydin, Barak A Pearlmutter, Alexey Andreyevich Radul, and Jeffrey Mark Siskind.
Automatic differentiation in machine learning: a survey. Journal of Marchine Learning Research,
18:1–43, 2018.
Marc G. Bellemare, Yavar Naddaf, Joel Veness, and Michael Bowling. The arcade learning envi-
ronment: An evaluation platform for general agents. Journal of Artiﬁcial Intelligence Research,
Vol. 47:253–279, 2012. cite arxiv:1207.4708.
Yoshua Bengio, Nicholas L´eonard, and Aaron C. Courville. Estimating or propagating gradients
through stochastic neurons for conditional computation. ArXiv, abs/1308.3432, 2013.
Aart Bik, Penporn Koanantakool, Tatiana Shpeisman, Nicolas Vasilache, Bixia Zheng, and Fredrik
Kjolstad. Compiler support for sparse tensor computations in mlir. ACM Trans. Archit. Code
Optim., 19(4), sep 2022. ISSN 1544-3566. doi: 10.1145/3544559. URL https://doi.org/
10.1145/3544559.
Aart J.C. Bik. Compiler Support for Sparse Matrix Computations. PhD thesis, Department of
Computer Science, Leiden University, 1996. ISBN 90-9009442-3.
Davis W. Blalock, Jose Javier Gonzalez Ortiz, Jonathan Frankle, and John V. Guttag. What is the
state of neural network pruning? ArXiv, abs/2003.03033, 2020.
Mathieu Blondel, Quentin Berthet, Marco Cuturi, Roy Frostig, Stephan Hoyer, Felipe Llinares-
L´opez, Fabian Pedregosa, and Jean-Philippe Vert. Efﬁcient and modular implicit differentiation.
arXiv preprint arXiv:2105.15183, 2021.
James Bradbury, Roy Frostig, Peter Hawkins, Matthew James Johnson, Chris Leary, Dougal
Maclaurin, George Necula, Adam Paszke, Jake VanderPlas, Skye Wanderman-Milne, and Qiao
Zhang. JAX: composable transformations of Python+NumPy programs, 2018. URL http:
//github.com/google/jax.
Aydın Buluc¸ and John R Gilbert. The combinatorial blas: Design, implementation, and applications.
The International Journal of High Performance Computing Applications, 25(4):496–509, 2011.
Pablo Samuel Castro, Subhodeep Moitra, Carles Gelada, Saurabh Kumar, and Marc G. Bellemare.
Dopamine: A Research Framework for Deep Reinforcement Learning.
2018.
URL http:
//arxiv.org/abs/1812.06110.
Stephen Chou, Fredrik Kjolstad, and Saman Amarasinghe. Format abstraction for sparse tensor
algebra compilers. Proc. ACM Program. Lang., 2(OOPSLA):123:1–123:30, October 2018. ISSN
2475-1421. doi: 10.1145/3276493. URL http://doi.acm.org/10.1145/3276493.
Gregory Cohen, Saeed Afshar, Jonathan Tapson, and Andr´e van Schaik. EMNIST: an extension
of MNIST to handwritten letters. CoRR, abs/1702.05373, 2017. URL http://arxiv.org/
abs/1702.05373.
W. Dabney, M. Rowland, Marc G. Bellemare, and R. Munos. Distributional reinforcement learning
with quantile regression. In AAAI, 2018a.
Will Dabney, Georg Ostrovski, David Silver, and Remi Munos.
Implicit quantile networks for
distributional reinforcement learning. In Proceedings of the 35th International Conference on
Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 1096–1105.
PMLR, 2018b.
Steven Dalton, Nathan Bell, Luke Olson, and Michael Garland. Cusp: Generic parallel algorithms
for sparse matrix and graph computations, 2014. URL http://cusplibrary.github.
io/. Version 0.5.0.
8

Published as a conference paper at ICLR 2023
Timothy A Davis. Algorithm 1000: Suitesparse: Graphblas: Graph algorithms in the language of
sparse linear algebra. ACM Transactions on Mathematical Software (TOMS), 45(4):1–25, 2019.
Mostafa Dehghani, Alexey Gritsenko, Anurag Arnab, Matthias Minderer, and Yi Tay. Scenic: A jax
library for computer vision research and beyond. In Proceedings of the IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR), pp. 21393–21398, 2022.
Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas
Unterthiner, Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, Jakob Uszko-
reit, and Neil Houlsby.
An image is worth 16x16 words: Transformers for image recogni-
tion at scale. In International Conference on Learning Representations, 2021. URL https:
//openreview.net/forum?id=YicbFdNTTy.
Erich Elsen, Marat Dukhan, Trevor Gale, and Karen Simonyan. Fast sparse convnets. In Proceedings
of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), June 2020.
Utku Evci, Trevor Gale, Jacob Menick, Pablo Samuel Castro, and Erich Elsen. Rigging the lottery:
Making all tickets winners. In International Conference on Machine Learning, pp. 2943–2952.
PMLR, 2020.
Jonathan Frankle.
OpenLTH, 2019.
URL https://github.com/facebookresearch/
open_lth.
Jonathan Frankle and Michael Carbin. The lottery ticket hypothesis: Finding sparse, trainable neural
networks. arXiv preprint arXiv:1803.03635, 2018.
Trevor Gale, Erich Elsen, and Sara Hooker. The state of sparsity in deep neural networks. arXiv
preprint arXiv:1902.09574, 2019.
Trevor Gale, Matei Zaharia, Cliff Young, and Erich Elsen. Sparse gpu kernels for deep learning.
In SC20: International Conference for High Performance Computing, Networking, Storage and
Analysis, pp. 1–14. IEEE, 2020.
Trevor Gale, Deepak Narayanan, Cliff Young, and Matei Zaharia. Megablocks: Efﬁcient sparse
training with mixture-of-experts, 2022. URL https://arxiv.org/abs/2211.15841.
Jonathan Godwin, Thomas Keck, Peter Battaglia, Victor Bapst, Thomas Kipf, Yujia Li, Kimberly
Stachenfeld, Petar Veliˇckovi´c, and Alvaro Sanchez-Gonzalez. Jraph: A library for graph neural
networks in jax., 2020. URL http://github.com/deepmind/jraph.
Laura Graesser, Utku Evci, Erich Elsen, and Pablo Samuel Castro. The state of sparse training
in deep reinforcement learning.
In Kamalika Chaudhuri, Stefanie Jegelka, Le Song, Csaba
Szepesvari, Gang Niu, and Sivan Sabato (eds.), Proceedings of the 39th International Confer-
ence on Machine Learning, volume 162 of Proceedings of Machine Learning Research, pp.
7766–7792. PMLR, 17–23 Jul 2022. URL https://proceedings.mlr.press/v162/
graesser22a.html.
Scott Gray, Alec Radford, and Diederik P Kingma. Gpu kernels for block-sparse weights. arXiv
preprint arXiv:1711.09224, 3:2, 2017.
Ga¨el Guennebaud, Benoit Jacob, et al. Eigen. URl: http://eigen. tuxfamily. org, 3, 2010.
Jonathan Heek, Anselm Levskaya, Avital Oliver, Marvin Ritter, Bertrand Rondepierre, Andreas
Steiner, and Marc van Zee. Flax: A neural network library and ecosystem for jax. Version 0.3, 3:
14–26, 2020.
Alexander Heinecke, Greg Henry, Maxwell Hutchinson, and Hans Pabst. Libxsmm: accelerating
small matrix multiplications by runtime code generation. In SC’16: Proceedings of the Inter-
national Conference for High Performance Computing, Networking, Storage and Analysis, pp.
981–991. IEEE, 2016.
Matteo Hessel, Joseph Modayil, H. V. Hasselt, T. Schaul, Georg Ostrovski, Will Dabney, Dan Hor-
gan, Bilal Piot, M. G. Azar, and D. Silver. Rainbow: Combining improvements in deep reinforce-
ment learning. In AAAI, 2018.
9

Published as a conference paper at ICLR 2023
Torsten Hoeﬂer, Dan Alistarh, Tal Ben-Nun, Nikoli Dryden, and Alexandra Peste. Sparsity in deep
learning: Pruning and growth for efﬁcient inference and training in neural networks. Journal
of Machine Learning Research, 2021. URL http://jmlr.org/papers/v22/21-0366.
html.
Matthew W. Hoffman, Bobak Shahriari, John Aslanides, Gabriel Barth-Maron, Nikola Momchev,
Danila Sinopalnikov, Piotr Sta´nczyk, Sabela Ramos, Anton Raichuk, Damien Vincent, L´eonard
Hussenot, Robert Dadashi, Gabriel Dulac-Arnold, Manu Orsini, Alexis Jacq, Johan Ferret, Nino
Vieillard, Seyed Kamyar Seyed Ghasemipour, Sertan Girgin, Olivier Pietquin, Feryal Behbahani,
Tamara Norman, Abbas Abdolmaleki, Albin Cassirer, Fan Yang, Kate Baumli, Sarah Henderson,
Abe Friesen, Ruba Haroun, Alex Novikov, Sergio G´omez Colmenarejo, Serkan Cabi, Caglar
Gulcehre, Tom Le Paine, Srivatsan Srinivasan, Andrew Cowie, Ziyu Wang, Bilal Piot, and Nando
de Freitas. Acme: A research framework for distributed reinforcement learning. arXiv preprint
arXiv:2006.00979, 2020. URL https://arxiv.org/abs/2006.00979.
Itay Hubara, Brian Chmiel, Moshe Island, Ron Banner, Joseph Naor, and Daniel Soudry. Accel-
erated sparse neural training: A provable and efﬁcient method to ﬁnd n: m transposable masks.
Advances in Neural Information Processing Systems, 34:21099–21111, 2021.
Andrei Ivanov, Nikoli Dryden, and Torsten Hoeﬂer. Project title, 2022. URL https://github.
com/spcl/sten.
Nal Kalchbrenner, Erich Elsen, Karen Simonyan, Seb Noury, Norman Casagrande, Edward Lock-
hart, Florian Stimberg, Aaron Oord, Sander Dieleman, and Koray Kavukcuoglu. Efﬁcient neural
audio synthesis. In International Conference on Machine Learning (ICML), 2018.
Sheng-Chun Kao. JAX-Pruning: A JAX implementation of structure and unstructure pruning, 2022.
URL https://github.com/felix0901/jax_pruning.
Jeremy Kepner, David Bader, Aydın Buluc¸, John Gilbert, Timothy Mattson, and Henning Meyer-
henke. Graphs, matrices, and the graphblas: Seven good reasons. Procedia Computer Science,
51:2453–2462, 2015.
Diederik P Kingma and Jimmy Ba. Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980, 2014.
Diederik P. Kingma and Jimmy Ba.
Adam: A method for stochastic optimization.
In Yoshua
Bengio and Yann LeCun (eds.), 3rd International Conference on Learning Representations, ICLR
2015, San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings, 2015. URL http:
//arxiv.org/abs/1412.6980.
Fredrik Kjolstad, Shoaib Kamil, Stephen Chou, David Lugato, and Saman Amarasinghe. The tensor
algebra compiler. Proc. ACM Program. Lang., 1(OOPSLA):77:1–77:29, October 2017. ISSN
2475-1421. doi: 10.1145/3133901. URL http://doi.acm.org/10.1145/3133901.
Fredrik Kjolstad, Peter Ahrens, Shoaib Kamil, and Saman Amarasinghe. Tensor algebra compila-
tion with workspaces. Proceedings of the 2019 IEEE/ACM International Symposium on Code
Generation and Optimization, pp. 180–192, 2019. URL http://dl.acm.org/citation.
cfm?id=3314872.3314894.
Lukasz Lew, Vlad Feinberg, Shivani Agrawal, Jihwan Lee, Jonathan Malmaud, Lisa Wang, Pouya
Dormiani, and Reiner Pope. Aqt: Accurate quantized training), 2022. URL http://github.
com/google/aqt.
Hao Li, Asim Kadav, Igor Durdanovic, Hanan Samet, and Hans Peter Graf. Pruning ﬁlters for
efﬁcient convnets. arXiv preprint arXiv:1608.08710, 2016.
Zhuohan Li, Eric Wallace, Sheng Shen, Kevin Lin, Kurt Keutzer, Dan Klein, and Joey Gonzalez.
Train big, then compress: Rethinking model size for efﬁcient training and inference of trans-
formers. In Proceedings of the 37th International Conference on Machine Learning, 2020. URL
https://proceedings.mlr.press/v119/li20m.html.
Shiwei Liu and Zhangyang Wang. Ten lessons we have learned in the new”sparseland”: A short
handbook for sparse neural network researchers. 2023.
10

Published as a conference paper at ICLR 2023
Huizi Mao, Song Han, Jeff Pool, Wenshuo Li, Xingyu Liu, Yu Wang, and William J Dally.
Exploring the regularity of sparse structure in convolutional neural networks.
arXiv preprint
arXiv:1705.08922, 2017.
Asit Mishra, Jorge Albericio Latorre, Jeff Pool, Darko Stosic, Dusan Stosic, Ganesh Venkatesh,
Chong Yu, and Paulius Micikevicius. Accelerating sparse deep neural networks. arXiv preprint
arXiv:2104.08378, 2021.
Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Andrei A. Rusu, Joel Veness, Marc G. Belle-
mare, Alex Graves, Martin Riedmiller, Andreas K. Fidjeland, Georg Ostrovski, Stig Petersen,
Charles Beattie, Amir Sadik, Ioannis Antonoglou, Helen King, Dharshan Kumaran, Daan Wier-
stra, Shane Legg, and Demis Hassabis. Human-level control through deep reinforcement learning.
Nature, 518(7540):529–533, February 2015.
Decebal Constantin Mocanu, Elena Mocanu, Peter Stone, Phuong H Nguyen, Madeleine Gibescu,
and Antonio Liotta. Scalable training of artiﬁcial neural networks with adaptive sparse connectiv-
ity inspired by network science. Nature Communications, 2018. URL http://www.nature.
com/articles/s41467-018-04316-3.
Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, and Jan Kautz.
Pruning Convolu-
tional Neural Networks for Resource Efﬁcient Transfer Learning. ArXiv, 2016. URL https:
//arxiv.org/abs/1611.06440.
Sharan Narang, Eric Undersander, and Gregory Diamos. Block-sparse recurrent neural networks.
arXiv preprint arXiv:1711.02782, 2017.
Maxim Naumov, L Chien, Philippe Vandermersch, and Ujval Kapasi. Cusparse library. In GPU
Technology Conference, 2010.
NVIDIA.
cusparselt: A high-performance cuda library for sparse matrix-matrix multiplication,
2021. URL https://docs.nvidia.com/cuda/cusparselt/index.html.
Michela Paganini.
Pytorch pruning tutorial,
2021.
URL https://pytorch.org/
tutorials/intermediate/pruning_tutorial.html.
Jeff Pool and Chong Yu. Channel permutations for n: m sparsity. Advances in Neural Information
Processing Systems, 34:13316–13327, 2021.
Jeff Pool, Abhishek Sawarkar, and Jay Rodge. Accelerating inference with sparsity using the nvidia
ampere architecture and nvidia tensorrt. NVIDIA Developer Technical Blog, https://developer.
nvidia. com/blog/accelerating-inference-with-sparsityusing-ampere-and-tensorrt, 2021.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi
Zhou, Wei Li, and Peter J. Liu. Exploring the limits of transfer learning with a uniﬁed text-to-
text transformer. Journal of Machine Learning Research, 21(140):1–67, 2020. URL http:
//jmlr.org/papers/v21/20-074.html.
Sashank J. Reddi, Zachary Charles, Manzil Zaheer, Zachary Garrett, Keith Rush, Jakub Koneˇcn´y,
Sanjiv Kumar, and H. Brendan McMahan.
Adaptive federated optimization.
CoRR,
abs/2003.00295, 2020. URL https://arxiv.org/abs/2003.00295.
Jae Hun Ro, Ananda Theertha Suresh, and Ke Wu. FedJAX: Federated learning simulation with
JAX. arXiv preprint arXiv:2108.02117, 2021.
Adam Roberts, Hyung Won Chung, Anselm Levskaya, Gaurav Mishra, James Bradbury, Daniel
Andor, Sharan Narang, Brian Lester, Colin Gaffney, Afroz Mohiuddin, Curtis Hawthorne, Aitor
Lewkowycz, Alex Salcianu, Marc van Zee, Jacob Austin, Sebastian Goodman, Livio Baldini
Soares, Haitang Hu, Sasha Tsvyashchenko, Aakanksha Chowdhery, Jasmijn Bastings, Jannis Bu-
lian, Xavier Garcia, Jianmo Ni, Andrew Chen, Kathleen Kenealy, Jonathan H. Clark, Stephan
Lee, Dan Garrette, James Lee-Thorp, Colin Raffel, Noam Shazeer, Marvin Ritter, Maarten
Bosma, Alexandre Passos, Jeremy Maitin-Shepard, Noah Fiedel, Mark Omernick, Brennan
Saeta, Ryan Sepassi, Alexander Spiridonov, Joshua Newlan, and Andrea Gesmundo. Scaling
up models and data with t5x and seqio.
arXiv preprint arXiv:2203.17189, 2022.
URL
https://arxiv.org/abs/2203.17189.
11

Published as a conference paper at ICLR 2023
Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng
Huang, Andrej Karpathy, Aditya Khosla, Michael Bernstein, Alexander C. Berg, and Li Fei-
Fei. Imagenet large scale visual recognition challenge. International Journal of Computer Vision
(IJCV), 2015.
Amit Sabne. Xla : Compiling machine learning for peak performance, 2020.
Wei Sun, Aojun Zhou, Sander Stuijk, Rob Wijnhoven, Andrew O Nelson, Henk Corporaal, et al.
Dominosearch: Find layer-wise ﬁne-grained n: M sparse schemes from dense neural networks.
Advances in neural information processing systems, 34:20721–20732, 2021.
Yujin Tang, Yingtao Tian, and David Ha. Evojax: Hardware-accelerated neuroevolution. arXiv
preprint arXiv:2202.05008, 2022.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez,
Ł ukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances in Neural Infor-
mation Processing Systems, 2017. URL https://proceedings.neurips.cc/paper_
files/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf.
Endong Wang, Qing Zhang, Bo Shen, Guangyong Zhang, Xiaowei Lu, Qing Wu, and Yajuan Wang.
Intel math kernel library. In High-Performance Computing on the Intel® Xeon Phi™, pp. 167–
188. Springer, 2014.
Carl Yang, Aydin Buluc, and John D Owens. Graphblast: A high-performance linear algebra-based
graph framework on the gpu. arXiv preprint arXiv:1908.01407, 2019.
Aojun Zhou, Yukun Ma, Junnan Zhu, Jianbo Liu, Zhijie Zhang, Kun Yuan, Wenxiu Sun, and Hong-
sheng Li. Learning n: M ﬁne-grained structured sparse neural networks from scratch. arXiv
preprint arXiv:2102.04010, 2021.
Michael Zhu and Suyog Gupta. To prune, or not to prune: Exploring the efﬁcacy of pruning for
model compression. In International Conference on Learning Representations Workshop, 2018.
URL https://arxiv.org/abs/1710.01878.
AUTHOR CONTRIBUTIONS
Joo Hyung Lee
designed and implemented the initial library together with Utku.
Wonpyo Park
implemented Straight-Through-Estimator for pruning algorithms. Worked on the
inference speciﬁc conversion and reviewed code.
Nicole Mitchell
integrated JaxPruner with FedJAX and ran experiments. Helped with testing the
code. Contributed to writing and editing the manuscript.
Jonathan Pilault
implemented sparse T5x library and helped with the experiments and writing.
Johan Obando-Ceron
integrated JaxPruner with Dopamine and helped with the experiments and
writing.
Han-Byul Kim
implemented the initial version for the sparsity preserving quantization example.
Namhoon Lee
helped with reviewing and writing the manuscript.
Elias Frantar
added the parallelization example to the quick start colab. Tested the code and ﬁxed
bugs.
Yun Long
helped with the jax2tf integration, Tensorﬂow N:M weight/activation sparsity imple-
mentation.
Amir Yazdanbakhsh
implemented N:M sparsity support.
12

Published as a conference paper at ICLR 2023
Shivani Agrawal
helped with the quantization-aware training example.
Suvinay Subramanian
helped with documentation and testing the library/colabs.
Xin Wang
helped with t5x integration.
Sheng-Chun Kao
implemented global sparsity.
Xingyao Zhang
conducted investigation on converting pruned JAX model parameters to use
jax.sparse.BCOO with Utku.
Trevor Gale
tested and open-sourced the Colabs.
Aart Bik
works on sparse compiler and sparse JAX. Contributed to the related work section in
paper.
Woohyun Han
implemented block sparsity.
Milen Ferev
implemented the tﬂite conversion colab.
Zhonglin Han
contributed Pax integration example (WIP).
Hong-Seok Kim
provided guidance and helped with the writing.
Yann Dauphin
implemented CraM and SAM examples.
Karolina Dziugaite
helped with direction and writing of the paper.
Pablo Samuel Castro
provided some high-level guidance, helped with Dopamine integration and
documentation.
Utku Evci
proposed the project, implemented the initial prototype, wrote the initial version of
the library together with Joo Hyung. Wrote the initial versions of the colabs and did the scenic
integration. Led the organization and direction of the project and provided guidance.
13

