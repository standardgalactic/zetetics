A Unified Framework to Enforce, Discover, and Promote
Symmetry in Machine Learning
Samuel E. Otto
ottoncdr@uw.edu
AI Institute in Dynamic Systems
University of Washington
Seattle, WA 98195-4322, USA
Nicholas Zolman
nzolman@uw.edu
AI Institute in Dynamic Systems
Mechanical Engineering
University of Washington
Seattle, WA 98195-4322, USA
J. Nathan Kutz
kutz@uw.edu
AI Institute in Dynamic Systems
Applied Mathematics
University of Washington
Seattle, WA 98195-4322, USA
Steven L. Brunton
sbrunton@uw.edu
AI Institute in Dynamic Systems
Mechanical Engineering
University of Washington
Seattle, WA 98195-4322, USA
Abstract
Symmetry is present throughout nature and continues to play an increasingly central role in physics
and machine learning. Fundamental symmetries, such as Poincaré invariance, allow physical laws
discovered in laboratories on Earth to be extrapolated to the farthest reaches of the universe.
Symmetry is essential to achieving this extrapolatory power in machine learning applications. For
example, translation invariance in image classification allows models with fewer parameters, such
as convolutional neural networks, to be trained on smaller data sets and achieve state-of-the-art
performance. In this paper, we provide a unifying theoretical and methodological framework for
incorporating symmetry into machine learning models in three ways: 1. enforcing known symmetry
when training a model; 2. discovering unknown symmetries of a given model or data set; and
3. promoting symmetry during training by learning a model that breaks symmetries within a
user-specified group of candidates when there is sufficient evidence in the data. We show that
these tasks can be cast within a common mathematical framework whose central object is the Lie
derivative associated with fiber-linear Lie group actions on vector bundles. We extend and unify
several existing results by showing that enforcing and discovering symmetry are linear-algebraic tasks
that are dual with respect to the bilinear structure of the Lie derivative. We also propose a novel
way to promote symmetry by introducing a class of convex regularization functions based on the Lie
derivative and nuclear norm relaxation to penalize symmetry breaking during training of machine
learning models. We explain how these ideas can be applied to a wide range of machine learning
models including basis function regression, dynamical systems discovery, multilayer perceptrons,
and neural networks acting on spatial fields such as images.
Keywords: Symmetries, machine learning, Lie groups, manifolds, invariance, equivariance, neural
networks, deep learning
1
arXiv:2311.00212v1  [cs.LG]  1 Nov 2023

Contents
1
Introduction
3
2
Related work
5
2.1
Enforcing symmetry
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5
2.2
Discovering symmetry . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5
2.3
Promoting symmetry . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
6
2.4
Additional approaches and applications . . . . . . . . . . . . . . . . . . . . . . . . . .
6
3
Elementary theory of matrix Lie group actions
7
3.1
Matrix Lie groups and subgroups . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7
3.2
Group representations, actions, and infinitesimal generators . . . . . . . . . . . . . .
8
4
Fundamental operators for studying symmetry
11
5
Enforcing symmetry with linear constraints
13
5.1
Multilayer perceptrons . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
13
5.2
Neural networks acting on fields . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
13
6
Discovering symmetry by computing nullspaces
15
6.1
Symmetries of submanifolds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
16
6.2
Symmetries of functions as symmetries of submanifolds . . . . . . . . . . . . . . . . .
17
6.3
Symmetries and conservation laws of dynamical systems . . . . . . . . . . . . . . . .
18
7
Promoting symmetry with convex penalties
20
7.1
Discrete symmetries
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
20
7.2
Continuous symmetries . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
21
7.3
Promoting symmetry in basis function regression . . . . . . . . . . . . . . . . . . . .
21
7.4
Promoting symmetry in neural networks . . . . . . . . . . . . . . . . . . . . . . . . .
22
8
Discretizing the operators
23
8.1
Numerical quadrature and Monte-Carlo
. . . . . . . . . . . . . . . . . . . . . . . . .
24
8.2
Subspaces of polynomials
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
24
9
Generalization to sections of vector bundles
25
9.1
Vector fields . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
28
9.2
Tensor fields . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
29
9.3
Hamiltonian dynamics . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
30
9.4
Equivariant integral operators . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
32
10 Invariant submanifolds and tangency
34
10.1 Symmetry of submanifolds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
34
10.2 The Lie derivative as a projection . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
35
11 Conclusion
36
A Proofs of minor results
43
B Proof of Proposition 17
44
C Proof of Proposition 20
45
D Proof of Theorem 21
48
E Proof of Theorem 22
49
F Proof of Theorem 30
49
G Proof of Theorem 31
51
H Proof of Theorem 34
52
2

1 Introduction
Symmetry is present throughout nature, and according to David Gross (1996) the discovery of
fundamental symmetries has played an increasingly central role in physics since the beginning of the
20th century. He asserts that
“Einstein’s great advance in 1905 was to put symmetry first, to regard the symmetry principle as
the primary feature of nature that constrains the allowable dynamical laws.”
According to Einstein’s special theory of relativity, physical laws including those of electromagnetism
and quantum mechanics are Poincaré-invariant, meaning that after predictable transformations
(actions of the Poincaré group), these laws can be applied in any non-accelerating reference frame,
anywhere in the universe, at all times. Specifically these transformations form a ten-parameter
group including four translations of space-time, three rotations of space, and three shifts or “boosts”
in velocity. For small boosts of velocity, these transformations become the Galilean symmetries of
classical mechanics. Similarly, the theorems of Euclidean geometry are unchanged after arbitrary
translations, rotations, and reflections, comprising the Euclidean group. In fluid mechanics, the
conformal (angle-preserving) symmetry of Laplace’s equation is used to reduce the study of idealized
flows in complicated geometries to canonical flows in simple domains.
In dynamical systems,
the celebrated theorem of Noether (1918) establishes a correspondence between symmetries and
conservation laws, an idea which has become a central pillar of mechanics (Abraham and Marsden,
2008). These examples illustrate the diversity of symmetry groups and their physical applications.
More importantly, they illustrate how symmetric models and theories in physics automatically
extrapolate in explainable ways to environments beyond the available data.
In machine learning, models that exploit symmetry can be trained with less data and use fewer
parameters compared to their asymmetric counterparts. Early examples include augmenting data
with known transformations (see Shorten and Khoshgoftaar (2019); Van Dyk and Meng (2001)) or
using convolutional neural networks (CNNs) to achieve translation invariance for image processing
tasks (see Fukushima (1980); LeCun et al. (1989)). More recently, equivariant neural networks
respecting Euclidean symmetries have achieved state-of-the-art performance for predicting potentials
in molecular dynamics Batzner et al. (2022). As with physical laws, symmetries and invariances allow
machine learning models to extrapolate beyond the training data, and achieve high performance
with fewer modeling parameters.
However, many problems are only weakly symmetric. Gravity, friction, and other external forces
can cause some or all of the Poincaré or Galilean symmetries to be broken. Interactions between
particles can be viewed as breaking symmetries possessed by non-interacting particles. Written
characters have translation and scaling symmetry, but not rotation (cf. 6 and 9, d and p, N and Z) or
reflection (cf. b and d, b and p). One of the main contributions of this work is to propose a method
of enforcing a new principle of parsimony in machine learning. This principal of parsimony by
maximal symmetry states that a model should break a symmetry within a set of physically reasonable
transformations (such as Poincaré, Galilean, Euclidean, or conformal symmetry) only when there is
sufficient evidence in the data.
In this paper, we provide a unifying theoretical and methodological framework for incorporating
symmetry into machine learning models in the following three ways:
Task 1. Enforce. Train a model with known symmetry.
Task 2. Discover. Identify the symmetries of a given model or data set.
Task 3. Promote. Train a model with as many symmetries as possible (from among candidates),
breaking symmetries only when there is sufficient evidence in the data.
3

While these tasks have been studied to varying extents separately, we show how they can
be situated within a common mathematical framework whose central object is the Lie derivative
associated with fiber-linear Lie group actions on vector bundles. As a special case, the Lie derivative
recovers the linear constraints derived by Finzi et al. (2021) for weights in equivariant multilayer
perceptrons. In full generality, we show that known symmetries can be enforced as linear constraints
derived from Lie derivatives for a large class of problems including learning vector and tensor fields
on manifolds as well as learning equivariant integral operators acting on such fields. For example the
kernels of “steerable” CNNs developed by Weiler et al. (2018); Weiler and Cesa (2019) are constructed
to automatically satisfy these constraints for the groups SO(3) (rotations in three dimensions) and
SE(2) (rotations and translations in two dimensions). We show how analogous steerable networks
for other groups, such as subgroups of SE(n), can be constructed by numerically enforcing linear
constraints derived from the Lie derivative on integral kernels defining each layer. Symmetries,
conservation laws, and symplectic structure can also be enforced when learning dynamical systems
via linear constraints on the vector field. Again these constraints come from the Lie derivative and
can be incorporated into neural network architectures and basis function regression models such as
Sparse Identification of Nonlinear Dynamics (SINDy) (Brunton et al., 2016).
Moskalev et al. (2022) identifies the connected subgroup of symmetries of a trained neural network
by computing the nullspace of a linear operator. Likewise, Kaiser et al. (2018, 2021) recovers the
symmetries and conservation laws of a dynamical system by computing the nullspace of a different
linear operator. We observe that these operators and others whose nullspaces encode the symmetries
of more general models can be derived directly from the Lie derivative in a manner dual to the
construction of operators used to enforce symmetry. Specifically, the nullspaces of the operators we
construct reveal the largest connected subgroups of symmetries for enormous classes of models. This
extends work by Gruver et al. (2022) using the Lie derivative to test whether a trained neural network
is equivariant with respect to a given one-parameter group, e.g., rotation of images. Generalizing
the ideas in Cahill et al. (2023), we also show that the symmetries of point clouds approximating
underlying submanifolds can be recovered by computing the nullspaces of associated linear operators.
This allows for the unsupervised mining of data for hidden symmetries.
The idea of relaxed symmetry has been introduced recently by Wang et al. (2022), along
with architecture-specific symmetry-promoting regularization functions involving sums or integrals
over the candidate group of transformations. The Augerino method introduced by Benton et al.
(2020) uses regularization to promote equivariance with respect to a larger collection of candidate
transformations. Promoting physical constraints through the loss function is also a core concept
of Physics-Informed Neural Networks (PINNs) introduced by Raissi et al. (2019). Our approach
to the third task (promoting symmetry) is to introduce a unified and broadly applicable class of
convex regularization functions based on the Lie derivative to penalize symmetry breaking during
training of machine learning models. As we describe above, the Lie derivative yields an operator
whose nullspace corresponds to the symmetries of a given model. Hence, the lower the rank of this
operator, the more symmetric the model is. The nuclear norm has been used extensively as a convex
relaxation of the rank with favorable theoretical properties for compressed sensing and low-rank
matrix recovery (Recht et al., 2010; Gross, 2011), as well as in robust PCA (Candès et al., 2011;
Bouwmans et al., 2018). Penalizing the nuclear norm of our symmetry-encoding operator yields a
convex regularization function that can be added to the loss when training machine learning models,
including basis function regression and neural networks. Likewise, we use a nuclear norm penalty
to promote conservation laws and Hamiltonicity with respect to candidate symplectic structures
when fitting dynamical systems to data. This lets us train the model and enforce data-consistent
symmetries simultaneously.
4

2 Related work
2.1 Enforcing symmetry
Data-augmentation, as reviewed by Shorten and Khoshgoftaar (2019); Van Dyk and Meng (2001), is
one of the simplest ways to incorporate known symmetry into machine learning models. Usually
this entails training a neural network architecture on training data to which known transformations
have been applied. The theoretical foundations of these methods are explored by Chen et al. (2020).
Data-augmentation has also been used by Benton et al. (2020) to construct equivariant neural
networks by averaging the network’s output over transformations applied to the data.
Symmetry can also be enforced directly on the machine learning architecture. For example,
Convolutional Neural Networks (CNNs), introduced by Fukushima (1980) and popularized by LeCun
et al. (1989), achieve translational equivariance by employing convolutional filters with trainable
kernels in each layer. CNNs have been generalized to provide equivariance with respect to symmetry
groups other than translation. Group-Equivariant CNNs (G-CNNs) (Cohen and Welling, 2016)
provide equivariance with respect to arbitrary discrete groups generated by translations, reflections,
and rotations. Rotational equivariance can be enforced on three-dimensional scalar, vector, or tensor
fields using the 3D Steerable CNNs developed by Weiler et al. (2018). Spherical CNNs Cohen et al.
(2018); Esteves et al. (2018) allow for rotation-equivariant maps to be learned for fields (such as
projected images of 3D objects) on spheres. Essentially any group equivariant linear map (defining a
layer of an equivariant neural network) acting fields can be described by group convolution (Kondor
and Trivedi, 2018; Cohen et al., 2019), with the spaces of appropriate convolution kernel characterized
by Cohen et al. (2019). Finzi et al. (2020) provides a practical way to construct convolutional layers
that are equivariant with respect to arbitrary Lie groups and for general data types. For dynamical
systems, Marsden and Ratiu (1999); Rowley et al. (2003); Abraham and Marsden (2008) describe
techniques for symmetry reduction of the original problem to a quotient space where the known
symmetry group has been factored out. Related approaches have been used by Peitz et al. (2023);
Steyert (2022) to approximate Koopman operators for symmetric dynamical systems (see Koopman
(1931); Mezić (2005); Mauroy et al. (2020); Otto and Rowley (2021); Brunton et al. (2022)).
A general method for constructing equivariant neural networks is introduced by Finzi et al. (2021),
and relies on the observation that equivariance can be enforced through a set of linear constraints.
For graph neural networks, Maron et al. (2018) characterizes the subspaces of linear layers satisfying
permutation equivariance. Similarly, Ahmadi and Khadir (2020) shows that discrete symmetries and
other types of side information can be enforced via linear or convex constraints in learning problems
for dynamical systems. Our work builds on the results of Finzi et al. (2021), Weiler et al. (2018),
Cohen et al. (2019), and Ahmadi and Khadir (2020) by showing that equivariance can be enforced in
a systematic and unified way via linear constraints for large classes of functions and neural networks.
2.2 Discovering symmetry
Early work by Rao and Ruderman (1999); Miao and Rao (2007) used nonlinear optimization to learn
infinitesimal generators describing transformations between images. Later, it was recognized by Cahill
et al. (2023) that linear algebraic methods could be used to uncover the generators of continuous
linear symmetries of arbitrary point clouds in Euclidean space. Similarly, Kaiser et al. (2018) and
Moskalev et al. (2022) show how conserved quantities of dynamical systems and invariances of trained
neural networks can be revealed by computing the nullspaces of associated linear operators. We
connect these linear-algebraic methods to the Lie derivative, and provide generalizations to nonlinear
group actions on manifolds. The Lie derivative has been used by Gruver et al. (2022) to quantify the
extent to which a trained network is equivariant with respect to a given one-parameter subgroup of
5

transformations. Our results show how the Lie derivative can reveal the entire connected subgroup
of symmetries of a trained model via symmetric eigendecomposition.
More sophisticated nonlinear optimization techniques use Generative Adversarial Networks
(GANs) to learn the transformations that leave a data distribution unchanged. These methods
include SymmetryGAN developed by Desai et al. (2022) and LieGAN developed by Yang et al.
(2023). In contrast, our methods for detecting symmetry are entirely linear-algebraic.
Liu and Tegmark (2022) discover hidden symmetries by optimizing nonlinear transformations
into spaces where candidate symmetries hold. Similar to our approach for promoting symmetry,
they use a cost function to measure whether a given symmetry holds. In contrast, our regularization
functions enable subgroups of candidate symmetry groups to be identified.
2.3 Promoting symmetry
Biasing a network towards increased symmetry is discussed by Wang et al. (2022), along with
architecture-specific methods, including regularization functions involving summations or integrals
over the candidate group of symmetries. While our regularization functions resemble these for
discrete groups, we use a radically different regularization for continuous Lie groups. By leveraging
the Lie algebra, our regularization functions eliminate the need to numerically integrate complicated
functions over the group — a task that is already prohibitive for the 10-dimensional non-compact
group of Galilean symmetries in classical mechanics.
Automated data augmentation techniques introduced by Cubuk et al. (2019); Hataya et al.
(2020); Benton et al. (2020) are another class of methods that arguably promote symmetry. These
techniques optimize the distribution of transformations applied to augment the data during training.
For example “Augerino” is an elegant method developed by Benton et al. (2020) which averages an
arbitrary network’s output over the augmentation distribution and relies on regularization to prevent
the distribution of transformations from becoming concentrated near the identity. In essence, the
regularization biases the averaged network towards increased symmetry.
In contrast, our regularization functions promote symmetry on an architectural level for the
original network. This eliminates the need to perform averaging, which grows more costly for larger
collections of symmetries. While a distribution over symmetries can be useful for learning interesting
partial symmetries (e.g. 6 stays 6 for small rotations, before turning into 9), as is done by Benton
et al. (2020), it is not clear how to use a continuous distribution over transformations to identify
lower-dimensional subgroups which have measure zero. On the other hand, our linear-algebraic
approach easily identifies and promotes symmetries in lower-dimensional connected subgroups.
2.4 Additional approaches and applications
There are several other approaches that incorporate various aspects of enforcing, discovering, and
promoting symmetries. For example, Baddoo et al. (2023) developed algorithms to enforce and
promote known symmetries in dynamic mode decomposition, through manifold constrained learning
and regularization, respectively.
Baddoo et al. (2023) also showed that discovering unknown
symmetries is a dual problem to enforcing symmetry. Exploiting symmetry has also been a central
theme in the reduced-order modeling of fluids for decades (Holmes et al., 2012). As machine learning
methods are becoming widely used to develop these models (Brunton et al., 2020), the themes of
enforcing and discovering symmetries in machine models are increasingly relevant. Known fluid
symmetries have been enforced in SINDy for fluid systems (Loiseau and Brunton, 2018) through linear
equality constraints; this approach was generalized to enforce more complex constraints (Champion
et al., 2020). Unknown symmetries were similarly uncovered for electroconvective flows (Guan et al.,
2021). Symmetry breaking is also important in many turbulent flows (Callaham et al., 2022).
6

3 Elementary theory of matrix Lie group actions
This section provides background and notation required to understand the main results of this
paper in the less abstract, but still remarkably useful setting of matrix Lie groups acting on vector
spaces. In Section 4 we use this theory to study the symmetries of smooth functions between vector
spaces. Such functions form the basic building blocks of many machine learning models such as
basis functions regression models, the layers of multilayer perceptrons, and the kernels of integral
operators acting on spatial fields such as images. We emphasize that this is not the most general
setting for our results, but we provide this section and simpler versions of our main Theorems in
Section 4 in order to make the presentation more accessible. We develop our main results in the
more general and abstract setting of fiber-linear Lie group actions on sections of vector bundles in
Section 9.
3.1 Matrix Lie groups and subgroups
Matrix Lie groups are ubiquitous in science and engineering. Some familiar examples include the
general linear group GL(n) consisting of all real, invertible, n × n matrices; the orthogonal group
O(n) =

Q ∈Rn×n : QT Q = I
	
;
(1)
and the special Euclidean group
SE(n) =
Q
b
0
1

: Q ∈Rn×n, b ∈Rn, QT Q = I, det(Q) = 1

,
(2)
which represents rotations and translations in an n-dimensional vector space. Observe that these sets
are closed under matrix multiplication and inversion, making them into (non-commutative) groups.
They are also smooth manifolds, which makes them Lie groups (Abraham et al., 1988). While Lie
groups need not consist of matrices (this general case is the setting in Section 9), we confine our
attention for now to “matrix Lie groups”, as they require less abstract machinery. Following Hall
(2015), a matrix Lie group G is defined as any closed subset of GL(n) with the property that G
contains the identity matrix and is closed under matrix multiplication and matrix inversion. All
matrix Lie groups are smooth, embedded submanifolds of GL(n) thanks to a result known as the
closed subgroup theorem (Corollary 3.45, also see Theorem 20.12 in Lee (2013)).
The most useful and profound property of a Lie group is the fact that it is almost entirely
characterized by an associated vector space called the Lie algebra. This allows global nonlinear
questions about the group — such as which elements leave a function unchanged — to be answered
using linear algebra. The Lie algebra, denoted Lie(G), of a matrix Lie group G is equal to the
tangent space TIG to the manifold G at the identity matrix I (Corollary 3.46). The identity element
of a group is usually denoted e for “einselement”, and we shall do this too. This real vector space is
closed under commutation, defined by the “Lie bracket”:
[X, Y ] = XY −Y X ∈Lie(G),
(3)
for every X, Y ∈Lie(G) (Theorem 3.20). For example, the Lie algebra of the orthogonal group O(n)
consists of all skew-symmetric matrices, and is denoted
o(n) =

S ∈Rn×n : S + ST = 0
	
.
(4)
7

The key tool relating global properties of a matrix Lie group to its Lie algebra is the matrix
exponential, defined by the absolutely convergent series
eA =
∞
X
k=0
1
k!Ak.
(5)
Restricting the matrix exponential to the Lie algebra yields the exponential map
exp : Lie(G) →G,
which provides a diffeomorphism of an open neighborhood of the origin 0 in Lie(G) and an open
neighborhood of the identity element e in G (Corollary 3.44). The connected component of G
containing the identity element is called the “identity component” of the Lie group and is denoted G0.
Any element in this component can be expressed as a finite product of exponentials (Corollary 3.47),
that is
G0 =

exp (X1) · · · exp (XN) : X1, . . . , XN ∈Lie(G), N = 1, 2, 3, . . .
	
.
(6)
The identity component is a normal subgroup of G (Proposition 1.10) and all of the other connected
components of G are diffeomorphic cosets of G0 (Proposition 7.15 in Lee (2013)), as we illustrate
in Figure 1. For example, the special Euclidean group SE(n) is connected, and thus equal to its
identity component. On the other hand, the orthogonal group O(n) has two components consisting of
orthogonal matrices Q whose determinants are 1 and −1. The identity component of the orthogonal
group is called the special orthogonal group and is denoted SO(n). The special orthogonal group is
compact. It is a general fact that when a Lie group is connected and compact, it is equal to the
image of the exponential map without the need to consider products of exponentials, see Tao (2011)
and Appendix C.1 of Lezcano-Casado and Martınez-Rubio (2019).
A subgroup H of a Lie group G is called a “Lie subgroup” when H is an immersed submanifold
of G and the group operations are smooth when restricted to H. An immersed submanifold does
not necessarily inherit its topology as a subset of G, but rather H has a topology and smooth
structure such that the derivative of the inclusion ıH : H ,→G is injective (see Lee (2013), and
also Theorem 5.23). When G is a matrix Lie group in GL(n), Lie subgroups H cannot generally be
regarded as matrix Lie groups in GL(n), owing to the fact that they may not be closed in GL(n)
(see Section 5.9 in Hall (2015)). When H is a closed Lie subgroup of G, then H is automatically a
matrix Lie group in GL(n). Interestingly, it does turn out that even when H is not closed in GL(n),
H can always be embedded as a matrix Lie group in a larger GL(n′), n′ ≥n thanks to Theorem 9
in Gotô (1950).
The tangent space to a Lie subgroup H ⊂G at the identity, defined as TeH = Range(d ıH(I)) ⊂
Lie(G), is closed under the Lie bracket and thus forms a “Lie subalgebra” of Lie(G), denoted Lie(H).
A fundamental result in Lie theory (Theorem 5.20) says that every Lie subalgebra h ⊂Lie(G) comes
from a unique connected Lie subgroup of G. These subgroups will correspond to specific model
symmetries in the context of machine learning.
3.2 Group representations, actions, and infinitesimal generators
A matrix Lie group homomorphism is a smooth map Φ : G1 →G2 between matrix Lie groups that
respects matrix multiplication, that is,
Φ(g1g2) = Φ(g1)Φ(g2).
(7)
8

Lie group, G
G0
TeG ∼= Lie(G)
e
exp(tξ)
ξ
g1
g1G0
g2
g2G0
Manifold, M
TxM
x
ˆθ(ξ)(x)
θexp(tξ)(x)
Action, θ
Figure 1: A Lie group G and its action θ on a manifold M. The Lie group G consists of three
connected components with G0 being the one that contains the identity element e. Each
non-identity component of G is a coset giG0 formed by translating the identity component
by an arbitrary element gi in the component. The Lie algebra Lie(G) is identified with the
tangent space TeG and an exponential curve exp(tξ) generated by an element ξ ∈Lie(G)
is shown. The infinitesimal generator ˆθ(ξ) is the vector field on M whose flow corresponds
with the action θexp(tξ) of group elements along exp(tξ).
The tangent map ϕ := d Φ(e) : Lie(G1) →Lie(G2) is a Lie algebra homomorphism (Theorem 3.28),
meaning that it is a linear map respecting the Lie bracket:
ϕ
 [ξ1, ξ2]

=

ϕ(ξ1), ϕ(ξ2)

.
(8)
Moreover, the Lie group homomorphism and its induced Lie algebra homomorphism are related by
the exponential maps on G1 and G2 via the identity
Φ
 exp(ξ)

= exp
 ϕ(ξ)

.
(9)
Another fundamental result (Theorem 5.6) is that any Lie algebra homomorphism Lie(G1) →Lie(G2)
corresponds to a unique Lie group homomorphism G1 →G2 when G1 is simply connected. When
G2 is the general linear group on a vector space, then the Lie group and Lie algebra homomorphisms
are called Lie group and Lie algebra “representations”.
A Lie group G can act on a vector space V via a representation Φ : G →GL(V) according to
θ : (x, g) 7→Φ(g−1)x,
(10)
with x ∈V and g ∈G. More generally, a nonlinear right action of a Lie group G on a manifold M
is any smooth map θ : M × G →M satisfying
θ(θ(x, g1), g2) = θ(x, g1g2)
and
θ(x, e) = x
(11)
9

for every x ∈M and g1, g2 ∈G. Figure 1 depicts the action of a Lie group on a manifold. We
make frequent use of the maps θg = θ(·, g), which have smooth inverses θg−1, and the “orbit maps”
θ(x) = θ(x, ·). For example, using a representation Φ : SE(3) →GL(R7), the position q and velocity
v of a particle in R3 can be rotated and translated via the action
θ


Q
b
0
1

,


q
v
1



= Φ
QT
−QT b
0
1
 

q
v
1

=


QT
0
−QT b
0
QT
0
0
0
1




q
v
1

=


QT (q −b)
QT v
1

.
The positions and velocities of n particles arranged as a vector (q1, . . . , qn, v1, . . . , vn, 1) can be
simultaneously rotated and translated via an analogous representation Φ : SE(n) →GL(R6n+1).
Group actions can be defined more generally, but to avoid abstraction, we temporarily confine
ourselves to group actions defined by representations. The general setting is considered later in
Section 9.
The key fact about a group action is that it is almost completely characterized by a linear map
called the infinitesimal generator. This map ˆθ assigns to each element ξ ∈Lie(G) in the Lie algebra,
a vector field ˆθ(ξ) on M defined by
ˆθ(ξ)(x) = d
d t

t=0
θexp(tξ)(x) = d θ(x)(e)ξ.
(12)
The infinitesimal generator and its relation to the group action are illustrated in Figure 1. For
the linear action in Eq. 10, the infinitesimal generator is the linear vector field ˆθ(ξ)(x) = −ϕ(ξ)x.
Crucially, the flow of the generator recovers the group action along the exponential curve exp(tξ),
i.e.,
Flt
ˆθ(ξ)(x) = θexp(tξ)(x).
(13)
For the linear right action in Eq. 10, this is easily verified by differentiation, applying Eq. 9, and the
fact that solutions of smooth ordinary differential equations are unique. For a nonlinear right action
this follows from Lemma 20.14 and Proposition 9.13 in Lee (2013).
Remark 1 In contrast to a “right” action θ : M × G →M, a “left” action θ : G × M →M
satisfies θ(g2, θ(g1, x)) = θ(g2g1, x). While our main results work for left actions too, e.g. θ(g, x) =
Φ(g)x, right actions are slightly more natural because the infintesimal generator is a Lie alegbra
homomorphism, i.e.,
ˆθ([ξ, η]) = [ˆθ(ξ), ˆθ(η)],
(14)
whereas this holds with a sign change for left actions. Every left action θL can be converted into an
equivalent right action defined by θR(x, g) = θL(g−1, x), and vice versa.
10

4 Fundamental operators for studying symmetry
Here we introduce our main theoretical results for studying symmetries of machine learning models
by focusing on a concrete and useful special case. The basic building blocks of the machine learning
models we consider here are smooth functions F : V →W between finite-dimensional vector spaces.
These functions could be layers of a multilayer neural network, integral kernels to be applied to
spatio-temporal fields, or simply linear combinations of user-specified basis functions in a regression
task as in Brunton et al. (2016). General versions of our results for smooth sections of vector bundles
are developed later in Section 9. Our main results show that two families of fundamental linear
operators encode the symmetries of these functions. The fundamental operators allow us to enforce,
promote, and discover symmetry in machine learning models as we describe in Sections 5, 6, and 7.
We consider representations ΦV : G →GL(V) and ΦW : G →GL(W) of a matrix Lie group
G along with their associated actions on the domain V and codomain W of F. The definition of
equivariance, the symmetry group of a function, and the first family of fundamental operators are
introduced by the following:
Definition 2 We say that F is equivariant with respect to a group element g ∈G if
(KgF)(x) := ΦW(g)F(ΦV(g)−1x) = F(x)
(15)
for every x ∈V. These elements form a subgroup of G denoted SymG(F).
The transformation operators Kg are linear maps of smooth functions to smooth functions with
addition and scalar multiplication defined point-wise. These fundamental operators form a group
with composition KgKh = Kgh and inversion K−1
g
= Kg−1. Thus, g 7→Kg is an infinite-dimensional
representation of G. These operators are useful for studying discrete symmetries of functions, as it
is impractical to work directly with the uncountable family {Kg}g∈G of a continuous group G.
The second family of fundamental operators are the key objects we use to study continuous
symmetries of functions. These are the Lie derivatives defined for each ξ ∈Lie(G) by
(LξF)(x) = d
d t

t=0
 Kexp(tξ)F

(x) = ϕW(ξ)F(x) −∂F(x)
∂x
ϕV(ξ)x.
(16)
We observe that the Lie derivative is linear with respect to both ξ and F. The fundamental operators
and the construction of the Lie derivative are depicted in Figure 2. It turns out (see Proposition 20)
that ξ 7→Lξ is the Lie algebra representation corresponding to g 7→Kg, meaning that
d
d t Kexp(tξ)F = LξKexp(tξ)F = Kexp(tξ)LξF.
and
L[ξ,η] = LξLη −LηLξ.
(17)
The first relation together with the closed subgroup theorem are key to proving our two main
theoretical results in the setting of smooth maps between vector spaces. The results stated below
are special cases of more general results developed later in Section 9.
Our first main result provides necessary and sufficient conditions for a smooth map F : V →W
to be equivariant with respect to the Lie group actions on V and W. This generalizes the constraints
derived by Finzi et al. (2021) for the linear layers of equivariant multilayer perceptrons.
Theorem 3 Let {ξi}dim(G)
i=1
be a basis for the Lie algebra Lie(G) and let {gj}nG−1
j=1
contain one
element from each non-identity component of G. Then a smooth map F : V →W is G-equivariant if
and only if
LξiF = 0
and
KgjF −F = 0
(18)
11

V
x
ΦV(exp(tξ))−1x
W
F(x)
LξF(x)
Kexp(tξ)F(x)
W
F
 ΦV(exp(tξ))−1x

ΦW(exp(tξ))
Figure 2: The fundamental operators for functions between vector spaces and linear Lie group
actions defined by representations. The finite transformation operators Kg act on the
function F : V →W by composing it with the linear transformation ΦV(g)−1 and then
applying ΦW(g) to the values in W. The function is g-equivariant when this process does
not alter the function. The Lie derivative Lξ is formed by differentiating t 7→Kexp(tξ) at
t = 0. Geometrically, LξF(x) is the vector in W tangent to the curve t 7→Kexp(tξ)F(x) in
W passing through F(x) at t = 0.
for every i = 1, . . . , dim(G) and every j = 1, . . . , nG −1. This is a special case of Theorem 21.
Since the fundamental operators Lξ and Kg are linear, these yield linear constraints for a smooth
map F to be G-equivariant.
Our second main result shows that the continuous symmetries of a given smooth function
F : V →W are encoded by its Lie derivatives.
Theorem 4 The symmetry group SymG(F) is a closed, embedded Lie subgroup of G with Lie
subalgebra
symG(F) = {ξ ∈Lie(G) : LξF = 0} .
(19)
This is a special case of Theorem 22.
This result completely characterizes the identity component of the symmetry group SymG(F) because
the connected Lie subgroups of G are in one-to-one correspondence with Lie subalgebras of Lie(G)
(see Theorem 5.20 in Hall (2015) or Theorem 19.26 in Lee (2013)). Since the condition LξF = 0 is
linear with respect to ξ, the Lie subalgebra of the subgroup of symmetries of F can be identified via
linear algebra. In particular it is found by computing the nullspace of the linear operator
LF : ξ 7→LξF.
(20)
This operator is defined on the Lie algebra and takes values in the space C∞(V; W) of smooth
functions V →W.
The preceding two theorems already show the duality between enforcing and discovering con-
tinuous symmetries with respect to the Lie derivative, viewed as a bilinear form (ξ, F) 7→LξF. To
discover symmetries, we seek generators ξ ∈Lie(G) satisfying LξF = 0 for a known function F. On
the other hand, to enforce a connected group of symmetries, we seek functions F satisfying LξiF = 0
with known generators ξ1, . . . , ξdim(G) spanning Lie(G).
12

5 Enforcing symmetry with linear constraints
Methods to enforce symmetry in neural networks and other machine learning models have been
studied extensively, as we reviewed briefly in Section 2.1. A unifying theme in these techniques has
been the use of linear constraints to enforce symmetry (Finzi et al., 2021; Loiseau and Brunton,
2018; Weiler et al., 2018; Cohen et al., 2019; Ahmadi and Khadir, 2020). The purpose of this section
is to show how several of these methods can be understood in terms of the fundamental operators
and linear constraints provided by Theorem 3.
5.1 Multilayer perceptrons
Enforcing symmetry in multilayer percetrons was studied by Finzi et al. (2021). They provide a
practical method based on enforcing linear constraints on the weights defining each layer of a neural
network. The network uses specialized nonlinearities that are automatically equivariant, meaning
that the constraints need only be enforced on the linear component of each layer. We show that the
constraints derived by Finzi et al. (2021) are the same as those given by Theorem 3.
Specifically, each linear layer F (l) : Vl−1 →Vl, for l = 1, . . . , L, is defined by
F (l)(x) = W (l)x + b(l),
(21)
where W (l) are weight matrices and b(l) are bias vectors. Defining group representations Φl : G →
GL(Vl) for each layer, yields fundamental operators given by
KgF (l)(x) −F (l)(x) =
 Φl(g)W (l)Φl−1(g)−1 −W (l)
x + Φl(g)b(l) −b(l)
(22)
LξF (l)(x) =
 ϕl(ξ)W (l) −W (l)ϕl−1(ξ)

x + ϕl(ξ)b(l).
(23)
Let {ξi}dim(G)
i=1
be a basis for Lie(G) and let {gj}nG−1
j=1
consist of an element from each non-identity
component of G. Using the fundamental operators and Theorem 3, it follows that the layer F (l) is
G-equivariant if and only if the weights and biases satisfy
ϕl(ξi)W (l) = W (l)ϕl−1(ξi),
and
Φl(gj)W (l) = W (l)Φl−1(gj),
(24)
ϕl(ξi)b(l) = 0,
and
Φl(gj)b(l) = b(l)
(25)
for every i = 1, . . . , dim(G) and j = 1, . . . , ng −1. These are the same as the linear constraints one
derives using the method by Finzi et al. (2021). The equivariant linear layers are then combined
with specialized equivariant nonlinearities σ(l) : Vl →Vl to produce an equivariant network
F = σ(L) ◦F (L) ◦· · · ◦σ(1) ◦F (1) : V0 →VL.
(26)
The composition of equivariant functions is equivariant, as one can easily check using Definition 2.
5.2 Neural networks acting on fields
Enforcing symmetry in neural networks acting on spatial fields has been studied extensively by Weiler
et al. (2018); Cohen et al. (2018); Esteves et al. (2018); Kondor and Trivedi (2018); Cohen et al.
(2019) among others. Many of these techniques use integral operators to define equivariant linear
layers, which are coupled with equivariant nonlinearities, such as the gated nonlinearities proposed
by Weiler et al. (2018). The key task is to identify appropriate bases for equivariant kernels. For
certain groups, such as the Special Euclidean group G = SE(3), bases can be constructed explicitly
13

using spherical harmonics, as in Weiler et al. (2018). We show that equivariance with respect to
arbitrary group actions can be enforced via linear constraints on the integral kernels derived using
the fundamental operators introduced in Section 4. Appropriate bases of kernel functions can then
be constructed numerically by computing an appropriate nullspace, as is done by Finzi et al. (2021)
for multilayer perceptrons.
For the sake of simplicity we consider integral operators acting on vector-valued functions
F : Rm →V, where V is a finite-dimensional vector space. Later on in Section 9.4 we study higher-
order integral operators acting on sections of vector bundles. If W is another finite-dimensional
vector space, an integral operator acting on F to produce a new function Rn →W is defined by
TKF(x) =
Z
Rm K(x, y)F(y) d y,
(27)
where K(x, y) : V →W are linear maps. In other words, this kernel is a function K : Rn × Rm →
W ⊗V∗, where V∗denotes the dual space of V. In machine learning applications, we seek to optimize
the kernel functions K defining one or more layers of a neural network.
With group actions defined by representations on Rm, Rn, V, W, functions F : Rm →V transform
according to
K(Rm,V)
g
F(x) = ΦV(g)F(ΦRm(g)−1x)
(28)
for g ∈G. Likewise, functions Rn →W transform via an analogous operator K(Rn,W)
g
.
Definition 5 The integral operator TK in Eq. 27 is equivariant with respect to g ∈G when
K(Rn,W)
g
◦TK ◦K(Rm,V)
g−1
= TK.
(29)
The elements g satisfying this equation form a subgroup of G denoted SymG(TK).
By changing variables in the integral, the operator on the left is given by
K(Rn,W)
g
◦TK ◦K(Rm,V)
g−1
F(x) =
Z
Rm KgK(x, y)F(y) d y,
(30)
where
KgK(x, y) = ΦW(g)K
 ΦRn(g)−1x, ΦRm(g)−1y

ΦV(g)−1 det

ΦRm(g)−1
.
(31)
The following result provides equivariance conditions in terms of the kernel, generalizing Lemma 1
in Weiler et al. (2018).
Proposition 6 Let K be continuous and suppose that TK acts on a function space containing all
smooth, compactly supported fields. Then
SymG(TK) = {g ∈G : KgK = K} .
(32)
We give a proof in Appendix A
The Lie derivative of the kernel is given by
LξK(x, y) = ϕW(ξ)K(x, y) −K(x, y)ϕV(ξ) −K(x, y) Tr[ϕRm(ξ)]
−∂K(x, y)
∂x
ϕRn(ξ)x −∂K(x, y)
∂y
ϕRm(ξ)y
(33)
14

The operators Kg and Lξ are the fundamental operators from Section 4 because the transformation
law for the kernel can be written as
KgK = ΦW⊗V∗(g)K ◦ΦRm×Rm(g)−1,
(34)
where
ΦRn×Rm(g) : (x, y) 7→(ΦRn(g)x, ΦRm(g)y) ,
ΦW⊗V∗(g) : T 7→ΦW(g)TΦV(g)−1 det

ΦRm(g)−1
(35)
are representations of G in Rm × Rm and W ⊗V∗.
As an immediate consequence of Theorem 3, we have the following corollary establishing linear
constraints for the kernel to produce an equivariant integral operator.
Corollary 7 Let {ξi}dim(G)
i=1
be a basis for the Lie algebra Lie(G) and let {gj}nG−1
j=1
contain one
element from each non-identity component of G. Under the same hypotheses as Proposition 6, the
integral operator TK in Eq. 27 is G-equivariant in the sense of Definition 5 if and only if
LξiK = 0
and
KgjK −K = 0
(36)
for every i = 1, . . . , dim(G) and every j = 1, . . . , nG −1.
These linear constraint equations must be satisfied to enforced equivariance with respect to a known
symmetry G in the machine learning process. By discretizing the operators Kg and Lξ, as discussed
later in Section 8, one can solve these constraints numerically to construct a basis of kernel functions
for equivariant integral operators.
As an immediate consequence of Theorem 4, the following result shows that the Lie derivative
encodes the continuous symmetries of a given integral kernel.
Corollary 8 Under the same hypotheses as Proposition 6, SymG(TK) is a closed, embedded Lie
subgroup of G whose Lie subalgebra is
symG(TK) = {ξ ∈Lie(G) : LξK = 0} .
(37)
This result will be useful for methods that promote symmetry of the integral operator, as we describe
later in Section 7.
6 Discovering symmetry by computing nullspaces
In this section we show that in a wide range of settings, the continuous symmetries of a manifold,
point cloud, or map can be recovered by computing the nullspace of a linear operator. For functions,
this is already covered by Theorem 4, which allows us to compute the connected subgroup of
symmetries by identifying its Lie subalgebra
symG(F) = Null(LF )
(38)
where LF : ξ 7→Lξ is the linear operator defined by Eq. 20. Hence, if a machine learning model F
has a symmetry group SymG(F), then its Lie algebra is equal to the nullspace of LF .
This section explains how this is actually a special case of a more general result allowing us to
reveal the symmetries of submanifolds via the nullspace of a closely related operator. We begin with
the more general case where we study the symmetries of a submanifold of Euclidean space, and we
explain how to recover symmetries from point clouds approximating submanifolds. The Lie derivative
15

described in Section 4 is then recovered when the submanifold is the graph of a function. We also
briefly describe how the fundamental operators from Section 4 can be used to recover symmetries
and conservation laws of dynamical systems.
6.1 Symmetries of submanifolds
We begin by studying the symmetries of submanifolds M of Euclidean space Rd using an approach
similar to Cahill et al. (2023). However, we use a different operator that generalizes more naturally
to nonlinear group actions on arbitrary manifolds (see Section 10) and recovers the Lie derivative
(see Section 6.2). With a representation Φ : G →GL(Rd) of a Lie group, we define invariance of a
submanifold as follows:
Definition 9 A submanifold M ⊂Rd is invariant with respect to a group element g ∈G if
Φ(g)z ∈M
(39)
for every z ∈M. These elements form a subgroup of G denoted SymG(M).
The subgroup of symmetries of a submanifold is characterized by the following theorem.
Theorem 10 Let M be a smooth, closed, embedded submanifold of Rd. Then SymG(M) is a closed,
embedded Lie subgroup of G whose Lie subalgebra is
symG(M) = {ξ ∈Lie(G) : ϕ(ξ)z ∈TzM
∀z ∈M}.
(40)
This is a special case of Theorem 31.
The meaning of this result and its practical use for detecting symmetry are illustrated in Figure 3.
To reveal the connected component of SymG(M), we let Pz : Rd →Rd be a family of linear
projections onto TzM ⊂Rd. These are assumed to vary continuously with respect to z ∈M.
Then under the assumptions of the above theorem, symG(M) is the nullspace of the symmetric,
positive-semidefinite operator SM : Lie(G) →Lie(G) defined by

η, SMξ

Lie(G) =
Z
M
zT ϕ(η)T (I −Pz)T (I −Pz)ϕ(ξ)z d µ(z)
(41)
for every η, ξ ∈Lie(G). We see in Figure 3 that (I −Pz)ϕ(ξ)z measures the component of the
infinitesimal generator not tangent to the submanifold at z. Here, µ is any strictly positive measure
on M that makes all of these integrals finite. The above formula is useful for computing the matrix
of SM in an orthonormal basis for Lie(G).
Alternatively, when the dimension of G is large, one can compute the nullspace using a Krylov
algorithm such as the one described in Finzi et al. (2021). Such algorithms rely solely on queries of
SM acting on vectors ξ ∈Lie(G), which can be obtained explicitly by
SMξ =
Z
M
d Φ(e)∗
(I −Pz)T (I −Pz)ϕ(ξ)zzT 
d µ(z),
(42)
where d Φ(e)∗: Rd×d →Lie(G) is the adjoint of d Φ(e) : Lie(G) →Rd×d.
In practice, one can use sample points zi on the manifold to obtain a Monte-Carlo estimate of
SM with approximate projections Pzi computed using local principal component analysis (PCA), as
described in Cahill et al. (2023). More accurate estimates of the tangent spaces can be obtained
16

M
TzM
z
ˆθ(ξ)(z) = ϕ(ξ)z
θexp(tξ)(z)
M
TzM
z
Pz ˆθ(ξ)(z)
(I −Pz)ˆθ(ξ)(z)
ˆθ(ξ)(z) = ϕ(ξ)z
θexp(tξ)(z)
Figure 3: Tangency of infinitesimal generators and symmetries of submanifolds. The infinitesimal
generator ˆθ(ξ) is everywhere tangent to the submanifold M if and only if the curves
t 7→θexp(tξ)(z), with z ∈M, lie in M for all t. The Lie algebra elements ξ satisfying this
tangency condition form the Lie subalgebra of symmetries of M. To test for tangency
of the infinitesimal generator we use a family of projections Pz onto the tangent spaces
TzM for every z ∈M. Specifically, (I −Pz)ˆθ(ξ) is the component of the infinitesimal
generator that does not lie tangent to M. Hence, ξ generates a symmetry of M if and
only if (I −Pz)ˆθ(ξ) = 0 for all z ∈M.
using the methods in Berry and Giannakis (2020). Assuming the Pzi are accurate, the results of
Section 8, below, can be used to show that the correct nullspace is obtained from finitely many
sample points zi almost surely.
6.2 Symmetries of functions as symmetries of submanifolds
The method described above for studying symmetries of submanifolds can be applied to reveal the
symmetries of smooth maps between vector spaces by identifying the map F : V →W with its graph
gr(F) = {(x, F(x)) ∈V × W : x ∈V}.
(43)
The graph is a smooth, closed, embedded submanifold of the space V × W by Proposition 5.7 in Lee
(2013). We show that this approach recovers the Lie derivative and our result in Theorem 4. By
choosing bases for the domain and codomain, it suffices to consider smooth functions F : Rm →Rn.
Supposing that we have representations ΦRm and ΦRn of G in the domain and codomain, we
consider a combined representation
Φ : g 7→
ΦRm(g)
0
0
ΦRn(g)

.
(44)
Defining a smoothly-varying family of projections
P(x,F(x)) =

I
0
d F(x)
0

(45)
17

onto T(x,F(x)) gr(F), it is easy to check that

0
LξF(x)

=
I
0
0
I

−

I
0
d F(x)
0

|
{z
}
I−P(x,F (x))
ϕRm(ξ)
0
0
ϕRn(ξ)

|
{z
}
ϕ(ξ)
 x
F(x)

.
(46)
We note that this is a special case of Theorem 34 describing the Lie derivative in terms of a projection
onto the tangent space of a function’s graph. The resulting operator Sgr(F) defined by Eq. 41 is
given by

η, Sgr(F)ξ

Lie(G) =
Z
Rm(LηF(x))T LξF(x) d µ(x),
(47)
for η, ξ ∈Lie(G) and an appropriate positive measure µ on Rm that makes the integrals finite.
Therefore, Theorem 4 is recovered from our result about symmetries of submanifolds stated in
Theorem 10.
Related quantities have been used to study the symmetries of trained neural networks, with
the F being the network and its derivatives computed via back-propagation.
The quantity

ξ, Sgr(F)ξ

Lie(G) = ∥LξF∥L2(µ) was used by Gruver et al. (2022) to construct the Local Equiv-
ariant Error or (LEE), measuring the extent to which a trained neural network F fails to respect
symmetries in the one-parameter group {exp(tξ)}t∈R. The nullspace of ξ 7→LξF in the special
case where ΦRn(g) = I acts trivially was used by Moskalev et al. (2022) to identify the connected
subgroup with respect to which a given network is invariant.
By viewing a function as a submanifold, we obtain a simple data-driven technique for estimating
the Lie derivative and subgroup of symmetries of the function. To approximate LξF, Sgr(F), and
symG(F) using input-output pairs (xi, yi = F(xi)), one simply needs to approximate the projection
in Eq. 45 using these data. To do this, we can obtain matrices Ui with m columns spanning
T(xi,yi) gr(F) by applying local PCA to the data zi = (xi, yi), or by pruning the frames computed in
Berry and Giannakis (2020). With E =

Im×m
0m×n

the projection in Eq. 45 is given by
Pzi = Ui(EUi)−1E
(48)
because any projection is uniquely determined by its range and nullspace (see Section 5.9 of Meyer
(2000)). This gives us a simple way to approximate (LξF)(zi), Sgr(F), and symG(F) using the
input-output pairs. However, many such pairs are needed since the tangent space to the graph of
F at xi is well-approximated by local PCA only when there are at least m neighboring samples
sufficiently close to xi. Even more samples are needed when they are noisy. The convergence
properties of the spectral methods in Berry and Giannakis (2020) are better, but they still require
enough samples to obtain accurate Monte-Carlo or quadrature-based estimates of integrals, in this
case over Rm.
6.3 Symmetries and conservation laws of dynamical systems
Here, we consider the case when F : Rn →Rn is a smooth function defining a dynamical system
d
d t x(t) = F(x(t))
(49)
with state variables x(t) ∈Rn. The solution of this equation is described by the flow map Fl :
(t, x(τ)) 7→x(τ +t), which is defined on a maximal connected open set D containing 0×Rn. In many
18

cases we write Flt(·) = Fl(t, ·). Given a Lie group representation Φ : G →GL(Rn), equivariance for
the dynamical system is defined as follows:
Definition 11 The dynamical system in Eq. 49 is equivariant with respect to a group element
g ∈G if the flow map satisfies
Kg Flt(x) := Φ(g) Flt(Φ(g)−1x) = Flt(x)
(50)
for every (t, x) ∈D.
Differentiating at t = 0 shows that equivariance of the dynamical system implies that F is equivariant
in the sense of Definition 2. The converse is also true thanks to Corollary 9.14 in Lee (2013), meaning
that equivariance for the dynamical system is equivalent to equivariance of F. Therefore, we can
study equivariance of the dynamical system in Eq. 49 by directly applying the tools developed in
Section 4 to the function F. Thanks to Theorem 4, identifying the connected subgroup of symmetries
for the dynamical system is a simple matter of computing the nullspace of the linear map ξ 7→LξF,
that is
symG(F) = {ξ ∈Lie(G) : LξF = 0}.
(51)
Here, the Lie derivative is given by
LξF(x) = ϕ(ξ)F(x) −∂F(x)
∂x
ϕ(ξ)x = [ˆθ(ξ), F](x),
(52)
where [ˆθ(ξ), F] is the Lie bracket of the infinitesimal generator defined by ˆθ(ξ)(x) = −ϕ(ξ)x and the
vector field F. Symmetries can also be enforced as linear constraints on F described by Theorem 3.
This was done by Ahmadi and Khadir (2020) for polynomial dynamical systems with discrete
symmetries. Later on in Section 9.1 we show that analogous results apply to dynamical systems
defined by vector fields on manifolds and nonlinear Lie group actions.
A conserved quantity for the system in Eq. 49 is defined as follows:
Definition 12 A scalar valued quantity f : Rn →R is said to be conserved when
Ktf(x) := f(Flt(x)) = f(x)
∀(t, x) ∈D.
(53)
In this setting, the composition operators Kt are often referred to as Koopman operators (see
Koopman (1931); Mezić (2005); Mauroy et al. (2020); Otto and Rowley (2021); Brunton et al.
(2022)). It is easy to see that a smooth function f is conserved if and only if
LF f := d
d t

t=0
Ktf = ∂f
∂xF = 0.
(54)
This relation is used by Kaiser et al. (2018, 2021) to identify conserved quantities by computing
the nullspace of LF restricted to finite-dimensional spaces of candidate functions. When the flow is
defined for all t ∈R, the operators Kt and LF can be viewed as the fundamental operators for the
nonlinear action of the flow map described later on in Section 9. These are the fundamental operators
from Section 4 for representations ΦV(t) = e−At and ΦW(t) = I of the Lie group G = (R, +) when
F(x) = Ax is a linear dynamical system.
Remark 13 For Hamiltonian dynamical systems Noether’s theorem establishes a remarkable equiva-
lence between the symmetries of the Hamiltonian and conserved quantities of the system. We study
Hamiltonian systems later in Section 9.3.
19

7 Promoting symmetry with convex penalties
In this section we show how to design custom convex regularization functions to promote symmetries
within a given candidate group during training of a machine learning model. This allows us to
train a model with as many symmetries as possible from among the candidates, while breaking
candidate symmetries only when the data provides sufficient evidence. We study both discrete and
continuous groups of candidate symmetries. We quantify the extent to which symmetries within the
candidate group are broken using the fundamental operators described in Section 4. For discrete
groups we use the transformation operators {Kg}g∈G and for continuous groups we use the Lie
derivatives {Lξ}ξ∈Lie(G). In the continuous case we penalize a convex relaxation of the codimension
of the subgroup of symmetries given by a nuclear norm (Schatten 1-norm) of the operator ξ 7→LξF
defined by Eq. 20; minimizing this codimension via the proxy nuclear norm will promote the largest
nullspace possible, and hence the largest admissible symmetry group. Once these regularization
functions are developed abstractly in Sections 7.1 and 7.2, we show how the approach can be applied
to basis function regression (Section 7.3) and neural networks (Section 7.4).
As in Section 4, the basic building blocks of the machine learning models we consider are smooth
functions F : V →W between finite-dimensional vector spaces. While we consider this restricted
setting here, our results readily generalize to smooth sections of vector bundles, as we describe
later in Section 9. These functions could be layers of a multilayer perceptron, integral kernels to be
applied to spatio-temporal fields, or simply linear combinations of user-specified basis functions in a
regression task. One does not usually optimize over all smooth functions, instead constraining F to
lie in a given subspace F ⊂C∞(V; W). Working within a finite-dimensional subspace of smooth
functions will be important when we seek to discretize the fundamental operators in Section 8
The candidate symmetries are described by a Lie group G acting on the domain and codomain of
functions F ∈F via group representations ΦV : G →GL(V) and ΦW : G →GL(W). Equivariance
in this setting is described by Definition 2. When fitting the function F to data, our regularization
functions penalize the size of G \ SymG(F). For reasons that will become clear, we use different
penalties corresponding to different notions of “size” when G is a discrete group versus when G is
continuous. The main result describing the continuous symmetries of F is Theorem 4.
7.1 Discrete symmetries
When the group G has finitely many elements, one can measure the size of G \ SymG(F) simply by
counting its elements:
RG,0(F) = |G \ SymG(F)|.
(55)
However, this penalty is impractical for optimization owing to its discrete values and nonconvexity.
Letting ∥· ∥be any norm on the space F′ = span{KgF : g ∈G, F ∈F} yields a convex relaxation
of the above penalty given by
RG,1(F) =
X
g∈G
∥KgF −F∥.
(56)
This is a convex function on F because Kg is a linear operator. For example, if c = (c1, . . . , cN) are
the coefficients of F in a basis for F′ and K
K
Kg is the matrix of Kg in this basis, then the Euclidean
norm can be used to define
RG,1(F) =
X
g∈G
∥K
K
Kgc −c∥2.
(57)
This is directly analogous to the group sparsity penalty proposed in Yuan and Lin (2006).
20

7.2 Continuous symmetries
We now consider the case where G is a Lie group of dimension greater than zero. Here we use
the dimension of SymG(F) to measure the symmetry of F, seeking to penalize the complementary
dimension or “codimension”, given by
RG,0(F) = codim(SymG(F)) = dim(G) −dim(SymG(F)).
(58)
We take this approach in the continuous case because it is no longer possible to simply count the
number of broken symmetries. While is is possible in principle to replace the sum in Eq. 56 by
an integral, the numerical quadrature required to approximate it becomes prohibitive for higher-
dimensional candidate groups. This is complicated even further by the fact that SymG(F) is a
set of measure zero in G whenever dim(SymG(F)) < dim(G) and our empirical observation that
∥F −KgF∥often increases very rapidly as g leaves the set SymG(F).
The dimension of SymG(F) is equal to that of its Lie algebra. Thanks to Theorem 4, this is the
nullspace of a linear operator LF : Lie(G) →C∞(V; W) defined by
LF : ξ 7→LξF,
(59)
where Lξ is the Lie derivative in Eq. 16. By the rank and nullity theorem, the codimension of
SymG(F) is equal to the rank of this operator:
RG,0(F) = codim(SymG(F)) = rank(LF ).
(60)
Penalizing the rank of an operator is impractical for optimization owing to its discrete values and
nonconvexity. A commonly used convex relaxation of the rank is provided by the Schatten 1-norm,
also known as the “nuclear norm”, given by
RG,∗(F) = ∥LF ∥∗=
dim(G)
X
i=1
σi(LF ).
(61)
Here σi(LF ) denotes the ith singular value of LF with respect to inner products on Lie(G) and
F′′ = span{LξF : ξ ∈Lie(G), F ∈F}. For certain rank minimization problems, penalizing the
nuclear norm is guaranteed to recover the true minimum rank solution (Recht et al., 2010; Gross,
2011).
Eq. 61 is a convex function on F because F 7→LF is linear. For example, if (c1, . . . , cN) are the
coefficients of F in a basis {F1, . . . , FN} for F and LLLFi are the matrices of LFi in orthonormal bases
for Lie(G) and F′′, then
RG,∗(F) = ∥c1LLLF1 + · · · + cNLLLFN ∥∗.
(62)
With {ξ1, . . . , ξdim(G)} and {u1, . . . , uN′′} being the orthonormal bases for Lie(G) and F′′, one can
compute a store the rank-3 tensor [LLLFi]j,k = ⟨uj, LξkFi⟩F′′.
7.3 Promoting symmetry in basis function regression
Here we consider regression problems for maps F : Rm →Rn of the form F(x) = WD(x) where
D : Rm →RN is a dictionary consisting of user-specified smooth functions and W ∈Rn×N is a
weight matrix we seek to fit. For example, the sparse identification of nonlinear dynamics (SINDy)
algorithm (Brunton et al., 2016) belongs to this type of learning, among other machine learning
21

algorithms (Brunton and Kutz, 2022). For such maps we have
(KgF)(x) −F(x) = ΦRn(g)−1WD(ΦRm(g)x) −WD(x)
(63)
(LξF)(x) = W ∂D(x)
∂x
ϕRm(ξ)x −ϕRn(ξ)WD(x),
(64)
which can be used in Eq. 56 and Eq. 61 to construct symmetry-promoting regularization functions
RG(W) that are convex with respect to the weight matrix W. Given a collection of data consisting
of input-output pairs {(xj, yj)}M
j=1 we can seek a regularized least-squares fit by solving the convex
optimization problem
minimize
W∈Rn×N
1
M
M
X
j=1
∥yj −WD(xj)∥2 + γRG(WD).
(65)
Here, γ ≥0 is a parameter controlling the strength of the regularization that can be determined
using cross-validation.
Remark 14 The solutions F = WD of Eq. 65 do not depend on how the dictionary functions are
normalized due to the fact that the function being minimized can be written entirely in terms of
F and the data (xj, yj). This is in contrast to other types of regularized regression problems that
penalize the weights W directly, and therefore depend on how the functions in D are normalized.
7.4 Promoting symmetry in neural networks
In this section we describe a convex regularizing penalty to promote G-equivariance in feed-forward
neural networks
F = F (L) ◦· · · ◦F (1)
(66)
composed of layers F (l) : Vl−1 →Vl with group representations Φl : G →GL(Vl). Since the
composition is g-equivariant if every layer is g-equivariant, the main idea is to measure the symmetries
shared by all of the layers. Specifically, we aim to maximize the “size” of the subgroup
L
\
l=1
SymG
 F (l)
= {g ∈G : KgF (l) = F (l), l = 1, . . . , L} ⊂SymG(F),
(67)
where the notion of “size” we adopt depends on whether G is discrete or continuous. The same
ideas can be applied to neural networks acting on fields with layers defined by integral operators as
described in Section 5.2. In this case we consider symmetries shared by all of the integral kernels.
We consider the case in which the trainable layers are elements of vector spaces Fl, over which
the optimization is carried out. For example, each layer may be given by F (l) = W (l)D(l) as in
Section 7.3, where W (l) is a trainable weight matrix and D(l) is a fixed dictionary of nonlinear
functions. Alternatively, we could follow Finzi et al. (2021) and use trainable linear layers composed
with fixed G-equivariant nonlinearities. In contrast with Finzi et al. (2021), we do not force the
linear layers to be G-equivariant. Rather, we penalize the breaking of G-symmetries in the linear
layers as a means to regularize the neural network and to learn which subgroup of symmetries are
compatible with the data and which are not.
22

As in Section 7.1, when G is a discrete group with finitely many elements, a convex relaxation of
the cardinality of G \ TL
l=1 SymG(F (l)) is
RG,1
 F (1), . . . , F (l)
=
X
g∈G
v
u
u
t
L
X
l=1
KgF (l) −F (l)2.
(68)
Again, this is analogous to the group-LASSO penalty developed in Yuan and Lin (2006).
When G is a Lie group with nonzero dimension, we follow the approach in Section 7.2 using the
following observation:
Proposition 15 The subgroup in Eq. 67 is closed and embedded in G; its Lie subalgebra is
L
\
l=1
symG
 F (l)
=
n
ξ ∈Lie(G) : LξF (l) = 0, l = 1, . . . , L
o
.
(69)
We provide a proof in Appendix A.
The Lie subalgebra in the proposition is equal to the nullspace of the linear operator LF (1),...,F (L) :
Lie(G) →LL
l=1 C∞(Vl−1; Vl) defined by
LF (1),...,F (L) : ξ 7→
 LξF (1), . . . , LξF (L)
.
(70)
By the rank and nullity theorem, minimizing the rank of this operator is equivalent to maximizing
the dimension of the subgroup of symmetries shared by all of the layers in the network. As in
Section 7.2, a convex relaxation of the rank is provided by the nuclear norm
RG,∗
 F (1), . . . , F (l)
=
LF (1),...,F (L)

∗=



LLLF (1)
...
LLLF (L)



∗
,
(71)
where LLLF (l) are the matrices of LF (l) in orthonormal bases for Lie(G) and the associated spaces F′′
l .
8 Discretizing the operators
This section describes how to construct matrices for the operators Lξ and LF for smooth functions
F in a user-specified finite-dimensional subspace F ⊂C∞(V; W). By choosing bases for the the
finite-dimensional vector spaces V and W, it suffices without loss of generality to consider the case
in which V = Rm and W = Rn. We focus on constructing the matrix of Lξ since the matrix of Kg
can be obtained in an analogous way with respect to appropriate bases. We assume that Lie(G) and
F are endowed with inner products and that {ξ1, . . . ξdim(G)} and {F1, . . . Fdim(F)} are orthonormal
bases for these spaces, respectively. The key task is to endow the subspace
F′ = span {LξF : ξ ∈Lie(G), F ∈F} ⊂C∞(Rm; Rn)
(72)
with a convenient inner product. Once this is done, an orthonormal basis {u1, . . . , uN} can be
constructed for F′ by applying a Gram-Schmidt process to the functions LξiFj. Matrices for Lξ and
23

LF are then easily obtained by computing

LLLξ

i,j =

ui, LξFj

F′,

LLLF

i,k =

ui, LξkF

F′.
(73)
The issue at hand is to choose the inner product on F′ in a way that makes computing these matrices
easy. A natural choice is to equip F′ with an L2(Rm, µ; Rn) inner product where µ is a positive
measure on Rm (such as a Guassian distribution) for which the L2 norms of function in F′ are finite.
The problem is that it is usually challenging or inconvenient to compute the required inner products

LξiFj, LξkFl

L2(µ) =
Z
Rm
 LξiFj

(x)T  LξkFl

(x) d µ(x)
(74)
analytically. In this section we discuss inner products that are easy to compute in practice.
8.1 Numerical quadrature and Monte-Carlo
When Eq. 74 cannot be computed analytically, one can resort to a numerical quadrature or Monte-
Carlo approximation. In both cases the integral is approximated by a weighted sum, yielding a
semi-inner product
⟨f, g⟩L2(µM) = 1
M
M
X
i=1
wif(xi)T g(xi)
(75)
that converges to ⟨f, g⟩L2(µ) as M →∞for f, g within certain function spaces. The following
proposition means that we do not have to pass to the limit M →∞in order to obtain a valid inner
product defined by Eq. 75 on F′.
Proposition 16 Suppose that F′ is a finite-dimensional and ⟨f, g⟩L2(µM) →⟨f, g⟩L2(µ) as M →∞
for every f, g ∈F′. Then there is an M0 such that Eq. 75 is an inner product on F′ for every
M ≥M0. We give a proof in Appendix A.
For example, in Monte-Carlo approximation, the samples xi are drawn independently from a
distribution ν with the assumption that both µ and ν are σ-finite and µ is absolutely continuous
with respect to ν. The weights are given by the Radon-Nikodym derivative wi = d µ
d ν (xi). Then for
every f, g ∈L2(µ) the approximate integral converges ⟨f, g⟩L2(µM) →⟨f, g⟩L2(µ) as M →∞almost
surely thanks to the strong law of large numbers (see Theorem 7.7 in Koralov and Sinai (2012)). By
the proposition, there is almost surely a finite M0 such that Eq. 75 is an inner product on F′ for
every M ≥M0.
8.2 Subspaces of polynomials
Here we consider the special case when F is a finite-dimensional subspace consisting of polynomial
functions Rm →Rn. Examining the expression in Eq. 16, it is evident that LξF is also a polynomial
function Rm →Rn with degree not greater than that of F ∈F. Thus, F′ is also a space of
polynomial functions with degree not exceeding the maximum degree in F. Since a polynomial that
vanishes on an open set must be identically zero, we can take the integrals defining the inner product
in Eq. 75 over a cube, such as [0, 1]m ⊂Rm. This is convenient because polynomial integrals over
the cube can be calculated analytically.
We can also use the sample-based inner product in Eq. 75 with randomly chosen points xi and
positive weights wi. The following proposition tells us exactly how many sample points we need.
24

Proposition 17 Let F′ be a space of real polynomial functions Rm →Rn and let πi : Rn →R be
the ith coordinate projection π(c1, . . . , cn) = ci. Let
M ≥M0 = max
1≤i≤n dim(πi(F′))
(76)
and let w1, . . . , wM > 0 be positive weights. Then for almost every set of points (x1, . . . , xM) ∈(Rm)M
with respect to Lebesgue measure, Eq. 75 is an inner product on F′. We give a proof in Appendix B.
This means that we can draw M ≥M0 sample points independently from any absolutely continuous
measure (such as a Gaussian distribution or the uniform distribution on a cube), and with probability
one, Eq. 75 will be an inner product on F′. When F consists of polynomials with degree at most d,
then taking
M ≥
d
X
k=0
k + m −1
m −1

(77)
is sufficient.
9 Generalization to sections of vector bundles
The machinery for promoting, discovering, and enforcing symmetry of maps F : V →W between
finite-dimensional vector spaces is a special case of more general machinery for sections of vector
bundles presented here. Applications of this more general framework include studying the symmetries
of vector and tensor fields on manifolds with respect to nonlinear group actions (Abraham et al.,
1988). Our analysis pertains to all finite-dimensional real Lie groups, and is not confined to matrix
Lie groups acting via representations as in Section 7. We rely heavily on background, definitions,
and results that can be found in Lee (2013) and Kolář et al. (1993). All manifolds and maps are
assumed to be smooth, that is infinitely continuously differentiable, C∞.
First, we provide some background on vector bundles that can be found in Lee (2013, Chapter 10).
A rank-k vector bundle E is a collection of k-dimensional vector spaces Ep, called “fibers”, organized
smoothly over a base manifold M. This fibers are organized by the “bundle projection” π : E →M,
a surjective map whose preimages are the fibers Ep = π−1(p). The smoothness of this organization
means that π is a smooth submersion where E is a smooth manifold covered by smooth local
trivializations
ψα : π−1(Uα) ⊂E →Uα × Rk
with {Uα}α∈A being open subsets covering M. The transition functions between local trivializations
are Rk-linear, meaning that there are smooth matrix-valued functions TTT α,β : Uα ∩Uβ →Rk×k
satisfying
ψα ◦ψ−1
β (p, v) = (p,TTT α,β(p)v)
(78)
for every p ∈Uα ∩Uβ and v ∈Rk. The bundle with this structure is often denoted π : E →M.
A “section” of the rank-k vector bundle π : E →M is a map F : M →E satisfying π ◦F = IdM.
The space of smooth sections, denoted Σ(E), is a vector space with addition and scalar multiplication
defined pointwise in each fiber Ep. A vector bundle and a section are depicted in Figure 4, along
with the fundamental operators for a group action that we introduce below.
We consider a “fiber-linear” right G-action Θ : E × G →E, meaning that every Θg = Θ(·, g) :
E →E is a vector bundle homomorphism. In other words, Θ descends under the bundle projection
25

M
p
q = θexp(tξ)(p)
Ep
F(p)
LξF(p)
Kexp(tξ)F(p)
Eq
F(q)
Θexp(−tξ)
E
π
Figure 4: Fundamental operators for sections of vector bundles equipped with fiber-linear Lie group
actions. The action Θg is linear on each fiber Ep and descends under the bundle projection
π : E →M to an action θg on M. Given a section F : M →E, the finite transformation
operators Kg produce a new section KgF whose value at p is given by evaluating F at
q = θg(p) and pulling the value in Eq back to Ep via the linear map Θg−1 on Eq. The
operators Kg are linear thanks to linearity of Θg−1 on every Eq. The Lie derivative Lξ is
the operator on sections formed by differentiating t 7→Kexp(tξ) at t = 0. Geometrically,
LξF(p) is the vector in Ep lying tangent to the curve t 7→Kexp(tξ)F(p) in Ep passing
through F(p) at t = 0.
π to a unique right G-action θ : M × G →M so that the diagram
E
E
M
M
Θg
π
π
θg
(79)
commutes and the restricted maps Θg|Ep : Ep →Eθ(p,g) are linear. We define what it means for a
section to be symmetric with respect to this action as follows:
Definition 18 A section F ∈Σ(E) is equivariant with respect to a transformation g ∈G if
KgF := Θg−1 ◦F ◦θg = F.
(80)
These transformations form a subgroup of G denoted SymG(F).
26

The operators Kg are depicted in Figure 4. Thanks to the vector bundle homomorphism properties
of Θg−1, the operators Kg : Σ(E) →Σ(E) are well-defined and linear. Moreover, they form a group
under composition Kg1Kg2 = Kg1·g2, with inverses given by K−1
g
= Kg−1.
The “infinitesimal generator” of the group action is the linear map ˆΘ : Lie(G) →X(E) defined by
ˆΘ(ξ) = d
d t

t=0
Θexp(tξ).
(81)
It turns out that this vector field is Θ-related to 0 × ξ ∈X(M × G) (see Lemma 5.13 in Kolář et al.
(1993), Lemma 20.14 in Lee (2013)), meaning that the flow of ˆΘ(ξ) is given by
Flt
ˆΘ(ξ) = Θexp(tξ).
(82)
Likewise, θexp(tξ) is the flow of ˆθ(ξ) =
d
d t

t=0 θexp(tξ) ∈X(M), which is π-related to ˆΘ(ξ).
Differentiating the smooth curves t 7→Kexp(tξ)F(p) lying in Ep for each p ∈M gives rise to the
Lie derivative Lξ : Σ(E) →Σ(E) along ξ ∈Lie(G) defined by
LξF = d
d t

t=0
Kexp(tξ)F = lim
t→0
1
t
 Θexp(−tξ) ◦F ◦θexp(tξ) −F

,
(83)
where the derivative and limit are understood pointwise and we identify TF(p)Ep ∼= Ep. This
construction is illustrated in Figure 4. We recover Eq. 16 from Eq. 83 in the special case where a
smooth function F : V →W is viewed as a section x 7→(x, F(x)) of the bundle π : V × W →V and
acted upon by group representations. Critically, the Lie derivative Lξ, as defined above, is a linear
operator on sections of the vector bundle E. This allows us to formulate convex symmetry-promoting
regularization functions as in Section 7 using Lie derivatives in the broader setting of vector bundle
sections.
Remark 19 (Lie derivatives using flows) Thanks to Eq. 82, the Lie derivative defined in Eq. 83
only depends on the infinitesimal generator ˆΘ(ξ) ∈X(E), and its flow for small time t. Hence, any
vector field in X(E) whose flow is fiber-linear, but not necessarily defined for all t ∈R, gives rise
to an analogously-defined Lie derivative acting linearly on Σ(E). These are the so-called “linear
vector fields” described by Kolář et al. (1993) in Section 47.9. In fact, more general versions of the
Lie derivative based on flows for maps between manifolds are described by Kolář et al. (1993) in
Chapter 11. However, these generalizations are nonlinear operators, destroying the convex properties
of the symmetry-promoting regularization functions in Section 7.
In addition to linearity, the key properties of the operators Kg and Lξ for studying symmetries
of sections are:
Proposition 20 For every F ∈Σ(E), ξ, η ∈Lie(G), and α, β, t ∈R, we have
d
d t Kexp(tξ)F = LξKexp(tξ)F = Kexp(tξ)LξF,
(84)
Lαξ+βη = αLξ + βLη,
(85)
and
L[ξ,η] = LξLη −LηLξ.
(86)
We give a proof in Appendix C.
27

Taken together, these results mean that Π : g 7→Kg and Π∗: ξ 7→Lξ are (infinite-dimensional)
representations of G and Lie(G).
The main results of this section are the following two theorems. The first gives necessary and
sufficient conditions for a section to be G-equivariant, while the second completely characterizes the
identity component of SymG(F) by correspondence with its Lie subgalgebra.
Theorem 21 Let G0 be the identity component of G. A section F ∈Σ(E) is G0-equivariant if and
only if
LξF = 0
∀ξ ∈Lie(G).
(87)
If, in addition, we have KgiF = F for a single gi from each non-identity component of G, then F is
G-equivariant. We give a proof in Appendix D.
Theorem 22 If F ∈Σ(E) is a section, then SymG(F) is a closed, embedded Lie subgroup of G
whose Lie subalgebra is
symG(F) = {ξ ∈Lie(G) : LξF = 0} .
(88)
We give a proof in Appendix E.
These results allow us to promote, enforce, and discover symmetries for sections of vector bundles
in fundamentally the same way we did for maps between finite-dimensional vector spaces in Sec-
tions 5, 6, and 7. In particular, symmetry can be enforced through analogous linear constraints,
discovered through nullspaces of analogous operators, and promoted through analogous convex
penalties based on the nuclear norm.
Remark 23 (Left actions) Theorems 21 and 22 hold without any modification for left G-actions
ΘL : G × E →E. This is because we can define a corresponding right G-action by ΘR(p, g) =
ΘL(g−1, p) with associated operators related by
KR
g = KL
g−1
and
LR
ξ = −LL
ξ .
(89)
The symmetry group SymG(F) does not depend on whether it is defined by the condition KR
g F = F
or by KL
g F = F. It is slighly less natural to work with left actions because ΠL : g 7→KL
g and
ΠL
∗: ξ 7→LL
ξ are Lie group and Lie algebra anti-homomorphisms, that is,
ΠL(g1g2) = ΠL(g2)ΠL(g1)
and
ΠL
∗
 [ξ, η]

=

ΠL
∗(η), ΠL
∗(ξ)

.
(90)
9.1 Vector fields
Here we study the symmetries of a vector field V ∈X(M) under a right G-action θ : M × G →M.
This allows us to extend the discussion in Section 6.3 to dynamical systems described by vector
fields on smooth manifolds and acted upon nonlinearly by arbitrary Lie groups. The tangent map
of the diffeomorhpism θg = θ(·, g) : M →M transforms vector fields via the pushforward map
(θg)∗: X(M) →X(M) defined by
((θg)∗V )p·g = d θg(p)Vp
(91)
for every p ∈M.
Definition 24 Given g ∈G, we say that a vector field V ∈X(M) is g-invariant if and only if
(θg)∗V = V , that is,
Vp·g = d θg(p)Vp
∀p ∈M.
(92)
28

Because (θg−1)∗(θg)∗= (θg−1 ◦θg)∗= IdX(M), it is clear that a vector field is g-invariant if and only
if it is g−1-invariant.
Recall that vector fields V ∈X(M) are smooth sections of the tangent bundle E = TM. The
right G-action θ on M induces a right G-action Θ : TM × G →TM on the tangent bundle defined
by
Θg(vp) = d θg(p)vp.
(93)
It is easy to see that each Θg is a vector bundle homomorphism descending to θg under the bundle
projection π. Crucially, we have
KgV = Θg−1 ◦V ◦θg = (θg−1)∗V,
(94)
meaning that a vector field V ∈X(M) is g-invariant if and only if it is g-equivariant as a section of
TM with respect to the action Θ. Recall that (by Lemma 20.14 in Lee (2013)) the left-invariant
vector field ξ ∈Lie(G) ⊂X(G) and its infinitesimal generator ˆθ(ξ) ∈X(M) are θ(p)-related,
where θ(p) : g 7→θ(p, g) is the orbit map. This means that θexp(tξ) is the time-t flow of ˆθ(ξ) by
Proposition 9.13 in Lee (2013). As a result, the Lie derivative in Eq. 83 agrees with the standard Lie
derivative of V along ˆθ(ξ) (see Lee (2013, p.228)), that is,
LξV (p) = lim
t→0
1
t
h
d θexp(−tξ)(θexp(tξ)(p))Vθexp(tξ)(p) −Vp
i
= [ˆθ(ξ), V ]p,
(95)
where the expression on the right is the Lie bracket of ˆθ(ξ) and V .
9.2 Tensor fields
Symmetries of a tensor field can also be revealed using our framework. This will be useful for our
study of Hamiltonian dynamics in Section 9.3 and for our study of integral operators, whose kernels
can be viewed as tensor fields, in Section 9.4. For simplicity, we consider a rank-k covariant tensor
field A ∈Tk(M), although our results extend to contravariant and mixed tensor fields with minimal
modification. We rely on the basic definitions and machinery found in Lee (2013, Chapter 12). Under
a right G-action θ on M, the tensor field transforms via the pullback map θ∗
g : Tk(M) →Tk(M)
defined by
(θ∗
gA)p(v1, . . . , vk) = (d θg(p)∗Ap·g)(v1, . . . , vk) = Ap·g(d θg(p)v1, . . . , d θg(p)vk)
(96)
for every v1, . . . , vk ∈TpM.
Definition 25 Given g ∈G, a tensor field A ∈Tk(M) is g-invariant if and only if θ∗
gA = A, that
is,
Ap·g(d θg(p)v1, . . . , d θg(p)vk) = Ap(v1, . . . , vk)
∀p ∈M.
(97)
To study the invariance of tensor fields in our framework, we recall that a tensor field is a section
of the tensor bundle E = T kT ∗M = `
p∈M(T ∗
p M)⊗k, a vector bundle over M, where T ∗
p M is the
dual space of TpM. The right G-action θ on M induces a right G-action Θ : T kTM × G →T kTM
defined by
Θg(Ap) = d θg−1(θg(p))∗Ap.
(98)
29

It is clear that each Θg is a homomorphism of the vector bundle T kT ∗M descending to θg under
the bundle projection. Crucially, we have
KgA = Θg−1 ◦A ◦θg = θ∗
gA,
(99)
meaning that A ∈Tk(M) is g-invariant if and only if it is g-equivariant as a section of T kT ∗M with
respect to the action Θ. Since θexp(tξ) gives the time-t flow of the vector field ˆθ(ξ) ∈X(M), the Lie
derivative in Eq. 83 for this action agrees with the standard Lie derivative of A ∈Tk(M) along ˆθ(ξ)
(see Lee (2013, p.321)), that is
(LξA)p = lim
t→0
1
t
h
d θexp(tξ)(p)∗Aθexp(tξ)(p) −Ap
i
= (Lˆθ(ξ)A)p.
(100)
The Lie derivative for arbitrary covariant tensor fields can be computed by applying Proposition 12.32
in Lee (2013) and its corollaries. More generally, thanks to 6.16-18 in Kolář et al. (1993), the Lie
derivative for any tensor product of sections of natural vector bundles can be computed via the
formula
Lξ(A1 ⊗A2) = (LξA1) ⊗A2 + A1 ⊗(LξA2).
(101)
For example, this holds when A1, A2 are arbitrary smooth tensor fields of mixed types. The Lie
derivative of a differential form ω on M can be computed by Cartan’s magic formula
Lξω = ˆθ(ξ) ⌟(d ω) + d(ˆθ(ξ) ⌟ω),
(102)
where d is the exterior derivative.
9.3 Hamiltonian dynamics
The dynamics of frictionless mechanical systems can be described by Hamiltonian vector fields on
symplectic manifolds. Roughly speaking, these encompass systems that conserve energy, such as
motion of rigid bodies and particles interacting via conservative forces. The celebrated theorem of
Noether (1918) says that conserved quantities of Hamiltonian systems correspond with symmetries
of the energy function (the system’s Hamiltonian). In this section, we briefly illustrate how to
enforce Hamiltonicity constraints on learned dynamical systems and how to promote, discover, and
enforce conservation laws. A thorough treatment of classical mechanics, symplectic manifolds, and
Hamiltonian systems can be found in Abraham and Marsden (2008); Marsden and Ratiu (1999).
This includes methods for reduction of systems with known symmetries and conservation laws. The
following brief introduction follows Chapter 22 of Lee (2013).
Hamiltonian systems are defined on symplectic manifolds. That is, a smooth even-dimensional
manifold S together with a smooth nondegenerate differential 2-form ω, called the symplectic form.
Nondegeneracy means that the map ˆωp : v 7→ωp(v, ·) is a bijective linear map of TpS onto its dual
T ∗
p S for every p ∈S. Thanks to nondegeneracy, any smooth function H ∈C∞(S) gives rise to a
smooth vector field
VH = ˆω−1(d H)
(103)
called the “Hamiltonian vector field” of H. A vector field V ∈X(S) is said to be Hamiltonian if
V = VH for some function H, called the Hamiltonian of V . A vector field is locally Hamiltonian if it
is Hamiltonian in neighborhood of each point of S.
The symplectic manifolds considered in classical mechanics usually consist of the cotangent
bundle S = T ∗M of an m-dimensional manifold M describing the “configuration” of the system,
30

e.g., the positions of particles. The cotangent bundle has a canonical symplectic form given by
ω =
m
X
i=1
d xi ∧d ξi,
(104)
where (xi, ξi) are any choice of natural coordinates on a patch of T ∗M (see Proposition 22.11 in
Lee (2013)). Here, each xi is a generalized coordinate describing the configuration and ξi is its
“conjugate” or “generalized” momentum. The Darboux theorem (Theorem 22.13 in Lee (2013)) says
that any symplectic form on a manifold can be put into the form of Eq. 104 by a choice of local
coordinates. In these “Darboux” coordinates, the dynamics of a Hamiltonian system are governed by
the equations
d
d t xi = VH(xi) = ∂H
∂ξi
,
d
d t ξi = VH(ξi) = −∂H
∂xi ,
(105)
which should be familiar to anyone who has studied undergraduate mechanics.
Enforcing local Hamiltonicity on a vector field V ∈X(S) is equivalent to the linear constraint
LV ω = 0
(106)
thanks to Proposition 22.17 in Lee (2013). Here LV is the Lie derivative of the tensor field ω ∈T2(S),
i.e., Eq. 100 with θ being the flow of V and its generator being the identity ˆθ(V ) = V . Note that
the Lie derivative still makes sense even when the orbits t 7→θt(p) = θexp(t1)(p) are only defined for
small t ∈(−ε, ε). In Darboux coordinates, this constraint is equivalent to the set of equations
∂V (xi)
∂xj
+ ∂V (ξj)
∂ξi
= 0,
∂V (ξi)
∂xj
−∂V (ξj)
∂xi
= 0,
∂V (xi)
∂ξj
−∂V (xj)
∂ξi
= 0
(107)
for all 1 ≤i, j ≤m. When the first de Rham cohomology group satisfies H1
dR(S) = 0, for example
when S is contractible, local Hamilonicity implies the existence of a global Hamiltonian for V , unique
on each component of S up to addition of a constant by Lee (2013, Proposition 22.17).
Of course our approach also makes it possible to promote Hamiltonicity with respect to candidate
symplectic structures when learning a vector field V . To do this, we can penalize the nuclear norm
of LV restricted to a subspace ˜Ωof candidate 2-forms using the regularization function
R˜Ω,∗(V ) =
 LV |˜Ω

∗.
(108)
The strength of this penalty can be increased when solving a regression problem for V until there is
a non-degenerate 2-form in the nullspace Null(LV ) ∩˜Ω. This gives a symplectic form with respect
to which V is locally Hamiltonian.
Another option is to learn a (globally-defined) Hamiltonian function H directly by fitting VH to
data. In this case, we can regularize the learning problem by penalizing the breaking of conservation
laws. The time-derivative of a quantity, that is, a smooth function f ∈C∞(S) under the flow of VH
is given by the Poisson bracket
{f, H} := ω(Vf, VH) = d f(VH) = VH(f) = −Vf(H).
(109)
Hence, f is a conserved quantity if and only if H is invariant under the flow of Vf — this is Noether’s
theorem. It is also evident that the Poisson bracket is linear with respect to both of its arguments.
In fact, the Poisson bracket turns C∞(S) into a Lie algebra with f 7→Vf being a Lie algebra
homomorphism, i.e., V{f,g} = [Vf, Vg].
31

As a result of these basic properties of the Poisson bracket, the quantities conserved by a given
Hamiltonian vector field VH form a Lie subalgebra given by the nullspace of a linear operator
LH : C∞(S) 7→C∞(S) defined by
LH : f 7→{f, H}.
(110)
To promote conservation of quantities in a given subalgebra g ⊂C∞(S) when learning a Hamiltonian
H, we can penalize the nuclear norm of LH restricted to g, that is
Rg,∗(H) =
 LH|g

∗.
(111)
For example, we might expect a mechanical system to conserve angular momentum about some axes,
but not others due to applied torques. In the absence of data to the contrary, it often makes sense
to assume that various linear and angular momenta are conserved.
9.4 Equivariant integral operators
In this section we provide machinery to study the symmetries of linear and nonlinear integral
operators acting on sections of vector bundles, yielding far-reaching generalizations of our results
in Section 5.2. Such operators can form the layers of neural networks acting on various vector and
tensor fields supported on manifolds.
Let π0 : E0 →M0 and πj : Ej →Mj be vector bundles with Mj being dj-dimensional orientable
Riemannian manifolds with volume forms dVj ∈Ωdj(T ∗Mj), j = 1, . . . , r. Note that here, dVj does
not denote the exterior derivative of a (dj −1)-form. A section K of the bundle
E = E0 ⊗E∗
1 ⊗· · · ⊗E∗
r :=
a
(p,q1,...,qr)∈M0×···×Mr
E0,p ⊗E∗
1,q1 ⊗· · · ⊗E∗
r,qr
(112)
can be viewed as a smooth family of r-multilinear maps
K(p, q1, . . . , qr) :
r
M
j=1
Ej,qj →E0,p.
(113)
The section K can serve as the kernel to define an r-multilinear integral operator TK : D(TK) ⊂
Lr
j=1 Σ(Ej) →Σ(E0) with action on (F1, . . . , Fr) ∈D(TK) given by
TK[F1, . . . , Fr](p) =
Z
M1×···×Mr
K(p, q1, . . . , qr)

F1(q1), . . . , Fr(qr)

dV1(q1) ∧· · · ∧dVr(qr). (114)
This operator is linear when r = 1. When r > 1 and E1 = · · · = Er, Eq. 114 can be used to define a
nonlinear integral operator Σ(E1) →Σ(E0) with action F 7→TK[F, . . . , F].
Given fiber-linear right G-actions Θj : Ej × G →Ej, there is an induced fiber-linear right
G-action Θ : E × G →E on E defined by
Θg(Kp,q1,...,qr)

v1, . . . , vr

= Θ0,g
 Kp,q1,...,qr

Θ1,g−1(v1), . . . , Θr,g−1(vr)

(115)
for Kp,q1,...,qr ∈E viewed as an r-multilinear map E1,q1 ⊕· · · ⊕Er,qr →E0,p and vj ∈Ej,θj,g(qj).
Sections Fj ∈Σ(Ej) transform according to
KEj
g Fj = Θj,g−1 ◦Fj ◦θj,g,
(116)
32

with the section defining the integral kernel transforming according to
KE
g K(p, q1, . . . , qr)[vq1, . . . , vqr] =
Θ0,g−1
n
K
 θ0,g(p), θ1,g(q1), . . . , θr,g(qr)

Θ1,g(vq1), . . . , Θr,g(vqr)
o
.
(117)
Using these transformation laws, we define equivariance for the integral operator as follows:
Definition 26 The integral operator in Eq. 114 is equivariant with respect to g ∈G if
KE0
g TK

KE1
g−1F1, . . . , KEr
g−1Fr

= TK

F1, . . . , Fr

(118)
for every (F1, . . . , Fr) ∈D(TK).
Using the fact that the integral is invariant under pullbacks by diffeomorphisms, we can express
the left-hand-side of the equivariance condition as
K0,gTK

K1,g−1F1, . . . , Kr,g−1Fr

(p) =
Θ0,g−1
( Z
M1×···×Mr
K(θ0,g(p), q1, . . . , qr)

Θ1,g ◦F1 ◦θ1,g−1(q1), . . . , Θr,g ◦Fr ◦θr,g−1(qr)

dV1(q1) ∧· · · ∧dVr(qr).
)
=
Θ0,g−1
( Z
M1×···×Mr
K(θ0,g(p), θ1,g(q1), . . . , θr,g(qr))

Θ1,g ◦F1(q1), . . . , Θr,g ◦Fr(qr)

θ∗
1,g dV1(q1) ∧· · · ∧θ∗
r,g dVr(qr).
)
=
Z
M1×···×Mr
KE
g K(p, q1, . . . , qr)

F1(q1), . . . , Fr(qr)

θ∗
1,g dV1(q1) ∧· · · ∧θ∗
r,g dVr(qr). (119)
Hence, the condition for equivariance of TK is equivalent to
Kg
 K dV1 ∧· · · ∧Vr

:= KE
g (K) θ∗
1,g dV1 ∧· · · ∧θ∗
r,g dVr = K dV1 ∧· · · ∧dVr,
(120)
where we note that KΩ
g (dVj) = θ∗
j,g dVj is the natural transformation of the differential form
dVj ∈Ωdj(T ∗Mj) (a covariant tensor field) described in Section 9.2. The Lie derivative of the
action on volume forms is given by
LΩ
ξ dV = d
 ˆθ(ξ) ⌟dV

= div ˆθ(ξ) dV
(121)
thanks to Cartan’s magic formula and the definition of divergence (see Lee (2013)). Therefore,
differentiating Eq. 120 along the curve g(t) = exp(tξ) yields the Lie derivative
Lξ
 K dV1 ∧· · · ∧dVr

=

LE
ξ (K) + K
r
X
j=1
div ˆθj(ξ)

dV1 ∧· · · ∧dVr .
(122)
For the integral operators discussed in Section 5.2, the formulas derived here recover Eqs. 31 and 33.
33

10 Invariant submanifolds and tangency
Studying the symmetries of maps can be cast into a more general framework in which we study
the symmetries of submanifolds. Specifically, the symmetries of a map F : M0 →M1 between
manifolds correspond to symmetries of its graph, gr(F), and the symmetries of a section of a vector
bundle F ∈Σ(E) correspond to symmetries of its image, im(F) — both of which are properly
embedded submanifolds of M0 × M1 and E, respectively. We show that symmetries of a large class
of submanifolds, including the above, are revealed by checking whether the infinitesimal generators
of the group action are tangent to the submanifold. In this setting, the Lie derivative of F ∈Σ(E)
has a geometric interpretation as a projection of the infintesimal generator onto the tangent space of
the image im(F), viewed as a submanifold of the bundle.
10.1 Symmetry of submanifolds
In this section we study the infinitesimal conditions for a submanifold to be invariant under the
action of a Lie group. Suppose that N is a manifold and θ : N × G →N is a right action of a
Lie group G on N. Sometimes we denote this action by p · g = θ(p, g) when there is no ambiguity.
Though our results also hold for left actions, as we discuss later in Remark 32, working with right
actions is standard in this context and allows us to leverage results from Lee (2013) more naturally
in our proofs. Fixing p ∈N, the orbit map of this action is denoted θ(p) : G →N. Fixing g ∈G,
the map θg : N →N defined by θg : p 7→θ(p, g) is a diffeomorphism with inverse θg−1.
Definition 27 A subset M ⊂N is G-invariant if and only if θ(p, g) ∈M for every g ∈G and
p ∈M.
Sometimes we will denote M · G = {θ(p, g) : p ∈M, g ∈G}, in which case G-invariance of M can
be stated as M · G ⊂M.
We study the group invariance of submanifolds of the following type:
Definition 28 Let M be a weakly embedded m-dimensional submanifold of an n-dimensional mani-
fold N. We say that M is arcwise-closed if any smooth curve γ : [a, b] →N satisfying γ((a, b)) ⊂M
must also satisfy γ([a, b]) ⊂M.
Submanifolds of this type include all properly embedded submanifolds of N because properly
embedded submanifolds are closed subsets (Proposition 5.5 in Lee (2013)). More interestingly, we
have the following:
Proposition 29 The leaves of any (nonsingular) foliation of N are arcwise-closed. We provide a
proof in Appendix A.
This means that the kinds of submanifolds we are considering include all possible Lie subgroups
(Lee (2013, Theorem 19.25)) as well as their orbits under free and proper group actions (Lee (2013,
Proposition 21.7)). The leaves of singular foliations associated with integrable distributions of
nonconstant rank (see Kolář et al. (1993, Sections 3.18–25)) can fail to be arcwise-closed. For
example, the distribution spanned by the vector field x ∂
∂x on R has maximal integral manifolds
(−∞, 0), {0}, and (0, ∞) forming a singular foliation of R. Obviously, the leaves (−∞, 0) and (0, ∞)
are not arcwise-closed.
The following theorem provides necessary and sufficient conditions for arcwise-closed weakly-
embedded submanifolds to be G-invariant.
34

Theorem 30 Let M be an arcwise-closed weakly-embedded submanifold of N and let θ : N ×G →N
be a right action of a Lie group G on N with infinitesimal generator ˆθ : Lie(G) →X(N). Let G0 be
the identity component of G. Then M is G0-invariant if and only if
ˆθ(ξ)p ∈TpM
∀p ∈M
∀ξ ∈Lie(G).
(123)
If, in addition, we have M · gi ⊂M for a single gi from each non-identity component of G, then M
is G-invariant. A proof is provided in Appendix F.
Since the infinitesimal generator ˆθ is a linear map and TpM is a subspace of TpN, the tangency
condition expressed in Eq. 123 can be viewed as a set of linear constraints satisfied by the elements
of the Lie algebra.
Given a candidate Lie group G and a submanifold M ⊂N with unknown symmetries, we can
ask which elements of the Lie algebra Lie(G) satisfy the tangency conditions. The following theorem
shows that these elements form the subalgebra corresponding to the largest connected Lie subgroup
of symmetries of M in G.
Theorem 31 Let M be an immersed submanifold of N and let θ : N × G →N be a right action
of a Lie group G on N with infinitesimal generator ˆθ : Lie(G) →X(N). Then
symG(M) =

ξ ∈Lie(G) : ˆθ(ξ)p ∈TpM
∀p ∈M
	
(124)
is the Lie subalgebra of a unique connected Lie subgroup SymG(M)0 ⊂G. If M is weakly-embedded
and arcwise-closed in N, then this subgroup has the following properties:
(i) M · SymG(M)0 ⊂M
(ii) If H is a connected Lie subgroup of G such that M · H ⊂M, then H ⊂SymG(M)0.
If M is properly embedded in N then SymG(M)0 is the identity component of the closed, properly
embedded Lie subgroup
SymG(M) = {g ∈G : M · g ⊂M}.
(125)
A proof is provided in Appendix G.
Remark 32 (Left actions) When the group G acts on N from the left according to θL : G × N →
N, we can always construct an equivalent right-action θR : N × N →N by setting θR(p, g) =
θL(g−1, p). The corresponding infinitesimal generators are related by ˆθR = −ˆθL. Since ˆθL(ξ)p ∈TpM
if and only if ˆθR(ξ)p ∈TpM, Theorems 30 and 31 hold without modification for left G-actions.
10.2 The Lie derivative as a projection
We provide a geometric interpretation of the Lie derivative in Eq. 83 by expressing it in terms of a
projection of the infinitesimal generator of the group action onto the tangent space of im(F). This
allows us to connect the Lie derivative to the tangency conditions for symmetry of submanifolds
presented in Section 10.1.
The Lie derivative LξF(p) lies in Ep, while TF(p) im(F) is a subspace of TF(p)E. To relate
quantities in these different spaces, the following lemma introduces a lifting of each Ep to a subspace
of TF(p)E.
35

Lemma 33 For every section F ∈Σ(E) there is a well-defined injective vector bundle homomorphism
ıF : E →TE that is expressed in any local trivialization Φ : π−1(U) →U × Rk as
d Φ ◦ıF ◦Φ−1 : U × Rk →T(U × Rk)
(p, v) 7→(0, v)Φ(F(p)).
(126)
We give a proof in Appendix H.
This is a special case of the “vertical lift” of E into the vertical bundle V E = {v ∈TE : d πv = 0}
described by Kolář et al. (1993) in Section 6.11. The “vertical projection” vprE : V E →E provides
a left-inverse satisfying vprE ◦ıF = IdE.
The following result relates the Lie derivative to a projection via the vertical lifting.
Theorem 34 Given F ∈Σ(E) and p ∈M, the map PF(p) := d(F ◦π)(F(p)) : TF(p)E →TF(p)E is
a linear projection onto TF(p) im(F) and for every ξ ∈Lie(G) we have
ıF ◦(LξF)(p) = −

I −PF(p)
ˆΘ(ξ)F(p).
(127)
We give a proof in Appendix H.
For the special case of smooth maps F : Rm →Rn viewed a sections x 7→(x, F(x)) of the bundle
π : Rm × Rn →Rm, this theorem reproduces Eq. 46. The following corollary provides a link between
our main results for sections of vector bundles and our main results for symmetries of submanifolds.
Corollary 35 For every F ∈Σ(E), ξ ∈Lie(G), and p ∈M we have
(LξF)(p) = 0
⇔
ˆΘ(ξ)F(p) ∈TF(p) im(F).
(128)
In particular, this means that Theorems 21 and 22 are special cases of Theorems 30 and 31.
11 Conclusion
This paper provides a unified theoretical approach to enforce, discover, and promote symmetries in
machine learning models. In particular, we provide theoretical foundations for Lie group symmetry
in machine learning from a linear-algebraic viewpoint. This perspective unifies and generalizes
several leading approaches in the literature, including approaches for incorporating and uncovering
symmetries in neural networks and more general machine learning models. The central objects
in this work are linear operators describing the finite and infinitesimal transformations of smooth
sections of vector bundles with fiber-linear Lie group actions. To make the paper accessible to a wide
range of practitioners, Sections 3–8 deal with the special case where the machine learning models are
built using smooth functions between vector spaces acted upon by matrix Lie group representations.
Our main results establish that the infinitesimal operators — the Lie derivatives — fully encode the
connected subgroup of symmetries for sections of vector bundles (resp. functions between vector
spaces). In other words, the Lie derivatives encode symmetries that the machine learning models are
equivariant with respect to. We illustrate that promoting and enforcing continuous symmetries in
large classes of machine learning models are dual problems with respect to the bilinear structure
of the Lie derivative. Moreover, we describe how symmetries can be promoted as inductive biases
during training of these models using convex penalties based on the fundamental operators. Finally,
we show how the linear algebraic framework for discovering continuous symmetries extends to identify
continuous symmetries of arbitrary submanifolds. When the submanifold is the image of a section of
36

a vector bundle (resp., the graph of a function between vector spaces) the fundamental operator for
the submanfold recovers the Lie derivative. We describe rigorous data-driven methods for discretizing
and approximating the fundamental operators to accomplish the tasks of enforcing, promoting,
and discovering symmetry. Importantly, these theoretical concepts, while extremely general, admit
efficient computational implementations via simple linear algebra.
The main limitations of our approach come from the need to make appropriate choices for key
objects including the candidate group G, the space of functions F defining the machine learning
model, and appropriate inner products for discretizing the fundamental operators. For example, it is
possible that the only G-symmetric functions in F are trivial, meaning that enforcing symmetry
results in learning only trivial models. One open question is whether our framework can be used in
such cases to learn relaxed symmetries, as described by Wang et al. (2022). In other words, we may
hope to find elements in F that are nearly symmetric, and to bound the degree of asymmetry based
on the quantities derived from the fundamental operators, such as their norms. Our proposed convex
penalization methods are likely affected by the choice of G and the discretization of the fundamental
operators. A natural question is whether a large candidate group of symmetries can be used without
biasing the model towards undesirable or nonphysical symmetries. Additionally, the choice of inner
products associated with the discretization of the fundamental operators could affect the results of
nuclear norm penalization. Our reliance on the Lie algebra to study continuous symmetries also
limits the ability of our proposed methods to account for partial symmetries, such as the invariance
in classifying the characters “Z” and “N” to rotations by small angles, but not large angles.
In a follow-up paper we will apply the proposed methods to a wide range of examples, explaining
in detail how to implement the methods in practice. A main goal will be to study to what extent
the nuclear norm relaxation can recover underlying symmetry groups and reduce the amount of data
required to train accurate machine learning models. Additionally, we will examine how the proposed
techniques perform in the presence of noisy data, with the goal of understanding the empirical
effects of problem dimension, noise level, and the candidate symmetry group. The follow-up will also
explore different choices of inner products in the discretization of the fundamental operators and the
effect this has on the performance of the proposed symmetry-promoting regularization methods.
Other important avenues of future work include investigating computationally efficient approaches
to discretize the fundamental operators and use them to enforce, discover, and promote symmetry
within our framework. This could involve leveraging sparse structure of the discretized operators
in certain bases to enable the use of efficient Krylov subspace algorithms. It will also be useful to
identify efficient optimization algorithms for training symmetry-constrained or symmetry-regularized
machine learning models. Promising candidates include projected gradient descent, proximal splitting
algorithms, and the Iteratively Reweighted Least Squares (IRLS) algorithms described by Mohan
and Fazel (2012). Using IRLS could enable symmetry-promoting penalties to be based on non-convex
Schatten p-norms with 0 < p < 1, potentially improving the recovery of underlying symmetry groups
compared to the nuclear norm where p = 1.
There are also several avenues we plan to explore in future theoretical work. These include
extending the techniques presented here via jet bundle prolongation to study symmetries in machine
learning for Partial Differential Equations (PDEs). Combining analogues of our proposed methods
in this setting with techniques using the weak formulation proposed by Messenger and Bortz (2021);
Reinbold et al. (2020) could provide robust ways to identify symmetric PDEs in the presence
of high noise and limited training data. We also aim to study the perturbative effects of noisy
data in algorithms to discover and promote symmetry with the goal of understanding the effects
of problem dimension, noise level, and number of data points on recovery of symmetry groups.
Another important direction of theoretical study will be to build on the work of Peitz et al. (2023);
Steyert (2022) by studying symmetry in the setting of Koopman operators for dynamical systems.
37

The connection between the fundamental operators in this paper and Koopman operators may be
strengthened by establishing that the fundamental operators {Kg}g∈G form a strongly continuous
group in an L2 space of sections of the appropriate vector bundle, and that the corresponding
infinitesimal generators are given by closures of the Lie derivatives. One could then show that the
Koopman operators of symmetric systems commute with these generators, and use this information
to constrain the spectral projections of the Koopman operators. This might be used to discover
underlying symmetries based on Koopman operators, as well as to enhance algorithms to approximate
Koopman operators by enforcing and promoting symmetry. To do this, one might follow the program
set forth by Colbrook (2023), where the measure preserving property of certain dynamical systems is
exploited to enhance the Extended Dynamic Mode Decomposition (EDMD) algorithm of Williams
et al. (2015).
Acknowledgements
The authors acknowledge support from the National Science Foundation AI Institute in Dynamic
Systems (grant number 2112085). SLB acknowledges support from the Army Research Office (ARO
W911NF-19-1-0045). The authors would also like to acknowledge valuable discussions with Tess
Smidt and Matthew Colbrook.
References
R. Abraham, J. E. Marsden, and T. Ratiu. Manifolds, Tensor Analysis, and Applications, volume 75
of Applied Mathematical Sciences. Springer-Verlag, 1988.
Ralph Abraham and Jerrold E Marsden. Foundations of mechanics. AMS Chelsea Publishing, 2
edition, 2008.
Amir Ali Ahmadi and Bachir El Khadir.
Learning dynamical systems with side information.
In Proceedings of the 2nd Conference on Learning for Dynamics and Control, volume 120 of
Proceedings of Machine Learning Research, pages 718–727. PMLR, 10–11 Jun 2020. URL https:
//proceedings.mlr.press/v120/ahmadi20a.html.
Peter J Baddoo, Benjamin Herrmann, Beverley J McKeon, J Nathan Kutz, and Steven L Brunton.
Physics-informed dynamic mode decomposition. Proceedings of the Royal Society A, 479(2271):
20220576, 2023.
Simon Batzner, Albert Musaelian, Lixin Sun, Mario Geiger, Jonathan P Mailoa, Mordechai Kornbluth,
Nicola Molinari, Tess E Smidt, and Boris Kozinsky. E(3)-equivariant graph neural networks for
data-efficient and accurate interatomic potentials. Nature communications, 13(1):2453, 2022.
Gregory Benton, Marc Finzi, Pavel Izmailov, and Andrew G Wilson. Learning invariances in neural
networks from training data. Advances in neural information processing systems, 33:17605–17616,
2020.
Tyrus Berry and Dimitrios Giannakis. Spectral exterior calculus. Communications on Pure and
Applied Mathematics, 73(4):689–770, 2020.
Thierry Bouwmans, Sajid Javed, Hongyang Zhang, Zhouchen Lin, and Ricardo Otazo. On the
applications of robust PCA in image and video processing. Proceedings of the IEEE, 106(8):
1427–1457, 2018.
38

S. L. Brunton and J. N. Kutz. Data-Driven Science and Engineering: Machine Learning, Dynamical
Systems, and Control. Cambridge University Press, 2nd edition, 2022.
Steven L Brunton, Joshua L Proctor, and J Nathan Kutz. Discovering governing equations from
data by sparse identification of nonlinear dynamical systems. Proceedings of the national academy
of sciences, 113(15):3932–3937, 2016.
Steven L. Brunton, Bernd R. Noack, and Petros Koumoutsakos. Machine learning for fluid mechanics.
Annual Review of Fluid Mechanics, 52:477–508, 2020.
Steven L Brunton, Marko Budišić, Eurika Kaiser, and J Nathan Kutz. Modern Koopman theory for
dynamical systems. SIAM Review, 64(2):229–340, 2022.
Jameson Cahill, Dustin G Mixon, and Hans Parshall. Lie PCA: Density estimation for symmetric
manifolds. Applied and Computational Harmonic Analysis, 2023.
Jared L Callaham, Georgios Rigas, Jean-Christophe Loiseau, and Steven L Brunton. An empirical
mean-field model of symmetry-breaking in a turbulent wake. Science Advances, 8(eabm4786),
2022.
Emmanuel J Candès, Xiaodong Li, Yi Ma, and John Wright. Robust principal component analysis?
Journal of the ACM (JACM), 58(3):1–37, 2011.
Richard Caron and Tim Traynor.
The zero set of a polynomial.
WSMR Report 05-02,
2005.
URL
https://www.researchgate.net/profile/Richard-Caron-3/publication/
281285245_The_Zero_Set_of_a_Polynomial/links/55df56b608aecb1a7cc1a043/
The-Zero-Set-of-a-Polynomial.pdf.
Kathleen Champion, Peng Zheng, Aleksandr Y Aravkin, Steven L Brunton, and J Nathan Kutz. A
unified sparse optimization framework to learn parsimonious physics-informed models from data.
IEEE Access, 8:169259–169271, 2020.
Shuxiao Chen, Edgar Dobriban, and Jane H. Lee. A group-theoretic framework for data augmentation.
J. Mach. Learn. Res., 21(1), jan 2020. ISSN 1532-4435.
Taco Cohen and Max Welling. Group equivariant convolutional networks. In Maria Florina Balcan
and Kilian Q. Weinberger, editors, Proceedings of The 33rd International Conference on Machine
Learning, volume 48 of Proceedings of Machine Learning Research, pages 2990–2999, New York,
New York, USA, 20–22 Jun 2016. PMLR. URL https://proceedings.mlr.press/v48/cohenc16.
html.
Taco S Cohen, Mario Geiger, Jonas Köhler, and Max Welling. Spherical CNNs. arXiv preprint
arXiv:1801.10130, 2018.
Taco S Cohen, Mario Geiger, and Maurice Weiler.
A general theory of equivariant CNNs on
homogeneous spaces. Advances in neural information processing systems, 32, 2019.
Matthew J Colbrook. The mpEDMD algorithm for data-driven computations of measure-preserving
dynamical systems. SIAM Journal on Numerical Analysis, 61(3):1585–1608, 2023.
Ekin D Cubuk, Barret Zoph, Dandelion Mane, Vijay Vasudevan, and Quoc V Le. Autoaugment:
Learning augmentation strategies from data. In Proceedings of the IEEE/CVF conference on
computer vision and pattern recognition, pages 113–123, 2019.
Krish Desai, Benjamin Nachman, and Jesse Thaler. Symmetry discovery with deep learning. Physical
Review D, 105(9):096031, 2022.
39

Carlos Esteves, Christine Allen-Blanchette, Ameesh Makadia, and Kostas Daniilidis. Learning SO(3)
equivariant representations with spherical CNNs. In Proceedings of the European Conference on
Computer Vision (ECCV), pages 52–68, 2018.
Marc Finzi, Samuel Stanton, Pavel Izmailov, and Andrew Gordon Wilson. Generalizing convolutional
neural networks for equivariance to Lie groups on arbitrary continuous data. In International
Conference on Machine Learning, pages 3165–3176. PMLR, 2020.
Marc Finzi, Max Welling, and Andrew Gordon Wilson.
A practical method for constructing
equivariant multilayer perceptrons for arbitrary matrix groups. In International Conference on
Machine Learning, pages 3318–3328. PMLR, 2021.
Kunihiko Fukushima. Neocognitron: A self-organizing neural network model for a mechanism of
pattern recognition unaffected by shift in position. Biological cybernetics, 36(4):193–202, 1980.
Morikuni Gotô. Faithful representations of Lie groups II. Nagoya mathematical journal, 1:91–107,
1950.
David Gross. Recovering low-rank matrices from few coefficients in any basis. IEEE Transactions
on Information Theory, 57(3):1548–1566, 2011.
David J Gross. The role of symmetry in fundamental physics. Proceedings of the National Academy
of Sciences, 93(25):14256–14259, 1996.
Nate Gruver, Marc Anton Finzi, Micah Goldblum, and Andrew Gordon Wilson. The Lie derivative
for measuring learned equivariance.
In The Eleventh International Conference on Learning
Representations, 2022.
Yifei Guan, Steven L Brunton, and Igor Novosselov. Sparse nonlinear models of chaotic electrocon-
vection. Royal Society Open Science, 8(8):202367, 2021.
Brian C. Hall. Lie Groups, Lie Algebras, and Representations: An Elementary Introduction. Springer,
2015.
Ryuichiro Hataya, Jan Zdenek, Kazuki Yoshizoe, and Hideki Nakayama. Faster autoaugment:
Learning augmentation strategies using backpropagation. In Computer Vision–ECCV 2020: 16th
European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part XXV 16, pages 1–16.
Springer, 2020.
P. J. Holmes, J. L. Lumley, G. Berkooz, and C. W. Rowley.
Turbulence, coherent structures,
dynamical systems and symmetry. Cambridge Monographs in Mechanics. Cambridge University
Press, Cambridge, England, 2nd edition, 2012.
Roger A. Horn and Charles R. Johnson. Matrix Analysis. Cambridge University Press, 2 edition,
2013.
Eurika Kaiser, J Nathan Kutz, and Steven L Brunton. Discovering conservation laws from data for
control. In 2018 IEEE Conference on Decision and Control (CDC), pages 6415–6421. IEEE, 2018.
Eurika Kaiser, J Nathan Kutz, and Steven L Brunton. Data-driven discovery of koopman eigenfunc-
tions for control. Machine Learning: Science and Technology, 2(3):035023, 2021.
Ivan Kolář, Peter W. Michor, and Jan Slovák. Natural Operations in Differential Geometry. Springer-
Verlag, 1993.
40

Risi Kondor and Shubhendu Trivedi. On the generalization of equivariance and convolution in neural
networks to the action of compact groups. In International Conference on Machine Learning,
pages 2747–2755. PMLR, 2018.
B. O. Koopman. Hamiltonian systems and transformations in Hilbert space. Proceedings of the
National Academy of Sciences, 17:315–318, 1931.
Leonid B. Koralov and Yakov G. Sinai. Theory of Probability and Random Processes. Springer, 2
edition, 2012.
Yann LeCun, Bernhard Boser, John S Denker, Donnie Henderson, Richard E Howard, Wayne
Hubbard, and Lawrence D Jackel. Backpropagation applied to handwritten zip code recognition.
Neural computation, 1(4):541–551, 1989.
John M. Lee. Introduction to Smooth Manifolds: Second Edition. Springer, 2013.
Mario Lezcano-Casado and David Martınez-Rubio. Cheap orthogonal constraints in neural networks:
A simple parametrization of the orthogonal and unitary group. In International Conference on
Machine Learning, pages 3794–3803. PMLR, 2019.
Ziming Liu and Max Tegmark. Machine learning hidden symmetries. Phys. Rev. Lett., 128:180201,
May 2022. doi: 10.1103/PhysRevLett.128.180201. URL https://link.aps.org/doi/10.1103/
PhysRevLett.128.180201.
J.-C. Loiseau and S. L. Brunton. Constrained sparse Galerkin regression. Journal of Fluid Mechanics,
838:42–67, 2018.
Haggai Maron, Heli Ben-Hamu, Nadav Shamir, and Yaron Lipman. Invariant and equivariant graph
networks. arXiv preprint arXiv:1812.09902, 2018.
J. E. Marsden and T. S. Ratiu. Introduction to mechanics and symmetry. Springer-Verlag, 2nd
edition, 1999.
Alexandre Mauroy, Y Susuki, and I Mezić. Koopman operator in systems and control. Springer,
2020.
Daniel A Messenger and David M Bortz. Weak SINDy: Galerkin-based data-driven model selection.
Multiscale Modeling & Simulation, 19(3):1474–1497, 2021.
Carl D Meyer. Matrix analysis and applied linear algebra, volume 71. Siam, 2000.
Igor Mezić. Spectral properties of dynamical systems, model reduction and decompositions. Nonlinear
Dynamics, 41:309–325, 2005.
Xu Miao and Rajesh PN Rao. Learning the Lie groups of visual invariance. Neural computation, 19
(10):2665–2693, 2007.
Karthik Mohan and Maryam Fazel. Iterative reweighted algorithms for matrix rank minimization.
Journal of Machine Learning Research, 13(1):3441–3473, 2012.
Artem Moskalev, Anna Sepliarskaia, Ivan Sosnovik, and Arnold Smeulders. LieGG: Studying learned
Lie group generators. Advances in Neural Information Processing Systems, 35:25212–25223, 2022.
E Noether.
Invariante variationsprobleme nachr. d. könig. gesellsch. d. wiss. zu göttin-
gen, math-phys. klasse 1918:
235-257.
English Reprint:
physics/0503066, http://dx. doi.
org/10.1080/00411457108231446, page 57, 1918.
41

Samuel E Otto and Clarence W Rowley. Koopman operators for estimation and control of dynamical
systems. Annual Review of Control, Robotics, and Autonomous Systems, 4:59–87, 2021.
Sebastian Peitz, Hans Harder, Feliks Nüske, Friedrich Philipp, Manuel Schaller, and Karl Worthmann.
Partial observations, coarse graining and equivariance in Koopman operator theory for large-scale
dynamical systems. arXiv preprint arXiv:2307.15325, 2023.
Maziar Raissi, Paris Perdikaris, and George Em Karniadakis. Physics-informed neural networks:
A deep learning framework for solving forward and inverse problems involving nonlinear partial
differential equations. Journal of Computational physics, 378:686–707, 2019.
Rajesh Rao and Daniel Ruderman. Learning Lie groups for invariant visual perception. Advances in
neural information processing systems, 11, 1999.
Benjamin Recht, Maryam Fazel, and Pablo A. Parrilo. Guaranteed minimum-rank solutions of linear
matrix equations via nuclear norm minimization. SIAM Review, 52(3):471–501, 2010.
Patrick AK Reinbold, Daniel R Gurevich, and Roman O Grigoriev. Using noisy or incomplete data
to discover models of spatiotemporal dynamics. Physical Review E, 101(010203), 2020.
Clarence W. Rowley, Ioannis G. Kevrekidis, Jerrold E. Marsden, and Kurt Lust. Reduction and
reconstruction for self-similar dynamical systems. Nonlinearity, 16(4):1257, 2003.
Connor Shorten and Taghi M Khoshgoftaar. A survey on image data augmentation for deep learning.
Journal of big data, 6(1):1–48, 2019.
Vivian T Steyert. Uncovering Structure with Data-driven Reduced-Order Modeling. PhD thesis,
Princeton University, 2022.
Terence Tao. Two small facts about Lie groups. https://terrytao.wordpress.com/2011/06/25/
two-small-facts-about-lie-groups/, 6 2011.
David A Van Dyk and Xiao-Li Meng. The art of data augmentation. Journal of Computational and
Graphical Statistics, 10(1):1–50, 2001.
Rui Wang, Robin Walters, and Rose Yu. Approximately equivariant networks for imperfectly
symmetric dynamics. In International Conference on Machine Learning, pages 23078–23091.
PMLR, 2022.
Maurice Weiler and Gabriele Cesa. General E(2)-equivariant steerable CNNs. Advances in neural
information processing systems, 32, 2019.
Maurice Weiler, Mario Geiger, Max Welling, Wouter Boomsma, and Taco S Cohen. 3D steer-
able CNNs: Learning rotationally equivariant features in volumetric data. Advances in Neural
Information Processing Systems, 31, 2018.
Matthew O Williams, Ioannis G Kevrekidis, and Clarence W Rowley. A data-driven approximation
of the Koopman operator: extending dynamic mode decomposition. Journal of Nonlinear Science,
6:1307–1346, 2015.
Jianke Yang, Robin Walters, Nima Dehmamy, and Rose Yu. Generative adversarial symmetry
discovery. arXiv preprint arXiv:2302.00236, 2023.
Ming Yuan and Yi Lin. Model selection and estimation in regression with grouped variables. Journal
of the Royal Statistical Society: Series B (Statistical Methodology), 68(1):49–67, 2006.
42

Appendix A. Proofs of minor results
Proof [Proposition 6] Obviously, if KgK = K then K(W)
g
◦TK ◦K(V)
g−1 = TK. On the other hand,
suppose that KgK(x0, y0) ̸= K(x0, y0) for some (x0, y0) ∈Rn × Rm. Hence, there are vectors v ∈V
and w ∈W∗such that ⟨w, KgK(x0, y0)v −K(x0, y0)v⟩> 0. This remains true for all y in a
neighborhood U of y0 by continuity of K and KgK. Letting F(x) = vφ(x) where φ is a smooth,
nonnegative, function with φ(y0) > 0 and support in U, we obtain

w, K(W)
g
◦TK ◦K(V)
g−1F(x) −TKF(x)

=
Z
Rm

w, KgK(x, y)v −K(x, y)v

φ(y) d y > 0,
(129)
meaning K(W)
g
◦TK ◦K(V)
g−1 ̸= TK. Therefore, KgK = K if and only if K(W)
g
◦TK ◦K(V)
g−1 = TK.
Proof [Proposition 15] As an intersection of closed subgroups, H := TL
l=1 SymG
 F (l)
is a closed
subgroup of G. By the closed subgroup theorem (see Theorem 20.12 in Lee (2013)), H is an embedded
Lie subgroup, whose Lie subalgebra we denote by h. If ξ ∈h then exp(tξ) ∈SymG
 F (l)
for all
t ∈R and every l = 1, . . . , L. Differentiating Kexp(tξ)F (l) = F (l) at t = 0 proves that LξF (l) = 0, i.e.,
ξ ∈symG(F (l)) by Theorem 4. Conversely, if LξF (l) = 0 for every l = 1, . . . , L, then by Theorem 4,
exp(tξ) ∈H. Since H is a Lie subgroup, differentiating exp(tξ) at t = 0 proves that ξ ∈h.
Proof [Proposition 16] Let f1, . . . , fN be a basis for F′. Consider the sequence of Gram matrices
GGGM with entries
[GGGM]i,j = ⟨fi, fj⟩L2(µM) .
(130)
It suffices to show that GGGM is positive-definite for sufficiently large M. Since the L2(µ) inner product
is positive-definite on F′, it follows that the Gram matrix GGG with entries
[GGG]i,j = ⟨fi, fj⟩L2(µ)
(131)
is positive-definite. Hence, its smallest eigenvalue λmin(GGG) is positive. Since the ordered eigen-
values of symmetric matrices are continuous with respect to their entries (see Corollary 4.3.15
in Horn and Johnson (2013)) and [GGGM]i,j →[GGG]i,j for all 1 ≤i, j ≤N by assumption, we have
λmin(GGGM) →λmin(GGG) as M →∞. Therefore, there is an M0 so that for every M ≥M0 we have
λmin(GGGM) > 0, i.e., GGGM is positive-definite.
Proof [Proposition 29] Consider a leaf M of an m-dimensional foliation on the n-dimensional
manifold N and let γ : [a, b] →N be a smooth curve satisfying γ((a, b)) ⊂M. First, it is clear
that M is a weakly embedded submanifold of N since M is an integral manifold of an involutive
distribution (Lee (2013, Proposition 19.19)) and the local structure theorem for integral manifolds
(Lee (2013, Proposition 19.16)) shows that they are weakly embedded.
By continuity of γ, any neighborhood of γ(b) in N must have nonempty intersection with M. By
definition of a foliation (see Lee (2013)), there is a coordinate chart (U, x) for N with γ(b) ∈U such
that x(U) is a coordinate-aligned cube in Rn and M ∩U consists of countably many slices of the
form xm+1 = cm+1, . . . , xn = cn for constants cm+1, . . . , cn. Since γ is continuous, there is a δ > 0
so that γ((b −δ, b]) ⊂U, and in particular, γ((b −δ, b)) ⊂M ∩U. By continuity of γ, there are
constants cm+1, . . . , cn such that xi(γ(t)) = ci for every i = m + 1, . . . , n and t ∈(b −δ, b). Hence,
43

we have
xi(γ(b)) = lim
t→b xi(γ(t)) = ci,
i = m + 1, . . . , n,
(132)
meaning that γ(b) ∈M. An analogous argument shows that γ(a) ∈M, completing the proof that
M is arcwise-closed.
Appendix B. Proof of Proposition 17
Our proof relies on the following lemma:
Lemma 36 Let P denote a finite-dimensional vector space of polynomials Rm →R. If M ≥dim(P)
then the evaluation map T : P →RM defined by
T(x1,...,xM) : P 7→(P(x1), . . . , P(xM))
(133)
is injective for almost every (x1, . . . , xM) ∈(Rm)M with respect to Lebesgue measure.
Proof Letting M0 = dim(P) and choosing a basis P1, . . . , PM0 for P, injectivity of T(x1,...,xM) is
equivalent to injectivity of the M × M0 matrix
TTT (x1,...,xM) =


P1(x1)
· · ·
PM0(x1)
...
...
...
P1(xM)
· · ·
PM0(xM)

.
(134)
Finally, this is equivalent to
ϕ(x1, . . . , xM) = det
 (TTT (x1,...,xM))TTTT (x1,...,xM)

(135)
taking a nonzero value. We observe that ϕ is a polynomial on the Euclidean space (Rm)M.
Suppose there exists a set of points (¯x1, . . . , ¯xM) ∈(Rm)M such that T(¯x1,...,¯xM) is injective. Then
for this set ϕ(¯x1, . . . , ¯xM) ̸= 0. Obviously, ϕ(0, . . . , 0) = 0, meaning that ϕ cannot be constant.
Thanks to the main result in Caron and Traynor (2005), this means that each level set of ϕ has zero
Lebesgue measure in (Rm)M. In particular, the level set ϕ−1(0), consisting of those x1, . . . , xM for
which T(x1,...,xM) fails to be injective, has zero Lebesgue measure. Therefore, it suffices to prove that
there exists (¯x1, . . . , ¯xM) ∈(Rm)M such that T(¯x1,...,¯xM) is injective. We do this by induction.
It is clear that there exists ¯x1 so that the 1 × 1 matrix
TTT 1 =

P1(¯x1)

(136)
has full rank since P1 cannot be the zero polynomial. Proceeding by induction, we assume that there
exists ¯x1, . . . , ¯xs so that
TTT s =


P1(¯x1)
· · ·
Ps(¯x1)
...
...
...
P1(¯xs)
· · ·
Ps(¯xs)


(137)
44

has full rank. Suppose that the matrix
˜T˜T˜T s+1(x) =


P1(¯x1)
· · ·
Ps(¯x1)
Ps+1(¯x1)
...
...
...
...
P1(¯xs)
· · ·
Ps(¯xs)
Ps+1(¯xs)
P1(x)
· · ·
Ps(x)
Ps+1(x)


(138)
has rank < s + 1 for every x ∈Rm. Since the upper left s × s block of ˜T˜T˜T s+1(x) is TTT s, we must
always have rank( ˜T˜T˜T s+1(x)) = s. The nullspace of ˜T˜T˜T s+1(x) is contained in the nullspace of the upper
s × (s + 1) block of ˜T˜T˜T s+1(x). Since both nullspaces are one-dimensional, they are equal. The upper
s × (s + 1) block of ˜T˜T˜T s+1(x) does not depend on x, so there is a fixed nonzero vector v ∈Rs+1 so
that ˜T˜T˜T s+1(x)v = 0 for every x ∈Rm. The last row of this expression reads
v1P1(x) + · · · + vs+1Ps+1(x) = 0
∀x ∈Rm,
(139)
contradicting the linear independence of P1, . . . , Ps+1. Therefore there exists ¯xs+1 so that TTT s+1 =
˜T˜T˜T s+1(¯xs+1) has full rank. It follows by induction on s that there exists ¯x1, . . . ¯xM0 ∈Rm so that
TTT (¯x1,...¯xM0) = TTT M0 has full rank. Choosing any M −M0 additional vectors yields an injective
TTT (¯x1,...¯xM), which completes the proof.
Proof [Proposition 17] The sum in Eq. 75 clearly defines a symmetric, positive-semidefinite bilinear
form on F′. It remains to show that this bilinear form is positive-definite. Suppose that there
is a function f ∈F′ such that ⟨f, f⟩L2(µM) = 0. Thanks to Lemma 36, our assumption that
M ≥dim(πi(F′)) means that the evaluation operator T(x1,...,xM) is injective on πi(F′) for almost
every (x1, . . . , xM) ∈(Rm)M with respect to Lebesgue measure. Since a countable (in this case
finite) intersection of sets of measure zero has measure zero, it follows that for almost every
(x1, . . . , xM) ∈(Rm)M with respect to Lebesgue measure, T(x1,...,xM) is injective on every πi(F′),
i = 1, . . . , n. Defining the positive diagonal matrix
D
D
D =
1
√
N


√w1
...
√wM

,
(140)
and using Eq. 75 yields
0 = ⟨f, f⟩L2(µM) =
n
X
j=1
 D
D
DT(x1,...,xM)πjf
TD
D
DT(x1,...,xM)πjf.
(141)
This implies that T(x1,...,xM)πjf = 0 for j = 1, . . . , n. Since T(x1,...,xM) is injective on each πj(F′) it
follows that each πjf = 0, meaning that f = 0. This completes the proof.
Appendix C. Proof of Proposition 20
We begin by proving
d
d t Kexp(tξ)F = LξKexp(tξ)F = Kexp(tξ)LξF.
(142)
45

The first equality follows from the composition law for the operators Kg and the definition of the Lie
derivative in Eq. 83, yielding
d
d t

t=t0
Kexp(tξ)F = lim
t→0
1
t

Kexp(tξ)Kexp(t0ξ)F −Kexp(t0ξ)F

= LξKexp(t0ξ)F.
(143)
To prove the second equality, we choose p ∈M, let p′ = θexp(t0ξ)(p), and compute
d
d t

t=t0
 Kexp(tξ)F

(p) = lim
t→0
1
t
 Kexp(t0ξ)Kexp(tξ)F

(p) −
 Kexp(t0ξ)F

(p)

= lim
t→0
1
t Θexp(−t0ξ) ◦
 Kexp(tξ)F −F

◦θexp(t0ξ)(p)
= Θexp(−t0ξ)

lim
t→0
1
t
 Kexp(tξ)F

(p′) −F(p′)

= Θexp(−t0ξ)
 LξF(p′)

=
 Kexp(t0ξ)LξF

(p).
(144)
In the third equality, we used the fact that Θexp(−t0ξ) is a continuous vector bundle homomorphism.
Next, we prove
Lαξ+βη = αLξ + βLη.
(145)
To do this, we choose F ∈Σ(E), p ∈M, and define the map h : G →Ep by
h : g 7→KgF(p) = Θ
 F(θ(p, g)), g−1
.
(146)
As a composition of smooth maps, h is smooth, and its derivative at the identity is
d h(e)ξe = d
d t

t=0
h(exp(tξ)) = LξF(p)
(147)
for every ξe ∈TeG ∼= Lie(G). Since the derivative is linear, it follows that ξ 7→LξF(p) is linear.
Finally, we prove that
L[ξ,η] = 1
2
d2
d t2

t=0
Kexp(tξ)Kexp(tη)Kexp(−tξ)Kexp(−tη) = LξLη −LηLξ.
(148)
Recall that Flt
ξ : g 7→g · exp(tξ) gives the flow of the left-invariant vector field ξ ∈Lie(G) (see
Theorem 4.18(3) in Kolář et al. (1993)). By Theorem 3.16 in Kolář et al. (1993) the curve γ : R →G
given by
γ(t) = Flt
−η ◦Flt
−ξ ◦Flt
η ◦Flt
ξ(e) = exp(tξ) exp(tη) exp(−tξ) exp(−tη).
(149)
satisfies γ(0) = e, γ′(0) = 0, and
1
2γ′′(0) = [ξ, η]e ∈TeG
(150)
in the sense that γ′′(0) : f 7→(f ◦γ)′′(0) is a derivation on C∞(G). Composing with the map in
Eq. 146 yields
0 = d h(e)γ′(0) = (h ◦γ)′(0) = d
d t

t=0
Kγ(t)F(p).
(151)
46

Combining Eq. 150 and Eq. 147 (noting the definition of the tangent map d h(e) acting on derivations,
as in Kolář et al. (1993), Lee (2013)) gives
L[ξ,η]F(p) = 1
2 d h(e)γ′′(0) = 1
2(h ◦γ)′′(0) = 1
2
d2
d t2

t=0
Kγ(t)F(p).
(152)
This proves the first equality in Eq. 148 thanks to the composition law
Kγ(t) = Kexp(tξ)Kexp(tη)Kexp(−tξ)Kexp(−tη).
(153)
To differentiate the above expression, we use the following observations. If Ft ∈Σ(E) is such that
(t, p) 7→Ft(p) is smooth for (t, p) ∈R×M, then obviously
d
d t Ft ∈Σ(E) with the usual identification
TEp ∼= Ep. Moreover, we have
d
d t KgFt(p) = d
d t Θg−1
 Ft(θg(p))

= Θg−1
 d
d t Ft(θg(p))

= Kg
 d
d t Ft

(p)
(154)
because Ft(θg(p)) ∈Eθg(p) for all t ∈R and Θg−1 is linear on Eθg(p). Using this, we obtain
d
d t LξFt(p) = d
d t
d
d τ

τ=0
Kexp(τξ)Ft(p)
=
d
d τ

τ=0
d
d t Kexp(τξ)Ft(p)
=
d
d τ

τ=0
Kexp(τξ)
 d
d t Ft

(p) = Lξ
 d
d t Ft

(p)
(155)
because (t, τ) 7→Kexp(τξ)Ft(p) lies in the vector space Ep, allowing us to exchanged the order of
differentiation. Since (t1, t2, t3, t4) 7→Kexp(tξ)Kexp(tη)Kexp(−tξ)Kexp(−tη)F(p) lies in the vector space
Ep for all (t1, t2, t3, t4) ∈R4, we can apply the chain rule and Eq. 154 to obtain
d
d t Kγ(t) =
∂
∂t1

t1=t
Kexp(t1ξ)Kexp(tη)Kexp(−tξ)Kexp(−tη)
+ Kexp(tξ)
∂
∂t2

t2=t
Kexp(t2η)Kexp(−tξ)Kexp(−tη)
+ Kexp(tξ)Kexp(tη)
∂
∂t3

t3=t
Kexp(−t3ξ)Kexp(−tη)
+ Kexp(tξ)Kexp(tη)Kexp(−tξ)
∂
∂t4

t4=t
Kexp(−t4η). (156)
47

Using Eq. 142 gives
d
d t Kγ(t) = Lξ
Kγ(t)
z
}|
{
Kexp(tξ)Kexp(tη)Kexp(−tξ)Kexp(−tη)
+ Kexp(tξ)LηKexp(tη)Kexp(−tξ)Kexp(−tη)
+ Kexp(tξ)Kexp(tη)L−ξKexp(−tξ)Kexp(−tη)
+ Kexp(tξ)Kexp(tη)Kexp(−tξ)Kexp(−tη)
|
{z
}
Kγ(t)
L−η. (157)
Applying the same technique to differentiate a second time and using the linearity in Eq. 145 to
cancel terms yields
d2
d t2

t=0
Kγ(t) = Lξ
d
d t

t=0
Kγ(t)
|
{z
}
0
+ LξLη + LηL−ξ + LηL−ξ + L−ξL−η
|
{z
}
2
 LξLη−LηLξ

+ d
d t

t=0
Kγ(t)
|
{z
}
0
L−η,
(158)
which completes the proof.
■
Appendix D. Proof of Theorem 21
If F ∈Σ(E) is G0-equivariant, then Kexp(tξ)F = F for all ξ ∈Lie(G) and t ∈R. Differentiating
with respect to t at t = 0 gives LξF = 0. Conversely, suppose that LξF = 0 for all ξ ∈Lie(G).
Choosing ξ ∈Lie(G) and p ∈M, the smooth curve γ : R →Ep defined by γ(t) = Kexp(tξ)F(p)
satisfies γ(0) = F(p) and
γ′(t) = d
d t Kexp(tξ)F(p) = Kexp(tξ)LξF(p) = 0 ∈Ep
(159)
thanks to Eq. 84 of Proposition 20.
Therefore, γ(t) = F(p) for all t ∈R, and in particular
γ(1) = F(p), meaning that
Kexp(ξ)F = F,
∀ξ ∈Lie(G).
(160)
We show by induction that KgF = F for every finite product g = hm, . . . , h1 of elements hi = exp(ξi).
Supposing this holds for any product g of m elements in the range of the exponential map and
choosing hm+1 = exp(ξm+1) we have
Khm+1gF = Khm+1KgF = Khm+1F = F,
(161)
proving the claim. The conclusion of the theorem now follows immediately from Lemma 37, below,
since G is the union of its connected components.
■
Lemma 37 Let G0 be the identity component of a Lie group G. Then every element g ∈G0 can
be expressed as a finite product g = hm · · · h1 of elements hi = exp(ξi) for ξi ∈Lie(G). Let Gi be a
connected component of G and let gi ∈Gi. Then every element g ∈Gi can be expressed as g = g0gi
for some g0 ∈G0.
Proof By the inverse function theorem (more specifically by Proposition 20.8(f) in Lee (2013)), the
range of the exponential map contains an open, connected neighborhood U of the identity element
e ∈G. The inverses of the elements in U also belong to the range of the exponential map thanks
48

to Proposition 20.8(c) in Lee (2013). By Proposition 7.14(b) and Proposition 7.15 in Lee (2013),
it follows that U generates the identity component G0 of G. That is, any element g ∈G0 can be
written as a finite product of elements in U and their inverses, which proves the first claim.
By Proposition 7.15 in Lee (2013), G0 is a normal subgroup of G and every connected component
of G is diffeomorphic to G0. In fact in the proof of this result it is shown that every connected
component of G is a coset of G0. Therefore, if Gi is a non-identity connected component of G and
gi ∈Gi then Gi = G0 · gi, which proves the second claim.
Appendix E. Proof of Theorem 22
We begin by showing that SymG(F) is a closed subgroup of G. It is obviously a subgroup, for if
g1, g2 ∈SymG(F) then
Kg1g2F = Kg1Kg2F = Kg1F = F,
(162)
meaning that g1g2 ∈SymG(F). To show that SymG(F) is closed, we observe that for each p ∈M,
the map hp : G →E defined by
hp : g 7→KgF(p) = Θ
 F(θ(p, g)), g−1
(163)
is smooth, as it is a composition of smooth maps. As F(p) is a single point in E, the preimage set
h−1
p
 {F(p)}

is closed in G. Since SymG(F) is an intersection,
SymG(F) =
\
p∈M
h−1
p
 {F(p)}

,
(164)
of closed sets, it follows that SymG(F) is closed in G. By the closed subgroup theorem (Theorem 20.12
in Lee (2013)) it follows that SymG(F) is an embedded Lie subgroup of G.
Let h = Lie(SymG(F)) be the Lie algebra of SymG(F). By Theorem 21, (or simply by differ-
entiating Kexp(tξ)F = F with respect to t) we have LξF = 0 for every ξ ∈h. This means that
h ⊂symG(F), as defined by Eq. 88. To show the reverse containment, choose ξ ∈symG(F), and
observe that Eq. 84 in Proposition 20 implies that
d
d t Kexp(tξ)F = Kexp(tξ)LξF = 0
∀t ∈R.
(165)
It follows that Kexp(tξ)F = F, that is, exp(tξ) ∈SymG(F) for all t ∈R. Differentiating at t = 0
proves that ξ ∈h. Therefore, h = symG(F), completing the proof.
■
Appendix F. Proof of Theorem 30
Our proof of the theorem relies on the following technical lemma concerning the integral curves of
vector fields tangent to weakly embedded, arcwise-closed submanifolds.
Lemma 38 Let M be an arcwise-closed weakly embedded submanifold of a manifold N.
Let
V ∈X(N) be a vector field tangent to M, that is
Vp ∈TpM
∀p ∈M.
(166)
If γ : I →N is a maximal integral curve of V that intersects M, then γ lies in M.
49

Proof By the translation lemma (Lemma 9.4 in Lee (2013)), we can assume without loss of generality
that 0 ∈I and p0 = γ(0) ∈M. Let ıM : M ,→N denote the inclusion map. Since M is an immersed
submanifold of N and V is tangent to M, there is a unique smooth vector field V |M ∈X(M) that
is ıM-related to V thanks to Proposition 8.23 in Lee (2013). Let ˜γ : ˜I →M be the maximal integral
curve of V |M with ˜γ(0) = p0. By the naturality of integral curves (Proposition 9.6 in Lee (2013))
ıM ◦˜γ is an integral curve of V with ıM ◦˜γ(0) = p0. Since integral curves of smooth vector fields
starting at the same point are unique (Theorem 9.12, part (a) in Lee (2013)) we have ˜I ⊂I and
ıM ◦˜γ(t) = γ(t)
∀t ∈˜I.
(167)
Therefore, it remains to show that ˜I = I.
By the local existence of integral curves (Proposition 9.2 in Lee (2013)), the domains I and ˜I of
the maximal integral curves γ and ˜γ are open intervals in R. Suppose, for the sake of producing
a contradiction, that there exists t ∈I with t > ˜I. Then it follows that the least upper bound
b = sup ˜I is an element of I. By Eq. 167 and continuity of γ we have
q0 = γ(b) = lim
t→b ıM ◦˜γ(t).
(168)
Since M is arcwise-closed, it follows that q0 ∈M.
To complete the proof, we use the local existence of an integral curve for V |M starting at q0 to
contradict the maximality of ˜γ. By the local existence of integral curves (Proposition 9.2 in Lee
(2013)) and the translation lemma (Lemma 9.4 in Lee (2013)), there is an ε > 0 and an integral
curve ˆγ : (b −ε, b + ε) →M of V |M such that ˆγ(b) = q0 = γ(b). Shrinking the interval, we take
0 < ε < b −a. Again, by nauturality and uniqueness of integral curves we must have ıM ◦ˆγ(t) = γ(t)
for all t ∈(b −ε, b + ε). Hence, by Eq. 167 and injectivity of ıM it follows that ˆγ(t) = ˜γ(t) for
all t ∈(b −ε, b). Applying the gluing lemma (Corollary 2.8 in Lee (2013)) to ˜γ and ˆγ yields an
extension of ˜γ to the larger open interval ˜I ∪(b −ε, b + ε). Since this contradicts the maximality of
˜γ, there is no t ∈I with t > ˜I. The same argument shows that there is no t ∈I with t < ˜I, and so
we must have ˜I = I.
Proof [Theorem 30] First, suppose that M is G0-invariant. In particular, this means that for every
p ∈M and ξ ∈Lie(G), the smooth curve γ(p)
ξ
: R →N defined by
γ(p)
ξ (t) = p · exp(tξ)
(169)
lies in M. Since M is weakly embedded in N, γ(p)
ξ
is also smooth as a map into M. Specifically,
there is a smooth curve ˜γ(p)
ξ
: R →M so that γ(p)
ξ
= ıM ◦˜γ(p)
ξ
where ıM : M ,→N is the inclusion
map. Differentiating at t = 0 yields
ˆθ(ξ)p = d
d t γ(p)
ξ (t)

t=0
= d ıM(p) d
d t ˜γ(p)
ξ (t)

t=0
,
(170)
which lies in TpM = Range (d ıM(p)).
Conversely, suppose that the tangency condition expressed in Eq. 123 holds. By Lemma 20.14
in Lee (2013) the vector fields ξ ∈Lie(G) and ˆξ = ˆθ(ξ) are θ(p)-related for every p ∈N. By the
50

naturality of integral curves (Proposition 9.6 in Lee (2013)) it follows that γ(p)
ξ
: R →N defined by
γ(p)
ξ (t) = p · exp(tξ)
(171)
is the unique maximal integral curve of ˆξ passing through p at t = 0. When p ∈M, this integral
curve lies in M thanks to Lemma 38. This means that M is invariant under the action of any group
element in the range of the exponential map on G.
Proceeding by induction, suppose that G is invariant under the action of any product of m group
elements in the range of the exponential map. If g = h1 · · · hm · hm+1 is a product of m + 1 elements
hi ∈Range(exp) ⊂G, then it follows from associativity and the induction hypothesis that
M · (h1 · · · hm · hm+1) = (M · h1 · · · hm) · hm+1 ⊂M · hm+1 ⊂M.
(172)
Therefore, M is invariant under the action of any finite product of group elements in the range of
the exponential map by induction on m. Thanks to Lemma 37, it follows that M is G0-invariant,
proving the first claim of the theorem.
Now suppose in addition that for each connected component Gi of G there is an element gi ∈Gi
satisfying M · gi ⊂M. By Lemma 37, we have Gi = G0 · gi. Therefore,
M · Gi = M · G0 · gi ⊂M · gi ⊂M,
(173)
proving that M is invariant under Gi. Since G is the union of its connected components it follows
that M is G-invariant.
Appendix G. Proof of Theorem 31
The set symG(M) is a subspace of Lie(G), for if ξ1, ξ2 ∈symG(M) and a1, a2 ∈R then
ˆθ(a1ξ1 + a2ξ2)p = a1 ˆθ(ξ1)
| {z }
∈TpM
+a2 ˆθ(ξ2)
| {z }
∈TpM
∈TpM
(174)
thanks to linearity of the infinitesimal generator ˆθ. To show that symG(M) is a Lie subalgrebra, we
must show that it is also closed under the Lie bracket. Recall that ˆθ is a Lie algebra homomorphism
(see Theorem 20.15 in Lee (2013)), and so ˆθ([ξ1, ξ2]) = [ˆθ(ξ1), ˆθ(ξ1)]. Since the Lie bracket of
two vector fields tangent to an immersed submanifold is also tangent to the submanifold (see
Corollary 8.32 in Lee (2013)), it follows that [ˆθ(ξ1), ˆθ(ξ1)] is tangent to M. Hence, symG(M) is
closed under the Lie bracket and is therefore a Lie subalgebra of Lie(G). By Theorem 19.26 in
Lee (2013), there is a unique connected Lie subgroup of SymG(M)0 ⊂G whose Lie subalgebra is
symG(M).
Now suppose that M is weakly embedded and arcwise-closed in N. Since SymG(M)0 is connected,
it is equal to its identity component. Applying Theorem 30 to the induced action of SymG(M)0
on M, we immediately obtain M · SymG(M)0 ⊂M. Suppose that H is another connected Lie
subgroup of G such that M · H ⊂M. Choosing any p ∈M and ξ ∈Lie(H), we have
p · exp(tξ) ∈M
∀t ∈R.
(175)
51

Since M is weakly embedded in N, this defines a smooth curve γ : R →M such that ıM ◦γ(t) =
p · exp(tξ). Differentiating and using the definition of the infinitesimal generator gives
ˆθ(ξ)p = d
d t

t=0
p · exp(tξ) = d ıM(p) d
d t

t=0
γ(t) ∈TpM.
(176)
Therefore, Lie(H) ⊂symG(M) which implies that H ⊂SymG(M)0 by Theorem 19.26 in Lee (2013).
Now suppose that M is properly embedded in N and denote
SymG(M) = {g ∈G : M · g ⊂M} =
\
p∈M
 θ(p)−1(M).
(177)
The equality of these expressions is a simple matter of unwinding their definitions. It is clear that
SymG(M) is a subgroup of G, for if g1, g2 ∈SymG(M) then the composition law for the group
action gives M · (g1 · g2) = (M · g1) · g2 ⊂M · g1 ⊂M. Since M is properly embedded, it is closed
in M (see Lee (2013, Proposition 5.5)), meaning that each preimge set
 θ(p)−1(M) is closed in G
by continuity of θ(p). As an intersection of closed subsets, it follows that SymG(M) is closed in G.
By the closed subgroup theorem (Lee (2013, Theorem 20.12)), SymG(M) is a properly embedded
Lie subgroup of G. The same holds for the identity component H0 of SymG(M) since H0 is closed
in SymG(M), which implies that H0 is closed in G thanks to the closure of SymG(M) in G.
Next we show that SymG(M)0 = H0 is the identity component of SymG(M). First, we observe
that SymG(M)0 ⊂H0 because SymG(M)0 is connected and contained in SymG(M). The reverse
containment follows from the fact that H0 is a connected Lie subgroup satisfying M · H0 ⊂M,
which by our earlier result implies that H0 ⊂SymG(M)0.
■
Appendix H. Proof of Theorem 34
Proof [Lemma 33] The map ıF defined in a local trivialization Φ by Eq. 126 is injective. It is a
vector bundle homomorphism because d Φ ◦ıF ◦Φ−1, Φ, and d Φ are vector bundle homomorphisms
and Φ and d Φ are invertible. It remains to show that the definition of ıF does not depend on the
choice of local trivialization. Given two local trivializations Φ and ˜Φ defined on π−1(U) ⊂E where
U is an open subset of M, it suffices to show that the following diagram commutes:
U × Rk
π−1(U) ⊂E
U × Rk
T(U × Rk)
T(π−1(U)) ⊂TE
T(U × Rk)
ıF
ıΦ◦F
ı˜Φ◦F
Φ
d Φ
˜Φ
d ˜Φ
(178)
Since ˜Φ ◦Φ−1 is a bundle homomorphism descending to the identity, it can be written as
˜Φ ◦Φ−1 : (p, v) 7→(p,TTT(p)v)
(179)
for a matrix-valued function TTT : U →Rk×k. Moreover, the matrices are invertible because the local
trivializations are bundle isomorphisms. Differentiating, we obtain
d ˜Φ ◦d Φ−1 : (wp, w)(p,v) 7→
 wp, dTTT(p)wp + TTT(p)w

˜Φ◦Φ−1(p,v),
(180)
52

where wp ∈TpU. Composing this with ıΦ◦F : (p, v) 7→(0, v)Φ(F(p)), we obtain
d ˜Φ ◦d Φ−1 ◦ıΦ◦F (p, v) = (0,TTT(p)v)˜Φ(F(p)) = ı˜Φ◦F (p,TTT(p)v) = ı˜Φ◦F ◦˜Φ ◦Φ−1(p, v),
(181)
proving that the diagram commutes.
Proof [Theorem 34] We observe that F ◦π : E →E is a smooth idempotent map whose image is
im(F) ⊂E. By differentiating the expression (F ◦π) ◦(F ◦π) = F ◦π at a point F(p) ∈im(F), we
obtain
d(F ◦π)(F(p)) d(F ◦π)(F(p)) = d(F ◦π)(F(p)),
(182)
meaning that d(F ◦π)(F(p)) : TF(p)E →TF(p)E is a linear projection. Since
d(F ◦π)(F(p)) = d F(p) d π(F(p)),
(183)
we have Range
 d(F ◦π)(F(p))

⊂Range(d F(p)) = TF(p) im(F). Differentiating F = (F ◦π) ◦F
yields
d F(p) = d(F ◦π)(F(p)) d F(p),
(184)
meaning that TF(p) im(F) ⊂Range
 d(F ◦π)(F(p))

. Since Range
 d(F ◦π)(F(p))

= TF(p) im(F)
it follows that d(F ◦π)(F(p)) is a linear projection onto TF(p) im(F).
We observe that the generalized Lie derivative in Eq. 83 can be expressed as
(LξF)(p) = lim
t→0
1
t

Θexp(−tξ)(F(θexp(tξ)(p))) −F(p)

= lim
t→0 Θ

exp(−tξ), 1
t

F(θexp(tξ)(p)) −Θexp(tξ)(F(p))

= lim
t→0
1
t

F(θexp(tξ)(p)) −Θexp(tξ)(F(p))

.
(185)
The first equality follows because Θg−1 is a vector bundle homomorphism, meaning that the
restricted map Θg−1|Ep·g : Ep·g →Ep is linear; here g = exp(tξ). The second equality follows because
Θ : E × G →E is continuous. Note that in the first expression the limit is taken in the vector space
Ep, whereas in the last expression the limit must be taken in E.
We proceed by expressing everything in a local trivialization Φ : π−1(U) →U × Rk of an open
neighborhood U ⊂M of p ∈M. Since the maps Θg, Φ, and Φ−1 are vector bundle homomorphisms,
there is a matrix-valued function TTT g : U →Rk×k such that
˜Θg = Φ ◦Θg ◦Φ−1 : (p, v) 7→(θg(p), TTT g(p)v).
(186)
Differentiating ˜Θexp(tξ)(p, v) with respect to t yields the generator
ˆ˜Θ(ξ)(p,v) = d
d t

t=0
˜Θexp(tξ)(p, v) =

ˆθ(ξ)p, ˆTˆTˆT(ξ)pv

(p,v) ,
(187)
where ˆTˆTˆT(ξ)p =
d
d t

t=0 TTT exp(tξ)(p). We define the function ˜
F : U →Rk by
(p, ˜
F (p)) = Φ ◦F(p)
∀p ∈U.
(188)
53

Using the above definitions, we can express the generalized Lie derivative in the local trivialization:
Φ ◦(LξF)(p) = lim
t→0

θexp(tξ)(p), 1
t
h
˜
F (θexp(tξ)(p)) −TTT exp(tξ) ˜
F (p)
i
=

p, d ˜
F (p)ˆθ(ξ)p −ˆTˆTˆT(ξ)p ˜
F (p)

.
(189)
Applying Lemma 33 allows us to express the left-hand-side of Eq. 127 as
d Φ [ıF ◦(LξF)(p)] =

0, d ˜
F (p)ˆθ(ξ)p −ˆTˆTˆT(ξ)p ˜
F (p)

Φ(F(p)) .
(190)
We can also express the quantities on the right-hand-side of Eq. 127 in the local trivialization. To
do this, we compute
d Φ(F(p)) d(F ◦π)(F(p))ˆΘ(ξ)F(p) = d
d t

t=0
Φ ◦F ◦π ◦Θexp(tξ)(F(p))
= d
d t

t=0
Φ ◦F(θexp(tξ)(p))
=

ˆθ(ξ)p, d ˜
F (p)ˆθ(ξ)p

Φ(F(p))
(191)
and
d Φ(F(p))ˆΘ(ξ)F(p) = d
d t

t=0
Φ ◦Θexp(tξ) ◦Φ−1 ◦Φ ◦F(p)
= ˆ˜Θ(ξ)Φ(F(p)) =

ˆθ(ξ)p, ˆTˆTˆT(ξ)p ˜
F (p)

Φ(F(p)) .
(192)
Subtracting these yields
d Φ
h
d(F ◦π)(F(p)) −IdTF (p)E

ˆΘ(ξ)F(p)
i
=

0, d ˜
F (p)ˆθ(ξ)p −ˆTˆTˆT(ξ)p ˜
F (p)

Φ(F(p)) ,
(193)
which, upon comparison with Eq. 190 completes the proof.
54

