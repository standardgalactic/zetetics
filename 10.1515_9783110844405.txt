Computational Statistics 
Editors: Herbert Biining • Peter Naeve 


Computational Statistics 
Editors: 
Herbert Büning • Peter Naeve 
W 
DE 
G 
Walter de Gruyter • Berlin • New York 1981 

CIP-Kurztitelaufnahme der Deutschen Bibliothek 
Computational statistics: [Wolfgang Wetzel zur Vollendung seines 
60. Lebensjahres]/ed. : Herbert Büning; Peter Naeve. - Berlin; New York: 
de Gruyter, 1981. 
ISBN 3-11-008419-8 
NE: Büning, Herbert [Hrsg.]; Wetzel, Wolfgang: Festschrift 
Library of Congress Cataloging in Publication Data 
Computational statistics. 
Festschrift dedicated to Dr. Wolfgang Wetzel. English and German. 
I. Mathematical statistics - Data processing -
Adresses, essays, lectures. 2. Wetzel, Wolfgang. I. Biining, Herbert. 
II. Naeve, Peter. III. Wetzel, Wolfgang. 
QA276.4.C57 
519.5'028'54 
81-12585 
ISBN 3-11-008419-8 
AACR2 
© Copyright 1981 by Walter de Gruyter & Co., vormals G. J. Göschen'sche Verlagshandlung, 
J. Guttentag, Verlagsbuchhandlung Georg Reimer, Karl. J. Trübner, Veit & Comp., Berlin 30. Alle 
Rechte, insbesondere das Recht der Vervielfältigung und Verbreitung sowie der Übersetzung, vor-
behalten. Kein Teil des Werkes darf in irgendeiner Form (durch Photokopie, Mikrofilm oder ein 
anderes Verfahren) ohne schriftliche Genehmigung des Verlages reproduziert oder unter Verwendung 
elektronischer Systeme verarbeitet, vervielfältigt oder verbreitet werden. - Printed in Germany. 
Druck: Karl Gerike, Berlin. - Bindearbeiten: Dieter Mikolai, Berlin. 

Wolfgang Wetzel 
zur Vollendung seines 
60. Lebensjahres 


Vorwort 
'Computers, the second revolution in statistics' war das Thema, das F. Yates 
für die erste Fisher Memorial Lecture im Jahr 1966 gewählt hatte. Nun 
scheint es ein Charakteristikum unserer Zeit zu sein, im wissenschaftli-
chen wie im nichtwissenschaftlichen Bereich fortwährend 'Revolutionen' zu 
verkünden. Viele dieser 'Revolutionen' entpuppen sich dann im Nachhinein 
als das Produkt einer übertreibenden Sprache. 
Es erscheint daher angebracht, einmal nach der von Yates konstatierten Re-
volution und ihren Folgen zu fragen. Der vorgelegte Sammelband versucht, 
darauf eine Antwort zu geben. 
Die erste Idee, einem skeptischen Frager zu begegnen, ist sicherlich, eine 
Reihe von 'state of the art' Artikeln renommierter Statistiker zusammenzu-
tragen und durch den - einmal unterstellten - positiven Tenor der einzel-
nen Beiträge die Zweifel an der Revolution zurückzuweisen oder noch besser 
auszuräumen. Diesen Weg haben die Herausgeber nicht beschritten. Sie la-
den den Leser ein, gleichsam dem Wissenschaftler bei der Arbeit über die 
Schulter zu sehen. Die Beiträge stellen Momentaufnahmen dar, die zeigen, 
welche Rolle der Computer in ihrer wissenschaftlichen Arbeit spielt. 
Die Herausgeber meinen, daß aus den vorgelegten Artikeln durchaus der Schluß 
gezogen werden kann, daß der Computer für die Verfasser ein Stück alltägli-
cher Praxis ist. Bedenkt man, daß viele der Autoren noch als 'jüngere Wis-
senschaftler' klassifiziert werden können, dann kann man allen Zweiflern an 
der Aussage Yates berechtigt entgegenhalten: diese Revolution lebt und ver-
wirklicht ihre Vorstellungen. 
Spricht man über Computer und Statistik liegt es nahe, das Thema wie folgt 
zu strukturieren: 
1. Der Einfluß des Computers auf die statistische Theorie. 
2. Der Einfluß des Computers auf die statistische Praxis. 
3. Der Einfluß des Computers auf die Statistik-Ausbildung. 

VIII 
Vorwort 
Eine solche Gliederung schwebte den Herausgebern bei der Vorbereitung des 
Buches vor. In der Konzeption, mit der der Verlag für die Publizierung des 
Buches gewonnen wurde, finden sich zur Beschreibung der drei Gliederungs-
punkte die folgenden Ausführungen: 
zu 1: Die große Rechengeschwindigkeit der Computer machte plötzlich schon 
in der Theorie vorhandene Ansätze attraktiv, da die Aussicht bestand, 
daß man zu anwendbaren Verfahren kommen kann. So wurden dadurch 
viele theoretische Arbeiten erst angeregt. Beispiele sind die ver-
teilungsfreie Statistik (insbesondere der multivariate Zweig) und 
die Theorie stochastischer Prozesse (ARIMA-Prozesse, Bilineare Pro-
zesse) . Insbesondere im Rahmen der Untersuchung robuster Verfahren 
der Schätz- und Testtheorie einschließlich der Berechnung der Güte-
funktion, die auf analytischem Wege häufig nicht möglich ist, sind 
in jüngster Zeit zahlreiche und umfangsreiche Simulationsstudien 
unter Einsatz von Großrechnern durchgeführt worden. Aber auch ganz 
neue und dazu noch mathematisch sehr anspruchsvolle Gebiete taten 
sich auf, wie z.B. die Zufallszahlengeneratoren. 
zu 2: Der Computer kann nicht nur die Arbeit eines menschlichen Rechen-
knechtes übernehmen, sondern auch - und darin liegt sein eigent-
licher Wert - Arbeiten in kurzer Zeit ausführen, die der menschli-
che Rechenknecht oft nicht in seinem ganzen Leben beenden kann. 
Dadurch wurden für praktische Arbeiten Verfahren erschlossen, die 
ohne den Rechner, wenn überhaupt, nur in irgendeinem Zeitschriften-
artikel vergilbten. Schlagendes Beispiel sind die weltweit ange-
wandten Verfahren zur Saisonbereinigung von Zeitreihen (Berliner 
Verfahren, X-11-ARIMA), die alle die Theorie stochastischer Pro-
zesse ausnutzen und meilenweit von guten alten Phasendurchschnitts-
verfahren entfernt sind. 
Außerdem eröffnet erst der Computer durch die Übernahme der lästi-
gen und für den Menschen zeitraubenden Rechenarbeit die Möglichkeit 
zu wahren statistischen Arbeiten. Man kann jetzt Daten interaktiv 
mit unterschiedlichen Verfahren analysieren. 

Vorwort 
IX 
zu 3: Hier sind zwei Bereiche zu unterscheiden. Wenn der Rechner für die 
praktische und theoretische Arbeit des Statistikers ein nicht mehr 
wegzudenkendes Hilfsmittel ist, dann ist es auch notwendig, ihn in 
das Curriculum einzubeziehen. Zum anderen stellt der Computer aber 
auch ein didaktisches Medium dar, das nutzbringend in der Statistik-
ausbildung verwandt werden kann. Mit Hilfe des Computers - der Zu-
gang müßte dem Dozenten beispielsweise durch ein Terminal im Hör-
saal gegeben sein - ließe sich dann praktische statistische Daten-
analyse im Hörsaal demonstrieren. 
Als dann aber die Arbeiten bei den Herausgebern eintrafen, zeigte sich ein-
mal mehr die Mehrdimensionalität statistischer Arbeiten. Die meisten Bei-
träge ließen sich trotz kooperativer Zusammenarbeit mit den Autoren nicht 
befriedigend nur einem der drei Themenkreise zuordnen. In einer solchen 
Situation erinnert man sich dann immer gerne der durch das Alphabet vor-
gegebenen Ordnung, und so finden sich die Beiträge nach diesem Prinzip 
angeordnet wieder. Aufmerksamen Lesern wird nicht entgehen, daß an einer 
Stelle die alphabetische Ordnung durchbrochen wurde. Der Beitrag von 
Seppo Mustonen fällt auch typographisch aus dem Rahmen. In dem Artikel 
wird die Verbindung von einem komfortablen Textverarbeitungssystem mit 
einem statistischen Programmsystem beschrieben. Da die Herausgeber glau-
ben, daß hiermit die Richtung angezeigt wird, um die Feststellung Mustonens 
'it is quite common that when writing a research report ... the Output from 
the Computer cannot be used as such ...' aufzuheben, haben sie diese Arbeit 
gleichsam als Ausblick auf die hoffentlich nahe Zukunft an das Ende ge-
stellt. 
Dieses Buch will aber nicht nur einen Bericht über die Verbindung von Sta-
tistik und Computer zu Computational Statistics abgeben. Es ist vor allem 
gedacht als Ehre und Dank für Professor Wetzel, der im Herbst 1981 die 
Vollendung seines 60. Lebensjahres feiert. Die Autoren wollen aus diesem 
Anlaß mit ihren Beiträgen ihre persönliche Wertschätzung seiner Person zum 
Ausdruck bringen. 
Viele haben sicher die großen Möglichkeiten gesehen, die in einer 'Ehe' von 

X 
Vorwort 
Computer und Statistik liegen. Aber nur wenige wie Professor Wetzel haben 
die Kraft und die Ausdauer aufgebracht, die notwendigen Maßnahmen zur Re-
alisierung dieser Möglichkeiten durchzusetzen. 
Die Probleme beginnen bei einer ignoranten Administration, - sei es inner-
halb sei es außerhalb der Universität - deren unausgesprochene Leitlinie 
oft zu sein scheint: die Arbeit ging ja bislang auch ohne Rechner, warum 
also unbedingt einen solchen anschaffen. Die Schwierigkeiten sind noch 
lange nicht zu Ende, wenn die Bewilligung und Finanzierung eines Computers 
sich im Gestrüpp der Zuständigkeiten der verschiedenen Einheiten wie Uni-
versität, Land, Bund abzuzeichnen beginnen. Da gibt es dann noch immer 
die etablierte Zunft der Mathematiker, Physiker und anderer "rechnender1 
Naturwissenschaftler, die dem 'weichen Wissenschaftler' aus der Statistik 
den Zugang zum gerade erstrittenen Rechner verwehren. Man muß schon ein-
mal selbst an einem solchen Vorgang beteiligt gewesen sein, um die ganze 
Leistung eines erfolgreichen Kämpfers für die tatsächliche Verbindung von 
Statistik und Computer beurteilen zu können. Einem Publikum, das sogar 
noch 'rational' mit Wissenschaft assoziiert, dürften detaillierte Schilde-
rungen zu sehr nach Baron von Münchhausen klingen. 
Professor Wetzel ist eine der wenigen Personen, die den geschilderten Pro-
zeß erfolgreich beendet haben. Das von ihm im Jahr 1965 an der Freien Uni-
versität Berlin aufgebaute Institut für Angewandte Statistik bot für jeden, 
der die Möglichkeit der Verbindung von Statistik und Rechner ausschöpfen 
wollte, hervorragende Arbeitsmöglichkeiten. Die Ausstattung des Instituts 
mit Personal, Bücherei und Sachmitteln wurde gekrönt durch eine für die 
damalige Zeit, zumindest in Deutschland, unglaubliche Rechnerkapazität. 
Das Institut hatte bereits eine eigene (jetzt schon legendäre) IBM 1130, 
als die Universität noch nicht einmal ein Rechenzentrum hatte. An der 
später einsetzenden Ausweitung der Rechnerkapazität innerhalb und außer-
halb der Freien Universität konnte das Institut dank der bahnbrechenden 
Bemühungen von Professor Wetzel in herausragender Weise partizipieren. 
Es bestand direkter Zugriff durch Terminals oder RJE-Stationen zu fast 
allen Großrechnern in Berlin. 
Die Generation von Professor Wetzel ist doppelt benachteiligt worden. 

Vorwort 
XI 
Durch den 2. Weltkrieg an der Aufnahme des Studiums gehindert, mußte sie 
in der Nachkriegszeit das Studium unter ungünstigeren Bedingungen in einem 
Lebensalter absolvieren, in dem heute bereits die ersten Stufen einer wis-
senschaftlichen Laufbahn abgeschlossen sind. Seine altruistische Wegbe-
reitung für andere gewinnt vor diesem Hintergrund noch eine ganz besondere 
Qualität. 
Eine Reihe von Autoren hatten das Glück, auf dem von Professor Wetzel vor-
bereiteten Weg gehen zu können. Für sie ist das vorgelegte Buch auch ein 
Rechenschaftsbericht an ihn. Sie hoffen, ihm durch .ihre Arbeiten zu zei-
gen, daß sich sein Einsatz und seine Mühe gelohnt haben. 
Nicht zuletzt wünschen die Autoren Herrn Professor Wetzel noch viele Jahre 
fruchtbarer Arbeit in der Forschung und Lehre. 
Allen Damen und Herren, die an der Anfertigung der Reinschrift des Manu-
skripts beteiligt waren, sei für ihre sorgfältige Arbeit und das Bemühen 
um die Einhaltung einer einheitlichen äußeren Form gedankt. 
Auch dem Verlag de Gruyter und insbesondere Herrn Schuder gilt unser Dank 
für die Unterstützung bei der Verwirklichung unserer beiden Ziele. 
H. Büning 
Berlin 
P. Naeve 
Bielefeld 


Preface 
When in 1966 the First Fisher Memorial Lecture was held by F. Yates he 
spoke about 'Computers the second revolution in statistics'. 
Reports on revolutions seem to be a characteristic phenomenon of our days, 
both, in the scientific as well as in the public community. And it is true 
too that most of those revolutions prove to be caused by exaggerated lan-
guage other than by plane facts. So there is a need to ask what has happened 
to the revolution proclaimed by Yates. This omnibus volume tries to answer 
the question whether there are any everlasting consequences caused by this 
revolution. 
It seems to be a good idea to collect some articles on the state of the art 
written by high ranking statisticians. Given their positive tenor those pa-
pers would turn the sceptic down. The editors decided not to take this way. 
They invite the reader to look over the shoulder of some scientists and 
see them work. The articles are snap shots showing the role computers play 
in the researchers' work. 
The editors feel that one may rightly infer that computers have become an 
integrated part of the authors everyday work. Considering the fact that most 
of the authors might be classified as 'junior scientists' the following 
statement is for sure justified and it appears to be a convincing argument 
against those who do not share Yates' opinion: This revolution is still 
alive and brings its ideas into action. 
When speaking about computers and statistics it sefems quite natural to 
structure the subject according to the following scheme: 
1. The impact of computers on statistical theory 
2. The impact of computers on statistical practice 
3. The impact of computers on the teaching of statistics 

XIV 
Preface 
Such a structure was underlying our argumentation when we discussed the 
project with the publisher. Our final conception included detailed state-
ments concerning the subsections of the scheme, such as ... 
ad 1: Facing the ever increasing speed of modern computers a lot of stat-
isticians were stimulated (or 'encouraged') to deal with known but 
still unsolved theoretical problems. The perspective of having the 
chance to obtain highly applicable solutions was for sure a stunning 
incentive. Out of this work many a theoretically oriented papers 
originate. Examples may be found in nonparametric statistics (espe-
cially in the multivariate branch) and in the theory of stochastic 
processes (ARIMA-processes, bilinear processes). Studying robust pro-
cedures, for example, the investigator may very soon run into severe 
difficulties if he tries to solve the problems (e.g.finding power 
functions) analytically. Therefore in the last years much research 
was done based on large simulation studies done on a computer. But 
also new and mathematically attractive fields of research developed 
such as random number generators. 
ad 2: The computer not only can do the job of the ready reckoner but more-
over can finish such tasks in short time which could not be completed 
by human beings within their lifetime. This latter fact accounts for 
the value of computers. Only due to their high speed in doing calcu-
lations many methods burried in some journal article could be made 
accessible for practical work. Striking examples are world-wide ap-
plied procedures for seasonal adjustment of time series (e. g. Xll-
ARIMA). All of these procedures are based on the theory of stochastic 
processes and miles away from the old paper and pencil methods. 
Letting the computer do all the tedious and for humans time consuming 
calculations gives free way for real statistical work. Now one has 
the opportunity to interactively analyse data using quite different 
methods. 

Preface 
XV 
ad 3: Two aspects are to be distinguished. When it is true that computers 
are such a valuable tool for the statisticians practical and theoret-
ical work it seems to be necessary to teach students how to utilize 
them. On the other hand the computer is a useful didactic tool which 
should be successfully applied in teaching statistics. For instance 
one could demonstrate practical data analysis within the class-room 
given the teacher has access to the computer via a terminal. 
But when the papers began pouring in once again the manifoldness of statis-
tical work proved. Even in cooperative work with the authors the editors 
were unable to establish a convincing mapping of the articles onto those 
three areas. In such a situation one happily remembers alphabetic order. 
And this is what the editors ultimately did with one exception. The paper 
by Seppo Mustonen not only breaks this alphabetic order but also appears in 
a different typographic design. The paper describes the linkage of a com-
fortable editor system to a statistical program system. The editors believe 
that this paper shows a way to overcome Mustonen's finding: 'it is quite 
common that when writing a research report ... the output from the computer 
cannot be used as such1. Placing his article at the end of this book is 
meant as an outlook into a hopefully near future. 
This book is not only a report on the marriage of computers and statistics 
to become computational statistics. First and foremost it is dedicated with 
respect and gratitude to Professor Wetzel on the occasion of his completing 
the 60th year of living. 
Surely many scientists have noticed the big opportunities that would be 
offered by a linkage of the computer to statistical work. And many did prove 
that those opportunities could be successfully realised, when they got ac-
cess to a computer. But only few did pull together enough strength and 
steadiness as Professor Wetzel did to go trough all the steps necessary to 
provide at last the computer facilities and finacial support for work in 
computational statistics. 
A long story could be told about the struggle with an ever ignorant admin-
istration, about the arrogant colleagues from the math or science depart-

XVI 
Preface 
ments for whom statistics belongs to the soft-science. In their eyes the 
computer facilities were not a necessity for the department of statistics. 
But it is better not to tell it, although everything would prove right the 
narrator would be looked at as a direct descent of the famous story-teller 
Baron Münchhausen. 
Nevertheless Professor Wetzel did succeed and was able to build up a very 
fine statistical department at the Free University of Berlin. In doing this 
he proved to be a real statistician. For as Sir Elderton once said, a sta-
tistician must be a utilitarian. This is certainly true with respect to 
Professor Wetzel. He is a member of that generation which had to pay twice. 
Firstly the second world war kept them away from universities and after the 
war they had to absolve their studies under very restricted and unpleasant 
conditions in an age in which nowadays the first steps of a scientific ca-
reer are completed. 
Several of the authors could luckily take the way Professor Wetzel did pro-
vide. They hope that they can prove by their papers that his engagement and 
labour was wortwhile. 
Last but not least the editors want to thank everyone who in one way or the 
other contributed to the completion of the book for their (kind) efforts. 
The authors who let themselves push to finish their papers in time, those 
who did the fine secretary work to make the papers all look alike with re-
spect to typographic form, the publisher and especially Mr. Schuder for 
support and encouragment. 
H. Büning 
Berlin 
P. Naeve 
Bielefeld 

Inhalt / Contents 
On the Investigation of Some Statistical Properties of the 
1 
Most Probable Number (MPN)-Procedure for Estimating the 
Density of Microorganisms by Use of Computer Simulations 
DR. GISELA ARNDT, PROF. DR. HARTMUT WEISS and 
DR. J. FELIX HAMPE, Free University of Berlin 
On Bilinear Models for Time Series Analysis and Forecasting 
19 
DR. WOLFGANG BIRKENFELD, University of Bielefeld 
Tests auf Gleichverteilung - Ein Gütevergleich 
35 
PROF. DR. HERBERT BÜNING, Freie Universität Berlin 
The Classical Ruin Problem and Electronic•Roulette Machines 
55 
PROF. DR. ULRICH DIETER, University of Graz 
Price Formation in the Chemical Industry of the Federal 
71 
Republic of Germany - Estimation of an Econometric Model 
with Parameter Restrictions Across Equations 
PROF. DR. JOACHIM FROHN, University of Bielefeld 
Vektorielle AR-Prozesse in der Makroökonomie 
89 
PROF. DR. HERMANN GARBERS, Universität Zürich 
The Robustness of Some Distributed Lag Estimators 
99 
PROF. DR: GERD HANSEN, University of Kiel 
Robust Estimates in Linear Regression - A Simulation Approach 
115 
PROF. DR. SIEGFRIED HEILER, University of Dortmund 
Nichtlineare Regression: Parameterschätzung in linearisier- 
137 
baren Regressionsmodellen 
PROF. DR. MAX-DETLEV JÖHNK, Universität Hannover 

XVIII 
Inhalt/Contents 
A Test for Independence of Dichotomous Stochastic Variables 
151 
Distributed Over a Regular Two-Dimensional Lattice 
PROF. DR. PETER KUHBIER and DIPL. KFM. JOACHIM SCHMIDT 
Free University of Berlin 
Computational Experiences with an Algorithm for the 
167 
Automatic Transfer Function Modelling 
PROF. DR. HANS-JOACHIM LENZ, Free University of Berlin 
Vergleiche zwischen empirischen und theoretischen Kenngrößen 
185 
von ARMA-Modellen im Zeitbereich - Eine zusätzliche Möglichkeit 
der Modell-Validierung 
DR. WALTER MOHR, Universität Kiel 
APL and the Teaching of Statistics 
215 
PROF. DR. PETER NAEVE, University of Bielefeld 
Optimale Schichtabgrenzung bei optimaler Aufteilung 
231 
unter Annahme einer bivariaten Lognormalverteilung 
PROF. DR. KARL-AUGUST SCHÄFFER, Universität Köln 
Nichtparametrische Tests auf Zufälligkeit bei verschiedenen 
249 
Stufen der Diskretisierung von Beobachtungsfolgen 
DR. RAINER SCHLITTGEN, Freie Universität Berlin 
A Linear Combination of Estimators in an Errors-in-Variables 
263 
Model - A Monte Carlo Study 
PROF. DR. HANS SCHNEEWEISS and DIPL. MATH. HORST WITSCHEL, 
University of Munich 
The Robustness of Sampling Plans for Inspection by Variables 
281 
DR. HELMUT SCHNEIDER and PROF. DR. PETER-THEODOR WILRICH, 
University of Berlin 
Über eine Verallgemeinerung der Spektralanalyse 
DR. BERND STREITBERG, Freie Universität Berlin 
297 

Inhalt/Contents 
XIX 
On a Generalized Iteration Estimator 
315 
DR. GÖTZ TRENKLER, University of Hannover 
Statistical Computing with a Text Editor 
PROF. DR. SEPPO MUSTONEN, University of Helsinki 
337 


On the Investigation of Some Statistical Properties 
of the Most Probable Number (MPN)-Procedure 
for Estimating the Density of Microorganisms by Use 
of Computer Simultations 
Gisela Arndt, Hartmut Weiß, and J. Felix Hampe 
1. INTRODUCTION 
The number of aerobic microorganisms ("plate count") has been one of the 
more commonly used microbiological indicators on the quality of food. 
There are various kinds of specific pathogenic microorganisms which should 
not be contained in food. For quality control it is in general not suffi-
cient to decide only on the presence or absence of pathogenic microorga-
nisms in food. Furthermore in many situations a quantitative examination 
of food for some specific microorganisms is of importance in deciding on 
the level of hazard for the consumer of this food. 
On the other hand it is impossible to find precise microbiological proce-
dures for the estimation of the density of specific pathogenic microorga-
nisms using classical plate-count techniques. 
The two main reasons are : 
(1) The average density of the majority of pathogenic microorganisms per 
ml or gm food is too low to be cultivated on plates. 
(2) Various kinds of pathogenic microorganisms cannot be cultivated on 
plates with non-liquid media at all. 
Taking into account these reasons, Phelps (1908) and McCrady (1915) deve-
loped the MPN-procedure for a quantitative examination of pathogenic mi-
croorganisms by use of liquid media for cultivation. 
The first general discussion of the MPN-procedure from a statistical view-
point was given by Cochran (1950). From his critique we quote : 
"We have seen that the m.p. n.is an estimate of the density of organisms. 
Considered more generally, it is a 'procedure for obtaining estimates' 

2 
Gisela Arndt, Hartmut Weiß, and J. Felix Hampe 
since the same argument could be applied to other statistical problems. 
The only justification which I have mentioned for the procedure is that 
it seems intuitively reasonable. From a reading of the literature I am 
not certain as to the reasons which led early investigators to select this 
estimate, though either the intuitive approach or an appeal to a theory 
of inverse probability may have been responsible. " 
And finally we read : 
".. .Consequently the m.p.n. method is now generally used in a great vari-
ety of problems of statistical estimation, though it more frequently goes 
by the name of the 'method of maximum likelihood'." 
A further important contribution on the MPN-procedure due to de Man (1975), 
regarding the statistical as well as the microbiological characteristics 
will be discussed in detail below. 
As the literature on the MPN-procedure shows a lack of detailed investiga-
tions of the statistical properties of the procedure in the case of small 
trials, the present paper tries to answer some of these questions by use 
of simulation studies. Finally the results lead us to deduce some guide-
lines for its application within microbiological research and routine. 
2. ESTIMATION OF THE DENSITY OF MICROORGANISMS BY THE MOST PROBABLE NUM-
BER (MPN) -PROCEDURE 
The usual definition of "most probable number" from a microbiological 
viewpoint can be stated as follows : 
MPN is the estimated number (per ml or per 100 ml) of microorganisms pre-
sent in a sample unit, based on the presence or absence of these micro-
organisms in replicated aliquots (tubes) which are prepared by dilution. 
This definition can be used only if the microbiological examination method 
fulfills the following requirements (for further details see ICMSF,1978) : 
(1) Standardized preparation of food homogenates as well as dilutions. 
(2) Standardized performance of the different steps (e.g. incubation 
at constant temperatures) within the microbiological procedure. 

Estimation of the Density of Microorganisms 
3 
At the end of the examination method one registers at each level of dilu-
tion the number of tubes which show presence of the microorganisms of in-
terest. For example one could have found the recording X = (3,1,05 as a 
sampling combination in an examination with the dilution levels 1:10, 
1:100, 1:1000 and 3 tubes at each level. So one has for a dilution level 
of 1:10 only "positive tubes" (presence of the specific microorganisms) 
and on the third dilution level 1:1000 only "negative tubes". The density 
of microorganisms per ml of the homogenate should therefore lie between 
10 and 100. With the MPN-procedure one would find a "most probable number" 
of 70, as will be shown below. 
From a statistical viewpoint the examination method is based on the follo-
wing assumptions : 
(1) The microorganisms are regarded as being randomly distributed 
within the homogenate (liquid). 
This implies that a germ is equally likely to be found in any 
portion of the homogenate and that there is no tendency of 
clustering or repulsion. 
(2) The growth of every germ within the homogenate. If one has a 
poor medium or another factor resulting in incomplete growth of 
the microorganisms, the "most probable number" will underesti-
mate the true germ density. 
(3) Furthermore it is necessary that the volumes of the sampled 
portions of homogenate (v^) are very small compared to the 
original amount of homogenate at each level i of dilution 
(i = 1,2,3,...,k). 
When there is only one germ in our homogenate V then the probability that 
this germ lies in the sampled portion v 
is simply the ratio of the vol-
ume of the sample to the whole homogenate, i.e. v/V . The probability 
that the germ does not lie in the sample is therefore (1-v/V). 
By use of the multiplication theorem in probability and on the basis of 
the above listed assumptions we can write for the probability that none 
of all b germs (say) in the homogenate is contained in the portion v. 

4 
Gisela Arndt, Hartmut Weiß, and J. Felix Hampe 
as 
C l - v./v ) b 
which can be approximated for small ratios v^/V by 
exp { - v^b/V } = exp { -v^ 6 } = ir 
( 1 ) 
with 6 
= b/V characterizing the real density of germs per ml of our 
homogenate. it 
is the probability that the sample v^ is sterile. 
Therefore we are able to give the probability for a positive reaction of 
a sample at the ith dilution level 
1 _ ir_ = 1 - exp{ - v.6 } 
(2) 
i 
i 
The recording X = (x^,x , . . . x ^ , . . i s the number of positive tubes 
of the examination at all dilution levels V = (v^,...v^,...v ) with 
N = (nj,..-n.,...n^) replications at each level. 
By use of the binomial distribution the probability of this combination 
may be written as 
k 
n. -x, 
x. 
P = L = n ("i) tt. 1 
1 (1 -ir. ) 1 
(3) 
. , x. 
l 
l 
1=1 
l 
After substituting expressions (1) and (2) in (3) we can write 
k 
n.-x. 
x. 
P = L = n C i) exp { -v.6 } 1 
1 (1 - exp { -v.6 }) 1 
i=l 
Xi 
1 
1 
Now we can estimate the unknown density 6 by the Likelihood function. 
In order to derive an estimate("most probable number") we have to maximize 
this Likelihood function with respect to 6 , or equivalently the corre-
sponding log-Likelihood function. (An example of a Likelihood function is 
shown in figure 1.) 

Estimation of the Density of Microorganisms 
5 
Fig. 1. : Likelihood function for the design V = (5,1,0.2) ; 
N = (5,5,5) and given X = (5,3,1) 
This leads to 
1
( V x i ) v i = . 
1 i exp{- v. 6} (l-exp{-v.6}) 
l 
l 
(5) 
which can be solved only by use of an iterative algorithm. 
3. SOME REMARKS ABOUT THE SAMPLING RESULTS 
We can estimate without difficulty the MPN for each positive sampling 
result. But we have to remember the dependence between v^S and the re-
sulting x. . For given 6 we would expect for decreasing v. (increasing 

6 
Gisela Ärndt, Hartmut Weiß, and J. Felix Hampe 
dilution levels) that the number of positive tubes x^ is also decreasing. 
If we had chosen four suitable dilution levels and N = (3,3,3,3) we would 
expect for instance a combination X = (3,2,1,0). If we instead achieved 
X = (0,0,0,3) the question would arise whether this result agrees with our 
assumptions of the MPN-procedure. In his discussion of this problem de Man 
offers a strategy to follow in practice which is briefly described below. 
First we estimate 6 by the MPN-procedure as described above, leading to 
an estimate 6 . Next we calculate the Likelihood function at the point 6 
for all other possible sampling combinations X. The result is a discrete 
probability distribution as can be seen from (4) when substituting 
6 for 
6. Sorting all the 
(n+1) sampling combinations in descending order 
of probability allows us to register the combinations which satisfy the 
inequality 
I 
P(X . ) < a. 
- a. 
with a. = 0 
for jn = O 
(6) 
j=ju+l 
[3] - 
3o 
3u 
Du 
De Man (1975, 1977) suggests to eliminate all those combinations fulfill-
ing condition (6) for a_. = 0.01 and j u = 0 as they can be regarded as 
either senseless or improbable. The remaining combinations are divided 
into two categories. Category 1 includes all combinations satisfying (6) 
with a. = 0.05 and a. = 0.01. Category 2 includes all combinations 
Do 
Du 
satisfying (6) for a. = 1 and a. = 0.05. Figure 2. illustrates this 
Do 
segmentation for the sampling combinations X = (1,1,0) and X = (3,0,2). 
It should be clear that the different probability distributions depend 
on the chosen dilution levels. Consequently between differently chosen 
dilution levels one will observe large differences with respect to the 
"probable" combinations belonging to one of the categories. This fact can 
be seen either from figure 2. or from table 1. for different MPN-designs. 

Fig. 2. 
The effect of different dilution ratios on the probability 
distribution of all 64 possible combinations for three given 
designs and two realized sampling combinations. 

8 
Gisela Arndt, Hartmut Weiß, and J. Felix Hampe 
table 1. : Calculated possible and 
"probable" combinations 
for various MPN-designs 
Designs 
Number of 
possible positive 
Number of 
.. probable "positive 
combinations 
(belonging to category 2) 
v= (to, I 
au 
N=(3j,3) 
y-rs 
l 
02) 
3) 
V= K. I 
0.25) 
N=(Ì J 
3) 
(2.1 
OS) 
N=(3,3, 
3) 
0.33. 
I 
0.75) 
N=(3, 
Ì 5) 
V= (10, I 
0.1) 
N=!5, 
5,5) 
0.2) 
N=<5, 
Î, 
5) 
V=(i, 
f 
0.2) 
N=(8, 
è, 
8) 
4. SOME REMARKS ABOUT THE DISTRIBUTION OF THE ESTIMABLE VALUES 
Figure 3. presents the distributions of the estimable bacterial densities 
for three different designs. 
The total number of tubes is the same in all designs also the dilution 
ranges are similar. 
The figure illustrates very impressively that the sample space is a dis-
crete one. For each design there are IL_^(n+l) possible realisations. 
The more dilution steps there are the smoother the distribution of the 
estimable values becomes. Nevertheless all the distributions show gaps 
in the tails. If the sample result gives an estimate in these regions it 
has to be interpreted very cautiously in practice. 
5. SOME IDEAS ABOUT INTERVAL ESTIMATION 
There are two proposals in literature concerning the construction of con-
fidence intervals for 6 . 

Estimation of the Density of Microorganisms 
9 
Fig. 3. : Distribution of the estimable values 6 for 
three different designs shown as frequency polygons 

10 
Gisela Arndt, Hartmut Weiß, and J. Felix Hampe 
Parnow (1972) and Cochran (1950) assume, that the distribution of the 
estimated bacterial density 6 can be approximated by the lognormal dis-
tribution. Then an approximate (1-a)-confidence interval is given by 
6 • exp{-Ul_a/2 • a ^ } < 6 < 6 • e x p i u ^ ^ • a ^ } 
(7) 
Here the (l-a/2)-percentile of the standard normal distribution is deno-
ted by 
U l_ a / 2. 
Following Parnow the generally unknown standard deviation a~ can be 
estimated using the negative second derivative of the log-Likelihood func-
tion (4) with respect to 6 
k 
„ 
-1 
a". 2 = [ 2. (x.v.2 exp{-v.6}) (1- exp{-v.<S}) 2 1 
(8) 
o 
L . , 
1 1 
1 
i 
' 
i=l 
With the approximation 
the confidence interval (7) can be calculated. The other proposal is due 
to de Man. His approach may by characterized as follows. 
Based on a noninformative a priori distribution of the parameter 6, de Man 
works effectively with an a posteriori distribution for 6 . This leads to 
a standardized Likelihood function which is regarded as the density func-
tion for 6 . By numerical integration one can then calculate, for a given 
sampling result, the limits 
of a central interval such that 
P ( 6 < S < 6) = 1 - a 
(10) 
u — 
— 

Estimation of the Density of Microorganisms 
11 
6. DESIGN OF THE SIMULATION STUDY 
The aim of our simulation study was to answer the following questions : 
(1) Is it possible to get unbiased estimators with 
the MPN-procedure ? 
(2) How do the estimation intervals (7) and (10) 
compare with those derived by simulations for 
small designs ? 
(3) Which strategy can be recommended for practical 
purposes in microbiological research ? 
Here we briefly describe the set up of our simulation study : 
step 1 : 
All simulation runs depend on given values for 
V, N, and 
6. Consequently we can immediately 
calculate : 
tt^ = exp{-v^6} 
and 
1 - it. = 1 - exp{-v.6} 
with i = l,2,...,k 
l 
I 
step 2 : 
Within the simulations we used binomially distributed pseudo 
random numbers in order to calculate the results 
X. 
step 3 : 
From the simulated sampling results 
6 was estimated, 
step 4 : 
Furthermore some properties of the estimation function 6 
were analyzed on the basis of 1000 simulation runs for 
each given 
6 . 
All calculations have been carried out with APL on a computer SIEMENS 
7.748, using library routines for the random number generation. 
7. INTERPRETATION OF THE SIMULATION RESULTS CONCERNING THE BIAS OF THE 
ESTIMATES 
For the selected MPN-designs 
V = (5,1,0.2) ; N = (5,5,5) 
V = (5,1,0.2) ; N = (7,7,7) 
V = (10,1,0.2); N = (5,5,5) 

12 
Gisela Arndt, Hartmut Weiß, and J. Felix Hampe 
we found in all our simulations that on average 6 were estimated very 
close to the given values 6 . This can be seen from figure 4. for 
6e [o.2, 1.] where the given values 6 are shown by vertical lines and 
the corresponding geometric means of the 1000 values 
6 are indicated by 
arrows. 
The discrepancies between the given <5 and the geometric means for all 
6 > 1.2 are in fact due to the above mentioned gaps in the tails of the 
distribution of estimable values. 
Consequently we cannot say in general to what extent the estimate 6 is 
biased as this depends primarily on the suitable choice of the design 
with respect to the true values of 6 . 
8. INTERPRETATION OF THE SIMULATION RESULTS CONCERNING THE INTERVAL 
ESTIMATIONS 
For the discussion of the problem of interval estimation we refer again to 
figure 4. and in addition to figure 5. 
Figure 5. shows for a given design a good correspondence between the inter-
val estimates derived from (7) and (10) respectively. Furthermore the in-
terval estimates based on the simulated values 6 are shown for compari-
son. Whereas the simulated 95% confidence intervals are calculated by use 
of the normality assumption taking the geometric means and standard devia-
tions of the estimated logarithmic values 6 , the central 95% intervals 
are computed from the simulated values 6 without use of a distribution 
assumption. At first sight it is quite obvious that all interval estimates 
are close to each other for low values of S. With increasing values of 6 
we observe increasing interval lengths as well as increasing differences 
between the four interval estimates. As already discussed in the last 
section the reason for this pattern seems to be mainly the decreasing num-
ber of estimable values 6 when 6 becomes large. 
Comparing next figure 4. with figure 5. it becomes evident that the inter-
val lengths differ extremely between the various designs. To illustrate 
this point in more detail we show in figure 6. some further simulation 
results. For three different designs and a fixed value of 6 = 8 
we see 

Estimation of the Density of Microorganisms 
13 
A =0.2 
S = 0.5 
S = 0.7 
5 - to 
6 
.12 
6 « 7.5 
5 =1.7 
6 
'2.0 
b = 2.2 
S = 0.2 
S =0.5 
Ä =0.7 
S - to 
¿=7.2 
5 --15 
6 «1.7 
6 * 2.0 
b - 2.2 
Fig. 4. : Simulated 95% confidence intervals for different designs and 
various given densities 6 of microorganisms. The arrows indi-
cate the geometric means of the simulated values 6 . 

14 
Gisela Arndt, Hartmut Weiß, and J. Felix Hampe 
Fig.5. : Theoretical and simulated 95% intervals for the design 
V=(5,1,0.2); N=(5,5,5) and various given densities 
8 
of microorganisms 
the frequency distributions of 1000 simulated values of 
6 for each de-
sign. Comparing the three frequency distributions we observe that a mis-
specification of the design with respect to the true (given) bacterial 
density causes a tremendous effect. 
For the design 
V = (5,1,0.2) ; N = (5,5,5) we find for instance only six 
estimable values, which explains the rugged shape of the corresponding fre-
quency distribution. On the other hand we obtain relatively smoother fre-
quency distributions for the more suitable designs with respect to the un-
derlying value 
6 . One consequence of such misspecifications is the 
enormously increased length of the interval estimates, e.g. for the central 
95% interval we compute : 
for 
V = (5, 1, 0.2) 
; N = (5, 5, 5) 
: 
(3.4, 46.5) 
for 
V = (1, 0.2, 0.04) 
; N = (5, 5, 5) 
: 
(3.6, 24.4) 
for 
V = (0.2, 0.04, 0.008) ; N = (5, 5, 5) 
: 
(3.0, 20.0) 
Remember that the true bacterial density was fixed to be 6 = 8. 

Estimation of the Density of Microorganisms 
15 
V= (5,1, 0 2 N = ( 5 , 5, 5) 
1 
0.5 
&S 
C5 
15.5 
20S 
25.5 
JQ5 
KS 
405 
4SS 
SOS 
5S5 
«IS 
Vs(1, Q2, Q0£)j N=(5, 5, 5) 
ill.) . 
as 
5.5 
US 
6.5 
105 
» 5 
».5 
3S.S 
40.5 
455 
505 
555 
6t5 
V=(0l2 0.04 0006). N=(5, S, 5) 
II 
I1J .. . 
•QS 
1&S 
205 
BS 
aas 
Ä5 
40.5 
t&S 
5Q5 
SSS 
515 
Fig. 6. 
Distributions of the simulated values 6 for three different 
designs and a fixed value of 6 = 8 (1000 simulations for each 
design). 

1 6 
Gisela Arndt, Hartmut Weiß, and J. Felix Hampe 
9. SOME CONSEQUENCES OF THE RESULTS FOR A STRATEGY OF PRACTICAL 
MICROBIOLOGICAL RESEARCH 
Our investigations based on various simulation runs have shown that esti-
mates of bacterial densities by the MPN-procedure depend especially on a 
proper choice of the design. 
Bringing together all different aspects and results we would suggest the 
following hints for future microbiological examinations : 
(1) If no or little information about the effective bacterial 
density is known, one should choose dilution levels which 
cover a wide range. This may ensure that positive tubes are 
obtained at the lowest dilution level and negative tubes at 
the highest level. 
(2) In order to be able to detect the presence - absence turnpoint 
the number of dilution levels should closely cover the whole 
range of dilutions. 
Certainly a compromise must be found between the number of 
dilution levels and the number of replications corresponding 
to the given budget constraints. 
(3) In addition to the tables for various MPN designs (these are 
given in the new literature on microbiology) one should also 
offer the experimenter information about the distributions 
of the estimable values. These can be easily calculated for 
any design. 
(4) One should in all cases check for every empirical sampling result : 
(a) whether it is "probable" 
(b) whether it leads to an estimable value 
S falling into 
the central 95% interval (say) of the distribution of 
estimable values. 
In case one or all of points (1) to (4) are not satisfied the estimates 
should be interpreted with extreme caution. If possible, it would be best 
to repeat the whole examination with a corrected design. 
If condition (4a) is violated one should check whether the assumptions of 
the MPN-procedure are unfulfilled or whether gross errors in the laborato-

Estimation of the Density of Microorganisms 
17 
ry work caused these results. 
Violation of condition (4b) indicates a misspecified design. 
Finally we would suggest that some care appears advisable when using the 
MPN-procedure in laboratory routine. 
Acknowledgement 
We thank Mr. H.Meyer for carefully preparing all the tables and graphics. 
References 
1. Cochran, W.G 
2. de Man,J.C. 
3. de Man,J.C. 
4. ICMSF 
5. Mc Crady,M.H 
6. Parnow, R.J. 
7. Phelps, E.B. 
: Estimation of bacterial densities by means of the 
"Most Probable Number". Biometrics !5, 105 (1950) 
: The probability of Most Probable Number. Europ.J.Appl. 
Microbiol.67 (1975) 
: MPN Tables for more than one test. Europ.J.Appl. 
Microbiol. 4, 307 (1977) 
: Microorganisms in foods. University of Toronto Press. 
(1978) 
: The numerical interpretation of fermentation-tube 
results. J.Infec.Dis. _T7, 183 (1915) 
: Computer program estimates of bacterial densities 
by means of the Most Probable Numbers. Food Technol. ]_, 
56 (1972) 
: A method for calculating the number of B.coli from the 
results of dilution test . Amer.J.Pub.Hyg. 4, 141(1908) 


On Bilinear Models for Time Series Analysis 
and Forecasting 
Wolfgang Birkenfeld 
1. INTRODUCTION 
Within the classical framework of time series analysis and forecasting it 
is customary to work under the following assumptions: 
(1) All series are stationary (at least up to second order), or they can 
be reduced to stationarity by simple transformations (e.g. difference 
filters for ARIMA models). 
(2) All models are linear with a finite number of parameters, i.e. they 
can be represented as linear functions of present and past values of 
an independent white noise process. 
(3) Many analysing and forecasting procedures implicitly assume a normal 
distribution of the series, so that their characterisation by second 
order statistics (covariances and spectra) is sufficient. 
Under these assumptions techniques for the identification of a model from 
a given set of data, the estimation of its parameters, the diagnostic 
checking of the model adequacy and the forecasting of the series have been 
developed by Box & Jenkins (1970) and others. 
However, one must always realize that stationarity, linearity and normal-
ity are methodic simplifications which, very often, can be just rough 
approximations of real life phenomena. 
In this paper the applicability of ^^^"li^ear time series models is dis-
cussed, i.e. the analysing and forecasting of time series without the re-
strictive assumptions of linearity and normality. Non-stationary time 
series are not considered here because their theory a,id practice is longer 
and much further developed than non-linear time series and, consequently, 
there exists an extensive literature on this subject over the last 16 years 
(e.g. Priestley 1965, 1967, 1970, Abdrabbo & Priestley 1967 and Hampe 1980). 

20 
Wolfgang Birkenfeld 
The insufficiency of considering just linear models and second order mo-
ments is very well illustrated by an example (Priestley 1978) : 
Consider the process 
with 
n = e + be 
. e „ , 
t 
t 
t-1 t-2 
where 
{e } is a sequence of independent, identically distributed random 
t 
2 
variables with zero expectation and variance 
a 
(strict white noise). 
It is easily seen that 
{n } is an uncorrelated process with expectation 
2 
zero and the same variance a 
, i.e. the processes 
i ^ l 
a r S 
identical as long as just second order properties are considered. However, 
the process 
i1^} definitely contains useful information, and a nontrivial 
forecast for 
rlt+j 
c a n '3e given if observations 
...,rit
 
a r e avail-
able. Under the mean square error criterion, the optimal forecast for 
is given by its conditional expectation, i.e. 
r i . = E [ n 
.In ,n .,...]= be e 
t+1 
't+1 t t-1 
t t-1 
The history of non-linear models in the field of time series analysis is 
still very young (4 to 5 years). Compared to linear models, a unified and 
as well established theory does not (yet) exist. The collection of empir-
ical experiences has just begun. One of the fundamental problems is that 
"the general non-linear model" does not (yet) exist. Hence, different 
models are built in order to approximate as closely as possible the (almost 
always existing but quite differently structured) non-linearity contained 
in the data. 
In this paper a fairly general class of non-linear models, the so called 
bilinear models, is treated. These models have been originally introduced 
by control theorists. Bilinear models are, because of their structural 
similarities, natural (non-linear) generalizations of the well known 
(linear) ARMA models. 
2. NON-LINEAR MODELS 
It is known that under certain conditions the connection between a system's 
input 
e and its output X can be written in form of a Volterra series: 

Bilinear Models for Time Series Analysis 
21 
OO 
CO 
00 
00 
00 00 
X = V g 
+ y 
y g 
+ y 
y 
y g 
e. 
e. 
e. 
+ ... 
(2.1) 
t 
L u t-u 
L 
L 
uv t-u t-v 
uvw t-u t-v t-w 
u=o 
u=o v=o 
u=o v=o w—o 
The first part pf this series, i.e. 
CO 
X = y g e 
t 
L yu t-u 
u=o 
is the well known general linear model. Hence, the first term of the rhs 
in equation (2.1) explains the linear part of the output X. The following 
terms, which can be considered as quadratic, cubic etc. terms, explain the 
non-linearity in the output X. 
Although mathematically very elegant, Volterra series and their represent-
ation in form of Hermitian polynomials (Wiener 1958) are not appropriate 
to model non-linear time series, since an infinite number of parameters 
can not efficiently be estimated from a finite set of data. However, models 
with a finite number of parameters and enough generality to describe many 
non-linear phenomena have been introduced by control theorists: the bilin-
ear models. 
The state space representation of a bilinear model (of order m) in the con-
trol theory literature (cf. Mohler 1973) is of the general form 
x(t) = Ax(t-l) + Be + e 
CX(t-l) 
t 
(2.2) 
= Hx (t) , 
where the system matrix A and the input matrix 
B are square matrices 
of order (m;m) ,- the state vector x and the control vector 
e are column 
vectors of order (m;l). A graphical representation of the equations (2.2) 
is given in figure 1. If C=0 , the broken lined part in figure 1 is in-
applicable and the model (2.2) reduces to the state space representation of 
an ARMA model. The bilinear model (2.2) approximates, over a finite time in-
terval, with arbitrary accuracy the model (2.1), provided the coefficent 
matrices A, B, C and H are suitably chosen (Brockett 1976). 
In control theory it is commonly assumed that the system and the control é 
are completely known, whereas in time series analysis the input 
e is a 
usually unobservable random process and the systems coefficient matrices 

22 
Wolfgang Birkenfeld 
Figure 1 : Bilinear state diagram 
are to be estimated. Granger & Andersen (1978) introduced the bilinear 
model into the time series context. They extended the model (2.2) to the 
following model which is given in its scalar form: 
P 
q 
? 
B 
= I a 
• 
. . i t-i 
i=l 
f Y b . E 
. + y 
Y c, X , e 
+ £ 
j = l 3 ^ 
k=l ¿ 1 
(2.3) 
Except for ET 
, X^ only depends on past X's and 
E'S . If 
c
k £ = 0 
f o r 
all k and 
SL , this model reduces to the well known ARMA(p,q) model. In 
this sense, the bilinear model is a non-linear extension of the linear 
ARMA model. 
Subba Rao (1977, 1978, 1979, 1981) systematically developed the general 
theory of bilinear time series models. He investigated mainly the bilinear 
autoregressive model of order 
(p,Q) , termed as BAR(p,Q). The scalar 
form of this model is 
P 
P 
Q 
X = a + y a x 
. + y 
y c, X , e 
° 
1 = 1 1 t " 1 
k=l ¿ I k f - t - x 
Í.+ et 
(2.4) 
If c =0 for all k and 
l , this model reduces to the well known AR(p) 
model. 
In the following sections, the practical applicability of this BAR(p,Q) 
model (2.4) will be investigated, and the model fitting and forecasting 
results will be compared to the results obtainable with linear models. 

Bilinear Models for Time Series Analysis 
23 
3. NON - LINEAR PHENOMENA 
Many important processes, not only in engineering, but also in biology, 
physiology, ecology and various other fields, may be modeled by bilinear 
systems. In order to motivate the interest in this class of models, a few 
typical examples of non-linear phenomena, which can be modeled by bilinear 
systems, are presented. 
Economics^ Bilinear models naturally arise in economic theory since the 
non-linearity of many model components is due to the products of variables: 
(1) costs = quantity x price 
(2) return from an investment = invested amount x interest rate 
(3) income of a government = national income x tax rate 
: 
Suppose that the true rate of return for a quantity X 
is 
generated by a MA(1) model, i.e. 
\ 
- Xt-1 = £ + be 
V i 
t" 1 
This equation bay be written as 
X = X 
+ e X 
+ be 
,X , 
t 
t-1 
t t-1 
t-1 t-1 
i.e. in a bilinear model form. 
Geophysics^ Seismological data (underground explosions or earthquakes, for 
instance) reveal as a typical pattern the short duration of an activity 
followed by a time interval where the series record seems to be due to 
noise. This sort of behaviour is displayed in figure 2. In reality however, 
the data shown in figure 2 are not seismological observations - they are 
artificially generated from the simple BAR(2,1) model 
Xfc = o.8Xt_1 - o.4Xt_2 + o.6Xt_1£t_1 - 0 . 7 X ^ 2 ^ + ^ 
, 
(3.1) 
2 
where the e 
are normally distributed 
(y=0 ; o =2.718) pseudo random 
numbers. 
Astrophysics^ Figure 3 contains a section of Wölfer's sunspot numbers 
(Waldmeier 1961, Schröter 1980). This cyclical series (and many others, 

24 
Wolfgang Birkenfeld 
Figure 3: Wolfer's sunspot numbers, 1801 - 1960 
e.g. the German unemployment series) shows a phenomenon that may be due to 
non-linearity: the upswings to the maxima and the consecutive downswings to 
the minima are differently steep (Akaike 1978). Later, the test of Subba 
Rao & Gabr (1979) has shown that this series is highly non-linear. 

Bilinear Models for Time Series Analysis 
25 
4 . 
PARAMETER ESTIMATION 
4.1. Preliminaries 
The problem of estimating the parameters of a bilinear model is, in prin-
ciple, not different from the problem of parameter estimation for linear 
models; provided the model order has been determined a priori. 
Consider the estimation of the parameters of the bilinear BAR(p,Q) model 
(2.4) under the following assumptions: (i) 
i s a n i-i-d. Gaussian 
process, (ii) the model (2.4) is invertible and (iii) for t=l,...,n 
ob-
servations of the series 
a r e available. Then, for given observations, 
the likelihood function L is a function of the parameter vector 0 , where 
0' = (ai , ckJ, | 1=0,1 
p ; k=l , . . . ,p ; 1=1 , . . . ,Q) . 
For m=max(p,Q) the likelihood function L(0) is approximately given as 
L(0) oc exp 
1 
V 
2 
~~2 
¿ 
£t 
2a t=m+l 
v 2 
Maximising L(0) 
is equivalent to minimising V(0) = 
with respect to 
0 . This minimising procedure results in a set of highly non-linear equa-
tions which can be solved, for instance, by one of the well known variants 
of the Newton-Raphson procedure (for the details cf. Subba Rao 1979, 1981). 
In practical applications a maximum model order 
(p 
,0 
) is preselected 
max max 
and a sequence of models BAR(1,0) , BAR(1,1) , . . . , BAR(1,0 
) , BAR(2,0) , 
max 
..., BAR(p 
,0 
) is estimated. The estimation of the models 
max *max 
BAR(1<i<p 
,j=0) consists in the estimation of linear AR(i) models by 
max 
a known standard technique, whereas for j>0 and fixed 
i the estimated 
parameters of BAR(i,j) can be used as initial estimates for the estma-
tion of the parameters of the BAR(i,j + 1) model. 
The order 
(p,Q) of the "true" model is then determined by the aid of some 
optimality criterion. The most commonly used criteria are 
(1) Akaike's (1977) information criterion 'A' which is defined as 
AIC(k) = -2 [maximum loglikelihood] + 2k = T log a^ + 2k , 

26 
Wolfgang Birkenfeld 
where T=n-max(p,Q) 
is the actual number of observations to evaluate 
2 
the likelihood function, a 
is the maximum likelihood estimate of o 
e 
£ 
and k=p+pQ+2 is the number of model parameters 
(2) the normalised AIC, denoted by NAIC, which seems to show a more stable 
behaviour in practical situations: 
~ ? 
?v 
NAIC (k) = log o + — 
e 
T 
"2 
(3) the mean sum of squares of residuals a 
£ 
Chosen is that model order 
(p,Q) for which the value of a criterion is 
minimum. It has been observed that all three criteria do not always lead 
to the same result. This is mainly due to the known fact that especially 
-2 
a 
(but sometimes AIC also, and less often NAIC) tends to overestimate the 
£ 
model order. 
4.2. 
Numerical Illustrations 
4.2.1. Simulated data 
For t=l,...,600 a time series was generated from the BAR(2,2) model 
2 
2 
2 
. + 
i=i 1 t _ 1 
k=i a=i 
xt = A v t - i 
A c k A - k E t - * + 
et 
' 
(4.1) 
Figure 4: Simulated data from model (4.1) 

Bilinear Models for Time Series Analysis 
27 
where 
-0.7 
o. 2 
o. lo o.2o 
o.15 o.22 
and 
e ~ N(0, 2.718) 
A part of this series is plotted in figure 4 and the values of the optimal-
ity criteria are given in table 1. 
UNNORMALISED AIC-VALUES FOR ALL MODELS: 
0= 
0 
1 
2 
3 
1 
P= I 
I 
1 I 
895.09157 
893.11617 
B83.58562 
881.81609 
886.01588 
I 
, 
2 I 
826.82519 
759.20091 
581.061801 581.00700 
587.71615 
I 
' 
3 I 
811.31921 
732.15781 
582.26670| 585.89119 
591.65528 
j 
1 I 
811.97586 
720.71515 
586.83132 
590.85579 
597.83376 
NORMALISED AIC-VALUES FOR ALL MODELS: 
==SS=5=SSSSS ========== 
"
"
"
"
" 
Q= 
Pi I 
0 
1 
2 
3 
1 
I 
1 I 
1.19131 
1.19101 
1.17757 
1 .18215 
1 .18665 
I 
2 I 
1.38265 
1.26957 
0.97669| 
0 .97821 
0 .98610 
I 
3 I 
1.35899 
1.22610 
0.975321 
0 .98139 
0 .99271 
I 
1 I 
1.36238 
1.20925 
0.98162 
0 .99137 
1 .00308 
RESIDUAL VARIANCES FOR ALL MODELS: 
Oi 
0 
1 
2 
3 
1 
P= I 
I 
1 I 0.1112E+01 0.1383E+01 0.1310E+01 0.1315E+01 0.1320E+01 
I 
2 I 0.3932E+01 0.3189E + 01 0.2586E+0l| 0.2572E+01 0.2575E-»01 
I 
3 I 0.3828E+01 0.3319E*01 0.2556E»01 0.2516E+01 0.2519E+01 
I 
1 I 0.3828E+01 0.3210E+01 0.2551E»01 0.2537Et01 O^ 3.^.! 0.!.! 
Table 1: Optimality criteria for the data from model (4.1) 
| : true model ; 
I: selected model 
According to AIC and NAIC, as is seen in table 1, the generating model of 
this series is identified as BAR(3,2), whereas the maximum model order (4,4) 
"2 
would have been chosen according to the a^ criterion. The estimated pa-
rameters of the selected BAR(3,2) model are 

28 
Wolfgang Birkenfeld 
-o.699 
o.196 
o. ol 5 
and 
o.125 o.226 
o.165 o.259 
o.ol2 o.o34. 
2 
The mean sum of squares of residuals is a = 2 . 5 6 
and the AIC value is 
AIC (11) = 582.3 
The best linear model fitted to the same series is an AR(5) model: 
X. = -0.60X 
+ o.43X „ + o.19X 
. + o.o4X 
„ + 0.06X 
+ E 
t 
t-1 
t-2 
t-3 
t-4 
t-5 
t 
with a 
3.81 and AIC(7) = 81o.2 
(4.2) 
4.2.2. The sunspot numbers 
Wolfer's sunspot numbers are one of the most frequently investigated series 
in time series analysis. Several linear models (e.g. Yule 1927, Box & Jen-
kins 1970, Haggan & Subba Rao 1979) and non-linear models (e.g. Granger & 
Andersen 1978, Subba Rao 1979 and 1981, Tong & Lim 1980) have been fitted 
to this series. In this paper, the sunspot numbers are considered for the 
years 
1700 - 1945 
(Waldmeier 1961), i.e. n=246 observations, for model 
fitting purposes and then (in section 5) we compare forecasts with the ob-
servations for the next ten years from 1946 to 1955. 
One of the best models fitted to the mean deleted 
(x=43.5) data is found to 
be an AR(9) model 
X^ = Y a. X 
. + e 
t 
. , i t-i 
t 
1=1 
(4.3) 
with estimated coefficients 
a = (1.21 -o.47 -o.l2 o.l4 -o.l6 0.08 -0.08 o.o9 o.lo). The mean 
sum of squares of residuals is o^ = 194.4 and the AIC is AIC(ll) = 1271.0 
Two bilinear models have been fitted to the raw data, a full and a subset 
model. The full model is a BAR(2,3) model of the form 
: = a + y a. x 
. + y 
y 
t 
o 
. , i t-i 
, L, .L 
i=l 
k=l ¿=1 
Ck5,Xt-ket-J¡. + £t 
(4.4) 

Bilinear Models for Time Series Analysis 
29 
a = 12.87 
o 
with the estimated coefficients 
a = (1 .47 
-o.77) 
0.08 
-o.41 
-1.27 
o.56 
-o.5o 
1.2o 
and 
10 
The mean sum of squares of residuals for this model (4.4) is a = 
176.0 
and the corresponding AIC value is AIC(10) = 1276.4 
The bilinear subset model has been fitted by Subba Rao (1981). It is 
X = a + Y a. X . 
t 
o 
.L, l t-i 
1=1 
9 
7 
+ 
I 
I 
cv, X 
k=l 
1=1 
with the estimated coefficients 
e 
+ £ 
k£ t-k t-i, 
t 
(4.5) 
a 
= 
5.89 
o 
a1 
= 1.21 
-o. 5o 
a g 
= o.17 
c2i 
= 
~°-°o98 
c 
= 0.00I6 
c 
= -o.oo48 
S Z 
o J 
C ^ = O.OOl4 
c 
= 
o.olo3 
o 1 
and all other coefficients equal to zero. The mean sum of squares of resid-
"2 
uals for this BAR(9,7) subset model is a^ = 141.2 and the AIC value is 
AIC (10) = 1193.2 . 
The numerical results of both sections 4.2.1 and 4.2.2 clearly show that 
fitting bilinear models to a time series can result in a substantial reduc-
tion of the mean sum of squares of residuals 
a^ and the AIC values. 
5. 
FORECASTS 
5.1. Preliminaries 
The efficiency of a time series model can be judged by its forecasting per-
formance. Hence, it is reasonable to compare the forecasting performance of 
a fitted bilinear model with that of a fitted linear model. 
Let 
{x 
| s > 0} 
be observations of a series 
{X } and, when at time 

30 
Wolfgang Birkenfeld 
t=n , suppose that a future value of 
x
n + h
 
h a s t o b e forecasted. This 
forecast, denoted by X , , is made at time n for h steps ahead. It 
n+h 
^ 
is well known that E[(X , - X , ) ] is minimum if and only if 
n+h 
n+h 
X 
= E[X 
I x 
; s > 0] 
n+h 
n+h 
n-s 
= E [X 
, 
c n+h 
(5.1) 
where E 
stands for conditional expectation, 
c 
The general procedure for obtaining forecasts for linear models (cf. Gran-
ger & Newbold 1977) can be similarly applied for bilinear models; i.e. the 
model equation is written down for X 
, optimum forecasts take the place 
n+h 
of anything that will occur after time n and everything on the rhs that 
has occured already is replaced by the observed values. Thus, the bilinear 
model (2.4), in its forecasting form, is 
Xn+h "" 
where 
1 a,X ±. . 
. , 
l n+h-i 
i=l 
P 
Q 
+ I 
I 
k=l 1=1 
CkJlXn+h-k£n+h-)i 
(5.2) 
X n+h-i 
n+h-i 
n+h-i 
for 
for 
i > h 
i < h 
(5.3) 
X 
e 
n+h-k n+h-il 
n+h-k n+h-«, 
X 
£ 
n+h-k n+h-i. 
E [X 
, e , 1 
c n+h-k n+h-i. 
for 
for 
for 
for 
k, £ > h 
k < h < I 
k < I < h 
k,h > I 
(5.4) 
Notice that care has to be taken with products X 
£ 
, whose forecasts 
r 
n+r n+s 
are, for r>s , not the products of the individual forecasts for X 
and 
n+r 
e 
, because these variables are not independent. Since £ 
is white 
n+s 
t 
is either zero (if s>0) or just the 
n+s 
(if s<0). In practical situations however, the E 
will 
n+s 
noise, the optimum forecast for 
value £ n 
not be known, but must be estimated. This is usually done by the residuals 
from the fitted model, if max(p,Q) is small in comparison to the finite 
length of the series. 

Bilinear Models for Time Series Analysis 
31 
5.2. 
Numerical Illustrations 
5.2.1. Simulated data 
In section 4.2.1, a linear AR(5) model and a bilinear BAR(3,2) model have 
been fitted to an artificial series which was generated from the BAR(2,2) 
model (4.1). Numerical values of forecasts for this simulated series can 
be omitted here for obvious reasons: since the true model is known to be 
BAR(2,2), it is evident that the forecasting performance of the fitted 
BAR (3,2) model is superior to the fitted AR(5) model. The reduction in the 
mean sum of squares of the one step ahead forecast errors is approximately 
32 per cent, i.e. quite substantial. 
5.2.2. The sunspot numbers 
In section 4.2.2, three models have been fitted to the sunspot numbers from 
1700 to 1945: the AR(9) model (4.3), theBAR(2,3) full model (4.4) and 
the BAR(9,7) subset model (4.5). For the next ten years, from 1946 to 1955, 
one step ahead forecasts were computed by the aid of the forecasting 
one step ahead forecasts from : 
year time 
t 
true values 
X t 
AR model 
(4.3) 
BAR model 
(4.4) 
BAR model 
(4.5) 
1946 
247 
92. 6 
59 .0 
56 5 
77.9 
1947 
248 
151 6 
118 .8 
120 .3 
130.0 
1948 
249 
136. 3 
157 .3 
131 .8 
150.0 
1949 
250 
134. 7 
105 .0 
103 .0 
120.0 
1950 
251 
83. 9 
105 .1 
83 .9 
86.3 
1951 
252 
69. 4 
44 .9 
45 .7 
51.7 
1952 
253 
31 5 
41 .0 
44 .7 
39.1 
1953 
254 
13. 9 
11 .9 
15 8 
11.9 
1954 
255 
4. 4 
4 .5 
12 8 
3.2 
1955 
256 
38. 0 
23 .9 
7 7 
25.9 
variance of 
cast errors 
fore-
415 .1 
323 6 
135.3 
variance reduction 
compared to AR model : 
0 .0 % 
22 0 % 
67.4 % 
Table 2: One step ahead forecasts for the sunspot numbers 

32 
Wolfgang Birkenfeld 
formulas (5.2)—(5.4)- The numerical results, obtained from all three mo-
dels, are summarized in table 2 above. Table 2 also contains the computed 
mean sums of squares of the one step ahead forecast errors as measures for 
the forecasting performance of the models. 
As can be seen from table 2, the forecasting performance of both fitted bi-
linear models (4.4) and (4.5) is clearly superior to the forecasting per-
formance of the fitted linear AR model (4.3). The 67.4 per cent reduction 
in the mean sum of squares of the one step ahead forecast errors is really 
convincing. 
6• CONCLUDING REMARKS 
Bilinear models, originally introduced by control theorists, are finite pa-
rameter models which can, to a certain degree of accuracy, approximate non-
linear phenomena. 
Compared to linear models, an as well established theory does not (yet) 
exist. However, it can be expected that ¿he so far existing theoretical 
knowledge will be extended and improved. 
From a practical point of view, the following pragmatic aspects of model 
building require special consideration: 
- the complexity of a model will be bounded by the number of available data, 
- the same effect can be produced by limited computing time or storage ca-
pacity of computers and 
- a complicated model is often hard to handle or to interpret. 
The collection of empirical experiences has just begun. This paper is to 
be considered as a step in that direction. 
Acknowledgements: The author is indebted to Dr. T. Subba Rao, University of 
Manchester Institute of Science and Technology (UMIST), 
for fruitful discussions on the subject and many valuable suggestions. 
Thanks are also due to Dr. V. Haggan (UMIST) and Dr. T.S. Rao for making 
computer programs available to the author. 

Bilinear Models for Time Series Analysis 
33 
References 
Abdrabbo N.A. & M.B. Priestley (1967), On the prediction of non-stationary 
processes, J. R. Statist. Soc . (B) , 29_ , 570-585 
Akaike H. (1977) , On entropy maximization principle, in: Krishnaiah P.R. 
(ed.), Applications of statistics, North-Holland, Amsterdam, 27-41 
Akaike H. (1978) , communication at the 1978 international conference on 
time series analysis and forecasting, Cambridge University 
Box G.E.P. & G.M. Jenkins (1970), Time series analysis, forecasting and 
control, Holden-Day, San Francisco 
Brockett R.W. (1976) , Volterra series and geometric control theory. Auto-
mática, 12_ , 167-176 
Granger C.W.J. & A.P. Andersen (1978) , An introduction to bilinear time se-
ries models, Vandenhoeck & Ruprecht, Göttingen 
Granger C.W.J. & P. Newbold (1977), Forecasting economic time series, Aca-
demic Press, New York 
Haggan V. & T. Subba Rao (1979) , personal communication 
Hampe J.F. (1980), Zur Theorie und Anwendung der Schätzung zeitabhängiger 
Spektren, Dissertation, University of Bielefeld 
Möhler R.R. (1973), Bilinear control processes - with applications to engi-
neering, ecology and medicine. Academic Press, New York 
Priestley M.B. (1965), Evolutionary spectra and non-stationary processes, 
J.R.Statist.Soc.(B), 27 , 204-237 
Priestley M.B. (1967), Power spectral analysis of non-stationary random pro-
cesses, J.Sound Vib., 6_ , 86-97 
Priestley M.B. (1970), Time dependent spectral analysis and its applications 
in prediction and control, Proc. 1969 symposium on random data analysis 
Southampton 
Priestley M.B. (1978) , Non-linear models in time series analysis, The Stat-
istitian, 27_ , 159-176 
Schröter E.-H. (1980), Porträt eines Sterns, in: Mannheimer Forum 80/81, 
Hoffmann und Campe, Hamburg, 115-171 
Subba Rao T. (1977), On the estimation of bilinear time series models, Paper 
presented at the 1977 ISI meeting. New Delhi 
Subba Rao T. (1978), On the theory of bilinear time series models, Technical 
report No. 87, University of Manchester - Institute of Science and 
Technology (UMIST), Manchester 
Subba Rao T. (1979), On the theory of bilinear time series models II, Tech-
nical report No. 121, UMIST, Manchester 
Subba Rao T. (1981), On the theory of bilinear time series models, J.R.Sta-
tist. Soc. (B), to appear 
Subba Rao T. 8, M.M. Gabr (1979) , A test for linearity of stationary time 
series. Technical report No. 105, UMIST, Manchester & J.R.Statist.Soc. 
(C), to appear 1981 
Tong H. & K.S. Lim (1980)(Threshold autoregression, limit cycles and cycli-
cal data, J.R.Statist.Soc . (B) , 42_ , 245-292 
Waldmeier H. (1961), The sunspot activity in the years 1610-1960, Schulthess, 
Zürich 
Wiener N. (1958), Non-linear problems in random theory, MIT Press, Cambridge 
Yule G.U. (1927), On a method of investigating periodicities in disturbed 
series with special reference to Wölfer1s sunspot numbers, Philosoph. 
Trans.R.Soc.(A), 226 , 267-298 


Tests auf Gleichverteilung - Ein Gütevergleich 
Herbert B üning 
1. VORBETRACHTUNG 
Ein vom praktischen wie vom theoretischen Standpunkt aus wichtiges Pro-
blem der statistischen Analyse bezieht sich auf die Oberprüfung eines 
bestimmten Verteilungstyps: 
Es seien 
unabhängige Zufallsvariablen mit unbekannter Vertei-
lungsfunktion F . Zu testen ist die Hypothese: 
H 
: F (x) = FQ(X) 
V x e ]R 
H 1 : F (x) 4 F (x) 
3 
x e 3R . 
Je nachdem, ob die Verteilungsfunktion F Q mit allen ihren Parametern 
vollständig spezifiziert ist oder nicht, heißt H q einfach oder zusammen-
gesetzt. Tests zur Überprüfung dieser Hypothese werden Anpassungstests 
(tests of goodness of fit) genannt; sie nehmen mittlerweile einen breiten 
Raum in der statistischen Literatur ein. 
Zu den bekanntesten und am häufigsten angewandten Anpassungstests zählen 
2 
ohne Zweifel Pearson's X -Test und der Kolmogorow-Smirnow-Test (K-S-Test); 
Vor-und Nachteile dieser beiden Tests im direkten Vergleich sind hinläng-
lich bekannt, siehe z.B. Büning/Trenkler (1978) . Eines der wichtigsten 
2 
Probleme bei der Anwendung des X -Tests liegt in der "Ungewißheit" begrün-
de^ wie gut bei vorgegebenem Stichprobenumfang und vorgegebener Klassen-
2 
9 
zahl die diskrete Verteilung der X -Statistik durch die stetige x -Ver-
teilung approximiert wird - zumindest am Rande der Verteilung im Hinblick 
auf die Festlegung kritischer Werte. Faiistregeln zur (angeblichen) Be-
seitigung dieser Unsicherheit gibt es in Fülle; sie schwanken, bezogen 
auf die zu erwartende Anzahl N. der Beobachtungen in der i-ten Klasse, 

36 
Herbert Büning 
zwischen den Forderungen E(N^) > 1 und E(N^) > 20. Für eine ausführ-
liche Diskussion dieses Approximationsproblems nebst Berechnungen exakter 
2 
Quantile der X -Verteilung sei auf Tate/Hyer (1973), Büning/Jordy (1976) 
und Smith u.a. (1979) verwiesen. Die begrenzte Anwendung des K-S-Tests 
resultiert aus der Tatsache, daß das vorliegende umfangreiche Tabellen-
werk mit Quantilen der Verteilung der K-S-Statistik sich nur auf den Fall 
einfacher Hypothesen unter (zusätzlicher) Annahme einer stetigen Vertei-
2 
lungsfunktion F q bezieht. Was den Gütevergleich zwischen dem X -Test 
und dem K-S-Test betrifft, so scheinen die vorliegenden Ergebnisse mehr 
2 
für den K-S-Test als für den X -Test zu sprechen. 
Aber gerade hier liegt das Problem. Welche Alternatiwerteilung Fj^ =f F q 
soll zwecks Gütestudien der Tests und entsprechender Vergleiche postuliert 
werden? In der statistischen Praxis wird in der Regel keine Alternativ-
verteilung vorgegeben; und vom theoretischen Standpunkt aus ist festzu-
halten, daß es im Gegensatz zu den parametrischen Testverfahren im Raum 
aller (stetigen) Verteilungsfunktionen keine "natürliche" Metrik gibt, mit 
der Abstand zwischen der Verteilung F q und einer beliebigen Alternative 
F^ zu messen ist. Ein mögliches Maß, das von Massey (195C) eingeführt 
wurde, bildet die (nichtgeordnete) Menge aller Alternativen in die (geord-
nete) Menge der positiven reellen Zahlen zwischen 0 und 1 ab: 
A = sup ] F. (x) - F (x) I . 
x 1 1 
0 
1 
In der oben zitierten Arbeit von Massey und in den Publikationen von Wil-
liams (1950) und Massey (1951) sind Güteberechnungen des K-S-Tests, die 
2 
auf dem Abstandsmaß A basieren, vorgenommen und Vergleiche zum X -Test 
durchgeführt worden. Das bessere Abschneiden des K-S-Tests mag auf die 
•^ür den K-S-Test "günstigere" Metrik A zurückzuführen sein. 
Shapiro u.a. (1968) gingen einen anderen Weg. In einer der wohl umfang-
reichsten Simulationsstudien zu diesem Themenkomplex wählten sie 45 Al-
ternativen aus 12 Verteilungsfamilien wie z.B. aus der Poissonverteilung 
und der x 2 - V e r t e i l u n 5 a u s u n d unterteilten diese Alternativen in 5 Grup-
pen, unterschieden nach Schiefe und Wölbung. Neun Anpassungstests auf 
Normalverteilung wurden unter diesen Alternatiwerteilungen miteinander 

Tests auf Gleichverteilung 
37 
verglichen. Wie fragwürdig die Auswahl von Alternativen sein kann, ver-
deutlichen Untersuchungen zu neu eingeführten Tests mit ausgesprochenen 
dürftigen Güteresultaten, wobei die Alternative(n) dann häufig so speziell 
(um nicht zu sagen: ausgefallen) gewählt ist (sind), daß der neue Test 
im Gütevergleich zu klassischen Anpassungstests besser abschneidet. 
Hier wird das Konstruktionsprinzip auf den Kopf gestellt nach dem Motto: 
Man denke sich eine Teststatistik aus und finde eine oder zwei Alternativen 
(was keine Schwierigkeiten machen dürfte), unter denen der neue Test hö-
here Güte als seine "Konkurrenten" aufweist. 
Diese knappen Überlegungen mögen aufzeigen, wie willkürlich die Auswahl 
von Alternativen im Zusammenhang mit der Diskussion und dem Vergleich von 
Anpassungstests sein kann. 
In den folgenden Abschnitten wollen wir uns auf den Fall einer einfachen 
Hypothese H^ : 
F = F q mit stetigem F q beschränken. Unter Berück-
sichtigung der "probability integral transformation U^: = FQ(X£'" redu-
ziert sich dann ein Test für diese Hypothese auf einen Test auf Gleichver-
teilung über 
(0, 1), da die Zufallsvariablen U^ über 
(0, 1) gleich-
verteilt sind. 
Tests auf Gleichverteilung spielen gerade bei Simulations-
studien im Zusammenhang mit der Erzeugung von Zufallszahlen eine wichtige 
Rolle. 
Im Fall einer zusammengesetzten Hypothese mit einer Verteilungsfunktion 
F (x|0,,...,e ) , in der die m Parameter 
0.,...,6 
durch m 
Statisti-
0 
1
m 
i
m 
ken 
vorab geschätzt werden müssen, sind die transformierten 
Variablen V^: = Fq(X^) weder unabhängig noch gleichverteilt; ihre Ver-
teilung hängt im allgemeinen von F_ und 6,,...,0 
ab, siehe David/ 
O 
i
m 
Johnson (1948). In einer Arbeit von O'Reilly/Quesenberry (1973) werden 
Transformationen - conditional probability integral transformations -
vorgeschlagen, unter denen viele Tests für zusammengesetzte Hypothesen wie 
im Falle einfacher Hypothesen auf Tests auf Gleichverteilung zurückgeführt 
werden können. In diesem Sinne gewinnen Anpassungstests auf Gleichvertei-
lung eine zusätzliche Bedeutung. 

38 
Herbert Büning 
Ziel ,dieser Arbeit soll es sein, auf der Basis vorliegender Gütestudien 
zu den verschiedensten Tests auf Gleichverteilung unter besonderer Berück-
sichtigung der Klassen von Alternativen nach Stephens (1974) Vergleiche 
unter diesen Tests anzustellen und für jede Klasse einen "besten" Test 
auszuzeichnen oder sogar eine globale Empfehlung auszusprechen. Es wird 
sich zeigen, daß die klassischen und zumeist bevorzugten Tests, der K-S-
2 
Test und der X -Test, in keinem Fall zu den "Spitzenreitern" gehören, und 
daß in der Praxis mehr oder weniger unbekannte Tests z.T. einen deutlichen 
Vorsprung haben. 
Bis vor einigen Jahren lagen - abgesehen vom K-S-Test - wegen der Schwie-
rigkeit der Berechnungen im finiten Fall fast nur Ergebnisse zu den asymp-
totischen Verteilungen der Teststatistiken unter H 
oder H^ vor. Mit 
dem Einsatz leistungsfähiger Rechner konnten in jüngster Zeit zahlreiche 
zeitaufwendige Simulationsstudien zur Bestimmung kritischer Werte und von 
Gütewerten durchgeführt werden; insbesondere sei auf die Arbeiten von 
Stephens (1974), Quesenberry/Miller (1977), Miller/Quesenberry (1979), 
Finkelstein/Schafer (1971), Lording (1973) und Taylor/Grubbs (1979) ver-
wiesen. Ohne die Entwicklung von Großrechnern wären Simulationsstudien 
diesen Umfangs zweifelsohne nicht möglich gewesen. 
2. AUSGEWÄHLTE TESTSTATISTIKEN UND IHRE VERTEILUNGEN UNTER H Q 
Wie bereits eingangs angedeutet, ist die Menge der vorliegenden Anpassungs-
2 
tests kaum noch überschaubar. Neben den bekanntesten Vertretern, dem X -
Test und dem K-S-Test, seien vor allem die Tests von Cramer-von Mises, 
Anderson-Darling, Watson, Kuiper, Neyman, Sherman, Riedwyl und Greenwood 
genannt. Einige der Anpassungstests, wie z.B. der Kuiper- und der Watson-
Test, sind eigentlich als Tests für Punkte "on a circle" konstruiert, 
können aber natürlich auch als Tests für Punkte "on a line" herangezogen 
werden; die Umkehrung gilt offensichtlich nicht, weil die Werte von Test-
statistiken für Punkte "on a line" in der Regel von der Wahl des Ursprungs 
abhängen. 
Eine große Teilmenge der Menge der Anpassungstests bilden die Tests, die 

Tests auf Gleichverteilung 
39 
auf spacings (coverings) basieren, so z.B. die Tests von Sherman und 
Greenwood. Als spacings werden die Differenzen 
~ 
^^,i=l,...,n+l 
mit 
= 0 und 
U(n+j) = 1 bezeichnet, wobei U ^ 
die i-te geordnete 
Statistik von über (0, 1) gleichverteilten Zufallsvariablen U, ,...,U 
1 
n 
bedeutet, siehe dazu Pyke (1965). 
Die wichtigste Klasse von Anpassungstests bilden aber sicherlich die soge-
nannten F^-Tests; sie basieren auf Statistiken, in die die empirische Ver-
teilungsfunktion F^ eingeht. Gemessen wird dabei der Abstand der theore-
tischen Verteilungsfunktion F^ von F^ . Die bekanntesten F^-Statistiken 
lassen sich nach dem sup-Kriterium und dem Integral-Kriterium ordnen. Zur 
ersten Gruppe gehören die Statistiken von Kolmogorow-Smirnow und Renyi, 
zur zweiten die von Anderson-Darling und Cramer-von Mises. 
Eine wichtige Modifikation der Anpassungstests für einfache Hypothesen ist 
Durbin (1961) zuzuschreiben; diese Modifikation basiert auf der Transfor-
mation der geordneten Variablen 
vor der Berechnung der 
Teststatistik: 
Sei 
Cl 
= U ( 1 ) 
Ci 
= ü(i) " °(i-l)' 1 = 2 
n 
Cn+1 = 1 " ü(n) ' 
und bezeichne 
C(jj 
i-te geordnete Statistik von Cj,...,Cn+j . Trans-
formieren wir nun die Variablen C
,
C
, 
. , in 
(1) 
(n+1) 
Cj = (n+2-i) (C(i)-C(i_1) ) , C ( Q ) = 0, i=l , . . . ,n+l , 
und setzen: 
u: = 
> c: 
J 
1 
3 
i=l 
= C ( 1 ) + ... + c 
} + (n+2-j) C ( j ), j = l , . . . ,n , 
so kann gezeigt werden (siehe Durbin (1961)), daß Ujjj,...,UJnj und 
U
.
U
, 
. dieselbe Verteilung haben. Das bedeutet: Ist U... die 
vi; 
In) 
U) 
i-te geordnete Statistik der über (0, 1) gleichverteilten Zufallsvariab-
len U.,...,U 
(Nullhypothese H ) , so hat jeder Test, ob er nun auf 
I
n 
O 
u(l)»•••'u(n) oder auf 
uq)'•••'u(n) basiert, die gleichen Eigenschaf-
ten unter HQ . Die Idee, die hinter dieser Transformation der Variablen 

40 
Herbert Büning 
U,,,,...,U. . steht, ist die, eine Verbesserung der Güte solcher Tests 
(1) 
(n) 
unter gewissen Typen von Alternativen zu erreichen. Auf eine Untersuchung 
von Quesenberry/Miller (1977), die dieser Vermutung von Durbin nachgeht, 
werden wir später zu sprechen kommen. 
Für den in Abschnitt 3 vorzunehmenden Gütevergleich von Tests auf Gleich-
verteilung über 
(0, 1) wollen wir folgende 7 Teststatistiken betrachten. 
Dabei sind zu jedem Test Angaben über die Verteilung unter H^ bzw. Lite-
raturhinweise zu Tabellen mit kritischen Vierten gemacht. 
2 
(1) Pearson's X -Statistik: 
s 
s 
(n 
-np ) 2 
X 2 = I — 3 
, 
S 
j=l 
n P j 
1 
2 
speziell für p^ = ... = pg = — und 
s = 10 bzw. s = 20 . X^ ist 
unter H^ asymptotisch x 2 - v e r t e i l t mit s - 1 FG ; zur exakten Ver-
teilung von X 2 
siehe Good u.a. (1970), Tate/Hyer (1973), Büning/ 
s 
Jordy (1976) und Smith u.a. (1979). 
(2) Pearson's Wahrscheinlichkeits-Produkt-Statistik O: 
n 
q = 
n 
u. . 
i 
i=l 
Diese Statistik ist äquivalent zu: 
n 
L = -2 y In U. . 
i=l 
1 
L ist unter H Q asymptotisch x 2 _ v e r t e i l t mit 2n FG . 
(3) 
Kolmogorow-Smi1now-Statistik: 
K = max { max ( |u,.. - 
1 I , iü,., - r- |)} 
1<i!n 
1 (i) 
n 
1 
1 (l) 
n 
' 
Zur exakten und approximativen Verteilung von K unter H^ siehe 
Büning/Trenkler (1978). 

Tests auf Gleichverteilung 
41 
2 
(4) Anderson-Darling-Statistik A : 
1 , , > 
>2 
2 
f n 
A = n 
-r.i ; 
du t 
wobei 
J 
u(l-u) 
0 
F^ die empirische Verteilungsfunktion von u^ , .. .,un ist mit 
E(F (u)) = u und Var (F (u)) = U ^ 
unter der Hypothese der 
n 
n 
n 
Gleichverteilung. Schreiben wir A~ in Summationsform, so ergibt 
sich: 
2 
1 
n 
A = -n 
Y (2i-l) (In U... + In (1-U, . ,J) . 
n 
(i) 
(n-i+1) 
2 
Zur asymptotischen Verteilung von A 
unter HQ siehe Anderson/ 
Darling (1952) ; Tabellen mit kritischen Werten sind bei Pearson/ 
Hartley (1972) zu finden. 
2 
(5) Cramer-von Mises-Statistik C : 
C2 = n 
(F (u) - u)2 du , 
n 
0 
oder in Summationsform: 
2 
1 
v , 
2i-l,2 
c = i ^ T + . V u ( i ) - — ' 1 
• 
1=1 
Tabellen mit kritischen Werten sind bei Pearson/Hartley (1972) zu 
finden. 
2 
(6) Watson-Statistik W : 
W2 = C2 - n(Ü - h 2 , wobei Ü = - \ U, ist 
n i=l 1 
2 
und C 
durch (5) gegeben ist. Quesenberry/Miller (1977) haben kri-
tische Werte über Simulationsmethoden für n = 2(1) 10 und a = O.Ol, 
0.05 und 0.1 bestimmt, siehe auch Pearson/Hartley (1972). 
2 
(7) Neyman-Statistik p^: 
o 
1 c 
n 
o 
r=l i=l 

42 
Herbert Büning 
wobei 7rr(u), r=l,...,c, Legendre1 sehe Polynome sind. Diese lauten 
für c = 0 , 1,..., 4: 
V u ) 
= 1 
TTjfu) = /l2 (u-j) 
TT2(U) = 
(6(u-j)2 - J) 
tt3(u) = /71 (20 (u -J) 3 - 3(u-j)) 
TT4(U) = 3 (70(u-j)4 - 15(u-i-)2 
Bei der Konstruktion dieser Statistik ging Neyman (1937) von einer 
Klasse von Alternatiwerteilungen 
(H^) aus, die zur Exponentialfa-
milie gehören und deren Dichten gegeben sind durch: 
c 
y 6 TT (u) 
l 
r r 
f (u | H ) = a(6, ,...,0 ) e r _ 0 
, 0 < u < 1 . 
c 
1 
c 
Durch geeignete Wahl der Parameter 6,,...,0 
können durch diese 
l 
c 
Klasse von Alternativen beliebig geringe Abweichungen von der Gleich-
n 
Verteilung über 
(0, 1) beschrieben werden. 
Da 
T 
= . Z.ir (u.) 
suf-
r 
1=1 r 
2. 
fizient für 0^ ist, r=l,...,c , können wir ohne Güteverlust irgend-
eine Funktion von T, ,...,T 
als Teststatistik wählen. Analog 
1 
C 
2 
der Frage nach der Wahl der Klassenzahl s beim Xg-Test stellt 
sich beim Neyman-Test das Problem der Festlegung eines geeigneten c , 
der Anzahl der Parameter zur Beschreibung der Alternatiwerteilung. 
Auch hier gibt es - wie nicht anders zu erwarten - keine verbindliche 
2 
2 
Antwort. Der Vorteil des p -Tests gegenüber dem X -Test mag darin 
C 
2 
S 
2 
liegen, c so zu wählen, daß der p -Test höhere Güte als der X -Test 
c 
2 
s 
hat, siehe Kendall/Stuart (1972). Die p^-Statistik ist unter H q 
asymptotisch x2-verteilt mit c FG, siehe Neyman (1937). Tabellen 
mit kritischen Vierten (über Simulationsmethoden) für c = 1, 2, 3, 4, 
n = 2(1)20, 30, 40, 50 und a = O.Ol, 0.05, 0.1 sind bei Miller/ 
Quesenberry (1979) zu finden. 
Für die Tests von Kolmogorow-Smirnow, Anderson-Darling, Cramêr-von Mises 
und Watson, die auf der empirischen Verteilungsfunktion F^ basieren, 
hat Stephens (1970, 1974) auf originelle Weise Modifikationen vorgeschla-

Tests auf Gleichverteilung 
43 
gen, die es erlauben, diese Tests ohne ausgedehnte Tabellen mit kritischen 
Werten anzuwenden. Für ein vorgegebenes a wird unabhängig vom Stichpro-
benumfang n ein fester kritischer Wert bei jeder dieser 4 Teststatisti-
ken ausgezeichnet. Die Modifikation wird dann über eine Teststatistik T* 
2 
2 
vorgenommen, die eine Funktion f der Original-Statistik T 
(K, A , C 
2 
* 
bzw. W ) und des Stichprobenumfangs n ist: T = f (T, n) . Die folgen-
de Tabelle gibt einen Auszug aus der Tabelle von Stephens (1974) wieder: 
Statistik T 
Transformation T 
a 
O.Ol a 
0.05 a = 0.1 
K 
K(;/n +0.12 +0.11/'/n1 
1 628 
1 358 
1 224 
A2 
für alle n > 5 
3 875 
2 492 
1 933 
c 2 
(C2 -0.4/n +0.6/n2) (1 + 1/n) 
0 743 
0 461 
0 347 
T„2 
(U2 -0.1/n + 0.1/n2) (1 + O.B/n) 
0 267 
0 187 
0. 152 
* 
Tab. 1: Kritische Werte von T 
Testverfahren: 
1. Schritt: Berechne den Wert der Original-Statistik T. 
* 
2. Schritt: Berechne die zu T gehörende Transformation T 
in Abhängig-
keit von n • 
* 
3. Schritt: Prüfe, ob T 
den entsprechenden kritischen Wert übersteigt; 
wenn ja, so wird H Q auf dem Niveau a 
(rechtseinseitiger 
Test) abgelehnt. 
* 
Die in der Tabelle angegebenen kritischen Werte für T 
sind die Quantile 
der asymptotischen Verteilung der entsprechenden Original-Teststatistik T 
unter H ; der Faktor bei T als Funktion von n wurde nach dem trial-
0 
2 
and-error-Prinzip gefunden. Die Statistik A 
konvergiert so rasch, daß 
keine Modifikation für n >_ 5 erforderlich ist. Für nähere Einzelheiten 
und zur (erstaunlichen) Güte der Approximation des vorgegebenen a durch 
* 
das tatsächliche a 
sei auf Stephens (1970) verwiesen. 

44 
Herbert Büning 
3. GÜTEVERGLEICH 
Für die in Abschnitt 2 vorgestellten Tests wollen wir nun unter Auswertung 
einiger Simulationsergebnisse von Stephens (1974) und Quesenberry/Miller 
(1977) und Miller/Quesenberry (1979) einen Gütevergleich vornehmen. Grund-
lage für die Güteberechnungen sind die bereits in Abschnitt 1 erwähnten 
3 Alternativmodelle von Stephens bzw. Modifikationen von Quesenberry/Mil-
ler. Diese 3 Alternativen zur Gleichverteilung über (0, 1) decken einen 
breiten Bereich ab. 
Stephens Alternativen: 
Alternative A: 
fft(u) = (k+1 ) (l-u)k für 0 < u < 1 
k+1 
Es gilt: E(U) = 1 - :—- , lim E(U) = 0 
k+2 
k-*» 
k+1 
Var(U) = 
, lim Var (U) = 0 
(k+2) (k+3) 
Abb. 1: Alternative A 

Tests auf Gleichverteilung 
Alternative B: 
r 
k k 
(k + 1)2 u 
für 0 < u £ 0.5 
f (u) = J 
B 
(k + 1 ) 2 (1 - u) 
für 0.5 < u < 1 
Es gilt: E(U) = -
V a r ( ü ) = 2 (k + 2) (k + 3) ' i S V a r ( U ) 
Abb. 2: alternative 3 
Alternative C: 
(k + l)2k ¿ - u ) k für 0 < u £0.5 
fc ( u ) = i 
k 
1 k 
' (k + l)2 
für 0.5 < u < 1 
Es gilt: E(U) = j 
var(u) = iTirisT - ÜS var(u) = i 

46 
Herbert Büning 
Abb. 3 : Alternative C 
Alternative A beschreibt die Konzentration der Punkte nahe 0 , Alterna-
tive B eine solche in der Nähe von 0.5 und Alternative C zwei Cluster 
nahe 0 und 1. 
Der Fall k = 0 in den 3 angegebenen Dichtefunktionen entspricht der Null-
hypothese der Gleichverteilung über (0, 1). Stephens (1974) hat für 
k = 0.5, 1 und 2 in den obigen Alternativen, für n = 10, 20, 40 und 
2 
2 
2 
2 
a = 0.1 folgende Tests miteinander verglichen: A , C , K, Q, W 
und X 
(mit Klassenzahl s = 4 für n = 20 bzw. s = 8 für n = 40). 
Der Q-Test ist bester Test unter der Alternative A, was unmittelbar aus 
dem Neyman-Pearson-Lemma folgt. Stephens kommt zu folgendem Ergebnis: 
2 
2 
2 
(1) A 
und C 
entdecken besser als K und W 
Abweichungen vom Er-
wartungswert . 
2 
2 
2 
(2) W 
entdeckt deutlich besser als A , C 
und K Abweichungen von 
der Varianz. 

Tests auf Gleichverteilung 
47 
(3) A 
und C 
erweisen sich insgesamt gesehen als besser im Vergleich 
zu K . 
2 
(4) Y. scheint den ^-Statistiken unterlegen zu sein; allerdings liegen 
für X2 nur wenige Gütewerte vor. 
(5) Kein Test ist bezüglich aller 3 Alternativen besser als seine Konkur-
renten. 
Quesenberry/Miller (1977) und Miller/Quesenberry (1979) haben die 3 Alter-
nativen A, B, C von Stephens leicht modifiziert. 
Alternative 1 (= A): 
k 
V u ) 
= T+k" u k + 1 V 
1)(U) • 
Alternative 2 (= B) 
Die Dichte g2 (u) ist die Dichte des Mittelwerts von (k + 1) über (0, 1) 
gleichverteilten, unabhängigen Zufallsvariablen: 
k+1 
g2(u) = j h.k(u) I 
<«) , 
1 = 1 
(k+T' k+T) 
wobei die Funktionen 
Polynome k-ten Grades sind, siehe Wilks (1962, 
S. 204) . 
Alternative 3 (= C): 
Diese Alternative ergibt sich aus Alternative 2 durch die Transformation: 
V = (U+|) 
1-, (U) + (U-|-) I . 
(U) , 
2 
2 
(1 1) 
wobei U die Dichte g2 hat. 
Graphische Darstellungen dieser Alternativen sind bei Quesenberry/Miller 
(1977) zu finden. Unter Zugrundelegung dieser 3 Alternativen (und einer 
weiteren) haben Quesenberry und Miller im Rahmen zweier umfangreicher Simu-
lationsstudien einen Gütevergleich zwischen insgesamt 14 Teststatistiken -
die hier betrachteten 7 Statistiken eingeschlossen - vorgenommen, und zwar 

48 
Herbert Büning 
für Stichprobenumfänge n = 2 , 5, 10, 15, 20, 50 und a=0.05. Jede der in 
der ersten Studie betrachteten 8 Teststatistiken wurde zweifach untersucht, 
einmal mit vr.d einmal ohne Durbin-Transformation. Es zeigte sich, daß in 
den meisten Fällen (72%) durch die Durbin-Transformation kein Güte-Gewinn 
zu verzeichnen war; vor allem bei alternative 1 war die Güte aller 8 Test-
statistiken mit Durbin-Transformation geringer als ohne eine solche Trans-
formation. Hinzu kommt, daß z.T. erhebliche Schwankungen innerhalb einer 
Teststatistik und einer Alternative hinsichtlich des Gewinns bzw. des Ver-
lusts der Güte für die Fälle "ohne - mit Durbin-Transformation"(o.T. - m.T.) 
auftraten, was am Beispiel des K-Tests für die Alternative 3 mit k = 1 
veranschaulicht sei: 
n 
2 
5 
10 
15 
20 
50 
o.T. 0 15 
0 14 0.19 0.27 0.31 0.64 
m.T. 0 10 
0 17 0.22 0.28 0.33 0.58 
Tab. 2: K-Test 
ohne - mit 
Durbin-Transformation 
Auf die Berücksichtigung der Durbin-Transfomation sei deshalb beim nun fol-
genden Gütevergleich der betrachteten sieben Teststatistiken verzichtet. 
In den nachstehenden Graphiken sind die Gütewerte dieser Tests für die Al-
ternativen 1, 2 und 3 - jeweils mit k = 1 und k = 2 - für Stichproben 
umfange n = 2, 5, 10, 15, 20 und 50 sowie a = 0.05 dargestellt. Da 
2 
2 
2 
die Statistiken p 
und X. 
fast durchweg besser abschnitten als p^ 
2 
2 
2 
bzw. X 2 Q , wurden nur p2 und 
berücksichtigt. 
(Die Quadratzahlen 
bei den Statistiken sind in den Graphiken aus Gründen der Übersichtlichkeit 
weggelassen.) 

Tests auf Gleichverteilung 
Abb. 4: Gütewerte für Alternative 1, k = 1 
Abb. 5: Gütewerte für Alternative 1, k = 2 

Herbert Buning 
Abb. 6: 
Gutewerte fiir A l t e r n a t i v e 2, k = 1 
Abb. 7: 
Gutewerte fiir A l t e r n a t i v e 2, k = 2 

Tests auf Gleichverteilung 
Abb. 8: 
Gütewerte für Alternative 3, k = 1 
Abb. 9: 
Gütewerte für Alternative 3, k = 2 

52 
Herbert Büning 
Die Ergebnisse, die aus diesen graphischen Darstellungen ersichtlich sind, 
lassen sich wie folgt zusammenfassen; sie entsprechen dort, wo ein Ver-
gleich möglich ist, denen von Stephens (1974): 
(1) Keine der sieben Statistiken schneidet für alle 3 Alternativen am 
2 
besten ab. Da W 
bzgl. Alternative 1 die geringste Güte hat, kann 
man wohl kaum der globalen Empfehlung von Quesenberry/Miller (1977) 
für diesen Test folgen. Q als bester Test unter Alternative 1 er-
weist sich bezüglich der anderen Alternativen als völlig indiskutabel. 
2 
(2) P2 gehört unter allen Alternativen stets zu den besten Tests. 
2 
2 
(3) XJQ und C 
schneiden mittelmäßig ab, wohingegen K zu den Tests 
mit geringster Güte zählt. A2 als F^-Test direkter Konkurrent von 
K erweist sich als durchweg besser. 
2 
Wagt man eine globale Empfehlung für einen dieser Tests, so könnte man p 
2 
oder - wegen seiner einfacheren Anwendung - A 
nennen. Besser jedoch 
ist es offensichtlich, jeweils nur eine spezielle Empfehlung für einen Test 
bei Vorkenntnis bzw. Vermutung eines bestimmten Alternativtyps zu geben; 
2 
so z.B. für W 
bezüglich Alternative 2. Eines jedoch dürfte nach den 
2 
vorliegenden Ergebnissen unumstritten sein: Der klassische X -Test wegen 
s 
seiner mittelmäßigen Güte sowie seiner nur auf approximativen Quantilen 
basierenden praktischen Anwendung und der klassische K-S-Test wegen seiner 
2 
2 
geringen Güte sollten in der Praxis nun endgültig durch A 
(oder p ) ab-
2 
. 
gelöst v/erden, zumal Tabellen mit kritischen 'Jerten für A 
in ausreichen-
dem Maße vorliegen. Der traditionelle Hinweis auf die einfachere numeri-
sche Handhabung des X2-Tests und des K-S-Tests im Vergleich zu ihren Kon-
kurrenten müßte im Zeitalter* des Computers gegenstandslos geworden sein. 
Literatur 
2 
Büning, H. und Jordy, A. (197G) , Heues vom X -Test und den F^-Tests, 
Diskussionsarbeit des Instituts für Quantitative Ökonomik und Statistik 
der Freien Universität Berlin. 
Bünir.g, H. und Trenkler, G. (1978) , Nichtparametrische statistische 
Methoden, De Gruyter, Berlin. 
David, F.N. und Johnson, N.L. (1948) , The probability integral transfor-
mation when parameters are estimated from the sample, Biometrika 35, 
182-190. 

Tests auf Gleichverteilung 
53 
Durbin, J. (1961), Some methods of constructing exact tests, Biometrika 48, 
41-55. 
Finkelstein, J.M. und Schafer, R.E. (1971) , Improved goodnes£:-of-f it tests, 
Biometrika 58, 641-645. 
Good, I.J., Gover, T.N. und Mitchell, G.J. (1970), Exact distribution for 
X^ and for the Likelihood-Ratio statistic for the equiprobable mul-
tinomial distribution, Jour. Am. Statist. Ass. 6_5, 267-283. 
Kendall, M.G. und Stuart, A. (1973), The advanced theory of statistics, 
Vol. II, Griffin, London. 
Lording, R.K. (1973), Three Kolmogorow-Smirnow-type-one-sample tests with 
improved power properties. Jour. Statist. Comp. Simul. 2_, 139-148. 
Massey, F.J. (1950), A note on the power of a nonparametric test, Ann. 
Math. Statist. 21_, 440-443. Errata: Ibid. (1952), 637-638. 
Massey, F.J. (1951), The Kolmogorow-Smirnow test of goodness of fit, Jour. 
Am. Statist. Ass. 46_, 68-78. 
Miller, F.L. und Quesenberry, C.P. (1979), Power studies of tests for uni-
formity II, Com. Statist. - Simula. Computa. B8 (3), 271-290. 
Neyman, J. (1937) , "Smooth" test for goodness of fit, Skandinavisk Aktua-
rietidskrift 20, 149-199. 
O'Reilly, F. und Quesenberry, C.P. (1973), The conditional probability 
integral transformation and applications to obtain chi-square-good-
nesS'Of-fit tests, Ann. Statist. 
74-83. 
Pearson, E.S. und Hartley, H.O. (1972), Biometrika tables for statisticians, 
Vol. II, Cambridge University Press, London. 
Pyke, R. (1965), Spacings, Jour. Roy. Statist. Soc. (B) 27_, 395-449. 
Quesenberry, C.P. und Miller, F.L. (1977), Power studies of some tests for 
uniformity, Jour. Statist. Comp. Simul. 5_, 169-191. 
Shapiro, S.S., Wilk, M.B. und Chen, H.J. (1968), A comparative study of 
various tests for normality, Jour. .Am. Statist. Ass. 63_, 1343-1372. 
Smith, P.J., Rae, D.S., Manderscheid, R.W. und Silbergeld, S. (1979), 
Exact and approximate distributions of the chi-square statistic for 
equiprobability, Com. Statist. - Simula. Computa. BS (2), 131-149. 
Stephens, M.A. (1970), Use of the Kolmogorow-Smirnow, Cramer-von Mises 
and related statistics without extensive tables, Jour. Roy. Statist. 
Soc. (B) 32_, 115-122. 
Stephens, M.A. (1974), EDF statistics for goodness of fit and some com-
parisons, Jour. Am. Statist. Ass. 69^, 730-737. 
n 
Tate, M. W. und Hyer, L.A. (1973), Inaccuracy of the x'--test of goodness 
of fit, when expected frequences are small, Jour. .An. Statist. Ass. 
68, 836-841. 
Taylor, M.S. und Grubbs, F.E. (1979), Use of tail areas for testing good-
ness-of-fit. Com. Statist. - T'neor. Meth. , AS (15), 1575-1584. 
Wilks, S.S. (1962), Mathematical statistics, Uilev, "Jew York - London. 
Williams, C.A. (1950), On the choice of the number and width of classes 
for the chi-square test of goodness of fit, Jour. Am. Statist. Ass. 
45, 77-86. 


The Classical Ruin Problem 
and Electronic Roulette Machines 
Ulrich Dieter 
1. INTRODUCTION 
In the early years of probability theory games of chance played 
an important role. The basic concepts of probability theory 
were applied to games and gambling phenomena could be explained 
thereafter. Chevalier de Méré's paradoxon could be explained 
in this way; games of cards as Bridge or Skat are often dis-
cussed in exercises in textbooks. A further example is a rather 
simplified version of the roulette game, sometimes called 'Red 
and Black Casinos': A gambler wins or loses a dollar with pro-
£ 
bability p £ 0,5. (If p is a rational number, say p = —, such 
a game can be realized by a roulette wheel: The wheel contains 
r red, r black numbers and s-2r zeros.) Originally, this game 
is unlimited in time. For a termination of the game the classi-
cal concept of vuin games is introduced: 
Let the gambler's initial capital be z and his adversaries 
capital - the bank - be a-z. The game continues until the 
gambler's capital is either reduced to zero or has increased 
to a, that is, until the player or his adversary is ruined. 
Using this concept the gambler's probability of ultimate ruin 
and the mean duration of his game can be calculated in ex-
plicit 
form. For this, see Chapter 14 'Random Walk and Ruin 
Problems' of volume I of Feller's book on probability theory. 
This research was supported by the Austrian Research Council (Fonds zur 
Forderung der wissenschaftlichen Forschung). 

56 
Ulrich Dieter 
However, such a simplified roulette game is rarely played. 
Contrary to general opinion realistic versions of the roulette 
game can also be discussed in this framework: thé probability 
of ultimate ruin and the mean duration of the game can be cal-
culated numerically by a computer. The real roulette with 36 
numbers and one or two zeros will be treated in joint articles 
by J.H. Ahrens and the author. In this paper an electronic 
roulette machine containing twelve numbers and one zero is 
discussed. 
2. DESCRIPTION OF THE GAME 
For the explanation of the game see the sketch of the roulette 
machine. First the gambler has to put money into the machine. 
Subsequently his bets are done by pushing some of the electro-
nic buttons. He has the following possibilities: 
Vie-in, i.e. pushing buttons on single numbers; pays 12 to 1. 
Transversal, 
i.e. pushing buttons on three numbers simul-
taneously; pays 4 to 1. 
Carré, i.e. pushing buttons on four numbers simultaneously; 
pays 3 to 1. 
Manque (1 to 6), Vasse (7 to 12), Rouge, Noir; pays 2 to 1 . 
There is no possibility to bet on two numbers simultaneously 
(called en cheval in roulette games). The player can push 
atmost four buttons en plein or on even chances (Rouge, Noir, 
Manque, Vasse). Otherwise there are one or two buttons. 
Consequently there exist only 
3^ 2^ 
different choices 
for him; many of these are equivalent. 
When all bets have been placed the player starts the game: a 
microcomputer produces one of the numbers 0,1,2,...,12 by a 
pseudo-random number generator. This number determines the 
gambler's gain or loss. 

Ruin Problem and Roulette Machines 
57 

58 
Ulrich Dieter 
3. THE DIFFERENCE EQUATIONS OF THE GAME 
If the gambler puts s^, units on k numbers simultaneously 
(k = 1,3,4,6), he gets 
1 2 
Sj, (-£- - 1) if he wins and -s^ if he loses. 
The probability of 
k 
k 
winning is equal to y^» of losing 1 - yj-
If he selects a strategy s e Y by pushing some buttons, his 
initial capital 
z will change to z+i with probability P . 
Now define 
q(z,a) = Probability of the gambler" s ultimate ruin. 
p(z,a) = Probability of the gambler1 s ultimate winning. 
Since the game is considered as a ruin game, one has 
q(z,a) + p(z,a) = 1. 
The ruin probabilities q(z,a) are now subject to the following 
system of difference equations: 
(1) q(z,a) = Z P. q(z+i,a) 
z = 1,2,...,a-1 
i 
1 
(2) q (0, a) = 1, q (z , a) = 0 for z ^ a-
The side conditions (2) are obvious: if his initial capital is 
zero, he is already ruined; if his initial capital is equal to 
a or greater, he can not get ruined since he will not play 
anyway. In the equation (1) the integers i and the probabilities 
P^ are determined by his strategy s e Tf which depends usually 
on his capital z. For example, if he pushes one button on 
k (k = 1,3,4, or 6) numbers simultaneously, the system of 
difference equations (1) becomes 
(1*) q ( z , a) = Jl q ( z + JL1 _i,a) + (1 - JL) q (z_ •] , a) 
z = 1,2,...,a-1• 
For k = 1 this means 
(1**) q(z,a) = yj q(z+11,a) + || q(z-1,a) z = 1,2,...,a-1. 
An explicit solution of this system of difference equations 
would run as follows: One has to equate q(z,a) to xz. This 
changes (1**) into the equation 

Ruin Problem and Roulette Machines 
59 
(3) 
x = TJ x 1 2 + TT 
o r 
x 1 2 ~ 1 3 x + 1 2 = 
Subsequently, one has to determine the zeros of (3), namely 
x^ = 1, x^i •••/ xi2" 
general solution of (1**) has the 
form 
12 
q(z,a) = I A. x^ 
i = 1 
1 
1 
where the A^ are determined by the side-conditions (2). 
First of all, it is not easy to calculate all zeros of a 
polynomial like (3). Second, if some zeros of (3) are complex 
numbers, the corresponding A^ are complex as well. The sub-
sequent determination of the A^ becomes then rather compli-
cated. This shows that this standard approach is not the 
appropriate method to solve the system (1). 
Fortunately, there are direct methods to solve the system (1) 
which will be discussed later on. 
Before, some expressions related to q(z,a) are defined: 
D(z,a) 
= Expected duration of the game. 
D(z,a|L) = Expected duration of the game, if the gambler 
loses. 
D(z,a|G) = Expected duration of the game, if the gambler 
wins. 
D(z,a) is subject to the following system of difference 
equations: 
(4) D(z,a) = I P.D(z+i,a) + 1 for z = 1,2,...,a-1 
i 
1 
(5) D (0, a) = 0, D (z,a) = 0 
for z ^ a. 
(4) is proved as follows: After one game the capital z has 
been increased to z+i with probability P^; the additive con-
stant 1 stems from the fact that one game has already been 
completed. 

60 
Ulrich Dieter 
Finally, equation (1) can be written in the form 
1 = E P . 
z = 1,2 
a-1 , 
i 
l q (z , a) 
which shows that P^ q(z+i,a)/q(z,a) is the conditional pro-
bability that the gambler is ruined after having reached the 
capital z+i. Consequently, the conditional expected durations 
are subject to the system of difference equations 
(6) D ( z , a I L) = I P. q(z+i,a) D ( z + i , a | L) +1 
i i q ( z , a; 
(7) D (z , a ] G) = X P. 
D ( z+i , a | G) + 1 
i p ^ ^ / aj 
where p(z,a) = 1 -q(z,a). The side-conditions become simply 
(8) D (0,a | L) = D (0,a | G) = 0 
(9) D (z , a | L) = D ( z , a ] G) = 0 
for 
z >_ a 
The transition probabilities P^ in (4), (6) and (7) are of 
course the same as in (1). They are dependent on the chosen 
strategies s e . 
4. SOLVING THE DIFFERENCE EQUATIONS 
Assume that the gambler pushes one button per game. In this 
case the ruin probability q(z,a) is subject to the system 
of difference equations (1 ) which might be written as 
(10) q ( z-1 , a) = 1 
q ( z , a) - 1 ^ 
q (z + 
- 1 ,a) • 
Similarly, the mean duration is given by 
(11 )D(z-1 ,a) = 
D ( z , a) ~ JJ^ D(z + ^ - 1 ,a) -
for z = a-1,a-2,...,2,1 and q(0,a) = 1, D(0,a) = 0, 
q(z,a) = D(z,a) = 0 for z ^ a. 

Ruin Problem and Roulette Machines 
61 
Assume that q(a-1,a) and D(a-1,a) are known. Then according to 
the side-conditions 
1 3 
q(a-2,a) = 1 3_ k q(a-1,a) 
= q a_ 2 q(a-1,a); 
q (a-3 ,a) = 1 
q (a-2 ,a) - 1 3^_k q (a-3 + ^r,a) = q a_ 3 q (a-1 ,a) . 
Consequently all q(a-v,a) can be expressed in the form 
q(a-v,a) = q 
q(a-1,a). 
d 
V 
Finally 
q(0,a) = q Q q(a-1,a) = 1 yields q(a-1,a) = 1/qQ. 
Similarly, 
D (a-2 , a) = ^ 
D (a-1 , a) - ^ 
= qa_2D (a-1 ,a) + d a_ 2 
D (a-3 , a) = y — D (a-2 , a) - 
D ( a - 3 + X ' a ) " Tl^k = qa-3° ( a _ 1 ' a ) 
+ d , 
a-3 
and in this way 
D(a-v,a) = q 
D(a-1,a) +d 
3. V 
3. V 
The side-condition D(0,a) = 0 yields 
0 = D(0,a) = q D (a-1 ,a) +d 
or D(a-1,a) = -d /q . 
^o 
o 
o o 
The calculation of D(z,a|L) and D(z,a|G) is slightly more 
complicated. Since 
D(z,a) = q(z,a)D(z,a|L) + p(z,a)D(z,a|G) 
holds, where p(z,a) = 1-q(z,a), only one of the two expressions 
has to be determined. Introducing 
DL(z,a) = q(z,a)D(z,a|L), 
the difference equation (6) changes into 
(6*) 
DL(z,a) = I P. DL(z+i,a) + q(z,a), 
i 
1 
and, for the special case pushing one button per game 

62 
Ulrich Dieter 
(12) DL (z-1 , a) = 
DL(z,a) - 
DL(z 
- 1 
q(z,a) . 
The side-conditions are of course DL(0,a) = 0 , DL(z,a) = 0 
for z ^ a. Assume again that DL(a-1,a) and q(a-1,a) are known. 
Then 
DL (a-2 , a) = 
DL (a-1 , a) - 
q(a-1,a) 
= q a_ 2 DL (a-1 , a) + SLa_2 q (a-1 , a) 
DL (a-3 , a) = 
DL (a-2 , a) ~ 
DL(a-3 + -^) ~ JJ^ q(a-2,a) 
= q , DL (a-1 ,a) + I 
q (a-1 ,a) 
d -j 
a— j 
and generally 
DL(a-v,a) = q 
DL(a-1,a) +I 
q(a-1,a). 
a-v 
a-v 
The side condition DL(0,a) yields finally 
0 = DL (0,a) = qQDL (a-1 , a) + i,Qq (a-1 ,a) or 
2 
DL(a-1,a) 
= -i q(a-1,a)/q = -I /q . 
O 
I'/ -i0 
QI 1g 
The three expressions q(z,a), D(z,a) and DL(z,a) can now be 
calculated simultaneously. First set 
q (a-1 , a) 
1 , D (a-1 , a) -<- 0, DL (a-1 , a) • -e 0. 
This determines the sequences q^ , d^ , l for v =a-2,a-1,...,1 ,0. 
When q , d , I are calculated set 
^o 
o 
o 
q (a-1 , a) 
1/q q, D(a-1,a) 
DL(a-1,a) 
~ l
0 / % 
and calculate q(z,a), D(z,a) and DL(z,a) for z = a-2 ,a-3,. ..,2,1 ,0. 
(The calculation of q(0,a) = 1, D(0,a) = 0, DL(0,a) = 0 should 
be used as a check for the program.) 
The complete algorithm is now stated in the style of D.E. Knuth. 
A program for the pocket-calculation TI 59 can be found in the 
appendix of this paper. 

Ruin Problem and Roulette Machines 
63 
Algorithm QD. (k is fixed to 6, 4, 3 or 1 ; a is integral) 
0. Set q (a-1 ) •*• 1 , q(a) •+• q (a+1 )<-... -<- q(a+10)+ 0; 
set D(a-1)^0, D(a) 
D (a+1 ) ... *• D(a+10)-<-0; 
set DL (a-1 ) f-0, DL (a) + DL(a+1) 
...-<- DL (a+1 0) -e 0. 
Set J -t- 1 
1. For z = a-2,a-3,...,1, 0 calculate q(z), D(z) and DL(z) 
from 
13 
, . „ x 
k 
. ,12, 
q(z) 
1 3-k 
13 
k 
1? 
13 
D(Z) ^ 
D (Z + 1 ) - 
D ( Z + ' ^ 
1 3-•k 
k 
1 3-•k 
k 
1 3-•k 
13-k 
' 
' 
13-k " 
k ' 
13-k 
DL(z) -e 
DL (z+1 ) - 
D L ( z + ^ ) - 
q(z + 1) 
Set J -«- -J 
2. If J = -1 set q (a-1 ) 
1/q(0), D(a-1) 
-D(0)/1(0), 
DL (a-1) -<- -DL(0)/q2(0) and go back to 1. 
Otherwise return for z = a-1,a-2,...,2,1,0 
q(z,a) 
q (z) , D (z, a) 
D(z), D(z,a|L) -e DL(z)/q(z), 
D (z , a ] G) f- (D (z) - DL (z ) ) / (1 -q (z) ) and stop. 
In the general case of a fixed strategy s e V the ruin pro-
babilities q(z,a) can be calculated by a modified GauB—Seidel 
iteration; Start with initial values q°(z,a) and determine 
1 
2 
q (z,a), q (z,a) recursively by 
(13) qr(z,a) = z p.(z,s) qr_1(z+i,a) 
z = 1,2,...,a-1 
i 
1 
After some iterations the process becomes stable. The mean 
duration D(z,a) and the conditional expectations D(z,a|L) are 
calculated in the same way; of course, the difference equations 
(4), (6) or (7) have to be used instead of (13). For the ini-
tial values 
(14) 
q°(z,a) = 1 - | 
CÜ 
and 
D°(z,a) = z(a-z) 

64 
Ulrich Dieter 
might be taken. They are the solutions q(z,a) and D(z,a) of a 
'fair' game where the probability of winning and losing is 
equal to 0.5. 
Finally, optimal strategies s out of a fixed set "¡5" should be 
discussed. They are determined in the following way: 
Start again with some initial values q°(z,a), for instance (14). 
Determine q (z,a) recursively by 
(15) qr(z,a) = Min {E P.(z,s) qr_1(z+i,a)} 
Sci i 
1 
After a large number of iterations the process becomes stable 
and the best strategies are calculated. This follows from a 
theorem of J.H. Ahrens. 
If q(z,a) is a solution of the set of linear equations 
(16) 
q(z,a) = Min < I P .(z,s) q( zH,a) ( j 
set 
v 
J 
then q(z,a) is minimal for all z where 0 < z < a and the corresponding 
strategy s = s(z,a) is a best strategy for every z. 
If the set of feasible strategies t is small, the calculation 
of optimal strategies contains no difficulties. For example, 
if Tf consists only 'pure' strategies s, i.e. strategies for 
which the capital z can only move to one value z' > z or one 
value z" < z, optimal strategies are easily determined. In a 
forthcoming paper the problem of calculating all optimal 
strategies for the electronic roulette machine will be dis-
cussed in greater detail. However, there is one important dif-
ference to the real roulette with 36 numbers: There the limit 
ofbetting is rather high; for the electronic roulette machine 
this 1 
imit is four for single numbers and for even chances. 
Hence bold strategies of betting as much as possible as in 
'How to gamble if you must1 by Dubins and Savage are not 
allowed.lt will be shown in a paper by J.H. Ahrens and the 
author that these bold strategies are not optimal for the 
real roulette. 

Ruin Problem and Roulette Machines 
65 
5. NUMERICAL RESULTS 
Numerical values for q(z,a) = q(z), D(z,a) = D(z), D(z,a|L) = 
D(z|L), D(z,a|G) = D(z|G) were calculated by Algorithm QD. 
They are given in the next tables for a = 10 and 20. 
z 
q ( z ) 
D ( z ) 
D ( z | L ) 
D ( z | G ) 
q ( z ) 
D ( z ) 
D ( z | L ) 
D ( Z | G ) 
0 
1. oo 
0 
0 
0 
1. oo 
0 
0 
0 
1 
• 92 
1 . o 
1. o 
1. o 
• 93 
3 . 0 
2 . 5 
10. 3 
2 
• 85 
1 . 9 
2 . 0 
1 . 5 
. 8 5 
5 - 5 
M 
10. 3 
3 
• 79 
2 . 8 
3 . 0 
1 . 9 
• 77 
7 - 5 
6 . 8 
1 0 . 2 
It 
. 7 3 
3 . 6 
lt.o 
2.1» 
• 69 
8 . 8 
8 . 5 
9 - 3 
5 
. 6 7 
It.3 
5 . o 
2 . 8 
. 6 o 
9 - 5 
10. 1 
8 . 6 
6 
. 6 2 
5 . o 
6 . 0 
3 . 3 
• 51 
9 - 9 
1 1 . 6 
8. 1 
7 
• 57 
5 . 6 
7 . 0 
3 . 7 
.ito 
8 . 6 
1 2 . 6 
6 . o 
Ö 
• 53 
6 . 1 
8. o 
lt. 1 
• 3o 
7 . 6 
1 3 . 6 
5 . 0 
9 
.1*9 
6 . 7 
9 . o 
1». 5 
. 2 3 
6 . 9 
1lt.6 
U.5 
k = 1 
k = 3 
0 
1. oo 
0 
0 
0 
1. oo 
0 
0 
0 
1 
• 93 
It.2 
3.1* 
1 6 . 1 
• 95 
7- 1 
5 - 9 
3 1 . 8 
2 
.86 
7 - 7 
6 . 5 
1 5 . 6 
• 9o 
1 3 . 2 
1 1 . 3 
3o. 8 
3 
• 79 
1 0 . 5 
9 . 2 
1 5 . 1 
. 8U 
1 8 . 2 
16. 1 
29. 2 
1* 
• 7o 
1 2 . 3 
1 1 . 6 
1 3 . 9 
• 77 
2 1 . 8 
2 o . 3 
2 6 . 8 
5 
. 6 2 
13.lt 
1 3 . 8 
1 2 . 8 
.68 
2 3 . 9 
2 3 . 9 
2 3 . 9 
6 
• 52 
1 3 . 2 
1 5 - 5 
1 0 . 8 
• 59 
2b. 1 
2 6 . 8 
20. 3 
7 
.1*2 
1 2 . 5 
1 7 . o 
9 . 3 
• lt7 
2 2 . 2 
2 9 . 2 
1 6 . 1 
Ö 
. 2 9 
9 . 7 
1 8 . 0 
6 . 3 
• 3lt 
1 7 . 9 
3 0 . 8 
1 1 . 3 
9 
• 2o 
7 . 7 
1 9 . 0 
!+• 9 
. 1 8 
1 0 . 6 
3 1 . 8 
5 - 9 
k = 1» 
k = 6 
The numerical values show that there is a tremendous difference 
between the four strategies. Betting on even chances yields a 
long time of gambling, but a high probability of getting ruined. 
If the initial capital z is small and the aimed capital a is 
relatively large as in the second table, the bold strategy of 
betting on one single number is superior to other strategies. 
It should be noted that optimal strategies can not be derived 
from tables like the one presented here - even if one can press 
only one button per game. Instead one has to apply formula 
(16) of the last theorem. Further details and numerical 
results will be presented in a forthcoming paper. 

66 
Ulrich Dieter 
z 
q(z) 
D(z) 
D(z|L) 
D(z G) 
q(z) 
D( z) 
D(z|L) 
D(z|G) 
0 
1. oo 
0 
0 
0 
1 . 00 
c 
0 
0 
1 
.96 
1 . 9 
1 . 7 
9 3 
• 97 
5. 6 
1*2. 3 
2 
• 93 
3.7' 
3 . 3 
9 6 
• 9b 
10. 8 
8.9 
1*2.2 
3 
• 89 
5.1* 
1*. 8 
10 0 
• 91 
15. 6 
13.0 
1+1.8 
lt 
.85 
7.0 
6.1* 
10 1+ 
.88 
19. 9 
17.0 
1+1.3 
5 
.81 
8.1* 
7 . 8 
10.8 
.85 
23. 8 
20.7 
1+0. 5 
6 
.78 
9 - 7 
9 - 3 
11 1 
.81 
27. 1 
21+.2 
39-5 
7 
.71» 
10.9 
10.7 
11 5 
• 77 
30. 0 
27.6 
38.3 
8 
• 71 
12. 1 
12. 1 
11 9 
• 73 
32. 3 
30.7 
36.9 
9 
.65 
12. 1 
1 3 . 1 
10 3 
.69 
31*. 1 
33.6 
35.2 
10 
.60 
1 2 . 2 
1l*. 1 
9 3 
.65 
35-2 
36.3 
33.1* 
11 
.56 
12. 3 
1 5 . 1 
8 7 
.60 
35- 7 
38.7 
3 1 . 2 
12 
• 51 
12. 3 
1 6 . 1 
8 3 
• 55 
35- 5 
1+1. 0 
28.9 
13 
.1*7 
12.1* 
1 7 . 1 
8 1 
• 5o 
31*. 8 
I+3.0 
26.6 
11+ 
.1*1+ 
12.1+ 
1 8 . 1 
8 0 
.1+1+ 
33. 0 
1+1+.8 
2 3 - 5 
15 
.lio 
12. 5 
1 9 . 1 
7 9 
.39 
30. 6 
1+6.1+ 
20.6 
16 
.37 
12.5 
20. 1 
8 0 
.33 
28. 0 
1+7-9 
18. 1 
17 
.31* 
12.5 
21. 1 
8 0 
.26 
22. 6 
1+8.9 
1 3 - 5 
18 
. 32 
12.6 
22. 1 
8 1 
.2o 
18.1+ 
1*9-9 
10.6 
19 
• 29 
12.6 
2 3 , 1 
8 2 
. 1 5 
15. 1 
50.9 
8.7 
k = 1 
k = 3 
0 
1.00 
0 
0 
0 
1.00 
0 
0 
0 
1 
.98 
7 - 5 
6.3 
63 0 
• 99 
10.9 
1 0 . 1 
116.0 
2 
.96 
11*.5 
12. 3 
62 6 
• 98 
2 1 . 5 
19.8 
1 1 5 . 0 
3 
• 93 
21. 1 
1 8 . 1 
62 0 
• 97 
31. 7 
2 9 . 3 
1 1 3 . 3 
lt 
• 91 
27. 1 
23.6 
61 0 
.96 
1+1. 1+ 
38.1* 
1 1 1 . 0 
5 
.88 
32.6 
28.8 
59 6 
.9U 
50. 5 
1+7.1 
108.0 
6 
.85 
37.1* 
33.7 
58 0 
• 93 
59- 0 
55.1* 
10I+. 1+ 
7 
.81 
1*1.6 
38.3 
56 0 
• 91 
66. 8 
63.3 
100.2 
8 
.78 
1*5., 1 
1*2.6 
53 7 
.88 
73. 6 
70.7 
95- 5 
9 
.71* 
1*7.8 
1*6.6 
51 1 
.86 
79. 5 
77-7 
90.1 
10 
• 7o 
1*9.6 
50.3 
1+8. 1 
.82 
81+. 2 
81*.2 
81+.2 
11 
.65 
50.6 
53.7 
1+1+ 9 
• 79 
87- 1+ 
90.1 
7 7 . 7 
12 
.60 
50.6 
56.8 
1*1 3 
89. 1 
95.5 
70.7 
• 55 
!*9. 5 
59-5 
37 5 
.69 
88. 9 
100. 2 
63.3 
1U 
• h9 
1+7.2 
6 1 . 9 
33 2 
.63 
86. 1+ 
10l*. 1* 
55-^ 
15 
. 1*3 
1+1*.0 
61*. 1 
28 9 
.56 
81. 1+ 
108.0 
1*7.1 
16 
.36 
38.8 
65.8 
23 8 
.1+8 
73. 1+ 
1 1 1 . 0 
38.1+ 
17 
• 29 
33.3 
' 
67.3 
19 3 
.39 
61. 9 
1 1 3 . 3 
29.3 
18 
.20 
21*. 1 
68.3 
12 9 
.28 
1+6. 3 
1 1 5 . 0 
19. 8 
19 
. 1 1 
1 7 - 7 
69.3 
9 .3 
. 1 5 
25. 9 
116. 0 
10. 1 
k = 1+ 
k = 6 

Ruin Problem and Roulette Machines 
6. A PROGRAM FOR THE POCKET CALCULATOR TI 59 
67 
(For a press button A, for k (1,3, 4 or 6) press A') 
LBL A CMs Prt Adv -1 = STO 00 STO 01 R/S 
LBL A' STO 05 Prt Adv + + 13 = STO 06 
12 i RCL 05 + 13 + STO 07 12 + STO 08 12 = STO 09 
12 STO 02 14 STO 03 1 STO 14 
LBL B 
((13 x RCL 14 - RCL 05 * RCL Ind 07) ^ RCL 06) 
INV If fig 1 B' Prt 
LBL B' Exc Ind 03 Op 23 Dsz 2 B' 12 STO 02 
LBL C 
((13 x RCL 26 - RCL 0 5 x RCL Ind 08 - 13) i RCL 06) 
INV If fig 1 C' Prt 
LBL C' Exc Ind 03 Op 23 Dsz 2 C' 12 STO 02 
LBL D 
((13 x RCL 3 8 - RCL 05 x RCL Ind 09 - 13 x RCL 1 5) v RCL 06) 
INV If fig 1 D' 
(STO 13 v RCL 14) Prt 
((RCL 26 - RCL 13) i (1 - RCL 14)) Prt Adv RCL 13 
LBL D' Exc Ind 03 Op 23 Dsz 2 D ' 
12 STO 02 14 STO 03 Dsz 1 B 
INV If fig 1 E Adv INV St fig 1 R/S 
LBL E RCL 14 1/x STO 14 Prt x RCL 26 = + STO 26 Prt 
RCL 14 x 2 x RCL 38 = + STO 38 -5- RCL 14 = Prt 
(RCL 26 - RCL 38) * (1- RCL 14) = Prt Adv 
11 STO 01 15 STO 02 27 STO 03 39 STO 04 
LBL E' 0 STO Ind 02 STO Ind 03 STO Ind 04 Op 22 Op 23 Op 24 
Dsz 1 E' St fig 1 RCL 00 STO 01 
12 STO 02 14 STO 03 
GTO B 

68 
Ulrich Dieter 
Remarks; 
1. The first 50 registers contain the following expressions:given in the 
subsequent table. Registers4 and 13 are used temporarily; registers 10, 
11 and 12 are not used at all. 
0 
a-1 
1 
a-1 
2 
12 
3 
m 
It 
5 
k 
6 
13-k 
7 
1 T + 1 3 
k 
8 
f
-
^ 
9 
f
+
3
7 
10 
11 
12 
13 
1 4 
q(z) 
1 5 
q(z+D 
1 6 
q(z+2) 
1 7 
q(z+3 ) 
18 
/ , v 
q(z+4) 
1 9 
q(z+5) 
20 , 
q(z+6) 
2 1 
q(z+7) 
2 2 
q(z+8) 
2 3 
q(z+9) 
2 k 
q(z+10 ) 
2 5 
q(z+1l) 
2 6 
D(z) 
2 7 
D(z+1 ) 
2 8 
D(Z+2) 
2 9 
D(Z+3 ) 
3 0 
D(z+lt) 
3 1 
D(Z+5 ) 
3 2 
D(Z+6) 
3 3 
D(Z+7) 
3 U 
D(Z+8) 
3 5 
D(z+9) 
3 6 
D(z+10) 
3 7 
D(z+11 ) 
3 8 
DL(z) 
3 9 
DL(z+1 ) 
1,0 DL( z+2 ) 
4 1 
DL(Z+3) 
h 2 
DL(z+h) 
1+3 
DL( Z+5 ) 
k k 
DL(z+6) 
1+5 DL( Z+7 ) 
k 6 
DL(z+8) 
k l 
DL(z+9) 
1+8 
DL( z+10 ) 1+9 DL( z+1 1 ) 
2. Label A and A' load register 0 - 3 , 5 - 9 and 1U. Registers15 through 
1+9 are zero in the beginning. 
3. Label B calculates the new q(z) in each step. Label B' stores the new 
q(z) into register 11+ and shifts the old q(z) by one step to the right. 
1+. Label C and C' do the same for D(z), label D and D' for DL(z). 
5. Label E calculates q(a-l), D(a-1), DL(a-l) and stores them into the 
registers 1U, 26 and 38. Label E' clears registers 15 through 25, 
27 through 37, 39 through 1+9- Changing flag 1, one goes back to label 
B. Subsequently q(a-2), D(a-2), DL(a-2), q(a-3), ..., q(0), D(0),DL(0) 
are calculated. 
6. The printout contains: a; k; q(z,a), D(z,a), D(z,a|L), D(z,a]G) for 
z = a-1 , a-2 , . . . , 1 , 0. 

Ruin Problem and Roulette Machines 
69 
REFERENCES 
Dieter, U.: Roulette as a Ruin Game. Proceedings of the 5th 
Symposium on Operations Research, Köln, 1980, edited by 
R. Burkard. Methods of Operations Research, Vol. 41 (1981) 
p. 75-78. 
Dieter, U. and Ahrens, J.H.: The real roulette as a ruin game: 
bold strategies are not optimal. To appear. 
Dubins, L.E. and Savage, L.J.: Inequalities for Stochastic 
Processes. (How to gamble if you must.) Dover Publ., Inc. 
New York, 1965. 
Feller, W.: An Introduction to Probability Theory and Its 
Applications. J. Wiley & Sons, New York, London, 1957. 
Jacobs, K.: Selecta Mathematica I. Springer Verlag, Berlin 
196 9., p. 28-52. 
Sagan, H.: Markov Chains in Monte Carlo. Mathematics Magazine, 
Vol. 54 (1981), p. 3-10. 
Takäcs, L.: On the Classical Ruin Problems. Journ. American 
Statistical Association, Vol. 64 (1969), p. 859-906. 


Price Formation in the Chemical Industry 
of the Federal Republic of Germany - Estimation 
of an Econometric Model with Parameter Restrictions 
Across Equations 
Joachim Frohn 
1. INTRODUCTION 
During the last two or three decades there has been a great number of em-
pirical studies making use of econometric production and/or price models 
based on microeconomic theory. Many of these models are subject to parameter 
restrictions across equations, resulting from an objective function which 
is constrained by some other equations of the model. 
Usually the resulting parameter restrictions will affect important features 
of the underlying economic theory, for instance elasticities. Despite this 
fact the estimation procedures used in the empirical studies in many cases 
do not make proper use of this kind of prior information: Either only parts 
of the model are estimated; or parameters affected by such restrictions are 
preestimated in one equation and the resulting estimates are used for these 
parameters in all the other equations. 
In this paper a small model with parameter restrictions across equations 
is specified to 'explain' the process of price formation in the chemical 
industry of the Federal Republic of Germany. It is shown how the method 
of restricted-three-stage-least-squares can be applied for an adequate 
estimation of the parameters. The empirical study is based on quarterly 
data from 1963 to 1976. 
2. THE MODEL 
The model used in this paper is an extended version of wellknown production 
models: It consists of a production function, a demand equation for the pro-
duct, given factor prices, and profit maximization as the entrepreneurial 
objective. In the following a short description of these elements of the 
model is given. 

72 
Joachim Frohn 
2.1 The production function 
As the product price in the chemical industry is likely to be influenced 
by the prices of raw materials, it is necessary to include intermediate 
goods as an additional input into the production function. Since the sub-
stitutionality between this input and the two other inputs labour and 
capital presumably is rather limited Leontief-type production functions 
have been tried. Some preliminary investigations indicated that this 
assumption is acceptable if one allows for increasing returns to scale 
and changes in productivity on account of technical progress in labour 
and capital. Therefore the following equations are used to describe the 
input-output-relations: 
Vt = b M t UMt ' 
(2.1) 
K 
r 
V,_ = ce 
K 
u 
, 
( ? ? 1 
t 
t 
Kt 
k z•*' 
X t 
V
t = d e 
L t U L t 
; 
( 2 - 3 ) 
with: 
V: output, M: intermediate goods, K: capital, L: labour, t: time, 
A ,A : rates of change in productivity resulting from technical progress. 
K L 
In the structural model these equations are solved for the input-variables: 
"t • b
X 
UMt ' 
, 2'>'> 
-
X 
i 
* 
r 
r 
* 
K t " ° » 
"t UKC ' 
, 2 - 2 " 
A
t 
i 
* 
r 
r 
* 
Lfc = d e 
V t u L t ; 
(2.3a) 

Estimation of an Econometric Model 
73 
The nonnegative error terms u*fc , u*fc , 
represent productivity changes 
not explained by the respective equation. It is assumed that the relation-
ships between the input-variables and the output hold on the average, 
i.e.: 
E ( u . t ) = 1 
for all t. 
M , K , L 
(2.4) 
Furthermore the error terms are assumed to be independent over time with 
variance independent of t: 
Var(u*fc) = 0 ? 
for all t, 
i = M , K , L . 
(2.5) 
2.2 The demand equation for the product 
Corresponding to the log-linear structure of the input-output-relationships 
the demand equation is specified as: 
V t - ap^ 
n 
x'i u 
(2.6) 
1=1 
with p: price of the product, x : further explanatory variables (in the 
empirical study the import price of the product, and an indicator of the 
economic situation of those sectors buying chemical goods were used). 
The same assumptions as for the error terms in equations (2.1a) to (2.3a) 
are made for u^ . It seems plausible to assume independence of 
and the error terms in the input-output-relations, since demand and product 
decisions are usually made by different agents. 

74 
2.3 The conditions on the factor markets 
Joachim Frohn 
It is assumed that for the process of price formation the factor prices 
are given. This assumption assures that the factor prices 
(for inter-
mediate goods) , 
,(for capital) , and 
(for labour) are included as 
exogenous variables in the price equation as derived from the entrepre-
neurial objective function. 
2.4 The entrepreneurial objective 
As entrepreneurial objective, maximization of the expected profit per 
period is assumed (see [5]). This must be regarded as a first approxi-
mation. Alternative and/or complementary objectives (like, for instance, 
price continuity) shall be investigated in further studies. 
Maximization of the expected profit leads to the following objective 
function: 
E ( pt vt - V t - zt Kt - V t 1 = M a x 
(2-7) 
pt 
by use of (2.1a) to (2.3a) and (2.6): 
1 
E i a p ^ + 1 
H 
x ^ 
u 
- [ a p ^ 
II 
x ± £ 
u
v J
r 
. 
(2.8) 
1=1 
1=1 
• (q b il 
+ Z c e 
uV4_ + w.d e 
uT ^ ) } = Max . 
t 
Mt 
t. 
Kt 
t 
Lt 
p-
Taking first derivatives with respect to p , setting this equation equal 
to zero, and making use of the assumptions for the error terms yields: 
1 
i= 1 
- ^ t 
^ t 
* 
r 
* 
r 
(b q t + c e 
z t + d e 
w ) = o 
(2.9) 

Estimation of an Econometric Model 
75 
Solving this equation for 
leads to a rather complicated price equation, 
which subsequently will make estimation of the parameters impossible. 
To overcome this problem, the expected output V , which corresponds to the 
e 
e- 
t 
optimal price, is substituted for 
II x^i in (2.9); this leads to the 
following implicit price equation: 
1 
^ 
1 - 1 
- ^ t 
-
X 
1 
E 
„ , r , — r 
r * 
* 
r 
* 
r 
, 
P t 
= 7 I T T E ( u v t ' ' V t 
[b q t + c e 
z t + d e 
wfc] (2. lo) 
For estimation purposes a useful indicator for V 
has to be found. In the 
empirical investigation the observed values of the output V 
were used. 
As equation (2.1o) will hold only approximately, a multiplicative error 
term u 
, subject to the same assumptions as for the other error terms, 
Pt 
is added:' 
i 
A
t 
p t " 7 T T 7 E ( u v t > vt' 
r 
* t + a*e 
1 
» t ) - v 
12.11) 
3• THE DATA 
In this section only a very brief discussion of the data used in this study 
can be given. A detailed description of all time series is included in [3]. 
The output 
(v ) of the chemical industry is represented by gross product as 
published by Statistisches Bundesamt (StBA). The data for the capital input 
(K^) have been interpolated from yearly values calculated by Deutsches In-
stitut für Wirtschaftsforschung (DIW), Berlin. (Actually utilized capital 
should be used as an input in the production function. Due to the specific 
calculation of the only available DIW-index of utilization, which implies a 
log-linear dependence between utilized capital and output, this variable ob-
viously could not be used in the empirical study.) Labour-input 
(L ) is 

76 
Joachim Frohn 
measured in hours worked, calculated on the basis of official data by assum-
ing that workers and employees have the same amount of working hours per 
month. The time series for intermediate goods 
is taken from the in-
put-output-tables of the DIW, Berlin. As there are only input-output-tables 
for 1962, 1967, 1972, and 1976 the values inbetween have been interpolated 
in correspondence with the development of gross product. 
The product price 
(p^) is represented by the index of producer prices which 
is published by StBA. A price index for capital goods 
(z^_) does not exist. 
Therefore capital costs (consisting of depreciation and interest) have been 
calculated and then related to capital 
As price for labour 
(w^) wages 
plus entrepreneurial payments for social security are used. The price index 
of intermediate goods 
(q^) has been obtained as an average of the pro-
ducer prices weighted by the respective amount of the intermediate goods. 
As further variables the DlW-index of utilization of capital in the chemi-
cal industry, the index of import prices of chemical goods and the price 
deflator of net social product of the Federal Republic of Germany have been 
used; the latter two time series are published by StBA. 
4. PARAMETER ESTIMATION 
For estimation purposes the log-linear version of the model (2.1a), (2.2a), 
(2.3a), (2.6), (2.11) is used: 
* 
1 
* 
InM 
= lnb 
+ - InV 
+ In u„. , 
(4.1) 
t 
r 
t 
Mt 
InK 
= lnc* " 
t + 7 l n V
t
 
+ l n U K t ' 
(4.2) 
InL 
= 1 nd* 
- t + - lnV^ + ln u* 
, 
(4.3) 
t 
r 
r 
t 
Lt 
1nV 
= Ina + elnp 
+ 
I 
lnx. t + ln 
, 
(4.4) 
i= 1 

Estimation of an Econometric Model 
77 
lnp. = 1 n [ 
E 
E ( u r ) ] + ( - - 1 ) InV 
+ 
t 
r ( e + 1 ) 
Vt 
r 
t 
(4.5) 
+ In (b 
+ c e 
z^ + d e 
+ In u 
. 
As V 
is used for 
the model (4.1) to (4.5) is an interdependent mod-
el, which is characterized by prior restrictions involving parameters of 
several equations. This follows since the price equation (4.5) has been 
derived from an objective function that is constrained by the demand equa-
tion and the input-output-relations . To reveal the specific parameter 
restrictions of the model, equations (4.1) to (4.5) are rewritten as 
follows: 
InM 
= a. + a. In V 
+ In u ^ 
, 
(4.1a) 
t 
1 
A 
t 
Mt 
l n K t = 
+ fc>2t + b 3 l n V t + In u* 
, 
(4.2a) 
lnLfc = c 1 + c 21 + c 3 In Vfc + In u 
(4.3a) 
InV 
= d 
+ d In p 
+ 
Z 
e. l n x 
+ In u 
(4.4a) 
t 
^ 
t 
l 
it 
Vt 
1=1 
l n p t = e x + e 2 I n V 
+ 
(4.5a) 
e 6 t 
e 8 t 
+ e. In [e q 
+ e e 
z 
+ e^e 
w j 
+ In u ^ 
J 
4 t 
5 
t 
7 
t 
pt 
Comparing (4.1a) to (4.5a) with (4.1) to (4.5) shows that the model is sub-
ject to the following restrictions: 

78 
Joachim Frohn 
a i = l n e 4 ' 
a 2 = b 3 = ° 3' bl = 
l n S 5 ' 
b 2 = e 6 ' 
( 4 " 6 ) 
C 1 = l n e 7 ' ° 2 = e 8 ' e 2 = a 2 - 
e 3 = 1 " 
In case of a linear interdependent model with linear restrictions across 
equations the method of restricted-three-stage-least squares (R3SLS) (see 
for instance [4], p. 523 and [1], p. 245 ff) is an appropriate estimation 
procedure which takes into account all prior restrictions. Since model 
(4.1a) to (4.5a) as well as the parameter restrictions (4.6) are nonlinear, 
the R3SLS-estimates of the parameters were determined in an iterative pro-
cedure: At a first stage the production system (4.1a) to (4.3a) was esti-
mated by R3SLS, enforcing the restrictions a = b = c . From the resulting 
~ 
~ 
2 
3 
3 
estimates, b , c , (A /r), d , and (A /r) were calculated and used to de-
K 
L 
termine 'observations' of a new variable 
, 
Â 
À 
- 
,* 
- 
- ' T ^ 
* - (-7) t 
y 
= b q t + c e 
z t + d e 
w t 
' 
(4.7) 
At a second stage the whole system with y 
as one of the two explanatory 
variables in (4.5) was estimated by R3SLS, imposing the restrictions 
a 2 = b^ = c^ , e 2 = a^ - 1 and e^ = 1 . This leads to new estimates for 
b*, c*, (A /r), d* and 
(A /r) . According to the discrepancy between these 
K 
L 
estimates and those obtained from estimation at the first stage new values 
for these five parameters and hence for the 'observations' of 
were 
determined. Then again the whole system was estimated by R3SLS a.s.o. until 
convergence. In the empirical study reported in this paper the solution was 
found after only six iterations. 
Of course, using this procedure one cannot be sure to have reached the op-
timal solution according to the R3SLS-criterion since there may be various 
local minima. However, if the final set of estimates is quite close to the 
starting point it seems plausible to accept this final set as the R3SLS-
estimates. 

Estimation of an Econometric Model 
79 
5• THE RESULTS 
After having specified the basic model two problems remained: The inclusion 
of further explanatory variables in the demand equation (4.4),and the ques-
tion whether nominal prices or prices divided by a price deflator should be 
used. 
Some preliminary tests on the basis of OLS-estimation showed that in case 
of the second problem, 'deflated' price variables lead to significantly 
better results. Therefore all price variables were divided by the price de-
flator of net social product. (Nevertheless the same symbols have been used 
for the 'deflated' prices.) 
As far as the first'problem is concerned the index of import prices for 
chemical goods turned out to be an important additional explanatory variable. 
In addition to that an indicator for the economic situation of the main 
buyers of chemical goods had to be included in the demand equation. As no 
adequate quarterly sectoral data are available the DlW-index of utiliza-
tion, lagged by one period, was used. The results of the estimation show 
that this variable can only be regarded as a very rough indicator (see 
below). 
As described in the preceding section estimation of the parameters of the 
model was separated into two stages: At the first stage the input-output-
relations were estimated by R3SLS, the price equation was estimated by OLS, 
subject to the restrictions resulting from the R3SLS-estimation of the pro-
duction submodel, and the demand equation was estimated by OLS. The esti-
mation was performed by using the program system EPS (see [2]). 
R3SLS-estimation of the input-output-relations yields the following 
results: 
InM 
= 
0.8731 
+ 
0.8605 
InV 
t 
(±0. 1173) 
(±0.0123) 
(4.8) 

80 
Joachim Frohn 
lnK t = 
2.8253 
- 0.0019 t + 
0.8605 
InV 
(4.9) 
(±0.1104) 
(+0.0003) 
(±0.0123) 
lnL t = -2.1535 
- 0.0158 t + 
0.8605 
InV 
(4.10) 
(±0.1123) 
(±0.0003) 
(±0.0123) 
The values in brackets give the estimated standard deviations. The observed 
and the estimated values of the endogenous variables are depicted in figures 
1 to 3. Regarding the simple structure of the production model and the data 
problem in case of K the fit seems to be tolerable: Except for the last two 
or three years the general development of the inputs is approximated quite 
well. 
Before estimating (4.5)subject to all restrictions, OLS-estimates of the 
parameters of the following equation were determined: 
lnp t = e j + e
2
 
l n V
t
 
+ e 3 l n Y t 
(4.11) 
with 
t in* 
, 
arr 
-0.0019 t 
„ ,,, 
-0.0158 t 
Yt = 2.394 
+ 16.866 zfce 
+ 0.116 w^_e 
The results of this estimation could indicate how close 'free' estimates of 
e^ and e^ would come to those values to be expected on the basis of the 
specification of the model and the R3SLS-estimation of the input-output-
relations, i.e.: e^ = -0.1395 , 
= 1-0 . 
This 'free' OLS-estimation yields: 
lnp t = -0.6446 
- 
0.1212 
InV 
+ 
1.4361 
lnyfc 
(4.12) 
(±0.3734) 
(±0.0249) 
(±0.1133) 
2 
R 
= 0.9626 ; DW = 1.3404 
Despite the estimate of e^ this result is not too far away from the 
expected one . 
A restricted OLS-estimation, imposing the constraints e^ = -0.14 and 
e = 1.0, gives: 


82 
Joachim Frohn 
lnp 
= 0.0990 
- 0.14 
+ 1.0 lnyfc 
(4.13) 
(±0.0035) 
Observed and estimated values of l nP t according to (4.12) and (4.13) are 
given in figures 4 and 5. 
Finally (4.4) was estimated by OLS with 
index of import prices of 
chemical goods (Ip
t) / an<3 x2t = D™-index of utilization of capital in the 
chemical industry, lagged by one period, (y 
lnvt = 8.7803 
- 1.2502 lnpfc - 0.7602 
lnIP + 0.9433 
l nY t_ 1 
(±0.8168) 
(±0.1562) 
(±0.1109) 
(±0.2361) 
(4.14) 
R 2 = 0.9214 ; DW = 0.5472 
The value of the Durbin-Watson-Statistic as well as figure 6 show that 
this function can only be regarded as a first approximation. Probably this 
is due to the lack of a reliable indicator of the economic situation of 
the buyers of chemical goods. 
The iterative R3SLS-estimation of the whole system, subject to all restric-
tions (4.6), gives: 
InM = 
1.4341 
+ 0.8018 InV 
(4.15) 
(±0.0888) 
(±0.0093) 
lnK_ = 3.3552 
- 0.0009 t + 0.8018 InV 
(4.16) 
(±0.0835) 
(±0.0002) 
(±0.0093) 
lnLfc = -1.6063 
- 0.0154 t + 0.8018 InV 
(4.17) 
(±0.0839) 
(±0.0003) 
(±0.0093) 
lnVfc = 8.8354 
- 1.2528 lnp - 0.4847 lnIPt + 0.6563 lnY t - 1 
(±0.6459) 
('0.1335) 
(±0.0919) 
(±0.1887) 
,0. 
(4.18) 


84 
Joachim Frohn 
lnpfc = 0.0995 
- 0.1982 InV 
+ 1.0 lny 
(4.19) 
(±0.0888) 
(±0.0093) 
with 
A m 
na er 
-0.0009 t 
^ „^ 
-0.0154 t 
y^ = 4.20 
+ 28.65 z e 
+ 0.20 w e 
Because of the a priori determination of y 
the estimated standard devi-
ations are not the true R3SLS-estimates, but they can be regarded as good 
indicators of the stability of the respective parameter estimates. 
Figures 7 to 11 show the observed and the R3SLS-estimated values of the 
endogenous variables. 
Considering the simplicity of the model and the heavy burden of nine prior 
restrictions the approximation of the development of the endogenous vari-
ables by the model seems to be quite acceptable. Of course, there must 
be a very careful analysis of the given specification and especially inves-
tigations of alternative specifications with more details until sound con-
clusions about the appropriateness of the model can be drawn. 
If one assumes the model to be 'correct' the following results concerning 
the main economic parameters of production and price formation in the chemi-
cal industry of the Federal Republic of Germany emerge: The estimated in-
put-output-relations indicate that there are increasing returns to scale 
(r = 1.25); the estimates of the quarterly rates of productivity changes 
caused by technical progress are 0.0123 for labour and 0.0007 for capital. 
The demand elasticity of the product price is lower than -1.0 as one 
should expect from economic theory. The negative coefficient of the import 
price variable can possibly be explained by complementarity of domestic and 
imported chemical goods. 

Estimation of an Econometric Model 
85 

86 
Joachim Frohn 
6. CONCLUSION 
It has been the intention of this paper to emphasize the fact that in many 
empirical econometric studies on the basis of microeconomic price and pro-
duction theory one has to be aware of prior restrictions connecting para-
meters of various equations. Very often these restrictions concern impor-
tant economic characteristics of the model and therefore must be incorpo-
rated into the estimation. Otherwise a reliable judgement of the model on 
the basis of the estimates of the parameters is impossible. 
In the paper a small econometric model for price formation in the chemical 
industry of the Federal Republic of Germany is used to demonstrate how 
such constraints can be taken into account by R3SLS-estimation. 
Acknowledgement: Some earlier studies on this subject were part of a re-
search project in cooperation with the DIW, Berlin, which also prepared 
the data for the empirical investigation reported in this paper. This 
research project was financed by Deutsche Forschungsgemeinschaft. I am 
grateful to R. Krengel and R. Pischner from DIW for many valuable dis-
cussions. 

Estimation of an Econometric Model 
87 
References 
[1] Frohn J. (198p), Zur Schätzung von Preismodellen, in Frohn J. , Stäglin 
R. (eds.) Empirische Wirtschaftsforschung, Festschrift für Rolf 
Krengel, Duncker und Humblot, Berlin 
[2] Haas H. J., Tschakert H. (1980), Handbuch zum ökonometrischen Programm-
System EPS, Bielefeld 
[3] Pischner R. (1980), Datenbasis für die Pilotstudie 'Preiserklärungs-
modelle1, unpublished, DIW, Berlin 
[4] 
Theil H. (1971), Principles of Econometrics, Wiley and Sons, New York 
[5] 
Zellner A., Kmenta J., Dreze J. (1966), Specification and Estimation 
of Cobb-Douglas-Production-Function-Models, Econometrica, 34, 
784-795 


Vektorielle AR-Prozesse in der Makroökonomie 
Hermann Garbers 
1. Der Unterzeichnete war in den Jahren 1966-1970 Assistent von Herrn Wet-
zel am Institut für angewandte Statistik der Freien Universität Berlin. In 
dieser Zeit hat man in der empirischen Wirtschaftsforschung noch intensiv 
an der Konstruktion grosser makroökonometrischer Modelle gearbeitet. Unser 
Chef stand diesen Arbeiten mit grosser Zurückhaltung gegenüber. Er plädier-
te mehr für exploratorische als für derartige bestätigende Datenanalysen. 
Das sogenannte "Vorwissen" der Oekonomen wollte er häufig nicht akzeptie-
ren. In seinen Augen handelte es sich dabei oft um Mengen von ursprünglich 
mehr oder weniger willkürlich gewählten Nullhypothesen, die anschliessend 
von Tests mit kurzen Zeitreihen nicht verworfen werden konnten - ein Vor-
wissen also ... on no positive grounds whatever (Edwards, 1972, S. 179). 
Herr Wetzel beschäftigte sich und einige seiner Mitarbeiter mit der Theorie 
und Anwendung schwach stationärer Prozesse. Sie waren für ihn das zentrale 
statistische Instrument zur Analyse passend gefilteter ökonomischer Zeit-
reihen; er bevorzugte im übrigen eine Dikussion dieser Prozesse im Fre-
quenzbereich . 
Im Rückblick ist es beeindruckend, mit welcher Konsequenz er damals die von 
ihm bezogenen Positionen erfolgreich vertreten hat. Für seine Art von Wirt-
schaftsforschung - eine exploratorische Datenanalyse zur Hypothesenfindung 
- benötigte er den Einsatz einer leistungsfähigen Rechenanlage und relativ 
anspruchsvolle mathematische und statistische Vorlesungen. Er hat sich 
"seinen" Rechner erkämpft und mit grossem Einsatz für ein entsprechendes 
Vorlesungsangebot gesorgt. 

90 
Hermann Garbers 
2. In der Zwischenzeit sind die grossen makroökonometrischen Modelle etwas 
aus der Mode gekommen. Willkürliche, auf divergierende Vorwissen beruhende 
Nullrestriktionen haben zu einem Nebeneinander von extrem unterschiedlichen 
Modellen geführt, die von Daten annähernd gleich gut oder schlecht gestützt 
werden. Einige monetaristisch orientierte Oekonomen haben dann den Vor-
schlag gemacht, die Modellbildung mit einer Art von Transfermodell zu be-
ginnen statt mit einer Menge von Strukturgleichungen. Sie publizierten das 
sogenannte St. Louis Modell der U.S.-Wirtschaft (Andersen,1970) das nur 
wenige Variablen enthält. ... Underlying this small model methodology is 
the belief that sectoral detail is not requiied to produce an accurate as-
sessment of the aggregative impact of monetary and fiscal actions. (Ander-
sen, 1974, S.306). Das Modell enthält im übrigen eine exogene Geldmengen-
variable. 
3. Die behauptete Exogenität gewisser Geldmengen wurde einer der Ausgangs-
punkte einer heftigen Auseinandersetzung, in die sich schliesslich auch C. 
Sims mit einem berühmt gewordenen Aufsatz ( Sims, 1972) eingeschaltet hat. 
Sims greift zunächst einen in der Literatur bekannten, auf Wiener und 
Granger zurückgehenden Kausalitätsbegriff auf - Granger-Kausalität. Dieser 
Begriff ist zwar nicht äquivalent zur Exogenität, aber er betont eine prin-
zipielle Konsequenz der Exogenität. Sims bemerkt zusätzlich, dass in einem 
korrekt spezifizierten ökonometrischen Mehrgleichungsmodell die endogenen 
Variable nicht Granger-kausal zu den exogenen Variablen sind. Er beweist 
schliesslich die folgenden beiden Sätze (siehe auch Geweke, 1980, S.286-
287): 
Satz 1: Es sei (X(t),Y(t))teZ/ ei n bivariabler schwach stationärer und in-
deterministischer Prozess mit dem Erwartungswert 0 . In dem Fall ist 
Y(.) genau dann nicht Granger-kausal zu X(.) , wenn eine MA-Darstellung 
existiert von der Form: 
X(t) 
A U ( L ) 
0 
ei(tj 
_Y(tJ 
_A2l(L) A22(LJ 
_e2(t) 

Vektorielle AR-Prozesse in der Makroökonomie 
91 
£l(.) und 
£2(•) sind unkorrelierte Prozesse des "weissen Rauschens". 
Satz 2: Unter der Voraussetzung von Satz 1 ist Y(.) genau dann nicht 
Granger-kausal zu X(.), wenn in der linearen Projektion 
+oo 
Y (t) = £ 
DTX(t—c) + U(t) 
T——® 
für alle % < 0 gilt: D T = 0 . 
Sims hat mit einer auf Satz 2 basierenden "Teststatistik" die Hypothese von 
der Exogenität gewisser Geldmengen (bezüglich des Bruttosozialproduktes der 
U.S.A.) nicht verwerfen und so eine der Grundannahmen der Monetaristen (und 
des St. Louis Modells) empirisch stützen können. Wenn dieses spezielle Er-
gebnis heute auch als nicht mehr sehr relevant angesehen werden muss, so 
hat Sims generell mit der empirischen Ueberprüfung verbreiteter Oberhypo-
thesen wohl eine wichtige Entwicklung eingeleitet. 
4. Zum Stellenwert seines früheren, gerade zitierten speziellen Ergebnisses 
bemerkt Sims im übrigen selber (Sims, 1980, S.251): Modern rational expect-
ations monetarism has shifted attention away from structural^' interpret-
ation of distributed-lag regressions of GNP on money stock ... (and) sugg-
ests that it is surprises in movements in the money stock which generate 
nonneutralitg ... "surprise" ... (can be) taken to mean "innovation" in the 
technical time-series sense of the "prediction error in a best linear pre-
dictor". 
Was aber heisst hier "strukturell"? Die folgende Sequenz von Begriffen will 
darauf eine Antwort geben; der Leser findet eine ausführlichere Diskussion 
bei Geweke (1980). 
Die Definitionen gehen von einem Ereignisraum S aus, der durch ein öko-
nomisches Modell restringiert wird. S wird durch Projektionen P x und 
P y 
auf zwei Räume X und Y abgebildet. Die beiden Räume symbolisieren 

92 
Hermann Garbers 
zwei Typen von Restriktionen: die durch das Modell direkt angegebenen Ver-
knüpfungen zwischen Eingangs- und Ausgangsgrössen 
(B) und die sich auf 
Eingangsgrössen des Modells beziehenden Beschränkungen 
(A). 
Definition 1: Das geordnete Paar von Restriktionen 
(A,B) auf S be-
stimmt genau dann eine kausale Ordnung von X auf Y , wenn 
Py(A) = Y und PX(A„B) = PX(A) . 
Definition 2: B s S akzeptiert X als Eingangsgrössen, wenn das ge-
ordnete Paar 
(A,B) für jedes nur X beschränkende A e S eine kausale 
Ordnung von X auf Y bestimmt. 
Definition 
': B s S ist strukturell bezüglich Eingangsgrössen X , wenn 
B X als Eingangsgrössen akzeptiert und für jede implementierte Menge 
C S X 
gilt, dass 
Py(Px"1(C) r> B) 
wahr ist. 
Definition 4: B £ S ist realisierbar mit den Zeitreihen X als Eingangs-
grössen, wenn B X als Eingangsgrössen akzeptiert und wenn 
P X t( A l) = PX^(Ä2) für alle nur X beschränkende A^ £ S , 
= S und 
alle t > r impliziert 
Py^(A]nB) = Pyr(A2^B) . 
In Definition 3 ist C S X 
beliebig, so dass für ein bestimmtes Modell B 
niemals bewiesen werden kann, ob es strukturell bezüglich X ist. Geweke 
(1980) formuliert deshalb ein Kausalitätsaxiom: B s S ist nur dann 3) 
strukturell bezüglich der Eingangsgrössen X , wenn B realisierbar ist 
mit den Zeitreihen X als Eingangsgrössen. 

Vektorielle AR-Prozesse in der Makroökonomie 
93 
5. Die weiter vorn erwähnte Philosophie kleiner Modelle, der Verzicht auf 
das a priori Postulieren von exogenen Variablen und die Betonung der Rolle 
der Prognosefehler rationaler Wirtschaftssubjekte (Innovationen) führen 
alle auf eine Modellierung (einiger weniger) zentraler ökonomischer Va-
riabler durch vektorielle AR-Prozesse der Form 
C(L)Y(t) = e(t) 
(1) 
mit 
•* 
Kov(£(t)) = Z 
und £ positiv definit. 
Die vorstehende autoregressive Form (1) ist für die interessante Untersu-
chung der Auswirkungen isolierter Innovationen in den einzelnen Variablen 
freilich nicht geeignet, weil sich derartige Innovationen hier bei £ # I 
in der gleichen Periode auf andere Variable ausbreiten können. Ueber eine 
Cholesky Zerlegung von £ kann man jedoch für Y(t) aus (1) eine passen-
dere Darstellung finden: 
a> 
Y(t) = G+Y(t) + E C*(s) Y(t-s) + V(t) 
(2) 
S=1 
G + ist eine Dreiecksmatrix mit lauter Nullen auf der Hauptdiagonalen. Die 
Komponenten von V(t) lassen sich dann als unkorrelierte Prognosefehler 
interpretieren, wenn spezielle Informationsmengen unterstellt werden. So 
wird z.B. zur Vorhersage der i-ten Komponente von 
Y(t) 
auf 
{Y(s) mit s < t} u {Yj(t) , j < i} 
zurückgegriffen. Die Transformation führt also dazu, dass die Anordnung der 
Komponenten in Y(.) plötzlich eine Rolle spielt. Man wird deshalb meist 
mit verschiedenen Anordnungen arbeiten müssen und dabei auf die Sensitivi-
tät der Resultate achten. 

94 
Hermann Garbers 
6. Die Auswirkungen von isolierten Innovationen untersucht Sims (1980) an-
hand der MA-Darstellung von Y(t) . Er unterstellt also die Invertierbar-
keit von C(L) und erhält4': 
CD 
-> 
00 
CO 
Y(t) = E B(s)e(t-s) = E (B(s)G-1)(Ge(t)) = £ (B(s)G_1)V(t) 
S=1 
S=1 
S=1 
Ist b^j*(s) das Element aus der i-ten Zeile und j-ten Spalte von 
(B(s)G"1) , 
so kann die Folge 
(b^j (s)) interpretiert werden als eine 
über die Zeit verteilte Antwort des Systems in seiner i-ten Variablen auf 
eine Innovation Vj(0) = 1 
Ist der Prognosehorizont gleich h + 1 , so erhält man für den optimalen 
Prognosefehler die Darstellung 
h 
E(t,h+1) : = £ (B(s)G"1)V(t) , 
s=0 
so dass 
^ 
h 
Kov(e(t,h+1)) = Z (B(s)G-1)(Kov((V(t)))(B(s)G-1)' 
• 
(3) 
s=0 
Interessanterweise leisten die Innovationen in M1 entsprechend der zitier-
ten Studie von Sims (1980) einen auffällig niedrigen Beitrag zur Varianz 
des Prognosefehlers in M1, während ihr Beitrag zum Prognosefehler der 
"Industriellen Produktion" verschwindend klein ist. Entsprechend dem unter 
Punkt 3 dieses Papiers erwähnten Satz 1 und im Gegensatz zu monetaristi-
schen Vorstellungen, scheint die Geldmenge demnach weder exogen zu sein 
noch erscheint sie für die Entwicklung des realen Bereichs einer Wirtschaft 
bedeutsam. Sims hat diese zitierten Resultate für die U.S.A. und die Nach-
kriegszeit erhalten. Sein Y(.) umfasste 4 Variablen: einen kurzfristigen 
Zinssatz, M1 
, einen Grosshandelspreisindex und die industrielle Produk-
tion, monatliche Werte von 1948-78, logarithmiert. Die angegebenen Resul-
tate scheinen invariant zu sein bezüglich verschiedener Anordnungen der Va-
riablen. Erste Anwendungen der Sims'schen Modellbildung auf schweizerische 
Daten, die u.a. auch den von ihm gewählten amerikanischen Zeitreihen zu 
entsprechen suchten, haben dagegen empfindlich auf Aenderungen in der An-
ordnung der Variablen reagiert (Böckli 1981). Man kann sich beim Anblick 

Vektorielle AR-Prozesse in der Makroökonomie 
95 
dieser Ergebnisse sogar des Eindrucks nicht erwehren, dass man es hier mit 
blossen statistischen Artefakten zu tun hat. 
7. Die vorangehende Feststellung wird denjenigen vielleicht nicht überra-
schen, der in der Relation (2) nichts anderes sieht als die reduzierte Form 
eines ökonometrischen Modells (ohne explizit ausgezeichnete exogene Varia-
ble). Die Störglieder der reduzierten Form sind aber bekanntlich Linear-
kombinationen von Störgliedern aus den verschiedenen Gleichungen der struk-
turellen Form, so dass eine Zuordnung der Innovationen zu einzelnen Varia-
blen nicht gerade sinnvoll zu sein scheint. 
Sims weist in diesem Zusammenhang jedoch auf eine häufig anzutreffende 
Block-Diagonalität von T aus der strukturellen Form 
TY(t) + BX(t) = e(t) 
hin - X(.) 
sei der Vektor der prädeterminierten Variablen. Unter diesen 
Umständen tangieren die Innovationen in den einzelnen Blöcken unmittelbar 
nur die relativ wenigen Variablen dieses Blocks, und in der reduzierten 
Form, sind die Störprozesse nur Linearkombinationen der Residuen aus den 
entsprechenden Gleichungen des Blocks. Die Innovationen können dann als In-
novationen dieser Blöcke (Sektoren) interpretiert werden. 
In Anwendungsfällen besteht freilich häufig kein sicheres Vorwissen über 
die Existenz derartiger Sektoren und die Zugehörigkeit gewisser Variabler 
zu ihnen. Ist z.B. die nicht-antizipierte Veränderung (Innovation) in den 
Löhnen mit den gleichzeitigen, nicht antizipierten Veränderungen in der 
Geldbasis stark korreliert, so sollte man diese beiden Variablen nicht ver-
schiedenen Sektoren zuordnen: In a model with many variables ... it may not 
be clear how to decide what is a "large" set of correlations ... If our 
connection of variables to sectors is correct and if correlations of shocks 
across sectors are small, conclusions based on ... (this models, H.G.) ... 
should not change much when the oräering of the sectors is varied before 
the triangular orthogonalization. (Sims, 1978, S.10). Die weiter vorn er-

96 
Hermann Garbers 
wähnten Resultate mit den schweizerischen Daten stützen demnach eine Hypo-
these, dass in dieser kleinen offenen Volkswirtschaft (mit grossen Banken) 
nur sehr wenige derartiger Sektoren existieren - vielleicht nur einer. Wie 
plausibel dieser Schluss aber auch immer klingen mag - er kann sich als 
vorschnell erweisen. Vielleicht hat man es hier am Ende doch nur mit einer 
Sammlung von statistischen Artefakten zu tun, die zu einer weitergehenden 
Modellbildung nichts beitragen kann. Aber was ist der Grund dafür, wenn es 
überhaupt so sein sollte? 
8. Man mag die in diesem Papier aufgeworfene Frage nach den in einer Volks-
wirtschaft existierenden Sektoren als für die Praxis der empirischen Wirt-
schaftsforschung nicht besonders wichtig empfinden. Die konventionelle 
Oekonometrie argumentiert schliesslich seit Jahrzehnten mit z.B. einen 
Konsum-, einen Investitions- und einen Aussenhandelssektor und beschreibt 
sie jeweils durch gewisse Verhaltensgleichungen. Es sind aber längst nicht 
mehr nur monetaristische Oekonomen, die bezweifeln, dass diese Art von Sek-
toren (bezüglich einer umfangreichen Klasse von Variationen) "Strukturen" 
einer Volkswirtschaft widerspieglen. Unter den Umständen könnte aber der 
hier vorgeschlagene frühe Einsatz statistischer Modelle in den makroöko-
nomischen Modellbildungsprozess zweckmässig sein. Ein ausschliesslich spä-
ter Einsatz exploratorischer Ansätze, etwa die Anwendung univariabler oder 
bivariabler ARMA-Prozesse auf die Residuen konventioneller ökonometrischer 
Modelle, impliziert die Annahme von empirisch schlecht gestützten Oberhy-
pothesen. Will man diesen Preis für eine geringe Reduktion der Anzahl Vor-
tests bezahlen? 
Fußnoten 
1) Unterstreichung durch H.G. 
2) Geweke verwendet in dieser Begriffsbestimmung "implementieren" und 
"wahr" als Grundbegriffe. 
3) Es heisst also nicht: "dann und nur dann." 
4) G ist die Matrix aus der Cholesky-Zerlegung von I 

Vektorielle AR-Prozesse in der Makroökonomie 
97 
Literatur 
Andersen, L. C. und Carlson, K. M. (1970): A Monetarist Model for Economic 
Stabilisation. Federal Reserve Bank of St. Louis Review, 7-25. 
Andersen, L. C. und Carlson, K. M. (1974): St. Louis Modell Revisted. In-
ternational Economic Review 15, Nr. 2, 305-327. 
Böckli, A. (1981): unveröffentlichtes Manuskript, Zürich. 
Edwards, A. W. F. (1972): Likelihood. Cambridge University Press. 
Geweke, J. (1980): Causality, Exogeneity and Inference. Vortrag am 4. Welt-
kongress der Econometric Society, Aix-en-Provence. 
Sargent, T. (1979): Macroeconomic Theory. New York, San Francisco, London: 
Academic Press. 
Sims, C. A. (1972): Money, Income and Causality. American Economic Review 
62, 540-552. 
Sims, C. A. (1978): Small econometric Models of the U.S. and West Germany 
without prior restrictions. Discussion Paper Nr. 78-105, Center for 
Economic Research, University of Minnesota. 
Sims, C. A. (1980): Comparison of Interwar and Postwar Business Cycles: Mo-
netarism Reconsidered. American Economic Review, Papers and Proceedings 
70, Nr. 2, 250-257. 


The Robustness of Some Distributed Lag Estimators 
Gerd Hansen 
1. INTRODUCTION 
Many econometric studies make use of rational distributed lags (RDL) and 
polynomial distributed lags (PDL). But criteria for choosing one of these 
models are not clear at all. One main difference of both models is the in-
finite and finite lag length respectively. Assuming lag distributions to 
be finite, the finite rational distributed lag (FRDL) provides a connec-
tion between the RDL- and PDL-model. Given the FRDL-model, the RDL- and 
PDL-model can be seen as misspecified. This paper analyses within some 
simulation experiments the bias and root mean square error as measures of 
robustness of different RDL- and PDL-estimators. Computational analysis 
of the robustness of estimators is a research topic of Prof. W. Wetzel, 
too, in whose honour this volume has been published. 
The finite distributed lag may be written as in equation (1) or (2), where 
x^ is assumed to be exogenous and 
to be a white noise error term. 
y t = * 
6i Xt-i + 6t 
1=0 
( 1 ) 
y = X 6 + e 
E(e) = O; E(ee') = azl 
( 2 ) 
X = 
xs +l 
X s 
•xi 
Bo 
Xs+2 
X s+1 
» 
•X2 
• 
P. — 
• 61 
• 
» 
• 
• 
I P — 
« 
• 
• 
• 
• 
X n 
X 
. . . 
n-1 
. X n-s 
Ss 

loo 
Gerd Hansen 
With respect to this specification the RDL- and PDL-model use different 
restrictions and probably different misspecifications. 
THE RDL-APPROXIMATION 
The RDL-specification (3) uses infinite lags and nonlinear restrictions 
(4) . 
A (L) 
y = P, , x + u 
(3) 
B (L) 
t 
t 
q 
A (L) = a +a,L+a,L2+. . .+a L P 
p 
o
i
l 
p 
B (L) = 1-b.L-b„L^-...-b L q 
q 
1
2 
q 
5i = ¥ i - l + ¥ i Y ' - + V i - q
+ a i 
; 
1=0,1,2... 
(4) 
with ß ,=ß =..,=ß =0 
-1 -2 
-q 
and a.=0 
if i>p 
i 
Misspecifications may result, if the parameter restriction (.4) is false 
and/or the lag length is finite. Choosing the appropriate degrees of the 
denominator and nominator polynomials, any lag distribution can be approxi-
mated by the RDL-model to a certain degree of accuracy. This is one reason 
for the popularity of the RDL-model in applied econometric work. If the 
variable x 
has an ARMA-representation (5) the RDL-model 
A (L) x t = B(L) e 
(5) 
can be understood as reduced form of a simple structural econometric model 
using optimal expectations for the exogenous variables 
(Grether, 1977) 
or rational expectations for some endogenous variables y 
(Wallis, 1980). 
If restriction (4) is valid for finite i (i=l,2...s) only, the RDL-model 
is called finite rational distributed lag model (FRDL). In other words, in 

Robustness of Some Distributed Lag Estimators 
101 
addition to (4) this model uses some zero-restriction (.6) (Pagan, 1978) . 
B. = 0 
if i>s 
(6) 
i 
3. 
THE PPL-APPROXIMATION 
The PDL-model employs the assumption that the parameters g 
(i=0,1,2...s) 
can be approximated by a polynomial 
.2 
.k 
g. = a +a,i+aa 
; 
i=0,l...s 
1 
o 1 
z 
Js. 
k>s 
This assumption can be written as a set of linear restrictions (7): 
6 = C a 
with C = (ij); 1 = ° ' l - - - s 
(7) 
j=0,l...k 
If the lag length s is identical with the polynomial degree k , the 
PDL-model coincides with the unrestricted lag model (2). As a polynomial 
of degree k satisfies restriction (8), 
k+1 
(1-L) 
B.=0 
i=k+l,k+2,.. .s 
(8) 
l 
the PDL-model can be seen as a special case of the FRDL-model (Pagan, 
1978) : 
k 
Afc(L) 
a +a]L+...+aKL 
Y t " v
T
^ 
X t + U t = 
d - D - 1 
X t + U t 
Thus the PDL-model is a FRDL-model with a nominator polynomial of degree 
k and a denominator polynomial of degree k+1 with unit characteristic 
roots. A denominator polynomial which does not satisfy this characteri-
stic root restriction can be written as 
B (L) = Cl-X.L) • ( 1-À L) ... (1-X L) 
q 
1 
2 
q 

102 
Gerd Hansen 
If a FRDL-model with given s, p and q is assumed to be correct, the 
PDL-restrictions may misspecify the lag length., the polynomial degree and/ 
or the characteristic roots. Within the PDL-model with correctly specified 
lag length s and polynomial degree k=p there are only misspecifica-
tions of the characteristic roots A^ . For example, a PDL-model with 
s=9 and k=2 misspecifies the corresponding FRDL-model with s=9 and 
p=2 , because some of characteristic roots 
an<^ 
z e r o o r 
at least different from unit. 
4. 
MODIFIED PDL-MODEL (MPDL) 
With respect to the problem of specifying the lag length s within the 
PDL-model, a combination of the RDL- and PDL-model has been proposed by 
assuming B(L) to be a first order polynomial and A(L) to satisfy the 
PDL-restriction (7) (Schmidt, 1974). 
5. 
ESTIMATION METHODS 
5.1. Estimation of the PDL-model 
Due to the misspecifications there seems to be no advantage in using the 
PDL-model, because the assumed restrictions are much stronger than in the 
RDL-model. But if the PDL-restrictions are correct, the OLS-estimator of 
the model (given k and s ) is BLUE. OLS can be performed after substi-
tuting (7) into (2) : 
The PDL-estimator 8 can be obtained from the OLS-estimator a by means of 
equation (10) 
y = 
= X ß + £ 
XCa + e = Za + e 
(9) 
ß = Ca = C(C'X'XC) 1 C'X'y 
= BX'y 
with B = C(C'X'XC) 1C" 
(IO) 

Robustness of Some Distributed Lag Estimators 
103 
Subsequently this estimator is denoted as P (s, k_)-estimator. If restriction 
(7) does not hold, estimator (10) will be biased. The bias results from 
equation (11) and (12). 
6 = BX' (Xp+e) 
(11) 
E(g)-B=(BX'X-I) 6 
(12) 
In addition, statistical inference using the variance-covariance-matrix 
(13) of the PDL-estimator may lead to too optimistic results, 
E (6-E (B)) (B-E(B))' = a 2B 
(13) 
because the bias is neglected. The mean square error-matrix (MSE) is 
easily calculated to be 
E(B-8) (8-B)' = (BX'X-I) B6' (BX'X-I) + O 2B 
(14) 
It is one issue of the present paper to analyse how the bias and MSE de-
pend on the parameter vector 
8 and therefore on the characteristic roots 
of the more general FRDL-model. 
5.2. Estimation of the RDL-model 
Estimation of the RDL-model does not cause problems, if the Koyck-reduced 
equation (15) has a white noise error process B(L) 
. 
B(L)yt = A(L)xt+B(L)ut 
(15) 
In matrix-notation (15) can be written as follows: 
y = Zy + v 
with y' = (b1,b2...b ,aQ,a1,...a ) 

104 
Gerd Hansen 
and Z = - [v»J 
y 
....y, 
x 
. x 
...X, 
q-1 
"'l 
p+1 p 
1 
'l+l 'q 
p+2 p+1 
y 
, y „...y 
x 
x 
. .. .x 
n-1 
n-2 
n-q 
n 
n-1 
n-p 
Maximum-Likelihood-estimation (ML) then results in OLS-estimation of (15), 
if the error term is normal distributed and initial values of the lagged 
dependent variables are fix. Subsequently this estimator is denoted 
R(p,q)-estimator. It is wellknown that the ML-estimator is biased but -
under appropriate conditions - consistent as well as asymptotically effi-
cient and asymptotically normal distributed. 
A priori there is little reason to prefer the assumption of white noise 
errors in equation (15) instead of those in equation (3). In the latter 
case there are several methods for estimating equation (3). For the sake 
of simplicity we refer to ML-estimation only. Assuming u^ in (3) to be 
normal distributed, the ML-estimator has been shown to be identical with 
the iterative instrumental variable estimator (16) (Dhrymes, 1971, p. 249). 
'k+l 
[ Z' <ik)zj 1 Z' (Çk)y 
(16) 
with Z(y, ) 
k [Y -U (y, ) ,X 1 
q q 'k 
pj 
where U (y, ) is the matrix of lagged residuals calculated from 
q k 
Y k 
cor-
responding to the matrix Y 
of lagged dependent variables. Iterations 
q 
are performed as long as the difference of successive estimates does not 
exceed some preassigned value e. Subsequently this estimator is denoted 
RI(p,q)-estimator. This estimator has been proved to be consistent and 
asymptotically normal distributed (Dhrymes, 1971). 
5.3. Estimating the FRDL-model 
The FRDL-model can be regarded as an ARMAX-model with nonlinear restric-
tions between the parameters. For example, the finite geometric lag (17) 

Robustness of Some Distributed Lag Estimators 
105 
may be written in ARMAX form CI8) (Pagan, 1978) . 
s 
yt = ao 
bl Xt-i + Ut 
( 1 7 ) 
1=0 
S+1 
<5 + 1 
(l-b1L)yt = aQ(l-b1 L 
)xt + (l-b1L)ut 
(18) 
It is easily seen that (18) will yields 
8- = 0 
for i>s. 
i 
The FRDL-model may therefore be estimated with existing ARMAX programs. 
5.4. Estimation of the MPDL-model 
The MPDL-model can be estimated by combining the PDL-estimator with one 
of the two RDL-estimators. Subsequently we will combine the P(s,k)- and 
the R(p,q)-estimator only. This is called MP(s,k)-estimator. 
6. 
EVALUATION OF ESTIMATORS BY MEANS OF SIMULATION 
It is rather difficult to compare the bias and root mean square error 
(RMSE) of the different estimators analytically. This is partly due to 
the different error processes implied by the estimators mentioned above. 
The P(s,k)- and RI(p,q)-estimator assume the errors of equation (15) to 
be a moving average process, whereas the R(p,q)-estimator assumes "white 
noise" errors. Specification errors may also result from misspecified po-
lynomial degrees and/or characteristic roots. Within the RDL-model poly-
nomial degrees may be specified by using the asymptotic Likelihood-Ratio-
Test (or the corresponding classical F-Test). The F-Test can also be used 
to specify the polynomial degree within the PDL-model, if the lag length 
s is given (Godfrey and Poskitt, 1975). Last not least, for a given 
sample size the P(s,k)-estimator has less degrees of freedom than the 
R(P(<l)- and RI(p,q)-estimator. 

106 
Gerd Hansen 
To evaluate the robustness of the estimators b y m e a n s of b i a s and RMSE, 
we therefore p e r f o r m some simulation experiments. These e x p e r i m e n t s are 
based on the F R D L - m o d e l (19) w i t h 
ACL) = .1+.2L+. 3 L 2 
B(L) = (1-AjL) (1-X 2L) ( 1 - A 3 D 
(19) 
and 
= 0 
for i>s 
The p a r a m e t e r s 
b ^ 
can be obtained from 
b l 
= 
X i + X
2
+ A 3 ; 
b 2 = - ( x
1
x
2
+ X l X 3 + > l 2 A 3 ) r b 3 = 
X 1 X 2 X 3 
In addition we use the q u a r t e r l y time series of W e s t - G e r m a n new orders of 
the investment goods industry, 1961 - 1977. 
6.1. R o b u s t n e s s of the P ( s , k ) - e s t i m a t o r 
In order to evaluate the sensitivity (or robustness) of the P ( s , k ) - e s t i m a -
tor w i t h respect to v i o l a t i o n s of the characteristic root restriction, 
different sets of characteristic roots 
are specified in the 
"true" FRDL-model. In order to allow c o m p a r i s o n s of the b i a s / R M S E - r e l a t i o n 
for different p a r a m e t e r sets, the error variance 
a 2 
of the "true" F R D L -
model must be adjusted. In our experiments we control 
a 2 
b y assuming the 
error v a r i a n c e to b e 1% of the v a r i a n c e of the d e p e n d e n t v a r i a b l e 
y . 
Given different characteristic roots and error v a r i a n c e s the relative ab-
solute bias and b i a s / R M S E - r e l a t i o n can be calculated by using equations 
(12) and (14). The results are given in table 1. 
The following c o n c l u s i o n s can be drawn from these results: 
1) The relative b i a s as well as the b i a s / R M S E - r e l a t i o n depends - as expec-
ted - on how m u c h the characteristic roots differ from unity. 

Robustness of Some Distributed Lag Estimators 
107 
Xl = "4r X2=°r X3=° 
X2=.5;X3=0 
k=3 
k=2 
k=3 
k=2 
rel. 
bias/ 
rel. 
bias/ 
rel. 
bias/ 
rel. 
bias/ 
bias 
RMSE 
bias 
RMSE 
bias 
RMSE 
bias 
RMSE 
B 
.47 
.65 
.83 
.08 
.54 
.21 
. 10 
.04 
B1 
B2 
.02 
. 16 
.32 
.92 
.33 
.57 
.02 
.05 
B1 
B2 
. 35 
.97 
.63 
1.00 
.04 
. 24 
.24 
.93 
63 
.29 
.93 
.20 
.96 
.02 
.24 
.15 
.95 
64 
.93 
.96 
.75 
.96 
.01 
.12 
.01 
.09 
e5 
.47 
.44 
2. 
.80 
.96 
.02 
.14 
. 14 
.68 
e6 
3.24 
.71 
7. 
.00 
.97 
.00 
.00 
.24 
.76 
7.95 
.70 
15. 
.70 
.93 
.04 
. 13 
.22 
.64 
*B 
B 9 
2.88 
.84 
33. 
.48 
.88 
.03 
.09 
.01 
.02 
*B 
B 9 
363.9 
.97 
68. 
.64 
.77 
.20 
.23 
.61 
.74 
Al = -9; A2=-7,'A3=-5 
X^-95; A2=-9; A3=-9 
k=3 
k=2 
k=3 
k=2 
rel. 
bias/ 
rel. 
bias/ 
rel. 
bias/ 
rel. 
bias/ 
bias 
RMSE 
bias 
RMSE 
bias 
RMSE 
bias 
RMSE 
B 
1.29 
.17 
.32 
.27 
6 
.05 
.16 
$ 
.01 
. 1 1 
B4 
.02 
.22 
B 
.00 
.03 
.01 
.07 
.01 
.07 
B n 
.02 
.15 
1.57 
.21 
.35 
.48 
.42 
.05 
. 14 
.60 
.15 
.02 
.27 
.00 
.02 
.18 
.00 
.02 
. 19 
.oo 
.02 
.13 
.00 
.02 
.11 
.00 
.03 
.17 
.oo 
.07 
. 30 
.00 
01 
.36 
.02 
02 
.06 
.02 
02 
.00 
.02 
00 
.01 
.01 
02 
.00 
.03 
01 
.00 
.01 
OO 
.00 
.01 
01 
.00 
.01 
01 
.00 
.02 
07 
.01 
.07 
Table 1: Relative absolute bias and bias/RMSE-relation of th.e P(s=9,k)-
estimator for different characteristic roots A. and polynomial 
degrees k. 

108 
Gerd Hansen 
2) Using two zero characteristic roots, especially the right tail of the 
estimated lag distribution is seriously biased. Little can be gained 
by varying the polynomial degree in this case, but the bias may be re-
duced by reducing the lag length. 
3) The relative bias becomes small even if one of the three characteristic 
roots is still zero. But there is a bias especially at the left hand 
tail of the distribution, which may be due to neglected near endpoint 
constraints of the P(s,k)-estimator. 
4) Reducing the polynomial degree results in an increasing absolute bias 
and in sequences of positive and negative biases. Furthermore the 
P(s,k)-estimator tends to cut the peak of the lag distribution, if 
there is a high peak. Since there will be high peaks only, if one (or 
all) characteristic roots are small, the PDL-model implicitly prefers 
a flat lag distribution. 
In summary the P(s,k)-estimator seems to be robust against misspecifi-
cations within a wide range of the characteristic roots. Since high 
biases may appear at the tails of the lag distribution, testing the 
characteristic root restriction seems to be more important than 
testing for the polynomial degree. 
6.2. Comparison of robustness of different estimators 
So far only the P(s,k)-estimator has been analysed. In order to compare 
this estimator with the R(p,q)-, RI(p,q)- and MP(s,k)-estimator mentioned 
above, we restrict ourselves to the most unfavorite cases of two zero 
characteristic roots (model A: X =.4; X.2=A3=0) and a small positive and 
negative characteristic root (Modell B: 
. These cha-
racteristic roots are very much in favour of the R(p,q)- and RI(p,q)-
estimator and therefore are strong tests of the relative robustness of 
the P(s,k)-estimator. With respect to the error process we assume moving 
average errors 

Robustness of Some Distributed Lag Estimators 
109 
as well as "white noise" errors 
Simulations are performed by drawing 50 repeated samples of size T for 
the normally distributed error e , calculating the corresponding endoge-
nous variables y and computing the different estimators. The average 
estimate (or bias) and RMSE of these estimators are calculated from the 
50 repetitions. In addition the trace of the mean square error matrix of 
the estimated dependent variable y is used as a measure of the overall 
performance of the estimators. This measure is a weighted average of the 
estimated lag parameters, denoted as conditional mean forecasting risk FR 
(Judge and Bock, 1978, p. 33). 
FR = tr E(XB-Xg) (Xg-Xg) ' 
= Ete-B) 'X'X(B-B) 
Furthermore Durbins's h-test (Durbin, 1970) or the Durbin-Watson-test is 
used to account the number of rejections of the hypothesis of serially 
uncorrelated errors within the 50 repetitions. 
The results of these simulations are given in table 2 and 3. The main con-
clusions are the following: 
A. Moving average errors: 
1. With respect to RMSE as well as to the conditional mean forecasting 
risk FR the P(9,3)- and P(5,3)-estimator perform better for the main part 
of the lag-distribution than the R(2,l)- and RI(2,1)-estimator. This re-
sult holds, even if the lag length is misspecified in the P(s,k)-estima-
tor. Reducing the lag length s seems to improve the estimates in terms 
of RMSE by avoiding negative lag coefficients at the right hand tail. But 
then the main part of the lag distribution is more seriously biased. 
2. The P(s,k)-estimator tends to underestimate the peak of the lag distri-
bution, whereas the R(p,q)-estimator overestimates the peak. 

110 
Gerd Hansen 
3. The R(2,l)- and RX (.2,1)-estimator perform better in estimating the 
right hand tail of the lag distribution. 
4. The Durbin-Watson-test detects quite often serially correlated P(s,k)-
residuals, although these residuals should be serially uncorrelated. To-
gether with negative lag coefficients at the right hand tail the serial 
correlation reflects the misspecification of the characteristic root re-
striction. 
5. The MP(6,2)-estimator does not perform better with respect to RMSE and 
FR than the P(s,k)-estimator. But it picks up the residual autocorrela-
tion. Moreover, the mean estimate of the lagged endogenous variable came 
out to be negative. This result is not surprising, because the PDL-model 
assumes too high characteristic roots. But the estimates have not been 
significant in most repetitions. Therefore we expect that little can be 
gained by using an autoregressive P(s,k)-estimator as is often done in 
empirical work. 
6. P(s,k)-estimator with k J 3 are not reported in table 2, because the 
polynomial degree k=3 is reliable specified by using the F-test. The 
P(s,4)- and P(s,2)-estimators have larger RMSE than the P(s,3)-estimator. 
B. White noise errors 
1. With respect to RMSE and FR the results are ambiguous. The R(p,q)-esti-
mator does not dominate the P(s,k)-estimator, although the P(s,k)-resi-
duals are highly autocorrelated as expected. 
2. In general there is little emphasis to use the PDL-model in this case, 
because the appropriate R(p,q)-estimator is easy to calculate. Neverthe-
less, the R(p,q)-estimator does not outfit the P(s,k)-estimator. 
Summarizing these results, the P(s,k)-estimator seems to be more robust 
than the R(p,q)- and RI(p,q)-estimator even if the characteristic root 
restriction is badly specified. Negative lag coefficients at the right 
hand tail indicate this misspecification. They may be avoided by redu-

Robustness of Some Distributed Lag Estimators 
111 
cing the lag length, or may still be accepted in order to obtain small 
RMSE at the main part of the lag distribution. The modified PDL-estimator 
does not outfit the P(s,k)-estimator. 
u = e -. 4 e 
t 
t 
t-1 
u = e 
t 
t 
R(2,l) RI (2, 1) P (9, 3) P (12, 3) P(5,3) MP(6,2) R(2,l) P(9,3) 
FR 
=0,100 
o 
1=0,240 
2=0.396 
3=.1584 
=.0634 
4 
5=.0253 
,.=.0101 
o 
7=.0040 
=.0016 
= .0006 
8 
1.67 
1.13 
1.46 
2.39 
.98 
1.49 
1.06 
1.98 
m 
. 108 
.0950 
.113 
. 163 
.075 
. 147 
.091 
. 103 
r .093 
.093 
.050 
.084 
.072 
.061 
.096 
.065 
m .228 
.259 
.241 
.207 
.302 
.218 
. 255 
.242 
r . 143 
. 146 
.019 
.036 
.084 
.025 
. 123 
.025 
m .517 
.383 
.271 
.214 
.314 
.248 
.407 
.276 
r . 155 
.113 
. 129 
. 183 
.094 
. 150 
.085 
. 128 
m . 106 
. 148 
.231 
. 193 
.207 
.231 
. 146 
.236 
r .072 
.017 
.078 
.044 
.073 
.078 
.038 
.086 
m .027 
.061 
. 149 
. 153 
.078 
. 170 
.058 
.152 
r .038 
.017 
.087 
.093 
.060 
. 109 
.021 
.091 
m . 007 
.027 
.053 
. 100 
.024 
.065 
.024 
.052 
r .019 
.013 
.037 
.077 
.090 
.043 
.011 
.025 
m .002 
.013 
-.029 
.044 
.000 -.085 
.010 -.034 
r .008 
.010 
.055 
.036 
.010 
. 108 
.006 
.035 
m .001 
.006 
-.070 -.007 
.OOO 
.008 
.004 -.075 
r . 003 
.006 
.083 
.022 
.004 
.017 
.003 
.093 
m .0002 .003 
-.042 -.045 
.000 -. 003 
.010 -.043 
r • OOOl .006 
.045 
.054 
.0016 .007 
.003 
.049 
m .0001 
.002 
.084 -.062 
.000 
.001 
.001 
.093 
r . 0005 .003 
. 108 
.069 
. 0006 .002 
.001 
. 133 
Autok (%) 
73 
37 
40 
40 
33 
7 
17 
70 
Table 2: Mean (m) and root mean square error (r) of various estimators 
of model A (A = 4; X =X =0) . 

112 
Getd Hansen 
R (2, 2) R(2,1) RI (2,2) RI(2,1) 
A(9,3) A(12,3) A(5,3) MP(6,2) 
FR 
1.42 
1.80 
1.08 
1.32 
1.39 
2.16 
1.03 
1.57 
B =.loo 
m 
.097 
. 109 
.081 
.085 
.114 
.159 
.079 
.149 
o 
r 
. 100 
.094 
.095 
.094 
.050 
.081 
.070 
.063 
B,=.230 
m 
.240 
.222 
.268 
.259 
.229 
. 199 
.287 
.207 
i 
r 
. 158 
. 142 
.152 
. 149 
.019 
.034 
.080 
.026 
S,=.379 
m 
.430 
.488 
.353 
.355 
.257 
. 206 
.292 
.232 
z 
r 
. 129 
. 144 
.118 
.099 
.127 
. 174 
.098 
. 149 
B =.137 
m 
.088 
.122 
.135 
.156 
.221 
. 187 
.192 
.217 
J 
r 
.072 
.045 
.066 
.028 
.089 
.057 
.078 
.086 
B =.0789 
m 
.087 
.036 
.092 
.071 
. 147 
. 149 
.083 
. 165 
4 
r 
.030 
.046 
.046 
.016 
.070 
.074 
.057 
.089 
6 =.373 
m 
.020 
.011 
.030 
.034 
.059 
.101 
.063 
.075 
Z> 
r 
.027 
.027 
.044 
.013 
.033 
.065 
.093 
.041 
& =.0191 
m 
.020 
.003 
.020 
.017 
-.016 
.049 
.OOO 
-.052 
o 
r 
.012 
.016 
.045 
.010 
.052 
.032 
.019 
.087 
8_=.0095 
m 
.004 
.001 
.012 
.009 
-.055 
.001 
• OOO 
.005 
7 
r 
.010 
.008 
. 060 
.007 
.074 
.021 
. 0095 
.012 
B 
0048 
m 
. 005 
.0004 
.005 
. 005 
-.033 
-.035 
.OOO 
- .002 
o 
r 
.005 
.0044 
.080 
.005 
.040 
.048 
.0048 
.007 
B =.0024 
m 
.001 
.0001 
.019 
.003 
.076 
-.053 
• OOO 
.0004 
9 
r 
.004 
.002 
.115 
.003 
. 106 
.062 
.0024 
.0024 
Autok % 
53 
77 
0 
40 
37 
37 
30 
10 
Table 3: Mean (m) and root mean square error 
(r) of various estimators of 
model B(X =.5, ^^=-.2,^^=0) 
with moving average errors 
( u t = e t - - 3 e t - r - l e t - 2 ) 

Robustness of Some Distributed Lag Estimators 
113 
References 
Dhrymes, Ph.. J. (1971) , Distributed lags, problems of estimation and 
formulation. 
Durbin, J.(1970), Testing for serial correlation in least-squares regres-
sion when some of the regressors are lagged dependent variables, 
Econometrica, 38, 410-421. 
Godfrey, G. and Poskitt, D.S.(1975), Testing the restrictions of the 
Almon lag technique, J.Am.Statist.Ass., 70, 105-108. 
Grether, D.M.(1977), A note on distributed lags, prediction, and signal 
extraction, Econometrica, 45, 1729-1734. 
Judge, G.G. and Bock, M.E.(1978), The statistical implications of pre-
test and Stein-rule estimators in Econometrics, Amsterdam. 
Pagan, A.(1978), Rational and polynomial lags: The finite connection, 
Journal of Econometrics, 8, 247-254. 
Schmidt, P.(1974) A Modification of the Almon distributed lag, J.Am.Sta-
tist.Ass., 69, 679-681. 
Schmidt, P. and Sickles, R.(1975), On the efficiency of the Almon lag 
technique, International Economic Review, 16, 792-795. 
Wallis, K.F.(1980), Econometric implications of the rational expectation 
hypothesis, Econometrica, 48, 49-74. 


Robust Estimates in Linear Regression -
A Simulation Approach 
Siegfried Heiler 
1. SUMMARY 
Finite sample properties of some 25 proposals for robust estimation in li-
near regression are analyzed by simulation. For this purpose four diffe-
rent design situations are combined with 18 error distributions (among 
them contaminated normals, the Cauchy, skew, and autoregressive types). As 
a remarkable result the assessment depends only very little on the specific 
design. Least squares is very poor already with fairly mild deviations from 
normality. M-estimates with redescending weight function turn out to work 
quite well. Their relative assessments depend more on the chosen parameter 
values than on the functional form. R-estimates, which have the advantage 
of being scale invariant and where no parameter has to be fixed in advance, 
are a good alternative. 
2. INTRODUCTION 
In the last years many proposals for robust estimation in linear regression 
models have been issued. Whereas first approaches may be found already in 
1950 (A.M. Mood, H. Theil), a broadly increasing interest can be stated 
only at the beginning of the seventies. After robust estimation of location 
and scale in univariate samples had been treated extensively, research tur-
ned to robust estimation of regression parameters by transfering the main 
aspect of the first case to the latter. This was contrived e.g. in contri-
butions of Huber (1973), Bickel (1973), Jureckova, Jaeckel, Koenker and 
Basset and by many others. For the practitioner who is looking for one 
appropriate alternative to the method of least squares, the great diversity 
of proposals may be quite confusing. For most of the procedures exact state-
ments can be made only with respect to their asymptotic properties. This 

116 
Siegfried Heiler 
was the motive for the extensive Princeton study (Andrews et al.) of the 
finite-sample behavior of 68 robust estimates of location. These estimates 
were applied to four sample sizes (5, 10, 20, 40) and 21 simulated sample 
situations and then assessed and compared with about 10 criteria. By the 
diversity of considered cases and the great number of possible comparisons 
the one or the other reader may get into confusion. The question 
"which is the best estimator?" ... "does not and will never admit a unique 
general answer, since the great variety of situations in applied statistics 
and real life will always demand a variety of tools" (p. 238). Nevertheless 
the pros and cons of groups of estimators in various situations may be seen 
clearly and rules for the practical usage can be derived. For instance the 
class of redescending M-estimates turned out to be very successful in a 
fairly broad range of situations. In the subsequent places estimates which 
work with some rejection rule (like the multiply skipped means) and trimmed 
means which don't trim too little, may be found. 
No comparably extensive investigations for robust regression are available 
hitherto. The intention of this paper is to briefly report about a 
study which is a modest step in this direction. Compared with the estimation 
of location as a further complication the influence of the design (the re-
gressor matrix) on the estimates has to be allowed for. 
The study is confined to linear regression and possible errors in the de-
sign (errors in the variables) are disregraded. Only distribution robust-
ness is thus considered. It is also not intended to enter into the idea of 
robustness in statistics itself as it was introduced by Hampel (in 1968 
and 1971). For that purpose the interested reader may be refered to an in-
creasing number of contributions (e.g. Dutter, Wahrendorf). For the time 
being 8 M-estimates, 4 R-estimates and 12 L-estimates were included in the 
study. They were applied to 4 different designs with 21 different error 
distributions each. In the sequel a brief description of the procedures is 
given. Then the designs and the error distributions are introduced. After 
this the results achieved so far are discussed and commented by means of a 
few assessment criteria. 

Robust Estimates in Linear Regression 
117 
3. 
ESTIMATES 
3.1 M-Estimates 
We consider the usual l i n e a r model y = Xg + e with the components 
y^ = x ( i ) ' B + e^» i = l , . . . , n , where x ( i ) ' denotes the i - t h row of X and 
6 i s the unknown k-vector of parameters. I f the e^ are i . i . d . 
random 
variables with density f Q ( x ) = 0 * f ( x / a ) then with p := - log f the 
maximum l i k e l i h o o d estimation f o r 6 and a leads to the condition 
n 
, 
Q(b,s) 
:= 
£ [ p ( z i ) 
+ log s ] = min. 
i = l 
with 
z. 
:= ( y ± - x ( i ) ' b ) / s 
or, a f t e r d i f f e r e n t i a t i o n , 
to 
(a) 
Z 
lf)(z. ) x ( i ) 
= 0 
i = l 
1 
and 
n 
(b) 
I 
[ijj(z. ) z. - 1 ] = 0 
i = l 
1 
1 
with 
l|) := P* = - 
f ' / f . 
2 
I f f 
i s the normal density, then p(x) = x /2, tp(x) = x and the equations 
° 
2 
(a,b) y i e l d the ordinary l e a s t squares estimates (OLS) of 3 and a . In non-
Gaussian situations the function ip(x) 
= x attaches too much weight to the 
more outlying observations. Therefore the general class of M-estimates f o r 
6 i s introduced as solutions, b, of an equation of the form ( a ) , 
n 
y. - x ( i ) 1 b 
E 
) x ( i ) = a , 
i = l 
where 
i s an odd function and s i s either determined independently or 
simultaneously with b from an equation of the form 
n 
y. - x ( i ) 1 b 
2 
x i - ^ - i 
) = 0. 
i = i 
s 

118 
Siegfried Heiler 
2 
2 
In Huber's proposal 2 (1964) x i s taken as X(x) = *P ( x) " 
Ci/J ), where 
with E^ the expectation under the standard normal distribution is denoted. 
This proposal implies that the scale is estimated by sums of squares of 
some functions of the residuals (SI). Hampel proposed the more robust al-
ternative (S2) to estimate the scale parameter by (med l 2 ^ ~ 
I )/ 0.6754, 
where z.^ denotes the median of the (ordered) z 
, i = l,...,n. 
For the time being 4 special proposals for ijj-functions are included in the 
study. 
. Ix| < k 
1. Huber (HU) : 
= i k sign(x), |x| > k 
w i t h * =1.0. 
2. Hampel (HA): 
ip(x) = sign(x) • 
M 
- W 
< a 
a 
, a < |x| < b 
c - I x I 
c - b a, b < x < c 
with a = 1.5, b = 5.0, c 
3. Andrews (AN): lfi(x) 
3.0. 
with k = 2.1. 
x > k IT 
4. Tukey (TU): 
^(x) 
' x[1 - (x/k)2]2, IxI < k 
0 
, Ixl > k 
with k = 6.0. 
The 4 proposals are used with the scale SI according to Huber's proposal 2 
(HU1, HA1, AN1, TU1) and with the robust scale S2 (HU2, HA2, AN2, TU2). 
3.2 Linear combinations of order statistics 
L-estimates were extended to the linear model by P. Bickel (1973). The 
most promissing proposal in this direction was made by Koenker and Basset 
in 1978. They extend the sample quantile to regression models. Let 
0 < a < 1 and 

Robust Estimates in Linear Regression 
119 
[ax 
, x > o 
pa ( x ) : = 1 (a-l)x, x < o ' 
Then a a-regression quantile is defined as a solution, b^, of the minimization 
problem 
~ 
k) = mi-ni 
These regression quantiles can be evaluated relatively easily by solving a 
linear programming problem. The simplest example of this kind is the 
regression median (MED) with a = 1/2. It determines a hyperplane to which 
(at least) half of the observations have a non-negative and (at least) 
half of the observations have a non-positive distance. 
With the above regression quantiles any linear combinations of order 
statistics may be constructed. In the study the trimean (TRI), 
b < 2 5 1/4 + b > 5 Q 1/2 + 
1/4 and the estimate proposed by Gastwirth 
in 1966 (GAS), 0.3 b — + 0.4 b 
+ 0.3 b —, is included. 
• 33 
• bO 
• 66 
A 2a-trimmed regression, proposed by Ruppert and Carroll, is given by 
deleting all observations from the sample, which have a non-positive 
distance to x'(i) b or a non-negative distance to x'(i) b, 
, i.e. for 
a 
y 
1-a 
which y. - x'(i) b < 0 or y. - x1(i) b, 
> 0. To the remaining obser-
i 
a 
i 
1-a 
^ 
vations least squares is applied. Two such trimmed least squares regression 
estimates with a = 0.1 (TRI) and a = 0.2 (TR2) are used in the study. 
If it is known in an application that the error distribution is skew, then 
it is reasonable to consider asymmetric 
trimming. One such estimator (ATR) 
which deletes the observations for which y^ - x'(i) 
< 0 or 
y^ - x(i)1 b#gj_ > 0 is also included. 
Since in the before mentioned proposal the points on the limiting hyper-
planes are also deleted, the number of actually skipped observations is 
greater than [2an]. This may have adverse effects in small samples. There-
fore Ruppert and Carroll (1979) proposed estimates where a certain propor-
tion of the residuals from some initial estimate b of 3 is deleted. As 
o 
initial estimates b = (b + b, 
)/2 (KBl, KB2 for a = 0.1 resp. a = 0.2), 

120 
Siegfried Heiler 
the median 
(ME1, ME2) and the least squares estimate (OLl) are used. 
Then the [na] smallest and largest residuals are deleted before applying 
least squares. Another version starts like KBl. Then the [2nd] (a = 0.1) 
largest absolute residuals are rejected (AB2). 
3.3 Rank-estimates 
Hodges and Lehmann proposed in 1963 estimates of location based on rank 
statistics. This idea was extended to the regression model by Adichie 
(1967), Jureckova (1971) and Jaeckel (1972). Jaeckel introduced the 
distribution-free measure of dispersion 
n 
D 
:= Z z. a (R.) 
z 
. , l n l 
1=1 
for z := (z.,...,z )', where a (•) is some monotone score function and R. 
I
n
n 
l 
is the rank of z^ in the ordered sample of the z's. He then proposed to 
estimate 6 by a vector b that minimizes the dispersion D(b) := 
the residuals. Since D is a non-negative continuous and convex function 
n 
of b, the minimum can be evaluated relatively easily. For E a (i) = 0 
i=l 
n 
D is location - invariant. Therefore an intercept cannot be estimated by 
Jaeckel's proposal. Such an estimate can be obtained by applying a Hodges 
and Lehmann location estimator to the residuals fa: Jaeckel's proposal, 
+ 
n 
+ 
+ 
i.e. by minimizing a signed rank statistic Sk. := £ a (R.) sign (z.-b ) 
1 
i=i n l 
l 1 
with respect to b^. If by the minimum an interval is defined, the midpoint 
is taken as estimate b. . In Sv[. the z. are the residuals of Jaeckel's 
1 
D1 
I 
proposal, R^ is the rank of 
I i n 
ordered sample of the absolute 
values and a+(•) is some signed rank score function, 
n 
In the study, as score functions the Wilcoxon scores (WIL) (this is the 
original proposal of Hodges and Lehmann), the median scores (MSC), the 
van der Waerden scores (VDW) and the normal scores (NOR) are considered. 

Robust Estimates in Linear Regression 
121 
4. THE SIMULATED SAMPLE SITUATIONS 
4.1 The Designs 
To meet various situations that may occur in practice 4 different designs 
are investigated. They are taken from Carroll (1978). 
Design 1: 
y. = 1 + .5x„. + e., 
n = 20 
l 
2i 
l 
= -.95 + .1 (i-1) 
— X'X = 
n 
.3325 
Design 2: 
y. = 1 + . 5x„. + e. 
n = 40 
l 
2i 
l 
2i 
.342 + .000641Ì(i-1) 
— X'X 
n 
0 
.0933 
Design 3: 
Yi 
1 + . 5x„ . + . 25x . + e., n = 30 
2i 
2i 
l 
x 2 i = (2i - 31)/30 
x 3 i = .60148 + (i-1)(i-30)/225 
X'X = 
/ 1 
0 
0 
.3333 
\ 0 
0 
0 
O 
.08833 
Design 4: 
y. = 1 + .5x_. + .25x-. + e., n 
l 
2i 
3i 
l 
x 2 i = -.34435 + .001149i(i-1) 
30 
X3i = (x2i+.34465)' 
.21374 
— X'X 
n 
No. 1 is a simple linear regression with a balanced design. No. 2 has a 
highly unbalanced design, since the x^^ are very unsymmetrically distri-
buted around zero. The sample size is therefore increased to n = 40. In 
the multiple example No. 3 the two regressors are orthogonal, whereas in 
No. 4 they are highly multicollinear (correlation = .96). 

122 
Siegfried Heiler 
4.2 The error distributions 
Pseudo-random numbers from a uniform (o,l) program were used to generate 
the 18 error distributions. To obtain a balanced situation between the 
variability of the regressors and the errors the normal distribution with 
2 
variance a = 0.02 (a = 0.141), N(0; .02), was chosen as standard. All 
other distributions were 'standardized' in a manner that their density has 
the same value at zero as this reference distribution. Asymmetrical distri-
butions were shifted such that their expectation was zero. 
Index of error distributions 
Symmetrical: 
Normal 
Logistic 
Double (two-sided) exponential 
t-distribution 5 degrees of freedom 
Cauchy 
Asymmetrical : 
Log-normal 
Chi-square 1 degree of freedom 
Contaminated: 
2 
F = (1 —c) N(0; .02) + c N(0;02) 
with P(c=l) = d, P (c=0) = 1 - d 
d = .05, .10, .20 
and 
2 
a 2 = .25 (gentle situation) 
CT„ 2.25 (vigorous situation) 
Tag 
N 
L 
DE 
T 
C 
LN 
CHI 
N5N1 
NlONl 
N20N1 
N5N2 
N10N2 
N20N2 
Nl 
N2 
F = (1 —c) N(0; .02) + c C 
d = .05, .10, .20 
Autocorrelated: 
Autocorrelated normal 
N5C 
NlOC 
N20C 
e. = 0.5 e. 4 + £., £. ~ N(0; .02) 
l 
l-l 
l 
i 
AN 
Autocorrelated contaminated normal 
e. 
i 
0.5 e. , + £ . , £ . 
ì-l 
1 
1 
NlONl 
ACN 

Robust Estimates in Linear Regression 
123 
5. PROPERTIES OF THE ESTIMATES 
5.1 Criteria of Assessment 
For each of the 18 error distributions M = 500 replications of pseudo-
random numbers were generated and attached to the four designs. With 
these 500 simulations the properties of the estimators were assessed 
mainly by the following criteria. 
(i) 
The mean of the estimates 
1 
M 
M(b±) = - E b 
, 
i = 1,2,3, 
m = 500, 
j=l 
where b.. is the estimator for 6. in the j-th simulation run 
and 3 = 1 (intercept), &2 = 0.5, 63 = O.25. 
(ii) 
The mean square error of the estimates 
M 
MSE(b.) = ~ 2 (b..-B.)2, 
i = 1,2,3. 
1 
M j=l 
1 
(iii) The mean square deviation of the regression line resp. regression 
plane 
M 
n 
MSD 
j=l i=l 
x:l 
0 
~ 
o 
with y. = 1 + 0.5 x_. {+ 0.25 x_.) and y.. estimator of y. in the 
1 
2i 
3r 
1 3 
i 
j-th replication. 
(iv) 
Within each of the four designs and for each of the 18 residual 
distributions the estimates are ranked according to their MSD-values 
and following the Princeton study the deficiencies 
DEF = 1 - MSD of best estimator / MSD 
are evaluated. (They depend of course on the arbitrariness of 
the respective reference estimator.) 
(v) 
The computer time needed. 

124 
Siegfried Heiler 
For assessing normality measures of skewness and kurtosis of the joint 
and the marginal distributions of the estimates have been introduced. 
For the time beeing these measures have only been applied on a part of 
the estimates. Therefore this aspect is not discussed here. 
Since space is limited only a small excerpt of the simulation results 
can be exhibited here. Furthermore only 14 of the 25 procedures are included 
in the appended tables. Hence the main outlines of the study will be summa-
rized by letter. Detailed tables may be obtained by writing to the author. 
5.2 Classification of Situations 
In the Princeton study the sample situations are broken down as gentle or 
vigorous, as modified Gaussians or alternative, as reasonable or as notional. 
The Cauchy is classified as notional, i.e. unreasonable. In our case and 
with our parameters (see 4.2) the two-sided exponential and also the logi-
stic have to be put into the same category. The class of gentle and 
reasonable situations consists of the t-distribution, the skew distributions 
2 
log-normal and Xj an<i bY the mixed Gaussians NdNl. The Gaussian mixed with 
Cauchy (NdC) also turned out to be 
gentle. The distributions labelled 
as notional (C,L,DE) describe vigorous situations. The situation becomes 
also vigorous if the proportion of contamination is high. The autoregressive 
normal (AN) and the autoregressive contaminated normal (ACN) were included 
in the study to examine how sensitive robust estimates are to autocorrelated 
errors. This is a situation which one encounters very often in practice. 
Of the two skew distributions the log-normal is 
relatively benign, the 
2 
Xj exhibits relatively large deviations from symmetry. 
5.3 The Influence of the Design 
In the outcomes no clear-cut tendency was visible that a procedure or a 
family of estimates had specific advantages or disadvantages in one of the 
four designs. Merely M-estimates with scale SI had small drawbacks in 

Robust Estimates in Linear Regression 
125 
design 1 (small sample size) for the vigorous situations N10N2 and N20N2, 
compared with some of the L-types and the trimmed regression was slightly 
better off for the unbalanced design 2 (exept N10N2, N20N2 and ACN) than 
for the other designs. 
The rank positions of the procedures very often didn't vary at all or varied 
only little between the four designs. And in most cases where greater diffe-
rences could be observed, the mean square deviations differed only little 
(see also the exhibited tables). Hence as a remarkable result of the study 
one can say that the benefits and shortcomings of the procedures depend 
mainly on the type of the parent residual distribution resp. the kind of 
contamination occuring and only little on the specific design situation. 
5.4 Symmetric Situations 
For symmetric error distributions the majority of the estimates achieved 
about the same precision (compared with other procedures) for the inter-
cept and for the slopes. Those L-estimates which delete a fixed proportion 
of the residuals from an initial estimate (KB1, KB2, OLl, ME 1, ME2) have 
a special feature. 
Having overall positions from mediocre to bad, in a 
great variety of situations they estimated the intercept fairly acceptable 
but were quite poor for the slopes. In cases of Cauchy-contamination (but 
not for the exact Cauchy) the R-estimates WIL and VDW were slightly in-
ferior in estimating the intercept, compared with the slopes. 
For the exact normal distribution OLS had (of course) the lowest mean square 
deviations. The next positions were taken by the redescending M-estimates 
with scale SI AN1, TU1 and HA1 (in this succession) which had deficiencies 
not exceeding 3.6 % (see table 1.1). The MSD-values here directly depend on 
the length of the increasing phase of the corresponding weight function. 

126 
Siegfried Heller 
T-functions of the four M-estimates 
This Ml-group is then followed by the R-estimates VDW and NOR. The next 
group of still good estimates consists of WIL, HU1 and AN2. 
Whereas OLS completely failed already with 5 % Cauchy contamination, the 
other estimates coped with this situation very well. For the low percentage 
the M-estimates AN1, TU1 and HA1 have the lowest MSD-values, followed 
(already) by AN2 and the R-estimates VDW and NOR. The next positions are 
held by WIL and HU1. For N20C the situation is similar to the mixed normal 
N5N1: Leading group is HA1, TU1 and AN2 , followed by HU1, AN1, these again 
followed by 
WIL, VDW and NOR. 
With increasing proportion of contamination and when the situation becomes 
more vigorous (N20N1, N10N2, N20N2) the redescending M-estimates with robust 
scale S2 take over the leading positions and the positions of the R-estimates 
deteriorate. For N20N1 and N10N2 HU1 and WIL still yield good results. For 
the most vigorous contamination N20N2 the L-estimates GAS and ME2 are 
acceptable alternatives. 
For the unrealistic case of the exact Cauchy the mean square deviations 

Robust Estimates in Linear Regression 
127 
vary considerably between the procedures. Here the median based estimates 
MED, ME2 and MSC are good alternatives to HA2, TU2 and HU2. 
For the autocorrelated normal, AN, we have almost the same situation as 
for the exact normal with MSD-values about three times as high. The highest 
mean square errors of all situations analysed resulted from the autocorre-
lated contaminated normal, ACN. The positions are comparable to N20N1: 
Leading TU2, HA2, followed by AN2, HU1 and HU2, these again followed by 
WIL, TRI and GAS. 
5.5 Asymmetric Situations 
With asymmetric error distributions the intercept were biased in the direc-
tion of the median for most of the robust procedures. The moderate asymme-
try of the log-normal had only small effects on the coefficients and the 
assessment of the estimates. Whereas the M-estimates with scale SI were 
barely influenced, M-estimates with robust scale, S2, reacted very sensi-
tively to asymmetry. Here already for the LN their results were relatively 
poor. (Of course ore might ask if the expectation is the "correct" location 
parameter in asymmetric situations. But we do not persue this question here.) 
2 
For the Xj-distribution OLS had (due to its unbiasedness) the smallest mean 
square error for the intercept, but it was among the worst for the slopes. 
The same tendency could be observed for the asymmetric trimming procedure, 
ATR, which cannot be recommended for that reason. 
The bias of the intercept was especially high for the unbalanced design 2, 
as may be seen by looking at the means of the intercepts for several 
selected estimates: 
Proc 
OLS 
HU1 
AN1 
HA1 
TU1 
HU2 
AN2 
HA2 
TU2 
WIL 
VDW 
NOR 
M(bj) .999 .970 .982 .974 .972 .961 .968 .955 .959 .974 .984 .984 
AN1, VDW and NDR had the smallest bias. This explains their positions in 

128 
Siegfried Heiler 
the MSD-table 1.2. The best positions for both, intercept and slopes, were 
held by the R-estimates NOR and VDW. WIL had position 3 for the slopes, but 
was slightly inferior for the intercept (because of its bias). AN1 works 
very good. The M-estimates with robust scale have medium positions for the 
slopes but give quite poor estimates for the intercept. 
5.6 Comparing estimates 
Of the three analyzed classes of estimators one can say - in so far in 
accordance with the Princeton study - that M-estimators do best. Properly 
chosen M-estimates are 
able to cope with almost every conceivable 
situation. In more gentle cases the scale SI is preferable. If one has to 
reckon with higher contamination or fat tails and if asymmetry is not to 
be expected then the robust scale S2 is superior. This scale should be 
avoided if asymmetry of the parent distribution cannot be excluded. HU1 
and AN2 were among the best estimators in the study (see the tables 2.1 
and 2.2). HA and TU showed similar results which were excellent in vigorous 
situations for the scale S2. 
In many cases R-estimates are a good alternative. They have the practical 
advantage that no parameter values have to be specified in advance and they 
are scale-invariant. On the other hand they are more time consuming. Where-
as VDW and NOR do well with asymmetry but are good only for fairly gentle 
situations not too far off the Gaussian, WIL has a broad applicability in 
non-vigorous cases. 
L-estimates achieved good positions only in specific vigorous situations. 
Their overall assessments were mediocre to bad. Trimmed least squares had 
some advantages in the unbalanced design 2. To achieve improvements in 
vigorous cases presumably the trimming proporties has to be increased. GAS 
had some good results for N20N1, N20N2, DE and ACN. No really good results 
were achieved with KB1 and KB2. 

Robust Estimates in Linear Regression 
129 
6. Tables 
ABLE 1. 1 : MEAN SQUARE DEVIATION OF 14 ESTIMATORS FOR : N(0;0.02) 
0 E S I <3 N 1 
0 £ 5 I G N 2 
D E S I G N 3 
D E S I G N ' . 
PROC II MSD I DEF I RANK II MSO I OEr I RANK I I MSO I DEF I RANK || MSD I OEF I RANK 
= = = = t 1 
= = = = = = = = = = = = = = = 1 
= = = = = = = = = = = = 
======== = = = * 
===== ======= = = = = 
OLS 1 1 
tI 0.0405 
t I 
1 0.0 1 1 
1 
10.0395 
I 
10.0 1 1 
I 1 
1 10.0597 
i t 
10.0 1 1 
1 1 
1 1 0. 
1 t 0606 1 0.0 1 1 
HU1 1 l 
110.0445 
1 I 
1 0.090 1 5 
1 
10.0433 
] 
10.088 | 7 
1 1 
I 10.0653 
j 
10.086 1 7 
1 I 
1 1 0. 
1 0668 10.093 1 7 
ANI ! 1 
]10.0406 
j j 
10.007 1 2 
10.0399 
1 
10.010 1 2 
J 1 
1 10.0601 
I 1 
10.007 | 2 
1 1 
1 1 0. 
I 0610 10.007 1 2 
HAI ! 10.0420 
i 
10.036 1 3 
1 
10.0407 
1 
10.029 | 3 
1 1 
I 10.0613 
( i 
10.026 | 3 
1 1 
1 1 0. 
1 ( 0621 1 0.024 I 3 
HU2 I i 
110.0495 10.182 1 10 
io.0473 
j 
10.165 | 12 
1 1 
I 10.0740 
j j 
io.193 1 10 
1 i 0. 
1 1 0759 10.202 1 11 
AN 2 
| j 
110.0453 10.106 1 7 
10.0425 
i 
10.071 1 6 
1 10.0651 
t t 
10.083 I 6 
1 1 
1 10. 
1 1 0661 10.083 i 6 
HA 2 1 l 
110. 04<J6 
1 I 
10.183 1 11 
! 10. 0468 
i 
10.156 1 11 
1 1 
1 10.C742 
1 1 
10.195 1 11 
1 1 
1 ! 0. 
1 1 0758 10.201 1 10 
WIL 
I i 
110.0446 
j | 
1 0.C93 1 6 
< 
10.0424 
l 
10.068 1 5 
1 1 
1 10.0643 
j j 
10.072 1 5 
I 1 
1 1 0. 
1 1 0655 10.075 I 5 
vow 110.0427 
| | 
I0.C52 1 4 
1 
10.0414 
i 
10.045 1 4 
1 10.0618 
I 1 
10.034 | 4 
1 ! 
1 1 0. 
1 1 0629 10.036 1 4 
TRI NO.0477 
11 
10.151 1 9 
1 
10.0442 
• 
10.106 1 8 
1 1 
110.0723 10.174 | 8 
1 1 
11 o. 
1 1 0754 10.196 1 9 
TR2 11 
110.0519 
i i 
10.220 1 14 
1 
10.0465 
j 
10.151 1 10 
| j 
1 10.0769 
I j 
10.224 | 13 
1 1 
1 1 0. 
! I 0784 10.227 1 13 
KB I 1 1 
I 10.0466 10.135 1 8 
10.0443 10.108 1 9 
1 10.0734 10.187 1 9 
1 1 0. 
1 1 072 8 10.168 1 8 
KB2 
I | 
110.0505 10.198 1 12. 5 I 
| 
10.0477 
I 
10.172 1 13 
j j 
110.0773 
I I 
10.228 I 14 
1 1 
1 1 0. 
1 1 0799 10.242 1 14 
GAS 
j | 
110.05C5 
j | 
10.198 1 12. 5 I 
1 
10.0491 
1 
10.196 1 14 
1 1 
I]0•0767 
| l 
10.222 1 12 
1 1 
1 10. 
1 1 0772 10.215 1 12 
SS3S 
*a s= = 
333 1 
= = = = = = = = = = = = 
1 I 
_ _ 
- -, _ _ _ 1 1 
= = = = = = = = = = = = = = = = 
ABLE 1. Z : 1EAN SilUARE DEVIATION OF 14 ESTIMATORS FOR : CHI-SSUARE (O.C855) 
D E S I G N 1 
D E S I G N 2 
D E S I G N 3 
D E S I G N 4 
PROC II MSD I DEF I RANK II MSD I DEF I RAflK I I MSD I OEF I RANK II MSO I OEr I RANK 
= = * = == = = = === = 
======== 
= = = = 
1 t 
======= ====== 
= = = = = = = = = 
t i 
======== 
= = = = 
J 1 ===== ======== 
= = -
CLS 
1 1 
110.0315 10.269 1 5 
1 I 
1 1 0.0291 
t 1 
0.127 1 2 
1 1 
I 10.0449 
I 1 
10.377 1 6 
1 i 
1 10. 
I I 0440 10.321 1 5 
HU1 1 1 
110.0313 
1 S 
10.264 | 4 
1 1 
110.0499 
1 i 
0.491 1 8 
1 1 
I 10.0441 
j j 
10.366 1 5 1 1 
Ilo. 
1 1 0453 10.340 1 6 
ANI ! 1 
110.0302 10.238 1 3 
1 1 
110.0363 
1 i 
0.300 1 3 
1 10.0402 
j j 
¡0.304 1 3 
Ilo. 
I 1 0400 10.253 1 3 
HAI 110.0319 
1 I 
10.278 1 6 
1 1 
110.0461 
1 1 
0.449 1 7 
1 10.0437 
I 1 
¡0.360 1 4 
I I 
no. 
II 043 5 10.313 1 4 
HU2 1 1 
110.0422 
( t 
10.454 I 13 
1 ! 
110.0732 
j j 
0.653 1 13 
1 1 
1 10.0611 
1 
10.542 1 12 
! 1 
Ilo. 
1 1 
C622 10.519 1 11 
AN 2 t t 
110.0383 
t I 
10.399 1 9 
110.0593 
t ] 
0.572 1 9 
1 1 
110.0556 
j j 
10.497 | 9 
Ilo. 
1 1 0580 10.485 1 9 
HA 2 I 1 
110.0517 
i > 
10.555 1 14 
1 1 
110.0^47 
t 1 
0.732 1 14 
1 10.0770 10.637 1 14 
1 1 
Ilo. 
1 1 0807 10.630 1 14 
MIL 
1 1 
1 1 0.0276 
1 1 
10.166 1 2 
1 1 
]10.0396 
j j 
0.359 1 4 
| j 
1 10.0380 
1 i 
10.264 1 2 
1 1 
1 lo. 
1 1 0398 10.248 1 2 
VDW I \ 
110.0230 
i 1 
1 0.0 1 1 
110.0254 
t I 
0.0 1 1 1 1 
I 10.0200 
! 1 
10.0 1 1 
1 1 
ilo. 
1 1 0299 10.0 1 1 
TRI I 1 
110.0344 
t I 
10.331 1 8 
1 1 
I 10.0448 
» i 
0.433 1 6 
1 1 
I 10.0479 
i 1 
10.416 | 7 
1 1 
Ilo. 
1 t 0509 10.413 1 7 
TR2 1 1 
110.0403 
I j 
10.429 1 11 
1 1 
110.0639 
j j 
0.603 1 11 
1 1 
II0.C602 
I | 
10.535 1 10 
1 1 
ilo. 
1 1 
0635 10.529 1 12 
KBl I I 
MO.0327 
II 
10.296 1 7 
( l 
|10.0444 
i > 
0.428 1 5 1 1 
110.0502 
1 
10.443 1 8 
Ilo. 
1 1 0522 10.427 1 8 
KB 2 I » 
110.0399 10.423 1 10 
I 1 
I 10.0631 
• i 
0.598 1 10 
1 1 
I 10.0667 
1 I 
10.581 1 13 
1 ( 
Ilo. 
1 1 072 6 10.588 1 13 
GAS 1 1 
110.0411 
1 i 
10.440 1 12 
11 
110.0723 
t t 
0.Ó49 Ì 12 
1 1 
1 10.0610 
| ] 
10.542 1 11 
I 1 
Ilo. 
1 1 
0610 10.510 1 10 
1 1 
sa:sss== 
= = = = 
1 ! 
= = = = = = = = = = = = - = . - - -= = = = = = 1 1 
= = = = = = = = »ss a 
= = = = = = = = = = = = = = = = 

130 
Siegfried Heiler 
T A B L E 
l . 
3 
: 
M E A N 
S H U A R E 
D E V I A T I O N 
O r 
1 4 
E S T I M A T O R S 
FOR 
: 
N ( 0 : 0 . 0 2 ) S N ( 0 ; 0 . 2 5 ) 
0 
E 
S
I
G
N 
1 
0 
E 
S
I
G
N 
2 
D 
E 
S
I
G
N 
3 
D 
E 
S
I
G
N 
4 
P R O C 
t n s : 
I | 
M S O 
1 
D E F 
1 
R A N K 
I I 
M S D 
1 
Dfc F 
1 
1 
R A N K 
1 1 
M S O 
1 1 
1 
D E F 
1 
R A N K 
1 1 
M S O 
s a s a 
t e s a : 
| | 
1 
D E F 
1 
RANK 
1 
O L S 
1 1 
| | 0 . 0 5 9 8 
1 0 . 2 0 4 
1 4 
1 1 
1 1 0 . 0 5 8 3 
1 
0 * 2 2 5 
1 
1 
1 4 
1 1 
1 1 0 . 0 9 6 7 
I 1 
1 0 . 2 6 0 
1 4 
I 1 0 . 0 9 6 2 
j | 
1 0 . 2 5 7 
1 4 
1 
H U I 
1 1 
1 1 0 . 0 4 8 7 
1 0 . 0 2 3 
3 
1 1 
I 1 0 . 0 4 7 0 
1 1 
0 . 0 3 8 
i 
1 
5 
I 
1 0 . 0 7 4 1 
j 1 
1 0 . 0 3 4 
3 
1 1 0 . 0 7 3 8 
| | 
1 0 . 0 3 1 
3 
1 
A N I 
1 1 
H O . 
0 4 9 2 
1 0 . 0 3 3 
4 
1 1 
1 1 0 . 0 4 0 8 
i I 
I 
0 . 0 3 4 
| 
1 
4 
I 
1 0 . 0 7 4 6 
i 
1 0 . 0 4 0 
4 
1 1 0 . 0 7 5 7 
| 1 
1 0 . 0 5 5 
5 
1 
H A I 
1 1 
I 1 0 . 0 4 7 6 
1 0 . 0 
1 
1 1 
1 1 0 . 0 4 5 2 
I 1 
1 
0 . 0 
1 
1 
1 
| 
1 0 . 0 7 1 6 
j j 
1 0 . 0 
1 
1 1 0 . 0 7 1 5 
| | 
1 0 . 0 
1 
1 
HU 2 
1 1 
I 1 0 . 0 5 3 7 
1 0 . 1 1 4 
9 
1 1 
1 1 0 . 0 5 1 1 
i t 
1 
0 . 1 1 5 
I 
1 
1 2 
I 
1 0 . 0 8 1 1 
1 j 
1 0 . 1 1 7 
9 
I 
1 0 . 0 8 1 0 
| | 
1 0 . 1 1 7 
7 
| 
AH 2 
I I 
I 1 0 . 0 4 7 9 
1 0 . 0 0 6 
2 
1 1 
1 1 0 . 0 4 5 5 
I i 
0 . 0 0 7 
1 
i 
2 
I 
1 0 . 0 7 3 1 
| J 
1 0 . 0 2 1 
2 
1 1 0 . 0 7 3 0 
| j 
I 0 . 0 2 1 
2 
1 
H A 2 
1 1 
| 1 0 . 0 5 3 8 
1 0 . 1 1 5 
1 0 
1 1 
I 
1 0 . 0 4 9 9 
1 
0 . 0 9 4 
| 
1 
9 
1 
1 0 . 0 7 9 4 
| ] 
1 0 . 0 9 8 
7 
1 1 0 . 0 8 1 7 
j | 
1 0 . 1 2 5 
8 
1 
W I L 
1 1 
1 | 0 . 0 4 9 7 
1 0 . 0 4 2 
6 
1 1 
I 1 0 . 0 4 6 5 
i 1 
1 
0 . 0 2 7 
1 
I 
3 
I 1 0 . 0 7 5 3 
j j 
1 0 . 0 5 0 
5 
1 1 0 . 0 7 5 3 
| | 
1 0 . 0 5 0 
4 
1 
VDW 
1 1 
I 1 0 . 0 4 9 5 
1 0 . 0 3 9 
5 
1 1 
I 1 0 . 0 4 7 5 
l 
0 . 0 4 9 
I 
1 
7 
1 1 0 . 0 7 6 7 
• t 
1 0 . 0 6 6 
6 
1 1 0 . 0 7 7 
0 
| j 
1 0 . 0 7 2 
6 
1 
TR 1 
1 1 
1 1 0 . 0 5 2 5 
1 0 . 0 9 3 
8 
1 1 
I 1 0 . 0 4 7 4 
] 
0 . 0 4 6 
1 • 
6 
| 
1 0 . 0 8 0 2 
i i 
1 0 . 1 0 7 
8 
1 | 0 . 0 8 4 8 
| j 
1 0 . 1 5 7 
1 0 
1 
TF 2 
1 1 
I 1 0 . 0 5 5 9 
1 0 . 1 4 8 
1 3 
1 1 
1 1 0 . 0 5 0 7 
t 1 
] 
0 . 1 0 8 
1 
1 0 
1 
1 0 . 0 8 5 0 
i j 
1 0 . 1 5 8 
1 2 
I 
1 0 . 0 8 9 8 
j | 
1 0 . 2 0 4 
1 2 
1 
K 8 1 
1 1 
1 1 0 . 0 5 1 6 
1 0 . 0 7 8 
7 
1 1 
1 1 0 . 0 4 9 0 
1 
0 . 0 7 8 
1 
I 
8 
I 
1 0 . 0 8 4 1 
I I 
1 0 . 1 4 9 
1 0 
I 
1 0 . 0 8 7 9 
| | 
1 0 . 1 8 7 
1 1 
1 
KB 2 
1 1 
1 1 0 . 0 5 5 5 
1 0 . 1 4 2 
1 2 
1 1 
1 I 0 . 0 5 1 0 
1 
0 . 1 1 4 - 
I 
t 
1 1 
I 
1 0 . 0 8 7 3 
j j 
1 0 . 1 8 0 
1 3 
1 1 0 . 0 9 1 1 
j | 
1 0 . 2 1 5 
1 3 
1 
G A S 
1 1 
I 1 0 . 0 5 4 6 
1 1 
1 0 . 1 2 8 
1 1 
1 1 
I 1 0 . 0 5 2 8 
1 1 
1 
0 . 1 4 4 
1 
! 
1 3 
1 1 0 . 0 8 4 9 
1 1 
1 0 . 1 5 7 
1 1 
1 1 0 . 0 8 3 5 
1 1 
1 0 . 1 4 4 
9 
1 
T A B L E 
1 . 
+ 
: 
M E A N 
O
E
S 
S Q U A R E 
D E V I A T I O N 
O F 
1 4 
I
G
N
I 
O
E
S 
P R O C 
S 8 3 I 3 
O L S 
H U I 
A N I 
H A I 
H U 2 
AN 2 
HA 2 
W ! L 
VOM 
T R I 
T R 2 
K B 1 
KB 2 
G A S 
I I 
M S O 
I 
D £ F 
I 
R A N K 
] | 
M S D 
1 
E S T I M A T O R S 
F O R 
I 
G N 
2 
O E F 
I 
R A N K 
I | 
: N ( 0 ; 0 . 
O
E
S 
M S O 
I 
0 2 ) 2 0 N ( 0 ; 2 « 
I 
G N 
3 
2 5 ) 
D E F 
| 
R A N K 
O
E
S 
MSD 
| 
I 
G N 
4 
D E F 
| 
R A N K 
I 
1 1 
1 1 0 . 9 1 3 3 
t i 
1 0 . 9 0 5 
1 4 
1 ! 
1 1 0 . 9 3 6 7 
t i 
1 0 . 9 3 4 
1 4 
1 1 
1 
1 1 . 4 7 2 7 
1 0 . 9 3 4 
1 4 
1 1 
1 1 1 . 4 7 9 2 
1 0 . 9 2 2 
1 4 
1 1 
1 1 0 . 1 1 7 5 
1 0 . 2 6 2 
6 
1 1 
1 1 0 . 0 8 5 9 
j j 
1 0 . 2 7 9 
4 
j | 
1 1 0 . 1 7 7 3 
! I 
1 0 . 4 4 8 
5 
j | 
1 1 0 . 1 9 3 3 
1 i 
1 0 . 4 0 1 
4 
j j 
11 0 . 3 5 4 4 
I 1 
1 0 . 7 5 5 
1 3 
1 1 0 . 1 9 5 2 
I I 
1 0 . 6 8 3 
1 2 
1 1 
I 1 0 . 4 8 3 0 
j | 
1 0 . 7 9 8 
1 3 
1 1 
1 1 0 . 5 2 6 8 
| | 
1 0 . 7 8 0 
1 3 
1 1 
! 
1 0 . 1 5 6 6 
| j 
1 0 . 4 4 6 
9 
1 1 
1 1 0 . 0 8 8 6 
j | 
1 0 . 3 0 1 
6 
] 1 0 . 1 8 8 6 
j j 
1 0 . 4 8 1 
6 
1 1 0 . 2 0 9 7 
| j 
1 0 . 4 4 8 
5 
1 1 0 . 1 0 1 3 
I i 
1 0 . 1 4 4 
3 
1 1 0 . 0 7 3 6 
1 I 
1 0 . 2 1 2 
3 
1 1 0 . 1 3 9 8 
| j 
1 0 . 3 0 0 
3 
1 [ 0 . 1 7 1 3 
| f 
1 0 . 3 2 4 
3 
l l 
1 1 0 . 0 9 7 0 
i 
1 0 . 1 0 6 
2 
i i 
I 
1 0 . 0 6 6 8 
j j 
1 0 . 0 7 3 
2 
I 1 0 . 1 0 9 4 
| j 
1 0 . 1 0 6 
2 
1 1 0 . 1 1 9 5 
j j 
1 0 . 0 3 1 
2 
1 1 
1 1 0 . 0 8 6 7 
| | 
1 0 . 0 
1 
1 1 0 . 0 6 X 9 
1 1 
1 0 . 0 
1 
1 1 0 . 0 9 7 8 
j | 
1 0 . 0 
1 
1 1 0 . 1 1 5 8 
| j 
1 0 . 0 
1 
1 1 0 . 1 2 4 3 
j 
1 0 . 3 0 2 
d 
I I 
] 1 0 . 0 9 6 9 
i i 
1 0 . 3 6 1 
7 
I 
1 0 . 2 0 6 0 
j j 
1 0 . 5 2 5 
8 
1 1 0 . 2 2 1 4 
j 1 
1 0 . 4 7 7 
7 
i ) 
1 1 0 . 1 6 5 5 
i 
1 0 . 4 7 6 
1 0 
I 1 
1 1 0 . 1 3 1 2 
1 i 
1 0 . 5 2 8 
1 0 
1 1 0 . 2 7 7 4 
| j 
1 0 . 6 4 7 
1 0 
t 1 
1 1 0 . 2 9 7 0 
j 
1 0 . 6 1 0 
9 
1 1 
1 1 0 . 1 9 6 9 
| | 
1 0 . 5 6 0 
1 1 
i l 
! 
1 0 . 1 8 0 3 
1 I 
1 0 . 6 5 7 
1 1 
1 1 0 . 3 7 9 0 
| j 
1 0 . 7 4 2 
1 1 
1 1 0 . 3 6 5 2 
j | 
1 0 . 6 8 3 
1 1 
1 1 0 . 1 1 3 7 
1 | 
1 0 . 2 3 7 
5 
l I 
1 1 0 . 1 0 0 6 
j | 
1 0 . 3 8 5 
8 
1 
1 0 . 1 9 1 4 
j | 
1 0 . 4 8 9 
7 
1 1 0 . 2 2 6 6 
j | 
1 0 . 4 8 9 
8 
i 1 0 . 2 5 5 0 
| j 
1 0 . 6 6 0 
1 2 
! 
1 0 . 2 1 7 7 
i j 
1 0 . 7 1 6 
1 3 
1 1 0 . 4 7 1 7 
1 0 . 7 9 3 
1 2 
1 1 0 . 4 7 2 7 
I 0 . 7 5 5 
1 2 
1 1 0 . 1 2 3 3 
1 0 . 2 9 7 
7 
1 1 0 . 1 0 4 6 
j j 
1 0 . 4 0 8 
9 
j | 
1 1 0 . 2 4 5 2 
I t 
I O . 6 0 1 
9 
| j 
1 1 0 . 3 2 9 1 
1 0 . 6 4 8 
1 0 
i | 
1 1 0 . 1 0 7 4 
I I 
1 0 . 
1 9 3 
4 
1 1 0 . 0 8 8 5 
I I 
1 0 . 3 0 1 
5 
1 i 
I 1 0 . 1 7 4 « 
1 1 
1 0 . 4 4 0 
4 
| j 
1 1 0 . 2 1 4 0 
1 1 
1 0 . 4 5 9 
6 

Robust Estimates in Linear Regression 
131 
T A B L E 
2 . 
1 : D E F I C I E N C I E S AND RANKS OF 
: 
HU1 
D
E
S
I
G
N
I 
D I S T 
|| 
DE F 
I RANK 
I 
I 
1 0 . 0 8 9 9 
I 
1 0 . 0 
I 
1 0 . 0 7 2 9 
I 
I 0 . 2 6 4 4 
I 
10.0226 
I 
l o . 0 1 2 a 
I 
1 0 . 0 
I 
1 0 . 0 7 3 0 
I 
10.0600 
I 
1 0 . 0 3 8 1 
I 
I 0 . 3 8 1 7 
I 
1 0 . 1 3 5 2 
I 
I 0.2621 
I 
1 0 . 0 0 7 9 
I 
1 0 . 0 5 2 0 
I 
I 0 . 3 8 7 9 
I 
1 0 . 0 3 9 0 
I 
1 0 . 0 2 9 2 
I 
D E S 
DEF 
O . C 8 7 a 
0.0 
0 . C 7 8 2 
0 . 4 9 1 1 
0 . 0 3 8 3 
0 . 0 4 3 9 
0 . 0 3 7 3 
0 . 0 7 3 4 
0.0626 
0 . 0 5 0 5 
0 . 1 C 2 9 
0 . 1 6 5 2 
0 . 2 7 9 4 
0.0 
0 . 0 7 3 6 
0 . 2 8 2 9 
0 . 0 3 5 5 
0 . 0 5 1 0 
G N 2 
RANK 
| 
7 
1 
7 
3 
5 
3 
3 
6 
6 
4 
4 
4 
4 
1 
5 
6 
7 
4 
D E S I 
I 
D;F 
I 
£I(33C sss: 
I 
1 0 . 0 8 5 8 
I 
1 0 . 3 0 4 1 
1 
1 0 . 0 7 0 2 
I 
1 0 . 3 6 5 8 
I 
1 0 . 0 3 3 7 
I 
1 0 . 0 2 9 3 
I 
1 0 . 0 5 0 7 
I 
1 0 . 0 7 0 0 
I 
1 0 . 0 5 4 9 
I 
1 0 . 0 4 5 6 
I 
1 0 . 1 0 5 7 
I 
1 0 . 1 9 4 2 
I 
1 0 . 4 4 8 4 
I 
1 0 . 0 1 4 0 
I 
1 0 . 0 2 9 8 
I 
1 0 . 3 1 7 0 
I 
1 0 . 0 3 9 1 
I 
1 0 . 0 1 9 8 
I 
G N 3 
D E S 
RArtK 
II 
DEF 
G N 4 
RANK 
I I 
0 . 0 9 2 8 
0 . 0 3 5 2 
0 . 0 7 7 5 
0 . 3 4 0 2 
0 . 0 3 1 2 
0 . 0 2 4 3 
0 . 0 3 8 Ó 
0.0611 
0 . 0 5 5 6 
0 . 0 5 9 0 
0.1122 
0 . 1 9 7 1 
0 . 4 0 0 9 
0 . 0 3 0 5 
0 . 0 0 7 8 
0 . 2 1 0 3 
0 . 0 3 4 6 
0.0260 

132 
Siegfried Heiler 
TABLE 2. 2 : DEF 
ICIENCIES ANO RANKS OF : 
AN2 
D E S I G N I 
D E S I G N 2 D E S : G N 3 D E S I G 
D1ST 1 1 DE" 
I I 
1 RANK I1 
1 
I 
DEF | 
- 
• i -1 
I 
RANK 1 1 DEF 
aasasasas 
1 i 
1 RA IK 
= 3 3 3333 
1 1 DEF 
33:313«a a 
l 1 
1 RA 
«333 
N 
1 1 
1(0.1060 
i i 
1 
1 
1 7 ! 
i 
t 
i 
0.0706 | 
1 6 
1 1 
I 10.0829 
t i 
I 6 
1 1 
110.0832 
• i 
1 6 
T 
1 1 
1 ! 0.0407 
I t 
< 
1 
1 4 | 
1 
i 
1 
0.0132 1 
i 6.5 I 1 
i10.0357 
1 1 
1 4 
1 i 
110.0352 
1 1 
1 3 
IN 
1 1 
110.0895 
1 i 
1 
1 
I 7 l 
1 
J 
0.0609 | 6 
1 1 
1 10.0731 
| j 
i 7 
I 1 
I 10.0789 
1 | 
1 7 
CHI l [ 
1 | 0.39 89 
i I 
1 
l 
1 9 I 
1 
| 0.5718 1 
1 9 
110.4970 
1 I 
1 9 
l I 
1I0*4846 
1 
1 9 
N5N1 1 1 
1|0.0063 
j I 
1 
1 
1 2 ] 
1 
i 
! 
O.OC66 1 2 
1 1 
i10.0205 1 2 
1 l 
I10.0205 
j j 
1 2 
N10N1 ) 10.0 
1 1 
l 
I 
1 1 l 
i 
i 0.0 
| 
j 1 
| j 
1 10.0 
1 1 
1 1 
1 io.o 
j 
1 1 
N20N1 110.0360 
i I 
1 
1 
1 4 1 
i 
i 0.0315 | 
1 2 
1 1 
110.0465 
t 1 
1 3 
i10.0540 
i i 
I 4 
N5C 
1 1 
1I0.0668 
j I 
I 
1 
1 4 ! 
1 
0.0404 | 
i 5 
1 I 
|10.0590 1 3 
1 1 
110.0387 
i I 
1 3 
NIOC I 1 
1I0.0457 
1 I 
1 
1 
i 3 1 
t 
] 
1 
0.0210 1 
1 3 
i J 
110.0387 
t i 
1 3 
1 I 
110.0244 
1 t 
1 3 
N20C I i 
110.0244 
i | 
! 
i 
1 2 ! 
1 
1 
1 
0.0 
1 1 1 
I 1 
110.0108 
i 1 
1 2 
l i 
1 lo.0 
1 
1 1 
N5N2 1 10.0 
1 1 
1 
1 
1 1 1 
1 
1 
1 
0.0 
1 
1 1 1 1 
110.0209 
I j 
1 2 
1 i 
1 Io.o 
1 I 
I 1 
N10N2 1 1 0.0 
1 1 
1 
1 
1 1 1 
1 
1 0.0 
1 1 1 1 10.0 
1 1 
1 1 
1 I 
1 10.0 
1 1 
1 l 
N20N2 1 1 
110.1062 
1 i 
1 
I 
! 2 ] 
1 
1 
1 
0.0734 | 
I 2 
1 1 
I io.1060 1 2 
1 1 
1¡0.0310 
i i 
1 2 
L 
1 1 
1|0.0492 
1 
1 
i 
1 6 i 
i 
I 
1 
0.0356 | 
I 5 
| | 
I 10.0346 
| 
1 6 
1 1 
|10.0467 
1 I 
1 6 
DE 
1 ! 
1I0.1007 
i | 
I 
l 
1 8 { 
1 
i 
] 
0.1435 1 
j 8 
1 10.0973 
j | 
1 7 
l I 
110.0410 
! j 
1 6 
C 
1I0.1822 
1 I 
i 
1 
1 3 I 
1 
] 0.1863 1 
I 4 
1 10.1964 
1 i 
1 3 
! 10.2509 
t I 
1 6 
AN 
\ 1 
110.0252 
1 I 
1 
1 
1 5 I 
i 
i 
1 
0.0197 | 
1 6 
1 1 
I 10.0382 
j j 
1 6 
1 1 
110.0332 
1 i 
1 6 
ACN 1 I 
110.0236 
1 i 
1 
1 
1 3 1 
I 
1 
0.0295 | 
1 
2 
1 10.0051 
I 1 
1 2 
1 i 
I Io.o 
II 
1 1 

Robust Estimates in Linear Regression 
133 
TABLE 2. 3 
: DEFICIENCIES AND RAIJKS Or : 
WIL 
DIST 
a s s e s s : 
0 = S : G N L 
II 
DS= 
I RANK 
D E S I 
I 
DEF 
I 
D E S I 
RANK 
II 
DEF 
I 
a^s : : u : i a i : 3 : 3 
6 N 3 
D E S I G N 4 
RANK II 
DEF 
I RANK I I 
0.0925 
0.0360 
0.0748 
0.1664 
0.0421 
0.0489 
0.0514 
0.0737 
0.0711 
0.0548 
0.1212 
0.1950 
0.3025 
0.0344 
0.0739 
0.3953 
0.0321 
0.0504 
10.0684 
I 
IC 
I 
10.0236 
! 
10.0519 
1 
10.3589 
I 
10.0274 
I 
10.C614 
I 
10.1045 
I 
10.0359 
I 
10.0413 
I 
I 0.0563 
! 
I0.1C92 
I 
10.2115 
I 
10.3613 
I 
10.0134 
I 
10.0S52 
I 
10.3379 
I 
10.0185 
1 
I0.C809 
0.C719 
0.0208 
0 . 0 5 6 6 
0 . 2 6 3 9 
0 . 0 4 9 7 
0.0558 
0.1019 
0.0708 
0 . 0 6 4 0 
0.0761 
0.1316 
0.2639 
0 . 5 2 5 3 
0.0168 
0.0452 
0 . 3 6 8 9 
0.0366 
0 . 0 4 4 7 
0.0753 
0 . 0 4 0 4 
0.0540 
0.2483 
0 . J 4 9 9 
0.0598 
0 . 0 ° 3 2 
0.0589 
0.0609 
0.0878 
0.1416 
0 . 2 4 9 4 
0.4769 
0.0252 
0.0117 
0.2960 
0.0304 
0.0486 

S i e g f r i e d 
H e i l e r 
ABLE 2. 4 : D E F I C I E N C I E S 
ANO BANKS OF : 
GAS 
D E S I G N I 
D E S I 
G N 2 
D E S I G N 
3 
D E S I G N * 
OIST 
I I 
DE F 
| RANK 
I I 
DEF 
I RANK 
I I 
DEF 
I RANK 
I I 
DEF 
I RANK I I 
N 
I 10.19d0 
I 1 2 . 5 
110.1955 
I 14 
1 1 0 . 2 2 1 6 
I 12 
110.2150 
I 12 
I I 
T 
I I O . 0 7 3 0 
I 10 
I 1 0 . 0 7 1 9 I 
9 
1 1 0 . 1 0 6 6 
I 
9 
¡ 1 0 . 1 1 1 5 
I 
9 
I I 
LN 
110.1844 
I 13 
I I 0 . 2 C 1 2 
I 14 
1 1 0 . 2 1 1 6 
I 12 
110.2365 
I 12 
I I 
CHI 
1 1 0 . 4 3 9 8 I 12 
1 1 0 . 6 4 8 8 
I 12 
110.5415 
I 11 
110.5100 
I 10 
I I 
N5N1 
110.1282 
I 11 
110.1439 
I 13 
110.1567 
I 11 
110.1437 
I 
9 
I I 
N10N1 110.0970 
I 
9 
110.1407 
I 13 
110.1106 
1 
9 
110.1136 
1 7 
I I 
N20N1 110.0574 
I 
6 
110.1029 
1 
7 
110.0797 
I 
5 
110.0900 
1 5 
I I 
N5C 
110.1832 
I 11 
1 1 0 . 1 7 8 9 
I 13 
110.1944 
I 11 
110.1752 
1 10 
|| 
N10C 
110.1654 
I 11 
110.1637 
I 13 
¡ ¡ 0 . 1 6 9 6 
I 10.5 
¡ 1 0 . 1 5 8 4 
I 
9 
I I 
N20C 
110.1209 
I 
9 
110.1277 
I 13 
1 ¡ 0 . 1 4 1 7 
Ì 
9 
110.1447 
I 
8 
I I 
N5N2 
110.1616 
I 
9 
1 1 0 . 1 9 2 6 I 13 
110.2209 
I 10 
I ¡ 0 . 2 0 8 0 
1 9 
l i 
N10N2 
110.1767 
I 
6 
1 1 0 . 2 3 0 1 1 
8 
110.2543 
Ì 
6 
¡ 1 0 . 2 8 3 5 
I 
7 
M 
N20N2 110.1927 
1 
4 
l i 0 . 3 0 0 6 
Ì 
5 
110.4399 
Ì 
4 
110.4509 
1 6 
I I 
L 
110.0883 
I 12 
I 1 J . C 7 9 1 I 14 
110.1259 
I 11 
110.1242 
I 
9 
I I 
DE 
110.0293 
I 
2 
1 1 0 . 0 3 2 0 
I 
2 
110.0482 
I 
4 
110.0033 
1 2 
I I 
C 
110.2862 
I 
4 
1 1 0 . 1 5 9 5 
I 
3 
1 1 0 . 3 3 0 1 I 
6 
1 1 0 . 1 9 3 1 
1 2 
I I 
AN 
110.0931 
I 12 
I l O . 1 0 C 6 
I 14 
||0.0860 
Ì 10 
110.0612 
1 9 
I I 
ACN 
110.0279 
I 
4 
110.0535 
I 
5 
110.0512 
I 
6 
1 1 0 . 0 5 4 1 1 6 
I I 

Robust Estimates in Linear Regression 
135 
References 
Adichie, J.N. (1967), Estimates of regression coefficients based on rank 
tests. Ann. Math. Statist., 38_, 894 - 904. 
Andrews, D.F. (1974), A robust method for multiple linear regression. 
Technometrics, 16, 52 3 - 5 31. 
Andrews, D.F. et al. (1972), Robust estimates of location: Survey and ad-
vances. Princeton University Press, Princeton, N.J. 
Barnett, V-, Lewis, T. (1978), Outliers in Statistical Data. Wiley, New York. 
Bickel, P.J. (1973), On some analogues to linear combination of order 
statistics in the linear model. Ann. Statist., J_, 597 - 616. 
Brown, G.W., Mood, A.M. (1951), On median tests for linear hypotheses. 
Proc. 2nd Berkeley Symp., 159 - 166. 
Carroll, R.J. (1978), An investigation into the effects of asymmetry on 
robust estimates of regression. Inst. Statist. Mimeo Ser. No 1172, 
Univ. of North Carolina. 
Dutter, R. (1980), Robuste Regression. Bericht Nr. 135, Institut für 
Statistik, Technische Universität Graz. 
Filipiak, B. (1980), Untersuchung der Verteilung einiger M-Schätzer im 
linearen Modell bei endlichen Stichproben mit Simulationen. Diplom-
arbeit, Dortmund. 
Gastwirth, J. (1966), On robust procedures. J. Am. Stat. Assoc. 61, 
929 - 948. 
Hampel, F. (1968), Contributions to the theory of robust estimation. Ph. 
D. Thesis, Berkeley. 
Hampel, F. (1971), A general qualitative definition of robustness. Ann. Math. 
Statist. 42, 1887 - 1896. 
Heiler, S-, Willers, R. (1979), On the asymptotic distribution of R-estimates 
in linear regression. Forschungsbericht 79/6, Abteilung Statistik, 
Universität Dortmund. 
Heiler, S. (1980), Robuste Schätzung im linearen Modell. In: Nowak, H. and 
Zentgraf, R.: Robuste Verfahren. 
Hodges, J.L., Lehmann, E.L. (1963), Estimates of location based on rank 
tests. Ann. Math. Statist., 34, 598 - 611. 
Huber, P.J. (1964), Robust estimation of a location parameter. Ann. Math. 
Statist., 35_, 73 - 101. 
Huber, P.J. (1973), Robust regression: Asymptotics, conjectures and Monte 
Carlo. Ann. Stat., l_, 799 - 821. 
Jaeckel, L.A. (1972), Estimating regression coefficients by minimizing the 
dispersion of the residuals. Ann. Math. Statist., 43_, 1449 - 1458. 
Jureckovä, J. (1971), Nonparametric estimate of regression coefficients. 
Ann. Math. Statistit, 42_, 1328 - 1338. 
Koenker, R., Bassett, G. (1978) Regression Quantiles. Econometrica, 46, 
33 - 50. 
Knötsch, R. (1980), Untersuchung der Verteilung einiger M-Schätzer im line-
aren Modell bei verschiedener Wahl der Skalenparameter und Vergleich 
mit R-Schätzern mit Simulationen. Diplomarbeit, Dortmund. 
Launer, R.L., Wilkinson, G.N. (ed.) .(1979), Robustness in Statistics. 
Academic Press, New York, N.Y. 
Lecher, K. (1980), Untersuchung von R-Schätzern im linearen Modell mit 
Simulationen. Diplomarbeit, Dortmund. 

136 
Siegfried Heiler 
Mood, A.M. (1950), Introduction to the theory of statistics. 
McGraw-Hill, 
New York, N.Y. 
Novak, H., Zentgraf, R., (ed.) (1980), Robuste Verfahren, Medizinische 
Informatik und Statistik No. 20, Springer, Berlin. 
Ruppert, D., Carroll, R.J. (1978), Robust regression by trimmed least-
squares estimation. Inst. Statist. Mimeo Ser. No. 1186, Univ. 
North Carolina. 
Ruppert, D., Carroll, R.J. (1979), Trimming the least-squares estimator 
in the linear model by using a preliminary estimator. Inst. Statist. 
Mimeo Ser. No. 1220, Univ. North Carolina. 
Theil, H. (1950), A rank-invariant method of linear and polynomial regression 
analysis. Proc. Kon. Ned. Akad. v. Wetensch. (A), 53_, 386 - 392, 
521 - 525, 1379 - 1412. 
Wahrendorf, J. (1980), Robuste Statistik: Eine einführende Übersicht. In: 
Nowak, H., Zentgraf, R. (ed.). Robuste Verfahren. 
Wold, A. (1940), Fitting of straight lines if both variables are subject 
to error. Ann. Math. Statist. 11, 284 - 300. 

Nichtlineare Regression: Parameterschätzung 
in linearisierbaren Regressionsmodellen 
Max-Detlev Jöhnk 
1. EINFÜHRUNG UND ÜBERSICHT 
Unter den nichtlinearen Regressionsmodellen spielen die linearisierbaren 
eine hervorragende Rolle. Dieser Aufsatz will versuchen, einen kleinen 
Beitrag zur Parameterschätzung im linearisierbaren Regressionsmodell zu 
leisten. 
Der nach dieser Einführung folgende zweite Abschnitt soll hauptsächlich 
den Begriff des linearisierbaren Regressionsmodells präzisieren. Im drit-
ten Abschnitt werden die wichtigsten verteilungsfreien Verfahren der Para-
meterschätzung im nichtlinearen Regressionsmodell jeweils mit dem Spezial-
fall des linearisierbaren Modells dargestellt. Diese Verfahren beruhen 
letztlich auf Iterationen. Ihre Brauchbarkeit hängt daher von einer guten 
Näherung als Anfangswert für den Iterationsprozeß ab. Im vierten Abschnitt 
wird ein Verfahren vorgeschlagen, die Parameter des Regressionsmodells 
näherungsweise zu schätzen. Der Schätzer kann, wenn die Näherung für nicht 
ausreichend gehalten wird,als Startwert für eines der im dritten Abschnitt 
dargestellten Iterationsverfahren verwendet werden. Der abschließende 
fünfte Abschnitt zeigt die Wirkungsweise der vorgeschlagenen Schätzver-
fahren am Beispiel einer Exponentialfunktion als Regressionsmodell. 
2. LINEARISIERBARE REGRESSIONSMODELLE 
Als Regressionsmodell soll im folgenden ein Modell verstanden werden, das 
die von zufälligen Störungen überlagerte Abhängigkeit einer Größe y von 
k erklärenden Größen x , ... , x 
darstellt. Das Modell wird vervoll-

138 
Max-Detlev Jöhnk 
ständigt durch Parameter 
••• , ß^ , die nicht bekannt sind und aus 
den Beobachtungen 
••• > 
' t = 1, ... , n geschätzt wer-
den müssen. Das korrespondierende störungsfreie Modell werde als Regres-
sionsfunktion bezeichnet. 
Eine nichtlineare Regressionsfunktion soll als linearisierbar bezeichnet 
werden, wenn es gelingt, sie durch eine passend gewählte Transformation 
der abhängigen Variablen linear in den Modellparametern 
6 , ... , ß 
1 
P 
zu machen. In diesem Fall ist p = k 
. Dabei kommt es darauf an, daß die 
Transformation vollständig bekannt und frei von unbekannten Größen ist. 
Für die Anwendbarkeit der noch darzustellenden Schätzverfahren muß zudem 
die Transformationsfunktion wenigstens zweimal differenzierbar sein. 
Die gängigen Regressionsfunktionen dieses Typs erfüllen diese Forderung. 
A. Haid (1, S. 56o) weist darauf hin, daß allein durch die Transforma-
tionen 
1/y 
, exp (y) und In (y) in Verbindung mit entsprechenden 
nichtlinearen Transformationen der erklärenden Variablen eine den prakti-
schen Ansprüchen meist genügende Vielfalt von Regressionsfunktionen dar-
stellbar ist. Ergänzend sind noch Transformationen der Art y° (a ^ 1) 
denkbar, von denen 
1/y ein Spezialfall ist; allerdings muß dann a vor-
gegeben sein. 
Bezeichne x' den aus den Variablen x,, ... , x, 
- im Fall einer inho-
1 
k 
mogenen Regression einschließlich der Scheinvariablen - gebildeten Zeilen-
vektor und 
ß den Spaltenvektor der Modellparameter, dann kann eine 
linearisierbare Regressionsfunktion als 
y = g(x'ß) 
(1) 
oder 
h(y) = x'ß 
(la) 
geschrieben werden; dabei ist h(.) die bereits erwähnte Transformation 
und g(.) die zugehörige Umkehrtransformation. 

Nichtlineare Regression 
139 
Der durch (1) bzw. (la) beschriebene Zusammenhang kann in der Regel nur 
in gestörter Form beobachtet werden. Für die Störung werden meistens un-
berücksichtigt gebliebene Einflußgrößen und/oder Beobachtungsfehler ver-
antwortlich gemacht. Dementsprechend können drei Modelle für den Ansatz-
punkt von Störeinflüssen unterschieden werden: 
Modell 1: Modell der externen Störung 
yfc = gCx^ß + ufc) 
(2) 
h(y ) = x^ß + ufc 
(2a) 
Modell 2: Modell der internen Störung 
y t = g(x^B) + v t 
h(yt - v t) 
x^ß 
(3) 
(3a) 
Modell 3: Modell der doppelten Störung 
Y t = gtx^ß + ufc) + 
h(yt - v t) 
V 
+ Ut 
(4) 
(4a) 
Die Modellbeziehungen gelten jeweils für t = 1, ... , n 
. Darin bezeich-
nen u 
und 
die den Beobachtungen 
(y^_, x^) zugeordneten Stör-
größen. 
Da die Modellbeziehungen (2) - (4) sowohl für die Modellvariablen als 
auch für die Beobachtungen gelten, soll im folgenden in der Symbolik 
nicht zwischen beiden unterschieden werden. Nötigenfalls wird darauf hin-
gewiesen, ob die Variable oder Beobachtung gemeint ist. Die Gefahr von 
Verwechslungen oder Mißverständnissen ist infolgedessen nicht gegeben. 

140 
Max-Detlev Jöhnk 
Bezüglich der Störvariablen 
und v 
sollen die üblichen Annahmen 
gelten. Sei also 
/ U 1 \ 
\UnJ 
und v 
/V1 \ 
Vnl 
dann wird 
E (u) = 0 
E (v) = 0 
E(uu') = a2 I 
u 
= a2 
u 
E(w') = a 2 I 
v 
E(uv1 ) = 0 
(5a) 
(5b) 
(5c) 
(5d) 
(5e) 
vorausgesetzt. 
Im Modell 1 sei y = E g(x^ + u^) und v = 
- y^ , womit die Dar-
stellung des Modells 2 erreicht ist. Die Unterscheidung der Modelle 1, 2 
und 3 gewinnt ihren Sinn daher vor allem dadurch, daß auf der Grundlage 
homoskedastischer Störvariabler eine breite Palette von Varianzmodellen 
dargestellt werden kann. 
In der nachfolgenden Darstellung werden häufig der Einfachheit halber die 
Beobachtungen y 
bzw. x^ zusammengefaßt als 
y = 
\ y n 
und X /*; \ 
W i 

Nichtlineare Regression 
141 
notiert. Ferner soll 
/h( Y l)\ 
h(y) = 
: 
\
h V 
sein. Entsprechend sind andere Notationen zu verstehen, in denen ein Vek-
tor als Argument einer Funktion auf IR verwendet wird, wie z.B. g(Xß) 
oder h'(y) 
3. 
ALLGEMEINE SCHÄTZVERFAHREN BEI ADDITIVEM STÖRGLIED 
Das linearisierbare Regressionsmodell kann in der Version 2 als Spezial-
fall des allgemeinen nichtlinearen Modells 
y
t
 = f ( xit 
x k f 
ßi 
y
+
v t 
( 6 ) 
= f(xt, ß) + v t 
aufgefaßt werden, worin v 
den Bedingungen (5b) und (5d) genügt. In 
diesem Modell liegt es nahe, den Parametervektor 
ß nach der Methode der 
kleinsten Quadrate zu schätzen. Dazu gibt es im wesentlichen zwei Vor-
gehensweisen. 
3.1. Direktes Verfahren 
Bei der direkten Minimierung wird das Prinzip der kleinsten Quadrate un-
mittelbar angewandt. Das führt zur Minimierung des Ausdrucks 
S 2 = Et (yfc - f(xt, ß))2 
(7) 
bezüglich 
ß . Dazu kann entweder das Minimum von (7) unmittelbar gesucht 
werden oder es wird mit 

142 
Max-Detlev Jöhnk 
V v 
ß0) := ä|-f(xt' ß) iß=ß° 
das nichtlineare Gleichungssystem 
Efc (yt - f(xfc, ß)) • fj(xt, ß) = 0 
(8) 
für j = 1, ... , p gelöst. Hinweise auf geeignete Minimierungstechniken 
geben Goldfeld und Quandt (2, Chpt 1). Im Spezialfall des linearisierba— 
ren Modells wird (8) zu 
Efc (yt - g(x^ß)) • g'(x^ß) • x 
= 0 
(9) 
für j = 1, ... , Je . 
3.2. Indirektes Verfahren 
* 
Die indirekte Minimierung geht von einer Näherung 
ß 
aus. Mittels der 
Taylor - Entwicklung 
f (x . ß) = f(x . ß*) + E. f. (x , ß*) • (ß. - ß*) + R 
t 
t 
3 
3 t 
3
3 
mit dem Restglied R ergibt sich näherungsweise die lineare Darstellung 
Yi_ - f(x . ß*) - E . f.(x . 8*) • (ß. - ß*) + v 
(10) 
t 
t 
3 ] t 
D 
D 
t 
die im linearisierbaren Fall als 
y - g(x'ß*) = E. g'(x'ß*)x 
• (ß - ß*) + v 
(11) 
t 
t 
3
t 
3 t 
J 
J 
geschrieben werden kann. Sei ß*° eine Näherung und Z die aus den Ele-
menten 
z . = f.(x . ß*°) 
3 t 
3 
t 

Nichtlineare Regression 
143 
bzw. im linearisierbaren Fall 
z. = g*(x' ß*°) x 
:t 
* t 
Dt 
gebildete Matrix, d a m ist mit 
yt(ß*°) = f(xt, 6*°) 
bzw. 
yfc(ß*°) = g(x;ß*°) 
durch 
ß*1 = ß*° + (Z'Z)"1 Z' (y - y(ß*)) 
(12) 
eine neue Schätzung gefunden, die sich nach dem gleichen Prinzip beliebig 
weiter verbessern läßt. Nähere Hinweise zu dem Verfahren gibt J.M. Cham-
bers (3, S. 5). 
4. 
APPROXIMATIVE PARAMETERSCHÄTZUNG IN LINEARISIERBAREN REGRESSIONS-
MODELLEN 
4.1. Modell 1 
Bei linearisierbaren Regressionsfunktionen besteht das gebräuchliche Ver-
fahren darin, das Modell durch die Transformation h(.) zu linearisieren 
und die Modellparameter 
ß aus dem linearisierten Modell nach der Metho-
de der kleinsten Quadrate zu schätzen. Sei h(y) gemäß Abschnitt 2 der 
aus den Elementen 
h(yt> 
gebildete Spaltenvektor, dann lautet demzufolge 
der Schätzer 
ß = (X'X)"1 X'h(y) 
. 
(13) 
Diese Vorgehensweise ist dann optimal, wenn die transformierte Zufalls-
größe h(y) homoskedastisch ist. Diese Bedingung trifft zu beim Modell 1; 
für dieses Modell ist das Verfahren der Variablentransformation sinnvoll 
und angemessen. Zudem schätzt in diesem Modell 
ß den Parametervektor 

144 
Max-Detlev Jöhnk 
ß unverzerrt; denn wegen 
E(h(yt) ) = x^ß + E(ufc) = x^ß 
ist 
E(ß) = ß . 
4.2. Modell 2 
Im Modell 2 sind die Zufallsvariablen y 
selbst homoskedastisch. Infol-
gedessen führt die Variablentransformation y 
h (y ) zu heteroske-
dastischen Zufallsvariablen. Damit ist der gewöhnliche Kleinstquadrate -
Schätzer nicht mehr optimal. Es ist jedoch möglich, den Einfluß der Varia-
blentransformation auf die Varianz näherungsweise zu bestimmen und zu 
neutralisieren. Dazu werde die Taylorentwicklung von h(y^ - v ) um y^ 
betrachtet und nach dem linearen Glied abgebrochen, wonach 
h(yfc - vt) = h(yt) 
v
t
h'(yt> + R 
entsteht. Daraus folgt wegen h(y - v ) = x'ß 
h (y ) = x^ß + v • h' (yt) . 
Demzufolge kann mit 
0 
0 
\ 
o . 
• o 
D 
0 
'0 
= {diag (h1 (y)) } 
- 2 

Nichtlineare Regression 
145 
der Parametervektor 
ß durch 
8 = (X'DX)-1 X'Dh(y) 
(14) 
geschätzt werden. Dieser Schätzer ist nicht mehr unverzerrt. Zur nähe-
rungsweisen Bestimmung der Verzerrung werde die Taylorentwicklung von 
h 
um y = g(x^8) betrachtet. Aus 
h(yfc) = h(yt) + vfc h' (yt> + | v^ h " (yt) + R(vt) 
ergibt sich wegen hfy^) = 
E(h(y )) = x'ß + \ a 2 h' • (yJ + E (R (v )) 
t 
t 
1 v 
t 
t 
Unter der Voraussetzung, daß E(R(v )) vernachlässigt werden kann, ist 
~ 
- 2 
mit D = {diag (h'(y))} 
E (ß) ~ ß + - CT2 (X'DX) 
X1 Dh' ' (y) 
. 
(15) 
Eine Schätzung für a 2 
läßt sich mit y = g(Xß) aus den Residuen 
y t - y t 
als 
a 2 = —
z 
(y. - y. ) 2 
v 
n - k 
t 
t 
gewinnen, während h'(y) und h 1 1(y) durch h'(y) bzw. h1'(y) ge-
schätzt werden können. Damit läßt sich erforderlichenfalls die Verzerrung 
von ß korrigieren. 
4.3. Modell 3 
Soll im gemischten Modell ähnlich vorgegangen werden, wie im Modell 2, 
dann muß wenigstens das Verhältnis der Varianzen a 2 und a 2 
bekannt 
u
v 
sein. Wird nämlich wieder die Taylorentwicklung von 
- v ) um y 
betrachtet und nach dem linearen Glied abgebrochen, dann entsteht 

146 
Max-Detlev Jöhnk 
h(yt) = x^ß + u t + vt-h" (y ) + R(ut, 
. 
Sei 
= u t + 
'^t' + R^ ut' vt' 
kombinierte Störgröße, dann gilt 
bei vernachlässigbarem Restglied 
'(z) = a 2 + o 2 | h" (y ) 
t 
u 
v '- 
t 
a z ( 
v 
— + l > ( y t ) ] 2 ) 
• 
Wäre das Verhältnis 
y? = o 2/a 2 
u v 
bekannt, dann könnte wie beim Modell 2 mit 
,2-1 
/(k2+|_h- (y^]2)" 
o . 
\ 
(k^+|_h- (y n)] 2) - 1/ 
diag { |_k2 + (h1 (y) ) 2 ] 1 } 
der Parametervektor 
ß durch 
= (X'DX)"1 X'Dh(y) 
(16) 
geschätzt werden. Wie beim Modell 2 ist es möglich, die Verzerrung von 
mit Hilfe der Taylorentwicklung um y = g(x^ß) näherungsweise zu be-
stimmen. Es ist 
h(y ) = x^ß + u + vth' (y ) 
+ j {u2 [g'ix^ß)]2 + 2utvt g'(x^ß) + v 2} h1 ' (yfc) + 

Nichtlineare Regression 
147 
und daher 
Sei 
!(h(v ) ) = x'ß + ^ {o2 
g1 (x'ß)]2 + a2 } h " (y ) 
t 
t 
2 
u 
t -1 
v 
t 
G = diag (g' (Xß)) 
(17) 
H = diag (h*(y)) = G _ 1 
(18) 
D = (k2I + H2)"1 
(19) 
dann ist infolgedessen 
E(ß) « ß + i- (X'DX)-1 X'D (CT2 G 2 + o2I) h"(y) 
2 
u
v 
= ß + x CT2 (X'DX)"1 X'G2h"(y) 
(20) 
2 v 
2 
9 
Dazu muß jedoch neben k 
auch a^ bekannt sein oder aus den Daten ge-
schätzt werden können. 
5. VERGLEICH DER SCHÄTZVERFAHREN AM BEISPIEL DER EXPONENTIALFUNKTION 
Der in ökonometrischen Modellen am häufigsten anzutreffende Fall eines 
linearisierbaren Regressionsmodell ist wohl ohne Zweifel die Exponential-
funktion, die als Wachstumsmodell dient oder mit logarithmierten erklä-
renden Variablen als Modell von Nachfrage- und Produktionsfunktionen. Aus 
diesem Grunde wurde dieses Regressionsmodell für einige erste Proberech-
nungen verwendet. 
a+ßx 
In der einfachsten Form lautet die Regressionsfunktion y = e 
. Für 
das Modell 2 können die Parameter verhältnismäßig einfach nach der direk-
ten Methode der kleinsten Quadrate geschätzt werden. Der Schätzer ß er-
gibt sich als Lösung von 

148 
Max-Detlev Jöhnk 
ßxfc 
2ßxt 
ßxt 
2ßxt 
E 
e 
• E 
e 
= E x^y^ e 
• E e 
und a kann danach aus 
ßx. 
a 
2ßx 
E y e 
/ E e 
ermittelt werden. Nach der im Abschnitt 4.2 dargestellten approximativen 
Methode lauten die Schätzer wegen h(y) = ln(y) und h'(y) = 1/y 
M 
fr 2 
n 
E y 
\ / 
E Xt Yt 
2 \ 
v 
2 
_ 2 2 
2 Xt Yt 
1 Xt Yt 
-1 
Z Yt l n Yt 
E x ty t ln y t 
Die Verzerrung kann wegen h1'(y) = -1/y 
durch 
'y 2 
v 
2 
Z Xt Yt 
2 
„ 2 2 
Z Xt Yt 
E Xt Yt, 
E x. 
geschätzt werden. Um verschiedene Schätzverfahren miteinander vergleichen 
zu können, wurden fünf Datenreihen mit x = 0 (1) 10 , a = 2 , 
6 = 0.2 und a^ = 25 erzeugt. Aus den Datenreihen wurden jeweils vier 
Schätzungen für o und ß durchgeführt und Zwar: 
NLKQ 
Nichtlineare Methode der kleinsten Quadrate (Abschnitt 3.1) 
ELRL 
Einfache lineare Regression mit logarithmierten Werten 
ARLl 
Approximativ gewichtete lineare Regression mit logarithmier-
ARL2 
ten Werten, mit und ohne Korrektur 
Danach ergaben sich folgende Schätzwerte für ß 

Nichtlineare Regression 
149 
Datenreihe 
1 
2 
3 
4 
5 
NLKQ 
0. 159 
0. 197 
0 20 7 
0 253 
0 205 
ELRL 
0. 118 
0. 331 
O 204 
0 195 
0 167 
ARLl 
0. 146 
0. 162 
0. 195 
0 227 
0 172 
ARL2 
0. 153 
0. 181 
0 199 
0 240 
0 186 
Diese Übersicht zeigt, daß die durch die approximativ gewichtete lineare 
Regression gefundenen Schätzwerte im Durchschnitt erheblich näher an den 
Schätzwerten liegen, die durch nichtlineare Regression gefunden werden 
und daher z.B. als Startwerte für eine iterative Lösung eher infrage kom-
men, als die Schätzwerte, die durch einfache lineare Regression der loga-
rithmierten Werte gefunden werden. 
Literatur 
1. Haid A. , Statistical Theory and Engineering Applications, New York 
1952. 
2. Goldfeld S.M. , Quandt R.E. , Nonlinear Methods in Econometrics, 
Amsterdam 1972. 
3. Chambers J.M. , Fitting nonlinear models: numerical techniques, 
Biometrika 6o, 1-13 (1973). 


A Test for Independence of Dichotomous 
Stochastic Variables Distributed 
Over a Regular Two-Dimensional Lattice 
Peter Kuhbier and Joachim Schmidt 
1. INTRODUCTION 
In economics and other social sciences "it is often necessary to consider 
the geographical distribution of some quality or phenomenon in the 
counties or states of a country, and one of the questions we may ask is 
whether the presence of some quality in a county makes its presence in 
neighbouring counties more or less likely" (Cliff and Ord 1973, p.l). 
There are at least two basic ways of answering such a question: 
a) The observed quality in each county is considered to be the realization 
of a corresponding stochastic variable, and, provided a suitable test 
statistic is at hand, a test is carried out determining whether these 
stochastic variables are uncorrelated with (or independent of) those 
in neighbouring counties. 
b) Presumed functional relationships between observations in neighbouring 
counties are formulated in terms of a model, and again, provided there 
is a suitable test statistic, a test of whether the model gives a 
satisfactory description of the data is carried out. 
Depending on the aim of the investigation under consideration, possibility 
a) and/or b) will be chosen. In this article, we shall confine our 
analysis to the problem of choosing a test statistic for spatial correla-
tion. The reason there are difficulties in defining a suitable test 
statistic is that any such statistic, first of all, crucially depends on 
the definition of which counties are neighbours and which are not, and, 
secondly, on the type of data (qualitative or quantitative) that is 
available. In order to facilitate the analysis, we make two simplifying 
assumptions: 

152 
Peter Kuhbier and Joachim Schmidt 
ASSUMPTION 1 
The counties under consideration form a regular lattice of squares (or rec-
tangles) in the plane. 
ASSUMPTION 2 
The quality or phenomenon under consideration is dichotomous. Its two 
possible realizations are denoted by "0" and "1", respectively. 
Given assumption 1 there are three straightforward definitions of neigh-
bourhood: Two rectangles are neighbours if they have (i) a common border 
(rook's case), (ii) a common vertex, but not a common border (bishop's 
case), (iii) a common vertex and/or a common border (queen's case). Fur-
thermore, given assumption 2 two simple join count statistics (BB and BW) 
have been proposed which can serve as test statistics for spatial indepen-
dence in all three cases (see for instance Cliff and Ord (1973), p. 4, and 
Moran (1948) p. 243): 
BB = i E w. .x,x . 
(1) 
2 i,j 
1 3 1 D 
and 
BW = -¡It I w. . (x. - x.)2 , 
(2) 
2 i,j 
1 3 
1 
3 
where i,j = 1,...,N label the regions under consideration, x^ is the 
realization of X. in region i (x. € {0,1}), and w. . is the weight of 
i
i
i
] 
contingency between regions i and j (while Moran (1948) only deals 
with w. . 6 {0,1}, Cliff and Ord (1973) allow arbitrary weights w. . € R ). 
+ 
If the hypothesis is valid that, with probability p^ und p^, respectively 
all realizations "0" or "1" occur independently of any realizations in 
neighbouring regions where neighbourhood is defined by means of the weights 
w^^, then, asymptotically, both test statistics are normally distributed 
and the usual tests of hypothesis can be carried out. However, these 
statistics have certain disadvantages, since even in the simple case of a 
chessboard-like pattern of observations three different results are ob-
tained (positive, negative, or no correlation) depending on which of the 
above defined types of neighbourhood is employed. 

Test for Independence of Dichotomous Variables 
153 
The most general approach to a description of spatially distributed 
phenomena in terms of their joint or conditional distribution is found in 
an article written by Besag (1974). There it is shown that, provided 
certain additional conditions hold (Besag 1974, pp. 195 and 197), the 
joint distribution of the variables at all sites P(x,,...,x ) or the 
I
N 
conditional distribution of variable X^ given the values of the varia-
bles at all other sites P(x I all other sites) can, in the discrete case, 
l 
be derived from the following function 
N 
Q(x,,...,x ) = I x.G. (x, ) + I 
I x.x.G. . (x.,x.) 
1 
N 
1=1 
1 1 
1 
l<i<j<N 
1 3 ^ 
1 
3 
+ - 
+
X r - V i 
N
( x i 
V 
• 
Here, all functions G. . 
, (x , x .,..., x, ) may, subject to the above-
,...,k i j 
k 
mentioned conditions, be chosen arbitrarily (Besag 1974, pp.197-98). Fur-
thermore, according to an unpublished theorem by Clifford and Hammersley, 
any function G. 
(. ) is non-null iff the sites 
(i,j,...,k) form a 
i ,. . . , k 
clique, i.e. iff all sites are neighbours of each other. Hence, various 
models can be derived depending on which sites are defined to be neigh-
bours and how the function G. . 
, (.) are chosen (cf. Besag (1974) 
,. .. ,k 
for details). 
In this article, we choose an approach somewhat different from Besag's 
and, hence, shall not deal with various models or distributions but rather 
concentrate on the investigation of one special test statistic for spatial 
independence between first and/or second order neighbours (first order 
neighbourhood = rook's case, second order neighbourhood = bishop's case 
or queen's case), provided the distribution of the phenomenon under con-
sideration is spatially stationary (for details cf.Cliff and Ord (1975), 
p. 299). 
For the purpose of practical applications of the proposed test statistic, 
the development of modern computer technology, here as well as in other 
statistical fields, is a necessary premise. For without high speed compu-
ters it would not be possible to calculate the numerous numerical 
characteristics from the observations within reasonable time limits. 

154 
Peter Kuhbier and Joachim Schmidt 
2. THE JOINT DISTRIBUTION OF X„,...,X, ON THE ASSUMPTION OF SPATIAL 
1 
N 
STATIONARITY AND A SECOND ORDER NEIGHBOURHOOD SCHEME 
Assume that there is a total of N sites forming a rectangular lattice 
of m rows and n columns. Two regions may be neighbours if they have 
a common border or a common vertex. In region i (i 6 {1,...,N}) we observe 
realizations x. of a stochastic variable X^ which is supposed to be 
dichotomous (x^ £ {0,1}). Let us denote the joint probability of observ-
ing 
(X =x )a. . .A(X =x^) by 
p 
:= P((X=x1)A...A(X = x j ) , 
X . . . . x 
1
1 
N 
N 
1 
N 
then the hypothesis that all X^ (i=l,...,N) are independently distribu-
ted can be written as 
V . . X 
= P x . " " P x 
for all X 
x £{0,1} 
- 
(3) 
1 
N 
1 
N 
If, on the other hand, the X^ are not independently distributed but 
spatially correlated, then an appropriate formulation of a corresponding 
alternative hypothesis is needed which we shall turn to now. Because of 
the following multiplication formula for probabilities 
ptBj n ... n bn> = pcbj) •p(b 2Ib 1) •••p(b nIb n_ 1 n ... n b ) 
we may write the joint probability p 
as 
X 1 " ' X N 
p 
= p 
• p 
| 
• • • p 
, 
(4) 
X 1 - - - X N 
X 1 
2 x i 
n I x N - T • - x i 
with p 
, 
being defined as 
x i | x i - r • -xi 
p 
, 
:= P (X. =x. I (X. =x. 
) A ... A (X =x ) ) . 
X. X 
X, 
1 
1 
1 - 1 
l - l 
1 
1 
1 
1 - 1 
1 
From (4) we see that possible dependencies of X^ on neighbouring varia-
bles have to be taken into account to the extent that only neighbours with 
indices smaller than i are involved. All sites with indices smaller than 
i will be called predecessors of site i. 

Test for Independence of Dichotamous Variables 
155 
If we label the regions in such a way that those in the first column get 
indices 1 to m starting in the first row, while those in the second 
column get indices m+1 
to 2m, and so forth, then, presuming a second 
order neighbourhood scheme, conditioning events in (4) have to be consi-
dered according to the following rule only: 
R : 
l 
ROLE 1 
Assuming a second order neighbourhood scheme, P x | x 
x
 
c a n 
i i-1 
1 
substituted by P(X.=x.l A 
(X. =x. 
)) where R, is defined as follows 
1 
i c „ 
1 - r 
1 - r 
i 
r £ R. 
l 
{1} 
if i e I := {2,,..,m} 
{m-1,m} 
if i £ 12' = 
2m+l,...,(n-l)m+l} 
{1, m, m+1} 
if i £ I 3:= {2M, 3M, ..., N} 
{1, m-1, m, m+1} 
if i £ I : = {2,...,N} - (I1UL2UL3) 
i.e. : 
1) Variables X^ corresponding to regions in the first column 
(i € 1^) 
have just one neighbour among their predecessors, namely the immediate 
predecessor. There is no predecessor of Xj. 
2) Variables X^ corresponding to regions in the first row 
(i £ I^) have 
exactly two neighbours among their predecessors, namely the neighbour 
to the left and the neighbour to the left in the second row. 
3) Variables X^ corresponding to regions in the last row 
(i £ 
have 
exactly three neighbours among their predecessors, namely the immediate 
predecessor and the neighbours to the left in the same and the 
preceding row. 
4) All other variables X^ (i £ 1^) have among their predecessors four 
neighbours, namely the immediate predecessor and the neighbours to the 
left in the same, the preceding, and the following row. 
Figure 1 gives a graphical representation of rule 1. 
Application of rule 1 leads to the following formulation of the joint 
probability distribution of X.,...,X 
1 
N 

1 5 6 
Peter Kuhbier and Joachim Schmidt 
p 
TT 
p 
I 
X 
n 
p 
I 
^x. . . .x„ 
. _ 
rx. x. . 
. _T 
x. x. 
±.,x. 
1 
N 
1 
1 - 1 
2 
1 
1 _ m + 1 
1 - m 
n p | 
. — 
X . X . . » X, 
/X. 
. 
i£l 
i' l-l 1-m i-m-1 
n p I 
iei„ Xi|Xi-l'Xi-m+l'Xi-m'Xi-m-l 
4 
Here, it has been assumed that x, is fixed, i.e. that p = 1. 
1 
% 
1 i i i i ( > ^p i 12 1 1 ' i < 
I 
h 
l i i l i ^\ i t i I i i 
i • 1 I • • l I i t 1 I 
• i % t i i I i I t i < 
I • I 1 I i I i I I I < 
l ) t | | l l 
| | | 1 t 
l i 1 1 I 
^ l i • 1 I I 
I 
h 
1 * 1 1 1 < ID 1 I3 1 1 1 ' ' 
Fig.1.: Sets of regions and their prodecessors 
indicated by arrows 
On the additional premise of spatial stationarity, • p 
can be 
Xl"''XN 
simplified once more according to the following rule: 
RULE 2 
If the distribution of X ,...,X 
is spatially stationary then the con-
ditional probabilities in (5) do not depend on the index i but only on 
the spatial constellation of the neighbouring events and, hence, by 
defining 
s:=x. 
.; t:=x. 
; U:=x. 
.; v:=x. 
w:=x. 
i-m-1 
l-m 
i-m+1 
l-l 
i 
we may write 

Test for Independence of Dichotomous Variables 
157 
1) p 
instead of p 
i 
if 
1 6 I, 
vw 
x. x. , 
1 
i l-l 
2) p 
instead of p 
i 
if i £ I_ 
tuw 
x. x. 
,(x. 
2 
i i-m+1 l-m 
3) p 
instead of p 
i 
if i £ I, 
stvw 
x. x. .,x. 
,x. 
, 
3 
i' i-I l-m i-m-1 
4) p 
instead of p 
i 
if i € I, 
stuvw 
x . x . . , x . 
. ,x. 
,x. 
. 
4 
i 
i-I 
i-m+1 
l-m 
i-m-1 
Application of rule 2 finally leads to the following formulation of the 
joint distribution of Xj,...,X 
n 
tt , •, vw 
, 
. tuw 
p 
= n 
(p ) 
x 
n 
(p 
) 
x 
X....X 
vw 
tuw 
1 
N 
v,w 
t,u,w 
n 
( P ^ ) n s t v w x 
stvw 
s,t,v,w 
n 
"stuvw 
stuvw 
(6) 
s,t,u(v,w 
s,t,u,v,w £ {0,1} 
where n 
is defined as the number of cases with X. = v and 
vw 
1-1 
X. = w (i £ I.), n 
, etc. 
are defined analogously (cf. rule 2). Corre-
l 
1 
tuw 
sponding to equation (6), equation (3) may also be simplified if spatial 
stationarity is assumed. Hence, we obtain instead of (3) 
no "l 
P x 2 ' " P X n = P 0 ^ l 
; 
n 0 + n i = N - l 
(7) 
3. TEST STATISTICS FOR SPATIAL INDEPENDENCE IN THE CASE OF SPATIAL 
STATIONARITY AND A SECOND ORDER NEIGHBOURHOOD SCHEME 
We are now prepared to formulate an alternative hypothesis to the null 
hypothesis that all X^ (i=l,...,N) are independently distributed. Using 
the denotation of equations (6) and (7), we obtain: 

158 
Peter Kuhbier and Joachim Schmidt 
NULL HYPOTHESIS 
On the assumption of spatial stationarity and a second order neighbourhood 
scheme, the hypothesis that all X. (i=l,...,N) are independently distri-
buted is equivalent to 
P
J
.
=
P
J
_
=
P
^
=
P
=
P 
(8) 
stuvw 
stvw 
tuw 
vw 
w 
for all s, t, u, v, and w € {0,1}. 
ALTERNATIVE HYPOTHESIS 
The alternative hypothesis to be tested against the null hypothesis is 
that, in (8), at least one equation does not hold. 
Next, we look for a test statistic that may be used to test the null hypo-
thesis against the alternative hypothesis. At first, however, we need 
estimates for the unknown probabilities p^, P v w< etc. The logarithms of 
the right hand sides of equations (6) and (7) can be interpreted as 
likelihood functions for the unknown parameters. Hence, observing the 
necessary side conditions 
E p 
=:p = 1 ; 
p 
= 1 ; 
p 
= 1 ; 
p 
= 1 ; 
p 
= 1 ; 
w 
. 
v. 
^tu. 
rstv. 
rstuv. 
w 
s , t , u , v , w e {0,1} 
for the conditional probabilities, we get the following maximum-likelihood 
estimators 
P,, = 
P, 
n 
n ^ 
n ^ 
vw 
. 
tuw 
. 
stvw 
. 
stuvw 
"w 
n 
vw 
n 
tuw 
n^ 
stvw 
n t 
stuvw 
n 
v. 
tu. 
stv. 
stuv. 
s,t,u,v,w £ {0,1} 
In connection with the investigation of Markov chains, Anderson and 
Goodman (1957) have shown that the likelihood-ratio between the right hand 
sides of equations (6) and (7) can be used as a test criterion for testing 
the null hypothesis against the alternative hypothesis. If the null 
hypothesis is true, then the following statistics asymptotically have 
x 2 -
distributions: 

Test for Independence of Dichotomous Variables 
159 
A 2 * 
S,= 
E n ( p 
- p ) / p 
(1 degree of freedom) 
1 
v. rvw 
rw 
^w 
V / W 
a 
A 2 ^ 
= 
E n 
(p 
-p ) /p 
(3 degrees of freedom) 
2 
t 
tu. tuw 
w 
w 
t,u,w 
* 
2 
S, = 
E 
n 
(p 
- p ) /p 
(7 degrees of freedom) 
3 
stv. ^stvw 
rw 
w 
s,t,v,w 
* 
y. 2 
S,= 
E 
n 
(p 
- p ) / p 
(15 degrees of freedom) 
4 
L 
stuv. stuvw 
w 
w 
s,t,u,v,w 
Since all test statistics are estimated from different sets of 
observations (S ^ from observations x^ with i £ 1^, S^ from observations 
x^ with i 6 I2, etc.), these statistics are independent from one another 
and, hence, their sum could also be used. 
4. EXAMPLES 
In order to illustrate the usefulness of the proposed statistic, we pre-
sent a few examples. We start with two artificial examples and then 
proceed to an example from urban economics. 
EXAMPLE 1 
Suppose the spatial interdependence of the phenomena under consideration 
is such that realizations basically result in a pattern of zeros and ones 
as follows 
1 
1 
1 
1 
1 
1 
0 
1 
0 
1 
0 
1 
o 
o 
0 
0 
0 
0 
1 
0 
1 
0 
1 
0 
1 
1 
1 
1 
1 
1 

160 
Peter Kuhbier and Joachim Schmidt 
or in a slight modification thereof where such modifications may be 
obtained by allowing that ones are replaced by zeros, and vice versa 
(with a small probability though). Then, even in the "pure" pattern of 
zeros and ones shown above, none of the join count statistics BB or 
BW will lead to significant results, i.e., they will not be of any help 
in discovering the underlying structure of dependencies, either in the 
rook's case or in the bishop's case or in the queen's case. If S^, 
however, is estimated from that pattern (or a slightly disturbed one), it 
will lead to a highly significant result provided there are enough 
observations. This is due to the fact that the following constellations 
of neighbours (stuv) 
1 0 0 1 
0 0 1 0 
1 1 0 0 
0 1 1 1 
occur only (or with high probability) if w = 1, and the constellations 
0 1 1 0 
1 1 0 1 
0 0 1 1 
1 0 0 0 
occur only (or with high probability) if w = 0. The remaining constella-
tions do not occur (or only with low probability). It can easily be 
verified that in this case, S^ approximately equals R, the number of 
members in 
Therefore, S^ is always significant if R is large 
enough. 
EXAMPLE 2 
Suppose a pattern of zeros and ones as follows 
1 
1 
0 
0 
1 
1 
0 
0 
1 
1 
0 
0 
1 
1 
0 
0 
0 
0 
1 
1 
0 
0 
1 
1 
0 
0 
1 
1 
0 
0 
1 
1 
1 
1 
0 
0 
1 
1 
0 
0 
or a slight modification thereof has been observed. Again, none of the 
join count statistics leads to significant results while the statistic S 

Test for Independence of Dichotamous Variables 
161 
approximately equals R, and, hence, becomes sufficiently large if R is 
large. 
In empirical applications, a sample taken at regularly distributed points 
in space may lead to a pattern similar to the one seen above if the 
underlying structure consists of clusters of ones and zeros and if samples 
can only be taken at sites which are located rather sparsely with respect 
to the given structure. Figure 2 may serve as an illustration. 
The pattern obviously shows positive spatial correlation. But if observa-
tions are only available 
at the indicated sites, then the resulting 
pattern is basically like the one shown above, and only S^ would dis-
cover the spatial interdependence of the quality under consideration while 
the join count statistics would not (provided, however, in an empirical 
application, the area is larger than in figure 2, since more observations 
are necessary to yield significant results) . 
Fig.2.: A (blown up) chessboard-like pattern of observations 
taken at points distributed regularly but sparsely 
with respect to the underlying structure 

162 
Peter Kuhbier and Joachim Schmidt 
EXAMPLE 3 
Figure 3 shows Berlin (West). Empty cells ("0") indicate areas where the 
ratio between the number of pupils attending a middle school and the 
number of pupils attending a secondary school is greater than 0,68 while 
cells containing a star 
("1") indicate that the corresponding ratio is 
less than 0.68. Where there are no cells at all, either no observations 
are available or there are no inhabitants. It is quite obvious that, 
because of the existence of large clusters, there is positive spatial 
correlation between observations in neighbouring areas. This is also 
confirmed by the highly significant values of the join count statistics 
as well as of the statistic S^. If, however, only every fourth observa-
tion were considered (such as in figure 2) and if the corresponding cells 
were blown up by a factor of 2 in each direction, we would obtain a 
pattern of observations which is shown in figure 4, and now the resulting 
values of our various test statistic are such as presented in table 1. 
Only two of the nine possible values of the counting statistics are 
significant. Hence, these statics hardly point to the underlying structure 
of spatial dependencies, while S 
at least displays a significant value. 
Rook's 
Bishop's 
Queen's 
Critical 
Statistic 
case 
case 
case 
Distribution values 
BB-E(BB) 
W(BB) 
1,49 
0,35 
0, 
,97 
asympt. 
normal 
1,96 
BW-E (BW) 
W (BW) 
-4,38 
-1,66 
-4, 
,30 
asympt. 
normal 
1,96 
WW-E (WW) 
W(WW) 
1,61 
0,84 
1, 
,29 
asympt. 
normal 
1,96 
S 4 
28, 
,76 
asympt. 
X 2 
25 
Table 1.: Values of various test statistics in the case of the 
pattern of observations shown in figure 4.po=p1=0,5; a=0,05. 
Source: Statistisches Landesamt Berlin: 
Ergebnisse der Volks-, Berufs- und Arbeitsstättenzählung in 
Berlin(West) am 27. Mai 1970. I, No. 218. Aggregated data. 

Test for Independence of Dichotomous Variables 
163 
Fig.3.: Areas in Berlin (West) where the ratio between the number of 
pupils attending a middle school and the number of pupils 
attending a secondary school is greater than 0,68 (empty cells) 
or less than 0.68 (cells with stars). 
Fig.4.: Subset of the set of observations in fig. 3.Every fourth 
cell has been taken. 

164 
Peter Kuhbier and Joachim Schmidt 
The three preceding examples show: 
a) The join counts (BB, BW, and WW) will not be able to discover even 
obvious second order relationships between neighbours if, irrespectively 
of what is observed at a site itself, in neighbouring sites the average 
number of observed zeros equals the average number of observed ones, 
while S^ in general will give significant results (cf. examples 1 and 
2). This is due to the fact that S^ not only takes into account the 
number of zeros and ones but also their specific constellations. 
b) If the observed realizations result in comparatively large clusters of 
zeros and ones then S 
as well as the join counts will display 
significant results (cf. example 3, figure 3) . 
c) In situations where it is less easy to infer the underlying structure 
from the observations, S^ seems to be more reliable than the counting 
statistics. 
5. GENERALIZATIONS, CRITICAL REMARKS, AND QUESTIONS 
Generalizations with respect to the number of states and the number of ob-
servations at each site pose no problem. Suppose that there are k states 
instead of two 
(k>2) , then the number of degrees of freedom of S_. is 
(k3 - 1) (k-1) for j = 1,2,3,4 (cf. Anderson and Goodman 1957, p. 102). 
Nothing else changes. If, on the other hand, more than one observation at 
each site is available, then n 
equals the sum En 
(T), where n (T) 
vw 
T vw 
vw 
is the number of observations 
{(X, (T)=W) A (X. 
(T)=V); i £ I,} at time 
th 
1 
T 
or in the 
T 
sample, n^ 
, n ^ 
, and n ^ 
are defined analogous-
tuw 
stvw 
stuvw 
ly. In this case, i.e. if there is more than one set of observations, one 
can also obtain ML-estimates p 
(T) , P. 
(T) , etc. separately for each 
vw 
tuw 
r 
sample and thus may test the hypothesis of spatial independence of 
X (T) , . . . 
in each sample as well as the hypothesis of identical 
distributions in all samples (cf. Anderson and Goodman 1957 for details 
in case of a Markov chain model). 
There is one important difference, however, between spatial models and a 
Markov chain model: in the Markov chain model, the variables X. form a 
l 
sequence in a natural and given way while there is no a priori knowledge 
of how to assign indices to regions in the spatial model. Hence, it is to 

Test for Independence of Dichotomous Variables 
165 
be expected that, depending on the way regions have been provided with 
indices, tests of hypothesis will lead to different results for different 
assignments of indices. However, since the proposed test statistics take 
into account the full information on spatial dependencies up to the second 
order, divergent results of comparable statistics should be due to random 
variations only and not to systematic components. In this context, statis-
tics are defined as comparable if the same number of neighbours in fixed 
constellations is taken into account. 
One could even consider assigning indices to regions randomly. In this 
case, the number of constellations of neighbours with respect to a given 
region increases drastically: There are eight different positions if there 
g 
is one neighbour among all predecessors; there are 
positions if there 
are two neighbours among all predecessors; etc. Therefore, since all these 
different., constellations have to be considered as different cases, a 
random assignment of indices to regions leads, in general, to statistics 
with too many degrees of freedom, so that enormous amounts of data are 
required to yield significant results, even if there is not doubt that the 
quality under consideration is spatially correlated. 
References 
Anderson, T.W., Goodman, L.A. (1957), Statistical Inference about Markov 
Chains, The Annals of Mathematical Statistics, 28^, 89-110. 
Besag, J. (1974), Spatial Interaction and the Statistical Analysis of 
Lattice Systems, Journal of the Royal Statistical Society (B), 36, 
192-225. 
Cliff, A.D., Ord, J.K. (1973), Spatial Autocorrelation, Pion Ltd., London. 
Cliff, A.D., Ord, J.K. (1975), Model Building and the Analysis of Spatial 
Pattern in Human Geography, Journal of the Royal Statistical Society 
(B), 37, 297-328. 
Moran, P.A.P. (1948), The Interpretation of Statistical Maps, Journal of 
the Royal Statistical Society (B), 10, 245-251. 
Statistisches Landesamt Berlin: Ergebnisse der Volks-, Berufs- und Arbeits-
stättenzählung in Berlin(West) am 27. Mai 1970. I, No. 218. 


Computational Experiences with an Algorithm 
for the Automatic Transfer Function Modelling 
Hans-Joachim Lenz 
1. INTRODUCTION 
Consider the following model relating the stationary input ("j^k-i 
N 
to the stationary output (yv). « 
which is stationarily coupled with 
.K 
1 f • • • iN 
yk = M l f 
uk-b + ek 
(1) 
where 
]r 
A (L) = 1 - a. L 
- a L 
and 
r 
1 
r 
B (L) = b,- b,L - ... - b LS . 
s 
1 
1 
s 
The function T(L) = B(L)Lb/A(L) is called a rational transfer function. 
^ is assumed to be an auto-correlated noise process of the 
ARMA type, i.e. 
D (L) 
" F O T 
Ek ' 
(2) 
P 
where 
C (L) = 1 - c L - ... - c LP , 
P 
1 
P 
D (L) = 1 - d.L - . . . - d L q . 
q 
i 
q 
(e^) is assumed to be a white noise process, i.e. E(ej.) = 
= a\' 
COV(e , e, ) = O for all k | h . 
k 
k 
For notational convenience we shall drop the arguments and the subscripts 
in the ploynomials A, B, C and D. Given a bivariate time-series 
(u, , y, ). . 
of length N and admitting only models of the type 
k 
k k—11 • *. jN 
defined in (1) and (2) the problem is to find automatically an appropriate 

168 
Hans-Joachim Lenz 
specification (r,£,s) x (p,q) and the corresponding estimates 
^ A A A A 2 
(A,B,C,D,cr ) . Of course, selecting a model only by looking at the data 
makes it necessary to test against model inadequacies. The iteration be-
tween specification, estimation and residual analysis was studied sytema-
tically by Box/Jenkins (1970). However, they have had a manual approach 
in mind. 
Fig. 1: The gross structure of the automa-
tic specification and estimation 
We shall not be concerned here with the estimation of A,B,C,D and a^ 
for given orders (r,b,s) x (p,q) . This is described in some detail by 
Box/Jenkins (1970). We are interested more in the specification itself 
and are looking for an algorithm to select the orders (r,b,s,) x (p,q) 
which fit "well enough" to the data. The attribute "well enough" means 
that the selected orders must fulfill certain model adequacy tests. 
2. TECHNIQUES OF ORDER SELECTION 
There are many ways how to select the orders of the involved polynomials. 
One may use for instance 
(i) 
Pattern Recognition Techniques 
or 

Algorithm for the Automatic Transfer Function Modelling 
169 
(ii) Combinatorial Search Techniques. 
Using the first techniques it is necessary to decompose the procedure of 
order selection at least into two sub-procedures. This is due to the fact 
that the selection of the orders (p,q) must be based on the noise process 
(e ), 
which is not observable, however. Using cross-correlation 
K K — 1 , • * • , N 
techniques the first procedure has to detect patterns in the estimated 
cross-correlation function CFF between the prewhitened input (u^) and 
output (y^) cf. Box/Jenkins (1970). Furthermore the detected patterns 
must be linked tentatively to the orders (r,b,s) . When the orders are 
selected, the parameters in the polynomials A and B can be estimated. 
Then the residuals (e, ) can be reconstructed. 
Fig. 2: Decomposition of the 
order selection procedure 
The second sub-procedure of the order selection method then starts. Using 
auto-correlation techniques the procedure has to detect patterns in the 
estimated auto-correlation function ACF of the tentatively specified and 
estimated noise process (ek). As described by Box/Jenkins (1970) these 
patterns are linked to the orders (p,q) . For fixed orders the unknown 
parameters in the polymonials C,D can be estimated and the corresponding 
residuals e, of the noise model estimated. 
However, notice that the specification and the estimation were processed 
sequentially instead of simultaneously. This will cause usually a bias. 

170 
Hans-Joachim Lenz 
This bias may be reduced by iterating between the transfer modelling and 
the noise modelling with a final step for simultaneous estimation of all 
parameters on each iteration step. This is sketched in Fig. 3, where 
(u*) means the prewhitened input series (u^) . 
( START 
1 ) 
Tentatively transfer 
function modelling 
Tentatively 
noise 
modelling 
Simultaneous 
estimation of all 
the parameters 
(e*> 
( e k ) 
white noise/ 
« \ 
-k 
V 
white noisq 
•5 
c 
(A,B) 
(r ,b, s) 
(A,B,C,D) 
. (r,b,s) 
(p»q) 
ÌL. 
j TOP 3 
Fig. 3: An order selection 
technique based on 
pattern recognition 

Algorithm for the Automatic Transfer Function Modelling 
171 
The main difficulties with the order selection technique by pattern re-
cognition are 
the detection of patterns in the ACF and CCF, which are not 
free of estimation errors, 
the lack of powerful white noise tests, 
the direction of re-specification in the case of any backtrack. 
Alternatively, the order selection could be performed by making use of a 
combinatorial search technique as follows. It is reasonable to restrict 
the orders of the polynomials and the delay-time: 
0 <_ r, s £ 2 
0 i P'9 i ^ 
and 
0 < b £ 10 . 
It makes sense to use Akaike's (1972) information criterion 
AiC = -2 log (maximum likelihood) + 2 (number of free 
(3) 
parameters) 
as an objective function for the order search. Then the following search 
procedure might be thought of: 
Fig. 4: An order selection technique 
based on a combinatorial search 

172 
Hans-Joachim Lenz 
Unfortunately, the number of combinations sums to 2475. Of course, this 
is prohibitive with respect to any available CPU-time if one cries to 
enumerate all the cases. However, even a limited enumeration approach 
involves some pitfals, mainly because of 
the initial point for the search, 
the direction of search to minimize the AIC and 
the kind of updating of the selected orders in the case of 
detected model inadequacies. 
3. A LIMITED ENUMERATION ALGORITHM 
Keeping in mind the pros and cons of both techniques discussed in the 
preceding chapter the following two-pass algorithm is an almalgamate of 
the pattern and the combinatorial search method. The orders (r,b,s) of 
the transfer function T(L) are selected by detecting patterns in the 
CCF while the orders (p,q) of the noise model are determined by an 
AlC-guided search procedure. For more details we refer to Kitagawa (1977) 
and Lenz (1980). The problem of a respecification in the case of detected 
model inadequacies is treated by allowing for a very limited set of orders 
to be identified instead of just a single item only. These cadidate lists 
are filled up in a different way. In the case of selecting the orders 
with respects to the polynomials of the transfer function and the delay 
time the significance bounds are slightly varied to obtain up to 3 diffe-
rent patterns of the CCF and corresponding orders. Contrarily, the prin-
ciple of the second best solution is used for selecting alternative (p,q) 
orders. Therefore the orders with local minimum AIC are stored in in-
creasing order in a list. They are retrieved from the top of this list 
if necessary. Notice that to each tripel (r,b,s) retrieved there exists 
a candidate list of (p,q) orders for the noise model. 
The full set-up is illustrated in Fig. 5 as a flow diagram and explained 
in detail within the algorithm SPEZI. 

Algorithm for the Automatic Transfer Function Modelling 
173 

174 
Hans-Joachim Lenz 
: Combined_search_for_orders_of the_transfer model and 
noise model 
The algorithm SPEZI selects the orders (r,b,s) and 
(p,q) of the polynomials A (L) , B(L)Lb, C(L) and D (L) 
in the model 
= - 
+ 
-
yk 
A 
Uk-b 
C ®k 
Input: 
(y. ,u. ) 
a bivariate stationary time-
JC 
IT JI— 1 / . * . F N 
series with stationarily coupled components 
Output: (r,b,s) x(p,q) and A,B,C,D 
Used subalgorithm: TRANSF, ARMA 
SPEZIl: (Order_search_for transfer_function models) 
Fill the list CLT with tentatively specified transfer function 
models B(L)/A(L) • L b by calling TRANSF for different values 
of X and let 1 1 and j 1 . 
IF 1 > 3 THEN STOP ELSE choose the l'th entry of CLT, i.e. 
(r,b,s) . Compute 
= y^ - (B/A) 
. 
(Order_search_f or_noise_models) 
CALL ARMA for given 
(e^) . 
SPEZI4: (Retrieve j^th best ARMA_model) 
IF j > 3 THEN STOP ELSE retrieve 
(p,q) e CLN as the j'th 
entry of CLN. 
SPEZI5: (Check residuals) 
Compute Q = (N-p-max{p,Q}) I ACF (T) where ACF is the estimated 
auto-correlation function T * of (e^) = ([c/D]ek) . 
IF Q > X? 
THEN j-«-j+l and GOTO SPEZI4. 
* 
1 -a; K-p-q 
J 
J 
SPEZI6: (Estimate parameter _smultaneously) 
Compute all the parameter estimates in the polynomials A,B,C,D 
for given order (r,b,s) x (p,q) . 

Algorithm for the Automatic Transfer Function Modelling 
175 
SPEZI7^ 
(Comgute_residuals) 
Compute E, = — [y. - — u. 
for all k and estimate the auto-
K 
D 
k 
A 
k - b 
correlation function 
ACF . 
SPEZI8: 
(Check_residuals) 
_ 
^ ^ 
K 
IF Q = (N-p-max{p,q}) £ ACF (x) > x? 
„ ~ ~ 
T = 1 
i-a; K-p-q 
THEN j<-0 , (ek) •*- (y - (B/Ä) uk_g) and GOTO SPEZI3 
SPEZI9: (Check transfer function) 
K 
Compute S = (N-p -max{p ,q }) £ CCF (x) where CCF is the 
estimated cross-correlation funlS ion between the residuals e, 
k 
and the suitably prewhitened input (u^). 
IF S > Y? 
THEN 1 + 1 + 1 
and GOTO SPEZI2 ELSE, STOP. 
1-a; k-r-s 
The following two algorithms outline the order search procedure for a 
transfer function model and for a set of corresponding ARMA models ordered 
by increasing AIC values. 
2E§fE_£2E_£i2SSfer_function 
The algorithm TRANSF selects the orders r,b,s of the 
transfer function T(L) = B(L)/A(L)L^ in the model 
y k = T(L) u^ + e^ 
where 
A(L) = 1 - a.L - ... - a Lr 
1 
r 
B(L) = b - b.L - . .. - b L S 
o 
1 
s 
Input : 
Estimated cross-correlation dunction CCF be-
tween suitable prewhitened input 
JJ 
and output ( y ^ ) j 
N ; the largest lag 
x 
of CCF ; the factor X for X-a-rule 
o 
Output: Tentatively specified orders (r,b,s) of T(L) 
TRANSFl: 
(Test_for_non-zero_CCF) 
IF there exists a b = min {x| |CCF(X) | > X/V'N-X' , 0 <_ x <_ Xq} 
THEN 
TRANSF3 , ELSE -V TRANSF2 . 

176 
H a n s - J o a c h i m 
L e n z 
TRANSF2: 
T R A N S F 3 : 
( D e c r e a s e 
s i g n i f i c a n c e _ b o u n d s ) 
S e t 
X 
X - 
1/vft1 . 
I F 
A <_ 1 
STOP, ELSE 
TRANSF1 
( i ' S ® i _ i 2 i _ § 5 t r e m e _ v a l u e s _ o f _ C C F ) 
C C F ( x ) | > X/'/N-t" 
Compute 
X 
such t h a t 
C C F ( t 
) = max{ 
C C F ( T ) 
e x 
e x 
f o r a l l 
0 < t < t 
} 
and 
— 
— 
o 
t . 
. = m a x i x l | c c f ( t ) I > X / / n - t ' , t 
< t 
< t 
} 
. 
l a s t 
^ 
1 
e x — 
— 
o 
I F 
T 
> b + 2 
THEN 
X 
X + 1/v'n 
and 
TRANSFl 
. 
— 
e x 
TRANSF4£ 
( T 5 S £ _ f o r 
i r r e g u l a r j a a t t e r n ) 
I F 
T, 
. - b < 3 
THEN 
( r , b , s ) -<- ( 0 , b , T , 
- b ) 
and 
— 
l a s t 
— 
l a s t 
STOP ELSE 
TRANSF5 
TRANSF5: 
{ S e l e c t 
s ) 
Compute 
s 
t 
- 
b 
TRANSF6£ 
(Segarate_CCF^_s 5 l t e r n a t i n g _ i n 
s i g n ) 
I F 
(NPOS > 0 A NNEG > 0) 
THEN 
r -<- 2 
and STOP where 
NPOS = 
| { t | c C F ( t ) 
> X / i f i - Y 
, b < T < T }| and 
~ 
— 
— 
° 
NNEG = 
| { t | c C F ( t ) 
< X / ' / n - t ' r b 
<_ t 
<_ 
T Q } | 
TRANSF7j_ 
( T e s t _ f o r _ d e l a Y _ i n _ C C F ) 
I F 
I C C F 
I 
> 
I C C F 
, [ f o r 
some 
x > b + s 
THEN 
( r , b , s ) 
- < - ( l , b , s ) 
— 
i 
T i 
i 
i 
_ 
and STOP ELSE 
( r , b , s ) f - ( 2 , b , s ) 
and STOP 
We i l l u s t r a t e 
t h e b e h a v i o u r o f 
t h e a l g o r i t h m TRANSF u s i n g 
t h e 
f o l l o w i n g 
c r o s s - c o r r e l a t i o n f u n c t i o n a s an 
e x a m p l e . 
C C F ( t ) 
.51 
.25 
- . 0 0 
-.25 
-X=2 
— 
— 
-X=1.9 
.t 
+. 
. . . 
. . . — . . . 
-*-ìl=I o 
> • 
+ 
• 
• 
+ A—1 .8 
t S I 
H
I 
+ • • 
• 
• 
+ 
« 
+ 
F i g . 
6 : 
An e s t i m a t e d c r o s s - c o r r e l a t i o n 
f u n c t i o n 
t o g e t h e r w i t h d i f f e r e n t 
s i g n i f i c a n c e 
bounds 
( X = L . 9 + 0 . 1 ) 

Algorithm for the Automatic Transfer Function Modelling 
177 
Running the algorithm TRANSF with these data then will output the following 
list CLT of tentatively spezified transfer function model orders 
A 
r 
b 
s 
2 
0 
6 
0 
1.9 
0 
5 
1 
1.8 
0 
c 
2 
Tab. 1: Candidate list for the orders (r,b,s) 
Algorithm ARMA:_ Order search_for_ARIlA_models 
The algorithm ARMA selects the orders p,q of the 
polynomials C(L) and (D(L) in the ARMA model 
= § o f 
ek 
w h e r e 
D(L) = 1 - d,L - . . . - d L q 
and 
q D 
C(L) = 1 - c.L - ... - c L P 
1 
P 
Input: 
'ek^k=l 
N a stat:"-onary time-series; 
p 
, q 
being upperbounds of p,q 
ITlclX 
itlSX 
Output: p,q , AIC(p,q) where AIC(p,q) = N • log a^ + 2 (p+q) 
ARMAl^ (Find_best £Ure_AR_model) 
Compute p + such that AIC(p+,0) = min AIC(p,0) 
for all O < p < p 
. Store (p+,0) in the list CLN. 
— 
— max 
ARMA2: (Diagonal search) 
Search along the diagonal p + q = p + for orders (p',q') 
such that 0 
q' <_ q ^ ^ and 
AIC (p',q') <_AIC(p,p -p) for all |p - p' | <_ e . 
Store (p',q') in the list DL. 
ARMA3: (Exglore the neighbourhood) 
IF DL = 0 THEN GOTO ARMA5 ELSE compute (p",q") for each 
(p',q') € DL where AIC(p",q") = min AIC(p,q) 
over {(p,q) HiF'-q') - (p,q) I I £ 2 } . 

178 
Hans-Joachim Lenz 
ARMA4: (Test for local minimal order) 
Eliminate (p',q') from DL. 
IF AIC(p",q") < AIC(p1,q') THEN store (p",q") in DL 
ELSE store (p',q') in CLN . GOTO ARMA3 . 
ARMA5:. (§ort_CLN) 
Sort all the (p,q) e CLN in ascending order STOP . 
The algorithm ARMA produced the following typical trace for a time-series 
hazardly picked out from a large data set used for testing purposes. 
,777.62 
1 
774.75 
1 
776.58 
I 
778.04 
J 
778.28 
1772.75<-—744.17 
I 
915.73 881.53 ¡805.84 480.40 
P--.. 1 A 
I 
872.33*-1137.43—V771.40 1013^92 
I'*-. 
1321.41 945.45 901.12 
775.19 
I 
773.58 
Fig. 
873.71 1321.25 
742.29-» 771.90 
771.02 1104.47 
7: A typical search path of ARMA 
(percentage of evaluated orders about 50%) 
The list CLN of candidates will include for example 
(p,q) 
AIC (p,q) 
(1,6) 
480 
(5,1) 
742 
(0,6) 
744 
(7,5) 
771 
(5,2) 
772 
Tab. 2: Candidate list CLN for selected 
noise model orders 

Algorithm for the Automatic Transfer Function Modelling 
179 
4. NUMERICAL RESULTS 
The details of the experimental design for the Monte-Carlo studies are 
fully explained in Lenz (1980). We consider a generator for stochastic 
processes producing stationary time-series which are stationarily cross-
correlated. Three shapes of the impulse response function, i.e. the 
•weighing coefficients of the transfer function T(L), are considered: 
X: 
Negatively skewed impulse response function (IRF) 
400-
300 
200. 
100 
ill 
II: 
Symmetrical impulse response function (IRF) 
400 
300 
200 
100 
X I I I I I I 
0 
1
2
3
4
5 
6
7
8
9 
10 
III: Positively skewed impulse response function (IRF) 
400-
300. 
200-
100. 
I I I • • -
0
1
2
3
4
5
6
7
8
9 
10 

180 
Hans-Joachim Lenz 
The length of the time series generated is set equal to 100. The 
following results are obtained: 
Case I: Negatively skewed IRF 
S \ s 
r 
O 
1 
2 
0 
0,5,0 
0,5,0 
0,7,0 
0,8,0 
0,1,1 
0,5,1 
0,7,1 
0,7,1 
1 
2 
2,0,0 
2,0,0 
2,1,0 
2,6,1 
Tab. 3: The finally accepted orders (r,b,s) 
\ 
q 
P 
0 
1 
2 
3 
0 
6 
2 
1 
1 
1 
2 
Tab. 3': The number of finally accepted orders (p,q) 
Performance_statistics: 
a) Mean number NT of evaluated transfer function models = 2 
b) Mean number NN of evaluated noise models =2.4 
c) Odds of the well accepted model to the doubtful ones: 7 : 5 
d) Means of the standard deviations : 68 

Algorithm for the Automatic Transfer Function Modelling 
181 
Case II: 
Symmetrical IRF 
0 
1 
2 
0 
0,1,0 
0,4,0 
0,6,0 
0,6,0 
0,5,1 
0,5,1 
0,5,1 
0,5,1 
0,6,1 
0,1,2 
0,5,2 
1 
2 
. 
2,5,0 
2,7,0 
Fig. 4: 
The finally accepted orders (r,b,s) 
v 
q 
P 
0 
1 
2 
3 
4 
0 
9 
1 
1 
1 
2 
1 
1 
Fig. 4': 
The number of finally accepted orders (p,q) 
a) 
NT = 1.8 
b) 
NN = 2.9 
c) 
9 : 4 
d) 
s = 62 

182 
Hans-Joachim Lenz 
Case III: Positively skewed IRF 
s 
r 
0 
1 
2 
0 
0,1,0 
0,4 ,0 
0,4,0 
0,5,0 
0,6,0 
0,3,1 
0,4,1 
0,5,1 
0,1,2 
0,4,2 
0,4,2 
1 
2 
2,4,0 
2,4,0 
2,4,0 
2,3,1 
X P 
P 
0 
1 
2 
3 
4 
0 
5 
3 
1 
1 
1 
2 
1 
2 
3 
1 
4 
1 
Tab. 5" : The number of 
nally accepted 
orders (p,q) 
Tab. 5: The finally^accepted 
orders (r,b,s) 
a) NT = 1.7 
b) NN = 1.9 
C) 10:5 
d) s = 62 
There is seme evidence from the above figures that the automatic spezifi-
cation of transfer function models with negatively skewed impulse response 
function causes the same trouble as is true in the case of running the 
specification procedure manually. 

Algorithm for the Automatic Transfer Function Modelling 
183 
References 
Box, G.E.P. and Jenkins, G.M.: Time Series Analysis Forecasting and 
Control, Holden Day, San Francisco etc., 1970 
Akaike, H.: Use of an Information Theorie Quantity for Statistical Model 
Identification, Proc. 5th Hawaii Inst. Conf. on System Sciences, 
pp 249-250, North Hollywood, 1972 
Kitagawa, G.: On a Search Procedure for the Optimal AR-Ma Order, 
Sm. Inst. Stat. Math., 29 (1977), 319-332 
Lenz, H-J.: A Comperative study on the Performance of two Forecasting 
Techniques Based Either on Distributed-lag Models or on Payoff 
Distribution, Problems of Time Series Analysis ed. by H. König, 
Bibl. Institut, Mannheim, 1980, 68-78 


Vergleiche zwischen empirischen und theoretischen 
Kenngrößen von ARMA-Modellen im Zeitbereich -
Eine zusätzliche Möglichkeit der Modell-Validierung 
Walter Mohr 
1. EINLEITUNG UND ÜBERBLICK 
Das Hauptproblem bei der Identifikation von ARMA-Modellen besteht darin, 
solche Modelle auszuwählen, deren theoretische Kenngrößenstruktur z.B. für 
die Autokorrelation und die Partielle Autokorrelation möglichst gut mit 
der entsprechenden empirischen übereinstimmt, die man aus dem Datenmate-
rial schätzt. 
Daher ist es sinnvoll, nach der Schätzung dieser Modelle die zugehörigen 
Kenngrößen direkt zu vergleichen.*' Meist steht die Überprüfung der Modell-
annahmen, insbesondere die Analyse der Restgrößen im Vordergrund, weil 
2) 
diese (theoretisch) die einfache Rauschstruktur besitzen sollen. 
Die 
hier zu diskutierende Vorgehensweise des Vergleichs von Kenngrößen für die 
relevante Zeitreihe aus Daten und Modellen ist zwar aufwendiger, stellt 
jedoch eine wesentliche Bereicherung und Ergänzung der bisher gebräuchli-
chen Überprüfungstechniken dar. Die Untersuchung der Restgrößen läßt sich 
darin auch als Spezialfall einbetten. 
Der Aufbau dieser Arbeit ist so angelegt, daß zunächst die Beziehungen 
zwischen den Modellkenngrößen im Zeitbereich dargestellt werden. Diese 
Zusammenhänge werden z.T. ausgenutzt, um zu vorgegebenen Modellen die zu-
gehörigen theoretischen Kenngrößen zu ermitteln. Danach erfolgt die Be-
rechnung der empirischen Kenngrößen aus dem Datenmaterial. Ferner werden 
Vergleichsmöglichkeiten (Tests) zwischen theoretischen und empirischen 
Kenngrößen betrachtet. Abschließend wird die Nützlichkeit der hier vorge-
legten Modellvalidierung anhand eines Beispiels demonstriert. 

186 
Walter Mohr 
2. BEZIEHUNGEN ZWISCHEN DEN MODELLKENNGRÖSSEN IM ZEITBEREICH 
Schaubild 1: Zusammenhänge temporaler Kenngrößen bei ARMA-Modeller. 
Bezeichnungen: 
AKF 
Autokorrelationsfolge 
(Autokovarianzfolge) 
IAKF Inverse Autokorrelationsfolge 
(Inverse Autokovarianzfolge) 
PAKF Partielle Autokorrelationsfolge 
IPAKF Inverse Partielle Autokorrelationsfolge 
IAF 
Impuls-Antwort-Folge 
AIAF Approximative Impuls-Antwort-Folge 
IIAF Inverse Impuls-Antwort-Folge 
AIIAF Approximative Inverse-Impuls-Antwort-Folge 
Autoregressive (AR) Modell-Koeffizienten 
Moving-Average (MA) Modell-Koeffizienten 
V 
vkk 
^ k k 
k=. 
k=. 
k=. 
k=. 
k=0 
k=0 
k=0 
li, ,=e, . k=0 
r k j 
ki . „ 
J 3=0 
'kj 
k=0 
k=0 
j=0 
k=0 
k=0 
-1,0, 1, ... 
-1,0,1, ... 
-1,0,1,... 
-1,0,1,... 

Kenngrößen von ARMA-Modellen im Zeitbereich 
187 
Kenngrößen, die im obigen Schaubild symmetrisch zur Dualitätsachse 1 lie-
gen, sind zueinander dual, in dem Sinne, daß sie sich ergeben, wenn an-
stelle des ursprünglichen ARMA-ModeIis <J> (B) X^_=0 (B) 
die dazu duale Ver-
sion 0 (B) X^=<}> (B) U^ verwendet wird. 
Die IAF erhält man bei gegebenem Modell durch Koeffizientenvergleich aus 
der Beziehung: 
^ (B}=0 (B) -4>_1 (B) . 
(1) 
4) 
Es ergibt sich: 
min(m,k) 
Tfj, =0, - 2 
<l> .<t>. 
k= 1,2, . . . mit i|> =1 . 
(2) 
k k . , 
k-1 i 
o 
Die dazu duale IIAF bestimmt man aus der Gleichung 
Tr(B)=<J>(B)e_1 (B)=i|)_1 (B) . 
(3) 
Es gilt:5' 
min(n,k) 
ir, =<(> - Z 
TT, .0. 
k=l, 2, . . . mit ir =1 . 
(4) 
k k . , 
k-i l 
o 
Die Formulierung von (3) für die entsprechenden temporalen Folgen ergibt 
die mit 2 im Schaubild gekennzeichnete Faltung:^' 
1 t=0 , 
^t * ^ t ^ t 
m i t ^t = 
(5) 
0 t=0 . 
Daraus folgt: 
lll =- E Ii), .TT. bzw. TT, =- E TT. .ll) . . 
(6) 
k 
. , k-i i 
k 
. , k-ir i 
i=l 
i=l 
Durch die Bezeichnung 3 ist im Schaubild die Autoproduktdarstellung ge-
kennzeichnet. 

188 
Walter Mohr 
Für die Autokovarianzfolge erhält man: 
00 
Y =a 2 CiJj . ® 
Zip.ip.j_. . 
(.7) 
't u r t w Tt 
u . 
J t+i 
j=o 
Die AKF kann analog als normiertes Autoprodukt berechnet werden. 
Y t 
a2 
fjft+j 
P t = - 
= 
= ^ 
(8J 
° 
x 
I 
^ 
Die entsprechenden Beziehungen für die Inverse Autokovarianz- bzw. Inver-
se Autokorrelationsfolge ^ erhält man durch Dualisierung: 
pt = ?
(
, t
Ä V
= 7 . ^ Y t + j - 
(9) 
u j=o 
E TT . TT 
Pt 
1 
t + 3 
rt = - = ^ r 
v 
= ^ 
( 1 0 ) 
P 
CT 
P 
o 
U O 
V 
2 
E TT . 
J 
j=o 
Ferner besteht zwischen 
und 
bzw. 
und 
der mit 2 gekenn-
zeichnete Zusammenhang in Form einer Faltung: 
Y t « P t = « t bzw. 
p 
« r = - 1 - 6 
(11) 
o o 
Weil die IAF und IIAF einseitige Folgen darstellen, kann man über die 
Formeln (5) oder (6) die eine aus der anderen bestimmen. 
Diese direkte Berechnungsmöglichkeit ist über (11) zwischen AKF und IAKF 
nicht ohne weiteres durchführbar, weil diese Folgen zweiseitig sind. 
Zur Identifikation werden in der Literatur meist nur die AKF und PAKF her-
angezogen. 
Vom Aufbau im Schaubild 1 her ist jedoch die IAKF zumindest modelltheore-
tisch der PAKF vorzuziehen, weil letztere nur approximativ dual zur AKF 
ist. Dieser Sachverhalt ist in der obigen Skizze durch die Stellung der 
PAKF bzw. IPAKF zwischen den entsprechenden einseitigen und zweiseitigen 

Kenngrößen von ARMA-Mode1len im Zeitbereich 
189 
Folgen veranschaulicht worden. 
Die PAKF und die dazu duale IPAKF sind einseitige Folgen und lassen sich 
als spezielle Approximationen (durch Verbindung 5 gekennzeichnet) der 
IIAF bzw. IAF interpretieren. Dieser Sachverhalt ergibt sich aus der Ver-
knüpfung zwischen PAKF und AKF über die sog. Yule-Walker Gleichungen bzw. 
10) 
die Durbinschen Formeln 
(Verbindung 4). Ein dualer Zusammenhang läßt 
sich zwischen IPAKF und IAKF formulieren.^' Schließlich soll durch Ver-
bindung 6 aufgezeigt werden, daß zwischen charakteristischen Verläufen 
von IAKF und PAKF bzw. AKF und IPAKF starke Ähnlichkeiten vorkommen. 
Z.B. sind bei einem AR(m)-Modell für das erste Paar die Werte nach dem 
m-ten Lag alle gleich Null. Entsprechendes gilt bei einem MA(n)-Prozeß 
für das zweite Paar, wenn die Lags großer als n sind. 
Für k=l,2,... und j=l,2,...,k sei <ji 
die Kleinst-Quadrat-Approximation 
k} 
der IIAF (AIIAF) von der Ordnung k. Die Werte berechnet man aus den Yule-
Walker-Gleichungen: 
r 
i 
Pk-1 Pk-2 
Pk-l\ 
k-2 rw 
\ - < w 
/Pi\ 
K
/ 
(12) 
Löst man das lineare Gleichungssystem sukzessive für k=l,2,... , so er-
, die man als partielle Autokorrelationsfolge 
kk 
gibt sich die Folge 
bezeichnet. 
Sie ist gleich dem partiellen Regressionskoeffizienten, der zur Variablen 
mit dem höchsten Lag gehört, wenn ein AR(k)-Modell wie folgt angepaßt 
wird: 
( * k o ^ k l B + - " ^ k k B ' V U t 
m i t * k o = 1 
(13) 

190 
Walter Mohr 
1 2 ) 
Mittels der Formeln von Durbin 
kann man die AIIAF rekursiv berechnen: 
» y ^ - . ^ + k - U - j V 
^ 
^ 
( 1 4 ) 
k-1 
I k-1 
3=0 
Es gilt: '('j.^1 
f ü r k=0, 1, . 
Dabei ist folgende Reihenfolge einzuhalten: 
11' 22 21' 33 3 1 3 2 
" W k l 
V k - l ' 
Durch Dualisierung (man ersetze p, durch r, sowie <b, . durch 0, .) erhält 
k 
k 
k] 
k] 
man die entsprechenden Formeln für die AIAF 
und die IPAKF - Q ^ • 
Die PAKF und IPAKF sind zwar als Regressionskoeffizienten gut zu interpre-
tieren, aber sie passen von der Theorie her nicht recht in das obige Be-
ziehungsschema. Zwischen beiden Folgen besteht - soweit bisher bekannt -
kein strenger Zusammenhang in Form einer Faltung wie zwischen den anderen 
dualen Folgen. Wegen ihrer für die Erkennung von Mustern unübersichtli-
chen rekursiven Berechnung lassen sich die allgemeinen Strukturen der 
PAKF und IPAKF auch oft nicht in geschlossener Form (als Lösung entspre-
chender Differenzengleichungen) angeben wie etwa bei der IAKF oder der 
AKF. 
Bei einem MA(n)-Prozeß genügt z.B. die IAKF für t>n der Differenzenglei-
chung 
0(B)r =0 , während die Herleitung der PAKF schon für einen 
t 
14) 
MA(2)-Prozeß mit immensen Schwierigkeiten verbunden ist. 
Entsprechen-
des gilt für AR(2)-Modelle bei der IPAKF. 

Kenngrößen von ARMA-Modellen im Zeitbereich 
3. BERECHNUNG DER KENNGRÖSSEN EINES VORGEGEBENEN ARMA-MODELLS 
191 
3.1. Autokorrelationsfolge 
Wir gehen von einem ARMA(m,n) -Prozeß mit den üblichen Annahmen aus:^' 
t T1 t-1 
Tm t-m t 1 t-1 
m t-m 
Die Outputgröße 
ist schwach stationär mit Mittelwert 0, d.h. E ( x
t
x
t + k ' 
=Yxx(k). Die Rauschgröße U^ ist zusätzlich noch unkorreliert. Ferner 
sind die beiden Zufallsprozesse schwach stationär korreliert, d.h. 
(17) 
Zunächst wird die Gleichung (16) von links mit 
x
t _ k multipliziert und an-
schließend der Erwartungswert gebildet. Man erhält: 
(k-j) = E 0,Yvr,(k-j) 
mit $ = 0 = 1 -
J AA 
J AU 
O O 
]=oJ 
3)=o 
( 1 8 ) 
Multipliziert man beide Seiten von (16) von rechts mit 
und bildet 
dann den Erwartungswert, so ergibt sich: 
£ 0 Y x u(j-k) = 
z e ^ ü - k ) ^ 
. 
3 = 0 
J 
] = o 
(19) 
Wegen der Unkorreliertheit von 
vereinfacht sich die rechte Seite von 
(19), die mit c^ bezeichnet werde. Es gilt: 
0 
k u 
k=0,1,...,n , 
0 
sonst 
(20) 

192 
Walter Mohr 
Mit Hilfe der zu Modell (16) gehörigen reinen Moving-Average-Darstellungs-
form, 
X =i|i(B)U = Z i|) .U . 
* 
j=o 3 
^ 
( 2 1 ) 
deren Koeffizienten t p d i e Impuls-Antwort-Folge bilden, erhält man 
als Kreuzkovarianzen 
EiX^i U4.)=Yv.„(-k)=E( E \jj .U 
t+k t 'XU 
U )=ip a z 
für k=0, 1,2,... 
J=o j t+k-j t 
rk 
u 
bzw. Yxu(-k)=yux(k) = 
0 
k<0 , 
ili, a 2 
k^O . 
Tk u 
(22) 
Für k>n+l 
ist die rechte Seite in (18) gleich Null, und man gewinnt 
nach Division durch y 
(O) die bekannte Rekursionsformel: 
'XX 
P.+^P, . + ...+<(> p. =0 . 
k 
1 k-1 
m k - m 
(23) 
Es wird s=max(m,n) gesetzt. Für k^s+1 kann die Beziehung (23) zur Be-
rechnung der Autokorrelationsfolge verwendet werden, wenn p,, ...,p be-
1 
s 
kannt sind. Diese Werte sollen jetzt bestimmt werden. 
Aus (2) läßt sich, da die Modellparameter 
A und 0.,...,0 
gege-
1 
m 
1 
n 
ben sind, die Impuls-Antwort-Folge 
rekursiv bestimmen. 
Daher kann die Gleichung (18) unter Einsetzen von (22) umgeschrieben 
werden zu 
" V x x ( ] ^ > = 
V x u ( k - i ) = a u j V i - k ^ u 
( 2 4 ) 
]=o 
i=o 
i=o 
i=k 
für k=0,1,... 
Für k>n ist offensichtlich b, =0 . 
k 

Kenngrößen von ARMA-Modellen im Zeitbereich 
193 
Setzt man in (24) k=0,l,...,s, so erhält man ein lineares Gleichungssystem. 
Beachtet man noch die Symmetrie der Autokovarianzen, d.h. y 
(k)=y 
(-k) , 
tt 
und verwendet man die infiniten AR-Koeffizienten 
mit 
|> 
k=0,1, . . . ,m , 
0 
sonst 
so lautet das Gleichungssystem: 
D • y = b. 
(25) 
(26) 
Es ist: Y , = (Yxx(o),yxx(l),...,yxx(s)) 
b' = (b ,b. 
b ) 
o l 
s 
D = (d. .) mit 
il 
* 
»i-1 
>i-j+»*i+j-2 
i=l,...,S+1;j=l 
i=l,...,S+l;j=2, 
,S+1 . 
(27) 
Die Restgrößenvarianz o 
, die - wie man aus (24) erkennt - in die Werte 
u 
eingeht, soll eliminiert werden. Dazu dividiert man in (24) bzw. (26) 
p 
alle Gleichungen durch 
crj . 
Sei b*=b /er2 und y* (k) =y 
(k)/a2 , so lautet das Gleichungssystem 
k k u 
XX 
XX 
u 
% 
% 
Ii 
D • y = b 
, wobei D und b 
durch die Modellkoeffizienten 
( < P , 0 ) 
eindeutig festgelegt sind. 
Da die Matrix D regulär 
ist, ergibt sich.: y* 
„-1 
D 
b 
(28) 
Aus dem Lösungsvektor y 
erhält man sofort p^ gemäß 
Y x x ( k ) 
Y x x(k) 
Pk = y x x(o) " y*xx(o) 
für k=l,...,s. 
(29) 
Der Berechnungsalgorithmus für die AKF soll der Übersichtlichkeit halber 
noch einmal in fünf Teilschritte zerlegt werden. In dieser Form sind auch 
die Rechnerprogramme erstellt worden. 

194 
Walter Mohr 
1. Schritt: Bestimmung der IAF nach (2) 
mi n(m,k) 
^k = °k " 
Z 
"^k-i^i 
m i t 
*o = 1 
f Ü r k = 1 
" • 
i=l 
2. Schritt: Berechnung des Rechtsvektors b 
nach (24) 
x 
bk 
" 
bk = ^
= 
1 e i h - k 
^
^ 
u 
i=k 
£ 
sowie b, =0 
für k=n+l,...,s. 
k 
3. Schritt: Festlegung der D-Matrix gemäß (25) und (27) 
4. Schritt: 
Inversion der D-Matrix und Bestimmung des Lösungsvektors 
Y* = (>"xx(o) ' * " ' , YXX ( S ) ' 
n a C h 
( 2 8 ) 
5. Schritt: Berechnung der AKF nach (29) und (23) 
T x x ( k ) 
p = — 
für k=l,...,s 
sowie 
Y 
(o) 
XX 
P, =-<('.P, „-• ••-<!> P, 
f ü r k>s+l . 
k 
Y1 k-1 
m k-m 
Eine zweite Berechnungsmöglichkeit ergibt sich dadurch, daß man die IAF 
in Schritt 1 nach Formel (2) bis zu einem hinreichend großen Lag K be-
stimmt und dann eine Approximation von (8) vornimmt. 
K-k 
T, ib iL 
• = 
j k+j 
P, = 
für k=l,2,... 
(30) 
Je 
K. 
E ip2 
j=o 
3 

Kenngrößen von ARMA-Modellen im Zeitbereich 
195 
3.2. Partielle Autokorrelationsfolge 
Die PAKF berechnet man mittels der AKF am einfachsten aus den in (14) und 
(15) dargestellten Durbinschen Rekursionsformeln. 
3.3. Inverse Autokorrelationsfolge 
Aufgrund der Dualitätsbeziehungen zwischen AKF und IAKF brauchen die für 
die AKF hergeleiteten Formeln nur dual umformuliert zu werden. 
Die wesentliche Beziehung ergibt sich aus der Dualisierung von (24). 
17) 
Es gilt: 
n 
m 
V x x ( k - j ) = ^ 
, £ A T i - A 
k=0,l,... 
(31) 
j=o J 
u i=k 
Für k^m+1 
ist /b k
= 0 , und man erhält nach Division durch- Pxx'°' d i e 
Rekursionsbeziehung 
r, +0 r 
. + ...+G r, =0 . 
(32) 
k 
1 k-1 
n k-n 
Die erste Berechnungsmöglichkeit läuft dann wie folgt ab: 
1. Schritt: Berechnung der IIAF nach (4) 
min(n,k) 
TT —d> — 
I 
TT, . 0. 
mit TT =1 
für k=l,...,m. 
k k 
. . k-i i 
o 
1=1 
2. Schritt: Berechnung des Rechtsvektors b 
nach (31) 
« 
•>" 
b, = a zb = E 4.tt. , 
für k=0,l,...,m, 
k 
u k 
. , l i-k 
i=k 
sowie '3]i
=<-) für k=m+l, ... ,s 

196 
Walter Mohr 
3. Schritt: Festlegung der D-Matrix 
0 k 
k=0,l,...,n , 
O 
sonst 
D = (d. .) mit 
i] 
d. . = 
ij 
°i-l 
i=l, . . . ,s+l; j = l 
(33) 
0*_j+0*+j_2 
i=l,...,s+l;j=2,...,s+l . 
4. Schritt: 
Inversion der D-Matrix und Bestimmung des Lösungsvektors 
* 
o 
* 
^* ^-l^* 
y = azp' = (p 
(o) , . . . ,p 
(s) ) 
gemäß y =D b 
Il 
XX 
XX 
(34) 
5. Schritt: Berechnung der IAKF aus (34) und (32) 
p x x ( k ) 
r k " P* (O) 
XX 
für k=l,...,s 
r, =-0, r 
-...-0 r 
für k>s+l . 
k 
1 k-1 
n k-n 
Auch hier ist ebenfalls eine approximative Berechnung über die IIAF 
möglich. 
K-k 
E TT II 
•_ 
1 k +D 
= 
für k=l, 2, . . . 
I TT? 
j=o 3 
(35) 
3.4. Inverse Partielle Autokorrelationsfolge 
Aus (31) folgt, daß für einen reinen MA(n)-Prozeß gilt: 
T 0.r, .=0 
für 1=1,2,... 
3 
1-D 
(36) 

Kenngrößen von ARMA-Modellen im Zeitbereich 
197 
Für k=l,2,... und j=l,2,...,k sei 0 
die KQ-Approximation der IAF (AIAF) 
-1 
k 
von der Ordnung k, d.h. wenn ein MA(k)-Modell X =(0, +0 B+. . . +0, B )U 
t 
ko kl 
kk 
t 
mit 0jco
=l angesetzt wird. Aus (36) erhält man für 1=1,...,k die sog. in-
versen Yule-Walker-Gleichungen: 
1 
18) 
1 
ri 
1 
r 
r 
k-l k-2 
1 
k-l 
Lk-2 
-0 k2 
i 
a 
kk 
(37) 
Löst man das lineare Gleichungssystem (37) sukzessive für k=l,2,..., so 
ergibt sich die Folge 
' die m a n 
IpAKF bezeichnet. 
Aus der Dualität zu (12) folgt, daß man die AIAF 0 . mittels der zu den 
k] 
Formeln (14) und (15) dualen Durbinschen Formeln berechnen kann. 
Man erhält: 
0, =1 
für k=0, 1, . . . 
ko 
0. .=0. . .+0, . . .0. . 
j=l, . . . ,k-l 
kj k-l,j k-l,k-3 kk 
(38) 
k-l 
k-l 
-0 = I r 
0 
/ 
X r 0 
kk 
j = Q 
k-j k-l,j I 
j = Q 
j k-l,j 
Bei der Berechnung ist folgende Reihenfolge einzuhalten: 
(39) 
0 
;0 
,0 :0 ,0 ,0 ;. ;0 
,0 ,0 ,...,0 
; 
11' 22' 21' 33' 31' 32 
kk' kl' k2' 
' k,k-l' 
4. BERECHNUNG DER EMPIRISCHEN KENNGRÖSSEN 
Die Schätzung der Kenngrößen aus dem Datenmaterial (x,,...,x ) soll hier 
19) 
1 
nur kurz dargestellt werden. 

198 
Walter Mohr 
Sei x = — 
Ex.. die Mittelwertschätzung. 
Die Autokovarianzen werden üblicherweise berechnet mittels 
1 n-k 
y. = - 
£ (x. -x) (x. ,-x) 
für k=0,1, . . . . 
k 
n . , i 
l+k 
Als Schätzer für die AKF verwendet man p, =v, /v 
. 
(40) 
k k o 
Für die Schätzung der AIIAF bzw. der PAKF werden im Yule-Walker-Gleichungs-
system (12) für 
die geschätzten Autokorrelationswerte 
PJ,...,P 
eingesetzt. D.h. man kann auch die entsprechenden Durbinschen 
Formeln (14) und (15) heranziehen. 
4, =1 
für k=0,1,2, . . . 
ko 
k-1 „ 
k-1 „ „ 
Pk-^k-l,j / 
Pj+k-1,3 
m itk=l,2,... 
(41) 
j=o 
/ 
3=0 
+k,j=+k-lfj+*k-l,k-j*kk 
m i t j = 1 ' 2 
k " 1 
( 4 2 ) 
Die Schätzung der IAKF kann auf verschiedene Arten erfolgen. 
Hier soll hauptsächlich eine Methode verwendet werden, bei der der Zeit-
bereich nicht verlassen wird. Dabei wird (10) derart approximiert, daß 
die IIAF ir. durch die AIIAF 
<f) 
(j=0,1, . . . ,k) ersetzt wird, und zwar 
J 
kj 
für alternative Ordnungen k . 
20) 
Somit ergibt sich als Schätzer k-ter Ordnung: 
k„ 
I 
k * 
3 = 3 
/ 
(43) 
rks 
sonst 
Die nach (43) geschätzte IAKF ist wegen der Variationsmöglichkeit von k 
mehrdeutig. Man wählt zweckmäßigerweise als repräsentativ ein hinreichend 
& 
großes k 
so aus, daß sich die Werte r w 
stabilisiert haben. 
k s 

Kenngrößen von ARMA-Mode1len im Zeitbereich 
199 
Die Schätzwerte lassen sich, in einem Dreiecksschema darstellen 
ril 
r 
r 
21 
22 
Die letzte Zeile bildet die geschätzte IAKF der Ordnung k" . 
Weitere Möglichkeiten ergeben sich, wenn man Konzepte aus dem Frequenzbe-
2 1 ) 
reich heranzieht. 
Schätzungen für die IPAKF erhält man durch Dualisieren der Beziehungen 
(41) und (42). 
Mittels der Schätzwerte r,* 
ermittelt man die AXAF 0, . aus: 
k"s 
k] 
g 
= 1 
k=0,l,2,... 
ko 
^
-
y 
j 
kj;i r ^ j 0 k - i , j k = i 
** 
( 4 4 ) 
3=o 
j=o 
®kj=\-l,j+®k-l,k-j®kk 
^ 
k - 1 ; k = 1 
k* 
( 4 5 ) 
Auch diese Werte lassen sich wieder in einem Dreiecksschema zusammenfassen. 
! n 
e 
e 
21 
22 
0 * 
0 » ... 0 * * 
k l 
k 2 
k k 
Die Folge 
^ r k=l,...,k 
ist die geschätzte IPAKF zur Ordnung k . 
Da k 
unterschiedlich ausgewählt werden kann, ist diese Folge ebenfalls 
" (k*) 
mehrdeutig. Um in der Symbolik darauf hinzuweisen, soll auch 0 ^ 
für k=l,...,k als Bezeichnung verwendet werden. 

200 
Walter Mohr 
5. VERGLEICHE ZWISCHEN EMPIRISCHEN UND THEORETISCHEN KENNGRÖSSEN 
Als Grundlage für den Vergleich der empirischen und theoretischen AKF wird 
22) 
ein Satz von T.W. Anderson herangezogen. 
Satz 1: 
Sei ein ARMA-Modell mit unabhängiger, identisch verteilter Rauschgröße U 
sowie E|iJj.[«>° und 
gegeben, 
j 
3 
j 
3 
Setze Y^= ^n(p^-p^) für k=l,2,... , wobei n den Stichprobenumfang 
angibt. Dann gilt für n-**>: 
f (Yj, . . . , Y )-+N (0,W) . 
(46) 
Die Kovarianzmatrix W besteht aus folgenden Elementen: 
» 
Cov(Yi,Y.)=w = Z (P k + iP ] l + j+P k_ iP k + j-2P jP k + iP k-2p iP k + jP k +2p ip p2) 
(47) 
k = - 0 0 
für läi,jsl. 
Setzt man i=j , so gilt für n-w»: 
f (Y. )-s-N(0,w. . ) . 
l 
il 
Var(p.)=Er(p.-p.)21=E(Y2/n)= - Var(Y.)= - w. . . 
l 
L i i
J 
l 
n 
l 
n
n 
(48) 
Zunächst kann man (48) zur Überprüfung signifikanter Abweichungen zwischen 
den Einzelwerten der empirischen und theoretischen AKF heranziehen, indem 
man z.B. untersucht, ob 
|p.-p.|>2» l/— w. . ausfällt. 
(49) 
1 l l1 
In 
n 
Weiter läßt sich auf der Basis von (46) ein Globaltest für die Überein-
stimmung von empirischer und theoretischer AKF entwickeln. 
Y 
Sei Y* = — = p -p, 
für k=l, 2, . . . und Y*= (Y* .. . , Y*) . 
k 
ir~ 
k k 
1
1 

Kenngrößen von ARMA-Modellen im Zeitbereich 
201 
Dann gilt: 
f(Y* ...,Y*) 
N(0, - W) 
(50) 
1
1 
n 
Daraus folgt bekanntlich: 
Y*' ( - W)_1Y*=n. Z 
Z Y*Y* wi:1'^ x2(l) 
(51) 
1=1 j=l 1 3 
1 3 
Die Elemente der Inversen W" * sind mit wi.*' bezeichnet worden. 
iD 
Die Nullhypothese auf Übereinstimmung beider AKF 
und 
wird ver-
worfen, wenn der empirisch berechnete Wert 
1 
1 * * (-1) 
X 2 
= n £ 
Z v.v. w. . 
größer als y? 
(1) ist, wobei 
1-ci das zu-
emp 
. . . . 
i in 
1-a 
1=1 ]=1 
gehörige fjiveau darstellt. 
Für die numerische Berechnung der Matrix W ist (47) derart zu approxi-
mieren, daß der Laufindex k von -L bis L läuft mit z.B. L=100 oder 
L=1000 . 
Falls mehrere Modelle untereinander verglichen werden sollen, können fol-
23) 
gende Verlustfunktionen 
sinnvoll sein: 
1 . 
V = n l (p.-p.) 
(-hc) 
(52) 
i=l 
1 
1 
V_ = x 2 
("Hc) 
(53) 
2 
Aemp 
Die Größe K 
soll die Anzahl der geschätzten Modellkoeffizienten als 
zweite Komponente der Schadensfunktion ins Spiel bringen, wie dies auch 
beim AIC und verwandten Kriterien der Fall ist. 
Für die PAKF gibt es keinen vergleichbaren Satz, wenn man von speziellen 
24) 
Aussagen für AR-Modelle absieht. 

202 
Walter Mohr 
Für den Vergleich von Modellen kann daher nur eine Verlustfunktion vcra 
ersten Typ herangezogen werden. 
= n. i (¿kk-*kk)2 
<«> 
(54) 
k=l 
Für den Vergleich von empirischer und theoretischer IAKF kann eine zu 
25) 
Satz 1 duale Aussage formuliert werden. 
Satz 2: 
Sei 
= \jn 
f ü r k=l,2,... . Dann gilt mit n-x» : 
f (Z , . . . ,Z ) -»- N(0,V) mit 
CO 
2 
Cov(Z. , Z.) =v. .= J (r, . r, ,+r, . r, ,-2r.r, ,r,-2r.r, ,r,+2r.r.r, ) 
(56) 
i j 
13 , 
k+i k+j k-i k+] 
j k+i k 
i k+j k 
i j k 
für l^i.jsl. 
2 
äi 
k 
Ä 
*1 
äi 
& 
Für Z, = — 
= r,-r, 
und Z = (Z. , . . . , Z,) gilt analog: 
k i / - 
k k 
1 
1 
V
n 
f(Z* 
Z») •> N(0, - V) 
(57) 
1
1 
n 
Z*' C- V)"1
 z*=n. £ 
£ Z * Z * 
x2(i) 
(58) 
" 
i=l j=l 1 3 
1 3 
Wie oben sollen auch hier zwei Verlustfunktionen definiert werden: 
1 . 
2 
V, = n- Z (r.-r.) 
(+k) 
(59) 
1 
i=l 
1 
1 
1 
1 
* 
" 
( - 1 ) 
V_ = n* l 
Z (r.-r.) (r.-r.) vi. ' (+k) 
(60) 
2 
i=l j=l 
1 
1 
1 3 
Wie beim Übergang von der IAK zur PAKF gibt es auch hier für die IPAKF 
26) 
keinen passenden Satz. 

Kenngrößen von ARMA-Modellen im Zeitbereich 
203 
Als Verlustfunktion wird verwendet: 
vi =n-vvw2 (+K) 
(61) 
k=l 
Es soll noch darauf hingewiesen werden, daß für r 
und 0 
aus Grün-
den der Eindeutigkeit die repräsentativen Schätzungen r 
und 0 
% 
für k=l,...,l zu nehmen sind. D.h. in den Formeln (43) - (45) ist k =1 
zu setzen. 
Auch die Ergebnisse dieses Abschnitts unterstreichen, daß die IAKF vcm 
theoretischen Standpunkt aus betrachtet, die geeignete Ergänzung zur AKF 
ist. Die PAKF besitzt praktische Vorteile, insbesondere weil sie eindeu-
tig und im Vergleich zur IAKF direkter schätzbar ist sowie einfacher inter-
pretiert werden kann. 
6. ANWENDUNGSBEISPIEL 
Die oben diskutierten Methoden sollen hier auf die Modellierung der Reihe 
der Arbeitslosen in der BRD (1/1967-12/1977) angewendet werden, weil die-
se Reihe eine interessante Kenngrößenstruktur besitzt und schon mehrfach 
27) 
untersucht worden ist. 
Dort ist u.a. gezeigt worden, daß es sinnvoll 
ist, den gemischten Differenzenfilter " t = " l 2 X t 
Z U betrachten, 
diesen werden hier die entsprechenden Kenngrößen analysiert und mit den 
theoretischen Kenngrößen der drei folgenden Modellen verglichen. 
Ml: ARMA (2,0,0,1) mit (¡^=-0.465; <j>2=-0.179; 012=-O.492 . 
M2: AR (14) mit (^=-0.589; 4>10=-0.169r $ =0.417,-
4>13=-°-533; ¿14=0.499. 
M3: MA(13) mit 0 =0.541,- 
02=O.529; 03=O.461; 6^0.279,-
01O=O.178; 012=-O.389; 9^=0.299. 

204 
Walter Mohr 
Die empirische AKF für 
W t = ^ 1 2 X t 
zeigt (siehe Tabelle 1) signifikante 
Werte für die Lags 1,2,3,12,14,15,16,17, wenn man die Grobschätzung 
Kenngrößen unterstellt wird. Diese (vorsichtige) Vorgehensweise ist inso-
fern gerechtfertigt, als die Werte für die Standardfehler, die mittels an-
derer Approximationen berechnet werden, durchweg größer ausfallen. 
Die theoretische AKF des ersten Modells imitiert die empirischen Werte 
recht gut; (siehe Tabelle 1) dieses kommt auch durch die Verlustfunktionen 
V^=52.3 und 
(bei < = 3 Parametern) zum Ausdruck. Es kann global 
keine signifikante Abweichung festgestellt werden. Größere (approximativ 
signifikante) Abweichungen bzgl. der Einzelwerte treten für die Lags 10 
und 13 auf. Dieser Sachverhalt schlägt sich z.B. auch in der AKF der 
Restgröße nieder, worauf hier jedoch nicht eingegangen werden soll. Man 
kann versuchen, durch den Einsatz additiver Modelle mit mehr Parametern 
eine noch bessere Anpassung zu erreichen. 
Das zweite Modell (rein autoregressiv mit <=5 Parametern) schneidet sogar 
noch etwas günstiger ab. (V^=48.4 und V^=27.2). Beim Gesamtvergleich 
liegt ebenfalls keine signifikante Abweichung vor. Bemerkenswerte Diffe-
renzen treten bei den Einzelwerten nur für die hohen Lags 26,29 und 31 
auf. (siehe Tabelle 1) 
Auch das dritte Modell - ein reiner MA-Prozeß mit k=7 Parametern - zeigt 
eine gute Anpassung. (Vj=64.7 und V2=11.5) Der sehr geringe Wert für 
kommt durch die im Vergleich zu den beiden ersten Modellen günstigeren 
Gewichte w ^ , ^ zustande. Signifikante Einzeldifferenzen treten für die 
iD 
Lags 13 bis 17 auf. (siehe Tabelle 1) 
Da die PAKF über die AKF berechnet wird und die Anpassung hinsichtlich 
der AKF für alle Modelle gut ist, ist die Übereinstimmung für die PAKF 
erwartungsgemäß ebenfalls gut. 
Die Struktur der empirischen PAKF ist derjenigen der empirischen AKF sehr 
ähnlich. Signifikant sind die Werte für die Lags 1,2,12,13,14. (siehe 
Tabelle 2) Vergleicht man die theoretischen Folgen damit, so treten 
als Standardfehler verwendet, was ebenfalls für die anderen 

Kenngrößen von ARMA-Modellen im Zeitbereich 
205 
signifikante Differenzen nur an den Stellen 14 (für Ml und M2) und 12 
(für M3) auf. (Tabelle 2) Die globale Anpassung ist für alle Modelle gut, 
wie aus den Werten für die Verlustfunktion V. in Tabelle 5 ersichtlich 
1 
ist. 
Die empirische IAKF (berechnet für 1=36) zeigt - allerdings mit entgegen-
gesetztem Vorzeichen - ein ähnliches Muster wie die AKF und PAKF. Die 
Saison kommt hier noch stärker zur Geltung. Signifikante Werte liegen an 
den Lags 1,6,12,13. Vergleicht man die entsprechenden theoretischen Fol-
gen, so schneiden Ml sehr gut und M2 gut ab, während die Ergebnisse für 
M3 ungenügend sind, (siehe Tabelle 3) Der entsprechende Globaltest zeigt 
28) 
für M3 ebenfalls eine hochsignifikante Abweichung. 
Aufgrund dieser Un-
terschiede ist es auch nicht verwunderlich, wenn hinsichtlich der IPAKF 
die Modelle Ml und M2 gute Ergebnisse liefern, während sie für M3 wieder 
schlecht ausfallen. (Tabelle 4) Beim Vergleich der Kenngrößen soll noch 
auf einen allgemeinen Aspekt aufmerksam gemacht werden. Es ist schon oben 
erwähnt worden, daß die PAKF und die IAKF sowie die IPAKF und die AKF von 
ähnlicher Bauart sind, wenn man nur für die PAKF und die IPAKF jeweils 
29) 
das entgegengesetzte Vorzeichen nimmt. 
Das würde bedeuten, daß man ent-
gegen der allgemeinen Konvention in der Literatur die Folgen 
un<^ 
{0^} als partielle Folgen definiert. 
In Tabelle 5 sind die wesentlichen Ergebnisse noch einmal zusammenge-
stellt und bewertet worden. Als Vergleich ist das Modell MO herangezogen 
worden, das nur aus dem gemischten Differenzfilter besteht. Die Werte die-
ses Ausgangsmodells sollten durch die anderen Modelle entscheidend verbes-
sert werden. Bei den früheren Untersuchungen hat die Güteprüfung für alle 
31) 
drei Modelle gute bzw. zufriedenstellende Ergebnisse gebracht. 
Hinsicht-
, 
2 
lieh der Modellanpassung (gemessen durch a^ , R und AIC) lautet die Rei-
henfolge M2, M3, Ml. Bei der Kenngrößenanalyse für die Restgrößen liegt M3 
vor M2 und Ml. Anders sieht es aus, wenn man die Prognosegüte (z.B. den 
durchschnittlichen, absoluten, prozentualen Fehler) betrachtet, hier ist 
Ml klar vor M2 und M3 einzustufen. Im hier durchgeführten direkten Ver-
gleich der Kenngrößen schneiden die Modelle Ml und M2 durchweg gut ab, wo-
bei Ml wegen der besseren Resultate bzgl. der IAKF noch vorzuziehen ist. 

206 
Walter Mohr 
Das Modell M3 ist wesentlich schlechter und sollte wegen seiner mangelhaf-
ten Anpassung bzgl. der IAKF und der IPAKF nicht als Prognosemodell ver-
wendet werden. Die Diskussion dieses Beispiels hat somit gezeigt, daß durch 
die in dieser Arbeit vorgestellten Methoden des direkten Kenngrößenver-
gleichs wesentliche, zusätzliche Informationen gewonnen werden konnten. 
Sie sollten daher unbedingt in den Katalog der Überprüfungstechniken für 
die Modellgüte aufgenommen werden. 

Kenngröße von ARMA-Modellen im Zeitbereich 
207 

208 
Walter Mohr 

Kenngröße von ARMA-Modelien im Zeitbereich 
209 
LO 
O 
CTI 
CN 
ID O o 
CTi 
O 
CN 
ro o o 
LO O ro 
O O O 
O O o" 
o o o 
o o 0 
1 
O ÍN CN 
IT) 
CN o o 
CN 
CN 
O O 
^ 
O O 
o o 
o m 
0 
1 0 
1 O 
0 
1 0 
1 0 
1 
0 
1 o o 
o O O 
LO in CN 
r» ro CN 
œ o g 
ID co ro 
O O 
O O O 
O o q 
ro m CN 
6 
i 0 
1 O 
O 
1 0 
1 0 
1 
0 
1 ó o 
0 
1 0 
1 0 
1 
m 
^ 
o O O 
LO o O 
CO CN 
o O O 
o o o 
o o Ö 
ro 
O 
0 
1 0 
1 0 
1 
o o o 
o o o 
O O 0 
1 
r- Ul 1 
o o o 
o o o 
co CN ^ 
O O o 
o o o 
o o o 
^ 
O O 
O o o 
o o o 
o o o 
O 
1 O O 
T co m 
o o o 
Q o g 
^ 
CTi 
O O 
o o o 
O o Ö 
o o 
— 1 
O O O 
o o o 
o o o 
ó 0 
1 O 
r- ID LO 
o o o 
o o o 
ro 
CN ^ 
o 
o o o 
o o o 
O CN ro 
0 
1 0 
1 6 
i 
o o o 
o o o 
0 
1 0 
1 O 
1 
OI CN o 
o o o 
o o o 
co ^ 
CTI 
o O o 
o o o 
o o o 
CN co 
O O o 
o o o 
o o o 
O O O 
ro 
S 
m CM 
o o o 
o Q 
r-
i t o 
-
o o o 
o o o 
CN i O 
<M 
O O o 
S 
o o o 
0 
1 o o 
0 
1 0 
1 O 
f 
^ 
O 
^ 
g o Q 
o o 
T CN CN 
O o Ö 
a 
O o o 
o o o 
CN O CO 
o o o 
u 
o o o 
o o o 
O O O 
1 1 1 
¡3 
1 
i« 
LO IT) o 
ro 
o 
CTI cû LD 
•-1 
o o 
È 
o O 
o CN o 
O r-> 
h 
o o o 
«í 
o o o 
o o o 
O O o 
s 
1 
1 
H 
1 1 1 
1 
H 
m o co 
01 
^ 
LO 
CN o o 
CN m T 
ro m o 
J3 
m .—4 O 
LT) 
o 
LD co cí) 
ai 
• 
• 
O 
« 
s. 
O O o 
U) 
O o o 
o o o 
O O O 
o 
1 1 1 
•H 
1 1 1 
1 1 
1 1 1 
M 
•P 
•rH 
0) 
CN T LO 
^ 
CN 
LO 
CN 
m 
CN 
LO 
-H 
CN m 
0 
CN m 
CN CN 
CN CO 
a< 
1 1 1 
(D 
1 1 1 
1 1 1 
1 1 1 
a 
1 ro LO 
.C 
»—i en in 
T-H ro LT) 
v-H co LO 
U 
^ 
CN 
H 
CN 
'—1 CN 
CN 
e 
<D 
Ol 
C 
O 
•H 
•U 
id 
i—i 
Q) 
H 
M 
0 
.y 
o 
+j 
3 
C 
0) 
m 
H 
<D 
> 
C 
a) 
u 
ai 
•H 
j-i 
a) 
n 
o 
<u 
•a 
c 
3 
ai 
.tí 
u 
I1 
« 
<1) 
£> 
tO 
E-i 

210 
Walter Mohr 

•Kenngröße von ARMA-Modellen im Zeitbereich 
211 

212 
Walter Mohr 
Fußnoten 
1) Erste Ansätze dazu findet man bei Kashyap, u.a., Seite 202 sowie 
Nerlove u.a., Seite 202 und 207 ff. 
2) Vergleiche Mohr (1980), insbesondere Seite 177 ff. 
3) Man erhält die Kenngrößen im Frequenzbereich durch Fouriertransforma-
tion der Impuls-Antwort-Folge, der Inversen Impuls-Antwort-Folge, der 
Autokorrelationsfolge und der Inversen Autokorrelationsfolge. 
Vergleiche Mohr (1976), Seite 69 ff. 
4) Siehe Mohr (1976), Seite 49. 
5) Daselbst, Seite 55. 
6) Daselbst, Seite 71. 
7) Daselbst, Seite 72 und 73. 
8) Die Inverse Autokorrelationsfolge ist 1972 von W.S. Cleveland einge-
führt worden. 
9) Siehe Mohr (1976), Seite 72-75. 
10) Daselbst, Seite 80 ff. 
11) Diese Beziehungen sind schon seit 1975 von Mohr in die entsprechenden 
Rechnerprogramme zur univariaten Box-Jenkins-Analyse eingearbeitet 
und - allerdings mit mäßigem Erfolg - zur Identifikation verwendet 
worden. 
Man vergleiche ansonsten die Arbeit von Hipel, McLeod und Lennox, 
Seite 569 ff. 
12) Vergleiche Durbin, Seite 234 ff. 
13) Siehe Mohr (1976), Seite 67 und 75. 
14) Vergleiche Box und Jenkins, Seite 71. 
15) Zur Vorgehensweise vergleiche man: 
Box und Jenkins, Seite 74 und 75. 
McLeod, Seite 255 und 256. 
Mohr (1976), Seite 57 ff. 
16) Nach einer Anmerkung in der Arbeit von Kitagawa hat Akaike diesen 
Sachverhalt nachgewiesen. 
17) Vergleiche Mohr (1976), Seite 66 und 67. 
18) Vergleiche die Bezeichnungsweise bei Hipel, McLeod und Lennox, 
Seite 569. 
Bei Bhansali werden diese Beziehungen nach ihren Urhebern Cleveland-
Parzen Gleichungen genannt. 
19) Für eine ausführlichere Darstellung vergleiche man Mohr (1976), 
Seite 118-127. 
20) Siehe Cleveland, Seite 281 oder Mohr (1976), Seite 125. 
21) Man kann die Aussage verwenden, daß die Inverse Autokovarianzfolge 
p^ 
und die Inverse Spektraldichte g(A) ein Paar von Fouriertransfor-
mierten sind. Dabei ist g(A)=l/f(X) , wenn f(A) die Spektraldichte 
bezeichnet. Die Spektraldichte kann u.a. auf zwei Arten über die Auto-
kovarianzfolge geschätzt werden, mit der sie ein Paar von Fourier-
transformierten bildet. Und zwar mit Hilfe von Gewichtsfenstern oder 
durch autoregressive Anpassung. 
Man vergleiche dazu Bhansali, Seite 553 und 555. 
Die autoregressive Technik im Spektralbereich ist mit der hier verwen-
deten Methode im Zeitbereich identisch, wie ein Vergleich von (43) mit 
der Formel (3.1b) bei Bhansali auf Seite 557 zeigt. 

Kenngröße von ARMA-Modellen im Zeitbereich 
213 
22) Siehe T.W. Anderson, Seite 489, Theorem 8.4.6 und Corollar 8.4.3. 
1 
. 
2 
23) Das Abweichungsmaß 
Z (P.~P.) 
ist rein deskriptiv. Man könnte auch 
1 » 
i=l 
1 
1 
Z p.-p.I 
verwenden. Zusätzlich wäre es sinnvoll, Gewichte g. zu 
. i i
1 
i 
1=1 
verwenden (g^>0, Zg^=l), die z.B. monoton fallen oder bei saisonalen 
Modellen in der Umgebung der Vielfachen des Saisonlags relativ groß 
sind. 
24) Für AR (m)-Prozesse gilt bekanntlich 
'c>m • 
Bei T.W. Anderson, Seite 216 und 221 wird für solche Modelle gezeigt: 
\ß'<t>kk 
N(0,1) bzw. <|>kk 
N (0, 
für k>m 
p 
sowie n- Z 4, , ->- v2 
(sog. Quenouille- Test) . 
. kk 
Ap-m 
k=m+l 
* 
25) Vergleiche Bhansali, insb. Seite 559 sowie Hosking, Seite 225. 
26) Man könnte höchstens versuchen, die Aussage von Fußnote 24) zu 
dualisieren. 
27) Siehe Mohr (1979) und Mohr (1980). 
28) Ein Grund dafür ist sicherlich, daß sich bei der Schätzung des reinen 
MA-Modells (M3) keine numerische Konvergenz eingestellt hat und im 
Verlauf der Iterationen die Modellkoeffizienten stärker schwanken. 
(Mohr (1980), S. 177. 
29) Man vergleiche auch die empirische PAKF und IAKF für ein Beispiel bei 
Chatfield, Seite 372. 
30) Es soll darauf hingewiesen werden, daß in Anlehnung an Box und 
Jenkins in der Literatur die AR- bzw. MA-Koeffizienten des Modells 
meist negativ festgelegt werden und dann <f> 
und 0 
die partiellen 
_ 
_ 
, 
_ 
KJv 
K K 
Folgen sind. 
Wenn man wie hier die Modellkoeffizienten positiv ansetzt, muß man 
zwangsläufig - wie im zweiten Abschnitt geschehen - die Folgen 
und 
als partielle Folgen definieren, wenn die Konvention 
beibehalten werden soll. 
31) Vergleiche Mohr (1979), S. 281 und Mohr (1980), S. 179. 

214 
Walter Mohr 
Literatur 
Anderson, T.W.: The Statistical Analysis of Time Series. Wiley, New York 
1971. 
Bhansali, R.J.: Autoregressive and window estimates of the inverse 
correlation function. Biometrika 67_, 551-566 (1980). 
Box, G.E.P., Jenkins, G.M.: Time Series Analysis, Forecasting and Control, 
2. Printing. Holden-Day, San Francisco 1971. 
Chatfield, C.: Inverse Autocorrelations. Journal of the Royal Statistical 
Society A, 142, 363-377 (1979). 
Cleveland, W.S.: The Inverse Autocorrelations of a Time Series and their 
Applications. Technometrics J_4, 277-298 (1972) . 
Durbin, J.: The Fitting of Time Series Models. Review of the International 
Statistical Institute 28, 233-243 (1960). 
Hipel, K.H., McLeod, A.I., Lennox W.C.: Advances in Box-Jenkins modeling, 
1. Model Construction. Water Resources Research 13, 567-575 (1977). 
Hosking, J.R.M.: The asymptotic distribution of the sample inverse auto-
correlations of an autoregressive-moving average process. Biometrika 
67, 223-226 (1980). 
Kashyap, R.L., Ramachandra Rao, A.: Dynamic Stochastic Models from Empiri-
cal Data. Academic Press, New York 1976. 
Kitagawa, G.: On A Search Procedure For The Optimal AR-MA Order. Ann.Inst. 
Stat.Math. ¿9, 319-322 (1977). 
McLeod, A.I.: Derivation of the Theoretical Autocovariance Function of 
Autoregressive-Moving Average Time Series. Applied Statistics 24, 
255-256 (1975). 
Mohr, W.: Univariate Autoregressive -Moving-Average Prozesse und die An-
wendung der Box-Jenkins-Technik in der Zeitreihenanalyse. Physica-
Verlag, Würzburg 1976. 
Mohr, W.: Prognoseuntersuchung für die Zeitreihe der registrierten Arbeits-
losen in der BRD. Statistische Hefte 20, 276-283 (1979). 
Mohr, W.: Grobidentifikation und Modellvergleich bei ARIMA-Modellen. 
Allgemeines Statistisches Archiv 6£, 164-183 (1980). 
Nerlove, M., Grether, D.M., Carvalho, J.L.: Analysis of Economic Time 
Series. Academic Press, New York 1979. 

APL and the Teaching of Statistics 
Peter Naeve 
1. INTRODUCTION 
Undoubtedly the statistical community has become aware of the great possi-
bilities of modern computer facilities. This might be seen by the ever in-
creasing list of books and articles dealing with such themes as 'statisti-
cal computing', by the growing number of conferences and symposia devoted 
to the impact of computer on statistics and by numereous computer programs 
ready made for statisticians' use. As there are already many papers on the 
special topic 'computers in the teaching of statistics' too (Evans (1973), 
Naeve (1978)) justification must be given why one more paper is added. 
With respect to teaching all this books, articles and conferences do not 
tell the true story. One gets an impression of a world that is wishful 
thinking compared with every days teaching of statistics. This is especial-
ly true for the role of APL in that behalf. 
With respect to APL computer scientists and programmers are divided in two 
hostile parties. This attitude seems to have been carried over to statisti-
cians without paying much attention if an argument that might be convincing 
from a computer scientist's point of view keeps its place when considered 
from a statistician's point of view. Or if there are other properties of 
APL which although of little value for a computer scientist make APL such 
a rich tool for a statistician. 
The following lines do not intend to give an introduction to APL. The APL 
novice may take Gilman, Rose (1974) , Pakin (1972) or Polivka (1975) as an 
advice. They are meant as a list of arguments why one should use it. 

216 
Peter Naeve 
2. APL AS A NOTATIONAL LANGUAGE IN STATISTICS 
The richness of build in primitive functions and their wide scope allows 
for almost one to one mapping of formules written in conventional mathe-
matical notation and executable 'programs'. To give one example if one 
had to evaluate 
SS = B'C(C'GC)~1C"B 
this could be done in APL as follows 
SS 
CB 
+ . * ( S ( i S C ) + . x G + . x C ) + . x C B 
( i S ) C ) + . x B 
. 
As Evans (1973) points out 'the programming details are minimized and one 
can get on to the statistical implications. 
The feature of user defined functions eases the development of an even 
more statistical tailored notation. Direct function definition even if not 
yet always implemented is of special value in this respect. The following 
example is an modified one from Rosenkrands (1974) . 
CORR : 
(SPD oi)iSQRT (SSD oi)°.*SSD oi 
SSD 
: 
1 1 §SPD gj 
SPD 
(<WEV w)+.*DEV ai 
DEV 
ai-(pùi)pMEANS ai 
MEANS 
(+/[lloi)H , P 6 j ) [ i : 
It is easily seen that APL even allows for a top down approach - one of 
computer specialists' favorite. Having introduced this set of functions 
which altogether compute the correlation matrix for n variates in the 
sequel CORR DATA 1 or CORE X are easily understood notations. 
3. APL IS STRUCTURED 
This statement will be strongly contradicted by most computer scientists. 

APL and the Teaching of Statistics 
217 
But it holds. The fact is that the meaning of 'structured' is almost to-
tally linked to logic-structured programming as Metzger (1980) points out 
With respect to while do , repeat until and other constructs APL surely 
is not structured at all. But if one takes the data on which an algorithm 
will act into consideration one soon notices that APL is structured with 
respect to data (Metzger (1980)). Take for instance a contingency table. 
As seen in figure 1 it is made up of four parts. 
XI 
XJ 
N 
Fig. 1.: contingency table 
The four parts are 
X 
observed cell frequencies 
XI 
row margins (row totals) 
XJ 
columns margin (columns totals) 
N 
number of observations (over all totals) 
Let XC denote the contingency table than one might construct XC with 
the following APL code starting from a given X 
XI *- + / X 
fi 
row totals 
XJ 
+ / 11] X 
fi 
column totals 
XJ *• + / X 
p 
equivalent expression 
N *-+/ 
+ / X 
n 
over all total 
N 
*-+/ 
+ / X 
n 
equivalent expression 
XC 
XX 
X, + /X 
fi 
XX is matrix 
X augmented with 
columns totals 

218 
Peter Naeve 
One should contrast this with the following PASCAL code to do the job of 
calculating column margins. 
for j := 1 ;to n do 
begin 
xj [j] := 0 
for i := 1 tea m do^ 
xj[j] := Xj[j] + x[i,j] 
end 
But remember there has to be a declaration part too and you are forced to 
provide all the necessary space in advance for PASCAL (like all the other 
'high level well structured1 languages) offers no 1catentation' like fea-
ture. 
4. APL EASES THE TASK OF REWRITING ONE'S PROGRAMS 
Even in situations where one did approach a statistical programming task 
according to the many rules of software engineering one might face many 
reasons afterwards why the program should be rewritten. The list starts 
with questions of optimization either computing time or space and ends with 
questions of better understanding (self explanatory programs). But as 
Abrams (1973) states: 'program rewriting is relatively rare in the 'real 
world1 of traditional programming languages'. Due to the power of the build 
in functions APL programs are usually much shorter than programs in any 
other language doing the same job. This fact and the interactive nature of 
APL makes the task of rewriting an easy one. But rewriting one's program is 
more than just a technical matter. Rewriting means rethinking and reana-
lyzing the algorithm. Rewriting will give deeper insight into the problem 
and its solution. Abrams (1973) illustrates these findings in connection 
with the problem of calculating the critical path of a project network. 
What follows is another exmaple done by an graduate student - which demon-
strates the benefits one gets from rewriting. Reaching deeper insight 
clearly is a valuable goal for a student. The ease with respect to pro-

APL and the Teaching of Statistics 
219 
gramming time and effort has not been measured in this example. The problem 
was to define a function to do the job of the Sweep Operator. The algorithm 
may be presented as follows : 
§ data matrix to be sweept, b result: swept matrix 
r index of row and column which are to be exchanged § 
b 
-e 1/data 
rr 
rr 
b. *• data, /data 
i*r 
ir 
ir 
rr 
b . 
data ./data 
j*r 
rj 
rj 
rr 
b. . *• data. . - (data. 
* data. )/data 
i,j*r 
i] 
l] 
ir 
jr 
rr 
The first solution offered was: 
V Z+-K SWEEP A ; AS1 
; AR ; AC 
[1] 
Ifr-pA 
[2] 
/IM7[1] 
[3] 
TH\N)-(\N) 
[4] 
T\_K]+- 1 
[5] 
TINV+(((iN)-((\N)-l))-T 
[ 6 ] 
ACHTINV/LlHT/A)) 
[7] 
ARHTINV/(T/il]A)) 
[8] 
R+TINV/11](TINV/A) 
[9] 
ASl*-H-(AC+.xAR)iAZK;K] 
T\[1](TINV\ARiAIK;K])) 
i l l ] 
AS\_K\K]+UAtK-,K] 
V 
Although there are no loops in that function as would be in an equivalent 
FORTRAN, PASCAL or ALGOL program it is far away from a 'good' APL func-
tions. But at the moment our concern are not problems like to many (unnec-
essary) brackets, to many lines - for instance one could easily combine 
line [1] and [2] to get 
N+- (H*-pA) [ 1 ] or even better 
flMp4)[l] 
or equiva-
lently Af«-ltpi4 . The main concern is that at this stage the student has 
not been fully aware of the fundamental difference between APL and the 

220 
Peter Naeve 
traditional programming languages. In contrast to the latter APL calls for 
thinking in higher dimensional structures. Although the traditional pro-
gramming languages allow for e.g. two dimensional arrays their syntax usu-
ally enforce upon a one dimensional (element by element) way of processing 
Compare for instance matrix addition in APL 
C «- A + B 
and in FORTRAN 
DO 
10 
I = 1 ,N 
DO 
10 
J = 1 ,N 
10 
C(I,J) = A ( I, J) + B (I, J) 
So when introduced to this point of view the student delivered after some 
thought the following function : 
V AS+K SWEEP DATA 
[1] 
F+(N+(pDATA)[ll)pl 
[2] FlK]+-~lxDATAlK-,Kl 
[3] 
PRHTE+K=\N)/lll(FID+DATAiF° 
.x \F) 
[4] 
PCOLD+TE/DATA 
[5] 
PEll;K]+PCOLDlK;iy*-0 
[6] 
AS+FID+PCOLD+.xPR 
[7] 
ASlKiK]+\ASlK;Kl 
V 
Still not perfect but much more in line with APL-thinking. And besides a 
more concise function the student surely has reached a level of deeper 
understanding of the whole procedure. Without the process of rewriting 
this insight scarcely would have been gained. 
5. APL EVOKES CREATIVITY 
Recently some students judged statistics to be boring compared to proba-
bility theory for in the latter there were usually several lines of attack 

APL and the Teaching of Statistics 
221 
to find a solution to a problem. Although one might find some strong argu-
ments against their opinion seen from a novice point of view there is some 
truth in it. 
But when one incorporates APL in the process of learning the before men-
tioned opinion turns out to be completely wrong. Take for instance the 
problem of calculating the ranks for a given vector X of observations 
when ties are possible. 
Smillie (1974) gives the following solution : 
Whereas Snyder (1973) comes up with this line of code : 
Let us take another simple problem: how to evaluate a polynomial 
n 
P(x) = Y c.x1 
• „ 
1 
x =0 
Here is a list of some functions to do the job. 
POL1 
: +/axtij* l + ip,a 
P0L2 
: (a>*~ l+ip,a)+.xa 
P0L3 
: 
(x\"l+l,(p,a)paj)+.xc( 
POLK 
: 
(Jl4>a 
POLS 
: 
(us.*~1+ip,a)+.xa 
P0L6 
: (x\0 "l + l,iS)((p,a),p,w)pilj)+.xa 
P0L1 
: (,£d)[((p,aj),l)pip ,w]±4>a 
Let C denote the vector of coefficients and X the argument for which 
the polynomial shall be evaluated then the result is given by calling 
C POLi X 

222 
Peter Naeve 
The last three functions are also capable of handling vector arguments i.e. 
the polynomial with coefficient vector C is evaluated at several points 
x^. . POLi,x= 1,2,5 are adapted from Iverson (1972). APL and efficient 
statistical computing are not contradictory. There is always the tempta-
tion to turn APL down because it is an interpretative language and there-
fore a natural second best compared to compiler languages with their run 
time superiority. 
But first of all we must be aware that this is a computer oriented type of 
argument. When a computer specialist's main concern lies in effective use 
of his computer with respective to optimization (minimization) of CPU-time 
and memory space this cannot necessarily be a statistician's point of view. 
It is easy to produce with highly efficient code very fast a lot of gar-
bage. So a second thought will show that it is not the time of the computer 
but our time that matters as Dennis Evans emphazised during the discussion 
following the presentation of his paper (Evans (1980)) at COMSTAT 1980. 
Statistics calls for insight in the process evoked by a statistical pro-
cedure i.e. the link between data and algorithm, the sequence of patterns 
which are build up and so on.And here APL is a useful tool as hopefully is 
shown by the foregoing arguments. So statisticians should hold their own 
interpretation of efficiency against that of the computer scientist when 
negotiations about implementing APL at the computer center are held. 
This does not mean that the other interpretation of efficiency shall be 
completely denied. As Kennedy and Gentle (1980) again and again point out: 
'the purpose ... is to present selected computational methods, algorithms, 
and other subject matter which is important to the effective use of the 
given methods and algorithms in the computer'. 
As might be seen in the before mentioned example by Abrams (1973) one might 
combine those questions of efficiency in the sense of Kennedy and Gentle 
with the aim to get deeper insight in the problem and the applied methods 
to reach a really efficient (fast and clear) solution. 
To write efficient programs in any high level language demands some know-
ledge of the structure and properties of the host computer and some insight 

APL and the Teaching of Statistics 
223 
in how the compiler does his job. But there is nothing different with APL. 
As Sykes (1973) states the essential point is 'know your operators'. After 
some trial and error and some thought the results given in the following 
table concerning the time for functions POLi, i=5,6,7 on an IBM 5110 will 
become clear. 
POL 
O i 51 
A>i20 
O . 5+i 51 
X^\20 
C«-. 5+1 51 
X*-. 5+120 
0 1 2 6 
X*-.5+150 
5 
36 
31.4 
31.7 
41.1 
6 
23.3 
19.4 
18.6 
24.9 
7 
12 
9.6 
8.8 
12.9 
Fig. 2: Computing time (in sec) 
6. WHAT KIND OF STATISTICS SHOULD WE TEACH ? 
It seems to be wise to stop for a moment in talking about the advantages 
of APL. The phrase 'the teaching of statistics1 rises two questions. 
Firstly 
: 
What should be taught, i.e. what kind of statistics ? 
Secondly : 
How could the goals be achieved, i.e. in our context : 
can APL help in reaching these goals ? 
Although the answers to both questions cannot be given independently, it 
appears as if the enthusiasm over that powerful tool computer often made 
people skip the first question. As a result much effort and money (computer 
time, programming cost etc.) has been wasted on projects which rank low in 
an overall list of teaching goals and connected means. 
The present author shares the point of view expressend by Tukey (1980). If 
one agrees that statistics is both confirmatory and exploratory this calls 
for a more subtile analysis of the role the computer may play in teaching 
statistics. 

224 
Peter Naeve 
7. APL AND CONFIRMATORY DATA ANALYSIS 
As Tukey puts it: 'confirmatory data analysis, especially as sanctification, 
is a routine relatively easy to teach and, hence a routine easy to comput-
erize'. Confirmatory analysis uses a body of well established statistical 
methods and procedures. Tukey is not to be misunderstood. Implementing any 
such method on a computer as a dependable, efficient and useful computer 
program is not an easy task. Many things have to be considered among which 
the choice of the programming language is only one. APL is here just one 
out of several competitors. 
In passing let us notice that the body of methods used in comfirmatory 
analysis has been widened tremendously through the existence of computers. 
Many more methods became computable with real data which had to be abonden-
ed before in the framework of pencil and paper calculations. 
Confirmatory analysis cannot be done without understanding the methods 
which will be used. This obvious fact can be made more specific by breaking 
it into three parts. 
Understanding the methods means 
firstly 
knowing how to interprete the results, knowing what kind of 
decisions and conclusions might be drawn from the result, 
secondly 
understanding the algorithm within the statistical procedure, 
thirdly 
knowing the way algorithm and data are linked together, 
understanding the dynamic properties of the procedure. 
As has been shown elsewhere (Naeve (1978)) the computer can be used in the 
process of teaching statistics on three different levels - named black box, 
glass box and open box - which roughly correspond to the before mentioned 
aspects of understanding. Due to lack of space this point will not be elabo-
rated here any further, the interested reader is asked to see the cited 
paper for more details. 
At the moment there is some discussion whether one should rely on an inter-

APL and the Teaching of Statistics 
225 
active or batch mode in using the computer (Klensin and Dawson (1980), 
Cooper and Emerson (1980)). This truely is an important question and he 
who is free in the choice of his tools should spend considerable time in 
wheighing the pros and cons. But all the other users have to take their 
computer as it stands i.e. bound to batch or capable of interactive usage. 
But independently of the computer at hand all designers of statistical 
software have to answer the question as to where the statistician should 
be placed - inside the computer or in front of it. 
To make this question more transparent two examples of computer output are 
presented. The first is part of a terminal session with the regression 
analysis modul of the IBM product APL Statistical Library. 
THE SIGNIFICANCE OF ABOVE F = 0.9787 
THIS FIT IS NOT CORRECT AT THE SIGNIFICANCE LEVEL'. 0.95 
TRY A HIGHER ORDER OR DIFFERENT MODEL 
The objections are twofold. Although for an experienced statistician there 
is nothing wrong with the first two lines things change when one imagines 
a statistical novice or nonexpert sitting at that terminal. Far too often 
he will not understand the information given in the first line and so 
blindly is caught by the message. 
THIS FIT IS NOT CORRECT AT THE SIGNIFICANCE LEVEL-. 0.95 
As most users of this type he only knows two significance levels i.e. 0.95 
or 0.99 and applies them without giving much thought to the meaning of the 
concept of significance level. A program should not enforce such an atti-
tude. A prompt for the significance level at the start of the program - as 
in this case - is no cure. One might say with respect to understanding : 
garbage in, garbage out. 
The advice given in the third line seems to be a nice feature. But is this 
really so. In case there is no linear model at all which would fit to the 
data. Is this incorporated in OR DIFFFRENT MODEL or is the meaning of 

226 
Peter Naeve 
these words: try another linear model i.e. take other independent variables 
into consideration ? So either the advice might be misleading or is of the 
kind: try something else. 
This point or view may look a bit dogmatic. But one has to be somewhat 
strict in the beginning if one wants to implement statistical thinking in 
the user's head and not in his computer. 
The following sample from the output produced by a similiar program are 
more along the recommended line of approach. The program was designed by a 
student of mine. 
KONFIDENZINTERVALLE FUER DIE REGKESSIONSKOEFFIZIENTEN 
Konfidenzintervall fur den Niveaukoeffizienten: 
P (362 .07H—T (1 -a/2; 8) x 13 . 906) = 1-ct 
Konfidenzintervall fur den Regressionskoeffizienten: 
P(53.314+-T(l-a/2;8)x3.627) = 1-a 
8. APL AND EXPLORATORY ANALYSIS 
Exploratory analysis is an ever growing body of helpful techniques. 
Although many of them can be applied on the pencil and paper level (perhaps 
with the need to use multi-color-pencils as Tukey (1977) would say)computer 
assistance might be welcomed. Several software products are available see 
Dawson, Klensin and Yntema (1980) or McNeil (1977) . As might be judged by 
the latter APL can contribute substantially in this respect. 
But exploratory is more than just a bundle of useful techniques and pro-
cedures, it is an attitude as Tukey (1980) puts it. Exploratory analysis 
in the long run cannot be done with fixed tools and routine lines of ap-
proach. Creativity ranks high and should not be hindered by provided pro-
grams. Although at a first glance the non-routine data analytic applica-
tions use many of the standard procedures and data processing tools there 
is a distinguishing characteristic - the necessity to allow for human 
thought and intervention (Heiberger (1979)). 

APL and the Teaching of Statistics 
227 
There are many reasons why APL is suited best for the requirements of 
exploratory. Exploratory analysis calls for rethinking and reconstructing 
one's tools. As has been shown this task is eased with APL as the program-
ming language. The workspace oriented concept of APL, the dinstinctions 
between global and local variables and the concept of a function quite 
easily offer the opportunity to enlarge the bundle of tools. The possibi-
lity to process in deskcalculator mode every global variable gives way to 
all kind of ad hoc manipulations with the data. The build-in function 
$ 
(activate) allows for user intervention with software products. This might 
be seen by the following example. 
Let us consider univariate Box Jenkins time series analysis. The time 
series data are kept in a variable TDATA 
. Usually the program would 
offer (for instance using a menu technique) some options to transform the 
data. One might bet that even the most careful planed program will face a 
user who is not satisfied by the options provided. Here the activate fea-
ture may help. The following line of code is a first simple solution 
TDATA -e $ 13 
The user might for instance answer the prompt by typing 
HxTDATA - 
TDATA , 0 
or whatever kind of transformation he wants. This line of approach surely 
can be widened to incorporate syntax checking (avoiding $ ERROR) or 
taking input via shared variables into consideration. 
Last but not least if analysis is an attitude with creativity as a vital 
content then APL seems to be the right host language for it supports 
creativity. 
9. LEARNING BY DOING 
To reach an attitude is a very difficult task. One promissing approach 

228 
Peter Naeve 
could be this. What has been said about computers and statistics can be 
combined in the following display. 
An interactive statistical system is something like the synthesis of the 
arguments and facts given before. It is a statistician oriented (speaking 
and understanding his language) system which does not pin the user to think 
in terms of computer science. Instead of that it allows for human interven-
tion. The idea of such a system has been brought up by Mustonen (1977, 
1980). He too showed that it can be realized even on a desk computer. It is 
evident that the teaching of statistics would benifit if such a system is 
at hand for the teacher. 
If such a system is not available so start and implement one. All what is 
needed is APL. If the students are allowed to participate in building 
moduls for such a statistical system one will find out that they learn 
much more than APL (best learned on the job) and certain statistical pro-
cedures. They develop step by step that kind of attitude Tukey calls for. 
This process benefits from the ease of rewriting programs (this means re-
thinking in the statistical field) as given by APL. 
The interpretative nature of APL allows for merging the usually separated 
phases of design, coding and checking (syntax, logic and acceptance). 
Leading princibles in the design such as, the user is a statistician and 
not a computer specialist, the statistical decisions should be made in 
front and not inside the computer quite easily can be pushed down onto the 
different levels of application during the process of program development. 

APL and the Teaching of Statistics 
229 
This is something like a princible guided trial and error process. And stu-
dents participating in that work will benefit for they get an ever growing 
understanding what statistics as an attitude really means. This is not 
wishful thinking. At the moment the author is working on such a statistical 
system to be implemented on a IBM 5110. APL as the reader may guess is the 
chosen programming language. Several students participate in this work. And 
they really demonstrate that the before mentioned effect is achieved. They 
all show a shift in their understanding about the nature of statistics in 
that direction Tukey wants us to teach. 
References 
Abrams Ph.S. (1973), Program writing, rewriting and style, APL Congress 73 
Gerliziv P. ,Helms H.J., Nielsen J.. 
Cooper B.E., Emerson 1*.J. , (1980) Interactive or batch, Compstat 1980, 
proceedings in computational statistics, Ed. Barrit M.M.,Wishart D.. 
Dawson R., Klensin J.C., Yntema D.B. (1980), The consistent system, The 
American Statistician vol 34 p 169 f.. 
Evans D.A. (1973) , The influence of the computer on the teaching of statis-
tics, J.R.Stat.Soc. A 136 p 135 f.. 
Evans D.A. (1980), APL 84 - an interactive, APL based, statistical compu-
ting package, Compstat 80 proceedings in computational statistics, Ed. 
Barrit M.M., Wishart D.. 
Gilman L., Rose A.J. (1974), APL - an interactive approach, 2.ed, 
J.Wiley, New York. 
Heiberger R.M. (1979), Software for statistical theory and practice, Techni-
cal report 42, Department of Statistics, The Wharton School, Univer-
sity of Pennsylvania. 
Iverson K.E. (1972), Algebra, an algorithmic treatment, Addison Wesley, 
Reading. 
Klensin J.C., Dawson R. (1980), Interactive computing versus computing in 
an interactive environment, Compstat 1980, proceedings in computa-
tional statistics, Ed. Barrit M.M., Wishart D.. 
McNeil D.R. (1977), Interactive data analysis, Wiley New York. 
Metzger R.C. (1980), Extended direct definition of APL functions, APL 80, 
Ed. v.d. Linden G.. 
Mustonen S. (1977), Survo 76, a statistical data processing 
system, 
Research report 6, Department of Statistics, University of Helsinki. 
Mustonen S. (1980), Interactive analysis in Survo 76, Compstat 1980, pro-
ceedings in computational statistics, Ed. Barrit M.M., Wishart D.. 
Naeve P. (1978), CAI and computational statistics, Compstat Lectures I, Ed. 
Skarabis H., Sint P.P.. 
Naeve P. (1979), Some aspects of the teaching of statistics, discussion 
paper 60, University of Bielefeld, Department of Economics. 
Pakin S. 
(1972), APL\360 Reference manual, Science research Associates. 
Polivka R.P. (1975), APL : the language and its usage, Prentice Hall 
Englewood Cliff. 

230 
Peter Naeve 
Rosenkrands B. (1974), APL as a notational language in statistics, Compstat 
1974, proceedings in computational statistics, Ed. Bruckmann G., 
Ferschl F., Schmetterer L.. 
Smillie K.W. (1974), APL\360 with statistical application, Addison Wesley, 
Reading. 
Snyder M. (1973), Interactive data analysis and nonparametric statistics, 
APL Congress 73, Ed. Gerl?iv P.,Helms H.J., Nielsen J.. 
Sykes R.A. (1973), Use and misuse of APL, efficient coding techniques, 
Scientific Time Sharing Corporation, Share XL. 
Tukey J.W. (1977), Exploratory data analysis, Addison Wesley, Reading. 
Tukey J.W. (1980), We need both exploratory and confirmatory, The American 
Statistician vol 34 p 23 f.. 

Optimale Schichtabgrenzung bei optimaler Aufteilung 
unter Annahme einer bivariaten Lognormalverteilung 
Karl-August Schaffer 
ZUSAMMENFASSUNG 
Optimale Schichtgrenzen bei optimaler Aufteilung des Stichprobenumfanges 
auf die Schichten werden unter der Voraussetzung berechnet, daß die gemein-
same Verteilung der untersuchten Variablen und der Schichtungsvariablen 
eine bivariate Lognormalverteilung ist. Die Ergebnisse werden verglichen 
mit den Grenzen, die optimal sind, falls die vollständige Erfassung einer 
Randschicht gefordert wird. Die Abhängigkeit der Schichtung von der Korre-
lation zwischen den Variablen, von der Zahl der Schichten und vom Gesamt-
auswahlsatz wird untersucht. 
SUMMARY 
In the context of stratified random sampling, the estimation variable and 
the stratification variable are assumed to have a bivariate lognormal 
distribution. Optimal boundaries for strata are obtained in the case of 
optimal allocation. The results are compared with the boundaries, that are 
optimal, if complete evaluation of a marginal stratum is postulated. The 
dependence of stratification on the correlation between the variables, on 
the number of strata and on the overall sampling fraction is investigated. 
1. EINLEITUNG 
Die Aufgabe, Schichten für das Ziehen einer Zufallsstichprobe vorteilhaft 
abzugrenzen, ist bereits 1950 von Dalenius als Optimalproblem formuliert 
worden. Die von ihm angegebenen Bedingungen für die Lösung des Problems, 
die Fehlervarianz einer Schätzfunktion zu minimieren, haben jeweils die 
Form eines Gleichungssystems. Diese Dalenius-Gleichungen 

232 
Karl-August Schäffer 
- sind verhältnismäßig schwer lösbar, 
- geben zwar notwendige, aber nicht immer hinreichende Bedingungen für die 
Lösung des Problems und 
- gelten nur unter Annahmen, die in der Praxis höchstens ausnahmsweise 
erfüllt sind. 
Die erste Schwierigkeit kann überwunden werden durch systematisches Pro-
bieren - vgl. Zindler (1956) und Strecker (1957) - oder durch Lösen eines 
einfacheren Ersatz-Gleichungssystems, das den Originalbedingungen unter 
vereinfachenden Annahmen über die Verteilung des betrachteten Merkmals 
äquivalent ist (vgl. Dalenius und Hodges (1957) und Ekman (1959)). 
Das Aufkommen von leistungsfähigen Computern ab 1960 hat die Möglichkeit 
eröffnet, die Dalenius-Gleichungen ohne Näherungsannahmen zu lösen (vgl. 
z.B. Sehti (1963)). Die Untersuchungen von Schneeberger (1967) haben jedoch 
gezeigt, daß diese Gleichungssysteme in manchen Fällen mehrere Lösungen be-
sitzen, die nicht notwendig dem globalen Minimum der Fehlervarianz entspre-
chen, sondern auch relative Minima sowie Sattelpunkte der Zielfunktion ent-
halten können. 
Die numerische Lösung des Optimalproblems über die Dalenius-Gleichungen ist 
ein unnötiger Umweg. Er läßt sich unter Zuhilfenahme von Computern vermei-
den, indem Minimalstellen der Zielfunktion unmittelbar mit einem Optimie-
rungsverfahren berechnet werden. Dieser unmittelbare Weg ist ganz allgemein 
gangbar, führt allerdings nicht mit Sicherheit zum globalen Minimum. Das 
globale Minimum kann jedoch nach Bühler und Deutler (1974) in einigen 
Fällen sicher mit Hilfe der dynamischen Programmierung erreicht werden. 
Die meisten Untersuchungen zur optimalen Schichtabgrenzung sind bisher 
(vgl. Deutler, 1976/77) durchweg von der vereinfachenden Annahme ausgegan-
gen, die für die Abgrenzung der Schichten verwendete Variable (im folgenden 
kurz "Schichtungsvariable" genannt) sei identisch mit der Untersuchungs-
variablen. Die vorliegende Arbeit verzichtet auf diese unrealistische An-
nahme . 
Weil die empirische Verteilung von vielen wirtschaftlich relevanten Größen 

Optimale Schichtabgrenzung bei optimaler Aufteilung 
233 
überraschend gut durch eine zweiparametrische Lognormalverteilung (vgl. 
Aitchison und Brown (1957)) approximiert werden kann, wird im folgenden 
vorausgesetzt, daß die gemeinsame Verteilung der Schichtungsvariablen und 
der Untersuchungsvariablen eine bivariate Lognormalverteilung ist. 
Unter dieser Voraussetzung werden optimale Schichtgrenzen bei optimaler 
Aufteilung des Stichprobenumfanges auf die Schichten berechnet. Die Ergeb-
nisse werden mit den optimalen Grenzen verglichen, die sich bei Totaler-
fassung einer Randschicht und optimaler Aufteilung des restlichen Stich-
probenumfanges auf die übrigen Schichten ergeben. Die Abhängigkeit der 
Schichtung von der Korrelation zwischen den Variablen, von der Zahl der 
Schichten und vom Gesamtauswahlsatz wird untersucht. 
2. 
PROBLEMSTELLUNG 
2.1. Theoretische Grundlagen 
Die Schichtung einer Gesamtheit, dh. ihre Einteilung in paarweise disjunkte 
Teilgesamtheiten ("Schichten") und das Ziehen von unabhängigen Zufalls-
stichproben aus den Teilgesamtheiten, erlaubt, den Mittelwert eines Unter-
suchungsmerkmals in der Gesamtheit mit einer Fehlervarianz zur schätzen, 
die in der Regel wesentlich kleiner ist als die Fehlervarianz des arith-
metischen Mittelwertes in einer uneingeschränkten Stichprobe vom gleichen 
Stichprobenumfang. 
Die Verkleinerung der Fehlervarianz hängt von folgenden Planungsgrößen ab: 
- von der Anzahl L der Schichten, 
- von der Abgrenzung der Schichten, 
- vom Gesamtumfang n der Stichprobe und 
- von der Aufteilung des Gesamtumfanges n auf die Schichten. 
Bei endlicher Gesamtheit ist ferner die Entscheidung wesentlich, ob die 
Zufallsstichproben mit oder ohne Zurücklegen gezogen werden sollen. In 

234 
Karl-August Schäffer 
dieser Arbeit wird vorausgesetzt, daß die betrachtete Gesamtheit endlich 
ist und die Auswahl stets ohne Zurücklegen vorgenommen wird. 
Unter der Annahme, außer der Anzahl L der Schichten sei auch ihre Abgren-
zung vorgegehen, sind die folgenden Größen für die h-te Schicht (h=l,.. ..,L) 
determiniert: 
N, 
= Anzahl der Einheiten, 
h 
Vi^ = Mittelwert des Untersuchungsmerkmals Y, 
Oy^ = Varianz des Untersuchungsmerkmals Y. 
Daraus folgt für den Umfang N der Gesamtheit 
N 
= I N. 
h=l 
h 
für den Anteil P. der h-ten Schicht an der Gesamtheit 
h 
P h 
= N h / N 
und für den Mittelwert 
des Untersuchungsmerkmals Y in der Gesamtheit 
L 
UY = * 
Ph ' ^Yh 
" 
h=l 
Der Mittelwert 
kann bei geschichteter Auswahl geschätzt werden durch 
die Funktion 
^Y = = * 
Ph • 7 h 
' 
( 1 ) 
h=l 
in der Y^ den Stichprobenmittelwert des Untersuchungsmerkmals Y aus der 
h-ten Schicht bezeichnet (h=l,...,L). 
Vorgegeben sei ferner der Gesamtumfang n der Stichprobe und damit der Ge-
samtauswahlsatz 
f 
= n / N 
(2) 

Optimale Schichtabgrenzung bei optimaler Rufteilung 
235 
Jede Aufteilung des Gesamtumfanges auf die L Schichten ist darstellbar als 
L-Tupel (n^,..., n^,..., n^) von ganzen Zahlen n^, das die Bedingungen 
O < n < N, 
für h= 1, . . . , L 
h — h 
und 
L 
E n, = n 
h=l 
h 
erfüllt. 
Sofern 
und 
für alle h=l,...,L nicht wesentlich voneinander ab-
weichen, hat die Schätzfunktion 
die Fehlervarianz 
. 
L N - n 
V{uy} = - L • I -Ü 
3L . a2 
. 
(3) 
N 
h=l 
n. 
h 
Sie hängt, wie Formel 
(3) unmittelbar zeigt, wesentlich von der Auftei-
lung (n ,...,n ) des Stichprobenumfanges n ab. Unter den oben genannten 
1 
L 
Vorgaben wird das Minimum der Fehlervarianz erreicht mit der von Neymann 
(1934) und von Tschuprow (1923) angegebenen Aufteilung 
"h = n ' V Y h / 
'\ V Y h 
' h = 1 
L' 
(4) 
/ h=l 
* 
sofern die so berechneten Stichprobenumfänge n^ die Nebenbedingungen 
n* £ N h 
h= 1, . .. ,L 
(5) 
¥ 
erfüllen (die Einhaltung der ibrderung, daß n h ganze Zahlen sein sollen, 
ist bei größerem Stichprobenumfang praktisch nicht erheblich). 
Falls eine Nebenbedingung, z.B. für die L-te Schicht, verletzt ist, müssen 
die Umfänge der Stichprobe in den Schichten nach der folgenden Vorschrift 
bestimmt werden: 
/L-l 
* 
X 
"h = < 
( n - V • V Y h / 
= V Y h 
h = 1 
(6) 
/ h=l 
. _ 
N 
h=L 
h 

236 
Karl-August Schäffer 
Die Fehlervarianz der in Formel (1) definierten Schätzfunktion y^ bei ge-
schichteter Zufallsauswahl ohne Zurücklegen ist darstellbar in der Form 
V{yy} = < 
1 ii 
N "jf 
L-l 
N f-P, 
L-l 
, 
falls n < N 
L 
Li 
falls n = N 
L 
L 
(7a) 
(7b) 
Dagegen hat der Mittelwert Y aus einer uneingeschränkten Zufallsstichprobe 
gleichen Umfanges die Fehlervarianz 
Y{Y> = I 
1-f 
f 
(8) 
darin bezeichnet 
die Varianz des Untersuchungsmerkmals Y in der Gesamt-
heit. Das aus den beiden Fehlervarianzen berechnete Reduktionsmaß 
Q 
:= 
V{yy} / V{Y} 
(9) 
quantifiziert den Schichtungseffekt: Es gibt an, um welchen Faktor der 
Standardfehler bei geschichteter Auswahl kleiner ist als der Standardfehler 
bei ungeschichteter Auswahl. Dieses Maß hat den Vorteil, daß es bei vorge-
gebenem Gesamtauswahlsatz wegen der Formeln (7a/b) und (8) nicht mehr vom 
Umfang N der Gesamtheit abhängt. 
2.2. Optimale Abgrenzung von Schichten 
Bisher wurde vorausgesetzt, die Schichten seien vorgegeben. Zur Untersu-
chung der Frage, in welcher Weise der Schichtungseffekt von der Festlegung 
der Schichten abhängt, wird im folgenden vorausgesetzt, daß eine vorgegebe-
ne Zahl L von Schichten anhand einer Schichtungsvariablen X mit dem Werte-
bereich (a,b) abgegrenzt werden soll. Jede Folge von L-l Werten 
a < X l < x 2 <...< x L - 1 < b 
(10) 
definiert dann eine Schichtung der Gesamtheit. Definiert man XQ=a und 
x^=b, dann ist die h-te Schicht gegeben durch das Intervall 
V i 
± 
< X < x. 
(11) 

Optimale Schichtabgrenzung bei optimaler Aufteilung 
237 
Unter der Voraussetzung, daß die zu untersuchende Variable Y den gleichen 
Wertebereich (a,b) besitzt und daß die Verteilung des Variablenpaares 
näherungsweise der Dichtefunktion g(x,y) entspricht, gelten die folgenden 
Formeln für h=l,...,L : 
*h 
b 
p h = / 
/ g(5,n)dnd? 
(12) 
*h-l 
a 
Yh 
P h 
xh 
/ 
b 
/ T1 • g(Ç,n)dndÇ 
(13) 
V i 
a 
xh 
b 
I I 
n 2 • g(ç,n)dndç -• " V
2 
(14) 
xh-l 
a 
• " V
2 
Yh 
P, 
Daraus folgt, daß die Varianz v{yY) der Schätzfunktion y bei Zufalls-
auswahl einer Stichprobe aus L Schichten mit Gesamtauswahlsatz f in der 
Form 
V{yy} = ^ • Fi(x1,...,xL_1|L,f,g) 
, 
i=l,2 
(15) 
darstellbar ist, wobei die Funktion F^ aus Formel (7a) für die Neymansche 
)rn 
erfassung der Randschicht hergeleitet ist. 
Aufteilung und die Funktion F aus Formel (7 b) für die Aufteilung mit Total-
Betrachtet man den Umfang N der Gesamtheit und die Verteilung g der Vari-
ablen (X,Y) als vorgegebene Daten, dann besitzt die Varianz V{uY} bei 
Neymanscher Aufteilung an der Stelle (x',...,x' 
) ihr globales Minimum, 
1 
L-1 
für die 
F 1 ( x j , . . . | L , f , g ) = 
Minimum 
F (x ,...,x 
|L,f,g) 
a < Xl < " - < XL-l < b 
gilt. Die so bestimmten Schichtgrenzen sind optimal, sofern die Aufteilung 
des Stichprobenumfanges die aus Formel (4) hergeleiteten Bedingungen 
L 
f • a„, < 
Z P er 
für h=l,...,L 
(16) 
Yh - 
' 
V=1 
erfüllt. Andernfalls sind die Schichtgrenzen (x.',...,x' ) nicht optimal. 
1 
L— 1 

238 
Karl-August Schäffer 
Wenn z.B. die Bedingung (5) für h=L verletzt ist, sind die Schichtgrenzen 
(x",...,x" 
) optimal, die der Forderung 
1 
L-1 
F 2 . . . | L , f , g ) = 
Minimum 
F 2 (x ,...,x 
|L,f,g) (17> 
a<x, <...< x 
<b 
1 
L-l 
entsprechen und die Bedingungen 
L-l 
(f-PL) 1 
Z P v a y v 
für h=l,...,L-1 
(18) 
v=l 
erfüllen. 
2.3 Optimale Schichtgrenzen bei Lognormalverteilung 
Die Größen P, , y. und a 2 in den Formeln (12) bis (14) lassen sich unmittel-
h 
h 
h 
bar zurückführen auf die Funktion 
x b 
A (x) := / / nj • f(S,H)dndS 
für j=0,1,2,... 
(19) 
a a 
Ihre Auswertung ist leicht möglich, falls das Variablenpaar (X,Y) im Be-
reich 0 < X,Y < co definiert ist und dort einer bivariaten Lognormalver-
teilung mit den Parametern 
(y*,y2,a*,a2,P*) folgt, dh. falls die Variablen 
log X - y 
log Y - u 2 
U = 
j 
und 
V = 
; 
ai 
°2 
gemeinsam eine bivariate Normalverteilung mit dem Korrelationskoeffizienten 
p besitzen. Unter dieser Voraussetzung gilt für x > 0 und jO,l,2,... 
$ 
i 2 
Aj (x) = exp(j«u2 + — j a 2 ) 
, 
* 
log x - y, 
1 
. * * 
- - jp a 
* 
al 
(20) 
wobei <f> die Verteilungsfunktion der Standardnormalverteilung bezeichnet. 
Die zu optimierenden Zielfunktionen F 
(i=l,2) hängen unter dieser Voraus-
setzung von den fünf Parametern der Lognormalverteilung ab: 
f n 
1 
. 
| 
^ »fr if« 
V{yY) = - • Fi(x1,...,xL_1|L,f,y1,y2,a1,CT2,p ) 
( 2 1 ) 

Optimale Schichtabgrenzung bei optimaler Aufteilung 
239 
Aus den Formeln (7) und (21) ergibt sich unter Berücksichtigung der Formel 
(20) die folgende Eigenschaft der Zielfunktionen (für i=l und 2): 
¡
1 
t ^ Î 
"f ) 
_
 
X
1 
* L - 1 l
L
'
f
' » v » V W
p
 
J 
• F i (uj, . . • 
|L,f,o,o,l,0*,p*j 
-, * 
r 
- 
(22) 
= e2U2 
mit 
¥ 
log x - y 
u h - — V -
1
 • 
ai 
Danach genügt es, die Optimalstellen der Funktion F. für den Spezialfall 
Uj = y^ = ° un<^ 
= 1 zu bestimmen. Der Aufwand, die optimalen Schicht-
grenzen aus der Formel (23) zu berechnen, läßt sich einsparen, wenn statt 
der Grenz&n x, jeweils der Anteil P, der h-ten Schicht an der Gesamtheit 
h 
h 
bzw. der Anteil 
V
V 
= 
pv 
( 2 4 ) 
v=l 
der ersten h Schichten an der Gesamtheit für h=l,...,L-l betrachtet wird; 
diese Anteile sind nach Formel (22) von v^, nach Formel (20) auch von 
¥ 
¥ 
Uj und Oj 
unabhängig. 
Aus der von Dalenius und Hodges (1957) entwickelten Näherungsformel für 
optimale Schichtgrenzen bei Neymanscher Aufteilung, die eine Auswahl mit 
Zurücklegen und die Übereinstimmung der Schichtungsvariablen X mit der 
Untersuchungsvariablen Y unterstellt, folgt für die Lognormalverteilung mit 
den Parametern 
(o,o,l,a*,l) die Formel 
log x^ = a* + /2 «T1 (h/L) 
, 
h=l,...,L-l 
. 
(25) 
Darin bezeichnet <|>-1 die Inverse der Verteilungsfunktion <j> der Standard-
normalverteilung. Daraus folgt in diesem Spezialfall für die Anteilswerte 
Ao ( Xh' = 
<t'(a2 + ^ " <t>""1 (h/L) j 
(26) 

240 
Karl-August Schäffer 
Bei sehr kleinem Gesamtauswahlsatz f und Korrelationsparameter p 
= 1 
hängen also die optimalen Anteilswerte für die ersten h Schichten an der 
Gesamtheit nur von dem Verhältnis h/L und von dem Parameter a* der biva-
riaten Lognormalverteilung ab. 
Nach dieser Formel kann A Ix1] als Funktion von a* in einem regulären Wahr-
em h 
^ 
2 
scheinlichkeitsnetz mit der Abszisse a a l s eine Schar von Geraden mit 
dem Scharparameter h/L dargestellt werden (vgl. Schäffer 1971, Abbil-
dung 1) . 
2.4. Verfahren zur Berechnung der optimalen Schichtgrenzen 
Die optimalen Schichtgrenzen für eine Kombination der Parameter (L,f,02,P ) 
sind gleich der Stelle, an der die in Formel (21) definierte Zielfunktion 
ihr globales Minimum annimmt. Für das Lösen dieses Optimierungsproblems 
sind Suchverfahren eingesetzt worden, weil die für Gradientenverfahren be-
nötigten Ableitungen der Zielfunktion verhältnismäßig rechenaufwendig sind. 
Zunächst wurde die von Hooke und Jeeves (1961) angegebene Suchstrategie 
verwendet; sie ist später durch das Verfahren von Neider und Mead (1965) 
ersetzt worden. 
Als Anfangsnäherung erwiesen sich für beide Verfahren die durch Formel (25) 
definierten Werte als vorteilhaft. Die von diesem Startpunkt ausgehende 
Suche führt in der Regel zu einem Ergebnis, das durch die anschließende 
zweite Suche nur in Ausnahmefällen noch verbessert werden kann. 
Nach dem Abschluß der Prozedur für die Zielfunktion F^ bleibt zu prüfen, 
ob die dem Optimum entsprechende Aufteilung die Bedingungen (16) erfüllt. 
Andernfalls muß das Rechenergebnis verworfen und durch das entsprechend 
berechnete Optimum der Zielfunktion F ersetzt werden. 

Optimale Schichtabgrenzung bei optimaler Aufteilung 
241 
3• 
ERGEBNISSE 
3.1 
Optimale Schichtabgrenzung bei Neymanscher Aufteilung 
Die optimalen Schichtgrenzen bei Neymanscher Aufteilung, die ihnen ent-
sprechenden Anteilswerte und das Reduktionsmaß nach Formel (9) sind für 
alle Kombinationen der folgenden Parameterwerte berechnet worden: 
- Anzahl der Schichten 
L=2,3,...,7 
- Dispensionsparameter 
a*=0.4,0.6, . . . , 1.6 
- Korrelationsparameter 
p*=0.4,0.5,...,1.0 
- Gesamtauswahlsatz 
f=0.Ol,0.02,0.05,0.10,0.15,0.20 
Einen kleinen Ausschnitt aus den Ergebnissen zeigt die Tabelle in Anhang 1 
dieser Arbeit. Die Trennlinien in dieser Tabelle machen die Bereiche kennt-
lich, in denen die Randschicht repräsentativ zu erfassen ist bzw. in denen 
eine Totalerfassung der Randschicht vorgesehen wird, weil sich dafür bei 
Neymanscher Aufteilung ein Auswahlsatz über 60 % ergibt. 
Ein Blick in den Anhang 1 zeigt, daß die Anteilswerte der Schichten nicht 
¥ 
nur vom Gesamtauswahlsatz und von dem Parameter a a b h ä n g e n (vgl. Schäffer 
(1971)), sondern in starkem Maße auch von der Korrelation zwischen der 
Schichtungsvariablen und der Untersuchungsvariablen beeinflußt werden. Die 
für den Fall der Übereinstimmung von Schichtungs- und Untersuchungsvariable 
(dh. für p =1) berechneten optimalen Schichtgrenzen sind also im Fall 
¥ 
p < 1 keineswegs optimal; sie können bei schwacher Korrelation zwischen 
den beiden Variablen sogar recht ungünstig sein. 
Die Abhängigkeit der optimalen Schichtanteile von den Parametern f, a 
* 
und p 
zeigt Abbildung 1 am Beispiel von L=3 Schichten in einen Wahrschein-
lichkeitsnetz. Für den Gesamtauswahlsatz f=0.01 und Korrelationsparameter 
* 
p =1 erhält man in diesem Netz die nach Formel (26) zu erwartende Gerade, 
* 
für p < 1 dagegen schwach gekrümmte Graphen, die unter der Geraden für 
* 
p =1.0 liegen. Daraus folgt, daß der Anteil der Randschicht (h=L) umso 
größer gewählt werden muß, je geringer die Korrelation zwischen den Varia-
blen ist. 

242 
Karl-August Schäffer 
Abbildung 1: Optimale Schichtanteile für L=3 Schichten 
bei Neymanscher Aufteilung 
Die Graphen für den Gesamtauswahlsatz f=0.10 (im rechten Teil der Abbil-
dung 1) liegen unter den entsprechenden Graphen für f=0.01 (im linken Teil 
der Abbildung). Der Umfang der optimal abgegrenzten Randschicht wächst also 
mit dem Gesamtauswahlsatz. 
3.2 Optimale Schichtabgrenzung bei modifizierter Neymanscher Aufteilung 
Für spezielle Aufgabenstellungen wird eine Modifikation der Neymanschen 
Aufteilung angewandt, bei der von vornherein die Totalerfassung der Rand-
schicht mit der Nummer h=L vorgeschrieben und der restliche Stichproben-
umfang optimal auf die Schichten h=l,...,L-l aufgeteilt wird. 
Optimale Schichtgrenzen für die so modifizierte. Neymansche Aufteilung 
sind für alle in Abschnitt 3.1 genannten Parameterkonstellationen berech-
net worden. Einige Ergebnisse für L=3 Schichten sind in Anhang 2 zusammen-
gestellt. 

Optimale Schichtabgrenzung bei optimaler Aufteilung 
243 
Die optimalen Grenzen für die modifizierte Aufteilung unterscheiden sich 
wesentlich von den entsprechenden Resultaten für die Neymansche Aufteilung, 
soweit diese nicht zu einer Totalerfassung der Randschicht führt. Im all-
gemeinen muß bei der modifizierten Aufteilung die erste Schicht deutlich 
größer und die Randschicht erheblich kleiner gewählt werden als bei 
Neymanscher Aufteilung. Die Tabelle in Anhang 2 zeigt ferner, daß die opti-
ci 
malen Schichtanteile auch vom Korrelationsparameter p abhängen, allerdings 
nicht so stark wie im Falle der Neymanschen Aufteilung. 
3.3 
Abhängigkeit des Schichtungseffektes von der Zahl der Schichten 
Der Schichtungseffekt, der durch das in Formel (9) definierte Reduktionsmaß 
Q quantifiziert wird, hängt wesentlich von der Zahl L der Schichten ab. 
Die Funktion Q(L) erscheint in einem doppelt-logarithmischen Netz als 
Gerade, sofern, bei kleinem Gesamtauswahlsatz 
f eine Neymansche Aufteilung 
vorgenommen wird und die mit der Schichtungsvariablen identische Unter-
suchungsvariable lognormal verteilt ist (vgl. Schäffer (1971), Seite 110). 
Hier soll die Frage untersucht werden, wie sich die Funktion Q(L) verhält, 
wenn die beiden Variablen der bivariaten Lognormalverteilung folgen. 
« 0.6 
<d 
S 
in 
c 
o 
•H 
•P 
3 
•O 
0.15 
NEYMANsche 
Aufteilung 
2 
3 
5 6 7 
Anzahl der Schichten 
Modizifierte 
: IE Yr 'IAii s c h e Aufteilung 
2 
3 
< 1 5 
6 7 
Anzahl der Schichten 
(0 
s 
Ol 
e 
0 
•rH 
-U 
3 
T3 01 
« 
0.15 
Abbildung 2: Schichtungseffekt bei optimaler Schichtabgrenzung 

244 
Karl-August Schäffer 
Nach dem linken Teil der Abbildung 2 kann der Schichtungseffekt durch eine 
Vergrößerung der Zahl der Schichten nicht wesentlich gesteigert werden, 
wenn die Untersuchungsvariable nur schwach mit der Schichtungsvariablen 
korreliert ist. Selbst bei p = 0.9 führt eine Erhöhung der Schichtzahl nur 
zu verhältnismäßig schwach verbesserten Resultaten. 
Diese Feststellungen für den Schichtungseffekt bei Neymanscher Aufteilung 
gelten im wesentlich auch für den Fall, in dem eine modifizierte 
Neymansche Aufteilung vorgeschrieben wird. Nach Abbildung 2 (rechter Teil) 
vermindert die Modifikation den Schichtungseffekt ganz allgemein. Die von 
der Modifikation verursachten Nachteile nehmen jedoch erwartungsgemäß mit 
wachsender Zahl der Schichten ab. 
Tabelle 1: Vergrößerung der Standardfehler der Schätzfunktion 
durch die Modifikation der Neymanschen Aufteilung 
für 
= 1.0 
Vergrößerungsfaktor der Standardfehler bei 
Anzahl L 
f = 0.01 
f = 0.20 
der Schichten 
der Schichten 
* 
p =0.6 
* 
p =0.8 
P =1 .O 
* 
P =0.6 
* 
p =0.8 
* 
p =1 .0 
2 
1.217 
1 .443 
1 .851 
1 .138 
1 .157 
1 .000 
3 
1.062 
1 .140 
1 .388 
1 .024 
1 .000 
1 .000 
4 
1.028 
1.064 
1 .242 
1 .011 
1 .000 
1 .000 
5 
1 .013 
1 .033 
1 .167 
1 .000 
1 .000 
1 .000 
6 
1.008 
1 .021 
1 .120 
1 .000 
1 .000 
1 .000 
7 
1.005 
1 .013 
1.093 
1 .000 
1 .000 
1 .000 
Nach Tabelle 1 bewirkt die Modifikation des Neymanschen Aufteilungsverfah-
rens in vielen Fällen eine erhebliche Vergrößerung der Standardfehler. Der 
Nachteil ist jedoch umso kleiner, je größer der Gesamtauswahlsatz 
f und 
je größer die Zahl der Schichten gewählt wird. 

Optimale Schichtabgrenzung bei optimaler Aufteilung 
245 
Literatur 
Aitchison, J., und Brown, J.A.C. (1957), The Lognormal Distribution, 
University Press Cambridge. 
Bühler, W. , und Deutler, T. (1974), Zur Berechnung kosten- und varianzmini-
maler Schichtungen mit Hilfe der dynamischen Programmierung, 
Proceedings in Operations Research, 311-322. 
Dalenius, T. (1950), The Problem of Optimum Stratification, Skand. 
Aktuarietidskrift, 33_, 203-213. 
Dalenius, T., und Hodges, J.L. (1957), The Choice of Strafification Points, 
Skand. Aktuarietidskrift, 40, 198-203. 
Deutler, T. (1976/77), Die Bestimmung optimaler Schichtungen - Ein Verfah-
rensvergleich, Jahrbücher für Nationalökonomie und Statistik, 191, 
153-173. 
Ekman, G. (1959), An Approximation Useful in Univariate Stratification, 
Ann. Math. Stat., 30, 219-229. 
Hooke, R., und Jeeves, T.A. (1961), 'Direct Search' Solutions of Numerical 
and Statistical Problems, Journ. of the Assoc. for Computing 
Machinery, 8_, 212-229. 
Neider, J.A., und Mead, R. (1965), A simplex method for function minimi-
zation, The Computer Journal, 7_, 808-213. 
Neyman, J. (1934), On the two different aspects of the representative 
method: the method of stratified sampling and the method of purposive 
selection, Journ. Roy. Stat. Society, 97, 558-606. 
Schäffer, K.-A. (1971), Optimale Schichtabgrenzung bei logarithmischer Nor-
malverteilung, Metrika, 
98-115. 
Schneeberger, H. (1971), Optimierung in der Stichprobentheorie durch 
Schichtung und Aufteilung, Unternehmensforschung, 
240-254. 
Sethi, V.K. (1963), A Note on Optimum Stratification of Populations for 
Estimating the Population Means, Australian Journal of Statistics, 
5_, 20-33. 
Strecker, H. (1957), Moderne Methoden in der Agrarstatistik, 
Physica Verlag, Würzburg. 
Tschuprow, A.A. (1923), On the mathematical expectation of the moments of 
frequency distributions in the case of correlated observations, 
Metron, 2_, 461-493, 646-683. 
Zindler, H.J. (1956), Über Faustregeln zur optimalen Schichtung bei Normal-
verteilung, Allg. Stat. Archiv, 40, 168-173. 

246 
Karl-August Schäffer 
Anhang 1: 
Qptimale Schichtabgrenzung bei Lognormalverteilung 
bei Neymanscher Aufteilung des Stichprobenumfanges 
Anteil der ersten h Schichten an der 
Reduktions-
Gesamtheit bei Gesamtauswahlsatz 
maß 
* 
J-l 
H 
n 
y 
0.01 
0.02 
0..05 
O. 10 
0.15 
0-, 20 bei f=0.01 
* 
a2 = 0,4 
2 
0 4 
i 
0 563 
0 563 
0 562 
0. 561 
0 560 
0 558 
0 944 
0 6 
i • 0 595 
0 594 
0 593 
0. 591 
0 589 
0 587 
0 870 
0 8 
i 
0 625 
0 625 
0 623 
0. 620 
0 617 
0 614 
0 757 
0 9 
i 
0 640 
0 639 
0 638 
0. 634 
0 631 
0 627 
0 682 
1 0 
i 
0 655 
0 654 
0 652 
0. 648 
0 644 
0 639 
0 588 
* 
° 2 
= 
.0 
2 
0 4 
i 
0 655 
0 655 
0 654 
0. 653 
0 652 
0 650 
0 920 
0 6 
i 
0 725 
0 725 
0 724 
0. 722 
0 719 
0 717 
0 822 
0 8 
i 
0 788 
0 787 
0 785 
0. 781 
0 777 
o 773 
0 687 
0 9 
i 
0 815 
0 814 
0 812 
0. 807 
0 801 
0 796 
0 605 
1 0 
i 
0 840 
0 839 
0 836 
0. 829 
0 822 
0 900 
0 510 
* 
°2 
= 
.6 
2 
0 4 
i 
0 739 
0 739 
0 738 
0. 738 
0 737 
0 737 
0 866 
0 6 
i 
0 831 
0 831 
0 831 
0. 830 
0 829 
0 828 
O 721 
0 8 
i 
0 899 
0 899 
0 898 
0. 896 | 0 930 
0 900 
0 549 
0 9 
i 
0 925 
0 924 
0 922 
o. 953 
0 921 
0 885 
0 459 
1 0 
i 
0 944 
0 943 
0 941 
o. 947 
0 912 
0 874 
0 368 
* 
a2 = 0.4 
3 
0 4 
i 
0 327 
0 325 
0 324 
0. 323 
0 321 
0 320 
0 928 
2 
0 780 
0 779 
0 778 
0. 777 
0 776 
0 775 
0 6 
1 
0 356 
0 355 
0 354 
0 352 
0 349 
0 347 
0 831 
2 
0 802 
0 801 
0 801 
0. 799 
0 796 
0 794 
0 8 
1 
0 388 
0 388 
0 386 
0 382 
0 379 
0 374 
0 679 
2 
0 822 
0 821 
0 820 
0. 817 
0 813 
0 810 
0 9 
1 
0 407 
0 406 
0 404 
0 400 
0 395 
0 390 
0 569 
2 
0 829 
0 829 
0 827 
0. 823 
0 819 
0 814 
1 0 
1 
0 437 
0 437 
0 435 
0 431 
0 426 
0 420 
0 414 
2 
0 829 
0 828 
0 825 
0 820 
0 813 
0 805 

Optimale Schichtabgrenzung bei optimaler Aufteilung 
247 
noch Anhang 1: Optimale Schichtabgrenzung bei Lognormalverteilung 
bei Neymanscher Aufteilung des Stichprobenumfanges 
Anteil der ersten h Schichten an der 
Reduktions-
Gesamtheit bei Gesamtauswahlsatz 
maß 
T 
y. 
Li 
P 
n 
y 
O.Ol 
0.02 
0.05 
0.10 
0.15 
0.20 
bei f=0.01 
* 
°2 
= 
.0 
3 
0 4 
i 
0 414 
0 414 
0 413 
0 . 412 
0 411 
0 408 
0.898 
2 
0 844 
0 844 
0 843 
0 . 842 
0 842 
0 840 
0 6 
1 
0 492 
0 491 
0 490 
0 . 487 
0 483 
0 479 
0.774 
2 
0 887 
0 887 
0 886 
0 . 884 
0 882 
0 880 
0 8 
1 
0 572 
0 571 
0 567 
0 . 560 
0 551 
0 606 
0.599 
2 
0 921 
0 920 
0 919 
0 . 915 
0 911 
0 949 
0 9 
1 
0 612 
0 610 
0 605 
0 . 595 
0 644 
0 599 
0.487 
2 
0 934 
0 933 
0 930 
0 . 926 
0 957 
0 933 
1 0 
1 
0 661 
0 658 
0 650 
0 . 635 
0 644 
0 597 
0.345 
2 
0 942 
0 940 
0 936 
0 . 928 
0 936 
0 903 
* 
°2 
= 
.6 
3 
0 4 
1 
0 509 
0 509 
0 508 
0 . 506 
0 506 
0 505 
0.832 
2 
0 896 
0 896 
0 896 
0 . 895 
0 
895 
0 895 
0 6 
1 
0 629 
0 628 
0 627 
0 . 625 
0 673 
0 628 
0.654 
2 
0 944 
0 944 
0 943 
0 . 942 
0 967 
0 945 
0 . 8 
1 
0 738 
0 736 
0 732 
0 . 729 
0 674 
0 624 
0.449 
2 
0 972 
0 972 
0 971 
0 . 970 
0 945 
0 917 
0 . 9 
1 
0 785 
0 783 
0 800 
0 . 731 
0 674 
0 622 
0.341 
2 
0 981 
0 980 
0 986 
0 . 963 
o 935 
0 903 
1 0 
1 
0 832 
0 827 
0 808 
0 . 739 
0 681 
0 629 
0.225 
2 
0 986 
0 985 
0 980 
0 . 950 
0 916 
0 879 

248 
Karl-August Schäffer 
Anhang 2: 
Optimale Schichtabgrenzung bei Lognormalverteilung 
für eine modifizierte Neymansche Aufteilung des 
Stichprobenumfanges 
L 
* 
h 
Anteil der ersten h Schichten an der 
Gesamtheit bei Gesamtauswahlsatz 
Reduktions-
maß 
Q 
aei f=0.01 
L 
* 
h 
0.01 j 
° - 0 2 
0.05 
0. 10 
0.15 
0.20 
Reduktions-
maß 
Q 
aei f=0.01 
* 
a2 = 0 4 
3 
0. 4 
1 
0 563 
0 563 
0 562 
0 561 
0 559 
0. 558 
0.944 
2 
1 000 
1 000 
1 000 
1 000 
1 000 
1 000 
0. 6 
1 
0 594 
0. 594 
0 593 
0 590 
0.. 583 
0. 574 
0.870 
2 
1 000 
1. 000 
1 000 
1 000 
0. 999 
0. 997 
0. 8 
1 
0 625 
0 624 
0 621 
0 607 
0 587 
0. 566 
0.757 
2 
1 000 
1 000 
1 000 
0 997 
0 992 
0. 983 
0. 9 
1 
0 640 
0 638 
0 630 
0 608 
0 582 
0 555 
0.682 
2 
1 000 
1 000 
0 999 
0 993 
0 983 
0 969 
1 0 
1 
0 654 
0 652 
0 636 
0 605 
0 573 
0 541 
0.588 
2 
1 000 
1 000 
0 996 
0 983 
0 964 
0 940 
* 
°2 = 1 0 
3 
0 4 
1 
0 655 
0 654 
0 654 
0 649 
0 638 
0 622 
0.920 
2 
1 000 
1 000 
1 000 
1 000 
0 998 
0 995 
0 6 
1 
0 725 
0 723 
0 711 
0 684 
0 652 
0 619 
0.822 
2 
1 000 
1 000 
0 999 
0 995 
0 988 
0 976 
0 8 
1 
0 781 
0 771 
0 741 
0 693 
0 649 
0 606 
0.683 
2 
1 000 
0 999 
0 996 
0 985 
0 970 
0 949 
0 9 
1 
0 802 
0 787 
0 748 
0 693 
0 644 
0 599 
0.593 
2 
1 000 
0 998 
0 993 
0 978 
0 957 
0 933 
1 0 
1 
0 818 
0 799 
0 754 
0 694 
0 644 
0 597 
0.479 
2 
0 999 
0 997 
0 987 
0 964 
0 936 
0 903 
* 
°2 = 1 .6 
3 
0 9 
1 
0 .878 
0 .854 
0 .800 
0 .731 
0 674 
0 .622 
0.395 
2 
0 .999 
0 996 
0 .986 
0 .963 
0 935 
0 903 
1 0 
1 
0 .889 
0 .864 
0 .808 
0 .739 
0 681 
0 629 
0.266 
i 
2 
0 .998 
0 .994 
0 .980 
0 .950 
0 .916 
0 .879 

Nichtparametrische Tests auf Zufälligkeit 
bei verschiedenen Stufen der Diskretisierung 
von Beobachtungsfolgen 
Rainer Schlittgen 
O. EINLEITUNG 
Es ist ein altes, in vielen Bereichen auftauchendes Problem, eine zeit-
lich geordnete Folge 
von Beobachtungen auf Zufälligkeit hin 
zu überprüfen. Dabei wird unter Zufälligkeit verstanden, daß die zugrun-
deliegenden Zufallsvariablen 
stochastisch unabhängig und iden-
tisch verteilt sind. Zu dieser Nullhypothese der Zufälligkeit sind die 
verschiedensten Alternativhypothesen denkbar. 
Die am häufigsten betrachteten sind (A) das Vorliegen eines monotonen 
Trends und (B) das Vorhandensein einer seriellen Korrelation. Es gibt 
weiterhin Fälle, wo keine spezifische Vorstellung über die mögliche Art 
der Verletzung der Zufälligkeit vorliegt, aber (C) jede Form der Abwei-
chung von der Nullhypothese relevant ist. 
Zu dem letzten Bereich gehört z.B. die Frage nach der Zuverlässigkeit von 
medizinischen Analyseautomaten. Als Mindestanforderung ist hier zu ver-
langen, daß die mit einem Automaten "im Dauerlauf" hintereinander durch-
geführten Messungen derselben Substanz eine Folge von Meßwerten produ-
ziert, die unter Berücksichtigung der Meß(un)genauigkeit genau die oben 
spezifizierte Form der Zufälligkeit besitzt. 
Wenn die Verteilung der X nicht bekannt ist oder die 
möglicherweise 
stärker zu extremen Werten neigen als unter der Normalverteilungsannahme, 
ist die Verwendung nichtparametrischer Tests auf Zufälligkeit angezeigt. 

250 
Rainer Schlittgen 
Es gibt nun zahlreiche nichtparametrische Tests auf Zufälligkeit, insbe-
sondere gegen die Alternativen (A) und (B), vgl. etwa Lienert (1973, 
1978). Eine Klasse von Tests beruht dabei auf der Dichotomisierung der x-
Werte entlang dem Median. Diese Dichotomisierung, das hard-clipping, ist 
die stärkste Form der Informationsreduktion, bei der noch statistische 
Aussagen möglich sind. Als schwächste kann dabei der Übergang zu Rang-
werten angesehen werden. Es ist naheliegend, nach Zwischenstufen zu fra-
gen und den Grad der Informationsreduktion anhand der Güte oder Effizienz 
entsprechender Tests zu vergleichen. 
Dies soll hier anhand von jeweils einem Test zu den drei angeführten Al-
ternativen durchgeführt werden. Als Zwischenstufe wird dabei das Clipping 
der x-Werte bezüglich der Quartile betrachtet, bei dem die Originalreihe 
in eine Folge von vier verschiedenen Werten transformiert wird, die die 
Lage der x-Werte bezüglich der Quartile kennzeichnen. 
Ausgegangen wird also von einer Folge von Beobachtungen x 
auf die 
ein für die entsprechende Alternative geeigneter Test anzuwenden ist. Je-
weils derselbe, soweit nötig modifizierte Test, ist dann auf die folgen-
den Reihen anzuwenden: 
- Reihe der Rangwert R , 
t 
= 
jIx £ x , 
j=l,...,N } 
L 
J 
"C 
( 1 ) 
- Reihe der entlang den Quantilen "geclippten" Werte y 
-3 
( 2 ) 
3 
x 0.75 < x t 

Nichtparametrische Tests auf Zufälligkeit 
251 
Reihe der entsprechend der Lage zum Median dichotomisierten Werte 
Zt 
-1 
x < x 
t 
0.5 
1 
x > x 
t 
0.5 
(3) 
Da es in dieser Arbeit im wesentlichen um den Vergleich der verschiedenen 
Transformationen geht, werden bei den Transformationen (2) und (3) jeweils 
die theoretischen Prozentpunkte zugrunde gelegt. 
Nur für den Fall der Trendalternative werden theoretische Resultate über 
die asymptotische relative Effizienz abgeleitet. Für die Autokorrelations-
alternativen wird wegen der zum Teil beträchtlichen theoretischen Schwie-
rigkeiten ein Vergleich mit geeigneten Monte-Carlo-Simulationen durchge-
führt. 
1 • TRENDALTERNATIVEN 
Ein einfacher Ansatz, um das Testproblem "Zufälligkeit gegen Trend" zu 
formalisieren, besteht in der folgenden Spezifikation von Null- und 
Gegenhypothese: 
H 
: X = a + U 
0 
t 
t 
(4) 
Hj : X = a + At + U 
wobei die U^, t=l,...,N unabhängige, indentisch (normal-)verteilte Zu-
fallsvariablen sind. Dafür sind Rangtests die effizientesten vertei-
lungsfreien Tests, siehe Stuart (1954). Der Spearman1sehe Rangkorrela-
tionstest und der Ra.ngkorrelationstest nach Kendall haben dabei dieselbe 
asymptotische relative Effizienz (A.R.E.). Hier wird der Test von Kendall 
betrachtet, da er sich in inhaltlich überzeugender Weise auf alle drei 
transformierten Reihen anwenden läßt. 

252 
Rainer Schlittgen 
Kenndalls Rangkorrelationstest basiert auf der Prüfgröße 
Q = I h 
s<t st 
(5) 
mit 
h 
=< 
st 
1 
X < X 
s 
t 
O 
X > x^ 
s — t 
(6) 
Q ist asymptotisch normalverteilt mit 
E ( Q ) 
(7) 
V(Q) = (N-l)N(2N+5) 
72 
siehe Kendali und Stuart (1976, S. 371ff) 
Diese Momente sind allerdings unter der Voraussetzung abgeleitet, daß die 
X stetige Zufallsvariablen sind und somit keine Bindungen (ties) auf-
treten können. Daher sind sie nur für die Reihe X bzw. für die Reihe der 
Rangwerte R korrekt, weil in Q nur die in den Rängen enthaltene In-
formation eingeht. Wird die auf die Ränge angewendete Statistik mit Q 
bezeichnet, so hat Q 
die Momente (7). 
Die Reihen 
und (Z ) , die aus X 
mit den Transformationen (2) und 
(3) gewonnen werden, sind nun Folgen von diskreten Zufallsvariablen, bei 
denen ties wesentlich vorkommen. Für sie sind die Momente der Teststatis-
tik neu zu bestimmen. 
Exemplarisch wird die Bestimmung der Momente unter H^ für die Reihe 
skizziert. 
Hier gilt 

Nichtparametrische Tests auf Zufälligkeit 
253 
h . = < 
st 
-1, 
Zfc = 1 
sonst. 
Damit ist zunächst 
E (h J = P (h =1) = P(Z = -i,Z =1) = - . 
st 
st 
s 
t 
4 
Weiter ist, wenn Q die auf (z ) angewendete Teststatistik ist: 
z 
t 
N t-1 
E(QZ) = E( Z h ) = E 
l - = 
s<t 
t=2 s=l 
1 
(N-l)N 
(8a) 
2 
Zur Bestimmung der Varianz wird von der Zerlegung Var(Qz> = E(QZ>-E(QZ) 
ausgegangen. Zunächst erhält man für s<t, u<v elementar: 
E (h h ) 
st uv 
1 
16 
S = U, t = V 
s = u, t * v oder s * u, t = v 
s * u, v und t * u, v 
E(QZ) ergibt sich nun durch Aufspalten der Summe 
N 
N t-1 v-1 
E (Q_) = Z Z Z Z E(h h ) 
Z 
t=2 v=2 s=l u=l 
S t U V 
in die drei Teilsummen, die durch v<t, v=t und v>t charakterisiert 
werden, und unter ausgiebigem Gebrauch der bekannten Formeln 
N
N 
N 
2 
3 
2 
2 
z t = N(N+l)/2, 
z t = N (N+l) (2N+l)/6 und 
Z t = N (N+l) /4 . 
t=l 
t=l 
Als Varianz von Q^ erhält man schließlich 
t=l 
Var (Q 
(N-l)N(2N+5) 
96 
(8b) 

254 
Rainer Schlittgen 
Analog berechnet man für die Teststatistik Q^, d.i. die auf die Reihe 
Y1''""'YN z u 9 e s c h n i t t e n e Prüfgröße Q: 
«V 
- 
^ 
(9) 
5 (N-1) N (4N+1) 
Var(Q) 
y 
384 
Ein Vergleich dieser Tests untereinander kann nun mit Hilfe der A.R.E. er-
folgen. Die A.R.E. 
(Q.,Q.) 
gibt das asymptotische Verhältnis der Stich-
probenumfänge an, bei denen die auf Q^ und Q_. beruhenden Tests die 
gleiche (asymptotische) Güte haben. Die analog zu Cox und Stuart (1955) 
durchgeführten Rechnungen ergeben zusammen mit den dortigen Angaben die 
folgende Aufstellung: 
Tabelle 1.: A.R.E. (TEST 1, TEST 2) 
"\^Test 2 
Test 1 ^ ^ ^ 
opt. Test 
q , q r 
q y 
2 Z 
q , Q r 
0.985 
1 
ÖY 
0.929 
0.94 
1 
2 z 
0.86 
0.87 
0.926 
1 
Zunächst ist zu beachten, daß diese Werte nur für das zu Beginn des Ab-
schnittes formulierte Regressionsmodell gelten, unter Berücksichtigung 
der Normalverteilung der U . Dafür ist der Regressionstest optimal. Er 
ist als opt. Test mit berücksichtigt worden. 
Während also der Übergang zu Rangwerten praktisch keinen Güteverlust be-
deutet, A.R.E. (opt. Test, Q) = 0.985, ist die Informationsreduktion bei 
den weiteren Transformationen beachtlicher: Man benötigt einen ca. 6% 
größeren Stichprobenumfang bei Q als bei Q r und einen ca. 7% größeren 
bei Q z als bei Q^, um dieselbe Testschärfe zu haben. Bei den drei 
informationsreduzierenden Transformationen steht das Clipping bezüglich 
der Quartile also etwa genau in der Mitte. Dies ist umso 

Nichtparametrische Tests auf Zufälligkeit 
255 
bemerkenswerter, als die transformierte Reihe 
auch nur eine Folge von 
vier verschiedenen Werten ist, und die vorletzte Stufe der Informations-
reduktion vor der durch hard-clipping entstandenen Reihe 
z 
darstellt. 
2. AUTOKORRELATIONSALTERNATIVEN 
Der serielle Autokorrelationstest, der auf der Teststatistik 
1 N - ! 
_ 
- 
i 
N 
_ 2 
r = - 
E (X-X) U.-X)/- 
l (X -X) 
(10) 
N
 t=l 
t + 1 
N t=l 
basiert, ist sensitiv gegen Korrelationen von jeweils zwei aufeinander-
folgenden Werten der Folge 
Um diese-Autokorrelation zu formalisieren, wird hier das Modell eines 
autoregressiven Prozesses erster Ordnung, kurz AR(1)-Prozeß, gewählt. 
Dafür gilt die Definitionsgleichung 
x t = p V i 
+ ü t 
' 
wobei die 
U 
unkorrelierte, identisch verteilte Zufallsvariablen sind, 
die hier zudem als normalverteilt mit 
E(U^)=0 vorausgesetzt werden. 
Das Testproblem Zufälligkeit versus Autokorrelation kann dann einfach 
durch 
H Q : p = 0 , 
H 
: p * 0 
(12) 
erfaßt werden. 
Die Prüfgröße 
r 
ist asymptotisch normalverteilt, siehe Knoke (1977), 
McNeil (1967). Bei der Anwendung von 
r 
auf die vier hier betrachteten 
Reihen, ergeben sich allerdings unterschiedliche Momente für 
r. Wenn 
r ,r ,r und r 
die auf die jeweilige Reihe angewendete Prüfgröße 
X 
K 
I 
Z 

256 
Rainer Schlittgen 
bezeichnet, so gilt unter H q : 
E(r ) = - 1/N 
, 
Var (r ) = (N-2) 2/N 2 (N-l) 
(13a) 
E (r ) = 0 
, 
Var (r ) « (N-3) (5N+6) /5 (N-l) 2 (N+l) 
(13b) 
R 
R 
E(r ) = 0 
, 
Var(ry) = (N-l)/N2 
(13c) 
E (r ) = O 
, 
Var (r ) = (N-l)/N2 
(13d) 
z 
z 
Bei der Bestimmung der Momente von r y und r w u r d e wieder davon aus-
gegangen, daß den Transformationen (2) und (3) die theoretischen Quartile 
zugrundeliegen. Bei der Standardisierung der Prüfgröße wurden ebenfalls 
die theoretischen Varianzen verwendet. 
Die A.R.E. von r R bzgl. r^ hat die untere Grenze 0.86 und beträgt 
0.91 für Normalprozesse, siehe Knoke (1977). Eine untere Grenze für die 
A.R.E. von r 
bzgl. r 
bei schwach stationären Normalprozessen ist 
Z 
X 
4/tt2 « 0.4, vergleiche McNeil (1967). 
Wegen r z = I Z ^ ^ / N = [ i N _ 1 _ 1
+ N
1 " 
(N_! i+Ni_ j > ]/N = 
2(N_1_1+N 
)/N - (N-l)/N ist r z äquivalent zur Prüfgröße 
T = N _ i _ 1
+ N u 
des optimalen Tests auf Markov-Abhängigkeit der Z^ 
; dabei ist 
die Anzahl der Zeitpaare, bei denen die Realisation 
j auf die Realisa-
tion i folgt. Siehe dazu Schlittgen (1980). 
Wenn der Ausgangsprozeß (X^) ein AR(l)-Prozeß ist, so stellt (Z^) keine 
Markov-Kette dar, da die Autokorrelationsfunktion von Z 
dann nicht 
streng exponentiell abfällt. Die Autokorrelationsfunktion ist jedoch der 
einer Markov-Kette sehr ähnlich, so daß der optimale Test auf Markov-
Abhängigkeit auch für die hier vorliegende Situation geeignet ist. Die 
Ergebnisse des auf einer Monte-Carlo-Studie basierenden Vergleichs der 
vier Tests sind im folgenden Abschnitt aufgeführt. 

Nichtparametrische Tests auf Zufälligkeit 
257 
Im Fall nicht-spezifizierter Autokorrelationsalternativen wird die An-
wendung des Periodograromtests empfohlen, Knoke (1977), Schlittgen (1980). 
Das Periodogramm einer Zeitreihe ist dabei definiert als 
I N ( X ) = ^ | 
" (X -X)e i 2 T X t| 2 
(14) 
N 
t=l 
mit 0 < X < O.5. 
Wenn die X^ unkorreliert und normalverteilt sind, so haben die Statis-
tiken 
i 
M 
K. = 
Z I(j/N) / 
Z I(j/N) 
, i=l,. . . ,M-1, 
(15) 
1 
j=l 
j=l 
M = [N/2] + 1, dieselbe gemeinsame Verteilung wie die geordneten Statis-
tiken einer Zufallsstichprobe vom Umfang M-l aus einer 
(0,1)-Recht-
eckverteilung. Dementsprechend ist ein auf die K angewendeter 
Kolmogorov-Smirnov-Anpassungstest auf Gleichverteilung ein Test auf die 
Zufälligkeit der Folge 
. Die Prüfgröße des Tests ist dabei 
d = 
max 
max{|K.-i/N|,|K.-(i-1)/N|} 
(16) 
i=l,...,M-l 
1 
1 
Wenn, wie bei den Reihen 
und 
die Ausgangsreihe keinen Normal-
prozeß bildet, gelten die verteilungstheoretischen Überlegungen für die 
K^ nicht mehr exakt. Das Periodogramm hängt aber wegen 
N-l 
I(A) = c 
+ 2 Z c 
cos 2TTAT 
(17) 
0 
x— 1 
T 
1 N _ T 
über die Autokovarianzen 
c = — 
Z (X -X)(X^ -X) von der Ausgangsreihe 
T 
N 
t 
t+T 
ab. Da die c 
unter sehr schwachen Voraussetzungen asymptotisch normal-
verteilt sind, kann der Periodogrammtest zumindest approximativ auch auf 
nicht-normal verteilte Reihen angewendet werden. Siehe dazu auch 
Hannan (1967, S. 101). 
Knoke (1977) vergleicht mit Hilfe eines Simulationsexperimentes die Tests 

258 
Rainer Schlittgen 
r , r und den auf (X ) basierenden Periodograirantest d für verschiedene 
X R 
t 
Alternativen. Dabei kommt er zu dem Ergebnis: 
"When the autocorrelation structure and the parent distribution can be 
specified, the likelihood derivative test < r
1
= r
x' should be employed. 
Additionally, the likelihood derivative test is robust for moderate 
departures from parent normality. For extreme departures from parent 
normality, however, the R test can be more powerful than r^. The 
r^ test is also moderately robust for alternatives other than first-
order autocorrelation. For alternatives with potentially strong components 
of autocorrelation of unspecifiable higher order, a periodogram test is 
appropriate. In this case the d test is recommended regardless of the 
parent distribution." 
Hier soll dennoch der Vergleich, der auf die verschiedenen Reihen ange-
wandten Periodogrammtests am Beispiel von AR(1)-Reihen 
durchgeführt 
werden, da es vor allem um eine Einschätzung der Informationsreduktion 
bei den Übergängen X^ 
R 
Y^ 
geht. Es wird wieder durch die 
Indizierung d ,d ,d und d 
kenntlich gemacht, auf welche Reihe der 
X R Y 
Z 
Test angewendet wird. 
3. EMPIRISCHER VERGLEICH DER AUTOKORRELATIONSTESTS 
Um die angesprochene Informationsreduktion bei den verschiedenen Diskre-
tisierungsstufen abschätzen zu können, wurde eine Monte Carlo Simulation 
durchgeführt. Für verschiedene Stichprobenumfänge (N=16,32,64) und ver-
schiedene Parameterwerte (p=0,0.1,0.2,0.4,0.8) wurden jeweils 1000 
(für N=16) bzw. 500(für N=32,64) Realisationen des AR(1)-Prozesses 
X = p 
j + u 
erzeugt. Dabei waren die 
normalverteilte Zufalls-
zahlen. 
Die Güte der Tests wurde jeweils durch die relativen Anteile von Ableh-
nungen der Nullhypothese geschätzt. Die Standardabweichungen der ge-
schätzten Signifikanzlevels beträgt also ungefähr V(ß(1-p)/500)' bzw. 
V(p (1-p) /lOOOy. Für p = 0.05 ist dies » O.Ol bzw. « 0.007. 

Nichtparametrische Tests auf Zufälligkeit 
259 
Die hier durchgeführte Studie beschränkt sich auf Untersuchungen bei 
festem Signifikanzniveau a = 0.05. 
Bei den Rangtests wurde stets von den Ablehnbereichen ausgegangen, die 
sich durch die Normalapproximation ergeben. Als kritische Werte der Peri-
odogrammtests-Statistiken wurden dagegen die exakten, z.B. bei Büning 
und Trenkler (1978) tabellierten Prozentpunkte des Kolmogorov-Smirnov-
Anpassungstest verwendet. 
Tab. 1.: Güte der Tests gegen Autokorrelationsalternativen 
für N = 16 
Test 
P 
0.0 
0.1 
0.2 
0.4 
0.8 
r x 
0.049 
0.058 
0. 120 
0.278 
0.772 
rR 
0.040 
0.044 
0.054 
0. 151 
0.640 
r Y 
0.042 
0.062 
0. 110 
0.263 
0.694 
r z 
0.031 
0.047 
0.056 
0. 138 
0.693 
dx 
0.028 
0.019 
0.038 
0.050 
0.388 
dR 
0.031 
0.031 
0.032 
0.061 
0.394 
dY 
0.034 
0.026 
0.036 
0.064 
0.292 
dz 
0.048 
0.041 
0.042 
0.067 
0.286 

260 
Rainer Schlittgen 
Tab. 2.: Güte der Tests gegen Autokorrelationsalternativen 
für N = 32 
Test 
P 
0 
0. 1 
0 . 2 
0 . 4 
0 . 8 
r X 
0 . 0 5 8 
0 . 0 9 4 
0 18 
0. 56 
0 988 
r R 
0 . 0 6 8 
0 . 0 9 3 
0 13 
0. 463 
0 966 
r Y 
0 . 0 7 0 
0 . 0 9 2 
0 178 
0. 476 
0 952 
r z 
0 . 0 6 6 
0 . 0 9 
0 166 
0. 372 
0 936 
d x 
0 . 0 7 
0 . 0 2 8 
0 076 
0. 294 
0 934 
d R 
0 . 0 6 6 
0 . 0 4 8 
0 090 
0. 292 
0 934 
d Y 
0 . 0 5 4 
0 . 0 3 8 
0 072 
0. 244 
0 86 
d z 
0 . 0 5 4 
0 . 0 5 0 
0 066 
0 154 
0 658 
Tab. 
3 . : Güte der Tests gegen Autokorrelationsalternativen 
für N = 64 
Test 
P 
0 
0. 1 
0 . 2 
0 . 4 
0 . 8 
r x 
0 . 0 5 6 
0 . 0 9 4 
0 298 
0 876 
1 000 
r R 
0 . 0 4 4 
0 . 108 
0 288 
0 834 
1 000 
r Y 
0 . 0 4 8 
0 . 0 8 6 
0 220 
0. 784 
0 998 
r z 
0 . 0 6 6 
0 . 0 7 8 
0 142 
0 518 
0 
CO 
<71 
d X 
0 . 0 6 
0 . 0 6 2 
0 160 
0 750 
1 000 
d R 
0 . 0 4 4 
0 . 0 8 0 
0 230 
0 730 
1 000 
d Y 
0 . 0 5 2 
0 . 0 6 2 
0 124 
0 628 
0 996 
d z 
0 . 0 6 4 
0 . 0 7 0 
0 086 
0 366 
0 
CO 

Nichtparametrische Tests auf Zufälligkeit 
261 
Die hier durchgeführte Simulationsstudie ist nicht umfangreich genug, um 
eine umfassende Einschätzung der Stärke der Informationsreduktion bei den 
verschiedenen Diskretisierungsstufen geben zu können. Aus den vorliegen-
den Ergebnissen lassen sich aber folgende Tendenzen ablesen: 
Für kleinere Stichprobenumfänge (N=16,32) ergibt sich ein sehr uneinheit-
liches Bild.(Das schlechte Abschneiden des Rangkorrelationstests dürfte 
dabei auf die Verwendung der approximativen Ablehnbereiche zurückzufüh-
ren sein.) Bei Werten von p, die relativ nahe bei Null liegen, scheinen 
jedoch die verschiedenen Stufen der Diskretisierung kaum einen Effekt zu 
haben. Bei dem Periodogrammtest ist die Dichotomisierung nach den vorlie-
genden Werten den anderen Diskretisierungsstufen sogar in Bezug auf die 
Güte teilweise überlegen. 
Erst bei größerem Abstand der Alternativen von der Nullhypothese bzw. bei 
größerem N (= 64) zeigt sich der Informationsverlust durch die verschieden 
starke Diskretisierung deutlicher. Wie bei den Tests auf Zufälligkeit 
gegen Trendalternativen ist insbesondere hervorzuheben, daß die Diskre-
tisierung an den Quartilen in etwa eine Mittelstufe zwischen der Trans-
formation in Rangwerte und dem "hard-clipping" einnimmt. 
Das recht gute Abschneiden der Tests r^ und r^, d 
und d 
im Ver-
gleich zu r^ bzw. d x 
bei kleinen Stichprobenumfängen und der stärke-
re Effizienzverlust bei größeren N ist vergleichbar mit den Ergebnissen 
von Dixon (1953) bzgl. der power efficiency des Zeichentests. Dixon 
zeigte, daß die Effizienz des Zeichentests sehr hoch ist bei kleinen 
Stichprobenumfängen und bei größeren gegen den Grenzwert 
2/tt 
abfällt. 
Literatur 
Billingsley, P. (1961), Statistical methods in Markov chains, Ann. 
Mathe. Statist. , 32_, 12-40. 
Büning, H., G.Trenkler (1978), Nichtparametrische statistische Methoden, 
de Gruyter Verlag Berlin. 

262 
Rainer Schlittgen 
Cox, D.R., A. Stuart (1955), Some quick sign tests for trend in location 
dispersion; Biometrika, 4^2, 80-95. 
David, F.N. (1947), A power function for tests of randomness in a sequence 
of alternatives, Biometrika, 3jl, 335-339. 
Dixon, W.J. (1953), Power function of the sign test and power efficiency 
for normal alternatives, Ann.Math.Statist., 24, 467. 
Hannan, E.J. (1967), Time Series Analysis, Chapman and Hall, London. 
Kendall, M.G. , A. Stuart (1976), The advanced theory of statistics, 3_; 
Griffin & Co., London. 
Knoke, J.D. (1977), Testing for randomness against autocorrelation: 
Alternative tests, Biometrika, 6_4, 523-529. 
Lehmann, E.L. (1959), Testing Statistical Hypothesis, Wiley, New York. 
Lienert, G.A. (1973), Verteilungsfreie Methoden in der Biostatistik I; 
Verlag Anton Hain, Meisenheim am Glan. 
Lienert, G.A. (1978), Verteilungsfreie Methoden in der Biostatistik II; 
Verlag Anton Hain, Meisenheim am Glan. 
Mc Neil, D.R. (1967), Estimating the covarince and Spectral density 
functions from a clipped stationary time series, J.R. Statist 
29, 180-195. 
Schlittgen, R. (1980), Eine Klasse von nichtparametrischen Tests auf 
Zufälligkeit; Diskussionsarbeit Nr. 6/80 aus dem Institut für 
Quantitative Ökonomik und Statistik der Freien Universität Berlin. 
Stuart, A. (1954), The asymptotic relative efficiencies of distribution-
free tests of randomness against normal alternatives, J.Amer. 
Statist. Ass., 49, 147-157. 

A Linear Combination of Estimators in 
an Errors-in-Variables Model - A Monte Carlo Study 
Hans Schneeweiss and Horst Witschel 
1. INTRODUCTION 
This is a report on a Monte Carlo study in econometrics. While it started 
out as a conventional investigation into the sample properties of some old 
and one new estimator, it turned out to provide us with some unexpected 
insights into the structure of these estimators, leading to a more efficient 
modification of the original estimators. 
The model under investigation is the simple linear errors-in-variables 
model which, as is well known, cannot be consistently estimated unless addi-
tional information about the errors or about the systematic part of the 
exogenous variable is provided.*' Several consistent estimators using some 
kind of additional information have been proposed and have been used in 
practical applications of the model. If more information is available than 
is actually needed for merely constructing a consistent estimator, the 
problem arises of how to use all the information efficiently. 
Supposing the variance of the error variable of the exogenous variable is 
given and, in addition, an instrumental variable is known, then two con-
sistent estimators of the slope of the regression line can be defined: the 
adjusted least squares and the instrumental variable estimator. By linearly 
combining these two a more efficient estimator can be constructed, as was 
proposed by one of the authors (Schneeweiss) at the World Congress of the 
Econometric Society in Toronto (1975). The asymptotic variances of all three 
estimators can be computed and can be compared. 
As always when asymptotic properties of an estimator are known, the question 
arises to what degree these properties still hold true when the sample size 

264 
Hans Schneeweiss and Horst Witschel 
is taken to be small or of medium size. This question is typically answered 
by carrying out Monte Carlo experiments when analytical methods are not 
available or seem to be too unhandy to be of any use. Data originating from 
the model are simulated with the help of a random number generator and esti-
mates are computed. These estimates computed from a large number of repli-
cations of the same experiment are further analyzed. Their mean and variance 
over the replications are taken to be estimates of the expected value and 
theoretical variance of the estimator under investigation and can be com-
pared with the analytically derived asymptotic values. It was this type of 
analysis that we carried out. 
So far this study ran along conventional lines. The surprise came when we 
found that in several cases the computed variance of the estimator was ex-
tremely high as compared to the asymptotic value. Thus for our model asymp-
totic theory is useless when small or even medium size samples are involved, 
at least in particular cases. 
These findings prompted us to look for better estimators. Modified adjusted 
least squares and modified instrumental variable estimators have in fact 
been proposed in the literature, but, to our knowledge, they were never 
applied in practice. Also, the motivation of modifying these well-estab-
lished estimators was not to improve their small sample properties, but 
rather to bring them to the standards of Maximum Likelihood theory. Although 
this was the general belief for a long time (see Kendall and Stuart 1961 
and 1979), the conventional estimators are not true ML estimators, whereas 
the modified ones are. The modification is of the kind that changes an 
estimate only in those rare cases when the estimate assumes extremely large 
or small values, reducing it to a more "reasonable" value. 
These modifications can also be applied to the combined estimator. 
The asymptotic theory of the estimators is not changed by such (seemingly 
small) modifications, but the small sample properties show marked diffe-
rences. In fact when we repeated the Monte Carlo study with the modified 
estimators, agreement with the asymptotic theory was found to be much 
closer. We even noticed a slight but significant downward deviation of the 

Estimators in an Errors-in-Variables Model 
265 
small sample variance as compared to the asymptotic variance. Modifying the 
estimators apparently improves their precision considerably. 
2. THE MODEL 
We consider a simple linear model with errors in the variables: 
n = a + 
y = n + w 
x = c + v. 
The only observable variables are x and y. w and v are error variables with 
2 
2 
2 
2 
Ev = Ew = O and Ev = a . Ew = a . E(vw) = 0. 5 and n are unobservable va-
v 
w 
riables, and (v,w) is stochastically independent of 
We assume that a 
sample 
' t = 
is observed, which originated from the model 
such that the pairs ( v
t> w
t) a r e i.i.d. and independent of all the 
t=l,...,T... 
The problem is to estimate the unknown parameters a and B-
Thé following regression-type relation can be derived from the model: 
y = a + 8 x + u , 
u = w - 8v. 
Considering this equation, one might be tempted to estimate a and 6 by ordi-
nary least squares (OLS) estimators: 
s 
- 
xy 
- 
— 
— 
— 
fiOLS = s 
' 
aOLS = y ~ 6OLS X' 
xx 
(The following conventional notation is used for any sequence 
( V b t ' t = 1 
T : Sab = T ¿ 1
 (at-"a) ( bt" B ) ' a = 
¿ = T ^ ^ f 
also s = = = s 2 .) 
aa 
a 
However, as is well known, these estimators are generally inconsistent. More 
specifically, inconsistency can be proved if the following condition regar-
ding the asymptotic behaviour of £ is satisfied: 
2 
2 
(A) lim £=: m and lim s =: S,.>0 exist with probability 1. 
T>°o 
T>°° 

266 
Hans Schneeweiss and Horst Witschel 
3. THREE CONSISTENT ESTIMATORS 
Consistent estimators can be constructed if certain types of additional in-
formation are supplied to the model. We shall consider two such types of 
information. 
2 
Suppose the error variance a is known. Then under (A) 
s 
6 
:= — S -
ALS 
2 2 
s -a 
x v 
is a consistent estimator. We call 13 
the adjusted least squares (ALS) 
- 
2 ) 
estimator. 
On the other hand, suppose an instrumental variable z is given such that 
its sample values z ,...,z are independent of the error variables, but 
are asymptotically correlated with 
in the sense that 
plim s. =: S_ jt 0. 
Zz 
5z 
If, in addition, the following assumption is satisfied 
2 
2 
(B) lim s z =: S^ exists with probability 1, 
then the instrumental variable (IV) estimator 
8 - . v 
3) 
s 
- = 
'IV ' s xz 
is consistent.' 
In any case, a = y - 6x will be a consistent estimator if B is consistent 
and (A) is satisfied. In the sequel we will concentrate on the problem of 
estimating B. 
Now suppose that both types of information are available so that one has 
more information than is actually needed for consistent estimation. In or-
der to utilize all the information given in a possibly optimal way one is 
4) 
-
naturally lead to consider a linear combination 
of B 
and B : 
2 
_ 
ALS 
IV 
5 = a B 
+ b £$' . 
C 
ALS 
IV 
This combined estimator is consistent if a + b = 1. It is optimal if a is 
chosen such that B^ has least asymptotic variance. 

Estimators in an Errors-in-Variables Model 
267 
Assuming normal error variables v , the asymptotic variances of B 
and 
5) 
6 
are known 
to be 
IV 
v 
= 
1+ke 
- I 
^ 
(1-e)2 
T 
V 
= i . I 
IV 
d 
T 
where the following abbreviations have been used: 
2 . 2 
^ 
2 . 2 
, 
2 
2 
2 
e := a /S 
, f := a /S 
, k -.= 13 a /a , 
v x 
u x 
v u 
2 
2 
2 
S^ being the asymptotic variance of x (i.e. Sx:=plim s^) and d the squared 
asymptotic correlation between x and z (i.e. the measure of asymptotic de-
2 
2 
2 2 
termination: d:=plim r = plim [s /(s s ) ]) . From the definition of u we 
X Z 
X Z 
X z 
have 
2 
2 
„2 2 
a = a + B a 
u 
w 
v 
In the same way the asymptotic covariance C of B ^ g and 13 
can be evalua-
ted: 
c = - 5 - • i- . 
* l-e 
T 
Given these expressions the asymptotic variance of £ can be computed as 
VC = 
^ A L S + b 2 v i V
 + 2 a b C 
and can be minimized with respect to a, where, of course, the restriction 
a + b = 1 has to be observed. The minimizing a is found to be 
= 
(1-e-d) (l-e) 
a 
(k+1)ed + (1-e-d) (l-e) 
and the minimal asymptotic variance of 8 is 
= 
(k+1) e + (1-e-d) 
f 
C 
(k+1)ed + (1-e-d) (l-e) ' T " 
Note that 1 i e + d and 0 £ a £ 1. 
In constructing the combined estimator 
one has to replace a, which is 
unknown, by a consistent estimate. This will not alter the asymptotic pro-
perties of 
Straightforward estimates of e and d are available: 
2 2 
e = a / s 
, d = r 
= s 
/ ( s s ) . 
v x 
xz 
xz 
x z 
2 
On the other hand, k, being dependent on 6 and a , must be estimated via 
the model. One could use 
o r 
or any combination to estimate J3 and 
could estimate a^ by the variance of the residuals. It is also possible to 
estimate k iteratively. 

268 
Hans Schneeweiss and Horst Witschel 
These results, found so far, refer to the asymptotic properties of the 
various estimators. In order to find out to what extent these asymptotic 
properties approximate the small sample properties of the estimators, a 
Monte-Carlo (MC) simulation study was carried out. 
4. A FIRST MONTE-CARLO STUDY 
We investigated bias and variance of the estimators B „, ¡3 
and B for 
nLb 
XV 
L 
sample sizes T = 25, 50, and 100, presuming that for T = 100 the bias would 
be near zero and the variance would be close to the asymptotic value. For 
T = 25 we expected some discrepancies with regard to the asymptotic theory 
and wanted to investigate its amount. We wondered whether T = 50 would yield 
good approximations to the asymptotic results at least for some cases. 
To be sure, expected value, bias, and variance need not exist for any of the 
estimators. When we talk of these terms, we mean expected value, bias, and 
variance arising from a distribution which comes near to the distribution 
of the estimator in the sense of an Edgeworth approximation (Sargan 1976) . 
In order to carry out the simulation experiment, we have to specify the 
distribution of the variables of the model. The error variables were taken 
to be normally distributed, which seems to be a plausible assumption to 
make. 
As to the variables £ we have to make a choice. In the literature two main 
cases are considered: the structural case, where the 
are i.i.d. random 
variables, and the functional case, where the £ are fixed (but unknown) 
numbers. We specified the 5 as fixed and chose them, for every sample size 
2 
T, as equidistant numbers with a variance s which was set equal to a pre-
2 
5 
assigned value of the parameter S and was kept constant for all T. 
The variables z^ were generated by the process 
= 
+ e with 
e^-i.N(0,a^) and independent of all (v^.w^). We could have chosen a more 
general linear relation, e.g. z = a + b£ + e . But as 8 
is invariant 

Estimators in an Errors-in-Variables Model 
269 
with respect to any linear transformation of z, the simpler equation 
z = Ç + e was sufficient for our purpose and saved us two more para-
2 
meters, a^ was chosen such that the empirical coefficient of determination 
of x and z would converge in probability to a preassigned value of d 
when T-"». Since 
0 
2 
_4 
d - plim 
s 
S. 
xz 
Ç 
2 2 
, 2 
2. 
2 2 
s s 
(S +a 
S.+a 
x z 
Ç v 
Ç e 
2 
we computed o^ from 
2 
1-e-d 
2 
a = — - — 
S_. 
e 
d 
5 
2 
So a^ is related to the parameters of the model and need not be considered 
as an additional parameter. 
As is well known, a MC experiment will not give a general answer to the 
problem posed. It will provide an answer only for a particular value of the 
parameter vector of the model. A general answer can be approximated by 
choosing a large set of such parameter vectors which more or less cover a 
relevant part of the parameter space (i.e. relevant from a practical point 
of view). Typically, however, the set of parameter vectors that may happen 
to turn up in any practical application of the model is very large indeed. 
It is therefore of utmost importance to reduce the dimension of the para-
meter space as far as possible in order to cut down the set of parameter 
vectors to a manageable size. 
The parameters of the model, after the variables have been specified as 
2 
2 
2 
2 
described above, are given by the vector (01,6,0^, a^, S^, d, T) , where a^ 
can, of course, be replaced by the parameter e. It turns out that the di-
mension of this vector can be reduced by 3, leaving only 4 parameters that 
have to be given preassigned values in every MC experiment. 
Indeed from the definition of » ^ g and B 
we find for the relative estima-
tion errors 
r 
R 
1 
2 
r 
ALS 
_ B 
x w 
X V 
v 
ALS : 
8 
2 
2 
s - a 
X 
V 

270 
Hans Schneeweiss and Horst Witschel 
ß 
- ß 
— s 
- s 
* 
= 
IV 
= 
e wz 
vz 
IV : 
ß 
s xz 
Introducing the variables 
v*:= v/Sç, w*:= w/(BS?), 5*: = Ç/S , x*: = x/S? = Ç* + v* 
we see that 
ß 
s ^ 
s * *+ e/(l-e) 
x*w* 
x*v* 
ALS 
2 
.,, , 
s - e/ ( 1-e) 
x* 
ß 
s 
it ~ s „ 
* 
_ w*z 
v*z 
"IV ~ 
s „ 
x*z 
This shows that for any fixed T the distributions of the relative estimation 
errors B* are determined by the variables x*, w*, v* and z and by the para-
meter e; x* in turn is determined by 
and v*. The distribution of these 
2 
2 
2 
2 2 
variables is governed by the parameters cj 
e/(1—e) t o ^ — o / (B 
> and 
d. For the sake of brevity let us introduce the parameter 
2.. < 
g := o * = 
w 
b2S\ ' 
We thus find that the parameter vector (g,e,d,T) is sufficient to describe 
the distributions of the relative estimation errors of fi , B 
and also 
ALS 
IV 
of J3 . From these distributions it is easy to derive the distributions of 
the estimators themselves. Thus e.g. E (6. 
) = B[l + E (§* J ] and 
— 
A Lib 
ALb 
V ( SALS } = f i V < B L 5 > -
This preliminary analysis shows that the results of the MC experiments will 
2 
not depend on the parameters a and S and will depend on 6 in a simple way. 
? 
2 
We therefore arbitrarily put a= 0, B = 1, and S^ = 100 throughout the whole 
MC study. 
We studied various combinations of the remaining parameters of the model, 
taking e = .1, .2, .4, .6; d = .2, .4, .6, .8; g = .01, .2, 1.0. Not all 
combinations are admissible, because the inequality e + d < 1 has to be ob-
served. 

Estimators in an Errors-in-Variables Model 
271 
For each parameter vector data points Cx^y^jZ^) were generated according 
to the model. From the generated sample (x^/y^/Z^), t=l,...,T the estimates 
8 
, 6 
, 6 were computed. This procedure was repeated N = 500 times. The 
nLb 
-L V 
L 
resulting N values for each of the estimators were averaged and their vari-
ances were computed (see Appendix). The average was taken as an estimate of 
the expected value E(B) of the estimator 6 and the variance as an estimate 
of the theoretical variance V(B) of B. These quantities refer to the para-
meter 6=1. If 8 takes some other value, the quantities E(6) and V(B) have 
2 
to be multiplied by 8 and 8 
respectively. 
Table 1 shows the results for g = .01. All the variances are multiplied by 
T in order to facilitate comparisons. The last column in each cell gives 
the asymptotic variances multiplied by T. 
As was to .be expected, when e is small and d is large the small sample pro-
perties are pretty close to the asymptotic properties at least for T^50. 
However, to our surprise, for large e huge discrepancies were found even 
for T = lOO. In fact, in some cases (denoted by 
) variances were larger 
than lOO (i.e. at least 10 times the asymptotic variance) and could not be 
printed with the layout chosen. 
5. MODIFIED ESTIMATORS 
A closer inspection of these results revealed that within the set of repli-
cated estimates that were used to compute the mean and variance of an esti-
mator, quite a few estimates violated certain restrictions that should be 
obeyed by any reasonable estimator of 8. Indeed, the following double in-
equality can be demonstrated for B: 
if**! < | (51 < JEL-
S 
1 1 
S 
xx 
I xyI 
where S 
= plim s 
, S 
= plim s 
, S 
= plim s 
. It would therefore 
xy 
xy 
xx 
xx 
yy 
yy 
seem natural to impose on an estimator 8 of 8 the analogous double inequa-
lity 

272 
Hans Schneeweiss and Horst Witschel 
xyl i fi * 
yy 
s I 
xy I 
With the notation 6. 
ILS 
s /s 
for the inverse least squares estimator 
yy xy 
of 8 this double inequality can also be written as 
I V s H s H ^ I -
Now the estimators B^g and fi do not necessarily obey these restrictions, 
whereas the following modifications do^': 
i f s2>a2 and I fi.T„ I < 16TT0 I 
ALS 
X 
V 
1 ALS1 1 ILS1 
ALSM 
8 
otherwise. 
ILS 
As long as sgn(B ) = sgn(fi 
) define 
IV 
OLS 
IVM 
OLS 
IV 
6, 
ILS 
l f 
IV^fiQLsl 
l f 
^OLS^IVI-^^ILSI 
i f 
I V H ^ s l 
If, however, sgn (ß^ ) ^ sgn(BLS), then 
IVM 
OLS 
IV 
ILS 
if r yz 
if r 
= m 
xy 
if r 
= m 
xz 
2
o
o 
where m = min(r , r , r ) . 
yz 
xy 
xz 
When ALS and IV estimators are modified in this way, the combined estimator 
will be modified as well: 
^ M = a *ALSM + (1"a)SIVM 
and 
obeys the double inequality, too. 
In passing, let us note that the distributions of the relative error of 
these modified estimators depend on the same paircimeteirs (g,e,ci/T) as the 

Estimators in an Errors-in-Variables Model 
273 
unmodified ones. The large sample properties of these modified estimators 
are the same as for the unmodified ones, but the small sample properties 
differ significantly, especially when e is large. 
6. A SECOND MONTE-CARLO STUDY 
When we repeated the MC experiment with the modified estimators in place of 
the original ones, the difficulties encountered before disappeared and we 
found a much closer correspondence of the small sample properties with the 
asymptotic ones (see Table 2 as an example for g = .01). The bias was grac-
tically zero for all cases considered. The variances were pretty close to 
the asymptotic variance, but were somewhat smaller in many cases. The dimi-
nuition of the variances is certainly due to the modification of the esti-
mators. But this observation needs some further analysis. 
We first of all tested the significance of the diminuition of the small 
sample variances with respect to the asymptotic variances by computing the 
standard errors of the estimated small sample variances along with the 
estimates. We found that the small sample variances, as estimated by our 
MC experiment, were indeed significantly smaller than the asymptotic vari-
ances. Of course for very large T the small sample variance will come closer 
to the asymptotic variance, but in our study the discrepancies between small 
sample and asymptotic variances were significant in most cases even for T 
as large as 100. (We also tried T=1000 and T=4000 and found significant 
(though smaller) discrepancies when we enlarged the number of replications 
N). (See also appendix.) 
The explanation for this phenomenon is readily found in the modification 
procedure for the estimators. Whenever the unmodified estimator comes up 
with a realization which lies outside the bounds of the double inequality 
for fi, it is reduced to a quantity which coincides with one of the bounds. 
The variance of the estimator is obviously made smaller thereby. This di-
minuition of the variance will be the more prominent the closer B is to one 
of the bounds of the double inequality for 6. 

274 
Hans Schneeweiss and Horst Witschel 
Now it can be shown that for g = .01 the parameter 6 is very close to the 
upper bound of the double inequality. When we raised the parameter g to 
g = .2 the distance between J3 and its upper bound became a lot larger and, 
as a consequence, the diminuition of the small sample variances was not 
observed any more. In fact (see Table 3) the small sample variances are now 
quite close to the asymptotic variances in almost all cases. (We also tried 
g = 1 and found the same tendency in the data, but the results were not 
very clear due to large sample fluctuations.) 
Finally let us note that the combined estimator has indeed the smallest 
variance of all three estimators in almost all cases. (Exceptions can be 
traced back to sample fluctuations of the MC experiment.) This is reflected 
by the quantity q, which is defined as the ratio of the variance of 
di-
vided by the smaller of the variances of B 
and Bjy- The smaller q the 
better the combined estimator as compared to the other two estimators, 
q is always (apart from a few exceptions) less than 1. 
7. CONCLUSIONS 
There are several conclusions to be drawn from our MC study which may be of 
more general interest and not just pertaining to errors-in-variables models. 
First, small sample properties may widely deviate from asymptotic properties 
even in a simple model set-up. 
Second, this deviation can be further analyzed and may lead to modified 
estimates with the same asymptotic properties, but with improved small . 
sample properties. In our case modified estimators were already available 
in the literature, but it was not clear that they would be useful for im-
proving the small sample properties. The modification procedure resembles 
in a sense the construction of robust estimators. It may well be that simi-
lar improvements of an estimator in finite samples can be found in other 
circumstances, too. 

Estimators in an Errors-in-Variables Model 
275 
When, as in our case, the modification of the estimator consists in "cutting 
down" its more extreme realisations, the resulting modified estimator will 
have a smaller variance than the original one. It may even have a smaller 
variance than the asymptotic one. This was the case predominantly found in 
our study. As a result, when one uses the asymptotic variance to construct 
confidence intervals one is always on the safe side. 
Finally, an estimator which is better than its competitors in large samples 
need not be so in small samples. Our MC investigation, however, showed that 
the linearly combined estimator which was designed to be optimal asymptoti-
cally, is also optimal in small samples. 
As a last point it should be noted that these results were found with the 
help of the computer and by a fruitful interaction of man and machine. The 
computer is not just an instrument to carry out prescribed tasks, it may 
also prompt the investigator to follow unforeseen paths into new areas of 
research. 
APPENDIX 
Let 6 denote any of the three estimators under investigation. Let b^ be the 
realization of 6 in the i-th replication of a particular MC experiment. 
(A MC experiment is specified when the parameter vector of the model has 
been given a specific numerical value.) We estimate expected value EI3 and 
variance VB by 
b := ^ Zb. 
N 
l 
s 2 (b) := ^ Z (b.-b) 2 = i? - b 2 
N 
l 
where N is the number of replications and the summation runs from i = 1 to 
i = N. These quantities can be computed while the replications are running, 
without having to store the b.. 
The standard error a- of b is estimated by 

276 
Hans Schneeweiss and Horst Witschel 
2 
In order to estimate the standard error of s (b) we first compute the va-
2 
- 
2 
riance s (d) of d. := (b.-b) : 
ì 
ì 
where 
and 
2 
2
-
2 
s (d) = d - d 
s2(b) 
2 
- 4 
d = (b.-b) 
i 
= b 4 - 4 b 3 b + 6 b 2b 2 - 3 b 4 . 
Again these quantities can be computed along with the replications. The 
2 
standard error of s (b) is now estimated by 
*2 
1 2 
0 2 
= 77 s 
d 
• 
s M b ) 
N 
Footnotes 
1) For some surveys on models with errors in the variables see e.g. Madansky 
(1959), Moran (1971), Kendall and Stuart (1979), Schneeweiss (1980). 
2) See e.g. Johnston (1963). The ALS estimator is a special case of the 
more general class of CALS estimators which were introduced by Kapteyn 
and Wansbeek (1979). 
3) See e.g. Sargan (1958). 
4) Linear combinations of estimators have been studied by Goldberger (1973). 
Feldstein (1974) considered a linear combination of the inconsistent OLS 
estimator and the consistent IV estimator. 
5) As for the ALS estimator, see Schneeweiss (1971, 1976); as for the IV 
estimator, see Sargan (1958). Both estimators are asymptotically normal 
with variances as presented. 
6) According to Birch (1964), 
i s the ML estimator of 6 in case all 
variables of the model are i.i. normal and a^ is known. Similarly, 
according to Learner (1978), 
is the ML estimator if z is given and 
all variables are i.i. normal. 

Estimators in an Error-in-Variables Model 
277 

2 7 8 
Hans Schneeweiss and Horst Witschel 
ff O K 
eoo 
e © e 
«V< M O 
© o e 
O © e 
® <£ 
eoo 
eoo 
lf\ IA O 
eoe 
eoo 
e © ® 
» O 9 
» o » 
-e o f» 
® e o 
eoe 
e f- -ö 
t» e o 
eoo 
hu 3 
«»> «r »n ^ 
^ -< O 
<r\ m f«t « 
o in o < 
r- —• .-« e 
NOVO 
© fi <D 
** e 
» 
e e ® 
» O o-
© O •«> 
» fr ^ 
eoo 
<r o e 
o o o 
O ^ 
r- e e 
eoo 
o e m w"i 
© ® ® K-
n M rw o 
en <on 
mn m« 
o «-" O Ä 
NIM H» 
«r* ^ »» < 
iMOI" 
^ fr 
O h- »H 
m» «« 
© e o «r 
O ci "I o 
«r ^ (n « 
frNHN 
r- f- ® 
P H CO 
e ir» 
<r «r 
o o o 
hNNO 
© ir» e e 
» no » 
f» «r 
KSN» 
5 o f 
«• «D ® O 
< © 
^ o o 
© e 
S 2 ^ 
01NK 
N H H 
® © e e 
oso© 
e r- r- e 
N HN 
<d « « 
o o o 
or fo ^ 
NM« N 
© © « e 
« « O m 
BONX 
0HK ^ 

Estimators in an Error-in-Variables Model 
279 
fr fr 
o o fr-
eo fr 
HN P 
-» -- O 
O O O 
lf> fr 
o o o 
co 
o o o 
o o o 
O fr fr 
M O O 
o o o 
«» — o 
eoe 
uj 3 
4> Û. _J 
tM o ^ 
»*> w fr 
ir> O- «n 
m «•> 
»>•> í»N m £* 
«r inj O fr 
fr to r- 
o 
r- o w <5 
«r r> fr 
© 
fr 
o © « ® 
t c ,r N 
O r» «r ^ 
^ fi <» 
e « o 
OMO 
o o o 
a- M -M 
o o o 
K. O 
O O O 
•T NO 
O O O 
o o o 
o o o 
2 
o > 
o^» 
ONC 
•£> 4) M 
mir o -f 
•o »•» -o fr 
o «o 
fr ^ ir> fr 
r- S o <D 
œ <o ir* ir 
fif ¿ r> 
S «v m o 
f» m <D 
o ir\ «n ® 
O M O i 
ce •£> o e 
h- O O «»• 
O -C -C fr 
í f» fr 
< 
c-
f» « fr 
ON 9 
o o o 
O O O 
«Ni O 
o o o 
e «v o 
o- o r* 
o fr — 
o œ> r- o 
.6 tr\ f» m 
«m iß fr 
n o H ® 
o m ^ O 
-- f» fr 
o o- o o 
O C H [> 
•O CD ÉO f» 
9 « f» o> 
» N m o 
M O S 
fSJ ÍSÍ 
•O H 
fr r» 
^ O N 
O O IM fr 
O O IT« «O 
m^oo 
•O «M ÍS» 
OWN? 
N Z1 (» f 
<o * - * r 
•/» «r « 

280 
Hans Schneeweiss and Horst Witschel 
TABLE CAPTIONS 
Table 1.: Unmodified estimators, g = .01, 
Table 2.: Modified estimators, g = .01. 
Table 3.: Modified estimators, g = .2. 
References 
Birch M.W. (1964), A note on the maximum likelihood estimation of a linear 
structural relationship, Journ. Am. Stat. Ass., 59_, 1175-1178. 
Feldstein M. (1974), Errors in variables: a consistent estimator with 
smaller MSE in finite samples, Journ.Am.Stat.Ass., 69, 990-996. 
Goldberger A.S. (1973), Efficient estimation in overidentified models: An 
interpretive analysis, Chapt. 7 in: Structural equation models in the 
social sciences. Goldberger A.S. and Duncan O.D.(Ed.). Seminar Press, 
New York-London, 131-152. 
Johnston J. (1963), Econometric methods, 1st ed., McGraw-Hill, New York-San 
Francisco-Toronto-London. 
Kapteyn A. and Wansbeek T.(1979), Errors in variables: consistent adjusted 
least squares (CALS) estimation, MRG Working Paper, 7910. 
Kendall M.G. and Stuart A.(1961), The advanced theory of statistics, Vol.2, 
4th ed.(1979), Griffin, London. 
Learner E.E.(1978), Least-squares versus instrumental variables estimation 
in a simple errors-in-variables model, Econometrica, 
961-968. 
Madansky A.(1959), The fitting of straight lines when both variables are 
subject to error, Journ.Am. Stat. Ass. , 
173-205. 
Moran P.A.P.(1971), Estimating structural and functional relationships, 
Journ.Multivariate Analysis, J^, 232-255. 
Sargan J.D.(1958), The estimation of economic relationships using instru-
mental variables, Econometrica, 26_, 393-415. 
Sargan J.D. (1976), Econometric estimators and the Edgeworth approximation, 
Econometrica, 44, 421-448. 
Schneeweiss H.(1971), Ökonometrie, Physica, Würzburg-Wien. 
Schneeweiss H.(1976), Consistent estimation of a regression with errors in 
the variables, Metrika, 23, 101-115. 
Schneeweiss H. (1980), Modelle mit Fehlern in den Variablen, Methods of 
Operat. Research, 37_, 41-77. 

The Robustness of Sampling Plans for 
Inspection by Variables 
Helmut Schneider and Peter-Theodor Wilrich 
O. INTRODUCTION 
Most of the published sampling plans for inspection by variables, e.g. the 
MIL-STD-414, assume a normal distribution of the inspected quality 
characteristic. However, these plans are not widely used since the assump-
tion of normal distribution is rarely justifiable. Hence, the question 
arises as to how robust these plans are in relation to deviations from the 
normal distribution. 
Only a few investigations concerning the robustness of the above-mentioned 
plans are available. A summary of work on variable acceptance sampling 
with emphasis on non-normality is given by Owen(1969). Das and Mitra 
(1964) use the Cornish-Fisher approximation up to three terms to compute 
the corresponding probabilities: i.e. those of rejecting lots with an 
"acceptable quality level" (AQL) and of accepting lots which have 
"limiting quality" (LQ). A non-normal distribution with known skewness and 
excess is assumed. The computed probabilities are compared with those in 
the tables. 
Massuda (1978), using simulation, investigates the robustness of normal 
sampling plans applied to Student and Lognormal distributions. Some papers 
deal with the design of variable sampling plans based on specific distri-
bution such as Gamma (Takagi 1972), Weibull (Hosono 1980, Takagi 1972) and 
Burr (Takagi 1972). Srivastava (1961) considers the Cornish-Fisher 
approximation to design variables plans. 
In this paper we intend, unlike Das and Mitra, to examine not only the 
probability of acceptance for lots of AQL, but rather the whole operating 

282 
Helmut Schneider and Peter-Theordor Wilrich 
characteristic curve (OC). We shall particularly concentrate on fraction 
defective less than AQL. It is obvious that the producer will only apply 
a variables sampling plan which will not be to his disadvantage, e.g. if 
the proportion of defective items in a lot is zero, the lot should be 
accepted with probability one. Accordingly, special interest shall be 
devoted to this property of the OC-curve. 
In the first section we review the method for determining variables sam-
pling plans when the quality characteristic inspected is normal distri-
buted. 
In the second section we shall examine a method of approximating the OC-
curve when the quality characteristic is not normal distributed. In the 
final section the robustness of the OC-curve will be investigated and a 
few typical examples shall be presented. 
1 . VARIABLES PLANS UNDER NORMALITY ASSUMPTION 
We assume the quality characteristic of the inspected item to be a normal 
distributed random variable X with mean v and standard deviation a, 
where p is always, and a very often, unknown. A one-sided specifica-
tion limit is assigned. Let us consider an upper limit U 
: items which 
have X > U are defective. Note that a lower limit L can be treated 
similarly. Since the variable X is normal distributed the proportion of 
defective items in a lot is 
(x-y)2 
00 
2 
f 
1 
2a 
p = J 
— e 
dx 
U \/2? a 
00 
= J tp(u) du 
u-u 
a 
= 
i 
- 
« 
( i ) 

Robustness of Sampling Plans 
283 
where tp(*) and $(•) are the standard normal pdf. and cdf. , respectively. 
According to an agreement between the producer and the consumer, lots 
with a fraction defective p 
AQL 
are presumed to be good and ought to 
be accepted with at least probability L^ = 1-a. Furthermore lots with 
p > LQ are not acceptable to the consumer and should be rejected with at 
least probability 
= 1-8. Lieberman and Resnikoff (1957) established 
the following procedure of variables acceptance sampling. If the specifi-
cation is an upper limit, the value t 
x + ka 
if a known 
(2) 
x + ks 
if a unknown, 
of the test statistic T is compared with the specification limit U. If 
the specification is a lower limit L, x - ka or x - ks is compared 
with L, respectively. 
On the basis of this comparison, each lot is either accepted (t <_ U) or 
rejected (t > U). Accordingly, a variables plan is specified by the para-
meters n(sample size) and k(acceptance constant). The desired (n,k)-plan 
has to fulfil the condition that the OC-curve of the plan will pass 
through the points defined by (AQL,1-a) and (LQ,B). 
1-a 
AQL 
LQ 
Fig. 1.: OC-curve of the 
(n,k)-plan 
To compute the sample size n and the acceptance constant k, we have to 
analyse the distribution of T = x + ka and T = X + kS, respectively! 
for a given fraction defective p. The OC-curve is given as 
L(p) = P (T < U | p) 
(3) 

284 
Helmut Schneider and Peter-Theodor Wilrich 
If the variance of X is known, the variable T is normal distributed. 
For unknown variance T can be transformed to a variable with a non-
central Student distribution. Hence, taking into consideration the require-
ments 
L(AQL) = P(T £U|AQL) = 1-a 
(4) 
L(LQ) = P(T £ u|LQ) = 6 
(5) 
it is not difficult, in principle, to calculate the parameters (n,k), 
either exactly (Lieberman and Resnikoff 1957), or approximately 
(Wilrich 1970). 
To obtain protection close to that given by the OC-curve of the variables 
plans discussed above, the distribution of the quality characteristic 
being inspected must be a normal distribution within the lot. All calcula-
tions used in constructing the plans have been based upon this assumption. 
Tests for verifying the assumption of normality may be used to ensure the 
practitioner that the application of variables plans is justified. If 
there seems to be a possibility that the product has been screened and 
hence has a truncated normal distribution, or if for any other reason a 
normal distribution can not be assumed, the user might want to know the 
deviation of the true OC-curve from that which holds under normality 
assumption. If the deviation is large, one might decide to apply sampling 
by attributes instead of variables plans. 
2. ASYMPTOTIC OC-CURVE FOR KNOWN SKEWNESS AND EXCESS 
In most cases where the assumption of normality does not hold the true 
distribution is not known. But on occasion excess and skewness of the 
quality characteristic might be available at least approximately. This 
being the case the asymptotic OC-curve can be computed. Following Cramer 
(1964) the test statistic T = X + kS is asymptotically normal 

Robustness of Sampling Plans 
285 
distributed with mean 
E[x + ks] = U + ka = u T 
( 6 ) 
and variance 
2 
2 
v[x + ks] = — 
(1 + V 
(6.-1) + k VF?) = oi 
(7) 
n 
4 
2 
I
T 
where y is the mean, a2 the variance, 
' the skewness, and 6^ 
excess 
of the quality characteristic inspected. Thus the OC-curve is given as 
L(p) = P(X + kS £ Ulp) 
(8) 
= p< z 1 
IP) 
T 
Since 
Z is asymptotically standard normal distributed we obtain 
approximately 
L(p) = 
- k) Ip) 
(9) 
where 
k2 
d = 1 + 
<B2 - 1) + k 
. 
do) 
To determine a (n,k)-plan for which its OC-curve passes through the given 
points (AQL,1-a) and (LQ,B), the parameters (n,k) have to satisfy the 
equations 
Ua = " ^ 
( \ - k ) 
(11) 
U6 " 
(12) 
where 

286 
Helmut Schneider and Peter-Theodor Wilrich 
= 1 - AQL, 
q 2 = 1 - LQ, 
®(u ) = a 
, 3>(u0) = 6 
, 
and 
a 
p 
y^ is the q-quantile of the standardized random variable Y = (X -y)/a. 
Variables plans for gamma and Weibull-distributed characteristics, which 
are based on the equations (11) and (12) are discussed by Takagi (1972). 
At first sight the problem of designing variables plans for non-normal 
distributions seems relatively easy to solve. But it should be noted that 
one has to know not only skewness and excess but also the quantiles 
y^ 
of the underlying distribution. Unfortunately these quantiles are 
rarely available. 
3. ROBUSTNESS OF VARIABLES SAMPLING PLANS 
In investigating the robustness of variables plans with respect to the 
normality assumption we have to take two factors into consideration. 
Firstly, the test statistic 
T 
is neither exact normal nor Student 
distributed if the underlying distribution is non-normal. But as we 
mentioned above this is not a very serious problem for large sample sizes 
n, since 
T 
is at least asymptotically normal distributed. Hence, this 
problem shall not be treated in this paper. 
Secondly, the quantile 
y 
of the true standardized cdf. will usually be 
different from u , the quantile of the standardized normal cdf.. Thus the 
substitution of v 
by u 
will result in a modification of the OC-curve. 
•*k 
q 
There are different ways of analysing these changes, dependent on the 
distribution of 
X 
which we assume. 
If one prefers not to select a specific distribution, one might use the 
Cornish-Fisher approximation (e.g. Das and Mitra 1964), assuming the 
excess and skewness are known. Another approach involves a mixture of two 
normal distributions for X. This form of non-normal distribution does 

Robustness of Sampling Plans 
287 
occasionally occur in practice. Two machines, for instance, producing the 
items of an inspection lot with measurements centering around well-
separated averages result in a bimodal distribution. However the most 
common departure from normality is the skewed distribution. It seems 
reasonable to expect a small amount of skewness as always being present. 
Excessive skewness may occur when the machine has a definite limit on one 
side of its customary operating range and when the product is manufactured 
while operating close to this limit. To give another example, extreme 
skewness might be found in the distribution when the lot is screened. In 
most cases the distribution will not only be skewed but the excess will 
also be much smaller than 3, namely the excess of the normal cdf.. In this 
paper we shall concentrate on those changes of the OC-curve caused by 
skewed and truncated distributions. In contrast to Das and Mitra we shall 
select a specific cdf. for the quality characteristic X, namely the 
Beta-distribution, where X varies only in a finite interval [a,b]. The 
Beta-cdf. is 
y 
iMy) = 
1 
_ 1 J" (x-a)P_1 (b-x)Q-1 dx 
(13) 
B(P,Q) (b-a)Q+P * a 
where 
B(P,Q) = / x P 
(1—x) ^ 1dx, P,Q > 0 
(14) 
0 
is the complete Beta-function of order (P,Q). 
It immediately follows from a<X<b that a<X<b, and consequently for any 
(n,k)-plan with a assumed to be known the equation 
L (p) = P(X + ka £ U I p) =< 
0 if U - ka <_ a 
(15) 
1 if U - ko > b 
holds. 

288 
Helmut Schneider and Peter-Theodor Wilrich 
Thus for k 
the lot will be rejected with probability 1. 
If k < ^ ^ the probability of X + ka > b is still greater than 0, which 
considerably reduces the probability of accepting lots with quality better 
than AQL (i.e. p <_ AQL). To study the OC-curve for large n (n-**>) we use 
the approximation 
L(p) = 
(y, -k)) 
(16) 
a 
l-p 
where y^ 
is the quantile of the standardized cdf.. Hence, 
U-a 
y 
= 
^ 
(17) 
1-P 
ao 
where 
* Ft 
and 
0 
b-a 
Let us now consider L(p) for p = 0. From (17) we obtain 
in contrast to the normal distribution the quantile y, 
tends to a 
l-p 
finite number as p+O. The probability of accepting lots with zero fraction 
defective, i.e. p=0, is thus 
L(0) = 
(-r-^ - k)) 
(20) 
d 
°0 
which is less than 1. For given d,ii^,ao the actual probability of rejec-
ting lots which are perfectly good, i.e. 1-L(0), depends on the sample 
size n and the number k. For constant k and increasing sample size 

Robustness of Sampling Plans 
289 
n three situations are of interest: 
1-y 
i) 
If k = 
then lim L(0) = 0.5. 
a 
0 
n-x» 
Let us denote this critical value as k^ 
ii) If k > k = 
then lim L(0) = 0 
0 
n-+°° 
iii) If k < k = 
then lim L(0) = 1 
O 
a„ 
0 
n-x» 
Thus the plans may be classified in terms of k. It is obvious 
that only plans with k < k Q 
are reasonable in practice, since for 
k > k^ the probability of accepting perfectly good lots would decrease 
with increasing sample size. Let us consider, for example, the uniform cdf., 
which is a Beta-distribution with parameters P=1 und Q=l. 
It is easily shown that 
yQ = 0.5 and aQ = 1 /VI? 
Hence 
1-y 0 
k_ = 
= 1.7321 
° 
°0 
If the quality characteristic inspected is uniformly distributed and if a 
variables plan with k > k^ = 1.7321 is applied, the probability of 
rejecting lots with zero fraction defective tends to 1 as the sample size 
increases. Fig.2 shows the OC for a variables plan with n=21 und k=1.8. 
The true probability of accepting lots with better quality than AQL is 
much lower than the probability given by the OC, based on normality 
assumption. 

290 
Helmut Schneider and Peter-Theodor Wilrich 
Fig.2.: OC-curves of variables- 
Fig.3.: Normal and uniform pdf. 
plan 
(n=21, k=1.8) 
with equal mean and 
variance 
But even in the case where ^<ko the probability of rejecting good lots, 
i.e. p^_AQL, can be remarkably high. To illustrate this let us consider 
a few examples of Beta-distributed quality characteristics, whose para-
meters are shown in Table 1, 
Table 1.: Examples of Beta-cdf. 
cdf. 
No. 
parameters 
P 
Q 
skewness 
excess 
ko 
1 
1 
1 
0 
1 8 
1 7321 
2 
2 
2 
0 
2 14 
2 2361 
3 
2 
3 
0. 29 
2 36 
3 0 
4 
3 
2 
- 0.29 
2 36 
2 0 
5 
3 
3 
0 
2 33 
2 6458 

Robustness of Sampling Plans 
291 
The following values 
AQL =0.01 
, 
a = 0.07 
LQ = 0.1 
, 
6 = 0.07 
are common in practice. If we take a as being unknown, under the 
assumption of normality we find the sampling plan n=21 and k=1.8. 
In table 2 the values L(0) are given for different sample sizes. 
Table 2.: Values L(0) of (n,k)-sampling plans 
with k = 1.8 
\ c d f . 
n 
1 
2 
3 
4 
5 
21 
0 . 394 
0 876 
0 993 
0 . 683 0 969 
30 
0 . 374 
0 916 0 998 
0 . 715 0 987 
40 
0 . 355 0 944 1 
0 744 0 995 
50 
0 339 0 963 1 
0 769 
0 998 
60 
0 . 325 0 976 1 
0 . 789 0 999 
CO 
0 
1 
1 
1 
1 

292 
Helmut Gehneider and Peter-Theodor Wilrich 
Fig.4.: OC-curve of variables plan 
Fig.5.: Normal and Beta (2,3) pdf. 
(n=21,k=l.8) for cdf. No.3 
p 
Fig.6.: OC-curve of variables plan 
Fig.7.: Normal and Beta (3,2) pdf. 
(n=21,k=l.8) for cdf. No.4 

Robustness of Sampling Plans 
293 
p 
8.: OC-curve of variables plan 
Fig.9.: Normal and Beta (3,3) pdf. 
(n=21,k=l.8) for cdf.No. 5 
Fig.10.: OC-curve of variables plan 
Fig.11.: Normal and Beta (2,2) pdf. 
(n=21,k=l.8) for cdf. No.2 

294 
Helmut Schneider and Peter-Theodor Wilrich 
4. CONCLUSION 
Let us summarize the results as follows: 
a) Negative skewed distributions are detrimental to the producer 
(see cdf. 3 und cdf. 4, Fig.4 and Fig. 6). 
b) L(0) decreases as excess decreases (see cdf. 1,2,5 in table 2). 
c) Both positive skewness and large excess are disadvantageous 
to the consumer (see Fig. 4). 
d) Sampling plans with k>kQ should not be applied, since L(0) tends to 
0 with increasing sample size (see cdf. 1 in table 2). 
e) If k<kQ the probability of accepting good lots can be increased 
by increasing the sample size. 
In general it can be stated that for Beta-distributed characteristics, 
the true OC will be below the OC, based on the normality assumption, if 
p is small and above if p is large. This is seen in Fig. 4. 
As mentioned earlier, the Cornish-Fisher approximation of y^ 
would be an 
alternative approach in the investigation of the robustness of variables 
sampling plans. If excess 
anc^ skewness VS^' are known, the quantile 
y, 
is approximately 
1-p 
*l-p = Ul-p 
+ \Ze?(u? 
- l)/6 
1 1-p 
+ (6,-3) (u? 
- 3u 
)/24 
2 
1-p 
1-p 
- 8. (2u^ 
- 5u 
)/36 . 
1 
1-p 
1-p 
Das and Mitra applied this formula up to three terms. This usually 
results in an inexact quantile. In comparing our results to those of the 
two authors, table 3 shows the values L(0) in both cases, i.e. using a 
Beta-distribution and applying the Cornish-Fisher approximation for the 
sampling plan n=21 and k=1.8. 

Robustness of Sampling Plans 
295 
Table 3.: Comparison of the L(0) values: Beta-cdf., Cornish-
Fisher approximation 
1 
2 
3 
4 
5 
true values 0.394 0.876 0.993 0.683 0.969 
Cornish-
Fisher 
0.836 0.968 0.999 0.807 0.989 
It is obvious that the Cornish-Fisher approximation is rather poor, since 
the values L(0), computed by this method, are much too high. 
References 
Cramer, H. (1963), Mathematical Methods of Statistics, Princeton, N.J. 
Das, N.G., S.K. Mitra (1964), The Effect of Non-Normality on Sampling 
Inspection, Sankhya, 261, 169-176. 
Hosono, Y., H. Okta, S. Kase (1980), Design of Single Sampling Plans for 
Doubly Exponential Characteristic, will appear. 
Lieberman, G.J., G.J. Resnikoff (1957), Tables of the Noncentral t-
Distribution, Stanford Univ. Press, Stanford, Cal. 
Masuda, K. (1978), Effect of Non-Normality on Sampling Plans by Lieberman 
and Resnikoff, ICQC, Tokyo, D3-7-D3-11. 
Owen, D.B. (1969), Summary of Recent Work on Variables Acceptance 
Sampling with Emphasis on Non-normality, Technometrics, 
631-637. 
Takagi, K. (1972), On Designing Unknown-Sigma Sampling Plans Based on 
a Wide Class of Non-Normal Distributions, Technometrics, 14, 
669-678. 
Srivastava, A.B.L. (1961), Variables Sampling Inspection for Non-Normal 
Samples, I.S.E.R., 145-152. 
Störmer, H. (1979), Über das Testen von Ausschußanteilen von verallgemei-
nerten t-Tests, Metrika, 26.' 5-24. 
Wilrich, P.-Th. (1970), Nomogramme zur Ermittlung von Stichprobenplänen 
für messende Prüfung bei einer einseitig vorgeschriebenen Toleranz-
grenze, Qualität und Zuverlässigkeit, IS_, 61-65, 181-187. 


Über eine Verallgemeinerung der Spektralanalyse 
Bernd Streitberg 
1. EINLEITUNG 
Im folgenden soll eine Verallgemeinerung der Spektraltheorie schwach sta-
tionärer Prozesse vorgestellt werden, mit deren Hilfe sich eine Vielzahl 
unterschiedlicher Abhängigkeitsstrukturen im Rahmen eines allgemeinen An-
satzes behandeln lassen. 
Betrachten wir zunächst die Vorgehensweise in der Zeitreihenanalyse von 
einem etwas formaleren Standpunkt! Es liege ein stochastischer Prozeß 
(Yt'teT 
e:*-ner Parametermenge T zugrunde. T wird nicht als un-
strukturierte Menge aufgefaßt, sondern es wird angenommen, daß auf T 
eine Gruppe G von (zeitlichen) Translationen operiert. Schwache Sta-
tionarität des Prozesses bedeutet dann, daß die 1. und 2. Momente der Y 
unter der Aktion von G invariant bleiben (also sich unter zeitlichen 
Verschiebungen nicht ändern). Ein konkretes Beispiel, an dem die alge-
braische Struktur ohne störendes analytisches Beiwerk herausgearbeitet 
werden kann, liefern sogenannte zirkuläre Prozesse. Hier ist 
T= {0,1,2,...,n-l} eine endliche Menge und G die Gruppe der zyklischen 
Permutationen von T , die von der Permutation h= (0 1 2 ... n-1) er-
zeugt wird. Die Permutationen g in G können durch Permutationsmatri-
zen D(g) der Ordnung n dargestellt werden, etwa h durch 
D(h) 
0 10 
0 
0 
1 
und hT e G durch D(hT)=D(h)T 
Ein stochastischer Prozeß auf T ist 
und schwache Stationarität von y be-
deutet E[D(g)y] = y und var[D(g)y] = var[y] für alle g e G 
. Eine di-
hier ein Zufallsvektor v = (Y ) 
* 
t'teT 

298 
Bernd Streitberg 
rekte Konsequenz dieser Stationaritätsannahme ist nun die Existenz einer 
Spektraldarstellung von y , d.h. die Möglichkeit einer Darstellung von 
y durch eine feste Lineartransformation unkorrelierter Zufallsgrößen. 
Im Fall des zirkulären Prozesses ist diese Lineartransformation bekannt-
lich die endliche Fouriertransformation. 
Diese Vorgehensweise läßt sich generalisieren, wenn man von der speziellen 
Bedeutung der Parametermenge T 
(als Menge von Zeitpunkten) und der Grup-
pe G 
(als zeitlichen Translationen von T ) abstrahiert. Wir betrach-
ten generell eine Menge T von Beobachtungseinheiten, deren innere Symme-
trien durch eine auf T operierende Gruppe G beschrieben werden. Einen 
verallgemeinerten Stationaritätsbegriff erhält man, wenn man annimmt, daß 
die 1. und 2. Momente eines stochastischen Prozesses 
(Y^) 
auf T un-
t teT 
ter der Aktion von G invariant bleiben. Zu jeder Gruppe G gehört unter 
dieser Annahme eine entsprechende Spektraltheorie, in der natürlich an die 
Stelle der Fouriertransformation jeweils andere (unitäre) Transformationen 
treten. 
Diese Verallgemeinerung ist zunächst formaler Natur - die Translationsgrup-
pe der Zeitreihenanalyse wird durch eine allgemeine Gruppe ersetzt. Der 
Verdacht "mathematischer Glasperlenspielerei" erscheint daher nicht ganz 
unbegründet. Es wird sich jedoch im folgenden zeigen, daß mit dem Ansatz 
zahlreiche Abhängigkeitsstrukturen, die in den praktischen Anwendungen der 
Statistik auftreten, in bequemer und einheitlicher Weise erfaßt werden kön-
nen. Insbesondere für geblockte Experimente wird ein Verfahren gegeben, 
mit dessen Hilfe die Varianzkomponentenanalyse derartiger Experimente di-
rekt aus der Beschreibung der Blockstruktur abgeleitet werden kann. Der 
hier vorgestellte Ansatz, der von den Arbeiten von Hannan (1965), James 
(1957) und McLaren (1963) ausgeht, wurde in ausführlicher Weise in Streit-
berg (1981) entwickelt (im folgenden nur durch St zitiert). 

Über eine Verallgemeinerung der Spektralanalyse 
299 
2 . II - STATIONÄRE PROZESSE 
Beginnen wir mit einem konkreten Experiment aus der Genetik (nach Hayman 
(1954))! Einer Menge von Varietäten der Art Nicotiniana rustica wurden 
zufällig 8 Varietäten entnommen und sämtliche 64 Kreuzungen zwischen den 
verschiedenen Varietäten vollzogen. In jeder solchen Kreuzung wurden 10 
Nachkommen behalten und zur Anpflanzung auf 2 Blecke verteilt. Die folgen-
de Tabelle enthält die Resultate des Experiments: jeweils die Blütezeiten 
in Tagen, gemittelt über die 5 Pflanzen mit gleicher Kreuzungs- und Block-
zugehörigkeit . 
• SB mmmmm mmmmmmmmmm 
mmmmm mmmmmm 
mm 
I 
1
2
3
4
5
6
7 
81 
II 
2 2 . 8 i h . I t 2 7 . 2 
1 7 . 2 1 8 . 3 I S . 2 I B . 6 
16.41 
21 1 5 . 4 1 7 . 2 1 4 . 8 1 8 . 6 1 5 . 2 1 7 . 0 1 4 . 4 
10.81 
31 3 1 . 8 2 1 . 0 2 4 . 8 2 4 . 6 1 9 . 2 2 9 . 8 1 2 . 8 
13.01 
41 1 6 . 2 1 1 . 4 1 6 . 8 1 8 . 4 1 2 . 4 16.B 1 2 . 6 
9.61 
•M 1 4 . 6 1 2 . 2 1 5 . 2 1 5 . 2 1 5 . 2 1 8 . 0 1 0 . 4 
13.41 
61 2 0 . 2 1 4 . 2 1 8 . 6 2 2 . 2 
1 4 . 3 2 0 . 2 
9 . 0 
11.81 
71 14.C 1 2 . 2 1 3 . 6 1 3 . B 1 5 . 6 1 5 . 6 
1 1 . 4 
13.01 
81 1 5 . 2 1 0 . 0 1 7 . 0 2 0 . 8 2 0 . 0 1 7 . 0 1 3 . 0 
14.01 
I 
1
2
3
^
5
6
7 
fil 
II 
2 4 . 2 1 6 . 2 1 0 . 8 2 7 . 0 2 0 . 2 1 6 . 8 1 4 . 4 
16.01 
21 1 6 . 5 18.fl 1 4 . 6 1 8 . 6 1 5 . 3 1 5 . 2 1 4 . 8 
13.21 
•>1 3 0 . 4 2 1 . 0 2 1 . 2 2 5 . 4 2 0 . 0 2 8 . 4 1 4 . 2 
14.41 
41 1 7 . 8 1 3 . 0 1 6 . 3 1 8 . 0 1 4 . 2 1 4 . 8 1 2 . 2 
11.21 
51 1 8 . 8 1 3 . 6 1 5 . 4 1 3 . 8 1 5 . 2 1 6 . 0 1 2 . 2 
20.01 
61 2 3 . 4 1 4 . 0 1 4 . 8 1 7 . 0 1 7 . 3 2 2 . 6 1 0 . 2 
12.81 
71 1 6 . 6 
9 . 2 1 6 . 2 1 4 . 4 1 5 . 6 1 1 . 0 1 0 . 6 
9.81 
81 1 7 . 2 
1 1 . 6 1 8 . 2 2 0 . 8 1 7 . 4 1 2 . 6 
9 . 8 
15.81 
Tabelle 1: Mittlere Blütezeiten (1. und 2. Block) 
Von Interesse ist hier die Untersuchung der Korrelationen, die zwischen 
den Eingängen in den Tabellen auftreten können. Die korrekte Varianzkom-
ponentenanalyse derartiger Experimente, die in der Genetik sehr häufig 
eingesetzt werden, ist äußerst umstritten (vgl. Cockerham/Weir (1977)). 
Aus Vereinfachungsgründen wollen wir hier zunächst nur die Daten des 1. 
Blocks (aufgefaßt als Originaldaten und nicht als Mittelwerte) betrachten. 
Bezeichnet T^ die Menge der 8 Varietäten, so können diese Beobachtungen 
durch das kartesische Produkt T=T^ x t 
indiziert werden, wobei in einem 
Paar (s,t) eT die 1. Komponente die Vatervarietät (= Zeilen der Tabelle) 
und die 2. Komponente die Muttervarietät (= Spalten der Tabelle) angibt. 

300 
Bernd Streitberg 
Die Daten sind dann y= (Y . ) , . . _ • 
1 
st (s,t)eT 
Eine geeignete Stationaritätsannahme geht hier direkt aus der Beschreibung 
des Experiments hervor: da die Varietäten zufällig ausgewählt wurden, 
sollten sich Erwartungswerte und Kovarianzen der Beobachtungen nicht än-
dern, wenn man die Varietäten anders anordnet, d.h. wenn man in Tabelle 1 
die Zeilen und die Spalten der gleichen Permutation unterwirft. Jede Per-
mutation g der 8 Varietäten induziert so eine Permutation der 64 Ein-
gänge in der Tabelle, die wieder durch eine Permutationsmatrix D(g) der 
Ordnung 64 beschrieben werden kann. 
Allgemein bezeichnen wir solche Situationen mit dem Begriff der Permutati-
onsstruktur (hier gegenüber St etwas vereinfacht): Sei T eine endliche 
Menge mit n Elementen, G eine endliche Gruppe und D eine Darstellung 
von G durch Permutationsmatrizen der Ordnung n (d.h. eine Abbildung 
g I—> D(g) der Elemente g e G in die Menge der Permutationsmatrizen, 
die D(gh) = D (g) D(h) für alle g,heG erfüllt). Dann heißt das Tripel 
II = (T,G,D) eine Permutationsstruktur 
(PS) . Ein stochastischer Prozeß 
y = (Y^) 
heißt (schwach) II - stationär, wenn E[D (g) y] = E[y] und 
var[D (g) y] = var [y ] für alle geG gilt. Er heißt streng II - stationär, 
wenn y und D(g)y für alle g e G die gleiche gemeinsame Verteilungs-
funktion haben. 
Die Entwicklung einer geeigneten Permutationsstruktur für einen vorliegen-
den Datensatz wird wesentlich vereinfacht, wenn man das "divide et impera"-
Prinzip benutzt: Man geht aus von einem Katalog einfacher Grundstrukturen 
und setzt aus diesen Bausteinen mit Hilfe bestimmter Operationen komplexere 
Permutationsstrukturen zusammen. 
Einige einfache Permutationsstrukturen: 
(1) Zyklische Permutationsstruktur C^ . T= {o,1,2,...,n-l} , G eine 
zyklische Gruppe der Ordnung n und D die natürliche Darstellung von G 
durch (n,n) - Permutationsmatrizen (vgl. die Einleitung). C 
kann zur 
n 
Beschreibung von zirkulär angeordneten Beobachtungseinheiten (etwa Jahres-
zeiten, Tagesstunden, Richtungen etc.) und (wie in der Zeitreihenanalyse 

Über eine Verallgemeinerung der Spektralanalyse 
301 
üblich) approximativ für linear angeordnete Beobachtungseinheiten einge-
setzt werden. 
(2) Symmetrische Permutationsstruktur S 
. T eine Menge mit n Ele-
menten, G die symmetrische Gruppe des Grades n und D die natürliche 
Darstellung von G durch 
(n,n) - Permutationsmatrizen. Hier durchläuft 
D(g) mit g e G 
sämliche n! Permutationsmatrizen. Die Annahme der S -
Stationarität ist eine Abschwächung der Unabhängigkeitsannahme. 
Starke 
Sn - Stationarität bedeutet Austauschbarkeit der Komponenten von y und 
die Annahme der schwachen S^ - Stationarität entspricht (wie man sich 
leicht überlegt) der Annahme konstanter Mittelwerte und Intraklassenkorre-
lation der Komponenten von y 
(gleiche Varianzen und gleiche Kovarianzen). 
S 
ergibt sich bei der zufälligen Auswahl oder Anordnung von Beobachtungs-
n 
einheiten und liegt geblockten Experimenten im Regelfall zugrunde. 
(3) Triviale Permutationsstruktur E 
. T eine Menge mit n Elementen, 
n— 
G die triviale Gruppe (die nur aus der Identität 1 besteht) und D(l) = 1 
(die Einheitsmatrix). Hier sind keinerlei Permutationen von T zugelas-
sen - entsprechend bedeutet E^ - Stationarität keinerlei Einschränkung der 
Verteilung von y . Dies ist z.B. angebracht, wenn die Komponenten von 
y inhaltlich verschiedene Variablen enthalten. 
Der Grundkatalog kann natürlich für spezielle Anwendungen beliebig erwei-
tert werden - im allgemeinen reichen C , S 
und E 
zusammen mit den 
n 
n 
n 
folgenden Operationen jedoch bereits aus. 
Operationen auf Permutationsstrukturen: 
(1) Die r'te Potenz einer Permutationsstruktur 11= (T,G,D) ist 
n r : = (Tr,G,Dr) 
, wobei D^(g) = D(g) ®D(g) ® ... ®D(g) die r'te Kron-
eckerpotenz von D(g) ist. Die Daten entsprechen hier einem r - dimen-
sionalen Array, dessen Zeilen, Spalten, Schichten etc. alle mit der glei-
chen Indexmenge T indiziert sind. Die oben angegebene Permutationsstruk-
tur des Kreuzungsexperiments kann also kurz als 
bezeichnet werden. 
o 
(2) Crossing zweier Permutationsstrukturen J^ = (T »G^Dj) , Jl2 = 
ist die Permutationsstruktur II x II = (T,G,D) mit T = T xt 
(kartesi-

302 
Bernd Streitberg 
sches Produkt), G = G ^ ® G 2 
(direktes Produkt der Gruppen) und 
Dttg^»^'' = D
1
® 
D
2
' 
0 r d n e t m a n 
D a t e n in Form einer Matrix 
Y 
(s e Tj ,t e T^) an, so sind hier alle Permutationen zulässig, die sich 
durch unabhängige Permutationen der Zeilen (gemäß G^ ) und der Spalten 
(gemäß G ) 
ergeben. 
Beispiele: C *E 
(p - variate Zeitreihen), S xs 
(Zweifachklassifika-
n 
p 
n 
m 
tion zweier Blockstrukturen), C x C 
(räumliche Prozesse mit vektorwerti-
n 
m 
gern Zeitparameter) , II x E 
(multivariates Analogon der PS II ) . Die Ta-
P 
2 
belle 1 könnte etwa mit der PS S xS 
beschrieben werden. 
O 
Z 
(3) Nesting zweier Permutationsstrukturen ü^= 
und 
n2 = (T2'G2'D2) 
h e i ß t d i e P S 
n i ^ n 2 : = 
( T' G' D ) 
m i t 
t = V
t
2 
• G = G i ^ " G 2 
(Kranzprodukt der Gruppen G^ und G^ ) und der entsprechenden Verknüpfung 
von Dj und D^ 
(präzise Definition siehe St 261 ff). Stellt man sich 
die Daten wieder in Form einer Matrix Y ^ (s e T. , t e T ) vor, so ist 
st 
1 
2 
hier jede Umordnung der Daten zulässig, die sich aus einer Permutation 
der Zeilen untereinander (gemäß G^ ) sowie möglicherweise jeweils ver-
schiedenen Permutationen der Elemente innerhalb der verschiedenen Zeilen 
(gemäß G^ ) ergibt. Diese Verknüpfung ist für Daten geeignet, bei denen 
nur die Zusammengehörigkeit der Elemente in einer Zeile, nicht jedoch de-
ren Reihenfolge inhaltlich bedeutsam ist. Typische Anwendungen sind 
s n 
(etwa: in jeder Zelle des durch II beschriebenen Experiments wer-
den n Meßwiederholungen durchgeführt), II-»• C 
(in jeder Zelle wird eine 
Zeitreihe der Länge n erhoben, wobei die Zeitpunkte in verschiedenen Zel-
len nicht vergleichbar sind), 
s
n " > n 
(das Experiment II wird n-mal repli-
ziert) . Dem von Hayman beschriebenen Kreuzungsexperiment entspräche also 
die PS 
(Sg x s ) S 5 . 
Crossing- und Nestingoperatoren wurden von Neider (1965) (ohne gruppenthe-
oretische Deutung) zur Beschreibung der Blockstruktur von Experimenten vor-
geschlagen (und sind z.B. in Programmpaketen wie GENSTAT implementiert). 
Die Theorie von Neider umfaßt jedoch nur eine kleine Teilmenge der hier 
möglichen Strukturen (nämlich die sogenannten einfachen Blockstrukturen, 
d.h. alle PS, die sich aus symmetrischen PS durch endlich viele Anwendungen 
von x und 
aufbauen lassen - also etwa bereits nicht die Strukturen 
2 
S_ oder C 
etc). 
o 
n 

Über eine Verallgemeinerung der Spektralanalyse 
303 
In der Zeitreihenanalyse folgt aus der Stationaritätsannahme die Gleich-
heit aller Erwartungswerte des Y 
sowie die Toeplitz-Form der Kovarianz-
matrix. Analog ergeben sich aus der 
II - Stationarität Aussagen darüber, 
welche Erwartungswerte einander gleich sind (die Teilmengen von T mit 
konstanten Erwartungswerten des Y 
sind die sogenannten G - Bahnen oder 
Transitivitätsbereiche von T unter G ) und darüber, welche Paare von 
Beobachtungen die gleiche Kovarianz haben (die entsprechenden Teilmengen 
2 
von T 
sind die sogenannten G - Relationen oder Transitivitätsbereiche 
2 
von T 
unter G ). Siehe dazu St 27 - 36, 38 - 48. 
3. GRUPPENDARSTELLUNGEN UND SPEKTRALTHEORIE 
KOMPLEXER II - STATIONÄRER PROZESSE 
Eine allgemeine Formulierung der Spektraltheorie kann am einfachsten aus 
einem Vergleich mit der Hauptkomponentenanalyse gewonnen werden. In der 
theoretischen Hauptkomponentenanalyse (vgl. Anderson (1958)) liegt ein Zu-
fallsvektor y mit bekannter Kovarianzmatrix 
Z vor und das Ziel ist, 
diese Kovarianzmatrix zu diagonalisieren. Schreibt man Z = QAQ 1 
, wobei 
A die Diagonalmatrix der Eigenwerte von I ist und die Spalten von Q 
die zugehörigenorthonormierten Eigenvektoren enthalten, so gilt offenbar, 
daß die Komponenten von z : = Q 
unkorreliert sind. Der Zufallsvektor 
y kann daher mittels einer nichtstochastischen und nichtsingulären Linear-
transformation y = Qz auf einen Vektor z unkorrelierter Zufallsvariablen 
(Hauptkomponenten) zurückgeführt werden. 
Die Fragestellung der Spektralanalyse ist die gleiche wie die der Hauptkom-
ponentenanalyse. Im Unterschied zur Hauptkomponentenanalyse ist hier je-
doch nur bekannt, daß y II - stationär ist, d.h. daß die Kovarianzmatrix 
var[y] in einer gegebenen Klasse von Matrizen liegt. Das Diagonalisie-
rungsproblem ist nun gleichzeitig für alle Kovarianzmatrizen in dieser 
Klasse zu lösen. Im Fall zeitlicher Prozesse führt diese Fragestellung 
unmittelbar auf die bekannte Spektraltheorie ( Q ist hier die Matrix der 
endlichen Fouriertransformation, vgl. St 129 - 133). Inwieweit eine der-
artige vollständige Spektralanalyse möglich ist, kann mit den Methoden der 
Gruppendarstellungstheorie entschieden werden. In jedem Fall liefert die-

304 
Bernd Streitberg 
se Theorie jedoch eine maximale Reduktion von var[y] auf Blockdiagonal-
form. 
Die zugrundeliegende Klasse von Kovarianzmatrizen ist durch die Forderung 
var[y ] = var [D (g) y] für alle g e G 
bestimmt. Dies ist äquivalent zu 
var[y]D(g) = D(g)var[y], d.h. varCy] ist Element der Kommutatoralgebra 
C(D) von D 
C(D) := {Ae Cn,n|ñD(g) = D(g)A für alle geG} . 
Sofern die Komponenten von y komplexwertige Zufallsvariablen sind, ist 
die Diagonalisierung aller Kovarianzmatrizen in C(D) 
(d.h. der positiv-
semidefiniten, hermiteschen Matrizen in C(D) ) äquivalent zur Diagonali-
sierung sämtlicher Elemente von C(D) 
(vgl. S 142 - 148). Eine komplexe 
Matrix AeC(D) kann statistisch als linearer Filter interpretiert wer-
den; Reduktion von A auf Diagonalform liefert das Analogon zur Transfer-
funktion des Filters (vgl. St 26 - 27, 142, 154 - 155). 
Es ist im allgemeinen mit erheblichen Schwierigkeiten verbunden, für eine 
konkret vorgelegte Klasse von Matrizen eine Reduktion auf (Block)-Diagonal-
form zu finden. Hier kann jedoch die Reduktion von C(D) auf das dazu 
"duale" Problem der Reduktion sämtlicher Matrizen D(g) , ge G zurückge-
führt werden, zu dessen Lösung die starken Hilfsmittel der Gruppendarstel-
lungstheorie zur Verfügung stehen. 
Wir erinnern an einige Grundbegriffe dieser Theorie (vgl. etwa St 60 - 125 
oder Ledermann (1977)): Eine (komplexe) Darstellung des Grades n der 
endlichen Gruppe G ist ein Homomorphismus D : G-* GL[<E,n] , also eine Zu-
ordnung von nichtsingulären komplexen n - reihigen Matrizen D(g) zu den 
Elementen g e G , die D (gh) = D (g) D (h) für alle g,heG erfüllt. Zwei 
Darstellungen D, E von G heißen äquivalent (in Zeichen: D ~ E ), wenn 
sie sich nur bis auf Basiswechsel des C n unterscheiden, d.h. wenn ein 
Q e GL[C,n] existiert mit D(g)=QE(g) Q 1 für alle g £ G 
(kurz: 
D = Q ^ E Q ) . Als Charakter einer Darstellung D bezeichnet man die kom-
plexwertige Funktion x = G + C mit x(g) = tr D(g) 
(Spur von D(g) ). Zwei 
Darstellungen sind genau dann äquivalent, wenn ihre Charaktere gleich sind 
(St 68, 117). 

Über eine Verallgemeinerung der Spektralanalyse 
305 
Eine Darstellung D heißt reduzibel, wenn sie zu einer Darstellung in 
Blockdiagonalform 
D ~ diag[Dj,D2] = 
äquivalent ist. Andernfalls heißt D irreduzibel. Die "Teildarstellungen" 
Dj , D^ von D sind unter Umständen weiter reduzibel. Aus dem Satz von 
Maschke (vgl. St 73 - 78) folgt, daß jede gegebene komplexe Darstellung ei-
ner endlichen Gruppe in eine kanonische Form A 
D~ A = diagtAj 
, . . . ,Aj , 
•• • 
< • • • f
 A
k/ A
k< • • • < 
überführt werden kann, in der die auftretenden Teildarstellungen A^ 
(a= 1,2,...,k) irreduzibel sind und in der verschiedene Teildarstellungen 
A , A„ für a 4 ß inäquivalent sind. Diese "kleinsten Bausteine" A 
, 
a
ß
1 
a 
aus denen sich die gegebene Darstellung D zusammensetzt, werden als irre-
duzible Konstituenten von D bezeichnet, die Häufigkeit, mit der A^ in 
D auftritt, als Vielfachheit n 
von A 
. Die kanonische Form A ist 
a 
a 
bis auf Reihenfolge und Äquivalenztransformation der A^ eindeutig be-
stimmt, wobei die irreduziblen Konstituenten A 
stets unitär gewählt 
a 
* 
— 1 
werden können (d.h. A (g) = A (g) 
für alle g e G ). Ist auch die Aus-
a 
a 
gangsdarstellung D unitär (dies ist in unseren Anwendungen stets der 
Fall, da D(g) hier eine Permutationsmatrix ist), so kann auch die "Trans-
formationsmatrix" Q in D=QAQ 1 unitär gewählt werden (vgl. St 64 - 68). 
Für die Anwendungen ist das Aufsuchen einer derartigen Transformationsma-
trix Q entscheidend; die Spalten von Q liefern eine Basis des c" , 
die optimal an die von D vermittelten Symmetrien angepaßt ist. 
Das Bindeglied zwischen der Reduktion von D und der gesuchten Reduktion 
von C(D) stellt das Lemma von Schur dar: Gilt A^(g)X= XA^(g) für alle 
g e G 
und irreduzible Darstellungen A , A„ , so ist X = 0 
, wenn A 
a
ß 
a 
und A 
inäquivalent sind und X eine Skalarmatrix X = XI (i e C) , wenn 
p 
A = A 
. Wendet man das Lemma auf die Kommutatoralgebra C(A) an, so 
ot 
p 
ergibt sich eine Blockdiagonalform für alle Matrizen in C(A) . Diese 
Form sei für das Beispiel A= diag[A^,Aj,,A^,A2] illustriert. 
Alle Matrizen BeC(A) haben hier eine Blockstruktur: 

306 
Bernd Streitberg 
V V 0 
0 
0 
b3 I V 0 
0 
0 
0 
0 V V V 
0 
0 V V V 
0 
0 V V V 
mit b^ e € (i = 1,2 ,. . . , 13) . Generell ergibt sich für BeC(A) die Form 
B = diag[B1® I , B ® I , ... , B^ ® I , ... , B k » i ] 
wobei B^ 
(a= 1,2,...,k) eine beliebige komplexe Matrix der Ordnung n^ 
und I in B ® I eine Einheitsmatrix mit der Ordnung von A 
ist. Die 
a 
1 
Matrizen Ae C(D) haben dann die Gestalt A = QBQ 
mit Bc C(A) . Da 
B^ beliebig ist, gibt Q die maximal mögliche Reduktion von C(D) auf 
Blockdiagonalform. 
(St 82 - 89, zur Frage der Eindeutigkeit von Q siehe 
St 89 - 97). 
Für komplexwertige Prozesse y können diese Ergebnisse direkt statistisch 
angewendet werden: Ist Ae C(D) die Kovarianzmatrix von y , so kann 
man mittels Q zu den Spektralkomponenten z : = Q 
übergehen, die dann 
die blockdiagonale Kovarianzmatrix B = Q *AQ besitzen mit positiv semide-
finiten, hermiteschen Spektraldichtematrizen B^ 
(a= 1,2,...,k) . Zer-
legt man z entsprechend der Blockstruktur von B in Teilvektoren z ^ 
(a= 1 ,2, .. . ,k , i = 1,2 , . .. ,n ) , so sind die Komponenten von z 
unterein-
ander unkorreliert und die Teilvektoren z ,,z ,...,z 
besitzen genau 
al a2 
ana
 
y 
die Kovarianzstruktur eines n^ - variaten komplexen Samples mit Kovarianz-
matrix B^ , dessen statistische Analyse trivial ist, sofern genügend 
Freiheitsgrade zur Verfügung stehen (d.h. z ^ 
mindestens n^ Komponenten 
enthält). Die Spektraldarstellung y= Qz des Prozesses führt damit den 
Zufallsvektor y auf k untereinander unkorrelierte multivariate Stich-
proben zurück. Eine vollständige Spektralanalyse (d.h. Diagonalform von 
var[z] ) erhält man genau dann, wenn jede irreduzible Konstituente A 
in 
D mit der Vielfachheit n = 1 auftritt. Zahlreiche Kriterien für die 
a 
Existenz einer vollständigen Spektralanalyse findet man in St 148-153. 

Über eine Verallgemeinerung der Spektralanalyse 
4. ZUR SPEKTRALTHEORIE REELLER H - STATIONÄRER PROZESSE 
307 
Für reellwertige Prozesse y bricht die befriedigende Dualität zwischen 
der Reduktion von D und C(D) teilweise zusammen. Die auftretenden 
Schwierigkeiten kündigen sich bereits in der Zeitreihenanalyse an, wo be-
kanntlich die Analyse reeller Prozesse nur im Komplexen durchgeführt wer-
den kann, sofern die betrachteten Prozesse multivariat sind (Existenz von 
Quadraturspektren, vgl. St 198). 
Auch für reellwertige 
II - stationäre Prozesse läßt sich jedoch eine Spek-
traltheorie entwickeln, wenn man von der Reduktion der Darstellung D 
über IR ausgeht, d.h. nur reelle Transformationsmatrizen Qe GLßR,n] be-
nutzt. Die wichtigsten Ergebnisse seien hier nur angedeutet: 
(1) Eine über IR irreduzible Darstellung A^ ist entweder auch über 
C 
irreduzibel (Darstellung 1. Art) oder zerfällt über I in zwei zueinander 
konjugiert komplexe, irreduzible Teildarstellungen A^ , A^ , die entweder 
äquivalent (Darstellung 2. Art) oder inäquivalent sind (Darstellung 3. Art). 
Vgl. St 170- 183. 
(2) Eine vollständige Spektralanalyse aller reellen, positiv-semidefini-
ten, symmetrischen Matrizen in C(D) existiert genau dann, wenn jede über 
3R irreduzible Konstituente A 
von D die Vielfachheit 1 besitzt. Vgl. 
a 
st 213 - 217. 
(3) Die Menge 
aller reellen Matrizen, die mit A^ kommutieren, 
ist ein Schiefkörper über IR , der 1, 2 oder 4-dimensional ist (für A^ 
1., 3. oder 2. Art). C^^CA ) ist damit isomorph zu IR , C oder der Qua-
ternionenalgebra E 
. Vgl. St 206. 
(4) Im Fall einer unvollständigen Spektralanalyse sind die Spektraldichte-
matrizen B^ genau dann beliebige reelle Kovarianzmatrizen, wenn A^ eine 
Darstellung 1. Art ist. Andernfalls unterliegen diese Matrizen noch weite-
ren Restriktionen, die sich nur dadurch beseitigen lassen, daß man die ent-
sprechenden Spektralkomponenten paarweise zu komplexwertigen Zufallsvari-
ablen ( A 
3. Art) oder quadrupelweise zu quaternionenwertigen Zufallsva-
riablen ( A 
2. Art) zusammenfaßt. 
(St 194 - 199, 206 - 213). 

308 
Bernd Streitberg 
Dies scheint das einzige Modell in der Statistik zu sein, in dem u.U. qua-
ternionenwertige Zufallsvariablen analysiert werden müssen. Freilich ist 
dies nur eine theoretische Notwendigkeit, da in den praktischen Anwendun-
gen Darstellungen 2. Art wohl nicht auftreten (so besitzen alle aus S^ 
und E^ mittels Potenzierung, Crossing und Nesting gebildeten Permutati-
onsstrukturen nur Konstituenten 1. Art und alle aus S , C 
und E 
durch 
n 
n 
n 
Crossing und Nesting gebildeten Permutationsstrukturen nur Konstituenten 
1. und 3. Art. 
5. SPEKTRALANALYSEN AUSGEWÄHLTER PERMUTATIONSSTRUKTUREN 
(1) Grundbausteine: Für C^ liefert die Gruppendarstellungstheorie die 
bekannte Fourieranalyse (Q- Matrix der diskreten Fouriertransformation, 
d.h. q 
= J^- exp (i2irja/n) für Beobachtungen j = 0,1, . . . ,n-l und Kon-
stituenten a= 0,1,2,...,n-l). Alle Konstituenten sind hier eindimensio-
nal, d.h. für das Spektrum an der Stelle a steht (bei komplexen Prozes-
sen) nur ein Freiheitsgrad zur Verfügung. 
Bei S^ findet man zwei Teildarstellungen A^ , A^ der Dimension 1 bzw. 
n-1 
. Anstatt diese Teildarstellungen anzugeben, kann man äquivalent da-
zu die entsprechenden invarianten Unterräume bestimmen. Ein Unterraum V 
eines Vektorraums W heißt dabei invariant unter D , wenn für jedes 
x e V 
und jedes g e G 
gilt D(g)xeV 
. Zu A^ gehört hier der Unter-
raum Vj aller konstanten Vektoren 
(c,c,c...c)' und A^ ist die soge-
nannte Hauptdarste1lung mit A^(g) = 1 für alle g € G . Zu 
gehört 
der Unterraum V» aller Kontraste 
(x.,x , ...,x )' mit Ex. = 0 
, A„ 
2 
1 2 
n 
l 
2 
sei daher als Kontrastdarstellung bezeichnet. Man erhält Q , wenn man 
Basen für die beiden invarianten Unterräume wählt. Für V^ ergibt sich 
als Basis Q. =-y- (1 1 .. 1) 1 und eine Basis für V_ findet man z.B., 
1 v/n 
2 
wenn man die Spalten der Helmert-Matrix 
_ 1 
1 ... 1 _ 
-1 1 
1 
0 - 2 
1 
0 
0 
1 
0 0 -(n-1) 

Ober eine Verallgemeinerung der Spektralanalyse 
309 
normiert. Die Transformationsmatrix 
Q ist dann Q= [Q ^ | Q^ H . Das Spek-
trum eines S^ - stationären (austauschbaren) Prozesses y hat entspre-
chend A , A 
zwei Komponenten 
of , 
und (beste erwartungstreue) 
- 2 
2 
Schätzer sind ny 
und 
s 
. Vgl. Halperin (1951). 
Bei E^ ergibt sich n - mal die Hauptdarstellung, d.h. 
A= diag[Aj,Aj,...,A^]= I und offenbar existieren keinerlei Reduktionsmög-
lichkeiten. 
(2) Crossing-Operator: Ist die Spektraltheorie zweier Permutationsstruk-
turen H1 , II2 bekannt, so findet man daraus die Spektraltheorie des 
Crossings 
x 
: 
Se:'' ^ai 
Teilt'as:'-s 
Spalten in Q ) für das 
i' te Auftreten der irreduziblen Konstituente 
A 
von II. und Q„. die 
a 
1 
Teilbasis für das j'te Auftreten der irreduziblen Konstituente 2L von 
^ 
p 
H 
, (i=l,2,...,n 
, j= 1,2,...,n ), so hat II x n 
die irreduziblen 
Konstituenten 
A ® A„ mit der Vielfachheit n n„ und den Teilbasen 
^ 
a 
ß 
a ß 
®ai ® ®ßj 
' Vgl. St 257 - 260. Einfachste Anwendungen sind die Ableitung 
einer multivariaten Spektralanalyse 
(II x E^) aus der entsprechenden uni-
variaten 
(II) oder die Ableitung der räumlichen Spektralanalyse 
(C^ x C^) 
aus der zeitlichen 
(C ) - die zweidimensionale Fouriertransformation 
n 
ist das Kronecker-Produkt 
Q ® Q 
der eindimensionalen. 
Ein m - faches Crossing II. x JT x . . . x n 
führt auf Spektralkomponenten 
1
2 
m 
z= (Qj ® Q 2 ® . . - ® S^) Y 
i wenn Q. die zu IL gehörende Transformations-
matrix der Ordnung KL 
ist. Der Algorithmus zur Berechnung dieser Trans-
formation ist recht aufschlußreich: Seien die Elemente von Q* mit 
q.(r.,t.) bezeichnet 
(l<r.,t.<N.) 
und y als m - dimensionales Array 
i
l
l 
i
l
l 
y(t,,t„,...,t ) dargestellt, dann ist auch z ein m - dimensionales 
1 2 
m 
Array, das durch fortlaufende Operationen an den einzelnen Dimensionen von 
y gebildet werden kann: 
[1] Initialisierung, y (t ,t ,...,t ) 
y (t ,t ,. . . ,t ) 
o 
1 2 
m 
1 2 
m 
[2] Berechne für 
i=l,2,...,m 
yi ( ri' r
2 
W
i 
V 
" 
N. 
1 
t.=l 
1 
I q . i r ^ t j y . ^ t r j , ^ 
r . ^ t . , ^ 
t j 

310 
Bernd Streitberg 
[3] Endergebnis: z(r, ,r , — ,r ) 
y (r ,r , — ,r ) 
1 2 
m 
m 1 2 
m 
Der Algorithmus benötigt nur die Speicherung eines m - dimensionalen 
Arrays sowie 
(N. + N„+ ...+ N )N,N ...N 
Multiplikationen. 
1
2 
m l 2 r a 
Für den Spezialfall S x s x . .. x s_ ist jedes Q. ~ 
2 
2 
2 
1 v2 
und es er-
1 1 
gibt sich der Yates-Algorithmus zur Berechnung der Interaktionen eines 
2 m - faktoriellen Designs durch fortlaufende Additionen und Sutraktionen 
(Yates (1937)). Verwendet man jeweils die oben angegebenen Helmert-Ma-
trizen, so entsteht für S^ x S^ x . . . x s 
der Box-Algorithmus (Box et al. 
(1954)) und für beliebige symmetrische PS S 
x s 
x ... x s 
eine Versi-
ni 
n2 
n m 
on des Algorithmus von Good (1958, 1960) zur Berechnung aller orthogonalen 
Kontraste zwischen den Interaktionen faktorieller Designs. Dabei werden 
die korrekten Divisoren für die Varianzanalyse automatisch erzeugt und die 
Orthogonalität der Transformationsmatrix liefert sofort den Umkehralgorith-
mus . 
Legt man zyklische PS zugrunde, so hat man einen Algorithmus für m - dimen-
sionale Fouriertransformationen und bei paarweise teilerfremden N^ eine 
Version der schnellen (eindimensionalen) Fouriertransformation (Good 
(1971)). Dies folgt aus der bekannten Tatsache, daß das direkte Produkt 
zyklischer Gruppen paarweise teilerfremder Ordnungen selbst zyklisch ist. 
(3) Auf die Wiedergabe der entsprechenden Resultate für den Nesting-Opera-
tor sei hier aus Platzgründen verzichtet (vgl. St 261 - 277). Mit diesen 
Ergebnissen können beliebige Permutationsstrukturen, die sich durch end-
lich viele Anwendungen von Crossings und Nestings aus Permutationsstruk-
turen mit bereits bekannten Spektraltheorien zusammensetzen lassen, analy-
siert werden. 
x 
(4) Dagegen ist die Spektraltheorie der Potenzen II 
im allgemeinen nicht 
2 
aus der Spektraltheorie von II alleine erschließbar. Für S 
seien die 
n 
invarianten Unterräume zusammen mit der jeweils gebräuchlichen genetischen 
Bezeichnung hier angegeben. Man überlegt sich dabei jeweils leicht, daß 
die entsprechenden Mengen von 
(n,n) - Matrizen Y = (y^j) unter gleichen 
Zeilen- und Spaltenpermutationen tatsächlich invariant bleiben. Daß diese 

Über eine Verallgemeinerung der Spektralanalyse 
311 
invarianten Unterräume auch zu irreduziblen Konstituenten gehören, wird in 
S 218 ff gezeigt. 
Vj 
(Mittelwert der Selbstungen) 
0 für i 4 j 
a für i = j 
(a 
(Mittelwert der Kreuzungen) 
V 2 1 (Kontraste zwischen Selbstungen) 
(Allgemeine Kombinationsfähigkeiten) 
I 0 für i = j , 
. 
y. . = i 
J 
(a cm) 
1 3 
a für i+ j 
Y = diag[a ,a ,...,a ] 
1 2 
n 
mit Za. = 0 
l (
0 
für i = j , _ 
. 
a. +a . für i 4 j 
1 3 
T 
V 2 3 (Reziproke Effekte) 
y. . = a. - a. 
ID 
i 
D 
(la. = 0) 
i 
(Spezifische Kombinationsfähigkeiten) 
Y symmetrische Matrix mit 
verschwindender Hauptdiagona-
le und verschwindenden Zei-
lensummen. 
V 4 
(Spezifische Reziproke Effekte) 
Y schiefsymmetrische Matrix 
mit verschwindenden Zeilen-
summen . 
Hier liegt die Situation einer unvollständigen Spektralanalyse vor, da 
V 2 1, V22' V23 3ewe-''ls z u r gleichen irreduziblen Konstituente gehören. 
Zwischen den entsprechenden Spektralkomponenten treten also u.U. Korrela-
tionen auf. Interpretiert man die Unterräume inhaltlich als Kontraste 
zwischen den einzelnen Varietäten, so ist dieses Ergebnis durchaus plausi-
bel. 
Wie in der Varianzanalyse kann auch hier eine Zerlegung der (mittelwert-
korrigierten) Sums-of-Squares angegeben werden, die sich aus der Projek-
tion der Beobachtungen auf die einzelnen invarianten Unterräume ergibt. 
Für die in Tabelle 1 mitgeteilten Daten ( Y^j^ 
s®i die Beobachtung in 
Zeile i , Spalte j und Block k der Permutationsstruktur S„x S ) 
8 
2 
sind dies einfach zwei getrennte Zerlegungen für die "Blockmittel" 
^ ( yijl + yij2 ) 
U n d d i e "Bl°ckkontraste" ¿• ( yijr yij2 ) 
• 

312 
Bernd Streitberg 
Quelle 
SSQ 
DP 
Varianzen 
Varietäten (V2> 
1865.6 
Selbstungen 
(V21) 
( 266.2 ) 
7 
38.03 
AI lg. Komb. F. (VJ2) 
(1225.3 I 
7 
175.04 
Reziproke E. 
<V
23> 
( 374.0 ) 
7 
53.43 
Spezifische sym. E. (V31) 
424.1 
20 
21.20 
Spezifische rez. 
(V41> 
244.9 
21 
11.66 
BLOCKMITTEL 
2534.6 
62 
Varietäten (Vj) 
79.3 
Selbstungen 
(v2I) 
( 13.3 1 
7 
1.90 
Allg. Komb. F (v2J) 
( 50.8 ) 
7 
7.26 
Reziproke E. 
' V 
( 15.2 ) 
7 
2.18 
Spezifische sym. E. (V3l) 
51.0 
20 
2.55 
Spezifische rez. E. (V41) 
82.2 
21 
3.91 
BLOCKKONTRASTE 
212.5 
62 
TOTAL 
2747.1 
124 
Tabelle 2: SSQ-Tabelle 
In der Spalte "Varianzen" sind die geschätzten Spektraldichten bzw. für 
V^ die Hauptdiagonalelemente der Spektraldichtematrizen angegeben. Man 
erkennt, daß die beobachteten Daten hauptsächlich durch genetische Varia-
tion zwischen den Varietäten bestimmt sind. Für eine genauere Aufschlüs-
selung können die Standardverfahren der multivariaten Analyse eingesetzt 
werden, da unter Normalverteilungsannahmen die geschätzten Spektraldichte-
matrizen unabhängig Wishart-verteilt (mit den angegebenen Freiheitsgraden) 
sind. 
6. ZUSAMMENFASSUNG 
Die hier überblicksartig vorgestellten algebraischen Verfahren erlauben 
(a) die Aufstellung von Modellen für korrelierte Zufallsvariablen allein 
aufgrund bestimmter Symmetrieannahmen 
(b) die Vereinfachung der statistischen Analyse derartiger Modelle durch 
Übergang zu einer maximal unkorrelierten Form. 

Über eine Verallgemeinerung der Spektralanalyse 
313 
Für zahlreiche praktische Anwendungen wäre es von Interesse, Korrelations-
strukturen der hier betrachteten Klassen mit beliebigen linearen Modellen 
für die "festen Effekte" zu verbinden. Statistische Schätz- und Testver-
fahren für derartige Situationen wurden in Streitberg (1979) behandelt. 
Literatur 
Anderson, T.W. (1958): Introduction to Multivariate Statistical Analysis. 
New York. 
Bartels, C.P.A. und Ketellapper, R.H. (1979): Exploratory and Explanatory 
Statistical Analysis of Spatial Data. Boston. 
Box, G.E. et al. (1954) : The Design and Analysis of Industrial Experiments. 
Edinburgh. 
Cockerham, C.C. und V.eir, B.S. (1977): Quadratic Analysis of Reciprocal 
Crosses. Biometrics Vol. 33, S. 187-203. 
Good, I.J. (1958): The Interaction Algorithm and Practical Fourier Analysis, 
JRSSSer. B Vol. 20, S. 361-372. 
Good, I.J. (1960): Addendum. JRSS Ser. B Vol. 22, S. 372 - 375 
Good, I.J. (1971): The Relationship between two Fast Fourier Transforms. 
IEEE Trans, on Comp. Vol 20, S. 310-317 
Halperin, M. (1951): Normal Regression Theory in the Presence of Intraclass 
Correlation. AMS Vol. 22, S. 573-580. 
Hannan, E.J. (1965) : Group Representations and Applied Probability. London. 
Hayman. B.I. (1954) : The Theory and Analysis of Diallel Crosses. Genetics 
Vol. 39, S. 789 - 809. 
James, A.T. (1957) : The Relationship Algebra of an Experimental Design. 
AMS Vol. 28, S. 993-1002. 
Ledermann, W. (1977): Introduction to Group Characters. Cambridge. 
McLaren, A.D. (1963): On Group Representations and Invariant Stochastic 
Processes. Proc. Camb. Phil. Soc., Vol. 59, S. 431 - 450. 
Neider, J.A. (1965): The Analysis of Randomized Experiments with Orthogonal 
Block Structures I. Proc. Roy. Soc. London A, Vol. 283, S. 147 - 162. 
Streitberg, B. (1979): Multivariate Models of Dependent Spatial Data, in: 
Bartels und Ketellapper (1979). 
Streitberg, B. (1981) : Über gruppentheoretische Methoden in der Varianzkom-
ponentenanalyse. FU Berlin. 
Yates, F. (1937): The Design and Analysis of Factorial Experiments. Harpen-
den. 


On a Generalized Iteration Estimator 
Götz Trenkler 
1• INTRODUCTION 
Consider the general linear model 
y = Xg + u 
where y is a Txl observed random vector, 
X is a given Txp regressor 
matrix of full column rank, 
g is a pxl vector of unknown parameters, 
E(u) = O , and Cov(u) = a2I with a2 unknown. Let the eigenvalues of 
X'X be denoted by X. > 
>...> X > 0 
. The most widely used estimator 
1 — 2 — 
— p 
of the parameter vector 6 is the least squares (LS) statistic 
B = (X'X)"1 X'y 
which by the well known Gauss-Markov theorem is superior to any other 
linear unbiased estimator of 8 . However, if multicollinearity is present 
among the columns of X , the total variance 
V(B) = a2 tr(X'X) 1 
A i 
3 = 1 
D 
may become inordinately large, and, "the values of t statistics are low, 
confidence intervals are broad, the influence of one regressor may be 
erroneously credited to another and it may be difficult to say that an 
individual regressor influences the dependent variable" (Kadiyala 1979 ) . 
In response to this problem, several alternative methods of estimation 
were proposed during the last decade. Although being 
biased these 

316 
Götz Trenkler 
estimators force back the effects of too small eigenvalues of X'X by 
aiming at a reasonable trade-off between bias and total variance. As a 
consequence, they outperform the LS-estimator by a number of mean squared 
error criteria. Some of them are now in widespread use, see for example 
the bibliographies of Alldredge/Gilb (1976), Vinod (1978) and Amemiya 
(198o). The most important biased estimators are 
Ridge estimator (k 
0) 
3, = (X'X + kl)_1 x'y 
k 
Hoerl/Kennard (197o a, b). 
Shrunken estimator (0 < s < 1) 
i = se 
s 
Sclove (1968), Mayer/Willke (1973). 
Principal components estimator (0 £ r < p) 
3 = (X'X)+ X'y 
r 
r 
Kendall (1957), Massy (1965), Marquardt (197o), where 
(X'X)* is a 
"generalized inverse" of X'X having prescribed rank r. 
The aim of the present article is to discuss a generalization of the 
recently introduced biased 
Iteration estimator ( 0 < a < 1/X ; n = 0, 1, 2, ...) 
n 
3 
= a 
E 
(I - aX'X)1 X'y 
n, a 
i=o 
Trenkler (1978), and 

On a Generalized Iteration Estimator 
317 
Inversion estimator (a > 0 ; n = 1, 2, 3, ... ) 
-i 
B 
= a 
I 
(I + aX'X) 
X'y 
Trenkler (1981), which is defined by 6 . = A y 
n,A 
n 
where 
n 
A = 
E 
(I - AX'X)1 AX' 
n 
i=o 
with a pxp matrix A fulfilling certain conditions to ensure convergence 
with respect to n . I t will be seen that this "generalized iteration 
estimator" (GIE) represents a wide class of biased estimators containing 
all estimators mentioned before if the control quantities A and n are 
suitably chosen. The matrix A 
can be calculated by an iterative 
n 
procedure whose speed of convergence is of first order, but, by an 
alternative method it is possible to identify a subsequence of A^ which 
converges more rapidly. 
The statistical properties of the GIE will be of special interest. It will 
be shown that it is 
- a linear homogeneous estimator 
- a linear transform of the LS-estimator 
- shorter than the LS-estimator with respect to the euclidean norm 
- superior to the LS-estimator with respect to various mean squared 
error criteria 
- insensitive to multicollinearity. 
2. THE GENERAL ITERATION ESTIMATOR 
Suppose A = diag {A_.} is the diagonal matrix consisting of the p positive 
eigenvalues of X'X . Let us assume that X is written in the following 
form 
X = Q'ftP 

318 
Götz Trer.kler 
1/2 
1 
(singular value decomposition) with the T*p matrix fi =|~A 
, 0~| and 
orthogonal matrices P and Q 
(cf. Rao/Mitra 1971, p. 6). Consequently 
X'X may be represented as X'X = P'AP 
Consider now a pxp matrix A = P'AP where A = diag 
with i . ^ 0 , 
j = 1, ... , p . It is evident that A is symmetrical and commutes with 
X'X . Further, there is an infinite number of such matrices A . For 
each of them we can define the sequence of matrices 
n 
A = 
£ 
(I - AX'X)1 AX' 
n 
i=o 
where n = 0, 1, 2, ... . Only converging sequences A 
will be 
n 
investigated by using the spectral norm 
| 
| = max^T^ with p j r 
j = 1, ... , p , being the eigenvalues of A 'A 
(cf. Ben-Israel/Greville 
n n 
1974, pp. 33-37). Since all matrix norms are equivalent convergence w.r.t. 
spectral norm implies pointwoise convergence and vice versa. 
To treat these questions of convergence first a result is established that 
will also turn out to be useful in the further context. 
Theorem 1: If 6 . ^ 0 , 1 = 1 , ... , p 
, the matrix A 
has the 
3 
n 
representation 
A = P' I" (I - (I - AA)n+1 )A _ 1 / 2 , 0 H Q . 
n 
-
Proof: From the singular value decomposition X = Q'fiP it is easily seen 
that 
(I - AX'X)1 = P'(I - AA)1 P and consequently 
(I - AX'X)1 AX' = P' 
(I - AA)1 AA 1 / 2 , 0 
Q 
so that 
n 
i 
1/? 
A = P' |_ 
Z 
(I - AA) AA ' , 0 ] Q . 
i=o 

On a Generalized Iteration Estimator 
319 
Since <5.^0 it follows that 1 - 6.A. ^ 1 for all j. From the identity 
D 
D J 
(x ? 1) 
£ 
x = 
i=o 
1 - X 
1 - x 
n+1 
we may derive the asserted representation. 
Remark 1: If |1 - 6 .A . | < 1 , j = 1, ... , p, then we have 6. ^ 0, and 
1 J 
+ 
J 
the sequence A 
converges with limit X = (X'X) 
X' . This is an easy 
n 
consequence of the identity 
A - x 
= max 
1<J <p I1 -¥j 
I n+l 
f? 
Remark 2: Let us now drop the assumption that all 6_. differ from zero 
by letting 
A = 
I A 
0 
r 
0 
A 
\ 
p-r 
< 5 . ^ 0 , j = l , . . . , r , A 
= 0 . 
3 
P - r 
If we partition I and A in the same way as A we obtain similar to the 
proof of theorem 1 
A = P' 
n 
(I - (I - A A ) n + 1 )A_1/2 
r 
r 
r r 
r 
Thus, if |l - <SX| < 1 for j = 1, ... , r and 
= 0 for 
j = r+1, ... , p , the sequence A 
converges, but now with limit 
+ 
n 
(X'X) X' where 
r 
(X'X) 
= P' 
r 
-1 
P . 

320 
Götz Trenkler 
By induction, it may be readily shown that A^ is also obtainable by the 
iterative procedure 
AX' 
A 
. = (I - AX' X) A + AX' 
n+1 
n 
( 1 ) 
It should be noted that the convergence of this iteration process can also 
be proved with the tools of Ben-Israel/Greville (1974, p. 292), if 
| |xx+ - XAX1 I I < 1 . This is equivalent to the condition 
|l - < 5 X | < 1, 
j = 1, ... , p 
, cf. remark 1. But as remark 2 has shown convergence may 
also happen in a more general framework which is of special importance if 
we want to establish some links to the principal components estimator. Let 
us now define the class of generalized iteration estimators (GIE) 
V = { g 
¡ 6 
= A y and A converges } 
n, A 1 n, A 
n 
n 
This means that either 
|l - A,6.1 < 1 for all j , or 
I1 — X.6.] < 1 , 
j : 
D J 
j = 1, ... , r and 
6. = 0 for j = r+1, ... , p 
. O f course, we could 
have considered still other matrices A to assure convergence of A^ , 
but we are interested only in the two cases mentioned above. 
Since the speed of convergence of the iterative procedure (1) can be 
tediously slow we shall now present another, rapidly converging sequence 
of matrices constituting a subsequence of A^ 
Theorem 2: Consider the following matrices 
Y = AX' 
o 
Y 
= (21 - Y X) Y 
n+1 
n 
n 
Then we have Y = A 
, n = 0, 1, 
n 
2n-l 

On a Generalized Iteration Estimator 
321 
Proof: By theorem 1, it suffices to prove 
Yn = P" [ (I - (I - AA)2 ) A~1/2 , oj O . 
(2) 
We shall do this by induction on n. Evidently, the assertion holds for 
2 n 
n = 0 
. Assume now that (2) is valid for n. After setting B = (I - ¿A) 
by the induction hypothesis we obtain 
Y 
= P' [ 2 (I - B) A"1/2 - (I - B)2 A_1/2 , O ] 0 
n+1 
= P' [ (I - B 2) A 1/2 , O J 0 
= P' [ (I - (I - AA) 
) A ' , o] Q 
= A 
2 n -1 
This is the assertion for n+1. 
Before discussing the statistical properties of the class V we now state 
some useful identities whose simple proofs will be omitted. 
Lemma 1: 
(i) A X = P1 (I - (I - AA)n+1 ) P 
n 
(ii) A A' = P' (I- (I - AA)n+1 )2 A_1 P 
n n 
(iii) I - A X = (I - AX'X)"+1 
n 
(iv) A XX+ = A 
n 
n 
3. BASIC STATISTICAL PROPERTIES 
Our first objective is to demonstrate that f contains all biased 
estimators from section 1, and, even the LS-estimator. 

322 
Götz Trenkler 
Theorem 3: 
(i) 
6 
= 6 
(LS-estimator) 
n, A 
if 
A = (X'X) 
1 
for all n. 
(ii) 
S 
= 3, 
(ridge estimator) 
n, A 
k 
if 
A = (X'X + k l ) _ 1 
and n = 0 . 
(iii) B 
= 6 
(shrunken estimator) 
n, A 
s -1 
if 
A = s(X'X) 
and n = 0 . 
(iv) 
® n a = ® r 
(principal components estimator) 
if 
A = (X'X) + 
for all n. 
r 
(v) 
6 
= 6 
(iteration estimator) 
n,A 
n,a 
if 
A = al 
. 
(vi) 
3 
= 6 
(inversion estimator) 
n,A 
n,a 
if 
A = a(I + aX'X)" 1 
. 
Proof: Properties (i) - (v) can be seen immediately, for (vi) observe that 
I - a(I + a X ' X ) - 1 X'X = I - (I + aX'X)" 1 (ctX'X + I - I) 
= (I + aX'X)" 1 
. 
The preceding theorem gives rise to an interesting generalization. Define 
the following estimators 
Iterative ridge estimator (k >_ O ; n = o, 1, 2,... ) 
~TRF 
^ 
i 
—i—1 
6 
= 
I 
k 
(X'X + kl) 
x'y 
i=o 
Iterative shrunken estimator (O 
s _< 1 ; n = 0) 
B I S E = (1 - (1 - s ) n + 1 ) ( X ' X ) _ 1 X'y 

On a Generalized Iteration Estimator 
323 
by considering the estimators from (ii) and (iii.) with a control parameter 
n which is now allowed to attain all nonnegative integers. These 
statistics have two control parameters, and converge to the ordinary 
LS-estimator for increasing n 
, if k > 0 and 0 < s <_ 1 respectively. 
A further step of extension is possible when we start from the matrix 
A = (X'X + P'KP)-1 
where K = diag {k_.} with k_. >_ 0 , j = 1, . . . , p . 
For n = 0 we obtain the generalized ridge estimator 
B k = (X'X + P'KP)"1 X'y 
(cf. Goldstein/Smith 1974), whereas for n > 0 we may introduce the 
Iterative generalized ridge estimator (k_. ^ 0 ; n = 0, 1,2, ... ) 
"TPRF 
^ 
i 
—1—1 
3 
= 
E 
(P'KP) 
(X'X + P'KP) 
X'y 
i=o 
which has p+1 control parameters. 
Let us now return to the more general case where A = P'AP 
. From 
condition (iv) of lemma 1 we can conclude that 6 
= A y = A XX+y 
n, A 
ni 
n 
= A X3 which shows 3 
to be a linear transform of 
3 . Obviously, 
n 
n,A 
Bn ^ is a homogeneous linear estimator whose squared euclidean length 
is given by 
3 ' 3 , = i ' ( A X ) 2 6 
n,A n,A 
n 
Hence, by condition (i) of lemma 1 we can see that with probability one 
3 , is shorter than 3 
n, A 
The next theorem is an easy consequence of the preceding results. The 
proof will be omitted since it requires only some knowledge about the 
characteristics of homogeneous linear estimator, see for example Trenkler 
(1981, p. 29). 

324 
Götz Trenkler 
Theorem 4: Let g 
be a member of T 
. Then g „ has the followinq 
n,A 
n,A 
characteristics 
Expectation 
E(ß 
) = A Xß 
— 
n, A 
n 
P" (I - (I - AA) n + 1) Pg 
Bias 
B(S „) = (A X - I) g 
n, A 
n 
= -P1 (X - AA)n+1 Pg 
2 
Quadratic bias 
D(g 
) = g' ( A X - I ) 
g 
n, A 
n 
i'P' (I - AA) 2 n + 2 Pß 
I 
(1 - 6.X.) 2 n + 2
 Y
2 
3=1 
where y = P(5 
Covariance matrix 
Cov(g 
„) = a 2 A A 
n,A 
n n 
a 2 P' (I - (I - AA) n + 1) 2 A 1 P 
Total variance 
V(g „) = o 2 tr(Cov(g 
J ) 
n,A 
n,A 
p 
(i - d - 
y ^
1 )
2 
b-l 
Mean squared error G(g 
) = V(ß J + D(ß „) 
— — 
— 
— — 
n,A 
n,A 
n,A 
P 
Z 
j = l 
(1 -(1 - 6.X.)n+1)2 
2n+2~ 
2 
D D 
. M_ * i i ,/ 
A . 
D 
+ (1- S.A.) y. 
D D 
D 
Mean squared 
M(g 
) = E | (ß 
A - g)(g 
- ß) ' 
n, A 
n, A 
n, A 
error matrix 
= Cov (ß 
) + B(ß 
) B (g 
) ' 
n,A 
n,A 
n,A 
= P' a 2 (I - (I - AA) n + 1) 2 A - 1 
. .. n+1 
, , 
, . . n+1 -i 
(I - AA) 
YY (I " AA) 
J P 

On a Generalized Iteration Estimator 
325 
When evaluating the quality of an estimator it is customary to use the 
mean squared error which coincides with the total variance when the 
estimator under consideration is unbiased. But mean squared error is only 
one measure of goodness of an estimator 6 . Another is generalized mean 
squared error given by 
Gh (6) = E I" (6 - 8) • H(B - 6) ] 
where H is a nonnegative definite matrix. This measure allows for 
weights on the components of 8 . However, there is an infinite number 
of weight matrices making it rather difficult to select one. Fortunately, 
we have the following result due to Theobald (1974): If M(BJ = 
E | 
- 8)(6^ - 8)' ] denotes the mean squared error matrix of any two 
estimators 8. , j = 1, 2, then it holds that G (8.) > G (B0) for all 
j 
H _ 1 
H^ ¿. 
positive definite matrices H if and only if 
MfB^ - M (82) 
i s 
positive 'definite. As a consequence, it makes sense to compare two 
estimators by inspecting the difference of their mean squared error 
matrices. It also appears natural to judge the performance of an estimator 
8 by a comparison with the widely used LS-estimator. Indeed, by choosing 
the control parameters suitably an extensive subclass of V is now shown 
to be better than the LS-estimator. 
Theorem 5: Suppose that 0 < 1 - 6 ^ < 1 f°r 
j = 1, ••• < P • Then 
each of the following conditions implies the other: 
(i) M(8) - M(B 
) is positive definite 
n, A 
X. (1 - 6.X.)n+1 
p 
( Ü ) y' diag { — 
3 3
 
n +. 
Y - . 
Y < a 2 
2 - (1 - 6 X.) 
J 
D 3 
Proof: In D. Trenkler/ G. Trenkler (1981) the following equivalence is 
derived where 8 is an arbitrary, not necessarily linear estimator for 
(i*) M(8) - M(8) is positive definite 
(ii')B(B)' I" (X'X)"1 - — 
Cov(B) 
B(8) < o2 
a2 

326 
Götz Trenkler 
provided that 
(X'X) * - ^rCov(B) is positive definite. But, clearly, 
by the assumed restrictions on 6 
the matrix 
_ 
. 
1 "(1 -(1 - 5 . X . ) n + V 
p 
(X'X) 
- -7- Cov (B ,) = P' diag { 
2_J 
} 
p 
u 
n,a 
^ 
j—l 
j 
is positive definite. With the aid of theorem 4 it is now an easy task to 
show that the expression on the left of {ii1) with B = B 
equals the 
n, A 
corresponding expression of (ii). 
Corollary 1: If 0 < 1 - 
< 1 for j = 1, ... , p there always 
exists an integer n 
such that M(ß) - M(ß 
) is positive definite 
o 
n, A 
for all n > n 
— o 
This follows from theorem 5 (ii), since the expression 
p 
, X. (1 - S.X.)n+1 
Z 
y 
— 
3-J 
j 
. . .n+1 
tends to zero when n approaches infinity. 
"1 — 1 
Z 
~ (1 — O.A.) 
D D 
Unfortunately, the integer n 
depends on the unknown parameters a 2 
o 
and B (via y = Pß) . But suppose we have some a priori information 
given by ß' ß <_ Ka2 where K is a known positive constant. To specify 
such a constant, knowledge of an upper bound for B ' B and a lower bound 
for a2 would suffice. Then we can explicitly find an integer n o 
depending only on K and the eigenvalues of X'X and A such that 
M(ß) - M(ß 
) is positive definite. This can immediately be derived 
n, A 
from assertion 
(iv) of the next corollary by choosing an integer n^ 
which fulfils X, inll0 + 1 < 1/K where m = max 
(1 - 6.X.) 
1 
1£3<_P 
3 3 
Corollary 2: If 0 < 1 - 6_.A_. < 1 for j = 1, 
, p , the following 
conditions are sufficient for M(ß) - M(ß ,) to be positive definite 
n, A 
p 
(i) 
y" diag {X. (1 - 6.X.)n+1 }. . y < a2 
J 
3 1 
D=1 
(ii) m n + 1 ß'X'Xß < a 2 

On a Generalized Iteration Estimator 
327 
(iii) B'X'XB < a2 
(iv) A m n + 1 B'B < a2 
(v) 
A1 6'6 < a2 
. 
Proof: (i) is a consequence of the fact that 1 < 2 -(1 - 6.A_.)n+1 < 2 , 
(ii) follows from m n + 1 B'X'XB = y' diag {m n + 1 A ^ } y 
n+1 
X. (1 - i.A.)n+1 
> y' diag {A. (1 - 6.A.) 
h 
> y'diag { -3 
} y 
, ( i i i ) 
3 
3 3 
2 - (1 - 6. A .) 
D 3 
and (iv) can be derived directly from (ii), and (v) follows from (iv). 
Note that conditions (iii) and (v) do not depend on the control parameter n. 
4. 
ADMISSIBILITY 
In the statistical literature there are two notions of admissibility. 
Either a class of estimators is compared with the LS-estimator by using 
the mean squared error (Mayer - Willke- admissibility) or an estimator is 
judged by a comparison with all homogeneous linear estimators by means of 
the risk function G (B) (weak admissibility). We shall discuss both 
H 
criteria of goodness for the GIE. Throughout this section it will be 
assumed that 0 < 1 -6.A. < 1 for j = 1, ... , p 
4.1. Mayer - Willke - admissibility 
A class E of estimators will be called Mayer - Willke - admissible if for 
every B € ® P there is a 6 in E such that G (B ) < G (B) = V(B) (see 
Mayer/Willke 1973). It will now be seen that the GIE-class ï is Mayer -
Willke - admissible. First we develop a more general result. 
Theorem 6: Suppose there is a positive definite weight matrix 
H = P' diag { K } P . Then 

328 
Götz Trenkler 
(i) the weighted mean squared error of 6 
is given by 
n, A 
P 
G (ß J = 
Z 
h. I az(l -(1 - 5.X.)"")' X. 
H n,A 
D 
D D 
: 
2,. 
.. 
. , ,n+l, 2 , -1 
j = l 
. , , 2n+2 2 
+ 1 - 5.X.) 
y. J 
J D 
D 
(ii) For each n there is an optimal set of control parameters 
* 
& 
A 
5 , ... , 5 
satisfying O < (1 -5.X.) < 1 which minimizes 
1 „ 
p 
" 
G (B „ ) . The corresponding minimum G (¡3 
*) is absolute 
H n,A 
H n,A 
and independent of n and H 
Proof: (i) Since G (6) = tr(HM(B)) for every estimator 6 (cf. Trenkler 
H 
1981, p. 28) by theorem 4 we may conclude 
i- 9 
n+1 2 -1 
G ( B 
) = tr P' diag {h.} PP'| a 2 (X - (I - ¿A) 
) A 
H n / A 
+ (I - AA)n+1 YY' (I - AA)n+1 ] P 
= 
Z h. Ta2(l -(1 - 5.X.)n+1)2 XT1 
j=l 
3 
3 3 
3 
+ Y
2 (1 - 6 , X . ) 2 n + 2 ] . 
D 
D D 
J 
(ii) Differentiating 
G
H(B n 
with respect to 5^. we obtain 
(6 J 
H 
n' A 
= h.(2n + 2) (1 - 5.X.)n 
a 2 (1 -(1 - S.X.)n+1) 
D 
D D 
D D 
35 . 
D 
The set of equations 
vi 
(1 - 5.X.)n 
X. ] 
J 
D
O 
D 
H n,A 
= 0 
, J = 1, ... , p 
35 . 
D 
has a unique solution given by 
1 
2 ^ 5 — )"+1 1 
, 3 = 1 
• 
(3) 
3 
X. 
" 
a 2 + Y X. 
J 
J D 

On a Generalized Iteration Estimator 
329 
As O < 
= 
< 1 we have 0 < ( 1 - 6 . A . ) < 1 
. Straiahtforward 
a 2
+ y 2 A. 
3 D 
3 3 
calculations yield 
^ 
K
1 
p 
n, A * = P' 
diag { -J 
J — 
} 
, O ] Qy 
„2 
„2 , 
3-1 
a 2 + y2 A 
j j 
showing that 6 
* and, consequently G (g 
X) do not depend on n 
n, A 
H n,n 
and H . Furthermore, after some simple steps of computation the matrix 
3 2 G
D<e 
J 
^ 
H n,A 
36 . 36, 
A=A~ 
3 
k 
is readily derived to be positive definite implying that Bn 
gives 
an absolute minimum. 
Unfortunately, 
a * depends on the unknown quantities y = P8 and 
a 2 
such that this estimator does not appear to be operational. It should 
be mentioned that Obenchain (1978) obtained 
8 n
 
by another approach 
as a member of the so-called "class of generalized ridge estimators". 
Since 
a 2 
1 
a 2 + y 2 X . 
1 + y 2 A /a2 
3 D 
J 3 
2 
it might suffice to use a reliable estimator of y A^/cr2 which could 
be inserted in (3). As a matter of fact, if the disturbance vector u is 
multinormally distributed there exist an unbiased and a maximum likeli-
hood estimator of y A^/cr2 (details may be taken from Obenchain's 
article). However, the resulting estimator exhibits stochastic control 
parameters so that the statistical properties do not seem to be easily 
derivable. It should be remarked that the preceding theorem could have 
been obtained as a special case of proposition 2 of D. Trenkler/G. Trenk-
ler (1981) . 

3 3 0 
Götz Trenkler 
Corollary 3: 
(i) 
There always exist an integer n^ such that 
G (8) - G (8 J > 0 
for all n > n 
H 
H n,A 
— o 
(ii) The class >? is Mayer - Willke- admissible. 
(iii) If there is a known constant K such that 8'8 ^ Ka2 
then one 
* 
can find a number n 
not depending on unknown parameters 
satisfying 
G„(6) - G (8 J > o 
for all n > n* . 
H 
H n,A 
— 
Proof: (i) A little algebra yields 
P h. (1 - 6.A.)n+1 
GH(Bn,A> = G H ( e ) + 
.Z
=1 -
L 7 "
J
J 
C 
" V
/ 
" 
(a2 + X .Y 2) - 2a2 1 
. 
3 D 
J 
Letting n -*• 00 we see that the expression inside the brackets becomes 
negative when n exceeds a certain integer n 
(ii) is obvious since we can choose H = I in (i). Note that the integer 
n^ in (i) does not depend on H 
, but on a 2 and 8 
(iii) Let n 
be the smallest integer satisfying the inequalities 
# 
r , ^ n +1 
2 
(1 - 6.A.) 
< 
, 
j = 1, ... , p . 
3 3 
KX + 1 
* 
Since 
8 ' 8 
= Y ' Y 
> J 2 we have 
2 
> 
( K A 
+ 
1 ) ( 1 
- 
6 . A . ) N 
+ 1 
> 
D 
1 
3 1 
~ 
* 
* 
( - — A. + 1) (1 - 6 .A . ) n + 1 
which gives 2CT2 > (a2 + A,y2) (1 - S .A . ) n + 1 
a2 
1 
D 3 
1 D 
J J 
for j = 1, ... , p 
. The decomposition of 
i n 
P r o o f 
(i) implies the asserted inequality for all n 
n 
(cf. corollary 1). 

On a Generalized Iteration Estimator 
331 
4.2. Weak admissibility 
The criterion of admissibility in the sense of Mayer and Willke judging a 
class of estimators by a comparison with only one estimator does not 
appear very satisfactory. Several authors, for example Shinozaki (1975), 
Rao (1976) and Hoffmann (1977 a, b) discuss another notion of admissi-
bility based on decision theory permitting a comparison will all 
homogeneous linear estimators by using the risk function G (8) - In 
H 
their approach it is assumed that the parameter vector may vary in a 
subset 0 
of IRP 
Let now H be a positive definite weight matrix being not necessarily 
of the form of section 4.1. . A homogeneous linear estimator 
8q 
(i.e. 
it may be written as C y with a nonstochastic matrix C 
) will be 
o 
o 
called 
(H, 0 ) - admissible if there exists no other linear homogeneous 
estimator 
6 such that for all S f 0 
we have G (8) < G„(8 ) with 
H 
^ 
H 
O 
strict inequality for at least one parameter 
8 £ 0 
• 8 Q is said to 
be 0 - admissible if it is 
(I, 0 ) - admissible. We shall now pay 
attention to the case where 
8 is constrained to the p-dimensional 
ellipsoid 
0„ = iB I 6 £ IRP and 
B'NS < a 2 } 
N 
1 
— 
with N being a given positive definite matrix. The following result of 
Hoffmann (1977 a, b) characterizes 
0^ - admissible estimators by a set 
of conditions that can easily be examined. It will be stated here without 
proof. Surprisingly, in this theorem the weight matrix H does not play 
any role. Indeed, Hoffmann has shown the equivalence of 
(H, 0 ) - and 
(I, 0 ) - admissibility. 
Theorem 7: Suppose 
8 = Cy is a homogeneous linear estimator. Then the 
following holds: 
8 is 0 
- admissible if and only if 
a) there exists a matrix F such that C = FX' 
1/2 
1/2 
b) (X'X) 
F (X'X) 
is nonnegative definite 
1/2 
1/2 
c) I - (X'X) ' F (X'X) 
is positive definite 
d) tr { (X'X)"1 N 
(I - FX'X)"1 - I ] } <_ 1 . 

332 
Gôtz Trenkler 
As an immediate conclusion we obtain 
Theorem 
(i) If N = X'X there always exists a member 
6 
of ¥ such 
a 
n, A 
that B 
is 
0 - admissible. A necessary and sufficient 
n, A 
N 
condition is given by 
P 
. 
I 
(1 - S.X.) 
< 1 + p 
. 
(4) 
3=1 
3
D
-
(ii) If N = 
I, K > 0, there always exists a member 
3 
of ¥ 
K 
^ 
Il/A 
such that 0 „ is 
0„ - admissible. A necessary and 
n, A 
N 
sufficient condition is given by 
p 
i _ ( i - 
6 . A . ) n + 1 
£ 
— - T 
1 K 
• 
(5) 
j=l A. (1 - 6 . X . ) n 
D 
1 3 
Proof: Only condition d) of theorem 7 has to be shown separately. By 
n 
choosing F = 
T. (I - AX'X) A we obtain a) . As can be seen directly 
i=o 
1/2 
1/2 
1/2 
1/2 
F commutes with 
(X'X) 
= P'A 
P implying that 
(X'X) 
F (X'X) 
= FX'X = A x by lemma 1 is nonnegative definite (actually, it is positive 
n 
1/2 
1/2 
definite) . The same reasoning shows that I - (X'X) 
F (X'X) 
is 
positive definite. Before treating d) note that 
(I - FX'X) 1 - I 
= P ' ( ( I - A A ) n l - I ) P 
. Consider now 
N = X'X : Condition d) becomes 
tr P' ((I - AA)~n_1 - I) P 1 1 
from which (i) follows. 
N = 1/K I: The inequality in d) may be written as 
tr P' A _ 1 ((I - ¿A)"""1 - I) P < K 
which gives (ii) 

On a Generalized Iteration Estimator 
333 
A sufficient condition for (4) to hold is given by m n +* > 
P 
where 
n+1 
~ 
~ 1 + p 
— 
-i 
m = min 
(1 - 6.X.). Similarly, if K 
— > tr (X'X) 
it is 
l£j<p 
1 - m 
easily seen that (5) is valid. By letting 6 
0 
it is evident that 
for each n there is a control matrix A making 
B 
either X'X -
^ 
A, n 
or — I - admissible. 
5. CONCLUDING REMARKS 
Computationally, the use of the GIE appears to be very attractive since 
no matrix inversion is required if we choose for A a polynomial in X'X: 
A = a I + a.X'X + ... + a (X'X)P 
o 
1 
p 
= P' (a + a A + ... + a A P ) 
P 
o 
1 
p 
By a skilful choice of the control parameters a_. it can be achieved 
that the elements 6 
of the corresponding diagonal matrix i satisfy 
the convergence condition 
|l - 5 A | < 1 . Likewise, it would be 
reasonable to take for A a polynomial in X'X 
and 
(X'X) * . For the 
present it remains an open question how the matrix A has to be chosen 
such that simultaneously the resulting estimator is statistically and 
computationally satisfactory. 
In a recent article Hild (198o) treated a class of heterogeneous linear 
estimators containing f as a subclass. Unfortunately, in his approach 
questions of convergence were not investigated, and problems of sta-
tistical dominance were not sufficiently explored. 
From theorem 4 it follows that 8 
is remarkably robust when multi-
n, A 
collinearity drastically increases the total variance of the LS-estimator, 
for, by virtue of 1' Hospitals rule 
) tends to a finite limit 
when 
A 
approaches zero. 

334 
Götz Trenkler 
Äs an alternative to the ridge trace analysis (see for example Hoerl/ 
Kennard 197o b) we may have a look at the "generalized iteration trace" , 
a plot of coefficients of 
ß 
and of the residual sum of squares 
« 
n, A 
d> = (y - Xß 
) 1 (y - Xß „) against increasing values of n which 
Yn 
n, A 
n, A 
hopefully will guide one to a proper choice of an optimal control 
parameter n 
Probably a more promising method would be to apply the McDonald - Galar-
neau - strategy that shortens the GIE such that its squared length equals 
an unbiased estimate of 
6'6 
(see McDonald/Galarneau 1975 for the ridge 
estimator) . Since the resulting estimator has stochastic control para-
meters only simulation studies can give some insight into its statistical 
properties. This strategy was also used to fix the parameter n of 
iteration and inversion estimator in the simulation study of Trenkler 
(1981) where the LS-estimator was shown to be worse in almost all cases. 
References 
Alldredge J.R. and Gilb N.S. (1976), Ridge regression: An annotated 
bibliography, International Statistical Review 44, 355-360. 
Amemiya T. (198o), Selection of regressors, International Economic 
Review 2J_, 331-354. 
Ben-Israel A. and Greville T.N.E. (1974), Generalized inverses: Theory 
and applications, John Wiley, New York. 
Goldstein M. and Smith A.F.M. (1974), Ridge-type estimators for regression 
analysis, J.R.Statist. Soc .B 36_, 284-291. 
Hild C. (198o), Über einige verzerrte Schätzer für die Koeffizienten 
eines linearen Modells, Statistische Hefte 2J_, 145-155. 
Hoerl Ä.E. and Kennard R.W. (197o a), Ridge regression: Biased estimation 
for nonorthogonal problems, Technometrics 12, 55-67. 
Hoerl A.E. and Kennard R.W. (197o b), Ridge regression: Applications to 
nonorthogonal problems, Technometrics 12, 69-82. 
Hoffmann K. (1977 a). Admissibility of linear estimators with respect to 
restricted parameter sets. Math. Operationsforsch.Statist.Ser. 
Statistics 8_, 425-438. 
Hoffmann K. (1977 b), Zulässigkeit und Robustheit von linearen Schätz-
funktionen im linearen Regressionsmodell, Dissertation, Akademie 
der Wissenschaften der DDR. 
Kadiyala K. (1979), Operational ridge regression estimators under the 
prediction goal. Communications in Statistics A 8_, 1377-1391. 

On a Generalized Iteration Estimator 
335 
Kendall M.G. (1957), A course in multivariate analysis, Griffin, London. 
Marquardt D.W. (197o), Generalized inverses, ridge regression, biased 
linear estimation, and nonlinear estimation, Technometrics 12, 
591-612. 
Massy W.F. (1965), Principal components regression in exploratory sta-
tistical research, j.Amer.Statist.Assoc. 6o, 234-256. 
Mayer L.S. and WillkeT.A. (1973), On biased estimation in linear models, 
Technometrics 15, 497-5o8. 
McDonald G.C. and Galarneau D.I. (1975), A Monte Carlo evaluation of some 
ridge-type estimators, J.Amer.Statist.Assoc. 7o, 4o7-416. 
Obenchain R.L. (1978), Good and optimal ridge estimators, Ann. Statist. 6_, 
1111-1121. 
Rao C.R. and Mitra S.K. (1971), Generalized inverse of matrices and its 
applications, John Wiley, New York. 
Rao C.R. (1976), Estimation of parameters in a linear model (The 1975 
Wald Memorial Lectures), Annals of Statistics 6, lo23-lo37. 
Sclove S.L. (1968), Improved estimators for coefficients in linear 
regression, J.Amer.Statist.Assoc. 63, 597-6o6. 
Shinozaki N. (1975), A study of generalized inverse of matrix and esti-
mation with quadratic loss, Ph.D. submitted to the Keio University, 
Japan. 
Theobald C.M. (1974), Generalizations of mean square error applied to 
ridge regression, J.R.Statist.Soc. B 36, lo3-lo6. 
Trenkler D. and Trenkler G. (1981), Ein Vergleich des Kleinst-Quadrate-
Schätzers mit verzerrten Alternativen, Lecture given at the Annual 
Meeting of the German Society of Operations Research, to be 
published in the Proceedings Volume, Springer Verlag, Berlin Heidel-
berg New York. 
Trenkler G. (1978), An iteration estimator for the linear model, COMPSTAT 
1978, Physica Verlag, 125-131. 
Trenkler G.(1981), Biased estimators in the linear regression model. 
Mathematical Systems in Economics 58, Verlag Anton Hain, Meisenheim. 
Vinod H.D. (1978), A survey of ridge regression and related techniques 
for improvements over ordinary least squares, The Review of Economics 
and Statistics 6o, 121-131. 


Statistical Computing with a Text Editor 
Seppo Mustonen 
1. INTRODUCTION 
It is quite common that when writing a research report containing numerical tables the 
output from the conputer cannot be used as such, but the results have to be retyped 
manually. This may happen even if the conputer output is well designed, since the needs 
of the user nay change during the reporting phase. 
One solution to this problen is to have the possibility of saving results in output 
-Files which can subsequently be treated as text files using a text editing program. 
Parallel to this approach we have developed another solution to these editorial problens 
within the interactive statistical systen SURVO 76 operating on the desk conputer Hang 
2200VP, (Mustonen 1980a). 
Our solution has been realized as a new extensive SURVO 76 
nodule called EDITOR and it is closely connected to other parts of the systen. 
SURVO 76 EDITOR works as a normal text editor, but it also includes several nunerical 
and statistical operations thus forning a snail scale statistical operating systen. The 
main purpose of EDITOR is to lessen the burden of a statistician in data management and 
report writing. 
EDITOR can be used for various tasks encountered in statistical data 
processing like 
1) input and editing unformatted data, 
2) saving data in SURVO 76 files, 
3) editing SURVO 76 files and results, 
4) manipulating lists and multiway tables, 
5) arithmetical and statistical computations, 
6) data analysis. 
Working in editing node differs considerably from conversational interactive data pro-
cessing, as used for instance in the majority of SURVO 76 modules. 
Since both the data 
and the operations or connands are displayed and handled together within the frane of an 
edit field, the user has an extrenely close contact with his work and he has the possi-
bility to control each detail of the conputational and editorial activities better than 
in other forns of interactive analysis. An editorial operating system denands nore expe-
rience from the user than a simple conversational system, but at the same time gives 
more scope for imagination. Editorial mode is no substitute for conversational mode, in 
general. 
There are, however, several forms of interactive data analysis and management 
where editorial mode is more natural than conversational use. 
For example, problens in 
analysis of variance and log-linear nodels for contingency tables can be easily handled 
in EDITOR (Mustonen 19B0b). 
It may be reasonable to have different interactive working principles available in the 
same interactive statistical system and let the user select the devices nost suitable 
for a particular problem. In the SURVO 76 system we try to provide many of the statisti-
cal operations both in conversational and editorial mode. 
In this paper we shall describe principles and applications of the editorial approach. 
The main emphasis will be on statistical and numerical operations. Pure text processing 
activities will not be discussed in this context. 
A comprehensive account of SURVO 76 
EDITOR is given in Mustonen (1980b). 

338 
Seppo Mustonen 
When working with EDITOR the whole editing process is controlled 
by the normal keyboard 
keys and 32 programmable 'soft keys' (F-keys) which are used -for simple text editing. 
For more complicated tasks various editing operations are available. 
All the information is represented in an edit fieId which consists, for example, of 100 
columns and 250 rows. 
The field is always partially visible on the CRT which is like a 
window to the field. The user can easily scroll the text on the CRT to any direction by 
pressing certain F-keys with arrows indicating the direction. The editing operations are 
also typed in the edit field and they can be treated as normal text. 
Any operation is 
activated by moving the cursor to the corresponding line and by pressing key CONTINUE. 
Whenever needed the contents of the edit field 
(tables, text and operations) can be 
saved in an edit file by activating a SAVE operation. 
The edit field is like a notebook for the user, but it is much more flexible, since text 
and data in that notebook can be worked upon by editing operations and the results of 
these operations can be directed to any part of the field. Since the editing operations 
themselves are typed among the text and data, the user can place them as he likes, usu-
ally .near the object of operation. 
If the user likes he can put the operations on adjacent lines and carry them out step by 
step as an editing program, but usually this is unnecessary. 
On the contrary, it is 
typical that during the editing process the field is filled with a mixture of text, data 
and operations, and the user scratches unessential ingredients when needed. 
When starting a new job with EDITOR the upper left side corner of a 100x100 edit field 
is displayed on the 24x80 screen: 
Disp.l 
1 
SURVQ 76 EDITOR 
<C)1979 S.hustonen 
(100x100) 
1 
*_ 
2 
* 
3 
* 
4 
* 
5 
* 
6 
* 
7 
* 
8 
* 
9 
* 
10 
* 
11 
* 
12 
* 
13 
* 
14 
* 
15 
* 
16 
* 
17 
* 
18 
* 
19 
* 
20 
* 
21 
* 
22 
* 
23 
* 

Statistical Computing with a Text Editor 
339 
The size of the -field can be altered by using a redimensioning operation RED IN m,n. 
Observe that there is no strict correspondence between the size of the edit -field and 
the size o-f a page in the -final report. 
It is typical to have rather large edit -fields 
and let EDITOR automatically split the fields into pages when the document is printed 
with a PRINT operation. 
In display 1 representing an empty edit field, the cursor is blinking in the first posi-
tion and the user may start typing text, data values and editing operations. The system 
can in many respects be operated like a normal typewriter and the text appears conti-
nuously on the screen. 
Typical text editing tasks are accomplished simply by F-keys. We thus have special keys 
for insertion and deletion of characters and lines, for moving the cursor, etc. 
The leading principle in all editorial activities is that minimum effort (i.e. minimum 
number of touches with the keyboard) is needed for them. 
2. EXAMPLE ON DATA ANALYSIS 
Assume now that we have typed the following small data set in the edit field: 
Disp.g 
1 
SUR VP 76 EDITOR 
(Q1979 S.Hustonen 
(100x100) 
1 
* 
2 
* 
Yearly consumption of certain beverages per inhabitant 
3 
* 
in some European countries in 1974-1978: 
4 
* 
Coffee Tea 
Beer Wine Spirits 
5 
6 
7 
8 
9 
10 
11 
12 
13 
14 
15 
16 
17 
IB 
* 
19 
* 
20 
* 
21 
* 
22 
* 
23 
* 
If we like to sort this data set in ascending order with respect to 'Wine consumption', 
this will be carried out by a SORT operation which may be typed on any editor line (here 
on line 19) in the form 
* 
(kg) 
(kg) 
(1) 
(1) 
(1) 
* Denmark 
11.4 
0.43 116.3 11.5 
1.7 
* England 
1.5 
3.42 118.3 
5.7 
1.6 
* Finland 
12.0 
0.17 
55.2 
8.7 
2.9 
« France 
5.3 
0.11 
45.8 101.8 
2.5 
* Germany 
5.8 
0.17 148.0 22.9 
3.0 
» Holland 
8.7 
0.54 
81.5 11.2 
2.9 
« Ireland 
0.2 
3.78 127.9 
4.5 
2.1 
* Italy 
3.5 
0.06 
14.2 99.1 
2.0 
* Norway 
9.0 
0.19 
45.1 
3.2 
1.8 
» Spain 
2.5 
0.03 
47.6 71.8 
2.8 
* Sweden 
11.7 
0.34 
56.0 
8.6 
3.0 
» Switzerland 
8.7 
0.25 
70.9 44.8 
2.0 

340 
Seppo Mustonen 
Disp.3 
SURVO 76 EDITOR 
(P1979 S.Hustonen 
(100x100) 
1 
* 
2 
* 
Yearly consumption of certain beverages per inhabitant 
3 
* 
in some European countries in 1974-1978: 
4 
* 
Coffee Tea 
Beer Wine Spirits 
5 
* 
(kg) 
(kg) 
(1) 
(1) 
(1) 
6 
* Denmark 
11.4 
0.43 116.3 11.5 
1.7 
7 
* England 
1.5 
3.42 118.3 
5.7 
1.6 
8 
* Finland 
12.0 
0.17 
55.2 
8.7 
2.9 
9 
* France 
5.3 
0.11 
45.8 101.8 
2.5 
10 
* Germany 
5.8 
0.17 148.0 22.9 
3.0 
11 
* Holland 
8.7 
0.54 
81.5 11.2 
2.9 
12 
* Ireland 
0.2 
3.78 127.9 
4.5 
2.1 
13 
* Italy 
3.5 
0.06 
14.2 99.1 
2.0 
14 
* Norway 
9.0 
0.19 
45.1 
3.2 
1.8 
15 
* Spain 
2.5 
0.03 
47.6 71.8 
2.8 
16 
* Sweden 
11.7 
0.34 
56.0 
8.6 
3.0 
17 
* Switzerland 
8.7 
0.25 
70.9 44.8 
2.0 
18 
* 
11111 
19 
»SORT 6,17,18 
20 
* 
SORT has three parameters. The two -first parameters 6,17 refer to the lines which are to 
be sorted and the last parameter 18 is also a line number indicating the line with the 
sort key which is given as a mask '11111' in the column corresponding to 'Wine'. 
Now in order to start sorting we have to activate the SORT operation by pressing the key 
C0NTINIE when the cursor is on line 19. When CONTINUE is pressed EDITOR always tries to 
interpret the contents of the current editor line as an editing operation. 
If the line 
corresponds to a valid operation, it is immediately executed. Otherwise an error message 
is displayed. 
In this case activation with CONTINUE leads to the following display 
Disp.4 
1 
SURVO 76 EDITOR 
(Q1979 S.Hustonen 
(100x100) 
1 
* 
2 
* 
Yearly consumption of certain beverages per inhabitant 
3 
* 
in some European countries in 1974-1978: 
4 
* 
Coffee Tea 
Beer Wine Spirits 
5 
* 
(kg) 
(kg) 
(1) 
(1) 
(1) 
6 
* Norway 
9.0 
0.19 
45.1 
3.2 
1.8 
7 
* Ireland 
0.2 
3.78 127.9 
4.5 
2.1 
8 
* England 
1.5 
3.42 118.3 
5.7 
1.6 
9 
* Sweden 
11.7 
0.34 
56.0 
8.6 
3.0 
10 
* Finland 
12.0 
0.17 
55.2 
8.7 
2.9 
11 
* Holland 
8.7 
0.54 
81.5 11.2 
2.9 
12 
* Denmark 
11.4 
0.43 116.3 11.5 
1.7 
13 
* Germany 
5.8 
0.17 148.0 22.9 
3.0 
14 
» Switzerland 
8.7 
0.25 
70.9 44.8 
2.0 
15 
* Spain 
2.5 
0.03 
47.6 71.8 
2.8 
16 
* Italy 
3.5 
0.06 
14.2 99.1 
2.0 
17 
* France 
5.3 
0.11 
45.8 101.8 
2.5 
18 
* 
11111 
19 
*S0RT 6,17,18_ 
20 
* 

Statistical_ 
with_ a Text_J3ditor 
341 
The lines are now in an ascending order according to 'Nine'. 
To display then in de-
scending order Me simply change the SORT operation to a -SORT operation by inserting a 
sign 
19 
»-SORT 6,17,18 
and reactivation of line 19 by CONTINUE leads nou to 
Disp.5 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
11 
12 
13 
14 
15 
16 
17 
18 
19 
20 
SURVO 76 EDITOR 
(Q1979 S.Hustonen 
(100x100) 
Yearly consumption of certain beverages per inhabitant 
France 
Italy 
Spain 
Switzerland 
Germany 
Denmark 
Holland 
Finland 
Sweden 
England 
Ireland 
Norway 
-SORT 6,17,18 
some European 
Coffee Tea 
(kg) 
(kg) 
5.3 
3.5 
2.5 
8.7 
5.8 
11.4 
8.7 
12.0 
11.7 
1.5 
0.2 
9.0 
countries in 
Beer Wine 
(1) 
(1) 
1974-1978: 
Spirits 
(1) 
0.11 
0.06 
0.03 
0.25 
0.17 
0.43 
0.54 
0.17 
0.34 
3.42 
3.78 
0.19 
45.8 101.8 
14.2 99.1 
47.6 
70.9 
148.0 
116.3 
81.5 
55.2 
56.0 
118.3 
127.9 
45.1 
71.8 
44.8 
22.9 
11.5 
11.2 
8.7 
8.6 
5.7 
4.5 
3.2 
11111 
2.5 
2.0 
2.8 
2.0 
3.0 
1.7 
2.9 
2.9 
3.0 
1.6 
2.1 
1.8 
As an example o-f hou typical data analytic problems are solved in EDITOR we try to esti-
mate a linear regression model where 'Wine' is the dependent variable. 
To have a con-
stant term in the model we first create a new column o-f plain l's and this takes place 
by a SET operation of the form 
Disp.6 
15 
* England 
1.5 
16 
» Ireland 
0.2 
17 
* Norway 
9.0 
18 
* 
19 
»SET 6,17,18_ 
20 
* 
3.42 118.3 
5.7 
1.6 
3.78 127.9 
4.5 
2.1 
0.19 
45.1 
3.2 
1.8 
saying that the non-blank characters on line 18 ought to be inserted on lines 6-17. 
After the activation of this operation we have 

342 
Seppo Mustonen 
Disp-7 
1 
SURVO 76 EDITOR 
(C)1979 S.HustoneTi 
(100x100) 
1 
* 
2 
* 
Yearly consumption of certain beverages per inhabitant 
3 
* 
in some European countries in 1974-1978: 
4 
* 
Coffee Tea 
Beer Wine Spirits 
5 
* 
(kg) 
(kg) 
(1) 
(1) 
(1) 
6 
* France 
5.3 
0.11 
45.8 101.8 
2.5 
7 
* Italy 
3.5 
0.06 
14.2 99.1 
2.0 
8 
* Spain 
2.S 
0.03 
47.6 71.8 
2.8 
9 
« Switzerland 
8.7 
0.25 
70.9 44.8 
2.0 
10 
* Germany 
5.8 
0.17 148.0 22.9 
3.0 
11 
* Denmark 
11.4 
0.43 116.3 11.5 
1.7 
12 
* Holland 
8.7 
0.54 
81.5 11.2 
2.9 
13 
* Finland 
12.0 
0.17 
55.2 
8.7 
2.9 
14 
* Sweden 
11.7 
0.34 
56.0 
8.6 
3.0 
15 
* England 
1.5 
3.42 118.3 
5.7 
1.6 
16 
* Ireland 
0.2 
3.78 127.9 
4.5 
2.1 
17 
* Norway 
9.0 
0.19 
45.1 
3.2 
1.8 
18 
* 
1 
19 
»SET 6,17,18_ 
In statistical operations our data set can now be referred to by a name COUNTRIES by 
inserting (here on line 19) a DATA specification 
19 
»DATA COUNTRIES,6,17,4 
where 6-17 are the lines for the data values and 4 refers to the line telling the names 
of the variables (columns). Thus our data set ready for statistical analysis looks like 
Disp.8 
1 
2 
3 
4 
5 
6 
7 
8 
9 
10 
11 
12 
13 
14 
15 
16 
17 
18 
19 
SURW 76 EDITOR 
(Q1979 S.Hustonen 
(100x100) 
Yearly consumption of certain beverages per inhabitant 
in some European countries in 1974-1978: 
Coffee Tea 
Beer Wine Spirits Constant 
(kg) 
(kg) 
(1) 
(1) 
(1) 
France 
Italy 
Spain 
Switzerland 
Germany 
Denmark 
Holland 
Finland 
Sweden 
England 
Ireland 
Norway 
5.3 
3.5 
2.5 
8.7 
5.8 
11.4 
8.7 
12.0 
11.7 
1.5 
0.2 
9.0 
0.11 
0.06 
0.03 
0.25 
0.17 
0.43 
0.54 
0.17 
0.34 
3.42 
3.78 
0.19 
45.8 101.8 
14.2 99.1 
47.6 
70.9 
148.0 
116.3 
81.5 
55.2 
56.0 
118.3 
127.9 
45.1 
71.8 
44.B 
22.9 
11.5 
11.2 
8.7 
8.6 
5.7 
4.5 
3.2 
2.5 
2.0 
2.8 
2.0 
3.0 
1.7 
2.9 
2.9 
3.0 
1.6 
2.1 
1.8 
»DATA COUNTRIES,6,17,4. 
Observe that we have also added the label 'Constant' above the last column. 
To compute the parameters of a linear regression model a REGRAN operation is entered: 

Statistical Computing with a Text editor 
343 
Disp.9 
IS 
* England 
1.5 
3.42 118.3 
5.7 
1.6 
1 
16 
* Ireland 
0.2 
3.78 127.9 
4.5 
2.1 
1 
17 
» Norway 
9.0 
0.19 
45.1 
3.2 
1.8 
1 
18 
* 
XXXX 
XXXX XXXXX YYYYY 
XXX 
X 
19 
»DATA COUNTRIES,6,17,4 
20 
»REGRAN COUNTRIES, 18,21_ 
21 
* 
REGRAN has three parameters: the name of the data set (COUNTRIES), number of the line 
specifying the model (18) and number of the the first line to be used for the results 
(21). The model is specified (on line 18) by placing Y's to the columns of the dependent 
variable (Wine) and X's to the columns of the independent variables (Coffee, Tea, Beer, 
Spirits and Constant). Furthermore, as an option, it is possible to obtain the residuals 
as a nem column by giving a numerical format as an image (-123.1). 
Activation of the 
REGRAN line 20 gives the results 
Disp.10 
1 
SURV0 75 EDITOR 
(01979 S.Mustonen 
100x100) 
1 
2 
* 
* 
Yearly consumption of certain beverages per inhabitant 
3 
* 
in some European countries in 1974--1978: 
4 
* 
Coffee Tea 
Beer Wine Spirits Constant 
5 
« 
(kg) 
(kg) 
(1) 
(1) 
(1) 
6 
* France 
5.3 
0.11 
45.8 101.8 
2.5 
1 
35.4 
7 
* Italy 
3.5 
0.06 
14.2 99.1 
2.0 
1 
5.7 
8 
* Spain 
2.5 
0.03 
47.6 71.8 
2.8 
1 
-15.3 
9 
* Switzerland 
8.7 
0.25 
70.9 44.8 
2.0 
1 
10.9 
10 
* Germany 
5.8 
0.17 148.0 22.9 
3.0 
1 
-7.3 
11 
* Denmark 
11.4 
0.43 116.3 11.5 
1.7 
1 
12.5 
12 
« Holland 
8.7 
0.54 
81.5 11.2 
2.9 
1 
-6.8 
13 
* Finland 
12.0 
0.17 
55.2 
8.7 
2.9 
1 
0.5 
14 
* Sweden 
11.7 
0.34 
56.0 
8.6 
3.0 
1 
2.9 
15 
» England 
1.5 
3.42 118.3 
5.7 
1.6 
1 
-1.7 
16 
* Ireland 
0.2 
3.78 127.9 
4.5 
2.1 
1 
1.3 
17 
* Norway 
9.0 
0.19 
45.1 
3.2 
1.8 
1 
-38.0 
18 
* 
XXXX 
XXXX XXXXX YYYYY 
XXX 
X 
-12.1 
19 
»DATA COUNTRIES,6,17,4 
20 
»REGRAN COUNTRIES,18, 
21 
»REGRESSION ANALYSIS: REGRESSAND:'Wine' DATA: COUNTRIES' 
22 
* 
TOTAL VARIANCE= 
1401.623 DF = 11 
23 
« 
RESIDUAL VARIANCE 
480.221 DF = 7 Rt2=0 7819 
24 
* VARIABLE 
REGR. C0EFF. STD.DEVIATION 
T 
25 
» Coffee 
-7. 689164 
2 .05534 
-3.741 
26 
* Tea 
-22. 694462 
8.28630 
-2.738 
27 
* Beer 
-0.268177 
0.20304 
-1.320 
28 
* Spirits 
-7. 087441 
13.62378 
-0.520 
29 
* Constant 
139.644319 
37.98259 
3.676 
The great advantage of the editorial approach is that we have all the ingredients (data, 
results and operations) on the same level and immediately available for new modifi-
cations and operations. For example, it is easy to alter the data values or the model. 
Here it seems to be reasonable to simplify the model by dropping the variable 'Spirits'. 
This is done by cancelling the X's for 'Spirits' on the image line 18 and by reactivat-
ing the REGRAN line 20. 

344 
Seppo Mustonen 
Disp.ll 
1 
SURV0 76 EDITOR 
(01979 S.Mustonen 
(100x100) 
15 
* England 
1.5 
3.42 118.3 
5.7 
1.6 
1 
16 
* Ireland 
0.2 
3.78 127.9 
4.5 
2.1 
1 
17 
* Norway 
9.0 
0.19 
45.1 
3.2 
1.8 
1 
18 
* 
XXXX 
XXXX XXXXX YYYYY 
X 
19 
»DATA COUNTRIES,6,17,4 
20 
»REGRAN COUNTRIES, 18,31_ 
21 
»REGRESSION ANALYSIS: REGRESSAND:'Wine' DATA: 'COUNTRIES 
22 
* 
TOTAL VARIANCE= 
1401.623 DF= 11 
23 
* 
RESIDUAL VARIANCE= 
480.221 DF= 7 Rt2=0 .7819 
24 
* VARIABLE 
REGR.C0EFF. STD.DEVIATION 
T 
25 
* Coffee 
-7.689164 
2.05534 
-3.741 
26 
* Tea 
-22.694462 
B.28630 
-2.738 
27 
» Beer 
-0.268177 
0.20304 
-1.320 
28 
* Spirits 
-7.087441 
13.62378 
-0.520 
29 
* Constant 
139.644319 
37.98259 
3.676 
30 
* 
31 
»REGRESSION ANALYSIS: REGRESSAND:'Wine' DATA: 'COUNTRIES 
32 
* 
TOTAL VARIANCE= 
1401.623 DF= 11 
33 
* 
RESIDUAL VARIANCE= 
436.439 DF= 8 Rt2=0 .7735 
34 
* VARIABLE 
RGR.C0EFF. STD.DEVIATION 
T 
35 
* Coffee 
-7.636371 
1.95702 
-3.902 
36 
* Tea 
-21.002946 
7.26597 
-2.890 
37 
» Beer 
-0.289111 
0.18972 
-1.523 
38 
» Constant 
122.855589 
19.09494 
6.433 
39 
* 
-1.7 
0.3 
1.3 -0.4 
-38.0 -33.8 
-12.1 
Observe that in order to preserve the previous results we have altered the first line of 
the results from 21 to 31 and moved the image of the residuals some steps to the right. 
Before taking a printout of the results obtained we may edit them as we like, for in-
stance, by removing technical lines and inserting new text as labels and comments. 
To have a printout on paper the simplest way is to type PRINT 2,39 and activate this 
operation by pressing CONTINUE and we get a list of lines 2-39 (without line numbers and 
other control information) on paper. The edit field can also be saved on disk for subse-
quent use by typing a SAVE <file name) operation on any editor line and by activating 
this line. Similarly any edit file may be called to the edit field by a LOAD <file name) 
operation. EDITOR also provides means for combining edit fields. 
3. NUMERICAL COMPUTATIONS IN THE EDIT FIELD 
When working with statistical data in the edit field it is natural to expect simple 
arithmetic operations to be readily available. 
As a matter of fact, standard calcu-
lations enriched with various mathematical, statistical and user-defined functions can 
easily be carried out in the edit field. 
The arithmetic expressions are written in the edit field in normal fashion and they are 
evaluated by typing a "=" sign after the expression itself and by pressing the acti-
vation key CONTINUE. The value obtained will then be displayed after the "=" sign. 
For instance, to compute the mean of the numbers 12, 17 and 25, we type 

Statistical Computing w i t h a Text Editor 
345 
Disp.lg 
1 
SURVO 76 EDITOR 
(C)1979 S.Hustonen 
(100x100) 
1 
» 
a 
* 
3 
* 
(12+17+25)/3=_ 
4 
* 
and activate by pressing CONTINUE. This leads to the display 
Disp.13 
1 
SURVQ 76 EDITOR 
(C)1979 S.Hustonen 
(100x100) 
1 
* 
2 
* 
3 
* 
(12+17+25)/3=18 
4 
* 
In order to avoid repeating various constants and variables appearing in the expressions 
it is possible to use symbolic notation. 
For instance, the preceding example could be 
written as follows: 
Disp.14 
1 
SURVO 76 EDITOR 
(C)1979 S.Hustonen 
(100x100) 
1 
* 
2 
* 
3 
* 
X=12, Y=17, Z=25 
4 
* arithmetic mean 
(X+Y+Z)/3=_ 
If the expression on line 4 is activated, EDITOR is capable of finding the definitions 
of X,Y and Z and inserting their values in the expression. 
The definitions of variables and constants in the edit field can be nested, i.e. a 
variable may be defined as a function of other variables. 
This property enables the 
construction of general arithmetic computation schemes. 
For instance, to compute values of the standard normal distribution function according 
to a well-known polynomial approximation, the following scheme will be sufficient: 
Disp.15 
1 
SURV0 76 EDITOR 
(C)1979 S.Hustonen 
(100x100) 
12 
* 
13 
« 
For X)_0, the standard normal distribution function F(X) 
14 
* 
is approximated by 
15 
* 
F=l-f*(bl*t+b2*tt2+b3*tt3+b4*tt4+b5*tt5) , 
16 
* 
where 
17 
* 
f=(l/sqr(2*pi))*exp(-Xt2/2) (density function) and 
18 
* 
t=l/(l+r*X) , r=0.2316419 
19 
* 
bl=.31938153 , 
b2=-.356563782 
20 
* 
b3=l.781477937 , 
b4=-l.821255978 
21 
* 
b5=l.330274429 , 
22 
* 
Pi=3.14159265359 . 
23 
* 
24 
* 
To compute F(X) enter X on line 26 and activate F 
25 
* 
26 
* 
X=_ 
F= 
27 
» 

346 
Seppo Mustonen 
The scheme needs no further explanations. The reader immediately sees the essential de-
tails and EDITOR can interpret all the components provided that everything is unambigu-
ously de-fined. 
Thus, inserting X=3 on line 26 and activating F on the same line gives 
the result 
Disp.16 
24 
* To compute F(X) enter X on line 26 and activate F 
25 
* 
26 
* X=3 F=0.9986500327 
27 
* 
Observe that this computation scheme included nested definitions as F depends on t which 
depends on r, etc. 
Some elementary functions like square root and exponential function 
were used, too. 
In addition to the standard functions several statistical functions are readily avail-
able. For example, the distribution function, the density function and the inverse dis-
tribution function of the normal distribution N(u,st2) are N.F(u,st2,x), N.f(u,st2,x) 
and N.G(u,st2,y), respectively. 
Hence the preceding scheme could be replaced simply by 
the function call N.F(0,1,X)=_. 
Since the arguments may be functions, we may generate 
random normal deviates in the edit field as the values of the function N.G(0,l,rnd(l)). 
The SURVO 76 EDITOR computation schemes can be considered simple programs involving 
arithmetic operations in a free form. 
Since the schemes and the results may easily be 
edited by normal editing operations, the results will be printed on paper precisely in 
the way the user likes. 
The possibility to combine computation schemes with other ac-
tivities provided by EDITOR strongly increases the versatility of the system. 
Hore advanced examples of the possibilities offered by the editorial approach are pre-
sented in the following schemes. 
Disp.17 
1 
SURVO 76 EDITOR 
(Q1979 S.Hustonen 
(100x100) 
52 
* 
53 
* The roots of the equation a*it2+b*z+c=0 are computed as follows: 
54 
* 
55 
* Let the roots be zl=xl+i»yl, z2=x2+i*y2 
56 
* and the discriminant=bt2-4*a*c. 
57 
* 
58 
* if discriminant)=0 then 
59 
* 
xl=(-b+sqr(discriminant))/(2*a), yl=0 
60 
* 
x2=(-b-sqr(discriminant))/(2*a), y2=0 
endif 
61 
* 
62 
* if discriminant<0 then 
63 
* 
xl=-b/(2*a), 
yl=sqr(-discriminant)/(2*a) 
64 
* 
x2=xl, 
y2=-yl 
endif 
65 
* 
66 
» 
Let a=l, b=4 and c=13. 
67 
* 
Then 
68 
* 
xl:=-2 
yl:=3 
69 
* 
x2:=-2 
y2:=-3 
70 
* 
discriminant:=-36 
71 
» 

Statistical Computing with a Text Editor 
347 
The scheme in display 17 illustrates the possibility of using conditional definitions 
(if-then-endif structure) and multiple activations. 
If an expression tailed with := is 
activated, all the expressions having the same tail will be activated simultaneously. 
Disp.18 
1 
SURVO 76 EDITOR 
(C)1979 S.Mustonen 
(100x100) 
1 
» 
2 
* 
Generating data according to a given regression model: 
3 
» 
4 
* 
Y=aO+al*Xl+a2»X2+eps, 
5 
* 
where eps=N.G(0,st2,rnd(l)), s=3, 
6 
« 
a0=100, al=10 and a2=l. 
1 
* 
OMR 11,22,10,9_ 
123.12 
Y 
8 
«COMP 
1 11,22,10,9_ 
9 
* 
XXXXXXXX 
XXX 
10 
* 
XI 
X2 
11 
* 1 -0.35449 
68 
12 
* 2 -0.49315 
110 
13 
* 3 
0.48293 
33 
14 
* 4 -0.55747 
86 
15 
« 5 
1.94759 
136 
16 
* 6 
1.21945 
30 
17 
* 7 -0.07194 
94 
18 
* 8 
1.54428 
144 
19 
* 9 
0.07833 
150 
20 
»10 -1.06143 
89 
21 
»11 -0.84473 
130 
22 
»12 -0.52181 
84 
23 
* 
In display 18 we are using a C0MP operation which enables construction of tables of 
functions. When C0MP on line 8 is activated the Y values will be computed for the XI and 
X2 values on lines 11-22. 
The auxiliary lines 9 and 10 indicate that Y is the function 
to be computed. 
EDITOR finds the definition 
of Y on line 4 and further explanations 
(definition of eps and values of s,a0,al and a2) on lines 5 and 6. On the basis of this 
information the Y column is created and displayed according to the format on the line 9: 
Disp.19 
1 
SUR VP 76 EDITOR 
(Q1979 S.Hustonen 
(100x100) 
8 
»COMP ' 11,22,10,9 
9 
* 
XXXXXXXX 
XXX 
123.12 
10 
* 
XI 
X2 
Y 
11 
* 1 -0.35449 
68 
165.65 
12 
* 2 -0.49315 
110 
195.23 
13 
» 3 
0.48293 
33 
137.54 
14 
* 4 -0.55747 
86 
179.11 
15 
* 5 
1.94759 
136 
252.33 
16 
* 6 
1.21945 
30 
141.12 
17 
* 7 -0.07194 
94 
200.35 
18 
* 8 
1.54428 
144 
263.96 
19 
* 9 
0.07833 
150 
252.53 
20 
»10 -1.06143 
89 
172.77 
21 
»11 -0.84473 
130 
218.60 
22 
»12 -0.52181 
84 
177.53 
23 
« 

348 
Seppo Mustonen 
To "check" this result we can easily per-for® a REGRAN operation (after inserting a 
column C of l's as a constant term): 
Disp.20 
24 
25 
26 
2? 
28 
29 
30 
31 
32 
33 
SUR VP 76 EDITOR 
(01979 S.Hustonen 
(100x100) 
Generating data according to a given regression model: 
Y=aO+al*Xl+a2*X2+eps, 
where eps=N.G(0,st2,rnd(l)), s=3, 
a0=100, al=10 and a2=l. 
8 
«COMP 11,22,10,9 
9 
K 
XXXXXXXX 
XXX 
123.12 
10 
» 
XI 
X2 
Y 
C 
11 
* 1 -0.35449 
68 
165.65 
1 
2.76 
12 
* 2 -0.49315 
110 
195.23 
1 
-8.09 
13 
* 3 
0.48293 
33 
137.54 
1 
0.13 
14 
* 4 -0.55747 
86 
179.11 
1 
0.52 
15 
* 5 
1.94759 
136 
252.33 
1 
-4.77 
16 
* 6 
1.21945 
30 
141.12 
1 
-1.66 
17 
* 7 -0.07194 
94 
200.35 
1 
8.23 
18 
* 8 
1.54428 
144 
263.96 
1 
3.44 
19 
* 9 
0.07833 
150 
252.53 
1 
2.69 
20 
»10 -1.06143 
89 
172.77 
1 
-3.08 
21 
*11 -0.84473 
130 
218.60 
1 
-0.72 
22 
»12 -0.52181 
84 
177.53 
1 
0.54 
23 
* 
XXXXXXXX 
XXX 
YYYYYY 
X 
-12.12 
»DATA TEST, 11,22,10 
*REGRAN TEST,23,26_ 
•REGRESSION ANALYSIS: REGRESSAND: 'Y' DATA: 'TEST' 
TOTAL VARIANCE1 
RESIDUAL VARIANCE1 
1821.113 DF= 11 
21.808 DF= 9 Rt2=0.9902 
VARIABLE 
XI 
X2 
C 
REGR.C0EFF. 
11.378313 
1.000256 
98.905259 
STD.DEVIATION 
1.44394 
0.03536 
3.64851 
T 
7.880 
28.281 
27.108 
REFERENCES 
Mustonen S.C1980), Interactive analysis in SURVO 76, Proceedings of Computational 
Statistics (Ed. M.N.Barritt, D.Wishart), Physica-Werlag, Wien, 253-259. 
Mustonen S. (1980), SURVO 76 Editor, a new tool -for interactive statistical computing, 
text and data management, Research report No.19, Dept.of Statistics, 
University of Helsinki. 

