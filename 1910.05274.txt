A Geometric Model of Opinion Polarization
Jan Hązła∗
Yan Jin†
Elchanan Mossel‡
Govind Ramnarayan§
Abstract
We introduce a simple, geometric model of opinion polarization. It is a model of political
persuasion, as well as marketing and advertising, utilizing social values.
It focuses on the
interplay between diﬀerent topics and persuasion eﬀorts. We demonstrate that societal opinion
polarization often arises as an unintended byproduct of inﬂuencers attempting to promote a
product or idea. We discuss a number of mechanisms for the emergence of polarization involving
one or more inﬂuencers, sending messages strategically, heuristically, or randomly.
We also
examine some computational aspects of choosing the most eﬀective means of inﬂuencing agents,
and the eﬀects of those strategic considerations on polarization.
1
Introduction
Opinion polarization is a widely acknowledged social phenomenon, especially in the context of po-
litical opinions [FA08, SH15, IW15], leading to recent concerns over “echo chambers” created by
mass media [Pri13] and social networks [CRF+11, Par11, BMA15, BAB+18, Gar18]. The objec-
tive of this paper is to propose a simple, multi-dimensional geometric model of the dynamics of
polarization where the evolution of correlations between opinions on diﬀerent topics plays a key
role.
Many models have been proposed to explain how polarization arises, and this remains an active
area of research [NSL90, Axe97, Noa98, HK02, MKFB03, BB07, DGL13, DVSC+17, KP18, PY18,
SCP+19]. Our attempt aims at simplicity over complexity. As opposed to a large majority of
previous works, our model does not require social network-based mechanism. Instead, we focus on
inﬂuences of advertising or political campaigns that reach a wide segment of the population.
We develop a high-dimensional variant of biased assimilation [LRL79] and use it as our main
behavioral assumption. The bias assimilation for one topic states that people tend to be receptive
to opinions they agree with, and antagonistic to opinions they disagree with.
The multi-dimensional setting reﬂects the fact that campaigns often touch on many topics.
For example, in the context of American politics, one might wonder why there exists a signiﬁcant
correlation between opinions of individuals on, say, abortion access, gun rights and urgency of
climate change [Pew14]. Our model attempts to illustrate how such correlations between opinions
can arise as a (possibly unintended) eﬀect of advertising exploiting diﬀerent topics and social values.
In mathematical terms, we consider a population of agents with preexisting opinions represented
by vectors in Rd. Each coordinate represents a distinct topic, and the value of the coordinate
∗Email: jan.hazla@epfl.ch. Partially supported by DMS-1737944.
†Email: yjin1@mit.edu. Partially supported by ARO MURI W911NF1910217.
‡Email: elmos@mit.edu. Partially supported by by Simons Investigator in Mathematics award (622132), NSF
DMS award 1737944 and CCF award 1918421, ARO MURI grant W911NF1910217 and by a Vannevar Bush Faculty
Fellowship ONR-N00014-20-1-2826.
§Email: govind@mit.edu. Partially supported by DMS-1737944 and ARO MURI W911NF1910217.
1
arXiv:1910.05274v4  [cs.SI]  25 Aug 2021

reﬂects the agent’s opinion on the topic, which can be positive or negative. As discussed more
fully in Section 1.4, we assume that all opinions lie on the Euclidean unit sphere. This reﬂects
an assumption that each agent has the same “budget of importance” of diﬀerent topics. We then
consider a sequence of interventions aﬀecting the opinions. An intervention is also a unit vector in
Rd, representing the set of opinions expressed in, e.g., an advertising campaign or “news cycle”.
We model the eﬀect of intervention v on an agent’s opinion u in the following way. Supposing
an agent starts with opinion u ∈Rd, after receiving an intervention v it will update the opinion to
the unit vector proportional to
w = u + η · ⟨u, v⟩· v ,
(1)
where η > 0 is a global parameter that controls the inﬂuence of an intervention. Most of our results
do not depend on a choice of η and in our examples we often take η = 1 for the sake of simplicity.
Smaller values of η could model campaigns with limited persuasive power. This and other design
choices are discussed more extensively in Section 1.4.
Intuitively, the agent evaluates the received message in context of its existing opinion, and
assimilates this message weighted by its “agreement” with it. Our model exhibits biased assimilation
in that if the intervening opinion v is positively correlated with an agent’s opinion u, then after
the update the agent opinion moves towards v, and conversely, if v is negatively correlated with u,
then the update moves u away from v and towards the opposite opinion −v.
One way to think of the intervention is as an exposure to persuasion by a political actor, like a
political campaign message. A diﬀerent way, in the context of marketing, is a product advertisement
that exploits values besides the quality of the product. In that context, we can think of one of the
d coordinates of the opinion vector as representing opinion on a product being introduced into the
market and the remaining coordinates as representing preexisting opinions on other (e.g., social or
political) issues. Then, an intervention would be an advertising eﬀort to connect the product with
a certain set of opinions or values [VSL77]. Some examples are corporate advertising campaigns
supporting LGBT rights [Sny15] or gun manufacturers associating their products with patriotism
and conservative values [SVS04]. Another scenario of an intervention is a company (e.g., a bank or
an airline [For18]) announcing its refusal to do business with the gun advocacy group NRA. Such
advertising strategies can have a double eﬀect of convincing potential customers who share relevant
values and antagonizing those who do not.
Our main results show that such interventions, even if intending mainly to increase sales and
without direct intention to polarize, can have a side eﬀect of increasing the extent of polarization
in the society. For example, it might be that, in a population with initial opinions distributed
uniformly, a number of interventions introduces some weak correlations. In our model, these corre-
lations can be proﬁtably exploited by advertisers in subsequent interventions. As a side eﬀect, the
interventions strengthen the correlations and increase polarization.
For example, suppose that after various advertising campaigns, we observe that people who tend
to like item A (say, electric cars) tend to be liberal, and people who like a seemingly unrelated item
B (say, ﬁrearms) tend to be conservative. This may result from the advertisers exploiting some
obvious connections, e.g., between electric cars and responding to climate change, and between
ﬁrearms and respect for the military. Subsequently, future advertising eﬀorts for electric cars may
feature other values associated with liberals in America to appeal to potential consumers: an
advertisement might show a gay couple driving to their wedding in an electric car. Similarly, future
advertisements for ﬁrearms may appeal to conservative values for similar reasons. The end result
can be that the whole society becomes more polarized by the incorporation of political topics into
advertisements.
2

Throughout the paper, we analyze properties of our model in a couple of scenarios. With respect
to the interventions, we consider two scenarios: either there is one entity (an inﬂuencer) trying
to persuade agents to adopt their opinion or there are two competing inﬂuencers pushing diﬀerent
agendas. With respect to the time scale of intervations, we also consider two cases: the inﬂuencer(s)
can apply arbitrarily many interventions, i.e., the asymptotic setting, or they need to maximize
inﬂuence with a limited number of interventions, i.e., the short-term setting. The questions asked
are: (i) What sequence of interventions should be applied to achieve the inﬂuencer’s objective? (ii)
What are the computational resources needed to compute this optimal sequence? (iii) What are
the eﬀects of applying the interventions on the population’s opinion structure? We give partial
answers to those questions. The gist of them is that in most cases, applying desired interventions
increases the polarization of agents.
1.1
Model deﬁnition
The formal deﬁnition of our model is simple. We consider a group of n agents, whose opinions are
represented by d-dimensional unit vectors, where each coordinate corresponds to a topic. We will
look into how those opinions change after receiving a sequence of interventions. Each intervention
is also a unit vector in Rd, representing the opinion contained in a message that the inﬂuencer (e.g.,
an advertiser) broadcast to the agents. Our model features one parameter: η > 0, signifying how
strongly an intervention inﬂuences the opinions.
The interventions v(1), . . . , v(t), . . . divide the process into discrete time steps.
Initially, the
agents start with opinions u(1)
1 , . . . , u(1)
n . Subsequently, applying intervention v(t) updates the opin-
ion of agent i from u(t)
i
to u(t+1)
i
.
After each intervention, the agents update their opinions by moving towards or away from
the intervention vector, depending on whether or not they agree with it (which is determined by
the inner product between the intervention vector v(t) and the opinion vector), and normalizing
suitably. The update rule is given by
u(t+1)
i
=
w(t+1)
i
w(t+1)
i

,
where
w(t+1)
i
= u(t)
i
+ η⟨u(t)
i , v(t)⟩· v(t) .
(2)
We note that, by expanding out the deﬁnition of w(t+1)
i
,
∥w(t+1)
i
∥2 = ⟨w(t+1)
i
, w(t+1)
i
⟩= 1 + (2η + η2)⟨u(t)
i , v(t)⟩2
(3)
In particular, this implies that ∥w(t+1)
i
∥≥1, and consequently that u(t+1)
i
is well-deﬁned. The
norm in (2) and everywhere else throughout is the standard Euclidean norm. Note that applying
v(t) or −v(t) to an opinion u(t)
i
results in the same updated opinion u(t+1)
i
.
1.2
Example
To illustrate our model, let us consider an empirical example with η = 1. Suppose an advertiser
is marketing a new product. The opinion of the population has four dimensions. The population
consists of 500 agents, each with initial opinions u(1)
i
= (ui,1, ui,2, ui,3, 0) ∈R4 subject to u2
i,1 +
u2
i,2 + u2
i,3 = 1. The opinion on the new product is represented by the fourth coordinate, which is
initially set to zero for all agents. These starting opinions are sampled independently at random
from the uniform distribution on the sphere. A typical arrangement of initial opinions is shown
under t = 1 in Figure 1.
3

Suppose the advertiser chooses to repeatedly apply an intervention that couples the product
with the preexisting opinion on the ﬁrst coordinate. More concretely, let the intervention vector be
v = (β, 0, 0, α) ,
where
α = 3
4 , β =
p
1 −α2 .
In that case, an application of the intervention v to an opinion u(1)
i
= (ui,1, ui,2, ui,3, 0) results in
⟨u(1)
i , v⟩= βui,1 and
u(2)
i
=
w(2)
i
∥w(2)
i
∥
,
w(2)
i
=
 (1 + β2)ui,1, ui,2, ui,3, βαui,1

,
∥w(2)
i
∥2 = 1 + 3β2u2
i,1 .
Note that after applying the intervention the ﬁrst and last coordinates have the same sign. In
subsequent time step, the intervention v is applied again to the updated opinions u(2)
i
and so on.
The evolution of opinions over ﬁve consecutive applications of v in this process is illustrated in
Figure 1. The interventions increase the aﬃnity for the product for some agents while antagonizing
others. Furthermore, they have a side eﬀect of polarizing the agents’ opinions also on the ﬁrst three
coordinates. A similar example is included in Appendix B.
1.3
Outline of our results
We analyze the strategy of inﬂuencers in several settings.
In an “asymptotic scenario”, the inﬂuencer wants to apply an inﬁnite sequence of interven-
tions v(1), v(2), . . . , that maximizes how many out of the n agent opinions converge to the target
vector v. As is standard, we say that a sequence of vectors u(1), · · · , u(t), . . . converges to a vector v
if limt→∞||u(t) −v|| = 0. One way to interpret this scenario is that a campaigner wants to establish
a solid base of support for their party platform.
In a “multiple-inﬂuencer scenario”, two inﬂuencers (such as two companies or two parties)
who have diﬀerent objectives apply their two respective interventions on the population in a certain
order. We ask how the opinions change under such competing inﬂuences. This scenario can be
interpreted as two parties campaigning their agendas to the population.
In a “short-term scenario”, the inﬂuencer is advancing a product/subject which is expressed
in the last coordinate of opinion vectors ui,d. The inﬂuencer assumes some ﬁxed threshold 0 < T < 1
and an upper bound K on the number of interventions, and asks, given n opinions u1, . . . , un, how to
choose v(1), · · · , v(K) in order to maximize the number of time-(K + 1) opinions u(K+1)
1
, . . . , u(K+1)
n
with u(K+1)
i,d
> T. One interpretation is that advertisers only have a limited number of opportunities
to publicize their products to consumers, and consumers with u(K+1)
i
> T will decide to buy the
product after the interventions v(1), · · · , v(K) are applied.
We brieﬂy summarize our results for these scenarios. In Section 3 we start by showing that
random interventions lead to a strong form of polarization. More precisely, assuming uniformly
distributed initial opinions, we prove that applying an independent uniformly random intervention
at each time step leads the opinions to form two equally-sized clusters converging to a pair of
(moving) antipodal points.
In Section 4 we consider the asymptotic scenario, where there is one inﬂuencer with a desired
campaign agenda v and unlimited numbers of interventions at its disposal. We ask which sequence
of interventions maximizes the number of opinions that converge to the agenda v. Somewhat sur-
prisingly, we show that such optimal strategy does not necessarily promote the campaign agenda
4

t = 1
t = 2
t = 3
t = 4
t = 5
t = 6
Figure 1: Graphical illustration of the example discussed in Section 1.2. Since we are working
in d = 4, we illustrate the ﬁrst three dimensions as spatial positions and the fourth dimension
with a color scale. Initially the opinions are uniformly distributed on the sphere, with the fourth
dimension equal to 0 (no opinion) everywhere. Consecutive applications of the intervention v =
(
√
7/4, 0, 0, 3/4) in R4 result in polarization both in spatial dimensions and in the color scale.
5

directly at every step. Instead, it ﬁnds a hemisphere containing the largest number of initial opin-
ions, concentrates the opinions in this hemisphere around an arbitrary point, and only in the last
stage nudges them gradually towards the target agenda. We then show that it is computationally
hard to approximate this densest hemisphere (and therefore the optimal strategy) to any constant
factor. Again, strong polarization emerges from our dynamic: there exists a pair of antipodal points
such that all opinions converge to one of them.
In Section 5 we study the short-term scenario where one inﬂuencer is allowed only one interven-
tion. In Section 5.1, we describe a case study with one inﬂuencer and two agents in the population.
We assume that the inﬂuencer wants to increase the correlations of agent opinions with the target
opinion v above a given threshold T > 0. We show consequences of optimal interventions depending
on if the inﬂuencer can achieve this objective for one or both agents. In Section 5.2, we consider a
similar scenario, but with a large number of agents. In that case, it surprisingly turns out that the
problem of ﬁnding optimal intervention in this short-term setting is related to the problem analyzed
in the asymptotic setting. Finding the optimal intervention is equivalent to ﬁnding a spherical cap
containing the largest number of initial opinions.
In Section 6, we study two competing inﬂuencers. At each time step, one of the inﬂuencers
is selected at random to apply its intervention. One might hope that having multiple advertisers
can make the resulting opinions more spread-out, but we prove that this not the case. We show
that, as time goes to inﬁnity, all opinions converge to the convex cone between the two intervention
vectors. Furthermore, we show that the if the correlation between the interventions is high enough,
the strong form of polarization emerges: the opinions of the population concentrate around two
antipodes moving around in the convex cones of the two interventions.
1.4
Design choices
Our goal in this work is to provide a simple, elegant and analyzable model demonstrating how
correlations between diﬀerent topics and natural interventions lead to polarization. That being the
case, there are many societal mechanisms related to polarization that we do not discuss here.
First, in contrast to majority of existing literature, we present a mechanism independent from
opinion changes induced by interactions between individuals. Second, we do not address aspects
such as replacement of the population or unequal exposure and eﬀects of the interventions. We
do not consider any external inﬂuences on the population in addition to the interventions. Our
model does not align with (limited) theoretical and empirical research suggesting that in certain
settings exposure to conﬂicting views can decrease polarization [PT06, MS10, GMGM17, GGPT17]
or works that question the overall extent of polarization in the society [FAP05, BG08].
In general we assume that the inﬂuencers have full knowledge of the agent opinions. This is not
a realistic assumption and in fact our results in Section 4 show that in some settings the optimal
inﬂuencer strategy is infeasible to compute even with the full knowledge of opinions. On the other
hand, we observe polarization also in settings where the inﬂuencers apply interventions that are
agnostic to the opinions, for example with purely random interventions in Section 3 or competing
inﬂuencers in Section 6.
We sometimes discuss the uniform distribution of initial opinions on Rd. We do this as the
uniform distribution may be viewed as the most diverse and establishing polarization starting from
the uniform distribution hints that we are modelic a generic phenomenon. Most of our results do
not make assumptions about the initial distribution.
We assume that any group of topics can be combined into an intervention with the eﬀect
given by (1). A more plausible model might feature some “internal” (content) correlations between
topics in addition to “external” (social) correlations arising out of the agents’ opinion structure. For
6

example, topics may have innate connections, causing inherent correlations between corresponding
opinions (e.g., being positive on renewable energy and recycling). Furthermore, there are certain
topics (e.g., undesirability of murder) on which nearly all members of the population share the same
inclination. As a matter of fact, it is common for marketing strategies to exploit unobjectionable
social values (see, e.g., [VSL77]). However, we presume that under suitable circumstances (e.g., due
to inherent correlations we just mentioned) the “polarizing” topics might present a more appealing
alternative for a campaign. Our model concerns such a case, where the “unifying” topics might
be excluded from the analysis. We note that other works have also suggested that focusing on
polarizing topics may be appealing for campaigns [PY18].
Below we discuss a couple of speciﬁc design choices in more detail:
Euclidean unit ball
We make an assumption that all opinions and interventions lie on the
Euclidean unit ball. Note that the interpretation of this representation is somewhat ambiguous.
The magnitude of an opinion on a given subject ui,k might signify the strength of the opinion, the
conﬁdence of the agent or relative importance of the subject to the agent. While these are diﬀerent
measures, there are psychological reasons to expect that, e.g., “issue interest” and “extremity of
opinion” are correlated [LBS00, Bal07, BB07]. Especially taking the magnitudes as signifying the
relative importance, we believe that the assumption that this “budget of importance” for any given
agent is ﬁxed is quite natural. That being said, we are also motivated by simplicity and tractability.
Multiple ways of relaxing or modifying this assumption are possible. While we do not study
these variants in this paper, we now discuss them very brieﬂy.
At least empirically, our basic
ﬁndings about ubiquity of polarization seem to remain valid for those modiﬁed models.
Perhaps the simplest modiﬁcation is to use the same update rule as in (2) with a diﬀerent norm
(eg., ℓ1 or ℓ∞norm). Such variant would also assume that opinions and interventions lie on the
unit sphere of the respective norm. Our experiments suggest that, qualitatively, both ℓ1 and ℓ∞
variants behave similarly to the Euclidean norm.
In another direction, rather than having all opinions on the unit sphere, ﬁxed, but diﬀerent
norms zi can be speciﬁed for diﬀerent agents. Then, the update rule (2) could be modiﬁed as
w(t+1)
i
= u(t)
i
+ η ·
*
u(t)
i
zi
, v(t)
+
· v(t) ,
with normalization preserving ∥u(t+1)
i
∥= zi. As long as the values of zi are bounded from below
and above, the resulting dynamic is essentially identical and our results carry over to this more
general setup.
Yet another possibility is to consider opinion unit vectors u ∈Rd+1 with ud+1 ≥0 and interpret
the ﬁrst d coordinates as opinions and the last coordinate as “unused budget”. Therefore, large
values of ud+1 signify generally uncertain opinions and small values of ud+1 correspond to strong
opinions. There are multiple possible rules for interventions, where an intervention can have d or
d + 1 coordinates, and with diﬀerent treatments of the last coordinate. We leave the details for
another time.
Eﬀects of applying v and −v
In our model, an eﬀect of an intervention v is exactly the same
as for the opposite intervention −v.
This might look like a cynical assumption about human
nature, but arguably it is not entirely inaccurate. For example, experiments on social media show
that not only exposure to similar ideas (the “echo chamber” eﬀect), but also exposure to opposing
opinions causes beliefs to become more polarized [BAB+18]. This is even more apparent if a broader
7

notion of an intervention is considered. Using a recent example, social media platforms banning or
disassociating from certain statements can have a polarizing eﬀect [BBC20]. Furthermore, in our
model this eﬀect occurs only if all the components of an opinion are negated.
A related, more general objection is that direct persuasion is not possible in our model. If an
agent has an opinion u with ⟨u, v⟩< 0, directly applying v only makes the situation worse. Instead,
an eﬀective inﬂuencer needs to apply interventions utilizing diﬀerent subjects to gradually move u
through a sequence of intermediate positions towards v. Our answer is that we posit that a lot of,
if not all, persuasion actually works that way: to convince that “x is good”, one argues that “x is
good, since it is quite like y, which we both already agree is good”.
Notions of polarization
While the notion of polarization is clear when discussing one topic, it
is not straightforward to interpret in higher dimensions. Let S ⊆Rd be a set of n opinions. Writing
u = (u1, . . . , ud) for u ∈S, a natural measure of polarization of S on a single topic i is
ρi(S) =
1
|S|2 max
T⊂S
X
u∈T,u′∈S\T
(ui −u′
i)2,
and we may generalize it to higher dimensions by measuring the polarization as:
ρ(S) =
1
|S|2 max
T⊂S
X
u∈T,u′∈S\T
∥u −u′∥2.
It is clear from the deﬁnition that
max
i
ρi(S) ≤ρ(S) ≤
X
i
ρi(S).
If we consider an example set S1 with n/2 opinions at u and n/2 opinions at −u, then clearly
ρ(S1) = P
i ρi(S1), but in any other example, the upper bound will not be tight. For example, if
S2 is the set of the 2d vertices of a hypercube, i.e., S2 = 1/
√
d · {−1, 1}d, then ρi(S2) = 1/d for
all i, but ρ(S2) converges to 1/2 as n →∞. This corresponds to the fact that while the society is
completely polarized on each topic, two random individuals will agree on about half of the topics.
In Section 2 we refer to such a situation as exhibiting issue radicalization, but no issue alignment.
Ultimately, in many of our results we do not worry about these issues, since we observe a strong
form of polarization, where all opinions converge to two antipodal points.
1.5
Other variants
Other than discussed above, there are many possible variants that can lead to interesting future
work. These include:
• “Targeting”, where the inﬂuencer can select subgroups of the population and apply interven-
tions groupwise.
• Models where the strength of an intervention η varies across agents and/or time steps.
• Perturbing preferences with noise after each step.
• Replacement of the population, e.g., introducing new agents with “fresh” opinions or removing
agents that stayed in the population for a long time or who already “bought” the product, i.e.,
exceeded the threshold ui,d > T. For example, this could correspond to ”one-time” purchase
product like a house or a fridge, or situations where the customer’s opinion is more diﬃcult
to change as time passes.
8

• Models where the initial opinions are not observable or partially observable.
• Expanding the model by adding peer eﬀects and social network structure and exploring the
resulting dynamics of polarization and opinion formation. This can be done in diﬀerent ways
and we expect that polarization will feature in many of them. For example, [GKT21] show
polarization for random interventions in what they term the “party model”.
• Strategic competing inﬂuencers: in the studied scenarios with competing inﬂuencers, we
assume that they apply ﬁxed interventions. One can ask: suppose the inﬂuencers have their
own target opinions, what is each campaigner’s optimal sequence of messages in face of the
other campaigner? Then, resulting equilibrium of opinion formation could be analyzed. This
can be modeled as a dynamic game where the game state is the opinion conﬁguration and
optimal strategies may be derived using sequential planning and control.
2
Related works
As mentioned, there is a multitude of modeling and empirical works studying opinion polarization in
diﬀerent contexts [NSL90, Axe97, BG98, Noa98, HK02, MKFB03, MS05, BB07, DGL13, DVSC+17,
KP18, SCP+19, PY18, BAB+18]. Broadly speaking, previous works have proposed various possible
sources for polarization, including peer interactions, bias in individuals’ perceptions, and global
information outlets.
There is an extensive line of models of opinion exchange on networks with peer interactions,
where individuals encounter neighboring individuals’ opinions and update their own opinions based
on, e.g., pre-deﬁned friend/hostile relations [SPJ+16], or the similarity and relative strength of
opinions [MS10], etc. This branch of work often attributes polarization to homophily of one’s social
network [DGL13] that is induced by the self-selective nature of social relations and segregation of
like-minded people [WMKL15] and exacerbated by the echo chamber eﬀect of social media [Par11].
A parallel proposed mechanism points to psychological biases in individuals’ opinion formation
processes. One example is biased assimilation [LRL79, DGL13, BB07, BAB+18]: the tendency to
reinforce one’s original opinions regardless if other encountered opinions align with them or not.
For example, [BAB+18] observed that even when social media users are assigned to follow accounts
that share opposing opinions, they still tend to hold their old political opinions and often to a more
extreme degree. On the modeling side, [DGL13] showed that DeGroot opinion dynamics with the
biased assimilation property on a homophilous network may lead to polarization.
Existing works have also proposed models where polarization occurs even when information is
shared globally [Zal92, MS05]. For example, [MS05] propose a model where competition for read-
ership between global information outlets causes news to become polarized in a single-dimensional
setting. Another example is [Zal92], a classical work on the formation of mass opinion. It theo-
rizes that each individual has political dispositions formed in their own life experience, education
and previous encounters that intermediate between the message they encounter and the political
statement they make. Therefore, hearing the same political message can cause diﬀerent thinking
processes and changes in political preferences in diﬀerent individuals.
It is noteworthy that the majority of previous work focuses on polarization on a single topic
dimension. Two exceptions are [BB07], which studies biased assimilation with opinions on multiple
topics and [BG08] that observed non-trivial correlations between people’s attitudes on diﬀerent
issues. We note that [BB07] uses a diﬀerent updating rule to observe dynamics that diﬀer from our
work: in their simulations, polarization on one issue typically does not result in polarization on oth-
ers. There is also a class of models [Axe97, Noa98, MKFB03] that concern multi-dimensional opin-
9

ions where an opinion on a given topic takes one of ﬁnitely many values (e.g., + or −). These models
do not seem to have a geometric structure of opinion space similar to ours and usually focus on
formation of discrete groups in the society rather than total polarization. Another model [PPTF17]
uses a geometric (aﬃne) rule of updating multi-dimensional opinions. Unlike us, they seem to be
modeling pre-existing, “intrinsic” correlations between topics rather than the emergence of new
ones and they are concerned mostly with convergence and stability of their dynamics.
A related paper [PY18] contains a geometric model of opinion (preference) structures. Both
this and our model propose mechanisms through which information outlets acting for their own
beneﬁt can lead to increased disagreement in the society. The key diﬀerence to our model is that
their population’s preferences are static and do not update, but the outlets are free to choose
what information to oﬀer to their customers.
By contrast, in our model, the inﬂuencers have
pre-determined ideologies and compete to align agents’ opinions with their own. In other words,
[PY18] focuses on modeling of competitive information acquisition, and our paper on modeling the
inﬂuence of marketing on the public opinion.
Our model suggests that under the conditions of biased assimilation, opinion manipulation by
one or several global information outlets can unintentionally lead to a strong form of polarization
in multi-dimensional opinion space. Not only do people polarize on individual issues, but also their
opinions on previously unrelated issues become correlated.
This form of polarization is known
as issue alignment [BG08] in political science and sociology literature. Issue alignment refers to
an opinion structure where the population’s opinions on multiple (relatively independent) issues
correlate. It is related to issue radicalization, where the opinions polarize for each issue separately.
Compared to issue radicalization, issue alignment is theorized to pose more constraints on the
opinions an individual can take, resulting in polarized and clustered mass opinions even when
the public opinions are not extreme in any single topic, and presenting more obstacles for social
integration and political stability [BG08].
In light of this, one way to view our model is as a
mathematical mechanism by which this strong form of polarization can arise and worsen due to
companies’, politicians’, and the media’s natural attempts to gain support from the public.
On the more technical side, we note that our update equation bears similarity to Kuramoto
model [JMB04] for synchronization of oscillators on a network in the control literature. In this
model, each oscillator i is associated with the point θi on the two-dimensional sphere, and i updates
its point continuously as a function of its neighbors’ points θj:
˙θi = ωi + K
N sin(θj −θi),
where K is the coupling strength and N is the number of nodes in the network. In two dimensions,
our model can be compared to Kuramoto model with ωi = 0 on a star graph, with the inﬂuencers
at the center of the star connected to the entire population, where the inﬂuencers’ opinions do not
change and the update strength is qualitatively similar to sin((θv −θu)/2) (see (15)). However, we
note a crucial diﬀerence: in the Kuramoto dynamic, θi always moves towards θj, i.e. nodes always
move towards synchronization, but in our dynamic, opinions θi are allowed to move further away
from θj when the angle between their opinions are obtuse. In addition, the central node in our
model can be strategic in choosing its positions, while the central node in Kuramoto model follows
the synchronization dynamics of the system. We think this property provides a better model for
opinion interactions.
Subsequent work
A work by Gaitonde, Kleinberg and Tardos [GKT21], announced after we
posted the preprint of this paper, proposes a framework that generalizes our random interventions
10

scenario from Section 3. They prove several interesting results, including a strong form of polariza-
tion under random interventions in some related models. They also shed more light on the scenario
of dueling inﬂuencers from Section 6, showing that in case the dueling interventions are orthogonal,
the resulting dynamics exhibits a weaker kind of polarization.
3
Asymptotic scenario: random interventions polarize opinions
In this section, we analyze the long-term behavior of our model in a simple random setting. We
assume that, for given dimension d and parameter η, at the initial time t = 1 we are given n
opinion vectors u(1)
1 , . . . , u(1)
n . Subsequently, we sample a sequence of interventions v(1), v(2), . . . ,
each v(t) iid from the uniform distribution on the unit sphere Sd−1. At time t we apply the random
intervention v(t) to every opinion vector u(t)
i , obtaining a new opinion u(t+1)
i
.
We want to show that the opinions {u(t)
i } almost surely polarize as time t goes to inﬁnity. We
need to be careful about deﬁning the notion of polarization: since the interventions change at every
time step, the opinions cannot converge to a ﬁxed vector. Instead, we show that for every pair of
opinions the angle between them converges either to 0 or to π. More formally:
Theorem 3.1. Consider the model of iid interventions described above for some d ≥2, η > 0 and
initial opinions u(1)
1 , . . . , u(1)
n . For any 1 ≤i < j ≤n and t →∞,
Pr
h
∥u(t)
i
−u(t)
j ∥→0 ∨∥u(t)
i
+ u(t)
j ∥→0
i
= 1 .
This leads to a corollary which follows by applying the union bound (with probability 0 in each
term) for each pair of opinions u(t)
i , u(t)
j :
Corollary 3.1. For any d ≥2, η > 0, initial opinions u(1)
1 , . . . , u(1)
n
and a sequence of uniform iid
interventions, almost surely, there exists J ⊆{1, . . . , n} such that the diameter of the set
n
(−1)1[i∈J] · u(t)
i
: i ∈{1, . . . , n}
o
converges to zero.
Remark 3.1. Consider initial opinions of n agents that are independently sampled from a dis-
tribution Γ that is symmetric around the origin, in the sense that Γ(−A) = Γ(A) for every set
A ⊆Sd−1.
Then, with high probability, the opinions converge to two polarized clusters of size
roughly n/2. Indeed, consider sampling n independent vectors u1, . . . , un from Γ and n independent
signs σ1, . . . , σn ∈{±1}. Then σ1u1, . . . , σnun are independent samples from Γ. Moreover, if the
sizes of the two clusters for u1, . . . , un are r and n−r then the size of each cluster for σ1u1, . . . , σnun
is distributed according to Bin(r, 1/2)+Bin(n−r, 1/2) = Bin(n, 1/2) (this is due to the observation
that ui and −ui converge to the opposite clusters).
Remark 3.2. For simplicity we do not elaborate on this later, but we note that, both empirically
and theoretically, the convergence in our results is quite fast. This concerns Theorem 3.1, as well
as the results presented in the subsequent sections.
We now proceed to the proof of Theorem 3.1:
11

3.1
Notation and main ingredients
Before we proceed with explaining the proof, let us make a general observation that we will use
frequently. Let d ≥2 and η > 0 and let f : Sd−1 × Sd−1 →Sd−1 be the function mapping an
opinion u and an intervention v to an updated opinion f(u, v), according to (2) and (3). It should
be clear that this function is invariant under isometries: namely, for any real unitary transformation
A : Sd−1 →Sd−1 we have
f(Au, Av) = Af(u, v) .
(4)
In our proofs we will be often using (4) to choose a convenient coordinate system.
Let us turn to Theorem 3.1. Again, let d ≥2 and η > 0. Without loss of generality we will
consider only two starting opinions called u(1)
1
and u(1)
2 . To prove Theorem 3.1, we need to show
that almost surely one of the vectors u(t)
1 −u(t)
2
and u(t)
1 + u(t)
2
vanishes.
We proceed by using martingale convergence. Speciﬁcally, let
αt := arccos⟨u(t)
1 , u(t)
2 ⟩.
That is, 0 ≤αt ≤π is the primary angle between u(t)
1
and u(t)
2 .
The proof rests on two claims. First, αt is a martingale:
Claim 3.1. E[αt+1 | αt] = αt.
Second, we show a property which has been called “variance in the middle” [BGN+18]:
Claim 3.2. For every ε > 0, there exists δ > 0 such that,
ε ≤αt ≤π/2 =⇒Pr

αt+1 < αt −δ | αt

> δ ,
(5)
and, symmetrically,
π/2 ≤αt ≤π −ε =⇒Pr

αt+1 > αt + δ | αt

> δ .
(6)
These two claims imply Theorem 3.1 by standard tools from the theory of martingales (eg., [Wil91]):
Claims 3.1 and 3.2 imply Theorem 3.1. As a consequence of applying Claim 3.2 ⌈π/δ⌉times, we
obtain that for every ε > 0 there exist k0 ∈N and η < 1 such that
ε ≤αt ≤π −ε =⇒Pr [∀1 ≤k ≤k0 : ε ≤αt+k ≤π −ε | αt] ≤η .
Subsequently, it follows that for any ﬁxed ε > 0 and T ∈N,
Pr [∀t ≥T : ε ≤αt ≤π −ε] = 0 .
(7)
By Claim 3.1, the sequence of random variables αt is a bounded martingale and therefore almost
surely converges. Accordingly, let α∗:= limt→∞αt. We now argue that Pr[0 < α∗< π] = 0. To
that end,
Pr[0 < α∗< π] ≤
∞
X
s=1
Pr
1
s < α∗< π −1
s

≤
∞
X
s=1
Pr

∃T : ∀t ≥T : 1
2s < αt < π −1
2s

≤
∞
X
s=1
∞
X
T=1
Pr

∀t ≥T : 1
2s < αt < π −1
2s

= 0 ,
where we applied (7) in the last line. Hence, almost surely, either α∗= 0, which is equivalent to
∥u(t)
1 −u(t)
2 ∥→0 or α∗= π, equivalent to ∥u(t)
1 + u(t)
2 ∥→0.
12

In the subsequent sections we proceed with proving Claims 3.1 and 3.2. In Section 3.2 we prove
Claim 3.1 for d = 2. In Section 3.3 we show the same claim for d ≥3 by a reduction to the case
d = 2. Finally, in Section 3.4 we use a continuity argument to prove Claim 3.2.
In the following proofs, we ﬁx d, η, time t and the opinions of two agents at that time. For
simplicity, we will denote the relevant vectors as u := u(t)
1 , u′ := u(t)
2
and v := v(t).
3.2
Proof of Claim 3.1 for d = 2
v
f(u′, v)u′
u
f(u, v)
αt
β
γ′
γ
u′
v
v∗
u
ε
Figure 2: On the left an illustration of the vectors and angles in the proof of Claim 3.1. On the
right an illustration for the proof of Claim 3.2.
It follows from (4) that we can assume wlog that u = (1, 0) and u′ = (cos αt, sin αt) (recall that
by deﬁnition 0 ≤αt ≤π holds). Let us write the random intervention vector as v = (cos β, sin β),
where the distribution of β is uniform in [0, 2π). We will also write (cf. Figure 2 for an overview)
f(u, v) = (cos γ, sin γ) ,
f(u′, v) = (cos(αt + γ′), sin(αt + γ′)) ,
γ, γ′ ∈[−π, π) .
Note that γ is a function of the intervention angle β, and it should be clear that γ(β) = −γ(−β).
Accordingly, the distribution of γ is symmetric around zero and in particular E γ = 0 (where the
expectation is over β). Applying (4), it also follows E γ′ = 0.
Let bα := αt + γ′ −γ. Since bα is equal to the directed angle from f(u, v) to f(u′, v), one might
think that we have just established E[αt+1 | αt] = αt. However, recall that we deﬁned αt+1 to be
the value of the (primary) undirected angle between f(u, v) and f(u′, v). In particular, it holds
that 0 ≤αt+1 ≤π, but we cannot assume that about bα. On the other hand, it is clear that if
0 ≤bα ≤π, then indeed αt+1 = bα. Therefore, in the following we will show that 0 ≤bα ≤π always
holds, which implies E[αt+1 | αt] = E[bα | αt] = αt + E γ′ −E γ = αt.
To that end, we start with showing a weaker bound −π < bα < 2π. To see this, we ﬁrst establish
that −π/2 < γ, γ′ < π/2. The argument for γ is as follows: if ⟨u, v⟩≥0, then f(u, v) is a convex
combination of u and v. Therefore, an intervention cannot move f(u, v) away from u by an angle
of more than π/2. If v ̸= u, then also f(u, v) ̸= v, so in fact the angle must be strictly less, that is
−π/2 < γ < π/2. If ⟨u, v⟩< 0, then −π/2 < γ < π/2 follows from the same argument applied to
−v (since the eﬀect of both interventions is the same). Finally, −π/2 < γ′ < π/2 holds by (4) and
the same proof. Since we know 0 ≤αt ≤π, we obtain −π < bα < 2π.
Since we know −π < bα < 2π, the inequality 0 ≤bα ≤π is equivalent to sin bα ≥0. Geometrically,
this property means that the ordered pair of vectors (u, v) has the same orientation as the pair
(f(u, v), f(u′, v)). To avoid case analysis, we prove this claim by a calculation:
13

Claim 3.3. sin bα ≥0.
Proof. We defer the proof to Appendix A.
3.3
Proof of Claim 3.1 for d ≥3
In this case we will write the random intervention vector as v = v∥+ v⊥where v∥is projection of v
onto the span of u and u′. In particular, v∥and v⊥are orthogonal. We will now prove a stronger
claim E

αt+1 | αt, ∥v∥∥

= αt.
Accordingly, condition on the value ∥v∥∥= R. Observe that, by symmetry, vector v∥is dis-
tributed uniformly in the two-dimensional space span{u, u′} among vectors of norm R. In other
words, we can write v∥= RV , where V is a uniform two-dimensional unit length vector.
Denote the non-normalized vectors after intervention as
bu := u + η⟨u, v∥⟩(v∥+ v⊥) ,
bu′ := u′ + η⟨u′, v∥⟩(v∥+ v⊥) .
Let c := 2η + η2. We proceed with calculations:
⟨bu, bu′⟩= ⟨u, u′⟩+ c⟨u, v∥⟩⟨u′, v∥⟩= ⟨u, u′⟩+ cR2⟨u, V ⟩⟨u′, V ⟩,
∥bu∥2 = 1 + c⟨u, v∥⟩2 = 1 + cR2⟨u, V ⟩,
∥bu′∥2 = 1 + c⟨u′, v∥⟩2 = 1 + cR2⟨u′, V ⟩.
Note that all these formulas are valid also for d = 2, with the only diﬀerence that R = 1 holds
deterministically in that case.
Since c(η) = 2η + η2 is a bijection on R>0, there exists bη > 0 such that cR2 = 2bη + bη2.
Accordingly, for any d ≥3 and η > 0, the joint distribution of ⟨bu, bu′⟩, ∥bu∥and ∥bu′∥conditioned
on αt = arccos(⟨u, u′⟩) and ∥v∥∥= R is the same as their joint distribution for d = 2 and bη,
conditioned on the same value of αt.
Since αt+1 = arccos

⟨bu,bu′⟩
∥bu∥∥bu′∥

, the same correspondence holds for the distribution of αt+1
conditioned on αt and ∥v∥∥= R. Therefore, E

αt+1 | αt, ∥v∥∥= R

= αt follows by Claim 3.1 for
d = 2, which we already proved.
3.4
Proof of Claim 3.2
Again we use (4) to choose a coordinate system and assume wlog that u = (1, 0, . . . , 0) and u′ =
(cos αt, sin αt, 0, . . . , 0). Our objective is to show that, with probability at least δ, we will have
αt+1 −αt > δ (in case αt ≤π/2) or αt+1 −αt < −δ (in case αt ≥π/2). To start with, we will show
that by symmetry we need to consider only the ﬁrst case αt ≤π/2.
Note that the intervention function f exhibits a symmetry f(−u, v) = −f(u, v). Furthermore,
we also have arccos⟨u, u′⟩= π −arccos⟨u, −u′⟩. Consequently,
αt+1 −αt = arccos⟨f(u, v), f(u′, v)⟩−arccos⟨u, u′⟩
= π −arccos⟨f(u, v), f(−u′, v)⟩−(π −arccos⟨u, −u′⟩)
= −
 arccos⟨f(u, v), f(−u′, v)⟩−arccos⟨u, −u′⟩

.
As a result, indeed it is enough that we prove (5) and then (6) follows by replacing u′ with −u′.
Consider vector v∗:= (cos ε, sin ε, 0, . . . , 0) (see Figure 2). We will now show that if ε ≤αt ≤π/2
and the intervention v is suﬃciently close to v∗, then v decreases the angle between u and u′. To
that end, let us use a metric on Sd−1 given by
D(u, v) := arccos⟨u, v⟩.
14

Note that this metric is strongly equivalent to the standard Euclidean metric restricted to Sd−1.
We can now use the triangle inequality to write
αt+1 = D(f(u, v), f(u′, v))
≤D(f(u, v), f(u, v∗)) + D(f(u, v∗), v∗) + D(v∗, f(u′, v∗)) + D(f(u′, v∗), f(u′, v)) .
(8)
Let us bound the terms in (8) one by one.
First, since, by (2), f(u, v∗) is a strict convex combination of u and v∗(note that in our
coordinate system neither u nor v∗depends on αt), we have
D(f(u, v∗), v∗) = d(ε) < D(u, v∗) = ε .
Similarly,
D(v∗, f(u′, v∗)) ≤D(v∗, u′) = αt −ε .
Second, since f is continuous, D(v, v∗) < δ′ for small enough δ′ > 0 implies that both D(f(u, v), f(u, v∗))
and D(f(u′, v∗), f(u′, v)) are as small as needed (for example, less than (ε −d(ε))/4).
All in all, we have that for some δ′ = δ′(ε) > 0,
D(v, v∗) < δ′ =⇒αt+1 < ε −d(ε)
4
+ d(ε) + (αt −ε) + ε −d(ε)
4
= αt −ε −d(ε)
2
.
However, clearly, the event D(v, v∗) < δ′ has some positive probability δ′′.
Therefore, taking
δ := min(δ′′/2, (ε −d(ε))/2), we have
Pr [αt+1 < αt −δ | αt] > δ ,
as claimed in (5).
4
Asymptotic scenario: ﬁnding densest hemisphere
In this section we study the asymptotic scenario with one inﬂuencer who wishes to propagate
a campaign agenda v∗∈Rd.
We assume that the inﬂuencer can use an unlimited number of
interventions and its objective is to make the opinions of as many agents as possible to converge
to v∗. More speciﬁcally, in this section we denote the initial opinions of agents at time t = 1 by
u1, . . . , un. Given these preexisting opinions of n agents, we want to ﬁnd a sequence of interventions,
v(1), v(2), v(3) . . . that maximizes the number of agents whose opinions converge to v∗.
The thrust of our results is that ﬁnding a good strategy for the inﬂuencer is computationally
hard. However, both the optimal strategy and some natural heuristics result in the polarization of
agents.
4.1
Equivalence of optimal strategy to ﬁnding densest hemisphere
We ﬁrst argue that the problem of ﬁnding an optimal strategy is equivalent to identifying an open
hemisphere that contains the maximum number of agents. An (open) hemisphere is an intersection
of the unit sphere with a homogeneous open halfspace of the form

x ∈Rd : ⟨x, a⟩> 0
	
for some
a ∈Rd.
15

Theorem 4.1. For any v∗, there exists a strategy to make at least k agents converge to v∗if and
only if there exists an open hemisphere containing at least k of the opinions u1, . . . , un.
A surprising aspect of Theorem 4.1 is that the maximum number of agents that can be persuaded
does not depend on the target vector v∗. As we argue in Remark 4.1, this is somewhat plausible
in the long-term setting with unlimited number of interventions. We also note that the number of
interventions required to bring the opinions up to a given level of closeness to v∗does depend on
v∗.
Proof of Theorem 4.1. First, we prove that the hemisphere condition is suﬃcient for the existence of
a strategy to make the agents’ opinions converge (Claim 4.1). Then we prove the trickier direction:
that the hemisphere condition is also necessary for the existence of such a strategy (Claim 4.5).
Claim 4.1. If opinions u1, . . . , uk are contained in an open hemisphere, then there is a sequence
of interventions making all of u1, . . . , uk converge to v∗.
Proof. By deﬁnition of open hemisphere, there is a vector a ∈Rd such that ⟨ui, a⟩> 0 for every
agent i = 1, . . . , k. By (2), it is clear that repeated application of a makes all the points converge
to a as time t →∞.
After all the points are clustered close enough to a, by a similar argument they can be “moved
around” together towards another arbitrary point v∗. For example, if ⟨v∗, a⟩> 0, the intervention
v∗can be applied repeatedly.
If ⟨v∗, a⟩≤0, one can proceed in two stages, ﬁrst applying an
intervention proportional to (v∗+ a)/2, and then applying v∗.
Remark 4.1. As a possible interpretation of the mechanism in Claim 4.1, it is not unheard of
in campaigns on political issues to use an analogous strategy. First, build a consensus around a
(presumably compromise) opinion. Then, “nudge” it little by little towards another direction.
In an extreme case one can imagine this mechanism even ﬂipping the opinions of two polarized
clusters. One example of this could be the reversal of the opinions on certain issues of 20th century
Republican and Democratic parties in the US (this particular phenomenon can be found in many
texts, e.g. [KW18]).
To prove the other direction of Theorem 4.1, we will rely on the notions of conical combination
and convex cone. A conical combination of points u1, . . . , un ∈Rd is any point of the form Pn
i=1 αiui
where αi ≥0 for every i.
A convex cone is a subset of Rd that is closed under ﬁnite conical
combinations of its elements. Given a ﬁnite set of points S ⊆Rd, the convex cone generated by S
is the smallest convex cone that contains S.
Claim 4.2. Suppose that for a given sequence of interventions, the opinions u1, . . . , un converge
to the same point v∗. Then, for any unit vector un+1 that lies in the convex cone of u1, . . . , un, we
have that un+1 also converges to v∗.
Proof. It suﬃces to prove that if at time t an opinion u(t)
n+1 lies in the convex cone of other opinions
u(t)
1 , . . . , u(t)
n , then after applying one intervention v(t) the new opinion u(t+1)
n+1 lies in the convex cone
of u(t+1)
1
, . . . , u(t+1)
n
. Then the claim follows by induction.
16

To prove this, we can simply write out u(t+1)
n+1 , using the relation u(t)
n+1 = Pn
i=1 λiu(t)
i
(where we
use the notation u ∝v to mean that u = c · v for some constant c > 0):
u(t+1)
n+1 ∝u(t)
n+1 + η
D
u(t)
n+1, v(t)E
· v(t)
=
n
X
i=1
λiu(t)
i
+ η ·
n
X
i=1
λi
D
u(t)
i , v(t)E
· v(t)
=
n
X
i=1
λi

u(t)
i
+ η ·
D
u(t)
i , v(t)E
· v(t)
=
n
X
i=1
λi · ciu(t+1)
i
(9)
where the constants in (9) are ci :=
u(t)
i
+ η · ⟨u(t)
i , v(t)⟩· v(t). Speciﬁcally, they are all non-
negative.
Claim 4.3. Suppose there are two opinions u1, u2 that are antipodal, i.e., u1 = −u2. Then these
two opinions will remain antipodal in future time steps. In particular, they will never converge to
a single point.
Proof. This follows directly from (2), noting that, for any intervention v, we have u1+η·⟨u1, v⟩·v =
−(u2 + η · ⟨u2, v⟩· v).
We will also use the following consequence of the separating hyperplane theorem:
Claim 4.4. A collection of unit vectors a1, . . . , an cannot be placed in an open hemisphere if and
only if the zero vector lies in the convex hull of a1, . . . , an.
Now we are ready to establish the reverse implication in Theorem 4.1.
Claim 4.5. Suppose that we start with agent opinions u1, . . . , un and that there is no hemisphere
that contains M of those opinions. Then, there is no strategy that makes M of the opinions converge
to the same point.
Proof. Assume towards contradiction that there exists a strategy that makes M opinions converge
to the same point, and assume wlog that they are u1, . . . , uM. By assumption, we know that there
is no hemisphere that contains all of u1, . . . , uM, hence, by Claim 4.4, there is a convex combination
of u1, . . . , uM that equals 0. Therefore, there is also a conical combination of u1, . . . , uM−1 that
equals −uM, where wlog we assume that the coeﬃcient on uM is initially nonzero. By Claim 4.2,
we conclude that if u1, . . . , uM−1 converge to the same point, then so does −uM. But that means
that −uM and uM converge to the same point, which is a contradiction by Claim 4.3.
That concludes the proof of Theorem 4.1.
Remark 4.2. One consequence of Theorem 4.1 is that if the agent opinions are initially distributed
uniformly on the unit sphere, and if the number of agents n is large compared to the dimension
d, an optimal strategy converging as many opinions as possible to v∗results, with high probability,
in dividing the population into two groups of roughly equal size, where the opinions inside each
group converge to one of two antipodal limit opinions (i.e., v∗and −v∗). Furthermore, this optimal
strategy, which, as discussed below, might not be easy to implement, will not perform signiﬁcantly
better than a very simple strategy of ﬁxing a random intervention and applying it repeatedly. Of
course the simple strategy will also polarize the agents into two approximately equally large groups.
17

4.2
Computational equivalence to learning halfspaces
Theorem 4.1 implies that an optimal strategy for the inﬂuencer is to compute the open hemisphere
that is the densest, i.e., it contains the most opinions, and then apply the procedure from Claim 4.1
to converge the opinions from this hemisphere to v∗. In this section we study the computational
complexity of this problem.
While diﬀerent approaches are possible, we focus on hardness of
approximation and worst-case complexity.
Deﬁnition 4.1 (Densest hemisphere). The input to the densest hemisphere problem consists of
parameters n and d and a set of n unit vectors D = {u1, . . . , un} with ui ∈Sd−1. The objective is
to ﬁnd vector a ∈Sd−1 maximizing the number of points from D that belong to the open halfspace

x ∈Rd : ⟨x, a⟩> 0
	
.
We analyze the computational complexity of the densest hemisphere problem in terms of the
number of vectors n, regardless of dimension d. In particular, the computationally hard instances
that exist as we will show in Theorem 4.2 have high dimension, without any guarantees beyond
d ≤n (which can always be assumed wlog). On the other hand, the algorithms from Theorem 4.3
run in time polynomial in n uniformly for all values d ≤n.
In contrast, the case of ﬁnding densest hemisphere in ﬁxed dimension d can be solved eﬃciently.
For example, an optimal solution can be found by considering O(nd) halfspaces deﬁned by d-tuples
of input vectors. We omit further details.
Our main result in this section relies on equivalence of the densest hemisphere problem and the
problem of learning noisy halfspaces. Applying a work by Guruswami and Raghavendra [GR09] we
will show that it is computationally diﬃcult to even approximate the densest hemisphere up to any
non-trivial constant factor:
Theorem 4.2. Unless P=NP, for any ε > 0, there is no polynomial time algorithm Aε that
distinguishes between instances of densest hemisphere problem such that, letting D := {u1, . . . , un}:
• Either there exists a hemisphere H such that |D ∩H|/n > 1 −ε.
• Or for every hemisphere H we have |D ∩H|/n < 1/2 + ε.
Consequently, unless P=NP, for any ε > 0 there is no polynomial time algorithm Aε that, given an
instance D that has a hemisphere with density more than 1 −ε, always outputs a hemisphere with
density more than 1/2 + ε.
In other words, even if guaranteed the existence of an extremely dense hemisphere, no poly-
nomial time algorithm can do signiﬁcantly better than choosing an arbitrary hyperplane and out-
putting the one of its two hemispheres that contains the larger number of points. At the same
time, [BDS00] (relying on earlier work [BDES02]) shows that there exists an algorithm that ﬁnds
a dense hemisphere provided that this hemisphere is stable in the sense that it remains dense even
after a small perturbation of its separating hyperplane:
Theorem 4.3 ([BDS00]). For every µ > 0, there exists a polynomial time algorithm Aµ, that, given
an instance D = {u1, . . . , un} of the densest hemisphere problem, provides the following guarantee:
Let a ∈Sd−1 be the vector that maximizes the size of intersection |D∩Ha,µ| for halfspace Ha,µ =
{x : ⟨x, a⟩> µ}. Then, the algorithm Aµ outputs a hemisphere corresponding to a homogeneous
halfspace Ha′ = {x : ⟨x, a′⟩> 0} such that |D ∩Ha′| ≥|D ∩Ha,µ|.
18

We emphasize that the only inputs to the algorithms are n, d and the set of vectors D, and that
the complexity is measured as a function of n. For example, the algorithm Aµ runs in polynomial
time for every µ > 0, but the running time is not uniformly polynomial in 1/µ.
In the remainder of this section we elaborate on how to obtain Theorem 4.2 from known results.
To that end, we start with deﬁning the related problem of ﬁnding maximum agreement halfspace.
Deﬁnition 4.2 (Maximum Agreement Halfspace). In the problem of maximum agreement halfs-
pace, the inputs are parameters n and d, and a labeled set of points D = {(x1, y1), . . . , (xn, yn)} ∈
Rd × {±1}. The objective is to ﬁnd a halfspace H = {x : ⟨x, a⟩> c} for some a ∈Rd and c ∈R
which maximizes the agreement
A(D, H) =
Pn
i=1 1 [yi · xi ∈H]
n
.
There is a strong hardness of approximation result for maximum halfspace agreement [GR09]
(see also [FGKP06, BB06, BDEL03, AK98] for related work):
Theorem 4.4 ([GR09]). Unless P=NP, for any ε > 0, there is no polynomial time algorithm Aε
that distinguishes the following cases of instances of maximum agreement halfspace problem:
• There exists a halfspace H such that A(D, H) > 1 −ε.
• For every halfspace H we have A(D, H) < 1/2 + ε.
As in Theorem 4.2, the hard instances are not guaranteed to have any dimension bounds beyond
trivial d ≤n.
As pointed out in [BDS00], there exists a reduction from the maximum agreement halfspace
problem to the densest hemisphere problem that preserves the quality of solutions.
Since this
reduction is only brieﬂy sketched in [BDS00], we describe it below.
The reduction proceeds as follows: Given a labeled set D = {(x1, y1), . . . , (xn, yn)} ∈Rd×{±1},
we map it to D′ = {x′
1, . . . , x′
n} ∈Rd+1 using the formula
x′
i =
1
p
1 + ∥xi∥2 · (yixi, 1) .
In other words, we proceed in three steps: ﬁrst, we negate each point that came with negative label
yi = −1. Then, we add a new coordinate and set its value to 1 for every point xi. Finally, we
normalize each resulting point so that it lies on the unit sphere in Sd.
This is a so-called “strict reduction”, which is expressed in the following claim:
Claim 4.6. The solutions (halfspaces) for an instance D of Maximum Agreement Halfspace are
in one-to-one correspondence with solutions (hemispheres) for the reduced instance D′ of Densest
Hemisphere. Furthermore, for a corresponding pair of solutions (H, H′) the agreement A(D, H) is
equal to the density |D′ ∩H′|/n.
Proof. It is more convenient to think of solutions for D′ as homogeneous, open halfspaces H′ =
{x ∈Rd+1 : ⟨x, a⟩> 0}.
With that in mind, we map a solution to the maximum agreement halfspace problem H =
{x ∈Rd : ⟨x, a⟩> c} to a solution to the densest hemisphere problem H′ = {(x, xd+1) ∈Rd+1 :
⟨(x, xd+1), (a, −c))⟩> 0}. Clearly, this is a one-to-one mapping between open halfspaces in Rd and
homogeneous open halfspaces in Rd+1.
Furthermore, it is easy to verify that yi · xi ∈H if and only if x′
i ∈H′ and therefore A(D, H) =
|D′ ∩H′|/n.
19

Theorem 4.2 follows from Theorem 4.4 and Claim 4.6 by standard (and straightforward) argu-
ments from complexity theory.
5
Short-term scenario: polarization as externality
The analysis of the asymptotic setting with unlimited interventions tells us what is feasible and
what is not. A fundamentally diﬀerent question is how to persuade as many as possible with a
limited number of interventions. This is motivated by bounded resources or time that usually allow
only limited placements of campaigns and advertisements. Furthermore, arguably only the initial
interventions can be considered eﬀective: in the long run the opinions might shift due to external
factors and become more unpredictable and harder to control. Therefore, in this section we discuss
strategies where the inﬂuencer has only one intervention at its disposal, and its goal is to get as
many agents as possible to exceed certain “threshold of agreement” with its preferred opinion.
Throughout this section, we ﬁx η = 1 in Equation 1, so an opinion u is updated to be proportional
to w = u + ⟨u, v⟩· v.
Both scenarios we discuss in this section describe a situation where a “new” product or idea
is introduced. Therefore, we assume that the agents have some preexisting opinions in Rd−1 and
that they are neutral as to the new idea, with the d-th coordinate set to zero for every agent. Our
results indicate signiﬁcant potential for polarization in such a situation. This is in spite of the fact
that the inﬂuencer might only care about persuading a number of agents towards the new subject,
without intention to polarize.
Since we are dealing with scenarios with only one intervention, we use the following notational
convention: an initial opinion of agent i is denoted ui and the opinion after intervention is denoted
eui.
5.1
One intervention, two agents: polarization costs
We consider a simple example that features only two agents and one inﬂuencer who is allowed one
intervention. We imagine a new product, such that the agents are initially agnostic about it, i.e.,
ui,d = 0 for i = 1, 2. Given an intervention v, we are interested in two issues: First, what will
be new opinions of agents about the product eui,d? Second, assuming that the initial correlation
between opinions is c = ⟨u1, u2⟩, what will be the new correlation ec = ⟨eu1, eu2⟩? We think of the
correlation as a measure of agreement between the agents and therefore interpret diﬀerences in
correlation as changes in the extent of polarization.
In order to answer these questions, we introduce notions of two- and one-agent interventions
corresponding to two natural strategies:
Deﬁnition 5.1. The two-agent intervention is an intervention that maximizes min(eu1,d, eu2,d). The
one-agent intervention maximizes max(eu1,d, eu2,d).
The motivation for this deﬁnition is as follows. Assume that there exists a threshold T > 0
such that agent i is going to make a positive decision (e.g., buy the product or vote a certain way)
if its coordinate eui,d exceeds T. Then, if the inﬂuencer cares only about inducing agents to make
the decision, it has two natural choices for the intervention. One option is the case where it is
possible to induce two decisions, i.e., achieve eu1,d, eu2,d > T. By continuity considerations, it is not
diﬃcult to see that an intervention that achieves this can be assumed to maximize min(eu1,d, eu2,d)
with eu1,d = eu2,d (such intervention is also optimal if the inﬂuencer bets on convincing both agents
without knowing T). The other case is to appeal only to one of the agents, disregarding the second
agent and concentrating only on achieving, say, eu1,d > T.
20

Let c = ⟨u1, u2⟩be the initial correlation between opinions and let ctwo and cone be the corre-
lations after applying, respectively, the two- and one-agent interventions. Our main result in this
section is:
Proposition 5.1. Let ρ := ctwo −cone be a value that we call the polarization cost. Then, we
always have ρ ≥0 with exact values given as
ctwo = 1 −
√
2(1 −c)
√3c + 5 ,
cone =
c
√
2
√
c2 + 1
.
(10)
The values of ρ, ctwo and cone as functions of c are illustrated in Figure 3. Proposition 5.1 states
that the one-agent intervention always results in smaller correlation than the two-agent intervention.
Note that we made a modeling assumption that the inﬂuencer will always choose an intervention
as opposed to doing nothing. This is consistent with a scenario where the inﬂuencer’s objective is
to increase the opinions above the threshold T. In that case doing nothing is certain to give no
gain to the inﬂuencer.
The main conclusion of this theorem is consistent with our other results. In the setting we
consider, in the absence of any external mitigation, the self-interested inﬂuencer without direct
intention to polarize might be incentivized to choose the intervention that increases polarization.
If polarization is regarded as undesirable, the polarization cost can be thought of as the externality
imposed on the society.
Looking at Figures 3 and 4, this eﬀect seems most pronounced for initial correlation around
c ≈−0.5, where the one-agent intervention increases polarization, the polarization cost is large and
the range of thresholds T for which the inﬂuencer proﬁts from the one-agent strategy is relatively
large. This suggests that a situation where the society is already somewhat polarized is particularly
vulnerable to spiraling out of control. It also suggests that situations where the level of commitment
required for the decision (i.e., the threshold T) is large increase the risk of polarization.
We also note that this overall picture is complicated by the case of positive initial correlation
c > 0. In that case both two- and one-agent interventions actually increase the correlation between
the agents, even though the two-agent intervention does so to a larger extent. The analysis leading
to the proof of Proposition 5.1 is contained in Appendix C.
5.2
One intervention, many agents: ﬁnding the densest spherical cap
A more general version of the problem of persuading with limited number of interventions features
n agents with opinions u1, . . . , un ∈Rd. The inﬂuencer is given a threshold 0 ≤T < 1 and can
apply one intervention v with the objective of maximizing the number of agents such that eui,d > T.
As before, we assume that intially ui,d = 0 and that T can be interpreted as a threshold above
which a consumer decides to buy the newly advertised product, or more generally take a desired
action, such as voting, donating, etc.
Interestingly, we show that this problem is equivalent to a generalization of the densest hemi-
sphere problem from the long-term scenario discussed in Section 4. More precisely, it is equivalent
to ﬁnding a densest spherical cap of a given radius (that depends on the threshold T) in d −1
dimensions.
We give the technical statement in the proposition below. We make an assumption 0 ≤T < 1/3,
since 1/3 is the maximum value that can be achieved in the d-th coordinate by a single intervention,
cf. Figure 4. In order to state Proposition 5.2, we slightly abuse notation and write vectors u ∈Rd
as u = (u∗, ud) for u∗∈Rd−1, ud ∈R.
21

Figure 3: Illustration of the polarization cost
as a function of the initial correlation c. The
dashed line is the initial correlation included
as a reference point. The red and blue lines
are correlations after applying two- and one-
agent interventions respectively. The green
line shows the polarization cost ctwo −cone.
Figure 4: The after-intervention opinions of
both agents eui,d as functions of initial corre-
lation c. The red line represents the opinion
of either agent after applying the two-agent
intervention. The blue line is the opinion of
the second agent after the one-agent inter-
vention. For reference, the dashed line (1/3)
shows the opinion of the ﬁrst agent in the
one-agent intervention (which does not de-
pend on c).
The grey area represents the
range of thresholds T where it is preferable
for the inﬂuencer to apply the one-agent in-
tervention.
Proposition 5.2. In the setting above, let
c :=
2T
1 −3T 2 ,
z :=
p√
1 + 3c2 −1
√
3c
,
β := arccos(z) .
Then, the number of agents with eui,d > T is maximized by applying an intervention
v := (cos β · v∗, sin β)
(11)
for a unit vector v∗∈Rd−1 that maximizes the number of agents satisfying
⟨u∗
i , v∗⟩> c .
The proof of Proposition 5.2 is contained in Appendix D. Note that the solution to this short-
term problem for T going to zero approaches the densest hemisphere solution to the long-term
problem discussed in Section 4.
22

6
Asymptotic eﬀects of two dueling inﬂuencers: two randomized
interventions polarize
Finally, we analyze a scenario where there are two inﬂuencers with diﬀering agendas, represented
by diﬀerent1 intervention vectors v and v′. We consider the randomized setup, where at each time
step, one of the inﬂuencers is randomly chosen to apply their intervention. We demonstrate that
this setting also results, in most cases and in a certain sense, in the polarization of agents.
Recall that a convex cone of two vectors v and v′ is the set {αv + βv′ : α, β ≥0}. A precise
statement that we prove is:
Theorem 6.1. Let ⟨v, v′⟩> 0 and let a starting opinion u(1) be such that ⟨u(1), v⟩̸= 0 or ⟨u(1), v′⟩̸=
0. Then, as t goes to inﬁnity and almost surely, either the Euclidean distance between u(t) and the
convex cone generated by v and v′ or between u(t) and the convex cone generated by −v and −v′
goes to 0.
In order to justify the assumptions of Theorem 6.1, note that if an agent starts with an opinion
u(1) such that
⟨u(1), v⟩= ⟨u(1), v′⟩= 0 ,
(12)
applying v or v′ never changes their opinion. In Theorem 6.1 we show that if (12) does not hold
and, additionally, ⟨v, v′⟩> 0, (if ⟨v, v′⟩< 0 we can exchange v′ with −v′ without changing the
eﬀects of any interventions), the opinion vector with probability 1 ends up either converging to the
convex cone generated by v and v′ or the convex cone generated by −v and −v′. In particular,
since vectors u for which (12) holds form a set of measure 0, if n initial opinions are sampled iid
from an absolutely continuous distribution, almost surely all opinions converge to the convex cones
(which are themselves sets of measure 0 for d > 2).
Furthermore, this notion of polarization is strengthened if the correlation between the two
interventions is large. As in Theorem 3.1, the best we can hope for is that for each pair of opinions
either the distance between u(t)
1
and u(t)
2
or between u(t)
1
and −u(t)
2
converges to 0. Letting V :=
span{v, v′} and W := V ⊥and writing any vector u as a sum of its respective projections u =
uV + uW , we show:
Theorem 6.2. Suppose that ⟨v, v′⟩> 1/√2 + η and let u(1)
1 , u(1)
2
be such that (u(1)
1 )V ̸= 0, (u(1)
2 )V ̸=
0. Then, almost surely, either ∥u(t)
1 −u(t)
2 ∥converges to 0, or ∥u(t)
1 + u(t)
2 ∥converges to 0.
In other words, the stronger notion of convergence, same as in Section 3 with uniformly drawn
random interventions, reappears in case the correlation between two interventions v and v′ is larger
than 1/√2 + η. In particular, we have strong convergence for any η > 0 and ⟨v, v′⟩≥
√
2/2 ≈0.71,
and for η = 1 for ⟨v, v′⟩>
√
3/3 ≈0.58. Our experiments suggest that this convergence occurs also
for other non-zero values of the correlation ⟨v, v′⟩, but we do not prove it here.
Also note that same in spirit as Remark 3.1, the usual argument from symmetry shows that if the
initial opinions are independent samples from a symmetric distribution, then with high probability
the opinions divide into two clusters of roughly equal size.
The case when v and v′ are orthogonal is diﬀerent. As we mentioned, if ⟨v, v′⟩> 0, i.e., the
angle between v and v′ is less than π/2, then all opinions converge to the two “narrow” convex
cones, respectively between v and v′ and between −v and −v′ — namely, the pairs of vectors among
1We also assume that v ̸= −v′, as otherwise the intervention eﬀects are the same in our model.
23

v, v′, −v, and −v′ between which there are acute angles. Similarly, if ⟨v, v′⟩< 0, then the opinions
converge to two cones between v and −v′ and between −v and v′. In case ⟨v, v′⟩= 0 the four
convex cones form right angles, so such a result is not possible.
However, we can still show that an initial opinion u(1) converges to the same quadrant in which
it starts with respect to v and v′. Namely, for all t, we have that sgn
 
u(t), v

= sgn
 
u(1), v

and sgn
 
u(t), v′
= sgn
 
u(1), v′
, and furthermore the distance between u(t) and the subspace
V goes to 0 with t:
Proposition 6.1. Suppose that ⟨v, v′⟩= 0 and let an initial opinion u(1) be such that ⟨u(1), v⟩̸= 0
and ⟨u(1), v′⟩̸= 0. Then, almost surely, the following facts hold:
1. ∥u(t)
W ∥→0 as t →∞.
2. For all t, sgn
 
u(t), v

= sgn
 
u(1), v

and sgn
 
u(t), v′
= sgn
 
u(1), v′
.
Fascinatingly, Gaitonde, Kleinberg and Tardos [GKT21] showed subsequently to our initial
preprint that strong polarization does not occur for orthogonal interventions. Speciﬁcally, they
proved that two opinions in Sd−1 with random interventions chosen iid from the standard basis
{e1, . . . , ed} do not polarize in the sense of u(t)
1 −u(t)
2
or u(t)
1 −u(t)
2
vanishing, but they do exhibit a
weaker form of polarization. We refer to their paper for more details.
In order to prove Theorem 6.1, we ﬁrst show that the distance between u(t) and V almost surely
goes to 0 as t →∞, by showing that the norm of the projection of u(t) onto W converges to 0.
Then, we demonstrate that the convex cone spanned by v and v′ is absorbing: when the projection
of u(T) onto V falls in the cone, then the projections of u(t) for t ≥T always stay in the cone as
well.
Finally, we show that almost surely the projection of u(t) onto V eventually enters either the
cone spanned by v and v′, or the cone spanned by −v and −v′. More concretely, we show that at
any time t, there is a sequence of T interventions that lands the projection of u(t+T) in one of the
cones, for some T that is independent of t. Since this sequence occurs with probability 2−T , which
is independent of t, the opinion almost surely eventually enters one of the cones.
6.1
Proofs of Theorem 6.1 and Proposition 6.1
We start with the fact the opinions converge to the subspace V spanned by the two intervention
vectors. Recall that V = span{v, v′} and that W = V ⊥. In the following we will write ⟨v, v′⟩= cos θ
for 0 < θ ≤π/2.
Proposition 6.2. Let ⟨v, v′⟩≥0 and take an opinion vector u such that ∥uV ∥= c ≥0. Further-
more, let eu be the vector resulting from randomly intervening on u with either v or v′. Then:
1. ∥euW ∥2 ≤∥uW ∥2.
2. With probability at least 1/2, ∥euW ∥2 ≤∥uW ∥2 · (1 −ξ), where
ξ = min
1
2, (η + η2/2) · c2θ2
16

.
Proof. Recall from (2)–(3) that if v ∈{v, v′} is the intervention vector, then
eu = k(u + η ⟨u, v⟩· v)
24

where k =
q
1
1+(2η+η2)·⟨u,v⟩2 is the normalizing constant. Observe that when we project onto W,
the component in the direction of v vanishes, so we have that
euW = k · uW ,
and the ﬁrst claim easily follows since k ≤1.
To establish the second point, we need to show that with probability 1/2 we have k2 < 1 or,
equivalently, ⟨u, v⟩2 = ⟨uV , v⟩2 > 0. Since θ ̸= 0, the projected vector uV cannot be orthogonal
both to v and v′ (cf. Figure 5). More precisely, for at least one of v ∈{v, v′} the primary angle
between uV and v (or −v) must be at most π/2 −θ/2 and consequently
|⟨uV , v⟩| ≥∥uV ∥· | cos(π/2 −θ/2)| ≥c · θ/4 ,
resulting in
k2 =
1
1 + (2η + η2) · ⟨uV , v⟩2 ≤max
1
2, 1 −(η + η2/2) · c2θ2
16

.
v
v′
uV
euV
θ
Figure 5: Projection onto the subspace V = span{v, v′}.
Next, we show that the convex cone of vectors v and v′ is absorbing:
Proposition 6.3. Let ⟨v, v′⟩≥0 and take u to be an opinion vector and eu to be a vector resulting
from intervening on u with either v or v′. If uV is a conical combination of v and v′, then also euV
is such a conical combination.
Proof. Assume wlog that the vector applied is v and let k be the same constant as in the proof of
Proposition 6.2. Then,
eu/k = u + η · ⟨u, v⟩· v = uV + η · ⟨uV , v⟩· v + uW .
Therefore, euV can be written as a nonnegative linear combination of uV and v, where we use the
fact that ⟨uV , v⟩is nonnegative, which follows since uV is a conical combination of v and v′, and
⟨v, v′⟩≥0.
Next, we prove that when ⟨v, v′⟩> 0, the opinion u(t) not only approaches subspace V , but also
a speciﬁc area of V , namely, either cone(v, v′) or cone(−v, −v′).
Proposition 6.4. Let ⟨v, v′⟩> 0 and consider a vector u(t) such that ∥u(t)
V ∥≥c > 0. Then, there
exists T := T(c, θ, η) such that with probability at least 2−T , vector u(t+T)
V
will either be a conical
combination of v and v′ or a conical combination of −v and −v′.
25

Proof. First, for any vector u(t) such that ∥u(t)
V ∥≥c > 0, at least one of v, v′, −v, −v′ has positive
inner product with u(t) (and u(t)
V ) which can be lower bounded by a function of c and θ (see Figure 5).
Take such a vector and call it v. By the argument from Proposition 6.2, applying v repeatedly will
bring u(t+T) arbitrarily close to it. More precisely, for every ε > 0, there exists T1 = T1(c, θ, η, ε)
such that ∥u(t+T1)
V
−v∥< ε and ∥u(t+T1) −v∥< ε both hold.
Furthermore, since ⟨v, v′⟩> 0, there exists ε > 0 such that if ∥u(t) −v∥< ε, then applying the
other intervention vector (v or v′) once guarantees that u(t+1)
V
enters the convex cone between v
and v′ or, respectively, between −v and −v′. In particular, if u(t)
V
already is in the convex cone,
then applying either intervention will keep it inside by Proposition 6.3. On the other hand, if u(t)
V
is not yet in the cone, but at the distance at most ε to v, then applying the other intervention will
bring it inside the cone (see Figure 5).
Therefore, there exists a sequence of T(c, θ, η) = T1 + 1 interventions that make u(t+T)
V
enter
cone(v, v′) or cone(−v, −v′). Clearly, this sequence occurs with probability 2−T .
We are now ready to prove Theorem 6.1.
Proof of Theorem 6.1. Let ∥u(1)
V ∥= c > 0. Proposition 6.2 tells us that the squared norm of the
projection u(t)
W onto subspace W = V ⊥never increases, and with probability 1/2 decreases by the
multiplicative factor 1 −ξ(c, η, θ) < 1. By induction (note that ξ increases with c), u(t)
W converges
to 0, and consequently ∥u(t) −u(t)
V ∥converges to 0, almost surely.
In order to show that convergence to one of the two convex cones occurs, we apply Proposi-
tion 6.4. Since at any time step t, there exists a sequence of T choices that puts u(t+T)
V
in one of
the convex cones, and since T depends only on the starting parameters c, θ, and η, we get that
u(t)
V
almost surely eventually enters one of the cones. By Proposition 6.3 and induction, once u(t)
V
enters a convex cone, it never leaves.
Proposition 6.1 follows as a corollary of Propositions 6.2 and 6.3:
Proof of Proposition 6.1. The ﬁrst statement is an inductive application of Proposition 6.2, exactly
the same as in the proof of Theorem 6.1.
The second statement follows from noting that out of four orthogonal pairs of vectors {v, v′},
{v, −v′}, {−v, v′}, or {−v, −v′}, there is exactly one such that u(1)
V
is a (strict) conical combination
of this pair (by assuming ⟨u(1), v⟩̸= 0 and ⟨u(1), v′⟩̸= 0 we avoid ambiguity in case u(1)
V
is parallel
to v or v′). By the same argument as in Proposition 6.3 and by induction, if the initial projection
u(1)
V
is strictly inside one of the convex cones, the projection u(t)
V remains strictly inside forever.
6.2
Proof of Theorem 6.2
Consider the subspace V = span{v, v′} with some coordinate system (cf. Figure 5) imposed on it.
As is standard, a unit vector u ∈V can be represented in this system by its angle α(u) ∈[0, 2π) as
measured counterclockwise from the positive x-axis.
Given a unit vector v ∈V , let fv : [0, 2π) →[0, 2π) be the function with the following meaning:
given a unit vector u ∈V with angle α = α(u), the value fv(α) = α(eu) represents the angle of
vector eu resulting from applying intervention v to vector u. Note that α(v) is a ﬁxed point of fv.
Also, by Proposition 6.3, both functions fv and fv′ map the interval corresponding to cone(v, v′)
to itself.
The main part of our argument is the following lemma, which we prove last:
26

Lemma 6.1. If ⟨v, v′⟩= cos θ > 1/√2 + η, then functions fv and fv′ restricted to the convex
cone of v and v′ are contractions, i.e., there exists k = k(θ, η) < 1 such that for all vectors
u, u′ ∈cone(v, v′), letting α := α(u), β := α(u′), v ∈{v, v′}, we have
|fv(β) −fv(α)| ≤k · |β −α| ,
(13)
where the distances |fv(β)−fv∗(α)| and |β −α| are in the metric induced by S1, i.e., “modulo 2π”.
Proof of Theorem 6.2. Lemma 6.1 implies that the angle distance between two opinions u(t)
1 , u(t)
2
∈
V starting in the convex cone deterministically converges to 0 as t goes to inﬁnity. Of course, this
is equivalent to their Euclidean distance ∥u(t)
1 −u(t)
2 ∥converging to 0. We now make a continuity
argument to show that such convergence almost surely occurs also for general u(t)
1 , u(t)
2
∈Sd−1. To
this end, we let gv, gv′ : Sd−1 →[0, 2π) as natural extensions of fv, fv′: the value gv(u) denotes the
angle of the projection euV of the new opinion onto V , after applying v on opinion u (cf. Figure 5).
Note that the value gv(u) depends only on the angle α(uV ) and the orthogonal projection length
∥uW ∥:
gv(u) = gv
 α(uV ), ∥uW ∥

.
In this parametrization, for u ∈V we have fv(α(u)) = gv(u) = gv(α(u), 0).
By Theorem 6.1, for any starting opinions u(1)
1
and u(1)
2
having non-zero projections onto V ,
almost surely there exists a t such that (u(t)
1 )V and (u(t)
2 )V end up inside (possibly diﬀerent) convex
cones. We consider the case of u(t)
1
and u(t)
2
both in cone(v, v′), other three cases being analogous.
Furthermore, almost surely, ∥(u(t)
1 )W ∥and ∥(u(t)
2 )W ∥converge to 0. Hence, it is enough that we
show that almost surely |α((u(t)
1 )V ) −α((u(t)
2 )V )| (in S1 distance) converges to zero.
To this end, let δ > 0. By uniform continuity of gv, we know that for small enough value of r,
we have
|gv(α, r) −gv(α, 0)| < 1 −k
4
· δ
for every α ∈[0, 2π), where k is the Lipschitz constant from (13). Therefore, almost surely, for t
large enough, for u(t)
1
and u(t)
2
parameterized as u(t)
1
= (α1, r1) and u(t)
2
= (α2, r2) we have
|gv(α1, r1) −gv(α2, r2)| ≤|gv(α1, r1) −gv(α1, 0)| + |gv(α1, 0) −gv(α2, 0)| + |gv(α2, 0) −gv(α2, r2)|
≤1 −k
4
· δ + k · |α1 −α2| + 1 −k
4
· δ ≤

k + 1 −k
2

· max(|α1 −α2|, δ) .
Since k + (1 −k)/2 < 1, and applying the same argument to fv′, we conclude by induction that the
distance |α1(t)−α2(t)| must decrease and stay below δ in a ﬁnite number of steps. Since δ > 0 was
arbitrary, it must be that |α1(t) −α2(t)| converges to 0, concluding the proof of Theorem 6.2.
It remains to prove Lemma 6.1:
Proof. Proof of Lemma 6.1. Recall that we assumed a two-dimensional coordinate system on V .
Let f := f(1,0), i.e., f corresponds to the intervention along the x-axis in this coordinate system.
Clearly, functions fv and fv′ are cyclic shifts of f modulo 2π. More precisely, we have
fv(α) = α(v) + f
 α −α(v)

,
(14)
where arithmetic in (14) is modulo 2π. Furthermore, f is symmetric around the intervention vector,
i.e., f(α) = 2π −f(2π −α) for 0 < α ≤π. Hence, to prove that fv and fv′ restricted to cone(v, v′)
27

Figure 6: The graph of the “pull function” α −f(α) in case η = 1.
are contractions, it is enough that we show that f restricted to the interval [0, θ] is a contraction
(recall that we assumed cos2(θ) > 1/(2 + η)).
To that end, we use (2) to calculate the formula for f for 0 ≤α ≤π/2 as
f(α) = arccos
 
(1 + η) cos α
p
1 + (2η + η2) cos2 α
!
.
(15)
More computation using elementary calculus (we omit the details) establishes that, additionally,
for every 0 ≤α < β ≤π/2:
1. f(α) ≤α. In other words, applying the intervention brings vector u closer to the intervention
vector.
2. f(α) < f(β), i.e., applying the intervention does not change relative ordering of vectors wrt
the intervention vector.
3. If β ≤θ∗:= arccos
q
1
2+η

, then 0 ≤α −f(α) < β −f(β), i.e., in absolute terms, the
“pull” on a vector is stronger the further away it is from the intervention vector (until the
correlation reaches the threshold 1/√2 + η, cf. Figure 6).
The preceding items taken together imply that for every 0 ≤α < β ≤θ∗we have 0 < f(β) −
f(α) < β −α. To conclude that f is a contraction, we observe that f and its derivative f′ are
continuous on the interval [0, θ∗]. If there exist sequences (αk) and (βk) in [0, θ] for θ < θ∗such that
|f(αk) −f(βk)|/|βk −αk| converges to 1, then, by compactness, there exist convergent sequences
αk →α∗and βk →β∗such that |f(αk) −f(βk)|/|βk −αk| →1. Then,
1. Either α∗̸= β∗and by continuity we get f(β∗) −f(α∗) = β∗−α∗, contradicting the third
property above.
28

2. Or α∗= β∗, which by continuity of f′ implies f′(α∗) = 1 for some 0 ≤α∗< θ∗.
But
that would imply that the derivative of α −f(α), i.e., 1 −f′(α), vanishes at α∗< θ∗, again
contradicting the third property above (see also Figure 6).
A
Proof of Claim 3.3
Let us embed our underlying space R2 in R3 by setting the last coordinate to zero. Letting ×
denote the cross product, we have
u × u′ = (0, 0, sin αt) ,
f(u, v) × f(u′, v) = (0, 0, sin bα) .
Since the case αt ∈{0, π} is easily handled by noticing that bα = αt, we can assume that 0 < αt < π.
In that case, it is enough that we prove

u × u′, f(u, v) × f(u′, v)

= sin αt sin bα ≥0 .
(16)
Setting C(w) :=
p
1 + (2η + η2)⟨w, v⟩2, we apply (2) and bilinearity of cross product to compute
f(u, v) × f(u′, v) =
1
C(u)C(u′)

u × u′ + η
 ⟨u, v⟩(v × u′) + ⟨u′, v⟩(u × v)

=
1
C(u)C(u′)

u × u′ + η
 u × u′ + (⟨u, v⟩v −u) × (u′ −⟨u′, v⟩v)

(17)
=
1 + η
C(u)C(u′) · u × u′ ,
(18)
where in (17) we used the identity a × b + c × d = a × d + c × b + (a −c) × (b −d), and in (18) we
used that both ⟨u, v⟩v −u and u′ −⟨u′, v⟩v are projections of vectors onto the line orthogonal to v,
and therefore they are parallel and their cross product vanishes.
Consequently, we conclude that f(u, v) × f(u′, v) is parallel to u × u′ with a positive propor-
tionality constant, which implies (16) and concludes the proof.
B
Example with two advertisers
For another slightly more involved example, suppose there are two advertisers marketing their
products. Agents’ opinions now have ﬁve dimensions (d = 5) with the fourth and ﬁfth coordinates
corresponding to the opinions on these two products.
Initially, 500 opinions on the ﬁrst three
coordinates are distributed randomly and uniformly on a three-dimensional sphere, and the last
two coordinates are equal to zero:
ui = (ui,1, ui,2, ui,3, 0, 0)
subject to
u2
i,1 + u2
i,2 + u2
i,3 = 1 .
Suppose the two advertisers apply interventions v1 and v2 in an alternating fashion. We take v1
and v2 to be orthogonal, letting
v1 = (β, 0, 0, α, 0) ,
v2 = (0, β, 0, 0, α) ,
α = 3
4, β =
p
1 −α2 .
We proceed to apply v1 and v2 in an alternating fashion. In Figure 7 we illustrate the agents’
opinions after each advertiser applied their intervention two, four and six times (so the total of,
respectively, four, eight and twelve interventions have been applied). A pattern of polarization on
29

the fourth and ﬁfth coordinates can be observed. At the same time, the pattern on the ﬁrst three
coordinates is more complicated. The opinions on these dimensions are scattered around a circle
on the plane spanned by the ﬁrst two coordinates. This is a somewhat special behavior that arises
because vectors v1 and v2 are orthogonal. It is connected to the diﬀerence between Theorem 6.1
and Proposition 6.1 discussed in Section 6.
C
Proof of Proposition 5.1
Recall that the two-agent intervention maximizes min(eu1,d, eu2,d). Due to symmetry, we will consider
wlog the one-agent intervention that maximizes eu1,d. Substituting into (2), we get that applying
an intervention v results in
eui,d =
⟨ui, v⟩· vd
p
1 + 3⟨ui, v⟩2 .
(19)
Recalling (4), we can apply any unitary transformation on the opinions without changing the
correlations, and hence assume that
u1 := (sin α, cos α, 0, . . . , 0) ,
u2 := (−sin α, cos α, 0, . . . , 0)
(20)
for some 0 ≤α ≤π/2 and accordingly, c = cos2 α −sin2 α = cos(2α). In particular, α = 0 means
that the agents are in full agreement, α = π/4 corresponds to the case of orthogonal opinions and
α = π/2 is the case where the opinions are antipodal.
Assuming (20), once we ﬁx the ﬁrst two coordinates of the intervention v1 and v2, also the
values of ⟨u1, v⟩and ⟨u2, v⟩become ﬁxed. Therefore, due to (19), the values of eui,d depend only
on vd in a linear fashion. Accordingly, the inﬂuencer should place as much weight as possible on
the last coordinate and we can conclude that both two- and one-agent interventions have vj = 0
for 2 < j < d. Hence, in the following we will assume wlog that d = 3, u1 = (sin α, cos α, 0) and
u2 = (−sin α, cos α, 0) (see Figure 8).
First, consider the one-agent intervention maximizing eu1,3. Clearly, the intervention should be
of the form
vone = cos β · u1 + sin β · (0, 0, 1)
for some 0 ≤β ≤π/2. Substituting in (19), we compute
(eu1,3)2 = cos2 β sin2 β
1 + 3 cos2 β .
(21)
Maximizing (21), we get the maximum at cos β =
√
3/3 and
vone =
√
3
3 · u1 +
√
6
3 · (0, 0, 1) ,
resulting in eu1,3 = 1/3. The value 1/3 is the benchmark for what can be achieved by one interven-
tion. It is a maximum value for eu1,3 attainable provided that initially u1,3 = 0.
What is the eﬀect of this intervention on the other opinion u2?
Since ⟨u2, vone⟩=
√
3c/3,
substituting into (19) we get
eu2,3 =
√
3c/3 ·
√
6/3
√
1 + c2
=
c
√
2
3
√
1 + c2 .
30

t = 5
t = 9
t = 13
Figure 7: Illustration of the process described in Appendix B. This time we need to visualize
ﬁve dimensions. This is done with spatial positions for the ﬁrst three dimensions j = 1, 2, 3 and
two diﬀerent color scales for j = 4, 5. Accordingly, two ﬁgures are displayed for each time step
t = 5, 9, 13. In each pair of ﬁgures the points in the left ﬁgure have the same spatial positions as in
the right ﬁgure and the colors illustrate dimensions j = 4 (on the left) and j = 5 (on the right).
31

u1
u2
v⊥=
√
3/3 · u1
α
α
u1
u2
v⊥
α
α
Figure 8: The projection of one-agent (left) and two-agent (right) interventions onto the ﬁrst two
dimensions.
The value of eu2,3 as a function of the correlation c ∈[−1, 1] is shown in blue in Figure 4. In
particular, it increases from −1/3 to 1/3, passing through 0 for c = 0.
Moving to the two-agent intervention, in this case it is not diﬃcult to see (cf. Figure 8) that
the intervention vector should be of the form
vtwo = (0, cos β, sin β)
for some 0 ≤β ≤π/2.
A computation in a computer algebra system (CAS) establishes that
eu1,3 = eu2,3 is maximized for
cos2 β =
√
2(√3c + 5 −
√
2)
3(c + 1)
,
yielding an expression
eu1,3 = eu2,3 =
s
3c + 7 −2√6c + 10
9(c + 1)
.
This function is depicted in Figure 4 in red. In particular, for c ∈[−1, 1], it increases from 0 to
1/3 and its value at c = 0 is approximately 0.27. Furthermore, its growth close to c = −1 is of the
square-root type.
Turning to the new correlation values cone and ctwo, another CAS computation using the for-
mulas for vone and vtwo gives
cone =
c
√
2
√
c2 + 1
,
ctwo = 1 −
√
2(1 −c)
√3c + 5 ,
establishing (10).
To conclude the proof we need another elementary calculation showing that
ctwo ≥cone always holds. We omit the details, referring to Figure 3 and noting that in the critical
region for c = 1 −ε we have
ctwo = 1 −1
2ε −3
32ε2 + O(ε3) ≥cone = 1 −1
2ε −3
8ε2 + O(ε3) .
D
Proof of Proposition 5.2
Let us write a generic intervention vector as
v = (cos β · v∗, sin β) ,
32

where 0 ≤β ≤π/2, v∗∈Rd−1 and ∥v∗∥= 1. If v is applied to an opinion vector ui = (u∗
i , 0) and
we let ci := ⟨u∗
i , v∗⟩, substituting into (2) we can compute
ui + ⟨ui, v⟩· v = (u∗
i , 0) + ci cos β(cos β · v∗, sin β) = (u∗
i + ci cos2 β · v∗, ci cos β sin β) ,
and therefore, using (3),
eui,d =
ci cos β sin β
q
1 + 3c2
i cos2 β
= ciz
√
1 −z2
q
1 + 3c2
i z2
,
(22)
where we let z := cos β.
Consider a ﬁxed unit vector v∗∈Rd−1.
In order to maximize eui,d for an opinion ui with
⟨u∗
i , v∗⟩= ci, we need to optimize over z in (22), resulting in z =
rq
1 + 3c2
i −1/(
√
3ci) and,
substituting,
eui,d =
q
1 + 3c2
i −1
3ci
.
(23)
The right-hand side of (22) is easily seen to be increasing in ci > 0 for a ﬁxed z. Therefore, in order
to maximize the number of points with eui,d > T for a ﬁxed v∗, we solve the equation T =
√
1+3c2−1
3c
for c, resulting in c =
2T
1−3T 2 and apply the intervention
v = (cos β · v∗, sin β) ,
just as claimed in (11). This intervention results in eui,d > T for all opinions satisfying ⟨u∗
i , v∗⟩> c.
In other words, the objective eui,d > T is achieved for exactly those opinions contained in the
spherical cap {x ∈Rd−1 : ⟨x, v∗⟩> c}. Maximizing over all directions v∗∈Rd−1 completes the
proof.
References
[AK98]
Edoardo Amaldi and Viggo Kann.
On the approximability of minimizing nonzero
variables or unsatisﬁed relations in linear systems.
Theoretical Computer Science,
209(1–2):237–260, 1998.
[Axe97]
Robert Axelrod. The dissemination of culture: A model with local convergence and
global polarization. Journal of Conﬂict Resolution, 41(2):203–226, 1997.
[BAB+18]
Christopher A. Bail, Lisa P. Argyle, Taylor W. Brown, John P. Bumpus, Haohan
Chen, M. B. Fallin Hunzaker, Jaemin Lee, Marcus Mann, Friedolin Merhout, and
Alexander Volfovsky. Exposure to opposing views on social media can increase political
polarization.
Proceedings of the National Academy of Sciences, 115(37):9216–9221,
2018.
[Bal07]
Delia Baldassarri. Crosscutting Social Spheres? Political Polarization and the Social
Roots of Pluralism. PhD thesis, Columbia University, 2007.
[BB06]
Nader H. Bshouty and Lynn Burroughs. Maximizing agreements and coagnostic learn-
ing. Theoretical Computer Science, 350(1):24–39, 2006.
33

[BB07]
Delia Baldassarri and Peter Bearman. Dynamics of political polarization. American
Sociological Review, 72(5):784–811, 2007.
[BBC20]
BBC. Trump signs executive order targeting Twitter after fact-checking row, 2020. 29
May 2020. https://www.bbc.com/news/technology-52843986.
[BDEL03]
Shai Ben-David, Nadav Eiron, and Philip M. Long. On the diﬃculty of approximately
maximizing agreements. Journal of Computer and System Sciences, 66(3):496–514,
2003.
[BDES02]
Shai Ben-David, Nadav Eiron, and Hans Ulrich Simon. The computational complexity
of densest region detection. Journal of Computer and System Sciences, 64(1):22–47,
2002.
[BDS00]
Shai Ben-David and Hans-Ulrich Simon. Eﬃcient learning of linear perceptrons. In
Advances in Neural Information Processing Systems (NIPS), pages 189–195, 2000.
[BG98]
Venkatesh Bala and Sanjeev Goyal. Learning from neighbours. The Review of Eco-
nomic Studies, 65(3):595–621, 1998.
[BG08]
Delia Baldassarri and Andrew Gelman. Partisans without constraint: Political po-
larization and trends in American public opinion.
American Journal of Sociology,
114(2):408–446, 2008.
[BGN+18]
Jarosław Błasiok, Venkatesan Guruswami, Preetum Nakkiran, Atri Rudra, and Madhu
Sudan. General strong polarization. In Symposium on Theory of Computing (STOC),
pages 485–492, 2018.
[BMA15]
Eytan Bakshy, Solomon Messing, and Lada A. Adamic.
Exposure to ideologically
diverse news and opinion on Facebook. Science, 348(6239):1130–1132, 2015.
[CRF+11]
Michael D. Conover, Jacob Ratkiewicz, Matthew Francisco, Bruno Gon¸calves, Filippo
Menczer, and Alessandro Flammini. Political polarization on Twitter. In International
Conference on Weblogs and Social Media (ICWSM), pages 89–96, 2011.
[DGL13]
Pranav Dandekar, Ashish Goel, and David T. Lee. Biased assimilation, homophily,
and the dynamics of polarization. Proceedings of the National Academy of Sciences,
110(15):5791–5796, 2013.
[DVSC+17] Michela Del Vicario, Antonio Scala, Guido Caldarelli, H. Eugene Stanley, and Wal-
ter Quattrociocchi. Modeling conﬁrmation bias and polarization. Scientiﬁc Reports,
7:40391, 2017.
[FA08]
Morris P. Fiorina and Samuel J. Abrams. Political polarization in the American public.
Annual Reviev of Political Science, 11:563–588, 2008.
[FAP05]
Morris P. Fiorina, Samuel J. Abrams, and Jeremy C. Pope. Culture War? The Myth
of a Polarized America. Pearson-Longman, 2005.
[FGKP06]
Vitaly Feldman, Parikshit Gopalan, Subhash Khot, and Ashok Kumar Ponnuswami.
New results for learning noisy parities and halfspaces. In Symposium on Foundations
of Computer Science (FOCS), pages 563–574, 2006.
34

[For18]
Jacey Fortin.
A list of the companies cutting ties with the N.R.A., 2018.
The
New York Times website, 24 February 2018. https://www.nytimes.com/2018/02/
24/business/nra-companies-boycott.html.
[Gar18]
Kiran Garimella. Polarization on Social Media. PhD thesis, Aalto University, 2018.
20/2018.
[GGPT17]
Kiran Garimella, Aristides Gionis, Nikos Parotsidis, and Nikolaj Tatti.
Balancing
information exposure in social networks. In Advances in Neural Information Processing
Systems (NIPS), pages 4663–4671, 2017.
[GKT21]
Jason Gaitonde, Jon Kleinberg, and ´Eva Tardos. Polarization in geometric opinion
dynamics. To appear in ACM Conference on Economics and Computation (EC), 2021.
[GMGM17] Kiran Garimella, Gianmarco De Francisci Morales, Aristides Gionis, and Michael
Mathioudakis. Reducing controversy by connecting opposing views. In International
Conference on Web Search and Data Mining (WSDM), pages 81–90. ACM, 2017.
[GR09]
Venkatesan Guruswami and Prasad Raghavendra. Hardness of learning halfspaces with
noise. SIAM Journal on Computing, 39(2):742–765, 2009.
[HK02]
Rainer Hegselmann and Ulrich Krause. Opinion dynamics and bounded conﬁdence.
models, analysis, and simulation. Journal of Artiﬁcial Societies and Social Simulation,
5(3), 2002.
[IW15]
Shanto Iyengar and Sean J. Westwood. Fear and loathing across party lines: New
evidence on group polarization. American Journal of Political Science, 59(3):690–707,
2015.
[JMB04]
Ali Jadbabaie, Nader Motee, and Mauricio Barahona. On the stability of the Kuramoto
model of coupled nonlinear oscillators. In American Control Conference (ACC), vol-
ume 5, pages 4296–4301, 2004.
[KP18]
Stefan Krasa and Mattias K. Polborn. Political competition in legislative elections.
American Political Science Review, 112(4):809–825, 2018.
[KW18]
Ilyana Kuziemko and Ebonya Washington. Why did the democrats lose the south?
Bringing new data to an old debate. American Economic Review, 108(10):2830–67,
2018.
[LBS00]
Howard Lavine, Eugene Borgida, and John L. Sullivan. On the relationship between
attitude involvement and attitude accessibility: Toward a cognitive-motivational model
of political information processing. Political Psychology, 21(1):81–106, 2000.
[LRL79]
Charles G. Lord, Lee Ross, and Mark R. Lepper. Biased assimilation and attitude
polarization: The eﬀects of prior theories on subsequently considered evidence. Journal
of Personality and Social Psychology, 37(11):2098–2109, 1979.
[MKFB03]
Michael W. Macy, James A. Kitts, Andreas Flache, and Steve Benard. Polarization in
dynamic networks: A Hopﬁeld model of emergent structure. Dynamic Social Network
Modeling and Analysis, pages 162–173, 2003.
35

[MS05]
Sendhil Mullainathan and Andrei Shleifer. The market for news. American Economic
Review, 95(4):1031–1053, 2005.
[MS10]
Elchanan Mossel and Grant Schoenebeck. Reaching consensus on social networks. In
Innovations in Computer Science (ITCS), 2010.
[Noa98]
Mark Noah. Beyond individual diﬀerences: Social diﬀerentiation from ﬁrst principles.
American Sociological Review, 63(3):309, 1998.
[NSL90]
Andrzej Nowak, Jacek Szamrej, and Bibb Latan´e. From private attitude to public
opinion: A dynamic theory of social impact. Psychological Review, 97(3):362, 1990.
[Par11]
Eli Pariser. The Filter Bubble: How the New Personalized Web Is Changing What We
Read and How We Think. Penguin, New York, 2011.
[Pew14]
Pew Research Center.
Political polarization in the American public:
How in-
creasing ideological uniformity and partisan antipathy aﬀect politics,
compro-
mise and everyday life,
2014.
https://www.people-press.org/2014/06/12/
political-polarization-in-the-american-public/.
[PPTF17]
Sergey E. Parsegov, Anton V. Proskurnikov, Roberto Tempo, and Noah E. Friedkin.
Novel multidimensional models of opinion dynamics in social networks. IEEE Trans-
actions on Automatic Control, 62(5):2270–2285, May 2017.
[Pri13]
Markus Prior. Media and political polarization. Annual Review of Political Science,
16:101–127, 2013.
[PT06]
Thomas F. Pettigrew and Linda R. Tropp. A meta-analytic test of intergroup contact
theory. Journal of Personality and Social Psychology, 90(5):751–783, 2006.
[PY18]
Jacopo Perego and Sevgi Yuksel. Media competition and social disagreement, 2018.
Working Paper.
[SCP+19]
Kazutoshi Sasahara, Wen Chen, Hao Peng, Giovanni Luca Ciampaglia, Alessan-
dro Flammini, and Filippo Menczer. On the inevitability of online echo chambers.
arXiv:1905.03919, 2019.
[SH15]
John Sides and Daniel J. Hopkins. Political polarization in American politics. Blooms-
bury Publishing USA, 2015.
[Sny15]
Brendan Snyder. LGBT advertising: How brands are taking a stance on issues. Think
with Google, 2015.
[SPJ+16]
Guodong Shi, Alexandre Proutiere, Mikael Johansson, John S. Baras, and Karl H.
Johansson. The evolution of beliefs over signed social networks. Operations Research,
64(3):585–604, 2016.
[SVS04]
Elizabeth A. Saylor, Katherine A. Vittes, and Susan B. Sorenson. Firearm advertising:
Product depiction in consumer gun magazines.
Evaluation Review, 28(5):420–433,
2004.
[VSL77]
Donald E. Vinson, Jerome E. Scott, and Lawrence M. Lamont. The role of personal
values in marketing and consumer behavior. Journal of Marketing, 41(2):44–50, 1977.
36

[Wil91]
David Williams. Probability with Martingales. Cambridge University Press, 1991.
[WMKL15] Hywel T.P. Williams, James R. McMurray, Tim Kurz, and F. Hugo Lambert. Network
analysis reveals open forums and echo chambers in social media discussions of climate
change. Global Environmental Change, 32:126–138, 2015.
[Zal92]
John R. Zaller. The Nature and Origins of Mass Opinion. Cambridge University Press,
1992.
37

