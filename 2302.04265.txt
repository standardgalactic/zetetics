PFGM++: Unlocking the Potential of Physics-Inspired Generative Models
Yilun Xu 1 Ziming Liu 1 Yonglong Tian 1 Shangyuan Tong 1 Max Tegmark 1 Tommi Jaakkola 1
Abstract
We introduce a new family of physics-inspired
generative models termed PFGM++ that uniﬁes
diffusion models and Poisson Flow Generative
Models (PFGM). These models realize generative
trajectories for N dimensional data by embed-
ding paths in N+D dimensional space while still
controlling the progression with a simple scalar
norm of the D additional variables. The new mod-
els reduce to PFGM when D=1 and to diffusion
models when D→∞. The ﬂexibility of choos-
ing D allows us to trade off robustness against
rigidity as increasing D results in more concen-
trated coupling between the data and the addi-
tional variable norms. We dispense with the bi-
ased large batch ﬁeld targets used in PFGM and
instead provide an unbiased perturbation-based
objective similar to diffusion models. To explore
different choices of D, we provide a direct align-
ment method for transferring well-tuned hyperpa-
rameters from diffusion models (D→∞) to any
ﬁnite D values. Our experiments show that mod-
els with ﬁnite D can be superior to previous state-
of-the-art diffusion models on CIFAR-10/FFHQ
64×64 datasets, with FID scores of 1.91/2.43
when D=2048/128. In class-conditional setting,
D=2048 yields current state-of-the-art FID of
1.74 on CIFAR-10. In addition, we demonstrate
that models with smaller D exhibit improved ro-
bustness against modeling errors. Code is avail-
able at https://github.com/Newbeeer/
pfgmpp
1. Introduction
Physics continues to inspire new deep generative models
such as diffusion models (Sohl-Dickstein et al., 2015; Ho
et al., 2020; Song et al., 2021b; Karras et al., 2022) based
on thermodynamics (Jarzynski, 1997) or Poisson ﬂow gener-
ative models (PFGM) (Xu et al., 2022) derived from electro-
statics (Grifﬁths, 2005). The associated generative processes
1Massachusetts Institute of Technology, MIT, Cambridge, MA,
USA. Correspondence to: Yilun Xu <ylxu@mit.edu>.
arXiv preprint.
involve iteratively de-noising samples by following phys-
ically meaningful trajectories. Diffusion models learn a
noise-level dependent score function so as to reverse the ef-
fects of forward diffusion, progressively reducing the noise
level σ along the generation trajectory. PFGMs in turn aug-
ment N-dimensional data points with an extra dimension
and evolve samples drawn from a uniform distribution over
a large N+1-dimensional hemisphere back to the z=0 hy-
perplane where the clean data (as charges) reside by tracing
learned electric ﬁeld lines. Diffusion models in particular
have been demonstrated across image (Song et al., 2021b;
Nichol et al., 2022a; Ramesh et al., 2022), 3D (Zeng et al.,
2022; Poole et al., 2022), audio (Kong et al., 2020; Chen
et al., 2020) and biological data (Shi et al., 2021; Watson
et al., 2022) generation, and have more stable training objec-
tives compared to GANs (Arjovsky et al., 2017; Brock et al.,
2019). More recent PFGM (Xu et al., 2022) rival diffusion
models on image generation.
In this paper, we introduce a broader family of physics-
inspired generative models that we call PFGM++. These
models extend the electrostatic view into higher dimen-
sions through multi-dimensional z ∈RD augmentations.
When interpreting N-dimensional data points x as posi-
tive charges, the electric ﬁeld lines deﬁne a surjection from
a uniform distribution on an inﬁnite N+D-dimensional
hemisphere to the data distribution located on the z=0 hy-
perplane. We can therefore draw generative samples by
following the electric ﬁeld lines, evolving points from the
hemisphere back to the z=0 hyperplane. Since the electric
ﬁeld has rotational symmetry on the surface of the D-dim
cylinder ∥z∥2 = r for any r > 0, we can track the sampling
trajectory with a simple scalar r instead of every compo-
nent of z. The use of symmetry turns the aforementioned
surjection into a bijection between an easy-to-sample prior
on a large r = rmax hyper-cylinder to the data distribution.
The symmetry reduction also permits D to take any positive
values, including reals. We derive a new perturbation-based
training objective akin to denoising score matching (Vincent,
2011) that avoids the need to use large batches to construct
electric ﬁeld line targets in PFGM. The perturbation-based
objective is more efﬁcient, unbiased, and compatible with
paired sample training of conditional generation models.
The models in the new family differ based on their aug-
mentation dimension D which is now a hyper-parameter.
By setting D=1 we obtain PFGM while D→∞leads to
arXiv:2302.04265v2  [cs.LG]  10 Feb 2023

PFGM++: Unlocking the Potential of Physics-Inspired Generative Models
PFGM (Xu et al, 2022)
D = 1
D →∞
D*
Sec 5 Sweet spot balancing
robustness and rigidity
Diffusion models
VE/VP (Song et al, 2021)
EDM (Karras et al, 2022)
PFGM++ 
 
(D ∈ℝ+)
Sec 3.1 Higher-dimensional augmentation 
Sec 3.2 Perturbation-based training objective
Thm 4.1 Field / Sampling equivalence
Prop 4.2 Training equivalence
Extension from PFGM
Equivalence between 
 and diffusion models
D →∞
Figure 1. Overview of paper contributions and structure. PFGM++ unify PFGM and diffusion models, as well as the potential to combine
their strengths (robustness and rigidity).
diffusion models. We establish D→∞equivalence with
popular diffusion models (Song et al., 2021b; Karras et al.,
2022) both in terms of their training objectives as well as
their inferential processes. We demonstrate that the hyper-
parameter D controls the balance between robustness and
rigidity: using a small D widens the distribution of noisy
training sample norms in comparison to the norm of the
augmented variables. However, small D also leads to a
heavy-tailed sampling problem at any ﬁxed augmentation
norm making learning more challenging. Neither D=1 nor
D→∞offers an ideal balance between being insensitive
to missteps (robustness) and allowing effective learning
(rigidity). Instead, we adjust D in response to different ar-
chitectures and tasks. To facilitate quickly ﬁnding the best
D we provide an alignment method to directly transfer other
hyperparameters across different choices of D.
Experimentally, we show that some models with ﬁnite
D outperform the previous state-of-the-art diffusion mod-
els (D→∞), i.e., EDM (Karras et al., 2022), on image
generation tasks. In particular, intermediate D=2048/128
achieve the best performance among other choices of D
ranging from 64 to ∞, with min FID scores of 1.91/2.43
on CIFAR-10 and FFHQ 64×64 datasets in unconditional
generation, using 35/79 NFE. In class-conditional genera-
tion, D=2048 achieves new state-of-the-art FID of 1.74 on
CIFAR-10. We further verify that in general, decreasing D
leads to improved robustness against a variety of sources of
errors, i.e., controlled noise injection, large sampling step
sizes and post-training quantization.
Our contributions are summarized as follows: (1) We
propose PFGM++ as a new family of generative models
based on expanding augmented dimensions and show that
symmetries involved enable us to deﬁne generative paths
simply based on the scalar norm of the augmented vari-
ables (Sec 3.1); (2) We propose a perturbation-based objec-
tive to dispense with any biased large batch derived electric
ﬁeld targets, allowing unbiased training (Sec 3.2); (3) We
prove that the score ﬁeld and the training objective of dif-
fusion models arise in the limit D→∞(Sec 4); (4) We
demonstrate the trade-off between robustness and rigidity
by varying D (Sec 5). We also detail the hyperparameter
transfer procedures from EDM/DDPM (D →∞) to ﬁnite
Ds in Appendix C.2; (5) We empirically show that mod-
els with ﬁnite D achieve superior performance to diffusion
models while exhibiting improved robustness (Sec 6).
2. Background and Related Works
Diffusion Model Diffusion models (Sohl-Dickstein et al.,
2015; Ho et al., 2020; Song et al., 2021b; Karras et al.,
2022) are often presented as a pair of two processes. A ﬁxed
forward process governs the training of the model, which
learns to denoise data of different noise levels. A corre-
sponding backward process involves utilizing the trained
model iteratively to denoise the samples starting from a fully
noisy prior distribution. Karras et al. (2022) propose a unify-
ing framework for popular diffusion models (VE/VP (Song
et al., 2021b) and EDM (Karras et al., 2022)), and their sam-
pling process can be understood as traveling in time with a
probability ﬂow ordinary differential equation (ODE):
dx = −˙σ(t)σ(t)∇x log pσ(t)(x)dt
where σ(t) is a predeﬁned noise schedule w.r.t. time, and
∇x log pσ(t)(x) is the score of noise-injected data distribu-
tion at time t. A neural network fθ(x, σ) is trained to learn
the score ∇x log pσ(t)(x) by minimizing a weighted sum of
the denoising score-matching objectives (Vincent, 2011):
Eσ∼p(σ)λ(σ)Ey∼p(y)Ex∼pσ(x|y)

∥fθ(x, σ) −∇x log pσ(x|y)∥2
2

(1)
where p(σ) deﬁnes a training distribution of noise levels,
λ(σ) is a weighting function, p(y) is the data distribution,
and pσ(x|y) = N(0, σ2I) deﬁnes a Gaussian perturbation
kernel which samples a noisy version x of the clean data y.
Please refer to Table 1 in Karras et al. (2022) for speciﬁc
instantiations of different diffusion models.
PFGM Inspired by the theory of electrostatics (Grifﬁths,
2005), Xu et al. (2022) propose Poisson ﬂow generative

PFGM++: Unlocking the Potential of Physics-Inspired Generative Models
models (PFGM), which interpret the N-dimensional data
x ∈RN as electric charges in an N+1-dimensional space
augmented with an extra dimension z: ˜x = (x, z) ∈RN+1.
In particular, the training data is placed on the z=0 hyper-
plane, and the electric ﬁeld lines emitted by the charges
deﬁne a bijection between the data distribution and a uni-
form distribution on the inﬁnite hemisphere of the aug-
mented space1. To perform generative modeling, PFGM
learn the following high-dimensional electric ﬁeld, which is
the derivative of the electric potential in a Poisson equation:
E(˜x) =
1
SN(1)
Z
˜x −˜y
∥˜x −˜y∥N+1 p(y)dy
(2)
where SN(1) is the surface area of a unit N-sphere (a ge-
ometric constant), and p(y) is the data distribution. Sam-
ples are then generated by following the electric ﬁeld lines,
which are described by the ODE d˜x = E(˜x)dt. In prac-
tice, the network is trained to estimate a normalized ver-
sion of the following empirical electric ﬁeld:
ˆE(˜x) =
c(˜x) Pn
i=1
˜x−˜yi
∥˜x−˜yi∥N+1 , where c(˜x) = 1/ Pn
i=1
1
∥˜x−˜yi∥N+1
and {˜yi}n
i=1 ∼˜p(˜y) is a large batch used to approximate
the integral in Eq. (2). The training objective is minimizing
the ℓ2-loss between the neural model prediction fθ(˜x) and
the normalized ﬁeld E(˜x)/∥E(˜x)∥at various positions of ˜x.
These positions are heuristically designed to carefully cover
the regions that the sampling trajectories pass through.
Phases of Score Field Xu et al. (2023) show that the score
ﬁeld in the forward process of diffusion models can be
decomposed into three phases. When moving from the
near ﬁeld (Phase 1) to the far ﬁeld (Phase 3), the perturbed
data get inﬂuenced by more modes in the data distribution.
They show that the posterior p0|σ(y|x) ∝pσ(x|y)p(y)
serves as a phase indicator, as it gradually evolves from
a delta distribution to uniform distribution when shifting
from Phase 1 to Phase 3. The relevant concepts of phases
have also been explored in Karras et al. (2022); Choi et al.
(2022); Xiao et al. (2022). Similar to the PFGM training
objective, Xu et al. (2023) approximates the score ﬁeld by
large batches to reduce the variance of training targets in
Phase 2, where multiple data points exert comparable but
distinct inﬂuences on the scores. These observations inspire
us to align the phases of different Ds in Sec 4.
3. PFGM++: A Novel Generative Framework
In this section, we present our new family of generative
models PFGM++, generalizing PFGM (Xu et al., 2022) in
terms of the augmented space dimensionality. We show that
the electric ﬁelds in N+D-dimensional space with D ∈Z+
still constitute a valid generative model (Sec 3.1). Fur-
thermore, we show that the additional D-dimensional aug-
mented variable can be condensed into their scalar norm due
1In practice, the hemisphere is projected to a hyperplane
z=zmax, so that all samples have the initial z.
to the inherent symmetry of the electric ﬁeld. To improve the
training process, we propose an efﬁcient perturbation-based
objective for training PFGM++ (Sec 3.2) without relying on
the large batch approximation in the original PFGM.
3.1. Electric ﬁeld in N+D-dimensional space
While PFGM (Xu et al., 2022) consider the electric ﬁeld
in a N+1-dimensional augmented space, we augment the
data x with D-dimensional variables z = (z1, . . . , zD), i.e.,
˜x = (x, z) and D ∈Z+. Similar to the N+1-dimensional
electric ﬁeld (Eq. (2)), the electric ﬁeld at the augmented
data ˜x = (x, z) ∈RN+D is:
E(˜x) =
1
SN+D−1(1)
Z
˜x −˜y
∥˜x −˜y∥N+D p(y)dy
(3)
Analogous to the theoretical results presented in PFGM,
with the electric ﬁeld as the drift term, the ODE d˜x=E(˜x)dt
deﬁnes a surjection from a uniform distribution on an inﬁnite
N+D-dim hemisphere and the data distribution on the N-
dim z=0 hyperplane. However, the mapping has SO(D)
symmetry on the surface of D-dim cylinder PD
i=1 z2
i = r2
for any positive r. We provide an illustrative example at
the bottom of Fig. 2 (D=2, N=1), where the electric ﬂux
emitted from a line segment (red) has rotational symmetry
through the ring area (blue) on the z2
1 + z2
2 = r2 cylinder.
Hence, instead of modeling the individual behavior of each
zi, it sufﬁces to track the norm of augmented variables —
r(˜x) = ∥z∥2 — due to symmetry. Speciﬁcally, note that
dzi = E(˜x)zidt, and the time derivative of r is
dr
dt =
D
X
i=1
zi
r
dzi
dt =
Z
PD
i=1 z2
i
SN+D−1(1)r∥˜x −˜y∥N+D p(y)dy
=
1
SN+D−1(1)
Z
r
∥˜x −˜y∥N+D p(y)dy
Henceforth we replace the notation for augmented data with
˜x = (x, r) for simplicity. After the symmetry reduction, the
ﬁeld to be modeled has a similar form as Eq. (3) except that
the last D sub-components {E(˜x)zi}D
i=1 are condensed into
a scalar E(˜x)r =
1
SN+D−1(1)
R
r
∥˜x−˜y∥N+D p(y)dy. There-
fore, we can use the physically meaningful r as the anchor
variable in the ODE dx/dr by change-of-variable:
dx
dr = dx
dt
dt
dr = E(˜x)x · E(˜x)−1
r
= E(˜x)x
E(˜x)r
(4)
Indeed, the ODE dx/dr turns the aforementioned surjection
into a bijection between an easy-to-sample prior distribution
on the r=rmax hyper-cylinder2 and the data distribution on
r=0 (i.e., z=0) hyperplane. The following theorem states
the observation formally:
2The hyper-cylinder here is consistent with the hemisphere in
PFGM (Xu et al., 2022), because hyper-cylinders degrade to hyper-
planes for D = 1, which are in turn isomorphic to hemispheres.

PFGM++: Unlocking the Potential of Physics-Inspired Generative Models
Figure 2. The augmented dimension D affects electric ﬁeld lines
(gray), which connect charge/data on a line (purple) to latent
space (green). When D = 1 (top) or D = 2 (bottom), electric ﬁeld
lines map the same red line segment to a blue line segment or onto
a blue ring, respectively. The mapping deﬁned by electric lines has
SO(2) symmetry on the surface of z2
1 + z2
2 = r2 cylinder.
Theorem 3.1. Assume the data distribution p ∈C1 and
p has compact support. As rmax→∞, for D ∈R+, the
ODE dx/dr = E(˜x)x/E(˜x)r deﬁnes a bijection between
limrmax→∞prmax(x) ∝limrmax→∞rD
max/(∥x∥2
2 + r2
max)
N+D
2
when r = rmax and the data distribution p when r = 0.
Proof sketch. The r-dependent intermediate distribution of
the ODE (Eq. (4)) is pr(x)∝
R
rD/∥˜x −˜y∥N+Dp(y)dy,
which satisﬁes initial/terminal conditions, i.e., pr=0=p,
limrmax→∞prmax ∝limrmax→∞rD
max/(∥x∥2
2 + r2
max)
N+D
2
, as
well as the continuity equation of the ODE, i.e., ∂rpr +∇x ·
(prE(˜x)x/E(˜x)r) = 0.
We defer the formal proof to Appendix A.1. Note that in the
theorem we further extend the domain of D from positive in-
tegers to positive real numbers. In practice, the starting con-
dition of the ODE is some sufﬁciently large rmax such that
prmax(x) ∝∼rD
max/(∥x∥2
2 + r2
max)
N+D
2
. The terminal condi-
tion is r= 0, which represents the generated samples reach-
ing the data support. The proposed PFGM++ framework
thus permits choosing arbitrary D, including D = 1 which
recovers the original PFGM formulation. Interestingly, we
will also show that when D→∞, PFGM++ recover the dif-
fusion models (Sec 4). In addition, as discussed in Sec 5,
the choice of D is important, since it controls two properties
of the associated electric ﬁeld, i.e., robustness and rigidity,
which affect the sampling performance.
3.2. New objective with Perturbation Kernel
Although the training process in PFGM can be directly ap-
plied to PFGM++, we propose a more efﬁcient training
objective to dispense with the large batch in PFGM. The
objective from PFGM paper (Xu et al., 2022) requires sam-
pling a large batch of data {yi}n
i=1∼pn(y) in each training
step to approximate the integral in the electric ﬁeld (Eq. (3)):
E˜ptrain(˜x)E{yi}n
i=1∼pn(y)Ex∼pσ(x|y1)
"fθ(˜x) −
Pn−1
i=0
˜x−˜yi
∥˜x−˜yi∥N+D
 Pn−1
i=0
˜x−˜yi
∥˜x−˜yi∥N+D

2 + γ

2
2
#
where ˜ptrain is heuristically designed to cover the regions
that the backward ODE traverses. This objective has several
obvious drawbacks: (1) The large batch incurs additional
overheads; (2) Its minimizer is a biased estimator of the
electric ﬁeld (Eq. (3)); (3) The large batch is incompatible
with typical paired sample training of conditional generation,
where each condition is paired with only one sample, such
as text-to-image (Rombach et al., 2021; Saharia et al., 2022)
and text-to-3D generation (Poole et al., 2022; Nichol et al.,
2022b).
To remedy these issues, we propose a perturbation-based ob-
jective without the need for the large batch, while achieving
an unbiased minimizer and enabling paired sample train-
ing of conditional generation. Inspired by denoising score-
matching (Vincent, 2011), we design the perturbation kernel
to guarantee that the minimizer in the following square loss
objective matches the ground-truth electric ﬁeld in Eq. (3):
Er∼p(r)Ep(y)Epr(x|y)

∥fθ(˜x) −(˜x −˜y)∥2
2

(5)
where r ∈(0, ∞), p(r) is the training distribution over r,
pr(x|y) is the perturbation kernel and ˜y=(y, 0)/˜x=(x, r)
are the clean/perturbed augmented data.
The mini-
mizer of Eq. (5) is f ∗
θ (˜x)∝
R
pr(x|y)(˜x −˜y)p(y)dy,
which
matches
the
direction
of
electric
ﬁeld
E(˜x)∝
R
(˜x −˜y)/∥˜x −˜y∥N+Dp(y)dy when setting the
perturbation kernel to pr(x|y)∝1/(∥x −y∥2
2 + r2)
N+D
2
.
Denoting the r-dependent intermediate marginal dis-
tribution as pr(x)=
R
pr(x|y)p(y)dy,
the following
proposition states that the choice of pr(·|y) guarantee that
the minimizer of the square loss to match the direction of
the electric ﬁeld:
Proposition 3.2. With perturbation kernel pr(x|y) ∝
1/(∥x −y∥2
2 + r2)
N+D
2
, for ∀x ∈RN, r > 0, the mini-
mizer f ∗
θ (˜x) in the PFGM++ objective (Eq. (5)) matches
the direction of electric ﬁeld E(˜x) in Eq. (3). Speciﬁcally,
f ∗
θ (˜x) = (SN+D−1(1)/pr(x))E(˜x).
We defer the proof to Appendix A.2. The proposition in-
dicates that the minimizer f ∗
θ (˜x) can match the direction
of E(˜x) with sufﬁcient data and model capacity. The cur-
rent training target in Eq. (5) is the directional vector be-
tween the clean data ˜y and perturbed data ˜x akin to de-
noising score-matching for diffusion models (Song et al.,
2021b; Karras et al., 2022). In addition, the new objective

PFGM++: Unlocking the Potential of Physics-Inspired Generative Models
allows for conditional generations under a one-sample-per-
condition setup. Since the perturbation kernel is isotropic,
we can decompose pr(·|y) in hyperspherical coordinates
to Uψ(ψ)pr(R), where Uψ is the uniform distribution over
the angle component and the distribution of the perturbed
radius R = ∥x −y∥2 is
pr(R) ∝
RN−1
(R2 + r2)
N+D
2
We defer the practical sampling procedure of the perturba-
tion kernel to Appendix B. The mean of the r-dependent
radius distribution pr(R) is around r
p
N/D. Hence we
explicitly normalize the target in Eq. (5) by r/
√
D, to keep
the norm of the target around the constant
√
N, similar to
diffusion models (Song et al., 2021b). In addition, we drop
the last dimension of the target because it is a constant —
(˜x −˜y)r/(r/
√
D) =
√
D. Together, the new objective is
Er∼p(r)Ep(˜y)Epr(˜x|˜y)
fθ(˜x) −x −y
r/
√
D

2
2

(6)
After training the neural network through objective Eq. (6),
we can use the ODE (Eq. (4)) anchored by r to generate sam-
ples, i.e., dx/dr = E(˜x)x/E(˜x)r = fθ(˜x)/
√
D, starting
from the prior distribution prmax.
4. Diffusion Models as D→∞Special Cases
Diffusion models generate samples by simulating ODE/SDE
involving the score function ∇x log pσ(x) at different inter-
mediate distributions pσ (Song et al., 2021b; Karras et al.,
2022), where σ is the standard deviation of the Gaussian
kernel. In this section, we show that both sampling and
training schemes in diffusion models are equivalent to those
in D→∞case under the PFGM++ framework. To begin
with, we show that the electric ﬁeld (Eq. (3)) in PFGM++
has the same direction as the score function when D tends
to inﬁnity, and their sampling processes are also identical.
Theorem 4.1. Assume the data distribution p ∈C1. Con-
sider taking the limit D →∞while holding σ = r/
√
D
ﬁxed. Then, for all x,
lim
D→∞
r=σ
√
D

√
D
E(˜x)r
E(˜x)x −σ∇x log pσ=r/
√
D(x)

2
= 0
where E(˜x
=
(x, r))x is given in Eq. (3).
Fur-
ther, given the same initial point, the trajectory of
the PFGM++ ODE (dx/dr=E(˜x)x/E(˜x)r) matches
the diffusion ODE (Karras et al., 2022) (dx/dt= −
˙σ(t)σ(t)∇x log pσ(t)(x)) in the same limit.
Proof sketch. By re-expressing the x component E(˜x)x
in the electric ﬁeld and the score ∇x log pσ in dif-
fusion models, the proof boils down to show that
limD→∞,r=σ
√
D pr(x|y)
∝
exp(−∥x −y∥2
2/2σ2) for
∀x, y ∈RN+D:
lim
D→∞,r=σ
√
D
1
(∥x −y∥2
2 + r2)
N+D
2
∝
lim
D→∞,r=σ
√
D
e−(N+D)
2
ln(1+ ∥x−y∥2
r2
)
=
lim
D→∞,r=σ
√
D
e−
(N+D)∥x−y∥2
2
2r2
= e−
∥x−y∥2
2
2σ2
(7)
The equivalence of trajectories can be proven by change-of-
variable dσ = dr/
√
D. Their prior distributions are also the
same since limD→∞prmax=σmax
√
D(x) = N(0, σmaxI).
We defer the formal proof to Appendix A.3. Since ∥x −
y∥2
2/r2 ≈N/D when x ∼pr(x), y ∼p(y), Eq. (7) ap-
proximately holds under the condition D ≫N. Remark-
ably, the theorem states that PFGM++ recover the ﬁeld
and sampling of previous popular diffusion models, such
as VE/VP (Song & Ermon, 2020) and EDM (Karras et al.,
2022), by choosing the appropriate schedule and scale func-
tion in Karras et al. (2022).
In addition to the ﬁeld and sampling equivalence, we demon-
strate that the proposed PFGM++ objective (Eq. (6)) with
perturbation kernel pr(x|y) ∝1/(∥x −y∥2
2 + r2)
N+D
2
re-
covers the weighted sum of the denoising score matching
objective (Vincent, 2011) for training continuous diffusion
model (Karras et al., 2022; Song et al., 2021b) when D→∞.
All previous objectives for training diffusion models can be
subsumed in the following form (Karras et al., 2022), under
different parameterizations of the neural networks fθ:
Eσ∼p(σ)λ(σ)Ep(y)Epσ(x|y)
fθ(x, σ) −x −y
σ

2
2

(8)
where pσ(x|y)
∝
exp(−∥x −y∥2
2/2σ2).
The ob-
jective of the diffusion models resembles the one of
PFGM++ (Eq. (6)). Indeed, we show that when D→∞, the
minimizer of the proposed PFGM++ objective at ˜x=(x, r)
is f ∗
θ (x, r = σ
√
D)=σ∇x log pσ(x), the same as the mini-
mizer of diffusion objective at the noise level σ=r/
√
D.
Proposition 4.2. When r = σ
√
D, D →∞, the minimizer
in the PFGM++ objective (Eq. (6)) is equaivalent to the
minimizer in the weighted sum of denoising score matching
objective (Eq. (8))
We defer the proof to Appendix A.4. The proposition states
that the training objective of diffusion models is essentially
the same as PFGM++’s when D→∞. Combined with The-
orem 4.1, PFGM++ thus recover both the training and sam-
pling processes of diffusion models when D→∞.
Transfer hyperparameters to ﬁnite Ds
The training
hyperparameters of diffusion models (D→∞) have been

PFGM++: Unlocking the Potential of Physics-Inspired Generative Models
0
20000
40000
60000
80000
r
0.0
0.2
0.4
0.6
0.8
1.0
Mean TVD
No alignment
D = 24
D = 28
D = 212
D = 216
D = 220
(a) No alignment
0
20
40
60
80
0.2
0.4
0.6
0.8
1.0
r =
D alignment
D = 24
D = 28
D = 212
D = 216
D = 220
(b) r = σ
√
D alignment
Figure 3. Mean TVD between the posterior p0|r(·|x) (x is per-
turbed sample) and the uniform prior, w/o (a) and w/ (b) the phase
alignment (r = σ
√
D).
highly optimized through a series of works (Ho et al.,
2020; Song et al., 2021b; Karras et al., 2022).
It mo-
tivates us to transfer hyperparameters, such as rmax and
p(r), of D→∞to ﬁnite Ds. Here we present an align-
ment method that enables a “zero-shot” transfer of hyper-
parameters across different Ds. Our alignment method is
inspired by the concept of phases in Xu et al. (2023), which
is related to the variation of training targets. We aim to
align the intermediate marginal distributions pr for two
distinct D1, D2 > 0. In Appendix C.1, we demonstrate
that when r ∝
√
D, the phase of the intermediate distribu-
tion pr is approximately invariant to all D > 0 (including
D→∞). In other words, when rD1/rD2 =
p
D1/D2, the
phases of prD1 and prD2 , under D1 and D2 respectively, are
roughly aligned. Theorem 4.1 further shows that the relation
r=σ
√
D makes PFGM++ equivalent to diffusion models
when D→∞. Together, the r=σ
√
D formula aligns the
phases of pσ in diffusion models and pr=σ
√
D in PFGM++
for ∀D>0. Such alignment enables directly transferring
the ﬁnely tuned hyperparameters σmax, p(σ) in previous
state-of-the-art diffusion models (Karras et al., 2022) with
rmax=σmax
√
D, p(r)=p(σ=r/
√
D)/
√
D. We put the prac-
tical hyperparameter transfer procedures in Appendix C.2.
We empirically verify the alignment formula on the CIFAR-
10 (Krizhevsky, 2009). Xu et al. (2023) shows that the pos-
terior p0|r(y|x) ∝pr(x|y)p(y) gradually grows towards
a uniform distribution from the near to the far ﬁeld. As a
result, the mean total variational distance (TVD) between a
uniform distribution and the posterior serves as an indicator
of the phase of pr: Epr(x)TVD
 U(·) ∥p0|r(·|x)

. Fig. 3
reports the mean TVD before and after the r=σ
√
D align-
ment. We observe that the mean TVDs of a wide range of
Ds take similar values after the alignment, suggesting that
the phases of pr=σ
√
D are roughly aligned for different Ds.
5. Balancing Robustness and Rigidity
In this section, we ﬁrst delve into the behaviors of PFGM++
with different Ds (Sec 5.1) based on the alignment formula.
Then we demonstrate how to leverage D to balance the
robustness and rigidity of models (Sec 5.2). We defer all
experimental details in this section to Appendix D.1.
5.1. Behavior of perturbation kernel when varying D
According to Theorem 4.1, when D→∞, the ﬁeld in
PFGM++ has the same direction as the score function,
i.e.,
√
DE(˜x)x/E(˜x)r=σ∇x log pσ=r/
√
D(x).
In addi-
tion to the theoretical analysis, we provide further em-
pirical study to characterize the convergence towards
diffusion models as D increases.
Fig. 4(a) reports
the average ℓ2 difference between the two quantities,
i.e., Epσ(x)[∥
√
DE(˜x)x/E(˜x)r−σ∇x log pσ(x)∥2] with
r=σ
√
D. We observe that the difference monotonically
decreases as a function of D, and converges to 0 as pre-
dicted by theory. For σ=1, the distance remains 0 since
the empirical posterior p0|r concentrates around a single
example for all D.
Next, we examine the behavior of the perturbation kernel
after the phase alignment. Recall that the isotropic per-
turbation kernel pr(x|y) ∝1/(∥x −y∥2
2 + r2)
N+D
2
can
be decomposed into a uniform angle component and a ra-
dius distribution pr(R) ∝RN−1/(R2 + r2)
N+D
2
. Fig. 4(b)
shows the variance of the radius distribution signiﬁcantly
decreases as D increases. The results imply that with rel-
atively large r, the norm of the training sample in pr(x)
becomes increasingly concentrated around a speciﬁc value
as D increases, reaching its highest level of concentration as
D→∞(diffusion models). Fig. 4(c) further shows the den-
sity of training sample norms in pr=σ
√
D(x) on CIFAR-10.
We can see that the range of the high-mass region gradually
shrinks when D increases.
5.2. Balancing the trade-off by controlling D
As noted in Xu et al. (2022), diffusion models (D→∞)
are more susceptible to estimation errors compared to
PFGM (D=1) due to the strong correlation between σ
and the training sample norm, as demonstrated in Fig. 4(c).
When D and r are large, the marginal distribution pr(x) is
approximately supported on the sphere with radius r
p
N/D.
The backward ODE can lead to unexpected results if the
sampling trajectories deviate from this norm-r relation
present in training samples. This phenomenon was em-
pirically conﬁrmed by Xu et al. (2022) for PFGM/diffusion
models (D=1 and D→∞cases) using a weaker architec-
ture NCSNv2 (Song & Ermon, 2020), where PFGM was
shown to be signiﬁcantly more robust than diffusion models.
Smaller D, however, implies a heavy-tailed input distribu-
tion. Fig. 4(c) illustrates that the examples used as the input
to the neural network have a broader range of norms when D
is small. In particular, when D<25, the variance of pertur-
bation radius can be larger than 210 (Fig. 4(b)). This broader
input range can be challenging for any ﬁnite-capacity neural
network. Although Xu et al. (2022) introduced heuristics
to bypass this issue in the D=1 case, e.g., restricting the
sampling/training regions, these heuristics also prevent the
sampling process from faithfully recovering the data distri-

PFGM++: Unlocking the Potential of Physics-Inspired Generative Models
20
25
210
215
220
D
0.00
0.25
0.50
0.75
1.00
1.25
1.50
Average 2 difference
1e
1
= 1
= 5
= 20
= 80
(a)
25
210
215
D
0
5
10
15
log2Varpr(R)
= 1
= 5
= 20
= 80
(b)
||x||2
1e3
0
1
2
3
4
5
6
7
8 10
20
40
80
pr =
D(||x||2)
D = 22
D = 26
D = 210
D = 214
D = 218
(c)
Figure 4. (a) Average ℓ2 difference between scaled electric ﬁeld and score function, versus D. (b) Log-variance of radius distribution
versus D. (c) Density of radius distributions pr=σ
√
D(R) with varying σ and D.
bution.
Thus, we can view D as a parameter to optimize so as to
balance the robustness of generation against rigidity that
helps learning. Increased robustness allows practitioners to
use smaller neural networks, e.g., by applying post-training
quantization (Han et al., 2015; Banner et al., 2018). In
other words, smaller D allows for more aggressive quantiza-
tion/larger sampling step sizes/smaller architectures. These
can be crucial in real-world applications where computa-
tional resources and storage are limited. On the other hand,
such gains need to be balanced against easier training af-
forded by larger values of D. The ability to optimize the
balance by varying D can be therefore advantageous. We
expect that there exists a sweet spot of D in the middle
striking the balance, as the model robustness and rigidity go
in opposite directions.
6. Experiments
In this section, we assess the performance of different gen-
erative models on image generation tasks (Sec 6.1), where
models with some median Ds outperform previous state-
of-the-art diffusion models (D→∞), consistent with the
sweet spot argument in Sec 5. We also demonstrate the
improved robustness against three kinds of error as D de-
creases (Sec 6.2).
6.1. Image generation
We consider the widely used benchmarks CIFAR-10
32×32 (Krizhevsky, 2009) and FFHQ 64×64 (Karras et al.,
2018) for image generation. For training, we utilize the im-
proved NCSN++/DDPM++ architectures, preconditioning
techniques and hyperparameters from the state-of-the-art
diffusion model EDM (Karras et al., 2022). Speciﬁcally,
we use the alignment method developed in Sec 4 to transfer
their tuned critical hyperparameters σmax, σmin, p(σ) in the
D→∞case to ﬁnite D cases. According to the experimen-
tal results in Karras et al. (2018), the log-normal training
distribution p(σ) has the most substantial impact on the ﬁnal
performances. For ODE solver during sampling, we use
Heun’s 2nd method (Ascher & Petzold, 1998) as in EDM.
Table 1. CIFAR-10 sample quality (FID) and number of function
evaluations (NFE).
Min FID ↓
Top-3 Avg FID ↓
NFE ↓
DDPM (Ho et al., 2020)
3.17
-
1000
DDIM (Song et al., 2021a)
4.67
-
50
VE-ODE (Song et al., 2021b)
5.29
-
194
VP-ODE (Song et al., 2021b)
2.86
-
134
PFGM (Xu et al., 2022)
2.48
-
104
PFGM++ (unconditional)
D = 64
1.96
1.98
35
D = 128
1.92
1.94
35
D = 2048
1.91
1.93
35
D = 3072000
1.99
2.02
35
D →∞(Karras et al., 2022)
1.98
2.00
35
PFGM++ (class-conditional)
D = 2048
1.74
-
35
D →∞(Karras et al., 2022)
1.79
-
35
Table 2. FFHQ sample quality (FID) with 79 NFE in unconditional
setting
Min FID ↓
Top-3 Avg FID ↓
D = 128
2.43
2.48
D = 2048
2.46
2.47
D = 3072000
2.49
2.52
D →∞(Karras et al., 2022)
2.53
2.54
We compare models trained with D→∞(EDM) and
D∈{64, 128, 2048, 3072000}. In our experiments, we ex-
clude the case of D=1 (PFGM) because the perturbation
kernel is extremely heavy-tailed (Fig. 4(b)), making it difﬁ-
cult to integrate with our perturbation-based objective with-
out the restrictive region heuristics proposed in Xu et al.
(2022). We also exclude the small D = 64 for the higher-
resolution dataset FFHQ. We include several popular gen-
erative models for reference and defer more training and
sampling details to Appendix D.
Results: In Table 1 and Table 2, we report the sample qual-
ity measured by the FID score (Heusel et al., 2017) (lower
is better), and inference speed measured by the number

PFGM++: Unlocking the Potential of Physics-Inspired Generative Models
of function evaluations. As in EDM, we report the min-
imum FID score over checkpoints. Since we empirically
observe a large variation of FID scores on FFHQ across
checkpoints (Appendix D.4), we also use the average FID
score over the Top-3 checkpoints as another metric. Our
main ﬁndings are (1) Median Ds outperform diffusion
models (D→∞) under PFGM++ framework. We ob-
serve that the D=2048/128 cases achieve the best perfor-
mance among our choices on CIFAR-10 and FFHQ, with
a current state-of-the-art min FID score of 1.91/2.43 in
unconditional setting, using the perturbation-based objec-
tive. In addition, the D=2048 case obtain better Top-3 aver-
age FID scores (1.93/2.47) than EDM (2.00/2.54) on both
datasets in unconditional setting, and achieve current state-
of-the-art FID score of 1.74 on CIFAR-10 class-conditional
setting. (2) There is a sweet spot between (1, ∞). Nei-
ther small D nor inﬁnite D obtains the best performance,
which conﬁrms that there is a sweet spot in the middle,
well-balancing rigidity and robustness. (3) Model with
D≫N recovers diffusion models. We ﬁnd that model
with sufﬁciently large D roughly matches the performance
of diffusion models, as predicted by the theory. Further re-
sults in Appendix E.1 show that D=3072000 and diffusion
models obtain the same FID score when using a more stable
training target (Xu et al., 2023) to mitigate the variations
between different runs and checkpoints.
6.2. Model robustness versus D
0.0
0.1
0.2
0.3
0.4
0
50
100
150
200
250
300
FID Score
D = 64
D = 128
D = 2048
D
 (Diffusion)
20
25
30
35
NFE
2.0
2.2
2.4
2.6
2.8
D = 64
D = 128
D = 2048
D
 (Diffusion)
Figure 5. FID score versus (left) α and (right) NFE on CIFAR-10.
In Section 5, we show that the model robustness degrades
with an increasing D by analyzing the behavior of pertur-
bation kernels. To further validate the phenomenon, we
conduct three sets of experiments with different sources of
errors on CIFAR-10. We defer more details to Appedix D.5.
Firstly, we perform controlled experiments to compare the
robustness of models quantitatively. To simulate the errors,
we inject noise into the intermediate point xr in each of the
35 ODE steps: xr = xr + αϵr where ϵr ∼N(0, r/
√
DI),
and α is a positive number controlling the amount of
noise.
Fig. 5(a) demonstrates that as α increases, FID
score exhibits a much slower degradation for smaller D.
In particular, when D=64, 128, the sample quality degrades
gracefully. We further visualize the generated samples in
Appendix E.2. It shows that when α=0.2, models with
D=64, 128 can still produce clean images while the sam-
pling process of diffusion models (D→∞) breaks down.
In addition to the controlled scenario, we conduct two more
realistic experiments: (1) We introduce more estimation
error of neural networks by applying post-training quan-
tization (Sung et al., 2015), which can directly compress
neural networks without ﬁne-tuning. Table 3 reports the FID
score with varying quantization bit-widths for the convolu-
tion weight values. We can see that ﬁnite Ds have better
robustness than the inﬁnite case, and a lower D exhibits
a larger performance gain when applying lower bit-widths
quantization. (2) We increase the discretization error dur-
ing sampling by using smaller NFEs, i.e., larger sample
steps. As shown in Fig. 5(b), gaps between D=128 and
diffusion models gradually widen, indicating greater robust-
ness against the discretization error. The rigidity issue of
smaller D also affects the robustness to discretization error,
as D=64 is consistently inferior to D=128.
Table 3. FID score versus quantization bit-widths on CIFAR-10.
Quantization bits:
9
8
7
6
5
D = 64
1.96
1.96
2.12
2.94
28.50
D = 128
1.93
1.97
2.15
3.68
34.26
D = 2048
1.91
1.97
2.12
5.67
47.02
D →∞
1.97
2.04
2.16
5.91
50.09
7. Conclusion and Future Directions
We present a new family of physics-inspired generative
models called PFGM++, by extending the dimensionality
of augmented variable in PFGM from 1 to D ∈R+. Re-
markably, PFGM++ includes diffusion models as special
cases when D→∞. To address issues related to large batch
training, we propose a perturbation-based objective. In addi-
tion, we show that D effectively controls the robustness and
rigidity in the PFGM++ family. Empirical results show that
models with ﬁnite values of D can perform better than previ-
ous state-of-the-art diffusion models, while also exhibiting
improved robustness.
There are many potential avenues for future research in the
PFGM++ framework. For example, it may be possible to
identify the “sweet spot” value of D for different architec-
tures and tasks by analyzing the behavior of errors. Since
PFGM++ enables adjusting robustness, another direction is
to apply aggressive network compression techniques, i.e.,
pruning and low-bit training, to smaller D. Furthermore,
there may be opportunities to develop stochastic samplers
for PFGM++, with the reverse SDE in diffusion models as
a special case. Lastly, as diffusion models have been highly
optimized for image generation, the PFGM++ framework
may show a greater advantage over its special case (diffusion
models) in emergent ﬁelds, such as biology data.

PFGM++: Unlocking the Potential of Physics-Inspired Generative Models
Acknowledgements
YX and TJ acknowledge support from MIT-DSTA Singa-
pore collaboration, from NSF Expeditions grant (award
1918839) “Understanding the World Through Code”, and
from MIT-IBM Grand Challenge project. ZL and MT would
like to thank the Center for Brains, Minds, and Machines
(CBMM) for hospitality. ZL and MT are supported by The
Casey and Family Foundation, the Foundational Questions
Institute, the Rothberg Family Fund for Cognitive Science
and IAIFI through NSF grant PHY-2019786. ST and TJ
also acknowledge support from the ML for Pharmaceutical
Discovery and Synthesis Consortium (MLPDS).
References
Arjovsky, M., Chintala, S., and Bottou, L. Wasserstein gen-
erative adversarial networks. In International Conference
on Machine Learning, 2017.
Ascher, U. M. and Petzold, L. R. Computer methods for
ordinary differential equations and differential-algebraic
equations. 1998.
Banner, R., Nahshan, Y., and Soudry, D. Post training
4-bit quantization of convolutional networks for rapid-
deployment. In Neural Information Processing Systems,
2018.
Brock, A., Donahue, J., and Simonyan, K. Large scale gan
training for high ﬁdelity natural image synthesis. ArXiv,
abs/1809.11096, 2019.
Chen, N., Zhang, Y., Zen, H., Weiss, R. J., Norouzi, M., and
Chan, W. Wavegrad: Estimating gradients for waveform
generation. ArXiv, abs/2009.00713, 2020.
Choi, J., Lee, J., Shin, C., Kim, S., Kim, H., and Yoon, S.
Perception prioritized training of diffusion models. In
Proceedings of the IEEE/CVF Conference on Computer
Vision and Pattern Recognition, pp. 11472–11481, 2022.
Grifﬁths, D. J. Introduction to electrodynamics, 2005.
Han, S., Mao, H., and Dally, W. J. Deep compression:
Compressing deep neural network with pruning, trained
quantization and huffman coding. arXiv: Computer Vi-
sion and Pattern Recognition, 2015.
Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., and
Hochreiter, S. Gans trained by a two time-scale update
rule converge to a local nash equilibrium. In NIPS, 2017.
Ho, J., Jain, A., and Abbeel, P. Denoising diffusion proba-
bilistic models. ArXiv, abs/2006.11239, 2020.
Jarzynski, C. Equilibrium free-energy differences from
nonequilibrium measurements: A master-equation ap-
proach. Physical Review E, 56:5018–5035, 1997.
Karras, T., Laine, S., and Aila, T. A style-based generator
architecture for generative adversarial networks. 2019
IEEE/CVF Conference on Computer Vision and Pattern
Recognition (CVPR), pp. 4396–4405, 2018.
Karras, T., Aittala, M., Aila, T., and Laine, S. Elucidating
the design space of diffusion-based generative models.
ArXiv, abs/2206.00364, 2022.
Kong, Z., Ping, W., Huang, J., Zhao, K., and Catanzaro, B.
Diffwave: A versatile diffusion model for audio synthesis.
ArXiv, abs/2009.09761, 2020.
Krizhevsky, A. Learning multiple layers of features from
tiny images. 2009.
Nichol, A., Dhariwal, P., Ramesh, A., Shyam, P., Mishkin,
P., McGrew, B., Sutskever, I., and Chen, M. Glide: To-
wards photorealistic image generation and editing with
text-guided diffusion models. In ICML, 2022a.
Nichol, A., Jun, H., Dhariwal, P., Mishkin, P., and Chen, M.
Point-e: A system for generating 3d point clouds from
complex prompts. ArXiv, abs/2212.08751, 2022b.
Poole, B., Jain, A., Barron, J. T., and Mildenhall, B.
Dreamfusion: Text-to-3d using 2d diffusion.
ArXiv,
abs/2209.14988, 2022.
Ramesh, A., Dhariwal, P., Nichol, A., Chu, C., and Chen,
M. Hierarchical text-conditional image generation with
clip latents. ArXiv, abs/2204.06125, 2022.
Rombach, R., Blattmann, A., Lorenz, D., Esser, P., and
Ommer, B.
High-resolution image synthesis with la-
tent diffusion models. 2022 IEEE/CVF Conference on
Computer Vision and Pattern Recognition (CVPR), pp.
10674–10685, 2021.
Saharia, C., Chan, W., Saxena, S., Li, L., Whang, J., Den-
ton, E. L., Ghasemipour, S. K. S., Ayan, B. K., Mah-
davi, S. S., Lopes, R. G., Salimans, T., Ho, J., Fleet,
D. J., and Norouzi, M. Photorealistic text-to-image dif-
fusion models with deep language understanding. ArXiv,
abs/2205.11487, 2022.
Shi, C., Luo, S., Xu, M., and Tang, J. Learning gradient
ﬁelds for molecular conformation generation. In ICML,
2021.
Sohl-Dickstein, J., Weiss, E., Maheswaranathan, N., and
Ganguli, S. Deep unsupervised learning using nonequi-
librium thermodynamics. In International Conference on
Machine Learning, pp. 2256–2265. PMLR, 2015.
Song, J., Meng, C., and Ermon, S. Denoising diffusion
implicit models. ArXiv, abs/2010.02502, 2021a.
Song, Y. and Ermon, S. Improved techniques for training
score-based generative models. ArXiv, abs/2006.09011,
2020.

PFGM++: Unlocking the Potential of Physics-Inspired Generative Models
Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A.,
Ermon, S., and Poole, B. Score-based generative mod-
eling through stochastic differential equations. ArXiv,
abs/2011.13456, 2021b.
Sung, W., Shin, S., and Hwang, K. Resiliency of deep neural
networks under quantization. ArXiv, abs/1511.06488,
2015.
Vincent, P. A connection between score matching and de-
noising autoencoders. Neural Computation, 23:1661–
1674, 2011.
Watson, J. L., Juergens, D., Bennett, N. R., Trippe, B. L.,
Yim, J., Eisenach, H. E., Ahern, W., Borst, A. J., Ragotte,
R. J., Milles, L. F., Wicky, B. I. M., Hanikel, N., Pellock,
S. J., Courbet, A., Shefﬂer, W., Wang, J., Venkatesh, P.,
Sappington, I., Torres, S. V., Lauko, A., Bortoli, V. D.,
Mathieu, E., Barzilay, R., Jaakkola, T., DiMaio, F., Baek,
M., and Baker, D. Broadly applicable and accurate pro-
tein design by integrating structure prediction networks
and diffusion generative models. bioRxiv, 2022.
Xiao, Z., Kreis, K., and Vahdat, A. Tackling the generative
learning trilemma with denoising diffusion GANs. In
International Conference on Learning Representations,
2022. URL https://openreview.net/forum?
id=JprM0p-q0Co.
Xu, Y., Liu, Z., Tegmark, M., and Jaakkola, T. Poisson ﬂow
generative models. ArXiv, abs/2209.11178, 2022.
Xu, Y., Tong, S., and Jaakkola, T. Stable target ﬁeld for
reduced variance score estimation in diffusion models.
ArXiv, abs/2302.00670, 2023.
Zeng, X., Vahdat, A., Williams, F., Gojcic, Z., Litany, O.,
Fidler, S., and Kreis, K. Lion: Latent point diffusion
models for 3d shape generation. ArXiv, abs/2210.06978,
2022.

PFGM++: Unlocking the Potential of Physics-Inspired Generative Models
Appendix
A. Proofs
A.1. Proof of Theorem 3.1
Theorem 3.1. Assume the data distribution p ∈C1 and p has compact support. As rmax→∞, for D ∈R+, the ODE
dx/dr = E(˜x)x/E(˜x)r deﬁnes a bijection between limrmax→∞prmax(x) ∝limrmax→∞rD
max/(∥x∥2
2 + r2
max)
N+D
2
when
r = rmax and the data distribution p when r = 0.
Proof. Let qr(x) ∝
R
rD/∥˜x −˜y∥N+Dp(y)dy.
We will show that qr ∝
R
rD/∥˜x −˜y∥N+Dp(y)dy is equal to
the r-dependent marginal distribution pr by verifying (1) the starting distribution is correct when r=0; (2) the con-
tinuity equation holds, i.e., ∂rqr + ∇x · (qrE(˜x)x/E(˜x)r) = 0.
The starting distribution is limr→0 qr(x) ∝
limr→0
R
rD/∥˜x −˜y∥N+Dp(y)dy ∝p(x), which conﬁrms that qr=p. The continuity equation can be expressed as:
∂rqr + ∇x · (qrE(˜x)x/E(˜x)r)
= ∂r
Z
rD
∥˜x −˜y∥N+D p(y)dy

+ ∇x ·
 Z
rD
∥˜x −˜y∥N+D p(y)dy
R
˜x−˜y
∥˜x−˜y∥N+D p(y)dy
R
r
∥˜x−˜y∥N+D p(y)dy
!
=
Z 
DrD−1
∥˜x −˜y∥N+D −
(N + D)r
∥˜x −˜y∥N+D−2

p(y)dy + ∇x ·

rD−1
Z
˜x −˜y
∥˜x −˜y∥N+D p(y)dy

=
Z 
DrD−1
∥˜x −˜y∥N+D −
(N + D)r
∥˜x −˜y∥N+D−2

p(y)dy + ∇x ·

rD−1
Z
˜x −˜y
∥˜x −˜y∥N+D p(y)dy

=
Z 
DrD−1
∥˜x −˜y∥N+D −
(N + D)r
∥˜x −˜y∥N+D−2

p(y)dy
+ rD−1
N
X
i=1
Z ∥˜x −˜y∥N+D −∥˜x −˜y∥N+D−2(xi −yi)2(N + D)
∥˜x −˜y∥2(N+D)
p(y)dy
=
Z 
DrD−1
∥˜x −˜y∥N+D −(N + D)rD+1
∥˜x −˜y∥N+D−2

p(y)dy
+ rD−1
Z N∥˜x −˜y∥N+D −∥˜x −˜y∥N+D−2∥x −y∥2(N + D)
∥˜x −˜y∥2(N+D)
p(y)dy
= rD−1
Z ∥˜x−˜y∥N+DD −(N+D)r2∥˜x −˜y∥N+D−2 + N∥˜x−˜y∥N+D −∥˜x−˜y∥N+D−2∥x−y∥2(N+D)
∥˜x−˜y∥2(N+D)
p(y)dy
= rD−1
Z (N + D)(∥˜x −˜y∥N+D −∥˜x −˜y∥N+D−2∥x −y∥2) −(N + D)r2∥˜x −˜y∥N+D−2
∥˜x −˜y∥2(N+D)
p(y)dy
= rD−1
Z (N + D)r2∥˜x −˜y∥N+D−2 −(N + D)r2∥˜x −˜y∥N+D−2
∥˜x −˜y∥2(N+D)
p(y)dy
= 0
It means that qr satisﬁes the continuity equation for any r ∈R≥0. Together, we conclude that qr = pr. Lastly, note that the
terminal distribution is
lim
rmax→∞prmax(x) ∝
lim
rmax→∞
Z
rD
max
∥˜x −˜y∥N+D p(y)dy =
lim
rmax→∞
Z
rD
max
(∥x −y∥2 + r2max)
N+D
2
p(y)dy
=
lim
rmax→∞
rD
max
(∥x∥2 + r2max)
N+D
2
+
lim
rmax→∞
Z  
rD
max
(∥x −y∥2 + r2max)
N+D
2
−
rD
max
(∥x∥2 + r2max)
N+D
2
!
p(y)dy
=
lim
rmax→∞
rD
max
(∥x∥2 + r2max)
N+D
2
(p has a compact support)

PFGM++: Unlocking the Potential of Physics-Inspired Generative Models
A.2. Proof of Theorem 3.2
Proposition A.1. With perturbation kernel pr(x|y) ∝1/(∥x −y∥2
2 + r2)
N+D
2
, for ∀x ∈RN, r > 0, the minimizer
f ∗
θ (˜x) in the PFGM++ objective (Eq. (5)) matches the direction of electric ﬁeld E(˜x) in Eq. (3). Speciﬁcally, f ∗
θ (˜x) =
(SN+D−1(1)/pr(x))E(˜x).
Proof. The minimizer at ˜x in Eq. (5) is
f ∗
θ (˜x) =
Z
pr(y|x)(˜x −˜y)d˜y =
R
pr(x|y)(˜x −˜y)p(y)dy
pr(x)
(9)
The choice of perturbation kernel is
pr(x|y) ∝
1
∥˜x −˜y∥N+D =
1
(∥x −y∥2
2 + r2)
N+D
2
By substituting the perturbation kernel in Eq. (9), we have:
f ∗
θ (˜x) =
R
˜x−˜y
(∥x−y∥2
2+r2)
N+D
2
p(y)dy
pr(x)
=
R
˜x−˜y
∥˜x−˜y∥2N+D p(y)dy
pr(x)
= (SN+D−1(1)/pr(x))E(˜x)
A.3. Proof of Theorem 4.1
Theorem 4.1. Assume the data distribution p ∈C1. Consider taking the limit D →∞while holding σ = r/
√
D ﬁxed.
Then, for all x,
lim
D→∞
r=σ
√
D

√
D
E(˜x)r
E(˜x)x −σ∇x log pσ=r/
√
D(x)

2
= 0
where E(˜x = (x, r))x is given in Eq. (3).
Further, given the same initial point, the trajectory of the PFGM++
ODE (dx/dr=E(˜x)x/E(˜x)r) matches the diffusion ODE (Karras et al., 2022) (dx/dt= −˙σ(t)σ(t)∇x log pσ(t)(x))
in the same limit.
Proof. The x component in the Poisson ﬁeld can be re-expressed as
E(˜x)x =
1
SN+D−1(1)
Z
x −y
∥˜x −˜y∥N+D p(y)dy
∝
Z
pr(x|y)(x −y)p(y)dy
where the perturbation kernel pr(x|y) ∝1/(∥x −y∥2
2 + r2)
N+D
2
. The direction of the score can also be written down in a
similar form:
∇x log pσ(x) =
R
pσ(x|y) y−x
σ2 p(y)dy
pσ(x)
∝
Z
pσ(x|y)(x −y)p(y)dy
where pσ(x|y) ∝exp −∥x−y∥2
2
2σ2
. Since p ∈C1, and obviously pr(x|y) ∈C1, then limD→∞
R
pr(x|y)(x −y)p(y)dy =
R
limD→∞pr(x|y)(x −y)p(y)dy. It sufﬁces to prove that the perturbation kernel pr(x|y) point-wisely converge to the

PFGM++: Unlocking the Potential of Physics-Inspired Generative Models
Gaussian kernel pσ(x|y), i.e., limD→∞pr(x|y) = pσ(x|y), to ensure E(x)x ∝∇x log pσ(x). Given ∀x, y ∈RN,
lim
D→∞pr(x|y) ∝lim
D→∞
1
(∥x −y∥2
2 + r2)
N+D
2
= lim
D→∞(∥x −y∥2
2 + r2)−N+D
2
∝lim
D→∞(1 + ∥x −y∥2
2
r2
)−N+D
2
= lim
D→∞(1 + ∥x −y∥2
2
Dσ2
)−N+D
2
(r = σ
√
D)
= lim
D→∞exp

−N + D
2
ln(1 + ∥x −y∥2
2
Dσ2
)

= lim
D→∞exp

−N + D
2
∥x −y∥2
2
Dσ2

( limD→∞
∥x−y∥2
2
Dσ2
= 0)
= exp −∥x −y∥2
2
2σ2
∝pσ(x|y)
Hence limD→∞pr(x|y) = pσ(x|y), and we establish that E(˜x)x ∝∇x log pσ(x). We can rewrite the drift term in the
PFGM++ ODE as
lim
D→∞
r=σ
√
D
√
DE(˜x)x/E(˜x)r =
lim
D→∞
r=σ
√
D
√
D
R
pr(x|y)(x −y)p(y)dy
R
pr(x|y)(−r)p(y)dy
=
lim
D→∞
r=σ
√
D
√
D
R
pr(x|y)(y −x)p(y)dy
rpr(x)
=
lim
D→∞
r=σ
√
D
√
D
R
pσ(x|y)(y −x)p(y)dy
rpσ(x)
= σ∇x log pσ(x)
(∇x log pσ(x) =
R
pσ(x|y) y−x
σ2 p(y)dy
pσ(x)
)
(10)
which establishes the ﬁrst part of the theorem. For the second part, by the change-of-variable dσ = dr/
√
D, the PFGM++
ODE is
lim
D→∞
r=σ
√
D
dx
dσ = dx
dr · dr
dσ
=
lim
D→∞
r=σ
√
D
E(˜x)x · E(˜x)−1
r
·
√
D
=
lim
D→∞
r=σ
√
D
σ∇x log pσ(x)
√
D
·
√
D
(by Eq. (10))
= σ∇x log pσ(x)
which is equivalent to the diffusion ODE.
A.4. Proof of Proposition 4.2
Proposition A.2. When r = σ
√
D, D →∞, the minimizer in the PFGM++ objective (Eq. (6)) is equaivalent to the
minimizer in the weighted sum of denoising score matching objective (Eq. (8))

PFGM++: Unlocking the Potential of Physics-Inspired Generative Models
Proof. For ∀x ∈RN, the minimizer in PFGM++ objective (Eq. (6)) at point ˜x = (x, r) is
f ∗
θ,PFGM++(˜x) =
lim
D→∞
r=σ
√
D
R
pr(x|y) x−y
r/
√
Dp(y)dy
pr(x)
=
lim
D→∞
r=σ
√
D
R
pσ(x|y) x−y
r/
√
Dp(y)dy
pσ(x)
(By Theorem 4.1, limD→∞pr(x|y) = pσ(x|y))
=
R
pσ(x|y) x−y
σ p(y)dy
pσ(x)
(11)
On the other hand, the minimizer in denoising score matching at point x in noise level σ = r/
√
N + D is
f ∗
θ,DSM(x, σ) =
R
pσ(x|y) x−y
σ p(y)dy
pσ(x)
(12)
Combining Eq. (11) and Eq. (12), we have
lim
D→∞
r=σ
√
D
f ∗
θ,PFGM++(x, σ
√
N + D) = f ∗
θ,DSM(x, σ)
B. Practical Sampling Procedures of Perturbation Kernel and Prior Distribution
In this section, we discuss how to simple from the perturbation kernel pr(x|y) ∝1/(∥x −y∥2
2 + r2)
N+D
2
in practice. We
ﬁrst decompose pr(·|y) in hyperspherical coordinates to Uψ(ψ)pr(R), where Uψ is the uniform distribution over the angle
component and the distribution of the perturbed radius R = ∥x −y∥2 is
pr(R) ∝
RN−1
(R2 + r2)
N+D
2
(13)
The sampling procedure of the radius distribution encompasses three steps:
R1 ∼Beta(α = N
2 , β = D
2 )
R2 =
R1
1 −R1
R3 =
p
r2R2
Next, we prove that p(R3) = pr(R3). Note that the pdf of the inverse beta distribution is
p(R2) ∝R
N
2 −1
2
(1 + R2)−N+D
2

PFGM++: Unlocking the Potential of Physics-Inspired Generative Models
By change-of-variable, the pdf of R3 =
p
r2maxR2 is
p(R3) ∝R
N
2 −1
2
(1 + R2)−N
2 −D
2 ∗2R3
r2max
∝
R3R
N
2 −1
2
(1 + R2)
N+D
2
=
(R3/r)N−1
(1 + (R2
3/r2))
N+D
2
∝
RN−1
3
(1 + (R2
3/r2))
N+D
2
∝
RN−1
3
(r2 + R2
3)
N+D
2
∝pr(R3)
(By Eq. (13))
Note that R1 has mean
N
N+D and variance O(
ND
(N+D)3 ). Hence when D = O(N), pr(R) would highly concentrate on a
speciﬁc value, resolving the heavy-tailed problem. We can sample the uniform angel component by u = w/∥w∥, w ∼
N(0, IN×N). Together, sampling from the perturbation kernel pr(x|y) is equivalent to setting x = y + R3u. On the other
hand, the prior distribution is
prmax(x) ∝
lim
rmax→∞
Z
rD
max/∥˜x −˜y∥N+Dp(y)dy =
lim
rmax→∞rD
max/(∥x∥2 + r2
max)
N+D
2
We observe that prmax(x) the same as the perturbation kernel prmax(x|y = 0). Hence we can sample from the prior following
x = R3u with R3, u deﬁned above and r = rmax.
C. r = σ
√
D for Phase Alignment
C.1. Analysis
In this section, we examine the phase of intermediate marginal distribution pr under different Ds to derive an alignment
method for hyper-parameters. Consider a N-dimensional dataset D in which the average distance to the nearest neighbor is
about l. We consider an arbitrary datapoint x1 ∈D and denote its nearest neighbor as x2. We assume ∥x1 −x2∥2 = l, and
uniform prior on D.
To characterize the phases of pr, ∀r > 0, we study the perturbation point y ∼pr(y|x1). According to Appendix B, the
distance ∥x1−y∥is roughly r
q
N
D−1. Since pr(y|x1) is isotropic, with high probability, the two vectors y−x1, x2−x1 are
approximately orthogonal. In particular, the vector product (y −x1)T (x1 −x2) = O(
1
√
N ∥y −x1∥∥x1 −x2∥) = O( rl
√
D)
w.h.p. It reveals that ∥y −x2∥=
q
l2 + r2
N
D−1 + O( rl
√
D). Fig. 6 depicts the relative positions of x1, x2 and the perturbed
point y.
The ratio of the posterior of the x2 and x1 — pr(x2|y)
pr(x1|y) — is an indicator of different phases of ﬁeld (Xu et al., 2023): point
in the nearer ﬁeld tends to have a smaller ratio. Indeed, the ratio would gradually decay from 1 to 0 when moving from the

PFGM++: Unlocking the Potential of Physics-Inspired Generative Models
Figure 6. Illustration of the phase alignment analysis
far to the near ﬁeld. We can calculate the ratio of the coefﬁcients after approximating the distance ∥y −x2∥:
pr(x2|y)
pr(x1|y) = pr(y|x2)
pr(y|x1) =
 l2 + r2
N
D−1 + O( rl
√
D) + r2
r2
N
D−1 + r2
! N+D
2
=
 
1 +
l2 + O( rl
√
D)
r2
N
D−1 + r2
! N+D
2
= exp
 
ln(1 +
l2 + O( rl
√
D)
r2
N
D−1 + r2 ) · N + D
2
!
≈exp
 l2 + O( rl
√
D)
r2
N
D−1 + r2 · N + D
2
!
= exp
 l2 + O( rl
√
D)
r2
·
N + D
2(N + D −1) · (D −1)
!
≈exp
 l2 + O( rl
√
D)
r2
· D
!
(14)
Hence the relation r ∝
√
D should hold to keep the ratio invariant of the parameter D. On the other hand, by Theorem 4.1
we know that pσ is equivalent to pr=σ
√
D when D →∞. To achieve phase alignment on the dataset, one should roughly set
r = σ
√
D.
C.2. Practical Hyperparameter Transfer from Diffusion Models
C.2.1. TRANSFER EDM TRAINING AND SAMPLING
We list out and compare the EDM training algorithm (Alg 1) and the PFGM++ with transferred hyper-parameters (Alg 2).
The major modiﬁcation is to replace the Gaussian noise ni ∼N(0, σ2I) with the additive noise Rivi ∼Uψ(ψ)pr(R),
where r = σ
√
D. We highlight the major modiﬁcations in blue.
We also show the sampling algorithms of EDM (Alg 3) and PFGM++ (Alg 4). Note that we only change the prior
sampling process while the for-loop is identical for both algorithms, since EDM (Karras et al., 2022) sets σ = t, and
dx
dr = x−fθ(x,r)
r
= x−fθ(x,r)
σ
√
D
=
dx
√
Ddσ = dx
dσ
dσ
dr = dx
dσ = dx
dt . Thus we can use the original samplers of EDM without
further modiﬁcation.

PFGM++: Unlocking the Potential of Physics-Inspired Generative Models
Algorithm 1 EDM training
1: Sample a batch of data {yi}B
i=1 from p(y)
2: Sample standard deviations {σi}B
i=1 from p(σ)
3: Sample noise vectors {ni ∼N(0, σ2
i I)}B
i=1
4: Get perturbed data {ˆyi = yi + ni}B
i=1
5: Calculate loss ℓ(θ) = PB
i=1 λ(σi)∥fθ(ˆyi, σi)−yi∥2
2
6: Update the network parameter θ via Adam optimizer
Algorithm 2 PFGM++ training with hyperparameter trans-
ferred from EDM
1: Sample a batch of data {yi}B
i=1 from p(y)
2: Sample standard deviations {σi}B
i=1 from p(σ)
3: Sample r from pr: {ri = σi
√
D}B
i=1
4: Sample radiuses {Ri ∼pri(R)}B
i=1
5: Sample uniform angles {vi =
ui
∥ui∥2 }B
i=1, with ui ∼
N(0, I)
6: Get perturbed data {ˆyi = yi + Rivi}B
i=1
7: Calculate loss ℓ(θ) = PB
i=1 λ(σi)∥fθ(ˆyi, σi) −yi∥2
2
8: Update the network parameter θ via Adam optimizer
Algorithm 3 EDM sampling (Heun’s 2nd order method)
1: x0 ∼N(0, σ2
maxI)
2: for i = 0, . . . , T −1 do
3:
di = (xi −fθ(xi, ti))/ti
4:
xi+1 = xi + (ti+1 −ti)di
5:
if ti+1 > 0 then
6:
d′
i = (xi+1 −fθ(xi+1, ti+1))/ti+1
7:
xi+1 = xi + (ti+1 −ti)( 1
2di + 1
2d′
i)
8:
end if
9: end for
Algorithm 4 PFGM++ training with hyperparameter trans-
ferred from EDM
1: Set rmax = σmax
√
D
2: Sample radius R ∼prmax(R) and uniform angle v =
u
∥u∥2 ,
with u ∼N(0, I)
3: Get initial data x0 = Rv
4: for i = 0, . . . , T −1 do
5:
di = (xi −fθ(xi, ti))/ti
6:
xi+1 = xi + (ti+1 −ti)di
7:
if ti+1 > 0 then
8:
d′
i = (xi+1 −fθ(xi+1, ti+1))/ti+1
9:
xi+1 = xi + (ti+1 −ti)( 1
2di + 1
2d′
i)
10:
end if
11: end for
C.2.2. TRANSFER DDPM (CONTINUOUS) TRAINING AND SAMPLING
Here we demonstrate the “zero-shot” transfer of hyperparameters from DDPM to PFGM++, using the r = σ
√
D formula.
We highlight the modiﬁcations in blue. In particular, we list the DDPM training/sampling algorithms (Alg 5/Alg 7), and
their counterparts in PFGM++ (Alg 6/Alg 8) for comparions. Let βT and β1 be the maximum/minimum values of β in
DDPM (Ho et al., 2020). Similar to Song et al. (2021b), we denote αt = e−1
2 t2( ¯βmax−¯βmin)−t ¯βmin, with ¯βmax = βT · T and
¯βmin = β1 · T. For example, on CIFAR-10, ¯βmin = 1e −1 and ¯βmax = 20 with T = 1000. We would like to note that the tis
in the sampling algorithms (Alg 7 and Alg 8) monotonically decrease from 1 to 0 as i increases.
Algorithm 5 DDPM training
1: Sample a batch of data {yi}B
i=1 from p(y)
2: Sample time {ti=t′
i/T}B
i=1 with t′
i∼U({1, . . . , T})
3: Get perturbed data {ˆyi = √αtiyi+√1 −αtiϵi}B
i=1,
where ϵi ∼N(0, I)
4: Calculate loss ℓ(θ) = PB
i=1 λ(ti)∥fθ(ˆyi, ti) −ϵi∥2
2
5: Update the network parameter θ via Adam optimizer
Algorithm 6 PFGM++ training with hyperparameter trans-
ferred from DDPM
1: Sample a batch of data {yi}B
i=1 from p(y)
2: Sample time {ti}B
i=1 from U[0, 1]
3: Get σi from ti: {σi =
q 1−αti
αti }
4: Sample r from pr: {ri = σi
√
D}B
i=1
5: Sample radiuses {Ri ∼pri(R)}B
i=1
6: Sample uniform angles {vi =
ui
∥ui∥2 }B
i=1, with ui ∼
N(0, I)
7: Get perturbed data {ˆyi = √αti(yi + Rivi)}B
i=1
8: Calculate loss ℓ(θ) = PB
i=1 λ(ti)∥fθ(ˆyi, ti)−
√
DRivi
r
∥2
2
9: Update the network parameter θ via Adam optimizer

PFGM++: Unlocking the Potential of Physics-Inspired Generative Models
Algorithm 7 DDIM sampling
1: xT ∼N(0, I)
2: for i = T, . . . , 1 do
3:
xi−1 =
q αti−1
αti xi
+(p1 −αti−1−
q αti−1
αti
√1 −αti)fθ(xi, ti)
4: end for
Algorithm 8 PFGM++ sampling transferred from DDIM
1: Set σmax =
q
1−α1
α1 , rmax = σmax
√
D
2: Sample radius R ∼prmax(R) and uniform angle v =
u
∥u∥2 ,
with u ∼N(0, I)
3: Get initial data xT = √α1Rv
4: for i = T, . . . , 1 do
5:
xi−1 =
q αti−1
αti xi
+(p1 −αti−1−
q αti−1
αti
√1 −αti)fθ(xi, ti)
6: end for
D. Experimental Details
We show the experimental setups in section 5, as well as the training, sampling, and evaluation details for PFGM++. All the
experiments are run on four NVIDIA A100 GPUs or eight NVIDIA V100 GPUs.
D.1. Experiments for the Analysis in Sec 5
In the experiments of section 4 and section 5.1, we need to access the posterior p0|r(y|x) ∝pr(x|y)p(y) to calculate the
mean TVD. We sample a large batch {yi}n
i=1 with n = 1024 on CIFAR-10 to empirically approximate the posterior:
p0|r(yi|x) = pr(x|yi)p(yi)
pr(x)
≈
pr(x|yi)
Pn
j=1 pr(x|yj) =
1/(∥x −yi∥2
2 + r2)
N+D
2
Pn
j=1 1/(∥x −yj∥2
2 + r2)
N+D
2
We sample a large batch of 256 to approximate all the expectations in section 5, such as the average TVDs.
D.2. Training Details
We borrow the architectures, preconditioning techniques, optimizers, exponential moving average (EMA) schedule, and
hyper-parameters from previous state-of-the-art diffusion model EDM (Karras et al., 2022). We apply the alignment method
in section 4 to transfer their well-tuned hyper-parameters.
For architecture, we use the improved NCSN++ (Karras et al., 2022) for the CIFAR-10 dataset (batch size 512), and the
improved DDPM++ for the FFHQ dataset (batch size 256). For optimizers, following EDM, we adopt the Adam optimizer
with a learning rate of 10e −4. We further incorporate the EMA schedule, learning rate warm-up, and data augmentations
in EDM. Please refer to Appendix F in EDM paper (Karras et al., 2022) for details.
The most prominent improvements in EDM are the preconditioning and the new training distribution for σ, i.e., p(σ).
Speciﬁcally, adding these two techniques to the vanilla diffusion objective (Eq. (8)), their effective training objective can be
written as:
Eσ∼p(σ)λ(σ)cout(σ)2Ep(y)Epσ(x|y)
Fθ(cin(σ) · x, cnoise(σ)) −
1
cout(σ)(y −cskip(σ) · x)

2
2

(15)
with the predicted normalized score function in the vanilla diffusion objective (Eq. (8)) re-parameterized as
fθ(x, σ) = cskip(σ)x + cout(σ)Fθ(cin(σ)x, cnoise(σ)) −x
σ
≈σ∇x log pσ(x)
cin(σ) = 1/
p
σ2 + σ2
data, cout(σ) = σ · σdata/
p
σ2 + σ2
data, cskip(σ) = σ2
data/(σ2 + σ2
data), cnoise(σ) = 1
4 ln(σ), with σdata =
0.5. {cin(σ), cout(σ), cskip(σ), cdata, cnoise(σ)} are all the hyper-parameters in the preconditioning. The training distribution
p(σ) is the log-normal distribution with ln(σ) ∼N(−1.2, 1.22), and the loss weighting λ(σ) = 1/cout(σ)2.
Recall that the hyper-parameter alignment rule r = σ
√
D can transfer the hyper-parameter from diffusion models (D→∞)
to ﬁnite Ds. Hence we can directly set σ = r/
√
D in those hyper-parameters for preconditioning. In addition, the training

PFGM++: Unlocking the Potential of Physics-Inspired Generative Models
distribution p(r) can be derived via the change-of-variable formula, i.e., p(r) = p(σ = r/
√
D)/
√
D. The ﬁnal PFGM++
objective after incorporating these techniques into Eq. (6) is:
Er∼p(r)λ(r/
√
D)cout(r/
√
D)2Ep(y)Epr(x|y)
Fθ(cin(r/
√
D) · x, cnoise(r/
√
D)) −
1
cout(σ)(y −cskip(r/
√
D) · x)

2
2

with the predicted normalized electric ﬁeld in the vanilla PFGM++ objective (Eq. (6)) re-parameterized as
fθ(˜x) = cskip(r/
√
D)x + cout(r/
√
D)Fθ(cin(r/
√
D)x, cnoise(r/
√
D)) −x
r/
√
D
≈
√
DE(˜x)x
E(˜x)r
D.3. Sampling Details
For sampling, following EDM (Karras et al., 2022), we also use Heun’s 2nd method (improved Euler method) (Ascher &
Petzold, 1998) as the ODE solver for dx/dr = E(˜x)x/E(˜x)r = fθ(˜x)/
√
D.
We adopt the same parameterized scheme in EDM to determine the evaluation points during N-step ODE sampling:
ri = (rmax
1
ρ +
i
N −1(rmin
1
ρ −rmax
1
ρ ))ρ
and
rN = 0
where ρ controls the relative density of evaluation points in the near ﬁeld. We set ρ = 7 as in EDM, and rmax = σmax
√
D =
80
√
D, rmin = σmin
√
D = 0.002
√
D (σmax, σmin are the hyper-parameters in EDM, controlling the starting/terminal
evaluation points) following the r = σ
√
D alignment rule.
D.4. Evaluation Details
For the evaluation, we compute the Fr´echet distance between 50000 generated samples and the pre-computed statistics of
CIFAR-10 and FFHQ. On CIFAR-10, we follow the evaluation protocol in EDM (Karras et al., 2022), which repeats the
generation three times with different seeds for each checkpoint and reports the minimum FID score. However, we observe
that the FID score has a large ﬂuctuation across checkpoints, and the minimum FID score of EDM in our re-run experiment
does not align with the original results reported in (Karras et al., 2022). Fig. 7(a) shows that the FID score could have a
variation of ±0.2 during the training of a total of 200 million images (Karras et al., 2022). To better evaluate the model
performance, Table 2 reports the average FID over the Top-3 checkpoints instead. In Fig. 7(b), we further demonstrate the
moving average of the FID score with a window of 10000K images. It shows that D = 2048 consistently outperforms other
baselines in the same training iterations, in agreement with the results in Table 2.
D.5. Experiments for Robustness
Controlled experiments with α
In the controlled noise setting, we inject noise into the intermediate point xr in each of
the 35 ODE steps by xr = xr + αϵr where ϵr ∼N(0, r/
√
DI). Since pr has roughly the same phase as pσ=r/
√
D in
diffusion models, we pick r/
√
D standard deviation of ϵr when the intermediate step is r.
Post-training quantization
In the post-training quantization experiments on CIFAR-10, we quantize the weights of
convolutional layers excluding the 32 × 32 layers, as we empirically observe that these input/output layers are more critical
for sample quality.
E. Extra Experiments
E.1. Stable Target Field
Xu et al. (2023) propose a Stable Target Field objective for training the diffusion models:
∇x log pσ(x) ≈Ey1∼p0|t(·|x)E{yi}n
i=2∼pn−1
" n
X
k=1
pt|0(x|yk)
P
j pt|0(x|yj)∇x log pt|0(x|yk)
#
where they sample a large batch of samples {yi}n
i=2 from the data distribution to approximate the score function at x. They
show that the new target can enhance the stability of converged models in different runs/seeds. PFGM++ can be trained in a

PFGM++: Unlocking the Potential of Physics-Inspired Generative Models
150000
160000
170000
180000
190000
200000
Kimg
2.45
2.50
2.55
2.60
2.65
2.70
2.75
2.80
FID Score
D = 128
D = 2048
D = 3072000
D
 (Diffusion)
(a) w/o moving average
150000
160000
170000
180000
190000
200000
Kimg
2.55
2.60
2.65
2.70
FID Score
D = 128
D = 2048
D = 3072000
D
 (Diffusion)
(b) w/ moving average
Figure 7. FID score in the training course when varying D, (a) w/o and (b) w/ moving average.

PFGM++: Unlocking the Potential of Physics-Inspired Generative Models
similar fashion by replacing the target
x−y
r/
√
D in perturbation-based objective (Eq. (6)) with
1
r/
√
D
 x −Ep0|r(y|x)[y]

≈
1
r/
√
D
 
x −Ey1∼p0|r(·|x)E{yi}n
i=2∼pn−1
" n
X
k=1
1/(∥x −yk∥2
2 + r2)
N+D
2
P
j 1/(∥x −yj∥2
2 + r2)
N+D
2
yk
#!
When n = 1, the new target reduces to the original target. Similar to (Xu et al., 2023), one can show that the bias of
the new target together with its trace-of-covariance shrinks to zero as we increase the size of the large batch. This new
target can alleviate the variations between random seeds. With the new STF-style target, Table 4 shows that when setting
D = 3072000 ≫N = 3072, the model obtains the same FID score as the diffusion models (EDM (Karras et al., 2022)). It
aligns with the theoretical results in Sec 4, which states that PFGM++ recover the diffusion model when D →∞.
Table 4. FID and NFE on CIFAR-10, using the Stable Target Field (Xu et al., 2023) in training objective.
FID ↓
NFE ↓
D = 3072000
1.90
35
D →∞(Karras et al., 2022)
1.90
35
E.2. Extended CIFAR-10 Samples when varying α
To see how the sample quality varies with α, we visualize the generative samples of models trained with D ∈{64, 128, 2048}
and D →∞. We pick α ∈{0, 0.1, 0.2}. Fig. 8 shows that the smaller Ds produce better samples compared to larger D.
Diffusion models (D →∞) generate noisy images that appear to be out of the data distribution when α = 0.2, in contrast
to the clean images by D = 64, 128.
E.3. Extended FFHQ Samples
In Fig. 9, we provide samples generated by the D = 128 case and EDM (the D →∞case).
F. Potential Negative Social Impact
The deep generative model is a burgeoning ﬁeld and has signiﬁcant potential for shaping our society. Our work presents a
novel family of generative models, the PFGM++, which subsume previous high-performing models and provide greater
ﬂexibility. The PFGM++ have many potential applications, particularly in areas that require both robustness and high-quality
output. However, it is important to note that the usage of these models can have both positive and negative implications,
depending on the speciﬁc application. For instance, the PFGM++ can be used to create realistic image and audio samples,
but it can also contribute to the development of deepfake technology and potentially lead to social scams. Additionally, the
data-collecting process for generative models may infringe upon intellectual property rights. To address these concerns,
further research is needed to provide robustness guarantees for generative models and to foster collaborations with experts
in socio-technical ﬁelds.

PFGM++: Unlocking the Potential of Physics-Inspired Generative Models
(a) D=64, α = 0 (FID=1.96)
(b) D=64, α = 0.1 (FID=1.97)
(c) D=64, α = 0.2 (FID=2.07)
(d) D=128, α = 0 (FID=1.92)
(e) D=128, α = 0.1 (FID=1.95)
(f) D=128, α = 0.2 (FID=2.19)
(g) D=2048, α = 0 (FID=1.92)
(h) D=2048, α = 0.1 (FID=1.95)
(i) D=2048, α = 0.2 (FID=2.19)
(j) D →∞, α = 0 (FID=1.98)
(k) D →∞, α = 0.1 (FID=9.27)
(l) D →∞, α = 0.2 (FID=92.41)
Figure 8. Generated samples on CIFAR-10 with varied hyper-parameter for noise injection (α). Images from top to bottom rows are
produced by models trained with D = 64/128/2048/∞. We use the same random seeds for ﬁnite Ds during image generation.

PFGM++: Unlocking the Potential of Physics-Inspired Generative Models
(a) D = 128 (FID=2.43)
(b) EDM (D →∞) (FID=2.53)
Figure 9. Generated images on FFHQ 64 × 64 dataset, by (left) D = 128 and (right) EDM (D →∞).

