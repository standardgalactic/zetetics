Improving and Generalizing Flow-Based
Generative Models with Minibatch Optimal Transport
Alexander Tong1,2
Nikolay Malkin1,2
Guillaume Huguet1,2
Yanlei Zhang1,2
Jarrid Rector-Brooks1,2
Kilian Fatras1,3
Guy Wolf1,2
Yoshua Bengio1,2,4
1Mila – Québec AI Institute
2Université de Montréal
3McGill University
4CIFAR Fellow

alexander.tong, nikolay.malkin,guillaume.huguet,yanlei.zhang
jarrid.rector-brooks,kilian.fatras,yoshua.bengio

@mila.quebec
guy.wolf@umontreal.ca
Abstract
Continuous normalizing flows (CNFs) are an attractive generative modeling tech-
nique, but they have been held back by limitations in their simulation-based maxi-
mum likelihood training. We introduce the generalized conditional flow matching
(CFM) technique, a family of simulation-free training objectives for CNFs. CFM
features a stable regression objective like that used to train the stochastic flow in
diffusion models but enjoys the efficient inference of deterministic flow models. In
contrast to both diffusion models and prior CNF training algorithms, CFM does not
require the source distribution to be Gaussian or require evaluation of its density. A
variant of our objective is optimal transport CFM (OT-CFM), which creates simpler
flows that are more stable to train and lead to faster inference, as evaluated in our
experiments. Furthermore, OT-CFM is the first method to compute dynamic OT
in a simulation-free way. Training CNFs with CFM improves results on a variety
of conditional and unconditional generation tasks, such as inferring single cell dy-
namics, unsupervised image translation, and Schrödinger bridge inference. Code is
available at https://github.com/atong01/conditional-flow-matching.
1
Introduction
Generative modeling considers the problem of approximating and sampling from a probability
distribution. Normalizing flows, which have emerged as a competitive generative modeling method,
construct an invertible and efficiently differentiable mapping between a fixed (e.g., standard normal)
distribution and the data distribution [54]. While original normalizing flow work specified this
mapping as a static composition of invertible modules, continuous normalizing flows (CNFs) express
the mapping by a neural ordinary differential equation (ODE) [11]. Unfortunately, CNFs have been
held back by difficulties in training and scaling to large datasets [11, 24, 50].
Meanwhile, diffusion models, which are the current state of the art on many generative modeling
tasks [16, 4, 12, 71], approximate a stochastic differential equation (SDE) that transforms a simple
density to the data distribution. Diffusion models owe their success in part to their simple regression
training objective, which does not require simulating the SDE during training. Recently, [39] showed
that CNFs could also be trained using a regression of the ODE’s drift similar to training of diffusion
models, an objective called flow matching (FM). FM was shown to produce high-quality samples and
stabilize CNF training. However, FM models make the assumption of a Gaussian source distribution.
The first main contribution of the present paper is to relax this assumption and enable learning of
Preprint. Under review.
arXiv:2302.00482v2  [cs.LG]  12 Jul 2023

Figure 1: Left: Conditional flows from FM [39], I-CFM (§3.2.2), and OT-CFM (§3.2.3). Right:
Learned flows (green) from moons (blue) to 8-Gaussians (black) using I-CFM (centre-right) and
OT-CFM (far right) – not possible using FM, which requires a Gaussian source.
ODE bridges between two arbitrary distributions. The general class of objectives we propose, termed
conditional flow matching (CFM), widely broadens the scope of applications of FM.
A major drawback of both CNF (ODE) and diffusion (SDE) models compared to other generative
models (e.g., variational autoencoders [29], (discrete-time) normalizing flows, and generative ad-
versarial networks [23]), is that integration of the ODE or SDE requires many passes through the
network to generate a high-quality sample, resulting in a long inference time. This drawback has
motivated work on enforcing an optimal transport (OT) property in neural ODEs [66, 20, 50, 40, 41],
yielding flows that can be integrated accurately in fewer neural network evaluations. Such regular-
izations have not yet been studied for models trained with FM-like objectives. Our second main
contribution is a variant of CFM called optimal transport conditional flow matching (OT-CFM) that
approximates dynamic OT via CNFs. We show that OT-CFM not only improves the efficiency of
training and inference, but also leads to more accurate OT flows than existing neural OT models
based on ODEs [66, 20], SDEs [15, 68], or input-convex neural networks [45]. Furthermore, an
entropic variant of OT-CFM can be used to efficiently train a CNF to match the probability flow of
a Schrödinger bridge. Our work is the first to enable simulation-free training of dynamic OT
maps and Schrödinger bridge probability flows for arbitrary source and target distributions.
In summary, our contributions are:
(1) We introduce a novel class of objectives called (generalized) conditional flow matching (§3.1).
CFM is able to learn conditional generative models from any samplable source distribution by
conditioning on paired source and target samples, generalizing existing methods [39, 1, 40, 53].
(2) We consider a special case of CFM that draws source and target samples according to an optimal
transport plan, allowing us to solve the dynamic OT and Schrödinger bridge problems in a
simulation-free way, using only static OT maps between marginal distributions (§3.2).
(3) We evaluate CFM and OT-CFM in experiments on single-cell dynamics, image generation,
unsupervised image translation, and energy-based models. We show that the OT-CFM objective
leads to more efficient training and decreases inference time while finding better approximate
solutions to the dynamic OT and Schrödinger bridge problems (§5).
2
Background: Optimal transport and neural ODEs
Throughout the paper, we consider the setting of a pair of data distributions over Rd with (possibly
unknown) densities q(x0) and q(x1) (also denoted q0, q1). Generative modeling considers the task of
fitting a mapping f from Rd to Rd that transforms q0 to q1, that is, if x0 is distributed with density q0
then f(x0) is distributed with density q1. This includes both the typical case when q0 is an easily
sampled density, such as a Gaussian, and the case when q0 and q1 are empirical data distributions
available as finite sets of samples.
2.1
ODEs and probability flows
A smooth1 time-varying vector field u : [0, 1] × Rd →Rd defines an ordinary differential equation:
dx = ut(x) dt,
(1)
where we use the notation ut(x) interchangeably with u(t, x). Denote by ϕt(x) the solution of the
ODE (1) with initial conditions ϕ0(x) = x; that is, ϕt(x) is the point x transported along the vector
field u from time 0 up to time t.
1To be precise, to ensure the uniqueness of integral curves (and thus of the corresponding flow), we assume
the vector field u is at least (locally) Lipschitz in x and Bochner integrable in t.
2

Given a density p0 over Rd, the integration map ϕt induces a pushforward pt = [ϕt]#(p0), which is
the density of points x ∼p0 transported along u from time 0 to time t. The time-varying density pt,
viewed as a function p : [0, 1] × Rd →R, is characterized by the well-known continuity equation:
∂p
∂t = −∇· (ptut)
(2)
and the initial conditions p0. Under these conditions, u is said to be a probability flow ODE for p,
and p is the (marginal) probability path generated by u.
Approximating ODEs with neural networks.
Suppose the probability path pt(x) and the vector
field ut(x) generating it are known and pt(x) can be tractably sampled. If vθ(·, ·) : [0, 1] × Rd →Rd
is a time-dependent vector field parametrized as a neural network with weights θ, vθ can be regressed
to u via the flow matching (FM) objective:
LFM(θ) = Et∼U(0,1),x∼pt(x)∥vθ(t, x) −ut(x)∥2.
(3)
[39] used a version of this objective with a stochastic regression target to fit ODEs that map a
Gaussian density q0 to a target q1. The starting point for this work is that this objective becomes
intractable for general source and target distributions; in §3, we develop generalizations that
allow more flexible and efficient generative modeling.
The case of Gaussian marginals.
Consider the special case of an ODE whose marginal densities
are Gaussian: pt(x) = N(x | µt, σ2
t ). While the ODE that generates these marginal densities is not
unique, one of the simplest is the one that satisfies
ϕt(x) = µt + σtx,
(4)
which is unique by the following theorem.
Theorem 2.1 (Theorem 3 of [39]). The unique vector field whose integration map satisfies (4) has
the form
ut(x) = σ′
t
σt
(x −µt) + µ′
t,
(5)
where σ′
t and µ′
t denote the time derivative of σt and µt, respectively, and the vector field u with
initial conditions N(µ0, σ2
0) generates the Gaussian probability path pt(x) = N(x | µt, σ2
t ).
2.2
Static and dynamic optimal transport
The (static) optimal transport problem seeks a mapping from one measure to another that minimizes
a displacement cost. A case of greatest interest is the 2-Wasserstein distance between distributions q0
and q1 on Rd with respect to the Euclidean distance cost c(x, y) = ∥x −y∥, defined by
W(q0, q1)2
2 = inf
π∈Π
Z
X 2 c(x, y)2 dπ(x, y),
(6)
where Π denotes the set of all joint probability measures on Rd × Rd whose marginals are q0 and q1.
The dynamic form of the 2-Wasserstein distance is defined by an optimization problem over vector
fields ut that transform one measure to the other:
W(q0, q1)2
2 = inf
pt,ut
Z
Rd
Z 1
0
pt(x)∥ut(x)∥2 dt dx,
(7)
with pt ≥0 and subject to the boundary conditions p0 = q0, p1 = q1 and the continuity equation (2).
[66, 20] showed that CNFs with L2 regularization approximate dynamic optimal transport. For
general marginals, however, these models required integrating over and backpropagating through
tens to hundreds of function evaluations, resulting in both numerical and efficiency issues. We aim to
avoid these issues by directly regressing to the vector field in a simulation-free way.
Optimal transport is also related to the Schrödinger bridge (SB) problem [36]. We show in §3.2.4 that
a variant of the algorithm we propose recovers the probability flow of the solution to a SB problem
with a Brownian noise reference process.
3

Algorithm 1 Conditional Flow Matching
Input: Efficiently samplable q(z), pt(x|z), and computable ut(x|z) and initial network vθ.
while Training do
z ∼q(z);
t ∼U(0, 1);
x ∼pt(x|z)
LCFM(θ) ←∥vθ(t, x) −ut(x|z)∥2
θ ←Update(θ, ∇θLCFM(θ))
return vθ
3
Conditional flow matching: ODEs from static couplings
3.1
Vector fields generating mixtures of probability paths
Suppose that the marginal probability path pt(x) is a mixture of probability paths pt(x|z) that vary
with some latent conditioning variable z, that is,
pt(x) =
Z
pt(x|z)q(z) dz,
(8)
where q(z) is some distribution over the latent variable. If the probability path pt(x|z) is generated
by the vector field ut(x|z) from initial conditions p0(x|z) (see §2.1), then the vector field
ut(x) := Eq(z)
ut(x|z)pt(x|z)
pt(x)
(9)
generates the probability path pt(x), under some mild conditions:
Theorem 3.1. The marginal vector field (9) generates the probability path (8) from initial conditions
p0(x).
All proofs appear in Appendix A. This surprising result extends [39, Theorem 1] to general condi-
tioning variables and delineates some minor conditions on q(z).
A regression objective for mixtures.
We are interested in the case where conditional probability
paths pt(x|z) and vector fields ut(x|z) are known and have a simple form, and we wish to recover
the vector field ut(x), defined by (9), that generates the probability path pt(x). Exact computation
via (9) is generally intractable, as the denominator pt(x) is defined by an integral (8) that may be
difficult to evaluate. Instead, we develop an unbiased stochastic objective for regression of a learned
vector field to ut(x), which generalizes the unconditional flow matching objective (3).
Let vθ(·, ·) : [0, 1] × Rd →Rd be a time-dependent vector field parametrized as a neural network
with weights θ. Define the conditional flow matching (CFM) objective:
LCFM(θ) = Et,q(z),pt(x|z)∥vθ(t, x) −ut(x|z)∥2.
(10)
The CFM objective describes how to regress against the marginal vector field ut(x) given by (9) with
access only to samples from the conditional probability path pt(x|z) and conditional vector fields
ut(x|z). This is formalized in the following theorem.
Theorem 3.2. If pt(x) > 0 for all x ∈Rd and t ∈[0, 1], then, up to a constant independent of θ,
LCFM and LFM are equal, and hence
∇θLFM(θ) = ∇θLCFM(θ).
(11)
The CFM objective is useful when the marginal vector field ut(x) is intractable but the conditional
vector field ut(x|z) is simple. As long as we can efficiently sample from q(z) and pt(x|z) and
calculate ut(x|z), we can use this stochastic objective to regress vθ to the marginal vector field ut(x).
We discuss the variance arising from the stochastic regression target, and ways to reduce it, in §C.1
and Propositions B.2 and B.3.
3.2
Sources of conditional probability paths
In this section, we introduce several forms of CFM depending on the choices of q(z), pt(·|z), and
ut(·|z). All of the CFM variants and related objectives from prior work are summarized in Table 1.
• §3.2.1: We interpret the algorithm of [39] (FM from a Gaussian) as a special case of CFM.
4

Table 1: Probability path definitions for existing methods which fit in the generalized conditional
flow matching framework (top) and our newly defined paths (bottom). We define two new probability
path objectives that can handle general source distributions and optimal transport flows.
Probability Path
q(z)
µt(z)
σt
Cond. OT
Marginal OT
General source
Var. Exploding [62]
q(x1)
x1
σ1−t
×
×
×
Var. Preserving [25]
q(x1)
α1−tx1
q
1 −α2
1−t
×
×
×
Flow Matching [39]
q(x1)
tx1
tσ −t + 1
✓
×
×
Independent CFM (≈[1, 40])
q(x0)q(x1)
tx1 + (1 −t)x0
σ
✓
×
✓
Optimal Transport CFM
π(x0, x1)
tx1 + (1 −t)x0
σ
✓
✓
✓
Schrödinger Bridge CFM
π2σ2(x0, x1)
tx1 + (1 −t)x0
σ
p
t(1 −t)
✓
✓
✓
• §3.2.2: We relax the Gaussian source requirement by letting the condition z be a pair (x0, x1) of
an initial and a terminal point. In the basic form of CFM (I-CFM), we take the distribution q(z) to
equal q(x0)q(x1), allowing generative modeling with an arbitrary source distribution.
• §3.2.3: We consider joint distributions q(z) = q(x0, x1) that are given by minibatch optimal
transport maps, causing the learned flow to be an (approximate) OT flow.
• §3.2.4: we consider q(z) given by an entropy-regularized OT map and show that the CFM objective
with this q(z) solves the Schrödinger bridge problem.
3.2.1
FM from the Gaussian
[39] considered the problem of unconditional generative modeling given a training dataset. Identifying
the condition z with a single datapoint z := x1, and choosing a smoothing constant σ > 0, one sets
pt(x|z) = N(x | tx1, (tσ −t + 1)2),
(12)
ut(x|z) = x1 −(1 −σ)x
1 −(1 −σ)t ,
(13)
which is a probability path from the standard normal distribution (p0(x|z) = N(x; 0, 1)) to a
Gaussian distribution centered at x1 with standard deviation σ (p1(x|z) = N(x; x1, σ2)). If one sets
q(z) = q(x1) to be the uniform distribution over the training dataset, the objective introduced by [39]
is equivalent to the CFM objective (10) for this conditional probability path.
We emphasize that although the conditional probability path pt(x|z) is an optimal transport path
from p0(x|z) to p1(x|z), the marginal path pt(x) is not in general an OT path from the standard
normal p0(x) to the data distribution p1(x).
3.2.2
Basic form of CFM: Independent coupling
In the basic form of CFM (I-CFM), we identify z with a pair of random variables, a source point x0
and a target point x1, and set q(z) = q(x0)q(x1). We let the conditionals be Gaussian flows between
x0 and x1 with standard deviation σ, defined by
pt(x|z) = N(x | tx1 + (1 −t)x0, σ2),
(14)
ut(x|z) = x1 −x0.
(15)
We note that the formulation of ut(x|z) follows from an application of Theorem 2.1 to the conditional
probability path with µt = tx1 + (1 −t)x0 and σt = σ. Furthermore, we note that pt(x|z) is
efficiently samplable and ut is efficiently computable, thus gradient descent on LCFM is also efficient.
For this choice of z, pt(·|z), and ut(·|z), we know the marginal boundary probabilities approach q0
and q1 respectively as σ →0. This is made explicit in the following Proposition:
Proposition 3.3. The marginal pt corresponding to q(z) = q(x0)q(x1) and the pt(x|z), ut(x|z) in
eqs. 14 and 15 has boundary conditions p1 = q1 ∗N(x | 0, σ2) and p0 = q0 ∗N(x | 0, σ2), where
∗denotes the convolution operator.
In particular, as σ →0, the marginal vector field ut(x) approaches one that transports the distribution
q(x0) to q(x1) and can thus be seen as a generative model of x1. Note that there is no requirement for
q(x0) to be Gaussian. Conditioning on x0 allows us to generalize flow matching to arbitrary source
distributions with intractable densities.
As in the case of FM from a Gaussian, while each conditional flow is the dynamic optimal transport
flow from N(x0, σ2) to N(x1, σ2), the marginal vector field ut(x) is not necessarily an OT flow. We
note that I-CFM is closely related to the algorithms proposed by [1, 40].
5

Connection to FM from the Gaussian.
There exists a set of conditional probability paths condi-
tioned on x1 and x0 ∼N(0, 1) that have an equivalent probability flow to the marginal pt of flow
matching from the Gaussian (§3.2.1), which is only conditioned on x1. These paths are defined by
pt(x|z) = N(x | tx1 + (1 −t)x0, (σt)2 + 2σt(1 −t)).
(16)
Prop. B.1 states an equivalence between I-CFM with these paths and the objective from §3.2.1.
3.2.3
Optimal transport CFM
In this section, we present our second main contribution. The formulation in the previous section can
readily be generalized to distributions q(z) = q(x0, x1) in which x0 and x1 are not independent, as
long as q(z) has marginals q(x0) and q(x1). Therefore, we propose to set q(z) to be the 2-Wasserstein
optimal transport map π achieving the infimum in (6), namely,
q(z) := π(x0, x1).
(17)
In this case, z is still a tuple of points, but instead of x0, x1 being sampled independently from their
marginal distributions, they are sampled jointly according to the optimal transport map π. We call
this method optimal transport CFM (OT-CFM). If one uses the pt(x|z) defined by eq. 14 and ut(x|z)
in eq. 15, OT-CFM is equivalent to dynamic optimal transport in the following sense.
Proposition 3.4. The results of Prop. 3.3 also hold for q(z) in eq. 17. Furthermore, assuming
regularity properties of q0, q1, and the optimal transport plan π, as σ2 →0 the marginal path pt and
field ut minimize eq. 7, i.e., ut solves the dynamic optimal transport problem between q0 and q1.
To the best of our knowledge, OT-CFM is the first method to solve the dynamic OT problem in a
simulation-free manner, using only the static OT map between q(x0) and q(x1) and regression to the
conditional flow at intermediate time steps.
Minibatch OT approximation.
For large datasets, the transport plan π can be difficult to compute
and store due to OT’s cubic time and quadratic memory complexity in the number of samples [13, 66].
If this cost is prohibitive, we can use a minibatch approximation of OT similar to [19]. Although
minibatch OT incurs an error relative to the exact OT solution, it has been successfully used in many
applications like domain adaptation or generative modeling [14, 22]. Specifically, for each batch of
data ({x(i)
0 }B
i=1, {x(i)
1 }B
i=1) seen during training, we sample pairs of points from the joint distribution
πbatch given by the OT plan between the source and target points in the batch. (The OT batch size
need not match the optimization batch size, but we keep them equal for simplicity.) Thus, we solve a
minibatch approximation of dynamic optimal transport. However, when the OT batch size equals the
support size of (q0, q1), we recover exact OT and therefore, by Prop. 3.4, learn the exact dynamic
optimal transport. We show empirically that the batch size can be much smaller than the full dataset
size and still give good performance, which aligns with prior studies [17, 18].
3.2.4
Schrödinger bridge CFM
Recently, there has been significant effort in learning diffusion models with general source distribu-
tions, formulated as a Schrödinger bridge problem [15, 68]. Here we show that SB-CFM, an entropic
variant of OT-CFM, can be used to train an ODE to match the probability flow of a Schrödinger
bridge with a Brownian motion reference process.
Let pref be the standard Wiener process scaled by σ with initial-time marginal pref(x0) = q(x0). The
Schrödinger bridge problem [58] seeks the process π that is closest to pref while having initial and
terminal marginal distributions specified by the data distribution q(x0) and q(x1):
π∗:=
arg min
π(x0)=q(x0),π(x1)=q(x1)
KL(π ∥pref).
(18)
We define the joint distribution
q(z) := π2σ2(x0, x1)
(19)
where π2σ2 is the solution of the entropy-regularized optimal transport problem [13] with cost
∥x0 −x1∥and entropy regularization λ = 2σ2 (see eq. 33 for the background on entropic OT). We
set the conditional path distribution to be a Brownian bridge with diffusion scale σ between x0 and
x1, with probability flow and generating vector field
pt(x|z) = N(x | tx1 + (1 −t)x0, t(1 −t)σ2)
(20)
ut(x|z) =
1 −2t
2t(1 −t)(x −(tx1 + (1 −t)x0)) + (x1 −x0),
(21)
6

Table 2: Comparison of neural optimal transport methods over four datasets and with (µ ± σ) over
five seeds in terms of fit (2-Wasserstein), optimal transport performance (normalized path energy),
and Runtime. ‘—’ indicates a method that requires a Gaussian source. Best in bold. CFM and RF
models are trained on a single CPU core, other baselines are trained with a GPU and two CPUs.
2-Wasserstein Error (↓)
Normalized Path Energy (↓)
Train time (s) ×103
8gaussians
moons-8gaussians
moons
scurve
8gaussians
moons-8gaussians
moons
scurve
Avg.
OT-CFM
1.262 ± 0.348
1.923 ± 0.391
0.239 ± 0.048
0.264 ± 0.093
0.018 ± 0.014
0.053 ± 0.035
0.087 ± 0.061
0.027 ± 0.026
1.129 ± 0.335
I-CFM
1.284 ± 0.384
1.977 ± 0.266
0.338 ± 0.109
0.333 ± 0.060
0.222 ± 0.032
2.738 ± 0.181
0.841 ± 0.148
0.867 ± 0.117
0.630 ± 0.365
2-RF [40]
1.436 ± 0.344
2.211 ± 0.423
0.278 ± 0.026
0.395 ± 0.111
0.069 ± 0.027
0.149 ± 0.101
0.076 ± 0.067
0.112 ± 0.085
0.862 ± 0.166
3-RF [40]
1.337 ± 0.367
2.700 ± 0.587
0.305 ± 0.026
0.395 ± 0.082
0.055 ± 0.043
0.123 ± 0.112
0.084 ± 0.051
0.129 ± 0.075
0.954 ± 0.116
FM [39]
1.062 ± 0.196
—
0.246 ± 0.077
0.377 ± 0.099
0.174 ± 0.030
—
0.778 ± 0.144
0.772 ± 0.081
0.708 ± 0.370
Reg. CNF [20]
1.144 ± 0.075
—
0.376 ± 0.040
0.581 ± 0.195
0.274 ± 0.060
—
0.620 ± 0.088
0.586 ± 0.503
8.021 ± 3.288
CNF [11]
1.055 ± 0.059
—
0.387 ± 0.065
0.645 ± 0.343
0.151 ± 0.064
—
2.937 ± 1.973
10.548 ± 8.100
18.810 ± 12.677
ICNN [45]
1.771 ± 0.398
2.193 ± 0.136
0.532 ± 0.046
0.753 ± 0.068
0.747 ± 0.029
0.832 ± 0.004
0.267 ± 0.010
0.344 ± 0.045
2.912 ± 0.626
where ut is given by eq. 5. The marginal coupling π2σ2 and ut(x|z) define ut(x), which is approxi-
mated by the regression objective in Algorithm 4. The solution of the SB is known to be the map
which is the solution of the entropically-regularized OT problem, motivating the next proposition.
Proposition 3.5. The marginal vector field ut(x) defined by eq. 19 and eq. 21 generates the same
marginal probability path as the solution π∗to the SB problem in eq. 18.
While we define SB-CFM with an entropic regularization coefficient of ε = 2σ2, the flow still
matches the marginals for any choice of ε. Interestingly, we recover OT-CFM when ε →0 and
I-CFM when ε →∞.
4
Related work
Simulation-free continuous-time modeling.
Simulation-free training is common in stochastic
flow models where backpropagating through the simulation is numerically challenging and has
high variance [38]. While these diffusion models have recently achieved exceptional generative
performance on many tasks [60, 62, 63, 25, 64, 16, 71], their simulation requires an inherently costly
SDE simulation with many follow-up works to improve inference efficiency [44, 56, 70, 61, 5]. These
methods generally consider a simple Gaussian diffusion process, and do not consider generalizing the
source distribution. Other works consider general source distributions but this makes optimization and
inference more challenging, needing multiple iterations or other tricks to perform well [69, 15, 68].
Prior work considering simulation-free training of CNFs considers algorithms that are equivalent to
CFM with Gaussian source distribution [55, 6, 39] or independent samples from q0, q1 [1, 2, 48].
Recent work also studies Schrödinger bridges from unpaired samples [59] and regularization of flows
using dynamic OT [41]. Contemporaneously with our work, [53] proposes objectives closely related
to our OT-CFM and SB-CFM, albeit with a focus on generative modeling in high dimensions, rather
than on high-fidelity solutions to the dynamic OT and Schrödinger bridge problems.
Dynamic optimal transport.
There are a variety of methods that consider dynamic OT between
continuous distributions with neural networks; however, these require constrained architectures [37,
45, 9] or use a regularized CNF, which is challenging to optimize [66, 20, 50, 27]. With our work it
is possible to achieve optimal transport flows without either of these constraints.
5
Experiments
In this section we empirically evaluate the I-CFM, OT-CFM, and SB-CFM objectives, as well as
algorithms from prior work, with respect to both optimal transport and generative modeling criteria.
All experiment details can be found in §E.
5.1
Low-dimensional data: Optimal transport and faster convergence
OT-CFM solves dynamic OT.
We evaluate how well various models perform dynamic optimal
transport and generative modeling in low dimensions.2 Table 2 summarizes our results showing
that OT-CFM flows generalize better to the test set and are very close to the dynamic OT paths as
measured by normalized path energy. We find transforming moons↔8gaussians to be particularly
challenging to learn for I-CFM as compared to OT-CFM; the learned paths are depicted in Figure 1
2To measure how well a model solves the OT problem we use normalized path energy (NPE), defined via
the 2-Wasserstein distance as NPE(vθ) = |PE(vθ) −W 2
2 (q0, q1)|/W 2
2 (q0, q1), where the path energy (PE) is
PE(vθ) = Ex(0)∼q(x0)
R 1
0 ∥vθ(t, x(t))∥2dt.
7

Figure 2: Left: OT-CFM trains faster, in terms of validation set error, than CFM and FM models.
Right: With different ODE integrators, OT-CFM reduces the error for a fixed number of function
evaluations during inference.
Table 3: Schrödinger bridge flow comparison,
showing average error over flow time to ground
truth averaged over 5 models for SB-CFM and 5
dynamics from DSB.
SB-CFM
DSB [15]
8gaussians
0.454 ± 0.164
1.440 ± 0.720
moon-8gaussians
1.377 ± 0.229
2.407 ± 1.025
moons
0.283 ± 0.048
0.333 ± 0.129
scurve
0.297 ± 0.064
0.383 ± 0.134
(bottom). Although OT-CFM uses a minibatch OT map, we find that OT-CFM requires surprisingly
small batches to approximate the OT map well, suggesting some generalization advantages of the
network optimization (Figure 4).
OT-CFM yields faster training.
By conditioning on minibatch optimal transport flows, OT-CFM
is substantially easier to train, which we posit is due to the variance reduction of the conditional flow.
In Figure 2 (left), we evaluate the performance over time of OT-CFM against CFM and FM objectives.
For the same number of steps OT-CFM has better performance on the validation set. In Table 7, we
compare the training times for various Neural OT methods whose performance can be seen in Table 2.
Simulation-free optimization is significantly faster to train with equal or superior performance.
OT-CFM yields faster inference.
We next evaluate the quality of samples during inference time.
In Figure 2 (right), we compare the quality of samples for different number of function evaluations
(NFEs) across different flow matching objectives. In this experiment we sample from the source
distribution test set and simulate the ODE over time for different solvers. We find that OT-CFM
consistently requires fewer evaluations to achieve the same quality and achieves better quality with
the same NFEs. This is consistent with previous work, which found OT paths lead to faster, higher
quality inference in regularized CNFs [20, 50] and flow matching vs. standard variance-preserving
and variance-exploding probability paths [39].
SB-CFM reproduces Schrödinger bridge flows.
There are a number of methods which theo-
retically converge to a Schrödinger bridge between two datasets. In Table 3 we compare SB-CFM
and the diffusion Schrödinger bridge (DSB) method introduced in [15] on the quality of the learnt
Schrödinger bridges based on the average 2-Wasserstein distance to ground truth Schrödinger bridge
samples over 18 time steps. Furthermore, SB-CFM is also significantly faster than DSB (Table 7).
Application to single-cell interpolation.
As a specific application, we consider the task of single-
cell trajectory interpolation. In this task we use leave-one-out validation over the timepoints. From
times data at times [0, t −1], [t + 1, T] we try to interpolate its distribution at time t following the
setup of [57, 66, 27]. Low error means we model individual cells well, which is useful in a number of
downstream tasks such as gene regulatory network inference [3, 72]. Following [28], we repurpose
the CITE-seq and Multiome datasets from a recent NeurIPS competition for this task [10]. We also
include the Embryoid-body data from [47, 66]. Table 4 shows the average earth mover’s distance
(1-Wasserstein) on left–out timepoints for three datasets. On all three datasets OT-CFM outperforms
other methods and baselines on average.
5.2
High-dimensional data: Lower-cost training and inference
We perform an experiment on unconditional CIFAR-10 generation from the Gaussian, replicating
as closely as possible3 the setup of FM [39], which uses the time-dependent U-Net architecture
from [49] (see §E). We find that:
• For a short computation budget, OT-CFM outperforms FM and (non-OT) CFM (Table 5, left).
3There is insufficient information to reproduce the setup precisely. Specifically, [39] does not specify the
σmin used (we chose 10−8) and number of samples for FID evaluation (we took 10k). Furthermore, there is a
discrepancy in the learning rate schedule and the number of training epochs as described in [39]. We reran FM
with controlled hyperparameter settings for a fair comparison.
8

Table 4: Single-cell compar-
ison over three datasets av-
eraged over leaving out in-
termediate timepoints measur-
ing EMD to left out distribu-
tion following [66]. *Indicates
values taken from aforemen-
tioned work.
Dataset
Cite
EB
Multi
T. Net [66]*
—
0.848 ± —
—
Reg. CNF [20]*
—
0.825 ± —
—
DSB [15]
0.953 ± 0.140
0.862 ± 0.023
1.079 ± 0.117
I-CFM
0.965 ± 0.111
0.872 ± 0.087
1.085 ± 0.099
SB-CFM
1.067 ± 0.107
1.221 ± 0.380
1.129 ± 0.363
OT-CFM
0.882 ± 0.058
0.790 ± 0.068
0.937 ± 0.054
Table 5: Left: FID scores on CIFAR-10 after a small number of training epochs. OT-CFM trains
faster with a lower FID score after the same number of training steps (100 steps of Euler integration
used for FID evaluation). Right: FID scores after 1000 epochs training. For a small number of
function evaluations, OT-CFM has significantly lower FID scores.
Train Epochs
Alg.
5
40
FM
134.702
57.461
I-CFM
127.693
48.202
OT-CFM
111.394
47.936
Function Evaluations
Alg.
1
5
10
50
100
FM
362.421
63.832
29.636
11.661
11.011
I-CFM
360.787
63.562
27.884
10.685
11.859
OT-CFM
238.981
38.567
20.855
11.732
11.139
• After a long training time, all methods achieve similar performance at a high number of function
evaluations, but OT-CFM performs significantly better with a small number of function evaluations
(i.e., allows more efficient inference), indicating straighter, easily integrable flows (Table 5, right).
• FM and CFM are equivalently computationally efficient per iteration and OT-CFM comes with a
low (<1%) computational overhead during training.
5.3
OT-CFM for unsupervised translation
We show how CFM can be used to learn a mapping between two unpaired datasets in high-dimensional
space using the CelebA dataset [42, 65], which consists of ∼200k images of faces together with
40 binary attribute annotations. For each attribute, we wish to learn an invertible mapping between
images with and without the attribute (e.g., ‘not smiling’↔‘smiling’).
To reduce dimensionality, we first train a VAE on the images and encode them as 128-dimensional
latent vectors. For each attribute, we learn a flow to map between the embeddings of images without
the attribute and those of images with the attribute. After the CNF is learned, we push forward a
held-out set of negative vectors by the CNF and compare them to the held-out positive vectors and
vice versa. As a metric of divergence, we use maximum mean discrepancy (MMD) with a broad
Gaussian kernel (exp(−∥x −y∥2/(2 · 128))). The results aggregated over all attributes are shown
in Table 6, showing that OT-CFM discovers a better mapping than other methods. Although MMD
is lower for larger σ, we found that the alignment is less natural when σ is large, and performance
begins to degrade when σ > 1. Figure 12 shows several visualizations of the learned trajectories.
Finally, while here we work in a latent space, future work should consider learning flows directly in
image space, where GAN-based approaches [74] continue to dominate.
5.4
Additional experiments and extensions
We present numerous other extensions, applications, and evaluations of CFM in §D, notably:
OT-CFM reduces variance in the regression target.
To accompany the theoretical results in §C.1,
in §D.1 we empirically study the variance of the stochastic regression objective in (OT-)CFM. The
results suggest an explanation for the faster convergence of models trained with OT-CFM.
Energy-based CFM.
In §C.2 and §D.2 we show how CFM can be used to fit samplers for
unnormalized density functions, where exact samples from q(x0) or q(x1) are not available.
Extension to stochastic dynamics.
[67] extends CFM to allow learning stochastic dynamics from
unpaired source and target data.
9

Table 6: MMD between target and transformed
source samples of CelebA latent vectors. Mean
and standard deviation over 40 attributes and both
translation directions (−↔+) for each attribute.
‘Identity’ refers to performing no translation and
treating source samples as approximate samples
from the target.
×10−3
σ = 0.1
σ = 0.3
σ = 1
Identity
9.17 ± 5.68
9.17 ± 5.68
9.17 ± 5.68
I-CFM
4.85 ± 5.09
3.44 ± 2.03
1.59 ± 0.83
OT-CFM
2.81 ± 2.62
1.91 ± 1.30
1.04 ± 0.60
6
Conclusion
We have introduced a novel class of simulation-free objectives for learning continuous-time flows
with a general source distribution. Our approach to training continuous normalizing flows and
conditional flow models does not require integration over time during training. We have shown that
lifting the static optimal transport problem to the dynamic setting leads to simulation-free solutions
to the dynamic OT and SB problems, while also allowing more efficient training and inference of
flow models by lowering variance of the objective and straightening flows.
One limitation of CFM is that it requires closed-form conditional flows, which hinders its appli-
cation to situations where we want to regularize the marginal vector field ut(x) based on prior
information [66]. In addition, the minibatch approximation to OT can incur error in high dimensions;
subsequent work can consider the use of neural-network approximations to OT maps [33, 32] in
conjunction with CFM. We expect future work to overcome these limitations and hope that ideas
from conditional flow matching will improve high-dimensional generative models.
Acknowledgments
We would like to thank Stefano Massaroli for productive conversations as well as thank Xinyu Yuan,
Marco Jiralerspong, Tara Akhound-Sadegh and Joey Bose for their helpful comments and feedback
on the manuscript. The authors acknowledge funding from CIFAR, Genentech, Samsung, and IBM.
In addition, K.F. acknowledges funding from NSERC (RGPIN-2019-06512) and G.W. acknowledges
funding from NSERC Discovery grant 03267 and NIH grant R01GM135929.
References
[1] Albergo, M. S. and Vanden-Eijnden, E. Building normalizing flows with stochastic interpolants.
International Conference on Learning Representations (ICLR), 2023.
[2] Albergo, M. S., Boffi, N. M., and Vanden-Eijnden, E. Stochastic interpolants: A unifying
framework for flows and diffusions. arXiv preprint 2303.08797, 2023.
[3] Aliee, H., Theis, F. J., and Kilbertus, N. Beyond predictions in neural ODEs: Identification and
interventions. arXiv preprint 2106.12430, 2021.
[4] Austin, J., Johnson, D. D., Ho, J., Tarlow, D., and van den Berg, R. Structured denoising
diffusion models in discrete state-spaces. Neural Information Processing Systems (NeurIPS),
2021.
[5] Bao, F., Li, C., Zhu, J., and Zhang, B. Analytic-dpm: An analytic estimate of the optimal
reverse variance in diffusion probabilistic models. International Conference on Learning
Representations (ICLR), 2022.
[6] Ben-Hamu, H., Cohen, S., Bose, J., Amos, B., Grover, A., Nickel, M., Chen, R. T. Q., and
Lipman, Y. Matching normalizing flows and probability paths on manifolds. International
Conference on Machine Learning (ICML), 2022.
[7] Benamou, J.-D. and Brenier, Y. A computational fluid mechanics solution to the Monge-
Kantorovich mass transfer problem. Numerische Mathematik, 84(3):375–393, 2000.
[8] Brenier, Y. Polar factorization and monotone rearrangement of vector-valued functions. Com-
munications on Pure and Applied Mathematics, 44(4):375–417, 1991. doi: 10.1002/cpa.
3160440402.
[9] Bunne, C., Meng-Papaxanthos, L., Krause, A., and Cuturi, M. Proximal optimal transport
modeling of population dynamics. Artificial Intelligence and Statistics (AISTATS), 2022.
10

[10] Burkhardt, D., Bloom, J., Cannoodt, R., Luecken, M. D., Krishnaswamy, S., Lance, C., Pisco,
A. O., and Theis, F. J. Multimodal single-cell integration across time, individuals, and batches.
In NeurIPS Competitions, 2022.
[11] Chen, R. T. Q., Rubanova, Y., Bettencourt, J., and Duvenaud, D. Neural ordinary differential
equations. Neural Information Processing Systems (NeurIPS), 2018.
[12] Corso, G., Stärk, H., Jing, B., Barzilay, R., and Jaakkola, T. Diffdock: Diffusion steps, twists,
and turns for molecular docking. arXiv preprint 2210.01776, 2022.
[13] Cuturi, M. Sinkhorn distances: Lightspeed computation of optimal transport. Neural Informa-
tion Processing Systems (NIPS), 2013.
[14] Damodaran, B. B., Kellenberger, B., Flamary, R., Tuia, D., and Courty, N. DeepJDOT: Deep
joint distribution optimal transport for unsupervised domain adaptation. European Conference
on Computer Vision (ECCV), 2018.
[15] De Bortoli, V., Thornton, J., Heng, J., and Doucet, A. Diffusion Schrödinger bridge with
applications to score-based generative modeling. Neural Information Processing Systems
(NeurIPS), 2021.
[16] Dhariwal, P. and Nichol, A. Diffusion models beat GANs on image synthesis. Neural Informa-
tion Processing Systems (NeurIPS), 2021.
[17] Fatras, K., Zine, Y., Flamary, R., Gribonval, R., and Courty, N. Learning with minibatch
Wasserstein: Asymptotic and gradient properties. Artificial Intelligence and Statistics (AISTATS),
2020.
[18] Fatras, K., Séjourné, T., Courty, N., and Flamary, R. Unbalanced minibatch optimal transport;
applications to domain adaptation. International Conference on Machine Learning (ICML),
2021.
[19] Fatras, K., Zine, Y., Majewski, S., Flamary, R., Gribonval, R., and Courty, N. Minibatch optimal
transport distances; analysis and applications. arXiv preprint 2101.01792, 2021.
[20] Finlay, C., Jacobsen, J.-H., Nurbekyan, L., and Oberman, A. M. How to train your neural
ode: The world of jacobian and kinetic regularization. International Conference on Machine
Learning (ICML), 2020.
[21] Flamary, R., Courty, N., Gramfort, A., Alaya, M. Z., Boisbunon, A., Chambon, S., Chapel,
L., Corenflos, A., Fatras, K., Fournier, N., Gautheron, L., Gayraud, N. T. H., Janati, H.,
Rakotomamonjy, A., Redko, I., Rolet, A., Schutz, A., Seguy, V., Sutherland, D. J., Tavenard,
R., Tong, A., and Vayer, T. POT: Python Optimal Transport. Journal of Machine Learning
Research (JMLR), 22, 2021.
[22] Genevay, A., Peyre, G., and Cuturi, M. Learning generative models with Sinkhorn divergences.
Artificial Intelligence and Statistics (AISTATS), 2018.
[23] Goodfellow, I. J., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., Courville,
A., and Bengio, Y. Generative adversarial nets. Neural Information Processing Systems (NIPS),
2014.
[24] Grathwohl, W., Chen, R. T. Q., Bettencourt, J., Sutskever, I., and Duvenaud, D. Ffjord: Free-
form continuous dynamics for scalable reversible generative models. International Conference
on Learning Representations (ICLR), 2019.
[25] Ho, J., Jain, A., and Abbeel, P. Denoising diffusion probabilistic models. Neural Information
Processing Systems (NeurIPS), 2020.
[26] Hoffman, M. D. and Gelman, A. The No-U-turn sampler: adaptively setting path lengths in
Hamiltonian Monte Carlo. Journal of Machine Learning Research (JMLR), 15:1593–1623,
2011.
11

[27] Huguet, G., Magruder, D. S., Tong, A., Fasina, O., Kuchroo, M., Wolf, G., and Krishnaswamy,
S. Manifold interpolating optimal-transport flows for trajectory inference. Neural Information
Processing Systems (NeurIPS), 2022.
[28] Huguet, G., Tong, A., Zapatero, M. R., Wolf, G., and Krishnaswamy, S. Geodesic Sinkhorn:
Optimal transport for high-dimensional datasets. arXiv preprint 2211.00805, 2022.
[29] Kingma, D. P. and Welling, M. Auto-encoding variational bayes. International Conference on
Learning Representations (ICLR), 2014.
[30] Klambauer, G., Unterthiner, T., Mayr, A., and Hochreiter, S. Self-normalizing neural networks.
Neural Information Processing Systems (NIPS), 2017.
[31] Korotin, A., Li, L., Genevay, A., Solomon, J. M., Filippov, A., and Burnaev, E. Do neural
optimal transport solvers work? a continuous wasserstein-2 benchmark. Neural Information
Processing Systems (NeurIPS), 2021.
[32] Korotin, A., Selikhanovych, D., and Burnaev, E. Kenrnel neural optimal transport. International
Conference on Learning Representations (ICLR), 2023.
[33] Korotin, A., Selikhanovych, D., and Burnaev, E. Neural optimal transport. International
Conference on Learning Representations (ICLR), 2023.
[34] Lehmann, E. L. and Casella, G. Theory of point estimation. Springer Science & Business
Media, 2006.
[35] Léonard, C. Some properties of path measures. In Donati-Martin, C., Lejay, A., and Rouault, A.
(eds.), Séminaire de Probabilités XLVI, pp. 207–230. Springer, 2014.
[36] Léonard, C. A survey of the Schrödinger problem and some of its connections with optimal
transport. Discrete and Continuous Dynamical Systems, 34(4):1533–1574, 2014.
[37] Leygonie, J., She, J., Almahairi, A., Rajeswar, S., and Courville, A. Adversarial computation of
optimal transport maps. arXiv preprint 1906.09691, 2019.
[38] Li, X., Wong, T.-K. L., Chen, R. T. Q., and Duvenaud, D. Scalable gradients for stochastic
differential equations. Artificial Intelligence and Statistics (AISTATS), 2020.
[39] Lipman, Y., Chen, R. T. Q., Ben-Hamu, H., Nickel, M., and Le, M. Flow matching for generative
modeling. International Conference on Learning Representations (ICLR), 2023.
[40] Liu, Q. Rectified flow: A marginal preserving approach to optimal transport. arXiv preprint
2209.14577, 2022.
[41] Liu, X., Gong, C., and Liu, Q. Flow straight and fast: Learning to generate and transfer data
with rectified flow. International Conference on Learning Representations (ICLR), 2023.
[42] Liu, Z., Luo, P., Wang, X., and Tang, X. Deep learning face attributes in the wild. International
Conference on Computer Vision (ICCV), 2015.
[43] Loshchilov, I. and Hutter, F. Decoupled weight decay regularization. International Conference
on Learning Representations (ICLR), 2019.
[44] Lu, C., Zhou, Y., Bao, F., Chen, J., Li, C., and Zhu, J. Dpm-solver: A fast ode solver for
diffusion probabilistic model sampling in around 10 steps. Neural Information Processing
Systems (NeurIPS), 2022.
[45] Makkuva, A. V., Taghvaei, A., Oh, S., and Lee, J. D. Optimal transport mapping via input
convex neural networks. International Conference on Machine Learning (ICML), 2020.
[46] Mallasto, A., Gerolin, A., and Ha Quang, M. Entropy-regularized 2-Wasserstein distance
between Gaussian measures. Information Geometry, 5, 07 2022.
12

[47] Moon, K. R., van Dijk, D., Wang, Z., Gigante, S., Burkhardt, D. B., Chen, W. S., Yim, K.,
van den Elzen, A., Hirn, M. J., Coifman, R. R., Ivanova, N. B., Wolf, G., and Krishnaswamy, S.
Visualizing structure and transitions in high-dimensional biological data. Nature Biotechnology,
37(12):1482–1492, 2019.
[48] Neklyudov, K., Severo, D., and Makhzani, A. Action matching: A variational method for
learning stochastic dynamics from samples. arXiv preprint 2210.06662, 2022.
[49] Nichol, A. and Dhariwal, P. Improved denoising diffusion probabilistic models. International
Conference on Machine Learning (ICML), 2021.
[50] Onken, D., Fung, S. W., Li, X., and Ruthotto, L. OT-Flow: Fast and accurate continuous nor-
malizing flows via optimal transport. Association for the Advancement of Artificial Intelligence
(AAAI), 2021.
[51] Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z.,
Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison, M., Tejani,
A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., and Chintala, S. PyTorch: An imperative style,
high-performance deep learning library. Neural Information Processing Systems (NeurIPS),
2019.
[52] Peyré, G. and Cuturi, M. Computational optimal transport. Foundations and Trends in Machine
Learning, 11(5-6):355–607, 2019.
[53] Pooladian, A.-A., Ben-Hamu, H., Domingo-Enrich, C., Amos, B., Lipman, Y., and Chen,
R. T. Multisample flow matching: Straightening flows with minibatch couplings. International
Conference on Learning Representations (ICLR), 2023.
[54] Rezende, D. J. and Mohamed, S. Variational inference with normalizing flows. International
Conference on Machine Learning (ICML), 2015.
[55] Rozen, N., Grover, A., Nickel, M., and Lipman, Y. Moser flow: Divergence-based generative
modeling on manifolds. Neural Information Processing Systems (NeurIPS), 2021.
[56] Salimans, T. and Ho, J. Progressive distillation for fast sampling of diffusion models. Interna-
tional Conference on Learning Representations (ICLR), 2022.
[57] Schiebinger, G., Shu, J., Tabaka, M., Cleary, B., Subramanian, V., Solomon, A., Gould, J., Liu,
S., Lin, S., Berube, P., Lee, L., Chen, J., Brumbaugh, J., Rigollet, P., Hochedlinger, K., Jaenisch,
R., Regev, A., and Lander, E. S. Optimal-transport analysis of single-cell gene expression
identifies developmental trajectories in reprogramming. Cell, 176(4):928–943.e22, 2019.
[58] Schrödinger, E. Sur la théorie relativiste de l’électron et l’interprétation de la mécanique
quantique. Annales de l’Institut Henri Poincaré, 2(4):269–310, 1932.
[59] Shi, Y., De Bortoli, V., Deligiannidis, G., and Doucet, A. Conditional simulation using diffusion
Schrödinger bridges. Uncertainty in Artificial Intelligence (UAI), 2022.
[60] Sohl-Dickstein, J., Weiss, E. A., Maheswaranathan, N., and Ganguli, S. Deep unsupervised
learning using nonequilibrium thermodynamics. International Conference on Machine Learning
(ICML), 2015.
[61] Song, J., Meng, C., and Ermon, S. Denoising diffusion implicit models. International Confer-
ence on Learning Representations (ICLR), 2021.
[62] Song, Y. and Ermon, S. Generative modeling by estimating gradients of the data distribution.
Neural Information Processing Systems (NeurIPS), 2019.
[63] Song, Y. and Ermon, S. Improved techniques for training score-based generative models. Neural
Information Processing Systems (NeurIPS), 2020.
[64] Song, Y., Sohl-Dickstein, J., Kingma, D. P., Kumar, A., Ermon, S., and Poole, B. Score-based
generative modeling through stochastic differential equations. International Conference on
Learning Representations (ICLR), 2021.
13

[65] Sun, Y., Wang, X., and Tang, X. Deep learning face representation by joint identification-
verification. Neural Information Processing Systems (NIPS), 2014.
[66] Tong, A., Huang, J., Wolf, G., van Dijk, D., and Krishnaswamy, S. TrajectoryNet: A dynamic
optimal transport network for modeling cellular dynamics. International Conference on Machine
Learning (ICML), 2020.
[67] Tong, A., Malkin, N., Fatras, K., Atanackovic, L., Zhang, Y., Huguet, G., Wolf, G., and Bengio,
Y. Simulation-free schrödinger bridges via score and flow matching. 2023.
[68] Vargas, F., Thodoroff, P., Lawrence, N. D., and Lamacraft, A. Solving Schrödinger bridges via
maximum likelihood. Entropy, 23(9), 2021.
[69] Wang, G., Jiao, Y., Xu, Q., Wang, Y., and Yang, C. Deep generative learning via Schrödinger
bridge. International Conference on Machine Learning (ICML), 2021.
[70] Watson, D., Chan, W., Ho, J., and Norouzi, M. Learning fast samplers for diffusion models by
differentiating through sample quality. International Conference on Learning Representations
(ICLR), 2022.
[71] Watson, J. L., Juergens, D., Bennett, N. R., Trippe, B. L., Yim, J., Eisenach, H. E., Ahern, W.,
Borst, A. J., Ragotte, R. J., Milles, L. F., Wicky, B. I. M., Hanikel, N., Pellock, S. J., Courbet,
A., Sheffler, W., Wang, J., Venkatesh, P., Sappington, I., Torres, S. V., Lauko, A., De Bortoli,
V., Mathieu, E., Barzilay, R., Jaakkola, T. S., DiMaio, F., Baek, M., and Baker, D. Broadly
applicable and accurate protein design by integrating structure prediction networks and diffusion
generative models. bioRxiv preprint 2022.12.09.519842, 2022.
[72] Yeo, G. H. T., Saksena, S. D., and Gifford, D. K. Generative modeling of single-cell time
series with PRESCIENT enables prediction of cell trajectories with interventions. Nature
Communications, 12(1):3222, 2021.
[73] Zhang, Q. and Chen, Y. Path integral sampler: a stochastic control approach for sampling.
International Conference on Learning Representations (ICLR), 2022.
[74] Zhu, J.-Y., Park, T., Isola, P., and Efros, A. A. Unpaired image-to-image translation using
cycle-consistent adversarial networks. International Conference on Computer Vision (ICCV),
2017.
14

A
Proofs of theorems
Theorem 3.1. The marginal vector field (9) generates the probability path (8) from initial conditions
p0(x).
Proof of Theorem 3.1. To verify this, we first check that pt and ut satisfy the continuity equation.
We start with the derivative w.r.t. time of eq. 8
d
dtpt(x) = d
dt
Z
pt(x|z)q(z)dz
by Leibniz Rule,
=
Z
d
dt (pt(x|z)q(z)) dz
since ut(·|z) generates pt(·|z),
= −
Z
div (ut(x|z)pt(x|z)q(z)) dz
exchanging the derivative and integral,
= −div
Z
ut(x|z)pt(x|z)q(z)dz

Using eq. 9,
= −div (ut(x)pt(x))
satisfying the continuity equation d
dtpt(x) + div (ut(x)pt(x)) = 0.
Theorem 3.2. If pt(x) > 0 for all x ∈Rd and t ∈[0, 1], then, up to a constant independent of θ,
LCFM and LFM are equal, and hence
∇θLFM(θ) = ∇θLCFM(θ).
(11)
Proof of Theorem 3.2. For this proof we need eq. 8, eq. 9 and the existence and exchange of many
integrals. As in [39] we assume that q, pt(x|z) are decreasing to zero at sufficient speed as ∥x∥→∞
and that ut, vt, ∇θvt are bounded.
∇θEpt(x)∥vθ(t, x) −ut(x)∥2 = ∇θEpt(x)
 ∥vθ(t, x)∥2 −2 ⟨vθ(t, x), ut(x)⟩+ ∥ut(x)∥2
= ∇θEpt(x)
 ∥vθ(t, x)∥2 −2 ⟨vθ(t, x), ut(x)⟩

∇θEq(z),pt(x|z)∥vθ(t, x) −ut(x|z)∥2 =
∇θEq(z),pt(x|z)
 ∥vθ(t, x)∥2 −2 ⟨vθ(t, x), ut(x|z)⟩+ ∥ut(x|z)∥2
= Eq(z),pt(x|z)∇θ
 ∥vθ(t, x)∥2 −2 ⟨vθ(t, x), ut(x|z)⟩

By bilinearity of the 2-norm and since ut is independent of θ. Next,
Ept(x)∥vθ(t, x)∥2 =
Z
∥vθ(t, x)∥2pt(x)dx
=
ZZ
∥vθ(t, x)∥2pt(x|z)q(z)dzdx
= Eq(z),pt(x|z)∥vθ(t, x)∥2
Finally,
Ept(x) ⟨vθ(t, x), ut(x)⟩=
Z 
vθ(t, x),
R
ut(x|z)pt(x|z)q(z)dz
pt(x)

pt(x)dx
=
Z 
vθ(t, x),
Z
ut(x|z)pt(x|z)q(z)dz

dx
=
ZZ
⟨vθ(t, x), ut(x|z)⟩pt(x|z)q(z)dzdx
= Eq(z),pt(x|z) ⟨vθ(t, x), ut(x|z)⟩
Where we first substitute eq. 9 then change the order of integration for the final equality. Since at all
times t the gradients of LFM and LCFM are equal, ∇θLFM(θ) = ∇θLCFM(θ)
15

Proposition 3.3. The marginal pt corresponding to q(z) = q(x0)q(x1) and the pt(x|z), ut(x|z) in
eqs. 14 and 15 has boundary conditions p1 = q1 ∗N(x | 0, σ2) and p0 = q0 ∗N(x | 0, σ2), where
∗denotes the convolution operator.
Proof of Proposition 3.3. We start with eq. 8 to show the result of the lemma. We note that q(z) =
q((x0, x1)) = q(x0)q(x1)
pt(x) =
Z
pt(x|z)q(z)dz
=
Z
N(x |tx1 + (1 −t)x0, σ2)q((x0, x1))d(x0, x1)
=
ZZ
N(x |tx1 + (1 −t)x0, σ2)q(x0)q(x1)dx0dx1
evaluated at i = 0, 1 respectively. Therefore, at t = 0,
p0(x) =
ZZ
N(x |x0, σ2)q(x0)q(x1)dx0dx1
=
Z
N(x |x0, σ2)q(x0)dx0
= q(x0) ∗N(x |0, σ2).
This is also true for t = 1.
Proposition 3.4. The results of Prop. 3.3 also hold for q(z) in eq. 17. Furthermore, assuming
regularity properties of q0, q1, and the optimal transport plan π, as σ2 →0 the marginal path pt and
field ut minimize eq. 7, i.e., ut solves the dynamic optimal transport problem between q0 and q1.
Proof of Proposition 3.4. We will assume certain regularity conditions on q0, q1, and π to allow
reduction to known results. We leave it to future work to determine which of these conditions are
necessary and which are redundant with other conditions. However, because we are concerned with
approximation of ut(x) with neural networks, which are typically smooth, results that relax the
regularity assumptions may be vacuous in practice.
Preliminaries. We assume that q0 and q1 are compactly supported and admit bounded densities with
respect to the Lebesgue measure. Then the conditions for Brenier’s theorem [8] are satisfied. By
Brenier’s theorem, the optimal joint π is unique and is supported on the graph (x, T(x)) of a Monge
map T : Rd →Rd, and pt(x) is equal to McCann’s interpolation [52, Chapter 7]
pt = ((1 −t)Id + tT)#p0.
(22)
In addition, we know that T(x) can be parameterized as the gradient of a convex function, i.e.
T(x) = ∇ψ(x). This characterization of T implies that the conditional probability paths, given by
ϕt(x) = x + t(T(x) −x), do not cross, i.e., pt(x|x0, T(x0)) = pt(x) for all (t, x).4 It is known that
the probability path pt = [ϕt]#p0 and its associated vector field, given by ut(ϕt(x)) = T(x) −x,
solve the optimal transport problem [7, Proposition 1.1].
We assume that the induced marginals pt have bounded densities with respect to Lebesgue measure
and that T is almost everywhere continuous in x, which implies the same for ϕt. Injectivity of ϕt and
noncrossing of paths implies ut(ϕt(x)) is almost everywhere continuous in ϕt(x).
Formal statement of the result. Denote by pσ
t (x), pσ
t (x|z) the densities in Proposition 3.3, and by
pσ
t (x). We will show that for pt-almost every x and almost every t,
ut(x) = lim
σ→0 ut(x) = lim
σ→0
Ez∼q(z)pσ
t (x|z)ut(x|z)
pσ
t (x)
.
(23)
4Proof: If the paths from distinct x0 and x′
0 cross, so ϕt(x0) = ϕt(x′
0), then (1 −t)x0 + t∇ψ(x0) =
(1 −t)x′
0 + t∇ψ(x′
0). Taking dot product with x0 −x′
0, (t −1)∥x0 −x′
0∥2 = t⟨∇ψ(x0) −∇ψ(x′
0), x0 −x′
0⟩.
However, we have (t −1)∥x0 −x′
0∥2 < 0 and t⟨∇ψ(x0) −∇ψ(x′
0), x0 −x′
0⟩≥0 by convexity of ψ,
contradiction.
16

Proof. It suffices to show the equality for x in the support of pt, or in the image of the support of
q0 under ϕt. We identify z in the expectation with a pair (x0, T(x0)) due to q(x0, x1) = π(x0, x1)
having support on the graph of the Monge map. Noting that
ut

x|
 x0, T(x0)

= u0(x0) = T(x0) −x0
∀x,
we see that (23) is equivalent to
u0(ϕ−1
t (x)) = lim
σ→0
Eq(x0)pσ
t

x|
 x0, T(x0)

u0(x0)
pσ
t (x)
.
(24)
By the same argument as in the proof of Proposition 3.3, we have that pσ
t = pt ∗N(0, σ2), by
integration over x0 of the conditional equality pt

· |
 x0, T(x0)

= N(ϕt(x0), σ2) = δϕt(x0) ∗
N(0, σ2).
Symmetry of the Gaussian implies that
pσ
t (x|(x0, T(x0))) = pσ
t (ϕt(x0)|(ϕ−1
t (x), T(ϕ−1
t (x)))).
Therefore
Eq(x0)pσ
t (x|(x0, T(x0)))u0(x0) = Eq(x0)

pσ
t (ϕt(x0)|(ϕ−1
t (x), T(ϕ−1
t (x))))u0(x0)

[by change of variables x′ = ϕt(x0)] = Ept(x′)

pσ
t (x′|(ϕ−1
t (x), T(ϕ−1
t (x))))u0(ϕ−1
t (x′))

= Epσ
t (x′|(ϕ−1
t
(x),T (ϕ−1
t
(x))))

pt(x′)u0(ϕ−1
t (x′))

= E∆x∼N(0,σ2)

pt(x + ∆x)u0(ϕ−1
t (x + ∆x))

=
 pt(·)u0(ϕ−1
t (·)) ∗N(0, σ2)

(x).
The standard fact that N(0, σ2)
σ→0
−−−→δ0 in distribution implies that if f is a bounded, almost
everywhere continuous, compactly supported function, then (f ∗N)(x) →f(x) pointwise for almost
every x. By the hypotheses, pt and pt · (u0 ◦ϕ−1
t ) have this property. It follows that, for every t and
almost all x,
lim
σ→0
Eq(x0)pσ
t (x|(x0, T(x0))u0(x0)
pσ
t (x)
= lim
σ→0
 (pt · (u0 ◦ϕ−1
t )) ∗N(0, σ2)

(x)
(pt ∗N(0, σ2)) (x)
= (pt · (u0 ◦ϕ−1
t ))(x)
pt(x)
= u0(ϕ−1
t (x)),
which proves (24).
Proposition 3.5. The marginal vector field ut(x) defined by eq. 19 and eq. 21 generates the same
marginal probability path as the solution π∗to the SB problem in eq. 18.
Proof of Proposition 3.5. Using Theorem 2.4 of [35], [15] showed that the initial and terminal
marginals of π∗are the solution to the static OT problem
π∗(x0, x1) = arg min KL(π∗(x0, x1)∥pref(x0, x1)),
while the conditional path distributions π∗(−|x0, x1) minimize
Ex0,x1∼π∗(x0,x1)KL(π∗(−|x0, x1)∥pref(−|x0, x1)).
The optimization problem for π∗(x0, x1) is equivalent to the entropy-regularized optimal transport
problem with optimum π2σ2, as observed by [15]. (The key observation is that log pref(x0, x1) =
c(x0,x1)α
2σ2
+ const., where c(x, y) = ∥x −y∥and α = 2.) The divergences between conditional path
distributions are optimized by Brownian bridges with diffusion scale σ pinned at x0 and x1, which
are well-known to have marginal probability path pt in eq. 20, and, by eq. 5, are generated by the
vector fields ut in eq. 21.
17

B
Additional theoretical results
Proposition B.1. For any σ ∈R+ conditional flow matching with conditional probability paths
given by (16) has an equivalent marginal probability flow pt(x) to [39] flow matching.
Proof. To prove the proposition, we use the fact that the Gaussian family can be generated by
location-scale transformations [see e.g. 34], i.e. we can express any Gaussian Z0 ∼N(µ0, σ2
0)
as Z0 = µ0 + σ0Z where Z ∼N(0, 1). Recall that the density pt(x) has the form pt(x) =
R
pt(x|z)q(z)dz, to show the equivalence between the flow from FM and source conditional flow
matching, we have to show that pt(x|x1) is the same for both methods, that is we show that CFM
with variance (σt)2 + 2σt(1 −t) is equivalent to FM with variance (tσ −t + 1)2 (Def. 12). Since
pt(x|x0, x1) is N(tx1 +(1−t)x0), σt(σt−2t+2)) we can write the random variable X|X0, X1 as
X|X0, X1 = tx1 + (1 −t)x0 + (σt(σt −2t + 2))1/2Z,
where Z ∼N(0, 1). Without conditioning on X0, we have
X|X1 = tx1 + (1 −t)X0 + (σt(σt −2t + 2))1/2Z.
By assumption X0 ∼N(0, 1), thus X|X1 is Gaussian, since a linear transformation of Gaussian
distributions is also Gaussian. To define its distribution, we only have to define its expectation and
variance. By linearity of expectation, we find E(X|X1) = tx1, and by independence of X0 and Z
we have
Var(X|X1) = (1 −t)2Var(X0) + (σt(σt −2t + 2))Var(Z)
= (1 −t)2 + (σt(σt −2t + 2)) = (tσ −t + 1)2,
hence the flow from source conditional flow matching is the same as FM.
Proposition B.2. If π is a Monge map, the objective variance of OT-CFM goes to zero as σ →0, i.e.,
Eq(z)∥ut(x|z) −ut(x)∥2 →0 as σ →0
for ut(x|z) in eq. 15.
Proof. This follows from a basic fact about the transport plan π. Specifically, that as σ →0,
DKL(pt(x|zi)∥pt(x|zj)) →∞for an t, x for two distinct zi, zj. This means that pt(x|z) = pt(x)
for any t, x, z therefore
ut(x) = Eq(z)ut(x|z)pt(x|z)/pt(x)
= ut(x|z)
Proposition B.3. The conditional vector field ut(x|¯z) defined by 26 converges to marginal vector
field ut(x) defined by 9 as m goes to population size, i.e.,
∥ut(x|¯z) −ut(x)∥2 →0
as m →|X|.
Proof. As |z| →|X|, by definition,
ut(x|¯z) =
Pm
i ut(x|zi)pt(x|zi)q(zi)
Pm
i pt(x|zi)q(zi)
=
P
z∈X ut(x|z)pt(x|z)q(z)
P
z∈X pt(x|z)q(z)
= Eq(z)
ut(x|z)pt(x|z)
pt(x)
= ut(x)
18

C
Algorithm extensions
In Algorithm 1 we presented the general algorithm for conditional flow matching given q(z), pt(x|z),
ut(x|z). In Table 1 we presented a number of settings of these leading to interesting probability paths.
In practice, we may wish to compute q(z) on the fly. Therefore in Algorithms 2, 3, and 4, we give
algorithms for the simplified conditional flow matching, and minibatch versions of OT conditional
flow matching and Schrödinger bridge conditional flow matching. In general these consist of first
sampling a batch of data from both the source and the target empirical distributions, then resampling
pairs of data either randomly (CFM) or according to some OT plan, (OT-CFM and SB-CFM).
C.1
Variance reduction by averaging across batches
An interesting consequence of introducing optimal transport to conditional flow matching is that it
greatly reduces variance of the regression target. Informally, as σ →0, Ex,t,z∥ut(x|z)−ut(x)∥2 →0
for OT-CFM and SB-CFM, which is not true of previous probability paths in Table 1 (See Propo-
sition B.2 for a precise statement). As flow models get larger, more powerful, and more costly,
reducing objective variance, and thereby faster training may lead to significant cost savings [71]. To
this end we also explore reducing the variance of the objective by averaging over a batch. This is not
feasible in score matching where the flow conditioned on multiple datapoints is complex. Our CFM
framework naturally extends from a pair of datapoints to a batch of pairs. Instead of conditioning on
a single pair of datapoints we can condition on a batch of pairs. As the batch increases in size, we
trade higher cost in computing the target for lower variance in the target as the batch size increases,
the variance in the target goes to zero (see Proposition B.3 for a precise statement).
As formalized in Proposition B.3, we can reduce variance in the target by averaging over multiple
datapoints. Specifically, in this case we let ¯z := {zi := (xi
0, xi
1)}m
i=1, where zi are i.i.d. from q(z)
and
pt(x|¯z) =
Pm
i pt(x|zi)q(zi)
Pm
i q(zi)
(25)
ut(x|¯z) =
Pm
i ut(x|zi)pt(x|zi)q(zi)
pt(x| ¯z)
(26)
It takes roughly m times as long to compute the conditional target ut(x|¯z) but reduces the variance.
As the evaluation and backpropagation through vθ gets more difficult this tradeoff can be beneficial.
C.2
Modeling energy functions
If we have access to an energy function two (unnormalized) energy functions R{0,1} : Rd →R+ at
the endpoints instead of i.i.d. samples Xt ∼qt(xt), then the objective must be slightly modified. We
formulate the Energy Conditional Flow Matching Objective as
LECFM =Et,ˆq0(x0),ˆq1(x1),pt(x|x0,x1)
R0(x0)R1(x1)
ˆq0(x0)ˆq1(x1) ∥vθ(t, x) −ut(x|x0, x1)∥2
2

(27)
We can use this object to train a flow which matches the energies without access to samples. This is
formalized in the following theorem.
Proposition C.1. Assuming that ˆq{0,1}(x), pt(x) > 0 for all x ∈X and t ∈[0, 1] then the gradients
of LFM and LECFM with respect to θ are equal up to some multiplicative constant c.
∇θLFM(θ) = c∇θLECFM(θ)
(28)
Proof. Let z0 =
R
X R0(x)dx, and z1 =
R
X R1(x)dx then q(x0) = R0(x0)/z0, similarly q(x1) =
R1(x1)/z1, then
LECFM(θ) = Et,ˆq0(x0),ˆq1(x1),pt(x|x0,x1)
R0(x0)R1(x1)
ˆq0(x0)ˆq1(x1) ∥vθ(t, x) −ut(x|x0, x1)∥2
2

(29)
= z0z1Et,ˆq0(x0),ˆq1(x1),pt(x|x0,x1)
q0(x0)q0(x1)
ˆq0(x0)ˆq1(x1)∥vθ(t, x) −ut(x|x0, x1)∥2
2

(30)
= z0z1
Z
t,x0,x1,x

q0(x0)q1(x1)∥vθ(t, x) −ut(x|x0, x1)∥2
2

pt(x|x0, x1)dx0dx1dx
(31)
= z0z1LCFM(θ)
(32)
19

Algorithm 2 Simplified Conditional Flow Matching
Input: Empirical or samplable distributions q0, q1, bandwidth σ, batchsize b, initial network vθ.
while Training do
/* Sample batches of size b i.i.d. from the datasets
*/
x0 ∼q0(x0);
x1 ∼q1(x1)
t ∼U(0, 1)
µt ←tx1 + (1 −t)x0
x ∼N(µt, σ2I)
LCFM(θ) ←∥vθ(t, x) −(x1 −x0)∥2
θ ←Update(θ, ∇θLCFM(θ))
return vθ
Algorithm 3 Minibatch OT Conditional Flow Matching
Input: Empirical or samplable distributions q0, q1, bandwidth σ, batch size b, initial network vθ.
while Training do
/* Sample batches of size b i.i.d. from the datasets
*/
x0 ∼q0(x0);
x1 ∼q1(x1)
π ←OT(x1, x0)
(x0, x1) ∼π
t ∼U(0, 1)
µt ←tx1 + (1 −t)x0
x ∼N(µt, σ2I)
LCFM(θ) ←∥vθ(t, x) −(x1 −x0)∥2
θ ←Update(θ, ∇θLCFM(θ))
return vθ
where we use substitution for the first step and change the order of integration in the last step. With
an application of Theorem 3.2 the gradients are equivalent up to a factor of z0z1 which does not
depend on x.
Of course LECFM leaves the question of sampling open for high-dimensional spaces. Sampling
uniformly does not scale well to high dimensions, so for practical reasons we may want a different
sampling strategy.
We use this objective in Figure 11 with a uniform proposal distribution as a toy example of this type
of training.
D
Additional results
We start this section by the definition of the entropy regularized OT problem:
W(q0, q1)2
2,λ = inf
πλ∈Π
Z
X 2 c(x, y)2πλ(dx, dy) −λH(π),
(33)
where λ ∈R+ and H(π) =
R
ln π(x, y)dπ(dx, dy).
Regularized CNF tuning
Continuous normalizing flows with a path length penalty optimize a
relaxed form of a dynamic optimal transport problem [66, 20, 50]. Where dynamic optimal transport
solves for the optimal vector field in terms of average path length where the marginals at time t = 0
and t = 1 are constrained to equal two input marginals q0 and q1. Instead of this pair of hard
constraints, regularized CNFs instead set q0 := N(x | 0, 1) and optimize a loss of the form
L(x(t)) = −log p(x(t)) + λe
Z 1
0
∥vθ(t, x(t))∥2dt
(34)
where dx
dt = vθ(t, x(t)) and log p(x(T)) is defined as
log p(x(T)) = p(x(0)) +
Z T
0
∂log p(x(t))
∂t
dt = p(x(0)) +
Z T
0
−tr
 dvθ
dx(t)

dt
(35)
20

Algorithm 4 Minibatch Schrödinger Bridge Conditional Flow Matching
Input: Empirical or samplable distributions q0, q1, bandwidth σ, batch size b, initial network vθ.
while Training do
/* Sample batches of size b i.i.d. from the datasets
*/
x0 ∼q0(x0);
x1 ∼q1(x1)
π2σ2 ←Sinkhorn(x1, x0, 2σ2)
(x0, x1) ∼π2σ2
t ∼U(0, 1)
µt ←tx1 + (1 −t)x0
x ∼N(µt, σ2t(1 −t)I)
ut(x|z) ←
1−2t
2t(1−t)(x −(tx1 + (1 −t)x0)) + (x1 −x0)
▷From eq. 21
LCFM(θ) ←∥vθ(t, x) −ut(x|z)∥2
θ ←Update(θ, ∇θLCFM(θ))
return vθ
Figure 3: Evaluation of regularization strength of λe over 6 seeds in the range [0, 10−5, 102].
λe = 0.1 performs the best in terms of minimizing path length and test error. We call this model
"Regularized CNF".
where the second equality follows from the instantaneous change of variables theorem [11, Theorem
1]. In practice it is difficult to pick a λe which both produces flows with short paths and allows the
model to fit the data well. We analyze the effect of this parameter over three datasets in Figure 3. In
this figure we analyze the Normalized 2-Wasserstein to the target distribution (which approaches 1
with good fit), and the Normalized Path Energy (NPE). We find a tradeoff between short paths (Low
NPE) and good fit (Low 2-Wasserstein). We choose λe = 0.1 as a good tradeoff across datasets,
which has paths that are not too much longer than optimal but also fits the data well.
Ablation results on batch size.
Since we use Minibatch-OT for OT-CFM, when the minibatch size
is equal to one, then OT-CFM is equivalent to CFM. This effect can be seen in Figure 4, where over
four datasets, OT-CFM starts with equal path length and approximately equal 2-Wasserstein. Then
the normalized path energy decreases surprisingly quickly plateauing after batchsize reaches ∼64.
While the minibatch size needed to approximate the true dynamic optimal transport paths will vary
with dataset (for example in the moon-8gaussian case we need a larger batch size) it is still somewhat
surprising that such small batches are needed as this is less than 0.5% of the entire 10k point dataset
per batch.
21

Figure 4: µ ± σ of mean path length prediction error over 5 seeds. Lower is better. Introducing OT
to CFM batches straightens paths lowering cost towards the optimal W 2 as compared to a standard
random conditional flow matching network over all batch sizes.
Figure 5: Evaluation of the effect of σ for conditional flow matching models. When σ < 1 OT-CFM
The effect of σ on fit and path length.
Next we consider σ, the bandwidth parameter of the
Gaussian conditional probability path. In Figure 5 we study the effect of σ on the fit (top) and the
path energy (bottom). With σ > 1 methods start to underfit with high 2-Wasserstein error and either
very long or very short paths. As for specific models, SB-CFM becomes unstable with σ too small
due to the lack of convergence for the static Sinkhorn optimization with small regularization. FM and
CFM follow similar trends where they fit fairly well with σ ≤1 but have paths that are significantly
longer than optimal by 2-3x. OT-CFM maintains near optimal path energies and near optimal fit until
σ > 1.
Schrödinger bridge fit over simulation time.
In Figure 9 we compare the fit of Diffusion
Schrödinger Bridge model with SB-CFM conditioned on time. The Diffusion Schrödinger Bridge
seems to outperform SB-CFM early in the trajectory, however fails to fit the bridge after many
integration steps.
22

Figure 6: Estimated Objective Variance Eq. 36 for different methods with batch size 512, σ = 0.1
across datasets. OT-CFM and SB-CFM have significantly lower objective variance than CFM and
FM which have roughly equivalent objective variance.
Figure 7: Validation 2-Wasserstein distance against training time with variance reduction by aggre-
gation either with no aggregation (Batchsize 1) or aggregation over a minibatch (Batchsize 512).
Variance reduction leads to faster training, especially for CFM where the objective variance is natu-
rally larger than OT-CFM which sees a small performance gain.
Figure 8: Extended results from Figure 2 (left) over two more datasets. OT-CFM is still consistently
the fastest converging method.
Figure 9: 2-Wasserstein Error between trajectories and ground truth Schrödinger Bridge samples over
simulation time.
23

Table 7: Mean training time till convergence in 103 seconds over 5 seeds, with the exception of DSB,
trained over 1 seed. CFM variants and DSB are trained on a single CPU with 5GB of memory where
other baselines are given two CPUs and one GPU. CFM, with significantly fewer resources, still
trains the fastest.
8gaussians
moons-8gaussians
moons
scurve
mean
OT-CFM
1.284 ± 0.028
1.587 ± 0.204
1.464 ± 0.158
1.499 ± 0.157
1.484 ± 0.192
CFM
0.993 ± 0.021
1.102 ± 0.171
1.059 ± 0.158
1.008 ± 0.106
1.046 ± 0.132
FM
0.839 ± 0.096
—
1.076 ± 0.126
1.127 ± 0.123
1.014 ± 0.170
SB-CFM
0.713 ± 0.386
0.794 ± 0.293
1.143 ± 0.389
1.230 ± 0.424
0.935 ± 0.397
Reg. CNF
2.684 ± 0.052
—
9.154 ± 1.535
9.022 ± 3.207
8.021 ± 3.288
CNF
1.512 ± 0.234
—
17.124 ± 4.398
27.416 ± 13.299
18.810 ± 12.677
ICNN
3.712 ± 0.091
3.046 ± 0.496
2.558 ± 0.390
2.200 ± 0.034
2.912 ± 0.626
DSB
5.418 ± —
5.682 ± —
5.428 ± —
5.560 ± —
5.522 ± —
Figure 10: (left) Variance of the objective for varying batch size. OT-CFM has a lower variance across
batch sizes. (right) Validation 2-Wasserstein performance with batch averaging as in Appendix C.1.
Reducing variance improves training efficiency.
D.1
Objective variance.
We consider the variance of the objective ut(x|z) with respect to z. While for any x we have
Eq(z)ut(x|z) = ut(x), we find a lower second moment speeds up training. Specifically, we seek to
understand the effect of the second moment which we call the objective variance defined as
OV = Et∼U(0,1),x∼pt(x),z∼q(z)∥ut(x|z) −ut(x)∥2
(36)
on training speed for different objectives in Table 1. We estimate the variance on a small data with a
known ut(x). We examine this estimated objective variance and its effect on training convergence
in Figure 10, showing that either OT-CFM or variance reduced CFM with averaging over the batch
results in lower variance of the objective. This in turn leads to faster training times as shown on
the right. Averaging over a batch of data leads to faster training particularly for methods with high
objective variance (CFM) and less so for those with low (OT-CFM), which already trains quickly.
Variance in the conditional objective target ut(x|z) varies across models. In Figure 6 we study the
objective variance across CFM objective functions. Here we estimate the objective variance in eq. 36
as
Ex,t,z∥ut(x|z) −vθ(t, x)∥2
(37)
after training has converged. After training has converged vθ should be very close to ut(x) so we use
it as an empirical estimator of ut(x) to compute the variance. We find that across all datasets OT-CFM
and SB-CFM have at least an order of magnitude lower variance than CFM and FM objectives. This
correlates with faster training as measured by lower validation error in fewer steps for lower variance
models as seen in Figure 2 (left).
We examine the objective variance OV by conditioning ut(x|z) on a batch of pairs of data points,
¯z := {zi := (xi
0, xi
1)}m
i=1, we can reduce the variance of the OV objective to 0 for all models as
batchsize goes to population size. For the batchsize m range from 1 to the number of the population,
we uniformly sample m pairs of points zi and compute the probability pt(x|¯z) and the objective
ut(x|¯z) from equation 25 and 26.
24

Figure 11: Flows (green) from (a) moons to (b) 8-Gaussians unnormalized density function learned
using CFM with RWIS.
Table 8: Energy-based CFM results on the 10-dimensional funnel dataset: log-partition function
estimation bias (mean and standard deviation over 10 runs) and time to generate 6000 samples from
the trained ODE. With adaptive integration, OT-CFM requires fewer function evaluations. With a
fixed-interval solver, OT-CFM has lower discretization error, leading to a better estimate. PIS baseline
is from [73].
RWIS
MCMC
log ˆZ
R
time
log ˆZ
R
time
adaptive Dormand-Prince (tolerance 0.01) integration
CFM
−0.068 ± 0.041
26.6 ± 8.4s
0.029 ± 0.037
34.6 ± 6.0s
OT-CFM
−0.076 ± 0.098
13.3 ± 1.7s
0.009 ± 0.045
12.8 ± 1.2s
FM
−0.033 ± 0.057
26.5 ± 7.7s
0.027 ± 0.031
30.9 ± 5.8s
Euler (N = 10) integration
CFM
0.281 ± 0.202
4.0 ± 0.8s
0.336 ± 0.030
3.7 ± 0.7s
OT-CFM
−0.039 ± 0.030
4.2 ± 0.6s
0.146 ± 0.107
4.1 ± 0.8s
FM
0.176 ± 0.044
4.1 ± 0.7s
0.334 ± 0.066
3.9 ± 0.6s
Euler-Maruyama (N = 100) integration
PIS (SDE)
−0.018 ± 0.020
We also find that averaging over batches makes the network acheive a lower validation error in fewer
steps and in less walltime (Figure 7).
D.2
Energy-based CFM
We show how CFM and OT-CFM can be adapted to the case where we do not have access to samples
from the target distribution, but only an unnormalized density (equivalently, energy function) of the
target, R(x1) (Figure 11). We consider the 10-dimensional funnel dataset from [26]. We aim to learn
a flow from the 10-dimensional standard Gaussian to the energy function of the funnel. We consider
two algorithms, each of which has certain advantages:
(1) Reweighted importance sampling (RWIS): We construct a weighted batch of target points x1 by
sampling x1 ∼N(0, I) and assigning it a weight of R(x1)/N(x1; 0, I) normalized to sum to 1
over the batch. The FM and CFM objectives handle weighted samples in a trivial way (by simply
using the weights as q(x1) in Table 1), while OT-CFM treats the weights as target marginals in
constructing the OT plan between x0 and x1. We expect RWIS to perform well when batches
are large and the proposal and target distributions are sufficiently similar; otherwise, numerical
explosion of the importance weights can hinder learning.
(2) MCMC: We use samples from a long-run Metropolis-adjusted Langevin MCMC chain on the
target density as approximate target samples. We expect this method to perform well when the
MCMC mixes well; otherwise, modes of the target density may be missed.
25

As an evaluation metric, we use the estimation bias of the log-partition function using a reweighted
variational bound, following prior work that studied the problem using SDE modeling [73]. The
computation of this metric for CNFs is given in the Appendix (§E.6).
The results are shown in Table 8. When an adaptive ODE integrator is used, all algorithms achieve
similar results (no pair of mean log-partition function estimates is statistically distinguishable with
p < 0.1 under a Welch’s t-test) but OT-CFM is about twice as efficient as CFM and FM. However,
with a fixed computation budget for ODE integration, OT-CFM performs significantly better.
E
Experiment and implementation details
E.1
Physical experimental setup
All experiments were performed on a shared heterogenous high-performance-computing cluster.
This cluster is primarily composed of GPU nodes with RTX8000, A100, and V100 Nvidia GPUs.
Since the network and nodes are shared, other users may cause high variance in the training times of
models. However, we believe that the striking difference between the convergence times in Table 7
and combined with the CFM training setup with a single CPU and the baseline models trained with
two CPUS and a GPU, paints a clear picture as to how efficient CFM training is. Qualitatively, we feel
that most CFMs converge quite a bit more rapidly than these metrics would suggest, often converging
to a near optimal validation performance in minutes.
E.2
2D, single-cell, and Schrödinger bridge experimental setup
For all experiments we use the same architecture implemented in PyTorch [51]. We concatenate the
flattened input x ∈Rd and the time t as the d + 1 inputs to a network with three hidden layers of
width 64 interspersed with SELU activations [30] followed by a linear output layer of width d. This
forms our vθ for all experiments. For all 2D and single-cell experiments we train for 1000 epochs
and implement early stopping on the validation loss which checks the loss on a validation set every
10 epochs and stops training if there is no improvement for 30 epochs. We also set a time limit of
100 minutes for each CFM model. This is hit almost exclusively for SB-CFM models with small σ
which are unstable to train due to instabilities and non-convergence of the Sinkhorn [13] transport
plan optimization. We use the AdamW [43] optimizer with weight decay 10−5 with batchsize 512 by
default in 2D experiments and 128 in the single cell datasets. For OT-CFM and SB-CFM we use exact
linear programming EMD and Sinkhorn algorithms from the python optimal transport package [21]
For evaluation of trajectories unless otherwise noted we use the Runge-Kutta45 (rk4) ODE solver
with 101 timesteps from 0 to 1.
E.3
Variance reduction by averaging
We tackle the exploration of the effects of reducing variance of the target ut(x|z) from two directions.
The first is for small example where we can compute the ground truth ut(x) quickly, and the second
is in the setting of trained models where we can estimate ut(x) with vθ(t, x) after vθ has converged.
We first consider the convergence of each flow matching objective (OT-CFM, CFM, FM, SB-CFM)
to zero as a function of the batch size relative to the dataset size. This is done by first sampling t, x, z
then computing the true objective variance across many samples. This appears in Figure 10.
We next consider the effect of averaging over a batch to reduce the variance of the objective in
Figure 10 (right). Here Batchsize refers to the size of the batch we are averaging over. We aggregate
this into a single target so that the model sees a single d dimensional target vector for one sampled
x, t. This means that we can compare different aggregation sizes fairly.
E.4
Schrödinger bridge evaluation setup
To evaluate how well Schrödinger Bridge models actually model a Schrödinger Bridge, we constrain
ourselves to a small example with 1000 points. We note that the closed-form Schrödinger marginals
are known for discrete densities, for Gaussians [46], and can be constructed for two approximate
datasets [31], which present other ways of evaluating Schrödinger bridge performance. For any time
t we can sample from the ground truth Schrödinger bridge density pt(x) as
(x0, x1) ∼π2σ2
Xt ∼N(x | tx1 + (1 −t)x0, σt(1 −t))
26

We sample trajectories of length 20 from t = 0 to t = 1 by integrating over time from t = 0 to
t = 1. At each of the 18 intermediate timepoints we compute the 2-Wasserstein distance between a
sample of size 1000 from the trajectories at that time and the ground truth Xt as above at that time.
We reported the average across the 18 intermediate timepoints in Table 3 and plot the 2-Wasserstein
distance over time in Figure 9.
SB-CFM Model
We train SB-CFM with σ = 1 and batchsize=512 for each of the datasets. We
save 1000 trajectories from a test set integrated with the tsit5 solver with atol=rtol=1e-4.
Diffusion Schrödinger bridge model implementation details
We use the implementation
from [15]. Only the networks were changed for a fair comparison with CFM. The forward and
backward networks are composed of an MLP with three hidden layers of size 64, with SELU activa-
tions in between layers. We used a time and a positional encoders composed of two layers of size
16 and 32 with LeakyReLU activations has inputs to the score network. The architectures are the
same for the 2D examples and the single-cell examples (except for the input dimension). During
training, we set the variance (γ in the author’s code) to 0.001 and did 20 steps to discretize the
Langevin dynamic. We trained for 10k iterations with 10k particles and batch size of 512, for 20
iterative proportional fitting steps, and a learning rate set to 0.0001. For the interpolation task we
used the tenth timepoint from the Langevin dynamic with the backward network trained to go from
the distribution at time t −1 to t + 1. All trajectories are evaluated from the backward dynamic. We
use σ = 1 and batchsize=512.
E.5
Single-cell experimental setup
We strove to be consistent with the experimental setup of [66]. For the Embryoid body (EB) data, we
use the same processed artifact which contains the first 100 principal components of the data. For
our tests we truncate to the first five dimensions, then whiten (subtract mean and divide by standard
deviation) each dimension. For the Embryoid body (EB) dataset which consists of 5 timepoints
collected over 30 days we train separate models leaving out times 1, 2, 3 in turn. We train a CFM over
the full time scale (0-4). During testing we push forward all points Xt−1 to time t as a distribution to
test against.
For the Cite and Multi datasets these are sourced from the Multimodal Single-cell Integration
challenge at NeurIPS 2022, a NeurIPS challenge hosted on Kaggle where the task was multi-modal
prediction [10]. In this competition they used this data to investigate the predictability of RNA from
chromatin accessibility and protein expression from RNA. Here, we repurpose this data for the task of
time series interpolation. Both of these datasets consist of four timepoints from CD34+ hematopoietic
stem and progenitor cells (HSPCs) collected on days 2, 3, 4, and 7. For more information and the raw
data see the competition site.5 We preprocess this data slightly to remove patient specific effects by
focusing on a single donor (donor 13176), then we again compute the first five principal components
and again whiten each dimension to further normalize the data.
E.6
Energy-based CFM
The 10-dimensional funnel dataset is defined by x0 ∼N(0, 1), x1,...,9 ∼N(0, exp(x0)I). We
attempted to mimic the SDE model architecture from [73] for the flow model vθ(t, x). The time step
t is encoded with 128-dimensional Fourier features, then both x and t are independently processed
with two-layer MLPs. The two representations are concatenated and processed through another
three-layer MLP to make the prediction. All MLPs use GELU activation and have 128 units per
hidden layer. We trained all models with σ = 0.05 and learning rate 10−2, the highest at which they
were table, for 1500 batches of size 300, to be consistent with the settings from [73].
The importance-weighted estimate of the log-partition function is defined
log ˆZ = log 1
K
K
X
i=1
R(x(i)
1 )
N(x(i)
0 ; 0, I)

∂x1
∂x0

x0=x(i)
0
,
where x(i)
0
are independent samples from the source distribution and x(i)
1
is x(i)
0
pushed forward by
the flow (note that the Jacobian can be computed by differentiating the ODE integrator). We used
K = 6000 samples.
5https://www.kaggle.com/competitions/open-problems-multimodal/data
27

For MCMC, to be consistent with [73], we generated 15000 samples, each of which was seen 30
times in training. We used 1000 steps of Metropolis-adjusted Langevin sampling with ϵ linearly
decaying from 0.1 to 0.
The flow network used to generate Fig. 11 followed similar settings to those used in §5.1.
E.7
Unsupervised translation
We trained a vanilla convolutional VAE, with about 7 million parameters in the encoder, on CelebA
faces scaled to 128 × 128 resolution.
For the flow network vθ(t, x), we used a MLP with four hidden layers of 512 units and leaky ReLU
activations taking the 129-dimensional concatenation of x and t as input. All models CFM and
OT-CFM were trained for 5000 batches of size 256 and the Adam optimizer with learning rate
10−3. Integration was performed using the Dormand-Prince integrator with tolerance 10−3. For each
attribute, 1000 positive and negative images each were used as a held-out test set.
Figure 12 shows some examples of the learned trajectories.
E.8
Unconditional CIFAR-10 experiments
For the CIFAR-10 experiments we followed the setup as described in [39]. All methods were trained
with the same setup, only differeing in the choice of probability path. Since code has not been
released for this work, there are a few parameters which may differ. We summarize the setup here,
where the exact parameter choices can be seen in the source code.
We used the Adam optimizer with β1 = 0.9, β2 = 0.999, ϵ = 10−8, and no weight decay. We used
the UNet architecture from [16] with channels = 256, depth = 2, channels multiple = [1, 2, 2, 2],
heads = 4, heads channels = 64, attention resolution = 16, dropout = 0.0, batch size per gpu = 128,
gpus = 2, epochs = 2000, maximum learning rate = 5 × 10−4, minimum learning rate = 0, with a
learning schedule that increases linearly from the minimum to the maximum learning rate over the
first 200 epochs, and decays linearly from back to the minimum after that. We use σ = 10−4 for all
models. For sampling, we use 100-step Euler integration using the torchdyn package.
28

Figure 12: Image-to-image translation in the latent space of CelebA images: An OT-CNF is trained
to translate between latent encodings of images that are negative and positive for a given attribute.
The first column is a reconstructed encoding x0 of a real negative image. The next ten columns
are decodings of images along the flow trajectory with initial condition x0, with x1 shown in the
right column. Top row: not smiling →smiling, not male →male, showing the preservation of
image structure and other attributes. Bottom row: no mustache →mustache, not wearing necktie →
wearing necktie, showing partial failure modes. Both features are well-predicted by the latent vector,
but infrequent in the dataset and highly correlated with other attributes, such as ‘male’, leading to
unpredictable behaviour for out-of-distribution samples and modification of attributes different from
the target.
29

