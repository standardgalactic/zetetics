Journal of Machine Learning Research 12 (2011) 1349-1388
Submitted 4/10; Revised 1/11; Published 4/11
Faster Algorithms for Max-Product Message-Passing∗
Julian J. McAuley†
JULIAN.MCAULEY@NICTA.COM.AU
Tib´erio S. Caetano†
TIBERIO.CAETANO@NICTA.COM.AU
Statistical Machine Learning Group
NICTA
Locked Bag 8001
Canberra ACT 2601, Australia
Editor: Tommi Jaakkola
Abstract
Maximum A Posteriori inference in graphical models is often solved via message-passing algo-
rithms, such as the junction-tree algorithm or loopy belief-propagation. The exact solution to this
problem is well-known to be exponential in the size of the maximal cliques of the triangulated
model, while approximate inference is typically exponential in the size of the model’s factors. In
this paper, we take advantage of the fact that many models have maximal cliques that are larger than
their constituent factors, and also of the fact that many factors consist only of latent variables (i.e.,
they do not depend on an observation). This is a common case in a wide variety of applications
that deal with grid-, tree-, and ring-structured models. In such cases, we are able to decrease the
exponent of complexity for message-passing by 0.5 for both exact and approximate inference. We
demonstrate that message-passing operations in such models are equivalent to some variant of ma-
trix multiplication in the tropical semiring, for which we offer an O(N2.5) expected-case solution.
Keywords: graphical models, belief-propagation, tropical matrix multiplication
1. Introduction
It is well-known that exact inference in tree-structured graphical models can be accomplished ef-
ﬁciently by message-passing operations following a simple protocol making use of the distributive
law (Aji and McEliece, 2000; Kschischang et al., 2001). It is also well-known that exact inference
in arbitrary graphical models can be solved by the junction-tree algorithm; its efﬁciency is deter-
mined by the size of the maximal cliques after triangulation, a quantity related to the tree-width of
the graph.
Figure 1 illustrates an attempt to apply the junction-tree algorithm to some graphical models
containing cycles. If the graphs are not chordal ((a) and (b)), they need to be triangulated, or made
chordal (red edges in (c) and (d)). Their clique-graphs are then guaranteed to be junction-trees,
and the distributive law can be applied with the same protocol used for trees; see Aji and McEliece
(2000) for a beautiful tutorial on exact inference in arbitrary graphs. Although the models in these
∗. Preliminary versions of this work appeared in The 27th International Conference on Machine Learning (ICML 2010),
and the 13th International Conference on Artiﬁcial Intelligence and Statistics (AISTATS 2010), The NIPS 2009
Workshop on Learning with Orderings, The NIPS 2009 Workshop on Discrete Optimization in Machine Learning,
and in Learning and Intelligent Optimization (LION 4).
†. Also at Research School of Information Sciences and Engineering, Australian National University, Canberra ACT
0200, Australia.
c⃝2011 Julian J. McAuley and Tib´erio S. Caetano.

MCAULEY AND CAETANO
(a)
(b)
(c)
(d)
Figure 1: The models at left ((a) and (b)) can be triangulated ((c) and (d)) so that the junction-
tree algorithm can be applied. Despite the fact that the new models have larger maximal
cliques, the corresponding potentials are still factored over pairs of nodes only. Our
algorithms exploit this fact.
(a)
(b)
(c)
(d)
Figure 2: Some graphical models to which our results apply: factors conditioned upon observations
have fewer latent variables than purely latent factors. White nodes correspond to latent
variables, gray nodes to an observation. In other words, factors containing a gray node
encode the data likelihood, whereas factors containing only white nodes encode priors.
Expressed more simply, the ‘node potentials’ depend upon the observation, while the
‘edge potentials’ do not.
examples contain only pairwise factors, triangulation has increased the size of their maximal cliques,
making exact inference substantially more expensive. Hence approximate solutions in the original
graph (such as loopy belief-propagation, or inference in a loopy factor-graph) are often preferred
over an exact solution via the junction-tree algorithm.
Even when the model’s factors are the same size as its maximal cliques, neither exact nor ap-
proximate inference algorithms take advantage of the fact that many factors consist only of latent
variables. In many models, those factors that are conditioned upon the observation contain fewer
latent variables than the purely latent factors. Examples are shown in Figure 2. This encompasses
a wide variety of models, including grid-structured models for optical ﬂow and stereo disparity as
well as chain and tree-structured models for text or speech.
In this paper, we exploit the fact that the maximal cliques (after triangulation) often have po-
tentials that factor over subcliques, as illustrated in Figure 1. We will show that whenever this is
the case, the expected computational complexity of message-passing between such cliques can be
improved (both the asymptotic upper-bound and the actual runtime).
Additionally, we will show that this result can be applied in cliques whose factors that are
conditioned upon an observation contain fewer latent variables than those factors consisting purely
1350

FASTER ALGORITHMS FOR MAX-PRODUCT MESSAGE-PASSING
of latent variables; the ‘purely latent’ factors can be pre-processed ofﬂine, allowing us to achieve
the same beneﬁts as described in the previous paragraph.
We show that these properties reveal themselves in a wide variety of real applications.
A core operation encountered in the junction-tree algorithm is that of computing the inner-
product of two vectors va and vb. In the max-product semiring (used for MAP inference), the
‘inner-product’ becomes
max
i∈{1...N}{va[i]×vb[i]}.
(1)
Our results stem from the realization that while (Equation 1) appears to be a linear time operation,
it can be decreased to O(
√
N) (in the expected case) if we know the permutations that sort va and
vb (i.e., the order statistics of va and vb). These permutations can be obtained efﬁciently when the
model factorizes as described above.
Preliminary versions of this work have appeared in McAuley and Caetano (2009), McAuley and
Caetano (2010a), and McAuley and Caetano (2010b).
1.1 Summary of Results
A selection of the results to be presented in the remainder of this paper can be summarized as
follows:
• Our speedups apply to the operation of passing a single message. As a result, our method can
be used regardless of the message-passing protocol.
• We are able to lower the asymptotic expected running time of max-product message-passing
for any discrete graphical model whose cliques factorize into lower-order terms.
• The results obtained are exactly those that would be obtained by the traditional version of the
algorithm, that is, no approximations are used.
• Our algorithm also applies whenever factors that are conditioned upon an observation contain
fewer latent variables than those factors that are not conditioned upon an observation, as in
Figure 2 (in which case certain computations can be taken ofﬂine).
• For pairwise models satisfying the above properties, we obtain an expected speed-up of at
least Ω(
√
N) (assuming N states per node; Ωdenotes an asymptotic lower-bound). For exam-
ple, in models with third-order cliques containing pairwise terms, message-passing is reduced
from Θ(N3) to O(N2√
N), as in Figure 1(d). For pairwise models whose edge potential is not
conditioned upon an observation, message-passing is reduced from Θ(N2) to O(N
√
N), as in
Figure 2.
• For cliques composed of K-ary factors, the expected speed-up generalizes to at least Ω( 1
KN
1
K ),
though it is never asymptotically slower than the original solution.
• The expected-case improvement is derived under the assumption that the order statistics of
different factors are independent.
• If the different factors have ‘similar’ order statistics, the performance will be better than the
expected case.
1351

MCAULEY AND CAETANO
• If the different factors have ‘opposite’ order statistics, the performance will be worse than the
expected case, but is never asymptotically more expensive than the traditional version of the
algorithm.
Our results do not apply for every semiring (⊕,⊗), but only to those whose ‘addition’ oper-
ation deﬁnes an order (for example, min or max); we also assume that under this ordering, our
‘multiplication’ operator ⊗satisﬁes
a < b∧c < d ⇒a⊗c < b⊗d.
(2)
Thus our results certainly apply to the max-sum and min-sum (‘tropical’) semirings (as well as max-
product and min-product, assuming non-negative potentials), but not for sum-product (for example).
Consequently, our approach is useful for computing MAP-states, but cannot be used to compute
marginal distributions. We also assume that the domain of each node is discrete.
We shall initially present our algorithm in terms of pairwise graphical models such as those
shown in Figure 2. In such models message-passing is precisely equivalent to matrix-vector mul-
tiplication over our chosen semiring. Later we shall apply our results to models such as those in
Figure 1, wherein message-passing becomes some variant of matrix multiplication. Finally we shall
explore other applications besides message-passing that make use of tropical matrix multiplication
as a subroutine, such all-pairs shortest-path problems.
1.2 Related Work
There has been previous work on speeding-up message-passing algorithms by exploiting different
types of structure in certain graphical models. For example, Kersting et al. (2009) study the case
where different cliques share the same potential function. In Felzenszwalb and Huttenlocher (2006),
fast message-passing algorithms are provided for cases in which the potential of a 2-clique is only
dependent on the difference of the latent variables (which is common in some computer vision
applications); they also show how the algorithm can be made faster if the graphical model is a
bipartite graph. In Kumar and Torr (2006), the authors provide faster algorithms for the case in
which the potentials are truncated, whereas in Petersen et al. (2008) the authors offer speed-ups for
models that are speciﬁcally grid-like.
The latter work is perhaps the most similar in spirit to ours, as it exploits the fact that certain
factors can be sorted in order to reduce the search space of a certain maximization problem.
Another course of research aims at speeding-up message-passing algorithms by using ‘informed’
scheduling routines, which may result in faster convergence than the random schedules typically
used in loopy belief-propagation and inference in factor graphs (Elidan et al., 2006). This branch of
research is orthogonal to our own in the sense that our methods can be applied independently of the
choice of message passing protocol.
Another closely related paper is that of Park and Darwiche (2003). This work can be seen to
compliment ours in the sense that it exploits essentially the same type of factorization that we study,
though it applies to sum-product versions of the algorithm, rather than the max-product version that
we shall study. Kjærulff (1998) also exploits factorization within cliques of junction-trees, albeit a
different type of factorization than that studied here.
In Section 4, we shall see that our algorithm is closely related to a well-studied problem known
as ‘tropical matrix multiplication’ (Kerr, 1970). The worst-case complexity of this problem has been
studied in relation to the all-pairs shortest-path problem (Alon et al., 1997; Karger et al., 1993).
1352

FASTER ALGORITHMS FOR MAX-PRODUCT MESSAGE-PASSING
Example
description
A;B
capital letters refer to sets of nodes (or similarly, cliques);
A∪B;A∩B;A\B
standard set operators are used (A \ B denotes set differ-
ence);
dom(A)
the domain of a set; this is just the Cartesian product of
the domains of each element in the set;
P
bold capital letters refer to arrays;
x
bold lower-case letters refer to vectors;
x[a]
vectors are indexed using square brackets;
P[n]
similarly, square brackets are used to index a row of a 2-d
array,
P[n]
or a row of an (|n|+1)-dimensional array;
PX;va
superscripts are just labels, that is, PX is an array, va is a
vector;
va
constant subscripts are also labels, that is, if a is a con-
stant, then va is a constant vector;
xi;xA
variable subscripts deﬁne variables; the subscript deﬁnes
the domain of the variable;
n|X
if n is a constant vector, then n|X is the restriction of that
vector to those indices corresponding to variables in X
(assuming that X is an ordered set);
ΦA;ΦA(xA)
a function over the variables in a set A; the argument xA
will be suppressed if clear, given that ‘functions’ are es-
sentially arrays for our purposes;
Φi, j(xi,xj)
a function over a pair of variables (xi,xj);
ΦA(n|B;xA\B)
if one argument to a function is constant (here n|B), then
it becomes a function over fewer variables (in this case,
only xA\B is free);
Table 1: Notation
2. Background
The notation we shall use is brieﬂy deﬁned in Table 1. We shall assume throughout that the max-
product semiring is being used, though our analysis is almost identical for any suitable choice.
MAP-inference in a graphical model G consists of solving an optimization problem of the form
ˆx = argmax
x
∏
C∈C
ΦC(xC),
where C is the set of maximal cliques in G. This problem is often solved via message-passing
algorithms such as the junction-tree algorithm, loopy belief-propagation, or inference in a factor-
graph (Aji and McEliece, 2000; Weiss, 2000; Kschischang et al., 2001).
Often, the clique-potentials ΦC(xC) shall be decomposable into several smaller factors, that is,
ΦC(xC) = ∏
F⊆C
ΦF(xF).
1353

MCAULEY AND CAETANO
Some simple motivating examples are shown in Figure 3: a model for pose estimation from Sigal
and Black (2006), a ‘skip-chain CRF’ from Galley (2006), and a model for shape-matching from
Coughlan and Ferreira (2002). In each case, the triangulated model has third-order cliques, but the
potentials are only pairwise. Other examples have already been shown in Figure 1; analogous cases
are ubiquitous in many real applications.
It will often be more convenient to express our objective function as being conditioned upon
some observation, y. Thus our optimization problem becomes
ˆx(y) = argmax
x
∏
C∈C
ΦC(xC|y)
(3)
(for simplicity when we discuss ‘cliques’ we are referring to sets of latent variables).
Further factorization may be possible if we express (Equation 3) in terms of those factors that
depend upon the observation y, and those that do not:
ˆx(y) = argmax
x
∏
C∈C
n
∏
F⊆C
ΦF(xF)
|
{z
}
data-independent
× ∏
Q⊆C
ΦQ(xQ|y)
|
{z
}
data-dependent
o
,
We shall say that those factors that are not conditioned on the observation are ‘data-independent’.
Our results shall apply to message-passing equations in those cliques C where for each data-
independent factor F we have F ⊂C, or for each data-dependent factor Q we have Q ⊂C, that
is, when all F or all Q in C are proper subsets of C. In such cases we say that the clique C is
factorizable.
The fundamental step encountered in message-passing algorithms is deﬁned below. The mes-
sage from a clique X to an intersecting clique Y (both sets of latent variables) is deﬁned by
mX→Y(xX∩Y) = max
xX\Y
(
ΦX(xX) ∏
Z∈Γ(X)\Y
mZ→X(xX∩Z)
)
(4)
(where Γ(X) is the set of neighbors of the clique X, that is, the set of cliques that intersect with X).
If such messages are computed after X has received messages from all of its neighbors except Y
(i.e., Γ(X) \Y), then this deﬁnes precisely the update scheme used by the junction-tree algorithm.
The same update scheme is used for loopy belief-propagation, though it is done iteratively in a
randomized fashion.
After all messages have been passed, the MAP-state for a set of latent variables M (assumed to
be a subset of a single clique X) is computed using
mM(xM) = max
xX\M
(
ΦX(xX) ∏
Z∈Γ(X)
mZ→X(xX∩Z)
)
.
(5)
For cliques that are factorizable (according to our previous deﬁnition), both (Equation 4) and
(Equation 5) take the form
mM(xM) = max
xX\M
(
∏
F⊆X
ΦF(xF) ∏
Q⊆X
ΦQ(xQ|y)
)
.
(6)
1354

FASTER ALGORITHMS FOR MAX-PRODUCT MESSAGE-PASSING
(a)
(b)
(c)
Figure 3: (a) A model for pose reconstruction from Sigal and Black (2006); (b) A ‘skip-chain CRF’
from Galley (2006); (c) A model for deformable matching from Coughlan and Ferreira
(2002). Although the (triangulated) models have cliques of size three, their potentials
factorize into pairwise terms.
Note that we always have Z∩X ⊂X for messages Z →X, meaning that the presence of the messages
has no effect on the ‘factorizability’ of (Equation 6).
Algorithm 1 gives the traditional solution to this problem, which does not exploit the factor-
ization of ΦX(xX). This algorithm runs in Θ(N|X|), where N is the number of states per node, and
|X| is the size of the clique X (for a given xX, we treat computing ∏F⊂X ΦF(xF) as a constant time
operation, as our optimizations shall not modify this cost).
In the following sections, we shall consider the two types of factorizability separately: ﬁrst, in
Section 3, we shall consider cliques X whose messages take the form
mM(xM) = max
xX\M
(
ΦX(xX) ∏
Q⊂X
ΦQ(xQ|y)
)
.
We say that such cliques are conditionally factorizable (since all conditional terms factorize); ex-
amples are shown in Figure 2. Next, in Section 4, we consider cliques whose messages take the
form
mM(xM) = max
xX\M ∏
F⊂X
ΦF(xF).
We say that such cliques are latently factorizable (since terms containing only latent variables fac-
torize); examples are shown in Figure 1.
3. Optimizing Algorithm 1: Conditionally Factorizable Models
In order to specify a more efﬁcient version of Algorithm 1, we begin by considering the simplest
nontrivial conditionally factorizable model: a pairwise model in which each latent variable depends
upon the observation, that is,
ˆx(y) = argmax
x
∏
i∈N
Φi(xi|y)
|
{z
}
node potential
× ∏
(i, j)∈E
Φi, j(xi,xj)
|
{z
}
edge potential
.
(7)
This is the type of model depicted in Figure 2 and encompasses a large class of grid- and tree-
structured models. Using our previous deﬁnitions, we say that the node potentials are ‘data-dependent’,
whereas the edge potentials are ‘data-independent’.
1355

MCAULEY AND CAETANO
Algorithm 1 Brute-force computation of max-marginals
Input: a clique X whose max-marginal mM(xM) (where M ⊂X) we wish to compute; assume that
each node in X has domain {1...N}
1: for m ∈dom(M) {i.e., {1...N}|M|} do
2:
max := −∞
3:
for z ∈dom(X \M) do
4:
if ∏F⊂X ΦF(m|F;z|F) > max then
5:
max := ∏F⊂X ΦF(m|F;z|F)
6:
end if
7:
end for {this loop takes Θ(N|X\M|)}
8:
mM(m) := max
9: end for {this loop takes Θ(N|X|)}
10: Return: mM
Message-passing in models of the type shown in (Equation 7) takes the form
mA→B(xi) = Φi(xi|y)×max
x j Φ j(xj|y)×Φi, j(xi,xj)
(8)
(where A = {i, j} and B = {i,k}). Note once again that in (Equation 8) we are not concerned
solely with exact inference via the junction-tree algorithm. In many models, such as grids and
rings, (Equation 7) shall be solved approximately by means of either loopy belief-propagation, or
inference in a factor-graph, which consists of solving (Equation 8) according to protocols other than
the optimal junction-tree protocol.
It is useful to consider Φi, j in (Equation 8) as an N × N matrix, and Φ j as an N-dimensional
vector, so that solving (Equation 8) is precisely equivalent to matrix-vector multiplication in the
max-product semiring. For a particular value xi = q, (Equation 8) becomes
mA→B(q) = Φi(q|y)×max
x j Φ j(xj|y)
|
{z
}
va
×Φi, j(q,xj)
|
{z
}
vb
,
(9)
which is precisely the ‘max-product inner-product’ operation that we claimed was critical in Section
1.
As we have previously suggested, it will be possible to solve (Equation 9) efﬁciently if we
know the order statistics of va and vb, that is, if we know the permutations that sort Φ j and every
row of Φi, j in (Equation 8). Sorting Φ j takes Θ(N logN), whereas sorting every row of Φi, j takes
Θ(N2 logN) (Θ(N logN) for each of N rows). The critical point to be made is that Φi, j(xi,xj) does
not depend on the observation, meaning that its order statistics can be obtained ofﬂine in several
applications.
The following elementary lemma is the key observation required in order to solve (Equation 1),
and therefore (Equation 9) efﬁciently:
Lemma 1 For any index q, the solution to p = argmaxi∈{1...N} {va[i]×vb[i]} must have va[p] ≥va[q]
or vb[p] ≥vb[q]. Therefore, having computed va[q]×vb[q], we can ﬁnd ‘p’ by computing only those
products va[i]×vb[i] where either va[i] > va[q] or vb[i] > vb[q].
1356

FASTER ALGORITHMS FOR MAX-PRODUCT MESSAGE-PASSING
Algorithm 2 Find i such that va[i]×vb[i] is maximized
Input: two vectors va and vb, and permutation functions pa and pb that sort them in decreasing
order (so that va[pa[1]] is the largest element in va)
1: Initialize: start := 1, enda := p−1
a [pb[1]], endb := p−1
b [pa[1]] {if endb = k, then the largest
element in va has the same index as the kth largest element in vb}
2: best := pa[1], max := va[best]×vb[best]
3: if va[pb[1]]×vb[pb[1]] > max then
4:
best := pb[1], max := va[best]×vb[best]
5: end if
6: while start < enda {in practice, we could also stop if start < endb, but the version given here is
the one used for analysis in Appendix A} do
7:
start := start +1
8:
if va[pa[start]]×vb[pa[start]] > max then
9:
best := pa[start]
10:
max := va[best]×vb[best]
11:
end if
12:
if p−1
b [pa[start]] < endb then
13:
endb := p−1
b [pa[start]]
14:
end if
15:
{repeat lines 8–14, interchanging a and b}
16: end while {this loop takes expected time O(
√
N)}
17: Return: best
This observation is used to construct Algorithm 2. Here we iterate through the indices starting
from the largest values of va and vb, stopping once both indices are ‘behind’ the maximum value
found so far (which we then know is the maximum). This algorithm is demonstrated pictorially
in Figure 4. Note that Lemma 1 only depends upon the relative values of elements in va and vb,
meaning that the number of computations that must be performed is purely a function of their order
statistics (i.e., it does not depend on the actual values of va or vb).
If Algorithm 2 can solve (Equation 9) in O( f(N)), then we can solve (Equation 8) in O(N f(N)).
Determining precisely the running time of Algorithm 2 is not trivial, and will be explored in depth
in Appendix A. At this stage we shall state an upper-bound on the true complexity in the following
theorem:
Theorem 2 The expected running time of Algorithm 2 is O(
√
N), yielding a speed-up of at least
Ω(
√
N) in cliques containing pairwise factors. This expectation is derived under the assumption
that va and vb have independent order statistics.
Algorithm 3 uses Algorithm 2 to solve (Equation 8), where we assume that the order statistics
of the rows of Φi, j have been obtained ofﬂine.
While the ofﬂine cost of sorting is not problematic in situations where the model is to be re-
peatedly reused on several observations, it can be avoided in two situations. Firstly, many models
have a ‘homogeneous’ prior, that is, the same prior is shared amongst every edge (or clique) of the
model. In such cases, only a single ‘copy’ of the prior needs to be sorted, meaning that in any model
1357

MCAULEY AND CAETANO
Step 1:















 6
 2
14 16  9
 7
12  8
10  3
11 13  1
15  4
 5
99 92 87 81 78 66 53 46 30 26 21 16 12 10  8
 6
 3
 4
 8
11  7
16 13  9
 6
 2
15 10 12  5
 1
14
98 93 85 76 71 70 67 65 63 57 48 42 39 37 26 17
don't search past this line
Step 2:















 6
 2
14 16  9
 7
12  8
10  3
11 13  1
15  4
 5
99 92 87 81 78 66 53 46 30 26 21 16 12 10  8
 6
 3
 4
 8
11  7
16 13  9
 6
 2
15 10 12  5
 1
14
98 93 85 76 71 70 67 65 63 57 48 42 39 37 26 17
Step 3:















 6
 2
14 16  9
 7
12  8
10  3
11 13  1
15  4
 5
99 92 87 81 78 66 53 46 30 26 21 16 12 10  8
 6
 3
 4
 8
11  7
16 13  9
 6
 2
15 10 12  5
 1
14
98 93 85 76 71 70 67 65 63 57 48 42 39 37 26 17
Step 4:















 6
 2
14 16  9
 7
12  8
10  3
11 13  1
15  4
 5
99 92 87 81 78 66 53 46 30 26 21 16 12 10  8
 6
 3
 4
 8
11  7
16 13  9
 6
 2
15 10 12  5
 1
14
98 93 85 76 71 70 67 65 63 57 48 42 39 37 26 17
Step 5:















 6
 2
14 16  9
 7
12  8
10  3
11 13  1
15  4
 5
99 92 87 81 78 66 53 46 30 26 21 16 12 10  8
 6
 3
 4
 8
11  7
16 13  9
 6
 2
15 10 12  5
 1
14
98 93 85 76 71 70 67 65 63 57 48 42 39 37 26 17
Figure 4: Algorithm 2, explained pictorially. The arrows begin at pa[start] and pb[start]; the red
dashed line connects enda and endb, behind which we need not search; a dashed arrow
is used when a new maximum is found. Note that in the event that va and vb contain
repeated elements, they can be sorted arbitrarily.
containing Ω(logN) edges, speed improvements can be gained over the na¨ıve implementation. Sec-
ondly, where an iterative algorithm (such as loopy belief-propagation) is to be used, the sorting step
need only take place prior to the ﬁrst iteration; if Ω(logN) iterations of belief-propagation are to
be performed (or in a homogeneous model, if the number of edges multiplied by the number of
1358

FASTER ALGORITHMS FOR MAX-PRODUCT MESSAGE-PASSING
Algorithm 3 Solve (Equation 8) using Algorithm 2
Input: a potential Φi, j(a,b)×Φi(a|yi)×Φ j(b|yj) whose max-marginal mi(xi) we wish to compute,
and a set of permutation functions P such that P[i] sorts the ith row of Φi, j (in decreasing order).
1: compute the permutation function pa by sorting Ψ j {takes Θ(N logN)}
2: for q ∈{1...N} do
3:
(va,vb) := (Ψ j,Φi, j(q,xj|yi,yj))
4:
best := Algorithm2(va,vb, pa,P[q]) {O(
√
N)}
5:
mA→B(q) := Φi(q)×Φ j(best)×Φi, j(q,best|yi,yj)
6: end for {this loop takes expected time O(N
√
N)}
7: Return: mA→B
iterations is Ω(logN)), we shall again gain speed improvements even when the sorting step is done
online.
In fact, the second of these conditions obviates the need for ‘conditional factorizability’ (or
‘data-independence’) altogether. In other words, in any pairwise model in which Ω(logN) iterations
of belief-propagation are to be performed, the pairwise terms need to be sorted only during the ﬁrst
iteration. Thus these improvements apply to those models in Figure 1, so long as the number of
iterations of belief-propagation is Ω(logN).
4. Latently Factorizable Models
Just as we considered the simplest conditionally factorizable model in Section 3, we now consider
the simplest nontrivial latently factorizable model: a clique of size three containing pairwise factors.
In such a case, our aim is to compute
mi, j(xi,xj) = max
xk Φi, j,k(xi,xj,xk),
(10)
which we have assumed takes the form
mi, j(xi,xj) = max
xk Φi, j(xi,xj)×Φi,k(xi,xk)×Φ j,k(xj,xk).
For a particular value of (xi,xj) = (a,b), we must solve
mi, j(a,b) = Φi, j(a,b)×max
xk Φi,k(a,xk)
|
{z
}
va
×Φ j,k(b,xk)
|
{z
}
vb
,
(11)
which again is in precisely the form shown in (Equation 1).
Just as (Equation 8) resembled matrix-vector multiplication, there is a close resemblance be-
tween (Equation 11) and the problem of matrix-matrix multiplication in the max-product semiring
(often referred to as ‘tropical matrix multiplication’, ‘funny matrix multiplication’, or simply ‘max-
product matrix multiplication’). While traditional matrix multiplication is well-known to have a
subcubic worst-case solution (see Strassen, 1969), the version in (Equation 11) has no known sub-
cubic solution (the fastest known solution is O(N3/logN), but there is no known solution that runs
in O(N3−ε) (Chan, 2007); Kerr (1970) shows that no subcubic solution exists under certain mod-
els of computation). The worst-case complexity of solving (Equation 11) can also be shown to be
1359

MCAULEY AND CAETANO
Algorithm 4 Use Algorithm 2 to compute the max-marginal of a 3-clique containing pairwise fac-
tors
Input: a
potential
Φi, j,k(a,b,c) = Φi, j(a,b) × Φi,k(a,c) × Φ j,k(b,c)
whose
max-marginal
mi, j(xi,xj) we wish to compute
1: for n ∈{1...N} do
2:
compute Pi[n] by sorting Φi,k(n,xk) {takes Θ(N logN)}
3:
compute P j[n] by sorting Φ j,k(n,xk) {Pi and P j are N × N arrays, each row of which is
a permutation; Φi,k(n,xk) and Φ j,k(n,xk) are functions over xk, since n is constant in this
expression}
4: end for {this loop takes Θ(N2 logN)}
5: for (a,b) ∈{1...N}2 do
6:
(va,vb) :=
 Φi,k(a,xk),Φ j,k(b,xk)

7:
(pa, pb) :=
 Pi[a],P j[b]

8:
best := Algorithm2(va,vb, pa, pb) {takes O(
√
N)}
9:
mi, j(a,b) := Φi, j(a,b)×Φi,k(a,best)×Φ j,k(b,best)
10: end for {this loop takes O(N2√
N)}
{the total running time is O(N2 logN +N2√
N), which is dominated by O(N2√
N)}
11: Return: mi, j
equivalent to the all-pairs shortest-path problem, which is studied in Alon et al. (1997). Although
we shall not improve the worst-case complexity, Algorithm 2 leads to far better expected-case per-
formance than existing solutions.
In principle Strassen’s algorithm could be used to perform sum-product inference in the set-
ting we discuss here, and indeed there has been some work on performing sum-product infer-
ence in graphical models that factorize (Park and Darwiche, 2003). Interestingly, there is also
a sub-quadratic solution to sum-product matrix-vector multiplication that requires preprocessing
(Williams, 2007), that is, the sum-product version of the setting we discussed in Section 3.
A prescription of how Algorithm 2 can be used to solve (Equation 10) is given in Algorithm 4.
As we mentioned in Section 3, the expected-case running time of Algorithm 2 is O(
√
N), meaning
that the time taken to solve Algorithm 4 is O(N2√
N).
5. Extensions
So far we have only considered the case of pairwise graphical models, though as mentioned our
results can in principle be applied to any conditionally or latently factorizable models, no matter the
size of the factors. Essentially our results about matrices become results about tensors. We ﬁrst treat
latently factorizable models, after which the same ideas can be applied to conditionally factorizable
models.
5.1 An Extension to Higher-Order Cliques with Three Factors
The simplest extension that we can make to Algorithms 2, 3, and 4 is to note that they can be
applied even when there are several overlapping terms in the factors. For instance, Algorithm 4 can
1360

FASTER ALGORITHMS FOR MAX-PRODUCT MESSAGE-PASSING
Step 1:















(1,2)
99 92 87 81 78 66 53 46 30 26 21 16 12 10  8
 6
98 93 85 76 71 70 67 65 63 57 48 42 39 37 26 17
(4,2) (3,2) (4,4) (2,1) (1,3) (3,4) (2,4) (2,2) (4,3) (2,3) (3,1) (4,1) (3,3) (1,4) (1,1)
(4,3) (1,4) (2,4) (2,3) (1,3) (4,4) (3,1) (2,1) (1,2) (4,2) (3,3) (2,2) (3,4) (1,1) (4,1) (3,2)
Figure 5: The reasoning applied in Algorithm 2 applies even when the elements of pa and pb are
multidimensional indices.
be adapted to solve
mi, j(xi,xj) = max
xk,xm Φi, j(xi,xj)×Φi,k,m(xi,xk,xm)×Φ j,k,m(xj,xk,xm),
(12)
and similar variants containing three factors. Here both xk and xm are shared by Φi,k,m and Φ j,k,m. We
can follow precisely the reasoning of the previous section, except that when we sort Φi,k,m (similarly
Φ j,k,m) for a ﬁxed value of xi, we are now sorting an array rather than a vector (Algorithm 4, lines 2
and 3); in this case, the permutation functions pa and pb in Algorithm 2 simply return pairs of
indices. This is illustrated in Figure 5. Effectively, in this example we are sorting the variable xk,m
whose domain is dom(xk)×dom(xm), which has state space of size N2.
As the number of shared terms increases, so does the improvement to the running time. While
(Equation 12) would take Θ(N4) to solve using Algorithm 1, it takes only O(N3) to solve using
Algorithm 4 (more precisely, if Algorithm 2 takes O( f(N)), then (Equation 12) takes O(N2 f(N2)),
which we have mentioned is O(N2√
N2) = O(N3)). In general, if we have S shared terms, then the
running time is O(N2√
NS), yielding a speed-up of Ω(
√
NS) over the na¨ıve solution of Algorithm 1.
5.2 An Extension to Higher-Order Cliques with Decompositions Into Three Groups
By similar reasoning, we can apply our algorithm to cases where there are more than three factors, in
which the factors can be separated into three groups. For example, consider the clique in Figure 6(a),
which we shall call G (the entire graph is a clique, but for clarity we only draw an edge when the
corresponding nodes belong to a common factor). Each of the factors in this graph have been
labeled using either differently colored edges (for factors of size larger than two) or dotted edges
(for factors of size two), and the max-marginal we wish to compute has been labeled using colored
nodes. We assume that it is possible to split this graph into three groups such that every factor is
contained within a single group, along with the max-marginal we wish to compute (Figure 6, (b)).
If such a decomposition is not possible, we will have to resort to further extensions to be described
in Section 5.3.
Ideally, we would like these groups to have size ≃|G|/3, though in the worst case they will
have size no larger than |G| −1. We call these groups X, Y, Z, where X is the group containing
the max-marginal M that we wish to compute. In order to simplify the analysis of this algorithm,
we shall express the running time in terms of the size of the largest group, S = max(|X|,|Y|,|Z|),
and the largest difference, S\ = max(|Y \ X|,|Z \ X|). The max-marginal can be computed using
Algorithm 5.
The running times shown in Algorithm 5 are loose upper-bounds, given for the sake of express-
ing the running time in simple terms. More precise running times are given in Table 2; any of the
1361

MCAULEY AND CAETANO
5
6
8
7
4
3
2
1
(a)
(b)
(a) We begin with a set of factors (indicated using colored lines), which are assumed to belong to some clique in our
model; we wish to compute the max-marginal with respect to one of these factors (indicated using colored nodes); (b)
The factors are split into three groups, such that every factor is entirely contained within one of them (Algorithm 5, line 1).
(c)
(d)
(e)
(c) Any nodes contained in only one of the groups are marginalized (Algorithm 5, lines 2, 3, and 4); the problem is now
very similar to that described in Algorithm 4, except that nodes have been replaced by groups; note that this essentially
introduces maximal factors in Y ′ and Z′; (d) For every value (a,b) ∈dom(x3,x4), ΨY (a,b,x6) is sorted (Algorithm 5,
lines 5–7); (e) For every value (a,b) ∈dom(x2,x4), ΨZ(a,b,x6) is sorted (Algorithm 5, lines 8–10).
c
b
a
M
(f)
(g)
(f) For every n ∈dom(X′), we choose the best value of x6 by Algorithm 2 (Algorithm 5, lines 11–16); (g) The result is
marginalized with respect to M (Algorithm 5, line 17).
Figure 6: Algorithm 5, explained pictorially. In this case, the most computationally intensive step
is the marginalization of Z (in step (c)), which takes Θ(N5). However, the algorithm can
actually be applied recursively to the group Z, resulting in an overall running time of
O(N4√
N), for a max-marginal that would have taken Θ(N8) to compute using the na¨ıve
solution of Algorithm 1.
1362

FASTER ALGORITHMS FOR MAX-PRODUCT MESSAGE-PASSING
Algorithm 5 Compute the max-marginal of G with respect to M, where G is split into three groups
Input: potentials ΦG(x) = ΦX(xX)×ΦY(xY)×ΦZ(xZ); each of the factors should be contained in
exactly one of these terms, and we assume that M ⊆X (see Figure 6)
1: Deﬁne: X′ := ((Y ∪Z)∩X)∪M; Y ′ := (X ∪Z)∩Y; Z′ := (X ∪Y)∩Z {X′ contains the variables
in X that are shared by at least one other group; alternately, the variables in X \X′ appear only
in X (sim. for Y ′ and Z′)}
2: compute ΨX(xX′) := maxX\X′ ΦX(xX) {we are marginalizing over those variables in X that
do not appear in any of the other groups (or in M); this takes Θ(NS) if done by brute-force
(Algorithm 1), but may also be done by a recursive call to Algorithm 5}
3: compute ΨY(xY ′) := maxY\Y ′ ΦY(xY)
4: compute ΨZ(xZ′) := maxZ\Z′ ΦZ(xZ)
5: for n ∈dom(X ∩Y) do
6:
compute PY[n] by sorting ΨY(n;xY ′\X) {takes Θ(S\NS\ logN); ΨY(n;xY ′\X) is free over
xY ′\X, and is treated as an array by ‘ﬂattening’ it; PY[n] contains the |Y ′ \X| = |(Y ∩Z)\X|-
dimensional indices that sort it}
7: end for {this loop takes Θ(S\NS logN)}
8: for n ∈dom(X ∩Z) do
9:
compute PZ[n] by sorting ΨZ(n;xZ′\X)
10: end for {this loop takes Θ(S\NS logN)}
11: for n ∈dom(X′) do
12:
(va,vb) :=
 ΨY(n|Y ′;xY ′\X′),ΨZ(n|Z′;xZ′\X′)

{n|Y ′ is the ‘restriction’ of the vector n to those
indices inY ′ (meaning that n|Y ′ ∈dom(X′∩Y ′)); hence ΨY(n|Y ′;xY ′\X′) is free in xY ′\X′, while
n|Y ′ is ﬁxed}
13:
(pa, pb) :=
 PY[n|Y ′],PZ[n|Z′]

14:
best := Algorithm2(va,vb, pa, pb) {takes O(pS\)}
15:
mX(n) := ΨX(n)×ΨY(best;n|Y ′)×ΨZ(best;n|Z′)
16: end for
17: mM(xM) := Algorithm1(mX,M) {i.e., we are using Algorithm 1 to marginalize mX(xX) with
respect to M; this takes Θ(NS)}
terms shown in Table 2 may be dominant. Some example graphs, and their resulting running times
are shown in Figure 7.
5.2.1 APPLYING ALGORITHM 5 RECURSIVELY
The marginalization steps of Algorithm 5 (lines 2, 3, and 4) may further decompose into smaller
groups, in which case Algorithm 5 can be applied recursively. For instance, the graph in Figure 7(a)
represents the marginalization step that is to be performed in Figure 6(c) (Algorithm 5, line 4). Since
this marginalization step is the asymptotically dominant step in the algorithm, applying Algorithm 5
recursively lowers the asymptotic complexity.
Another straightforward example of applying recursion in Algorithm 5 is shown in Figure 8,
in which a ring-structured model is marginalized with respect to two of its nodes. Doing so takes
O(MN2√
N); in contrast, solving the same problem using the junction-tree algorithm (by triangulat-
ing the graph) would take Θ(MN3). Loopy belief-propagation takes Θ(MN2) per iteration, meaning
1363

MCAULEY AND CAETANO
Description
lines
time
Marginalization of ΦX, without recursion
2
Θ(N|X|)
Marginalization of ΦY
3
Θ(N|Y|)
Marginalization of ΦZ
4
Θ(N|Z|)
Sorting ΦY
5–7
Θ(|Y ′\X|N|Y ′| logN)
Sorting ΦZ
8–10
Θ(|Z′\X|N|Z′| logN)
Running Algorithm 2 on the sorted values
11–16
O(N|X′|√
N|(Y ′∩Z′)\X′|)
Table 2: Detailed running time analysis of Algorithm 5; any of these terms may be asymptotically
dominant
Graph:
{A complete
graph KM,
with pairwise
terms}
(a)
(b)
(c)
(d)
(e)
Algorithm 1:
Θ(N5)
Θ(N3)
Θ(N11)
Θ(N6)
Θ(NM)
Algorithm 5:
O(N3√
N)
O(N2√
N)
O(N6√
N)
O(N5)
O(N5M/6)
Speed-up:
Ω(N
√
N)
Ω(
√
N)
Ω(N4√
N)
Ω(N)
Ω(NM/6)
Figure 7: Some example graphs whose max-marginals are to be computed with respect to the col-
ored nodes, using the three regions shown. Factors are indicated using differently colored
edges, while dotted edges always indicate pairwise factors. (a) is the region Z from Fig-
ure 6 (recursion is applied again to achieve this result); (b) is the graph used to motivate
Algorithm 4; (c) shows a query in a graph with regular structure; (d) shows a complete
graph with six nodes; (e) generalizes this to a clique with M nodes.
that our algorithm will be faster if the number of iterations is Ω(
√
N). Naturally, Algorithm 4 could
be applied directly to the triangulated graph, which would again take O(MN2√
N).
5.3 A General Extension to Higher-Order Cliques
Naturally, there are cases for which a decomposition into three terms is not possible, such as
mi, j,k(xi,xj,xk) = max
xm Φi, j,k(xi,xj,xk)×Φi, j,m(xi,xj,xm)×
Φi,k,m(xi,xk,xm)×Φ j,k,m(xj,xk,xm)
(13)
(i.e., a clique of size four with all possible third-order factors). However, if the model contains
factors of size K, it must always be possible to split it into K + 1 groups (e.g., four in the case of
Equation 13).
Our optimizations can easily be applied in these cases simply by adapting Algorithm 2 to solve
problems of the form
max
i∈{1...N}{v1[i]×v2[i]×···×vK[i]}.
(14)
1364

FASTER ALGORITHMS FOR MAX-PRODUCT MESSAGE-PASSING
O(N2)
+
O(2N2√
N)
+
O(4N2√
N) (by Algorithm 4)
Figure 8: In the above example, lines 2–4 of Algorithm 5 are applied recursively, achieving a total
running time of O(MN2√
N) for a loop with M nodes (our algorithm achieves the same
running time in the triangulated graph).
Step 1:































 6
 2
14 16  9
 7
12  8
10  3
11 13  1
15  4
 5
99 92 87 81 78 66 53 46 30 26 21 16 12 10  8
 6
 3
 4
 8
11  7
16 13  9
 6
 2
15 10 12  5
 1
14
98 93 85 76 71 70 67 65 63 57 48 42 39 37 26 17
don't search past this line
11  4
 5
10 14  6
 9
 7
 3
16 12  2
 8
13 15  1
97 95 81 78 75 60 55 50 44 39 37 31 30 27 26 20
Figure 9: Algorithm 2 can easily be extended to cases including more than two sequences.
Pseudocode for this extension is presented in Algorithm 6. Note carefully the use of the variable
read: we are storing which indices have been read to avoid re-reading them; this guarantees that
our Algorithm is never asymptotically worse than the na¨ıve solution. Figure 9 demonstrates how
such an algorithm behaves in practice. Again, we shall discuss the running time of this extension in
Appendix A. For the moment, we state the following theorem:
Theorem 3 Algorithm 6 generalizes Algorithm 2 to K lists with an expected running time of O(KN
K−1
K ),
yielding a speed-up of at least Ω( 1
KN
1
K ) in cliques containing K-ary factors. It is never worse than
the na¨ıve solution, meaning that it takes O(min(N,KN
K−1
K )).
Using Algorithm 6, we can similarly extend Algorithm 5 to allow for any number of groups
(pseudocode is not shown; all statements about the groups Y and Z simply become statements
1365

MCAULEY AND CAETANO
Algorithm 6 Find i such that ∏K
k=1 vk[i] is maximized
Input: K vectors v1 ...vK; permutation functions p1 ... pK that sort them in decreasing order; a
vector read indicating which indices have been read, and a unique value T /∈read {read is
essentially a boolean array indicating which indices have been read; since creating this array is
an O(N) operation, we create it externally, and reuse it O(N) times; setting read[i] = T indicates
that a particular index has been read; we use a different value of T for each call to this function
so that read can be reused without having to be reinitialized}
1: Initialize: start := 1,
max := maxp∈{p1...pK} ∏K
k=1 vk[p[1]],
best := argmaxp∈{p1...pK} ∏K
k=1 vk[p[1]]
2: for k ∈{1...K} do
3:
endk := maxq∈{p1...pK} p−1
k [q[1]]
4:
read[pk[1]] = T
5: end for
6: while start < max{end1 ...endK} do
7:
start := start +1
8:
for k ∈{1...K} do
9:
if read[pk[start]] := T then
10:
continue
11:
end if
12:
read[pk[start]] := T
13:
m := ∏K
x=1 vx[pk[start]]
14:
if m > max then
15:
best := pk[start]
16:
max := m
17:
end if
18:
ek := maxq∈{p1...pK} p−1
k [q[start]]
19:
endk := min(ek,endk)
20:
end for
21: end while {see Appendix A for running times}
22: Return: best
about K groups {G1 ...GK}, and calls to Algorithm 2 become calls to Algorithm 6). The one
remaining case that has not been considered is when the sequences v1 ···vK are functions of different
(but overlapping) variables; na¨ıvely, we can create a new variable whose domain is the product
space of all of the overlapping terms, and still achieve the performance improvement guaranteed by
Theorem 3; in some cases, better results can again be obtained by applying recursion, as in Figure 7.
As a ﬁnal comment we note that we have not provided an algorithm for choosing how to split
the variables of a model into (K + 1)-groups. We note even if we split the groups in a na¨ıve way,
we are guaranteed to get at least the performance improvement guaranteed by Theorem 3, though
more ‘intelligent’ splits may further improve the performance.
Furthermore, in all of the applications we have studied, K is sufﬁciently small that it is inexpen-
sive to consider all possible splits by brute-force.
1366

FASTER ALGORITHMS FOR MAX-PRODUCT MESSAGE-PASSING
5.4 Extensions for Conditionally Factorizable Models
Just as in Section 5.2, we can extend Algorithm 3 to factors of any size, so long as the purely latent
cliques contain more latent variables than those cliques that depend upon the observation. The
analysis for this type of model is almost exactly the same as that presented in Section 5.2, except
that any terms consisting of purely latent variables are processed ofﬂine.
As we mentioned in 5.2, if a model contains (non-maximal) factors of size K, we will gain a
speed-up of Ω( 1
KN
1
K ). If in addition there is a factor (either maximal or non-maximal) consisting
of purely latent variables, we can still obtain a speed-up of Ω(
1
K+1N
1
K+1 ), since this factor merely
contributes an additional term to (Equation 14). Thus when our ‘data-dependent’ terms contain only
a single latent variable (i.e., K = 1), we gain a speed-up of Ω(
√
N), as in Algorithm 3.
6. Performance Improvements in Existing Applications
Our results are immediately compatible with several applications that rely on inference in graphical
models. As we have mentioned, our results apply to any model whose cliques decompose into
lower-order terms.
Often, potentials are deﬁned only on nodes and edges of a model. A Dth-order Markov model
has a tree-width of D, despite often containing only pairwise relationships. Similarly ‘skip-chain
CRFs’ (Sutton and McCallum, 2006; Galley, 2006), and junction-trees used in SLAM applications
(Paskin, 2003) often contain only pairwise terms, and may have low tree-width under reasonable
conditions. These are examples of latently factorizable models. In each case, if the tree-width is
D, Algorithm 5 takes O(MND√
N) (for a model with M nodes and N states per node), yielding a
speed-up of Ω(
√
N).
Models for shape-matching and pose reconstruction often exhibit similar properties (Tresadern
et al., 2009; Donner et al., 2007; Sigal and Black, 2006). In each case, third-order cliques factorize
into second-order terms; hence we can apply Algorithm 4 to achieve a speed-up of Ω(
√
N).
Another similar model for shape-matching is that of Felzenszwalb (2005); this model again
contains third-order cliques, though it includes a ‘geometric’ term constraining all three variables.
Here, the third-order term is independent of the input data, meaning that each of its rows can be
sorted ofﬂine, as described in Section 3. This is an example of a conditionally factorizable model.
In this case, those factors that depend upon the observation are pairwise, meaning that we achieve a
speed-up of Ω(N
1
3 ). Further applications of this type shall be explored in Section 7.4.
In Coughlan and Ferreira (2002), deformable shape-matching is solved approximately using
loopy belief-propagation. Their model has only second-order cliques, meaning that inference takes
Θ(MN2) per iteration. Although we cannot improve upon this result, we note that we can typically
do exact inference in a single iteration in O(MN2√
N); thus our model has the same running time as
O(
√
N) iterations of the original version. This result applies to all second-order models containing
a single loop (Weiss, 2000).
In McAuley et al. (2008), a model is presented for graph-matching using loopy belief-propagation;
the maximal cliques for D-dimensional matching have size (D + 1), meaning that inference takes
Θ(MND+1) per iteration (it is shown to converge to the correct solution); we improve this to
O(MND√
N).
Interval graphs can be used to model resource allocation problems (Fulkerson and Gross, 1965);
each node encodes a request, and overlapping requests form edges. Maximal cliques grow with the
1367

MCAULEY AND CAETANO
Reference
description
running time
our method
McAuley et al. (2008)
D-d graph-matching
Θ(MND+1) (iter.)
O(MND√
N) (iter.)
Sutton and McCallum (2006)
Width-D skip-chain
O(MND)
O(MND−1√
N)
Galley (2006)
Width-3 skip-chain
Θ(MN3)
O(MN2√
N)
Tresadern et al. (2009)
Deformable matching
Θ(MN3)
O(MN2√
N)
Coughlan and Ferreira (2002)
Deformable matching
Θ(MN2) (iter.)
O(MN2√
N)
Sigal and Black (2006)
Pose reconstruction
Θ(MN3)
O(MN2√
N)
Felzenszwalb (2005)
Deformable matching
Θ(MN3)
Θ(MN
8
3 ) (online)
Fulkerson and Gross (1965)
Width-D interval graph
O(MND+1)
O(MND√
N)
Table 3: Some existing work to which our results can be immediately applied (M is the number of
nodes in the model, N is the number of states per node. ‘iter.’ denotes that the algorithm
is iterative).
number of overlapping requests, though the constraints are only pairwise, meaning that we again
achieve an Ω(
√
N) improvement.
Finally, in Section 7.4 we shall explore a variety of applications in which we have pairwise
models of the form shown in (Equation 7). In all of these cases, we see an (expected) reduction of
a Θ(MN2) message-passing algorithm to O(MN
√
N).
Table 3 summarizes these results. Reported running times reﬂect the expected case. Note that
we are assuming that max-product belief-propagation is being used in a discrete model; some of the
referenced articles may use different variants of the algorithm (e.g., Gaussian models, or approxi-
mate inference schemes). We believe that our improvements may revive the exact, discrete version
as a tractable option in these cases.
7. Experiments
We present experimental results for two types of models: latently factorizable models, whose cliques
factorize into smaller terms, as discussed in Section 4, and conditionally factorizable models, whose
factors that depend upon the observation contain fewer latent variables than their maximal cliques,
as discussed in Section 3.
We begin with an asymptotic analysis of the running time of our algorithm on the ‘inner product’
operations of (Equation 1) and (Equation 14), in order to assess Theorems 2 and 3 experimentally.
7.1 Comparison Between Asymptotic Performance and Upper-Bounds
For our ﬁrst experiment, we compare the performance of Algorithms 2 and 6 to the na¨ıve solution of
Algorithm 1. These are core subroutines of each of the other algorithms, meaning that determining
their performance shall give us an accurate indication of the improvements we expect to obtain in
real graphical models.
For each experiment, we generate N i.i.d. samples from [0,1) to obtain the lists v1 ...vK. N is
the domain size; this may refer to a single node, or a group of nodes as in Algorithm 6; thus large
values of N may appear even for binary-valued models. K is the number of lists in (Equation 14);
we can observe this number of lists only if we are working in cliques of size K + 1, and then only
if the factors are of size K (e.g., we will only see K = 5 if we have cliques of size 6 with factors
1368

FASTER ALGORITHMS FOR MAX-PRODUCT MESSAGE-PASSING
0
200
400
600
800
1000
N
0
10
20
30
40
50
60
Number of entries read
Performance and bounds for 2 lists
N
E1(M)
min(N, 2
√
N)
experimental
0
200
400
600
800
1000
N
0
50
100
150
200
250
300
Number of entries read
Performance and bounds for 3 lists
N
min(N, 3N
2
3 )
experimental
0
200
400
600
800
1000
N
0
100
200
300
400
500
600
700
Number of entries read
Performance and bounds for 4 lists
N
min(N, 4N
3
4 )
experimental
Figure 10: Performance of our algorithm and bounds. For K = 2, the exact expectation is shown,
which appears to precisely match the average performance (over 100 trials). The dotted
lines show the bound of (Equation 23). While the bound is close to the true performance
for K = 2, it becomes increasingly loose for larger K.
of size 5); therefore smaller values of K are probably more realistic in practice (indeed, all of the
applications in Section 6 have K = 2).
The performance of our algorithm is shown in Figure 10, for K = 2 to 4 (i.e., for 2 to 4 lists).
When K = 2, we execute Algorithm 2, while Algorithm 6 is executed for K ≥3. The performance
reported is simply the number of elements read from the lists (which is at most K × start). This
is compared to N itself, which is the number of elements read by the na¨ıve algorithm. The upper-
bounds we obtained in (Equation 23) are also reported, while the true expected performance (i.e.,
Equation 19) is reported for K = 2. Note that the variable read was introduced into Algorithm 6 in
order to guarantee that it can never be asymptotically slower than the na¨ıve algorithm. If this variable
is ignored, the performance of our algorithm deteriorates to the point that it closely approaches the
upper-bounds shown in Figure 10. Unfortunately, this optimization proved overly complicated to
include in our analysis, meaning that our upper-bounds remain highly conservative for large K.
7.2 Performance Improvement for Dependent Variables
The expected-case running time of our algorithm was derived under the assumption that each list
has independent order statistics, as was the case for our previous experiment. We suggested that we
will obtain worse performance in the case of negatively correlated variables, and better performance
in the case of positively correlated variables; we shall assess these claims in this experiment.
Figure 11 shows how the order statistics of va and vb can affect the performance of our algo-
rithm. Essentially, the running time of Algorithm 2 is determined by the level of ‘diagonalness’ of
the permutation matrices in Figure 11; highly diagonal matrices result in better performance than
the expected case, while highly off-diagonal matrices result in worse performance. The expected
case was simply obtained under the assumption that every permutation is equally likely.
We report the performance for two lists (i.e., for Algorithm 2), where each (va[i],vb[i]) is an
independent sample from a 2-dimensional Gaussian with covariance matrix
Σ =
 1
c
c
1

,
1369

MCAULEY AND CAETANO
←best case
permutation:
operations:
1
1
3
3
5
worst case →
permutation
operations:
7
7
9
10
10
Figure 11: Different permutation matrices and their resulting cost (in terms of entries
read/multiplications performed). Each permutation matrix transforms the sorted val-
ues of one list into the sorted values of the other, that is, it transforms va as sorted by pa
into vb as sorted by pb. The red (lighter) squares show the entries that must be read be-
fore the algorithm terminates (each corresponding to one multiplication). See Figure 23
for further explanation.
meaning that the two lists are correlated with correlation coefﬁcient c (here we are working in the
max-sum semiring). This dependence between the values of the two lists leads to a dependence in
their order statistics, so that in the case of Gaussian random variables, the correlation coefﬁcient
precisely captures the ‘diagonalness’ of the matrices in Figure 11. Performance is shown in Fig-
ure 12 for different values of c (c = 0, is not shown, as this is the case observed in the previous
experiment).
7.3 Message-Passing in Latently Factorizable Models
In this section we present experiments in models whose cliques factorize into smaller terms, as
discussed in Section 4.
7.3.1 2-DIMENSIONAL GRAPH-MATCHING
Naturally, Algorithm 5 has additional overhead compared to the na¨ıve solution, meaning that it
will not be beneﬁcial for small N. In this experiment, we aim to assess the extent to which our
approach is faster in real applications. We reproduce the model from McAuley et al. (2008), which
performs 2-dimensional graph-matching, using a loopy graph with cliques of size three, containing
only second-order potentials (as described in Section 6); the Θ(NM3) performance of McAuley
et al. (2008) is reportedly state-of-the-art. We also show the performance on a graphical model with
random potentials, in order to assess how the results of the previous experiments are reﬂected in
terms of actual running time.
1370

FASTER ALGORITHMS FOR MAX-PRODUCT MESSAGE-PASSING
0
200
400
600
800
1000
N
0
10
20
30
40
50
60
Number of entries read
Performance and bounds for c = 0.2
N
E1(M)
min(N, 2
√
N)
experimental
0
200
400
600
800
1000
N
0
10
20
30
40
50
60
Number of entries read
Performance and bounds for c = 0.5
N
E1(M)
min(N, 2
√
N)
experimental
0
200
400
600
800
1000
N
0
10
20
30
40
50
60
Number of entries read
Performance and bounds for c = 1.0
N
E1(M)
min(N, 2
√
N)
experimental
0
200
400
600
800
1000
N
0
20
40
60
80
100
Number of entries read
Performance and bounds for c = −0.2
N
E1(M)
min(N, 2
√
N)
experimental
0
200
400
600
800
1000
N
0
50
100
150
200
Number of entries read
Performance and bounds for c = −0.5
N
E1(M)
min(N, 2
√
N)
experimental
0
200
400
600
800
1000
N
0
200
400
600
800
1000
Number of entries read
Performance and bounds for c = −1.0
N
E1(M)
min(N, 2
√
N)
experimental
Figure 12: Performance of our algorithm for different correlation coefﬁcients. The top three plots
show positive correlation, the bottom three show negative correlation. Correlation coef-
ﬁcients of c = 1.0 and c = −1.0 capture precisely the best and worst-case performance
of our algorithm, resulting in O(1) and Θ(N) performance, respectively (when c = −1.0
the linear curve obscures the experimental curve).
We perform matching between a template graph with M nodes, and a target graph with N nodes,
which requires a graphical model with M nodes and N states per node (see McAuley et al. 2008 for
details). We ﬁx M = 10 and vary N.
Figure 13 (left) shows the performance on random potentials, that is, the performance we hope
to obtain if our model assumptions are satisﬁed. Figure 13 (right) shows the performance for graph-
matching, which closely matches the expected-case behavior. Fitted curves are shown together with
the actual running time of our algorithm, conﬁrming its O(MN2√
N) performance. The coefﬁcients
of the ﬁtted curves demonstrate that our algorithm is useful even for modest values of N.
We also report results for graph-matching using graphs from the MPEG-7 data set (Bai et al.,
2009), which consists of 1,400 silhouette images (Figure 14). Again we ﬁx M = 10 (i.e., 10 points
are extracted in each template graph) and vary N (the number of points in the target graph). This
experiment conﬁrms that even when matching real-world graphs, the assumption of independent
order statistics appears to be reasonable.
7.3.2 HIGHER-ORDER MARKOV MODELS
In this experiment, we construct a simple Markov model for text denoising. Random noise is applied
to a text segment, which we try to correct using a prior extracted from a text corpus. For instance
1371

MCAULEY AND CAETANO
0
100
200
300
400
500
600
700
800
N (number of states)
0
50
100
150
200
250
300
350
400
450
Average wall time (seconds)
Random potentials (5 iterations)
na¨ıve method
0.00000079N 3 (r = 546.33)
our method
0.00000388N 2.5 (r = 30.06)
0
100
200
300
400
500
600
700
800
N (size of target graph)
0
100
200
300
400
500
Average wall time (seconds)
2D Graph matching
na¨ıve method
0.00000083N 3 (r = 361.61)
our method
0.00000422N 2.5 (r = 11.60)
Figure 13: The running time of our method on randomly generated potentials, and on a graph-
matching experiment (both graphs have the same topology). Fitted curves are also ob-
tained by performing least-squares regression; the residual error r indicates the ‘good-
ness’ of the ﬁtted curve.
0
100
200
300
400
500
N (size of target graph)
0
5
10
15
20
25
30
35
Average wall time (seconds)
2D Graph matching (MPEG-7 data)
na¨ıve method
0.00000018N 3 (r = 14.73556)
our method
0.00000095N 2.5 (r = 0.01651)
Figure 14: The running time of method our on graphs from the MPEG-7 data set.
wondrous sight of th4 ivory Pequod is corrected to
wondrous sight of the ivory
Pequod.
In such a model, we would like to exploit higher-order relationships between characters, though
the amount of data required to construct an accurate prior grows exponentially with the size of the
maximal cliques. Instead, our prior consists entirely of pairwise relationships between characters (or
‘bigrams’); higher-order relationships are encoded by including bigrams of non-adjacent characters.
1372

FASTER ALGORITHMS FOR MAX-PRODUCT MESSAGE-PASSING








	



	
		
	
	
Figure 15: Left: Our model for denoising. Its computational complexity is similar to that of a
skip-chain CRF, and models for named-entity recognition (right).
Speciﬁcally, our model takes the form
ΦX(xX) =
|X|−1
∏
i=1
Φi,i+1(xi,xi+1)×
|X|−2
∏
i=1
Φi,i+2(xi,xi+2)
where
Φi, j(xi,xj) = ψi, j(xi,xj)p(xi|oi)p(xj|o j).
Here ψ is our prior (extracted from text statistics), and p is our ‘noise model’ (given the observation
o). The computational complexity of inference in this model is similar to that of the skip-chain CRF
shown in Figure 3(b), as well as models for part-of-speech tagging and named-entity recognition,
as in Figure 15. Text denoising is useful for the purpose of demonstrating our algorithm, as there
are several different corpora available in different languages, allowing us to explore the effect that
the domain size (i.e., the size of the language’s alphabet) has on running time.
We extracted pairwise statistics based on 10,000 characters of text, and used this to correct a
series of 25 character sequences, with 1% random noise introduced to the text. The domain was
simply the set of characters observed in each corpus. The Japanese data set was not included, as the
Θ(MN2) memory requirements of the algorithm made it infeasible with N ≃2000; this is addressed
in Section 7.4.1.
The running time of our method, compared to the na¨ıve solution, is shown in Figure 16. One
might expect that texts from different languages would exhibit different dependence structures in
their order statistics, and therefore deviate from the expected case in some instances. However, the
running times appear to follow the ﬁtted curve closely, that is, we are achieving approximately the
expected-case performance in all cases.
Since the prior ψi,i+1(xi,xi+1) is data-independent, we shall further discuss this type of model
in reference to Algorithm 3 in Section 7.4.
7.4 Experiments with Conditionally Factorizable Models
In each of the following experiments we perform belief-propagation in models of the form given in
(Equation 7). Thus each model is completely speciﬁed by deﬁning the node potentials Φi(xi|yi), the
edge potentials Φi, j(xi,xj), and the topology (N ,E) of the graph.
Furthermore we assume that the edge potentials are homogeneous, that is, that the potential for
each edge is the same, or rather that they have the same order statistics (for example, they may
differ by a multiplicative constant). This means that sorting can be done online without affecting
the asymptotic complexity. When subject to heterogeneous potentials we need merely sort them
ofﬂine; the online cost shall be similar to what we report here.
1373

MCAULEY AND CAETANO
0
200
400
600
800
1000
1200
N (alphabet size)
0
200
400
600
800
1000
1200
Total wall time (seconds)
Korean
Text denoising
na¨ıve method
0.00000076N 3 (r = 6.35880)
our method
0.00000146N 2.5 (r = 0.00079)
70
80
90 100 110 120 130 140 150
0.0
0.1
0.2
0.3
0.4
0.5
0.6
Figure 16: The running time of our method compared to the na¨ıve solution. A ﬁtted curve is also
shown, whose coefﬁcient estimates the computational overhead of our model.
7.4.1 CHAIN-STRUCTURED MODELS
In this section, we consider chain-structured graphs. Here we have nodes N = {1...Q}, and edges
E = {(1,2),(2,3)...(Q−1,Q)}. The max-product algorithm is known to compute the maximum-
likelihood solution exactly for tree-structured models.
Figure 17 (left) shows the performance of our method on a model with random potentials, that
is, Φi(xi|yi) = U[0,1), Φi,i+1(xi,xi+1) = U[0,1), where U[0,1) is the uniform distribution. Fitted
curves are superimposed onto the running time, conﬁrming that the performance of the standard
solution grows quadratically with the number of states, while ours grows at a rate of N
√
N. The
residual error r shows how closely the ﬁtted curve approximates the running time; in the case of
random potentials, both curves have almost the same constant.
Figure 17 (right) shows the performance of our method on the text denoising experiment. This
experiment is essentially identical to that shown in Section 7.3.2, except that the model is a chain
(i.e., there is no Φi,i+2), and we exploit the notion of data-independence (i.e., the fact that Φi,i+1
does not depend on the observation). Since the same Φi,i+1 is used for every adjacent pair of nodes,
there is no need to perform the ‘sorting’ step ofﬂine—only a single copy of Φi,i+1 needs to be sorted,
and this is included in the total running time shown in Figure 17.
7.4.2 GRID-STRUCTURED MODELS
Similarly, we can apply our method to grid-structured models. Here we resort to loopy belief-
propagation to approximate the MAP solution, though indeed the same analysis applies in the case
of factor-graphs (Kschischang et al., 2001). We construct a 50×50 grid model and perform loopy
belief-propagation using a random message-passing schedule for ﬁve iterations. In these experi-
ments our nodes are N = {1...50}2, and our edges connect the 4-neighbors, that is, the node (i, j)
is connected to both (i+1, j) and (i, j +1) (similar to the grid shown in Figure 2(a)).
1374

FASTER ALGORITHMS FOR MAX-PRODUCT MESSAGE-PASSING
0
100
200
300
400
500
N (number of states)
0.0
0.5
1.0
1.5
2.0
2.5
3.0
3.5
4.0
Total wall time (seconds)
Random potentials (2500 node chain)
na¨ıve method
0.00002N 2 (r = 0.00514)
our method
0.00002N 1.5 (r = 0.00891)
0
500
1000
1500
2000
N (alphabet size)
0
10
20
30
40
50
60
70
80
Total wall time (seconds)
Korean
Japanese
Text denoising
na¨ıve method
0.00002N 2 (r = 0.15)
our method
0.00015N 1.5 (r = 5.38)
75
90
105 120 135 150
0.00
0.15
0.30
0.45
0.60
Figure 17: Running time of inference in chain-structured models: random potentials (left), and text
denoising (right). Fitted curves conﬁrm that the exponent of 1.5 given theoretically is
maintained in practice (r denotes the sum of residuals, that is, the ‘goodness’ of the ﬁtted
curve).
Figure 18 (left) shows the performance of our method on a grid with random potentials (similar
to the experiment in Section 7.4.1). Figure 18 (right) shows the performance of our method on an
optical ﬂow task (Lucas and Kanade, 1981). Here the states encode ﬂow vectors: for a node with
N states, the ﬂow vector is assumed to take integer coordinates in the square [−
√
N/2,
√
N/2)2 (so
that there are N possible ﬂow vectors). For the unary potential we have
Φ(i, j)(x|y) =
Im1[i, j]−Im2[(i, j)+ f(x)]
,
where Im1[a,b] and Im2[a,b] return the gray-level of the pixel at (a,b) in the ﬁrst and second images
(respectively), and f(x) returns the ﬂow vector encoded by x. The pairwise potentials simply encode
the Euclidean distance between two ﬂow vectors. Note that a variety of low-level computer vision
tasks (including optical ﬂow) are studied in Felzenszwalb and Huttenlocher (2006), where the highly
structured nature of the potentials in question often allows for efﬁcient solutions.
Our ﬁtted curves in Figure 18 show O(N
√
N) performance for both random data and for optical
ﬂow. Clearly the ﬁtted curve for optical ﬂow deviates somewhat from that obtained for random data;
naturally the potentials are highly structured in this case, as exploited by Felzenszwalb and Hutten-
locher (2006); it appears that some aspect of this structure is slightly harmful to our algorithm,
though a more thorough analysis of this type of potential remains as future work. More ‘harmful’
structures are explored in the following section.
7.4.3 FAILURE CASES
In our previous experiments on graph-matching, text denoising, and optical ﬂow we observed run-
ning times similar to those for random potentials, indicating that there is no prevalent dependence
structure between the order statistics of the messages and the potentials.
1375

MCAULEY AND CAETANO
0
100
200
300
400
500
N (number of states)
0
10
20
30
40
50
60
70
80
90
Total wall time (seconds)
Random potentials (50 × 50 grid, 5 iterations)
na¨ıve method
0.00034N 2 (r = 24.26)
our method
0.00252N 1.5 (r = 15.26)
0
100
200
300
400
500
N (number of states)
0
20
40
60
80
100
Total wall time (seconds)
Optical ﬂow (50 × 50 grid, 5 iterations)
na¨ıve method
0.00038N 2 (r = 28.04)
our method
0.00386N 1.5 (r = 1.76)
Figure 18: Running time of inference in grid-structured models: random potentials (left), and opti-
cal ﬂow (right).
In certain applications the order statistics of these terms are highly dependent in a way that
is detrimental to our algorithm. This behavior is observed for certain types of concave potentials
(or convex potentials in a min-sum formulation). For instance, in a stereo disparity experiment,
the unary potentials encode the fact that the output should be ‘close to’ a certain value; the pairwise
potentials encode the fact that neighboring nodes should take similar values (Scharstein and Szeliski,
2001; Sun et al., 2003).
In these applications, the permutation matrices that transform the sorted values of va to the
sorted values of vb are block-off-diagonal (see the sixth permutation in Figure 11). In such cases,
our algorithm only decreases the number of multiplication operations by a multiplicative constant,
and may in fact be slower due to its computational overhead. This is precisely the behavior shown
in Figure 19 (left), in the case of stereo disparity.
It should be noted that there exist algorithms speciﬁcally designed for this class of potential
functions (Kolmogorov and Shioura, 2007; Felzenszwalb and Huttenlocher, 2006), which are prefer-
able in such instances.
We similarly perform an experiment on image denoising, where the unary potentials are again
convex functions of the input (see Geman and Geman, 1984; Lan et al., 2006). Instead of using a
pairwise potential that merely encodes smoothness, we extract the pairwise statistics from image
data (similar to our experiment on text denoising); thus the potentials are no longer concave. We see
in Figure 19 (right) that even if a small number of entries exhibit some ‘randomness’ in their order
statistics, we begin to gain a modest speed improvement over the na¨ıve solution (though indeed, the
improvements are negligible compared to those shown in previous experiments).
7.5 Other Applications of Tropical Matrix Multiplication
As we have mentioned, our improvements to message-passing in graphical models arise from a
fast solution to matrix multiplication in the max-product semiring. In this section we discuss other
1376

FASTER ALGORITHMS FOR MAX-PRODUCT MESSAGE-PASSING
0
100
200
300
400
500
N (number of states)
0
20
40
60
80
100
120
Total wall time (seconds)
Stereo disparity (50 × 50 grid, 5 iterations)
na¨ıve method
0.00033N 2 (r = 15.21)
our method
0.00852N 1.5 (r = 253.57)
0
100
200
300
400
500
N (number of states)
0
20
40
60
80
100
Total wall time (seconds)
Image denoising (50 × 50 grid, 5 iterations)
na¨ıve method
0.00037N 2 (r = 43.63)
our method
0.00727N 1.5 (r = 14.04)
Figure 19: Two experiments whose potentials and messages have highly dependent order statistics:
stereo disparity (left), and image denoising (right).
problems which include max-product (or ‘tropical’) matrix multiplication as a subroutine. Williams
and Williams (2010) discusses the relationship between this type of matrix multiplication problem
and various other problems.
7.5.1 MAX-PRODUCT LINEAR PROGRAMMING
In Sontag et al. (2008), a method is given for exact MAP-inference in graphical models using LP-
relaxations. Where exact solutions cannot be obtained by considering only pairwise factors, ‘clus-
ters’ of pairwise terms are introduced in order to reﬁne the solution. Message-passing in these clus-
ters turns out to take exactly the form that we consider, as third-order (or larger) clusters are formed
from pairwise terms. Although a number of applications are presented in Sontag et al. (2008), we
focus on protein design, as this is the application in which we typically observe the largest domain
sizes. Other applications with larger domains may yield further beneﬁts.
Without going into detail, we simply copy the two equations from Sontag et al. (2008) to which
our algorithm applies. The ﬁrst of these is concerned with passing messages between clusters, while
the second is concerned with choosing new clusters to add. Below are the two equations, reproduced
verbatim from Sontag et al. (2008):
λc→e(xe) ←−2
3
 λe→e(xe)+ ∑
c′̸=c,e∈c′
λc′→e(xe)

+ 1
3 max
xc\e
h
∑
e′∈c\e
 λe′→e′(xe′)+ ∑
c′̸=c,e′∈c′
λc′→e′(xe′)
i
(15)
(see Sontag et al., 2008, Figure 1, bottom), which consists of marginalizing a cluster (c) that decom-
poses into edges (e), and
d(c) = ∑
e∈c
max
xe be(xe)−max
xc
"
∑
e∈c
be(xe)
#
,
(16)
1377

MCAULEY AND CAETANO
(see Sontag et al., 2008, (Equation 4)), which consists of ﬁnding the MAP-state in a ring-structured
model.
As the code from Sontag et al. (2008) was publicly available, we simply replaced the appropriate
functions with our own (in order to provide a fair comparison, we also replaced their implementation
of the na¨ıve algorithm, as ours proved to be faster than the highly generic matrix library used in their
code).
In order to improve the running time of our algorithm, we made the following two modiﬁcations
to Algorithm 2:
• We used an adaptive sorting algorithm (i.e., a sorting algorithm that runs faster on nearly-
sorted data). While Quicksort was used during the ﬁrst iteration of message-passing, sub-
sequent iterations used insertion sort, as the optimal ordering did not change signiﬁcantly
between iterations.
• We added an additional stopping criterion to the algorithm. Namely, we terminate the algo-
rithm if va[pa[start]]×vb[pb[start]] < max. In other words, we check how large the maximum
could be given the best possible permutation of the next elements (i.e., if they have the same
index); if this value could not result in a new maximum, the algorithm terminates. This check
costs us an additional multiplication, but it means that the algorithm will terminate faster in
cases where a large maximum is found early on.
Results for these two problems are shown in Figure 20. Although our algorithm consistently
improves upon the running time of Sontag et al. (2008), the domain size of the variables in question
is not typically large enough to see a marked improvement. Interestingly, neither method follows
the expected running time closely in this experiment. This is partly due to the fact that there is
signiﬁcant variation in the variable size (note that N only shows the average variable size), but it
may also suggest that there is a complicated structure in the potentials which violates our assumption
of independent order statistics.
7.5.2 ALL-PAIRS SHORTEST-PATH
The ‘all-pairs shortest-path’ problem consists of ﬁnding the shortest path between every pair of
nodes in a graph. Although the most commonly used solution is probably the well-known Floyd-
Warshall algorithm (Floyd, 1962), the state-of-the-art expected-case solution to this problem is that
of Karger et al. (1993), whose expected-case running time is O(N2 logN) when applied to graphs
with distances sampled from the uniform distribution.
Unfortunately, the solution of Karger et al. (1993) requires a Fibonacci heap or similar data
structure in order to achieve the reported running time (i.e., a heap with O(1) insertion and decrease-
key operations); such data structures are known to be inefﬁcient in practice (Fredman and Tarjan,
1987). When their algorithm is implemented using a standard priority queue, it has running time
O(N2 log2 N).
In Aho et al. (1983), a transformation is shown between the all-pairs shortest-path problem
and min-sum matrix multiplication. Using our algorithm, this gives us an expected-case O(N2√
N)
solution to the all-pairs shortest-path problem, assuming that the subproblems created by this trans-
formation have i.i.d. order statistics; this assumption is notably different than the assumption of
uniformity made in Karger et al. (1993).
1378

FASTER ALGORITHMS FOR MAX-PRODUCT MESSAGE-PASSING
20
30
40
50
60
70
80
90
N (average variable size)
0.0
0.5
1.0
1.5
2.0
2.5
3.0
3.5
Average wall time (seconds per iteration)
Protein design (Equation 15)
na¨ıve method
our method
20
30
40
50
60
70
80
90
N (average variable size)
0
5
10
15
20
25
Average wall time (seconds)
Protein design (Equation 16)
na¨ıve method
our method
Figure 20: The running time of our method on protein design problems from Sontag et al. (2008).
In this ﬁgure, N reﬂects the average domain size amongst all variables involved in the
problem; ﬁtted curves are not shown due to the highly variable nature of the domain
sizes included in each problem instance.
In Figure 21, we show the performance of our method on i.i.d. uniform graphs, compared to the
Floyd-Warshall algorithm, and that of Karger et al. (1993). On graph sizes of practical interest, our
algorithm is found to give the fastest performance, in spite of its more expensive asymptotic cost.
Our solution is comparable to that of Karger et al. (1993) for the largest graph size shown; larger
graph sizes could not be shown due to memory constraints. Note that while these algorithms are fast
in practice, each has Θ(N3) worst-case performance; more ‘exotic’ solutions that improve upon the
worst-case bound are discussed in Alon et al. (1997) and Chan (2007), among others, though none
are truly subcubic (i.e., O(N3−ε)).
It should also be noted that the transformations given in Aho et al. (1983) apply in both direc-
tions, that is, solutions to the all-pairs shortest-path problem can be used to solve min-sum matrix
multiplication. Thus any subcubic solution to the all-pairs shortest-path problem can be applied to
the inference problems in graphical models presented in Section 4. However, the transformation
of Aho et al. (1983) introduces a very high computational overhead (namely, solving min-sum ma-
trix multiplication for an N × N matrix requires solving all-pairs shortest-path in a graph with 3N
nodes), and moreover it violates the assumptions on the graph distribution required for fast infer-
ence given in Karger et al. (1993). In practice, we were unable to produce an implementation of
min-sum matrix multiplication based on this transformation that was faster than the na¨ıve solution.
Interestingly, a great deal of attention has been focused on expected-case solutions to all-pairs
shortest-path, while to our knowledge ours is the ﬁrst work to approach the expected-case analy-
sis of min-sum matrix multiplication. Given the strong relationship between the two problems, it
remains a promising open problem to assess whether the analysis from these solutions to all-pairs
shortest-path can be applied to produce max-product matrix multiplication algorithms with similar
asymptotic running times.
1379

MCAULEY AND CAETANO
28
29
210
211
212
N (size of graph)
2−7
2−5
2−3
2−1
21
23
25
27
29
Wall time (seconds)
All-Pairs Shortest-Path
Floyd/Warshall
O(N 3)
Karger et al. w/ Fibonacci heap
O(N 2 log N)
Karger et al. w/ std::set
O(N 2 log2 N)
Aho et al. w/ na¨ıve method
O(N 3)
Aho et al. w/ our method
O(N 2√
N)∗
* assumes that subproblems have i.i.d. order statistics
Figure 21: Our algorithm applied to the ‘all-pairs shortest-path’ problem. The expected-case run-
ning times of each algorithm are shown at right.
7.5.3 L∞DISTANCES
The problem of computing an inner product in the max-sum semiring is closely related to computing
the L∞distance between two vectors
||va −vb||∞=
max
i∈{1...N}
va[i]−vb[i]
.
(17)
Na¨ıvely, we would like to solve (Equation 17) by applying Algorithm 2 to va and −vb with the mul-
tiplication operator replaced by a×b = |a+b|, however this violates the condition of (Equation 2),
since the optimal solution may arise either when both va[i] and −vb[i] are large, or when both va[i]
and −vb[i] are small (in fact, this operation violates the semiring axiom of associativity).
We address this issue by running Algorithm 2 twice, ﬁrst considering the largest values of va
and −vb, before re-running the algorithm starting from the smallest values. This ensures that the
maximum solution is found in either case.
Pseudocode for this solution is given in Algorithm 7, which adapts Algorithm 4 to the problem
of computing an L∞distance matrix. Similarly, we can adapt Algorithm 3 to solve L∞nearest-
neighbor problems, where an array of M points in RN is processed ofﬂine, allowing us compute the
distance of a query point to all M other points O(M
√
N).
Figure 22 shows the running time of our algorithm for computing an L∞distance matrix (where
M = N), and the online cost of performing a nearest-neighbor query. Again the expected speedup
over the na¨ıve solution is Ω(
√
N) for both problems, though naturally our algorithm requires larger
values of N than does Algorithm 4 in order to be beneﬁcial, since Algorithm 2 must be executed
twice in order to solve (Equation 17).
A similar trick can be applied to compute message in the max-product semiring even for poten-
tials that contain negative values, though this may require up to four executions of Algorithm 2, so
it is unlikely to be practical.
1380

FASTER ALGORITHMS FOR MAX-PRODUCT MESSAGE-PASSING
Algorithm 7 Use Algorithm 2 to compute an L∞distance matrix
Input: an M ×N array A containing M points in RN
1: initialize an M ×M distance matrix D := 0
2: for x ∈{1...M} do
3:
compute −→
P [x] by sorting A[x] {takes ΘN logN}
4:
compute ←−
P [x] by sorting −A[x] {i.e., −→
P [x] in reverse order}
5: end for {this loop takes Θ(MN logN)}
6: for x ∈{1...M} do
7:
for y ∈{x+1...M} do
8:
best1 := Algorithm2

A[x],−A[y],−→
P [x],←−
P [y]

{takes O(
√
N); Algorithm 2 uses the operator a×b = |a+b|}
9:
best2 := Algorithm2

A[y],−A[x],−→
P [y],←−
P [x]

10:
D[x,y] := max
 A[x,best1]−A[y,best1]
,
A[x,best2]−A[y,best2]

11:
D[y,x] := D[x,y]
12:
end for
13: end for {this loop takes expected time O(M2√
N)}
0
1000
2000
3000
4000
5000
6000
7000
8000
N (dimensionality of each vector)
0.00
0.05
0.10
0.15
0.20
0.25
0.30
0.35
0.40
Average wall time (seconds)
L∞nearest neighbor
na¨ıve method
5.4e −09N 2 (r = 0.00002)
our method
1.9e −07N
√
N (r = 0.00008)
0
500
1000 1500 2000 2500 3000 3500 4000
N (dimensionality of each vector)
0
50
100
150
200
250
Average wall time (seconds)
L∞distance matrix
na¨ıve method
2.7e −09N 3 (r = 0.76253)
our method
8.3e −08N 2√
N (r = 2.07389)
Figure 22: The running time of our method compared to the na¨ıve solution. A ﬁtted curve is also
shown, whose coefﬁcient estimates the computational overhead of our model.
8. Discussion and Future Work
We have brieﬂy discussed the application of our algorithm to the all-pairs shortest-path problem, and
also mentioned that a variety of other problems are related to max-product matrix multiplication via
a series of subcubic transformations (Williams and Williams, 2010). To our knowledge, of all these
problems only all-pairs shortest-paths has received signiﬁcant attention in terms of expected-case
analysis. The analysis in question centers around two types of model: the uniform model, where
edge weights are sampled from a uniform distribution, and the endpoint-independent model, which
1381

MCAULEY AND CAETANO
essentially makes an assumption on the independence of outgoing edge weights from each vertex
(Moffat and Takaoka, 1987), which seems very similar to our assumption of independent order
statistics. It remains to be seen whether this analysis can lead to better solutions to the problems
discussed here, and indeed if the analysis applied to uniform models can be applied in our setting to
uniform matrices.
It is interesting to consider the fact that our algorithm’s running time is purely a function of the
input data’s order statistics, and in fact does not depend on the data itself. While it is pleasing that
our assumption of independent order statistics appears to be a weak one, and is satisﬁed in a wide
variety of applications, it ignores the fact that stronger assumptions may be reasonable in many
cases. In factors with a high dynamic range, or when different factors have different scales, it may
be possible to identify the maximum value very quickly, as we attempted to do in Section 7.5.1.
Deriving faster algorithms that make stronger assumptions about the input data remains a promising
avenue for future work.
Our algorithm may also lead to faster solutions for approximately passing a single message.
While the stopping criterion of our algorithm guarantees that the maximum value is found, it is
possible to terminate the algorithm earlier and state that the maximum has probably been found.
A direction for future work would be to adapt our algorithm to determine the probability that the
maximum has been found after a certain number of steps; we could then allow the user to specify
an error probability, or a desired running time, and our algorithm could be adapted accordingly.
9. Conclusion
We have presented a series of approaches that allow us to improve the performance of exact and
approximate max-product message-passing for models with factors smaller than their maximal
cliques, and more generally, for models whose factors that depend upon the observation contain
fewer latent variables than their maximal cliques. We are always able to improve the expected com-
putational complexity in any model that exhibits this type of factorization, no matter the size or
number of factors.
Acknowledgments
We would like to thank Pedro Felzenszwalb, Johnicholas Hines, and David Sontag for comments on
initial versions of this paper, and James Petterson and Roslyn Lau for helpful discussions. NICTA
is funded by the Australian Government’s Backing Australia’s Ability initiative, and the Australian
Research Council’s ICT Centre of Excellence program.
Appendix A. Asymptotic Performance of Algorithm 2 and Extensions
In this section we shall determine the expected-case running times of Algorithm 2 and Algorithm 6.
Algorithm 2 traverses va and vb until it reaches the smallest value of m for which there is some
j ≤m for which m ≥p−1
b [pa[ j]]. If M is a random variable representing this smallest value of m,
then we wish to ﬁnd E(M). While E(M) is the number of ‘steps’ the algorithms take, each step
takes Θ(K) when we have K lists. Thus the expected running time is Θ(KE(M)).
To aid understanding our algorithm, we show the elements being read for speciﬁc examples of
va and vb in Figure 23. This ﬁgure reveals that the actual values in va and vb are unimportant, and
1382

FASTER ALGORITHMS FOR MAX-PRODUCT MESSAGE-PASSING
 2
 4
72
87
 8
28
12
85
93
32
25
31
42
72
18
 4
start = 1
start = 2
start = 3
start = 4
93724232312518 4
87
85
72
28
12
 8
 4
 2
93724232312518 4
87
85
72
28
12
 8
 4
 2
93724232312518 4
87
85
72
28
12
 8
 4
 2
93724232312518 4
87
85
72
28
12
 8
 4
 2
(a)
(b)
Figure 23: (a) The lists va and vb before sorting; (b) Black squares show corresponding elements
in the sorted lists (va[pa[i]] and vb[pb[i]]); red squares indicate the elements read during
each step of the algorithm (va[pa[start]] and vb[pb[start]]). We can imagine expanding a
gray box of size start ×start until it contains an entry; note that the maximum is found
during the ﬁrst step.
(a)
(b)
(c)
(d)
Figure 24: (a) As noted in Figure 23, a permutation can be represented as an array, where there is
exactly one non-zero entry in each row and column; (b) We want to ﬁnd the smallest
value of m such that the gray box includes a non-zero entry; (c) A pair of permutations
can be thought of as a cube, where every two-dimensional plane contains exactly one
non-zero entry; we are now searching for the smallest gray cube that includes a non-zero
entry; the faces show the projections of the points onto the exterior of the cube (the third
face is determined by the ﬁrst two); (d) For the sake of establishing an upper-bound, we
consider a shaded region of width f(N) and height m.
it is only the order statistics of the two lists that determine the performance of our algorithm. By
representing a permutation of the digits 1 to N as shown in Figure 24 ((a), (b), and (d)), we observe
that m is simply the width of the smallest square (expanding from the top left) that includes an
element of the permutation (i.e., it includes i and p[i]).
Simple analysis reveals that the probability of choosing a permutation that does not contain a
value inside a square of size m is
P(M > m) = (N −m)!(N −m)!
(N −2m)!N!
.
(18)
This is precisely 1−F(m), where F(m) is the cumulative density function of M. It is immediately
clear that 1 ≤M ≤⌊N/2⌋, which deﬁnes the best and worst-case performance of Algorithm 2.
1383

MCAULEY AND CAETANO
Using the identity E(X) = ∑∞
x=1 P(X ≥x), we can write down a formula for the expected value
of M:
E(M) =
⌊N/2⌋
∑
m=0
(N −m)!(N −m)!
(N −2m)!N!
.
(19)
The case where we are sampling from multiple permutations simultaneously (i.e., Algorithm 6)
is analogous. We consider K −1 permutations embedded in a K-dimensional hypercube, and we
wish to ﬁnd the width of the smallest shaded hypercube that includes exactly one element of the
permutations (i.e., i, p1[i],..., pK−1[i]). This is represented in Figure 24(c) for K = 3. Note carefully
that K is the number of lists in (Equation 14); if we have K lists, we require K −1 permutations to
deﬁne a correspondence between them.
Unfortunately, the probability that there is no non-zero entry in a cube of size mK is not trivial
to compute. It is possible to write down an expression that generalizes (Equation 18), such as
PK(M > m) =
1
N!K−1 × ∑
σ1∈SN
··· ∑
σK−1∈SN
m
^
i=1

max
k∈{1...K−1}σk(i) > m

(20)
(in which we simply enumerate over all possible permutations and ‘count’ which of them do not fall
within a hypercube of size mK), and therefore state that
EK(M) =
∞
∑
m=0
PK(M > m).
(21)
However, it is very hard to draw any conclusions from (Equation 20), and in fact it is intractable
even to evaluate it for large values of N and K. Hence we shall instead focus our attention on
ﬁnding an upper-bound on (Equation 21). Finding more computationally convenient expressions
for (Equation 20) and (Equation 21) remains as future work.
A.1 An Upper-Bound on EK(M)
Although (Equation 19) and (Equation 21) precisely deﬁne the running times of Algorithm 2 and
Algorithm 6, it is not easy to ascertain the speed improvements they achieve, as the values to which
the summations converge for large N are not obvious. Here, we shall try to obtain an upper-bound
on their performance, which we assessed experimentally in Section 7. In doing so we shall prove
Theorems 2 and 3.
Proof [Proof of Theorem 2] (see Algorithm 2) Consider the shaded region in Figure 24(d). This
region has a width of f(N), and its height m is chosen such that it contains precisely one non-zero
entry. Let ˙M be a random variable representing the height of the gray region needed in order to
include a non-zero entry. We note that
E( ˙M) ∈O( f(N)) ⇒E(M) ∈O( f(N));
our aim is to ﬁnd the smallest f(N) such that E( ˙M) ∈O( f(N)). The probability that none of the
ﬁrst m samples appear in the shaded region is
P( ˙M > m) =
m
∏
i=0

1−f(N)
N −i

.
1384

FASTER ALGORITHMS FOR MAX-PRODUCT MESSAGE-PASSING
Next we observe that if the entries in our N × N grid do not deﬁne a permutation, but we instead
choose a random entry in each row, then the probability (now for ¨M) becomes
P( ¨M > m) =

1−f(N)
N
m
(22)
(for simplicity we allow m to take arbitrarily large values). We certainly have that P( ¨M > m) ≥
P( ˙M > m), meaning that E( ¨M) is an upper-bound on E( ˙M), and therefore on E(M). Thus we
compute the expected value
E( ¨M) =
∞
∑
m=0

1−f(N)
N
m
.
This is just a geometric progression, which sums to N/ f(N). Thus we need to ﬁnd f(N) such that
f(N) ∈O
 N
f(N)

.
Clearly f(N) ∈O(
√
N) will do. Thus we conclude that
E(M) ∈O(
√
N).
Proof [Proof of Theorem 3] (see Algorithm 6) We would like to apply the same reasoning in the
case of multiple permutations in order to compute a bound on EK(M). That is, we would like to
consider K −1 random samples of the digits from 1 to N, rather than K −1 permutations, as random
samples are easier to work with in practice.
To do so, we begin with some simple corollaries regarding our previous results. We have shown
that in a permutation of length N, we expect to see a value less than or equal to f after N/ f steps.
There are now f −1 other values that are less than or equal to f amongst the remaining N −N/ f
values; we note that
f −1
N −N
f
= f
N .
Hence we expect to see the next value less than or equal to f in the next N/ f steps also. A conse-
quence of this fact is that we not only expect to see the ﬁrst value less than or equal to f earlier in
a permutation than in a random sample, but that when we sample m elements, we expect more of
them to be less than or equal to f in a permutation than in a random sample.
Furthermore, when considering the maximum of K −1 permutations, we expect the ﬁrst m el-
ements to contain more values less than or equal to f than the maximum of K −1 random sam-
ples. (Equation 20) is concerned with precisely this problem. Therefore, when working in a K-
dimensional hypercube, we can consider K −1 random samples rather than K −1 permutations in
order to obtain an upper-bound on (Equation 21).
Thus we deﬁne ¨M as in (Equation 22), and conclude that
P( ¨M > m) =

1−f(N,K)K−1
NK−1
m
.
1385

MCAULEY AND CAETANO
Thus the expected value of ¨M is again a geometric progression, which this time sums to (N/ f(N,K))K−1.
Thus we need to ﬁnd f(N,K) such that
f(N,K) ∈O
 
N
f(N,K)
K−1!
.
Clearly
f(N,K) ∈O

N
K−1
K

will do. As mentioned, each step takes Θ(K), so the ﬁnal running time is O(KN
K−1
K ).
To summarize, for problems decomposable into K + 1 groups, we will need to ﬁnd the index
that chooses the maximal product amongst K lists; we have shown an upper-bound on the expected
number of steps this takes, namely
EK(M) ∈O

N
K−1
K

.
(23)
References
Alfred V. Aho, John E. Hopcroft, and Jeffrey D. Ullman. Data Structures and Algorithms. Addison-
Wesley, 1983.
Srinivas M. Aji and Robert J. McEliece. The generalized distributive law. IEEE Transactions on
Information Theory, 46(2):325–343, 2000.
Noga Alon, Zvi Galil, and Oded Margalit. On the exponent of the all pairs shortest path problem.
Journal of Computer and System Sciences, 54(2):255–262, 1997.
Xiang Bai, Xingwei Yang, Longin Jan Latecki, Wenyu Liu, and Zhuowen Tu. Learning context-
sensitive shape similarity by graph transduction. IEEE Transactions on Pattern Analysis and
Machine Intelligence, 32(5):861–874, 2009.
Timothy M. Chan. More algorithms for all-pairs shortest paths in weighted graphs. In Annual ACM
Symposium on Theory of Computing, pages 590–598, 2007.
James M. Coughlan and Sabino J. Ferreira. Finding deformable shapes using loopy belief propaga-
tion. In ECCV, 2002.
Ren´e Donner, Georg Langs, and Horst Bischof. Sparse MRF appearance models for fast anatomical
structure localisation. In BMVC, 2007.
Gal Elidan, Ian Mcgraw, and Daphne Koller. Residual belief propagation: informed scheduling for
asynchronous message passing. In UAI, 2006.
Pedro F. Felzenszwalb. Representation and detection of deformable shapes. IEEE Transactions on
Pattern Analysis and Machine Intelligence, 27(2):208–220, 2005.
Pedro F. Felzenszwalb and Daniel P. Huttenlocher. Efﬁcient belief propagation for early vision.
International Journal of Computer Vision, 70(1):41–54, 2006.
1386

FASTER ALGORITHMS FOR MAX-PRODUCT MESSAGE-PASSING
Robert W. Floyd. Algorithm 97: Shortest path. Communications of the ACM, 5(6):345, 1962.
Michael L. Fredman and Robert Endre Tarjan. Fibonacci heaps and their uses in improved network
optimization algorithms. Journal of the ACM, 34(3):596–615, 1987.
Delbert R. Fulkerson and O. A. Gross. Incidence matrices and interval graphs. Paciﬁc Journal of
Mathematics, (15):835–855, 1965.
Michel Galley. A skip-chain conditional random ﬁeld for ranking meeting utterances by importance.
In EMNLP, 2006.
Stuart Geman and Donald Geman. Stochastic relaxation, gibbs distribution and the bayesian restora-
tion of images. IEEE Transactions on Pattern Analysis and Machine Intelligence, 6(6):721–741,
1984.
David R. Karger, Daphne Koller, and Steven J. Phillips. Finding the hidden path: time bounds for
all-pairs shortest paths. SIAM Journal of Computing, 22(6):1199–1217, 1993.
Leslie R. Kerr. The effect of algebraic structure on the computational complexity of matrix multi-
plication. PhD Thesis, 1970.
Kristian Kersting, Babak Ahmadi, and Sriraam Natarajan. Counting belief propagation. In UAI,
2009.
Uffe Kjærulff. Inference in bayesian networks using nested junction trees. In Proceedings of the
NATO Advanced Study Institute on Learning in Graphical Models, 1998.
Vladimir Kolmogorov and Akiyoshi Shioura. New algorithms for the dual of the convex cost net-
work ﬂow problem with application to computer vision. Technical report, University College
London, 2007.
Frank R. Kschischang, Brendan J. Frey, and Hans-Andrea Loeliger. Factor graphs and the sum-
product algorithm. IEEE Transactions on Information Theory, 47(2):498–519, 2001.
M. Pawan Kumar and Philip Torr. Fast memory-efﬁcient generalized belief propagation. In ECCV,
2006.
Xiang-Yang Lan, Stefan Roth, Daniel P. Huttenlocher, and Michael J. Black. Efﬁcient belief prop-
agation with learned higher-order markov random ﬁelds. In ECCV, 2006.
Bruce D. Lucas and Takeo Kanade. An iterative image registration technique with an application to
stereo vision. In IJCAI, 1981.
Julian J. McAuley and Tib´erio S. Caetano. Exact inference in graphical models: is there more to it?
CoRR, abs/0910.3301, 2009.
Julian J. McAuley and Tib´erio S. Caetano. Exploiting within-clique factorizations in junction-tree
algorithms. AISTATS, 2010a.
Julian J. McAuley and Tib´erio S. Caetano. Exploiting data-independence for fast belief-propagation.
ICML, 2010b.
1387

MCAULEY AND CAETANO
Julian J. McAuley, Tib´erio S. Caetano, and Marconi S. Barbosa. Graph rigidity, cyclic belief prop-
agation and point pattern matching. IEEE Transactions on Pattern Analysis and Machine Intelli-
gence, 30(11):2047–2054, 2008.
Alistair Moffat and Tadao Takaoka.
An all pairs shortest path algorithm with expected time
O(n2 logn). SIAM Journal of Computing, 16(6):1023–1031, 1987.
James D. Park and Adnan Darwiche. A differential semantics for jointree algorithms. In NIPS,
2003.
Mark A. Paskin. Thin junction tree ﬁlters for simultaneous localization and mapping. In IJCAI,
2003.
Kersten Petersen, Janis Fehr, and Hans Burkhardt. Fast generalized belief propagation for MAP
estimation on 2D and 3D grid-like markov random ﬁelds. In DAGM, 2008.
Daniel Scharstein and Richard S. Szeliski. A taxonomy and evaluation of dense two-frame stereo
correspondence algorithms. International Journal of Computer Vision, 47(1–3):7–42, 2001.
Leonid Sigal and Michael J. Black. Predicting 3D people from 2D pictures. In AMDO, 2006.
David Sontag, Talya Meltzer, Amir Globerson, Tommi Jaakkola, and Yair Weiss. Tightening LP
relaxations for MAP using message passing. In UAI, 2008.
Volker Strassen. Gaussian elimination is not optimal. Numerische Mathematik, 14(3):354–356,
1969.
Jian Sun, Nan-Ning Zheng, and Heung-Yeung Shum. Stereo matching using belief propagation.
IEEE Transactions on Pattern Analysis and Machine Intelligence, 25(7):787–800, 2003.
Charles Sutton and Andrew McCallum. Introduction to Conditional Random Fields for Relational
Learning. MIT Press, 2006.
Philip A. Tresadern, Harish Bhaskar, Steve A. Adeshina, Chris J. Taylor, and Tim F. Cootes. Com-
bining local and global shape models for deformable object matching. In BMVC, 2009.
Yair Weiss. Correctness of local probability propagation in graphical models with loops. Neural
Computation, 12:1–41, 2000.
Ryan Williams. Matrix-vector multiplication in sub-quadratic time (some preprocessing required).
In SODA, pages 1–11, 2007.
Virginia Vassilevska Williams and Ryan Williams. Subcubic equivalences between path, matrix,
and triangle problems. In FOCS, 2010.
1388

