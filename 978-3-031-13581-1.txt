The History of 
the GPU - Eras 
and Environment
Jon Peddie

The History of the GPU - Eras and Environment

Jon Peddie 
The History of the GPU -
Eras and Environment

Jon Peddie 
Jon Peddie Research 
Tiburon, CA, USA 
ISBN 978-3-031-13580-4
ISBN 978-3-031-13581-1 (eBook) 
https://doi.org/10.1007/978-3-031-13581-1 
© The Editor(s) (if applicable) and The Author(s), under exclusive license to Springer Nature 
Switzerland AG 2022 
This work is subject to copyright. All rights are solely and exclusively licensed by the Publisher, whether 
the whole or part of the material is concerned, speciﬁcally the rights of translation, reprinting, reuse 
of illustrations, recitation, broadcasting, reproduction on microﬁlms or in any other physical way, and 
transmission or information storage and retrieval, electronic adaptation, computer software, or by similar 
or dissimilar methodology now known or hereafter developed. 
The use of general descriptive names, registered names, trademarks, service marks, etc. in this publication 
does not imply, even in the absence of a speciﬁc statement, that such names are exempt from the relevant 
protective laws and regulations and therefore free for general use. 
The publisher, the authors, and the editors are safe to assume that the advice and information in this book 
are believed to be true and accurate at the date of publication. Neither the publisher nor the authors or 
the editors give a warranty, expressed or implied, with respect to the material contained herein or for any 
errors or omissions that may have been made. The publisher remains neutral with regard to jurisdictional 
claims in published maps and institutional afﬁliations. 
This Springer imprint is published by the registered company Springer Nature Switzerland AG 
The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland

Foreword 
Computer graphics has attracted the world’s leading computer scientists and the 
most prominent computer companies. Computer graphics is attractive for its grand 
computer science challenge, visceral beauty, and broad market impact by sitting at 
the intersection of computing and art, the simulation of light, physics, and virtual 
worlds. 
The industry was chaotic and fast-changing, with countless companies innovating. 
In less than three decades, 3D graphics became standard in computers, evolved 
from a ﬁxed function accelerator into a programmable shading GPU, became a 
technology juggernaut, and real-time ray tracing became a reality. The GPU became 
general purpose, and GPU computing was born, democratizing scientiﬁc computing, 
and enabling deep learning and the modern AI era. No one would have believed a 
chip design that started out running Quake would evolve to become the engine for 
conversational AI, self-driving cars, climate simulation, and countless applications 
that impact most modern life. 
As the longest running historian of the graphics industry, Jon masterfully lays out 
the lineage and broad “family tree” of the GPU and the markets and industries it 
has come to serve. Its adoption, the rapid rate of its improvements, and breadth and 
depth of its application make it worthy of having its “origins story” explored in the 
detail and level of completeness for which Jon’s books are known. 
As you read Jon’s book, you will beneﬁt from his insight and ability to “paint 
the picture” of how we got here, what it means, and where we might go. The reader 
will see the awe-inspiring amount of innovation, brilliance, determination, guts, and 
effort from casts of thousands over decades that have made the GPU worthy of such 
a treatise. Enjoy!! 
Nvidia, California, US
Jenson Huang 
Curtis Priem
v

Preface 
This is the second book in the three-book series on the History of the GPU. 
The integrated Graphics Processing Unit (GPU) has been employed in many 
systems (platforms) and evolved since 1996. 
This second book in the series covers the developments that lead up to the 
integrated GPU, from the early 1990s to the late 1990s. 
The book has two main sections, the PC platform, and other platforms. 
Other platforms include workstations, game machines, and others, which include 
vehicles—GPUs are used everywhere in almost everything. 
Each chapter is designed to be read independently; hence, there may be some 
redundancy. Hopefully, each one tells an interesting story. 
In general, a company is discussed and introduced in the year of its formation. 
However, a company may be discussed in multiple time periods in multiple chapters 
depending on how signiﬁcant their developments were and what impact they had on 
the industry.
vii

viii
Preface
History of the GPU
 Steps to Invention 
Book 1 
Eras and Environment 
Book 2 
New Developments 
Book 3 
1. Preface
1. Preface
1. Preface 
2. History of 
the GPU 
2. Race to build 
the first GPU 
2. Second Era of 
GPUs (2001-2006) 
3. 1980-1990 
Graphics Controllers 
on Other Platforms 
3. GPU Functions
3. Third to Fifth Era 
of GPUs 
4. 1980-1989 
Graphics Controllers 
on PCs 
4. Major Era of GPUs
4. Mobile GPUs 
5. 1990-1995 
Graphics Controllers 
on PCs 
5. First Era of GPUs
5. Game Console GPUs 
6. 1990-1999 
Graphics Controllers 
on Other Platforms 
6. GPU 
Environment-Hardware
6. Compute GPUs 
7. 1996-1999 
Graphics Controller 
on PCs 
7. Application Program 
Interface (API)
7. Open GPUs 
8. What is a GPU 
8. GPU 
Environment-Software 
Extensions 
8. Sixth Era of GPUs 
The History of the GPU - Eras and Environment 
I mark the GPU’s introduction as the ﬁrst fully integrated single chip with hardware 
geometry processing capabilities—transform and lighting. Nvidia gets that honor on 
the PC by introducing their GeForce 256 based on the NV10 chip in October 1999. 
However, Silicon Graphics Inc. (SGI) introduced an integrated GPU in the Nintendo 
64 in 1996, and ArtX developed an integrated GPU for the PC a month after Nvidia. 
As you will learn, Nvidia did not introduce the concept of a GPU, nor did they

Preface
ix
develop the ﬁrst hardware implementation of transform and lighting. But Nvidia was 
the ﬁrst to bring all that together in a mass-produced single-chip device. 
The evolution of the GPU, however, did not stop with the inclusion of the trans-
formation and lighting (T&L) engine because the ﬁrst era of such GPUs had ﬁxed-
function T&L processors—that was all they could do, and when they were not doing 
that, they sat idle using power. The GPU kept evolving and has gone through six eras 
of evolution ending up today as a universal computing machine capable of almost 
anything. 
The Author 
A Lifetime of Chasing Pixels 
I have been working in computer graphics since the early 1960s, ﬁrst as an engineer, 
then as an entrepreneur (I founded four companies and ran three others), ending up 
in a failed attempt at retiring in 1982 as an industry consultant and advisor. Over the 
years, I watched, advised, counseled, and reported on developing companies and their 
technology. I saw the number of companies designing or building graphics controllers 
swell from a few to over 45. In addition, there have been over 30 companies designing 
or making graphics controllers for mobile devices. 
I’ve written and contributed to several other books on computer graphics (seven 
under my name and six co-authored). I’ve lectured at several universities around the 
world, written uncountable articles, and acquired a few patents, all with a single, 
passionate thread—computer graphics and the creation of beautiful pictures that tell 
a story. This book is liberally sprinkled with images—block diagrams of the chips, 
photos of the chips, the boards they were put on, and the systems they were put in, 
and pictures of some of the people who invented and created these marvelous devices 
that impact and enhance our daily lives—many of them I am proud to say are good 
friends of mine. 
I laid out the book in such a way (I hope) that you can open it up to any page and 
start to get the story. You can read it linearly; if you do, you’ll probably ﬁnd new 
information and probably more than you ever wanted to know. My email address is 
in various parts of this book, and I try to answer everyone, hopefully within 48 h. I’d 
love to hear comments, your stories, and your suggestions. 
The following is an alphabetical list of all the people (at least I hope it’s all of 
them) who helped me with this project. A couple of them have passed away, sorry to 
say. Hopefully, this book will help keep the memory of them and their contributions 
alive. 
Thanks for reading 
Jon Peddie—Chasing pixels, and ﬁnding gems

x
Preface
Acknowledgments and Contributors 
The following people helped me with editing, interviews, data, photos, and most of 
all encouragement. I literally and ﬁguratively could not have done this without them. 
Ashraf Eassa—Nvidia 
Andrew Wolfe—S3 
Anand Patel—Arm 
Atif Zafar—Pixilica 
Borger Ljosland—Falanx 
Brian Kelleher—DEC, and ﬁnally Nvidia 
Bryan Del Rizzo—3dfx & Nvidia 
Carrell Killebrew—TI/ATI/AMD 
Chris Malachowsky—Nvidia 
Curtis Priem—Nvidia 
Dado Banatao—S3 
Dan Vivoli—Nvidia 
Dan Wood—Matrox, Intel 
Daniel Taranovsky—ATI 
Dave Erskine—ATI & AMD 
Dave Orton—SGI, ArtX, ATI & AMD 
David Harold—Imagination Technologies 
Dave Kasik—Boeing 
Emily Drake—Siggraph 
Edvaed Sergard—Falanx 
Eric Demers—AMD/Qualcomm 
Frank Paniagua—Video Logic 
Gary Tarolli—3dfx 
Gerry Stanley—Real3D 
George Sidiropoulos—Think Silicon 
Henry Chow—Yamaha & Giga Pixel 
Henry Fuchs—UNC 
Henry C. Lin—Nvidia 
Henry Quan—ATI 
Hossain Yassaie—Imagination Technologies 
Iakovos Istamoulis—Think Silicon 
Ian Hutchinson—Arm 
Jay Eisenlohr—Rendition 
Jay Torberg—Microsoft 
Jeff Bush—Nyuzi 
Jeff Fischer—Weitek & Nvidia 
Jem Davis—Arm 
Jensen Huang—Nvidia 
Jim Pappas—Intel 
Joe Curley—Tseng/Intel

Preface
xi
Jonah Alben—Nvidia 
John Poulton—UNC & Nvidia 
Karl Guttag—TI 
Karthikeyan (Karu) Sankaralingam—University of Wisconsin-Madison 
Kathleen Maher—JPA & JPR 
Ken Potashner—S3 & SonicBlue 
Kristen Ray—Arm 
Lee Hirsch—Nvidia 
Luke Kenneth Casson Leighton—Libre-GPU 
Mark Kilgard—Nvidia (Iris GL) 
Mary Whitton—Iknoas 
Megan Zea—PCI SIG 
Melissa Scuse—Arm 
Mike Diehl—HP 
Mike Mantor—AND 
Mikko Nurmi—Bitboys 
Mikko Alho—Siru 
Neal Leavitt—Editing 
Neil Trevett—3Dlabs & Khronos 
Nick England—Iknoas 
Pedro Duarte—Universities of Coimbra 
Peter McGuinness—SGS Thompson 
Peter.L.Segal—AT&T 
Petri Norlund—Bitboys 
Phil Roges—ATI 
Richard Huddy—ATI 
Richard Selvaggi—Tseng Labs 
Rick Bergman—ATI/AMD 
Robert Dow—JPR 
Ross Smith—3dfx 
Ruchika Saini—Editing 
Sasa Marinkovic—ATI & AMD 
Simon Fenny—Video Logic & Imagination Technologies 
Steve Brightﬁeld—SiliconArts 
Steve Edelson—Edson Labs 
Stefan Demetrescu—Stanford 
Stephen Morein—Stellar 
Tatsuo Yamamoto—Sega/DMP 
Tim Leland—Qualcomm 
Timothy Miller—Traversal Technology 
Tom Forsyth—3Dlabs 
Tony Tamasi—3dfx & Nvidia 
Trevor Wing—Video Logic 
Tiburon, USA
Jon Peddie

Contents 
1 Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1 
1.1 
Nvidia’s NV10—First Integrated PC GPU (September 1999) . . . . .
2 
1.1.1
Nvidia’s Non GPU GPU . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9 
1.1.2
The S3 Nvidia Patent Suit and Silicon Valley Gossip . . . . .
10 
1.1.2.1
Meanwhile, Back in Reality? . . . . . . . . . . . . . . . .
11 
1.1.2.2
Nvidia and S3 Enter into a Cross-Licensing 
Agreement . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
12 
1.2 
Summary and Conclusion
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
14 
1.3 
Epilog—All the Others . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
15 
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
16 
2 The GPUs’ Functions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
19 
2.1 
The Pipeline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
20 
2.1.1
The Meaning of Real Time . . . . . . . . . . . . . . . . . . . . . . . . . . .
21 
2.1.2
Binning and Branding . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
22 
2.1.3
The Frame Buffer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
23 
2.1.3.1
GPUs and Memory . . . . . . . . . . . . . . . . . . . . . . . .
25 
2.1.3.2
Resizable Base Address Register (2010)
. . . . . .
26 
2.1.4
Object-Level Clipping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
27 
2.1.4.1
Looking Back at the Geometry 
Processor’s Origins . . . . . . . . . . . . . . . . . . . . . . . .
27 
2.1.4.2
Digital Signal Processors Used 
for Geometry . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
30 
2.2 
The Rendering Equation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
31 
2.3 
The Geometry Creation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
32 
2.4 
The Software Story: The All-Important APIs . . . . . . . . . . . . . . . . . . .
36 
2.4.1
The Triangle Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
37 
2.4.2
Drawing and Shading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
37 
2.4.3
Triangle Sorting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
37 
2.4.4
Texture Mapping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
38 
2.4.4.1
3D Texture Filtering . . . . . . . . . . . . . . . . . . . . . . . .
41
xiii

xiv
Contents
2.5 
Fill Rate, Rendering Pipelines, and Triangle Size . . . . . . . . . . . . . . . .
41 
2.5.1
Rendering Techniques . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
43 
2.5.1.1
Polygon Rendering . . . . . . . . . . . . . . . . . . . . . . . . .
45 
2.5.1.2
Scan-Line Rendering . . . . . . . . . . . . . . . . . . . . . . .
45 
2.5.1.3
Immediate Mode Rendering . . . . . . . . . . . . . . . . .
45 
2.5.1.4
Tile Rendering . . . . . . . . . . . . . . . . . . . . . . . . . . . .
46 
2.5.1.4.1 
Tile-Based Deferred 
Rendering . . . . . . . . . . . . . . . . . . . . . . .
46 
2.5.1.4.2 
Immediate Mode Tile 
Rendering . . . . . . . . . . . . . . . . . . . . . . .
48 
2.5.1.5
Ray-Traced Rendering . . . . . . . . . . . . . . . . . . . . . .
49 
2.5.2
Instancing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
50 
2.5.3
Aliasing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
50 
2.5.4
Scaling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
53 
2.5.5
Environment Mapping . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
55 
2.6 
Generating the Image: Hardware Issues . . . . . . . . . . . . . . . . . . . . . . . .
56 
2.6.1
VPU—Visual Processing Unit . . . . . . . . . . . . . . . . . . . . . . . .
57 
2.6.2
Multi-display . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
57 
2.7 
Crypto GPU . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
58 
2.8 
Audio, In, Out, In Again . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
58 
2.9 
Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
59 
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
60 
3 The Major GPU Eras . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
63 
3.1 
The First Era—Transform and Lighting—DirectX 7 (1999) . . . . . . .
66 
3.1.1
Shading and Shaders . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
66 
3.1.1.1
Shading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
68 
3.1.2
Geometry Processing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
70 
3.2 
The Second Era—Programmable Shaders—DirectX 8 and 9 
(2001–2006) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
72 
3.2.1
Pixel (Fragment) Shader Stage . . . . . . . . . . . . . . . . . . . . . . . .
73 
3.2.2
How Many Shaders? Is There a Limit? . . . . . . . . . . . . . . . . .
74 
3.3 
The Third Era—The Uniﬁed Shader—DirectX 10 and 11 
(2006–2009) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
75 
3.3.1
Geometry Shader (2006) . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
78 
3.4 
The Fourth Era—Compute Shaders—DirectX 11 
(2009–2015) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
79 
3.4.1
Tessellation Shader (October 2009) . . . . . . . . . . . . . . . . . . . .
80 
3.4.2
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
83 
3.5 
The Fifth Era—Ray Tracing and AI—DirectX 12 (2015–2020) . . .
83 
3.5.1
Ray Tracing Shaders . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
84 
3.5.2
Real-Time Ray Tracing with AI . . . . . . . . . . . . . . . . . . . . . . .
87 
3.5.3
Variable Rate Shading—2019 . . . . . . . . . . . . . . . . . . . . . . . . .
89 
3.6 
The Sixth Era—Mesh Shaders—DirectX 12 Ultimate (2020) . . . . .
91 
3.6.1
Primitive and Mesh Shaders—2017–2020 . . . . . . . . . . . . . .
92

Contents
xv
3.6.2
Sampler Feedback . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
93 
3.7 
Summary on Shading . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
93 
3.7.1
Mobile . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
93 
3.7.1.1
GPU Sources for Mobile Devices . . . . . . . . . . . .
94 
3.7.2
GPU-Compute . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
96 
3.8 
FLOPS Versus Fraps: Cars and GPUs . . . . . . . . . . . . . . . . . . . . . . . . .
96 
3.8.1
Why Good Enough is Not . . . . . . . . . . . . . . . . . . . . . . . . . . . .
98 
3.9 
Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
100 
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
101 
4 The First Era of GPUs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
105 
4.1 
The Golden Age—Transform, and Lighting Changes 
the Industry (1999–2001) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
107 
4.1.1
On Being First . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
108 
4.2 
First Era Discrete PC-Based GPUs . . . . . . . . . . . . . . . . . . . . . . . . . . . .
108 
4.2.1
Glaze3D Bitboys 2.0 (1999–2001) . . . . . . . . . . . . . . . . . . . . .
109 
4.2.1.1
Inﬁneon . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
112 
4.2.1.2
Bitboys Gets the Axe . . . . . . . . . . . . . . . . . . . . . . .
114 
4.2.1.3
Bitboys Gets Hammered . . . . . . . . . . . . . . . . . . . .
115 
4.2.1.4
The Final Blow . . . . . . . . . . . . . . . . . . . . . . . . . . . .
116 
4.2.2
S3 Savage 2000 (November 1999) . . . . . . . . . . . . . . . . . . . . .
116 
4.2.2.1
S3 Chrome
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
119 
4.2.2.2
Epilogue: The Curious Trail to Zhaoxin . . . . . . .
120 
4.2.3
ATI and Nvidia: First Era GPUs (1999–2002) . . . . . . . . . . .
121 
4.2.4
ATI Radeon R100—256 (April 2000) . . . . . . . . . . . . . . . . . .
121 
4.2.4.1
Pixel Tapestry Architecture . . . . . . . . . . . . . . . . . .
125 
4.2.5
Nvidia’s NV15—GeForce 2 GTS (April 2000) . . . . . . . . . .
128 
4.2.6
STMicroelectronics—Imagination Technologies 
Kyro II (2001–2002) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
128 
4.2.6.1
PowerVR3 STG4000 Kyro—2001 . . . . . . . . . . .
132 
4.2.6.2
PowerVR3 STG4500 Kyro II—2001 . . . . . . . . .
132 
4.2.6.3
The End . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
134 
4.2.6.4
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
134 
4.3 
The Development and History of the Integrated GPU (1999–) . . . . .
135 
4.3.1
ArtX . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
136 
4.3.1.1
ArtX: First Company to Announce 
a PC-Based iGPU . . . . . . . . . . . . . . . . . . . . . . . . . .
137 
4.3.1.2
ATI Acquires ArtX (February 2000) . . . . . . . . . .
140 
4.3.2
ATI’s First IGP (March 2000) . . . . . . . . . . . . . . . . . . . . . . . . .
141 
4.3.3
SiS’ First PC-Based IGP (December 2000) . . . . . . . . . . . . .
142 
4.3.4
Nvidia’s nForce 220 IGP (June 2001) . . . . . . . . . . . . . . . . . .
145 
4.3.5
ATI’s IGP 320 (2002) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
146 
4.4 
IGP Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
146 
4.5 
The Expansion Years (2001–2016) . . . . . . . . . . . . . . . . . . . . . . . . . . . .
147

xvi
Contents
4.5.1
The Collapse and the Rise of Graphics Chip 
Companies . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
148 
4.6 
Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
149 
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
149 
5 The GPU Environment—Hardware . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
151 
5.1 
It Takes a Village to Build a GPU . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
151 
5.2 
Semiconductor Technology . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
152 
5.2.1
Intel Introduces Angstroms . . . . . . . . . . . . . . . . . . . . . . . . . . .
153 
5.2.1.1
Fins to Sheets . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
155 
5.2.1.2
GPU Memory . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
156 
5.2.1.2.1 
Shared Versus Private Memory . . . . .
158 
5.2.1.3
Memory Type and GPU
. . . . . . . . . . . . . . . . . . . .
159 
5.2.2
Chiplets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
160 
5.3 
PC Bus Architectures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
165 
5.3.1
Industry Standard Architecture: 1981 . . . . . . . . . . . . . . . . . .
167 
5.3.2
Micro Channel Architecture: 1987 . . . . . . . . . . . . . . . . . . . . .
168 
5.3.3
Extended ISA: 1988 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
168 
5.3.4
VESA Local Bus: 1992 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
169 
5.3.5
Peripheral Component Interconnect: 1992 . . . . . . . . . . . . . .
169 
5.3.6
Accelerated Graphics Port: 1997 . . . . . . . . . . . . . . . . . . . . . .
172 
5.3.7
Peripheral Component Interconnect Express: 2003 . . . . . . .
172 
5.3.8
Other I/O . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
176 
5.4 
GPU Video Outputs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
177 
5.4.1
VGA: 1987 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
179 
5.4.2
DVI (1999–) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
179 
5.4.3
HDMI (2002–) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
179 
5.4.4
High Dynamic Range (2015) . . . . . . . . . . . . . . . . . . . . . . . . .
181 
5.4.5
DisplayPort . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
182 
5.4.6
Seeing More . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
183 
5.4.7
Virtual Reality Headsets . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
183 
5.4.8
Augmented Reality Glasses . . . . . . . . . . . . . . . . . . . . . . . . . .
184 
5.4.9
Mixed Reality Headsets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
185 
5.4.10 
Monitor Synchronization: 2013–2015 . . . . . . . . . . . . . . . . . .
186 
5.4.10.1 
Those Damn Scalers . . . . . . . . . . . . . . . . . . . . . . .
192 
5.4.10.2 
Flickering . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
193 
5.4.10.3 
Adaptive Sync, FreeSync, and G-Sync . . . . . . . .
194 
5.5 
Multiple AIBs in a System . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
194 
5.5.1
Multi-GPIs (1996) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
196 
5.6 
Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
199 
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
199

Contents
xvii
6 Application Program Interface (API) . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
201 
6.1 
Application Program Interface . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
202 
6.1.1
APIs and OSs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
203 
6.1.1.1
Chaos in the Mobile Market . . . . . . . . . . . . . . . . .
207 
6.1.1.2
DirectX Shaders . . . . . . . . . . . . . . . . . . . . . . . . . . .
207 
6.1.1.3
Comparison of Vertex Shaders . . . . . . . . . . . . . . .
209 
6.1.2
History of DirectX . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
209 
6.1.2.1
Hardware Feature Levels . . . . . . . . . . . . . . . . . . . .
214 
6.1.2.2
Microsoft’s Japanese Bigotry . . . . . . . . . . . . . . . .
214 
6.1.3
The History of OpenGL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
215 
6.1.4
The Fahrenheit Project . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
215 
6.1.5
Low-Level APIs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
217 
6.1.5.1
Mantle . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
218 
6.1.5.2
Metal . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
219 
6.1.5.3
Vulkan . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
220 
6.1.5.3.1 
Cross-Platform Support . . . . . . . . . . .
220 
6.1.6
WebGPU . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
222 
6.1.7
DirectX 12 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
223 
6.1.7.1
Ultimate . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
223 
6.1.7.1.1 
Ray Tracing . . . . . . . . . . . . . . . . . . . . .
224 
6.1.7.1.2 
Variable Rate Shading . . . . . . . . . . . .
225 
6.1.7.1.3 
Getting to Mesh Shaders . . . . . . . . . .
226 
6.1.7.1.4 
HW T&L Engines, 1981 
to 1996: IRIS GL to Direct X 
7.0 . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
226 
6.1.7.1.5 
Vertex and Pixel Shaders, 
1997 to 2008: Direct3D 10 . . . . . . . .
227 
6.1.7.1.6 
Tessellation . . . . . . . . . . . . . . . . . . . . .
229 
6.1.7.1.7 
The Pipeline Expands . . . . . . . . . . . . .
230 
6.1.7.1.8 
Uniﬁed Shader, 2006–2010: 
DirectX 9.0 and OpenGL 3.3 . . . . . .
232 
6.1.7.1.9 
Getting to Compute: Task 
Shaders 2016–2017 (DirectX 
12, Vulkan 2) . . . . . . . . . . . . . . . . . . . .
233 
6.1.8
Microsoft DirectX Shader Model 4.0: Enhancements . . . . .
234 
6.1.8.1
Geometry Shaders . . . . . . . . . . . . . . . . . . . . . . . . .
234 
6.1.8.1.1 
Mesh Shaders, 2018–2020: 
DirectX 12 Ultimate, Vulkan 
Extension . . . . . . . . . . . . . . . . . . . . . . .
235 
6.1.8.1.2 
Meshlets . . . . . . . . . . . . . . . . . . . . . . . .
237 
6.1.8.1.3 
Benchmarking . . . . . . . . . . . . . . . . . . .
242 
6.1.8.1.4 
Interactive Mode . . . . . . . . . . . . . . . . .
242 
6.1.8.1.5 
Mesh Demo . . . . . . . . . . . . . . . . . . . . .
243

xviii
Contents
6.2 
Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
247 
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
248 
7 The GPU Environment—Software Extensions and Custom 
Features . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
251 
7.1 
Software Libraries and Tools . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
251 
7.1.1
Ambient Occlusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
252 
7.1.2
Nvidia’s DLSS (February 2019) . . . . . . . . . . . . . . . . . . . . . . .
253 
7.1.3
AMD’s Fidelity FX Super Resolution (May 2021) . . . . . . .
257 
7.1.4
Intel’s XeSS (March 2022) . . . . . . . . . . . . . . . . . . . . . . . . . . .
259 
7.2 
More Than a Driver . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
261 
7.2.1
SYCL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
261 
7.2.2
GLSL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
262 
7.2.3
HLSL . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
262 
7.2.4
SPIR-V . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
263 
7.2.5
Textures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
263 
7.3 
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
266 
7.4 
Software Development Kits for Developers . . . . . . . . . . . . . . . . . . . . .
266 
7.4.1
Nvidia’s GameWorks (2014-) . . . . . . . . . . . . . . . . . . . . . . . . .
267 
7.4.2
AMD’s FX Library (2014) . . . . . . . . . . . . . . . . . . . . . . . . . . .
270 
7.4.3
AMD’s GPUOpen (2015–) . . . . . . . . . . . . . . . . . . . . . . . . . . .
271 
7.4.4
Application Enhancement Software . . . . . . . . . . . . . . . . . . . .
273 
7.4.5
AMD’s Gaming Evolved . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
274 
7.4.6
Nvidia’s GeForce Experience . . . . . . . . . . . . . . . . . . . . . . . . .
276 
7.5 
Conclusion . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
279 
7.6 
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
279 
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
280 
Appendix A: Acronyms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
283 
Appendix B: Deﬁnitions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
287 
Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
327

List of Figures 
Fig. 1.1
Die shot of Nvidia’s ﬁrst GPU, the NV10. Reproduced 
with permission from Curtis Priem . . . . . . . . . . . . . . . . . . . . . . . .
3 
Fig. 1.2
Nvidia’s GeForce 256 (NV10) block diagram with OpenGL
. . .
3 
Fig. 1.3
VisionTek GeForce 256 DDR. Source Hyins for Wikipedia . . . .
4 
Fig. 1.4
Dan Vivoli named the GPU . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
5 
Fig. 1.5
Pat Gelsinger launched Intel’s IDF in 1999. Source Intel . . . . . . .
6 
Fig. 1.6
The Utah teapot, a model by Martin Newell (1975) 
and used ever since. Reproduced with permission 
from Dhatﬁeld for Wikipedia . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
7 
Fig. 1.7
An example of a texture mapped to the faces of a cube 
box, also called a skybox. Reproduced with permission 
from Arieee for Wikipedia . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
8 
Fig. 1.8
Jen-Hsun (Jensen) Huang introduced the GPU in 1999. 
Source Nivida . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9 
Fig. 1.9
The founders of Nvidia: Curtis Prien, Jensen Huang, 
and Chris Malachowsky. Reproduced with permission 
from Curtis Prien . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
14 
Fig. 1.10
The rise and fall of graphics chip suppliers . . . . . . . . . . . . . . . . . .
15 
Fig. 2.1
The graphics pipeline has been functionally the same 
since the 1970s . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
20 
Fig. 2.2
A digital image of Richard Shoup, rendered on the Super 
Paint system, April 1973. Source The Richard G. Shoup 
Estate . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
24 
Fig. 2.3
A viewing frustum. Source Computer desktop encyclopedia 
1988 intergraph computer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
28 
Fig. 2.4
The silicon graphics inc. geometry engine-circa 1980s. 
Source Wikipedia: http://www.Shieldforyoureyes . . . . . . . . . . . .
28 
Fig. 2.5
Rendering equation light characteristics . . . . . . . . . . . . . . . . . . . .
31 
Fig. 2.6
Wire-frame models of a cube, icosahedron, 
and approximate sphere. Source Wikipedia . . . . . . . . . . . . . . . . . .
33 
Fig. 2.7
Three, 2D views and a perspective view. Source Wikipedia
. . . .
33
xix

xx
List of Figures
Fig. 2.8
An eight-sided faceted circle approximation (left) 
and a perfect circle (right)
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
34 
Fig. 2.9
Compare Lora Croft from 1996 to 2014—a measure 
of how CG has improved due to hardware that enabled 
more powerful and advanced software. Source Wikipedia . . . . . .
35 
Fig. 2.10
Hitman codename 47 from the 2016 version of the game 
(left) compared to Hitman 3 2021. Source Wikipedia . . . . . . . . .
35 
Fig. 2.11
Comparison of the graphics quality of Call of Duty 
2003–2021. Reproduced with permission from activision . . . . . .
36 
Fig. 2.12
Mipmaps . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
39 
Fig. 2.13
Example of texture mapping. Taking an image (left) 
overlaying on a 3D model (center) and creating a realistic 
image (left) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
40 
Fig. 2.14
Comparison of Trilinear versus Anisotropic 
ﬁltering—notice the street tiles are in focus 
to the vanishing point in the right image. Source 
Cobblestones. JPG Wikipedia Thomas . . . . . . . . . . . . . . . . . . . . .
41 
Fig. 2.15
Fully rendered image of a 2021 Audi E-Tron GT. 
Reproduced with permission from TurboSquid . . . . . . . . . . . . . .
42 
Fig. 2.16
Wire-frame model of an Audi E-Tron GT. Reproduced 
with permission from TurboSquid . . . . . . . . . . . . . . . . . . . . . . . . .
42 
Fig. 2.17
A Red, Blue, Green (RGB)-shaded triangle. Source 
Tilmann R, Public domain, via Wikimedia commons . . . . . . . . .
43 
Fig. 2.18
Texture mapping is a way to add realism to 3D models . . . . . . . .
43 
Fig. 2.19
A sphere without bump mapping (left). A bump map 
(middle) is applied to the sphere on the left. The sphere 
with the bump map is shown on the right. It appears 
to have a mottled surface resembling an orange and a dent 
to represent where the stem was attached. Source Brion 
VIBBER, McLoaf, Vierge Marie, CC BY-SA 3.0, 
via Wikimedia Commons . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
44 
Fig. 2.20
A tile-based rendering (TBR) graphics pipeline . . . . . . . . . . . . . .
47 
Fig. 2.21
The rays in ray tracing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
49 
Fig. 2.22
An example of instancing to create a ﬁeld of sunﬂowers. 
Reproduced with permission from http://www.Blender.org . . . . .
50 
Fig. 2.23
Changing the intensity or color of the pixels 
between the line and the background gives the effect 
of an even line; the eye (brain) is tricked into seeing 
a smooth line. Source Blender . . . . . . . . . . . . . . . . . . . . . . . . . . . .
51 
Fig. 2.24
Bilinear upscaling ﬁlters out most of the raster Jaggies. 
Reproduced with permission from AMD . . . . . . . . . . . . . . . . . . .
53 
Fig. 2.25
AMD’s FidelityFX super-resolution sharpened and ﬁltered 
the image. Reproduced with permission from AMD . . . . . . . . . .
54 
Fig. 2.26
An unﬁltered or sharpened image at the monitor’s native 
resolution. Reproduced with permission from AMD . . . . . . . . . .
54

List of Figures
xxi
Fig. 2.27
A scene of a park. Source Springer . . . . . . . . . . . . . . . . . . . . . . . .
55 
Fig. 2.28
Reﬂection mapping is used to place objects into scenes. 
Source Springer [50] . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
55 
Fig. 2.29
Images of GPU dies with multi-thousand shader 
processors. Reproduced with permission from AMD (left) 
and Nvidia . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
56 
Fig. 2.30
The many names of a GPU . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
59 
Fig. 3.1
Performance of graphics leading up to GPUs. Graphics 
hardware had been progressing at a rate faster than Moore’s 
law. Data Courtesy of John Poulton, UNC Chapel Hill . . . . . . . .
64 
Fig. 3.2
The six eras of GPU evolution and development . . . . . . . . . . . . .
64 
Fig. 3.3
DirectX basic pipeline
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
64 
Fig. 3.4
A shaded triangle. Source TilmannR, Public domain, 
via Wikimedia Commons . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
68 
Fig. 3.5
Shader stages relative to memory (dark blue boxes), 
the white boxes are not shaders . . . . . . . . . . . . . . . . . . . . . . . . . . .
68 
Fig. 3.6
The ﬁrst single chip GPU with an integrated T&L engine, 
Nvidia’s NV10 used in the GeForce 256, circ 1999. 
Reproduced with permission from Konstantin Lanzet, 
Wikipedia . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
71 
Fig. 3.7
Raja Koduri. Source Intel . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
71 
Fig. 3.8
Vertex shading in the DirectX 8 pipeline . . . . . . . . . . . . . . . . . . . .
73 
Fig. 3.9
A scan-converted triangle quantized by pixel size . . . . . . . . . . . .
73 
Fig. 3.10
Mike Mantor, Corporate Fellow and Chief GPU Architect 
at AMD. Reproduced with permission from Expreview.com
. . .
75 
Fig. 3.11
ATI/AMD Xenos block diagram . . . . . . . . . . . . . . . . . . . . . . . . . .
76 
Fig. 3.12
A typical uniﬁed architecture GPU, with 112 streaming 
processor cores, organized as 14 multithreaded streaming 
multiprocessors. Nvidia’s Tesla GeForce 8800 [15] . . . . . . . . . . .
77 
Fig. 3.13
DirectX 11 rendering pipeline featuring tessellation (gray 
box) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
78 
Fig. 3.14
Tessellation in the chamber for the circumcision 
of the princes in the Imperial Topkapı Sarayı, Istanbul, 
ﬁfteenth century
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
81 
Fig. 3.15
Catmull–Clark subdivision of a cube. Reproduced 
with permission from Romainbehar, Wikipedia . . . . . . . . . . . . . .
81 
Fig. 3.16
The ﬁrst GPU with integrated hardware tessellation 
was the ATI Radeon 8500. Reproduced with permission 
from Palit, ixbt.com
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
82 
Fig. 3.17
ATI’s tessellation pipeline
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
82 
Fig. 3.18
Appel projected light at a 3D computer model 
and displayed the results on a plotter using a form 
of tone-mapping to create light and dark areas. Source 
Arthur Appel . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
84 
Fig. 3.19
The ray tracing paradigm . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
85

xxii
List of Figures
Fig. 3.20
Bounding boxes enclosing a patch from the teapot model 
are tested for an intersection with a ray . . . . . . . . . . . . . . . . . . . . .
86 
Fig. 3.21
Shader evaluation pipeline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
86 
Fig. 3.22
Ray-traced image of a Mercedes-Benz SS Roadster. 
Reproduced with permission from Volkan Kaçar . . . . . . . . . . . . .
87 
Fig. 3.23
DLSS off and on.n Source Nvidia . . . . . . . . . . . . . . . . . . . . . . . . .
88 
Fig. 3.24
Reduced resolution rendering followed by upscaling gives 
an enhanced image at high frame rates. Reproduced 
with permission from Nvidia . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
90 
Fig. 3.25
Variable rate shading architecture. Reproduced 
with permission from Nvidia . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
91 
Fig. 3.26
GPU taxonomy.n Reproduced with permission from JPR . . . . . .
94 
Fig. 3.27
Mobile gaming device . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
95 
Fig. 3.28
Mobile GPU design sources . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
95 
Fig. 3.29
Comparison of GFLOPS of GPUs over time
. . . . . . . . . . . . . . . .
97 
Fig. 3.30
Simpliﬁed AIB block diagram . . . . . . . . . . . . . . . . . . . . . . . . . . . .
98 
Fig. 3.31
Simpliﬁed integrated GPU block diagram . . . . . . . . . . . . . . . . . . .
99 
Fig. 3.32
AMD’s view of the eras of the GPU. Reproduced 
with permission by AMD . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
100 
Fig. 3.33
GPU architectural introductions through the eras . . . . . . . . . . . . .
101 
Fig. 4.1
The rise and fall of PC graphics chip suppliers 
versus market growth . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
106 
Fig. 4.2
Block diagram of a PC . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
106 
Fig. 4.3
The Bitboys: Petri Nordland, Mika Tuomi, and Kai Tuomi 
(1999) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
112 
Fig. 4.4
Bitboys’ Glaze3D block diagram with triangle setup engine . . . .
113 
Fig. 4.5
Bitboys’ Glaze3D pixel pipeline . . . . . . . . . . . . . . . . . . . . . . . . . .
114 
Fig. 4.6
Bitboys’Glaze3D prototype circa 1999. Source Petri 
Nordlund . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
115 
Fig. 4.7
S3 Savage 2000 AIB with T&L. Source Swaaye-Wikipedia . . . .
117 
Fig. 4.8
Die photo of ATI’s R100 chip. Source Fritzchens Fritz 
Wikipedia . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
123 
Fig. 4.9
Example of vertex skinning. Source ATI . . . . . . . . . . . . . . . . . . . .
123 
Fig. 4.10
ATI R100 Radeon 7000 AIB. The R100 chip is 
under the fan. Source ATI . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
124 
Fig. 4.11
Block diagram of the ATI R100 GPU . . . . . . . . . . . . . . . . . . . . . .
127 
Fig. 4.12
ATI’s multi-texturing capabilities let developers add more 
special effects without impacting performance. Source ATI . . . .
128 
Fig. 4.13
Block diagram of the ATI R100 Charisma engine . . . . . . . . . . . .
129 
Fig. 4.14
Tim Chambers (courtesy of Bill DeBerry Funeral 
Directors—Denton TX) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
130 
Fig. 4.15
Changing of partners . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
131 
Fig. 4.16
Imagination Technologies Kryo-based AIB. Source Trio3D 
Wikipedia . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
133 
Fig. 4.17
GDC to IGC to IGP to iGPU (1980–2009) . . . . . . . . . . . . . . . . . .
136

List of Figures
xxiii
Fig. 4.18
ArtX’s Aladdin 7 3D integrated GPU controller . . . . . . . . . . . . . .
137 
Fig. 4.19
SiS’ 315 ﬁrst integrated chipset GPU . . . . . . . . . . . . . . . . . . . . . .
143 
Fig. 4.20
Nvidia’s nForce IGP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
145 
Fig. 4.21
ATI/AMD’s GPU developments over time . . . . . . . . . . . . . . . . . .
147 
Fig. 4.22
Nvidia’s GPU developments over time . . . . . . . . . . . . . . . . . . . . .
148 
Fig. 5.1
Signiﬁcant developments in the GPU ecosphere . . . . . . . . . . . . . .
152 
Fig. 5.2
The environment of the GPU . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
153 
Fig. 5.3
From the ATI R520 of 2005 to the AMD Polaris 
architecture of 2016, the feature size was halved four 
times. Reproduced with permission from AMD . . . . . . . . . . . . . .
153 
Fig. 5.4
In 2021, Intel announced it intended to build chips 
with a feature size of 20 angstroms in 2024. Reproduced 
with permission from Intel . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
154 
Fig. 5.5
Intel’s CEO, Pat Gelsinger, circa 2021. Reproduced 
with permission from Intel . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
154 
Fig. 5.6
The road map for transistor design, from FinFET to CFET. 
Reproduced with permission from IMEC . . . . . . . . . . . . . . . . . . .
155 
Fig. 5.7
A typical computer architecture with a graphics AIB 
connected via PCI Express . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
159 
Fig. 5.8
Typical integrated GPU with allocated system memory . . . . . . . .
159 
Fig. 5.9
Types of GPU memory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
160 
Fig. 5.10
Multi-chip module alternatives. Source OCP ODSA . . . . . . . . . .
161 
Fig. 5.11
MCM-GPU: aggregating GPU modules and DRAM 
on a single package. Source ISCA/Nvidia . . . . . . . . . . . . . . . . . . .
162 
Fig. 5.12
Intel’s die-to-die stacking is like the interposer technology 
in EMIB. Source Intel . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
162 
Fig. 5.13
AMD’s The die-to-die interface uses a direct 
copper-to-copper bond with no solder bumps. Source AMD . . . .
163 
Fig. 5.14
Heterogeneous integration enabled by an open chiplet 
ecosystem. Source UCIe . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
164 
Fig. 5.15
Packaging options using UCIe. Source UCIe Consortium . . . . . .
165 
Fig. 5.16
The development of graphics interconnect systems 
over time . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
166 
Fig. 5.17
The physical evolution of graphics AIBs over time. Source 
Free Online Dictionary of Computing . . . . . . . . . . . . . . . . . . . . . .
168 
Fig. 5.18
A typical VL AIB. Source Wikipedia . . . . . . . . . . . . . . . . . . . . . .
170 
Fig. 5.19
A riser card with a black slot for ISA and a white slot 
for PCI AIBs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
171 
Fig. 5.20
A typical PCI AIB. Source Wikipedia . . . . . . . . . . . . . . . . . . . . . .
171 
Fig. 5.21
AGP signaling and power conventions. Reproduced 
with permission from JigPu at English Wikipedia . . . . . . . . . . . .
173 
Fig. 5.22
Jim Pappas. Source Intel . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
174 
Fig. 5.23
Evolution of PCIe technology. Reproduced with permission 
from Intel . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
175

xxiv
List of Figures
Fig. 5.24
PCIe results and road map. Reproduced with permission 
from PCI SIG
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
175 
Fig. 5.25
PCIe form factors. Reproduced with permission from Intel . . . . .
176 
Fig. 5.26
The ﬁrst eGPU: ATI’s XGA (2008) . . . . . . . . . . . . . . . . . . . . . . . .
177 
Fig. 5.27
BT.709, sRGB, SMPTE 1886 (Gamma 2.4) = today’s 
digital content, BT.2020, SMPTE 2084 (PQ) = HDR 
Content’s color container. Source SMPTE . . . . . . . . . . . . . . . . . .
178 
Fig. 5.28
A virtual reality system . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
184 
Fig. 5.29
An augmented reality system . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
185 
Fig. 5.30
AR tracking and other components . . . . . . . . . . . . . . . . . . . . . . . .
186 
Fig. 5.31
An MR system . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
187 
Fig. 5.32
A simpliﬁed representation of a metaverse continuum 
(Milgram, 1994) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
187 
Fig. 5.33
Examples of telepresence robots. Reproduced 
with permission from Projexive . . . . . . . . . . . . . . . . . . . . . . . . . . .
188 
Fig. 5.34
Tearing when V-Sync was switched off. Reproduced 
with permission from Nvidia . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
190 
Fig. 5.35
Disabling V-Sync allows the GPU frame to be displayed 
when ready, which causes tearing on the screen as two 
or more frames were displayed in each refresh cycle . . . . . . . . . .
190 
Fig. 5.36
Frames from the GPU with V-Sync: if a particular frame is 
early or late, the result is stutter and input delay . . . . . . . . . . . . . .
190 
Fig. 5.37
If the G-Sync monitor was capable of a variable refresh 
rate, the GPU determines that rate . . . . . . . . . . . . . . . . . . . . . . . . .
190 
Fig. 5.38
FreeSync could sometimes cause brightness ﬂickering 
with FPS ﬂuctuations. Reproduced with permission 
from Display Ninja . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
194 
Fig. 5.39
Voodoo2 in SLI conﬁguration. Reproduced with permission 
from imgur.com . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
197 
Fig. 5.40
AMD’s XDMA multi-AIB. Reproduced with permission 
from AMD . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
198 
Fig. 6.1
EDSAC was the world’s ﬁrst stored-program computer. 
It began operation on May 6, 1949, 3,000 vacuum tubes 
and 12KW. Source Wikipedia Copyright Computer 
Laboratory, University of Cambridge. Reproduced 
by permission
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
202 
Fig. 6.2
Block diagram of the API and its relationship to the other 
components in a computer . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
203 
Fig. 6.3
The history of APIs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
204 
Fig. 6.4
Mobile graphics evolution through OpenGL ES to Vulkan. 
Source Khronos . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
208 
Fig. 6.5
A retained-mode API . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
213 
Fig. 6.6
An immediate-mode API . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
213

List of Figures
xxv
Fig. 6.7
Timeline of the evolution from IRIS GL to OpenGL 
evolution. Reproduced with permission from SIGGRAPH 
Asia 2008 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
215 
Fig. 6.8
The history of OpenGL to 2008. Reproduced 
with permission from SIGGRAPH Asia 2008 . . . . . . . . . . . . . . .
216 
Fig. 6.9
The Fahrenheit project was a clever idea that did not work 
out (image used with permission from Microsoft) . . . . . . . . . . . .
216 
Fig. 6.10
AMD’s Mantle program. Reproduced with permission 
from AMD . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
218 
Fig. 6.11
Apple’s depiction of a thin API between the GPU 
and application. Reproduced with permission from Apple . . . . .
219 
Fig. 6.12
A family tree of graphics APIs. Reproduced 
with permission from Neil Trevett . . . . . . . . . . . . . . . . . . . . . . . . .
220 
Fig. 6.13
The Vulkan development team, circa 2017. Reproduced 
with permission from Khronos . . . . . . . . . . . . . . . . . . . . . . . . . . . .
221 
Fig. 6.14
API layering. Reproduced with permission from Khronos . . . . .
221 
Fig. 6.15
Microsoft’s DirectX 12 Ultimate logo (Used 
with permission from Microsoft) . . . . . . . . . . . . . . . . . . . . . . . . . .
224 
Fig. 6.16
The shading rate is dynamically adjusted across the image, 
meaning that each 16 × 16-pixel region of the screen could 
have a different shading rate. Reproduced with permission 
from Nvidia . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
226 
Fig. 6.17
Clipping the portion of a model that is not visible . . . . . . . . . . . .
227 
Fig. 6.18
DirectX 8 introduced The programmable vertex shader 
in 2000 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
228 
Fig. 6.19
Expanded view of the geometry pipeline
. . . . . . . . . . . . . . . . . . .
228 
Fig. 6.20
George Lucas (AP Wirephoto—eBay, public domain, 
Wikipedia) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
229 
Fig. 6.21
Increasing the level of detail of a model through 
tessellation. Reproduced with permission from GDC
. . . . . . . . .
230 
Fig. 6.22
The geometry pipeline as of 2007—long and complicated . . . . .
232 
Fig. 6.23
AMD’s vertex processor can be a domain shader or a vertex 
shader . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
233 
Fig. 6.24
Evolution of the GPU pipeline . . . . . . . . . . . . . . . . . . . . . . . . . . . .
236 
Fig. 6.25
The traditional pipeline . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
237 
Fig. 6.26
The three stages of a mesh shader (where the choice of data 
was left to the developer) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
237 
Fig. 6.27
Breaking down a model into sections using meshlets . . . . . . . . . .
239 
Fig. 6.28
Triangles and vertices in a strip . . . . . . . . . . . . . . . . . . . . . . . . . . .
239 
Fig. 6.29
Triangles and vertices in an array . . . . . . . . . . . . . . . . . . . . . . . . . .
240 
Fig. 6.30
Injection of the ampliﬁcation shader . . . . . . . . . . . . . . . . . . . . . . .
240 
Fig. 6.31
Dataﬂow in the mesh shader pipeline . . . . . . . . . . . . . . . . . . . . . .
241 
Fig. 6.32
The iconic 3DMark mesh shader test image . . . . . . . . . . . . . . . . .
242

xxvi
List of Figures
Fig. 6.33
A screenshot from the interactive mode, in which 
the meshlets in the scene were visualized. Notice 
that the foreground occludes the faces behind . . . . . . . . . . . . . . . .
243 
Fig. 6.34
An improvement of 730% in the FPS was achieved using 
mesh shaders . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
243 
Fig. 6.35
This entire image was calculated—it is not a photograph 
or texture map. Reproduced with permission from Epic 
Games . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
244 
Fig. 6.36
This image is what 20-million triangles look like at 4 
k—each color represents a different triangle. Reproduced 
with permission from Epic Games . . . . . . . . . . . . . . . . . . . . . . . . .
245 
Fig. 6.37
A soldier consisting of 30 million triangles, rendered in real 
time with global illumination. Reproduced with permission 
from Epic Games
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
246 
Fig. 6.38
There are extensive models, tremendous depth of ﬁeld, 
real-time physics, character animation, motion blur, 
and more. Reproduced with permission from Epic Games . . . . .
247 
Fig. 7.1
Data ﬂow for Nvidia’s DLSS 2.0 process . . . . . . . . . . . . . . . . . . .
254 
Fig. 7.2
Amazon’s Bistro demo . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
256 
Fig. 7.3
Anton Kaplanyan, developer of denoising. Source Intel
. . . . . . .
257 
Fig. 7.4
AMD’s super-resolution gaming patent. Source U.S. Patent 
and Trademark Ofﬁce . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
258 
Fig. 7.5
Alexander Potapov, machine learning graphics specialist 
at AMD . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
258 
Fig. 7.6
Comparison of high resolution, XeSS scaled image, 
and low resolution. Courtesy Intel . . . . . . . . . . . . . . . . . . . . . . . . .
260 
Fig. 7.7
SYCL was a cross-platform and OS model. Source Khronos . . .
262 
Fig. 7.8
Getting from an image to maps and back again. Source 
Khronos . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
265 
Fig. 7.9
KTX was a lightweight container format for consistent, 
cross-vendor distribution of GPU textures and contained 
all the parameters necessary for efﬁcient texture loading 
and handling. Source Khronos . . . . . . . . . . . . . . . . . . . . . . . . . . . .
265 
Fig. 7.10
AMD tried to convince game developers to develop 
for both platforms, circa 2013. Source AMD . . . . . . . . . . . . . . . .
267 
Fig. 7.11
Nvidia launched its GameWorks suite in 2014 to persuade 
developers to take advantage of Nvidia’s special features. 
Source Nvidia . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
268 
Fig. 7.12
Percentage closer soft shadows softens shadows, making 
them more realistic. Source Nvidia . . . . . . . . . . . . . . . . . . . . . . . .
269 
Fig. 7.13
Comparison of AMD’s and Nvidia’s revenue and graphics 
market share over time . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
270 
Fig. 7.14
Richard Huddy being interviewed about GameWorks. 
Source PC perspective
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
271 
Fig. 7.15
AMD’s end-to-end open-source compute software stack . . . . . . .
273

List of Figures
xxvii
Fig. 7.16
Dennis “Thresh” Fong in the Ferrari he won from John 
Carmack (right). Source Heresy22, CC BY-SA 3.0 
Wikipedia . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
275 
Fig. 7.17
UI for AMD’s Adrenalin
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
276 
Fig. 7.18
Screenshot of the control panel for Nvidia’s GeForce 
experience . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
277 
Fig. 7.19
UI for Nvidia’s GeForce experience . . . . . . . . . . . . . . . . . . . . . . .
278 
Fig. 7.20
Ray marching takes a different approach to the ray-object 
intersection problem ray marching does not calculate 
an intersection analytically. Instead, it marches a point 
along the ray until it ﬁnds a point that intersects an object
. . . . .
279

List of Tables 
Table 1.1
Graphics chip supplier from 1980 to 2023 . . . . . . . . . . . . . . . . . .
16 
Table 2.1
Shading rates and coarse pixel size . . . . . . . . . . . . . . . . . . . . . . . .
52 
Table 3.1
The evolution of the APIs and OS relative to the eras 
of GPUs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
67 
Table 4.1
First-generation GPUs . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
109 
Table 4.2
Early GPUs from ATI and Nvidia . . . . . . . . . . . . . . . . . . . . . . . . .
122 
Table 4.3
ATI’s R100 had three texture units per graphics pipeline . . . . . .
127 
Table 4.4
Northbridge comparisons . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
139 
Table 5.1
Types and generations of memory . . . . . . . . . . . . . . . . . . . . . . . . .
157 
Table 5.2
Comparison of GDDR5, GDDR5X, HBM, and HBM2 
memory . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
158 
Table 5.3
PC bus standards . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
167 
Table 5.4
AGP and PCI: 32-bit buses operated at 66 and 33 MHz, 
respectively . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
173 
Table 5.5
AGP power provisioning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
174 
Table 5.6
Formats and versions of HDMI video (Wikipedia) . . . . . . . . . . .
180 
Table 5.7
Comparison of AR and VR characteristics. Source Phillip 
Rauschnabel . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
189 
Table 5.8
G-Sync modes and results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
191 
Table 6.1
History of modern APIs for all platforms . . . . . . . . . . . . . . . . . . .
205 
Table 6.2
DirectX 9 shader version . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
208 
Table 6.3
Comparison of DirectX vertex shaders . . . . . . . . . . . . . . . . . . . . .
209 
Table 6.4
Compatibility of APIs with OSs and their deployment . . . . . . . .
210 
Table 6.5
New programmable features introduced to the API 
in DirectX 10 and 11 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
230 
Table 6.6
Comparison of the characteristics of the DirectX 8, 
DirectX 9, and DirectX 10 shaders . . . . . . . . . . . . . . . . . . . . . . . .
234 
Table 7.1
The ﬁve modes of AMD’s FidelityFX FSR . . . . . . . . . . . . . . . . .
259 
Table 7.2
Quality modes offered by Intel’s XeSS scaling options . . . . . . . .
260 
Table 7.3
Intel’s XeSS data-ﬂow diagram . . . . . . . . . . . . . . . . . . . . . . . . . . .
260 
Table 7.4
GPU-compressed texture format fragmentation . . . . . . . . . . . . . .
263
xxix

xxx
List of Tables
Table 7.5
Comparison of the two modes of Basis Universal, UASTC, 
and ETC1S . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
265 
Table 7.6
AMD’s initial GPUOpen resources from 2015 . . . . . . . . . . . . . . .
272

Chapter 1 
Introduction 
By the mid-nineties, the forerunners of the fully integrated single-chip GPU were 
appearing with a range of functionality. What companies built often reﬂected their 
origin of their fonders, and target markets. 
LSI graphics accelerator chips appeared in the early 1980s as 2D drawing engines, 
with CAD being the primary application. In 1981, SGI introduced the ﬁrst 3D trans-
form engine, the Geometry Engine [1]. The ﬁrst company to offer a 3D AIB was 
Matrox in 1987 with the SM 640 that used the SGI Geometry Engine [2]. 
Semiconductor companies saw the need to decouple the geometry processing 
from the CPU and make it more tightly coupled with the pixel pipeline. Several 
companies developed ﬂoating-point processors (FPUs) to do the job. 
Weitek introduced its ﬂoating-point processor (FFP) in the early 1980s and was 
successful with it as a coprocessor to the CPU. Some groups tried using it for geom-
etry processing. The Pixel Planes system used it and it was used in the Sun worksta-
tion, and even by SGI. However, it never was used in any PC graphics AIBs. It was not 
a geometry processor per se but a general-purpose ﬂoating-point coprocessor. In the 
early 1990s, the company introduced a VGA clone, the Power 9100. The company 
did not do well, and in late 1996, Rockwell’s Semiconductor Systems bought the 
assets. 
In May 1994, 3Dlabs announced its ﬁrst consumer 3D graphics chip, Gigi (which 
stood for Game Glint). It was a scaled-down version of the GLINT 300TX. Creative 
Labs built an AIB with it, the 3D Blaster VLB, in early 1995. However, Windows 
did not have an API that exposed T&L functions in the graphics hardware. There-
fore, 3Dlabs chips came with 3Dlabs drivers for Windows 95/NT (accelerated 2D), 
accelerated D3D drivers, and accelerated OpenGL drivers. The 3D drivers would use 
whatever acceleration was enabled by the API and available on the current hardware. 
The ﬁrst consumer grade Geometry Engine chip was the Fujitsu (MB86242) 
Pinolite FXG-1, revealed at the Hot Chips conference at Stanford University in 1995 
[3] and formally announced in July 1997 [4]. Rendition was the ﬁrst company to 
use (and helped Hercules develop) an AIB with it and Rendition’s 3D chip in 1998. 
However, the AIB’s release got canceled because 3Dfx came out with a faster product.
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2022 
J. Peddie, The History of the GPU – Eras and Environment, 
https://doi.org/10.1007/978-3-031-13581-1_1 
1

2
1
Introduction
Also, DirectX did not expose the geometry engine to applications so custom drivers 
had to be written. 
Also, in 1997, 3Dlabs announced their Glint Gamma (G1) stand-alone geometry 
processor for the professional graphics market. Then they adapted it to a Creative 
Labs consumer AIB in 1988. 
Being ﬁrst is always a tricky thing to identify. First to announce or ﬁrst to ship? 
Or, if you could ﬁnd out, ﬁrst start a design? However, the one thing that can be said 
is that 3D consumer chips appeared in 1995. 
Those early 3D chips did not include hardware transform and lighting (T&L) 
capabilities. The CPU or a separate coprocessor was used for those operations. But 
Moore’s law was on the march, and it would be only a few years until the ﬁrst fully 
integrated single-chip graphics processing unit (GPU) was introduced, marking the 
beginning of the GPU era. 
1.1 
Nvidia’s NV10—First Integrated PC GPU (September 
1999) 
The term GPU has been in use since at least the 1980s. Nvidia popularized it in 1999 
by marketing the GeForce 256 AIB as “the world’s ﬁrst GPU.” It offered integrated 
transform, lighting, triangle setup/clipping, and rendering engines as a single-chip 
processor. 
It had all the features for a truly integrated GPU. 
Very large-scale integrated circuitry (VLSI) started taking hold in the early 1990s. 
As the number of transistors that engineers could incorporate on a single chip 
increased almost exponentially, the number of functions in the CPU and the graphics 
processor increased. Among the biggest consumers of the CPU were graphics 
transformation compute elements into graphics processors. Architects from various 
graphics chip companies decided that transform and lighting (T&L) was a function 
that should be in the graphics processor. A T&L engine is a vertex shader and a 
geometry translator—many names for the little FFP. 
In 1997, 3Dlabs (in the UK) developed its Glint Gamma processor, the ﬁrst 
programmable transform and lighting engine, as part of its Glint workstation graphics 
chips. They introduced the term GPU—geometry processor unit. 3Dlabs’ GPU was 
a separate chip named Delta and was known as the DMX. 3Dlabs’ GMX was a 
coprocessor to the Glint rasterizer. 
Then in September 1999, Nvidia introduced the NV10 GPU with an integrated 
T&L engine for their consumer graphics chip. ATI quickly followed with their 
Radeon graphics chip and called it a VPU—visual processing unit. But Nvidia popu-
larized the term GPU. It has forever since been associated with the GPU and credited 
with inventing it (Fig. 1.1).
Built on TSMC’s 220 nm process, the 120 MHz NV10 had 23 million (incorrectly 
listed as 17 on some websites) transistors in a 139 mm2 die and used DirectX 7.0.

1.1 Nvidia’s NV10—First Integrated PC GPU (September 1999)
3
Fig. 1.1 Die shot of Nvidia’s ﬁrst GPU, the NV10. Reproduced with permission from Curtis Priem
The GeForce 256 AIB employed the NV10 with SDR memory. Refer to the block 
diagram in Fig. 1.2. 
The chip had many advanced features, including four independent pipelined 
engines that ran at 120 MHz. That allowed the GPU to produce a 480 Mpix/s ﬁll rate.
Fig. 1.2 Nvidia’s GeForce 256 (NV10) block diagram with OpenGL 

4
1
Introduction
Fig. 1.3 VisionTek GeForce 256 DDR. Source Hyins for Wikipedia 
The video output was VGA, came with hardware alpha blending, and was HDTV 
(1080i) compliant. 
In addition to advanced graphics features, the chip also had a powerful video 
processing capability. It had a TV out capability and integrated NTSC/PAL encoders. 
It supported S-VHS and Composite video input and stereo 3D. 
Before Nvidia started offering Nvidia-branded AIBs, the company relied on AIB 
partners to build and sell the boards (Fig. 1.3). However, Nvidia did offer a reference 
design to its OEM partners. 
The GPU had a large 128-bit memory interface and could use DDR or SGRAM 
memory, a choice left to the OEM board partners and usually made according to a 
price-performance trade-off. The AIB shown in Fig. 1.3 has four 8 MB SGRAM 
chips. Since it was a 1999 AIB, it used the AGP 4X interface with sustained DMA 
and supported the Direct3D 7.0 API and OpenGL 1.2.1 with transform and lighting. 
In addition to advanced graphics features, the chip also had a powerful video 
processing capability. It had a TV out capability and integrated NTSC/PAL encoders. 
I supported S-VHS and Composite video-input, and stereo 3D (Fig. 1.4).
Sometime in July 1999, before announcing the new processor, Nvidia’s vice pres-
ident of Marketing, Dan Vivoli, took a stroll in the parking lot with fellow SGI alumni 
Sanford Russell, hoping to come up with a name for the new chip. 
“We knew we had to position it away from all the other graphics chips. It was big (and 
therefore costly), so we needed to command a premium. And it was a breakthrough since 
it had T&L. And we knew that our road map was evolving to very capable processors. 
We ﬁgured positioning our products closer to the CPU would be a clever way to distance 
ourselves from the dozens of other graphics chip companies.

1.1 Nvidia’s NV10—First Integrated PC GPU (September 1999)
5
Fig. 1.4 Dan Vivoli named 
the GPU
“We intentionally did not trademark or copyright the term GPU. We wanted it to become a 
category,” said Vivoli [5]. 
The GPU was Almost Blocked 
In 1997, Intel’s Pat Gelsinger, Intel’s youngest Vice President, launched its Intel’s 
Developer Forum (IDF). It was an extremely popular event, and any company or 
person involved with the PC wanted to attend or exhibit. The ﬁrst IDF in October 
1996 featured a presentation by Intel of its new Multi-Media Extensions—MMX. 
MMX was a single instruction, multiple data (SIMD) instruction set architecture 
within the Pentium P5. Among the many uses of the dedicated function were geom-
etry calculations for graphics based on developments Intel had done with the i860 
introduced in 1989. 
Prior to MMX, geometry calculations ran on the CPU’s FPU—MMX would speed 
up the operation and free up the FPU. Intel told application developers to write to 
the MMX, which, with the higher clock speed of the Pentium, would give their apps 
a huge performance boost with minimal effort—a simple software patch. And so, 
the developers modiﬁed their applications, including game developers. A year later, 
AMD introduced their answer to MMX, a SIMD extension called 3DNow! 
Nvidia had been working on the NV10 for a while and saw an opportunity to 
introduce it at the 1999 IDF to be held in Palm Springs, California in September that 
year. But Nvidia could not just show up with a disrupting technology and hope to 
have Intel welcome them with open arms, especially when it threatened to shift work 
from the CPU and minimize Intel’s perception of its value. And there was the little 
matter of the R&D and manufacturing investment Intel had made putting MMX in 
the CPU. Every square nanometer of silicon represented cost, and there had to be an 
ROI to justify it. Nvidia was about to reduce that ROI, or at least until Intel could ﬁnd 
other applications to run on MMX—which it would do without too much difﬁculty, 
but Intel was not about to cede any territory at the time. 
Knowing all that, Vivoli formed a team to explain and try to inﬂuence the senior 
Intel managers not to put in any microcode that would block Nvidia’s T&L engine 
from accessing the applications. Nvidia had the team go to Microsoft with the same

6
1
Introduction
objective. Microsoft controlled the API, and all developers had to work with it and 
through it. That was close to a life-or-death situation for Nvidia. If their wonderful 
T&L was blocked, that would be dead silicon and denial of a major product announce-
ment. Nvidia had developed cube-mapping to exploit the T&L engine, and a lot of 
that investment would be lost if T&L was forced back onto the CPU. Vivoli told the 
team, “Just get it done, no matter what it takes [5].” 
The effort at Microsoft was a little easier, Ted Hase and Chas Boyd were heavily 
involved in the DirectX team, and Kevin Bachus was the marketing guy. He too was 
a former SGI employee. If Nvidia could convince him, things would be easier. The 
Microsoft people were very technical and knew that moving T&L to the graphics chip 
would be the right thing to do, so Nvidia had Microsoft’s support, but not advocacy. 
At Intel, it was a multipronged effort to gather support from many different groups 
across the company. The Nvidia team—David Kirk, chief architect and director of 
applications, and Mark Reed, the director of strategic marketing—spoke with Intel’s 
James Akiyama (CPU bus architecture), David Swanson (platform architecture), 
Brian Napier (CPU SDV group, Oregon), Kevin Corbett (director of strategic product 
planning), and David Fair (director of IHV relations). Fair, living up to his name, 
provided input into the CTO organization that the GPU’s T&L engine would provide 
a complementary and not competing technology for CPUs. The CPU could do more 
with physics and AI, which game developers and movies studios were begging for. 
The Nvidia people showed their planned press release to all the Intel people, and 
ﬁnally, everyone agreed it would be OK. Intel put out a statement by Pat Gelsinger 
(Fig. 1.5). 
“Intel has been working with Nvidia to shape the future of PC platforms and provide new 
levels of intelligence and realism in simulations, entertainment, and enhanced Internet expe-
riences,” said Pat Gelsinger, Intel’s vice president and general manager of the Desktop 
Products Group. “The Pentium III processor, when balanced with next-generation GPU 
architectures like Nvidia’s, enables dramatically increased levels of lifelike 3D graphics on 
Intel high-performance desktop platforms [6].” 
Integrating transform and lighting capability into the GPU was a signiﬁcant differ-
entiator for the GeForce 256. Before the GeForce 256 and the 3DLabs stand-alone 
T&L Glint processor, previous and competitive 3D accelerators used the CPU to run
Fig. 1.5 Pat Gelsinger 
launched Intel’s IDF in 1999. 
Source Intel 

1.1 Nvidia’s NV10—First Integrated PC GPU (September 1999)
7
those functions. Incorporating the T&L capability reduced cost for consumer AIBs 
while simultaneously improving performance. Before the GF256, only professional 
AIBs designed for CAD had a T&L coprocessor engine. It also expanded Nvidia’s 
market by allowing the company to enter the professional graphics segment. Nvidia 
marketed those AIBs as Quadro. The Quadro AIB used the same NV10 as the GeForce 
AIBs and developed certiﬁed drivers for various professional graphics applications. 
Transform, Clipping, and Lighting 
Although discussed in several chapters in this book, a few words here on transforma-
tions, clipping, and lighting are appropriate as we transition the story into the realm 
of the GPU. 
As discussed, we deﬁne the era of the GPU as beginning with Nvidia’s successful 
introduction and production of a graphics controller with an integrated transform and 
lighting (T&L) engine at consumer prices. T&L had been a requirement of computer 
graphics (CG) since the beginning of the 1950s. The ﬁrst stand-alone T&L chip was 
Jim Clark’s Geometry Engine in 1980. Shrinking that capability and integrating it 
took 19 years, and Moore’s law to get from 3,000 nm to 229 nm. Clark’s device had 
40,000 transistors. Nvidia’s chip had 17 million. 
Cubic Environment Mapping 
The late 1970s and early 1980s saw the development of ideas and techniques for 
reﬂection mapping and environment mapping. In 1976, while at NASA’s Jet Propul-
sion Laboratory (JPL), Jim Blinn got together with Martin Newell at the University 
of Utah, who, among many accomplishments, was the creator of the famous Utah 
teapot (Fig. 1.6). 
Fig. 1.6 The Utah teapot, a model by Martin Newell (1975) and used ever since. Reproduced with 
permission from Dhatﬁeld for Wikipedia

8
1
Introduction
Fig. 1.7 An example of a 
texture mapped to the faces 
of a cube box, also called a 
skybox. Reproduced with 
permission from Arieee for 
Wikipedia 
Blinn and Newell took Ed Catmull’s algorithmic work (while at the University of 
Utah) in 1974 for rendering images of bivariate (a result that involves or depends on 
two variables) surface patches and extended it. 
Blinn and Newell’s extensions looked at the areas of texture simulation and 
lighting models. Their patch rendering algorithm allowed accurate computation of 
the surface normal to the patch at each picture element, which permitted the simu-
lation of mirror reﬂections. In 1976, they presented a paper on it at SIGGRAPH 
[7]. 
Then in 1986, Ned Green, who was at NYIT, showed how the concepts from 
Catmull, Blinn, and Newell could be applied to a cube, whose interior was the 
environment for the object(s) to be portrayed and displayed. Green published a paper 
in Proceedings of Graphics Interface and Vision Interface, in 1986 and it has become 
the standard reference for cube-mapping (Fig. 1.7) [8]. 
However, in 1986, only high-end systems such as Evans and Sutherland’s ES-1 
supercomputer and SGI could simultaneously handle the math workloads to access 
six texture images. It was not until 13 years later, when Nvidia released the NV10 
GeForce 256 in 1999, that such capabilities came to the PC and everyone. 
Working with Microsoft, Nvidia convinced Redmond to incorporate cube envi-
ronment mapping (CEM) into the DirectX 7.0 release. It was already available in 
OpenGL 1.2.1 as an extension developed by Nvidia. It became ofﬁcial in 2001 in 
OpenGL 1.3. 
Hardware-accelerated cube environment mapping allowed game developers to 
create accurate, real-time reﬂections and use specular lighting effects and reﬂections 
for environments to increase the illusion of immersion. Wrapping the six-sided cube 
map around the scene provided a new and relatively easy development path to creating 
reﬂective images [9]. 
The cube-map enabled linear mapping in all directions within a six-panel texture 
cube. As a result, the reﬂections did not suffer the warping or distorting singularities

1.1 Nvidia’s NV10—First Integrated PC GPU (September 1999)
9
Fig. 1.8 Jen-Hsun (Jensen) 
Huang introduced the GPU 
in 1999. Source Nivida 
one got with a sphere map. That was especially true at the edges of the reﬂection 
(Fig. 1.8). 
Jensen Huang, Nvidia’s CEO, stated matter of factly in 1999: 
The GPU is a major breakthrough for the industry and will fundamentally transform the 
3D medium. It will enable a new generation of amazing interactive content that is alive, 
imaginative, and captivating. The richness of this new 3D medium will have a profound 
impact on the future of storytelling and will broaden the appeal of 3D far beyond the game 
enthusiasts [10]. 
1.1.1 
Nvidia’s Non GPU GPU 
As has been pointed several times in this book, Nvidia is credited with establishing 
the name GPU and producing the ﬁrst PC-based fully integrated single chip capable 
of independent transformations and lighting processing. (boy is that a mouthful). 
And we all know that chip was the NV10 and was instantiated on the GeForce 256 
AIB. 
But Nvidia actually had a functional transform engine before that in the Riva 
128 AIB, powered by the NV3. Everything refers the Riva 128 as having a massive 
ﬂoating point transformation engine (4 times 4 × 4 matrix), not just a ﬂoating-point 
interface like the Sun GX did—refer to the Nvidia Riva 128 [11]. 
“The NV3 deployed 3.5 million transistors and 20 billion operations per second 
dedicated to 3D graphics.” Said Curtis Priem, the designer of the NV3. 
The 20 billion operations per second was referring to the calculations (multi-
ples and adds) that were being done in the pixel pipeline (which included the ﬁve 
GFLOPS for transforms). The pixel pipeline was ﬁxed point. The ﬁnal values were 
8-bit integers for all the color components and 16-bit for the z-buffer. Everything 
was the bare minimum needed for each result. (The ﬁve-GFLOP operations plus 15 
GFixedPoint operations per second add up to 20 Giga Operations per second.) 
The NV3 had the hardware to do transforms (and lighting) but it couldn’t be 
accessed by the applications. The NV3 was constrained by the Direct3D 5.0 API of

10
1
Introduction
the time which relegated the FPU in the Riva128 to triangle setup. The RIVA128 
was however compatible with OpenGL but OpenGL through Windows was slow and 
buggy and only one game developer tried to use it (Id for Doom). 
As explained above, the accession to the throne of an integrated single-chip T&L-
based graphics processor was a road riddled with trip wires. APIs were not available 
or buggy, competitors with opposing agendas could thwart the use of the engine, and 
applications were needed to exploit it. 
1.1.2 
The S3 Nvidia Patent Suit and Silicon Valley Gossip 
In early 1998, as the graphics controller market was showed signs of consolidation. 
S3 was one of the most successful companies. Cirrus Logic, formerly number one, 
had slipped due to an inability keep up with the technological developments being 
successfully exploited by its competitors. Nonetheless, Cirrus had plenty of develop-
ments of its own and the foresight to patent many of them. Many were foundational 
patents with little to no challenge about prior art—they were valuable patents. 
Lawsuits were popping up all the time as companies tried to protect their market 
position in a rapidly shifting technology-driven environment. So, it was not much of 
a surprise in January 1998 when S3 announced it had agreed to a $40 million patent 
purchase and cross-licensing deal with Cirrus Logic to enable a patent portfolio 
exchange between the two companies [12]. 
S3 would buy ten graphics patents and 25 graphics patent applications from 
Cirrus Logic for $40 million through the patent purchase. Concerning Cirrus Logic’s 
remaining patents not covered in the purchase and S3’s patents, S3 and Cirrus Logic 
entered an accompanying cross-licensing agreement. Under the terms of the deal, 
S3 and Cirrus would have a perpetual license to each other’s graphics patents and 
additional licenses for the other party’s patents. 
Cirrus wanted $50+ million, and they were also negotiating with Intel. S3 needed 
to let its customers know things were OK, and there were no obstacles to them from 
then on—none except Real3D, that is. Real3D had even more signiﬁcant and more 
foundational patents. 
The ink had hardly dried on the deal, and S3 was still in the process of hanging 
the patents on their walls when they sued Nvidia for violating 15 of their patents 
[13]. The remedy S3 sought was to have the court shut down Nvidia and make them 
stop shipping its products. Thankfully for Nvidia, the court declined S3’s request in 
July 1998. 
Silicon Valley is a Really Small Town 
A few days later, a big shot from Compaq (a Houston-based computer company, later 
acquired by HP, that bought thousands of AIBs) came to the valley to check out the 
latest crop of chips and potential AIB builders. Usually, those suppliers fought over 
coach seats to get to Houston, but that time it was abalone season in California, and 
the Compaq manager wanted to get out of the swelter of Texas. His ﬁrst visit was to

1.1 Nvidia’s NV10—First Integrated PC GPU (September 1999)
11
S3. S3 then took him to meet the Hercules people who would build the AIB with an 
S3 chip that Compaq might buy. Compaq wanted something fast, and if it was a fast 
AIB you needed, Hercules was where you went. That meeting went well, and then 
the Compaq guy excused himself and went, well, elsewhere [14]. 
Having visited “elsewhere,” he and the Nvidia rep were driving to dinner on North 
101, the highway that runs from San Diego to Oregon and right through Silicon Valley. 
Highway 101 at any time of day is exciting, but for the Nvidia guy, during rush hour 
with a hot prospect in the car and the steaming heat outside (for California was 
having a heatwave then, too), it was too much. He plowed into the back of another 
car. A car driven by a guy from S3. A guy from S3 with a German passenger, a guy 
from Hercules. It seems the S3 and Hercules people were sufﬁciently pleased with 
Compaq’s visit that they had decided to treat themselves to dinner. 
The S3 guy and the Nvidia guy exchanged driver and insurance data. Meanwhile, 
the Hercules guy talked with the Compaq guy while standing in the emergency lane 
of 101 while stars of the silicon game whizzed by in their SUVs. Having gotten all 
the data and after a few handshakes, the respective (and now respectful) parties drove 
off to their destination dinner. 
The story could have ended there, except that this was Silicon Valley. While the 
S3–Hercules party was waiting in the restaurant for their table, the Compaq and 
Nvidia people came in and asked for a table. They say it’s hard to keep secrets in 
Silicon Valley, it was even harder in those days. 
1.1.2.1
Meanwhile, Back in Reality? 
By the end of 1998, rumors were ﬂoating around that 3Dfx would buy Diamond and 
Creative would buy 3Dfx. And then there were the stories involving aliens and S3. 
Obviously, there were good reasons for those rumors. The market was consolidating 
rapidly. It was seen ﬁrst in the high end when E&S acquired Accel and 3Dlabs 
acquired Dynamic Pictures. Elsa almost acquired Hercules, Diamond was preaching 
the turnkey manufacturing mantra, and the writing was on the wall. The problem 
was that the writing was hard to read, and everyone had a different translation. 
For example, NEC/VideoLogic was looking at the PC graphics business with 
renewed enthusiasm. The deal they would make with Sega gave the remaining chip 
companies a strong lineup of customers. One of the most immediate effects of the 
agreement was that it fueled speculation about the futures of Nvidia and S3. Both 
companies looked ripe for acquisition. But there were only a few companies in a 
position to do any acquisitioning. 
Intel, the Grand Aquisitioner, bought a stake in S3, as protection from S3’s IP 
claims. S3 had suits pending all over based on their IP, acquired from Cirrus. The 
Intel deal served notice to the industry that S3 had a case. However, several pointed 
out that the company could not survive on IP alone. The question was if the company 
would have the products to move forward with the Savage line. 
Meanwhile, with their line of Riva TNT chips already in the market, Nvidia looked 
like it was in the best position to beneﬁt from the fallout of the STB/3Dfx debacle.

12
1
Introduction
Nvidia was, observers said, the company most attractive to potential buyers. Maybe, 
but Nvidia was conﬁdent of its ability to go it alone in the graphics market, and they 
were still planning an IPO. 
Nvidia would be ﬁghting it out with the market leaders from Canada, ATI, and 
Matrox. ATI had the lead and was working out strategies for diversiﬁcation. On the 
other hand, Matrox wanted everyone to know that rumors of their imminent demise 
were exaggerated. They expected to bounce back stronger from the second-quarter 
slump that had affected everyone’s game plan. 
The 3Dfx deal had also dramatically highlighted the evolution of the AIB business. 
It was contracting into the game market, but that market had clearly deﬁned limits. 
After all, the graphics industry got to the consumer through the back door. The 
game market exploded because of a natural overlap between many computer users: 
young, ﬁnancially comfortable, technically savvy men and game players who did not 
get out enough. So, when the computer world talked about consumers, the pundits 
and publishers tended to think of them as game players. But that was shortsighted. 
Consumers are families, singles, and seniors; they watch television, watch movies, 
go out, buy things, and have whims as ﬂeeting as summer breezes. Those people 
would never be part of the game market. 
The real opportunities in the graphics market were elsewhere and maybe every-
where. Some opportunities existed in specialized markets such as arcades and 
location-based entertainment, ﬁnance, CAD, scientiﬁc visualization, simulation, and 
content creation. In other cases, it was going after the consumers where they live—in 
the living room. And that took everyone back to S3’s patents, of which many revolved 
around video. 
1.1.2.2
Nvidia and S3 Enter into a Cross-Licensing Agreement 
In February 2000, after pouring millions of dollars into lawyers’ pockets, Nvidia 
and S3 entered a seven-year broad cross-licensing deal that effectively ended the two 
lawsuits the companies had pending against each other [15]. At issue was S3’s lawsuit 
initiated a year before that accused Nvidia of patent infringement regarding its use 
of video technology and its suit against S3 regarding S3’s use of audio technology. 
Speciﬁcally, S3 took issue with Nvidia’s implementation of video scaling, video in a 
window, and some implementation of VGA technology. Somewhat later, right around 
the time S3 was making its ambitions for a potential IPO for RioPort known, Nvidia 
sued S3 based on Nvidia’s rights to audio and some I/O technology. In the end, it 
seemed a good time for a come-to-your-senses meeting, and indeed both companies 
did exceptionally well leveraging their patents for beneﬁcial deals. 
The agreement between the two companies was comprehensive and covered all 
patents held by the two companies. It did not include patents with third parties, such 
as S3’s patents with Intel and Nvidia’s patents with SGI. 
Nonetheless, both companies described the deal as a win-win. And, yes any agree-
ment that let the companies get back to business and out of the lawyer’s ofﬁces was a 
win. Jensen Huang, president, and CEO of Nvidia commented on S3’s broad patent

1.1 Nvidia’s NV10—First Integrated PC GPU (September 1999)
13
portfolio. “Over the years,” said Huang, “S3 has developed and acquired a rich port-
folio of technologies in graphics, multimedia, microprocessors, and system logic. 
Our license agreements with SGI and S3, combined with Nvidia’s rich technology 
patent portfolio, gives us a strong technology base on which to build winning products 
[16].” 
Andy Wolfe, S3’s CTO, observed that “access to intellectual property is rapidly 
becoming the highest barrier to entry in today’s PC graphics market.” Like Huang, 
he highlighted his company’s broad portfolio. He said the deal positioned the two 
companies as winners in the graphics market: “Over the past ten years, S3 has built 
an incredibly strong IP portfolio through internal development, as well as through 
licensing agreements with Intel, Exponential, Cirrus and now Nvidia. Based on our 
IP strength, S3 and Nvidia rank as possibly the only two companies positioned to 
win long-term in the PC graphics market [16].” 
Nvidia’s VP Michael Hara, the spin master, observed that the best use of lawsuits 
was to forge alliances between companies. As a result of Nvidia and SGI’s suits, 
the two companies had devised an arrangement that could result in more powerful 
graphics products for SGI. In this case,” said Hara, “the agreement between SGI and 
Nvidia lets the two companies get back to work instead of sending important company 
executives and engineers to depositions. Lawsuits simply consume the valuable time 
of valuable people,” and no doubt that was as much a reason to come to the bargaining 
table to settle as actual costs. In the long run,” said Hara, “the agreement is better for 
the industry. If you look at the bigger picture,” said Hara, “the goal for those of us 
in the graphics industry is to enable and improve all aspects of the industry. When 
you spend too much time maneuvering, you are not building better products for your 
customers [16].” 
S3’s public relations manager Paul Crossley said basically the same thing, “We 
thought it would be better to get back to work,” he said. However, Crossley also said 
that S3 believed they had a good shot at winning their lawsuit; they just decided 
that Nvidia’s IP was the bigger prize. Crossley also noted, “the partnership between 
the two companies is another example of all that co-opetition we have been hearing 
about. The deal,” said Crossley, “creates a barrier of entry for other companies [16].” 
Nvidia went on to become the leading and, some say, dominant supplier in the 
market. S3 went bankrupt, and their patents found their way to Via Technologies in 
Taiwan, HTC, and then Tianshu Zhixin in Shanghai (discussed in Book Three, New 
Developments) (Fig. 1.9).
The picture in Fig. 1.9 was taken on Nov. 27, 2018, at Nvidia’s Endeavor building 
just after it was built. The team’s families got together for a founder’s dinner. They 
would mainly cross paths at philanthropy events (Huang’s children were the most 
interested in the stories)—they were still doing it in 2021 despite the Covid. There is 
not a picture of when they started Nvidia in Prien’s townhouse. Unfortunately, they 
were not into documenting the journey then.

14
1
Introduction
Fig. 1.9 The founders of Nvidia: Curtis Prien, Jensen Huang, and Chris Malachowsky. Reproduced 
with permission from Curtis Prien
1.2 
Summary and Conclusion 
The evolution from 2D monochromatic graphic controllers to powerhouse 3D SIMD 
processors was like any other species’ evolution: fraught with expansion, collapse, 
and natural selection. The 2D controllers evolved to 3D as dramatically as ﬁsh moving 
to land. Color evolved from 2-bit monochrome to 36-bit HDR, like the expression of 
sentient development. Manufacturing and marketing challenges revealed the differ-
ences between the comfortable and small professional graphics segment and the 
take-no-prisoners world of consumer electronics. 
Dozens of companies tried to transition from 2 to 3D and discovered it was orders 
of magnitude more complicated and difﬁcult. Companies that had mastered on-off 
and a little intensity of pixels were crushed by the complexities of color spaces and 
the 3D nature of color. Suppliers who had mastered line drawing were overwhelmed 
by the addition of u, v, to x, y, z. Then came u1, v1. Fluctuations in memory prices 
not unlike any other commodity pricing environment completely confounded and 
confused organizations structured with a cost-plus, 12-month contract price supply 
chain. 
Companies that knew about computer graphics were naïve about scaling. Dozens 
of companies in the professional space could not understand consumer suppliers’ 
organizational structures and learned you could not scale down. In contrast, consumer 
suppliers such as ATI and Nvidia showed them how easy it was to scale up and satisfy 
the professional segment. 
The consumer space was not just about x, y, z graphics. It also included audio and 
video, and to succeed, you had to really be a multimedia company. You also had to 
be a software company and understand everything about APIs, operating systems,

1.3 Epilog—All the Others
15
drivers, applications, and security. The technical complexities that go into a modern 
GPU comprise a very long list. It was incredible that any single company could 
master it and is a testament to the suppliers’ population size—there are, by evidence, 
very few who could. Getting to the GPU was not an easy or direct path, and the ﬁeld 
is littered with glittering casualties, companies made up of brilliant people that just 
couldn’t get the next step done, or done in time. 
1.3 
Epilog—All the Others 
In 1990, 20 companies were trying to build or had declared they would, a 3D graphics 
chip. By 1997, the number of suppliers exploded to 46. And by 2001, the number of 
suppliers dropped to six. All told, 85 companies entered the graphics chip market at 
one time or another. 
Figure 1.10 shows the population of graphics chip suppliers over time. The 
complexities of developing a 3D chip and the shift of venture capital funding from the 
semiconductor builders to the booming internet companies killed dozens of compa-
nies. Other, better-funded companies like IBM and Lockheed withdrew for different 
reasons. 
The introduction of the single-chip integrated GPU was the last nail in the cofﬁn 
for dozens of companies. Table 1.1 lists all the companies that made or are making 
a graphics chip.
The list does not include specialty chips like ART’s ray tracing chip video 
processing chips.
Fig. 1.10 The rise and fall of graphics chip suppliers 

16
1
Introduction
Table 1.1 Graphics chip supplier from 1980 to 2023 
3Dfx
Headland
Samsung 
3Dlabs
IBM microelectronics
SGS Thomson/STMicro 
Accelerix
IGS (InteGraphics)/TiVa
Sierra 
AccelGraphics
IIT/Xtec
Sigma designs 
Acer labs(Ali)
IMS
Silicon reality 
Alliance
Intel
Silicon engineering 
Appian graphics
Intergraph/Intense3D
Silicon engines 
Arcobel
Jingjia (2014)
Silicon motion 
ARK logic
LG Semicon
SiliconArt (2019) 
Artist graphics
LSI logic
Silicon magic 
ArtX
Macronix
SiS 
AT&T bell labs
Matrox
S-MOS 
ATI
Media reality
SP3D 
Avance logic
MetaX 2020
Stellar graphics 
BitBoys
Mitsubishi (VSIS)
TI 
Bolt graphics 2021
NCR
Trident 
Brooktree
NEC
TriTech 
Chips and technology
NeoMagic
Tseng labs 
Chromatic research
Number nine
ULSI 
Cirrus logic
Nvidia
VIA 
Compaq
Oak technology
Video logic 
Creative
Paradise
W-D 
Digital media professionals
Philips
Weitek 
Digital semiconductor
Raycer
XGI 
Dynamic pictures
Real3D
Xiangdixian computing (2020) 
E&S
Rendition
Yahama 
Fujitsu
RSSI
Zhaoxin (2020) 
Gigapixel
S3
Zoran 
H P
References 
1. Peddie, J. Famous Graphics Chips: Geometry Engine, https://www.computer.org/publications/ 
tech-news/chasing-pixels/geometry-engine 
2. Peddie, J. The History of Visual Magic in Computers, Springer Science & Business Media. 
(Jun 13, 2013) 
3. Awaga, M. 3D Graphics Processor Chip Set, Hot Chips VII, (August 14, 1985), Wayback 
Machine (archive.org) 
4. Fujitsu Developed the world’s ﬁrst 3D geometric conversion processor for personal computers, 
(July 2, 1997), https://www.tinyurl.com/a2sfnjhh 
5. Interview with Dan Vivoli, April 28, 2021

References
17
6. Nvidia, 
Nvidia Launches the World’s First Graphics Processing Unit: Geforce 
256, https://www.pressreleases.responsesource.com/news/3992/nvidia-launches-the-world-s-
ﬁrst-graphics-processing-unit-geforce-256/ (August 31, 1999) 
7. Blinn, J. F. and Newell, M. E. Texture and reﬂection in computer generated images. 
Communications of the ACM Vol. 19, No. 10, 542–547 (1976) 
8. Green, N. Proceedings of Graphics Interface and Vision Interface ‘86, Vancouver, British 
Columbia, Canada: 26 -Pp 108–114 (May 30, 1986) 
9. Perfect Reﬂections and Specular Lighting Effects With Cube Environment Mapping, http:// 
www.developer.download.nvidia.com/assets/gamedev/docs/CubeEnvMapping2.pdf?display= 
style-table 
10. Peddie, J. Nvidia launches the NV10 GeForce, Volume XII, Number 36, pp1421, (September 
6, 1999) 
11. Peddie, J. Famous Graphics Chips: Nvidia’s RIVA 128, IEEE Computer Society 
(August 2019), https://www.computer.org/publications/tech-news/chasing-pixels/famous-gra 
phics-chips-nvidias-riva128 
12. Peddie, J. S3 and Cirrus Logic agree to a $40 million patent purchase and cross-licensing deal, 
The Peddie Report, Volume XI, Number 4, pp.158, (February 2, 1998) 
13. S3 ﬁles patent suit against Nvidia, The Peddie Report, Volume XI, Number 19, (May 18, 1998) 
14. Only in Silicon Valley, The Peddie Report, Volume XI, Number 32, (August 17, 1998) 
15. Nvidia and S3 enter into cross licensing agreement, The Peddie Report, Volume XIII, Number 
7, (February 14, 2000) 
16. Maher, K. Nvidia and S3 enter into cross licensing agreement, Volume XIII, Number 7, pp 
224, (February 14, 2000)

Chapter 2 
The GPUs’ Functions 
The Evolutionary Path—Darwinism in GPUs 
As the GPU has evolved over time and across platforms, its developers have navi-
gated the technology transitions at different rates and with unique concepts and 
designs. Some developers abandoned their original plans to compete better to adapt 
to more commonly accepted approaches. Others have opted for innovation at the risk 
of isolation as others catch up. The evolutionary path of the GPU has often been 
Darwinian with trial-and-error developments and winning and failing experiments. 
The suppliers try to outdo each other in performance, power reduction, features, and 
price. 
On the other hand, they have also striven for differentiation, (A) for distinction, 
and (B) to defeat easy comparisons. And, surprisingly, the population of suppliers 
is increasing. It has expanded and contracted following market forces. In the past, 
growth has been haphazard and boom-driven resulting in terrible losses of investment 
capital, jobs, and some novel designs. It is different now, and the new entrants are 
from established companies or start-ups with more seasoned investors, and in some 
cases, governments behind them. 
Given all this, tracing the history of the GPU is not a simple step A, step B, step 
C story. It is organic. It is one of assimilation and sad failures. Most people who 
get into the GPU industry do not stay with the same company, but many remain in 
the same ﬁeld, and that is positive. It creates pollination, the spreading of ideas, and 
exposure to new challenges and opportunities. The leading participants meet within 
various organizations to develop and drive standards. They also meet at conferences 
and read each other’s papers. All that acts as a catalyst for yet more ideas. There is 
a joy and delight in computer graphics that you will not ﬁnd in many other indus-
tries. You can see the results, the fantastic movies, and games, the simulations of 
exotic scientiﬁc experiments, the tremendous capabilities of robots, the designs of 
skyscrapers, automobiles, and electric toothbrushes.
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2022 
J. Peddie, The History of the GPU – Eras and Environment, 
https://doi.org/10.1007/978-3-031-13581-1_2 
19

20
2
The GPUs’ Functions
2.1 The Pipeline 
The image generation pipeline, or computer graphics interactive (CGI) pipeline, is 
a complex process as illustrated in Fig. 2.1. For simpliﬁcation, three primary stages 
describe it as follows: 
●Geometry generation, and model construction 
– Design, modeling, transformations, and rigging.
●Shading the geometric model and world 
– Surfaces (texture and color), staging, animation, lighting, and effects.
●Post – the ﬁnal stage of image generation (rasterizing) 
– Clipping, hidden surface removal composite, touch-up, rendering, and ﬁnal 
ﬁlm/video output. 
Although we refer to it as a pipeline, the steps are interdependent and accomplished 
through a ballet of hardware and software [1]. 
Before discussing the pipeline, we must establish a bit of GPU terminology. As 
discussed in the previous chapter, a GPU comprises dozens to tens of thousands of 
32-bit ﬂoating-point processors known as shaders. Those shaders are used in most 
of the pipeline stages and thus are called a Stage’s shader—e.g., Vertex shader. 
Brieﬂy, the major stages of the graphics rendering pipeline work like this: 
Each vertex that comprises the objects in a scene is retrieved from the 3D model’s 
database, one at a time, and processed by the vertex shader. Each vertex in the stream 
gets processed into an output vertex. It is then formed into a triangle, a geometric
Fig. 2.1 The graphics pipeline has been functionally the same since the 1970s

2.1 The Pipeline
21
primitive and the fundamental building block of all objects. Optionally, there is a 
tessellation stage to reduce (or add) triangles as needed for viewing acuity. 
Rasterizer 
When all the processing work is ﬁnished the ﬁnal image is passed to the output 
stage known as the rasterizer. That is, here the timing work is done the drive the 
display. Early graphics controllers (discussed in subsequent chapters) drove a scan-
line cathode ray tube (CRT) which was based on TV standards. Therefore, the timing 
circuitry in the output stage was called a CRT controller (CRTC). The CRTC ensured 
the video signal swept across the CRT in a correctly timed raster fashion, i.e., one 
line was displayed (painted) on the screen (from left to right, and then quickly sent 
back to the left and moved down one line and swept again. When LCDs replaced 
CRTs, the same process was used. 
Geometry Shader and Fragments 
A geometry shader sets up the primitives, transforms them, and clips the perspective 
to the viewport window space. Then scan conversion and primitive parameter inter-
polation are done, and in the process, fragments are generated. Fragments are the 
component parts of a pixel. A fragment shader processes each fragment and generates 
several outputs. Fragment shaders compute the renderings of a shape’s colors and 
other attributes. A fragment shader is the same as a pixel shader. One main difference 
is that a vertex shader can manipulate the attributes of vertices, which are the corner 
points of polygons. 
On the other hand, the fragment shader takes care of how the pixels between the 
vertices look. Programmable fragment processors are the last programmable stage 
in the graphics pipeline. An additional operation that may be conducted includes a 
scissor test, a stencil test, a depth test, blending, logical operations/tests, and a write 
mask. 
You can think of the processes as sheets, and sections can be made transparent 
or added to other sheets or opaque. Think of a mask as having cutouts that let the 
light come through. The mask, or other memory sheets, do not have an identiﬁable 
physical position but rather an address in a memory bank, which will change with 
each application. It is a virtual mask. 
The following is additional information on the pipeline and its associative parts. 
2.1.1 
The Meaning of Real Time 
Throughout this book, you will encounter the term, real time. Most of us have heard 
those terms for decades and take them for granted. Real time means now. If I move 
my hand from my chest to the top of my head, that is done in real time. If I move my 
cursor from the left side to the right, it moves in real time. But it doesn’t. Although 
your cursor feels like an extension of your hand, its movement across the screen is 
a series of movements. It is not like moving your hand to your head; it is more like

22
2
The GPUs’ Functions
an animation, a series of pictures of the cursor in different locations, and your brain 
makes up the parts in between because they are supposed to, should be, there. 
Real time has no universally accepted deﬁnition. For movies, it is 24 frames a 
second (fps). It is either 30 or 60 fps, or anything in between, or above for games. 
Generally, real time in games and computers is thought of as a time series of images 
(frames) that doesn’t ﬂicker. 
The quest of computer graphics, which includes CAD, animation, simulation, 
and gaming, is the production of the sense of realism—the suspension of disbelief. 
It is possible with a computer using ray tracing and global illumination to produce 
a perfectly accurate synthetic image, say from a CAD model, which reproduces the 
light’s behavior indistinguishable from reality. It has been possible since the late 
1970s. But, it hasn’t been fast. It has not been in real time or even close to it. Studios 
like Pixar, Dream Works, and Blue Sky will take as much as a day to generate one 
frame of a perfectly produced image. The time it takes is a function of the number of 
light sources, the number of objects in the scene, the number of materials, and how 
long you are willing to run the process—ray-traced images get better the longer you 
run them. 
But you can make very beautiful images without using ray tracing. Computer 
graphics has been called tricker or fake graphics. It uses clever algorithms techniques 
to fool us into thinking the scene we are looking at is real, or at least realistic enough 
to allow us to accept the story, play the game, and believe the simulation. 
In the previous section, the concept of the pipeline was introduced. Let’s do some 
numbers to get a sense of the workload. Let’s assume 30 fps deﬁnes real time. And 
let’s assume our screen is 1920 × 1080 or 2.14 million pixels, and for easy math, 
let’s say 2 million. And let’s say the pipe has ﬁve stages. (I know, I promised no 
math). Here’s the raw data: 30 fps times 2 million (pixels) equals 60 million pixels 
a second. Each stage of the pipeline creates a more detailed and complicated image. 
The ﬁrst stage is just a few points, the vertices of the triangles, and some information 
about the colors the ﬁnal pixels should be. The back-end stages turn that information 
into the actual visible pixels, and in the middle, the stages determine what is visible 
in the images. That’s a lot of work going on at once. 
And going on at once is the secret behind a GPU—a modern GPU can have 10 s 
of thousands of jobs going at once at GHz speeds. But it took over ﬁfty years to get 
to that point, the point of real-time super high-resolution, absolutely beautiful, and 
physically accurate images. 
In the following sections of this chapter, we’ll identify the parts that make up this 
wonderful device we call a GPU, and then we will trace the history of how it came 
about. 
2.1.2 
Binning and Branding 
When a silicon wafer is made with dozens to hundreds of chips on it, the chips near 
the center will often be a higher quality. Designers know this and target their designs

2.1 The Pipeline
23
for the mid-band of the wafer. Chip failures are usually on the outer ring of the 
wafer, and parts between the outer ring and the mid-band usually are functional but 
don’t quite satisfy the speciﬁcation goals after testing. All of that results in the chip 
companies selecting the various chips and putting them in performance bins. Hence, 
the process is called binning. The lower performing parts are used for products in a 
lower segment, and not just thrown away because they didn’t meet the desired design 
goals. 
Almost all complex semiconductors are built with redundancy. If the speciﬁcation 
says 100 shaders the actual chip may have 110, the extra shaders are there to replace 
any that might have failed in testing, or never came to life. Sometimes, the exceptional 
parts come out with all the shaders working, and at peak clock frequencies. If there 
are enough exceptional parts, the semiconductor company will offer them as a super 
version. They might be called overclockers. Nvidia established the Titanium brand 
for such parts, abbreviated Ti. The ﬁrst GPUs from Nvidia in that category were the 
150 nm NV15 part used on the GeForce2 Ti in October 2001. Nvidia has followed that 
branding scheme ever since, only skipping a generation when the yield of exceptional 
parts wasn’t big enough. 
Binning is a sorting process in which superior-performing chips are sorted from 
speciﬁed and lower performing chips. It can be used for CPUs, GPUs (graphics 
cards), and RAM. The manufacturing process is never perfect, especially given 
the incredible precision necessary and number of transistors to produce GPUs 
and other semiconductors. Manufacturing high-performance and expensive 
GPUs results in getting some that cannot run at the speciﬁed frequencies. 
Those parts however may be able to run at slower speeds and can be sold as 
less expensive GPUs. 
2.1.3 
The Frame Buffer 
The frame buffer plays a primary role in computer graphics and multimedia in 
general. It is a large memory array that takes input from almost every graphics pipeline 
stage. The frame buffer is a three-dimensional memory array, with its minimal speci-
ﬁcations being the resolution of the display (x, y, or pixels multiplied by lines) and the 
color range or depth (z, Intensity, or color)). The color depth should not be confused 
with the depth of a rendered scene of objects. Information about the order of those 
objects (and their visibility) gets stored in a companion memory array, the z-buffer. 
A frame buffer with a depth of 8-bits (8-bits for each pixel’s location) can provide 
2 × 108, or 16.8 million unique shades or colors. Higher quality color depth such 
as high dynamic range (HDR) color requires 10- or 12-bits of color depth. A 10-bit

24
2
The GPUs’ Functions
frame buffer can provide 210, or 1.07 billion colors, while a 12-bit frame buffer can 
offer 212, or 68.7 billion colors. 
For a little more explanation on how the bits translate to colors or shades, refer to 
The GPU Environment-Hardware. 
When digital computers started to appear in the 1940s, memory was expensive, 
and the cost of memory was a critical limiting factor until the early 1970s. Researchers 
knew the need for a frame buffer and approached it from different directions [2]. The 
many techniques tried included storage tubes, delay lines, and other apparatus. Joan 
Miller gets credited with creating a simple 3-bit paint program at Bell Labs in 1969 
that could generate half-tone images on a raster display in real time. Miller’s frame 
buffer used a drum storage system. The storage of new graphical data in the memory 
was a function of the drum’s rotation speed [3]. Miller developed his system 3 years 
before Shoup’s work, but his work has been largely undocumented (Fig. 2.2). 
Meanwhile, like the graphics processors, semiconductor memory was beneﬁting 
from VLSI. And as predicted by Gordon Moore, as density went up, so did speed, 
while prices per bit went down. 
As a result, in the early 1970s, metal oxide semiconductor (MOS) memory chips 
with one kb of storage became economical. The ﬁrst known person to take advantage 
of this increased density at lower prices was Richard Shoup at Xerox PARC. 
As far as we know, Shoup built the ﬁrst 8-bit raster memory frame buffer in 1972, 
which he named the SuperPaint system [4]. Others followed, such as Evans and 
Sutherland in 1974, who built a 12-bit frame buffer. Universities like Ohio State and 
the New York Institute of Technology (NYIT) connected three of them to form an 
astounding 24-bit frame buffer.
Fig. 2.2 A digital image of Richard Shoup, rendered on the Super Paint system, April 1973. Source 
The Richard G. Shoup Estate 

2.1 The Pipeline
25
NYIT led the research into making a digital movie and attracted leading 
researchers from PARC and Utah, including Martin Newell and Ed Catmull. 
In 1984, Loren Carpenter at Pixar added the alpha channel to the frame buffer. 
The alpha channel enabled the compositing of objects with anti-aliasing. 
The alpha channel is a component that represents the degree of transparency (or 
opacity) of a color. Alpha blending or alpha compositing is mixing one image with 
a background to create the appearance of partial or total transparency. 
The alpha channel (also called alpha planes) is a color component that repre-
sents the degree of transparency (or opacity) of a how a pixel is rendered when 
blended with another.RGBA stands for red, green, blue, and alpha. Sometimes 
described as a color space, it is a three-channel RGB color style augmented with 
a fourth, the alpha channel. The alpha value indicates how opaque each pixel 
is and allows an image to be combined over others using alpha compositing, 
with transparent areas and anti-aliasing of the edges of opaque regions.Alvy 
ray Smith and Ed Catmull invented the alpha channel while they were at NYIT 
in the late 1970s. 
2.1.3.1 
GPUs and Memory 
There are two conﬁgurations of GPUs in a PC, discrete or stand-alone, and integrated 
(with the CPU). Mobile devices such as phones and tablets use a multi-function 
semiconductor called a system on a chip (SoC) or application processor (AP) with 
an integrated GPU [5]. 
Discrete GPUs are very powerful, and part of their performance comes from 
having a bank of private, high-speed memory, whereas an integrated GPU must share 
the main memory with the CPU. Main memory is slower than dedicated graphics 
memory, thus affecting performance. 
The type of memory used in modern systems is double data rate random access 
memory or DDR RAM. There are a series of DDR memory types reﬂecting the 
development over time. As of this writing, the fastest systems memory for PCs 
and servers was DDR5, and for mobile devices, it was LPDDR (low-power DDR). 
Discrete GPUs use a specialized very high-speed DDR known as GDDR or graphics 
DDR, and the current version is GDDR6. 
DDR is also referred to as DRAM—dynamic RAM. DRAM uses a capacitor, 
a transistor per bit., and was invented by Robert Dennard at IBM in 1968 and 
manufactured by newly formed Intel in 1970 [6]. 
As frame buffers evolved over the years, they were initially made with conven-
tional DRAM; the same as was used for system memory. Various RAM designs were 
introduced, and the GPU suppliers tried everyone one of them: Extended Data Out 
RAM (EDO), Rambus [7], Video RAM (VRAM), High-bandwidth memory (HMB),

26
2
The GPUs’ Functions
and ultimately Graphics DDR (GDDR). The individual memory types are discussed 
in The GPU Environment. 
GPUs need very wide but not so deep memory for fast read-writes and paging. 
2.1.3.2 
Resizable Base Address Register (2010) 
Before late 2020, the base address registers in PC processors could only access a 
256 MB section of the GPU’s memory. As a result, the size of texture maps and other 
data transferred limited performance. 
The PCI Express 3.0 speciﬁcation released in 2010 included resizable base address 
registers (BAR) provisions. However, it became an optional PCI Express interface 
feature because it required speciﬁc support at the CPU and GPU levels. 
A resizable BAR allows the CPU to access the GPU’s entire frame buffer, sending 
more assets at once. That improves performance because GPU does not have to wait 
as long for all the data needed. 
In October 2020, with the Radeon RX 6000 Series AIBs, AMD introduced its 
Smart Access Memory feature (SAM). SAM enabled more memory space to be 
mapped to the base address register providing performance gains in select games 
(some game developers took advantage of it, others did not.) The AIBs had to 
be paired with an AMD Ryzen 5000 Series processor or select Ryzen 3000 series 
processors. 
In February 2021, Intel said it would bring resizable BAR to its 11th-gen Tiger 
Lake CPUs and H35-series mobile chipsets and systems equipped with 10th-gen 
Comet Lake-H CPUs and H45-series chipsets. 
Nvidia announced it supported resizable BAR in GeForce RTX 3000 Series AIBs 
and laptop GPUs in March 2021. For desktops to utilize Nvidia’s resizable BAR, 
systems needed a supported video basic input-out system (VBIOS), a compatible 
CPU, a motherboard, system BIOS (SBIOS) update, and the newest GeForce Game 
Ready driver. 
The Geometry Engines 
Workstations have used dedicated geometry hardware processing since the late 1970s 
and arcade and home console game systems; since 1993, home video game consoles 
adopted it, beginning with the Sega Saturn in 1994. 
A ﬂoating-point processor is for geometry processing, the transformation of prim-
itive’s positions. Geometric processing includes vertex processing, clipping, back-
face removal, and lighting calculations. Floating-point processors, also known as 
ﬂoating-point units (FPU), have been implemented in several ways and in several 
devices as stand-alone chips, coprocessors, and integrated within a CPU. 
Vertex processing involves all the vertices of the object. Those vertices get trans-
formed to the perspective of the objects into viewpoint coordinates—what the viewer 
or user sees. 
In the past, it was a challenging problem, but clever techniques to reduce the 
complexity and speed up the process have evolved. Some companies developed

2.1 The Pipeline
27
dedicated FPUs as a coprocessor to the CPU. Those devices also got used for the 
graphics pipeline. Digital signal processors (DSP) got used, and when the FPU got 
integrated into the CPU, the CPU got used for front-end geometry processing. 
The clipping function determines if an object straddles one or more clipping 
planes, i.e., if part of the object is outside the ﬁeld of view (FOV). The visible 
portions of the object get calculated using the Sutherland-Cohen algorithm [8]’ and 
are processed further. Back-face removal detects the object’s polygons not visible 
from the viewpoint. The 3D objects get modeled as surface polygons. Therefore, the 
inside of an object is not visible. So, when the inside face of a polygon is within the 
ﬁeld of view, it is also discarded. Lighting calculations compute ambient, diffuse, 
and specular contributions to an object’s appearance and the subsequent shadows 
and reﬂections. 
We mark the GPU’s introduction as the ﬁrst production graphics controller with 
an integrated geometry or transform and lighting engine (T&L). 
Vertex shaders enable developers to control the GPU’s operations for each vertex 
directly; in essence, a vertex shader replaces the per-vertex T&L section of the 
pipeline. 
We will use the terms geometry engine, vertex shader or processor, T&L engine, 
and math-coprocessor interchangeably throughout this book. (Although Pixar intro-
duced the term shader in May 1988 (in version 3.0 of Pixar’s RenderMan [9]), the 
term shader as applied to a hardware vertex processor did not occur until 2000 with 
DirectX 8.0 [10]). 
However, as Jim Clark introduced the geometry engine in 1981, that term was 
used for quite some time. (Clark’s Geometry Engine gets discussed in more detail in 
the following sections.) 
Geometry engines are in the front of the pipeline and should not be confused with 
geometry shaders just before the rendering shaders. 
2.1.4 
Object-Level Clipping 
The application selects and positions objects and light sources for each scene. The 
ﬁrst stage in geometry processing is object-level clipping, the process that determines 
whether objects are within the ﬁeld of view (viewing frustum) or not. Each object 
is assigned a bounding box, a list of eight vertices. The frustum is the area of the 
modeled world that appears on the screen (Fig. 2.3).
If the bounding box is outside the user’s FOV, the object is outside the display’s 
ﬁeld of view and can be disregarded; otherwise, the object must be processed further. 
2.1.4.1 
Looking Back at the Geometry Processor’s Origins 
The transformation and perspective corrections from the model’s coordinate system 
to the screen is a critical and complex operation. Transformations were usually done

28
2
The GPUs’ Functions
Fig. 2.3 A viewing frustum. Source Computer desktop encyclopedia 1988 intergraph computer
by the CPU or a dedicated ﬂoating-point coprocessor. Transformation is now the 
work of the geometry processor, also known as a transform and lighting engine or 
shader (Fig. 2.4). 
Fig. 2.4 The silicon 
graphics inc. geometry 
engine-circa 1980s. Source 
Wikipedia: http://www.Shi 
eldforyoureyes

2.1 The Pipeline
29
The geometry engine used in Silicon Graphics workstations consisted of a pipeline 
of identical geometry engine chips. Each VLSI chip consisted of four ﬂoating-point 
ALUs and register ﬁles that operated parallel. Control circuitry and microcode RAM 
within each element permitted the device to be programmed for various functions. 
The functions included transformation, clipping, and perspective division. A pipeline 
of Geometry Engines handled object rotation, translation and scaling, six-plane 
clipping, perspective viewing, and scaling to screen coordinates. Because of that 
complexity (and the number of transistors to accomplish it), the geometry engine, 
however, implemented, was a separate, external coprocessor. 
In 1987, the University of Sussex developed a geometry processor subsystem 
called: Multiple Application Graphics Integrated Circuits—MAGIC. 
The architecture of MAGIC was a pipeline of arithmetic and register ﬁle elements, 
with each element connected via several multiplexers to the next. An element rate 
transformation pipeline performed back-face removal, parallel projection, perspec-
tive, and clipping (and required 16 instances of MAGIC). The pipeline could read 
a single Cartesian coordinate vertex with an associated intensity value every four 
clock periods. With the pipeline running at I00 ns, 2.5 million vertices got processed 
a second, roughly 10,000 polygons per frame at a 25 Hz frame rate. 
Coincidental to Clark’s introduction of the Geometry Engine, the Weitek 
Corporation started with the goal to build and sell a ﬂoating-point coprocessor. 
Weitek offered an alternative to custom chips like Silicon Graphics. The use 
of such a general-purpose processor promised faster inherent speed, which meant 
fewer units would do more work than the slower custom units from SGI. Also, a 
general-purpose processor could accommodate a wider variety of algorithms run 
by the same unit. One such example was to do the rendering of planar and curved 
primitives. However, since the units had to transfer the pixels into an external frame 
buffer for screen refresh, their application to dynamically interactive systems was 
somewhat limited. 
Weitek processors were used in the early to mid-1980s in parallel processing 
supercomputers, high-end computers, and specialized image processing systems. 
The famous PixelFlow machines used them (see the ﬁrst book, Inventing the GPU, 
The First Decade on Other platforms). And they were used extensively in PCs to 
augment the 386, which did not have a built-in ﬂoating-point processor [11]. 
In addition to Weitek, AMD, Intel, and Motorola, there were several other math 
coprocessors suppliers such as Cyrix, ITT, ULSI, and Chips & Technologies (C&T) 
SuperMATH. However, only Weitek got used in PCs, workstations, and supercom-
puters. Motorola ﬂoating-point processors got employed in workstations and Apple 
computers. 
Dedicated geometry engines for graphics like SGI’s Geometry Engine also were 
introduced. 3Dlabs launched their geometry engine the GLINT Delta and Gamma 
in 1997, and Fujitsu introduced their DSP-based ﬂoating-point processor (FFP), 
Pinolite, in 1997.

30
2
The GPUs’ Functions
2.1.4.2 
Digital Signal Processors Used for Geometry 
Because of their independent input, memory, and processing, DSPs with built-
in ﬂoating-point processors were used successfully as geometry engines. The 
devices utilize a VLIW to enable parallel processing and its speed-ups. However, 
a programmable DSP shifts the design effort to software and programming a DSP 
can be difﬁcult. But using a DSP as a coprocessor pays off for the speed of the vertex 
processing it delivers—typically a factor of ﬁve compared to a high-performance 
microprocessor. 
The single chip architecture of a DSP includes a parallel multiplier and accumu-
lator. It is a Harvard architecture (multiple buses with separate storage and signal 
pathways for instructions and data) [12] rather than a von Neumann single bus (like 
an X86 processor where program instructions and data share the same memory and 
pathways) [13]. A DSP offers fast operation at consumer-level component prices. It 
performs operations such as accumulating the sum of multiple products much faster 
than an ordinary microprocessor can. A typical DSP has two data memories (RAM) 
and two data buses. With these, it can deliver the required two operands for a single 
cycle execution of the multiply-accumulate function [14]. A key element of a typical 
DSP is a fast parallel multiplier and accumulator that allows a multiply-accumulate 
operation executed in a single cycle. That made it attractive as a geometry engine. 
Although popular in some specialized systems, DSPs were difﬁcult to program. 
Using them for ﬂoating-point processing did not take advantage of all their function-
ality, so they did not see widespread use as a geometry engine. Later, GPUs would 
incorporate DSP within them for audio and communications functions. 
Some of the ﬁrst systems to feature a DSP for T&L were arcade games from Sega 
in the early 1990s. They employed a Fujitsu MB86232 DSP. Namco (creator of the 
famous Pac-Man) also used a Fujitsu DSP in their arcade systems. Seeking to expand 
their customer-based, Fujitsu adopted their MB86252 DSP and released a smaller 
version of it for the PC named Pinolite, or FGX-1. 
In 1997, Fujitsu introduced their Pinolite (FXG-1) geometry and lighting 
processor, a specialized DSP with dual PCI interfaces [15]. Because of its general-
ized capabilities, the processor also found use in non-graphics applications. Fujitsu 
said at the time a C compiler would be made available for the chip—it was up to 
developers to ﬁgure out how to put it to work. But the Pinolite was designed with 
3D in mind. 
There were several applications for the Pinolite. An example was performing colli-
sion detection. The Pinolite was developed as an optimized 3D geometry processor 
for PC graphics and took the CPU’s heavy burden of geometric transformations. That 
offered a better-balanced overall system performance and improvement of system 
throughput. 
The leading-edge CPUs of the time, such as the Pentium II-266, offered very high 
ﬂoating-point operation performance, as much as 500,000 polygons/s depending on 
the APIs. Even though host CPU performance had signiﬁcantly improved, the CPU 
had to work almost exclusively on the geometric transformations to achieve that level 
of performance. As a result, all other tasks suffered, and even the layer process and

2.2 The Rendering Equation
31
collision detection of highly complicated polygon objects got degraded. That made a 
smooth and dynamic interactive 3D graphics operation difﬁcult. The Pinolite offered 
a way out of that fundamental problem of a shortage of ﬂoating-point execution 
resources and promised to complement the capabilities of CPUs. 
Fujitsu said the Pinolite could perform collision detection while the host CPU 
feed graphics data to a 3D accelerator. The Pinolite could take care of clipping and 
lighting functions on a slower system with less ﬂoating-point capability. The Pinolite 
could accomplish the setup operations like the GLINT Delta when used with a 3D 
accelerator that did not do a triangle setup [16]. 
However, the Pinolite was not successful in the PC market. One of the pioneer 
graphics AIB companies, Hercules, used a Pinolite processor with a Rendition Verité 
V2200 chip and announced a new AIB, the Thriller Conspiracy, in 1998. However, 
when Microsoft’s DirectX (DX) did not support T&L Rendition tried writing some 
custom drivers that would use the Pinolite T&L, but it was buggy. As a result, Hercules 
abandoned the project, although Rendition/Hercules had several prototypes. Micron 
acquired Rendition later that year, and that story is in Book one, Inventing the GPU. 
2.2 The Rendering Equation 
In computer graphics, the rendering equation is an integral equation in which the equi-
librium radiance leaving a point is the sum of emitted plus reﬂected radiance under 
a geometric optics approximation. There are various realistic rendering techniques 
in computer graphics attempt to solve this equation (Fig. 2.5). 
The well known rendering equation describes the total amount of light emitted 
from a point x along a particular viewing direction, given a function for incoming 
light and a bidirectional reﬂectance distribution function (BRDF, see page Error! 
Bookmark not deﬁned.).
Fig. 2.5 Rendering equation light characteristics 

32
2
The GPUs’ Functions
where 
x
is the location in space, 
wo
is the direction of the outgoing light,
Ω
is the unit hemisphere centered around {n} containing all possible values for 
wi, 
wi 
is the weakening factor of outward irradiance due to the incident angle, as the 
light ﬂux spreads across a surface whose area is larger than the projected area 
perpendicular to the ray; written usually as cos θ i. 
The BRDF is a fundamental radiometric concept, and accordingly is used in 
computer graphics for photorealistic rendering of synthetic scenes. 
Solving the rendering equation for any given scene is the primary challenge in 
realistic rendering. One approach to solving the equation is based on ﬁnite element 
methods, leading to the radiosity algorithm. Another approach using Monte Carlo 
methods has led to many different algorithms including path tracing, photon mapping, 
and Metropolis light transport, among others [17]. 
Monte Carlo ray tracing requires a highly detailed and physically based scene 
description as input. The algorithm applies the laws of physics to simulate the 
propagation of light through the scene, rather than ad hoc approximations of visual 
phenomena. This type of simulation requires extremely detailed geometric models 
(engineering models, for example, are typically accurate to a fraction of a millimeter). 
Because ray tracing is less performance-sensitive to geometric complexity, all 
surfaces can be ﬁnely tessellated. In addition to high geometric detail, photo-realistic 
rendering requires that the physical properties of the material’s surface appearance 
are modeled correctly. In contrast to rasterization, where shaders are used to achieve 
certain visual effects, the materials in a photo-realistic ray tracer describe how light is 
scattered when striking a surface. A BRDF represents this information. Their phys-
ical emission properties also describe light sources. A common representation is the 
high dynamic range (HDR) environment light. It models the lighting conditions of 
a real location in a single HDR image. This image is considered as a light source in 
the rendering system. Virtual objects illuminated by this light appear as if they were 
in the actual location. 
2.3 The Geometry Creation 
Generating a beautiful computer graphic (CG) image begins with a designer’s idea, 
an idea for a building, a character, an airplane, a toaster, or a watch —almost anything 
imaginable. The designer then translates the concept into a series of lines displayed 
on the screen of a computer monitor. Those lines represent the outline and several 
details of the thing imagined, and logically enough, it is called a wire-frame model 
[18, 19]. To build a wire-frame model, the designer uses triangles, so we deﬁne 
models in terms of how many triangles they have. We also evaluate a processor by 
how many millions of triangles a second it can process (Fig. 2.6).

2.3 The Geometry Creation
33
Fig. 2.6 Wire-frame models of a cube, icosahedron, and approximate sphere. Source Wikipedia 
One constructs a wire-frame in 3D (x, y, z) space, or in the case of cartoons, 
simple games, or ﬁrst impressions as in a storyboard, just 2D (x, y). In the case of 
architectural and many mechanical drawings, the designer uses four views, a ﬂat 2D 
front, side, and top view (known as an A-B-C view, and then in the fourth quadrant 
an orthogonal or perspective view.) (Fig. 2.7). 
When the designer needs curves in a drawing, as in an ultra-modern building, an 
automobile, an iron, or a fantastic-looking character, the designer uses one of two 
techniques, and sometimes combinations of them. There is the faceted or piece-part 
curve and the algorithmic curve. 
As mentioned in the beginning, CG is about tricks to balance resources and time. 
To draw a simple circle on a computer screen can make high demands on a computer.
Fig. 2.7 Three, 2D views and a perspective view. Source Wikipedia 

34
2
The GPUs’ Functions
Fig. 2.8 An eight-sided 
faceted circle approximation 
(left) and a perfect circle 
(right) 
The circle has a theoretically inﬁnite number of points to describe it. However, 
computers do not have an endless amount of memory, resolution, or time; there-
fore, reasonable compromises must be made. That is where the designer enters a 
negotiation with the viewer. 
One of the qualities of the human brain is that it can make up for missing or incor-
rect image features [20]. Our brains compensate for incorrectness (or incompleteness) 
in an image. For example, computing a curve or a circle requires a calculation per 
pixel. If the user had a high-resolution screen, that could be a time-consuming series 
of calculations. Straight lines, however, are easier for a computer to draw, so we 
can approximate a circle by a series of straight-line segments. The computational 
time versus desired realism determines the number of segments used. These types of 
approximations are what inﬂuence the viewer’s suspension of disbelief (Fig. 2.8). 
As you can imagine, it is even trickier when designing something with a complex 
non-linear curve like an automobile fender, a person’s leg, or a graceful long 
cantilevered streetlamp. 
Therefore, the developers of software and algorithms used in the CG industry 
have spent the last several decades, since about 1950 [21], generating techniques 
or tricks that would give the most believable image possible with the resources 
available. Thanks to miraculous scaling capability in the manufacturing of semicon-
ductors referred to as Moore’s law [22], the CG industry has had faster, smaller, and 
less expensive computers every 18 months. Therefore the developers made better-
looking images every year by combining improved algorithmic work and faster, less 
expensive hardware. 
Games require new images generated at least 30 times a second. They use every 
trick they can to present a decent image while meeting the refresh rate requirements 
of the display. The difference in the quality of the images below is primarily due to 
hardware. 
In 1996, one of the most popular games was Tomb Raider [23], and rendering 
a believable-looking character who ran through jungles and tombs was quite chal-
lenging. Lora Croft’s three-dimensional character consisted of around 230 polygons 
in the ﬁrst Tomb Raider. By 2013, the number of polygons in the model increased 
to over 50,000. The geometry engine had to process all those polygons’ vertices in 
30 ms (ms) or less [24].

2.3 The Geometry Creation
35
The 2013 version was even more esoteric—hair. How could one render hair to 
look natural (responding to light, breezes, and length) and maintain the frame rate? 
Game developer Crystal Dynamics created special algorithms to render each hair 
[25] (Fig. 2.9). 
In contrast to Lara Croft in Tomb Raider, the main character (known as “47”) 
in the Hitman game has no hair. Game developer IO Interactive used sub-surface 
scattering and tricky surface reﬂections to allow light to reﬂect off a semi-oily head 
(Figs. 2.10 and 2.11). 
Fig. 2.9 Compare Lora Croft from 1996 to 2014—a measure of how CG has improved due to 
hardware that enabled more powerful and advanced software. Source Wikipedia 
Fig. 2.10 Hitman codename 47 from the 2016 version of the game (left) compared to Hitman 3 
2021. Source Wikipedia

36
2
The GPUs’ Functions
Fig. 2.11 Comparison of the graphics quality of Call of Duty 2003–2021. Reproduced with 
permission from activision 
Improvements in geometry due to hardware advances have made the characters 
and worlds in modern games so realistic the interactive immersion has been described 
as being in a movie rather than just watching one. It could never have been realized 
if not for affordable, integrated, and efﬁcient geometry processors. 
2.4 The Software Story: The All-Important APIs 
T&L hardware is not any good without a means for the application to take advantage 
of it. For that, you need an API that exposes the hardware’s capabilities. 
The OpenGL API had supported T&L since 1992, and similar functionality 
showed up in DirectX 7.0 (1999), and GPUs added shading to their capabilities. 
Each pixel could be processed using a short program that includes image textures. 
And geometric vertex data could also be processed by a short program before it got 
projected onto the screen. In 2000, an independent consortium called Khronos was 
formed and took over management and development of OpenGL and subsequently 
dozens of other APIs. 
Nvidia was the ﬁrst to produce a graphics controller with programmable shading, 
the NV20-based GeForce 3. By October 2002, with the introduction of the ATI 
Radeon 9700 (also known as R300), the ﬁrst 3D DirectX 9.0 accelerator, pixel, 
and vertex shaders could implement looping and lengthy ﬂoating-point math. GPUs 
became as ﬂexible and powerful as CPUs and orders of magnitude faster for image-
array operations. 
The APIs supplied by Microsoft and Khronos revealed (programmers say 
exposed) the features of the GPU to the application developer. The API enabled 
developers to use the shaders and any accelerators, Codecs, or GPU’s look-up tables.

2.4 The Software Story: The All-Important APIs
37
2.4.1 
The Triangle Setup 
After the transform and the lighting operations get completed, the data is still relative 
to a 3D scene. The 3D scene must get changed to the 2D frame that becomes displayed 
on the screen. The triangle setup procedure is done one triangle at a time. If multiple 
objects are in the scene, some triangles in one object may get obscured by other 
objects’ triangles. However, that is unknown at this stage. The triangle setup ﬁlls the 
triangles with pixels. Each triangle or polygon is a list of vertex coordinates (x, y, z in 
model coordinates) and some speciﬁcation of the surface’s material properties (i.e., 
color, texture, shininess, etc.) and the normal vectors to the surface at each vertex. 
The pixels in the triangle get x- and y-coordinates for the screen. They also get a 
z-coordinate for the depth of information. When completed, the data gets passed to 
the rasterization shader. 
Before introducing the single chip GPU, many graphics controllers had a triangle 
setup but not a T&L processor sometimes called an engine. The CPU or a dedicated 
ﬂoating-point unit did the transformations. That added extra trips for the data from 
the CPU to the graphics controller and its memory. The process was inefﬁcient, so 
chip developers sought to off-load the T&L work from the CPU, which eventually led 
us to the geometry processor. The rest of the pipeline expanded and then contracted 
over time. The following section describes some of the work done in the various 
sections. 
2.4.2 
Drawing and Shading 
The drawing engine (or shader) creates the lines between the vertices—i.e., it 
connects the dots. (In some explanations, it is called a dot drawing engine). A polygon 
is split into trapezoids with horizontal top and bottom edges before being drawn. 
Horizontal and vertical gradients across the trapezoid get calculated. The trapezoid 
gets rasterized, and the rows of pixels get drawn. 
2.4.3 
Triangle Sorting 
Graphics processing architectures have been distinguished by where they do the 
sorting. In his 1992 paper on Pixel Planes, Steve Molnar classiﬁed the sorting as 
either screen (object) subdivision or image composition [26]. In 2001, Mathew 
Eldridge [27] presented an expanded view of parallel graphics architecture sorting 
techniques (in his Ph.D. thesis). Eldridge noted the difference between sorting frag-
ments after rasterization and sorting samples after fragments get merged with the 
frame buffer. Eldridge introduced three new forms of communication between func-
tions: distribution, routing, and texturing, in addition to sorting. Distribution connects

38
2
The GPUs’ Functions
object parallel pipeline stages, routing connects image pipeline stages, and texturing 
connects untextured fragments with texture memory. The issue Molnar and Eldridge 
were trying to solve was the scalability of parallel systems. 
Molnar points out that most high-performance architectures use objectparallelism 
for geometry processing; they distribute primitives over a parallel array of ﬂoating-
point processors, which perform transformation, clipping, and perspective division. 
Molnar used sort-last sparse in the Pixel Planes design. Eldridge used sorting before 
depth test, alpha blending, and fragment processing. Their work was done before the 
introduction of pixel shaders; therefore, no distinction is made for sorting after the 
computing process and before executing the pixel shader. 
GPUs often use Eldridge’s sort-last fragment approach. That allows pixel shader 
results to be sorted into a small number of screen-aligned regions just before depth 
testing and blending. Sort-last fragment allows relatively small ﬁrst-in, ﬁrst-out 
(FIFO) buffers for sorting the data. In immediate mode rendering, however, if a pixel 
is accessed multiple times, it will be read and written to memory numerous times. 
The screen-aligned regions are often associated with individual memory controllers 
for more efﬁcient access. 
2.4.4 
Texture Mapping 
Modern GPUs offer speciﬁc ﬁxed function texture mapping units (also called texture 
samplers). The texture mapping units usually offer trilinear ﬁltering or multi-tap 
anisotropic ﬁltering, employing mipmapping. 
Mipmaps (also MIP maps) are pre-calculated sequences of pre-ﬁltered images, 
each of which is a progressively smaller size and resolution version of the previous. 
They are used to present different levels of detail (LOD) of the texture according 
to distance from the camera. They are often stored in sequences of textures called 
mipmap chains or pyramids, or mipmapping, with each level half as small as the 
previous one (Fig. 2.12).
Level of detail is a general design term for video game landscapes in which 
closer objects are rendered with more polygons than objects that are farther 
away. 
Level of detail refers to the complexity of a 3D model’s representation. 
LOD techniques also improve the efﬁciency of rendering by decreasing the 
workload on graphics pipeline stages, usually vertex transformations. 
Bilinear, trilinear, and anisotropic are the order from least to best image quality 
(IQ), and as might be expected, in increasing order regarding processing demand. 
Texture ﬁltering manipulates how a 2D texture gets displayed on a 3D model. A 
pixel in a texture is a texel (a texture element). The simplest texture ﬁltering method

2.4 The Software Story: The All-Important APIs
39
Fig. 2.12 Mipmaps
is bilinear ﬁltering (also known as nearest neighbor). It gets used when a pixel falls 
between texels; it samples the four nearest texels to determine the average or closest 
color. 
Trilinear ﬁltering also performs a linear interpolation between mipmaps and 
extends the bilinear texture ﬁltering method. 
Bilinear and Trilinear are isotropic (e.g., uniform in all orientations—no perspec-
tive). 
Anisotropic ﬁltering considers the camera orientation; the output polygon may 
not be rectangular. Anisotropic ﬁltering does additional computations to determine 
the effect the camera angle has on the dimensions of the output texture. There is 
more on anisotropic ﬁltering in the following section. 
There is often hardware for speciﬁc format decoding (e.g., S3 texture compression, 
also called DXT—a lossy texture compression algorithm that can reduce texture 
storage requirements and decrease texture bandwidth). Texture mapping hardware 
is in SOCs, iGPUs, and all dGPUs. 
Texture mapping copies pixels from a 2D bit map, such as a photograph, and 
applies the texture or pattern to the 3D surface of an object—it wraps the image 
around the object’s surface. Described as painting a picture on the surface of an 
object, texture mapping is also referred to as diffuse mapping (Fig. 2.13).

40
2
The GPUs’ Functions
Fig. 2.13 Example of texture mapping. Taking an image (left) overlaying on a 3D model (center) 
and creating a realistic image (left) 
Each bit in the texture map is adjusted in size to map correctly to the 3D surface, 
making some bits smaller, some larger, and some distorted in their geometric shape. 
The letters u and v represent the axes of the 2D texture because x, y, and z are used 
already to signify the axes of the 3D object in model space. 
Specialized hardware for texture mapping was developed for ﬂight simulations, 
exempliﬁed by Evans and Sutherland’s image generators developed in the late 1980s 
and for workstations like Silicon Graphics from the same period. Texture mapping 
has also been used extensively in console and arcade game systems, PC AIBs in the 
mid-1990s, and broadcast digital video effects systems. 
Since the mid-1990s, multi-texturing, mipmapping, multi-pass rendering, height 
(bump or normal) mapping, reﬂection mapping, specular mapping, displacement 
mapping, occlusion mapping, cube mapping, and many other variations on the basic 
technique were introduced. More advanced procedures employ a materials library 
and have approached photorealism quality. 
In tile-based deferred rendering and scan-line rendering, the texture mapping 
hardware combines texture mapping with hidden surface removal and only fetches 
visible texels. Those systems rely on a z-buffering approach, which reduces the 
texture mapping workload to front-to-back sorting. 
Bump mapping gives the illusion of depth by adding highlights and shadows to the 
surface. It does not add geometry, so it uses less memory, but it requires the silhouettes 
to remain ﬂat; only the surface is normal and not the underlying geometry is altered. 
Robert Cook overcame that problem when he introduced displacement mapping In 
the late 1980s [28]. 
Cook’s [29] technique was an alternative to bump mapping introduced by Jim 
Blinn in 1978 [30]. Displacement mapping is a texture or a heightmap that modulates 
a surface. Widely adopted for creating maps or mountains, it made designing games 
a lot easier [31]. 
Displacement mapping adds detail to a polygon-based surface model while 
keeping the polygon count low.

2.5 Fill Rate, Rendering Pipelines, and Triangle Size
41
Fig. 2.14 Comparison of Trilinear versus Anisotropic ﬁltering—notice the street tiles are in focus 
to the vanishing point in the right image. Source Cobblestones. JPG Wikipedia Thomas 
2.4.4.1 
3D Texture Filtering 
Before the introduction of the GPU, 3D accelerators used bilinear ﬁltering with 
relatively low-resolution 2D texture maps. When mapped onto large polygons, the 
result was usually blurry with poor image quality. More sophisticated techniques 
like trilinear ﬁltering (bilinear ﬁltering of two different 2D textures) and anisotropic 
ﬁltering (AF) were employed to improve the appearance of textures viewed from a 
very sharp angle (Fig. 2.14). 
Nvidia was the ﬁrst to offer anisotropic ﬁltering (AF) with the Riva TNT in 1998 
[32]. 
A 3D texture contains three-axis information. The third dimension provides 
developers with a depth component of texture information and width and height. 
A 3D texture is a volume of texels, just like a 2D texture is a plane or sheet of 
texels. Even though it occupies volume in 3D space, it is still a texture and is only 
visible where it intersects polygon surfaces. ATI was the ﬁrst to market with 3D 
textures with their Pixel Tapestry engine in the original R600 Radeon announced in 
May 2000. 
2.5 Fill Rate, Rendering Pipelines, and Triangle Size 
One starts with a wire-frame model to get an image like the rendering of an Audi in 
Fig. 3.14 (Fig. 2.15).
The wire-frame model (Fig. 2.16) has 404,433 polygons with 439,107 vertices 
and looking more closely reveals that some triangles are extremely small—so small 
they blend with the other small triangles around them, forming dark areas. If zoomed 
up, the smaller triangles become visible. Many of those small triangles represent 
one or even less than one pixel. That is an essential issue for the ﬁll rate of the 
pixel-rendering unit.

42
2
The GPUs’ Functions
Fig. 2.15 Fully rendered image of a 2021 Audi E-Tron GT. Reproduced with permission from 
TurboSquid
Fig. 2.16 Wire-frame model of an Audi E-Tron GT. Reproduced with permission from TurboSquid 
The pixel-rendering shader or stage can only be fed with one triangle at a time 
from the T&L and triangle setup stages. If the triangle consists of fewer pixels than 
the amount of rendering pipelines in the pixel-rendering unit, some of those pipelines 
will be idle. That is a signiﬁcant disadvantage of several dedicated parallel rendering 
pipelines. 
A circa 2000 GPU such as ATI’s R100 with two pipelines or Nvidia’s GeForce2 
GTS with four rendering pipelines would have some pipelines idle if a triangle 
contained fewer pixels than pipelines. Frames with many small triangles would never 
be able to live up to the high ﬁll rate claims. Benchmarking would expose such 
drawbacks.

2.5 Fill Rate, Rendering Pipelines, and Triangle Size
43
2.5.1 
Rendering Techniques 
The conventional graphics pipeline, as depicted in Fig. 2.1. The graphics pipeline 
was the organization of GPUs until mesh shading—a step-wise process introduced 
in the big bang era of 2020. Mesh shading is discussed in more detail in The GPU 
Environment—APIs. 
Other rendering techniques are discussed in this section. 
To get beautiful images like Fig. 2.15, the GPU ﬁlls in the triangles with colors 
[33] (Fig. 2.17). 
The basic ﬁlling in of a triangle is called ﬂat shading or rendering, and it is a 
valuable technique to quickly get a feeling or impression of how a thing looks. During 
the design phases, designers generally work with ﬂat-shaded models because they 
are fast and not distracting. 
After the basic shading, the designers apply textures to the surfaces to create 
photorealistic images (Fig. 2.18). 
Fig. 2.17 A Red, Blue, Green (RGB)-shaded triangle. Source Tilmann R, Public domain, via 
Wikimedia commons 
Fig. 2.18 Texture mapping is a way to add realism to 3D models

44
2
The GPUs’ Functions
Fig. 2.19 A sphere without bump mapping (left). A bump map (middle) is applied to the sphere on 
the left. The sphere with the bump map is shown on the right. It appears to have a mottled surface 
resembling an orange and a dent to represent where the stem was attached. Source Brion VIBBER, 
McLoaf, Vierge Marie, CC BY-SA 3.0, via Wikimedia Commons 
Computer graphics scientists and engineers strive to manage and simulate how 
light behaves in a scene to realize a beautiful and realistic image. The reﬂections, 
shadows, and special effects like glints, blur, and color contribution from one object 
to another put a big computation load on the GPU and CPU. That in and of itself is 
not the only challenge, generating a high-resolution image quickly, 30–60 times a 
second, is the goal. 
Using a heightmap based on brightness, a technique known as bump mapping can 
create the illusion of depth by manipulating shadows. It can quickly add realism to 
an image by changing the lighting calculations based on the highlights and shadows 
[34] (Fig. 2.19). 
Bump maps achieve this effect by changing how an illuminated surface reacts to 
light without modifying the size or shape of the surface. 
Shading and reﬂection techniques are the heart and soul of computer graphics. 
Whether the goal is a movie, a ﬂight simulator, a fast-paced, realistic game, a car 
shoot, an architectural walkthrough, or a unique idea, the goal is to create a vision 
that faithfully reﬂects the author’s image without compromise. Shading is a broad 
subject, and many textbooks are written about it. There are several techniques, from 
basic ﬂat shading to highly realistic and complex shading. 
Flat shading is a technique to shade (color) each polygon of an object based on 
the angle between the polygon’s surface (the normal) and the light source. One uses 
ﬂat shading for high-speed rendering, where more advanced shading techniques are 
too computationally demanding. In contrast to ﬂat shading is smooth shading, where 
the color changes from pixel to pixel. 
GPUs and associated application program interfaces (APIs) continued to evolve. 
In the fall of 2018, Nvidia introduced its Turing architecture GPU. The Turing GPU 
had task shaders which Nvidia labeled as mesh shaders and introduced the concept 
of meshlets [35], a subset of a mesh created through an intentional partition of 
the geometry. The meshes and meshlets could deal with the complexities of modern 
digital graphics. The new capabilities were accessible through extensions in OpenGL 
and Vulkan, and DirectX 12 Ultimate. Microsoft introduced the new general-purpose 
computational shader as part of DirectX 12 Ultimate in late 2020.

2.5 Fill Rate, Rendering Pipelines, and Triangle Size
45
In late 2020, Microsoft made a signiﬁcant upgrade to its Windows API DirectX12 
(D3D12), and among its updates, it added two new shader stages: the Mesh Shader 
and the Ampliﬁcation Shader (AS). Mesh shaders took advantage of the power of 
generalized GPU compute to the 3D pipeline and brought a new level of ﬂexibility 
and performance [36]. 
2.5.1.1 
Polygon Rendering 
Polygon rendering is a method for creating two-dimensional images (what we see 
on the screen) from a group of three-dimensional objects. These three-dimensional 
objects consist of ﬂat polygons; one can approximate any shape using polygons. 
The set of algorithms, which produce the image, collectively known as the rendering 
pipeline [37], process the image one step at a time. Most 3D computer graphics 
applications use a polygon rendering pipeline to generate images [38]. That is the 
step taken before the ﬁnal rendering (imaging drawing on the screen) takes place. 
2.5.1.2 
Scan-Line Rendering 
Scan-line rendering is an algorithm used to determine which surfaces of the objects 
in the scene will be visible [39]. All the polygons are rendered and sorted by their 
depth location (z). The image data (pixels) is fed to the display beginning with the top 
row or scan-line rather than polygon-by-polygon. This technique of sorting vertices 
along the normal of the scanning plane reduces the number of comparisons needed 
between edges [40]. The method came from the scanning process used in television 
dating back to the late 1930s. 
The scan-line renderer passes a ray from the screen, referred to as the camera or 
viewer, through every pixel of the rendered image. If the ray passes through the face 
of a polygon, it is used. If it does not ﬁnd a polygon, the pixel takes the background 
color. It does this for all pixels, line after line [41]. 
Traditional 3D graphics devices have concentrated on the hardware that trans-
forms points and lines from object space to screen space. As users’ needs for 
displaying realistic solid objects have increased, the demands on graphics archi-
tectures have changed signiﬁcantly. The challenges include increased transformation 
rate, incorporation of real-time illumination calculations, and dramatic pixel ﬁll rates 
increase. 
2.5.1.3 
Immediate Mode Rendering 
In immediate mode rendering, the GPU renders each polygon in a scene without any 
information about the scene. Every forward-facing polygon is rendered, shaded, and 
textured using x, y, z, and a color value. The z-buffer then checks the location of 
the most forward polygon against all other polygons in the scene. If the z-value of

46
2
The GPUs’ Functions
the forward polygon is greater than the z-value of the other polygons, the forward 
polygon gets stored in the frame buffer; otherwise, the shaded and textured polygon 
gets deleted. 
Since the check against the z-buffer occurs after the pixel is already shaded and 
textured, this technique is inefﬁcient and results in wasted processing power and 
memory bandwidth. Pixels get rendered even though they would not be visible on 
the screen. 
2.5.1.4 
Tile Rendering 
As mentioned in Book one, Inventing the GPU, the early work on tile rendering was 
part of the Pixel Planes 5 architecture (1981) [42]. The Pixel Planes 5 project (1988) 
validated the tiled approach and invented many techniques now viewed as standard 
for tiled renderers. 
Tile-based deferred rendering architecture captures the whole scene before begin-
ning rendering. It identiﬁes occluded pixels and rejects them before they are 
processed. The geometry data is divided into small rectangular regions and tiles, 
which get processed as one image. Each tile is rasterized and gets processed inde-
pendently. Since the size of the rendered area is small, all the data can be kept in the 
processor’s memory. 
2.5.1.4.1
Tile-Based Deferred Rendering 
The processor suspends texturing and shading operations in deferred rendering until 
all objects are tested for visibility. The efﬁciency of hidden surface removal (HSR) 
allows overdraw to be removed entirely for completely opaque renders, which signif-
icantly reduces system memory bandwidth usage, and therefore increases perfor-
mance and reduces power requirements. That is an essential beneﬁt for mobile and 
other devices where battery life makes all the difference. 
The elements or stages up to the tiling are considered the tiler, and the elements 
from raster forward are part of the renderer. 
Tiling is also referred to as chunking. The screen is segmented into a plane of 
tiles, each with its rendering and processing engines. This approach, it was argued, 
was more efﬁcient since polygons would not have to wait for preceding polygons to 
be processed. 
In addition, it was claimed that tile-based rendering could do culling more efﬁ-
ciently through the mechanism of tile-based deferred rendering (TBDR). And that 
could be done using less power than a conventional pipeline, so it found a ready 
market in mobile devices. 
However, fast TBDR hardware design is tricky, and it does not use pixel shaders. 
Vertex shaders became so fast that the vertex load of a game was not a limiting 
function anymore. Nonetheless, tilling helps with memory bandwidth usage. TBDR’s 
primary advantage is a lower ﬁll rate requirement,

2.5 Fill Rate, Rendering Pipelines, and Triangle Size
47
Fig. 2.20 A tile-based rendering (TBR) graphics pipeline 
Nvidia and ATI did not embrace TBDR. The parts of the graphics pipeline it 
helped were already very functional, accurate, and fast, so other than memory band-
width, they didn’t see the beneﬁt of TBDR. They chose to focus more on the shader 
processing power of the chips, which a TBDR offers less to help with and takes up 
valuable die space. 
TBDR made more sense for more limited applications such as integrated graphics, 
game consoles, etc. 
While performance is the main goal for computer graphics, there is the require-
ment for balancing performance against power consumption and memory bandwidth. 
Whereas Moore’s law produces faster and higher performance processors, making 
computations relatively inexpensive, the designer still must be judicious in routing 
data lines. The further data must be moved, the more power it consumes [43]. 
CG systems, especially those in a mobile device, use tile-based rendering to reduce 
the bandwidth demand. As the name implies, the image is broken up into a grid of 
tiles [44] (Fig. 2.20). 
Tile-based rendering allows moving the smaller pieces of the image (the tiles) 
into the graphic processor’s internal memory. Since that memory is in the processor 
and close to where the computations occur, far less power is required to access it, 
and it can be fast. 
The early work on tiled rendering was part of the Pixel Planes 5 architecture 
(1988) [45, 46]. The Pixel Planes 5 project validated the tiled approach and invented 
many techniques now considered standard for tiled renderers. However, Microsoft’s 
Talisman project in 1996 made it famous. Refer to Microsoft Talisman—the Chip 
That Never Was, in Book one, Inventing the GPU.

48
2
The GPUs’ Functions
2.5.1.4.2
Immediate Mode Tile Rendering 
Tiling or bucket rendering is a technique in which the frame buffer is subdivided 
into coherent regions rendered independently. Chen et al. [47] pointed out in their 
1998 paper. A beneﬁt of the tiling technique is the decrease in the size of the frame 
buffer memory. There is also the possibility of processing multiple regions in parallel. 
Intermediate mode rendering is discussed in Sect. 2.5.1.3. 
In immediate mode rendering, the GPU renders each polygon in a scene without 
any information about the scene. Every forward-facing polygon is rendered, shaded, 
and textured using x, y, z, and a color value. The z-buffer then checks the location 
of the most forward polygon against all other polygons in the scene. If the z-value 
of the forward polygon is greater than the z-value of the other polygons, the forward 
polygon gets stored in the frame buffer; otherwise, the shaded and textured polygon 
gets deleted. 
Since the check against the z-buffer occurs after the pixel is already shaded and 
textured, this technique is inefﬁcient and results in wasted processing power and 
memory bandwidth. Pixels get rendered even though they would not be visible on 
the screen. 
Tile Rendering 
To review, in immediate mode rendering, the GPU renders each polygon in a scene. 
Every forward-facing polygon is rendered, shaded, and textured using x, y, z, and a 
color value and the z-buffer then checks the location of the most forward polygon 
against all other polygons in the scene. According to the z-value of the forward 
polygon compared to the z-value of the other polygons, the forward polygon gets 
stored in the frame buffer, or the polygons get deleted because they would be invisible. 
Since the check against the z-buffer occurs after the pixel is shaded and textured, 
this technique is inefﬁcient and results in wasted processing power and memory 
bandwidth. 
However, the cost of computing the regions overlapped by a triangle creates 
redundant work processing triangles multiple times and is considered a drawback of 
the technique. Tile size is a critical parameter in bucket rendering systems: smaller 
tile sizes allow smaller memory footprints and better parallel load balancing but 
exacerbate the problem of redundant computation. 
Chen and others argue that the impact of overlap is limited, especially when 
primitives are small compared to region size, which occurs as the triangle count of 
a scene increases. 
Tiling designs such as Imagination Technologies’ PowerVR series, Intel’s 
Graphics Media Accelerator 900 Series, ARM’s Mali, and others like Talisman are 
classiﬁed as sort-middle architectures. Those processors will be discussed further in 
subsequent chapters.

2.5 Fill Rate, Rendering Pipelines, and Triangle Size
49
Fig. 2.21 The rays in ray tracing 
2.5.1.5 
Ray-Traced Rendering 
Ray tracing can simulate a variety of optical effects, such as reﬂection, refraction, 
soft shadows, scattering, depth of ﬁeld, motion blur, caustics, ambient occlusion, 
and dispersion phenomena (such as chromatic aberration) [48]. Ray tracing is not 
the only rendering technique, nor is it the fastest, but it can be the most accurate and 
can be the most photorealistic. It is one method within a continuum of rendering 
a computer-generated image, but it has revolutionized rendering for art, gaming, 
engineering, and architecture (Fig. 2.21). 
One of the main points about ray tracing is that it produces physically accurate 
images and can be photorealistic depending upon the desires of the artist and producer. 
For example, BMW wants an exact image that is photorealistic. However, Pixar wants 
a physically accurate image (for the reﬂections of their cars) and does not want it to 
be photorealistic but stylistic. 
Stylistic is possibly more challenging because if the image is not physically accu-
rate (to the fantasied model), the results creep into the uncanny valley, the illusion is 
broken, and disbelief sets in—the death of storytelling and one’s immersion. 
Physically accurate means the light behaves correctly on the objects in a scene—it 
does not mean the scene is necessarily physically accurate. In animation or special 
effects-driven scenes, the laws of physics, especially gravity, may be completely 
wrong. People and animals do not walk on air for a few steps before failing. Their 
bodies do not stretch as a chasm opens beneath them, and trucks do not ﬂip up into 
the air.

50
2
The GPUs’ Functions
Fig. 2.22 An example of instancing to create a ﬁeld of sunﬂowers. Reproduced with permission 
from http://www.Blender.org 
2.5.2 
Instancing 
When drawing many instances of a model, such as a leaf or trees, the operation will 
rapidly hit a performance limit because of the many draw calls. It would be much 
easier to send data to the GPU once and then tell DirectX, OpenGL, or Vulkan to 
draw multiple objects using this data with a single drawing call—instancing allows 
that to be done. 
With instanced rendering, a GPU can be handed a basic model of a sunﬂower, for 
example, and then render it 1,000 times in different sizes and orientations to create a 
ﬁeld of sunﬂowers. The concept was developed in OpenGL and migrated to Vulkan 
and mesh shaders. Instancing was initiated in the Windows Graphics Foundation 
(WGF) in 2004 (Fig. 2.22). 
Today, instancing is a technique to draw many (equal mesh data) objects at once (in 
Vulkan or DirectX) with a single render call, saving the CPU to GPU communications 
each time an object is rendered. To render using instancing, all that is needed is to 
change the render calls (requests). 
2.5.3 
Aliasing 
The ﬁnal rendering stage is where some of the most interesting and important tricks 
are applied. It is where special coloring, anti-aliasing of edges and lines, and pixel 
polishing1 are done. Anti-aliasing (AA) is a CG trick to smooth out the chunky
1 “Pixel polishing” is a term used by computer graphics people to refer to the improvement in 
appearance of the image. 

2.5 Fill Rate, Rendering Pipelines, and Triangle Size
51
Fig. 2.23 Changing the 
intensity or color of the 
pixels between the line and 
the background gives the 
effect of an even line; the eye 
(brain) is tricked into seeing 
a smooth line. Source 
Blender 
pixelated edges, colloquially known as jaggies, from the edges of images and lines 
(Fig. 2.23). 
Rendering is an all-encompassing term and can refer to the ﬁnal stage where 
the actual pixels are produced or to the whole pipeline after the geometry is created. 
Rendering is the last step in creating the ﬁnal form of the models and animation. With 
the increasing sophistication of computer graphics since the 1970s, it has become a 
more distinct subject. 
The technique for producing the image is done in one of two ways and often 
a combination of both. The two primary rendering methods are scan conversion 
or scan-line rendering (also known as rasterization). All the techniques and tricks 
discussed so far are ﬁnally converted in a time sequence to the scan lines of the 
display and ray tracing or ray casting. 
A special case of scan-line rendering is tile-based rendering, primarily used in 
smartphones and tablets. 
Over the decades, anti-aliasing has been a topic of discussion and development. It 
seemed a new technique was developed every year. The following sections describe 
some of the most popular methods used. 
Multi-sampling Anti-aliasing 
Multi-sampling anti-aliasing (MSAA) is, as the name suggests, a sample-based 
execution or supersampling technique. MSAA is used to reduce geometric aliasing 
and improve the rendering quality of an image. MSAA is conﬁgured (setup) by 
sample count, which can be 1x, 2x, 4x, 8x, or 16x. The sample count governs the 
number of samples of a rendered pixel. However, the MSAA sample count must be 
known before the rendering begins—and it cannot be changed once selected. 
Supersampling anti-aliasing (SSAA) is the gold standard of anti-aliasing methods 
in terms of image quality. It is broadly used in ofﬂine rendering for cinema by content 
creators like Pixar. However, the impact on performance is severe—4X supersam-
pling can take four times as long to render. Graphics AIBs used to offer a supersam-
pling option in their control panel. However, SSAA has fallen out of favor as more 
efﬁcient edge-based AA methods like multi-sampling have grown more popular. 
The downscaling ﬁlter is like the ﬁlters used to scale the video down from higher 
resolutions, e.g., when displaying a 1080p video on a 720p display. The fact that the

52
2
The GPUs’ Functions
ﬁlter uses 13 taps, or samples, indicates how it works: it takes samples from within 
the target pixel area and outside the pixel boundary. 
Supersampling 
Supersampling (SSAA) renders frames (in the frame buffer) at a higher resolu-
tion than the display resolution and then compresses them down to the screen size. 
However, SSAA is computationally expensive because it triggers the pixel shader 
once per sample, at a higher quality and at a higher performance cost than simple 
per-pixel calculation. 
Application developers can choose the rate of shading by selecting per-pixel-
based calculations or MSAA-with-supersampling. However, those choices do not 
offer very ﬁne control. A developer may choose a lower shading rate for a portion 
of the image in situations such as transparencies, blurs (depth-of-ﬁeld, motion, etc.), 
objects behind user interface (UI) head-up display (HUD) elements, and virtual 
reality (VR) optical distortions. Still, with per-pixel or MSAA, it is not possible 
because the shading quality is ﬁxed for the total image. 
The variable rate shading adds a new model for coarse shading that expands 
supersampling-with-MSAA. That extends shading to be performed at a lower 
frequency than one pixel. It groups an array of pixels and teats them as one for 
shading. The results are then broadcast to all samples in the group. 
The coarse shading API allows applications to specify the number of pixels for 
a (shaded) group. What is more, the coarse pixel size can be varied after the render 
target is allocated. Therefore, portions of the screen or draw passes can have different 
subsampling rates, as Microsoft depicted in Table 2.1 [49]. 
The table indicates a 2 × 4 cap and no 4 × 2 for  2  × MSAA based on hardware 
across some platforms.
Table 2.1 Shading rates and coarse pixel size 
Source Microsoft 

2.5 Fill Rate, Rendering Pipelines, and Triangle Size
53
2.5.4 
Scaling 
In March 2021, AMD introduced its FidelityFX Super-Resolution (FSR) upscaling 
technology. Designed to make lower resolution images look shaper through upscaling 
and edge ﬁltering (sharpening). 
Over the years, various techniques were developed for ﬁltering and scaling 
computer graphics images to make them sharper, higher resolution, or smaller to 
reduce memory requirements (Fig. 2.24). 
In addition to the above, there is trilinear, anisotropic, and nearest-neighbor inter-
polation, to name a few. All impose a trade-off of speed (performance) vs. quality 
and memory or processor usage and are chosen on a job-by-job basis—no single 
solution ﬁts all problems. Having been in the computer graphics industry since 
1985, ATI/AMD knows all the tricks and has invented many of them. Applying 
that knowledge to the technology available at the time, the company introduced its 
super-resolution feature (Fig. 2.25).
As the above example suggest, the AMD super-resolution technique is superior 
to point upscaling and bilinear ﬁltering, and native (Fig. 2.26).
As resolution increases, the effects of unﬁltered images (such as jaggies) become 
less annoying and distracting. However, as resolution increases, the computing cost 
of ﬁltering goes up with it. So, the GPU and application suppliers will continually 
tweak and improve their techniques. 
Dynamic Super-Resolution (DSR) is an Nvidia technique developed in 2014 to 
render a game at a higher resolution and intelligently shrinks the result back down 
to the resolution of one’s monitor, thereby providing 4K-quality graphics on a High-
Deﬁnition (HD) or 1440 screen.
Fig. 2.24 Bilinear upscaling ﬁlters out most of the raster Jaggies. Reproduced with permission 
from AMD 

54
2
The GPUs’ Functions
Fig. 2.25 AMD’s FidelityFX super-resolution sharpened and ﬁltered the image. Reproduced with 
permission from AMD
Fig. 2.26 An unﬁltered or 
sharpened image at the 
monitor’s native resolution. 
Reproduced with permission 
from AMD
The process is known as Downsampling or Supersampling. DSR improves the 
downsampling process by applying a high-quality 13-tap Gaussian ﬁlter designed 
for the task. DSR is not quite supersampling, but it is closely related.

2.5 Fill Rate, Rendering Pipelines, and Triangle Size
55
2.5.5 
Environment Mapping 
One of the most popular and practical tricks for creating a realistic image is to reﬂect 
the environment on 3D objects placed in the scene. Imagine you had a digitized 
photograph of the middle of a street, entering a park. Now imagine you created a 
cube, and your view of it reveals ﬁve sides. On the far (back) side, you put (map) 
your scene image. Now create a cylinder, give it a highly reﬂective shiny surface, and 
place it in front of the scene to look like it is part of it. And now map the photo around 
the cylinder. If you get everything right, it will be like the cylinder is reﬂecting the 
street scene shown in Figs. 2.27 and 2.28. 
Your 3D object can be a cylinder, a sphere, a cube, a robot, an airplane, or any 
object you choose. 
Environment mapping or reﬂection mapping is an image-based lighting technique 
for efﬁciently approximating the appearance of a reﬂective surface by using a texture. 
The texture is used to create the rendered object’s environment.
Fig. 2.27 A scene of a park. Source Springer 
Fig. 2.28 Reﬂection mapping is used to place objects into scenes. Source Springer [50] 

56
2
The GPUs’ Functions
2.6 Generating the Image: Hardware Issues 
One must employ a lot of processing to get a realistic image, almost a processor per 
triangle. In 1999, Nvidia introduced the ﬁrst integrated single chip programmable 
parallel processor exclusively designed to manage and output graphics and called it 
the Graphics Processing Unit, or GPU  [51]. Today, that term is part of the vocabulary 
of anyone in the computer industry; it did cause a little confusion because the industry 
had been using the Geometry Processor Unit (GPU) term since 1980 [52]. 
The ﬁrst consumer GPUs had four major 32-bit processors, called pipelines at the 
time, and have since evolved to hundreds and thousands of parallel processors now 
called shaders. Modern GPUs built-in 2014 had 5,632 shaders, and thanks to Moore’s 
law, the number continues to rise. Today discrete semiconductor (“chips”) GPUs have 
more transistors and use more power than the most powerful × 86 CPU. In 2020, 
Nvidia introduced the Ampere GPU, a dGPU that had 28.3 billion transistors, 10,496 
shader cores, plus 328 texture mapping units (TMU), and 328 Tensor (AI) cores, and 
82 ray tracing (RT) cores. It can produce 35.6 TFLOPS. And in October 2020, AMD 
released it is Navi 21 Radeon DNA (RDNA) 2.0 with 26.9 billion transistors, 5,120 
shader cores, 320 TMUs, and 80 RT cores, illustrated in Fig. 2.29. 
While GPU developers were increasing the number of shaders, screen resolutions 
on all devices were going up. In 2014, a 5.7-inch smartphone with 2560 × 1440 
resolution was introduced. It produced 515 PPI (pixels per inch). This compares 
with the iPhone’s retina display resolution of 326 PPI introduced in 2010 and the
Fig. 2.29 Images of GPU dies with multi-thousand shader processors. Reproduced with permission 
from AMD (left) and Nvidia 

2.6 Generating the Image: Hardware Issues
57
Galaxy S5’s 432 PPI introduced in 2014. At the same time, PCs were being equipped 
with 4k displays—3840 × 2180 resolution—8.3 million (mega) pixels. PPI became 
the new metric for evaluating displays, with bigger being better. In 2017, Dell intro-
duced the industry’s ﬁrst mass-market 8K (7680 × 4320) 1.07 billion colors, 32-inch 
280 PPI display aimed at professional designers, engineers, photographers, and soft-
ware developers. The world was becoming hooked on high-quality, high-resolution 
displays. In 2021, Dell introduced a 40-inch 5120 × 2160 5K UHD monitor. 
To drive those super-high-resolution displays, the industry had to switch from the 
old display interface of Digital Visual Interface (DVI) and the older analog (1989) 
VGA to DisplayPort and high-deﬁnition multimedia interface (HDMI). Refer to GPU 
video outputs in, The GPU Environment. 
2.6.1 
VPU—Visual Processing Unit 
Video processing unit, visual processing unit, the VPU acronym has been used 
for several devices. ATI used the visual processing unit (VPU) term in 2002 as a 
marketing substitute for GPU, and after 2006, they dropped it. From 2016 to 2018, 
Jon Peddie Research (JPR) used the term VPU for the video input of AI front ends 
in its VPU Report. 
2.6.2 
Multi-display 
Using two or more displays on a PC or × 86-based workstation used to be very 
difﬁcult. It required an AIB for each monitor and specialized driver. But the market 
demand for such capability was strong from users in the ﬁnancial, video, and CAD 
markets. 
In 1994, Appian Technology developed a proprietary driver for driving multi-
displays under Windows NT using its Renegade 1024 AIB, based on the ACG98032 
graphics controller. When Windows 95 was introduced, the company extended its 
capabilities to support the new operating system (OS). From 1987 to 1999, graphics 
AIBs had one output connector, VGA. Therefore, one AIB was needed per monitor. 
In 1998, Matrox introduced its Productiva G100-Quad graphics AIB which could 
support up to four monitors from a single PCI slot AIB. A special driver written by 
Matrox allowed Matrox AIBs to support dual or quad multi-displays. 
In September 2009, AMD launched its multi-monitor Eyeﬁnity technology with 
the Radeon HD 5000 series. With Eyeﬁnity, AMD could present a single large surface 
to games and applications, allowing them to draw to 3 or 6 monitors as if they were 
a single monitor. It allowed for computing and gaming at a very wide ﬁeld of view 
approaching the limits of human vision. When AMD introduced its new multi-display 
technology, Eyeﬁnity, Rick Bergman, head of AMD’s Products Group at the time,

58
2
The GPUs’ Functions
claimed it would mark “an inﬂection point in the PC industry” and the company’s 
new AIB. 
It did not, but it did get the user’s attention. Issues about monitor’s bezels got in 
the way, which drove monitor manufacturers on a quest to shrink the width of the 
bezels. Also, most gamers did not have the desk space or the budget to support two 
or more big monitors, So the usage was limited to the ultra-high-end gamer. 
2.7 Crypto GPU 
Crypto mining became proﬁtable when Bitcoin was introduced. Because of the tightly 
coupled and high-speed local memory on an AIB, it was discovered that AIBs could 
be used to prove transactions. Doing so, yielded a small commission to the prover. 
In 2017, crypto-miners. A special custom chip known as an application-speciﬁc IC 
(ASIC) was developed for Bitcoin, and the demand for GPUs disappeared. Then a 
new coin, Ethereum, was introduced, and its proof work did very well using a GPU. 
In 2016 and 2917, AIB sales soared as miners bought all the AIBs they could ﬁnd 
and set up huge farms near low-cost power sources. 
Again in 2019 and 2020, Ethereum prices spiked and created a demand for AIBs. 
Because the miners were taking every AIB they could get, games were being pushed 
out as miners bid up the prices of AIB. To counter that, in 2021, Nvidia introduced a 
new class of GPU they called the. Because miners are not in the business of creating 
images, they did not need boards that sent pixels to a monitor; Nvidia could sell them 
“headless” graphics boards that did not have video outputs to the monitor. 
As a result of Nvidia’s CMP, another category had to be added to the GPU 
taxonomy—a Miner’s GPU—mGPU (Fig. 2.30).
2.8 Audio, In, Out, In Again 
Nvidia was the ﬁrst to include audio in their graphics controller, the NV1. Subsequent 
graphics processors from Nvidia did not have any built-in audio capability because 
customers would not pay for it (It was redundant to soundboards or the built-in audio 
on the motherboard). 
In early 2000, ATI acquired ArtX. ArtX was supplying the graphics controller to 
Nintendo for the GameCube console. The ATI/ArtX chip, code named the Flipper 
chip, included 2D/3D graphics and a DSP for audio processing. 
In 2004, the new digital rights management (DRM) High-bandwidth Digital 
Content Protection (HDCP), high-speed high-deﬁnition TV (HDTV) HDMI was 
introduced for TV and Digital Video Disc or Digital Versatile Disc (DVD) players. 
Two years later, Taiwanese AIB builder Sapphire introduced the ﬁrst Radeon 1600 
(based on the ATI RV516, codename Pablo, GPU), which offered HDMI output. 
HDMI can also carry audio.

2.9 Conclusions
59
Fig. 2.30 The many names of a GPU
In late 2005, ATI introduced the PC-based Radeon X1300 AIB. It used the 90 nm 
RV515 GPU, Codename: Dali, and had a Tensilica DSP IP block for audio. 
AMD, which acquired ATI in 2005, linked several of its AIBs together and, at the 
Consumers Electronic Show (CES) in 2013, demonstrated its audio capabilities in a 
domed tent called the Surround House. The company connected thirty-two speakers 
and four subwoofers that synchronized audio with images on a screen [53]. This 
was driven by a workstation running AMD’s FirePro professional graphics card and 
Kaveri accelerated processor unit (APU). 
2.9 Conclusions 
In this chapter, the pipeline concepts have been introduced, notably the transform 
and lighting engine or processor. It is the T&L that distinguishes and differentiates 
the GPU. This chapter and this book are not intended to be a course on computer 
graphics or semiconductor design and manufacturing. However, it is necessary to 
have some awareness of those subjects to appreciate how we got to a GPU and what 
it is capable of. 
The following chapters will take us up on the introduction of the GPU in 
October 1999. The story is split between graphics controllers developed for the 
PC and graphics controllers designed for other platforms like game consoles, mobile 
devices, workstations, and location-based entertainment systems (LBEs in arcades 
and casinos).

60
2
The GPUs’ Functions
The following chapters trace the development of the GPU in chronological order, 
although that is not always entirely possible, it is a general organization. 
References 
1. Graphics pipeline. (n.d.). Computer Desktop Encyclopedia, http://www.answers.com/topic/gra 
phics-pipeline 
2. Gaboury, J. The random-access image: Memory and the history of the computer screen, Grey  
Room, 70(70), 24–53. (2018), https://doi.org/10.1162/GREY_a_00233 Retrieved from https:// 
www.escholarship.org/uc/item/0b3873pn 
3. Gaboury, J. Image Objects: An Archaeology of Computer Graphics, MIT Press, (2021), https:// 
www.google.com/books/edition/Image_Objects/juQ2EAAAQBAJ?hl=en&gbpv=1 
4. Hiltzik, M. Dealers of Lightning: Xerox PARC and the Dawn of the Computer Age, Harper  
Business, New York, (1999). https://www.amazon.com/Dealers-Lightning-Xerox-PARC-Com 
puter/dp/0887309895 
5. The Great Debate: SOC vs. SIP, EE Times, (March 21, 2005), https://www.eetimes.com/the-
great-debate-soc-vs-sip/# 
6. DRAM – History of Dynamic Random Access Memory, https://www.history-computer.com/ 
inventions/dram-history-of-dynamic-random-access-memory/ 
7. RDRAM, Wikipedia, https://www.en.wikipedia.org/wiki/RDRAM 
8. Principles of Interactive Computer Graphics, p. 124, 252, by Bob Sproull and William M. 
Newman, 1973. McGraw–Hill Education, 
9. The Renderman interface, version 3.1, page 10, (September 1988), http://www.groups.csail. 
mit.edu/graphics/classes/6.838/S97/rispec31_4.pdf 
10. Microsoft Announces Release of DirectX 8.0, (November 9, 2000), https://www.news.micros 
oft.com/2000/11/09/microsoft-announces-release-of-directx-8-0/ 
11. Norbert, J. Everything You Always Wanted To Know About Math Coprocessors. (January 1993), 
https://www.dougx.net/gaming/coproc.html 
12. Harvard architecture, https://www.en.wikipedia.org/wiki/Harvard_architecture 
13. von Neumann architecture, https://www.en.wikipedia.org/wiki/Von_Neumann_architecture 
14. Bland, D., and Bonne T., The Use of Digital Signal Processors in Computer Graphics, North-
Holland Computer Graphics Forum 7, p91–96, (June 1988), https://doi.org/10.1111/j.1467-
8659.1988.tb00594.x 
15. Fujitsu Develops World’s First Three Dimensional Geometry Processor, (July 2, 1997), https:// 
www.pr.fujitsu.com/jp/news/1997/Jul/2e.html 
16. Romanick, Ian D., GLINT Delta v. Pinolite, (June 3, 1998), http://www.web.cecs.pdx.edu/~idr/ 
graphics/pinolite.html 
17. Kajiya, James T. (1986), “The rendering equation” (PDF), SIGGRAPH 1986: 143, doi:https:// 
doi.org/10.1145/15922.15902, ISBN 0-89791-196-2 
18. Nasifoglu, Yelda. Renaissance wire-frame. Architectural Intentions from Vitruvius to the 
Renaissance Studio Project for ARCH 531. McGill University. 
19. Principles of Engineering Graphics by Maxwell Macmillan International Editions 
20. Harmon, Katherine, The Brain Adapts in a Blink to Compensate for Missing Information, 
Scientiﬁc American, http://www.scientiﬁcamerican.com/article/brain-adapts-in-a-blink/ 
21. Peddie, J. The History of Visual Magic in Computers: How Beautiful Images are Made in CAD, 
3D, VR and AR, Springer-Nature, Berlin, Germany (2013) 
22. Moore, G. E. Cramming more components onto integrated circuits, (PDF). Electronics 
Magazine. p. 4. (1965). 
23. Cohen, M. (2000) Lara Croft: the art of virtual seduction, Prima Pub. Roseville, Ca., https:// 
www.archive.org/details/laracroftartofvi00mark/mode/2up

References
61
24. Kurland, D. 20 Weird Facts About Lara Croft’s Anatomy, (July 8, 2019), https://www.thethi 
ngs.com/facts-about-lara-crofts-anatomy/ 
25. Marco. (June 2013) Evolving an Icon: Lara Croft 2.0, https://www.tombraider.tumblr.com/ 
post/123389886680/e3-ambassador-blog-evolving-an-icon-lara-croft 
26. Molnar, S., Eyles, J., Poulton, J. Pixelﬂow: High Speed Rendering Using Image Composition. 
Computer Graphics, (Proceedings of SIGGRAPH 92), volume 26, number 2, pages 231–240. 
(July 01, 1992), https://doi.org/10.1145/142920.134067 
27. Eldridge, M. 2001. Designing Graphics Architectures Around Scalability and Communication. 
PhD thesis, Stanford. (June 2001), http://www-graphics.stanford.edu/papers/eldridge_thesis/ 
eldridge_phd.pdf 
28. Karhu, K. Displacement Mapping, Helsinki University Of Technology Telecommunications 
Software and Multimedia Laboratory (Spring 2002), https://www.researchgate.net/publication/ 
228860117_Displacement_Mapping 
29. Cook, R.L. Shade Trees, Computer Graphics (Proceedings of SIGGRAPH 84), 18(3):223–231, 
July 1984. Held in Minneapolis, Minnesota. 
30. Blinn, J. F. Simulation of Wrinkled Surfaces, Computer Graphics, Vol. 12 (3), pp. 286–292 
SIGGRAPH-ACM (August 1978) 
31. Donnelly, W. Per-Pixel Displacement Mapping with Distance Functions, University of  
Waterloo, GPU Gems 2, Chapter 8, (2005) Addison-Wesley, https://www.developer.nvi 
dia.com/gpugems/gpugems2/part-i-geometric-complexity/chapter-8-pixel-displacement-map 
ping-distance-functions 
32. Riva TNT product review, Nvidia, (1988), http://www.vgamuseum.info/images/doc/nvidia/riv 
atnt_specs.pdf 
33. Hinzman, L. Fundamentals of the Vulkan Graphics API: Why Rendering a Triangle is 
Complicated, (21 July 2020), https://www.liamhinzman.com/blog/vulkan-fundamentals 
34. Søren D. Bump Mapping Using CG (3rd Edition). Retrieved 2007-05-30. 
35. Kubisch C. Introduction to Turing Mesh Shaders, (September 17, 2018), https://developer.nvi 
dia.com/blog/introduction-turing-mesh-shaders/ 
36. Reinventing the geometry pipeline: Mesh Shaders in DirectX 12, https://www.youtube.com/ 
watch?v=vV7ebQ3IfcM 
37. Funkhouser, T. 3D Polygon Rendering Pipeline, (2000), https://www.cs.princeton.edu/courses/ 
archive/fall00/cs426/lectures/pipeline/pipeline.pdf, 
38. Akeley, K., Jermoluk, T. High-Performance Polygon Rendering, Computer Graphics, Volume 
22, Number 4, (August 1988) 
39. Wylie, C., Romney, G. W., Evans, D. C., and Erdahl, A. A, Halftone Perspective Drawings by 
Computer, Proc. AFIPS FJCC 1967, Vol. 31, 49 
40. Newell, M. E., Newell R. G., and Sancha, T. L. A New Approach to the Shaded Picture Problem, 
Proc ACM National Conf. 1972 
41. Difference between scan-line rendering and raytracing rendering, (July 23, 2020), https:// 
www.cgvizstudio.com/2020/07/23/difference-between-scan-line-rendering-and-raytracing-
rendering/?privacy=updated 
42. Fuchs, H. and Poulton, J., Pixel-Planes: A VLS1-Oriented Design for a Raster Graphics Engine, 
VLSI Design, No. 3, 20. (1981) 
43. Torborg, J., and Kajiy, J., Talisman: Commodity real-time 3D graphics for the PC, SIGGRAPH 
1996, 353–363. (1996) 
44. Molnar, S. A Sorting Classiﬁcation of Parallel Rendering, IEEE. Retrieved 2012-08-24. (1994-
04-01) 
45. Mahaney, J. History. Pixel-Planes. University of North Carolina at Chapel Hill. (1998-06-22). 
46. Fuchs, H. Pixel-planes 5: a heterogeneous multiprocessor graphics system using processor-
enhanced memories, Pixel-Planes. ACM, (July 01, 1989) 
47. Chen, M., Stoll, G., Igehy, H., Proudfoot, K., and Hanrahan P. Simple models of the impact 
of overlap in bucket rendering. In Proceedings of the ACM SIGGRAPH / EUROGRAPHICS 
Workshop on Graphics Hardware (Lisbon, Portugal, August 31 - September 01, 1998). S. N. 
Spencer, Ed. HWWS ‘98. ACM, New York, NY, 105–112, https://doi.org/10.1145/285305. 
285318

62
2
The GPUs’ Functions
48. Peddie, J. Ray Tracing: A Tool for All (2019) Springer Link, https://doi.org/10.1007/978-3-
030-17490-3 
49. Variable Rate Shading, https://microsoft.github.io/DirectX-Specs/d3d/VariableRateShading. 
html 
50. Chheang, V., et all. Natural embedding of live actors and entities into 360° virtual reality 
scenes, The Journal of Supercomputing, Springer Nature (2018), https://doi.org/10.1007/s11 
227-018-2615-z 
51. Peddie, J. Nvidia’s GeForce graphics processor, The Peddie Report, Volume XII, Number 
36–1421, (September 6, 1999) 
52. Clark, J. Special Feature A VLSI Geometry Processor For Graphics. Computer. pp. 59–68. 
(July 1980). 
53. Boyd, B. The AMD Tech that Brought “Surround House 2: Monsters in the Orches-
tra” to Life, https://community.amd.com/t5/amd-corporate-blog/the-amd-tech-that-brought-
surround-house-2-monsters-in-the/ba-p/414157

Chapter 3 
The Major GPU Eras 
From the 1970s to 1999, graphics hardware was typically designed for speciﬁc needs, 
CAD, simulation, special effects, and gaming. Graphics workloads always have and 
always will demand increasing processing power. During that period, performance 
increased at almost two and half times a year when compared to what transistor 
doubling could have provided (~1.8x/year). Figure 3.1 shows the graphics perfor-
mance trajectory as chronicled by Prof. John Poulton from the University of North 
Carolina (UNC) [1].
Using the metric of triangles per second, graphics hardware performance was 
increasing faster than Moore’s law predicted. This was due to the massive parallelism 
available in the computations of computer graphics, which hardware was able to 
leverage. By 2001, PC graphics were already signaling the end of huge systems 
designed for graphics. 
The ears of the GPU are marked by the introduction of the widest used API, 
DirectX. DirectX becomes the ear designation because it was the API that exposed 
the hardware features of the eras to the software and application developers. Prior to 
the stabilization of the APIs by Microsoft, the hardware suppliers introduced their 
own APIs so developers could access the features of the graphics devices. That led 
to the API wars, confusion, and instability which threaten the growth of the industry. 
To understand and appreciate the eras, we have to look at the evolution of DirectX. 
The ﬁrst era of the GPU began with the release of Microsoft’s DirectX 7.0, as 
shown in Fig. 3.2.
DirectX 7.0 was released in September 1999, introducing support for transform 
and lighting hardware acceleration and the allocation of vertex buffers in hard-
ware memory. Hardware implementation of vertex buffers represented a signiﬁ-
cant improvement in Microsoft’s Direct3D (later renamed DirectX) over OpenGL. 
Direct3D 7.0 also included multitexture hardware resources. And at the time, 
it represented the high point of ﬁxed function multitexture pipeline capabilities 
(Fig. 3.3).
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2022 
J. Peddie, The History of the GPU – Eras and Environment, 
https://doi.org/10.1007/978-3-031-13581-1_3 
63

64
3
The Major GPU Eras
Fig. 3.1 Performance of graphics leading up to GPUs. Graphics hardware had been progressing at 
a rate faster than Moore’s law. Data Courtesy of John Poulton, UNC Chapel Hill
Fig. 3.2 The six eras of GPU evolution and development
Fig. 3.3 DirectX basic pipeline 
The most exciting features of DirectX 7 included hardware acceleration of trans-
formation and lighting, cube environment mapping, vertex blending, and particle 
systems.

3
The Major GPU Eras
65
DirectX 8.0, introduced in November 2000, launched programmability in vertex 
and pixel shaders. That allowed developers to write programs without having to 
keep track of the hardware state. 
DirectX 8.1 was for Windows 2000, XP, and derivatives and came out on October 25, 2001. 
Following it was DirectX 8.1a in 2002, which included an update to Direct3D (D3d8.dll), 
and then in June 2002, DirectX 8.1b came out as a ﬁx to DirectShow on Windows 2000. 
There was also a DirectX 8.2 for DirectPlay. 
Direct3D 9, released in December 2002, added a new version of the high-level 
shading language (HLSL) support for ﬂoating-point texture formats, multiple render 
targets (MRT), multiple-element textures, texture lookups in the vertex shader, and 
stencil buffer techniques. 
DirectX 9.0c launched in August 2004 and incorporated Shader Model 3.0, which 
extended vertex and pixel shader proﬁles by increasing the number of instructions. 
That, in turn, allowed for more complex shaders. 
Direct3D 10, introduced in November 2006, broke the model of one vertex in/one 
vertex out and allowed geometry generated from within a shader entirely within the 
GPU’s hardware. DirectX 10 included Shader Model 4.0, which extended the vertex 
and pixel shaders’ functionality and introduced the geometry shader’s new capability. 
DirectX 11 came out in October 2009 with Shader Model 5.0, which expanded 
the vertex, pixel, and geometry shaders. It also introduced tessellation and compute 
shader proﬁles. Tessellation shaders provided the ability at runtime to increase the 
detail of a wireframe using the GPU instead of the CPU. 
Tessellation converts low-detail subdivision surfaces into higher detail primi-
tives on the GPU. Tessellation tiles (or breaks up) high-order surfaces into suitable 
structures for rendering. 
The graphics pipeline could evaluate lower detail (lower polygon count) models 
and render them in greater detail by implementing tessellation in hardware. Tessella-
tion implemented by hardware could generate an enormous amount of visual detail 
(including support for displacement mapping) without adding graphic detail to the 
model sizes and dragging down refresh rates. 
DirectX 12 was introduced in July 2015 and provided a lower level of hardware 
abstraction than earlier versions, enabling future games to improve multithreaded 
scaling and decrease CPU utilization signiﬁcantly. DirectX 12, like Vulkan, borrowed 
architectural structures from AMD’s Mantle API developed for consoles. 
DirectX 12 brought in Shader Model 5.1. It added volume-tiled resources, 
shader-speciﬁed stencils, better collision and culling with conservative rasteriza-
tion, rasterizer-ordered views (ROVs), standard swizzles, default texture mapping, 
compressed resources, additional blend modes, and efﬁcient order-independent 
transparency (OIT). 
DirectX 12 Ultimate introduced in November 2020, brought a ﬂood of new compute 
techniques and aligned the PC with game consoles which greatly enhanced and sped 
up game development. Dx12U introduced Mesh Shaders and Sampler Feedback.

66
3
The Major GPU Eras
Mesh shading would transform the detail and quality of computer images without 
penalizing performance. 
A summary of the generations of DirectX is shown in Table 3.1.
DirectX 12 extended the API to incorporate ray tracing and Direct12 Ultimate 
introduced mesh shaders and established the GPU platform as a computing device 
for graphics. 
3.1 The First Era—Transform and Lighting—DirectX 7 
(1999) 
The introduction of the single chip, integrated GPU with ﬁxed function transform 
and lighting capabilities was the ﬁrst era of GPUs, and the shortest one. 
The data used to create a 3D image with perspective on a screen is processed 
through different frames of reference known as spaces, such as world space, eye 
space, and screen space. World space holds all the 3D objects that are part of the 3D 
world or model. Eye space manages lighting and culling—what is seen and how it 
looks, and screen space stores the scene in the graphics frame buffer for ﬁnal display. 
These techniques and names are used in all graphics systems not just Windows-based 
systems. Windows uses them through DirectX, and other systems use OpenGL, or 
Vulkan, or Metal as in the case of Apple systems. 
However, these spaces use different coordinate systems and views. Therefore, 3D 
data must be converted or transformed from one space to another as it moves through 
the 3D pipeline to the viewer. The transform engine performs those mathematical 
transformations. 
3.1.1 
Shading and Shaders 
The transformation process is a critical step in 3D graphics and a signiﬁcant challenge. 
The transformation of the coordinates of the vertices of the 3D model became known 
as a vertex shader. Why a shader? The vertex transformation required a ﬂoating-point 
processor, or FPU—ﬂoating-point unit. Coloring or shading a triangle also made use 
of an FPP and it, too, became known as a shader, as in shading the colors of an image. 
Shading a triangle meant its color could change across the triangle from each 
vertex (shown in Fig. 3.4)—not a trivial operation. As a result, any processor in the 
graphics pipeline became known as a shader, and the name stuck. The processor or 
shader stages are illustrated in Fig. 3.5.
Each logical stage in the pipeline maps to GPU hardware, a GPU processor. 
Programmable shader stages are blue, ﬁxed function are white, and memory objects 
are dark blue in the gray box. Each stage processes a vertex, geometric primitive, or 
pixel in a streaming data-ﬂow fashion.

3.1 The First Era—Transform and Lighting—DirectX 7 (1999)
67
Table 3.1 The evolution of the APIs and OS relative to the eras of GPUs 
DirectX
Release date
Features relative to 
the eras 
Shader model
Shader 
proﬁle 
First Era T&L 
DirectX 7 
Win 98 
September 1999
Transform and 
lighting hardware 
acceleration 
Second era programmable 
DirectX 8.0 
Win 2000 
November 12, 2000
Programmable 
vertex and pixel 
shaders 
1.0–1.1
vs_1_1 
DirectX 8.1 
Win XP 
October–November 
2001 
1.3 and 1.4 
DirectX 9.0 
Win XP, Xbox 
November 19, 2002
2.0
vs_2_0, 
vs_2_x, 
ps_2_0, 
ps_2_x 
DirectX 9.0c 
Win XP 
August 4, 2004
3.0
vs_3_0, 
ps_3_0 
Third era uniﬁed 
DirectX 10.0 
in XP 
November 30, 2006
Uniﬁed pipeline, 
geometry shader 
4.0
vs_4_0, 
ps_4_0, 
gs_4_0 
DirectX 10.1 
Win Vista 
February 4, 2008
4.1
vs_4_1, 
ps_4_1, 
gs_4_1 
Fourth era compute 
DirectX 11.0 
Win 7, 8, 8.1 
October 22, 2009
Tessellation
5.0
vs_5_0, 
ps_5_0, 
gs_5_0, 
ds_5_0, 
hs_5_0, 
cs_5_0 
Fifth era ray tracing and AI 
DirectX12 
Win 10 
July 29, 2015
Mesh and 
ampliﬁcation 
shaders 
5.1 to 6.3  
DirectX12 
Win 10 
October 10, 2018
DirectX ray tracing 
(DXR) 
Sixth era mesh 
DirectX 12 
Ultimate Win 10 
March 19, 2020
Ray tracing, AI, 
VRS 
6.5 
vs = vertex shader, gs = geometry shader, hs = hull shader, ps = pixel shader

68
3
The Major GPU Eras
Fig. 3.4 A shaded triangle. Source TilmannR, Public domain, via Wikimedia Commons 
Fig. 3.5 Shader stages relative to memory (dark blue boxes), the white boxes are not shaders
3.1.1.1 
Shading 
Pixar deﬁnes shading as the process of computing the color of a point on a surface. 
A GPU consists of dozens to thousands of 32-bit ﬂoating-point processors, which 
have become known as shaders. The functions of the processing or operation or 
shading vary. In early GPUs, each shader was a stage in a pipeline. In newer GPUs, 
what used to be stages are now compute functions. We will look at the function ﬁrst 
(shading) and then at the processors (shaders). 
The term “shader” came from Pixar when they introduced version 3.0 of their 
RenderMan Interface Speciﬁcation in May 1988 [2]. 
That came about because Robert Cook, a Software Engineer at Pixar, wanted to 
make computer-generated images look more realistic. To do that, he came up with 
the idea of writing a shading program that could blend qualities such as light, shadow, 
color, etc. He wanted to create a program to analyze a 3D object and the surrounding 
scene and ascertain where shadows were and where light reﬂected onto or off the 
model. Then the shader processor could modify image to replicate the real world with 
computer-generated images. Today that would be thought of as a rendering shader, 
which led to the Reyes renderer program that Pixar introduced in 1984.

3.1 The First Era—Transform and Lighting—DirectX 7 (1999)
69
Ed Catmull, the Co-Founder of Pixar, with Loren Carpenter, and Robert Cook 
were the masterminds behind the Reyes renderer. Reyes is an acronym for Render 
Everything You Ever Saw. Ironically, at Industrial Light and Magic (ILM), one of 
the programmers working with the Reyes renderer was Bill Reyes. The joke was that 
he should sue Pixar for copyright infringement of his name. 
In 1985, after leaving Apple, Steve Jobs bought Pixar (at the time called Graphics 
Group) from Lucasﬁlm for $5 million. In 2006, Disney bought Pixar for $7 billion. 
During those years, the shader development of Reyes expanded and grew into the 
RenderMan program, which was the primary rendering tool used for Pixar movies. 
Then at SIGGRAPH 1987 in Anaheim, California, the Reyes image rendering archi-
tecture paper was ﬁrst delivered by Robert Cook, Loren Carpenter, and Edwin 
Catmull [3]. A year later, RenderMan, as a program, was ofﬁcially born. 
Pixar presented RenderMan as a rendering standard at the 1993 SIGGRAPH 
conference. It was developed with comments and suggestions from 19 companies 
plus 6 partners. The design for the RenderMan Interface came from Pat Hanrahan 
of Stanford University, acting as an advocate and project manager. At the time, Ed 
Catmull, CEO of Pixar, said no software product met the rendering standard required 
by Pixar [4]. Two years later, RenderMan did. 
The GPU as we know it today did not exist yet. Shader programs ran on general-
purpose CPUs. And the types of shaders (programs) continuously expanded in 
the search for more realistic images in movies, games, TV, and simulations. The 
following sections are a list of the more popular shaders, somewhat in order of 
appearance. After the introduction of the GPU, shaders became speciﬁc processors 
within the GPU. And later, in 2020, they became general-purpose processors in favor 
of a general-purpose computing environment, the full circle—from a general-purpose 
CPU to a general-purpose GPU. 
The shading process requires information about the light sources in the scene 
(location, lens, color), the properties of surfaces and their materials, and the atmo-
spheric effects (sometimes referred to as caustics). Shading does not include the 
interpolation of color across a primitive as used by a Gouraud or a Phong interpo-
lation. All the shading processes get determined by a function that mathematically 
describes a given part of the process. Throughout this book, the term shader refers to 
procedures that perform one of the three primary functions of a shader as described 
by Pixar’s RenderMan documentation: Light-source shaders, Surface shaders, and 
Volume shaders are discussed in the following sections [5]: 
Light-Source Shaders 
A light-source shader calculates the color of the light produced from the light source to the 
surface of an object. A light will typically have a color, an intensity, a directional dependency, 
and a falloff with distance [6]. Lights may exist singularly, or they may be attached to 
geometric primitives as reﬂections. 
Surface shaders 
Surface shaders are connected to geometric primitives and are used to model the optical 
characteristics of the surface materials of the primitives. The RenderMan documentation 
describes a surface shader as one that computes the light reﬂected in a particular direction

70
3
The Major GPU Eras
by summing the surface’s incoming light and considering the properties (materials) of the 
surface. 
Volume shaders 
Volumes are deﬁned (by RenderMan and most applications) as the insides of solid objects. 
Volume shaders adjust the color of a ray of light as it travels through a volume. 
2D Shaders 
A 2D shader modiﬁes a pre-rendered image known as a texture. It can be a photograph, 
a design, or random dots. They are used in conjunction with pixel shaders as part of a 
preprocessing step—preparing the texture for the pixel shader. 
3D Shaders 
The term 3D shader is a generic description for any process (program) that acts on the 3D 
model or geometry. That would include translation, color modiﬁcation, textures, and lighting 
of the model. A 3D shader consists of a vertex shader and possibly a geometry shader [7]. 
As stated several times in this book, when computer graphics started in the 1960s 
to late 1970s, the CPU handled the variety of transform tasks. The work placed a 
burden on the CPU, and engineers looked for alternative co-processors to ofﬂoad 
the work. They tried several types of processors such as dedicated array processors, 
digital signal processors (DSPs), and other devices. They were successful but they 
were not cost-effective or easy to program. 
3.1.2 
Geometry Processing 
In 1981, Jim Clark developed the geometry processor to do transformations, and it 
kicked off a new era in computer graphics and processors. The geometry processor, 
however, remained a stand-alone device for many years. 
In the early 1990s, as very large-scale integration (VLSI) started taking hold, 
engineers began incorporating small transformation compute elements into graphics 
processors known at the time as transform and lighting processors, or just T&L 
engines. 
In 1997, Malcom Wilson, David Baldwin, and Nick Murphy at 3DLABS (in the 
UK) developed the Glint Gamma processor, the ﬁrst programmable T&L engine, 
as part of its Glint workstation graphics chipset and introduced the term GPU— 
Geometry Processor Unit [8]. 3DLABS’s GPU was a separate chip named Delta and 
was known as the DMX. 3DLABS’s GMX was a coprocessor to the Glint rasterizer 
(Fig. 3.6).
Then in 1999, Nvidia developed an integrated T&L engine for their consumer 
graphics chip, the GeForce 256. ATI quickly followed with their Radeon graphics 
chip. But Nvidia popularized the term GPU and Nvidia has forever since been 
associated with it and credited with inventing the GPU (Fig. 3.7).

3.1 The First Era—Transform and Lighting—DirectX 7 (1999)
71
Fig. 3.6 The ﬁrst single chip GPU with an integrated T&L engine, Nvidia’s NV10 used in the 
GeForce 256, circ 1999. Reproduced with permission from Konstantin Lanzet, Wikipedia
Fig. 3.7 Raja Koduri. 
Source Intel
GPUs are one of the most complex chips to build—they are a collection of multiple 
subsystems that interact with each other, some synchronous, others asynchronous. And the 
software stack to support multiple APIs and languages with compliance and performance 
makes it even harder. 

72
3
The Major GPU Eras
—Raja Koduri, Senior Vice President, Chief Architect, and General Manager of Architecture, 
Graphics, and Software at Intel Corporation [9]. 
As the resolution of models increased, transforming the coordinates of triangles 
became burdensome even for dedicated and plentiful vertex shaders. 
Vertex Shader 
The vertex shader was introduced with T&L capability, ﬁrst as an external device and 
later integrated into the GPU. The geometry engine Clark invented was an external, 
vertex processor or shader. The terms geometry processor, vertex processor, vertex 
shader, and T&L all get used interchangeably. They all are transformation engines. 
A vertex shader translates the position of triangle vertices from the 3D model 
coordinates to the 2D screen’s coordinates, altering their orientation, position, and 
color. The vertex shader accepts a ﬂoating-point vertex position (x, y, z) and computes 
(a ﬂoating-point) x, y screen position. Vertex shaders enable robust control over the 
details of position, movement, lighting, and color in any scene involving 3D models. 
Vertex shaders were the primary and ﬁrst 3D shaders used in computer graphics 
(see Shader stages in Fig. 3.5). They ran each time a vertex came to the GPU from 
the application. The vertex shader also calculates the depth value for the z-buffer 
used for culling [10]. 
Vertex shaders control the properties of a triangle; however, they cannot create 
new vertices [11]. The vertex shader’s output goes to the next stage in the pipeline, 
either through geometry or a rasterizer. 
3.2 The Second Era—Programmable Shaders—DirectX 8 
and 9 (2001–2006) 
The OpenGL and Direct3D APIs have deﬁned the GPU pipeline (aka the rendering 
pipeline). The GPU’s pipeline has evolved signiﬁcantly as more features and capa-
bilities have been added, enabled by Moore’s law. Dramatic changes have occurred 
with the evolution from a ﬁxed function pipeline (FFP) to a programmable pipeline. 
Although each step has incorporated some compromise between performance or cost. 
Moore’s law mitigated it through the principle of twice the performance at the same 
price. Moving from a tightly coupled ﬁxed function processor to a more complex 
programmable one requires transistors, compilers, more memory, and, because of all 
that overhead, higher speed if it is to deliver better performance than the previous 
generation. 
The ﬁrst Microsoft API Shader Model created in D3D/DirectX was Shader Model 
1 (Fig. 3.8). It established vertex and pixel shaders as the ﬁrst implementation of the 
programmable pipeline. Shader model 1 started with DirectX 8, and is abbreviated 
as sm1.
Nvidia was the ﬁrst to introduce a GPU with a programmable pixel shader in the 
NV20, used on the GeForce 3 AIB and released in February 2001.

3.2 The Second Era—Programmable Shaders—DirectX 8 and 9 (2001–2006)
73
Fig. 3.8 Vertex shading in the DirectX 8 pipeline
The application program calls the pixel shader once per-pixel, whereas a vertex 
shader gets called for every vertex. Its data come from the 3D model. It calculates 
the required pixel color (pixel depth or any other values are possible) and returns 
them to the next stage in the DirectX pipeline. 
With programmable shaders came a host of new functions to enhance the graphics 
images. 
3.2.1 
Pixel (Fragment) Shader Stage 
A pixel shader (also known as a fragment shader in OpenGL) processes each triangle 
fragment and the pixel’s color. The input to the pixel shader comes from the previous 
stages in the pipeline, typically the rasterizer. (More complex pixel shaders with 
multiple inputs/outputs are also possible.) Pixel shaders are used primarily for calcu-
lating surface properties, lighting, and postprocess effects and were the ﬁrst shaders 
to be developed (see Pixar story above). 
After transforming (converting) the coordinates of a triangle from the 3D model’s 
world coordinates to window coordinates (the display), the triangle undergoes a 
process called scan conversion or rasterization. That scan conversion breaks up the 
triangle based on the association of visible window pixels over the output image that 
the triangle covers. Then it goes to the fragment shader (Fig. 3.9). 
The basic triangle (left) gets sent to the scan converter (center). It shows the grid 
of the output (display) pixels. During scan conversion (rasterization), a triangle will 
generate a fragment of the triangle for every pixel that ﬁts within the area of the
Fig. 3.9 A scan-converted triangle quantized by pixel size 

74
3
The Major GPU Eras
triangle (right illustration). If the area is less than 50% of the pixel area, it does not 
get generated. 
A pixel shader can range from a simple fragment operation (e.g., always output 
of the same color) to a complex shader that generates shadows (e.g., for a bump), 
applying specular highlights, translucency, and other lighting effects. Pixel shaders 
modify the depth of the fragment (for z-buffering). 
A pixel shader cannot produce complex effects because it operates on only one 
fragment and has no information about the scene’s geometry, which would come 
from the vertex information. 
However, pixel shaders do get information about the screen coordinates. And if 
the entire screen gets passed to the pixel shader as a texture, it can sample it and the 
nearby pixels. That technique enables postprocessing effects such as blur or edge 
detection. 
3.2.2 
How Many Shaders? Is There a Limit? 
The development of parallel processors in the late 1960s led to the observation 
that there were only so many processors one could run together and still get an 
acceleration of processing. Dr. Gene Amdahl, known for being the architect of the 
powerful and famous IBM 360, presented a paper at the American Federation of 
Information Processing Societies (AFIPS) Spring Joint Computer Conference in 
1967 on the topic of how many processors one could use. He presented a formula 
that showed how the speedup of adding processors falls after a certain number. That 
became known as Amdahl’s Law [12]. 
Empirical experiments conﬁrmed his observations. It became a generally accepted 
fact that one can add only so many processors until servicing them becomes so 
burdensome the work gets delayed; latency encroaches on the workﬂow (also known 
as the law of diminishing returns). 
In 1988, Dr. John L Gustafson, a noted computer scientist, introduced a counter-
proposed observation, known as Gustafson’s Law [13]. Gustafson said that the theo-
retical speedup in latency of the execution of a program was a function of the number 
of processors executing it for different values of percentage of the workload of the 
whole task. 
The expansion of shaders (processors) within a GPU was phenomenal over the 
past decades. How had that been possible, and when would it stop? Why wasn’t 
Amdahl’s Law a factor? Was it because they were SIMD processors and do not 
do much inter-processor communications? Did memory controllers evolve so well 
that there was little to no contention there? Or was Amdahl wrong and Gustafson 
correct? [1] Is there an upper limit to how many shaders can ﬁt on the head of a pin? 
(Fig. 3.10). 
Mick Mantor, Corporate Fellow and Chief GPU Architect at AMD, said (in 2019),

3.3 The Third Era—The Uniﬁed Shader—DirectX 10 and 11 (2006–2009)
75
Fig. 3.10 Mike Mantor, 
Corporate Fellow and Chief 
GPU Architect at AMD. 
Reproduced with permission 
from Expreview.com 
It is amazing, the expansion that has happened. The simple expansion is silly data-parallel 
with vertex/pixel/ray processing as there are millions of these. Compute codes also can 
have a supercomputer level of parallelization. Serial dependencies are always a concern, 
and different tasks and processes can sometimes hide serial dependencies issues. Increasing 
resolutions and work per parallel thread/pixel/ray/vertex, etc., is contributing enablement 
of more threads in ﬂight. It seems both laws have relational properties involved that enable 
continued growth. Also, while GPU clock speeds are improving, some forms of Amdahl’s 
law are invoked as some latencies and speeds move at different rates of progress. My quick 
study of Gustafson’s Law suggests that it modiﬁes Amdahl’s law appropriately. I think we 
are far from done; we are power- and money-limited more than anything else today. 
So the answer is a shrug and a yes, both conditions (would be laws) can be correct. 
Processor architects clearly understand the issues and add caches, and processing 
short-cuts to mitigate the barriers. There will always be a bottleneck somewhere in 
the system and it gets moved with each new design, often ending up at the same 
place with everything running ten times as fast. 
3.3 The Third Era—The Uniﬁed Shader—DirectX 10 
and 11 (2006–2009) 
GPUs evolved to having thousands of shaders or processors in them, with DirectX 
10 and Shader Model 4.0, in 2006. GPUs used a uniﬁed Shader Model. The previous 
shader abstractions used different and speciﬁc instruction sets for vertex and pixel 
shaders, vertex shaders having a much more ﬂexible instruction set. But when vertex 
shading was not needed, those shaders sat idle. 
A GPU with uniﬁed processor cores is organized into multithreaded multipro-
cessors known as streaming processors (SPs) by AMD and Nvidia or execution 
units (EUs) by Intel and others. Nvidia also refers to them as Compute Uniﬁed 
Device Architecture—CUDA processors. For this book, the term SP is for nonde-
clared processors or shaders—processors that do not have speciﬁc functions such as 
a vertex or pixel shader. Each SP (also referred to as a core) is part of a multithreaded

76
3
The Major GPU Eras
processor (MP), and the MP can manage dozens of concurrent threads. A thread 
on the GPU is a fundamental element of the data to be processed. The processors 
connect banks of memory (RAM) via an interconnection network. 
The Direct3D 10 as uniﬁed shader Model 4.0 refers to a form of shader hardware 
in a GPU where all the shader stages in the rendering pipeline (geometry, vertex, 
pixel, etc.) have the same capabilities. 
ATI Technologies pioneered a uniﬁed architecture in the Xbox 360 in 2006 with 
the Xenos chip (Fig. 3.11) [14]. Nvidia introduced uniﬁed shaders with their Tesla 
architecture (Fig. 3.12) in November 2006, and AMD launched a uniﬁed shader with 
their TeraScale architecture in Radeon HD 2900 XT in 2007. The concept has been 
employed ever since. 
However, a GPU is not required to have uniﬁed shading architecture to support 
the uniﬁed Shader Model. A DirectX 10-level hardware could have dedicated vertex, 
geometry, and pixel processors. And an earlier (e.g., Shader Model 3.0) GPU could 
also have a uniﬁed architecture, such as ATI’s Xenos graphics chip in Xbox 360. 
A streaming multiprocessor (SM) is part of the GPU. Each SM has some number of 
SP cores; the number varies by manufacturer and model. Eight is a typical number of 
cores, like those shown in Fig. 3.12. The SM will usually also have one or two special 
function units (SFUs). SFUs compute transcendental values such as sine, cosine, 
reciprocal, and square root. In early GPU designs, the transcendental functions were
Fig. 3.11 ATI/AMD Xenos block diagram

3.3 The Third Era—The Uniﬁed Shader—DirectX 10 and 11 (2006–2009)
77
Fig. 3.12 A typical uniﬁed architecture GPU, with 112 streaming processor cores, organized as 14 
multithreaded streaming multiprocessors. Nvidia’s Tesla GeForce 8800 [15]
ﬁxed (hardwired) and limited in range. Also, there were local instruction and constant 
caches and a thread issue controller or manager, also known as a multithreaded 
instruction unit. 
Traditional graphics functions for vertex, geometry, and pixel shading run on the 
uniﬁed SMs and their SP cores; computing programs run on the same processors. 
Due to the redundancy of cores in a GPU, it is relatively easy to scale the design 
(in manufacturing) up or down to meet different application and market segment 
needs. The GPU also includes a texture mapping unit. Originally it was a separate 
physical processor. A TMU is used to transform (rotate), resize, and distort a bitmap 
image on any plane of a 3D model. 
In DirectX 9.0c, even with the similar vertex shader and pixel shader functions, 
GPU manufacturers still had to divide two areas in the GPU to store the vertex 
shader array and pixel shader texture pipeline. That was a redundancy of resources; 
it also increased the difﬁculty and cost of GPU design and complicated programming. 
DirectX 10 subdivided the rendering process into vertex shader, geometry shader, 
and pixel shader operations, independent of the hardware, and integrated those two 
kinds of rendering together [16]. The uniﬁcation made better use of resources and 
was an easier programming model.

78
3
The Major GPU Eras
3.3.1 
Geometry Shader (2006) 
Microsoft introduced geometry shaders with Direct3D 10 in 2006, and Khronos did 
it with OpenGL 3.2 in 2009. The geometry shader is the dark blue block in Fig. 3.13. 
The geometry shader stage of the pipeline accepts primitives (i.e., a point, line, 
or triangle) from the vertex shader as an input. However, unlike the vertex shader, 
the geometry shader can modify, remove, or add geometry to the primitives. 
A geometry shader (GS) is a program to govern the processing of primitives. It 
comes after the vertex shaders (or the optional tessellation stage) and feeds the ﬁxed
Fig. 3.13 DirectX 11 
rendering pipeline featuring 
tessellation (gray box) 

3.4 The Fourth Era—Compute Shaders—DirectX 11 (2009–2015)
79
function vertex postprocessing stage. However, the use of the geometry shader is 
optional. 
A geometry shader can perform tessellation, shadow volume extrusion, single-
pass rendering to a cube map, and sprite generation geometry. 
One of the beneﬁts of a geometry shader is automatic mesh complexity or more 
triangles. A series of basic line strips representing control points for a curve gets 
passed to the geometry shader. It generates extra lines that provide a better curve 
approximation (depending on the required complexity). 
Geometry shaders can generate more geometry, but they do not scale well. For 
example, a multi-pass rendering for a cube map is faster than a single pass in a 
geometry shader [17]. 
In mobile devices, geometry shaders get implemented in software. Mobile GPUs 
that offer geometry shaders with OpenGL ES have stopped reporting (and supporting) 
them in Vulkan. 
However, if one wants to draw particles or turn points into simple geometry, a 
geometry shader does an excellent job. They have also been used for marching cubes, 
and they work for transform feedback to save a resulting mesh. Transform feedback 
occurs when the geometry shader does more compute operations, such as stream 
compaction. (Stream compaction is an important parallel computing primitive that 
produces a reduced (compacted) output stream consisting of only valid elements 
from an input stream containing both invalid and valid elements.) 
3.4 The Fourth Era—Compute Shaders—DirectX 11 
(2009–2015) 
Compute shaders expose the shaders for nongraphical tasks such as stream processing 
and physics acceleration. Compute shaders are not limited to high-performance 
computing (HPC), supercomputers, or powerful servers. Graphics applications can 
use the same execution resources as GPU-compute (GPGPU) applications. Compute 
shaders get used in the graphics pipeline for additional operations in animation or 
lighting. In most rendering APIs, compute shaders share data resources with the 
graphics pipeline. 
The multicore processor in a GPU, known as a shader, is constructed as single-
instruction, multi-data (SIMD), also referred to as single-instruction multiple-thread 
(SIMT). 
To manage and execute dozens to hundreds of threads running several different 
programs simultaneously, the GPU employs an SIMD setup. It creates, manages, 
schedules, and executes concurrent threads in groups of parallel threads called warps. 
The term warp originates from weaving, the ﬁrst parallel thread technology. A multi-
processor GPU uses a SIMD warp size of 32 threads, executing 4 threads in each of 
the 8 streaming processor (SP) cores over 4 clocks. 
Compute shaders can also be part of mesh shaders as mentioned above.

80
3
The Major GPU Eras
3.4.1 
Tessellation Shader (October 2009) 
Tessellation (the gray bounded block in Fig. 3.13) is a part of the vertex processing 
stage in the OpenGL rendering pipeline, where vertex data get subdivided into smaller 
primitives. In OpenGL, it consists of two shader stages and a ﬁxed function stage. 
The tessellation shader gets used for variable subdivisions. It requires adjacency 
information, so smoothing gets done correctly, and gaps do not get created. A 
geometry shader can do minor subdivisions, but that is not its intended purpose. 
In late 2009, the tessellation shader got added to Direct3D 11. OpenGL got it in 
version 4.0 in 2013, shown in Fig. 3.13. 
Direct3D 11 included two new shader stages—a tessellation control shader (also 
known as hull shaders [18]) and a tessellation evaluation shader (also known as 
Domain shaders [19]). According to a mathematical function, the Hull and Domain 
shaders enable meshes to get subdivided into ﬁner meshes at runtime (according to 
a mathematical function). 
The amount of tessellation gets calculated on the distance of an object to the 
viewing camera. It provides a dynamic level-of-detail scaling. (Objects closest to 
the camera have greater detail; objects further away have fewer meshes but seem 
comparable in quality.) 
Tessellation also reduces mesh bandwidth because meshes get reﬁned inside the 
shader units instead of down-sampling complex ones from memory. Some algorithms 
can up-sample any arbitrary mesh, whereas others allow hinting in meshes to dictate 
the most distinct vertices and edges. 
Tessellation 
As the triangles began piling up, it became apparent something had to be done—the 
obvious conclusions were that they would increase exponentially to inﬁnity. Was 
there a point of diminishing returns? Yes, there was. You do not need a million 
triangles to describe a hill that is a kilometer in the distance. But you might need to 
use a million triangles to describe the nose of a character 20 cm from you. 
A tessellation is a group of polygons ﬁtted closely together in a repeated pattern 
as in the back wall in Fig. 3.14. The origin of tessellation can be traced back to 4,000 
BCE when the Sumerians used clay tiles to compose decoration features in their 
homes and temples [20] (Fig. 3.14).
There are only three regular shapes that can tessellate or tile a plane: the triangle, 
square, and hexagon. The artist M. C. Escher used tessellations for artistic effect, 
employing Euclidean geometry and hyperbolic geometry [21]. 
In computer graphics, tessellation describes a dynamic system for adding and 
subtracting triangles from a 3D polygon mesh based on control parameters such as 
the camera (viewer’s) distance. 
One of the ﬁrst mentions of tessellation in computer graphics was in 1978. Ed 
Catmull and Jim Clark developed their rules for bidirectional subdivision (Fig. 3.15). 
It worked from ultraﬁne mesh to cube and vice versa [22]. Subdivision is also referred 
to as Sub-D.

3.4 The Fourth Era—Compute Shaders—DirectX 11 (2009–2015)
81
Fig. 3.14 Tessellation in the chamber for the circumcision of the princes in the Imperial Topkapı 
Sarayı, Istanbul, ﬁfteenth century
Fig. 3.15 Catmull–Clark subdivision of a cube. Reproduced with permission from Romainbehar, 
Wikipedia 
Tessellation and subdivision algorithms were developed on the CPU, just as trans-
formations had been—just another math function and, like transformations, a CPU-
consuming algorithm. Naturally, it made sense for something so important and so 
frequently used in computer graphics to be hardware-accelerated. 
In 2001, the design team led by ATI’s CTO Adrian Hartog introduced the Radeon 
8500 AIB based on the R200 GPU (Fig. 3.16). It was the ﬁrst GPU to offer hardware 
tessellation. The company marketed the feature as TruForm, although a small group 
wanted it called the Harttog engine.
The tessellation feature of the GPU required a unique API to expose the new 
capabilities to application developers. Later it was added to OpenGL via the GL ATI 
triangles extension. However, no other GPUs had it yet, so it was not approved to 
be in the OpenGL core. The R200 provided programmable support through DirectX 
and OpenGL for the initial vertex and the ﬁnal fragment processing (Fig. 3.17). In 
between those stages, it used ﬁxed function tessellation hardware with predeﬁned 
tessellation evaluation modes.
The only variable the tessellator accepted was the level of tessellation. That 
controlled how many divisions were performed over each edge of the input primitive 
(triangle) [23]. The ATI GL triangles were quite different within that tessellation

82
3
The Major GPU Eras
Fig. 3.16 The ﬁrst GPU with integrated hardware tessellation was the ATI Radeon 8500. 
Reproduced with permission from Palit, ixbt.com
Fig. 3.17 ATI’s tessellation pipeline
compared to those introduced years later in DirectX 11 and OpenGL 4. They were 
inﬂexible and therefore required extra work to be implemented. ATI/AMD eventu-
ally dropped ATI GL triangles. Developers did not like the additional programming 
required to use them. 
Hardware tessellation support became available in Direct3D 11 (July 2008) and 
OpenGL 4 (2010). By then, ATI had a newer version of its tessellation engine called 
TeraScale 2, which it introduced in September 2009. 
Nvidia’s (2010) GTX 480 (code name Fermi) featured 11 dedicated tessellation 
engines.

3.5 The Fifth Era—Ray Tracing and AI—DirectX 12 (2015–2020)
83
Hull Shader 
Also known as the tessellation evaluator, the hull shader is used only for tessellation. 
It receives the vertices as input control points and converts them to control points 
that make up a patch (a fraction of a surface) [24]. 
Domain Shader 
Like the Hull shader, the domain shader (also known as the primitive assembler) is 
used only for tessellation. This stage calculates a vertex position of a point in the 
patch created by the Hull shader. A domain shader facilitates the third and ﬁnal stages 
in the tessellation pipeline. The domain shader generates surface geometry from the 
hull shader’s transformed control points and u,v-coordinates. 
One domain shader got created for each point that got generated by the (ﬁxed 
function) tessellation shader. The inputs were u-, v-, and w-coordinates of the patch 
and all the output data from the hull shader control points and patch constants. 
3.4.2 
Summary 
Hardware-accelerated tessellation was one of the most powerful and exciting devel-
opments of GPUs in the late 2000s. It showed the way toward the GPU becoming a 
compute device and not just an extension of special function processors. 
3.5 The Fifth Era—Ray Tracing and AI—DirectX 12 
(2015–2020) 
Ray tracing’s ability to generate physically accurate and almost perfect images in a 
computer have made it an attractive and desirable function since 1968 when Arthur 
Appel described in his paper, “Some techniques for shading machine renderings of 
solids,” in May 1968 [25] (Fig. 3.18).
And the earliest recorded reference to ray tracing system is the computerized ray 
tracer used at the Ballistic Research Laboratory (BRL) [26]. 
In 1979, Turner Whitted would elaborate on the ray casting algorithms developed 
in the 60 s to introduce recursive ray tracing [27]. Whitted followed the path of a ray 
beyond the initial surface it hits. He said after a ray hits an object, it generates three 
new rays, a reﬂection, refraction, and shadow, which can also be traced to greatly 
improve realism. Whitted’s paper can mark the beginning of the pursuit to use a 
computer to generate photorealistic and physically accurate renditions of 3D objects 
in 3D space. 
In 2009, Intel showed real-time ray tracing running on a 16-core 2.93 GHz Xeon 
processor (see Larrabee story in Book one). And predictions based on Moore’s law 
were made as to when it might be possible with a GPU.

84
3
The Major GPU Eras
Fig. 3.18 Appel projected 
light at a 3D computer model 
and displayed the results on a 
plotter using a form of 
tone-mapping to create light 
and dark areas. Source 
Arthur Appel
In 2014, Silicon Arts showed an FPGA implementation of their real-time ray 
tracing accelerator, and that same year Imagination Technologies showed real-time 
ray tracing silicon with their R6500 test chip code named Plato. 
In 2017, Nvidia showed a real-time ray tracing demo four Quadro AIBs and it 
was speculated we would see real-time ray tracing by 2021 or 2022. 
Nvidia ﬁrst revealed its planned Volta architecture on a road map in March 2013 at 
Nvidia’s GPU Technology Conference (GTC) in San Jose [28]. In May 2017, Nvidia 
announced the new GPU. And on September 20, 2018, Nvidia started shipping AIBs. 
The Volta architecture was in the GV100 series GPUs, which were in the RTX 3000 
series AIBs. 
The Volta series GPUs introduced two new processors: the ray tracing accelerator 
cores (intersection detection engines) and the tensor cores, FP32 matrix math engines. 
The GPU was compatible with DirectX 12 for hardware ray tracing, variable rate 
shading, and other features. 
3.5.1 
Ray Tracing Shaders 
Ray tracing is a computationally intense operation, equivalent to drawing the same 
image 10–20 times. Therefore, real-time ray tracing would require a processor that 
could generate 20 times 30 frames a second—600 fps—and at 4 K resolution. One 
can appreciate the computational burden by looking at the work needed to create such 
an image. Look at Fig. 3.19. The ray tracing paradigm (derived from the RenderMan 
documentation).

3.5 The Fifth Era—Ray Tracing and AI—DirectX 12 (2015–2020)
85
Fig. 3.19 The ray tracing paradigm 
In a recursive ray tracer, rays are cast from the eye to a point on the image plane. 
Each ray meets a surface that causes new rays to be reﬂected. The rays are directed 
to the light sources in the directions of maximum reﬂection or transmittance. When 
a ray travels through space, its color and intensity get altered by its environment. 
When a ray hits a surface, the shader associated with the surface is activated and 
generates new rays. It also determines the color and intensity of the incoming ray 
from the outgoing rays and the surface’s properties. Finally, when a ray gets sent to 
a light source, the shader associated with that light source determines the intensity 
and color of the ray emitted. 
The DirectX ray tracing (DXR) pipeline required programmable ray generation to 
set up each ray and material shading after traversal. The DXR employed a bounding 
volume hierarchy (BVH). 
A BVH is a tree structure on a set of geometric objects. All geometric objects are 
wrapped in bounding volumes that form the leaf nodes of the tree. These nodes are 
then grouped as small sets and enclosed within larger bounding volumes (Fig. 3.20).
Geometry contained by the bounding volumes which are not intersected by the 
ray can be safely rejected. 
The queries against the BVH beneﬁted from caching. For most solutions, the 
intersection engine is located close to a cache and invoked by a compute shader 
or something similar, separate from the actual graphics pipeline. DXR 1.1 enabled 
RayQuery from any shader stage. Some solutions might use the rasterizer for a full-
screen rectangle to initiate a primary ray launch. None of the companies supporting

86
3
The Major GPU Eras
Fig. 3.20 Bounding boxes enclosing a patch from the teapot model are tested for an intersection 
with a ray
DXR clearly articulated how they accomplish RT. Shader evaluation pipeline is 
illustrated in Fig. 3.21. 
Ray tracing scales differently compared to polygonal rasterization. Polygon 
rendering (described previously in this and other chapters) slows down as the number 
of polygons increase; ray tracing is not affected as much by the number of polygons, 
which suggests ray tracing could render more polygons in an image and improve 
realism and accuracy. With AI techniques to aid in real-time ray tracing frame rate, 
developers may be encouraged to be more ambitious. 
Conversely, polygonal rasterization is not as affected by increased screen resolu-
tion. Ray tracing and path-tracing complexity increase linearly with the number of 
pixels. 
To partially offset the pixel barrier, ray tracing does not get used for everything 
in an image. Ray tracing and hybrid ray tracing get used for minor things such as 
reﬂections and shadows.
Fig. 3.21 Shader evaluation 
pipeline 

3.5 The Fifth Era—Ray Tracing and AI—DirectX 12 (2015–2020)
87
Fig. 3.22 Ray-traced image of a Mercedes-Benz SS Roadster. Reproduced with permission from 
Volkan Kaçar 
The other lighting advantage is that a typical path-tracing engine has perfect anti-
aliasing, equivalent to around 100× MSAA at the least. Nvidia’s RTX did not do 
this. The primary camera rays were not ray-traced; only secondary rays were. 
The history of ray tracing dates to Newton in his treatise Opticks [29] and one can 
ﬁnd references as far back as Euclid and his student Archimedes. Initially used for 
calculating the effects of lenses, the mathematic techniques were adapted to computer 
graphics in 1975 [30]. In 1980, an improved and faster process was introduced [31]. 
Ray tracing is a procedure used in computer graphics to generate an image by 
tracing the path of light from pixels in the image and simulating the effects of light 
as it interacts with virtual objects [32]. Figure 3.22 is a perfect example of creating 
a beautiful image that is physically accurate. 
Ray tracing produces physically perfect renditions of a scene. The drawback is 
it is a computationally heavy operation, especially as resolution and color depth is 
increased. 
3.5.2 
Real-Time Ray Tracing with AI 
With the experience of using GPUs for AI training, in 2018, Nvidia introduced a 
technique called deep learning super sampling (DLSS). With DLSS, the company 
was able to demonstrate real-time ray tracing (RTRT). PC gaming had been the initial 
and primary use for RTRT; however, other simulation applications could easily use 
it—and did. 
AI revolutionized gaming, ﬁrst used for game physics, then animation simulation, 
and in the late 2010s for real-time renderings. Using DLSS techniques, Nvidia accel-
erated real-time rendering using AI-based super-resolution—they rendered fewer

88
3
The Major GPU Eras
pixels ﬁrst and then used AI to create sharp, higher resolution images. It was 
incredibly effective and pushed ray tracing into the mainstream. 
The process was started by separating several frames with aliasing from the game. 
Such frames render faster but are not too attractive. For each such frame, Nvidia 
created a matching frame. To do that, they applied either supersampling or accu-
mulation rendering to the image (Fuchs described using a successive reﬁnement 
method that uses accumulation to create anti-aliased (AA) images by rendering the 
scene repeatedly with subpixel offsets) [33]. 
That resulted in having two frames or images—one perfect and one not. Nvidia 
then fed the two images to its supercomputer using AI and deep learning neural 
network techniques (DLNN). The supercomputer trained a DL supersampling 
(DLSS) model to recognize aliased inputs and generate high-quality AA images 
that matched the perfect frame as closely as possible. They repeated the process, but 
Nvidia trained the model to create additional new pixels rather than AA on the second 
pass. Adding pixels (interpretative or tweening [34] pixels) resulted in increasing the 
resolution of the original frame (Fig. 3.23). Combining both techniques enabled the 
GPU to render ray-traced images at the monitor’s full resolution and higher frame 
rates [35, 36]. 
Nvidia used an AI network called a convolutional autoencoder for the DLSS. A 
convolutional autoencoder compares the output image to a 16 k ofﬂine reference 
image. The difference was transmitted back into the network, and it continued to 
learn and improve its results. They repeated the process tens of thousands of times on 
the supercomputer until the network generated high-quality, high-resolution images. 
After training the network, Nvidia delivered the AI model through its drivers. So, 
it was not a universal solution but a game-speciﬁc solution. The game developers 
loved and supported it. 
The DLSS was a proprietary system that ran on Nvidia AIBs that had dedicated 
tensor core AI processors. Given that DSS used AI training, it was not an automatic 
process. With the initial 1.0 roll-out of DLSS, Nvidia trained the AI for each game it 
was willing to support. With DLSS 2.0, Nvidia improved the AI network, resulting
Fig. 3.23 DLSS off and on.n Source Nvidia 

3.5 The Fifth Era—Ray Tracing and AI—DirectX 12 (2015–2020)
89
in better image and motion quality, faster networks to enable all Nvidia RTX GPUs 
and resolutions, and a generalized network to support all games. 
DLSS let users choose image quality modes—from quality to ultra-performance 
(depending on the game)—that controlled the game’s internal rendering resolution. 
Performance mode enabled up to 4X AI super-resolution (i.e., 1080p render resolu-
tion to 4 K output resolution), whereas the ultra-performance mode enabled up to 
9X AI super-resolution (1440p to 8 K). 
Nvidia’s Game Ready Driver offered three Freestyle ﬁlters—SSRTGI, SSAO, and 
Dynamic DOF—that were accessible by pressing Alt + F3 during gameplay when 
GeForce Experience’s in-game overlay was enabled in the app’s settings menu.
• SSRTGI (Screen Space Ray Traced Global Illumination), commonly known as the 
Ray Tracing ReShade Filter, enhanced lighting and shadows of games to create 
a greater sense of depth and realism. SSRTGi only used the depth buffer from 
games to approximate dynamic indirect lighting to objects within the screen view. 
It did not interact with engine information.
• SSAO (Screen Space Ambient Occlusion) emphasized the appearance of shadows 
near the intersections of 3D objects, especially within dimly lit/indoor environ-
ments.
• Dynamic DOF (Depth of Field) applied bokeh-style blur based on the proximity 
of objects within the scene giving your game a more cinematic suspenseful feel. 
Real-time ray tracing was initiated in 2018 and primarily used for PC gaming 
and later applied to other simulation applications. The content creation industry 
began deploying the technology to enhance all sorts of workﬂows, including virtual 
production, architectural visualization, animated ﬁlms, product design, simulations, 
and data generation [37]. DLSS found acceptance in performance-hungry, real-time 
productions in virtual production, architectural visualization, and product design. 
3.5.3 
Variable Rate Shading—2019 
Performance constraints can limit rendering, altering the level of quality within parts 
of its output image. Variable rate shading (aka coarse pixel shading—CPS) allocates 
rendering performance at different rates that vary across a rendered image. 
Multisampling anti-aliasing (MSAA) with sample-based execution (also known 
as supersampling) is an alternative to variable rate shading (VRS). 
Nvidia introduced VRS with its Turing architecture in the RTX2000-series AIBs 
in August 2018. It was a new and more ﬂexible technique to control the shading rate. 
The shading rate could be adjusted dynamically across the image in 16 × 16-pixel 
tiles. 
At the Game Developer’s Conference (GDC) in San Francisco in 2019, Microsoft 
announced its DirectX API would include VRS. Microsoft said at the time that games 
would beneﬁt from double-digit rendering speed gains using the technique. Microsoft

90
3
The Major GPU Eras
said at the time that VRS scenes were indistinguishable in quality compared with 
scenes that did not use the method. 
Microsoft also employed VRS in the Xbox One and the Sony PlayStation 5 in 
2020. 
Due to performance limitations, graphics renderers cannot consistently deliver the 
same degree of quality to every part of the output image. VRS enables the allocation 
of rendering performance at varying rates across the rendered image. 
Visually, there are cases where reducing the shading rate has little or no reduction 
in perceptible image quality. 
AMD and Nvidia had tool kits for game developers to accomplish VRS. The 
tools analyzed the luminance and motion of frames to guide developers on where 
to maximize rendering for the best image quality without lowering the performance 
[38]. 
If a game is run at reduced resolution (to improve performance), it creates jagged 
edges for the objects (aliasing), illustrated in Fig. 3.24. It can also compromise the 
visibility of smaller objects, resulting in shimmering and ﬂickering [39]. 
Anti-aliasing techniques such as multisampling or supersampling can mitigate 
aliasing. However, anti-aliasing can affect performance due to wasted rendering. 
Multi-resolution shading (MRS) and lens-matched shading (LMS) used in VR 
systems minimize excessive pixel shading. MRS is best suited for applications that 
have limited ﬂexibility in terms of pixel shading patterns. VR uses LMS for prob-
lems with headset lens distortions. Rasterization is done at lower resolution in both 
techniques, requiring a separate upscaling pass. 
VRS reduces excessive pixel shading load, allows customizing shading rates 
within the frame, selectively improves visual quality with supersampling, and
Fig. 3.24 Reduced resolution rendering followed by upscaling gives an enhanced image at high 
frame rates. Reproduced with permission from Nvidia 

3.6 The Sixth Era—Mesh Shaders—DirectX 12 Ultimate (2020)
91
Fig. 3.25 Variable rate shading architecture. Reproduced with permission from Nvidia 
preserves the edges and visibility of the objects (Fig. 3.25). VRS works at screen 
space, making it simple to integrate into applications. 
The goal of VRS was to save GPU work (where it does not signiﬁcantly contribute 
to the ﬁnal frame). Gamers wanted to play VR at high resolution, so the pixels on 
the screen were small. Adjacent pixels often have similar colors (if they belong to 
the same primitive). Postprocessing effects like AA, depth of ﬁeld, or motion blur 
further reduce the difference between adjacent pixels. 
3.6 The Sixth Era—Mesh Shaders—DirectX 12 Ultimate 
(2020) 
This GPU environment chapter discusses the evolution of the GPU from ﬁxed func-
tion T&L to compute mesh shaders. One of the signiﬁcant stages of that evolution 
was the adoption of mesh shaders, which moved the GPU into the general compute 
arena. 
There is some disagreement in the industry whether the DirectX 12 Ultimate 
introduction of mesh shaders truly constitutes an evolution of an era or is just an 
extension of the existing era. As is shown in Fig. 6.36. in the image from Epic 
games’ Land of Nanite, you will discover mesh shaders bring an extraordinary level 
of detail without an over burdening of work. Even game consoles with their limited 
number of shaders, clock speeds, and memory can run mesh shaders and accomplish 
astoundingly detailed and globally illuminated images.

92
3
The Major GPU Eras
3.6.1 
Primitive and Mesh Shaders—2017–2020 
In 2017, AMD added support for a new shader stage in their Vega microarchitecture 
they called the primitive shader (referring to the triangle primitives). Primitive shaders 
were developed to eliminate the number of triangles that are displayed by eliminating 
those farthest from the front objects or any triangle that is not going to be shown 
on the screen. The following year, Nvidia introduced its Turing microarchitecture 
with mesh and task shaders, which provided similar functionality and, like AMD’s 
primitive shaders, were modeled after computing shaders [40, 41]. 
In January 2018, AMD canceled the development of its primitive shaders. AMD 
would not be implementing primitive shaders separately but released a speciﬁc API 
patch for developers that used it. AMD did not say why it canceled the project, but 
it was probably because mesh shaders were in DirectX and Vulkan, and primitive 
shaders never were. 
Mesh shaders bring the full compute programming model to the graphics pipeline. 
In a mesh shader, threads get used cooperatively to generate compact meshes (mesh-
lets). The meshlets become the input for the rasterizer. Applications dealing with 
high-geometric complexity beneﬁt from the ﬂexibility of the two-stage approach, 
which allows efﬁcient culling, level-of-detail techniques, and procedural generation. 
The mesh shader gave developers new possibilities for avoiding bottlenecks and 
wasted compute cycles. The memory is read once and kept in the GPU instead of 
previous approaches (such as shader-based primitive culling) [42]. 
Instead of processing one function that shades one vertex or one primitive, 
mesh shaders operated across an entire compute thread group. They have access 
to the group’s shared memory, and advanced compute features such as cross-lane 
wave intrinsics (communicating values between threads typically done via shared 
memory). That provided a ﬁner grained control over the hardware execution. 
The mesh shader produces triangles for the rasterizer but internally uses a cooper-
ative thread model instead of a single-thread program model, like compute shaders. 
Ahead of the mesh shader in the pipeline is the task shader. The task shader operates 
similarly to the control stage of tessellation in that it can generate work dynamically. 
However, like the mesh shader, it uses a cooperative thread model. Instead of taking 
a patch as input and tessellation decisions as output, its input and output are user 
deﬁned. 
The task shader simpliﬁed GPU geometry creation. In rigid and limited tessellation 
and geometry shaders, threads were for speciﬁc tasks; mesh shaders were the next 
step in handling geometric complexity. 
Mesh shaders also align the PC and game consoles making it easier and faster 
for developers to release comparable games for both platforms sooner. That was 
demonstrated in the Unreal preview used on the PlayStation 6.

3.7 Summary on Shading
93
3.6.2 
Sampler Feedback 
Sampler feedback enables developers to load in textures when needed. So when 
the camera moves swiftly through the scenes, some objects look out of position. 
Sampler feedback will make sure to offer better visual quality, shorter load time, and 
less stuttering. The general process of loading texture data on-demand, rather than 
upfront-all-at-once, is called texture streaming. 
Use of sampler feedback with streaming is sometimes abbreviated as SFS. It is 
also sometimes called sparse feedback textures, or SFT, or PRT+, which stands for 
“partially resident textures.” 
3.7 Summary on Shading 
The foundations for the GPU were the front-end vertex processor and the back-end 
pixel processor. Before the introduction of the single chip integrated GPU, those 
processors were usually separate. As mentioned, the term shader was given to us in 
May 1988 by Pixar’s RenderMan Interface. Since then, the processors, or processor 
functions, have been referred to as shaders—regardless of if they had anything to do 
with shading (coloring) a pixel. 
When referring to a GPU, the term shader means a 32-bit ﬂoating-point processor 
within a GPU. The function of shaders has evolved from ﬁxed or dedicated processes, 
such as vertex transformation and pixel coloring to multipurpose shaders or uniﬁed 
shaders, to unrestricted compute shaders or mesh shaders. 
GPUs get used in ﬁve primary platform segments: mobile devices, PCs, consoles, 
server- and cloud-based systems, and location-based entertainment (LBE) systems. 
Applications and location differentiate the platforms. Secondarily the types of 
applications are also segments. Brief comments and deﬁnitions are given about the 
applications, but this is a hardware book, not a software book. 
Figure 3.26 shows the overall taxonomy of GPU uses.
The following sections discuss the individual property volumes of the platform 
categories. 
3.7.1 
Mobile 
GPUs used in mobile devices include smartphones, dedicated gaming handheld 
devices, and hybrid devices, as illustrated in Fig. 3.27.
Various companies build the SoCs that go into mobile devices, such as Apple, 
MediaTek, and Samsung that make their own SoCs.

94
3
The Major GPU Eras
Fig. 3.26 GPU taxonomy.n Reproduced with permission from JPR
3.7.1.1 
GPU Sources for Mobile Devices 
Mobile PC and Chromebook devices get their GPUs from a broad variety of sources 
as depicted in Fig. 3.28.

3.7 Summary on Shading
95
Fig. 3.27 Mobile gaming device
Fig. 3.28 Mobile GPU design sources

96
3
The Major GPU Eras
All the builders of mobile devices buy semiconductors from a fabless supplier 
such as AMD, MediaTek, Nvidia, Qualcomm, or in the case of Apple, directly from 
a fab  like TSMC.  
3.7.2 
GPU-Compute 
GPUs’ parallel processing characteristics and capability have made them valuable 
and economically signiﬁcant as compute coprocessor units. This functionality is 
known as GPU-compute and is also popularly referred to in the press and elsewhere 
as GP GPU; the GP in GP GPU stands for general purpose. The term is slightly 
inaccurate. Due to its unique SIMD architecture and lack of an X86 instruction 
set compatibility, a GPU can never be a general-purpose processor. It cannot run 
a Windows or Linux operating system or run popular Windows programs (e.g., 
Microsoft Ofﬁce)—but it can run thousands of threads at the same time and produce 
answers at orders of magnitude greater and faster than a serial processor like a CPU. 
GPU-compute will increase the sales of AIBs because they get employed for 
applications other than graphics. AMD and Nvidia have speciﬁcally branded GPU-
compute products (FireStream from AMD, Intel’s Ponte Vecchio, and Tesla from 
Nvidia). The companies also have speciﬁcally branded professional graphics AIBs 
for the workstation market (FirePro from AMD and Quadro from Nvidia). However, 
in 2021, AMD’s new W-series of AIBs replaced the FireStream line. The W-Series 
served as the AIB for both the professional market and the server-based market. 
In addition, both companies have speciﬁcally branded consumer and commercial 
graphics boards (Radeon from AMD, Arc from Intel, and GeForce from Nvidia). All 
those products can perform GPU-compute functions in conjunction with program-
ming languages such as Open CL (from the open standards organization Khronos), 
DirectCompute (from Microsoft in Windows 7 and subsequent operating systems), 
and CUDA (a proprietary programming environment and family of languages from 
Nvidia). Therefore, the precise measurement of AIBs used for GPU-compute is not 
knowable, and only AMD’s, Intel’s, and Nvidia’s speciﬁcally branded GPU-compute 
products can be counted. The net result of this market segment is an increase in AIB 
sales and that some systems, such as servers, supercomputers, and custom-built 
computers, have multiple AIBs with the CPU(s). 
3.8 FLOPS Versus Fraps: Cars and GPUs 
GPUs’ performance, programmability, size, and power consumption have improved 
and gone up in price since the introduction in the late 1990s. 
Fraps is a Windows benchmarking software tool that reported what the frames 
per second (fps) were in real time. It also reported average fps. Fps is a common 
measurement of performance for applications.

3.8 FLOPS Versus Fraps: Cars and GPUs
97
Fig. 3.29 Comparison of GFLOPS of GPUs over time 
Computer performance gets measured in several ways, as does computer graphics 
performance. No one way is the best, or more correct, and it is a matter of what is 
important to the user. Mainly due to Moore’s law, performance advancement has been 
evident on all platforms. Figure 3.29 shows the comparison between the dGPUs in 
a PC, and iGPUs in SoCs in game consoles and mobile devices, and shows the 
comparison of PCs, consoles, and mobile. 
The notion of smaller platforms (e.g., mobile devices) or integrated graphics (e.g., 
CPU with GPU) catching up to desktop PC GPUs is absurd—Moore’s law works 
for all silicon devices. Intel’s best integrated GPU in 2021 could produce 1152 
GFLOPS—a Giga (billion) ﬂoating-point operations-per-second, almost equivalent 
to a 2010 desktop PC discrete GPU (i.e., 1300 GFLOPS). 
Shaders are the other measures of a GPU’s power; mobile devices have 16 to 64, 
consoles have 512 to 384, and desktop PCs have up to 3800—more is better. 
Clever techniques such as tessellation, deferred rendering, and texture compres-
sion have added to the performance of GPUs by allowing them to deliver more frames 
per second while using less power. 
The quest for higher performance while staying within power budgets continues, 
and even if Moore’s law does slow down, the improvement of GPUs will not.

98
3
The Major GPU Eras
GFLOPS is as good a measurement as any and can work across different platforms 
for comparison. The real point of this chart is the roll-off performance gains over 
time. As many have said, Moore’s law is slowing down. That is true, and what it 
reveals is that we will see new, clever, innovative ways to squeeze more performance 
out of our nanometer-sized transistors—architectural tricks with caches, multi-chips, 
memory, and most of all software. 
3.8.1 
Why Good Enough is Not 
In 2000, IGCs were evolving into IGPs thanks to Moore’s law. The criticism leveled 
at them was brushed aside by their proponents, most notably Intel. Intel’s repeated 
failures in producing a competitive graphics processor forced the company to take a 
dismissive position toward GPUs. When criticized for the poor game performance 
of an IGC, which really was not a fair criticism, Intel said their IGCs were good 
enough. That was true for almost all non-game applications and even many casual 
and role-playing games (RPG). But it wasn’t good enough for a ﬁrst-person shooter-
type game. The IGC and the subsequent IGP just were not good enough, and they 
never will be. They can’t be. 
An AIB has three interfaces on it, as depicted in Fig. 3.30: the PCI connector at the 
bottom (where the AIB plugs into the motherboard), the video output connectors on 
the back panels (which are DisplayPort and HDMI), and those on the board between 
the GPU and the high-speed graphics memory (known as GDDR). 
Fig. 3.30 Simpliﬁed AIB 
block diagram

3.8 FLOPS Versus Fraps: Cars and GPUs
99
Fig. 3.31 Simpliﬁed 
integrated GPU block 
diagram 
Graphics performance and image quality are highly dependent on memory. The 
memory had to be fast and tightly coupled to the GPU physically and electrically for 
maximum performance. 
An integrated GPU (Fig. 3.31) must share the system memory with the CPU in 
what is known as a Uniﬁed Memory Architecture (UMA). The GPU also shares the 
PCIe bus with the CPU. The system memory is slower than the graphics memory. 
And because the GPU must share resources, it is limited. So limited system resources 
and slower memory restrict the IGP. 
Also, because the CPU’s package can only handle a certain amount of heat, the 
IGP must run slower than a discrete GPU (dGPU)yy on an AIB. 
IGPs are also limited to the number of processors (shaders) they can have. An 
AIB’s GPU will have thousands of shaders, whereas an IGP may have dozens. 
So limited processing elements (shaders), limited and shared resources, slower 
memory, and restricted clock speed all contribute to the limitation of the performance 
of an IGP when compared with an AIB. Refer to the chart given in Fig. 3.29. Compar-
ison of GFLOPS of GPUs over time, and you will see that an IGP is 6–7 years behind 
an AIB and will never catch up; in fact, they can only fall behind. 
The FLOPS (ﬂoating-point operations-per-second) rating of a processor, espe-
cially a GPU, is not an absolute performance indicator; however, it is useful as a 
metaphor for a car. 
FLOPS is what horsepower (HP) is to a car, and frames per second (FRAPS or 
FPS) is what 0–60 miles per hour elapsed time is to a car. So, automobile A with a 
400 HP engine may achieve 0–60 in 5 s, whereas automobile B with a 400 HP engine 
can do 0–60 in 3.5 s. Similarly, GPU A with 5,000 FLOPS can get 40 FPS, whereas 
GPU B with 5,000 FLOPS can get 60 FPS because of differences in architecture, 
memory controller, and API driver.

100
3
The Major GPU Eras
Within the engine, there is the compression ratio equivalent to the memory band-
width in a GPU; both have a signiﬁcant impact on the work output of the engine or 
device. The efﬁciency and number of valves in an engine are equivalent to the caches 
in a GPU. And maybe you could make the analogy of cylinders to shaders. 
The API is the equivalent of the automobile’s ﬂywheel that connects the engine 
to the transmission and regulates the ﬂow, whereas the automobile’s transmission is 
equivalent to the software driver of a GPU. To complete the analogy, the computer’s 
display is to the user what the road is to the tires—where the rubber meets the road. 
The software driver is one of the major (some might say few) places a GPU 
supplier can differentiate. Until recently, all GPUs were made at the same fab and 
used the same silicon mix; they were constrained by the same OS and API, even 
bounded by the same power supply and cooling system. And yet, two GPUs from 
two different companies or two different generations could have the same FLOPS 
capability, and one would outperform the other. And, just as ratings can be capricious, 
your mileage may vary. 
3.9 Conclusion 
In the following chapters, we will dig into the GPU, what makes it work, who made 
it work, and where they made it work—in PCs, game consoles, mobile devices, 
supercomputers, and automobiles. 
The processors made by AMD, Intel, and Nvidia, the leading dGPU providers, 
have lined up with the eras described above. AMD produced a graphic (Fig. 3.32) that 
showed their synchronization [43]. It differs slightly from ours in that they combine 
the 5th and 6th into their 5th. 
The introduction of GPUs during the eras are shown in Fig. 3.2 and here in 
Fig. 3.33.
Fig. 3.32 AMD’s view of the eras of the GPU. Reproduced with permission by AMD 

References
101
Fig. 3.33 GPU architectural introductions through the eras 
Although we end this section with era six, that won’t be the end of the GPU’s 
evolution. 
References 
1. Ming C., Lin, M. C., and Manocha, D. Interactive Geometric and Scientiﬁc Computations 
Using Graphics Hardware Siggraph’03 Tutuorial Course #11. (August 2003), http://gamma. 
cs.unc.edu/SIG03_COURSE/SIGCOURSE-11.pdf 
2. RenderMan Interface Speciﬁcation, https://en.wikipedia.org/wiki/RenderMan_Interface_Spe 
cification 
3. Cook, R. L., Carpenter, L., Catmull, E. The Reyes Image Rendering Architecture, Computer 
Graphics, Volume 21, Number 4, (July 1987), https://tinyurl.com/bddues5z 
4. Seymour, M. Pixar’s RenderMan turns 25, FX Guide (July 25, 2013), https://www.fxguide. 
com/fxfeatured/pixars-renderman-turns-25/ 
5. The Shading Process: An Overview, RenderMan, https://renderman.pixar.com/resources/Ren 
derMan_20/shadingProcessOverview.html 
6. RenderMan 2.0—The Shading Process: An Overview, https://renderman.pixar.com/resources/ 
RenderMan_20/shadingProcessOverview.html 
7. Wilhelmsen, P. Unity 5 Shader Programming #1: An introduction to shaders, (September 
2, 2015), https://digitalerr0r.net/2015/09/02/unity-5-shader-programming-1-an-introduction-
to-shaders/ 
8. Leavitt, N. An Overview of Key Technologies Rolled Out Since 1946, IEEE Computer 
Society, https://www.computer.org/publications/tech-news/neal-notes/an-overview-of-key-tec 
hnologies-rolled-out-since-1946 
9. https://twitter.com/Rajaontheedge/status/1350567193201631234, (Jan 16, 202). 
10. Villar, J. R. OpenGL Shading Language, Course Chapter 3 — Basic Shaders, https://www.ope 
ngl.org/sdk/docs/tutorials/TyphoonLabs/Chapter_3.pdf 
11. GLSL Tutorial – Vertex Shader. (June 9, 2011), https://www.lighthouse3d.com/tutorials/glsl-
tutorial/vertex-shader/ 
12. Amdahl, G. M. Validity of the Single Processor Approach to Achieving Large-Scale Computing 
Capabilities, AFIPS Conference Proceedings (30): 483–485, (1967), https://doi.org/10.1145/ 
1465482.1465560

102
3
The Major GPU Eras
13. Gustafson, J. L. Reevaluating Amdahl’s Law, Communications of the ACM. (May 1988), 
https://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.509.6892 
14. Baumann, D. ATI Xenos: Xbox 360 Graphics Demystiﬁed, Beyond3D, (13th Jun 2005), https:// 
www.beyond3d.com/content/articles/4/ 
15. Nvidia GeForce 8800 GPU Architecture Overview (November 2006) https://www.nvidia.co. 
uk/content/PDF/Geforce_8800/GeForce_8800_GPU_Architecture_Technical_Brief.pdf 
16. Common-Shader Core, Microsoft, (May 31, 2018), https://docs.microsoft.com/en-us/windows/ 
win32/direct3dhlsl/dx-graphics-hlsl-common-core?redirectedfrom=MSDN 
17. Barczak, J. Why Geometry Shaders Are Slow (Unless you’re Intel), The Burning Basis Vector 
Uncivilized Computer Graphics Musings, (March 18, 2015), http://www.joshbarczak.com/ 
blog/?p=667 
18. https://en.wikipedia.org/wiki/Convex_hull 
19. https://docs.microsoft.com/en-us/windows/win32/direct3d11/direct3d-11-advanced-stages-
tessellation 
20. Tessellation Patterns - From Mathematics to Art, Widewalls Editorial, https://www.widewalls. 
ch/magazine/tessellation-mathematics-method-art, (July 10, 2016). 
21. Tessellation. https://en.wikipedia.org/wiki/Tessellation 
22. Catmull, E.; Clark, J., Recursively generated B-spline surfaces on arbitrary topological meshes, 
(PDF). Computer-Aided Design. 10. (1978). 
23. Rákos, D. History of hardware tessellation, (2010 September), https://rastergrid.com/blog/ 
2010/09/history-of-hardware-tessellation/ 
24. Convex hull, https://en.wikipedia.org/wiki/Convex_hull 
25. Appel, A. (1968)Sometechniquesforshadingmachinerenderingsofsolids, AFIPS ’68 (Spring) 
Proceedings of the April 30--May 2, 1968, spring joint computer conference, Pages 37–45, 
Atlantic City, New Jersey, April 30 - May 02, 1968 
26. Peddie, J. (2019) Ray Tracing: A Tool for all, Springer Link, https://link.springer.com/book/ 
https://doi.org/10.1007/978-3-030-17490-3 
27. Whitted, T. An Improved Illumination Model for Shaded Display, Communications of the ACM, 
Vol. 23, No. 6, June 1980, pp. 34-~9, https://doi.org/10.1145/358876.358882 
28. Peddie, J. Nvidia piles memory on GPU just like Volta piled zinc on copper, TechWatch, (March 
22, 2013) 
29. I. Nenson, Oigtlcks, A Tratiee of The Reﬂections, Refraction, inﬂections & Colors of Light, 
4th edn., London, 1730 
30. Bui-Tuong, P. Illumination, for Computer-Generated Pictures, Comm. ACM 18(6) ((975),311 
317. 
31. Whitted, T. An Improved illumination Model for Shaded Display, Comm. ACM 23(6)(1980), 
343–349. 
32. Peddie, Jon, Ray tracing: A tool for all, Springer-Nature, Berlin, Germany (2019) 
33. Fuchs, F., et al. Fast Spheres, Shadows, Texture, Transparencies, and Image Enhancements in 
Pixel-Planes, Computer Graphics, (1985) 
34. Inbetweening, https://en.wikipedia.org/wiki/Inbetweening 
35. Edelsten, A. Nvidia DLSS: Your Questions, Answered, (February 15, 2019), https://www.nvi 
dia.com/en-us/geforce/news/nvidia-dlss-your-questions-answered/ 
36. Cozzi, Patrick and Riccio, Christophe, OpenGL Insights, CRC Press, Taylor & Francis Group, 
Baco Raton, London, and New York, and A.K. Peters Book.(2012) 
37. Hou, D. Creative Powerhouses Adopt Nvidia DLSS for AI-Accelerated Performance Boosts, 
(February 23, 2021), https://blogs.nvidia.com/blog/2021/02/23/dlss-ai-creative-performance/ 
38. Hodes, S. Introducing ﬁdelityfx variable shading, (November 2020), https://gpuopen.com/wp-
content/uploads/slides/AMD_RDNA2_DirectX12_Ultimate_FidelityFXVariableShading.pdf 
39. Bhonde, S. Turing Variable Rate Shading in VRWorks, (September 24, 2018), https://develo 
per.nvidia.com/blog/turing-variable-rate-shading-vrworks/ 
40. Kilgariff, E., Moreton, H., Stam, N., and Bell, B. Nvidia Turing Architecture In-Depth, 
(September 14, 2018), https://developer.nvidia.com/blog/nvidia-turing-architecture-in-depth/

References
103
41. Kubisch, C. Introduction to Turing Mesh Shaders, (September 17, 2018), https://developer.nvi 
dia.com/blog/introduction-turing-mesh-shaders/ 
42. Haar, U, Aaltonen, S, GPU-Driven Rendering Pipelines, SIGGRAPH 2015: Advances in Real-
Time Rendering in Games, (August 9, 2015) http://advances.realtimerendering.com/s2015/aal 
tonenhaar_siggraph2015_combined_ﬁnal_footer_220dpi.pdf 
43. Introducing RDNA architecture, (August 2019) https://www.amd.com/system/ﬁles/documents/ 
rdna-whitepaper.pdf

Chapter 4 
The First Era of GPUs 
As laid out earlier in this book, the GPU era begins with the introduction of the ﬁrst 
fully integrated single 3D chip with transform and lighting (T&L). 
Transformation is the task of producing a 2D view of a 3D scene. It involves 
only drawing the parts of the scene that were in the view space after the rendering. 
Lighting is the function of computing the color of each pixel or the amount of light 
reﬂected for different surfaces in the scene [1]. 
The ﬁrst T&L engines were ﬁxed function and would eventually lead to 
programmable same-instruction, multiple data (SIMD) vector processors, or shaders. 
They led to vertex shaders and graphics (rendering) shaders, then to uniﬁed shaders 
and mesh shaders. 
DirectX 7.0 was the ﬁrst consumer API to support hardware T&L. OpenGL 
supported T&L earlier for 3D accelerators designed for workstations for visualiza-
tion, simulation, computer-aided design (CAD), and other professional applications 
instead of games, although OpenGL was used by some game developers (Fig. 4.1).
The PC has two classes of GPU: discrete and integrated. Those classes are archi-
tecturally different but have many similarities. As might be obvious, discrete GPUs 
were able to more quickly take advantage of the technology advances supported by 
DirectX and the Khronos APIs (OpenGL et al.). New features waterfalled down from 
discrete to integrated and the APIs helped smooth the path for developers. Direct X 
and Khronos’ APIs deﬁned the ﬁve eras of the GPUs described in further detail in 
this chapter but the API suppliers did not create them. That role fell to the graphics 
hardware developers. Ironically, as they helped push the technology forward, smaller 
companies fell behind as the race picked up speed. This era also saw the decimation 
of the graphics hardware developer ranks. 
VLSI had empowered the graphics chip companies to take T&L operations away 
from the CPU, which upended the trajectory of the CPU, where had heretofore 
been doing everything. Here now was a coprocessor that would rival the CPU for 
computational dominance, a contest that would be fought for decades afterward. And 
the incorporation of T&L was not relegated to just the big fabless semiconductor 
companies in the U.S., although they dominated it.
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2022 
J. Peddie, The History of the GPU – Eras and Environment, 
https://doi.org/10.1007/978-3-031-13581-1_4 
105

106
4
The First Era of GPUs
Fig. 4.1 The rise and fall of PC graphics chip suppliers versus market growth
Fig. 4.2 Block diagram of a PC 
A PC’s architecture at the turn of the century consisted of the CPU and one or two 
supporting chipsets that provided the communications between the CPU and main 
memory; storage memory; peripherals such as keyboard, mouse, communications 
(Internet, Ethernet, etc.); and audio, video, and the display, as illustrated in Fig. 4.2. 
The overall design concept has not changed and is very similar to a smartphone or 
game console. The independence of the CPU (or external geometry engine) ﬂoating-
point processor of a T&L was integrated not only into the discrete graphics controller 
but also the integrated graphics controller (IGC), turning it into an integrated graphics 
processor (IGP).

4.1 The Golden Age—Transform, and Lighting Changes the Industry …
107
4.1 The Golden Age—Transform, and Lighting Changes 
the Industry (1999–2001) 
With the introduction of the GPU, the graphics chip competitors gradually got 
squeezed out of the market. Most of the AIB suppliers also failed. Only ATI and 
Nvidia would survive but there were plenty of contenders along the way. 
Intel watched the encroachment of GPUs into its territory with a certain amount 
of outrage. The company had been on the defensive building barricades around its 
position that the CPU could handle graphics tasks, especially geometry. When it 
recognized it could not hold its position, Intel announced in 1998 it would build a 
better, cheaper graphics processor, the i740. 
Meanwhile SiS/XGI, which is proﬁled in more detail in Book three, had also 
announced a more economical graphics processor, the Xaber. 3Dfx had been 
insisting, it’s competitive chip was just around the corner, and managed to keep 
devoted gamer fans holding off on buying other products for a while. 
Intel’s attempt to enter the GPU/AIB market with the i740 failed, and the company 
withdrew it. SiS/XGI’s effort with Xaber failed, and the company abandoned the 
market, and SiS went back to IGPs. 3Dfx, once the darling of the gamers, ﬁnally 
admitted it couldn’t leap the technology and ﬁnancial hurdles required to keep up 
and ended in bankruptcy [2]. Matrox never developed T&L with its MGA chip and 
managed to survive in niche markets until 2014, when it dropped its successful 
long-term MGA chip and adopted AMD’s GPU. 
The furious competition between ATI/AMD and Nvidia drove the companies 
further ahead as they exploited Moore’s law and introduced one great graphics feature 
after another for over two decades. 
Things would change in 2019 when new companies began entering the GPU 
market. They were attracted by the success AMD and Nvidia were enjoying from 
the gaming market, the high-end AI training market, and specialized markets like 
game consoles and the automotive industry. AMD also offered IP to SoC builders 
like Samsung. GPU companies like Jingjia Micro and Innosilicon (discussed in Book 
three) sprouted in China, with massive funding directed by the government as part 
of China’s plan to become technologically independent. 
Somewhat in parallel with the product and feature introductions made by 
ATI/AMD and Nvidia, Intel was making its own GPUs. Not all its attempts were 
successful. For instance, the i740 was released but failed and Larrabee never got out 
of the lab. Nevertheless, they were noteworthy. Intel kept investing in and improving 
the graphics capabilities of its CPU-embedded GPU, referred to as an iGPU. AMD 
also developed CPUs with embedded GPUs (iGPUs), which it markets as an Accel-
erated Processing Units (APUs). There is more detail on Intel’s GPU work (also in 
Book three) including its reentry into discrete GPUs with Xe architecture. 
ATI/AMD and Nvidia continued their staggered launches. An example is the 
evolution from using VRAM and DDR memory to Graphics DDR (GDDR) and 
high-bandwidth memory (HBM).

108
4
The First Era of GPUs
The ﬁrst generation of T&L engines had a basic 32-bit single-precision, ﬂoating-
point (SPFP) multiply-accumulate hardware processor. Lighting required various 
scalar and dot products. 
Further, 8-bit to 12-bit limited precision was required, including transcendental 
functions. 
Transform and lighting went from being a dedicated function in the pipeline to 
being an algorithm that could be run on any available shader in a GPU when GPUs 
went to uniﬁed shaders and then mesh shaders (discussed in future chapters). 
4.1.1 
On Being First 
This book focuses on the ﬁrst discrete GPU in a PC. There is also the type of graphics 
device (integrated or discrete) and the platform. 
SGI was the ﬁrst iGPU in a game console in 1996 (Nintendo 64). 
Nvidia was the ﬁrst discrete PC GPU (NV10 GeForce 256) in October 1999, ATI 
was second (Radeon 2000). 
ArtX was the ﬁrst iGPU in a PC in November 1999 (Aladdin 7). 
4.2 First Era Discrete PC-Based GPUs 
Nvidia is credited with introducing the ﬁrst single chip GPU for the PC with the 
Nvidia NV10 (Celsius) GeForce 256 in October 1999. It proved to be a signiﬁcant 
turning point in the industry—but one that would take a few years to play out. 
Eight companies tried to make an integrated GPU; four of them succeeded and 
produced ﬁve examples. 
3Dlabs, ATI, Bitboys, Nvidia, PixelFusion, S3, SiS, and Tseng and others raced 
to produce a GPU for the graphics market. ATI, Nvidia, S3, and SIS succeeded, but 
only ATI and Nvidia won enough customers to sustain themselves and ultimately 
owned the whole market (Table 4.1).
The table uses the following terms:
●Process: The minimum feature size in nanometers (billionths of a meter) for the 
semiconductor process fabricating each microchip.
●Transistors: An approximate measure, in millions, of the chip design and 
manufacturing complexity.
●Anti-aliasing ﬁll rate: A measurement in millions of 32-bit RGBA pixels per 
second, assuming two-sample anti-aliasing.
●Polygon rate: The GPU’s triangle drawing speed, measured in millions of triangles 
a second.

4.2 First Era Discrete PC-Based GPUs
109
Table 4.1 First-generation GPUs 
Era or 
generation 
Year
Product 
name 
Arch
Process 
(nm) 
Transistors 
(millions) 
Anti-aliasing 
ﬁll rate 
Polygon 
rate 
Pre
August 
1999 
ATI
Rage 
4 
250
8 
Pre
Early 1999 Nvidia, 
RIVA TNT2 
NV6
220
9
75 M
9 M  
First
September 
1999 
Nvidia, 
GeForce 
256 
NV10 
220
23
120 M
15 M 
Post
April 2000 
ATI
Rage 
6 
180
30 
Post
June 2000
Nvidia, 
GeForce2 
NV15 
180
25
200 M
25 M 
Post
November 
1999 
S3, 
Savage2000 
180
12
500 
Post
December 
2000 
SIS
315
150
332
4 M
4.2.1 
Glaze3D Bitboys 2.0 (1999–2001) 
In August 1999, 2 months before Nvidia announced and showed its GPU, the Finnish 
developers Bitboys announced it was working on a new 3D controller, codename 
Anvil. The name was changed because some of the team disliked it. After a long 
company meeting, they changed the name. The story is the debate turned to food and 
to donuts and that’s how they arrived at Glaze3D [3]. Afterward, some of the product 
demos contained different kinds of donuts. 
From Bitboys’ point of view, their original chip done with TriTech (discussed in 
Book one) was killed by growing “featuritis” that was turning it into something like 
the unsuccessful Mpact chip from Chromatic. As outlined in Book one, Chromatic’s 
Mpact was a full-featured video chip/VGA/3D chip that the market had no idea what 
to do with at the time. The Mpact failed to ﬁnd an audience because it costs too 
much, and few applications could utilize its capabilities. The writing on the wall was 
clear, and TriTech’s executives closed the project. But Petri Nordlund, Kaj Tuomi, 
and Mika Tuomi were determined to salvage their 3D part of the design [4]. For legal 
reasons, Glaze3D did not share any code with the Bitboys’ earlier design, Pyramid3D. 
Bitboys started working on it well in advance of TriTech’s problems as it saw that 
the product was not going anywhere. Pyramid3D algorithms and architecture were 
also outdated at that point. 
Glaze3D, their new-generation quad-pipeline controller promised a 400 Mpix/s 
dual-textured rendering rate. It was being developed as a licensable, scalable core. 
The company was also looking for a silicon partner.

110
4
The First Era of GPUs
The 2D/3D core was a synthesizable model of a graphics rendering pipeline to be 
integrated with a bus and memory interfaces to form a complete graphics accelerator 
chip aimed at the consumer market. Following were some highlights of the core:
●Scalable architecture.
●Balanced architecture.
●2D and 3D acceleration, a single chip solution.
●A true color-rendering pipeline.
●True color framebuffer.
●Trilinear mipmapping.
●A programmable triangle setup engine.
●Four simultaneous trilinear-ﬁltered textures in a single pass.
●No performance penalty on translucent surfaces.
●Bilinear bump mapping, also with a surface texture.
●Anti-aliasing. 
According to the designers, the architecture allowed the incorporation of 
numerous advanced features without compromising speed. Diffuse and specular 
shading, fogging with vertex or fog table, edge anti-aliasing, alpha blending, and 
z-buffering could be enabled in all modes without any performance penalty, as shown 
in the following list:
●400 Mpix/s Gouraud shaded pixels, no texture.
●400 Mpix/s single texture with trilinear mipmapping.
●400 Mpix/s dual texture with bilinear ﬁltering.
●200 Mpix/s dual texture with trilinear mipmapping.
●200 Mpix/s quad texture with bilinear ﬁltering.
●200 Mpix/s bump texture + surface texture.
●100 Mpix/s quad texture with trilinear mipmapping. 
Bitboys claimed the Glaze3D would render translucent surfaces with 400 million 
dual-textured, alpha blended pixels per second. A great deal of the pixel ﬁll rate 
desired for games came from translucent surfaces such as smoke, lighting effects, 
clouds, and explosions, and, according to Bitboys, the Glaze3D could easily double 
or triple a title’s pixel ﬁll rate requirements. 
The design boasted of an integrated programmable ﬂoating-point triangle setup 
engine. It was said to be able to handle a constant feed of three-million fully featured 
triangles per second making it a capable GPU—if it ever got built. 
The team tuned the performance of the Glaze3D design on their behavioral delay 
model simulator of the core architecture. They used test data from games such as 
id Software’s Quake and Quake II and benchmarks like the Viewperf so they could 
test with real-world application data. The delay model gave them cycle-accurate 
information about the rendering time and possible bottlenecks in the architecture. 
Bitboys designed Glaze3D to be compatible with the DirectX and OpenGL 
APIs. The architecture supported perspective-correct diffuse and specular shading, 
perspective-correct per-pixel fogging, fog table, edge anti-aliasing, stencil opera-
tions, z-buffering, and all alpha blending modes.

4.2 First Era Discrete PC-Based GPUs
111
The Glaze3D had a 32-bit true color-rendering pipeline. It provided a true color 
frame buffer and 32-bit ﬂoating-point z-buffer, eliminating the artifacts common in 
16-bit rendering architectures. 
The chip did four simultaneous textures in a single rendering pass with trilinear 
mipmapping. Most architectures then only offered one or two textures in a single 
pass, forcing game developers to use multiple rendering passes, increasing geometry 
processing requirements, and decreasing rendering quality. 
Glaze3D also included the bump mapping technique that Microsoft licensed from 
TriTech and put in Direct3D 6.0. The bilinearly interpolated bump map could be 
combined with a surface texture supported in hardware in a single rendering pass. 
Bitboys claimed its design was scalable from 200 to 800 million rendered pixels 
per second. The speed of the triangle setup engine was scalable from 3 million 
triangles upward, and a geometry processor design was also available for the core to 
either supplement the triangle setup engine or replace it. 
The memory interface of the Glaze3D was ﬂexible like that of the Pryamind3D, 
allowing the use of different types of memory based on the bandwidth requirement 
of the Glaze3D conﬁguration. Using Rambus Direct RDRAM, they achieved the 
6.4 GB/s bandwidth needed for 400 million pixels per second true color rendering. 
The memory interface could utilize SDRAM, SGRAM, RDRAM, and SLDRAM 
memory types, among others. 
Learning from the past experiences, and with the help of its PCIBuilder simulator, 
Bitboys said it had taken its design further than the earlier project. What it had in 
1999 was at a much more complete stage compared with how the team developed 
Pyramid3D. 
In August 1999, Bitboys was looking for a fab partner. Bitboys said its core 
development was complete. Thanks again to its PCIBuilder and the in-house abilities 
on the driver side, it would be possible to start work quickly after selecting a partner, 
and the silicon design started. It was also clear that the product had to get out quickly 
to succeed. However, with the projected performance, it thought it could beat the 
market for some time. And with the scalable internal architecture, it could quickly 
respond to performance changes in the market. Realistically, the process would take 
anywhere from 6 months to a year. 
In the August 1999 incarnation, Bitboys employed seven people working on hard-
ware and software products. At the time, the Glaze3D design appeared to be as much 
as four times faster than consumer-level 3D graphics chips available then, and it was 
scalable. The fastest chips of the time could render 90 million dual-textured pixels 
per second. Dual-texture pixel ﬁll rates have doubled every 18 months. With a 400 
Mpix/s dual-texture ﬁll rate, the Glaze3D would be a formidable competitor in 2000 
(Fig. 4.3).
By October 1999, 2 months after Nvidia announced its GPU, the Glaze3D design 
was complete and ready for tape out. Bitboys also found a fab partner: Inﬁneon, the 
former Siemens Munich fab spun out in March 1999. Inﬁneon said it would make 
the chip for Bitboys. It was looking like it might become a reality.

112
4
The First Era of GPUs
Fig. 4.3 The Bitboys: Petri Nordland, Mika Tuomi, and Kai Tuomi (1999)
4.2.1.1 
Inﬁneon 
Bitboys’ design used embedded DRAM, ﬂying in the face of those who claimed to 
embed DRAM was too difﬁcult to be practical for desktop designs. Some claimed 
it was a technology better left to notebook design. Obviously, Inﬁneon, which had 
perfected a method of embedding DRAM in chips, disagreed, and the company was 
interested in attracting customers who would showcase its process. 
Petri Nordlund, Bitboys’ Chief Architect, formally presented the Glaze3D design 
at the prestigious SIGGRAPH Eurographics event in Los Angeles, California, in 
August 2001 (Fig. 4.4) [5].
Bitboys was a good ﬁt for Inﬁneon. The Glaze3D chip had 72 Mbits of embedded 
frame buffer memory and a 512-bit memory interface. Embedded memory provided 
efﬁciencies for graphic processing that Bitboys was eager to exploit. It would use 
Inﬁneon’s 220 nm technology, bucking the trend toward millions and millions of 
transistors on a chip. For example, on hearing the description of Nvidia’s latest design, 
the GeForce, which had 23 million transistors, Petri asked, “What do they need all 
those transistors for?” Heresy? Maybe, but Bitboys claimed it could build chips that 
did not have the larger chips’ heat problems and could provide the performance to 
beat the game chips currently on the market. 
There were several paths Bitboys could have taken—some were treacherous, and 
others were uninspiring. The trio decided on the most challenging path available: 
they decided to build their chip and hoped to ﬁnd a partner or customer to take it 
to market. Despite the rapidly shrinking graphics market, the Bitboys team believed 
several possibilities were open because their chip could work in arcade machines,

4.2 First Era Discrete PC-Based GPUs
113
Fig. 4.4 Bitboys’ Glaze3D block diagram with triangle setup engine
desktop graphics, and even consoles. The pipeline of the Glaze3D design is shown 
in Fig. 4.5.
If all went well, Bitboys would have had to change. Either the company had to 
grow and add another design team to meet the unrelenting demands of the computer 
market, or it could have become another company’s design team. Mika, Kaj, and 
Petri believed then that their plans for scaling Glaze3D gave them a little bit of time 
before they would have to decide, but they knew they had difﬁcult choices ahead. 
They weren’t sure they could work within a larger company, company in Finland, 
but ﬁnding help was not easy up there in the ice. Not that the talent was not there, but 
like their friends from the demo days, much of the high-tech talent in Finland was 
self-taught and freewheeling. They were often designers and engineers who could 
make money and live well taking contract jobs, and they did not want to work for 
anyone but themselves. 
The Bitboys people themselves had a little bit of that tendency. Could they work 
as a design team in a larger company? Mika opened his hands wide and said quite 
honestly, “I do not know.” They were young guys who would already accomplish a 
great deal beyond most people their age.

114
4
The First Era of GPUs
Fig. 4.5 Bitboys’ Glaze3D pixel pipeline
Fueled by their youthful energy and conﬁdence that kept them up all night plugging 
away in a tricky business, their enthusiasm was tempered by the disappointment of 
their experience with Pyramid 3D. Mika, Kai, and Petri recognized success might 
mean they would have to give up some of their freedom. At the same time, they were 
entirely driven by the desire to see their designs come to life. They were willing to 
sacriﬁce for that. Indeed, the demo days were over, and it was time for real life to 
begin (Fig. 4.6).
With Glaze3D, Bitboys introduced its Xtreme Bandwidth Architecture—XBA. 
Those were the heady days when novel architectures were being explored by compa-
nies like PixelFusion, Raycer, and others. VC money was starting to ﬂow, and Inﬁneon 
put some Dmarks into Bitboys to the tune of $2 million U.S. By then, Inﬁneon had 
moved to 170 nm. 
But things did not go smoothly for the boys, and they missed a couple of deadlines. 
To make up for it, Bitboys announced its next-generation part: the Axe. 
4.2.1.2 
Bitboys Gets the Axe 
Glaze3D never got into production, and Bitboys did a redesign (with the codename 
Axe) to take advantage of DirectX 8 and stay current with the competition. Bitboys’ 
new version had an additional 3 MB of embedded DRAM (eDRAM), proprietary

4.2 First Era Discrete PC-Based GPUs
115
Fig. 4.6 Bitboys’Glaze3D prototype circa 1999. Source Petri Nordlund
matrix anti-aliasing, and a signiﬁcantly improved ﬁll rate. It also incorporated a 
programmable vertex shader and a wider internal memory bus. They planned to 
release the AIB as Avalanche3D by the end of 2001. 
The main problem with Inﬁneon’s eDRAM process was that while the main 
eDRAM memory was small, the logic and especially static RAM (SRAM) took 
quite a lot of die space. Inﬁneon was using a process designed for manufacturing 
memory, not logic. At one point in time, it faced a challenge to shrink the size of the 
chip by 50% in 2 weeks to keep the project ongoing. Remarkably, it managed to do 
so—but it was a constant challenge to keep the die size at something reasonable to 
sell at market. 
4.2.1.3 
Bitboys Gets Hammered 
At that point, Bitboys had heard from Inﬁneon that it would shut down the eDRAM 
manufacturing line, so there was no way to continue with the earlier architecture. A 
third design, code named Hammer, was started as Axe lost viability toward the end 
of 2001. Hammer had a more traditional architecture with just external memory, and 
Bitboys invested quite a bit in algorithms to reduce memory bandwidth, including 
delay streams, which it presented at Siggraph 2003. 
The new AIB would be a high-end DirectX 9 board with improved rendering 
performance, occlusion culling, and other innovations. However, like those before 
it, it never went into production.

116
4
The First Era of GPUs
4.2.1.4 
The Final Blow 
And then, in 2001, Inﬁneon decided it did not want to be in the embedded memory 
business any longer—which came as a terrible blow to Bitboys, and other customers 
like NeoMagic, MediaQ, and others. After it spun off from Siemens in 1999, Ulrich 
Schumacher, Inﬁneon’s CEO, said he believed one of Inﬁneon’s major achievements 
in 2001 was simply surviving [6]. Right after that, the Bitboys founders were as 
depressed as the inﬁnitely optimistic glass-is-one-tenth-full guys ever got. 
And then, Bitboys hooked up with LSI, and things looked like they might get 
back on track. Their team managed to pull in $10 million more in investments, but 
even that did not help, and in 2002, Bitboys closed its U.S. operations. 
But they never really stopped trying, and word leaked out of Finland that the boys 
would build a handheld device, joining MediaQ, NeoMagic, Epson, and ATI. And 
so, we move on to Bitboys 3.0 in the “Mobile GPUs” in Book three. 
4.2.2 
S3 Savage 2000 (November 1999) 
From VGA to GPU-compute in China 
Right on the heels of Nvidia, and 4 months before ATI’s ﬁrst GPU, S3 announced 
its Savage 2000 GPU in November 1999. Founded in 1989, S3 was best known for 
its popular S3 Virge 86C385 VGA graphics controller. The Savage 2000 was to be 
S3’s answer to Nvidia’s GeForce 256. 
Earlier, in June, S3 announced it intended to buy AIB builder Diamond Multi-
media. S3’s CEO, Kenneth Potashner, said, “the PC market requires a vertically 
integrated business that offers both chips and the add-on cards that use them.” S3’s 
competitors, such as ATI, Matrox, 3Dfx/STB, and had already chosen that business 
model. 
“OEMs want one-stop shopping for graphics chips, their software drivers, and 
add-on cards themselves,” said Bob McQuillan, add-in board analyst at Jon Peddie 
Associates. “It eliminates the middleman,” he added [7]. 
S3 introduced the chip on its new Diamond Viper II AIB. The S3 Savage 2000 GPU 
was built on 180 nm and was supposed to offer on-chip T&L (S3TL), hardware texture 
compression (S3TC), single-pass quad-texture blending, and hardware-assisted DVD 
playback through motion compensation and 16-tap upscaling/downscaling. On paper 
it was supposed to double the bandwidth of the Savage4 and quadruple its processing 
texture. In reality it ran at 125 MHz and the memory at 155 MHz, 25% lower than 
the anticipated 200 MHz expected. 
Savage 2000 was a simple chip and had half as many transistors as Nvidia’s 
GeForce 256 (13 million). The company said it was fabricated on a 180 nm 
manufacturing process, the Nvidia GeForce256 used 220 nm. 
All of that bothered Nvidia who sued S3 in December 1999, claiming S3 had 
breached ﬁve patents. Strangely which patents was never revealed to the public, but

4.2 First Era Discrete PC-Based GPUs
117
Fig. 4.7 S3 Savage 2000 
AIB with T&L. Source 
Swaaye-Wikipedia 
it was assumed they had to do with T&L techniques. S3 denounced Nvidia’s legal 
aggressiveness. Nvidia had made similar legal attacks against 3dfx and speculation 
was it was a war of attrition and Nvidia being the stronger company would wear 
down S3 as it did with 3dfx. The companies settled out of court. 
In the meantime, Diamond Multimedia built an AIB using the Savage 2000 
(Fig. 4.7), but it didn’t do well in the market. 
The reputation of S3 had suffered in the gaming community because the drivers 
it provided were buggy and did not enable all the chip’s features. When the Savage 
2000 came out, it suffered the same results, and the T&L did not work. Those kinds 
of problems in addition to several business mistakes ultimately caused the company 
to fail. 
The irony of the situation was S3’s move to acquire Diamond was seen as a 
reaction to 3dfx’s buyout of STB. But both acquisitions were as bad for the twisted 
same reasons. The best AIBs STB produced used S3 chips (ViRGE and Savage 3D). 
And the best AIBs Diamond made used 3dfx chips, Diamond Monster 3D, Monster 
3D II, and Monster Fusion (Banshee). So the cross-wise deals for all four companies 
were stymied and resulted in killing all four. Hoping to correct the situation, in June 
1999, 3dfx offered to buy or merge with S3, but the deal never got anywhere and 
3dfx went bankrupt. Even Intel was rumored to wanting to buy S3. 
In early 2000, the Company announced plans to purchase AIB builder Number 
Nine. That seemed a bit of a mystery until one considered that S3 had cast a covetous 
eye at the arrangement Number Nine had to provide IBM with AIBs, Number Nine’s 
only real customer for its Savage 4-based SR9 board. The week before the announce-
ment, rumors were ﬂying that S3 would pull out of the Number Nine deal, leaving 
the longsuffering board company with nothing to sell to IBM in the future. Instead, 
Potashner said, “we did the right thing.” To protect S3 from Number Nine’s debts, 
Number Nine announced bankruptcy as part of the deal to help ensure S3 got IBM’s 
business. 
So S3 added struggling board company Number Nine to its corral with another 
struggling board company in Diamond. That was part of Potashner’s campaign to

118
4
The First Era of GPUs
make S3 more appealing to investors. S3 had been stuck in the ugliest part of the 
business: low- to mid-range graphics chips. Companies in those segments struggled 
to make a product and then were forced to sell at the lowest possible price, with 
ATI always ready to come in at a lower price if possible. ATI itself had some tough 
challenges holding its position as a low-end leader in terms of keeping up with 
technology innovation and making a proﬁt at the same time. S3’s strategy was to 
broaden its product range and change its image in the market as more than a graphics 
vendor. That was a play to investors, most of whom had become disillusioned with 
the graphics business. 
With offers from 3dfx and Intel not getting any traction, S3 struck a deal with IGC 
supplier Via Technologies (Via). The S3 and Via deal were initially announced in the 
spring of 1999. S3 formed a strategic partnership with Via in Taiwan to develop and 
market a family of integrated graphics chips (IGCs) for Intel and AMD processors. 
“We expect that the joint S3/Via chipsets will complement our existing road map 
of graphics-only solutions,” stated Potashner. 
The ﬁrst S3/Via product, code named SavageNB, combined S3’s Savage4 graphics 
engine with Via’s Apollo Pro core logic design. 
Then, in November, the two companies announced plans to form a partnership 
to produce parts using integrated graphics, and Via took a 2.5% stake in S3 for $14 
million. Soon, Via increased its investment in S3 with an additional purchase of 
shares of S3 stock. As a result of the deal, Via had a 14.9% stake in S3. As the 
company shared executives, it started to become evident the company could have 
too many chiefs, with S3’s Potashner as chairman, Via’s Wen-Chi Chen as CEO, and 
S3’s VP Rick Bergman as general manager. 
By April 2000, S3 and Via were structuring a deal. To avoid antitrust Hart– 
Scott–Rodino Act1 (HSR) issues, the semiconductor graphics group would remain 
a subsidiary of S3, even though Via had the most ownership. However, with Via 
in control of S3’s graphics, reports of buggy drivers and declining sales continued 
into June. Then, in July, the Taiwanese government told the two companies it would 
investigate the deal. Finally, by September, the companies closed their agreement, 
and S3 got some cash from Via. The money would prolong S3’s decline but it did not 
save it. The Taiwanese government was worried about Via’s cash ﬂow because of the 
deal. Therefore, S3 agreed to take less money upfront and let more of the agreement 
be in S3 stock. 
The deal was only for semiconductor operations. The AIB operations that came 
from Diamond stayed with S3/Diamond. That gave S3/Diamond several prod-
ucts: modems, consumer appliances (e.g., Rio, Daytripper/Webpad), Spea high-end 
professional AIBs, Diamond mainstream AIBs, and Number Nine AIBs. 
In November 2000, S3 renamed itself SonicBlue. The Fire GL Professional 
Graphics Division, formerly a division of S3, was sold to ATI in March 2001.
1 The Hart–Scott–Rodino Act established the federal premerger notiﬁcation program, which 
provides the FTC and the Department of Justice with information about large mergers and acqui-
sitions before they occur. The parties to certain proposed transactions must submit premerger 
notiﬁcation to the FTC and DOJ. 

4.2 First Era Discrete PC-Based GPUs
119
In March 2003, SonicBlue put its remaining products up for sale while ﬁling for 
a Chap. 11 bankruptcy reorganization. 
Then on April 11, 2000, VIA announced it had purchased S3’s graphics division 
for 323 million dollars (the ﬁnal price reaching, $377 million). The speculation 
behind the buyout was that VIA didn’t like having to pay for graphics chips and 
bought S3 so it could make its own. Then the Taiwan government stepped and halted 
the deal saying the price was too high. By October 30, the price dropped from $377 
to $288 million with $208 million paid in cash. 
4.2.2.1 
S3 Chrome 
In late 2002, S3 introduced its AlphaChrome series of GPUs and hinted about its 
2003 killer part. 
S3 was one of the leading accelerated VGA graphics chip companies, helped 
usher in the era of the GUI accelerator (with its BitBlt engine), and advanced the 
industry with texture-map compression and color-conversion techniques. By 2003, 
the Company was struggling to regain its status since the scattering of its resources. 
Via carried on and produced a new generation of S3 GPUs, which it named the 
Chrome series. At the Via Technologies 2003 conference, the Company formally 
announced its AlphaChrome strategy (after hinting about it at CeBit and Computex) 
and teased the industry with the Columbia, the next-generation part it was developing. 
The AlphaChrome project never reached its ﬁnish. 
The Company did introduce its DeltaChrome graphics core, claiming it would 
have DirectX 9.0 programmable hardware shaders compatible with Pixel Shader 
version 2.0+ and Vertex Shader 2.0+. DeltaChrome’s V8 pipeline was an 8-pixel 
pipeline, and S3 claimed it had a 2.4 GB/s ﬁll rate. 
However, by the time Chrome was released, the market had moved on. The 
rapid progression of GPU performance from ATI and Nvidia made S3’s offerings 
uncompetitive in the proﬁtable high-end consumer market. 
The Chrome program sputtered and stalled several times, with announced prod-
ucts such as the AlphaChrome and discrete version of the UniChrome never being 
released. Some versions of the Chrome GPU found their way into chipsets, such as 
UniChrome. But basically, Chrome was a modernized version of the underperforming 
S3 ProSavage [8]. 
Via introduced its AcceleRAM technology, which enabled system RAM to supple-
ment the AIB’s memory. It was a feature like ATI’s HyperMemory and TurboCache 
from Nvidia. However, Via could not match the performance of ATI and Nvidia 
GPUs and was excluded from the proﬁtable and rapidly growing PC gamer market. 
But, that did not stop Via from trying. 
With limited resources and a reluctance to spend money, Via struggled to exploit 
its investment in S3 graphics. It tried one scheme after another to ﬁnd a niche or a 
custom base, but nothing seemed to work.

120
4
The First Era of GPUs
In 2004, the Company tried to launch a personal gaming console (PGC) with a 
GPU with 64 Mbytes of high-bandwidth DDR400 SDRAM. Via branded the device 
as the Via Glory PGC Platform. It never amounted to anything. 
In 2006, Via sponsored a competitive gaming team, the Girlz of Destruction, a 
professional all-female gaming team using Via-suppled hardware—another 15 min 
of fame. 
Via said it was planning two new GPUs for mobile devices, Chrome 440 and 430, 
to be released by the end of 2007. 
In 2008 at the Game Developers Conference in San Francisco, Via introduced 
its MultiChrome technology. To introduce it, the Company showed its Chrome 400 
AIBs. To impress the attendees, Via showed a car racing game running on three large 
screens. MultiChrome allowed matched Chrome AIBs to be run together in a system 
to increase graphics performance. It was a feature like ATI CrossFire and Nvidia’s 
SLI. 
4.2.2.2 
Epilogue: The Curious Trail to Zhaoxin 
In 2001, Via sold S3 Graphics to HTC Corporation. When S3 Graphics became 
undercapitalized in 2005, and it engaged a private investment company in which 
Cher Wang, Chairman of Via, was a signiﬁcant shareholder (and the wife of WenChi 
Chen, CEO of Via). Cher Wang also owned HTC. Nothing came out of the attempt 
to raise capital. 
By 2013 Via had given up trying to be a GPU or AIB supplier and instead applied 
those aging and underpowered products to the digital signage market, kiosks, POS 
systems, video walls, dashcam, forklift safety systems, and menu boards. 
Also, that same year, Via Technologies and the Shanghai Alliance Investment 
Ltd. (of the Shanghai Municipal Government) formed Shanghai Zhaoxin Co., Ltd. 
[9] as a fabless semiconductor company. Zhaoxin, afﬁliated with the Shanghai State-
Owned Assets Supervision and Administration Commission (SASAC), held 80.1% 
of the Company, and Via Alliance held 19.9%. Zhaoxin is also known as Via Alliance 
Semiconductor Co., Ltd. 
And in 2020, Via announced it would sell certain intellectual property rights 
related to chip products (excluding patent rights) to Shanghai Zhaoxin [10], and 
Via’s stake was reduced to 14.75%. 
And then the story ends. With a complex set of names, subsidiaries, joint ventures, 
and other Byzantine and baroque arrangements, all information about the ﬁrst 
Chinese graphics GPU stopped. Zhaoxin is discussed in Book three.

4.2 First Era Discrete PC-Based GPUs
121
4.2.3 
ATI and Nvidia: First Era GPUs (1999–2002) 
ATI and Nvidia began introducing one revolutionary feature after another. The power 
of a sea of processors, combined with Moore’s law, made the possibilities seem 
endless. 
The two companies’ ﬁrst-generation GPUs were, not surprisingly, very similar, 
as shown in Table 4.2.
Supporting DirectX 9 meant that the R300 pipeline used ﬂoating point from 
beginning to end, which increased the transistor count of the chip 78% from the 
previous generation. 
The two companies went on a leapfrogging process for the next 20 years and on. 
4.2.4 
ATI Radeon R100—256 (April 2000) 
ATI marked its entry into the GPU class in April 2000 by changing the name of 
its AIBs from Rage to Radeon. With the name change, the new Charisma hardware 
transform clipping and lighting engine was introduced. 
Although the R100 was designed for Direct3D 7.0, ATI incorporated as many 
DirectX 8 features as possible without having all the speciﬁcations. 
The GPU and AIB were called several names by the press and even ATI. It was 
known as the Rage 6, Radeon DDR, the Radeon 256 (as competitive branding to 
Nvidia’s GeForce 256), the Radeon R100, Radeon 7000, and sometimes just Radeon. 
This book will refer to it as the Radeon 100 GPU (abbreviated as R100), used in the 
Radeon 7xx0 AIBs. The R100 was a Direct3D 7/OpenGL 1.3 T&L capable device. 
ATI introduced the R100 (Codename Rage 6 or Piglet) in April 2000. It was 
manufactured in a 180 nm process at TSMC with a die area of 115 mm2 and 30 
million transistors (Fig. 4.8). The R100 featured two-pixel shaders, one vertex shader, 
six texture mapping units, and two render output units or raster operations pipelines 
(ROPS).
The AIB had 32 or 64 MB DDR memory connected via a 128-bit memory 
interface. The GPU and the memory ran at 166 or 183 MHz. 
The Radeon 7000 AIB, as it became known, was a single-slot AGP 4x AIB with 
a VGA output that drew 23 W and did not need an additional power connector. It 
sold for $399 with 64 MB; $279 for the 32 MB DDR version. 
ATI’s HyperZ, part of the R100, was an early culling technology ﬁrst introduced 
by the tile rendering PowerVR chips manufactured by STMicroelectronics. That 
Z-culling buffer optimization technique became popular in the evolution of GPU 
rendering optimization and could be considered the ﬁrst non-title rendering-based 
implementation. It was also compatible with DirectX 7. 
The Charisma engine did more than Nvidia’s NV10 (GeForce 256) T&L engine. In 
addition to transform and lighting calculations, ATI’s T&L engine provided clipping

122
4
The First Era of GPUs
Table 4.2 Early GPUs from ATI and Nvidia 
Nvidia
ATI
Nvidia
ATI
Nvidia
ATI 
Date
1999
2000
2001
2001
2002
2002 
GPU
NV10
R100
NV20
R200
NV25
R300 
Codename
Celsius
Rage
Kelvin
Chaplin
Kelvin
Khan 
AIB
GeForce 256
Radeon 
7200 
GeForce 3
Radeon 
8500 
GeForce4 Ti
Radeon 
9500 
NM
220
180
150
150
150
150/130 
Transistors 
(M) 
17
30
57
60
63
107 
Vertex 
shaders 
4
1
1
2
2
8 
Pixel 
shaders 
4
2
4
4
4
8 
Texture 
mapping 
units 
4
6
8
8
8
8 
Core clock 
(MHz) 
120
183
175
275
250
275 
Fill rate 
(Mpix/s) 
480
366
700
1100
1000
2200 
Fill rate 
(Mtex/s) 
480
1100
1400
2200
2000
2200 
Memory 
clock 
(MHz) 
166
366
400
550
250
250 
Memory 
BW (GB/s) 
4.8
5.9
6.4
8.8
8
8.6 
BU.S.
AGP 4x
AGP 4x
AGP 4x
AGP 4x
AGP 8x
AGP 8x 
API
DirectX 7.0 
Shader 0.5 
OpenGL 1.2 
DirectX 
7.0 
Shader 
0.5 
OpenGL 
1.3 
Direct3D 
8.0 
Shader 1.1 
OpenGL 
1.3 
Direct3D 
8.1 
Shader 1.4 
OpenGL 
1.3 
Direct3D 8.0 
Shader 1.1 
OpenGL 1.3 
Direct3D 
9.0 
Shader 
2.0 
OpenGL 
2.0 
Special 
features 
Motion 
compensation 
3D 
textures, 
lightspeed 
memory, 
nFiniteFX 
engine, 
shadow 
buffers 
HW 
tessellation 
TruForm 
Accuview 
anti-aliasing, 
lightspeed 
memory II, 
nView

4.2 First Era Discrete PC-Based GPUs
123
Fig. 4.8 Die photo of ATI’s 
R100 chip. Source 
Fritzchens Fritz Wikipedia
operations, vertex skinning, and keyframe interpolation—functions previously done 
by the host processor. 
Vertex skinning (Fig. 4.9) made bending and moving of the polygons in animals 
and people joints more realistic. Known as skeletal animation, characters began to 
move more realistically, eliminating the blocky motion seen in the past. The idea 
was to deﬁne the bones in limbs and then attach the surrounding tissue in vertices 
around the bone. The behavior of those tissue vertices then depended on the position 
and movement of the bones. The trouble with skeletal animation was it created gaps 
or stretching if the joint between two bones was moved too far. 
Fig. 4.9 Example of vertex skinning. Source ATI

124
4
The First Era of GPUs
The other part of the process of keyframe interpolation, also known as tweening 
and keyframe morphing, was used for animation effects. The movements or displace-
ment of objects in the beginning frame are compared to the ending frames of the 
animation. Then the changes between the two frames are measured. Intermediate 
interpolated frames can then be generated to make the animation smoother and more 
lifelike. ATI’s vertex skinning was somewhat like Nvidia’s vertex blending. 
ATI’s Vertex Skinning went beyond the usual two-matrix vertex blending and 
skinning used by Nvidia. ATI connected the two bones, and that created a more 
realistic joint and movement. 
ATI’s R100 employed a four-matrix skinning process. A bent joint in skeletal 
animation requires different matrix transformations for the vertices of the tissue 
around each bone. However, the transformation of added matrix operations required 
additional computational work. The work could be sent to the host CPU, as Nvidia 
did. The R100 was able to transform up to four matrices in the GPU, which off-loaded 
the CPU. 
ATI had a time advantage in that it introduced its GPU after Nvidia. However, 
it is fair to assume that ATI was already working on those features before Nvidia’s 
announcement because development takes several years. The R100 is under the fan 
in the photo of the Radeon 7000 shown in Fig. 4.10. 
Another advantage of the multiple texturing capabilities of the ATI R100 chip was 
its ability to create 3D textures for volumetric effects such as glowing light, fogs, 
clouds, etc. ATI said, “three turned out to be the magic number,” as it worked with 
developers to determine what they would use the most [11].
Fig. 4.10 ATI R100 Radeon 7000 AIB. The R100 chip is under the fan. Source ATI 

4.2 First Era Discrete PC-Based GPUs
125
4.2.4.1 
Pixel Tapestry Architecture 
ATI also beefed up the rendering engine with its Pixeal Tapestry rendering engine. 
The dual-pipeline architecture included three texture units per rendering pipeline 
allowing up to three ﬁltered textures to be applied to every pixel in a scene without 
impacting performance. Multi-texturing is a feature well understood by game devel-
opers. The creative use of textures for lightmaps, shadow maps, bump maps, specular 
maps, and detail textures has long been essential tools in the developers’ arsenal of 
tricks to get signiﬁcant effects and high frame rates. However, the developers still 
had to trade off detail for performance, knowing they would ﬁll up the pipeline as 
they added maps. With two texture units per rendering pipeline, the dual-pipelined 
graphics processor could add two textures per pass. Add another texture, and you 
would have to wait for another clock cycle. In their demonstrations, ATI showed 
models with various maps applied to increase the realism of an object and give 
animators more freedom than drawing many detailed textures. With three texture 
units per pipeline, developers could get more out of a single pass. 
ATI’s Pixel Tapestry feature could handle multiple types of bump mapping. The 
three most popular ones were Dot Product 3 (also used by Permedia 3), environment-
mapped bump mapping (EMBM), and embossed bump mapping. Embossed bump 
mapping, which most AIBs supported at the time, handled effects such as specular 
shading, volumetric explosion, refraction, waves, vertex blending, shadow volumes, 
and elevation mapping on a per-pixel basis via hardware. 
Environment-mapped bump mapping made water appear more natural. Prior to 
EMBM, attempts at rendering water resulted in something that looked like a piece 
of glass that was ﬂoating. EMBM was one of the most impressive demos of the time. 
Regrettably, there was not much support by game developers for EMBM, partially 
because it was more challenging to program, and there was a performance hit. 
The use of 3D textures was an example of ATI’s ambitions to push new features 
into DirectX. Every graphics company had to meet the core feature set of DirectX 
(and still do), and they also needed a stand-out feature to push the envelope, i.e., 
extend the limits of what is possible. ATI’s 3D textures were supported in OpenGL 
and ATI hoped it would soon be included in DirectX 8. 
In general, ATI did not make choices where it did not have to; it just added 
everything. In bump mapping, for example, it added all techniques, including emboss, 
the more sophisticated Dot Product 3, and EMBM, which enabled developers to 
create realistic effects like water because they could change the heightmap in real 
time to create ripples, etc. 
Likewise, ATI did not choose what kind of environment mapping to add to its 
arsenal of tricks. It included spherical, dual-paraboloid, and cubic environment 
mapping. Pixel Tapestry also had priority buffers that stored polygons according 
to how close or far from the viewer each pixel was. That enabled features such as 
projective textures, range-based fog, and more sophisticated shadow mapping tech-
niques. ATI said it had been working closely with Microsoft, so it hoped advanced 
shadow mapping would make its way into DirectX as well.

126
4
The First Era of GPUs
The priority buffer assigned a value to every object or triangle, depending on how 
far it was from a light source. The priority value adjusted the light intensity on the 
polygons/objects so objects closer to a light source received a brighter lighting effect 
while those further away received less intensity. 
It took game developers a while to use 3D textures, a case of the hardware leading 
the software. However, the feature became part of DirectX 8, and that sped up its 
adoption. 
3D acceleration in the R100 included the following:
●Triangle setup engine.
●Single-pass trilinear ﬁltering.
●Six perspective-correct texturing modes.
●Video texturing.
●Gouraud and specular shading.
●A host of video enhancements for 3D special effects.
●Floating-point setup engine rated at 1.2 million triangles/s.
●Integrated 230 MHz DAC.
●Increased command FIFO size—512 entries. 
3D textures were very helpful in determining the shadows produced from the light 
sources. 3dfx had a somewhat similar capability with its T-buffer. 
Figure 4.11 shows the general organization of the R100 GPU.
ATI’s Pixel Tapestry architecture allowed for effects such as specular shading, 
volumetric explosion, refraction, waves, vertex blending, shadow volumes, bump 
mapping, and elevation mapping to be applied on a per-pixel basis via hardware. 
One of the more revolutionary features introduced by ATI’s Pixel Tapestry was 
3D textures. Unlike a ﬂat 2D texture or surface, a 3D texture has depth to it. 3D 
textures could also provide more realistic lighting effects. 
Using 3D textures in a game is the same as using 2D textures; they just use 
more memory because more information must be stored. 3D textures took advantage 
of texture compression developments such as S3’s Texture Compression (S3TC) 
developed in 1998 for its Savage 3D computer graphics accelerator. 
ATI expanded the rendering engine for the R100. The dual-pipeline architecture 
included three texture units per rendering pipeline. Therefore, the Pixel Tapestry 
rendering engine allowed up to three ﬁltered textures to be applied to every pixel 
in a scene without visibly impacting performance. Multi-texturing was a feature 
well understood by game developers at the time. The creative use of textures for 
lightmaps, shadow maps, bump maps, specular maps, and detail textures have long 
been essential tools in the developers’ arsenal of tricks to get great effects and high 
frame rates. However, the developers still had a trade-off, knowing they would ﬁll up 
the pipeline and add maps. With two texture units per rendering pipeline, the dual-
pipelined graphics processor could add two textures per pass (Table 4.3). Add another 
texture, and you would have to wait for another clock cycle. In its demonstrations, 
ATI showed models with a variety of maps applied to increase the realism of an 
object without requiring developers to draw many detailed textures.

4.2 First Era Discrete PC-Based GPUs
127
Fig. 4.11 Block diagram of the ATI R100 GPU
Table 4.3 ATI’s R100 had 
three texture units per 
graphics pipeline 
Textures per pixel
Texture units per rendering pipeline 
One
Two
Three 
1 bilinear
1 pass
1 pass
1 pass  
1 trilinear
12 passes
1 pass
1 pass  
2 bilinear
2 passes
1 pass
1 pass  
1 bilinear and 1 trilinear
2–3 passes
1–2 passes
1 pass  
3 bilinear
3 passes
2 passes
1 pass  
As shown in the table above, ATI wanted software developers to see they could 
add more effects without suffering a performance loss. Figure 4.12 shows how ATI 
demonstrated to developers that they could add six textures within two passes to get 
much more realism.

128
4
The First Era of GPUs
Fig. 4.12 ATI’s multi-texturing capabilities let developers add more special effects without 
impacting performance. Source ATI 
Pixel Tapestry also provided full-scene anti-aliasing and the same type of motion 
blur/depth of ﬁeld effects that 3dfx’s Voodoo5 offered. According to ATI, it only 
added those features to stay competitive with 3dfx. However, ATI did not think they 
were important features, especially full-scene anti-aliasing (FSSA). ATI’s primary 
concern was being able to display higher resolution images. 
ATI’s R-series GPU with the Charisma engine (Fig. 4.13) was a breakthrough 
design that set GPU and API design standards for years to come.
4.2.5 
Nvidia’s NV15—GeForce 2 GTS (April 2000) 
In late April 2000, Nvidia responded to ATI’s R100 with the 150 nm NV15-based 
GeForce 2 GigaTexel Shader (GTS). The GTS offered Nvidia’s shading rasterizer, 
which had similar capabilities to ATI’s Pixel Tapestry architecture. 
Nvidia had the GTS design in the NV10 (GeForce 256) GPU, but a hardware 
failure kept the company from using it. However, a problem can be an opportunity, 
and Nvidia used it to move to a smaller manufacturing process and increases the 
clock rate. 
The GTS also adopted ATI’s Charisma engine concept that employed the GPU 
for transform, clipping, and lighting calculations. 
4.2.6 
STMicroelectronics—Imagination Technologies Kyro 
II (2001–2002) 
In April 1999, VideoLogic was laying the foundation for reentering the PC graphics 
industry after the embarrassment of its failed Neon 250 product. The company

4.2 First Era Discrete PC-Based GPUs
129
Fig. 4.13 Block diagram of 
the ATI R100 Charisma 
engine
announced it was signing STMicroelectronics (STMicro, STM, or just ST) for its 
PC products as its new fab and partner for PC graphics [12]. 
STMicroelectronics was a French–Italian Dutch-domiciled multinational elec-
tronics and semiconductors manufacturer headquartered in Geneva, Switzerland, 
resulting from the merger in 1987 of two government-owned semiconductor 
companies: Thomson Semiconductors of France and SGS Microelectronics of Italy. 
NEC Electronics and STM said they would pursue different market segments, 
working independently on respective PowerVR road maps. The VideoLogic and NEC 
relationship would continue on other fronts, including gaming consoles and Video-
Logic’s lucrative arcade gaming machines business, including Pachinko and casino 
machines. STM would combine its set-top and consumer TV expertise, combining its 
technology with VideoLogic’s PowerVR graphics core for both PCs and consumer 
appliances. 
NEC licensed and manufactured its Series 1 and 2 generations of VideoLogic 
graphics accelerators and continued working with VideoLogic. 
Going forward, in early 1999, VideoLogic said it would build and sell the Series 
3 parts, which it planned to produce later that year. 
The announcement drew mixed reactions from the industry. “It clearly made sense 
to focus on the Dreamcast activity,” said Hossein Yassaie, CEO of VideoLogic. “From 
a planning point of view, we have retargeted that product to be more cost-effective.” 
Hossein declined to comment further on the cost reduction.

130
4
The First Era of GPUs
Fig. 4.14 Tim Chambers 
(courtesy of Bill DeBerry 
Funeral Directors—Denton 
TX) 
The VideoLogic Series 2 parts ran at 125 MHz and provided similar performance 
to competitors using faster 175-MHz parts. Those efﬁciency advantages led to STM’s 
commitment to VideoLogic for its graphics architecture of the future, explained Tim 
Chambers, Vice President, and General Manager of STMicroelectronics’s Graphics 
Products Division. 
“If an architecture requires 200 MHz, large frame buffers, then that will impact 
the system cost on the desktop,” Chambers said “Everything else equal, PowerVR is 
very good, even with the smaller 64-bit frame buffer interface” (Fig. 4.14). 
According to Yassaie, the market would likely see Series 3 parts from both NEC 
and STM, but with different feature sets and capabilities and for different markets. 
Although Chambers said he did not envision direct competition between the two 
companies, he did not rule out the possibility. 
“We still have a relationship with NEC,” Yassaie said, citing the company’s 
strategy of developing partners to deﬁne products, as both VideoLogic and STM 
would do. Both companies declined to discuss features of the Series 3 parts, but 
Chambers said the parts would be fabricated in a 180 nm process, either by STM or 
a third-party foundry. 
VideoLogic’s decision would have a domino effect because STM also had an 
ongoing relationship with Nvidia. Chambers, an affable and dexterous diplomat, said 
STM’s deal with Nvidia had reached “a natural end of its relationship” [13]. STM 
had been instrumental in helping Nvidia and built the ﬁrst products for the company. 
The opportunity—or the deal—to work with VideoLogic over Nvidia must have been 
compelling. STM produced the Nvidia controllers from the NV1 to the Riva 128, 
one of Nvidia’s most successful products (Fig. 4.15).
Chambers praised PowerVR’s unique architecture and visual quality on Sega’s 
Dreamcast and Naomi arcade systems, emphasizing its algorithmic approach to 3D 
processing eliminated redundant processing and memory bottlenecks. Additionally, 
noted Chambers, PowerVR kept as much processing as possible on-chip, minimizing 
the frame buffer requirements and bandwidth cost of accesses to off-chip memory, a 
growing need for competitive solutions. (Naomi (also known as Sega NAOMI) was 
an arcade system board developed and used by Sega for a wide variety of their arcade 
releases from the late 1990s to mid-2000s.)

4.2 First Era Discrete PC-Based GPUs
131
Fig. 4.15 Changing of 
partners
Under the terms of the deal, VideoLogic provided its PowerVR core architecture 
to STM, which would use it to build a new line of controllers targeted at PC and 
digital consumer markets, such as PC motherboards, graphics AIBs, STBs, and digital 
TVs. STM had considerable IP, including MPEG-2 decoding technology, and the 
company expected to take advantage of that and its strategic relationships to sell the 
new controllers to OEMs worldwide. Unfortunately, the pace of development did not 
move as fast for STM as it hoped. Nonetheless, the company resisted the urge to act 
prematurely and make announcements before it was sure it could deliver in volume 
and at the desired performance level. 
Aside from the levelheadedness of the company’s approach, one had to ask: Why 
would such a seemingly intelligent company do such an insane thing as enter the PC 
graphics market when it was in a state of consolidation and abandonment? 
Tim Chambers and his crew, led by long-time graphics guy Stuart McLaren, were 
not fools and had been in the graphics and semiconductor industry long enough to 
avoid stepping on the land mines, even the cleverly hidden ones. 
They had success with the Nvidia Riva 128 (shipping over 5 million units), taking 
it to markets Nvidia could not ﬁnd or penetrate. STM extended the useful life of 
the core by coupling it up to the Clarity Digital Video Processor and introduced it 
into the digital TV (DTV) and set-top box (STB) markets. STM had been uniquely 
successful in those markets with a domineering market share in DSS and other STBs. 
STM felt it had a signiﬁcant price advantage with the PowerVR architecture. It 
would represent a very cost-effective alternative or adjunct to the more expensive 
hyper-performance Nvidia GPUs in the market. STM believed the AIB builders were 
a little uncomfortable with just one major chip supplier (Nvidia) and were uncertain 
about the Via acquisition of S3. The only other source that already had an AIB line 
of its own had conﬂict of interest issues. 
Yassaie said, “STM’s excellence in manufacturing, process technology, and the 
video graphics market are a signiﬁcant new asset to PowerVR and will further extend 
the availability of this innovative technology to its widest ever PC audience. The 
combination of STM’s strengths and VideoLogic’s PowerVR technology will give 
PC users everywhere access to the best in 3D graphics.”

132
4
The First Era of GPUs
STM said it would continue to manufacture the Nvidia Riva 128 controllers if 
there was demand but emphasized its 1999 road map with new 3D controllers would 
solely be based on the PowerVR. 
“The relationship with STMicro,” said Trevor Wing, Imagination’s Vice President 
of marketing, “is more than just a fab—they are the sales channel.” Given STM’s 
experience with selling Nvidia’s Riva, the company would be the ﬁrst to ship the 
Series 3, and it had input into the design. STM had its blocks and 250 nm experience, 
he added. 
In August 1999, VideoLogic reorganized and changed its name to Imagination 
Technologies and continued working on the third-generation designs. 
VideoLogic launched the Neon at the end of 2000 in two versions, one for AGP and 
one for PCI. The speciﬁcations got a lot of companies’ attention and startled Nvidia. 
Nvidia reacted and quickly slashed their prices by almost 50%. The GeForce3 was 
reduced from $599 to $300; the GeForce2 MX was cut from $200 to $99 dollars; the 
company cut the popular MX from $400 to $129 dollars—the price reductions took 
everyone by surprise, especially Nvidia’s competitors. 
4.2.6.1 
PowerVR3 STG4000 Kyro—2001 
Imagination’s third-generation PowerVR3 (STG4000) Kyro was released by STM in 
2001. Imagination redesigned the graphics accelerator’s architecture and expanded 
it to a dual-pipeline design for more performance and better game compatibility. 
The chip failed to attract any big-name AIB suppliers. 
4.2.6.2 
PowerVR3 STG4500 Kyro II—2001 
The PowerVR3 Kyro II was process shrink, from 250 nm for Kyro I to 180 nm for 
Kyro II. That improved clock rates and memory bandwidth. 
The Kyro II had these speciﬁcations:
●180 nm manufacturing process.
●175 MHz controller clock (synchronized with the memory).
●128-bit memory bus, up to 64 Mbytes SDR.
●A tile rendering architecture; the tile size was 32 × 16 pixels.
●Two-pixel pipelines; each had one texture unit.
●Capable of blending up to eight textures in a single pass.
●Arbitrary ﬁltering setup for each texture (bi- and trilinear, anisotropic with up to 
16 samples).
●Effective super sampling anti-aliasing (SSAA), also called full-scene anti-aliasing 
(FSAA) implementation that did not require the increase of the frame buffer size 
stored in the chip’s local memory.

4.2 First Era Discrete PC-Based GPUs
133
Fig. 4.16 Imagination Technologies Kryo-based AIB. Source Trio3D Wikipedia 
●Effective rendering: only visible pixels were textured (no need to save the depth 
values in the local memory); the actual ﬁll rate was equal to the effective (350 
million pixels per second).
●AGP 1X/2x/4x interface.
●DotProduct3 and EMBM bump mapping hardware implementation.
●TDMS transmitter DVI interface.
●A hardware T&L block was not present. 
One of the ﬁrst AIB suppliers to sign up for the new chip was the venerable 
Hercules Corporation, which introduced the 3D Prophet 4000XT (Fig. 4.16). It 
became a popular AIB and helped extend Hercules’ marginal life. 
However, few other AIB companies used the part, and it was viewed as a stop-gap 
product. This was now the era of the GPU and Imagination’s main competitors; ATI 
and Nvidia had hardware T&L engines. The other company with a big following but 
slipping was 3Dfx, which also lacked hardware T&L. 
Imagination continued to rely on the CPU’s ﬂoating-point engine, as it had 
done with the Dreamcast. Despite that shortcoming, the chip had some admirable 
features, including Direct3D 8.1 compliance—Imagination had struggled with 
DirectX compatibility in the past. The Kyro II also offered eight-layer multi-texturing 
(not eight-pass), trilinear/anisotropic ﬁltering, EMBM, FSAA, and could perform dot 
product (Dot3) bump mapping at speeds similar to GeForce 2 GTS. But it did not 
have cube environment mapping and legacy 8-bit palette texture support, although 
it did offer DirectX texture compression (DXTC), previously known as S3TC—S3 
Texture compression. 
The Kyro II was a highly functional part even though it lacked hardware T&L, a 
fact that Nvidia repeatedly pointed out in aggressive marketing presentations [14].

134
4
The First Era of GPUs
That lack became a bigger liability as games increasingly started to include more 
geometry to take advantage of integrated T&L and the Kyro II lost its competitiveness. 
4.2.6.3 
The End 
Imagination planned a PowerVR4, Kyro III (STG5500) AIB for 2002 to get in step 
with the times and included a T&L front end. Some samples of the AIB were sent 
to reviewers. However, in late 2001 STM had decided to sell its graphics division 
(as well as the licenses for PowerVR Series 4 and PowerVR Series 5). The potential 
sale of the graphics business said STMicroelectronics was in line with its strategy to 
focus more attention on core areas in communications, automotive electronics. 
However, the company was unsuccessful when negotiations with Via collapsed at 
the eleventh hour [15]. In 2002, it shuts down the graphics division. Therefore, the 
new part could not be brought to market or a new fab found and brought up to speed 
in time. 
In February 2002, STM announced it was exiting the competitive PC graphics 
accelerator chip market and seeking a buyer for the related assets of the business. The 
company said PC graphics ICs accounted for only $15 million of its $6.36 billion 
revenues in 2001 [16]. 
The new design employed an enhanced T&L, which consisted of a hardware 
assist with T&L software emulation. It was a software process enhanced by the Kyro 
hardware. The Kyro hardware sorted the surfaces and 3D objects from the angle of 
vision and removed invisible triangles before applying the T&L. Those sorted 3D 
objects were animated and lighted in software. The concept was adopted into the 
driver for previous Kyro AIBs. 
After STM shuts down its graphics group, Imagination decided to exit the PC 
AIB market and concentrate on IP licensing. 
The company looked out over the landscape and saw the ﬁeld littered with once-
powerful contenders, many with good products that fell on hard times because of 
the brutal and unrelenting demand for a new product from the PC marketplace. It 
decided to broaden its options and serviced Sega’s game console and the arcade 
market ﬁrst—those were paying customers. It would also move into the mobile 
market. 
“The company’s business plan,” said Trevor Wing, “calls for the development of 
products and the development of an IP portfolio that give them the ability to mix and 
match components for SoCs.” 
4.2.6.4 
Summary 
This era was characterized by several technology approaches. Several companies 
pursued and promoted tiling engines. Imagination was one of the pioneers of the 
technology, beginning in the early 1990s. Other companies like Microsoft, Gigapixel, 
and Stellar followed. Microsoft promoted its Talisman project, and a few companies

4.3 The Development and History of the Integrated GPU (1999–)
135
said they would develop such products. At the same time, the pace of consolidation 
picked up. Gigapixel got acquired by 3dfx, and Broadcom bought Stellar. Then 
3dfx went bankrupt in December 2000; Nvidia bought its assets and received the 
tiling technology. Later in 2015, Nvidia used tile rendering in the 2014 Maxwell 
architecture and again in 2016 in the Pascal microarchitectures for a specialized 
section of geometry processing. The Maxwell and Pascal designs used immediate-
mode rasterizers. They buffered the pixel output instead of using conventional full-
screen immediate-mode rasterizers [17]. 
A rose by any other name 
VideoLogic started in 1985. In 1994, the company went public (ﬂoated) as Vide-
oLogic Plc with two divisions: VideoLogic systems focused on AIB sales, and 
PowerVR focused on GPU licensing. In 1999, VideoLogic decided to rebrand the 
company as Imagination Technologies to reﬂect the new focus on licensing IP. Shortly 
after that, it renamed the VideoLogic Systems group to PURE Digital (later just Pure) 
and added two other technology groups to PowerVR: Ensigma and META (later 
replaced with the purchase of MIPS). 
4.3 The Development and History of the Integrated GPU 
(1999–) 
Technology advancements allowing more transistors to be added to processors meant 
that processors could take on more capabilities. The trend was not limited to CPUs 
or graphics controllers and GPUs, it extended to the chipset companions to the CPUs 
that included the North Bridge and memory controller, and soon enough followed 
many other processors into the CPU. 
When GDCs were put in CPU chipsets in 1991 to 1995, they were called IGCs— 
integrated graphics controllers (Fig. 4.17). Then when GPUs (with T&L) were put 
in chipsets (2000) they became IGPs—integrated graphics processors. As the course 
of integration continued GPUs were integrated in with the CPU (2010) and they 
became iGPUs. Integrated graphics processor units enabled smaller and lower cost 
PCs.
There were three candidates for the title of the ﬁrst PC chipset integrated GPU 
(IGP), ArtX, ATI, and SiS. Each will be presented with a summary. 
With the introduction of geometry processing and T&L, the integrated graphics 
controller (IGC) found in the Northbridge chipset evolved into the IGP[1]. 
Arcade game system boards have had hardware T&L since 1993 (Sega Model 2 
by Real3D), and video game consoles have had it since the Nintendo 64’s Reality 
Coprocessor GPU created by SGI in 1996. PCs implemented T&L in software until 
1999. The game console GPU history is discussed in future chapters. 
The evolution of the technology is not always in synch with the evolution of terms 
and names. Therefore, you will see the same device called multiple thigs. Sometimes

136
4
The First Era of GPUs
Fig. 4.17 GDC to IGC to IGP to iGPU (1980–2009)
that is because of the industry learning what a new device is, and other times it is 
because the manufacturers want to differentiate their part. 
4.3.1 
ArtX 
In 1997, ArtX was formed by SGI engineers who were members of the design team 
that developed the Nintendo 64’s graphics chip. When SGI decided, it did not want 
to do the second-generation chip for Nintendo, 20 former team members chose to 
leave SGI and start ArtX to design the next-generation chip for Nintendo. ArtX was 
led by Dr. Wei Yen, who had been SGI’s head of Nintendo Operations. Dave Orton, 
Vice President of Visual Computing and Advanced Systems at SGI, joined ArtX in 
1999 and took over as CEO. 
In September 1999, David Orton, who led the development of the Cobalt chipset 
while Vice President of SGI’s advanced graphics division, left SGI and became Pres-
ident of ArtX. It showed its ﬁrst integrated graphics chipset with a built-in geometry 
engine at COMDEX in the fall of 1999, then marketed by Acer Labs of Taiwan. 
Seeing that, Nintendo contacted ArtX to create the graphics processor (called the 
Flipper chip) for its fourth game console, the GameCube. Then, in February 2000, 
ATI announced it would buy ArtX.

4.3 The Development and History of the Integrated GPU (1999–)
137
4.3.1.1 
ArtX: First Company to Announce a PC-Based iGPU 
In addition to working on the new Nintendo chip, ArtX also wanted to enter the 
integrated graphics chipset market, and it did so by developing the 3D graphics for 
Acer/ALi’s new Aladdin 7, a 128-bit Socket 7 for AMD K6-II and K6-III processors. 
It was the ﬁrst to include a hardware-accelerated geometry T&L engine, announced 
in November 1999, 1 month after Nvidia’s ﬁrst dGPU. 
In June 1995, Intel released its Socket 7 speciﬁcation. It was a physical and elec-
trical speciﬁcation for an x86-style CPU socket on a personal computer motherboard. 
The socket superseded Intel’s earlier Socket 5 and accepted P5 Pentium microproces-
sors manufactured by Intel, as well as compatible ones made by Cyrix/IBM, AMD, 
IDT, and others. 
The Aladdin 7 chip combined Northbridge functions with ArtX’s 3D geometry and 
graphics accelerator. The company claimed it would deliver benchmark performance 
two to three times greater than other Socket 7-based systems when running popular 
3D game titles by incorporating a ﬂoating-point engine for T&L functions and a 
UMA 128-bit architecture. 
The Aladdin 7 chipset was the last Super Socket 7 chipset by Acer Laboratories 
and included the M1561 Northbridge with a 2D/3D graphics accelerator and the 
M1535D Southbridge, integrated Super I/O, audio, and soft modem. The M1561 
supported the AMD K6-II and K6-III microprocessors as well as all other socket 7 
CPUs. A 100 MHz FSB was kept, with conﬁgurations from 8 MB to 1 GB of main 
memory. The basic outline of the chip’s organization is shown in Fig. 4.18. 
The Northbridge included a PC99-compliant 128-bit uniﬁed or shared memory 
architecture (UMA/SMA) data streaming architecture, a 33 MHz PCI 2.2 compliant 
I/O bus, and an Advanced Conﬁguration and Power Interface (ACPI) power 
management.
Fig. 4.18 ArtX’s Aladdin 7 3D integrated GPU controller 

138
4
The First Era of GPUs
ArtX proudly pointed out it was the ﬁrst integrated Northbridge graphics controller 
with hardware-accelerated geometry T&L. It could deliver a 12.5 M polygons/s 
transform rate with a 250 M pixels/s rendering rate (with fully lit 32-bit pixels), 
which was achieved by processing four pixels per clock. 
The hardware features included
●ﬂoating-point geometry and texture coordinate transformation;
●clipping, perspective projection, specular;
●diffuse and ambient lighting;
●3D points;
●lines;
●triangles;
●polygons;
●perspective-correct trilinear-ﬁltered mipmapped texture mapping;
●alpha blending;
●exponential pixel fog;
●depth;
●stencil buffering;
●anti-aliasing; and
●high-speed frame buffer clear and copy. 
The IGP also included 30 fps DVD playback with hardware acceleration (motion 
compensation), 128-bit BitBlt engine, 256 raster operations, RGBA, color expansion, 
and color keying as well as YUV planar 4:2:0, 4:2:2 video overlay, video scaling, 
and ﬁltering. The DAC output was 170 MHz. It was the ﬁrst product that did not 
offer a digital ﬂat panel output. However, for the market it was targeting (<$1,000 
PCs) that was not a drawback. 
In early 1999, an adverting company launched Free-PC.com. They would give 
away a PC to anyone who was willing to let directed ads appear on the screen. Over 
a half million people signed up. 
“The problem with the low-cost consumer PC and the ‘Free PC’ today is that end 
users must sacriﬁce graphics performance to get the low system price,” commented 
ALi’s president, Dr. Chin Wu [18]. “Since most of these PCs are going into homes, 
where PC-oriented entertainment is prevalent, this low performance is disappointing 
to users. With the new capabilities delivered by our Aladdin 7, consumers can have 
game-enthusiast performance at consumer-oriented price points.” 
The Company positioned the chip against Intel, SiS, and Via and offered the 
comparison shown in Table 4.4.
The chipset die size was 35 mm2 and was built using 250 nm technology at IBM’s 
fab and packaged in a 492-BGA. At the time, it sold in 10 K quantities for $32. 
In those days, a game-oriented performance PC sold for hundreds of dollars more 
than a low-end consumer PC due to the extra cost of the high-performance 3D 
graphics AIB and memory, higher performance core logic with cache memory, and 
higher end CPUs. By adding a 3D geometry graphics engine, ALi and ArtX hoped 
to set a new performance standard in the value market segment.

4.3 The Development and History of the Integrated GPU (1999–)
139
Table 4.4 Northbridge comparisons 
Aladdin7
i810 (Whitney)
SiS540
Via MVP4 
System memory 
bus (CPU access) 
128 bits
64 bits
64 bits
64 bits 
Graphics bus
128 bits
64 bits
128 bits
64 bits 
CPU performance* High
Low
Med
Med 
Virtual AGP speed
8 × 2.1 GB/s (with 
PC-1XX) 
4 × 1 GB/s
4 × 1 GB/s (limited 
by main mem at 64 
bits) 
4 × 1 GB/s  
Socket 7 and 
Slot1/370 versions 
were available 
Yes
No
Yes
No 
Integrated AC97 
audio 
Yes w/ALi 1535D
Yes
Yes
Yes 
Integrated super 
I/O 
Yes w/ALi 1535D
No
No
Yes 
PC-1XX SDRAM
Y
N
Y
Y 
Maximum 
SDRAM 
1 GB
256 MB
1 GB
768 MB 
*Based on off-loading the CPU from ﬂoating-point functions. Source ArtX
It was a competitive environment at the time. In an interview, the ArtX people 
cautiously said they had many multiples of FPUs on the chip, including dot product, 
inverse, viewport scales, and bias. 
“We would rather not quantify all of these numerous units,” said Rick Calle, 
ArtX’s Director of Marketing, “but rather, state that the transform processor is a 
super-pipelined VLIW engine with vector and superscalar architecture. We have 
tuned it to not only be fast, but … most important, small (given our price point)” 
[19]. 
ArtX worked with Acer for quite a while and had prototypes of the part avail-
able for several months (showing it to OEMs.) The ﬁrst part did not have AGP bus 
mastering, but its 128-bit UMA interface, even at 100 MHz FSB speeds, could reach 
a burst speed of 1.6 GB/s. Like other parts limited by memory bandwidth, the Aladdin 
7 scaled nicely when faster memories were used. However, the Company was not 
planning to use Rambus, and as Dave Orten, ArtX’s President said, “We like DDR.” 
ArtX picked an excellent partner in Acer. ALi was one of the leading suppliers 
of ICs for several market segments, including PC and peripherals, DVD players, 
embedded PC systems, and appliances. Its product offerings included core logic 
chipsets, super I/O controllers, DVD products, and imaging peripherals. Founded in 
1987 and headquartered in Taipei, Acer had 475 employees in 1999 and a revenue 
of $130 million in 1999. 
The people at ArtX thought of themselves as an IP provider and supplier of system 
products to the graphics and multimedia industry. Not long after ArtX was started in

140
4
The First Era of GPUs
1997, Nintendo latched on to ArtX’s capabilities and chose that Company to develop 
the graphics technologies for its next-generation console. 
In late 1999, Free-PC.Com said it would no longer give away computers and 
Internet access. It agreed to be acquired by eMachines, a maker of low-cost personal 
computers that intended to use Free-PC’s software to put advertising on the screens 
of its computers and to collect data about what users do. 
4.3.1.2 
ATI Acquires ArtX (February 2000) 
In 2000, ATI was still the market leader (38%), but Nvidia (20%) was closing the 
gap fast. ATI’s competitors were gloating that ATI had lost its edge. 
And then ATI’s Founder and CEO, K. Y. Ho, surprised the industry and demon-
strated why his Company was the top moneymaker in the graphics business. In 
February 2000, ATI announced the acquisition of ArtX in a stock swap. It was a deal 
that brought the company a dynamite design staff, new management, a strong position 
in the console market, a product for the brand new and growing integrated graphics 
market, and the expertise to advance its graphics design for high-end next-generation 
discrete chips. 
The deal was worth $400 million. ATI gave 32.6 million shares in exchange for 
100% of ArtX. The deal was structured so that 49% was in common stock, and 
51% was in stock options over 38 months. Because most of ArtX’s key employees 
held shares in the Company, they were effectively tied to the Company. Dave Orton, 
ArtX’s President, and CEO took over as ATI’s COO. The day-to-day running of ATI 
would be overseen by the three heavyweights: Dave Orton, K. Y. Ho, and technologist 
Adrian Hartog. 
In an interview with investors, Henry Quan, ATI’s Vice President of corporate 
marketing, revealed that ArtX had revenues of $10 million in 1999 and, according 
to company projections, the Company planned to make $200 million. To make those 
projections, the Company was looking at royalties from the Nintendo deal. Also, 
ArtX was supplying Acer the core logic for the Aladdin 7, an integrated graphics 
chipset Socket 7 system. As Quan pointed out, ArtX stood to bring in considerably 
more money when it was incorporated into ATI products [20]. 
The ALi-integrated chipset was distinguished because it was the only chipset with 
a 128-bit graphics engine and T&L capabilities. Also, ArtX had a UMA design that, 
the Company said, gave it desktop-level performance in an integrated chip. As an 
example, the chipset had a throughput of 12.5 triangles/s. 
Orton told analysts the deal was a great one for his Company. He commented that 
no matter how successful his Company might be on the course it was headed, it could 
never do what it was doing with ATI’s resources. He said, “Combining ArtX’s inno-
vative graphics expertise in the consumer appliance markets with ATI’s distribution, 
manufacturing, and supply know-how in the PC market is an unbeatable combina-
tion. Moreover, we quickly enhance and amplify two major initiatives evoked by 
ATI—developing high-performance integrated and e-appliance solutions.”

4.3 The Development and History of the Integrated GPU (1999–)
141
In addition to giving ATI a play in the big biz of console gaming, the deal for 
Nintendo’s Dolphin machine ﬁts with ATI’s desktop box strategy for IGPs. It is 
important to note that the Dolphin was a new-generation console box with capabilities 
beyond gaming that also had DVD capabilities. 
In a presentation to analysts, K. Y. Ho said the company had identiﬁed a key inﬂec-
tion point 3 years ago and that the deal was an important part of the company’s game 
plan. “This acquisition accelerates the implementation of our longer-term strategic 
plan to be a key supplier to both the PC and consumer electronics industries,” he said 
[21]. ATI’s Quan admitted the company had not quite worked out all the deal details 
and how the company would sort out overlap. The Chromatic team seems to have 
some overlap with the new ArtX gang. 
Other changes to the management team include the addition of Dr. Wei Yen, who 
was the Co-Founder and Chairman of ArtX, to ATI’s Board of Directors. Yen was 
previously Senior Vice President of products and technologies at SGI. He also headed 
MIPS Technology Inc., which is why ArtX could strike up a new relationship with 
Nintendo. Orton was Head of SGI’s visual computing group and part of the team 
that developed the Reality Engine for the Onyx Inﬁnite Reality, Origin 200 and 2000 
servers, and Performer. He was also a driving force in SGI’s OpenGL project. Orton 
worked with Yen at SGI for 7 years. Orton stuck it out at SGI but left when the 
company sent its best graphics engineers off to Nvidia. 
It sounded at the time as if ATI got really interested in ArtX at Comdex 2000 
when the Acer/ALi deal was announced. When reporters asked how long it had been 
going on, the participants laughed and said they had been talking for a long time. 
4.3.2 
ATI’s First IGP (March 2000) 
Shortly after the ArtX acquisition, ATI delivered on its promise and introduced the 
ATI S1-370 TL, an integrated Northbridge and GPU for the Pentium Slot 1 and 
Socket 370. That product came from ArtX and would be marketed to the OEM and 
motherboard manufacturers. ATI said it would also produce motherboards with the 
S1-370 TL to facilitate the introduction of the chipset to the market. 
ATI hoped introducing the S1-370 TL chipset would catapult the company into 
the price/performance leader in the integrated market. 
The S1-370 TL chipset had T&L, something usually found only in high-end 
graphics subsystems and add-in boards. T&L was one of the major elements required 
for next-generation game titles and other applications, and that was one of ATI’s major 
markets. 
ATI claimed throughput of 12.5 million polygons per second, four pixels per 
clock, and greater than 330 million textured pixels/s (the exact same performance as 
the chip ArtX was building for Acer). The 3D features included accelerated ﬂoating-
point geometry; texture coordinate transformation clipping; perspective projection;

142
4
The First Era of GPUs
accelerated specular, diffuse, and ambient lighting; perspective-correct trilinear-
ﬁltered mipmapped textures; alpha blending; exponential pixel fog; depth; and stencil 
buffering and anti-aliasing. 
ATI said at the time it was conﬁdent it would outperform the Intel i810e and 
claimed the S1-370 would provide twice as much 3D and game performance than 
an i810e (as measured by WinBench 2000 and games like Quake III Arena). The 
128-bit architecture of the S1-370 TL provided signiﬁcantly faster overall system 
performance. ATI said the chipset was compatible with the ALi M1535D and M1543 
Southbridge parts. Claiming to be the only 128-bit data streaming architecture for 
SMA applications, ATI’s S1-370 TL boasted of a patented algorithm designed to 
enhance memory bandwidth to improve both graphics and system performance. 
With a 128-bit memory interface for standard PCs 133/100/66 MHz SDRAM, ATI 
claimed the S1-370 TL would provide 2.1 GB/s bandwidth. 
The S1-370 TL also featured motion compensation, 32-bit color, and resolutions 
up to 1600 × 1200 (non-interlaced). It came with drivers for DirectX 7 with Direct3D, 
OpenGL’s installable client driver (ICD), hardware motion compensation (HWMC), 
and graphics drawing interface (GDI) acceleration on Windows 98 and Windows 
2000. Drivers were also available for Windows NT 4.0. 
ATI’s S1-370 TL chipset was fabricated in 250 nm at IBM—the same as ArtX’s 
Acer chip. ATI priced the S1-370 TL at $36 each in units of 10,000. 
Thanks to the ArtX technology, ATI enjoyed an advantage with its superior 
integrated graphics performance compared to competitors including Intel. 
However, it was observed that T&L on the datasheet does not translate to actual 
T&L [22]. Nvidia’s early implementation of the technology was not as great as the 
Company said it was; S3 actually hurt performance. And a review published on 
Tom’s Hardware [23] noted Imagination Technologies’ Kyro’s T&L performance 
was nonexistent. Prudence, therefore, dictated that ArtX’s claims about its T&L 
should be qualiﬁed as just that, claims, said Kathleen Maher [24]. Also, it has not 
exactly been proven out that T&L is all that necessary in a market that really seems 
to want digital audio, video, TV, and so on, Maher added. 
4.3.3 
SiS’ First PC-Based IGP (December 2000) 
Founded in 1987, Silicon Integrated Systems (SiS), headquartered in Taiwan, was 
partially funded by United Microelectronics Corp. (UMC), the other merchant fab in 
Taiwan. UMC had a history of investing in IP companies and was behind MediaTek 
as well. 
For a long time, SiS claimed it was the ﬁrst company to introduce an IGP with 
T&L. In December 2000, just 2 years after Nvidia introduced the discrete integrated 
GPU, SiS introduced T&L to its integrated graphics controller (IGC) in the Real256 
chipset based on the company’s SiS315 3D T&L core (Fig. 4.19) [25, 26]. However, 
it did not ship the part until 2001.

4.3 The Development and History of the Integrated GPU (1999–)
143
Fig. 4.19 SiS’ 315 ﬁrst integrated chipset GPU 
The SiS315 would bring T&L to a more extensive range of modestly priced 
computers, expanding its attraction in users’ eyes, especially designers and gamers. 
On December 21, 2000, SiS held a groundbreaking ceremony for the company’s 
ﬁrst 300 mm wafer fab and product R&D center. SiS secured a 12-ha site in the 
Taiwan Science-Based Industrial Park with the plan to construct two 300 mm wafer 
fabs and an R&D building with space for 1,500 engineers. The construction of the 
ﬁrst fab was to be completed in 2002. The fab started with the company’s existing 
150 nm process. 
SiS did not follow the lead of Intel in splitting out the North and Southbridge 
functions and chose to integrate them into one chip. 
SiS also jumped over traditional graphics supplier Matrox by introducing its 
T&L part, joining the mainstream market’s ranks of 3Dlabs, ATI, and Nvidia. Those 
companies were not so happy to see a lower priced T&L part for the mainstream 
segment. 
ALi and ArtX (now ATI) had a 3D T&L part designed and announced but decided 
to pull it back for a redesign (adding DDR FSB capability). After learning about ALi’s 
efforts, SiS took the leap with DDR, and using its fab was able to come out with the 
next-generation part. So ﬁrst to announce versus ﬁrst to ship—who is ﬁrst? 
In 2003, the management of the graphics design team persuaded SiS and UMC to 
spin them off and form a separate graphics company, which they did, and it became 
Xabre Graphics (XGI). Headquartered in Taipei, Taiwan, and Founded in June 2003, 
it was sometimes called eXtreme Graphics Innovation or XGI Technology Inc.

144
4
The First Era of GPUs
In the process, they picked up the graphics group from Trident Microsystems to 
ﬁll out the notebook segment. In 2012, after trying to become digital TV (DTV) chip 
supplier by buying other companies, Trident ﬁled for bankruptcy. 
XGI consisted of over 240 people in 2003; about 170 came from the graphics 
group of SiS and were in Taiwan, and 70 others came from Trident and were in 
Sunnyvale. About 140 of the people in Taiwan were in R&D or engineering. 
With the acquisition, XGI would combine the operating assets and intellectual 
property of Trident’s and XGI’s graphics business. Michael Chen, President and 
CEO of SiS and Chairman of the XGI board, said the combination of Trident’s 
technology with that of SiS would create a “formidable power in both the mobile 
and desktop graphics market” [27]. 
XGI had some different ideas; not the least was to segment the GPU into two 
separate chips: one for vertex processing and one for pixel processing. XGI named 
the system Volari, and the idea was that it could expand the design as needed in either 
or both sections. That did not prove practical or economical, but it did generate some 
novel designs. 
An important part of XGI’s business plan was to be the second chip supplier for 
all the companies that settled on either ATI or Nvidia as their ﬁrst chip line. That 
strategy, the company reasoned, would give XGI a win with everyone. However, it 
did not work, and the company could not ﬁnd any customers for its chipset. 
In 2004, the company went into the AIB business to try to build up some brand 
equity. That was not a great success either, but it did result in some sales in China 
and Taiwan. But, as pointed out many times, companies need to have several design 
teams to be in the GPU business. XGI did not have the investment capital from UMC 
or the revenue from its efforts to fund such expansion, so it was relegated to a new 
part every 4 years or so—not enough to keep up in the fast-turn GPU market. 
At the same time, SiS fell behind in the IGC and IGP race, and it never got into the 
IGP space. SiS sadly watched ATI, and Via ﬁlled the vacuum Intel had left. Enough, 
said the ﬁnancial guys at UMC, and so the word went out that XGI was up for sale, 
but not the parts in Taiwan—SiS wanted those back. That still left more than 100 
people in Shanghai and almost two dozen in Santa Clara. 
In March 2006, ATI announced it would buy Shanghai-based Macrosynergy assets 
and hire the personnel working out of XGI Technology’s Santa Clara location [28]. 
Macrosynergy was an XGI Technology Alliance company. 
What ATI acquired were Macrosynergy’s assets and people. The price was 
publicly stated, meaning that it was signiﬁcant relative to ATI’s balance sheet 
(and therefore ATI was obliged to report it). However, ATI did not disclose how 
much it paid for Macrosynergy or the form of that payment. Dave Erskine, ATI’s 
spokesperson, said the deal, cost ATI less than $10 million [29]. All of that earned 
ATI a put-down by “The Motley Fool,” which referred to it as ATI’s Hush-Hush Mush 
for a press release that contained almost no information. But The Motley Fool did 
not know anything about the history of the graphics market, so it did not recognize 
what a win it was for a GPU company to pick up an in-place design team and expand 
its presence in China [30]. 
XGI was split up, with parts of it going to ATI and SiS.

4.3 The Development and History of the Integrated GPU (1999–)
145
In 2007, SIS professed to be quite comfortable with the success of Intel and the 
company’s new chipsets because SiS had come to beneﬁt from Intel’s “the enemy of 
my enemy can sit next to me” philosophy. Because companies like Dell insisted on 
dual suppliers for their components, Intel recommended SiS. 
AMD and Intel moved the system memory manager into the CPU to reduce 
memory latency between the CPU and system RAM, eliminating the Northbridge 
and most of the need for an IGP. 
And in 2010, Intel moved the GPU in the IGP into the CPU. That created the IGP 
and eliminated the need for an IGP chipset. AMD did the same thing in 2012. 
By 2008 SiS had stopped offering IGPs and moved to touch-screen controllers, 
and in 2015, the company joined the Universal Stylus Initiative (USI) and offered 
pen controller chips. SiS had been developing USI technology since 2016. It had a 
USI-enabled touch controller and USI stylus controller available for testing that it 
planned to have commercially ready in the ﬁrst half of 2019. 
SiS had the lead in designing highly integrated chipsets. The company combined 
the Northbridge and its integrated graphics with the Southbridge and created an all-
in-one processor to take on Via. The thing the Company had not been successful in 
doing was making money. 
4.3.4 
Nvidia’s nForce 220 IGP (June 2001) 
Nvidia saw what SiS did and introduced its IGP, the nForce 220, for AMD Athlon 
CPU in June 2021 (Fig. 4.20). 
Fig. 4.20 Nvidia’s nForce IGP

146
4
The First Era of GPUs
The nForce was a motherboard chipset Nvidia created for AMD Athlon and Duron 
CPUs. Later Nvidia added the Intel 5 series processors. There were three varieties 
of the chipset: 220, 415, and 420 [1]. 
Built on the 180 nm process, the GeForce2 MX + nForce 220 IGP had 20 million 
transistors and was based on Nvidia’s Celsius architecture the company branded as 
Crush11. Even though the chip did not have a vertex shader, it was compatible with 
DirectX 7.0. 
At the Computex conference in June 2004, Nvidia announced the next-generation 
nForce3 Ultra MCP, a new media and communications processor for motherboards 
and PC systems based on the AMD64 computing platform. MCP was Nvidia’s 
acronym for Media and Communications Processor. 
The company also introduced the MCP-T, which differed from the MCP in three 
aspects: the MCP-T offered an integrated FireWire controller and a second network 
controller, and Nvidia’s Audio Processing Unit (APU). 
In 2008, when Intel moved from the front-side bus (FSB) architecture to Quick-
Path, a serial link interface (copying the hyperlink design from AMD), it declared 
Nvidia’s bus license invalid since, by Intel’s reasoning, there was no bus. After 
a prolonged legal battle, Nvidia won a settlement from Intel, and in 2012 Nvidia 
exited the IGP market. That left only AMD, Intel, and Via. All other companies in 
the market were either bought or driven out of the market by competition. 
4.3.5 
ATI’s IGP 320 (2002) 
Two years after its acquisition of ArtX, in January 2002, ATI introduced its ﬁrst IGP, 
the 320 (code named ATI A3). 
Four years after ATI introduced its IGP, AMD bought ATI to develop a processor 
with a real GPU integrated (which would become known as an IGP). At the time, Dave 
Orten was ATI’s CEO. However, developing a CPU with a GPU proved harder than 
either company thought—different design tools, different fabs, and most difﬁcult, 
different corporate cultures. 
4.4 IGP Conclusion 
During the 1990s, the PC industry saw the GPU enjoying the beneﬁts of Moore’s 
law and integrating more and more features formally provided by co-processors. It 
was a natural assumption that the CPU would assimilate the GPU along with its path 
of being the total, one-chip PC. The ﬁrst step in that process was the development 
of the IGP. And in true Moore’s law fashion, the IGP assimilated more and more 
functions. Along with the enhancements and miniaturization of the IGP came the 
notion that it would also replace the AIB. But the GPU proved to be too powerful, 
too big, too big a power consumer, and too demanding on local memory to ever be

4.5 The Expansion Years (2001–2016)
147
totally assimilated by the CPU or the best IGP. GPUs did ﬁnd their way into the CPU 
and did eliminate the lowest tier of the AIB market. But even their Chief Advocate, 
Intel, had to concede that discrete GPUs (dGPUs) were in a class of their own and 
Intel introduced its own dGPU in mid-2022. 
4.5 The Expansion Years (2001–2016) 
For 15 years, ATI/AMD and Nvidia dominated the GPU landscape and leapfrogged 
each other from one product cycle to the next. Little features were added, but it was 
mostly about more shaders, ever-increasing chip sizes, and power consumption. 
Nvidia broke out of the grind in 2017 when it introduced its Volta architecture 
with Tensor cores. And then, in 2018, it added ray tracing cores. AMD added ray 
tracing cores to its design in 2020. AMD and Nvidia kept tweaking their shaders, 
adding software tricks and features, and, in general, pushing the industry to new 
levels of performance and realism. 
Figures 4.21 and 4.22 are generalized depictions of their architectural steps over 
time. The charts show the date of introduction. Various generations were produced 
for several years, overlapping newer architecture, often with a fab process shrink 
(sometimes referred to as mid-life kicker). 
Nvidia’s development closely matched ATI/AMD’s due in part to semiconductor 
manufacturing process steps.
Fig. 4.21 ATI/AMD’s GPU developments over time

148
4
The First Era of GPUs
Fig. 4.22 Nvidia’s GPU developments over time
The following sections will discuss the signiﬁcant feature developments leading 
to real-time ray tracing, uniﬁed shaders, and ultimately mesh shaders. 
4.5.1 
The Collapse and the Rise of Graphics Chip Companies 
The PC and console graphics chip market peaked in terms of suppliers in 1998. Even 
though the supplier population collapsed, the demand rose and continued to grow 
until 2011, when 506 million PC GPUs were shipped. 
The PC and console GPU supplier pool stayed level from 2017 to 2021, when 
new companies entered the market, encouraged by the demand for AI training and 
crypto-mining. Neither application took advantage of all the fantastic developments 
in rendering and the ability to generate beautiful images. 
The other conclusion one can take away from the above chart is that the GPU 
propelled the market. The Internet bubble popped in 2000, setting back growth for 
a short period, but things took off after that. And the great recession of 2009 did not 
slow things down either. 
The tablet, however, impacted the PC market, and consumers who were using a 
PC for minor tasks like email and simple photoediting found the affordable, mobile, 
thin, and lightweight device an ideal solution. Simultaneously, smartphones were 
getting larger with higher resolution screens and amazing SoCs with surprisingly 
good GPUs in them. 
Unlike the PC market, the mobile market is still a healthy place for IP GPU 
suppliers. Mobile devices are discussed in Book three.

References
149
4.6 Conclusion 
The biggest and fastest consolidation of graphics chip suppliers took place during the 
ﬁrst era of GPUs between 1999 and 2001. Coincidently that was also the period of 
the great Internet bubble popping that wiped out fortunes, and cratered companies. 
As the Internet bubble was expanding in late 1998 to late 1999, the smart (so they 
thought) investors shoved their money into Internet start-up megastars Facebook 
(Meta), Twitter, and YouTube. 
Today, all the graphics chip and most of the software bubble start-ups have 
disappeared. 
The Internet bubble companies drew the VCs away from hardware companies 
denying them needed capital to ﬁnish their designs. They ran out of cash before they 
could leap over the technology hurdles demanded to compete with GPUs. Hardware 
wasn’t fashionable and wouldn’t be until 2018. The leading graphics chip survivors 
were ATI, Matrox, and Nvidia. The other 19 companies that made it to 2001 would 
be acquired or just shut their doors. Only VideoLogic/Imagination Technologies 
would successfully make the transition from a hardware supplier to an IP supplier 
and change their name in the process. 
In the next chapters, we will look at the second era of GPU development. 
References 
1. Peddie, J. Famous Graphics Chips: The Integrated Graphics Controller, IEEE Computer 
Society, https://www.computer.org/publications/tech-news/chasing-pixels/the-integrated-gra 
phics-controller 
2. Singer, G. History of the Modern Graphics Processor, Part 3: Market Consolidation, The Nvidia 
vs. ATI Era Begins, (December 6, 2020), https://www.techspot.com/article/657-history-of-the-
gpu-part-3/ 
3. Bitboys Oy Unveils Glaze3D Product Family (August 2, 1999), http://www.shugashack.com/ 
docs/press/080299bitboysoy.htm 
4. Maher, K. Bitboys Oy: We’re here because we live here, The Peddie Report, Volume XII, 
Number 40 (October 11, 1999) 
5. Nordlund, P. Hot 3D Session II: Panel, 3D for PCs and Game Consoles SIGGRAPH Euro-
graphics, Los Angeles, CA, (August 12, 2001) http://www.graphicshardware.org/previous/ 
www_2001/program.html 
6. Robertson, J. One-on-One: For Inﬁneon, DRAM is a matter of survival, EETimes, (December 
14, 2001), https://www.eetimes.com/one-on-one-for-infineon-dram-is-a-matter-of-survival/ 
7. Hachman Mark, S3 acquires Diamond Multimedia in stock swap, (June 23, 1999), https://www. 
eetimes.com/s3-acquires-diamond-multimedia-in-stock-swap/# 
8. Leonidas, What’s going on at S3?,3D Center.De, (November 22, 2005), http://alt.3dcenter.org/ 
artikel/s3_nov2005/index_e.php 
9. Zhaoxin, https://en.wikipedia.org/wiki/Zhaoxin 
10. Zhang, Phate, VIA sells technologies including x86 to Shanghai Zhaoxin for $257 million, 
(October 28, 2020), https://cntechpost.com/2020/10/28/via-sells-technologies-including-x86-
to-shanghai-zhaoxin-for-257-million/ 
11. ATI shows off its new Charisma Engine and Pixel Tapestry architecture to game developers, 
The Peddie Report, Volume XIII, Number 20 (May 15, 2000)

150
4
The First Era of GPUs
12. Hachman, M. VideoLogic, ST form graphics partnership, EE Times (April 08,1999), https:// 
www.eetimes.com/videologic-st-form-graphics-partnership/ 
13. McCullan, B.STMicroelectronicsandVideoLogicteamingupfor“third-generation”PowerVR, 
The Peddie Report, Volume XII, Number 15, pp.622, (April 12, 1999) 
14. https://www.vogonswiki.com/index.php/File:Nvidia_on_kyro.pdf 
15. Via in talks to buy STMicro’s graphics IC operation, says report, EETimes, (February 6, 2002), 
https://www.eetimes.com/via-in-talks-to-buy-stmicros-graphics-ic-operation-says-report/ 
16. STMicroelectronics N.V., Securities and Exchange Commission FORM 20-F, (May 24, 2002), 
https://investors.st.com/static-ﬁles/62e736e9-4b26-47fd-8b12-15d279202a00 
17. Kanter, D. Tile-based Rasterization in Nvidia GPUs, (August 1, 2016), https://www.realworld 
tech.com/tile-based-rasterization-nvidia-gpus/ 
18. Sommefeldt, R. (January 18, 2019) The fabled ALi Aladdin 7 IGPU, GPU Tools Ltd., https:// 
forum.beyond3d.com/threads/the-fabled-ali-aladdin-7-igpu.61051/ 
19. Peddie, J. ArtX Aladdin 7 integrated T&L controller with Northbridge, The Peddie Report, 
Volume XII, Number 45 (November 8, 1999) 
20. Maher, K. ATI acquires ArtX, Inc. for $400 million, The Peddie Report, Volume XIII, Number 
8 (Febuary 21, 2000) 
21. ATI to buy ArtX for $400M, (February 16, 2000), https://money.cnn.com/2000/02/16/deals/ati/ 
22. Maher, K. ATI starts the turnaround with cost reductions, The Peddie Report, Volume XIIv, 
Number 3(January 15, 2001) 
23. Weinand L. Radeon 8500 vs. Ti500 - Overclocked Graphics, Tom’s Hardware (2001), https:// 
www.tomshardware.com/reviews/radeon-8500,385-10.html 
24. Peddie, J. ATI S1–370 TL chipset, The Peddie Report, Volume XIII, Number 10 (March 6, 
2000) 
25. Hung, Faith, SIS to introduce DDR chipsets for Intel, AMD processors, EE Times (11 December, 
2000), https://www.eetimes.com/sis-to-introduce-ddr-chipsets-for-intel-amd-processors/# 
26. Introduced SiS315, a 256-byte 3D graphics chip supporting high-performance T&L, (Dec. 
2000) https://www.sis.com/About_Milestones.aspx 
27. Smith, T. SiS spins off Xabre, buys Trident’s graphics biz, (June 13, 2003), https://www.thereg 
ister.com/2003/06/13/sis_spins_off_xabre_buys/ 
28. EDN staff, ATI Buys Shanghai-based XGI, EDN, (March 7, 2006), https://www.edn.com/ati-
buys-shanghai-based-xgi/ 
29. Wolverton, T. ATI Tech Buys Chinese Chip Firm, (March 6, 2006), https://www.thestreet.com/ 
technology/ati-tech-buys-chinese-chip-ﬁrm-10272036 
30. Peddie, J. ATI—working at it, TechWatch Volume 6, Number 6, pp 9, (March 13, 2006)

Chapter 5 
The GPU Environment—Hardware 
The GPU’s success as a ubiquitous processor relies on the stands that surround it, 
its ecosphere. In one sense the ecosphere limits some of the freedom for innovation. 
The GPU suppliers use the same fabs, their GPUs must comply with the same I/O 
structures, they have to work with the same APIs and operating systems, and they 
all use the same type of memory. 
In this chapter, we’ll look at the hardware elements that surround the GPU, and 
in the next chapter, we’ll examine the software elements, Fig. 5.1 shows how the 
hardware elements have evolved over time.
Because the events described in this chapter happened over a decade starting in 
2000, it might be helpful to remember that ATI was acquired by AMD in 2005. So 
through this discussion, the company will be referred to as ATI before 2005 and 
AMD after. 
The explication of the gaps is almost as interesting as the events. You can see the 
effect of Moore’s law slowing down starting in 2015. PCI-SIG, for example, targets 
a 3-year update, but market demand and varying development timelines, sometimes 
delays the timing speciﬁcation release. 
GPUs have grown in complexity (and size) from simple CRT control devices to 
supercomputers on a chip. A GPU is a collection of technologies inﬂuenced by many 
external items. This chapter will look at those elements, as it is impossible to fully 
appreciate or understand a GPU without being aware of the environment in which it 
lives, and the forces applied to it. 
5.1 
It Takes a Village to Build a GPU 
Standards, protocols, and regulations surround the design of a GPU, and other param-
eters, none of which were controlled by the GPU manufacturer—inﬂuenced maybe, 
but not controlled (Fig. 5.2).
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2022 
J. Peddie, The History of the GPU – Eras and Environment, 
https://doi.org/10.1007/978-3-031-13581-1_5 
151

152
5
The GPU Environment—Hardware
Tech nm DirectX PCIe DVI/HDMI 
DP
Memory 
2000
150.0
8.0
DVI 1.0
DDR/GDDR1 
2001
150.0
8.1
1.0 
2002
150.0
8.0a/9.0
PCIe
HDMI 1.0
GDDR2 
2003
130.0
9Xa
PCIe 1.0
DDR2/GDDR3 
2004
110.0
9.0b/9.0c 
2005
90.0
HDMI 1.2
GDDR4 
2006
80.0
9.0L
HDMI 1.3
1.0 
2007
65.0
10.0/10.1 PCIe 2.0
DDR3/GDDR5 
2008
55.0
1.1a 
2009
40.0
11.0
HDMI 1.4 
2010
11.2
1.2 
2011
28.0
3.0
HBM 
2012
DDR4 
2013
HDMI 2.0
1.2a 
2014
1.3 
2015
12.0
GDDR5X 
2016
14.0
1.4/2 
2017
HDMI 2.1 
2018
12.0
12U
4.0
1.4a
GDDR6 
2019
7.0
5.0 
2021
12.0U 
2022
5.0 
2023
GDDR7 
Fig. 5.1 Signiﬁcant developments in the GPU ecosphere
This chapter covers the main elements in the environment of the GPU. The CPU 
is mentioned only in the context of an SoC and its relationship to an iGPU. And 
the OS is discussed only in terms of its connection to an API. The system memory, 
simulators, and design tools are mentioned but not discussed in detail. There is no 
discussion of power supplies (PSUs). 
5.2 
Semiconductor Technology 
Today, we take high-resolution, deep-color frame buffers and z-buffers for granted, 
but the computer graphics and GPU industries would not exist if it were not for 
super-VLSI memory and Moore’s law (Fig. 5.3).
In 2019, AMD built GPUs and CPUs with 7 nm nodes and Nvidia with 8 nm 
nodes. TSMC and Samsung announced that 3 nm nodes would be in production 
within 2022–23.

5.2 Semiconductor Technology
153
Fig. 5.2 The environment of the GPU
Fig. 5.3 From the ATI R520 of 2005 to the AMD Polaris architecture of 2016, the feature size was 
halved four times. Reproduced with permission from AMD
5.2.1 
Intel Introduces Angstroms 
Intel took matters into their own hands and deﬁned the new nodes using an Intel 
classiﬁcation; others could use that if they liked, said the company, or not (Fig. 5.4).

154
5
The GPU Environment—Hardware
Fig. 5.4 In 2021, Intel announced it intended to build chips with a feature size of 20 angstroms in 
2024. Reproduced with permission from Intel 
From 2021 onwards, the Intel process nodes would be 10 nm (super ﬁn), Intel 7, 
Intel 4, Intel 3, and Intel A20. Intel built very specialized ribbon-ﬁt transistors with 
some parts so small they were measured in angstroms. Intel would ship those parts 
by 2024 and even smaller Å sizes. 
Although some observers have pronounced the end of Moore’s law, it was far 
from over. As Intel’s CEO Pat Gelsinger said when introducing the chart shown in 
Fig. 5.4, “Until the periodic table is exhausted, Moore’s law goes on” (Fig. 5.5). 
Intel used ultra-short wavelength EUV lithography to print the microscopic Intel 4 
features. Intel 4 went into production in 2022, with a performance-per-watt increase 
of approximately 20 percent.
Fig. 5.5 Intel’s CEO, Pat 
Gelsinger, circa 2021. 
Reproduced with permission 
from Intel 

5.2 Semiconductor Technology
155
Intel 3 leveraged FinFET, increased EUV, and offered a performance-per-watt 
increase of approximately 18 percent over Intel 4. Intel 3 products would be ready 
for manufacturing in the second half of 2023. 
Intel 20A ushered in the angstrom era with two innovative technologies, 
RibbonFET, and PowerVia. In RibbonFET, Intel had developed a gate-all-around 
transistor, the company’s ﬁrst new transistor architecture since FinFET in 2011. Intel 
said it would deliver faster transistor switching speeds while achieving the same drive 
current as multiple ﬁns, with a smaller footprint. 
PowerVia was Intel’s implementation of backside power delivery, which the 
company said would optimize signal transmission and eliminate the need for power 
routing on the front side of the wafer. 
5.2.1.1
Fins to Sheets 
A 3D stack concept called FinFET was introduced into the fabrication process in 
2015, involving ﬁeld-effect transistors (FETs) with ﬁns to improve transistor efﬁ-
ciency. The industry’s ﬁrst 25 nm transistor running on just 0.7 V was presented in 
December 2002 by TSMC. Then, in 2004, Samsung showed a “bulk FinFET” design, 
which made it possible to mass-produce FinFET devices. 
The FinFET design served the industry well until 2023 when it moved towards 
what was known as nanosheets. From 2022 or 2023 onward, the leading semicon-
ductor manufacturers, IBM, Intel, Samsung, and TSMC, accepted that there needed 
to be a gradual transition from the workhorse FinFET transistor architectures to 
nanosheet-like architectures to produce the logic chips of the 3 nm or 2 nm technology 
generations. 
From nanosheets, the next transition would be to forksheets, and then to CFET. 
The forksheet derived its name from its tri-gate architecture, which took the form of 
a fork [1] (Fig. 5.6). 
CFETs could be beneﬁcial for future logic and SRAM area scaling and complete 
the nanosheet device architecture family as the ultimate CMOS device architecture. 
All those technological innovations would be going on the fabs of the memory 
manufacturers as well.
Fig. 5.6 The road map for transistor design, from FinFET to CFET. Reproduced with permission 
from IMEC 

156
5
The GPU Environment—Hardware
5.2.1.2
GPU Memory 
The ability to move data in an out of memory has a critical effect on graphics controller 
performance. Graphics controller memory ﬁrst evolved from a simple DRAM to 
extended data out (EDO) and faster static RAM (SRAM). The subsequent evolutions 
were synchronous graphics RAM (SGRAM), video RAM (VRAM), Windows RAM 
(WRAM), and Rambus DRAM (RDRAM). GPUs have used high bandwidth memory 
(HBM) and graphics DDR (GDDR). 
The suppliers of graphics controllers and then GPUs constantly searched for the 
fastest possible memory. GPU suppliers were (and still are) risk-takers when it came 
to trying new technologies for memory. The following are some of the types of 
memories used in controllers and GPUs. 
The early graphics boards used VRAM (1980) and fast page mode DRAM (FPM) 
in the early 1990s. In the mid-1990s, EDO RAM became available, eliminating wait 
states by keeping the output buffer active until the next cycle began, which was bene-
ﬁcial for graphics operations. SDRAM superseded EDO memory and then Rambus 
(RDRAM) in 1997–8. Rambus DRAM (RDRAM) was developed in the 1990s for 
high bandwidth applications and was positioned by Rambus as a replacement for 
various types of contemporary memories, such as SDRAM. 
Incorporated in 2003, Rambus announced that Toshiba and Elpida Memory would 
produce its new memory technology, known as XDR DRAM 6. That memory could 
run at 3.2 GHz, faster than any other available at the time. 
Products such as the Nintendo 64, Microsoft’s Talisman 3D graphics chipset, 
Creative Labs Graphics Blaster 3D graphics AIBs for PCs, workstations manufac-
tured by Silicon Graphics, and Intel’s system memory chipsets for PCs integrated 
Rambus RDRAM technology. Rambus had participated in the Joint Electron Device 
Engineering Council (JEDEC) standards body but did not disclose critical parts of its 
technology and patent status to the group. Rambus was ﬁercely protective of its tech-
nology and between 2007 and 2012 the company spent ﬁling and ﬁghting lawsuits, 
having its patents dismissed and reinstated, and becoming a pariah in the industry 
[2–4]. 
Nystedt, D. Rambus sues Broadcom, Nvidia, four others, over patents, Comput-
erworld, (December 2, 2010), https://www.computerworld.com/article/2514660/ram 
bus-sues-broadcom-nvidia-four-others-over-patents.html. 
In 2013, Ronald Black, CEO of Rambus, said, “Somehow we got thrown into the 
patent troll bunch… This is just not the case” [5]. Unfortunately, that was the image 
the company had acquired. 
When Samsung introduced Graphics Double Data Rate (GDDR) in 2000, it 
quickly became the preferred memory for GPU suppliers. Although it had similari-
ties with the DDR SDRAM used as system memory, GDDR had several distinctions 
that make its performance much faster:
●GDDR had much higher bandwidth due to a wider memory bus.
●GDDR1 could send 16 data bits, compared to DDR1’s 9 bits.

5.2 Semiconductor Technology
157
●GDDR could request and receive data in the same clock cycle, while DDR could 
not.
●GDDR used less power and therefore less heat, allowing a simpler cooling system 
and higher performance.
●GDDR3 typically used 256- and 512-bit buses across 4–8 channels, while DDR1, 
DDR2, and DDR3 have a 64-bit bus (or 128-bit in the dual-channel version). 
ATI developed the GDDR3 speciﬁcation with DRAM vendors, including Elpida 
Memory, Micron, Hynix Semiconductor, and Inﬁneon. Later JEDEC adopted it as a 
standard. 
Although GDDR3 was designed by ATI, Nvidia was the ﬁrst to use the tech-
nology in 2004 on the GeForce FX 5700 Ultra. The next AIB to use GDDR3 was 
Nvidia’s GeForce 6800 Ultra, and ATI then began using the memory in its Radeon 
X800 AIBs. GDDR3 was Sony’s choice for the graphics memory of the PlaySta-
tion 3 gaming console. However, its Nvidia-based GPU could also access the main 
system memory, which consisted of an XDR DRAM designed by Rambus Incorpo-
rated (Nvidia marketed a similar technology as TurboCache for PC platform GPUs). 
Microsoft’s Xbox 360 had 512 MB of GDDR3 memory, and Nintendo’s Wii also 
had 64 MB of GDDR3 memory. 
Table 5.1 shows the history of memory developments (Table 5.2).
Memory speed, bus width, and power consumption represent signiﬁcant chal-
lenges for a GPU designer and probably always will. The old saying goes, “you 
cannot be too thin or too rich,” or have too much memory in computer graphics. 
Table 5.1 Types and 
generations of memory 
Type
Introduced
Memory clock 
rate (MHZ) 
Bandwidth (GB/s) 
DDR
2000
200–400
1.6–3.2 
GDDR1
2000
300
1.2 
DDR2
2003
400–1066.67
3.2–8.533 
GDDR2
2002
400–800
2.0 
GDDR3
2003
800–1600
25 
GDDR4
2005
3000–4000
160–256 
DDR3
2007
800–2133.33
6.4–17.066 
GDDR5
2007
1000–2000
288–336.5 
HBM
2011
250–1000
512–1024 
DDR4
2012
1600–4866
12.8–25.6 
GDDR5X
2015
1000–1750
160–673 
GDDR6
2018
1365–1770
336–672 
GDDR7
2023
3200
532

158
5
The GPU Environment—Hardware
Table 5.2 Comparison of GDDR5, GDDR5X, HBM, and HBM2 memory 
Memory
GDDR5
GDDR5X
HBM
HBM2 
Manufacturer
Samsung, Hynix, 
Elpida 
Micron
Hynix, Samsung 
Samsung, Hynix 
Appearance
Square/rectangular 
chip 
Square/rectangular 
chip 
Cube/cuboid
Cube/cuboid 
Maximum 
capacity 
8 GB per  die
16 GB per die
1 GB per stack
4 GB/8 GB per  
stack 
Maximum 
speed 
8 Gbps
10 to 14 Gbps (16 
Gbps in future) 
1 Gbps
2.4 Gbps 
Bus width
32 bits per chip
64 bits per chip
1024 bits per 
stack 
1024 bits per 
stack or more 
Power 
consumption 
Low
Same/lower than 
GDDR5 
Lower than 
GDDR5 and 
GDDR5X 
Lower than 
HBM 
Graphics AIBs 
used in 
Many graphics 
AIBs from budget, 
mid-range to 
high-end, e.g., GT 
740, GTX 1070, 
RX 480, etc 
GeForce GTX 
1080, GTX 1080 
Ti, GTX 1060, 
Nvidia Titan X 
(Pascal) 
Radeon R9 Fury 
X, Radeon Pro 
Duo 
Nvidia Tesla 
P100, Nvidia 
Quadro GP100, 
Radeon RX 
Vega 56, 
Radeon RX 
Vega 64, Nvidia 
Titan V, AMD 
Radeon VII
5.2.1.2.1
Shared Versus Private Memory 
Zero-copy is a computer operation where the CPU does not copy data from one 
memory area to another, see Fig. 5.7. Zero-copy saved CPU cycles and memory 
bandwidth.
However, zero-copy between GPU and CPU was not possible since both proces-
sors had different types of memory, and data had to be copied from one to the other 
to be shared. 
In an integrated graphics subsystem with partitioned main memory, part of the 
main memory was allocated exclusively to the iGPU, as shown in Fig. 5.8.
However, a zero-copy operation was impossible, and data got copied over the 
system memory bus, from one partition to another. 
An integrated graphics subsystem with uniﬁed main memory was in AMD’s 
Kaveri APU and the Sony PlayStation 4.

5.2 Semiconductor Technology
159
Fig. 5.7 A typical computer architecture with a graphics AIB connected via PCI Express
Fig. 5.8 Typical integrated 
GPU with allocated system 
memory
5.2.1.3
Memory Type and GPU 
Figure 5.9 shows the various platforms GPU are used in and the little red boxes 
indicate the type of memory they use on those platforms, dedicated (i.e., private) 
discrete memory, or shared (i.e., uniﬁed) memory.
Also shown are the GPUs’ output data devices. It is a screen in PCs and can also 
be a VR HMD. On the screen of a handheld mobile device, it is an integrated screen 
and can also be AR glasses. And in the data center, the server, the cloud, it is the 
network or what is known as the cloud. In that case, examples being Google’s Stadia 
or Nvidia’s GeForce Now, the GPU’s pixels are sent through the network to a client’s 
machine and displayed on the local display.

160
5
The GPU Environment—Hardware
Fig. 5.9 Types of GPU memory
5.2.2 
Chiplets 
A chiplet is one part (one chip) of a processing module composed of several chips 
to create a larger device. Instead of manufacturing a large processor in a single die,

5.2 Semiconductor Technology
161
chiplets allow semiconductor suppliers to employ several smaller chips to create a 
larger integrated circuit. 
The concept is not new and used to be called multi-chip modules (MCM). 
Multi-chips were used in the Intel Pentium Pro in 1995 and in IBM memory 
modules in the 1970s. 
Don Scansen of EE Times traces the introduction of the term chiplets to ﬁrst being 
mentioned (in a patent) in 1970 but becoming popular in 2010 [6]. 
AMD popularized the term chiplets in 2019 with their Ryzen Threadripper, and 
Epyc CPUs, (based on AMD’s Zen architecture). The term was extended to include 
multiple chiplets working together in a multi-chip module. 
Semiconductor suppliers use chiplets to meet production targets since there is less 
wastage compared to larger monolithic designs. 
Chiplets depend on an inter-processor communications substrate to create a single, 
uniﬁed integrated circuit. 
Companies that used MCM or chiplet designs developed their own die-to-die 
interconnect systems. However, as chiplets became popular for CPUs and GPUs, it 
became obvious an industry standard would be desirable allowing companies to use 
components from different suppliers to create heterogeneous SoCs. As discussed in 
Book three, Intel’s multi-chip Kaby Lake (2016) incorporated an Intel CPU chip and 
an AMD GPU chip in an MCM package. 
In January 2019, the Open Compute Project Foundation (OCP) proposed an Open 
Domain-Speciﬁc Architecture (ODSA) for SoC designs shown in Fig. 5.10. As a  
result, the OCP created an ODSA sub-project within the foundation to deﬁne an open 
interface and architecture that would enable the mixing and matching of available 
silicon die from different suppliers onto a single SoC for data center applications. 
In 2019, Intel was the ﬁrst company to declare they would use a chiplet design 
with their Xe GPUs. (see Chapter sixteen, Intel Xe).
Fig. 5.10 Multi-chip module alternatives. Source OCP ODSA 

162
5
The GPU Environment—Hardware
In January 2020, Nvidia introduced its Ampere GPU, the largest GPU ever built 
with 54.2 billion transistors in an 826 mm2 die. The company also ﬁled a patent on 
chiplet techniques (shown in Fig. 5.11), the writing was on the wall. Clearly, the 
future of semiconductor manufacturing would be based on chiplets. 
In July 2020, Intel developed its Foveros technology (Fig. 5.12) to connect chiplets 
on an MCM along with its embedded multi-die interconnect bridge (EMIB). 
Fig. 5.11 MCM-GPU: aggregating GPU modules and DRAM on a single package. Source 
ISCA/Nvidia 
Fig. 5.12 Intel’s die-to-die stacking is like the interposer technology in EMIB. Source Intel

5.2 Semiconductor Technology
163
Fig. 5.13 AMD’s The die-to-die interface uses a direct copper-to-copper bond with no solder 
bumps. Source AMD 
And in December 2020, AMD disclosed in a patent notiﬁcation a GPU chiplet 
array for linking processors via a high-speed interposer called a crosslink (see 
Fig. 5.13). According to the patent, AMD’s design connects two GPU chiplets to 
a CPU using a traditional communications bus while a passive crosslink provides a 
bridge to a second GPU chiplet. 
In early 2022, Intel announced that it would develop a new architecture codenamed 
Falcon Shores, an × 86, and a Xe GPU together in a Xeon socket—like AMD’s 
APUs. AMD later announced it too would employ chiplets for its APUs. Part of the 
motivation of Hector Ruiz and Dave Orton for the merger of AMD and ATI was 
the integration of the CPU and GPU now that strategy would become more widely 
adopted. 
In March 2022, Apple introduced its M1 Ultra with M1 Max chip dies joined 
within the same package using an interconnect technology. Apple called their die-
to-die interconnect technology Ultra Fusion. Apple claimed their Ultra Fusion archi-
tecture used a silicon interposer that had twice the connection density of any tech-
nology available. It connected over 10,000 signals and provided 2.5 TB per second 
of low-latency interprocessor bandwidth between the two-die using very little power. 
And also in March 2022, Intel and nine other companies formed a consortium 
called Universal Chiplet Interconnect Express (UCIe) with an open speciﬁcation 
that deﬁned an interconnect between chiplets within a package. It would, said the 
consortium, enable an open chiplet ecosystem and ubiquitous interconnect at the 
package level. The initial UCIe members were Advanced Semiconductor Engi-
neering (ASE), AMD, Arm, Google Cloud, Intel, Meta, Microsoft, Qualcomm, 
Samsung, and TSMC. Others were expected to join. 
As of this writing, Nvidia has not joined. At recent Nvidia’s GTC conference in 
March 2022, Jensen Huang was asked why that company had seemed to prefer the

164
5
The GPU Environment—Hardware
Fig. 5.14 Heterogeneous integration enabled by an open chiplet ecosystem. Source UCIe 
monolithic approach to chip design. Simply put, Huang said, monolithic is better. 
“You should always do a superchip before you do chiplets,” he said explaining that the 
design of the superchip helps deﬁne how to put chiplets to work. Nvidia depends on 
their NVLink-C2C technology to support their chiplet design. The UCIe consortium 
was formed to standardize an open, die-to-die interconnect system, which would 
enable companies to use components from various sources to create GPUs, CPUs, 
SoCs, and heterogeneous designs. UCIe 1.0 speciﬁed the die-to-die I/O physical 
layer, protocol stack, software model (including PCIe and CXL1 standards), and 
compliance testing.5.3 
Jim Pappas, the Intel director and CXL chair acting as a consultant to the UCIe consortium, 
said, “Intel put the pen to the [original] spec, but this is a consortium-owned spec at this 
point, and all future work on this is not [under the direction of] any one company. Any 
company can join (within the scope of U.S. laws).” 
“The nice thing about starting with a spec is that much of the early decisions have been 
made,” Pappas added. “You know what it is and where it’s going. The end result won’t 
necessarily be what was originally donated; it will evolve over time, it’ll get faster, it will 
have new capabilities, new protocols – the industry will evolve.” 
Besides the modularity it brings, implementing the standard speeds up the devel-
opment of modern technologies and lower manufacturing costs for semiconductor 
companies. A symbolic diagram of the chiplet concept is shown in Fig. 5.14.
1 Compute Express Link (CXL) is an open standard for high-speed central processing unit-to-device 
and CPU-to-memory connections. 

5.3 PC Bus Architectures
165
Fig. 5.15 Packaging options using UCIe. Source UCIe Consortium 
Chiplets give designers greater ﬂexibility, open new opportunities of reuse, and 
lower costs while improving performance, and reducing power consumption. 
“Moore foresaw this day. Now we believe chiplets are the key to extending Moore’s law 
through the next decade and beyond. Our consortium colleagues agree,” said Kurt Lender, 
IO Technology Solution Team Strategist at Intel. “And we’ll get to the next set of computing 
breakthroughs faster if we begin by settling on a well-deﬁned speciﬁcation. With Intel’s 
experience building these systems, we were able to donate a mature spec to the consortium 
that gave us a starting place.” 
Compute processors could be manufactured in the latest process node, while IO 
controller and memory chips could use a previous generation node. Not only did that 
offer optimization of semiconductor costs but also eliminated the costs of porting IP 
and validation over again. 
UCIe deﬁned an optimized physical layer in standard (2D) and advanced (2.5D) 
packaging choices, such as used by the founding companies Intel and TSMC and 
shown in Fig. 5.15. 
Moving to a chiplet architecture brought other beneﬁts to the industry, too. 
Customers were able to leverage different manufacturers more easily for any compo-
nent of their solutions. That motivated manufacturers to deliver new levels of quality, 
price, and customer service. The competition took place on a level playing ﬁeld, 
where products and services were the differentiators, not artiﬁcially constrained 
ecosystems, or technological incumbency. 
5.3 
PC Bus Architectures 
The ﬁrst microcomputer bus was the 2 MB/s S-100 bus, introduced in 1974. Five 
subsequent bus designs ended in 2003 with the abandonment of power-hungry buses 
and a shift towards point-to-point high-speed serial communication systems. As 
illustrated in Fig. 5.16 [7].

166
5
The GPU Environment—Hardware
Fig. 5.16 The development of graphics interconnect systems over time 
By 2021, the communication speed had reached 126 GB/s and doubled in 2024– 
25. and would double in the 2024–5-time frame. In the following sections, there is a 
brief history of the stages in this evolution. 
Table 5.3 shows the history of bus introductions and their characteristics.
PC AIB buses physically and electrically evolved for 20 years from 1984 to 2004 
when PCIe was introduced as illustrated in Fig. 5.17. Thereafter, only the bandwidth 
evolved but the basic physical construction of an AIB remained the same—PCI-
Express versions are backward compatible.
Notice how the connector tab of the AIBs moved from the AIB’s back panel 
(where the video output connector is) and how the size and number of pins changed. 
That made it impossible to plug an AIB from one generation into another generation’s 
socket. Cynically, one might say that was planned obsolesce. But the reality was that 
it was the evolution of technology, understanding and self-protection to keep users 
from damaging the AIBs they had.

5.3 PC Bus Architectures
167
Table 5.3 PC bus standards 
Bus
Width (bits) Clock rate (MHz) Bandwidth 
(MB/s) 
Style 
ISA XT
8
4.77
8
Parallel 
ISA AT
16
8.33
16
Parallel 
MCA
32
10
20
Parallel 
NUBU.S
32
10
10–40
Parallel 
EISA
32
8.33
32
Parallel 
VESA
32
40
160
Parallel 
PCI
32–64
33–100
132–800
Parallel 
AGP 1x
32
66
264
Parallel 
AGP 2x
32
66
528
Parallel 
AGP 4x
32
66
1000
Parallel 
AGP 8x
32
66
2000
Parallel 
PCIe ×1
1
2500/5000
250/500
Serial 
PCIe × 4
1 × 4
2500/5000
1000/2000
Serial 
PCIe × 8
1 × 8
2500/5000
2000/4000
Serial 
PCIe × 16
1 × 16
2500/5000
4000/8000
Serial 
PCIe ×1 2.0  (https://en.wikipedia.org/ 
wiki/Video_card-cite_note-55) 
1
500/1000
Serial 
PCIe × 4 2.0
1 × 4
2000/4000
Serial 
PCIe × 8 2.0
1 × 8
4000/8000
Serial 
PCIe ×16 2.0
1 × 16
5000/10000
8000/16000
Serial 
PCIe × 1 3.0
1
1000/2000
Serial 
PCIe × 4 3.0
1 × 4
4000/8000
Serial 
PCIe × 8 3.0
1 × 8
8000/16000
Serial 
PCIe × 16 3.0
1 × 16
16000/32000 Serial
5.3.1 
Industry Standard Architecture: 1981 
The 8-bit ISA bus came with the ﬁrst IBM PC in 1981, and the oldest implementation 
was in the form of an 8-bit bus running at 4.77 MHz. The ISA underwent a few 
upgrades, ﬁrst to 16-bits, and then through speed increases, from 6 to 8 MHz and 
again to 8.33 MHz. ISA would be included in new systems for legacy support for 
older devices, although it was on its last legs.

168
5
The GPU Environment—Hardware
Fig. 5.17 The physical evolution of graphics AIBs over time. Source Free Online Dictionary of 
Computing
5.3.2 
Micro Channel Architecture: 1987 
Introduced by IBM, MCA was a 32-bit architecture with plug-and-play and bus 
mastering. Only IBM PS/2 systems and a few other companies used it because IBM 
charged other companies to pay a royalty fee to use it. MCA came out around the 
same time as EISA, but neither of those formats was very successful in the computer 
industry. 
5.3.3 
Extended ISA: 1988 
the ISA bus had a ﬁnal improvement which increased it to 32 bits wide while operating 
at the same frequency of 8.33 MHz to ensure backward compatibility with old ISA 
devices. Developed by Compaq, EISA extended the bandwidth of the aging and 
slow ISA. It featured double the bandwidth of the ISA bus (and nearly the same 
as the MCA), IRQ sharing, and a basic plug-and-play implementation. EISA was a

5.3 PC Bus Architectures
169
signiﬁcant step in the right direction, but it was never seen as a practical solution 
because it suffered from numerous technical problems. 
5.3.4 
VESA Local Bus: 1992 
The VL-Bus was an innovative technology that tapped directly into the front-side bus, 
rather than communicating with the CPU via the core logic chipset like other buses. 
The standard was stringent because those devices had to be very well behaved, and 
there was no chipset to moderate the devices. Although directly connecting devices 
to the CPU would have provided a fast connection, it would have placed too high a 
demand on the CPUlimited the speed the bus could run and limited the number of 
VL-Bus devices in each system to two. 
The speed of that bus was the same as for the front-side bus, which in most 
486 designs meant a 32-bit 33 MHz bus. VL-Bus AIBs plugged into the mother-
board through regular ISA slots but had a special connector that required a dedicated 
connector on the motherboard so the AIB could gain access to the front-side box. 
After the VL-bus was adopted, Dell contacted some VESA members and claimed 
it had obtained a patent for VL-bus in 1991 and that they were violating it by using 
the VL-bus standard. 
Then in 1995, in a precedent-setting decision, Dell agreed to drop patent claims 
affecting millions of PCs using the VL-bus. The decision followed Federal Trade 
Commission (FTC) charges that Dell hampered competition in the PC industry 
and undermined the standard-setting process by threatening to exercise undisclosed 
patent rights against computer companies adopting the VL-bus standard. Dell agreed 
not to enforce its patent rights against computer manufacturers using the VL-bus to 
settle the FTC charges [8]. 
Dell was a VESA member when the organization began setting a standard for 
a computer bus for faster graphics performance. Almost all major U.S. computer 
hardware and software manufacturers voted to approve the VL-bus standard in 1992. 
And Dell was part of that approval. A Dell representative said he did not know of 
any patent, trademark, or copyright the bus design would violate. 
5.3.5 
Peripheral Component Interconnect: 1992 
Intel had been working on a design for a new PC peripheral bus which they called the 
local glueless bus2 as a replacement for the VL-Bus. Within Intel, it was concluded 
the design was sound and valuable. Intel invited four other companies (DEC, IBM, 
Compaq, NCR) to help make it an industry and open standard; they called it PCI.
2 A glueless interface is the term used when an extra interface circuit of some kind is not needed to 
connect two ICs or circuit blocks, that is, they have the same I/O speciﬁcations. 

170
5
The GPU Environment—Hardware
Fig. 5.18 A typical VL AIB. Source Wikipedia 
In June 1992 the consortium launched the PCI Special Interest Group. Jim Pappas, 
who would later be known as the father of USB, was DEC’s representative. In June 
1994, Pappas joined Intel Labs and shortly thereafter led the team that developed 
USB. Then in 1997, Pappas left the labs and joined Intel’s Desktop Products group 
under Pat Gelsinger and drove all of his desktop technology initiatives. 
Notice, in Fig. 5.20, how the orientation of the AIB changed from right to left 
(see Fig. 5.18). When PCI was introduced, network interfaces (ethernet, Arcnet, Star, 
etc.) where AIBs, and network boards were known as a network-interface card (NIC). 
Printer interfaces were also in use then, and other specialized I/O devices. Users had 
such boards, and they were ISA-based. Since PCI was new, PCI I/O boards hadn’t 
been introduced yet. The PC suppliers didn’t want to obsolete the user’s I/O boards 
so they built the motherboard with one or two ISA slots, a PCI slot, and in between 
the PCI and ISA slots a special slot that could accommodate either a PCI AIB or an 
ISA AIB, as shown in Fig. 5.19.
Rather than connecting components directly to the FSB like the VL-Bus, the 
PCI bus used a special connection through the northbridge, which still allowed it 
dedicated access to the CPU and main memory. The PCI bus was faster than the 
VL-Bus and did not suffer the electrical problems that plagued the VL-Bus. Those 
factors meant that it caught on quickly and was used in all x86 systems. 
When Intel introduced PCI 2.0, it had a 32-bit 33 MHz bus, in the same way as 
the VL-Bus, except that the front-side bus operated at twice that speed, i.e., 66 MHz. 
The PCI 2.1 speciﬁcation was for a 66 MHz 32-bit bus. That doubled the effective 
speed of the bus and required limited hardware modiﬁcations to achieve. Many older 
video AIBs got built for that format, but it later evolved into the Accelerated Graphics 
Port (AGP). 
The PCI 2.1 speciﬁcation called for a 66 MHz 64-bit bus used in high-end servers 
for connections like ﬁber channel, SCSI, and Ethernet.

5.3 PC Bus Architectures
171
Fig. 5.19 A riser card with a black slot for ISA and a white slot for PCI AIBs 
Fig. 5.20 A typical PCI AIB. Source Wikipedia

172
5
The GPU Environment—Hardware
5.3.6 
Accelerated Graphics Port: 1997 
Unlike hard drives or audio boards, graphics AIBs require frequent access to main 
memory, along with large texture transfers. The original accelerated graphics port 
(AGP) speciﬁcation introduced by Intel only involved a dedicated 66 MHz PCI bus 
for the graphics AIB and was very similar to the PCI 2.1 speciﬁcation. In addition 
to its extra speed and dedicated memory access, AGP had other beneﬁcial features 
such as AGP texturing, sidebanding, write combining, and fast writes. (Sideband 
addressing opened up another side channel in the AGP bus which fed commands 
through, instead of them having to go along the normal AGP bus which could cause 
bottlenecks. Fast writes was an algorithm which did as its name implies, it wrote 
data to the video faster so that it could display faster.) 
AGP allowed textures to be accessed directly from system memory during 
rendering rather than pre-fetched to local graphics memory [9]. The original AGP 
interface only operated as a 32-bit 66 MHz bus, and Intel quickly revised it to a 32-bit 
133 MHz bus, which was capable of two complete transfers per clock cycle. As a 
result, it got named AGP 2x since it was twice as fast as the original (Table 5.4). 
The AGB bus grew physically and electrically adding little keyways to prevent 
older AIBs from being able to plugging into the newer sockets while maintaining 
backward compatibility, see Fig. 5.21.
Although the bandwidth of the AGP increased from 254 to 508 MB/s, even that 
was not enough, Intel expanded it to make use of four complete transfers per clock 
cycle in the AGP 4x speciﬁcation (Table 5.5). The 4x version had a 32-bit width and 
a bandwidth of 266 MHz up to 1007 MB/s.
By 2010, few new motherboards had AGP slots, and no new chipsets had AGP 
compatibility. 
Instead, some peripherals from that period used the new PCI Express, a general 
purpose standard (i.e., not restricted to graphics) that supported higher data transfer 
rates and full-duplex. AIBs had a PCIe-to-AGP bridge chip to convert PCIe signals 
to and from AGP signals to achieve AGP compatibility. That increased the cost due 
to the need for an additional bridge chip and a separate AGP-designed circuit board. 
5.3.7 
Peripheral Component Interconnect Express: 2003 
First discussed by Louis Burns, Intel vice president, at the Intel Developer’s Forum 
(IDF) in February 2001, the Peripheral Component Interconnect Express became 
part of the PCI-SIG with the released 1.0 speciﬁcation in June 2003. 
In its early developmental stages, PCIe, initially referred to as HSI for high-
speed interconnect, underwent a name change to 3GIO (third-generation I/O) before 
the ﬁnal PCI-SIG name of PCI Express. Version 1.0 of the PCI Express, ofﬁcially 
abbreviated as PCIe or PCI-e, came out in 2003 (Fig. 5.22).

5.3 PC Bus Architectures
173
Fig. 5.21 AGP signaling and power conventions. Reproduced with permission from JigPu at 
English Wikipedia 
Table 5.4 AGP and PCI: 32-bit buses operated at 66 and 33 MHz, respectively 
Speciﬁcation
Voltage (V)
Clock (MHz)
Speed
Transfers/clock
Rate (MB/s) 
PCI
3.3/5
33
–
1
133 
PCI 2.1
3.3/5
33/66
–
1
133/266 
AGP 1.0
3.3
66
1×
1
266 
AGP 1.0
3.3
66
2×
2
533 
AGP 2.0
1.5
66
4×
4
1066 
AGP 3.0
0.8
66
8×
8
2133 
AGP 3.5°
0.8
66
8×
8
2133

174
5
The GPU Environment—Hardware
Table 5.5 AGP power provisioning 
lot Type
3.3 V
5 V
12 V
3.3 V Aux
1.5 V
3.3 Va
12 Va
Total power 
AGP
6 A
2 A
1 A
0.375 mA
2 A
–
–
48.25 Wb 
AGP Pro110
7.6 A
9.2 A
50 to 110 W 
AGP Pro50
7.6 A
4.17 A
25 to 50 W
Fig. 5.22 Jim Pappas. 
Source Intel 
Released in 2007 by Intel, Jim Pappas from Intel championed PCIe 2.0. It had 
double the bandwidth of 1.0 and offered better ﬂexibility while keeping compatibility 
with PCIe 1.1. It seemed like pure magic at the time, as it provided a 5 GT/s transfer 
rate with a throughput of 8 GB/s with 16 lanes. When asked how far he thought they 
could push PCI and stay on copper, Pappas, an engineer’s engineer, straightforward 
and thoughtful, rubbed his chin (a chin that was level with most people’s eyebrows) 
and said, “I don’t know, maybe 2010?” 
Twelve years later, PCIe was still on copper, but in 2022 PCIe 6.0 could push data 
at 128 GB/s, almost 16 times as fast. How did—or how do—they do it? Some new 
lower dielectric materials (for the motherboard) and good engineering. Add to that 
the development of innovative timers, switches, repeaters, and tuned up the pluming. 
Intel Fellow Dr. Debendra Das Sharma created a terriﬁc diagram of the evolution 
of the pluming and performance gains from PCIe1.0 to 5.0, shown in Fig. 5.23.
The PCIe SIG [10] (special interest group) governs and maintains the speciﬁca-
tion. They target a 3-year cadence to double the bandwidth between speciﬁcation 
generations. Due to market demand and varying development timelines, sometimes 
the speciﬁcation release timing shifts. However, PCI-SIG is on track to maintain our 
3-year cadence of doubling the bandwidth with the upcoming PCIe 6.0 speciﬁcation. 
I’ve attached a chart to better highlight the PCI-SIG road map (Figs. 5.23 and 5.24).
While PCIe 4.0 products were currently shipping, the PCI-SIG had been working 
on the PCIe 5.0 speciﬁcation at 32 GT/s, which far exceeded the initial expectations

5.3 PC Bus Architectures
175
Fig. 5.23 Evolution of PCIe technology. Reproduced with permission from Intel
Fig. 5.24 PCIe results and road map. Reproduced with permission from PCI SIG
for that technology. In addition to those speed increases, the PCIe speciﬁcation has 
provided a protocol and power enhancements that keep up with the trends in device 
design. 
As PCIe technology has evolved, it has been integrated into the CPU, making 
it a ubiquitous interconnect that can deliver high bandwidth with low latency and 
power. PCIe has been employed in many applications, such as storage, high-speed

176
5
The GPU Environment—Hardware
Fig. 5.25 PCIe form factors. Reproduced with permission from Intel 
I/O, and, of course, graphics. It has found its way into mobile devices, HPCs, and all 
PCs (Fig. 5.25). 
Developed in the early 1990s as a bus-based I/O interface, the PCI architecture has 
proven very robust. It underwent width and speed increases while sustaining the I/O 
needs of the rapidly changing computing industry over more than a decade. After 
hitting a performance wall with the bus-based PCI architecture, PCI-SIG moved 
to a serial, point-to-point, full-duplex, and differential interconnect link called PCI 
Express (PCIe) and released the PCIe 1.0 speciﬁcation in 2002. 
PCIe freed GPUs from the tyranny of buses, with a scalable architecture that is 
still delighting and surprising us years later, with no end in sight. 
Today, the PCIe architecture is well positioned to continue as the ubiquitous I/O for 
a wide range of computing applications in the near future. Its strength stems from its 
open standard, backed by the collective expertise of 730+ member companies. The 
organization has had a rich and successful experience navigating several technology 
transitions over three decades. Its history would ensure that this technology would 
offer open, scalable, cost-effective, power-efﬁcient, and leading-edge solutions with 
multi-generational relevance across all market segments and usage models in the 
computing continuum. 
5.3.8 
Other I/O 
The power and space needed for a powerful GPU became a limiting factor in how 
powerful a dGPU could be put in a notebook. Hardware developers experimented 
with bringing out PCIe lines. However, the complexities of cabling, connectors, and 
line drivers proved too expensive and cumbersome to be effective (Fig. 5.26).

5.4 GPU Video Outputs
177
Fig. 5.26 The ﬁrst eGPU: ATI’s XGA (2008) 
Fujitsu offered a notebook they called the Amilo GraphicsBooster, but it wasn’t 
a very successful product. 
The problem was somewhat solved with the introduction of the high-speed serial 
input–output (I/O) communications technology, Thunderbolt, which would also be 
called Mini DisplayPort—IEEE 1394 (2011) and then USB-C (2017). The new spec-
iﬁcation made it possible to support an external GPU by sending PCIe signals over 
a low-cost, high-bandwidth cable and connector. However, the additional case and 
power supply kept the price high. The new speciﬁcation did, of course, revolutionize 
the design of laptops with a standardized input allowing more streamlined designs 
with less bulky I/O ports and one consistent spec as will be discussed in more detail 
in this chapter. 
5.4 
GPU Video Outputs 
The early microcomputers had built-in displays (e.g., Commodore PET, RadioShack 
TRS80) or used TV studio monitors with analog RGB inputs. When IBM introduced 
the PC, it came with a detachable monitor connected via a nine-pin digital interface 
connector (DE-9). It also had RGB, but in a binary mode, i.e., on or off. Nonetheless, 
it could produce 16 colors. 
Displays evolved to analog signals, red, blue, and green (and sometimes a sync 
signal) and could display 16 million shades. 
Next came the ﬁrst digital signals, DVI, then HDMI, and ﬁnally DisplayPort— 
discussed brieﬂy in the following sections. 
But ﬁrst, a brief discussion about bits and colors. 
A frame buffer with a depth of 8-bits (8-bits for each each primary color - red, blue, 
green) can generate 2×1024, or 16.8 million unique shades or colors, as discussed

178
5
The GPU Environment—Hardware
Fig. 5.27 BT.709, sRGB, SMPTE 1886 (Gamma 2.4) = today’s digital content, BT.2020, SMPTE 
2084 (PQ) = HDR Content’s color container. Source SMPTE 
in Fig. 5.27. Higher quality color depth such as high dynamic-range (HDR) color 
requires 10- or 12-bits of color depth. A 10-bit frame buffer can provide 210, or 1.07  
billion colors, while a 12-bit frame buffer can offer 212, or 68.7 billion colors. But 
what does that mean? 
A display mixes primary colors red, blue, and green. Consider a system with 8-bit 
color. Each primary has 8-bits, 28 combinations, or 256 levels (or shades, colors, or 
luminance). In other words, the base value of zero is no color, black, no illumination. 
The max, 255, is the brightest possible representation of the primary color. 
A screen (monitor or display) mixes those primary colors, so mixing red with 
blue makes purple. How much it is pure purple vs. reddish purple or bluish purple 
depends on the value of each primary. If it is 128 red and 255 blue, it will be very 
bluish purple. 
Since the primary colors get mixed, their combinations are the product, so our 
maximum bright purple, 255 red, and 255 blue would give us 255×255 combinations 
or shades as the primary red and blue values varied from 0 to 255. When green gets 
added to the mix, we get 2553, 255 cubed and 16.777 million combinations or shades 
or colors.

5.4 GPU Video Outputs
179
5.4.1 
VGA: 1987 
The Professional Graphics Controller (PGC), introduced in 1984, launched analog 
signaling with a more extensive color palette of 4096 colors to the PC. And in 
1987, IBM introduced the ubiquitous Video Graphics Array (VGA) adaptor with a 
256-color palette. 
Independent companies such as ATI and Hercules offered add-in boards (AIBs) 
with color palettes of 4096, while high-end boards that used the Texas Instrument 
TMS 34010 offered 16 million color capabilities. 
Monitors came with three or four BNC connectors (RGB and sync, although 
the sync signal could be inserted into the green line). They also included a VGA 
connector, which became the standard, and analog monitors could display between 
1600×1200 (1.9 Mpixels just under HD) and 2000×2000 (4 Mpixels, twice HD) 
with 16 million colors at a refresh rate of 60 Hz. 
5.4.2 
DVI (1999–) 
In the late 1990s, just as GPUs emerged, thin, ﬂat-panel LCD screens were becoming 
popular. LCD screens were inherently digital, so the monitor needed an analog-to-
digital converter (A/D) added to it to accept an analog RGB signal. That drove up 
costs, and a frustrating situation arose in which the graphics AIBs were also digital 
and had to employ a digital-to-analog (D/A) converter to drive the monitor. 
In 1999, the Digital Display Working Group (DDWG) introduced the Digital 
Visual Interface standard. To provide backward compatibility, suppliers of LCD 
monitors now had to include two signaling capabilities, DVI and VGA. The DVI 
speciﬁcation incorporated backward compatibility with VGA in terms of the signal 
but did not include a physical connector capability, thus requiring adaptor cables. 
5.4.3 
HDMI (2002–) 
In this period, High-Deﬁnition (HD) content was beginning to enter the consumer 
space, as were DVD movies. A new standard was proposed and introduced in 
2002 called High-Deﬁnition Multimedia Interface (HDMI). That was a proprietary 
audio/video interface that included content protection to prevent copying of movies 
from DVDs. HDMI monitors could also accept a DVI signal with no quality loss via 
a special adaptor cable. HDMI could also carry up to eight 192 kHz audio channels. 
The HDMI founders were Hitachi, Panasonic, Philips, Silicon Image, Sony, 
Thomson, and Toshiba. Digital Content Protection, LLC provided HDCP (which 
was developed by Intel) for HDMI. HDMI also had the support of motion picture

180
5
The GPU Environment—Hardware
producers Fox, Universal, Warner Bros., and Disney, along with system operators 
like DirecTV. 
As screen resolutions increased, the HDMI speciﬁcation expanded, as shown in 
Table 5.6. 
HDMI was considered a consumer electronics interface, and computer graphics 
users and games continued to use DVI and even VGA. HDMI also required licensing, 
which increased the cost.
Table 5.6 Formats and versions of HDMI video (Wikipedia) 
Name
Resolution Refresh Data rate
1.0–1.1
1.2–1.2a 1.3–1.4b 
2.0–2.0b 2.1 
Rate 
(Hz) 
3.96 
Gbit/s 
3.96 
Gbit/s 
8.16 
Gbit/s 
14.4 
Gbit/s 
42.6 
Gbit/s 
Introduced
Dec 
2002-May 
2004 
Aug 
2005 
Jun 
2006-Jun 
2009 
Sep 
2013 
Nov 
2017 
720p
1280×720
30
720 Mbit/s
Yes
Yes
Yes
Yes
Yes 
60
1.45 Gbit/s
Yes
Yes
Yes
Yes
Yes 
120
2.99 Gbit/s
No
Yes
Yes
Yes
Yes 
1080p
1920× 
1080 
30
1.58 Gbit/s
Yes
Yes
Yes
Yes
Yes 
60
3.20 Gbit/s
Yes
Yes
Yes
Yes
Yes 
120
6.59 Gbit/s
No
No
Yes
Yes
Yes 
144
8.00 Gbit/s
No
No
Yes
Yes
Yes 
240
14.0 Gbit/s
No
No
4:02:00
Yes
Yes 
1440p
2560× 
1440 
30
2.78 Gbit/s
No
Yes
Yes
Yes
Yes 
60
5.63 Gbit/s
No
No
Yes
Yes
Yes 
75
7.09 Gbit/s
No
No
Yes
Yes
Yes 
120
11.6 Gbit/s
No
No
4:02:02
Yes
Yes 
144
14.1 Gbit/s
No
No
4:02:00
Yes
Yes 
240
24.62 
Gbit/s 
No
No
No
4:02:00
Yes 
4 K
3840× 
2160 
30
6.18 Gbit/s
No
No
Yes
Yes
Yes 
60
12.54Gbit/s No
No
4:02:00
Yes
Yes 
75
15.8 Gbit/s
No
No
4:02:00
4:02:02
Yes 
120
25.8 Gbit/s
No
No
No
4:02:00
Yes 
144
31.3 Gbit/s
No
No
No
No
Yes 
240
54.8 Gbit/s
No
No
No
No
DSC 
5 K
5120× 
2880 
30
10.9 Gbit/s
No
No
4:02:02
Yes
Yes 
60
22.1 Gbit/s
No
No
No
4:02:00
Yes 
120
45.6 Gbit/s
No
No
No
No
DSC 
8 K
7680× 
4320 
30
24.4 Gbit/s
No
No
No
4:02:00
Yes 
60
49.6 Gbit/s
No
No
No
No
DSC 
120
102. Gbit/s
No
No
No
No
DSC 

5.4 GPU Video Outputs
181
The ﬁrst AIB supplier to offer a graphics board with HDMI was Sapphire in 
June 2006 [11]. In addition to the HDMI port, the board had a VGA connector for 
backward compatibility and came with a VGA-to-DVI adaptor. By 2006, HDMI had 
reached version 1.3. 
HDMI established a new forum and HDMI 2 was the ﬁrst update from it. Since a 
different body was managing it, they raised the version number from 1 to 2, reﬂecting 
the notable change. 
In 2022 The HDMI forum released yet another version, HDMI 2.1a with mixed 
support from TV manufacturers, cable makers, and monitor suppliers for 120 Hz 
gaming on a console. HDMI 2.1a added Source-Based Tone Mapping (SBTM). 
SBTM was a HDR feature that ofﬂoaded some of the HDR tone mapping to the 
content source (like your computer or set-top box) alongside the tone mapping that 
the TV or monitor was doing. 
SBTM was not a new HDR standard and not designed to replace HDR10 or 
Dolby Vision. It was intended to help existing HDR setups perform more efﬁciently 
by letting the content source optimize what it passes to the display. 
The acceptance of SBTM was complicated by the other advances brought by 
HDMI 2.1, including variable refresh rates, automatic low latency connections, and 
the bandwidth necessary to offer up to 10 K resolution or 120 Hz refresh. In compar-
ison, SBTM was something manufacturers could support—but were not required to 
support. 
5.4.4 
High Dynamic Range (2015) 
Released on April 8, 2015, HDMI 2.0a added support for high dynamic range (HDR) 
video. 
High Dynamic Range (HDR) compatibility was introduced with 4 K displays, 
using 10-bit HDR color. HDR technology emerged in 2015 as the high bar for overall 
display quality. HDR panels were characterized by
●Brightness between 600 and1200 cd/m2 of luminance, with an industry goal to 
reach 2000.
●Contrast ratios that closely mirror human visual sensitivity to contrast (SMPTE 
2084).
●The Rec.2020 color gamut could produce over one billion colors at 10-bits per 
color. 
HDR displays exploited the great black depth of OLED displays or the vivid 
brightness of local dimming LCD. Both were suitable for gaming. TVs were already 
available, and consumer monitors came in the second half of 2016. Such displays 
would offer unrivaled color accuracy, saturation, brightness, and black depth—in 
short; they would come remarkably close to simulating the real world. 
Nvidia supported HDR with their GTX 900 series in September 2014, ahead of 
the ofﬁcial release of the speciﬁcation. AMD has supported HDR since 2017.

182
5
The GPU Environment—Hardware
The HDMI Forum announced HDMI 2.1 on January 4, 2017, and released it on 
November 28, 2017. That added support for higher resolutions and refresh rates, 
including 4 K 120 Hz and 8 K 120 Hz. HDMI 2.1 also introduced a new category 
of HDMI cable called Ultra High Speed (which was referred to as 48G during its 
development). 
5.4.5 
DisplayPort 
In May 2005, VESA, the video electronics standards organization (a consortium 
formed 16 years earlier by NEC and other monitor and graphics AIB companies), 
announced that it had begun work on a new display interface called DisplayPort. 
ATI, Dell, Genesis, HP, Molex, Nvidia, Philips Electronics, Samsung Electronics, 
and Tyco developed the DisplayPort interface proposal for VESA. One year later, the 
organization ratiﬁed the standard and offered it to the industry. In addition to video, 
it could also carry audio, USB, and other forms of data. 
Luminance in the real world 
Sunlight = 1,600,000,000 nits 
Fluorescent light = 10,000 nits 
Highlights = 1,000 to 10,000 nits 
White Range = 250 to 1,000 nits 
Most typical objects = 1 to 250 nits 
Shadow details = 0.01 to 1 nit 
Ultra Blacks = 0 to 0.01 nit  
Most Current PC displays = 0.1 to 250 nits max 
Excellent Current LCD HDTVs = 0.1 to 350-400 nits max 
HDR UHDTVs = new CE industry standard
●0.005 to 10,000 cd/m2 = Nits
●HDR LCDs w LED local dimming = peak 1K today going to 2K nits 
holiday’16
●HDR OLEDs = peak 500 today ti 1K nits holiday 16 with absolute blacks 
A “nit”, is a unit of measurement of luminous intensity approximately 
equivalent to one candle 
Thunderbolt and Thunderbolt 2 were different from Mini DisplayPort since 
although they had the same shape, they used other symbols on the cable and port. 
Thunderbolt 2 did work with displays that had Mini DisplayPort. Thunderbolt 2 
was also compatible with DisplayPort 2.1, but not its daisy-chaining capabilities.

5.4 GPU Video Outputs
183
5.4.6 
Seeing More 
In 1996, the best performance and resolution delivered by the CG industry was 
an image of 100,000 triangles, refreshed 30 times per second, on a 1024 ×768 
screen. By 2013, it had become possible to generate over 100 million triangles (also 
called polygons) at 60–120 times per second on screens with resolutions greater than 
HD (1920×1080). By 2015, UHD or 4 K displays were commonplace, offering a 
resolution of 3840×2160 and again posing challenges for hardware engines. 
In 2019, large-scale (40–49-inch), high-resolution (5120 ×1440), curved displays 
became available, and in 2021, Dell and LG introduced 5 k (5120×2160) curved 40-
inch HDR displays. Those big, bright, full-color displays allowed for fantastic quality 
and the minute details of images and models. 
5.4.7 
Virtual Reality Headsets 
A virtual reality (VR) head-mounted display (HMD) is, in the simplest terms, a 
display positioned on the user’s head, to enable an immersive experience. 
VR systems got used on various platforms, such as workstations for design and 
simulation projects. VR is also used on PCs for travel, consumer experiences, enter-
tainment, and on game consoles for all types of entertainment, with games being the 
primary application. 
However, VR HMDs differ from passive displays. They constantly send location 
and movement information back to the application—i.e., where the viewer is looking 
and how quickly their head is moving. That movement and the bandwidth needed 
to support it to provide a real-time experience were the main challenges of VR for 
PCs. That was also the cause of VR sickness (Fig. 5.28).
Some VR HMDs had small eye-tracking cameras trained on the user’s eyes to see 
where the user was looking. That was known as foveated rendering, and it reduced the 
resolution of the display towards the periphery, which saved bandwidth and allowed 
for a higher refresh rate. It was the refresh rate that was the key to combating VR 
sickness. 
VR HMDs had integrated sensors to monitor muscle movements, gaze, pupil size, 
and pulse and transfer the data to the platform. By capturing user responses in real 
time, an application could adjust each user’s experience. 
VR HMDs often had built-in headphones. Some of the major development issues 
when crafting a VR experience involve sound management, direction, and acoustic 
location. VR systems also employed hand tracking and haptic feedback, and the CPU 
processed those signals rather than the GPU. 
Due to bandwidth limitations and power requirements, VR HMDs had cables that 
tethered them to the platform. Although wireless VR HMDs have improved over the 
years, issues such as battery life, screen resolution, and bandwidth will continue to 
pose challenges for some time.

184
5
The GPU Environment—Hardware
Fig. 5.28 A virtual reality system
5.4.8 
Augmented Reality Glasses 
Although it was often confused with VR, augmented reality (AR) is a separate and 
distinct display system. Unlike VR, which removes the real world and immerses 
the user in a closed environment, AR overlays information on what the viewer sees 
via special glasses, or through a mobile device. Glasses, which would come to be 
called smart glasses had the advantage of allowing the wearer to see the world as 
they usually would (including vision correction lenses) with added information. 
Superimposed on the glasses is information that could provide navigation, alerts, 
entertainment, and bio-monitoring. AR promises to be the ultimate personal device 
when tied to a smartphone, as it can provide the same audio, visual, and physical 
information as a smartphone, but on a display right in front of one’s eyes, with the 
natural world behind it (Fig. 5.29).
Widely used for training, telepresence, gaming, informational uses, and ﬁnding 
animated characters in parks and living rooms, AR is one of the most disruptive 
technologies introduced in a long time. An AR system was more complex than a 
VR system in that a VR system had a single level of brightness in a conﬁned space, 
whereas an AR system had to deal with both high levels of ambient light (when 
outside) and low levels (when inside or at night). 
A VR system needs to track where the HMD is within a room and where the 
wearer is looking. 
An AR system must determine where the wearer is in the world, including their 
elevation (for example, in a tall building or a cave). An AR system also must create 
a map of the environment as the user moves about (simultaneous localization and 
mapping, or SLAM). VR HMDs have headphones, whereas AR smart glasses may

5.4 GPU Video Outputs
185
Fig. 5.29 An augmented reality system
have headphones (or bone-phones) and two microphones: listening to the wearer and 
the environment and anyone talking to the wearer (Fig. 5.30).
In addition, consumer-grade AR smart glasses need to be lightweight, inconspic-
uous, and affordable, with long battery life. 
5.4.9 
Mixed Reality Headsets 
VR HMD uses forward-looking cameras, it was known as a mixed reality (MR) 
device. The concept of mixing refers to a combination of AR and VR. Those systems 
got primarily used in design centers and for product demonstration (Fig. 5.31).
Some people have suggested that MR was the same as VR (mixing AR and VR). 
However, that view is a misreading of Paul Milgram’s and Fumio Kishino’s 1994 
deﬁnition (see Fig. 5.32) of AR as part of the continuum or middle ground between 
VR (completely synthetic) and telepresence (completely real) [12].
Telepresence is the experience of being there and was implemented in a remote 
control and display device for teleoperation. It was known as virtual presence 
(Fig. 5.33) in the context of computer-generated simulation. You may have seen

186
5
The GPU Environment—Hardware
Fig. 5.30 AR tracking and other components
examples of a computer screen on a stand at eye height with a motorized platform 
at the bottom.
A comparison of the key features and effects of AR and VR is given in Table 5.7.
GPUs drove the displays and most of the machines as the drive controller. 
5.4.10 
Monitor Synchronization: 2013–2015 
Gamers often disable V-Sync to get the best input response possible; however, that 
introduces visual artifacts such as tearing and stutter. Screen tearing resulted from a 
mismatch between the frame rate of a game and the monitor’s refresh rate. 
Since their earliest days, displays have had ﬁxed refresh rates, and they were typi-
cally 50 or 60 fps. However, GPUs render frames at varying rates due to the dynamic 
nature and compositional complexity of the various scenes in computer games. As 
the GPU sought to synchronize with the monitor, persistent tearing occurred. Turning 
on V-Sync eliminated tearing but caused increased lag and stutter (also known as 
frame drop) as the GPU and monitor refreshed at different rates (Fig. 5.34).

5.4 GPU Video Outputs
187
Fig. 5.31 An MR system
Fig. 5.32 A simpliﬁed representation of a metaverse continuum (Milgram, 1994)
With the monitor technology available in 2013, every game would tear if V-Sync 
was not engaged. In tearing, users see off-shifted parts of two or more frames on 
screen, impacting gameplay and immersion, especially when objects were in the 
scene or the view. 
The yellow lines in Fig. 5.34 illustrate the severity of image tearing. 
Figure 7.20 illustrates how frames looked when V-Sync was switched off:
1. A frame was delivered to the monitor when rendering on the GPU was complete. 
2. The completed frame was presented to the display from the GPU, even if the 
previous frame had not ﬁnished. The monitor now contained two frames.

188
5
The GPU Environment—Hardware
Fig. 5.33 Examples of 
telepresence robots. 
Reproduced with permission 
from Projexive
3. The process continues, with tearing appearing wherever each new GPU frame 
begins (Fig. 5.35).
With V-Sync, the frames generated by the GPU arrived either earlier or later 
than the V-Sync timing signal and were rarely correctly aligned, causing shuddering 
(stutter) and input delay (lag). 
For frames, the GPU could not render at 60 FPS or higher, and V-Sync required the 
GPU to wait until the next monitor refresh cycle to update the screen, and sometimes 
longer. The consequence was that the same frame got displayed several times in each 
refresh cycle (Fig. 5.36). That affected the animation of the screen and produced 
stutter; it also increased the input latency, as new data took longer and longer to 
reach the display.
At an event in Montreal organized by Nvidia in October 2013, the company intro-
duced its G-Sync technology, which aimed to solve the issues of onscreen tearing, 
stuttering, and lag. 
That solution required monitor builders to add a small circuit board, which added 
about $100–$120 to the end-user price of the monitor.

5.4 GPU Video Outputs
189
Table 5.7 Comparison of AR and VR characteristics. Source Phillip Rauschnabel 
Feature
Augmented reality
Virtual reality 
Role of the local physical 
environment 
Extended/diminished
Replaced 
Usage time frame 
(potential) 
Enduring
Temporary 
Typical usage context
Everywhere
In a “secure” area (e.g., at 
home) or in speciﬁc contexts 
(e.g., therapy, amusement parks, 
shop, etc.) 
Technology
Devices: 
Stationary, mobile, wearable, 
on-/in-body projectors 
Display techniques: 
Video see-through displays 
Optical see-through displays 
Projection 
Devices: 
Wearables (HMDs), caves 
(declining practical relevance) 
Display techniques: 
Video displays, projection 
Physical risks
Through distraction
Through collision 
Privacy concerns
User and surrounding people
User 
Motion sickness
Rarely applicable
Signiﬁcant 
Speciﬁc mechanism
Local presence
Telepresence 
Typical use cases
Situations where a combined 
experience of real and virtual 
content is beneﬁcial (e.g., to 
compare sizes, such as for 
furniture) and possible (e.g., the 
home in which the furniture 
will be placed already exists) 
Situations where the physical or 
story context does not exist 
(e.g., a ﬁctitious game), is not 
accessible to a user (e.g., the 
moon, time travel), or where the 
actual physical context is not 
desirable (e.g., in situations that 
would be dangerous in the real 
world) 
Note Rauschnabel refers here to generic experiences and standard devices. There may be situations 
where these differences do not (fully) apply
Nvidia added some memory and sync circuits to keep the images running as fast 
as possible without lag, frame drop, stuttering, or tearing; they called that G-Sync, 
borrowing from the G of GPU and Nvidia’s AIB brand, GeForce. 
In that approach, the GeForce GPU sends a signal to the G-Sync controller in the 
monitor, telling the monitor when to update the display. That provides a smooth and 
responsive experience, with a possible reduction in the frame rate when the rendering 
time in the GPU was longer than 16.7 ms., shown in Fig. 5.37.
However, at the same time as Nvidia was launching G-Sync, monitor suppliers 
were also bringing out new monitors with high and super-high refresh rates. Asus 
introduced the ﬁrst of those in 2012, which had a refresh rate of 144 Hz. Testing 
showed that if the monitor were fast enough, no problems would be experienced.

190
5
The GPU Environment—Hardware
Fig. 5.34 Tearing when V-Sync was switched off. Reproduced with permission from Nvidia
Fig. 5.35 Disabling V-Sync allows the GPU frame to be displayed when ready, which causes 
tearing on the screen as two or more frames were displayed in each refresh cycle
Fig. 5.36 Frames from the GPU with V-Sync: if a particular frame is early or late, the result is 
stutter and input delay
Fig. 5.37 If the G-Sync monitor was capable of a variable refresh rate, the GPU determines that 
rate

5.4 GPU Video Outputs
191
Nvidia acknowledged there were cases in which G-Sync would not add any value, 
for example, if rendered at more than 100 fps and the user had a monitor with a high 
refresh rate. 
Table 5.8 summarizes the cases in which the use of G-Sync could help. 
The way to read the table is that there were cases in which a high FPS gives a 
good enough experience. When V-Sync was on, it was the same as G-Sync if the 
FPS was greater than the refresh rate. 
Nvidia enjoyed some success in persuading monitor suppliers to add their timing 
controller (TCON) enhancer board. The Nvidia brand and the unique G-Sync tech-
nology gave monitor suppliers a differentiated product that they could charge more
Table 5.8 G-Sync modes and results 
GPU render rate (FPS)
Monitor refresh rate (Hz)
Sync mode
Outcome 
>60
60
V-Sync on
Perfect—monitor in sync 
with GPU rendering 
>60
60
V-Sync off
Tear—although rates are 
nearly the same, they are 
not in sync 
>60
60
G-Sync
Perfect 
30–60
60
V-Sync on
Bad stutter 
30–60
60
V-Sync off
Tear and stutter 
30–60
60
G-Sync
Perfect 
>120
120
V-Sync on
Perfect—monitor in sync 
with GPU rendering 
>120
120
V-Sync off
Tear—although rates are 
nearly the same, they are 
not in sync. It may be less 
visible than at 60 Hz since 
the average tear is smaller 
>120
120
G-Sync
Perfect 
60–120
120
V-Sync on
Stutter, but each stutter is 
8 ms instead of 16 ms. 
60–120
120
V-Sync off
Tear. It may be less visible 
than at 60 Hz since the 
average tear 
60–120
120
G-Sync
Perfect 
30–60
120
V-Sync on
Stutter, but each stutter is 
8 ms instead of 16 ms. 
30–60
120
V-Sync off
Tear. Although the rates are 
nearly the same, they are 
not in sync. It may be less 
visible than at 60 Hz since 
the average tear 
30–60
120
G-Sync
Perfect 

192
5
The GPU Environment—Hardware
for—always a welcome gift in the commoditized and highly competitive monitor 
business. 
In January 2015, AMD spoiled the party by presenting its new FreeSync solution. 
They introduced two Toshiba laptops in which the displays supported AMD’s feature 
in its last three generations of GPUs they called dynamic refresh rate. The company 
explained that they built that capability into their GPUs for power-saving purposes 
since unnecessary vertical refresh cycles use power and give no beneﬁt. 
It reduced the number of frame-write cycles per second saved on power to the 
timing controller (TCON), the DisplayPort receiver (in the monitor), and the row 
and column drivers controlling the pixel gates. They were able to reduce the memory 
read cycle on the GPU side, and there were also other opportunities for optimization. 
AMD presented its dynamic variable refresh rate (VRR) (which adjusted the 
vertical blank time) to VESA in 2013. It got incorporated into the next generation of 
the VESA display port speciﬁcation, DP 1.3. Although not widespread, AMD stated 
that some panel makers had adopted it and that AMD’s Catalyst drivers already 
supported it. Adaptive-Sync had become a proven and widely adopted technology 
by 2015 and had been a standard component of VESA’s embedded DisplayPort (eDP) 
speciﬁcation since its introduction in 2009. 
On May 12, 2014, VESA announced the addition of Adaptive-Sync to its Display-
Port 1.2a video interface standard. The dynamic refresh rate got incorporated into 
the embedded eDP spec since version 1.0, and AMD stated that they had veriﬁed 
that with many TCON and panel vendors. 
AMD believed that several monitor suppliers would be ready with hardware to 
support DP 1.3 when it came out in September 2014. However, previous releases of 
DP speciﬁcations had not initially been smooth or universally accepted by monitor 
suppliers and not even by AIB suppliers. Nonetheless, AMD stated that by exploiting 
modern monitors’ inherent variable frame rate via the VESA speciﬁcations, they 
could accomplish the same results as Nvidia’s G-Sync for free, at least in a notebook. 
The frame rate reduction was used with embedded notebook displays for quite 
some time to conserve power, particularly for static images. That concept appeared in 
the ﬁrst generation of the VESA DP speciﬁcation for embedded display port (eDP), 
which connected notebook displays directly to GPUs. 
5.4.10.1
Those Damn Scalers 
Each GPU supplier had a proprietary methodology, and AMD would not comment 
on how closely they followed the VESA speciﬁcations. However, it ignored the 
fact that desktop monitors have scalers (although notebooks do not). The G-sync 
module replaced the desktop monitor scaler board and enabled VRRs. The scalers 
in the desktop monitors were acting as a bottleneck, and Nvidia and several monitor 
suppliers decided not to wait for scaler chip suppliers to develop new chips and then 
to get them into new monitors; they did it on their own. 
There were other issues as well. For example, how would a variable rate panel 
deal with ﬂicker and color-shifting caused by a VRR?

5.4 GPU Video Outputs
193
The LCD is a sample-and-hold circuit and updating the pixel at lower rates (less 
than 60 Hz) will result in artifacts at some point, both in terms of visible ﬂicker and 
a brightness or color shift. The use of a color-processing engine in the TCON could 
compensate for some of the color shift—but not the ﬂicker. A GPU needs to operate 
within the acceptable range based on Extended Display Identiﬁcation Data (EDID) 
declarations in some implementations. 
Many monitors now work down to 40 Hz with no shift, and there were technologies 
such as IGZO panels that could go even lower and potentially to below 20 Hz. 
By reading the monitor’s EDID, the controller (GPU) determines the lowest 
refresh rate the monitor could tolerate. The critical point is that gaming below 30 Hz 
was not desirable, and if systems have panels that could support 60 Hz to a lower 
range (30–40 Hz), the source could match the rendering rate to the panel rate, to 
achieve stutter-free gaming with V-Sync on. 
In theory, those panels could also take advantage of panel self-refresh (PSR), a 
speciﬁcation for monitors (based on input from Intel) that tells a monitor how to 
refresh when GPU frame rates become too low. 
Neither AMD nor Nvidia had addressed the issue of VRRs for connections other 
than DP. 
Another approach would be to use what was known as a Direct Drive Monitor 
(DDM), which forms part of the DP speciﬁcation. DDM mimicked the direct GPU-
to-LVDS driver connections and did not use a scaler (part of the DDM speciﬁcation 
was to have no controls on the monitor, as the computer controls it). Although Apple 
had used DDM in its monitors for some time, it operated in a closed environment 
(like a notebook); other PC vendors do not enjoy that luxury and must live in a 
plug-and-play, legacy-compatible world. 
Direct Drive bypasses various in-monitor circuitry, and as the name implies, drives 
the monitor directly from the AIB, thereby delivering the highest frame rate and the 
lowest lag. DDMs were available from other companies such as Iiyama and Dell. 
They may include scalers and may also have power on/off and control buttons for 
brightness and so forth. 
5.4.10.2
Flickering 
Some FreeSync monitors with high refresh rates could cause brightness ﬂickering 
with FPS ﬂuctuations (Fig. 5.38). Brightness ﬂickering in FreeSync was common 
with high refresh rate vertical alignment (VA) panel displays. Still, it could also affect 
displays based on other panel technologies such as IPS and TN, although rare and 
would not necessarily be present in every game.
A variable refresh monitor (48–144 Hz) would trigger low framerate compen-
sation (LFC) once the FPS drops to 47 or less if certain AIBs (e.g., AMD) were 
driving it. LFC multiplies the framerate to end tearing when the FPS dips below the 
monitor’s VRR range; for instance, at 47 FPS, it tripled to 141 Hz. 
If the FPS was constantly around 48, LFC rapidly switched on and off, causing 
brightness ﬂickering. If LFC was disabled, V-Sync eliminates screen tearing at lower

194
5
The GPU Environment—Hardware
Fig. 5.38 FreeSync could sometimes cause brightness ﬂickering with FPS ﬂuctuations. Repro-
duced with permission from Display Ninja
frame rates, and FreeAdaptive or G-Sync took care of higher frame rates with no 
issues [13]. 
5.4.10.3
Adaptive Sync, FreeSync, and G-Sync 
FreeSync was AMD’s counterpoint to G-Sync, and both of those use VESA’s 
Adaptive-Sync protocol. Just as G-Sync requires an Nvidia AIB, FreeSync requires 
an AMD AIB. 
In addition to all the above, VRR was a signiﬁcant advancement, and the effect 
in games was stunning. 
5.5 
Multiple AIBs in a System 
Double Your Pleasure, Double Your Speed? 
Artist Graphics and 3Dlabs were the ﬁrst to connect two AIBs in the eighties, and 
3Dfx introduced the dual-processor VooDoo 2 AIB in late 1997, with scan-line 
interlace (SLI). 
When 3D graphics controllers were just emerging in the late 1990s, one company, 
in particular, 3dfx, experimented with ways to scale up the performance by accel-
erating the 3D gameplay. Their idea was scan-line interlace (SLI), introduced in

5.5 Multiple AIBs in a System
195
1998 as part of their second-generation chip introduction, Voodoo2 (or Voodoo2). In 
SLI mode, two Voodoo2 add-in-boards (AIBs) could run in parallel, with each one 
drawing every other line of the display. The original Voodoo Graphics also had SLI 
capability but was used only in the arcade and professional markets. 
That concept emerged again when Alienware announced a motherboard handling 
two PCI Express graphics AIBs in May 2004. That ignited the industry, and soon 
Nvidia announced they had re-invented SLI in a better way than the original. Outra-
geous claims were made for their invention, and the industry, and especially the 
gamer press, went nuts over it. 
Although various gamer websites ran SLI benchmarks with results far from 2x, 
maybe reaching 1.5x, the loyal fans were hooked, and SLI was what they wanted. 
ATI pointed out that they had invented that approach years ago (with Rage Fury 
Maxx in 1999) but had not seen its beneﬁts. Then the company said it would also 
introduce a version—evidently, beneﬁts had been found). 
Meanwhile, AIB builder Gigabyte engineered a practical and affordable SLI AIB 
that did not require a special motherboard, and there were no additional costs and no 
extra AIB slots [14]. The performance gain was about 1.73x, which outperformed 
two Nvidia SLI AIBs by about 6 percent; however, two Nvidia 6600 GT SLI AIBs 
plus a special SLI motherboard cost about $600, while one Gigabyte GV-3D1 was 
$549 with a standard motherboard. Hence, by 2005, dual AIBs were back in the 
gamer world. Leadtek and PNY also announced that they would introduce dual-GPU 
workstation AIBs. 
Hoping to ride the wave of renewed multi-AIB enthusiasm, ATI introduced Cross-
Fire Edition AIBs as upgrades in October 2005. The idea was that a user would buy 
a system with a proper chipset (for example, ATI’s Xpress200 for AMD or Intel, or 
Intel’s 955x) and an ATI Radeon graphics board X800. The master CrossFire Edition 
AIB could be added later for dual-GPU processing joy. 
However, the mixing and matching were less robust than hoped; one could only 
mix components within a given family, as only x800-class AIBs would work with 
an X800 master, and only X850 AIBs could use an X850 master. 
The added costs, the difﬁculties of getting drivers to work with all games, and 
the lack of a sufﬁcient return on investment in terms of performance ﬁnally caught 
up with ATI/AMD and Nvidia, and enthusiasm for dual AIBs faded like yesterday’s 
newspaper. It was hard to keep a bad idea down, and in August 2021, Apple introduced 
a dual-GPU AMD AIB in their Mac Pro workstation, with a $5,000 price and limited 
application assurance. 
The problem was, and always will be Amdahl’s law. In 1967, Gene Amdahl 
introduced a formula that showed how much speed could be obtained by adding 
resources. As more were added, the overhead costs and the time needed to manage 
the process created an asymptotic curve that prevented additional gains. In the case 
of AIBs, the asymptote was two, or more realistically, 1.5.

196
5
The GPU Environment—Hardware
5.5.1 
Multi-GPIs (1996) 
The Promise and Failure of Scaling Add-in Boards 
In addition to theoretically reducing the scan time, it also increased the available 
frame buffer’s memory size. That allowed bigger models to be loaded, and it also 
increased the maximum screen resolution. However, the texture memory was not 
doubled because each AIB needed to duplicate the scene data, and that, combined 
with other overhead issues, depreciated the theoretical performance. As 3D models 
and screen resolutions got larger, the size and number of texture maps further reduced 
the proposed beneﬁts. 
3dfx tried to overcome that problem by adding a third chip, the texture mapping 
unit (TMU). The TMU allowed a second texture to be drawn during the same graphics 
engine pass with no performance penalty. At the time of its introduction, Voodoo2 
was the only 3D AIB capable of single-cycle dual texturing. Usage of the Voodoo2’s 
second TMU depended on the application software; however, two very popular games 
of the time, Quake II and Unreal exploited dual texturing with great success. 
It took a little while before the price-performance analysis showed up. An 8 MB 
Voodoo2 AIB sold for $249 in 1998, about $480 today. So, two Voodoo2 AIBs would 
have been about $500 then. The average performance improvement, however, was 
about 60–70 percent, depending upon the game. So, the payoff was never there, 
nor could it ever have been. But SLI had something more valuable—sex appeal 
(Fig. 5.39).
When Nvidia bought 3dfx’s assets in 2000, included in the IP package was SLI. 
However, Nvidia did not (re)introduce it until 2004. And being Nvidia, tweaked the 
branding and called it the scan-line interface. Nvidia also expanded the concept, 
making it capable of using up to four AIBs (which 3dfx had done in the professional 
space with its Quantum3D products). And they added multiple modes: split-frame 
rendering (half per AIB), alternate frame rendering, and even SLI anti-aliasing and 
the ability to use an integrated GPU, a mode they called Hybrid SLI. 
But expansion and rebranding could not change SLI’s basic functionality. It never 
delivered anything more than 170 percent improvement for 200 percent of the cost, 
and AIBs increased yearly. In addition, the driver support Nvidia had to provide, 
amounting to a tweak for almost every game, was adding up with each new generation. 
But the concept still had sex appeal. 
In late 2005, reacting to Nvidia’s promotion of SLI, AMD, who had just acquired 
ATI, introduced its version called CrossFire. 
AMD expanded the concept of alternate scan-line rendering and introduced alter-
native frame rendering (AFR) and split-frame rendering (SFR). AMD also added a 
SuperTiling rendering technique. The latter increased performance in speciﬁc appli-
cations but did not work with OpenGL or support accelerated geometry processing. 
Also, like SLI, Crossﬁre faced its share of driver-related troubles. 
Then in 2013, AMD advanced the concept further and removed the over-the-top 
(OTT) bus (Fig. 5.40). AMD ﬁgured out how to use an extended direct memory

5.5 Multiple AIBs in a System
197
Fig. 5.39 Voodoo2 in SLI conﬁguration. Reproduced with permission from imgur.com
access (XDMA) technique that opened a direct communication channel between 
multiple GPUs in a PC via PCI Express (PCIe).
AMD’s XDMA direct communication channel between the AIBs transferred 
graphics data among AIBs, main memory, and the CPU. The AIBs of the time were 
not using all the bandwidth PCIe offered then, which was considerably more than an 
OTT bridge could. The bandwidth of an external OTT bridge was only 900 MB/s, 
whereas PCIe 3.0 with 16 lanes could provide up to 32 GB/s. 
AMD’s added bandwidth and elimination of the OTT (which Nvidia began 
charging extra for) gave the company a competitive advantage. However, its AIBs of 
the time were not at the same performance level as Nvidia’s so that did not help them 
much in the marketplace. Ironically, when AMD introduced the RX480 in 2016, the 
company suggested users buy two AMD AIBs, which AMD said would outperform 
one Nvidia AIB and cost less. It was a clever marketing pitch, but it did not help 
AMD’s sales. It also was not valid. 
In 2017, as AMD and Nvidia rolled out Dx12 AIBs, AMD dropped support 
for CrossFire and said, “In DirectX 12, we reference multi-GPU as applications 
must support mGPU, whereas AMD had to create the proﬁles for DX11. We have 
accordingly moved away from using the CrossFire tag for multi-GPU gaming.” [15]. 
Nvidia followed suit in 2019 and made it ofﬁcial in 2020. For its professional 
graphics AIB line Quadro, Nvidia introduced a newer, much higher bandwidth

198
5
The GPU Environment—Hardware
Fig. 5.40 AMD’s XDMA multi-AIB. Reproduced with permission from AMD
scheme it calls NVLink for multi-AIBs. NVLink speciﬁes a point-to-point connection 
with data rates of 20, 25, and 50 Gbit/s. 
In late 2020, the company introduced a high-end consumer AIB, the RTX3090, 
and made NVLink an option. Nvidia introduced the 350-W RTX 3090 at $1,499. 
Nvidia made bridges for it for $80. AIB partners were also able to make their own 
bridges. Physical compatibility depends on the industrial design, but the Nvidia direct 
bridge should work with any two identical AIBs from Nvidia or AIB partners. 
It was not likely very many gamers would spend $3,000 plus another approxi-
mately $90 for the NVLink and might need to add a larger power supply (PSU) for 
the added performance. However, content creators might be willing to spend that. 
If you are fascinated and curious about 3dfx and all the things they innovated 
(such as SLI), then we recommend The Legacy of 3dfx by Prieto [16].

References
199
5.6 
Conclusion 
As mentioned elsewhere, the GPU sits within a cocoon of standards and rules, 
elements that can’t be modiﬁed, nor should they. The operating system and its APIs 
are ﬁxed, the video output connectors are ﬁxed, the power supply is ﬁxed, the type of 
memory is ﬁxed, and the connectors the AIB plugs into are ﬁxed—refer to Fig. 5.2. 
The environment of the GPU. 
In one sense those boundaries, that cocoon, may seem restrictive, and to a certain 
degree, it is. But is also enabling in that it allows all the innovation and development 
to take place in the GPU. That innovation would be severely slowed down if the GPU 
designers also have to create and test all the I/O and supporting environment. So be 
thankful for the standards. And be thankful they don’t change very fast or an AIB 
bought today could be made obsolete tomorrow. 
References 
1. Horiguchi, N. Entering the Nanosheet Transistor Era, IMEC for EE Times, (August 12, 2021), 
https://www.eetimes.com/entering-the-nanosheet-transistor-era/2/ 
2. U.S. government invalidates potent Rambus patent, Reuters, (January 27, 2012), https://www. 
reuters.com/article/us-rambus-patent-idUSTRE80Q24E20120127 
3. FTC Issues Final Opinion and Order in Rambus Matter, Federal Trade Commission, 
(February 5, 2007),https://www.ftc.gov/news-events/news/press-releases/2007/02/ftc-issues-
ﬁnal-opinion-order-rambus-matter 
4. Nystedt, D. Rambus sues Broadcom, Nvidia, four others, over patents, Computer-
world, (December 2, 2010),https://www.computerworld.com/article/2514660/rambus-sues-
broadcom–nvidia–four-others–over-patents.html 
5. Clark, D. Rambus Settles Chip Dispute With SK Hynix, The Wall Street Journal, (June 11, 
2013), https://www.wsj.com/articles/SB10001424127887323949904578539720368087016 
6. Scansen, D. Chiplets: A Short History, EE Times March 14, 2021.https://www.eetimes.com/ 
chiplets-a-short-history/# 
7. Gulbranden, J. A history of buses—from ISA to PCI express, (October 21, 2015) https://www. 
youtube.com/watch?v=51YEyPqvkTk 
8. Peddie, J. Dell settles FTC charges, The PC Graphics Report, Volume V111, number, number 
46, pp22, (November 7, 1995) 
9. Hardware Implementation of AGP,https://www.cs.umd.edu/users/meesh/cmsc411/website/pro 
jects/agp/hardwareagp.htm 
10. https://pcisig.com/ 
11. Smith, Tony, Sapphire ready to ship ﬁrst HDMI graphics card, (June 29, 2006), https://tinyurl. 
com/3ra2f8ju 
12. Milgram, P; Takemura; U, and Kishino, F. Augmented Reality:A class of displays on the reality-
virtuality continuum (pdf). Proceedings of Telemanipulator and Telepresence Technologies, 
pp. 2351–(December 21, 1955), 34. https://tinyurl.com/2wmjcsy2 
13. Shafer, R. What Is FreeSync Brightness Flickering, and Can You Fix It? Display Ninja, (May 
21, 2021), https://www.displayninja.com/what-is-freesync-brightness-ﬂickering/ 
14. Peddie, J. JPT’s TechWatch, Volume 5, Number 3, (February 14, 2005)

200
5
The GPU Environment—Hardware
15. McGlaun, S. AMD Abandons CrossFire Branding For Multi-GPU Setups Since DX12 Does 
It Differently, Hot hardware, (September 24, 2017), https://hothardware.com/news/amd-aba 
ndons-crossfire-branding-for-multi-gpu-setups-since-dx12-does-it-differently 
16. Prieto, M. G. The Legacy of 3dfx, https://www.jonpeddie.com/?ACT=80&key=YN1km2n8g 
Fglb4aZ

Chapter 6 
Application Program Interface (API) 
An application programming interface (API) is the software link between a processor 
such as a GPU and the computer’s programs, the applications, and the operating 
system. It is a form of a software interface. It can also be considered a translator, where 
high-level code (software) is converted to low-level machine code. An API hides the 
internal details of how a GPU works, exposing only those parts a programmer would 
ﬁnd helpful or essential. 
The concept of an API goes back to the earliest days of computing in the late1940s. 
The Electronic delay storage automatic calculator (EDSAC) computer (Fig. 6.1) built 
at the University of Cambridge, had an organization of APIs. The term appeared in 
the late 1960s in a paper called data structures and techniques for remote computer 
graphics presented at an AFIPS conference in 1968.
Term API ﬁrst appeared in Herman Goldstein and John von Neumann’s Planning 
and Coding of Problems for an Electronic Computing Instrument—Part II, Volume 
III [1]. In it was a key idea: Most programs would make use of common operations. 
Library subroutines would reduce the amount of new code and errors. In reality, it 
was unlikely what Goldstein and von Neumann proposed would have worked [2]. 
In his highly informative and entertaining presentation, Joshua Bloch discusses the 
history of APIs. He points out that Sir Maurice Wilkes at the Cambridge Mathematical 
Laboratory is credited with developing the ﬁrst API, but he didn’t label it as such 
[3]. 
By June 1949. I was trying to get working my ﬁrst non-trivial program, which was for the 
numerical integration of Airy’s differential equation. It was on one on my journeys between 
the EDSAC room and the punching equipment that ‘hesitating at the angles of the stairs’ the 
realization came over me that a good part of the remainder of my life was going to be spent 
in ﬁnding the errors in my own programs.—Maurice Wilkes, Memoirs 
Wilkes saw subroutine libraries as the solution. He assigned the task to his postdoc 
student, David Wheeler. Wheeler created the concept of coding orders to augment 
initial orders—orders being the term they used for programs then. Those subroutines 
were strips of punched tape they stored in a small metal ﬁling cabinet [4]. Wheeler’s
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2022 
J. Peddie, The History of the GPU – Eras and Environment, 
https://doi.org/10.1007/978-3-031-13581-1_6 
201

202
6
Application Program Interface (API)
Fig. 6.1 EDSAC was the world’s ﬁrst stored-program computer. It began operation on May 6, 1949, 
3,000 vacuum tubes and 12KW. Source Wikipedia Copyright Computer Laboratory, University of 
Cambridge. Reproduced by permission
idea was brilliant and a breakthrough in the development of electronic computers 
[5]. 
So, APIs are not new, and they are critical to the stable operation of a computer, 
especially a GPU. 
6.1 Application Program Interface 
GPUs, like several other peripherals, require a software program called a driver. An 
API is a translator that reads instructions from an application and translates them 
into instructions native to the GPU (refer to Fig. 6.2). The reverse path is similar; 
GPU instructions get translated into equivalents that an application can understand.
APIs helped the industry progress and even deﬁned graphics in some ways. 
The driver is a program that runs on the CPU. It contains a compiler that turns the 
shader code of the program into the native code of the GPU and offers an application 
program interface (API). 
The instruction set for a CPU in a PC or workstation is the ×86 standard. Programs 
written for Android and Apple phones are compiled for the ARM instruction set. 
In contrast, there is no standard instruction set for GPUs—that is one of the 
few unique things about a GPU. There were only standards for a minimal set of 
unaccelerated 2D functionalities: VGA and VESA BIOS Extensions (VBEs). Those 
standards apply to the boot text and logo that appears before the drivers of the GPU 
were loaded; they were used for limited graphics such as Windows in safe mode 
and ancient DOS games, but the CPU did all the calculations involved in drawing 
anything.

6.1 Application Program Interface
203
Fig. 6.2 Block diagram of the API and its relationship to the other components in a computer
In today’s GPUs, the most used API standards are Direct3D, Metal, and Vulkan. 
Graphics APIs date back to Plot 10 in 1971 that Tektronix developed for their 
graphics terminals. Computer graphics were centered on professional graphics in 
those early days, with CAD being the primary application. Several APIs were subse-
quently developed, such as CORE (1972), GKS (1977), and PHIGS (1979), produced 
by the Association for Computing Machinery (ACM) [6]. However, Silicon Graphics 
(SGI) IRIS-GL, introduced in 1981 by Silicon Graphics (SGI), got the world’s atten-
tion. It spawned OpenGL, OpenGL ES, and inﬂuenced DirectX and many other 
consumer APIs [7]. 
This book only examines the development of APIs since DirectX and OpenGL ES, 
speciﬁcally since 2000, when GPUs entered the market, and summarized in Fig. 6.3.
Apple had one API for its mobile devices and PCs, Khronos had two APIs, one 
for mobile and one for PCs, and they would evolve to open API, and Microsoft had 
one API. 
6.1.1 
APIs and OSs 
Graphics APIs provide a standard interface that enables applications to gain access 
to hardware features. The API exposes the features of the hardware without requiring 
application developers to write hardware-speciﬁc code. Instead of writing specialized 
code for a given GPU, a programmer could use an API such as DirectX or Vulkan. 
The API would manage the draw calls and instruct the hardware on how to do it.

204
6
Application Program Interface (API)
Fig. 6.3 The history of APIs
There were three signiﬁcant graphics APIs for the PC, Apple’s Metal, Khronos’ 
Vulkan, and Microsoft’s DirectX. Mobile devices use Khronos’ OpenGL ES and 
Vulkan. 
Since Windows 95, every new release of Windows included an updated version 
of DirectX, see Table 6.1.
APIs are used to carry subroutines for drawing primitives such as lines and circles. 
Over time, those subroutine libraries grew, and searches through the draw library for 
the necessary element or function took longer. As GPUs evolved, they incorporated

6.1 Application Program Interface
205
Table 6.1 History of modern APIs for all platforms 
API
Developer
OS
Introduced
API
Developer
OS
Introduced 
DirectX 1.0
Microsoft
Windows 3.1
3 January 1995
DirectX 10
Microsoft
Vista
1 November 2006 
Direct3D
Microsoft
MS-DOS
17 June 1995
OpenGL ES 2.0
Khronos
Android
1 March 2007 
DirectX 2.0
Microsoft
95 and NT 4.0
6 January 1996
DirectX 10.1
Microsoft
Vista SP1
1 February 2008 
DirectX 3.0
Microsoft
NT 4.0 SP3
1 September 1996
OpenGL 3.0
Khronos
Windows
11 August 2008 
OpenGL 1.1
SGI
Linux/Unix
4 March 1997
OpenGL 3.1
Khronos
Windows
24 March 2009 
DirectX 5.0
Microsoft
98
1 July 1997
OpenGL 3.2
Khronos
Windows
3 August 2009 
OpenGL 1.2
SGI
Linux/Unix
16 March 1998
DirectX 11
Microsoft
Windows 7
1 October 2009 
GDI
Microsoft
98
1 July 1998
OpenGL 3.3
Khronos
Windows
11 March 2010 
DirectX 6.0
Microsoft
98 SE and ME
1 August 1998
OpenGL 4.0
Khronos
Windows
11 March 2010 
OpenGL 1.2.1
SGI
Linux/Unix
14 October 1998
OpenGL 4.1
Khronos
Windows
26 July 2010 
DirectX 7.0
Microsoft
2000
1 September 1999
OpenGL 4.2
Khronos
Windows
8 August 2011 
DirectX 8.0
Microsoft
2000
1 November 2000
OpenGL ES 3.0
Khronos
Android
1 August 2012 
OpenGL 1.3
SGI
Linux/Unix
14 August 2001
OpenGL 4.3
Khronos
Windows
6 August 2012 
DirectX 8.1
Microsoft
XP
1 November 2001
OpenGL 4.4
Khronos
Windows
22 July 2013 
OpenGL 1.4
SGI
Linux/Unix
24 July 2002
OpenGL ES 3.1
Khronos
Android
1 March 2014 
DirectX 9.0
Microsoft
XP
1 December 2002
Metal
Apple
iOS
2 June 2014
(continued)

206
6
Application Program Interface (API)
Table 6.1 (continued)
API
Developer
OS
Introduced
API
Developer
OS
Introduced
DirectX 9.0a
Microsoft
XP
1 March 2003
OpenGL 4.5
Khronos
Windows
11 August 2014 
OpenGL ES 1.0
Khronos
Android
28 July 2003
DirectX 12
Microsoft
Windows
29 July 2015 
OpenGL 1.5
SGI
Linux/Unix
29 July 2003
OpenGL ES 3.2
Khronos
Android
1 August 2015 
DirectX 9.0b
Microsoft
XP
1 August 2003
OpenGL 4.6
Khronos
Windows
31 July 2017 
DirectX 9.0c
Microsoft
XP
1 August 2004
Vulkan 1.2
Khronos
Win, iOS
15 February 2020 
OpenGL 2.0
SGI
Linux/Unix
7 September 2004
DirectX 12 U
Microsoft
Windows
18 November 2020 
OpenGL 2.1
Khronos
Windows
2 July 2006

6.1 Application Program Interface
207
drawing functions and many other aspects of creating an image. They could execute 
those functions faster, so the API library became redundant and an unnecessary 
memory consumer. That realization (and acknowledgment) led to low-level APIs for 
game consoles, then PCs, and later, mobile devices. In the following sections, we 
discuss that development and the latest improvements. 
6.1.1.1 
Chaos in the Mobile Market 
Perhaps on area where the acceptance of standard APIs has made the most difference 
is in the mobile market. The chaos in the mobile market before OpenGL ES was 
unmanageable for application developers and held back the market. An app written 
for a Blackberry would not run on a Nokia unless the app developer made a special 
driver. In addition to the chaos, alongside 3D APIs, there were several proprietary 
2D APIs. There were literally over a thousand versions of APIs, OS, and SoCs. That 
meant app developers had to limit the machines they could afford to support and 
that denied some users desirable apps. It also had the effect of driving some users to 
certain mobile devices. The confusion and difﬁculties for developers tended to favor 
the incumbents, which may or may not be keeping up with trends and customer 
interests. 
Nokia’s Symbian OS had almost 80% market share with their various APIs. It 
wasn’t until 2003 when Khronos released OpenGL ES 1.0 that a common, open, API 
appeared and started the process of stabilizing the wild-west of the mobile market. 
OpenGL ES stabilized the mobile market and empowered the phenomenal growth 
of the market. Google’s Android became the dominant OS and Apple’s iOS was 
second. A few variants of Linux also existed but Android and the Linux versions 
all used OpenGL ES, while Apple offered Core Graphics framework and Quartz 2D 
APIs. In 2007 Apple switched to OpenGL ES, and then in 2014, Apple introduced 
the Metal API of iOS. In February 2016, Khronos introduced Vulkan a universal API 
that would serve the PC, mobile devices, game consoles, and other platforms. By 
2021 Vulkan was being used on 71% of mobile devices as shown in Fig. 6.4.
6.1.1.2 
DirectX Shaders 
In the case of DirectX (AKA Direct3D), there were several generations of shader 
capabilities and speciﬁcations (Table 6.2).
The NV10 was the ﬁrst graphics processor referred to by Nvidia as a GPU—with 
integrated transform and lighting (T&L) (ﬁxed function). That was the GeForce 256. 
Pixel and vertex shaders got introduced in DirectX 8; in their NV20, Nvidia made 
the geometry programmable with the introduction of the vertex shader, and the pixel 
shader supported limited programmability. That was the ﬁrst time they were referred 
to in that way.

208
6
Application Program Interface (API)
Fig. 6.4 Mobile graphics evolution through OpenGL ES to Vulkan. Source Khronos
Table 6.2 DirectX 9 shader version 
Direct3D
9.0
9.0
9.0b
9.0c 
Date
August 02
January 03
September 04
October 05 
Pixel shader model
2.0
2.0a
2.0b
3.0 
Dependent texture limit
4
No limit
4
No limit 
Texture instruction limit
32
Unlimited
Unlimited
Unlimited 
Position register
–
–
–
Yes 
Instruction slots
32+64
512
512
≥512 
Executed instructions
32+64
512
512
65,535 
Interpolated registers
2+8
2+8
2+8
10 
Instruction predication
–
Yes
–
Yes 
Indexed input registers
–
–
–
Yes 
Temp registers
12
22
32
32 
Constant registers
32
32
32
224 
Arbitrary swizzling
–
Yes
–
Yes 
Gradient instructions
–
Yes
–
Yes 
Loop count register
–
–
–
Yes 
Face register (two-sided lighting)
–
–
–
Yes 
Dynamic ﬂow control depth
–
–
–
24

6.1 Application Program Interface
209
6.1.1.3 
Comparison of Vertex Shaders 
Vertex shaders developed with the second era of GPUs when the vertex shader 
evolved from a ﬁxed function to a programmable shader. Table 6.3 shows some 
of the evolutionary steps that the vertex shader has gone through. 
Some APIs could be used with different operating systems, some were speciﬁc, 
and some were proprietary. Table 6.4 summarizes some of those differences.
Layered implementations had become popular, as multiple production applica-
tions were shipped with them (for example, Khronos put a lot of effort into Vulkan 
being an effective layering target). 
Table 6.4 indicates where native drivers were available and where developers have 
used a layer. 
6.1.2 
History of DirectX 
Since the introduction of the PC in 1981, with its Microsoft MS-DOS operating 
system, gaming has been a critical application. The ﬁrst PC game sold was Microsoft 
Adventure, which formed part of the PC launch [14]. 
DOS was a relatively simple operating system and was well deﬁned, a requirement 
that IBM imposed on Microsoft. As a result, it was straightforward to use. It was 
also a reasonably stable OS, with few updates and almost no extensions. Although 
programmers from that era might disagree with this cheery view of things MS-DOS 
was an excellent platform for porting games and other applications thanks to its
Table 6.3 Comparison of DirectX vertex shaders 
Vertex shader version
VS 1.1
VS 2.0
VS 2.0a
VS 3.0
VS 4.0, VS 4.1, VS 5.0 
Number of instruction slots
128
256
256
≥512
≥65,536 
Max number of instructions 
executed 
128
1024
65,536
65,536
Unlimited 
Instruction predication
No
No
Yes
Yes
Yes 
Temp registers
12
12
16
32
4096 
Number of constant registers ≥96
≥256
256
≥256
16 × 4096 
Static ﬂow control
No
Yes
Yes
Yes
Yes 
Dynamic ﬂow control
No
No
Yes
Yes
Yes 
Dynamic ﬂow control depth
N/A
N/A
24
24
64 
Vertex texture fetch
No
No
No
Yes
Yes 
Number of texture samplers
N/A
N/A
N/A
4
128 
Geometry instancing support No
No
No
Yes
Yes 
Bitwise operators
No
No
No
No
Yes 
Native integers
No
No
No
No
Yes 

210
6
Application Program Interface (API)
Table 6.4 Compatibility of APIs with OSs and their deployment 
OS/API
DirectX
Metal
OpenGL
OpenGL ES
Vulkan 
Windows (×86)
Platform/IHV 
Dx12 only on 
Windows 10 
IHV [8]
IHV or layer 
ANGLE by 
Google [9] 
IHV 
Windows (Arm)
Platform/IHV 
DX12 only 
Layer 
GLON12 
by 
Microsoft 
[10] 
Layer 
VKON12 by 
Microsoft 
macOS (×86)
Platform 
Platform 
deprecated 
[11] 
Layer 
ANGLE by 
Google 
Layer 
Vulkan 
portability 
[12] 
macOS (Arm)
Platform 
Layer 
by Apple 
Layer 
ANGLE by 
Google 
Layer 
Vulkan 
portability 
iOS (Arm)
Platform
Platform 
deprecated 
Layer 
Vulkan 
portability 
[13] 
Android
Platform/IHV 
Platform/IHV 
Linux
Layers 
VKD3D 
(12)/DXVK(9–11) 
IHV
IHV
IHV 
PLATFORM
API in platform deﬁnition and implemented by the platform owner 
PLATFORM/IHV 
API in platform deﬁnition and implemented by IHV 
IHV
Native API drivers implemented by IHV 
LAYER
Layered implementation
consistency and simplicity. The PC was gaining in popularity by the day, creating a 
sizeable installed base of potential buyers; it certainly wasn’t hindered by its common 
operating system. 
When Apple launched the Mac in January 1984, it had a bit-mapped graphic user 
interface (GUI) adopted (some say stolen) from Xerox Parc Alto. It was easy to 
use, and every point on the screen was interactive and relational. The Mac quickly 
won advocates and attracted world-class applications such as Aldus PageMaker and 
Adobe PostScript. Microsoft and Bill Gates had always been envious of Apple; 
whatever Apple introduced, Microsoft soon copied. Apple featured software from 
Aldus in much of its advertising and helped the ﬂedgling company distribute its 
program. In 1995, Adobe acquired Aldus and re-introduced PageMaker under the 
name Adobe PageMaker [15]. 
Microsoft rushed out its Windows OS in November 1985, almost two years after 
Apple. The ﬁrst Windows was a just GUI extension of Microsoft’s existing disk 
operating system, MS-DOS. The GUI was not very stable and poorly documented.

6.1 Application Program Interface
211
However, one of Microsoft’s qualities was never (well, rarely) giving up. By the time 
they introduced Windows 3.1 in April 1992, it had become a valuable and respectable 
GUI-based OS [16]. Bundled with Windows 3.1 came WinG, a graphics API. 
Microsoft designed the WinG API to provide faster graphics performance on 
Windows 3.x and to make DOS games’ porting to Windows easier. Before WinG, the 
GDI (graphics drawing interface) was used to draw bitmaps through the Windows 
video driver, which gave unpredictable performance depending on who wrote the 
driver. 
Microsoft had set up a small Skunkworks-like software lab in the late 1980s to 
ﬁx bugs in Windows drivers and make them run faster and more reliably. One of the 
team members, Chris Hecker, developed a library-based driver that could run Doom, 
a top-rated DOS game, almost as fast on Windows as on DOS. WinG emerged from 
that development [17]. 
As mentioned above, games were a vital application sector for the PC and would 
become even more so with a bit-mapped display. WinG provided a library that elim-
inated the differences in performance between DOS and Windows graphics. That 
provided Windows-based games with equal or better performance to their DOS 
counterparts on the same hardware. 
Windows 3.1 ran as an add-on for DOS. It was 16-bit, kludgy and slow. In contrast, 
Windows 95 (code named Chicago and introduced in August 1995) was a new and 
complete operating system. It used WinG, a 32-bit graphics DLL (Dynamic Link 
Library), which approached the speed and in-game performance of DOS. Addition-
ally, the operating system offered support for joysticks, networks, and modems, and 
DOS no longer formed part of it [18]. 
With WinG, game developers could spend their time writing games rather than 
drivers; the hardware vendors would develop the driver and ensure it conformed to the 
installation standard, representing signiﬁcant industry stabilization (and maturity). 
The full potential of Win 95 was demonstrated when 32-bit games were run under 
Chicago in native mode, and WinG was a critical element in that. The 32-bit DIL 
allowed game developers to write graphics routines for Chicago and Win 3.1 (in 
16-bit format), giving graphics performance comparable to DOS. 
However suitable and welcome WinG was, it was still just a way to draw bitmaps 
in memory and output frames when the drawing was ﬁnished—it did not scale and 
had an inherent speed limit due to memory read-writes. 
In 1991, Servan Keodjian, Kate Seekings, and Doub Rabson founded the London-
based company RenderMorphic to develop the C-based three-dimensional rendering 
library Reality Lab and various middleware tools. That was able to work with many 
other APIs on different platforms and had its own highly efﬁcient API. 
That API provided a standardized interface designed for creating games and was 
one of the three major providers of real-time 3D middleware on the market at the time; 
the others were Argonaut Software’s BRender and Criterion Software’s RenderWare. 
RenderMorphics’ Reality Lab software was a scene graph API that would run 
with acceptable performance on graphics AIBs or the CPU of the host computer. 
During the early to mid-1990s, games ran on the CPU rather than the graphics board.

212
6
Application Program Interface (API)
Jake Richter, one of the founders of VESA and the developer of the VAGI VESA 
API, explained in his column in The PC Graphics Report that if a wide range of 
applications was 3D-enabled, which could help create a demand for broad, afford-
able 3D hardware support [19]. In addition, widely available, low-cost 3D hardware 
justiﬁed the development of 3D-enabled software. However, Richter pointed out that 
there was an obvious problem with that approach, in that it was a chicken and egg 
situation—which one would happen ﬁrst? He added: “It has been an issue for any 
3D API on the PC, as well as for all the myriad high-end 3D graphics boards and 
applications various vendors have tried to be successful within the PC market (and 
frequently failed with as well).” 
Ironically, it looked as if the thing that would break the chicken/egg cycle would 
be pure software. Enter 3D rendering APIs. More speciﬁcally, 3D rendering APIs 
provided decent performance but without the use of hardware acceleration, and those 
included Intel’s 3DR, Criterion’s RenderWare, RenderMorphics, Microsoft’s Win/G, 
Argonaut, and the proprietary mechanism used by the gruesome but insanely popular 
3D game Doom, by ID Software. However, the performance of all those mechanisms 
depended on the system CPU rather than on the graphics hardware. 
Mike King of Criterion explained that RenderWare’s main task was to render a 
local PC memory-resident buffer, which then got copied to the display on a frame-by-
frame basis. However, even if the graphics hardware had extra capabilities, Render-
Ware did not take advantage of them; the only exception was when a low-level 3D 
library, such as that offered by Matrox for its MGA boards, was available. In that 
case, RenderWare could use the functionality but potentially sacriﬁced some of the 
performance it gained from cutting corners in terms of mathematical accuracy in 
favor of visual presentation and performance. 
Doom worked fundamentally the same way (and with no hardware acceleration 
support at all) in rendering to a PC-based memory buffer. Following that approach, 
Microsoft designed its Win/G interface to provide virtual memory to screen blitting 
capabilities to Windows applications. 
To move game developers from DOS to the Windows environment, Microsoft 
showed Doom running under Win/G Argonaut, and RenderWare libraries also 
supported Win/G. 
With the visual performance that these software-only renders seem to provide, one wonders 
why 3D hardware is even necessary. 
“With the visual performance that these software-only renders seem to provide, 
it almost leaves one wondering why 3D hardware is even necessary,” commented 
Richter. “Then you see Jurassic Park, T2, or some other such 3D graphics animation 
feature, or play some of the newest VR games, and realize software may be ﬁne 
for some things, but powerful graphics hardware was required nonetheless, for real 
improvements in display performance. Now, if only the various APIs would take 
advantage of it—it will happen, and I would argue, must happen, as part of the 
normal evolutionary process in PC technology. However, the low-cost 3D software, 
using software rendering needs to happen ﬁrst, to provide for future extensions which 
deal with hardware acceleration.”

6.1 Application Program Interface
213
It was clear that the PC was on the threshold of a signiﬁcant move towards 
accelerated 3D graphics, and Microsoft had to get out in front of that. 
After a little over 3 years on the market, Microsoft acquired RenderMorphics in 
February 1995 and used Reality Labs as the basis for Direct3D. Reality Lab was a 
3D computer graphics API created by RenderMorphics. The API had an immediate-
mode and a retained-mode functionality. And in June 1996, Direct3D was shipped 
for the ﬁrst time in the DirectX 2.0 SDK. 
The very ﬁrst version of Direct3D used a retained-mode API, a technique in 
which the graphics library keeps the scene (complete object model of the rendering 
primitives) in memory for rendering (see Fig. 6.5). 
In contrast, an immediate-mode API was procedural. The graphics library did 
not store a scene model between frames when drawing a new frame. The application 
issues the drawing commands, and the application keeps track of the scene (Fig. 6.6). 
Since APIs do more work, including initialization, state maintenance, and cleanup, 
retained-mode APIs could be simpler to use, but as APIs imposed their scene models, 
they were less ﬂexible. A retained-mode API could also require more memory, as 
it needed to provide a general purpose scene model. With an immediate-mode API, 
targeted optimizations were implemented. 
Microsoft envisioned the APIs as thin, with no unnecessary OS calls, no check-
sums, and limited libraries, and called them “direct,” i.e., directly to/from memory.
Fig. 6.5 A retained-mode API 
Fig. 6.6 An immediate-mode API 

214
6
Application Program Interface (API)
The names of all its APIs began with Direct, such as DirectDraw, DirectMusic, 
DirectPlay, DirectSound, and of course, Direct3D. The name DirectX got applied 
as a variable term for the speciﬁc API. That convention evolved into a pattern for 
referring to the collection of APIs. Hence, in 2000, when Microsoft launched the 
console project, they used X in the name and called it the Xbox—a subtle, unde-
clared signal that the console used DirectX technology. The term “X” carried its 
magic, and gamers and the press rallied and loved using it. As a result, the X brand 
has been carried forward when naming Microsoft’s consoles and its APIs. 
6.1.2.1 
Hardware Feature Levels 
In addition to Shader models and Render models, in DirectX 11, Microsoft added 
Feature levels. 
To handle the diversity of GPUs and AIBs in new and existing PCs, with Direct3D 
11, Microsoft introduced the concept of feature levels. Each IHV’s GPU could 
employ a certain level of Microsoft DirectX (DX) functionality depending on its 
capabilities. A feature level was a well-deﬁned set of GPU functions. For instance, 
the Dx 11.0 feature level implemented the functionality in Direct3D 11 [20]. 
Hardware feature levels are different from API versions. For example, there was 
a Direct3D11.3 API, but there was no 11.3 hardware feature level. 
There were three different numbering systems: 
• Direct3D versions use a period; for example, Direct3D 12.0. 
• Shader models use a period; for example, Shader Model 5.1. 
• Feature levels use an underscore; for instance, feature level 12_0. 
Feature levels captured the speciﬁc capabilities of a GPU and the mandatory 
requirements of an API. The feature levels could exploit a particular capability of 
the API. The levels got grouped in strict supersets of each other, so each higher level 
includes all features required on every lower level. 
6.1.2.2 
Microsoft’s Japanese Bigotry 
Microsoft’s DirectX codename was Manhattan Project. Alex St. John, who was the 
head of Microsoft’s DirectX evangelism at the time, claimed the name was chosen 
because the DirectX team wanted to displace Japanese video game-makers from their 
dominance of the video game industry [21]. However, Microsoft publicly denies that 
account, instead claiming that the logo was merely an artistic design [22]. Nonethe-
less, Microsoft’s console codename was Midway, which appears to be another slur 
relating to defeating the Japanese company Sony. 
Of course, that was devised in the mid-1990s by young, enthusiastic engineers, 
and that judgment was made with the sensitivities of 2022. Times and people change.

6.1 Application Program Interface
215
Fig. 6.7 Timeline of the evolution from IRIS GL to OpenGL evolution. Reproduced with 
permission from SIGGRAPH Asia 2008 
6.1.3 
The History of OpenGL 
Mark Segal and Kurt Akeley developed the IRIS-GL (Integrated Raster Imaging 
System Graphics Library) at SGI in the early 1980s. A proprietary graphics API 
was created to produce 2D and 3D computer graphics on SGI’s IRIX-based IRIS 
graphical workstations [23]. In 1993, SGI removed their proprietary code from IRIS-
GL, reworked several system calls, and released the industry-standard OpenGL [24]. 
SGI began developing OpenGL in 1991 and released it on June 30, 1992 [25, 26]. 
The evolution of IRIS GL to OpenGL is shown in Fig. 6.7. 
The ﬁrst version of OpenGL, version 1.0, was released on June 30, 1992, by Mark 
Segal and Kurt Akeley. The evolution of OpenGL is shown in Fig. 6.8.
When OpenGL 4.x was released in 2010, it required support for FP64 shader 
compliance. AMD and Nvidia met that requirement by running their 32-bit FPU 
in two cycles to pass Khronos’ compliance test. As a result, when one looks at the 
speciﬁcations, the performance of FP64 (measured in TFLOPS) was often half that 
of FP32. 
OpenGL continued to evolve under the stewardship of Khronos, and the last 
version, 4.6, was released in 2017. At the time, it was expected that the Vulkan API 
would replace OpenGL. 
6.1.4 
The Fahrenheit Project 
While Microsoft was learning about 3D APIs and the associated tools and middle-
ware, programmers needed to develop applications. Many of them already knew how

216
6
Application Program Interface (API)
Fig. 6.8 The history of OpenGL to 2008. Reproduced with permission from SIGGRAPH Asia 
2008
to use OpenGL, which became the API of choice for the technical market—a market 
Microsoft coveted and hoped to take away from Unix with NT. Microsoft now saw 
itself as competing with OpenGL (Fig. 6.9). 
In late 1995, SGI entered the beginning of a long decline, which ended in early 
2009 when it sold its assets to a smaller company named Rackable Systems (which 
then renamed itself SGI.) 
In 1997, SGI began dropping projects, laying off employees, and selling assets. 
The company also had its ﬁrst new president since 1986. Bo Ewald former, chief 
at Cray Computers was tapped to help out after the departure of Tom Jermoluk, Ed 
McCracken, and more. 
Desperate for cash, SGI announced a strategic alliance with Microsoft in late 
1997 to create a common, extensible architecture that would bring advanced and 
powerful graphics to the computer market, i.e., future Windows-based products. 
The companies agreed to jointly deﬁne, develop, and deliver those new graphics 
technologies as part of a project code named Fahrenheit [27].
Fig. 6.9 The Fahrenheit 
project was a clever idea that 
did not work out (image used 
with permission from 
Microsoft) 

6.1 Application Program Interface
217
The Fahrenheit project involved creating a suite of APIs for Microsoft’s DirectX 
architecture on the Windows operating system and the SGI UNIX-based platform. 
Fahrenheit would combine Microsoft Direct3D and DirectDraw APIs with SGI’s 
complementary technologies, such as OpenGL, OpenGL Scene Graph, and OpenGL 
Optimizer. 
SGI and Microsoft had been working together since 1991 to develop OpenGL for 
Windows NT, and the Fahrenheit project inaugurated the next phase of that long and 
somewhat testy relationship. 
In 1998, SGI would develop the Fahrenheit Scene Graph, the primary mid-sized 
API used in most applications, and Fahrenheit Large Model, a modiﬁed version for 
handling large models in CAD applications. Microsoft provided a new low-level 
rendering engine for Windows known as Fahrenheit Low-level, designed to replace 
the Reality Lab-based version of Direct3D. 
By 1999, although Microsoft was ofﬁcially developing the low-level API, there 
was no intention to see it through and no resources dedicated to it. Microsoft was 
signiﬁcantly investing in its next API, DirectX 7.0, and Fahrenheit could not be deliv-
ered on Windows without a Low-Level API, and the project died. Eventually, SGI 
abandoned work on Fahrenheit and started working on other Scene Graph products, 
the last in many such ventures. The only tangible thing that came out of that project 
was some cool-looking T-shirts. 
6.1.5 
Low-Level APIs 
An API, that offers the highest detail level and allows a programmer to manipulate 
functions within a software module or hardware at a very granular level, is a low-level 
API. It was also sometimes referred to as programming close to the metal (CTM) or 
a thin API. That meant as few translation levels as possible between the driver code 
and the GPU’s registers to eliminate latencies and overhead. 
In this section, we will look at modern low-level APIs and some of their history.

218
6
Application Program Interface (API)
6.1.5.1 
Mantle 
In the fall of 2013, AMD revealed its Mantle program, which was a low-level API 
speciﬁcation developed as an alternative to DirectX and OpenGL, primarily for 
gaming (Fig. 6.10). That offered a tighter (or ‘thinner’) interface to the GPU for 
the application and wholly exposed all the GPU’s features. AMD claimed that the 
improved communication efﬁciency would allow up to nine times more draw calls 
than any other API. 
Mantle worked with Microsoft’s High-level shading language (HLSL), used in 
DirectX. Shaders that used HLSL were usually tightly written programs and some-
times referred to as hand-coded (instead of automatically compiled by a software 
tool). However, they were high-level and only tightly written if the coder was highly 
skilled. They were the most efﬁcient but also usually the buggiest. However, the ones 
that worked were reused in various games (and other apps) and were the best and 
had optimized loops that had been tweaked and tuned over the years. 
AMD was encouraged to develop Mantle by one of its game console builder 
customers. Electronic Arts’ DICE operation announced that the Frostbite 3 engine in 
Battleﬁeld 4 used Mantle. Mantle was also out in the open for anyone to use for free. 
Others who were thinking about developing a low-level API could look at Mantle 
and take some ideas from it. 
Multithreading was that tie forward used extensively for command scheduling 
rather than the current single-threaded model. (Multithreading in OpenGL was
Fig. 6.10 AMD’s Mantle program. Reproduced with permission from AMD 

6.1 Application Program Interface
219
reportedly even worse than its Direct3D counterpart, which held back the use of 
GPUs for compute acceleration.) 
6.1.5.2 
Metal 
In mid-2014, as part of its introduction of iOS 8, Apple introduced its proprietary API, 
Metal; it would provide access to its GPUs’ graphics and compute capabilities. Like 
the other new APIs, metal represented a move away from the traditional OpenGL 
architecture, which had served the industry well for 25 years, to an API that mapped 
more readily to the features of modern GPUs while also increasing the efﬁciency of 
issuing commands from the CPU. 
However, although a Metal driver was thinner than OpenGL, Apple decided to 
forgo the detailed level of hardware control that Vulkan and DirectX 12 provided 
(to enable the highest performance levels) in return for a more straightforward API 
to program (Fig. 6.11). Apple also integrated the powerful compute capabilities of 
OpenCL (that it had initiated at Khronos) into Metal. 
At Apple’s developer conference in 2014, Crytek, Unity, Electronic Arts, and Epic 
Games announced support for Metal in their respective engines. 
Metal, Mantle, and DirectX 12 appeared within months of one another. Each had 
low latency as a primary goal and borrowed concepts from the CTM proprietary 
APIs used in games consoles. High-budget (AAA) game developers requested they 
be made available in desktop and mobile APIs. AMD developed Mantle, and although 
it was never widely shipped, it formed the basis for the development of both DirectX 
12 and Vulkan, whereas Apple independently developed Metal.
Fig. 6.11 Apple’s depiction of a thin API between the GPU and application. Reproduced with 
permission from Apple 

220
6
Application Program Interface (API)
6.1.5.3 
Vulkan 
The non-proﬁt Khronos Group announced Vulkan at GDC 2015 [28]. Initially, the 
API was referred to as the next-generation OpenGL initiative or OpenGL next [29], 
but Khronos discontinued using those names when it announced Vulkan [30]. 
Vulkan used constructs from AMD’s Mantle API. AMD donated Mantle to the 
Khronos group, replacing the shader part with their Standard, Portable, Intermediate 
Representation, SPIR-V. However, contrary to some people’s opinions on API devel-
opment, DirectX 12 and Vulkan were directly inﬂuenced by Mantle, as illustrated in 
the family tree in Fig. 6.12. 
Khronos also signiﬁcantly refactored (turn dirty code into clean code) Mantle to 
make Vulkan portable across multiple mobile and desktop vendors and use SPIR-V 
(Fig. 6.13).
Sixty software developers from dozens of organizations came together and 
developed the Vulkan API for several years. 
6.1.5.3.1
Cross-Platform Support 
Running Linux in the cloud was less expensive than Windows for vendors building a 
game streaming service. As a result, organizations wanted to run DirectX games on
Fig. 6.12 A family tree of graphics APIs. Reproduced with permission from Neil Trevett 

6.1 Application Program Interface
221
Fig. 6.13 The Vulkan development team, circa 2017. Reproduced with permission from Khronos
Fig. 6.14 API layering. Reproduced with permission from Khronos 
Linux systems and future Arm-based Linux servers. Vulkan was a new generation 
3D API available on Linux (and many other OS). such as Steam Deck. 
Signiﬁcant effort had been put into layering frameworks such as Valve’s Proton 
that used the open-source DirectX-over-Vulkan (DXVK) [31] to layer DirectX 9– 
11 over Vulkan and vkd3d-Proton for DirectX 12 over Vulkan. Vulkan evolved 
signiﬁcant functionality to ensure it effectively accelerated layered DirectX 9–12, 
including ray tracing (Fig. 6.14). 
Stadia, Google’s online gaming service, used Debian Linux. That required game 
developers to use the Vulkan cross-platform API, which slowed the development of 
ports to Stadia more than other cloud gaming services, as developers had to build it 
for Stadia (whereas GeForce Now used native Windows drivers). In 2021, Google 
introduced a new Stadia Porting Toolkit, eliminating the need to port from DirectX

222
6
Application Program Interface (API)
to Vulkan manually. That toolkit included a set of translation libraries for DirectX to 
Vulkan, comparable to DXVK and VKD3D-Proton for Steam Play Proton. Proton 
developed a compatibility layer that allowed Windows games to run on Linux-based 
operating systems, and CodeWeavers developed Proton For Valve in cooperation 
with developers. 
6.1.6 
WebGPU 
On June 8, 2016, Google presented an Explicit web graphics API to the WebGL 
working group. The presentation explored the basic ideas and principles of building 
a new API to eventually replace WebGL, aka WebGL Next. 
Then on January 24, 2017, Khronos hosted an IP-free meeting dedicated to discus-
sion of WebGL Next ideas, which collided with WebGL working group meeting in 
Vancouver. The Google team presented the NXT prototype implementing a new API 
that could run in Chromium with OpenGL, or standalone with OpenGL and Metal. 
NXT borrowed concepts from all of Vulkan, Direct3D 12, and Metal native APIs. 
Apple and Mozilla representatives also showed their prototypes built on Safari and 
Servo correspondingly, both of which closely replicated Metal API. 
WebGPU (WGPU) is an API that exposes the capabilities of GPU hardware for 
the Web. It was developed by the W3C GPU for the Web Community Group with 
engineers from Apple, Mozilla, Microsoft, Google, and others. 
The API is designed to efﬁciently map to native GPU APIs. WebGPU is not related 
to WebGL and does not explicitly target OpenGL ES. WGPU is a cross-platform, 
safe, API developed in the Rust programming language. It runs natively on Vulkan, 
Metal, D3D12, D3D11, and OpenGLES; and on top of WebGPU on wasm. 
After 4 years of development in the W3C’s GPU for the Web Community Group, 
WebGPU was ready for developers to try in Chrome and give feedback on the API 
and the shading language on August 26, 2021. 
WebGPU interprets physical GPU hardware as GPUAdapters. It provides a 
connection to an adapter via GPUDevice, which manages resources, and the device’s 
GPUQueues, which execute commands. 
GPUs execute commands encoded in GPUCommandBuffers by feeding data 
through a pipeline, which is a mix of ﬁxed-function and programmable stages. 
Programmable stages execute shaders, which are special programs designed to run 
on GPU hardware. Most of the state of a pipeline is deﬁned by a GPURenderPipeline 
or a GPUComputePipeline object. The state not included in these pipeline objects is 
set during encoding. 
First appearing in 2010, Rust was designed by Graydon Hoare at Mozilla Research 
[20] with contributions from Dave Herman, Brendan Eich, and others. Rust is a 
multi-paradigm, general-purpose programming language designed for performance 
and safety, especially safe concurrency. Syntactically similar to C++, Rust is notable 
for enforcing memory safety—that is, that all references point to valid memory— 
without requiring the use of a garbage collector or reference counting like other

6.1 Application Program Interface
223
memory-safe languages. Rust is considered a systems programming language with 
mechanisms for low-level memory management, but also offers high-level features 
such as functional programming. 
6.1.7 
DirectX 12 
At GDC in San Francisco, in March 2014, Microsoft announced DirectX 12 and 
said it would be released next year. Microsoft ofﬁcially launched Dx12 along with 
Windows 10 on July 29, 2015. The primary feature, highlights for the new release of 
DirectX, was the introduction of advanced low-level programming APIs for Direct3D 
12, which could reduce driver overhead. 
The major difference between DX11 and DX12 was DX12 and was more low-
level like Mantel, meaning DX12 gave developers more ﬁne-grained control of how 
their game interacted with the CPU and GPU. 
Although Microsoft never explicitly stated that DirectX 12 was related to Mantle, 
their initial documentation was word-for-word the same as Mantle’s in many places 
before people took notice and they reworded their docs. 
Like Mantel and Metal, DirectX 12 offered developers unique low-level access 
to the GPU [32]. However, although that enabled developers to write considerably 
faster and more efﬁcient code, it came at a cost: the API was more complicated, 
which meant more opportunities for mistakes. 
In October 2018, DirectX 12 was enhanced with the DXR extension to support ray 
tracing, introduced by Nvidia with their Turing GPU and RTX 2080 AIB in August 
2018 at SIGGRAPH. Nvidia had demonstrated real-time ray tracing using four Volta-
based Titan V AIBs in March 2018 when Microsoft announced the DirectX 12 DXR 
extension. 
6.1.7.1 
Ultimate 
In late 2019, Microsoft announced it would be releasing an expansion to DirectX 12, 
DirectX 12 Ultimate. 
DirectX 12 Ultimate was a tsunami in the computer graphics industry, changing the 
landscape forever. It introduced four substantial changes to the GPU’s functionality 
at once. Any one of them would have been signiﬁcant, four at once was stunning, 
and it took developers years to fully realize and successfully exploit them. 
Microsoft knew DirectX 12U was signiﬁcant and generated a logo for it, the ﬁrst 
one for a speciﬁc version in the API’s 25-year history. Microsoft called it the culmi-
nation of the best graphics technology we’ve ever introduced in an unprecedented 
alignment between PC and Xbox Series X (Fig. 6.15).
DirectX 12 Ultimate did not affect a game’s compatibility with one’s existing 
hardware if it did not support the entire breadth of DirectX 12 Ultimate features.

224
6
Application Program Interface (API)
Fig. 6.15 Microsoft’s 
DirectX 12 Ultimate logo 
(Used with permission from 
Microsoft)
Newer games that used DirectX 12 Ultimate would run on non-DirectX 12 Ultimate 
hardware; they just would not invoke DirectX 12U features. 
The new DirectX 12 Ultimate API calls did not just enable new hardware features; 
they provided access to lower level (and potentially more efﬁcient) hardware features 
and resources already present. That was beneﬁcial to the installed base of Nvidia 
RTX AIBs. AMD announced that its RDNA 2 GPUs would have full support for the 
DirectX 12 Ultimate API, but no prior generations of AMD AIBs. 
In early 2020, Microsoft ofﬁcially introduced DirectX 12 Ultimate [33]. The 
feature set added to the API included support for new and future generation graphics 
hardware features, including DirectX ray tracing, variable rate shading, mesh shaders, 
and sampler feedback. 
When DirectX 12 Ultimate was released, only Nvidia RTX 3000 series AIB 
were able to use all its features. AMD’s RX5000 available at the time could not. 
In March 2020, AMD announced its next-generation RDNA2-based AIBs would 
support DirectX 12 Ultimate and pointed out that the AMD APUs in the new 
Microsoft X Box Series X and Sony PlayStation 5 did support DirectX 12 Ulti-
mate. Then in March 2021, AMD introduced their Radeon RX 6000 AIBs with 
DirectX 12 Ultimate compatibility. 
6.1.7.1.1
Ray Tracing 
Unlike previously used heuristics-based calculations, DirectX Ray tracing (DXR 
1.1) involved tracing light paths with accurate physics calculations, giving a far 
more precise simulation. DXR 1.1, which was an incremental add-on to DXR 1.0, 
included three signiﬁcant new capabilities: 
• Streaming: With DXR 1.1, streaming engines could load ray tracing shaders more 
efﬁciently when users moved around the world and new objects became visible.

6.1 Application Program Interface
225
• GPU ray tracing work creation: Work creation enabled shaders on the GPU to 
invoke ray tracing without an intervening round-trip back to the CPU. One of the 
essential features of GPU ray tracing was work compaction. When more than half 
of the rays in a warp (a group of 32 threads) terminated, the warp ended non-
terminated rays got copied to the next warp. DXR work creation helps in adaptive 
ray tracing scenarios such as shader-based culling, sorting, classiﬁcation, and 
reﬁnement. In those scenarios, ray tracing work was prepared on the GPU and 
spawned immediately. 
• Inline: A form of ray tracing that allowed developers to control the ray tracing 
procedure explicitly is called inline. It got used as an alternative to scheduling 
work for the system (also known as dynamic shading). Any shader stage could 
invoke it, such as pixel shaders, compute shaders, etc. The inline forms of ray 
tracing and dynamic shading use opaque acceleration structures. 
Microsoft’s new inline and dynamic shader ray tracing got used in different situ-
ations. With DXR 1.1, developers chose between the two approaches, which could 
be combined within a single renderer. Both types of DXR ray tracing use the same 
acceleration structure format, which was beneﬁcial for hybrid rendering. The same 
underlying traversal state machine drove those approaches. 
6.1.7.1.2
Variable Rate Shading 
Variable rate shading (VRS), introduced in 2018 by Nvidia in its Turing GPU, enables 
content creators to control the shading rate on an object-to-area basis. A shader 
calculates the attributes of a pixel. The pixels could be individually shaded or in 
groups. The number of pixels shaded at one time is called the shading rate, or the 
shading resolution (and is different from the screen resolution). The higher that rate, 
the fewer pixels are shaded together, and thus the higher the ﬁdelity of the image. 
Shading more pixels uses more of the GPU [34]. 
Referring to Fig. 6.16, without VRS, each pixel in the scene would need to be 
shaded individually (as shown in the blue grid with size 1 × 1). However, when using 
VRS, a developer could vary the shaded triangles.
The sky, the car, and the foliage used full-rate shading colored overlays shown to 
the right of the image. The green area next to the car was shaded once per four pixels. 
To the far left, the road and periphery was shaded once per eight pixels (yellow) [35]. 
VRS allowed content creators to reduce shading in an image where the visual 
quality would not be affected; that could release GPU resources and provide better 
performance in an application such as a game.

226
6
Application Program Interface (API)
Fig. 6.16 The shading rate is dynamically adjusted across the image, meaning that each 16 × 
16-pixel region of the screen could have a different shading rate. Reproduced with permission from 
Nvidia
6.1.7.1.3
Getting to Mesh Shaders 
Mesh shaders opened up the GPU as a proper parallel processing compute engine and 
did away with ﬁxed function shaders and other specialized engines. Mesh shaders 
were ﬁrst tried on CPUs, and a lot of learning and discovery was made there. 
To appreciate mesh shaders, we must step back in time and look at the evolution 
of the GPU. 
Computer graphics depends on geometry and rendering. The geometry, based on 
a triangle and its vertices, and the triangles make up a model must get translated 
from the model’s space (x, y, z) to the computer’s viewport space (u, v, w). Lighting 
or coloring information also gets manipulated in the transformation process. In the 
early days of computer graphics, the CPU’s FPUs did that work then sent it to a 
rasterizer for display. 
In the early 1980s, Jim Clark introduced a dedicated ﬁxed function geometry 
engine and moved the translation operation to the geometry pipeline of the graphics 
subsystem. Silicon Graphics was based on Clark’s design and became the première 
graphics company between 1981 and the early 2000s. 
6.1.7.1.4
HW T&L Engines, 1981 to 1996: IRIS GL to Direct X 7.0 
The foundation for the GPU was laid by the Pixel Planes project. Jim Clark, founder of 
SGI, in 1981, moved the development further with the introduction of the geometry 
engine. The GPU era began with the ﬁrst fully integrated 3D chip with T&L in 
1999 by Nvidia. In 2009, DirectX 7.0 (Direct3D 7) in Windows 2000 was the ﬁrst 
consumer API to support hardware T&L, although OpenGL had supported it earlier 
for 3D accelerators designed for CAD. Arcade game systems had used hardware 
T&L since 1993 and home video game consoles since the Nintendo 64’s Reality 
Coprocessor GPU of 1996.

6.1 Application Program Interface
227
Fig. 6.17 Clipping the portion of a model that is not visible 
Transformation is the task of producing a 2D view of a 3D scene. Clipping means 
drawing only those parts of a scene that remained in the view area, illustrated in 
Fig. 6.17. Lighting is the task of altering the color of the various surfaces of the 
scene based on lighting information, and culling avoids processing those parts of the 
model that are not visible. 
The ﬁrst T&L engines were ﬁxed function modules, and those would give way to 
programmable SIMD vector processors known as shaders. 
The ﬁrst GPUs used a ﬁxed method of transforming pixels and vertices in a ﬁxed 
function pipeline. That made it difﬁcult for developers to modify how pixels and 
vertices were transformed or processed after passing them to the GPU. Opportunities 
for differentiation and experiment were limited, and everyone’s results were similar. 
6.1.7.1.5
Vertex and Pixel Shaders, 1997 to 2008: Direct3D 10 
Vertex shaders are used once for each vertex and then sent to the graphics processor. 
They transform the 3D position of each vertex in virtual space to the viewport 
(display) coordinates, i.e., the visible 2D coordinate portion of the screen based 
on the depth value (from the z-buffer). DirectX 8 changed all that (Fig. 6.18).
The ﬁrst generation of GPUs had code embedded within them; those were ﬁxed 
function operations and were are not very programmable, if at all. Chipmakers added 
new features by redesigning or upgrading GPUs and adding new hardware for extra 
features or functions and more programmability. 
New features such as cube maps, DOT3 bump mapping, register combiners, 2 × 
anisotropic ﬁltering, trilinear ﬁltering, and DXT texture compression were added to 
those new GPUs. 
Microsoft developed an assembly language program to address that issue, solving 
several problems and making specialized shaders possible. However, the required

228
6
Application Program Interface (API)
Fig. 6.18 DirectX 8 introduced The programmable vertex shader in 2000
Fig. 6.19 Expanded view of the geometry pipeline 
developers to work in assembly language; that made it challenging to develop shaders, 
and Windows Direct3D 8.0 only supported Shader Model 1.0. Assembly language 
is explicit, complex to program, and hard to read [36] (Fig. 6.19). 
We have George Lucas and the Pixar team to thank for moving the industry from 
assembly language to a C-like shading language, ﬁrst implemented in the Reyes 
renderer [37] in 1984. Reys (render everything you ever saw) gave rise to the  world-
famous RenderMan (Fig. 6.20).
Eventually, a high-level shading language reached the PC in 2002, with DirectX 
9 in Windows XP. DirectX 9 used HLSL, created to enable a programmable 3D 
pipeline. The entire pipeline could be programmed using HLSL instructions rather 
than the assembly shading language. Vertex shaders programmed in C using HLSL 
were found in the DirectX, OpenGL [38], and Vulkan APIs and embedded into the 
device driver. 
In 2007, DirectX 10 introduced a new shader called the Geometry Shader, which 
formed part of Shader Model 4.0 [39]. DirectX 11, released in 2009 with Windows 
7, included shaders for tessellation and compute shaders for GP-computer.

6.1 Application Program Interface
229
Fig. 6.20 George Lucas (AP 
Wirephoto—eBay, public 
domain, Wikipedia)
6.1.7.1.6
Tessellation 
In 2002, ATI presented a demo of dynamic terrain rendering on a GPU using real-
time tessellation, using a custom library extension to DirectX 9. That approach was 
also capable of tessellating meshes. The demo showed the conversion of a ﬂat, 900-
polygon grid into a 1,000,000-polygon mountain landscape at interactive frame rates. 
However, although game developers were impressed, they were not willing to disrupt 
their projects to use it, and even though Microsoft would use it in their Xbox 360 
(2005), they did not add the extension to DirectX. 
Tessellation did not get included in DirectX 10 either, so ATI took the hardware 
accelerator out of their GPU, although not before they had created more impressive 
demonstrations (Fig. 6.21).
Tessellation allowed a highly detailed mesh sent to the GPU as a simple lattice 
and a displacement map. That process traded the ALU operations in a shader for 
memory bandwidth, and thanks to Moore’s law, ALUs scaled faster than bandwidth. 
Tessellation was considered for Direct3D 10, but later abandoned. GPUs such as 
the Radeon R600 featured a tessellation engine that could be used with Direct3D 
9/10/10.1 and OpenGL but was not compatible with Direct3D 11 [40]. 
Finally, in DirectX 11 (October 2009), Microsoft added programmable tessellation 
(Table 6.5). It allowed the programmer to decide which objects were tessellated. That 
meant that objects closer to the screen could be tessellated, while things in the distance 
we are not, and that the level of detail would increase as objects came closer, rather 
than just becoming larger. Thus, resources were not wasted on meshes t too far away 
for the tessellation effects to be viewed [41].
ATI continued its pioneering approach to tessellation using hardware. In June 
2009, at the Computex conference in Taipei, Taiwan, ATI (now part of AMD) 
presented their DirectX 11 AIB, the RV870-based Radeon 5870.

230
6
Application Program Interface (API)
Fig. 6.21 Increasing the level of detail of a model through tessellation. Reproduced with permission 
from GDC
Table 6.5 New 
programmable features 
introduced to the API in 
DirectX 10 and 11 
Feature
DX10
DX10.1
DX11 
Tessellation
N
N
Y 
Shader model 5.0
N
N
Y 
HDR texture compression
N
N
Y 
Multi-threading
Y
Y
Y 
Direct compute (vision)
Y
Y
Y
Nvidia introduced their DirectX 11 AIB with hardware tessellation as part of their 
Fermi-based GeForce 4xx series of AIBs, released in April 2010. 
6.1.7.1.7
The Pipeline Expands 
By that time, GPUs and APIs had evolved to include a dizzying list of functions, 
such as: 
• Vertex shaders 
• Geometry shaders 
• Tessellation shaders 
• Hull shader stages (only used for tessellation) 
• Domain shader stages (only used for tessellation)

6.1 Application Program Interface
231
• Geometry shader stages 
• Pixel shader stages 
In addition to the major shading stages in a geometry pipeline, the setup processes 
also need to be run. To be as efﬁcient as possible, triangle fans—connected triangles 
were used. 
When an object gets drawn in 3D space, one of the ﬁrst things is to run a shader to 
process the vertices and make decisions about the view space. That involved culling 
if a primitive was inside or outside the view; if objects were occluded from view, 
there was no reason to include them in the rendering pipeline. Bounding boxes are 
used to decide if the scene is complex and contains a lot of objects. The region visible 
on the screen is the viewing frustum, and frustum culling is a method of determining 
hidden surfaces; refer to Fig. 6.17. 
The vertex shader had been a remarkably effective programming model. It 
expressed a simple concept that only one vertex is shaded in isolation, thus enabling 
the GPU to render an image massively parallell. However, there were some prob-
lems with that model. For example, the input layout was somewhat inﬂexible—the 
vertex shader took in index buffers (with compressed geometry data). Then the vertex 
shader buffer fed the input assembler. It was a ﬁxed function process that limited 
the index shader’s acceptance. DirectX 10 eliminated the use of triangle fans, and 
the assembler did not allow for software emulation. In addition, culling could not be 
applied until the very end, meaning the shading of a vertex needed to be completed 
before it could be culled. That meant the GPU would have done a lot of work before 
determining if a given triangle did not appear within the viewing frustum. 
GPU hardware included the following: 
• A set of SIMD processor cores to process a group of threads (the data) 
simultaneously. 
• A relatively small amount of fast on-chip memory (cache) used for: 
– Writing outputs 
– Exchanging information between threads (group shared memory) 
– Wave intrinsics—in earlier Shader Models, HLSL programming exposed 
only a single thread of execution. New wave-level operations, starting with 
model 6.0, provided an explicit advantage of the parallelism of current GPUs, 
meaning that many threads could be executed in lockstep on the same core 
simultaneously. 
…and that is precisely what a GPU shader was. 
Meanwhile, the vertex shader continued to evolve. Despite the addition of hull, 
domain, geometry shaders, etc., none of it looked like a SIMD HW shader, and it 
was effectively a linear pipeline (Fig. 6.22).
They all ran on a single thread. Looking (and being used like) a single thread 
defeated the need for tessellation to expand in thread count—more points out than 
went in. And geometry shaders do not map well to a single thread at all. 
By 2005, it became painfully evident that the GPU had become overburdened 
with too many specialized ﬁxed function shading operations, and those resources

232
6
Application Program Interface (API)
Fig. 6.22 The geometry pipeline as of 2007—long and complicated
were being wasted in the process. The ﬁrst graphics AIB with a programmable 
pixel shader was the Nvidia GeForce 3 (NV20), released in 2001. Geometry shaders 
were introduced with Direct3D 10 and OpenGL 3.2. Eventually, graphics hardware 
evolved toward a uniﬁed Shader Model. 
6.1.7.1.8
Uniﬁed Shader, 2006–2010: DirectX 9.0 and OpenGL 3.3 
A uniﬁed shader architecture was a hardware design in which each shader processing 
unit in a GPU can handle any type of shading task. That allowed GPUs to make more 
efﬁcient use of their processing power. 
ATI introduced a uniﬁed architecture in the hardware they developed for the Xbox 
360, which supported DirectX 9.0c, and Nvidia quickly followed with their Tesla 
GeForce 8 AIB series design in November 2006. AMD introduced a uniﬁed shader 
in the TeraScale R600 GPU, a “graphics next” architecture used on the Radeon HD 
2900 AIB in 2007. 
The new uniﬁed shader functionality used a very long instruction word (VLIW) 
architecture, in which the GPU executed operations in parallel. The R600 core, like 
the Nvidia Tesla core, processed vertex, geometry, and pixel shaders as outlined by 
the Direct3D 10.0 speciﬁcation for Shader Model 4.0 (see glossary), in addition to 
full OpenGL 3.0 support. 
In DirectX, the ﬁrst uniﬁed shader was model 4.0, introduced in 2006 as part of 
Direct3D 10 in Windows Vista. In OpenGL 3.3 (2010). All three types of shaders 
were then merged into a single instruction set for all shaders. 
The die had been cast, so to speak, and the industry began to move closer to an 
all-compute programming model for the GPU. Additional information on uniﬁed 
shaders can be found in the chapter entitled What is a GPU?

6.1 Application Program Interface
233
Fig. 6.23 AMD’s vertex processor can be a domain shader or a vertex shader 
6.1.7.1.9
Getting to Compute: Task Shaders 2016–2017 (DirectX 12, 
Vulkan 2) 
In 2016, AMD introduced its new Graphics Core Next (GCN) ﬁfth-generation archi-
tecture, code named Vega, via a patent application. Within the GCN was a new class 
of shaders called primitive shaders [42]. According to Mike Mantor, AMD corporate 
fellow, primitive shaders would have “the same access that a compute shader would 
have to coordinate how you bring work into the shader.” Mantor also pointed out 
that primitive shaders would give developers access to all the data needed to process 
geometry effectively [43]. 
AMD’s Vega GCN microarchitecture added primitive shaders that were like 
compute shaders, with access to the data necessary to process geometry. Similarly, 
Nvidia introduced mesh and task shaders with its Turing microarchitecture in 2018; 
that provided similar functionality, and like AMD’s primitive shaders, was modeled 
on compute shaders. 
Vega’s primitive shader (or ‘prim shader’), illustrated in Fig. 6.23, used draw-
stream binning rasterization (DSBR) and small primitive discarding. Back-faced 
culling was employed and discarded items too small to render. The goal was to 
remove elements from the pipeline that did not require attention and free up the 
pipeline for more critical tasks. 
To increase the efﬁciency (in Vega), AMD looked for a way to discard primitives 
before storing the attribute data. There was a way to determine whether a vertex falls 
inside a view frustum. The advantage of that was that removing primitives before 
any attributes were stored. Viewport culling (VPC) was used to discard triangles. 
The core ideas of reconﬁgurable pipelines and task graphs have been around since I started 
this type of work, and primitive shader, dispatch draw and async compute node graphs, and 
mesh shaders are all very related. I expect things to continue to evolve until we see very 
ﬂexible heterogeneous systems. The mesh shader was initiated, in my humble opinion, as 
an outshoot from primitive shaders [44]—Mike Mantor

234
6
Application Program Interface (API)
Table 6.6 Comparison of the characteristics of the DirectX 8, DirectX 9, and DirectX 10 shaders 
DX8 SMI.x
DX9 SM2
DX9 SM3
DX10 
Vertex instructions
128
256
512
64 k 
Pixel instructions
4+8
32+64
512
64 
Vertex constants
96
256
256
16 × 4096 
Pixel constants
16
32
224
16 × 4096 
Vertex temps
16
16
16
4096 
Pixel temps
2
12
32
4096 
Vertex inputs
16
16
16
16 
Pixel inputs
4+2
8+2
10
32 
Render targets
1
4
4
8 
Vertex textures
na
4
128 
Pixel textures
8
16
16
128 
20 texture size
2 k  × 2 k
8 k  × 8 k  
Int ops
√ 
Load op
√ 
Derivatives
√
√ 
Vertex ﬂow control
Static
Static/dynamic
Dynamic 
Pixel ﬂow control
Static/dynamic
Dynamic 
6.1.8 
Microsoft DirectX Shader Model 4.0: Enhancements 
Microsoft speciﬁed attributes such as the number of instructions that can make up a 
shader program, the precision, and the number of registers available in their shader 
deﬁnitions (shown in Table 6.6). Table 6.6 compares the DirectX 9 and DirectX 10 
Shader Models [45]. 
Until the introduction of DirectX 10, shaders had been simply ﬂoating-point 
processors. Operations that used integer values, such as memory addressing and array 
indexing, therefore needed to be treated carefully to avoid interpolation. Microsoft 
added integer and bitwise operations with DirectX 10, enabling programmers to use 
traditional data structures and memory operations. 
6.1.8.1 
Geometry Shaders 
DirectX 10 introduced a type of shader called a geometry shader. That was like a 
vertex shader and obtained data from the vertex shader and operated on the geom-
etry before sending it to the pixel shader and rasterizer. Geometry shaders work on 
large blocks known as meshes, which are vertices and get used in several ways. 
Vertices can be added or removed from a mesh. The geometry shader offers the 
added capability of reprocessing vertices added or altered by the geometry shaders.

6.1 Application Program Interface
235
That represented an extension to geometry instancing and provides more ﬂexibility 
in terms of manipulating instanced geometry, thus eliminating a “cut and paste” look. 
That can improve things like particle systems and allow moving them from the CPU 
to the GPU. 
In previous graphics controllers and earlier GPUs, the image quality was a function 
of how many triangles could be rendered per second (which in turn was a measure 
of performance). 
Those triangles became smaller as the number used was increased to add more 
detail to an image. Increases in density required additional system resources, such 
as buses, memory, and processor time, and developers started using texture maps to 
simulate geometry and reduce the number of triangles. Clever tricks such as bump 
mapping (discussed in Chapter two) were used to reduce the size of a model while 
giving it realistic (but not accurate) lighting effects. The use of per-pixel lighting with 
uncompressed normal maps was able to do an excellent job of simulating geometry. 
Texture map buffers increased to 8 × 8 k in DirectX 10, which offered the exciting 
potential for using pixel processing to simulate geometry. GPUs, combined with 
uniﬁed shaders and geometry shaders, which gave developers much more ﬂexibility 
in approaching the problem of ﬁne detail in geometry. Mesh shaders took that to an 
even greater level. 
6.1.8.1.1
Mesh Shaders, 2018–2020: DirectX 12 Ultimate, Vulkan 
Extension 
A mesh is a set of, edges, vertices, and faces used in 3D graphics to describe the 
shape and location of an object. In legacy 3D pipelines, processing the geometry data 
of a mesh was sequential before any further reﬁnements could be applied, and that 
sequential, step-wise operation created a performance bottleneck. 
Mesh shaders replaced the legacy pipeline with a new model that brought a 
computer programming model’s power, ﬂexibility, and control to the 3D pipeline. 
The GPU’s geometry pipeline hid the parallel nature of its hardware execution behind 
simpliﬁed programming before the mesh shader. The abstraction only provided 
access to linear shader functions, and therefore a vertex shader function got used 
for each vertex in a serial execution. However, the mesh shader packed adjacent 
vertices to ﬁll a SIMD wave, then executed 32 or 64 vertex shader functions in 
parallel on a single shader core. 
In the fall of 2018, Nvidia introduced task shaders as part of its Turing architecture, 
referred to them as a mesh shader, and introduced the concept of meshlets [46]. Those 
new capabilities were accessible via extensions to OpenGL and Vulkan, and DirectX 
12 Ultimate. 
In late 2019, Microsoft announced it would put out a signiﬁcant upgrade to its 
Windows API DirectX12(D3D12). The company said it would add two new shader 
stages: the Mesh Shader and the Ampliﬁcation Shader. Mesh shaders made the power 
of generalized GPU compute performance available to the 3D pipeline and offered 
a new level of ﬂexibility [47].

236
6
Application Program Interface (API)
Fig. 6.24 Evolution of the GPU pipeline 
The future of the geometry pipeline involves reducing the concept of the linear 
pipeline: 
Mesh shaders enable more detailed and dynamic worlds by facilitating more complex algo-
rithms; this gives developers more ﬂexibility when they are working on geometry—Nvidia 
Mesh shaders expand the capabilities and performance of the geometry pipeline. 
Those incorporate the beneﬁts of vertex and geometry shaders into a single shader 
stage by processing batches of vertices and primitives before the rasterizer. They 
were also capable of amplifying and culling geometry. 
Mesh shaders incorporated most aspects of vertex and geometry shaders into a 
single shader stage. The vertex, hull, domain, and geometry shader stages got replaced 
with ampliﬁcation and mesh shaders [48]. 
Those additions streamlined the rendering pipeline, which in turn improved the 
efﬁciency and ﬂexibility. Mesh and ampliﬁcation shaders replaced much of the legacy 
graphics pipeline, including the input assembler and the vertex, geometry, tessella-
tion, domain, and hull shaders, with more robust and general-purpose capabilities 
[49] (Fig. 6.24). 
The feature set of the mesh shader was a signiﬁcant part of DirectX 12 Ultimate, 
developed to increase the ﬂexibility and performance of the geometry pipeline. Other 
vital components of DirectX 12 Ultimate included ray tracing, variable rate shading, 
and sampler feedback. 
AMD incorporated DirectX 12 Ultimate capabilities into its second-generation 
Radeon DNA (RDNA 2) architecture, and Nvidia’s Turing and subsequent GPU 
architectures also included those. 
In March 2021, Khronos adopted Nvidia’s mesh extension for Vulkan [50]; there 
was a debate involving Khronos in the working group regarding whether to follow 
the DirectX 12 design of mesh shaders, as some developers had suggested alternative 
approaches. 
Mesh shaders simpliﬁed the graphics pipeline by introducing an innovative 
approach to geometry processing and offer developers additional ﬂexibility and 
control (Fig. 6.25).
Graphics processors were initially imagined in a linear pipeline, as illustrated in 
Fig. 6.25, but evolved to incorporate parallel processors known as shading engines. 
Although based on a SIMD design, the shaders’ roles expanded, and they were

6.1 Application Program Interface
237
Fig. 6.25 The traditional pipeline
Fig. 6.26 The three stages of a mesh shader (where the choice of data was left to the developer) 
organized as a linear pipeline rather than a true compute SIMD architecture. Mesh 
shaders, shown in Fig. 6.26, changed all that but introduced certain costs. 
The mesh shader outputs triangles that got passed to the rasterizer; those took the 
form of a set of threads, and it was left to the developer to decide how they would 
work. With a mesh shader, programming was done in a group. Many threads worked 
together in cooperation rather than locally. When that was ﬁnished, a small index 
triangle list was output. Each thread operates individually, and the vertex data remain 
unchanged. 
Both mesh and task shaders follow the programming model of compute shaders, 
using cooperative thread groups to compute their results and taking no inputs other 
than a workgroup index. 
6.1.8.1.2
Meshlets 
GPUs and their associated APIs continued to evolve, and in late 2020, a new general-
purpose computational shader emerged. In the fall of 2018, Nvidia had introduced 
its Turing architecture GPU, which contained task shaders, which Nvidia referred 
to as mesh shaders, and introduced the concept of meshlets. Those new capabilities 
were accessible via extensions to OpenGL and Vulkan, and DirectX 12 Ultimate.

238
6
Application Program Interface (API)
Mesh shaders offered almost inﬁnite detail. Smaller mesh sections, known as 
meshlets, were processed in parallel, which provided a much greater degree of 
ﬂexibility and control. 
It begins with the ampliﬁcation shader (the thread dispenser in Fig. 6.24). 
Ampliﬁcation shaders could determine which meshlets are visible before shading 
(culling). 
As mentioned above, a meshlet is a subset of the main mesh. It is created by 
partitioning the primary geometry of the model. It could include between 32 and 
200 primary vertices, depending on the number of attributes of the vertices. A 
meshlet could also contain as many shared vertices as possible for vertex reuse during 
rendering. That partitioning would be pre-computed and stored with the geometry to 
avoid computation at runtime, unlike the previous input assembler, which attempted 
to identify vertex reuse each time a mesh got drawn. Titles could convert meshlets 
into regular index buffers for vertex shader fallback if a device did not support mesh 
shaders.ccvii 
The Inﬂuence of Instancing 
When drawing many instances of a model, such as leaves or trees, the operation 
would quickly reach a performance bottleneck due to the numerous draw calls. It 
would be much more convenient if data got sent to the GPU only once and used 
to draw multiple objects based on that data with a single drawing call. That could 
be done using instancing. With instanced rendering, a GPU can be given a basic 
sunﬂower model, for example, and then render it 1,000 times to create a ﬁeld of 
sunﬂowers. That concept was developed in OpenGL and migrated to Vulkan using 
mesh shaders. Instancing was initiated in the Windows Graphics Foundation (WGF) 
in 2004. 
There was no concept of instancing in the mesh shader API as in the legacy 
pipeline, and the logic of instancing meshes was left entirely up to the application 
code. 
However, the mesh shader output was limited to 256 vertices and 32 k bytes per 
instance. Models were much larger than that. Meshlets were introduced to solve 
the limited vertices problems. A meshlet represented only a tiny piece of a model. 
As described above, vertices are shared within the triangles as far as possible for 
efﬁciency purposes (in the same way as vertex cache optimization in the old pipeline). 
The basic idea was to take what the input assembler used to do in real-time while 
rendering graphics and move it to the output bake time inside the pipeline. 
Mesh shaders write small indexed primitive lists (meshlets)—smaller sub-meshes 
of an original larger mesh (Fig. 6.27). They are ﬁxed in size, and a single mesh shader 
thread group can output exactly one meshlet. Then it can be used with bounding boxes 
for culling (depending on the instructions in the ampliﬁcation stage), but that may 
impose a considerable burden in terms of upfront calculation.
Mesh shaders offer greater control of the geometry pipeline and parallel process 
smaller batches of primitives (called meshlets). Traditional vertex-to-pixel shader 
pipelines processed geometry whole and in-order using a linear index buffer.

6.1 Application Program Interface
239
Fig. 6.27 Breaking down a model into sections using meshlets
Fig. 6.28 Triangles and 
vertices in a strip 
In DirectX 12 and Vulkan, three new buffers got introduced to replace the classic 
index buffer (Fig. 6.22. The geometry pipeline as of 20): the meshlet description 
buffer, the vertex index buffer, and the primitive index buffer. 
Meshlets Allow More Primitives Than Vertices 
With meshlets, it was possible to achieve higher vertex reuse and hence to save on 
bandwidth. A ratio of twice as many triangles as vertices was ideal, with the average 
ratio being around 1.6x (Fig. 6.28). 
The memory footprint of those three buffers was lower than for a classic index 
buffer (Fig. 6.29).
The more vertices that can be reused, the lower the bandwidth needed. 
Index buffer building issues: 
Pros: 
• Works universally can beat ﬁxed function on high rejection 
• Can save bandwidth, as unused vertex attributes do not get fetched 
Cons: 
• Compute launch overhead (need large jobs) 
• Requires storing of index buffer, refetching of vertices (slower for low culling 
rates) 
• On-chip allocation for workgroups with culled clusters (registers etc.) 
However, with a mesh shader, data processing was required before signiﬁcant 
user-deﬁned culling could be applied. That requires developers to pre-process their 
input geometry.

240
6
Application Program Interface (API)
Fig. 6.29 Triangles and vertices in an array
Fig. 6.30 Injection of the ampliﬁcation shader 
Introducing the Ampliﬁcation Shader 
With the introduction of DirectX 12, Microsoft renamed the task shader introduced 
by AMD as an ampliﬁcation shader. However, as part of Nvidia’s patent applications 
[51] of meshlets, Nvidia used the term task shader (Fig. 6.30). 
An application could use an ampliﬁcation shader on its own or with a mesh shader. 
It was a compute thread group that runs cooperatively over a set of threads working 
together, which had beneﬁts in terms of culling. 
Sampler feedback and mesh shaders were two critical features of DirectX 12 
Ultimate (referred to as feature level 12.2 on the Xbox Series X) that provided greater 
ﬂexibility in how the GPU processed geometry and texture data. That enabled novel 
techniques to be implemented. Mesh shaders do not use any ﬁxed functions and 
involve only compute operations. 
A mesh shader is a compute shader that can rasterize geometry and submit a small, 
indexed triangle list to the rasterization stage for pixel shading. It replaces all the 
geometry stages of the input assembler and the vertex, geometry, domain, and hull 
shaders. 
It represented a signiﬁcant step in the evolution of GPUs. Over time, the 
design of GPUs had been tending away from ﬁxed function shaders towards more 
programmable shaders.

6.1 Application Program Interface
241
Fig. 6.31 Dataﬂow in the mesh shader pipeline 
The ideal is a ‘mesh’ class that can contain an entire 3D object (i.e., hundreds 
of thousands of triangles) in a single vertex buffer and draw it using as few 
DrawPrimitive calls as possible (Fig. 6.31). 
Mesh shaders provide data laid out in ways that make sense to the GPU (regardless 
of whether that involves compressed geometry or hierarchical levels of detail), which 
the shader accesses directly. 
There was also the possibility of custom culling without the additional compute 
geometry pre-passes, which had become common. 
Decisions on the level of detail and tessellation in the ampliﬁcation shader can 
be passed to the mesh shaders. That inﬂuences which meshlets get rendered and 
how many. That was a much better method for implementing geometry-shader-style 
pipelines. 
The barrier to entry for that feature can be pretty high, as the change in geometry 
baking required to create extensive collections of meshlets touches on many aspects 
of the design of a game engine. A substantial amount of geometry processing was 
needed to create meshlets, taking time to tune. 
Sampler feedback provides information about the texture data read by a simple 
operation. It had two mapping formats: 
• MinMip, which records the highest Mip level requested in the sample model and 
• MipRegionUsed, which records the requested Mip level used in the operation. 
It was written in HLSL to provide a feedback resource in a lower resolution than 
the texture and works with a user-speciﬁed tiled granularity. 
Mesh shaders combined the full power of generalized GPU compute operations 
with the geometry pipeline, allowing developers to build more dynamic worlds than 
before without compromising performance. That allowed for advanced culling tech-
niques, a high LOD, and inﬁnitely more procedural topology generation in each 
scene. An impressive demo by Nvidia, known as Asteroids, was published in 2018 
[52]. 
Games also had passes in which geometric complexity can dominate, such as 
visibility buffers, depth pre-passes, or shadow maps.

242
6
Application Program Interface (API)
Fig. 6.32 The iconic 3DMark mesh shader test image 
6.1.8.1.3
Benchmarking 
In early 2021, as part of its 3DMark benchmark, Underwriter Labs (Futuremark) 
created a DirectX 12 Ultimate-based benchmark for mesh shading [53], which 
demonstrated that a game could achieve higher frame rates using a mesh shader 
pipeline [54]. 
The Underwriter Labs’ test ran in two passes. The ﬁrst used a traditional approach 
to geometry culling to provide a performance baseline, while the second used mesh 
shaders to efﬁciently cull hidden meshlets. Although the tests look the same, the 
difference in performance was remarkable (Figs. 6.32 and 6.33).
The average frame rate was calculated for each pass, and the difference between 
the two was expressed as a percentage [55]. 
The benchmark was run using an Nvidia RTX 3080 AIB on an 11th-generation 
Intel i9 processor, and the results were astonishing (Fig. 6.34).
3DMark feature tests focused on speciﬁc techniques, resulting in more synthetic 
than traditional 3DMark benchmarks. In that feature test, the performance difference 
was huge, although the beneﬁts would be somewhat less in a game. 
6.1.8.1.4
Interactive Mode 
The 3DMark mesh shader feature test includes an interactive mode that can help to 
visualize the beneﬁts of using mesh shaders. In that test, a developer can pause and 
jump to various parts of the timeline and change the settings in real-time. Visualizer 
options were used to highlight meshlets or to view the LOD used for each meshlet.

6.1 Application Program Interface
243
Fig. 6.33 A screenshot from the interactive mode, in which the meshlets in the scene were 
visualized. Notice that the foreground occludes the faces behind
Fig. 6.34 An improvement of 730% in the FPS was achieved using mesh shaders
DirectX 12 Ultimate added new features and capabilities to the API, enabling 
game developers to create more realistic graphics while improving performance and 
frame rates. 
6.1.8.1.5
Mesh Demo 
In May 2020, before Microsoft and Sony released their new generation of consoles 
(i.e., PlayStation 5 and Xbox Series X), the game engine developer Epic Games 
demonstrated its new Unreal Engine 5, which developers had used in the consoles. 
The demo contained extraordinary detail at high frame rates.

244
6
Application Program Interface (API)
Fig. 6.35 This entire image was calculated—it is not a photograph or texture map. Reproduced 
with permission from Epic Games 
At the time, Epic said one of its goals for the new engine was to achieve photo-
realism equivalent to movie CG and real life. Epic also wanted it to be accessible to 
development teams of all sizes with productive tools and content libraries. Based on 
the demo, it looked as if they had accomplished that mission. 
To demonstrate the power of their new engine (power, in that case, measured in 
terms of ﬁne detail, made possible by an order of magnitude more polygons than 
pixels), the Unreal team created a nine-minute movie called Lumen in the Land of 
Nanite [56]. Epic had developed that demo for the PlayStation 5. 
The visuals are stunning and created using what Epic called Nanite virtualized 
micro-polygon geometry. The company stated that that new LOD geometry would 
free artists to develop as much polygon detail as the eye could see, which may have 
been an understatement. The use of Nanite’s virtualized geometry meant ﬁlm-quality 
source art, comprising hundreds of millions or billions of polygons, could be imported 
into the Unreal Engine—from ZBrush sculpts to photogrammetry scans and CAD 
data. Nanite geometry, said Epic, was streamed and scaled in real time. There were 
no more polygon count budgets, polygon memory budgets, or draw count budgets. 
There was no need to bake details to normal maps or manually author LODs, and 
there was no loss in quality. 
One of the keys to that approach was dynamic global illumination in the system 
they call Lumen [57] (Fig. 6.35). GI poses the main challenge in real-time graphics, 
as there are just so many things to deal with at once. 
Virtualized geometry allowed for the composition or partition of geometry and 
the creation of new virtual geometric entities. Virtual geometry operations adjusted 
the geometry and let the application of mapping, sub-mapping, and sweeping 
mesh generation schemes. That capability improved the workﬂow efﬁciencies, thus

6.1 Application Program Interface
245
Fig. 6.36 This image is what 20-million triangles look like at 4 k—each color represents a different 
triangle. Reproduced with permission from Epic Games 
allowing an artist to do art rather than IT chores, such as cleaning up a highly detailed 
(high-quality) image to ﬁt within a given geometry budget. 
Epic’s VP of Engineering, Nick Penwarden, said that the demo hit 1440p 30 FPS 
on the PlayStation 5 with dynamic resolution scaling enabled (the sweet spot for 
gaming resolution was between 1080p and 4 k) [58]. 
A million triangles each for 8 k textures meant a billion triangles of source geom-
etry in each frame; that engine can reduce that number to 20 million triangles in a 
lossless fashion (Fig. 6.36). 
With 20 million triangles and a color code, as used by Epic in Fig. 6.36, and 
an 8-bit display (not HDR), gave 16 million color possibilities, meaning that there 
were more triangles in the image than could be displayed with unique colors. It may 
take some time for that to be appreciated. And, to just make it more mind-warping, 
assume we had a 4 k screen, which contained only 8.3 million pixels so that there 
were 2.4 triangles per-pixel. The LOD exceeded the display capability, which meant 
that some weighting had to be applied or a decision made to cull the choices, which 
was part of Epic’s secret sauce. 
And then there are the shadows—and there has been no discussion yet of ray 
tracing. The explanation of how Epic got shadow data out of the billion or so triangles 
it started with involves yet more magic. Epic said that all the lighting was dynamic 
and included multi-bounce illumination with no lightmaps or baking. It was a genuine 
global illumination calculation in real time. 
Epic did not just consider pixels; they also developed a new audio engine. 
They sampled sounds (in the example shown in their video, they use the sound 
of rocks falling) to achieve realistic, immersive sound, which they called sound ﬁeld 
rendering.

246
6
Application Program Interface (API)
Fig. 6.37 A soldier consisting of 30 million triangles, rendered in real time with global illumination. 
Reproduced with permission from Epic Games 
The Unreal Engine also incorporated ﬂuid dynamics and objects for non-playing 
characters (NPC) communication. The Unreal Engine includes the Chaos physics 
system used for the rock dynamics in the demo and the character’s hair and scarf. 
Epic also reworked its animation system so that human and animal bodies 
moved ﬂuidly and looked natural. The added predictive foot placement and motion-
warping to modify the inverse kinematics (IK) and body positions dynamically. They 
combined that with a procedural AI animation to allow the character to react to its 
environment. In the demo, one could see the character reach out to hold the edge of 
a partially open door, and her hand touched the surface in a smooth, ﬂuid way. 
Lumen reacted immediately to light and scene changes—the system rendered 
diffuse interreﬂection with inﬁnite bounces and indirect specular reﬂections in huge, 
detailed environments. The scenes could scale from kilometers to millimeters. That 
allowed artists and designers to create more dynamic scenes using Lumen, said Epic, 
for example, by changing the sun angle to represent the time of day, or turning on a 
ﬂashlight, or blowing a hole in a ceiling; the indirect lighting would adapt accordingly. 
Lumen eliminated the need to wait for lightmap bakes to ﬁnish and author light map 
u, v, textures which meant big-time savings for artists. When an artist moved a light 
into the Unreal Editor, the lighting looked the same as when the game ran on the 
console. 
Epic employed GI very effectively and achieved a kind of HDR effect in dark 
places. A specular effect on surfaces was also achieved, which added to the overall 
realism and believability. They even set up a reactive capability, illustrated where 
bugs reacted to (ran away from) the character’s ﬂashlight. It was clever stuff, and all 
in real time—utterly amazing (Fig. 6.37).

6.2 Conclusion
247
Fig. 6.38 There are extensive models, tremendous depth of ﬁeld, real-time physics, character 
animation, motion blur, and more. Reproduced with permission from Epic Games 
In one scene (Fig. 6.37), the hero comes across a giant soldier statue standing 
guard. Epic said they imported it directly from ZBrush, and it consisted of over 
30 million triangles (in a 2.3-million pixel frame). Then in another chamber, they 
replicated the soldier 100 times, generating more than 16 billion triangles. 
The soldiers had no LOD parameters or normal maps (such as one would use with 
bump mapping). That gave artists almost inﬁnite detail to work with, analogous to 
fractals, where each closer view gives ﬁner detail (Fig. 6.38). 
The numbers involved are staggering and are difﬁcult to grasp. Epic said that in 
the overall model, there were hundreds of billions of triangles. 
6.2 Conclusion 
It’s been a long and exciting journey from simple 2D CAD drawings to amazing 
high-deﬁnition color and ultra-high resolution images that update in milliseconds 
and are physically accurate. Looking at the quality of the scenes that applications 
like Epic’s Lumen in the Land of Nanite could generate in 2021, backs us want to 
ask, Are we there yet? 
Well we are sure somewhere. Somewhere pretty wonderful and amazing and 
in a world where suspension of disbelief is no longer a conscious effort, we are 
never taken out of belief the images are so perfect and ﬂuid. The applications beyond 
entertainment for medicine, geophysics, metallurgy and chemistry are mind-boggling 
and imagination stimulating—what can we do next?

248
6
Application Program Interface (API)
Next? Voxels. In this book, we have spoken exclusively about pixels. Pixels as 
represented in computer graphics and not pure mathematics as Alvy Ray Smith does 
in his book are two-dimensional points in space with color. They are ﬂat and you 
can only see the front of them, not their sides (if they have any), their backs, or their 
insides. And that’s OK because you don’t need to any more than you need to see the 
inside of a person’s face or the engine under the hood of your car. But sometimes 
some people do need to see those things. They have information that can clarify what 
we are looking at. 
Voxels can be thought of as a 3D pixel. If a pixel is a ﬂat 2D, x–y representation, 
then a voxel has six possible surfaces, like a cube. 
So where does the GPU go next? 
References 
1. Goldstein, H. and von Neumann, J. Planning and Coding of Problems for an Electronic 
Computing Instrument—Part Il, Volume Ill (Institute for Advanced Study, Princeton University, 
(1948). https://tinyurl.com/fhd8djbm 
2. Campbell-Kelly, M. From Theory to Practice: The Invention of Programming, In: Jones C.B., 
Lloyd J.L. (eds) Dependable and Historic Computing. Lecture Notes in Computer Science, vol 
6875. Springer, Berlin, (1947–51), Heidelberg. https://doi.org/10.1007/978-3-642-24541-1_4 
3. Bloch, J. A Brief, Opinionated History of the API (Speech). QCon New York, 2018, (August 
8, 2018), https://www.infoq.com/presentations/history-api/ 
4. Wilkes, M.V., Wheeler, D., Gill, S. The Preparation of Programs for an Electronic Digital 
Computer with special reference to the EDSAC and the use of a library of subroutines. X + 
170 S. m. 2 Abb. Cambridge (Mass) 1951. Addison-Wesley Press., https://onlinelibrary.wiley. 
com/doi/abs/10.1002/zamm.19520320215 
5. Campbell-Kelly, M. In Praise of ‘Wilkes, Wheeler, and Gill’, Communications of the ACM, 
September 2011, Vol. 54 No. 9, Pages 25–27, In Praise of ‘Wilkes, Wheeler, and Gill’, Commu-
nications of the ACM, (September 2011), https://cacm.acm.org/magazines/2011/9/122802-in-
praise-of-wilkes-wheeler-and-gill/fulltext 
6. Association for Computing Machinery. https://www.acm.org/ 
7. Peddie, J. The Development of 3D Controllers, The History of Visual Magic in Computers, 
Springer Nature Switzerland AG., 2013, pages 233–249. 
8. OpenGL on Arm Windows, https://devblogs.microsoft.com/directx/announcing-the-opencl-
and-opengl-compatibility-pack-for-windows-10-on-arm/ 
9. Google Angle, https://github.com/google/angle 
10. Microsoft Layering for WSL, https://devblogs.microsoft.com/directx/directx-heart-linux/ 
11. OpenGL on Apple M1, https://unlimited3d.wordpress.com/2020/12/09/opengl-on-apple-
m1/#:~:text=Itpercent20ispercent20OpenGLpercent204.1percent20implemented,platformp 
ercent20alsopercent20onpercent20otherpercent20GPUs.&text=Forpercent20surepercent2 
Cpercent20M1percent20ispercent20a,onlypercent20topercent20lowpercent2Dendpercent20 
GPUs 
12. Vulkan Portability on Apple, https://www.khronos.org/blog/new-release-of-vulkan-sdk 
13. Khronos Layering Blog, https://www.khronos.org/blog/ﬁghting-fragmentation-vulkan-portab 
ility-extension-released-implementations-shipping 
14. Microsoft Adventure, Microsoft, (1981) https://archive.org/details/MicrosoftAdventure_DOS_ 
1981 
15. PageMaker – Complete History of the Aldus PageMaker, https://history-computer.com/sof 
tware/pagemaker-complete-history-of-the-aldus-pagemaker/#page-content

References
249
16. Gibbs, S. From Windows 1 to Windows 10: 29 years of Windows evolution, The Guardian, 
(October 2, 2014), https://www.theguardian.com/technology/2014/oct/02/from-windows-1-to-
windows-10-29-years-of-windows-evolution 
17. Colayco, B. Alex St. John Interview, Firing Squad, (March 7, 2000), https://web.archive.org/ 
web/20130421142140/http://www.ﬁringsquad.com/features/alexstjohn/ 
18. Weksler, M. The Wild World of Windows, Computer Gaming World, page 146, (July 1994). 
http://www.cgwmuseum.org/galleries/issues/cgw_120.pdf 
19. Richter, J. 3-D Misconceived: The Richter scale, an occasional column by Jake Richter, The  PC  
Graphics Report, The JPA Newsletter on Desktop Graphics, Volume VII, Number 32, (August 
2, 1994) 
20. Hardware Feature Levels, (May 31, 2018), https://docs.microsoft.com/en-us/windows/win32/ 
direct3d12/hardware-feature-levels 
21. Willetts, S. How DirectX deﬁned PC gaming... with help from a shotgun-toting Bill Gates, 
PCGamer, (July 27, 2020), https://www.pcgamer.com/history-direct-x-windows-microsoft/ 
22. Craddock, D. Alex St John Interview, page 2. Shack News (March 2007), https://www.shackn 
ews.com/article/46338/alex-st-john-interview 
23. Seddon, C. History of OpenGL, OpenGL Game Development. Wordware. p. 43. ISBN 1– 
55622–989–5. (2005) 
24. Segal, M. and Akeley, K. The OpenGL Graphics System: A Speciﬁcation, Silicon Graphics, 
Inc., (1992) 
25. Peddie, J. Who’s the Fairest of Them All? Computer Graphics World. (July 2012). 
26. SGI – OpenGL Overview, (October 31, 2004.), https://web.archive.org/web/20041031094901/ 
http://www.sgi.com/products/software/opengl/overview.html 
27. Peddie, J. Microsoft, and Silicon Graphics form alliance to deﬁne future 3D graphics API 
framework, The Peddie Report, 1538, (December 22, 1997) 
28. Olsen, T., Sellers, G,. Trevett, N., and Kessenich, J. More on Vulkan and SPIR - V: The future 
of high-performance graphics, Khronos Group. p. 10. (August 11, 2016), https://www.create 
ursdemondes.fr/wp-content/uploads/2015/03/Khronos-Vulkan-GDC-Mar15.pdf 
29. Smith, R. Khronos Announces Next Generation OpenGL Initiative, (September 24, 
2016), 
https://www.anandtech.com/show/8363/khronos-announces-next-generation-opengl-
initiative 
30. Batchelor, J. glNext revealed as Vulkan graphics API, Develop (Now part of MCV, mcvuk.com) 
(March 3, 2015) 
31. DXVK (DirectX-over-Vulkan) - stability improvements & potential performance boost: 
installation guide & details, https://steamcommunity.com/sharedﬁles/ﬁledetails/?id=246101 
9058 
32. DirectX 12, https://devblogs.microsoft.com/directx/directx-12/ 
33. Announcing DirectX 12 Ultimate, https://devblogs.microsoft.com/directx/announcing-directx-
12-ultimate/ 
34. van Rhyn, J. Variable Rate Shading: a scalpel in a world of sledgehammers, (March 18th, 
2019), https://devblogs.microsoft.com/directx/variable-rate-shading-a-scalpel-in-a-world-of-
sledgehammers/ 
35. VRWorks - Variable Rate Shading (VRS), https://developer.nvidia.com/vrworks/graphics/var 
iablerateshading 
36. Ritscher, W. HLSL and Pixel Shaders for XAML Developers, O’Reilly Media, (2012). 
37. Cook, Robert L.; Carpenter, Loren; Catmull, Edwin, The Reyes Image Rendering Architecture, 
Computer Graphics, V21m num 4, (July 1987), https://graphics.pixar.com/library/Reyes/paper. 
pdf 
38. OpenGL 2.1 (https://www.khronos.org/registry/OpenGL-Refpages/gl2.1/). 
39. Direct3D 10 Shader Model 4.0, Microsoft, (May 31, 2018), https://docs.microsoft.com/en-us/ 
windows/win32/direct3dhlsl/dx-graphics-hlsl-sm4 
40. Direct3D 11 Features, https://docs.microsoft.com/en-us/windows/win32/direct3d11/direct3d-
11-features?redirectedfrom=MSDN#Full

250
6
Application Program Interface (API)
41. Rocco, D. GPU Tessellation, Slide presentation, CIS 665: GPU Programming and Architecture 
University of Pennsylvania, (Spring 2010), https://studylib.net/doc/5706085/gpu-tessellation 
42. Primitive shader, AMD,  https://patents.google.com/patent/US20180082399A1/en 
43. Burke S. Primitive Discarding in Vega: Mike Mantor Interview, (August 05, 2017), https:// 
www.gamersnexus.net/guides/3010-primitive-discarding-in-vega-with-mike-mantor 
44. Mantor, M. E-mail interview. Conducted by Jon Peddie, (12 February 13, 2021) 
45. Torres, G. DirectX Versions, Hardware Secrets, (July 2, 2008), https://hardwaresecrets.com/dir 
ectx-versions/ 
46. Kubisch C. Introduction to Turing Mesh Shaders, (September 17, 2018), https://developer.nvi 
dia.com/blog/introduction-turing-mesh-shaders/ 
47. Reinventing the geometry pipeline: Mesh Shaders in DirectX 12, https://www.youtube.com/ 
watch?v=vV7ebQ3IfcM 
48. DirectX-Specs, Mesh Shader, https://microsoft.github.io/DirectX-Specs/d3d/MeshShader. 
html 
49. Coming to DirectX 12— Mesh Shaders and Ampliﬁcation Shaders: Reinventing the Geometry 
Pipeline, Microsoft, (November 18, 2019), https://tinyurl.com/22jbe2el 
50. Vulkan 1.2.173 - A Speciﬁcation (with all registered Vulkan extensions), (March 21, 
2021), https://www.khronos.org/registry/vulkan/specs/1.2-extensions/html/chap50.html#VK_ 
NV_mesh_shader 
51. Nvidia patent 10,600,229, March 20, 2020, Techniques for representing and processing 
geometry within a graphics processing pipeline. 
52. Kraemer, M. Using Turing Mesh Shaders: Nvidia Asteroids Demo, (December 18, 2018), 
https://developer.nvidia.com/blog/using-turing-mesh-shaders-nvidia-asteroids-demo/ 
53. Underwriters Laboratories 3DMark benchmark, https://benchmarks.ul.com/3dmark 
54. New 3DMark test measures mesh shader performance, (2021), https://benchmarks.ul.com/ 
news/new-3dmark-test-measures-mesh-shader-performance 
55. UL Mesh test animation, https://benchmarks.ul.com/hwc/tmp/3dmark-mesh-shader-feature-
test-interactive-mode-visualizer-1024x512.gif 
56. A ﬁrst look at Unreal Engine 5, (May 18, 2020), https://youtu.be/EsFEPFGX8Jo?t=1 
57. Lumen in the Land of Nanite, Epic, (June 15, 2020), https://www.unrealengine.com/en-US/ 
blog/a-ﬁrst-look-at-unreal-engine-5 
58. Strickland, D. PlayStation 5 Unreal Engine 5 demo ran at 1440p 30FPS with dynamic res, 
May 13 2020), https://www.tweaktown.com/news/72510/playstation-5-unreal-engine-demo-
ran-at-1440p-30fps-with-dynamic-res/index.html

Chapter 7 
The GPU Environment—Software 
Extensions and Custom Features 
Are AMD, Intel, and Nvidia software or hardware companies? Are they software 
companies that sell hardware? Jensen Huang, President, and Founder of Nvidia has 
often proudly said the company has more software engineers than hardware [1]. 
AMD is no different, and Intel has long maintained groups within the company to 
build, maintain and even and market software. 
7.1 
Software Libraries and Tools 
Software applets, small programs that work with other programs, tools (like compilers 
and libraries), and drivers are produced and updated all the time by the GPU semicon-
ductor hardware vendors. Once just part of the package, necessary components for the 
operation of a product, the software delivered with hardware products soon became 
a chance for differentiation in a rapidly commoditizing segments and marketing 
opportunities for companies giving them a chance to carve out areas of expertise. 
Most of the software a GPU company makes is for developers. But the GPU 
suppliers also make user interfaces that provide users with information about the 
GPU or AIB, and offer adjustment capabilities for things such as resolution, color, 
brightness, and other tuning features such as clock speed. 
This ﬁnal chapter will look at some of the software provided by the GPU suppliers. 
AMD and Nvidia, and to a growing extent Intel, were constantly looking for ways 
to improve image quality (IQ), performance (FPS), and performance per watt (PPW). 
Each time they had developed a new approach, they showed it to Microsoft and 
Khronos. Depending upon their roadmap and schedule, the latter two organizations 
would incorporate the new concepts. The GPU suppliers added it to the API as an 
extension whenever the schedule was too far ahead, or the API organization had not 
shown very much enthusiasm for a new feature. The Vulcan API was intrinsically 
extensional, whereas DirectX was less so (although it was possible). 
In the following sections, we examine some of those extensions.
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2022 
J. Peddie, The History of the GPU – Eras and Environment, 
https://doi.org/10.1007/978-3-031-13581-1_7 
251

252
7
The GPU Environment—Software Extensions and Custom Features
7.1.1 
Ambient Occlusion 
To create realistic shadowing around objects, developers use an effect called ambient 
occlusion (AO). Unlike traditional shadowing, ambient occlusion could manage the 
occlusion of light, creating non-uniform shadows that add depth to the scene; it 
provides an approximation of how bright light should be at different parts of a visible 
surface. One of the techniques used in games to render AO effects is screen space 
ambient occlusion (SSAO). There were several alternatives to ambient occlusion, 
although all of them were based on early developments of the technique. However, 
they had limited shadow deﬁnition and quality, resulting in a limited increase in IQ 
compared to the same scene without ambient occlusion. 
Screen space ambient occlusion is the oldest technique. It created shadows around 
the edges of objects, sometimes incorrectly (i.e., around edges of objects that were 
not close to each other). It was less accurate, but it improved image quality with a 
lower performance requirement. 
At the 2008 SIGGRAPH conference, Nvidia introduced an enhanced version of 
SSAO called horizon-based ambient occlusion (HBAO). Unlike screen space ambient 
occlusion, HBAO used a physically based algorithm to address the grain and noise 
that came with pixel depth measurements of SSAO. It did so by considering the 
ambient light and the environment instead of just the pixels. The HBAO algorithm 
could generate a higher quality image with shadows while increasing the number 
of samples per-pixel. That improved the deﬁnition, quality, and visibility of the 
shadowing, but took more processing. The HBAO algorithm worked with DX11 
GPUs, and Nvidia offered its HBAO library with binaries for Windows, Linux, and 
OSX and the source code.i. 
Horizon-based ambient Occlusion resulted in less incorrect shadowing than 
SSAO, but it was sometimes too pronounced especially around grass, leaves, and 
ﬂowers. The impact on the framerate was virtually identical to SSAO on AMD 
AIBs, and even very slightly faster than SSAO on Nvidia AIBs. 
High-deﬁnition ambient occlusion (HDAO) was introduced with DirectX 11. 
HBAO and HDAO were essentially the same things, but with different vendor imple-
mentations. A user or developer would choose HDAO if they had an ATI/AMD AIB 
or HBAO if they had an Nvidia AIB. However, there were some differences in how 
they worked, looked, and impacted the framerate. 
High deﬁnition was more subtle than SSAO and horizon-based ambient occlusion 
and was probably the most accurate, as there was much less incorrect darkening. 
The choice of high-deﬁnition ambient occlusion over SSAO or HBAO resulted in 
a slight reduction in framerate on AMD AIBs, and a more signiﬁcant one on Nvidia 
AIBs. 
Which one looked the best? The answer is subjective. The high-deﬁnition version 
was probably the most realistic looking, but some people may prefer screen space 
ambient occlusion because it produced an image with higher contrast, which may be 
more dramatic or pleasing to the eye.

7.1 Software Libraries and Tools
253
7.1.2 
Nvidia’s DLSS (February 2019) 
Reconstructing Higher Resolution Images Through AI 
The basic concept of DLSS—Deep Learning Super Sampling—was exactly what its 
name implied. Nvidia trained a deep learning AI model with high-quality images on 
a supercomputer, and then provided that model with their GPU driver. The model 
was used during gameplay to create a higher resolution image, using computations 
based on lower resolution game renders along with the power of the AI (as described 
in more detail below). Lowering the compute requirements in the GPU allowed for 
a higher frame rate, which in some cases was over twice as high. 
Other image scaling techniques offered similar speedup but reduced the image 
quality relative to the processing at full resolution. Deep Learning Super Sampling 
provided a way of reconstructing those images. It used an AI model and the tensor 
cores in RTX GPUs and maintained almost native resolution image quality—in some 
cases, it could remove artifacts that appeared in the native resolution output, to give 
even better image quality. 
Nvidia brought real-time ray tracing to games with their Turing architecture 
(GeForce RTX 20 series) in the fall of 2018. The company considered ray tracing 
to be such an important differentiation for them that they would brand products that 
could accelerate ray tracing in hardware with RTX. 
Ray tracing was more computationally demanding than traditional rendering tech-
niques, and gamers were forced to choose between the improved visual ﬁdelity and 
realism of ray tracing or reaching the maximum frame rate (without ray tracing). 
All rendering was subject to the rules of performance versus quality, with quality 
being measured on an arbitrary scale and performance typically in frames per second. 
Nvidia developed DLSS to address that trade-off, and to allow games to use ray 
tracing while improving frame rates. It turned out that DLSS enhanced the frame 
rate for supported games, regardless of whether ray tracing was enabled, without loss 
of quality. 
For example, when one plays a game using a 4 k monitor, a lot of pixels need to be 
processed in each frame—over 8 million of them, in fact. Using DLSS, Nvidia ran 
the game at 1080p and fed the output to a neural network, as illustrated in Fig. 7.1. 
Motion vectors were obtained from the game engine and sent to the network, to 
give the neural network more information on where the images were heading. The 
network processed the frame and then sent a native resolution 4 k output to the 
monitor, creating 6 million new, accurate pixels on the ﬂy. The gamer therefore 
reaped the beneﬁts of 4 k gaming at a much higher frame rate than without DLSS.
DLSS achieved its image quality by using four inputs to create the ﬁnal frame 
seen by the user:
• The game engine rendered the image in its base resolution (e.g., 1080p) (shown 
on the left in Fig. 7.1).
• Motion vectors from the same image generated by the game engine were extracted 
(shown at the center left in Fig. 7.59). Those motion vectors informed the DLSS

254
7
The GPU Environment—Software Extensions and Custom Features
Fig. 7.1 Data ﬂow for Nvidia’s DLSS 2.0 process
algorithm of the directions in which the objects in the scene would move between 
frames, and those data were later used to direct the supersampling algorithm.
• The high-resolution output of the previous DLSS-enhanced frame (4 k) was used.
• Nvidia used an extensive data set of 16 k-resolution ground truth images, acquired 
from different game content, to train the AI network running on an Nvidia 
supercomputer (although the games did not use those 16 k images at runtime). 
A convolutional autoencoder AI network received the current 1080p base reso-
lution frame. The motion vectors and a previous high-resolution frame were used 
to determine what was needed to generate a higher resolution version of the current 
frame, on a pixel-by-pixel basis. 
By examining the motion vectors and the prior high-resolution frame, the DLSS 
algorithm was able to track objects from one frame to the next. That information 
provided motion stability, and reduced ﬂickering, popping, and scintillation artifacts. 
That process was known as temporal feedback and relied on historical information 
from the images to let the algorithm know what to expect. 
The DLSS algorithm could track each pixel through access to prior frames and 
motion vectors. The algorithm used multiple samples of the same pixel across frames 
(known as temporal supersampling), an approach that could deliver greater detail and 
edge quality than traditional upscaling solutions. 
Ofﬂine, during the training process, Nvidia compared the output 4 k super-
resolution image (from the network) to an ultra-high quality 16 k reference image, 
referred to as the ground truth. The difference between them was sent back into 
the network, allowing it to continue to learn and improve its results. The reference 
images were drawn from different types of game content (with and without ray 
tracing), which Nvidia rendered to 16 k before comparing them to the output of the 
DLSS algorithm. That process formed an iterative learning cycle and ran until the 
network could consistently reproduce a similar image. The algorithm repeated the 
iteration tens of thousands of times on the supercomputer until the reliability of the 
network was high enough to output high-quality, high-resolution images. 
The diagram in Fig. 7.59 shows both the active component (at runtime) and the 
ofﬂine training.

7.1 Software Libraries and Tools
255
The DLSS algorithm learned to predict high-resolution frames with greater accu-
racy through that training process based on a large data set of 16 k-resolution images. 
Nvidia used its supercomputers for continued training, allowing DLSS to learn how 
to deal with new classes of content—such as ﬁre, smoke, and particle effects—at a 
rate that could not be matched by engineers doing hand-coding of non-AI algorithms. 
Nvidia’s GPUs could provide up to 285 TFLOPS, enabling DLSS to run in real time 
on an intensive 3D game. 
DLSS successfully exploited the tensor cores in GeForce RTX AIBs, which could 
deliver up to 285 TFLOPS of dedicated AI processing. As a result, DLSS could be 
run in real time with an intensive 3D game. It was compatible with the Vulkan API, 
and was also on Proton, thus enabling Linux gamers to use the Tensor Cores of their 
GeForce RTX GPUs to accelerate frame rates in games. 
In 2021, Nvidia revamped the DLSS system from its original version (1.0) to a 
new and improved version (2.0). The two versions had little in common, and it would 
be a mistake to compare them. 
Nvidia’s solution was more of an image reconstruction process than an upscaler. 
Edge ﬁltering and upscaling using tweener techniques was an effective process but 
came at the cost of image quality. The intermediate pixels created in that way were 
approximations and best guess values. DLSS was accurate all the way and used root 
data and AI to derive the expanded image. 
The advantages of DLSS were its speed and ultimate frame rate speedup, 
combined with a very high-resolution image. The main drawback was that it only 
ran on Nvidia’s RTX series AIBs or notebook PCs (of course, Nvidia did not see that 
as a problem). 
In July 2021, Nvidia demonstrated DLSS running on a notebook version of its 
RTX 3060 GPU and an ARM CPU running Arch Linux, a Linux distribution created 
for computers with ×86–64 processors. 
Nvidia also showed how RTX could enhance Amazon’s Bistro demo (Fig. 7.2), 
showing a detailed, ray-traced urban scene in France while running on an ARM-
based system. Bistro used Amazon’s Lumberyard game engine and ran on Linux 
[2].
Those demos were created by Nvidia using ﬁve of its software development kits 
for implementing RTX technology on ARM and Linux, as follows:
• Deep Learning Super Sampling (DLSS), which used AI to boost frame rates and 
generate sharp images for games.
• RTX Direct Illumination (RTXDI), which let developers add dynamic lighting to 
their gaming environments.
• RTX Global Illumination (RTXGI), which helped to recreate the way light 
bounced around in real-world environments.
• Nvidia Real-time Denoisers (NRD), a denoising library designed to work with 
low ray per-pixel signals.
• RTX Memory Utility (RTXMU), which optimized the way applications used 
graphics memory.

256
7
The GPU Environment—Software Extensions and Custom Features
Fig. 7.2 Amazon’s Bistro demo
GeForce RTX technology also included GPU-accelerated ray tracing, Tensor 
Cores to accelerate machine learning, and Nvidia DLSS. 
One of computer graphics ﬁrst usages of machine learning was neural denoising 
(now part of Nvidia’s OptiX ray tracing software). Anton Kaplanyan was a Research 
Scientist at Nvidia from 2015 to 2017 who was on the team that ﬁrst demon-
strated neural denoising for one sample per-pixel ray tracing [3]. That invention 
was one of several foundational technologies that Nvidia researchers built upon in 
their discovery of DLSS, the ﬁrst real-time rendering machine learning product, a 
couple years later. 
He was at Crytek (a game developer whose CryEngine game engines were one 
of the most impressive engines in the early 2010s). 
He left Nvidia and went to Facebook’s artiﬁcial intelligence and rendering research 
group. Then in 2021, he left Facebook to join Intel and help develop their GPU, which 
the company said would have advanced ray tracing capabilities that the company 
called Xe Supersampling (Xe SS) (Fig. 7.3).
At Facebook, he was one of the developers of the Neural Supersampling for 
real-time rendering, which introduced a machine learning approach to convert low-
resolution input images to high-resolution outputs for real-time rendering—the kind 
of thing Nvidia’s DLSS did. The up-sampling process used neural networks, trained 
on statistics from various scenes, to reestablish details and reduce the computational 
requirements of rendering those details in real-time applications [4]. 
In 2020, at Siggraph, Facebook reported they achieved a 16 × supersampling of 
rendered content with high spatial and temporal ﬁdelity, outperforming prior work 
by a large margin [5]. 
Machine learning and light simulation had been at the heart of Anton’s contri-
butions, “I enjoy advancing research in computer graphics with a current focus on 
real-time rendering, including neural rendering, perceptual rendering, shading, and

7.1 Software Libraries and Tools
257
Fig. 7.3 Anton Kaplanyan, 
developer of denoising. 
Source Intel
appearance, as well as differentiable rendering”. His PhD. was on light transport 
simulation. 
7.1.3 
AMD’s Fidelity FX Super Resolution (May 2021) 
AMD announced their FidelityFX technology as an open-source technique and 
image quality toolkit offering eight solutions developers could implement in games 
optimized for AMD RDNA and RDNA 2 architectures. 
AMD’s FidelityFX Super Resolution (FSR) was like and yet different from 
Nvidia’s Deep Learning Super Sampling (DLSS). It was like DLSS in that Fideli-
tyFX super sampling was an image reconstruction technique that made a game (or 
presumably any bit-mapped application) look like it had been rendered at a higher 
resolution. For example, if the GPU rendered the app at 1080p (or less), then the 
AMD’s algorithm ﬁlled in the missing pixels between the low-res image and the 
target resolution, for instance, 1440 or 2160 lines. 
It differed because AMD’s FidelityFX used a hybrid supersampling technique 
that combined linear and non-linear upscaling approaches to create an image that 
preserved as much detail as possible while adding new, made-up parts. 
AMD described the process in a patent, which included the diagram in Fig. 7.4 
and description: 
A processing device is provided, which includes memory and a processor. The processor is 
conﬁgured to receive an input image having a ﬁrst resolution, generate linear down-sampled 
versions of the input image by down-sampling the input image via a linear upscaling network, 
and generate non-linear down-sampled versions of the input image by down-sampling the 
input image via a non-linear upscaling network. The processor is also conﬁgured to convert 
the down-sampled versions of the input image into pixels of an output image having a second 
resolution higher than the ﬁrst resolution and provide the output image for display.
AMD combined linear and non-linear upscaling. It maintained the ﬁdelity of 
major upscaled features, such as large objects easily recognized by the human eye. 
It preserved the more detailed features such as curved lines and surfaces, and items 
not easily perceived in low resolution.

258
7
The GPU Environment—Software Extensions and Custom Features
Fig. 7.4 AMD’s super-resolution gaming patent. Source U.S. Patent and Trademark Ofﬁce
In that patent, Alexander Potapov, Skyler Saleh, Swapnil Sakharshete, and Vineet 
Goel, developers based at AMD’s San Diego ofﬁce, explained that linear techniques 
based on a neural network do not consider non-linear information. That, said the 
developers, “typically results in blurry and corrupted images.” The team also cited 
“deep learning approaches” in the patent, commenting that those result in “lost color 
and lost detail information” because they do not incorporate essential aspects of the 
original image (Fig. 7.5). 
In their technique, two paths were created for the reconstructed image. The original 
low-res image passes through a linear upscaling network, while at the same time, 
a non-linear upscaling network extracted different bits of information using both 
paths. The two images were then combined to create a matrix of pixels, which were 
then expanded on each pixel for a higher resolution image. A few high-pass ﬁlters 
were applied to clean up the image, and the reconstructed image was then delivered 
to the screen.
Fig. 7.5 Alexander Potapov, 
machine learning graphics 
specialist at AMD 

7.1 Software Libraries and Tools
259
Table 7.1 The ﬁve modes of AMD’s FidelityFX FSR 
FPS
49
78
99
124
150 
Mode
Native
Ultra-quality
Quality
Balanced
Performance 
Source AMD 
AMD said their FidelityFX Super Resolution had four quality modes, known as 
Ultra, Quality, Balanced, and Performance. 
The company claimed that the Ultra option could boost frame rates by 50 percent 
or more while retaining as much detail as possible (refer to Table 7.1). As the quality 
was reduced (i.e., moving down the list of options), each subsequent performance 
mode offered a higher frame rate with reduced image quality. 
AMD released their FidelityFX ray tracing software on June 22, 2021. Following 
the company’s tradition, they took an open approach with FSR, and it became another 
of AMD’s GPUOpen technologies that was open source and free for developers to 
use. It was unclear whether that included making the algorithm open source or not, 
but FidelityFX could be used on several generations of GPUs from AMD and Nvidia. 
AMD said that Ryzen APUs were also supported, and so too were AMD’s custom 
APUs in the Sony PS5 and Microsoft’s Xbox series X. 
There was probably no reason why Nvidia’s DLSS could not be adapted to other 
GPUs. However, Nvidia did have dedicated tensor cores, which may have given it a 
performance advantage. 
7.1.4 
Intel’s XeSS (March 2022) 
Intel XeSS, or Xe Super Sampling, is an upscaling feature of Intel Arc Alchemist 
graphics AIBs. Like Nvidia’s DLSS it renders a game at a lower resolution and 
then upscales it using machine learning to improve performance. Figure 7.6 shows 
a comparison of image quality between a 4 K screen, an HD screen, and a XeSS 
scaled screen.
Even though the raw image was at 1080, its image quality was comparable to a 
native 4 K image. Intel also offered ﬁve user selectable scaling modes as shown in 
Table 7.2.
Intel’s deep-learning-based temporally amortized super sampling technique 
replaces the temporal anti-aliasing stage in the rendering pipeline (Table 7.3).
Intel’s XeSS used motion, depth, lighting, and color inputs from a game. Then 
it executed the upscaling operation and stored the results in an internal history log. 
That history log was fed to the next incoming frame and the cycle repeated. At the 
same time, the XeSS processor took the frame the user was seeing and executed a 
temporal anti-aliasing (TAA) function to clean up jagged edges. Intel used XeSS to 
replace the usual postprocessing traditional anti-aliasing feature.

260
7
The GPU Environment—Software Extensions and Custom Features
Fig. 7.6 Comparison of high resolution, XeSS scaled image, and low resolution. Courtesy Intel
Table 7.2 Quality modes 
offered by Intel’s XeSS 
scaling options 
Mode
Resolution scaling factor 
Ultra-performance
2.3x 
Performance
2x 
Balanced
1.7x 
Quality
1.5x 
Ultra-quality
1.3x
Table 7.3 Intel’s XeSS data-ﬂow diagram 
Courtesy Intel

7.2 More Than a Driver
261
The company’s GPUs included Xe Matrix Extension (XMX) cores, which ran the 
AI model to perform the upscaling. They were similar to Nvidia’s Tensor cores on 
the RTX 30-series graphics AIBs. 
However, XeSS could also work without the XMX cores. Graphics AIBs that 
supported DP4a instructions (used for A.I. calculations) also worked. 
XeSS did not require training on individual titles which was criticism of Nvidia’s 
DLSS when ﬁrst released. Nvidia subsequentially built the process to run on a general 
AI model. 
Spatial upscaling means the only data used for upscaling a frame comes from 
the frame itself. Temporal upscaling uses data from the current frame as well as 
previous frames for better results, and it also uses the depth buffer (z-buffer) and 
a motion vector buffer to better track changes between frames. AMD’s FSR 1.0 
didn’t do anti-aliasing, while FSR 2.0, DLSS, and XeSS handle both upscaling and 
anti-aliasing. 
7.2 
More Than a Driver 
Intel, Khronos, and Microsoft expanded the programming environment above the API 
into the compute elements and added new programming techniques and languages. 
The GPU, with its dozens to thousands of FPUs, represented a massive computing 
resource, although that required special handling considerations. 
7.2.1 
SYCL 
One of the most noteworthy and unifying developments was SYCL, developed by 
Khronos (Fig. 7.7).
That was introduced in 2014 as a high-level programming model for OpenCL, 
which was also based on C++. SYCL (pronounced “sickle”) was a royalty-free 
abstraction layer that enabled code for heterogeneous processors to be written across 
various platforms, in which the host and kernel code were contained in the same 
source ﬁle. The code set was formally released in early 2021 [6]. 
Several vendors offer SYCL implementations, including support for diverse accel-
eration API back-ends in addition to OpenCL (which was introduced in 2009 by 
Khronos). 
Developers could program with SYCL at a higher level than in the native acceler-
ation API, but always had access to lower level code through its seamless integration 
with the native acceleration API via the interoperability mode, C/C++ libraries, and 
frameworks such as OpenCV or OpenMP. 
SYCL originally stood for SYstem-level OpenCL (which was a bit of a stretch, but 
made a nice pairing with SPIR). As SYCL was no longer tied exclusively to OpenCL, 
Khronos now downplays the background explanation, as it could be confusing.

262
7
The GPU Environment—Software Extensions and Custom Features
Fig. 7.7 SYCL was a cross-platform and OS model. Source Khronos
7.2.2 
GLSL 
OpenGL Shading Language (GLSL) was a high-level shading language based on C. 
Created by the OpenGL ARB and introduced in 2002, it offered more direct control 
over the graphics pipeline without the need to use assembly or hardware-speciﬁc 
languages. 
GLSL was a shading language with a C-style syntax, in which programs had a 
main function that was invoked for each object. Rather than using parameters for the 
input and returning a value as the output, GLSL used global variables to handle both 
the input and output. 
7.2.3 
HLSL 
Microsoft’s proprietary high-level shader language (HLSL) was a C-like high-level 
shader language that could be used with programmable shaders in DirectX [7]. Intro-
duced in 2000 for DirectX 9, HLSL could be applied to write a vertex shader or pixel 
shader, and to use those shaders in an implementation of the renderer in a Direct3D 
application. HLSL programs come in six forms: pixel shaders (fragment in GLSL), 
vertex shaders, geometry shaders, compute shaders, tessellation shaders (hull and 
domain shaders), and ray tracing shaders. HLSL worked solely on the Windows 
platform.

7.2 More Than a Driver
263
7.2.4 
SPIR-V 
Khronos’ Standard, Portable, Intermediate Representation (SPIR) was introduced 
in 2015, and was an intermediate language for parallel computing and graphics. 
Unlike earlier APIs, shader code in Vulkan must be speciﬁed in a bytecode format, 
as opposed to human-readable syntax like GLSL and HLSL. That byte code format 
was called SPIR-V and was designed to be used with both Vulkan and OpenCL. 
7.2.5 
Textures 
The computer graphics industry has tried to make texture mapping faster and more 
efﬁcient ever since Ed Catmull’s pioneering 1974 thesis [8]. Work on texture mapping 
was done because it was such a powerful tool; it led to Jim Blinn’s famous 1978 paper 
on bump mapping [9] and other developments. 
Texture maps are bit planes that lend themselves to compression. Over the years, 
several clever techniques had been developed to make those compressions both loss-
less and lossy. The main problem had been that those compression tricks were not 
universal—what worked on one GPU, CPU, or API would not work on another. 
That created a dilemma for application developers, who needed to support multiple 
compressors as they could not know what type of decompressor a client machine 
might have. Table 7.4 illustrates this point. 
In 2021, Khronos announced the ratiﬁcation of KTX 2.0, its Esperanto for 
universal texture scheme communication. To create that, Khronos incorporated Basis 
Universal supercompression technology as the KTX container format for reliable,
Table 7.4 GPU-compressed texture format fragmentation 
Source Khronos 

264
7
The GPU Environment—Software Extensions and Custom Features
ubiquitous distribution of GPU textures. With KTX 2.0, Khronos glTF users could use 
Basis’ KTXS compression technology to create glTF 3D assets that were texture-rich 
and compact and could be efﬁciently rendered on diverse platforms. 
Basis Universal was a compression technology developed by Binomial, an image 
and texture compression company, founded in 2016 that produced compact textures 
that could be efﬁciently transcoded to a variety of GPU-compressed texture formats 
at runtime. In addition, Khronos had released the KHR_texture_basisu extension, 
which enables glTF to contain KTX 2.0 textures, resulting in universally distributable 
glTF assets that reduce the download size and use natively supported texture formats 
to reduce the GPU memory size and boost the rendering speed on diverse devices and 
platforms. Lastly, Khronos had released open-source tools and transcoders, together 
with developer and artist guidelines, to enable and encourage widespread usage of 
KTX 2.0 textures throughout the glTF ecosystem, including the three.js, Babylon.js, 
and Gestaltor viewers, which had already integrated support. 
Runtime 3D assets have typically used JPG or PNG compressed images to trans-
port textures with reduced ﬁle sizes. However, those formats cannot be processed 
directly by GPUs, and must be decompressed into full size images in GPU memory, 
consuming precious memory space and bandwidth and resulting in poor rendering 
performance and high-power consumption, which was particularly problematic on 
mobile devices. Although GPU-compressed texture formats could create compact in-
memory textures, with optimized memory access for faster, more efﬁcient rendering, 
it was not practical to use GPU texture formats in widely distributed glTF assets, as 
the highly fragmented GPU texture format landscape would make them unusable on 
many target devices. 
Binomial’s cross-platform, Basis, Universal compression technology (Fig. 7.8) 
solved that problem by deﬁning a universal compressed texture format that could 
be efﬁciently transcoded at runtime into a natively supported GPU format on the 
target device. It provided developers with two options for compression that combine 
selected modes of the Khronos-deﬁned ETC1 and ASTC GPU texture formats with 
RDO encoding and LZ-based supercompression for compact texture ﬁle sizes. The 
ETC1S mode could achieve signiﬁcantly smaller transmission and memory sizes 
than JPEG and PNG textures, whereas the UASTC mode delivered higher quality 
textures than ETC1S, which were particularly suitable for normal maps, while still 
achieving smaller ﬁle sizes and signiﬁcant memory savings (Table 7.5).
“KTX 2.0 super-compressed textures complement glTF’s Draco geometry 
compression to enable beautiful, compact and efﬁcient 3D assets for everyone,” 
said Don McCurdy, Chair of the Khronos 3D Formats working group and a Software 
Engineer at Google. 
Binomial developed the royalty-free Basis Universal technology that made KTX 
2.0 universal GPU-compressed textures possible, illustrated in Fig. 7.9.
To encourage the rapid roll-out of KTX 2.0, Khronos created a set of open-
source KTX tools, which together with tools from the industry enable the creation, 
validation, and inspection of KTX ﬁles. The company had also produced a set of 
open-source optimized transcoders for integration into applications and engines, to 
handle KTX 2.0 textures. In addition, the glTF Working Group had created artist

7.2 More Than a Driver
265
Fig. 7.8 Getting from an image to maps and back again. Source Khronos 
Table 7.5 Comparison of the two modes of Basis Universal, UASTC, and ETC1S 
Use case
Baseline 
Khronos 
compressed 
texture format 
Format 
with 
selected 
modes 
LZ-based 
supercompression 
Super-compressed 
bits or pixel 
(typical rate for 
24-bit textures) 
Compressed 
image 
comparison 
(typical rate 
for 24-bit 
textures) 
High 
quality 
ASTC (8 bpp 
includes alpha) 
UASTC
Optional RDO 
encoding + zstd 
6 bpp
PNG 6 bpp 
Smallest 
size 
ETC1 (4 bpp, 8 
bpp with 
alpha) 
ETC1S
Custom basis LZ
1.0 bpp
JPG 1.5 bpp
Fig. 7.9 KTX was a lightweight container format for consistent, cross-vendor distribution of GPU 
textures and contained all the parameters necessary for efﬁcient texture loading and handling. Source 
Khronos

266
7
The GPU Environment—Software Extensions and Custom Features
and developer guides that provide a step-by-step KTX asset creation workﬂow and 
describe how to effectively use the new KHR_texture_basisu extension. 
7.3 
Summary 
For over two decades, the Khronos group had done the impossible. Beginning 
modestly in 2000 with OpenGL ES, it built an organizational model and philosophy 
that allowed and enabled competitive companies to work together for the common 
good. It now had over 150 members from widely divergent areas and offers 18 stan-
dards that were used in all computer-based industries worldwide. Khronos can be 
credited with unifying the use of graphics in the smartphone industry and empow-
ering its amazing growth by making all apps compatible with all phones. KTX and 
glTF were the latest, but far from the last, interchange software standards that the 
organization has brought to the industry. 
7.4 
Software Development Kits for Developers 
In the quest to design games that ran best on their own platforms, AMD and Nvidia 
offered special software features and hardware accelerators to game developers. 
Developers did not always take advantage of those for the PC, making it a second-
class citizen in the game developer’s world; they targeted consoles ﬁrst, and then 
created ports of the game for the PC. The console was preferred because it was a 
long-term (5–10 years) stable platform, with maximum ROI opportunities. Game 
development was (and still is) very expensive and time-consuming, and as powerful 
as the PC was, it changed every 12–24 months. Developers also believed the console 
market had a larger population of gamers than the PC market. Although it was true 
that console gamers were dedicated to gaming and represented a large population (of 
100 million or more) more people played games on PCs than on consoles—but since 
the PC was not dedicated solely to gaming, they were not considered serious gamers. 
Gaming and gamers, as a matter of fact, is a social media subculture, and a big one. 
The serious PC gamer community, sometimes called enthusiasts, has been estimated 
to be about 20 million-strong. However, the statistics were poor and improperly 
interpreted. 
Developers preferred consoles. AMD made strong arguments in 2013 about the 
ease of doing cross-platform development, with the aim of attracting developers to 
the PC (Fig. 7.10). At the time, AMD was providing the GPUs for all three consoles 
(Microsoft Xbox, Nintendo Wii, and Sony PlayStation). Microsoft and Sony were 
also using an X86 CPU, meaning that all (or almost all) of the development tools used 
for a console could also be used for a PC. AMD argued that instead of doing game 
development in a serial fashion, developers could do it in parallel, and that formed 
the basis for a campaign to convince developers to launch games simultaneously on

7.4 Software Development Kits for Developers
267
Fig. 7.10 AMD tried to convince game developers to develop for both platforms, circa 2013. Source 
AMD 
all platforms. It took a few years before game developers began to do that, but the 
message was loud and clear. 
Specialized programs developed by GPU suppliers formed a signiﬁcant part of 
game developers’ tools, and a battle developed between AMD and Nvidia in terms 
of tool development. 
7.4.1 
Nvidia’s GameWorks (2014-) 
In 2014, Nvidia announced a middleware software suite called GameWorks, which 
provided game developers with graphics libraries and tools to improve the visual 
quality of games run on Nvidia GPUs (Fig. 7.11). The company said that the Game-
Works toolkit would enable developers to add visual effects and physics simulations 
and could save on development time.
The suite included PhysX, Visual FX, and Optix. PhysX was Nvidia’s proprietary 
physics engine whereas VisualFX included Nvidia’s optimized rendering techniques 
and in-game visual effects, such as shadows, anti-aliasing, depth of ﬁeld, global 
illumination, hair simulation, ambient occlusion, lighting, and other effects. 
The solutions provided by VisualFX for rendering and effects included the 
following:
• HBAO+: Enhanced horizon-based ambient occlusion.
• TXAA: Temporal anti-aliasing.
• Soft Shadows: An improvement on Percentage Closer Soft Shadows (PCSS) 
which offered new levels of performance with quality and the ability to render 
cascaded shadow maps.

268
7
The GPU Environment—Software Extensions and Custom Features
Fig. 7.11 Nvidia launched its GameWorks suite in 2014 to persuade developers to take advantage 
of Nvidia’s special features. Source Nvidia
• Depth of Field: A combination of diffusion-based DOF and a ﬁxed-cost, constant-
size bokeh effect.
• FaceWorks: A library for implementing high-quality skin and eye shading.
• WaveWorks: Cinematic-quality ocean simulation for interactive applications.
• HairWorks: Enabled the simulation and rendering of fur, hair, and anything with 
ﬁbers.
• GI Works: Global illumination that dramatically improved the realism of the 
rendered image.
• Turbulence: High-deﬁnition smoke and fog with physical interaction as well as 
supernatural effects. 
Optix was a ray tracing API in which the computations were moved to the GPUs 
through the low-level or high-level APIs introduced with Nvidia’s GPU programming 
environment, CUDA. CUDA was only available for Nvidia’s graphics products. 
Nvidia had several motivations for creating the GameWorks program. For 
example, it would allow the company to widen its scope of reach and speed up 
the adoption of its technologies. Like AMD, Nvidia had grown frustrated by how 
slowly game developers embraced new technology while waiting for an installed 
base of users. Nvidia needed to accelerate the return on its investment, and faster and 
broader adoption would mean that more games could leverage more of Nvidia’s tech-
nology. That could be translated into the introduction of more games that exploited 
Nvidia’s features and functions. 
The software development kits (SDKs) were partially open source, and the suite 
contained sample code for DirectX and OpenGL developers, as well as tools for 
debugging, proﬁling, optimization, and Android development.

7.4 Software Development Kits for Developers
269
However, GameWorks was proprietary, and Nvidia did not allow developers to 
share the code with AMD or Intel to optimize its drivers. That meant that for some 
games, players using AMD or Intel hardware could see a reduction in performance. 
Responding to criticisms about the closed nature of GameWorks, Nvidia’s PR 
Gaming Technology and GameWorks spokesperson Brian Burke said, “GameWorks 
licenses follow standard industry practice. GameWorks source code was provided to 
developers that request it under license, but they cannot redistribute our source code 
to anyone who does not have a license” [10]. 
Nvidia continued to add to their GameWorks suite, and introduced Percentage 
Closer Soft Shadows (PCSS), which was developed by Michael Schwärzler and 
colleagues [11]. Those new and improved shadows more accurately reﬂected the 
appearance of shadows, which softened as the distance from the shadow caster 
increases. For example, the shadows of leaves at the top of a 20-foot tree are not 
clearly seen on the ground, with sharp outlines, as illustrated in Fig. 7.12. 
When PCSS was enabled, shadows with variable sharpness and softness were 
created, and their clarity and visibility will also be varied, thus creating a more 
realistic environment. Together with Nvidia HBAO+ and Nvidia Depth of Field, 
PCSS further improved the image quality and immersion of popular games. 
Nvidia’s Founder and CEO, Jensen Huang was asked why his company develops 
so much software and yet doesn’t get paid for it. 
“I have never, not even one time, thought that we weren’t being paid for software,” 
he said. “Nvidia has always been paid for software. It just so happens that you got it 
in the device, and that is no different than an Apple iPhone, and look at the richness 
of the software that comes in that device” [12].
Fig. 7.12 Percentage closer soft shadows softens shadows, making them more realistic. Source 
Nvidia 

270
7
The GPU Environment—Software Extensions and Custom Features
7.4.2 
AMD’s FX Library (2014) 
ATI, and then the AMD graphics group, had developed libraries and special effects 
programs over the years. AMD referred to them as their FX library and gave them 
to any developer who wanted to use them. Like Nvidia, AMD would even offer to 
send a programmer to the developer to show them how to use their tools. There were 
places in numerous games where AMD and Nvidia programmers had written the 
code for special effects, and that practice continues today. 
The combined ATI/AMD, although rich with technology, experienced economic 
difﬁculties after 2005, immediately after the acquisition. By 2015, the company’s 
revenue was at a low not seen since 2001. The company was laying people off, 
dropping some products, and slowing the introduction of others; at the same time, it 
was carrying enormous debt and was selling assets as quickly as it could ﬁnd buyers. 
Between 2007 and 2014, the company had four CEOs. Under those circumstances, 
the company did not have extra resources to invest in items that did not offer a quick 
return on investment (ROI), and that included software development tools for game 
developers (Fig. 7.13). 
When Nvidia announced GameWorks, AMD saw that as an aggressive move. 
“Nvidia’s GameWorks represents a clear and present threat to AMD gamers by 
deliberately crippling performance on AMD products,” said Richard Huddy, AMD’s 
gaming scientist (Fig. 7.14).
GameWorks was supplied in the form of DLLs, which were bits of code that 
had already been complied and cannot be changed. A game developer typically 
wants source code, not DLLs. Huddy’s argument was that the provision of DLLs 
gave Nvidia the ability to write code that could sense the presence of an AMD 
AIB and cause it to execute features (in Nvidia’s DLL) more slowly than on an 
Nvidia AIB. Furthermore, AMD asserted that Nvidia prohibited game developers 
from optimizing Nvidia’s DLLs on AMD hardware. AMD and Nvidia would sign
Fig. 7.13 Comparison of AMD’s and Nvidia’s revenue and graphics market share over time 

7.4 Software Development Kits for Developers
271
Fig. 7.14 Richard Huddy being interviewed about GameWorks. Source PC perspective
marketing agreements with game developers to get a game to play best on their 
hardware. The result of those deals was that the game could not be optimized on the 
competitor’s hardware. 
In 2014, Huddy proposed giving AMD’s FX library to Khronos, making it an 
OpenFX library, and encouraging other HW vendors to do the same thing. That was 
a noble idea, and Kronos might had been receptive, but it was not taken forward. 
Khronos was never approached, and the decision was taken by AMD to move ahead 
quickly with OpenGPU. There were several proponents of OpenGPU within AMD, 
including Huddy and Raja Koduri. 
Khronos was unlikely to have picked up AMD’s offer, simply because it was 
at a higher level than Khronos (which focused mostly on lower level APIs) would 
have wanted. There may also have been some straightforward politics involved in 
the decision (with no reasonable expectation of any support from Nvidia). OpenGPU 
was therefore chosen instead. 
7.4.3 
AMD’s GPUOpen (2015–) 
In mid-December of 2015, AMD announced its GPUOpen, which offered developers 
open-source tools, graphics effects, libraries, and SDKs. The signiﬁcant distinction 
between their developer’s toolkit and Nvidia’s offer was that AMD’s suite consisted 
of open-source code rather than precompiled and unmodiﬁable DLLs. Furthermore,

272
7
The GPU Environment—Software Extensions and Custom Features
as pointed out AMD’s Huddy, GPUOpen served the Windows and DirectX ecosystem 
and extended the beneﬁts to Linux. It was all accessible from the GPUOpen portal 
on GitHub. 
AMD did not offer their libraries and code as middleware solutions, meaning that 
they could be integrated directly into games. To do that, developers needed to have 
full access to the source code. 
AMD adopted MIT’s open-source license, which allowed developers to use every-
thing without restriction. It also allowed the assets to be modiﬁed or improved and 
then sold for proﬁt by any developer, which offered an incentive to independent 
and big-name developers, larger studios, and even visual computing companies to 
employ AMD’s programs and libraries. As discussed above in the section on ambient 
occlusion, there are (always) several different ways to solve a problem. In the case of 
computer graphics, those solutions will result in a different look, which will create 
a speciﬁc feeling in the viewer. Any developer using AMD’s libraries and programs 
would therefore be likely produce a distinct look depending on the GPU it ran on, 
and a knowledgeable viewer would recognize those characteristics. That effect could 
be seen in the difference in the look of a Pixar movie versus one from DreamWorks 
or another studio. 
AMD launched the GPUOpen platform with a variety of effects, tools, and SDKs, 
as shown in Table 7.6. 
GPUOpen included a variety of effects such as soft shadows; high-deﬁnition 
ambient occlusion; various techniques for anti-aliasing, hair simulation, global 
illumination, and other effects, shown in Fig. 7.15.
AMD expanded their GPUOpen offering in 2021 with sharpening and super-
resolution upscaling in a suite called FidelityFX, as discussed above.
Table 7.6 AMD’s initial 
GPUOpen resources from 
2015 
Graphics resources for games 
Effects (DirectX 
11) 
Tools
Libraries and 
SDKs 
TessFX 3.0
CodeXL static 
analyzer CLI 
AMD LiquidVR 
SDK 
FireRays SDK 
GeometryFX
CodeXL DirectX 12 
plug-in 
FireRender SDK 
AOFX
Tootle triangle order 
optimization tool 
AMD compute 
tools 
ShadowFX
DirectX 11 
samples 
DirectX 12 
samples 
AMD graphics 
services 

7.4 Software Development Kits for Developers
273
Fig. 7.15 AMD’s end-to-end open-source compute software stack
7.4.4 
Application Enhancement Software 
GPU suppliers learned very early that the programmable capabilities of a GPU could 
be exploited to enhance special features and capabilities with their GPUs by manip-
ulating the elements of a game. If a game had several special effects running in a 
particular scene (for example, explosions, fast movements, numerous characters), 
the GPU’s driver could be programmed to recognize those calls and then reduce the 
resolution so that the frame rate would slow down. Likewise, the driver could use 
the GPU’s drivers to enhance certain scenes. Those types of operations, which had 
been present in drivers since 2000, were game-speciﬁc, and usually became obvious 
when the GPU supplier announced a patch or a driver update. Driver updates to 
ﬁx bugs were common. Driver updates to improve image quality or performance 
evolved. However, gamers looked forward to those updates, as they often brought 
performance improvements, and they soon became a point of pride for GPU suppliers 
rather than shame. 
Third-party software suppliers built similar programs known as game launchers 
or enhancers. One of the common features of those launchers was a scan of the user’s 
PC to identify programs running in the background that could be terminated, to free 
up memory and reduce any interrupts that might affect the performance. The game 
management, launching, and enhancement programs offered by GPU suppliers did 
not disable background programs.

274
7
The GPU Environment—Software Extensions and Custom Features
7.4.5 
AMD’s Gaming Evolved 
ATI Technologies developed the Catalyst Control Center (CCC) to complement their 
Radeon AIBs in 2002. That was a device driver and utility software package, which 
was released with its Radeon 8500 AIB. After AMD had acquired ATI, that utility 
became known as the AMD CCC. 
Rumors had been circulating in the fall of 2013 about Nvidia’s new user interface 
for the optimization of GPU settings, to maximize performance or image quality. 
AMD had similar ideas, but limited resources. 
In October of 2013, AMD introduced Gaming Evolved, a GPU tuning application 
based on software developed by Raptr. Dennis Fong, a celebrity gamer, had founded 
Raptr in 2007, and the company offered an instant messenger service and a social 
networking website for gamers. It also included game/achievement tracking, an in-
game overlay, and game (i.e., GPU) management. That was the game management 
software AMD wanted. 
Dennis “Thresh” Fong (sitting in the driver’s seat in Fig. 7.16) has been called 
the world’s ﬁrst pro gamer. In 1997, he won world champion competitions in Doom 
and Quake, and was probably most famous for winning John Carmack’s Ferrari in a 
Quake tournament (Carmack is leaning on his former car). At one point, Salon called 
him the Michael Jordan of gaming [13].
AMD packaged its Raptr-based Gaming Evolved application as an extra feature 
of the Catalyst. It could detect games, hardware, and settings, and created an 
FPS histogram each time one played. It also offered collaborative ﬁltering to 
determine optimal settings (i.e., crowd sourcing), a feature from Raptr’s website. 
That saved AMD the expense of running centralized testing, a labor-intensive and 
time-consuming task. 
In 2014, AMD introduced its Catalyst Control Center, which formed a component 
of the AMD Catalyst software engine Crimson. 
The Gaming Evolved application included a rewards system with achievements, 
which was a potentially signiﬁcant social aspect. Although such environments 
worked on consoles, PC gamers did not seem attracted to them at the time. The 
uniﬁcation of multiple features across gaming consoles and PC was a great idea, but 
was not widely implemented, due to the reliance on the under-used social aspects 
of the application. In addition, for Raptr to be worthwhile to AMD (in terms of 
collaborative tuning), it needed a large social presence, which it just did not have. 
Then, in late November of 2015, AMD introduced its Radeon Software Crimson 
ReLive Edition. The version numbers were based on shades of red: the ﬁrst edition 
was “Crimson”; the second was the “Crimson ReLive” Edition; and in 2017, AMD 
introduced its Adrenalin edition (as in Adrenalin Rose). That was AMD’s advanced 
graphics software, which could enable high-performance gaming and offer engaging 
VR experiences. Users could create, capture, and share their experiences, and the 
feedback provided was used to adjust the settings to improve performance and 
efﬁciency.

7.4 Software Development Kits for Developers
275
Fig. 7.16 Dennis “Thresh” Fong in the Ferrari he won from John Carmack (right). Source Heresy22, 
CC BY-SA 3.0 Wikipedia
The user interface for Crimson was a signiﬁcant improvement over the old ATI 
CCC. The Radeon Software Crimson UI was easier to navigate and was intuitive, 
which was a welcome advancement for AMD graphics driver suites. 
In 2016, AMD launched its Radeon 400 series of AIBs and, in the process, quietly 
dropped its gaming Evolved application. The application would still work, but AMD 
terminated compatibility testing, installation support, and technical support for the 
application. It was also no longer available through Radeon Software or its installer. 
AMD dropped Raptr from its driver package in September 2016, and by the end 
of September 2017, Raptr ceased to exist. 
But AMD had not ﬁnished trying to get its act together in terms of an end-user 
software suite. The company’s sales and proﬁts were increasing, which allowed it to 
take on more projects (see Fig. 7.13, comparison of AMD’s revenue and graphics 
market share to Nvidia over time). In December 2017, the company announced it 
would be changing the Radeon Crimson Driver to Adrenalin (Fig. 7.17).
AMD’s Adrenalin Software had almost everything that Nvidia’s GeForce Expe-
rience had and came with the AMD drivers.

276
7
The GPU Environment—Software Extensions and Custom Features
Fig. 7.17 UI for AMD’s Adrenalin
7.4.6 
Nvidia’s GeForce Experience 
In late 2013, Nvidia introduced their user application enhancement application, 
GeForce Experience. That application had two features: ShadowPlay, a DVR-like 
recorder that videoed the user’s actions, and an optimizer/tuner for game settings. 
The application inventoried all the games on the user’s machine (well, most games, 
the application concentrated on games supported by Nvidia), and the main screen 
was then displayed (Fig. 7.18).
The main screen showed a list of the user’s games, and when one was selected, a 
screenshot of the game appeared in the window, under the settings list. The user could 
then decide if they wanted the game to be optimized or not; if so, the application 
matched the settings to the AIB in the user’s machine. 
Users have spent hours, both literally and ﬁguratively, trying to ﬁnd the optimum 
settings for a game, and long-time, dedicated gamers know that each game required 
different settings, and could not be guessed even though one game may use the same 
game engine as another game. 
The application also sets up the user’s game machine to stream to the Nvidia 
Shield. In-game settings such as the shadow quality, AA, etc. were optimized. The 
application did not modify settings like the GPU temperature targets, voltage, or 
overclocking of the GPU or hardware, but those functions could be controlled by 
AIB’s partner utilities, such as EVGA’s Precision, MSI’s Afterburner, Asus GPU 
Tweak, and others.

7.4 Software Development Kits for Developers
277
Fig. 7.18 Screenshot of the control panel for Nvidia’s GeForce experience
GeForce Experience also kept the user’s drivers up to date, automatically opti-
mized game settings, and gave the user an easy way to share their greatest gaming 
moments with friends. 
The Freestyle game ﬁlter allowed postprocessing ﬁlters to be changed while in 
the game. It could change the look and mood of a game with tweaks to the color 
or saturation and could apply postprocessing ﬁlters such as HDR. The program also 
offered a broadcast ability, allowing gamers to send videos or real-time action to 
Facebook Live: Watch Video (Fig. 7.19).
Later versions of Nvidia’s GeForce Experience software added an in-game overlay 
sharing, gallery, recording, instant replay, broadcast, keyboard shortcut, notiﬁcations, 
and other options. However, although those options were often helpful for gamers or 
streamers, they used memory, and could hamper performance by constantly running 
in the background. 
Since some users did not want to run the software in the background, Nvidia added 
a disabling feature, which could speed up gameplay for graphics-intensive games. 
Ray Tracing-Like Software for Non-RT Games 
In a Fast-paced Game, Players Won’t Be Able to Tell the Difference 
Nvidia added an option to allow end users to add ray tracing-like qualities to 
their games through the Nvidia game ﬁlters option and Screen Space Ray Traced 
Global Illumination (SSRTGI). SSRTGI, screen space ambient occlusion, and 
dynamic depth of ﬁeld ﬁlters allow users to make games look more cinematic and 
photorealistic.

278
7
The GPU Environment—Software Extensions and Custom Features
Fig. 7.19 UI for Nvidia’s GeForce experience
Nvidia used Pascal Gilcher’s ReShade software to add the ﬁlters into Nvidia’s 
Geforce Experience application launcher for the PC. The ray tracing Reshade ﬁlter 
had been one of Gilcher’s most popular releases, and Nvidia made it accessible to 
its gaming community. Users could add ray tracing-like quality to supported titles 
without waiting for the game developer to add it. 
SSRTGI isn’t the same as traditional ray tracing, which looks for intersections in 
the triangles. SSRTGI renders in the pixel buffer and uses a ray-marching technique. 
It still gives a realistic look by evaluating the distance from the light source and 
looking (for) objects that are a barrier to light (Fig. 7.20).
Most ray tracing experts deﬁne triangle-based intersection as ray tracing. There-
fore, if the image enhancement is done in the pixel shader stage, it is not ray tracing. 
So by that deﬁnition, ray marching is not a ray tracing solution. It is an approximation 
resulting in false lighting, just as scan line or raster imaging is false lighting. The word 
false should not be seen as a criticism; it is merely a deﬁnition or a differentiation. 
In addition to the ray-marching process, Nvidia added other depth-based ﬁlters 
to Ge Force Experience Freestyle. 
SSAO (screen space ambient occlusion) emphasizes the appearance of shadows 
near the intersections of 3D objects, especially within dimly lit/indoor environments. 
Dynamic DOF (depth of ﬁeld) applies Bokeh-style blur based on the proximity 
of objects within the scene, giving a game a more cinematic look and feel.

7.6 Summary
279
Fig. 7.20 Ray marching takes a different approach to the ray-object intersection problem ray 
marching does not calculate an intersection analytically. Instead, it marches a point along the ray 
until it ﬁnds a point that intersects an object
Since the SSRTGI is ray marching and RM does its work in the pixel shader, 
SSRTGI does not use any RTX RT engines? Theoretically, SSRTGI could work on 
any GPU. 
7.5 
Conclusion 
Software is what squeezes the last ounce of performance out of a GPU. Poor GPU 
supporting software has broken companies, and embarrassed others. Getting a driver 
to be efﬁcient, fast, compliant, backward compatible, and stable is a momentously 
difﬁcult problem. 
Getting applications to exploit all the features and capabilities of GPU is a 
non-ending job. The GPU suppliers develop special programs to aid the software 
developers, and to expose the strengths of the GPU. 
So are the GPU hardware supplier companies that offer software, or software 
companies that sell hardware. The answer is yes. 
7.6 
Summary 
Although the principles of the graphics pipeline were well understood having been 
developed from the 1970s, the introduction of the GPU caused people to rethink 
some of foundational ideas. From 1999 to 2020, the linear pipeline evolved and with 
the introduction of mesh shaders, it became a parallel processor compute engine and 
not just a collection of special function shaders.

280
7
The GPU Environment—Software Extensions and Custom Features
This chapter introduces the concept of the eras of the GPU’s evolution and devel-
opment that emerged with the development of expanded and enriched APIs such as 
DirectX, Metal, OpenGL, and Vulkan. 
In general, the APIs have been the followers of the hardware. And in some cases, 
the software’s latency has even caused the GPU suppliers to remove special features 
they developed for lack of support.
• The ﬁrst era—transform and lighting—DirectX 7 (1999) (discussed in Book one, 
Steps to Invention)
• The second era—programmable shaders—DirectX 8 and 9 (2001–2006)
• The third era—the uniﬁed shader—DirectX 10 and 11 (2006–2009)
• The fourth era—compute shaders—DirectX 11 (2009–2015)
• The ﬁfth era—ray tracing and AI—DirectX 12 (2015–2020)
• The sixth era—mesh shaders—DirectX 12 ultimate (2020) (Discussed in Book 
Three, Subsequent GPUs) 
Building a GPU is a complicated and difﬁcult undertaking. As pointed out in 
this second book of the series, there is a large ecosystem surrounding the GPU. 
That places some constraints and neutralizes a lot of the proprietary aspects most 
companies like to employ to lock a user into the supplier’s device. 
However, the ecosystem makes plug and play from machine to machine possible, 
stabilizes the software drivers, and gives the application developers a reliable and 
predictable platform to design to. 
In the next book of the series, New Developments, we will examine the eras of 
the GPU and how the chip developers have inﬂuenced the development of the APIs, 
which expose all the rich new features of the GPU to the application developer. Book 
three also looks at the effort to develop an open GPU, as well as GPUs used game 
consoles, GPU acceleration (GPGPU), autonomous vehicles and mobile devices like 
smartphones. 
I hope you’ve enjoyed this review of the development of the GPU its environment 
and the preview of the eras of the GPU. 
References 
1. Nvidia: A Software Company That Produces Chips? (June 30, 2021), https://www.unhedged. 
com/exchange/5f96fef4b18e6c40f3fa2d1b/ 
2. Bistro Demo - Amazon Lumberyard, (March 2017), https://youtu.be/JXPA0LOf9JA 
3. Chaitanya, R.A.C., and Kaplanyan, A., et al. Interactive Reconstruction of Monte Carlo Image 
Sequences Using a Recurrent Denoising Autoencoder, (August 2, 2017), https://dl.acm.org/ 
doi/10.1145/3072959.3073601 
4. Xiao, L. Introducing neural supersampling for real-time rendering, (July 1, 2020), https://res 
earch.fb.com/blog/2020/07/introducing-neural-supersampling-for-real-time-rendering/ 
5. Xiao, L. Nouri, S, Chapman, M. Fix, A. Lanman, A. and Kaplanyan, A. Neural Supersampling 
for Real-time Rendering, ACM SIGGRAPH, (August 17, 2020), https://research.fb.com/pub 
lications/neural-supersampling-for-real-time-rendering/ 
6. SYCL 2020 is Here!, https://www.khronos.org/sycl/

References
281
7. Programming guides for HLSL, https://docs.microsoft.com/en-us/windows/win32/direct3dh 
lsl/dx-graphics-hlsl-pguide 
8. Catmull, E. A subdivision algorithm for computer display of curved surfaces (PhD thesis). 
University of Utah, (1974), https://ohiostate.pressbooks.pub/app/uploads/sites/45/2017/09/cat 
mull_thesis.pdf 
9. Blinn, J. F. Simulation of Wrinkled Surfaces, Computer Graphics, Vol. 12 (3), pp. 286–292 
SIGGRAPH-ACM (August 1978), https://dl.acm.org/doi/10.1145/800248.507101 
10. Parlock, J. AMD’s angry at Nvidia because of Geralt’s fabulous hair, PC Perspective, (2015), 
https://www.destructoid.com/amds-angry-at-nvidia-because-of-geralts-fabulous-hair/ 
11. Schwärzler, M., Luksch, C., Scherzer, D., and Wimmer, M. Fast Percentage Closer Soft 
Shadows using Temporal Coherence, Proceedings of ACM Symposium on Interactive 3D 
Graphics and Games, (March 2013), https://tinyurl.com/vdutpdy8 
12. Morgan, T.P. Nvidia CEO on Competition, Software, And The Omniverse, (November 
16, 2021), https://www.nextplatform.com/2021/11/16/nvidia-ceo-on-competition-software-
and-the-omniverse/ 
13. Kushner, D. The Michael Jordan of gaming: Dennis “Thresh” Fong leaves the deathmatch 
arena to try his hand at building a business, Salon, (September 5, 2000), https://www.salon. 
com/2000/09/05/thresh/

Appendix A 
Acronyms 
Does anyone really read a glossary? Hopefully yes. They take a lot of time and 
research to write, and can inform, clear up ambiguities, and ever cause some people 
to change their perspective. The trick is to know what to put in and leave out. 
Throughout this book, speciﬁc terms are used that assume the reader understands 
and is familiar with the industry. 
Terminology and conventions change over time. 
Common acronyms used in this book and the computer graphics industry (product 
names are not included). 
;
Virtual super-resolution
CPS
Coarse pixel shading 
AA
Anti-aliasing
DLNN
Deep learning neural network 
AF
Anisotropic ﬁltering
DOF
Depth of ﬁeld 
AP
Application processor
FFP
Fixed function pipeline 
AS
Ampliﬁcation shader
FLOPS
Floating-point operations per second 
ASIC
Application-speciﬁc IC
GS
Geometry shader 
BAR
Base addressable register
GTC
GPU Technology Conference, 
Nvidia 
BIOS
Basic input–output system
HLSL
High-level shading language 
DRAM
Dynamic random-access 
memory 
HS
Hull shader
(continued)
© The Editor(s) (if applicable) and The Author(s), under exclusive license 
to Springer Nature Switzerland AG 2022 
J. Peddie, The History of the GPU – Eras and Environment, 
https://doi.org/10.1007/978-3-031-13581-1 
283

284
Appendix A: Acronyms
(continued)
DRAM
Dynamic RAM
LMS
Lens-matched shading 
DRM
Digital rights management
MP
Multithreaded processor 
DSR
Downsampling resolution
MRS
Multi-resolution shading 
DSR
Dynamic super-resolution
MRT
Multiple render targets 
DVD
Digital Video Disc or Digital 
Versatile Disc 
RPG
Role-playing game 
DX
DirectX
RTRT
Real-time ray tracing 
EDO
Extended data out RAM
SFU
Special function units 
FFP
Floating-point processor
SIMT
Single-instruction multiple-thread 
FIFO
First-in, ﬁrst-out
SP
Streaming processor 
FOV
Field of view
SSAO
Screen Space Ambient Occlusion 
FSR
FidelityFX super-resolution
ACPI
Advanced Conﬁguration and Power 
Interface 
HBM
High bandwidth memory
APU
Audio Processing Unit 
HDCP
High bandwidth digital content 
protection 
DTV
Digital TV 
HDR
High dynamic range
DXTC
DirectX texture compression 
HSR
Hidden surface removal
eDRAM
Embedded DRAM 
HUD
Head up display
EMBM
Environment-mapped bump 
mapping 
IMR
Immediate mode rendering
FSAA
Full-scene (or screen) anti-aliasing 
IQ
Image quality
FSB
Front-side bus 
JPR
Jon Peddie Research
GDI
Graphics drawing interface 
LPDDR
Low-power DDR
HSR
Hart-Scott-Rodino Act 
MAGIC
Multiple Application Graphics 
Integrated Circuits 
HWMC
Hardware Motion Compensation 
mGPU
A GPU used by crypto-miners 
(AKA, CMP) 
ICD
Installable client driver 
MOS
Metal Oxide Semiconductor
SRAM
Static random-access memory 
ms
Milliseconds
USI
Universal Stylus Initiative 
MSAA
Multi-sample anti-aliasing
FET
Field-effect transistors 
NYIT
New York Institute of technology FPM
Fast page mode (DRAM) 
OS
Operating system
MCM
Multi-chip module 
RDNA
Radeon DNA
OTT
Over-the-top bus 
RGB
Red, blue, green
VRR
Variable refresh rate 
RGBA
Red, blue, green, alpha
CTM
Close to (the) metal
(continued)

Appendix A: Acronyms
285
(continued)
RT
Ray tracing
DSBR
Draw-stream binning rasterization 
SAM
Smart access memory
DXVK
DirectX-over-Vulkan 
SBIOS
System BIOS
IK
Inverse kinematics 
SSAA
Supersampling anti-aliasing
IRIS GL
Integrated Raster Imaging System 
Graphical Library 
TBDR
Tile-based deferred rendering
NPC
Non-playing character 
TMU
Texture mapping unit
SPIR
Standard, Portable, Intermediate 
Representation 
u, v
Texture map axis
VPC
View port culling 
UI
User interface
AO
Ambient occlusion 
VBIOS
Video basic input-out system
GLSL
OpenGL Shading Language 
VR
Virtual reality
HBAO
Horizon-Based Ambient Occlusion 
VRAM
Video random access memory
HDAO
High-Deﬁnition Ambient Occlusion 
WGF
Windows Graphics Foundation
IQ
Image quality 
CEM
Cube environment mapping
PCSS
Percentage-Closer Soft Shadows 
IDF
Intel Developer Forum
PPW
Performance per watt 
BVH
Bounded volume hierarchy

Appendix B 
Deﬁnitions 
2D—Two-dimensional, used to refer to “ﬂat” graphics that only have two axes (plural 
of axis), X & Y, along which drawing occurs, such as those used in normal Windows 
applications. Includes drawing functions such as line drawing, BitBLTs, text display, 
polygons, etc. Most common form of computer graphics, since displays are 2D as 
well. 
3D—Three-dimensional, used to refer to the rendering/display of graphics which 
are 3D in nature (i.e., exist along three axes, X, Y, and Z). In existing PC graphics 
systems, these 3D data need to be rendered into a 2D surface, namely the display. 
This is something that graphics chips that offer 3D acceleration specialize in, offering 
features such as 3D lines, texture mapping, perspective correction, alpha blending, 
and color interpolation for smooth shading (used in simulating lighted scenes). 
3D Scene—A 3D scene is composed of interlocking groups of triangles that make 
up all visible surfaces. By performing mathematical operations on the vertices at the 
corners of each triangle, the geometry-processing engine can place, orient, animate, 
color, and light every object and surface that needs to be drawn. Small programs called 
vertex shaders, uploaded to the graphics chip and executed by the vertex-processing 
engine, control the process. 
ASIC—An “Application Speciﬁc Integrated Circuit” is similar to an FPGA, but ﬁxed 
at the factory, and much cheaper to produce in quantity. 
Adaptive Sync—Technology for LCD displays that support a dynamic refresh rate 
aimed at reducing screen tearing. In 2015, VESA announced Adaptive-Sync as an 
ingredient component of the DisplayPort 1.2a speciﬁcation. See FreeSync. 
Adder—A device with two or more inputs, which performs the operation of adding 
the inputs and outputting the result. Traditional use in computing is a binary adder, 
in which the inputs and output are binary numbers. Inputs can range in width from 
one bit to many bits. The output of an adder is typically one bit wider than the largest 
input to account for a possible carry situation.
© The Editor(s) (if applicable) and The Author(s), under exclusive license 
to Springer Nature Switzerland AG 2022 
J. Peddie, The History of the GPU – Eras and Environment, 
https://doi.org/10.1007/978-3-031-13581-1 
287

288
Appendix B: Deﬁnitions
Adobe RGB—Adobe RGB (1998) is a color space, developed by Adobe Systems in 
1998. It has a wider gamut than the sRGB (mainly in the cyan-green range of colors) 
and is widely used in professional printing. 
AGP—Acronym for Accelerated Graphics Port. This is a new bus technology that 
Intel introduced in the mid-1990s to provide faster access to graphics boards, and 
ultimately allow these graphics boards to utilize system memory for storage of addi-
tional off-screen graphics elements. However, while some PCs with AGP support 
(usually in the form of a single slot for a graphics board) have been shipping since late 
in 1997, there’s no commercially available popular software that currently takes any 
real advantage of AGP’s system memory sharing ability. This is probably because so 
few installed systems currently offer AGP support, and because memory prices have 
dropped enough so that graphics board makers can offer huge amounts of memory 
(4 MB, 8 MB, and even 12 MB) on graphics boards at very low prices, eliminating 
the need to use system memory for additional graphics storage. 
AIB (Add-in board)—An add-in board, also known as a card is a board that gets 
plugged into the PC. When an AIB contains a GPU and memory it is known as a 
graphics AIB or graphics card. It plugs into either PCI express or the older bus AGP. 
ALU, Arithmetic Logic Unit—The circuits in a microprocessor where all arithmetic 
and logical instructions are carried out. Distinguished from an Arithmetic Unit by 
the inclusion of logical functions (shift, compare, etc.) as well as arithmetic (add, 
subtract, multiply, etc.) in its repertoire of functions. 
Ambient Occlusion—To create realistic shadowing around objects, developers use 
an effect called Ambient Occlusion (AO); sometimes called “poor man’s ray tracing.” 
AO can account for the occlusion of light, creating non-uniform shadows that add 
depth to the scene. Most commonly, games use Screen Space Ambient Occlusion 
(SSAO) for the rendering of AO effects. There are many variants, though all are 
based on early AO tech, and as such suffer from a lack of shadow deﬁnition and 
quality, resulting in a minimal increase in image quality (IQ) compared to the same 
scene without AO. 
Anaglyph 3D—Unrelated to 3D. This is a method of simulating a depth image on 
a ﬂat 2D display by overlaying colored images representing the view from left and 
right eyes, then ﬁltering the image presented to each eye through an appropriately 
colored lens. 
Anisotropic Filtering (AF)—A method of enhancing the image quality of textures 
on surfaces of computer graphics that are at oblique viewing angles with respect to 
the camera where the projection of the texture (not the polygon or other primitive on 
which it is rendered) appears to be non-orthogonal (thus the origin of the word: “an” 
for not, “iso” for same, and “tropic” from tropism, relating to direction; anisotropic 
ﬁltering does not ﬁlter the same in every direction). 
API—Acronym for Application Programming Interface. A series of functions 
(located in a specialized programming library), which allow an application to perform

Appendix B: Deﬁnitions
289
certain specialized tasks. In computer graphics, APIs are used to expose or access 
graphics hardware functionality in a uniform way (i.e., for a variety of graphics 
hardware devices) so that applications can be written to take advantage of that func-
tionality without needing to completely understand the underlying graphics hard-
ware, while maintaining some level of portability across diverse graphics hardware. 
Examples of these types of APIs include OpenGL, and Microsoft’s Direct3D. An 
API is a software program that interfaces an application (Word, Excel, a game, etc.) 
to the GPU as well as the CPU and operating system of the PC. The API informs the 
application of the resources available to it, which is called exposing the functionality. 
If a GPU or CPU has certain capabilities and the API doesn’t expose them then the 
application will not be able to take advantage of them. The leading graphics APIs 
are DirectX and OpenGL. 
API—Support Rendering and computing APIs supported by the GPU and the driver. 
APU—The AMD Accelerated Processing Unit (APU), formerly known as Fusion, 
is the marketing term for a series of 64-bit microprocessors from Advanced Micro 
Devices (AMD), designed to act as a central processing unit (CPU) and graphics 
accelerator unit (GPU) on a single chip. 
ARIB STD-B67—Hybrid Log-Gamma (HLG) is a high dynamic range (HDR) stan-
dard that was jointly developed by the BBC and NHK. HLG deﬁnes a nonlinear 
transfer function in which the lower half of the signal values use a gamma curve and 
the upper half of the signal values use a logarithmic curve. 
ASP—Average selling price. 
Architecture—The name of the design, the microarchitecture used for the GPU. It, 
too, will be a proper noun such as AMD’s Radeon DNA or Nvidia’s Hopper. 
Aspect ratio—The ratio of length to height of computer and TV screens, video, ﬁlm, 
or still images. Nearly all TV screens are 4:3 aspect ratio. Digital TVs are moving 
to widescreen which is 16:9 aspect ratio. 
Attach Rate—An attach rate (also called an attach ratio) measures how many add-on 
products are sold with each of the basic product or platform and is expressed as a 
percentage. 
AU, Arithmetic Unit—The circuits in a microprocessor where all arithmetic instruc-
tions are carried out. Often found in combination with separate logic and other units, 
controlled by a long, or very long, instruction word. 
Augmented Reality—Augmented Reality (AR) overlays digitally-created content 
into the user’s real-world environment. AR experiences can range from informa-
tional text overlaid on objects or locations to interactive photorealistic virtual objects. 
AR differs from Mixed Reality in that AR objects (e.g., graphics, sounds) are 
superimposed on, and not integrated into, the user’s environment.

290
Appendix B: Deﬁnitions
Backlight—The backlight is the source of light of the LCD display panels. The type 
of backlight determines the image quality and the color space of the display. There 
are various backlights such as CCFL, LED, WLED, RGB-LED, etc. 
BGA—Ball-grid array, a type of surface-mount packaging (a chip carrier) used for 
integrated circuits. 
Bidirectional Reﬂectance Distribution Function (BRDF)—A function of four real 
variables that deﬁnes how light is reﬂected at an opaque surface. It is employed in the 
optics of real-world light, in computer graphics algorithms, and in computer vision 
algorithms. The function takes an incoming light direction, and outgoing direction, 
(taken in a coordinate system where the surface normal lies along the z-axis) and 
returns the ratio of reﬂected radiance exiting to the irradiance incident on the surface 
from direction of the light source. 
Bidirectional Scattering Distribution Function (BSDF)—Introduced in 1980 by 
Bartell, Dereniak, and Wolfe, it is often used to name the general mathematical func-
tion which describes the way in which the light is scattered by a surface. However, in 
practice, this phenomenon is usually split into the reﬂected and transmitted compo-
nents, which are then treated separately as BRDF (bidirectional reﬂectance distribu-
tion function) and BTDF (bidirectional transmittance distribution function). BSDF 
is a superset and the generalization of the BRDF and BTDF. 
Bidirectional scattering-surface reﬂectance distribution function (BSSRDF)— 
Or B surface scattering RDF describes the relation between outgoing radiance and 
the incident ﬂux, including the phenomena like subsurface scattering (SSS). The 
BSSRDF describes how light is transported between any two rays that hit a surface. 
Bidirectional texture functions (BTF)—Bidirectional texture function is a six-
dimensional function depending on planar texture coordinates as well as on view 
and illumination spherical angles. In practice, this function is obtained as a set of 
several thousand color images of material sample taken during different camera and 
light positions.

Appendix B: Deﬁnitions
291
Bilinear Filtering—When a small texture is used as a texture map on a large surface, 
a stretching will occur and large block pixels will appear. Bilinear ﬁltering smoothens 
out this blocky appearance by applying a blur. 
Binary—A counting system in which only two digits exist, “0” and “1.” Also known 
as the base-2 counting system. Each digit represents an additive magnitude of a power 
of 2, based on its position, with the right-most digit representing 2 to the 0th power 
(20), the next digit representing 2 to the 1st power (21), etc. For example, the binary 
number 1001B converts to a decimal or base-10 number as follows: 1*23 + 0*22 + 
0*21 + 1*20 = 8 + 0 + 0 + 1 = 9. The binary system is the basis for all digital 
computing. 
Binary Digits—The numbers “0” and “1” in the binary counting system. Also called 
a bit. 
Binary Notation—In various graphics hardware reference documents, as well as in 
some programming languages, it’s common to see binary numbers (a combination 
of binary digits) listed as the binary digits followed by the letter “B” or “b,” as in the 
example listed under the term “Binary.” 
Binary Units—One or more bits. 
Binning—Binning is a sorting process in which superior-performing chips are sorted 
from speciﬁed and lower-performing chips. It can be used for CPUs, GPUs (graphics 
cards), and RAM. The manufacturing process is never perfect, especially given the 
incredible precision necessary and number of transistors to produce GPUs and other 
semiconductors. Manufacturing high-performance and expensive GPUs results in 
getting some that cannot run at the speciﬁed frequencies. Those parts however may 
be able to run at slower speeds and can be sold as less expensive GPUs. 
Bit—Acronym derived from the term “Binary digIT” (see deﬁnition above). 
Bit-Depth-BPP—See Bits per pixel and BPP. 
Bitmap—A bitmap image is a dot matrix data structure that represents a generally 
rectangular grid of pixels (points of color), viewable via a monitor, paper, or other 
display medium. A bitmap is a way of describing a surface, such as a computer screen 
(display) as having several bits or points that can be individually illuminated, and at 
various levels of intensity. A bit-mapped 4 k monitor would have over 8-million bits 
or pixels. 
Bits Per Channel—See Bits per pixel. 
Bits Per-Pixel—Bits per channel are the number of bits used to represent one of 
the color channels (Red, Green, Blue). The “bit depth” setting when editing images, 
speciﬁes the number of bits used for each color channel—bits per channel (BPC). 
The human eye can only discern about 10 million different colors. An 8-bit neutral 
(single color) gradient can only have 256 different values which is why similar tones 
in an image can cause artifacts. Those artifacts are called posterization. A 16-bit

292
Appendix B: Deﬁnitions
setting (BPC) would result in 48 bits per pixel (BPP). The available number of pixel 
values of that is (248). 
Bilter—A BitBlt process or engine. BitBit is a data operation commonly used in 
computer graphics in which several bitmaps are combined into one using a Boolean 
function. The operation involves at least two bitmaps, one source and destination, 
possibly a third that is often called the “mask” and sometimes a fourth used to create 
a stencil. 
BPP—BPP is an acronym for Bits Per Pixel. The number of bits per pixel deﬁnes 
the depth of the color space usable by a graphics device. The following table shows 
the relationship between BPP and colors: 
BPP
Number of available colors 
1
2 
2
4 
4
16 
8
256 
15
32,768 
16
65,536 
24
16,777,216 
Also see Bits per pixel. 
Brightness—An attribute of visual perception in which a source appears to be radi-
ating or reﬂecting light. In other words, brightness is the perception elicited by the 
luminance of a visual target. It is not necessarily proportional to luminance. This 
is a subjective attribute/property of an object being observed and one of the color 
appearance parameters of color appearance models. Brightness refers to an absolute 
term and should not be confused with Lightness. 
Bump-Mapped—Ump mapping is a technique for creating the appearance of depth 
from a 2D image or texture map. Bump mapping gives the illusion of depth by adding 
surface detail by responding to light direction—it assumes brighter parts are closer 
to the viewer. It was developed by Jim Blinn and is based on Lambertian reﬂectance 
that postulates the apparent brightness of a Lambertian surface to an observer is the 
same regardless of the observer’s angle of view. 
Bus Interface—The connection that attaches the graphics processor to the system 
(typically an expansion slot, such as PCI, AGP, or PCIe). 
Byte (kbyte, Mbyte, Gbyte, Tbyte)—1 Byte  = 8bits (1 byte = 256 discrete values 
(brightness, color, etc.) A collection of 8 bits, accessible as a single unit. As such, a 
byte may represent one of 256 (28) numbers. 
• 1 kilobyte = ~ 1000 bytes (1024 bytes) 
• 1 Megabyte = ~ 1000 kilobytes (1,048,576 bytes)

Appendix B: Deﬁnitions
293
• 1 Gigabyte = ~ 1000 Megabytes 
• 1 Terabyte = ~ 1000 Gigabytes 
CAD—Computer-aided design. 
CAE—Computer-aided engineering. 
CAGR—Compound average growth rate. 
Cache, Cache Memor—Many processor chips depend on external memory to store 
the bulk of their data. Since access to external memory is slow compared to processor 
speeds, a smaller, faster on-chip memory called a cache is used to improve perfor-
mance. Since the cache holds only a small part of the required data, the cache 
controller runs one of a set of algorithms that attempt to ensure that the processor 
has the fastest possible access to the data it needs at any one time. Many processors 
have a hierarchy of progressively larger and slower on-chip caches in an attempt to 
match the speed and data locality requirements of the processor with the external 
DRAM array. These are referred to as level one (L1), level two (L2) etc. 
Calligraphic Display—See Vector scope. 
CFD—Computational ﬂuid dynamics. 
CGI—Computer-generated imagery. 
Chipset—Typically, a pair of chips that manage the data ﬂows and trafﬁc between 
the system memory, CPU, disk drives, keyboard and mouse, and various I/O ports 
(e.g., USB, Ethernet, etc.)—see southbridge and northbridge. 
Chrominance—Chrominance (chroma or C for short) is the signal used in video 
systems to convey the color information of the picture, separately from the accom-
panying luma signal (or Y for short). Chrominance is usually represented as two 
color-difference components: U = B, − Y, (blue − luma) and V = R, − Y, (red − 
luma). Each of these different components may have scale factors and offsets applied 
to it, as speciﬁed by the applicable video standard. 
Complementary Metal–Oxide–Semiconductor (CMOS) Sensor—A CMOS 
sensor is an array of active pixel sensors in complementary metal–oxide–semi-
conductor (CMOS) or N-type metal oxide semiconductor (NMOS, Live MOS) 
technologies. 
Clamp—A clamp is a device that takes an input and produces an output that is 
bounded. A traditional clamp will have two or three different inputs: the signal or 
number to be clamped; the upper bound to clamp to; and possibly a lower bound to 
clamp to. When the signal/numeric input to be clamped is received, it is compared 
against the upper bound, and if it exceeds it, is replaced by the upper bounding value. 
Similarly, if there is a lower bound, the signal/numeric input is compared and if found 
lower than the lower bound, it’s replaced with the lower bound. The result of all the 
bounding is then passed on to the output of the device.

294
Appendix B: Deﬁnitions
Clone Mode—Duplicates the computer’s screen on the other monitor(s), it’s referred 
to as “Duplicate (in multiple displays’ pull-down menu window). It can be useful 
for presentations, and sometimes to provide a different representation of the same 
output. 
Codename—The GPU manufacturer’s engineering codename for the device. 
Color—In current computer graphics systems, color display information is generated 
as a blend of three colored light components: red, green, and blue (RGB). The 
combination of all three of these color components at full intensity produces a white 
output, while the absence of all three produces black output. Blending these three-
color components at different intensities can produce a near inﬁnite number of distinct 
colors. While a display monitor tends to require each color component to have a 
voltage from 0 (off) to 0.7 V (full intensity), a computer graphics subsystem tends to 
deal with color in digital terms, on a pixel per-pixel basis. Each pixel has a speciﬁc 
depth, also known as BPP. Each pixel, in the process of being displayed from video 
memory, passes through a component called a RAMDAC. For 8 BPP or less, the 
pixel value read from video memory is usually passed through the LUT portion of 
a RAMDAC in order to produce the requisite RGB information. For greater than 8 
BPP modes, pixels generally bypass the LUTs and go directly to the DACs. In order 
to do this such pixels must be deﬁned with ﬁxed possible ranges of RGB. Therefore, 
it is standard that 15-bit pixels have 5 bits each of R, G, and B, with one bit left 
unused; 16-bit pixels have 5 bits each of R and B, and 6 bits of G; and 24 and 32-bit 
pixels have 8 bits each of R, G, and B (with 8 bits unused in 32-bit pixels). 5 bits give 
32 distinct intensity levels of a color component, 6 bits give 64 levels, and 8 bits give 
256 intensity levels. It should be noted that pixel modes that go through the LUT are 
called “indexed” color modes, while those that don’t are referred to as “direct color” 
or “true color” modes. 
Color Gamut—The entire range of colors available on a particular device such as 
a monitor or printer. A monitor, which displays RGB signals, typically has a greater 
color gamut than a printer, which uses CMYK inks. Also see Gamut, and wide color 
gamut. 
Color Space—See color gamut and gamut. 
Combine—The verb used to describe an operation in which two or more values or 
signals are added or concatenated with each other in order to produce a combined 
output. 
Comparator—A comparator is a device that generally takes two inputs, compares 
them, and based on the result of the comparison, produces a binary output or signal to 
indicate the result of the comparison. For example, for a “greater-than” comparator, 
the ﬁrst input would be compared against the second input, and if the ﬁrst is larger, 
a TRUE (usually a binary 1) would be output. 
Computational Photography—Processing of still or moving images with the 
objective of modifying, enhancing or manipulating the images themselves.

Appendix B: Deﬁnitions
295
Conformal Rendering—Foveation that offers a smoothly varying transition from 
the high acuity region and the low acuity region. Considered more efﬁcient than 
traditional foveated rendering because it requires fewer rendered pixels than other 
techniques. 
Conservative Raster—When standard rasterization does not compute the desired 
result is shown, where one green and one blue triangle have been rasterized. These 
triangles overlap geometrically, but the standard rasterization process does not detect 
this fact. 
Comparing Standard and conservative rasterization 
With conservative rasterization, the overlap is always properly detected, no matter 
what resolution is used. This property can enable collision detection. 
Constant Dither—A constant dither is the application of a dither value that doesn’t 
change over the course of a set of dithering operations. 
Contrast Ratio—The contrast ratio is a property of a display system, deﬁned as 
the ratio of the luminance of the brightest color (white) to that of the darkest color 
(black) that the system is capable of producing. A high contrast ratio is a desired 
aspect of any display. It has similarities with dynamic range. 
Convolution—Convolution is a mathematical operation on two functions (f and 
g); it produces a third function, that is typically viewed as a modiﬁed version of 
one of the original functions, giving the integral of the pointwise multiplication of 
the two functions as a function of the amount that one of the original functions is 
translated. Convolution is similar to cross-correlation. It has applications that include 
probability, statistics, computer vision, natural language processing, image and signal 
processing, engineering, and differential equations.

296
Appendix B: Deﬁnitions
By Cmglee—Own work, CC BY-SA 3.0, https://commons.wikimedia.org/w/index.php?curid= 
20,206,883 
Core clock—The GPU’s reference or base frequency (and boost if available) is 
expressed in MHz or GHz. 
CNN (Convolutional Neural Network)—A Deep Neural Network (DNN) that has 
the connectivity in one or more of its layers arranged so that each node in Layer N is a 
convolution between a rectangular subset of the nodes in layer N-1 and a convolution 
kernel whose weights are found by training. The arrangement is designed to mimic 
the human visual system and has proven to be very successful at image classiﬁcation 
as long as very large training data sets are available. 
CPU—Acronym for Central Processing Unit. In PC terms, this refers to the 
microprocessor that runs the PC, such as an Intel Pentium chip. 
Crossbar—A crossbar switch, or matrix switch is an assembly of individual switches 
between multiple inputs and multiple outputs that connects the inputs to the outputs 
in a matrix manner. Crossbar switches were developed for information processing 
applications such as telephony and circuit switching. 
CRT—Cathode Ray Tube. Technical name for a display, screen, and/or monitor. 
Most commonly associated with computer displays. 
DAC—Digital to Analog Converter. A DAC is used to translate a digital (integer) 
input, such as a pixel value, into an analog (non-integer) voltage signal. DACs are 
used in CD players to convert CD data into sounds. DACs are also a key component of 
any graphics subsystem, since they convert the pixel values into colors on the screen. 
Graphics boards typically use a device known as a RAMDAC, which combines DACs 
with Look-Up Tables (LUTs). RAMDACs typically contain three LUTs and three 
DACs, one each for the red, green, and blue color components of a pixel. See “Color” 
and “LUT” for more information.

Appendix B: Deﬁnitions
297
DCI P3—DCI P3 is a color space, introduced in 2007 by the SMP T E. It is used in 
digital cinema and has a much wider gamut than the sRGB. 
dGPU—The basic, discrete (stand-alone) processor that always had its own private 
high-speed (GDDR) memory. dGPUs are applied to AIBs and system boards in 
notebooks. 
Desktop GPU Segments—The desktop is segmented into ﬁve categories, and the 
desktop discrete GPUs follow the same designations. 
• Workstation 
• Enthusiast 
• Performance 
• Mainstream 
• Value 
Device Driver—A device driver is a low-level (i.e., close to the hardware) piece 
of software that allows operating systems and/or applications to access hardware 
functionality without actually having to understand exactly how the hardware oper-
ates. Without the appropriate device drivers, one would not be able to install a new 
graphics board, for example, to use with Windows, because Windows wouldn’t know 
how to communicate with the graphics board to make it work. 
De-Warp—In vision systems, this refers to the process of correcting the spherical 
distortion introduced by the optical components of the system. Especially where a 
single camera is capturing a very wide ﬁeld of view, signiﬁcant distortion can be 
present. This is usually, but not always, removed in the ISP before any signiﬁcant 
vision processing or further computational photography is done. 
Die Size—The square area of the chip, typically measured in square millimeters 
(mm2). 
Direct3D—Also known as D3D, Direct3D is the 3D graphics API that’s part of 
Microsoft’s DirectX foundation library for hardware support. Direct3D actually has 
two APIs, one which calls the other (called Direct3D Retained Mode or D3D RM) 
and hides the complexity of the lower level API (called Direct3D Immediate Mode 
or D3D IM). Direct3D is becoming increasingly popular as a method used by games 
and application developers to create 3D graphics, because it provides a reasonable 
level of hardware independence, while still supporting a large variety of 3D graphics 
functionality (see “3D”). 
Display Port—DisplayPort is a VESA digital display interface standard for a digital 
audio/video interconnect, between a computer and its display monitor, or a computer 
and a home-theater system. DisplayPort is designed to replace digital (DVI) and 
analog component video (VGA) connectors in the computer monitors and video 
cards. 
Dithering—Used to hide the banding of colors when rendering with a low number 
of colors (for example 16 bits). Banding is what happens when there are not enough

298
Appendix B: Deﬁnitions
shades of colors, resulting in the eye being able to see a distinct change of colors 
between two shades. Dithering is also a way to visually simulate a larger number 
of colors on a display monitor by interleaving pixels of more limited colors in a 
small grid or matrix pattern, much in the way a magazine’s color pictures are actu-
ally composed of small colored dots. Dithering takes advantage of the human eye’s 
capability to blend regions of color. For example, if you could only display red and 
blue pixels, but wanted to give the visual impression of purple, you would create a 
matrix of interleaved red and blue pixels, as depicted using letters below (B = Blue, 
R = Red): 
BRBRBRBR 
RBRBRBRB 
BRBRBRBR 
RBRBRBRB 
When viewed from a distance, the human eye would blend the red and blue pixels 
in this pattern, making the area appear to be a shade of purple. This technique allows 
one to simulate thousands of color in exchange for a small loss in detail, even when 
there are only 16 or 256 colors available for display as might be the case when a 
graphics subsystem is conﬁgured to display in an indexed color mode (see “Color”). 
DMCVT—Dynamic Metadata for Color Volume Transforms, SMPTE ST 2094. 
Dolby Vision—12-bit HDR, BT.2020, PQ, Dolby Vision dynamic metadata. 
DVI (Digital Visual Interface)—DVI is a VESA (Video Electronics Standards Asso-
ciation) standard interface for a digital display system. DVI sockets are found on the 
back panel of AIBs and some PCs and also on ﬂat panel monitors and TVs, DVD 
players, data projectors and cable TV set-top boxes. DVI was introduced in and uses 
TMDS signaling. DVI supports High bandwidth Digital Content Protection, which 
enforces digital rights management (see HDCP). 
Dynamic Contrast—The dynamic contrast shows the ratio between the brightest 
and the darkest color, which the display can reproduce over time, for example, in the 
course of playing a video. 
EDF—Emissive Distribution Functions. 
eGPU—An AIB with a dGPU located in a stand-alone cabinet (typically called a 
breadbox) and used as an external booster and docking station for a notebook. 
Electronic Imaging—Electronic Imaging is a broad term that deﬁnes a system of 
image capture using a focusing lens sensor with a sensor behind it to translate the 
image into electronic signals. Those signals are then ﬁltered, processed, and made 
available for storage and/or display. A technique for inputting, recording, processing, 
storing, transferring, and using images. (ISO 12651–1). Using computers and/or 
specialized hardware/software to capture (copy), store, process, manipulate, and 
distribute ‘ﬂat information’ such as documents, photographs, paintings, drawings, 
and plans, through digitization.

Appendix B: Deﬁnitions
299
End-to-End Latency—See Motion-to-photon latency. 
Energy Conservation—The concept of energy conservation states that an object 
cannot reﬂect more light than it receives. 
Energy conservation scales 
For practical purpose, more diffuse and rough materials will reﬂect dimmer and 
wider highlights, while smoother and more reﬂective materials will reﬂect brighter 
and tighter highlights. 
Error Correction Model (ECM)—Belongs to a category of multiple time series 
models most commonly used for data where the underlying variables have a long-
run stochastic trend, also known as cointegration. ECMs are a theoretically-driven 
approach useful for estimating both short-term and long-term effects of one-time 
series on another. The term error-correction relates to the fact that last-period devia-
tion from a long-run equilibrium, the error, inﬂuences its short-run dynamics. Thus, 
ECMs directly estimate the speed at which a dependent variable returns to equilibrium 
after a change in other variables. 
Extended Mode—Extended mode creates one virtual display with the resolution of 
all participating monitors. Depending on the hardware and software employed, the 
monitors may have to have the same resolution. (there’s more on this in the next 
sections). Both of these modes present the display space to the user as a contiguous 
area, allowing objects to be moved between, or even straddled across displays as if 
they are one. 
Fab—The fabrication process. The average feature size of the transistors in the GPU 
expressed in nanometers (nm). 
FEA—Finite element analysis. 
Field of View—The ﬁeld of view (also ﬁeld of vision, abbreviated FOV) is the 
extent of the observable world that is seen at any given moment. In case of optical 
instruments or sensors, it is a solid angle through which a sensor detects the presence 
of light.

300
Appendix B: Deﬁnitions
Fill Rate: 
• Pixel The rate at which the raster operators can render pixels to a display, measured 
in Pixels/s. 
• Texture The rate at which the texture mapping units can map surfaces onto a 
polygon mesh, measured in Texels/s. 
Fixed Function—Fixed-function accelerator AIBs take some of the load off the CPU 
by executing speciﬁc graphics functions, such as BitBlt operations and line draws. 
That makes them better than frame buffers for environments that heavily load the 
system CPU, such as Windows. Those types of AIBs have also been called Windows 
and graphical user interface (GUI) accelerators. 
A ﬁxed function can also apply to the graphics pipeline, such as a T&L stage or 
a tessellation stage. 
Flat Shading—A rendering method to determine brightness by the normal vector 
on a polygon and the position of the light source and to shade the entire surface of 
a polygon with the color of the brightness. This rendering method produces a clear 
difference in the colors of adjacent polygons, making their boundary lines visible, 
so it is unsuitable for rendering smooth surfaces. 
Floating-Point Unit—An Arithmetic Unit that operates on ﬂoating-point data. Most 
general-purpose ﬂoating-point units observe the IEEE 754 standard which governs 
formats, precision, rounding, handling of exceptions, etc. Special purpose AUs found 
in GPUs and other DSPs optimized for speciﬁc tasks do not always do so and hence 
different results can be obtained for the same instructions executed on different AUs. 
This is one of the challenges of heterogeneous computing. 
FLOP—An acronym for Floating-point Operations Per Second used as a measure 
of the computational throughput of a ﬂoating-point arithmetic unit. 
FOV, Field of View—The ﬁeld of view (also ﬁeld of vision, abbreviated FOV) is the 
extent of the observable world that is seen at any given moment. In case of optical 
instruments or sensors it is a solid angle through which a sensor detects the presence 
of light. 
Foveated Imaging—A digital image processing technique in which the image reso-
lution, or amount of detail, varies across the image according to one or more “ﬁxation 
points.” A ﬁxation point indicates the highest resolution region of the image and 
corresponds to the center of the eye’s retina, the fovea. 
Foveated Rendering—A graphics rendering technique that uses an eye tracker inte-
grated with a virtual reality headset to reduce the rendering workload by limiting the 
image quality in the peripheral vision (outside of the zone gazed by the fovea). 
FPGA—A Field Programmable Gate Array is a reprogrammable logic gate chip 
whose internal gate connections can be altered by downloading a bitstream to the 
card with a special program written for that purpose.

Appendix B: Deﬁnitions
301
FPU—A ﬂoating-point unit (FPU) is a part of a computer system specially designed 
to carry out operations on ﬂoating-point numbers. Typical operations are addition, 
subtraction, multiplication, division, and square root. FPUs can be found within a 
CPU, in GPU shaders, and in DSPs and stand-alone coprocessors. 
Fragment Shader—Pixel shaders, also known as fragment shaders, compute color 
and other attributes of each fragment. The simplest kinds of pixel shaders output one 
screen pixel as a color value; more complex shaders with multiple inputs/outputs are 
also possible. Pixel shaders range from always outputting the same color, to applying 
a lighting value, to doing bump mapping, shadows, specular highlights, translucency 
and other phenomena. They can alter the depth of the fragment for z-buffering. 
Frame Buffer—The separate and private local memory for a GPU on a graphics 
AIB. The term frame buffer is a bit out of date since the GPU’s local memory holds 
much more than just a frame or an image for the display as they did when originally 
developed. Today the GPU’s local memory holds programs (known as shaders) and 
various textures, as well as partial results from various calculations, and two to three 
sets of images for the display as well as depth information known as a z-buffer. 
Frame Rate Control (FRC)—A method, which allows the pixels to show more color 
tones. With quick cyclic switching between different color tones, an illusion for a 
new intermediate color tone is created. For example, by using FRC, a 6-bit display 
panel can show 16.7 million colors, which are typical for 8-bit display panels, and 
not the standard 262,200 colors, instead. There are different FRC algorithms. 
Frame-Rate Converter (FRC)—Frame rate, also known as frame frequency and 
frames per second (FPS), is the frequency (rate) at which an imaging device produces 
unique consecutive images called frames. FRC (Frame Rate Conversion) algorithms 
are used in compression, video format conversion, quality enhancement, stereo 
vision, etc. FRC algorithm increases the total number of frames in the video sequence. 
This is performed by inserting new frames (interpolated frames) between each pair 
of neighbor frames of original video sequence. 
FreeSync—The brand name for an adaptive synchronization technology for LCD 
displays that support a dynamic refresh rate aimed at reducing screen tearing. 
FreeSync was initially developed by AMD. FreeSync is a hardware/software solution 
that utilizes DisplayPort Adaptive-Sync protocols to enable smooth, tearing-free and 
low-latency gameplay. 
Frustrum, Viewing—A viewing frustum is the 3D volume in a scene relative to 
the viewer. The shape of the volume affects how models are projected from camera 
space onto the screen. The most common type of projection, a perspective projection, 
is responsible for making objects near the camera appear bigger than objects in 
the distance. For perspective viewing, the viewing frustum can be visualized as a 
pyramid, with the camera positioned at the tip. This pyramid is intersected by a front 
and back clipping plane. The volume within the pyramid between the front and back 
clipping planes is the viewing frustum. Objects are visible only when they are in this 
volume.

302
Appendix B: Deﬁnitions
G-Buffer—Tile-Based Deferred Rendering (TBDR). 
Gamma Correction—Gamma correction, gamma nonlinearity, gamma encoding, 
or often simply gamma, is the name of a nonlinear operation used to code and decode 
luminance or tristimulus values in video or still image systems. Gamma correction 
is, in the simplest cases, deﬁned by the following power-law expression: 
Plot of the sRGB standard gamma-expansion nonlinearity (red), and its local gamma value, slope 
in log–log space (blue) 
In most computer systems, images are encoded with a gamma of about 0.45 
and decoded with a gamma of 2.2. The sRGB color space standard used with most 
cameras, PCs, and printers does not use a simple power-law nonlinearity as above, but 
has a decoding gamma value near 2.2 over much of its range. Gamma is sometimes 
confused and/or improperly used as “Gamut.” 
Gamut—In color reproduction, including computer graphics and photography, the 
gamut, or color gamut is a certain complete subset of colors. 
Typical gamut map. The grayed-out horseshoe shape is the entire range of possible 
chromaticities, displayed in the CIE 1931 chromaticity diagram format.

Appendix B: Deﬁnitions
303
The most common usage refers to the subset of colors which can be accurately represented in a 
given circumstance, such as within a given color space or by a certain output device 
Also see Color gamut, and wide color gamut. 
GDDR—An abbreviation for double data rate type six synchronous graphics 
random-access memory, is a modern type of synchronous graphics random-access 
memory (SGRAM) with a high bandwidth (“double data rate”) interface designed 
for use in graphics cards, game consoles, and high-performance computation. 
Geometry Engine—Geometric manipulation of modeling primitives, transforma-
tions, is applied to the vertices of polygons, or other geometric objects used as 
modeling primitives, as part of the ﬁrst stage in a classical geometry-based graphic 
image rendering pipeline, which is referred to as the geometry engine. Geometry 
transformations were originally implemented in software on the CPU or a dedicated 
ﬂoating-point unit, or a DSP. In the early 1980s, a device called the Geometry Engine 
was developed by Jim Clark and Marc Hannah at Stanford University. 
Geometry Shaders—Geometry shaders, introduced in Direct3D 10 and OpenGL 
3.2, generate graphics primitives, such as points, lines, and triangles, from primi-
tives sent to the beginning of the graphics pipeline. Executed after vertex shaders 
geometry shader programs take as input a whole primitive, possibly with adjacency 
information. For example, when operating on triangles, the three vertices are the 
geometry shader’s input. The shader can then emit zero or more primitives, which 
are rasterized and their fragments ultimately passed to a pixel shader.

304
Appendix B: Deﬁnitions
Global Illumination—“Global illumination” (GI) is a term for lighting systems 
that model this effect. Without indirect lighting, scenes can look harsh and artiﬁcial. 
However, while light received directly is fairly simple to compute, indirect lighting 
computations are highly complex and computationally heavy. 
Gouraud Shading—A rendering method to produce color gradual shading over the 
entire surface of a polygon is performed by determining brightness with the normal 
vector at each vertex of a polygon and the position of the light source, and performing 
linear interpolation between vertices. 
The normal vector at each vertex can be determined by taking an average of 
the normal vectors of all the polygons having the common vertex. For a triangular 
polygon, the brightness at each vertex is determined by the normal vector obtained 
for each vertex and the position of the light source. Therefore, the brightness of pixels 
inside a triangle is determined by interpolation. This rendering method represents 
color gradual variations between adjacent polygons, so it is suitable for rendering 
smooth surfaces. 
GPC—A graphics-processing cluster (GPC) is group, or collection, of specialized 
processors known as shaders, or simultaneous multiprocessors, or stream processors. 
Organized as a SIMID processor they can execute (process) a similar instruction 
(program, or kernel) simultaneously, or in parallel. Hence, they are known as a 
parallel processor. (A shader is a computer program that is used to do shading: the 
production of appropriate levels of color within an image.) 
GPU (Graphics Processing Unit)—The GPU is the chip that drives the display 
(monitor) and generates the images on the screen (and has also been called a Visual 
Processing Unit or VPU). The GPU processes the geometry and lighting effects and 
transforms objects every time a 3D scene is redrawn—these are mathematically-
intensive tasks and hence the GPU has upwards to hundreds of ﬂoating-point 
processor (also called Shaders or Stream Processors.) Because the GPU has so many 
powerful 32-bit ﬂoating-point processors, it has been employed as a special purpose 
processor for various scientiﬁc calculations other than display ad is referred to as a 
GPGPU in that case. The GPU has its own private memory on a graphics AIB which 
is called a frame buffer. When a small (less than ﬁve processors) GPU is put inside a 
northbridge (making it an IGP) the frame buffer is dropped and the GPU uses system 
memory. The GPU has to be compatible with several interface standards including 
software APIs such as OpenGL and Microsoft’s DirectX, physical I/O standards 
within the PC such as Intel’s Accelerated Graphics Port (AGP) technology and PCI 
Express, and output standards known as VGA, DVI, HDMI, and Display Port. 
GPU-Compute (GPGPU—General-Purpose Graphics Processor Unit)—The 
term “GPGPU” is a bit misleading in that general-purpose computing such as the 
type an × 86 CPU might perform cannot be done on a GPU. However, because GPUs 
have so many (hundreds in some cases) powerful (32-bit) ﬂoating-point processors, 
they have been employed in certain applications requiring massive vector operations 
and mathematical intensive problems in science, ﬁnance, and aerospace applications.

Appendix B: Deﬁnitions
305
The application of a GPU can yield several orders of magnitude higher performance 
than a conventional CPU. 
GPU Preemption—The ability to interrupt or halt an active task (context switch) on 
a processor and replace it with another task, and then later resume the previous task 
this is a concept In the era of single core CPUs preemption was how multitasking was 
accomplished. Interruption in a GPU, which is designed for streaming processing, is 
problematic in that it could necessitate a restart of a process and thereby delay a job. 
Modern GPUs can save state and resume a process as soon as the interruptive job is 
ﬁnished. 
Graphics Adapters—A graphics adapter is the device, subsystem, add-in board, 
chip, or adapter used to generate a synthetic image and drive a display. It has been 
called many things over the decades. Here are the names used in this book. The 
differences may seem subtle, but they are used to differentiate one device from 
another. For example, it is common to see the acronym GPU used when speaking or 
writing about an add-in board. They are not synonyms, and a GPU is a component 
of an AIB. That is not a pedantic diatribe. It would be like referring to an engine 
or transmission to denote an entire automobile or truck. Part of the reason for the 
misuse of terms is misunderstanding, another reason is the ease of speech (like calling 
someone Tom instead of Thomas), and the third is that it is more fun and exciting to 
use. People like to say GPGPU, an initialism for general-purpose GPU, as a shorthand 
notation for GPU-computer. So, we cannot be the terminology police, but we can try 
to clarify the differences. Generally, an acronym should be a pronounceable word. 
Graphics Controller—A graphics controller or graphics chip is a nonprogrammable 
device designed primarily to drive a screen. More advanced versions have some prim-
itive drawing or shading graphic capabilities. The primary differentiation between a 
controller and coprocessor or GPU is the programmable capability. 
Graphics Coprocessors—Co-processors (also written as coprocessors) can serve 
as programmable processors, such as the Texas Instruments’ TI TMS34010 and Tl 
TMS34020 series. Co-processors can run all the graphics functions of an API and 
display lists for applications such as CAD. 
Graphics Driver—A device driver is a software stack that controls computer 
graphics hardware and supports graphics rendering APIs and is released under a free 
and open-source software license. Graphics device drivers are written for speciﬁc 
hardware to work within the context of a speciﬁc operating system kernel and to 
support a range of APIs used by applications to access the graphics hardware. They 
may also control output to the display, if the display driver is part of the graphics 
hardware. 
G-Sync—A proprietary adaptive sync technology developed by Nvidia aimed 
primarily to eliminate screen tearing and the need for software deterrents such as 
V-sync. G-Sync eliminates screen tearing by forcing a video display to adapt to the 
frame rate of the outputting device rather than the other way around, which could

306
Appendix B: Deﬁnitions
traditionally be refreshed halfway through the process of a frame being output by 
the device, resulting in two or more frames being shown at once. 
HBAO+—Developed by Nvidia, HBAO+ claims the company, improves upon 
existing Ambient Occlusion (AO) techniques and adds richer, more detailed, more 
realistic shadows around objects that occlude rays of light. Compared to previous 
techniques, Nvidia claims HBAO+ is faster, more efﬁcient, and signiﬁcantly better. 
HBM (High Bandwidth Memory)—HMB is a high-performance RAM interface 
for 3D-stacked DRAM from AMD and Hynix. It is to be used in conjunction with 
high-performance graphics accelerators and network devices. The ﬁrst devices to use 
HBM are the AMD Fiji GPUs. 
HDCP (High Bandwidth Digital Content Protection)—HDCP is an encryption 
system for enforcing digital rights management (DRM) over DVI and HDMI inter-
faces. The copy protection system (DRM) resides in the computer and prevents the 
user of the PC from copying the video content. 
HDMI (High-Deﬁnition Multimedia Interface)—HDMI is a digital, point-to-point 
interface for audio and video signals designed as a single-cable solution for home 
theater and consumer electronics equipment and also supported in graphics AIBs 
and some PC motherboards. Introduced in 2002 by the HDMI consortium, HDMI is 
electrically identical to video-only DVI. 
Heterogeneous Processors—Heterogeneous computing refers to systems that use 
more than one kind of processor or cores. These systems gain performance or energy 
efﬁciency not just by adding the same type of processors, but by adding dissim-
ilar coprocessors, usually incorporating specialized processing capabilities to handle 
particular tasks. 
Hexadecimal—Hexadecimal is the base-16 number system, which has the following 
digits: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, A, B, C, D, E, and F. Each hexadecimal digit, therefore, 
can also be represented by 4 bits (also called a “Nibble”), with two hexadecimal digits 
fully occupying a byte (8 bits). Also referred to as “Hex.” Hexadecimal notation is 
frequently used in low-level programming, such as accessing a graphics chip or 
writing device drivers. In the C and C + +  programming languages, hexadecimal 
numbers are designated by preﬁxing them with a “0x” (zero “x”), while in Intel 
assembly language, hexadecimal numbers have a sufﬁx of “H,” and may have a 
preﬁx of “0” (zero) if the ﬁrst digit is greater than “9.” For example, the hex number 
notation for “E988” would appear as 0xE988 in C or C + +  and as 0E988H in 
assembly language. 
HDR10—10-bit HDR using BT.2020, PQ and static metadata. 
High Dynamic Range (HDR)—A dynamic range higher than what is consid-
ered to be standard dynamic range. The term is often used in discussing displays, 
photography, 3D rendering. 
High Dynamic Range TV (ITU-R BT.2100). 
Also see Wide color gamut.

Appendix B: Deﬁnitions
307
High-Dynamic-Range Imaging (HDRI)—The compositing and tone-mapping of 
images to extend the dynamic range beyond the native capability of the capturing 
device. 
HEVC—High-Efﬁciency Video Codec (ITU-T h.265)—2 × more efﬁcient than 
AVC. 
HFR—High Frame Rate (100 & 120 fps). 
HLG—Hybrid Log Gamma Transfer Function for HDR signals (ITU-R BT.2100). 
HLG deﬁnes a nonlinear transfer function in which the lower half of the signal 
values use a gamma curve (SD & HD) and the upper half of the signal values use a 
logarithmic curve. HLG is backwards compatible with SDR. 
HPU—(Heterogeneous Processor Unit)—An integrated multi-core processor with 
two or more × 86 cores, and four or more programmable GPU cores. 
Hull Shaders—See Tessellation shaders. 
IGP (Integrated Graphics Processor)—An IGP is a chip that is the result of inte-
grating a graphics processor with the northbridge chip (see northbridge and chipset) 
An IGP may refer to enhanced video capabilities, such as 3D acceleration, in contrast 
to an IGC (integrated graphics controller) that is a basic VGA controller. When a 
small (less than ﬁve processors) GPU is put inside a northbridge (making it an IGP) 
the frame buffer is dropped and the GPU uses system memory, this is also known as 
a UMA—uniﬁed memory architecture. 
iGPU—A scaled down version, with fewer shaders (processors) than a discrete GPU 
that uses shared local RAM (DDR) with the CPU. 
Image Generation—The image generation stage in a GPU where the ﬁnal, displayed 
pictures are created before being sent to the screen. It is where the user engages with 
the results of the entire system. In the case of movies or TV, it is passive. In the 
case of computers, it is interactive, such as playing a game. In the case of interactive 
images, they can be for content creation work or content consumption. There is a 
relentless demand and need for high quality, fast response, and image generation in 
all cases. 
Image Sensor—An image sensor, photo-sensor, or imaging sensor is a device, which 
detects the presence of visible light, infrared transmission (IR), and/or ultraviolet 
(UV) energy. That information constitutes an image. It does so by converting the 
variable attenuation of waves of light (as they pass through or reﬂect off objects) 
into electrical signals. Image sensors are used in electronic imaging devices of both 
analog and digital types, which include digital cameras, camera modules, medical 
imaging equipment, night vision equipment such as thermal imaging devices, radar, 
sonar, and others. The Digital Image Sensor is an Integrated Circuit Chip which has 
an array of light sensitive components on the surface. The array is formed by the 
individual photosensitive points. Each photosensitive sensor point inside the image

308
Appendix B: Deﬁnitions
circle acts to convert the light to an electrical signal. The full set of electrical signals 
is converted into an image by the on-board computer. 
ISP, Image Synthesis Processor—An ISP refers to a processing unit that accepts 
as input the raw samples from an imaging sensor and converts them into a human-
viewable image. The samples may have undergone some pre-processing by the sensor 
circuitry to abstract certain details of the sensor operation but in general, they are 
presented in the form of a “mosaic” of color samples without correction for things 
like lens distortion, defective pixels and temporal sampling artifacts. These things, as 
well as extracting the image from the color sample mosaic and encoding the output 
into a standard format are the responsibility of the ISP. 
ITU-R BT.2020—AKA Rec2020 deﬁnes various aspects of ultra-high-deﬁnition 
television (UHDTV) with standard dynamic range (SDR) and wide color gamut 
(WCG), including picture resolutions, frame rates with progressive scan, bit depths, 
color primaries. 
ITU-R BT.2100—deﬁnes various aspects of high dynamic range (HDR) video such 
as display resolution (HDTV and UHDTV), bit depth, Bit Values (Files), frame rate, 
chroma subsampling, color space. 
ITU-R BT.709—AKA Rec709 standardizes the format of high-deﬁnition television, 
having 16:9 (widescreen) aspect ratio. 
Jitter—In computer graphics, to “jitter a pixel” means to place it off side of its normal 
placement by some random amount in order to achieve a more natural appearance. 
It is also described as shaking. The term is used in several ways, but it always refers 
to some offset of time and space from the norm. 
JPR—Jon Peddie Research. 
Judder—Vertical synchronization can also cause artifacts in video and movie presen-
tations, as they are generally recorded at frame rates signiﬁcantly lower than the 
typical monitor frame rates (24–30 frame/s). When such a movie is played on a 
monitor set for a typical 60 Hz refresh rate, the video player misses the monitor’s 
deadline fairly frequently, in addition to the interceding frames being displayed at 
a slightly higher rate than intended for, resulting in an effect similar to judder. (See 
Telecine: Frame rate differences.) 
KB—Kilobyte. 1024 bytes, where each byte consists of 8 bits of data. 
Launch—There is no standard. It can be the date the GPU ﬁrst shipped, or the date 
of the announcement. 
LCD (Liquid Crystal Display)—The technology used for displays in notebook 
and other smaller computers. Like light-emitting diode (LED) and gas-plasma tech-
nologies, LCDs allow displays to be much thinner than cathode ray tube (CRT) 
technology.

Appendix B: Deﬁnitions
309
Least Signiﬁcant Bit—In a number in binary representation, the least signiﬁcant bit 
is the one with the lowest value, i.e., the rightmost bit when a number is shown in 
traditional binary form. When this term is in plural form, it refers to multiple bits of 
least signiﬁcance. 
Level of Detail—A term used to describe one or more different sets of detail deﬁning 
a particular geometry or raster image. A low level-of-detail geometry or image is one 
that would be used for rendering an image at a great viewing distance, while a high 
Level of Detail would be used when at a short viewing distance. In raster images 
where texture mapping is used for rendering, Level of Detail is used interchangeably 
with the term “mip-map.” In normal 3D graphics usage, a low numbered Level of 
Detail refers to higher detail, with a Level of Detail 0 (zero) being the highest level 
of detail. See “mip-map.” 
LOD—See Level of Detail. 
LOD Value—A value, used mostly in raster rendering, to deﬁne a calculated viewing 
distance in terms of LODs. For example, an LOD value of 1.585 would indicate a 
view distance located between LOD 1 and LOD 2. 
Long Short-Term Memory (LSTM)—A component of a recurrent neural network 
that includes a memory cell. The component can be used to ‘remember’ events over 
arbitrary periods of time. LSTM can also refer to any network that makes use of 
LSTM components. 
LSB—See Least Signiﬁcant Bit. 
Luminance—A photometric measure of the luminous intensity per unit area of light 
traveling in a given direction. It describes the amount of light that passes through, is 
emitted or reﬂected from a particular area, and falls within a given solid angle. The 
SI unit for luminance is candela per square meter (cd/m2). A non-SI term for the 
same unit is the “nit”. The CGS unit of luminance is the stilb, which is equal to one 
candela per square centimeter or 10 kcd/m2. 
LUT—Acronym for Look-Up Table. LUTs are part of the RAMDAC of a graphics 
subsystem, and in modern graphics chips are usually located within the chip itself. 
The LUT is the part of the output section of a graphics board that translates a pixel 
value (primarily in 4 or 8 BPP indexed color modes) into its red, green, and blue 
components. Once the components have been determined, they are passed through 
the three DACs (red, green, and blue) to generate displayable signals. A diagram of 
this operation, showing an 8-bit pixel, with a value of 250, going through the LUTs, 
is below:

310
Appendix B: Deﬁnitions
LUX—A Lux is one lumen per square meter. 
M&A—Mergers and acquisitions. 
M&E—Media and entertainment. 
Mapped—A term that is used often in computer graphics, which loosely means to 
be ﬁtted to something. One maps to a spatial distribution of (something). A texture 
map is a 2D image of something, bricks, or wood paneling for example. 
MB—Megabyte. 1 megabyte is equal to 1 KB * 1 KB = 1024 × 1024 = 1,048,576 
bytes. 
MDL—Material deﬁnition library. 
Memory: 
• Bus width. The bit width of the memory bus. 
• Size of the graphics memory expressed in Gigabytes (GB). 
• Clock. The reference or base frequency of the memory clock, expressed in MHz 
or GHz. 
• Bandwidth. The maximum rate of data transfer across the memory expressed in 
mega- or gigabytes per second (GB/s, or MB/s). 
MIP-Map—A mip-map is one of a series of different versions of the same texture, 
each at a different resolution. Each version is generally one-quarter the size of 
the version preceding it. See also “Mip mapping,” “Bilinear Filtering,” “Trilinear 
Filtering,” “Texel,” and “Texture Mapping.” Mip-mapping is the use of mip-maps 
during the rendering process. For example, when an image is rendered using nearest 
mip-map selection, the version of the texture that most closely matches the size of the

Appendix B: Deﬁnitions
311
image is the one chosen for rendering. In a linearly interpolated mipmapping oper-
ation (also known as “Trilinear Filtering”), a weighted average of the two nearest 
mip-maps based on the LOD value. The term MIP is an acronym for the latin expres-
sion “Multum in parva”—(many in small) implying the presence of many images in 
a small package. 
Mixed Reality—Mixed Reality (MR) seamlessly blends a user’s real-world envi-
ronment with digitally-created content, where both environments coexist to create 
a hybrid experience. In MR, the virtual objects behave in all aspects as if they are 
present in the real world, e.g., they are occluded by physical objects, their lighting 
is consistent with the actual light sources in the environment, they sound as though 
they are in the same space as the user. As the user interacts with the real and virtual 
objects, the virtual objects will reﬂect the changes in the environment as would any 
real object in the same space. 
Model—The marketing name for a GPU assigned by the manufacture, for example, 
AMD’s Radeon, Intel’s Xe, and Nvidia’s GeForce. A model can also be a 3D object. 
For example, the design of a car is a 3D model. 
Motherboard—The main circuit board in a PC, also known as a system boar or a 
planar (by IBM.) Graphics AIBs and other cards (i.e., audio, gigabyte Ethernet, etc.), 
as well as memory, the CPU, and disk drive cables plug into the motherboard. 
Motion-To-Photon Latency (MTPL)—Also known as the End-to-end latency is the 
delay between the movement of the user’s head and the change of the VR device’s 
display reﬂecting the user’s movement. As soon as the user’s head moves, the VR 
scenery should match the movement. The more delay (latency) between the two 
actions, the more unrealistic the VR world seems. To make the VR world realistic, 
VR systems want low latency of < 20 ms. 
Multi Frame Noise Reduction (MFNR)—Automatically take multiple images 
continuously, combine them, reduce the noise, and record them as one image. With 
multiframe noise reduction, one can select larger ISO numbers than the maximum 
ISO sensitivity. The image recorded is one combined image. 
Multiplexer—A multiplexer, also known as a MUX, is an electronic device that acts 
as a switching circuit. A mux has two or more data inputs, along with a switch or 
select input which determines which of the data inputs is passed to the output portion 
of the device. 
Multi-Projection—Multi-projection can refer to an image created using multiple 
projectors mapped onto a screen, or set of screens (as in a CAVE) for 3D projection 
mapping using multiple projectors. It can also refer to multiple projections within a 
screen in computer graphics. 
Multiplayer Game—Multiplayer games have traditionally meant that humans are 
playing with other humans cooperatively, competing against each other, or both. 
Artiﬁcial Intelligence controlled players have historically been excluded from the 
traditional deﬁnition of multiplayer game. However, as AI technology progresses

312
Appendix B: Deﬁnitions
this is likely to change. In the future human-controlled player’s skill and behavior 
tracked over time could program the skill and behavior of a unique AI that can be 
substituted for the human’s participation in the game. 
Battle Royale is a game mode that creates a translucent dome (or other demarcation) 
over/around the entire playing area. As the match progresses the dome starts to shrink 
toward a random point on the map. Players must stay within the bounds of the dome or take 
damage leading to death. The shrinking dome “herds” players into smaller and smaller areas, 
eventually ensuring that they will be in “close combat.” In summary, Battle Royale mode 
allows large-scale combat using long range weapons and vehicles over large distances; but 
eventually forces the remaining players into CQC (close quarter combat); ensuring that the 
round time does not extend too long; and a new round can begin. 
Permadeath is a video game and simulation feature where the player’s death eliminates them 
from the ability to continue participation in the game or continue as the speciﬁc entity they 
were playing. Permadeath can come in a number of forms. In multiplayer combat games, 
this usually means having to wait until the next round starts if killed. In most multiplayer 
combat games, rounds last 5 to 30 minutes; however, it is theoretically possible that death 
in a game would permanently exclude the player from further participation. 
In other multiplayer combat games, permadeath can mean losing all your equipment and 
your position on the map, forcing the player to respawn with no equipment as a new “entity.” 
Even though there is no waiting period, the ramiﬁcations of dying are signiﬁcant, as the 
player has often spent signiﬁcant time equipping themselves and moving to strategic areas 
of the map. 
Persistent Wrld Games track (or attempt to track) the entire game universe as individual 
objects and the state of each object in one single instance. For example, in a massive multi-
player persistent world games if a tree is cut down the tree will forever be cut down for all 
players, and for all of time. In single player games, the user sometimes has the ability to 
“restart” the universe or run multiple iterations of the universe. Running multiple iterations 
is known as “sharding” the universe. In this former case, the tree would reappear or in the 
latter case have various states of being dependent on the shard being played. 
Sharded World Games can have multiple simultaneous existences of the same base game 
universe in varying degrees of state. Sharded world games that employ procedurally gener-
ated universes can have multiple simultaneous versions of non-matching universes. In either 
case for multiplayer, this is usually done to reduce the server load of players and reduce 
latency by grouping players from geographical regions into the most optimal “shard.” There 
can be hundreds of servers running the same universe but with unique player participation 
and parametric states of being. 
MUX—See Multiplexer. 
Nit—A nit is candela per square meter (cd/m). 
Normal Map—Normal maps can be referred to as a newer, better type of bump 
map. A normal map creates the illusion of depth detail on the surface of a model but 
it does it differently than a bump map that uses grayscale values to provide either 
up or down information. It is a technique used for faking the lighting of bumps and 
dents—an implementation of bump mapping. It is used to add details without using 
more polygons.

Appendix B: Deﬁnitions
313
Northbridge—The Northbridge is the controller that interconnects the CPU to 
memory via the frontside bus (FSB). It also connects peripherals via high-speed 
channels such as PCI Express, and the AGP bus. 
NTSC (National Television Systems Committee) —Analog color television system 
standard used in USA, Canada, Mexico and Japan. Other standards include. 
NURBS—Non-uniform rational basis spline (NURBS) is a mathematical model 
commonly used in computer graphics for generating and representing curves and 
surfaces. It offers great ﬂexibility and precision for handling both analytic (surfaces 
deﬁned by common mathematical formulae) and modeled shapes. 
Oscilloscope—Early oscilloscopes used cathode ray tubes (CRTs) as their display 
element. Storage Oscilloscopes used special storage CRTs to maintain a steady 
display of a signal brieﬂy presented. Storage scopes (e.g., Tektronix 4010 series) 
were often used in computer graphics as a vector scope. 
ODM—Original device manufacturer. 
OLED (Organic Light-Emitting Diode)—A light-emitting diode (LED) in which 
the emissive electroluminescent layer is a ﬁlm of organic compound that emits light 
in response to an electric current. This layer of organic semiconductor is situated 
between two electrodes; typically, at least one of these electrodes is transparent. 
OLEDs are used to create digital displays in devices such as television screens, 
computer monitors, portable systems such as mobile phones. 
Open Graphics Library (OpenGL)—A cross-language, cross-platform application 
programming interface (API) for rendering 2D and 3D vector graphics. The API is 
typically used to interact with a graphics processing unit (GPU), to achieve hardware-
accelerated rendering. 
OpenVDB—OpenVDB is an Academy Award-winning open-source C + +  library 
comprising a novel hierarchical data structure and a suite of tools for the efﬁcient 
storage and manipulation of sparse volumetric data discretized on three-dimensional 
grids. It was developed by DreamWorks Animation for use in volumetric applica-
tions typically encountered in feature ﬁlm production and is now maintained by the 
Academy Software Foundation (ASWF). https://github.com/AcademySoftwareFou 
ndation/openvdb. 
Ordered Dither—An ordered dither is the application of a series of dither values that 
change in according to a particular pattern, most often in the form of a matrix (also 
referred to as a “dither matrix”), over the course of a set of dithering operations. 
An ordered dither is traditionally applied positionally, using the modulus of the 
destination pixel X and Y position as an index into the dither matrix. 
Outside-In-Tracking—Outside-In-Tracking is a form of positional tracking where 
ﬁxed external sensors placed around the viewer are used to determine the position 
of the headset and any associated tracked peripherals. Various methods of tracking 
can be used, including, but not limited to, optical and IR.

314
Appendix B: Deﬁnitions
PAL—Analog TV system used in Europe, and elsewhere. 
Palette—The computer graphics term “palette” is derived from the concept of an 
artist’s palette, the ﬂat piece of material upon which the artist would select and blend 
his colors to create the desired shades. The palette on a graphics board speciﬁes the 
range of colors available in any one pixel. For example, standard VGAs tend to have 
a palette of 262,144 colors, stemming from the fact that each color in the palette is 
composed of 6 bits each of red, green, and blue (total of 18 bits, and 2 Mod ifyingA bove With caret 18 equals 262 comma 144 right parenthesis. 
However, since the VGA can only display 16 or 256 colors on-screen at any one time, 
it means that each one of these 16 or 256 colors must be chosen from the larger palette 
via a set of LUTs. See “LUT” for details. 
PAM—Potential available market. 
PCI—Acronym for Peripheral Component Interface. PCI is a bus standard which 
Intel developed to overcome the performance bottlenecks inherent in the ISA bus 
design, and most modern graphics boards are PCI-based (i.e., they need to be inserted 
into the PCI bus in order to work). 
Performance: 
• Shader operations. How many operations the pixel shaders (or uniﬁed shaders) 
can perform, measured in Operations/s. 
• Vertex operations The number of operations processed on the vertex shaders in 
Direct3D 9.0c and older GPUs, expressed in Vertices/second. 
Phong Shading—Refers to an interpolation technique for surface shading in 3D 
computer graphics. It is also called Phong interpolation or normal-vector interpola-
tion shading. Speciﬁcally, it interpolates surface normals across rasterized polygons 
and computes pixel colors based on the interpolated normals and a reﬂection model. 
Phong shading may also refer to the speciﬁc combination of Phong interpolation and 
the Phong reﬂection model. 
Ping-Pong Buffering—A technique for managing the sharing of real-time streaming 
data between software threads or hardware units. Two or more buffers are allocated 
and the thread or hardware unit responsible for acquiring the data is given control 
of the ﬁrst buffer. As soon as that buffer is ﬁlled, control is handed to the thread or 
unit responsible for processing the data. While processing is happening, control of 
the second buffer is given to the input thread or unit, and is ﬁlled with the streaming 
data. Filling and processing continue to alternate, or ping-pong between buffers in 
this fashion indeﬁnitely. 
Pixel—Acronym for PIX ELement (“Pix” is a shortened version of “Picture”). The 
name given to one sample of picture information. Can refer to an individual sample of 
RGB luminance or chrominance information. A pixel is the smallest unit of display 
that a computer can access to display information but may consist of one or more 
bits (see “BPP”).

Appendix B: Deﬁnitions
315
Pixel Density—Information of the number of pixels in a unit of length. With the 
decrease of the display size and the increase of its resolution, the pixel density 
increases. 
Pixel Pitch—The pixel pitch shows the distance from the centers of two neighboring 
pixels. In displays, which have a native resolution (the TFT ones, for example), the 
pixel pitch depends on the resolution and the size of the screen. 
Player Versus Player (PvP)—Player versus player (PvP) refers to a game that is 
designed for gamers to compete against other gamers, rather than against the game’s 
artiﬁcial intelligence (AI). PvP games generally feature an AI that acts as a second 
player if the gamer plays solo. PvP games are the opposite of player versus envi-
ronment (PvE) games, where the player contends largely with computer-controlled 
characters or situations. 
Polygonal Modeling—In 3D computer graphics, polygonal modeling is an approach 
for modeling objects by representing or approximating their surfaces using polygon 
meshes. Polygonal modeling is well suited to scanline rendering and is therefore the 
method of choice for real-time computer graphics. 
Power Island—In chip design, it is common practice to isolate unrelated parts of 
the circuitry from each other and supply the power to each isolate region separately 
so that they can be individually powered down when not required. This saves power 
because CMOS transistors in particular “leak” charge into the substrate even when 
inactive, as long as they are powered. It is necessary to take special steps to isolate 
circuits in MOS devices because, unless modiﬁed, all transistors are (somewhat 
weakly) connected together via the substrate. 
PPI (Pixels Per Inch)—The pixel density (resolution) of an electronic image device, 
such as a computer monitor or television display, or image digitizing device such as 
a camera or image scanner also referred to as pixels per centimeter (PPCM).

316
Appendix B: Deﬁnitions
The term “dots per inch” (dpi), extended from the print medium, is sometimes used 
instead of pixels per inch. The dot pitch determines the absolute limit of the possible 
pixels per inch. However, the displayed resolution of pixels (picture elements) that 
is set up for the display is usually not as ﬁne as the dot pitch. 
Projection Mapping—Projection mapping, also known as video mapping and 
spatial augmented reality, is a projection technology used to turn objects, often irreg-
ularly shaped, into a display surface for video projection. The technique dates back to 
the late 1960s, where it was referred to as video mapping, spatial augmented reality, 
or shader lamps. This technique is used by artists and advertisers alike who can add 
extra dimensions, optical illusions, and notions of movement onto previously static 
objects. 
PQ—Perceptual Quantizer Transfer Function for HDR signals (SMPTE ST 2084, 
ITU-R BT.2100). 
PvP—See Player versus Player. 
RAMDAC—Acronym for Random Access Memory Digital to Analog Converter. 
The “RAM” portion of a RAMDAC refers to the LUTs, which by necessity are 
RAMs, while the “DAC” refers to the Digital to Analog Converters. See “DAC” and 
“LUT” for more details. 
Raster Graphics—Also called scan-line, and bitmap graphics, a type of digital 
display that uses tiny four-sided but not necessarily square pixels, or picture elements, 
arranged in a grid formation to represent an image. Raster scan graphics has origins in 
television technology, with images constructed much like the pictures on a television 
screen. 
Raster-Scan Display—A CRT uses a raster scan. Developed for television tech-
nology, an electron beam sweeps across the screen, from top to bottom covering one 
row at a time. The beam intensity is turned on and off as it moves across each row 
to create images. The screen points are referred to as pixels. 
Recurrent Neural Network (RNN)—A class of Neural Networks whose connec-
tions form a directed cyclic graph. In other words, unlike a Feedforward network 
such as a CNN, the connections include feedback so that outputs can affect subse-
quent inputs, giving rise to temporal behaviors. An example of an RNN is the Long 
Short-Term Memory (LSTM) network popular in speech recognition. 
Reﬂective Shadow Maps—Reﬂective shadow maps (RSMs) are an extension to a 
standard shadow map, where every pixel is considered as an indirect light source. 
The illumination due to these indirect lights is evaluated on-the-ﬂy using adaptive 
sampling in a fragment shader. By using screen-space interpolation of the indirect 
lighting, it is possible to achieve interactive rates, even for complex scenes. Since 
visualizations and games mainly work in screen space, the additional effort is largely 
independent of scene complexity. The resulting indirect light is approximate but leads 
to plausible results and is suited for dynamic scenes.

Appendix B: Deﬁnitions
317
Register File—Microprocessors hold data for immediate processing in a small 
amount of fast, local memory referred to as a register ﬁle. This memory is closely 
managed by the compiler and the amount and characteristics of it are crucial to the 
performance of the processor. There is typically, but not always, one register ﬁle for 
each ALU or execution unit in the processor and the organization of the register ﬁle 
closely matches the organization of the execution unit. For example, a register ﬁle 
associated with the scalar unit of a 32-bit processor would be 32-bit wide, whereas 
one associated with a SIMD vector unit with four 32-bit paths would be 128-bit wide 
to hold the required four 32-bit operands. 
Relative luminance—Relative luminance is formed as a weighted sum of linear 
RGB components, not gamma-compressed ones. Even so, luma is often erroneously 
called luminance. SMPTE EG 28 recommends the symbol Y, to denote luma and 
the symbol Y to denote relative luminance. 
Render Farm—A render farm is high performance computer system, e.g., a 
computer cluster, built to render computer-generated imagery (CGI), typically for 
ﬁlm and television visual effects. 
Resolution, Screen Resolution—The number of horizontal and vertical pixels on a 
display screen. The more pixels, the more information is visible without scrolling. 
Screen resolutions have a pixel count such as 1600 × 1200, which means 1,600 
horizontal pixels and 1,200 vertical pixels. 
RGB—Red, Green, and Blue. Color components of a pixel blended to create a 
speciﬁc color on a display monitor. See “Color” for additional details. 
Room-Scale VR—Room-Scale VR is an implementation of 6 DoF including the 
required use of Spherical Video tracking, where the user is able to move around 
a room-sized environment using real-world motion as reﬂected in the virtual 
environment. 
ROP—ROP stands for Raster Operator; Raster Operators (ROPs) handle several 
chores near the end of the pixel pipeline. ROPs handle anti-aliasing, Z and color 
compression, and the actual writing of the pixel to the output buffer. 
Rounding—An arithmetic operation adjusts a number up or down relative to its 
magnitude in relation to some deﬁned magnitude. Normally, rounding is used to 
adjust a number in integer-fractional format to integer, with fractional values of 0 
to less than .5 (or 1/2) being adjust downward, and fractional values of .5 to less 
than 1 being adjusted upward. Programmatically, rounding is usually accomplished 
by adding .5 to a number, and then truncating the resulting fractional amount. See 
“Truncation.” For example, 1.585 rounded is 2, while 1.499 rounded is 1. 
RSMs—See reﬂective shadow maps. 
RT—Ray tracer or ray tracing. 
SaaS—Software as a service.

318
Appendix B: Deﬁnitions
SAM—Served available market. 
Scan-line Display—See Raster graphics display. 
Scanline Rendering—An algorithm for visible surface determination, in 3D 
computer graphics, that works on a row-by-row basis rather than a polygon-by-
polygon or pixel-by-pixel basis. 
Screen Size—On 2D displays, such as computer monitors and TVs, the display size 
(or viewable image size or VIS) is the physical size of the area where pictures and 
videos are displayed. The size of a screen is usually described by the length of its 
diagonal, which is the distance between opposite corners, usually in inches. 
Screen Tearing—A visual artifact in video display where a display device shows 
information from multiple frames in a single screen draw. The artifact occurs when 
the video feed to the device is not in sync with the display’s refresh rate. This can be 
due to non-matching refresh rates—in which case the tear line moves as the phase 
difference changes (with speed proportional to difference of frame rates). 
SDK—Software development kit. 
SECAM—Analog TV system used in France and parts of Russia and the Mideast. 
SDR—Standard Dynamic Range TV (Rec.601, Rec.709, Rec.2020). 
Shaders—Shaders is a broadly used term in graphics and can pertain to the 
processing of specialized programs for geometry (known as vertex shading or 
transform and lighting), or pixels shading. 
Shifter—A device shifts numbers 1 or more bit positions. For example, the decimal 
number 14 (1110b), when passed through a shift that shifts one bit to the left, would 
produce decimal 7 (0111b). Each bit shift to the right is equivalent to an integer 
divide by 2, while each bit shift to the left is equivalent to an integer multiply by 2. 
Shifters are normally used to scale values up or down. 
SIMD—Same Instruction Multiple Data describes computers with multiple 
processing elements that perform the same operation on multiple data points simul-
taneously. Such machines exploit data level parallelism, but not concurrency: there 
are simultaneous (parallel) computations, but only a single process (instruction) at a 
given moment. SIMD is particularly applicable to common tasks like adjusting the 
contrast and colors in a digital image. 
SOM—Share of market. 
Southbridge—The Southbridge controller handles the remaining I/O, including the 
PCI bus, parallel and Serial ATA drives (IDE), USB, FireWire, serial and parallel 
ports and audio ports. Earlier chipsets supported the ISA bus in the Southbridge. 
Starting with Intel’s 8xx chipsets, Northbridge and Southbridge were changed to 
Memory Controller and I/O Controller (see Intel Hub Architecture).

Appendix B: Deﬁnitions
319
Span Mode—Some applications, such as games, have an explicit screen resolution 
setting. They will typically default to monitor’s registered resolution. In span mode, 
which is a feature of the driver provided by the GPU supplier, it is possible to make 
one contagious display that spans across all the monitors you choose. Then, when 
the application is opened, it will ﬁll the screens. 
sRGB—sRGB is a color space, developed jointly by Hewlett- Packard and Microsoft 
in 1996. It is used in different devices such as printers, displays, TV sets, cameras, 
etc. The sRGB color space covers about 72% of the NTSC color space. 
Static Contrast—The static contrast shows the ratio between the brightest and the 
darkest color, which the display can reproduce simultaneously, for example, within 
one and the same frame/scene. 
Stream Processors—A stream processor is a ﬂoating-point processor found in a 
GPU and is also known as a shader processor. 
Stuttering—A term used to describe a quality defect that manifests as irregular 
delays between frames rendered by the GPU(s), causing the instantaneous frame 
rate of the longest delay to be signiﬁcantly lower than the frame rate reported (by 
benchmarking application). In lower frame rates when this effect may be apparent 
the moving video appears to stutter, resulting in a degraded gameplay experience in 
the case of a video game, even though the frame rate seems high enough to provide 
a smooth experience. Single-GPU conﬁgurations do not suffer from this defect in 
most cases and can in some cases output a subjectively smoother video compared 
to a multi-GPU setup using the same video card model. Micro stuttering is inherent 
to multi-GPU conﬁgurations using alternate frame rendering (AFR), such as Nvidia 
SLi and AMD CrossFireX but can also exist in certain cases in single-GPU systems. 
Variations in the rate of data input and processing speed can result in overﬂow, 
which can sometimes be prevented by allocating more than two buffers, in which 
case the system is referred to as a circular buffer. 
Subdivision Surface—Subdivision smooths and adds extra resolution to curves and 
surfaces at display and/or render time. The renderer subdivides the surface until 
it’s smooth down to the pixel level. The smooth surface can be calculated from the 
coarse mesh as the limit of recursive subdivision of each polygonal face into smaller 
faces that better approximate the smooth surface. This lets one work with efﬁcient 
low-polygon models and only add the smoothing “on demand” on the graphics card 
(for display) or in the renderer. The tradeoff is that subdivision curves/surfaces take 
slightly longer to render. However, smoothing low-resolution polylines using curve 
subdivision is still much faster than working with inherently smooth primitives such 
as NURBS curves. 
Subsurface Scattering (SSS)—Also known as subsurface light transport (SSLT), is 
a mechanism of light transport in which light penetrates the surface of a translucent 
object, is scattered by interacting with the material, and exits the surface at a different 
point.

320
Appendix B: Deﬁnitions
Sub-pixel Morphological Anti-aliasing (SMAA)—This ﬁlter detects edges in a 
rendered image and classiﬁes edge crossings into various shapes and shades, in an 
attempt to make the edges or lines look smoother. Almost every GPU developer has 
their own version of anti-aliasing. 
Super-ray—A grouping of rays within and across views, as a key component of a 
light ﬁeld processing pipeline. 
TAM—Total available market. 
Taring and Frame Dropping—Vsync, where the monitor is synchronized to the 
powerline frequency, can cause the screen to be refreshed halfway through the process 
of a frame being output by the GPU, resulting in two or more frames being shown 
at once. 
TBP—The typical AIB power consumption, measured in watts. 
TDP (Thermal design power)—The maximum heat generated by the GPU, expressed 
in watts. 
Telecine—Telecine is the process of transferring motion picture ﬁlm into video. The 
most complex part of telecine is the synchronization of the mechanical ﬁlm motion 
and the electronic video signal. Normally, best results are then achieved by using a 
smoothing (interpolating algorithm) rather than a frame duplication algorithm (such 
as 3:2 pulldown, etc.). 
Tessellation Shaders—A Tessellation Shader adds two new shader stages to the 
traditional model. Tessellation Control Shaders (also known as Hull Shaders) and 
Tessellation Evaluation Shaders (also known as Domain Shaders), which together 
allow simpler meshes to be subdivided into ﬁner meshes at run-time according to a 
mathematical function. The function can be related to a variety of variables, most 
notably the distance from the viewing camera to allow active level-of-detail scaling. 
This allows objects close to the camera to have ﬁne detail, while further away ones can 
have coarser meshes, yet seem comparable in quality. It also can drastically reduce 
mesh bandwidth by allowing meshes to be reﬁned once inside the shader units instead 
of down-sampling very complex ones from memory. Some algorithms can up-sample 
any arbitrary mesh, while others allow for “hinting” in meshes to dictate the most 
characteristic vertices and edges. Tessellation shaders were introduced in OpenGL 
4.0 and Direct3D 11. 
One cannot use tessellation to implement subdivision schemes that require the 
previous vertex position to compute the next vertex positions. 
Texel—Acronym for TEXture ELement or TEXture pixEL—the unit of data that 
makes up each individually addressable part of a texture. A texel is the texture 
equivalent of a pixel. 
Texture Mapping—The act of applying a texture to a surface during the rendering 
process. In simple texture mapping, a single texture is used for the entire surface, 
no matter how visually close or distant the surface is from the viewer. A somewhat

Appendix B: Deﬁnitions
321
more visually appealing form of texture mapping involves using a single texture 
with bilinear ﬁltering, while an even more advanced form of texture mapping uses 
multiple textures of the same image but with different levels of detail, also known 
as mip mapping. See also “Bilinear Filtering,” “Level of Detail,” “Mip-map,” “Mip 
mapping,” and “Trilinear Filtering.” 
Texture Map—Same thing as “Texture.” 
Texture—A texture is a special bitmap image, much like a pattern, but which is 
intended to be applied to a 3D surface in order to quickly and efﬁciently create a 
realistic rendering of a 3D image without having to simulate the contents of the image 
in 3D space. That sounds complicated, but in fact it’s very simple. For example, if 
you have a sphere (a 3D circle) and want to make it look like the planet Earth, you 
have two options. The ﬁrst is that you meticulously plot each nuance in the land 
and sea onto the surface of the sphere. The second option is that you take a picture 
of the Earth as seen from space, use it as a texture, and apply it to the surface of 
the sphere. While the ﬁrst option could take days or months to get right, the second 
option can be nearly instantaneous. In fact, texture mapping is used broadly in all 
sorts of real-time 3D programs and their subsequent renderings, because of its speed 
and efﬁciency. 3D games are certainly among the biggest beneﬁciaries of textures, 
but other 3D applications, such as simulators, virtual reality, and even design tools 
take advantage of textures too. 
Tile-Based Deferred Rendering (TBDR)—Defers the lighting calculations until all 
objects have been rendered, and then it shades the whole visible scene in one pass. 
This is done by rendering information about each object to a set of render targets 
that contain data about the surface of the object this set of render targets is normally 
called the G-buffer. 
Tiled Rendering—The process of subdividing a computer graphics image by a 
regular grid in optical space and rendering each section of the grid, or tile, separately. 
The advantage to this design is that the amount of memory and bandwidth is reduced 
compared to immediate mode rendering systems that draw the entire frame at once. 
This has made tile rendering systems particularly common for low-power handheld 
device use. Tiled rendering is sometimes known as a “sort middle” architecture 
because it performs the sorting of the geometry in the middle of the graphics pipeline 
instead of near the end. 
ToF—An acronym for Time of Flight. Used to refer to active sensors which measure 
distance to objects in a scene by emitting infra-red pulses and measuring the time 
taken to detect the reﬂection. These sensors simplify the computational task of 
producing a point cloud from image data but are more expensive and lower resolution 
than regular CMOS sensors. 
Tone Mapping—A technique used in image processing and computer graphics to 
map one set of colors to another to approximate the appearance of high-dynamic-
range images in a medium that has a more limited dynamic range.

322
Appendix B: Deﬁnitions
Transcoding—Transcoding is the process of converting a media ﬁle or object from 
one format to another. Transcoding is often used to convert video formats (i.e., Beta to 
VHS, VHS to QuickTime, QuickTime to MPEG). But it is also used to ﬁt HTML ﬁles 
and graphics ﬁles to the unique constraints of mobile devices and other Web-enabled 
products. 
Trilinear Filtering—A combination of bilinear ﬁltering and mipmapping enhances 
the quality of texture-mapped surfaces. For each surface that is rendered, the two 
mip-maps closest to the desired level of detail will be used to compute pixel colors 
that are the most realistic by bilinearly sampling each mip-map and then using a 
weighted average between the two results to produce the rendered pixel. 
Trilinear Mipmapping—See above, trilinear ﬁltering. 
Transistors—The number of transistors in the GPU or chip. 
Truncation—An arithmetic operation simply removes the fractional portion of a 
number in integer-fraction format to produce an integer, without regard for the magni-
tude of the fractional portion. Therefore, 2.99 and 2.01 truncated are both 2. See also 
“Rounding.” 
UDIM—An enhancement to the UV mapping and texturing workﬂow that makes 
UV map generation easier and assigning textures simpler. The term UDIM comes 
from U-Dimension and design UV ranges. UDIM is an automatic UV offset system 
that assigns an image onto a speciﬁc UV tile, which allows one to use multiple lower 
resolution texture maps for neighboring surfaces, producing a higher resolution result 
without having to resort to using a single ultra-high-resolution image. UDIM was 
invented by Richard Addison-Wood and came from Weta Digital (circa 2002). 
UI—User interface. 
UHD Alliance Premium Logo—High-end HDR TV requirements Rec.709, P3 or 
Rec.2020. 
Ultra HD Blu-ray—HDR disc format using HEVC, HDR10, and optionally Dolby 
Vision. 
UMA (Uniﬁed Memory Architecture)—When an IGP is employed in a PC, it 
needs memory (sometimes called a frame buffer). One of the beneﬁts of an IGP 
is the reduced cost realized by eliminating a separate frame buffer, and to replace 
that extra memory a portion of the PC’s main or system memory is used for the 
frame buffer. When that it is done the organization is known as a uniﬁed memory 
architecture. 
USB, Universal Serial Bus—The Universal Serial Bus (USB) is a common inter-
face that enables communication between devices and a host controller such as a 
personal computer (PC). It connects peripheral devices such as digital cameras, mice, 
keyboards, printers, scanners, media devices, external hard drives and ﬂash drives. 
VAR—Value-added reseller.

Appendix B: Deﬁnitions
323
Vblank—In a raster graphics display, the vertical blanking interval (VBI), also 
known as the vertical interval or VBLANK, is the time between the end of the ﬁnal 
line of a frame or ﬁeld and the beginning of the ﬁrst line of the next frame. It is 
present in analog television, VGA, DVI and other signals. 
Vector—In computer programming, a vector quantity refers to any group of similar 
values which are grouped together and processed as a unit, either serially or in 
parallel. A vector can contain any number of elements. An example from computer 
graphics is the vector which describes the location of a point in four-dimensional 
space. P = x,y,z,w. Commonly referred to as Vec4 as it has four elements, it can be 
efﬁciently processed by a four-wide SIMD Vector Unit. 
Vector Error Correction Model (VECM)—The basic ECM approach as described 
above suffers from a number of weaknesses. Namely, it is restricted to only a single 
equation with one variable designated as the dependent variable, explained by another 
variable that is assumed to be weakly exogeneous for the parameters of interest. It 
also relies on pretesting the time series to ﬁnd out whether variables are I(0) or I(1). 
These weaknesses can be addressed through the use of Johansen’s procedure. Its 
advantages include that pretesting is not necessary, there can be numerous cointe-
grating relationships, all variables are treated as endogenous and tests relating to 
the long-run parameters are possible. The resulting model is known as a vector error 
correction model (VECM), as it adds error correction features to a multi-factor model 
known as vector auto-regression (VAR). 
Vector Display/Scope—A display used for computer graphics up through the 1970s. 
A type of CRT, like an oscilloscope. In a vector display, the image is composed of 
drawn lines rather than an array of pixels as in raster graphics. The CRT’s electron 
beam draws lines along an arbitrary path between two points, rather than following 
the same horizontal raster path for all images. Vector displays had no aliasing and 
were so accurate physical measurements could be taken from the screen. For that 
reason, they were also called calligraphic displays. 
Vector Graphics—Refers to a method of generating electronic images using math-
ematical formulae to calculate the start, end, and path of a line. Images of varying 
complexity can be produced by combining lines into curved and polygonal shapes, 
resulting in inﬁnitely scalable objects with no loss of deﬁnition. 
Vector Unit (SIMD Vector Unit)—An Arithmetic Unit or Arithmetic Logic Unit 
operates on one or more vectors at a time, using the same instruction for all values 
in the vector. 
Verilog/HDL—A “Hardware Description Language” is a textual representation of 
logic gates and registers. It differs from a programming language mainly in that 
it describes a parallel structure in space rather than a sequence of actions in time. 
Verilog is one of the most popular HDLs and resembles C or C + +  in its syntax.

324
Appendix B: Deﬁnitions
VESA—Video Electronics Standards Association, a technical standards organization 
for computer display, PC, workstation and computing environments standards. The 
organization incorporated in California July 1989. 
VFX—Visual effects. 
VGA (Video Graphics Array)—VGA is a resolution and electrical interface stan-
dard original developed by IBM It was the defector display standard for the PC. VGA 
has three analog signals, red blue, and green (RGB) and uses an analog monitor. 
Graphics AIBs output analog signals. All CRTs and most ﬂat panel monitors accept 
VGA signals, although ﬂat panels may also have a DVI interface for display adapters 
that output digital signals. 
vGPU—An AIB with a powerful dGPU located remotely in the cloud or a campus 
server. 
Vignetting—A reduction of an image’s brightness or saturation at the periphery 
compared to the image center. 
VPNA—See Visual Processing Unit. 
Virtual Reality—Virtual Reality (VR) is a fully immersive user environment 
affecting or altering the sensory input(s) (e.g., sight, sound, touch, and smell) and 
allowing interaction with those sensory inputs by the user’s engagement with the 
virtual world. Typically, but not exclusively, the interaction is via a head-mounted 
display, use of spatial or other audio, and/or hand controllers (with or without tactile 
input or feedback). 
VR Video and VR Images—VR Video and VR Images are still or moving imagery 
specially formatted as separate left and right eye images usually intended for display 
in a VR headset. VR Video capture and subsequent display are not exclusive to 360° 
formats and may also include content formatted to 180° or 270°; content does not 
need to visually surround a user to deliver a sense of depth and presence. 
Vision Processing—Processing of still or moving images with the objective of 
extracting semantic or other information. 
VLIW (Very Long Instruction Word)—Often abbreviated to VLIW. A micropro-
cessor instruction combines multiples of the lowest level of instruction words and 
presents them simultaneously to control multiple execution units in parallel. 
Voxel—A voxel is a value in three-dimensional space. Voxel is a combination of 
“volume” and “pixel” where pixel is a combination of “picture” and “element.” This 
is analogous to a texel, which represents 2D image data in a bitmap (also referred 
to as a pixmap). Voxels are used in the visualization and analysis of medical and 
scientiﬁc data. (Some volumetric displays use voxels to describe their resolution. 
For example, a display might be able to show 512 × 512 × 512 voxels.) Both ray 
tracing and ray-casting, as well as rasterization, can be applied to voxel data to obtain 
2D raster graphics to depict on a monitor.

Appendix B: Deﬁnitions
325
VGPR (Vector General-Purpose Registers) 
VPU (Vector Processing Unit)—A vector processor or array processor implements 
an instruction set containing instructions that operate on one-dimensional arrays of 
data called vectors. 
Today’s CPUs architectures have instructions for a form of vector processing 
on multiple (vectorized) data sets, typically known as SIMD (Single Instruction, 
Multiple Data). Common examples include Intel × 86’s MMX, SSE and AVX 
instructions, AMD’s 3DNow! Extensions, as well as Arm’s Neon and its scalable 
vector extension (SVE). 
VPU (Visual Processing Unit)—A Visual Processing Unit (VPU) is a silicon chip 
or IP block dedicated to Computational Photography and/or Vision Processing. 
A vision processing unit (VPU) is an emerging class of microprocessor; it is a 
speciﬁc type of AI accelerator, designed to accelerate machine vision tasks. Vision 
processing units are distinct from video processing units (which are specialized 
for video encoding and decoding) in their suitability for running machine vision 
algorithms such as CNN (convolutional neural networks). The name belies the real 
importance of the function and should include neural network accelerator, which 
results in the acronym VPNA. 
VR—Virtual reality. 
V-sync—Vertical synchronization of the monitor’s refresh rate based on the power 
line frequency, 60 or 50 Hz. 
VXGI—Is a new approach to computing a fast, approximate form of global illumi-
nation (GI) dynamically in real-time on the GPU. This new GI technology uses a 
voxel grid to store scene and lighting information, and a novel voxel cone tracing 
process to gather indirect lighting from the voxel grid. The purpose for VXGI is to 
run in real-time and doing full ray tracing of the scene is too computationally intense, 
so approximations are required. 
VXGI—Voxel Global Illumination (VXGI), developed by Nvidia, features one-
bounce indirect diffuse, specular light, reﬂections, and area lights. It is an advance-
ment in realistic lighting, shading and reﬂections. VGXI is a three-step process, 
voxelization, Light injection, and ﬁnal gathering, and is employed in next-generation 
games and game engines. 
WCG—Wide Color Gamut—anything wider than Rec.709, DCI P3, Rec.2020 
See wide color gamut 
Wave— 
Wide Color Gamut—High Dynamic Range (HDR) displays a greater difference in 
light intensity from white to black, Wide Color Gamut (WGC) provides a greater 
range of colors. The wide-gamut RGB color space (or Adobe Wide Gamut RGB) 
is an RGB color space developed by Adobe Systems, that offers a large gamut by

326
Appendix B: Deﬁnitions
using pure spectral primary colors. It is able to store a wider range of color values 
than sRGB or Adobe RGB color spaces. 
Also see HDR, and Color gamut. 
X Reality—X Reality (XR) is a general term to cover the multiple types of expe-
riences and technologies across VR, AR, MR and any future similar areas. All of 
these systems have in common some level of display technology (e.g., video, audio) 
mixed with a method to track where the user is looking or moving (e.g., up/down, 
side-to-side, turning around). How those systems work individually, and together, 
determines which of the more deﬁned experiences the product would be named – VR, 
AR, MR, or some future XR. 
Z-buffer—A memory buffer used by the GPU that holds the depth of each pixel 
(Z axis.) When an image is drawn, each (X-Y) pixel is matched against the z buffer 
location. If the next pixel in line to be drawn is below the one that is already there, 
it is ignored.

Index 
A 
A-B-C view, 33 
Accelerated Graphics Port (AGP), 170, 172 
Accelerated Processor Unit (APU), 59 
Adaptive-Sync, 192, 194 
Add-In-Board (AIB), 98 
AGP texturing, 172 
Akeley, Kurt, 215 
Algorithms, 34 
Alpha channel, 25 
Alternative Frame Rendering (AFR), 196 
Ambient Occlusion (AO), 252 
Amdahl, Gene, 74 
Amdahl’s law, 74 
AMD, Navi 21, 56 
AMD Radeon RX 5000, 224 
AMD Radeon RX 6000, 224 
Ampliﬁcation shader, 235, 241 
Angstroms, 153 
Anisotropic Filtering (AF), 41, 227 
Anti-aliasing, 50 
API & OS, 209 
Application processor, 25 
Application Programming Interface (API), 
201 
Approximations, 34 
Archimedes, 87 
Arch Linux, 255 
ArtX, 137 
Assembly language, 228 
Association for Computing Machinery 
(ACM), 203 
ATI Charisma engine, 121 
ATI HyperZ, 121 
ATI Pixel Tapestry, 125 
Augmented reality, 184 
B 
Bachus, Kevin, 6 
Baldwin, David, 70 
Basis Universal, 263 
Basis, Universal compression technology, 
264 
Bergman, Rick, 58, 118 
Bidirectional Reﬂectance Distribution 
Function (BRDF), 31 
Bidirectional subdivision, 80 
Bilinear ﬁltering, 41 
Binning, 23 
Blinn, Jim, 7, 40, 263 
Bounding Volume Hierarchy (BVH), 85 
Boyd, Chas, 6 
Bump mapping, 40, 44, 227, 235 
Burk, Brian, 269 
Burns, Louis, 172 
C 
Calle, Rick, 139 
Carmack, John, 274 
Carpenter, Loren, 25, 69 
Catmull-Clark subdivision, 80 
Catmull, Ed, 8, 25, 69, 263 
CGI pipeline, 20 
Chambers, Tim, 130 
Chen, Wen-Chi, 118 
Chiplets, 161 
Cirrus Logic, 10 
Clark, Jim, 7, 70, 226 
Coarse Pixel Shading (CPS), 89 
Color mixing, 178 
Compute shaders, 79, 237
© The Editor(s) (if applicable) and The Author(s), under exclusive license 
to Springer Nature Switzerland AG 2022 
J. Peddie, The History of the GPU – Eras and Environment, 
https://doi.org/10.1007/978-3-031-13581-1 
327

328
Index
Compute Uniﬁed Device Architecture 
(CUDA), 75 
Convolutional autoencoder, 88 
Cook, Robert, 40, 68 
Core, 203 
Core Graphics framework, 207 
CrossFire, 196 
Cross-lane wave intrinsics, 92 
Crossley, Paul, 13 
CRT Controller (CRTC), 21 
Cryptocurrency Mining Processor (CMP), 
58 
Cube Environment Mapping (CEM), 8 
Cube mapping, 40 
Culling, 227, 240 
D 
Deep Learning Neural Network (DLNN), 
88 
Deep Learning Super Sampling (DLSS), 
87, 88 
Depth of ﬁeld, 268 
Diamond Multimedia, 116 
Diffuse mapping, 39 
Digital TV (DTV), 131, 144 
Digital Video Disc or Digital Versatile Disc 
(DVD), 58 
Digital Visual Interface (DVI), 57, 179 
Diminishing returns, law of, 74 
Direct Drive Monitor (DDM), 193 
Direct3D 10, 78 
Direct3D 11, 80 
DirectX 7.0, 105 
DirectX 8, 227 
DirectX 9, 228 
DirectX 9.0c, 232 
DirectX 10, 228, 234 
DirectX 12, 223 
DirectX 12 Ultimate, 224 
DirectX (DX), 31 
DirectX Raytracing (DXR), 86 
DirectX Texture Compression (DXTC), 133 
Discrete GPU (dGPU), 25, 99 
Displacement mapping, 40 
DisplayPort, 57, 98, 182 
DLSS, Nvidia, 253 
Domain shader, 83, 231 
Double Data Rate Random Access Memory 
(DDR RAM), 25 
Downsampling, 54 
DXR 1.1, 224 
DXT texture compression, 39 
Dynamic DOF (Depth of Field), 278 
Dynamic global illumination, 244 
Dynamic Random Access Memory 
(DRAM), 25, 156 
Dynamic Super-Resolution (DSR), 53 
E 
EISA, 168 
Eldridge, Mathew, 37 
Electronic Delay Storage Automatic 
Calculator (EDSAC), 201 
Embedded DRAM (eDRAM), 114 
Embossed bump mapping, 125 
Environment-Mapped Bump Mapping 
(EMBM), 125 
Epic Games, 243, 244 
Epyc CPU, AMD, 161 
Euclid, 87 
Execution Unit (EU), 75 
Extended Data Out (EDO), 25, 156 
Extended Direct Memory Access (XDMA), 
197 
Extended Display Identiﬁcation Data 
(EDID), 193 
F 
Fahrenheit, 216 
Fahrenheit Scene Graph, 217 
Fair, David, 6 
Fast Page Mode DRAM (FPM), 156 
Fermi, Nvidia, 82 
FidelityFX Super-Resolution (FSR), 53, 54, 
257 
Field of View (FOV), 27 
First 3D chip, 2 
First-generation GPUs, 107, 121 
First GPU, 2 
First-In, First-Out (FIFO), 38 
5K, 57 
Fixed Function Pipeline (FFP), 72 
Flat shading, 43 
Floating-Point Processor (FPP), 29, 66 
Floating-Point Unit (FPU), 26, 66 
Fong, Dennis, 274 
Foveated rendering, 183 
Fragment shader, 21, 73 
Frame buffer, 23 
Free-PC, 138 
FreeSync, 192, 193 
Fujitsu Pinolite, 1 
Full-Scene Anti-Aliasing (FSAA), 132 
Futuremark, 242

Index
329
G 
GameCube, 58 
Game launchers, 273 
Gamma, Glint processor, 2 
GeForce 256, 3, 70 
Gelsinger, Pat, 5, 6, 154, 170 
General-Purpose Graphics Processor Unit 
(GPGPU), 79 
Geometric model, 20 
Geometry engine, 7, 27 
Geometry engine, SGI, 1 
Geometry generation, 20 
Geometry processor, 56, 70 
Geometry Processor Unit (GPU), 2, 25, 44, 
56 
Geometry shader, 78, 232, 234 
Gigi, 3Dlabs, 1 
Gilcher, Pascal, 278 
GKS, 203 
Glint, 70 
GPU-compute, 79 
GPUOpen, 259 
Graphics Core Next, 233 
Graphics DDR (GDDR), 25, 98, 107, 156 
Graphics Drawing Interface (GDI), 142 
Graphics Processing Unit, 56 
Graphics shader, 105 
Green, Ned, 8 
G-Sync, 188, 194 
Gustafson, John L, 74 
Gustafson’s Law, 74 
H 
Hara, Michael, 13 
Hardware Feature Levels, DirectX, 214 
Hardware Motion Compensation (HWMC), 
142 
Hartog, Adrian, 81, 140 
Hase, Ted, 6 
Head-Mounted Displays (HMDs), 183 
Head-Up Display (HUD), 52 
Hidden Surface Removal (HSR), 46 
High bandwidth Digital Content Protection 
(HDCP), 58 
High Bandwidth Memory (HBM), 25, 107, 
156 
High-Deﬁnition Ambient Occlusion 
(HDAO), 252 
High Deﬁnition (HD), 53, 183 
High-Deﬁnition Multimedia Interface 
(HDMI), 57, 98, 179 
High Dynamic-Range (HDR), 23, 32, 183 
High-Level Shading Language (HLSL), 65, 
218, 228, 262 
High-Performance Computing (HPC), 79 
Ho, K.Y., 140 
Horizon-Based Ambient Occlusion 
(HBAO), 252 
Huang, Jen-Hsun, 13 
Huang, Jen-Hsun (Jensen), 9 
Hull shader, 83, 231 
I 
IGC with T&L, 142 
Image generation pipeline, 20 
Image Quality (IQ), 38 
Immediate Mode Rendering (IMR), 45 
Index buffer, 239 
Indexed triangle list, 240 
Industry Standard Architecture (ISA), 167 
Installable Client Driver (ICD), 142 
Instances, 238 
Instancing, 50 
Integrated GPU, 25 
Integrated Graphics Controller (IGC), 106, 
135, 142 
Integrated Graphics Processor (IGP), 99, 
106, 135 
Intel Developer Forum (IDF), 5 
Intel i860, 5 
Intel Xe, 161 
IOS, 207 
IRIS-GL, 203, 215 
K 
Kaby Lake G, 161 
Kaplanyan, Anton, 257 
Keodjian, Servan, 211 
Keyframe morphing, 124 
Khronos, 36, 207, 220, 263 
King, Mike, 212 
Kirk, David, 6 
Kishino, Fumio, 185 
Koduri, Raja, 72 
L 
Lender, Kurt, 165 
Lens-Matched Shading (LMS), 90 
Level of Detail (LOD), 38, 241, 245 
Light-source shaders, 69 
Low Framerate Compensation (LFC), 193 
Lucas, George, 228 
Lumen, 244 
Lumen in the Land of Nanite, 244

330
Index
M 
Maher, Kathleen, 142 
Main memory, 25 
Mantle, 218 
Mantor, Mike, 74, 233 
Math-coprocessor, 27 
McLaren, Stuart, 131 
McQuillan, Bob, 116 
Meshlets, 235, 237, 238 
Mesh shader, 44, 92, 105, 235 
Metal, 207, 219 
Metal, close to, 217 
Metal Oxide Semiconductor (MOS), 24 
Micro Channel Architecture (MCA), 168 
Milgram, Paul, 185 
Miller, Joan, 24 
Milliseconds (ms), 34 
Miner GPU (mGPU), 58 
Mipmapping, 38 
Mipmaps, 38 
Mixed reality, 185 
Model construction, 20 
Molnar, Steve, 37 
Monte Carlo, 32 
Moore’s law, 34 
Moore, Gordon, 25 
Multi-Chip Modules (MCM), 161 
Multi-displays, 57 
Multi-Media Extensions (MMX), 5 
Multiple Application Graphics Integrated 
Circuits (MAGIC), 29 
Multi-Resolution Shading (MRS), 90 
Multi-texturing, 40 
Multithreaded Processor (MP), 76 
Murphy, Nick, 70 
N 
Naomi, 130 
Newell, Martin, 7, 25 
Newtown, 87 
Nintendo 64 Reality Coprocessor, 135 
Non-linear line, 34 
Nordlund, Petri, 109 
Normal mapping, 40 
NV10, 2 
Nvidia, 70 
Nvidia, Ampere, 56 
Nvidia GigaTexel Shader, 128 
Nvidia GPU Technology Conference 
(GTC), 84 
Nvidia RTX 3000 series, 224 
Nvidia Shading Rasterizer, 128 
Nvidia Ti products, 23 
Nvidia vendor extension, 236 
NVlink, 198 
NXT, 222 
O 
Open Compute Project Foundation (OCP), 
161 
Open Domain-Speciﬁc Architecture 
(ODSA), 161 
OpenGL, 36, 203, 215, 228 
OpenGL 3.2, 78 
OpenGL 3.3, 232 
OpenGL ES, 207 
OpenGL Shading Language (GLSL), 262 
Operating System (OS), 57 
Orton, Dave, 136, 140 
Over-The-Top (OTT) bus, 196 
P 
Panel Self-Refresh (PSR), 193 
Pappas, Jim, 164, 170, 174 
Parallel processor, 56 
PCIe, 172 
Penwarden, Nick, 245 
Percentage-Closer Soft Shadows (PCSS), 
267 
Peripheral Component Interconnect (PCI), 
170 
Personal Gaming Console (PGC), 179 
Perspective view, 33 
PHIGS, 203 
Photorealistic image, 56 
Pinolite, Fujitsu, 30 
Pipeline, 21 
Pixar, 68 
Pixel ﬁll, 45 
Pixel shader, 21, 73 
Pixels Per Inch (PPI), 56 
Plot 10, 203 
Post, 20 
Potapov, Alexander, 258 
Potashner, Ken, 116 
Poulton, John, 63 
Priem, Curtis, 9 
Prieto, Martín Gamero, 198 
Primitive index buffer, 239 
Primitive shaders, 92, 233 
Priority buffer, 126 
Q 
Quartz 2D, 207 
QuickPath, Intel, 146

Index
331
R 
Rabson, Doub, 211 
Radeon, 70, 121 
Radeon 100, 121 
Radeon 7000, 121 
RAMBUS, 156 
Raptr, 274 
Rasterizing, 20 
Raster Operation Pipeline (ROP), 121 
Ray marching, 278 
RayQuery, 85 
Ray Tracing (RT), 49, 51, 56, 87, 224 
RDEAM, 156 
RDNA 2, 236 
Reality Coprocessor GPU, Nintendo 64, 
226 
Real-Time Ray Tracing (RTRT), 87 
Red, Green, Blue, and Alpha (RGBA), 25 
Reﬂection techniques, 44 
Render Everything You Ever Saw (Reyes), 
69, 228 
Rendering equation, The, 31 
Rendering pipeline, 45 
Rendering stage, 50 
RenderMan, 27, 68, 69, 228 
Resizable Base Address Register (BAR), 26 
Resolution, 56 
Reyes, Bill, 69 
Richter, Jake, 212 
Rigging, 20 
Riva 128, 9 
Role-Playing Game (RPG), 98 
Russell, Stanford, 4 
Rust, 222 
Ryzen Threadripper, AMD, 161 
S 
S3, 116 
S3 Chrome, 119 
S3 Savage 3D, 126 
S3 Texture Compression (S3TC), 39, 126 
Same-instruction, multiple data, 105 
Scan conversion, 51 
Scan-Line Interlace (SLI), 194 
Scan-line rendering, 45 
Scansen, Don, 161 
Screen Space Ambient Occlusion (SSAO), 
252, 278 
Screen Space Ray Traced Global 
Illumination (SSRTGI), 277 
Seekings, Kate, 211 
Segal, Mark, 215 
Sega Model 2, 135 
Set-Top Box (STB), 131 
SGS Microelectronics, 129 
Shader, 20, 56, 66, 68, 99 
Shader evaluation pipeline, 86 
Shader model, 72 
Shading, 20, 44 
Shadow maps, 241 
Sharma, Debendra Das, 174 
Shoup, Richard, 24 
Silicon Integrated Systems (SiS), 142 
Single Instruction, Multiple Data (SIMD), 
227 
Single-Instruction Multiple-Thread 
(SIMT), 79 
SLI, hybrid, 196 
Smart-glasses, 184 
SonicBlue, 118 
Special function units, 76 
SPIR-V, 263 
Split Frame Rendering (SFR), 196 
Standard, Portable, Intermediate 
Representation, SPIR-V, 220 
Static RAM (SRAM), 115, 156 
STMicroelectronics, 129 
Stream compaction, 79 
Streaming multiprocessor (SM), 76 
Streaming Processor (SP), 75 
Stylistic, 49 
Sub-D, 80 
Subdivision, 80 
Sub-surface scattering, 35 
Supersampling, 90 
Super Sampling Anti-Aliasing (SSAA), 51, 
132 
Surface shaders, 69 
Suspension of disbelief, 34 
Symbian, 207 
Synchronous Graphics RAM (SGRAM), 
156 
SYstem-level OpenCL (SYCL), 261 
System on a Chip (SoC), 25 
T 
Task shaders, 92 
Tektronix, 203 
Telepresence, 185 
Temporally amortized, 259 
TeraScale, AMD, 82 
Tessellation, 80 
Tessellation shader, 80, 232 
Texel, 38 
Texture compression, 227, 263 
Texture mapping, 39

332
Index
Texture Mapping Unit (TMU), 56, 77, 196 
Thomson Semiconductors, 129 
3DMark, 242 
3DNow!, 5 
3D shader, 70 
3D textures, 124 
Tile-Based Deferred Rendering (TBDR), 46 
Tile-based rendering, 47, 51 
Timing Controller (TCON), 191 
Transform and Lighting (T&L), 2, 70, 105, 
226 
Transformation engines, 72 
Transformations, 20 
Triangle fans, 231 
Trident Microsystems, 144 
Trilinear ﬁltering, 227 
TruForm, ATI, 81 
Tuomi, Kai, 109 
Tuomi, Mika, 109 
Turing, 236 
Turing architecture, 237 
Turing, Nvidia, 44, 235 
Turning, 225 
Tweening, 124 
U 
Ultra Fusion, 163 
Ultra High Deﬁnition (UHD), 183 
Underwriter Labs, 242 
Uniﬁed Memory Architecture (UMA), 99 
United Microelectronics Corp (UMC), 142 
Unreal Engine, 243 
User Interface (UI), 52 
Utah teapot, 7 
u, v, 40 
V 
Variable Rate Shading (VRS), 89, 225 
Variable Refresh Rate (VRR), 193 
Vega microarchitecture, 233 
Vertex blending, 124 
Vertex index buffer, 239 
Vertex shader, 2, 27, 72, 105, 227 
Vertex skinning, 123 
Very Large-Scale Integration (VLSI), 70 
Video Electronics Standards Association 
(VESA), 182 
Video Graphics Array (VGA), 57, 179 
Video RAM (VRAM), 25, 156 
Viewing frustrum, 28 
Viewport, 226 
Viewport Culling (VPC), 233 
Virtual presence, 185 
Virtual Reality (VR), 52, 183 
Visibility buffer, 241 
Visual Processing Unit (VPU), 2, 57 
Vivoli, Dan, 4 
VL bus, 169 
Volari, 144 
Volta, Nvidia, 84 
Volume shaders, 70 
Vulkan, 207, 220, 228, 236 
W 
W3C, 222 
Wang, Cher, 120 
Warp, 79 
WebGL, 222 
WebGPU (WGPU), 222 
Wilson, Malcom, 70 
Windows 7, 228 
Windows 11, 228 
Windows Graphics Foundation (WGF), 50 
Windows RAM (WRAM), 156 
Wing, Trevor, 132 
Wire-frame, 32 
Wolfe, Andy, 13 
Wu, Dr. Chin, 138 
X 
Xabre Graphics (XGI), 143 
Xbox 360, 232 
XDMA, AMD, 197 
XDR DRAM 6, 156 
Xe Supersampling (XeSS), 256, 259 
Y 
Yassaie, Hossein, 129 
Yen, Dr. Wei, 136, 141 
Z 
ZBrush, 244

