Understanding plasticity in neural networks
Clare Lyle 1 Zeyu Zheng 1 Evgenii Nikishin 1
Bernardo Avila Pires 1 Razvan Pascanu 1 Will Dabney 1
Abstract
Plasticity, the ability of a neural network to
quickly change its predictions in response to new
information, is essential for the adaptability and
robustness of deep reinforcement learning sys-
tems. Deep neural networks are known to lose
plasticity over the course of training even in rel-
atively simple learning problems, but the mech-
anisms driving this phenomenon are still poorly
understood. This paper conducts a systematic
empirical analysis into plasticity loss, with the
goal of understanding the phenomenon mechanis-
tically in order to guide the future development
of targeted solutions. We ﬁnd that loss of plas-
ticity is deeply connected to changes in the cur-
vature of the loss landscape, but that it typically
occurs in the absence of saturated units or diver-
gent gradient norms. Based on this insight, we
identify a number of parameterization and opti-
mization design choices which enable networks
to better preserve plasticity over the course of
training. We validate the utility of these ﬁndings
in larger-scale learning problems by applying the
best-performing intervention, layer normalization,
to a deep RL agent trained on the Arcade Learning
Environment.
1. Introduction
It is a widely observed phenomenon that neural networks
trained to ﬁt a series of different learning objectives exhibit
a reduced ability to solve new tasks (Lyle et al., 2021; Nik-
ishin et al., 2022; Dohare et al., 2021). The loss of plasticity
occurs most robustly when the relationship between inputs
and prediction targets changes over time, and the network
must learn to ‘overwrite’ its prior predictions (Lyle et al.,
2021). While such scenarios are relatively rare in supervised
learning, they are baked into the way that deep reinforce-
ment learning (RL) agents are trained. Understanding how
1DeepMind.
Correspondence to:
Clare Lyle <clare-
lyle@deepmind.com>.
plasticity is lost, and whether this loss can be mitigated, is
crucial if we wish to develop deep RL agents which can
continually learn to solve complex tasks. Existing methods
to promote trainability act on a wide variety of potential
mechanisms by which plasticity might be lost, including
resetting of layers (Nikishin et al., 2022) and activation units
(Dohare et al., 2021), and regularization of the features (Ku-
mar et al., 2020; Lyle et al., 2021). While all of these works
observe performance improvements, it is unlikely that they
are all obtaining these improvements by the same mecha-
nism. As a result, it is difﬁcult to know how to improve on
these interventions to further preserve plasticity.
This paper seeks to identify the mechanisms by which plas-
ticity loss occurs. We begin with an analysis of two inter-
pretable case studies, illustrating the mechanisms by which
both adaptive optimizers and naive gradient descent can
drive the loss of plasticity. Prior works have conjectured,
implicitly or explicitly, that a variety of network properties
might cause plasticity loss: we present a falsiﬁcation frame-
work inspired by the study of causally robust predictors for
generalization (Dziugaite et al., 2020), and leverage this
framework to show that loss of plasticity cannot be uniquely
attributed to any of these properties. While difﬁcult to char-
acterize explicitly, we provide evidence that the curvature of
the loss landscape induced by new tasks on trained parame-
ters is a crucial factor determining a network’s plasticity.
We conclude by completing a broad empirical analysis of
methods which aim to improve the ability of a network
to navigate its optimization landscape throughout training,
ranging from architectural choices to regularization and nor-
malization schemes. We ﬁnd that architectures which have
been conjectured to smooth out the loss landscape, such as
those that use categorical encodings and layer normalization,
provide the greatest improvements to plasticity, while meth-
ods which perturb the parameters or provide other forms of
regularization tend to see less beneﬁt. To test the generality
of these ﬁndings, we apply the best-performing interven-
tion, layer normalization, to a standard DQN architecture
and obtain signiﬁcant improvements in performance on the
Arcade Learning Environment benchmark. We conclude
that controlling the loss landscape sharpness and optimizer
stability present highly promising avenues to improve the
robustness and usability of deep RL methods.
arXiv:2303.01486v1  [cs.LG]  2 Mar 2023

Understanding plasticity in neural networks
2. Background
It has long been observed that training a network ﬁrst on
one task and then a second will result in reduced perfor-
mance on the ﬁrst task (French, 1999). This phenomenon,
known as catastrophic forgetting, has been widely studied
by many works. This paper concerns itself with a different
phenomenon: in certain situations, training a neural network
on a series of distinct tasks can result in worse performance
on later tasks than what would be obtained by training a
randomly initialized network of the same architecture.
2.1. Preliminaries
Temporal difference learning. This work will study the
loss of plasticity under non-stationary learning problems,
with a particular focus on temporal difference (TD) learning
with neural networks. We assume the standard reinforce-
ment learning setting of an agent interacting with an environ-
ment M, with observation space S, action space A, reward
R and discount factor γ. Networks trained via temporal dif-
ference learning receive as input sampled transitions from
an agent’s interaction with the environment, of the form
τt = (st−1, at, rt, st), where st−1, st ∈S, at ∈A, and
rt = R(st) some reward. The network f : Θ×S ×A →R
is trained to minimize the TD loss
ℓ(θ, τt) =

f(θ, st−1, at) −□(rt + γf(θ, st, a′))
2
(1)
where □denotes a stop-gradient, γ < 1 is some discount
factor, and the choice of a′ is determined by the vari-
ant of TD learning used. Crucially, the regression target
rt + γf(θ, st, a′) depends on the network’s parameters θ
and changes as learning progresses, and so we obtain a natu-
ral form of non-stationarity. We will use the shorthand ℓ(θ)
for the expectation of this loss over some input distribution.
Loss landscape analysis. We will be particularly interested
in the study of the structure of the loss landscape traversed
by an optimization algorithm. We will leverage two princi-
pal quantities in this analysis: the Hessian of the network
with respect to some loss function, and the gradient covari-
ance. The Hessian of a network f at parameters θ with
respect to some loss ℓ(θ) is a matrix deﬁned as
Hℓ(θ) = ∇2
θℓ(θ) ∈Rd×d
(2)
where d = |θ| is the number of parameters. Of particular rel-
evance to optimization is the eigenspectrum of the Hessian
Λ(Hℓ(θ)) = (λ1 ≥· · · ≥λd). The maximal eigenvalue,
λ1, can be interpreted as measuring the sharpness of the
loss landscape (Dinh et al., 2017), and the condition number
λ1/λd has signiﬁcant implications for convergence of gra-
dient descent optimization in deep neural networks (Gilmer
et al., 2022).
We will also take interest in the covariance structure of the
gradients of different data points in the input distribution,
a property relevant to both optimization and generalization
(Fort et al., 2019; Lyle et al., 2022). We will estimate this co-
variance structure by sampling k training points x1, . . . , xk,
and computing the matrix Ck ∈Rk×k deﬁned entrywise as
Ck[i, j] = ⟨∇θℓ(θ, xi), ∇θℓ(θ, xj)⟩
∥∇θℓ(θ, xi)∥∥∇θℓ(θ, xj)∥.
(3)
If the off-diagonal entries of Ck contain many negative
values, this indicates interference between inputs, wherein
the network cannot reduce its loss on one region without
increasing its loss on another. If the matrix Ck exhibits
low rank (which, given a suitable ordering σ of the data
points xσ(1), . . . , bxσ(k) will yield a block structure) then
the gradients are degenerate and largely colinear, which
can indicate both signiﬁcant generalization when their dot
product is positive, or interference when their dot product is
negative.
2.2. Deﬁning plasticity
The study of plasticity has concerned neuroscience for sev-
eral decades (Mermillod et al., 2013; Abbott & Nelson,
2000), but has only recently emerged as a topic of interest
in deep learning (Berariu et al., 2021; Ash & Adams, 2020).
Classical notions of complexity from the computational
learning theory literature (Vapnik, 1968; Bartlett & Mendel-
son, 2002) evaluate whether a hypothesis class contains
functions that capture arbitrary patterns, but are agnostic to
the ability of a particular search algorithm, such as gradient
descent, to ﬁnd these functions, presenting a challenge in
their application to practical deep learning systems. For
example, a billion-parameter neural network architecture
might have the capacity to represent a rich class of functions,
but if all of its activation units are saturated then it cannot
be trained by gradient descent to realize this capacity. In
order to reﬂect this intuition, we will use the term plasticity
to refer to a problem-dependent property which captures
the interaction between the network state, optimization pro-
cess, and training data, while capacity will refer to a ﬁxed
property of a network architecture.
This work will take an empirical approach similar to Lyle
et al. (2021) in deﬁning plasticity. Intuitively, our deﬁnition
will measure the ability of a network to update its predictions
in response to a wide array of possible learning signals. We
consider an optimization algorithm O : (θ, ℓ) 7→θ∗which
takes initial parameters θ ∈Θ and some objective function
ℓ: Θ →R, and outputs a new set of parameters θ∗. The pa-
rameters θ∗need not be an optimum: O could, for example,
run gradient descent for ﬁve steps. In order to measure the
ﬂexibility with which a network can update its predictions
under this optimization algorithm, we consider a distribution

Understanding plasticity in neural networks
over a set of loss functions L each deﬁned by some learning
objective. For example, we might consider a distribution
over regression losses ℓf,X(θ) = Ex∼X[(f(θ, x) −g(x))2],
where g is induced by a random initialization of a neural
network. In order to match the intuition that more adaptable
networks should have greater plasticity, we set a baseline
value b to be the loss obtained by some baseline function
(e.g. if ℓis a regression loss on some set of targets, we set b
to be the variance of the targets), and then deﬁne plasticity
to be the difference between the baseline and the expectation
of the ﬁnal loss obtained by this optimization process after
starting from an initial parameter value θt and optimizing a
sampled loss function ℓsubtracted from the baseline b.
P(θt) = b −Eℓ∼L[ℓ(θ∗
t )] where θ∗
t = O(θt, ℓ)
(4)
Concretely, we set X to be the set of transitions gathered by
an RL agent and stored in some replay buffer, and f to be the
neural network architecture. Given some offset a ∈R, we
will apply the transformation g(x) = a + sin(105f(x; θ0))
to construct a challenging prediction objective which mea-
sures the ability of the network to disentangle its input space
into arbitrary groupings. Because the mean prediction out-
put by a deep RL network tends to evolve away from zero
over time as the policy improves and the reward propagates
through the value function, we will set a to be equal to the
network’s mean prediction in order not to bias the objec-
tive in favour of random initializations, which have mean
much closer to zero. The optimizer O will be identical to
that used by the network on its primary learning objective,
and we found running this optimizer for a budget of two
thousand steps enabled reasonably efﬁcient iteration time
while also providing enough opportunity for most random
initializations to solve the task.
Given this framework, we then deﬁne the loss of plasticity
over the course of a trajectory (θt)N
t=0 as the difference
P(θt) −P(θ0). We note that this deﬁnition of plasticity
loss is independent of the value of the baseline b, i.e. the
difﬁculty of the probe task.
3. Two simple studies on plasticity
We begin with some interpretable examples of learning
problems where plasticity loss occurs. These examples
illustrate how the design of optimizers can interact with
nonstationarity to produce instabilities that drive plasticity
loss in one of the examples above, and explore how the
dynamics of gradient-based optimizers might affect more
subtle properties of the loss landscape.
3.1. Optimizer instability and non-stationarity
The robustness of existing optimizers across a wide range
of datasets and network architectures has played a key role
0
15000
30000
45000
60000
75000
90000
0.0
0.2
0.4
0.6
0.8
Fraction of dead units
0.0
0.2
0.4
0.6
0.8
1.0
Accuracy
Optimizer Instability
Figure 1. Abrupt task changes can drive instability in optimizers
which depend on second-order moment estimates for adaptive
learning rate scaling. Setting these estimators to be more robust
to small gradient norms and to update moment estimates more
quickly mitigates this issue.
in the widespread adoption of deep learning methods. For
example, the Adam optimizer (Kingma & Ba, 2015) with
a learning rate of 10−3 will often yield reasonable initial
results on a range of network architectures from which the
practitioner can iterate. However, when the assumptions on
stationarity underlying the design of this optimizer no longer
hold, the optimization process can experience catastrophic
divergence, killing off most of the network’s ReLU units.
We can see an example of this in a simple non-stationary
task in Figure 1. A two-hidden-layer fully-connected neural
network is trained to memorize random labels of MNIST
images (full details provided in Appendix A.1). After a
ﬁxed training budget, the labels are re-randomized, and the
network continues training from its current parameters. This
process quickly leads a default Adam optimizer to diverge,
saturating most of its ReLU units and resulting in trivial
performance on the task that a freshly initialized network
could solve perfectly.
The mechanism of this phenomenon emerges when we con-
sider the update rule for Adam, which tracks a second-order
estimate ˆvt along with a ﬁrst-order moment estimate ˆmt of
the gradient via an exponential moving average
ut = α
ˆmt
√ˆvt + ¯ϵ + ϵ.
(5)
Gradients tend to have small norm when the training loss
has converged, and large norm when the loss is large. When
the loss changes suddenly, as is the case when the perfectly-
memorized MNIST labels are re-randomized (or when the
target network is updated in an RL agent), ˆmt and ˆvt will no
longer be accurate estimates of the current gradient distribu-
tion moments. Under the default hyperparameters for deep
supervised learning, ˆmt is updated more aggressively than
ˆvt, and so the updates immediately after a task change will
scale as a large number divided by a much smaller number,
contributing to the instability we observe in Figure 1. In
this instance, the solution is simple: we simply increase ϵ
and set a more aggressive decay rate for the second-moment

Understanding plasticity in neural networks
0
2
4
Eigenvalue 
0.0
0.1
0.2
0.3
0.4
Density
Initialization
0
10
20
30
40
50
Eigenvalue 
After 5 target updates
Gradient descent
Brownian motion
Loss landscape curvature
Gradient Descent 
 1 Iteration
Brownian Motion 
 1 Iteration
Gradient Descent 
 20 Iterations
Brownian Motion 
 20 Iterations
Gradient Covariance
Figure 2. Evolution of the gradient and Hessian in under gradient-
based optimization compared to random perturbation of the pa-
rameters. Gradient descent induces more gradient interference
between inputs and greater curvature of the loss landscape.
estimate, and the network avoids catastrophic instability.
Intriguingly, a large value of ϵ is frequently used in deep
RL algorithms such as DQN (Mnih et al., 2015) relative
to the default provided by optimization libraries, suggest-
ing that the community has implicitly converged towards
optimizer hyperparameters which promote stability under
nonstationarity.
3.2. Loss landscape evolution under non-stationarity
Even when optimization is sufﬁciently stable to avoid satu-
rated units, prior work still observes reductions in network
plasticity (Lyle et al., 2021). The causes of this phenomenon
are more difﬁcult to tease apart; neural network initializa-
tions have been tuned over several decades to maximize
trainability, and many properties of the network change dur-
ing optimization which could be driving the loss of plasticity.
A natural question we can ask in RL is whether the optimiza-
tion dynamics followed by a network bias the parameters
to become less trainable, or whether the loss of plasticity
is a natural consequence of any perturbation away from a
carefully chosen initialization distribution.
We frame this question as a controlled experiment, in which
we compare the evolution of two coupled updating proce-
dures: one follows gradient-based optimization on a non-
stationary objective (full details in Appendix A.1); the sec-
ond follows a random walk, where we add a Gaussian per-
turbation to the parameters with norm equal to the size of
the update performed by the gradient-based optimizer. Both
trajectories start from the same set of randomly initialized
parameters and apply updates of equal norm; the only differ-
ence is the direction each step takes. We evaluate how the
structure of the local loss landscape with respect to a probe
task evolves in both networks by comparing the Hessian
eigenvalue distribution, and by comparing the covariance
structure Ck of gradients on sampled inputs, with k = 512
equal to the batch size used for training. We compute the
Hessian matrix for a regression loss towards a perturbation
of the network’s current output to obtain a proxy for how
easily the network can update its predictions in arbitrary
directions; we do not evaluate the Hessian or gradient struc-
ture of the primary learning objective as these will trivially
differ between the trajectories.
We observe that the spectral norm of the Hessian of both
processes increases over time; however, the outliers of the
spectrum grow signiﬁcantly faster in the network trained
with gradient descent. Additionally, the network trained
with gradient descent begins to exhibit negative interference
between gradients, a phenomenon not observed in the Brow-
nian motion. In other words, the inductive bias induced by
gradient descent can push the parameters towards regions
of the parameter space where the local loss landscape is
less friendly to optimization towards arbitrary new objec-
tives than what would be obtained by blindly perturbing
randomly initialized parameters.
4. Explaining plasticity loss
While in some instances it is straightforward to deduce the
cause of plasticity loss, most learning problems induce com-
plex learning dynamics that make it difﬁcult to determine
root causes. This section will show that a number of plausi-
ble explanations of plasticity loss, including the rank of the
network’s features, the number of saturated units, the norm
of its parameters, and the rank of the weight matrices, do
not identify robust causal relationships. We provide some
evidence supporting the hypothesis that plasticity loss arises
due to changes in the network’s loss landscape, and con-
clude with a discussion of the potential trade-offs that must
be faced between preserving a trainable gradient structure
and accurately predicting a value function.
4.1. Experimental setting
The experimental framework we consider, and which will
be revisited in Section 5 is as follows. We construct a simple
MDP analogue of image classiﬁcation, i.e. the underlying
transition dynamics are deﬁned over a set of ten states and
ten actions, and the reward and transition dynamics depend
on whether or not the action taken by the agent is equal
to the state’s latent label. We construct three variants of a
block MDP whose state space can be given by either the
CIFAR-10 or MNIST image dataset.
Easy: each state s of the MDP produces an observation
from that class in the underlying classiﬁcation dataset.

Understanding plasticity in neural networks
250
500
750
1000 1250 1500
Weight norm
0.0
0.1
0.2
0.3
0.4
0.5
Plasticity loss
Varying observation space
cifar10 
mnist 
300
350
400
450
500
Weight rank
0.0
0.1
0.2
0.3
Varying observation space
cifar10 
mnist 
0
100
200
300
400
Dead units
0.0
0.2
0.4
0.6
Varying architecture
cnn 
mlp 
10
20
30
Feature rank
0.01
0.02
0.03
0.04
0.05
Varying reward function
easy 
hard 
sparse 
Falsification of explanations of plasticity
Figure 3. Results of our experimental falsiﬁcation design: for any variable we consider, it is possible to construct a set of learning problems
in which the variable exhibits either a positive or a negative correlation with plasticity. For example, weight norm and weight rank exhibit
differing correlation signs depending on the observation space, while feature rank and sparsity depend on the reward structure of the
environment.
Given action a, the reward is the indicator function δa=s.
The MDP then randomly transitions to a new state.
Hard: observation is assigned a random label in {0 . . . 9},
and the observation from an MDP state is sampled from im-
ages with the same randomly assigned label. The dynamics
are otherwise identical to the easy environment.
Sparse: exhibits the same observation mapping as easy.
The reward is equal to δa=s=9. The MDP transitions to a
random state if a ̸= s and otherwise transitions to s + 1.
We design these environments to satisfy two principal
desiderata: ﬁrst, that they present visually interesting pre-
diction challenges, and second that they allow us to isolate
non-stationarity due to policy and target network updates
independent of a change in the state visitation distribution.
In the easy and hard variants, the transition dynamics do not
depend on the agent’s action, whereas in the sparse environ-
ment the policy inﬂuences the state visitation distribution.
The different reward functions allow us to compare tasks
which are aligned with the network’s inductive bias (in the
easy task) and those which are not (the hard task).
We train a set of DQN agents on each environment-
observation space combination, and evaluate the ability of
each network to ﬁt a randomly generated set of target func-
tions as described in Section 2.2 after a ﬁxed number of
training steps. In the experiments shown here, we run the
DQN agents with a target network update period of 1,000
steps; as mentioned previously, this is the principal source
of non-stationarity in the ‘easy’ and ‘hard’ tasks. Every
5000 steps, we pause training, and from a copy of the cur-
rent parameters θt we train the network on a set of new
regression problems to probe its plasticity. We log the loss
at the end of 2,000 steps of optimization, sampling 10 differ-
ent random functions, then resume training of the RL task
from the saved parameters θt. We consider two network
architectures: a fully-connected network (MLP) and a con-
volutional network architecture (CNN). Full details of the
environments are included in Appendix A.2.
4.2. Falsiﬁcation of prior hypotheses
Prior work has proposed a number of plausible explanations
of why neural networks may exhibit reduced ability to ﬁt
new targets over time. Increased weight norm (Nikishin
et al., 2022), low rank of the features or weights (Kumar
et al., 2020; Gulcehre et al., 2022), and inactive features
(Lyle et al., 2021; Dohare et al., 2021) have all been dis-
cussed as plausible mechanisms by which plasticity loss may
occur. However, the explanatory power of these hypotheses
has not been rigorously tested. While a correlation between
a particular variable and plasticity loss can be useful for di-
agnosis, only a causal relationship indicates that intervening
on that variable will necessarily increase plasticity.
This section will seek to answer whether the above candi-
date explanations capture causal pathways. Our analysis is
based on a simple premise: that for a quantity to exhibit
explanatory power over plasticity loss, it should exhibit a
consistent correlation across different experimental interven-
tions (B¨uhlmann, 2020). If, for example, parameter norm
is positively correlated with plasticity in one observation
space and negatively correlated in another, then it can be
ruled out as a causal factor in plasticity loss. To construct
this experiment, we train 128 DQN agents under a range of
tasks, observation spaces, optimizers, and seeds. Over the
course of training, we log several statistics of the parameters
and activations, along with the plasticity of the parameters
at each logging iteration.
In Figure 3, we show scatterplots illustrating the relationship
between plasticity and each statistic, where each point in the
scatterplot corresponds to a single training run. We see that
for each of four quantities, there exists a learning problem
where the quantity positively correlates with plasticity, and
one in which it exhibits a negative correlation. In many
learning problems the correlation between plasticity loss
and the quantity of interest is nonexistent. In all cases we
note that the correlation between each quantity and plastic-
ity is already quite weak; even so, the ability to reverse the

Understanding plasticity in neural networks
0
250
500
750
1000
1250
1500
1750
2000
Optimizer steps
0.0
0.1
0.2
0.3
0.4
0.5
Loss on new target
Learning curve evolution over time
iteration 0
iteration 10
iteration 20
iteration 50
iteration 100
Figure 4. In-depth view of learning curves on a new target ﬁtting
task starting from network checkpoints at different points in train-
ing. This ﬁgure illustrates a CNN trained on the ‘easy’ MDP
described in Section 5 with a CIFAR-10 observation space.
sign of this correlation is a further mark against the utility
of these simple statistics as causal explanations of plastic-
ity. For example, we see a positive correlation between
weight norm and plasticity loss in environments which use
CIFAR-10 observations, but a slight negative correlation in
environments which sample observations from MNIST. A
similar reversal happens with respect to feature rank across
environments.
4.3. Loss landscape evolution during training
If the simple statistics we have considered thus far lack
explanatory power, how should we characterize plasticity
loss? One open question is whether the reduced ability
to ﬁt arbitrary new targets arises because the optimization
process gets caught in local optima, or whether it arises
due to overall slow or inconsistent optimization progress.
To answer this question, we turn our attention towards the
learning curves obtained by networks when we ask them
to ﬁt new target functions. We study these learning curves
primarily because they convey precisely the ease or difﬁculty
of navigating the loss landscape. In particular, the learning
curve tells us whether optimization is getting trapped in
bad minima (in which case the learning curve would hit an
early plateau at a large loss value), or whether the network
has greater difﬁculty reducing the loss enough to ﬁnd a
minimum in the ﬁrst place (corresponding to a ﬂatter slope).
We show in Figure 4 the learning curves obtained by an
optimization trajectory from parameters θt on the probe task
from different timesteps t of training on the RL task. We
see that parameters from early training checkpoints quickly
attain low losses, but that the slopes of these learning curves
are monotonically increasing with the parameter age t. Of
particular note is the increasing variance of the curves: in
the full-batch case, this non-monotonicity is associated with
increasing loss landscape sharpness (Cohen et al., 2021).
In the mini-batch optimization setting, we observed both
increasing interference between minibatches as well as non-
monotonicity in the loss even on the minibatch on which the
gradient was computed. In short, we see that it is increasing
5
10
multiplier
4
3
2
1
Plasticity loss 
 (log scale)
target update period : 1
5
10
multiplier
4
3
2
1
target update period : 100
5
10
multiplier
5
4
3
2
1
target update period : 1000
mlp on mnist
cnn on mnist
mlp on cifar10
cnn on cifar10
Effect of network width on plasticity loss
Figure 5. We observe a consistent decline in plasticity loss across
different target update frequencies as a result of scaling in several
architecture-dataset combinations; however, even when scaling the
architecture to the point where it no longer ﬁts on a single GPU,
we are still unable to completely eliminate plasticity loss on these
simple classiﬁcation-inspired problems.
difﬁculty of navigating the loss landscape, rather than poor
local minima, that appears to drive plasticity loss.
5. Solutions
Thus far, we have demonstrated that neural networks can be
demonstrated to lose plasticity even in a task as simple as
classifying MNIST digits, assuming that a degree of non-
stationarity is introduced into the optimization dynamics.
We turn our attention to means of reducing or reversing this
loss of plasticity. Section 5.1 will evaluate whether scaling
is sufﬁcient to mitigate plasticity loss. Section 5.2 will
evaluate the effect of a variety of interventions on plasticity
across a range of architectures. We will test the applicability
of these ﬁndings to larger scale tasks in Section 5.3.
5.1. The role of scaling on plasticity
Before considering sophisticated methods to address plas-
ticity loss, we must ﬁrst answer the question of whether this
is simply a disease of small networks. In the context of the
impressive successes of large models and the resultant ‘scal-
ing laws’ phenomenon (Kaplan et al., 2020), it is entirely
plausible that plasticity loss, like many other challenges,
vanishes in the limit of inﬁnite computation. We ﬁnd that
while plasticity loss is easiest to induce in extreme forms
in small networks, scaling a CNN to the limit of a single
GPU’s memory is insufﬁcient to eliminate plasticity loss
even in the simple classiﬁcation tasks described in the previ-
ous section. We visualize the relationship between network
width and plasticity loss in Figure 5.
These observations suggest that plasticity loss is unlikely
to be the limiting factor for sufﬁciently large networks on
sufﬁciently simple tasks. However, for tasks which do not
align with the inductive bias of the network, or for which the
network is not sufﬁciently expressive, we see a reduction in
the ability to ﬁt new targets even in larger architectures. Be-

Understanding plasticity in neural networks
resnet
transformer
cnn
mlp
regress
reset_last_layer
weight_decay
spectral_norm
use_layernorm
shrink_and_perturb
None
0.12 ± 0.15
0.05 ± 0.04
0.09 ± 0.17
0.02 ± 0.02
0.00 ± 0.00
0.04 ± 0.07
0.36 ± 0.32
0.10 ± 0.10
0.26 ± 0.18
0.17 ± 0.10
0.34 ± 0.08
0.28 ± 0.23
0.41 ± 0.21
0.43 ± 0.23
0.24 ± 0.16
0.25 ± 0.21
0.79 ± 0.21
0.23 ± 0.03
0.56 ± 0.20
0.61 ± 0.07
0.20 ± 0.21
0.13 ± 0.03
0.31 ± 0.06
0.27 ± 0.21
Effect of interventions on plasticity loss
Figure 6. Effect of architectural and optimization interventions on
plasticity loss. Colour indicates change in loss on challenge targets
between initial and ﬁnal epoch of training on RL task. Darker
shading indicates less plasticity loss.
cause we typically cannot guarantee a priori that a learning
problem will fall in the ﬁrst category, we therefore turn our
attention to other design choices which might further insure
networks against plasticity loss.
5.2. Interventions in toy problems
In this section we evaluate the effect of a variety of inter-
ventions on plasticity loss. We evaluate interventions on the
same task used in Section 4.1, training for 100 iterations of
1000 steps. We consider four architectures: a multi-layer
perceptron (MLP), a convolutional neural network (CNN)
without skip connections, a ResNet-18 (He et al., 2016), and
a small transformer based on the Vision Transformer (ViT)
architecture (Dosovitskiy et al., 2020).
We consider the following interventions: resetting the last
layer of the network at each target network update, a sim-
pliﬁed variant of the scheme proposed by Nikishin et al.
(2022); resetting the network optimizer state at each tar-
get network update; adding layer normalization (Ba et al.,
2016) after each convolutional and fully-connected layer of
the CNN and the MLP; performing Shrink and Perturb
(Ash & Adams, 2020): multiplying the network weights
by a small scalar and adding a perturbation equal to the
weights of a randomly initialized network; spectral nor-
malization of the initial linear layer of the CNN and the
MLP (Gogianu et al., 2021); and weight decay, setting the
ℓ2 penalty coefﬁcient to 10−5.
These methods were chosen to be representative samples of
a number of approaches to mitigating plasticity loss: reset-
ting the optimizer state and last layer temporarily remove a
source of poor conditioning from the optimization process;
layer normalization and residual connections tend to make
networks more robust to optimizer choices; weight decay
and spectral normalization both regularize the parameters of
the network in different ways; shrink and perturb applies a
perturbation to the current parameters without signiﬁcantly
changing the decision boundary (though we note that for
regression tasks this will still inﬂuence the scale of the net-
work outputs, and so may not be suitable ).
We visualize our key takeaways in Figure 6, which compares
plasticity loss after 100 iterations of training on each of the
architecture-intervention combinations. Overall, explicitly
constructing a network parameterization which smooths out
the loss landscape is the most effective means of preserving
plasticity of all approaches we have considered, and has a
greater effect on plasticity than resetting the ﬁnal layer of
the network. We visualize some learning curves of networks
with and without layer normalization in Figure 16 in the
supplementary material.
We note that while the two-hot encoding does demonstrate
signiﬁcant reductions in plasticity loss, it does so at the cost
of stability of the learned policy in several instances we con-
sidered. Additionally, this intervention required signiﬁcantly
different optimizer hyperparameters from the regression pa-
rameterization, suggesting that while it can be a powerful
tool to stabilize optimization, it might not be suitable as
a plug-in solution to mitigate plasticity loss in an existing
protocol.
5.3. Application to larger benchmarks
We now evaluate whether the beneﬁts of layer normalization
on plasticity in toy classiﬁcation tasks translate to larger-
scale benchmarks. We use the standard implementation of
double DQN (Van Hasselt et al., 2016) provided by Quan
& Ostrovski (2020), and evaluate three seeds on each of
the 57 games in the Arcade Learning Environment bench-
mark (Bellemare et al., 2013). We use the RMSProp op-
timizer, ϵ-greedy exploration, and frame stacking (Mnih
et al., 2015). Full implementation details can be found in
Appendix A.3. The only difference between the baseline
implementation and our modiﬁcation is the incorporation of
layer normalization after each hidden layer in the network.
We see in Figure 7 that the introduction of layer normaliza-
tion robustly improves performance across the benchmark.
We emphasize that we did not perform any optimizer or
other hyper parameter tuning. While this improvement can-
not be deﬁnitively attributed to a reduction in plasticity loss
from the evidence provided, it points towards the regular-
ization of the optimization landscape as a fruitful direction
towards more robust RL agents. We further observe that
many of the environments where layer normalization of-
fers a signiﬁcant boost to performance are those where the
gradient covariance structure of the default architecture is
degenerate or where the Hessian is ill-conditioned, and the
LN networks which obtain performance improvements tend
to have correspondingly better behaved gradient and Hes-
sian structures. We provide a hint into this phenomenon
in Figure 7, and defer the complete evaluation over all 57
games to Appendix B.3.

Understanding plasticity in neural networks
−25
0
25
50
75
100
125
150
double_dunk
bowling
centipede
venture
atlantis
kangaroo
assault
amidar
pong
robotank
gravitar
montezuma
battle_zone
pitfall
solaris
ice_hockey
asteroids
frostbite
star_gunner
hero
yars_revenge
private_eye
beam_rider
skiing
berzerk
breakout
ms_pacman
road_runner
kung_fu_master
seaquest
fishing_derby
crazy_climber
qbert
chopper
boxing
demon_attack
name_this_game
krull
freeway
riverraid
alien
tutankham
phoenix
jamesbond
bank_heist
surround
time_pilot
zaxxon
defender
gopher
up_n_down
asterix
video_pinball
tennis
wizard_of_wor
space_invaders
enduro
-52.4%
-8.7%
-7.3%
-4.6%
-3.3%
-2.7%
-2.2%
-0.6%
-0.1%
-0.1%
0.0%
0.0%
0.4%
0.4%
0.7%
1.4%
1.5%
2.2%
2.5%
4.3%
4.8%
5.7%
10.3%
14.6%
17.2%
17.2%
18.2%
21.1%
22.2%
22.9%
24.6%
26.9%
27.4%
28.2%
29.6%
31.4%
35.4%
38.1%
40.7%
46.7%
49.9%
52.2%
63.2%
64.2%
64.5%
65.0%
66.8%
90.9%
92.3%
93.6%
93.6%
105.4%
119.9%
124.4%
132.6%
140.9%
144.2%
default
layernorm
default
layernorm
Figure 7. Top: Human-normalized improvement score (Wang et al.,
2016) of adding layer normalization over the default double DQN
agent. Bottom: Gradient covariance matrices for Freeway (left)
and Kangroo (right). In environments where layer normalization
signiﬁcantly improves performance, it also induces weaker gradi-
ent correlation.
6. Related Work
Trainability: the problem of ﬁnding suitable initializations
for neural networks to enable training has a long history
(Glorot & Bengio, 2010; He et al., 2015; Sutskever et al.,
2013). Without careful initialization and architecture design,
it is common to run into the issue that gradients will either
explode or vanish as the depth of the network grows (Yang
& Schoenholz, 2017). ResNets (He et al., 2016) in particular
are known to resolve many of these pathologies by biasing
each layer’s mapping towards the identity function, leading
to better-behaved gradients (Balduzzi et al., 2017). Mean-
ﬁeld analysis (Yang & Schoenholz, 2017; Schoenholz et al.,
2017; Yang et al., 2019), information propagation (Poole
et al., 2016), and deep kernel shaping (Zhang et al., 2021b;
Martens et al., 2021) have all been applied to study trainabil-
ity in neural networks. A wealth of prior work additionally
studies the role of loss landscape smoothness in generaliza-
tion and performance (Li et al., 2018; Santurkar et al., 2018;
Ghorbani et al., 2019; Park & Kim, 2022). Other works
highlight the chaotic behaviour of early training periods
(Jastrzebski et al., 2020), in particular the ‘edge of stability’
phenomenon (Cohen et al., 2021) and the ‘catapault mech-
anism’ (Lewkowycz et al., 2020), and relate closely to the
observations grounding ‘linear mode connectivity’ (Frankle
et al., 2020) to explain generalization and trainability in
deep neural networks; however, these approaches all focus
on supervised learning with a stationary objective.
Resetting + continual learning: (Zhang et al., 2021a), (Be-
rariu et al., 2021) (Hadsell et al., 2020; Rolnick et al., 2019).
(Ostapenko et al., 2019) class-incremental learning, differs
from our setting because the input distribution changes, not
the functional relationship. Thangarasa et al. (2020) propose
a modiﬁed Hebbian learning rule. Studies of plasticity in
task-shift continual learning usually focus on ability to learn
under new input distributions (Rolnick et al., 2019), rather
than the ability to modify predictions on a ﬁxed distribution.
Most related to our study is the identiﬁcation of the loss of
plasticity as a potentially limiting factor in deep reinforce-
ment learning (Lyle et al., 2021; Dohare et al., 2021). This
study can be motivated by the rich literature studying the
effect of resetting and distillation on performance (Fedus
et al., 2020; Nikishin et al., 2022; Igl et al., 2021; Schmitt
et al., 2018).
7. Conclusions
The ﬁndings of this paper highlight a divide between the
study of curriculum learning and foundation models, which
identify suitable early training objectives to accelerate learn-
ing and improve generalization on later tasks, and the phe-
nomena we have identiﬁed concerning the loss of plasticity
in non-stationary prediction problems. However, as rein-
forcement learning algorithms scale up to more complex
tasks, the divide between these regimes shrinks. While it is
possible that in many settings, plasticity loss is not a lim-
iting factor in network performance and so need not be a
concern for many of the relatively small environments used
to benchmark algorithms today, we conjecture that as the
complexity of the tasks to which we apply RL grows, so
will the importance of preserving plasticity.
The ﬁndings of this paper point towards stabilizing the loss
landscape as a crucial step towards promoting plasticity.
This approach is likely to have many ancillary beneﬁts, pre-
senting an exciting direction for future investigation. A
smoother loss landscape is both easier to optimize and has
been empirically observed to exhibit better generalization,
and it is an exciting direction for future work to better dis-
entangle the complementary roles of memorization and gen-
eralization in plasticity.

Understanding plasticity in neural networks
Acknowledgements
We thank Mark Rowland, Tom Schaul, Georg Ostrovski,
Hado van Hasselt, Diana Borsa, and Samuel Smith for valu-
able feedback and discussions during the development of
this work.
References
Abbott, L. F. and Nelson, S. B. Synaptic plasticity: taming
the beast. Nature neuroscience, 3(11):1178–1183, 2000.
Ash, J. and Adams, R. P. On warm-starting neural network
training. Advances in Neural Information Processing
Systems, 33:3884–3894, 2020.
Ba, J. L., Kiros, J. R., and Hinton, G. E. Layer normalization.
arXiv preprint arXiv:1607.06450, 2016.
Balduzzi, D., Frean, M., Leary, L., Lewis, J., Ma, K. W.-D.,
and McWilliams, B. The shattered gradients problem:
If resnets are the answer, then what is the question? In
International Conference on Machine Learning, pp. 342–
350. PMLR, 2017.
Bartlett, P. L. and Mendelson, S. Rademacher and gaussian
complexities: Risk bounds and structural results. Journal
of Machine Learning Research, 3(Nov):463–482, 2002.
Bellemare, M. G., Naddaf, Y., Veness, J., and Bowling, M.
The arcade learning environment: An evaluation plat-
form for general agents. Journal of Artiﬁcial Intelligence
Research, 47:253–279, 2013.
Berariu, T., Czarnecki, W., De, S., Bornschein, J., Smith, S.,
Pascanu, R., and Clopath, C. A study on the plasticity of
neural networks. arXiv preprint arXiv:2106.00042, 2021.
B¨uhlmann, P. Invariance, causality and robustness. Statisti-
cal Science, 35(3):404–426, 2020.
Cohen, J. M., Kaur, S., Li, Y., Kolter, J. Z., and Talwalkar,
A. Gradient descent on neural networks typically occurs
at the edge of stability. arXiv preprint arXiv:2103.00065,
2021.
Deng, L. The mnist database of handwritten digit images
for machine learning research. IEEE Signal Processing
Magazine, 29(6):141–142, 2012.
Dinh, L., Pascanu, R., Bengio, S., and Bengio, Y. Sharp min-
ima can generalize for deep nets. In International Con-
ference on Machine Learning, pp. 1019–1028. PMLR,
2017.
Dohare, S., Mahmood, A. R., and Sutton, R. S. Continual
backprop: Stochastic gradient descent with persistent
randomness. arXiv preprint arXiv:2108.06325, 2021.
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn,
D., Zhai, X., Unterthiner, T., Dehghani, M., Minderer, M.,
Heigold, G., Gelly, S., et al. An image is worth 16x16
words: Transformers for image recognition at scale. In
International Conference on Learning Representations,
2020.
Dziugaite, G. K., Drouin, A., Neal, B., Rajkumar, N., Ca-
ballero, E., Wang, L., Mitliagkas, I., and Roy, D. M. In
search of robust measures of generalization. Advances
in Neural Information Processing Systems, 33:11723–
11733, 2020.
Fedus, W., Ghosh, D., Martin, J. D., Bellemare, M. G., Ben-
gio, Y., and Larochelle, H. On catastrophic interference
in atari 2600 games. arXiv preprint arXiv:2002.12499,
2020.
Fort, S., Nowak, P. K., and Narayanan, S. Stiffness: A new
perspective on generalization in neural networks. CoRR,
abs/1901.09491, 2019. URL http://arxiv.org/
abs/1901.09491.
Frankle, J., Dziugaite, G. K., Roy, D., and Carbin, M. Linear
mode connectivity and the lottery ticket hypothesis. In
International Conference on Machine Learning, pp. 3259–
3269. PMLR, 2020.
French, R. M. Catastrophic forgetting in connectionist net-
works. Trends in cognitive sciences, 3(4):128–135, 1999.
Ghorbani, B., Krishnan, S., and Xiao, Y. An investigation
into neural net optimization via hessian eigenvalue den-
sity. In International Conference on Machine Learning,
pp. 2232–2241. PMLR, 2019.
Gilmer, J., Ghorbani, B., Garg, A., Kudugunta, S. R.,
Neyshabur, B., Cardoze, D., Dahl, G. E., Nado, Z.,
and Firat, O.
A loss curvature perspective on train-
ing instability in deep learning. In ICLR, 2022. URL
https://arxiv.org/abs/2110.04369.
Glorot, X. and Bengio, Y. Understanding the difﬁculty
of training deep feedforward neural networks. In Pro-
ceedings of the thirteenth international conference on
artiﬁcial intelligence and statistics, pp. 249–256. JMLR
Workshop and Conference Proceedings, 2010.
Gogianu, F., Berariu, T., Rosca, M. C., Clopath, C., Busoniu,
L., and Pascanu, R.
Spectral normalisation for deep
reinforcement learning: an optimisation perspective. In
International Conference on Machine Learning, pp. 3734–
3744. PMLR, 2021.
Gulcehre, C., Srinivasan, S., Sygnowski, J., Ostrovski, G.,
Farajtabar, M., Hoffman, M., Pascanu, R., and Doucet,
A. An empirical study of implicit regularization in deep
ofﬂine rl. Transactions of Machine Learning Research,
2022.

Understanding plasticity in neural networks
Hadsell, R., Rao, D., Rusu, A. A., and Pascanu, R. Embrac-
ing change: Continual learning in deep neural networks.
Trends in Cognitive Sciences, 24(12):1028–1040, 2020.
ISSN 1364-6613.
He, K., Zhang, X., Ren, S., and Sun, J. Delving deep
into rectiﬁers: Surpassing human-level performance on
imagenet classiﬁcation. In Proceedings of the IEEE inter-
national conference on computer vision, pp. 1026–1034,
2015.
He, K., Zhang, X., Ren, S., and Sun, J. Deep residual learn-
ing for image recognition. In Proceedings of the IEEE
conference on computer vision and pattern recognition,
pp. 770–778, 2016.
Igl, M., Farquhar, G., Luketina, J., Boehmer, W., and White-
son, S. Transient non-stationarity and generalisation in
deep reinforcement learning. In International Confer-
ence on Learning Representations, 2021. URL https:
//openreview.net/forum?id=Qun8fv4qSby.
Jastrzebski, S., Szymczak, M., Fort, S., Arpit, D., Tabor,
J., Cho*, K., and Geras*, K. The break-even point on
optimization trajectories of deep neural networks. In
International Conference on Learning Representations,
2020. URL https://openreview.net/forum?
id=r1g87C4KwB.
Kaplan, J., McCandlish, S., Henighan, T., Brown, T. B.,
Chess, B., Child, R., Gray, S., Radford, A., Wu, J., and
Amodei, D. Scaling laws for neural language models.
arXiv preprint arXiv:2001.08361, 2020.
Kingma, D. P. and Ba, J. Adam: A method for stochastic
optimization. In ICLR (Poster), 2015.
Kumar, A., Agarwal, R., Ghosh, D., and Levine, S. Im-
plicit under-parameterization inhibits data-efﬁcient deep
reinforcement learning. In International Conference on
Learning Representations, 2020.
Lewkowycz, A., Bahri, Y., Dyer, E., Sohl-Dickstein, J.,
and Gur-Ari, G.
The large learning rate phase of
deep learning: the catapult mechanism. arXiv preprint
arXiv:2003.02218, 2020.
Li, H., Xu, Z., Taylor, G., Studer, C., and Goldstein, T.
Visualizing the loss landscape of neural nets. Advances
in neural information processing systems, 31, 2018.
Lyle, C., Rowland, M., and Dabney, W. Understanding and
preventing capacity loss in reinforcement learning. In
International Conference on Learning Representations,
2021.
Lyle, C., Rowland, M., Dabney, W., Kwiatkowska, M., and
Gal, Y. Learning dynamics and generalization in deep
reinforcement learning. In International Conference on
Machine Learning, pp. 14560–14581. PMLR, 2022.
Martens, J., Ballard, A., Desjardins, G., Swirszcz, G., Dal-
ibard, V., Sohl-Dickstein, J., and Schoenholz, S. S. Rapid
training of deep neural networks without skip connections
or normalization layers using deep kernel shaping. arXiv
preprint arXiv:2110.01765, 2021.
Mermillod, M., Bugaiska, A., and Bonin, P. The stability-
plasticity dilemma: Investigating the continuum from
catastrophic forgetting to age-limited learning effects,
2013.
Mnih, V., Kavukcuoglu, K., Silver, D., Rusu, A. A., Veness,
J., Bellemare, M. G., Graves, A., Riedmiller, M., Fidje-
land, A. K., Ostrovski, G., et al. Human-level control
through deep reinforcement learning. nature, 518(7540):
529–533, 2015.
Nikishin, E., Schwarzer, M., D’Oro, P., Bacon, P.-L., and
Courville, A. The primacy bias in deep reinforcement
learning. In International Conference on Machine Learn-
ing, pp. 16828–16847. PMLR, 2022.
Ostapenko, O., Puscas, M., Klein, T., Jahnichen, P., and
Nabi, M. Learning to remember: A synaptic plasticity
driven framework for continual learning. In Proceedings
of the IEEE/CVF Conference on Computer Vision and
Pattern Recognition (CVPR), June 2019.
Park, N. and Kim, S. How do vision transformers work? In
International Conference on Learning Representations,
2022. URL https://openreview.net/forum?
id=D78Go4hVcxO.
Poole, B., Lahiri, S., Raghu, M., Sohl-Dickstein, J.,
and Ganguli, S.
Exponential expressivity in deep
neural networks through transient chaos.
In Lee,
D., Sugiyama, M., Luxburg, U., Guyon, I., and
Garnett, R. (eds.), Advances in Neural Information Pro-
cessing
Systems,
volume
29.
Curran
Associates,
Inc.,
2016.
URL
https://proceedings.
neurips.cc/paper/2016/file/
148510031349642de5ca0c544f31b2ef-Paper.
pdf.
Quan, J. and Ostrovski, G. DQN Zoo: Reference imple-
mentations of DQN-based agents, 2020. URL http:
//github.com/deepmind/dqn_zoo.
Rolnick, D., Ahuja, A., Schwarz, J., Lillicrap, T.,
and Wayne, G.
Experience replay for continual
learning.
In Wallach, H., Larochelle, H., Beygelz-
imer, A., d'Alch´e-Buc, F., Fox, E., and Garnett,
R. (eds.),
Advances in Neural Information Pro-
cessing
Systems,
volume
32.
Curran
Associates,

Understanding plasticity in neural networks
Inc.,
2019.
URL
https://proceedings.
neurips.cc/paper/2019/file/
fa7cdfad1a5aaf8370ebeda47a1ff1c3-Paper.
pdf.
Santurkar, S., Tsipras, D., Ilyas, A., and Madry, A. How
does batch normalization help optimization? In Bengio,
S., Wallach, H., Larochelle, H., Grauman, K., Cesa-
Bianchi, N., and Garnett, R. (eds.), Advances in Neural
Information Processing Systems, volume 31. Curran As-
sociates, Inc., 2018. URL https://proceedings.
neurips.cc/paper/2018/file/
905056c1ac1dad141560467e0a99e1cf-Paper.
pdf.
Schmitt, S., Hudson, J. J., Zidek, A., Osindero, S., Doersch,
C., Czarnecki, W. M., Leibo, J. Z., Kuttler, H., Zisserman,
A., Simonyan, K., et al. Kickstarting deep reinforcement
learning. arXiv preprint arXiv:1803.03835, 2018.
Schoenholz, S. S., Gilmer, J., Ganguli, S., and Sohl-
Dickstein, J.
Deep information propagation.
In In-
ternational Conference on Learning Representations,
2017. URL https://openreview.net/forum?
id=H1W1UN9gg.
Sutskever, I., Martens, J., Dahl, G., and Hinton, G. On the
importance of initialization and momentum in deep learn-
ing. In International conference on machine learning, pp.
1139–1147. PMLR, 2013.
Thangarasa, V., Miconi, T., and Taylor, G. W. Enabling con-
tinual learning with differentiable hebbian plasticity. In
2020 International Joint Conference on Neural Networks
(IJCNN), pp. 1–8. IEEE, 2020.
Van Hasselt, H., Guez, A., and Silver, D. Deep reinforce-
ment learning with double q-learning. In Proceedings of
the AAAI conference on artiﬁcial intelligence, volume 30,
2016.
Vapnik, V. On the uniform convergence of frequencies of
occurrence of events to their probabilities. Theory of
Probability and its Applications, 16, 1968.
Wang, Z., Schaul, T., Hessel, M., Hasselt, H., Lanctot, M.,
and Freitas, N. Dueling network architectures for deep
reinforcement learning. In International conference on
machine learning, pp. 1995–2003. PMLR, 2016.
Yang, G. and Schoenholz, S. Mean ﬁeld residual networks:
On the edge of chaos. Advances in neural information
processing systems, 30, 2017.
Yang, G., Pennington, J., Rao, V., Sohl-Dickstein, J., and
Schoenholz, S. S. A mean ﬁeld theory of batch nor-
malization. In International Conference on Learning
Representations, 2019. URL https://openreview.
net/forum?id=SyMDXnCcF7.
Zhang, C., Bengio, S., and Singer, Y. Are all layers created
equal? Journal of Machine Learning Research, 2021a.
Zhang, G., Botev, A., and Martens, J. Deep learning without
shortcuts: Shaping the kernel with tailored rectiﬁers. In
International Conference on Learning Representations,
2021b.

Understanding plasticity in neural networks
A. Experiment details
A.1. Case studies
Optimizer instability: we consider a memorization problem on the MNIST dataset (Deng, 2012), where the network
is trained to classify its inputs according to randomly permuted labels of a subset of 5000 MNIST images. We use a
fully-connected multi-layer perceptron (MLP) with two hidden layers of width 1024. At the end of each training iteration we
log the accuracy on a sample of 4096 states, and re-randomize the labels, keeping the input set ﬁxed. We train the network
with an adam optimizer with learning rate equal to 0.001, ﬁrst-order moment decay b1 = 0.9, second-order moment decay
b2 = 0.999, ε = 10−9, and ¯ε = 0. With the tuned optimizer, we se b2 = 0.9 and ¯ε = 10−3.
Brownian motion: we train the network via a Q-learning loss on batches of transitions generated by the easy classiﬁcation
MDP with the MNIST observation space. We use a stochastic gradient descent optimizer with learning rate 0.001. We use a
batch size of 512.
We evaluate the gradient covariance matrix Ck by sampling a batch of transitions and computing the gradient of each
transition individually. We then take the normalized dot product matrix and permute the rows and columns according to a
k-means clustering, where we set k = 10 to match the number of latent states in the environment. We update the target
network in the Q-learning objective once every 5000 steps.
To compute the Hessian eigenvalue density, we follow the implementation of Ghorbani et al. (2019) in order to obtain a
Gaussian approximation to the eigenvalue distribution. We sample a single large batch of transitions, and use the Lanczos
algorithm to obtain a set of centroids which are then convolved with a Gaussian distribution to obtain the ﬁnal density.
A.2. Toy RL environments
We use the same agent structure for both the Atari and classiﬁcation MDP environments. The agent collects data by
interacting with the environment following an ϵ-greedy policy and stores states in a replay buffer. We then interleave
interaction with the environment and optimization on sampled batches from the replay buffer. In the classiﬁcation MDP, we
train the network for 10,000 steps before updating the target network. We tried a variety of target network update frequencies
and found that shorter update periods resulted in poor action-value estimation in the hard environment. We probe the ability
of the network to ﬁt a new set of regression targets once every 5000 optimizer steps: we draw 10 randomly sampled target
functions generated by the procedure described in Section 2.2, and for each run the network’s optimizer from the current
parameters to minimize the loss with respect to these new targets for 2000 steps.
The architectures we consider in our plasticity evaluations are as follows:
• MLP: we use two hidden layers of varying width. For all evaluations other than the width sweep used to generate
Figure 5, we use a width of 512. For the width sweep, we set a base width of 16 and then multiply by factors of 1, 2, 4,
8, 12, and 16.
• CNN: we use two convolutional layers followed by two fully-connected layers. The ﬁrst convolutional layer uses 5x5
kernels, while the second uses 3x3; both have 64 channels. The fully-connected layers have widths of 256.
• ResNet: we use a standard resnet18 architecture based on the implementation by ().
• Vision Transformer: we use our own implementation based on (Dosovitskiy et al., 2020). We use a patch size of 3 to
construct the convolutional embeddings, model dimension of 256 and a feedforward width of 1024. We use a single
transformer block, and a dropout rate of 0.1. All components of the model are trained from scratch on the task.
A.3. Double DQN
We follow the standard training protocol on Atari, training for 200 million frames and performing optimizer updates once
every 4 environment steps (Quan & Ostrovski, 2020). We add layer normalization after each hidden layer of the network.
We use a replay buffer of size 100,000, and follow an ϵ-greedy policy during training with ϵ = 0.1.

Understanding plasticity in neural networks
resnet
transformer
cnn
mlp
regress
reset_last_layer
weight_decay
spectral_norm
use_layernorm
shrink_and_perturb
None
0.12 ± 0.15
0.05 ± 0.04
0.09 ± 0.17
0.02 ± 0.02
0.00 ± 0.00
0.04 ± 0.07
0.36 ± 0.32
0.10 ± 0.10
0.26 ± 0.18
0.17 ± 0.10
0.34 ± 0.08
0.28 ± 0.23
0.41 ± 0.21
0.43 ± 0.23
0.24 ± 0.16
0.25 ± 0.21
0.79 ± 0.21
0.23 ± 0.03
0.56 ± 0.20
0.61 ± 0.07
0.20 ± 0.21
0.13 ± 0.03
0.31 ± 0.06
0.27 ± 0.21
Effect of interventions on plasticity loss
resnet
transformer
cnn
mlp
reset_last_layer
weight_decay
spectral_norm
use_layernorm
shrink_and_perturb
0.18 ± 0.06
0.19 ± 0.21
0.14 ± 0.13
0.03 ± 0.07
0.21 ± 0.18
0.00 ± 0.04
0.09 ± 0.15
0.06 ± 0.15
0.19 ± 0.15
0.01 ± 0.07
-0.00 ± 0.00
0.05 ± 0.16
0.08 ± 0.06
0.11 ± 0.04
0.14 ± 0.11
0.04 ± 0.09
Effect of interventions on plasticity loss (categorical)
resnet
transformer
cnn
mlp
reset_last_layer
weight_decay
spectral_norm
use_layernorm
shrink_and_perturb
0.00 ± 0.00
0.04 ± 0.07
0.36 ± 0.32
0.10 ± 0.10
0.26 ± 0.18
0.17 ± 0.10
0.34 ± 0.08
0.28 ± 0.23
0.41 ± 0.21
0.43 ± 0.23
0.24 ± 0.16
0.25 ± 0.21
0.79 ± 0.21
0.23 ± 0.03
0.56 ± 0.20
0.61 ± 0.07
Effect of interventions on plasticity loss (regression)
Figure 8. We repeat the analysis of Figure 6, but also include the effect of interventions on regression and categorical output encodings.
We observe a signiﬁcant beneﬁt in the transformer, CNN, and MLP architectures from using the categorical encoding. This ﬁgure shows
results for the CIFAR-10 dataset.
resnet
transformer
cnn
mlp
regress
reset_last_layer
weight_decay
spectral_norm
use_layernorm
shrink_and_perturb
None
0.20 ± 0.04
0.11 ± 0.10
-0.02 ± 0.13
0.04 ± 0.12
0.01 ± 0.01
0.00 ± 0.01
0.57 ± 0.30
0.11 ± 0.09
0.37 ± 0.07
0.08 ± 0.06
0.25 ± 0.08
0.31 ± 0.20
0.39 ± 0.18
0.40 ± 0.26
0.32 ± 0.13
0.43 ± 0.28
0.66 ± 0.21
0.11 ± 0.10
0.42 ± 0.13
0.34 ± 0.17
0.32 ± 0.10
0.06 ± 0.03
0.27 ± 0.08
0.34 ± 0.24
Effect of interventions on plasticity loss
resnet
transformer
cnn
mlp
reset_last_layer
weight_decay
spectral_norm
use_layernorm
shrink_and_perturb
0.11 ± 0.12
0.05 ± 0.08
0.00 ± 0.13
0.01 ± 0.09
0.33 ± 0.10
0.09 ± 0.04
0.02 ± 0.09
0.07 ± 0.08
0.09 ± 0.11
0.06 ± 0.12
0.03 ± 0.05
0.07 ± 0.07
0.06 ± 0.06
0.14 ± 0.07
0.09 ± 0.10
0.09 ± 0.11
Effect of interventions on plasticity loss (categorical)
resnet
transformer
cnn
mlp
reset_last_layer
weight_decay
spectral_norm
use_layernorm
shrink_and_perturb
0.01 ± 0.01
0.00 ± 0.01
0.57 ± 0.30
0.11 ± 0.09
0.37 ± 0.07
0.08 ± 0.06
0.25 ± 0.08
0.31 ± 0.20
0.39 ± 0.18
0.40 ± 0.26
0.32 ± 0.13
0.43 ± 0.28
0.66 ± 0.21
0.11 ± 0.10
0.42 ± 0.13
0.34 ± 0.17
Effect of interventions on plasticity loss (regression)
Figure 9. We repeat the analysis of Figure 6, but also include the effect of interventions on regression and categorical output encodings.
We observe a signiﬁcant beneﬁt in the transformer, CNN, and MLP architectures from using the categorical encoding.
B. Additional analysis
B.1. Detailed intervention analysis
We provide a more detailed analysis of the effects of a variety of interventions on plasticity loss in different neural network
architectures. Figures 8 and 9 show the change in the average ﬁnal probe task loss after training on the toy RL environments
for 1 million optimizer steps. We see that resetting the last layer, incorporating a two-hot output representation, and
performing layer normalization have beneﬁcial effects on plasticity, whereas shrink and perturb, weight decay, and resetting
only the optimizer state do not improve plasticity.
In Figures 10 and 11 we show the effect of each intervention scheme on the initial and ﬁnal losses. Some interventions
improve trainability even from initialization, for example using a categorical output for the transformer, in addition to
reducing the ﬁnal probe task loss. The categorical representation appears to combine nicely with other interventions such as
layer normalization, resetting, and shrink and perturb. The beneﬁts of the categorical parameterization on shrink and perturb
are expected, as regression targets are not output-scale-invariant in the same way that softmax logits are.
resnet
transformer
cnn
mlp
regress
reset_last_layer
weight_decay
spectral_norm
use_layernorm
shrink_and_perturb
None
0.08 ± 0.15
0.03 ± 0.04
0.08 ± 0.17
0.48 ± 0.02
0.01 ± 0.00
0.26 ± 0.07
0.15 ± 0.32
0.37 ± 0.10
0.01 ± 0.18
0.24 ± 0.10
0.14 ± 0.08
0.36 ± 0.23
0.13 ± 0.21
0.38 ± 0.23
0.01 ± 0.16
0.30 ± 0.21
0.01 ± 0.21
0.27 ± 0.03
0.14 ± 0.20
0.39 ± 0.07
0.01 ± 0.21
0.28 ± 0.03
0.16 ± 0.06
0.40 ± 0.21
resnet
transformer
cnn
mlp
reset_last_layer
weight_decay
spectral_norm
use_layernorm
shrink_and_perturb
0.05 ± 0.06
0.03 ± 0.21
0.11 ± 0.13
0.46 ± 0.07
0.05 ± 0.18
0.05 ± 0.04
0.07 ± 0.15
0.43 ± 0.15
0.12 ± 0.15
0.48 ± 0.07
0.01 ± 0.00
0.38 ± 0.16
0.06 ± 0.06
0.04 ± 0.04
0.08 ± 0.11
0.45 ± 0.09
Effect of interventions on initial loss (categorical)
resnet
transformer
cnn
mlp
last_layer
ght_decay
tral_norm
ayernorm
d_perturb
0.01 ± 0.00
0.26 ± 0.07
0.15 ± 0.32
0.37 ± 0.10
0.01 ± 0.18
0.24 ± 0.10
0.14 ± 0.08
0.36 ± 0.23
0.13 ± 0.21
0.38 ± 0.23
0.01 ± 0.16
0.30 ± 0.21
0.01 ± 0.21
0.27 ± 0.03
0.14 ± 0.20
0.39 ± 0.07
Effect of interventions on initial loss (regression)
resnet
transformer
cnn
mlp
regress
reset_last_layer
weight_decay
spectral_norm
use_layernorm
shrink_and_perturb
None
0.20 ± 0.15
0.09 ± 0.04
0.17 ± 0.17
0.49 ± 0.02
0.01 ± 0.00
0.30 ± 0.07
0.50 ± 0.32
0.47 ± 0.10
0.27 ± 0.18
0.41 ± 0.10
0.47 ± 0.08
0.64 ± 0.23
0.54 ± 0.21
0.81 ± 0.23
0.25 ± 0.16
0.55 ± 0.21
0.80 ± 0.21
0.50 ± 0.03
0.70 ± 0.20
1.00 ± 0.07
0.21 ± 0.21
0.41 ± 0.03
0.47 ± 0.06
0.66 ± 0.21
resnet
transformer
cnn
mlp
reset_last_layer
weight_decay
spectral_norm
use_layernorm
shrink_and_perturb
0.23 ± 0.06
0.22 ± 0.21
0.25 ± 0.13
0.49 ± 0.07
0.27 ± 0.18
0.05 ± 0.04
0.16 ± 0.15
0.49 ± 0.15
0.31 ± 0.15
0.48 ± 0.07
0.01 ± 0.00
0.43 ± 0.16
0.14 ± 0.06
0.14 ± 0.04
0.22 ± 0.11
0.49 ± 0.09
Effect of interventions on final loss (categorical)
resnet
transformer
cnn
mlp
reset_last_layer
weight_decay
spectral_norm
use_layernorm
shrink_and_perturb
0.01 ± 0.00
0.30 ± 0.07
0.50 ± 0.32
0.47 ± 0.10
0.27 ± 0.18
0.41 ± 0.10
0.47 ± 0.08
0.64 ± 0.23
0.54 ± 0.21
0.81 ± 0.23
0.25 ± 0.16
0.55 ± 0.21
0.80 ± 0.21
0.50 ± 0.03
0.70 ± 0.20
1.00 ± 0.07
Effect of interventions on final loss (regression)
Figure 10. Initial and ﬁnal loss evaluation on CIFAR-10 input distribution.

Understanding plasticity in neural networks
resnet
transformer
cnn
mlp
regress
reset_last_layer
weight_decay
spectral_norm
use_layernorm
shrink_and_perturb
None
0.12 ± 0.04
0.12 ± 0.10
0.25 ± 0.13
0.23 ± 0.12
0.00 ± 0.01
0.41 ± 0.01
0.22 ± 0.30
0.28 ± 0.09
0.00 ± 0.07
0.37 ± 0.06
0.23 ± 0.08
0.27 ± 0.20
0.18 ± 0.18
0.28 ± 0.26
0.04 ± 0.13
0.17 ± 0.28
0.00 ± 0.21
0.37 ± 0.10
0.20 ± 0.13
0.28 ± 0.17
0.00 ± 0.10
0.39 ± 0.03
0.20 ± 0.08
0.25 ± 0.24
Effect of interventions on initial target-fitting loss
resnet
transformer
cnn
mlp
reset_last_layer
weight_decay
spectral_norm
use_layernorm
shrink_and_perturb
0.10 ± 0.12
0.10 ± 0.08
0.20 ± 0.13
0.24 ± 0.09
0.05 ± 0.10
0.09 ± 0.04
0.22 ± 0.09
0.21 ± 0.08
0.27 ± 0.11
0.26 ± 0.12
0.02 ± 0.05
0.10 ± 0.07
0.07 ± 0.06
0.13 ± 0.07
0.22 ± 0.10
0.20 ± 0.11
Effect of interventions on initial target-fitting loss (categori
resnet
transformer
cnn
mlp
reset_last_layer
weight_decay
spectral_norm
use_layernorm
shrink_and_perturb
0.00 ± 0.01
0.41 ± 0.01
0.22 ± 0.30
0.28 ± 0.09
0.00 ± 0.07
0.37 ± 0.06
0.23 ± 0.08
0.27 ± 0.20
0.18 ± 0.18
0.28 ± 0.26
0.04 ± 0.13
0.17 ± 0.28
0.00 ± 0.21
0.37 ± 0.10
0.20 ± 0.13
0.28 ± 0.17
Effect of interventions on initial target-fitting loss (regress
resnet
transformer
cnn
mlp
regress
reset_last_layer
weight_decay
spectral_norm
use_layernorm
shrink_and_perturb
None
0.31 ± 0.04
0.23 ± 0.10
0.23 ± 0.13
0.27 ± 0.12
0.02 ± 0.01
0.41 ± 0.01
0.79 ± 0.30
0.39 ± 0.09
0.37 ± 0.07
0.45 ± 0.06
0.48 ± 0.08
0.58 ± 0.20
0.58 ± 0.18
0.68 ± 0.26
0.35 ± 0.13
0.60 ± 0.28
0.66 ± 0.21
0.49 ± 0.10
0.62 ± 0.13
0.62 ± 0.17
0.32 ± 0.10
0.45 ± 0.03
0.47 ± 0.08
0.59 ± 0.24
Effect of interventions on final target-fitting loss
resnet
transformer
cnn
mlp
reset_last_layer
weight_decay
spectral_norm
use_layernorm
shrink_and_perturb
0.21 ± 0.12
0.15 ± 0.08
0.20 ± 0.13
0.25 ± 0.09
0.38 ± 0.10
0.19 ± 0.04
0.24 ± 0.09
0.29 ± 0.08
0.36 ± 0.11
0.33 ± 0.12
0.04 ± 0.05
0.18 ± 0.07
0.13 ± 0.06
0.26 ± 0.07
0.30 ± 0.10
0.29 ± 0.11
Effect of interventions on final target-fitting loss (categoric
resnet
transformer
cnn
mlp
reset_last_layer
weight_decay
spectral_norm
use_layernorm
shrink_and_perturb
0.02 ± 0.01
0.41 ± 0.01
0.79 ± 0.30
0.39 ± 0.09
0.37 ± 0.07
0.45 ± 0.06
0.48 ± 0.08
0.58 ± 0.20
0.58 ± 0.18
0.68 ± 0.26
0.35 ± 0.13
0.60 ± 0.28
0.66 ± 0.21
0.49 ± 0.10
0.62 ± 0.13
0.62 ± 0.17
Effect of interventions on final target-fitting loss (regressio
Figure 11. Initial and ﬁnal loss evaluation on MNIST input distribution.
0
50
100
150
200
0.2
0.4
0.6
0.8
1.0
regress
0
50
100
150
200
0.2
0.4
0.6
0.8
weight_decay
0
50
100
150
200
0.05
0.10
0.15
0.20
0.25
0.30
0.35
shrink_and_perturb
0
50
100
150
200
0.2
0.4
0.6
0.8
1.0
reset_last_layer
0
50
100
150
200
0.2
0.4
0.6
0.8
1.0
use_layernorm
0
50
100
150
200
0.2
0.4
0.6
0.8
None
resnet
transformer
cnn
mlp
Accuracy on easy MDP (CIFAR-10)
0
50
100
150
200
0.1
0.2
0.3
0.4
regress
0
50
100
150
200
0.0
0.2
0.4
0.6
weight_decay
0
50
100
150
200
0.06
0.08
0.10
0.12
0.14
shrink_and_perturb
0
50
100
150
200
0.06
0.08
0.10
0.12
0.14
0.16
0.18
reset_last_layer
0
50
100
150
200
0.2
0.4
0.6
0.8
1.0
use_layernorm
0
50
100
150
200
0.2
0.4
0.6
None
resnet
transformer
cnn
mlp
Accuracy on hard MDP (CIFAR-10)
Figure 12. We visualize agent performance on the ‘easy’ and ‘hard’ tasks on MDPs using the CIFAR-10 observation space.
We additionally validate whether these interventions have the potential to interfere with learning on the primary task of
interest in Figures 12 and 13. We observe that the methods which perturb the network weights often interfere with learning,
particularly in the more challenging ‘hard’ reward structure that requires training for several tens of thousands of steps to
make performance improvements. We note that we did not perform extensive hyperparameter tuning for each intervention,
so it is possible that are more carefully tuned optimizer would produce better performance. Instead, these results should be
taken holistically as an indicator of the robustness of an intervention to using a reasonable optimizer as was used to train the
original network to which it is being applied.
B.2. Learning curves for classiﬁcation MDPs
B.2.1. TRAINING ACCURACY
Figure 15 and 14 provides a visualization of the learning curves of different networks during training in the classiﬁcation
MDP experiment. Notably, while performance on the easy and hard reward functions differs dramatically in most agents, the
TD losses behave more similarly. This can be attributed to two properties: ﬁrst, the TD loss changes less in an environment
where the agent receives fewer rewards, so the poorer-performing policies induce easier learning problems. Second, the
value functions of the two problems are very similar even though the optimal policies are very different. As a result, an
‘accurate’ value function with mean squared error of 0.1 can nonetheless correspond to a policy that does no better than
random guessing. The temporal difference losses also give us an indication on whether the agent’s predictions are diverging,
identifying that layer normalization and the two-hot encodings enable signiﬁcantly more stable learning, even though this
does not always correspond to optimal policies.

Understanding plasticity in neural networks
0
50
100
150
200
0.0
0.2
0.4
0.6
0.8
1.0
regress
0
50
100
150
200
0.2
0.4
0.6
0.8
1.0
weight_decay
0
50
100
150
200
0.0
0.2
0.4
0.6
0.8
shrink_and_perturb
0
50
100
150
200
0.2
0.4
0.6
0.8
1.0
reset_last_layer
0
50
100
150
200
0.2
0.4
0.6
0.8
1.0
use_layernorm
0
50
100
150
200
0.2
0.4
0.6
0.8
1.0
None
resnet
transformer
cnn
mlp
Accuracy on easy MDP (MNIST)
0
50
100
150
200
0.1
0.2
0.3
0.4
regress
0
50
100
150
200
0.2
0.4
0.6
0.8
weight_decay
0
50
100
150
200
0.06
0.08
0.10
0.12
0.14
0.16
0.18
shrink_and_perturb
0
50
100
150
200
0.050
0.075
0.100
0.125
0.150
0.175
0.200
reset_last_layer
0
50
100
150
200
0.2
0.4
0.6
0.8
use_layernorm
0
50
100
150
200
0.0
0.2
0.4
0.6
0.8
None
resnet
transformer
cnn
mlp
Accuracy on hard MDP (MNIST)
Figure 13. We visualize agent performance on the ‘easy’ and ‘hard’ tasks on MDPs using the MNIST observation space.
0
50
100
150
200
0.0
0.2
0.4
0.6
0.8
1.0
regress
0
50
100
150
200
0.0
0.2
0.4
0.6
0.8
1.0
weight_decay
0
50
100
150
200
0.0
0.2
0.4
0.6
0.8
1.0
shrink_and_perturb
0
50
100
150
200
0.0
0.2
0.4
0.6
0.8
1.0
reset_last_layer
0
50
100
150
200
0.0
0.2
0.4
0.6
0.8
1.0
use_layernorm
0
50
100
150
200
0.0
0.2
0.4
0.6
0.8
1.0
None
resnet
transformer
cnn
mlp
TD loss on easy MDP (MNIST)
0
50
100
150
200
0.0
0.2
0.4
0.6
0.8
1.0
regress
0
50
100
150
200
0.0
0.2
0.4
0.6
0.8
1.0
weight_decay
0
50
100
150
200
0.0
0.2
0.4
0.6
0.8
1.0
shrink_and_perturb
0
50
100
150
200
0.0
0.2
0.4
0.6
0.8
1.0
reset_last_layer
0
50
100
150
200
0.0
0.2
0.4
0.6
0.8
1.0
use_layernorm
0
50
100
150
200
0.0
0.2
0.4
0.6
0.8
1.0
None
resnet
transformer
cnn
mlp
TD loss on hard MDP (MNIST)
Figure 14. We visualize the TD loss obtained by agents trained on the ‘easy’ and ‘hard’ tasks on MDPs using the MNIST observation
space.
0
50
100
150
200
0.0
0.2
0.4
0.6
0.8
1.0
regress
0
50
100
150
200
0.0
0.2
0.4
0.6
0.8
1.0
weight_decay
0
50
100
150
200
0.0
0.2
0.4
0.6
0.8
1.0
shrink_and_perturb
0
50
100
150
200
0.0
0.2
0.4
0.6
0.8
1.0
reset_last_layer
0
50
100
150
200
0.0
0.2
0.4
0.6
0.8
1.0
use_layernorm
0
50
100
150
200
0.0
0.2
0.4
0.6
0.8
1.0
None
resnet
transformer
cnn
mlp
TD loss on easy MDP (CIFAR-10)
0
50
100
150
200
0.0
0.2
0.4
0.6
0.8
1.0
regress
0
50
100
150
200
0.0
0.2
0.4
0.6
0.8
1.0
weight_decay
0
50
100
150
200
0.0
0.2
0.4
0.6
0.8
1.0
shrink_and_perturb
0
50
100
150
200
0.0
0.2
0.4
0.6
0.8
1.0
reset_last_layer
0
50
100
150
200
0.0
0.2
0.4
0.6
0.8
1.0
use_layernorm
0
50
100
150
200
0.0
0.2
0.4
0.6
0.8
1.0
None
resnet
transformer
cnn
mlp
TD loss on hard MDP (CIFAR-10)
Figure 15. We visualize the TD loss obtained by agents trained on the ‘easy’ and ‘hard’ tasks on MDPs using the CIFAR-10 observation
space.

Understanding plasticity in neural networks
0
50
100
150
200
250
300
350
400
Step
0.0
0.1
0.2
0.3
0.4
0.5
Probe loss
seed 0
t=0
t=10
t=50
t=100
0
50
100
150
200
250
300
350
400
Step
0.0
0.1
0.2
0.3
0.4
0.5
Probe loss
seed 1
t=0
t=10
t=50
t=100
t=150
0
50
100
150
200
250
300
350
400
Step
0.0
0.1
0.2
0.3
0.4
0.5
Probe loss
seed 2
t=0
t=10
t=50
t=100
t=150
CNN losses over training
(regression, no layernorm) easy mnist
0
50
100
150
200
250
300
350
400
Step
0.0
0.1
0.2
0.3
0.4
0.5
Probe loss
seed 0
t=0
t=10
t=50
t=100
t=150
0
50
100
150
200
250
300
350
400
Step
0.0
0.1
0.2
0.3
0.4
0.5
Probe loss
seed 1
t=0
t=10
t=50
t=100
t=150
0
50
100
150
200
250
300
350
400
Step
0.0
0.1
0.2
0.3
0.4
0.5
Probe loss
seed 2
t=0
t=10
t=50
t=100
t=150
CNN losses over training
(regression, no layernorm) easy cifar10
0
50
100
150
200
250
300
350
400
Step
0.0
0.1
0.2
0.3
0.4
0.5
Probe loss
seed 0
t=0
t=10
t=50
t=100
t=150
0
50
100
150
200
250
300
350
400
Step
0.0
0.1
0.2
0.3
0.4
0.5
Probe loss
seed 1
t=0
t=10
t=50
t=100
t=150
0
50
100
150
200
250
300
350
400
Step
0.0
0.1
0.2
0.3
0.4
0.5
Probe loss
seed 2
t=0
t=10
t=50
t=100
t=150
CNN losses over training
(with layernorm) easy mnist
0
50
100
150
200
250
300
350
400
Step
0.0
0.1
0.2
0.3
0.4
0.5
Probe loss
seed 0
t=0
t=10
t=50
t=100
t=150
0
50
100
150
200
250
300
350
400
Step
0.0
0.1
0.2
0.3
0.4
0.5
Probe loss
seed 1
t=0
t=10
t=50
t=100
t=150
0
50
100
150
200
250
300
350
400
Step
0.0
0.1
0.2
0.3
0.4
0.5
Probe loss
seed 2
t=0
t=10
t=50
t=100
t=150
CNN losses over training
(with layernorm) easy cifar10
0
50
100
150
200
250
300
350
400
Step
0.0
0.1
0.2
0.3
0.4
0.5
Probe loss
seed 0
t=0
t=10
t=50
t=100
t=150
0
50
100
150
200
250
300
350
400
Step
0.0
0.1
0.2
0.3
0.4
0.5
Probe loss
seed 1
t=0
t=10
t=50
t=100
t=150
0
50
100
150
200
250
300
350
400
Step
0.0
0.1
0.2
0.3
0.4
0.5
Probe loss
seed 2
t=0
t=10
t=50
t=100
t=150
CNN losses over training
(two-hot encoding with layernorm) easy mnist
0
50
100
150
200
250
300
350
400
Step
0.0
0.1
0.2
0.3
0.4
0.5
Probe loss
seed 0
t=0
t=10
t=50
t=100
t=150
0
50
100
150
200
250
300
350
400
Step
0.0
0.1
0.2
0.3
0.4
0.5
Probe loss
seed 1
t=0
t=10
t=50
t=100
t=150
0
50
100
150
200
250
300
350
400
Step
0.0
0.1
0.2
0.3
0.4
0.5
Probe loss
seed 2
t=0
t=10
t=50
t=100
t=150
CNN losses over training
(two-hot encoding with layernorm) easy cifar10
0
50
100
150
200
250
300
350
400
Step
0.0
0.1
0.2
0.3
0.4
0.5
Probe loss
seed 0
t=0
t=10
t=50
t=100
t=150
0
50
100
150
200
250
300
350
400
Step
0.0
0.1
0.2
0.3
0.4
0.5
Probe loss
seed 1
t=0
t=10
t=50
t=100
t=150
0
50
100
150
200
250
300
350
400
Step
0.0
0.1
0.2
0.3
0.4
0.5
Probe loss
seed 2
t=0
t=10
t=50
t=100
t=150
CNN losses over training
(two-hot encoding) easy mnist
0
50
100
150
200
250
300
350
400
Step
0.0
0.1
0.2
0.3
0.4
0.5
Probe loss
seed 0
t=0
t=10
t=50
t=100
t=150
0
50
100
150
200
250
300
350
400
Step
0.0
0.1
0.2
0.3
0.4
0.5
Probe loss
seed 1
t=0
t=10
t=50
t=100
t=150
0
50
100
150
200
250
300
350
400
Step
0.0
0.1
0.2
0.3
0.4
0.5
Probe loss
seed 2
t=0
t=10
t=50
t=100
t=150
CNN losses over training
(two-hot encoding) easy cifar10
Figure 16. We see markedly different trajectories in networks trained with and without layernorm when tasked with a new optimization
objective.
B.2.2. PROBE TASKS
We include a visualization of the learning curves of some networks on the probe tasks to illustrate the subtlety of measuring
plasticity loss in Figures 16 and 17.
B.3. Qualitative ﬁndings in DDQN
In addition to the gradient covariance analysis presented in the main body, we show more detailed learning curves and
illustrate the evolution of the gradient covariance over time in Figures 19 and 18. We also visualize the spectrum of the
network Hessian in Figures 20 and 21. We observe particularly intriguing trends in the gradient covariance heat maps shown
in Figure 18. We note that the covariance structure of gradients varies signiﬁcantly across environments, networks, and even
random seeds. In many situations, gradients appear to be largely colinear, corresponding to signiﬁcant interference (both
positive and negative) between transitions in a minibatch.
One phenomenon we noticed in several environments was a tendency for network gradients to start off highly colinear, and
then to become more independent later in training. In general, networks which stay in this colinear phase longer are also
those whose learning curves struggle to ‘take off’. Notably, in the game Freeway which is known to produce extremely
high variance outcomes hwerein agents either maximize the game score or fail to learn at all, we saw a one-to-one mapping
between gradient degeneracy and learning progress. All random seeds where the agent made learning progress exhibited
heavier weight on as opposed to off the diagonal, whereas the random seeds which did not ever improve preserved their
initial degenerate gradient structure. A representative example can be observed in the ﬁrst row of Figure 18. In general,
networks with layer normalization exhibited a slight bias towards less degenerate gradients. However, it is not clear whether
gradient degeneracy is a symptom or a cause of performance plateaus. Further investigation into this phenomenon presents
an exciting avenue for future work.
We did not observe any obvious correlations between the spectrum of the Hessian and the agent’s performance, but include
the computed spectra for a single seed of each game in Figures 20 and 21.

Understanding plasticity in neural networks
0
50
100
150
200
250
300
350
400
Step
0.0
0.1
0.2
0.3
0.4
0.5
Probe loss
seed 0
t=0
t=10
t=50
t=100
t=150
0
50
100
150
200
250
300
350
400
Step
0.0
0.1
0.2
0.3
0.4
0.5
Probe loss
seed 1
t=0
t=10
t=50
t=100
t=150
0
50
100
150
200
250
300
350
400
Step
0.0
0.1
0.2
0.3
0.4
0.5
Probe loss
seed 2
t=0
t=10
t=50
t=100
t=150
CNN losses over training
(regression, no layernorm) hard mnist
0
50
100
150
200
250
300
350
400
Step
0.0
0.1
0.2
0.3
0.4
0.5
Probe loss
seed 0
t=0
t=10
t=50
t=100
t=150
0
50
100
150
200
250
300
350
400
Step
0.0
0.1
0.2
0.3
0.4
0.5
Probe loss
seed 1
t=0
t=10
t=50
t=100
t=150
0
50
100
150
200
250
300
350
400
Step
0.0
0.1
0.2
0.3
0.4
0.5
Probe loss
seed 2
t=0
t=10
t=50
t=100
t=150
CNN losses over training
(regression, no layernorm) hard cifar10
0
50
100
150
200
250
300
350
400
Step
0.0
0.1
0.2
0.3
0.4
0.5
Probe loss
seed 0
t=0
t=10
t=50
t=100
t=150
0
50
100
150
200
250
300
350
400
Step
0.0
0.1
0.2
0.3
0.4
0.5
Probe loss
seed 1
t=0
t=10
t=50
t=100
t=150
0
50
100
150
200
250
300
350
400
Step
0.0
0.1
0.2
0.3
0.4
0.5
Probe loss
seed 2
t=0
t=10
t=50
t=100
t=150
CNN losses over training
(with layernorm) hard mnist
0
50
100
150
200
250
300
350
400
Step
0.0
0.1
0.2
0.3
0.4
0.5
Probe loss
seed 0
t=0
t=10
t=50
t=100
t=150
0
50
100
150
200
250
300
350
400
Step
0.0
0.1
0.2
0.3
0.4
0.5
Probe loss
seed 1
t=0
t=10
t=50
t=100
t=150
0
50
100
150
200
250
300
350
400
Step
0.0
0.1
0.2
0.3
0.4
0.5
Probe loss
seed 2
t=0
t=10
t=50
t=100
t=150
CNN losses over training
(with layernorm) hard cifar10
0
50
100
150
200
250
300
350
400
Step
0.0
0.1
0.2
0.3
0.4
0.5
Probe loss
seed 0
t=0
t=10
t=50
t=100
t=150
0
50
100
150
200
250
300
350
400
Step
0.0
0.1
0.2
0.3
0.4
0.5
Probe loss
seed 1
t=0
t=10
t=50
t=100
t=150
0
50
100
150
200
250
300
350
400
Step
0.0
0.1
0.2
0.3
0.4
0.5
Probe loss
seed 2
t=0
t=10
t=50
t=100
t=150
CNN losses over training
(two-hot encoding with layernorm) easy mnist
0
50
100
150
200
250
300
350
400
Step
0.0
0.1
0.2
0.3
0.4
0.5
Probe loss
seed 0
t=0
t=10
t=50
t=100
t=150
0
50
100
150
200
250
300
350
400
Step
0.0
0.1
0.2
0.3
0.4
0.5
Probe loss
seed 1
t=0
t=10
t=50
t=100
t=150
0
50
100
150
200
250
300
350
400
Step
0.0
0.1
0.2
0.3
0.4
0.5
Probe loss
seed 2
t=0
t=10
t=50
t=100
t=150
CNN losses over training
(two-hot encoding with layernorm) hard cifar10
0
50
100
150
200
250
300
350
400
Step
0.0
0.1
0.2
0.3
0.4
0.5
Probe loss
seed 0
t=0
t=10
t=50
t=100
0
50
100
150
200
250
300
350
400
Step
0.0
0.1
0.2
0.3
0.4
0.5
Probe loss
seed 1
t=0
t=10
t=50
t=100
t=150
0
50
100
150
200
250
300
350
400
Step
0.0
0.1
0.2
0.3
0.4
0.5
Probe loss
seed 2
t=0
t=10
t=50
t=100
t=150
CNN losses over training
(two-hot encoding) hard mnist
0
50
100
150
200
250
300
350
400
Step
0.0
0.1
0.2
0.3
0.4
0.5
Probe loss
seed 0
t=0
t=10
t=50
t=100
t=150
0
50
100
150
200
250
300
350
400
Step
0.0
0.1
0.2
0.3
0.4
0.5
Probe loss
seed 1
t=0
t=10
t=50
t=100
t=150
0
50
100
150
200
250
300
350
400
Step
0.0
0.1
0.2
0.3
0.4
0.5
Probe loss
seed 2
t=0
t=10
t=50
t=100
t=150
CNN losses over training
(two-hot encoding) hard cifar10
Figure 17. We see markedly different trajectories in networks trained with and without layernorm when tasked with a new optimization
objective.

Understanding plasticity in neural networks
Figure 18. Gradient covariance plots vs performance for three sample games from the Arcade Learning Environment, which highlight the
role of the gradient structure in learning progress. We ﬁnd that when agents fail to learn, they tend to exhibit highly degenerate gradient
structure, corresponding to the large off-diagonal values in the heatmaps visualized here.

Understanding plasticity in neural networks
Figure 19. Gradient covariance plots vs performance for three sample games from the Arcade Learning Environment, which highlight the
role of the gradient structure in learning progress. In games where agents make consistent positive progress increasing their returns, we
see covariance structures with heavier weight on the diagonal.

Understanding plasticity in neural networks
Figure 20. Visualization of Hessian approximations for a double DQN agent without layer normalization.

Understanding plasticity in neural networks
Figure 21. Visualization of Hessian approximations for a double DQN agent with layer normalization.

