Alternating cyclic extrapolation methods for
optimization algorithms
Nicolas Lepage-Saucier∗
April 2021
Abstract
This article introduces new acceleration methods for ﬁxed-point iterations.
Speed and stability are achieved by alternating the number of mappings to com-
pute step lengths and using them multiple times by cycling. A new type of step
length is also proposed with good properties for nonlinear mappings. The meth-
ods require no speciﬁc adaptation and are especially eﬃcient for high-dimensional
problems. Computation uses few objective function evaluations, no matrix inver-
sion and little extra memory. A convergence analysis is followed by seven appli-
cations, including gradient descent acceleration for unconstrained optimization.
Performances are on par or better than alternatives.
The algorithm is available as a stand-alone Julia package and may be down-
loaded at https://github.com/NicolasL-S/ACX.
Keywords: ﬁxed point; mapping; extrapolation; nonlinear optimization; acceler-
ation technique; ﬁrst order method; vector sequences; gradient descent; quasi-
Newton
1
Introduction
Let F : Rn →Rn with n ∈N + denote a mapping which admits continuous, bounded
partial derivatives. Finding a ﬁxed point of F, x∗: F(x∗) = x∗, is the basis of countless
applications in disciplines like statistics, computer science, physics, biology and eco-
nomics, driving the development of many general and domain-speciﬁc iterative schemes.
For reviews and insightful comparisons of old and recent methods, see notably Jbilou
and Sadok (2000), Rami`ere and Helfer (2015), Brezinski et al. (2018) and the textbook
by Brezinski and Redivo-Zaglia (2020).
The starting point for the approach proposed in this paper is a vector version of
Aitken’s ∆2 process, usually attributed to Lemar´echal (1971) but also discovered by
Irons and Tuck (1969) and Jennings (1971):
∗Toulouse School of Economics
1
arXiv:2104.04974v1  [math.OC]  11 Apr 2021

xk+1 = xk −⟨∆2xk, ∆xk⟩
||∆2xk||2
∆xk,
where ∆xk = F(xk) −xk, ∆xp
k = ∆p−1F(xk) −∆p−1xk for p > 1, ||y|| = √y⊺y is the
2-norm of a vector y and ⟨y, z⟩= y⊺z is the inner product of vectors y and z.
Barzilay and Borwen (1988) also proposed a similar method which requires a single
mapping per iteration by using the Cauchy step length of the previous iteration. Known
as the Barzilay-Borwein (BB) method, it spawned a rich line of research in gradient-
based methods for linear problems. A branch of this literature investigates the link
between the optimal step size and the Hessian spectral properties (see for instance
Birgin et al. (2014)) to which this paper is also relevant.
Lemar´echal’s method may be interpreted as a simpliﬁed quasi-Newton method
xk+1 = xk −M −1
k ∆xk
where Mk is the approximation of the Jacobian of ∆xk taking the form Mk = s−1
k I.
The Jacobian is approximated by the secant method using two consecutive evaluations
of the mapping F:
d∆xk
dxk
≈∆2xk
∆xk
.
The step length sk is
sk = arg min
s
s−1I −∆2xk
∆xk

2
= ⟨∆2xk, ∆xk⟩
||∆2xk||2
.
(1)
Early on, this acceleration proved to be generally faster and more stable than compa-
rable techniques (see Henrici (1964) or Macleod (1986)).
Our main contribution is to consider a new way of computing step lengths designed
to target speciﬁc error components to speed up subsequent convergence. For exposition,
let us consider a system of linear equations
Ax = b,
x ∈Rn
where A ∈Rn×n, b ∈Rn, n ≥1.
The solution of the system also constitutes the
minimizer to the quadratic function f(x) = 1
2x⊺Ax−x⊺b with gradient ▽f(x) = Ax−b
and Hessian A. To avoid the need of a change of coordinate, assume A = diag(λ1...λn)
is diagonal with positive entries of diﬀerent magnitudes. Also assume A has m ≤n
distinct eigenvalues with the smallest and largest labeled λmin and λmax, respectively.
The problem may be formulated as ﬁnding the ﬁxed point to the mapping
F(x) = x −(Ax −b),
with unique solution x∗representing a ﬁxed point of F at which Ax∗= b. To study
the convergence of Lemar´echal’s method, deﬁne the error as a deviation from the ﬁxed
2

point: e = x −x∗. Direct computation gives ∆x = −(Ax −b) = −Ae and, in general,
∆px = (−A)pe for p ∈N +. Since A is diagonal, sk may be expressed explicitly as
sk = ⟨A2ek, −Aek⟩
||A2ek||2
= −e⊺
kA3ek
e⊺
kA4ek
= −
Pn
i=1

e(i)
k λ2
i
2 1
λi
Pn
i=1

e(i)
k λ2
i
2 .
(2)
The error at iteration k + 1 can be written compactly as
ek+1 = (I + skA) ek
and its jth component may be individually expressed as
e(j)
k+1 = (1 + skλj) e(j)
k ,
j = 1, ..., n.
(3)
As shown by equation (3), if sk was somehow set exactly equal to −1
λj , the error e(j)
k+1
would be perfectly annihilated. It would also remain zero for all subsequent iterations,
regardless of sk+1, sk+2, .... Since A has m distinct eigenvalues, all error components
could be successively reduced to zero in m iterations. Conversely, from a starting point
with at least one positive error component e(j)
0
for each distinct eigenvalue, m is also the
minimum number of steps necessary to annihilate all e(j) exactly. Of course, as shown
by (2), sk ∈[−λ−1
min, −λ−1
max] is in fact the negative weighted average with more weight
for larger eigenvalues and larger error components. As a result, all e(j) get imperfectly
reduced simultaneously at each iteration, which may lead to slow convergence if A has
a wide spectrum. Instead, a new method is suggested to target successively speciﬁc
parts of the spectrum of A to improve convergence.
Note that at the ﬁxed point x∗, any higher order diﬀerences ∆px is zero, not only
∆1x.
The rate of change of ∆px for p > 1 also carries information which may be
exploited for optimization. Deﬁne a step length of order p as
s(p) = ⟨∆px, ∆p−1x⟩
||∆px||2
(4)
where s(2) ≡s deﬁned in (1). In the linear example, a “cubic” step length s(3)
k
would
be
s(3)
k
= −e⊺
kA5ek
e⊺
kA6ek
= −
Pn
i=1

e(i)
k λ3
i
2 1
λi
Pn
i=1

e(i)
k λ3
i
2 .
Obviously, s(3)
k
puts more weights eigenvalues of larger magnitudes. That should anni-
hilate their associated error terms more aggressively and reduce their weights in subse-
quent steps computations. Then, if this cubic iteration at step k is followed by a regular
squared iteration at step k+1, that s(2)
k+1 step length should target more precisely errors
associated with smaller eigenvalues. Schemes based on step length s(2) alone rely on
3

the hope that ∆x changes at similar rates for all components of e, which is not the
case if A has a large spectrum. Starting with an s(3)
k
extrapolation selects a path in
parameter space where the following s(2)
k+1 extrapolation suﬀers less from this source
of error. A single cubic iteration may not instantly converge faster, but alternating
between diﬀerent orders may dynamically enhance convergence over time.
Obviously, computing higher-order step lengths s(p) requires more mappings.1 Thank-
fully, this cost can be mitigated by the ability to reuse s(p) for many extrapolations
(Friedlander et al. (1999)), thanks to the important idea of cycling pioneered by Ray-
dan and Svaiter (2002). Since the iterates of the Barzilai-Borwein method are them-
selves mappings with the same ﬁxed point x∗, Raydan and Svaiter (2002) noted that
the same step length could be used for a second extrapolation with little extra com-
putational cost. Deﬁne an intermediate series yk,(i) constructed from accelerations of
xk, F(xk), F(F(xk)), ...: yk,(1) = xk −sk∆xk, yk,(2) = F(xk) −sk∆F(xk), ... . Note that
the series yk,(1), yk,(2), ... also converges to x∗and can therefore be extrapolated in the
same fashion using the same step length:
yk+1 = yk,(1) + sk(yk,(2) −yk,(1))
After substitution, the squared cyclic extrapolation can be written as
xk+1 = yk+1 = xk −2sk∆xk + s2
k∆2xk.
The authors showed that this new scheme, labeled the Cauchy-Barzilai-Borwein
(CBB) method, has good convergence properties.
It was successfully adapted to a
variety of nonlinear contexts by Varadhan and Rolland (2004, 2005 and 2008), notably
to accelerate the expectation maximization algorithm (EM) (Ortega (1970), Dempster
et al. (1977)) under the label SQUAREM.
For a p-order step length, cycling may be performed p times in the same recursive
manner. In the linear system previously deﬁned, with a step s(p)
k
and p-order cycling,
the error ek+1 becomes
ek+1 =

I + s(p)
k A
p
ek.
By alternating between diﬀerent extrapolation orders, the proposed scheme is best
understood as an alternating cyclic extrapolation method (ACX). The next section
describes the algorithm formally and introduces a new type of step length with good
properties for ACX in nonlinear contexts.
Section 3 establishes Q-linear convergence of ACX for linear mappings in a suit-
able norm and discusses its convergence properties in nonlinear contexts. Section 4
follows with stability issues and implementation detail, notably an adaptation of ACX
to gradient descent acceleration with dynamic adjustment of the descent step size.
1It may explain why few acceleration schemes with third-order or even higher-order diﬀerences have
been put forth. Notable exceptions are Marder and Weitzner (1970), Lebedev and Zabelin (1995), and
Brezinski and Chehab (1998).
4

ACX has many advantages. It can be used “as-is” for a wide variety of problems
without extra specialization and close to no tuning. It requires few to no objective func-
tion evaluations, no matrix inversion, uses little extra memory and shows good empirical
performances. Section 5 presents 7 applications. Most are high-dimensional problems
of the sort that have become ever more prevalent with the proliferation of large datasets
and sparse data structures. These stand to beneﬁt most from quasi-Newton numerical
methods that avoid the inversion of large Hessians. For gradient descent acceleration,
ACX is used to accelerate a high-dimensional logistic regression, a 1000-parameter
Rosenbrock function and 87 unconstrained problems from the CUTEst collection (see
Bongartz et al. (1995)). ACX’s performances are compared with the limited-memory
Broyden–Fletcher–Goldfarb–Shanno algorithm (L-BFGS) (see Liu and Nocedal (1989)
and Nocedal and Wright (2006)) and a nonlinear conjugate gradient method (CG) pro-
posed by Hager and Zhang (2006). For acceleration of general ﬁxed-point iterations, the
four problems selected are the expectation maximization (EM) for Poisson admixture,
alternating least squares (ALS) for canonical tensor decomposition, the power method
for computing dominant eigenvalues and the method of alternating projections (von
Neumann (1950), Halperin (1962)) applied to regression with high-dimensional ﬁxed
eﬀects. The performances of ACX is compared to competitive general purpose accel-
eration algorithms: the quasi-Newton acceleration of Zhou et al. (2011), the objective
acceleration approach of Riseth (2019) and the Anderson Acceleration version of Hen-
derson and Varadhan (2019). For the high-dimensional ﬁxed-eﬀect regression, ACX is
compared with equivalent packages in various programming languages.
ACX is also available as a stand-alone Julia package at github.com/NicolasL-S/ACX.
2
Alternating-orders cyclic extrapolations
A p-order ACX may be synthesized as:
xk+1 =
p
X
i=0
 p
i

(σ(p)
k )i∆ixk
p ≥2
(5)
where ∆0xk = xk,
 p
i

=
p!
i!(p−i)! is a binomial coeﬃcient, and σ(p)
k
= |s(p)
k | ≥0 is
the absolute value of the step length (4). Two schemes based on (5) are considered:
ACX3,2, which alternates between cubic (p = 3) and squared (p = 2) extrapolations,
and ACX3,3,2 which performs two cubic extrapolations before one squared extrapolation.
While these display good overall empirical properties, higher order extrapolations or
other sequences could be considered in speciﬁc contexts.
Step lengths other than s (1) (sometimes referred to as sBB2), have been suggested
in the literature. Barzilay and Borwen (1988) suggested sBB1 = ||∆x||2/⟨∆2x, ∆x⟩and
Roland and Varadhan (2008) introduced sRV = −
√
sBB1sBB2 = −||∆x||/||∆2x|| for
general optimization problems. They note that for nonlinear mappings, using sBB1 can
compromise stability since the denominator ⟨∆2x, ∆x⟩may be close to zero or even
5

positive. Similarly, sBB2 may be problematic if ⟨∆2x, ∆x⟩is positive and ||∆2x||2 is
small. In contrast, sRV has a guaranteed negative sign for better overall stability. In the
case of ACX3,2 and ACX3,3,2 however, an even better choice turns out to be −|s(p)| =
−σ(p). It is a simple way of avoiding wrong signs with better overall convergence. Given
the growing literature on optimal step sizes for descent algorithms (see for instance Dai
et al. (2019) for a recent contribution), other options could also be explored in the
future.
The ACX algorithm is formalized as follows.
Algorithm 1 Given x0 ∈Rn, a vector of orders (p1, ..., pP)
pj ∈{2, 3}, this algorithm
produces a sequence xk converging to a ﬁxed point of the mapping F : Rn →Rn.
1
for k = 0, 1, 2, ... until convergence
2
pk = (k mod P) + 1
3
∆0 = xk
4
∆1 = F(xk) −xk
5
∆2 = F 2(xk) −2F(xk) + xk
6
If pk = 3: ∆3 = F 3(xk) −3F 2(xk) + 3F(xk) −xk
7
σ(pk)
k
= |⟨∆pk, ∆pk−1⟩|/ ∥∆pk∥2
8
xk+1 = Ppk
i=0
 pk
i

(σ(pk)
k
)i∆i
9
end for
Note: To improve global convergence, constraints on σ(pk)
k
may be imposed at step 7
and bounds checking on xk+1 at step 8. Also, a preliminary mapping before step 3
may improve the convergence of certain algorithms (see Section 4 for implementation
detail).
To illustrate the advantage of alternating orders, let us revisit the linear example
of Barzilay and Borwen (1988) with A = diag(20, 10, 2, 1), b = (1, 1, 1, 1)⊺and starting
point x0 = (0, 0, 0, 0)⊺. The performance of ACX2, a purely squared scheme equivalent
to the CBB method in the linear case, can be compared with the convergence of ACX3,2.
The stopping criterion is ∥∆xk∥2 ≤10−8.
Figure 1 shows the trajectory of |e1| =
|x1 −x∗
1| = |x1 −1/20| on the horizontal axis and |e2| = |x2 −x∗
2| = |x2 −1/10| on
the vertical axis for both algorithms. The starting point is (|e1|, |e2|) = (1/20, 1/10) at
the top right. The ACX2 method needs 34 gradient evaluations to pass the stopping
criterion, but the ACX3,2 only needs 20 (while the BB method requires 25 gradient
evaluations and steepest descent needs 314). On the ACX2 trajectory, shown in dotted
lines, each iteration accomplishes a mild reduction of the errors. Substantial decrease
of |e2| only occurs at iteration 5. The ACX3,2 trajectory in solid lines shows errors
being annihilated more aggressively; |e1| and |e2| must only be reduced substantially
twice before convergence. While this example illustrates well the idea of alternating,
its beneﬁts are more systematic for general nonlinear problems with many parameters
as will be seen in Section 5.
6

10−18
10−15
10−12
10−9
10−6
10−3
100
|e1|
10−18
10−15
10−12
10−9
10−6
10−3
100
|e2|
ACX2
Square. extr.
ACX3, 2
Cubic extr.
Square extr.
Figure 1: Convergence of |e1| and |e2| for
the linear system, from initial values of
(1/20, 1/10) towards the solution (0, 0) for
ACX2 and ACX3,2
0.00
0.25
0.50
0.75
1.00
x1
0.00
0.25
0.50
0.75
1.00
x2
ACX3, 2
Square. extr.
Cubic extr.
Figure 2: Convergence of ACX3,2 for the
2-parameter Rosenbrock function from
(0,0).
3
Convergence of ACX
Before exploring nonlinear applications, it is useful to study the convergence of ACX
for linear systems of equations. It is a good approximation for the behavior of a general
mapping F around its ﬁxed point x∗. Raydan and Svaiter (2002) has shown that the
CBB method converges Q-linearly in an appropriate norm. The following proof extends
the result to any ACX algorithm.
Consider the linear system of equations deﬁned Qx = b where Q is symmetric
positive deﬁnite. Deﬁne the elliptic norm
∥x∥Q−1 =
p
x⊺Q−1x
induced by the inner product ⟨., .⟩Q−1
⟨x, y⟩Q−1 = x⊺Q−1y.
The inner product satisﬁes the Cauchy-Schwarz inequality:
∥x∥2
Q−1 · ∥y∥2
Q−1 ≥⟨x, y⟩2
Q−1
for vectors x, y.
To simplify the following computation, let us introduce the function
qp(e) = e⊺Qpe.
7

for some vector e and the special case qp(ek) ≡qp for ek. Since Q is positive deﬁnite,
qp(e) ≥0 ∀p ∈R. Observe notably that ||e||2
Q−1 = e⊺Q−1e = q−1(e).
Finally, let us introduce a useful lemma which is proven in appendix:
Lemma 1 For x, y, elements of a commutative ring, and p ∈N+ \{0, 1}, (x + y)p may
be decomposed as
(x + y)p = xp + yp −
⌊p/2⌋
X
i=1
p(p −i −1)!(−xy)i
i!
(x + y)p−2i
(p −2i)! .
(6)
where ⌊p/2⌋outputs the greatest integer less than or equal to p/2.
Theorem 1 The sequence {xk} generated by the ACXp1,...,pP method (5) applied to the
mapping F(x) = x −(Qx −b) converges Q-linearly in the norm Q−1 for any set of
values p1, ..., pP with pj ≥2.
Proof. After a p-order extrapolation, the error ek+1 may be expressed as
ek+1 = (I −σ(p)
k Q)pek,
where p ≥2 is the extrapolation order at iteration k and σ(p)
k
=
|−e⊺
kQ2p−1ek|
e⊺
kQ2pek
= q2p−1
q2p .
The squared Q−1 norm of ek+1 is
||ek+1||2
Q−1 = ||(I −σ(p)
k Q)pek||2
Q−1 = e⊺
k(I −σ(p)
k Q)pQ−1(I −σ(p)
k Q)pek.
Rearrange the terms on the right hand side:
∥ek+1∥2
Q−1 = e⊺
kQ−1(I −σ(p)
k Q)(I −σ(p)
k Q)2p−1ek.
Rewrite the last parenthesis of the right hand side using (6) to get
∥ek+1∥2
Q−1 = e⊺
kQ−1(I −σ(p)
k Q)
"
I + (−σ(p)
k Q)2p−1−
P⌊(2p−1)/2⌋
i=1
c(p, i)(σ(p)
k Q)i(I −σ(p)
k Q)2p−1−2i
#
ek.
where c(p, i) = (2p−1)(2p−i−2)!
i!(2p−1−2i)! . Note that p ∈N+ \ {0, 1} →⌊(2p −1)/2⌋= p −1.
Multiplying back the terms outside the bracket and simplifying using the qp(e) notation,
we get
q−1(ek+1) =
q−1 −σ(p)
k q0 −(σ(p)
k )2p−1(q2p−2 −σ(p)
k q2p−1)
−Pp−1
i=1 c(p, i)(σ(p)
k )iq−1(Qi/2(I −σ(p)
k Q)p−iek).
where q−1(Qi/2(I −σ(p)
k Q)p−iek) = e⊺
k(I −σ(p)
k Q)p−iQi−1(I −σ(p)
k Q)p−iek. Factoring
the ﬁrst q−1 of the right hand side, rewrite the expression as
q−1(ek+1) = q−1 · (1 −θ1 −θ2 −θ3)
where
8

• θ1 = σ(p)
k
q0
q−1
• θ2 =
(σ(p)
k
)2p−1
q−1
(q2p−2 −σ(p)
k q2p−1) =
(σ(p)
k
)2p−1
q−1q2p (q2p−2q2p −q2
2p−1)
• θ3 = Pp−1
i=1 c(p, i)(σ(p)
k )i q−1(Qi/2(I−σ(p)
k
Q)p−iek)
q−1(ek)
.
Let us now show that θ1 ≥
λmin
λmax, θ2 ≥0, and θ3 ≥0.
Note that θ1 may be
written as the ratio of two Rayleigh quotients, with σ(p)
k
=
q2p−1
q2p
=

q2p
q2p−1
−1
=

e⊺
kQp−1/2QQp−1/2ek
e⊺
kQp−1/2Qp−1/2ek
−1
∈[λ−1
max, λ−1
min] and
q0
q−1 =
e⊺
kQ−1/2QQ−1/2ek
e⊺
kQ−1/2Q−1/2ek
∈[λmin, λmax]. Hence,
θ1 = q2p−1
q2p
q0
q−1 ∈
h
λmin
λmax, λmax
λmin
i
.
Further, note that
q2p−2 = e⊺
kQ2p−2ek = ||Qp−1/2ek||2
Q−1
q2p = e⊺
kQ2pek = ||Qp+1/2ek||2
Q−1
q2p−1 = e⊺
kQ2p−1ek = ⟨Qp+1/2ek, Qp−1/2ek⟩Q−1.
By the Cauchy-Schwarz inequality,
q2p−2q2p −q2
2p−1 = ||Qp−1/2ek||2
Q−1 · ||Qp+1/2ek||2
Q−1 −⟨Qp+1/2ek, Qp−1/2ek⟩2
Q−1 ≥0.
Hence,
θ2 = (σ(p)
k )2p−1
q−1q2p
(q2p−2q2p −q2
2p−1) ≥0.
Finally, for θ3, note that c(p, i) > 0 for p ≥2, i ≤p−1 and that all
q−1(Qi/2(I−σ(p)
k
Q)p−iek)
q−1(ek)
terms are Rayleigh quotients with minimum of zero when σ(p)
k
=
1
λr , r ∈{1, ..., n}.
Thus, we have
∥ek+1∥Q−1
∥ek∥Q−1
=
p
1 −θ1 −θ2 −θ3 ≤
r
λmax −λmin
λmax
which establishes the result.
We may convince ourselves that this linear convergence result is representative of
any nonlinear mapping F in a neighborhood close to its ﬁxed point x∗. Taking a p-order
Taylor approximation,
F p(x) = x∗+ Jp · (x −x∗) + o(x −x∗)
where J is the Jacobian of F at x∗. For a p-order diﬀerence, we have
∆px = (J −I)pe + o(e).
9

Applying a p-order extrapolation from xk (near x∗), the error at iteration k + 1 may be
expressed as
ek+1 = (I −σ(p)
k (I −J))pek + o(ek).
Hence, up to a ﬁrst-order approximation, ACX3,2 and ACX3,3,2 for nonlinear map-
pings should exhibit similar convergence in an appropriately close neighborhood of x∗.
Theorem 1 also shows that from any starting point xk in the neighborhood of x∗,
all p-order extrapolation show the same worst case scenario of
∥ek+1∥Q−1
∥ek∥Q−1
=
q
λmax−λmin
λmax
.
What diﬀerentiates ACX is its ability to make better scenarios more likely.
Away from x∗, J(x) may change substantially between each mapping, and even more
between extrapolations. Convergence is of course not guaranteed. But alternating be-
tween squared and cubic extrapolations may be advantageous like hybrid optimization
algorithms are. Switching among constituent algorithms helps escape situations such
as zigzagging or locally ﬂat objective functions where a single algorithm would strug-
gle. In fact, Roland and Varadhan (2004) did mention that SQUAREM – a purely
squared extrapolation scheme – sometimes experiences near stagnation or breakdown
when ∆x and ∆2x are nearly orthogonal and sBB1 or sBB2 are used. For ACX3,2 and
ACX3,3,2, the probability that ∆x and ∆2x be orthogonal and that ∆2x and ∆3x also
be orthogonal is lower.
4
Implementation detail and stability
This section discusses implementation detail of ACX in various situations. The sug-
gested parameters show good empirical properties, but other sets of parameter values
could probably perform well too.
Adaptive step size for gradient descent acceleration
Consider minimizing the function f by gradient descent. The mapping is
F(x) = x −α∇f(x).
To be amenable to ACX acceleration, the descent step size α must of course be
constant within each extrapolation cycle. Since ACX is based on second and third
order diﬀerences in mappings, it must also remain within an acceptable range of values.
An excessively small α may lead to small ∆2x or ∆3x and large σ(2) or σ(3), resulting
in imprecise extrapolations when f has weak curvature. An excessively large α could
on the other hand lead to zigzagging and at worst divergence of the algorithm. Here
is a simple adaptive procedure to automatically choose a reasonable α and ensure σ(p)
remains within reasonable bounds as often as possible.
The initial step size is the
largest possible α1 which satisﬁes the Armijo–Goldstein condition (Armijo (1966)).2
2Also, to stabilize the start of ACX3,2 and ACX3,3,2, σ(2)
1
is compute at the ﬁrst iteration. If it is
below 1, a sign α is too large, the algorithm starts with a squared extrapolation (an interpolation in
that case) rather than a cubic one.
10

Then, after each after iteration k, set αk+1 = αkθ if σ(p)
k
< Lσ and αk+1 = αk/θ if
σ(p)
k
> ¯Lσ where θ > 1 and where Lσ and ¯Lσ > Lσ are lower and upper thresholds
within which σ(p)
k
should preferably remain. The rational for this simple step function
is that σ(p)
k
can take very large or very small values for a single iteration. Overcorrecting
αk+1 would be a disproportionate response and lead to worse results in the following
iterations. In the empirical section, the parameters are θ = 1.5, Lσ = 1 and ¯Lσ = 2,
and the Armijo-Goldstein constant is 0.25.3
Accelerating mappings other than gradient descent
Constraining σ(p)
For mappings with guaranteed improvement in the objective, such as the EM algorithm
or gradient descent with exact line search, the step length may be constrained to σ(p) =
max(1, σ(p)). Otherwise, if |⟨∆px, ∆p−1x⟩| is close to zero, σ(p) may be small as well and
progress may be slow. If the underlying mapping has guaranteed progress, then σ(p) = 1
ensures that ACX makes the same progress as the last mapping.4
This constraint
will be used to improve ACX convergence in the ALS application for canonical tensor
decomposition.
Stabilization mappings
In many mapping applications such as the EM or MM algorithm (Lange (2016)) or ALS,
the mapping F(xk) takes the form of a constrained optimization given the parameters
of the previous iteration. For example, for the MM algorithm, the objective function
f(x) is maximized iteratively by the constrained maximization of a surrogate function
g(x). The mapping is F(xk) = arg maxx g(x|xk−1). Since the starting point x0 is not
in general a constrained maximum, the value of the objective function f can improve
signiﬁcantly following the ﬁrst iteration. In the subsequent iterations, progress is typ-
ically much slower as xk steadily converges from constrained maximum to constrained
maximum toward its ﬁxed point. This also means that the change in x after the ﬁrst
mapping may be sizable and comparatively modest in the following iterations. Since
ACX relies on extrapolation, using this initial mapping may provide little information
on the true direction of the ﬁxed point. To improve the accuracy of the extrapolation,
3Note that near the optimum, if αk is small, ∆pxk may be too small for machine precision and lead
to an imprecise σ(p)
k . This is prevented with a progressive approach. Whenever ||∆pxk||∞is small
considering the available machine precision (||∆pxk||∞< 10−50 in the applications), σ(p)
k
is set to 1
and αk+1 is set to min(1, 21+tαk), where t is the total number of times the same situation has occurred
in the past.
4This is also suggested by Roland and Varadhan (2008). Note that it would not be appropriate for
applications where we often have σ(p) ∈(0, 1), such as the mapping operators considered by Lemar´echal
(1971) with Lipschitz constants L ∈(0, 1). To limit zigzagging, one solution is to replace F by F ◦F,
like in the ALS application.
11

an initial “stabilization mapping” may be computed before each extrapolation. In prac-
tice of course, it is diﬃcult to anticipate whether convergence will improve suﬃciently
to warrant the investment in this extra mapping. In the applications, it was very bene-
ﬁcial for EM algorithm acceleration with ACX2, and only mildly beneﬁcial for ACX3,2
and ACX3,3,2.
Non-monotonicity
Like most extrapolation algorithms, ACX does not guarantee steady improvement in
the objective at every iteration. To guarantee global convergence, Roland and Varadhan
(2008) focus on accelerating algorithms with guaranteed progress. In case of a worsening
of the objective after an extrapolation, they suggest starting from the last mapping
and successively halving the distance between sk and −1, (sk →(−1 + sk)/2) until the
extrapolation yields an improvement in the objective. The monotonicity and global
convergence of the EM/MM algorithms in the relevant part of the parameter space
guarantees the monotonicity and global convergence of the accelerated schemes as well.
The same strategy would naturally extend to ACX.
Although theoretically appealing, it rarely turned out to be beneﬁcial in practice.
To illustrate this, Figure 2 shows the convergence of ACX3,2 for the two-variable Rosen-
brock function starting from (0, 0). As can be seen, the most fruitful steps toward the
minimum (1, 1) are also causing temporary setbacks in the objective.
But the lost
progress is quickly regained a few steps later. Ample testing has convinced us that re-
ducing the size of those steps to enforce monotonicity almost always slows convergence,
sometimes dramatically. Since a fast algorithm is more valuable than a slow monotonic
one, it was not implemented.
Backtracking
While monotonicity may be undesirable, the algorithm must not reach parameter values
where the gradient is undeﬁned or inﬁnite.5 In such cases, backtracking and reducing
step lengths is necessary. Since such outcomes generally occur after extremely large
steps, an aggressive reduction of σ(p)
k
is preferable (set to a factor of 10 in the applica-
tions). For schemes with guaranteed progress in the objective, progressively reducing
the distance between σ(p)
k
and 1 (σ(p)
k
→(0.9 + 0.1σ(p)
k ) is enough to guarantee im-
provement in the objective. For gradient descent acceleration, the distance between
σ(p)
k
and 0 must be divided (σ(p)
k
→0.1σ(p)
k ), until f(xk+1) < f(xk). As σ(p)
k
→0, the
ACX scheme with backtracking tends to a normal gradient descent with backtracking
because higher order terms ∆pxk vanish. To illustrate this, consider a p-order extrap-
olation with reduced step length σ(p)
k /ζ where ζ = 1, 10, 100, .... The resulting xk+1
5This has happened in 4 CUTEst problems and no other applications.
12

is
xk+1 =
p
X
i=0
 p
i
(σ(p)
k )i
ζi
∆ixk
= xk + pσ(p)
k ζ−1∆xk +
p
X
i=2
 p
i
(σ(p)
k )i
ζi
∆ixk
= xk −pσ(p)
k ζ−1(xk −F(xk)) + o
 ζ−1
as ζ →∞.
where for gradient descent, we would have xk −F(xk) = αk▽f(xk). Note that for
gradient descent acceleration, since divergence may also be caused by an excessively
large step size αk, αk also gets divided by 2.
After backtracking, the algorithm should restart from the iterate with the lowest
objective value. In practice however, it is not necessary to compute the value of the
objective at every step. In the applications, it is computed every other step. In case
the algorithm leads to an extreme value, the algorithm backtracks to the iterate with
the best computed objective and the step lengths σ(p)
k
and αk are reduced in every
following iterations. If the objective eventually improves, the algorithm resumes with
normal step lengths. But if there is a new infeasible value before an improvement in
the objective, the algorithm backtracks again with σ(p)
k
and αk being reduced again.
Bounds checking
Stalling may occur if an extrapolation leads to a saddle point or a portion of the
parameter space where although F(x) may be deﬁned, ∆x is very small. Bound checks
may be a way to avoid this.
They would also be necessary if ACX was extended
to constrained optimization. Let S ∈Rn be the set of feasible starting points that
may be represented as the Cartesian product of n open intervals: S = Qn
i=1 I(i) where
I(i) = (x(i)
min, x(i)
max) i ∈(1, ..., n). This simple representation – sometimes referred to as a
box constraint – is generally appropriate in most applications, but could be generalized.
Let xk ∈S be a starting point and xk+1 be the next extrapolation. The following
procedure selects a point ¯xk+1 ∈S which is a weighted average of xk and xk+1, leaving
a buﬀer between ¯xk+1 and S parametrized by ω ∈(0, 1). For each i, compute
δ(i)
k =



ω(x(i)
max −x(i)
k )/(x(i)
k+1 −x(i)
k )
ω(x(i)
min −x(i)
k )/(x(i)
k+1 −x(i)
k )
1
if x(i)
k+1 −x(i)
k > ω(x(i)
max −x(i)
k )
if x(i)
k+1 −x(i)
k < ω(x(i)
min −x(i)
k )
otherwise.
The constrained extrapolation is
¯xk+1 = δkxk+1 + (1 −δk)xk,
where δk = min(δ(1)
k , δ(2)
k , ..., δ(n)
k ) ∈(0, 1]. This strategy ensures that no extrapolation
covers more than a fraction ω of the distance between x(i)
max −x(i)
k or x(i)
k −x(i)
min, keeping
xk within S at every step. A bound may still be reached asymptotically if it does
contain the ﬁxed point x∗. In the EM algorithm application, ω is set to 0.9.
13

5
Applications
In this section, ACX is compared to fast alternatives. For gradient descent acceleration,
the three applications are a logistic regression, a multivariate Rosenbrock function and
87 unconstrained optimization problems from the CUTEst collection. The general map-
ping applications were selected from various ﬁelds where previous schemes left room for
improvement. They are a Poisson admixture model solved by EM algorithm, alternat-
ing least squares applied to rank tensor decomposition, the power method for ﬁnding
dominant eigenvalues, and the method of alternating projection applied to regressions
with high-dimensional ﬁxed eﬀects.
Algorithms vary greatly in terms of gradient and objective computation as well
as internal computation.
Their performances were thus primarily assessed by CPU
time via the performance proﬁles of Dolan and Mor´e (2002). These graphs represent
how often each algorithm was within a certain multiple of the best compute time for
each draw. Appendix B also shows the average number objective function evaluations,
gradient evaluations or mappings, evaluation time (for the draws that converged), and
convergence rates.
Most computations were performed in Julia6 with the exception of the tensor canon-
ical decomposition performed in MATLAB and the alternating projections application
which compares packages of various languages. The benchmark stopping criterion is
||▽f(x)||∞< 10−7 for gradient descent acceleration and ||∆x||∞< 10−7 for general
mapping applications. To ensure meaningful comparisons, we only kept problems for
which all algorithms converged to the same objective. Precisely, the condition is
|f(xT,i) −min
j (f(xT,j))| < 10−5
∀i,
(7)
where xT,i is the ﬁnal iterate of any algorithm i, and minj(f(xT,j)) is the minimum over
all ﬁnal iterates j, including for algorithms that have not converged. This stringent set
of criteria was designed specially with the CUTEst problems in mind. For each applica-
tion, 2000 draws were computed, except for the CUTEst problems and the alternating
projections application. More detail on test implementation, software, package versions
and hardware used is presented in Appendix C.
Gradient descent applications
For gradient descent applications, the performances of ACX2, ACX3,2 and ACX3,3,2 are
compared with the L-BFGS algorithm and a nonlinear CG method proposed by Hager
and Zhang (2006) from the package Optim.jl. The L-BFGS is implemented with Hager-
Zhang line search and a window of 10 past iterates to build the Hessian approximation.
The nonlinear conjugate gradient method is also implemented with Hager-Zhang line
6Its just-in-time compiler guarantees little computing overhead. This is important to get accurate
ideas of the relative number of operations required for each method. We exclude the ﬁrst execution of
any function to discount compile time.
14

1
2
3
4
5
6
7
8
0.00
0.25
0.50
0.75
1.00
ACX2
ACX3, 2
ACX3, 3, 2
L-BFGS
Conj Grad
Figure 3: Performance proﬁles for the lo-
gistic regression
1
2
3
4
5
6
7
8
0.00
0.25
0.50
0.75
1.00
ACX2
ACX3, 2
ACX3, 3, 2
L-BFGS
Conj Grad
Figure 4:
Performance proﬁles for the
1000-parameter Rosenbrock
search. The algorithm combines features of Hager and Zhang (2006) and Hager and
Zhang (2013) and multiple revisions to the code since publication.
A logistic regression
An advantage of ACX schemes is their limited reliance on objective functions. A good
example is the logistic regression for which the log likelihood requires taking logs but
the gradient does not. For the test, the log-likelihood function was
l(y|X, β) =
n
X
i=1
[yi × x⊺
i β −log(1 + exp(x⊺
i β))]
where y ∈Rn, X ∈Rn×m, n = 2000, m = 100. The coeﬃcients and the covariates
were generated from uniform distributions U[−1, 1] and the matrix X contained a
column of ones. For each draw, the starting point was β0 = 0.
The results are shown in Figure 3 and Appendix Table 3. The three ACX algo-
rithms show very similar performance, dominating L-BFGS and the nonlinear CG in
the number of gradient evaluation, objective function evaluation and time. Table 3
show that they use on average 0.06 seconds while the L-BFGS needs 0.17 seconds and
the nonlinear CG takes 0.25 seconds. For ACX, only a few objectives are needed to
initialize the gradient descent step size α.
The Rosenbrock function
The Rosenbrock is a well-known test case for new algorithms. In high dimensions, a
useful test involves estimating a 1000-parameter version of the function
15

f(x(1), ..., x(N)) =
N/2
X
i=1

100
 x(2i−1)2 −x(2i)2
+
 x(2i−1) −1
2
N = 1000
with starting points x(i)
0
drawn from uniform distributions U[−5, 5].
The results are displayed in Figure 4 and Appendix Table 4. The three ACX algo-
rithms show advantageous performances. ACX3,3,2 clearly performs best, being fastest
almost 75% of times and dominating in the number of gradient and objective evalua-
tions.
The set of unconstrained minimization problems from the CUTEst suite
The CUTEst problem set – successor of CUTE (Constrained and Unconstrained Test-
ing Environment) and CUTEr – has become the benchmark for prototyping new opti-
mization algorithms. In January 2021, it contained 154 unconstrained problems with
objective function deﬁned as either quadratic, sum of squares or “other”. Of these, we
keep those with a non-zero initial gradient. When possible, we used the default number
of parameters. When the default was below 50, we set the number to 50 or 100, and
exclude those with fewer than 50 parameters.
1
2
3
4
5
6
7
8
0.00
0.25
0.50
0.75
1.00
ACX2
ACX3, 2
ACX3, 3, 2
L-BFGS
Conj Grad
Figure 5: Performance proﬁles for 87 unconstrained optimiza-
tion from te CUTEst collection (each problem is one draw)
We let algorithms run for at most 100 seconds and rejected problems for which no
algorithm converged before 10 seconds, or for which all algorithms took fewer than
5 mappings to converge. To avoid stalling, the initial stopping criterion was set to
16

||▽f(x)||∞< 10−5. When the ﬁnal objectives diverged such that criterion (7) was not
met, the stopping criterion was made progressively more stringent, up to ||▽f(x)||∞<
10−8. In the end, 87 problems were kept for comparison. Since CUTEst problems can
be challenging to optimize, ACX was implemented with backtracking.
Performance proﬁles are shown in Figure 5. Here again ACX3,3,2 clearly outper-
forms the other ACX. The CG performs remarkably well, being the fastest 35% of the
time. ACX3,3,2 and CG both solve around 80% of the problems in at most twice the
time taken by the fastest algorithm. Of course diﬀerent algorithms show advantages in
diﬀerent problems. The detail on the number of gradient and objective function evalu-
ations, compute time, convergence status and minimum objective attained is available
at sites.google.com/site/nicolaslepagesaucier/output CUTEst.txt.
Other mapping applications
EM algorithm for Poisson admixture model
The EM algorithm is an ubiquitous method in statisticians toolbox. While stable, it can
be notoriously slow to converge and has motivated the development of many acceleration
methods. We apply acceleration algorithms to the classic example of Hasselblad (1969)
who models of the death notices of women over 80 years old reported in the London
Times over a period of three years. Table 1 reproduces Hasselblad’s data.
Table 1: Number of death notices
Observed death count (i)
0
1
2
3
4
5
6
7
8
9
Frequency of occurrence (yi)
162
267
271
185
111
61
27
8
3
1
The data are modeled as a mixture of two Poisson distributions to capture higher
death rates during winter. The log likelihood is
L(y(0)...y(9)|µ(1), µ(2), π) =
9
Y
i=0

πe−µ(1) (µ(1))i
i!
+ (1 −π)e−µ(2) (µ(2))i
i!
y(i)
where µ1 and µ2 are the means of the distributions of subpopulations 1 and 2 and π is
the probability that a random individual is part of subpopulation 1. The EM algorithm
mappings are
µ(1)
k+1 =
P9
i=0 y(i)iw(i)
k
P9
i=0 y(i)w(i)
k
; µ(2)
k+1 =
P9
i=0 y(i)i(1 −w(i)
k )
P9
i=0 y(i)(1 −w(i)
k )
; πk+1 =
P9
i=0 y(i)w(i)
k
P9
i=0 y(i)
where
w(i)
k =
πke−µ(1)
k

µ(1)
k
i
πke−µ(1)
k

µ(1)
k
i
+ (1 −πk)e−µ(2)
k

µ(2)
k
i.
17

1
2
3
4
5
6
7
8
0.00
0.25
0.50
0.75
1.00
ACX2
ACX3, 2
ACX3, 3, 2
QNAMM 3
DAAREM (2)
Figure 6: Performance proﬁles for the EM
algorithm acceleration of Poisson admix-
tures
1
2
3
4
5
6
7
8
0.00
0.25
0.50
0.75
1.00
ACX2
ACX3, 2
ACX3, 3, 2
QNAMM 3
QNAMM 5
QNAMM 10
O-Accel (3)
O-Accel (5)
O-Accel (10)
DAAREM (10)
Figure 7:
Performance proﬁles for the
canonical vector decomposition
We estimated the model with random starting points sampled from uniform distri-
butions π0 ∽U[0.05, 0.95], µ(i)
0
∽U[0, 20]
(i = 1, 2). The ACX algorithms were im-
plemented with bound checks µ(i) ≥0,(i = 1, 2) and π ∈[0, 1] with a buﬀer of ω = 0.9
and stabilizing mapping. Their performances were compared with those of QNAMM
(3) and DAAREM (2)7, both adapted to Julia from their respective R packages.
The results are displayed in Figure 6 and appendix Table 5. With only three pa-
rameters to estimate, ACX3,2 performs slightly better than ACX3,3,2. Both outperform
QNAMM (3) and DAAREM (2) on all metrics. For QNAMM and DAAREM, the need
to check the objective and their general computational burdens clearly played a ma-
jor role. Henderson and Varadhan (2019)’s results suggest DAAREM should perform
better for EM-like problems with more parameters.
Alternating least squares for tensor rank decomposition
ALS is an iterative method used in matrix completion, canonical tensor decomposition
and matrix factorization with applications such as online rating systems, signal pro-
cessing, vision and graphics, psychometrics, and computational linguistics. Recently,
De Sterck (2012) designed an algorithm combining an ALS step and a nonlinear gener-
alized minimal residual method (N-GMRES) (Saad and Schultz (1986)) to iteratively
solve for canonical tensor decomposition and Riseth (2019) improved on the approach
with a general purpose acceleration method called O-ACCEL.
The test, ﬁrst introduced by Tomasi and Bro (2006) and Acar et al. (2011), involves
computing a 450 variable approximation of a three-way tensor of size 50 × 50 × 50 with
7As recommended by the authors, we use min(10, ⌈n/2⌉) lags where n is the number of parameters.
18

collinearity and noise (see the code for detail). We adapted Riseth (2019)’s MATLAB
code and compared O-ACCEL-ALS’s performance to the ACX algorithms, QNAMM
and DAAREM (both adapted to MATLAB by the author).
ACX convergence was
improved by using σ(p). For both O-ACCEL and QNAMM, we tested windows of 3,
5 and 10 past iterates to obtain a numerical approximation of the Hessian. We do
not show other algorithms like the N-GMRES or L-BFGS which Riseth (2019) showed
didn’t perform as well.
The results are summarized in Figure 7 and Appendix Table 6. Note that one ALS
iteration is registered as one mapping. The ACX3,3,2 acceleration shows a clear, but
moderate advantage over the other, closely followed by ACX3,2 and the three QNAMM.
O-ACCEL requires fewer mappings on average, but has a larger overall computational
burden. Here, the advantage of alternating is obvious given the poor performance of
ACX2.
Iterative methods for dominant eigenvalues
Several problems involving big data with sparse structures require computing a few
dominant eigenvalues. For these, iterative algorithms such as the power method or
the inverse power method are advantageous if they converge fast. Since these methods
involve simple linear operations on matrices, Jennings (1971) proposed accelerating
them using the same multivariate version of the Aitken’s ∆2 process suggested by
Lemar´echal (1971). We selected two algorithms for testing, described in Saad (2011).
The power method
Start with a non-zero vector x0.
Compute xk+1 =
Qxk
||Qxk||∞until convergence.
The shifted inverse power method
Start with a non-zero vector x0.
Compute xk+1 =
(Q−ξI)−1xk
||(Q−ξI)−1xk||∞until convergence.
Under mild conditions, the power method generates a series of vectors xk converging
to the eigenvector associated with the largest eigenvalue of Q in magnitude. The shifted
inverse power method converges to the eigenvector associated with the eigenvalue closest
to ξ. For the shifted inverse power method, factoring Q −ξI = LU initially is faster
than computing the inverse.
Each step involves more computation than the power
method, but convergence is faster, especially when ξ is close to its closest eigenvalue
and far from the next closest one.
We generated a dense, symmetric matrix Q = B + B⊺, where the matrix B ∈
Rn×n, n = 1000, has entries drawn from a uniform distribution U[−1, 1]. The shifting
parameter was set to ξ = 50, a slightly smaller value than the largest eigenvalue of Q
on average.
19

1
2
3
4
5
6
7
8
0.00
0.25
0.50
0.75
1.00
ACX2
ACX3, 2
ACX3, 3, 2
No accel
Eigen LAPACK
DAAREM (10)
Figure 8:
Performance proﬁles for the
power method for dominant eigenvalues
1
2
3
4
5
6
7
8
0.00
0.25
0.50
0.75
1.00
ACX2
ACX3, 2
ACX3, 3, 2
No accel
Eigen LAPACK
DAAREM (10)
Figure 9:
Performance proﬁles for the
shifted inverse power method for domi-
nant eigenvalues
We contrast the performances of ACX acceleration with the non-accelerated power
methods, as well as the Fortran Linear Algebra package (LAPACK) algorithm for com-
puting all eigenvalues. Of course, the beneﬁts of iterative methods over direct methods
should be much greater for sparse matrices. QNAMM is not applicable without ob-
jective function, but a version of DAAREM with no objective function is available,
which was implemented with 10 lags. Since the algorithms are prone to zigzagging, two
subsequent iterations were used as mappings in the ACX and DAAREM algorithms.
The results are displayed in Figures 8 and 9 and in Table 7. For the inverse power
method, compute times include the initial LU decomposition of the Q −ξI matrix.
All ACX schemes perform well, taking slightly under 0.1 seconds for both methods
on average. The unaccelerated power method is more than one order of magnitude
slower. The shifted inverse power method may be fast on its own, but shows much
slower convergence for draws where ξ happens to be farther from an eigenvalue. In
these cases, ACX acceleration is again very helpful.
Alternating projections for high-dimensional ﬁxed-eﬀects models
Since von Neumann (1949), ﬁnding the intersection between closed subspaces has been
an active area of research in the ﬁeld of numerical methods (see Escalante and Raydan
(2011) for a good treatment of the topic). Since the method of alternating projec-
tions can be arbitrarily slow to converge if the Friedrichs angle between the subspaces
is small, many algorithms were designed to accelerate it.
Among them, a notable
entry isGearheart and Koshy (1989) who suggested a generalized Aitken’s acceleration
method (Scheme 3.4 in their paper) which actually corresponds to Lemar´echal’s method
20

applied to the cyclic projection algorithm (alternating projections applied sequentially
to two or more subspaces).
ACX acceleration of alternating projections is a natural continuation of this eﬀort.
We illustrate it in the context of a popular application suggested by Gaure (2013). In
several ﬁelds of social sciences such as labor economics or international trade, researchers
study the impact of a few variables on large samples of potentially time-varying observa-
tions while controlling for stable unobserved eﬀects. These could be worker eﬀect, ﬁrm
eﬀect, school eﬀect, teacher eﬀect, doctor eﬀects, hospital eﬀect etc. A useful example
is Head et al. (2010) since they use publicly available data. They wish to estimate the
impact of colonial relations on trade ﬂows. Rather than reproducing their exact results,
consider the simple model
ln xijt = cijtβ + ditγ1 + djtγ2 + dijγ3 + uijt
where xijt is the export volume from country of origin i to country destination j in
year t (keeping only non-zero trade ﬂows), cijt is a dummy variable that equals 1 if
country i is still a colony of country j in year t and uijt represents unobserved trade
costs between the two countries at time t. The ﬁxed eﬀects are dummies capturing
observed or unobserved factors aﬀecting the trade ﬂows of the origin country at time t
(dit), the destination country at time t (djt) and stable characteristics of the exporter-
importer dyad such as common language, distance, etc. (dij). The parameter β should
capture the impact of being a colony on log exports to the metropolitan state, but since
colonial status is potentially correlated with the ﬁxed eﬀects, omitting them from the
model would most likely bias the estimate of β.
The full sample of trading countries is 707, 368 observations and, more importantly,
the model contains 9569 + 9569 + 29, 603 = 48, 741 ﬁxed eﬀects. Needless to say, OLS
estimation is impractical purely in terms of memory. To understand Gaure (2013)’s
method, deﬁne x and c as column vectors containing the xijt and cijt, respectively,
and D = [Dit, Djt, Dij] as the matrix of ﬁxed eﬀects. The method employs the Frisch-
Waugh-Lovell theorem (Frisch and Waugh (1933), Lovell (1963)) to estimate β by
regressing MDx on MDc, where MD = I −D(D⊺D)−1D⊺projects onto the orthogonal
complement of the column space of D. To avoid computing MD directly, use Halperin
(1962)’s Theorem 1: MD = limk→∞(MDitMDjtMDij)k. Since the projections onto each
set of ﬁxed eﬀects are equivalent to simple group demeaning, the algorithm can be very
fast and has become standard packages in R and Python.
The potential pitfalls of alternating projections remain however. If the panel is not
well balanced and diﬀerent sets of dummies are near collinear, the method may need
many iterations to converge and actually take longer than alternative algorithms. In
such cases, ACX acceleration could provide discernible beneﬁts.
We implemented Gaure’s method and compared its performance with and without
ACX acceleration. For additional comparison, the model was also estimated with a
variety of equivalent packages in R, Python, Stata and Julia. Of course, they have
diﬀerent overhead and implement diﬀerent algorithms. In Julia, the FixedEﬀectModel.jl
package projects D using the LSMR algorithm (Fong and Saunders (2011)) based on
21

Table 2: Performances for the trade ﬂows regression with high-dimensional
ﬁxed eﬀects
Algorithm / Package
Whole sample
Partial sample
Maps
Sec.
Maps
Sec.
No acceleration
21
0.54
81.5
1.32
ACX2
16
0.44
26
0.50
ACX3,2
16.5
0.44
25
0.46
ACX3,3,2
16
0.44
24
0.44
FixedEﬀectModel (LSMR) (Julia)
0.70
0.57
FELM (R)
0.84
0.90
FixedEﬀectModel (Python)
2.75
5.84
Reghdfe (Stata)
6.69
6.01
Observations
707,368
530,504
Note: For Python, the time only includes demeaning. Also, Python and R both use
Gaure (2013)’s method as well, but do not display the number of mappings.
the Golub-Kahan bidiagonalization (Golub and Kahan (1964)). For Stata’s Reghdfe
algorithm, see Correia (2016). To test for the impact of collinearity, the same model was
estimated on the full sample and on a subsample excluding the 25% trading partners
with the longest geographical distance, making trading blocks more localized and the
panel less balanced.
Table 2 displays the CPU time and average mapping for the demeaning of x and
c, performed independently.8 In terms of CPU time, the ACX accelerated alternating
projections performs best. ACX3,3,2 is only a slight favorite, an unexpected outcome
given the large sample size. For the whole sample, the acceleration provides a modest
advantage over the unaccelerated algorithm. However, for the partial sample, it divides
by 3 the required mappings and the compute time.
6
Discussion
This article has introduced new acceleration methods for ﬁxed-point iterations. By
alternating between squared and cubic extrapolations, the ACX schemes target spe-
ciﬁc error components and dynamically speed-up convergence in subsequent iterations.
Thanks to cycling, the extra computation needed for cubic extrapolations is essentially
free. For linear mappings, ACX is Q-linear convergent.
Many popular optimization methods and ﬁxed point accelerations store information
8Even though both the R and Python packages also use Gaure (2013)’s method, the information on
the number of mappings is not available. As to Reghdfe, it is too diﬀerent for a meaningful comparison
of the number of mappings.
22

from past iterates, which makes them eﬃcient in some contexts, but possibly less so in
others. By only extrapolating from two or three mappings, ACX schemes are remark-
ably fast, stable and versatile. Applied to gradient descent, they are competitive with
the best nonlinear solvers. They also speed up other ﬁxed-point iterations like the EM
algorithm, ALS, the power method and alternating projections. These probably repre-
sent a small subset of potential uses, which may extend to image processing, physics
and other big data applications with sparse representations.
References
Acar, E., Dunlavy, D. and Kolda, T. (2011). A scalable optimization approach for ﬁtting
canonical tensor decompositions. Journal of Chemometrics 25: 67–86.
Armijo, L. (1966). Minimization of functions having Lipschitz continuous ﬁrst partial
derivatives. Paciﬁc Journal of Mathematics 16: 1–3.
Barzilay, J. and Borwen, J. (1988). Two-point step size gradient methods. IMA Journal
of Numerical Analysis 8: 141–148.
Bezanson, J., Edelman, A., Karpinski, S. and Shah, V. B. (2017). Julia: A fresh ap-
proach to numerical computing. SIAM review 59: 65–98.
Birgin, E., Mart´ınez, J. and Raydan, M. (2014). Spectral projected gradient methods:
Review and perspectives. Journal of Statistical Software 60: 1–21.
Bongartz, I., Conn, A. R., Gould, N. and Toint, P. L. (1995). CUTE: Constrained
and unconstrained testing environment. ACM Trans. Math. Softw. 21: 123–160, doi:
10.1145/200979.201043.
Brezinski, C. and Chehab, J. (1998). Nonlinear hybrid procedures and ﬁxed point iter-
ations. Numerical Functional Analysis and Optimization 19: 465–487.
Brezinski, C. and Redivo-Zaglia, M. (2020). Extrapolation and Rational Approximation.
Springer Nature Switzerland.
Brezinski, C., Redivo-Zaglia, M. and Saad, Y. (2018). Shanks sequence transformations
and Anderson acceleration. SIAM Review 60: 646–669.
Correia, S. (2016). Linear models with high-dimensional ﬁxed eﬀects: An eﬃcient and
feasible estimator. Tech. rep., Duke University, working paper.
Dai, Y.-H., Huang, Y. and Liu, X.-W. (2019). A family of spectral gradient methods
for optimization. Computational Optimization and Applications 63: 43–65.
De Sterck, H. (2012). A nonlinear GMRES optimization algorithm for canonical tensor
decomposition. SIAM Journal on Scientiﬁc Computing 34: A1351–A1379.
23

Dempster, A., Laird, N. and Rubin, D. (1977). Maximum likelihood from incom-
plete data via the EM algorithm. Journal of the Royal Statistical Society: Series
B (Methodological) 39: 1–38.
Dolan, E. and Mor´e, J. (2002). Benchmarking optimization software with performance
proﬁles. Mathematical Programming 91: 201–213.
Escalante, R. and Raydan, M. (2011). Alternating Projection Methods. SIAM. Society
for Industrial and Applied Mathematics.
Fong, D. and Saunders, M. (2011). LSMR: An iterative algorithm for sparse least-
squares problems. SIAM Journal on Scientiﬁc Computing 33: 2950–2971.
Friedlander, A., Mart´ınez, J., Molina, B. and Raydan, M. (1999). Gradient method
with retards and generalizations. SIAM Journal on Numerical Analysis 36: 275–289.
Frisch, R. and Waugh, F. (1933). Partial time regressions as compared with individual
trends. Econometrica 1: 387–401.
Gaure, S. (2013). OLS with multiple high dimensional category variables. Computa-
tional Statistics and Data Analysis 66: 8–18.
Gearheart, W. and Koshy, M. (1989). Acceleration schemes for the method of alternat-
ing projections. Journal of Computational and Applied Mathematics 26: 235–249.
Golub, G. and Kahan, W. (1964). Calculating the singular values and pseudo-inverse
of a matrix. Journal of the Society for Industrial and Applied Mathematics Series B
Numerical Analysis 2: 205–224.
Hager, W. and Zhang, H. (2006). Algorithm 851: CG DESCENT, a conjugate gradient
method with guaranteed descent. ACM Transactions on Mathematical Software 32:
113–137.
Hager, W. and Zhang, H. (2013). The limited memory conjugate gradient method.
SIAM Journal on Optimization 23: 2150–2168.
Halperin, I. (1962). The product of projection operators. Acta Scientiarum Mathemati-
carum (Szeged) 23: 96–99.
Hasselblad, V. (1969). Estimation of ﬁnite mixtures of distributions from the exponen-
tial family. Journal of the American Statistical Association 64: 1459–1471.
Head, K., Mayer, T. and Ries, J. (2010). The erosion of colonial trade linkages after
independence. Journal of International Economics 81: 1–14.
Henderson, N. and Varadhan, R. (2019). Damped Anderson acceleration with restarts
and monotonicity control for accelerating EM and EM-like algorithms. Journal of
Computational and Graphical Statistics 28: 834–846.
24

Henrici, P. (1964). Elements of numerical analysis. R.E. Krieger Pub. Co.
Irons, B. and Tuck, R. (1969). A version of the Aitken accelerator for computer iteration.
International Journal of Numerical Methods in Engineering 1: 275–277.
Jbilou, K. and Sadok, H. (2000). Vector extrapolation methods. Applications and nu-
merical comparison. Journal of Computational and Applied Mathematics 122: 149–
165.
Jennings, A. (1971). Accelerating the convergence of matrix iterative processes. IMA
Journal of Applied Mathematics 8: 99–110.
Lange, K. (2016). MM Optimization Algorithms. SIAM.
Lebedev, V. and Zabelin, V. (1995). Combined trinomial iterative methods with Cheby-
shev parameters. East-West Journal of Numerical Mathematics 3: 145–162.
Lemar´echal, C. (1971). Une m´ethode de r´esolution de certains syst`emes non lin´eaires
bien pos´es. In Comptes rendus hebdomadaires des s´eances de l’Acad´emie des sciences,
A, Sciences Math´ematiques 272. Paris, France: Acad´emie des sciences, 605–607.
Liu, D. and Nocedal, J. (1989). On the limited memory BFGS method for large scale
optimization. Numerical optimization 45: 497–528.
Lovell, M. (1963). Seasonal adjustment of economic time series and multiple regression
analysis. Journal of the American Statistical Association 58: 993–1010.
Macleod, A. (1986). Acceleration of vector sequences by multi-dimensional ∆2 methods.
Communications in Applied Numerical Methods 2: 385–392.
Marder, B. and Weitzner, H. (1970). A bifurcation problem in E-layer equilibria. Plasma
Physics 12: 435–445.
Mogensen, P. K. and Riseth, A. N. (2018). Optim: A mathematical optimization pack-
age for Julia. Journal of Open Source Software 3: 615, doi:10.21105/joss.00615.
Nocedal, J. and Wright, S. (2006). Numerical Optimization, Second Edition. Springer
Series in Operational Research. Springer, 2nd ed.
Ortega, W. C., James M..; Rheinboldt (1970). Iterative Solutions of Nonlinear Equa-
tions in Several Variables. SIAM.
Rami`ere, I. and Helfer, T. (2015). Iterative residual-based vector methods to accelerate
ﬁxed point iterations. Computers and Mathematics with Applications 70: 2210–2226.
Raydan, M. and Svaiter, B. (2002). Relaxed steepest descent and Cauchy-Barzilai-
Borwein method. Computational Optimization and Applications 21: 155–167.
25

Riseth, A. (2019). Objective acceleration for unconstrained optimization. Numerical
Linear Algebra With Applications 26: 1–17.
Roland, C. and Varadhan, R. (2004). Squared Extrapolation Methods (SQUAREM):
A New Class of Simple and Eﬃcient Numerical Schemes for Accelerating the Con-
vergence of the EM Algorithm. Working papers, Johns Hopkins University, Dept. of
Biostatistics.
Roland, C. and Varadhan, R. (2005). New iterative schemes for nonlinear ﬁxed point
problems, with applications to problems with bifurcations and incomplete-data prob-
lems. Applied Numerical Mathematics 55: 215–226.
Roland, C. and Varadhan, R. (2008). Simple and globally convergent methods for ac-
celerating the convergence of any EM algorithm. Scandinavian Journal of Statistics
35: 335–353.
Saad, Y. (2011). Numerical Methods for Large Eigenvalue Problems. Society for Indus-
trial and Applied Mathematics, 2nd ed.
Saad, Y. and Schultz, M. (1986). GMRES: A generalized minimal residual algorithm
for solving nonsymmetric linear systems. SIAM Journal on Scientiﬁc Computing 7:
856–869.
Tomasi, G. and Bro, R. (2006). A comparison of algorithms for ﬁtting the PARAFAC
model. Computational Statistics & Data Analysis 50: 1700–1734.
von Neumann, J. (1949). On rings of operators. Reduction theory. Annals of Mathe-
matics Second Series 50: 401–485.
von Neumann, J. (1950). Functional Operators. Princeton University Press.
Zhou, H., Alexander, D. and Lange, K. (2011). A quasi-Newton acceleration for high-
dimensional optimization algorithms. Statistics and computing 21: 261–273.
26

Appendix
A
Proofs
Proof of Lemma 1. From the binomial formula, express (x+y)p as the sum of xp +yp
and an extra term:
(x + y)p =
p
X
j=0
p!
j! (p −j)!xp−jyj
= xp + yp +
p−1
X
j=1
p!
j!(p −j)!xp−jyj
= xp + yp +
p−2
X
j=0
p!
(j + 1)!(p −j −1)!xp−j−1yj+1
= xp + yp + pxy(R1)
(8a)
where
R1 =
p−2
X
j=0
p −1
(j + 1)(p −1 −j)
(p −2)!
j!(p −2 −j)!xp−2−jyj.
Consider the case of a general extra term:
Ri =
p−2i
X
j=0
i(p −i)
(j + i)((p −i) −j)
(p −2i)!
j!(p −2i −j)!xp−2i−jyj.
Let us consider diﬀerent cases. By direct calculation, if p is even and i = p/2, Ri = 1.
If p is odd and i = (p −1)/2, Ri = x + y. If 1 ≤i < ⌊p/2⌋, using the binomial formula:
Ri = (x + y)p−2i +
p−2i
X
j=0

i(p −i)
(j + i)((p −i) −j) −1

(p −2i)!
j!(p −2i −j)!xp−2i−jyj
Ri = (x + y)p−2i −
p−2i
X
j=0
j
j + i
p −2i −j
p −i −j
(p −2i)!
j!(p −2i −j)!xp−2i−jyj.
If j = 0 or j = p −2i then
j
j+i
p−2i−j
p−i−j = 0. We therefore write
Ri = (x + y)p−2i −
p−2i−1
X
j=1
j
j + i
p −2i −j
p −i −j
(p −2i)!
j!(p −2i −j)!xp−2i−jyj
27

and rewrite the sum as
Ri = (x + y)p−2i −
p−2i−2
X
j=0
j + 1
j + 1 + i
p −2i −j −1
p −i −j −1
(p −2i)!
(j + 1)!(p −2i −j −1)!xp−2i−j−1yj+1
Ri = (x + y)p−2i −
"
xy (p−2i)(p−2i−1)
(i+1)(p−(i+1)) ×
Pp−2(i+1)
j=0
(i+1)(p−(i+1))
(j+i+1)(p−(i+1)−j)
(p−2(i+1))!
j!(p−2(i+1)−j)!xp−2(i+1)−jyj
#
Ri = (x + y)p−2i −xy(p −2i)(p −2i −1)
(i + 1)(p −i −1) Ri+1.
By recursively substituting Ri+1 back in 8a, we may rewrite the whole expression using
a summation for i = 1 to ⌊p/2⌋and recover (6).
B
Tables of numerical results
Table 3: Average performances: Logistic
regression
Algorithm
Gradients
Obj.
Sec.
ACX2
57.42
4.13
0.058
ACX3,2
54.87
4.13
0.059
ACX3,3,2
55.34
4.13
0.059
L-BFGS
64.79
64.79
0.172
Conj. Gr.
61.65
94.08
0.246
Table 4:
Average performances:
1000-
parameter Rosenbrock
Algorithm
Gradients
Obj.
Sec.
ACX2
926.80
7.00
0.131
ACX3,2
745.71
7.00
0.107
ACX3,3,2
660.21
7.00
0.095
L-BFGS
760.48
760.48
0.157
Conj. Gr.
932.02
1860.04
0.258
Table 5: Average performances: EM algorithm
for Poisson admixture
Algorithm
Maps
Obj.
ms
Conv.
ACX2
107.12
0
0.67
1.00
ACX3,2
55.62
0
0.34
1.00
ACX3,3,2
62.03
0
0.38
1.00
QNAMM (3)
108.85
103.85
1.57
1.00
DAAREM (2)
63.79
82.07
1.59
1.00
28

Table 6: Average performances: canonical tensor
decomposition
Algorithm
Maps
Obj.
Sec.
Conv.
ACX2
228.53
57.47
1.61
1.00
ACX3,2
90.59
18.53
0.63
1.00
ACX3,3,2
82.80
15.99
0.58
1.00
QNAMM (3)
94.11
89.11
0.73
1.00
QNAMM (5)
89.12
82.12
0.69
1.00
QNAMM (10)
96.21
84.21
0.74
1.00
O-Accel (3)
67.77
67.77
0.82
1.00
O-Accel (5)
59.02
59.02
0.75
1.00
O-Accel (10)
56.73
56.73
0.74
1.00
DAAREM (10)
597.67
651.44
1.09
0.52
Table 7: Average performances: dominant eigenvalues
Algorithm
Power method
Inverse power method
Maps
Sec.
Conv.
Maps
Sec.
Conv.
ACX2
865.45
0.32
0.99
33.27
0.19
0.99
ACX3,2
391.61
0.15
0.99
29.67
0.18
1.00
ACX3,3,2
370.24
0.14
1.00
31.53
0.18
1.00
No acceleration
6086.56
2.26
0.92
111.70
0.32
0.99
Eigen LAPACK
0.00
0.61
1.00
0.00
0.54
1.00
DAAREM (10)
3100.68
1.39
0.93
43.63
0.21
0.99
29

C
Software and hardware used
For the CUTEst and the alternating projections applications, each problem was run
ﬁve times if the median time was over 0.1 seconds, and 100 times if it was below 0.1
seconds. Then the median time is reported. All tests in Julia were run once before
recording time to exclude compile time. See the code for detail.
The main software used for the numerical experiments were Julia v.1.5.3 (Bezan-
son et al. (2017)), FixedEﬀectModels.jl v1.1.0, Optim.jl v1.2.4 (Mogensen and Riseth
(2018)), CUTEst.jl v0.10.3, MATLAB R2018a, Stata 13, reghdfe (Stata package),
Python v3.8.5, FixedEﬀectModel v0.0.2 (Python), R v4.0.4, felm (lfe v 2.8-6, R), LA-
PACK v3.9.1.
All computations were single-threaded, done on HP ZBooks 15 with Intel Core
i7-4900MQ CPUs with 2.80GHz and 32 Go of RAM. The CUTEst application was
computed on Ubuntu 20.04, others on Windows 10.
30

