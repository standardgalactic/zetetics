Noisy dynamical systems evolve error correcting codes and modularity
Trevor McCourt,1, 2, ∗Ila R. Fiete,3, 4 and Isaac L. Chuang1, 5, 2
1Department of Electrical Engineering and Computer Science,
Massachusetts Institute of Technology, Cambridge, MA 02139, USA
2The NSF AI Institute for Artiﬁcial Intelligence and Fundamental Interactions, Cambridge, Massachusetts 02139, USA
3Department of Brain and Cognitive Sciences, MIT, Cambridge, MA, USA
4McGovern Institute for Brain Research, Department of Brain and Cognitive Sciences,
Massachusetts Institute of Technology, Cambridge, MA 02139
5Department of Physics, Massachusetts Institute of Technology, Cambridge, MA 02139, USA
(Dated: April 14, 2023)
Noise is a ubiquitous feature of the physical world. As a result, the ﬁrst prerequisite of life is
fault tolerance: maintaining integrity of state despite external bombardment. Recent experimental
advances have revealed that biological systems achieve fault tolerance by implementing mathe-
matically intricate error-correcting codes and by organizing in a modular fashion that physically
separates functionally distinct subsystems. These elaborate structures represent a vanishing vol-
ume in the massive genetic conﬁguration space. How is it possible that the primitive process of
evolution, by which all biological systems evolved, achieved such unusual results? In this work,
through experiments in Boolean networks, we show that the simultaneous presence of error correc-
tion and modularity in biological systems is no coincidence. Rather, it is a typical co-occurrence in
noisy dynamic systems undergoing evolution. From this, we deduce the principle of error correction
enhanced evolvability: systems possessing error-correcting codes are more eﬀectively improved by
evolution than those without.
Even under the inﬂuence of entirely deterministic natu-
ral laws, a ﬁnite agent that interacts with a large environ-
ment will experience some degree of randomness. This is
because it will eventually lose record of all its previous
interactions, and will only be able to reason on probabilis-
tic terms. This is exempliﬁed in physics by observations
of Brownian motion [1], in which an observed particle in
contact with a large bath of unobserved particles under-
goes seemingly random, yet entirely determined, motion.
Therefore it is reasonable to suggest that anywhere life
emerges, it is probably doing so despite a stochastic en-
vironment, as it has on Earth.
Von Neumann appreciated the fundamental nature of
noise and observed that the biological information pro-
cessing systems that underpin life handle it elegantly [2].
In particular, biological systems are generally not halted
by a single component failure or error. In contrast, the
engineered computers of his time were generally brittle,
and errors in a single component had to be identiﬁed and
addressed before the rest of the system could continue
operation. Von Neumann thought this approach would
begin to become infeasible as more and more complex
systems were constructed [3]. To resolve this and to con-
tribute further to our understanding of the connection
between computing in the presence of noise and life, he
established the theory of fault-tolerant computation [4].
Fault-tolerant computing systems compose noisy prim-
itive computational units (e.g.
logic gates) into struc-
tures that compute the same primitive, robustly [4, 5].
This concept is not limited to digital computation: fault
tolerance has recently been demonstrated in neural net-
∗tmccourt@mit.edu
works with brain-like representations [6] and is also an
active area of theoretical and experimental research in
quantum computing [7–10], where it is particularly im-
portant due to the high susceptibility of real qubits to
noise. Today, fault-tolerant computing is generally ac-
complished by computing within the context of an error-
correcting code. An error-correcting code speciﬁes a way
to robustly store or transmit messages in the presence
of noise [11]. The basic idea is to encode messages in
codewords, which adds redundancy such that if part of
the codeword is lost the message can still be recovered.
Fault-tolerant computing further requires that the recov-
ery be done by the noisy system itself.
Since the time of von Neumann, many speciﬁc exam-
ples of robust behavior in biological information process-
ing systems have been discovered. Genomes are one ex-
ample of this. Single-knockout experiments in a partic-
ular yeast found that only 19% of the genes were re-
quired for the organism to continue living [12, 13]. The
same resilience is seen in the human brain, which can
tolerate substantial death of neurons with minimal cog-
nitive impairment [14, 15]. This level of robustness im-
plies the existence of mechanisms for error correction.
Studying these mechanisms is an emerging topic of the-
oretical interest in genetics [16]. There is also substan-
tial evidence for formal error correction mechanisms in
the brain. Many neural circuits have population activi-
ties that live on low dimensional manifolds [17–20], rem-
iniscent of topological error-correcting codes [21, 22]. In
particular, it is thought that grid cells use an eﬃcient
error-correcting code to robustly encode position in the
presence of noise [23].
One way that large-scale natural and engineered sys-
tems seem to achieve robustness is through modularity,
arXiv:2303.14448v2  [q-bio.PE]  12 Apr 2023

2
the division of a large system into functionally indepen-
dent parts. The genome is believed to have spatial and
functional modularity [24], and a common view of the
brain is that it is composed of many modules responsible
for diﬀerent functions that combine to form our intel-
ligence [25]. At a large scale, organizations, processes,
and machines frequently have a division of responsibility
for separate subtasks, such that if one subsystem fails the
system can still complete its function [26–28]. In general,
an overarching motif observed in natural systems is that
they appear to be assembled out of smaller, individually
robust components.
At the same time, biological structures emerge by mak-
ing local changes that improve ﬁtness, a process of adap-
tive evolution. It is incredible that evolution was able
to produce modular, error-correcting structures through
this process, given that they represent a vanishing frac-
tion of the conﬁguration space of possible solutions to a
given problem. A tantalizing question is therefore what
conditions and principles drive evolution toward such
structures? The answer to this question is signiﬁcant, as
it gets to the root of forces that drive the crystalization
of life.
Any property that increases the eﬃcacy of adaptive
evolution may be referred to as evolvable.
Evolvable
properties should emerge as a typical result of evolution
since obtaining some of an evolvable property increases
an organism’s ability to acquire more of it. Identifying
evolvable properties is a topic of constant interest [29–32].
For example, Kauﬀman famously showed that systems
with a phase transition from frozen to chaotic behavior
(a so-called "ordered" phase) tend to be more evolvable,
and seek the edge of chaos [33].
In this work, we show that both error-correcting codes
and modularity are typical co-emergent results of evo-
lution in a noisy environment.
We show that this oc-
curs because organisms with error-correcting properties
are better protected from lethal mutations than those
without, allowing them to more eﬀectively search conﬁg-
uration space for improvements. From this, we introduce
the concept of error correction-enhanced evolvability: or-
ganisms with error-correcting codes are more evolvable
than those without. This principle is illustrated in Fig. 1.
Noise bootstraps this phenomenon, suggesting that noise
plays an important role in evolving complex structures.
We begin by introducing Boolean networks and moti-
vating their use in the study of evolution and life. We
then show that when evolved to perform primitive com-
putations in the presence of noise, Boolean networks al-
most always develop strong error-correcting codes. By
probing the eﬀect of mutations on evolved networks,
we then demonstrate that this occurs because error-
correcting codes increase the number of neutral muta-
tions seen by organisms, allowing them to more eﬀec-
tively search locally for improvements than their non-
error-correcting counterparts. Finally, we show that un-
der composite computational tasks (that require more
than one primitive), evolution tends to produce struc-
Organism Fitness
Genome Conﬁguration
Without error correction
Survivability thresold
Error correction enhanced evolvability
With error correction
FIG. 1.
Error correction smooths valleys in ﬁtness
landscapes. Noisy systems initially evolve error correction
to protect themselves from random environmental ﬂuctua-
tions that aﬀect their state.
However, once evolved, error
correction also helps neutralize permanent genetic mutations
that would otherwise be lethal.
During evolution, this al-
lows organisms that possess error-correcting codes to search
further in genome conﬁguration space for improvements than
their non-error-correcting counterparts. This is illustrated in
the ﬁgure: the organism that uses error correction can sustain
the ﬁrst genome mutation without falling into a valley of ﬁt-
ness that is too low to survive. It can then acquire a further
mutation which brings it to a new, improved local ﬁtness max-
imum, corresponding to an improved error-correcting code.
With this new code, it will even more easily be able to ﬁnd
improvements, and so on. Error correction begets more error
correction, leading to its ubiquity in the natural world.
turally modular organisms that employ modular error-
correcting codes, beautifully reﬂecting what is seen in
biology.
Originally developed to model gene regulatory net-
works
[34–36],
Boolean
networks
are
autonomous,
discrete-time
dynamical
systems
with
binary-valued
states. Boolean networks can be viewed as dynamic func-
tion evaluators. A function input can be given as an ini-
tial state to a network and the output is simply the state
at some speciﬁed later time. Boolean networks are there-
fore reasonable computational models of primitive living
organisms: receiving input and later producing the cor-
rect output is equivalent to an agent responding to en-
vironmental stimulus to avoid death [37]. Despite their
simple binary nature, Boolean networks can express very
complex behavior. For example, they possess the afore-
mentioned ordered phase that seems to be crucial for suc-
cessful adaptation [33, 38]. This complexity makes them

3
a candidate for modeling large-scale brain dynamics [39].
Here, we leverage Boolean networks to computation-
ally probe evolution in a noisy environment. We assign
Boolean network organisms a measure of ﬁtness based on
their ability to compute a simple function in the presence
of noise. We provide an organism with the input to the
computation in a raw, unencoded form, and allow it to
undergo noisy dynamics for several timesteps. It is then
queried for the output, which can be compared to the
correct output for ﬁtness calculation. Producing the cor-
rect output in the presence of noise is a daunting task: an
organism has to have a method for encoding the inputs,
robustly performing the computation, and stabilizing the
answer against noise in memory. We study the evolution
of organisms that compute primitive and more compli-
cated composite computations to study the emergence of
error correction and modularity.
The state of a Boolean network of N nodes is given by
a length N binary vector x. The state of the ith node xi
at time t + 1 is given as an arbitrary Boolean function fi
of the state of ki ≤kmax other nodes at time t. There-
fore, a Boolean network can be thought of as a directed
graph with truth tables of length 2ki living on each node
(Fig. 2a) The genome of such a Boolean network is there-
fore speciﬁed by a matrix containing the truth tables for
each node and a matrix deﬁning the connectivity between
nodes. For further information on how the Boolean net-
work genome was speciﬁed in these experiments, see [40]
or supplementary section 1.
The space of possible Boolean networks is truly mas-
sive. For a network with exactly k connections per node,
the number of unique networks is [41, 42]
 
22kN!
(N −k)!
!N
.
(1)
The networks considered in this work are ragged, which
means that they have ki ≤kmax connections per node.
As such, the number of networks is larger than what is
indicated by Eq. 1. For the N = 8 and kmax = 3 network
shown in Fig. 2a, this evaluates to ∼1040. This number
grows very rapidly with N; for N = 15 there are more
conﬁgurations than there are bits of information in the
observable universe [43]. Therefore, any particular con-
ﬁguration of a Boolean network resides in a vanishing vol-
ume of conﬁguration space. At the same time, Boolean
network loss landscapes are generally rugged and multi-
peaked, making adaptation non-trivial [33]. Therefore,
any conﬁguration that is consistently reached by ran-
domly initialized adaptive walks must be somehow at-
tractive: it is exceptionally diﬃcult to stumble upon par-
ticular solutions by accident when working with Boolean
networks. This makes Boolean networks particularly use-
ful for studying evolvability.
We introduce stochasticity to our Boolean networks
[44] by injecting independent Bernoulli noise alongside
the deterministic state update described above:
x[t + 1] = f(x[t]) ⊕Bernoulli(pphys) ,
(2)
where f is the collection of all the fi and represents the
deterministic update previously described, ⊕indicates
bitwise XOR (exclusive OR), and pphys ∼10−2 is the
physical error probability. In other words, the noise acts
to ﬂip each bit of f(x[t]) independently with probability
pphys. Fig 2b shows an example of the state of a noisy
Boolean network vs time for diﬀerent inputs.
We provide input to our organisms via designated in-
put nodes, which are initialized with the input to a com-
putational problem at t = 0. For example, an organism
that solves the AND task has two input nodes, as shown
in Fig 2a, which could be initialized as any two-bit binary
string. The rest of the nodes are initialized to 0. The or-
ganism then undergoes T −1 steps of noisy dynamics,
corresponding to deterministic state updates followed by
exposure to noise (Eq. 2). One more state update is then
applied, serving as a noiseless decoding step, as standard
in the study of fault-tolerant subsystems [4]. The state
of the output nodes is then taken as the logical output of
the organism. We randomize T within some small range
to force the organism to stabilize the solution instead of
being allowed to pass through it transiently. The struc-
ture and complexity of the computational task can be
varied to probe diﬀerent properties of the solutions. The
unﬁtness (or loss) of a given organism is its logical error
probability plog. plog is the probability that the output
node contains the wrong value at time T (after the de-
coding step), averaged over noise and all possible input
binary strings. As an example, for the network shown in
Fig 2a, plog is the probability that at time T the output
node does not contain the AND of the values initially
supplied to the input nodes. plog therefore reﬂects the
eﬀect of physical error pphys on the network. For a ﬁxed
pphys, a low plog means the network is performing the
computation in a way that is robust to noise.
Endowed with a measure of ﬁtness, we can then ex-
periment with adaptive evolution in conﬁguration space.
Starting from a random conﬁguration, organisms of in-
creasing ﬁtness are found by manipulating the genome
via pairwise crossover operations (breeding) and local
mutations. This procedure is intentionally primal and is
meant to mirror operations that would have been feasible
during bio-genesis. We tried a few diﬀerent crossover op-
erators, including no crossover (pure local search), and
we found none of them substantially outperformed the
local search (supplementary section 4).
Despite the apparent simplicity of Boolean networks,
exploring their evolution in the presence of noise is ex-
tremely computationally intensive. For every evolution-
ary step, several hundred populations of several hundred
organisms must be run through several hundred diﬀer-
ent noise trajectories starting from tens of diﬀerent input
states. This number quickly approaches batches of 108
networks.
Such large experiments were made possible
by accelerating evaluation using modern Graphics Pro-
cessing Units (GPUs) with large memories [40]. Trillions
of networks were simulated in the course of this work,
consuming several GPU months of compute time.

4
p
log
p
log
c
d
e
f
p
phys
p
phys
p
phys
p
log=
a
Network State
Time
Time
Noise Events
b
0
1
2
C0
C1
p
log
p
phys
FIG. 2. Boolean networks adapting into error correction. a A Boolean network that has adapted to solve the AND
task in the presence of noise. Inputs are provided to the orange nodes 0 and 1 at time t=0 and, the answer is expected at the
green node 2 at t=T. A Boolean network is speciﬁed by a directed graph with a truth table living on each node. The state of
each head node at time t+1 is a binary function of the state of each tail node at time t. For example, the state of node 6 at
time t+1 is computed by applying a length 23 truth table to the states of nodes 0, 1, and 3 at time t. The edge coloring reﬂects
the inﬂuence of a head node on a tail node. Inﬂuence is the probability (taken over the entire truth table) that the head node
ﬂips if the tail node ﬂips. b Examples of noisy dynamical trajectories of the network in (a) for each of the 22 possible input
states. The network encodes the answer in codewords of maximal Hamming distance and stabilizes the codeword against noise
events. Codewords may be taken as the ﬁnal state of the network x[T] for the diﬀerent output values. The two codewords are
visualized on the network graph. The boundary of the circles indicates the node function, and the ﬁll indicates the codeword.
c Evolutionary trajectories of 150 randomly initialized populations learning to solve the XOR and AND tasks. Populations
were not pruned, every population that was started is included in the statistics. The solid line indicates the median logical
error probability, and the shaded region indicates the interquartile range. d Distribution of ﬁnal logical error probabilities over
all populations. In both tasks, typical organisms learn to suppress errors far below the physical noise level (pphys, indicated by
the dashed black line). e Distribution of average Hamming distance between the codewords C0 and C1 over all populations.
The typical organism uses a strong error-correcting code to suppress errors. f Scaling of the logical error probability with
the physical error probability for the highest performing organisms: logical error probability is strongly suppressed relative to
physical error probability.
We performed several experiments that probed the
ability of Boolean networks to robustly perform the prim-
itive computations AND and XOR in the presence of
noise. Fig 2 a shows a typical resulting organism for the
AND task (XOR results are similar, see supplementary
section 3). The organism itself has little structure, as ex-
pected for a primitive task. One interesting point is that
all of the resulting Boolean functions are of moderate
inﬂuence. This is expected for a fault-tolerant solution,
as heavy inﬂuence edges more readily propagate errors.
What is most interesting about the evolved organisms is
that they seem to achieve low error rates by implementing
error-correcting codes. There are two possible answers to
both the AND and XOR tasks: 0 and 1. As shown in
Fig 2b, the network given in Fig 2a rapidly converges to
a ﬁxed point corresponding to the correct output for a
given input, and the equilibrium state in each case may
be taken as a codeword C0 or C1. The codewords are
shown overlayed on the network graphs in Fig 2c. The
codewords are of maximum Hamming distance from each
other, meaning that C0 diﬀers from C1 by every possi-
ble bit. This is desirable, as it means the two states are
maximally distinguishable in the presence of noise. The
evolved organism also stabilizes the codeword: the state
returns to the codeword one or two timesteps after per-
turbation by noise.
What is remarkable is that error-correcting codes seem
to be a typical result of this experiment. The lower half
of Fig. 2 shows results from 150 independent evolutionary
experiments. These populations were all completely ran-
domly initialized, and the results were not pruned: every
population that was initialized has been included in the
statistics. The distribution of population ﬁtnesses over
time is summarized in Fig. 2c. As can be seen in Fig. 2d,

5
Surviving sequential
 mutations
Single mutation
lethality
a
Truth Table
 Mutations
Connectivity
 Mutations
b
c
Mutation lethality decreases
with 
p
log
p
log
FIG. 3. The eﬀect of error correction on evolvability. a The lethality of diﬀerent kinds of mutations for error-correcting
and non-error-correcting AND organisms. A mutation is "lethal" if an organism that computes AND perfectly in the absence
of noise can no longer do so after the mutation. Mutations to the truth tables consist of all possible bitﬂips applied to each
node’s truth tables. Mutations to the connectivity constitute adding or deleting a single connection between nodes at random.
The lethal mutation fraction is the fraction of all of the possible mutations that lead to death for a given organism. This is
plotted as a distribution over all organisms for each error type. b Comparing the probability of surviving several sequential
truth table mutations for error-correcting and non-error-correcting AND organisms. Error-correcting organisms can survive
three sequential mutations with the same probability that a non-error-correcting organism can survive one. c Comparing the
median logical error probability to the fraction of lethal truth table mutations for the populations of AND organisms from
Fig. 2. We see that the lethal mutation fraction decreases with logical error, a direct demonstration of the principle of error
correction enhanced evolvability.
over 80% of the organisms compute the primitive with
plog < pphys, with the median organism suppressing er-
ror by nearly an order of magnitude. Fig. 2e shows that
organisms accomplish this using error-correcting codes.
Most organisms develop codewords of nearly maximum
Hamming distance. Fig. 2f shows that the organism sup-
presses logical error super-linearly, a hallmark of error
correction.
It should be emphasized here that no prior information
about error-correcting codes was supplied to the evo-
lutionary procedure in the form of extra loss terms or
otherwise: the mathematical concept of error correction
emerges here exclusively from the primitive concept of
surviving a noisy environment. This separates this result
from previous studies where tailored optimization proce-
dures were used explicitly to ﬁnd good error-correcting
codes [45–47].
The overwhelming typicality of error-correcting codes
in these results is surprising. Complete convergence of so
many randomly initialized populations is not expected
for adaptive walks on rugged loss landscapes [33]. We
would expect many populations to get trapped in local
minima and not ﬁnd good codes. The implication of all
of this in the presence of noise evolutionary processes
somehow seek error correction. Note that the search of
conﬁguration space performed by evolution was nowhere
near exhaustive.
From Fig. 2c, we can see that con-
vergence was achieved in ∼106 generations, with ∼102
variations being explored each generation. The adaptive
procedure explored a minuscule fraction of conﬁguration
space, 108/1040 = 10−32. This is far beyond ﬁnding a
needle in a haystack (∼10−10) and is closer to looking in
the same haystack for a particular carbon atom.
One explanation is that error-correcting codes not only
protect an organism from noise that strikes randomly
during the dynamics but also from systematic manip-
ulation of the genome. This is supported by data pre-
sented in Fig. 3.
For non-error-correcting organisms,
most genome mutations are lethal, such that an organ-
ism that perfectly performs a particular computation in
the absence of noise before mutation no longer does so
after mutation. However, as shown in Fig. 3a, the same
is not true for error-correcting organisms, for which only
a small percentage of mutations are lethal.
The implication of this is error correction converts
many lethal mutations into neutral mutations, such that
organisms possessing error-correcting codes experience
more neutral mutations than those without.
This is
widely recognized as a sign of an evolvable property [48–
50].
It follows that error-correcting organisms can ac-
cumulate longer strings of mutations before suﬀering a
lethal mutation, as shown in Fig. 3b.
As illustrated in Fig. 3c, the lethal mutation fraction
decreases with logical error probability. Therefore, hav-
ing some error correction makes it easier to search for
more error correction, which will lead to error correction
being a typical result in noisy evolutionary processes.
This is the essence of the principle of error correction
enhanced evolvability. This principle explains the preva-
lence of error correction in our experimental results, and
possibly the natural world.
Interestingly, noise is re-
quired to bootstrap this process, as there is no evolution-

6
Module 1
Module 2
Second Output Bit
First Output Bit
0
1
0
1
Codewords
0
1
2
3
4
0
1
2
3
4
0
1
2
3
4
5
0
2
4
4
1
3
5
a
b
Generation 0
Generation 150k
Evolved with noise
Evolved without noise
c
f
C00
C01
C11
C00
C01
C11
C00 C01 C11
d
e
FIG. 4. Emergent structure in noisy adapting dynamic systems. a Organisms that have adapted to solve the parallel
AND task with and without noise, all other experimental parameters held equal. The structures of the noisy organisms reﬂect
the structure of the computational task, while the noiseless organisms have no obvious structure.
This implies that noise
alone can lead to modular dynamic systems. Experiments were also completed using the shared (b) and sequential (c) AND
tasks. The computational structure of these tasks is also reﬂected beautifully in the evolved organisms. d An example of the
codewords developed for the shared AND task. The organism possesses two distinct memory modules of equal size, each of
which implements an independent maximum Hamming distance code. e The codewords associated with the shared AND task,
which has three possible outputs. The codewords are all of maximum Hamming distance from one another. f The structure
of a network that computes a six-bit function before and after adaptation. We can see that the adaptive process imprints the
computational structure on the organism, and also leads to the pruning of many unnecessary nodes.
ary pressure to develop an initial, small error-correcting
code without it. Noise enhances evolutionary processes.
Mutations are unit distance movements in conﬁgura-
tion space. Random sequential mutations trace out ran-
dom walks in conﬁguration space, and a lethal mutation
occurs when this walk encounters a conﬁguration with a
ﬁtness that is too low to be survivable. Therefore, these
results may be equivalently understood as error correc-
tion locally smoothing over valleys in the ﬁtness land-
scape being searched by evolution.
Fig. 3a shows the
smoothing of the region that can be reached by a single
mutation and therefore may be interpreted as error cor-
rection reducing the magnitude of the "gradient" of the
ﬁtness with respect to the conﬁguration (although this
is not a true gradient, since the conﬁguration space is
discrete). Fig. 3b shows the same information over long
distances. This concept of error correction smoothing the
local ﬁtness landscape and allowing error-corrected or-
ganisms to search further for improvements is illustrated
by the cartoon in Fig. 1.
Fig. 4 shows that our results extend to composite com-
putational tasks. The most striking feature of these or-
ganisms is that when evolved in the presence of noise,
the structure of the computation is strongly imprinted on
the structure of the organism. This is shown clearly in
Fig. 4a. Here, a parallel AND task is used, in which two
non-interacting AND computations must be completed.
The task can be completed by two independent modules,
each containing a pair of inputs and their correspond-
ing outputs. When evolved in a noisy environment, this
is exactly the structure present in the organisms. Inde-
pendent modules saturate Newman’s modularity metric
[51]: this solution could not be any more modular. When
evolved in the absence of noise (with all other experimen-
tal parameters held constant) the structure vanishes, and
the solutions resemble random graphs. The story is the
same for tasks with more complicated modular struc-
tures, as shown in Fig. 4b and c. It seems that noise,
and noise alone, can be enough to drive the evolution of
structural modularity in dynamic systems.
Although the idea of modularity emerging naturally
to increase the robustness of systems to noise is old [52],
experimental demonstrations of this principle are rare.
Understanding the emergence of modularity in cognition
is a recent topic of interest.
Most methods that have
been developed involve imposing a special cost or envi-
ronmental constraint that encourages modularity, such as
rapidly varying a learning objective in a modular fashion
[53, 54] and associating a cost with adding connections
to a network [55]. Results on noise alone leading to mod-
ularity are sparse, and limited to small neural networks
[56]. Therefore, our results on modularity taken alone
are signiﬁcant, as they demonstrate a long-believed con-
cept within the physically relevant formalism of dynamic

7
systems.
It should be noted that like the primitive organ-
isms, the organisms shown in Fig. 4 also possess error-
correcting codes. As shown in Fig. 4d, the structurally
distinct modules seen in the shared AND organism also
make up functionally distinct memory modules.
Each
memory module implements an independent maximum
Hamming distance code for each output bit, akin to the
AND code shown in Fig. 1.
This modular error cor-
rection reﬂects the way the brain seems to implement
independent error correcting codes in diﬀerent regions
[18, 19, 23].
The way the modules are constructed is
also interesting, as from a ﬁtness standpoint each output
bit was made equally important and the same number of
nodes are dedicated to each module. This kind of code
is formally eﬃcient in the sense that the number of bits
it can encode scales linearly with the number of physical
bits (it is asymptotically a constant rate code). The se-
quential AND organism develops a diﬀerent kind of code,
as shown in Fig. 4e. In this case, memory modules would
not be an eﬃcient solution, as there are only 3 possible
outputs for this task: 00, 01, and 11. Instead, this organ-
ism develops a distributed maximum Hamming distance
code, reminiscent of Hamming codes themselves [57].
To explore the emergence of modularity at larger scales
while maintaining computational tractability, instead of
starting searches from random states one can seek to im-
prove the fault tolerance of a network that already per-
forms a particular computation in the absence of noise.
In particular, we sample random networks that compute
a function on a certain number of bits in the absence
of noise, and then evolve them in a noisy environment
and observe the results. An example of an organism that
implements a six-bit function is shown in Fig. 4f. We
can see that substantial structure is evolved. The ﬁnal
network is modular, with nodes 1 and 3 being distinctly
separated from the other nodes. Additionally, the evo-
lutionary process removes many of the nodes present in
the original network, suggesting they were useless to the
computation and only increased noise cross-section.
It can be hypothesized that the general principle be-
hind noise-induced modularity is that noise punishes net-
work size, both in terms of the number of connections
and the number of nodes. Noise punishes connectivity
because errors can propagate further and faster in more
densely connected networks. For example, the separate
modules evolved to solve the noisy parallel AND task pre-
vent errors that strike one module from ever propagating
to the other. In general, connectivity in the organisms
evolved to solve noisy tasks reﬂects the computationally
required connectivity to move information between in-
puts and outputs. The shared AND organisms generally
separate into two modules, with connections present be-
tween the two only to communicate the state of node
1. Similar features are observed in the noisy sequential
AND organisms.
Noise punishes network size because
larger networks have more components subject to error.
The experimental results presented in this paper
demonstrate that error correction and modularity go
hand-in-hand, and may be expected to emerge as a re-
sult of noisy evolutionary processes. Explicitly, we show
that error correction is a typical result of evolving to
perform a primitive computation in the presence of noise
and that this principle scales to small composite compu-
tations, where solutions that are both modular and error
correcting emerge. We reason that this occurs because of
error correction enhanced evolvability: systems possess-
ing error-correcting codes are easier to improve. With
this principle in hand, we believe that these results should
extend to larger scales more comparable with biological
information processing systems.
Such regimes are out
of reach of our current techniques as evolving orders of
magnitude larger organisms is presently computationally
intractable. Future work may explore alternative mod-
eling mechanisms, including direct physical implementa-
tions of noisy dynamics with electrical, mechanical, or
biological mechanisms.
AUTHOR CONTRIBUTIONS
TM designed and ran the experiments, and analyzed
the results.
TM, IRF, and ILC wrote and edited the
manuscript. TM wrote the supplemental material.
DATA AND CODE AVAILABILITY
The code used in this work to evaluate large batches of
Boolean networks on GPUs has been open-sourced and
is available on GitHub [40]. The code and data that can
be used to reproduce the experiments and ﬁgures in the
work are also available on GitHub [58].
ACKNOWLEDGMENTS
This work was supported in part by the Institute
for Artiﬁcial Intelligence and Fundamental Interactions
(IAIFI) through NSF Grant No. PHY-2019786, and by
NTT Research. TM would like to thank Cian Schmitt-
Ulms, Olive Garst, and Mikail Khona for their useful
comments that helped clarify the manuscript. TM would
also like to thank the administrators of the subMIT clus-
ter, on which many of the experiments presented in this
work were completed.

8
[1] A. Einstein, Ann. Phys 322, 549 (1905).
[2] J. von Neumann, in Cerebral Mechanisms in Behaviour,
edited by L. A. Jeﬀress (Wiley, 1951).
[3] J. V. Neumann and A. W. Burks, Theory of Self-
Reproducing Automata (University of Illinois Press, USA,
1966).
[4] J. von Neumann, “Probabilistic logics and the synthe-
sis of reliable organisms from unreliable components,” in
Automata Studies. (AM-34), Volume 34, edited by C. E.
Shannon and J. McCarthy (Princeton University Press,
Princeton, 1956) pp. 43–98.
[5] W. Carter, IEEE Transactions on Computers C-22, 225
(1973).
[6] A. Zlokapa,
A. K. Tan,
J. M. Martyn,
I. R. Fi-
ete,
M.
Tegmark,
and
I.
L.
Chuang,
(2022),
arXiv:2202.12887.
[7] P. W. Shor, Proceedings of 37th Conference on Founda-
tions of Computer Science , 56 (1996).
[8] G. Q. Ai and S. Feynman, Nature 614, 676 (2023).
[9] C. K. Andersen,
A. Remm,
S. Lazar,
S. Krinner,
N. Lacroix, G. J. Norris, M. Gabureac, C. Eichler, and
A. Wallraﬀ, Nature Physics 16, 875 (2020).
[10] S. Krinner, N. Lacroix, A. Remm, A. D. Paolo, E. Genois,
C. Leroux, C. Hellings, S. Lazar, F. Swiadek, J. Her-
rmann, G. J. Norris, C. K. Andersen, M. Müller, A. Blais,
C. Eichler, and A. Wallraﬀ, Nature 605, 669 (2022).
[11] C. E. Shannon, The Bell System Technical Journal 27,
379 (1948).
[12] S. Dow, A. Lucau-danila, K. Anderson, A. P. Arkin,
A. Astromoﬀ, M. E. Bakkoury, R. Bangham, R. Ben-
ito, S. Brachat, B. Andre, D. F. Jaramillo, D. E. Kelly,
S. L. Kelly, and P. Ko, Nature , 387 (2002).
[13] D. Deutscher, I. Meilijson, M. Kupiec, and E. Ruppin,
Nature Genetics 38, 993 (2006).
[14] D. A. Drachman, Neurology 64, 2004 (2005).
[15] T. H. Mosley, D. S. Knopman, D. J. Catellier, N. Bryan,
R. G. Hutchinson, C. A. Grothues, A. R. Folsom, L. S.
Cooper, G. L. Burke, D. Liao, and M. Szklo, Neurology
64, 2056 (2005).
[16] L. C. Faria, A. S. Rocha, J. H. Kleinschmidt, M. C.
Silva-Filho, E. Bim, R. H. Herai, M. E. Yamagishi,
and R. Palazzo, PLoS ONE 7 (2012), 10.1371/jour-
nal.pone.0036644.
[17] K. Yoon, M. A. Buice, C. Barry, R. Hayman, N. Burgess,
and I. R. Fiete, Nature Neuroscience 16, 1077 (2013).
[18] R. Chaudhuri, B. Gerçek, B. Pandey, A. Peyrache, and
I. Fiete, Nature Neuroscience 22, 1512 (2019).
[19] R. J. Gardner, E. Hermansen, M. Pachitariu, Y. Burak,
N. A. Baas, B. A. Dunn, M. B. Moser, and E. I. Moser,
Nature 602, 123 (2022).
[20] M. Khona and I. R. Fiete, Nature Reviews Neuroscience
23, 744 (2021).
[21] J. D. Franson, Nature (London) 482, 478 (2012).
[22] Quantum Error Correction (Cambridge University Press,
2013).
[23] S. Sreenivasan and I. Fiete, Nature Neuroscience 14, 1330
(2011).
[24] L. Zheng and W. Wang, Nature Communications 13
(2022), 10.1038/s41467-022-32911-y.
[25] D. Meunier, R. Lambiotte,
and E. Bullmore, Frontiers
in Neuroscience 4 (2010), 10.3389/fnins.2010.00200.
[26] M. Landau, Public Administration Review 29, 346
(1969).
[27] A. W. Lerner, Administration & Society 18, 334 (1986),
https://doi.org/10.1177/009539978601800303.
[28] I. Nonaka, California Management Review 32, 27 (1990),
https://doi.org/10.2307/41166615.
[29] N. Arndt, Nature 407, 458 (2000).
[30] M. Pigliucci, Nature Reviews Genetics 9, 75 (2008).
[31] J. L. Payne and A. Wagner, Nature Reviews Genetics 20,
24 (2019).
[32] E. D. Vaishnav, C. G. de Boer, J. Molinet, M. Yassour,
L. Fan, X. Adiconis, D. A. Thompson, J. Z. Levin, F. A.
Cubillos, and A. Regev, Nature 603, 455 (2022).
[33] S. A. Kauﬀman et al., The origins of order:
Self-
organization and selection in evolution (Oxford Univer-
sity Press, USA, 1993).
[34] S. Kauﬀman, Journal of theoretical biology. 22 (1969-3).
[35] S. A. Kauﬀman, Nature 224, 177 (1969).
[36] R. Thomas, Journal of Theoretical Biology 42, 563
(1973).
[37] T. Parr, G. Pezzulo, and K. J. Friston, Active Inference:
The Free Energy Principle in Mind, Brain, and Behavior
(The MIT Press, 2022).
[38] S. Kauﬀman, Scientiﬁc American 265, 78 (1991).
[39] F. Bertacchini, C. Scuro, P. Pantano,
and E. Bilotta,
Scientiﬁc Reports, Vol. 12 (Nature Publishing Group UK,
2022) pp. 1–31.
[40] T. McCourt, “Gpu accelerated kauﬀman networks,”
https://github.com/trevormccrt/gpuaffman_
networks (2023).
[41] C. Gershenson, (2004), arXiv:0408006 [nlin].
[42] I. Harvey and T. Bossomaier, Proceedings of the Fourth
European Conference on Artiﬁcial Life , 67 (1997),
arXiv:1104.0592.
[43] M.
M.
Vopson,
AIP
Advances
11
(2021),
10.1063/5.0064475.
[44] I. Shmulevich, E. R. Dougherty, S. Kim, and W. Zhang,
Bioinformatics 18, 261 (2002).
[45] A. Elkelesh, M. Ebada, S. Cammerer,
and S. t. Brink,
IEEE Transactions on Communications 67, 4521 (2019).
[46] L. Huang, H. Zhang, R. Li, Y. Ge, and J. Wang, IEEE
Transactions on Communications 68, 26 (2020).
[47] M. Jaraiz Simon, J. Gomez Pulido, M. Vega Rodriguez,
J. Sanchez Perez,
and J. Granado Criado, in MELE-
CON 2006 - 2006 IEEE Mediterranean Electrotechnical
Conference (2006) pp. 807–810.
[48] O. Tenaillon and I. Matic, Current Biology 30, R527
(2020).
[49] A. Wagner, Proceedings of the Royal Society B: Biologi-
cal Sciences 275, 91 (2008).
[50] R. E. Lenski, J. E. Barrick, and C. Ofria, PLoS Biology
4, 2190 (2006).
[51] M.
E.
J.
Newman,
Proceedings
of
the
Na-
tional
Academy
of
Sciences
103,
8577
(2006),
https://www.pnas.org/doi/pdf/10.1073/pnas.0601602103.
[52] G. Wagner, M. Pavlicev,
and J. Cheverud, Nature re-
views. Genetics 8, 921 (2008).
[53] N. Kashtan and U. Alon, Proceedings of the National
Academy of Sciences of the United States of America
102, 13773 (2005).

9
[54] N. Kashtan, E. Noor, and U. Alon, Proceedings of the
National Academy of Sciences of the United States of
America 104, 13711 (2007).
[55] J. Clune, J. B. Mouret,
and H. Lipson, Proceedings
of the Royal Society B: Biological Sciences 280 (2013),
10.1098/rspb.2012.2863.
[56] B. A. Høverstad, Artiﬁcial Life 17, 33 (2011).
[57] R. W. Hamming, The Bell System Technical Journal 29,
147 (1950).
[58] T. McCourt, “Code to accompany the paper "emer-
gent eﬀecient error correction and modularity in noisy
dynamic systems",” https://github.com/trevormccrt/
noise_emergence (2023).

Supplementary material to ”Noisy dynamical systems
evolve error correcting codes and modularity”
Trevor McCourt, Ila R. Fiete, Isaac Chuang
March 22, 2023
1
Boolean Networks
The state of an N-node Boolean network is given by a length N binary vector x. The
state of a particular node of a Boolean network xi is updated in time according to some
discrete update rule,
xi[t + 1] = fi(x[t])
(1)
Where fi is an arbitrary Boolean function. Traditionally, fi depends on k ≤N of the x,
such that each node in the network is inﬂuenced by exactly k other nodes. Each fi can
therefore be represented by a length 2k truth table. Applying the update rule over and
over again starting from some initial state generates a dynamic trajectory. An example
of an N = 4 k = 2 network is shown in Fig. 1.
t=0
t=1
t=2
t=3
a
b
Figure 1: An example Boolean network. a A trajectory of an N = 4 k = 2 Boolean
network. The network reaches a ﬁxed point (steady-state) at t = 2. b The truth ta-
bles associated with each of the 4 nodes that, along with an initial state, generate the
trajectory shown in (a).
This kind of constant k Boolean network can be compactly speciﬁed by two matrices.
The binary-valued function matrix F has dimension 2k × N, and has columns that are
the output row of the truth table for each node. For example, for the network shown in
Fig. 1,
F =




0
0
1
0
0
1
0
1
0
1
0
1
1
0
0
1




(2)
1
arXiv:2303.14448v2  [q-bio.PE]  12 Apr 2023

The second matrix is the integer-valued connectivity matrix C. This matrix has dimen-
sion k × N, and the columns contain the node labels that inﬂuence each node. For the
Fig. 1 network,
C =
1
2
3
0
3
0
1
2

(3)
We work with ragged networks, which are networks with variable k. Each node has
its own degree ki ≤kmax. An example of a ragged modiﬁcation of the Fig. 1 network is
shown in Fig. 2.
0
1
2
3
Figure 2: An example ragged Boolean network
It is desirable to have a way of specifying ragged networks using rectangular data
structures, such that batches of network speciﬁcations can be packed into tensors for
accelerated simulation. To do this, we maintain the F and C matrices described above,
which now are of dimension 2kmax × N and kmax × N, respectively. We then deﬁne a
binary masking matrix U that indicates which connections in C are active. For example,
the U matrix used to generate the Fig. 2 network from the Fig. 1 network is,
U =
1
0
1
1
1
1
0
1

(4)
Therefore, a ragged boolean network can be speciﬁed by the triplet of matrices (F, C, U).
2
Accelerating Boolean Network Evaluation
With the data structures described in Sec. 1 in hand, it is straightforward to accelerate
the evaluation of large batches of networks. The updating of a network can be handled
using vectorized tensor operations. We’ve open-sourced our python implementation of the
update rule on GitHub [4]. Networks can be evaluated either on a CPU using the NumPy
library [2] or on an NVIDIA GPU using CuPy, [6].
In practice, GPUs substantially
accelerate the evaluation of large batches of networks, see Fig. 3.
2

103
104
105
106
107
108
Batch Size
106
107
108
109
Update Rate [Hz]
Binary Function Apply Rate, k=3
Numpy
Cupy
Figure 3: Applying batches of binary functions on the CPU vs GPU A GPU can
accelerate the simulation of large batches of Boolean networks by more than 2 orders of
magnitude. The GPU used here was an NVIDIA RTX 3090 and the CPU was an AMD
Ryzen 9 5950x
3
XOR Evolution Details
One curious diﬀerence between AND and XOR is that it seems higher connectivity is re-
quired to fault-tolerantly perform XOR. Fig. 4 shows the results of many (short, only 50k
generations run) evolutionary experiments comparing AND and XOR for many diﬀerent
values of N and kmax.
a
b
Figure 4: The evolutionary characteristics of AND and XOR for diﬀerent values
of N and kmax Comparing the ﬁtness of many independent AND (a) and XOR (b)
populations with diﬀerent values of N and kmax after 50k generations of evolution. It
seems that k = 4 is required for XOR to reach low error rates (at least in the same
amount of time as AND).
3

This result is reasonable, as it is well known that XOR is a more complicated function
to learn than AND [5]. The reason for this is XOR has a more complicated truth table,
with exactly half the bits being one (as opposed to only 1 bit for AND). As a result of
this, we used kmax = 3 for AND and kmax = 4 for XOR in all of our experiments.
Fig. 5 shows an example XOR network and noisy trajectory.
a
b
Figure 5: An XOR network and a noisy trajectory a A network that implements
XOR. b A noisy trajectory of an XOR network.
Note that the shown network only implements a distance 7 code, one bit is the same
in each codeword. This seems to be typical of XOR networks, as shown in the main text
ﬁgure 2. This is reasonable, as the evolutionary pressure to increase the code distance
from 7 to 8 is low, and the optimization problems are more diﬃcult for a kmax = 4
network compared to a kmax = 3 network, since the conﬁguration space is larger and the
loss landscape is more correlated [3].
4
Eﬀects of Various Crossover Operators
Crossover operators are used in genetic algorithms to combine parent organisms to form
a child organism, in the hope that the child will inherit the best features of all the parents
and be of higher ﬁtness. In a traditional genetic algorithm, crossover is applied to the
ﬁttest organisms in a population, and the children are then mutated and added back to
the population. If no crossover operator is applied, we simply mutate one of the parents,
and this process becomes equivalent to a pure random adaptive walk (greedily moving in
random directions that improve ﬁtness). We tried a few diﬀerent crossover operators in
this work and found that they were only slightly higher performing than a pure adaptive
walk. This is shown in 6.
4

102
103
104
105
Number of Generations
10
2
10
1
Logical Error Rate
Rectangular Crossover
Graph Crossover
No Crossover
Figure 6: Evolving AND with various crossover operators
Rectangular crossover refers to splicing together the F, C, and U matrices of two
organisms. This is done by cutting each set of matrices vertically in the same position
and re-assembling the components. This method does not respect the structure of the
graphs, as the node label number is arbitrary.
Graph crossover refers to cutting the
graphs of two parents and wiring them back together in a way that respects the original
connectivity [1].
The rectangular crossover slightly outperforms no crossover, converging to the same
minimum error rate slightly before the random walk. This diﬀerence is small. Interest-
ingly, it seems the graph crossover operation is the lowest performing, although it could
not be tested to convergence as it is computationally intensive and requires transferring
a large amount of data from GPU to CPU every generation.
References
[1]
Al Globus et al. “JavaGenes and Condor: cycle-scavenging genetic algorithms”. In:
(Dec. 2002), pp. 134–139. doi: 10.1145/337449.337524.
[2]
Charles R. Harris et al. “Array programming with NumPy”. In: Nature 585.7825
(Sept. 2020), pp. 357–362. doi: 10.1038/s41586-020-2649-2. url: https://doi.
org/10.1038/s41586-020-2649-2.
[3]
Stuart A Kauﬀman et al. The origins of order: Self-organization and selection in
evolution. Oxford University Press, USA, 1993.
[4]
T McCourt. GPU accelerated Kauﬀman networks. https://github.com/trevormccrt/
gpuaffman_networks. 2023.
[5]
Marvin Minsky and Seymour A. Papert. Perceptrons: An Introduction to Computa-
tional Geometry. The MIT Press, Sept. 2017. isbn: 9780262343930. doi: 10.7551/
mitpress/11301.001.0001. url: https://doi.org/10.7551/mitpress/11301.
001.0001.
5

[6]
Ryosuke Okuta et al. “CuPy: A NumPy-Compatible Library for NVIDIA GPU Cal-
culations”. In: Proceedings of Workshop on Machine Learning Systems (LearningSys)
in The Thirty-ﬁrst Annual Conference on Neural Information Processing Systems
(NIPS). 2017. url: http://learningsys.org/nips17/assets/papers/paper_16.
pdf.
6

