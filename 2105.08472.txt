arXiv:2105.08472v1  [math.NA]  18 May 2021
Yet another eigenvalue algorithm for solving
polynomial systems
Mat´ıas R. Bender*
Simon Telen†
Abstract
In latest years, several advancements have been made in symbolic-numerical
eigenvalue techniques for solving polynomial systems. In this article, we add to
this list by reducing the task to an eigenvalue problem in a considerably faster and
simpler way than in previous methods. This results in an algorithm which solves
systems with isolated solutions reliably and efﬁciently, outperforming homotopy
methods in overdetermined cases. We provide an implementation in the proof-of-
concept Julia package EigenvalueSolver.jl.
Key words — polynomial systems, eigenvalue theorem, symbolic-numerical algorithm
AMS subject classiﬁcations — 65H04, 65H10
1
Introduction
Polynomial systems arise in many areas of applied science [39, 20]. This paper is concerned with
solving such systems of equations using numerical computations, that is, using ﬁnite precision,
ﬂoating point arithmetic. Two important classes of numerical algorithms are algebraic algorithms
[29] and homotopy continuation methods [16, 39]. See [20, Ch. 2] for an overview. In this work,
we focus on algorithms of the former type.
Algebraic algorithms are also called eigenvalue algorithms. They consist of two steps. Step
(A) uses linear algebra operations to reduce the problem to an eigenvalue problem or univariate
polynomial root ﬁnding problem. Step (B) is to solve the eigenvalue or univariate root ﬁnding prob-
lem using numerical tools. Classical examples include Gr¨obner basis and resultant algorithms, see
[23, Ch. 2] or [6]. These use symbolic manipulations for step (A), pushing the numerical linear
algebra back to the eigenvalue computation in step (B). The reason for this is that, when performed
in ﬁnite precision arithmetic, these approaches are numerically unstable for step (A), see for in-
stance [34]. Border basis methods have been developed to remedy this unstable behaviour [36, 40]
and variants based on nullspace computations were introduced in [25]. Methods for performing
step (B) are based on linear algebra [19] or, recently, on multilinear algebra [46].
Two special types of structured matrices play a central role in algebraic algorithms: Macaulay
(or Sylvester) matrices and multiplication matrices.
Macaulay matrices have a sparse, quasi-
Toeplitz structure. They contain the coefﬁcients of the equations and are manipulated in step
(A). The result of these manipulations is a set of multiplication matrices. These are structured
*Department of Mathematics, Technische Universit¨at Berlin, mbender@math.tu-berlin.de
†Max Planck Institute for Mathematics in the Sciences, Leipzig, simon.telen@mis.mpg.de
1

in the sense that they commute. Multiplication matrices represent multiplication operators in the
coordinate ring of the solution set [23, Ch. 5] and their eigenstructure reveals the coordinates of
the solutions [22, Ch. 2].
In practice, to construct these multiplication matrices, we need to choose a basis for the afore-
mentioned coordinate ring. The numerical stability of step (A) strongly depends on this choice
[44]. Gr¨obner and border basis methods use bases corresponding to special sets of monomials.
For instance, they require these monomials to come from a monomial ordering [23, Ch. 2,§2] or
to be ‘connected-to-1’ [36]. Recent developments showed that numerical linear algebra heuristics
can be applied to choose bases that improve the accuracy substantially [44]. This has lead to the
development of truncated normal forms [43], which use much more general bases of monomials
coming from QR factorizations with optimal column pivoting, or non-monomial bases coming
from singular value decompositions or Chebyshev representations [37].
Other than making a good choice of basis, in order to stabilize algebraic algorithms it is neces-
sary to take solutions at inﬁnity into account. Loosely speaking, a polynomial system has solutions
at inﬁnity if the slightest random perturbation of the nonzero coefﬁcients introduces new solutions
with large coordinates. This is best understood in the language of toric geometry [24]. Situations in
which there are ﬁnitely many solutions at inﬁnity (see Assumption 1) can be handled by introduc-
ing an extra randomization in the algorithm, which was ﬁrst used in [41, 11]. Where classically the
multiplication matrices represent ‘multiplication with a polynomial g’, the multiplication matrices
in these papers represent ‘multiplication with a rational function g/ f0’, where f0 is a polynomial
that does not vanish at any of the solutions to the system. For details and a geometric interpreta-
tion, we refer to [41, 11]. We will use a similar approach in this paper. Choosing the denominator
randomly is essential because the conditions in Lemma 2.1 may not be satisﬁed for f0 = 1, while
they are for a generic f0.
We summarize the contributions of the present paper. First, we adapt the eigenvalue the-
orem [22, Ch. 2, Thm. 4.5] to reduce the problem of solving polynomial systems to the com-
putation of eigenvalues. Our new version allows to compute solutions from matrices that need
not to have the classical interpretation of multiplication matrices, but can be obtained from much
smaller Macaulay matrices (Theorem 2.1). We propose an easy-to-state and easy-to-verify cri-
terion for Macaulay matrices to be ‘large enough’ for constructing such matrices (Lemma 2.1).
We distil these new insights, together with the recent advances in numerical eigenvalue algo-
rithms explained above, into an algorithm (Algorithm 2). We introduce the notion of admissi-
ble tuples (Deﬁnition 2.1), which parametrize Macaulay matrices satisfying our criterion from
Lemma 2.1 and show how to construct such tuples for structured systems of equations. The
discussion includes an algorithm for computing admissible tuples for overdetermined, unmixed
systems (Algorithm 3). We provide a Julia implementation of our algorithms, available online at
https://github.com/simontelen/JuliaEigenvalueSolver. Our experiments illustrate the
efﬁciency and accuracy of this package. They contain a comparison with the state-of-the-art ho-
motopy continuation package HomotopyContinuation.jl [14]. We show that our eigenvalue
methods are competitive, and in strongly overdetermined cases, they are considerably faster.
To make the paper accessible to a wide audience, we state most of our results and proofs using
only terminology from linear algebra. For results that require more background in algebraic (and
in particular toric) geometry, we sketch proofs and provide full references.
The paper is organized as follows. In Section 2, we introduce our adapted eigenvalue theorem,
admissible tuples and our algorithm. In Section 3, we present constructions for admissible tuples
for different families of polynomial systems. Finally, in Section 4, we demonstrate the effective-
2

ness of our algorithms through extensive numerical experimentation. Our computations are done
using the Julia package EigenvalueSolver.jl.
2
The algorithm
Consider the polynomial ring R := C[x1,...,xn] and a tuple of s polynomials F := ( f1,..., fs) ∈Rs,
with s ≥n. Our aim in this section is to present an algorithm for solving the system of equations
F(x) = 0, where we use the short notation x for (x1,...,xn). A point ζ ∈Cn is called a solution of
F if F(ζ) = 0, that is, fi(ζ) = 0 for every i ∈{1,...,s}. For a vector α = (α1,...,αn) ∈Nn, we
denote by xα the monomial ∏n
i=1 xαi
i ∈R. We say that α is the exponent of the monomial xα. In
what follows, we write each polynomial fi as
fi := ∑
α∈Nn
ci,α xα.
where ci,α ∈C are the coefﬁcients of fi and ﬁnitely many of them are nonzero. We deﬁne the
support Ai of fi as the set of exponents α ∈Nn corresponding to non-zero coefﬁcients ci,α ∈C,
Ai := {α ∈Nn : ci,α ̸= 0}.
Given two subsets E1,E2 ⊂Nn, we denote by E1 +E2 the Minkowski sum of E1,E2, that is,
E1 +E2 := {α +β : α ∈E1,β ∈E2}.
For a ﬁnite set of exponents E ⊂Nn we denote by RE the subvector space of R spanned by the
monomials with exponent in E. That is,
RE :=
M
α∈E
C ·xα.
Observe that, given g1 ∈RE1 and g2 ∈RE2, we have that g1g2 ∈RE1+E2.
Consider a tuple of s ﬁnite sets of exponents E := (E1,...,Es), where Ei ⊂Nn, and another
ﬁnite set of exponents D ⊂Nn such that for every i ∈{1,...,s}, D contains the exponents in Ai+Ei.
An essential ingredient for our eigenvalue algorithm is the Sylvester map
Sylv(F,E;D) :
RE1 ×···×REs
→
RD
(g1,...,gs)
7→
∑i gi fi.
This is a linear map between ﬁnite dimensional vector spaces, so we can represent it by a matrix
M(F,E;D) ∈C#D×(∑i #Ei).
Matrices obtained by using the standard monomial bases for the vector spaces REi and RD in
this representation are often called Macaulay matrices. We index the rows of the matrix with
the exponents belonging to D and the columns with pairs {(i,βi) : i ∈{1,...,s},βi ∈Ei}. The
(α,(i,βi))-entry of M(F,E;D) contains the coefﬁcient ci,(α−βi) of fi, that is,
M(F,E;D)(α,(i,βi)) := ci,α−βi.
Observe that this coefﬁcient might be zero. The ordering of the exponents is of no importance in
the scope of this work. We will therefore not specify it and assume that some ordering is ﬁxed for
all tuples Ai,Ei,D throughout the paper.
3

Example 1. To avoid subscripts, we replace the variables x1 and x2 by x and y, respectively.
Consider the sets of exponents A1,...,A3 and the system F := ( f1, f2, f3) given by
A1 := {(0,0),(1,0),(0,1),(0,2)},
A2 := {(0,0),(1,0),(2,0),(0,1)},
A3 := {(0,0),(1,0),(2,0),(0,1)},
f1 := −1+2x+2y +y2
∈RA1,
f2 := −1+ x+ x2 +y
∈RA2,
f3 := −1+2x+2x2 +y
∈RA3.
We construct the Macaulay matrix M(F,E;D), where E := (E1,E2,E3) and
E1 := {(0,0),(1,0)},
E2 = E3 := {(0,0),(0,1)},
D :=
{(0,0),(1,0),(2,0),(0,1),
(1,1),(2,1),(0,2),(1,2)}.
M(F,E;D) =
f1
x f1
f2
y f2
f3
y f3
1
−1
−1
−1
y
2
1
−1
1
−1
y2
1
1
1
x
2
−1
1
2
xy
2
1
2
xy2
1
x2
2
1
2
x2 y
1
2
△
Remark 2.1. Given ζ ∈Cn and a ﬁnite subset E ⊂Nn, we denote by ζ E the row vector
(ζ α : α ∈E).
The vector obtained by the product ζ D · M(F,E;D) ∈C∑i #Ei is indexed by the tuples {(i,βi) :
i ∈{1,...,s},βi ∈Ei} and the (i,βi)-entry is given by fi(ζ)ζ βi. If ζ is a solution of F, then ζ D
belongs to the cokernel of M(F,E;D). Moreover, if ζ ∈Cn is such that ζ Ei ̸= 0 for all i, the
opposite implication also holds. This is the case, for instance, for any solution ζ ∈(C\{0})n.
We deﬁne the value HF(F,E;D) as the corank of M(F,E;D):
HF(F,E;D) := #D−Rank(M(F,E;D)).
Let Coker(F,E;D) ∈CHF(F,E;D)×#D be a cokernel matrix (or left null space matrix) of M(F,E;D).
That is, Coker(F,E;D) has rank HF(F,E;D) and
Coker(F,E;D)·M(F,E;D) = 0
We will index the columns of Coker(F,E;D) with the exponents in D.
Example 2 (Cont.). The system F has one solution (−1,1) ∈C2. The vector
(−1,1)D = (1,1,1,−1,−1,−1,1,1)
belongs to the cokernel of M(F,E;D). Moreover, we have that HF(F,E;D) = 2 and
Coker(F,E;D) =
1
y
y2
x
xy
xy2
x2
x2 y


1
1
1
−1
−1
−1
1
1
0
0
0
0
−1
2
0
1 .
△
4

Consider two ﬁnite sets of exponents A0,E0 ∈Nn such that A0 +E0 ⊂D. For each polynomial
f0 ∈RA0, we deﬁne the matrix Nf0 as
Nf0 := Coker(F,E;D)·M( f0,E0;D).
Observe that Nf0 ∈CHF(F,E;D)×#E0 and the columns of Nf0 are indexed by the exponents in E0
(more precisely, by the pairs (0,α) for each α ∈E0).
Lemma 2.1. For any f0 ∈RA0 we have that
HF(( f0,F),(E0,E);D) = 0 ⇐⇒Nf0 has rank HF(F,E;D),
where ( f0,F) = ( f0, f1,..., fs) and (E0,E) = (E0,E1,...,Es). Moreover, in that case, for every
solution ζ ∈Cn of F such that the vector ζ D is non-zero, we have f0(ζ) ̸= 0.
Proof. The ⇒direction of the ﬁrst statement follows directly from
  Nf0
0 
= Coker(F,E;D)·M(( f0,F),(E0,E);D).
For the ⇐direction, suppose that Nf0 has rank HF(F,E;D). Then Coker( f0,E0;D)∩Coker(F,E;D) =
{0}, which implies that the cokernel of M(( f0,F),(E0,E);D) is trivial, and hence it has rank
#D. The second statement follows from the fact that M(( f0,F),(E0,E);D) has trivial coker-
nel; as ζ D is a non-zero vector, if f0(ζ) = 0, by Remark 2.1, ζ D belongs to the cokernel of
M(( f0,F),(E0,E);D).
Example 3 (Cont.). We consider the sets of exponents A0,E0 and the polynomial f0 given by
A0 = {(0,0),(1,0),(0,1)},
E0 = {(0,0),(1,0),(0,1)},
f0 := 1+3x+y ∈RA0.
In this case, we have
Nf0 =
1
x
y


−1
1
−1
0
−1
3 .
△
In what follows, we say that a property holds for generic points of a vector space if it holds
for all points not contained in a subset of Lebesgue measure zero. Note that Nf0 is a matrix whose
entries depend linearly on the coefﬁcients of f0. This means that if there exists f0 ∈RA0 such that
Nf0 has rank HF(F,E;D), then rank(Nh) = HF(F,E;D) for generic elements h ∈RA0. Below,
we assume that there is f0 ∈RA0 such that rank(Nf0) = HF(F,E;D) and we ﬁx such an f0 ∈RA0.
This assumption is very mild and given F,A0,E,D, it is easy to check if it holds.
For ease of notation, we will write γ := HF(F,E;D). Given a set of exponents B ⊂E0, we
deﬁne the submatrix Nf0,B = Coker(F,E;D)·M( f0,B;D) ∈Cγ×#B of Nf0 consisting of its columns
indexed by B. We ﬁx B ⊂E0 of cardinality γ such that Nf0,B ∈Cγ×γ is invertible. For each g ∈RA0,
we deﬁne the matrix Mg ∈Cγ×γ, deﬁned as
Mg := Ng,B ·N−1
f0,B.
5

Example 4 (Cont.). We ﬁx the basis B = {1,x} and the matrix Nf0,B =
−1 1
0 −1

. Then, for g =
−1+3x+2y, we have
Mg =

−2
2
0
−2

·

−1
−1
0
−1

=

2
0
0
2

and
Mx =

1
0
0
0

△
Remark 2.2. The map g ∈RA0 7→Mg ∈Cγ×γ is a linear map. That is, for λ ∈C, g1,g2 ∈RA0,
Mg1+λ g2 = Mg1 +λ Mg2.
Moreover, Mf0 is the identity matrix.
A key observation is that we can solve the system of equations F(x) = 0 by computing the
eigenstructure of these matrices Mg, for g ∈RA0. For that, we adapt the classical eigenvalue the-
orem from computational algebraic geometry [22, Ch. 2, Theorem 4.5] or [21] for a historical
overview. We say that a non-zero row vector v is a left eigenvector of a matrix M with correspond-
ing eigenvalue λ if it satisﬁes v·M = λv.
Theorem 2.1 (Eigenvalue theorem). For each solution ζ ∈Cn of F such that ζ D ̸= 0, Mg has a
left eigenvector vζ such that vζ · Coker(F,E;D) = ζ D. The corresponding eigenvalue is g
f0(ζ).
Conversely, if v is a left eigenvector of Mg such that v· Coker(F,E;D) is proportional to ζ D ̸= 0
for some ζ ∈Cn such that ζ Ei ̸= 0 for all i, then ζ is a solution of F. Moreover, the corresponding
eigenvalue of Mg is g
f0(ζ).
To prove this theorem, we need two auxiliary lemmas.
Lemma 2.2. Let ζ ∈Cn be a solution of F such that ζ D ̸= 0 and let g ∈RA0 be such that g(ζ) = 0.
Then, the matrix Mg is singular.
Proof. By Remark 2.1, ζ D belongs to the cokernel of M(F,E;D), hence there is a row vector vζ ∈
Cγ \{0} such that vζ ·Coker(F,E;D) = ζ D. Moreover, ζ D belongs to the cokernel of M(g,E0;D).
Hence, vζ ·Ng = 0 and so vζ ·Ng,B = 0.
Lemma 2.3. Let ζ ∈Cn be a solution of F. If ζ B = 0, then ζ D = 0.
Proof. Since ζ is a solution of F, there is a rwo vector vζ such that vζ ·Coker(F,E;D) = ζ D. By
observing that Nf0,B = Coker(F,E;D)·M( f0,B;D), the lemma follows from
vζ ·Nf0,B = (vζ ·Coker(F,E;D))·M( f0,B;D)
= ζ D ·M( f0,B;D)
= (ζ α f0(ζ) : α ∈B) = 0,
where the last line uses ζ B = 0. Since Nf0,B is invertible, this implies vζ = 0, and thus ζ D = 0.
Proof of Theorem 2.1. The proof is based on the following observations. By Remark 2.2, the
eigenvalues of Mg correspond to the values λ ∈C such that Mg −λ id = Mg−λ f0 is singular. By
Lemma 2.1, we have f0(ζ) ̸= 0 for each solution ζ of F. Let ζ be a solution of F. If λ = g
f0(ζ),
the polynomial g −λ f0 ∈RA0 vanishes at ζ. As by assumption ζ D ̸= 0, from Lemma 2.2 we
6

deduce that Mg−λ f0 is singular. Therefore, g
f0(ζ) is an eigenvalue of Mg. For the associated left
eigenvector, let vζ be as in the proof of Lemma 2.2. We have vζ ·Ng−λ f0,B = vζ ·(Ng,B−λNf0,B) = 0
and multiplying from the right by N−1
f0,B gives vζ ·(Mg −λid) = 0.
Conversely, suppose that v is a left eigenvector of Mg such that v· Coker(F,E;D) = ζ D ̸= 0
for some ζ ∈Cn (we may assume equality after scaling). By Remark 2.1, under the assumption
ζ Ei ̸= 0 for all i, ζ is a solution of F, see Remark 2.1. We now compute the corresponding
eigenvalue. By deﬁnition, v· (Mg −λid) = 0 for some λ. Multiplying from the right by Nf0,B we
see that v · Ng−λ f0,B = (v · Coker(F,E;D)) · M(g −λ f0,B,D) = ζ D · M(g −λ f0,B,D) = 0. By
Lemma 2.3, ζ B ̸= 0 and since f0(ζ) ̸= 0 (Lemma 2.1) we conclude λ = g
f0(ζ).
Example 5 (Cont.). The unique eigenvalue of Mg =
 2 0
0 2

is 2 = g
f0(−1,1). Moreover, we have
that 1 = x
f0(−1,1) is an eigenvalue of Mx =
 1 0
0 0

whose associated eigenvector (1,0) satisﬁes
(1,0)·Coker(F,E;D) = (−1,1)D.
△
We now characterize row vectors v that are an eigenvector of all matrices in M := {Mh : h ∈
RA0}. Observe that, by Remark 2.2, M is a vector space. For any nonzero row vector v ∈Cγ \{0},
we deﬁne the subspace
M(v) = {Mh ∈M : v is a left eigenvector of Mh}.
One can check that M(v) is a vector space. We say that v is an eigenvector of M if M(v) = M.
Proposition 2.1. If v is a left eigenvector of Mg for some g ∈RA0, then for generic h ∈RA0 we have
that v is an eigenvector of M if and only if v is a left eigenvector of Mh.
Proof. The ‘only if’ direction is clear. For the ‘if’ direction, suppose that v is an eigenvector of
Mh but not an eigenvector of M. Then Mh ∈M(v) ⊊M. As M(v) is a proper subspace, this is
a contradiction for generic Mh. Note that the linear map RA0 →M : q 7→Mq from Remark 2.2 is
surjective, so that the pre-image of a proper subspace of M is a proper subspace of RA0. Hence,
we arrive at a contradiction for generic h ∈RA0.
Example 6 (Cont.). Any vector in C2 is an eigenvector of Mg =
2 0
0 2

, but for h = 1 +x +y, we
obtain Mh =
 −1 0
0 1

, which has eigenvectors (1,0) and (0,1). These vectors are also eigenvectors
of Mx =
1 0
0 0

. It is straightforward to check that {Mg,Mh,Mx} generate M as a vector space, so
that (1,0) and (0,1) are eigenvectors of M.
△
Proposition 2.2. If v1,...,vm are eigenvectors of M, then for generic g ∈RA0 we have that
v1,...,vm correspond to the same eigenvalue of Mg if and only if v1,...,vm correspond to the
same eigenvalue of Mh for all Mh ∈M.
Proof. Again, the ‘only if’ direction is clear. For the opposite implication, note that
W = {Mh ∈M : v1,...,vm correspond to the same eigenvalue of Mh}
is a vector subspace of M. If v1,...,vm correspond to the same eigenvalue of Mg but this does not
hold for any Mh ∈M, then W ⊊M and Mg must belong to a proper subspace. As in the proof of
Proposition 2.1, this translates to g belonging to a proper subspace of RA0.
7

Propositions 2.1 and 2.2 give a simple procedure for computing, for a given eigenvalue λg
of Mg, the intersection of the corresponding left eigenspace of Mg with the eigenvectors of M.
Suppose that this eigenspace is spanned by the rows of the matrix Vλg. We simply need to check
which elements in the row span of Vλg are also eigenvectors of Mh, for a random element h ∈RA0.
Proposition 2.2 guarantees that these eigenvectors, if they exist, belong to a unique eigenvalue
of Mh. This is summarized in Algorithm 1. In line 8 of the algorithm, we solve the generalized
eigenvalue problem (GEP) given by the pencil (B1,B2) := (Vλg ·Mh·O,Vλg ·O), that is, we compute
all eigenvalues µi and a basis for the left eigenspace Ci = {ci ∈Cm | ciB1 = µiciB2}. In line 9, we
select (if possible) the unique eigenvalue µi whose corresponding eigenspace Ci gives the desired
intersection V = Ci ·Vλg.
Algorithm 1 GETEIGENSPACE
Input: An eigenvalue λg of Mg for generic g ∈RA0, a matrix Vλgof size m×γ whose rows contain
a basis for the corresponding eigenspace and the matrix Mh for a generic h ∈RA0
Output: A matrix V whose rows are a basis for the intersection of the row span of Vλg with the
eigenvectors of M.
1: if m = 1 then
2:
if rank

Vλg
Vλg ·Mh

= 1 then
3:
V ←Vλg
4:
else
5:
V ←{0}
6: else
7:
O ←random matrix of size γ ×m
8:
{(µi,Ci)} ←solve the GEP ci ·(Vλg ·Mh ·O) = µici ·(Vλg ·O)
9:
C ←Ci such that Ci ·Vλg ·Mh = µi ·Ci ·Vλg
10:
if C is empty then
11:
V ←{0}
12:
else
13:
V ←C ·Vλg
14: return V
Proposition 2.2 also has the following direct corollary.
Corollary 2.1. Let λg be an eigenvalue of Mg and let V ∈Cm×γ be a matrix whose rows are a
basis for the left eigenspace of Mg corresponding to λg intersected with the eigenvectors of M. If
g is generic, there is exactly one tuple (λα)α∈A0 such that
VMxα = λαV,
for all α ∈A0.
Remark 2.3. This has the practical implication that ˜M = VMxαT(VT)−1 = diag(λα,...,λα) for a
random matrix T ∈Cγ×m has only one eigenvalue λα, equal to trace( ˜M)/m. If V has only one row
we obtain λα from the Rayleigh quotient λα = VMxαV∗/(VV∗), where ∗is the conjugate transpose.
Proposition 2.3 (Criterion for eigenvalues). Let λg be an eigenvalue of Mg and let V ∈Cm×γ be a
matrix whose rows are a basis for the left eigenspace of Mg corresponding to λg intersected with
8

the eigenvectors of M. Then λg = g
f0(ζ) for some solution ζ ∈Cn of F satisfying ζ D ̸= 0 if and
only if the tuple (λα)α∈A0 from Corollary 2.1 satisﬁes
C ·(λα)α∈A0 = ζ A0 for some C ∈C\{0}.
Proof. If λg = g
f0(ζ) for some solution ζ ∈Cn of F satisfying ζ D ̸= 0, then by Theorem 2.1 we
know that vζ is a corresponding eigenvector of M. Therefore, there exists cζ ∈Cm \{0} such that
cζV = vζ and another consequence of Theorem 2.1 is
cζVMxα = xα
f0
(ζ)cζV.
Conversely, suppose that cVMxα = λαcV, where C · (λα)α∈A0 = ζ A0 for some C ∈C \ {0}. If
h = ∑α∈A0 dh,αxα we have that
cV
 
∑
α∈A0
dh,αMxα
!
= cVMh =
 
∑
α∈A0
dh,αλα
!
cV = h(ζ)
C cV.
Setting h = g we see that λg = g(ζ)/C and setting h = f0 we ﬁnd C = f0(ζ).
The results discussed above suggest several ways of extracting the coordinates of a solution
ζ ∈Cn of F(x) = 0 form the eigenstructure of the matrices Mg. Both the eigenvectors (Theorem
2.1) and the eigenvalues (Proposition 2.3) reveal vectors of the form ζ A for some set of exponents
A ⊂Nn. We now recall how to compute the coordinates of ζ from the vector ζ A and discuss the
assumptions that we need on A in order to be able to do this.
For any subset A ⊂Nn, we write
NA := { ∑
α∈A
nα ·α : nα ∈N} ⊂Nn,
ZA := { ∑
α∈A
mα ·α : mα ∈Z} ⊂Zn.
If A = {α1,...,αk} with α1 = 0 and the condition ZA = Zn is satisﬁed, then for ℓ= 1,...,n, there
exist integers m2,ℓ,...,mk,ℓsuch that m2,ℓα2 +···+mk,ℓαk = eℓ, where eℓis the ℓ-th standard basis
vector of Zn. These integers mj,ℓcan be computed, for instance, using the Smith normal form of an
integer matrix whose columns are the elements of A. If this is the case, from ζ A = (ζ α1,...,ζ αk)
we can compute the ℓ-th coordinate ζℓof ζ as
ζℓ=
k
∏
j=2
(ζ α j)m j,ℓ,
ℓ= 1,...,n.
(2.1)
This approach can be used to compute the coordinates of ζ ∈(C\{0})n from ζ A, i.e. all points with
all non-zero coordinates. Note that some of the mj,ℓmay be negative, which may be problematic
in the case where ζ has zero coordinates. If the stronger condition NA0 = Nn is satisﬁed (this
implies eℓ∈A, ℓ= 1,...,n), then the integers mj,ℓcan be taken non-negative and we can obtain
the coordinates of all points ζ in Cn from ζ A. We will continue under the assumption that we
are mostly interested in computing points in (C \ {0})n, as this is commonly assumed in a sparse
setting. However, solutions in Cn can be computed by replacing ZA = Zn in what follows by the
stronger assumption NA = Nn. Note that if ZA = Zn, the outlined approach suggests a way of
9

checking whether or not a vector q ∈C#A with qα1 = 1 is of the form ζ A for some ζ ∈(C\{0})n.
Indeed, one computes the coordinates ζℓ= ∏k
j=2(qα j)m j,ℓand checks whether ζ A = q.
We turn to the eigenvalue method for extracting the roots ζ from the matrices Mg.
Let
{ζ1,...,ζδ} ⊂Cn be a set of solutions of F such that ζ D
i ̸= 0 for all i. By Theorem 2.1, for
each of these solutions there is an eigenvalue λg of the matrix Mg and a space of dimension m ≥1,
spanned by the rows of a matrix V, of eigenvectors of M. Suppose we have computed this matrix
V (for instance, using Algorithm 1). We write A0 = {α1,...,αk} ⊂Nn and assume that α1 = 0.
The unique eigenvalue (Corollary 2.1) of Mxα j corresponding to V is denoted by λi j and can be
computed using Remark 2.3. By Theorem 2.1,
λi j = ζ α j
i
f0(ζi)
and hence
λi j
λi1
= ζ α j
i ,
i = 1,...,δ, j = 1,...,k.
(2.2)
We would like to recover the coordinates of ζi from the tuple (ζ α2
i ,...,ζ αk
i ) ∈Ck−1. Assuming
ZA0 = Zn and applying (2.1), we ﬁnd
ζi,ℓ=
k
∏
j=2
λi j
λi1
m j,ℓ
,
ℓ= 1,...,n.
(2.3)
Remark 2.4. In many cases, one can take A0 = {0,e1,...,en}, in which case mj,ℓ= 1 if j = ℓ+1
and mj,ℓ= 1 otherwise.
Motivated by this discussion, we make the following deﬁnition.
Deﬁnition 2.1. We say that a tuple (F = ( f1,..., fs),A0,(E0,E) = (E0,...,Es),D) is admissible
if satisﬁes the following three conditions,
• Compatibility condition: For i = 0,...,s, Ai +Ei ⊂D.
• Rank condition: There exists f0 ∈RA0 such that rank(Nf0) = Coker(F,E;D).
• Lattice condition: The set A0 satisﬁes 0 ∈A0 and ZA0 = Zn.
The results in this section lead to Algorithm 2 for solving F(x) = 0, given an admissible tuple
(F,A0,(E0,E),D). We discuss some aspects of the algorithm in more detail.
In practice, the number of columns ∑s
i=1 #Ei of the Macaulay matrix M(F,E;D) is often
much larger than the number #D of rows. Multiplying from the right by a random matrix of size
(∑s
i=1#Ei)×#D does not affect the left nullspace, but reduces the complexity of computing it. This
is what happens in line 2. See [37, Sec. 4.2] for details. If (∑s
i=1 #Ei) ≲#D, that is, the number of
columns is not much larger than the number of rows, this step can be skipped.
In theory, we may pick B arbitrary such that Nf0,B is an invertible matrix. In practice, it is
crucial to pick B such that Nf0,B is well-conditioned. This was shown in [43, 44]. In line 6, we use
a standard numerical linear algebra procedure for selecting a well-conditioned submatrix from Nf0:
QR factorization with optimal column pivoting. This computes matrices Q0,R0 and a permutation
p = (p1,..., p#E0) of the columns of Nf0 such that Nf0[:, p] = Q0R0, where Q0 ∈Cγ×γ is a unitary
matrix, R0 ∈Cγ×#E0 is upper triangular and Nf0[:, p] is Nf0 with its columns permuted according
to p. The leftmost γ columns of R0 form the square, upper triangular matrix ˆR0. The column
permutation p is such that columns p1,..., pγ form a well-conditioned submatrix of Nf0. In line 8,
10

these columns are selected to form the matrix Nf0,B. Using the identities N∗
f0,BM∗
g = N∗
g,B , where
∗is the conjugate transpose, and Nf0,B = Q0 ˆR0, we see that the solution Q∗
0M∗
gQ0 to the linear
system ˆR∗
0X = N∗
g,BQ0 is similar to the matrix M∗
g in this section, and it can be obtained by back
substitution since R∗
0 is lower triangular. Since we extract the coordinates of the roots form the
eigenvalues, not the eigenvectors, we may work with Q∗
0M∗
gQ0 as well. This is exploited in line 11.
In line 17, we invoke Algorithm 1. Lines 18-25 are a straightforward implementation of Remark
2.3. As pointed out, in the case m = 1, λi j can alternatively be computed as a Rayleigh quotient.
Algorithm 2 SOLVE
Input: An admissible tuple (F,A0,(E0,E),D) with A0 = {α1 = 0,α2,...,αk}
Output: A set of solutions of F, containing all solutions in (C\{0})n
1:
O ←random matrix of size (∑s
i=1 #Ei)×#D
2:
MO ←M(F,E;D)·O
3:
Compute Coker(F,E;D) via the SVD of MO
4: f0 ←random element in RA0
5:
Nf0 ←matrix of size γ ×(#E0) given by Coker(F,E;D)·M( f0,E0;D).
6: Q0,R0, p ←apply QR decomposition with optimal pivoting to Nf0
7:
ˆR0 ←square, upper triangular matrix given by the ﬁrst γ columns of R0
8: B ←exponents in E0 corresponding to columns p1,..., pγ of Nf0
9: for j = 1,...,k do
10:
Nxα j,B ←Coker(F,E;D)·M(xα j,B;D)
11:
M∗
xα j ←solve ˆR∗
0X = N∗
xα j,BQ0 for X by back substitution
12: Mg ←random linear combination of Mxα,α ∈A0
13: {(µ1,Vµ1),...,(µδ,Vµδ)} ←distinct eigenvalues of Mg and corresponding left eigenspaces
14: Mh ←a different random linear combination of Mxα,α ∈A0
15: Z ←{}
16: for i = 1,...,δ do
17:
V ←GETEIGENSPACE(µi,Vµi,Mh)
18:
if V ̸= {0} then
19:
m ←number of rows of V
20:
T ←random matrix of size γ ×m
21:
for j = 1,...,k do
22:
˜M ←VMxα jT(VT)−1
23:
λi j ←trace( ˜M)/m
(when m = 1, use Rayleigh quotient, see Remark 2.3)
24:
ζi ←compute the coordinates of ζi via (2.1) and (2.2)
25:
Z ←Z ∪{ζi}
26: return Z
Remark 2.5. Alternatively, by Theorem 2.1 we may look for a vector vζ in the row span of V such
that C ·ζ D = vζ ·Coker(F,E;D) for some nonzero constant C. If 0 ∈D and ZD = Zn, we scale v
such that C = 1 and ﬁnd ζ from ζ D as above. We do not elaborate on this approach here.
It is clear that the size of the matrices in Algorithm 2 depends on the cardinality of the expo-
nent sets in the admissible tuple. Constructing admissible tuples for certain families of polynomial
11

systems is an active ﬁeld of research, strongly related to the study of regularity of ideals in polyno-
mial rings, in the sense of commutative algebra [26, Sec. 20.5]. Recent progress in this area, for the
case where n = s, was made in [11]. In the next section, we will summarize some of these results
by explicitly describing some admissible tuples for systems with important types of structures.
Classically, the matrices Mg considered in this section represent pairwise commuting multipli-
cation operators in the algebra R/I, where I is the ideal generated by the polynomials in F [22,
Ch. 2]. In the very general setting we consider here, assuming only that (F,A0,(E0,E),D) is an
admissible tuple, the matrices Mg do not necessarily represent such multiplication operators. How-
ever, under some extra assumptions, they do commute. In this case, we can simplify Algorithm 2
by computing the simultaneous Schur factorization of (Mxα)α∈A0 as in [11, Sec. 3.3].
Theorem 2.2 (Criterion for commutativity). Let (F,A0,(E0,E),D) be an admissible tuple and
γ = HF(F,E;D). Let f0 be such that Nf0 satisﬁes the Rank condition (Deﬁnition 2.1). If
HF(( f 2
0 ,F),(E0,E1 +A0,...,Es +A0);D+A0)−HF(F,(E1 +A0,...,Es +A0);D+A0) = γ,
we have that for every g1,g2 ∈RA0, Mg1 Mg2 = Mg2 Mg1.
Proof. In what follows, we ﬁx two vector spaces ID := Im(Sylv(F,(E1,...,Es);D)) and ID+A0 :=
Im(Sylv(F,(E1+A0,...,Es+A0);D+A0)). Observe that, for every g ∈RA0 and f ∈ID, g f ∈ID+A0. We
write B = {b1,...,bγ} and given v ∈Cγ, we set v·B := ∑i vi xbi.
In this proof, for each g ∈RA0, we consider the map ˜Mg := N−1
f0,B · Mg · Nf0,B. The maps Mg
and ˜Mg are similar, so it is enough to prove that ˜Mg1 ˜Mg2 = ˜Mg2 ˜Mg1. It is not hard to show that for
v,w ∈Cγ, such that ˜Mg(v) = w ∈Cγ, we have g(v·B) ≡f0 (w·B) modulo ID.
First, observe that, for every v ∈Cγ, g1 g2(v·B) ≡f 2
0 (( ˜Mg1 ˜Mg2 v)·B) modulo ID+A0. Indeed,
g1(v·B) = f0(( ˜Mg1 v)·B)+h1, for h1 ∈ID and for w = ˜Mg1 v, we have that g2(w·B) = f0 (( ˜Mg2 w)·
B)+h2, for h2 ∈ID. Hence, g1g2(v·B) = g2 f0 (( ˜Mg1 v)·B)+g2h1 = f 2
0 (( ˜Mg2 ˜Mg1 v)·B)+ f0h2 +
g2h1. As f0 h2+g2 h1 ∈ID+A0, the claim follows. Since g1g2 = g2g1, it also holds that g1 g2(v·B) ≡
f 2
0 (( ˜Mg2 ˜Mg1 v)·B) modulo ID+A0.
Second, we show that { f 2
0 xbi : bi ∈B} is a basis of the vector space V spanned by { f 2
0 xe :
e ∈E0} modulo ID+A0. By construction of B ⊂E0, { f0 xbi : bi ∈B} is a basis of the vector space
spanned by { f0 xe : e ∈E0} modulo ID, so { f 2
0 xbi : bi ∈B} generates V. Moreover, by the assump-
tion on the difference of coranks, the dimension of the vector space V is γ = #{ f 2
0 xbi : bi ∈B}.
By the ﬁrst observation, we have that f 2
0
    ˜Mg1 ˜Mg2 −˜Mg2 ˜Mg1

v

·B

≡0 modulo ID+A0 for
every v ∈Cγ. By the second observation, the elements in { f 2
0 xbi : bi ∈B} are linearly independent
modulo ID+A0. Therefore, we have that ˜Mg1 ˜Mg2 = ˜Mg2 ˜Mg1.
Remark 2.6. This criterion is similar to Bayer and Stillman’s criterion to compute the Castelnuovo-
Mumford regularity of ideals deﬁning a zero dimensional projective scheme [5, Thm. 1.10]. Under
further assumptions on A0,E0,D, and B, the commutativity of the matrices implies that γ is the
number of isolated solutions of the system F, see [36, Thm. 3.1].
3
Construction of admissible tuples
In this section, we ﬁx an s-tuple of sets of exponents A := (A1,...,As), where Ai ⊂Nn, and
consider a polynomial system F = ( f1,..., fs) ∈RA1 × ··· × RAs. We construct tuples that are
12

admissible under mild assumptions on F (Assumption 1). This allows us to compute the solutions
of the system F using Algorithm 2. Section 3.1 states explicit formulas for admissible tuples that
in practice are near-optimal in the case where s = n. In the overdetermined case (s > n), we can
obtain admissible tuples leading to smaller matrices by using incremental constructions. These are
the topic of Subsection 3.2.
The section uses the following notation. The convex hull of a ﬁnite subset E ⊂Rn is the
polytope Conv(E) ⊂Rn deﬁned as,
Conv(E) :=
(
∑
e∈E
λee : ∑
e∈E
λe = 1,λe ≥0 for all e ∈E
)
.
By a lattice polytope we mean a convex polytope P ⊂Rn that arises as Conv(E), where E ⊂Nn.
Such a lattice polytope is called full-dimensional if it has a positive Euclidean volume in Rn. Given
two polytopes P1,P2 ⊂Rn and c ∈N, we denote by P1 + P2 the Minkowski sum of P1,P2 and by
c·P1 the c-dilation of P1, that is,
P1 +P2 := {α +β : α ∈P1,β ∈P2},
c·P1 := {cα : α ∈P1}.
We denote the Cartesian product of two subsets P1 ⊂Rn1 and P2 ⊂Rn2 by P1 ×P2 := {(α,β) : α ∈
P1,β ∈P2} ⊂Rn1 ×Rn2 = Rn1+n2. Throughout, we use the notation ∆n = Conv({0,e1,...,en}) ⊂
Rn for the standard simplex in Rn.
Example 7. Consider the sets of exponents E1 = {(0,0),(1,0),(1,1),(2,0),(0,1)} and
E2 = {(0,0),(1,0),(0,1)}. In Figure 1, the polytopes P1 := Conv(E1), P2 := Conv(E2), and
P1 +P2 ⊂R2 are displayed. Observe that P2 is the two-dimensional standard simplex ∆2.
+
=
0
1
2
0
1
P1
0
1
0
1
P2
0
1
2
3
0
1
2
P1 +P2
Figure 1: Polytopes from Example 7.
△
3.1
Explicit constructions
We present explicit constructions of admissible tuples for the following types of polynomial sys-
tems, listed in (more or less) increasing order of generality.
1. Dense systems. These are systems for which fi may involve all monomials of degree at
most di, where (d1,...,ds) ∈Ns
>0 is an s-tuple of positive natural numbers. For dense
systems, we have Ai = {α ∈Nn : α1 +···+αn ≤di} = (di ·∆n)∩Nn.
13

2. Unmixed systems. We say that the polynomial system F is unmixed if there is a full-
dimensional lattice polytope P and integers d1,...,ds such that di · P = Conv(Ai). The
codegree of P is the smallest t ∈N>0 such that (t · P) ∩Nn ̸= ∅, i.e. t · P contains a point
with integer coordinates in its interior. Note that dense systems can be viewed as unmixed
systems with P = ∆n.
3. Multi-graded dense systems. A different, natural generalization of the dense case allows
different degrees for different subgroups of the variables x1,...,xn. Let {I1,...,Ir} be a
partition of {1,...,n}, i.e. Ij ⊂{1,...,n}, Ij ∩Ik = ∅and Sr
j=1Ij = {1,...,n}. This way
we obtain subsets xI1,...,xIr ⊂{x1,...,xn} of the variables, indexed by the Ij. In a multi-
graded dense system, fi may contain all monomials of degree at most di, j in the variables
xI j. If the variables are ordered such that the ﬁrst n1 variables are indexed by I1, the next n2
variables by I2 and so on, this means Ai = ((di,1 ·∆n1)×···×(di,r ·∆nr))∩Nn. Necessarily
we have n1 +···+nr = n. A dense system is a multi-graded dense system with r = 1.
4. Multi-unmixed systems. This is a generalization of the unmixed and the multi-graded
dense case, where there are full-dimensional lattice polytopes P1 ⊂Rn1,...,Pr ⊂Rnr such
that 0 ∈Pi and n = ∑i ni and for each i ∈{1,...,s}, an r-tuple (di,1,...,di,r) ∈Nr such that
Conv(Ai) = (di,1 ·P1)×···×(di,r ·Pr). That is, the convex hull of Ai is the product of dila-
tions of the polytopes P1,...,Pr. Note that a multi-graded dense system is a multi-unmixed
system with Pi = ∆ni, and an unmixed system is a multi-unmixed system with r = 1.
5. Mixed systems. This is the most general case, our only assumption on each Ai is that the
lattice polytope ∑s
i=1Conv(Ai) ⊂Rn is full-dimensional.
If the full-dimensionality requirements in the previous list are not fulﬁlled, one can reformulate
the system using fewer variables. For polynomial systems from these nested families, admissible
tuples are presented in Table 1. In what follows, we discuss them in more detail.
The tuples presented in Table 1 are admissible under a zero-dimensionality assumption on the
system F. Unfortunately, it is not enough to require that F(x) = 0 has ﬁnitely many solutions in
Cn or (C\{0})n. Loosely speaking, we need that the lifting of F to a certain larger solution space
has ﬁnitely many solutions. This is best understood in the context of toric geometry. We refer the
reader to [41, Sec. 3] or [11, Sec. 2] for a description of the zero-dimensionality assumption in this
language. Here, we omit terminology from toric geometry and state the assumption in terms of
face systems, following [12]. We will use the notation
F = ( f1,..., fs) ∈RA1 ×···×RAs,
fi = ∑
α∈Ai
ci,αxα.
(3.1)
For any vector v ∈Rn, we deﬁne
Ai,v := {α ∈Ai : ⟨v,α⟩= min
β∈Ai
⟨v,β⟩},
where ⟨v,α⟩= v1α1 +···+vnαn ∈R. This gives a new polynomial system
Fv = ( f1,v,..., fs,v) ∈RA1,v ×···×RAs,v,
fi,v = ∑
α∈Ai,v
ci,αxα,
called the face system associated to v. Note that F is the face system F0 associated to 0 ∈Rn.
14

1. Dense case, Conv(Ai) = di ·∆n
([11, Cor. 4.3])
A0
∆n ∩Nn
(d0 = 1)
Ei
((∑j̸=i dj −n)·∆n)∩Nn
D
((∑j dj −n)·∆n)∩Nn
2. Unmixed case, Conv(Ai) = di ·P
([11, Thm. 4.5])
A0
P∩Nn
(d0 = 1)
Ei
((∑j̸=i dj −CODEGREE(P)+1)·P)∩Nn
D
((∑j dj −CODEGREE(P)+1)·P)∩Nn
3. Multi-graded dense case, Conv(Ai) = (di,1 ·∆n1 ×···×di,r ·∆nr)
([11, Ex. 10])
A0
(∆n1 ×···×∆nr)∩Nn
(d0,k = 1)
Ei
∏k((∑j̸=i dj,k −nk)·∆nk)∩Nn
D
∏k((∑j dj,k −nk)·∆nk)∩Nn
4. Multi-unmixed case, Conv(Ai) = (di,1P1 ×···×di,r Pr)
([11, Ex. 10])
A0
(P1 ×···×Pr)∩Nn
(d0,k = 1)
Ei
∏k((∑j̸=i dj,k −CODEGREE(Pk)+1)·Pk)∩Nn
D
∏k((∑j dj,k −CODEGREE(Pk)+1)·Pk)∩Nn
5. Mixed case, Conv(Ai) = Pi
([11, Thm. 4.4])
A0
∆n ∩Nn
(P0 = ∆n)
Ei
(∑j̸=i Pj)∩Nn
D
(∑j Pj)∩Nn
Table 1: Admissible tuples for ﬁve families of structured polynomial systems. In the table, we assume that all di > 0,
di,j ≥0 and P ⊂Rn,Pi ⊂Rni are full dimensional lattice polytopes.
Assumption 1 (Zero-dimensionality assumption). For every v ∈Rn, the face system Fv(x) = 0 has
ﬁnitely many (possibly zero) solutions in (C\{0})n.
Remark 3.1. Assumption 1 holds for a generic element F ∈RA1 ×···×RAs, in the sense of Sec-
tion 2. In fact, for a generic system F all face systems Fv for v ̸= 0 have no solutions in (C\{0})n,
and the condition for this to hold only depends on the coefﬁcients associated to some vertices of
the polytopes Conv(Ai), see [17]. The fact that we can allow ﬁnitely many solutions for all face
systems comes from the recent contributions [41, 11]. In practice, this means that our algorithm
is robust in the presence of isolated solutions at or near inﬁnity (where this is understood in the
appropriate toric sense).
15

Theorem 3.1. Consider a polynomial system F = ( f1,..., fs) with supports A1,...,As satisfy-
ing Assumption 1.
Consider (A0,(E0,...,Es),D) as deﬁned in Table 1.
Then, we have that
(F,A0,(E0,...,Es),D) is an admissible tuple.
Proof. We sketch the proof. We need to show that the three conditions in Deﬁnition 2.1 are
satisﬁed.
Observe that, by construction, the elements from the tuple satisfy the Compatibil-
ity condition and A0 satisﬁes the Lattice condition.
By Assumption 1, for generic f0 ∈RA0,
the system ( f0,..., fs) has no solutions on the toric variety associated to the lattice polytope
Conv(A1) + ··· + Conv(As) and we can adapt [11, Thm. 4.3] straightforwardly to the case of no
solutions (the Koszul complex of sheaves in that proof is exact by [32, Ch. 2.B, Prop.1.4.a], see
also [35, Thm. 3.C]). Hence, following the same procedure as in [11, Sec. 4], we can show that
HF(( f0,F),(E0,E);D) = 0. Therefore, by Lemma 2.1, the Rank condition holds.
Remark 3.2 (The number γ and the number of solutions). Consider F satisfying Assumption 1.
We ﬁx an admissible tuple (F,A0,(E0,...,Es),D) from Table 1. If n = s, the dimension γ =
HF(F,E;D) is the number of solutions deﬁned by F on the compact toric variety X ⊃(C\{0})n
from [11, Thm. 4.4], counted with multiplicities. For generic F, all solutions have multiplicity 1
and lie in (C \ {0})n,
which means that γ
is the mixed volume of the polytopes
Conv(A1),...,Conv(An) [12, Thm. A]. Additionally, in these cases, we have a complete charac-
terization of the invariant subspaces of Mg, see [11, Sec. 3.2]. It was pointed out to us by Laurent
Bus´e that [18, Lem. 6.2] should imply that the same holds for s > n, see the proof of [15, Prop.3]
for an example of how to prove such a result in the multihomogeneous case.
Macaulay matrices deﬁned by the tuples from Table 1 have been used in different algorithms
for solving sparse polynomial systems, e.g. sparse resultants [29], truncated normal forms [44],
Gr¨obner bases [8, 9], and others [35]. When restricted to Macaulay matrices, these constructions
are often near-optimal when s = n. However, there exist other kind of smaller matrices which
can be also used to solve the system [7, 10]. When s > n, we can often work with much smaller
Macaulay matrices. This is the topic of the next subsection.
3.2
Incremental constructions
Even though the tuples from Theorem 3.1 are admissible, they might lead to the construction
of unnecessarily big matrices in Algorithm 2. To avoid this, we present an incremental approach
which leads to the construction of potentially smaller matrices. For ease of exposition, we consider
only the unmixed case. The ideas can be extended to the other cases.
In what follows, we ﬁx a polytope P such that 0 ∈P and integers d0,...,ds ∈N>0. We consider
sets of exponents A0,A1,...,As ⊂Nn such that, for each i ∈{0,...,s}, we have Conv(Ai) = di ·P.
For each λ ∈N, we deﬁne
Dλ := (λ ·P)∩Nn,
Eλ
i :=

((λ −di)·P)∩Nn
if λ ≥di
∅
otherwise
for i = 0,...,s,
(3.2)
and Eλ = (Eλ
1 ,...,Eλ
s ).
Theorem 3.2. With the above notation, consider an unmixed polynomial system F ∈RA1 ×··· ×
RAs, with Conv(Ai) = di · P, satisfying Assumption 1. For any λ ∈N such that there is f0 ∈RA0
16

satisfying rank(Nf0) = HF(F,Eλ;Dλ), we have that the tuple (F,A0,(Eλ
0 ,Eλ),Dλ) is admissi-
ble. Moreover, for any λ ≥∑i di −CODEGREE(P) + 1 and for generic f0 ∈RA0, we have that
rank(Nf0) = HF(F,Eλ;Dλ).
Proof. By construction, the tuple satisﬁes the Compatibility and Lattice conditions. By assump-
tion, it satisﬁes the Rank condition, so it is admissible. The proof follows as in Theorem 3.1.
Remark 3.3 (The number γ and the number of solutions). In contrast to Remark 3.2, the condition
rank(Nf0) = Coker(F,Eλ;Dλ) does not imply that γ = HF(F,Eλ;Dλ) agrees with the number of
solution of F on some toric compactiﬁcation. In fact, we will see examples in Section 4 where γ is
strictly larger than the number of solutions. For the reader familiar with the Castelnuovo-Mumford
regularity, we note that this happens because the degree Dλ belongs to the regularity of { f0,F},
but not necessarily to the one of F.
Remark 3.4 (Bounds for semi-regular* sequences). When ( f0,F) forms a semi-regular* sequence,
see Appendix A, we can improve the lower bound ∑i di−CODEGREE(P)+1 on λ from Theorem 3.2
in terms of the ﬁrst non-positive coefﬁcients in the expansion of a series (Theorem A.1).
Theorem 3.2 suggests an algorithm for ﬁnding an admissible tuple for an unmixed system F:
we simply check, for a random element f0 ∈RA0 and increasing values of λ, whether rank(Nf0) =
HF(F,Eλ;Dλ) with Nf0 = Coker(F,Eλ;Dλ) · M(F,Eλ
0 ;Dλ).
In order to do this efﬁciently,
instead of computing Coker(F,Eλ+1;Dλ+1) directly as the left nullspace of the large matrix
M(F,Eλ+1;Dλ+1), we will obtain it from the previously computed Coker(F,Eλ;Dλ) and a
smaller Macaulay matrix. This technique was applied in the dense setting (P = ∆n) in [4, 37],
where it is also called ‘degree-by-degree’ approach. See also [38] for a recent complexity analysis.
Note that, by construction, Eλ
i ⊂Eλ+1
i
and Dλ ⊂Dλ+1. The ﬁrst step is to construct the
following 2×2 block matrix
(Coker(F,Eλ;Dλ)×id) :=
Dλ
Dλ+1 \Dλ


Coker(F,Eλ;Dλ)
0
0
id
.
(3.3)
Here id denotes the identity matrix of size #(Dλ+1 \ Dλ). Note that the columns of the matrix
(Coker(F,Eλ;Dλ) ×id) are indexed by Dλ+1, where the ﬁrst block column is indexed by Dλ ⊂
Dλ+1. Next, we set Eλ+1 \Eλ := (Eλ+1
1
\Eλ
1 ,...,Eλ+1
s
\Eλ
s ) and construct the Macaulay matrix
M(F,Eλ+1 \ Eλ;Dλ+1). Here we require that the ordering of the rows is compatible with the
ordering of the columns in (3.3). Let Lλ+1 be a left nullspace matrix of the matrix product
(Coker(F,Eλ;Dλ)×id)·M(F,Eλ+1 \Eλ;Dλ+1).
(3.4)
Then Coker(F,Eλ+1;Dλ+1) = Lλ+1 · (Coker(F,Eλ;Dλ) × id) is a left nullspace matrix for the
Macaulay matrix M(F,Eλ+1;Dλ+1). The power of this approach lies in the fact that (3.4) is much
smaller than M(F,Eλ+1;Dλ+1), which leads to a much cheaper left nullspace computation.
This gives an iterative algorithm for updating the left nullspace matrix Coker(F,Eλ;Dλ). We
start our iteration by considering λ = maxi di, as we want to take into account all of the equations.
This discussion is summarized in Algorithm 3. Note that the algorithm computes the cokernel
Coker(F,E;D) for the admissible tuple as a by-product, as well as the matrix Nf0. This allow us
to skip the steps before line 6 in Algorithm 2.
17

Algorithm 3 GETADMISSIBLETUPLEUNMIXED
Input: An unmixed system F satisfying Assumption 1, the polytope P ∋0, the degrees
(d1,...,ds).
Output: An admissible tuple (F,A0,(E0,E),D), a left nullspace matrix Coker(F,(E0,E);D)
and a corresponding matrix Nf0
1: d0 ←1
2: A0 ←P∩Nn
3: f0 ←a random element of RA0
4: λ ←maxi di
5: Coker(F,Eλ;Dλ) ←left nullspace of M(F,Eλ;Dλ), for the sets of exponents in (3.2)
6: r ←number of rows of Coker(F,Eλ;Dλ)
7: Nf0 = Coker(F,Eλ;Dλ)·M( f0,Eλ
0 ;Dλ)
8: while rank(Nf0) ̸= r do
9:
(Coker(F,Eλ;Dλ)×id) ←the matrix from (3.3)
10:
Lλ+1 ←a left nullspace matrix for the matrix in (3.4)
11:
Coker(F,Eλ+1;Dλ+1) ←Lλ+1 ·(Coker(F,Eλ;Dλ)×id)
12:
λ ←λ +1
13:
r ←number of rows of Coker(F,Eλ;Dλ)
14:
Nf0 = Coker(F,Eλ;Dλ)·M( f0,Eλ
0 ;Dλ)
15: return (F,A0,(Eλ
0 ,Eλ),Dλ), Coker(F,Eλ;Dλ), Nf0
Remark 3.5 (Other incremental constructions). There are alternative incremental constructions
for the matrices Nf0 which also reuse information from previous steps to speed up the computations,
for example, the F5 criterion in the context of Gr¨obner bases [30]. These ideas extend naturally to
the mixed setting, see [9]. However, these approaches based on monomial orderings lead to bad
numerical behaviour. In the context of sparse resultants for mixed systems, Canny and Emiris [27]
proposed an alternative incremental algorithm to construct admissible tuples leading to smaller
Macaulay matrices. Their procedure can be enhanced with the approach followed in this section.
4
Experiments
In this section we illustrate several aspects of the methods presented in this paper via numerical
experiments. We implemented these algorithms in the new Julia package EigenvalueSolver.jl,
which is freely available at https://github.com/simontelen/JuliaEigenvalueSolver. For
all computations involving polytopes, we use Polymake.jl (version 0.5.3), which is a Julia in-
terface to Polymake [33]. We compare our results with the package HomotopyContinuation.jl
(version 2.3.1), which is state-of-the-art software for solving systems of polynomial equations us-
ing homotopy continuation [14]. All computations were run on a 16 GB MacBook Pro with an
Intel Core i7 processor working at 2.6 GHz.
To evaluate the quality of a numerical approximation ζ ∈Cn of a solution for a polynomial
system F given by (3.1). We deﬁne the backward error BWE(ζ) of ζ as
BWE(ζ) = 1
s
s
∑
i=1
fi(ζ)
∑α∈Ai |ci,αζ α|+1.
18

n
number of variables
δ
number of solutions
crt
number of connected components computed by certify
ton
online computation time in seconds
toff
ofﬂine computation time in seconds
BWE
maximum backward error of all computed approximate solutions
BWE
geometric mean of the backward errors of all computed solutions
γ
the number of rows of Coker(F,E;D)
#D
cardinality of D, i.e. the number of columns of Coker(F,E;D)
Table 2: Notation in the experiments in Section 4.
This error can be interpreted as a measure for the relative distance of F to a system F′ for which
F′(ζ) = 0, see [42, App. C].
Additionally, we validate our computed solutions via certiﬁcation. For that, we use the certiﬁ-
cation procedure implemented in the function certify of HomotopyContinuation.jl, which is
based on interval arithmetic, as described in [13]. This function takes as an input a list of approx-
imate solutions to F and tries to compute a list of small boxes in Cn, each of them containing an
approximate input solution and exactly one actual solution to F. The total number of connected
components in the union of these boxes is denoted by crt in what follows. Each of these connected
components contains exactly one solution of F, and one or more approximate input solutions. This
means that crt is a lower bound on the number of solutions to F. If crt equals the number of
solutions, the solutions of F are in one-to-one correspondence with the approximate input solu-
tions. In this case, we say that all solutions are certiﬁed. The function certify assumes that F is
square, i.e. F should have as many equations as variables (s = n). If this is not the case (s > n),
we use certify on a system obtained by taking n random C-linear combinations of f1,..., fs.
The main function of our package EigenvalueSolver.jl is solve EV, which implements
Algorithm 2. It takes as an input an admissible tuple (see Deﬁnition 2.1). This tuple can be
computed using the auxiliary functions provided in our implementation, which are tailored to take
into account the speciﬁc structure of the systems. These functions use the explicit and incremental
constructions from Section 3.
It is common in applications that we have to solve many different generic systems F with the
same supports A1,...,As. In this case, the computation of the admissible tuple can be seen as an
ofﬂine computation that needs to happen only once. We will therefore report both the ofﬂine and
the online computation time. The ofﬂine computation time is the time needed for computing an
admissible tuple and executing solve EV. The online computation re-uses a previously computed
admissible tuple to execute solve EV.
Table 2 summarizes the notation that we use to describe our experiments.
The section is organized as follows. In Section 4.1, we consider square systems (s = n) and
show how to use EigenvalueSolver.jl to solve them. In Section 4.2, we solve overdetermined
systems (s > n) using our incremental algorithm. We perform several experiments summarized
in Table 4 and Table 5. In Section 4.3, we consider systems for which one solutions drifts off to
‘inﬁnity’. In Section 4.4, we compare our algorithm with homotopy continuation methods.
19

4.1
Square systems
In this subsection, we demonstrate some of the functionalities of EigenvalueSolver.jl by solv-
ing square systems, that is s = n, for each of the families in Table 1. The code used for the
examples can be found at https://github.com/simontelen/JuliaEigenvalueSolver in the
Jupyter notebook /example/demo EigenvalueSolver.ipynb. We ﬁx the parameters of Table 1
and consider speciﬁc supports Ai as described below. We construct random polynomial systems by
assigning random real coefﬁcients to each of the monomials, which we draw from a standard nor-
mal distribution. By Remark 3.2, the number γ equals the number of solutions δ for all examples
in this subsection.
For our ﬁrst example, we intersect two degree 20 curves in the plane. That is, we consider
a square, dense system F1 with n = 2 and d1 = d2 = 20. The equations are generated by the
following simple commands:
@polyvar x[1:2]; ds = [20;20];
f = EigenvalueSolver.getRandomSystem_dense(x, ds)
By B´ezout’s theorem, this system has δ = 400 different solutions, which we can compute as follow:
sol = EigenvalueSolver.solve_CI_dense(f, x; DBD = false)
In the previous line, the option DBD = false indicates that we do not want to use the ‘degree-by-
degree’ approach for solving this system, that is, the incremental approach described in Section 3.2.
Experiments show that this strategy is only beneﬁcial for square systems with n ≥3. The letters CI
in the name of the function stand for complete intersection, which indicates that a zero-dimensional
square system is expected as its input. In this example, we have #D = 820 and the computation
took toff = 0.83 seconds. To validate the solutions, we compute their backward errors.
BWEs = EigenvalueSolver.get_residual(f, sol, x)
The maximal value, computed using the command maximum(BWEs), is BWE ≈10−12. The func-
tion certify from HomotopyContinuation.jl certiﬁes crt = 400 distinct solutions. If we per-
form the same computation with parameters n = 3, (d1,d2,d3) = (4,8,12), we obtain δ = γ =
crt = 384, #D = 2300, toff = 3.10, BWE ≈10−11.
@polyvar x[1:3]; ds = [4;8;12];
f = EigenvalueSolver.getRandomSystem_dense(x, ds)
sol = EigenvalueSolver.solve_CI_dense(f, x)
For our next example, we consider an unmixed system F2 with parameters
n = 2,
P = Conv(A),
A = {(0,0),(1,0),(1,1),(0,1),(2,2)},
(d1,d2) = (5,12).
(4.1)
The following code executes this example,
@polyvar x[1:2]; A = [0 0; 1 0; 1 1; 0 1; 2 2]; d = [5;12];
f = EigenvalueSolver.getRandomSystem_unmixed(x, A, d)
sol, A0, E, D = EigenvalueSolver.solve_CI_unmixed(f, x, A, d)
20

In this case, we obtain δ = γ = crt = 240, #D = 685, toff = 0.94, BWE ≈10−11. We remark that
the function solve CI unmixed also returns the admissible tuple (A0,E,D), so that it can be used
to solve another generic unmixed system with the same parameters, without redoing the polyhedral
computations to generate this tuple. This can be done in the following way,
sol = EigenvalueSolver.solve_EV(f, x, A0, E, D; check_criterion = false)
The option check criterion = false in the previous line indicates that the input tuple is admis-
sible, so we do not need to spend time on checking whether the criterion in Lemma 2.1 is satisﬁed.
Using this option, the online computation is faster and takes ton = 0.41 seconds, yet the parameters
δ,γ,crt,BWE are comparable to the ofﬂine case. To illustrate how the unmixed function exploits
the structure of the equations, in Figure 2, we plot the exponents in D for this example, together
with the exponents in D for our dense example F1. In both plots, we have highlighted the expo-
nents in the set B that were selected using QR factorization with optimal column pivoting. These
monomial bases clearly do not correspond to any standard (Gr¨obner) or border basis. Figure 2
should be compared to, for instance, Figure 2 in [44].
0
10
20
30
40
0
10
20
30
40
0
10
20
30
40
0
10
20
30
40
Figure 2: Exponents in D, constructed as in Table 1, for the dense systems F1 (left) and the unmixed system F2
(right). The exponents are shown as lattice points. Dark coloured dots correspond to the set B ⊂D chosen by the QR
factorization in line 6 in Algorithm 2.
We can solve multi-graded dense and multi-unmixed systems using the implemented functions
solve CI multi dense and solve CI multi unmixed, respectively. Table 3 summarizes our
choice of parameters and the results of our experiments for these systems.
family
parameters
results
multi-graded, dense
n1 = n2 = 2,(d j,k) =




1
6
2
1
3
2
4
1



,
δ = γ = crt = 219, #D = 3025,
toff = 15.46, BWE ≈10−11
multi-unmixed
n1 = n2 = 2,(d j,k) =




1
1
1
1
1
1
1
1



,P1 = Conv(A)
P2 = 2 ·∆2
δ = γ = crt = 96, #D = 2745,
toff = 12.84, ton = 11.92, BWE ≈10−9
Table 3: Computational data and results for multi-graded and multi-unmixed examples. Here A is as in (4.1).
21

To conclude this subsection, we present a classical example of a square mixed system in n = 3
variables coming from molecular biology [28, Sec. 3.3]. The following code generate and solve
these equations:
@polyvar t[1:3]
b = [-13 -1 -1 24 -1; -13 -1 -1 24 -1; -13 -1 -1 24 -1]
mons1 = [1 t[2]^2 t[3]^2 t[2]*t[3] t[2]^2*t[3]^2]
mons2 = [1 t[3]^2 t[1]^2 t[3]*t[1] t[3]^2*t[1]^2]
mons3 = [1 t[1]^2 t[2]^2 t[1]*t[2] t[1]^2*t[2]^2]
f = [b[1,:]’*mons1’;b[2,:]’*mons2’;b[3,:]’*mons3’][:]
sol, A0, E, D = EigenvalueSolver.solve_CI_mixed(f,t)
In this case, we obtain δ = γ = crt = 16, #D = 200, toff = 0.53, ton = 0.02, BWE ≈10−13. The
function certify tells us that all 16 solutions are real, conﬁrming the observation made in [28].
4.2
Overdetermined systems
We now consider examples of overdetermined systems, by which we mean cases where s > n. We
will limit ourselves to unmixed systems and use Algorithm 3 to ﬁnd admissible tuples leading to
small Macaulay matrices. These systems arise, for instance, in tensor decomposition problems
[45]. We present examples where γ is signiﬁcantly larger than δ and show that, nevertheless, our
algorithms successfully extract δ < γ relevant eigenvalues and consistently return all solutions of
the input systems.
The overdetermined systems consider in this section are constructed as follows. For a ﬁxed
number of variables n, number of solutions δ and set of exponents A, we generate δ random
points ζ1,...,ζδ in Cn by drawing their coordinates from a complex standard normal distribution.
We construct a Vandermonde type matrix Vdm whose rows consist of the vectors ζ A
i /∥ζ A
i ∥2,i =
1,...,δ. The nullspace of Vdm is computed using SVD and its columns represent s = #A −δ
polynomials f1,..., fs with support A. If we do not pick too many points, we have that s > n and
the solutions of F = ( f1,..., fs) are exactly the points ζ1,...,ζδ.
4.2.1
Dense, overdetermined systems
In this subsection, we consider dense overdetermined systems, i.e. A = (d · ∆n) ∩Nn for some
degree d ∈N>0. The ofﬂine computation uses Algorithm 3 to ﬁnd an admissible tuple, as well
as a left nullspace, and then execute Algorithm 2 from line 6 on. The online computation uses
this admissible tuple to execute Algorithm 2 directly. This means that the ofﬂine version uses an
incremental strategy for computing the left nullspace, while the online version works directly with
the large Macaulay matrix. The online version can be adapted to work incrementally as well. We
have chosen not to do this in order to illustrate that, depending on n,s, the incremental approach
may be less or more efﬁcient than the direct approach. In cases where the incremental approach is
more efﬁcient, this may cause toff < ton. In the square case (s = n), this happens for n ≥3 [37, 38],
but our results show that in the overdetermined case this might not happen. Further research is
necessary to make an automated choice. Table 4 gives an overview of the computational results,
which we will now discuss in more detail.
22

The ﬁrst 10 rows in Table 4 correspond to systems of 6 equations in 3 variables of increasing
degree d = 2,4,...,20. Note that γ > δ for d > 4. In all cases, δ distinct solutions were computed
using our algorithms and crt = δ. This means that exactly δ out of γ eigenvalues were selected
and correctly processed to compute solution coordinates. The maximum backward error grows
faster with the degree of the equations than for square systems [44]. This can be remedied by
using larger admissible tuples to bring γ closer to δ, at the cost of computing cokernels of larger
matrices. However, our experiment shows that can we ﬁnd certiﬁed approximations for all 1765
intersection points of 6 threefolds of degree 20 within less than 10 minutes. All of these are within
two Newton reﬁnement steps from having a backward error of machine precision.
The next 5 rows of Table 4 contain results for 18 dense equations in 6 variables of increasing
degree d = 2,3,...,6. Note that ton > toff for d > 2. This is due to the incremental approach for
the ofﬂine phase, as mentioned above.
In the following 7 rows of Table 4, we illustrate the effect of increasing the number of variables
when we ﬁx the degree d = 3. We work with overdetermined systems for which s = 2n. Although
the complexity of eigenvalue methods usually scales badly with the number of variables, these
results show that when the system is ‘sufﬁciently overdetermined’, our algorithms can ﬁnd feasible
admissible tuples to solve cubic equations in 8 variables in no more than 20 seconds.
Finally, the last rows of Table 4 correspond to systems of cubic equations in 15 variables with
an increasing number δ = 200,300,...,600 of solutions. Note that the computation time decreases
with the number of solutions. The reason is that for all these values of δ, we can work with the
same support D for the Macaulay matrix. This means that the matrix has the same number of rows
for each system. The number of columns, however, depends on the number of equations, which
increases with decreasing δ by construction. For δ = 700, we need a larger set of exponents D,
causing memory issues.
4.2.2
Unmixed, overdetermined systems
We now use our algorithms to solve overdetermined unmixed systems. The results are summarized
in Table 5. First, we set n = 3 and choose δ such that s = 6. We deﬁne A0 as the columns of


2
2
0
1
1
0
0
0
1
1
0
0
1
0
1
0
0
2
1
0
2
1
2
0

.
The support A is obtained as A = (d ·Conv(A0))∩N3 for increasing values of d. The conclusions
are similar to those for the n = 3 experiments in the previous subsection. Note that d = 8 is the
only reported case for which one solution could not be certiﬁed.
Next, we set n = 15,δ = 100 and we deﬁne A0 = {0,e1,e2,...,e13,e13 +e14,e14 +e15}, where
ei is the i-th standard basis vector of Z15. We set A = (2·Conv(A0))∩N15. There are 136 exponents
in A, of degree at most 4.
Remark 4.1 (Noisy coefﬁcients). As an important direction for future research, we note that our
eigenvalue algorithms can be used to compute ‘solutions’ to overdetermined systems with noisy
coefﬁcients. For instance, the noise level needs to be taken into account when setting the relative
tolerance for computing the left nullspace in line 3 of Algorithm 2. This is expected to work
especially well for strongly overdetermined problems with only a few solutions.
23

n
s
d
δ
crt
γ
#D
BWE
BWE
toff
ton
3
6
2
4
4
4
10
5.75e-16
3.20e-16
1.25e-03
1.34e-03
3
6
4
29
29
29
84
1.70e-14
2.54e-15
9.41e-03
6.33e-03
3
6
6
78
78
100
220
7.07e-12
2.23e-14
7.00e-02
5.23e-02
3
6
8
159
159
224
560
1.21e-12
4.67e-14
4.47e-01
2.90e-01
3
6
10
280
280
465
969
6.32e-10
6.63e-13
1.99e+00
1.32e+00
3
6
12
449
449
820
1540
5.09e-09
7.90e-12
8.76e+00
6.04e+00
3
6
14
674
674
1280
2600
1.51e-08
7.78e-12
3.88e+01
2.21e+01
3
6
16
963
963
1938
3654
3.57e-07
3.98e-11
1.26e+02
7.36e+01
3
6
18
1324
1324
2776
4960
1.83e-06
5.77e-10
3.54e+02
2.08e+02
3
6
20
1765
1765
3780
7140
1.11e-05
9.96e-10
9.85e+02
5.38e+02
6
18
2
10
10
10
84
1.53e-14
2.96e-15
1.45e-02
8.82e-03
6
18
3
66
66
66
462
4.51e-14
5.59e-15
1.69e-01
1.74e-01
6
18
4
192
192
204
1716
2.95e-12
6.36e-14
2.11e+00
3.79e+00
6
18
5
444
444
1225
5005
7.52e-12
1.76e-13
5.18e+01
7.86e+01
6
18
6
906
906
4060
12376
5.28e-10
2.37e-12
1.01e+03
1.33e+03
2
4
3
6
6
6
10
3.02e-15
1.12e-15
1.15e-03
1.23e-03
3
6
3
14
14
14
35
5.95e-15
1.49e-15
2.92e-03
2.35e-03
4
8
3
27
27
27
126
3.85e-14
2.27e-15
1.56e-02
1.27e-02
5
10
3
46
46
46
252
6.59e-14
8.89e-15
4.19e-02
2.04e-01
6
12
3
72
72
126
462
3.70e-12
1.46e-13
1.61e-01
1.51e-01
7
14
3
106
106
127
1716
6.20e-12
3.96e-14
2.29e+00
4.26e+00
8
16
3
149
149
483
3003
8.31e-12
1.05e-13
1.16e+01
1.92e+01
15
616
3
200
200
200
3876
1.45e-13
1.04e-14
9.78e+01
5.80e+01
15
516
3
300
300
300
3876
3.66e-13
9.37e-15
8.25e+01
5.64e+01
15
416
3
400
400
400
3876
5.46e-13
1.44e-14
8.50e+01
5.42e+01
15
316
3
500
500
500
3876
4.25e-13
1.26e-14
6.38e+01
5.81e+01
15
216
3
600
600
600
3876
4.86e-13
1.41e-14
4.91e+01
4.65e+01
Table 4: Computational results for overdetermined, dense systems. See Table 2 for the notation.
n
s
d
δ
crt
γ
#D
BWE
BWE
toff
ton
3
6
1
3
3
3
33
8.91e-16
5.36e-16
1.25e+00
7.59e-01
3
6
2
27
27
27
165
2.71e-13
1.99e-14
1.96e+00
2.30e-02
3
6
3
76
76
93
291
3.94e-12
8.52e-14
2.07e+00
9.36e-02
3
6
4
159
159
216
708
8.53e-11
6.62e-13
3.27e+00
5.06e-01
3
6
5
285
285
415
1405
1.99e-08
6.42e-12
6.25e+00
2.78e+00
3
6
6
463
463
891
1881
2.00e-06
6.15e-11
1.56e+01
1.06e+01
3
6
7
702
702
1387
3133
4.56e-05
7.06e-10
5.66e+01
4.51e+01
3
6
8
1011
1010
2031
4845
9.29e-05
3.78e-10
1.86e+02
1.61e+02
15
36
2
100
100
100
3876
2.88e-13
7.42e-15
8.07e+01
4.59e+01
Table 5: Computational results for overdetermined, unmixed systems. See Table 2 for the notation.
4.3
Solutions at inﬁnity
An important feature of our algorithms is that they can deal with systems having isolated solu-
tions at or near inﬁnity. To illustrate this, we work with the same set-up as in Section 4.2.1 with
parameters n = 7,d = 3 and s = 14, implying δ = 106. We generate 106 random complex points
ζ1,...,ζ106 as before, and then multiply the coordinates of ζ106 by a factor 10e for increasing values
of e. That is, we let one of 106 solutions drift off to inﬁnity. Figure 3 shows the maximal 2-norm
of the computed solutions as well as the maximal backward error BWE for e = 0,...,14. The re-
24

sults clearly show that the accuracy is not affected by the ‘outlier’ solution. As e grows larger, the
solution ζ106 corresponds to an isolated solution of the face system Fv with v = (1,1,1,1,1,1,1),
see Remark 3.1. For all considered values of e, our algorithm computed crt = δ = 106 distinct
certiﬁed approximate solutions.
0
2
4
6
8
10
12
14
10−14
10−5
104
1013
e
Figure 3: Maximum backward error BWE (
) and norm of the largest computed solution (
) for the experiments
in Section 4.3.
4.4
Comparison with homotopy continuation methods
Homotopy continuation algorithms form another important class of numerical methods for solving
polynomial systems [39]. These methods transform a start system with known solutions continu-
ously into the target system, which is the system we want to solve, and track the solutions along
the way. This process can usually only be set up for square systems, i.e. s = n. In these cases,
especially when n = s is large (≥4), homotopy continuation methods often outperform eigenvalue
methods. When the system F is overdetermined (s > n), homotopy methods solve a square system
Fsquare obtained by taking n random C-linear combinations of the s input polynomials. The set
of solutions of F is contained in the set of solutions of Fsquare, so that the solutions of F can be
extracted by an additional ‘ﬁltering’ step. Often Fsquare has many more solutions than F, so that
many of the tracked paths do not end at a solution of F. Below, we use the notation δsquare for the
number of solutions of Fsquare.
Several implementations of homotopy methods exist, including Bertini [3] and PHCpack [47].
Here, we choose to compare our computational results with the relatively recent Julia impementa-
tion HomotopyContinuation.jl [14]. The motivation is twofold: it is implemented in the same
programming language as EigenvalueSolver.jl, and it is considered the state of the art for the
functionalities we are interested in.
We point out that due to the extremely efﬁcient implementation of numerical path tracking
in HomotopyContinuation.jl, the package can outperform our eigenvalue solver even when
δsquare is signiﬁcantly larger than δ. For instance, in the case n = 3,d = 20 from Table 4, we have
δsquare = 8000 > δ = 1765, but HomotopyContinuation.jl tracks all these 8000 paths in no
more than 40 seconds. The performance is comparable for the row n = 6,d = 5 in Table 4, where
HomotopyContinuation.jl tracks δsquare = 15625 paths in about 45 seconds. For all the above
computations, we used the option start system = :total degree, which is optimal for dense
systems and avoids polyhedral computations to generate start systems.
25

However, for strongly overdetermined system, our algorithm outperforms the homotopy ap-
proach. For example, for all the cases n = 15,d = 3, Table 4 shows that our algorithms take no
more than 2 minutes for δ ≤600. On the other hand, the number δsquare equals 315 = 14348907,
for which HomotopyContinuation.jl shows an estimated duration of more than 2 days. Addi-
tionally, for the case n = 15,d = 2 in Table 5, we have δsquare = 32765 and the path tracking takes
over 10 minutes, as compared to 48 seconds for the online version of our algorithm and 65 seconds
for the ofﬂine version. In this last case we use the default start system = :polyhedral.
We conclude that for strongly overdetermined systems (s ≫n), EigenvalueSolver.jl out-
performs HomotopyContinuation.jl, which suggests that eigenvalue methods are more suitable
to deal with this kind of systems.
Acknowledgments
Part of this work was done during the visit of the second author to TU Berlin for the
occasion of the MATH+ Thematic Einstein Semester on Algebraic Geometry, Varieties, Polyhedra, Computation. We
thank the organizers of this nice semester for making this collaboration possible.
The ﬁrst author was funded by the ERC under the European’s Horizon 2020 research and innovation programme
(grant agreement No 787840).
References
[1] M. Bardet, J.-C. Faug`ere, B. Salvy, and B.-Y. Yang. Asymptotic behaviour of the degree of regularity of semi-
regular polynomial systems. In Proceedings of MEGA, volume 5, 2005.
[2] M. Bardet, J.-C. Faug`ere, B. Salvy, and P.-J. Spaenlehauer. On the complexity of solving quadratic Boolean
systems. Journal of Complexity, 29(1):53–75, Feb. 2013.
[3] D. J. Bates, A. J. Sommese, J. D. Hauenstein, and C. W. Wampler. Numerically solving polynomial systems with
Bertini. SIAM, 2013.
[4] K. Batselier, P. Dreesen, and B. De Moor. A fast recursive orthogonalization scheme for the Macaulay matrix.
Journal of Computational and Applied Mathematics, 267:20–32, 2014.
[5] D. Bayer and M. Stillman. A criterion for detecting m-regularity. Inventiones mathematicae, 87(1):1–11, 1987.
[6] M. R. Bender. Algorithms for sparse polynomial systems: Groebner basis and resultants. PhD thesis, Sorbonne
Universit´e, June 2019.
[7] M. R. Bender, J.-C. Faug`ere, A. Mantzaﬂaris, and E. Tsigaridas. Koszul-type determinantal formulas for families
of mixed multilinear systems. SIAM Journal on Applied Algebra and Geometry, in press.
[8] M. R. Bender, J.-C. Faug`ere, and E. Tsigaridas. Towards mixed gr¨obner basis algorithms: The multihomoge-
neous and sparse case. In Proceedings of the 2018 ACM International Symposium on Symbolic and Algebraic
Computation, ISSAC ’18, pages 71–78. ACM, 2018.
[9] M. R. Bender, J.-C. Faug`ere, and E. Tsigaridas. Gr¨obner basis over semigroup algebras: Algorithms and ap-
plications for sparse polynomial systems. Proceedings of the 44th International Symposium on Symbolic and
Algebraic Computation, 2019.
[10] M. R. Bender, J.-C. Faug`ere, A. Mantzaﬂaris, and E. Tsigaridas. Bilinear Systems with Two Supports: Koszul
Resultant Matrices, Eigenvalues, and Eigenvectors. In Proceedings of the 2018 ACM International Symposium
on Symbolic and Algebraic Computation, ISSAC ’18, pages 63–70, New York, NY, USA, 2018. ACM.
[11] M. R. Bender and S. Telen.
Toric Eigenvalue Methods for Solving Sparse Polynomial Systems.
arXiv:2006.10654v2 [cs, math], June 2020. preprint, arXiv: 2006.10654v2.
[12] D. N. Bernshtein. The number of roots of a system of equations. Functional Analysis and Its Applications,
9(3):183–185, July 1975.
[13] P. Breiding, K. Rose, and S. Timme. Certifying zeros of polynomial systems using interval arithmetic. arXiv
preprint arXiv:2011.05000, 2020.
[14] P. Breiding and S. Timme. Homotopycontinuation. jl: A package for homotopy continuation in Julia. In Inter-
national Congress on Mathematical Software, pages 458–465. Springer, 2018.
[15] L. Bus´e, M. Chardin, and N. Nemati.
Multigraded Sylvester forms, Duality and Elimination Matrices.
arXiv:2104.08941 [cs, math], Apr. 2021. arXiv: 2104.08941.
26

[16] P. B¨urgisser and F. Cucker. Condition: The Geometry of Numerical Algorithms. Grundlehren der mathematis-
chen Wissenschaften. Springer-Verlag, Berlin, 2013.
[17] J. Canny and J. M. Rojas. An Optimal Condition for Determining the Exact Number of Roots of a Polynomial
System. In Proceedings of the 1991 International Symposium on Symbolic and Algebraic Computation, ISSAC
’91, pages 96–102, New York, NY, USA, 1991. ACM.
[18] M. Chardin. Powers of ideals and the cohomology of stalks and ﬁbers of morphisms. Algebra & Number Theory,
7(1):1–18, Jan. 2013. Publisher: MSP.
[19] R. M. Corless, P. M. Gianni, and B. M. Trager. A reordered schur factorization method for zero-dimensional
polynomial systems with multiple roots. In Proceedings of the 1997 international symposium on Symbolic and
algebraic computation, pages 133–140, 1997.
[20] D. A. Cox. Applications of Polynomial Systems. CBMS Regional Conference Series in Mathematics. Conference
Board of the Mathematical Sciences, 2020.
[21] D. A. Cox. Stickelberger and the eigenvalue theorem. arXiv preprint arXiv:2007.12573, 2020.
[22] D. A. Cox, J. Little, and D. O’Shea. Using Algebraic Geometry. Graduate Texts in Mathematics. Springer-
Verlag, New York, 2 edition, 2005.
[23] D. A. Cox, J. Little, and D. O’Shea. Ideals, varieties, and algorithms: an introduction to computational alge-
braic geometry and commutative algebra. Springer Science & Business Media, 2013.
[24] D. A. Cox, J. Little, and H. K. Schenck. Toric Varieties. American Mathematical Soc., 2011.
[25] P. Dreesen, K. Batselier, and B. De Moor. Back to the roots: Polynomial system solving, linear algebra, systems
theory. IFAC Proceedings Volumes, 45(16):1203–1208, 2012.
[26] D. Eisenbud. Commutative Algebra: with a View Toward Algebraic Geometry. Graduate Texts in Mathematics.
Springer-Verlag, New York, 2004.
[27] I. Z. Emiris and J. F. Canny. Efﬁcient Incremental Algorithms for the Sparse Resultant and the Mixed Volume.
Journal of Symbolic Computation, 20(2):117–149, Aug. 1995.
[28] I. Z. Emiris and B. Mourrain. Computer algebra methods for studying and computing molecular conformations.
Algorithmica, 25(2):372–402, 1999.
[29] I. Z. Emiris and B. Mourrain. Matrices in elimination theory. Journal of Symbolic Computation, 28(1-2):3–44,
1999.
[30] J.-C. Faug`ere, P.-J. Spaenlehauer, and J. Svartz. Sparse gr¨obner bases: the unmixed case. In Proceedings of the
39th International Symposium on Symbolic and Algebraic Computation, pages 178–185, 2014.
[31] R. Fr¨oberg. An inequality for hilbert series of graded algebras. Mathematica Scandinavica, 56(2):117–144,
1985. Publisher: Mathematica Scandinavica.
[32] I. M. Gelfand, M. M. Kapranov, and A. V. Zelevinsky. Discriminants, Resultants, and Multidimensional Deter-
minants. Birkh¨auser Boston, Boston, MA, 1994.
[33] M. Kaluba, B. Lorenz, and S. Timme. Polymake. jl: A new interface to polymake. In International Congress on
Mathematical Software, pages 377–385. Springer, 2020.
[34] A. Kondratyev, H. J. Stetter, and F. Winkler.
Numerical computation of Gr¨obner bases.
Proceedings of
CASC2004 (Computer Algebra in Scientiﬁc Computing), pages 295–306, 2004.
[35] C. Massri. Solving a sparse system using linear algebra. Journal of Symbolic Computation, 73:157–174, 2016.
[36] B. Mourrain. A New Criterion for Normal Form Algorithms. In G. Goos, J. Hartmanis, J. van Leeuwen,
M. Fossorier, H. Imai, S. Lin, and A. Poli, editors, Applied Algebra, Algebraic Algorithms and Error-Correcting
Codes, volume 1719, pages 430–442. Springer Berlin Heidelberg, Berlin, Heidelberg, 1999.
[37] B. Mourrain, S. Telen, and M. Van Barel. Truncated normal forms for solving polynomial systems: Generalized
and efﬁcient algorithms. Journal of Symbolic Computation, 2019.
[38] S. Parkinson, H. Ringer, K. Wall, E. Parkinson, L. Erekson, D. Christensen, and T. J. Jarvis. Analysis of normal-
form algorithms for solving systems of polynomial equations. arXiv preprint arXiv:2104.03526, 2021.
[39] A. Sommese and C. Wampler. The numerical solution of systems of polynomials arising in engineering and
science. World Scientiﬁc, Jan. 2005.
[40] H. J. Stetter. Numerical polynomial algebra, volume 85. Siam, 2004.
[41] S. Telen. Numerical root ﬁnding via Cox rings. Journal of Pure and Applied Algebra, 224(9), 2020.
[42] S. Telen. Solving Systems of Polynomial Equations. PhD thesis, KU Leuven, Leuven, Belgium, 2020. Retrieved
from Lirias.
[43] S. Telen, B. Mourrain, and M. Van Barel. Solving polynomial systems via truncated normal forms. SIAM
Journal on Matrix Analysis and Applications, 39(3):1421–1447, 2018.
27

[44] S. Telen and M. Van Barel. A stabilized normal form algorithm for generic systems of polynomial equations.
Journal of Computational and Applied Mathematics, 342:119–132, 2018.
[45] S. Telen and N. Vannieuwenhoven.
Normal forms for tensor rank decomposition.
arXiv preprint
arXiv:2103.07411, 2021.
[46] J. Vanderstukken, A. Stegeman, and L. De Lathauwer. Systems of polynomial equations, higher-order tensor
decompositions and multidimensional harmonic retrieval: a unifying framework–part i: The canonical polyadic
decomposition. Technical Report 17-133, KU Leuven — ESAT/STADIUS, 2017.
[47] J. Verschelde. Algorithm 795: PHCpack: A general-purpose solver for polynomial systems by homotopy con-
tinuation. ACM Transactions on Mathematical Software (TOMS), 25(2):251–276, 1999.
A
Unmixed systems and semi-regular* sequences
In this section, we focus on overdetermined unmixed systems to provide even better bounds than
the ones in Theorem 3.1. We follow the same notation as in Section 3.2.
We deﬁne the Ehrhart series of a polytope P as the series
ESP(t) = ∑
λ≥0
#((λ ·P)∩Zn) tλ.
Moreover, given a polynomial system F0 := ( f0,..., fs) ∈RA0 ×... ×RAs, we deﬁne the Hilbert
series of F0 as
HSF0(t) := ∑
λ≥0
HF(F0,(Eλ
0 ,...,Eλ
s );Dλ) tλ.
Deﬁnition A.1 (Semi-regularity*). We say that F0 is a semi-regular* sequence if
HSF0 =
"
ESP(t)
s
∏
i=0
(1−tdi)
#
+
,
where [ · ]+ means that we truncate the series in its ﬁrst negative coefﬁcient.
Observe that we write semi-regular* sequence with an asterisk as the deﬁnition of semi-regular
sequence asks for this condition on the Hilbert series to hold for every subsystem ( f0,..., fi), i ≤s.
However, semi-regular sequences are too restrictive for our purposes.
Even in the standard case, that is, whenever P is a standard simplex, semi-regular* sequences
are not understood as well as regular sequences. For example, Fr¨oberg’s conjecture states that
being a semi-regular* sequence is a generic condition [31]. This conjecture, supported by a lot of
empirical evidence, was extended to the unmixed case [30].
Theorem A.1. Given a semi-regular* sequence ( f0,..., fs) ∈RA0 ......RAs, let λ be the degree of
the smallest monomial of ESP(t) ∏s
i=0(1 −tdi) whose coefﬁcient is non-positive. Then, we have
that the tuple (( f1,..., fs),A0,(Eλ
0 ,...,Eλ
s );Dλ) is admissible.
Proof. The proof follows from the fact that HF(( f0,..., fs),(Eλ
0 ,...,Eλ
s );Dλ) = 0 as the sequence
is semi-regular*.
Semi-regular* sequences give us an inexpensive heuristic to discover values for λ for which
we can obtain admissible tuples. It was observed in practice [1, 30] that for many systems F not
having much solutions outside the torus (see Remark 3.1), they can be extended to semi-regular*
sequences. Moreover, there are asymptotic estimations on the expected value of λ [1], as these
sequences have a central role in the hardness estimation of cryptography systems [2].
28

