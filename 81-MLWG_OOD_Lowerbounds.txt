Information-theoretic limitations on
novel task generalization
James Lucas
Mengye Ren
Richard Zemel
University of Toronto
Vector Institute
Abstract
Machine learning models have traditionally been developed under the assumption
that the training and test distributions match exactly. However, recent success in
few-shot learning and related problems are encouraging signs that these models
can be adapted to more realistic settings where train and test differ. Unfortunately,
there is severely limited theoretical support for these algorithms and little is known
about the difﬁculty of these problems. In this work, we provide novel information-
theoretic lower-bounds on minimax rates of convergence for algorithms which are
trained on data from multiple sources and tested on novel data. Our bounds depend
intuitively on the information shared between sources of data and characterizes the
difﬁculty of learning in this setting for arbitrary algorithms.
1
Introduction
Many practical machine learning applications deal with distributional shift from training to testing.
One example is few-shot learning [16, 19], where new tasks need to be learned at test time with
only a few examples. Recently, few-shot learning has seen increased success; however, many
theoretical properties of this problem remain poorly understood. While some progress has been made
on understanding the generalization performance of gradient-based meta-learners [9, 13, 6], these
approaches do not apply to metric-based methods for few-shot learning [10, 17] which typically
require specialized analysis even for i.i.d data [11].
In this paper, we provide a general framework to study generalization properties of algorithms
which are trained on data from multiple sources and are expected to generalize to novel tasks. Our
framework captures both metric learning and adaptive methods (e.g. MAML [8]). While existing
work has studied generalization upper-bounds for novel data distributions [4, 1], to our knowledge,
the inherent difﬁculty of these tasks relative to the i.i.d case has not been characterized. In this work,
we extend tools from information theory [12, 20, 14] to present hardness results in this setting.
2
Background
Notation:
We consider algorithms which learn from data lying in the space Z = X × Y. In the
typical i.i.d setting, the algorithm is provided training data S = {z1, . . . , zm}, where each zi is
drawn independently from the same distribution P.
Multi-task setting:
In order to extend beyond the i.i.d assumption, we will consider the general
framework proposed in Baxter [2]. In this framework, the space Z is endowed with a task environment,
P and an accompanying task distribution Q. To draw training data, tasks P1, . . . , PM are ﬁrst drawn
from Q and data is then drawn according to these tasks. In this work, we consider each datapoint as
drawn from a mixture, zi ∼PM
i=j wjPj, where wj are mixture weights summing to 1 (we use 1/M
throughout for simplicity). We denote this set SP = {z1, . . . , zn}. We also consider a small sample
of novel data, S′ = {z′
1, . . . , z′
k}, with z′
i ∼P ′ ∈P.
In Baxter [2], the learner is tasked with ﬁnding a hypothesis space which provides a good inductive
bias for the full task distribution after only seeing this ﬁnite data sample. In the setting we consider,
NeurIPS 2019 Workshop on Machine Learning with Guarantees, Vancouver, Canada.

we wish to ﬁnd an algorithm that can reliably produce decisions which generalize to novel tasks
(while maintaining good performance on training tasks).
3
Minimax environment risk
Most existing theoretical work studying out-of-distribution generalization focuses on providing
upper-bounds on generalization performance [4, 15, 1]. In this work we explore the converse instead:
what is the best performance we can hope to achieve on any given task in the environment? We will
use tools from information theory to derive these lower bounds.
Consider a symmetric loss function φ(a, b) = ψ(ρ(a, b)) for non-decreasing ψ and arbitrary metric ρ.
We seek to estimate the output of θ : P →Ω, a functional that maps distributions to a metric space Ω.
We deﬁne the i.i.d minimax risk,
R∗= inf
ˆθ
sup
P ∈P
ES∼P n
h
ES′∼P k[φ(ˆθ(S)(S′), θ(P))]
i
.
(1)
For notational convenience, we denote the function output of θ(P) by θP (or simply θi = θ(Pi)).
Similarly, ˆθ : Zn →F maps a sample dataset to a function f ∈F : Zk →Ωthat can be thought of
as a learning procedure itself. Similar to θ, we write ˆθS = ˆθ(S). This quantity is almost identical to
the one presented by Loh [14], but considers ˆθ that produce functions which enables analysis when
we want to generalize to novel tasks with a little available data.
To extend this to the environment setting, we consider a set of tasks P1, . . . , PM+1 ∈P. We deﬁne
the minimax environment risk as follows,
R∗
P = inf
ˆθ
sup
P1,...,PM+1∈P
ESP∼¯
P n
1:M
h
ES′∼P k
M+1[φ(ˆθSP(S′), θ(PM+1))]
i
,
(2)
The estimator now depends on data drawn from ¯P1:M =
1
M
PM
i=1 Pi and we evaluate performance
on the ﬁnal task, PM+1, where k ≪n points samples from PM+1 are available for learning. This
quantity addresses the domain shift expected at test-time in the few-shot learning setting and allows
the estimator to utilize data from multiple tasks.
The goal of ˆθ in this setting is to learn an inductive bias from SP such that useful inferences can
be made with only k data points from the novel distribution, PM+1. In this setting, k plays the role
of the number of shots in the few-shot learning setup where it is typically assumed that some small
amount of labelled data is available for the novel task.
We will ﬁrst prove lower-bounds on the minimax risk for the i.i.d setting and then extend these results
to the environment minimax risk.
3.1
Examples
Information theory allows us to be abstract in terms of the estimation problem and form of estimators
used. This is valuable but it can be difﬁcult to ground these ideas in a practical example. Thus, we
present two synthetic examples for illustrative purposes; one based on metric learning (Figure 1) and
the other on adaptively learning regression (Figure 2) under an environment of distributions.
In Figure 1 we aim to learn an embedding function which is close to some true task representation.
For example, the embedding layer of a prototypical network [17] trained in an environment where
a good metric space deﬁned by θ exists. The learned embedding function ˆθS is able to embed the
novel points close to their true representation. In Figure 2, instead of learning an embedding for the
data points we aim to output an algorithm which recovers the parameters of a regression model from
limited data. If the true labelling functions in the environment are related, for example if they are
all polynomials of the same order with the same noise variance in their labels, we could imagine
learning a useful prior from the training distributions.
4
Information theoretic lower bounds on novel generalization
We begin by introducing some useful Lemmas.
2

(P0)
(P1)
(P2)
(P3)
S(x) : x
P0
S(x) : x
P1
S(x) : x
P2
S(x) : x
P3 (novel)
Metric learning novel task generalization
Figure 1: Metric learning (synthetic): An em-
bedding function is estimated that is on-average
close to a true representation of each task (‘+’).
The embeddings are shown compared to the tar-
get representation for each task distribution.
Fitting P1
Fitting P2
1
0
1
Fitting P3
1
0
1
Fitting P4 (novel)
f
f
Learning to learn 1D-regression
Figure 2: Meta-learning 1D-regression (syn-
thetic): The parameters of a 1D regression
model are ﬁtted from a small support set. The
training distributions (P1, P2, P3) give a useful
inductive bias for ﬁtting P4 using only 6 points.
Lemma 1. Fano’s Inequality [7, 5] For any estimator ˆY of Y such that Y →Z →ˆY forms a
Markov chain, it holds that,
P( ˆY ̸= Y ) ≥H(Y |Z) −1
log2 |Y |
= H(Y ) −I(Y ; Z) −1
log2 |Y |
.
Lemma 2. Local packing lemma [14] Consider distributions P1, . . . , PM ∈P. Let Y be a random
variable distributed uniformly on {1, . . . , M} and deﬁne the conditional distribution of Z given Y
by Z|{Y = j} ∼Pj. Then,
I(Y ; Z) ≤
1
M 2
X
1≤i,j≤M
DKL(Pi||Pj).
Lemma 3. Mutual information equality [12] Consider random variables Z1, Z2, Y , then,
I(Y ; (Z1, Z2)) + I(Z1; Z2) = I(Z1; (Z2, Y )) + I(Z2; Y )
4.1
Theoretical lower bounds
We now present lower bounds for function estimation in both the i.i.d and novel task settings. We
begin with the i.i.d case which is, in essence, the same as the one presented in Loh [14].
Theorem 1 (IID performance lower-bound). Suppose {P1, . . . , PM} ⊆P satisfy ρ(θ(Pi), θ(Pj)) ≥
2δ for all i ̸= j. Then,
inf
ˆθ
sup
P ∈P
ES∼P n
h
ES′∼P k[φ(ˆθS(S′), θP )]
i
≥ψ(δ)

1 −(n + k)I(Y ; Z) + 1
log2 M

where Y is distributed uniformly on {1, . . . , M} and the conditional distribution of Z given Y is
Z|{Y = j} ∼Pj.
We include the proof in Appendix A.1. Intuitively, we turn the statistical estimation problem into a
decoding problem where we must predict which distribution the data we observe was generated from.
Fano’s inequality provides best-case error probabilities for such a problem. The main difference to
Theorem 1 of Loh [14] is that the estimator here returns a function — this will be useful when we
move to estimation within novel tasks. We have the following immediate corollary.
Corollary 1. Assume the same setting as in Theorem 1. Additionally assume that
DKL(Pi||Pj) ≤α
for all pairs i and j, then,
inf
ˆθ
sup
P ∈P
ES∼P n
h
ES′∼P k[φ(ˆθS(S′), θP )]
i
≥ψ(δ)

1 −1 + (n + k)α
log2 M

,
Proof. The proof follows directly from Lemma 2 and Theorem 1.
3

Corollary 1 uses a standard bounding technique known as local-packing. Typically, such bounds
require the design of an appropriate set of distributions whose corresponding parameters are 2δ-
separated but maintain small KL divergences. In the multi-task setting such an assumption is
intuitively reasonable: challenging tasks should require separated parameters for ideal explanations
(2δ-separated) but should satisfy some relatedness measure (small KL). Relatedness is a necessary
feature of upper-bounds too but are typically difﬁcult to deﬁne (see e.g. Ben-David and Borbely [3]).
Theorem 2 (Minimax environment risk lower bound). Suppose {P1, . . . , PM+1} ⊆P satisfy
ρ(θi, θj) ≥2δ for all i ̸= j. Then,
inf
ˆθ
sup
P1,...,PM+1∈P
ESP∼¯
P n
1:M
h
ES′∼P k
M+1[φ(ˆθSP(S′), θPM+1)]
i
≥ψ(δ)

1 −nI(Y ; W) + kI(Y ; Z) + 1
log2(M + 1)

where Y is distributed uniformly on {1, . . . , M + 1} and the conditional distribution of W given Y
is deﬁned as follows: W|{Y = i} ∼¯P−i = 1
M
PM+1
j̸=i Pj (a mixture of all but the ith distribution)
and Z is deﬁned as in Theorem 1.
This result looks similar to the i.i.d bound of Theorem 1 but with a mutual information term that
describes a different decoding problem. In words, SP is a leave-one-task-out sample set and one
must predict which task distribution (of the M + 1) did not contribute any mass to the sample. We
present the proof in Appendix A.1.
From the statement of Theorem 2 it is not clear how the lower-bound compares to that of the i.i.d
case (Theorem 1). We derive a novel local-packing bound below which allows such a comparison.
Lemma 4 (Leave-one-task-out local packing). Let ¯P−i =
1
M
P
j̸=i Pj be the leave-one-task-
out mixture distribution.
The overall mixture distribution is denoted as ¯P
=
1
M+1
P
j Pj.
Let W be a single random variable drawn from the leave-one-task-out distribution and Y
be the discrete random variable of the index of the held-out distribution.
Then I(Y ; W) ≤
1
M(M+1)2
P
1≤i,j≤M+1 DKL(Pi∥Pj).
The proof can be found in Appendix A.2. Combining this Lemma 4 and Theorem 2 gives the
following immediate corollary.
Corollary 2. Assume the same setting as in Theorem 2. Additionally assume that,
DKL(Pi||Pj) ≤α
for all pairs i and j, then,
inf
ˆθ
sup
P1,...,PM+1∈P
ESP∼¯
P n
1:M
h
ES′∼P k
M+1[φ(ˆθSP(S′), θPM+1)]
i
≥ψ(δ)

1 −1 + ( n
M + k)α
log2(M + 1)

,
5
Discussion
Comparing bounds:
Assuming that some environment admits M + 1 distributions which are 2δ
separated and have DKL(Pi∥Pj) < α for all pairs. Corollary 2 admits a larger lower bound than
the i.i.d case. While this does not prove in general that the novel learning task is harder (which
should be intuitively obvious) it does allow us to identify cases where this is true — namely, when
the i.i.d bound is tight. For example, in Lasso regression [18] the i.i.d bound is tight [14] and thus
generalizing to novel tasks is strictly harder.
Limitations:
Theorem 2 ties together the choice of packing distributions and the number of tasks
seen by the training algorithm. This decision makes the proof and statement simpler than otherwise
but also means that adding more training tasks gives a worse bound as the environment complexity
increases (the decoding problem is harder).
Future work:
In addition to addressing the above limitations, there are several promising future
directions. One simple extension would add the notion of a query set instead of comparing the
estimator against the oracle representation. Similarly, we may wish to consider performance over
several novel tasks simultaneously as in N-way few-shot classiﬁcation. Finally, a natural extension
would be to introduce upper-bounds through the presented framework. It is not obvious how this
could be achieved through local-packing but Yang and Barron [20] utilized metric entropy to provide
both lower and upper bounds through information theory in the i.i.d setting.
4

References
[1] R. Amit and R. Meir. Meta-learning by adjusting priors based on extended pac-bayes theory.
arXiv preprint arXiv:1711.01244, 2017.
[2] J. Baxter. A model of inductive bias learning. Journal of artiﬁcial intelligence research, 12:
149–198, 2000.
[3] S. Ben-David and R. S. Borbely. A notion of task relatedness yielding provable multiple-task
learning guarantees. Machine learning, 73(3):273–287, 2008.
[4] S. Ben-David, J. Blitzer, K. Crammer, A. Kulesza, F. Pereira, and J. W. Vaughan. A theory of
learning from different domains. Machine learning, 79(1-2):151–175, 2010.
[5] T. M. Cover and J. A. Thomas. Elements of information theory. John Wiley & Sons, 2012.
[6] A. Fallah, A. Mokhtari, and A. Ozdaglar. On the convergence theory of gradient-based model-
agnostic meta-learning algorithms. arXiv preprint arXiv:1908.10400, 2019.
[7] R. Fano. Transmission of information. A Statistical Theory of Communication, 1961.
[8] C. Finn, P. Abbeel, and S. Levine. Model-agnostic meta-learning for fast adaptation of deep
networks. In Proceedings of the 34th International Conference on Machine Learning-Volume
70, pages 1126–1135. JMLR. org, 2017.
[9] C. Finn, A. Rajeswaran, S. Kakade, and S. Levine. Online meta-learning. arXiv preprint
arXiv:1902.08438, 2019.
[10] E. Hoffer and N. Ailon. Deep metric learning using triplet network. In International Workshop
on Similarity-Based Pattern Recognition, pages 84–92. Springer, 2015.
[11] R. Jin, S. Wang, and Y. Zhou. Regularized distance metric learning:theory and algorithm. In
Advances in Neural Information Processing Systems 22, pages 862–870, 2009.
[12] R. Z. Khas’ minskii. A lower bound on the risks of non-parametric estimates of densities in the
uniform metric. Theory of Probability & Its Applications, 23(4):794–798, 1979.
[13] M. Khodak, M.-F. Balcan, and A. Talwalkar. Provable guarantees for gradient-based meta-
learning. arXiv preprint arXiv:1902.10644, 2019.
[14] P.-L. Loh. On lower bounds for statistical learning theory. Entropy, 19(11):617, 2017.
[15] A. Pentina and C. Lampert. A pac-bayesian bound for lifelong learning. In International
Conference on Machine Learning, pages 991–999, 2014.
[16] S. Ravi and H. Larochelle. Optimization as a model for few-shot learning. International
Conference on Learning Representations, 2016.
[17] J. Snell, K. Swersky, and R. Zemel. Prototypical networks for few-shot learning. In Advances
in Neural Information Processing Systems, pages 4077–4087, 2017.
[18] R. Tibshirani. Regression shrinkage and selection via the lasso. Journal of the Royal Statistical
Society: Series B (Methodological), 58(1):267–288, 1996.
[19] O. Vinyals, C. Blundell, T. Lillicrap, D. Wierstra, et al. Matching networks for one shot learning.
In Advances in neural information processing systems, pages 3630–3638, 2016.
[20] Y. Yang and A. Barron. Information-theoretic determination of minimax rates of convergence.
Annals of Statistics, pages 1564–1599, 1999.
5

Appendices
A
Proofs
A.1
Lower-bounds
Theorem 1 (IID performance lower-bound). Suppose {P1, . . . , PM} ⊆P satisfy ρ(θ(Pi), θ(Pj)) ≥
2δ for all i ̸= j. Then,
inf
ˆθ
sup
P ∈P
ES∼P n
h
ES′∼P k[φ(ˆθS(S′), θP )]
i
≥ψ(δ)

1 −(n + k)I(Y ; Z) + 1
log2 M

where Y is distributed uniformly on {1, . . . , M} and the conditional distribution of Z given Y is
Z|{Y = j} ∼Pj.
Proof. First, notice that,
sup
P ∈P
ES∼P n
h
ES′∼P k[φ(ˆθS(S′), θP )]
i
≥1
m
M
X
i=1
ES∼P n
i
h
ES′∼P k
i [φ(ˆθS(S′), θPi)]
i
.
Now deﬁne the decision rule,
f(S, y) = arg min
1≤j≤M φ(ˆθS(y), θPj)],
with ties broken arbitrarily. We proceed by bounding the expected loss. First, using Markov’s
inequality,
ES∼P n
i
h
ES′∼P k
i [φ(ˆθS(S′), θi)]
i
≥ψ(δ)PS∼P n
i ,S′∼P k
i
h
ψ(ρ(ˆθS(S′), θi)) ≥ψ(δ)
i
,
= ψ(δ)PS∼P n
i ,S′∼P k
i
h
ρ(ˆθS(S′), θi) ≥δ
i
.
Next, consider the case ρ(ˆθS(S′), θi)) < δ. Through the triangle inequality,
ρ(ˆθS(S′), θj)] ≥ρ(θPi, θPj) −ρ(ˆθS(S′), θPi)
≥2δ −δ > ρ(ˆθS(S′), θi)
Thus, the probability that the distance is less than δ is at least as large as the probability that the
estimator is correct.
ψ(δ)Pi
h
φ(ˆθS(S′), θi) ≥ψ(δ)
i
≥ψ(δ)P(f(S, S′) ̸= i).
Now, using Lemma 1 with ˆY = f(S, S′) (and the corresponding Markov chain
Y →(S, S′) →f(S, S′)), we have,
1
M
M
X
i=1
P(f(S, S′) ̸= i) ≥log2 M −I(Y ; (S, S′)) −1
log2 M
.
Using Lemma 3, noting that each element in S and S′ are i.i.d given Y , we have I(Y ; (S, S′)) ≤
I(Y ; S) + I(Y ; S′) ≤(n + k)I(Y ; Z). Combining the above inequalities gives the ﬁnal result.
Theorem 2 (Minimax environment risk lower bound). Suppose {P1, . . . , PM+1} ⊆P satisfy
ρ(θi, θj) ≥2δ for all i ̸= j. Then,
inf
ˆθ
sup
P1,...,PM+1∈P
ESP∼¯
P n
1:M
h
ES′∼P k
M+1[φ(ˆθSP(S′), θPM+1)]
i
≥ψ(δ)

1 −nI(Y ; W) + kI(Y ; Z) + 1
log2(M + 1)

where Y is distributed uniformly on {1, . . . , M + 1} and the conditional distribution of W given Y
is deﬁned as follows: W|{Y = i} ∼¯P−i = 1
M
PM+1
j̸=i Pj (a mixture of all but the ith distribution)
and Z is deﬁned as in Theorem 1.
6

We sketch the proof below, highlighting differences compared to the proof of Theorem 1.
Proof. (Sketch) As in the i.i.d case, we ﬁrst bound the supremum from below with an average,
sup
P1,...,PM+1∈P
ESP∼¯
P n
1:M
h
ES′∼P k
M+1[φ(ˆθSP(S′), θPM+1)]
i
≥
1
M + 1
M+1
X
i=1
ESP∼¯
P n
−i
h
ES′∼Pi[φ(ˆθSP(S′), θPi)]
i
.
This term can be thought of as a leave-one-task-out estimator, wherein the algorithm is trained on
data from all but one task and evaluated on the missing task.
We deﬁne the following estimator,
f(SP, y) = arg
min
1≤j≤M+1 ρ(ˆθSP(y), θPj)
Using Markov’s inequality, and then following the proof of Theorem 1, with the use of Lemma 1, we
arrive at,
1
M + 1
M+1
X
i=1
ESP∼¯
P n
−i
h
ES′∼P k
i [φ(ˆθSP(S′), θPi)]
i
≥
ψ(δ)
M + 1
M+1
X
i=1
P(f(SP, S′) ̸= i),
≥ψ(δ)log2 (M + 1) −I(Y ; (SP, S′)) −1
log2 (M + 1)
.
Conditioned on Y , each element of SP and S′ are independent but they are not identically distributed.
Thus, using Lemma 3,
I(Y ; (SP, S′)) ≤kI(Y ; Z) + nI(Y ; W)
The result follows by combining these inequalities.
A.2
Local packing results
Lemma 4 (Leave-one-task-out local packing). Let ¯P−i =
1
M
P
j̸=i Pj be the leave-one-task-
out mixture distribution.
The overall mixture distribution is denoted as ¯P
=
1
M+1
P
j Pj.
Let W be a single random variable drawn from the leave-one-task-out distribution and Y
be the discrete random variable of the index of the held-out distribution.
Then I(Y ; W) ≤
1
M(M+1)2
P
1≤i,j≤M+1 DKL(Pi∥Pj).
Proof. From Lemma 2 (and some simple arithmetic) we have,
I(Y ; W) =
1
M + 1
M+1
X
i=1
DKL( ¯P−i∥¯P).
(3)
Note that by the deﬁnition of the mixture distribution,
¯P =
M
M + 1
¯P−i +
1
M + 1Pi.
7

Using the convexity of the KL divergence,
I(Y ; W) =
1
M + 1
M+1
X
i=1
DKL

¯P−i

M
M + 1
¯P−i +
1
M + 1Pi

≤
1
M + 1
M+1
X
i=1
M
M + 1DKL( ¯P−i∥¯P−i) +
1
M + 1DKL( ¯P−i∥Pi)
=
1
(M + 1)2
M+1
X
i=1
DKL( ¯P−i∥Pi)
=
1
(M + 1)2
M+1
X
i=1
DKL

1
M
X
1≤j≤M+1,j̸=i
Pj
Pi


≤
1
M(M + 1)2
M+1
X
i=1
X
1≤j≤M+1,j̸=i
DKL(Pj∥Pi)
=
1
M(M + 1)2
X
1≤i,j≤M+1
DKL(Pi∥Pj).
Noting for the last step that the KL is zero if and only if the distributions are the same almost
everywhere.
8

