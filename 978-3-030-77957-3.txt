Springer Proceedings in Mathematics & Statistics
Frédéric Barbaresco
Frank Nielsen   Editors
Geometric Structures 
of Statistical Physics, 
Information Geometry, 
and Learning
SPIGL’20, Les Houches, France, 
July 27–31

Springer Proceedings in Mathematics &
Statistics
Volume 361

This book series features volumes composed of selected contributions from
workshops and conferences in all areas of current research in mathematics and
statistics, including operation research and optimization. In addition to an overall
evaluation of the interest, scientiﬁc quality, and timeliness of each proposal at the
hands of the publisher, individual contributions are all refereed to the high quality
standards of leading journals in the ﬁeld. Thus, this series provides the research
community with well-edited, authoritative reports on developments in the most
exciting areas of mathematical and statistical research today.
More information about this series at http://www.springer.com/series/10533

Frédéric Barbaresco
• Frank Nielsen
Editors
Geometric Structures
of Statistical Physics,
Information Geometry,
and Learning
SPIGL’20, Les Houches, France, July 27–31
123

Editors
Frédéric Barbaresco
Thales Land & Air Systems,
Technical Directorate
Thales
Limours, France
Frank Nielsen
Sony Computer Science Laboratories Inc.
Tokyo, Japan
ISSN 2194-1009
ISSN 2194-1017
(electronic)
Springer Proceedings in Mathematics & Statistics
ISBN 978-3-030-77956-6
ISBN 978-3-030-77957-3
(eBook)
https://doi.org/10.1007/978-3-030-77957-3
Mathematics Subject Classiﬁcation: 62-06, 82B30, 70-06, 68Txx, 68T05, 82-06, 53B12, 62B11, 22-06,
62Dxx, 80-06
© The Editor(s) (if applicable) and The Author(s), under exclusive license
to Springer Nature Switzerland AG 2021
This work is subject to copyright. All rights are solely and exclusively licensed by the Publisher, whether
the whole or part of the material is concerned, speciﬁcally the rights of translation, reprinting, reuse of
illustrations, recitation, broadcasting, reproduction on microﬁlms or in any other physical way, and
transmission or information storage and retrieval, electronic adaptation, computer software, or by similar
or dissimilar methodology now known or hereafter developed.
The use of general descriptive names, registered names, trademarks, service marks, etc. in this
publication does not imply, even in the absence of a speciﬁc statement, that such names are exempt from
the relevant protective laws and regulations and therefore free for general use.
The publisher, the authors and the editors are safe to assume that the advice and information in this
book are believed to be true and accurate at the date of publication. Neither the publisher nor the
authors or the editors give a warranty, expressed or implied, with respect to the material contained
herein or for any errors or omissions that may have been made. The publisher remains neutral with regard
to jurisdictional claims in published maps and institutional afﬁliations.
This Springer imprint is published by the registered company Springer Nature Switzerland AG
The registered company address is: Gewerbestrasse 11, 6330 Cham, Switzerland

Preface
Geometric
Structures
of
statistical
Physics,
Information
Geometry, and Learning
Ecole de Physique des Houches SPIGL’20 Summer Week
SPRINGER Proceedings in Mathematics & Statistics, 2021
Subject
This book is proceedings of Les Houches Summer Week SPIGL’20 (Joint
Structures and Common Foundation of Statistical Physics, Information Geometry
and Inference for Learning) organized from July 27–31, 2020, at L’Ecole de
Physique des Houches:
Website https://franknielsen.github.io/SPIG-LesHouches2020/
Videos: https://www.youtube.com/playlist?list=PLo9ufcrEqwWExTBPgQPJwA
JhoUChMbROr
The conference SPIGL’20 has developed the following topics:
Geometric Structures of Statistical Physics and Information
• Statistical mechanics and geometric mechanics
• Thermodynamics, symplectic and contact geometries
• Lie groups thermodynamics
• Relativistic and continuous media thermodynamics
• Symplectic integrators
v

Physical Structures of Inference and Learning
Stochastic gradient of Langevin’s dynamics
Information geometry, Fisher metric, and natural gradient
Monte Carlo Hamiltonian methods
Variational inference and Hamiltonian controls
Boltzmann machine
Organizers
Frédéric
Barbaresco
THALES, KTD PCC, Palaiseau, France
Silvère Bonnabel
Mines ParisTech, CAOR, Paris, France
Géry de Saxcé
Université de Lille, LaMcube, Lille, France
François
Gay-Balmaz
Ecole Normale Supérieure Ulm, CNRS & LMD, Paris,
France
Bernhard Maschke
Université Claude Bernard, LAGEPP, Lyon, France
Eric Moulines
Ecole Polytechnique, CMAP, Palaiseau, France
Frank Nielsen
Sony Computer Science Laboratories, Tokyo, Japan
vi
Preface

Scientiﬁc Rational
In the middle of the last century, Léon Brillouin in “The Science and The Theory of
Information” or André Blanc-Lapierre in “Statistical Mechanics” forged the ﬁrst
links between the theory of information and statistical physics as precursors.
In the context of artiﬁcial intelligence, machine learning algorithms use more
and more methodological tools coming from the physics or the statistical
mechanics. The laws and principles that underpin this physics can shed new light on
the conceptual basis of artiﬁcial intelligence. Thus, the principles of maximum
entropy, minimum of free energy, Gibbs–Duhem’s thermodynamic potentials and
the generalization of François Massieu’s notions of characteristic functions enrich
the variational formalism of machine learning. Conversely, the pitfalls encountered
by artiﬁcial intelligence to extend its application domains question the foundations
of statistical physics, such as the construction of stochastic gradient in large
dimension, the generalization of the notions of Gibbs densities in spaces of more
elaborate representation like data on homogeneous differential or symplectic
manifolds, Lie groups, graphs, and tensors.
Sophisticated statistical models were introduced very early to deal with unsu-
pervised learning tasks related to Ising–Potts models (the Ising–Potts model deﬁnes
the interaction of spins arranged on a graph) of statistical physics and more gen-
erally the Markov ﬁelds. The Ising models are associated with the theory of mean
ﬁelds (study of systems with complex interactions through simpliﬁed models in
which the action of the complete network on an actor is summarized by a single
mean interaction in the sense of the mean ﬁeld).
The porosity between the two disciplines has been established since the birth of
artiﬁcial intelligence with the use of Boltzmann machines and the problem of robust
methods for calculating partition function. More recently, gradient algorithms for
neural network learning use large-scale robust extensions of the natural gradient of
Fisher-based information geometry (to ensure reparameterization invariance), and
stochastic gradient based on the Langevin equation (to ensure regularization), or
their coupling called “natural Langevin dynamics”.
Concomitantly, during the last ﬁfty years, statistical physics has been the object
of new geometrical formalizations (contact or symplectic geometry, ...) to try to
give a new covariant formalization to the thermodynamics of dynamic systems. We
can mention the extension of the symplectic models of geometric mechanics to
statistical mechanics, or other developments such as random mechanics, geometric
mechanics in its stochastic version, Lie groups thermodynamics, and geometric
modeling of phase transition phenomena.
Finally, we refer to computational statistical physics, which uses efﬁcient
numerical methods for large-scale sampling and multimodal probability measure-
ments (sampling of Boltzmann–Gibbs measurements and calculations of free
energy, metastable dynamics and rare events, ...) and the study of geometric inte-
grators (Hamiltonian dynamics, symplectic integrators, ...) with good properties of
covariances and stability (use of symmetries, preservation of invariants, ...).
Preface
vii

Machine learning inference processes are just beginning to adapt these new inte-
gration schemes and their remarkable stability properties to increasingly abstract
data representation spaces.
Artiﬁcial intelligence currently uses only a very limited portion of the conceptual
and methodological tools of statistical physics. The purpose of this conference is to
encourage constructive dialog around a common foundation, to allow the estab-
lishment of new principles and laws governing the two disciplines in a uniﬁed
approach. However, it is also about exploring new chemins de traverse.
Main contributors in thermodynamics, statistical physics, information geometry
and Lie group representation theory:
viii
Preface

Frédéric Barbaresco
April 2021
Frank Nielsen
Preface
ix

Contents
Part I: Tribute to Jean-Marie Souriau Seminal Works
Structure des Systèmes Dynamiques Jean-Marie Souriau’s Book
50th Birthday . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
Géry de Saxcé and Charles-Michel Marle
Jean-Marie Souriau’s Symplectic Model of Statistical Physics:
Seminal Papers on Lie Groups Thermodynamics - Quod
Erat Demonstrandum . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
12
Frédéric Barbaresco
Part II: Lie Group Geometry and Diffeological Model
of Statistical Physics and Information Geometry
Souriau-Casimir Lie Groups Thermodynamics
and Machine Learning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
53
Frédéric Barbaresco
An Exponential Family on the Upper Half Plane and Its
Conjugate Prior . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
84
Koichi Tojo and Taro Yoshino
Wrapped Statistical Models on Manifolds: Motivations, The Case SE
(n), and Generalization to Symmetric Spaces . . . . . . . . . . . . . . . . . . . . .
96
Emmanuel Chevallier and Nicolas Guigui
Galilean Thermodynamics of Continua . . . . . . . . . . . . . . . . . . . . . . . . .
107
Géry de Saxcé
Nonparametric Estimations and the Diffeological Fisher Metric. . . . . . .
120
Hông Vân Lê and Alexey A. Tuzhilin
xi

Part III: Advanced Geometrical Models of Statistical
Manifolds in Information Geometry
Information Geometry and Integrable Hamiltonian Systems . . . . . . . . .
141
J.-P. Françoise
Relevant Differential Topology in Statistical Manifolds . . . . . . . . . . . . .
154
Michel Nguiffo-Boyom
A Lecture About the Use of Orlicz Spaces in Information Geometry . . .
179
Giovanni Pistone
Quasiconvex Jensen Divergences and Quasiconvex
Bregman Divergences . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
196
Frank Nielsen and Gaëtan Hadjeres
Part IV: Geometric Structures of Mechanics,
Thermodynamics and Inference for Learning
Dirac Structures and Variational Formulation of Thermodynamics
for Open Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
221
Hiroaki Yoshimura and François Gay-Balmaz
The Geometry of Some Thermodynamic Systems . . . . . . . . . . . . . . . . .
247
Alexandre Anahory Simoes, David Martín de Diego,
Manuel Lainz Valcázar, and Manuel de León
Learning Physics from Data: A Thermodynamic Interpretation . . . . . .
276
Francisco Chinesta, Elías Cueto, Miroslav Grmela, Beatriz Moya,
Michal Pavelka, and Martin Šípka
Computational Dynamics of Reduced Coupled Multibody-Fluid
System in Lie Group Setting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
298
Zdravko Terze, Viktor Pandža, Marijan Andrić, and Dario Zlatar
Material Modeling via Thermodynamics-Based Artiﬁcial
Neural Networks . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
308
Filippo Masi, Ioannis Stefanou, Paolo Vannucci, and Victor Mafﬁ-Berthier
Information Geometry and Quantum Fields . . . . . . . . . . . . . . . . . . . . .
330
Kevin T. Grosvenor
Part V: Hamiltonian Monte Carlo, HMC Sampling
and Learning on Manifolds
Geometric Integration of Measure-Preserving Flows for Sampling . . . .
345
Alessandro Barp
xii
Contents

Bayesian Inference on Local Distributions of Functions and
Multidimensional Curves with Spherical HMC Sampling . . . . . . . . . . .
356
Anis Fradi, Ines Adouani, and Chaﬁk Samir
Sampling and Statistical Physics via Symmetry . . . . . . . . . . . . . . . . . . .
374
Steve Huntsman
A Practical Hands-on for Learning Graph Data Communities on
Manifolds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
428
Thomas Gerald, Hadi Zaatiti, and Hatem Hajri
Contents
xiii

Part I: Tribute to Jean-Marie Souriau
Seminal Works

Structure des Systèmes Dynamiques
Jean-Marie Souriau’s Book 50th Birthday
Géry de Saxcé1(B) and Charles-Michel Marle2
1 Univ. Lille, CNRS, Centrale Lille, UMR 9013 – LaMcube – Laboratoire
de mécanique multiphysique multiéchelle, Lille, France
gery.de-saxce@univ-lille.fr
2 Université Pierre et Marie Curie,
today Sorbonne Université, Paris, France
Abstract. Jean-Marie Souriau’s book “Structure des systèmes dynami-
ques”, published in 1970, republished recently by Gabay, translated in
English and published under the title “Structure of Dynamical Systems,
a Symplectic View of Physics”, is a work with an exceptional wealth
which, ﬁfty years after its publication, is still topical. In this paper, we
give a brief description of its content and we intend to highlight the ideas
that to us, are the most creative and promising.
1
A Few Introductory Words
Graduated of the Ecole Normale Supérieure in 1942, Jean-Marie Souriau obtains
the aggregation of mathematics in 1945, ranked second. After a brief passage at
the CNRS, he joins as an engineer the ONERA recently created and prepares his
Ph.D thesis on the airplane stability. Defended in 1952, this work was used for the
design of several airplanes, in particular the Concorde. Next he obtains a position
as Professor at the Tunis Faculty in 1952 and at Aix-Marseille University in 1964,
where he published “Structure des Systèmes Dynamique”. For more details on
Jean-Marie Souriau’s life and personality from his friends and persons who have
known him very well, you can read the paper paying hommage to him in [9].
Jean-Marie Souriau’s book “Structure des systèmes dynamiques” [6], was pub-
lished in 1970 in a book collection for students in the ﬁrst year of master’s degree
in Mathematics, directed in fact to mathematicians, beginners or experienced,
wishing to know the applications of mathematics to physical sciences, and to
physicists concerned with knowing certain mathematical tools useful for their
researches. The author was very aware of this since, in his Introduction, he
gives reading recommendations adapted to both reader categories. The book is
illustrated by many ﬁgures which mostly are very meaningful schematic repre-
sentations of the geometric constructions used by the author. The notations are
often very non standard but they are totally consistent.
The original edition is out of print, although it is possible to ﬁnd copies of
second hand. Nevertheless, it was republished recently by Gabay. It was also
translated in English and published under the title “Structure of Dynamical
c
⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2021
F. Barbaresco and F. Nielsen (Eds.): SPIGL 2020, PROMS 361, pp. 3–11, 2021.
https://doi.org/10.1007/978-3-030-77957-3_1

4
G. de Saxcé and C.-M. Marle
Systems, a Symplectic View of Physics” [7]. The traduction was supervised by
Richard Cushman and Gijs Tuynman who have known very well the author. It
is also possible to download a scanned version from Jean-Marie’s Souriau oﬃcial
website [8], and a lot of other interesting thinks.
2
Introduction
The book begins by a large introduction of 20 pages in which Souriau highlights
the guiding ideas. He considers the classical analytical mechanics, that originated
in the book by Jean-Louis Lagrange Mécanique analytique [3] and remains an
essential ingredient of the current physical theories, is not outdated although
certain used concepts have become so because they have not the required covari-
ance, in other words because they are in contradiction with Galilean relativity.
He wishes to show in his book that a better consideration of Lagrange’s thought
allows giving to this theory a form compatible with the more recent discoveries
of the physical sciences.
3
Chapter I: Diﬀerential Geometry
In the ﬁrst chapter, Souriau presents in less than 70 pages numerous tricky
notions from diﬀerential manifolds to Lie groups and calculus of variation. The
author presents the concept of diﬀerential manifold in a rather original way,
bypassing the one of topological space, undoubtedly to make this notion easily
accessible to beginning students. The diﬀerential manifolds are not supposed
Hausdorﬀ. Examples of non Hausdorﬀmanifolds such as the space of motions of
certain mechanical systems will be indeed encountered in Sect. 5.
Note author’s language particularity that could cause confusion: he calls
embedding what most of geometers call injective immersion. This was corrected
in the English version. This choice is very reasonable because injective immer-
sions are much more frequently encountered than embeddings. For example,
orbits of a Lie group action, as well as leaves of a foliation, always are immersed
in the manifold in which they are contained, and much more rarely embedded.
4
Chapter II: Symplectic Geometry
The second chapter is also essentially mathematical but shorter than the previ-
ous one (about 45 pages). It starts with algebraic notions concerning the skew-
symmetric forms, then 2-forms and denoted σ. If it is non degenerate, it is called
symplectic, otherwise presymplectic.
Next, Souriau deﬁnes the symplectic and presymplectic manifolds, and stud-
ies their properties. He shows that under certain conditions the quotient of a
presymplectic manifold by its kernel is a symplectic manifold, result that he
will use latter on to deﬁne the space of motions of a dynamical system that

Structure des Systèmes Dynamiques
5
plays a central role in his theory. The elements of the quotient are leaves of a
characteristic foliation.
Let u be a diﬀerentiable function deﬁned on a symplectic manifold. Its dif-
ferential du is a 1-form. If the form σ is symplectic, there exists a unique vector
associated to du that he denotes grad u and call symplectic gradient, such that:
−du ≡σ(grad u)
The ﬂow of this vector ﬁeld let the symplectic form invariant. Let us remark
in passing the non standard notation for the interior product σ(grad u) of the
vector grad u and the form σ.
Souriau calls dynamical group (that other authors call symplectic group) of a
symplectic or presymplectic manifold V a Lie group G acting on it by canonical
transformations. He calls moment of the dynamical group G a diﬀerentiable map
ψ from a point x of V onto an element μ of the vector space G∗, dual of the Lie
algebra G of G, such that for every generator Z ∈G its inﬁnitesimal action is
the symplectic gradient of the function linking, at every x ∈V the real μ · Z:
σ(ZV(x)) ≡−d [μ · Z]
In the terminology used by most of geometers, this inﬁnitesimal generator is the
Hamiltonian vector space, the corresponding Hamiltonian being this function of
value μ · Z.
The author gives several examples of dynamical groups, indicates suﬃsant
conditions for a dynamical group admitting a moment and studies its properties.
This deals him to propose a generalization of Noether’s theorem.
We arrive to one of the most original Souriau’s theories, the symplectic coho-
mology of Lie Groups.
In one hand, we let the element a act on x, next we take the value of the
moment map. On the other hand, we take the value μ of the moment map ψ,
next we apply the coadjoint representation of the group for the element a of G.
Souriau proved that the diﬀerence between the two previous results does not
depends on the current point x on the manifold, then only on a. This deﬁnes a
map:
θ(a) ≡ψ(aV(x)) −aG∗(ψ(x))
from the group to the dual of its Lie algebra. It veriﬁes an identity that means
that the group acts on the moment space by aﬃne representation with linear
part the coadjoint action aG∗and translation part θ(a):
θ(a × b) ≡θ(a) + aG∗(θ(b))
The diﬀerential of θ at the identity, denoted f , is a bilinear form on the dual
and, even less obvious, it is skew-symmetric, then a 2-form and it veriﬁes an
identity that generalizes Jacobi’s one:
f (Z)([Z ′, Z ′′]) + f (Z ′)([Z ′′, Z]) + f (Z ′′)([Z, Z ′]) = 0

6
G. de Saxcé and C.-M. Marle
The map θ measures the defect of symplectic cohomology of the dynamical group.
It is called symplectic cocycle. It is possible to deﬁne also symplectic cobound-
aries, and passing to the quotient to deﬁne classes of symplectic cohomology.
We arrive to another fundamental result called now Kirillov-Kostant-Souriau
theorem. In fact, it is not necessary to study case-by-case the action a dynamical
group on every symplectic manifold. You can disregard the manifold and take
interest on the action of the group onto the dual of its Lie algebra. Then, on
each orbit U, there exists a canonical symplectic structure and the identity map
of the orbit is a moment map. In other words, we may say that each point μ of
the orbit U is its own moment.
5
Chapter III: Mechanics
The starting point is the following 2-form that Souriau attributes to Lagrange.
The Lagrange form of a system of material points is the sum of the Lagrange
forms of all points of the system, where mj is the mass of particle j, r j its
position, vj its velocity and F j the resultant of the forces acting upon it:
σ =

j

mjδvj −F jδt, δ′r j −vjδ′t

−

mjδ′vj −F jδ′t, δr j −vjδt

that could written:
σ =

j
mjdvj −F jdt ∧dr j −vjdt
where the symbol ∧denotes the operator combining the dot product and the
exterior product. You can observe here one of the peculiarities of Souriau’s nota-
tions. He does not use the symbol ∧for the exterior product of diﬀerential forms,
which has some advantages but also drawbacks. Then, the well-known equations
of motion can be written in a very compact manner: the tangent vector to the
trajectory in the evolution space of the positions, velocities and time belongs to
the kernel of the presymplectic form.
Next, Souriau deﬁnes the vector E j in terms of the force F j occurring in
Lagrange 2-forms and a vector Bj freely chosen:
E j ≡F j + Bj × vj
This leads him to formulate what he calls Maxwell’s principle in the following
form: the vector ﬁelds Bj may be chosen in such a way that the Lagrange form is
closed. Hence this condition determines these ﬁelds in a unique manner. Stating
this principle, the author proves that:
1. the ﬁelds E j and Bj must not depend on the velocities of the material points
2. Bj does not depends on the position of other particles than j
3. They verify a condition in which we recognize Maxwell reciprocity principle
(in fact a version of the action and reaction principle for forces acting at
distance)

Structure des Systèmes Dynamiques
7
4. And they verify two equations in which we recognize Faraday equation and
the law of induction (or absence of magnetic monopoles). By contrast, Ampere
and Gauss equations are relativistic eﬀects which do not appear.
Souriau adopts Maxwell’s principle as a new law of the mechanics (not only
of the electromagnetism) and presents various examples: the N-body problem
of Celestial mechanics, the gravitation, where E is the usual gravity force and
B × v is Coriolis’ force, and of course the electromagnetic ﬁeld. Without denying
the importance of the principle of least action or even the usefulness of these
formalisms, the author declares these concepts seem to him less fundamental as
the Maxwell principle.
One of the originally of Souriau’s work is to emphasize the role played by the
space of motions. Starting from the evolution space which is equipped with a
presymplectic structure, the space of motions is a symplectic manifold obtained
by quotient of the foliation. The space of motions of a dynamical system is not
very often considered by modern authors, though it appeared as soon as 1808 in
the works of Lagrange. This very natural concept has a nice mathematical prop-
erty: the space of motions is always endowed with a smooth manifold structure.
One may wonder why the concept of space of motions is not used more by
modern authors. Maybe it is because for some dynamical systems, the space of
motions is a non-Hausdorﬀmanifold. Another possible explanation is that some
scientists are interested in the thorough description of particular motions of a
system, rather than by the study of the set of all possible motions. By showing
that many important results can be deduced from the symmetries of the space
of motions of a system, the author proves that this reluctance is unfounded.
Next Souriau introduce Galilei group, the symmetry group of Mechanics, a
Lie group of dimension 10. deﬁned as the set of matrices :
a ≡
⎡⎢⎢⎢⎢⎣
A b c
0 1 e
0 0 1
⎤⎥⎥⎥⎥⎦
,
where, with his notations, A is a rotation, b the Galilean boost, c a space trans-
lation ans e a clock change. When the system is isolated, the Galilei group acts
onto the evolution space and the space of motions. On the evolution space, the
moment of this action is a ﬁrst integral of the motion. The author details its 10
components, which can be regrouped in three vectors p, l, g and a scalar E. He
gives their physical meaning:
1. p is the total linear momentum;
2. l is the total angular momentum;
3. the equality g = Constant conveys the fact that the center of mass moves on
a straight line at constant velocity;
4. the scalar E, deﬁned modulo an additive constant, is the total energy.
By a calculation similar to Bargmann’s one, Souriau proved that the symplec-
tic cohomology of the Galilei group is of dimension 1. He considers the number

8
G. de Saxcé and C.-M. Marle
m that spots the class of cohomology of the action of the Galilei group on the
space of motions as being the mass.
In the paragraph The principles of symplectic mechanics, the author ﬁrst
takes place in the frame of the classical mechanics, non relativistic. He is no
longer limited to the systems of material points and he adopts the three following
assertions as new axioms of the mechanics:
I. The space of the motions of a dynamical system is a connected symplectic
manifold.
II. If several dynamical systems evolve independently, the manifold of motions
of the composite system is the symplectic direct product of the spaces of
motions of the component systems.
III. If a dynamical system is isolated, its manifold of motions admits the Galilei
group as a dynamical group.
It is an extension of the principles generally admitted of the classical mechan-
ics, which will allow to the author considering new dynamical systems having a
physical interest.
In special relativity, the passage of a Lorentz frame to another one is made
by the action of an element of the restricted Poincaré group. The transition to
the Relativistic case consists in a simple change of the symmetry group in the
third axioms:
III. If a dynamical system is isolated, its manifold of motions admits the
restricted Poincaré group as a dynamical group.
In a long section, the author proposes a mechanistic description of elemen-
tary particles. In the framework of relativistic Mechanics, an isolated dynamical
system is said to be elementary when the Poincaré group acts transitively on
the space of its motions. The moment map of its action is then a symplectic
diﬀeomorphism of this space onto a coadjoint orbit of the Poincaré group. For
him, so deﬁned elementary systems are mathematical models for the elementary
particles of physicists. Besides the classical energy-momentum P, Souriau intro-
duces the polarization 4-vector W = ∗(M) · P, built from P and the moment M
associated to the inﬁnitesimal Lorentz transformation, through a linear map ∗
transforming an anti-hermitian operator into another anti-hermitian operator.
As example let us consider a particle with spin. It is when P is timelike and when
W (which, being orthogonal to P, is spacelike) is non-zero. It is characterized by
two invariants, the mass m stemming from the energy-momentum and the spin
s from the polarization.
The classiﬁcation of the coadjoint orbits of Poincaré group provides us the
table of elementary particles:

Structure des Systèmes Dynamiques
9
I. A particule with spin (previously described)
II. A particle without spin. It is when P is timelike and W = 0
III. A massless particle. It is when both P and W are non-zero and lightlike.
The author deﬁnes three real numbers, the sign of the energy η, the helicity
χ and the spin s
The Nonrelativistic particles are obtained by scaling of lengths and times,
next considering the limit when the velocity approaches zero.
6
Chapter IV: Statistical Mechanics
It is devoted to Statistical Mechanics. It contains two sections. The ﬁrst one is
a very condensed course in Measure Theory and Integration, with some notions
in Probability. The second one presents the principles of Statistical Mechanics
in a very original way based on the Lie group theory. Souriau calls generalized
Gibbs probability law any completely continuous probability law of density:
f (x) ≡e−[z+Z(Ψ(x))]
where z is a scalar, Z an inﬁnitesimal generator living in the Lie algebra of
the group and Ψ is the moment map such that the law is integrable. Then, he
establishes the relation between the entropy s, the Planck potential z and the
mean value M of the moment Ψ:
s = z + Z(M)
The author explains that the entropy of the system increases with time, so
assuming that the natural equilibria of the gas are elements of the Gibbs set
of the group of time translations is a very reasonable assumption. Each Gibbs
state is determined by an element Z of the one-dimensional Lie algebra of this
group, which is a way of measuring the gas temperature. Since the group of time
translations is a subgroup, but not a normal subgroup, of the Galilei group,
a dynamical system conservative in some inertial reference frame is not con-
servative in a diﬀerent inertial reference frame. This important remark leads
the author to introduce the new concept of covariant statistical mechanics by
proposing the following principle:
When a dynamical system is invariant by the action of some Lie subgroup G′ of
the Galilei group, its natural equilibria are the elements of the Gibbs set of the
action of G′ .
He then discusses in greater detail several examples, among them a gas in a
centrifuge (this relative motion is now a rotation around an axis at a constant
angular velocity). He considers also a system made by particles with spin, and
ﬁnds that the most probable orientation of the particles spin is parallel to the
rotation axis.

10
G. de Saxcé and C.-M. Marle
7
Chapter V: A Method of Quantization
The theory of geometric quantization was developed in the early 70’ indepen-
dently by Souriau and Kostant [2] in order to bring into focus some vaguely
perceived analogies between representation theory and quantum mechanics that
had for a long time been part of the “ﬂolklore” of modern physics. The author
deﬁnes a quantum manifold as a smooth manifold Y endowed with a contact
1-form ϖ, a presymplectic structure for the exterior diﬀerential σ of ϖ, such
that all integral curves are the orbits of an action on Y of the 1-dimensional
torus. The set of these curves, in other words the quotient of Y by this action, is
a symplectic manifold U, called by the author the base of the quantum manifold
Y.
In practice, the Physicist only knows the space of motion U of a classical
system and he has to address the following issue: is there a quantum manifold Y
and a symplectomorphism from the base of Y onto the space of motion? This is
the problem of the quantization. The author proves that the manifold of motions
of a non-relativistic particle with spin is quantizable if and only if the spin of
the particle is integer or half integer, when expressed with ℏas unit.
Isomorphisms of quantum manifolds are called by the author quantomor-
phisms. Any quantomorphism between two quantum manifolds projects onto a
symplectomorphism between their bases. A group Γ of quantomorphisms of a
quantum manifold Y projects onto a group of symplectomorphisms of its basis
U, and its projection is a group homomorphism. Conversely, a group G of sym-
plectomorphisms of the basis U is said to be liftable if there exists a group Γ of
quantomorphisms of Y which projects onto it. He then discusses the quantization
of a dynamical group of a quantizable symplectic manifold U, with the quantum
manifold Y as quantization. He proves that when a dynamical group of U is
quantizable, its symplectic cohomology is zero. He gives examples which prove
that this necessary condition is not suﬃcient. A dynamical group of U which is
liftable, but not quantizable, may have an extension which still is a dynamical
group of U and is quantizable.
All this deep analysis on geometric basis allow to give solid foundations to
the well known correspondance principle. Let u be a smooth function on the
space of motions U, representing an observable. Then Souriau explains how to
associate to any observable u deﬁned on the base U an operator ˆu on the Hilbert
space H(Y) of the state vectors Ψ of the quantum manifold Y, deﬁned by this
relation :
ˆu(Ψ)(ξ) ≡−i δ
u [Ψ(ξ)]
where in the right hand member, the notation δ
u means the derivative of Ψ in
the direction of the symplectic gradient of u Then Souriau proved that:
1. The operator ˆu is hermitian,
2. The map from u to ˆu is linear and injective,
3. For u = 1, ˆu is the identity of the Hilbert space,
and he proved the correspondance between quantum commutators and Poisson
brackets (originally proposed by Dirac).

Structure des Systèmes Dynamiques
11
8
Conclusions
In this brief presentation, we have just skimmed over the book content of excep-
tional depth. A more detailled presentation of the book will be published soon
in [1].
When reading this book, we cannot fail to be impressed by the extent and the
thoroughness of the author’s knowledge, as well in Mathematics as in Mechanics
or in Physics, and by the originality and the depth of his thoughts.
We would like also to bring our attention to the fact that Jean-Marie Souriau
is the author of two other very remarkable books in French, both published in
1964, « Géométrie et Relativité” [4] and “Calcul linéaire” [5]. In that respect, we
would like to recall the origin of the last book. From 1958, while Souriau is still
engineer at ONERA, his taste for the teaching drives him on to create a free
course (and free of charge) entitled “New methods of the Mathematical physics”,
as shown in this poster. It is a great success, the amphitheater, even able to
contain 200 persons, is full and he has to teach twice. The program of the linear
algebra gave rise to the book “Calcul Linéaire”. “Géométrie et Relativité” and
“Calcul linéaire”, which too are very rich and original, deserve —as “Structure
des systèmes dynamiques”— to be read, . . . and read again.
References
1. de Saxcé, G., Marle, C.-M.: Presentation of Jean-Marie Souriau’s book “Structure
des systèmes dynamiques’ ”, to appear in Mathematics and Mechanics of Complex
Systems
2. Kostant, B.: Quantization and Unitary Representations, Part 1. Prequantization.
LNM, vol. 170, pp. 87–208 (1970)
3. Lagrange, J.-L.: Mécanique analytique. Première édition chez la veuve Desaint,
Paris (1808.) Réimprimé par Jacques Gabay, Paris, 1989. Deuxième édition par
Mme veuve Courcier, Paris, 1811. Réimprimé par Albert Blanchard, Paris. Qua-
trième édition (la plus complète) en deux volumes, avec des notes par M. Poinsot,
M. Lejeune-Dirichlet, J. Bertrand, G. Darboux, M. Puiseux, J. A. Serret, O. Bon-
net, A. Bravais, dans Œuvres de Lagrange, volumes XI et XII, Gauthier-Villars,
Pari (1888)
4. Souriau, J.-M.: Géométrie et relativité. Hermann, collection enseignement des sci-
ences, Paris. Réimprimé par les éditions Jacques Gabay, Paris (1964)
5. Souriau, J.-M.: Calcul linéaire. Presses universitaires de France, collection Euclide,
Paris, tome I (1964), et tome II (1965). Réimprimé en un seul volume par les éditions
Jacques Gabay, Paris
6. Souriau, J.-M.: Structure des systèmes dynamique. Dunod, collection Dunod Uni-
versité, Paris. Réimprimé par les éditions Jacques Gabay, Paris (1970)
7. Souriau, J.-M.: Structure of dynamical systems. A Symplectic View of Physics
Translated by C.H. Cushman-de Vries. Translation Editors R.H. Cushman and
G.M. Tuynman. Progress in Mathematics, vol. 149, Birkhäuser, Boston (1997)
8. Site oﬃciel de Jean-Marie Souriau. http://www.jmsouriau.com
9. Hommage à Jean-Marie Souriau, La Gazette des mathématiciens, 133, juillet 2012

Jean-Marie Souriau’s Symplectic Model
of Statistical Physics: Seminal Papers on Lie
Groups Thermodynamics - Quod Erat
Demonstrandum
Frédéric Barbaresco(B)
Thales Land and Air Systems, Voie Pierre-Gilles de Gennes, 91470 Limours, France
frederic.barbaresco@thalesgroup.com
Abstract. The objective of this chapter is to make better known Jean-Marie
Souriau works, more particularly his symplectic model of statistical physics, called
“Lie groups thermodynamics”. This model was initially described in chapter IV
“Statistical Mechanics” of his book “Structure of dynamical systems” published
in 1969. We have translated in English some parts of three Souriau’s publica-
tions which provide more details about this geometric model of Thermodynamics.
Entropy acquires a geometric foundation as a function parameterized by mean of
moment map in dual Lie algebra, and in term of foliations. Souriau established the
generalized Gibbs laws when the manifold has a symplectic form and a connected
Lie group G operates on this manifold by symplectomorphisms. Souriau Entropy
is invariant under the action of the group acting on the homogeneous symplectic
manifold. As quoted by Souriau, these equations are universal and could be also
of great interest in Mathematics.
Keywords: Symplectic geometry · Statistical physics · Moment map ·
Thermodynamics · Lie groups · Representation theory
1
Preamble
«Il est évident que l’on ne peut déﬁnir de valeurs moyennes que sur des objets
appartenant à un espace vectoriel (ou afﬁne); donc - si bourbakiste que puisse sem-
bler cette afﬁrmation - que l’on n’observera et ne mesurera de valeurs moyennes
que sur des grandeurs appartenant à un ensemble possédant physiquement une
structure afﬁne. Il est clair que cette structure est nécessairement unique - sinon
les valeurs moyennes ne seraient pas bien déﬁnies.» - Jean-Marie Souriau
Jean-Marie Souriau has introduced his model of “Lie Groups Thermodynamics” in
the chapter IV “Statistical Mechanics” of his book “Structure of Dynamical Systems”
published in 1969 [1, 2]. This chapter IV remained little known for a long time, because
the ﬁrst readers of the work were more interested in the symplectic model introduced
for classical and quantum mechanics, than its extension for Statistical Physics.
This chapter is a translation of some part from three following Jean-Marie Souriau
papers where this Lie Groups Thermodynamics is developed:
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2021
F. Barbaresco and F. Nielsen (Eds.): SPIGL 2020, PROMS 361, pp. 12–50, 2021.
https://doi.org/10.1007/978-3-030-77957-3_2

Jean-Marie Souriau’s Symplectic Model of Statistical Physics
13
• Mécanique statistique, groupes de Lie et cosmologie [3]
• Géométrie Symplectique et Physique Mathématique [4]
• Mécanique Classique et Géométrie Symplectique [5]
These papers by Jean-Marie Souriau are little known because they were only pub-
lished in French in CNRS publications. In these papers, Souriau consolidated the bedrock
of what he called “Lie Groups Thermodynamics”. This model is of great importance in
the context of the geometrization of information sciences and geometric theory of heat.
Souriau built a thermodynamics in which the density of Gibbs is completely covariant.
He introduced there a new Riemannian metric invariant under the action of the group,
from the “moment map” (geometrization of Emmy Noether Theorem [17, 19]) and a
cocycle which translates the lack of equivariance of the coadjoint operator on the moment
map and linked to the cohomology of Lie algebra. We have discovered that this metric
was in fact a generalization of the Koszul-Fisher metric in Information Geometry. More
recently, we have observed [20–25] that the invariance of the Entropy deﬁned on the
coadjoint orbits made it possible to identify the Entropy with a generalized Casimir func-
tion in coadjoint representation. This deﬁnition of Entropy only depends on symmetries,
and is an alternative to Shannon and von Neumann approaches who had only given an
axiomatic deﬁnition. To allow access to a larger number, we have made a translation of
these three Souriau’s seminal papers, which retains all of the author’s original notations.
We have only translated parts concerning the symplectic model of statistical Mechanics
and his model of Lie Groups Thermodynamics.
From Lagrange’s in Analytic Mechanics [18], Jean-Marie Souriau, based on pre-
vious works of F. Gallisot [15] and A. Blanc-Lapierre [16], conceived his Symplectic
model of Statistical Physics [1–5] and other geometric models in Physics [6–12]. This
model was studied by his PhD student P. Iglesias [13, 14], and rediscovered by G.
de Saxcé [31–34], C.M. Marle [25–29] and F. Barbaresco [20–24, 45–47]. Symplec-
tic model used by Souriau, especially the non-null cohomology case, was deepened by
Jean-Louis Koszul in [30]. New fruitful results have been established [20–24] making
links with Maximum Entropy Theory studied by D. Dacunha-Castelle and F. Gamboa
[36], Information Geometry applied in Physics by R. Balian [37–41] and the Theory of
Integrable Systems synthetized by P. Cartier in [35]. Main objective of Souriau’s project
in Physics was summarized in his sentence “There is nothing more in physical theories
than symmetry groups except the mathematical construction which allows precisely to
show that there is nothing more” [Il n’y a rien de plus dans les théories physiques que
les groupes de symétrie si ce n’est la construction mathématique qui permet précisément
de montrer qu’il n’y a rien de plus]. In [43], Jean-Marie Souriau summarized his con-
tribution to thermodynamics: “The second principle of thermodynamics is independent:
it indicates that the entropy S increases during a dissipation; here we mean entropy
in the sense of Clausius-Boltzmann, which is a function of the statistical state ρ. If
therefore a state possesses, for a given mean value of the moment, greatest entropy, it
will not be subject to dissipation. These states, if they exist, thus represent the terminal
state of dissipation. They are indexed by a parameter βwith values in the Lie algebra
of the Lorentz-Poincaré group; they generalize the Gibbs equilibrium states, βplaying
the role of temperature. The invariance with respect to the group, and the fact that the
entropy S is a convex function of β, imposes very strict, universal conditions – i.e.,

14
F. Barbaresco
independent of the system considered. For a large class of systems, for example, there
exist necessarily a critical temperature beyond which no equilibrium can exist. In the
cases where an equilibrium exists, it generally consists of a rigid rotation about the
barycenter, etc. These purely theoretical results are evidently conﬁrmed by numerous
astronomical examples: the Earth and the starts rotating about themselves; dissipative
evolution imposes a solid rotation on the central regions of the galaxies, which itself can
lead to a gravitational instability of the “quasar” type; the Clapeyron relations extend
to the geometrical-dynamical quantities, etc. One can, if one wishes, interpret βas a
space-time vector (the temperature vector of Planck), giving to the metric tensor g a
null Lie derivative. This suggests describing the dissipative processes by a temperature
vector βwhich is no longer compelled by this condition; the corresponding Lie deriva-
tive of g, the “friction tensor,” becomes the source of the dissipation. One obtains in this
way a phenomenological model of continuous media which presents some interesting
properties: the temperature vector and entropy ﬂux are in duality; the positive entropy
production is a consequence of Einstein’s equations; the Onsager reciprocity relations
are generalized; in the case of a ﬂuid in the non-relativistic approximation, the model
uniﬁes heat conduction and viscosity (equations of Fourier and Navier).”
2
Jean-Marie Souriau Biography
The name Souriau means “mouse” in “the Perche”. In the “Vendomois”, the Souriau
from 1490 to 1819 were all “Master Plowmen” or “Master Millers”. Jean-Marie Souriau
was born June 3, 1922 in Paris in the 6th arrondissement. Jean-Marie Souriau comes
from a family of Philosopher all graduated from ENS Paris. His father, Michel Souriau
joined the ENS Paris in 1910 and obtained the aggregation in philosophy in 1914 and
wrote in 1938 an article on “Introduction to mathematical symbolism” before being
mobilized as a battalion commander. His uncle, Etienne Souriau, is a French philosopher,
specialized in aesthetics, entered the ENS Paris in 1912, received ﬁrst in the aggregation
in philosophy in 1920. In 1958, Etienne Souriau was elected member of the Academy of
moral sciences and policies by a committee in which Charles de Gaulle appears, and will
be the director of the thesis of the ﬁlmmaker Éric Rohmer. Etienne Souriau published a
book on “Structure of the Work of Art (Structure de l’Oeuvre d’Art)”. His grandfather,
Paul Souriau, is a French philosopher known for his work on the theory of invention and
aesthetics, who entered the ENS Paris in 1873 and aggregated in philosophy in 1876.
It can be noted that his grand-father, Paul Souriau, composed a thesis titled “Theory
of Invention” (théorie de l’invention), published in 1881, and also a Latin thesis titled
“De motus perceptione”, which aimed to determine the importance of vision for the
perception of movements (the initial thesis title was “De visione motus” and was a
precursor to his future work on the perception of movement). We can assume that Jean-
Marie Souriau read his grandfather’s thesis and was inﬂuenced by it for his own work.
In 1889, Paul Souriau published a book on “The Aesthetics of Movement (L’Esthétique
du mouvement)” which describes two levels of aesthetics for movement: mechanical
beauty (the adaptation of movement to fulﬁll its purpose) and meaning of movement
(the meaning that the movement communicates to an outside observer). In doing so, Paul
Souriau distinguished movement from perception of movement, concepts that would

Jean-Marie Souriau’s Symplectic Model of Statistical Physics
15
later become the subject of motor cognition and psychophysics. It is interesting to note
that Etienne Souriau studied the structures of aestheticism, Paul Souriau developed the
aestheticism of the movement and Jean-Marie Souriau founded the structures of the
movement. This triptych will remain an important element of French philosophy at the
hinge of this 1900 Spirit (Esprit 1900).
Fig. 1. Jean-Marie Souriau, a student at the Ecole Normale Supérieure in Paris in 1942, with
Jacques Dixmier and René Deheuvels among others
Jean-Marie Souriau from 1932 to 1942 did his secondary studies in Nancy, Nîmes,
Grenoble and Versailles. Jean-Marie Souriau married Christianne Hoebrechts, who died
prematurely in 1985 and with whom he had ﬁve children Isabelle, Catherine, Yann,
Jérôme and Magali. He entered the ENS Paris in 1942, passing twice in the unoccupied
zone in Lyon and a second time in Paris. Also received at the Ecole Polytechnique,
he resigned to join the ENS Paris. During his studies at the ENS, he took courses
at the Sorbonne from the physicist Yves Rocard and the mathematician Elie Cartan.

16
F. Barbaresco
He volunteered for “La France Libre” in 1944. On his return in 1946, he passed the
mathematics aggregation, and the same year joined a laboratory working on the scanning
electron microscope and then entered as a researcher in a “theoretical physics” session
at the CNRS (Fig. 1).
He ﬁnally opted for a career as an aeronautical engineer at ONERA by becoming head
of research groups and defending his thesis in June 1952 on the theme of “aircraft stability
“which was supervised by André Lichnerowicz (professor at the College of France) and
Joseph Pérès (collaborator of Vito Volterra), which was useful for the design of the
“Caravelle” and “Concorde” aircrafts (ONERA obtains royalties from Souriau patents).
In this thesis, he refers to the book by Yves Rocard on “General dynamics of vibrations”.
On his thesis, he wrote [43] “I studied the problems of vibrations and stability which arise
in aeronautics and in some other techniques; this work allowed me to develop stability
criteria which are presented in the form of algorithms which can be easily calculated
from theoretical data or from tests; they have since been regularly used in various ﬁelds
(subsonic and supersonic airplanes, navigation instruments, etc.)”. I obtained a copy of
this thesis through colleagues at ONERA, whose cover I am reproducing below (Fig. 2).
Fig. 2. Cover page of the thesis of Jean-Marie Souriau “On the stability of planes” defended on
June 20, 1952, as well as the bibliographic page which refers to the work of Yves Rocard.
During this period, he also invented in 1948 an algorithm named Le Verrier-Souriau
algorithm [44] which allows to compute the characteristic polynomial of a matrix and
which was used on the ﬁrst IBM computers in the United States. From 1948 to 1952, he
also provided continuing education at the Special School of Aeronautical Works (ESTA,
Paris) under the general title “New Methods of Mathematical Physics”. From 1951 to

Jean-Marie Souriau’s Symplectic Model of Statistical Physics
17
1952, he created and ran the Mechanics course in the third year of the École Normale
Supérieure de l’Enseignement Technique (ÉNSET, Paris). From 1952, he also had a
university education in the following disciplines: Mathematics, Mechanics, Relativity,
Mathematical Methods of Physics and Computer Science.
After his thesis in 1954, he joined the “Institut des Hautes Etudes”, rue de Rome in
Tunis, and moved with his wife to Carthage. It is during this period that he rereads and
deepens the work of Lagrange in Analytical Mechanics and discovers the symplectic
structures that he will formalize in his book “Structure of dynamic systems”. It is by
thinking of his discussions with ONERA engineers that he invents his masterpiece, the
“moment map” (application moment). We can read in the interview by Patrick Iglesias
[14] “It was with the memory of discussions with engineers who asked themselves the
following question: what is essential in mechanics. I remember very well an engineer
who asked me: is mechanics simply the principle of conservation of energy? This is ﬁne
for a one-parameter system, but once there are two, it is not enough. I had learned of
course the Lagrange equations and all the analytical principles of mechanics, but it was
all a cookbook; we did not see any real principles”. He remained in Tunis from 1952 to
1958, as Lecturer, then as Full Professor at the Institut des Hautes Études. In 1953, he
participated in Strasbourg in the conference on Differential Geometry (Fig. 3).
Fig. 3. Jean-Marie Souriau at the Conference on “Differential Geometry” in Strasbourg in 1953.
In the same picture, Jean-Louis Koszul, André Weil, Shiing-Shen Chern, Georges de Rham,
Charles Ehresmann, Lucien Godeaux, Heinz Hopf, André Lichnerowicz (the director of the thesis
of Jean-Marie Souriau), Bernard Malgrange, John Milnor, Georges Reeb, Laurent Schwartz, René
Thom, Paulette Libermann.
In 1958, he became Professor at the University of Aix-Marseille. He remained in
Marseille throughout his career, and from 1978 to 1985 became Director of the Center for
Theoretical Physics of Marseille (CNRS laboratory) in charge of the teams in Theoretical

18
F. Barbaresco
Mechanics, Geometry and Quantiﬁcation, Astronomy and Cosmology. He was also
professor of Mathematics at the University of Provence (Aix-Marseille I) and ended up
as an exceptional Professor with second echelon. He was also a member of the “Société
Mathématique de France” and of the French Society of Specialists in Astronomy. For
ﬁve years, he also taught the course of the third interuniversity cycle of Pure Mathematics
in Marseille and the third interuniversity cycle of Theoretical Physics in Marseille-Nice.
He was a member of the Editorial Board of the Journal of Geometry and Physics in
Florence. He organized two International Colloquiums of the CNRS in 1968 and 1981
and Days of the “Société Mathématique de France”. Honored by the Academic Palms
and of the National Order of Merit, he obtained the Prize on the subject “Vibrations “put
up for competition by the Association for Aeronautical Research in 1952, the Prize on
the subject “Cosmology” put out for competition by the Foundation Louis Jacot in 1978,
the Grand Prix Jaffe of the French Academy of Sciences in 1981 and the Great Scientist
prize of the City of Paris in 1986. Jean-Marie Souriau died in 2012 in its 90th year.
3
1st Souriau Paper: “Statistical Mechanics, Lie Group
and Cosmology - 1st Part: Symplectic Model of Statistical
Mechanics”
The classical notion of Gibbs’ canonical ensemble is extended to the case of a sym-
plectic manifold on which a Lie group has a symplectic action (“dynamic group”). The
rigorous deﬁnition given here makes it possible to extend a certain number of classical
thermodynamic properties (temperature is here an element of the Lie algebra of the
group, heat an element of its dual), notably inequalities of convexity. In the case of
non-commutative groups, particular properties appear: the symmetry is spontaneously
broken, certain relations of cohomological type are veriﬁed in the Lie algebra of the
group. Various applications are considered (rotating bodies, covariant or relativistic sta-
tistical Mechanics). [These results specify and complement a study published in an
earlier work [8], which will be designated by the initials SSD].
3.1
Distribution Functions
The initial concept of classical statistical mechanics is the distribution function; it is a
real function ρ, deﬁned on the phase space of a dynamic system, such as the integral

D
ρdp1 . . . dpndq1 . . . dqn
(1)
is equal to the probability that the point (p1, . . . , pn, q1, . . . , qn) representing the state
of the system at time t is contained in a domain D. This obviously requires that ρ is ≥0
and that the integral (1), extended to the entire phase space, be equal to 1.
It is further assumed that ρ veriﬁes Liouville equation
∂ρ
∂t +

j
∂ρ
∂qj
∂h
∂pj
−∂ρ
∂pj
∂h
∂qj
= 0
(2)
h being hamiltonian function.

Jean-Marie Souriau’s Symplectic Model of Statistical Physics
19
3.2
Statistical States
It is convenient to give a geometric interpretation; let us consider (Fig. 4) the space of
evolution of the system V2n+1, manifold parameterized by the variables p1, . . . , qn, t; it
projects onto the space of movements U2n, a manifold of which each point is a movement
(curve drawn in V, solution of Hamilton’s equations), and whose spaces of successive
phases (section t = Cte of V) constitute an atlas. U has a symplectic structure such that
all of these charts are canonical. We will note σ the corresponding 2-form.
Then Liouville’s Eq. (2) simply expresses that ρ is a function deﬁned on U; the
integral (1) is written:

D′
ρvol
(3)
where D
′ denotes an open set of U and vol the Liouville density U.
Thus, a distribution function is interpreted by a probability law (positive measure
of mass 1) of the manifold of movements, probability law which we will call statistical
state (the statistical states deﬁned by a continuous function ρ are completely continuous;
we can consider each classical movement as a statistical state x by identifying it with
the Dirac measure at the point x.
Fig. 4. Phase space and movement space
3.3
Image of Measures
We know how to deﬁne the image a(μ) of a measure μ by an homeomorphism a; it is
characterized by the formula
a(μ)(f ) = μ(f ◦a)
(4)
where f denotes a test function with compact support; the image of a probability law is
a probability law.

20
F. Barbaresco
The image of Liouville’s measure of U by a symplectomorphism a is still a Liou-
ville’s measure; it follows that one obtains the distribution function of the image of
measure (3) by a sympectomorphism a by composing (x →ρ) with a−1. If the dynamic
systemconsideredisconservative,thegroupoftranslationsintimeactsonU bysymplec-
tomorphism (a translation τ makes correspond to a movement x the movement having
the same trajectory, but whose time law is delayed by τ); a statistical state is said steady
state if it is invariant under time translations (the same as a steady state “in the classic
sense” is a movement invariant by temporal translations).
3.4
Tensorial Products of Measure
If μ1 and μ2 are measures of two manifolds V1 and V2 the tensor product μ1 ⊗μ2 is
the measure deﬁned on the product manifold V1 × V2 by the formula
(μ1 ⊗μ2)(f ) = μ1(x1 →μ2(x2 →f (x1, x2)))
(5)
where f denotes a test function with compact support on V1 × V2.
If V1 and V2 are symplectic manifolds, V1 ×V2 is canonically symplectic (by adding
the 2-forms σ1 and σ2 on V1 and V2); Liouville measure on V1 × V2 coincides with
the tensor product of those of V1 and V2; so if two statistical statements of V1 and V2
are deﬁned by distribution functions x1 →ρ1, x2 →ρ2, their tensor product is the
statistical state of V1 × V2 deﬁned by the distribution function (x1, x2) →ρ1 × ρ2.
The tensor product corresponds to the notion of independent random variables; thus,
in the case of a gas composed of N identical molecules whose interactions are neglected,
the hypothesis of “molecular chaos” consists in supposing that the statistical state of the
system is the tensor power Nth of a statistical state with a single molecule.
3.5
Entropy
In a statistical state deﬁned using a distribution function ρ, the mean value s of the
function Log(1/ρ) is called with Boltzmann, Entropy:
s = −

U
ρLog ρ vol
(6)
This assumes that the function −ρ Log(ρ) is summable for the Liouville measure: of
course it is extended by 0 out of domains where ρ cancels).
The results of chapters 3.3 and 3.4 show that the entropy of a statistical state is
invariant through the action of any symplectomorphism; that the entropy of the tensor
product of two statistical states is the sum of the two entropies.
3.6
Canonical Gibbs Ensemble
Experience shows (in the case of an ideal gas, it is equivalent to assume at each point
the distribution of the Maxwell velocities, which has a direct spectroscopic veriﬁcation
“line width”. All the thermodynamic formulas that we are going to establish constitute

Jean-Marie Souriau’s Symplectic Model of Statistical Physics
21
indirect veriﬁcation) that thermodynamic equilibria achieved in nature are characterized
by the following property: the logarithm of the distribution function is an afﬁne function
of energy E; there are then two numbers z and  such that:
ρ = e−[z+E]
(7)
by writing that the mass of the measurement is equal to 1, it obviously comes
z = log

U
e−Evol
(8)
Consequently these states (which constitute the “Canonical Gibbs Ensemble”) are only
indexed by the number . In the case of a system composed of two independent parts, the
total energy E is the sum of the energies E1 and E2; consequently any Gibbs state of the
compound system is the tensor product of two Gibbs states of the component systems (the
Gibbs states of a perfect gas therefore verify the molecular chaos hypothesis) giving the
same value to the parameter . As experience shows that two systems are in equilibrium
when they are at the same temperature; a detailed study of the perfect gas thermometer
actually shows that  is related to absolute temperature T by the relation
 = 1
T
(9)
if we choose a temperature unit such that the Boltzmann constant is equal to 1 (1° K =
1.3840 × 10−16 erg).
The average value of energy, namely the integral
Q =

U
e−[z+E]E vol
(9 bis)
is interpreted as the heat of the system (because we will study here only evolutions of
isolated systems, therefore without work exchange); z is called the Planck thermody-
namic potential; it exists between variables , z, Q, s relationships that we will build in
a broader context.
3.7
Gibbs Ensemble of a Dynamic Group
Let U be a separate symplectic manifold having a dynamic group G; which means that
G is a Lie group which acts by differentiation on U by symplectomorphisms; we will
note aU the transformation of U associated with an element a of G; a →aU is therefore
a group morphism from the group G in the group of symplectomorphisms of U; if this
morphism is injective, we say that it acts effectively.
If Z is an element of the Lie algebra g of G, one can associate a vector ﬁeld Z deﬁned
on U, which characterizes the inﬁnitesimal action of the group (can be deﬁned by the
formula ∂
∂t

exp(tZ)U(x)

= ZU(exp(tZ)U(x))).

22
F. Barbaresco
Under fairly general hypotheses (see SSD) the dynamic group has a moment: it is
a differentiable map x →E from the manifold U in the dual g∗of the Lie algebra g,
characterized (except for an additive constant) by the formula
σ(δx, ZU(x)) = [δE](Z)
(10)
where σ denotes the symplectic form of U, δx an arbitrary variation of the point x (δE
is the corresponding variation of E).
In the case of a conservative dynamic system, the energy is - to the nearest sign
- the moment of the dynamic group of translations in time (acting on the symplectic
movements manifold of the system) (more generally in the case of dynamic systems the
components of the moment are the Noetherian quantities attached to symmetry).
We will therefore generalize the thermodynamic deﬁnitions of Chapter 6 by the
following statement:

Let G a dynamic group of a separate symplectic manifold U, having the moment
E. We will call (generalized) temperature any element 0 of the Lie algebra g of
G as the integral
I0() =

U
e⟨E,⟩vol
converges normally in a neighborhood of 0(⟨E, ⟩note the duality of g∗and
g; vol denotes the density of Liouville of U).
(11)
The set of generalized temperatures is obviously an open set 	 of g; we have the
theorem (see appendix):

The function I0 is of C∞class in 	 ; its nth derivative is, for all  , the tensorial
integral
In =

U
e⟨E,⟩E⊗nvol
This integral is locally normally convergent.
(12)
With each (generalized) temperature , we can associate the probability law on U
whose distribution function is
e⟨E,⟩−z
(13)
z being a constant; by writing that this probability law has a mass equal to 1, we ﬁnd as
above:
z = log(I0) = log

U
e⟨E,⟩vol
(14)
The set of these probability laws, indexed by , will be called Gibbs Ensemble (of the
dynamic group); z, C∞function on 	, will be called thermodynamic potential; we will
call (generalized) heat the integral
Q =

U
e⟨E,⟩−zE vol
(15)

Jean-Marie Souriau’s Symplectic Model of Statistical Physics
23
Average moment value E in Gibbs state; we note that:
Q = I1
I0
(16)
It therefore follows from theorem (12) that this integral is convergent, and that
Q = dz
d
(17)
Heat is therefore a C∞function of temperature , with value in the dual of Lie algebra.
It follows from (17) that the derivative dQ
d is a symmetric second-order tensor; we
calculate it by deriving (16)
dQ
d = I2
I0
−I1 ⊗I1
I2
0
= I2
I0
−Q ⊗Q
(18)
what can still be written
dQ
d =

U
e⟨E,θ⟩−z[E −Q] ⊗[E −Q]vol
(19)
This shows that the quadratic form associated with the tensor dQ
d is positive; it is even
positive deﬁnite for all x unless there is a non-zero element Z of the Lie algebra such as
⟨E −Q, Z⟩= 0
(20)
This expresses that the moment E varies in an afﬁne sub-variety of g∗and shows, by
deriving, by applying (10), and by remembering that the symplectic form σ is injective
ZU(x) = 0
(21)
The deﬁnition of ZU(x) shows that the 1-parameter subgroup of the exp(tZ) acts trivially
on U; hence the theorem

If G acts effectively on U, the symmetric tensor dU
d is positive deﬁnite for all
 ∈	n
(22)
(in the case of classical thermodynamics, this expresses that the heat capacity dϒ
dT is
always positive).
The tensor dQ
d is the second (or Hessian) derivative of the thermodynamic potential
(see (17)); hence the corollary:

The thermodynamic potential z is a convex function on the open convex set 	
(23)
(the convexity of 	 easily results from the inequality ρλ
0ρ1−λ
1
≤sup(ρ0, ρ1) where ρ0
and ρ1 are positive, λ ∈[0, 1]. This same inequality shows if 	 is not empty, that any
point where the inequality converges, I0 is adherent to 	).

24
F. Barbaresco
The graph of the convex map  →z is located above its tangent hyperplanes; hence
the inequality:
z0 + ⟨Q0, 1 −0⟩≤z1
(24)
which turns into strict inequality if 1 ̸= 0 and if G operates effectively, due to (22); it
follows that Q1 ̸= Q0, therefore that the application  →Q is injective; as its derivative
dQ
d is invertible according to (22), we have the theorem

If G operates effectively on U, the map  →Q is a diffeomorphism of 	 on
an open set 	∗⊂g∗
(25)
We can then apply Legendre transformation to the map  →z; it consists in
associating with it the function - obviously of C∞class
Q →z −
 ∂z
∂, 
	
(26)
We now verify that
z −
 ∂z
∂, 
	
=

U
e⟨E,⟩−z[z −⟨E, ⟩]vol
(27)
the integral which appears in the second member is very equal to the entropy s (formula
(6)).
So, if G operates effectively on U, the Legendre transform of the map  →z is the
map
Q →s
(28)
s being the entropy (27), which is C∞on the open set 	∗.
We immediately check the reciprocal formulas:
dz
Q
d
=
Θ
ds
dQ = −Θ
z
s
z
∂
=
−
Θ
∂Θ
, ds
z
s
Q dQ
=
−
(29)
The formula ds
dQ = − includes an abuse of notation (it identiﬁes g with its bidual);
it shows that the Hessian of Q →s is −d
dQ , therefore, except for the sign, the inverse of
that of Q →s; it is therefore deﬁned as negative; which shows that Q →s is a concave
function.
All these formulas apply to classical statistical mechanics; in particular the formula
ds
dQ = −, which is written
ds = dϒ
T
(30)

Jean-Marie Souriau’s Symplectic Model of Statistical Physics
25
ϒ being the classical heat, shows the equivalence of the Boltzmann deﬁnition of entropy
with its thermodynamic deﬁnition; it is easy to deduce from it the fundamental formulas
of thermodynamics of equilibria (ideal gases, speciﬁc heats, Mayer’s formula, Maxwell,
Gibbs-Helmholtz, Clapeyron-Clausius, etc. See SSD).
3.8
Broken Symmetries
In the case of classical thermodynamics, Gibbs states are states of equilibrium: the group
G (here the temporal translations) leaves them all invariant.
This is no longer true in the general case: it is said that the symmetry is broken.
Let μ be a Gibbs state, to an element of G. The image of μ by aU (in the sense
of chapter 3) is still a probability law from which we obtain the distribution law by
composing x →e⟨E,⟩−z with a−1
U ; the entropy does not change (chapter 5).
To calculate aU(μ), we must therefore calculate ﬁrst ψ ◦a−
−1, ψ being the map
x →E. This is given by the following formulas (see SSD), obtained assuming convex
U:
There is map 
*
(
)
G
ϕ
→g
such that
(
)
(
)
( )
*
( )
( )
  
,
U
a
x
a
x
a
a
G
x
U
ψ
ψ
ϕ
=
+
∀∈
∀∈
g
*
a
ag is the dual representation of the adjoint representation a
ag  : 
where 
(31)
*
1
( )
a
E
E a−
=
g
g
(32)
ϕ verifies identity
(
)
( )
(
)
*
( )
  
,
a b
a
a
b
a b
G
ϕ
ϕ
ϕ
×
=
+
∀
∈
g
(33)
which expresses that ϕ is 
*
g –cocycle of group G , whence results 
(
)
(
)
*
1
1
( )
0 (  = neutral element of ),
( )   
,
e
e
G
a
a
a
a
G
x
U
ϕ
ϕ
ϕ
−
−
=
= −
∀∈
∀∈
g
(34)
We immediately get the distribution function from that aU(μ) which is written
e⟨E,∗⟩−z∗, with
z∗= z −

ϕ

a−1
, 

,
∗= ag()
(35)

26
F. Barbaresco
Note that
z∗= z +

ϕ(a), ∗
(35 bis)
These formulas show how symmetry can be broken; they also show that the set 	
of generalized temperatures is invariant by the adjoint action of G and that
aU(μ) = μag()
(36)
They show ﬁnally that the map  →z which deﬁnes the thermodynamic potential
is invariant by the substitution of the variables ∗, z∗given in (35).
Apply the formula (17)
δz −⟨Q, δ⟩= 0
(37)
To a variation δ tangent to the orbit, therefore generated by an element Z of the
Lie algebra; the derivation of (35) gives
f ()(Z) + ⟨Q, [, Z]⟩= 0
(38)
[, Z] designating a Lie bracket, f the derivative of the map ϕ at point e: we know
(SSD) that f is a 2-form of g and that it veriﬁes the identity:
f (Z)

Z′, Z′′
+ f

Z′
Z′′, Z

+ f

Z′′
Z, Z′
= 0
(39)
We will write (38) in the form
 ∈ker(f)
(40)
by posing
f(Z)

Z′
= f (Z)

Z′
+

Q,

Z′Z′
(41)
Apply also the deﬁnition (4) of the image from one measure to the function f = ψ;
we ﬁnd that the heat Q∗in the state μ∗is equal to the mean value in the state μ of
the variable ψ

aU(x)

; either, thanks to (31):
Q∗= ag∗(Q) + ϕ(a)
(42)
As before, by derivating tangentially to the orbit with respect to Z ∈g, and using
the positivity of dQ
d, we ﬁnd:
⟨Q, [Z, [, Z]]⟩+ f (Z)([, Z]) ≥0
(43)
What is still written with the notation (41)
f(Z)([, Z]) ≥0
(44)

Jean-Marie Souriau’s Symplectic Model of Statistical Physics
27
The inequality being strict if [, Z] ̸= 0 and if G operates effectively; this inequality
can be interpreted by checking (thanks to (40)) that:

f is a 2 −form of g which veriﬁes (39);
there exists asymmetric tensor g, deﬁned on the set of value of ad() by the
formula
g([, Z])

, Z
′
= f(Z)

, Z
′
∀Z, Z
′ ∈g
(45)
then (44) expresses that the quadratic form associated with g is positive, and deﬁned
positive if G operates effectively.
3.9
Thermodynamic Applications
Apart from their possible mathematical interest, it turns out that the previous results
apply in thermodynamics.
Gibbs’ law itself is not an acceptable formulation of thermodynamics, because it
violates the principle of Galilean relativity: a state of equilibrium obviously ceases to be
one under the action of a Galileo transformation.
A tempting idea a priori consists in supposing that the states of equilibrium are Gibbs
states associated with the Galileo group itself; we know that the space of movements of
a free dynamic system has this Galileo group as a dynamic group.
But the set of such Gibbs states is empty: indeed, the relation (38), which should be
veriﬁed ∀ ∈	, implies a condition for ; however this relation is not veriﬁed in any
open set.
[The class of symplectic cohomology (see SSD) of the system is indexed by a real
m which is interpreted as the mass of the system and which provides the value of the
2-form f which appears in (38); assuming the non-zero mass, we draw from (40) the
relation ⟨⃗ω, ⃗β⟩= 0 where ⃗ω is the inﬁnitesimal rotation, ⃗β the inﬁnitesimal Galileo
transformation in .]
Theinterpretationof this result is of acosmological type; it is easytosee, for example,
that a gas cannot reach equilibrium if it is free in an unlimited space; you have to put it in
a box. But the existence of this box breaks the symmetry: we can only reach equilibrium
for a non-free system, of which an appropriate mechanism will limit the symmetry to a
subgroup of the Galileo group.

We are led to give a priori an element  of the Lie algebra of the Galileo group,
and to seek if  can be the generalized temperature of a dynamic system whose
symmetry includes in particular the group with a parameter generated by .
To interpret , consider its inﬁnitesimal action on space-time: we will call
temperature-vector ε the quadri-vector
ε(X ), X =
⃗r
t

being an arbitrary point in space - time.
(46)
This procedure succeeds if the time component of the temperature vector (which
is independent of X ) is positive: it is then interpreted as the inverse of the (classic)
equilibrium temperature.

28
F. Barbaresco
The temperature vector further deﬁnes a velocity ﬁeld: that of a medium whose
universe lines would be the space-time orbits of the group at a parameter generated by
. It turns out that the movement of this medium is solid, but that it is not generally a
movement of uniform rectilinear translation; the boxes likely to contain thermodynamic
equilibria can be accelerated, but only according to certain rules; we ﬁnd for example the
uniformly accelerated box, the box in uniform rotation (centrifuge). In these different
cases, we can completely interpret the associated Gibbs states: thus, in the case of a
centrifuge, we ﬁnd the formula giving the variation in concentration of the components
of a non-homogeneous gas - the very one that explains the use of centrifuges for uranium
enrichment (see SSD).
We can however temper the veto opposite to the equilibrium of free systems thanks to
a particular property of the Galileo group G: it has a connected invariant abelian subgroup
G0 on the Lie algebra of which the 2- form f is invertible [G0 is generated by spatial
translations and Galileo transformations]; We deduce that the space of movements of
a free dynamic system of non-zero mass is a direct symplectic product (see SSD): the
product of a variety of dimension 6 (that of the movements of the barycenter considered
as a free material point endowed with all mass) by the space of movements around the
barycenter. As a result, the Galileo group acts doubly on the system; in fact, it involves
movementsaroundthebarycenterviathequotientG / G0,isomorphtoO(3)×R(partially
interpreted by physicists under the name of “spin group”).
We can therefore study the Gibbs equilibria of these movements; they are indexed by
an element  of the Lie algebra of O(3)×R; this generalized temperature is composed of
a conventional temperature T and a rotation speed ⃗ω; generalized heat Q uniﬁes classic
heat and angular momentum ⃗σ; the relation (38) shows that there exists a number I such
that ⃗σ = I ⃗ω; (44) shows that I is positive. These predictions are completely independent
of the choice of a model for the system; we know that they are effectively veriﬁed by
the “equilibria” of the isolated celestial bodies: with good precision, the Earth, like the
stars, has a uniform rotational movement around a main axis of inertia.
Other predictions are possible - for example those obtained by explaining the sym-
metry of the tensor dQ
d; we also know that the set 	, traversed by

⃗ω
T , 1
T

is open, convex
and invariant by the adjoint action of the group (which consists simply in rotating the
vector ⃗ω without changing T). It follows that for each value of T, the admissible values
of ⃗ω form an open ball (possibly zero radius or inﬁnity).
In addition, the convexity 	 shows that the radius of this ball ∥ω∥max is of the form
Tf (1 / T), f being concave; we simply deduce that g : T →Tf (1/T) is also concave;
and ∥⃗ω∥max is a concave function of the temperature.
It seems quite rare that this maximum speed of rotation increases with temperature;
Let us therefore assume that, for a given dynamic system and for at least a couple of
temperatures (T1, T2), ∥⃗ω∥max is decreasing (Fig. 5).
While the concavity of the function g shows that there will be a critical temperature
Tc above which the body will explode - for lack of a state of equilibrium - whatever the
rotation speed, even zero. This result, based on the only symplectic model of classical
mechanics, is perhaps an element of explanation of certain astrophysical phenomena
(solar wind, novae, supernovae?).

Jean-Marie Souriau’s Symplectic Model of Statistical Physics
29
Fig. 5. Critical temperature
3.10
Relativistic Thermodynamics
It is easy to extend the previous theory to special relativity: it sufﬁces to replace the
Galileo group by the Poincaré group. The temperature-vector then becomes a Killing
vector of the Minkowskian metric; condition (46) is easily transposed: the temperature-
vector must be of the future kind; at each point its direction deﬁnes the local equilibrium
reference frame; its length is the inverse of its own temperature; its meaning is the
“arrow of time”. The consideration of this temperature-vector gives a precise answer to
the problem - constantly debated since Planck’s work - of the spatio-temporal status of
temperature.
As in the Galilean case, we show that a free system does not have a Gibbs equilibrium:
for a system whose mass at rest is not zero, the relation (41), applied to the Lie algebra of
Poincaré group, shows that the inﬁnitesimal Lorentz transformation which is included
in  has a null pfafﬁan (it is the relativistic equivalent of the scalar product ⟨⃗ω, ⃗β⟩of
the Galilean case); this relationship cannot be veriﬁed in an open set 	.
As before, we will seek the equilibria of systems having a symmetry limited to a
subgroup of the Poincaré group; we still arrive at reasonable results in this way (see
SSD); we ﬁnd the perfect relativistic gas of Synge, whose compressibility law brings
into play Bessel functions; the description of the centrifuge remains possible, but there
appears a relativistic effect (the natural temperature is a little greater than on the edges
of the centrifuge than on the axis; this effect is noticeable only if the speed of the edge
does not is not negligible compared to that of light); etc.
3.11
What is a Thermodynamic Equilibrium?
In practical mechanics of continuous mediums - whether Galilean or relativistic - we
generally admit that there exists, at each point in space-time, a temperature and an
average speed of the medium, therefore a temperature-vector (we often admit also that

30
F. Barbaresco
the medium behaves locally as in an equilibrium, as regards for example the law of
distribution of the speeds of Maxwell, the laws of compressibility, etc.; only the thickness
of the shock waves is an exception).
But the temperature-vector is no longer required to be part of the Lie algebra of
the Galileo or Poincaré group; it is simply subject to phenomenological equations, like
Navier’s equations.
The success of this kind of compromise provides us with indications on what could
be a true thermodynamics of out of equilibrium systems - still to be built.
By adopting this point of view, the previous study suggests a deﬁnition of ther-
modynamic equilibria: we recognize that the temperature-vector is a Killing vector of
space-time (we now adopt the only relativistic point of view).
We will now broaden this point of view, using radio-astronomical observation.
In 1965, we discovered infrared radiation, called cosmological radiation, which has
the property of coinciding - to the precision of observations - with the thermal radiation
of an oven (of temperature 2.7 °K) in which the Earth would be motionless.
Such radiation deﬁnes not only a temperature, but also a local equilibrium reference
frame (if we communicated a speed of a few hundred km/s to the Earth, the Doppler
effect would break the isotropy and isothermia of the radiation) There is therefore at
each point X in the universe, a temperature-vector (X ).
The cosmological principle - if you prefer, the idea that the Earth and our time do
not occupy a privileged place in space-time - leads us to postulate that of all galaxies, at
all times, we observe likewise black body radiation.
How to reconcile this hypothesis with the redshift of distant galaxies? A precise
geometricstudyoftheDoppler-Einsteineffectresponsiblefortheredshift(seethesecond
part of this talk) shows that there is indeed compatibility, with the only condition that
the vector (X ) must be an inﬁnitesimal conformal transformation of the metric of
Universe, which can be written in the form
ˆ∂μv + ˆ∂νμ = λgμν
(47)
ˆ∂μ being covariant derivations, and v being components of temperature-vector.
It is remarkable that this result reveals a property of the temperature-vector itself (and
more precisely of the group with 1 parameter which it generates), and not for example
of the parallel vector whose length would be T (instead of 1 / T), sometimes proposed
as a basic thermodynamic quantity.
Furthermore, in the particular case λ = 0 (which would correspond to a zero expan-
sion rate of the universe), we recognize in (47) the Killing equation; the observation
of cosmological radiation therefore suggests a generalization of the previous notion of
equilibrium.
Interestingly enough, some properties of the equilibria are preserved: from Eq. (47)
alone, we can deduce the equations
ˆ∂μT μν = 0,
∂μSμ = 0
(48)
where the T μν are the components of the impulse-energy tensor of the radiation, the Sμ
the components of the entropy ﬂow vector.

Jean-Marie Souriau’s Symplectic Model of Statistical Physics
31
These conservation equations therefore express the exchange of impulse, energy and
entropy between matter and radiation are made with a zero balance: an essential property
of balance is saved, although the temperature is not constant (the redshift sign shows
that the universe is cooling). We refer to the second part of this talk for the cosmological
implications of these geometric properties.
3.12
Proof of the Theorem (12)
Let U a manifold, x →E be a continuous map of U in the dual g∗of a vector space
g of ﬁnite dimension: vol a continuous positive density on U. We will choose on g a
Euclidean norm; it follows from this, on g∗and more generally on the tensor spaces

g∗⊗n
, a norm of (multi) linear maps.
Assume that the integral
I0 =

U
e⟨E,⟩vol
(49)
depending on the variable ( ∈g) is normally convergent in a neighborhood of 0;
that is, there is a positive number ε and a continuous function ρ such that
(A) e⟨E,⟩≤ρ if ∥ −0∥< ε
(B)

U
ρ vol < ∞
(50)
∀α ∈R, ∀n ∈N, we have obviously

2α
n

n
≤
2sh
α
n

n
=
eα / n −e−α / nn =

n

p=0
[−1]pCp
ne[−1+2p/n]α

(51)
hence by choosing V ∈g and by setting α = ⟨E, V⟩,
2
n
n
e⟨E,⟩|⟨E, V⟩|n ≤

n

p=0
[−1]pCp
ne⟨E,+[2p/n−1]V⟩

(52)
and therefore, thanks to (A)
e⟨E,θ⟩|⟨E, V⟩|n ≤nnρ if ∥ −0∥≤ε
2, ∥V∥≤ε
2
(53)
For any unitary U, by setting V = Uε/2, we therefore have
e⟨E,⟩|⟨E, U⟩|n ≤
2n
ε
n
ρ
(54)
The equality
E⊗n = Sup
U
|⟨E, U⟩|n therefore shows that
(C)
e⟨E,⟩E⊗n ≤
2n
ε
n
ρ∀ ∈B′(0, ε/2)
(55)

32
F. Barbaresco
It follows from (B) that the integral with tensorial value
In =

U
e⟨E,⟩E⊗nvol
(56)
is normally convergent in the neighborhood of 0.
Using Taylor’s formula for the exponential
eα −1 −α = α2
2 eλα
λ ∈[0, 1]
(57)
We ﬁnd the formula:
e⟨E,+V⟩E⊗n −e⟨E,θ⟩E⊗n −e⟨E,θ⟩E⊗n+1(V) = 1
2e⟨E,+λV⟩E⊗n+2(V)(V)
(58)
where the notation T(V) designates the contraction of a covariant tensor T with the
vector V. Thanks to (B), the norm of the second member can be bounded by
1
2
2(n + 2)
ε
n+2
ρ∥V∥2
(59)
By integrating on U and by using (C), we therefore have
In+V −In −In+1(V)
 < a
2
2(n + 2)
ε
n+2
∥V∥2
(60)
For that  ∈B′(0, ε/4) and that ∥V∥≤ε/4 (a denotes the value of the integral
(C)); it therefore follows that the map  →In is continuous and differentiable in a
neighborhood of 0, and that the derived linear map is In+1; so I0 is a C∞class map
that admits In as the n-th derivative.
Q.E.D.: Quod Erat Demonstrandum.
4
2nd Souriau Paper: “Symplectic Geometry and Mathematical
Physics”
Where it is a question of classical physics, relativity and statistics.
4.1
§ 1 - Since 1788. The Mechanics are Symplectic
4.1.1
Movements Manifold
Consider a material point subjected to a force ⃗F; one designates its mass by m, its position
and its speed at time t by ⃗r and ⃗v; we suppose that is a function of class C∞of (t, ⃗r, ⃗v).
General theorems on differential equations show that “equations of motion”
(1.1)
d⃗r
dt = ⃗v
,
md⃗v
dt = ⃗F
(61)

Jean-Marie Souriau’s Symplectic Model of Statistical Physics
33
can be integrated starting from arbitrary initial conditions (t, ⃗r, ⃗v).
Each solution of (1.1), or “movement”, will be designated by x (we can if we want to
identify x with the map t →(⃗r, ⃗v); we will designate by U the set of all the movements.
Let us choose a date t, and locate each movement x by 6 components of ⃗r and ⃗v at the
date t: we thus obtain a system of local coordinates of U; by varying t, we will obtain
an atlas of U,which gives it a structure of differentiable manifold (of dimension 6).
4.1.2
Symplectic Structure
Let δ and δ
′ be arbitrary variations of motion; at every time t, there are variations δr, δ⃗v
and δ′r, δ′⃗v of positions and speeds; Lagrange showed (in his “Mécanique Analytique”,
written in 1788) that the number:
(1.2)

mδ⃗v, δ′⃗r

−

mδ′v, δ⃗r

(62)
where the ⟨, ⟩means the scalar product in R is independent of the choice t; we can
therefore note it
(1.3) σ

δx, δx′
(63)
We notice that:
a)
σ is a 2-form of U (i.e. an antisymmetric tensor of rank 2),
b) σ is invertible (Lagrange deﬁnes the components of σ and those of σ −1 in any
coordinates of U; these numbers are called today “parentheses” and “brackets” of
Lagrange);
c)
σ is ﬂat, in the sense that there exists an atlas in which the components of σ are
constant (the atlas indexed by time that we have just encountered).
A manifold with an invertible, ﬂat 2-form is today called a symplectic manifold.
This symplectic structure of the space of movements immediately extends to the case of
a system of material points (if the forces derive from a potential).
4.2
§ 2 - Emmy Noether and Measurable Quantities
4.2.1
Dynamic Groups
Let be U a symplectic manifold; the diffeomorphisms of U that preserve the form σ are
called symplectomorphisms; they form a group that we will note Sympl(U); Sympl(U)
is a group of inﬁnite dimension; it can have ﬁnite dimensional subgroups (Lie groups)
which we call dynamic groups.
Let therefore be G a dynamic group (1), g its Lie algebra (Fig. 4).
(1): We suppose deﬁned a morphism a →au of G in the group of symplecto-
morphisms of U such that (a, x) →au(x) is a differentiable map (from G × U in
U).
Any element Z acts inﬁnitely on U: it associates to each x ∈U a tangent vector that
we will note Z(x). Since G preserves σ, the corresponding Lie derivative of σ is zero;
well- known formulas of E. Cartan show that there is (at least locally) an application
(2.1)
x →μ
(64)

34
F. Barbaresco
from U in the dual of Lie algebra, such that:
(2.2) σ(δx, Z(x)) = δμ(Z) for any derivation δ
(65)
This formula deﬁnes μ up to an additive constant: μ is called “moment map” of the
dynamic group G (Fig. 6).
Fig. 6. Moment map deﬁnition
A further example is that of a “conservative” system, whose symplectic structure
is invariant by translation in time. The moment map is nothing else than Energy. The
calculation is exposed in Lagrange (1).
(1) On this occasion, Lagrange writes the expression today called Hamiltonian, and
designates it by the letter H; and yet Sir William Rowan Hamilton was older than ﬁve
years from the publication of the book.
4.2.2
Noether’s Theorem
These results generalize a theorem of Emmy Noether (which concerned the case of a
variational problem whose Lagrangian is invariant by a group with one parameter); we
see how the symplectic structure associates “conserved quantities” with the symmetries
of a dynamic system. These quantities will be measurable experimentally if the measur-
ing device has the same symmetry as the studied system: thanks to Noether’s theorem,
the interaction of the system and the device simply transfers the quantity μ(example: we
measure the riﬂe bullet impulse by landing it on a soft target attached to a pendulum).
4.3
$ 3 - Mass and Cosmology
4.3.1
Symplectic Cohomology
Let us place ourselves in the general case of a dynamic group (cf. § 2}.

Jean-Marie Souriau’s Symplectic Model of Statistical Physics
35
Some differential geometry formulas make it possible to establish that the quantity
(3.1) σ

Z(x), Z′(x)

−μ

Z, Z′
(66)
where Z and Z′ are two elements of g and their Lie bracket does not depend on x; so we
can then write.
(3.2) σ

Z(x), Z′(x)

= μ

Z, Z′
+ f

Z, Z′
(67)
f being a 2-form of Lie algebra; we verify the identity of Bargmann
(3.3) f

Z,

Z′, Z′′
+ f

Z′,

Z′′, Z

+ f

Z′′,

Z, Z′
= 0, ∀Z, Z′, Z′′ ∈g
(68)
The 2-forms satisfying (3.3) are called symplectic cocycles of Lie algebra.
Remember that μ is only deﬁned up to the addition of a constant; if we change μ to
μ −μ1. We see in (3.2) that f is changed to f + f1, with
(3.4)
f1

Z, Z′
= μ1

Z, Z′
(69)
f1 is obviously still a cocycle; it is called coboundary of μ1; we say also that f and f +f1
are cohomologous; because of the arbitrariness of μ1. It is the cohomology class of f
which is deﬁned by the action of a dynamic group.
Note that the symplectic cohomology classes of a Lie group G give a ﬁnite dimen-
sional vector space, which can be calculated directly (by applying formulas (3.3) and
(3.4)) without ﬁrst knowing any symplectic action of G. Each symplectic action of G
will then deﬁne a point of this space.
4.4
§ 7 – Thermodynamics and Lie Groups
4.4.1
Statistical States
A movement of a dynamic system - a “classical state” - is therefore a point x of the
manifold U of movements. We will rise to the “statistical mechanics” by replacing the
variable x by a random variable: a statistical state that is by deﬁnition a law of probability
on the manifold U (which can especially be concentrated at the point x, which includes
classical states among statistical states).
It happens that the symplectic manifolds have a completely continuous measure
invariant under diffeomorphism, the Liouville measure. Consequently, any statistical
state, if it is completely continuous, will be the product of the measure of Liouville by
a scalar function ρ, called the distribution function.
We have seen (§ 1) how to associate a movement space map with each date t; we can
obviously relate the distribution function to these coordinates (but its expression will
obviously depend on t); we can easily see that it veriﬁes a partial differential equation,
called Liouville’s equation.
This makes it possible to make the link with the traditional formulation of statisti-
cal mechanics; this initially supposes the existence of a distribution function ρ, solu-
tion of the Liouville equation. In particular, Boltzmann founded the kinetic theory of

36
F. Barbaresco
gases by giving a method of approximation of the Liouville equation; the result of this
approximation is called the “Boltzmann equation”.
It is important to seek the statistical states which are established spontaneously in
systems in thermodynamic equilibrium: this was the work of Maxwell (starting from
his Initial researches on the rings of Saturn), completed by Gibbs: the formulation is
very simple: log(ρ) is an afﬁne function of energy (result which is veriﬁed directly by
spectroscopy of hot gases, and indirectly by the formulas of thermodynamics).
Since energy is deﬁned as the moment map of the group of translations in time, the
Maxwell-Gibbs distribution will be generalized immediately to any symplectic manifold
possessing a dynamic group.
If we denote by generalized energy E (that is to say the moment map which belongs,
let us remember, to the dual of the Lie algebra g of the group), we will write:
(7.1) ρ = eE−z
(70)
 being an element of g which indexes the statistical state (and which will generalize
the temperature), a constant of normalization, which one determines by writing that the
mass of the law of probability is equal to:
(7.2) z = log

U
eEdλ(x)
(71)
formula where λ denotes the Liouville measure; z, thus deﬁned as a function of
“temperature” , generalizes the thermodynamic potential of Planck.
This integral should obviously converge; we deﬁne the set of canonical Gibbs set
	 as the biggest open set (in the Lie algebra) where this integral is normally locally
convergent (in ). We show that 	 is convex, and that z is a C∞function on 	; that its
derivative Q = ∂z
∂ with the mean value of the energy E (Q thus generates heat); that the
tensor ∂Q
∂ is symmetrical and positive (it generalizes the heat capacity). It follows
that z is a convex function of ; the Legrendre transform associates a concave function
with it, namely:
(7.3) Q →s = z −Q
(72)
s is entropy.
These results provide the bases of thermodynamics if we apply them to the group
of temporal translations (it sufﬁces to deﬁne the absolute temperature T by the formula
T = 1
, having chosen a temperature unit which gives the value 1 to the “Boltzmann
constant”).
What can be the physical interest of the generalization of Gibbs states to the case of
a more general dynamic group? Before answering this question, it is important to make
some mathematical remarks.
If we apply the theory to the case of a non-commutating group, a new fact appears
“the symmetry is broken”; it is no longer the Gibbs states which are individually
invariant by the group (in the classical case, they constituted “equilibrium” states, that
is to say they were invariant by temporal translation), but only the canonical set 	 itself

Jean-Marie Souriau’s Symplectic Model of Statistical Physics
37
(on which the group acts through the intermediary of the adjoint representation). This
fact gives rise to new formulas, characteristic of broken symmetries. Thus, for each
“temperature” , let’s deﬁne a tensor f, sum of the cocycle f (deﬁned in (3.2)) and the
coboundary of heat:
(7.4) f

Z, Z′
= f

Z, Z′
+ Q

Z, Z′
(73)
f has then the following properties:
a)
f is a symplectic cocycle
b)
 ∈Ker f
(74)
c)
The symmetric tensor g deﬁned on the set of values of ad() by
g

[, Z],

, Z′
= f

Z,

, Z′
(75)
is positive (and even positive deﬁnite if the group action is effective).
These formulas are universal in the sense that they do not involve the symplectic
manifold U - but only the group G, its symplectic cocycle f and couples , Q.
Perhaps this “Lie groups thermodynamics”is of mathematical interest.
– Physically, the theory gives good results if we apply it to the various subgroups of the
Galileo group which are characteristic of thermodynamic devices: thus a cylindrical
box in which we enclose a ﬂuid leaves it an invariance sub-group of dimension 2:
rotations around the axis, temporal translations. From this results a two- dimensional
temperature vector, which can be “transmitted” to the ﬂuid by means of the box, (by
cooling it, for example and by making it turn); the results of the theory are the very
ones that are exploited in centrifuges (for example to make butter, uranium 235 or
ribonucleic acids).
– Note that the process by which a refrigerated centrifuge transmits its own tempera-
ture vector to its content has two different names: thermal conduction and viscosity,
depending on the component of the temperature vector that is considered; conduc-
tion and viscosity should therefore be uniﬁed in a fundamental theory of irreversible
processes (theory which remains to be built).
– The transition to relativistic mechanics is immediate (replacement of the Galileo group
by the Poincaré group); and is obtained without conceptual difﬁculty, the description
of a perfect relativistic gas (its law of compressibility causes Bessel functions to
intervene), a relativistic centrifuge and even a statistical equilibrium of photons.
4.5
§ 8 – Why the Earth Turns
We have just studied “thermodynamic machines” associated with subgroups of the
Galileo group (or of the Poincaré group); the simplest machine would obviously be

38
F. Barbaresco
the one associated with the whole group: it would simply consist of a system (a gas for
example) free in space.
But the machine cannot work if the Galilean group is a dynamic group of any sym-
plectic manifold, and if the cohomology (and therefore mass) is not zero. The associated
Gibbs set (§6) is empty. This follows immediately from (7.5); by developing the relation
(7.5b), one notes that it is not veriﬁed in any open set.
An analogous theorem is valid with regard to the Poincaré group. In the Galilean
case, trouble could be easily analyzed thanks to the barycentric decomposition theorem
(see Sect. 3): If the system had a state of equilibrium, it would be the same with the
movement of its center of gravity (which includes as a free material point); and if there
was a statistical equilibrium state for a free material point, there would also exist for
any ideal gas (composed of atoms whose interactions are neglected). But whoever has
found one day a ﬂat tire knows that the gas admit equilibrium only if their freedom is
carefully limited by an appropriate device.
But nothing prevents us to seek thermodynamic equilibrium with respect to the
barycenter: we know how to deﬁne the symplectic manifold of the movements with
respect to the barycenter (§3); Galileo’s group is still a dynamic group, but through
the intermediate of its quotient SO(3) × R. So that there is no more cohomology; the
equilibria (within the meaning of §7) are a priori possible.
This research leads to the following results: the equilibrium around the barycenter
is a uniform rotational movement around a main axis of inertia (the generalized four-
dimensional temperature) is made up of the ordinary temperature T and the rotation
vector ⃗ω;thegeneralizedheatcapacityassociatestheusualheatcapacitywiththemoment
of inertia around the axis of rotation; all the usual thermodynamic relationships (one-
dimensional) are extended by four-dimensional formulas.
If ⃗ω is not zero, the symmetry is spontaneously broken; the theorems of §7 give
new thermodynamic results. One of the simpler (the Gibbs set 	 is convex in the Lie
algebra) leads to the following proposition: if there exists, for each value of temperature
T, a rotational speed limit ∥⃗ω∥Max. ∥⃗ω∥Max is a concave function (Fig. 7): if then, for a
couple of temperatures T1, T2, the function T →∥⃗ω∥Max is decreasing, there will exist
a critical temperature TC beyond which no equilibrium can no longer exist, even at zero
rotation.
Astronomy provides us with many examples of relative equilibria of this type, more
or less perfect: the rotation of the Earth; the mutual movement of the Earth and the Moon
(if it were completed, the Earth, like the Moon, would always show the same face to
its share; the tides work towards this result); the Trojan planets, which approximately
realize, with the Sun and Jupiter, Lagrange’s solution of the three-body problem (an
equilateral triangle which turns uniformly around its barycenter).
It is not impossible that the concepts developed here may be useful in explaining other
astronomical phenomena (rotation of galaxies; supernovae explosions) at the same at the
microphysical scale (heavy particles “hadrons” explode beyond a critical temperature
close to 10 12 K).

Jean-Marie Souriau’s Symplectic Model of Statistical Physics
39
Fig. 7. Critical temperature
5
3rd Souriau Paper: “Classical Mechanics and Symplectic
Geometry”
In recent years, common ground between mathematicians and physicists has appeared:
experimental veriﬁcations of gauge theories have shown that differential geometry is
essential for understanding particle physics.
But we can place ourselves at a much more basic level, apparently: that of classical
mechanics. The differential geometry makes it possible to “revisit” this territory - and
to discover some surprising landscapes there: the present work is only a guide of these
countries still little explored.
All these aspects of mechanics are invisible if we remain imprisoned in the scholasti-
cism that the epigones of Lagrange called “analytical mechanics”: we must ﬁrst refer to
the old moons the conﬁguration space, the phase space and the Hamiltonian formalism:
these concepts result from a splitting of the dynamics; arbitrary splitting which breaks
the global structure.
On this clean table, we can build “symplectic materialism”, a conceptualization of the
concrete that ﬁts better with the facts. Classical or more recent mechanical notions (force,
impulse, moment, energy, mass, action and reaction, electrical charge, spin, magnetic
moment, etc.) then enter geometry; they give material examples of various mathematical
structures: symplectic and pre-symplectic manifolds, cohomology of groups and Lie
algebras, homotopy, homology, etc.
For the mathematician reader, these pages can therefore serve as an illustration of
these theories - and suggest some new problems. The physicist will ﬁnd there better
structured models of these familiar objects. The organization that is revealed in this way
enables sensitive questions to be answered, such as interaction problems.
Finally, we indicate how geometry establishes a link between classical mechanics and
more precise descriptions of matter: statistical mechanics, special relativity. However,
geometric quantization is not discussed here.

40
F. Barbaresco
5.1
Statistical Mechanics (Chapter 3.2)
This diffusion technique makes it possible to process a system of particles enclosed in a
box and likely to interact with each other and with the walls: in “hard” collision models
the problem of the regularization of multiple collisions arises; in “elastic” models, the
corresponding problem of interactions has several bodies.
The KINETIC THEORY OF GAS can be described by ﬁrst constructing a separate
and symplectic space of movements X by this method; then by simply deﬁning the
STATISTICAL STATES as the probability laws on X (on the phase space, one must
introduce an evolutionary state solution of the Liouville equation).
Among these statistical states, the THERMODYNAMIC EQUILIBRIUM STATES
are given by Gibbs phenomenological rule; however, this rule has a geometric
formulation which goes far beyond the framework of kinetic theory.
On any connected symplectic manifold X , there exists a measure λ invariant by
the symplectomorphisms; this property deﬁnes it except for a constant factor: it is the
MEASURE OF LIOUVILLE.
Let G a Lie group endowed with an effective symplectic action on X , and having
a moment  [see Sect. 4.2]. If Z is an element of the Lie algebra, we will call GIBBS
STATE the probability law λZ whose density with respect to λ is equal:
e(x)Z
(76)
up to a constant factor, of course. We put this factor in exponential form:
dλz(x) = e(x)Z−zdλ(x)
(77)
the mass of the probability having to be equal to 1, we have:
z = log

X
e(x)Zdλ(x)
(78)
The domain of convergence of the integral is a convex part of the Lie algebra, and also
a union of orbits of the adjoint representation; z is a convex analytic function on the
interior C of this set.
We now take Z in C The mean value of the moment (x) in the Gibbs state is equal
to the derivative:
Q = z′(Z)
(79)
Z →Q is an analytical diffeomorphism of C over a convex set of g∗; the Legendre
transform s of z:
s(Q) = QZ −z
(80)
is convex and satisﬁes Z = s′(Q): the second derivative:
K = z′′(Z)
(81)

Jean-Marie Souriau’s Symplectic Model of Statistical Physics
41
is a positive tensor, whose inverse is equal to s
′′(Q).
K endows the set C with a Riemannian structure invariant by the action of the group;
for this structure, the linear application Ad(Z) is antihermitian.
The application fZ, deﬁned by:
fz

Z′, Z′′
= K

Z, Z′
, Z′′
, Z′, Z′′ ∈g
(82)
is a symplectic cocycle, cohomologous to f [see Sect. 4.2]; its kernel is the orthogonal
of the adjoint orbit of Z for the Riemannian structure of C.
The real thermodynamic equilibria are thus associated with a SUB-GROUP OF THE
GALILEE GROUP.
In the classical case, we consider only the group of dimension 1 of the temporal
translations (which is deﬁned only after having chosen a reference frame - for exam-
ple that of the box which contains the gas). So with decent units, Z is the reciprocal
of the ABSOLUTE TEMPERATURE; z is PLANCK’s THERMODYNAMIC POTEN-
TIAL; −s is ENTROPY; Q is INTERNAL ENERGY; K characterizes the CALORIFIC
CAPACITY.
But a larger subgroup makes it possible to describe other real states, for example
those which are observed - and used - in centrifuges.
In particular, one can think of the equilibria of an isolated system, on which the
Galileo group acts symplectically - in principle [see Sect. 4.2].
But the results we have just indicated show that such equilibria do not exist: the
integrals are always divergent. This difﬁculty is of the “cosmological” type: in such
an equilibrium, the position of the barycenter should be equipartite in space - which is
incompatible with the inﬁnite volume of this one.
It is precisely the “barycentric decomposition” [see Sect. 4.2] which allows us to
eliminate this difﬁculty: it sufﬁces to describe the statistics of the movements around
the barycenter; movements which themselves constitute a symplectic manifold. As we
know, the action of Galileo’s group is then reduced to that of G = SO(3) × R.
Gibbs states are therefore indexed by an element Z of the Lie algebra of G; Z is inter-
preted by the vectors ﬁeld that it deﬁnes on space-time; ﬁeld whose constant temporal
component deﬁnes the ABSOLUTE TEMPERATURE T and whose direction deﬁnes a
VELOCITY FIELD; this velocity ﬁeld is that of a SOLID ROTATION AROUND THE
BARYCENTRE, and it is characterized by a constant angular velocity ⃗	.
The fact that the set C is convex and variable by the action of the group shows that
there can exist a maximum value of ⃗	, which is then a concave function of the absolute
temperature; resulting in the existence of a critical temperature TC above where there is
no more equilibrium - even zero rotation.
Let us examine the dual variables: Q, element of g∗, simultaneously describes the
internal energy and the KINETIC MOMENT ⃗I; this one is necessarily parallel to ⃗	;
in the same way K associates the caloriﬁc capacity and the MOMENT OF INERTIA
around the polar axis.
All the usual thermodynamic relations generalize to these geometric variables.
This schematic description resulting from the theory of groups alone is independent
of any particular model: only the fact, that the action of the Galileo group is symplectic,
is used. It therefore aims at universality.

42
F. Barbaresco
Indeed, we ﬁnd that it describes well the celestial bodies - to the extent that they are
isolated (not too many tides or radiation) and in balance. To summarize, it is due to the
group of Galileo that the Earth turns…
5.2
Galilean Relativity (Chapter 2.7 in Souriau Paper)
We deﬁne a SUBGROUP G of the Coriolis group (“Coriolis group is deﬁned in Sect. 2.6
of the original Souriau’s paper which is not translated) by choosing the function A
constant and the function ⃗b afﬁne.
The action of G on space-time is therefore written:
t →t + e ,
e ∈R
⃗r →A⃗r + ⃗bt + ⃗c ,
A ∈SO(3), ⃗b, ⃗c ∈R3
(83)
and is lifted canonically to the ﬁber of the tangent directions:
⃗v →A⃗v + ⃗b
(84)
G is a connected Lie group of dimension 10, which we call GALILEE GROUP.
A preliminary remark: the subgroup G IS NOT NORMAL (or INVARIANT) in the
Coriolis group; and more precisely he is his own normalizer.
It follows that the deﬁnition of the Galileo group (as a group of space-time diffeo-
morphisms) by the above formulas implies the choice of a class of reference points;
those called INERTIA FRAMES; class which is deduced from one of these references
by the very action of G.
When Galileo claimed that the Earth rotates, the Holy Ofﬁce should have simply
understood that the terrestrial frames did not belong to the inertia class - which would
have avoided many misunderstandings.
This being the case, the “Galilean principle of relativity”, in its classical “weak”
form, expresses that the action of this group G on the space of motions Y of an isolated
system preserves the equations of motion - in other words the foliation Ker(σ).
The “material” interpretation of the form σ suggests a stronger hypothesis: the
LAGRANGE FORM ITSELF must be invariant by the action of G. With notations,
g(s) = s,
∀g ∈G
(85)
Is there really a part of the Universe that we can consider as isolated? We will not concern
ourselves here with this question; it arises in the same way if we want to use the weak
Galileo principle - and we seek what is speciﬁc to the strong principle.
The geometric situation is as follows:
a separate connected presymplectic manifold (Y, σ) on which a connected Lie group
G acts by automorphisms of σ.
Such a “symplectic action” deﬁnes several important geometric objects which we
will describe.
For every integer p, the p-forms ω deﬁned on the group G and invariant (by the left
translations, considered as diffeomorphisms of G) constitute a space of ﬁnite dimension.
On the other hand, the exterior derivative dω of an invariant form is an invariant form.

Jean-Marie Souriau’s Symplectic Model of Statistical Physics
43
This therefore deﬁnes a cohomology; cohomology groups H p are ﬁnite dimensional
vector spaces; they can be calculated from the single Lie algebra of G: for example the
size is equal to the codimension of the derived algebra.
Let us choose a point y of the (pre)symplectic manifold Y; it is clear that the reciprocal
image of σ by the application G →Y:
g →g(y)
(86)
is a 2-form σy of G, invariant and closed: we will show that its cohomology class is
independent of the choice of y: it therefore constitutes an element of the vector space
H 2(a “symplectic cohomology class”) which is characteristic of G action on Y.
Note ﬁrst that any 2-form invariant on G is characterized by its value at the neutral
element, therefore by an (algebraic) 2-form f of the Lie algebra g: it will be closed if f
veriﬁes the identity of the 2-cocycles:
(A) f

Z,

Z′, Z′′
+ f

Z′,

Z′′, Z

+ f

Z′′,

Z, Z′
= 0
(87)
for all Z, Z
′, Z
′′ in g. The exact forms will be those which are linear functions of the Lie
bracket.
On the other hand, any element Z of g deﬁnes a vector ﬁeld on Y, for which the Lie
derivative of σ is zero; it follows from a classical Cartan formula that the contracted 1-
form σ of this vector is closed. By considering a basis of g, we deduce the local existence
of a differentiable function  with values in the dual g∗, of Lie algebra, such that
(B) σ(δy, Zy) = δ[(y)Z],
∀δy, ∀Z
(88)
it is called MOMENT MAP (“application moment” in French); we can obviously add
an arbitrary constant.
Because σ is closed, the distribution Ker(σ) is integrable (Cartan); if one chooses
δy into Ker(σ) the expression (B) is zero – for any Z, so the moment function is locally
constant on the leaves of Ker(σ), generally constant on these leaves if the moment is
globally deﬁned.
This result is a generalization of NOETHER THEOREM (which concerns the
presymplectic structure of variational problems and 1-dimensional Lie groups).
Then taking the Lie derivative of identity (B) for a second element Z′ of g, we see
that the expression
σ

Zy, Z′y

−(y)

Z, Z′
(89)
is locally constant; what we will write:
(C) σ

Zy, Z′y

= (y)

Z, Z′
+ f

Z, Z′
(90)
f necessarily being a 2-cocycle cohomologous to the invariant 2-form σy; therefore
the symplectic cohomology class of σy is locally constant - hence constant, since Y is
assumed to be connected.
We can ﬁnally use the additive constant which appears in each local moment to
choose everywhere the same representative f of the cohomology in formula (C); Then

44
F. Barbaresco
if 1 and 2 are two local moments, the locally constant 1-form 1(y) −2(y) will
belong to space H 1 (because the exact invariant 1-forms are zero). The Cech cohomology
technique allows to deﬁne a group morphism:
(D) H1(Y) →H 1(G)
(91)
H1 designating the ﬁrst cohomology group of the manifold Y; morphism which
constitutes the only obstruction to the existence of a global moment.
In particular, there will exist a global moment if H 1(G) is zero or if H(Y) is zero,
and a fortiori if 1(Y) is zero, that is to say if Y is simply connected.
Let us also consider a 2-cocycle of G, that is to say an invariant closed 2-form σ. It
makes of G a presymplectic manifold, on which G acts leaving σ invariant: the results
established for the manifold Y therefore also apply to G.
There is therefore a moment  in the vicinity of the neutral element e; its deﬁnition
(B) shows that the derivative of  at the point G, a priori a linear application of g in its
dual, is antisymmetric - because it coincides with the value of σ on e.
On the other hand we can choose the constant which appears in  such that
(e) = 0
(92)
then formula (C) shows that the cocycle f associated with this choice is also equal to
this initial value of σ.
Finally, as in the general case, the obstruction to the global existence of  is a
morphism:
H1(G) →H 1(G)
(93)
since G is a connected group, H1(G) is moreover equal to the homotopy group 1(G).
Let us return to the manifold Y, and choose on Y two local moments 1 and 2,
both verifying condition (C) with the same 2-cocycle f .
Let us also consider a local moment 3 on G, also associated with f . A derivative
calculation shows that the expression:
(E)

1(g(y)) −3(g)

Ad(g) −2(y)
(94)
where Ad denotes the ADJOINT REPRESENTATION of G on g, is locally constant on
G × Y.
In particular 1, 2, 3 are three local moments of G associated to the same 2-
cocycle f , function:
(F)

1

gg′
−3(g)

Ad(g) −2

g′
(95)
is locally constant over G × G.
We will now apply these various results to the case of the Galileo group, whose
elements we will note:
g = (A, ⃗b, ⃗c, e)
(96)
The dimensions of the spaces that interest us are as follows:

Jean-Marie Souriau’s Symplectic Model of Statistical Physics
45
p
0 1
2
Forms
1 10 45
Closed forms 1 1
10
Exact forms
0 0
9
which shows that the spaces H 1 and H 2 are each of dimension 1.
The element of H 2 associated with any symplectic action of group, therefore belongs
to a space of dimension 1: this is called a “measurable quantity”. To measure it, it is
necessary to choose a “unit”, that is to say here a non-exact closed 2-cocycle f◦; for
example:
f◦

δg, δ′g

=

δ⃗b, δ′⃗c

−

δ′⃗b, δ⃗c

(97)
we know that we can choose the local moments  so as to realize the condition:
(C
′) σ

Zy, Z′y

= (y)

Z, Z′
+ mf◦
Z, Z′
(98)
m is the number that measures symplectic cohomology.
An element of the dual of Lie algebra can be written:
μ = {⃗l, ⃗g, ⃗p, E}
(99)
⃗l, ⃗g, ⃗p being three vectors and E a scalar such that:
μ(δg) = −1
2Tr[j(⃗l)δA] −⟨⃗g, δ⃗b⟩+ ⟨⃗p, δ⃗c⟩−Eδe
(100)
j(⃗l) denotes the operator vectorial product: ⃗l × ∗.
It is clair that G is diffeomorphic to SO(3) × R7, so that his homotopy group is Z2.
The fact that every morphism Z2 →R is zero shows there exists on G a GLOBAL time
associated with each 2-cocycle f ; in this case f = f◦, the calculation gives the associated
moment:
◦(g) =

⃗c × ⃗b, ⃗c −⃗be, ⃗b, 1
2
⃗b2

(101)
it necessarily veriﬁes the identity:

F′
◦

gg′
= Ad∗(g)

◦

g′
+ ◦(g)
(102)
Regarding the symplectic actions of G on a manifold Y, the condition (C′) allows to
deﬁne the action of the moment on the derived Lie algebra of g, let δe = 0; therefore we
can globally set 9/l0th components, namely ⃗l, ⃗g, ⃗p; E will be deﬁned only to an additive
constant near - and possibly multiform.
If  is a global moment, we know that the function:
(E
′)

(g(y)) −m◦(g)

Ad(g) −(y)
(103)

46
F. Barbaresco
is locally constant on G × Y, so constant; it sufﬁces to do g = e to see that it is zero.
This gives us the VARIANCE of the moment μ = (y) under the action of an element
g = {A, ⃗b, ⃗c, e} of the group:

E
′′
μ →{A⃗l + ⃗c × A⃗p −⃗b × A⃗g, A[⃗g −⃗pe], A⃗p, E + ⟨⃗b, A⃗p⟩}
+m

⃗c × ⃗b, ⃗c −⃗be, ⃗b, ⃗b2
2

(104)
In the case where the number m is not null, we can build a new object, the function:
⃗G(t) = ⃗g + ⃗pt
m
(105)
whose variance is:
⃗G →AG + ⃗bt + ⃗c
(106)
the same as that of a POSITION ⃗r. Therefore ⃗G appears as a MOVING POINT in space,
visibly animated by a uniform rectilinear translational movement; in other words, a line
of space-time.
All these quantities have been deﬁned in this way BY THE SINGLE THEORY OF
GROUPS; to be able to give a “mechanical” interpretation, we are going to calculate
them in a particular case: this of a system of material points in interaction. The calculation
is possible because we know on the one hand the form of Lagrange, on the other hand
the action of the Galileo group;
It leads to the following results:
⎧
⎪⎪⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎪⎪⎩
m = !
j
mj
⃗l = ! mj⃗rj × ⃗vj
⃗g = ! mj

⃗rj −⃗vjt

⃗p = ! mj⃗vj
E = 1
2
! mj⃗v2
j + V
(107)
V being a (local) potential of forces.
These objects are effectively known and named: m is the MASS;⃗l is the ANGULAR
MOMENTUM; ⃗p LINEAR MOMENTUM; E ENERGY. The fact that all these values
are the MOTION CONSTANT appears here as a consequence of the principle of strong
Galilean relativity. One can easily verify on these particular expressions the general
formula of variance (E′′).
The only exception is the object ⃗g; its bizarre variance protected it from the
eponymous names. We preferred to name the associated object ⃗G, which is worth:
⃗G =
! mj⃗rj
m
(108)
and which is therefore the BARYCENTRE (or CENTER OF MASS). The associated
theorem is the fact that the motion of ⃗G is straight and uniform.

Jean-Marie Souriau’s Symplectic Model of Statistical Physics
47
The Galileo group G can be decomposed into semi-direct product, thanks to the
existence of a commutative subgroup of dimension 6:
A = 1, e = 0
(109)
which is the kernel of an idempotent endomorphism:
⃗b →0, ⃗c →0
(110)
If the total mass of a system is not zero, then a general theorem implies a DECOM-
POSITION of the system (“barycentric decomposition”); the symplectic manifold X of
movements is the Cartesian product of a symplectic manifold of dimension 6 (which
can be interpreted as the space of movements of the center of mass) and of a second
symplectic manifold which describes the “movements around of the center of mass”.
Example: the Newtonian problem with two bodies (of masses m1 and m2) is thus
reduced to the case of a material point attracted by a ﬁxed point (1, 2). The masses
involved in the various symplectic factors are respectively m1 + m2 and
m1m2
m1+m2 .
The Galileo group acts separately on each of these manifolds; it results from it the
existence of a group of symmetries greater than G: its dimension is 14 and it is isomorphic
to the direct product
G × SO(3) × R
(111)
the components of the moment are the sum of TWO independent CONSTANTS OF
MOVEMENT (those which are related to the movement of ⃗G being called “orbitals”
and the others “proper”): ⃗p and ⃗g have only orbital components, but there is a “own
kinetics moment” and an “own energy”, associated with the action of SO(3) × R. The
values of “orbital angular momentum” and “orbital energy” are respectively:
⃗g × ⃗p
m
,
⃗p2
2m
(112)
There are Galilean dynamical systems which are not simple assemblies of n mate-
rial points; for example those which contain continuous solid bodies. If we manage
to describe them by means of a presymplectic evolutionary space and invariant by the
Galilean group, we can apply the previous results to them. Mass, angular momentum,
momentum, energy, barycenter will also be DEFINED geometrically by (B) and (C′),
and will verify general theorems.
Imagine for example a magnetic monopoly in the presence of the ﬁeld created by a
permanent current loop. We are even in a case, allowed by the strong Galileo principle,
where the energy E is MULTIFORM.
Such a system must constitute an inexhaustible source of proﬁts: energy is drawn
from it and it returns to its initial state…
Unfortunately these wonderful objects do not exist. Why? We will ﬁnd an
“explanation” in (3, 2).
Let us return to the particular case of a system of interacting points. The Noether’s
theorem taught us that ⃗l, ⃗g, ⃗p are also constant of motion; it sufﬁces to derive to deduce

48
F. Barbaresco
two conditions relating to the forces ⃗Fj.
⎧
⎪⎨
⎪⎩
!
j
⃗Fj = 0
!
j
⃗rj × ⃗Fj = 0
(113)
conditions that can be translated by the existence of a decomposition:
⃗Fj =

k
⃗Fjk
(114)
with
⃗Fjk + ⃗Fkj = 0,

⃗rk −⃗rj

× ⃗Fjk
∀j, k
(115)
This result expresses the EQUALITY OF ACTION AND REACTION - which is there-
fore a consequence of the strong principle of Galilean relativity. In particular, an isolated
material point is not subjected to any force, its movement is straight uniform line (this
is what is sometimes called the “Galilean principle”).
Remarkably, this fact IS NOT A CONSEQUENCE OF THE WEAK PRINCIPLE:
let us consider a system of two points which are subjected to the same force:
⃗F1 = ⃗F2 = ⃗r2 −⃗r1
(116)
(and not two opposing forces as requested by the equality of action and reaction). It is
immediate that equations of the movement:
δy ∈Ker(σ)
(117)
are invariant by the Galileo group; but the strong principle is viviﬁed - simply because
the system does not verify Maxwell’s principle dσ = 0; in other words, the manifold of
evolution Y is not pre-symplectic.
This is why, in traditional mechanics, one is obliged to make equality of action and
reaction an SUPPLEMENTARY PRINCIPLE - which is unnecessary here.
Finally, the “inductions” ⃗Bj are all zero. This last result is paradoxical: there are
objects, the magnets, which are capable of creating a magnetic induction ⃗B in a system
in which they are incorporated.
The resolution of this paradox will ask some efforts - but will be instructive.
References
1. Souriau, J.-M.: Structure des systèmes dynamiques. Dunod (1969)
2. Souriau, J.-M.: Structure of Dynamical Systems: A Symplectic View of Physics. Progress
in Mathematics, vol. 149. Springer, New York (1997). https://doi.org/10.1007/978-1-4612-
0281-3
3. Souriau, J.-M.: Mécanique statistique, groupes de Lie et cosmologie. Colloque International
du CNRS “Géométrie symplectique et physique Mathématique”, Aix-en-Provence 1974 (Ed.
CNRS, 1976)

Jean-Marie Souriau’s Symplectic Model of Statistical Physics
49
4. Souriau, J.-M.: Géométrie Symplectique et Physique Mathématique, Deux Conférences de
Jean-Marie Souriau, Colloquium do la Société Mathématique de France, (Paris. Ile-de-
France), 19 Février 1975 - 12 Novembre 1975
5. Souriau, J.-M.: Mécanique Classique et Géométrie Symplectique, CNRS-CPT-84/PE.1695,
Novembre 1984
6. Souriau, J.M.: Equations Canoniques et Géométrie Symplectique; Publications Scientiﬁques
de l’Université d’Alger Publisher, Série A; vol. 1, fasc.2, pp. 239–265 (1954)
7. Souriau, J.M.: Géométrie de l’Espace des Phases, Calcul des Variations et Mécanique
Quantique, Tirage Ronéotypé; Faculté des Sciences: Marseille, France, (1965)
8. Souriau, J.-M.: Réalisations d’algèbres de Lie au moyen de variables dynamiques. Il Nuovo
Cim. A 49, 197–198 (1967). https://doi.org/10.1007/bf02739084
9. Souriau, J.-M.: Déﬁnition covariante des équilibres thermodynamiques, Supplemento al
Nuovo cimento, vol. IV no. 1, pp. 203–216 (1966)
10. Souriau, J.-M.: Thermodynamique et géométrie. In: Bleuler, K., Reetz, A., Petry, H.R.
(eds.) Differential Geometry Methods in Mathematical Phys-ics II, pp. 369–397. Springer,
Heidelberg (1978). https://doi.org/10.1007/BFb0063682
11. Souriau, J.-M.: La structure symplectique de la mécanique décrite par Lagrange en 1811.
Math. Sci. Hum. (94), 45–54 (1986)
12. Souriau, J.-M.: Grammaire de la nature (1996)
13. Iglesias, P.: Thermodynamique géométrique appliquée aux conﬁguration tournantes en
astrophysique, Thèse de 3ème cycle, Université de Provence, 9 April 1979
14. Iglesias, P.: Itinéraire d’un mathématicien : Un entretien avec Jean-Marie Souriau, Le journal
de Maths des élèves, ENS Lyon, 1 October 1995
15. Gallisssot, F.: Les formes extérieures en mécanique. Annales de l’Institut Fourier 4, 145–297
(1952)
16. Blanc-Lapierre, A., Casal, P., Tortrat, A.: Méthodes mathématiques de la mécanique
statistique, Masson, Paris (1959)
17. Noether, E.: Invariante Variationsprobleme. Nachrichten von der Königlichen Gesellschaft
der Wissenschaften zu Göttingen, Mathematisch-physikalische Klasse, pp. 235–257 (1918)
18. Lagrange, J.-L.: Mécanique analytique. La veuve Desaint, Paris (1808)
19. Kosmann-Schwarzbach, Y.: Siméon-Denis Poisson, Les Mathématiques au Service de la
Science; Ecole Polytechnique: Palaiseau, France (2013)
20. Barbaresco, F.: Lie group statistics and lie group machine learning based on souriau lie
groups thermodynamics & koszul-souriau-ﬁsher metric: new entropy deﬁnition as generalized
casimir invariant function in coadjoint representation. Entropy 22, 642 (2020)
21. Barbaresco, F., Gay-Balmaz, F.: Lie group cohomology and (multi)symplectic integrators:
new geometric tools for lie group machine learning based on souriau geometric statistical
mechanics. Entropy 22, 498 (2020)
22. Barbaresco, F.: Souriau-Casimir Lie Groups Thermodynamics & Machine Learning, Joint
Structures and Common Foundations of Statistical Physics, Information Geometry and
Inference for Learning, Les Houches Summer Week SPIGL’20, 27 July 2020
23. Barbaresco, F.: Lie Groups Thermodynamics & Souriau-Fisher Metric, SOURIAU 2019
conference, Institut Henri Poincaré, 31 May 2019
24. Barbaresco, F.: Souriau-Casimir Lie Groups Thermodynamics & Machine Learning,
SPIGL’20 Proceedings, Les Houches Summer Week on Joint Structures and Common Foun-
dations of Statistical Physics, Information Geometry and Inference for Learning. Springer
Proceedings in Mathematics & Statistics (2021)
25. Libermann, P., Marle, C.-M.: Symplectic Geometry and Analytical Mechanics; Reidel:
Kufstein, Austria (1987)
26. Marle, C.M.: Géométrie Symplectique et Géométrie de Poisson; Calvage & Mounet: Paris,
France (2018)

50
F. Barbaresco
27. Marle, C.-M.: From tools in symplectic and poisson geometry to J.-M. Souriau’s theories of
statistical mechanics and thermodynamics. Entropy 18, 370 (2016). https://doi.org/10.3390/
e18100370
28. Marle, C.-M.: Projection Stéréographique et Moments, Hal-02157930, Version 1; June 2019.
https://hal.archives-ouvertes.fr/hal-02157930/. Accessed 31 May 2020
29. Marle, C.-M.: On Generalized Gibbs States of Mechanical Systems with Symmetries, arXiv:
2012.00582v2 [math.DG], 13 January 2021
30. Koszul,J.-L.,Zou,Y.M.:IntroductiontoSymplecticGeometry.SpringerScienceandBusiness
Media LLC, Berlin/Heidelberg (2019)
31. De Saxcé, G., Marle C-M.: Présentation du livre de Jean-Marie Souriau “Structure des
systèmes dynamiques”, preprint, April 2020
32. De Saxcé, G., Vallée C.: Galilean Mechanics and Thermodynamics of Continua. Wiley (2016)
33. De. Saxcé, G.: Link between lie group statistical mechanics and thermodynamics of continua.
Entropy 18, 254 (2016)
34. Saxcé, G.: Euler-poincaré equation for lie groups with non null symplectic cohomology.
application to the mechanics. In: Nielsen, F., Barbaresco, F. (eds.) GSI 2019. LNCS, vol.
11712, pp. 66–74. Springer, Cham (2019). https://doi.org/10.1007/978-3-030-26980-7_8
35. Cartier, P.: Some Fundamental Techniques in the Theory of Integrable Systems,
IHES/M/94/23, SW9421 (1994). https://cds.cern.ch/record/263222/ﬁles/P00023319.pdf.
Accessed 31 May 2020
36. Dacunha-Castelle, D., Gamboa, F.: Maximum d’entropie et problème des moments Annales
de l’I.H.P., section B, tome 26, no. 4, pp. 567–596 (1990)
37. Balian, R., Alhassid, Y., Reinhardt, H.: Dissipation in many-body systems: a geometric
approach based on information theory. Phys. Rep. 131, 1–146 (1986)
38. Balian, R.: From Microphysics to Macrophysics, vol. 1–2. Springer Science and Business
Media LLC, Berlin/Heidelberg (1991)
39. Balian, R., Valentin, P.: Hamiltonian structure of thermodynamics with gauge. Eur. Phys. J.
B 21, 269–282 (2001)
40. Balian, R.: The entropy-based quantum metric. Entropy 16, 3878–3888 (2014)
41. Balian, R.: François Massieu et les Potentiels Thermodynamiques, Évolution des Disciplines
et Histoire des Découvertes; Académie des Sciences: Paris, France (2015)
42. Barbaresco, F.: Entropy Geometric Structure as Casimir Invariant Function in Coadjoint Rep-
resentation: Geometric Theory of Heat & Information based on Souriau Lie Groups Ther-
modynamics and Lie Algebra Cohomology, Encyclopedia of Entropy Across the Disciplines.
World Scientiﬁc (2021)
43. Souriau, J.-M.: On Geometric Dynamics, Discrete and Continuous Dynamical Systems, vol.
19, no. 3, pp. 595–607, November 2007
44. Barbaresco, F.: Souriau exponential map algorithm for machine learning on matrix lie groups.
In: Nielsen, F., Barbaresco, F. (eds.) GSI 2019. LNCS, vol. 11712, pp. 85–95. Springer, Cham
(2019). https://doi.org/10.1007/978-3-030-26980-7_10
45. Barbaresco, F.: Koszul lecture related to geometric and analytic mechanics, Souriau’s Lie
group thermodynamics and information geometry. Inf. Geom. (2021)
46. Barbaresco, F.: Invariant Koszul Form of Homogeneous Bounded Domains and Information
Geometry Structures
47. Barbaresco, F.: Archetypal model of entropy as invariant casimir function in coadjoint rep-
resentation and geometric heat fourier equation. In: GSI’21 Conference, Paris Sorbonne
University, July 2021

Part II: Lie Group Geometry
and Diffeological Model of Statistical
Physics and Information Geometry

Souriau-Casimir Lie Groups Thermodynamics
and Machine Learning
Frédéric Barbaresco(B)
Thales Land and Air Systems, Voie Pierre-Gilles de Gennes, 91470 Limours, France
frederic.barbaresco@thalesgroup.com
Abstract. In 1969, Jean-Marie Souriau introduced a “Lie Groups Thermodynam-
ics”intheframeworkofSymplecticmodelofStatisticalMechanics.ThisSouriau’s
model considers the statistical mechanics of dynamic systems in their “space of
evolution” associated to a homogeneous symplectic manifold by a Lagrange 2-
form, and deﬁnes in case of non-null cohomology (non equivariance of the coad-
joint action on the moment map with appearance of an additional cocyle) a Gibbs
density(ofmaximumEntropy)thatiscovariantundertheactionofdynamicgroups
of physics (eg., Galileo’s group in classical physics). Souriau method could then be
applied on Lie Groups to deﬁne a covariant maximum Entropy density by Kirillov
representation theory. Based on this model, we will introduce a geometric char-
acterization of Entropy as a generalized Casimir invariant function in coadjoint
representation, and Massieu characteristic function, dual of Entropy by Legendre
transform, as a generalized Casimir invariant function in adjoint representation,
where Souriau cocycle is a measure of the lack of equivariance of the moment
mapping. The dual space of the Lie algebra foliates into coadjoint orbits that are
also the level sets on the Entropy. The information manifold foliates into level
sets of the Entropy that could be interpreted in the framework of Thermodynam-
ics by the fact that motion remaining on this complex surfaces is non-dissipative,
whereas motion transversal to these surfaces is dissipative. We will explain the 2nd
Principle in thermodynamics by deﬁnite positiveness of Souriau tensor extending
the (Koszul-)Fisher metric from Information Geometry.
Keywords: Symplectic geometry · Statistical physics · Moment map ·
Thermodynamics · Lie groups · Representation theory
1
Preamble
“Under this aspiration, physics which was ﬁrst a science of “agents” must become
a science of “media”. It is by addressing new media that we can hope to push the
diversiﬁcation and analysis of phenomena to the point of provoking their ﬁne and
complex geometrization, which is truly intrinsic… Without doubt, reality has not
yet delivered all of its models to us, but we already know that it cannot have more
of them than the one assigned to it by mathematical group theory.”
Gaston Bachelard, Etude sur l’Evolution d’un problème de Physique –La
propagation thermique dans les solides, 1928
© The Author(s), under exclusive license to Springer Nature Switzerland AG 2021
F. Barbaresco and F. Nielsen (Eds.): SPIGL 2020, PROMS 361, pp. 53–83, 2021.
https://doi.org/10.1007/978-3-030-77957-3_3

54
F. Barbaresco
We will introduce ﬁrst an extended deﬁnition of classical “Gauss density”, as intro-
duced by Jaynes, in term of density of Maximum Entropy based on Souriau Lie Groups
Thermodynamics (symplectic model of Statistical Physics) [2–5]. First, we remind the
classical Euclidean case, where the Entropy S(η) could be deﬁned as the Legendre trans-
form of minus the log-partition function (θ) = −log

R
e−⟨θ,y⟩dy (deﬁned by Laplace
transform on real axis R) by the following equation S(η) = ⟨θ, η⟩−(θ) with ηi =
∂(θ)
∂θi
and θi = ∂S(η)
∂ηi . The next step was to explain how to extend the log-partition
function for Lie groups. We will then consider the Laplace transform in the framework
of Lie group representation theory as introduced by Alexander Kirillov [7] and Geo-
metric Statical Mechanics as modeled by Jean-Marie Souriau [2–5]. It will preserve the
same Legendre structure, and will deﬁne the Entropy S(Q), parametrized on the dual
space of the Lie algebra Q ∈g∗(called geometric heat), as Legendre transform of minus
of the log-partition function (β) = −log

M
e−⟨J(ξ),β⟩dλω, parametrized on the Lie
algebra by β ∈g (called geometric Planck Temperature), from a Laplace transform
deﬁned on the homogeneous symplectic manifold (associated to the Lie group by the
Kirillov-Kostant-Souriau 2-form called KKS 2-form in the litterature). By introducing
the moment map J: M →g∗, fundamental tool of representation theory introduced by
Souriau, we will able to deﬁne the log-partition function on the coadjoint orbit of the Lie
group, (β) = −log

g∗e−⟨J(ξ),β⟩dλω. The Entropy will be then given by the Legendre
transform S(Q) = ⟨Q, β⟩−(β) with Q = ∂(β)
∂β
∈g∗and β = ∂S(Q)
∂Q
∈g. We
will ﬁnally introduce the Gauss density for Lie groups as the density that maximizes
this Entropy S(Q) under the constraint of its associated ﬁrst moment Q = ∂(β)
∂β
=

M
J(ξ)pGibbs(ξ)dλω. The Gauss density is then established by analogy with thermody-
namics as the Gibbs density pGibbs(ξ) = e(β)−⟨J(ξ),β⟩=
e−⟨J(ξ),β⟩

M
e−⟨J(ξ),β⟩dλω . But this is not
enough, because this density is not given in the good parametrization. We will propose
to express the Gibbs density with respect to the 1st statistical moment Q (statistical
mean of moment map) by inverting the relation Q = ∂(β)
∂β
= 
(β). The Gibbs density
pGibbs,Q(ξ) = e(β)−

J(ξ),
−1(Q)

with β = 
−1(Q) will provide the extended deﬁnition
of Gauss density in ﬁnal good parametrization.
In a second step, we will introduce a deeper study of Souriau model structure [12–15].
We will observe that Souriau Entropy S(Q) deﬁned on coadjoint orbit of the group has
a property of invariance S

Ad#
g (Q)

= S(Q) with respect to Souriau afﬁne deﬁnition
of coadjoint action Ad#
g (Q) = Ad∗
g (Q) + θ(g) where θ(g) is called the Souriau cocyle.
In the framework of Souriau Lie groups Thermodynamics, we will then characterize
the Entropy as a generalized Casimir invariant function in coadjoint representation, and
Massieu characteristic function (or log-partition function), dual of Entropy by Legendre
transform, as a generalized Casimir function in adjoint representation. When M is a
Poisson manifold, a function on M is a Casimir function if and only if this function is
constant on each symplectic leaf (the non-empty open subsets of the symplectic leaves
are the smallest embedded manifolds of M which are Poisson submanifolds). Classically,

Souriau-Casimir Lie Groups Thermodynamics and Machine Learning
55
the Entropy is deﬁned axiomatically as Shannon or von Neumann Entropies without any
geometric structures constraints. In this paper, the Entropy will be presented as solution
of the Casimir equation

ad∗
∂S
∂Q Q

j
+ 

∂S
∂Q

j = Ck
ijad∗
∂S
∂Q
iQk + 
j = 0, where it
appears in case of non-null cohomology that 
(X ) = Teθ(X (e)) with ˜
(X , Y) =
⟨
(X ), Y⟩= J[X ,Y] −{JX , JY } (non-equivariance of coadjoint operator on the moment
map), with θ(g) the Souriau Symplectic cocycle. The dual space of the Lie algebra
foliates into coadjoint orbits that are also the level sets on the Entropy. The KKS (Kostant-
Kirillov Souriau) 2-form that associates a structure of homogeneous symplectic manifold
to coadjoint orbits, will be linked with, what we will call, the Souriau-Koszul-Fisher
metric. The information manifold foliates into level sets of the Entropy that could be
interpreted in the framework of Thermodynamics by the fact that motion remaining on
this complex surfaces is non-dissipative, whereas motion transversal to these surfaces
is dissipative, where the dynamic is given by dQ
dt
= {Q, H} ˜
 = ad∗
∂H
∂Q Q + 

∂H
∂Q

where H is an arbitrary Hamiltonian and with stable equilibrium when H = S ⇒dQ
dt =
{Q, S} ˜
 = ad∗
∂S
∂Q Q + 

∂S
∂Q

= 0. We will also observed that dS = ˜
β

∂H
∂Q , β

dt
where ˜
β

∂H
∂Q , β

= ˜

∂H
∂Q , β

+
	
Q,

∂H
∂Q , β
 
, showing that Entropy production is
linked with Souriau tensor related to Fisher metric.
The Casimir equations [1] that we will introduce in non-zero cohomology [21] case
are consequences of the constancy of the Entropy on adjoint orbits of the Lie algebra
and of the equivariance of the map between the set of generalized temperatures and
the dual space of the Lie algebra, as introduced by Jean-Marie in his 1974 paper. We
will explain this fact in the paper by starting elaboration of Casimir equations from
the Souriau equation. Casimir equations will be then presented in this context, as a
fully equivalent form written in a new way, especially in the framework of Souriau Lie
Groups Thermodynamics. Souriau has not observed that the Entropy is an invariant
Casimir function in coadjoint representation, but we can assume that he was fully aware
of this invariant structure.
From Souriau equation ⟨Q, [β, Z]⟩+ ˜
(β, Z) = 0 published in 1974, we will rewrite
this equation on a Casimir form ad∗
∂S
∂Q Q + 

∂S
∂Q

= 0 in case of non-null cohomol-
ogy. This equation preserves the geometric structures included in Souriau equation but
allow us to consider the Entropy from the point of view of Casimir invariant func-
tion. The concept of Entropy and the concept of Casimir function were, for the time
being, two disjoint concepts that have been developed independently in the past. There
is a large literature on Casimir function, especially the russian one that have character-
ized properties of Casimir function. We refer to Igor V. Shirokov who has proposed a
method for constructing invariants of the coadjoint representation of Lie groups with an
arbitrary dimension and structure based on local symplectic coordinates on the coad-
joint orbits. With Oleg L. Kurnyavko, Igor V. Shirokov [30–33] has also proposed a
general method for constructing invariant Casimir functions. A.T. Fomenko and V.V.
Troﬁmov have also deeply studied Casimir functions (but in case of null cohomol-
ogy) and have developed the following equation that we can write for Entropy in null

56
F. Barbaresco
cohomology case S

Ad∗
etξ Q

= S(Q) +
∞

n=1
(−φ(ξ))nS
n!
(Q).tn with φ : g →Vec()
a representation of Lie algebras deﬁned on basis (e1, e2, ..., en) in g. We will refer
to consequences of this new deﬁnition of Entropy as an invariant Casimir function:
the associated Euler-Poincaré equation dQ
dt
= ad∗
∂H
∂Q Q + 

∂H
∂Q

and the stochas-
tic extension [13] based on a new Stratonovich differential equation for the stochas-
tic process given by the following relation by mean of Souriau’s symplectic cocycle
dQ +

ad∗
∂H
∂Q Q + 

∂H
∂Q

dt +
N
i=1

ad∗
∂Hi
∂Q
Q + 

∂Hi
∂Q

◦dWi(t) = 0.
Based on Information Geometry [27, 28], coupling Hendrik Casimir model [1] and
Jean-Marie Souriau’s Symplectic model of Statistical Physics [2–6] (translated in [36]),
we have established based on Kirillov’s theory of Lie Group representation [7–11] new
fundamental equations [12–15] characterizing the geometric structure of Entropy as
invariant Casimir function in coadjoint representation. Souriau’s Lie groups thermody-
namics is a new active domain at the interface between Statistical Physics (as studied
by R.F. Streater [29], C.M. Marle [16–20] and G. de Saxcé [23–25]) and Information
Geometry [12–15, 28]. This geometric model of Entropy will open the door to new
research topics to consolidate links with invariants theory of Lie groups representations
[30–35] and theory of Integrable Systems [26].
2
Souriau Lie Groups Thermodynamics and Covariant Gibbs
Density
We identify the Riemanian metric introduced by Souriau based on cohomology, in the
framework of “Lie groups thermodynamics” as an extension of classical Fisher metric
introduced in information geometry. We have observed that Souriau metric preserves
Fisher metric structure as the Hessian of the minus logarithm of a partition function,
where the partition function is deﬁned as a generalized Laplace transform on a sharp
convex cone. Souriau’s deﬁnition of Fisher metric extends the classical one in case of
Lie groups or homogeneous manifolds. Souriau has developed this “Lie groups thermo-
dynamics” theory in the framework of homogeneous symplectic manifolds in geometric
statistical mechanics for dynamical systems, but as observed by Souriau, these model
equations are no longer linked to the symplectic manifold but equations only depend on
the Lie group and the associated cocycle. With the Souriau approach, the Fisher metric
couldbeextended,bySouriau-Fishermetric,todesignnaturalgradientsonhomogeneous
manifolds. Information geometry has been derived from invariant geometrical structure
involved in statistical inference. The Fisher metric deﬁnes a Riemannian metric as the
Hessian of two dual potential functions, linked to dually coupled afﬁne connections in a
manifold of probability distributions. With the Souriau model, this structure is extended
preserving the Legendre transform between two dual potential function parametrized in
Lie algebra of the group acting transentively on the homogeneous manifold.

Souriau-Casimir Lie Groups Thermodynamics and Machine Learning
57
2.1
Geometric Structure of Information
Classically, to optimize the parameter θ of a probabilistic model, based on a sequence
of observations yt, is an online gradient descent:
θt ←θt−1 −ηt
∂lt(yt)T
∂θ
(1)
with learning rate ηt, and the loss function lt = −log p(yt/ˆyt). This simple gradient
descent has a ﬁrst drawback of using the same non-adaptive learning rate for all parameter
components, and a second drawback of non invariance with respect to parameter re-
encoding inducing different learning rates. Amari has introduced the natural gradient to
preserve this invariance to be insensitive to the characteristic scale of each parameter
direction. The gradient descent could be corrected by I(θ)−1 where I is the Fisher
information matrix with respect to parameter θ, given by:
I(θ) =

gij

with gij =

−Ey−p(y/θ)
∂2 log p(y/θ)
∂θi∂θj

ij
(2)
with natural gradient:
θt ←θt−1 −ηtI(θ)−1 ∂lt(yt)T
∂θ
(3)
Amari has proved that for exponential family, the Fisher information matrix is deﬁned
by:
gij = −
 ∂2
∂θi∂θj

ij
with (θ) = −log

R
e−⟨θ,y⟩dy
(4)
and the dual potential, the Shannon Entropy, is given by the Legendre transform:
S(η) = ⟨θ, η⟩−(θ) with ηi = ∂(θ)
∂θi
and θi = ∂S(η)
∂ηi
(5)
We can observe that (θ) = −log

R
e−⟨θ,y⟩dy = −log ψ(θ) is linked with the cumulant
generating function.
J.L. Koszul and E. Vinberg have introduced an afﬁnely invariant Hessian metric on
a sharp convex cone  through its characteristic function, considering its dual ∗:
(θ) = −log

∗
e−⟨θ,y⟩dy = −log ψ(θ) with θ ∈ sharp convex cone
ψ(θ) =

∗
e−⟨θ,y⟩dy with Koszul−Vinberg Characteristic function
(6)
Jean-Louis Koszul has introduced the following forms.

58
F. Barbaresco
1st Koszul form α:
α = d(θ) = −d log ψ(θ)
(7)
2nd Koszul form γ :
γ = Dα = Dd log ψ(θ)
(8)
with the following property of positive deﬁnitiveness:
(Dd log ψ(x)) (u) =
1
ψ(u)2
⎡
⎢⎣

∗
F(ξ)2dξ ·

∗
G(ξ)2dξ −
⎛
⎝

∗
F(ξ) · G(ξ)dξ
⎞
⎠
2⎤
⎥⎦> 0
with F(ξ) = e−1
2 ⟨x,ξ⟩
and G(ξ) = e−1
2 ⟨x,ξ⟩⟨u, ξ⟩
(9)
Koszul has deﬁned the following diffeomorphism:
η = α = −d log ψ(θ) =

∗
ξpθ(ξ)dξ with pθ(ξ) =
e−⟨ξ,θ⟩

∗e−⟨ξ,θ⟩dξ
(10)
with preservation of Legendre transform:
S(η) = ⟨θ, η⟩−(θ) with η = d(θ) and θ = dS(η)
(11)
2.2
Lie Groups Thermodynamics and Souriau-Koszul-Fisher Metric
These relations have been extended by Jean-Marie Souriau in geometric statistical
mechanics, where he developed a “Lie groups thermodynamics” of dynamical systems
where the (maximum Entropy) Gibbs density is covariant with respect to the action of
the Lie group. In the Souriau model, previous structures of information geometry are
preserved:
I(β) = −∂2
∂β2 with (β) = −log

M
e−⟨U(ξ),β⟩dλω and U : M →g∗
(12)
S(Q) = ⟨Q, β⟩−(β) with Q = ∂(β)
∂β
∈g∗and β = ∂S(Q)
∂Q
∈g
(13)
In the Souriau Lie groups thermodynamics model, β is a “geometric” (Planck) temper-
ature, element of Lie algebra g of the group, and Q is a “geometric” heat, element of
the dual space of the Lie algebra g∗of the group. Souriau has proposed a Riemannian
metric that we have identiﬁed as a generalization of the Fisher metric:
I(β) = [gβ] with gβ([β, Z1], [β, Z2]) = 
β(Z1, [β, Z2])
(14)
with 
β(Z1, Z2) = 
(Z1, Z2) +

Q, adZ1(Z2)

where adZ1(Z2) = [Z1, Z2]
(15)

Souriau-Casimir Lie Groups Thermodynamics and Machine Learning
59
Souriau has proved that all co-adjoint orbit of a Lie Group given by OF
=

Ad∗
g F, g ∈G

subset of g∗, F ∈g∗carriesanaturalhomogeneoussymplecticstructure
by a closed G-invariant 2-form.
If we deﬁne K = Ad∗
g =
 
Adg−1
!∗and K∗(X ) = −(adX )∗with
	
Ad∗
g F, Y

=

F, Adg−1Y

, ∀g ∈G, Y ∈g, F ∈g∗where if X ∈g, Adg(X ) = gXg−1 ∈g, the G-
invariant 2-form is given by the following expression σ(adX F, adY F) = BF(X , Y) =
⟨F, [X , Y]⟩, X , Y ∈g. Souriau Foundamental Theorem is that «Every symplectic
manifold on which a Lie group acts transitively by a Hamiltonian action is a cov-
ering space of a coadjoint orbit». We can observe that for Souriau model, Fisher
metric is an extension of this 2-form in non-equivariant case gβ([β, Z1], [β, Z2]) =

(Z1, [β, Z2]) + ⟨Q, [Z1, [β, Z2]]⟩.
The Souriau additional term 
(Z1, [β, Z2]) is generated by non-equivariance through
Symplectic cocycle. The tensor 
 used to deﬁne this extended Fisher metric is deﬁned
by the moment map J(x), application from M (homogeneous symplectic manifold) to
the dual space of the Lie algebra g∗, given by:

(X , Y) = J[X ,Y] −{JX , JY }
(16)
with J : M →g∗
such that JX (x) = ⟨J(x), X ⟩, X ∈g
(17)
This tensor 
 is also deﬁned in tangent space of the cocycle θ(g) ∈g∗(this cocycle
appears due to the non-equivariance of the coadjoint operator Ad∗
g , action of the group
on the dual space of the lie algebra; the action of the group on the dual space of the
Lie algebra is modiﬁed with a cocycle so that the momentum map becomes equivariant
relative to this new afﬁne action):
Q
 
Adg(β)
!
= Ad∗
g (Q) + θ(g)
(18)
θ(g) ∈g∗is called nonequivariance one-cocycle, and it is a measure of the lack of
equivariance of the moment map.

(X , Y) : g × g →ℜ
with 
(X ) = Teθ(X (e))
X, Y →
⟨
(X ), Y⟩
(19)
The cocycle should verify:
θ(st) = J((st).x) −Ad∗
stJ(x)
θ(st) =

J(s.(t.x)) −Ad∗
s J(t.x)

+

Ad∗
s J(t.x) −Ad∗
s Ad∗
t J(x)

θ(st) = θ(s) + Ad∗
s

J(t.x) −Ad∗
t J(x)

θ(st) = θ(s) + Ad∗
s θ(t)
(20)
We can also compute tangent of one-cocycle θ at neutral element, to compute 2-cocycle

:
ζ ∈g,
θζ(s) = ⟨θ(s), ζ⟩= ⟨J(s.x), ζ⟩−

Ad∗
s J(x), ζ


60
F. Barbaresco
θζ(s) = ⟨J(s.x), ζ⟩−

J(x), Ads−1ζ

Teθζ(ξ) =

TxJ.ξp(x), ζ

+

J(x), adξζ

with ξp = X⟨J,ξ⟩
Teθζ(ξ) = X⟨J(x),ξ⟩[⟨J(x), ζ⟩] + ⟨J(x), [ξ, ζ]⟩
Teθζ(ξ) = −{⟨J, ξ⟩, ⟨J, ζ⟩} + ⟨J(x), [ξ, ζ]⟩= 
(ξ)
(21)
We can also write: TxJ
 
ξp(x)
!
= −ad∗
ξ J(x) + 
(ξ, .)
By differentiating the equation on afﬁne action, we have:
dJ(Xx) = adX J(x) + dθ(X ), x ∈M , X ∈g
⟨dJ(Xx), Y⟩= ⟨adX J(x), Y⟩+ ⟨dθ(X ), Y⟩, x ∈M , X , Y ∈g
⟨dJ(Xx), Y⟩= ⟨J(x), [X , Y]⟩+ ⟨dθ(X ), Y⟩= {⟨J, X ⟩, ⟨J, Y⟩}(x)
⟨J(x), [X , Y]⟩−{⟨J, X ⟩, ⟨J, Y⟩}(x) = −⟨dθ(X ), Y⟩
(22)
It can be then deduced that the tensor could be also written:

(X , Y) = J[X ,Y] −{JX , JY } = −⟨dθ(X ), Y⟩, X , Y ∈g
(23)
with the cocycle property:

([X , Y], Z) + 
([X , Y], Z) + 
([X , Y], Z) = 0,
X , Y, Z ∈g
(24)
By noting the action of the group on the dual space of the Lie algebra:
G × g∗→g∗, (s, ξ) →sξ = Ad∗
s ξ + θ(s)
(25)
Associativity is also derived:
(s1s2)ξ = Ad∗
s1s2ξ + θ(s1s2) = Ad∗
s1Ad∗
s2ξ + θ(s1) + Ad∗
s1θ(s2)
(s1s2)ξ = Ad∗
s1
 
Ad∗
s2ξ + θ(s2)
!
+ θ(s1) = s1(s2ξ),
∀s1, s2 ∈G, ξ ∈g∗
(26)
This study of the moment map J equivariance, and the existence of an afﬁne action of
G on g∗, whose linear part is the coadjoint action, for which the moment J is equiv-
ariant, is at the cornerstone of Souriau theory of geometric mechanics and Lie groups
thermodynamics.
2.3
Souriau Entropy and Souriau-Fisher-Koszul Metric Invariance
and Covariant Souriau Gibbs Density
In Souriau’s Lie groups thermodynamics, the invariance by re-parameterization in infor-
mation geometry has been replaced by invariance with respect to the action of the group.
When an element of the group g acts on the element β ∈g of the Lie algebra, given by
adjoint operator Adg. Under the action of the group Adg(β), the Entropy S(Q) and the
Fisher metric I(β) are invariant:
β ∈g →Adg(β) ⇒
"S

Q
 
Adg(β)
!
= S(Q)
I

Adg(β)

= I(β)
(27)

Souriau-Casimir Lie Groups Thermodynamics and Machine Learning
61
In the framework of Lie group action on a symplectic manifold, equivariance of moment
map could be studied to prove that there is a unique action a(.,.) of the Lie group G on
the dual g∗of its Lie algebra for which the moment map J is equivariant, that means for
each x ∈M :
J
 
Adg(x)
!
= Ad∗
g (J(x)) + θ(g)
(28)
When coadjoint action is not equivariant, the symmetry is broken, and new “cohomolog-
ical” relations should be veriﬁed in Lie algebra of the group. A natural equilibrium state
will thus be characterized by an element of the Lie algebra of the Lie group, determining
the equilibrium temperature β. The Entropy s(Q), parametrized by Q the geometric heat
(mean of moment map J, element of the dual space of the Lie algebra) is deﬁned by
the Legendre transform of the Massieu potential (β) parametrized by β ((β) is the
minus logarithm of the partition function ψ(β)).
A Gibbs state, in the usual sense, is a statistical state at which the Entropy is stationary
with respect to all inﬁnitesimal variations of the statistical state for which the mean
value of the energy remains constant. In the sense of Souriau, a generalized Gibbs state
is a statistical state at which the Entropy is stationary with respect to all inﬁnitesimal
variations of the statistical state for which the mean value of the moment map remains
constant. This generalization is very natural, since the energy can be considered as
the moment map of the Hamiltonian action of the one-dimensional Lie group of time
translations. Furthermore, each generalized Gibbs state is associated to an element of
the Lie algebra of the group, called by Souriau a generalized temperature, and that the
set of possible generalized temperature is not, in general the whole Lie algeba, but an
open convex subset of the Lie algebra, which may be empty, for which some integrals
encountered in the expression of the generalized Gibbs state are normally convergent.
So, for some Lie groups, generalized Gibbs states do not exist, and there is no Souriau
Lie groups thermodynamics.
Souriau has then deﬁned a Gibbs density that is covariant under the action of the
group:
pGibbs(ξ) = e(β)−⟨U(ξ),β⟩=
e−⟨U(ξ),β⟩

M
e−⟨U(ξ),β⟩dλω , with (β) = −log

M
e−⟨U(ξ),β⟩dλω
Q = ∂(β)
∂β
=

M
U(ξ)e−⟨U(ξ),β⟩dλω

M
e−⟨U(ξ),β⟩dλω
=

M
U(ξ)p(ξ)dλω
(29)
We can express the Gibbs density with respect to Q by inverting the relation Q = ∂(β)
∂β
=

(β). Then pGibbs,Q(ξ) = e(β)−

U(ξ),
−1(Q)

with β = 
−1(Q) (Figs. 1, 2).
Souriau completed his “geometric heat theory” by introducing a 2-form in the Lie
algebra, that is a Riemannian metric tensor in the values of adjoint orbit of β, [β, Z]
with Z an element of the Lie algebra. This metric is given for (β, Q):
gβ([β, Z1], [β, Z2]) = ⟨
(Z1), [β, Z2]⟩+ ⟨Q, [Z1, [β, Z2]]⟩
(30)
where 
 is a cocycle of the Lie algebra, deﬁned by 
 = Teθ with θ a cocycle of the Lie
group deﬁned by θ(g) = J
 
Adgβ
!
−Ad∗
g (Q).

62
F. Barbaresco
Fig. 1. Fundamental Equation of Souriau Lie Groups Thermodynamics. Q is the geometric heat
in dual Lie algebra, β is the geometric temperature in Lie algebra.
Fig. 2. Souriau Model of Lie Groups Thermodynamics.
We observe that Souriau Riemannian metric, introduced with symplectic cocycle,
is a generalization of the Fisher metric, that we call the Souriau-Fisher metric, that
preserves the property to be deﬁned as a Hessian of the partition function logarithm
gβ = −∂2
∂β2 = ∂2 log ψ
∂β2
as in classical information geometry. We will establish the
equality of two terms, between Souriau deﬁnition based on Lie group cocycle 
 and
parameterized by “geometric heat” Q (element of the dual space of the Lie algebra)
and “geometric temperature” β (element of Lie algebra) and Hessian of characteristic

Souriau-Casimir Lie Groups Thermodynamics and Machine Learning
63
function 
(β) = −log ψ(β) with respect to the variable β:
gβ([β, Z1], [β, Z2]) = ⟨
(Z1), [β, Z2]⟩+ ⟨Q, [Z1, [β, Z2]]⟩= ∂2 log ψ
∂β2
(31)
If one assumes that U(gξ) = Ad∗
g U(ξ) + θ(g) , g ∈G, ξ ∈M which means that
the energy U: M →g∗satisﬁes the same equivariance condition as the moment map
μ: M →g∗, then one has for g ∈G and β ∈ (subset where Laplace transform
converges):
ψ
 
Adgβ
!
=

M
e−⟨U(ξ),Adgβ⟩dλ(ξ) =

M
e
−
	
Ad−1
ξ−1U(ξ),β

dλ(ξ)
ψ
 
Adgβ
!
=

M
e−

U
 
g−1ξ
!
−θ
 
g−1!
,β

dλ(ξ) = e

θ
 
g−1!
,β
 
M
e−

U
 
g−1ξ
!
,β

dλ(ξ)
ψ
 
Adgβ
!
= e

θ
 
g−1!
,β

ψ(β)

 
Adgβ
!
= −log ψ
 
Adgβ
!
= (β) −
	
θ

g−1
, β

(32)
To consider the invariance of Entropy, we have to use the property that
Q(Adgβ) = Ad∗
g Q(β) + θ(g) = g.Q(β),
β ∈, g ∈G
(33)
So starting from Entropy deﬁnition by Legendre transform:
s(Q(β)) = ⟨Q(β), β⟩−(β)
(34)
We can prove the invariance under the action of the group:
s
 
Q
 
Adgβ
!!
=

Q
 
Adgβ
!
, Adgβ

−
 
Adgβ
!
s
 
Q
 
Adgβ
!!
=
	
Ad∗
g Q(β) + θ(g), Adgβ

−(β) +
	
θ

g−1
, β

s
 
Q
 
Adgβ
!!
=
	
Ad∗
g Q(β), Adgβ

−(β) +
	
Ad∗
g−1θ(g) + θ

g−1
, β

s
 
Q
 
Adgβ
!!
= s(Q(β)) using Ad∗
g−1θ(g) + θ

g−1
= θ

g−1g

= 0
(35)
For β ∈, let gβ be the Hessian form on Tβ ≡g with the potential (β) =
−log ψ(β). For X , Y ∈g, we deﬁne:
gβ(X , Y) = −∂2
∂β2 (X , Y) =
 ∂2
∂s∂t

s=t=0
log ψ(β + sX + tY)
(36)

64
F. Barbaresco
The positive deﬁnitiveness is given by Cauchy-Schwarz inequality:
gβ(X , Y) =
1
ψ(β)2
⎧
⎪⎪⎨
⎪⎪⎩

M
e−⟨U(ξ),β⟩dλ(ξ) ·

M
⟨U(ξ), X ⟩2e−⟨U(ξ),β⟩dλ(ξ)
−
'

M
⟨U(ξ), X ⟩e−⟨U(ξ),β⟩dλ(ξ)
(2
⎫
⎪⎪⎬
⎪⎪⎭
=
1
ψ(β)2
⎧
⎪⎪⎪⎨
⎪⎪⎪⎩

M
 
e−⟨U(ξ),β⟩/2!2dλ(ξ) ·

M
 
⟨U(ξ), X ⟩e−⟨U(ξ),β⟩/2!2dλ(ξ)
−
'

M
e−⟨U(ξ),β⟩/2 · ⟨U(ξ), X ⟩e−⟨U(ξ),β⟩/2dλ(ξ)
(2
⎫
⎪⎪⎪⎬
⎪⎪⎪⎭
≥0
(37)
We observe that gβ(X , X ) = 0 c if and only if ⟨U(ξ), X ⟩is independent of ξ ∈M ,
which means that the set {U(ξ); ξ ∈M } is contained in an afﬁne hyperplane in g∗
perpendicular to the vector X ∈g.
We will see that gβ = −∂2
∂β2 , that is a generalization of classical Fisher metric from
Information geometry, and will give the relation the Riemannian metric introduced by
Souriau.
gβ(X , Y) =
,
−∂Q
∂β (X ), Y
-
for X , Y ∈g
(38)
we have for any β ∈, g ∈G and Y ∈g:

Q
 
Adgβ
!
, Y

=

Q(β), Adg−1Y

+ ⟨θ(g), Y⟩
(39)
Let us differentiate the above expression with respect to g. Namely, we substitute g =
exp(tZ1), t ∈R and differentiate at t = 0. Then the left-hand side becomes
 d
dt

t=0
	
Q

β + t[Z1, β] + o

t2
, Y

=
,∂Q
∂β ([Z1, β]), Y
-
(40)
and the right-hand side is calculated as:

d
dt

t=0

Q(β), Y −t[Z1, Y] + o
 
t2!
+

θ
 
I + tZ1 + o
 
t2!!
, Y

= −⟨Q(β), [Z1, Y]⟩+ ⟨dθ(Z1), Y⟩
(41)
Therefore,
,∂Q
∂β ([Z1, β]), Y
-
= ⟨dθ(Z1), Y⟩−⟨Q(β), [Z1, Y]⟩
(42)
Substituting Y = −[β, Z2] to the above expression:
gβ([β, Z1], [β, Z2]) =
	
−∂Q
∂β ([Z1, β]), [β, Z2]

gβ([β, Z1], [β, Z2]) = ⟨dθ(Z1), [β, Z2]⟩+ ⟨Q(β), [Z1, [β, Z2]]⟩
(43)
We deﬁne then symplectic 2-cocycle and the tensor:

(Z1) = dθ(Z1)
˜
(Z1, Z2) = ⟨
(Z1), Z2⟩= J[Z1,Z2] −
.
Jz1, Jz2
/

Souriau-Casimir Lie Groups Thermodynamics and Machine Learning
65
Considering ˜
(Z1, Z2) = Q(β), [Z1, Z2] + ˜
(Z1, Z2), that is an extension of KKS
(Kirillov-Kostant-Souriau) 2 form in the case of non-null cohomology. Introduced by
Souriau, we can deﬁne this extension Fisher metric with 2-form of Souriau (Fig. 3):
gβ([β, Z1], [β, Z2]) = ˜
β(Z1, [β, Z2])
(44)
Fig. 3. Souriau-Fisher metric as extension of KKS 2-form in case of non-null Cohomogy.
As the Entropy is deﬁned by the Legendre transform of the characteristic function,
a dual metric of the Fisher metric is also given by the Hessian of “geometric Entropy”
S(Q) with respect to the dual variable given by Q: ∂2S(Q)
∂Q2 .
For the maximum Entropy density (Gibbs density), the following three terms coin-
cide: ∂2 log ψ
∂β2
that describes the convexity of the log-likelihood function, I(β) =
−E ∂2 log Pβ(ξ)
∂β2
the Fisher metric that describes the covariance of the log-likelihood gra-
dient, whereas I(β) = −E
0
(ξ −Q)(ξ −Q)T1
= Var(ξ) that describes the covariance
of the observables. We can also observe that the Fisher metric I(β) = −∂Q
∂β is exactly
the Souriau metric deﬁned through symplectic cocycle:
I(β) = ˜β(Z1[β, Z2]) = gβ([β, Z1], [β, Z2])
(45)
The Fisher metric I(β) = −∂2(β)
∂β2
= −∂Q
∂β has been considered by Souriau as a
generalization of “heat capacity”. Souriau called it K the “geometric capacity”.
R.F. Streater has studied in 1999 [29], Information Geometry for some Lie algebra
where for certain unitary representation of a Lie algebra, he has deﬁned the statistical
manifold of states as convex cone for which the partition function is ﬁnite, making
reference to Bogoliubov-Kubo-Mori metric. But Streater has only developed the case
with null cohomology for so(3) and sl(2,R) Lie algebras. Nevertheless, as observed
by R.F. Streater in his paper “Information Geometry for some Lie algebras” [29],
referring to Kirillov work and Roger Balian paper [28], “We can expect further natural
structures to arise in this case. Indeed, it is known that the dual to the Lie algebra, which
parametrizes the state-space in this case, foliates into coadjoint orbits; there are also the

66
F. Barbaresco
level sets on the Entropy; Kirillov form, and the BKM (Bogoliubov-Kubo-Mori) metric,
together makeeachorbit intoKähler space, alongthelines proposedbyKostant. Motion
along these holomorphic directions is nondissipative. The transversal to the orbits is a
real half-line, which represents the dissipative direction…We study the case of sl(2,R)
in the discrete series of representations. We show the information manifold foliates into
level sets of the Entropy, each being isometric to H, the Poincaré upper half-plane… The
states of constant Entropy are the hyperboloids and β is the dissipative coordinate…For
anintegrablesystemdescribedbyaLiealgebrainatraceablerepresentation,weﬁndthat
the information manifold foliates into complex spaces; the level sets of Entropy can be
given a complex structure by the method of Kostant. Motion remaining on the complex
surfaces is non dissipative, whereas motion transversal to these surfaces is dissipative.
In information geometry, the state is parametrized by the canonical coordinates. Which
function of them is measured by a thermometer? In our models, it is reasonable to
designate 1
2
β to be the temperature; it is a dissipative coordinate, and it increases with
time, showing that the system is thermalizing”.
3
New Entropy Characterization as Generalized Casimir Invariant
Function in Coadjoint Representation
In his 1974 paper, Jean-Marie Souriau has written ⟨Q, [β, Z]⟩+ ˜(β, Z) = 0. To prove
this equation, we have to consider the parametrized curve t →Adexp(tZ)β with Z ∈
g and t ∈R. The parameterized curve Adexp(tZ)β passes, for t = 0, through the point β,
since Adexp(0) is the identical map of the Lie Algebra g. This curve is in the adjoint orbit
of β. So by taking its derivative with respect to t, then for t = 0, we obtain a tangent
vector in β at the adjoint orbit of this point. When Z takes all possible values in g, the
vectors thus obtained generate all the vector space tangent in β to the orbit of this point:
d
 
Adexp(tZ)β
!
dt
33333
t=0
=
4
d
dβ ,
'
d
 
Adexp(tZ)β
!
dt
33333
t=0
(5
= ⟨Q, adZβ⟩=⟨Q, [Z, β]⟩
As we have seen before 
 
Adgβ
!
= (β) −

θ
 
g−1!
, β

. If we set g = exp(tZ),
we obtain 
 
Adexp(tZ)β
!
= (β) −⟨θ(exp(−tZ)), β⟩and by derivation with respect
to t at t = 0, we ﬁnally recover the equation given by Souriau: d(Adexp(tZ)β)
dt
333
t=0 =
⟨Q, [Z, β]⟩= −⟨dθ(−Z), β⟩with ˜
(X , Y) = −⟨dθ(X ), Y⟩.
Souriau has stopped by this last equation, the characterization of Group action on
Q = d
dβ . Souriau has observed that S

Q
 
Adg(β)
!
= S

Ad∗
g (Q) + θ(g)

= S(Q).
We propose to characterize more explicitly this invariance, by characterizing
Entropy as an invariant Casimir function in coadjoint representation.
From last Souriau equation, if we use the identities β = ∂S
∂Q, adβZ = [β, Z] and
˜(β, Z) = ⟨(β), Z⟩, then we can deduce that
,
ad∗
∂S
∂Q Q + 

∂S
∂Q

, Z
-
= 0,
∀Z. So,
Entropy S(Q) should verify ad∗
∂S
∂Q Q + 

∂S
∂Q

= 0, characterizes an invariant Casimir
function in case of non-null cohomology, that we propose to write with Poisson brackets,

Souriau-Casimir Lie Groups Thermodynamics and Machine Learning
67
{S, H} ˜
(Q) = 0 where {S, H} ˜
(Q) =
	
Q,

∂S
∂Q, ∂H
∂Q

+ ˜

∂S
∂Q, ∂H
∂Q

= 0, ∀H : g∗→
R, Q ∈g∗.
In a Poisson manifold, Casimir functions S ∈C∞(g∗), in case of null cohomology,
are functions whose Poisson brackets will vanish, {S, H}(Q) = 0, ∀S ∈C∞(g∗), Q ∈
g∗. In the dual of the Lie algebra of a connected Lie Group G, the Casimir functions are
the Ad∗-invariant functions, because if S, H ∈C∞(g∗) and Q ∈g∗, then {S, H}(Q) =
	
Q,

∂S
∂Q, ∂H
∂Q

=

Q, ad ∂S
∂Q
∂H
∂Q

=
,
ad∗
∂S
∂Q Q, ∂H
∂Q
-
vanishes for all H ∈C∞(g∗) if
and only if ad∗
∂S
∂Q Q = 0. A function S on g∗is Ad∗-invariant if g.S = S,
∀g ∈G
where Lie group G acts on functions on g∗by (g.S)(Q) = S

Ad∗
g Q

,
Q ∈g∗, S ∈
C∞(g∗), g ∈G, and where inﬁnitesimal characterizations of Ad∗-invariant functions
on g∗, d
dt S

Ad∗
exp(tx)Q

|t=0 =
	
ad∗
x Q, ∂S
∂Q

= −
,
ad∗
∂S
∂Q Q, x
-
. The symplectic leaves
of a Poisson manifold are contained in the connected components of the level sets of
the Casimir functions and Casimir function is constant on a symplectic leaf. Coadjoint
orbits lie on level sets of the Casimir functions, which are conserved quantities. Casimir
functions level sets are symplectic manifolds. Coadjoint motion of the moment map
Q(t) = Ad∗
g(t)Q(0) for a solution curve g(t) ∈C(G) take place on the intersections of
levels sets of the Hamiltonian and the Casimir functions. Alexis Arnaudon has studied
stochastic coadjoint processes whose solutions lie on coadjoint orbits.
We have observed that (S, H) ˜
(Q) =
	
Q,

∂S
∂Q, ∂H
∂Q

+ ˜

∂S
∂Q, ∂H
∂Q

= 0,
∀H :
g∗
→
R,
Q
∈
g∗, that shows that Souriau Entropy is a Casimir func-
tion in case with non-null cohomology when an additional cocycle should be
taken into account. Indeed, inﬁnitesimal variation is characterized by the follow-
ing differentiation:
d
dt S

Q

Adexp(tx)β
333
t=0 =
d
dt S

Ad∗
exp(tx)Q + θ(exp(tx))
333
t=0 =
−
,
ad∗
∂S
∂Q Q + 

∂S
∂Q

, x
-
. We recover extended Casimir equation in case of non-null
cohomologyveriﬁedbyEntropy,ad∗
∂S
∂Q Q+

∂S
∂Q

= 0,andthenthegeneralizedCasimir
condition {S, H} ˜
(Q) = 0. Hamiltonian motion on these afﬁne coadjoint orbits is given
by the solutions of the Lie-Poisson equations with cocycle.
The identiﬁcation of Entropy as an Invariant Casimir Function in Coadjoint represen-
tation is also important in Information Theory, because classically Entropy is introduced
axiomatically. With this new approach, we can build Entropy by constructing the Casimir
Function associated to the Lie group and also in case of non-null cohomology. Igor V.
Shirokov has proposed a method for constructing invariants of the coadjoint representa-
tion of Lie groups with an arbitrary dimension and structure based on local symplectic
coordinates on the coadjoint orbits. The idea of the method of constructing coadjoint

68
F. Barbaresco
invariants is to construct the canonical transition to the Darboux coordinates on the orbits
of the dual Lie algebra g∗of maximal dimension dual to the Lie algebra g of the Lie
group G. These relations provide invariants of the coadjoint representation of the Lie
group G.
This geometric framework uniﬁes several earlier works on the subject, including
Souriau’s symplectic model of statistical mechanics, and approaches developed in Infor-
mation Geometry and Quantum Information Geometry. This approach helps to identify
the common geometric structures appearing in various domains from statistical mechan-
ics to statistical learning. The emphasis is put on the role of the afﬁne equivariance with
respect to Lie group actions, as extension of the Fisher metric in presence of equivariance
and the associated Lie-Poisson equations with cocycle (afﬁne Lie-Poisson equations).
The Entropy of the Souriau model as a Casimir function can be used to apply a geo-
metric model for energy preserving Entropy production on Lie algebras. We can exploit
the geometric framework of this new equation to build geometric numerical integrator
schemes for some of the equations associated to Souriau’s model and its polysymplectic
extension. This new equation is important because it introduce new structure of dif-
ferential equations in case of non-null cohomology and for an arbitrary Hamiltonian
H : g∗→R: dQ
dt = ad∗
∂H
∂Q Q + 

∂H
∂Q

.
The equation dQ
dt
= ad∗
∂H
∂Q Q + 

∂H
∂Q

is important because it allows extending
stochastic perturbation of the Lie-Poisson equation with cocycle within the setting of
stochastic Hamiltonian dynamics, which preserves the afﬁne coadjoint orbits. We can
extend model for stochastic geometric modeling in ﬂuid dynamics via variational prin-
ciples. This extension results in new Stratonovich differential equation for stochastic
process dQ +

ad∗
∂H
∂Q Q + 

∂H
∂Q

dt +
N
i=1

ad∗
∂Hi
∂Q
Q + 

∂Hi
∂Q

· dWi(t) = 0.
This new equation is also very usefull for geometric symplectic Lie group integrator
for Lie-Poisson equations with cocycle that preserves the afﬁne coadjoint orbits for
general Hamiltonian. This equation is also very relevant in the framework of dynamics
with Casimir dissipation/production, to formulate a dynamical geometric model for
dissipation/production of this Casimir. This allows to extend the general Lie algebraic
approach developed for Casimir dissipation, to take into account of a cocycle, and to a
wider class of dissipation.
This equation dQ
dt = ad∗
∂H
∂Q Q + 

∂H
∂Q

could be used also to make the link with 2nd
principle of Thermodynamique, that will be deduced from positivity of Souriau-Fisher

Souriau-Casimir Lie Groups Thermodynamics and Machine Learning
69
metric:
S(Q) = ⟨Q, β⟩−(β) with dQ
dt = ad∗
∂H
∂Q Q + 

∂H
∂Q

dS
dt =
	
Q, dβ
dt

+
,
ad∗
∂H
∂Q Q + 

∂H
∂Q

, β
-
−d
dt
dS
dt =
	
Q, dβ
dt

+
,
ad∗
∂H
∂Q Q, β
-
+
	


∂H
∂Q

, β

−d
dt
dS
dt =
	
Q, dβ
dt

+
	
Q,

∂H
∂Q , β

+ ˜

∂H
∂Q , β

−d
dt
dS
dt =
	
Q, dβ
dt

+ ˜
β

∂H
∂Q , β

−
	
∂
∂β , dβ
dt

dS
dt =
	
Q, dβ
dt

+ ˜
β

∂H
∂Q , β

−
	
∂
∂β , dβ
dt

with ∂
∂β = Q
dS
dt = ˜
β

∂H
∂Q , β

≥0, ∀H (link to positivity of Fisher metric)
if H = S
⇒
∂S
∂Q =Q
dS
dt = ˜
β(β, β) = 0 because β ∈Ker ˜
β
(46)
Entropy production is then linked with Souriau-Fisher structure, dS = ˜

∂H
∂Q , β

dt
with ˜
β

∂H
∂Q , β

= ˜

∂H
∂Q , β

+
	
Q,

∂H
∂Q , β

Souriau tensor related to Fisher metric.
3.1
Souriau Entropy as Generalized Casimir Invariant in Coadjoint
Representation
Hendrik Brugt Gerhard Casimir, a Dutch physicist, studied what is called Casimir oper-
ators and Casimir invariants (H. Casimir and Van der Waerden studied the SU(2) group,
the group of isospin/angular momentum, as the model of the algebraic approach to the
study of the unitary representations of semi-simple compact Lie groups). Kirillov has
explained that Casimir operators are in one-to-one correspondence with polynomial
invariants characterizing orbits of the coadjoint representation. Solutions are not nec-
essarily polynomials and the nonpolynomial solutions are called generalized Casimir
invariants. For certain classes of Lie algebras, all invariants of the coadjoint representa-
tion are functions of polynomial ones. In physics, Hamiltonians and integrals of motion
of classical integrable Hamiltonian systems are not polynomials in the momenta.
In Souriau Lie groups Thermodynamics, we will see that coadjoint orbits lie on level
sets of the Entropy that could be considered as a Casimir invariant function:
S : g∗→R
Qa →S(Q)
(47)
We will consider ﬁrst the case of null-cohomology, Entropy as Casimir invariant function
isaconservedquantity,becauseCasimirfunctionhasnullLiePoissonbracketsfunctions:
.
{S, H}(Q) =
	
Q,

∂S
∂Q, ∂H
∂Q

= 0
∀H : g∗→R, Q ∈g∗, ⟨A, B⟩= B(A, B)
Cartan−Killing form
with
∂S(Q) = d
dεS(Q + δQ)
333
ε=0 =
	
δQ, ∂S
∂Q

(48)

70
F. Barbaresco
We can observe that β = ∂S
∂Q, then:
,
Q,

β, ∂H
∂Q
-
=
,
Q.adβ
∂H
∂Q
-
= 0, ∀H : g∗→R, Q ∈g∗, adab = [a, b]
(49)
We can also write:
,
Q,
 ∂S
∂Q, ∂H
∂Q
-
=
,
Q, ad ∂S
∂Q
∂H
∂Q
-
=
,
ad∗
∂S
∂Q Q, ∂H
∂Q
-
= 0,
∀H : g∗→R
(50)
It means that ad∗
∂S
∂Q Q, ad∗
βQ = 0,
β =
∂S
∂Q. We can remark that if we note

ad∗
∂S
∂Q
Q

= Ck
ijad∗
∂S
∂Q
iQk = 0 with Ck
ij the structure tensor, we observe that this
equation is in fact the Casimir condition for invariant function in coadjoint represen-
tation as we will see hereafter. The restriction of the Lie-Poisson bracket to an orbit
generates a symplectic structure on the orbit, called the KKS (Kirillov-Kostant-Souriau)
structure, or the canonical symplectic structure. Casimir function is characterized as a
quantity which commutes with each linear functional on the Poisson manifold, and then
it is conserved by dynamics of any Hamiltonian.
Given a Hamiltonian H : g∗→R, the equation of motion for Q ∈g∗is:
dQ
dt = {Q, H} = ad∗
∂H
∂Q Q with H = S ⇒dQ
dt = {Q, S} = ad∗
∂S
∂Q Q = 0
(51)
In case of non-null cohomology, the Lie Poisson brackets functions are given by:
{S, H} ˜
(Q) =
,
Q,
 ∂S
∂Q, ∂H
∂Q
-
+ ˜
 ∂S
∂Q, ∂H
∂Q

= 0 , ∀H : g∗→R, Q ∈g∗
with ˜
(X , Y) = J⌊X ,Y⌋−{JX , JY }
where JX (X ) = ⟨J(x), X ⟩
˜
(X , Y) : g × g →ℜ
with

(X ) = Teθ(X (e))
X, Ya →⟨
(X ), Y⟩
(52)
That we can develop in the following:
{S, H} ˜
(Q) =
	
Q,

∂S
∂Q, ∂H
∂Q

+
	


∂S
∂Q

, ∂H
∂Q

= 0
{S, H} ˜
(Q) =
	
Q, ad ∂S
∂Q
∂H
∂Q

+
	


∂S
∂Q

, ∂H
∂Q

= 0
{S, H} ˜
(Q) =
,
ad∗
∂S
∂Q Q, ∂H
∂Q
-
+
	


∂S
∂Q

, ∂H
∂Q

= 0
∀H, {S, H} ˜
(Q) =
,
ad ∂S
∂Q
∗Q + 

∂S
∂Q

+, ∂H
∂Q
-
= 0 ⇒ad∗
∂S
∂Q
Q + 

∂S
∂Q

= 0
(53)
We have found the generalized Casimir equation for Entropy in the non-null cohomology
case:
{S, H} ˜
(Q) = 0
(54)

Souriau-Casimir Lie Groups Thermodynamics and Machine Learning
71
That could be also written:
ad∗
∂S
∂Q Q + 
 ∂S
∂Q

= 0
(55)
This equation was observed by Souriau in his paper of 1974, where he has written that
geometric temperature β is a kernel of ˜
β, that is written:
β ∈Ker ˜
β ⇒⟨Q, [β, Z]⟩+ ˜
(β, Z) = 0
(56)
That we can develop to recover the Casimir equation:
⇒

Q, adβZ

+ ˜
(β, Z) = 0 ⇒
	
ad∗
βQ, Z

+ ˜
(β, Z) = 0
β = ∂S
∂Q ⇒
,
ad ∂S
∂Q
∗Q, Z
-
+ ˜

∂S
∂Q, Z

=
,
ad ∂S
∂Q
∗Q + 

∂S
∂Q

, Z
-
= 0, ∀Z
⇒ad∗
∂S
∂Q Q + 

∂S
∂Q

= 0
(57)
Then the generalized Casimir Equation in non-null cohomogy is given by:

ad∗
∂S
∂Q Q

+ 
 ∂S
∂Q

j
= Ck
ijad∗
∂S
∂Q
iQk + 
j = 0
(58)
Given a Hamiltonian H : g∗→R, the equation of motion for Q ∈g∗is:
dQ
dt = ad∗
∂H
∂Q Q + 
∂H
∂Q

with H = S ⇒dQ
dt = ad∗
∂S
∂Q Q + 
 ∂S
∂Q

= 0
(59)
Level sets of the Casimir Entropy function, on which the coadjoint orbits lie, are
symplectic manifolds.
From this equation, we can introduce a Geometric Heat Fourier Equation:
∂Q
∂β .∂β
∂t = ad∗
∂H
∂Q Q + 
∂H
∂Q

= {Q, H} ˜
(60)
where ∂Q
∂β geometric heat capacity is given by gβ(X , Y) =
,
−∂Q
∂β (X ), Y
-
for X , Y ∈g
with gβ(X , Y) = ˜
β(X , Y) = ⟨Q(β), [X , Y]⟩+ ˜
(X , Y) related to Souriau−Fishertensor
3.2
Souriau Entropy Invariance in Coadjoint Representation
If we note
the space of analytic function on the dual space of the Lie algebra
g∗, a function
is a Casimir invariant if for any g ∈G, X ∈g∗, we have
F∗
Ad∗
g X

= F∗(X ). We have observed previously that Souriau’s Entropy analytic
function S(Q) deﬁned on dual space of the Lie algebra g∗by Legendre transform of
Massieu Characteric analytic function (β) (minus logarithm of Laplace transform)
deﬁned on Lie algebra g was an invariant function under the afﬁne coadjoint action

72
F. Barbaresco
S

Q
 
Adg(β)
!
= S

Ad∗
g (Q) + θ(g)

= S(Q). In case of null-cohomology, Souriau
cocycle cancels θ(g) = 0, and we recover Casimir invariant function in coadjoint
representation S

Ad∗
g (Q)

= S(Q).
We can then observe that Souriau Entropy is an extended Casimir invariant func-
tion in case of non-null cohomogy. This characteristic of Souriau Entropy could be a
new characterization of Entropy. In Souriau Lie Groups Thermodynamics, Entropy
S(Q) is a generalized Casimir invariant function for coadjoint representation in case
of non-null cohomology, and Massieu Characteristic function by Legendre duality is
a generalized Casimir function for adjoint representation.
We will explain how to prove that Souriau Entropy is invariant under the action of
the group, starting from its deﬁnition:
S(Q) = ⟨Q, β⟩−(β) with Q = ∂(β)
∂β
∈g∗
and β = ∂S(Q)
∂Q
∈g
(61)
with (β) = −log

M
e−(U(ξ),β)dλω and U : M →g∗
Considering Souriau Entropy S(Q) where the heat Q = ∂(β)
∂β
∈g∗an element of
the dual space of the Lie algebra is parameterized by β ∈g an element of the Lie algebra,
the Lie group G acts through g ∈G by adjoint operator Adg, the Entropy is given by
S

QAdg(β)

with QAdg(β) given by fundamental Souriau equation:
Q
 
Adg(β)
!
= Ad∗(Q) + θ(g)
(62)
The invariance of Souriau Entropy is deduced from the following developments:
β ∈g →Adg(β) ⇒
 
Adg(β)
!
=

M
e−⟨U,Adg(β)⟩dλω

 
Adg(β)
!
=

M
e
−
	
Adg−1U,β

dλω =

M
e
−U

Adg−1β

−θ
 
g−1!
,β

dλω

 
Adg(β)
!
= e

θ
 
g−1!
,θ

(β)
θ
 
g−1!
= −Ad∗
g−1θ(g) ⇒
 
Adg(β)
!
= e
−
	
Ad∗
g−1(g),β

(β)
(β) = −log (β)
⇒
 
Adg(β)
!
= (β) −

θ
 
g−1!
, β

= (β) +
	
Ad∗
g−1θ(g), β

(63)

Souriau-Casimir Lie Groups Thermodynamics and Machine Learning
73
Based on this expression of Massieu Characteristic function transform by action of the
group, we can use Legendre transform to study how Souriau Entropy is changed:
(64)
We ﬁnally prove that Souriau Entropy is invariant in coadjoint representation
S

Ad∗
g (Q) + θ(g) = S(β)

in general case of non-null cohomology, that we could write
S

Ad#
g (Q) = S(β)

, if we note afﬁne coadjoint action Ad#
g (Q) = Ad∗
g (Q)+θ(g). This is
also true in case of null-cohomology when the Souriau cocycle cancels θ(g) = 0, and we
recover classical generalized Casimir invariant function deﬁnition on coadjoint represen-
tation for Entropy S

Ad∗
g (Q) = S(β)

generalized Casimir invariant function deﬁnition
on adjoint representation for Massieu Characteristic function 
 
Adg(β)
!
= (β).
3.3
Algebraic Method for Construction of Casimir Invariant Functions
in Coadjoint Representation
We will describe recent characterization of generalized Casimir invariant functions
by Oleg L. Kurnyavko and Igor V. Shirokov who have proposed Algebraic method
for construction of Casimir invariants of Lie groups coadjoint representations. Mod-
ern invariant theory based on geometric methods, which was credited classically as
non-constructive, has some exception admitting a constructive solution related to the
constructing invariants of Lie groups representations.
Let G be a connected Lie group, T(G) a representation of the group G in the linear
space V, Tg the operators associated to the representation of the group G on the linear
space V, then the invariants are given by the following equation:
F
 
Tgx
!
= F(x), x ∈V, g ∈G, Tg ∈T(G), F(x) ∈C∞(V)
(65)
With the properties that:
Te = I, Tgagb = TgaTgb, Tg−1 =
 
Tg
!−1
(66)
Solution is given by the following differential equation in local coordinates:
−
dim V
6
i,j
ti
kjxj ∂F(x)
∂xi
= 0 with ti
kj =
∂
 
Tg
!i
j
∂gk
333333
g=e
and k = 1, . . . , dim G
(67)

74
F. Barbaresco
ti
kj are elements of the matrices of the Lie algebra representation basis of G.
That we can write tk = −ti
kjxj ∂
∂xi and tkF(x) = 0.
If we consider the dual space V ∗, the co-tangent representation is given by:

T ∗(g)X , T(g)x

= ⟨X , x⟩
(68)
And co-representation invariants are given by:
t∗
k F∗(X ) = 0 with
t∗
k = ti
kjXi
∂
∂Xj
(69)
They have underlined the relationship between invariants of representations and con-
jugate representations, where the algebraic construction of Lie groups representations
invariants are given by invariants of the conjugate representation with respect to the
invariants of the original representation.
Shirokov has considered F(x) the representation invariant T(G), and F∗(x) the
representation invariant T ∗(G) conjugate to T(G), with the conditions:
−ti
kjxj ∂F(x)
∂xi
= 0 and ti
ljXi
∂F∗(X )
∂Xj
= 0
(72)
ti
ljXi ∂F∗(X )
∂Xj
= ti
ljXi ∂
∂Xj

xk(X )Xk −F(x(X ))

ti
ljXi ∂F∗(X )
∂Xj
= ti
ljXi ∂xk
∂Xj Xk + ti
ljXixk ∂Xk
∂Xj −ti
ljXi ∂F(x)
∂xk
∂xk
∂Xj
ti
ljXi ∂F∗(X )
∂Xj
= ti
ljXi ∂xk
∂Xj
∂F(x)
∂xk
+ ti
lj
∂F(x)
∂xi xkδj
k −ti
lj
∂F(x)
∂xi
∂F(x)
∂xk
∂xk
∂Xj
ti
ljXi ∂F∗(X )
∂Xj
= ti
ljxj ∂F(x)
∂xi
= 0
(73)
Invariant Casimir Functions of the coadjoint representation has been studied for com-
pletely integrable Hamiltonian systems, as classical systems on the orbits of the coad-
joint representation. Oleg L. Kurnyavko and Igor V. Shirokov have considered the rela-
tionship between invariants of representations of Lie groups and their conjugate dual
representations.
Considering the coadjoint action given by:
	
Ad∗
g X , x

=

X , Adg−1x

, g ∈G, X ∈g∗, x ∈g
(74)

Souriau-Casimir Lie Groups Thermodynamics and Machine Learning
75
Invariants of a coadjoint representation are called Casimir functions, with the property:
F∗
Ad∗
g X

= F∗(X )
(75)
the inﬁnitesimal invariance is given by the equations:
Cij(X )∂F∗(X )
∂Xj
= 0 with Cij(X ) = Ck
ijXk,
i, j, k = dim g
(76)
The number of functionally independent invariants is given by the rank of the matrix
Cij(X), called the index of the Lie algebra g: indg = dim g∗−sup
X ∈g∗rankCij(X ).
From these adjoint and coadjoint representation, Shirokov has introduced the
following theorem:
Nota:
Ck
ijXk
∂F∗(X )
∂Xj
= 0
,
i, j, k = dim g, with
7
Ck
ijXk = Cij(X ) = Bij
Bx(x, y) = Bijxiyj =

X ,

x, y

(79)
I. V. Shirokov has proposed a method for constructing invariants of the coadjoint
representation of Lie groups with an arbitrary dimension and structure based on local
symplectic coordinates on the coadjoint orbits. Oleg L. Kurnyavko and Igor V. Shirokov
have also proposed a general method for constructing Casimir invariants.
4
Souriau Gibbs Density for Classical Lie Groups
We will illustrate Souriau model for SU(1,1) Lie Group, a case with null cohomology,
and for SE(2) Lie group, a case of non-null cohomology.

76
F. Barbaresco
4.1
Gibbs Density for SU(1,1) Lie Groups and Poincaré Disk in Case of Null
Cohomology
We will introduce Souriau moment map for SU(1,1)/U(1) group that acts transitively on
Poincaré Unit Disk, based on moment map. Considering the Lie group
SU(1, 1) =
" a b
b∗a∗
8
a, b ∈C, |a|2 −|b|2 = 1
9
(80)
and its Lie algebra given by elements
su(1, 1) =
" ir
η
η∗−ir
8
r ∈R, η ∈C
9
(81)
A basis for this Lie algebra su(1, 1) is (u1, u2, u3) ∈g with:
ui = i
2
1 0
0 −1

,
u2 = −1
2
0 1
1 0

and u3 = 1
2
0 −i
i 0

(82)
with [u1, u3] = −u2[u1, u2] = u3[u2, u3] = −u1.
The compact subgroup is generated by u1, while u2 and u3 generate a hyperbolic
subgroup. The dual space of the Lie algebra is given by:
su(1, 1)∗=
"
z
x + iy
−x + iy
−z

/x, y, z ∈R
9
(83)
with the basis
 
u∗
1, u∗
2, u∗
3
!
∈g∗with
u∗
1 =
1 0
0 −1

, u∗
2 =
0 i
i 0

and u∗
3 =
 0 1
−1 0

(84)
Let consider D = {z ∈C/|z| < 1} be the open unit disk of Poincaré. For each ρ > 0,
the pair
 
D, ωρ
!
is a symplectic homogeneous manifold with ωρ = 2iρ dz∧dz∗
 
1−|z|2!2 , where
ωρ is invariant under the action:
SU(1, 1) × D →D
(g, z) →g.z =
az + b
b∗z + a∗
(85)
This action is transitive and is globally and strongly Hamiltonian. Its generators are the
Hamiltonian vector ﬁelds associated to the functions:
J1(z, z∗) = ρ 1 + |z|2
1 −|z|2 ,J2(z, z∗) = ρ
i
z −z∗
1 −|z|2 ,J3(z, z∗) = −ρ z + z∗
1 −|z|2
(86)
The associated moment map J : D →su∗(1, 1) deﬁned by J(z).ui = Ji(z, z∗), maps
D into a coadjoint orbit in su∗(1, 1). Then, we can write the moment map as a matrix

Souriau-Casimir Lie Groups Thermodynamics and Machine Learning
77
element of su∗(1, 1):
J(z) = J1
 
z, z∗!
u∗
1 + J2
 
z, z∗!
u∗
2 + J3
 
z, z∗!
u∗
3
J(z) = ρ
⎛
⎝
1+|z|2
1−|z|2 −2
z∗
1−|z|2
2
z
1−|z|2 −1+|z|2
1−|z|2
⎞
⎠∈g∗
(87)
The moment map J is a diffeomorphism of D onto one sheet of the two-sheeted hyper-
boloid in su∗(1, 1), determined by the following equation J 2
1 −J 2
2 −J 2
3 = ρ2,J1 ≥ρ with
J1u∗
1 + J2u∗
2 + J3u∗
3 ∈su∗(1, 1). We note O+
ρ the coadjoint orbit Ad∗
SU(1,1) of SU(1, 1),
given by the upper sheet of the two-sheeted hyperboloid given by previous equation. The
orbit method of Kostant-Kirillov-Souriau associates to each of these coadjoint orbits a
representation of the discrete series of SU(1, 1), provided that ρ is a half integer greater
or equal to 1 (ρ = k
2, k ∈N and ρ ≥1). When explicitly executing the Kostant-Kirillov
construction, the representation Hilbert spaces Hρ are realized as closed reproducing
kernel subspaces of L2 
D, ωρ
!
. The Kostant-Kirillov-Souriau orbit method shows that
to each coadjoint orbit of a connected Lie group is associated a unitary irreducible
representation of G acting in a Hilbert space H.
Souriau has oberved that action of the full Galilean group on the space of motions
of an isolated mechanical system is not related to any equilibrium Gibbs state (the open
subset of the Lie algebra, associated to this Gibbs state is empty). The main Souriau
idea was to deﬁne the Gibbs states for one-parameter subgroups of the Galilean group.
We will use the same approach, in this case we will consider action of the Lie group
SU(1, 1) on the symplectic manifold (M,ω) (Poincaré unit disk) and its momentum map
J are such that the open subset β =
7
β ∈g/

D
e−⟨J(z),β⟩dλ(z) < +∞
:
is not empty.
This condition is not always satisﬁed when (M, ω) is a cotangent bundle, but of course
it is satisﬁed when it is a compact manifold. The idea of Souriau is to consider a one
parameter subgroup of SU(1, 1). To parametrize elements of SU(1, 1) is through its Lie
algebra. In the neighborhood of the identity element, the elements of g ∈SU(1, 1) can
be written as the exponential of an element β of its Lie algebra:
g = exp(εβ) with β ∈g
(88)
The condition g+Mg = M for M =
1 0
0 −1

can be expanded for ε << 1 and is
equivalent to β+M + M β = 0 which then implies β =
 ir
η
η∗−ir

, r ∈R, η ∈C. We
can observe that r and η = ηR + iηI contain 3 degrees of freedom, as required. Also
because det g = 1, we get Tr(β) = 0. We can then exponentiate β with exponential map
to get:
g = exp(εβ) =
∞
6
k=0
(εβ)k
k!
=
 aε(β) bε(β)
b∗
ε(β) a∗
ε(β)

(89)

78
F. Barbaresco
If we make the remark that we have the following relation β2 =
 ir
η
η∗−ir
 ir
η
η∗−ir

=
 |η|2 −r2!
I, we can developed the exponential map:
g = exp(εβ)
g =
'
cosh(εR) + ir sinh(εR)
R
η sinh(εR)
R
η∗sinh(εR)
R
cosh(εR) −ir sinh(εR)
R
(
with R2 = |η|2 −r2
(90)
As R2 should be positive, one condition is that |η|2 −r2 > 0 then the subset to consider
is given by the subset β =
"
β =
 ir
η
η∗−ir

, r ∈R, η ∈C/|η|2 −r2 > 0
9
such that

D
e−⟨J(z),β⟩dλ(z) < +∞. The generalized Gibbs states of the full SU(1, 1) group do
not exist. However, generalized Gibbs states for the one-parameter subgroups exp(αβ),
β ∈β, of the SU(1, 1) group do exist. The generalized Gibbs state associated to β
remains invariant under the restriction of the action to the one-parameter subgroup of
SU(1, 1) generated by exp(εβ).
To go futher, we will develop the Souriau Gibbs density from the Souriau moment
map J(z) and the Souriau temperature β ∈β. If we note b =
1
1−|z|2
 1
−z

, we can
write the moment map:
J(z) == ρ
 
2Mbb+ −Tr
 
Mbb+!
I
!
with M =
1 0
0 −1

(91)
We can write the covariant Gibbs density in the unit disk given by moment map of the
Lie group SU(1, 1) and geometric temperature in its Lie algebra β ∈β:
pGibbs(z) =
e−⟨J(z),β⟩

D
e−⟨J(z),β⟩dλ(z)with dλ(z) = 2iρ dz ∧dz∗
 
1 −|z|2!2
(92)
pGibbs(z) = e−⟨ρ(2ℑbb+−Tr(ℑbb+)I),β⟩

D
e−⟨J(z),β⟩dλ(z)
= e
−
4
ρ
⎛
⎜⎜⎝
1+|z|2
 
1−|z|2!
−2z∗
 
1−|z|2!
2z
 
1−|z|2! −1+|z|2
 
1−|z|2!
⎞
⎟⎟⎠,
⎛
⎝ir
η
η∗−ir
⎞
⎠
5

D
e−⟨J(z),β⟩dλ(z)
(93)
To write the Gibbs density with respect to its statistical moments, we have to express
the density with respect to Q = E[J(z)]. Then, we have to invert the relation between Q
and β, to replace this last variable β =
 ir
η
η∗−ir

∈β by β = 
−1(Q) ∈g where
Q = ∂(β)
∂β
= 
(β) ∈g∗with (β) = −log

D
e−⟨J(z),β⟩dλ(z), deduce from Legendre

Souriau-Casimir Lie Groups Thermodynamics and Machine Learning
79
transform. The mean moment map is given by:
Q = E[J(z)] = E
⎡
⎣ρ
⎛
⎝
1+|w|2
 
1−|w|2!
−2w∗
 
1−|w|2!
2w
 
1−|w|2! −1+|w|2
 
1−|w|2!
⎞
⎠
⎤
⎦where w ∈D
(94)
4.2
Gibbs Density for SE(2) Lie Groups in Case of Non-null Cohomology
We will restrict the study to the 2D case, but it could be eextended to SE(3). We will
consider Souriau model for SE(2) Lie group with non-null cohomology and then with
introduction of Souriau one-cocycle.
We consider SE(2) = SO(2) × R2:
SE(2) =
"Rϕ τ
0 1

/Rϕ ∈SO(2), τ ∈R2
9
(95)
The Lie algebra se(2) of SE(2) has underlying vvector space R3 and Lie bracket:
(ξ, u) ∈se(2) = R × R2,
−ξℑu
0
0

∈se(2) with ℑ=
 0 1
−1 0

(96)
Coadjoint action of SE(2) is given by:
Ad∗
(Rϕ,τ)(m, ρ) =
 
m + ℑRϕρ.τ, Rϕρ
!
(97)
The moment map J : R2 →se∗(2) of SE(2) is deﬁned by: J(ξ,u)(x) = J(x).(ξ, u) with
the right action of SE(2) on R2:
J(ξ,u)(x) = −2
1
2ξ∥x∥2 + ℑu.x

= −2
1
2∥x∥2, −ℑx

.(ξ, u)
J(ξ,u)(x) = J(x).(ξ, u) ⇒J(x) = −2
1
2∥x∥2, −ℑx

, x ∈R2
(98)
With the expression of moment map, we can compute Souriau covariant Gibbs density of
maximum entropy. Considering the symplectic form ω(ζ, υ) = ζ.ℑυ with ℑ=
 0 1
−1 0

on R2, we have seen that the action of SE(2) is symplectic and admits the momentum
map, J(x) = −
 1
2∥x∥2, −ℑx
!
, x ∈R2.
Souriau Gibbs density is deﬁned for generalized temperature β
∈

=
.
(b, B) ∈se(2)/b < 0, B ∈R2/
and given by:
pGibbs(x) =
e−⟨J(x),β⟩

R2
e−⟨J(x),β⟩dλ(x) =
e
1
2 b∥x∥2−B.ℑx

R2
e
1
2 b∥x∥2−B.ℑxdλ(x)
(99)

80
F. Barbaresco
The Massieu Potential could be computed:
(β) = log

R2
e
1
2 b∥x∥2−B.ℑxdλ(x) = log

−2π
b e−1
2b ∥B∥2
(100)
By derivation of Massieu potential, we can deduce expression of Heat:
Q ∈∗=

(m, M ) ∈se∗(2)/m + ∥M ∥2
2
< 0

Q = ∂(β)
∂β
=

1
b −∥B∥2
2b2 , 1
bB

= 
(β)
(101)
We can inverse this relation to express generalized temperature with respect to the heat:
β = 
−1(Q) =
'
m + 1
2∥M ∥2
−1
,

m + 1
2∥M ∥2
−1
M
(
(102)
We can express the Gibbs density with respect to the Heat Q which is the mean of
moment map:
pGibbs(x) = e
1
2 ∥x∥2−M .ℑx

m+ 1
2 ∥M ∥2


with  =

R2
e
1
2 ∥x∥2−M .ℑx

m+ 1
2 ∥M ∥2

dλ(x)
with (m, M ) = E(J(x)) =

−E

∥x∥2
, 2ℑE(x)

(103)
So we can rewrite the Gibbs density:
pGibbs(x) =
e
1
2 ∥x∥2+2E(x).Ix
(−E(∥x∥2)+2∥E(x)∥2)

R2
e
1
2 ∥x∥2+2E(x).Ix
(−E(∥x∥2)+2∥E(x)∥2) dλ(x)
(104)
5
Conclusion
We have seen that Lie Group tools based on Representation Theory and Orbits Methods
could be used with Souriau-Fisher Metric on Coadjoint Orbits as an extension of Fisher
Metric for Lie Group through homogeneous Symplectic Manifolds on Lie Group Co-
Adjoint Orbits.
With Lie groups Thermodynamics, we have presented Souriau tools to extend Gibbs
density for Lie groups. In this extension for Lie Groups, an important tool is the log-
Laplace transform related to the Massieu Characteristic Function in Thermodynamics
(a re-parameterization of the free energy by Planck temperature preserving Legendre
transform with respect to Entropy).

Souriau-Casimir Lie Groups Thermodynamics and Machine Learning
81
Based on this model, we have introduced a geometric characterization of Entropy
as a generalized Casimir invariant function in coadjoint representation, and Massieu
characteristic function, dual of Entropy by Legendre transform, as a generalized Casimir
invariant function in adjoint representation, where Souriau cocycle is a measure of the
lack of equivariance of the moment mapping. The dual space of the Lie algebra foliates
into coadjoint orbits that are also the level sets on the Entropy. The information manifold
foliates into level sets of the Entropy that could be interpreted in the framework of
Thermodynamics by the fact that motion remaining on this complex surfaces is non-
dissipative, whereas motion transversal to these surfaces is dissipative. We have ﬁnally
explained the 2nd Principle in thermodynamics by deﬁnite positiveness of Souriau tensor
extending the (Koszul-)Fisher metric from Information Geometry (Fig. 4).
Fig. 4. On the left, Jean-Marie Souriau student at Ecole Normale Supérieure in Paris, on the right,
Hendrik Casimir PhD student supervised by Niels Bohr & Paul Ehrenfest.
References
1. Casimir, H.G.B.: Uber die konstruktion einer zu den irreduziblen darstellungen halbeinfacher
kontinuierlicher gruppen gehörigen differentialgleichung. Proc. R. Soc. Amsterdam 34(844–
846), 4 (1931)
2. Souriau, J.-M.: Structure des systèmes dynamiques, Dunod (1969)
3. Souriau, J.-M.: Structure of dynamical systems: a symplectic view of physics, the Progress
in Mathematics book series PM, vol. 149. Springer (1997)
4. Souriau, J.-M.: Mécanique statistique, groupes de Lie et cosmologie. Colloque International
duCNRS.GéométriesymplectiqueetphysiqueMathématique,Aix-en-Provence1974.CNRS
(1976
5. Souriau, J.-M.: Géométrie Symplectique et Physique Mathématique, Deux Conférences de
Jean-Marie Souriau, Colloquium do la Société Mathématique de France, Paris, Ile-de-France,
19 Février 1975–12 Novembre 1975

82
F. Barbaresco
6. Souriau, J.-M.: Mécanique Classique et Géométrie Symplectique, CNRS-CPT-84/PE.1695,
Novembre 1984
7. Kirillov, A.A.: Elements of the Theory of Representations, vol. 220. Springer Science and
Business Media LLC, Berlin/Heidelberg, Germany (1976)
8. Guichardet, A.: La methode des orbites: Historiques, principes, résultats. In Leçons de
Mathématiques d’Aujourd’hui, vol. 4, pp. 33–59. Cassini, Paris, France (2010)
9. Vergne, M.: Representations of lie groups and the orbit method. In: Srinivasan, B., Sally, J.D.
(eds.) Emmy Noether in Bryn Mawr, pp. 33–59. Springer, New York (1983). https://doi.org/
10.1007/978-1-4612-5547-5_5
10. Rais, M.L.: Représentation coadjointe du groupe afﬁne. Ann. Inst. Fourier 28, 207–237 (1978)
11. Berzin, D.V.: Invariants of the co-adjoint representation for Lie algebras of a special form.
Russ. Math. Surv. 51, 137–139 (1996)
12. Barbaresco,F.:LiegroupstatisticsandliegroupmachinelearningbasedonSouriauLiegroups
thermodynamics & Koszul-Souriau-ﬁsher metric: new entropy deﬁnition as generalized
casimir invariant function in coadjoint representation. Entropy 22, 642 (2020)
13. Barbaresco, F., Gay-Balmaz, F.: Lie group cohomology and (multi)symplectic integrators:
new geometric tools for Lie group machine learning based on Souriau geometric statistical
mechanics. Entropy 22, 498 (2020)
14. Barbaresco, F. : Lie Groups Thermodynamics & Souriau-Fisher Metric. In: SOURIAU 2019
Conference, Institut Henri Poincaré, 31st May 2019
15. Barbaresco, F.: Souriau-Casimir Lie groups thermodynamics & machine learning. In: SPIGL
2020 Proceedings, Les Houches Summer Week on Joint Structures and Common Foun-
dations of Statistical Physics, Information Geometry and Inference for Learning, Springer
Proceedings in Mathematics & Statistics (2021)
16. Libermann, P., Marle, C.-M.: Symplectic Geometry and Analytical Mechanics. Reidel,
Kufstein (1987)
17. Marle, C.M.: Géométrie Symplectique et Géométrie de Poisson. Calvage & Mounet, Paris
(2018)
18. Marle, C.-M.: From tools in symplectic and poisson geometry to J.-M. Souriau’s theories of
statistical mechanics and thermodynamics. Entropy, 18, 370 (2016). https://doi.org/10.3390/
e18100370
19. Marle, C.-M.: Projection Stéréographique et Moments, Hal-02157930, Version 1; June 2019.
https://hal.archives-ouvertes.fr/hal-02157930/. Accessed 31 May 2020
20. Marle, C.-M.: On generalized Gibbs states of mechanical systems with symmetries, Preprint,
October 2020
21. Koszul,J.-L.,Zou,Y.M.:IntroductiontoSymplecticGeometry.SpringerScienceandBusiness
Media LLC, Heidelberg (2019). https://doi.org/10.1007/978-981-13-3987-5
22. De Saxcé, G., Marle, C-M.: Présentation du livre de Jean-Marie Souriau “Structure des
systèmes dynamiques, preprint, April 2020
23. De. Saxcé, G., Vallée, C.: Galilean Mechanics and Thermodynamics of Continua. Wiley,
Hoboken (2016)
24. De. Saxcé, G.: Link between Lie group statistical mechanics and thermodynamics of continua.
Entropy 18, 254 (2016)
25. Saxcé, G.: Euler-poincaré equation for lie groups with non null symplectic cohomology.
Application to the mechanics. In: Nielsen, F., Barbaresco, F. (eds.) GSI 2019. LNCS, vol.
11712, pp. 66–74. Springer, Cham (2019). https://doi.org/10.1007/978-3-030-26980-7_8
26. Cartier, P.: Some Fundamental Techniques in the Theory of Integrable Systems,
IHES/M/94/23, SW9421 (1994). https://cds.cern.ch/record/263222/ﬁles/P00023319.pdf.
Accessed 31 May 2020
27. Dacunha-Castelle, D., Gamboa, F.: Maximum d’entropie et problème des moments Annales
de l’I.H.P., section B, tome 26, no. 4, pp. 567–596 (1990)

Souriau-Casimir Lie Groups Thermodynamics and Machine Learning
83
28. Balian, R., Alhassid, Y., Reinhardt, H.: Dissipation in many-body systems: a geometric
approach based on information theory. Phys. Rep. 131, 1–146 (1986)
29. Nencka, H., Streater, R.F.: Information geometry for some Lie algebras. Inﬁn. Dimens. Anal.
Quantum Probab. Relat. Top. 2, 441–460 (1999)
30. Shirokov, I.V.: Differential invariants of the transformation group of a homogeneous space.
Sib. Math. J. 48, 1127–1140 (2007)
31. Shirokov, I.V.: Darboux coordinates on K-orbits and the spectra of Casimir operators on lie
groups. Theor. Math. Phys. 123, 754–767 (2000)
32. Kurnyavko, O.L., Shirokov, I.V.: Algebraic method for construction of inﬁnitesimal invariants
of Lie groups representations. arXiv:1710.07977v1 [math.RT], 9 May 2019
33. Mikheyev, V.V., Shirokov, I.V.: Application of coadjoint orbits in the thermodynamics of
non-compact manifolds. Electron. J. Theor. Phys. 2, 1–10 (2005)
34. Mikheev, V.: Method of orbits of co-associated representation in thermodynamics of the Lie
non-compact groups. In: Nielsen, F., Barbaresco, F. (eds.) GSI 2017. LNCS, vol. 10589,
pp. 425–431. Springer, Cham (2017). https://doi.org/10.1007/978-3-319-68445-1_50
35. Schmidt, J.R.: The Darboux coordinate system and Holstein-Primakoff representations on
Kahler manifolds. Canad. J. Phys. 84(10), 891 (2006)
36. Barbaresco, F.: Souriau Entropy based on Symplectic Model of Statistical Physics: three Jean-
Marie Souriau’s seminal papers on Lie Groups Thermodynamics, Encyclopedia of Entropy
Across the Disciplines, World Scientiﬁc (2021)

An Exponential Family on the Upper
Half Plane and Its Conjugate Prior
Koichi Tojo1(B) and Taro Yoshino2
1 RIKEN Center for Advanced Intelligence Project, Tokyo, Japan
koichi.tojo@riken.jp
2 Graduate School of Mathematical Sciences, The University of Tokyo, Tokyo, Japan
yoshino@ms.u-tokyo.ac.jp
Abstract. Conjugate prior plays an important role in the ﬁeld of
Bayesian statistics. In fact, families of conjugate priors for various fami-
lies of distributions have been well-studied and widely used. In this paper,
we construct new conjugate priors. More precisely, we give a family of
conjugate priors in the explicit form for the family of Poincar´e distribu-
tions on the upper half plane. Here, Poincar´e distributions are essentially
the same with the hyperboloid distributions on the 2-dimensional hyper-
bolic space in the sense of Jensen.
1
Introduction
Conjugate prior plays an important role in the ﬁeld of Bayesian statistics. In fact,
families of conjugate priors for widely used families of distributions have been
well-studied [F97]. If we have a family of conjugate priors with explicit form,
then we can easily perform Bayesian inference. Exponential families are known
to admit families of conjugate priors. However, since it is diﬃcult to calculate
the normalizing constants of conjugate priors in general, we can not always
describe the families of conjugate priors with explicit form. If we determine
the normalizing constants, we can describe the prior and posterior predictive
distribution explicitly. Therefore, the following problem naturally arises:
Problem 1. For an exponential family of distributions, give a family of conju-
gate priors explicitly including the normalizing constant.
In this paper, we consider Problem 1 for the family of Poincar´e distributions
(see Subsect. 1.2), which is an exponential family on the upper half plane H.
Recall that SL(2, R) acts on H as the linear fractional transformation. The
family of Poincar´e distributions is compatible with this SL(2, R)-action (see
Fact 1). We will construct the family of conjugate priors for the family of Poincar´e
distributions. Moreover, we will see it is also an exponential family compatible
with SL(2, R)-action (Proposition 3). The family of Poincar´e distributions is
obtained by G/H-method, so let us begin with a quick review of G/H-method
in the next subsection.
c
⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2021
F. Barbaresco and F. Nielsen (Eds.): SPIGL 2020, PROMS 361, pp. 84–95, 2021.
https://doi.org/10.1007/978-3-030-77957-3_4

An Exponential Family on the Upper Half Plane
85
1.1
G/H-Method
In this subsection, we give a quick review of G/H-method, which is a method to
construct good families on homogeneous spaces by using representation theory.
See [TY18,TY20] for the detailed explanation.
Setting 1. Let G be a Lie group with ﬁnitely many connected components and
H a closed subgroup of G. Then the quotient space X := G/H is a locally
compact Hausdorﬀspace and is called a homogeneous space of G. We assume
that there exists a nonzero relatively G-invariant measure on X.
Roughly speaking, G/H-method is a method to construct a family of distribu-
tions on G/H from a representation of G and an H-ﬁxed vector under the setting
above.
Deﬁnition 1 (G/H-method). Let V be a ﬁnite dimensional real vector space
and π: G →GL(V ) a representation of G. Set V H := {v ∈V | π(h)v =
v for any h ∈H}. We call an element of V H an H-ﬁxed vector. We take an
H-ﬁxed vector v0 ∈V H. By using (π, v0), we construct a family of distributions
on X as follows.
We consider the following ﬁnite dimensional vector space:
W0(G, H) := {τ : G →R | τ is a continuous group homomorphism, τ|H = 0}.
This space can be regarded as a subspace of C(X) by the condition τ|H = 0.
Here, C(X) denotes the set of all R-valued continuous functions on X. We ﬁx a
nonzero relatively G-invariant measure μ ∈R(X). Here, R(X) denotes the set
of all Radon measures on X. We denote by V ∨the dual vector space of V . For
θ := (ξ, τ) ∈V ∨⊕W0(G, H), we deﬁne a measure ˜pθ on X by
d˜pθ(x) := exp(−⟨ξ, xv0⟩+ τ(x))dμ(x)
= exp(−⟨ξ, xv0⟩+ ⟨τ, evx⟩)dμ(x)
(x ∈X).
Here, evx : W0(G, H) →R is the evaluation map at x, and ⟨·, ·⟩is the pairing
on V ∨× V or W0(G, H) × W0(G, H)∨. The notation xv0 is well-deﬁned by the
condition that v0 is H-ﬁxed. Moreover, we normalize them as follows:
Θ :=

θ = (ξ, τ) ∈V ∨⊕W0(G, H)


X
d˜pθ < ∞

,
ϕ(θ) := log

X
d˜pθ
(θ ∈Θ),
pθ := e−ϕ(θ)˜pθ
(θ ∈Θ).
As a result, we obtain a family P := {pθ}θ∈Θ of distributions on X.
Remark 1 ([TY20, Corollary 4.27]). The family P does not depend on the
choice of the nonzero relatively G-invariant measure μ.

86
K. Tojo and T. Yoshino
Fact 1 ([TY20, Theorem 3.2])
The family P obtained by G/H-method is a G-invariant exponential family if
Θ ̸= ∅(see Deﬁnition 3 for exponential family and [TY20] for the deﬁnition of
G-invariance).
As we will see in the next subsection, the family of Poincar´e distributions is
obtained by G/H-method with (G, H) = (SL(2, R), SO(2)) (see [TY18,TY20]
for other examples). In this case, we have W0(G, H) = {0}. In general, in the
case of W0(G, H) = {0}, the parameter space Θ is naturally considered as a
subset of V ∨. As a suﬃcient condition for the equation W0(G, H) = {0}, we
have the following:
Fact 2 ([TY20, Proposition 4.28])
We have W0(G, H) = {0} if one of the following conditions holds:
(i) g = h + [g, g],
(ii) G is compact,
(iii) G is semisimple.
Here, g and h are the Lie algebras of G and H, respectively.
1.2
Poincar´e Distribution
In this subsection, we give an exponential family on the upper half plane H :=
{x + iy ∈C | y > 0} by using G/H-method.
Example 1 (Poincar´e distribution)
Let G := SL(2, R), H := SO(2) and X := G/H ≃H. Here, the identiﬁcation
X ≃H is given as follows:
H →G/H, z →α(z)H,
α(z) :=
√y
x
√y
0
1
√y

(z = x + iy ∈H).
Put V := Sym(2, R). We consider a representation π: G →GL(V ) given by
π(g)v := gvtg
(g ∈G, v ∈Sym(2, R)).
We take an H-ﬁxed vector v0 := I2 ∈Sym(2, R). Then, let us see that the
following exponential family on H is obtained by G/H-method.
De2D
π
exp

−a(x2 + y2) + 2bx + c
y
	 dxdy
y2

⎛
⎝a b
b c
⎞
⎠∈Sym+(2,R)
.
(1)
Here, D =
√
ac −b2, Sym+(2, R) := {S ∈Sym(2, R) | S is positive deﬁnite}.
In fact, take a nonzero (relatively) G-invariant measure dxdy
y2
∈R(X). From
Fact 2 (iii), we have W0(G, H) = {0}. By taking the natural inner product on

An Exponential Family on the Upper Half Plane
87
Sym(2, R), we identify V ∨with Sym(2, R). For θ =
a b
b c
	
∈Sym(2, R) ≃V ∨,
we deﬁne a measure ˜pθ by
d˜pθ(z) = exp(−Trace(θα(z)I2
tα(z)))dxdy
y2
(z = x + iy)
= exp

−a(x2 + y2) + 2bx + c
y
	 dxdy
y2 .
By Lemma 4 in page 11, we have
Θ = Sym+(2, R),
ϕ(θ) = log
π
De2D .
Here, D =
√
ac −b2. Therefore,
dpθ(z) = e−ϕ(θ)d˜pθ = De2D
π
exp

−a(x2 + y2) + 2bx + c
y
	 dxdy
y2 ,
As a result, we obtain (1).
The family obtained above is “compatible” with Poincar´e metric on the upper
half plane, so we call it the family of Poincar´e distributions.
Remark 2. The family of Poincar´e distributions is essentially the same with
the family of hyperboloid distributions on two dimensional hyperbolic space in
[BN78b,J81].
As mentioned in Fact 1, the family of Poincar´e distributions is SL(2, R)-
invariant. Therefore, SL(2, R) also acts on the parameter space Θ
:=
Sym+(2, R) ⊂V ∨.
Proposition 1. The action of G = SL(2, R) on Θ is given as follows:
G × Θ ∋(g, v) →tg−1vg−1 ∈Θ.
Proof. When the action of G on Θ is given as above, it is enough to show
g∗pθ = pg·θ for any g ∈G. Here, g∗pθ denotes the push forward of the measure
pθ by the map g: H →H, z →g · z. Take any g ∈G, Then, we have
d(g∗pθ)(z) ∝exp(−Trace(θα(g−1z)I2
tα(g−1z)))g · (dxdy
y2 )
= exp(−Trace(θg−1α(z)tα(z)tg−1))dxdy
y2
= exp(−Trace(tg−1θg−1α(z)tα(z)))dxdy
y2
∝dpg·θ(z).

88
K. Tojo and T. Yoshino
Remark 3. Usually, the natural G-action on V := Sym(2, R) is given as follows:
G × V →V, (g, v) →gvtg.
On the other hand, the natural G-action (contragredient representation of the
natural action above) on V ∨is given as follows:
G × V ∨→V ∨, (g, θ) →tg−1θg−1.
Note that we regard Θ ⊂V ∨, so the G-action on Θ is given as in Proposition 1.
1.3
Conjugate Prior of Exponential Family
In this subsection, we review conjugate prior quickly. In particular, there exists
a family of conjugate priors for an exponential family (Proposition-Deﬁnition 2).
For more details, see [DY79].
Let X and Θ be locally compact Hausdorﬀspaces, and R(X) and R(Θ)
denote the sets of all Radon measures on X and Θ, respectively. Conjugate
prior, which is deﬁned below, is an important notion in the ﬁeld of Bayesian
inference. We do not discuss Bayesian inference here, but give a deﬁnition of
conjugate prior directly. See [RS61] for Bayesian inference.
Deﬁnition 2 (Conjugate prior [RS61]). Let P = {pθ(x)dμ(x)}θ∈Θ ⊂R(X)
be a family of distributions on X with the parameter space Θ, and Q =
{qc(θ)dλ(θ)}c∈C ⊂R(Θ) a family of distributions on Θ with the hyperparameter
space C. Then, Q is conjugate to P (or a family of conjugate priors for P) if for
any x0 ∈X, pθdμ ∈P and c ∈C, there exists c′ ∈C such that pθ(x0)qc(θ) is
equal to qc′(θ) as a function on Θ up to a positive scalar.
Remark 4. The notion of a family of conjugate priors is well-deﬁned. In fact,
it does not depend on the choice of μ of P. However, a family of conjugate priors
for given P is not unique. In fact, it does depend on the choice of λ of Q.
We can perform a computation of Bayesian inference easily for a family of
distributions having a family of conjugate priors. However in general, there does
not exist a family of conjugate priors. On the other hand, an exponential family
always have a family of conjugate priors, which is also an exponential family.
Here let us review the deﬁnition of exponential family.
Deﬁnition 3 (exponential family, [BN70, §5]). A non-empty subset P ⊂
R(X) consisting of probability measures is called an exponential family on X if
there exists a triple (μ, V, T) satisfying the following four conditions:
1. μ ∈R(X),
2. V is a ﬁnite dimensional real vector space,
3. T : X →V is a continuous map,
4. for any p ∈P, there exists θ ∈V ∨such that
dp(x) = exp(−⟨θ, T(x)⟩−ϕ(θ))dμ(x)
(x ∈X).

An Exponential Family on the Upper Half Plane
89
Here ϕ(θ) := log

x∈X exp(−⟨θ, T(x)⟩)dμ(x). We call ϕ(θ) log normalizer, X the
sample space of P, the triple (μ, V, T) a realization of P, and μ the base measure
of P.
Remark 5. In [BN70], the notion of exponential families is deﬁned in the cate-
gory of measurable spaces and measurable maps. On the other hand, we consider
exponential families in the category of topological spaces and continuous maps.
In our setting, we do not need the assumption that μ is σ-ﬁnite (see [TY20,
§4.2]). In [BN70], the triple (μ, V, T) is called a representation. However, to avoid
the conﬂict between the triple and group representations, we call the triple a
realization.
Let us see that an exponential family has a family of conjugate priors based
on [DY79].
Proposition-Deﬁnition 2 ([DY79]). Let P be a regular exponential family
(see [BN78a, §8.1] for the deﬁnition), and (μ, V, T) a minimal realization of P.
We deﬁne functions ft,v on Θ parameterized by (t, v) ∈R>0 × V as follows:
ft,v(θ) := exp(−⟨θ, v⟩−tϕ(θ)).
(2)
We take a measure λ ∈R(Θ) on Θ and deﬁne a subset Cλ of R>0 × V by
Cλ :=

(t, v) ∈R>0 × V


Θ
ft,vdλ < ∞

.
Moreover, we deﬁne probability measures qt,v parameterized by Cλ as follows:
dqt,v(θ) := c−1
t,vft,v(θ)dλ(θ),
ct,v :=

Θ
ft,vdλ
((t, v) ∈Cλ).
Then, Q := {qt,v}(t,v)∈Cλ is a family of conjugate priors for P. For given data
x0 ∈X, the hyperparameter of the posterior is updated as follows:
(t, v) →(t + 1, v + T(x0)).
(3)
Remark 6. In [DY79], they considered only the Lebesgue measure as the mea-
sure λ on Θ. However, we can take any Radon measure on Θ by the deﬁnition
of conjugate prior. In this paper, we take a measure compatible with the group
action on Θ.

90
K. Tojo and T. Yoshino
2
Main Theorem
In this section, we state our main theorem and give its proof.
2.1
Main Theorem
In this subsection, we give a family of conjugate priors explicitly for the family of
Poincar´e distributions (Theorem 1). Since the normalizing constant of conjugate
priors is described with explicit form, our family is useful in Bayesian inference.
To state our main theorem, we use the following:
Setting 2. Let G := SL(2, R), H := SO(2) and V := Sym(2, R). We consider
the action of G on Θ := Sym+(2, R) ⊂V ∨as given in Proposition 1. As we will
see in Lemma 1 in page 9, we identify G/H with the upper half plane H and Θ
with R>0 × H as G-spaces.
Theorem 1 (Main theorem). The following family of distributions on Θ is a
family of conjugate priors for the family of Poincar´e distributions (Example 1):

c−1
t,v
re2r
π
	t
exp(−(αa + 2βb + γc))dadbdc
2r2

(t,v)∈C

a b
b c
	
∈Θ
	
,
(4)
where r :=
√
ac −b2 and v =
α β
β γ
	
∈Sym+(2, R) ⊂V . The hyperparameter
space C and the normalizing constant ct,v are given as follows:
C :=

t,
α β
β γ
		
∈R>0 × Sym+(2, R)
 t < D

,
ct,v :=
Γ(t)
2tπt−1D(D −t)t ,
where D :=

αγ −β2.
To prove Theorem 1, we use the coordinate (r, ξ + iη) ∈R>0 × H based on
the identiﬁcation R>0 × H ≃Θ in Setting 2. Then the family (4) is rewritten as
follows:

c−1
t,v
re2r
π
	t
exp

−r
γ(ξ2 + η2) −2βξ + α
η
		
drdξdη
η2

(t,v)∈C
.
(5)
This comes from
a = r
η , b = −rξ
η , c = r(ξ2 + η2)
η
,
drdξdη
η2
= dadbdc
2r2
.

An Exponential Family on the Upper Half Plane
91
Proposition 3. The family of conjugate priors in Theorem 1 is also a G-
invariant exponential family on Θ.
Proof. From Proposition 4, each element of the family (4) is a probability mea-
sure on Θ. It is clear that the family is an exponential family on Θ. Since the
measure dr dξdη
η2
is G-invariant, it is enough to check that {ft,v}(t,v)∈C is closed
under the action of G (see “Proof of main theorem” below for the deﬁnition
of ft,v). For g ∈G, we have gft,v = ft,gvtg. Since the G-action on Sym(2, R)
preserves the determinant, the hyperparameter (t, gvtg) is also in C if (t, v) is an
element of C.
⊓⊔
We prove Theorem 1 by using the next proposition, which is just an integral
computation. A proof of the proposition is given in the next subsection.
Proposition 4. For t ∈R>0, α, β, γ ∈R, we have

(r,x,y)∈R>0×R×R>0
re2r
π
	t
exp

−r
γ(x2 + y2) −2βx + α
y
		
drdxdy
y2
(6)
=

Γ(t)
2tπt−1D(D−t)t
(γ > 0 and t2 < αγ −β2),
∞
(otherwise).
(7)
Here D :=

αγ −β2.
Proof (Main theorem). We show that the family above coincides with the family
Q in Proposition-Deﬁnition 2 if we take a measure dλ = dr dξdη
η2
∈R(Θ). The
function ft,v of (2) is given as follows:
ft,v(r, ξ + iη) = exp

−Trace
r
η
 1
−ξ
−ξ ξ2 + η2
	 α β
β γ
			 re2r
π
	t
=
re2r
π
	t
exp

−r
γ(ξ2 + η2) −2βξ + α
η
		
Then from Proposition 4, the family in Theorem 1 coincides with the
family Q.
⊓⊔
Remark 7. For given data x0 + iy0 ∈H, the hyperparameter is updated as
follows:
C →C,

t,

α β
β γ
		
→

t,

α β
β γ
		
+

1, 1
y0

x2
0 + y2
0 x0
x0
1
		
.
This comes from (3) and the identiﬁcation H and G/H in Lemma 1. We can
show that the updated hyperparameter is also in C directly by using Minkowski’s
determinant theorem (Fact 3). It is enough to check the inequality

det

α β
β γ
	
+ 1
y0

x2
0 + y2
0 x0
x0
1
			 1
2
> t + 1.

92
K. Tojo and T. Yoshino
From Fact 3, we have

det

α β
β γ
	
+ 1
y0

x2
0 + y2
0 x0
x0
1
			 1
2
≥

det

α β
β γ
		 1
2
+

det 1
y0

x2
0 + y2
0 x0
x0
1
		 1
2
> t + 1.
Since C is closed under multiplication of positive scalars, this essentially shows
that C is convex. Namely, C is a convex cone.
Fact 3 (Minkowski’s determinant theorem, [MM64, §4.2]). For positive
deﬁnite Hermitian matrices A, B of size n, the following inequality holds:
det(A + B)
1
n ≥(det A)
1
n + (det B)
1
n .
The identiﬁcation Θ with R>0 × H in Setting 2 comes from the following:
Lemma 1. We consider SL(2, R)-actions on H, R>0 and Θ as the linear frac-
tional action, the trivial action and the action in Proposition 1, respectively.
Then, the following maps give isomorphisms as SL(2, R)-spaces.
1.
H →G/H →{S ∈Θ | det S = 1},
ξ + iη →
√η
ξ
√η
0
1
√η

H →1
η
 1
−ξ
−ξ ξ2 + η2
	
.
Here we consider the action on the subset above of Θ as the restriction of
that on Θ.
2.
R>0 × {S ∈Θ | det S = 1} →Θ,
(r, S) →rS.
3.
R>0 × H →Θ,
(r, ξ + iη) →r
η

1
−ξ
−ξ ξ2 + η2
	
.
Since the proof of this lemma is elementary, we omit it.
2.2
Proof of Proposition 4
In this subsection, we give a proof of Proposition 4 at the end of this subsection.
First, we prepare the following lemma to show next Lemma 3.

An Exponential Family on the Upper Half Plane
93
Lemma 2. For r ∈R>0, we have
 ∞
0
exp(−r(x −x−1)2)dx = 1
2
π
r .
Proof. Since r is positive, this integral converges. We put Ir :=

 ∞
0
exp(−r(x −
x−1)2)dx. By the change of variable x = s−1, we have dx = −s−2ds and
Ir =
 ∞
0
s−2 exp(−r(s −s−1)2)ds.
Therefore, we get 2Ir =

 ∞
0 (1 + s−2) exp(−r(s −s−1)2)ds. Moreover, we put
u := s −s−1. Since we have du = (1 + s−2)ds, we obtain
Ir = 1
2
 ∞
−∞
exp(−ru2)du = 1
2
π
r .
Next, we consider the following lemma to prove Lemma 4.
Lemma 3. For a, b ∈R, we have
 ∞
0
x−3
2 exp(−(ax + bx−1))dx =
 π
b e−2
√
ab
(a, b > 0),
∞
(otherwise).
(8)
Proof. We divide the integral interval into two parts [1, ∞) and (0, 1]. Then,
the integrability condition implies a > 0 and b > 0, respectively. In the case of
a, b > 0, by the change of variable s =
 b
a
 1
4 x−1
2 , we have x−3
2 dx = −2
 a
b
 1
4 ds
and
 ∞
0
x−3
2 exp(−(ax + bx−1))dx =
 ∞
0
2
a
b
 1
4 exp(−
√
ab(s2 + s−2))ds =

π
b e−2
√
ab.
In the last equality above, we used Lemma 2.
⊓⊔
Then, we show the following lemma for Example 1 and Proposition 4.
Lemma 4. For a, b, c ∈R, we have

(x,y)∈R×R>0
exp

−a(x2 + y2) + 2bx + c
y
 dxdy
y2
=

π
De2D
(a > 0 and ac −b2 > 0),
∞
(otherwise).
Here, D :=
√
ac −b2.
Proof. From Tonelli’s theorem, we have

(x,y)∈R×R>0
exp

−a(x2 + y2) + 2bx + c
y
	 dxdy
y2
(9)
=
 ∞
0
exp

−ay −c
y

y2
 ∞
−∞
exp

−ax2 + 2bx
y
	
dx
	
dy.
(10)

94
K. Tojo and T. Yoshino
For y > 0, by Gaussian integral, we have
 ∞
−∞
exp

−ax2 + 2bx
y
	
dx =
 πy
a exp

b2
ay

(a > 0),
∞
(otherwise).
Therefore, in the case of a > 0, we get
(10) =
π
a
 ∞
0
y−3
2 exp

−

ay + ac −b2
ay
		
dy.
(11)
Then, Lemma 3 implies
(11) =
 π
a

aπ
ac−b2 e−2
√
ac−b2 =
π
De2D
(ac −b2 > 0),
∞
(otherwise).
Finally, let us give a proof of Proposition 4.
Proof (Proposition 4). By Tonelli’s theorem, we have
(6) =
 ∞
0

re2r
π
	t 
(x,y)∈R×R>0
exp

−r

γ(x2 + y2) −2βx + α
y
		
dxdy
y2

dr
=
⎧
⎨
⎩

 ∞
0

re2r
π
t
π
rDe2rD dr
(γ > 0 and αγ −β2 > 0),
∞
(otherwise).
In the second equality, we used Lemma 4 and put D :=

αγ −β2. In the case
of γ > 0 and αγ −β2 > 0, from Note 1, we have
(6) =
1
Dπt−1
 ∞
0
rt−1e−2r(D−t)dr
=

Γ(t)
2tπt−1D(D−t)t
(t < D),
∞
(otherwise).
We have proved Proposition 4.
⊓⊔
Note 1. For s, t ∈R, by the deﬁnition of Gamma function, we have
 ∞
0
rt−1e−rsdr =
 Γ(t)
st
(s, t > 0),
∞
(otherwise).
Acknowledgements. We thank Professor Kei Kobayashi for useful comments. This
work was supported by JST, ACT-X Grant Number JPMJAX190K, Japan.

An Exponential Family on the Upper Half Plane
95
References
[BN70] Barndorﬀ-Nielsen, O.E.: Exponential family: exact theory, Various Publica-
tion Series, No. 19, Matematisk Institut, Aarhus Universitet, Aarhus (1970)
[BN78a] Barndorﬀ-Nielsen, O.E.: Information and Exponential Families in Statistical
Theory. Wiley, Chichester (1978)
[BN78b] Barndorﬀ-Nielsen, O.E.: Hyperbolic distributions and distribution on hyper-
bolae. Scand. J. Statist. 8, 151–157 (1978)
[DY79] Diaconis, P., Ylvisoker, F.: Conjugate priors for exponential families. Ann.
Stat. 7(2), 269–281 (1979)
[F97] Fink, D.: A compendium of conjugate priors, CiteSeerX 10.1.1.157.5540
(1997)
[J81] Jensen, J.L.: On the hyperboloid distribution. Scand. J. Statist. 8, 193–206
(1981)
[MM64] Marcus, M., Minc, H.: A Survey of Matrix Theory and Matrix Inequalities.
Allyn and Bacon Inc., Boston (1964)
[RS61] Raiﬀa, H., Schlaifer, R.: Applied statistical decision theory. Graduate School
of Business Administration, Harvard University, Division of Research (1961)
[TY18] Tojo, K., Yoshino, T.: A method to construct exponential families by repre-
sentation theory, arXiv:1811.01394v3
[TY20] Tojo, K., Yoshino, T.: Harmonic exponential families on homogeneous spaces.
Info. Geo. (2020). https://doi.org/10.1007/s41884-020-00033-3

Wrapped Statistical Models on Manifolds:
Motivations, The Case SE(n),
and Generalization to Symmetric Spaces
Emmanuel Chevallier1(B) and Nicolas Guigui2
1 Aix Marseille Univ, CNRS, Centrale Marseille, Institut Fresnel,
13013 Marseille, France
emmanuel.chevallier@fresnel.fr
2 Universit´e Cˆote d’Azur, Inria Epione, 06902 Sophia Antipolis, France
nicolas.guigui@inria.fr
Abstract. We address here the construction of wrapped probability
densities on Lie groups and quotient of Lie groups using the exponen-
tial map. The paper starts by brieﬂy reviewing the diﬀerent approaches
to build densities on a manifold and shows the interest of wrapped dis-
tributions. We then construct wrapped densities on SE(n) and discuss
their statistical estimation. We conclude by an opening to the case of
symmetric spaces.
1
Introduction
Consider X1, .., Xk i.i.d. random variables Ω →M on the manifold M endowed
with the volume measure v. Given a realisation of these variables we would like
to estimate the underlying distribution.
An important problem in density estimation on manifolds is that most algo-
rithms lead to heavy computations, partially because the standard probability
densities do not enjoy the same properties as in the vector space case. Hence
the motivation for building statistical models leading to algorithms with reduced
amount of computation.
Wrapped models for directional statistics are constructed from distributions
on R or Rn wrapped around the circle or the torus by the exponential map. Deﬁne
informally wrapped models on manifolds as densities pushed from tangent spaces
to the manifold. Due to their interesting properties in terms of computational
complexity and invariances, we chose to focus on wrapped models based on the
exponential map of Lie groups or on the exponential of Riemannian manifolds
arising as quotients of Lie groups. Though in most wrapped distributions for
directional statistics the densities are expressed as series, we will restrict our
study to the case where distributions in tangent spaces are contained in injec-
tivity domains of the exponential maps. Hence densities on the manifold are
expressed using the Jacobian of the exponential map at only one point of the
tangent space, and not by series (Fig. 1).
c
⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2021
F. Barbaresco and F. Nielsen (Eds.): SPIGL 2020, PROMS 361, pp. 96–106, 2021.
https://doi.org/10.1007/978-3-030-77957-3_5

Wrapped Statistical Models on Manifolds
97
Fig. 1. A set of i.i.d. draws x1, .., xk and the level lines of the estimated density.
In Sect. 2, we present some classical probability densities on manifolds. In
Sect. 3, we review the relevant properties of probability densities and statistical
models on manifolds, and describe wrapped statistical models. In Sect. 4, we
summarize results of [4] and describe the case SE(n). The last section is an
opening towards the general case of symmetric spaces.
2
Some Classical Probability Densities on Manifolds
Families of densities on manifolds are often deﬁned in order to verify particular
properties like maximizing an entropy constraint, having a particular form for
the maximum likelihood estimator or verifying a particular pde. For instance
Gaussian distributions deﬁned in [8] on Riemannian manifolds,
fx0(x) = α(γ, x0)e−d(x,x0)2
2γ2
(1)
and
fx0(x) = α(Γ, x0)e−Γ(logx0(x),logx0(x))
(2)
were γ ∈R and Γ is a positive deﬁnite bilinear form, are the maximal entropy
distributions given their ﬁrst and second moments, see Sect. 3.2 for the deﬁnition
of moments. As shown in [11], on non-compact symmetric spaces distributions
(1) are also such that the maximum likelihood estimator of parameter x0 is the
empirical mean.
Another usual way of deﬁning Gaussian distributions on manifolds is by using
a Laplacian operator: Gaussian distributions deﬁned as heat kernels, i.e. Green
functions of the heat equation ∂f
∂t = −Δf.
Due to its practical importance several distributions have also been proposed
on spheres, such as the Fisher distributions and their anisotropic counterpart,
the Kent distributions:
∀μ, x ∈S2, κ ∈R,
fμ,κ(x) =
κ
4π sinh(κ)eκμT x
(Fisher)
(3)
∀x ∈S2, κ, β ∈R,
fγi,κ,β(x) =
1
c(κ, β)eκγT
1 x+β(γT
2 x)2−(γT
3 x)2)),
(Kent)
(4)
where γ1 ⊥γ2 ⊥γ3 ∈S2, see [5] and [6].

98
E. Chevallier and N. Guigui
None of the previous distributions are naturally interpreted as wrapped dis-
tributions. Wrapped distributions are not deﬁned by a theoretical property but
by the procedure to construct them: given a measurable map φ from a tan-
gent space Tx0M to the manifold, wrapped densities are densities which are
ﬁrst deﬁned on a tangent space and pushed forward by φ. In the present paper,
we only address the case where φ is a diﬀeomorphism on a neighborhood U of
0 ∈Tx0M. Given a Lebesgue measure on Tx0M and a probability density h
on Tx0M whose support is included in U, the density of the push forward at
x ∈φ(U) is given by
f(x) = (φ∗h)(x) =
h(x)
J(φ−1(x))
where J is the Jacobian determinant of φ. Among choices of φ an interesting
candidate is the exponential map, due to its algebraic and geometric properties.
Recall that on a Lie group, the exponential map at identity is extended to
arbitrary points g as
expg = Lg ◦exp ◦dLg−1,
where Lg is the left multiplication by g and dLg its diﬀerential. Since Lg ◦Rg−1 ◦
exp = exp ◦dLg ◦dRg−1, the deﬁnition remains the same if left multiplication are
replaced by right multiplications. Extending the Lie group exponential in this
way enables to push densities from arbitrary tangent spaces to the group. On spe-
ciﬁc manifolds, other projection-retraction maps might present computational
advantages over the exponential map while preserving the desired invariances,
especially when the Riemannian exponential cannot be computed explicitly, see
[1]. Nonetheless, we chose focus on the exponential map.
3
Some Important Characteristics of Statistical Models
on Manifolds
3.1
Expression of the Density Functions
Many of the common probability densities on manifolds do not have explicit
expressions. For Gaussian distributions deﬁned in Eq. 1 and Eq. 2, or for the
Kent distribution on spheres, the normalizing constant has an explicit expression
only in exceptional cases. Even though this factor can be numerically estimated,
it is interesting to construct densities whose expressions are fully known.
Heat kernels have a similar problem: they can be computed only on a few
exceptional manifolds. Furthermore while being a natural object on Riemannian
manifolds, there is usually not a canonical Laplacian on Lie groups.
For wrapped distributions, the expressions require knowing φ−1 and J. It
turns out that when φ is an exponential map restricted to an injective domain,
φ−1 and J can be computed on numerous Lie groups and quotients of Lie groups.

Wrapped Statistical Models on Manifolds
99
3.2
Moments
Let M be a Lie group or a Riemannian manifold endowed with the Haar or
Riemannian measure v. Following [9], we say that ¯x is a mean of the density f
when
Ef (log¯x(x)) =

M
log¯x(x)f(x)dv(x) = 0.
Hence ¯x is an mean if the vectorial mean on the distribution lifted in T¯xM is
zero. This expression assumes the existence of Ef (log¯x(x)), hence the existence
of a mean is related to the injectivity and surjectivity of the exponential map.
Given the expression of a density f, computing the mean is not always easy.
Hence, an important property of Gaussians of Eq. 1–2 is that when expx0 is a
bijection between Tx0M and M the parameter x0 is the unique mean of the
distribution. Due to the symmetry of the sphere, it can also be checked that μ
and γ1 are means of the Fisher and Kent distributions.
When a mean ¯x exists, we deﬁne the corresponding higher order moments as
in the vectorial case after lifting the distribution in T¯xM,
Tn = Ef

log¯x(x)⊗n
∈(T¯xM)⊗n.
The covariance being the second order moment, we have
Σ = Ef (log¯x(x) ⊗log¯x(x)) .
(5)
Note that when the mean is not unique, higher-order moments are deﬁned with
respect to a mean.
Unfortunately, the parameters γ or Γ of Gaussians of the form (1)–(2) do
not have an explicit link with the second moments of the distributions, see
[8,11]. Similarly, to our knowledge the covariance of the Kent distribution is not
explicitly related to γi, κ and β.
An advantage of wrapped distribution is that they can have prescribed
moments, which is rarely the case for other distributions. Let h be a proba-
bility density on T¯xM such that, its support is included in an injectivity domain
of exp¯x, its barycenter is 0, its covariance is Σ and such that h(x) = h(−x). Let
f(x) = (exp¯x∗h)(x) =
h(x)
J(log(x))
(6)
be the push forward. It can be shown (it is clear) that ¯x is a mean of the
probability density f. It follows, by deﬁnition (5), that the covariance of f with
respect to ¯x is Σ. Under some assumptions on Σ, it can also be shown that ¯x is
unique.
3.3
Invariances and Estimation
Given a statistical model S of densities on M, the construction of estimators
from a set of i.i.d. samples is a central problem. Let T be an estimator, that is

100
E. Chevallier and N. Guigui
to say a function which maps sets of samples x1, .., xk ∈M to elements in S.
There are mostly three criteria to evaluate the interest of T as an estimator (the
quality of an estimator T). The ﬁrst criterion is how fast T(x1, .., xk) converges
to f as the number of samples increases, when samples are i.i.d. from f ∈S.
The second criterion is the ease of evaluation of T. And the third criterion is
the invariance by symmetry: when a group G acts on M, T should commute
with the action. Implicitly, this assumes that the statistical model S is invariant
under the pushforward action of G on densities, and that
T(g.x1, .., g.xk) = g∗.T(x1, .., xk) ∈S.
For instance, when M is a Riemannian manifold, G is the group of isometries
and when M is a Lie group, G = M is the group itself acting by left or right
multiplications.
As mentioned in Sect. 2, Gaussians of Eq. 1 on non-compact symmetric spaces
parametrized by their mean and variance verify the interesting properties: the
maximum likelihood estimation of the mean is the empirical mean and the vari-
ance is a simple function of the empirical covariance. The set of Gaussians is
invariant under isometries and the densities are easily computed once the nor-
malizing constant has been numerically estimated. Hence statistical models on
non-compact symmetric spaces based on Gaussians of Eq. 1 have several desirable
properties, their main limitation being that they only contain isotropic densities.
On the other hand Gaussians of Eq. 2 can model anisotropy but no results have
been shown on maximum likelihood estimation and the relations between the
covariance parameter Γ and the normalizing factor is more involved than in the
isotropic case. Beyond this limitation, Gaussians of types (1) and (2) lose many
of their properties when the exponential maps are not bijections, for instance
on sphere and more generally on non-compact symmetric spaces. On the other
hand, the estimation of parameters of the Kent distributions on spheres is not
straightforward. Hence our motivation for studying wrapped statistical models.
3.3.1
Wrapped Statistical Models
In this section we introduce the
wrapped statistical models deﬁned by a kernel K. Let K : R+ →R+ be a
function such that
K(x ≥1) = 0,

Rn K(∥x∥) = 1,

Rn xK(∥x∥) = 0.
Let

Rn xxtK(∥x∥) = βI,
where n is the dimension of M and I is the identity matrix. Since K(x ≥1) = 0,
we have 0 < β < 1. Given a symmetric positive deﬁnite bilinear form B on TxM,
we deﬁne the density hx,B by
hx,B(u) = K(

βB(u, u)).

Wrapped Statistical Models on Manifolds
101
It can be checked that hx,B is a probability density on TxM with respect to the
Lebesgue measure normalized by βB. Since B is deﬁnite it induces an isomor-
phism between TxM and TxM∗. B itself can thus be mapped to a bilinear map
Σ on TxM∗. If M is the matrix of B in a basis, M −1 is the matrix of Σ on the
dual basis. Again, it can be checked that Σ is the covariance of hx,B, and we will
now write hx,Σ. The push forward of hx,Σ by the exponential is now a density
fx,Σ = expx∗(hx,Σ)
on M. If the support of hx,Σ is contained in an injectivity domain we have Eq. 6,
fx,Σ =
hx,Σ(x)
J(log(x)).
In many examples there is an injectivity domain in the tangent space of the
form U = B + E ⊂TxM where B is an open ball in a vector space F and E
a vector space such that TxM = E ⊕F. It is for instance on case on SE(n),
see Sect. 4. Imposing that the density is supported in U can then be done in the
following way. Note I the bilinear form associated with the ball B and deﬁne the
admissible set of covariance Ax as
Ax = {Σ ∈TxM ⊗TxM|
∀u ∈F, βBΣ (u, u) ≥I(u, u)},
where as previously BΣ is the bilinear form on TxM ⊗TxM associated with Σ.
The statistical model on M constructed from the function K is then deﬁned by
S = {fx,Σ|x ∈M, Σ ∈Ax}.
(7)
As mentionned in Sect. 3.2, x is not always the unique mean of fx,Σ, nonetheless
it is true for suﬃciently small Σ.
An important strength of this approach is that it only depends on the expo-
nential and its Jacobian, which are known on many Lie groups and quotient of
Lie groups. Thus, it can be applied to numerous manifolds, it is for instance a
natural way to build models on compact symmetric spaces.
3.3.2
Estimators in Wrapped Statistical Models
By construction, the exponential map commutes with the action of isometries
on a Riemannian manifold, and with the action of group multiplications on a Lie
group. Hence by construction the model S has the desired symmetry properties.
Maximum likelihood estimation in S is unfortunately non trivial in most cases.
On the other hand, S is constructed such that the moment matching estimator
has a simple form.
The ﬁrst step of the moment matching estimator consists in computing the
empirical mean ˆx of the sample and is achieved with a gradient descent. The sec-
ond step consists in computing the empirical covariance ˆΣ in TˆxM and choosing
a covariance p(ˆΣ) in Aˆx as close as possible to ˆΣ. This choice can be achieved in
the following way. As in Sect. 3.3.1, consider an injectivity domain of the form

102
E. Chevallier and N. Guigui
U = B + E ⊂TxM. Let I be the bilinear form associated with B on the vector
space F generated by B and let f be the self-adjoint endomorphism on F deﬁned
by
∀u, v ∈F, β ˆB (u, v) = I(u, f(v)).
Note λ the smallest eigenvalue of f. If λ ≥1 then ˆΣ ∈Aˆx. If λ < 1, we have
β ˆB (u, u) = λI(u, u) < I(u, u),
for some u ∈F, and ˆΣ /∈Ax. It is easy to see that the smallest multiple of ˆB
compatible with Aˆx is 1
λ ˆB, hence a simple choice in Aˆx is
p( ˆB) = 1
λ
ˆB,
p(ˆΣ) = λˆΣ.
Currently, there are only few results on moments estimation on manifolds. In
[2], authors proved a central limit theorem for the empirical mean on Riemannian
manifolds, and the author of [10] establishes results on the moments of the
empirical mean on manifolds with an aﬃne connection. To our knowledge, there
are still no results on the estimation of the higher order moments, either on
Riemannian manifolds, on Lie groups, or more generally on manifolds with an
aﬃne connection.
We have seen that the moment matching estimator is easy to compute, has
the desired invariances, but convergence properties remain to be shown.
4
Probability Densities on SE(n)
In this section we summarize results from [4] and describe the construction of
wrapped models on the special Euclidean group SE(n).
4.1
Wrapped Models on SE(n)
As a manifold SE(n) is a product between SO(n) and Rn but the group structure
is a semi-direct product:
SE(n) = SO(n) ⋉Rn
(R, t)(R′, t′) = (RR′, Rt′ + t)
and elements of its Lie algebra are parametrized by couples (A, T) where A is
a skew-symmetric matrix and T ∈Rn. Recall that a skew-symmetric matrix
can be block-diagonalized with 2 by 2 rotations on the diagonal, followed by a
0 when the dimension is odd. For each n by n skew-symmetric matrix A, we
note θ1, . . . , θ⌊n
2 ⌋the set of angles of the 2 by 2 rotations. Let U be the subset
in TeSE(n) deﬁned by
U = {u = (A, T)|
θi ∈[−π, π[, i = 1, . . . , n(n−1)
2
}

Wrapped Statistical Models on Manifolds
103
It can be checked that the exponential map on U is a bijection. Hence we can
deﬁne the logarithm on SE(n) as the inverse of the exponential on U. Isometries
(R, t) of SE(n) can be embedded in GL(n + 1) as

R t
0 1

∈GLn+1(R).
which enable to compute the exponential map of SE(n) using the matrix expo-
nential of GL(n + 1) and the logarithm using the matrix principal logarithm.
In order to be consistent with the previous Sect. 3.3.1, U should in fact be
restricted to a domain ˜U of the form B × E,
˜U = {u = (A, T)|

trace(AAT ) < π}.
The diﬀerential of the exponential map on Lie groups at u in the Lie algebra
is given by the following formula:
d expu = dLexp u ◦
⎛
⎝
k≥0
(−1)k
(k + 1)!adk
u
⎞
⎠.
It can be checked that on SE(n), volume forms induced by left invariant ﬁelds
of basis are also right invariant: the group has a bi-invariant Haar measures. Fix
an arbitrary reference basis e1, .., ep of the Lie algebra TeSE(n) and consider
the corresponding left invariant ﬁeld of basis. The computation of the Jacobian
determinant J in a left (or right) invariant basis ﬁeld gives,
J = det(d expu) =

i
21 −cos(θi)
θ2
i
α
×..

i<j

4.1 + cos(θi + θj)
(θi + θj)2
.1 + cos(θi −θj)
(θi −θj)2

where α = 1 when n is even and α = 2 when n is odd and θi are the angles of the
planar rotations of the block diagonalization of A. This simpliﬁes to J(θ, T) =
2 1−cos(θ)
θ2
 on SE(2).
Given a kernel K on R+ we can deﬁne the wrapped probability distribution
fg,Σ(expg(u)) =
1
J(u)

det(Σ)
K

βutΣ−1u

,
where u and Σ are respectively a coordinate vector and a positive deﬁnite matrix
expressed in the reference left invariant basis ﬁeld.

104
E. Chevallier and N. Guigui
4.2
Density Estimation on SE(n)
As noted in Sect. 3.3.2, there are no theoretical results on the convergence the
moment matching estimator. Hence we will only mention the empirical results
obtained in [4] for the case SE(2). Samples are drawn from a uniform distribu-
tion fe,Σ on a ball in the injectivity domain in the Lie algebra. The empirical
moments are then computed using the Python package geomstats, see [7], and
the error between the original density and the estimated densities are computed
using the L1 distance between densities ∥fe,Σ −fˆx,ˆΣ∥1. The L1 distance between
densities is the total variation between measures and present the interest of being
independent of the reference measure. The L1 error is compared to the same esti-
mation error in R3 where the density is estimated directly in TeSE(2) ∼R3 using
a vectorial moment matching estimator. Results displayed in Fig. 2 indicate that
the errors of the SE(2) and R3 problems are asymptotically related by a factor
close to 1.
5
Towards a Generalization to Symmetric Spaces
As seen in previous sections, the main challenges in building wrapped models
are to compute the Jacobian determinants and to ﬁnd injectivity domains for
the exponential maps. We show here that symmetric spaces are an interesting
class of manifolds regarding the computation of the Jacobian determinant. Con-
sider a manifold M with a connection ∇. The manifold M is said to be locally
symmetric when
∇R = 0
where R is the curvature tensor of ∇. Recall that Jacobi ﬁelds Y (t) along a
geodesic γ are solution of the Jacobi equation,
∇γ(∇γY )(t) + R(γ′(t), Y (t))γ′(t) = 0
or
∇γ(∇γY )(t) + ˜R(t) (Y (t)) = 0
where ˜R(t) is the linear map Y (t) →R(γ′(t), Y (t))γ′(t). Let u be a tangent
vector in TxM and Yu be the Jacobi Field along γ with initial condition Y (0) =
0 ∈TxM and

∇γ′(0)Y

(0) = u. For t > 0, the diﬀerential of the exponential
at tγ′(0) in the direction u is given by,
∂expx
∂u
(tγ′(0)) = 1
t Y (t).
Hence the Jacobian determinant along tγ′(0) is given by
J(t) = det (A(t))
where A follows the second order diﬀerential equation
∇γ(∇γA)(t) + ˜R(t)A(t) = 0.
When the space is locally symmetric, the coeﬃcient ˜R(t) is constant. Hence
the symmetric property is a natural framework to study the Jacobian of the
exponential map.

Wrapped Statistical Models on Manifolds
105
Fig. 2. L1 errors and their ratios on SE(2) and TeSE(2) ∼R3, see ([4]).
6
Conclusion
Across the paper, we have seen how wrapped models can be constructed on
manifolds with an exponential map. We have analysed the case of the Lie group
SE(n) and mentioned Riemannian manifolds, though such wrapped models can
be considered on any manifold with an aﬃne connection. The main limitation of
these models is that the underlying manifold should have an exponential map, a
log map and a Jacobian which can be computed at a reasonable cost. Note that
this is often the case when the manifold is a homogeneous space. Future eﬀorts

106
E. Chevallier and N. Guigui
should focus on three problems. The ﬁrst one is to obtain convergence results for
the density estimation using wrapped models. The second one is to understand
and characterize the shape of the injectivity domains in the diﬀerent possible
settings, such as Lie groups and Riemannian symmetric spaces, see [12]. And the
third problem is to identify on which manifolds with an aﬃne connection the
Jacobian of the exponential map can be computed explicitly. As shown in Sect. 5,
symmetric spaces seem to be an interesting setting to study this property. Note
however that there are known examples outside of this class where the Jacobian
is explicit, see for instance the Wasserstein metric on Gaussian distributions [3].
Funding. N. Guigui has received funding from the European Research Council (ERC)
under the European Union’s Horizon 2020 research and innovation program (granta-
greement G-Statistics No 786854.
References
1. Absil, P.-A., Malik, J.: Projection-like retractions on matrix manifolds. SIAM J.
Optim. 22(1), 135–158 (2012)
2. Bhattacharya, R., Patrangenaru, V.: Large sample theory of intrinsic and extrinsic
sample means on manifolds: II. Ann. Stat. 1225–1259 (2005)
3. Chevallier, E., Kalunga, E., Angulo, J.: Kernel density estimation on spaces of
Gaussian distributions and symmetric positive deﬁnite matrices. SIAM J. Imaging
Sci. 10, 191–215 (2017)
4. Chevallier, E., Guigui, N.: A bi-invariant statistical model parametrized by mean
and covariance on rigid motions. Entropy 22(4), 432 (2020). https://doi.org/10.
3390/e22040432
5. Fisher, R.A.: Dispersion on a sphere. Proc. R. Soc. Lond. Ser. Math. Phys. Sci.
217(1130), 295–305 (1953)
6. Kent, J.T.: The Fisher-Bingham distribution on the sphere. J. R. Stat. Soc. Ser.
B (Methodol.) 44(1), 71–80 (1982)
7. Miolane, N., Guigui, N., Le Brigant, A., Mathe, J., Hou, B., et al.: Geomstats: a
python package for riemannian geometry in machine learning (2020). hal-02536154
8. Pennec, X.: Intrinsic statistics on Riemannian manifolds: basic tools for geometric
measurements. J. Math. Imaging Vis. 25, 127 (2006)
9. Pennec, X., Arsigny, V.: Exponential barycenters of the canonical Cartan connec-
tion and invariant means on Lie groups. In: Matrix Information Geometry, pp.
123–166. Springer, Heidelberg (2013)
10. Pennec, X.: Curvature eﬀects on the empirical mean in Riemannian and aﬃne man-
ifolds: a non-asymptotic high concentration expansion in the small-sample regime
(2019). arXiv preprint arXiv:1906.07418
11. Said, S., Hajri, H., Bombrun, L., Vemuri, B.C.: Gaussian distributions on Rieman-
nian symmetric spaces: statistical learning with structured covariance matrices.
IEEE Trans. Inf. Theory 64(2), 752–772 (2017)
12. Yang, L.: Injectivity radius for non-simply connected symmetric spaces via Cartan
polyhedron. arXiv preprint math/0703521 (2007)

Galilean Thermodynamics of Continua
G´ery de Saxc´e(B)
Univ. Lille, CNRS, Centrale Lille, UMR 9013 – LaMcube – Laboratoire de m´ecanique
multiphysique multi´echelle, Lille, France
gery.de-saxce@univ-lille.fr
Abstract. We
take
the
Relativity
as
model,
process
termed
“geometrization”, but with Bargmann symmetry group, an extension of
Galileo’s group acting on a 5-dimension space. The entropy is generalized
as a 4-vector and the temperature as a 5-vector. The introduction of the
friction and momentum tensors allows to obtain a covariant formulation
of the ﬁrst and second principles of Thermodynamics.
1
Some Words of Introduction
As the general relativity is widely based on diﬀerential geometry, we regard this
construction as a geometrization of thermodynamics. Souriau proposed in [10,11]
such a formalism in general relativity. In his footstep, one can quote the work
by Vall´ee [16] who studied the invariance of constitutive laws in the context of
special relativity where the gravitation eﬀects are neglected.
In the present work, our goal is to develop a geometrization of thermodynam-
ics within the classical approximation where the velocity of the light is considered
inﬁnite, but nevertheless in the spirit of relativity. In other words, we want to pro-
pose a thermodynamics of classical continua, consistent with Galileo’s principle
of relativity. We draw our inspiration from the previously quoted geometrization
in the frame of general and special relativity but with some important infringe-
ments needed by the classical mechanics. The reader interested by more details
and the proofs of the results stated here, is referred to [3–5].
2
Space-Time and Galileo’s Group
Latin indices run over the spatial coordinate labels while Greek indices run over
all coordinates labels. The space-time will be consider as a diﬀerential manifold
U of dimension 4. A point X ∈U represents an event. The 4-column vector of its
coordinates (Xα)0≤α≤3 in a chosen frame will be denoted X. The tangent space
at X is ordinarily deﬁned as vector space but it can be also seen as endowed
with its structure of associated aﬃne space. Thereby the elements of the tangent
space can be perceived as points of an aﬃne space that we call tangent points
at X [4]. By the choice of an aﬃne frame deﬁned by an origin and a basis, we
c
⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2021
F. Barbaresco and F. Nielsen (Eds.): SPIGL 2020, PROMS 361, pp. 107–119, 2021.
https://doi.org/10.1007/978-3-030-77957-3_6

108
G. de Saxc´e
can associate to any tangent point its components stored in a column dX ∈R4.
To each change of aﬃne frames is associated an aﬃne transformation of R4:
dX′ = PdX + C
(1)
where P ∈GL(4) and C ∈R4. The frames in which the distances and times
are measured will be called Galilean. In such frames, X0 = t is the time and
Xi = xi for 1 ≤i ≤3 are the spatial coordinates. The aﬃne transformations
preserving the distances, the time durations, the uniform straight motions and
the oriented volumes are called Galilean transformations. Their linear part
is of the form:
P =

1 0
vt R

(2)
where vt ∈R3 is the velocity of transport, or Galilean boost, and R ∈SO(3)
is a spatial rotation. The set of all these transformations is a Lie subgroup of
the aﬃne group GA (4) called Galileo’s group. According to (1), it acts by aﬃne
representation on the space R4 of the components of the aﬃne points at X and
equips the space-time with a structure equivalent to the one proposed by Toupin
[13], taken up later on by Noll [8] and K¨unzle [7]. The toupinian structure of
the space-time is based on two canonical tensors, a semi-metric of signature
(0 + + +) and a linear form τ called clock form in the sequel. This neoclassic
modelling oﬀers a theoretical frame for the universal or absolute time and space.
Of course, the Euclidean transformations are particular Galilean transformations
only compound of a space translation and a rotation. As there are Euclidean
tensors, we can consider Galilean tensors by restricting the general action of
the linear group GL (4) to the subgroup of linear Galilean transformations.
Any coordinate change on U representing a rigid body motion and a clock
change:
x′ = (R (t))T (x −x0 (t)),
t′ = t + τ0
where t →R (t) ∈SO(3) and t →x0(t) ∈R3 are smooth mappings, and τ0 ∈R
is a constant, is called a Galilean coordinate change. Indeed, a space-time
coordinate change is Galilean if and only if the corresponding Jacobean matrix
is a linear Galilean transformation where:
vt = ϖ (t) × (x −x0 (t)) + ˙x0 (t)
(3)
is the well-known velocity of transport, with Poisson’s vector ϖ such that:
˙R = j (ϖ) R, j (ϖ) being the 3 × 3 skew-symmetric matrix such that:
j (ϖ) v = ϖ × v
(4)
There exists a family of coordinate systems which are deduced one from each
other by such coordinate changes. We call them Galilean coordinate systems.
Now we can state Galileo’s principle of relativity:

Galilean Thermodynamics of Continua
109
Principle The statement of the physical laws of the classical mechanics is the same
in all the Galilean coordinate systems.
To respect this principle in practice, the laws are expressed in a covariant
form, using a connection ∇that represents the gravitation in Relativity. The
tangent vector −→
U to the trajectory t →X(t) of a particle in the space-time is
called 4-velocity and is represented in this coordinate system by:
U = dX
dt =

1
˙x

=

1
v

(5)
the linear 4-momentum of a particle of mass m is deﬁned as T = m U. Following
´E. Cartan [2], we claim that the motion of the particle embedded in a gravitation
ﬁeld is governed by the covariant equation:
∇UT = ˙T + Γ(U) T = 0
(6)
where Γ(dX) is the 1-form valued connection matrix of components
(Γ(dX))α
β = Γα
μβdXμ, the coeﬃcients Γα
μβ being Christoﬀel’s symbols.
Following Souriau [9], the motion of a 3D continuum can be modelized by a
line bundle π0 : U →U0. Its total space U is the space-time. Its base space U0 of
dimension 3 is the set of material particles. The ﬁber π−1
0 (X0) over the particle
X0 is its trajectory. In a local chart of U0, the particle X0 is represented by
s′ ∈R3, called Lagrangian coordinates. In the literature, they are often chosen
as the coordinate of its position at a given date taken as reference. In local charts
of U and U0, the projection X →X0 = π0(X) is represented by the mapping
(t, x) →s′ = κ(t, x) which identiﬁes the material point located at position x
at time t. The space coordinates x are called Eulerian. Obviously s′ being an
invariant of the motion, the material derivative vanishes
ds′
dt = ∂s′
∂t + ∂s′
∂x
dx
dt = 0
(7)
or, owing to (5):
∂s′
∂X U =
∂s′
∂t
∂s′
∂x
 1
v

= 0
(8)
The Bargmannian transformations are aﬃne transformations d ˆX′ →
d ˆX = ˆP d ˆX′ + ˆC of R5 such that.
ˆP =
⎡
⎣
1
0
0
vt
R
0
1
2 ∥vt ∥2 vT
t R 1
⎤
⎦.
(9)
It is straightforward to verify that the set of the Bargmannian transformations
is a subgroup of GA(5) called Bargmann’s group. Bargmann’s group was
introduced to solve problems of group quantization [1]. In fact, Galileo’s group
is not quantizable [12] and the reason of this failure is cohomologic. Although
Bargmann’s group was introduced for applications to quantum mechanics, it

110
G. de Saxc´e
turns out to be also very useful in thermodynamics. In the geometric quantiza-
tion, it is linked to the construction of the prequantum manifold, a ﬁber bundle
of base space the space-time. Its ﬁbers are lines along which the extra coordi-
nate has the physical meaning of the action (in the sense of the principle of least
action). In thermodynamics, the need to recover in an uniﬁed formalism both
the balance of the mass and energy leads to introduce an extra coordinate that
has also the meaning of an action. This point will be explained in the comment
following hypothesis (H2).
3
Geometric Structure of Thermodynamics
Now, we present in axiomatic form the corner stones of the theory involving
an underlying geometric structure of the Nature. This theoretical frame can
now appear rather arbitrary but that will be justiﬁed further by the agreement
with the physical phenomena. Therefore, we propose to characterize a Galilean
thermodynamics including gravitational eﬀects on the basis of ﬁve hypothesis
presented below with some comments:
(H1) There exists a line bundle π0 : U →U0 which characterizes the matter
and its evolution where U is a manifold of dimension 4 called space-time.
Comments. This hypothesis has been already discussed in Sect. 2. The space-
time U is a ﬁber bundle of which the base space is the set U0 of material particles.
The ﬁber over each particle is its trajectory. It is essentially an elegant way to
modelize the moving matter.
(H2) The universe is a line bundle π : ˆU →U where the manifold ˆU is of
dimension 5. U is perceived as a submanifold of ˆU as image of a section ˆf : U →
ˆU
Comments. The reality is that we are living in a 4D space-time. In special
and general relativity, the mass and the energy of a particle are strongly linked
by e = mc2 and it is not necessary to consider an extra dimension. In galilean
relativity, this two essential quantities are, in contrast, independent one of each
other (for particles at rest, the mass is non zero while the (kinetic) energie
vanishes). The space ˆU is a pure mathematical artefact to take into account this
oddness of classical mechanics. Working in the manifold ˆU amounts to introduce
a ﬁfth dimension of action (in the sense of the principle of least action).
(H3) U is equipped with a Galilean connexion and ˆU with the corresponding
Bargmannian connection representing the gravitation.
Comments. The interpretation of the connection as representing the gravitation
ﬁeld comes from the general relativity. It is the classical version that will be
detailed further. The Bargmannian connection can be seen as an asymptotic
expansion up to the order one in the small parameter 1/c2 of the relativistic
connection, as showed in [6]. At the order zero, we ﬁnd the galilean connection
that restitues the newtonian gravitation.

Galilean Thermodynamics of Continua
111
(H4) There exists on U a smooth ﬁeld X →ˆ
−→
W (X) ∈TX ˆU called the tem-
perature 5-vector. The covariant derivative of its projection −→
W = Tπ
ˆ
−→
W onto
TX U
f = ∇−→
W .
is called the friction tensor.
Comments. The temperature 4-vector is the geometrized version of the scalar
reciprocal temperature 1/T proposed by Planck in the form of a 4-ﬂux. The tem-
perature 5-vector is the natural extention to the 5D manifold with an interesting
interpretation of the ﬁfth component as the value of a thermodynamical poten-
tial as discussed in Sect. 6. The friction tensor, introduced by Souriau, mixing
the components of the temperature gradient occurring in Fourier’s heat conduc-
tion law and the strain rate occurring in Newton’s viscous ﬂow law, is a natural
candidate to provide thermodynamic forces in the dissipative phenomenologic
laws, as developed in Sect. 7.
(H5) There exists a smooth section X →ˆT(X) ∈Hom (T ˆ
f(X ) ˆU, TX U) called
the momentum tensor. It is covariant divergence free:
Div ˆT = 0
(10)
Comments. The momentum tensor is a natural extension of the energy-
momentum tensor of the general relativity. Representing it in local charts by
a matrix, a new column corresponding to the ﬁfth coordinate allows to recover
the balance of mass. This is rendered necessary by the independence between
mass and energy in classical mechanics, as discussed before.
Now we can turn to the presentation of more technical details. It is worth
noting that Bargmann’s group preserves the metrics δijdXidXj −2 dX0 dX4,
that allows to equip the universe ˆU with a Riemannian structure and Levi-Civita
connection, called Bargmannian connection and given by its 1-form matrix:
ˆΓ(d ˆ
X) =
⎡
⎢⎢⎣
0
0
0
j(Ω) dx −g dt
j(Ω) dt
0
∂φ
∂t −A · g

dt + (grad φ −Ω × A) · dx [(grad φ −Ω × A) dt −gradsA dx]T 0
⎤
⎥⎥⎦
where the operator j is deﬁned by (4) and occurs the gravitation potentials
φ ∈R and A ∈R3, the gravity g ∈R3 and the spinning Ω ∈R3:
g = −grad φ −∂A
∂t ,
Ω = 1
2 curl A
this last component generating Coriolis’ force. Indeed, by restriction to the four
coordinates of the space-time, we obtains the galilean connection matrix:
Γ(dX) =

0
0
j(Ω) dx −g dt j(Ω) dt


112
G. de Saxc´e
Following ´E. Cartan’s prescription (6), the motion of a free falling particle is
given by:
˙m = 0,
˙p = m (g −2 Ω × v)
the ﬁrst equation giving the conservation of the mass and the second one the
balance of linear momentum where occurs Coriolis’s force. This covariant form of
the equation of motion compatible with Galileo’s principle of relativity was pro-
posed in [2] by ´E. Cartan in the particular case without spinning and generalized
by J.M. Souriau in [12].
4
Temperature Vector and Friction Tensor
As there are Euclidean and Galilean tensors, we can consider Bargmannian
tensors by restricting the general action of the linear group GL (5) to the sub-
group of linear Bargmannian transformations. To begin with, let us study the
Bargmannian vectors ˆ
−→
W represented by a 5-column:
ˆW =
W
ζ

=
⎡
⎣
β
w
ζ
⎤
⎦,
(11)
where W ∈R4, w ∈R3 and β, ζ ∈R. Let us consider now a Bargmannian
coordinate system ˆX′ in which the vector ˆ
−→
W has a reduced form:
ˆW ′ =
⎡
⎣
β
0
ζint
⎤
⎦.
We claim the considered elementary volume is at rest. Let X be another Galilean
coordinate system obtained from X′ through a Galilean boost v. According to
(9) with a Galilean boost v, we obtain:
ˆW = ˆP ˆW ′ =
⎡
⎣
β
β v
ζint + β
2 ∥v ∥2
⎤
⎦.
then w = β v. When β = 1 / θ = 1 / kBT is the reciprocal temperature, where
kB is Boltzmann’s constant and T the absolute temperature, ˆ
−→
W is called the
temperature vector. Let us observe also that:
−→
W = β −→
U
The temperature 4-vector −→
W is represented by a column decomposed by block
as:
W =

β
w

.
(12)

Galilean Thermodynamics of Continua
113
Deﬁnition The friction tensor is the 1-covariant and 1-contravariant mixed
tensor:
f = ∇−→
W
5
Momentum Tensors and First Principle
Deﬁnition A momentum tensor is a linear map from the tangent space to
ˆ
M at ˆX = ˆf (X) into the tangent space to M at X, hence a mixed tensor
ˆT of rank 2, then a bundle map over the space-time ˆT : ˆf ∗(T ˆ
M) →TM. It is
represented in a Bargmannian coordinate system by a 4 × 5 matrix structured
as follows:
ˆT =

H −pT ρ
k
σ∗p

,
(13)
where H ∈R, p, k ∈R3 and σ∗∈Msymm
33
.
Let us consider a Bargmannian coordinate system in which the tensor ﬁeld ˆT at
a given point of coordinates ˆX′ has a reduced form:
ˆT ′ =
ρeint 0 ρ
h′
σ′ 0

,
for an elementary volume around the point x′ at rest at time t′. Let ˆX be another
Bargmannian coordinate system obtained from ˆX′ through a Galilean boost v
combined with a rotation R. Applying the transformation law:
ˆT = P ˆT ′ ˆP −1 ,
(14)
with (2) and (9), we obtain:
p = ρv,
σ⋆= σ −ρvvT ,
H = ρ
1
2 ∥v ∥2 +eint
	
,
(15)
k = h + Hv −σv ,
(16)
provided:
h = R h′,
σ = R σ′RT .
(17)
The previous method turns out the physical meaning of the components:
• the Hamiltonian density H,
• the mass density ρ, the statical stresses σ and the dynamic stresses σ⋆,
• the energy ﬂux k composed of h, further identiﬁed to the heat ﬂux, the
hamiltonian ﬂux Hv and the stress ﬂux σv.

114
G. de Saxc´e
We could name ˆT the stress-mass-energy-momentum tensor but for briefness we
call it momentum tensor. Finally, it has the form:
ˆT =

H
−pT
ρ
h + Hv −σv σ −vpT ρv

.
(18)
In the last column of (18), we spot the 4-ﬂux of mass:
N = ρ U .
(19)
Therefore, we can write:
ˆT =

T N

,
(20)
where T represents the classical energy-momentum tensor of the Relativity.
The ﬁrst principle of thermodynamics claims that the total energy of a system
is conserved. We are now able to proposed an enhanced local version including
the balance of mass and the equation of the motion (balance of linear momen-
tum). It is based on the following result.
Principle The momentum tensor of a continuum is covariant divergence free:
Div ˆT = 0 .
(21)
then, we have:
♦balance of mass: ∂ρ
∂t + div (ρ v) = 0 ,
♥balance of linear momentum:
ρ
∂v
∂t + ∂v
∂x v

= (div σ)T + ρ (g −2 Ω × v) ,
♠balance of energy: ∂H
∂t + div (h + Hv −σv) = ρ
∂φ
∂t −∂A
∂t · v
	
.
⊓⊔
In the left hand member of ♥, we recognize the convective derivative of the
velocity and, in the right hand member, the stress divergence, the gravity force
and Coriolis’ force. In the literature, there are many expressions of the balance
of energy. The present formulation is inusual but it has the advantage to be
covariant.
6
Reversible Processes and Thermodynamical Potentials
To modelize the reversible processes, we have to add a new hypothesis to our
axiomatic theory. The pioneering works [10,11,16] give rise to assume that

Galilean Thermodynamics of Continua
115
(H6) there exists a smooth numerical function X →ζ(π0(X), TX π0, W(X)) ∈
R giving the value of the ﬁfth bargmannian coordinate ˆX4 and called
Planck’s thermodynamic potential (or Massieu’s potential). It sat-
isﬁes the principle of material indiﬀerence which claims that the behaviour
of a material must be independent of the observer.
Considering a coordinate system, it is represented by a smooth mapping
X →ζ(s′, ∂s′/∂X, W) ∈R. To satisfy the principle of material indiﬀerence, ζ
is assumed to depend on ∂s′/∂X through its invariants by any Galilean trans-
formations dX′ = P dX. A straightforward calculation shows that Planck’s
potential ζ depends on ∂s′/∂X through right Cauchy strains C = F T F where
F = ∂x/∂s′.
On this ground, we prove the following proposition.
Theorem If ζ is a smooth function of s′, C and W, then:
TR = U ΠR +

0
0
−σRv σR

,
(22)
with:
ΠR = −ρ ∂ζ
∂W ,
(23)
σR = −2ρ
β F ∂ζ
∂C F T ,
(24)
is such that:
♦Tr

ˆTR∇ˆW

= 0 ,
♥TR U = −ρ
 ∂ζ
∂W U
	
U ,
♠ˆTR =

TR N 
represents a momentum tensor ˆT R ,
♣ˆTR ˆW =

ζ −∂ζ
∂W W
	
N .
⊓⊔
Before going further, let us make some comments:
• In ♦, the power of the momentum and the friction, represented by the Trace
of the product, is null (hence the name of friction tensor because if the
behaviour of the medium is reversible, there is no friction).
• The relation ♥means that the 4-velocity U is an eigenvector of TR.
• In ♠, adding to TR the 4-ﬂux of mass N, we obtain a matrix representing a
momentum tensor. We have just to verify that it has the expected covariant
form (18).

116
G. de Saxc´e
• The ♣property has important consequences that we are going now to detail.
Traditionally, the entry point to Thermodynamics is the introduction of
potentials. Here, they are by-products.
Planck’s potential ζ is a prototype of scalar functions called thermodynam-
ical potentials and derived as follows:
• Owing to (15) and w = βv, it holds:
eint = −∂ζint
∂β
.
(25)
This potential, called internal energy (by unit volume), is function of s′,
C and W as derivative of ζint.
• Taking into account ♣, the 4-vector −→
S = ˆTR ˆ
−→
W is a 4-ﬂux that reads for
convenience:
−→
S = s −→
N = ρ s −→
U
and one has:
s = ζint −β ∂ζint
∂β
(26)
This quantity is called speciﬁc entropy and −→
S is its 4-ﬂux. Hence, −s
appears as Legendre’s transform of ζint with respect to β. The latter equation
and (25) are called state equations of the continuum.
• The Helmholtz free energy (by unit volume) can be deﬁned as:
ψ = −1
β ζint = −θ ζint .
(27)
Indeed, by simple calculations, we obtain:
−eint = θ ∂ψ
∂θ −ψ,
−s = ∂ψ
∂θ .
(28)
Hence, −eint appears as Legendre’s transform of the free energy ψ (s′, C, θ)
with respect to θ.
7
Dissipative Continuum and Second Principle
In Sect. 5, we showed that the thermodynamical behaviour of a continuum is
modeled by the momentum tensor ˆT. By the theorem of the previous section we
prove that ˆTR is a momentum tensor. We deﬁne ˆTI = ˆT −ˆTR, that amounts
to introduce an additive decomposition of the momentum tensor:
ˆT = ˆTR + ˆTI ,
(29)

Galilean Thermodynamics of Continua
117
into a reversible part ˆTR deﬁned by the previous theorem and an irreversible part
ˆTI of which we will examine now the detailed representations. The momentum
tensor ˆTR given by (22) is represented by:
ˆTR =

HR
−pT
ρ
HRv −σRv σR −vpT ρv

.
Substracting the previous matrix to (18) leads to:
ˆTI =

HI
0 0
h + HIv −σIv σI 0

,
(30)
where:
• the hamiltonian density HI = H −HR = −ρqI where qI are the speciﬁc
irreversible heat source
• The column h is the heat ﬂux,
• The symmetric 3 × 3 matrix σI = σ −σR represents dissipative stresses (for
instance due to viscous eﬀects).
Introducing the symmetric gradient of the velocity (or strain rate) D and the
heat capacity at constant volume:
cv = θ ∂s
∂θ = −θ ∂2ψ
∂θ2 ,
and using the balance of energy of Sect. 5 leads to the equation of heat trans-
fer:
ρ cv
dθ
dt = θ Tr
∂σR
∂θ D
	
+ Tr (σI D) + g · v −div h + ρ dqI
dt .
(31)
The physical interpretation of this equation is that the variation of reversible
thermal energy, at the left hand member, is equal, at the right hand member, to
the contributions of each term to the dissipation due to:
• the reversible stress variation resulting from the temperature one,
• the dissipative stresses,
• the gravity,
• the heat ﬂux,
• the irreversible heat sources.
We are now able to state the geometrized form of the second principle of
thermodynamics:
Principle The local production of entropy of a continuous medium char-
acterized by ﬁelds of velocity vector −→
U , temperature vector ˆ
−→
W and momentum
tensor ˆT is non negative:
Φ = Div

ˆT
ˆ
−→
W
	
−

τ(f(−→
U ))
 
τ(T I(−→
U ))

≥0 ,
(32)
and vanishes if and only if the process is reversible.

118
G. de Saxc´e
In this expression, −→
U is the 4-velocity and τ is the linear form dt, called
clock form and represented in any Galilean coordinate system by the 4-row:
τ =

1 0 0 0

.
(33)
It can be veriﬁed that this expression is a Galilean invariant, according to the
transformation law of the linear forms τ ′ = τP and (2). In terms of tensor ﬁelds,
expression (32) is covariant, then consistent with Galileo’s principle of relativity.
It can be veriﬁed that, using the balance of mass, the local production of
entropy reads:
Φ = ρ ds
dt −ρ
θ
dqI
dt + div
h
θ
	
≥0 .
(34)
This relation is known in the literature as Clausius-Duhem inequality but it
seems ﬁrst appearing in Truesdell’s works ([14,15]).
Now, we establish a new expression of the production of entropy:
Theorem If the momentum tensor ˆT is divergence free, the local production of
entropy (32) is given by:
Φ = h · grad β + β Tr (σI D) ≥0 .
(35)
The interest of this results is turning out a duality between thermodynamic
forces (or aﬃnities):
a = grad β,
A = β gradsv = β D
and the corresponding thermodynamic ﬂuxes h and σI. In the most simple
situations, a dissipative constitutive law is given by a map (h, σI) = g (a, A).
For instance, under assumptions of linearity ans isotropy, we can recover
Fourier’s law or law of heat conduction:
h = k1 grad β = −k grad θ
where k = k1/θ2 ≥0 is the thermal conductivity, and Newton’s viscous
ﬂow law:
σI = η Tr(D) IR3 + 2μ D ,
where η = k2/θ and μ = k3/2 θ ≥0 is the dynamic viscosity. For simple ﬂuids,
the law can be simpliﬁed by assuming that the viscous stresses σI are traceless
(Stokes hypothesis), that leads to Navier-Stokes equations.

Galilean Thermodynamics of Continua
119
References
1. Bargmann, V.: On unitary representation of continuous groups. Ann. Math. 59,
1–46 (1954)
2. Cartan, ´E.: Sur les vari´et´es `a connexion aﬃne et la th´eorie de la relativit´e
g´en´eralis´ee (premi`ere partie). Annales de l’´Ecole Normale Sup´erieure 40, 325–412
(1923)
3. de Saxc´e, G., Vall´ee, C.: Bargmann group, momentum tensor and Galilean invari-
ance of Clausius-Duhem inequality. Int. J. Eng. Sci. 50, 216–232 (2012)
4. de Saxc´e, G., Vall´ee, C.: Galilean Mechanics and Thermodynamics of Continua.
Wiley-ISTE, Hoboken (2016)
5. de Saxc´e, G., Vall´ee, C.: ´El´ements de M´ecanique galil´eenne, une approche
g´eom´etrique. Collection M´ecanique th´eorique, C´epadu`es ´editions (2019)
6. de Saxc´e, G.: Asymptotic expansion of general relativity with Galilean covariance.
Gen. Relativ. Gravitat. 52(9), 89 (2020)
7. K¨unzle, H.P.: Galilei and Lorentz structures on space-time: comparison of the
corresponding geometry and physics. Ann. Inst. H. Poincar´e Sect. A 17, 337–362
(1972)
8. Noll, W.: Lectures on the foundations of continuum mechanics and thermodynam-
ics. Arch. Ration. Mech. Anal. 52, 62–92 (1973)
9. Souriau, J.M.: G´eom´etrie et relativit´e. Coll. Enseignement des Sciences. Hermann,
Paris (1964)
10. Souriau, J.M.: Thermodynamique et g´eom´etrie. In: Lecture Notes in Mathematics,
vol. 676, pp. 369–397. Springer, Berlin (1976/1977)
11. Souriau, J.M.: Thermodynamique relativiste des ﬂuides. Rend. Sem. Mat. Univers.
Politec. Torino 35, 21–34 (1978)
12. Souriau, J.M.: Structure of Dynamical Systems. A Symplectic View of Physics.
Birkh¨auser Verlag, New York (1997)
13. Toupin, R.: World invariant kinematics. Arch. Ration. Mech. Anal. 1, 181–211
(1957/1958)
14. Truesdell, C.: The mechanical foundation of elasticity and ﬂuid dynamics. J.
Ration. Mech. Anal. 1, 125–171 (1952)
15. Truesdell, C., Toupin, R.: The classical ﬁeld theories. In: Fl¨ugge, S. (ed.) Ency-
clopedia of Physics, vol II/1, Principles of Classical Mechanics and Field Theory.
Springer, Berlin (1960)
16. Vall´ee, C.: Relativistic thermodynamics of continua. Int. J. Eng. Sci. 19, 589–601
(1981)

Nonparametric Estimations
and the Diﬀeological Fisher Metric
Hˆong Vˆan Lˆe1(B) and Alexey A. Tuzhilin2
1 Institute of Mathematics of the Czech Academy of Sciences,
Zitna 25, 11567 Praha 1, Czech Republic
hvle@math.cas.cz
2 Faculty of Mechanics and Mathematics, Moscow State University Lomonosov,
119991 Moscow, Russia
tuz@mech.math.msu.su
Abstract. In this paper, ﬁrst, we survey the concept of diﬀeological
Fisher metric and its naturality, using functorial language of proba-
bilistic morphisms, and slightly extending Lˆe’s theory in [Le2020] to
include weakly Ck-diﬀeological statistical models. Then we introduce
the resulting notions of the diﬀeological Fisher distance, the diﬀeolog-
ical Hausdorﬀ–Jeﬀrey measure and explain their role in classical and
Bayesian nonparametric estimation problems in statistics.
1
Introduction
In the present paper we survey the concept of the diﬀeological Fisher metric,
introduced in [Le2020], and explain its role in frequentist and Bayesian non-
parametric density estimations. Diﬀeological Fisher metric is a natural exten-
sion of the Fisher metric to singular statistical models, which are ubiquitous
in machine learning [Watanabe2009,Amari2016]. Among diﬀerent approaches to
singular spaces, we ﬁnd the Souriau theory of diﬀeological spaces [Souriau1980]
best suitable for our study of statistical models, parameterized statistical mod-
els and dynamics on them. The role of the diﬀeological Fisher metric in fre-
quentist nonparametric estimation is expressed via the Cram´er–Rao inequality
(Theorem 2, Remark 6). The role of the diﬀeological Fisher metric in Bayesian
estimations is expressed via the choice of the objective a prior Hausdorﬀ–Jeﬀrey
measure on 2-integrable diﬀeological statistical models (Deﬁnition 9, Theorem
3). The Hausdorﬀ-Jeﬀrey measure is a natural generalization of the Jeﬀrey mea-
sure, using the concept of the diﬀeological Fisher distance that is introduced
in the present paper, and combining with the concept of the Hausdorﬀmea-
sure in geometric measure theory. Geometric measure theory could be described
as diﬀerential geometry, generalized through measure theory to deal with sin-
gular mappings and singular spaces and applied to the calculus of variations
[Federer1969,AT2004,Morgan2009]. Hausdorﬀmeasures play an important role
in several areas of mathematics, e.g., in the theory of fractals, in the theory
of stochastic processes. We also refer the reader to [JLT2021] for a categorical
c
⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2021
F. Barbaresco and F. Nielsen (Eds.): SPIGL 2020, PROMS 361, pp. 120–138, 2021.
https://doi.org/10.1007/978-3-030-77957-3_7

Nonparametric Estimations and the Diﬀeological Fisher Metric
121
treatment of the Dirichlet (a prior) measure on the set P(X) of all probability
measures on a measurable space X whose σ-algebra will be denoted by ΣX.
The remaining part of our paper is organized as follows. In Sect. 2 we recall
the concept of a Ck-diﬀeological space, adapted from [IZ2013], and the resulting
concepts of a Ck-diﬀeological statistical model and a weakly Ck-diﬀeological sta-
tistical model (Deﬁnitions 1, 3, Example 2, Lemma 2, Remarks 1, 2), the notion
of the diﬀeological Fisher metric, slightly extending the concepts introduced by
Lˆe in [Le2020]. Then we introduce the notion of the diﬀeological Fisher distance
(Deﬁnition 5, Theorem 1, Remark 3). In the last part of Sect. 2 we recall the
concept of probabilistic morphisms and the monotonicity (resp. the invariance)
of the diﬀeological Fisher metric under probability morphisms (resp. suﬃcient
probabilistic morphisms). Then we deduce similar functorial properties for the
diﬀeological Fisher distance. In Sect. 3 we recall the concept of a nonparamet-
ric ϕ-estimator introduced in [Le2020] and the related diﬀeological Cram´er–Rao
inequality, proved in [Le2020], see also Remark 6, where we discuss the validity
of the diﬀeological Cr´amer–Rao for weakly Ck-diﬀeological statistical models. In
Sect. 4 we introduce the resulting notion of the Hausdorﬀ–Jeﬀrey measure (Def-
inition 9). Then we derive their monotonicity and invariance property from the
corresponding properties of the Fisher distance (Theorem 3). In the last section
we discuss some open questions and future directions.
2
Diﬀeological Fisher Metric, Diﬀeological Fisher
Distance and Probabilistic Morphisms
First let us recall the notion of a Ck-diﬀeological space.
Deﬁnition 1 [Le2020, Deﬁnition 3], cf. [IZ2013, §1.5]. For k ∈N ∪∞and a
nonempty set X, a Ck-diﬀeology of X is a set D of mappings p : U →X, where
U is an open domain in Rn, and n runs over nonnegative integers, such that the
three following axioms are satisﬁed.
D1. Covering. The set D contains the constant mappings x : r →x, deﬁned
on Rn, for all x ∈X and for all n ∈N.
D2. Locality. Let p : U →X be a mapping. If for every point r ∈U there
exists an open neighborhood V of r such that p|V belongs to D then the map p
belongs to D.
D3. Smooth compatibility. For every element p : U →X of D, for every real
domain V, for every ψ ∈Ck(V, U), p ◦ψ belongs to D.
A Ck-diﬀeological space X is a nonempty set X equipped with a Ck-diﬀeology
D. Elements p : U →X of D will be called Ck-maps from U to X.
A map f : (X, D) →(X′, D′) between two Ck-diﬀeological spaces is called a
Ck-map, if for any p ∈D we have f ◦p ∈D′.

122
H. V. Lˆe and A. A. Tuzhilin
Digression. Recall that a map ϕ : U →V is called weakly (Fr´echet)1 diﬀerentiable
in u0 ∈U if there exists a bounded linear operator dϕu0 : Rn →V such that
[AJLS2017, p. 384]
w- lim
v→0
ϕ(u0 + v) −ϕ(u0) −dϕu0(v)
||v||
= 0,
where w- lim denotes the weak limit. In this case dϕu0 is called the weak dif-
ferential of ϕ at u0. Denote by Lin(E,V) the Banach space of bounded linear
maps from a Banach space E to a Banach space V with the induced norm. A
map ϕ : U →V is called a weak Ck-map, if it is weakly diﬀerentiable, and if the
inductively deﬁned maps d1 := dϕ : U →Lin(Rn,V), and
dr+1ϕ : Rn →Lin((Rn)r,V), u →d(drϕ)u ∈Lin((Rn)r−1,V)
are weakly diﬀerentiable for r = 1, · · · , k −1 and weakly continuous for r = k
[AJLS2017, p. 384]. Clearly the composition of weak Ck-maps is a weak Ck-map
and a weak Ck-map between ﬁnite dimensional smooth manifolds is a Ck-map.
We also write shorthand “w-Ck-map” for “weak Ck-map”.
The concept of a weak Ck-map is a natural extension of the concept of weak
convergence. The weak convergence of measures is one of most important tools
in applied and theoretical statistics [Bogachev2018]. It is known that the class of
weakly diﬀerentiable maps is strictly larger than the class of diﬀerentiable maps
[Kaliaj2016].
Example 1. (1) Let V be a Banach space. Then V has the canonical Ck-diﬀeology
Dk
can that consists of all Ck-mappings p : U →V, where U is an open domain
in Rn. The space V has also another Ck-diﬀeology Dk
w that consists of all
weak Ck-mappings U →V, where U is an open domain in Rn.
(2) Assume that (X′, D′) is a Ck-diﬀeological space and f : X →X′ is a map.
Then the pullback diﬀeology f ∗(D′) is the Ck-diﬀeology on X deﬁned as
follows [IZ2013, p. 14],
f ∗(D′) := {p : U →X| f ◦p ∈D′},
where U is an open subset of Rn.
(3) Let (X, D) be a Ck-diﬀeological space and f : X →X′ a map. Then the push-
forward diﬀeology f∗(D) is the diﬀeology on X′ that consists of all mappings
p : U →X′ where U ⊂Rn is an open subset and p satisﬁes the following
property [IZ2013, p. 24]. For every u ∈U, there exists an open neighborhood
O(u) ⊂U of u such that, either p|O(u) is a constant map, or there exists a
map q : O(u) →X such that p|O(u) = f ◦q.
Let us recall that the space S(X) of all ﬁnite signed measures on a measurable
space X is a Banach space, denoted by S(X)TV, with the total variation norm
1 In this paper we shall consider only (possibly weakly) Fr´echet diﬀerentiable mappings
and we shall omit “Fr´echet” in the remaining part of this paper.

Nonparametric Estimations and the Diﬀeological Fisher Metric
123
∥· ∥TV. For any statistical model PX, which is, by deﬁnition, any subset in
P(X) ⊂S(X) [McCullagh2002,Le2020], we denote by i : PX →S(X) the natural
inclusion.
Deﬁnition 2 cf. [Le2020, Deﬁnition 3]. (1) A statistical model PX endowedwith
a Ck-diﬀeology DX is called a Ck-diﬀeological statistical model or a weakly Ck-
diﬀeological statistical model, respectively, if i∗(DX) ⊂Dk
can or i∗(DX) ⊂Dk
w,
respectively. A Ck-diﬀeology on a weakly Ck-diﬀeological statistical model
will be called a weak Ck-diﬀeology.
(2) Let DX be a Ck-diﬀeology on a statistical model PX. For l ∈N ∪∞we
shall call an element p : U →PX in DX of class Ck+l or of class w-Ck+1,
respectively, if i ◦p ∈Dk+l
can or i ◦p ∈Dk+l
w , respectively. In other words, there
is a ﬁltration of diﬀeologies (i∗(D∞
can) ∩DX) ⊂· · · ⊂(i∗(Dk
can) ∩DX) = DX or
(i∗(D∞
w ) ∩DX) ⊂· · · ⊂(i∗(Dk
w) ∩DX) = DX), respectively.
Examples of Ck-diﬀeological statistical models are the image (p(M), p∗(Dk
can))
of parameterized statistical models (M, X, p), where M is a smooth Banach man-
ifold and i ◦p : M →S(X) is a Ck-map, see [Le2020, Example 8.2]. There are
many parameterized statistical models (M, X, p) whose image p(M) are singular
statistical models [Amari2016,Watanabe2009], see also Example 3 below. We
shall provide an example of an weakly C1-diﬀeological statistical model, which
is not a C1-diﬀeological statistical model.
Example 2. Let X = [−π, π] with the Lebesgue measure dx. For t ∈[−1, 1] \ {0}
we set
ft(x) = sin( x
t )
and we let f0(x) = 0. Then for all t we have ft ∈L1(X, dx) and ft is weakly
continuous in L1(X, dx) but not strongly continuous. Next we deﬁne a function
Ft(x) for t ∈(−1, 1) and x ∈[−π, π] as follows.
Ft(x) :=
∫t
0
fs(x)ds.
Since fs(x) = −fs(−x), for all t ∈[−1, 1]
∫π
−π
Ft(x) dx = 0.
(1)
Since Ft(x) is continuous in t and in x there exists a number A > 0 such that
2π|Ft(x)| ≤A for all (t, x) ∈[−1, 1] × [−π, π].
Finally we deﬁne a map c : (−1, 1) →P(X) ⊂S(X)
c(t) :=  1
2π + Ft(x)
2A
dx.

124
H. V. Lˆe and A. A. Tuzhilin
Clearly c(t) is diﬀerentiable, but its derivative c′(t) =
1
2A ft(x)dx is only weakly
continuous, therefore the map c is a weak C1-map but not a C1-map. Hence
the image of c is a weakly C1-diﬀeological statistical model, which is not a C1-
diﬀeological statistical model.
Concerning weakly Ck-diﬀeological statistical models we have the following local
structure result.
Let us recall that a ﬁnite signed measure ν ∈S(X) is said to be dominated
by a non-negative measure μ on X, if μ(A) = 0 implies ν(A) = 0 for any A ∈ΣX.
Alternatively, ν is called absolutely continuous w.r.t. μ, see e.g. [Neveu1970,
Chapter IV].
Lemma 1 cf. [AJLS2017, Proposition 3.3, p. 150]. Assume that U ⊂Rn is an
open connected domain and ϕ : U →P(X) is a map such that i ◦ϕ : U →S(X) is
a weak C1-map. Then there exists μ0 ∈P(X) that dominates ϕ(u) for all u ∈U.
Proof. Let UQ ⊂U be the subset of all points in U with rational coordi-
nates in Rn. Then UQ is a countable set. By [AJLS2017, Lemma 3.1, p. 146],
cf. [Neveu1970, Ex.IV.1.3] there is a measure μ0 ∈P(X) that dominates ϕ(u) for
all u ∈UQ. Now let u ∈U. We shall prove that ϕ(u) ≪μ0. Assume that A ∈ΣX
is a null-set of μ0. Then for all k we have ϕ(uk)(A) = 0. Since ϕ : U →S(X)
is weakly continuous, and uQ is dense in U, it follows that ϕ(u)(A) = 0. Hence
ϕ(u) ≪μ0. This completes the proof of Lemma 2.
⊓⊔
The concept of the tangent space of a Ck-diﬀeological statistical model
(PX, DX) at a point ξ ∈PX [Le2020, Remark 2]) extends naturally to the case of
weakly Ck-diﬀeological statistical models, see Deﬁnition 3 below. We also refer
the reader to [Souriau1980, (5.1)], [IZ2013, p. 166] for a bit more abstract app-
roach. Note that any Ck-diﬀeological statistical model is a weakly Ck-diﬀeological
statistical model.
Deﬁnition 3 cf. [Le2020, Remark 2]. Let (PX, DX) be a weakly Ck-diﬀeological
statistical model. Let c : (−ε, ε) →(PX, DX) be a Ck-map. The tangent vector
∂tc(0) at c(0) is the image of the map dc0(∂t) ∈S(X), where dc0 is the weak
diﬀerential of c at 0. For ξ ∈PX, the tangent cone Cξ(PX, DX) consists of all
tangent vectors ∂tc(0) at c(0) = ξ, where c : (0, 1) →(PX, DX) be a Ck-map, and
the tangent space Tξ(PX, DX) is the linear hull of Cξ(PX, DX).
Lemma 2. Let v be a tangent vector at ξ in a weakly Ck-diﬀeological statistical
model (PX, DX). Then v is dominated by ξ.
Proof. The proof of Lemma 2 uses the same argument in the proof for the
case of tangent vectors of Ck-diﬀeological statistical models [Le2020, Remark
2], [Bogachev2010, Corollary 3.3.2, p.77], [AJLS2017, Theorem 3.1, p. 142]. Let
v = ∂tc(0), where c : (−ε, ε) →(PX, DX) is a weak Ck-map. Let A ∈ΣX such that
c(0)(A) = 0. Since the map IA : S(X) →R, : μ →μ(A), is a linear bounded map,

Nonparametric Estimations and the Diﬀeological Fisher Metric
125
the map IA ◦c :→R is a C1-map, see e.g. [AJLS2017, Proposition C.2, p. 385].
It follows that
d
dt |t=0IA ◦c(t) = IA(v) = 0
since IA ◦c(t) ≥0. Hence v ≪c(0) = ξ. This completes the proof of Lemma 2. ⊓⊔
Lemma 2 implies that for any tangent vector v at a point ξ of a weakly Ck-
diﬀeological statistical model (PX, DX), the logarithmic representation of v
log v := dv/dξ
(2)
is an element of L1(X, ξ). The set {log v| : v ∈Cξ(PX, DX)} is a subset in L1(X, ξ).
We denote it by log(Cξ(PX, DX)) and will call it the logarithmic representation
of Cξ(PX, DX). In [AJLS2017, Deﬁnition 3.6, p. 152], for a C1-map c : (0, 1) →
PX ⊂S(X) we call dc(∂t)/dc(t) the logarithmic derivative of c in the direction
∂t ∈Tt(0, 1), since in the classical case where c(t) = f (t) · μ0 is a dominated
measure family with diﬀerentiable density function f (t), then dc(∂t)/dc(t) =
(d/dt) log f (t).
Remark 1. Any bounded function H on X deﬁnes a continuous linear function
IH on the Banach space S(X)TV as follows
IH : S(X)TV →R, : μ →
∫
X
Hdμ.
Assume that a map ϕ : (0, 1) →P(X), : t →μt, is weakly diﬀerentiable. Let
μ′
t := ∂t(ϕ(t)) ∈S(X). Then we have
d
dt| t=0
∫
X
Hdμt =
∫
X
Hd(μ′
0).
(3)
The identity (3) is central for many applications, see e.g. [Pﬂug1996] and Remark
6, and therefore the concept of weakly Ck-diﬀeological statistical models is useful.
Note that measure valued weak diﬀerentiable maps from an open subset of Rn
have been ﬁrst introduced by Pﬂug [Pﬂug1988], see also [Pﬂug1996, Deﬁnition
3.25, p. 158] in the case X is a metric space with Borel σ-algebra, using (3) as
the deﬁnition (with H bounded and continuous).
Deﬁnition 4 [Le2020,
Deﬁnition
4].
A
Ck-diﬀeological
statistical
model
(PX, DX) will be called almost 2-integrable, if logCξ(PX, DX) ⊂L2(X, ξ) for
all ξ ∈PX. In this case we deﬁne the diﬀeological Fisher metric g on PX as
follows. For each v, w ∈Cξ(PX, DX) we set
gξ(v, w) := ⟨log v, log w⟩L2(X,ξ) =
∫
X
log v · log w dξ.
(4)
The Fisher metric on Cξ(PX, DX) extends naturally to a positive quadratic
form on Tξ(PX, DX), which is also called the Fisher metric.
An almost 2-integrable Ck-diﬀeological statistical model (PX, DX) will be
called 2-integrable, if for any Ck-map p : U →PX in DX, the function v →
dp(v)

g is continuous on TU.

126
H. V. Lˆe and A. A. Tuzhilin
Remark 2. (1) As in Deﬁnition 4, we shall say that a weakly Ck-diﬀeological
statistical model is almost 2-integrable, if we can deﬁne the Fisher metric on
its tangent cone as in (4). We shall say that an almost 2-integrable weakly Ck-
diﬀeological statistical model (PX, DX) is 2-integrable, if for any weak Ck-map
p : U →PX in DX, the function v →|dp(v)|g is continuous on TU.
(2) On Ck-diﬀeological spaces, in particular on (weakly) Ck-diﬀeological sta-
tistical models (PX, DX), we can deﬁne the notion of Ck-functions. If the
dimension of its tangent spaces Tξ(PX, DX) is ﬁnite for all ξ ∈PX, then
we can deﬁne the notion of a gradient of a Ck-diﬀerentiable function on
(PX, DX).
Example 3. Let us consider an example of a 2-integrable C∞-diﬀeological statisti-
cal model which is the image of a parameterized statistical model (W, R, p = p·μ0)
where
W = {(a, b) ∈R2| a ∈[0, 1], b ∈R},
μ0 is the Lebesgue measure on R, and
p(x|a, b) := (1 −a)e−x2/2 + ae−(x−b)2/2
√
2π
.
This family is a typical example of Gaussian mixture models [Watanabe2009,
Example 1.2, p. 14], which comprise also the changing time model (the Nile
River model) and the ARMA model in time series [Amari2016, §12.2.6, p. 311].
We decompose W as a disjoint union of its subsets as follows
W = W−∪W0 ∪W+
where
W−=

(a, b) ∈W | a ∈(0, 1), b < 0

,
W0 =

(a, b) ∈W | a ∈(0, 1) & b = 0 or a = 0 & b ∈R

,
W+ =

(a, b) ∈W | a ∈(0, 1), b > 0

.
The restriction of p to W−∪W+ is injective, and p(W0) = p(0, 0). We compute
∂ap(x|a, b) = −e−x2/2+e−(x−b)2/2
√
2π
,
∂bp(x|a, b) = a(x−b)e−(x−b)2/2
√
2π
.
In [Watanabe2009, p. 14], using the expression of the Fisher metric via the
Kullback-Leibler divergence, Watanabe showed that the Fisher metric on the
parameterized statistical model (W, R, p = p · μ0) exists and is continuous. It
follows that (p∗(W), p∗(Dk
can)) is a 2-integrable Ck-diﬀeological statistical model
for any k ∈N.
Lemma 3. The
statistical
model
p∗(W)
has
two
diﬀerent C1-diﬀeologies
p∗(D1
can) and i∗(D1
can).

Nonparametric Estimations and the Diﬀeological Fisher Metric
127
Proof. Since ∂ap(x|0, 0) = ∂bp(x|0, 0) we have Tp(0,0)(p(W), p∗(D1
can)) = {0}. Now
we shall show that Tp(0,0)(p(W), i∗(D1
can)) contains a nonzero vector. Let us con-
sider a C1-curve c : (−1, 1) →p(W)
i→S(R) deﬁned as follows
c(t) := (1 −α(t))e−x2/2 + α(t)e−(x−β(t))2/2
√
2π
,
where α(t), β(t) are the following functions on (−1, 1):
α(t) =
∫t
0
dτ
log(τ2) for t  0 and α(0) = 0,
β(t) = t log(t2) for t  0 and β(0) = 0.
Clearly α, β are continuous functions. Moreover α is a C1-function and β is a
C1-function outside the point 0 ∈(−1, 1) and
c(t) = −α(t)e−x2/2 + α(t)e−(x−β(t))2/2 + α(t)(x −β(t)) β(t)e−(x−β(t))2/2
√
2π
Since
α(t) =
1
log(t2),
β(t) = log(t2) +
2t2
log(t2),
we have
lim
t→0 c(t) = xe−x2/2
√
2π
.
This implies that c(t) is a C1-curve in p(W), i∗(D1
can) and c(0) = 0, c(0)  0.
This completes the proof of Lemma 3.
⊓⊔
Example 4. Let X be a measurable space and λ be a σ-ﬁnite measure. In
[Friedrich1991, p. 274] Friedrich considered a family P(λ) := {μ ∈P(X)| μ ≪λ}
that is endowed with the following diﬀeology D(λ). A curve c : R →P(λ) is a
C1-curve, iﬀ
log c(t) ∈L2(X, c(t)).
Hence (P(λ), D(λ)) is an almost 2-integrable C1-diﬀeological statistical model, see
[Le2020, Example 10]. Next we shall prove that P(λ) is not a 2-integrable C1-
diﬀeological statistical model for X = (−1, 1) and λ being the Lebesgue measure
dx. It suﬃces to show a C1-curve c : (−1, 1) →P(dx) such that c(t) ∈L2(X, c(t))
for all t ∈(−1, 1) but | c(t)|g is not continuous at t = 0. We shall construct such
a curve using [AJLS2017, Example 3.4, p. 155]. First we consider a smooth
function f : [0, ∞) →R such that
f (u) > 0, f ′(u) < 0 for u ∈[0, 1), and f (u) = 0 for u ≥1.
For t ∈(−1, 1) we deﬁne p : (−1, 1) →S(X), : p(t) = p(t, x)dx, where
p(t, x) :=
⎧⎪⎪⎪⎨
⎪⎪⎪⎩
1
if x ≤0 and t ∈R
|t| f ( x
t )2 dx
if x > 0 and t  0
0
otherwise

128
H. V. Lˆe and A. A. Tuzhilin
Then for all t ∈(−1, 1) we have p(t)(X) = ∥p(t)∥TV ≥1. By op. cit.,
|p(t) −p(0)||TV = t2
∫1
0
f (u)2 du ≤A
(5)
for some ﬁnite constant A, hence ∥p(t)∥TV ≤2A for all t ∈(−1, 1). It has been
shown ibid. that p : (−1, 1) →S(X) is a C1-map. Now we set
c(t) :=
p(t)
∥p(t)∥TV
.
Then c : (−1, 1) →S(X) is a C1-curve lying on P(X). By local. cit. we have for
t  0
p(t) = χ(0,1)(x)sign(t)

f (u)2 −2u f f ′(u)

|u=x |t |−1dt
and p(0) = 0. It follows that p(t) ∈L2(X, p(t)). Furthermore we have
c(t) =
p(t)
∥p(t)∥TV + p(t)(d/dt)∥p(t)∥TV
∥p(t)∥2
TV
,
∥ct ∥2
g =
∫
X

c(t)
c(t)

2
c(t) dt =
1
∥p(t)∥TV
∫
X

p(t)
p(t) + (d/dt)∥p(t)∥TV
∥p(t)∥TV

2
p(t) dt < ∞.
Thus c(t) ∈L2(X, c(t)) for all t ∈(−1, 1) and c(0) = 1. Since limt→0 ∥p(t)∥TV = 0,
it follows
lim
t→0
d
dt ∥p(t)∥TV = 0.
Since ∥p(0)∥TV = 1, it follows that
lim
t→0 ∥ct ∥g = lim
t→0 ∥p(t)∥g
which is positive by Ay-Jost-Lˆe-Schwachh¨ofer’s result loc. cit. This proves our
claim that P(dx) is not a 2-integrable C1-diﬀeological statistical model.
We shall use the diﬀeological Fisher metric to deﬁne the Fisher distance
on 2-integrable Ck-diﬀeological statistical models (PX, DX). Recall that PX is a
topological space with the strong topology induced from the strong topology on
the Banach space S(X).
Deﬁnition 5. Let (PX, DX) be a 2-integrable Ck-diﬀeological statistical model.
(1) A map c : [a, b] →(PX, DX) will be called a Ck-curve, if there exists ε > 0
and a Ck-map: cε : (a −ε, b + ε) →(PX, DX) such that the restriction of cε
to [a, b] is c.
(2) A continuous map c : [0, 1] →(PX, DX) will be called a piece-wise Ck-curve,
if there exists a ﬁnite number of points 0 = a0 < a1 < a2 · · · < am = 1 such
that the restriction of c to [ai−1, ai] is a Ck-curve for i ∈[1, m].

Nonparametric Estimations and the Diﬀeological Fisher Metric
129
(3) Let c : [0, 1] →(PX, DX) be a Ck-curve connecting q1, q2 ∈PX such that
c(0) = q1 and c(1) = q2. We deﬁne the length of c by
L(c) =
∫1
0
| c(t)|g dt
where | · |g denotes the length deﬁned by the diﬀeological Fisher metric g.
The length of a piece-wise Ck-curve will be deﬁned as the sum of the lengths
of its Ck-smooth sub-intervals.
(4) The diﬀeological Fisher distance ρg(x, y) between two points x, y ∈PX will
be deﬁned as the inﬁmum of the length over the space of piece-wise Ck-
curves connecting x, y. In particular, if there is no Ck-path connecting x, y
then ρg(x, y) = ∞.
Theorem 1. The distance function ρg(x, y) is an extended metric, i.e., it can
be inﬁnite somewhere.
Proof. Clearly ρg(x, y) is a symmetric nonnegative function and ρg(x, y) satisﬁes
the triangle inequality. It remains to show that ρg(x, y) = 0 iﬀx = y. Since
constant maps belong to DX, it follows that ρg(x, x) = 0 for all x ∈PX. To prove
that ρg(x, y) = 0 implies x = y, it suﬃces to prove the following
Lemma 4. For any x, y ∈PX we have
ρg(x, y) ≥∥x −y∥TV .
Proof. Let γ : [a, b] →PX ⊂S(X) be a Ck-curve joining x and y. Since
d γ(t)
dγ(t) ∈L2(γ(t))
for all t, we have
∥y −x∥TV =
γ(b) −γ(b)

TV =

∫b
a
γ(t) dt

TV
≤
∫b
a
 γ(t)

TV dt ≤
∫b
a
 γ(t)

g dt.
This proves Lemma 4 for Ck-curves γ.
Next we assume that γ : [0, 1] →PX →S(X) is a piece-wise Ck-curve. Com-
bining the previous argument and the triangle inequality for the total variation
norm, we complete the proof of Lemma 4 immediately.
⊓⊔
This completes the proof of Theorem 1,
⊓⊔
Remark 3. Note that Deﬁnition 5 also works for weakly Ck-diﬀeological sta-
tistical models, but the proof of Lemma 4 does not work for weak C1-maps
γ : [a, b] →PX ⊂S(X). Since any weakly diﬀerentiable map γ : [a, b] →S(X) is
a.e. diﬀerentiable [Kaliaj2016, Theorem 3.2], we conjecture that Lemma 4 and
Theorem 1 also hold for weakly Ck-diﬀeological statistical models.

130
H. V. Lˆe and A. A. Tuzhilin
Note that our deﬁnition of the diﬀeological Fisher metric and the diﬀeolog-
ical Fisher distance is coordinate-free. In the remainder of this section we shall
show the naturality of the diﬀeological Fisher metric and the diﬀeological Fisher
distance, using the language of probabilistic morphisms.
In 1962 Lawvere proposed a categorical approach to probability theory, where
morphisms are Markov kernels, and most importantly, he supplied the space
P(X) with a natural σ-algebra Σw, making the notion of Markov kernels and
hence many constructions in probability theory and mathematical statistics func-
torial [Lawvere1962].
Let us recall the deﬁnition of Σw. Given a measurable space X, let Fs(X)
denote the linear space of simple functions on X. There is a natural homomor-
phism I : Fs(X) →S∗(X) := HomS(X), R, f →If , deﬁned by integration:
If (μ) :=
∫
X f dμ for f ∈Fs(X) and μ ∈S(X). Following Lawvere [Lawvere1962],
we deﬁne Σw to be the smallest σ-algebra on S(X) such that If is measurable for
all f ∈Fs(X). Let M(X) denote the space of all ﬁnite nonnegative measures on
X. We also denote by Σw the restriction of Σw to M(X), M∗(X) := M(X) \ {0},
and P(X).
Deﬁnition 6 [JLT2021, Deﬁnition 1]. A probabilistic morphism (or an arrow)
from a measurable space X to a measurable space Y is an measurable mapping
from X to P(Y), Σw
.
We shall denote by T : X →P(Y), Σw
 the measurable mapping deﬁn-
ing/generating a probabilistic morphism T : X  Y. Similarly, for a measurable
mapping p : X →P(Y) we shall denote by p : X  Y the generated probabilis-
tic morphism. Note that a probabilistic morphism is denoted by a curved arrow
and a measurable mapping by a straight arrow.
From now on we shall always assume that P(X) is a measurable space with
the σ-algebra Σw. Let δx ∈P(X) denote the Dirac measure concentrated at x
on X. Giry proved that the inclusion i : X →P(X), x →δx, is a measurable
mapping [Giry1982]. It follows that any measurable mapping κ : X →Y assigns
a probabilistic morphism i ◦κ : X  Y, which we shall write shorthand as
κ : X  Y. Hence the set of probabilistic mappings between X and Y contains
a subset of measurable mappings between X and Y.
Given a probabilistic mapping T : X  Y, we deﬁne a linear map S∗(T) :
S(X) →S(Y), called Markov morphism, as follows [Chentsov1972, Lemma 5.9,
p. 72]
S∗(T)(μ)(B) :=
∫
X
T(x)(B)dμ(x)
(6)
for any μ ∈S(X) and B ∈ΣY. We also denote by T∗the map S∗(T) if no confusion
can arise. It is known that T∗(P(X)) ⊂P(Y) [Le2020, Proposition 1]. Abusing
notation, given a probabilistic mapping T : X  Y and a Ck-diﬀeological statis-
tical model (PX, DX) we deﬁne a Ck-diﬀeological space (T∗(PX),T∗(DX)) as the
image of (PX, DX) under the smooth map T∗: P(X) →P(Y).
Diﬀeological (almost/2-integrable) statistical models are preserved under
probabilistic morphisms.

Nonparametric Estimations and the Diﬀeological Fisher Metric
131
Proposition 1 [Le2020, Theorem 1]. Let T : X  Y be a probabilistic mor-
phism and (PX, DX) a Ck-diﬀeological statistical model.
1. Then T∗(PX),T∗(DX) is a Ck-diﬀeological statistical model.
2. If (PX, DX) is an almost 2-integrable Ck-diﬀeological statistical model, then
T∗(PX),T∗(DX) is also an almost 2-integrable Ck-diﬀeological statistical
model.
3. If
(PX, DX)
is
a
2-integrable
Ck-diﬀeological
statistical
model,
then
T∗(PX),T∗(DX) is also a 2-integrable Ck-diﬀeological statistical model.
Remark 4. Proposition 1 also holds for weakly Ck-diﬀeological statistical mod-
els, because the transformation T∗: P(X) →P(X), where T : X  Y is a
probabilistic morphism, is the restriction of the linear bounded map T∗= S∗(T)
from S(X) to itself.
Furthermore, the diﬀeological Fisher metric (and hence the diﬀeological
Fisher distance) is decreasing under probabilistic morphisms and invariant under
suﬃcient probabilistic morphisms. Denote by L(X) the space of bounded mea-
surable functions on X. Recall that a probabilistic morphism T : X  Y is called
suﬃcient for PX if there exists a probabilistic morphism p : Y  X such that
for all μ ∈PX and h ∈L(X) we have ([JLT2021, Deﬁnition 2.22], cf. [MS1966])
T∗(hμ) = p∗(h)T∗(μ), i.e., p∗(h) = dT∗(hμ)
dT∗(μ) ∈L1(Y,T∗(μ)).
(7)
Examples of probabilistic morphisms T : X  Y that are suﬃcient for a
statistical model PX ⊂X are 1-1 measurable mappings [Le2020, Example 20],
and measurable mappings κ : X →Y that are “regular” and satisfying the
Fisher-Neymann condition, see [JLT2021, Example 4], [Le2020, Example 19] for
more details.
Proposition 2 [Le2020, Theorem 2]. Let T : X  Y be a probabilistic mor-
phism and (PX, DX) an almost 2-integrable Ck-diﬀeological statistical model.
Then for any μ ∈PX and any v ∈Tμ(PX, DX) we have the following mono-
tonicity
gμ(v, v) ≥gT∗μ(T∗v,T∗v)
with the equality if T is suﬃcient for PX.
Remark 5. Proposition
2
also
holds
for
almost
2-integrable
weakly Ck-
diﬀeological statistical models, since the monotonicity assertion follows from the
fact that, given a probabilistic morphism T : X  Y, the norm of the associated
linear bounded map T∗: S(X) →S(Y) in Remark 4 is less than or equal to
1. From the monotonicity assertion we obtain the second assertion concerning
suﬃcient probabilistic morphisms, since if T : X  Y is suﬃcient w.r.t. PX then
by [JLT2021, Theorem 2.8.2] there exists a probabilistic morphism p : Y →X
such that p∗(T∗(PX)) = PX and therefore p∗(T∗(DX)) = DX.
The monotonicity (and the invariance under suﬃcient probabilistic mor-
phisms) of the diﬀeological Fisher metric suggests that the diﬀeological Fisher
metric can be regarded as information metric on almost 2-integrable (weakly)
Ck-diﬀeological statistical models cf. [AJLS2015,AJLS2017,AJLS2018,Le2017].

132
H. V. Lˆe and A. A. Tuzhilin
3
Diﬀeological Cram´er–Rao Inequality
For a locally convex topological vector space V we denote by Map(PX,V) the
space of all mappings ϕ : PX →V and by V ′ the topological dual of V. Sometime
we need to estimate only a “coordinate” ϕ(ξ) of a probability measure ξ ∈PX,
which determines ξ uniquely if ϕ is an embedding.
Deﬁnition 7 [Le2020, Deﬁnition 8]. Let PX be a statistical model and ϕ ∈
Map(PX,V). A nonparametric ϕ-estimator ˆσϕ is a composition ϕ ◦ˆσ : X
ˆσ→
PX
ϕ→V.
Example 5. (1) In supervised learning with an input space X and a label space
Y we are interested in the stochastic relation between x ∈X and its label
y ∈Y, which is expressed via a measure μ ∈(P(X × Y) that governs the
distribution of labelled pair (x, y) ∈X × Y. Finding μ is a density estima-
tion problem, assuming that we are given a sequence of i.i.d. labelled pairs
{(x1, y1), · · · , (xn, yn)}. In practice, we are interested only in knowing the condi-
tional probability μY |X(·|x), which is regular under very general assumptions
[Faden1985]. Then ﬁnding the conditional probability μY |X(·|x) is equivalent
to ﬁnding a measurable mapping T : X →P(Y), or equivalently, a probabilis-
tic morphism T : X  Y. Usually Y is represented as a subset in Rn and the
knowledge of μY |X(·|x) is often not required, it is suﬃcient to determine one
of its characteristics, for example the regression function
rμ(x) =
∫
Y
ydμY |X(y|x).
In this case, the map ϕ : P(X × Y) →Map(X, R), μ →rμ, is deﬁned as the
composition of the mappings deﬁned above
P(X × Y) →Probm(X, Y) →Map(X, R),
where Probm(X, Y) denotes the space of probabilistic morphisms from X to
Y.
(2) A classical example of a ϕ-map is the moment of a probability measure in
a 1-dimensional statistical model p(Θ), where Θ is an interval or the real
line. Given a real function g(x), we deﬁne
ϕ(p(θ)) :=
∫
g(x)dp(θ).
Under a certain condition this map is 1–1 [Borovkov1998, p. 55].
Now we shall deﬁne an admissible class of ϕ-estimators, introduced in
[Le2020]. Let (PX, DX) be a Ck-diﬀeological statistical model and V a locally
convex vector space. For ϕ ∈Map(PX,V) and l ∈V ′ we denote by ϕl the com-
position l ◦ϕ. Then we set
L2
ϕ(X, PX) :=
ˆσ: X →PX | ϕl ◦ˆσ ∈L2
ξ(X) for all ξ ∈PX and l ∈V ′
.

Nonparametric Estimations and the Diﬀeological Fisher Metric
133
For ˆσ ∈L2
ϕ(X, PX) we deﬁne the ϕ-mean value of ˆσ, denoted by ϕˆσ : PX →V ′′,
as follows (cf. [AJLS2017, (5.54), p. 279])
ϕˆσ(ξ)(l) := Eξ(ϕl ◦ˆσ) for ξ ∈PX and l ∈V ′,
where Eξ denoted the mathematical expectation w.r.t. the probability measure
ξ ∈P(X). Let us identify V with a subspace in V
′′ via the canonical pairing.
The diﬀerence bϕ
ˆσ := ϕˆσ −ϕ ∈Map(PX,V
′′) will be called the bias
of the
ϕ-estimator ϕ ◦ˆσ.
For all ξ ∈PX we deﬁne a quadratic function MSEϕ
ξ [ˆσ] on V ′, which is
called the mean square error quadratic function at ξ, by setting for l, h ∈V ′ (cf.
[AJLS2017, (5.56), p. 279])
MSEϕ
ξ[ˆσ](l, h) := Eξ
ϕl ◦ˆσ −ϕl(ξ) · ϕh ◦ˆσ −ϕh(ξ)
.
(8)
Similarly we deﬁne the variance quadratic function of the ϕ-estimator ϕ ◦ˆσ at
ξ ∈PX is the quadratic form Vϕ
ξ [ˆσ] on V ′ such that for all l, h ∈V ′ we have (cf.
[AJLS2017, (5.57), p. 279])
Vϕ
ξ [ˆσ](l, h) = Eξ[(ϕl ◦ˆσ −Eξ(ϕl ◦ˆσ)) · (ϕh ◦ˆσ −Eξ(ϕh ◦ˆσ))].
Then it is known that (cf. [AJLS2017, (5.58), p. 279])
MSEϕ
ξ[ˆσ](l, h) = Vϕ
ξ [ˆσ](l, h) + bϕ
ˆσ(ξ)(l) · bϕ
ˆσ(ξ)(h).
(9)
Now we assume that (PX, DX) is an almost 2-integrable Ck-diﬀeological sta-
tistical model. For any ξ ∈PX let Tg
ξ(PX, DX) be the completion of Tξ(PX, DX)
w.r.t. the Fisher metric g. Since Tg
ξ(PX, DX) is a Hilbert space, the map
Lg : Tg
ξ(PX, DX) →(Tg
ξ(PX, DX))′, Lg(v)(w) := ⟨v, w⟩g,
is an isomorphism. Then we deﬁne the inverse g−1
ξ
of the Fisher metric gξ on
(Tg
ξ(PX, DX))′ as follows
g−1
ξ (Lgv, Lgw) := gξ(v, w)
(10)
Deﬁnition 8 [Le2020, Deﬁnition 9], cf. [AJLS2017, Deﬁnition 5.18, p. 281].
Assume that ˆσ ∈L2
ϕ(X, PX). We shall call ˆσ a ϕ-regular estimator, if for all
l ∈V ′ the function ξ →∥ϕl ◦ˆσ∥L2(X,ξ) is locally bounded, i.e., for all ξ0 ∈PX
lim
ξ→ξ0 sup ∥ϕl ◦ˆσ∥L2(X,ξ) < ∞.
For any ξ ∈PX we denote by (gϕ
ˆσ)−1
ξ
to be the following quadratic form on
V ′:
(gϕ
ˆσ)−1
ξ (l, k) := g−1
ξ (dϕl
ˆσ, dϕk
ˆσ) = gξ(gradg(ϕl
ˆσ), gradg(ϕk
ˆσ)).
(11)
If ϕ : PX →V is a local coordinate chart around a point ξ ∈PX and ˆσ is
ϕ-unbiased then (gϕ
ˆσ)−1
ξ
is the inverse of the Fisher metric at ξ, see [AJLS2017,
§5.2.3 (A), p. 286].
In [Le2020] Lˆe proved the following diﬀeological Cram´er–Rao inequality.

134
H. V. Lˆe and A. A. Tuzhilin
Theorem 2 [Le2020,
Theorem
3].
Let
(PX, DX)
be
a
2-integrable
Ck-
diﬀeological statistical model, ϕ a V-valued function on PX and ˆσ ∈L2
ϕ(X, PX) a
ϕ-regular estimator. Then the diﬀerence Vϕ
ξ [ˆσ]−(ˆgϕ
ˆσ)−1
ξ is a positive semi-deﬁnite
quadratic form on V ′ for any ξ ∈PX.
Remark 6. The proof of Theorem 2 does not extend to the case of 2-integrable
weakly Ck-diﬀeological statistical models (PX, DX). The main problem is the
validity of the diﬀerentiation under integral sign for a Ck-curve c : (0, 1) →
(PX, DX), : t →μt,
d
dt
∫
X
l ◦ϕ ◦ˆσ dμt =
∫
X
l ◦ϕ ◦ˆσ dμ′
t
(12)
where μ′
t = ∂w
t c(t). This identity is valid if i ◦c : (0, 1) →S(X) is a C1-map and if
the function ξ →∥ϕl ◦ˆσ∥L2(X,ξ) is locally bounded, see [AJLS2017, Lemma 5.2,
p. 282], whose proof involves estimations using the total variation norm. This
local boundedness condition has been stated in Deﬁnition 8 and Theorem 2.
The identity (12) has been used in the proof of [Le2020, Proposition 2], which is
an important ingredient of the proof of the diﬀeological Cram´er–Rao inequality
[Le2020, Theorem 3]. If instead of the condition that the function ξ →∥ϕl ◦
ˆσ∥L2(X,ξ) is locally bounded, we assume a stronger condition that l◦ϕ◦ˆσ : X →R
is a bounded function for all l ∈V ′, by Remark 1 the identity (12) holds. Under
this stronger assumption, the Cr´amer-Rao equality holds for 2-integrable weakly
Ck-diﬀeological statistical models (PX, DX), since other arguments used in the
proof of the diﬀeological Cr´amer-Rao inequality [Le2020, Theorem 3] also hold
for this general case. We conjecture that Theorem 2 is also valid for weakly Ck-
diﬀeological statistical models, since any weakly C1-map [0, 1] →S(X)TV is a.e.
diﬀerentiable by [Kaliaj2016, Theorem 3.2].
4
Diﬀeological Hausdorﬀ–Jeﬀrey Measure
In the previous sections we demonstrated that the diﬀeological Fisher metric is a
natural extension of the Fisher metric, and the diﬀeological Fisher metric plays
the same role in frequentist nonparametric estimation as the Fisher metric in
frequentist parametric estimation. In this section we shall introduce the concept
of the Hausdorﬀ–Jeﬀrey measure, using the diﬀeological Fisher metric and the
concept of the Hausdorﬀmeasure, which plays a fundamental role in geometric
measure theory [Federer1969].
Let us ﬁrst recall the concept of the Hausdorﬀmeasure on a metric space
(E, d), following [Federer1969,AT2004,Morgan2009]. Recall that for any subset
S ⊂E the diameter of S is
diam (S) = sup{d(x, y)| x, y ∈S}.
For k ∈N let αk denote the Lebesgue measure of the closed unit ball Bk(0, 1)
of radius 1 and centered at 0 in Rk. Let A ⊂E. For small δ cover A eﬃciently

Nonparametric Estimations and the Diﬀeological Fisher Metric
135
by countably many sets Aj with diam (Aj) ≤δ, and the k-dimensional Hausdorﬀ
measure of A is deﬁned as follows
H k(A) := lim
δ→0 αk inf

j∈I
diam (Aj)
2
k
| diam (Aj) ≤δ & A ⊂

j∈I
Ai

.
(13)
It is known that H k is a regular Borel measure [Federer1969, p. 171], see also
[AT2004, Theorem 2.1.4, p. 21]. Furthermore, H 0 is the counting measure.
The deﬁnition of the k-dimensional Hausdorﬀmeasure extends to any non-
negative real dimension k, by extending the deﬁnition of αk with the following
deﬁnition
αk :=
πk/2
Γ(1 + k/2) where Γ(t) :=
∫∞
0
xt−1e−x dx.
The Hausdorﬀdimension H- dim(A) of a nonempty set A ⊂(E, d) is deﬁned
as
H- dim(A) := inf{m ≥0| H m(A) = 0}.
It is known that if k > H- dim(A) then H k(B) = 0 and if k < H- dim(A) then
H k(A) = ∞.
The Hausdorﬀmeasure enjoys the following natural properties.
Proposition 3. (1) Let (Mm, g) be a Riemannian manifold, regarded as a metric
space with the Riemannian distance dg. Then the Hausdorﬀmeasure H m on
Mm coincides with the standard volume.
(2) Let ϕ : A ⊂(Mk, g) →(Nn, g′) be a Lipschitz map from an open domain A in
a Riemannian manifold (Mk, g) of dimension k to a Riemannian manifold
(Nn, g′) of dimension n and n ≥k. By Rademacher’s theorem dϕ and its
area factor
J dϕ :=

det(dϕ)∗◦(dϕ)
are deﬁned H k-almost everywhere on A. If k = n then we have the following
area formula
H n(ϕ(A)) =
∫
A
JdϕdH n(x).
For a proof of Proposition 3(1) see [AT2004, p. 29,30]. For a proof of Proposition
3(2) see [AT2004, p. 44, 45].
Deﬁnition 9. Let (PX, DX) be a 2-integrable Ck-diﬀeological statistical model
with the diﬀeological Fisher distance dg and m ∈R the Hausdorﬀdimension of
(PX, dg). Then the Hausdorﬀmeasure H m
g
on (PX, dg) will be called the diﬀeo-
logical Hausdorﬀ–Jeﬀrey measure of (PX, DX).
Now we shall relate the diﬀeological Hausdorﬀ–Jeﬀrey measure H m
g with the
unnormalized Jeﬀrey prior measure J m
g
deﬁned on a 2-integrable parameter-
ized statistical model (Mm, X, p), where p : Mm →S(X) is an injective C1-map
[Jeﬀrey1946]. Recall that Jg is equal to the Riemannian volume of the (possibly
degenerate) Riemannian manifold (Mk, g), whose density is zero at the points of
M where the Fisher metric g is degenerate.

136
H. V. Lˆe and A. A. Tuzhilin
Theorem 3. (1) Let (Mm, X, p) be a 2-integrable parameterized statistical model,
where Mm is a smooth manifold of dimension m and p: Mm →S(X) is an
injective C1-map. We regard p as a C1-map from Mm to the 2-integrable
C1-diﬀeological statistical model p(M), p∗(Dcan). Then
p∗(J m
g ) = H m
g .
(2) Let T : X  Y be a probability morphism and (PX, DX) a 2-integrable Ck-
diﬀeological statistical model. Then for any k ∈R and any Borel set in
(PX, dg) we have
H k
g (A) ≥H k
g
p∗(A).
(14)
The inequality in (14) turns to inequality if T is suﬃcient w.r.t. PX.
Proof. The ﬁrst assertion of Theorem 3 follows from Proposition 3. The second
assertion is a consequence of Proposition 2.
⊓⊔
According to Jordan [Jordan2011], to justify the choice of an a prior probabil-
ity measure is one of main theoretical problems in Bayesian statistics. Theorem 3
justiﬁes the choice of the Hausdorﬀ–Jeﬀrey measure as natural objective a prior
measure on 2-integrable Ck-diﬀeological statistical models.
5
Conclusion and Outlook
In this paper we showed that the concept of the diﬀeological Fisher metric is a
natural extension of the notion of the Fisher metric, originally deﬁned on param-
eterized statistical models. There are two advantages of the nonparametric diﬀe-
ological Fisher metric: (1) it can be deﬁned on singular statistical models, (2) it
turns a 2-integrable Ck-diﬀeological statistical model to a length space, on which
the Hausdorﬀmeasure is a natural generalization of the Jeﬀrey measure. We also
discussed some open questions concerning extending results from Ck-diﬀeological
statistical models to weakly Ck-diﬀeological statistical models. To make more use
of the diﬀeological Fisher metric we expect that we need to put certain assump-
tions on the singularities of underlying 2-integrable Ck-diﬀeological statistical
models. In the case a Ck-diﬀeological statistical model does not admit a diﬀeo-
logical Fisher metric, we might consider instead diﬀeological Finsler metric as in
[Amari1984]. In view of recent developments of Barbaresco’s and Gay-Balmaz’
geometric theory of Gibbs probability densities with promising applications in
machine learning [BG2020], we plan to develop a theory of diﬀeological exponen-
tial models for a diﬀeological treatment of inﬁnite dimensional families of Gibbs
probability densities.
Acknowledgement. HVL would like to thank Professor Fr´ed´eric Barbaresco and
Professor Frank Nielsen for their invitation to the Les Houches conference in July 2020
where a part of the results in this paper has been reported. She is grateful to Professor
Sun-ichi Amari who discussed with her the phenomena of degeneration and explosion
of the Fisher metric and sent her a copy of his paper [Amari1984] several years ago.
The authors would like to thank the referee for helpful comments which considerably
improve the exposition of the present paper.

Nonparametric Estimations and the Diﬀeological Fisher Metric
137
References
[AJLS2015] Ay, N., Jost, J., Lˆe, H.V., Schwachh¨ofer, L.: Information geometry
and suﬃcient statistics. Probab. Theory Relat. Fields 162, 327–364
(2015)
[AJLS2017] Ay, N., Jost, J., Lˆe, H.V., Schwachh¨ofer, L.: Information Geometry.
Springer, Cham (2017)
[AJLS2018] Ay, N., Jost, J., Lˆe, H.V., Schwachh¨ofer, L.: Parametrized measure
models. Bernoulli 24, 1692–1725 (2018)
[Amari1984] Amari, S.: The Finsler geometry of families nonregular distributions.
Translated into English by S. V. Sabau in 2002 (1984)
[Amari2016] Amari, S.: Information Geometry and Its Applications. Applied
Mathematical Sciences, vol. 194. Springer, Berlin (2016)
[AT2004] Ambrisio, L., Tilli, P.: Topics in Analysis on Metric Spaces. Oxford
University Press, Oxford (2004)
[BG2020] Barbaresco,
F.,
Gay-Balmaz,
F.:
Lie
group
cohomology
and
(multi)symplectic integrators: new geometric tools for Lie group
machine learning based on Souriau geometric statistical mechanics.
Entropy 22, 498 (2020). https://doi.org/10.3390/e22050498
[Bogachev2010] Bogachev, V.I.: Diﬀerentiable Measures and the Malliavin Calculus.
AMS, Providence (2010)
[Bogachev2018] Bogachev, V.I.: Weak convergence of measures, Mathematical Sur-
veys and Monographs, vol. 234. American Mathematical Society,
Providence (2018)
[Borovkov1998] Borovkov, A.A.: Mathematical Statistics. Gordon and Breach Science
Publishers, Amsterdam (1998)
[Faden1985] Faden, A.M.: The existence of regular conditional probabilities: nec-
essary and suﬃcient conditions. Ann. Probab. 13, 288–298 (1985)
[Federer1969] Federer, H.: Geometric Measure Theory, Die Grundlehren der math-
ematischen Wissenschaften, vol. 153. Springer, New York (1969)
[Friedrich1991] Friedrich, T.: Die Fisher-Information und symplektische Strukturen.
Math. Nachr. 153, 273–296 (1991)
[Giry1982] Giry,
M.:
A
categorical
approach
to
probability
theory.
In:
Banaschewski, B. (ed.) Categorical Aspects of Topology and Anal-
ysis. Lecture Notes in Mathematics, vol. 915, pp. 68–85. Springer,
Heidelberg (1982)
[Chentsov1972] Chentsov, N.: Statistical Decision Rules and Optimal Inference.
Nauka, Moscow, Russia (1972). English translation in: Translation
of Math. Monograph, vol. 53. American Mathematical Society, Prov-
idence, RI, USA, 1982
[IZ2013] Iglesias-Zemmour, P.: Diﬀeology. American Mathematical Society,
Providence (2013)
[Jeﬀrey1946] Jeﬀrey, H.: An invariant form for the prior probability in estimation
problems. Proc. Ro. Soc. Lond. Ser. Math. Phys. Sci. 186, 453–461
(1946)
[JLT2021] Jost, J., Lˆe, H.V., Tran, T.D.: Probabilistic morphisms and Bayesian
nonparametrics. Eur. Phys. J. Plus 136, 441 (2021). https://doi.org/
10.1140/epjp/s13360-021-01427-7
[Jordan2011] Jordan, M.I.: What are the open problems in Bayesian statistics?
ISBA Bull. 18, 1–4 (2011)

138
H. V. Lˆe and A. A. Tuzhilin
[Kaliaj2016] Kaliaj, S.B.: Diﬀerentiability and weak diﬀerentiability. Mediterr.
J. Math. 13, 2801–2811 (2016). https://doi.org/10.1007/s00009-015-
0656-6
[Lawvere1962] Lawvere,W.F.:Thecategoryofprobabilisticmappings(1962).Unpub-
lished. https://ncatlab.org/nlab/ﬁles/lawvereprobability1962.pdf
[Le2017] Lˆe, H.V.: The uniqueness of the Fisher metric as information metric.
Ann. Inst. Stat. Math. 69, 879–896 (2017). arXiv:math/1306.1465
[Le2020] Lˆe, H.V.: Diﬀeological statistical models, the Fisher metric and prob-
abilistic mappings. Mathematics 8(2), 167 (2020). https://doi.org/10.
3390/math8020167
[Morgan2009] Morgan, F.: Geometric Measure Theory A Beginner’s Guide, 4th edn.
Elsevier, Amsterdam (2009)
[McCullagh2002] McCullagh, P.: What is a statistical model. Ann. Stat. 30, 1225–1310
(2002)
[MS1966] Morse, N., Sacksteder, R.: Statistical isomorphism. Ann. Math. Stat.
37, 203–214 (1966)
[Neveu1970] Neveu, J.: Bases Math´ematiques du Calcul de Probabilit´es, deuxi`eme
edition, Masson, Paris (1970)
[Pﬂug1996] Pﬂug, G.: Optimization of Stochastic Models. Kluwer Academic,
Boston (1996)
[Pﬂug1988] Pﬂug, G.: Derivatives of probability measures - concepts and applica-
tions to the optimization of stochastic systems. In: Lecture Notes in
Control and Information Science, vol. 103, pp. 252–274. Springer, Hei-
delberg (1988)
[Souriau1980] Souriau, J.-M.: Groupes diﬀ´erentiels. In: Lecture Notes in Mathemat-
ics, vol. 836, pp. 91–128. Springer, Heidelberg (1980)
[Watanabe2009] Watanabe, S.: Algebraic Geometry and Statistical Learning Theory.
Cambridge University Press, Cambridge (2009)

Part III: Advanced Geometrical Models
of Statistical Manifolds in Information
Geometry

Information Geometry and Integrable
Hamiltonian Systems
J.-P. Fran¸coise(B)
Sorbonne Universit´e, Laboratoire Jacques–Louis Lions, UMR 7598 CNRS,
4 Place Jussieu, 75252 Paris, France
jean-pierre.francoise@sorbonne-universite.fr
Abstract. We consider the integrable Hamiltonian System of the
Peakons-Anti Peakons associated with the Camassa-Holm equation. Fol-
lowing previous contributions of Nakamura for the Toda Lattice, we dis-
cuss its link with the Geometry of Information.
1
Introduction
This talk presents in parallel the (open) Toda Lattice and the ﬁnite Peakons
system. Their scattering theory relates both with Jacobi ﬂows and relies on a
theorem of Stieltjes. We show that both of these systems linearize in the setting
of Information Geometry.
1.1
The Toda Lattice and the Flaschka Transform
Denoting the position of the mass points by xk, k = 1, ..., n, we form the Hamil-
tonian
H = 1
2Σn
k=1y2
k + Σn−1
k=1e(xk−xk+1);
(1)
thus we can write our system as
˙xk = exk−1−xk −exk−xk+1,
(2)
provided we set the formal boundary condition x0 = −∞and xn = +∞. This
system is called the ﬁnite open Toda Lattice. We follow the presentations of
Moser [12] and Beals-Sattinger-Szmigielski [4–7].
We set, with Flaschka,
ak = 1
2e(xk−xk+1)/2, bk = −1
2yk,
(3)
c
⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2021
F. Barbaresco and F. Nielsen (Eds.): SPIGL 2020, PROMS 361, pp. 141–153, 2021.
https://doi.org/10.1007/978-3-030-77957-3_8

142
J.-P. Fran¸coise
so that the diﬀerential equations get transformed into
˙ak = ak(bk+1 −bk)
˙bk = 2(a2
k −a2
k−1),
(4)
with the boundary conditions a0 = an = 0.
Flaschka noted that the above system can be expressed in matrix form
˙L = [B, L],
(5)
with the tridiagonal Jacobi matrix
L =
⎛
⎜
⎜
⎜
⎜
⎝
b1 a1 ...
0
a1 b2 a2
...
...
...
bn−1 an−1
0 ...
an−1
bn
⎞
⎟
⎟
⎟
⎟
⎠
(6)
and
B =
⎛
⎜
⎜
⎜
⎜
⎝
0
a1 ...
0
−a1 0 a2
...
...
...
0
an−1
0
...
−an−1
bn
⎞
⎟
⎟
⎟
⎟
⎠
(7)
.
1.2
The Peakons System
In 1993, Camassa-Holm used scaling and approximate Hamiltonian which is
formally integrable by the method of inverse scattering [9]. The strongly non-
linear equation they obtained:
ut −1
4uxxt + 3
2(u2)x −1
8(u2
x)x −1
4(uuxx)x = 0,
(8)
supports solutions, dubbed “peakons” that are continuous but only piecewise
analytic (cf. [4]).
Motivated by the form of traveling wave solutions of their equation, Camassa
and Holm proposed solutions of the form:
u(x, t) = 1
2Σn
j=1mj(t)exp −2 | x −xj(t) | .
(9)
The Camassa-Holm equation can be seen as the compatibility conditions:
mt + (um)x + mux = 0,
2m = 4u −uxx.
(10)
In the following, m is taken to be a discrete measure with weights mj at
location xj:
m = Σn
j=1mjδxj,
x1 < x2 <, ..., < xn.
(11)

Information Geometry and Integrable Hamiltonian Systems
143
The two equations above are readily interpreted in the sense of distributions.
Setting the coeﬃcients of δxj and Dδxj, D = d/dx equal to zero, we obtain
the Hamiltonian system:
˙xj =
∂H
∂mj = u(xj),
˙mj = −∂H
∂xj = −< ux(xj) > mj
< ux(xj) >= ux(xj−)+ux(xj+)
2
,
(12)
with
H(x, m) = 1
4Σj,kmjmke−2|xj−xk| =
 +∞
−∞
(u2 + 1
4u2
x)dx.
(13)
1.3
Information Geometry, Toda System and Peakon System
It seems that the ﬁrst time the name “Information Geometry” was used is in
–Amari, Kurata and Nagaoka; Information Geometry of Boltzmann machines
(1992)– Amari and Nagaoka discovered that smooth families of probability dis-
tributions admit dual connections as their natural structures (see the textbooks
[1,3]). Since then, Information Geometry aims to study Information Theory from
the viewpoint of dual connections. The theory of dually aﬃne structures turns
to be an analogue of Kahler geometry in a real as opposed to a complex set-
ting. This viewpoint underlines the textbook of Shima “Geometry of Hessian
Manifolds” and reveals the early inﬂuence of J.L. Koszul and J. Vey (cf. [16]).
In a series of articles, Nakamura uncovered the linearization of the Toda
Lattice in the ﬂat coordinates associated to a Hessian manifold (cf. [13]). The
Toda lattice (Dynamical system of Newtonian type) and the peakons system
(geodesic ﬂow) look apriori quite diﬀerent although this article explains the
peakons-antipeakons System also linearizes in the ﬂat coordinates of a Hessian
manifold.
2
Jacobi Flows and String Equation
In this section, we follow the lines of [4,5,7].
Let J be a Jacobi matrix:
J =
⎛
⎜
⎜
⎜
⎜
⎝
b1 a1 ...
0
a1 b2 a2
...
...
...
bn−1 an−1
0 ...
an−1
bn
⎞
⎟
⎟
⎟
⎟
⎠
(14)
so that aj > 0. The eigenvalues of J are simple, because the equation Jv = λv
allows determination of vj+1 from vj.
An isospectral ﬂow of J has the Lax form:
˙J = [J, B], Bt = −B.
(15)

144
J.-P. Fran¸coise
Denote by Φ(J) a function of J and B(J) the skew-symmetric matrix so that
B −Φ(J) is lower triangular:
B(J)jk = sgn(k −j)Φ(J)jk,
j ̸= k
B(J)jk = 0,
j = k.
(16)
This deﬁnes a Jacobi ﬂow by the Lax equation ˙J = [J, B(J)]. Note that along
a Jacobi ﬂow,
˙aj = [Φ(J)jj −Φ(J)j+1,j+1]aj,
(17)
which preserves the positivity condition aj > 0.
The Lax pair deﬁned for the Toda Lattice via the Flaschka transform is an
example of Jacobi ﬂow associated to Φ(J) = J.
We can assume (by eventually replacing J by J −λI) that the eigenvalues
of J are such that:
λ1 < λ2 < ... < λn = 0.
Let v = (v1, ..., vn)t be the unique vector so that Jv = 0, || v ||= 1, vn > 0.
This is called the normalized null vector.
The associated Weyl function is deﬁned as:
W(λ) = 1
v2n
(λI −J)−1
nn,
(18)
W(λ) = Σn
j=1
cj
λ −λj
, cj = 1
v2n
(Ej)nn,
(19)
where Ej is the spectral projection for the eigenvalue λj.
Lemma 1. The entries vj of the normalized null vector of a normalized Jacobi
matrix are positive.
Proof. Set D0 = 1, let Dk, 1 ≤k ≤n, be the k×k upper principal minor of J.
Expanding this minor along the last row and the last column, we ﬁnd
Dk = bkDk−1 −a2
k−1Dk−2,
with a0 = 0. We claim that (−1)kDk > 0.
By assumption, Dn = 0. Since J is negative semi-deﬁnite (−1)kDk ≥0.
Suppose that k −1 is the ﬁrst index for which Dk = 0. If k −1 < n then Dk−2
and Dk have opposite signs, a contradiction.
Deﬁne a vector w by:
w1 = 1, wk = (−1)k−1Dk−1
a1...ak−1
, k = 2, ...n.
It follows that
akwk+1 = −bkwk −ak−1wk−1 = 0.
These equations are equivalent to J.w = 0. Each wj is positive, thus the
normalized null vector v =|| w ||−1 w has positive entries.
⊓⊔

Information Geometry and Integrable Hamiltonian Systems
145
Theorem 1. Consider the Jacobi ﬂow, and assume that Φ(0) = 0, the residues
of the Weyl function evolve linearly:
˙cj = 2Φ(λj)cj, j = 1, ..., n.
(20)
The evolution of rj = (Ej)nn is given by:
˙rj = [Ej, B]nn = 2Φ(λj)rj −2Φ(J)nnrj.
(21)
The matrix entries rj are proportional to the cj and sum up to Inn = 1. It
follows that:
rj(t) =
rj(0)e2Φ(λj)t
Σn
k=0rk(0)e2Φ(λk)t .
(22)
This was ﬁrst proved by Moser for Φ(λ) = λ and Φ(λ) = λ2 corresponding
to the Toda and the Kac-van Moerbeke ﬂow (see [12]). Nakamura uncovered the
link with the averaged Hebbian learning equation ([13,14]).
The collection {λj, cj} or equivalently {λj, rj | Σn
j=1rj = 1} are called the
scattering data. We consider the inverse scattering problem which is to reconsti-
tute from the scattering data the collection {aj, j = 1, ..., n −1, bj, j = 1, ..., n}.
In this case, the inverse scattering problem is solved by a theorem of Stieltjes
([17]).
2.1
Stieltjes Theorem
Theorem 2. We can reconstruct from the collection {λj, cj} or equivalently
{λj, rj > 0, Σjrj = 1} the collection {aj > 0, j = 1, ..., n −1; bj, j = 1, ..., n}.
Indeed f(λ) displays a unique continued fraction expansion:
f(λ) =
1
λ −bn −
a2
n−1
λ−bn−1+
a2
n−2
···−
a2
1
λ−b1
.
(23)
Proof. To prove this, we denote Δn the characteristic polynomial of L and
Δk, k = n −1, ..., 1 the subdeterminant obtained from Δn by cancelling the last
n−k rows and columns of (λI −L). By expanding Δk by the last row, one ﬁnds:
Δk = (λ −bk)Δk−1 −a2
k−1Δk−2, k = 3, 4, ..., n,
(24)
and we can easily extends this recursive relation to hold also for k = 1, 2 by
setting Δ−1 = Δ0 = 0. Then the ratio:
sk = Δk/Δk−1,
satisﬁes:
sk = λ −bk −a2
k−1
sk−1
,
(25)

146
J.-P. Fran¸coise
and this yields the ﬁnite continued fraction for sn = Δn/Δn−1. Finally we can
check that f(λ) = sn. For this purpose, we compute
z = (λI −L)−1en,
(26)
and this displays:
zk+1 = Δk
Δn ak+1...an−1, k = 0, ..., n −2
zn = Δn−1
Δn .
(27)
2.2
Hamburger Theorems and Stieltjes Integral
This part relates with the introduction by Nakamura of a τ-function (see [18])
for the Hierarchy generated by the (free) Toda Lattice (see [13,15]).
Let α(λ) be a non-decreasing function with a discrete set of points of increase
in ] −∞, +∞[. There exists an associated set of orthogonal polynomials for the
positive measure dα(λ):
 +∞
−∞
pn(λ)pm(λ)dα(λ) = δnm,
(28)
the polynomial pn(λ) is of degree n and the coeﬃcient of λn is positive. Let us
suppose that the moments exists:
< λk >=
 +∞
−∞
λkdα(λ).
(29)
The moment problem of Hamburger is to ﬁnd a non-decreasing function α(λ)
from a given set of moments.
Next we consider the rational function:
fn(λ) = pn−1(λ)
pn(λ) .
(30)
The polynomial pn(λ) has n real zeros denoted by {λj} which are ordered:
λ1 < λ2 < ... < λn.
(31)
Each interval [λj, λj+1] contains exactly one zero of pn−1(λ) and the rational
function admits a partial fraction expansion:
fn(λ) = Σn
j=1
lj
λ −λj
,
(32)
where the residues lj > 0 are called the Christoﬀel symbols.
Thus there exists a step function αn(λ):
dαn(λ) = Σn
j=1ljδ(λ −λj)dλ,
(33)

Information Geometry and Integrable Hamiltonian Systems
147
so that:
fn(λ) =
 +∞
−∞
dαn(μ)
λ −μ .
(34)
The associated moments are
< λk >n=
 +∞
−∞
λkdαn(λ) = Σn
j=1λk
j lj.
(35)
One notes that < λk >n are the Markov parameters of fn(λ):
fn(λ) = Σ+∞
k=0 < λk >n λ−(k+1.
(36)
Markov’s theorem says that αn(λ) →α(λ) as n →+∞for an arbitrary point
λ ∈C−] −∞, +∞[.
This convergence provides the Pad´e approximation of the Stieltjes transform
of the measure dα(λ).
 +∞
−∞
dα(μ)
λ −μ .
(37)
2.3
Discrete String
By a discrete string, we mean a collection of masses m1, ..., mn−1 located at
points y1 < y2 < ... < yn−1 which we take to lie in the interval (0, +1). Thus, a
string is a discrete measure:
m = Σn−1
j=1 mjδyj, y0 = 0 < y1 < y2 < ... < yn−1 < yn = 1.
(38)
The associated string problem is to determine the non trivial solutions {u, λ}
of:
D2u = λmu, u(0) = u(1) = 0.
(39)
The function u is continuous piecewise linear whose derivatives have jumps
only at the yj.
We set y0 = 0 and yn = 1, lj = yj −yj−1, j = 1, ..., n, qj = u(yj). Note that
l1 + ... + ln = 1.
The string problem, ﬁnding the slopes pj so that
qj −qj−1 = ljpj, j = 1, ..., n
pj+1 −pj = λmjqj, j = 1, ..., n −1
q0 = qn = 0,
(40)
is equivalent to the matrix problem for the vector of slopes p = (p1, ..., pn)t:
Mp = λLp,
(41)

148
J.-P. Fran¸coise
with L = Diag(l1, ..., ln) and the Jacobi matrix M where:
Mjj = −
1
mj−1
−1
mj
, Mj,j+1 = 1
mj
, 1
m0
=
1
mn
= 0.
The problem is equivalent to the Jacobi spectral problem:
Ju = λu, λ ̸= 0,
(42)
u = L1/2p, J = L−1/2ML−1/2,
(43)
or in the usual notations for Jacobi matrices:
aj = Jj,j+1 =
1
mj
	
ljlj+1
, j = 1, ..., n −1,
bj = Jjj = −1
lj
( 1
mj
+
1
mj−1
).
(44)
Theorem 3. To any normalized Jacobi matrix J is associated a unique discrete
string problem (L, M) so that J = L−1/2ML−1/2.
Proof. The (n−1) frequencies of the string problem are negative. The remaining
eigenvalue of J is λ = 0 with eigenvector
v = L1/2(1, ..., 1)t = (
	
l1, ...,
	
ln)t.
Conversely, suppose that J is a normalized Jacobi matrix with normalized null
vector v. By previous lemma, the entries of v are positive. So we may deﬁne lj
by
	
lj = vj. Since || v ||= 1, the lj sum up to 1. The mj, j = 1, ..., n −1 are
deﬁned by aj =
1
mj√
ljlj+1 . Since v is annihilated by J,
aj−1
	
lj−1 + bj
	
lj + aj
	
lj+1 = 0,
we get bj = −1
lj ( 1
mj +
1
mj−1 ) and the data lj, mj maps into the given matrix J.
Let q0 = 0 and p1 = 1, we use
qj −qj−1 = ljpj
(45)
pj+1 −pj = λmjqj,
(46)
to determine the polynomials in λ, qj = qj(λ), pj = pj(λ) recursively.
Deﬁnition 1. The associated Weyl function of the string problem is
W(λ) = pn(λ)
λqn(λ).
Theorem 4. The Weyl function of the string problem coincides with the Weyl
function of the associated Jacobi matrix.

Information Geometry and Integrable Hamiltonian Systems
149
The construction of the pj and qj is arranged so that the ﬁrst n −1 entries
of (λL −M).p vanish. To ﬁnd the last entry:
qn −qn−1 = lnpn
(47)
pn −pn−1 = λmn−1qn−1
(48)
λpnln = λqn −( pn
qn−1
−pn−1
qn−1
).
Therefore,
(λL −M).p = λqnen, en = (0, ...0, 1)t
(49)
pn
λqn
= (λL −M)−1
nn = 1
ln
(λI −J)−1
nn,
(50)
which ends the proof as ln = v2
n.
3
Finite Information Geometry
In this section, we follow the lines of the textbook ([2]).
Let I be a ﬁnite set. Consider the algebra of real functions of I :→R, F(I).
There is a canonical basis ei, i ∈I, such that ei(j) = 1, if i = j, and ei(j) = 0,
i ̸= j. Every function f ∈F(I) can be written:
f = Σi∈If iei, fi = f(i).
Linear forms μ : F(I) →R are interpreted as signed measures on I. Note
S(I) = F(I)∗. We use the sets Sa(I), M(I), M+(I), P(I), P+(I) and the tangent
bundles:
TS(I) = S(I)×S(I), T ∗S(I) = S(I)×F(I), TM+(I) = M+(I)×S(I), T ∗M+ = M+(I)×F(I),
TP+(I) = P+(I)×S0(I), T ∗P+ = P+(I)×(F(I)/R.
Given a measure μ, the natural L2 product on F(I) is:
< f, g >μ= μ(f.g).
(51)
Given two vector ﬁelds A = (μ, a), B = (μ, b) of TμM+(I), the Fisher metric is
deﬁned by:
gμ(A, B) =< a, b >μ= μ(a.b).
(52)
The Fisher metric was introduced as a Riemannian metric by Rao. It is
relevant for estimation theory within statistics and also appear in mathematical
population genetics where it is known as the Shahshahani metric.
The Fisher metric induces a bundle isomorphism Φ : TM+(I) →T ∗M+(I);
it maps linear forms to functions and represent a simple version of the Radon-
Nicodym derivative with respect to μ:

150
J.-P. Fran¸coise
(μ, a) →Σi
ai
μi
ei.
gij(μ) = Σk
1
μk
δkiδkj +
1
μn + 1,
(53)
and its inverse
gij(μ) = μi(1 −μi), i = j
gij = −μiμj, i ̸= j
(54)
This is nothing else that the covariance matrix of the probability measure μ.
Kolomogorov gave a series of lectures at the Institut Henri Poincar´e in 1955,
where he discussed what should be natural diﬀerentiable structures for statisti-
cal distributions. Chentsov [10] (and independently Morse and Sacksteder [11])
invented the category of statistical model. Its objects are the statistical models
M such that there exists an immersion φ:
M →M+(I)
ξ →Σiφi(ξ)δi.
(55)
And the morphisms are the congruent Markov kernels. Given two non-empty
sets I and I′, a Markov kernel is a map
K : I →P(I′)
i →Ki = Σi′∈I′Ki
i′δi′.
(56)
Information geometry is the diﬀerential geometric treatment of statistical
models.
Particular examples of Markov Kernels are given in terms of maps
f : I →I′, Kf : i →δf(i).
(57)
Each Markov Kernel induces a corresponding map between probability dis-
tributions:
K∗: P(I) →P(I′)
μ = Σiμiδi →ΣiμiKi.
(58)
Assume that | I |≤| I′ |.
A Markov Kernel is said to be congruent if there is a partition Ai, i ∈I of I′
such that Ki
i′ > 0 if and only if i′ ∈Ai.
If a Markov Kernel is congruent, it implies a diﬀerential map K∗: P+(I) →
P+(I) of diﬀerential
dμ : TμP+(I) →TK∗μP+(I′)
(μ, ν −μ) →(K ∗μ, K ∗ν −K ∗μ).
(59)
Given a statistical model φ : M →M+(I), it is possible to deﬁne the Fisher
metric on M as the pull-back of the Fisher metric on M+(I). This yields:
gξ(A, B) = Σi∈I
1
pi(ξ)
∂pi
∂A (ξ)∂pi
∂B (ξ),
gξ(A, B) = Σi∈Ipi(ξ)∂logpi
∂A
(ξ)∂logpi
∂B
(ξ)

Information Geometry and Integrable Hamiltonian Systems
151
Theorem 5. (Chentsov 1972) We assign to each non-empty and ﬁnite set I a
metric hI on P+(I). If for all congruent Markov Kernel K : I →P(I′), we have
K ∗(hI) = K ∗(hI′), then there exists a constant α such that hI = (α)gI, where
gI is the Fisher metric on P+(I).
For the proof and historical aspects see ([2]).
The tangent bundle TM+(I) and T ∗M+(I) are Cartesian products and so
there are natural parallel transports:
Πm
μ,ν : TμM+(I) →TνM+(I)
(μ, a) →(ν, a)
Π∗m
μ,ν : T ∗
μM+(I) →T ∗
ν M+(I)
(μ, f) →(μ, f).
(60)
With the bundle isomorphism induced by the Fisher metric Φ : TM+(I) →
T ∗M+(I) we can construct a second parallel transport:
Πe
μ,ν : TμM+(I) →TνM+(I)
(μ, a) →Φ−1(Π∗m
μ,ν(Φ(μ, a)))
(μ, a) →(μ, Σiνi
ai
μi δi).
(61)
These parallel transports are in fact associated with aﬃne connections noted
respectively ∇m and ∇e. The corresponding (maximal) m- and e-geodesic with
initial point μ ∈M+(I) and velocity a ∈TμM+(I) are:
γ(m) :]t−, t+[→M+(I), t →μ + ta,
t−= −min{μi
ai
, i ∈I, ai > 0}, t+ = min{ μi
| ai |, i ∈I, ai < 0}
γ(e) : R →M+(I), t →exp(t da
dμ)μ.
The corresponding (maximal) m- and e-geodesic with initial point μ ∈P+(I)
and velocity a ∈TμP+(I) are:
γ(m) :]t−, t+[→M+(I), t →μ + ta,
t−= −min{μi
ai
, i ∈I, ai > 0}, t+ = min{ μi
| ai |, i ∈I, ai < 0}
γ(e) : R →M+(I), t →
exp(t da
dμ)μ
μ(exp(t da
μ )).
The e-geodesic is given by
t →Σi
μiet ai
μi
Σjμje
t
aj
μj
δi
The link with the Toda Lattice and the e-geodesic can now be seen with
Eq. (22).

152
J.-P. Fran¸coise
4
Conclusions and Perspectives
Nakamura discovered the link between the (free) Toda Lattice and the Informa-
tion Geometry as he proved that the Toda Flow is conjugated to the e-geodesic
ﬂow on a Hessian manifold of the Hankel matrices. He used his construction to
deﬁne a τ-function for the Toda Lattice ([13]). The positivity of this τ-function
was further proved using Hamburger theory of Moments ([15]). In this article,
we have checked that the Hamiltonian deﬁned by peakon-antipeakon associated
with Camassa-Holm equation is also conjugated to a e-geodesic in the context
of the Information theory of a ﬁnite set. To prove this, we rely on the article [7]
and the string density problem. Further studies should be developed in the gen-
eral setting of completely integrable gradient ﬂows introduced in [8]. The case
of periodic peakons ([6]) is also an interesting perspective.
References
1. Amari, S.I.: Information geometry and its applications. Applied Mathematical
Sciences, vol. 194. Springer, Japan (2016). https://doi.org/10.1007/978-4-431-
55978-8
2. Ay, N., Jost, J., Lˆe, H., Schwachhofer, L.: Information geometry. Ergebnisse der
Mathematik und ihrer Grenzgebiete. A Series of Modern Surveys in Mathematics,
vol. 64. Springer, Cham (2017)
3. Amari, S., Nagaoka, H.: Methods of information geometry. Translated from the
1993 Japanese original by Daishi Harada. Translations of Mathematical Mono-
graphs, vol. 191, x+206 pp. American Mathematical Society, Providence, RI;
Oxford University Press, Oxford (2000)
4. Beals, R., Sattinger, D.H., Szmigielski, J.: Multipeakons and the classical moment
problem. Adv. Math. 154(2), 229–257 (2000)
5. Beals, R., Sattinger, D.H., Szmigielski, J.: Peakons strings, and the ﬁnite Toda
lattice. Comm. Pure Appl. Math. 54(1), 91–106 (2001)
6. Beals, R., Sattinger, D.H., Szmigielski, J.: Periodic peakons and Calogero-Fran¸coise
ﬂows. J. Inst. Math. Jussieu 4(1), 1–27 (2005)
7. Beals, R., Sattinger, D.H., Szmigielski, J.: The string density problem and the
Camassa-Holm equation. Philos. Trans. R. Soc. Lond. Ser. A Math. Phys. Eng.
Sci. 365(1858), 2299–2312 (2007)
8. Bloch, A.M., Brockett, R.W., Ratiu, T.S.: Completely integrable gradient ﬂows.
Comm. Math. Phys. 147(1), 57–74 (1992)
9. Camassa, R., Holm, D.D.: An integrable shallow water equation with peaked soli-
tons. Phys. Rev. Lett. 71, 1660–1664 (1993)
10. Chentsov, N.: Category of mathematical statistics. Dokl. Akad. Nauk SSSR 164,
511–514 (1965)
11. Morse, N., Sacksteder, R.: Statistical isomorphism. Ann. Math. Stat. 37, 203–214
(1966)
12. Moser, J.: Finitely many mass points on the line under the inﬂuence of an expo-
nential potential – an integrable system. In: Moser J. (eds.) Dynamical Systems,
Theory and Applications. Lecture Notes in Physics, vol. 38. Springer, Berlin, Hei-
delberg (1975). https://doi.org/10.1007/3-540-07171-7 12

Information Geometry and Integrable Hamiltonian Systems
153
13. Nakamura, Y.: A tau-function for the ﬁnite Toda molecule, and information spaces.
Symplectic Geometry and Quantization (Sanda and Yokohama, 1993), vol. 179,
pp. 205–211. Contemporary Mathematics, American Mathematical Society, Prov-
idence, RI (1994)
14. Nakamura, Y.: Neurodynamics and nonlinear integrable systems of Lax type. Japan
J. Indust. Appl. Math. 11(1), 11–20 (1994)
15. Nakamura, Y., Kodama, Y.: Moment problem of Hamburger, hierarchies of inte-
grable systems, and the positivity of tau-functions. KdV ’95 (Amsterdam, 1995).
Acta Appl. Math. 39(1–3), 435–443 (1995)
16. Shima, H.: The Geometry of Hessian Structures. World Scientiﬁc Publishing, Sin-
gapore (2007)
17. Stieltjes, T.-J.: Sur la r´eduction en fraction continue d’une s´erie proc´edant suivant
les puissances descendantes d’une variable. (French) [Reduction to a continued
fraction of a series in descending powers of a variable] Reprint of the 1889 original.
Ann. Fac. Sci. Toulouse Math. 5(6)(1), H1–H17 (1996)
18. Sato, M., Sato, Y.: Soliton equations as dynamical systems on inﬁnite-dimensional
Grassmann manifold. Nonlinear Partial Diﬀerential Equations in Applied Science
(Tokyo, 1982), vol. 81, pp. 259–271. North-Holland Mathematics StudiesLecture
Notes in Numerical and Applied Analysis 5, North-Holland, Amsterdam (1983)

Relevant Diﬀerential Topology
in Statistical Manifolds
Michel Nguiﬀo-Boyom(B)
IMAG Alexander Grothendieck Research Institute, UMR CNRS 5149,
University of Montpellier, Montpellier, France
boyom@math.univ-montp2.fr
Abstract. The author agrees and assumes that this paper is a mix-
ture of several subjects and ongoing perspectives but he emphasizes that
their unifying framework is formed of the trio described in Prologue.
Statistical models form a subcategory of the category of statistical man-
ifolds. Intuitively, think of the notion of Foliation as (geometrical or
topological) study of (discrete, continuous or diﬀerentiable) partitions.
The Geometry of Statistical manifolds provides a uniﬁed framework for
these studies. From the conceptual point of view the statistical geometry
highlights many bridges betwen the Riemannian Geometry and the Car-
tan geometry or gauge geometry, (which may be understood nowadays
as the studies of Koszul connections). This paper is devoted to point
out some relevant invariants of the diﬀerential topology which are linked
with the structure of the statistical manifold. We aim to point out that a
structure of statistical manifold yields many interesting dynamical sys-
tems and many relevant structured foliations, i.e. foliations whose leaves
unformally carry a prescribed geometric structure.
1
Prologue
This paper is devoted to brievely explore and introductively exploit some hori-
zons illuminated by the trio composed as it follows: (α) the canonical Koszul
class of a diﬀerential action a group {H : M}; (β) the sheaf of solutions of the
Hessian equation of a symmetric gauge structure {M, ∇}; (γ) the sheaf of solu-
tions of the gauge equation associated with a statistical structure {M, g, ∇}.
Though many subsections must be considered as a preparatory work for further
investigations, we aim to develop fundamental mathematical tools which are
useful for rigorous understanding of experiment results such as those from the
survey samples. Regarding impacts on fundamental mathematics, among rele-
vant results we overview is the complete answer to the fundamental question
in quantitative diﬀerential topology raised by Etienne Ghys in Molino (1990).
How to construct all Riemannian foliations, (Ghys). We involve the van-
ishing of canonical Koszul classes to introduce the notion of gauge extension
of a gauge structure by another gauge structure. This notion of exten-
sion doen’t product new gauge structures. In a compact manifold, a remarkable
byproduct of our manipulations is that we use extensions of gauge structures to
c
⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2021
F. Barbaresco and F. Nielsen (Eds.): SPIGL 2020, PROMS 361, pp. 154–178, 2021.
https://doi.org/10.1007/978-3-030-77957-3_9

Relevant Diﬀerential Topology in Statistical Manifolds
155
product diﬀerentiable dynamical systems of ﬁnite dimensional aﬃnely ﬂat Lie
groups. In generic situations we are looking for conditions under which gauge
extensions produce gauge structures. That is the reason why we introduce the
notion of Transversal gauge extension of gauge structures. This kind of
gauge extension produces new gauge structures. It also provides a new technical
way to construct locally ﬂat structures in manifolds as in Theorem 2.11.
2
Intoduction
At the begining the study of Cartan geometry is originally linked with the search
of geometric structures admitting canonical connections. The illuminating pio-
neering example is the Levi Civitation connection of a Riemannian manifold.
Apart pioneering works of Elie Cartan (Cartan) among references for intrin-
sic interests in the theory of connections are Lichnerowicz (1962), Kobayashi
(1957). This research work is a sketch of the intrinsic study of gauge structures,
i.e. the Koszul connections in tangent vector bundle of diﬀerentiable manifolds.
We introduce some manipulations involving these connections but with conse-
quences of various nature. Interests in every manipulation are illuminated by an
example innovative result.
The paper is devised into six short sections including prologue (Sect. 1) and
this introduction (Sect. 2).
In Sect. 3 is introduced and brievely discussed the notion of gauge exten-
sion whose usefulness in diﬀerentiable dynamical systems is proven and illus-
trated as in Theorem 3.11.
In Sect. 4 we introduce the notion of gauge reductions of proper homogeneous
spaces and we highlight some relevant impacts in the diﬀerential topology of the
starting homogeneous space. Examples of impacts are Theorem 4.5, Corollary
4.6 and Theorem 4.7.
Section 5 is devoted to some impacts of our manipulations in Information
Geometry. Relevant arsenal of Information Geometry in a functional statisti-
cal model uncludes its Fisher information and its Amari-Chentsov family of
α-connections. We associate Fisher information with the Hessian diﬀerential
equation of its Levi Civita connection. We introduce the α-equivalence relation
between functional statistical models. Hessian diﬀerential equation is involved to
point out invariants of the moduli space of α-equivalence relation. These invari-
ants are objects of the diﬀerential topology of the bas manifolds of statistical
models.
Section 6 is a continuation of Sect. 5. We use the gauge equation of statistical
manifolds to relate the sheaf of Riemannian foliations to the sheaf of symplectic
foliations. This relationship is used to answer a question raised by Etienne: How
to construct all Riemannian foliations. We introduce the notion of symplec-
tic statistical manifold. We involve the gauge equation to provide a construction
of symplectic statistical foliations in a statistical mnifold.

156
M. Nguiﬀo-Boyom
3
Basic Deﬁnitions
Throughout the paper manifolds paracompact and the class of diﬀerentiability
if C∞. Without the statement of the contrary manifolds are ﬁnite dimensional.
The ﬁeld of real numbers is denoted by R and the ring of integers is denoted
by Z.
Given a manifold M, C∞(M) is the vector space diﬀerentiable functions
deﬁned in M; X(M) is the C∞(M)-module of diﬀerentiable vector ﬁelds in M
and Gau(M) is the category of gauge structures in M; objects of Gau(M) are
pair (M, ∇) where ∇is a Koszul connections in M.
3.1
The Canonical Koszul Class of a Symmetric Gauge Structure
Let (M, ∇) be a gauge structure in M. We recall that ∇is a real bilinear product
X(M) × X(M) ∋(X, Y ) →∇XY ∈X(M)
which subject to the following identity
∇fXhY = fdh(X)Y + fh∇XY, ∀(X, Y ) ⊂X(M), ∀(f, h) ⊂C∞(M).
The curvature tensor and the torsion tensor of a gauge structure (M, ∇) are
denoted by R∇and by T ∇respectively. The vector space of (2,1)-tensors in M
is denoted by T 2
1 (M), its elements are C∞(M) bi-linear mappings of X(M) in
itself.
Deﬁnition. The Hessian operator of (M, ∇)
X(M) ∋X →∇2X ∈T 2
1 (M)
is deﬁned by
(∇2X)(Y, Z) = ∇Y ∇ZX −∇∇Y ZX, ∀(Y, Z) ⊂X(M).
Deﬁnition. The ﬁrst fundamental equation FE(∇) is deﬁned by
FE(∇) : ∇2X = 0♣
The sheaf of solutions of FE(∇) is denoted by J∇(M). It is a sheaf of real
associative algebras whose product X.Y is deﬁned by ∇, i.e.
X.Y = ∇XY.
The real vector space of sections of J∇(M) is denoted by J∇.
Proposition. When ∇is torsion J∇is a ﬁnite dimensional real Lie subalgebra
of the real Lie algebra X(M).
⊓⊔

Relevant Diﬀerential Topology in Statistical Manifolds
157
The equation FE(∇) is involutive in the sense of Elie Cartan; further it
is of order 2. Thereby to specialists of the global analysis the proposition
stated above is a folklore, Guillemin (1965) Guillemin-Sternberg (1990), Singer-
Sternberg (1965). We ﬁx a simply connected Lie group G∇whose Lie algebra is
simorphic to J∇. Then J∇⊂X(M) is a localy eﬀective inﬁnitesimal action of
G∇in M.
Deﬁnition. According to Palais formalism J∇is called completely integrable if
its the linear counterpart of a locally eﬀective diﬀerentiable acion
G∇× M ∋(γ, x) →γ.x ∈M♣
3.2
The Canonical Koszul Class of a Gauge Structure
Let ∇be an arbitrary Koszul connection in M, (i.e. in the tangent bundle TM).
Let g∇⊂X(M) be the smallest real Lie subalgebra containing J∇. The vector
space T 2
1 (M) formed of (2,1)- tensors in M is a left module of g∇.
Given another Koszul connection ∇⋆we consider the Chevalley-Eilenberg
complex CCE(g∇, T 2
1 (M)) and its 1-cocycle
g∇∋X →k∇
∞(X) = LX∇⋆∈T 2
1 (M).
LX∇⋆is the Lie derivative of ∇⋆in the direction X. It is easy to check that the
cohomology class
[k∇
∞] ∈H1
CE(g∇, T 2
1 (M))
does not depend on the choice of the connection ∇⋆.
Deﬁnition. (a) The cohomology class [k∇
∞] is called canonical Koszul class of
∇.
(b) (M, ∇) is called Koszul-rigid if its canonical Koszul class does not
vanish ♣
⊓⊔
An Important Warning
For every symmetric gauge structure (M, ∇) the canonical Koszul class [k∇
∞] is
easy to compute.
Indeed let RD be the curvature tensor of a connection D. Given a subset
A ⊂X(M)
we set
IARD =

iXRD ∈T 2
1 (M), X ∈A

The (2-1)-tensor iXRD is deﬁned as it follows
X(M) × X(M) ∋(Y, Z) →iXRD(Y, Z) = RD(X, Y )Z,
∀(Y, Z) ⊂X(M).
Therefore when we view the cohomology class
[k∇
∞] ∈H1
CE(g∇, T 2
1 (M))

158
M. Nguiﬀo-Boyom
as a set of 1-cocycle pairwise cohomologue, we have
[k∇
∞] = IJ∇R∇+ LJ∇(T 2
1 (M))♣
Here
LJ∇T 2
1 (M) =

LXθ, (X, θ) ⊂J∇× T 2
1 (M)

♣
We observe that the sheaf of associative algebras J∇(M) may be trivial; such
gauge structure are named Hessian-trivial.
Examples of Hessian-trivial connections are Levi Civita connections of Ein-
stein manifolds. Indeed when ∇is the Levi Civita connection of a Rie-
mannian manifold (M, g) sections of the sheaf J∇(M) belong the kernel
of the Ricci curvature of (M, g).
Throughout this paper we confus on non Hessian-trivial gauge structures.
Their associative algebras sheaves are not trivial.
The
sheaf
of
inﬁnitesimal
transformations
of
(M, ∇)
is
denoted
by
Aff(M, ∇). Its sections are vector ﬁelds X subject to the following identity,
[X, ∇Y Z] −∇[X,Y ]Z −∇Y [X, Z] = 0, ∀(Y, Z) ⊂X(M).
Deﬁnition. A symmetric gauge structure (M, ∇⋆) is called an aﬃne represen-
tation of another symmetric gauge structure (M, ∇) if every section of J∇(M)
is a section of Aff(M, ∇⋆)♣
⊓⊔
It is easy to check the following claim.
Proposition. A symmetric gauge structure (M, ∇) admits an aﬃne represen-
tation if and only if its canonical Koszul class [k∇
∞] vanishes.
⊓⊔
Deﬁnition. A symmetric gauge structure (M, ∇) is called 1-rigid if Either
it is Hessian-trivial
Or all of its aﬃne representations are Koszul
-rigid♣
⊓⊔
3.3
Gauge Extensions of Gauge Dynamics
Loosely speaking what is called gauge dynamic of a gauge structure (M, ∇)
is the (inﬁnitesimal) action of a simply connected Lie group G∇whose Lie alge-
bra is (isomorphic to) J∇. We go to introduce a notion of extension which has
other impacts elsewhere. In this paper we emphasize its impacts on the topology-
geometry in statistical manifolds.
We consider a non Hessian-trivial symmetric gauge structure (M, ∇) and its
associative algebras sheaf J∇(M).
We assume that (M, ∇) is not 1-rigid. What we name inﬁnitesimal gauge
dynamic of (M, ∇) is the Lie subalgebra of sections of J∇(M), namely J∇.
Since (M, ∇) is not 1-rigid it admits a non Hessian-trivial aﬃne representa-
tion (M, ∇⋆). Thus the Lie algebra X(M) the Lie subalgebra J∇is contained in
the normalizer of the J∇⋆.

Relevant Diﬀerential Topology in Statistical Manifolds
159
Thus We consider the vector space
J∇⋆∇= J∇⋆⊕J∇;
it is an semi-direct product of the Lie algebra J∇by the Lie algebra J∇⋆. It
bracker is deﬁned as it follows; we present its elements as
X⋆+ X := (X⋆; X)
and we put
[(X⋆; X), (Y ⋆, Y )] = [X⋆, Y ⋆] + [X, Y ⋆] −[Y, X⋆]; [X, Y ])
This bracket yields the following splitting short exact sequence of Lie algebras
0 →J∇⋆→J∇⋆∇→J∇→0.
At another side we endow J∇⋆∇with the R-bilinear map D which is deﬁned as
it follows,
D(X⋆;X)(Y ⋆; Y ) = (∇⋆
X⋆Y ⋆+ [X, Y ⋆]; ∇XY ).
Let G∇⋆∇be a simply connected Lie group whose Lie algebra is isomorphic to
J∇⋆∇. It is easy to check that the product D yields a left invariant locally ﬂat
structure in the Lie group G∇⋆∇. In particular one has
D(X⋆;X)(Y ⋆; Y ) −D(Y ⋆;Y )(X⋆; X) = [(X⋆; X), (Y ⋆; Y )].
Furthermore our construction yields the following splitting short exact sequence
of left invariant locally ﬂat Lie groups,
1 →(G∇⋆, ∇⋆) →(G∇⋆∇, D) →(G∇, ∇) →1.
The underlying set space of the Lie group G∇⋆∇is the Cartesian set product
G∇⋆× G∇.
The action of G∇in G∇⋆is de denoted by ad; so the product of G∇⋆∇deﬁned
as it follows
(σ⋆; σ).(γ⋆; γ) = (σ⋆.adσ(γ⋆); σ.γ).
If one assumes that both J∇⋆and J∇are completely integrable, then J∇⋆∇
is completely integrable too. Since both σ and γ⋆may be regarded as diﬀeomor-
phisms of the manifold M one has
adσ(γ⋆) = σ ◦γ⋆◦σ−1
A diﬀerentiable dynamical system, (i.e. a diﬀerentiable Lie group action)
G × M →M
is denoted by <G, M>.

160
M. Nguiﬀo-Boyom
Deﬁnition. The diﬀerentiable dynamical system <G∇⋆∇, M> is called a gauge
extension of <G∇, M> by <G⋆
∇, M>♣
⊓⊔
Warning. The product D as deﬁned in J∇⋆∇can be extension to a
locally ﬂat structure in M if the dynamic <G∇⋆∇> is transitive.
The vector space T 2
1 (M) is module of the two Lie algebras J∇⋆and J∇⋆∇
We just inform (without any detail that) heir canonical Koszul class are
closely related by the Lyndon-Hochschild-Serre spectral sequence Lyndon (1948),
Hochschild-Serre (1953)
Deﬁnition. Let (M, ∇⋆) be an aﬃne representation of (non 1-rigid) (M, ∇).
The couple {∇, ∇⋆} is transverse if
SpanC∞(M)(J∇, J∇⋆) = X(M).
The the meaning of the deﬁnition above is following
(TR) : J∇⋆(x) ⊕J∇(x) = TxM, ∀x ∈M.
Thereby the diﬀerentiable dynamical system <G∇⋆∇, M> is transitive. We con-
clude by the following statement.
Theorem. If a non 1-rigid symmetric gauge structure (M, ∇) admits a trans-
verse aﬃne representation (M, ∇⋆) then M admits a homogenous locally ﬂat
structure (M, D). Further the orbits of G⋆
∇are leaves of GD-homogeneous locally
ﬂat foliation ♣
⊓⊔
Hint. Remember the bracket of J∇⋆∇⊂X(M) is the Poisson bracket of vector
ﬁelds. In addition J∇⋆∇is transitive as in equality (TR) above. Therefor we
extend in a torsion free connection which is denoted by D. According to Nguiﬀo
Boyom (2019)The locally ﬂat manifold (M, D) has the Hessian defect rb(M, D)
which is deﬁne as we go to remind. Let Rie(M) be the category positive deﬁnite
Riemannian structure in M. Let g ∈Rie(M), the Koszul Dg is deﬁned as it
follows
g(Dg
XY, Z) = Xg(Y, Z) −g(Y, DXZ).
The non negative integers r(D, g) and rb(M, D) are deﬁned as it following
rb(D, g) = min {dim(M) −dim(JDg(x)), x ∈M} ,
rb(M, D) = min

rb(D, g), g ∈Rie(M)

The non negative integer is a characteristic obstruction to the existence of Hes-
sian structure in the gauge structure (M, G). In short, rb(M, D) if and only if
there existence g⋆∈Rie(M) such that (M, g⋆, D) is a locally ﬂat statistical
manifold.
In term of KV homology, g⋆yields the cohomology class [g⋆] ∈H2
KV (D),
Nguiﬀo Boyom (preprint). When we assume that M is compact the vanishing of

Relevant Diﬀerential Topology in Statistical Manifolds
161
[g⋆] is equivalent to the hyperbolicity of (M, D). The hyperbolicity means that
the compact manifold M is diﬀeomorphic to a quotient
Ω
Γ
where Ω ⊂Rm is an open convex domain not containing any straight line
and Γ is a discrect subgroup of the aﬃne group Aﬀ(m). We keep the notation
which is used above.
Deﬁnition. When a non 1-rigid gauge structure (M, ∇) admits a transversal
aﬃne representation (M, ∇⋆) the extension (M, D) is called an locally ﬂat defor-
mation of the pair
{(M, ∇⋆), (M, ∇)} ♣
Before pursuing we highlight that the construction of (G∇⋆∇involute but the
argument that (M, ∇) is not 1-rigid.
So our construct may look abstract. It is easy to product examples of
transversal aﬃne representation. For instance consider the splitting sort exact
sequence of left invariant locally ﬂat Lie groups we have constructed, namely
1 →(G∇⋆, ∇⋆) →(G∇⋆∇, D) →(G∇, ∇) →1.
In the manifold G∇⋆∇we extend both ∇⋆and ∇as connections ˜∇⋆and ˜∇. Thus
˜∇⋆is a transversal aﬃne representation of ˜∇. Then the gauge structure
(G∇⋆∇, D)
is a locally ﬂat deformation of the couple,

(G∇⋆∇, ˜∇⋆), (G∇⋆∇, ˜∇)

This con-
struct involves buth the vanishing of the canonical Koszul class [k∞(∇].
3.4
Transverse Statistical Structures
We consider a statistical manifold (M, g, ∇) and its g-dual (M, g, ∇⋆) and we
assume that (M, ∇⋆) is a transversal aﬃne representation of (M, ∇). As seen in
the precedent subsection the couple
{(M, ∇⋆), (M, ∇)}
admits a homogeneous localemet ﬂat deformation (M, D) with a Hessian
defect rb(M, D). The non negative rb(M, D) vanishes if and only if the cou-
ple {(M, ∇), (M, ∇⋆)} admit a Hessian deformation (M, g⋆, D).
A technical challenge is the applications of these manipulatons to
statistical models and to their family of α −connections.
Another Look. We consider a functional statistical model of a measurable set
(Ξ, Ω), namely
M = {E, π, M, P}

162
M. Nguiﬀo-Boyom
whose Fisher information is g. The family of α-connections, namely {∇α, α ∈R}
is viewed as one parameter deformation of the Levi Civita connection of
(M, g).
Then another challenge is the question of whether our manipulations could
lead to a locally ﬂat deformation M, D) of (M, ∇LC) such that (M, g, D) is a
ﬂat statistical manifold, i.e. (M, g, D) is a Hessian manifold.
If a locally ﬂat deformation (M, D) exits then {E, π, M, P} is an exponential
model.
In fact a characteristic obstruction to (M, g) being a Hessian manifold is
known Nguiﬀo Boyom (2019). It is denoted by rb(M, g). It is deﬁned as it follows
rb(M, g) = min

rb(∇g), ∇∈Kos(M)

.
We remind that Kos(M) is the category of symmetric gauge structures in M.
Theorem. Given a statistical model M whose base manifold is M and whose
Fisher information is g, M is an exponential model if and only if the numerical
invariant rb(M, g) vanishes.
⊓⊔
Regarding details readers are referred to Nguiﬀo Boyom (2019; Nguiﬀo Boyom).
4
Reductions of Homogeneous Statistical Models
We go to deal with compact homogenous spaces of ﬁnite dimensional Lie groups
whose canonical Koszul classes vanish. This situation includes all of the transitive
actions of compact Lie groups on compact manifolds.
Our manipulations are similar to those which are performed in Marsden-
Weinstein-Marsden reductions of Hamiltonian actions in symplectic manifolds.
Relevant requirements on moment maps are replaced with vanishing of canonical
Koszul classes of Lie groups actions.
Let M be a compact homogeneous space of a Lie group H. Let (M, ∇) be a
gauge structure. Of the vector space (2,1)-tensors T 2
1 (M) is a H-module under
the following action: an element θ ∈T 2
1 (M) is C∞(M)-bilinear map of X(M) in
itself. The linear action
H × T 2
1 (M) ∋(γ, θ) →ρ(γ).θ ∈T 2
1 (M)
is deﬁned as it follows
ρ(γ).θ(X, Y ) = γ⋆(θ(γ−1
⋆(X), γ−1
⋆(Y )).
Here γ⋆is the diﬀerential of the diﬀeomorphism γ. We concider the cochain
complex of the group H with coeﬃcients in its module T 2
1 (M). We are interested
in the 1-cochain kH
∞which is deﬁned by a gauge structure (M, ∇) as it follows
(a) := kH
∞(γ) = ρ(γ).∇−∇∈T 2
1 (M).

Relevant Diﬀerential Topology in Statistical Manifolds
163
Warning: while ∇is not an element of T 2
1 (M) the right hand member of (a)
belongs to T 2
1 (M).
It is easy to check that kH
∞is a 1-cocycle whose cohomology class
[kH
∞] ∈H1
ρ(H, T 2
1 (M))
does not depend on the choice of ∇.
Deﬁnition. The cohomology class [kH
∞] is called the canonical Koszul class of
the dynamical system
H × M →M♣
Proposition. The canonical Koszul class vanishes if and only if tehre a gauge
structure (M, ∇) which is ﬁxed by the action
H × Gau(M) ∋(γ, ∇) →ρ(Γ).∇∈Gau(M)♣
Remember Gau(M) is the category of gauge structures in M.
Along next manipulations we will always deal with diﬀerentiable dynamical
systems whose canonical Koszul calsse vanishes.
Warning: It is clear that if H is compact the canonical Koszul classes
of its diﬀerentiable dynamics vanish.
4.1
Canonical Projective Systems of Aﬃnely Foliated
H-Homogeneous Manifolds
Given a compact homogeneous space of a Lie group H namely
H × M →M
we aim to construct ﬁnite projective system of H-homogeneous folated manifolds

(M p, Fp), Πp
q, q ≤p

ΠP
q : (M p, Fp) →(M q, Fq)
is a submersion of H-homogeneous foliated manifolds.
We start from a compact proper H-homgeneous space of a compact Lie group
H, namely
H × M →M.
Since the action of H is proper we ﬁx a positive Riemannian structure which is
H-invariant.
Deﬁnition. A proper action of H is non 1-rigid if the exists a H-invariant
positive Riemannian metric tensor whose Levi-Civita connection is non 1-rigid
♣
⊓⊔
Without statement of the contrary all of the proper actions we deal
with are non 1-rigid ♣

164
M. Nguiﬀo-Boyom
STEP 1.
We start from a non 1-rigid proper H-homogeneous space
H × M0 →M0.
We choose a H-invariant positive Riemannian structure (M0, g0) whose Levi-
Civita connection ∇0 is non 1-rigid.
The Hessian equation FE(∇0gives rise to a simply connected Lie group G∇0
whose Lie algebra is isomorphic to J∇0 ⊂X(M0).
Since M0 is compact J∇0 is the inﬁnitesimal counterpart of the locally eﬀec-
tive action
G∇0 × M0 →M0.
The foliation by the orbites of G∇0, namely F0 is H-homogeneous.
Indeed given a G∇0, say G∇0 and γ ∈H one has
γ(G∇0(x) = (γ.G∇0.γ−1)(γ(x)).
To make simpler we will assume that F0 has compact leaves. Therefore the space
leaves is denoted by
M1 = M0
F0
.
The quotient action of H, namely
H × M1 →M1
is proper and we assume that is non 1-rigid.
STEP 2.
We ﬁx a non 1-rigid H-invariant positive Riemannian structure (M1, g1 whose
Levi-Civita connection is denoted by ∇1. The fundamental Hessian equation
FE(∇1 gives rise to the simply connected Lie group G∇1 whose Lie algebra is
isomorphic to J∇1 ⊂X(M1). The Lie algebra J∇1 is the inﬁnitesimal versus of
the global action of G∇1 Palais (1957), namely
G∇1 × M1 →M1.
The foliation of M1 by the orbites of G∇1 id denoted by F1. As announced we
assume that F1 has compact leaves and we put the space of leaves
M2 = M1
F1
.
The quotient transitive action
H × M2 →M2
is assumed to be non 1-rigid.
By iteration under the constant assumption that H acts properly with 1-
rigidity we stop at the lowest step, namely
H × Mk →Mk.

Relevant Diﬀerential Topology in Statistical Manifolds
165
It is to notice that at each level (Mp, ∇p) the foliation Fp is a foliations
by ﬂat Riemannian submanifolds of the Riemannian manifold (Mp, gp).
Since we has assume that the leaves are compact those leaves are Euclidean
tori
(Ts
Γ , g⋆
0).
Here
Ts = Rs
Zs ,
g⋆
0 is the Riemannian metric induced by the Euclidian metric of Rs,
Γ is a ﬁnite group of isometries of of the ﬂat torus ♣
At the minimum level (Mk, ∇k both H and G∇k act transitively in Mk. So
the foliation Fk is trivial. It easily seen that (Mk, ∇k) is a compact locally ﬂat
manifold and that both Gk
∇and H act aﬃnely in (Mk, ∇k).
By the considerations d which are discussed above the manifold minimum
level Riemannian manifold (Mk, g⋆
0) is an Euclidean torus
(Mk, g⋆
k) = (Tm
Γ , gm).
4.2
Projective Sequence of Homogeneous Aﬃnely Foliated
Manifolds
The manipulations we have performed in the precedent subsection yield a ﬁnite
projective series of compact homogeneous aﬃnley ﬂiations manifolds.
We have constructed the following series of foliated manifolds,
(Mp, Fp)
with the canonical projections
Πp
p+1 : Mp →Mp+1 = Mp
Fp
Given non negative integers p < q we deﬁne
Πp
q : Mp →Mq
byputting
Πp
q = Πq−1
q
◦.... ◦Πp+1
p+2 ◦Πp
p+1.
The series

MP , Πp
q, p ≤q

is ﬁnite projective system.
Let qmin be the lowest level of our process of reduction. Then we have the
sequence of1 step reductions
(CR) : M0 →M1 →.. →Mqmin.

166
M. Nguiﬀo-Boyom
Here (CR) stands for complete reduction. We keep all of our constant assump-
tions as in the preedent sections. In every reduction Mq all of the leaves of
Fq have the same homotopy type. In particular they have the same ﬁrst Betti
number which is denoted by b1(Fq). Further we have the following inequality
b1(Fq) ≤rank(Fq).
The following statement is a direct consequence of the classiﬁcation of ﬂat Rie-
mannian manifolds Wolf (1974).
Proposition. In every reduction Mq the following assertions are equivalent:
(1) Fq admits a compact leaf;
(2) b1(Fq) = rank(Fq)♣
⊓⊔
4.3
Relative Invariant Subordinate Foliations
Given non negative integers
0 ≤p ≤q ≤qmin
we associate every projection Πp
q with foliated projection
Πp
q : (Mp, Fp
q ) →(Mq, Fq)
where Fp
q stands for the pullback of Fq by the projection Πp
q, i.e.
Fp
q = (Πp
q)−1(Fq).
It is clear that every triple
p ≤q ≤r ≤qmin
one has the following subordination
Fp
q ⊂Fp
r
which means leaves of Fp
q are included is leaves of Fp
r . Thus we view the projec-
tion Πp
q as sequence of foliations
(Mp, Fp) →(Mp, Fp
q ) →(Mq, Fq).
We consider
(x, y) ∈Mp × Mq
such that
y = ΠP
q (x);
F p
q (x) ⊂Mp is the leaf of Fp
q containing x.
Fq ⊂Mq is the leaf of Fq containing y.

Relevant Diﬀerential Topology in Statistical Manifolds
167
It is easy to check that F p
q (x) is compact. Though the Riemannian struc-
tures (Mp, gp) and (Mq, gq) are not related, the projection
Πp
q : F p
q (x) →Fq(y)
is a ﬂat Euclidean cylinders ﬁber bundle over a ﬂat Euclidean cylinder.
Indeed remember that
(Πp
q)−1(y) = G∇p(x)
and the orbites of G∇p are ﬂat cyclinders over torus whose Riemannian met-
ric is induced from the Euclidean metrics. Formally we view the situation just
described as it follows
Tk
Γ × Rm−k →F p
q (x) →Ts
Γ∗× Rn−s.
Deﬁnition. A complete reduction which satisﬁes conditions as in Proposition
4.4 is called a torus reduction ♣
⊓⊔
Thus a torus reduction gives rise to foliations Fp
q whose a F p
q (x) is the total
space of a ﬁber bundle whose leaves and base space are Euclidean tori, namely
Tr
Γ →F p
q (x) →Tr
Γ⋆.
Deﬁnition. A complete reduction {Mq, Fq, ≤q ≤qmin} is called ﬂat if for every
pair p ≤q and at the leaves level the leaves of the following foliation ﬁbration
Fp →Fp
q →Fq
gives rise ﬂat torus ﬁber bundle over ﬂat torus , namely
Tk →F p
q (x) →Ts♣
Henceforth, without statement of the contrary we go to deal with ﬂat
complete reductions. Therefore all of the foliation ﬁbrations yield the ﬁber
bundle as it follows
ΠP
q : Tk →F p
q (x) →Ts♣
Nevertheless these informations are toplogically remarkable. They tell that
in favorable circumstances every reduction Mq admits a foliation whose leaves
are (non principal) torus ﬁber bundle over torus. In particular, since the minimal
reduction Mqmin is a torus, we view Mqmin as the following quotient
M
F0
qmin . So
we are led to the following (topological) structure theorem.
Theorem (topological Structure Theorem). Let M be a compact manifold
which acted properly and transitively by a non 1-rigid a Lie group H. We
assume that this action admits a ﬂat complete reduction. Then M admits a
H-invariant foliation F whose space of leaves M
F is a ﬂat torus and whose leaves
are (not principal) torus ﬁber bundle over torus ♣
⊓⊔

168
M. Nguiﬀo-Boyom
Roughly speaking we view the (ﬁber bundle) projection Πp
q at foliation ﬁber
bundle, i.e.
Fp →Fp
q →Fq.
The following corollary is a direct application of the machineries of Palais-
Stewart (1961)
Corollary. The data are those as in structure Theorem 4.5. Let us assume that
all of the following ﬁber bundles
Tk →F p
q (x) →Ts
are principal toric ﬁbrations.
Then each leaf of Fp
q locally freely acted by a simply connected 2-step nilpo-
tent Lie group Gp
q♣.
⊓⊔
In ﬁnal, under more restrictive requirements we obtain the following statement.
Theorem. If data and assumptions are as in Corollary above,viz M is a com-
pact manifold admitting a non 1-rgid transitive proper action of a Lie group
H having a ﬂat complete reduction wich gives rise to principal toric ﬁber
bundles over torus. Them M is a 2-step nilmanifold, viz M is locally freely and
transitively acted by a simply connected 2-step nilpotent Lie group ♣
⊓⊔
Theorem 4.9 is a straightforward corollary of theorem due to Palais
and Stewart (1961)
4.4
Subordinate Foliations and Topology of <H, M>
We go to impliment the machineries

Mp, Fp ⊂Fp
q , Πp
q

.
The notation and relevant assumptions are those involved in the precedent sub-
sections. Thus deal with non 1-rigid proper transitive actions of a Lie group H in
a compact manifold M. Remember that every step (Mq, gq is a H-invariant pos-
itive deﬁnite Riemannian structure; ∇q is the Levi Civita connection of (Mq, gq)
In the manifold M one has the following subordinate foliations

F0 ⊂F0
1 ⊂.. ⊂F0
q ⊂.. ⊂F0
qmin

.
This picture is a geometric invariant of the diﬀerentiable dynamical system
H × M →M.
Every point x ∈M is associated with the following ﬁltration of M
F(x) ⊂F 0
1 (x) ⊂.. ⊂F 0
qmin(x) ⊂M.

Relevant Diﬀerential Topology in Statistical Manifolds
169
Here F 0
q (x) is the leaf of F0
q containing x. These machineries impact the de Rham
complex of M. For instance The cochain complex of basic diﬀerential forms of
F0
q , namely

Ω(M, F0
q+1), d

is a subcomlex of the complex

Ω(M, F0
q ), d

These impacts are not discussed in this paper.
Interesting examples which are not discussed here are the reduc-
tion of spheres acted by orthogonal groups
SO(m + 1) × Sm →Sm.
4.5
Metric Rigidity of F E(∇)
Let (M, ∇) be a symmetric gauge structure. the notation J∇and G∇keep the
same meaning as in the precedent subsections. Here is usel result we will use.
We assume that (M, ∇) is a symmetric gauge structure and M is compact;
Rie(M, ∇) is the set of positive Riemannian structure whose Levi Civita con-
nection is ∇.
Since M is compact, the Lie subalgebra
J∇⊂X(M)
is the inﬁnitesimal counterpart of a locally eﬀective diﬀerentiable action of G∇
in M. The orbites of G∇form a locally ﬂat connection (M, ∇); we denote this
foliation by F(∇). For every g ∈Rie(M, ∇) we set
(F(∇), g) = {(F, g⋆, ∇), F ∈F(∇)}
here F is a leaf of F(∇) and (F, g⋆) stands for the Riemannian structure induced
from (M, g). Thus (F, g⋆, ∇) is a ﬂat Hessian manifold.
Theorem. For every leaf F the functor
Rie(M, ∇) →(F, g⋆, ∇)
is a constant functor ♣
⊓⊔
Hint. The conclusion is a straight consequence of the ﬂatness of (F, g⋆, ∇).
Indeed let m be the dimension of F and let s be its ﬁrst Betti number then
(F, g) = (Ts
Γ × Rm−s, g0)
In the right hand member of the equality above one has:
Ts = Rs
Zs

170
M. Nguiﬀo-Boyom
g0 is induced from the Euclidean metric of Rm♣
Need of Examples. Our manipulations show that for a positive deﬁnite
Riemannian manifold (M, g) containing isometrically embedded ﬂat Euclidean
cylinders it suﬃcies that
JLC
∇
̸=, 0
Here ∇LC is the Levi Civita connection of (M, g).
5
The Case of Fisher Information
We consider a functional statistical model
M(Ξ, Ω) = {E, π, (M, D), P}
associated with a measurable set (Ξ, Ω); we assume that its Fisher information
g is deﬁnite. Here Γ) is the group of (measurable) isomorphisms of (Ξ, Ω).
Remember the group action
Γ × E × M ∋(γ, e, x) →(γ.E, γ.x) ∈E × M.
Remember that
E ∋e →π(e) ∈M
is a Γ-equivariant locally trivial ﬁber bundle whose ﬁber type is Ξ.
Remember that the non negative real valued function
E ∋e →P(e) ∈R
is a probability density in the following meaning: (1) if s is a local section of π
then the function P ◦s is diﬀerentiable
(2) for every ﬁber of π, namely Ex one has
Σe∈ExP(e) = 1;
(3) if s is a section of π then
∂
∂x[ΣExP(s(x))] = ΣEx[ ∂
∂xP(s(x))]♣
Remember that the Fisher information g is a symmetric bilinear form
which deﬁned in the manifold M as it follows; given a section of π,
namely s,
g(x).(X, Y ) = ΣEx[(P ◦s)[(X(log(P ◦s)))(Y (Log(P ◦s)))]](x)♣
Let ∇bre the Levi Civita connection of (M, g).
J∇is an inﬁnitesimal locally eﬀective action of G∇. We assume to be dealing
with one of the alternatives:
either M is compact
or (M, g) is complete.

Relevant Diﬀerential Topology in Statistical Manifolds
171
In each of the two cases J∇is the inﬁnitesimal counterpart of a locally eﬀec-
tive action
G∇× M →M.
Each m-dimensional orbite
F(x) = G∇(x)G
is a cylinder
F(x) = Ts
Γ⋆× Rm−s
and the metric that it inherits from g coincides with the metric inherited from
the Euclidean metric g⋆.
In addition the statistical model
Mx(Ξ, Ω) =

EF (x), π, F(x), P

is an exponential model, Amari (1990), Nguiﬀo Boyom (2016) Nguiﬀo Boyom
(2019), Shima (2007).
The restriction of M(Ξ, Ω) to the locally ﬂat foliations F(∇) is denoted by
M∇= {E, π, F(∇), P)}
5.1
α-equivalence
We introductuce to introduce an equivalence relation between structures of func-
tional statistical model on the same ﬁber bundle
{E, π, M}
Deﬁnition. Two functional statistical models
{E, π, M, P}
and
{E, π, M, P ⋆}
are α-equivalent if they have the same family of α-connections ♣
⊓⊔
The interest in the α-equivalence is highlighted as it follows
Theorem. If
{E, π, M, P}
and
{E, π, M, P ⋆}
are α-equivalent then
{E, π, F(∇), P} = {E, π, F(∇), P ⋆} ♣
Remark. Theorem we just stated highlights a status of the family
of α-connections. The family of α-connections, {∇α, α ∈R} rigidiﬁes
geometric informations along the orbites of the Lie groups G∇.
Remember that ∇is the Levi Civita connection of the Fisher information
∇= 1
2(∇α + ∇−α)♣

172
M. Nguiﬀo-Boyom
6
Relevant Foliations in Statistical Manifolds
In this section we aim to explore a few impacts of the Fundamental gauge equa-
tion of the diﬀerential tology of statistical manifolds. A remarkable impact we go
to point out without heavy details is the complete answer to the question raised
by Etienne Ghys (Ghys): How to construct all Riemannian foliations.
6.1
Statistical Manifolds
Deﬁnition. A statistical manifold is a triple (M, g, ∇) formed of a Riemannian
manifold (M, g), a symmetric gauge structure (M, ∇) and the identity
(∇Xg)(Y, Z) −(∇Y g)(X, Z) = 0, ∀(X, Y, Z) ⊂X(M)
The g-dual of ∇, namely ∇g deﬁned by the following identity
g(∇g
XY, Z) = Xg(Y, Z) −g(Y, ∇XZ), ∀(X, Y, Z) ⊂X(M).
It is easy to check that (M, g, ∇g) is a statistical manifold.
6.2
Gauge Diﬀerential Operators
In a diﬀerentiable manifold M every pair of gauge structure [(M, ∇), (M, ∇⋆)]
is associated with a ﬁrst diﬀerential operator
D∇∇⋆: T 1
1 (M) →T 2
1 (M).
Remember the Notation
T 1
1 (M) = T ⋆M ⊗TM,
T 2
1 (M) = T ⋆⊗2M ⊗TM.
Sect(T 1
1 (M) is the C∞(M)-module of (diﬀerentiable) sections of T 1
1 (M).
Deﬁnition. The ﬁrst order diﬀerential operator D∇∇⋆
Sect(T 1
1 (M)) ∋φ →∇⋆◦φ −φ ◦∇∈T 2
1 (M)
is deﬁned as
[D∇∇⋆φ](X, Y ) = ∇⋆
Xφ(Y ) −φ(∇XY )♣
What we call the gauge equation of the pair (∇, ∇⋆) is the ﬁrst order linear
equation
FE(∇∇⋆) : D∇∇⋆φ = O.
The sheaf of solutions of FE(∇∇⋆) is denoted by J∇∇⋆(M) and The vector
space of its sections is denoted by J∇∇⋆.

Relevant Diﬀerential Topology in Statistical Manifolds
173
6.3
Relevant Constructions in Gauge Structures
Notation. Given a Koszul connection ∇(in TM) the vector space of ∇-invariant
skex symmetric bilinear forms in M is denoted by Ω∇
2 (M). The vector space of
∇-invariant symmetric bilinear forms in M is denoted by S∇
2 (M).
We go to focus on the case
{∇⋆= ∇g, (∇, g) ∈Gau(M) × Rie(M)} .
Then we put
J∇,g(M) = J∇∇g(M).
It is easy to check the following claim.
Proposition. The three vector spaces Ω∇
2 (M), J∇,g, S∇
2 (M) are related by a
canonical short exact sequence of vector spaces
O →Ω∇
2 (M) →J∇∇⋆→S∇
2 (M) →0
Hint. Decompose every φ ∈J∇,g as
Φ⋆←φ →Φ
Where the couple (Φ⋆, Φ) ⊂J∇,g is deﬁned as it follows
Ω∇
2 (M) ∋ω(g, φ) :
ω(g, φ)(X, Y ) = 1
2[g(φ(X), Y )−g(X, φ(Y )] = g(Φ⋆(X), Y ),
S∇
2 (M) ∋q(g, φ) :
q(g, φ)(X, Y ) = 1
2[g(φ(X), Y )+g(X, φ(Y )] = g(Φ(X), Y )♣
The following claims are obvious
Both ω(g, φ) and q(g, φ) have constant rank.
The distributions Ker(Φ⋆), Ker(Φ) are ∇-parallel.
The distributions Im(Φstar), Im(Φ) are ∇g-parallel ♣
Assume that ∇is torsion free.
Then the diﬀerential 2-form ω(g, φ) is deRham closed and
LXq(g, φ) = 0 whenever iXq(g, φ) = 0,
LXω(g, φ) = 0 whenever iXω(g, φ) = 0. Major topological features when
∇is tosion free.
ω(g, φ) is a symplectic foliation ∀φ ∈J∇,g.
q(g, φ) is a Riemannian foliation ∀φ ∈J∇,g.
Here LX is the Lie derivative in the direction X.
iX is the inner product by X.
The following yields the answer to the question raised by Etienne Ghys
Theorem. For a Riemannian structure (M, g) not admitting any Riemannian
foliation it necessary and suﬃcient that the sheaf J∇,g(M) be trivial ∀∇∈
Kos(M)♣
⊓⊔

174
M. Nguiﬀo-Boyom
6.4
Relevant Foliations in Positive Statistical Manifols
Our aim is to implement the constructions as in the precedent subsection in the
framework of statistical manifolds.
We view a statistical manifold as a quadruple
{M, g, ∇, ∇⋆:= ∇g}
Here the Riemannian metric tensor g is positive deﬁnite. TO avoid triviality
we implicitly assume that the sheaf J∇,g(M) is not trivial. Thus the vector
space of solutions of the gauge equation FE(∇∇⋆, namely J∇,g is viewed as
an extension of the space of ∇-invariant Riemannian foliations S∇
2 (M) by the
space ∇-parallel symplectic foliations Ω∇
2 (M). This notion of extension is
materialized by the following short exact sequence
0 →Ω∇
2 (M) →J∇,g →S∇
2 (M) →0.
Φ⋆←φ →Φ
Since the metric tensor g is positive deﬁnite the following sums are g-orthogonal
TM = Ker(Φ⋆) ⊕Im(Φ⋆)
TM = Ker(Φ) ⊕Im(Φ.)
6.5
Symplectic Statistical Foliations
We introduce the following notion.
Deﬁnition. A symplectic statistical manifold is the data
{M, g, ∇, ∇⋆, ω}
formed of a statistical structure (M, g, ∇, ∇⋆), a symplectic structure (M, ω)
such that
Either
∇Xω = 0, ∀X ∈X(M)
Or
∇⋆
Xω = O, ∀X ∈X(M)♣
Deﬁnition. A symplectic statistical foliation in a statistical manifold (M, g, ∇)
is a statistical foliation
(F, g, ∇, ∇⋆) ⊂(M, g, ∇, ∇⋆)
whose leaves are uniformally symplectic statistical manifolds ♣
⊓⊔
Thus every leaf F is Either ∇-autoparallel Or ∇⋆-auto-parallel.
The following feature is a straightforward consequence of our constructure.

Relevant Diﬀerential Topology in Statistical Manifolds
175
Theorem. Let (M, g, ∇, ∇g) be a statistical manifold. Then for every solution φ
of the gauge equation FE(∇∇g) the foliation Im(Φ⋆) ia a symplectic statistical
foliation ♣
⊓⊔
Hint
Since ∇g is torsion free the distribution Im(Φ⋆) is completely integrable. Fur-
ther every leaf F ∇g-auto-parallel. Therefore the triple (F, g, ∇g) is a statistical
manifold in the following sense
(∇g
Xg)(Y, Z) = (∇g
Y g)(X, Z), ∀(X, Y, Z) ⊂X(F).
At another side, in F the restriction of ω(g, φ) is symplectic form.
In F the g-dual of ∇g is the g-projection of ∇.
6.6
Almost Hermitian Foliations in Statistical Manifolds
We keep the same notation as in the precedent subsection.
For every
φ ∈J∇,g
we consider the couple
(ω(g, φ), Φ⋆).
The linear operator
−Φ⋆2 = −Φ⋆◦Φ⋆
is positive in the following sense
0 ≤g(−Φ⋆2(X)X).
The restriction of Φ⋆2 to (Im(Φ⋆), g) is reversible and positive. That allows the
construction of the complex structure in the vector bundle Im(Φ⋆), namely
J = Ψ−1 ◦Φ⋆
where Ψ is the positive square root of −Φ⋆2 Thus in every leaf F the triple
{F, ω, J}
is an almost Hermitian manifold. In ﬁnal
{Im(Φ⋆, ω(g, φ), J}
is a almost Hermitian statistical foliation in (M, g, ∇, ∇g)♣

176
M. Nguiﬀo-Boyom
6.7
Riemannian Statistical Foliations
The symmetric alter ego of symplectic statistical foliation is Riemannian
statistical foliation.
Deﬁnition. A structure of Riemannian statistical manifold is the data
{M, g, ∇, ∇⋆, Q}
formed of a statistical structure (M, g, nabla⋆) and a Riemanian structure (M, Q
which subject to the following requirements
Either
∇XQ = 0, ∀X,
Or
∇⋆
XQ = O, ∀X♣
Theorem. In a statistical manifold (M, g, ∇, ∇⋆) every solution of the gauge
equation, namely
Φ⋆←φ →Φ
gives rise to the Riemannian statistical foliation
{Im(Φ), g, ∇, q(g, φ)} ♣
Hint: Mutatis mutandis the idea of proof is similar to the case of
symplectic statistical foliation ♣. It is to notice both Ker(Φ⋆and Ker(Φ)
are statistical foliations. Thus every
φ ∈J∇∇⋆
gives rise to the statistical 4-web
{Ker(Φ), Ker(Φ⋆), Im(Φ), Im(Φ⋆)}
Examples. In dimension three the symplectic statistical foliations Im(Φ⋆) are
statistical Kaehlerian foliations.
In dimension four the symplectic statistical foliations Im(Φ⋆) are either
Kaelerian foliations or trivial uniformally almost Hermitian foliations.
6.8
α-Family of 4-Webs in Statistical Models of Measurable Sets
Warning
For the attention of users of applied statistics it is not without interest to point
out that the polls are nothing else than coarse foliations, i.e. set a partitions. Thus
from the strict point of view of Applied Statistics the study of webs in statistical
models is capital and responds to two questions raised independently, the ﬁrst by
Peter McCullagh in 2002, What is a statistical model MacCullagh (2002),
the second by Misha Gromov in 2012, The search of structures,
Gromov

Relevant Diﬀerential Topology in Statistical Manifolds
177
(2013). Among the fundamental questions that motivated Nguiﬀo Boyom (2016)
there are these two questions of McCullagh and Gromov.
We will also understand the impacts of the diﬀerential topology in statistical
models Henceforth we restrict ourself to functional statistical models
{E, π, M, P} .
Among relevant invariants of a functional statistical model is its family of α-
connections
{∇α, α ∈R}
Our machineries endow the base manifold of functional model with one param-
eter family of gauge equations

F(∇α∇−α), α ∈R

.
So to every real number α one many plenty of statistical 4-webs which are incoded
by the vector space
Jα,g = J∇α∇−α,
Here g is the Fisher information of the model.
Theorem. In a functional statistical model {E, π, M, P} let D be the Levi
Civita connection of the Fisher information g. Then for every φ ∈Jα,g the
4-web
{Ker(Φ), Ker(Φ⋆), Im(Φ), Im(Φ⋆)}
is totally D-geodesic ♣
⊓⊔
Hint. Involve ingredients used in proofs of both Theorem 5.9 and Theorem
5.10.♣
Remember the α-equivalence relation between functional statistical
models. The family of D-geodesic 4-webs as in Theorem above is an
invariant of the moduli space of α-class of functional statistical models
♣
References
Amari, I.-S.: Geometrical Methods in Statistics. Lecture Notes in Statistics, vol. 28.
Springer, New York (1990)
Amari, I.-S., Nagaoka, H.: Methods of Information Geometry. Translations of Mathe-
matical Monographs, vol. 191. AMS-OXFORD (2000)
Cartan, E.: Sur les vari´et´es `a coonexions aﬃnes et la th´eorie de relativit´e g´en´erals´ee.
Ann. Sci. Ec. Norm. Sup. (1) 40, 325–412 (1923); (2) 40, 1–25 (1924). (3) 42, 17–88
(1925)
Ghys, E.: How to construct all Riemannian foliations, in Molino’s, Riemnnian foliations,
Appendix
Gromov, M.: The search of structures: (i) ECM6, KRAKOW 2012, (ii) MaxEnt 2014.
Proc. Amer Soc Phys (2013)

178
M. Nguiﬀo-Boyom
Guillemin, V.: The integrability problem for G-structures. Trans. Am. Math. Soc. 116,
544–560 (1965)
Guillemin, V., Sternberg, S.: An algebraic model for transitive diﬀerential geometry.
Bull. Am. Math. Soc. 70, 16–47 (1964)
Hochschild, G., Serre, J.P.: Cohomology of group extensions. Trans. Amer Math Soc.
74(1), 110–134 (1953)
Kobayashi, S.: The theory of connections. Annali di Matematica 43, 119–194 (1957)
Singer, I., Sternberg, S.: The inﬁnte groups of Lie and Cartan. J. Analyse Math. 15,
114 (1965)
Koszul, J.-L.: Sur les deformations des varietes localement plates
Sur l’homologie des formes diﬀ?rentielles d’ordre superieur
Lichnerowicz, Theorie globale des connexions et des groupes d’holonomie. Edizioni
Cremonese (1962)
Lyndon, R.C.: The conhomology of group extensions. Duke Math. J. 15(1), 271–292
(1948)
MacCullagh, P.: What is a statistical model. Ann. Stat. 30(5), 1225–1310 (2002)
Moerdijk, I., Mrcun, J.: Introduction to Foliations and Lie groupoids. Cambridge Stud-
ies in Advanced Mathematics, vol. 91 (2003)
Molino, P.: Riemannian Foliations. Birkhauser, Boston (1988)
Murray, M.K., Rice, J.W.: Diﬀerential Geometry and Statistics. Monographs on Statis-
tics and Applied Probability, vol. 48. Chapman-Hall CRC, London (1993)
Nguiﬀo Boyom, M.: Foliation-web-heessian geometry-information geometry-enropy and
cohomology. Entropy 18(12), 433 (2016)
Nguiﬀo
Boyom,
M.:
The
last
formula
of
Koszul.
J.
Inf.
Geom.
(2019).
https://doi.org/10.10071-48
Nguiﬀo Boyom, M.: New Invariants and Old open Problems in the quantitative Global
Analysis (preprint)
Palais, R.: A global formulation of the lie theory of transformation groups. Mem. Am.
Math. Soc. 22 (1957)
Palais, R.S., Stewart, T.E.: Torus bandle over torus. Proc. Am. Math. Soc. 12, 26–40
(1961)
Reihnardt, B.L.: Foliated manifolds with bundlelike metrics. Ann. Math. 69(2), 119–
1329 (1959)
Shima, H.: The Diﬀerential Geometry of Hessian Manifolds. World Scientiﬁc Publish-
ing, Hackensack (2007)
Wolf, J.: Spaces of Constant Curvature. Publish of Perish, Boston (1974)

A Lecture About the Use of Orlicz Spaces
in Information Geometry
Giovanni Pistone(B)
de Castro Statistics, Collegio Carlo Alberto, Turin, Italy
giovanni.pistone@carloalberto.org
Abstract. This chapter is a revised version of a tutorial lecture that
I presented at the ´Ecole de Physique des Houches on July 26–31 2020.
Topics include: Non-parametric Information Geometry, the Statistical
bundle, exponential Orlicz spaces, and Gaussian Orlicz-Sobolev spaces.
1
Introduction
This chapter is a revision of the lecture and the related hand-out which I pre-
sented to the ´Ecole de Physique des Houches on July 26–31 2020. Due to its
strictly tutorial character, I shall not give detailed primary references. I shall
mention some references that expand and support speciﬁc points, and I shall
add some in a ﬁnal section to point to further developments.
I aim to review the basics of a peculiar setting for Information Geometry
(IG) in the sense of [3]. This setting has the following peculiarities.
• It is non-parametric and inﬁnite-dimensional.
• It provides an aﬃne manifold modeled on a Banach space in the sense of [15],
the Banach space being an Orlicz space as deﬁned [1, Ch. 8].
• It focuses on a particular expression of the tangent bundle, called Statistical
Bundle (ST).
• It allows for the use of (weakly) diﬀerentiable densities when the reference
measure is Gaussian as in [19, Ch. V].
A previous tutorial paper [31] presents this non-parametric construction in
the case of ﬁnite state space. A general presentation of the ﬁnite case is in [4,
Ch. 2]. Here, I will focus on the preliminaries of the inﬁnite state space case. A
comprehensive modern introduction to the whole topic is found in [23].
There are many other successful presentations of IG which are indeed non-
parametric. I will give a few relevant references in the concluding section. Any
proper presentation should explain and include its historical development ele-
ments below 1 to 5.
1. The starting point to consider is the now classical work of R. Fisher, see,
for example, [10, Ch. 4]. A regular statistical model is a mapping from a set
of parameters Θ which is an open domain of Rd to probability densities on a
given measured sample space P(X, X, μ), θ →p(θ), such that the following
c
⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2021
F. Barbaresco and F. Nielsen (Eds.): SPIGL 2020, PROMS 361, pp. 179–195, 2021.
https://doi.org/10.1007/978-3-030-77957-3_10

180
G. Pistone
computation is feasible. If f is a given random variable, one wants to compute
the variation of the expectation (assuming its existence), as
∂
∂θi
Ep(θ) [f] = ∂
∂θi

f p(θ) dμ = ∂
∂θi
⟨f, p(θ)⟩μ
=

f, ∂
∂θi
p(θ)

μ
=

f,
∂
∂θi p(θ)
p(θ)
p(θ)

μ
=

f, ∂
∂θi
log p(θ)

p(θ)
=

f −Ep(θ) [f] , ∂
∂θi
log p(θ)

p(θ)
.
The random vector with components
∂
∂θi log p(θ) is the Fisher score of the model
al θ. Its expected value with respect to p(θ) is 0 and its variance matrix with
respect to p(θ) is the Fisher information matrix,
I(θ) =

Ep(θ)
 ∂
∂θi
log p(θ) ∂
∂θj
log p(θ)

ij
=
	
∂
∂θi p(θ) ∂
∂θj p(θ)
p(θ)
dμ

ij
.
(1)
The computations above shows many peculiar features:
• One computes the variation of the expectation as a function of the statistical
model. Moreover, a moving inner product θ →⟨·, ·⟩p(θ) appears naturally.
• The score ∂i log p(θ) represents the velocity of variation of the statistical
model, while f −Ep(θ) [f] represents the gradient of the expectation func-
tion.
• The velocity at θ lives in the space of random variables centred at p(θ).
• The information matrix provides squared norms and scalar products of the
velocities in the moving inner product.
2. One explanation of the Fisher computations results from the assumption of
an exponential model,
p(θ) = e

i θiui−κ(θ) ,
where ui are the suﬃcient statistics and κ(θ) is the cumulant of 
i θiui, see, for
example, [10, Ch. 5]. In such a case, the score is the centered suﬃcient statistics,
∂i log p(θ) = ui −∂iκ(θ) = ui −Ep(θ) [ui] ,
and the information matrix equals the Hessian of the convex function θ →κ(θ),
I(θ) = [∂i∂jκ(θ)]ij .
See [6, Ch. 2] for a full account of analytic properties of exponential families.
3. C. R. Rao has been the ﬁrst statistician to remark that the Fisher information
matrix is positive deﬁnite and smooth in an adequately deﬁned regular model.
Hence it deﬁnes a Riemannian metric on the space of parameters. Moreover,
he provided an embedding argument for the resulting manifold. What possibly

Orlicz Spaces in IG
181
inspired him in his construction was Diﬀerential Geometry use in Physics. The
original presentation of IG in [3] can be considered the complete unfolding of
this approach.
The mapping
Θ ∋θ →2

p(θ) = P(θ)
maps the parameters’ space Θ into the L2(μ)-sphere of radius 2. The vectors
∂iP(θ) = ∂i2

p(θ) = ∂ip(θ)

p(θ)
are in the tangent space at P(θ) of the sphere, and the inner product between
tangent vector is

∂iP(θ)∂jP(θ) dμ =
 ∂ip(θ)∂j(x, θ)
p(x; θ)
dμ ,
that is, the (i, j) element of the Fisher information matrix.
The Rao’s computations above reproduce all the metric structure of the
Fisher computations but in one point. That is, now the velocity is not expressed
by the logarithmic derivative ∂i log p(θ) = ∂ip(θ)/p(θ), but it is expressed by
∂i2

p(θ) = ∂ip(θ)/

p(θ).
It is possible to make sense, at least formally, of the apparent contradiction
by considering that there are here three diﬀerent expressions of the same object.
• TP> is the tangent bundle of the set of positive densities P>; the tangent
vectors ˙p satisfy

˙p dμ = 0.
• TS2 is the tangent bundle of the L2(μ) sphere S2.
• SP> is the Fisher’s statistical bundle consisting of all couples (p.u) such that
p ∈P> and Ep [u] = 0.
The Rao’s embedding p →2√p provides the identiﬁcation of TP with TS2.
In fact, the computation of the tangent mapping of s: P →P 2/4 = p gives
ds(P)[ ˙P] = √p ˙P.
The identiﬁcation of TS2 with SP is provided by
TS2 ∋(P, ˙P) →

1
4P 2, 2
˙P
P

= (p, u) ∈SP .
In fact,

p dμ = 1
4

P 2 dμ = 1 ,

u p dμ = 1
2

˙P
P P 2 dμ = 1
2

P ˙P dμ = 0 ,

u1u2 p dμ =

˙P1
P
˙P2
P P 2 dμ =

˙P1 ˙P2 dμ .

182
G. Pistone
A large part of the literature in IG uses the expressions TP and TS2. Still,
my own choice is to use SP. It ﬁts well with the statistical picture and the
exponential representation of strictly positive densities.
The choice of the exponential expression and Fisher’s score might seem arbi-
trary, but the following argument shows is not, see [4, Ch. 3]. Assume t →μ(t) is
a one-dimensional smooth model of probability measures and assume the map-
ping t →μ(A; t) is smooth for all measurable A. Fix a value ¯t of the parameter
t. If a measurable set A is a zero set for μ(¯t), then t →μ(A, t) is minimum at
t = ¯t then the derivative is zero at ¯t, namely ˙μ(¯t) = 0. Il follows that the mea-
sure ˙μ(¯t) is absolutely continuous with respect to μ(¯t). The resulting logarithmic
derivative d ˙μ(¯t)/dμ(¯t) is clearly a generalization of the Fisher’s score.
4. The Riemannian approach by C. R. Rao can lead to a more in-depth study of
the second-order properties of the manifold, namely the Levi-Civita connection
and the curvature. Several authors, notably S-I. Amari, B. Efron, Ph. Dawid,
S. Lauritzen have promoted a more general perspective in studying statistical
models’ geometry. In modern terminology, a statistical manifold consists of a
metric and a couple of ﬂat connections, in duality for the given inner product.
This set-up nicely solves the divide between Fisher’s and Rao’s approach by
producing a uniﬁed theory. Moreover, the speciﬁc type of aﬃne manifold relevant
for IG is a Hessian manifold. That is, all its structure depends on a master convex
functional.
5. Both theoretical and applied research have recently shown interest in a par-
ticular type of non-parametric statistical models. Namely, models where Rn is a
model for the sample space, the reference measure is either the Lebesgue mea-
sure or the Gaussian measure and the densities are required to have some level
of smoothness.
For example, there is the statistical estimation method based on Hyv¨arinen’s
divergence,
DH (p|q) = 1
2

|∇log p(x) −∇log q(x)|2 p(x) dx ,
(2)
see [2, §13.6.2], where p, q are positive probability densities of the n-dimensional
Lebesgue space. It is assumed that the log-densities are smooth and the integral
exists. If we deﬁne the Otto’s inner product by
⟪f, g⟫p =

∇f(x) · ∇g(x) p(x) dx ,
(3)
where p is a probability density and f, g are smooth random variables such
that Ep [f] = Ep [g] = 0, then the development of the square in the Hyv¨arinen
divergence produces the term
⟪log p −Ep [log p] , log q −Ep [log q] ⟫p .
Notice that the equation above with p = q reminds of the Fisher information of
Eq. (1), with parameter derivative replaced by spatial derivatives. In particular,

Orlicz Spaces in IG
183
parameter and spatial derivatives coincide in the case of translation models, that
is, when p(x, θ) = p(x + θ).
In the Gaussian case, the celebrated log-Sobolev inequality, see [16, Ch. 5],
can be written as
Ent(p) =

p(x) log p(x) γ(x)dx
≤2
 ∇

p(x)

2
γ(x)dx = 1
2 ⟪log p −Ent(p), log p −Ent(p)⟫p ,
where γ is the standard Gaussian density, p is a density with respect to γ and
|·| is the Euclidean norm.
In the following, I shall focus on the exponential representation of positive
densities p = eu−K(u). The reference measure is specialised to be μ(dx) = γ(x)dx
where γ is the standard Gaussian density of Rn. The suﬃcient statistics u is
assumed to belong to an exponential Orlicz space, to be deﬁned next, and such
that

u(x) γ(x) dx = 0. The normalizing constant u →K(u) is a convex func-
tion deﬁned on the exponential Orlicz space. The interior of the proper domain
of K is the parameter set of a maximal exponential Gaussian space model. Here,
maximal means that the model contains all possible ﬁnite dimensional exponen-
tial families. I will aim to sketch a theory in which all concerns of items 1 to 5
meet a solution of a sort.
2
Orlicz Spaces
First, let us review a few elements of the theory of Orlicz spaces and ﬁx a
convenient notation. I do not not aim to full generality, cf. [1, Ch. 8]. If φ ∈
C[0, +∞[ satisﬁes:
1. φ(0) = 0,
2. φ is strictly increasing, and
3. limu→+∞φ(u) = +∞,
then its primitive function
Φ(x) =
 x
0
φ(u) du ,
x ≥0 ,
is strictly convex. The function Φ is extended to R by symmetry, Φ(x) = Φ(|x|),
and is called Young function.
The inverse function ψ = φ−1 has the same properties 1) to 3) as φ, so that
its primitive
Ψ(y) =
 y
0
ψ(v) dv ,
y ≥0 ,
is again a Young function. The couple (Φ, Ψ), is a couple of conjugate Young
functions. The relation is symmetric and we write both Ψ = Φ∗and Φ = Ψ∗.
The Young inequality holds true,
Φ(x) + Ψ(y) ≥xy ,
x, y ≥0 ,

184
G. Pistone
and the Legendre equality holds true ,
Φ(x) + Ψ(φ(x)) = xφ(x) ,
x ≥0 .
Here are the speciﬁc cases I am going to use. The sub-2 index denotes the
2nd Taylor remainder.
Φα(x) = xα
α ,
Ψβ(y) = yβ
β ,
α, β > 1 ,
1
α + 1
β = 1 ;
(4)
exp2(x) = ex −1 −x ,
(exp2)∗(y) = (1 + y) log(1 + y) −y ;
(5)
cosh2(x) = cosh x −1 ,
(cosh2)∗(y) =
 y
0
sinh−1(v) dv ;
(6)
gauss2(x) = exp
1
2x2

−1 .
(7)
Given a Young function Φ and a probability measure μ, the Orlicz space LΦ (μ)
is the Banach space whose closed unit ball is

f ∈L0(μ)
 
Φ(|f|) dμ ≤1

. This
deﬁnes the Luxemburg norm, characterized by
∥f∥LΦ(μ) ≤ρ
if, and only if,

Φ(ρ−1 |f|) dμ ≤1 .
Because of the Young inequality, it holds

|uv| dμ ≤

Φ(|u|) dμ +

Φ∗(|v|) dμ .
This provides a separating duality ⟨u, v⟩μ =

uv dμ of LΦ (μ) and LΦ∗(μ) such
that
⟨u, v⟩μ ≤2 ∥u∥LΦ(μ) ∥v∥LΦ∗(μ) .
From the conjugation between Φ and Ψ, an equivalent norm can be deﬁned,
namely, the Orlicz norm
∥f∥LΦ(μ)∗= sup

⟨f, g⟩μ
 ∥f∥LΨ (μ) ≤1

.
The domination relation between Young functions imply continuous injection
properties for the corresponding Orlicz spaces. We wll say that Φ2 eventually
dominates Φ1, written Φ1 ≺Φ2, if there is a constant k such that Φ1(x) ≤Φ2(kx)
for all x larger than some ¯x. As, in our case, μ is a probability measure, the
continuous embedding LΦ2 (μ) →LΦ1 (μ) holds if, and only if, Φ1 ≺Φ2. If
Φ1 ≺Φ2, then (Φ2)∗≺(Φ1)∗.
A special case occurs when there exists a function C such that Φ(ax) ≤
C(a)Φ(x) for all a ≥0, which is the case, for example, for a power function and
in the case of the functions (exp2)∗and (cosh −1)∗. In such a case, the dual
couple is a couple of reﬂexive Banach spaces, and bounded functions are a dense
set. I will return to this important topic below.

Orlicz Spaces in IG
185
The spaces corresponding to power case (4) coincides with the ordinary
Lebesgue spaces. The norm are related by
∥f∥LΦα(μ) = α1/α ∥f∥Lα(μ) .
With reference to our examples (5) and (6), we see that exp2 and (cosh −1) are
equivalent. They both are eventually dominated by gauss2 (7) and eventually
dominate all powers (4). The cases (5) and (6) provide isomorphic B-spaces
L(cosh −1) (μ) ↔Lexp2 (μ) which are of special interest for us as they provide the
model spaces for our non-parametric version of IG, see Sect. 4 below.
Clearly, a function belongs to the space Lcosh2 (μ) if, and only if, its moment
generating function λ →

eλf is ﬁnite in a neighborhood of 0. In turn, this
implies that the moment generating function is analytic at 0, see [6, Ch. 2].
The same property is equivalent to a large deviation inequality, see [38, Ch.
2]. A function f belongs to Lcosh −1 (μ) if, and only if, it is sub-exponential, that
is, there exist constants C1, C2 > 0 such that
μ(|f| ≥t) ≤C1 exp (−C2t) ,
t ≥0 .
Let us check the equivalence above. If ∥f∥Lcosh2(μ) = ρ, then

eρ−1|f| dμ ≤4.
It follows that
μ(|f| > t) = μ

eρ−1|f| > eρ−1t
≤

eρ−1|f| dμ

e−ρ−1t ≤4e−ρ−1t .
The sub-exponential inequality holds with C1 = 4 and C2 = ∥f∥−1
Lcosh2(μ). Con-
versely, for all λ > 0,

eλf dμ ≤
 ∞
1
μ

eλf + > t

dt ≤C1
 ∞
0
e−(C2λ−1−1)s ds .
The right-hand side is ﬁnite if λ < C2 and the same bound holds for −f.
A sub-exponential random variable is of particular interest in applications
because they admit an explicit exponential bound in the Law of Large Numbers.
Another class of interest consists of the sub-Gaussian random variables, that is,
those random variables whose square is sub-exponential.
The theory of sub-exponential random variables provides an equivalent norm
for the space Lcosh2 (μ), namely the norm
f →sup
k

(2k)!−1

f 2k dμ
1/2k
= ⎪
⎪
⎪f⎪
⎪
⎪cosh2 .
See [7] or [36]. Let us prove the equivalence. If ∥f∥Lcosh2(μ) ≤1, then
1 ≥

cosh2 f dμ ≥
1
(2k)!

f 2k dμ
for allk = 1, 2, . . . ,

186
G. Pistone
so that 1 ≥⎪
⎪
⎪f⎪
⎪
⎪cosh2. Conversely, if the latter inequality holds, then

cosh2(f/
√
2) dμ =
∞

k=1
1
(2k)!

f 2k dμ
1
2
k
≤1 ,
so that ∥f∥Lcosh2(μ) ≤
√
2.
It is covenient to introduce a further notation. For each Young function Φ,
the function Φ(x) = Φ(x2) is again a Young function such that ∥f∥LΦ(μ) ≤λ if,
and only if,
|f|2
LΦ(μ) ≤λ2. We will denote the resulting space by L2
Φ (μ).
For example, gauss2 and cosh −1 are ≺-equivalent, hence the isomorphisn
Lgauss2 (μ) ↔L2
(cosh −1) (μ). As an application of this notation, consider that for
each increasing convex Φ it holds Φ(fg) ≤Φ((f 2 + g2)/2) ≤(Φ(f 2) + Φ(g2))/2.
It follows that when the L2
Φ (μ)-norm of f and of g is bounded by one, the
LΦ (μ)-norm of f, g, and fg, are all bounded by one. The space Lcosh2 (μ) has
a continuous injection in the Fr´echet space L∞−0(μ) = ∩α>1Lα(μ), which is an
algebra. When we need the product, we can either assume the factors are both
sub-Gaussian or move up the functional framework to the Lebesgue spaces’ inter-
section.
Let us now discuss speciﬁc issues of the Gaussian exponential Orlicz spaces
Lcosh2 (γ), γ the standard n-variate Gaussian density. Dominated convergence
does not hold in this space. The squared-norm function f(x) = |x|2 belongs to
the Gaussian exponential Orlicz space Lcosh2 (γ) because

cosh2(λf(x)) γ(x)dx < ∞
for all
λ < 1/2 .
The sequence fN(x) = f(x)(|x| ≤N) converges to f point-wise and in all Lα(γ),
1 ≤α < ∞. However, the convergence does not hold in the Gaussian exponential
Orlicz space. In fact, for all λ ≥1/2,

cosh2(λ(f(x) −fN(x))) γ(x)dx =

|x|>N
cosh2(λf(x)) γ(x)dx = ∞,
but convergence would imply
lim sup
N→∞

cosh2(λ(f(x) −fN(x))) γ(x)dx ≤1
for all λ > 0 .
The closure in Lcosh2 (γ) of the vector space of bounded functions is called
Orlicz class and it is denoted by Mcosh2(γ). One can prove that f ∈Mcosh2(γ)
if, and only if. the moment generating function λ →

eλf(x) γ(x)dx is ﬁnite for
all λ, see [29]. An example is f(x) = x. Bounded convergence holds in the Orlicz
class. Assume f ∈Mcosh2(γ) and consider the sequence fN(x) = (|x| ≤N)f(x).
Now,

cosh2(λ(f(x) −fN(x)) γ(x)dx =

|x|≥N
cosh2(λf(x)) γ(x)dz
converges to 0 as N →∞.

Orlicz Spaces in IG
187
3
Calculus of the Gaussian Space
I will review here a few simple facts about the analysis of the Gaussian space,
the so-called Malliavin’s calculus, see [19, Ch. V].
Let us denote by Ck
poly(Rn), k = 0, 1, . . . , the vector space of functions which
are diﬀerentiable up to order k and which are bounded, together with all deriva-
tives, by a polynomial. This class of functions is dense in L2(γ). For each couple
f, g ∈C1
poly (Rn), we have

f(x) ∂ig(x) γ(x) dx =

δif(x) g(x) γ(x) dx ,
where the divergence operator δi is deﬁned by δif(x) = xif(x) −∂if(x). Multi-
dimensional notations will be used, for example,

∇f(x) · ∇g(x) γ(x) dx =

f(x) δ · ∇g(x) γ(x) dx ,
f, g ∈C2
poly (Rn) ,
with δ · ∇g(x) = x · ∇g(x) −Δg(x).
For example, in this notation, the Hyv¨arinen divergence of Eq. (2) with
P = p · γ, Q = q · γ, and p, q ∈C2
poly (Rn), becomes
1
2

|∇log p(x) −∇log q(x)|2 p(x)γ(x)dx ,
while the Otto’s inner product of Eq. (3) becomes, with P = p · γ and f, g, p ∈
C2
poly (Rn), gives

∇f(x) · ∇g(x) p(x) γ(x) dx =

f(x)δ · ∇(g(x)p(x)) γ(x) dx .
Hermite polynomials Hα = δα1 provide an orthogonal basis for L2(γ) such
that ∂iHα = αiHα−ei. Fourier expansion in Hermite polynomials provides proof
of the closure of both operator ∂i and δi on a domain which is a Hilbert subspace
of L2(γ). Moreover, the closure of ∂i is the translation operator’s inﬁnitesimal
generator.
4
Exponential Statistical Bundle
In this section, I will very brieﬂy review and slightly generalize my construction
of the statistical manifold as a Banach manifold modeled on the exponential
Orlicz space Lcosh2 (γ). For a detailed presentation, see [27,29]. The general set-
up is specialized to the Gaussian space.
The support of the manifold is the maximal exponential model E (γ) consist-
ing of probability densities on (Rn, γ) of the form
q = exp (u −K1(u)) ,
u ∈B1 = Lcosh2 (γ) ∩

u


u(x) γ(x)dx = 0

.

188
G. Pistone
The quantity K1(u) = log

eu(x) γ(x)dx is the unique normalising constant
(log- partition function) of eu and is assumed to be ﬁnite. A further restriction
is needed to avoid the border of the set {K1(u) < ∞}. We can easily prove that
the mapping K1 : Lcosh2 (γ) →R is convex and that the topological interior of
its proper domain in non-empty because it contains the open unit ball of B1.
We restrict the model to all u in such domain. The mapping
s1 : E (γ) ∋q →u = log q −Eγ [log q] ∈B1
provides a global chart to the manifold. If we chose to express the velocity of
a curve t →E (γ) by the Fishers score
d
dt log q(t), then the tangent bundle of
the manifold is expressed by the statistical bundle SE (γ) consisting of all the
couples (q, v) such that q is a density of the maximal exponential model and v
is a q-centered random variable in the exponential Orlicz space.
The following statement is crucial to prove consistency in inﬁnite dimensions.
It shows that the statistical bundle ﬁbers are isomorphic as Banach spaces.
For all p, q ∈E (γ) it holds q = eu−Kp(u) ·p, where u ∈L(cosh −1) (γ), Ep [u] =
0, and u belongs to the interior of the proper domain of the convex function Kp.
This property is equivalent to any of the following:
1. p and q are connected by an open exponential arc;
2. Lcosh2 (p) = Lcosh2 (q) and the norms are equivalent;
3. p/q ∈∪a>1La(q) and q/p ∈∪a>1La(p).
See [34,35] for a detailed proof. The following argument is a generalization of
the proof of item 1 ⇒item 2. Let F be logarithmically convex on R and such
that Φ = F −1 is a Young function. For example, the assumption holds for both
F(x) = cosh x and F(x) = ex2/2. For all real A and B, the function
R2 ∋(λ, t) →F(λA)etB = exp (log F(λA) + tB)
is convex and so is
C(λ, t) =

F(λf(x))etu(x) γ(x)dx ,
where f ∈LΦ (γ) with and u ∈Lcosh2 (γ) with

u(x) γ(x)dx = 0. With-
out restriction of generality, assume ∥f∥LΦ(γ) = 1. Let us derive two marginal
inequalities. First, for t = 0, the deﬁnition of Luxemburg norm gives
C(λ, 0) =

F(λf) γ(x)dx ≤2 ,
−1 ≤λ ≤1 .
Second, for λ = 0, consider K1(tu) = log

etu γ(x)dx, where t belongs to an an
open interval I containing [0, 1] and such that K1(tu) < +∞. It follows that
C(0, t) =

etu γ(x)dx = eK1(tu) < +∞.

Orlicz Spaces in IG
189
Choose a t > 1 in I and consider the convex combination
t −1
t
, 1

= t −1
t
(1, 0) + 1
t (0, t)
and the inequality
C
t −1
t
, 1

≤t −1
t
C(1, 0) + 1
t C(0, t) ≤2t −1
t
+ 1
t eK1(tu) .
Now,

Φ
t −1
t
f(x)

eu(x)−K1(u)γ(x)dx
=

F
t −1
t
f(x)

eu(x)−K1(u)γ(x)dx −1
= e−K1(u)C
t −1
t
, 1

−1 ≤e−K1(u)

2t −1
t
+ 1
t eK1(tu)

−1
As the right-hand-side is ﬁnite, we have proved that f ∈LΦ (p) for p =
eu−K1(u). Conversely, a similar argument shows the other implication. We have
proved that all Orlicz spaces LΦ (p), p ∈E (γ) are equal. In turn, equality of
spaces implies the equivalence norms. It is possible to derive explicit bounds by
choosing a t such that the right-hand-side is smaller or equal to 1, see [36].
5
Gaussian Orlicz-Sobolev Spaces
In this ﬁnal section, I plan to extend the exponential statistical bundle’s con-
struction to allow for diﬀerentiable densities in the Gaussian space. Namely, I
suggest a model with sub-exponential random variables of the Gaussian space
whose weak derivatives are sub-Gaussian random variables. The presentation
focusses on functional analytic properties, see [1] and [5] as a general reference.
As a model of the statistical manifold, let us deﬁne the function space
W 1L1,2
cosh2 (γ) =

f ∈L1
cosh2 (γ)
 ∂if ∈L2
cosh2 (γ) , i = 1, . . . , n

,
(8)
where the weak partial derivative ∂jf exists if
⟨∂jf, φ⟩γ = ⟨f, δjφ⟩γ
for all
φ ∈C1
0(Rn) .
The above deﬁnition of weak derivative coincides this the usual deﬁnition of
derivative in the sense of Schwartz distributions because φ ↔φ · γ is a bijection
of C∞
0 (Rn) and
⟨f, δiφ⟩γ = −

f(x) ∂
∂xi
(φ(x)γ(x)) dx .
Weak derivatives do not provide tools for non-linear computations. Hence we
want to recall the relation between translation and weak derivative, the weak

190
G. Pistone
version of Calculus’s fundamental theorem. Let be given a locally integrable real
mapping G ∈L1
loc(R), and assume there exists a locally integrable function G′
which is the weak derivative of G, that is,

G(x)φ′(x) dx = −

G′(x)φ(x) dx ,
φ ∈C∞
0 (R) .
(9)
Deﬁne the translation τhG, h ∈R, by thG(x) = G(x −h), h ∈R. It follows
immediately τhG ∈L1
loc(R) and

(τ−hG(x) −G(x))φ(x) dx =

G(x + h)φ(x) dx −

G(x)φ(x) dx
=

G(x)(φ(x −h) −φ(x)) dx
=

G(x) φ(x −sh)|s=1
s=0 dx
= −h

G(x)
 1
0
φ′(x −sh) ds dx
= −h
 1
0

G(x)φ′(x −sh) dx ds
= h
 1
0

G′(x)φ′(x −sh) dx ds
= h
 1
0

G′(x + sh)φ′(x) dx ds
=
 
h
 1
0
G′(x + sh) ds

φ(x) dx
As φ is any function in C∞
0 (R), we have proved that
τ−hG −G = h
 1
0
τ−shG′ ds = hG′ + h
 1
0
(τ−shG′ −G′) ds
(10)
in L1
loc(R). In particular, if G′ is bounded by a constant K, then G is almost
surely K-Lipschitz, |G(x −h) −G(x)| ≤K |h|.
Conversely, if Eq. (10) holds, then Eq. (9) holds, see, for example,
[5, Lemma 8.1-2].
The argument above extends to n-variate functions f ∈W 1,1
loc (Rn), that is,
f, ∂if ∈L1
loc(Rn), i = 1, . . . , n, by considering, for each h ∈Rn, the univariate
function t →τthf deﬁned by τthf(x) = f(x −th). We obtain
τ−thf −f = t∇f · h + t
 1
0
(τ−sth∇f −∇f) · h ds ,
(11)
where the equality holds in L1
loc(Rn). The same equality holds in all function
space whose elements are locally integrable.

Orlicz Spaces in IG
191
The result about the functional diﬀerentiability of translations is the follow-
ing. For all f ∈W 1L1,2
cosh2 (γ) the following ﬁrst increment equation holds,
(τ−thf −f) −t∇f · h = t
 1
0
(τ−sth∇f −∇f) · h ds ,
and diﬀerentiability holds in L∞−0(γ) = ∩α>1Lα(γ). In fact, the translations
are continuous in all Lα(γ).
It is possible to extend the previous result to a property of the derivative of
the composite function G ◦f. The increment of the composition expands as
G(f(x + th)) −G(f(x)) = G(f(x) + (f(x + th) −f(x))
= (f(x + th) −f(x))G′(f(x))
+ (f(x + th) −f(x))
 1
0
(G′(f(x) + s(f(x + th) −f(x))) −G′(f(x))) ds ,
(12)
and the weak derivative of the composite function exists if G′ is bounded and
f ∈W 1L1,2
cosh2 (γ). However, diﬀerentiability holds in L∞−0(γ).
An interesting example of application is the neuron of a neural network [10,
Ch. 18]. If f1, . . . , fk ∈W 1L1,2
cosh2 (γ), G is an activation function with linear
growth, for example, the linear rectiﬁer G(x) = x+, and ai, wij, bi are given
constants, then
h

i=1
aiG
⎛
⎝
k

j=1
wijfj −bi
⎞
⎠∈W 1L1,2
cosh2 (γ) .
A crucial feature of the space deﬁned in Eq. (8) is the fact that each element
of W 1L1,2
cosh2 (γ) has a continuous version. The embedding
L1
cosh2 (γ) , L2
cosh2 (γ) ⊂∩α≥1Lα(γ)
allows to use the standard Sobolev inequalities to our case. W 1,α
loc (Rn) denotes the
space of functions whose restriction to each open ball Bρ = {x ∈Rn | |x| < ρ} is
α-integrable, together with all weak partial derivatives. Cλ(Bρ) denotes λ-H¨older
functions on the closed ball.
1. The following restriction and embedding hold true and are continuous,
W 1L1,2
cosh2 (γ) →W 1,α(Bρ) ⊂Cλ(Bρ) ,
ρ > 0 ,
0 < λ < 1 .
2. The following inclusions hold true and are continuous:
W 1L1,2
cosh2 (γ) ⊂∩α≥1W 1,α
loc ∩L1
cosh2 (γ) ⊂C(Rn) ∩L1
cosh2 (γ) ,
where the space of continuous functions C(R) is endowed with the uniform
convergence on compact sets.

192
G. Pistone
The embedding are easily veriﬁed. If f ∈L1
cosh −1 (γ), then, for all k ∈N, the
inequalities x2k/(2k)! ≤cosh2(x) and (2π)−n/2e−ρ2/2 ≤γ(x) for x ∈Bρ imply
the inequality
(2π)−n/2e−ρ2/2
(2k)!

Bρ

f(x)
∥f∥L1
cosh2(γ)
2k
dx ≤

cosh2

f(x)
∥f∥L1
cosh2(γ)

γ(x) dx ≤1 ,
so that
∥f∥2k
L2k(Bρ) ≤(2π)n/2(2k)!eρ2/2 ∥f∥L1
cosh2(γ) .
A similar argument applies to the weak partial derivatives. Now we can use the
Sobolev embedding theorem, see [1, Th. 4.12].
Let us conclude by explicitly reviewing the main properties of our space.
1. The space W 1L1,2
cosh2 (γ) contains the constants and all polinomial up to order
2.
2. Each element has a continuous version.
3. If G : R →R is the primitive of a bounded function, then G ◦f ∈
W 1L1,2
cosh2 (γ).
4. min(f, g), max(f, g) ∈W 1L1,2
cosh2 (γ).
Moreover, our space is a Banach space: The mapping
W 1L1,2
Φ (γ) ∋f →∥f∥L1
Φ(γ) +
n

i=1
∥∂if∥L2
Φ(γ)
is a complete norm and thus deﬁnes a Banach space. The argument is a standard
one in Functional Analysis. The weak gradient ∇is a closed operator from
L1
Φ (γ) →
"
L2
Φ (γ)
#n, that is, the graph of ∇is closed in L1
Φ (γ) ×
"
L2
Φ (γ)
#n. In
fact, given a converging sequence in the graph, say fn →f and ∂if →f i, it
holds
⟨∂if, φ⟩γ = ⟨f, δiφ⟩γ = lim
n ⟨fn, δiφ⟩γ = lim
n ⟨∂if, φ⟩γ =
$
f i, φ
%
γ .
The identiﬁcation of the space W 1L1,2
Φ (γ) with the graph of ∇provides a com-
plete norm.
Finally, all exponential Orlicz spaces are isomorphic in a maximal exponential
model. This, in turn implies the isomorphism
W 1L1,2
cosh2 (γ) ↔W 1L1,2
cosh2 (p · γ)
p ∈E (γ) .
It follows that this space is suitable as a model of the ﬁbers of a statistical
bundle.

Orlicz Spaces in IG
193
6
Selected Bibliography
General monograph on IG are [3,4,13]. The type of analytic framework I suggest
to use in IG is that of Banach manifolds as in [15]. A ﬁrst version of this project
has been developed in [8,11,32,33]. In a series of papers [17,26,28,29], we have
explored a version of the non-parametric Information Geometry (IG) for smooth
densities on Rn. Especially, we have considered the IG associated to Orlicz spaces
on the Gaussian space. The analysis of the Gaussian space is discussed, for
example, in [20,24].
This set-up provides a way to construct a statistical manifold modelled on
functional spaces of smooth densities, but other modelling options are in fact
available, for example the global analysis methods of [14] and the approach based
on deformed exponentials [22]. For the two examples involving derivatives, see
[12,17,18,25].
We have worked with a restricted type of Young functions. See the more
general cases in [21, Ch. II] and [1, Ch. VII]. There is a large literature about
sub-exponential random variables, for example, [7,37,38]. Application to IG is
discussed in [36]. The need to control the product of two random variables in
L(cosh −1) (μ) appears, notably, in the study of the covariant derivatives of the
statistical bundle, see [9,11,18,30].
See [27,29] for all details about the construction of the statistical bundle that
are missing here. In particular the isomorphism of ﬁbers is discussed in detail
in [34,35]. For Malliavin’s calculus, see [19,20]. General references about the
functional background used in the construction of the Gaussian Orlicz-Sobolev
space are [1,5].
Acknowledgements. The author acknowledges the support by de Castro Statistics,
Collegio Carlo Alberto, Turin, Italy. He is a member of GNAMPA-INDAM. The author
thanks an anonymous reviewer and L. Malag`o for their helpful comments.
References
1. Adams, R.A., Fournier, J.J.F.: Sobolev Spaces, Pure and Applied Mathematics
(Amsterdam), 2nd edn., vol. 140. Elsevier/Academic Press, Amsterdam (2003)
2. Amari, S.I.: Information geometry and its applications. Appl. Math. Sci. 194
(2016). https://doi.org/10.1007/978-4-431-55978-8
3. Amari, S., Nagaoka, H.: Methods of information geometry. Am. Math. Soc. (2000).
Translated from the 1993 Japanese original by Daishi Harada
4. Ay, N., Jost, J., Lˆe, H.V., Schwachh¨ofer, L.: Information geometry, Ergebnisse
der Mathematik und ihrer Grenzgebiete. 3. Folge. A Series of Modern Surveys in
Mathematics [Results in Mathematics and Related Areas. 3rd Series. A Series of
Modern Surveys in Mathematics], vol. 64. Springer, Cham (2017). https://doi.org/
10.1007/978-3-319-56478-4
5. Brezis, H.: Functional analysis. In: Sobolev Spaces and Partial Diﬀerential Equa-
tions. Universitext. Springer, New York (2011)
6. Brown, L.D.: Fundamentals of statistical exponential families with applications in
statistical decision theory, No. 9 in IMS Lecture Notes. Monograph Series, Institute
of Mathematical Statistics (1986)

194
G. Pistone
7. Buldygin, V.V., Kozachenko, Y.V.: Metric characterization of random variables
and random processes, Translations of Mathematical Monographs, vol. 188. Amer-
ican Mathematical Society, Providence, RI (2000). Translated from the 1998 Rus-
sian original by V. Zaiats
8. Cena, A.: Geometric structures on the non-parametric statistical manifold. Ph.D.
thesis, Universit`a degli Studi di Milano (2002)
9. Chirco, G., Malag`o, L., Pistone, G.: Lagrangian and Hamiltonian mechanics for
probabilities on the statistical manifold (2020). arXiv:2009.09431
10. Efron, B., Hastie, T.: Computer age statistical inference, Institute of Mathematical
Statistics (IMS) Monographs, vol. 5. Cambridge University Press, New York (2016).
https://doi.org/10.1017/CBO9781316576533. Algorithms, evidence, and data sci-
ence
11. Gibilisco, P., Pistone, G.: Connections on non-parametric statistical manifolds by
Orlicz space geometry. IDAQP 1(2), 325–347 (1998)
12. Hyv¨arinen, A.: Estimation of non-normalized statistical models by score matching.
J. Mach. Learn. Res. 6, 695–709 (2005)
13. Kass, R.E., Vos, P.W.: Geometrical foundations of asymptotic inference. Wiley
Series in Probability and Statistics: Probability and Statistics. Wiley, New York
(1997). https://doi.org/10.1002/9781118165980. A Wiley-Interscience Publication
14. Kriegl, A., Michor, P.W.: The convenient setting of global analysis. Math. Surv.
Monographs, vol. 53. American Mathematical Society, Providence, RI (1997).
https://doi.org/10.1090/surv/053.
15. Lang, S.: Diﬀerential and Riemannian Manifolds. In: Graduate Texts in Mathe-
matics, 3rd edn., vol. 160. Springer, New York (1995)
16. Ledoux, M.: The concentration of measure phenomenon. Math. Surv. Monogr. 89
(2001). https://doi.org/10.1090/surv/089
17. Lods, B., Pistone, G.: Information geometry formalism for the spatially homoge-
neous Boltzmann equation. Entropy 17(6), 4323–4363 (2015)
18. Lott, J.: Some geometric calculations on Wasserstein space. Comm. Math.
Phys.277(2), 423–437 (2008). https://doi.org/10.1007/s00220-007-0367-3
19. Malliavin, P.: Integration and Probability. Graduate Texts in Mathematics,
vol. 157. Springer, New York (1995). With the collaboration of H´el´ene Airault,
Leslie Kay and G´erard Letac, Edited and translated from the French by Kay,
With a foreword by Mark Pinsky
20. Malliavin,
P.:
Stochastic
Analysis,
Grundlehren
der
Mathematischen
Wis-
senschaften [Fundamental Principles of Mathematical Sciences], vol. 313. Springer,
Heidelberg (1997)
21. Musielak, J.: Orlicz Spaces and Modular Spaces. Lecture Notes in Mathematics,
vol. 1034. Springer, Heidelberg (1983)
22. Newton, N.J.: Sobolev statistical manifolds and exponential models. In: Geometric
science of information, Lecture Notes in Computer Science, vol. 11712, pp. 443–452.
Springer, Cham (2019)
23. Nielsen, F.: An elementary introduction to information geometry. Entropy 22(10)
(2020). https://doi.org/10.3390/e22101100
24. Nourdin, I., Peccati, G.: Normal approximations with Malliavin calculus. InL Cam-
bridge Tracts in Mathematics, vol. 192. Cambridge University Press, Cambridge
(2012). https://doi.org/10.1017/CBO9781139084659. from Stein’s method to uni-
versality
25. Otto, F.: The geometry of dissipative evolution equations: the porous medium
equation. Comm. Partial Diﬀer. Equ. 26(1-2), 101–174 (2001)

Orlicz Spaces in IG
195
26. Pistone, G.: Examples of the application of nonparametric information geometryto
statistical physics. Entropy 15(10), 4042–4065 (2013). https://doi.org/10.3390/
e15104042
27. Pistone, G.: Nonparametric information geometry. In: Nielsen, F., Barbaresco, F.
(eds.) Geometric science of information, Lecture Notes in Computer Science, First
International Conference, GSI 2013 Paris, France, 28–30 August 2013, Proceedings,
vol. 8085, pp. 5–36. Springer, Heidelberg (2013)
28. Pistone, G.: Translations in the exponential Orlicz space with Gaussian weight. In:
Nielsen, F., Barbaresco, F. (eds.) Geometric Science of Information. Third Inter-
national Conference, GSI 2017, Paris, France, 7–9 November 2017, Proceedings,
LNCS, No. 10589, pp. 569–576. Springer (2017)
29. Pistone, G.: Information geometry of the Gaussian space. In: Information Geom-
etry and Its Applications. Springer Proceedings in Mathematics and Statistics,
vol. 252, pp. 119–155. Springer, Cham (2018)
30. Pistone, G.: Lagrangian function on the ﬁnite state space statistical bundle.
Entropy 20(2), 139 (2018). https://doi.org/10.3390/e20020139
31. Pistone, G.: Information geometry of the probability simplex: a short course. Non-
linear Pheno. Complex Syst. 23(2), 221–242 (2020). arXiv:1911.01876
32. Pistone, G., Rogantin, M.: The exponential statistical manifold: mean parameters,
orthogonality and space transformations. Bernoulli 5(4), 721–760 (1999)
33. Pistone, G., Sempi, C.: An inﬁnite-dimensional geometric structure on the space
of all the probability measures equivalent to a given one. Ann. Statist. 23(5),
1543–1561 (1995)
34. Santacroce, M., Siri, P., Trivellato, B.: New results on mixture andexponential
models by Orlicz spaces. Bernoulli 22(3), 1431–1447(2016). https://doi.org/10.
3150/15-BEJ698
35. Santacroce, M., Siri, P., Trivellato, B.: Exponential models by Orlicz spacesand
applications. J. Appl. Probab. 55(3), 682–700 (2018).https://doi.org/10.1017/jpr.
2018.45
36. Siri, P., Trivellato, B.: Robust concentration inequalities in maximalexponential
models. Statist. Probab. Lett. 170, 109001 (2021). https://doi.org/10.1016/j.spl.
2020.109001
37. Vershynin, R.: High-Dimensional Probability: An Introduction with Applications
in Data Science. Cambridge Series in Statistical and Probabilistic Mathematics,
vol. 47. Cambridge University Press, Cambridge (2018). https://doi.org/10.1017/
9781108231596. With a foreword by Sara van de Geer
38. Wainwright, M.J.: High-dimensional statistics: a non-asymptotic viewpoint. Cam-
bridge Series in Statistical and Probabilistic Mathematics. Cambridge University
Press, Cambridge (2019). https://doi.org/10.1017/9781108627771

Quasiconvex Jensen Divergences and
Quasiconvex Bregman Divergences
Frank Nielsen1(B)
and Ga¨etan Hadjeres2
1 Sony Computer Science Laboratories Inc., Tokyo, Japan
Frank.Nielsen@acm.org
2 Sony Computer Science Laboratories, Paris, France
Gaetan.Hadjeres@sony.com
Abstract. We ﬁrst introduce the class of strictly quasiconvex and
strictly quasiconcave Jensen divergences which are asymmetric distances,
and study some of their properties. We then deﬁne the strictly quasicon-
vex Bregman divergences as the limit case of scaled and skewed quasicon-
vex Jensen divergences, and report a simple closed-form formula which
shows that these divergences are only pseudo-divergences at countably
many inﬂection points of the quasiconvex generators. To remedy this
problem, we propose the δ-averaged quasiconvex Bregman divergences
which integrate the pseudo-divergences over a small neighborhood in
order obtain a proper divergence. The formula of δ-averaged quasicon-
vex Bregman divergences extend even to non-diﬀerentiable strictly qua-
siconvex generators. These quasiconvex Bregman divergences between
distinct elements have the property to always have one orientation ﬁnite
while the reverse orientation is inﬁnite. We show that these quasiconvex
Bregman divergences can also be interpreted as limit cases of general-
ized skewed Jensen divergences with respect to comparative convexity by
using power means. Finally, we illustrate how these quasiconvex Bregman
divergences naturally appear as equivalent divergences for the Kullback-
Leibler divergences between probability densities belonging to a same
parametric family of distributions with nested density supports.
1
Introduction, Motivation, and Contributions
A dissimilarity D(O, O′) is a measure of the deviation of an object O′ from a
reference object O (i.e., DO(O′) := D(O, O′)) which satisﬁes the following two
basic properties:
• Non-negativity property: D(O, O′) ≥0, ∀O, O′
• Property of the indiscernibles: D(O, O′) = 0 if and only if O = O′.
In other words, a dissimilarity D(O, O′) satisﬁes D(O, O′) ≥0 with equal-
ity if and only if O = O′. A pseudo-dissimilarity is a measure of deviation for
which the non-negativity property holds but not necessarily the law of the indis-
cernibles [35]. The objects O and O′ can be vectors, probability distributions,
random variables, strings, graphs, etc. In general, a dissimilarity may not be
c
⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2021
F. Barbaresco and F. Nielsen (Eds.): SPIGL 2020, PROMS 361, pp. 196–218, 2021.
https://doi.org/10.1007/978-3-030-77957-3_11

Quasiconvex Jensen Divergences and Quasiconvex Bregman Divergences
197
symmetric, i.e., we may potentially have D(O, O′) ̸= D(O′, O). In that case, the
dissimilarity is said to be oriented, and we consider the following two reference
orientations of the dissimilarity: the forward ordinary dissimilarity D(O : O′)
and its associated reverse dissimilarity Dr(O : O′) := D(O′ : O). Notice that we
used the ’:’ notation instead of the usual comma delimiter ’,’ between the dis-
similarity arguments to emphasize that the dissimilarity may be asymmetric. In
the literature, a dissimilarity is also commonly called a divergence [3] although
several additional meanings may be associated to this term like a dissimilarity
between probability distributions instead of vectors (e.g., the Kullback-Leibler
divergence [13] in information theory) or like a notion of smoothness (e.g., a
C3 contrast function in information geometry [3]). A dissimilarity may also be
loosely called a distance although this may convey to mathematicians in some
contexts the additional notion of a dissimilarity satisfying the metric axioms
(non-negativity, law of the indiscernibles, symmetry and triangular inequality).
The Bregman divergences [10,11] were introduced in operations research, and
are widely used nowadays in machine learning and information sciences. For a
strictly convex and smooth generator F, called the Bregman generator, we deﬁne
the corresponding Bregman divergence between parameter vectors θ and θ′ as:
BF (θ : θ′) := F(θ) −F(θ′) −(θ −θ′)⊤∇F(θ′).
(1)
Bregman divergences are always ﬁnite when θ and θ′ both belong an open convex
set Θ, and generalize many common distances [5], including the Kullback-Leibler
(KL) divergence and the squared Euclidean and squared Mahalanobis distances.
(Notice that although the Mahalanobis distance is a metric, the squared Maha-
lanobis distance is not a metric.) Furthermore, the KL divergence between two
probability densities belonging to a same exponential family [5,6] amount to a
reverse Bregman divergence between the corresponding parameters when setting
the Bregman generator to be the cumulant function of the exponential family [4].
Moreover, a one-to-one correspondence (bijection) between regular exponential
families [6] and the so-called class of “regular Bregman divergences” was reported
in [5] and used for learning statistical mixtures showing that the expectation-
maximization (EM) algorithm is equivalent to a Bregman clustering algorithm
with soft membership. Bregman divergences have been extended to many non-
vector data types like matrix arguments [36] or functional arguments [17].
In this chapter, we consider deﬁning the notion of Jensen divergences [29]
for strictly quasiconvex or strictly quasiconcave generators, and we investigate
the induced notion of Bregman divergences. We term them quasiconvex Breg-
man divergences (and omit to preﬁx it by ‘strictly’ for sake of brevity). We then
establish a connection between the KL divergence between parametric families
of densities with nested density supports and these quasiconvex Bregman diver-
gences.
We summarize our main contributions as follows:
• By using quasiconvex generators instead of convex generators (i.e., relaxing
convex functions to quasiconvex functions), we deﬁne for a scalar α ∈(0, 1)
the α-skewed quasiconvex Jensen divergences (Deﬁnition 1) and derived

198
F. Nielsen and G. Hadjeres
thereof the quasiconvex Bregman divergences in the limit case of α →1
or the reverse quasiconvex Bregman divergences when α →0 (Deﬁnition 3
and Theorem 1). The quasiconvex Bregman divergences turn out to be only
pseudo-divergences at inﬂection points of the generator. Since this can hap-
pen only at countably many points, we still loosely call them quasiconvex
Bregman divergences. Then we integrate the quasiconvex Bregman (pseudo-)
divergence over a small neighborhood and obtain a δ-averaged quasiconvex
Bregman divergence in Sect. 3.2. The δ-averaged quasiconvex Bregman diver-
gence are also well-deﬁned for strictly quasiconvex but not diﬀerentiable
generators. Quasiconvex Bregman divergences between distinct parameters
always have one orientation ﬁnite while the reverse orientation evaluates to
inﬁnity.
• We show that quasiconvex Jensen divergences and quasiconvex Bregman
divergences can be reinterpreted as generalized Jensen and Bregman diver-
gences with comparative convexity [26,33] using power means in the limit case
(Sect. 2.3 and Sect. 2.3).
• We exhibit some parametric families of probability distributions with strictly
nested density supports such that the Kullback-Leibler divergences between
them amount to equivalent quasiconvex Bregman divergences (Sect. 4).
The paper is organized as follows:
Section 2 deﬁnes the quasiconvex and quasiconcave diﬀerence distances by
analogy to Jensen diﬀerence distances [38] (also called Burbea-Rao diver-
gences [29]), study some of their properties, and show how to obtain them as
generalized Jensen divergences [33] obtained from comparative convexity using
power means. Henceforth their name: quasiconvex Jensen divergences. When the
generator is quasilinear instead of quasiconvex, we call them quasilinear Jensen
divergences (qln Jensen divergences for short). We then deﬁne the quasiconvex
Bregman divergences in Sect. 3 as limit cases of scaled and skewed quasiconvex
Jensen divergences, and report a closed-form formula which highlights the fact
that one orientation of the distance is always ﬁnite while the reverse orientation
is always inﬁnite (for divergences between distinct elements). Since the quasi-
convex Bregman divergences are only pseudo-divergences at inﬂection points,
we deﬁne the δ-averaged quasiconvex Bregman divergences in Sect. 3.2. We also
recover the formula by taking the limit case of power means Bregman divergences
that were introduced using comparative convexity [33].
In Sect. 4, we consider the problem of ﬁnding a parametric family of proba-
bility distributions for which the Kullback-Leibler divergence amount to a qua-
siconvex Bregman divergence. We illustrate one example showing that nested
supports of the densities ensure the property of having one orientation ﬁnite
while the other one is inﬁnite. Finally, Sect. 5 concludes this chapter and hints
at applications perspectives of these quasiconvex Bregman divergences, including
ﬂat center-based clustering [31] and hierarchical clustering [27].

Quasiconvex Jensen Divergences and Quasiconvex Bregman Divergences
199
2
Divergences Based on Inequality Gaps of Quasiconvex
or Quasiconcave Generators
2.1
Quasiconvex and Quasiconcave Diﬀerence Dissimilarities
In this work, a divergence or a distance D(θ : θ′) refers to a dissimilarity such
that D(θ : θ′) ≥0 with equality iﬀ. θ = θ′. A pseudo-divergence or pseudo-
distance only satisﬁes the non-negativity property but not necessarily the law of
the indiscernibles of the dissimilarities.
Consider a function Q : Θ ⊂RD →R which satisﬁes the following “Jensen-
type” inequality [9] for any α ∈(0, 1):
Q((θθ′)α) < max{Q(θ), Q(θ′)},
θ ̸= θ′ ∈Θ ⊂R,
(2)
where (θθ′)α := (1−α)θ+αθ′ denotes the weighted linear interpolation of θ with
θ′, and Θ the parameter space. Function Q is said strictly quasiconvex [8,9,18,37]
as it relaxes the strict convexity inequality:
Q((θθ′)α) < (1 −α)Q(θ) + αQ(θ′) ≤max{Q(θ), Q(θ′)}.
(3)
Let Q denote the space of such strictly quasiconvex real-valued function, and
let C denote the space of strictly convex functions. We have C ⊂Q: Any strictly
convex function or any strictly increasing function is quasiconvex, but not nec-
essarily the converse: Some examples of quasiconvex functions which are not
convex are Q(θ) =
√
θ, Q(θ) = θ3, Q(θ, θ′) = log(θ2 + (θ′)2), etc. Decreas-
ing and then increasing functions are quasiconvex but may not be necessarily
smooth. Some concave functions like Q(θ) = log θ are quasiconvex. The sum
of quasiconvex functions are not necessarily quasiconvex (see also [7]). In the
same spirit that function convexity can be reduced to set convexity via the epi-
graph representation of the function, a function Q is quasiconvex if the level
set Lα := {x : Q(x) ≤α} is (set) convex for all α ∈R. When Q is univari-
ate, a quasiconvex function is also commonly called unimodal (i.e., decreasing
and then increasing function). Thus a multivariate quasiconvex function can
be characterized as being unimodal along each line of its domain. Figure 1 dis-
plays some examples of quasiconvex functions with one function that fails to
be quasiconvex. Notice that strictly monotonic functions which are both strictly
quasiconvex and strictly quasiconcave are termed strictly quasilinear. The ceil
function ceil(θ) = inf{z ∈Z
:
z ≥θ} is an example of quasilinear func-
tion (idem for the ﬂoor function). Another example, are the linear fractional
functions Qa,b,c,d(θ) =
a⊤θ+b
c⊤θ+d which are quasilinear functions on the domain
Θ = {θ
: c⊤θ + d > 0}. We denote by L ⊂Q the set of strictly quasilinear
functions, and by H the set of strictly quasiconcave functions.
Deﬁnition 1 (Quasiconvex diﬀerence distance). The quasiconvex diﬀer-
ence distance (or qcvx distance for short) for α ∈(0, 1) is deﬁned as the inequality
diﬀerence gap of Eq. 2
qcvxJα
Q(θ : θ′) := max{Q(θ), Q(θ′)} −Q((θθ′)α) ≥0,
(4)
= max{Q(θ), Q(θ′)} −Q((1 −α)θ + αθ′)).
(5)

200
F. Nielsen and G. Hadjeres
Fig. 1. The ﬁrst three functions (from left to right) are quasiconvex because any level
set is convex, but the last function is not quasiconvex because the dotted line intersects
the function in four points (and therefore the level set is not convex). The ﬁrst function
is convex, the second function is quasiconvex but not convex (a chord may intersect the
function in more than two points), the third function is monotonous and here concave
(quasilinear)).
By deﬁnition, the quasiconvex diﬀerence distance is a dissimilarity satisfying
qcvxJα
Q(θ : θ′) = 0 iﬀ. θ = θ′ when the generator Q is strictly quasiconvex (see
Eq. 2).
Remark 1. Notice that we could also have deﬁned a log-ratio gap [35] as a dis-
similarity:
qcvxJLα
Q(θ : θ′) := −log

Q((θθ′)α)
max{Q(θ), Q(θ′)}

.
(6)
However, in that case we should have required the extra condition that the
generator does not vanish in the domain, i.e., Q(θ) ̸= 0 for any θ ∈Θ.
Property 1. Let a > 0 and b ∈R, and deﬁne Qa,b(θ) = aQ(θ) + b. Functions
Qa,b are quasiconvex, and qcvxJα
Qa,b(θ : θ′) = a qcvxJα
Q(θ : θ′).
Similarly, we can characterize a strictly quasiconcave real-valued function H ∈
H : Θ ⊂RD →R by the following inequality for α ∈(0, 1):
H((θθ′)α) > min{H(θ), H(θ′)},
θ ̸= θ′ ∈Θ ⊂RD.
(7)
This allows one to deﬁne the quasiconcave diﬀerence distance (or qccv distance
for short):
Deﬁnition 2 (Quasiconcave diﬀerence distance). For H a quasiconcave
function and α ∈(0, 1), we deﬁne the quasiconcave distance as:
qccvJα
H(θ : θ′) := H((θθ′)α) −min{H(θ), H(θ′)},
(8)
= H((1 −α)θ + αθ′) −min{H(θ), H(θ′)}
(9)
Similarly, we have qccvJα
Ha,b(θ : θ′) = a qccvJα
H(θ : θ′) for a > 0 and b ∈R.
Now, observe that for any a, b ∈R, we have1 min{a, b} = −max{−a, −b} (or
equivalently max{a, b} = −min{−a, −b}). Thus it follows the following identity:
1 Indeed, max{a, b} = a+b
2
+ 1
2|b −a| = −( −a−b
2
−1
2|b −a|) = −( −a−b
2
−1
2| −b + a|) =
−min{−a, −b}.

Quasiconvex Jensen Divergences and Quasiconvex Bregman Divergences
201
Property 2. A quasiconcave diﬀerence distance with quasiconcave generator H
is equivalent to a quasiconvex diﬀerence distance for the quasiconvex generator
Q = −H:
qccvJα
H(θ : θ′) = qcvxJα
−H(θ : θ′),
qcvxJα
Q(θ : θ′) = qccvJα
−Q(θ : θ′).
(10)
Proof.
qccvJα
H(θ : θ′) = H((θθ′)α) −min{H(θ), H(θ′)},
(11)
= max{−H(θ), −H(θ′)} −(−H((θθ′)α)),
(12)
= qcvxJα
−H(θ : θ′).
(13)
⊓⊔
Therefore, we consider without loss of generality quasiconvex diﬀerence dis-
tances in the reminder.
2.2
Relationship of Quasiconvex Diﬀerence Distances with Jensen
Diﬀerence Distances
Since for any a, b ∈R, we have max(a, b) = a+b
2
+ 1
2|b −a|, min(a, b) = a+b
2
−
1
2|b −a| and max(a, b) −min(a, b) = |b −a|, we can rewrite Eq. 4 to get
qcvxJα
Q(θ : θ′) = Q(θ) + Q(θ′)
2
+ 1
2
Q(θ) −Q(θ′)
 −Q((θθ′)α),
(14)
= eJα
Q(θ : θ′) + 1
2
Q(θ) −Q(θ′)
 + Q(θ)

α −1
2

+ Q(θ′)
1
2 −α

,
(15)
where
eJα
Q(θ, θ′) := (Q(θ)Q(θ′))α −Q ((θθ′)α) ,
(16)
is called the extended Jensen divergence, a Jensen-type divergence extended to
quasiconvex generators instead of ordinary convex generators.
Property 3 (Upperbounded the extended Jensen divergence by qcvxJα
Q). We have:
eJα
Q(θ : θ′) ≤qcvxJα
Q(θ : θ′)
(17)
since (Q(θ)Q(θ′))α ≤max{Q(θ), Q(θ′)}. In particular, when Q = F is strictly
convex, we have 0 ≤Jα
F (θ : θ′) ≤qcvxJα
F (θ : θ′).
Notice that eJα
Q(θ, θ′) ≥0 when Q is strictly convex, but may be negative
when only quasiconvex. For example, Q(θ) = log θ is a quasiconvex and concave
function, and therefore eJα
Q(θ, θ′) ≤0.

202
F. Nielsen and G. Hadjeres
When α = 1
2, we get the following identity:
Property 4 (Regularization of extended Jensen divergences).
qcvxJQ(θ : θ′) = Q(θ) + Q(θ′)
2
+ 1
2|Q(θ) −Q(θ′)| −Q
θ + θ′
2

,
(18)
= eJQ(θ, θ′) + 1
2|Q(θ) −Q(θ′)|,
(19)
where
eJQ(θ, θ′) := Q(θ) + Q(θ′)
2
−Q
θ + θ′
2

,
(20)
is an extension of the Jensen divergence [12,38] to a quasiconvex generator Q.
Thus when the generator is convex, we can interpret the quasiconvex diver-
gence as a ℓ1-regularization of the ordinary Jensen divergence. When the gen-
erator Q is not convex, beware that eJQ(θ, θ′) may be negative but we always
have eJQ(θ, θ′) ≥−1
2|Q(θ) −Q(θ′)|.
Similarly, when the generator H is strictly quasiconcave, we rewrite the qua-
siconvex diﬀerence distance as
qccvJH(θ : θ′) = H
θ + θ′
2

−H(θ) + H(θ′)
2
+ 1
2|H(θ) −H(θ′)|,
(21)
= eJ−H(θ, θ′) + 1
2|H(θ) −H(θ′)|.
(22)
2.3
Quasiconvex Diﬀerence Distances from the Viewpoint
of Comparative Convexity
In [33], a generalization of the skewed Jensen divergences with respect to compar-
ative convexity [26] is obtained using a pair of weighted means. A mean between
two reals x and y belonging to an interval I ⊂R is a bivariate function M(x, y)
such that
min{x, y} ≤M(x, y) ≤max{x, y}.
(23)
That is, a mean satisﬁes the in-betweenness property (see [26], p. 328). A
weighted mean Mα for α ∈[0, 1] can always be built from a mean by using
the unique dyadic expansions of real numbers [26]. That is, we deﬁne a weighted
mean M(x, y; w) := M(x, y; w, 1 −w) for a weight w ∈[0, 1] as follows: Set
M(x, y; 1, 0) = x, M(x, y; 0, 1) = y, and M(x, y; 1
2, 1
2) := M(x, y). Then we
deﬁne the weighted means M

x, y; 3
4, 1
4

:= M(M(x, y), x) and M

x, y; 1
4, 3
4

:=
M(M(x, y), y), and so on by induction for dyadic numbers with ﬁnite binary rep-
resentations. Thus we can deﬁne the weighted mean M(p, q; w) for w =
i
2k with
i ∈{0, . . . , 2k}. For example, when k = 3, we get the following weighted means:

Quasiconvex Jensen Divergences and Quasiconvex Bregman Divergences
203
M

p, q; 0
8 = 0

= q
M

p, q; 1
8

= M(M(M(p, q), q), q)
M

p, q; 2
8 = 1
4

= M(M(p, q), q)
M

p, q; 3
8

= M(M(M(p, q), p), q)
M

p, q; 4
8 = 1
2

= M(p, q)
M

p, q; 5
8

= M(M(M(p, q), q), p)
M

p, q; 6
8 = 3
4

= M(M(p, q), p)
M

p, q; 7
8

= M(M(M(p, q), p), p)
M

p, q; 8
8 = 1

= p
Let w = ∞
i=1
di
2i be the unique dyadic expansion of the real w ∈(0, 1) where
the di’s are binary digits (i.e., di ∈{0, 1}). Finally, we deﬁne the weighted mean
M(x, y, w, 1 −w) of two positive reals p and q for a real weight w ∈(0, 1) as
M(x, y, w, 1 −w) := lim
n→∞M

x, y,
n

i=1
di
2i , 1 −
n

i=1
di
2i
	
.
(24)
Consider two weighted means Mα and Nα.
A function F is said (M, N) convex if and only if we have
Nα(F(θ), F(θ′)) ≥F(Mα(θ, θ′)),
θ, θ′ ∈Θ.
(25)
We recover the ordinary convexity (Jensen’s midpoint convexity) when Mα =
Nα = Aα, where Aα(x, y) = (1 −α)x + αy is the weighted arithmetic mean.
We can deﬁne the α-skewed (M, N)-Jensen divergence as:
JM,N
F,α (θ : θ′) := Nα(F(θ), F(θ′)) −F(Mα(θ, θ′)).
(26)
By deﬁnition, JM,N
F,α (θ : θ′) ≥0 when F is a (M, N)-strictly convex function.
A quasi-arithmetic mean [26] is deﬁned for a continuous strictly increasing
function f : I ⊂R →J ⊂R as:
Mf(p, q) := f −1
f(p) + f(q)
2

.
(27)

204
F. Nielsen and G. Hadjeres
These quasi-arithmetic means are also called Kolmogorov-Nagumo-de Finetti
means [14,22,25]. Without loss of generality, we assume strictly increasing func-
tions instead of monotonic functions since M−f = Mf. By choosing f(x) = x,
f(x) = log x or f(x) =
1
x, we recover the Pythagorean arithmetic, geo-
metric, and harmonic means, respectively. Choosing fLSE(x) = exp(x) (with
f −1
LSE(x) = log(x)), we get a mean related to the log-sum-exp function LSE [34]:
MfLSE(p, q) = log ep+eq
2
= LSE(p, q) −log 2, where
max{p, q} < LSE(p, q) := log(ep + eq) ≤max{p, q} + log 2.
Now, consider the family of power means for x, y > 0:
P0(x, y) := √xy,
(28)
and
Pδ(x, y) :=
xδ + yδ
2
 1
δ
,
δ ̸= 0.
(29)
These means fall in the class of quasi-arithmetic means obtained for fδ(x) = xδ
for δ ̸= 0 with I = J = (0, ∞), and include in the limit cases the maximum
and minimum values: limδ→+∞Pδ(a, b) = max{a, b} and limδ→−∞Pδ(a, b) =
min{a, b}.
The power mean Jensen divergence [33] is deﬁned as a special case of the
(M, N)-Jensen divergence by:
JPδ
F (θ : θ′) := JA,Pδ
F
(θ : θ′) = Pδ(F(θ), F(θ′)) −F((θθ′)α),
(30)
for a (A, Pδ) strictly convex generator F.
Let us now observe that the quasiconvex diﬀerence distance is a limit case of
power mean Jensen divergences:
Property 5 (qcvxJQas a limit case of power mean Jensen divergences). We have
qcvxJQ(θ : θ′) = lim
δ→∞JPδ
Q (θ : θ′).
(31)
Notice that a strictly quasiconvex function Q is interpreted as a (A, max)-
strictly convex function in comparative convexity, a limit case of (A, Pδ)-
convexity when δ →∞. From now on, we term the quasiconvex diﬀerence
distance the quasiconvex Jensen divergence.
3
Bregman Divergences for Quasiconvex Generators
3.1
Quasiconvex Bregman Divergences as Limit Cases
of Quasiconvex Jensen Divergences
Recall that for a strictly quasiconvex generator Q, deﬁne the α-skewed quasicon-
vex distance for α ∈(0, 1) as
qcvxJα
Q(θ : θ′) := max{Q(θ), Q(θ′)} −Q((θθ′)α).
(32)

Quasiconvex Jensen Divergences and Quasiconvex Bregman Divergences
205
We have
qcvxJα
Q(θ : θ′) ≥0,
(33)
with equality if and only if θ = θ′. Notice that we do not require smoothness [20]
of Q, and qcvxJQ = qcvxJ
1
2
Q is symmetric. For an asymmetric divergence D(θ : θ′),
denote Dr(θ : θ′) = D(θ′ : θ) the reverse divergence.
By analogy to Bregman divergences [5] being interpreted as limit cases of
scaled and skewed Jensen divergences [29,41]:
lim
α→1−
1
α(1 −α)Jα
F (θ : θ′) = BF (θ : θ′),
(34)
lim
α→0+
1
α(1 −α)Jα
F (θ : θ′) = Br
F (θ : θ′) = BF (θ′ : θ).
(35)
Let us deﬁne the following divergence:
Deﬁnition 3 (Quasiconvex Bregman pseudo-divergence). For a strictly
quasiconvex generator Q ∈L, we deﬁne the quasiconvex Bregman pseudo-
divergence as
qcvxBQ(θ : θ′) := lim
α→1−
1
α(1 −α)
qcvxJα
Q(θ : θ′).
(36)
As it will be shown below, we get only a pseudo-divergence in the limit case.
Theorem 1 (Formula
for
the
quasiconvex
Bregman
pseudo-
divergence).
For a strictly quasiconvex and diﬀerentiable generator Q, the
quasiconvex Bregman pseudo-divergence is
qcvxBQ(θ : θ′) =

−(θ −θ′)⊤∇Q(θ′) if Q(θ) ≤Q(θ′)
+∞
otherwise (i.e., Q(θ) > Q(θ′)).
(37)
Proof. By deﬁnition, we have
qcvxBQ(θ : θ′) = lim
α→1−
1
α(1 −α) (max{Q(θ), Q(θ′)} −Q((θθ′)α)) .
Applying a ﬁrst-order Taylor expansion to Q ((θθ′)α), we get
Q ((θθ′)α)) ≃α→1 Q(θ′) −(1 −α)(θ −θ′)⊤∇Q(θ′).
(38)
Thus we have
qcvxBQ(θ : θ′)
= lim
α→1−
1
α(1 −α)

max{Q(θ), Q(θ′)} −Q(θ′) −(1 −α)(θ −θ′)⊤∇Q(θ′)

.(39)

206
F. Nielsen and G. Hadjeres
Consider the following two cases:
• Case max{Q(θ), Q(θ′)} = Q(θ′): That is, Q(θ′) ≥Q(θ). Then it follows that
qcvxBQ(θ : θ′) = lim
α→1−
1
α(1 −α)

−(1 −α)(θ −θ′)⊤∇Q(θ′)

,
(40)
= −(θ −θ′)⊤∇Q(θ′).
(41)
• Case max{Q(θ), Q(θ′)} = Q(θ): That is, Q(θ) ≥Q(θ′). Then we have
qcvxBQ(θ : θ′) = lim
α→1−
1
α(1 −α)

Q(θ) −Q(θ′) −(1 −α)(θ −θ′)⊤∇Q(θ′)

.
We have limα→1−Q(θ) −Q(θ′) −(1 −α)(θ −θ′)⊤∇Q(θ′) = Q(θ) −Q(θ′) =
ΔQ(θ : θ′) that is ﬁnite and diﬀerent from 0 when θ ̸= θ′, and therefore
limα→1−
1
α(1−α)ΔQ(θ : θ′) = +∞.
Let us now prove the axiom of non-negativity and disprove the law of
the indiscernibles at inﬂection points for the quasiconvex Bregman pseudo-
divergences.
Fig. 2. An example of a strictly quasiconvex function Q with (countably) many inﬂec-
tion points (at locations θi’s) for which the derivative vanishes Q′(θi) = 0 and the
second derivative Q′′ changes sign at the θi’s.
• Law of the indiscernibles: Clearly, qcvxBQ(θ : θ) = 0 for all θ ∈Θ. So consider
θ ̸= θ′, and qcvxBQ(θ : θ′) = −∇Q(θ′)⊤(θ −θ′) = 0 for Q(θ′) ≥Q(θ). It is
enough to consider the 1D case, by considering the divergence restricted to
the line passing through θ and θ′ intersected by the domain Θ. We may have
countably many inﬂection points θ′ for which Q′(θ′) = 0. At those inﬂection
points, we may ﬁnd θ ̸= θ′ such that qcvxBQ(θ : θ′) = 0. Thus the quasiconvex
Bregman divergence does not satisfy the law of the indiscernibles. Figure 2

Quasiconvex Jensen Divergences and Quasiconvex Bregman Divergences
207
displays an example of such a quasiconvex function with a few inﬂection
points.
For example, consider the strictly quasiconvex generator Q(x) = x3, with
θ < 0 and θ′ = 0. We have:
qcvxJα
Q(θ : θ′) = max{Q(θ), Q(θ′)} −Q((1 −α)θ + αθ′) = −(1 −α)3θ3 > 0.
(42)
Deﬁning the corresponding quasiconvex Bregman divergence by taking the
limit of scaled quasiconvex Jensen divergence yields
qcvxBQ lim
α→1
1
α(1 −α)
qcvxJα
Q(θ : θ′) = lim
α→1−−(1 −α)2
α
θ3 = 0.
(43)
Thus the quasiconvex Bregman divergence is only a pseudo-divergence at
countably many inﬂection points. Section 3.2 will overcome this problem by
introducing the δ-averaged quasiconvex Bregman divergence.
• Non-negativity follows from a classic theorem of quasiconvex analysis which
reports a ﬁrst-order condition for a function to be quasiconvex2: A C1 func-
tion Q : Θ ⊂RD →R is quasiconvex iﬀ. the following property holds (see
Theorem 21.14 of [39] and §3.4.3 of [9]):
Q(θ′) ≥Q(θ) ⇒∇Q(θ′)(θ −θ′) ≤0.
(44)
That is equivalent to ∇Q(θ′)⊤(θ −θ′) ≤0 or qlnBQ(θ : θ′) = −∇Q(θ′)⊤(θ −
θ′) ≥0.
Notice that when Q = F is strictly convex and diﬀerentiable, then the prop-
erty also follows from the non-negativity of the corresponding Bregman diver-
gence BF (θ : θ′) ≥0 and F(θ′) ≥F(θ):
F(θ) −F(θ′) −(θ −θ′)⊤∇F(θ′) ≥0,
(45)
−(θ −θ′)⊤∇F(θ′)



qcvxBF (θ:θ′)
≥F(θ′) −F(θ) ≥0.
(46)
⊓⊔
Notice that −(θ −θ′)⊤∇Q(θ′) = (θ′ −θ)⊤∇Q(θ′) ≥0 when Q(θ) ≤Q(θ′).
Figure 3 illustrates the quasiconvex Bregman divergence for a strictly quasicon-
vex generator which is strictly concave and has no inﬂection point.
An interesting property is that if qcvxBQ(θ : θ′) < ∞for θ ̸= θ′ then nec-
essarily qcvxBQ(θ′ : θ) = ∞, and vice-versa (when both parameters are not at
inﬂection points). The forward qcvxBQ and reverse qcvxBr
Q quasiconvex Bregman
2 By analogy to a classic second-order condition for a strictly convex and diﬀerentiable
function F to be convex: To have its Hessian ∇2 positive-deﬁnite (Alexandrov’s
theorem). Similarly, the ﬁrst-order condition for convexity of a function states that
a diﬀerentiable function F with convex domain is convex iﬀ. F(θ) ≥F(θ′) + (θ −
θ′)⊤∇F(θ′) from which we recover the Bregman divergence: BF (θ : θ′) = F(θ) −
F(θ′) −(θ −θ′)⊤∇F(θ′) ≥0.

208
F. Nielsen and G. Hadjeres
pseudo-divergences are both ﬁnite only when Q(θ) = Q(θ′) and then we have
qcvxBQ(θ : θ) = 0 or when one parameter is an inﬂection point.
Moreover, we have the following decomposition for a quasiconvex function
Q ∈Q:
eBQ(θ : θ′) = Q(θ) −Q(θ′) + qcvxBQ(θ : θ′),
(47)
when Q(θ) ≤Q(θ′), where eBQ stands for the extended Bregman divergence, i.e.,
the Bregman divergence extended to a quasiconvex generator.
Fig. 3. Illustration of the quasiconvex Bregman divergence for a strictly quasilinear
function Q chosen to be concave (e.g. logarithmic type).
Remark 2 (Separability/non-separability of generators and divergences). When
the D-dimensional generator Q is separable, i.e., Q(θ) = D
i=1 Qi(θi) where θ =
(θ1, . . . , θD) and the Qi’s are diﬀerentiable and quasiconvex univariate functions,
the quasiconvex Bregman divergence rewrites as
qcvxBQ(θ : θ′) =

−D
i=1(θi −θ′
i)Q′
i(θ′
i) if Q(θ) ≤Q(θ′)
+∞
otherwise (Q(θ) > Q(θ′)).
(48)
Notice that the condition for the quasiconvex Bregman divergence to be
inﬁnite is Q(θ) > Q(θ′), and not that there exists one index i ∈{1, . . . , D}
such that Qi(θi) > Q(θ′
i). Thus, we have qcvxBQ(θ : θ′) ̸= D
i=1
qlnBQi(θi : θ′
i).
This is to contrast with Bregman divergences for which the separability of the
generator F(θ) = D
i=1 Fi(θi) yields the separability of the divergence: BF (θ :
θ′) = D
i=1 BFi(θi : θ′
i).

Quasiconvex Jensen Divergences and Quasiconvex Bregman Divergences
209
3.2
The δ-averaged Quasiconvex Bregman Divergence
We shall overcome the problem of indiscernability for quasiconvex Bregman
pseudo-divergences:
qcvxBQ(θ : θ′) = (θ′ −θ)Q(θ′)
for
Q(θ′) ≥Q(θ).
(49)
Since the number of inﬂection points is at most countable for a strictly qua-
siconvex generator Q, we can integrate over a neighborhood of the parameters
to obtain a strictly positive divergence when θ′ ̸= θ.
Given a prescribed parameter δ > 0 (chosen to be small), we introduce the
δ-averaged quasiconvex Bregman divergence qcvxBδ
Q via the following deﬁnition:
qcvxBδ
Q(θ, θ′) := 1
δ
 δ
0
qcvxBQ(θ + u : θ′ + u)du.
(50)
This divergence is inﬁnite if there exists u ∈]0, δ[ such that Q(θ + u) >
Q(θ′ + u). This δ-averaged quasiconvex Bregman divergence now satisﬁes the
law of the indiscernables.
When Q is diﬀerentiable, we obtain:
qcvxBδ
Q(θ, θ′) := 1
δ
 δ
0
(θ′−θ)Q′(θ′+u)du = (θ′−θ)
Q(θ′ + δ) −Q(θ′)
δ

, (51)
when Q(θ′ + u) ≥Q(θ + u)
∀0 ≤u < δ.
Since Q is strictly quasiconvex, we can prove that the condition
Q(θ′ + u) ≥Q(θ + u)
∀0 ≤u < δ
(52)
is in fact equivalent to
Q(θ′) ≥Q(θ)
and
Q(θ′ + δ) ≥Q(θ + δ).
(53)
In 1D, a quasiconvex function is a decreasing then increasing function (i.e., a
unimodal function). It is trivial that Q(θ′) ≥Q(θ) and Q(θ′ + δ) ≥Q(θ + δ)
implies that Q(θ′ + u) ≥Q(θ + u)
∀0 ≤u < δ for monotonic functions. The
only remaining case is when θ′ lies on the decreasing part and θ on the increasing
part. If θ′ +δ also lies on the decreasing part, then since Q is decreasing between
θ′ and θ′ + δ we have
Q(θ′) ≥Q(θ′ + u) ≥Q(θ′ + δ)
∀0 ≤u < δ,
(54)
and, similarly,
Q(θ) ≤Q(θ + u) ≤Q(θ + δ)
∀0 ≤u < δ,
(55)
so that
Q(θ′ + u) ≥Q(θ + u)
∀0 ≤u < δ.
(56)
Finally, if θ′ + δ lies on the increasing part of Q, since θ > θ′, we have
θ′ + δ < θ + δ so Q(θ′ + δ) < Q(θ + δ) which is a contradiction.

210
F. Nielsen and G. Hadjeres
Thus, the condition (52) for the ﬁniteness of the integral (51) can only be
checked at the endpoints (Eq. (53)).
Using the same reasoning as before, we can double check that the rhs. of (51)
is indeed positive if conditions (Eq. (53)) are veriﬁed.
Also, the rhs. of (51) can also serve as the deﬁnition of the qcvxBδ
Q divergences,
even when the strictly quasiconvex function Q is not diﬀerentiable:
Deﬁnition 4 (δ-averaged quasiconvex Bregman divergence). For a pre-
scribed δ > 0 and a strictly quasiconvex generator Q not necessarily diﬀeren-
tiable, the δ-averaged quasiconvex Bregman divergence is deﬁned by
qcvxBδ
Q(θ, θ′)
:=
 1
δ (θ′ −θ) (Q(θ′ + δ) −Q(θ′)) if Q(θ′ + u) ≥Q(θ + u)
∀0 ≤u < δ
+∞
otherwise
(57)
Let us report some examples of δ-averaged quasiconvex Bregman divergences:
• Q(x) = x.
qcvxBQ(θ : θ′) = (θ′ −θ)θ′ + δ −θ′
δ
= θ′ −θ,
when θ′ ≥θ, or +∞otherwise.
• Q(x) = x2.
qcvxBQ(θ : θ′) = (θ′ −θ)(2θ′ + δ),
when |θ′| ≥|θ| and θ′2 −θ2 + 2δ(θ′ −θ) ≥0, or +∞otherwise.
• Q(x) = x3.
qcvxBQ(θ : θ′) = (θ′ −θ)((θ′ + δ)3 + θ′3)
δ
,
when θ′ ≥θ, or +∞otherwise. When θ′ = 0, we now have
qcvxBQ(θ : θ′) = −δ2θ > 0
∀θ < 0.
These examples show that the second condition of (53) is only useful for
non-monotonic functions.
3.3
Multivariate Quasiconvex Generators Q
The construction of the preceding section also applies when Q is multivariate.
We suppose for now that the quasiconvex function Q is diﬀerentiable so that
we have
qcvxBQ(θ : θ′) = (θ′ −θ)⊤∇Q(θ′).
(58)
Let us ﬁx δ > 0, and deﬁne the δ-averaged quasiconvex Bregman divergence
qcvxBδ
Q when Q is multivariate as:
qcvxBδ
Q(θ : θ′) := 1
δ
 δ
0
qcvxBQ (θ + u(θ′ −θ) : θ′ + u(θ′ −θ)) du,
(59)
for θ′ ̸= θ and Q(θ′) ≥Q(θ).

Quasiconvex Jensen Divergences and Quasiconvex Bregman Divergences
211
Using Eq. 58, we obtain:
qcvxBδ
Q(θ : θ′) = 1
δ (Q (θ′ + δ(θ′ −θ)) −Q(θ′)) .
(60)
As this expression does not involves the derivatives of Q, we can use Eq. 60
to deﬁne the δ-averaged quasiconvex Bregman divergence in the case where the
quasiconvex function Q is not diﬀerentiable.
Let us report one example of quasiconvex Bregman divergence for a bivariate
generator: Let Q(θ) = max

θ3
1, θ3
2

.
Q is quasiconvex as it is the maximum of two quasiconvex functions [1]. Let
θ′ = (0, 0). We have ∇Q(θ′) = (0, 0) so that qcvxBQ(θ′ : θ) = 0 for all θ such
that Q(θ) < Q(θ′) = 0.
Now, considering the δ-averaged quasiconvex Bregman divergence, we obtain
qcvxBQ(θ′ : θ) = 1
δ max

(−δθ1)3, (−δθ2)3
= −δ2 min

θ3
1, θ3
2

> 0,
(61)
since max

θ3
1, θ3
2

= Q(θ) < 0 implies that θ1 and θ2 are strictly negative.
3.4
Quasiconvex Bregman Divergences as Limit Cases of Power
Mean Bregman Divergences
For sake of simplicity, consider scalar divergences below. In [33], the (M, N)-
Bregman divergence is deﬁned as the limit case:
BM,N
F
(p : q) = lim
α→1−
1
α(1 −α)JM,N
F,α (p : q),
(62)
= lim
α→1−
1
α(1 −α) (Nα(F(p), F(q))) −F(Mα(p, q))) .
(63)
In particular, the univariate power mean Bregman divergences are obtained
by taking the power means, yielding the following formula:
Bδ1,δ2
F
(p : q) = F δ2(p) −F δ2(q)
δ2F δ2−1(q)
−pδ1 −qδ1
δ1qδ1−1 F ′(q).
(64)
Let δ2 = r and δ1 = 1. Then we get the subfamily of r-power Bregman
divergences:
Br
F (θ : θ′) = F r(θ) −F r(θ′)
rF r−1(θ′)
−(θ −θ′)F ′(θ′),
(65)
= =
F r(θ)
rF r−1(θ′) −F(θ′)
r
−(θ −θ′)F ′(θ′).
(66)
In Eq. 66, when F(θ) > F(θ′) then we have limr→∞Br
F (θ : θ′) = ∞since

F r(θ)
F r−1(θ′)

diverges. Otherwise qlnBF (θ : θ′) = limr→∞Br
F (θ : θ′) = −(θ −
θ′)F ′(θ′) since limr→
F (θ′)
r
= 0 (because |F(θ′)| < ∞).

212
F. Nielsen and G. Hadjeres
When r →∞, the power mean operator Pr tends to the maximum operator:
limr→∞Pr(a, b) = max{a, b}, and the (A, Pδ)-Bregman divergence tends to the
quasiconvex Bregman pseudo-divergence.
3.5
Some Illustrating Examples of Quasiconvex Bregman
Divergences
We concisely report two univariate quasiconvex scalar Bregman divergences:
• For Q(θ) = θ with θ ∈R, we have
qcvxJα
Q(θ : θ′) = max{θ, θ′} −(1 −α)θ + αθ′.
We consider the two cases for calculating the limit
qcvxBQ(θ : θ′) =
limα→1−
1
α(1−α)
qcvxJα
Q(θ : θ′):
– When θ′ ≥θ:
lim
α→1−
1
α(1 −α)
qcvxJα
Q(θ : θ′) =
lim
α→1−
1
α(1 −α) (−(1 −α)θ + (1 −α)θ′) = θ′ −θ ≥0.
– When θ > θ′:
lim
α→1−
1
α(1 −α)
qcvxJα
Q(θ : θ′) = lim
α→1−
1
α(1 −α)(θ −(1 −α)θ −αθ′),
= lim
α→1−
1
1 −α(θ −θ′) = +∞.
Thus we have the following quasiconvex Bregman divergence: qcvxBQ(θ : θ′) =
θ′ −θ for θ′ ≥θ and +∞when θ′ < θ.
• When Q(θ) = log θ, we have Q′(θ) =
1
θ and qcvxBQ(θ : θ′) = 1 −θ
θ′ for
log θ′ ≥log θ (i.e. θ′ ≥θ) and +∞when θ′ < θ.
• For Q(θ) =
√
θ and θ ∈Θ = (0, ∞), we have Q′(θ) =
1
2
√
θ and qcvxBQ(θ :
θ′) = 1
2
√
θ′ −
θ
√
θ′

for
√
θ′ ≥
√
θ (i.e., θ′ ≥θ), and +∞when θ′ < θ.
4
Statistical Divergences, Parametric Families
of Distributions and Equivalent Parameter Divergences
Consider a probability space (X, F, μ) with X, F, and μ denoting the sample
space, the σ-algebra and the positive measure, respectively. The most celebrated
statistical divergence between two densities pθ ≪μ and pθ′ ≪μ absolutely
continuous with respect to a measure μ is the Kullback-Leibler (KL) divergence
(also called relative entropy [13]), deﬁned by:
KL[p : q] =

x∈X p(x) log p(x)
q(x)dμ(x), supp(p) ⊂supp(q),
+∞,
supp(p) ̸⊂supp(q). ,
(67)

Quasiconvex Jensen Divergences and Quasiconvex Bregman Divergences
213
where supp(p) = {x ∈R
:
p(x) > 0} denotes the support of a distribution
p(x), and log 0
0 = 0 by convention. Thus the KL divergence is said unbounded
in general.3
In general, a statistical divergence between densities belonging to the same
parametric family P = {pθ}θ of mutually absolutely continuous densities is
equivalent to a corresponding parameter divergence B:
B(θ : θ′) := D[pθ : pθ′].
(68)
For example, when P = {pθ(x) = exp(x⊤θ −F(θ))dμ(x)}θ is an exponen-
tial family [5,6,24] on a probability space (X, F, μ), then the Kullback-Leibler
divergence between two densities of the exponential family (e.g., two Gaussians
distributions belonging to the Gaussian exponential family) amount to a reverse
Bregman divergence [5] for the Bregman generator set to the cumulant function
F(θ) = log

exp(x⊤θ)dμ(x):
KL[pθ : pθ′] = B(θ : θ′) = BF
r(θ : θ′) = BF (θ′ : θ).
(69)
Banerjee et al. [5] proved a one-to-one correspondence (bijection) between
regular natural exponential families and so-called regular Bregman divergences.
Note that since the Ali-Silvey-Csisz´ar’s f-divergence [2,3] (including the KL
divergence) is invariant to one-to-one smooth mapping m(x) of the sample space
x, the same Bregman divergence equivalent to the KL divergence can be obtained
for diﬀerent exponential families where y = m(x). For example, the KL diver-
gence between two normal distributions or two “equivalent” log-normal distri-
butions is the same (using the mapping y = log x). This can be also noticed by
the matching of their cumulant function: Fnormal(θ) = Flognormal(θ).
Quasiconvex Bregman divergences have the interesting property to be ﬁnite
for one orientation and inﬁnite for the other orientation. Thus to ﬁnd an exam-
ple of parametric family of distributions which the KL divergence amount to
a quasiconvex Bregman divergence, we shall consider parametric distributions
with nested supports (or nested densities), so that one orientation of the KL
divergence will be ﬁnite while the other is will be equal to inﬁnity.
For example, consider the family of univariate uniform densities (D = 1):
pθ(x) = 10<x<eθ e−θ,
(70)
where 1A denotes the indicator function of A. We have supp(pθ′) ⊂supp(pθ) for
0 < θ′ ≤θ. Then we have
KL[pθ : pθ′] =

θ′ −θ = qlnBQ(θ : θ′) 0 < θ ≤θ′,
+∞
θ′ > θ.
,
(71)
for Q(ω) = ω.
Notice that the family P = {pθ} is not an exponential family since the family
has not a ﬁxed support. A truncated exponential family with ﬁxed truncation
3 The Jensen-Shannon divergence [28] is a particular symmetrization of the KL diver-
gence which is always bounded, and may accept densities with diﬀerent supports.

214
F. Nielsen and G. Hadjeres
parameters yields an exponential family which may neither be regular nor steep
(e.g., the singly truncated normal distributions [15]).
Now, consider the parametric family {qθ}θ of nested densities:
qθ(x) = 10<x<eθαxα−1
eθα ,
(72)
for a prescribed α > 1. After a short calculation (or using a computer algebra
system as reported in Sect. 6), we ﬁnd that
KL[qθ : qθ′] =

α(θ′ −θ) = qlnBQ(θ : θ′) θ′ ≥θ > 0,
+∞
θ′ < θ.
,
(73)
for Q(ω) = ω. Thus we have built several parametric families of nested densities
that up to a scaling factor yields the same quasiconvex Bregman divergence.
For parametric densities belonging to the same exponential family, it is known
that the Bhattacharrya distance amount to a Jensen divergence [29]. For an
exponential family pθ(x) = exp(θ⊤x −F(θ))dμ(x) with cumulant function F,
the cross-entropy between two densities [30] is
h(pθ : pθ′) =

−pθ(x) log pθ′(x)dμ(x) = F(θ′) −(θ′)⊤∇F(θ),
(74)
and the entropy is
h(pθ) = h(pθ : pθ) = F(θ) −θ⊤∇F(θ).
(75)
Since KL(pθ : pθ′) = BF (θ′ : θ) = F(θ′) −F(θ) −(θ′ −θ)⊤∇F(θ), when
F(θ′) ≤F(θ), we have −(θ′ −θ)⊤∇F(θ) = qlnBF (θ′ : θ), and it follows that
qlnBF (θ′ : θ) = KL(pθ : pθ′) + F(θ) −F(θ′),
F(θ′) ≤F(θ).
(76)
The Wasserstein distance between two nested univariate distributions has
been studied in [23] with applications to Bayesian statistics to study the inﬂuence
of the prior distribution in the posterior distribution in the ﬁnite sample size
setting.
5
Conclusion and Perspectives
In this chapter, we have introduced two novel families of distortions between vec-
tor parameters: The quasiconvex Jensen divergences and the quasiconvex Breg-
man divergences. We showed that the quasiconvex Jensen divergences measuring
the diﬀerence gaps of the quasiconvex inequalities can be interpreted as a ℓ1-
regularized ordinary Jensen divergence. We noticed that any quasiconcave Jensen
divergence amounts to an equivalent quasiconvex Jensen divergence for the nega-
tive generator. We then derived the quasiconvex Bregman pseudo-divergences as
limit cases of scaled and skewed quasiconvex Jensen divergences for strictly qua-
siconvex generators. The quasiconvex Bregman pseudo-divergences is a pseudo-
divergence only at countably many inﬂection points of the generators. We thus

Quasiconvex Jensen Divergences and Quasiconvex Bregman Divergences
215
propose to deﬁne the δ-averaged quasiconvex Bregman divergences by integrat-
ing the pseudo-divergence over a small neighborhood. This yields a formula
(Eq. 57) that can be used as the deﬁnition of the quasiconvex Bregman diver-
gence even for non-diﬀerentiable strictly quasiconvex generators. We also showed
how to derive again the result of the quasiconvex Bregman pseudo-divergences
using comparative convexity [33] using the limit case of power means. A key
property of the quasiconvex Bregman divergences between distinct elements is
that they are necessarily ﬁnite on one orientation and inﬁnite for the reverse
orientation. Finally, we showed how some of these quasiconvex Bregman diver-
gences can be obtained from the Kullback-Leibler divergence between probability
densities belonging to the same parametric family of distributions with nested
density support. We can retrieve the Bregman pseudo-divergences and quasicon-
vex Bregman pseudo-divergences from ﬁrst-order convexity and quasiconvexity
conditions, as illustrated in Table 1. Additional conditions on the generators
ensure that the pseudo-divergences are proper divergences and satisfy the law of
the indiscernibles (i.e., strict convexity and diﬀerentiability for Bregman diver-
gences and strict quasiconvexity without inﬂection points for the quasiconvex
Bregman divergences).
We plan to consider applications of these novel divergences in clustering:
We note that the generic k-means++ probabilistic seeding analysis reported
in [32] does not apply because of the forward/reverse inﬁnite property of these
quasiconvex Bregman divergences. We may consider discrete k-means, k-center
(with the minimum enclosing ball obtained from quasiconvex programming [1,
16,19,21] when k = 1), and quasiconvex Bregman hierarchical clustering [40].
Table 1. Bregman divergence and Bregman quasidivergence with their relationship to
ﬁrst-order convexity and quasiconvexity.
First-order condition
Pseudo-divergence/condition for divergence
Convexity
F (θ) ≥F (θ′) + (θ −θ′)⊤∇F (θ′)
BF (θ : θ′) = F (θ) −F (θ′) + (θ −θ′)⊤∇F (θ′)
of F
Divergence when F strictly convex and diﬀerentiable
Quasiconvexity
Q(θ) ≤Q(θ′) ⇒(θ −θ′)⊤∇Q(θ′) ≤0

−(θ −θ′)⊤∇Q(θ′) if Q(θ) ≤Q(θ′)
+∞
otherwise.
of Q
Divergence when Q strictly quasiconvex with no inﬂection point
6
Calculations Using a Computer Algebra System
Using the computer algebra system (CAS) Maxima4, we report a snippet code
for the calculation of the Kullback-Leibler divergence for nested probability den-
sities.
4 Freely downloadable at http://maxima.sourceforge.net/.

216
F. Nielsen and G. Hadjeres
assume(alpha>1);
assume(theta>0);
p(x,theta):=alpha*(x**(alpha-1))/(exp(theta*alpha));
integrate(p(x,theta),x,0,exp(theta));
assume(thetap>theta);
/* Kullback-Leibler divergence */
integrate(p(x,theta)*log(p(x,theta)/p(x,thetap)),x,0,exp(theta));
References
1. Agrawal, A., Boyd, S.: Disciplined quasiconvex programming. arXiv preprint
arXiv:1905.00562 (2019)
2. Ali, S.M., Silvey, S.D.: A general class of coeﬃcients of divergence of one distribu-
tion from another. J. Roy. Stat. Soc.: Ser. B (Methodol.) 28(1), 131–142 (1966)
3. Amari, S.: Information Geometry and Its Applications, vol. 194. Springer, Tokyo
(2016)
4. Azoury, K.S., Warmuth, M.K.: Relative loss bounds for on-line density estimation
with the exponential family of distributions. Mach. Learn. 43(3), 211–246 (2001)
5. Banerjee, A., Merugu, S., Dhillon, I.S., Ghosh, J.: Clustering with Bregman diver-
gences. J. Mach. Learn. Res. 6(Oct), 1705–1749 (2005)
6. Barndorﬀ-Nielsen, O.: Information and Exponential Families in Statistical Theory.
Wiley, Hoboken (2014)
7. Baryshnikov, Y., Ghrist, R.: Unimodal category and topological statistics. In: Pro-
ceedings of Nonlinear Theory and Its Applications (NOLTA) (2011)
8. Bereanu, B.: Quasi-convexity, strictly quasi-convexity and pseudo-convexity
of composite objective functions. Revue fran¸caise d’automatique informatique
recherche op´erationnelle. Math´ematique 6(R1), 15–26 (1972)
9. Boyd, S., Vandenberghe, L.: Convex Optimization. Cambridge University Press,
Cambridge (2004)
10. Br`egman, L.M.: Finding the common point of convex sets by the method of suc-
cessive projection. Dokl. Akad. Nauk SSSR 162(3), 487–490 (1965). (in Russian)
11. Bregman, L.M.: The relaxation method of ﬁnding the common point of convex
sets and its application to the solution of problems in convex programming. USSR
Comput. Math. Math. Phys. 7(3), 200–217 (1967)
12. Burbea, J., Rao, C.R.: Entropy diﬀerential metric, distance and divergence mea-
sures in probability spaces: a uniﬁed approach. J. Multivar. Anal. 12(4), 575–596
(1982)
13. Cover, T.M., Thomas, J.A.: Elements of Information Theory. Wiley, Hoboken
(2012)
14. De Finetti, B.: Sul concetto di media. Istituto italiano degli attuari 3, 369–396
(1931)
15. Del Castillo, J.: The singly truncated normal distribution: a non-steep exponential
family. Ann. Inst. Stat. Math. 46(1), 57–66 (1994)
16. Eppstein, D.: Quasiconvex programming. Comb. Comput. Geom. 52(287–331), 3
(2005)
17. Frigyik, B.A., Srivastava, S., Gupta, M.R.: Functional Bregman divergence. In:
2008 IEEE International Symposium on Information Theory, pp. 1681–1685. IEEE
(2008)
18. Greenberg, H.J., Pierskalla, W.P.: A review of quasi-convex functions. Oper. Res.
19(7), 1553–1570 (1971)

Quasiconvex Jensen Divergences and Quasiconvex Bregman Divergences
217
19. Hazan, E., Levy, K., Shalev-Shwartz, S.: Beyond convexity: stochastic quasi-convex
optimization. In: Advances in Neural Information Processing Systems, pp. 1594–
1602 (2015)
20. Iyer, R., Bilmes, J.A.: Submodular-Bregman and the Lov´asz-Bregman divergences
with applications. In: Advances in Neural Information Processing Systems, pp.
2933–2941 (2012)
21. Ke, Q., Kanade, T.: Quasiconvex optimization for robust geometric reconstruction.
IEEE Trans. Pattern Anal. Mach. Intell. 29(10), 1834–1847 (2007)
22. Kolmogorov, A.N.: Sur la notion de moyenne. Acad. Naz. Lincei Mem. Cl. Sci. His.
Mat. Natur. Sez. 12, 388–391 (1930)
23. Ley, C., Reinert, G., Swan, Y., et al.: Distances between nested densities and a
measure of the impact of the prior in Bayesian statistics. Ann. Appl. Probab. 27(1),
216–241 (2017)
24. Miao, W., Hahn, M.: Existence of maximum likelihood estimates for multi-
dimensional exponential families. Scand. J. Stat. 24(3), 371–386 (1997)
25. Nagumo, M.: ¨Uber eine Klasse der Mittelwerte. Jpn. J. Math. Trans. Abs. 7, 71–79
(1930)
26. Niculescu, C.P., Persson, L.E.: Convex Functions and Their Applications: A Con-
temporary Approach, 2nd edn. Springer, Cham (2018)
27. Nielsen, F.: Hierarchical clustering. In: Introduction to HPC with MPI for Data
Science, pp. 195–211. Springer, Cham (2016)
28. Nielsen, F.: On the Jensen-Shannon symmetrization of distances relying on
abstract means. Entropy 21(5), 485 (2019)
29. Nielsen, F., Boltz, S.: The Burbea-Rao and Bhattacharyya centroids. IEEE Trans.
Inf. Theory 57(8), 5455–5466 (2011)
30. Nielsen, F., Nock, R.: Entropies and cross-entropies of exponential families. In:
2010 IEEE International Conference on Image Processing, pp. 3621–3624. IEEE
(2010)
31. Nielsen, F., Nock, R.: Further heuristics for k-means: the merge-and-split heuristic
and the (k, l)-means. arXiv:1406.6314 (2014)
32. Nielsen, F., Nock, R.: Total Jensen divergences: deﬁnition, properties and clus-
tering. In: 2015 IEEE International Conference on Acoustics, Speech and Signal
Processing (ICASSP), pp. 2016–2020. IEEE (2015)
33. Nielsen, F., Nock, R.: Generalizing skew Jensen divergences and Bregman diver-
gences with comparative convexity. IEEE Signal Process. Lett. 24(8), 1123–1127
(2017)
34. Nielsen, F., Sun, K.: Guaranteed bounds on information-theoretic measures of
univariate mixtures using piecewise log-sum-exp inequalities. Entropy 18(12), 442
(2016)
35. Nielsen, F., Sun, K., Marchand-Maillet, S.: On H¨older projective divergences.
Entropy 19(3), 122 (2017)
36. Nock, R., Magdalou, B., Briys, E., Nielsen, F.: Mining matrix data with Bregman
matrix divergences for portfolio selection. In: Matrix Information Geometry, pp.
373–402. Springer, Heidelberg (2013)
37. Penot, J.P.: Glimpses upon quasiconvex analysis. In: ESAIM Proceedings, vol. 20,
pp. 170–194. EDP Sciences (2007)
38. Rao, C., Nayak, T.: Cross entropy, dissimilarity measures, and characterizations
of quadratic entropy. IEEE Trans. Inf. Theory 31(5), 589–593 (1985)
39. Simon, C.P., Blume, L., et al.: Mathematics for Economists, vol. 7. Norton, New
York (1994)

218
F. Nielsen and G. Hadjeres
40. Telgarsky, M., Dasgupta, S.: Agglomerative Bregman clustering. In: Proceedings of
the 29th International Conference on International Conference on Machine Learn-
ing, pp. 1011–1018. Omnipress (2012)
41. Zhang, J.: Divergence function, duality, and convex analysis. Neural Comput.
16(1), 159–195 (2004)

Part IV: Geometric Structures
of Mechanics, Thermodynamics
and Inference for Learning

Dirac Structures and Variational
Formulation of Thermodynamics
for Open Systems
Hiroaki Yoshimura1(B) and Fran¸cois Gay-Balmaz2
1 Waseda University, 3-4-1, Ohkubo Shinjuku, Tokyo 169-8555, Japan
yoshimura@waseda.jp
2 CNRS, LMD, IPSL, Ecole Normale Sup´erieure, 24 Rue Lhomond,
75005 Paris, France
francois.gay-balmaz@lmd.ens.fr
Abstract. In this paper, we make a review of our recent development of
Dirac structures and the associated variational formulation for nonequi-
librium thermodynamics (see, [15,16]). We speciﬁcally focus on the case
of simple and open systems, in which the thermodynamic state is repre-
sented by one single entropy and the transfer of matter and heat with
the exterior is included. We clarify the geometric structure by introduc-
ing an induced Dirac structure on the covariant Pontryagin bundle and
then develop the associated dynamical system (the port-Dirac systems)
in the context of time-dependent nonholonomic systems with nonlin-
ear constraints of thermodynamic type. We also present the variational
structure associated with the Dirac formulation in the context of the
generalized Lagrange-d’Alembert-Pontryagin principle.
1
Fundamentals of Open Systems
In this section, we brieﬂy present the fundamental setting for open thermody-
namic systems. We focus on the case of simple systems in the sense of Stueckel-
berg, see [26].
1.1
Stueckelberg’s Formulation of Nonequilibrium Thermodynamics
In order to give a macroscopically dynamic theory to account for irreversible
processes, we use the phenomenological approach to nonequilibrium thermo-
dynamics developed by Stueckelberg [26]. This approach enables us to treat
nonequilibrium thermodynamics as a natural extension of classical mechanics.
We emphasize that this is a pure macroscopic and phenomenological approach
deduced from the axiomatic formulation of the two laws, which does not aim to
derive the equations for the open systems in terms of microscopic arguments.
We refer to e.g. [8,20,21] for the systematic use of Stueckelberg’s formulation
for several examples of closed thermodynamic systems.
c
⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2021
F. Barbaresco and F. Nielsen (Eds.): SPIGL 2020, PROMS 361, pp. 221–246, 2021.
https://doi.org/10.1007/978-3-030-77957-3_12

222
H. Yoshimura and F. Gay-Balmaz
Let us denote by Σ a physical system and by Σext its exterior. Stueckelberg’s
axiomatic formulation of the two laws are given as follows.
First Law: For every system Σ, there exists an extensive scalar state function
E, called energy, which satisﬁes
d
dtE(t) = P ext
W (t) + P ext
H (t) + P ext
M (t),
(1)
where t denotes time, P ext
W
is the power associated with the work done on the
system, P ext
H
is the power associated to the heat transfer into the system, and
P ext
M
is the power associated with the matter transfer into the system.
Deﬁnition 1. For a given thermodynamic system, we employ the following
terminology:
• A system is said to be closed, if there is no exchange of matter, i.e.,
P ext
M (t) = 0. If P ext
M (t) ̸= 0, the system is said to be open.
• A system is said to be adiabatically closed, if it is closed and there is no
heat exchanges, i.e., P ext
M (t) = P ext
H (t) = 0.
• A system is said to be isolated, if it is adiabatically closed and there is no
mechanical power exchange, i.e., P ext
M (t) = P ext
H (t) = P ext
W (t) = 0.
From the ﬁrst law, it follows that the energy of an isolated system is preserved.
Second Law: For every system Σ, there exists an extensive scalar state function
S, called entropy, which obeys the following two conditions.
(a) Evolution part: If the system is adiabatically closed, the entropy S is a
non-decreasing function with respect to time, i.e.,
d
dtS(t) = I(t) ≥0,
where I(t) is the entropy production rate of the system accounting for the
irreversibility of internal processes.
(b) Equilibrium part: If the system is isolated, as time tends to inﬁnity the
entropy tends towards a ﬁnite local maximum of the function S over all the
thermodynamic states ρ compatible with the system, i.e.,
lim
t→+∞S(t) =
max
ρ compatible S[ρ].
By deﬁnition, the evolution of an isolated system is reversible if I(t) = 0,
namely, the entropy is constant. In general, the evolution of a system Σ is said to
be reversible, if the evolution of the total isolated system with which Σ interacts
is reversible.

Thermodynamics of Open Systems
223
1.2
An Illustrative Example of Open Systems
We present here the fundamental setting for nonequilibrium thermodynamics of
open systems with the help of an illustrative example. We recall that a thermo-
dynamic system is called simple when one (scalar) thermal variable and a ﬁnite
set of non-thermal variables are suﬃcient to describe entirely the state of the
system. From the second law we can always choose the thermal variable to be the
entropy. Recall also that the system is called open if it has a non-vanishing power
exchange due to matter transfer with exterior. In this paper, we speciﬁcally focus
on the case of simple and open systems.
A
Piston-Cylinder
System. Consider a single piston-cylinder system
described in Fig. 1, where the cylinder contains an ideal gas and there exists
a power exchange due to matter and heat transfer with the exterior, as well as a
mechanical power exchange with the exterior by the piston. In addition, there is
an internal irreversible process associated with the collision of the ﬂuid particles
on the piston, macroscopically described by a friction force, see [20].
Fig. 1. An example of a simple and open system: a piston with exterior ports and heat
sources.
For such an open system, we can choose one entropy variable S ∈R as the
thermodynamic variable and N ∈R as the number of moles of the chemical
species in the compartment, while we denote the mechanical variable by q ∈Q,
where Q is given by R. Let U(q, S, N) be the internal energy associated with the
ideal gas contained inside of the cylinder. Let F ext be the external force acting
on the system via the piston and let F fr be the friction force appearing in the
irreversible process associated with the collision of the ﬂuid particles.
Assume that the system has A ports, through which the matter ﬂows out or
into the system and also that there are B ports associated with heat sources.
For the variables associated with these ports, we denote the chemical potential,
temperature, and molar ﬂow rate at the a-th port by μa, T a, and J a (a =
1, ..., A), respectively, and we denote the temperature and entropy ﬂow rate at
the b-th heat source by ¯T b and
¯
J b
S (b = 1, ..., B), respectively. It is common
that the thermodynamic quantities at the ports are given by the pressure and
the temperature Pa, T a, from which the other thermodynamic quantities such

224
H. Yoshimura and F. Gay-Balmaz
as μa = μa(Pa, T a) or Sa = Sa(Pa, T a) can be computed by the state equations
of the gas; see, for instance, [22]. Notice that these thermodynamic quantities at
the ports are time-dependent in general.
Mass Balance Equation. For such an open system with A ports with a single
chemical component, the mole balance equation is written as
d
dtN =
A

a=1
J a,
(2)
where J a is the molar ﬂow rate into the system through the a-th port, so that
J a > 0 for ﬂow into the system and J a < 0 for ﬂow out of the system.
Energy Balance Equation. As matter enters or leaves the system, it may
carry its internal, potential as well as kinetic energy. The energy ﬂow rate at the
a-th port is given by the product UaJ a, where Ua and J a respectively denote
the energy per mole (or molar energy) and the molar ﬂow rate at the a-th port.
Further, as matter enters or leaves the system, it may also exert work on the
system that is associated with pushing the species into or out of the system.
Hence the power exchange due to the mass transfer is given by
P ext
M =
A

a=1
J a(Ua + PaVa) =
A

a=1
(J aμa + J a
S T a).
Here Pa and Va are the pressure and the molar volume of the substance ﬂowing
through the a-th port. Furthermore, J a
S := SaJ a is the entropy ﬂow rate with
Sa the molar entropy at the a-th port and we have used the relation Ha =
Ua +PaVa = μa +T aSa, where Ha indicates the molar enthalpy at the a-th port.
The heat power exchange with exterior is given by
P ext
H
=
B

b=1
¯
J b
S ¯T b,
where
¯
J b
S and ¯T b respectively denote the entropy ﬂow rate and the temperature
at the b-th port. The mechanical power associated with the external force F ext
is given by
P ext
W
= ⟨F ext, ˙q⟩.
Thus, the ﬁrst law for open system, i.e., the energy balance equation reads as
dE
dt = ⟨F ext, ˙q⟩+
A

a=1
(J aμa + J a
S T a) +
B

b=1
¯
J b
S ¯T b,
(3)
where E is the total energy of the system.
Entropy Balance Equation. Regarding the second law for open systems, we
have the entropy balance equation, namely, the rate of total entropy of the
system, given by

Thermodynamics of Open Systems
225
˙S = I +
A

a=1
J a
S +
B

b=1
¯
J b
S,
(4)
where I is the rate of internal entropy production given by
I = −1
T ⟨F fr, ˙q⟩
B

b=1



mechanical friction
+ 1
T
A

a=1

J a
μa −μ
	
+ J a
S

T a −T
	



mixing of matter ﬂowing into the system
+ 1
T
B

b=1
¯
J b
S

¯T b −T
	



heating
,
(5)
with T := ∂U
∂S the temperature and μ := ∂U
∂N the chemical potential. The second
law imposes that the rate of internal entropy production I is always positive
whereas the sign of the rate of entropy ﬂow into the system due to the matter
exchange through A ports and to the heat exchange through B ports (the second
and third terms on the right-hand side of (4)) is arbitrary.
Equation of Motion for the Piston-Cylinder System. The Lagrangian of
the system is given by L(q, ˙q, S, N) = 1
2m ˙q2 −U(q, S, N). We note that if we
were applying the conventional Lagrange-d’Alembert principle
δ
 t2
t1
L(q, ˙q, S, N)dt +
 t2
t1
⟨F fr + F ext, δq⟩dt = 0,
(6)
for all δq with the ﬁxed endpoint conditions, we would get the following equation
d
dt
∂L
∂˙q −∂L
∂q = F fr + F ext.
(7)
In the Lagrange-d’Alembert principle (6) the friction force is treated as if it were
external similarly to F ext. However, the friction force is an essentially internal
force and the power associated with the friction force does not appear on the
right-hand side in the ﬁrst law (3). It appears as a part of the internal entropy
production rate given in (5), in conjunction with the second law (4). Thus, it is
clear that there is a crucial ﬂaw in the variational formulation (6) which uses the
conventional Lagrange-d’Alembert principle. In addition this principle cannot
provide all the required equations for nonequilibrium thermodynamics, namely,
the mass balance equation (2), the entropy balance equation (4), the energy
balance equation (3) as well as the equation of motion (7). In the next section,
we present a general variational formulation for nonequilibrium thermodynamics
of open systems that enables to formulate all of the required evolution equations.
2
A Variational Formulation for Open Systems
Before going into details on the variational formulation, we ﬁrst present
the fundamental setting for the Lagrangian formulation of nonequilibrium

226
H. Yoshimura and F. Gay-Balmaz
thermodynamics for a simple and open system. Our variational formulation
is a generalized Lagrange-d’Alembert principle, which is an extension of the
Hamilton principle of time-dependent nonholonomic systems with nonlinear con-
straints, see [10–12,14]. This variational formulation is useful as a modelling tool,
see [7,9], and also useful to derive the main bracket formalisms in nonequilibrium
thermodynamics, see [6,17].
2.1
Fundamental Setting for Open Nonequilibrium
Thermodynamics
Time-Dependent Lagrangians. As we mentioned, an open system is in gen-
eral explicitly time-dependent. Hence the state variables that are required to
describe the whole system are (t, q, vq, S, N) ∈R × TQ × R × R, where the time
t is explicitly included and (q, vq) ∈TQ indicate the state variables associated
with the mechanical part of the system, where TQ denotes the tangent bundle
of an n-dimensional conﬁguration manifold Q.
Here we consider the variational formulation for simple and open thermody-
namic systems by following [12]. Let L be a time-dependent Lagrangian deﬁned
on the extended state space, namely,
L : R × TQ × R × R →R,
(t, q, vq, S, N) →L(t, q, vq, S, N),
which is usually given by the kinetic energy minus the internal energy of the
system. For the open system of Fig. 1, the Lagrangian is simply given by L =
1
2mv2
q −U(q, S, N), which is not time-dependent, however we shall here consider
a more general case in which L is given as a time-dependent function. Recall that
the system is assumed to be subject to an external force F ext : R×TQ×R×R →
T ∗Q and a friction force F fr : R × TQ × R × R →T ∗Q. Recall also that the
system has A ports, through which species can ﬂow into or out of the system
and B ports associated with heat sources.
Thermodynamic Displacements. An essential ingredient for our variational
formulation of thermodynamics is the concept of thermodynamic displacements
(see [10,11,14]). By deﬁnition, the thermodynamic displacement associated with
an irreversible process is given by the primitive in time of the thermodynamic
force (or aﬃnity) of the process. For the process of heat transfer, such an aﬃnity
is given by the temperature T, and hence the thermodynamic displacement is
deﬁned as a variable Γ such that ˙Γ = T. The variable Γ is known as a ther-
mal displacement. For the process of mass transfer, the aﬃnity is the chemical
potential μ, and hence the thermodynamic displacement is deﬁned as a variable
W such that ˙W = μ. Here, by deﬁnition, we recall that the chemical potential
and temperature are respectively given by μ := −∂L
∂N and T := −∂L
∂S . In addition
to these thermodynamic displacements, our variational formulation also involves
the variable Σ, with entropy units, which is deﬁned by the primitive in time of
the rate of internal entropy production of the system. Note that Σ is distinct
from S for open systems.

Thermodynamics of Open Systems
227
2.2
A Lagrangian Variational Formulation for Open Systems
In the context of the fundamental setting mentioned above, we have the following
theorem giving the Lagrangian variational formulation for open systems:
Theorem 1. Suppose
that
q(t), S(t), Γ(t), Σ(t), W(t), N(t)
are
critical
curves for the variational condition
δ
 t2
t1

L(t, q, ˙q, S, N) + ˙WN + ˙Γ(S −Σ)

dt +
 t2
t1
⟨F ext, δq⟩dt = 0,
(8)
subject to the kinematic (phenomenological) constraint
∂L
∂S
˙Σ = ⟨F fr, ˙q⟩+
A

a=1

J a( ˙W −μa)+J a
S ( ˙Γ−T a)

+
B

b=1
¯
J b
S( ˙Γ−¯T b), (9)
and for variations subject to the variational constraint
∂L
∂S δΣ = ⟨F fr, δq⟩+
A

a=1

J aδW + J a
S δΓ

+
B

b=1
¯
J b
SδΓ,
(10)
with δq(t1) = δq(t2) = 0, δW(t1) = δW(t2) = 0, and δΓ(t1) = δΓ(t2) = 0.
Then
q(t), S(t), N(t)
are
solutions
of
the
generalized
Lagrange-
d’Alembert equations
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎩
d
dt
∂L
∂˙q −∂L
∂q = F fr + F ext,
d
dtN =
A

a=1
J a,
∂L
∂S

˙S −
A

a=1
J a
S −
B

b=1
¯
J b
S
	
= ⟨F fr, ˙q⟩
−
A

a=1

J a
∂L
∂N + μa	
+ J a
S

∂L
∂S + T a	
−
B

b=1
¯
J b
S

∂L
∂S + ¯T b	
,
(11)
and Γ(t), W(t), Σ(t) satisfy
˙Γ = −∂L
∂S ,
˙W = −∂L
∂N ,
˙Σ = ˙S −
A

a=1
J a
S −
B

b=1
¯
J b
S.
(12)
Proof. This theorem is veriﬁed by a direct computation. By taking variations
of the action integral in (8), integrating by parts, using δq(t1) = δ(t2) = 0,
δW(t1) = δW(t2) = 0, δΓ(t1) = δΓ(t2) = 0, together with the variational
constraint (10), we obtain the system of evolution equations (11) for the curves
q(t), S(t), N(t), together with the conditions (12) which imposes Γ(t) and W(t)
to be the thermodynamic displacements and Σ(t) to be the primitive in time of
the rate of internal entropy production of the system.
⊓⊔

228
H. Yoshimura and F. Gay-Balmaz
We can recover all the relations developed in Sect. 1.2. Since the total energy
is given from the given Lagrangian as E(t, q, ˙q, S, N) = ⟨∂L
∂˙q , ˙q⟩−L, the energy
balance for this system is computed as
d
dtE(t, q, ˙q, S, N) = −∂L
∂t +

F ext, ˙q




=P ext
W
+
A

a=1
(J aμa + J a
S T a)



=P ext
M
+
B

b=1
¯
J b
S ¯T b



=P ext
H
.
When the given Lagrangian does not explicitly depend on time t, the ﬁrst law
for the open system is recovered as in (3). Furthermore, it follows from the last
equation in (11) that we can recover the entropy balance equation (4), where we
get the rate of internal entropy production as I = ˙Σ in view of ˙W = −∂L
∂N = μ
and ˙Γ = −∂L
∂S = T.
Remark 1 (Relation between the kinematic and variational constraints). Note
that the variational constraint (10) follows from the kinematic (phenomenologi-
cal) constraint (9) by formally replacing the time derivatives ˙Σ, ˙q, ˙W, ˙Γ by the
corresponding virtual displacements δΣ, δq, δW, δΓ, while the time-dependent
terms that depend uniquely on the exterior, namely, the terms J aμa, J a
S T a,
and
¯
J b
S ¯T b have to be removed in the variational constraint. In the following
Sect. 3 concerning the Dirac formulation, we will clarify how this diﬃculty
can be naturally solved in terms of the geometric setting of time-dependent
mechanics.
3
Dirac Formulation for Time-Dependent Nonholonomic
Systems of Thermodynamic Type
To develop the Dirac formulation for open systems in which there is a power
exchange of matter between the system and the exterior, there are two distinct
levels of essential diﬃculties in the treatment of the nonlinear constraints of
thermodynamic type. First, the constraint becomes explicitly time-dependent.
Second, the link between the kinematic (phenomenological) and variational con-
straints appearing in (8) and (9) is broken by additional terms that only depend
on the exterior of the system. It is remarkable that both diﬃculties can be simul-
taneously solved by using the geometric setting of ﬁeld theories as it applies to
the case of time-dependent mechanics. In particular, the covariant Pontryagin
bundle and the covariant generalized energy have to be employed instead of the
Pontryagin bundle and the generalized energy in mechanics. In this setting, we
show that the time-dependent nonlinear constraint associated with the entropy
production in the open thermodynamic system can be naturally recovered from
a Dirac dynamical system (also called a port-Dirac system, see [16]), in which a
Dirac structure induced from a linear distribution on the covariant Pontryagin
bundle is employed.

Thermodynamics of Open Systems
229
3.1
Time-Dependent Constraints of Thermodynamic Type
In this subsection, we ﬁrst introduce an extended conﬁguration manifold needed
to treat the time-dependence in the kinematic and variational constraints. Then,
we construct a systematic treatment of the kinematic and variational constraints
for the class of time-dependent nonholonomic constraints of thermodynamic type
which extends treatment of those constraints in isolated systems, see [13].
Extended Conﬁguration Manifolds. Given a conﬁguration manifold Q, we
deﬁne the extended conﬁguration manifold as
Y := R × Q ∋(t, x),
which can be regarded as a trivial ﬁber bundle over R, namely, Y = R×Q →R,
(t, x) →t. Note that this is known as the geometric setting of time-dependent
mechanics in the context of classical ﬁeld theories, where Y = R × Q →R is
the conﬁguration bundle in the ﬁeld theory (see [18]).
Time-Dependent Variational and Kinematic Constraints. Consider the
vector bundle (R × TQ) ×Y TY →Y over Y whose ﬁber at y = (t, x) ∈Y is
given by TxQ × T(t,x)Y = TxQ × (R × TxQ). An element in the ﬁber at each
y = (t, x) is denoted by (v, δt, δx). A variational constraint is a subset
CV ⊂(R × TQ) ×Y TY ,
such that the set CV (t, x, v), deﬁned by
CV (t, x, v) := CV ∩

{(t, x, v)} × T(t,x)Y

,
for all (t, x, v) ∈R×TQ, is a vector subspace of T(t,x)Y . A kinematic constraint
is given by a submanifold of TY as
CK ⊂TY .
In general, CK and CV are independent from each other.
Nonlinear Nonholonomic Constraints of Thermodynamic Type. For
the case in which the given constraints CV and CK have no speciﬁc relation
between them, one can develop the equations of motion by a variational for-
mulation based on the generalized Lagrange-d’Alembert principle as in [2], while
we cannot establish a formulation with Dirac structures in general, because of
the existence of nonlinearity in the constraints. However, for the case of non-
linear nonholonomic constraints of thermodynamic type that typically appear in
thermodynamics, a Dirac structure formulation can be developed by using the
speciﬁc relation between CV and CK described below. We refer to [13] for the
case of isolated systems.
Deﬁnition 2. A variational constraint CV ⊂(R×TQ)×Y TY and a kinematic
constraint CK ⊂TY are called nonlinear constraints of thermodynamic type if
CK is deﬁned from CV as
CK =

(t, x, ˙t, ˙x) ∈TY | (t, x, ˙t, ˙x) ∈CV (t, x, ˙x)

⊂TY .
(13)

230
H. Yoshimura and F. Gay-Balmaz
In local coordinates, if the variational constraint CV is given by
CV =

(t, x, v, δt, δx) ∈(R × TQ) ×Y TY |
n

i=1
Ar
i (t, x, v)δxi + Br(t, x, v)δt = 0, r = 1, ..., m

(14)
for functions Ar : R × TQ →T ∗Q and Br : R × TQ →R, r = 1, ..., m, then the
associated kinematic constraint CK deﬁned in (13) reads
CK =

(t, x, ˙t, ˙x) ∈TY |
n

i=1
Ar
i (t, x, ˙x) ˙xi + Br(t, x, ˙x)˙t = 0, r = 1, ..., m

.
3.2
Dirac Structures on Covariant Pontryagin Bundles
In this subsection, we ﬁrst introduce the covariant Pontryagin bundle by using
the geometric setting of classical ﬁeld theory and then we develop a time-
dependent Dirac structure on the covariant Pontryagin bundle that is induced
from the distribution CV and a presymplectic form.
The Covariant Pontryagin Bundle. The construction of the geometric set-
ting for time-dependent mechanics is based on that of ﬁeld theory, see [18,27]. In
this case, the conﬁguration bundle is the trivial bundle Y = R × Q →X = R.
We have the following canonical identiﬁcations:
J1Y ∼= R × TQ,
J1Y ⋆∼= T ∗Y = T ∗(R × Q),
ΠY ∼= R × T ∗Q,
with J1Y
→Y the ﬁrst jet bundle, J∗Y
→Y the dual jet bundle, and
ΠY →Y the restricted dual jet bundle associated to Y →X . By analogy with
the Pontryagin bundle in time-independent mechanics, we deﬁne the covariant
Pontryagin bundle over Y = R × Q as
π(P ,Y ) : P = J1Y ×Y J1Y ⋆→Y = R × Q,
whose ﬁber at y = (t, x) ∈Y is denoted by (v, p, p). Therefore, for the covariant
Pontryagin bundle over Y = R×Q, we have the identiﬁcation J1Y ×Y J1Y ⋆∼=
(R × TQ) ×Y T ∗Y .
Induced Distributions on the Covariant Pontryagin Bundle. From the
given variational constraint CV ⊂J1Y ×Y TY , the distribution ΔP on the
covariant Pontryagin bundle is deﬁned by
ΔP (t, x, v, p, p) :=

T(t,x,v,p,p)π(P ,Y )
−1(CV (t, x, v)) ⊂T(t,x,v,p,p)P.
(15)
When CV is locally described as in (14), the local expression of ΔP is given by
ΔP (t, x, v, p, p) =

(δt, δx, δv, δp, δp) ∈T(t,x,v,p,p)P |
n

i=1
Ar
i (t, x, v)δxi + Br(t, x, v)δt = 0, r = 1, ..., m

. (16)

Thermodynamics of Open Systems
231
Dirac Structures on the Covariant Pontryagin Bundle. Associated with
T ∗Y , there is the canonical symplectic form ΩT ∗Y = −dΘT ∗Y , where ΘT ∗Y is
the canonical one-form on T ∗Y . In local coordinates, we have
ΘT ∗Y = pidxi + pdt
and
ΩT ∗Y = dxi ∧dpi + dt ∧dp.
Using the projection π(P ,T ∗Y ) : P →T ∗Y , (t, x, v, p, p) →(t, x, p, p) onto
T ∗Y , the presymplectic form on the covariant Pontryagin bundle is obtained as
ωP = π∗
(P ,T ∗Y )ΩT ∗Y ,
(17)
with local expression ωP = dxi ∧dpi + dt ∧dp.
Given the distribution ΔP in (15) and the presymplectic form ωP in (17),
the induced Dirac structure DΔP on P is deﬁned by, for each x ∈P,
DΔP (x) =

(ux, ax) ∈TxP × T ∗
x P | ux ∈ΔP (x),
⟨ax, vx⟩= ΩP (x)(ux, vx), ∀vx ∈ΔP (x)

. (18)
When the local expression of the distribution is given as in (16), using local
coordinates x = (t, x, v, p, p) ∈P, ux = (ut, ux, uv, up, up) ∈TxP, vx =
(δt, δx, δv, δp, δp) ∈TxP, and ax = (π, α, β, γ, w) ∈T ∗
x P, the condition that
an element (ux, ax) ∈TxP × T ∗
x P belongs to the section of the induced Dirac
structure
(ux, ax) ∈DΔP (x)
takes the following form:
⎧
⎨
⎩
ux = w,
ut = γ,
β = 0,
(t, x, ut, ux) ∈CV (t, x, v),
(up + π, up + α) ∈CV (t, x, v)◦.
(19)
In the above, CV (t, x, v)◦⊂T ∗
(t,x)Y denotes the annihilator of CV (t, x, v) ⊂
T(t,x)Y , which is deﬁned by
CV (t, x, v)◦=

a ∈T ∗
(t,x)Y | ⟨a, η⟩= 0, ∀η ∈CV (t, x, v)

and hence the local coordinate expression of the annihilator is
CV (t, x, v)◦= {(t, x, π, α) ∈T ∗
(t,x)Y | π = λrBr(t, x, v), α = λrAr
i (t, x, v), λr ∈R}.
Thus, the local coordinate expressions for (19) are given by
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
ui
x = wi,
ut = γ,
βi = 0,
n
i=1
Ar
i (t, x, v)ui
x + Br(t, x, v)ut = 0, r = 1, ..., m,
up + π =
m

r=1
λrBr(t, x, v),
(up)i + αi =
m

r=1
λrAr
i (t, x, v), i = 1, ..., n.
(20)
We refer to [28] for the basic construction of the induced Dirac structure from a
distribution in nonholonomic mechanics.

232
H. Yoshimura and F. Gay-Balmaz
3.3
Dirac Dynamical Systems on the Covariant Pontryagin Bundle
In this subsection, we deﬁne a Dirac dynamical system by employing the induced
Dirac structure and the covariant generalized energy on the covariant Pontryagin
bundle. This is an example of the port-Dirac system, see [16].
The Covariant Generalized Energy. Given a time-dependent Lagrangian
L : J1Y = R×TQ →R, we deﬁne the covariant generalized energy E : P →R
on P by
E (t, x, v, p, p) = p + ⟨p, v⟩−L (t, x, v).
(21)
We also deﬁne the generalized energy E : R × (TQ ⊕T ∗Q) →R by
E(t, x, v, p) = ⟨p, v⟩−L (t, x, v),
(22)
which satisﬁes the relation E = E + p. It is important to note that the general-
ized energy (22) is not induced by an intrinsic object for general ﬁeld theories,
as opposed to the covariant generalized energy (21). In our case, since the con-
ﬁguration bundle is trivial, the expression (22) is still well-deﬁned.
External Forces. Let F ext : J1Y →T ∗Q be an external force ﬁeld, with
F ext(t, x, v) ∈T ∗
xQ, for all (t, x, v) ∈J1Y . Using the natural projection π(P ,Q) :
P →Q, (t, x, v, p, p) →x, the external force ﬁeld F ext on T ∗Q can be lifted as
a horizontal one-form on P as, for (t, x, v, p, p) ∈P and W ∈T(t,x,v,p,p)P,
⟨F ext(t, x, v, p, p), W⟩= ⟨F ext(t, x, v), T(t,x,v,p,p)π(P ,Q)(W)⟩.
Locally, we have F ext(t, x, v, p, p) = (t, x, v, p, p, 0, F ext(t, x, v), 0, 0, 0).
Proposition 1. Given ΔP , L (t, x, v), and F ext(t, x, v) as above, a curve of the
form
x(t) = (t, x(t), v(t), p(t), p(t))
(23)
on the covariant Pontryagin bundle P satisﬁes the condition for a time-
dependent Dirac dynamical system

˙x(t), dE (x(t)) −F ext(x(t))

∈DΔP (x(t))
(24)
if and only if it is a solution curve of
i˙xΩP −dE (x) + F ext(x) ∈ΔP (x)◦,
˙x ∈ΔP (x).
(25)
Proof. Using (18), we can easily compute the condition in (24) to derive the
intrinsic equations of motion in (25) for the time-dependent Dirac dynamical
system.
⊓⊔
Remark 2. In the above, the curve x(t) ∈P in (23) is not an arbitrary curve in
P since its ﬁrst component is t. In the language of ﬁeld theory, it is a section of
the covariant Pontryagin bundle seen as a bundle over R.

Thermodynamics of Open Systems
233
Local Coordinate Expressions. Let us derive the local coordinate expressions
of the Dirac dynamical system. The diﬀerential of E is given by
dE (t, x, v, p, p) =

−∂L
∂t , −∂L
∂x , p −∂L
∂v , 1, v

and recall that the external force ﬁeld is
F ext(t, x, v, p, p) =

t, x, v, p, p, 0, F ext(t, x, v), 0, 0, 0

.
Therefore, using the expression (19) of the Dirac structure, the condition for the
curve x(t) ∈P to satisfy the Dirac dynamical system (24) can be written as:

˙x = v,
˙t = 1,
p = ∂L
∂v ,
(t, x, ˙t, ˙x) ∈CV (t, x, v),
˙p −∂L
∂t , ˙p −∂L
∂x −F ext
∈CV (t, x, v)◦.
(26)
By using the local expressions in (20), the equations of motion of the Dirac
dynamical system in (26) take the form:
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
˙xi = vi,
˙t = 1,
pi −∂L
∂vi = 0,
i = 1, ..., n,
n
i=1
Ar
i (t, x, v) ˙xi + Br(t, x, v) = 0,
r = 1, ..., m,
˙pi −∂L
∂xi =
m

r=1
λrAr
i (t, x, v) + F ext
i
(t, x, v),
˙p −∂L
∂t =
m

r=1
λrBr(t, x, v).
(27)
Note that ˙t = 1 is always satisﬁed and also that the last equation in (27) can
be solved apart from the others (as an output equation). Therefore, the equa-
tions of motion (27) reduce to the following evolution equations for the curve
(x(t), v(t), p(t)) ∈TQ ⊕T ∗Q:
⎧
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎩
˙xi = vi,
pi = ∂L
∂vi (t, x, v),
˙pi −∂L
∂xi (t, x, v) =
m

r=1
λrAr
i (t, x, v) + F ext
i
(t, x, v), i = 1, ..., n,
n
i=1
Ar
i (t, x, v) ˙xi + Br(t, x, v) = 0, r = 1, ..., m.
(28)
Finally, the system of evolution equations (28) yields the following equations for
the curve x(t) ∈Q:
⎧
⎪
⎨
⎪
⎩
d
dt
∂L
∂˙xi −∂L
∂xi (t, x, ˙x) =
m

r=1
λrAr
i (t, x, ˙x) + F ext
i
(t, x, v), i = 1, ..., n,
n
i=1
Ar
i (t, x, ˙x) ˙xi + Br(t, x, ˙x) = 0,
r = 1, ..., m,
(29)
which are the Lagrange-d’Alembert equations with time-dependent nonlinear con-
straints. In particular, we notice that the second equation in (29) recovers the
nonlinear kinematic constraints CK, although CV was only used to introduce the

234
H. Yoshimura and F. Gay-Balmaz
Dirac structure DΔP . This is due to the special relation between the constraints
CV and CK of thermodynamic type, see Deﬁnition 2.
The Lagrange-d’Alembert equations with time-dependent and nonlinear non-
holonomic constraints given in (29) provide the general abstract setting for open
simple thermodynamic systems, as will be illustrated in Sect. 4.
Energy Balance. For the covariant generalized energy E (t, x, v, p, p) deﬁned
in (21), we have the energy balance equation along the solution curve x(t) =
(t, x(t), v(t), p(t), p(t)) of the Dirac dynamical system (27) as
d
dtE (t, x, v, p, p) =

F ext(t, x, v), ˙x

.
(30)
Note that E does not represent the total energy of the system. The total energy
is represented by the generalized energy E in (22). In terms of E, we get from
(30) the following equation:
d
dtE(t, x, v, p) = −d
dtp +

F ext(t, x, v), ˙x

= −∂L
∂t (t, x, v) −
m

r=1
λrBr(t, x, v) +

F ext(t, x, v), ˙x

,
(31)
which yields the balance of energy for the Dirac dynamical system. Note that
d
dtp is interpreted as the power ﬂowing out of the system. The ﬁrst term on the
right-hand side is uniquely due to the explicit dependence of the Lagrangian
on time, while the second term is due to the aﬃne character of the kinematic
constraint, which will be interpreted later as the energy ﬂowing in or out of the
system through the external ports. For the case where there is no constraint, the
energy balance equation of time-dependent mechanics is recovered (see [24]).
It is also interesting to note that the equation for p is solved apart from
the other equations. A natural initial condition for p is p(0) = −E(0), so that
the covariant generalized energy vanishes, i.e., E (t, x, v, p, p) = 0 for all t, which
is the generalized energy analogue of the super-Hamiltonian constraint (see, for
instance, [18]).
Remark 3 (Time-Dependent Dirac dynamical systems). It is noteworthy that
(24) is called the condition for a time-dependent Dirac dynamical system, since
the Dirac structure DΔP ⊂TP ⊕T ∗P is time-dependent, being deﬁned at
each point x = (t, x, v, p, p) ∈P of the covariant Pontryagin bundle where
the time t is included as a variable. It is remarkable that the Dirac structure
DΔP can incorporate constraints that are time-dependent, nonlinear, as well as
nonholonomic. From the ﬁeld theoretic point of view, the time-dependent Dirac
system that satisﬁes (24) can be interpreted as a special instance of a multi-
Dirac formulation for constrained ﬁeld theories that extends the multi-Dirac
ﬁeld theory developed in [27].
Remark 4 (A Lagrange-d’Alembert-Pontryagin principle). Note that the equa-
tions (29) can be obtained from the following Lagrange-d’Alembert-Pontryagin
principle:

Thermodynamics of Open Systems
235
δ
 t2
t1

p, ˙x −v

−L (t, x, v)

+
 t2
t1

F ext(t, x, v), δx

dt
(32)
subject to the kinematic constraints
n

i=1
Ar
i (t, x, v) ˙xi + Br(t, x, v) = 0, r = 1, ..., m
(33)
and for variations subject to the variational constraints
n

i=1
Ar
i (t, x, v)δxi = 0, r = 1, ..., m
(34)
with δx(t1) = δx(t2) = 0.
While this principle does yield the complete equations of motion (29), only
a subset of the conditions associated to the Dirac dynamical system (27) are
recovered; namely, the equation for p is missing. We shall give below a Lagrange-
d’Alembert-Pontryagin principle whose stationary conditions exactly coincide
with the equations given by the Dirac dynamical system.
3.4
The Lagrange-d’Alembert-Pontryagin Principle
on the Covariant Pontryagin Bundle
Associated with the Dirac dynamical systems, there exists an associated
variational formulation which is called the generalized Lagrange-d’Alembert-
Pontryagin principle, where the critical condition yields the time-dependent evo-
lution equations obtained from the Dirac dynamical system.
The Lagrange-d’Alembert-Pontryagin Principle. We begin with introduc-
ing an action functional for arbitrary curves x(τ) on the covariant Pontryagin
bundle P, namely,
x(τ) = (t(τ), x(τ), v(τ), p(τ), p(τ)) ∈P,
(35)
rather than just on sections, while we will see that the critical curve is nec-
essary a section up to a constant rescaling of time, i.e., t(τ) = τ + t0. Let
us denote by x′ the derivative with respect to τ. For such curves x(τ) =
(t(τ), x(τ), v(τ), p(τ), p(τ)) we consider the Lagrange-d’Alembert-Pontryagin
principle
δ
 τ2
τ1

θP (x(τ)), x′(τ)

−E (x(τ))

dτ +
 τ2
τ1
⟨F ext(x(τ)), δx(τ)⟩dτ = 0,
(36)
subject to the kinematic and variational constraints
x′(τ) ∈ΔP (x(τ))
and
δx(τ) ∈ΔP (x(τ)),
(37)
with the endpoint conditions Tπ(P ,Y )(δx(τ1)) = Tπ(P ,Y )(δx(τ2)) = 0.

236
H. Yoshimura and F. Gay-Balmaz
Equivalently, the critical point condition (36) reads
d
dε

ε=0
 τ2
τ1

θP (xε(τ)), x′
ε(τ)

−E (xε(τ))

dτ +
 τ2
τ1
⟨F ext(x(τ)), δx(τ)⟩dτ = 0,
for xε(τ) = (tε(τ), xε(τ), vε(τ), pε(τ), pε(τ)) such that
δx(τ) = d
dε

ε=0
xε(τ) = (δt(τ), δx(τ), δv(τ), δp(τ), δp(τ))
satisﬁes the variational constraint given in the second equation of (37). Note
that the above variations are not necessarily vertical. This variational condition
yields the Lagrange-d’Alembert-Pontryagin equations as:
ix′(τ)ωP −dE (x(τ)) + F ext(x(τ)) ∈ΔP (x(τ))◦,
x′(τ) ∈ΔP (x(τ)).
(38)
On the other hand, note that the evolution equations for arbitrary curves
x(τ) ∈P obtained from the condition of the Dirac dynamical system, namely,

x′(τ), dE (x(τ)) −F ext(x(τ))

∈DΔP (x(τ))
(39)
are equivalent with the equations in (38) obtained from the Lagrange-
d’Alembert-Pontryagin principle (36) and (37).
Local Expressions. By noting the equalities

θP (x), x′
−E (x) =

p, x′
+ pt′ −E (t, x, v, p, p)
=

p, x′ −v

+ p(t′ −1) −L (t, x, v),
the local expression for the Lagrange-d’Alembert-Pontryagin principle in (36)–
(37) for a curve x(τ) given in (35) becomes
δ
 τ2
τ1

p, x′
+ pt′ −E

t, x, v, p, p

dτ +
 τ2
τ1
⟨F ext(t, x, v), δx⟩dτ
= δ
 τ2
τ1

p, x′−v

+ p(t′−1)−L (t, x, v)

dτ +
 τ2
τ1
⟨F ext(t, x, v), δx⟩dτ
= 0,
(40)
subject to the kinematic constraints
n

i=1
Ar
i

t, x, v

x′i + Br
t, x, v

t′ = 0, r = 1, ..., m,
(41)
and for variations subject to the variational constraints
n

i=1
Ar
i

t, x, v

δxi + Br
t, x, v

δt = 0, r = 1, ..., m.
(42)

Thermodynamics of Open Systems
237
By direct computations, (40)–(42) yield the following equations
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
˙x′i = vi,
t′ = 1,
pi = ∂L
∂vi (t, x, v),
p′
i −∂L
∂xi (t, x, v) =
m

r=1
λrAr
i (t, x, v) + F ext
i
(t, x, v),
n
i=1
Ar
i (t, x, v)x′i + Br(t, x, v) = 0,
p′ −∂L
∂t =
m

r=1
λrBr(t, x, v).
(43)
These equations are the local coordinate expressions of (38). From the condition
t′(τ) = 1, we have t(τ) = τ + t0, and hence the critical curve of (40) is of the
form x(τ) = (τ + t0, x(τ), v(τ), p(τ), p(τ)), for a constant t0 which can be set to
zero by imposing the initial condition t0 := t(0) = 0, so that x becomes a section.
In this case (43) reduces to the system (27) associated to the time-dependent
Dirac dynamical system (24) for sections.
We summarize the obtained statements in the following theorem.
Theorem 2 (Time-Dependent Dirac Dynamical Systems). Given a
variational constraint CV ⊂J1Y ×Y TY as in (14), consider the induced
Dirac structure DΔP on P = J1Y ×Y T ∗Y as in (18). Let L : J1Y →R
be a time-dependent Lagrangian and E : P →R be the associated covariant
generalized energy. Let F ext : J1Y →T ∗Q be an external force ﬁeld. Then
the following statements are equivalent:
• The curve x(τ) = (t(τ), x(τ), v(τ), p(τ), p(τ)) ∈P satisﬁes the system
ix′ωP −dE (x) + F ext(x) ∈ΔP (x)◦,
x′ ∈ΔP (x),
which is locally given by (43).
• The curve x(τ) = (t(τ), x(τ), v(τ), p(τ), p(τ)) ∈P is a solution of the
Dirac dynamical system

x′(τ), dE (x(τ)) −F ext(x(τ))

∈DΔP (x(τ)).
• The curve x(τ) = (t(τ), x(τ), v(τ), p(τ), p(τ)) ∈P is a critical point of
the variational formulation
δ
 τ2
τ1

θP (x(τ)), x′(τ)

−E (x(τ))

dτ = 0,
subject to the kinematic and variational constraints
x′(τ) ∈ΔP (x(τ))
and
δx(τ) ∈ΔP (x(τ)),
with the ﬁxed endpoint conditions Tπ(P ,Y )(δx(τ1)) = Tπ(P ,Y )(δx(τ2)) =
0.

238
H. Yoshimura and F. Gay-Balmaz
Note that the kinematic constraints (41) are imposed on general curves x(τ) ∈
P. On a critical curve we have x′ = ˙x and t′ = ˙t = 1 so that one recovers the
kinematic constraints n
i=1 Ar
i

t, x, v

˙xi + Br
t, x, v

= 0.
Remark 5 (Curves versus Sections). So far we have employed arbitrary curves
x(τ) = (t(τ), x(τ), v(τ), p(τ), p(τ)) for the variational formulation. However, we
can assume that all critical curves x(τ) = (t(τ), x(τ), v(τ), p(τ), p(τ)) are sec-
tions, i.e., they are of the form x(t) = (t, x(t), v(t), p(t), p(t)), with the ﬁrst
component being given by t. This is not a restriction since we have just seen
above that the critical condition imposes, modulo a time translation, that criti-
cal curves are sections.
4
Dirac Formulation for Open Thermodynamic Systems
In this section we apply the Dirac dynamical system formulation for time-
dependent nonholonomic systems with nonlinear constraints of thermodynamic
type developed in Sect. 3 to the case of open thermodynamic systems.
4.1
Application to the Piston-Cylinder System with External Ports
We start by describing the geometric setting, the Lagrangian, and the constraints
for the case of a simple and open thermodynamic system with external ports,
in which the power exchange with exterior is due to matter transfer and heat
sources.
Geometric Setting. Consider a simple and open systems with A exterior ports
and B heat sources, such as the one illustrated in Fig. 1. Let Q be the conﬁgura-
tion manifold of the mechanical part of the system with elements denoted q and
let R5 be the conﬁguration space of the thermodynamic part of the system with
elements (S, N, Γ, W, Σ) ∈R5, where S is the entropy and N is the number of
moles of the chemical species of the system. Recall that W ∈R and Γ ∈R denote
the thermodynamic displacements that are deﬁned such that ˙W = μ and ˙Γ = T,
where μ is the chemical potential and T is the temperature. Recall also that
Σ ∈R denotes the internal entropy. Hence, the thermodynamic conﬁguration
space becomes
Q = Q × R5 ∋x = (q, S, N, Γ, W, Σ).
Associated with the external ports, we have the time-dependent external vari-
ables pa, T a, J a, a = 1, ..., A and ¯T b,
¯
J b
S, b = 1, ..., B. We also consider an
external force F ext : J1Y →T ∗Q; (t, x, v) →F ext(t, x, v) and a friction force
F fr : J1Y →T ∗Q; (t, x, v) →F fr(t, x, v), where we recall that Y = R × Q with
(t, x) ∈Y and x = (q, S, N, Γ, W, Σ).
We use the local coordinates v = (vq, vS, vN, vΓ, vW , vΣ) for v ∈TxQ, the
local coordinates (t, x, δt, δx) ∈TY with δx = (δq, δS, δN, δΓ, δW, δΣ) ∈TxQ
and the local coordinates (t, x, p, p) ∈T ∗Y with p = (pq, pS, pN, pΓ, pW , pΣ) ∈
T ∗
xQ.

Thermodynamics of Open Systems
239
Augmented Lagrangians for Open Thermodynamic Systems. Consider
the Lagrangian for the system given by a function L(t, q, ˙q, S, N) which can be
time-dependent in general. Recall that (q, ˙q) ∈TQ are the state variables of the
mechanical part.
Given L, we deﬁne the time-dependent augmented Lagrangian L : J1Y →R
as
L (t, x, ˙x) := L(t, q, ˙q, S, N) + ˙WN + ˙Γ(S −Σ).
(44)
Concerning the augmented terms ˙WN + ˙Γ(S −Σ) in (44), the ﬁrst part ˙WN(=
μN) is the expression corresponding to the Gibbs free energy associated with
the chemical substance, while the second part ˙Γ(S −Σ) can be interpreted as a
constraint for the minimum power due to the entropy production associated with
the exchange with the exterior, where S indicates the total entropy of the system
and Σ the internal entropy production. Therefore, the term S −Σ corresponds
to the part of the entropy of the system that is due to the exchange of entropy
with exterior and where ˙Γ(= T) may be interpreted as a Lagrange multiplier.
Constraints for Open Thermodynamic Systems. In the general context of
nonlinear nonholonomic constraints of thermodynamic type in Sect. 3, we choose
the coeﬃcients Ar
i (t, x, ˙x) and Br
i (t, x, ˙x) in the constraints (14) as
Ar
i (t, x, ˙x)δxi = −∂L
∂S δΣ+⟨F fr, δq⟩+
A

a=1

J aδW +J a
S δΓ
	
+
B

b=1
¯
J b
SδΓ,
Br
i (t, x, ˙x) = −
A

a=1

J aμa + J a
S T a	
−
B

b=1
¯
J b
S ¯T b.
(45)
It is important to recall that the molar ﬂow rates J a, the entropy ﬂow rates J a
S ,
¯
J b
S, as well as the temperatures T a, ¯T b and the chemical potentials μa at the A
ports and the B ports are explicit functions of time such as J a = J a(t, x, ˙x),
T a = T a(t, x, ˙x), and
¯
J b
S =
¯
J b
S(t, x, ˙x).
A First Lagrange-d’Alembert-Pontryagin Principle. With the Lagrangian
(44) and the choice for Ar
i and Br given in (45), the Lagrange-d’Alembert-
Pontryagin principle (32)–(34) recovers the Pontryagin version of the variational
formulation (8)–(10) presented in Sect. 2.2.
4.2
Dirac Dynamical Systems on the Covariant Pontryagin Bundle
In this subsection, we demonstrate that the equations of motion for simple and
open thermodynamic systems can be systematically developed in the context of
Dirac systems.
The Presymplectic Form on the Covariant Pontryagin Bundle. Recall
that the local coordinates for the covariant Pontryagin bundle P = J1Y ×Y
T ∗Y over Y = R × Q are given as x = (t, x, v, p, p) ∈P.

240
H. Yoshimura and F. Gay-Balmaz
The one-form and presymplectic form θP = π∗
(P ,T ∗Y )ΘT ∗Y
and ωP =
π∗
(P ,T ∗Y )ΩT ∗Y on P induced from the canonical forms ΘT ∗Y and ΩT ∗Y on
T ∗Y , have the local expressions
θP = pdx + pdt
= pqdq + pSdS + pNdN + pΓdΓ + pW dW + pΣdΣ + pdt,
ωP = dx∧dp + dt ∧dp
= dq∧dpq+dS∧dpS+dN ∧dpN +dΓ∧dpΓ+dW ∧dpW +dΣ∧dpΣ+dt∧dp,
where of course ωP = −dθP holds.
The Variational and Kinematic Constraints. By using the general deﬁni-
tion of the variational constraint given in (14) in view of (45), we get
CV =

(t, x, v, δt, δx) ∈J1Y ×Y TY
 ∂L
∂S δΣ = ⟨F fr, δq⟩
+
A

a=1

J a(δW −μaδt) + J a
S (δΓ −T aδt)

+
B

b=1
¯
J b
S(δΓ −¯T bδt)
	
, (46)
where the aﬃne part of the constraint is now associated with δt. Following
the general construction of the kinematic constraint CK in (13), the kinematic
constraint becomes
CK =

(t, x, ˙t, ˙x) ∈TY
 ∂L
∂S
˙Σ = ⟨F fr, ˙q⟩
+
A

a=1

J a( ˙W −μa ˙t) + J a
S ( ˙Γ −T a ˙t)

+
B

b=1
¯
J b
S( ˙Γ −¯T b ˙t)

,
where the aﬃne part of the constraint is now associated with ˙t.
Dirac Structures for Open Thermodynamic Systems. As in (15), the
variational constraint CV induces a distribution ΔP on P. Then, we can deﬁne
the induced Dirac structure on P, i.e., DΔP ⊂TP⊕T ∗P from the distribution
ΔP and the presymplectic form ωP as in (18).
In order to develop the local expressions of the Dirac structure from CV
in (46), we employ the following notations: For each x = (t, x, v, p, p) ∈P,
we write ux = (˙t, ˙x, ˙v, ˙p, ˙p) ∈TxP and ax = (π, α, β, γ, w) ∈T ∗
x P, where
˙v = (˙vq, ˙vS, ˙vN, ˙vΓ, ˙vW , ˙vΣ), ˙p = ( ˙pq, ˙pS, ˙pN, ˙pΓ, ˙pW , ˙pΣ), α = (αq, αS, αN, αΓ,
αW , αΣ), β = (βq, βS, βN, βΓ, βW , βΣ), and w = (wq, wS, wN, wΓ, wW , wΣ).
The
annihilator
CV (t, x, v)◦
⊂
T ∗Y
of
the
variational
constraint
CV (t, x, v) ⊂T(t,x)Y is locally represented by
CV (t, x, v)◦= {(t, x, π, α) ∈T ∗
(t,x)Y | π = λrBr(t, x, v), α = λrAr
i (t, x, v), λr ∈R}.

Thermodynamics of Open Systems
241
Hence, eliminating the Lagrange multiplier λ by substituting λ = αΣ
∂L
∂S , the anni-
hilator may be given by the covectors (t, x, π, α) ∈T ∗Y that satisfy the following
conditions:
π = αΣ
∂L
∂S
 
A

a=1
(J aμa + J a
S T a) +
B

b=1
¯
J b
S ¯T b
!
,
αq + αΣ
∂L
∂S
F fr = 0,
αS = 0,
αN = 0,
αΓ + αΣ
∂L
∂S
 
A

a=1
J a
S +
B

b=1
¯
J b
S
!
= 0,
αW + αΣ
∂L
∂S
A

a=1
J a = 0.
(47)
Using (19), (46) and (47), the condition that (ux, ax) ∈TxP × T ∗
x P belongs
to the section of the Dirac structure DΔP , i.e., the condition

(˙t, ˙x, ˙v, ˙p, ˙p), (π, α, β, γ, w)

∈DΔP (t, x, v, p, p),
is explicitly described by
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
˙t = γ,
˙q = wq,
˙S = wS,
˙N = wN,
˙Γ = wΓ,
˙W = wW ,
˙Σ = wΣ,
βq = βS = βN = βΓ = βW = βΣ = 0,
˙p + π =
1
∂L
∂S ( ˙pΣ + αΣ)
"
A

a=1
(J aμa + J a
S T a) +
B

b=1
¯
J b
S ¯T b
#
,
˙pq + αq +
1
∂L
∂S ( ˙pΣ + αΣ)F fr = 0,
˙pS + αS = 0,
˙pN + αN = 0,
˙pΓ + αΓ +
1
∂L
∂S ( ˙pΣ + αΣ)
"
A

a=1
J a
S +
B

b=1
¯
J b
S
#
= 0,
˙pW + αW +
1
∂L
∂S ( ˙pΣ + αΣ)
A

a=1
J a
S = 0,
∂L
∂S ˙Σ=⟨F fr, ˙q⟩+
A

a=1

J a( ˙W −μa ˙t)+J a
S ( ˙Γ −T a ˙t)

+
B

b=1
¯
J b
S( ˙Γ −¯T b ˙t).
(48)
Covariant Generalized Energy and External Forces. Recall from (44) that
the augmented Lagrangian on J1Y is given by L (t, x, v) = L(t, q, vq, S, N) +
vW N + vΓ(S −Σ). Therefore, the covariant generalized energy, see (21), is here
given by
E (t, x, v, p, p) = p + ⟨p, v⟩−L (t, x, v)
= p + ⟨pq, vq⟩+ pSvS + pNvN
+ (pΓ + Σ −S)vΓ + (pW −N)vW + pΣvΣ −L(t, q, vq, S, N).
The diﬀerential of dE is obtained by
dE (t, x, v, p, p) =

−∂L
∂t , −∂L
∂x , p −∂L
∂v , 1, v

= (π, α, β, γ, w),

242
H. Yoshimura and F. Gay-Balmaz
where
π = −∂L
∂t = −∂L
∂t ,
α = −∂L
∂x =

−∂L
∂q , −vΓ −∂L
∂S , −vW −∂L
∂N , 0, 0, vΓ

,
β = p −∂L
∂v =

pq −∂L
∂vq
, pS, pN, pΓ + Σ −S, pW −N, pΣ

,
γ = 1, w = v = (vq, vS, vN, vΓ, vW , vΣ).
Using π(P ,Q) : P →Q, (t, x, v, p, p) →x, the external force F ext : J1Y →
T ∗Q can be lifted as a horizontal one-form F ext on P by, for W ∈T(t,x,v,p,p)P,
⟨F ext(t, x, v, p, p), W⟩= ⟨F ext(t, x, v), T(t,x,v,p,p)π(P ,Q)(W)⟩.
Dirac Dynamical Systems on P for Open Thermodynamic Systems.
The Dirac formulation for open systems is given in the following theorem.
Theorem 3 (Dirac Formulation of Open Systems). Consider a simple
and open thermodynamic system described as before with the Lagrangian
L, molar ﬂow rates J a, entropy ﬂow rates J a
S ,
¯
J b
S, external variables T a,
μa, ¯T b, and the external force F ext. Consider the associated Dirac structure
DΔP as deﬁned above.
If the curve x(t) = (t, x(t), v(t), p(t), p(t)) ∈P, with x = (q, S, N, Γ,
W, Σ), v = (vq, vS, vN, vΓ, vW , vΣ) and p = (pq, pS, pN, pΓ, pW , pΣ), is a solu-
tion curve of the Dirac dynamical system

˙x, dE (x) −F ext(x)
	
∈DΔP (x),
(49)
then the following evolution equations are satisﬁed:
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
pq = ∂L
∂˙q ,
pΓ = S −Σ,
pW = N,
˙p −∂L
∂t = −
A

a=1
(J aμa + J a
S T a) −
B

b=1
¯
J b
S ¯T b,
˙pq = ∂L
∂q + F fr + F ext,
˙pΓ =
A

a=1
J a
S +
B

b=1
¯
J b
S,
˙pW =
A

a=1
J a,
˙Γ = −∂L
∂S ,
˙W = −∂L
∂N ,
∂L
∂S ˙Σ=⟨F fr, ˙q⟩+
A

a=1

J a( ˙W −μa)+J a
S ( ˙Γ−T a)

+
B

b=1
¯
J b
S( ˙Γ−¯T b),
(50)
which are equivalently reduced to the evolution equations of simple and open
thermodynamic systems given in (11).

Thermodynamics of Open Systems
243
Proof. Using the local expressions of the Dirac structure given in (48), we get
the following system equations for the Dirac dynamical system:
⎧
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎪
⎩
˙t = 1,
˙q = vq, ˙S = vS,
˙N = vN,
˙Γ = vΓ,
˙W = vW ,
˙Σ = vΣ,
pq −∂L
∂vq = 0, pS = 0, pN = 0, pΓ + Σ −S = 0, pW −N =0, pΣ = 0,
˙p −∂L
∂t =
1
∂L
∂S ( ˙pΣ + vΓ)
"
A

a=1
(J aμa+J a
S T a)+
B

b=1
¯
J b
S ¯T b
#
,
˙pq −∂L
∂q +
1
∂L
∂S ( ˙pΣ + vΓ)F fr = F ext,
˙pS −vΓ −∂L
∂S = 0,
˙pN −vW −∂L
∂N = 0,
˙pΓ + 0 +
1
∂L
∂S ( ˙pΣ + vΓ)
"
A

a=1
J a
S +
B

b=1
¯
J b
S
#
= 0,
˙pW + 0 +
1
∂L
∂S ( ˙pΣ + vΓ)
A

a=1
J a
S = 0,
∂L
∂S ˙Σ=⟨F fr, ˙q⟩+
A

a=1

J a( ˙W −μa ˙t)+J a
S ( ˙Γ−T a ˙t)

+
B

b=1
¯
J b
S( ˙Γ−¯T b ˙t).
Since pS = 0, we have vΓ = −∂L
∂S from an equation in the fourth line. From this
and pΣ = 0, we obtain
1
∂L
∂S ( ˙pΣ+vΓ) = −1. Then we obtain the evolution equations
in (50). By eliminating some variables and making further rearrangements, we
can check that the evolution equations (50) are ﬁnally reduced to the Lagrange-
d’Alembert equations for the open system given in (11).
Remark 6 (The Lagrange-d’Alembert-Pontryagin principle for open systems).
The variational formulation associated with the Dirac system formulation (49)
for open systems is obtained directly from (40)–(42) by making the correspond-
ing replacements. In particular it yields equations of motion (50).
Interpretation of the Thermodynamic Variables. The system of equations
in (50) allows to make useful physical interpretations of the variables involved
in the Dirac system formulation.
The two equations in the fourth line of (50) attribute to the variables Γ and
W the meaning of thermodynamic displacements associated to the process of
heat and matter transport.
The momentum pW = N conjugate to W is interpreted as the number of
moles in the system. Therefore, from the third equation in the third line of (50),
the mass balance equation given in (2) can be recovered as
˙pW =
A

a=1
J a.
The momentum pΓ = S −Σ conjugate to Γ corresponds to the part of the
entropy of the system that is due to the exchange of entropy with exterior. It
follows from the second equation in the third line of (50) that its rate of change
becomes
˙pΓ = ˙S −˙Σ =
A

a=1
J a
S +
B

b=1
¯
J b
S

244
H. Yoshimura and F. Gay-Balmaz
and we recover the rate of total entropy change of the system as
˙S = ˙Σ + ˙pΓ = I +
A

a=1
J a
S +
B

b=1
¯
J b
S.
(51)
The internal entropy production ˙Σ = I is always positive by the second law of
thermodynamics, while ˙pΓ = A
a=1 J a
S + B
b=1
¯
J b
S, giving the rate of entropy
ﬂowing from exterior into the system, has an arbitrary sign. It is noteworthy
that equation (51) is usually denoted in the form
dS = diS + deS
in conventional physics textbooks (see, for instance, [5]), where dS denotes the
inﬁnitesimal change of the total entropy, diS the entropy produced inside the
system and deS the entropy supplied to the system by its surroundings. In our
formulation, it reads as
diS = ˙Σdt
and
deS = ˙pΓdt.
The momentum p represents minus the rate of energy associated with the
matter and heat exchange with the exterior through the ports. In fact, from the
equation in the second line of (50), one obtains
d
dtp = ∂L
∂t −
A

a=1
(J aμa + J a
S T a)



=P ext
M
−
B

b=1
¯
J b
S ¯T b



=P ext
H
.
Finally, associated with the momentum pq =
∂L
∂vq , the ﬁrst equation in the
third line of (50) represents the Lagrange-d’Alembert equations for the mechan-
ical part of the system:
 d
dt
∂L
∂vq = ∂L
∂q + F fr + F ext,
˙q = vq.
Energy Balance. Consider the total energy of the system given by
E(t, q, ˙q, S, N) =
$∂L
∂˙q , ˙q
%
−L(t, q, ˙q, S, N).
This energy coincides with the energy E(t, x, ˙x), x = (q, S, N, Γ, W, Σ) deﬁned
from the augmented Lagrangian L (t, x, ˙x) = L(t, q, ˙q, S, N) + ˙WN + ˙Γ(S −Σ)
via the formula E(t, x, ˙x) =
 ∂L
∂˙x , ˙x

−L (t, x, ˙x). The energy balance equation
is obtained as
d
dtE = ⟨F ext, ˙q⟩−d
dtp
= −∂L
∂t + ⟨F ext, ˙q⟩
B

b=1



=P ext
W
+
A

a=1
(J aμa + J a
S T a)



=P ext
M
+
B

b=1
¯
J b
S ¯T b



=P ext
H
.

Thermodynamics of Open Systems
245
Regarding the covariant generalized energy, we have d
dtE = d
dtE+ d
dtp = ⟨F ext, ˙q⟩.
For the case in which the given Lagrangian L does not depend on time t explicitly,
we recover the ﬁrst law for open systems given in (1) as:
d
dtE = P ext
W + P ext
H
+ P ext
M .
Acknowledgements. H.Y. is partially supported by JSPS Grant-in-Aid for Scientiﬁc
Research (17H01097), JST CREST Grant Number JPMJCR1914, the MEXT Top
Global University Project, Waseda University (SR 2020C-194) and the Organization
for University Research Initiatives (Evolution and application of energy conversion
theory in collaboration with modern mathematics). F.G.B. is partially supported by
the ANR project GEOMFLUID, ANR-14-CE23-0002-01.
References
1. Bloch, A.M.: Nonholonomic Mechanics and Control. Interdisciplinary Applied
Mathematics, vol. 24. Springer, New York (2003). With the collaboration of J.
Baillieul, P. Crouch and J. Marsden, and with scientiﬁc input from P. S. Krish-
naprasad, R. M. Murray and D. Zenkov
2. Cendra, H., Ibort, A., de Le´on, M., de Diego, D.: A generalization of Chetaev’s
principle for a class of higher order nonholonomic constraints. J. Math. Phys. 45,
2785 (2004)
3. Courant, T.J.: Dirac manifolds. Trans. Amer. Math. Soc. 319, 631–661 (1990)
4. Courant, T., Weinstein, A.: Beyond poisson structures. In: Actions Hamiltoniennes
de Groupes. Troisi`eme Th´eor`eme de Lie (Lyon, 1986), vol. 27, pp. 39–49. Hermann,
Paris (1988)
5. de Groot, S.R., Mazur, P.: Nonequilibrium Thermodynamics. North-Holland, Ams-
terdam (1969)
6. Eldred, C., Gay-Balmaz, F.: Single and double generator bracket formulations of
multicomponent ﬂuids with irreversible processes. J. Phys. A: Math. Theor. 53,
395701 (2020)
7. Eldred, C., Gay-Balmaz, F.: Thermodynamically consistent semi-compressible ﬂu-
ids: a variational perspective, arXiv:2102.08293v1 (2021)
8. Ferrari, C., Gruber, C.: Friction force: from mechanics to thermodynamics. Eur.
J. Phys. 31(5), 1159–1175 (2010)
9. Gay-Balmaz, F.: A variational derivation of the nonequilibrium thermodynamics of
a moist atmosphere with rain process and its pseudoincompressible approximation.
Geophys. Astrophysical Fluid Dynamics 113(5–6), 428–465 (2019)
10. Gay-Balmaz, F., Yoshimura, H.: A Lagrangian variational formulation for nonequi-
librium thermodynamics. Part I: discrete systems. J. Geom. Phys. 111, 169–193
(2017)
11. Gay-Balmaz, F., Yoshimura, H.: A Lagrangian variational formulation for nonequi-
librium thermodynamics. Part II: continuum systems. J. Geom. Phys. 111, 194–212
(2017)
12. Gay-Balmaz, F., Yoshimura, H.: A variational formulation of nonequilibrium ther-
modynamics for discrete open systems with mass and heat transfer. Entropy 163,
1–26 (2018). https://doi.org/10.3390/e20030163
13. Gay-Balmaz, F., Yoshimura, H.: Dirac structures in nonequilibrium thermodynam-
ics. J. Math. Phys. 59, 012701-29 (2018)

246
H. Yoshimura and F. Gay-Balmaz
14. Gay-Balmaz, F., Yoshimura, H.: From Lagrangian mechanics to nonequilibrium
thermodynamics: a variational perspective. Entropy 21(1), 8 (2019). https://doi.
org/10.3390/e21010008
15. Gay-Balmaz, F., Yoshimura, H.: Dirac structures in nonequilibrium thermodynam-
ics for simple open systems. J. Math. Phys. 61, 09270 (2020). https://doi.org/10.
1063/1.5120390
16. Gay-Balmaz, F., Yoshimura, H.: Dirac structures and variational structures of
port-Dirac systems in nonequilibrium thermodynamics. IMA J. Math. Control.
Inf. 37(4), 1298–1347 (2020). https://doi.org/10.1093/imamci/dnaa015
17. Gay-Balmaz, F., Yoshimura, H.: From variational to bracket formulations in
nonequilibrium thermodynamics of simple systems. J. Phys. Geom. (2020, in press)
18. Gotay, M., Isenberg, J., Marsden, J.E., Montgomery, R.: Momentum maps
and classical relativistic ﬁelds, Part I: Covariant ﬁeld theory (66 pages).
arXiv:physics/9801019v2 (1997)
19. Green, A.E., Naghdi, P.M.: A re-examination of the basic postulates of thermo-
mechanics. Proc. R. Soc. London Ser. Math. Phys. Eng. Sci. 432(1885), 171–194
(1991)
20. Gruber, C.: Thermodynamics of systems with internal adiabatic constraints: time
evolution of the adiabatic piston. Eur. J. Phys. 20, 259–266 (1999)
21. Gruber, C., Br´echet, S.D.: Lagrange equation coupled to a thermal equation:
mechanics as a consequence of thermodynamics. Entropy 13, 367–378 (2011)
22. Klein, S., Nellis, G.: Thermodynamics. Cambridge University Press, Cambridge
(2011)
23. Kondepudi, D., Prigogine, I.: Modern Thermodynamics. Wiley, Hoboken (1998)
24. Lanczos, C.: Variational Principles of Mechanics, 4th edn. Dover (1970)
25. Marsden, J.E., Ratiu, T.S.: Introduction to Mechanics and Symmetry. Texts in
Applied Mathematics, vol. 17, 2nd edn. Springer, Heidelberg (1999)
26. Stueckelberg,
E.C.G.,
Scheurer,
P.B.:
Thermocin´etique
Ph´enom´enologique
Galil´eenne. Birkh¨auser (1974)
27. Vankerschaver, J., Yoshimura, H., Leok, M.: The Hamilton-Pontryagin principle
and multi-Dirac structures for classical ﬁeld theories. J. Math. Phys. 53, 072903-
1–072903-25 (2012)
28. Yoshimura, H., Marsden, J.E.: Dirac structures in Lagrangian mechanics Part I:
Implicit Lagrangian systems. J. Geom. and Phys. 57, 133–156 (2006)
29. Yoshimura, H., Marsden, J.E.: Dirac structures in Lagrangian mechanics Part II:
Variational structures. J. Geom. and Phys. 57, 209–250 (2006)

The Geometry of Some Thermodynamic
Systems
Alexandre Anahory Simoes, David Mart´ın de Diego, Manuel Lainz Valc´azar,
and Manuel de Le´on(B)
Instituto de Ciencias Matem´aticas (CSIC-UAM-UC3M-UCM), Calle Nicol´as Cabrera,
13-15, Campus Cantoblanco, UAM, 28049 Madrid, Spain
{alexandre.anahory,david.martin,manuel.lainz,mdeleon}@icmat.es
Abstract. In this article, we continue the program started in [2] of
exploring an important class of thermodynamic systems from a geo-
metric point of view. The contents of this paper and the one already
published in [2] provide a geometrical formulation, which tries to shed
more light on the properties of thermodynamic systems without claim-
ing to be a deﬁnitive theory. In order to model the time evolution of
systems verifying the two laws of thermodynamics, we show that the
notion of evolution vector ﬁeld is adequate to appropriately describe
such systems. Our formulation naturally arises from the introduction of
a skew-symmetric bracket to which numerical methods based on discrete
gradients ﬁt nicely. Moreover, we study the corresponding Lagrangian
and Hamiltonian formalism, discussing the fundamental principles from
which the equations are derived. An important class of systems that is
naturally covered by our formalism are composed thermodynamic sys-
tems, which are described by at least two thermal variables and exchange
heat between its components.
1
Introduction
In this paper, we continue the diﬀerential geometric study initiated in our paper
about the evolution vector ﬁeld associated with a contact system [2]. However,
when we analyze more complex thermodynamic examples, we will soon realize
that it is often necessary to consider alternative geometrical structures from
those of contact geometry and/or combine them properly.
Our approach diﬀers from others in the previous literature, where diﬀerent
authors introduce simultaneously a skew-symmetric and a symmetric bracket
with combined properties that allow the two laws of thermodynamics to be
satisﬁed. This is the case, for instance, of metriplectic structures (see [27,31]
and references therein) or also the so-called single generation formalism [17].
Alternatively, other authors such as Gay-Balmaz and Yoshimura introduce in [20,
21] a “variational principle” for the description of thermodynamic systems by
means of phenomenological and variational constraints. Moreover, in [18,22] the
authors show how this variational formalism is used to systematically derive the
two bracket formalism mentioned before.
c
⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2021
F. Barbaresco and F. Nielsen (Eds.): SPIGL 2020, PROMS 361, pp. 247–275, 2021.
https://doi.org/10.1007/978-3-030-77957-3_13

248
A. A. Simoes et al.
The application of contact geometry (see [3,25]) to model thermodynamics is
suggested by Gibbs’ fundamental relation, which relates extensive and intensive
variables deﬁning the state of thermodynamic systems. Contact geometry is the
odd dimensional counterpart of symplectic geometry [23,28]. From this point of
view, the ﬂow of the restriction of the contact vector ﬁeld to a Legendre sub-
manifold of an appropriate contact manifold is interpreted as a thermodynamic
process [32,33]. The most familiar symplectic framework can be obtained by
symplectiﬁcation of the contact manifold, obtaining a symplectic manifold with
an additional structure of homogeneity bringing together energy and entropy
representations (see also [4,36]).
In the present paper, after quickly reviewing contact geometry in Sect. 2, we
focus not on the dynamics derived from the contact vector ﬁeld as in [6,7,12]
but instead we will choose a diﬀerent vector ﬁeld: the so-called evolution vector
ﬁeld. We remark that the evolution vector ﬁeld is deﬁned exclusively in terms
of a bi-vector or, equivalently, a skew-symmetric bracket of functions. This fact
allows the use of discrete gradient methods [2,24,26] to numerically simulate the
dynamics of thermodynamic processes. In [2], we show that the evolution vector
ﬁeld preserves the Hamiltonian, which means that it models an isolated system.
Moreover, we show that this vector ﬁeld is tangent to the kernel of the contact
form, which corresponds to the ﬁrst law of thermodynamics.
As a novelty, we state in Sect. 3 a “variational principle” for the evolution
dynamics in a Lagrangian framework, which is a generalization of Chetaev’s
principle (cf. [9]).
In Sect. 4, we brieﬂy review some basic principles of Thermodynamics and
how the evolution vector ﬁeld models isolated simple thermodynamic systems
with friction (see [2]).
In the ﬁnal part of our paper, we will show in Sect. 5 that the evolution
vector ﬁeld or Hamiltonian vector ﬁeld associated to a skew-symmetric bracket
is also useful to describe more complex systems outside the scope of contact
geometry. For instance, we study as an example a system composed of at least
two subsystems exchanging heat with each other [34,35].
After that, in Sect. 6 we present numerical methods that have the property
of satisfying the two laws of thermodynamics. To illustrate this point, we run a
simulation for each class of systems addressed in the paper.
Finally, in the last section, we conclude pointing out some possible directions
for further research.
2
Contact Geometry
In this section, we recall the facts about contact geometry which will be necessary
for developing our formalism [12,23,28]. We will deﬁne the Hamiltonian and the
evolution vector ﬁelds and compare its properties.
Let (M, η) be a contact manifold. That is, M is a (2n + 1)-dimensional
manifold and η is a 1-form that satisﬁes η ∧(dη)n ̸= 0 at every point.

The Geometry of Some Thermodynamic Systems
249
The Reeb vector ﬁeld R ∈X(M) is the unique vector ﬁeld satisfying
iRη = 1
and
iRdη = 0 .
The contact form η induces an isomorphism of C∞(M, R) modules by
♭: X(M) −→Ω1(M)
X −→iXdη + η(X)η
Observe that ♭−1(η) = R.
Using the generalized Darboux theorem, we have canonical coordinates
(qi, pi, S), 1 ≤i ≤n in a neighborhood of every point x ∈M, such that the
contact 1-form η and the Reeb vector ﬁeld are:
η = dS −pi dqi
and
R = ∂
∂S .
The contact structure also provides a Whitney sum decomposition of the
tangent bundle TM = ker η ⊕⟨R⟩, with projectors
P = Id −R ⊗η and Q = R ⊗η,
(1)
onto ker η and ⟨R⟩, respectively.
An important example of contact manifold is the extended cotangent bundle
T ∗Q × R, where Q is n-dimensional manifold, which is naturally equipped with
the following contact form
ηQ = pr∗
2(dS) −pr∗
1(θQ) ≡dS −θQ
where pr1 : T ∗Q×R →T ∗Q and pr2 : T ∗Q×R →R are the canonical projections
and θQ is the Liouville 1-form on the cotangent bundle deﬁned by
θQ(Xμq) = ⟨μq, TμqπQXμq⟩
being Xμq ∈TμqT ∗Q. Taking bundle coordinates (qi, pi) on T ∗Q we have that
η = dS −pidqi.
On this situation, we notice that ω = dη|ker η, then (ker η, ω) is a symplectic
vector bundle over T ∗Q × R. Hence, there is a unique vector ﬁeld ΔQ tangent
to ker η satisfying iΔQω = θQ|ker η. We call ΔQ the Liouville vector ﬁeld and, in
Darboux coordinates, it is given by
ΔQ = pi
∂
∂pi
.
(2)
We remark that (dS, −dθQ) is a cosymplectic structure which can be natu-
rally constructed on T ∗Q × R. Indeed, dS and −dθQ are exact and dS ∧(dθ)n is
a volume form. However, the dynamics arising from this cosymplectic structure
are diﬀerent from the ones arising from the contact structure [13].

250
A. A. Simoes et al.
2.1
The Jacobi Structure of a Contact Manifold
Contact manifolds have an associated Jacobi structure. Indeed, we deﬁne the
bivector Λ as
Λ(α, β) = −dη(♭−1(α), ♭−1(β)),
α, β ∈Ω1(M) .
(3)
In local coordinates, the bivector reads as
Λ =
∂
∂pi
∧
 ∂
∂qi + pi
∂
∂S

.
(4)
So that the pair (Λ, E = −R) satisﬁes
[Λ, Λ] = 2E ∧Λ
and
[Λ, E] = 0 .
We deﬁne the morphism of C∞(M, R)-modules
♯Λ : Ω1(M) →X(M)
by ⟨β, ♯Λ(α)⟩= Λ(α, β) with α, β ∈Ω1(M).
From this Jacobi structure we can deﬁne the Jacobi bracket as follows:
{f, g} = Λ(df, dg) + fE(g) −gE(f),
f, g ∈C∞(M, R)
The mapping { , } : C∞(M, R) × C∞(M, R) −→C∞(M, R) is bilinear, skew-
symmetric and satisﬁes the Jacobi’s identity but, in general, it does not satisfy
the Leibniz rule; this last property is replaced by a weaker condition:
Supp {f, g} ⊂Supp f ∩Supp g .
This last condition is equivalent to the generalized Leibniz rule
{f, gh} = g{f, h} + h{f, g} + ghE(f),
(5)
In this sense, this bracket generalizes the well-known Poisson brackets. Indeed,
a Poisson manifold is a particular case of Jacobi manifold in which E = 0.
In local coordinates, the bracket is given by
{f, g} = ∂f
∂qi
∂g
∂pi
−∂f
∂S

pi
∂g
∂pi
−g

+ ∂g
∂S

pi
∂f
∂pi
−f

We can deﬁne another bracket, the Cartan bracket, by only using the bivector
of the Jacobi structure. This bracket is bilinear, skew-symmetric and satisﬁes the
Leibniz rule, but does not obey the Jacobi identity
[f, g] = Λ(df, dg)
= ∂f
∂pi
∂g
∂qi −∂f
∂qi
∂g
∂pi
−∂f
∂S

pi
∂g
∂pi

+ ∂g
∂S

pi
∂f
∂pi


The Geometry of Some Thermodynamic Systems
251
The third bracket can be deﬁned from the following bivector
Λ0 = Λ + R ∧ΔQ
which is Poisson, that is, [Λ0, Λ0] = 0. In coordinates,
Λ0 =
∂
∂pi
∧∂
∂qi
is like the canonical Poisson bracket on T ∗Q but now applied to functions on
T ∗Q × R. In fact, this bracket is the Jacobi bracket related to the cosymplectic
structure (dS, −dθQ) [16].
On the case that M = T ∗Q × R, the Cartan bracket can be rewritten in
terms of the Poisson bracket induced by Λ0 and an extra term describing the
thermodynamic behaviour. That is,
[f, g] = {f, g}Λ0 −∂f
∂S ΔQg + ∂g
∂S ΔQf
We will denote by
{f, g}ΔQ = ∂g
∂S ΔQf −∂f
∂S ΔQg
then the Cartan bracket is written as in the single generation formalism [17] as
[f, g] = {f, g}Λ0 + {f, g}ΔQ
(6)
2.2
Hamiltonian and Evolution Vector Fields
Given a function f : M →R on a contact manifold (M, η) we deﬁne the following
vector ﬁelds
• Hamiltonian or contact vector ﬁeld Xf deﬁned by
Xf = ♯Λ(df) −fR
equivalently, Xf is the unique vector ﬁeld such that
♭(Xf) = df −(R(f) + f) η .
In canonical coordinates:
Xf = ∂f
∂pi
∂
∂qi −
 ∂f
∂qi + pi
∂f
∂S
 ∂
∂pi
+

pi
∂f
∂pi
−f
 ∂
∂S
• The evolution or horizontal vector ﬁeld
Ef = ♯Λ(df) = Xf + fR
or
♭(Ef) = df −R(f) η .
In canonical coordinates:
Ef = ∂f
∂pi
∂
∂qi −
 ∂f
∂qi + pi
∂f
∂S
 ∂
∂pi
+ pi
∂f
∂pi
∂
∂S
(7)

252
A. A. Simoes et al.
Now we will compare some properties of the Hamiltonian and evolution vector
ﬁelds. Proofs can be found in [2,12].
Proposition 1. The Hamiltonian function f is preserved by the associated evo-
lution vector ﬁeld, while it is dissipated by the corresponding Hamiltonian vector
ﬁeld, i.e.,
Xf(f) = −R(f)f,
Ef(f) = 0.
The Hamiltonian vector ﬁeld preserves ker η (indeed, the ﬂow of Xf changes
η by a conformal factor), but the evolution vector ﬁeld does not
LXf η = −R(f)η,
LEf η = −R(f)η + df.
The evolution vector ﬁeld is everywhere contained in ker η, while the Hamil-
tonian vector ﬁeld is only in ker η at the zero level set of the energy
iXf η = −f,
iEf η = 0.
We compare the dynamics of the Hamiltonian [8] and evolution [2] vector
ﬁelds
Theorem 1. Let Lc(f) = f −1(c) be a level set of f : M →R where c ∈R. We
assume that Lc(f) ̸= 0 and R(f)(x) ̸= 0 for all x ∈Lc(f).
Then The 2-form ωc ∈Ω2(Lc(f)) deﬁned by
ωc = −di∗
cη
is an exact symplectic structure, where ic : Lcf →M is the canonical inclusion
Let Δc is the Liouville vector ﬁeld of TLc(f), given by
iΔcωc = i∗
cη
1. For the Hamiltonian vector ﬁeld Xf:
• When c ̸= 0, Xf is the Reeb vector ﬁeld of the contact form ˜η = η/f.
iXf ˜η = 1,
iXf ˜η = 0.
• When c = 0, the dynamics of Xf is a reparametrization of the dynamics
of the Liouville vector ﬁeld.
Xf

L0(f) = R(f)

Lc(f)Δ0
2. The dynamics of Ef is a reparametrization of the dynamics of the Liouville
vector ﬁeld at any constant energy hypersurface.
Ef

Lc(f) = R(f)

Lc(f)Δc

The Geometry of Some Thermodynamic Systems
253
3
The Lagrangian Formalism
3.1
The Geometric Setting
Let L : TQ × R −→R be a regular contact Lagrangian function (see [13,14]).
As before, let us introduce coordinates on TQ × R, denoted by (qi, ˙qi, S), where
(qi) are coordinates in Q, (qi, ˙qi) are the induced bundle coordinates in TQ and
S is a global coordinate in R.
Given a Lagrangian function L, using the canonical endomorphism S on TQ
locally deﬁned by
S = dqi ⊗∂
∂˙qi ,
one can construct a 1-form λL on TQ × R given by
λL = S∗(dL)
where now S and S∗are the natural extensions of S and its adjoint operator S∗
to TQ × R [15].
Therefore, we have that
λL = ∂L
∂˙qi dqi.
Now, the 1-form on TQ × R given by ηL = dS −λL or, in local coordinates,
by
ηL = dS −∂L
∂˙qi dqi
is a contact form on TQ × R if and only if L is regular; indeed, if L is regular,
then we may prove that ηL ∧(dηL)n ̸= 0, and the converse is also true.
The corresponding Reeb vector ﬁeld is given in local coordinates by
RL = ∂
∂S −W ij ∂2 L
∂˙qj∂S
∂
∂˙qi ,
where (W ij) is the inverse matrix of the Hessian (Wij) with
Wij =
∂2 L
∂˙qi∂˙qj .
(8)
The energy of the system is deﬁned by
EL = Δ(L) −L
where Δ = ˙qi
∂
∂˙qi is the natural extension of the Liouville vector ﬁeld on TQ to
TQ × R. Therefore, in local coordinates we have that
EL = ˙qi ∂L
∂˙qi −L.

254
A. A. Simoes et al.
Denote by ♭L : T(TQ × R) −→T ∗(TQ × R) the vector bundle isomorphism
given by
♭L(v) = iv(dηL) + (ivηL) ηL
where ηL is the contact form on TQ × R previously deﬁned. We shall denote its
inverse isomorphism by ♯L = (♭L)−1.
Let ξL be the unique vector ﬁeld satisfying the equation
♭L(ξL) = dEL −(RLEL + EL) ηL.
(9)
A direct computation from Eq. (9) shows that if (qi(t), ˙qi(t), S(t)) is an inte-
gral curve of ξL, then it satisﬁes the generalized Euler-Lagrange equations con-
sidered by G. Herglotz in 1930:
d
dt
 ∂L
∂˙qi

−∂L
∂qi = ∂L
∂˙qi
∂L
∂S ,
˙S = L(qi, ˙qi, S) .
(10)
Now, given a regular Lagrangian function L, we may deﬁne the bi-vector ΛL
on TQ × R as in (3) associated to the contact form ηL. That is,
ΛL(α, β) = −dηL(♭−1
L (α), ♭−1
L (β)),
α, β ∈Ω1(TQ × R) .
(11)
If (qi(t), ˙qi(t), S(t)) is an integral curve of the evolution vector ﬁeld ΓL asso-
ciated to the contact form ηL, which is the SODE vector ﬁeld deﬁned by
ΓL = ♯ΛL(dEL) or ♭L(ΓL) = dEL −(RLEL) ηL ,
then the curve satisﬁes the thermodynamic Herglotz equations
d
dt
 ∂L
∂˙qi

−∂L
∂qi = ∂L
∂˙qi
∂L
∂S .
˙S = ˙qi ∂L
∂˙qi .
(12)
Moreover, if H is the Hamiltonian function deﬁned by H = EL ◦(FL)−1,
where FL : TQ × R →T ∗Q × R is the Legendre transform, then the evolution
vector ﬁeld EH associated to H is FL-related to ΓL.
3.2
Generalized Chetaev Principle
In order to present a pseudo-variational principle, we will ﬁrst formulate a vari-
ational problem for systems subjected to a particular non-standard type of non-
holonomic constraints.
Let M be a manifold and let η be a semibasic 1-form on TM. Locally, if (qi)
are coordinates on M and (qi, ˙qi) are natural bundle coordinates on TM, we
have that
η(q, ˙q) = ηk(q, ˙q)dqk.

The Geometry of Some Thermodynamic Systems
255
To every semibasic 1-form, we may associate in a canonical way a force map
F : TM →T ∗M by the formula
⟨F(q, ˙q), TτM(X)⟩= ⟨η(q, ˙q), X⟩,
X ∈T(q, ˙q)(TM),
where τM : TM →M is the canonical tangent bundle projection. Locally, we
have that
F(q, ˙q) = ηk(q, ˙q)dqk.
Now, given a vector wq ∈TqM, consider the set
Dwq = {vq ∈TqM | ⟨F(wq), vq⟩= 0} ⊆TqM.
A semibasic nonholonomic constraint is speciﬁed by a semibasic 1-form η or,
equivalently, by a smooth assignment
wq →Dwq.
Then, a trajectory c : I →M is said to satisfy the constraint if ⟨F(˙c), ˙c⟩= 0 or,
equivalently, ˙c ∈D ˙c.
Deﬁnition 1 (Generalized Chetaev principle). Given a Lagrangian func-
tion L : TM →R and a semibasic nonholonomic constraint η, a trajectory
c : I →M satisﬁes the generalized Chetaev principle if
δ

Ldt = 0
and
˙c ∈D ˙c,
(13)
among variations with ﬁxed endpoints satisfying δc ∈D ˙c.
Similarly to what happens with variational calculus we may obtain a set of
equations as necessary and suﬃcient conditions to ﬁnd trajectories satisfying the
generalized Chetaev principle.
Lemma 1. Given a Lagrangian function L : TM →R and a semibasic non-
holonomic constraint η, a trajectory c : I →M satisﬁes the generalized Chetaev
principle if and only if ot satisﬁes the equations
d
dt
 ∂L
∂˙qi

−∂L
∂qi = ληi
ηi ˙qi = 0.
(14)
Proof. Using the standard arguments from calculus of variations, we deduce
that
δ

Ldt =
  ∂L
∂qi −d
dt
 ∂L
∂˙qi

δq dt,
where we used integration by parts and the fact that the inﬁnitesimal variations
vanish at the endpoints. Then, in order that the integrand vanishes for arbitrary
variations satisfying δc ∈D ˙c we must have that
∂L
∂qi −d
dt
 ∂L
∂˙qi

∈Do
˙c.

256
A. A. Simoes et al.
But Do
˙c = span{F(˙c)} ⊆T ∗
c M. Thus, the expression above must be a scalar
multiple of the co-vector F(˙c). Putting this fact together with the constraint
˙c ∈D ˙c we obtain Eqs. (14).
⊓⊔
The variational principle we have just stated is satisﬁed by the Lagrangian
evolution vector ﬁeld. More precisely, the integral curves of the Lagrangian evo-
lution vector ﬁeld satisfy a nonholonomic variational principle with nonlinear
constraints. Indeed, the solutions of the equations of motion are critical points
of the action with a condition of tangency to the contact distribution. This non-
linear nonholonomic principle is exactly the one described above and it is similar
to the one introduced in [20].
To observe this clearly, take M to be the manifold Q × R and consider the
extended Lagrangian function ˆL : T(Q × R) →R deﬁned by
ˆL(vq, ζS) = L(vq, S),
(vq, ζS) ∈T(q,S)(Q × R),
where L : TQ × R →R is a contact Lagrangian function. Take the pullback of
the Lagrangian 1-form ηL to T(Q × R), which we will also denote by ηL and
it is a semibasic 1-form. Let (qi) be coordinates on Q, (qi, ˙qi) natural tangent
bundle coordinates on TQ and (qi, S, ˙qi, ˙S) be coordinates on T(Q × R). The
local expression of the semibasic 1-form ηL is
ηL(q, S, ˙q, ˙S) = dS −∂L
∂˙qi dqi.
(15)
As before, we may associate to ηL a force map given by
FL : T(Q × R) →T ∗(Q × R)
(q, S, ˙q, ˙S) →dS −∂L
∂˙qi dqi.
The nonholonomic constraint is determined at each point (vq, ζS)
∈
T(q,S)(Q × R) by those vectors (γq, ξS) ∈T(q,S)(Q × R) such that
⟨FL(vq, ζS), (γq, ξS)⟩= 0.
Proposition 2. Given a contact Lagrangian L : TQ × R →R, a curve
(q(t), ˙q(t), S(t)) is an integral curve of the evolution vector ﬁeld ΓL if and only if
the associated curve (q(t), S(t), ˙q(t), ˙S(t)) satisﬁes the generalized Chetaev prin-
ciple for the extended Lagrangian ˆL with nonholonomic constraints determined
by the semibasic 1-form ηL.
Proof. We will compare the equations satisﬁed by integral curves of the evo-
lution vector ﬁeld with the ones satisﬁed by the solutions of the corresponding
generalized Chetaev principle.
Indeed, the solution of the generalized Chetaev principle satisfy
d
dt
 ∂L
∂˙qi

−∂L
∂qi = −λ ∂L
∂˙qi .
(16a)
d
dt
∂L
∂˙S

−∂L
∂S = λ.
(16b)

The Geometry of Some Thermodynamic Systems
257
Since L does not depend on ˙S, the last equation is reduced to
∂L
∂S = −λ,
(17)
So we retrieve the equations for the Lagrangian evolution vector ﬁeld (12) by
adding the constraint equation
˙S = ∂L
∂˙qi ˙qi
(18)
Remark 1. Observe that the generalized Chetaev principle associated to a con-
tact Lagrangian function L : TQ×R →R is similar to to the varational principle
with phenomenological constraints proposed in [20] in the absence of external
forces.
4
The Evolution Vector Field and Simple Mechanical
Systems with Friction
4.1
About the First and Second Laws of Thermodynamics
In this section we will apply the evolution vector ﬁeld to the description of simple
mechanical vector ﬁeld with friction. These systems can be described through
one scalar thermal variable (in our formulation it will be the entropy) and ﬁnitely
many mechanical variables (positions and velocities, in the Lagrangian formal-
ism, or positions and momenta in Hamiltonian formalism). Furthermore, our
system will be isolated: there will be no transfer of any form of matter or energy
with its surroundings.
The dynamics of the system will be described through a Lagrangian
L : TQ × R −→R,
(qi, ˙qi, S) −→L(qi, ˙qi, S),
or a Hamiltonian
H : T ∗Q × R −→R,
(qi, pi, S) −→H(qi, pi, S),
which, assuming that the Lagrangian is regular, both systems are connected
through the Legendre transform, as explained in Sect. 3. The physical meaning
of each coordinate will be explained below.
The integral curves of the evolution vector ﬁeld will describe the trajectories
of a system satisfying the ﬁrst and second laws of thermodynamics for an iso-
lated system, that is, a system that does not exchange matter or heat with the
surrounding environment and on which no external force acts upon it.
The thermodynamic space of a Lagrangian system is naturally equipped with
two linearly independent one forms: the work δW and the heat δQ one-forms.
The energy of the system is given by an energy function H : T ∗Q×R →R. For a
closed system (one that does not exchange matter, but may exchange heat with

258
A. A. Simoes et al.
the surrounding environment), the ﬁrst law can be written as follows. Along any
process, χ,
dH = δQ −δW.
(19)
Since our system is simple, that is, it is described by a single thermal variable
which in our formalism will be the entropy S, the one-form δQ can be written
as
δQ = TdS,
(20)
for some function T called the temperature. Furthermore, δW can be written
locally as follows
δW = Pidqi,
(21)
with as many functions Pi and qi as the rank of δW. Moreover, qi and S are
functionally independent. In the physical interpretation, Pi is the intensive vari-
able conjugate to qi. In the case that qi is a volume, Pi would be the pressure.
Hence, along χ, the following is satisﬁed
dH = TdS −Pidqi.
(22)
From this, by contracting with ∂/∂S, we obtain the relationship
T = ∂H
∂S .
(23)
Furthermore, for an isolated system, the energy must be constant along χ. Hence,
we must have the relationship
0 = TdS −Pidqi,
(24)
or, dividing by the temperature, and identifying
pi = Pi/T,
(25)
we obtain
ηQ = dS −pidqi = 0
(26)
Hence, χ satisﬁes the ﬁrst law of thermodynamics for an isolated system if and
only if the energy is constant along χ and χ is tangent to ker ηQ.
The second law of thermodynamics, may be stated in the following form:
along any process χ the entropy S is a non-decreasing function.
From the previous comments and Proposition 1, we can extract the following
conclusion.
Proposition 3. As it is the case for isolated systems, the energy is constant
along the integral curves of EH, that is
dH
dt = 0.
Moreover, the time evolution of the entropy is locally given by
dS
dt = pi
dqi
dt ,
which is exactly the ﬁrst law of thermodynamics with pi = Pi/T.

The Geometry of Some Thermodynamic Systems
259
Remark 2. Note that the ﬁrst law of thermodynamics for an isolated system
may be geometrically written as a tangency condition, that is,
iEHη = 0.
The second law of thermodynamics follows from the expression of the evolu-
tion vector ﬁelds (7), (12) and depends on the choice of Hamiltonian function.
Proposition 4. The integral curves of EH (respectively ΓL) satisfy the Second
law of thermodynamics, that is,
dS
dt ≥0
(27)
if and only if ΔQ(H) ≥0 (respectively, Δ(L) ≥0).
Remark 3. In this formulation temperature is not an independent variable of our
system. This is a consequence of choosing the entropy as our thermal variable.
Since we assumed that our system is simple, one thermal variable is enough to
completely characterize its state. Nonetheless, in other formulations of thermo-
dynamics, such as [5,37], a temperature variable T can be introduced by con-
sidering instead the symplectiﬁed manifold T ∗(Q × R). The redundancy on this
description is reﬂected in this formulation as a gauge symmetry. A symplectiﬁed
version of our theory might exist, but it has not yet been researched.
Remark 4. Although the integral curves of EH fulﬁll the laws of thermodynamics
in this formalism, it is not the case that any process fulﬁlling the two laws of
thermodynamics can be described through the evolution vector ﬁeld EH for some
Hamiltonian. Indeed, the vector ﬁeld S∂/∂p is a trivial counterexample, provided
the energy does not depend on p.
However, the dynamics of some interesting systems is given by EH. Moreover,
they satisfy a variational principle (Proposition 2) that might be relevant to the
description of these processes.
4.2
Examples
Let the Hamiltonian H be given by
H(qi, pi, S) = 1
2gijpipj + V (q, S)
(28)
where (gij) is a symmetric bilinear tensor on Q. Note that all the integral curves
this system satisﬁes the second law of thermodynamics if and only if
ΔQ(H) = 2 gijpipj ≥0,
(29)
that is, if gij is a positive semideﬁnite metric.

260
A. A. Simoes et al.
This can also be expressed form the brackets deﬁned in (6). Indeed, we have
that
˙f = {f, H}T ∗Q + {f, H}ΔQ.
(30)
Obviously, {H, H}T ∗Q = {H, H}ΔQ = 0 (ﬁrst law) and {S, H}T ∗Q = 0 and
{S, H}ΔQ = ΔQH ≥0 (second law). Observe that in Eq. (30) both brackets
are using the function H as “generator”. This is the reason that typically this
formalism is known as single generator formalism [17].
Example 1. Linearly damped system
Consider a linearly damped system [2] described by coordinates (q, p, S), where
q represents the position, p the momentum of the particle and S is the entropy
of the surrounding thermal bath. We assume that the system is subjected to a
viscous friction force, proportional to the minus velocity of the particle.
The
system is described by the Hamiltonian
H(q, p, S) = p2
2 m + V (q) + γS,
γ > 0
and T = ∂H
∂S = γ > 0 represents the temperature of the thermal bath.
Therefore, the equations of motion for EH = ♯Λ(dH) are:
˙q = p
m
˙p = −V ′(q) −γp
˙S = p2
m
Obviously, the system is isolated since ˙H = 0 and it is also clear from the
equation for ˙S that the ﬁrst and second laws are satisﬁed since ˙S ≥0.
In the Lagrangian side we obtain the system given by the Lagrangian
L(q, ˙q, S) = m ˙q2
2
−V (q) −γS,
and the equations of motion are given by
m¨q = −V ′(q) −m ˙q
˙S = m ˙q2.
Observe that in this system the friction force is given by the map Ffr : TQ →
T ∗Q given by
Ffr(q, ˙q) = −mγ ˙qdq.
Therefore, the equation of entropy production can be rewritten in terms of
the friction force as follows
T ˙S = −⟨Ffr(q, ˙q), ˙q⟩

The Geometry of Some Thermodynamic Systems
261
These equations coincide with the set of equations proposed in [20,21] for this
particular choice of Lagrangian L and friction force Ffr. Observe that, in this
particular example where the temperature satisﬁes T = γ, the equations are
only deﬁned for values γ > 0 and thus we are only modelling thermodynamic
systems with non-zero temperature. Moreover, note that −Ffr is precisely the
work done by the friction forces δW with P = γp = γm ˙q. Hence, Eq. (24) is
satisﬁed.
Observe that the two brackets give
{H, g}Λ0 = p
m
∂g
∂q −∂g
∂pV ′(q)
{H, g}ΔQ = p2
m
∂g
∂S −γp∂g
∂p
and
EH(g) = ˙g = {H, g}Λ0 + {H, g}ΔQ.
Therefore it is clear that {H, H}Λ0 = 0 and {H, H}ΔQ = 0 (by skew-symmetry)
and {H, S}Λ0 = 0 and {H, S}ΔQ = γ
mp2 ≥0.
Example 2. A general potential function
Consider now a Hamiltonian function on T ∗Q × R, with dim(Q) = 1, given by
H(q, p, S) = p2
2 m + V (q, S).
In this case, the temperature would be
T = ∂H
∂S = ∂V
∂S .
Then, the equations of motion are just
˙q = p
m
˙p = −∂V
∂q −p∂V
∂S
˙S = p2
m
As in the previous example, we might construct the associated Lagrangian func-
tion
L(q, ˙q, S) = m ˙q2
2
−V (q, S)
and the equations of motion are given by
m¨q = −∂V
∂q −m ˙q ∂V
∂S
˙S = m ˙q2.

262
A. A. Simoes et al.
Observe that in this system the friction force is given by the map Ffr : TQ×R →
T ∗Q given by
Ffr(q, ˙q, S) = −m ˙q ∂V
∂S dq.
Therefore, the equation of entropy production can be rewritten in terms of the
friction force as follows
T ˙S = −⟨Ffr(q, ˙q), ˙q⟩
These equations also coincide with the set of equations proposed in [20,21] for
this particular choice of Lagrangian L and friction force Ffr. Since −Ffr is the
work done by the friction forces δW with P = Tp = Tm ˙q. Hence, Eq. (24) is
satisﬁed.
5
Composed Thermodynamic Systems Without Friction
In this section we will present a model for systems composed of at least two
subsystems exchanging heat with each other (see [10,35]). As we will see below,
the dynamical vector ﬁeld we obtain in this model is not associated to a con-
tact structure. This constitutes further evidence that contact geometry may be
insuﬃcient to describe thermodynamic systems.
Consider two thermodynamic systems indexed by 1 and 2 which may interact
through a conducting wall. On each system we have deﬁned the corresponding
Hamiltonian:
H : T ∗(Q1 × Q2) × R2 →R
where we consider coordinates (qα, pα, Sα) on T ∗Qα × R, α = 1, 2, where Sα are
the entropy variables of each subsystem and (qα, pα) are mechanical variables
describing mechanical states of the system.
In thermo-mechanical systems, as it is usual in Thermodynamics, the partial
derivative of the energy with respect to the entropy will be the temperature of
the system so that
Tα(q1, p1, S1, q2, p2, S2) = ∂H
∂Sα
(q1, p1, S1, q2, p2, S2)
denotes the temperature of subsystem α.
Consider the Poisson tensor ΛQ1×Q2×R on T ∗(Q1 × Q2) × R2 given by
ΛQ1×Q2×R = ΛQ1 + ΛQ2 +
∂
∂S1
∧
∂
∂S2
,
where ΛQα is the canonical Poisson tensor on T ∗Qi with α = 1, 2.
Assume that both subsystems exchange heat according to Fourier Law:
h = k(T2 −T1)
where k is the coeﬃcient of thermal conductivity. Suppose that Tα > 0.

The Geometry of Some Thermodynamic Systems
263
Consider the function K : T ∗(Q1 × Q2) × R2 →R
K = k

1
∂H
∂S1
−
1
∂H
∂S2
	
= k
 1
T1
−1
T2

which will be called Fourier factor. Deﬁne the two-tensor (with Fourier factor
K) denoted by ΛK, given by
ΛK = ΛQ1 + ΛQ2 + K ∂
∂S1
∧
∂
∂S2
Observe that now ΛK is a skew-symmetric almost Poisson structure [11].
The matrix representation of ΛK is:
⎛
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎜
⎝
0
In1×n1
0
0
0
0
−In1×n1
0
0
0
0
0
0
0
0
0
0
k

1
T1 −
1
T2

0
0
0
0
In2×n2
0
0
0
0
−In2×n2
0
0
0
0
k

1
T2 −
1
T1

0
0
0
⎞
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎟
⎠
where dim Qi = ni, i = 1, 2.
The corresponding evolution vector ﬁeld EH,K:
EH,K = ♯ΛK(dH)
(31)
The integral curves of EH,K are:
˙q1 = ∂H
∂p1
˙p1 = −∂H
∂q1
˙S1 = K ∂H
∂S2
˙q2 = ∂H
∂p2
˙p2 = −∂H
∂q2
˙S2 = −K ∂H
∂S1
.
(32)
Observe that the total entropy S = S1 + S2 satisﬁes
˙S = EH,K(S1 + S2)
= k(T2
T1
−1) + k(T1
T2
−1)
= k (T2 −T1)2
T1T2
⩾0
Moreover, in absence of external forces, the total energy H is conserved since by
skew-symmetry of ΛK we have that
EH,K(H) = 0,

264
A. A. Simoes et al.
hence the system is isolated.
Since the system presents no friction, there is no work done by friction forces.
Thus, the ﬁrst law of thermodynamics for this system is just given by
dH = δQ = δQ1 + δQ2 = T1dS1 + T2dS2.
(33)
It is easy to comprove that the equality is satisﬁed by the integral curves of
EH,K, since the system is isolated and so the energy is constant along curves.
Thus, we have shown that:
Proposition 5. Given a Hamiltonian function H, the integral curves of the
evolution vector ﬁeld EH,K in the skew-symmetric manifold (T ∗(Q×Q×R), ΛK),
describe the dynamics of an isolated system, that is,
dH
dt = 0,
which is composed by two thermodynamic subsystems without friction exchanging
heat with each other, satisfying the ﬁrst and second laws of Thermodynamics,
that is
dH = T1dS1 + T2dS2
and
˙S ⩾0.
Remark 5. The time evolution of the entropy variables S1 and S2 is closely
related to the choice of heat ﬂow function h. In fact, the construction of the
skew-symmetric structure ΛK always leads to the evolution equations
˙S1 = h
T1
and
˙S2 = −h
T2
which are the expected expressions for the entropy production.
Example 3. The simplest toy model for this case is the two free thermo-particles
example, composed by two particles at rest exchanging heat. The thermodynamic
phase space is simply R2 on which we deﬁne the Hamiltonian function
H(Sa, Sb) = cae
Sa
ca + cbe
Sb
cb ,
where ca, cb are the heat capacities of each particle. Then the evolution vector
ﬁeld satisﬁes the equations
˙Sa = κ
Tb −Ta
Ta

˙Sb = κ
Tb −Ta
Tb

,
where the temperatures are the functions given by
Ta = ∂H
∂Sa
,
Tb = ∂H
∂Sb
and hence
Ta = e
Sa
ca ,
Tb = e
Sb
cb .

The Geometry of Some Thermodynamic Systems
265
Example 4. A slight sophistication of of the previous toy model is the two free
thermo-particles example, now composed by two particles moving freely on the
manifolds Qa, Qb. The thermodynamic phase space is now given by (T ∗Qa ×
R) × (T ∗Qb × R) on which we deﬁne the Hamiltonian function
H(qa, pa, Sa, qb, pb, Sb) = 1
2
 p2
a
ma
+ p2
b
mb

+ cae
Sa
ca + cbe
Sb
cb ,
where ma, mb are the masses of each particle.
Example 5. Now, to obtain more interesting examples, we may add to the
previous example a potential function depending on the position variables
V : Qa × Qb →R. Indeed, a physical example is the two thermo-spring sys-
tem (cf. [10]).
In this case, the system is modelled by a Hamiltonian function of the type
H(qa, pa, Sa, qb, pb, Sb) = 1
2
 p2
a
ma
+ p2
b
mb

+ V (qa, qb) + cae
Sa
ca + cbe
Sb
cb .
6
Geometric Integration of Thermodynamic Systems
Numerical methods for general thermodynamic systems are implemented usually
using the metriplectic formalism (see [19,30]). However, in our case, for the
examples that we are considering, we can easily adapt the construction of discrete
gradient methods to the bivector Λ.
For simplicity, we will assume that Q = Rn. Then the systems that we want
to study are described by the ODEs
˙x = (♯Λ)x(∇H(x)),
with x = (qi, pi, S) ∈T ∗Q × R, the map H : T ∗Q × R →R is the Hamiltonian
function and ∇H(x) ∈X(T ∗Q×R) is the standard gradient in T ∗Q×R identiﬁed
with R2n+1, with respect to the Euclidean metric.
Using discretizations of the gradient ∇H(x) it is possible to deﬁne a class of
integrators which preserve the ﬁrst integral H exactly.
Deﬁnition 2. Let H : RN −→R be a diﬀerentiable function. Then ¯∇H :
R2N −→RN is a discrete gradient of H if it is continuous and satisﬁes
¯∇H(x, x′)T (x′ −x) = H(x′) −H(x) ,
for all x, x′ ∈RN ,
(34a)
¯∇H(x, x) = ∇H(x) ,
for all x ∈RN .
(34b)
Some examples of discrete gradients are (see [29] and references therein)
• The mean value (or averaged) discrete gradient given by
¯∇1H(x, x′) :=
 1
0
∇H((1 −ξ)x + ξx′)dξ ,
for x′ ̸= x .
(35)

266
A. A. Simoes et al.
• The midpoint (or Gonzalez) discrete gradient given by
¯∇2H(x, x′) := ∇H
1
2(x′ + x)

(36)
+ H(x′) −H(x) −∇H
 1
2(x′ + x)
T (x′ −x)
|x′ −x|2
(x′ −x) ,
for x′ ̸= x.
• The coordinate increment discrete gradient where each component
given by
¯∇3H(x, x′)i = H(x′
1, . . . , x′
i, xi+1, . . . , xn) −H(x′
1, . . . , x′
i−1, xi, . . . , xn)
x′
i −xi
1 ≤i ≤N, when x′
i ̸= xi, and
¯∇3H(x, x′)i = ∂H
∂xi
(x′
1, . . . , x′
i−1, x′
i = xi, xi+1, . . . , xn),
otherwise.
6.1
Simple Thermodynamic Systems with Friction
Let H : T ∗Q × R →R be the Hamiltonian function. If we choose the midpoint
discrete gradient ¯∇2H, it is straightforward to deﬁne an energy-preserving inte-
grator by the equation
xk+1 −xk
h
= ♯Λ
xk + xk+1
2
  ¯∇2H(xk, xk+1)

,
(37)
where Λ is the bivector associated to the canonical contact structure ηQ of
Q = R2n+1, given in local coordinates by (4).
As in the continuous case, it is immediate to check that H is exactly preserved
using (37) and the skew-symmetry of Λ
H(xk+1) −H(xk) = ¯∇2H(xk, xk+1)T (xk+1 −xk)
= hΛ( ¯∇2H(xk, xk+1), ¯∇2H(xk, xk+1)) = 0.
On the other hand, by (37) the entropy satisﬁes
Sk+1 −Sk = hΛ( ¯∇2H(xk, xk+1), dS).

The Geometry of Some Thermodynamic Systems
267
If H is of the form (28) with V a quadratic function then
H(xk+1) −H(xk) = dH
xk + xk+1
2

(xk+1 −xk).
In fact this is a well-known property of quadratic functions. Hence, we must have
dH
xk + xk+1
2

= ¯∇2H(xk, xk+1),
so that
Sk+1 −Sk = hΛ

dH
xk + xk+1
2

, dS

= h

pi
k + pi
k+1
2
	
∂H
∂pi
xk + xk+1
2

≥0,
since by (4) we have that
Λ(dqi, dS) = 0,
Λ(dpi, dS) = pi
and
Λ(dS, dS) = 0.
Example 6. Consider the Hamiltonian function H : T ∗Q →R given by
H(q, p, S) = p2
2 + q2
2 + γS,
(38)
where Q = R, which is the Hamiltonian function associated with the damped
harmonic oscillator.
Now, if we may apply the midpoint discrete gradient and the associated
integrator given by (37), we obtain the following integrator
q1 =2γhq0 −h2q0 + 4 hp0 + 4q0
2γh + h2 + 4
p1 = −2γhp0 + h2p0 + 4 hq0 −4p0
2γh + h2 + 4
S1 =S0h4 + (4S0γ + 4q2
0)h3 + (4S0γ2 −16p0q0 + 8S0)h2
(2γh + h2 + 4)2
+ (16S0γ + 16p2
0)h + 16S0
(2γh + h2 + 4)2
.
(39)
Of course, using Eqs. (39) we obtain an integrator with constant energy and
increasing entropy. In Fig. 1 we can see that the qualitative behaviour of the
integrator is fairly accurate, while in Fig. 2 we see the entropy increases at the
same rate as the exact one.

268
A. A. Simoes et al.
Fig. 1. Trajectory of (39): the initial data are q0 = 0, p0 = 10 and S0 = 0; the step
is h = 0.1 and γ = 0.1. We plot the positions qk and compare the integrator with the
integral curve of the evolution dynamics EH.
6.2
Composed Thermodynamic Systems
Let H : T ∗(Q1 × Q2) × R2 →R be the Hamiltonian function deﬁned in a
2(n1 + n2) + 2 manifold. Moreover, suppose that Qi is a ni-dimensional vector
space for i = 1, 2.
Fig. 2. Entropy of (39): using the same initial data and settings from Fig. 1, we plot
the error with respect to the exact motion.

The Geometry of Some Thermodynamic Systems
269
Using a discrete-gradient approach, given a Hamiltonian function H : Rn →
R we may ﬁnd the midpoint discrete gradient ∇2H : R2n →Rn. Then, in our
case, we may deﬁne an algorithm in the following way:
xk+1 −xk
h
= ♯ΛK
xk+1 + xk
2

(∇2H(xk, xk+1)),
where xk = (qk
1, pk
1, Sk
1 , qk
2, pk
2, Sk
2 ) is a point in T ∗(Q1 × T ∗Q2) × R2.
This method will lead to an energy preserving algorithm. Moreover, we have
the following result describing the evolution of the total entropy.
Lemma 2. If H is a quadratic function and Ti = ∂H
∂Si we have that
Sk+1 −Sk = hk (T2 −T1)2
T1T2
⩾0,
where Sk = Sk
1 + Sk
2 is the total entropy at step k.
Proof. If H is a quadratic function, the it is not diﬃcult to prove that
∇2H(xk, xk+1) = dH
xk + xk+1
2

.
Moreover, observe that for i = 1, 2
Sk+1
i
−Sk
i = hΛK (∇2H(xk, xk+1), dSi) .
Then using the hypothesis that H is a quadratic function, the previous equation
becomes
Sk+1
i
−Sk
i = hΛK

dH
xk + xk+1
2

, dSi

= ±hK ∂H
∂Sj
,
where j = 1, 2 and j ̸= i. So,
Sk+1 −Sk = Sk+1
1
−Sk
1 + Sk+1
2
−Sk
2
= hK(−T1 + T2)
= hk (T2 −T1)2
T1T2
⩾0.
Thus, when the Hamiltonian function is a quadratic function, we have a
geometric integrator satisfying the ﬁrst and second laws of Thermodynamics.
Example 7. In the thermo-particle example, described by the Hamiltonian func-
tion
H(Sa, Sb) = cae
Sa
ca + cbe
Sb
cb
the energy is constant by deﬁnition of the discrete gradient function and the
skew-symmetry of the structure ΛK. Moreover, the total entropy is strictly
increasing as it is shown in Fig. 3 and the temperatures converge to the same
value (see Fig. 4).

270
A. A. Simoes et al.
Fig. 3. Total entropy of the two thermal particle system. We used k = 1, T1 = 273.15,
T2 = 300, h = 0.1 over 500 steps.
6.3
“Variational Integration” of the Evolution Vector Field
Now, we propose to construct a numerical integrator for EL based on a similar
method to the discrete Herglotz principle [1,38].
Fig. 4. The temperature of each thermal particle in the system. We used k = 1,
T1(0) = 273.15, T2(0) = 300, h = 0.1 over 500 steps.
Let Ld : Q × Q × R →R be a discrete Lagrangian function. Then a possible
integrator for the evolution dynamics is
D1Ld(q1, q2, S1) + (1 + DSLd((q1, q2, S1))D2Ld(q0, q1, S0) = 0
(40)

The Geometry of Some Thermodynamic Systems
271
and the entropy is subjected to
S1 −S0 = (q1 −q0)D2Ld(q0, q1, S0).
(41)
Example 8. Consider again the Hamiltonian function (38) of the damped har-
monic oscillator. Since H is regular, we may consider the corresponding
Lagrangian function L : TQ × R →R given by
L(q, ˙q, S) = ˙q2
2 −q2
2 −γS.
A standard discretization of this Lagrangian function is given by means of a
quadrature rule like
Ld(q0, q1, S0) = (q1 −q0)2
2h
−h(q1 + q0)2
8
−hγS0.
The discrete Herglotz equations (40) together with (41) give the explicit inte-
grator
q2 = γh3q0 + γh3q1 + 4γhq0 −4γhq1 −h2q0 −2 h2q1 −4q0 + 8q1
h2 + 4
S1 = S0 + (q1 −q0)2
h
−hq2
1 −q2
0
4
.
(42)
In Figs. 5 we plot the integrator given by Eqs. (42). We see that the qualita-
tive behaviour of the integrator is also quite good. In fact, an open question is
whether the error can be improved by considering discrete Lagrangian functions
approximating well enough the exact discrete Lagrangian function.
As a last comment, the entropy for Eqs. (42) is increasing and the Hamilto-
nian oscillates before stabilizing around a constant value (cf. Fig. 6).
Fig. 5. Trajectory of (42): the initial data are q0 = 0, q1 = 1 and S0 = 0; the step is
h = 0.1 and γ = 0.1. We plot the positions qk and compare the integrator with the
integral curve of the evolution dynamics ΓL.

272
A. A. Simoes et al.
Fig. 6. Hamiltonian of (42): using the same initial data and settings from Fig. 5, we
plot the Hamiltonian function along the iterations of the integrator.
7
Conclusions and Future Work
In this paper, we have given a variational interpretation of the evolution vector
ﬁeld and clariﬁed its relationship with the laws of thermodynamics on isolated
systems. Furthermore, we have extended our theory to deal with composed ther-
modynamic systems without friction and we also provided geometric integrators
for this framework.
Nevertheless, there are still many open questions. Related to simple systems,
we would like to see if a similar formalism holds for open systems, in which the
number of particles and the chemical potentials have to be taken into account,
and for closed non-isolated systems, in which the energy is not necessarily con-
stant, such as the ones experimenting isobaric or isothermal processes.
In addition, Hamiltonians of the form (28) where gij is a Lorentzian met-
ric could have interesting applications to relativity, in particular, in black hole
thermodynamics. Though it is true that the second law of thermodynamics will
not hold for all integral curves of the evolution vector ﬁeld associated with a
semi-Riemannian metric, it does hold for time-like curves which are the ones
that describe the allowed trajectories for matter.
In what concerns multi-component systems, there is still much work to do.
Indeed, a Lagrangian formulation and a nonholonomic constraint of the type
δQ = 0 could be used to introduce a “variational principle” from which we derive
the integral curves of the evolution vector ﬁeld EH,K. Also, one could introduce
a similar formulation in order to account for friction. It could be interesting to
formulate it in such a way one could model thermo-visco-elastic systems with it
(cf, [10]). Finally, it is reasonable to think that our formalism could be generalized
without much eﬀort to the case of N subsystems exchanging heat with each
other. In order to accomplish this, we must consider a skew-symmetric tensor

The Geometry of Some Thermodynamic Systems
273
ΛK, encoding in component function Kij the heat ﬂux interchanged by particles
i and j, with i ̸= j ∈{1, ..., N}.
Acknowledgements. The authors acknowledge ﬁnancial support from the Spanish
Ministry of Science and Innovation, under grants PID2019-106715GB-C21, MTM2016-
76702-P, “Severo Ochoa Programme for Centres of Excellence in R&D” (CEX2019-
000904-S) and from the Spanish National Research Council, through the “Ayuda
extraordinaria a Centros de Excelencia Severo Ochoa” (20205CEX001). A. Simoes is
supported by the FCT (Portugal) research fellowship SFRH/BD/129882/2017 partially
funded by the European Union (ESF). The authors would also like to thank the refer-
ees for their useful comments and remarks that helped to improve the content of the
paper.
References
1. Simoes, A.A., de Le´on, M., Lainz, M., de Diego, D.M.: On the geometry of discrete
contact mechanics. J. Nonlinear Sci. 31(3), paper no. 53 (2021)
2. Simoes, A.A., de Le´on, M., Valc´azar, M.L., de Diego, D.M.: Contact geometry for
simple thermodynamical systems with friction. Proc. R. Soc. A Math. Phys. Eng.
Sci. 476(2241), 20200244 (2020)
3. Arnold, V.I.: Contact geometry: the geometrical method of Gibbs’s thermodynam-
ics. In: Proceedings of the Gibbs Symposium, New Haven, CT, 1989, pp. 163–179
(1990)
4. Balian, R., Valentin, P.: Hamiltonian structure of thermodynamics with gauge.
Eur. Phys. J. B Condens. Matter Phys. 21(2), 269–282 (2001)
5. Balian, R., Valentin, P.: Hamiltonian structure of thermodynamics with gauge.
Eur. Phys. J. B Condens. Matter Complex Syst. 21(2), 269–282 (2001)
6. Bravetti, A.: Contact Hamiltonian dynamics: the concept and its use. Entropy
19(12), 535 (2017)
7. Bravetti, A.: Contact geometry and thermodynamics. Int. J. Geom. Methods Mod.
Phys. 16(supp01), 1940003 (2018)
8. Bravetti, A., de Le´on, M., Marrero, J.C., Padr´on, E.: Invariant measures for con-
tact Hamiltonian systems: symplectic sandwiches with contact bread. J. Phys. A
53(45), 455205 (2020)
9. Cendra, H., Ibort, A., de Le´on, M., de Diego, D.M.: A generalization of Chetaev’s
principle for a class of higher order nonholonomic constraints. J. Math. Phys. 45(7),
2785–2801 (2004)
10. Mart´ın, S.C.: Energy-entropy-momentum time integration methods for coupled
smooth dissipative problems. Ph.D. thesis, ETSI Caminos, Canales y Puertos,
UPM (2016)
11. Dazord, P., Lichnerowicz, A., Marle, C.-M.: Structure locale des vari´et´es de Jacobi.
J. Math. Pures Appl. (9), 70(1), 101–152 (1991)
12. de Le´on, M., Valc´azar, M.L.: Contact Hamiltonian systems. J. Math. Phys. 60(10),
102902 (2019)
13. de Le´on, M., Valc´azar, M.L.: Singular Lagrangians and precontact Hamiltonian
systems. Int. J. Geom. Methods Modern Phys. 16(10), 1950158 (2019)
14. de Le´on, M., Valc´azar, M.L.: Inﬁnitesimal symmetries in contact Hamiltonian sys-
tems. J. Geom. Phys. 153, 103651 (2020)

274
A. A. Simoes et al.
15. de Le´on, M., Rodrigues, P.R.: Methods of Diﬀerential Geometry in Analytical
Mechanics, vol. 158. Elsevier, Amsterdam (1987)
16. de Le´on, M., Sard´on, C.: Cosymplectic and contact structures for time-dependent
and dissipative Hamiltonian systems. J. Phys. A: Math. Theor. 50(25), 255205
(2017)
17. Edwards, B.J., Beris, A.N.: Noncanonical Poisson bracket for nonlinear elasticity
with extensions to viscoelasticity. J. Phys. A 24(11), 2461–2480 (1991)
18. Eldred, C., Gay-Balmaz, F.: Single and double generator bracket formulations of
multicomponent ﬂuids with irreversible processes. J. Phys. A Math. Theor. 53(39),
395701 (2020)
19. Orden, J.C.G., Romero, I.: Energy-entropy-momentum integration of discrete
thermo-visco-elastic dynamics. Eur. J. Mech. A. Solids 32, 76–87 (2012)
20. Gay-Balmaz, F., Yoshimura, H.: A Lagrangian variational formulation for nonequi-
librium thermodynamics. Part I: discrete systems. J. Geom. Phys. 111, 169–193
(2017)
21. Gay-Balmaz, F., Yoshimura, H.: From Lagrangian mechanics to nonequilibrium
thermodynamics: a variational perspective. Entropy 21(1), 8 (2019)
22. Gay-Balmaz, F., Yoshimura, H.: From variational to bracket formulations in
nonequilibrium thermodynamics of simple systems. J. Geom. Phys. 158, 103812
(2020)
23. Godbillon, C.: G´eom´etrie diﬀ´erentielle et m´ecanique analytique. Hermann, Paris
(1969) OCLC: 1038025757
24. Gonz´alez, ´O.: Time integration and discrete Hamiltonian systems. J. Nonlinear
Sci. 6(5), 449–467 (1996)
25. Hermann, R.: Linear and tensor algebra. Robert Hermann, Mathematics Depart-
ment, Rutgers University, New Brunswick, N.J. Interdisciplinary Mathematics. II
(Algebra, with applications to physics and systems theory, Part II) (1973)
26. Itoh, T., Abe, K.: Hamiltonian-conserving discrete canonical equations based on
variational diﬀerence quotients. J. Comput. Phys. 76(1), 85–102 (1988)
27. Kaufman, A.N.: Dissipative Hamiltonian systems: a unifying principle. Phys. Lett.
A 100(8), 419–422 (1984)
28. Libermann, P., Marle, C.-M.: Symplectic geometry and analytical mechanics, vol-
ume 35 of Mathematics and its Applications. D. Reidel Publishing Co., Dordrecht
(1987). Translated from the French by Bertram Eugene Schwarzbach
29. McLachlan, R.I., Quispel, G.R.W., Robidoux, N.: Geometric integration using
discrete gradients. R. Soc. Lond. Philos. Trans. Ser. A Math. Phys. Eng. Sci.,
357(1754), 1021–1045 (1999)
30. Mielke, A.: Formulation of thermoelastic dissipative material behavior using
GENERIC. Contin. Mech. Thermodyn. 23(3), 233–256 (2011)
31. Morrison, P.J.: A paradigm for joined Hamiltonian and dissipative systems, vol.
18, pp. 410–419 (1986). Solitons and coherent structures (Santa Barbara, Calif.,
1985)
32. Mrugala, R.: Continuous contact transformations in thermodynamics. In: Proceed-
ings of the XXV Symposium on Mathematical Physics (Toru´n, 1992), vol. 33, pp.
149–154 (1993)
33. Mrugala, R., Nulton, J.D., Sch¨on, C., Salamon, P.: Contact structure in thermo-
dynamic theory. Rep. Math. Phys. 29(1), 109–121 (1991)
34. Portillo, D., Orden, J.C.G., Romero, I.: Energy-entropy-momentum integration
schemes for general discrete non-smooth dissipative problems in thermomechanics.
Internat. J. Numer. Methods Engrg. 112(7), 776–802 (2017)

The Geometry of Some Thermodynamic Systems
275
35. Romero, I.: Algorithms for coupled problems that preserve symmetries and the
laws of thermodynamics Part I: monolithic integrators and their application to
ﬁnite strain thermoelasticity. Comput. Methods Appl. Mech. Eng. 199(25–28),
1841–1858 (2010)
36. Van der Schaft, A., Maschke, B.: Geometry of thermodynamic processes. Entropy
20(12), 925 (2018)
37. van der Schaft, A., Maschke, B.: About some system-theoretic properties of port-
thermodynamic systems. In: Geometric science of information. Lecture Notes in
Computer Science, vol. 11712, pp. 228–238. Springer, Cham (2019)
38. Vermeeren, M., Bravetti, A., Seri, M.: Contact variational integrators. J. Phys. A,
52(44), 445206, 28 (2019)

Learning Physics from Data:
A Thermodynamic Interpretation
Francisco Chinesta1(B), El´ıas Cueto2, Miroslav Grmela3, Beatriz Moya2,
Michal Pavelka4, and Martin ˇS´ıpka4
1 ESI Group Chair @ PIMM Lab, Arts et Metiers Institute of Technology,
155 Boulevard de l’Hopital, 75013 Paris, France
Francisco.Chinesta@ensam.eu
2 Aragon Institute of Engineering Research, Universidad de Zaragoza,
Maria de Luna, s.n., 50018 Zaragoza, Spain
3 ´Ecole Polytechnique de Montr´eal, C.P. 6079 succ. Centre-ville, Montr´eal,
Qu´ebec H3C 3A7, Canada
4 Mathematical Institute, Faculty of Mathematics and Physics, Charles University,
Sokolovsk´a 83, 186 75 Prague, Czech Republic
Abstract. Experimental data bases are typically very large and high
dimensional. To learn from them requires to recognize important features
(a pattern), often present at scales diﬀerent to that of the recorded data.
Following the experience collected in statistical mechanics and thermody-
namics, the process of recognizing the pattern (the learning process) can
be seen as a dissipative time evolution driven by entropy from a detailed
level of description to less detailed. This is the way thermodynamics
enters machine learning. On the other hand, reversible (typically Hamil-
tonian) evolution is propagation within the levels of description, that
is also to be recognized. This is how Poisson geometry enters machine
learning. Learning to handle free surface liquids and damped rigid body
rotation serves as an illustration.
Helena: Will they be happier when they can feel pain?
Dr. Gall: On the contrary. But they will be technically more perfect.
—Karel ˇCapek, R.U.R. (Rossum’s universal Robots).
1
Introduction
An ideal gas that is left undisturbed reaches a state, called an equilibrium state,
at which its behavior is found to be well described by the classical equilibrium
thermodynamics. The features of the ideal gas that play an important role in
the classical equilibrium thermodynamics are thus revealed in the process that
prepares the ideal gas for equilibrium thermodynamics. The equation govern-
ing the time evolution describing the preparation process has been introduced
at the end of nineteen century by Ludwig Boltzmann [1]. The equation is now
called the Boltzmann equation. The equilibrium thermodynamics emerges from
c
⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2021
F. Barbaresco and F. Nielsen (Eds.): SPIGL 2020, PROMS 361, pp. 276–297, 2021.
https://doi.org/10.1007/978-3-030-77957-3_14

Learning Physics from Data: A Thermodynamic Interpretation
277
its solutions as features of the solutions that survive the dissipation eliminating
gradually in the course of the time evolution the irrelevant details. The “Natu-
ral Intelligence” (NI), c.f. [2], entering the dissipation-driven pattern recognition
process is the realization that the binary collisions are the principal culprits of
the disorder generation that creates the irrelevant details and makes the pattern
to emerge. Due to very fast and very large changes of the gas particle trajecto-
ries that occur during the collisions, details of the trajectories are escaping our
attention that is speciﬁed by choosing only the one particle distribution function
as the variable describing states of the ideal gas. Such loss of details enters the
Boltzmann equation as a new dissipative term that breaks the time reversibility
of mechanics and brings solutions eventually to equilibrium states.
Can we see the dissipation-driven pattern recognition process1 as a result
of a data-driven learning? Let us assume that we have in our disposition tra-
jectories of a large number of gas particles. This is our data base with which
we begin our investigation. We now apply to the data base the methods Proper
Orthogonal Decomposition (POD), Locally Linear Embedding (LLE), Topolog-
ical Data Analysis (TDA), etc. developed in [3–6], for ﬁnding a structure in the
data base. We conjecture that such analysis would lead to the same structure as
the one revealed in the Boltzmann dissipation-driven pattern recognition pro-
cess. In other words, we conjecture that “Artiﬁcial Intelligence” (AI) analysis
also reveals the Boltzmann insight that the binary collisions represent the essen-
tial physics involved in the possibility to use equilibrium thermodynamics for
describing the experimentally observed behavior of ideal gases.
We shall use hereafter the following terminology. NI modeling is the “natural
intelligence” modeling that has unfolded from Newton’s formulation of mechan-
ics. An NI model is the time evolution Eq. (1). AI modeling is the “artiﬁcial
intelligence” modeling that is also referred to as machine learning. Data base
plays important but diﬀerent roles in both NI and AI modeling.
In the NI modeling the data base serves ﬁrst only as one of the inspirations
leading to a physical insight needed to write down the time evolution Eq. (1)
that then represents the NI model. Equation (1) is subsequently solved and its
solutions (i.e. predictions of the NI model) are compared with the data base. The
comparison is the process of the validation of the NI model. The real beginning of
the NI modeling is the time evolution Eq. (1). The NI modeling is rather insight
driven than data driven. The data base however participates in the formulation
of Eq. (1) and then continues to inspire also the process of solving it (see more
in Sect. 2) and ﬁnally it is used to validate the NI model.
In the AI modeling the data base is the principal input. The AI modeling is
truly data driven [7–9]. The objective is to formulate Eq. (1) (or an equivalent
to it set of instructions for computer that allow to simulate the time evolution)
that generates the data base. The beginning of the AI modeling is thus the data
1 In this work we mean pattern recognition in a broad sense as a process of extract-
ing any information from the data. The dissipative-driven pattern recognition can
be then imagined as a retouche of the original data leading to recognition of the
important aspects.

278
F. Chinesta et al.
base, its ﬁnal result is the physical insight introduced at the beginning of the
NI modeling in the form of Eq. (1). In this sense, the AI modeling is a learning
process.
In this paper we recall ﬁrst (in Sect. 2) some aspects of the NI modeling that,
as we show in Sect. 3, play very likely an important role also in the machine learn-
ing. We focus our attention in particular on the passage from the time evolution
Eq. (1) to a simple time evolution equation in which the essential overall features
(the pattern in solutions of (1)) became manifestly displayed and unimportant
details were ignored. Such passage is a principal step in getting an insight needed
to make predictions based on (1). Such passage is also, as we recall in Sect. 2, a
general formulation of thermodynamics. Our principal objective in this paper is
thus to contribute to the development of thermodynamics of machine learning.
In trying to recognize common features in the NI and AI modeling we follow the
spirit of Machine Learning via Dynamical Systems proposed in [10,11].
The analysis presented in Sects. 2 and 3 are illustrated in Sect. 4 on the
example of free surface ﬂuid ﬂows discussed already in [12] and in Sect. 5 on
learning from evolution of a freely rotating damped rigid body. It is shown that
by learning only mechanics (rigid body without dissipation), one can not learn
the enrgy completely. The problem is that the energy can be shifted by a Casimir
invariant (here magnitude of the angular momentum) leaving the mechanics
unaltered. However, when learning also dissipation, the Casimirs are learned as
well, and one can infer the whole expression for energy.
Novelty of this paper lies in the following points. Learning is dictated by
entropy production, i.e. removing details and capturing ﬁrst order insights. This
is the main aim of dimensionality reduction (linear and nonlinear). When learn-
ing physics, thermodynamics is the appropriate framework for accomplishing it
safely and precisely. This provides a thermodynamic interpretation of the rather
numerical approach from [12]. It is moreover important to recognize both the
projection and the inverse embedding between the diﬀerent detailed and less
detailed manifolds (scales), as within the MaxEnt framework. We learn from
detailed data, by removing details, etc. Then we predict in the reduced space, in
which we created our (reduced) model, but we validate in the rich space, and for
that the embedding is needed. At least when addressing physics, both scales are
thermodynamically linked and we move from one to the other for coming back
later. This thermodynamic link can be exploited in the numerical algorithms.
Moreover, learning can be enhanced by recognition of the geometric structure
generating the evolution, as for instance in Sect. 5 when learning kinetic energy
of rigid body from trajectory of its angular momentum.
2
Pattern Recognition in Statistical Physics and
Thermodynamics
In this section we recall some ideas and methods that have emerged in statistical
mechanics and thermodynamics and that, as shown in Sect. 3, are also pertinent
in machine learning.

Learning Physics from Data: A Thermodynamic Interpretation
279
2.1
Reduction and Pattern Recognition
Consider a manifold M with coordinates x ∈M, and assume that there is a
vector ﬁeld X ∈X(M) on the manifold. The vector ﬁeld determines a ﬂow on
the manifold. In other words, components of the vector ﬁeld are the right hand
sides of evolution equations for x,
˙xi = Xi(x),
(1)
and the evolution simply follows arrows of the vector ﬁeld. We note that if
the system under investigation is a physical system composed of atoms and
molecules, then one possible model in the form of Eq. (1) is in principle known.
The state variable x consists of the position vectors and momenta of all the par-
ticles involved (provided we limit ourselves to the classical mechanics) and the
vector ﬁeld X is the vector ﬁeld of classical mechanics (right hand side of Hamil-
ton canonical equations). To specify it we need to know (or assume to know) all
the forces participating in the time evolution. If the system under investigation
is still a physical system, but the data base addresses some macroscopic features
(e.g. ﬂuid ﬂows), then x has to address the quantities entering the data base and
an additional insight is needed to formulate the vector ﬁeld X.
Now we turn to the problem of solving Eq. (1), i.e. to the problem of ﬁnding
the ﬂow generated by (1). There are two routes to follow. On the ﬁrst route we ﬁnd
all details of the trajectories generated by (1). This, of course, is in general a very
diﬃcult task even for very well performing computers. Moreover, the result, i.e.
the phase portrait generated by (1), still needs to be subjected to a pattern recog-
nition process in order to be useful. The complexity of the phase portrait has to be
reduced by highlighting important features and ignoring unimportant details. On
the second route the objective is not to ﬁnd all the details of solutions of Eq. (1)
but only their important qualitative features. We shall follow the second route.
Consider a projection π : M →N, range of which determines a reduced
manifold N. An insight (inspired also by the data base in our disposition) is
needed to specify the projection π. As an example, we take (1) to be the Boltz-
mann kinetic equation (i.e. x is the one particle distribution function) and π
the projection to hydrodynamic ﬁelds (that are the ﬁrst ﬁve moments of the
distribution function in the velocity variable).
The projection π maps each point x ∈M to a point y ∈N. To each point x
there is an arrow attached (vector ﬁeld X), and this arrow (an instruction how
to proceed in the time evolution in M) can be also mapped to the tangent bun-
dle of N, i.e. to a vector tangent to N attached to a point y ∈N. The projected
vector ﬁeld then generates the time evolution in N. However, in a thermody-
namic setting—this is not the case in projection-based model reduction—there
are typically many points from M projected to single y ∈N, there are many
vectors to be attached to y. How to choose the right one (i.e. the one expressing
properly the induced ﬂow on N) and consequently how to determine the vector
ﬁeld on Y ∈X(N),
˙ya = Y a(y)
(2)
representing the reduced dynamics?

280
F. Chinesta et al.
In order to answer this question we need again an insight. Imagine a phase
portrait where trajectories of a dynamical system are depicted. For a physical
system it is usually possible to ﬁnd a pattern where typical trajectories are
contained, see e.g. [13]. When starting somewhere in the phase space, the point
typically evolves towards the pattern. The reduction introduced above takes the
phase space (or the vector ﬁeld generating it) and ﬁnds a reduced manifold where
typical evolution takes place, i.e. leads to the pattern recognition. In particular,
geometry of the reversible evolution on N is inherited from the geometry on M.
It is therefore not surprising to anticipate (see more in Sect. 3) that dynamic
reductions provide inspiration for machine learning and vice versa. Let us now
recall several methods of the dynamic reduction.
2.2
Reducing Dynamics, Thermodynamics
We begin with an example. Let M be the state space of kinetic theory (i.e., the
physical system under investigation is a gas and x is the one-particle distribution
function) and N is the state space of the classical equilibrium thermodynamics
(i.e., y = (V, N, E) ∈R3, where V is the volume of the region in R3 in which
the gas under investigation is conﬁned, N is the number of moles, and E is the
total energy of the microscopic particles composing the gas). In this case, no
time evolution takes place in N. The projection π is thus the projection on the
ﬁxed points of the time evolution taking place in M. Let us assume that the
models in M and in N have been validated by their corresponding data bases.
The question that we ask now is of what we have learned by relating the
two models, i.e., by reducing the model in M to the model in N. If the model
in N was a model with the time evolution then we would clearly obtain the
time evolution in N as a reduced dynamics and thus learn how to see the time
evolution in N from the point of view of M. But in the case when the model
in N is the equilibrium thermodynamics (i.e., there is no time evolution in
N, there is no reduced dynamics) the question becomes particularly pertinent.
Following Boltzmann, we answer the question as follows. A part of the data base
corresponding to the kinetic theory is an observation of the process that prepares
the gas under investigation to states at which its behavior can be well described
by the model in N. According to Boltzmann, the time evolution describing
the preparation process is governed by the Boltzmann equation. It is the time
evolution generated by the Boltzmann equation that makes the projection π. We
call the dynamics making the projection π a reducing dynamics. The dynamics
expressed in the Boltzmann equation is thus an example of reducing dynamics.
Following solutions to the Boltzmann equation, kinetic theory becomes reduced
to equilibrium thermodynamics.
The potential driving the reduction is called an entropy in M. We shall call
it an upper entropy ↑S. This potential, if evaluated at the states in M reached
as t →∞, becomes the entropy in N, called a lower entropy ↓S. In the case of
N being the state space of the equilibrium thermodynamics, ↓S is the entropy

Learning Physics from Data: A Thermodynamic Interpretation
281
S(V, N, E) entering the model in N. The reduction from M thus gives us the
fundamental thermodynamic relation in N.
Following [14–17], the reducing dynamics to the equilibrium thermodynam-
ics is expressed mathematically by the General Equation for Non-Equilibrium
Reversible-Irreversible Coupling, GENERIC,
˙xi = ↑L
ij ∂↑E
∂xj + ∂Ξ
∂x∗
i

x∗
i = ∂↑S
∂xi
.
(3)
The Boltzmann kinetic equation as well as many other equations (e.g., the
Navier-Stokes-Fourier of ﬂuid mechanics) expressing dynamics in other state
spaces M (see [14–17]) are particular examples of Eq. (3). We now explain the
meaning of the symbols appearing on the right hand side of Eq. (3).
The ﬁrst part of the right hand side is the Hamiltonian evolution, constructed
from the Poisson2 bivector ↑L and the gradient of energy ↑E. Hamiltonian
dynamics conserves energy (due to the antisymmetry of ↑L) and entropy (due to
the requirement that ↑S is the Casimir of Poisson bracket, i.e., the requirement
that
↑L
ij ∂↑S
∂xj = 0
∀i.
(4)
The second term in Eq. (3) is a gradient dynamics, where x∗
i are conjugate
variables and Ξ(x, x∗) is a dissipation potential with convex dependence on them
(see more in [17]). From the convexity it follows that
˙
↑S =

x∗
i
∂Ξ
∂x∗
i
 
x∗
i = ∂↑S
∂xi
≥0,
(5)
Moreover, the dissipation potential Ξ and entropy ↑S have to be such that energy
↑E is conserved in the gradient dynamics. These properties of ↑E, ↑S, ↑L, Ξ,
together with the convexity of ↑S and the requirement that Ξ reaches its min-
imum at x∗= 0, makes it possible to regard (−↑S) as a Lyapunov function
displaying the approach, as t →∞, to the equilibrium states at which the
entropy ↑S reaches its maximum. Such states then form N ⊂M (in the sense
that N be isomorphic to a submanifold of M). The Hamiltonian mechanics is
moreover reversible with respect to time-reversal transformation while gradient
dynamics is irreversible [18], and generalized Onsager reciprocal relations [19–22]
are automatically fulﬁlled, see [16,17,23].
Let us assume now that we are projecting from M to N on which the time
evolution still takes place. In the next subsection we shall discuss the reduced
dynamics, i.e. the projection of the vector ﬁeld X ∈X on the the vector ﬁeld
Y ∈X(N). For a moment, we assume that the reduced dynamics is known. It
has been conjectured in [24,25] that (3) with an appropriate modiﬁcations of the
properties required from ↑E, ↑S, ↑L, Ξ, expresses also reducing dynamics to N
on which the time evolution takes place. In such case, the result of the dynamic
2 The Poisson bracket corresponding to the Poisson bivector is {F, G} = ⟨Fx|↑L|Gx⟩,
where ⟨•|•⟩denotes a scalar product.

282
F. Chinesta et al.
reduction is the reduced dynamics (that we discuss in more detail in the next
subsection) and thermodynamics in N that is inherited from the entropy ↑S
generating the reducing time evolution leading from M to N.
Summing up, we see that the dynamical reduction from M to N, that can be
seen as a process of learning the model in N from the model in M, makes pos-
sible to see the dynamics in N as a reduced dynamics from M and, in addition,
introduces into N a new element that has been absent in the original model in
N. The new element is thermodynamics. It is the fundamental thermodynamic
relation in N expressed in the entropy ↓S. If the model in N is the equilibrium
thermodynamics, then the fundamental thermodynamic relation arising in the
dynamical reduction is the fundamental thermodynamic relation constituting
the model in N (i.e. the equilibrium thermodynamics). If, on the other hand,
the model in N involves the time evolution, then such model does not (at least in
general) involve any thermodynamic relation and thus the fundamental thermo-
dynamic relation arising in the dynamic reduction is a new information obtained
from seeing the model in N from the point of view of the more detailed model
in M.
Still another thermodynamics in N arises if we regard the model in N as a
more detailed than another model in N. The upper entropy ↑S appearing in (3)
with x replaced by y, i.e. the upper entropy ↑S generating the time evolution
from N to N, introduces thermodynamics in N (that is diﬀerent from the ther-
modynamics introduced by ↓S) obtained from seeing the model in N as a basis
for reduction to a less detailed model in N.
Finally, we note that if we are interested only in the result of the time evo-
lution generated by (3), then we can replace (3) by simply a MaxEnt reduction
which consists of the maximization of the upper entropy ↑S subjected to the
constraints π(x), as shown in the appendix of [23]. The Lagrange multipliers
in this maximization are y∗. This is indeed the principle of maximum entropy
(MaxEnt) formulated by Shannon [26] and Jaynes [27]. The question that arises
in this static viewpoint of the reduction is of what is the entropy ↑S, how shall
we ﬁnd it. In the dynamical viewpoint the upper entropy ↑S is the potential
generating the reducing time evolution (that is, in general, a part of the data
base associated with the model in M). In the static viewpoint of the reduction
one has to turn to other insights (see [26] and [27] for more details).
2.3
Reduced Dynamics
We turn our attention now to the reduced dynamics, i.e., to the projection of
X ∈X to Y ∈X(N).
Perhaps the simplest method of projecting X ∈X to Y ∈X(N) is provided
by MaxEnt. Pick one point y ∈N. Due to the MaxEnt embedding there is an
associated point π∗(y) ∈M. Take the vector attached to that point and project
it to y. The vector ﬁeld Y ∈X(N) obtained by repeating this for each y ∈N is
the MaxEnt projection of X onto N
Y a(y) = ∂πa
∂xi

x(y)Xi(x(y)).
(6)

Learning Physics from Data: A Thermodynamic Interpretation
283
But this vector ﬁeld has a drawback. The trajectories obtained by solving evo-
lution equations ˙y = Y approximate poorly the trajectories on the M manifold.
This is because the approach towards states with higher entropy is not explicitly
contained in Y . Therefore, a more precise approximation is needed, see [28,29].
A classical example of reduction beyond MaxEnt is the Chapman-Enskog
expansion [30]. Let M be the state space of kinetic theory (i.e., the physical sys-
tem under investigation is a gas and x is the one-particle distribution function)
and N is the state space of the hydrodynamics (i.e., hydrodynamic ﬁelds of den-
sity, momentum density and energy density, y = (ρ, u, e)). In this case, the time
evolution takes place in N is often well described by the Navier-Stokes-Fourier
system of equations, see e.g. [22], obtained by the Chapman-Enskog expansion.
The projection π is the projection on the ﬁrst 5 moments of the distribution func-
tion, and the detailed Boltzmann equation (vector ﬁeld X) is reduced to less
detailed Navier-Stokes-Fourier equations (vector ﬁeld Y ). The upper entropy ↑S
is the Boltzmann entropy and it generates a lower-level entropy ↓S, expressed
by the Sackur-Tetrode relation for ideal gases [17,31]. The embedding π∗is the
MaxEnt mapping from hydrodynamic ﬁelds to the locally Maxwellian distri-
bution functions. The locally Maxwellian distribution functions form the local
equilibrium submanifold of M, which is isomorphic to N. When the evolution
in M takes place close to the local equilibrium submanifold, the evolution in N
is close to the detailed evolution in M. The Chapman-Enskog expansion, how-
ever, also has a few drawbacks. Firstly, it relies on the a priori unknown form of
asymptotic expansion and, secondly, it requires the presence of dissipative terms
in vector ﬁeld X.
Another robust method of projecting the vector ﬁeld X to Y was formu-
lated by Bruce Turkington in [32]. The reduction consists of the following steps.
Consider a manifold M. Liouville equation for the probability distribution func-
tion on the manifold is formulated, and linear projection from the distribution
function is deﬁned, range of which determines a manifold N. Shannon entropy
is assumed for the distribution function, which forms and embedding π∗of N
onto M.
Let us ﬁrst project Hamiltonian mechanics on M (the Liouville equation) to
Hamiltonian mechanics on N. The upper3 Poisson bivector ↑L is projected as a
twice contravariant tensor ﬁeld on the space of state variables and, if necessary,
evaluated at the MaxEnt embedding,
↓L
ab =
∂πa
∂xi
↑L
ij(x)∂πa
∂xi
 
x=π∗(y).
(7)
To construct the Hamiltonian vector ﬁeld on the lower level one further needs a
Hamiltonian, energy on the lower level.
Let energy on M be ↑E(x). Energy on the lower level N is inherited from
the higher level through the MaxEnt mapping ↓E(y) = ↑E(π∗(y)). However,
since some energy modes present on the higher level have already been damped
3 The more detailed level is referred to as the upper while the less detailed (reduced)
as lower.

284
F. Chinesta et al.
on the lower level, typically ↓E(π(x)) ̸= ↑E(x). If the latter relation were an
equality, one could project the higher-level evolution to the lower-level easily as
one would obtain that time derivative of π(x) be equal to ↓L · d↓E, which would
be the lower-level purely Hamiltonian vector ﬁeld. Since, however, the equality
typically does not hold, simple projection does not give the desired result.
Instead, a lack-of-ﬁt Lagrangian is deﬁned which compares projections of the
exact trajectories on M with trajectories on N. Minimization of the Lagrangian
then leads to a GENERIC evolution on N and gives a dissipation potential driv-
ing thermodynamic evolution on N. The method has recently been generalized
in [33].
Still another method of constructing the reduced vector ﬁeld is the Ehrenfest
method developed in [34–36] and [28]. The method has the following ingredients:
detailed manifold M equipped with entropy and with a vector ﬁeld (evolution
equations), manifold N and projection π from M to N. MaxEnt then provides
the embedding of N into M as usually. The vector ﬁeld on M does not need to
have the GENERIC structure, but it is advantageous as shown in [37].
The vector ﬁeld X ∈X(M) is ﬁrst projected to a vector ﬁeld Y 0 ∈X(N)
by the MaxEnt projection. This vector ﬁeld, however, needs to be corrected.
Therefore, the vector ﬁeld X is lifted to the tangent bundle TM and subse-
quently projected back to M, which results in a smoothed vector ﬁeld on M,
ER(X(M)), which expresses a sort of overall motion on M, called Ehrenfest
regularization in [38]. The same is done with vector ﬁeld Y 0, which results in
vector ﬁeld ER(Y 0) ∈X(N). Finally, vector ﬁeld ER(X) is MaxEnt-projected
to N and compared with ER(Y 0). A correction term is then added to Y 0, form-
ing a new vector ﬁeld Y 1 ∈X(N), which makes ER(X) equal to ER(Y 1) (to a
given order of relaxation time parameter). Vector ﬁeld Y 1 then represents the
evolution on N, its components are right hand sides of evolution equations for
y ∈N. This is the Ehrenfest reduction of detailed evolution on M.
Another method of dynamic reduction is the Dynamic MaxEnt developed in
[23,29,39]. The main idea is to ﬁrst promote the conjugate variables x∗in the
GENERIC framework (3) to independent variables, which is natural from the
point of view of contact geometry [17,40]. The goal is to reduce a GENERIC
model for state variables on manifold M so that a fast variables relaxes and
becomes enslaved by the remaining slower variables, N being the manifold of
slow variables.
The fast variable is ﬁrst evaluated at the MaxEnt value determined by the
remaining state variables. But since the conjugate fast variable is still present in
the evolution equations for the slow variables, we need to express the conjugate
variable in terms of the remaining state and conjugate variables. The fast conju-
gate variable is found as the solution to the evolution equation of the fast state
variable evaluated at the MaxEnt value of the state variable. The conjugate fast
variable is thus determined by compatibility of the MaxEnt value of the fast
variable and the evolution equation for the fast variable. This way we end up
with a vector ﬁeld for the slow variables (on manifold N) compatible with the
MaxEnt embedding of the slow manifold into the original manifold.

Learning Physics from Data: A Thermodynamic Interpretation
285
3
Pattern Recognition in Machine Learning
Imagine now a robot [41] that is, for instance, supposed to perform a mechanical
task with a physical system, as e.g. in [12], while learning by itself how to operate
the system. The robot has as the input a set of discrete trajectories on M, G(M).
It should give as output an approximation of them by a low dimensional vector
ﬁeld which can be used to predict future evolution of the system approximately
(so that it can be operated in a reasonable way).
3.1
General Scheme
For simplicity we shall illustrate the machine learning using the Proper Orthog-
onal Decomposition (POD), but the general picture will be applicable also to
other methods. The problem is that the robot has discrete trajectories on a high-
dimensional manifold M, and it would be too costly to reconstruct the vector
ﬁeld X ∈X(M) from them; the vector ﬁeld would have too many dimensions.
Moreover, such high dimensional model would not provide the insight we look
for. The trajectories must be approximated by trajectories on a low dimensional
manifold N. Therefore, the task consists of the following steps:
1. Manifold recognition: Find a low-dimensional manifold N such that a projec-
tion of trajectories G(M) to N well approximates the original set G(M) of
trajectories on M. To accomplish this task, the robot needs the following:
a. To measure distances and deﬁne orthogonality, the robot needs a metric
on M, g(•, •), e.g. the l2 scalar product with or without weights.
b. Find a projection operator π : M →N.
c. To compare in M the trajectories G(M) with their projections to N,
which is the means of assessing “goodness” of the approximative manifold
N, the robot needs an embedding mapping π∗: N ×· · ·×N →M×· · ·×
M, mapping trajectories on N to trajectories on M. The embedding
is typically determined by MaxEnt in thermodynamics, but it is often
diﬃcult to construct it outside thermodynamics. Alternatively, the robot
can compare the trajectories on the reduced manifold N, for which the
embedding is not needed. On the other hand, the embedding will be
needed in the last step below anyway.
2. Recognition of the reduced vector ﬁeld: Once having the low-dimensional man-
ifold and projected trajectories π(G(M)), the goal is to ﬁnd a vector ﬁeld
Y ∈X(N) approximating the trajectories on N. This is done by choosing
an Ansatz on the form of the vector ﬁeld, e.g. GENERIC, and ﬁtting the
unknown parameter so that the trajectories on M and N coincide in a sense.
Once this step is successfully ﬁnished, the robot has recognized how the typ-
ical trajectories are created, he has learned how the system works.
3. To use this acquired knowledge, the robot is then supposed to integrate the
vector ﬁeld Y to future times in order to predict future states on the N man-
ifold. These states are then embedded into the M manifold of experimental
data by mapping π∗to obtain prediction of future states of on manifold M.

286
F. Chinesta et al.
Note that steps 1 and 2 can be seen as pattern recognition (manifold recognition
and vector ﬁeld recognition).
3.2
Reduced Manifold Recognition by POD
Let us now demonstrate the ﬁrst step (manifold recognition) on a standard
reduction method—the proper orthogonal decomposition (POD) or principal
component analysis (PCA), see e.g. [4,42].
3.2.1
Loss of Information
Let us have N time snapshots of m-dimensional experimental data, assuming
that m >> N, ordered to a N × m matrix Z. This matrix represents the high
dimensional trajectories on manifold of the data M. This matrix is now to be
approximated by POD. The core of POD is the singular value decomposition of
matrix Z,
Z = UΣV T ,
(8)
where U is an orthogonal N ×N matrix, V is an m×m orthogonal matrix and Σ
is an N × m matrix with entries only on the diagonal. The entries are the called
singular values, they are non-negative and ordered, σ1 ≥σ2 ≥. . . σN. Note that
there no information has been lost so far. The singular values are calculated as
square roots of eigenvalues of the symmetric positive deﬁnite N × N matrix
Q = ZZT = UΣΣT U T .
(9)
In this way we also obtain the matrix U, which consists of the eigenvectors of
Q. Now only k ﬁrst singular values are taken into account while setting σl = 0
for all l > k, which turns Σ to a new matrix ¯Σ. This is the crucial point where
reduction takes place. The advantage of SVD is that it gives the best possible
k-dimensional approximation of Z provided the l2 metric is used.
3.2.2
Projection
Finally—look at Eq. (8)—, the relevant part (ﬁrst k rows, since other are mul-
tiplied by zeroes) of matrix V is calculated from U T Z = ¯ΣV T def
= B. There are
k non-zero rows of this N × m matrix B, and these rows, denoted as vj ∈Rm,
form a basis of the k-dimensional submanifold N ⊂M. Step (a) in the above
abstract procedure is given made by choosing the usual l2 scalar product and
corresponding Frobenius norm. Step (b) is made by orthogonal projection π to
the basis of N,
N ∋y =
k

j=1
⟨x, vj⟩vj
∀x ∈M.
(10)

Learning Physics from Data: A Thermodynamic Interpretation
287
3.2.3
Embedding
Consider now a trajectory (y1, . . . , yN) on N. We construct an N × m matrix
Y (y1, . . . , yN) rows of which correspond to yi = k
j=1 cj
ivj. The embedding π∗
is then given by
π∗(y) = UY (y),
(11)
which is a trajectory on M. Step (c) has been ﬁnished. The POD method
took the set of trajectories on M, encoded it into matrix Z, and identiﬁed a
k−dimensional submanifold N ⊂M that approximates the trajectories Z. More-
over, there is an orthogonal projector π onto the basis of N and an embedding π∗
mapping trajectories on N to trajectories on M. Step 1, manifold recognition,
is thus ﬁnished.
3.2.4
Thermodynamics
We shall now look at the reduction described above through the eyes of ther-
modynamics recalled in Sect. 2.2. We regard the embedding of the projected
manifold N to the original M as a result of a learning time evolution which
has revealed the important features in the data base collected in M. We thus
interpret the reducing time evolution as the learning time evolution. This dissi-
pative evolution is generated by an entropy. Having the entropy and focusing our
interest only on the ﬁnal outcome of the learning time evolution, we can also see
the passage from M to N as maximization of the entropy (MaxEnt principle).
We now proceed to identify the entropy associated with POD.
The crucial step in POD where information is lost is the dropping of eigenval-
ues. We shall seek its thermodynamic interpretation. A way to calculate eigen-
values is based on minimization of the Rayleigh quotient in a dynamical system4,
see [45]. Let us interpret the Rayleigh quotient as entropy of a vector related to
a matrix A,
S(x) = xT · A · x
xT · x
.
(12)
Gradient dynamics of x is then prescribed as
˙x = ∂Ξ
∂x∗

x∗=Sx
= τ ∂
∂x
xT · A · x
xT · x
,
(13)
for Ξ = 1
2τ(x∗)2. The magnitude of x is conserved by the dynamical system, so we
can regard x to be normalized to unity. This dynamical system has stationary
points corresponding to eigenvectors of matrix A, and as it converges to the
stationary values, it converges to the eigenvectors. From the eigenvectors, the
eigenvalues can be recovered as the Rayleigh quotients, i.e. as the values of
entropy in the stationary states. Eigendecomposition can be seen as result of a
thermodynamic evolution.
However, as the matrix has typically more eigenvectors, the dynamical sys-
tem (13) has more stationary points. Typically it converges to the eigenvector
4 Another dynamical system converging to eigenvalues of a matrix was found in [43],
where the double bracket dissipation, geometrized in [44], was found.

288
F. Chinesta et al.
corresponding to the dominant eigenvalue (highest entropy), but there are other
lower eigenvalues (lower entropy) that also represent stationary solutions of the
system. By being restricted only to some region around the global maximum of
entropy, we obtain the information loss from POD.
Finally, the projection from all vectors normalized to unity (manifold M)
to the chosen eigenvectors (manifold N, also represented by the eigenvalues) is
simply the usual orthogonal projection to the span of the eigenvectors. Since the
eigenvectors are contained in the original manifold M, the embedding is trivial
(identity).
The reduction by POD, where only part of spectrum is considered while the
remaining eigenspaces being ignored, can be seen as a dynamic reduction driven
by entropy and implying a maximum entropy principle.
3.2.5
Comparison with Locally Linear Embedding
Locally linear embedding (LLE) [5] typically provides better approximation of
the low-dimensional manifold than POD. Let us therefore brieﬂy mention the
method. Starting with points x ∈M, a weight matrix Wij is found, which
provides local interpolation of points on M by their chosen number of neighbors.
Then points y ∈N are found as the points that are best interpolated by weights
Wij. This provides the projection π : M →N.
How to construct the embedding π∗: N →M? We see three possible routes:
(i) One can use a crude interpolation between y and x, as e.g. in [12]. (ii) One
can reverse the LLE procedure. Starting with points on N, constructing new
weights ¯Wij and ﬁnding x ∈M that are best interpolated by the new weights, as
suggested in [5]. (iii) Finally, one can reformulate the LLE projection as gradient
dynamics maximizing an entropy. The embedding could be then constructed by
the MaxEnt procedure with respect to that entropy. Let us comment on this
possibility in more detail.
The LLE algorithm consists of two steps, namely ﬁnding the weights Wij
and subsequently ﬁnding the projection π. Both the steps are formulated as
minimizations of certain cost functions. It can be therefore anticipated that LLE
can be reformulated as gradient dynamics. The ﬁrst step stands for minimization
of cost functions
ϵi(W) = (xi −

j
Wijηj)2,
(14)
where xi is the i-th vector from M and ηj is the j-th, j = 1, . . . , K, neighbor
of xi. Note that the choice of K and the notion of distance (metric on M) are
needed. Moreover, the weights are supposed to sum to one for each i, 
j Wij = 1,
since this is the gauge freedom of the cost function. By minimization subject to
the sum-to-one constraint one obtains
Wij = −λi

k
C−1(i)
jk
+

l
xi · ηlC−1(i)
jl
(15)

Learning Physics from Data: A Thermodynamic Interpretation
289
with C(i)
jk = ηj · ηk being the correlation matrix, C−1(i) is its inverse, and λi =
xi·
j ηj·
k C−1(i)
kj

jk C−1(i)
jk
being the Lagrange multiplier.
The second step is minimization of cost function
φ(y) =

i
(yi −

j
Wijyj)2
(16)
subject to the constraints that 
i yi = 0 and yi⊗yj ∝I, I being the d×d identity
matrix on the low-dimensional manifold. This step can be seen as eigenvalue
decomposition, and d eigenvectors are then the sought vectors yi, see [5] for
more details.
Therefore the LLE projection can be seen as eigendecomposition of matrix
Wij given by Eq. (15). It has already been noted in Sect. 3.2.4 that eigendecom-
position can be seen as gradient dynamics, which means that LLE itself can
be seen as gradient dynamics with entropy (12) for matrix (15) subject to the
constraints imposed on y.
The LLE projection can be seen as gradient dynamics with its own entropy.
Let us now assume that a position on the low-dimensional manifold y ∈N
is known. Can the entropy lead to a consistent construction of the embedding
π∗? We do not know the answer, but we would like to attract attention to this
question.
3.3
Reduced Vector Field
In Step 2 a vector ﬁeld Y on N is sought. We shall now regard the process of
identifying Y through the eyes of Sect. 2.3. The vector ﬁeld Y is found in such a
way that the trajectories on N corresponding to the vector ﬁeld are as close as
possible to the measured trajectories. The comparison can be made either on M
(embedding trajectories on N into M), or on N (projecting trajectories from
M onto N).
3.3.1
Prediction
Finally, Eq. (17) can be solved to obtain future trajectories on N. The embedding
then lifts the trajectories to future trajectories on M, which is a prediction of
future trajectories on M.
4
Illustration on Learning from Particle Dynamics
Let us now illustrate the foregoing theoretical construction on a recent success-
ful method of machine learning in dynamical systems [12]. The physical system
under investigation is a free-surface ﬂuid, the objective is to teach a robot how
to handle it. First, we address the NI modeling of such a system. The standard
modeling based on the classical ﬂuid mechanics with the Navier-Stokes equation
serving as the governing equation leads to a very complex mathematical formu-
lation. In order to avoid the diﬃculties associated with numerical solutions of

290
F. Chinesta et al.
partial diﬀerential equations, we choose the Lagrange formulation of ﬂuid ﬂows
(the ﬂuid is seen as composed of ﬂuid particles) and then still a simpler formula-
tion known as the method of Smoothed Particle Hydrodynamics (SPH), see [46],
and the method of Smoothed Dissipative Particle Dynamics (SDPD), see [47,48].
The data base DB presented to the robot thus consists of pseudo-experimental
data. These are the ﬂuid particle trajectories calculated as solutions to the sys-
tem of ordinary diﬀerential equations serving as the governing equations in the
SPH and SDPD formulations of ﬂuid ﬂows.
4.1
Smoothed Particle Hydrodynamics
First, we brieﬂy recall the SPH and SDPD methods. Imagine a ﬂuid motion.
Instead of the usual way based on partial diﬀerential equations (e.g., Navier-
Stokes equations), the ﬂuid can be described as composed of ﬂuid quasi-particles.
Dynamics of these particles is governed by Hamilton canonical equations, which
are ordinary diﬀerential equations. The particles are also equipped with their
energy or entropy, which makes it possible to addresses the thermodynamic
behavior, see e.g. [46,47].
Apart from the Hamiltonian part, the evolution equations also contain irre-
versible terms. These terms can be constructed by direct discretization of the
continuous viscous terms (as in SPH) or by including ﬂuctuations compatible
with the continuous terms through the ﬂuctuation-dissipation theorem (SDPD),
see e.g. [48].
4.2
Reduced Manifold
Let us now recall a recent successful approach to machine learning taking advan-
tage of the GENERIC framework [12]. In this approach a pseudo-experimental
data of ﬂuid motion are ﬁrst acquired from an SPH simulation, having a few
thousand particles, n being the number of particles. The detailed manifold M
is thus 7n-dimensional, since each particle has its position (3), velocity (3) and
energy (1). The measured states of the particles represent trajectories on M,
G(M).
Then three diﬀerent methods searching for a suitable lower-dimensional sub-
manifold are employed, namely POD (see above), locally linear embedding (LLE)
and topological data analysis (TDA). Each of the methods leads to a diﬀerent
manifold N. The best performance was given by TDA, where the manifold N
was consisting of a few particles5 (instead of a few thousand) while still giv-
ing reasonable approximation of the pseudo-experimental data. In all the three
5 It is often assumed that the reduced manifold keeps the structure of a cotangent
bundle, such that a reversible evolution is generated by the canonical Poisson bivec-
tor (equipped with entropy) as on the original manifold. Therefore, the reduced
dynamics can be interpreted as dynamics of a lower number of (quasi-)particles,
since otherwise an another Poisson bivector would have to be sought. This is not,
however, strictly necessary nor a limitation of the method, see for instance [49,50].

Learning Physics from Data: A Thermodynamic Interpretation
291
approaches, however, the reduced manifold N was similar to the original high-
dimensional manifold M in the sense that it also described pseudo-particle states
(although much lower number of them). The methods provided a projection π
from M to N as well as the embedding of N into M. This is the manifold
recognition.
4.3
Reduced Vector Field
In Step 2 a vector ﬁeld Y on N is sought. It is assumed that the vector ﬁeld on
N has the GENERIC structure
˙ya = ↓L
ab↓Eyb + ↓M
ab↓Syb,
(17)
where ↓L is a Poisson bivector, ↓E is an energy on N, ↓M is a dissipative matrix
on N and ↓S is an entropy on N.
Energy ↓E is assumed to be quadratic in y so that its gradient is linear
operator on y (a matrix), and the same is assumed for entropy ↓S. The dissi-
pative matrix6 is assumed to be piecewise constant—data are ﬁtted by regions,
not necessarily monolithically—, symmetric and positive deﬁnite. The unknown
matrices ↓Eyb, ↓Syb and ↓M are then ﬁtted by least squares so that the trajec-
tories given by integration of Eq. (17) coincide with projection of the measured
trajectories as much as possible. Least squares can also be interpreted as a result
of gradient dynamics [51], which means that reduction takes place in that step.
Note that Eq. (17) can be simpliﬁed to
˙ya = ↓L
ab↓Fyb −T ↓M
ab↓Fyb
(18)
for isothermal systems. Here ↓F = ↓E −T ↓S is the Helmholtz free energy. In
this case only two matrices would be necessary.
4.4
Prediction
Finally, Eq. (17) are solved to obtain future trajectories on N. The embedding
then lifts the trajectories to future trajectories on M, which is a prediction of
future trajectories on M, showing remarkable precision in [12].
5
Illustration on Learning Rigid Body Mechanics
Let us now illustrate the power of geometry when learning energy of a rigid body
from observation of its instantaneous axis of rotation. Imagine a freely rotating
rigid body, see e.g. [52]. Its rotations form the group SO(3), playing the role of
the detailed manifold M. Rate of rotation of the rigid body is expressed by the
its angular momentum regarded from the reference frame attached to the body,
6 Corresponding to dissipation potential Ξ = 1
2y∗
a
↓M
aby∗
b .

292
F. Chinesta et al.
m, playing the role of state variables x. By the Poisson reduction technique [53–
55] it is possible to show that dynamics of the angular momentum is generated
by non-canonical Poisson bracket
{F, G} = −m ·
 ∂F
∂m × ∂G
∂m

,
(19)
where F and G are two arbitrary functions of m. This bracket implies evolution
equation
˙m = m × ∂E
∂m,
(20)
where E(m) is kinetic energy of the body. The energy is quadratic in m,
E = 1
2Eijmimj,
(21)
where the symmetric positive deﬁnite matrix Eij is typically diagonal,
Eexact = 1
2

m2
x
Ix
+ m2
y
Iy
+ m2
z
Iz

,
(22)
as such suitable reference frame can always be chosen.
Any function of m2, the Euclidean norm of m squared, is not aﬀected by the
reversible evolution because {m2, H} = 0 for any functional H. The magnitude
of angular momentum is thus conserved regardless the choice of energy, it is
a Casimir of the Poisson bracket. Therefore, the Casimir does not aﬀect the
reversible part of the evolution and, consequently, it can not be learned from the
measured trajectory.
In reality, however, one typically observes not only the reversible mechani-
cal behavior, but also the irreversible thermodynamic behavior. A rotating rigid
body typically conserves its angular momentum while dissipating the kinetic
energy [38]. An irreversible term referred to as the energetic Ehrenfest regular-
ization [38], that leads to such behavior, is added to the reversible Eq. (20),
˙m = m × ∂E
∂m −τ
2LT ·
∂2E
∂m∂m · L · ∂E
∂m,
(23)
where Lij = −mkϵkij is the Poisson bivector generating the Poisson bracket.
Equation (23) keeps the magnitude m2 constant while dissipating kinetic energy,
˙E ≤0. Therefore, the rigid body tends to rotate around the axis with highest
moment of inertia as in Fig. 1. By adding dissipation, the Casimirs now play a
role in the dynamics, and the can be learned from the trajectory. The dissipative
dynamics eventually drives the system towards an equilibrium state, where the
angular momentum is aligned with the axis of highest moment of inertia. As
the magnitude of the angular momentum is conserved, that state is determined
uniquely and it is described by the value kinetic energy. Kinetic energy and
the magnitude of angular momentum thus play the role of the lower-level state

Learning Physics from Data: A Thermodynamic Interpretation
293
variables y. In this Section, we employ the one-level pattern recognition, where
properties of dynamics on manifold M = SO(3) are sought while dynamics on
the lower level can be obtained by projection a posteriori and does participate
in the recognition process itself.
Fig. 1. The rigid body starts rotating around the x−axis, which has the lowest moment
of inertial. Due to the energetic Ehrenfest regularization in Eq. (23) it changes its rota-
tion to the axis with highest moment of inertia. The magnitude of angular momentum
is conserved while energy is dissipated.
After having obtained the trajectories, we can approach the learning pro-
cedure. In the physics learning, we would like to reconstruct the formula for
energy (22) from the trajectory of m(t) at discrete times tn. The learning is
done by minimizing least squares using the Scipy method curve ﬁt. The energy
is assumed to be a quadratic form of m. The Poisson bivector and time step of
the numerical scheme are assumed to be known.
First we tried to recognize the energy a trajectory with dissipation switched
oﬀ, i.e. with τ = 0. As expected we could reconstruct the energy from most
of the initial conditions (using only 3 subsequent values of m(t)) only up to
the shift by Casimirs m2. When using a trajectory with dissipation, τ > 0,
the whole energy was reconstructed. We demonstrate this on the following. We
simulate the rigid body dynamics using Crank-Nicolson numerical scheme with a
variable parameter τ. The energetic Ehrenfest regularization is used, since it has
the desired properties, conserving m2 while dissipating kinetic energy (see [38]).
The smaller τ is, the less dissipative evolution we are observing. When τ is zero,
we return to the reversible Hamiltonian evolution. Therefore, it is interesting to
observe what happens with the learning of energy as τ increases from zero to
some small value. We let the rigid body evolve for 200 steps while τ is in the
interval τ ∈[0, 8 · 10−3]. We then perform the learning procedure and observe
error of the learned energy and also error of the learned τ compared to the exact
ones (used when generating trajectories). The results can bee seen in Fig. 2.
In summary, knowledge of geometry makes it possible to understand and
develop learning of dynamics more complex than mechanics of particles where

294
F. Chinesta et al.
Fig. 2. The absolute squared error of the ﬁt of τ and the energy. The error of energy is
calculated as the sum of squares of diﬀerences between entries of the exact and learned
matrices of second derivatives of the energy. The energy is initially ﬁtted with a signif-
icant error that eventually vanishes as the dissipation becomes signiﬁcant. This means
that with the dissipation we can learn the energy more accurately. The τ coeﬃcient
can be also learned with reasonable precision. We observe that the higher the exact
value of the coeﬃcient (stronger dissipation), the hither is the error between the exact
and learned values.
the kinematics is not generated by Hamilton canonical equations, but by Pois-
son geometry with a non-canonical bracket. Such Poisson brackets involve for
instance ﬂuid mechanics, kinetic theory, electrodynamics and rigid body rota-
tions. The latter kinematics was simulated and enhanced with terms leading to
thermodynamic behavior. We ﬁnd that the energy functional of a general Poisson
bracket can be learned from a trajectory only up to the Casimirs of the bracket.
However, when adding dissipation, the whole energy can be learned. The code
is available on [56].
6
Conclusion
Learning is a process of getting an insight that allows to make quick predictions.
If the input of learning is a dynamical system, then the insight is an information
about important qualitative features (about a pattern) in the phase portrait (i.e.
in the collection of trajectories). One way to get such information is to reduce
the dynamical system under investigation to a simpler dynamical system whose
phase portrait is the pattern in the phase portrait corresponding to the origi-
nal dynamical system. The reduction process in which the pattern is recognized
can be interpreted as the learning process. This process can also be regarded as
a time evolution generated by a dynamics that we call a reducing dynamics or
also a learning dynamics. In the reducing time evolution the pattern in the phase

Learning Physics from Data: A Thermodynamic Interpretation
295
portrait of the original dynamical system emerges. The reducing dynamics is dis-
sipative and is driven by a potential called entropy. We can use this terminology,
since in the particular case of reductions investigated in statistical mechanics
such potentials are indeed physical entropies arising in thermodynamics.
In the machine learning the input of learning is the phase portrait (data base).
In this paper we suggest that the approach to learning via reducing dynamics
and associated thermodynamics, that has been developed in the context of the
dynamical system theory, can also be applied and can be useful in the machine
learning. We illustrate the suggestion on the example worked out in [12] and in
learning energy of a rotating rigid body from observation of its angular momen-
tum, Sect. 5. In particular, we show that when learning dynamics with more
complex Poisson brackets than canonical, one can reconstruct the energy only
up to the Casimirs of the Poisson bracket unless the dynamics is dissipative.
When having both the reversible and irreversible (dissipative) terms, the whole
energy can be learned from the simulated trajectory.
In summary, we display a new viewpoint of reductions that have been recently
developed in non-equilibrium multiscale thermodynamics. We argue that this
new viewpoint of thermodynamics is particularly pertinent to and suitable for
machine learning. This global geometric picture linking thermodynamics and
machine learning is illustrated on earlier works in this direction and on a new
example using non-canonical Poisson brackets.
In the future we intend to explore new routes opened by the connection with
thermodynamics. For instance, thermodynamics provides a close connection of
entropy to ﬂuctuations. We are suggesting that the entropy drives the learning
dynamics. This means that an appropriate analysis of ﬂuctuations involved in
the data base can serve as a complementary tool in machine learning.
Acknowledgements. We are grateful to V´aclav Klika for discussing the manuscript.
F.Ch. thanks ESI Group through its research chair at “Arts et M´etiers ParisTech”,
whose ﬁrst invited position was Prof. M. Grmela, for performing the researches here
addressed. F. Ch. also knowledges Dr. Alain de Rouvray by the rich and inspiring dis-
cussions on pattern recognition as the ﬁrst step towards machine learning and artiﬁcial
intelligence, motivating the preset work. The support from ANR (Agence Nationale
de la Recherche, France) through its grant AAPG2018 DataBEST is also gratefully
acknowledged.
E.C. also acknowledges the ﬁnancial support of ESI Group through the project
“Simulated Reality”. The support given by the Spanish Ministry of Economy and
Competitiveness through Grant number DPI2017-85139-C2-1-R, and by the Regional
Government of Aragon and the European Social Fund, research group T88, is also
greatly acknowledged.
M.G. was supported by the Natural Sciences and Engineering Research Council of
Canada, Grants 3100319 and 3100735.
B.M. acknowledges the support of the Spanish Ministry of Science, Innovation and
Universities through grant number PRE2018-083211.
M.P. and M.ˇS were supported by Czech Science Foundation, Project No. 20-22092S.
M.P. was supported by Charles University Research Program No. UNCE/SCI/023.

296
F. Chinesta et al.
References
1. Gesamtausgabe, L.: Ludwig Boltzmann Gesamtausgabe - Collected Works (1983)
2. Gorban, A.N., Grechuk, B., Tyukin, I.Y.: Augmented artiﬁcial intelligence: a con-
ceptual framework (2018)
3. Kosambi, D.D.: J. Indian Math. Soc. 7, 76 (1943)
4. Golub, G., Van Loan, C.: Matrix Computations. Johns Hopkins Studies in the
Mathematical Sciences. Johns Hopkins University Press (2013). https://books.
google.cz/books?id=X5YfsuCWpxMC
5. Roweis, S.T., Saul, L.K.: Science 290(5500), 2323 (2000)
6. Wasserman, L.: Ann. Rev. Stat. Appl. 5(1), 501 (2018)
7. Brunton, S., Proctor, J., Kutz, J.: Proceedings of the National Academy of Sciences
(2016). https://doi.org/10.1073/pnas.1517384113
8. Kaiser, E., Kutz, J., Brunton, S.: Discovering conservation laws from data for
control (2018)
9. Kevrekidis, Y., Samaey, G.: Scholarpedia 5(9), 4847 (2010)
10. Weinan, E., Commun. Math. Stat. 5, 1 (2017). https://doi.org/10.1007/s40304-
017-0103-z
11. Weinan, E., Han, J., Li, Q.: Res. Math. Sci. 6(1), 10 (2018). https://doi.org/10.
1007/s40687-018-0172-y
12. Moya, B., Gonzalez, D., Alfaro, I., Chinesta, F., Cueto, E.: Comput. Mech. 64(2),
511 (2019). https://doi.org/10.1007/s00466-019-01705-3
13. Maes, C., Netoˇcn´y, K.: eprint arXiv:cond-mat/0202501 (2002)
14. Grmela, M., ¨Ottinger, H.C.: Phys. Rev. E 56, 6620 (1997). https://doi.org/10.
1103/PhysRevE.56.6620
15. ¨Ottinger, H.C., Grmela, M.: Phys. Rev. E 56, 6633 (1997). https://doi.org/10.
1103/PhysRevE.56.6633
16. ¨Ottinger, H.: Beyond Equilibrium Thermodynamics. Wiley (2005)
17. Pavelka, M., Klika, V., Grmela, M.: Multiscale Thermo-Dynamics (De Gruyter,
Berlin.
Boston
(2018).
https://doi.org/10.1515/9783110350951.
http://www.
degruyter.com/view/books/9783110350951/9783110350951/9783110350951.xml
18. Pavelka, M., Klika, V., Grmela, M.: Phys. Rev. E 90 (2014)
19. Onsager, L.: Phys. Rev. 37, 405 (1931). https://doi.org/10.1103/PhysRev.37.405.
http://link.aps.org/doi/10.1103/PhysRev.37.405
20. Onsager, L.: Phys. Rev. 38, 2265 (1931). https://doi.org/10.1103/PhysRev.38.2265
21. Casimir, H.B.G.: Rev. Mod. Phys. 17, 343 (1945). https://doi.org/10.1103/
RevModPhys.17.343
22. de Groot, S.R., Mazur, P.: Non-equilibrium Thermodynamics. Dover Publications,
New York (1984)
23. Grmela, M., Klika, V., Pavelka, M.: Phys. Rev. E 92 (2015)
24. Grmela, M.: J. Stat. Phys. 166(2), 282 (2017)
25. Grmela, M.: J. Phys. Commun 2 (2018)
26. Shannon, C.E.: Bell Syst. Tech. J. 27, 379 (1948)
27. Jaynes, E.T.: Phys. Rev. 106(4), 620 (1957)
28. Gorban, A., Karlin, I.: Invariant Manifolds for Physical and Chemical Kinet-
ics. Lecture Notes in Physics. Springer (2005). http://books.google.cz/books?
id=hjvjPmL5rPwC
29. Klika, V., Pavelka, M., V´agner, P., Grmela, M.: Entropy 21, 715 (2019). https://
doi.org/10.3390/e21070715

Learning Physics from Data: A Thermodynamic Interpretation
297
30. Chapman, S., Cowling, T., Burnett, D., Cercignani, C.: The Mathematical Theory
of Non-uniform Gases: An Account of the Kinetic Theory of Viscosity, Thermal
Conduction and Diﬀusion in Gases. Cambridge Mathematical Library. Cambridge
University Press (1990). https://books.google.cz/books?id=Cbp5JP2OTrwC
31. Callen, H.: Thermodynamics: An Introduction to the Physical Theories of Equi-
librium Thermostatics and Irreversible Thermodynamics. Wiley (1960). http://
books.google.cz/books?id=mf5QAAAAMAAJ
32. Turkington, B.: J. Stat. Phys. 152, 569 (2013)
33. Pavelka, M., Klika, V., Grmela, M.: J. Stat. Phys. Accepted (2020)
34. Ehrenfest, P., Ehrenfest, T.: The Conceptual Foundations of the Statistical App-
roach in Mechanics. Dover Books on Physics. Dover Publications (1990)
35. Gorban, A.N., Karlin, I.V., ¨Ottinger, H.C., Tatarinova, L.L.: Phys. Rev. E 63
(2001)
36. Karlin, I.V., Tatarinova, L.L., Gorban, A.N., ¨Ottinger, H.C.: Physica A: Stat.
Mech. Appl. 327(3–4), 399 (2003)
37. Pavelka, M., Klika, V., Grmela, M.: Entropy 20 (2018)
38. Pavelka, M., Klika, V., Grmela, M.: Physica D: Nonlinear Phenomena 399, 193
(2019).
https://doi.org/10.1016/j.physd.2019.06.006.
http://www.sciencedirect.
com/science/article/pii/S0167278918305232
39. Grmela,
M.:
Comput.
Math.
Appl.
65(10),
1457
(2013).
https://doi.org/
10.1016/j.camwa.2012.11.019.
http://www.sciencedirect.com/science/article/pii/
S0898122112006803
40. Grmela, M.: Entropy 16(3), 1652 (2014). https://doi.org/10.3390/e16031652
41. ˇCapek, K.l.: R.U.R. (Rossum’s Universal Robots). Penguin Books, London (2004)
42. Chatterjee, A.: Curr. Sci. 78, 7 (2000)
43. Brockett, R.: Linear Algebra Appl. 146, 79 (1991). https://doi.org/10.1016/0024-
3795(91)90021-N
44. Bloch,A.M., Krishnaprasad, P.S., Marsden, J.E., Ratiu, T.S.: Annales de l’I.H.P.
Analyse non lin´eaire 11(1), 37 (1994)
45. Absil, A.: Int. J. Unconvent. Comput. 2(4), 291 (2006)
46. Gingold, R., Monaghan, J.: Mon. Not. R. Astron. Soc. 181(3), 375 (1977)
47. Espa˜nol, P., Revenga, M.: Phys. Rev. E 67 (2003). https://doi.org/10.1103/
PhysRevE.67.026705. https://link.aps.org/doi/10.1103/PhysRevE.67.026705
48. Ellero, M., Espaˇnol, P.: Appl. Math. Mech. 39(1), 103 (2018)
49. Gonz´alez, D., Chinesta, F., Cueto, E.: Continuum Mechanics and Thermodynamics
(2018). https://doi.org/10.1007/s00161-018-0677-z
50. Gonz´alez, D., Chinesta, F., Cueto, E.: Front. Mat. 6, 14 (2019)
51. Brockett, R.: Linear Algebra Appl. 122-124, 761 (1989). Special Issue on Linear
Systems and Control
52. Landau, L., Lifshitz, E.: Mechanics. Butterworth-Heinemann (1976)
53. Arnold, V.: Annales de l’institut Fourier 16(1), 319 (1966)
54. Marsden, J., Ratiu, T., Weinstein, A.: Trans. Am. Math. Soc. 281(1), 147 (1984).
https://doi.org/10.2307/1999527
55. Simo,
J.C.,
Marsden,
J.E.,
Krishnaprasad,
P.S.:
Arch.
Rat.
Mech.
Anal.
104(2), 125 (1988). https://doi.org/10.1007/BF00251673. https://doi.org/10.
1007/BF00251673
56. Pavelka, M., ˇS´ıpka, M.: Machine-learning-rigid-body (2020). https://github.com/
enaipi/machine-learning-rigid-body.git

Computational Dynamics of Reduced
Coupled Multibody-Fluid System in Lie
Group Setting
Zdravko Terze(B), Viktor Pandˇza, Marijan Andri´c, and Dario Zlatar
Faculty of Mechanical Engineering and Naval Architecture,
University of Zagreb, Ivana Luˇci´ca 5, Zagreb, Croatia
zdravko.terze@fsb.hr
Abstract. In order to study dynamics of multibody system (MBS) mov-
ing in ambient ﬂuid, we adopt geometric modeling approach of fully
coupled MBS-ﬂuid system, incorporating boundary integral method and
time integrator in Lie group setting. By assuming inviscid and incom-
pressible ﬂuid, the conﬁguration space of the MBS-ﬂuid system is reduced
by eliminating ﬂuid variables via symplectic reduction without compro-
mising any accuracy. Consequently, the equations of motion for the sub-
merged MBS are formulated without explicitly incorporating ﬂuid vari-
ables, while eﬀect of the ﬂuid ﬂow to MBS overall dynamics is accounted
for by ‘added mass’ eﬀect to the submerged bodies. In such approach,
the ‘added masses’ are expressed as boundary integral functions of the
ﬂuid density and the ﬂow velocity potential. In order to take into account
additional viscous eﬀects and include ﬂuid vorticity and circulation in the
system dynamics, vortex shedding and evolution mechanism is incorpo-
rated in the overall model by unsteady potential ﬂow method, enforc-
ing Kutta conditions on MBS sharp edges. In summary, presented app-
roach exhibits signiﬁcant computational advantages in comparison to the
standard numerical procedures that - most commonly - comprise ﬁnite
volume discretization of the whole ﬂuid domain and (loosely coupled)
solving ﬂuid and MBS dynamics on diﬀerent meshes.
1
Introduction
The conventional approach to simulating dynamics of multibody system (MBS)
moving in ambient ﬂuid most commonly includes discretization of the large ﬂuid
domain, using separate meshes for the ﬂuid and solid part of the system [1]. This
leads to the calculation of large amount of ﬂuid data, that are usually not of the
prime interest, since we are mostly concerned with eﬀects that the ﬂuid exerts
on the MBS motion. In order to circumvent these deﬁciencies and obtain numer-
ically more eﬃcient method for simulating system coupled dynamics, we adopt a
geometric modeling approach of fully coupled MBS-ﬂuid system, incorporating
boundary integral method for calculating ‘added masses’, and time integrator in
Lie group setting.
c
⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2021
F. Barbaresco and F. Nielsen (Eds.): SPIGL 2020, PROMS 361, pp. 298–307, 2021.
https://doi.org/10.1007/978-3-030-77957-3_15

Computational Dynamics of Reduced Coupled Multibody-Fluid System
299
By assuming inviscid and incompressible ﬂuid, the conﬁguration space of
the coupled MBS-ﬂuid system is reduced by eliminating ﬂuid variables via sym-
plectic reduction, without compromising any accuracy. The reduction exploits
‘particle relabeling’ symmetry, associated with the conservation of circulation:
ﬂuid kinetic energy, ﬂuid Lagrangian and associated momentum map are invari-
ant with respect to this symmetry [2]. Consequently, the equations of motion for
the submerged MBS are formulated without explicitly incorporating the ﬂuid
variables, while eﬀect of the ﬂuid ﬂow to MBS overall dynamics is accounted for
by the ‘added masses’ to the submerged bodies [3,4]. In such approach, the added
masses are expressed as boundary integral functions of the ﬂuid density and the
ﬂow velocity potential function. Additionally, in order to allow for dynamics sim-
ulation of bodies with non-smooth boundaries, the point vortex shedding and
evolution mechanism is incorporated in the ﬂuid-structure dynamical model.
The vortices are assumed to be irrotational, and are being modeled by unsteady
potential ﬂow method enforcing Kutta condition at sharp edges. By using the
proposed framework it is possible to include an arbitrary combination of smooth
and sharp-edged bodies in a coupled MBS-ﬂuid system, as long as the major vis-
cosity eﬀects of the ﬂuid on the body can be described by shedding and evolution
of the irrotational point vortices. This broadens the possibilities for utilization
of the presented method by removing the necessity of having smooth shape of
the moving bodies immersed in the ambiental ﬂuid, such as in the case of the
models described in [3]. Also, by formulating mathematical model of the coupled
MBS-ﬂuid dynamics in the DAE-index-1 general form (2) (which is based on the
extended Kirchoﬀequations that may include all kind of external forces acting
on MBS), it is assured that general MBS-ﬂuid motion problem can be solved (i.e.
no special requests - such as neutral buoyancy condition assumed in [3,5] - are
necessary). It can be noted that rigid body dynamics in potential ﬂow based on
symmetry reductions is discussed also in [6,7]. However - on the contrary to the
general formulation presented here - in [6,7] additional assumption in the form
of nonholonomic constraint on the rigid body velocity in ambient ﬂuid - that
turns MBS into ’hydrodynamic Chaplygin sleigh’ - is imposed on the system,
restricting these derivations only to those (mainly academical) problems where
such an assumption is valid.
2
Dynamics of Multibody System in Fluid Flow
We consider multibody system (MBS) consisting of k articulated submerged
rigid bodies in ideal ﬂuid, which is at rest at the inﬁnity. At any time t, the
MBS-ﬂuid system occupies an open connected region M of the Euclidean space,
which we identify with R3. More speciﬁcally, the rigid bodies occupy regions
Bi, i = 1, . . . , k and the ﬂuid occupies a connected region F ⊂M such that
M can be written as a disjoint union of open sets as M = B1 ∪· · · ∪Bk ∪F.
Conﬁguration space Q(M) := {f : M →M} of such a system is the set of
all appropriately smooth maps from M to M, where ﬂuid part Qf ∈Diﬀvol (F)
represents volume preserving position ﬁeld of the ﬂuid particles. Diﬀvol (F) is the

300
Z. Terze et al.
set of volume preserving diﬀeomorphisms [5,8], possessing properties of the Lie
group. Bodies part QBi, i = 1, 2, . . . , k, represents rigid body motion of the i-th
solid body Bi ⊂M with boundary ∂Bi, meaning that the conﬁguration space
of a MBS comprising k unconstrained rigid bodies is modeled as 6k dimensional
Lie group G = R3 × SO(3) × · · · × R3 × SO(3) (k copies of R3 × SO(3)) with
the elements of the form p = (x1, R1, . . . , xk, Rk). The left multiplication in the
group is given as Lp : G →G, p →p · p, where origin of the group G is given
as e = (01, I1, . . . , 0k, Ik). With G so deﬁned, its Lie algebra is given as g =
R3×so(3)×· · ·×R3×so(3) with the elements of the form v = (v1, ˜ω1, . . . , vk, ˜ωk),
vi being velocity of ith body mass center and ˜ωi ith body angular velocity in
skew symmetric matrix form [9].
Moreover, the MBS state space is then S = G × g, i.e. S = R3 × SO(3) ×
· · · × R3 × SO(3) × R3 × so(3) × · · · × R3 × so(3) ∼= TG with the elements
x = (x1, R1, . . . , xk, Rk, v1, ˜ω1, . . . , vk, ˜ωk) [9]. S is the left-trivialisation of the
tangent bundle TG. This is a Lie group itself that possesses the Lie algebra
S = R3 × so(3) × · · · × R3 × so(3) × R3 × R3 × · · · × R3 × R3 with the element
z = (v1, ˜ω1, . . . , vk, ˜ωk, ˙v1, ˙ω1, . . . , ˙vk, ˙ωk).
By assuming now that motion of k bodies in MBS is constrained with kine-
matical constraint matrix C, meaning that kinematical constraints at the accel-
eration level can be formulated as [9]
C ˙z = ξ,
(1)
MBS dynamics in ﬂuid ﬂow can be expressed as DAE-index-1 problem formu-
lated as

M CT
C
0
 
˙z
λ

=

Q
ξ

,
(2)
where M represents inertia matrix and Q is the force vector, while ξ is a right
hand side obtained after diﬀerentiating twice kinematical constraints at the posi-
tion level [9]. Inertia matrix M contains standard inertial properties of the MBS
rigid bodies, supplemented by ‘added mass’ eﬀect [10] that the MBS perceives
due to interaction with the ﬂuid potential ﬂow (i.e. ‘added inertia’, see below).
The force vector Q consists of general external forces and torques acting on the
system bodies according to the extended Kirchoﬀequations [4], which might
also include additional forces and torques imposed on the system due to the ﬂow
circulatory part (that is - if it is existent - consistent with the system vortex
shedding mechanism). The ‘added mass’ eﬀect needs to be calculated by bound-
ary element method (BEM) (described in the Sect. 3), after symplectic reduction
on Q is performed. Note that ‘added mass momentum’ gradient (rate of change
in time) should also be computed and introduced in (2) through the force vector
Q, as well as standard right hand side expressions of the Kirchoﬀequations [5].
The model (2) can be used for solving MBS-ﬂuid dynamics problem in general
form and no additional requests or assumptions on the system’s motion - such as
necessary neutral buoyancy of the mechanical system [3,5] - need to be imposed.

Computational Dynamics of Reduced Coupled Multibody-Fluid System
301
Rationale that dynamics mathematical model of MBS-ﬂuid system can be
shaped in the form (2) stems from the fact that - if non-vortical ideal ambiental
ﬂuid is assumed - symplectic reduction at zero vorticity yields [11]
J−1
F (0) /Diﬀvol (F) = T ∗(Q/Diﬀvol (F)) = T ∗G,
(3)
indicating that the whole dynamics of the system evolves in cotangent bundle
T ∗G and ﬂuid inﬂuence on the MBS dynamics is reduced to ‘added mass’ eﬀect
only [3].
This basically means that ﬂuid terms vanish from the model completely,
except for the circulatory ﬂow part of the vector Q that is consistent with the
vortex shedding mechanism. Actually, this part of Q is also reduced out from
the model if ambient ﬂow is fully potential and no vorticities of any kind are
considered (for example, in the case when moving bodies are blunt - with no
sharp edges - and no external vorticities in ambiental ﬂow are present). In (3), JF
is a momentum map associated with the action of Diﬀvol (F) on Q representing
vorticity advection [5]. Since Lagrangian of the ﬂuid [6] is invariant on Diﬀvol (F)
action (‘particle relabeling’ symmetry), JF is conserved leading to the well known
Kelvin circulation theorem.
The Eq. (2) needs to be integrated to obtain system velocities z, while - due
to the fact that numerical integration of (1) should also satisfy MBS kinemati-
cal constraints on the position and velocity level that will be inevitably violated
during its straightforward integration - constraint violation stabilization algo-
rithm needs to be simultaneously applied [9]. After obtaining system velocities,
a reconstruction of the system translation is a straightforward task, while rota-
tional part requires a non-linear update. To this end, Munthe-Kaas Lie group
method [12] is utilized, where kinematic reconstruction for each body in the
system is performed by seeking an incremental rotation vector θi in every time
step from a known angular velocity ﬁeld ωi by integrating ODE equation in Lie
algebra
˙θi = dexp−1
−θi (ωi) ,
θi (0) = 0,
where the closed-form solution of the inverse diﬀerential exponential operator
can be written as
dexp−1
−˜
θi (ωi) = ωi + 1
2
˜θiωi −
||θi|| cot

||θi||
2

−2
2 ||θi||2
˜θi
2ωi,
(4)
with ˜· representing the skew symmetric operator. The body rotation group
matrix Ri ∈SO(3) in time step n + 1 is then reconstructed as
Ri,n+1 = Ri,n exp (θi,n+1) ,
where exp represents exponential mapping on SO (3) (more details can be found
in [9]).

302
Z. Terze et al.
Fig. 1. Illustration of the multibody system used as a numerical example.
3
Added Mass Eﬀect via Boundary Element Method
In mathematical model of MBS-ﬂuid dynamics formulated in (2), the added
mass eﬀect of the hydrodinamically coupled bodies Bi is to be determined via
BEM. As an example, the two-dimensional system of three rigid bodies linked
by revolute joints moving in a ﬂuid is chosen. The system consists of the middle
ellipsoid shaped body, connected with airfoil on each side. The airfoils are rotat-
ing symmetrically, around a translationally ﬁxed ellipsoid at its center of mass
via revolute joint, as illustrated in Fig. 1.
The domain of rigid bodies is deﬁned as B = B1∪B2∪B3, and the ﬂuid domain
is denoted as F. As the region F is connected, if the ﬂow is irrotational, the
velocity ﬁeld u can be written in terms of a potential u = ∇φ. Incompressibility
now implies that the Laplacian of φ is zero [2], i.e.
Δφ = 0
in F.
After applying non-penetrating boundary condition, i.e. the constraint that ﬂuid
cannot pass through the boundary of the rigid body, but may slip freely along
the edges, the Neumann conditions for the boundary value problem are obtained
∇φ · ni = vi · ni
on ∂Bi,
for i = 1, 2, 3,
(5)
where ni represents the outward pointing unit normal to the i-th body. This,
together with condition that the ﬂuid is at rest at inﬁnity, i.e.
∇φ = 0
at
∞,
completes the full description of the exterior Laplace boundary value problem
with the Neumann boundary conditions. Following approach proposed in [13],
the boundary value problem can be formulated as
P (P) =
lim
PE→P

B

φ(T)∂G
∂n (PE, T) + ∂φ
∂n(T)G(PE, T)

dT = 0
(6)

Computational Dynamics of Reduced Coupled Multibody-Fluid System
303
where P, T ∈B. G represents the Green’s function and it is deﬁned for a 2D
Laplace problem, together with its ﬁrst derivative, as
G(P, T) = −log(r)
2π
,
(7)
∂G
∂n (P, T) = −1
2π
n · R
r2
,
(8)
where R = T −P and r = ||R||.
After discretizing the boundary B, the boundary Eq. (6) could be enforced in
each of the nodes. However, in this framework we employ a Galerkin approach,
where the equation is enforced in a weighted manner, i.e.

B
ψi(P)P(P)dP = 0,
(9)
where ψi(P) represents all non-zero shape functions at node Pi. Although this
approach leads to the computationally more challenging task, the beneﬁt is that
the solution can be interpreted as the projection of the exact solution onto the
subspace consisting of linear combination of the shape functions. In other words,
the Galerkin solution is the linear combination of shape functions that is closest
to the exact solution [13].
The discretized form of Eq. (3) becomes singular when points P and T belong
to the coincident or adjacent elements, since r tends towards zero. However, the
solutions in the integral sense are convergent and can be obtained for coincident
and adjacent case by analytic integration. The details of the analytic derivation
are out of the scope of this paper and will be presented in subsequent publica-
tions.
After obtaining the values of the potential along the boundary, the ‘added
mass’ eﬀect can be computed. Although not required for the ‘added mass’ cal-
culation, the resulting ﬂuid potential around the MBS can, for the purpose of
illustration, be reconstructed by equation
φ(P) = −

B

φ(T)∂G
∂n (P, T) + ∂φ
∂n(T)G(P, T)

dT,
(10)
while the limit is not needed in this case, because the expressions are non-singular
since P ∈F, while T ∈B and therefore the distance is always non-zero.
For this numerical example, the boundary is discretized with linear elements,
and after solving Eq. (3) and (10) the ﬂow can be represented by the streamlines
plot in Fig. 2.

304
Z. Terze et al.
Fig. 2. The streamlines plot for the inviscid potential ﬂow around the MBS.
4
Vorticity Eﬀects
As discussed in precedent chapters, if no vorticity eﬀects are included in the anal-
ysis, ﬂuid variables do not appear explicitly in the coupled MBS-ﬂuid dynamics
computational model (2). In this case, the MBS motion dynamics is simulated in
a fully potential ambiental ﬂow with zero circulation around MBS, i.e. symplectic
reduction at the zero vorticity level of the MBS-ﬂuid-ﬂow yields ‘ﬂuid-variable-
free’ formulation, where inﬂuence of ﬂuid on the moving bodies’ dynamics is
given by the ‘added mass’ eﬀect only.
To this end, BEM discretization is required only at the boundary of the MBS,
as opposed to the conventional approach, which utilizes (most frequently ﬁnite
volume) discretization of the large domain of ambiental ﬂuid. However, in this
case, the extent of the applications to which this model can be applied is limited,
mainly due to the inviscid (no vortices, no circulation) ﬂuid assumption. On the
other hand, there is a number of application where viscosity plays an important
role, but can be reasonably well approximated by shedding and evolution of
the irrotational point vortices. One example of such applications are insect-type
ﬂapping micro aerial vehicles, as described in [14–16].
Modeling of viscosity eﬀects via irrotational point vortices (generating thus
circulatory ﬂow part of the external force vector Q, as introduced in Sect. 2) is
especially useful for the MBS with sharp edges, since such a shedding can be
modeled by applying Kutta condition at the edges, enforcing physical meaning-
fulness of the velocities. If there is an non-zero angle at the sharp edge, Kutta
condition basically means that the ﬂuid velocity at the sharp edge must be zero
[17], which generates circulation around MBS while - in the same time - irrota-
tional point vortices are shed in the ﬂuid wake (Kelvin circulation theorem).
On the computing side, the circulatory ﬂow ﬁeld around moving bodies in
an incompressible and irrotational ﬂuid can be obtained by using one of the
Unsteady Potential Flow methods. In these methods, time dependency is intro-
duced through changing boundary conditions, since the continuity equation does
not directly include unsteady terms. These conditions include requirement of

Computational Dynamics of Reduced Coupled Multibody-Fluid System
305
zero normal velocity on the surface of the body (same as the previously intro-
duced Neumann boundary conditions for potential ﬂow (5)) and fulﬁllment of the
Kelvin condition, which states that the circulation around a curve surrounding
bodies and their wake is conserved (which is direct consequence of the vortic-
ity advection condition, i.e. Diﬀvol (F) invariance of the momentum map JF
introduced in (3)).
The wake shed from the sharp edge of the body can be modeled by vortex
distribution, but it will be shed only if the circulation of the body varies with
time. In order to calculate the strength of each wake vortex, its location must
be speciﬁed with help of the Kutta condition. Once the solution in the form of
the velocity ﬁeld is found, it can be used to calculate pressure ﬁeld by using the
modiﬁed unsteady Bernoulli equation
pinf −p
ρ
= 1
2
∂Φ
∂x
2
+
∂Φ
∂y
2
+
∂Φ
∂z
2
−v · ∇Φ + ∂Φ
∂t ,
(11)
where Φ represents a ﬂuid velocity potential, while v represents the body velocity.
The developed numerical solution in this paper is based on 2D Unsteady
Lumped-Vortex Element Method described in [18] and here extended in order to
enable multiple shed points to exist. The method uses lumped-vortex as a singu-
larity element and its algorithm is organized as follows. In the chosen numerical
example, geometry of the MBS consisting of three rigid bodies is discretized by
representing each body with set of discrete vortices placed on the mid-line of the
body. When the circulation changes with time, vortices are shed from the sharp
edges of the bodies and consequently, wake can be modeled by the same vortex
model.
Discretized geometry is divided into subpanels, each having a vortex point
placed at the quarter chord and collocation point at three-quarter point, where
the zero normal ﬂow boundary condition must be fulﬁlled. To fully specify this
condition, the kinematic conditions need to be known. In the tested numerical
example, they are calculated from prescribed symmetrical movement of the ﬁrst
and the last body of the MBS. The latest lumped-vortex elements shed from the
two sharp edges at each time-step satisfy the Kutta condition by being placed
along the mid-line of a body at 0.3 of the distance covered by that sharp edge
during the latest time step.
In each time point, the inﬂuence coeﬃcients of the vortices are calculated for
each collocation point, together with the normal velocity component due to the
motion of the MBS. Now, by solving a set of linear equations in matrix form,
strength of each panel vortex and two shed vortices can be obtained. Before
ﬁnishing each time-step vortex wake rollup needs to be updated. This wake is
force free, which means that each vortex moves with local stream velocity, that
is induced by the current ﬂow ﬁeld. By looping through the algorithm for set
amount of time, new wake vortices will be shed, as shown in Fig. 3, which will
keep changing the ﬂow ﬁeld surrounding the MBS. It is important to note that
the existing vortices tend to move away from the MBS and, therefore, their
inﬂuence on the body diminishes.

306
Z. Terze et al.
Fig. 3. Subpanels of discretized bodies and shed vortices from two sharp edges of the
multibody system.
It is important to note that vorticity eﬀects modeled as described in this
chapter are used only for formulating circulatory ﬂow part of the vector Q in
(2), while the ‘added mass’ eﬀect introduced in M (and calculated via BEM as
described in Sect. 3) stays unchanged. Indeed, it can be shown that symplectic
reduction, similar to the one introduced in (3), can also be performed when
vorticity level is not set to zero, but possesses certain value that equals to the
system circulation that is in consistence with the introduced vortex shedding
mechanism [11,19]. To this end, mathematical model expressed by (2) governs
MBS-ﬂuid coupled physics also when vorticity (circulation) eﬀects are included
in the analysis and introduced in (2) via circulatory part of the vector Q.
5
Conclusion
By using approach described in this work it is possible to model the coupled
MBS-ﬂuid system by discretizing boundary of the bodies, instead of conven-
tional discretization of the whole ﬂuid domain. Model governing equations are
formulated in the general form and no additional assumptions on the system
motion are necessary to be imposed. If vorticity eﬀects are excluded from the
analysis (i.e. MBS moves in potential ﬂow without circulation and no trailing
edge vortices have been shed), the symplectic reduction based formulation yields
coupled MBS-ﬂuid mathematical model not containing explicit ﬂuid variables.
In the case of MBS consisting of bodies with sharp edges moving in the ﬂuid
with circulation, the ﬂuid variables are needed to describe vortical eﬀects only
(introduced here via shed irrotational point vortices enforcing Kutta condition,
while satisfying non-penetrability boundary conditions). Because of the improved
numerical eﬃciency compared to the conventional models, this approach is well
suited for applications in design optimization and optimal control of MBS motion
in the ﬂuid ﬂow.
Acknowledgements. This work has been fully supported by Croatian Science Foun-
dation under the project IP-2016-06-6696.

Computational Dynamics of Reduced Coupled Multibody-Fluid System
307
References
1. Vazquez, J.G.V.: Fluid-Structure Interaction Analysis. VDM Verlag Dr. M¨uller,
Saarbr¨ucken (2008)
2. Arnold, V.I., Khesin., B.A.: Topological Methods in Hydrodynamics. Springer-
Verlag, Berlin, Heidelberg (1998). https://doi.org/10.1007/b97593
3. Kanso, E., Marsden, J.E., Rowley, C.W., Melli-Huber, J.B.: Locomotion of artic-
ulated bodies in a perfect ﬂuid. J. Nonlinear Sci. 15(4), 255–289 (2005)
4. Leonard, N.E.: Stability of a bottom-heavy underwater vehicle. Automatica 33(3),
331–346 (1997)
5. Vankerschaver, J., Kanso, E., Marsden, J.E.: The geometry and dynamics of inter-
acting rigid bodies and point vortices. J. Geom. Mecha. 1 (2009)
6. Garc´ıa-Naranjo, L.C., Vankerschaver, J.: Nonholonomic ll systems on central exten-
sions and the hydrodynamic chaplygin sleigh with circulation. J. Geom. Phys. 73,
56–69 (2013)
7. Fedorov, Y.N., Garc´ıa-Naranjo, L.C.: The hydrodynamic chaplygin sleigh. J. Phys.
A: Math. Theor. 43(43), 434013 (2010)
8. Arnold, V.: Sur la g´eom´etrie diﬀ´erentielle des groupes de lie de dimension inﬁnie
et ses applications `a l’hydrodynamique des ﬂuides parfaits. Annales de l’Institut
Fourier 16(1), 319–361 (1966)
9. Terze, Z., M¨uller, A., Zlatar, D.: Lie-group integration method for constrained
multibody systems in state space. Multibody Sys. Dyn. 34(3), 275–305 (2015)
10. Shashikanth, B.N., Marsden, J.E., Burdick, J.W., Kelly, S.D.: The hamiltonian
structure of a two-dimensional rigid circular cylinder interacting dynamically with
n point vortices. Phys. Fluids 14(3), 1214–1227 (2002)
11. Marsden, J.E., Misiolek, G., Ortega, J.-P., Perlmutter, M., Ratiu, T.: Hamiltonian
Reduction by Stages. Springer-Verlag, Berlin, Heidelberg (2007). https://doi.org/
10.1007/978-3-540-72470-4
12. Munthe-Kaas, H.: Runge-Kutta methods on Lie groups. BIT Numer. Math. 38(1),
92–111 (1998)
13. Sutradhar, A., Paulino, G., Gray, L.J.: Symmetric Galerkin Boundary Element
Method. Springer-Verlag, Berlin, Heidelberg (2008). https://doi.org/10.1007/978-
3-540-68772-6
14. Terze, Z., Pandˇza, V., Kasalo, M., Zlatar, D.: Discrete mechanics and optimal
control optimization of ﬂapping wingdynamics for Mars exploration. Aerosp. Sci.
Technol. 106, 106131 (2020)
15. Terze, Z., Pandˇza, V., Kasalo, M., Zlatar, D.: Optimized ﬂapping wing dynamics
via DMOC approach. Nonlinear Dynamics. (accepted)
16. Yongliang, Y., Binggang, T., Huiyang, M.: An analytic approach to theoretical
modeling of highly unsteady viscous ﬂow excited by wing ﬂapping in small insects.
Acta Mechanica Sinica 19(6), 508–516 (2003)
17. Pollard, B., Fedonyuk, V., Tallapragada, P.: Swimming on limit cycles with non-
holonomic constraints. Nonlinear Dyn. 97, 2453–2468 (2019)
18. Katz, J., Plotkin, A.: Low-Speed Aerodynamics, 2nd edn. Cambridge Aerospace
Series. Cambridge University Press (2001)
19. Vankerschaver, J., Kanso, E., Marsden, J.E.: The dynamics of a rigid body in
potential ﬂow with circulation. Regul. Chaotic Dyn. 15, 606–629 (2010)

Material Modeling via
Thermodynamics-Based Artiﬁcial Neural
Networks
Filippo Masi1(B), Ioannis Stefanou1, Paolo Vannucci2,
and Victor Maﬃ-Berthier3
1 Institut de Recherche en G´enie Civil et M´ecanique, UMR 6183, CNRS,
Ecole Centrale de Nantes, Universit´e de Nantes, Nantes, France
{filippo.masi,ioannis.stefanou}@ec-nantes.fr
2 LMV, UMR 8100, Universit´e de Versailles et Saint-Quentin, Versailles, France
paolo.vannucci@uvsq.fr
3 Ing´erop Conseil et Ing´enierie, Rueil-Malmaison, France
victor.maffi-berthier@ingerop.com
Abstract. Machine Learning methods and, in particular, Artiﬁcial Neu-
ral Networks (ANNs) have demonstrated promising capabilities in mate-
rial constitutive modeling. One of the main drawbacks of such approaches
is the lack of a rigorous frame based on the laws of physics.
Here we propose a new class of data-driven, physics-based, neural net-
works for constitutive modeling of strain-rate independent processes at
the material point level, which we deﬁne as Thermodynamics-based Arti-
ﬁcial Neural Networks (TANNs). Relying on automatic diﬀerentiation,
derivatives of the free-energy, the dissipation rate and their relation with
the stress and internal state variables are hardwired in the network.
The proposed network does not have to identify the underlying pat-
tern of thermodynamic laws during training, reducing the need of large
data-sets, improving the robustness and the performance of predictions.
Finally and more important, the predictions remain thermodynamically
consistent, even for unseen data.
TANNs are herein used to model history-dependent materials, with
kinematic softening. While the motivating examples considered may be
rather simple, we emphasize that the proposed class of ANN can be suc-
cessfully applied (without any modiﬁcation) to materials with diﬀerent
or more complex behavior.
Based on these features, TANNs are a starting point for data-driven,
physics-based constitutive modeling with neural networks.
1
Introduction
Machine Learning (ML) is increasingly recognized as a promising tool for many
scientiﬁc branches, from biological to mechanical sciences. At present, several
successful applications of ML approaches and, in particular, of Artiﬁcial Neural
Networks (ANNs) have been implemented for learning representations of mate-
rial behaviors from data, either originated through experimental tests or detailed
c
⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2021
F. Barbaresco and F. Nielsen (Eds.): SPIGL 2020, PROMS 361, pp. 308–329, 2021.
https://doi.org/10.1007/978-3-030-77957-3_16

Material Modeling via Thermodynamics-Based Artiﬁcial Neural Networks
309
micro-mechanical simulations. We refer, without being exhaustive, to the works
of Ghaboussi and Sidarta [12], Leﬁk and Schreﬂer [32], Jung and Ghaboussi
[28], Settgast et al. [44], Liu and Wu [34], Lu et al. [36], Xu et al. [46], Huang
et al. [25], Liu and Wu [34], Gajek et al. [10], Gorji et al. [18], Heider et al. [20],
Ghavamian and Simone [15], Mozaﬀar et al. [41], Frankel et al. [9], Gonzalez et
al. [16], Gorji et al. [18].
Further achievements were accomplished in the ﬁeld of numerical modeling of
mechanics. Starting from the so-called autoprogressive trained ANNs proposed
by Ghaboussi et al. [14], numerous approaches have been implemented to solve
numerical simulations by resorting to ANNs. Neural networks, trained on some
data characterizing the response of a given material, can be employed as mate-
rial subroutines, replacing the standard constitutive equations or algorithms, in
numerical simulations [see 28,32,33,44, among others].
In the last two decades, attention has been focused on the implementation
of ANN methods for speeding-up the computing time of multiscale analyses for
materials with diﬀerent characteristic lengths (or scales). Indeed, in multiscale
simulations, the micro-scale problem has to be solved repeatedly, but with dif-
ferent input parameters, which renders the computing time of such approaches
soon prohibitive. Accelerating techniques based on ANNs have been promoted
as excellent tools to overcome the aforementioned issues, see e.g. [1,8,15,34,35].
Nevertheless, constitutive modeling via ANNs has some major issues. For
instance, ANNs perform well when evaluated suﬃciently close to the training
domain. However, the extrapolation capabilities appear limited when predictions
are performed for values far beyond the training range. Furthermore, vast amounts
of high quality data (e.g. with reduced noise and free of outliers) are generally
needed to enable ANNs to identify and learn with high accuracy the constitutive
stress-strain response. These issues essentially stems from the lack of a rigorous
framework based on the physical laws governing the material systems under inves-
tigation. For a trained ANN, nothing guarantees that its predictions will respect
the basic principles of thermodynamics, e.g. the ﬁrst and second laws.
Very recently, a new class of methods, denoted as physics-driven approaches,
have received particular attention. We refer, for instance, to Raissi et al. [43]
who developed the so-called Physics-Informed Neural Networks (PINNs), for
the resolution of partial diﬀerential equations by enforcing basic laws of physics,
using the reverse-mode autodiﬀtechnique [2]. In the ﬁeld of mechanics, we record
the works of Gonzalez et al. [17], Hern´andez et al. [21] in which, by leveraging
the metriplectic structure of dissipative Hamiltonian systems, the behavior of
physical systems can be retrieved by ANNs, whose predictions comply with the
ﬁrst and second principles of thermodynamics.
In the ﬁeld of mechanics and constitutive material modeling, Kirchdoerfer
and Ortiz [30] proposed a new data driven computing approach in which the
boundary value problem (BVP), in material numerical simulations, is solved
directly from experimental material data (measurements), bypassing the ANN
material modeling step [6,26,27,30,31]. The integration of basic laws of thermo-
dynamics, in the forms of a minimization problem, allows predictions to fulﬁll
physical requirements, such as the thermodynamic consistency.

310
F. Masi et al.
With the aim of accelerating multiscale simulations, we propose here a new
class of Artiﬁcial Neural Networks for the material modeling, based on the basic
laws of thermodynamics. Our method, which we denote as Thermodynamics-
based Artiﬁcial Neural Networks (TANNs), assures thermodynamically con-
sistent network’s predictions, for data both close to and beyond the training
domain. The proposed approach can be advantageous when modeling complex
and abstract constitutive behaviors, which are not a priori known. It can be
used even if the BVP does not have a unique solution due to important non-
linearities and bifurcation phenomena (e.g. loss of uniqueness, strain localiza-
tion at the length of interest, multiphysics, runway instabilities etc.). Finally,
TANNs are characterized by reduced computing time, diﬀerently from other
thermodynamics-based approaches [6,7], which render them an excellent accel-
erating tool for multiscale simulations.
For the implementation of Artiﬁcial Neural Networks and Thermodynamics-
based
Artiﬁcial
Neural
Networks,
we
leverage
Tensorﬂow
v2.0.
Source
codes are available at https://github.com/ﬂpmasi/Thermodynamics-Neural-
Networks [38].
This Chapter is organized as follows. Section 2 presents an overview of ANN
methods and their implementation for material modeling, denoted here as stan-
dard ANNs. We present in Sect. 3 the new class of Thermodynamics-based Artiﬁ-
cial Neural Networks, as opposed to standard ANN methods, for the constitutive
modeling, at the material point level. Finally, Sect. 4 presents a motivating appli-
cation of our approach in modeling history-dependent material behaviors with
kinematic softening. Extensive comparisons with standard ANNs, which are not
based on thermodynamics, are also presented.
2
Artiﬁcial Neural Networks for Constitutive Modeling
Machine Learning is a general term to describe a large spectrum of numerical
methods. Some of them oﬀer very rich interpolation spaces, which, in theory,
could be used for approximating complicated functions belonging to uncommon
spaces. Here we focus on the method of Artiﬁcial Neural Networks (ANNs),
which is considered to be a sub-class of Machine Learning methods. According
to Cybenko [4] and Chen and Chen [3], ANNs have proved to be universal
approximators, due to their rich interpolation space.
2.1
Artiﬁcial Neural Networks
Artiﬁcial Neural Networks (ANNs) can be regarded as non-linear operators [11,
24], composed of an assembly of mutually connected processing units−nodes−,
which take an input signal I and return the output O, namely
O = ANN@I.
(1)

Material Modeling via Thermodynamics-Based Artiﬁcial Neural Networks
311
ANNs consist of at least three types of layers: input, output and hidden layers,
with equal or diﬀerent number of nodes. Figure 1 depicts a network composed of
three hidden layers, with three nodes each, an input layer with three nodes, and
an output layer with one node. When an ANN has two or more hidden layers, it
is called a deep neural network [11]. Denoting the input array with I = (it), with
t = 1, 2 . . . , nI (nI is the number of inputs), and the outputs with O = (oj),
with j = 1, 2 . . . , nO (nO is the number of outputs), the signal ﬂows from layer
(l −1) to layer (l) according to
p(l)
k = A(l) 
z(l)
k

,
with
z(l)
k
=
n(l−1)
N
s

w(l)
ksp(l−1)
s

+ b(l)
k ,
(2)
where p(l)
k
are the outputs of node k, at layer (l); A(l) is the activation function
of layer (l); n(l−1)
N
is the number of neurons in layer (l −1); w(l)
ks are the weights
between the s-th node in layer (l −1) and the k-th node in layer (l); and b(l)
k
are
the biases of layer (l).
The weights and biases of interconnections are adjusted, in an iterative proce-
dure [gradient descent algorithm 11], to minimize the error between the bench-
mark, O, and prediction, O, that is measured by a loss function, L. In the
following, the Mean (over a set of N samples) Absolute Error (MAE) is used as
loss function, i.e.,
L =
N
i=1 |Oi −Oi|
N
,
(3)
where i = 1, 2, . . . N. The errors related to each node of the output layer are
hence back-propagated to the nodes in the hidden layers and used to calculate
the gradients of the loss function, namely
∂L
∂w(l−m)
ks
= ∂z(l−m+1)
k
∂w(l−m)
ks
∂p(l−m+1)
k
∂z(l−m+1)
k
∂L
∂p(l−m+1)
k
∂L
∂p(l−m)
k
=
n(l−m+1)
N
j=1
∂z(l−m+1)
j
∂p(l−m)
k
∂p(l−m+1)
j
∂z(l−m+1)
j
∂L
∂p(l−m+1)
j
(4)
which are then used to update weights and biases, and force the minimization
of the loss function values, i.e.
w(l)−new
ks
:= w(l)
ks −ϵ ∂L
∂w(l)
ks
,
(5)
where ϵ is the so-called learning rate. The weights and biases updating, the so-
called training process, is performed on a subset of the input-output data-set,
deﬁned as training set, known from experimental tests or numerical simulations
of the phenomenon investigated. The ANN is trained. The training process is
stopped as the loss function is below a speciﬁc tolerance. Then a test set, a sub-
set of the input-output data-set diﬀerent to the training set, is used to check the

312
F. Masi et al.
Fig. 1. Graph illustration of an ANN structure with three inputs, one output, and three
hidden layer with three nodes. In particular, the ANN structure models the incremental
stress-strain constitutive response of a material. From the knowledge of the material
state at time t, the model predicts the material stress increment, Δσ, corresponding
to the given material strain increment, Δε.
error of the network predictions. Once the ANN is trained, it is used in recall
mode to obtain the output of the problem at hand.
Although ANNs have proved to be universal approximators [3,4], the choice
of hyper-parameters, such as the number of neurons, the network topology, the
weights, etc. are problem-dependent. The same stands for the activation func-
tions, which may be chosen to have some desirable properties of non-linearity, dif-
ferentiation, monotonicity, etc. Most of these properties stem from issues related
to the gradient descent algorithm and the so-called vanishing gradient problem.
2.2
Material Modeling via Artiﬁcial Neural Networks
Artiﬁcial Neural Networks (ANNs) have demonstrated to be successful in the
constitutive modeling of history-dependent materials from model identiﬁcation
based on experiments and detailed numerical simulations. Starting form the
seminal work of Ghaboussi et al. [13] and without being exhaustive, we refer
to Ghaboussi and Sidarta [12], Leﬁk and Schreﬂer [32], Jung and Ghaboussi
[28], Settgast et al. [44], Liu and Wu [34], Lu et al. [36], Xu et al. [46], Huang
et al. [25], Liu and Wu [34], Gajek et al. [10], Gorji et al. [18], Heider et
al. [20], Ghavamian and Simone [15], Mozaﬀar et al. [41], Frankel et al. [9],
Gonz´alez et al. [16], Gorji et al. [18]. The main idea in these works is to appro-
priately train ANNs, feeding them with material data, and predict the material
response at the material point level. The trained models are then used as material

Material Modeling via Thermodynamics-Based Artiﬁcial Neural Networks
313
subroutines, replacing the standard constitutive equations or algorithms, in
numerical simulations and speed up the otherwise prohibitive computing time of
multiscale analyses [1,37]. This replacement is straightforward and non-intrusive
in Finite Element (FE) codes. We record, for instance, the successful embedding
of ANNs as material description subroutines in FE codes by Leﬁk and Schreﬂer
[32], Jung and Ghaboussi [28], Leﬁk et al. [33], Settgast et al. [44]. Ghavamian
and Simone [15] further implemented ANNs in a FE2 scheme for accelerating
multiscale FE simulations for materials displaying strain softening, with Perzyna
viscoplaticity model.
Several ANN models and architectures have been developed to promote the
accuracy of the material response predictions. For instance, Fig. 1 depicts a vari-
ant of the Nested Artiﬁcial Neural Networks ﬁrst proposed by Ghaboussi and
Sidarta [12] for the modeling of geo-materials, with history-dependent behav-
ior. Furthermore, we record the works of Heider et al. [20], Mozaﬀar et al. [41],
Gorji et al. [18] who developed ANN models incorporating some knowledge in an
informed, guided graph with intermediate history-dependent variables or detect-
ing history-dependent features. We show in Fig. 2 the graph of a standard ANN
for the modeling of history-dependent material responses. The inputs of the net-
work are the strain increment, Δε, and the material state at time t, characterized
by strains, εt, stresses, σt, and an additional set of variables (e.g., plastic strains,
temperature, etc.), denoted here as Zt. The model outputs are the additional
variables increments (e.g. the plastic strain or temperature increment), ΔZ, and
the stress increments, Δσ, computed via two distinct standard ANNs: sANNZ
and sANNσ, respectively. In an early work of the authors [39], the foregoing
model was successfully used to model von Mises plasticity with kinematic hard-
ening and softening. Nevertheless, several issues aﬄicting the model accuracy
were found. The reason lies in the lack of a rigorous framework based on the
laws of physics for standard ANNs, see e.g. [39,43]. A large number of quality
and error-free data is usually needed to enable standard ANNs to identify and
learn the underlying thermodynamic laws (although without any guarantee that
the predictions will be thermodynamically consistent).
The seminal work of Raissi et al. [43] showed the possibility of adding phys-
ical constraints within the architecture of Artiﬁcial Neural Networks, by taking
advantage of the reverse-mode autodiﬀ[2] in the numerical computation of the
derivatives of an ANN with respect to its inputs. The new class of physics-
based ANNs, denoted as Physics-Informed Artiﬁcial Neural Networks (PINNs),
demonstrated its superiority with respect to standard ANNs [43]. Inspired by
the aforementioned PINNs, Xu et al. [46] developed Symmetric Positive Deﬁnite
Neural Networks (SPD-NNs) for modeling constitutive relationships by assuring
symmetric positive deﬁnite stiﬀness matrices.

314
F. Masi et al.
Fig. 2. Graph of an informed ANN for the modeling of history-dependent material
responses. The inputs of the network are the strain increment, Δε, and the material
state at time t, characterized by strains, εt, stresses, σt, and an additional set of
variables, Zt. The model outputs are the additional variables increments (e.g. the
plastic strain or temperature increment), ΔZ, and the stress increments, Δσ, computed
via two distinct standard ANNs: sANNZ and sANNσ, respectively
3
Thermodynamics-Based Artiﬁcial Neural Networks
Inspired by the work of Raissi et al. [43], we propose a new class of neu-
ral networks based on the laws of thermodynamics, which we denote as
Thermodynamics-based Artiﬁcial Neural Networks (TANNs). This is accom-
plished by hardwiring the ﬁrst and second principle of thermodynamics in the
architecture of the network. In comparison with standard ANNs, two additional
scalar functions needs to be identiﬁed in the training data-set. These are the
free-energy and the mechanical dissipation rate. However, these quantities are
easily accessible in micromechanical simulations [e.g. 8,29,42,45] and can also
be obtained experimentally in some cases.
The theoretical framework of our model is presented in Sect. 3.1 and the
architecture of TANNs is further detailed in Sect. 3.2.
3.1
Thermodynamics Principles and Theoretical Framework
Let us consider an isothermal constitutive stress-strain response. The (local)
power balance equation of energy can be expressed as
˙F = W −D,
(6)
where W = σ · ˙ε is the mechanical rate of work, with σ being the Cauchy stress
tensor, ε and ˙ε the inﬁnitesimal strain tensor and its rate of change, and “·”

Material Modeling via Thermodynamics-Based Artiﬁcial Neural Networks
315
denoting the contraction of adjacent indices; F and ˙F are the Helmholtz free
energy potential and its rate of change; and D is the rate of the mechanical
dissipation.
From the second principle of thermodynamics, the rate of the mechanical
dissipation must be non-negative,
D ≥0.
(7)
Furthermore, let us consider strain-rate independent constitutive material
responses with the following form of the Helmholtz free energy,
F := F (ε, Z) ,
(8)
and dissipation rate,
D := D

ε, Z, ˙Z

,
(9)
being a ﬁrst-order homogeneous function of ˙Z, where Z = (ζi, . . . , ζN) denotes
a set of N (additional) internal state variables, ζi, i = 1, . . . , N. We deﬁne here
(thermodynamic) state variables those macroscopic quantities characterizing the
state of a system, see e.g. [40]. The physical representation of ζi is not a pri-
ori prescribed. For instance, in the case of isotropic damage, ζ is a scalar; for
anistotropic damage, a tensor; in the case of elasto-plasticity, a second order
tensor, etc. We emphasize that identiﬁcation of the internal variables set may be
not trivial for systems with increasing complexity. As proposed in [6], internal
variables can be interpreted as history variables, such as the strain and stress at
precedent times. This renders the formalism here adopted general and ﬂexible,
depending on the applications.
By time diﬀerentiating the energy potential, we obtain
˙F = ∂F
∂ε · ˙ε +
N

i=1
∂F
∂ζ i
· ˙ζi,
(10)
which, after substitution in Eq. (6), leads to
∂F
∂ε −σ

· ˙ε −
	 N

i=1
∂F
∂ζ i
· ˙ζi + D

= 0.
(11)
From the arbitrariness of ˙ε and ˙ζi, we obtain the following relations
σ = ∂F
∂ε ,
(12a)
D = −
N

i=1
∂F
∂ζ i
· ˙ζi.
(12b)

316
F. Masi et al.
3.2
Architecture of TANNs
By exploiting the theoretical background presented above, Thermodynamics-
based Artiﬁcial Neural Networks are ANNs which respect, by deﬁnition, the
thermodynamic principles, holding true for any class of material. In this frame-
work, TANNs posses the special feature that the entire constitutive response of
a material can be derived from deﬁnition of only two (pseudo-) potential func-
tions: an energy function and a dissipation function [23]. TANNs are fed with
thermodynamics “information”, namely Eqs. (12a) and (12b), by relying on the
automatic diﬀerentiation technique [2] to diﬀerentiate neural networks outputs
with respect to their inputs. This strategy allows to construct a general frame-
work of neural networks material models which, in principle, can be exploited to
predict the behavior of any material and assure that the predictions of TANNs
will be thermodynamically consistent even for inputs that exceed the training
range of data. Herein, we only focus on strain-rate independent processes. Nev-
ertheless, our approach can be extended, following the developments in [22], to
materials showing viscosity and strain-rate dependency.
The model relies on an incremental formulation and can be used in existing
Finite Element formulations (among others), see e.g. [32]. Figure 3 illustrates
the scheme of TANNs. The model inputs are the strain increment, the previous
material state at time t, which is identiﬁed herein through the material stress,
σt, and the internal state variables, ζt
i, namely I = (εt, Δε, σt, Zt). The primary
outputs are the internal variables increment, ΔZ, and the energy potential at
time t + Δt, Ft+Δt. In particular, the increment of the internal variables are
predicted by a sub-ANNs,
ΔZ = ANNZ@{εt, Δεt, σt, Zt}.
(13)
A second sub-ANN is used to predict the energy potential, i.e.,
Ft+Δt = ANNF@{εt+Δt, Zt+Δt}.
(14)
Then, secondary outputs−that is, outputs computed by diﬀerentiation of the
neural network with respect to the inputs−are computed: the stress increment,
Δσ, and the dissipation rate, Dt+Δt, i.e., ∇IO = (Δσ, Dt+Δt). In particular, the
stress increment is computed by subtraction of the diﬀerential Eq. (12a) and the
stress at time t, i.e.,
Δσ = ∂Ft+Δt
∂εt+Δt −σt.
(15)
While, the dissipation rate is computed using Eq. (12b), by approximating the
rate of the internal variables as ˙Zt+1 ≈ΔZ
Δt .
4
Application to History-Dependent Materials
with Softening
Relying on the aforementioned architecture of TANNs, we investigate their abil-
ity in modeling the response of history-dependent materials. In particular, we

Material Modeling via Thermodynamics-Based Artiﬁcial Neural Networks
317
Fig. 3.
Graph
of
Thermodynamics-based
Artiﬁcial
Neural
Networks
for
the
isothermal constitutive modeling of strain-rate independent materials. The model
involves the following steps: (1)
prediction of the kinematic variables ΔZ
=
ANNZ@

εt+Δt, Δε, σt, Zt
; (2)
computation of the updated kinematic variables
rates,
˙Zt+1 ≈ΔZ/Δt, and the updated kinematic variables, Zt+1 := Zt + ΔZt;
(3)
prediction of the updated energy potential Ft+Δt = ANNF@{εt+Δt, Zt+Δt};
(4)
computation of the updated dissipation rate using Eq. (12b); (5)
computation
of the stress increment, Eq. (12a), Δσ = ∂Ft+Δt/∂εt+Δt −σt.
use TANNs in modeling 1D elasto-plastic materials with kinematic softening.
While the motivating example we consider may be rather simple, we emphasize
that the proposed class of ANN can be successfully applied (without any modi-
ﬁcation) to materials with diﬀerent or more complex behavior, see [39].
4.1
Material Model and Data
The elasto-plastic 1D model with kinematic softening−1D spring-slider [23], see
Fig. 4−is characterized by the following expressions for the Helmholtz free-energy
potential, dissipation rate, and yield function:
F =E
2 (ε −εp)2 + H
2 (εp)2 ,
D = k| ˙εp|,
and
y =|σ −Hεp|
k
−1 ≤0,

318
F. Masi et al.
where ε is the total strain; εp the plastic strain; σ the Cauchy stress; E the
Young modulus; H the kinematic softening parameter; and k the yield strength
(slider threshold). The incremental material response is given by
˙σ =

E ˙ε
when y < 0,
E∗˙ε
when y = 0;
˙εp =
⎧
⎨
⎩
0
when y < 0,
E∗
H ˙ε
when y = 0;
(16)
with E∗= EH/(E + H).
As it follows, we consider the following material parameters: E = 200 GPa,
k = 200 MPa, and H = −100 MPa. Furthermore, we select the internal variable
ζ−see Eqs. (8) and (8)−such that they coincide with the plastic deformation,
i.e., ζ = εp. We emphasize that our approach is general and not limited to this
kind of state variables.
Fig. 4. Schematic representation of the spring-slider model with kinematic softening
(top) and cyclic stress-strain behavior (bottom).
Data are generated by identifying random states for the material at time t.
Random strain increments ˙ε are then applied, assuming constant and unitary
time increment Δt = 1 ( ˙ε = Δε). The material state at time t + Δt is then
obtained by numerical integration of the incremental constitutive relations (16).
In particular, 2000 data (random increments at random states) are generated.
Additionally, an artiﬁcial subset of data is considered and constructed assuming
Δε = 0 for the initial random states. Such an artiﬁcial data-set is added to the
generated set of data, in order to facilitate the network to understand that to
zero increments of strain correspond zero increments of stress and plastic strain.
Figure 5 shows the sampling for total strain, internal variable, and stress. We
present in Table 1 the mean (μ), standard deviation (st), and maximum values
(max) of the data-set.

Material Modeling via Thermodynamics-Based Artiﬁcial Neural Networks
319
Fig. 5. Sampling of the material data: total strains εt, Δε, εt+Δt (top), plastic strains
ζt, Δζ, ζt+Δt (center), and stresses σt, Δσ, σt+Δt (bottom). 2000 random increments
are applied to random states (a), then an artiﬁcial subset of data is added considering
Δε = 0 for the initial random states (b).
4.2
Training
Training is performed with 50% of the generated data. A validation set consisting
of 25% of the generated data is used to avoid over-ﬁtting. We take advantage
of the technique of early-stopping−that is, training is stopped as the error of
a validation set starts to increase while the learning error still decreases [11].
Finally, a test set (25% of the generated data) is used to evaluate the network
predictions, once training is accomplished.
The hyper-parameters characterizing the network (i.e., number of hidden lay-
ers, neurons, activation functions, etc.) are selected to give the best predictions,
while requiring minimum number of hidden layers and nodes per layer. This is
accomplished by comparing the learning error on the set of test patterns, per
each trial choice of the hyper-parameters.

320
F. Masi et al.
Table 1. Mean (μ), standard deviation (st), and maximum values (max) of the training
data-sets.
Data
μ
st
max
εt
(-)
5 × 10−4 0.010 0.038
Δε
(-)
1 × 10−4 0.003 0.040
ζt
(-)
5 × 10−4 0.010 0.040
Δζ
(-)
1 × 10−4 0.003 0.001
σt
i
(MPa)
3.724
208.6 458.8
Δσi
(MPa)
−7.745
217.6 400.0
Ft+Δt
(N-mm)
−0.500
0.853 0.105
Dt+Δt (N-mm/s) 0.384
0.416 2.333
Adam optimizer with Nesterov’s acceleration gradient [5] is selected and a
batch size of 10 samples is used. We use the Mean Absolute Error (MAE) as loss
functions for each output in order to assure the same precision between data of
low and high numerical values (cf. Mean Square Error). Regularized weights are
used to have consistent order of magnitude of diﬀerent quantities involved in the
loss functions. The architecture of TANNs, as presented in Sect. 3.2 consists of
two sub-ANNs. The former, ANNZ, is composed of four input nodes, one hidden
layer with 6 neurons and leaky ReLU activation function, and output node for
the predictions of Δζ. The latter sub-network, ANNF, has two input nodes, one
hidden layer with 9 neurons and a modiﬁed version of ELU as activation function,
A(zk) =

z2
k
if zk > 0,
exp(zk) −1
else,
output node to predict Ft+Δt. The modiﬁed ELU activation function is used to
avoid the so-called second-order vanishing gradients issue, see [37,39] for more.
The output layers for both sub-networks have linear activation function and
biases set to zero. The corresponding number of degrees of freedom, i.e., the
number of the hyper-parameters, is 72. Higher number of hidden layers could be
used as well, but this is out of the scope of our investigations. Figure 6 displays
the loss functions of each output as the training is performed, i.e., in number
of epochs. The early stopping rule assures convergence with MAEs of the same
order of magnitude for the 4 outputs, Δζ, Ft+Δt, Δσ, and Dt+Δt.
4.3
Predictions in Recall Mode
Once the neural network has been trained, we use it in recall mode to predict the
stress increment for a given strain, strain increment, and possibly other variables,
and we compare the predictions with the corresponding targets. The results of
the numerical integration scheme−Eq. (16)−are here considered as the exact

Material Modeling via Thermodynamics-Based Artiﬁcial Neural Networks
321
0
10
100
1000
epochs
MAE
Δζtrain
Δζval
Ftrain
Fval
Δσ train
Δσ val
Dtrain
Dval
10
1
10−1
10−2
10−3
10−4
10−5
Fig. 6. Errors in terms of the adimensional Mean Absolute Error (MAE) of the pre-
dictions of TANN (loss functions), as the training is being performed, evaluated with
respect to the training (train) and validation (val) sets. Weights and biases update are
computed only on the training set.
solution of the material response. In particular, starting from an initial conﬁgu-
ration, we make cyclic (or random) increments of the strain, Δε. TANNs hence
predict the corresponding increments, {Δζ, Δσ}, which will be transformed into
the inputs in the successive call, as well as the energy and dissipation rates,
{Ft+Δt, Dt+Δt}. This procedure is applied recursively. The neural network is so
self-fed. Figure 7 illustrate the predictions of TANNs for random loading paths
with strain increments varying from 10−5 and 0.01.
TANNs are found to successfully predict all quantities of interest. Moreover,
and most important, the architecture and the training of the network allows to
obtain thermodynamically consistent results. The ﬁrst law of thermodynamics
is automatically satisﬁed as a result of the structure of TANNs and the pre-
dicted dissipation rate is always positive. Indeed, even if the second principle of
thermodynamics is not explicitly assured by the TANNs architecture, the fact
that the training has been performed with consistent material data (i.e., positive
dissipation rate) results automatically in the fulﬁllment of the second principle.
It worth noticing that the random loading path consists of values of strain
increments beyond the training range (see Table 1). However, TANNs predictions
are extremely accurate. Indeed, the thermodynamics framework renders the gen-
eralization capability of TANN (i.e., the ability to make predictions for loading
paths diﬀerent from those used in the training operation) remarkably good. This
is usually not observed in standard ANNs as extensively discussed by Leﬁk and
Schreﬂer [32]. We investigate more in detail the generalization capabilities of our
model with respect to standard ANNs, in the next paragraph.
4.4
TANNs Versus Standard ANNs
We compare herein the performance and generalization ability of TANNs with
respect to standard ANNs for constitutive modeling [13,32]. In particular,
we select the ANN architecture depicted in Fig. 2. The hyper-parameters are
selected to give the best performance while assuring the same amount of degrees
of freedom, hyper-parameters, with respect to TANNs. Both sub-networks,

322
F. Masi et al.
Fig. 7. Predictions of TANNs for a random loading path with strain increments varying
from 10−5 and 0.01 (beyond the training range, see Table 1): (a) loading path, (b) stress
and plastic strain, (c) energy potential and dissipation rate.
sANNZ and sANNσ, consist of one hidden layer, with 6 neurons each and leaky
ReLU activation function. As for TANNs, the output layers have linear activa-
tion function and zero bias. Training is performed on the same set of samples
that are used for the thermodynamics-based network. Figure 8 displays the error
of the predictions of standard ANNs, as training is performed, and compares it
with TANNs.
It is worth emphasizing that both ANNs and TANNs are dependent on the
choice of the user, concerning, for instance, the number of hyper-parameters.
Moreover, the actual conﬁgurations of both networks may beneﬁt of alterna-
tives/extensions, such as Recurrent Neural Networks. Nevertheless, the following
comparisons show the added value of our approach compared to standard ones
that do not explicitly contain physics.

Material Modeling via Thermodynamics-Based Artiﬁcial Neural Networks
323
Δζtrain-TANN
Δζtrain-ANN
Δζval-TANN
Δζval-ANN
0
10
100
1000
epochs
MAE
10−1
10−2
10−3
10−4
10−5
(a) Mean Absolute Error (MAE) of Δζ predic-
tions
Δσtrain-TANN
Δσ train-ANN
Δσ val-TANN
Δσ val-ANN
0
10
100
1000
epochs
MAE
1
10−1
10−2
10−3
10−4
(b) Mean Absolute Error (MAE) of Δσ predic-
tions
Fig. 8. Training of ANNs compared with TANNs evaluated with respect to the training
(train) and validation (val) sets.
Once both networks have been trained, we compare the predictions of TANNs
and standard ANNs, for a cycling loading path Δεn = Δε sgn

cos nπ
2N

−with
Δε ∈(10−5, 1). The results are presented in Fig. 9, in terms of stress and plastic
strain increments.
TANNs are clearly superior in terms of (a) accuracy of the prediction and (b)
generalization with respect to the inputs. Moreover, standard ANNs predictions
do not fulﬁll the principles of thermodynamics, even though the training of the
network has been performed on consistent material data. This is clearly shown
by computing from the predictions of ANNs the increment of the Helmholtz
free-energy and dissipation rate using the corresponding deﬁnitions, Eq. (4.1).
Figure 10 displays the comparison in terms of energy potential, F, and dissipa-
tion rate, D. The predictions of the standard ANNs clearly do not respect the
thermodynamics principles (both the ﬁrst and second laws).
We emphasize that even for relatively large and far beyond the training range
strain increments, the predictions of TANNs are extremely accurate thanks to
their thermodynamic basis. Moreover, TANNs successfully learn the Jacobian,
i.e. the elasto-plastic matrix ∂Δσ
∂Δϵ [see 19]. The Jacobian is computed for both
networks, standard ANNs and TANNs, as the derivative of the stress increment
(Δσ, output) with respect to the strain increment (Δε, input). Figure 11 shows
the Jacobian and the comparison with the predictions of TANNs and standard
ANNs. The excellence of TANNs is clear: the predictions of the Jacobian are in
very good agreement with the reference values (with accuracy between 99.0%
and 99.99% for all considered data-sets). This is not true for standard ANNs,
especially for very small and large strain increments.
As a result, TANNs are found to be an excellent candidate for such a sub-
stitution, allowing to foresee their application to eﬃciently speed-up multiscale
analyses and FE simulations.

324
F. Masi et al.
Fig. 9. Predictions of TANNs and standard ANNs for a cyclic loading path, with
diﬀerent strain increments (a-d), in terms of stresses (left column) and plastic strain
(right column).
It is worth noticing that increasing the size of the training data-sets and/or
the number of layers and neurons could improve the accuracy of standard
ANNs. However, there will not be guarantee that the predictions would be ther-
modynamically consistent. Thermodynamics-based Neural Networks excel over
physics-unaware ANNs.

Material Modeling via Thermodynamics-Based Artiﬁcial Neural Networks
325
-1.0
-0.5
0.0
0.5
1.0
1.5
ε −ζ (-)
1e-3
0.0
0.1
0.2
0.3
0.4
0.5
F (N-mm)
-1.0
-0.5
0.0
0.5
1.0
Δζ (-)
1e-5
-0.001
0.000
0.001
0.002
0.003
0.004
0.005
D (N-mm/s)
model
TANN
ANN
model
TANN
ANN
(a) strain increment: Δε = 10−5 (inside training range)
-1.0
-0.5
0.0
0.5
1.0
ε −ζ (-)
1e-3
0.0
0.1
0.2
0.3
0.4
F (N-mm)
-1.0
-0.5
0.0
0.5
1.0
Δζ (-)
1e-3
0.0
0.1
0.2
0.3
0.4
0.5
D (N-mm/s)
model
TANN
ANN
model
TANN
ANN
model
TANN
ANN
(b) strain increment: Δε = 10−3 (inside training range)
-1.0
-0.5
0.0
0.5
ε −ζ (-)
1e-2
0
500
1000
1500
2000
F (N-mm)
-1.0
-0.5
0.0
0.5
1.0
Δζ (-)
1e-2
-300
-200
-100
0
100
200
300
D (N-mm/s)
model
TANN
ANN
model
TANN
ANN
(c) strain increment: Δε = 10−2 (beyond training range)
-1.00-0.75-0.50-0.25 0.00 0.25
ε −ζ (-)
-500
0
500
1000
1500
2000
2500
3000
F (kN-mm)
-1.0 -0.5
0.0
0.5
1.0
1.5
Δζ (-)
-600
-400
-200
0
200
400
600
800
D (kN-mm/s)
model
TANN
ANN
model
TANN
ANN
(d) strain increment: Δε = 1 (beyond training range)
Fig. 10. Predictions of TANNs and standard ANNs for a cyclic loading path, with
diﬀerent strain increments (a-d), in terms of Helmholtz free energy (left column) and
mechanical dissipation rate (right column).

326
F. Masi et al.
Fig. 11. Jacobian predictions for a cyclic loading path, with diﬀerent strain increments
(a-d), cf. Figures 9 and 10.
5
Conclusions
A new class of artiﬁcial neural networks models to replace constitutive laws and
predict the material response at the material point level was proposed. The two
basic laws of thermodynamics were directly encoded in the architecture of the
model, which we refer to as Thermodynamics-based Neural Networks (TANNs).
TANNs, relying on an incremental formulation and on a theoretical frame-
work of thermodynamics, posses the special feature that the entire constitutive
response of a material can be derived from deﬁnition of only two scalar func-
tions: the free-energy and the dissipation rate. This assures thermodynamically
consistent predictions for data both close to and beyond the training domain.
Diﬀerently from the standard ANN approaches, TANN does not have to iden-
tify, through learning, the underlying thermodynamic laws. Indeed, predictions
of standard ANNs may be thermodynamically inconsistent, even though the
training of the network has been performed on consistent material data.
For the cases here investigated, TANNs are found to be characterized by
high accuracy of the predictions, higher than those of standard approaches.
The integration of thermodynamic principles inside the network renders TANN’s
ability of generalization (i.e., make predictions for loading paths diﬀerent from

Material Modeling via Thermodynamics-Based Artiﬁcial Neural Networks
327
those used in the training operation) remarkably good. Consequently, TANNs
are excellent candidates for replacing, in future applications, constitutive cal-
culations at Finite Element incremental formulations. Moreover, thanks to the
implementation of the free-energy in the network predictions and its thermo-
dynamical relation with the stresses, the Jacobian at the material point level
is excellently predicted, even for increments far outside the training data-set
range. As a result quadratic convergence in implicit formulations can be pre-
served, reducing the calculation cost.
Whilst the motivating example to a 1D elasto-plastic material with kinematic
softening is easier than real, complex 3D materias, further extensions of TANNs
to a wider range of applications are straightforwards [see e.g. 39], as the ther-
modynamics principles hold true for any known class of material, at any length
(micro- and macro-scale).
Acknowledgements. The author I.S. would like to acknowledge the support of the
European Research Council (ERC) under the European Union Horizon 2020 research
and innovation program (Grant agreement ID 757848 CoQuake).
References
1. Alber, M., et al.: Multiscale modeling meets machine learning: What can we learn?
arXiv preprint arXiv:1911.11958 (2019)
2. Baydin, A.G., Pearlmutter, B.A., Radul, A.A., Siskind, J.M.: Automatic diﬀer-
entiation in machine learning: a survey. J. Mach. Learn. Res. 18(1), 5595–5637
(2017)
3. Chen, T., Chen, H.: Universal approximation to nonlinear operators by neural net-
works with arbitrary activation functions and its application to dynamical systems.
IEEE Trans. Neural Netw. 6(4), 911–917 (1995)
4. Cybenko, G.: Approximation by superpositions of a sigmoidal function. Math.
Control Signals Syst. 2(4), 303–314 (1989)
5. Dozat, T.: Incorporating Nesterov momentum into Adam (2016)
6. Eggersmann, R., Kirchdoerfer, T., Reese, S., Stainier, L., Ortiz, M.: Model-free
data-driven inelasticity. Comput. Methods Appl. Mech. Eng. 350, 81–99 (2019)
7. Eggersmann, R., Stainier, L., Ortiz, M., Reese, S.: Model-free data-driven com-
puational mechanics enhanced by tensor voting. arXiv preprint arXiv:2004.02503
(2020)
8. Feyel, F.: A multilevel ﬁnite element method (FE2) to describe the response of
highly non-linear structures using generalized continua. Comput. Methods Appl.
Mech. Eng. 192(28-30), 3233–3244 (2003). ISSN 00457825. https://doi.org/10.
1016/S0045-7825(03)00348-7
9. Frankel, A.L., Jones, R.E., Alleman, C., Templeton, J.A.: Predicting the mechani-
cal response of oligocrystals with deep learning. Comput. Mater. Sci. 169, 109099
(2019)
10. Gajek, S., Schneider, M., B¨ohlke, T.: On the micromechanics of deep material
networks. Journal of the Mechanics and Physics of Solids, p. 103984 (2020)
11. G´eron, A.: Hands-On Machine Learning with Scikit-Learn, Keras, and TensorFlow:
Concepts, Tools, and Techniques to Build Intelligent Systems. O’Reilly Media,
Sebastopol (2019)

328
F. Masi et al.
12. Ghaboussi, J., Sidarta, D.: New nested adaptive neural networks (NANN) for con-
stitutive modeling. Comput. Geotech. 22(1), 29–52 (1998)
13. Ghaboussi, J., Garrett, J.H., Wu, X.: Knowledge-based modeling of material behav-
ior with neural networks. J. Eng. Mech. 117(1), 132–153 (1991). https://doi.org/
10.1061/(ASCE)0733-9399(1991)117:1(132)
14. Ghaboussi, J., Pecknold, D.A., Zhang, M., Haj-Ali, R.M.: Autoprogressive training
of neural network constitutive models. Int. J. Numer. Methods Eng. 42(1), 105–126
(1998)
15. Ghavamian, F., Simone, A.: Accelerating multiscale ﬁnite element simulations of
history-dependent materials using a recurrent neural network. Comput. Methods
Appl. Mech. Eng. 357, 112594 (2019)
16. Gonz´alez, D., Chinesta, F., Cueto, E.: Learning corrections for hyperelastic models
from data. Front. Mater. 6, 14 (2019)
17. Gonz´alez, D., Chinesta, F., Cueto, E.: Learning non-markovian physics from data.
J. Comput. Phys. 428, 109982 (2020)
18. Gorji, M.B., Mozaﬀar, M., Heidenreich, J.N., Cao, J., Mohr, D.: On the potential
of recurrent neural networks for modeling path dependent plasticity. Journal of the
Mechanics and Physics of Solids, p. 103972 (2020)
19. Hashash, Y., Jung, S., Ghaboussi, J.: Numerical implementation of a neural net-
work based material model in ﬁnite element analysis. Int. J. Numer. Methods Eng.
59(7), 989–1005 (2004)
20. Heider, Y., Wang, K., Sun, W.: SO(3)-invariance of informed-graph-based deep
neural network for anisotropic elastoplastic materials. Comput. Methods Appl.
Mech. Eng. 363, 112875 (2020). ISSN 0045-7825. https://doi.org/10.1016/j.cma.
2020.112875
21. Hern´andez, Q., Bad´ıas, A., Gonz´alez, D., Chinesta, F., Cueto, E.: Structure-
preserving neural networks. J. Comput. Phys. 426, 109950 (2020)
22. Houlsby, G., Puzrin, A.: A thermomechanical framework for constitutive models
for rate-independent dissipative materials. Int. J. Plast. 16(9), 1017–1047 (2000)
23. Houlsby, G.T., Puzrin, A.M.: Principles of hyperplasticity: an approach to plastic-
ity theory based on thermodynamic principles. Springer Science & Business Media
(2006)
24. Hu, Y.H., Hwang, J.N.: Handbook of neural network signal processing (2002)
25. Huang, D.Z., Xu, K., Farhat, C., Darve, E.: Learning constitutive relations
from indirect observations using deep neural networks. Journal of Computational
Physics, p. 109491 (2020)
26. Iba˜nez, R.: Data-driven non-linear elasticity: constitutive manifold construction
and problem discretization. Comput. Mech. 60(5), 813–826 (2017)
27. Ibanez, R., Abisset-Chavanne, E., Aguado, J.V., Gonzalez, D., Cueto, E., Chinesta,
F.: A manifold learning approach to data-driven computational elasticity and
inelasticity. Arch. Comput. Methods Eng. 25(1), 47–57 (2018)
28. Jung, S., Ghaboussi, J.: Neural network constitutive model for rate-dependent
materials. Comput. Struct. 84(15–16), 955–963 (2006)
29. Karapiperis, K., Stainier, L., Ortiz, M., Andrade, J.: Data-driven multiscale mod-
eling in mechanics. Journal of the Mechanics and Physics of Solids, p. 104239
(2020)
30. Kirchdoerfer, T., Ortiz, M.: Data-driven computational mechanics. Comput. Meth-
ods Appl. Mech. Eng. 304, 81–101 (2016)
31. Kirchdoerfer, T., Ortiz, M.: Data-driven computing in dynamics. Int. J. Numer.
Methods Eng. 113(11), 1697–1710 (2018). https://doi.org/10.1002/nme.5716

Material Modeling via Thermodynamics-Based Artiﬁcial Neural Networks
329
32. Leﬁk, M., Schreﬂer, B.A.: Artiﬁcial neural network as an incremental non-linear
constitutive model for a ﬁnite element code. Comput. Methods Appl. Mech. Eng.
192(28–30), 3265–3283 (2003)
33. Leﬁk, M., Boso, D., Schreﬂer, B.: Artiﬁcial neural networks in numerical modelling
of composites. Comput. Methods Appl. Mech. Eng. 198(21–26), 1785–1804 (2009)
34. Liu, Z., Wu, C.: Exploring the 3D architectures of deep material network in data-
driven multiscale mechanics. J. Mech. Phys. Solids 127, 20–46 (2019)
35. Liu, Z., Wu, C., Koishi, M.: A deep material network for multiscale topology learn-
ing and accelerated nonlinear modeling of heterogeneous materials. Comput. Meth-
ods Appl. Mech. Eng. 345, 1138–1168 (2019)
36. Lu, X., Giovanis, D.G., Yvonnet, J., Papadopoulos, V., Detrez, F., Bai, J.: A
data-driven computational homogenization method based on neural networks for
the nonlinear anisotropic electrical response of graphene/polymer nanocomposites.
Comput. Mech. 64(2), 307–321 (2019)
37. Masi, F.: Fast-dynamics response and failure of masonry structures of non-standard
geometry subjected to blast loads. PhD thesis, Ecole Centrale de Nantes (2020)
38. Masi, F., Stefanou, I.: Thermodynamics-neural-networks. https://github.com/
ﬂpmasi/Thermodynamics-Neural-Networks,
https://doi.org/10.5281/zenodo.
4482668 (2021)
39. Masi, F., Stefanou, I., Vannucci, P., Maﬃ-Berthier, V.: Thermodynamics-based
artiﬁcial neural networks for constitutive modeling. Journal of the Mechanics and
Physics of Solids 147, 104277 (2021)
40. Maugin, G.A., Muschik, W.: Thermodynamics with internal variables. Part I, Gen-
eral concepts (1994)
41. Mozaﬀar, M., Bostanabad, R., Chen, W., Ehmann, K., Cao, J., Bessa, M.: Deep
learning predicts path-dependent plasticity. Proc. Nat. Acad. Sci. 116(52), 26414–
26420 (2019)
42. Nitka, M., Combe, G., Dascalu, C., Desrues, J.: Two-scale modeling of granular
materials: a dem-fem approach. Granular Matter 13(3), 277–281 (2011)
43. Raissi, M., Perdikaris, P., Karniadakis, G.E.: Physics-informed neural networks: a
deep learning framework for solving forward and inverse problems involving non-
linear partial diﬀerential equations. J. Comput. Phys. 378, 686–707 (2019)
44. Settgast, C., Abendroth, M., Kuna, M.: Constitutive modeling of plastic deforma-
tion behavior of open-cell foam structures using neural networks. Mech. Mater.
131, 1–10 (2019)
45. Van den Eijnden, A., B´esuelle, P., Collin, F., Chambon, R., Desrues, J.: Modeling
the strain localization around an underground gallery with a hydro-mechanical
double scale model; eﬀect of anisotropy. Comput. Geotech. 85, 384–400 (2017)
46. Xu, K., Huang, D.Z., Darve, E.: Learning constitutive relations using symmetric
positive deﬁnite neural networks. arXiv preprint arXiv:2004.00265 (2020)

Information Geometry and Quantum
Fields
Kevin T. Grosvenor(B)
Max-Planck-Institut f¨ur Physik komplexer Systeme and W¨urzburg-Dresden Cluster
of Excellence ct.qmat, N¨othnitzer Str. 38, 01187 Dresden, Germany
kevinqg@pks.mpg.de
Abstract. We explore the information geometry associated with a vari-
ety of simple physical systems: the massless scalar φ4 theory and the
2d classical Ising model. We distill out a number of general lessons with
important implications for the application of information geometry to
holography: a geometry may be deﬁned both on the states of one theory
as well as on a family of theories and this geometry reﬂects the symme-
tries, stability, and phase structure of the physical theory. We describe
a family of connections, the full physical content of which remains to be
studied.
1
Introduction
This contribution to the Les Houches Summer Week SPIGL’20 proceedings is
a snippet of the article that I wrote with my collaborators Johanna Erdmenger
and Ro Jeﬀerson [1]. I have elected to omit the examples of the 1d free fermion
description of the 2d Ising model near criticality as well as the quantum coherent
states. I urge the interested reader to take a look at that article for further details
and examples.
Recent years have seen the convergence of information theory, quantum ﬁeld
theory, and gravity in the context of the AdS/CFT correspondence or hologra-
phy, which exploits a duality between a gravitational theory living in Anti-de Sit-
ter (AdS) spacetime and a conformal ﬁeld theory (CFT) living on the boundary
of the “bulk” AdS spacetime. Some examples of contributions from the perspec-
tive of information theory are entanglement-based probes of bulk geometry, such
as Ryu-Takayanagi surfaces and their covariant and quantum extensions [2–4],
quantum error correction in bulk reconstruction from boundary CFT data [5–8],
tensor networks [9–11], and holographic distance measures [12,13], to name a
few.
An overarching goal in this line of research is to show in detail how bulk
geometry emerges essentially as the “information space” of the boundary quan-
tum ﬁeld theory. This aspiration is succinctly summarized in the name of the “It
from Qubit” initiative. However, here, we will not necessarily limit ourselves to
the entanglement probes, or even to the question of complexity [14–16], which are
topics generating a great deal of work. We simply ask whether and in what sense
c
⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2021
F. Barbaresco and F. Nielsen (Eds.): SPIGL 2020, PROMS 361, pp. 330–341, 2021.
https://doi.org/10.1007/978-3-030-77957-3_17

Information Geometry and Quantum Fields
331
a geometric space naturally arises in association with the information content of
a physical theory.
The question of geometry based on information is, in fact, an old and well-
established subject in the ﬁeld of statistics, pioneered by Fisher [17]. The key
quantity of interest in this ﬁeld is the Fisher metric describing the information
geometry as well as a choice of connection on this manifold. Naturally, early
applications to physics were in statistical physics and thermodynamics, in the
work of Souriau [18] and Ruppeiner [19] (see the more recent review [20] of ther-
modynamics from the perspective of information geometry, particularly from
the works of Souriau and Koszul). However, there are a whole host of applica-
tions ranging from mechanics [21], to condensed matter models [22,23], to string
theory [24], to machine learning and neural networks [25,26].
Numerous works have tried to interpret holography from the perspective of
information geometry, e.g., [33–36]. Such works associate the appearance of AdS
geometry with holography. However, we posit that the ubiquity of AdS geometry
in these works is more a reﬂection of the symmetries of the physical theories of
immediate interest in holography, namely CFTs, and does not necessarily give
us new insights into the duality itself. In fact, the most well-understood and
established examples of the AdS/CFT correspondence make use of the large N
saddle point approximation, which is the limit in which the supergravity action
becomes classical and takes on a Gaussian form. Since the duality maps the CFT
partition function to the gravitational one, and the gravitational one is Gaussian
in this limit, we know that the CFT partition function is a Gaussian probabil-
ity distribution even though we may not know the explicit degrees of freedom
which form the “random variables” of this Gaussian distribution. However, it
is a well-known fact that the geometry of Gaussian distributions is hyperbolic
[27] and therefore the appearance of hyperbolic space is not in itself necessarily
signiﬁcant.
An example of a direct application of the Fisher metric to AdS/CFT is the
work of Blau, Narain and Thompson [28] which aims to show how the bulk
AdS5 spacetime of Type IIB supergravity on AdS5 × S5 may arise as the infor-
mation geometry on the moduli space of instantons of N = 4 and N = 2 SU(N)
super Yang-Mills theory. This claim is based on and substantiated by earlier
work demonstrating that these instantons are good probes of the bulk geometry
[32]. We will examine a simpliﬁed version of this calculation in the form of the
massless scalar theory with an inverted φ4 interaction potential – the instanton
information geometry in this theory is also 5d hyperbolic space.
Our work represents our initial exploration into information geometry and its
physics content with a view towards applications to holography. We hope that
the lessons learned from the examples in this work will prove helpful for future
investigations into this fascinating subject.
2
Symmetries and the Fisher Metric
Symmetries are an important aspect of the AdS/CFT duality and the matching
of the conformal symmetry group on the CFT side with the isometry group

332
K. T. Grosvenor
on the AdS side was an important check of the duality in the ﬁrst place. The
conformal symmetry of the CFT is reﬂected in the isometries of the information
geometry on its states, which enjoy the same conformal symmetry. Therefore, it
is likely that the frequent appearance of AdS in the applications of information
geometry to holography is merely a reﬂection of this symmetry principle and is
not necessarily related to holography per se. Let us substantiate this claim.
Firstly, the Fisher metric [17] on a family of probability distributions p(x; θ)
on a random variable x ∈X parametrized by the label θ ∈Θ is deﬁned to be
gij(θ) =
∫
dμ(x) p(x; θ) ∂ln p(x; θ)
∂θi
∂ln p(x; θ)
∂θ j
,
(1)
where μ(x) is a measure on the random variable space X.1 Technically, we also
assume that the map θ →p(x; θ) is injective, so that Θ is immersed into the
family of probability distributions. In fact, in the simplest cases of interest to us
presently, we can assume that the map is bijective, so that the immersion can
be upgraded to an embedding.
A symmetry of the family of probability distributions p(x; θ) is a map ˜θ :
Θ →Θ such that there exists a map ˜x : X →X such that
p(x; ˜θ) dμ(x) = p(˜x; θ) dμ(˜x).
(2)
Since the random variable x is a “dummy” variable that is eventually integrated
over, the above relation simply states that the transformation from θ to ˜θ can be
undone by an appropriate change of random variable. Bear in mind that there is
an implicit relative factor relating dμ(x) and dμ(˜x), which is the usual Jacobian
factor for the x →˜x(x) change of variables.2
A simple example is the isotropic d-dimensional Gaussian distribution. In
this case, X = Rd with Cartesian coordinates x ∈X equipped with the standard
Lebesgue measure, while Θ = Rd × R>0 with coordinates µ ∈Rd and σ ∈R>0:
p(x; σ, µ) =

1
√
2π σ
d
exp

−|x −µ|2
2σ2

.
(3)
This is clearly symmetric under the constant shift of the mean µ →˜µ = µ + c
since this can be undone by shifting the random variable by the same amount
x →˜x = x + c. A less trivial example is the scaling or dilatation symmetry of
1 It is possible to extend the Fisher metric on probability distributions, which have
to be normalized via
∫
dμ(x) p(x; θ) = 1, to non-negative measures by relaxing
the normalization condition. In that setting, the metric is often called the Fisher-
Shahshahani metric in honor of the work of Shahshahani [29]. The denormalization
of a statistical manifold is also discussed in Sect. 2.6 of Amari’s textbook [27] (see
also [30]). However, we will not pursue this topic here.
2 There is a more general concept in the mathematical literature of a restricted Markov
morphism (see Deﬁnition 4.4 of [31]), which encompasses the deﬁnition of symmetry
given above and generalizes it to transformations among families of distributions
and not just within one family.

Information Geometry and Quantum Fields
333
the Gaussian, in which the mean and the standard deviation are simultaneously
multiplied by the same positive real constant: (σ, µ) →(˜σ, ˜µ) = (λσ, λµ). Again,
this is undone by scaling the random variable by the same amount x →˜x = λx.
In this case, the Jacobian arising from the rescaling of x is precisely what one
needs in order to maintain the relation (2).
One can show that a transformation θ →˜θ, which is a symmetry of the
probability distribution, generates a corresponding isometry of the Fisher metric
(1). However, it is important to bear in mind that the converse is not true: just
because a metric has certain isometries does not mean that all statistical models
with that metric as their associated Fisher metric have to have the corresponding
symmetries.
In the case of the Gaussian family (3), the isometries associated with the
aforementioned symmetries are enough to force the Fisher metric to be the
(d + 1)-dimensional hyperbolic metric.3 Indeed, direct computation yields the
line element
ds2 = gij(θ) dθi dθ j = 2dσ2 + |dµ|2
σ2
.
(4)
Up to irrelevant constants, one derives the same geometry from the Cauchy
distribution parametrized by the half-width-at-half-max σ and the center µ.
There is nothing especially “holographic” about the Gaussian or Cauchy dis-
tributions. In fact, there are inﬁnitely many distributions that produce hyper-
bolic space as their corresponding information geometry [37] and most of these
do not even share the conformal symmetry of hyperbolic space. Therefore, one
needs separate and independent reasons to justify associating the appearance of
hyperbolic space with holography.
3
Moduli Space of Instantons
An example of work in which the appearance of hyperbolic space is accompanied
by additional partial evidence supporting the association with holography is
the calculation of the Fisher metric on the space of Yang-Mills instantons [28].
Independent earlier work already showed that these instantons are good probes
of the bulk geometry in well-known cases [32].
We can show how this computation works in the much simpler example of
a massless scalar theory in four dimensions with an inverted quartic interaction
whose action in Euclidean signature is given by
S =
∫
d4x |∇φ|2 −g2φ4.
(5)
The Euler-Lagrange equations of motion are solved by the family of isotropic
four-dimensional Cauchy distributions given by
φinst(x; σ, µ) = 2
gσ

σ2
|x −µ|2 + σ2

.
(6)
3 We will work in Euclidean signature here, but a-posteriori Wick rotation applies
these same arguments to AdS spacetime.

334
K. T. Grosvenor
This is called an instanton solution because it gives a positive ﬁnite value for
the action (5). The action can be normalized on this instanton by picking the
value of g appropriately, namely g2 = 8π2/3. Hitchin’s prescription [38] would
then be to take the Lagrangian density as the probability distribution on the
space of these instantons and use this to calculate the Fisher metric, which is
the method followed in [28]. However, we encounter one problem here that did
not appear in [28]: the scalar action is unstable as it has an inverted potential,
which is unbounded from below. In fact, the Lagrangian density evaluated on
the instanton solution is
|∇φinst|2 −g2φ4
inst = 16σ2
g2
 |x −µ|2 −σ2
(|x −µ|2 + σ2)4

,
(7)
which is negative in the region |x−μ| < σ and therefore cannot be interpreted as a
probability distribution. We can remedy this by subtracting the total derivative
term
1
2∇2(φ2) = |∇φ|2 + φ ∇2φ from the Lagrangian density, where ∇2 is the
Laplacian, so that the action now reads
S = −
∫
d4x φ∇2φ + g2φ4.
(8)
This does not change the equation of motion or the solution (6), but it does
change the Lagrangian density, which when evaluated on the solution now gives4
−φinst∇2φinst −g2φ4
inst =
16
g2σ4

σ2
|x −µ|2 + σ2
4
,
(9)
which is non-negative and normalized by the choice g2 = 8π2/3. Therefore, this
can work as a probability distribution and, since it enjoys the symmetries of
translation in x and dilatations in σ and x, the Fisher metric is indeed 5d
hyperbolic space.
The lesson here is that the Fisher metric depends crucially on the choice
of probability distribution and we must be careful not to attribute to it more
than is justiﬁed by that possibly contrived choice. It is all too easy to force the
Fisher metric to have some set of desirable traits. As we stated in the start of
this section, the reason why we might attribute any connection to holography to
the hyperbolic Fisher metric on the Yang-Mills instantons in [28] is that earlier
work [32] had already established the Yang-Mills instantons as good probes of
the bulk geometry in well-studied cases of holography.
4
The 2D Ising Model
In the previous example, we computed a Fisher metric on the space of states of
a given theory, namely the instantons of a scalar ﬁeld theory. Here, we will show
4 Note that while (7) is correct in any dimension, (9) is the correct expression only
in dimension d = 4. Of course, this is ﬁne because (6) is the solution to the Euler-
Lagrange equations of motion only in dimension d = 4.

Information Geometry and Quantum Fields
335
how to construct a Fisher metric on a space of theories parametrized by a set
of coupling constants. The theory we will study is the 2d classical Ising model
on an N × N square lattice with periodic boundary conditions (i.e., on a torus),
with the Hamiltonian
H = −
N

i,j=1
σi,j
Jσi+1,j + Kσi,j+1
,
(10)
where σi,j = ±1 and J and K are the horizontal and vertical coupling constants
between nearest neighbor pairs. The probability distribution on the space of
theories is parametrized by the couplings J and K and its random variables
are the states of the spins, which we collectively denote by σ. Consider the
thermodynamic Boltzmann distribution
p(σ; J, K) = e−H
Z ,
(11)
where Z is the thermodynamic partition function, and where we have set the
inverse temperature to unity.5 Since this is in the exponential family of distri-
butions, the Fisher metric is simply the Hessian of the reduced free energy per
site,
gij = 1
N ∂i∂j ln Z,
i, j = J, K.
(12)
The explicit expression for gij is complicated and not particularly illuminating,
but we can get a sense of it by numerically evaluating the Ricci scalar, which,
since we are in two dimensions, completely describes the curvature of this space.
The result is plotted in Fig. 1. The curvature diverges on either side of the critical
line separating the disordered phase, around weak coupling, and the ferromag-
netic phase, around strong coupling. This line is described by the relation
sinh(2J) sinh(2K) = 1.
(13)
The phenomenon of the divergence of the curvature of the Fisher metric along
phase transition lines is well documented and has been used in the past to extract
various critical exponents, e.g. in [39–42]. Intuitively, this is not surprising given
that the thermodynamic Fisher metric is the Hessian of the free energy and the
curvature is essentially the second derivative of the metric. Therefore, the Ricci
scalar is intuitively the derivative of a susceptibility, which we expect to diverge
at criticality. However, it turns out that there are many notions of curvature in
the ﬁeld of information geometry and we will see that some of these seem to
carry no physical content. We turn now to the topic of curvature.
5 In the isotropic case, one sets J = K = 1 and leaves the temperature as the remaining
variable. The Fisher metric for the isotropic case with an external magnetic ﬁeld was
studied in [39].

336
K. T. Grosvenor
Fig. 1. The Ricci scalar for the 2d Ising model as function of couplings J, K showing the
divergence along the transition line between the disordered phase, around weak cou-
pling, and the ferromagnetic phase, around strong coupling. (This plot was generated
by Ro Jeﬀerson for the article [1].)
5
The Curvature Menagerie
A geometric manifold is equipped not only with a metric gij, but also with
an aﬃne connection, which tells us how to transport vectors from the tangent
space at one point on the manifold to that on another point. We can describe the
connection by the Christoﬀel symbols Γkij. In the case of information geometry,
Γkij ought to be a generalized ﬁrst derivative of gij, which, as in (1), already
contains two derivatives on factors of
ℓ≡ln p.
(14)
Only derivatives of ℓwith respect to θ and not ℓitself should enter into geometric
quantities, as it does in the Fisher metric (1). Derivatives on ℓwill be denoted
by subscript indices:
ℓi···j = ∂i · · · ∂jℓ.
(15)

Information Geometry and Quantum Fields
337
In addition, as in the Fisher metric, the connection must be an expectation value
of derivatives of ℓwith respect to p, which we denote by angled brackets:
⟨f ⟩=
∫
dμ(x) p(x; θ) f .
(16)
In this notation, the Fisher metric in (1) is written as gij = ⟨ℓiℓj⟩, which can
equally well be written as gij = −⟨ℓij⟩.
Finally, we impose that the information geometric manifold be torsion-free,
meaning that Γkij is symmetric in the last two indices. It follows that Γkij must
be a linear combination of the four terms
akij = ⟨ℓkℓij⟩,
bkij = ⟨ℓkℓiℓj⟩,
ckij = ⟨ℓkij⟩,
dkij = 2⟨ℓ(iℓj)k⟩.
(17)
Under a coordinate transformation θ →θ′, the Christoﬀel symbol must trans-
form as
Γ′
cab = ∂θk
∂θ′c
∂θi
∂θ′a
∂θ j
∂θ′b Γkij + ∂θk
∂θ′c
∂2θi
∂θ′a ∂θ′b gik.
(18)
If the Christoﬀel symbols were to form a bona-ﬁde 3-tensor, then only the ﬁrst
term on the right hand side of the above equation would have shown up. One can
check that akij transforms in the same way as Γkij and therefore must enter into
Γkij with unit coeﬃcient. On the other hand, bkij transforms as a bona-ﬁde 3-
tensor and can therefore enter into Γkij with an arbitrary coeﬃcient. Meanwhile,
ckij and dkij can enter into Γkij only through the linear combination ckij + dkij,
which transforms like Γkij. However, one can also easily show that
akij + bkij + ckij + dkij = 0,
(19)
which can be used to eliminate ckij + dkij. Thus, the most general connection is
the so-called α-connection [27]:
Γkij = ⟨ℓkℓij⟩+ 1 −α
2
⟨ℓkℓiℓj⟩.
(20)
The value α = 0 corresponds to the standard Levi-Civita connection and is the
only connection above which preserves the Fisher metric (i.e., is Riemannian).
The α = 1 connection is closely associated with the exponential family of prob-
ability distributions, which include thermodynamic distributions such as (11),
which we used for the 2d Ising model. These models are 1-ﬂat, meaning that they
are ﬂat with respect to the 1-connection. Therefore, the α = 1 measure of curva-
ture would carry no information about the critical behavior of a thermodynamic
system if the Boltzmann distribution is used to deﬁne probabilities.
6
Discussion
In this note, we have discussed the construction of the information geometry on a
space of states (the instanton moduli space of the massless scalar φ4 theory) and

338
K. T. Grosvenor
on a space of theories (the 2d classical Ising model). We demonstrated how the
underlying symmetries of the physical system manifest themselves as isometries
of the Fisher metric. Therefore, in trying to apply techniques of information
geometry to holography, one should be careful to disentangle the properties of
the Fisher metric that are determined solely by conformal symmetry from the
properties that may have more to say about the duality itself. Furthermore, the
example of the φ4 theory shows how the issue of the stability of the quantum
ﬁeld theory may be reﬂected in the computation of the Fisher metric.
Bearing this in mind, it is certainly very interesting to try to use informa-
tion geometry to connect the information content of the ﬁeld theory to the bulk
dual geometry within holography. For example, does this perspective help in
extending the duality to a broader context away from conformal invariance and
maximal symmetry? Can this teach us anything about holography as renormal-
ization? The link to renormalization is a natural one given the intuitive picture of
renormalization as a coarse-graining of degrees of freedom from high to low ener-
gies, a process by which, in principle, we lose information about the system (see
[43–45] for examples of works that study renormalization from the perspective
of information geometry).
We also derived a one-parameter family of connections on information geom-
etry space. In the thermodynamic case, the unique Riemannian connection gives
us a measure of curvature which diverges along phase transition lines, when the
physical system becomes critical. However, at least the α = 1 connection and,
due to an α →−α duality [27], the α = −1 connection have vanishing curvature
and are thus entirely insensitive to this critical behavior. What can we gain from
these other measures of curvature? There has already been some work in this
direction. For example, Dolan and Lewis compared the renormalization group
ﬂow lines in the space of coupling constants of a quantum ﬁeld theory with the
auto-parallel ﬂow lines in the information geometry on that space with respect
to these diﬀerent connections [47].6 No straightforward one-to-one relationship
between the two was found. Nevertheless, these connections beg further study
and it would be interesting to map out their physical content.
Acknowledgments. The author is supported by the W¨urzburg-Dresden Cluster of
Excellence on Complexity and Topology in Quantum Matter – ct.qmat (EXC 2147,
Project-id No. 39085490) through the Hallwachs-R¨ontgen fellowship program.
References
1. Erdmenger, J., Grosvenor, K.T., Jeﬀerson, R.: Information geometry in quantum
ﬁeld theory: lessons from simple examples. SciPost Phys. 8(5), 073 (2020). https://
doi.org/10.21468/SciPostPhys.8.5.073. [arXiv:2001.02683 [hep-th]]
2. Ryu, S., Takayanagi, T.: Holographic derivation of entanglement entropy from
AdS/CFT. Phys. Rev. Lett. 96 (2006). https://doi.org/10.1103/PhysRevLett.96.
181602. [arXiv:hep-th/0603001 [hep-th]]
6 Auto-parallel ﬂow lines correspond to geodesics only for the Levi-Civita connection.

Information Geometry and Quantum Fields
339
3. Hubeny, V.E., Rangamani, M., Takayanagi, T.: A Covariant holographic entangle-
ment entropy proposal. JHEP 07, 062 (2007). https://doi.org/10.1088/1126-6708/
2007/07/062. [arXiv:0705.0016 [hep-th]]
4. Engelhardt, N., Wall, A.C.: Quantum extremal surfaces: holographic entanglement
entropy beyond the classical regime. JHEP 01, 073 (2015). https://doi.org/10.
1007/JHEP01(2015)073. [arXiv:1408.3203 [hep-th]]
5. Almheiri, A., Dong, X., Harlow, D.: Bulk locality and quantum error correction
in AdS/CFT. JHEP 04, 163 (2015). https://doi.org/10.1007/JHEP04(2015)163.
[arXiv:1411.7041 [hep-th]]
6. Pastawski, F., Yoshida, B., Harlow, D., Preskill, J.: Holographic quantum error-
correcting codes: toy models for the bulk/boundary correspondence. JHEP 06, 149
(2015). https://doi.org/10.1007/JHEP06(2015)149. [arXiv:1503.06237 [hep-th]]
7. Harlow, D.: The Ryu-Takayanagi formula from quantum error correction. Com-
mun. Math. Phys. 354(3), 865–912 (2017). https://doi.org/10.1007/s00220-017-
2904-z. [arXiv:1607.03901 [hep-th]]
8. Harlow, D.: TASI lectures on the emergence of bulk physics in AdS/CFT. PoS
TASI2017, 002 (2018). https://doi.org/10.22323/1.305.0002. [arXiv:1802.01040
[hep-th]]
9. Swingle, B.: Entanglement renormalization and holography. Phys. Rev. D
86, (2012). https://doi.org/10.1103/PhysRevD.86.065007. [arXiv:0905.1317 [cond-
mat.str-el]]
10. Bao, N., et al.: Consistency conditions for an AdS multiscale entanglement renor-
malization ansatz correspondence. Phys. Rev. D 91(12), 125036 (2015). https://
doi.org/10.1103/PhysRevD.91.125036. [arXiv:1504.06632 [hep-th]]
11. Hayden, P., Nezami, S., Qi, X.L., Thomas, N., Walter, M., Yang, Z.: Holographic
duality from random tensor networks. JHEP 11, 009 (2016). https://doi.org/10.
1007/JHEP11(2016)009. [arXiv:1601.01694 [hep-th]]
12. Miyaji, M., Numasawa, T., Shiba, N., Takayanagi, T., Watanabe, K.: Dis-
tance
between
Quantum
States
and
Gauge-Gravity
Duality.
Phys.
Rev.
Lett. 115(26), 261602 (2015). https://doi.org/10.1103/PhysRevLett.115.261602.
[arXiv:1507.07555 [hep-th]]
13. Banerjee, S., Erdmenger, J., Sarkar, D.: Connecting ﬁsher information to bulk
entanglement in holography. JHEP 08, 001 (2018). https://doi.org/10.1007/
JHEP08(2018)001. [arXiv:1701.02319 [hep-th]]
14. Susskind, L.: Computational complexity and black hole horizons. Fortsch. Phys.
64, 24–43 (2016). https://doi.org/10.1002/prop.201500092. [arXiv:1403.5695 [hep-
th]]
15. Brown, A.R., Roberts, D.A., Susskind, L., Swingle, B., Zhao, Y.: Holographic com-
plexity equals bulk action? Phys. Rev. Lett. 116(19), 191301 (2016). https://doi.
org/10.1103/PhysRevLett.116.191301. [arXiv:1509.07876 [hep-th]]
16. Chapman,
S.,
Marrochio,
H.,
Myers,
R.C.:
Complexity
of
formation
in
holography. JHEP 01, 062 (2017). https://doi.org/10.1007/JHEP01(2017)062.
[arXiv:1610.08063 [hep-th]]
17. Fisher, R.A.: Theory of statistical estimation. Math. Proc. Cambridge Philos. Soc.
22(5), 700 (1925). https://doi.org/10.1017/S0305004100009580
18. Souriau, J.M.: Thermodynamique Relativiste des Fluides, Volume 35. Rendiconti
del Seminario Matematico, Universit`a Politecnico di Torino; Torino, Italy, pp. 21-
34 (1978)
19. Ruppeiner, G.: Riemannian geometry in thermodynamic ﬂuctuation theory. Rev.
Mod. Phys. 67, 605–659 (1995). https://doi.org/10.1103/RevModPhys.67.605.
[erratum: Rev. Mod. Phys. 68 (1996), 313-313]

340
K. T. Grosvenor
20. Barbaresco, F.: Geometric theory of heat from Souriau Lie groups thermodynamics
and Koszul Hessian geometry: applications in information geometry for exponential
families. Entropy 18(11), 386 (2016)
21. Souriau, J.M.: Milieux continus de dimension 1, 2 ou 3: Statique et dynamique.
In: Proceedings of the 13eme Congr`es Fran¸ais de M´ecanique; Poitiers, Francy, 1–5
September 1997, pp. 41–53 (1997)
22. Janke, W., Johnston, D.A., Kenna, R.: The information geometry of the spheri-
cal model. Phys. Rev. E 67, (2003). https://doi.org/10.1103/PhysRevE.67.046106.
[arXiv:cond-mat/0210571 [cond-mat]]
23. Dolan, B.P., Johnston, D.A., Kenna, R.: The information geometry of the one-
dimensional Potts model. J. Phys. A 35, 9025–9036 (2002). https://doi.org/10.
1088/0305-4470/35/43/303. [arXiv:cond-mat/0207180 [cond-mat]]
24. Heckman, J.J.: Statistical inference and string theory. Int. J. Mod. Phys. A 30(26),
1550160 (2015). https://doi.org/10.1142/S0217751X15501602. [arXiv:1305.3621
[hep-th]]
25. Amari, S.-I., Kurata, K., Nagaoka, H.: Information geometry of Boltzmann
machines. IEEE Trans. Neural Netw. 3(2), 260 (1992)
26. Amari, S.-I.: Information geometry of neural networks – an overview. In: Ellacott,
S.W., Mason, J.C., Anderson, I.J. (eds.) Mathematics of Neural Networks. Oper-
ations Research/Computer Science Interfaces Series, vol. 8, pp. 15–23. Springer,
Boston (1997). https://doi.org/10.1007/978-1-4615-6099-9 2
27. Amari, S.-I., Nagaoka, H.: Methods of Information Geometry, vol. 191, 2nd edn.
Americal Mathematical Society, Oxford (2007)
28. Blau, M., Narain, K.S., Thompson, G.: Instantons, the information metric, and
the AdS/CFT correspondence. [arXiv:hep-th/0108122 [hep-th]]
29. Shahshahani, S.: A New Mathematical Framework for the Study of Linkage and
Selection. American Mathematical Society, Providence (1979)
30. Ay, N., Jost, J., Vˆan Lˆe, H., Schwachh¨ofer, L.: Information Geometry. Springer,
Heidelberg (2017)
31. Ay, N., Jost, J., Vˆan Lˆe, H., Schwachh¨ofer, L.: Information geometry and
suﬃcient statistics. Probab. Theory Relat. Fields 162(1–2), 327–364 (2015).
[arXiv:1207.6736 [math.ST]]
32. Dorey, N., Khoze, V.V., Mattis, M.P., Vandoren, S.: Yang-Mills instantons in the
large N limit and the AdS/CFT correspondence. Phys. Lett. B 442, 145–151
(1998).
https://doi.org/10.1016/S0370-2693(98)01233-7.
[arXiv:hep-th/9808157
[hep-th]]
33. Suzuki, Y., Takayanagi, T., Umemoto, K.: Entanglement wedges from the informa-
tion metric in conformal ﬁeld theories. Phys. Rev. Lett. 123(22), 221601 (2019).
https://doi.org/10.1103/PhysRevLett.123.221601. [arXiv:1908.09939 [hep-th]]
34. Kusuki, Y., Suzuki, Y., Takayanagi, T., Umemoto, K.: Looking at shadows of
entanglement wedges. [arXiv:1912.08423 [hep-th]]
35. Matsueda, H.: Geometry and dynamics of emergent spacetime from entanglement
spectrum. [arXiv:1408.5589 [hep-th]]
36. Matsueda, H.: Geodesic distance in ﬁsher information space and holographic
entropy formula. [arXiv:1408.6633 [hep-th]]
37. Clingman, T., Murugan, J., Shock, J.P.: Probability density functions from the
ﬁsher information metric. [arXiv:1504.03184 [cs.IT]]
38. Hitchin, N.J.: The geometry and topology of moduli spaces. In: Francaviglia, M.,
Gherardelli, F. (eds.) Global Geometry and Mathematical Physics. Lecture Notes
in Mathematics, vol. 1451, pp. 1–48. Springer, Heidelberg (1990). https://doi.org/
10.1007/BFb0085064

Information Geometry and Quantum Fields
341
39. Brody, D.C., Ritz, A.: Geometric phase transitions. [arXiv:cond-mat/9903168]
40. Janke, W., Johnston, D.A., Kenna, R.: Information geometry and phase tran-
sitions. Phys. A 336, 181 (2004). https://doi.org/10.1016/j.physa.2004.01.023.
[arXiv:cond-mat/0401092 [cond-mat]]
41. Carollo, A., Valenti, D., Spagnolo, B.: Geometry of quantum phase transi-
tions. Phys. Rept. 838, 1–72 (2020). https://doi.org/10.1016/j.physrep.2019.11.
002. [arXiv:1911.10196 [quant-ph]]
42. Mera, B.: Information geometry in the analysis of phase transitions. Spin 1, 2
(2019)
43. Kar, S.: The geometry of RG ﬂows in theory space. Phys. Rev. D 64 (2001).
https://doi.org/10.1103/PhysRevD.64.105017. [arXiv:hep-th/0103025 [hep-th]]
44. B´eny, C., Osborne, T.J.: Information-geometric approach to the renormalization
group. Phys. Rev. A 92(2), 022330 (2015). https://doi.org/10.1103/PhysRevA.92.
022330. [arXiv:1206.7004 [quant-ph]]
45. Balasubramanian, V., Heckman, J.J., Maloney, A.: Relative Entropy and Proxim-
ity of Quantum Field Theories. JHEP 05, 104 (2015). https://doi.org/10.1007/
JHEP05(2015)104. [[arXiv:1410.6809 [hep-th]]
46. Jeﬀerson, R., Myers, R.C.: Circuit complexity in quantum ﬁeld theory. JHEP 10,
107 (2017). https://doi.org/10.1007/JHEP10(2017)107. [arXiv:1707.08570 [hep-
th]]
47. Dolan, B.P., Lewis, A.: Renormalization group ﬂow and parallel transport with
nonmetric compatible connections. Phys. Lett. B 460, 302–306 (1999). https://
doi.org/10.1016/S0370-2693(99)00792-3. [arXiv:hep-th/9904119 [hep-th]]

Part V: Hamiltonian Monte Carlo, HMC
Sampling and Learning on Manifolds

Geometric Integration of
Measure-Preserving Flows for Sampling
Alessandro Barp(B)
The Alan Turing Instituted and Department of Engineering,
The University of Cambridge, Cambridge CB3 0FA, UK
ab2286@cam.ac.uk
Abstract. Many of the state-of-the-art Monte Carlo sampling algo-
rithms are inspired by Hamiltonian Monte Carlo and constructed from
measure-preserving Langevin diﬀusions through an appropriate geomet-
ric integrator. In this article, following [9] we discuss the general con-
struction of such Hamiltonian-based samplers starting from the canon-
ical recipe of measure-preserving diﬀusions. Moreover, we discuss the
properties that make Hamiltonian mechanics particularly appropriate to
devise eﬃcient Monte Carlo methods.
1
Introduction
Eﬃcient algorithms to approximate measures are fundamental to many applica-
tions in physics and statistics, in particular in view of estimating the expected
value of functions of observables. Speciﬁcally, given a target measure P on a
smooth manifold M, we are interested in algorithms that construct discrete
measures Pℓ≡
1
ℓ
ℓ
i=1 δxi s.t., P ≈Pℓin the sense that for an appropriate
observable f : M →R the expectations with respect to Pℓprovide an accurate
estimate of the expected value of f,
E[f] ≡

fP ≈

fPℓ= 1
ℓ
ℓ

i=1
f(xℓ),
and

fPℓconverges rapidly to E[f] as ℓ→∞.
For example in decision theory one is interested in evaluating the “risk”
R associated to a decision d, which is the expected value of a loss function
R ≡E

L

·, d(·)
	
. In molecular systems consisting of a large number of par-
ticles, M is the phase space whose elements (q, p) ∈M represent microstates,
and the observables are functions of the microstates, such as the speciﬁc heat or
bulk pressure [19]. The macroscopic properties are characterised by the expecta-
tion (and variance) of the observables with respect to the canonical distribution
1
Z e−βHdqdp, where H is the energy, β > 0, and Z =

e−βHdqdp is the canonical
partition function which also requires integrating over phase space. In the con-
text of computational statistical physics expectations also arise in the problem of
c
⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2021
F. Barbaresco and F. Nielsen (Eds.): SPIGL 2020, PROMS 361, pp. 345–355, 2021.
https://doi.org/10.1007/978-3-030-77957-3_18

346
A. Barp
thermodynamic integration and free energy calculations [63]. In Bayesian statis-
tics the target measure is the posterior distribution on a space M of parameters
generated by some observation of the data process.
A wide range of methods have been proposed to approximate measures,
including kernel, transport, and Stein operator based algorithms [8,20,21,37,
47,51,58], and Monte Carlo samplers [50,55,61,64]. For smooth target mea-
sures, many of the state-of-the-art methods are inspired by the Hybrid Monte
Carlo (HMC) algorithms [24,35,38] which combine a geometrically integrated
Hamiltonian ﬂow, that preserves the volume and a shadow probability den-
sity close to the target density, with a tractable “thermostat” diﬀusion process
that improves the ergodicity. Amongst these we note methods constructed from
the overdamped Langevin diﬀusion [2,27,62,66], the underdamped Langevin
process [1,23,41,42,45,56,57], shadow Hamiltonians [36,39,59], the informa-
tion geometric metric [29,48]. See also for other variants and review articles
[3,6,7,11,13,15,30,33,34,55].
In this article we discuss the general construction of Markov chains obtained
by a HMC-like geometric integration of measure-preserving diﬀusions that main-
tains some of the core geometric guarantees of HMC. The theory holds on arbi-
trary manifolds under the single assumption that the target measure is a smooth
globally supported Radon measure P. This is an important feature as many
applications require sampling from distributions that are supported on non-
trivial manifolds, such as the Fisher–Bingham distributions on Stiefel manifolds
for directional statistics/principal component analysis, canonical distributions in
molecular dynamics or degenerate posterior distributions in statistics on holo-
nomic manifolds, distributions on covariances and Hermitian positive matrices
for learning spectral density matrix, or discrete actions on gauge groups. Just as
importantly, having a geometric theory of HMC-like samplers enables us to eas-
ily incorporate crucial geometric guarantees, notably symmetries and conserved
quantities, and better understand the connection between statistical methods
and mathematical physics.
Throughout P denotes a globally supported smooth Radon measure, known
up to scaling, on a manifold M of dimensions N. We denote by Ωk(M), Xk(M),
and Ωk
Or(M) the sections of the bundle of k-forms, k-multivectors, and k-twisted
forms respectively. Hence we can view the target measure as an element of
ΩN
Or(M). We denote by d both the exterior derivative and its extension to twisted
forms, and by Hk
Or(M) the associated twisted de Rham kth cohomology group.
2
Monte Carlo Methods from Measure-Preserving
Diﬀusions
The archetypal Hamiltonian Monte Carlo algorithm considers the measure-
preserving underdamped Langevin process [63]
dQt = M −1Ptdt,
dPt = −∇V (Qt)dt



Hamiltonian Dynamics
−γ(Qt)M −1Ptdt + σ(Qt)dWt



Ornstein–Uhlenbeck heat bath
(1)

Geometric Integration of Measure-Preserving Flows for Sampling
347
and approximates it by splitting it into two P-preserving components: the deter-
ministic Hamiltonian ﬂow and the Ornstein–Uhlenbeck stochastic process. The
latter is tractable, while the deterministic Hamiltonian ﬂow is implemented using
an appropriate geometric integrator Ψδt that confers it important guarantees,
such as volume-preservation, the existence of symmetries, or of a shadow Hamil-
tonian [10,32,43,53,54]. For a discussion on the tuning and choice of the inte-
grator in HMC we refer to [12,14,17,18,22,39,60].
In order to construct more general samplers we will need a theory of measure-
preserving diﬀusions. In Euclidean space M = RN, it was recently shown that
any P ∝e−V dx diﬀusion satisfying some L1(dx)-integrability and uniqueness
assumptions, has the form [49, Thm. 2]
dZt = −(Q∇V + D∇V ) dt + ∇·

Q + D

dt +
√
2D dWt ,
(2)
for some antisymmetric Q and positive semi-deﬁnite D matrix ﬁelds. We will
discuss the generalisation of this result to manifolds in the next section, but
before doing so we derive a general intrinsic MCMC algorithm that can be
used to correct for the bias introduced by the approximate integration of the
deterministic ﬂow.
The amount in which a general diﬀeomorphism R : M →M warps a the
measure P is characterized by the Jacobian function,
R−1
∗P = J (P, R) · P,
where R−1
∗
≡(R−1)∗is the pushforward of R−1, or
J (P, R) ≡dR−1
∗P
dP
.
Note the Jacobian only depends on P up to multiplicative scaling. The integrator
Ψδt will be said to be R-reversibile when
Ψ−1
δt = R ◦Ψδt ◦R−1
or equivalently
R = Ψδt ◦R ◦Ψδt.
When R is involutive, then the integrator Ψδt is R-reversible iﬀthe composition
R ◦Ψδt is involutive. We have the following generalisation of the compressible
HMC algorithm [28], proved in [5].
Theorem 1 (Intrinsic
MCMC).
Suppose
R
is
a
P-preserving
C1-
diﬀeomorphism, that its composition with the integrator R ◦Ψδt is a C1-
diﬀeomorphism, and that Ψδt is R-reversible. The following algorithm generates
a P-preserving Markov chain. Given qℓ∈M:
1. q∗←Ψδt(qℓ)
2. set qℓ+1 ←q∗with probability min

1, J (P, Ψδt)(qℓ)

, else qℓ+1 ←R(qℓ),
When P ≡e−HμM, for some reference measure μM, we can also write
J (P, Ψδt) = e−H◦Ψδt+HJ (μM, Ψδt).

348
A. Barp
3
A Complete and Canonical Characterisation
of Measure-Preserving Diﬀusion
In order to construct a canonical and complete recipe of P-preserving diﬀusions,
we need to introduce some operators describing the intrinsic geometry of P.
First, following the Poisson geometry literature [26,65], we note that P deﬁnes P-
musical isomorphisms between twisted diﬀerential forms and multi-vector ﬁelds
P ♭: Xk(M) →ΩN−k
Or
(M),
via the interior product of multi-vector ﬁelds [52], P ♭(W ) ≡iW P, with the
inverse isomorphism denoted P ♯. In particular P ♯restricted to smooth Radon
measures coincides with the Radon–Nikodym derivative. If
dα ≡d −α∧,
denotes the Lichnerowicz–deRham diﬀerential of a ﬁxed closed 1-form α [31], we
can construct the rotationnel [25,40]
curlP,α ≡P ♯◦dα ◦P ♭: Xk(M) →Xk−1(M),
which is a boundary operator on multi-vector ﬁelds. When α = 0, we call curlP ≡
curlP,0 the P-rotationnel, which is a diﬀerential operator canonically associated
to P. It was shown in [4,9] that any P-preserving diﬀusion on a manifold has
the form
dZt = curlP (A)dt + P ♯(ζ)dt + 1
2curlP (Yi)Yidt + Yi ◦dW i
t ,
(3)
where dWt is a standard Brownian motion on RN, (Yi)i are “noise” vector ﬁelds,
A is a 2-vector ﬁeld, and ζ a representative element of the (N −1)-deRham
twisted cohomology group. In Euclidean space (3) recovers (2), thus showing
the assumptions of [49, Thm. 2] were a consequence of the speciﬁc derivation
used therein. Speciﬁcally, to obtain (2) we begin by expressing the target in
terms of a reference measure P = e−HμM, and then use the fact that
curlP = curlμM,dH = curlμM −idH,
to split curlP (A) and curlP (Yi) in (3) into terms corresponding to the target
density and the reference measure, and ﬁnally use the fact that HN−1
Or
(RN) =
{0}, so that the term P ♯(ζ), which represents topological obstructions, vanishes.
Having a characterisation as in (3) reduces the problem of constructing sam-
plers to appropriately choosing the parameters {A, μM, (Yi)i, ζ}. For simplicity
we can set ζ = 0. Next, we follow the strategy of (1) and split (3) into its
deterministic
dZD
t
dt
= curlP (A),
and noise component
dZN
t
= 1
2curlP (Yi)Yidt + Yi ◦dW i
t .

Geometric Integration of Measure-Preserving Flows for Sampling
349
In order to implement the latter, we can use the fact that the process dZN
t
is
reversible, in the sense that its generator is symmetric in L2(P) [9]. This allows
us to build explicit integrators, as shown for example in [16]. We note that on
a general manifold M it is convenient to begin by lifting P to a measure on
a vector bundle F →M over M, and construct a measure-preserving diﬀusion
on F so that we can set the noise vector ﬁelds to be vertical, which ensure the
process dZN
t
evolves within the ﬁbres, i.e., vector spaces, and thus considerably
simpliﬁes the implementation.
Hence we can focus on the choices of 2-vector ﬁeld A and reference measure
μM. This is the content of the next section.
4
A Complete Characterisation of Measure-Preserving
Mechanics
We now discuss the integration of the P-preserving vector ﬁeld curlP (A). We
deﬁne the A-Hamiltonian vector ﬁeld of a “Hamiltonian” H ∈C∞(M) by
XA
H ≡−idHA.
In order to implement the ﬂow of curlP (A) we will need to ensure it has suﬃcient
symmetries and conserved quantities. Unfortunately, the target measure P is
typically complex which obstructs the ability to construct ﬂows which are both
P-preserving and computationally tractable. In order to proceed, we will rely on
splitting integrators.
Most manifolds have a canonical reference measure which is invariant under
a Lie group of “nice” symmetries. We let μM be this invariant measure and split
our vector ﬁeld as
curlP (A) = curlμM(A) + XA
H.
Choosing 2-vector ﬁelds for which curlμM(A) is tractable is considerably simpler
as a result of the symmetries of μM, for example curlμM(A) will inherit any sym-
metry shared by A and μM. For our purposes it will be convenient to construct
As s.t., curlμM(A) = 0, due to the following theorem, proved in [4,5].
Theorem 2. Let XμM
A
: C∞(M) →C∞(M) be the diﬀerential operator
XμM
A
(f) ≡divμM(XA
f ).
Then
XμM
A
= curlμM(A).
When A is a Poisson structure, XμM
A
corresponds to the modular vector ﬁeld,
whose class in the ﬁrst Poisson cohomology characterises the existence of an

350
A. Barp
invariant measure for the Poisson mechanics [25,65]. For arbitrary A ∈X2(M),
the above theorem shows that μM will be invariant under the ﬂows of the space
Mech(A) ≡{XA
f : f ∈C∞(M)}
of all A-Hamiltonian vector ﬁelds if and only if curlμM(A) = 0.
Hence, curlμM(A) = 0 not only means the vector ﬁeld curlP (A) simpliﬁes to
curlP (A) = XA
H,
but also implies that if we decompose the target density as a product e−H =

j e−Hj, so that curlP (A) becomes
curlP (A) =

j
XA
Hj,
then the splitting integrator obtained by composition of the ﬂows of XA
Hj will
be μM-preserving, since each vector ﬁeld XA
Hj preserves μM. We have thus
devised a general strategy to construct a volume-preserving splitting integra-
tor for curlP (A). We note the similarity with the standard implementation of
Hamiltonian Monte Carlo wherein the Hamiltonian ﬂow is approximated via the
leapfrog integrator that splits the target density into a potential and kinetic
term, and preserves the symplectic measure.
The above strategy is however only useful if we do have a general method
to construct 2-vector ﬁelds A for which μM is an invariant measure of the A-
mechanics Mech(A) in the sense described above. Fortunately, we have a simple
and canonical characterisation of such measure-preserving mechanics:
Theorem 3 ([5]). The 2-vector ﬁeld A is such that μM is an invariant measure
of all A-Hamiltonian vector ﬁelds if and only if
A = curlμM(V ) + (μM)♯(γ)
for some V ∈X3(M) and representative element γ of the (N−2)-deRham twisted
cohomology group.
Moreover, ignoring topological obstruction, the component of the splitting
integrator now have the form X
curlμM(V )
Hj
, and if we choose a V that shares the
symmetries of Hj and μM, then X
curlμM(V )
Hj
will inherit these symmetries.
5
Hamiltonian Monte Carlo
The above discussion provides a general promising mechanism to build measure-
preserving diﬀusions with desirable geometric guarantees that make them suit-
able to construct samplers. Nonetheless, Hamiltonian mechanics has many pow-
erful properties that explains why most of the statistics MCMC community has
focused on it and struggled to derive competing deterministic proposals.

Geometric Integration of Measure-Preserving Flows for Sampling
351
First, any manifold has a natural 2-vector ﬁeld A on its cotangent bundle
that has an invariant measure μT ∗M, namely the Poisson 2-vector ﬁeld of the
canonical symplectic structure, with μT ∗M the symplectic measure. In particular,
for practitioners this means that we do not need to spend time tuning A or the
reference measure.
Second, the target P is typically expressed in terms of a Riemannian measure
μM, and if we lift P to a measure on T ∗M using the Gaussian measures on the
ﬁbres, the resulting measure on T ∗M as a probability log-density with respect
to the symplectic measure which reduces to the standard mechanical system
(“potential + kinetic energies”)
H = V + T = V + 1
2∥· ∥2,
where P ∝e−V μM, and T is the half Riemannian norm squared. In particular
the ﬂow of H decomposes into the ﬂow of V , a tractable vertical gradient on
the ﬁbres, and the ﬂow of T, which is a geodesic ﬂow, and is tractable provided
the Riemannian metric has suﬃcient symmetries. We thus have a natural and
usually tractable splitting integrator: the leapfrog method.
Third, the leapfrog integrator can be turned into an involution using the
momentum ﬂip (which ﬂips the sign of the ﬁbre coordinate), which allows us to
correct for the bias introduced by the numerical approximation using a Metropo-
lis accept/reject step.
Fourth, the numerical integration of Hamiltonian ﬂows has been an important
topic in mathematical physics, and the many results derived in that context can
be immediately applied to Hamiltonian-based samplers.
Fifth, it is easy to construct symplectic integrators and these can be shown to
preserve a shadow Hamiltonian for exponentially long time which remains close
to the target log-density, thus ensuring that not only the integrator is volume
preserving, but also that the probability density is “almost” preserved during
the numerical integration.
Finally, for the two main classes of manifolds of interest in applications, the
level sets and the homogeneous manifolds, there are eﬃcient geometric integra-
tors, namely the RATTLE with reversibility check [46] and geodesic-RATTLE
integrator [41] for level sets, and the implementation via symplectic reduction for
geodesic orbit manifolds [7], as well as the Riemannian integrator for symmetric
spaces [44].
References
1. Arnaudon, A., Barp, A., Takao, S.: Irreversible Langevin MCMC on lie groups. In:
Nielsen, F., Barbaresco, F. (eds.) Geometric Science of Information. Lecture Notes
in Computer Science, vol. 11712, pp. 171–179. Springer, Cham (2019). https://doi.
org/10.1007/978-3-030-26980-7 18
2. Atchad´e, Y.F.: An adaptive version for the metropolis adjusted langevin algorithm
with a truncated drift. Methodol. Comput. Appl. Probab. 8(2), 235–254 (2006)

352
A. Barp
3. Barp, A.: Hamiltonian Monte Carlo on Lie groups and constrained mechanics on
homogeneous manifolds. arXiv preprint arXiv:1903.04662 (2019)
4. Barp, A.: The bracket geometry of statistics. Ph.D., thesis, Imperial College Lon-
don (2020)
5. Barp, A., et al.: Unravelling a geometric conspiracy: the intrinsic geometry of
measures and their invariant ﬂows as a foundation for scalable Markov Chain
Monte Carlo (2020, in Preparation)
6. Barp, A., Briol, F.-X., Kennedy, A.D., Girolami, M.: Geometry and dynamics for
Markov Chain Monte Carlo. Ann. Rev. Statist. Appl. 5, 451–471 (2018)
7. Barp, A., Kennedy, A., Girolami, M.: Hamiltonian Monte Carlo on symmetric and
homogeneous spaces via symplectic reduction. arXiv preprint arXiv:1903.02699
(2019)
8. Barp, A., Oates, C., Porcu, E., Girolami, M., et al.: A riemannian-stein kernel
method. arXiv preprint arXiv:1810.04946 (2018)
9. Barp, A., Takao, S., Betancourt, M., Arnaudon, A., Girolami, M.: A unifying
description of measure-preserving diﬀusions on manifolds using a novel bracket
geometry (2020, Submitted to NeurIPS)
10. Benettin, G., Giorgilli, A.: On the hamiltonian interpolation of near-to-the identity
symplectic mappings with application to symplectic integration algorithms. J. Stat.
Phys. 74(5–6), 1117–1143 (1994)
11. Betancourt, M.: A general metric for Riemannian manifold Hamiltonian Monte
Carlo. In: Nielsen, F., Barbaresco, F. (eds.) Geometric Science of Information.
Lecture Notes in Computer Science, vol. 8085, pp. 327–334. Springer, Heidelberg
(2013). https://doi.org/10.1007/978-3-642-40020-9 35
12. Betancourt, M.: Identifying the optimal integration time in Hamiltonian Monte
Carlo. ArXiv e-prints 1601.00225, January 2016
13. Betancourt, M.: A conceptual introduction to Hamiltonian Monte Carlo. arXiv
preprint arXiv:1701.02434 (2017)
14. Betancourt, M., Byrne, S., Girolami, M.: Optimizing the integrator step size for
Hamiltonian Monte Carlo. ArXiv e-prints, 1410.5110, November 2014
15. Betancourt, M., Byrne, S., Livingstone, S., Girolami, M., et al.: The geometric
foundations of Hamiltonian Monte Carlo. Bernoulli 23(4A), 2257–2298 (2017)
16. Bou-Rabee, N., Donev, A., Vanden-Eijnden, E.: Metropolis integration schemes
for self-adjoint diﬀusions. Multiscale Model. Simul. 12(2), 781–831 (2014)
17. Bou-Rabee, N., Sanz-Serna, J.M.: Geometric integrators and the Hamiltonian
Monte Carlo method. arXiv preprint arXiv:1711.05337 (2017)
18. Campos, C.M., Sanz-Serna, J.M.: Palindromic 3-stage splitting integrators, a
roadmap. J. Comput. Phys. 346, 340–355 (2017)
19. Cances, E., Legoll, F., Stoltz, G.: Theoretical and numerical comparison of some
sampling methods for molecular dynamics. ESAIM: Math. Model. Numer. Anal.
41(2), 351–389 (2007)
20. Chen, W.Y., et al.: Stein point Markov Chain Monte Carlo. arXiv preprint
arXiv:1905.03673 (2019)
21. Chen, W.Y., Mackey, L., Gorham, J., Briol, F.-X., Oates, C.J.: Stein points. arXiv
preprint arXiv:1803.10161 (2018)
22. Clark, M.A., Kennedy, A.D., Silva, P.J.: Tuning HMC using poisson brackets. arXiv
preprint arXiv:0810.1315 (2008)
23. Dobson, P., Fursov, I., Lord, G., Ottobre, M.: Reversible and non-reversible markov
chain monte carlo algorithms for reservoir simulation problems. arXiv preprint
arXiv:1903.06960 (2019)

Geometric Integration of Measure-Preserving Flows for Sampling
353
24. Duane, S., Kennedy, A.D., Pendleton, B.J., Roweth, D.: Hybrid Monte Carlo. Phys.
Lett. B 195(2), 216–222 (1987)
25. DufourDufour, J.-P., Haraki, A.: Rotationnnels et structures de poisson quadra-
tiques. Comptes rendus de l’Acad´emie des sciences. S´erie 1, Math´ematique 312(1),
137–140 (1991)
26. Dufour, J.-P., Zung, N.T.: Poisson Structures and Their Normal Forms, vol. 242.
Springer, Heidelberg (2006)
27. Durmus, A., Moulines, E., Pereyra, M.: Eﬃcient Bayesian computation by proximal
Markov chain Monte Carlo: when Langevin meets Moreau. SIAM J. Imag. Sci.
11(1), 473–506 (2018)
28. Fang, Y., Sanz-Serna, J.-M., Skeel, R.D.: Compressible generalized hybrid monte
carlo. J. Chem. Phys. 140(17), 174108 (2014)
29. Girolami, M., Calderhead, B.: Riemann manifold Langevin and Hamiltonian Monte
Carlo methods. J. R. Statist. Soc.: Ser. B (Statist. Methodol.) 73(2), 123–214
(2011)
30. Graham, M.M., Thiery, A.H., Beskos, A.: Manifold Markov Chain Monte Carlo
methods for bayesian inference in a wide class of diﬀusion models. arXiv preprint
arXiv:1912.02982 (2019)
31. Guedira, F., Lichnerowicz, A.: G´eom´etrie des alg´ebres de lie locales de kirillov. J.
de math´ematiques pures et appliqu´ees 63(4), 407–484 (1984)
32. Hairer, E., Lubich, C., Lubich, G.: Geometric Numerical Integration: Structure-
preserving Algorithms for Ordinary Diﬀerential Equations, vol. 31. Springer, Hei-
delberg (2006)
33. Holbrook, A., Lan, S., Vandenberg-Rodes, A., Shahbaba, B.: Geodesic Lagrangian
Monte Carlo over the space of positive deﬁnite matrices: with application to
Bayesian spectral density estimation. J. Statist. Comput. Simul. 88, 982–1002
(2017)
34. Holbrook, A., Vandenberg-Rodes, A., Shahbaba, B.: Bayesian inference on matrix
manifolds for linear dimensionality reduction. arXiv preprint arXiv:1606.04478
(2016)
35. Horowitz, A.M.: A generalized guided Monte Carlo algorithm. Phys. Lett. B
268(CERN–TH–6172–91), 247–252 (1991)
36. Izaguirre, J.A., Hampton, S.S.: Shadow hybrid Monte Carlo: an eﬃcient propagator
in phase space of macromolecules. J. Comput. Phys. 200(2), 581–604 (2004)
37. Joseph, V.R., Dasgupta, T., Tuo, R., Wu, C.F.J.: Sequential exploration of complex
surfaces using minimum energy designs. Technometrics 57(1), 64–74 (2015)
38. Kennedy, A.D., Pendleton, B.: Cost of the generalised hybrid monte carlo algorithm
for free ﬁeld theory. Nucl. Phys. B 607(3), 456–510 (2001)
39. Kennedy, A.D., Silva, P.J., Clark, M.A.: Shadow hamiltonians, poisson brackets,
and gauge theories. Phys. Rev. 87(3), 034511 (2013)
40. Koszul, J.-L.: Crochet de schouten-nijenhuis et cohomologie. Ast´erisque 137, 257–
271 (1985)
41. Leimkuhler, B., Matthews, C.: Eﬃcient molecular dynamics using geodesic integra-
tion and solvent-solute splitting. Proc. R. Soc. A: Math. Phys. Eng. Sci. 472(2189),
20160138 (2016)
42. Leimkuhler, B., Matthews, C., Stoltz, G.: The computation of averages from equi-
librium and nonequilibrium Langevin molecular dynamics. IMA J. Numer. Anal.
36(1), 13–79 (2015)
43. Leimkuhler, B., Reich, S.: Simulating Hamiltonian Dynamics, vol. 14. Cambridge
University Press, Cambridge (2004)

354
A. Barp
44. Leimkuhler, B.J., Reich, S., Skeel, R.D.: Integration methods for molecular dynam-
ics. In: Mesirov, J.P., Schulten, K., Sumners, D.W. (eds.) Mathematical Approaches
to Biomolecular Structure and Dynamics. The IMA Volumes in Mathematics and
its Applications, vol. 82. Springer, New York (1996). https://doi.org/10.1007/978-
1-4612-4066-2 10
45. Lelievre, T., Rousset, M., Stoltz, G.: Langevin dynamics with constraints and
computation of free energy diﬀerences. Math. Comput. 81(280), 2071–2125 (2012)
46. Leli`evre, T., Rousset, M., Stoltz, G.: Hybrid monte carlo methods for sampling
probability measures on submanifolds. Numer. Math. 143(2), 379–421 (2019)
47. Liu, Q., Wang, D.: Stein variational gradient descent: a general purpose bayesian
inference algorithm. Adv. Neural. Inf. Process. Syst. 29, 2378–2386 (2016)
48. Livingstone, S., Girolami, M.: Information-geometric Markov Chain Monte Carlo
methods using diﬀusions. Entropy 16(6), 3074–3102 (2014)
49. Ma, Y.-A., Chen, T., Fox, E.: A complete recipe for stochastic gradient MCMC.
In: Advances in Neural Information Processing Systems, pp. 2917–2925 (2015)
50. MacKay, D.J.C.: Information Theory Inference and Learning Algorithms. Cam-
bridge University Press, Cambridge (2003)
51. Mak, S., Joseph, V.R., et al.: Support points. Ann. Statist. 46(6A), 2562–2592
(2018)
52. Marle, C.-M.: The schouten-nijenhuis bracket and interior products. J. Geom.
Phys. 23(3), 350–359 (1997)
53. Marsden, J.E., West, M.: Discrete mechanics and variational integrators. Acta
Numer 10, 357–514 (2001)
54. McLachlan, R.I., Reinout, G., Quispel, W.: Splitting methods. Acta Numer 11,
341–434 (2002)
55. Neal, R.M., et al.: MCMC using hamiltonian dynamics. In: Handbook of Markov
Chain Monte Carlo, vol. 2, no. 11, p. 2 (2011)
56. Ottobre, M.: Markov Chain Monte Carlo and irreversibility. Rep. Math. Phys.
77(3), 267–292 (2016)
57. Ottobre, M., Pillai, N.S., Pinski, F.J., Stuart, A.M., et al.: A function space HMC
algorithm with second order Langevin diﬀusion limit. Bernoulli 22(1), 60–106
(2016)
58. Parno, M.D.: Transport maps for accelerated Bayesian computation. Ph.D., thesis,
Massachusetts Institute of Technology (2015)
59. Radivojevi´c, T., Akhmatskaya, E.: Modiﬁed Hamiltonian Monte Carlo for Bayesian
inference. Statist. Comput. 30, 377–404 (2019)
60. Radivojevi´c, T., Fern´andez-Pend´as, M., Sanz-Serna, J.M., Akhmatskaya, E.: Multi-
stage splitting integrators for sampling with modiﬁed Hamiltonian Monte Carlo
methods. J. Comput. Phys. 373, 900–916 (2018)
61. Robert, C., Casella, G.: Monte Carlo Statistical Methods. Springer, Heidelberg
(2004)
62. Roberts, G.O., Tweedie, R.L., et al.: Exponential convergence of Langevin distri-
butions and their discrete approximations. Bernoulli 2(4), 341–363 (1996)
63. Stoltz, G., Rousset, M., et al.: Free Energy Computations: A Mathematical Per-
spective. World Scientiﬁc, Singapore (2010)
64. Vanetti,
P.,
Bouchard-Cˆot´e,
A.,
Deligiannidis,
G.,
Doucet,
A.:
Piecewise-
deterministic markov chain monte carlo. arXiv preprint arXiv:1707.05296 (2017)

Geometric Integration of Measure-Preserving Flows for Sampling
355
65. Weinstein, A.: The modular automorphism group of a poisson manifold. J. Geom.
Phys. 23(3–4), 379–394 (1997)
66. Xifara, T., Sherlock, C., Livingstone, S., Byrne, S., Girolami, M.: Langevin diﬀu-
sions and the metropolis-adjusted Langevin algorithm. Statist. Probab. Lett. 91,
14–19 (2014)

Bayesian Inference on Local Distributions
of Functions and Multidimensional
Curves with Spherical HMC Sampling
Anis Fradi1, Ines Adouani2, and Chaﬁk Samir3(B)
1 CNRS UCA and Faculty of Sciences of Monastir, University of Monastir,
Monastir, Tunisia
anis.fradi@etu.uca.fr
2 KFUPM, Dhahran, Saudi Arabia
ines.adouani@kfupm.edu.sa
3 CNRS LIMOS (UMR 6158), UCA, Clermont-Ferrand, France
chafik.samir@uca.fr
Abstract. The aim of this chapter is to deﬁne a geometric framework
and a nonparametric statistical model for learning distributions from
functions and multidimensional curves. Using the population background
where the clustering is the process of grouping curves into homoge-
neous sub-populations, we introduce a distribution function for each sub-
population, and thereby the statistical geometry of the space of smooth
densities to explore a Bayesian model with a spherical Gaussian process
prior. For classiﬁcation and regression on shapes, we suppose that we are
given a ﬁnite set of observations from a population of curves or functions
with values in Rd, for a ﬁxed d ≥1 and arguments in I ⊂R, we repre-
sent them by their shapes, and we give all details for how to build the
Bayesian model and how to solve it with spherical Hamiltonian Monte
Carlo sampling. We also give the expression of the log-posterior distribu-
tion. Finally, the practical interest of the proposed method is illustrated
with numerical results on multidimensional curves.
1
Introduction
Nonparametric statistical models have been used extensively over the past
decade for data lying on manifolds [2,4,24]. For example, statistical shape analy-
sis were successfully applied for analyzing medical and biological data [10,25,28].
Regression on the set of symmetric and positive deﬁnite matrices appears in
many applications that use tensor analysis. Moreover, positive deﬁnite matri-
ces take a key role in tensor computing and statistical analyses [5,20]. Other
typical examples include linear subspaces on Grassmann manifolds, low rank
orthogonal matrices on Stiefel manifolds, longitudinal data and probability den-
sities on the Hilbert sphere, and many others. Such applications and numerical
approaches encourage further research for statistical models on manifolds. How-
ever, their generalization to Riemannian manifolds is not yet straightforward
c
⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2021
F. Barbaresco and F. Nielsen (Eds.): SPIGL 2020, PROMS 361, pp. 356–373, 2021.
https://doi.org/10.1007/978-3-030-77957-3_19

Bayesian Inference on Local Distributions
357
since it depends on the geometric structure and the properties of statistical
models on the underlying manifold.
Recently, nonparametric regression models have been generalized to some
Riemannian spaces: the special orthogonal group SO(n), the set of symmetric
positive deﬁnite matrices Pn
+, the Kendall shape space, the ﬁnite-dimensional
hyperbolic space with constant curvature, the unit sphere [18], the space of prob-
ability densities [7], etc. These more advanced methods turned out to be more
ﬂexible and powerful than the parametric models and oﬀer the ability to model
observed data with more precision. This chapter is motivated by the necessity
for automated methods to learn and predict shapes of objects. Indeed, for many
applications in several branches of science, involving machine learning, robotics,
biology, medical imaging, computer vision, and nanomanufacturing, it is desir-
able to be able to characterize objects for detection, recognition and prediction of
their behavior at unobserved locations. For instance, when quantifying growths
and decays of tumors after medical diagnosis using medical images, a method
for analyzing shapes can help to identify changes in organs and consequently
deduce the progression of diseases [14,28].
The problem we consider in this chapter is the following: Let L2(I, Rd) be
the set of square integrable functions from the unit interval I in R to Rd. For
example, let η : I −→Rd denote a parameterized curve in L2(I, Rd) satisfying:
(i) η is absolutely continuous (ii) ˙η ∈L2(I, Rd). Note that absolute continuity is
equivalent to requiring that ˙η(t) exists for almost t in I, that ˙η(t) is summable
and that η(t) = η(0) +
 t
0 ˙η(s)ds. Now, we shall take a set of curves η1, ..., ηN
satisfying conditions described above and consider the shape representation of
each ηi, i = 1, ..., N by their SRVFs (Square Root Velocity Functions) q1, ..., qN.
Let M denote the space of all qis. Indeed, M is an inﬁnite dimensional manifold
with a Riemannian structure on it as will be detailed in Sect. 2. Our main goal
is to learn a nonparametric predictive model from ηi, i = 1, ..., N. Since M is
not a linear space, it is not immediately obvious that there is a sensible way to
deﬁne this model.
In recent years, particular attention has been paid to the notion of unsuper-
vised and semi-supervised statistical learning from populations of functions and
multidimensional curves. In longitudinal studies and functional data analysis
(FDA) [28], data are usually temporal observations that reside on a manifold.
For planar and spatial curves, they are analyzed according to their shapes which
open a broad range of applications in probability theory, statistics, mathemati-
cal engineering, etc. The reader is referred to, e.g., [10] for a detailed survey. As
a result, diﬀerent models have been proposed, e.g., point-based, domain-based,
and curve-based representations. In particular, studying curves has become an
important tool in evolutionary biology [14] and medical imaging [8]. Progress has
been made but there is still much to be done, especially in regard to modeling
stochastic processes and inferring population distributions [1] as well as their
asymptotic properties.
In this chapter, we introduce a new statistical framework for clustering pop-
ulations of functions or multidimensional curves where the domain is a real

358
A. Fradi et al.
interval I. We assume that we are given a ﬁnite set of observed curves or func-
tions η1, . . . , ηN where each curve has values in Rd, i.e. ηi : I →Rd with d ≥1.
If d = 1, the curve is simply a real function otherwise it becomes a multidi-
mensional curve. For simplicity, we will use the term curve to refer to both of
them and will highlight diﬀerences if any. The main goal of this study is to
form homogeneous groups of curves from N observations, where it seems nat-
ural to assume that the whole population is composed of K (K ≪N) clusters
(sub-populations) with diﬀerent local characteristics. In practice, the curves are
numerically represented by their discrete versions and a common problem when
clustering curves or their shapes is to ignore the distribution of the underlying
discretization. This problem will be solved with the help of a valid Bayesian
model for estimating the local characteristics of each sub-population.
We consider that the population is from a stochastic process. When we draw
a sub-population Mi for a SRVF qi under the probability p(Mi = k) = πk
with k = 1, . . . , K, we impose the condition K
k=1 πk = 1. This formulation is
widely used as a model in many diﬀerent ﬁelds such us economics and biology.
Consequently, the density of i-th curve that deﬁnes the components of the k-th
sub-population is p(qi|Mi = k) and the probability that ηi belongs to the k-
th sub-population is p(Mi = k|qi). Moreover, the k-th sub-population of curves
results from its own local distribution, which can be identiﬁed with a Cumulative
Distribution Function (CDF) F k, unknown and to be estimated. In particular,
we are interested in computing the posterior probability of all CDFs with a
Bayesian model. This task is challenging since we have to assign each curve to
a sub-population.
In the case of clustering a population of Rd elastic or domain-shifted curves
with d ≥2, landmarks are not always available and continuous representations
are successful in this respect. Unlike other approaches, the distance between two
shapes of elastic curves must take into account their parametrizations, identiﬁed
with CDFs through this chapter. The reader can refer to [10] for a survey on
shapes of curves as elements of ﬁnite or inﬁnite-dimensional Riemannian mani-
folds.
However, in practice, choosing such a representation for curves and an eﬃ-
cient distance between their shapes would greatly assist in that regard. We are
particularly interested in SRVF representation [29] of continuous curves in Rd
denoted by qi instead of the original curve ηi. It has been shown that this is
an eﬃcient representation for analyzing shapes of curves. Furthermore, it is the
representation in which the elastic metric reduces to a L2 metric. Several varia-
tions have been proposed, such as dynamic programming or quasi-Newton [17]
to compute the optimal re-parametrization between curves. The proposed model
diﬀers from these methods since we study re-parametrization as CDF and we
propose a Bayesian framework which optimizes class separation to increase clus-
tering performances. This allows us to get a Bayesian inference on CDFs with a
practical interest for potential applications on curves.
To summarize, the proposed method consists in generalizing the clustering of
multidimensional curves represented by their shapes as elements on a Rieman-

Bayesian Inference on Local Distributions
359
nian manifold. The optimization in CDFs space, as a group of diﬀeomorphisms,
is a very hard task due to boundary constraints. In particular, there is no nat-
ural metric or geometric structure on it. This was our motivation to establish
the link with the Hilbert sphere due to its nice properties and geometric appli-
cability. In fact, one of the advantages is that the Riemannian metric becomes
simply the Fisher Rao, the only metric invariant under re-parametrizations. For
this reason, the inference on CDFs F 1, . . . , F K becomes more aﬀordable on the
resulting coeﬃcients A1, . . . , AK of the Karhunen-Lo`eve (K-L) expansion, which
is performed with Hamiltonian dynamics on Sn−1.
For good estimates of constrained distributions on non-linear spaces which
is the case of posterior distributions in this work, one needs to recall the Hamil-
tonian Monte Carlo (HMC) mechanism [11,22,30]. First, [13,27] have deﬁned
a Riemannian HMC sampling from target distributions of high dimensional or
strong correlations. Recently, [9] showed how HMC methods may be applied to
distributions deﬁned on manifolds embedded in Euclidean spaces, by exploiting
the existence of explicit forms for geodesics where constrained domains were
identiﬁed as sub-manifolds. More recently, [19] have proposed a HMC sampling
for target distributions with spherical constraints. We follow the same idea, and
we draw the new samples by solving a system of diﬀerential equations describing
the paths of Hamiltonian dynamics. The iterations are made to control both the
spherical position and the corresponding tangent velocity, jointly.
The rest of the chapter is organized as follows. Section 2 provides some details
about the geometric background of curves and makes connection with the Hilbert
sphere. We present the CDF details in Sect. 3. Section 4 introduces the spherical
Gaussian prior and gives its decomposition. We describe the Bayesian clustering
model for population of functions and curves in Sect. 5. Finally, Sect. 6 shows
numerical examples on spatial curves with some conclusions in Sect. 7.
2
The Space of Smooth Curves
The relationship between Riemann geometry and statistics has been employed in
the development of, primarily asymptotic, statistical theory; see for example [21].
Geometric concepts of distance, curvature, manifolds, geodesics are of natural
interest in statistical methodology and in what follows we present some of these
in the development of statistical shape analysis. In fact, we are interested in
SRVF representation of continuous functions and multidimensional curves in Rd.
It is shown that this is an eﬃcient representation for analyzing shapes of curves.
Furthermore, it is the representation in which the elastic metric reduces to a
simple L2 metric and the space of unit length curves becomes the unit Hilbert
sphere [28]. For the remainder, we supposed the curves to be of dimension d = 3
for simplicity, but all results remain valid for any d ≥1.
2.1
Curve Representation and Shape Space
Let η : I −→R3 denote a parameterized oriented curve in L2(I, R3) satisfying
conditions described in the previous section. For the purpose of studying its

360
A. Fradi et al.
shape, we will represent it using its SRVF q : I −→R3 deﬁned as:
q(s) =

˙η(s)
√
|| ˙η(s)||2 ∈R3
if ˙η(s) ̸= 0
0
otherwise
(1)
Here ∥.∥2 denotes the Euclidean 2-norm in R3. Besides, the original curve η
can be reconstructed using η(s) = η(0) +
 s
0 ||q(t)||2 q(t) dt. The vector valued
function q is the tangent vector ﬁeld normalized by the square-root of the instan-
taneous speed along the curve and is a local descriptor of the geometry of the
curve. Therefore, it becomes an element of a unit sphere in the Hilbert manifold
L2(I, R3) that we will denote M if we assume that all curves are of unit length
since

I ||q(s)||2
2ds =

I || ˙η(s)||2ds. More precisely, we can deﬁne the manifold
(M, ||.||) in the form:
M =

q ∈L2(I, R3)
 ||q||2 =

I
||q(s)||2
2ds = 1

.
(2)
Note that the normalization of the velocity vector ﬁeld in q will play a central
role since it allows us to deﬁne a re-parametrization invariant metric, as we will
see in the next section.
2.2
Geodesics in Shape Space, Exponential Map and Parallel
Transport
An important geometrical construct for the statistical analysis of shapes is the
deﬁnition of the tangent space. Since M is a Hilbert sphere in L2(I, R3), at any
curve q ∈M, we deﬁne the tangent space and we denote Tq(M). We equip the
tangent space of M with a smoothly varying Riemannian metric that measures
inﬁnitesimal lengths on the shape space. This inner product is ﬁrst deﬁned gen-
erally on L2 and then induced on the tangent space of M. The metric deﬁned on
M has a nice physical interpretation of being an elastic metric. More precisely,
let w1 and w2 be two tangent vectors in Tq(M), the metric is deﬁned as,
⟨w1, w2⟩=

I
⟨w1(s), w2(s)⟩2ds.
(3)
Another important step in our shape analysis is to compute geodesic paths
between shapes with respect to the chosen metric. With respect to the SRVF,
M is represented as the Hilbert sphere in L2(I, R3) and obviously a lot is known
about the geometry of a sphere, including geodesics and the exponential map.
Therefore, geodesics between any two points q1 and q2 (not antipodal to q1) on
M are great circles and it is expressed in terms of a nontrivial tangent direction
w ∈Tq1(M) as,
χt(q1; w) = cos (t∥w∥) q1 + sin (t∥w∥) w
∥w∥.
(4)

Bayesian Inference on Local Distributions
361
This equation gives the constant-speed parametrization of the geodesic passing
through q1 with velocity vector w at t = 0. As a result, the exponential map
exp : Tq1(M) −→M is deﬁned as
expq1(w) = q2 = cos(∥w∥)q1 + sin(∥w∥) w
∥w∥.
(5)
The length of the geodesic determines an elastic quantitative distance between
two shapes q1 and q2 in M given by
dM(q1, q2) = cos−1 (⟨q1, q2⟩) .
(6)
From Eqn. 4, the velocity vector along the geodesic path χt is obtained as ˙χt|t=0.
It is also noted that χ0(q1) = q1, and χ1(q1) = expq1(w) = q2. Conversely, given
two shapes q1 and q2, the inverse exponential map (also known as the logarithmic
map) allows the recovery of the tangent vector w between them, and is computed
as follows
exp−1
q1 (q2) = w =
cos−1 (⟨q1, q2⟩)
sin(cos−1 (⟨q1, q2⟩)) (q2 −⟨q1, q2⟩q1) .
(7)
For any two points q1 and q2 on M, the map Γ : Tq1M −→Tq2M parallel
transports a vector w from q1 to q2 and is given by:
Γq1−→q2(w) = w −2 (q1 + q2)

I(w, q2)ds

I(q1 + q2, q1 + q2)ds.
(8)
Fig. 1. Illustration of geometric tools needed for our analysis. An example of a manifold
M and its tangent space Tp(M) at p ∈M. A geodesic curve γ between p, q ∈M. The
geodesic direction is given by v = dγ
dt |t=0. The expp map is a diﬀeomorphism from the
ball B(p, r) to its image on M.
To summarize, the exponential map takes points in the tangent plane to
points on the sphere, preserving distance from q; it also preserves the tangential

362
A. Fradi et al.
direction from q. Concretely, the exponential map only preserves angles and dis-
tances for points in the tangent plane which have distance ⪇π from q; however,
we shall implicitly assume this condition holds whenever it is needed. Given the
above tools for constructing geodesics and inverse exponential maps on the shape
space, we will indicate in the next section how these equations may be used to
solve the problem of ﬁtting a path to a given set of data points q1, . . . , qN on
the shape space. We assume that all qj are not antipodal to any qi, j ̸= i or, in
general, qj are in the cut locus of qi (Fig. 1).
3
Analyzing Warping Functions
Before we give details of the proposed model, we would like to recall some notions
about the geometry of manifold of probability distributions, some corresponding
metrics, and the induced geometrical structure. The metric is ﬁxed to be the
Fisher-Rao metric. The ﬁrst choice of representations is the space of CDFs (also
called warping functions) restricted to be deﬁned on I, non-decreasing, and
satisfying
F =
	
F : I →I | F(0) = 0, F(1) = 1, and
˙F is nonnegative

.
(9)
When equipped with the L2 metric, this space can be viewed as a sub-manifold
of the Hilbert space L2(I, R). We identify any tangent space of F, locally at F,
by
TF (F) =
	
f : I →R |

I
˙f(t)dt = 0

.
(10)
The tangent space contains functions that are inﬁnitesimally diﬀerentiable.
By following [16], we have a constructive method to form a local version of
any arbitrary F that is continuously diﬀerentiable in a small neighborhood and
null outside. Now that we have a smooth manifold and its tangent space, we
can introduce a Riemannian metric. This choice is very important since it will
determine the structure of F and consequently the covariance function of the
Gaussian process. More details about the importance of the metric and the
induced Riemannian structure are discussed in [24]. Among several metrics, we
are particularly interested in the Fisher-Rao metric deﬁned, for any tangent
vectors f1, f2 ∈TF (F), by

f1, f2

F =

I
˙f(t) ˙f2(t)
˙F(t)
dt.
(11)
Although this metric has nice properties and is of great interest [31], F equipped
with

., .

F is still numerically intractable. Therefore, instead of working on
F directly, we consider a mapping from F to the Hilbert upper-hemisphere
(positive part) around the unity 1F such that 1F(t) = t for all t in I. This
space has the advantage of exploiting some nice statistical tools on the sphere

Bayesian Inference on Local Distributions
363
such as geodesics, exponential maps, log maps, etc. This is related to the notion
of so-called statistical manifolds or statistical geometry, which can arise from
the embedding of probability density functions in the Hilbert space of square-
integrable functions. In probability theory, one typically deals with a probability
density function ˙F on I. For the function ˙F to represent the density of some
random variables, we require that ˙F(t) is non-negative for all t ∈I and that

I ˙F(t)dt = 1. If we consider the transformation ψ(t) =

˙F(t) for all t ∈I, then
ψ belongs to the space L2(I, R). In particular, since ψ satisfy the normalization
condition

I ψ2(t)dt = 1, it is an element of the Hilbert sphere S∞⊂L2(I).
More precisely, the space of functions ψ is given by
H =
	
ψ ∈L2(I) | ψ is nonnegative, and ||ψ||L2 =
 
I
ψ2(t)dt
1/2
= 1

,(12)
which can be identiﬁed with the Hilbert upper-hemisphere S∞
+ and the tangent
space of H at ψ is
Tψ(H) =
	
g : I →R |

I
ψ(t)g(t)dt = 0

.
(13)
For any two tangent vectors g1, g2 ∈Tψ(H), the Fisher-Rao metric on this rep-
resentation is taken to be the integral inner product, deﬁned by

g1, g2

L2 =

I
g1(t)g2(t)dt.
(14)
Note that for each ψ ∈H, we can deﬁne a unique CDF F, satisfying
F(t) =
 t
0
ψ2(s)ds, ∀t ∈I.
(15)
Fig. 2. Left: Schematic illustration of the geodesic path on the sphere. Right: The
corresponding CDFs along the set of points on that path.

364
A. Fradi et al.
The advantage of working in the Hilbert sphere H rather than the space of
probability density functions or even CDFs is that H is a space endowed with
various geometric features that are more familiar. To illustrate this point, we
suppose that we have two distribution functions F1 and F2, and we wish to
compare their overlap or separation. If the corresponding Hilbert sphere repre-
sentations are given respectively by ψ1 and ψ2, then the overlap is measured in
terms of the inner product, i.e., both have unit norm. This overlap is given by
the angle (arccos) of the shortest arc (geodesic) between them on S∞[7] (Fig. 2).
4
The Spherical Gaussian Process
In order to simplify the estimation of CDFs, we propose to use the K-L expansion
of ψ as a linear sum of basis functions in L2(I) with random coeﬃcients [15].
In this respect, we consider a Gaussian process ψ : I →R. We recall that ψ is
a stochastic process such that for any m ∈N∗and for any t1, . . . , tm ∈I, the
random vector (ψ(t1), . . . , ψ(tm)) is a Gaussian vector [32]. We assume that ψ
has mean function zero, i.e., E

ψ(t)

= 0 for t ∈I. Empirically, setting the prior
mean to zero is often achieved by subtracting the mean from all observations.
Thus ψ is as a random function, itself drawn from a Gaussian process (GP)
with a continuous and square-integrable covariance function c(., .) over I × I,
i.e.,
ψ(t) ∼GP

0, c(t, s)

, where ψ ∈H.
(16)
We assume that the covariance function is stationary, i.e., there exists a kernel
C : I →R such that for (t, s) ∈I × I,
c(t, s) = C(|t −s|).
Generally, the GP is a distribution over functions, which is controlled by its
mean and its covariance function. The kernel C is positive deﬁnite in the sense
of the following deﬁnition
Deﬁnition 1. A function C : I →R is positive deﬁnite if for any t1, . . . , tm ∈
I, the m × m matrix [C(|ti −tj|)]i,j is positive semi-deﬁnite.
We will consider a set of functions {Cθ; θ ∈Θ} where θ is an hyper-parameter
for which Cθ is positive deﬁnite.
Let (φj)j denote a system of orthonormal eigenfunctions in L2(I) and (λj)j
the associated non-negative eigenvalues of c(., .). We deﬁne the Hilbert-Schmidt
integral operator as a mapping from L2(I) into itself, expressed by L : φj →Lφj
and satisfying
(Lφj)(s) =

I
c(t, s)φj(t)dt.
(17)
By Mercer’s theorem [6], the covariance function can be expressed as
c(t, s) =
∞

j=1
λjφj(t)φj(s).
(18)

Bayesian Inference on Local Distributions
365
Furthermore, (λj)j and (φj)j are the eigenvalues and the eigenfunctions of L,
obtained when solving the Fredholm integral equation
(Lφj)(s) = λjφj(s).
(19)
To maintain the constraint that ψ ∈H, we only focus on the restriction that
||ψ||L2 = 1, since ψ is non-negative does not impose any additional constraint.
Then, the K-L expansion of ψ is
ψ(t) =
∞

j=1
ajφj(t), with aj
ind
∼N

0, λj

.
(20)
In this chapter, we take into account a truncated version at order n of the K-L
expansion, given by
ψn(t) =
n

j=1
ajφj(t),
(21)
with the approximation error
en(t) =
∞

j=n+1
ajφj(t).
(22)
This choice results from the fact that among all versions expressed in Eq. 21,
the truncated K-L expansion is optimal in the sense of minimizing the mean
integrated squared error (MISE) given by

I E

(en(t)2
dt. From Eq. 15, we get
Fn(t) =
 t
0
ψ2
n(s)ds, ∀t ∈I
(23)
=
n

j=1
a2
j
 t
0
φ2
j(s)ds + 2
n

j=1
n

r=j+1
ajar
 t
0
φj(s)φr(s)ds.
Theorem 1. Fn is a CDF if and only if A =

a1, . . . , an

belongs to Sn−1.
Proof. The proof is made easy with the following steps:
• t →Fn(t) is a C1 mapping on I, since t →ψn(t) is a continuous one for all
t ∈I.
• Fn(0) =
 0
0 ψ2
n(s)ds = 0, by deﬁnition.
• Fn(1) =

I ψ2
n(s)ds = 1, if and only if A ∈Sn−1.
•
˙Fn(t) = ψ2
n(t) is non-negative for all t ∈I.
□
Consequently, the resulting version makes it easier to check that Fn is a CDF
instead of F. It translates directly to a ﬁnite-dimensional spherical constraint on
the random coeﬃcients aj, for j = 1, . . . , n. For certain covariance operators, the
expansions were obtained with the help of special basis functions. In particular,
the trigonometric functions, the Legendre polynomials, and recently, the Bessel
functions. More about the numerical decomposition of a stochastic process are
given by [15].

366
A. Fradi et al.
5
The Bayesian Model
Let η be a curve deﬁned on I with values in R3 (η : I →R3) and ||.||2 refer
to the standard norm. For the purpose of studying its shape, we will represent
each curve by its SRVF q deﬁned in Eq. 1. We remind that q is well deﬁned and
is an element of M. If the domain of q is transformed with a CDF F ∈F, then
q ◦F will be represented by q∗verifying
q∗(t) =

˙F(t)q(F(t)) = ψ(t)q
  t
0
ψ2(s)ds

.
(24)
The advantage of this representation is that it is invariant to translation due
to derivatives and it takes into account the uniform scaling (multiplication with
a scalar). In this work, we only focus on CDFs and we assume that rotations
can be easily removed with the Generalized Procrustes Analysis (GPA) [10]. We
consider that the original curves have been represented with {q∗
i }N
i=1, then their
Fr´echet mean in M becomes the simple mean given by
˜q = argmin
q∈M
N

i=1
inf
Fi∈F ||q −q∗
i ||2,
(25)
= 1
N
N

i=1
q∗
i ,
where each Fi is updated iteratively until convergence.
5.1
Maximum Likelihood
Now, we are ready to reformulate our problem for clustering multidimensional
curves. We assume that we have a ﬁnite set of N curves q∗
1, . . . , q∗
N to be grouped
into K populations with K ≪N. For each q∗
i , we draw a class Mi with val-
ues in {1, . . . , K} under the probability p(Mi = k) = πk where K
k=1 πk = 1.
Consequently, the density of the i-th curve that deﬁnes the components of the
k-th sub-population is given by p(q∗
i |Mi = k). We consider that a discretiza-
tion q∗
i (th) ∈R3, h = 1, . . . , m is observed and we note T = (t1, . . . , tm). We
can model q∗
i (T)|Mi = k with a multivariate Gaussian, since q∗
i is a continu-
ous function. For simplicity, we assume that q∗
i (T)|Mi = k ∼N

˜qk(T), γ2I

where γ2 > 0 is the variance parameter and I is the 3m × 3m identity matrix.
This work deals with estimating the unknown CDF for the k-th sub-population.
Let F k denote this (unknown) function. The density of qi with components of
sub-population k becomes
p(qi|F k, ˜qk(T), γ2) ∝exp

−
1
2γ2 ||q∗
i (T) −˜qk(T)||2
2

.
(26)
For the prior laws, we use F k
n as detailed in Eqn. 23 to approximate F k. Conse-
quently, the prior on F k becomes a simple prior on Ak = (ak
1, . . . , ak
n) ∈Sn−1,

Bayesian Inference on Local Distributions
367
satisfying
p(Ak) ∝exp

−
n

j=1
ak
j
2
2λj

× δAk∈Sn−1,
(27)
since Ak ∼N

0n, diag(λ1, . . . , λn)

where we assume that Aks are indepen-
dent variables and result from the same integral equation deﬁned in Eq. 19. We
summarize all in the following theorem.
Theorem 2. Given D = {qi}N
i=1, π1, . . . , πK, ˜q1(T), . . . ˜qK(T) and γ2, the log-
posterior of A1, . . . , AK is
log p(A1, . . . , AK|D, π1, . . . , πK, ˜q1(T), . . . , ˜qK(T), γ2)
(28)
∝
N

i=1
log
 K

k=1
πk exp

−
1
2γ2 ||q∗
n,i(T) −˜qk(T)||2
2

−1
2
K

k=1
n

j=1
ak
j
2
λj
.
under the constraint that A1, . . . , AK belong to Sn−1 and where q∗
n,i(T) is the
modiﬁed version of q∗
i (T) when replacing F by Fn.
The proof is detailed in [12].
5.2
Spherical Hamiltonian Monte Carlo
We now give short descriptions of the spherical HMC method. For more details
and extensive review see [19]. The Hamiltonian which forms the basis of HMC
sampling will now be deﬁned in a general form on the unit sphere. In a more
general context, with the help of tangent bundle, the deﬁnition of the Hamil-
tonian on a Riemann manifold becomes more practical. In some ways, this was
done in geometric mechanics to solve partial diﬀerential equations [22]. In this
chapter, we call it the spherical HMC on Sn−1 for simulating from the poste-
rior of A = (A1, . . . , AK) with an extra Metropolis-Hasting or Gibbs sampling
for updating π1, . . . , πK, γ2. For this purpose, we estimate the Fr´echet mean
of ˜q1(T), . . . , ˜qK(T), iteratively, from Eq. 25. The HMC sampling augments the
state space with an auxiliary velocity variable V = (V 1, . . . , V K) (satisfying:
V kAk = 0, k = 1, . . . , K) compared to the MCMC sampling [23]. It also simu-
lates from a Hamiltonian dynamic (H) from two terms (H = H1 + H2) with a
potential energy deﬁned by the minus of log-posterior computed in Eq. 28, i.e.,
H1(A) = −log p(A|D, π1, . . . , πK, ˜q1(T), . . . , ˜qK(T), γ2),
(29)
and a kinetic energy, satisfying
H2(V) = 1
2
K

k=1
V kT GV k,
(30)
where G refers to the canonical spherical metric.

368
A. Fradi et al.
The spherical HMC establishes the link between the unit sphere Sn−1 in Rn
and the unit ball Bn−1
0
in Rn−1 [19]. In particular, if we restrict the coeﬃcients
vector ¯Ak = (ak
1, . . . , ak
n−1) to take the (n−1) ﬁrst components of Ak then ¯Ak ∈
Bn−1
0
with a norm strictly less than 1. Therefore, we can rewrite a decomposition
of Ak as Ak = ( ¯Ak,

1 −|| ¯Ak||2
2). We detail all steps of the spherical HMC in
Algorithm 1 in terms of A = (A1, . . . , AK), ¯A = ( ¯A1, . . . , ¯AK) and the block
diagonal matrix
M =
⎛
⎜
⎜
⎜
⎜
⎝

In−1
0

0
0
. . .
0

In−1
0

0
. . .
0
. . .
0
In−1
0




(n−1)K
⎞
⎟
⎟
⎟
⎟
⎠
⎫
⎪
⎪
⎪
⎪
⎬
⎪
⎪
⎪
⎪
⎭
nK.
Algorithm 1: Spherical HMC sampling.
Require: Negative log-posterior H1(.) and its gradient ∇H1(.)
Ensure: ˆA
1: Initialize A0
2: Sample a new momentum value V0 ∼N(0, I) where I is the nK × nK identity matrix
3: V0 := V0 −A0AT
0 V0
4: Calculate H(A0, V0) := H1( ¯A0) + H2(V0)
5: for t = 1, 2, . . . , N do
6:
Vt−1
2 := Vt−1 −ϵ
2

M −At−1 ¯AT
t−1

∇H1( ¯At−1)
7:
At := At−1 cos(||Vt−1
2 ||2ϵ) +
Vt−1
2
||Vt−1
2
||2 sin(||Vt−1
2 ||2ϵ)
8:
Vt−1
2 := −At−1||Vt−1
2 ||2 sin(||Vt−1
2 ||2ϵ) + Vt−1
2 cos(||Vt−1
2 ||2ϵ)
9:
Vt := Vt−1
2 −ϵ
2

M −At ¯AT
t

∇H1( ¯At)
10: end for
11: Calculate H(AN, VN) := H1( ¯AN) + H2(VN)
12: Acceptance probability
α := min

1, exp(−H(AN, VN))
exp(−H(A0, V0))

13: Simulate u ∼U([0, 1])
14: if u < α then
15:
Accept the proposal ˆA := AN
16: else
17:
Reject the proposal ˆA := A0
18: end if

Bayesian Inference on Local Distributions
369
Once we have estimated all of the model parameters, we can evaluate the
conditional probability that the i-th curve belongs to k-th sub-population by
p(Mi = k|qi) = p(Mi = k, qi)
p(qi)
(31)
= p(Mi = k)p(qi|Mi = k)
p(qi)
=
πkp(qi|Mi = k)
K
k=1 πkp(qi|Mi = k)
=
πk exp

−
1
2γ2 ||q∗
i (T) −˜qk(T)||2
2

K
k=1 πk exp

−
1
2γ2 ||q∗
i (T) −˜qk(T)||2
2
.
6
Numerical Examples
In this section, we demonstrate the eﬀectiveness of our method to a clustering
task of curves in order to assign each observed curve to its sub-population.
As baseline, we compare results of our method with coeﬃcients estimated by
spherical HMC sampling in a Bayesian framework against:
• The geodesic kmeans detailed in [3]. The geodesic distance is computed on
the embedded sphere S(m−1)d−1. All data are centered and normalized using
the GPA algorithm [10]. This results in a representation which is invariant
under the eﬀects of translation and scaling. The CDFs have been factorized
out: We pre-computed the CDFs to the Fr´echet mean using a Riemannian
quasi-Newton algorithm [17]. The main goal of these experiments is to show
the importance of the Bayesian inference on CDFs detailed in Sect. 5.
• The geodesic kmedoids: We update the classical kmedoids clustering [26] with
the same geodesic distance as for geodesic kmeans where the CDFs have been
factorized out.
We restrict ourselves to the binary case (K = 2) where we focus on two simu-
lated and one real datasets. For the covariance operator, we model ψ by a GP
with a Hilbert-Schmidt integral satisfying L =

ϵ −∂2
x
−γ, with scale param-
eter ϵ and smoothness parameter γ. By solving Eqn. 19, one can check that
the corresponding eigen-values and eigen-functions are λj =

ϵ + j2π2−γ and
φj(t) =
√
2 cos(jπt), for j ≥0. In all experiments, the hyper-parameter setting
of the Hilbert-Schmidt operator is ﬁxed to θ = (ϵ, γ) = (0.5, 1). Additionally,
104 HMC iterations were used to make each ﬁgure with n = 30.
We show the performance of the proposed framework in term of accuracy
using two simulations. We perform experiments on two examples of parameter-
ized curves: planar curves in R2 and spatial curves in R3. Each curve is most
likely in class ˆk which maximizes the conditional probability given in Eq. 31,
i.e., ˆk = argmaxk p(Mi = k|qi) for each i. The classiﬁcation error, noted (CE),
occurs if the observed class and the true class are diﬀerent.

370
A. Fradi et al.
6.1
Planar Curves
In this section, we illustrate the proposed method using some simulated exam-
ples of planar curves. The goal is to evaluate the cluster recovery and to test
the sensitivity with respect to the theoretical part. We consider two families of
parametric curves: for all t ∈I,
η1(t) =

x(t) = ( 1
2 + cos( 5
2πt)) cos( 5
2πt)
y(t) = ( 1
2 + cos( 5
2πt)) sin( 5
2πt)
η2(t) =
x(t) = ( 1
3 + cos(2πt)) cos(2πt)
y(t) = ( 1
3 + cos(2πt)) sin(2πt)
We display two examples (η1 in blue and η2 in red) in Fig. 3 (a). In simulations,
we generate N = 100 curves for each class using a Gaussian perturbation model
where the i-th conﬁguration is obtained as follows
ηi(T)|Mi = k ∼N(ηk ◦F(T), γ2I), i = 1, . . . , N, k = 1, . . . , K,
where the discretization of I is given for m = 20, i.e., T = (t1, . . . , tm). The true
CDF is the identity function, i.e., F(t) = t for all t ∈I and I is the 2m × 2m
identity matrix. The plots in Fig. 3 illustrate the simulated curves forming the
noisy data in (b, c) with two variance levels γ2 = 0.01 and γ2 = 0.1, respectively.
We use the observed curves ηi(T) for the comparison methods. For our proposed
approach, we need to use the transformed curves q∗
i (T) in order to search the
optimal CDF per class.
Fig. 3. An example of observed planar curves: original (left), with variance γ2 = 0.1
(middle) and γ2 = 0.01 (right). Class 1 in blue and Class 2 in red.
Table 1. Error rates for planar curves.
Methods
γ2 = 0.01 γ2 = 0.1
Geodesic kmeans
82%
77%
Geodesic kmedoids 84%
78%
Proposed
91%
83%

Bayesian Inference on Local Distributions
371
Table 1 summarizes the error rates for planar curves and at each level where it
is shown that the proposed method is very accurate and has a better predictor.
This result clearly shows the utility of the CDF estimates, obtained by max-
imizing the log-posterior distribution on their associated coeﬃcients on Sn−1
(n = 30), to reach good results.
6.2
Spatial Curves
In this experiment, we illustrate our method using spatial curves. We consider
two classes of curves deﬁned by:
η1(t) =
⎧
⎨
⎩
x(t) = sin(12πt)
y(t) = cos(12πt)
z(t) = 2t
η2(t) =
⎧
⎨
⎩
x(t) = sin(15πt)
y(t) = cos(15πt)
z(t) = 2t
Fig. 4. An example of observed spacial curves: original (left), with variance γ2 = 0.1
(middle) and γ2 = 0.01 (right). Class 1 in blue and Class 2 in red.
We display two examples (η1 in blue and η2 in red) in Fig. 4 (a). We simulate
N = 100 curves for each class using the same model as for planar curves where
the discretization of I is m = 100 and I is the 3m × 3m identity matrix.
An example of the simulated data forming the observed curves is given in
Fig. 4 (b,c) with two variance levels γ2 = 0.01 and γ2 = 0.1, respectively. Table 2
summarizes the error rates for spatial curves. This conﬁrms the good results on
planar curves and clearly shows that the CDF estimates from the log-posterior
distribution on Sn−1 (n = 30) is optimal.

372
A. Fradi et al.
Table 2. Error rates for spacial curves.
Methods
γ2 = 0.01 γ2 = 0.1
Geodesic kmeans
84%
75%
Geodesic kmedoids 84%
76%
Proposed
88%
81%
7
Conclusion
In this chapter, we have presented a population background with a Bayesian clus-
tering for functions and multidimensional curves. We have considered that each
sub-population has its own unknown local distribution and we have formulated
the problem to estimate them jointly. Thanks to the geometric structure, the
proposed method solves the optimization problem, originally deﬁned on inﬁnite-
dimensional and complex space of CDFs, by an equivalent and more practical
one. We have tested the model on planar and spacial curves and showed several
beneﬁts by estimating the CDF for each class.
References
1. Abhishek, B., Rabi, B.: Nonparametric Inference on Manifolds: With Applications
to Shape Spaces, 1st edn. Cambridge University Press, New York (2012)
2. Absil, P.-A., Mahony, R., Sepulchre, R.: Optimization Algorithms on Matrix Man-
ifolds. Princeton University Press, Princeton (2008)
3. Amaral, G., Dore, L., Lessa, R., Sto¨sc, B.: k-means algorithm in statistical shape
analysis. Commun. Stat. Simul. Comput. 39, 1016–1026 (2010)
4. Amari, S.-I.: Diﬀerential geometry of statistical inference. In: Prokhorov, J.V., Itˆo,
K. (eds.) Probability Theory and Mathematical Statistics, pp. 26–40. Springer,
Heidelberg (1983). https://doi.org/10.1007/BFb0072900
5. Arsigny, V., Fillard, P., Pennec, X., Ayache, N.: Log-Euclidean metrics for fast and
simple calculus on diﬀusion tensors. Magnetic Resonance Med. 56, 411–421 (2006)
6. Bertinet, A., Agnan, T.C.: Reproducing Kernel Hilbert Spaces in Probability and
Statistics. Springer, Boston (2004). https://doi.org/10.1007/978-1-4419-9096-9
7. Bhattacharyya, A.: On a measure of divergence between two statistical populations
deﬁned by their probability distributions. Bull. Calcutta Math. Soc. 35, 99–109
(1943)
8. Bishop, C.M.: Pattern recognition and machine learning. Springer-Verlag, Berlin,
Heidelberg (2006)
9. Byrne, S., Girolami, M.: Geodesic monte carlo on embedded manifolds. Scand. J.
Stat. 40, 825–845 (2013)
10. Dryden, I.L., Mardia, K.V.: Statistical shape analysis, with applications in R, 2nd
edn. John Wiley and Sons, Chichester (2016)
11. Ernst, H., Christian,L., Gerhard, W.: Geometric Numerical Integration: Structure-
Preserving Algorithms for Ordinary Diﬀerential Equations, 2nd edn. Springer, Hei-
delberg (2006). https://doi.org/10.1007/3-540-30666-8

Bayesian Inference on Local Distributions
373
12. Fradi, A., Samir, C.: Bayesian cluster analysis for registration and clustering homo-
geneous subgroups in multidimensional functional data. Commun. Stat. Theory
Methods 49, 1–17 (2020)
13. Girolami, M., Calderhead, B.: Riemann manifold Langevin and Hamiltonian Monte
Carlo methods. J. R. Stat. Soc. Ser. B Methodol. 73, 123–214 (2011)
14. Grenander, U., Chow, Y., Keenan, D.M.: Hands: A Pattern Theoretic Study of Bio-
logical Shapes. Springer, Heidelberg (1991). https://doi.org/10.1007/978-1-4612-
3046-5
15. Gutierrez, R., Ruiz-Molina, J.C., Valderrama, M.: On the numerical expansion of
a second order stochastic process. Appl. Stochastic Models Data Anal. 8, 67–77
(1992)
16. Helgason, S.: Diﬀerential Geometry, Lie Groups, and Symmetric Spaces. Academic
Press, New York (1978)
17. Huang, W., Gallivan, K.A., Srivastava, A., Absil, P.-A.: Riemannian optimization
for registration of curves in elastic shape analysis. J. Math. Imaging Vis. 54, 320–
343 (2016)
18. Krakowski, K.A., Manton, J.H.: On the computation of the Karcher mean on
spheres and special orthogonal groups. In: Workshop Robot, Math (2007)
19. Lan, S., Zhou, B., Shahbaba, B.: Spherical hamiltonian monte carlo for con-
strained target distributions. In: Proceedings of the 31st International Conference
on Machine Learning, Bejing, China, 2014, PMLR, pp. 629–637 (2017)
20. Moakher, M., Zerai, M.: The Riemannian geometry of the space of positive deﬁnite
matrices and its application to the regularization of positive-deﬁnite matrix-valued
data. J. Math. Imaging Vis. 40, 171–187 (2011)
21. Murray, M., Rice, J.: Diﬀerential Geometry and Statistics. Taylor & Francis, Boca
Raton (1993)
22. Neal, R.M.: MCMC using Hamiltonian dynamics. Handbook of Markov Chain
Monte Carlo 54, 113–162 (2010)
23. Ravenzwaaij, D., Pete, C., Scott, B.D.: A simple introduction to Markov Chain
Monte Carlo sampling. Psychonomic Bull. Rev. 25, 143–154 (2018)
24. Samir, C., Absil, P.-A., Srivastava, A., Klassen, E.: A gradient-descent method for
curve ﬁtting on Riemannian manifolds. Found. Comput. Math. 12, 49–73 (2012)
25. Samir, C., Srivastava, A., Daoudi, M., Klassen, E.: An intrinsic framework for
analysis of facial surfaces. Int. J. Comput. Vision 82, 80–95 (2009)
26. Sang, P.H., Hyuck, J.C.: A simple and fast algorithm for K-medoids clustering.
Expert Syst. Appl. 36, 3336–3341 (2009)
27. Shahbaba, B., Lan, S., Johnson, W.O., Neal, R.M.: Split Hamiltonian Monte Carlo.
Stat. Comput. 24, 339–349 (2012)
28. Srivastava, A., Klassen, E.: Functional and Shape Data Analysis. Springer, New
York (2016). https://doi.org/10.1007/978-1-4939-4020-2
29. Srivastava, A., Klassen, E., Joshi, S.H., Jermyn, I.H.: Shape analysis of Elastic
curves in Euclidean spaces. IEEE Trans. Pattern Anal. Mach. Intell. 33, 1415–
1428 (2011)
30. Teschl, G.: Ordinary Diﬀerential Equations and Dynamical Systems. Graduate
studies in mathematics. American Mathematical Society (2012)
31. ˇCencov, N.N.: Statistical Decision Rules and Optimal Inference. Translations of
Mathematical Monographs. American Mathematical Society (1982)
32. Williams, C.K.I., Rasmussen, C.E.: Gaussian processes for regression. In: Advances
in Neural Information Processing Systems, vol. 8, pp. 514–520. MIT press (1996)

Sampling and Statistical Physics
via Symmetry
Steve Huntsman(B)
Alexandria, VA, USA
Abstract. We formulate both Markov chain Monte Carlo (MCMC)
sampling algorithms and basic statistical physics in terms of elemen-
tary symmetries. This perspective on sampling yields derivations of well-
known MCMC algorithms and a new parallel algorithm that appears to
converge more quickly than current state of the art methods. The sym-
metry perspective also yields a parsimonious framework for statistical
physics and a practical approach to constructing meaningful notions of
eﬀective temperature and energy directly from time series data. We apply
these latter ideas to Anosov systems.
1
Introduction
Sampling and statistical physics are essentially dual concepts. Phenomenologists
sample from physical models to obtain data, and theorists construct physical
models to explain data. For simple data and/or systems, the pushforward of an
initial state distribution under a deterministic dynamical model may be theo-
retically adequate (at least up to a Lyapunov time or its ilk), but for complex
data and/or systems, an intrinsically statistical model is typically necessary.
Moreover, sampling strategies and physical models are frequently manifes-
tations of each other [1]. For instance, Glauber (spin-ﬂip), Kawasaki (spin-
exchange), and Swendsen-Wang (spin-cluster) dynamics are each both special-
purpose Markov chain Monte Carlo (MCMC) algorithms and models for the
time evolution of a spin system. Each algorithm/model has its own physical fea-
tures, e.g. spin-ﬂip dynamics are suited to the canonical ensemble; spin-exchange
dynamics preserve an order parameter; and spin-cluster dynamics are both more
eﬃcient and descriptive for systems near criticality [2]. As the chaotic hypothesis
essentially stipulates that spin systems are generic statistical-physical systems
[3,4], this blurring of the distinction between algorithm and model can also be
regarded as generic.1
Meanwhile, physics has a long tradition of formulating theories in terms of
symmetries. Perhaps surprisingly, both sampling strategies and the basic struc-
ture of statistical physics itself can also be formulated in terms of symmetries.
We outline these respective formulations with an eye towards (in Sect. 2) eﬃ-
cient parallel MCMC algorithms and (in Sect. 3) eﬀective temperatures and
1 NB. Even SU(N) ﬁeld theory can be treated as a spin system: see, e.g. [5].
c
⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2021
F. Barbaresco and F. Nielsen (Eds.): SPIGL 2020, PROMS 361, pp. 374–427, 2021.
https://doi.org/10.1007/978-3-030-77957-3_20

Sampling and Statistical Physics via Symmetry
375
energy functions that can be obtained directly from data for descriptive pur-
poses. Finally, in Sect. 4 we apply the ideas of Sect. 3 to Anosov systems, where
they suggest a broader framework for nonequilibrium statistical physics.
2
Sampling via Symmetry
MCMC algorithms estimate expected values by running a Markov chain with
the desired invariant measure. Though they arose from computational physics,
MCMC algorithms have become ubiquitous, particularly in statistical inference
and machine learning, and their importance in the toolkit of numerical algo-
rithms is diﬃcult to overstate [6,7].
As such, there is a vast literature on MCMC algorithms. However, there is
also much still left unexplored. As we shall see, the interface between MCMC
algorithms and the theory of Lie groups and Lie algebras holds a surprise. A key
observation is that the space of transition matrices with a given invariant mea-
sure is a monoid that is closely related to a Lie group. Certain natural elements
of this monoid with simple closed form expressions naturally lead to construc-
tions of the classical Barker and Metropolis MCMC samplers. These construc-
tions generalize, leading to higher-order versions of samplers that respectively
correspond to the ensemble MCMC algorithm of [8] and an algorithm of [9].
A further generalization leads to a new algorithm that we call the higher-order
programming solver and whose convergence appears to improve on the state of
the art. Each of these algorithms is only presently deﬁned for ﬁnite state spaces
and leaves the proposal mechanism unspeciﬁed: indeed, our entire focus is on
acceptance mechanisms.2
In this section, which is based on the conference paper [10], we review the
basics of MCMC, Lie theory, and related work in Sect. 2.1. We then brieﬂy
consider the Lie group generated by a probability measure in Sect. 2.2. In par-
ticular, we construct a convenient basis of the subalgebra of the stochastic Lie
algebra that annihilates a target probability measure p. This basis only requires
knowledge of p up to a multiplicative factor (e.g., a partition function), and this
fact is the essential reason why MCMC algorithms work in general. In Sect. 2.3,
we show how we can analytically produce transition matrices that leave p invari-
ant. We then construct the Barker and Metropolis samplers from Lie-theoretic
considerations in Sect. 2.4. In Sect. 2.5, we extend earlier results, leading to
generalizations of the Barker and Metropolis samplers that entertain multiple
proposals at once and that we explicitly construct in Sect. 2.6. We then demon-
strate the behavior of these samplers on a small spin glass in Sect. 2.7. In Sect.
2.8, we outline the construction of multiple-proposal transition matrices that
are closest in Frobenius norm to the “ideal” transition matrix 1p, and we intro-
duce and demonstrate the resulting higher-order programming solver. Finally,
we close our discussion of MCMC algorithms with remarks in Sect. 2.9.
2 By repeated sampling, we can extend any proposal mechanism for single states to
multiple states.

376
S. Huntsman
Algorithm 1. MCMC
Input: Runtime T and Pjk = qjkαjk with pP = p
Initialize t = 0 and X0
repeat
for each state k do
Propose k with probability qjk
end for
Accept Xt+1 = k with probability αjk
Set t = t + 1
until t = T
Output: {Xt}T
t=0 ∼p×(T +1) (approximately)
2.1
Background
2.1.1
Markov Chain Monte Carlo
As we have already mentioned in Sect. 2 and (e.g.) [11] discusses at length,
MCMC algorithms estimate expected values of functions with respect to a prob-
ability measure p that is infeasible to construct. The archetypal instance comes
from equilibrium statistical physics, where pj = Z−1 exp(−βEj) is hard to com-
pute because the partition function Z is unknown due to the scale of the problem,
but the energies Ej are individually easy to compute. The miracle of MCMC is
that we can construct an irreducible, ergodic Markov chain with invariant mea-
sure p using only unnormalized and easily computable terms such as exp(−βEj).
Let Xt denote the state of such a chain at time t. In the limit, Xt ∼p for any
initial condition, and Epf(X) = limt→∞1
t
t
j=1 f(Xj) even though the Xj are
correlated. The problem of constructing such a chain is typically decomposed into
proposal and acceptance steps as in Algorithm 1, with respective probabilities
qjk := P(X′ = k|Xt = j) and αjk := P(Xt+1 = k|X′ = k, Xt = j). The proposal
and acceptance are combined to form the chain transitions via Pjk := P(Xt+1 =
k|Xt = j) = qjkαjk.
The reasonably generic Hastings algorithm employs an acceptance mecha-
nism of the form αjk =
sjk
1+tjk , where tjk := pjqjk
pkqkj and s need only be symmetric
with entries sjk ∈(0, 1 + min(tjk, tkj)]. The Barker sampler corresponds to the
choice sjk = 1, while the Metropolis-Hastings sampler corresponds to the opti-
mal [12] choice sjk = 1 + min(tjk, tkj).
2.1.2
Lie Groups and Lie Algebras
For the sake of self-containment, we brieﬂy restate the basic concepts of Lie
theory in the real and ﬁnite-dimensional setting. For general background on Lie
groups and algebras, see, e.g. [13,14].
A Lie group is a manifold with a smooth group structure. The tangent space
of a Lie group G at the identity is the Lie algebra lie(G): the group structure is
echoed in the algebra via a bilinear antisymmetric bracket [·, ·] that satisﬁes the
Jacobi identity
[X, [Y, Z]] + [Y, [Z, X]] + [Z, [X, Y ]] = 0.

Sampling and Statistical Physics via Symmetry
377
By Ado’s theorem, a real ﬁnite-dimensional Lie group G is isomorphic to
a subgroup of the group GL(n, R) of invertible n × n matrices over R. In this
circumstance, lie(G) is isomorphic to a subalgebra of real n × n matrices, with
bracket as the usual matrix commutator [X, Y ] := XY −Y X. Meanwhile, the
matrix exponential sends lie(G) to G in a way that respects both the algebra
and group structures.
2.1.3
Related Work
The higher-order Barker and Metropolis samplers we construct have previously
been considered in [8] and [9], respectively. Besides ensemble algorithms, [15]
details approaches to accelerating MCMC algorithms via multiple try algorithms
as in [16–18]; and by parallelization as in [19].
Discrete symmetries that (possibly approximately) preserve the level sets
of a target measure have also been exploited to accelerate MCMC algorithms
in [20–25]. Similarly, “group moves” for MCMC algorithms were considered in
[26,27]. However, we are not aware of previous attempts to consider continuous
symmetries preserving a target measure in the context of MCMC.
That said, Markov models on groups have been studied in, e.g., [28,29]. How-
ever, although notional applications of Lie theory to Markov models motivate
work on the stochastic group, actual applications themselves are few in number,
with [30] serving as an exemplar.
If we ignore considerations of analytical tractability and/or computational eﬃ-
ciency, we can consider generic MCMC algorithms that optimize some criterion
over the relevant monoid. Optimal control considerations lead to algorithms such
as those of [31–34] that optimize convergence while sacriﬁcing reversibility/de-
tailed balance. Meanwhile, [35–39] seek to optimize the asymptotic variance.
2.2
The Lie Group Generated by a Probability Measure
For 1 < n ∈N, let p be a probability measure on [n] := {1, . . . , n}. Relying on
context to resolve any ambiguity, we write 1 = (1, . . . , 1)T ∈Rn. Now following
[40–43], we deﬁne the stochastic group
STO(n) := {P ∈GL(n, R) : P1 = 1},
(1)
i.e., the stabilizer ﬁxing 1 on the left in GL(n, R), and
⟨p⟩:= {P ∈STO(n) : pP = p},
(2)
i.e., the stabilizer ﬁxing p on the right in STO(n). We call ⟨p⟩the group generated
by p. STO(n) and ⟨p⟩are both Lie groups, with respective dimensions n(n −1)
and (n−1)2. If P ∈STO(n) is irreducible and ergodic, then its unique invariant
measure is ⟨P⟩:= 1T (P −I +11T )−1. Now pP = p iﬀ⟨p⟩= ⟨⟨P⟩⟩, and ⟨p⟩−I ⊂
lie(⟨p⟩) ⊂lie(STO(n)).
For (j, k) ∈[n] × [n −1], write
e(j,k) := ej(eT
k −eT
n),
(3)

378
S. Huntsman
where
{ej}j∈[n]
is
the
standard
basis
of
Rn.
Now
the
matrices
{e(j,k)}(j,k)∈[n]×[n−1] form a basis of lie(STO(n)) and
e(j,k)e(ℓ,m) = ej(eT
k −eT
n)eℓ(eT
m −eT
n)
= (δkℓ−δnℓ)e(j,m),
(4)
so upon considering j ↔ℓ, k ↔m we have that

e(j,k), e(ℓ,m)

= (δkℓ−δnℓ)e(j,m) −(δmj −δnj)e(ℓ,k).
(5)
This basis has the obvious advantage of computationally trivial decompositions.
For j, k ∈[n −1], we set rj := pj/pn and
e(p)
(j,k) := e(j,k) −rje(n,k)
= (ej −rjen) (eT
k −eT
n).
(6)
If pj ≡Lj/Z, say with Lj ≡exp(−βEj), then rj = Lj/Ln does not
depend on Z at all. This is the basic reason why MCMC methods can
avoid grappling with normalization factors such as partition functions,
and in turn why MCMC methods are so useful.
For future reference, write r := (r1, . . . , rn−1, 1) and r−:= (r1, . . . , rn−1).
Lemma 1. For i ∈N,

e(p)
(j,k)
i
=

I,
i = 0;
(δjk + rj)i−1 e(p)
(j,k),
i > 0.
(7)
Proof. Using the rightmost expression in (6) and using j, k, ℓ, m ̸= n to simplify
the product of the innermost two factors, we obtain
e(p)
(j,k)e(p)
(ℓ,m) = (δkℓ+ rℓ) e(p)
(j,m).
(8)
Taking j = ℓand k = m establishes the result for i ≤2. The general case follows
by induction on i.
⊓⊔
Theorem 1. The e(p)
(j,k) form a basis for lie(⟨p⟩) and

e(p)
(j,k), e(p)
(ℓ,m)
	
= (δkℓ+ rℓ) e(p)
(j,m) −(δmj + rj) e(p)
(ℓ,k).
(9)
Proof. Note that pe(p)
(j,k) = (pj −rjpn)

eT
k −eT
n

≡0. Furthermore, linear inde-
pendence and the commutation relations are both obvious, so we need only show
that exp te(p)
(j,k) ∈⟨p⟩for all t ∈R. By Lemma 1,
exp te(p)
(j,k) = I + e(p)
(j,k)
∞

i=1
ti (δjk + rj)i−1
i!
= I + et(δjk+rj) −1
δjk + rj
e(p)
(j,k).
(10)

Sampling and Statistical Physics via Symmetry
379
For later convenience, we write
f (p)
(j,k)(t) := e−t(δjk+rj) −1
δjk + rj
.
(11)
2.3
The Positive Monoid of a Measure
Elements of STO(n) need not be bona ﬁde stochastic matrices because they
can have negative entries; on the other hand, stochastic matrices need not be
invertible. We therefore consider the monoids (i.e., semigroups with identity;
compare [44])
STO+(n) := {P ∈M(n, R) : P1 = 1 and P ≥0},
(12)
where P ≥0 is interpreted per entry, and
⟨p⟩+ := {P ∈STO+(n) : pP = p}.
(13)
Note that STO+(n) ̸⊂STO(n) and ⟨p⟩+ ̸⊂⟨p⟩, since the left hand sides contain
noninvertible elements. Also, STO+(n) and ⟨p⟩+ are bounded convex polytopes.
Lemma 2. If tj ≥0, then exp

−
j tje(p)
(j,j)

∈⟨p⟩+.
Proof. By hypothesis and (6), −
j tje(p)
(j,j) has nonpositive diagonal entries and
nonnegative oﬀ-diagonal entries; the result follows by regarding the sum as the
generator matrix of a continuous-time Markov process.
⊓⊔
In particular, for t ≥0 we have that
exp

−te(p)
(j,j)

= I + f (p)
(j,j)(t) · e(p)
(j,j) ∈⟨p⟩+,
(14)
where f (p)
(j,j) is as in (11). Unfortunately, aside from (14), Lemma 2 does not
give a convenient way to construct explicit elements of ⟨p⟩+ in closed form. This
situation is an analogue of the highly nontrivial quantum compilation problem
(see [45]).
Indeed, even if the sum in the lemma’s statement has only two terms, we are
immediately confronted with the formidable Zassenhaus formula (see [46]):
exp(t(X + Y )) = exp(tX) exp(tY )
∞

i=2
exp(tiCi),
where
C2 = −1
2[X, Y ];
C3 = 1
3[Y, [X, Y ]] + 1
6[X, [X, Y ]];
C4 = −1
8 ([Y, [Y, [X, Y ]]] + [Y, [X, [X, Y ]]]) −1
24[X, [X, [X, Y ]]],

380
S. Huntsman
and higher order terms have increasingly intricate structure. While a com-
puter algebra system can evaluate exp

−t(j,k)e(p)
(j,k) −t(ℓ,m)e(p)
(ℓ,m)

in closed
form, the results involve many pages of arithmetic for the case corresponding to
Lemma 2, and the other possibilities all yield some manifestly negative entries.
2.4
Barker and Metropolis Samplers
Although Lemma 2 oﬀers only a weak foothold for explicit analytical construc-
tions, we can still use (14) to produce a MCMC algorithm that is parametrized
by t.
Here and throughout our discussion of MCMC algorithms, we use a simple
trick of relabeling the current state as n and then reversing the relabeling,
so that the transition n →j becomes generic.
For P = exp

−te(p)
(j,j)

, we have Pjj = 1 + f (p)
(j,j)(t), Pjn = −f (p)
(j,j)(t), Pnj =
−f (p)
(j,j)(t)rj, and Pnn = 1 + f (p)
(j,j)(t)rj. In particular,
Pjn
Pnj
= 1
rj
= pn
pj
.
That is, detailed balance automatically holds.
The value of t that is optimal for convergence is t = ∞, since this maximizes
the oﬀ-diagonal terms. With this parameter choice, we obtain Pjj =
rj
1+rj , Pjn =
1
1+rj , Pnj =
rj
1+rj , and Pnn =
1
1+rj . The corresponding MCMC algorithm is the
so-called Barker sampler.
However, in light of (14), we can almost trivially improve on the Barker
sampler. We have that I −τe(p)
(j,j) ∈⟨p⟩+ iﬀ0 ≤τ ≤min(1, r−1
j ). But

I −min(1, r−1
j ) · e(p)
(j,j)

nj = min(1, rj)
(15)
is precisely the Metropolis acceptance ratio. In other words:
We have derived the Barker and Metropolis samplers from basic consider-
ations of symmetry and (in the latter case) optimality.
Note that the mechanism for proposing the state j is neither speciﬁed nor con-
strained by our construction. That is, our approach separates concerns between
proposal and acceptance mechanisms, and focuses only on the latter. However,
a good proposal mechanism is of paramount importance for MCMC algorithms.
These observations will continue to apply throughout our later discussion, though
in Sect. 2.7 we select the elements of proposal sets uniformly at random without
replacement for illustrative purposes.

Sampling and Statistical Physics via Symmetry
381
Algorithm 2. Metropolis
Input: Runtime T and oracle for r
Initialize t = 0 and X0
repeat
Relabel states so that Xt = n
Propose j ∈[n −1]
Accept Xt+1 = j with probability (15)
Undo relabeling; set t = t + 1
until t = T
Output: {Xt}T
t=0 ∼p×(T +1) (approximately)
2.5
Some Algebra
The Barker and Metropolis samplers are among the very “simplest” MCMC
algorithms in that (14) is among the very sparsest nontrivial matrices in ⟨p⟩+.
But if we consider possible transitions to more than one state, we can trade oﬀ
sparsity for both faster convergence and increased algorithm complexity. The
degenerate limiting case is the matrix 1p, and the practical starting case is the
Barker and Metropolis samplers. A central question for interpolating between
these cases is how (or if) we can systematically construct denser elements of ⟨p⟩+
than (14).
To answer this question, we ﬁrst generalize Lemma 1. For J := {j1, . . . , jd} ⊆
[n −1] and a matrix α ∈Mn−1,n−1, deﬁne α(J) ∈Md,d by (α(J))uv := αjujv,
α(p)
(J) := d
u,v=1 αjujve(p)
(ju,jv) ∈lie(⟨p⟩), and r(J) := (rj1, . . . , rjd).
Lemma 3. Let J := {j1, . . . , jd} ⊆[n −1]. If γ(p)
(J) = α(p)
(J)β(p)
(J), then
γ(J) = α(J)(I + 1r(J))β(J).
(16)
Proof
α(p)
(J)β(p)
(J) =

u,v,w,x
αjujvβjwjxe(p)
(ju,jv)e(p)
(jw,jx)
=

u,v,w,x
αjujv (δjvjw + rjw) βjwjxe(p)
(ju,jx)
=

u,x

α(J)(I + 1r(J))β(J)

ux e(p)
(ju,jx).
where the second equality follows from (8) and the third from bookkeeping.
⊓⊔
The heavy notation introduced for Lemma 3 is genuinely worthwhile: the
case d = 2 takes a page to write out by hand without it. More importantly,
we can readily construct an analytically convenient matrix in lie(⟨p⟩) using
Lemma 3.

382
S. Huntsman
Theorem 2. Let J := {j1, . . . , jd} ⊆[n −1], ω ∈R and
A(p;ω)
(J)
:= ω

u,v

δjujv −
1
1 + r(J)1rjv

e(p)
(ju,jv).
(17)
Then
exp tA(p;ω)
(J)
= I + eωt −1
ω
A(p;ω)
(J) .
(18)
Moreover, exp

−tA(p;ω)
(J)

∈⟨p⟩+ ∩GL(n, R) if t ≥0. In particular, the Barker
matrix
B(p)
(J) := I −A(p;1)
(J)
(19)
is in ⟨p⟩+.
Proof. The Sherman-Morrison formula [47] gives that
ω(I + 1r(J))−1 = ω

I −
1
1 + r(J)11r(J)

;
the elements of this matrix are exactly the coeﬃcients in (17). Using the nota-
tion introduced for the statement of Lemma 3, we can rewrite (17) as A(p;ω)
(J)
=

ω(I + 1r(J))−1(p)
(J), and invoking Lemma 3 itself yields

A(p;ω)
(J)
i+1
= ωiA(p;ω)
(J)
for i ∈N. The result now follows along lines similar to the proof of Theorem 1.⊓⊔
Let Δ denote the map that sends a matrix to the (column) vector of its
diagonal entries, and indicate the boundary of a subset of a topological space
using ∂.
Lemma 4. The Metropolis matrix
M(p)
(J) := I −
1
max Δ

A(p;1)
(J)
A(p;1)
(J)
(20)
is in ∂⟨p⟩+.
Proof. Writing A ≡A(p;1)
(J)
for the moment, the result follows from three basic
observations: Δ(A) ≥0, max Δ (A) > 0, and A −Δ (Δ (A)) ≤0.
⊓⊔
2.5.1
Example
To illustrate the Barker and Metropolis matrix constructions, consider J =
{1, 2, 3} and p = (1, 2, 3, 4, 10)/20. Now (17) is
A(p;ω)
(J)
= ω
16
⎛
⎜
⎜
⎜
⎜
⎝
15 −2 −3 0 −10
−1 14 −3 0 −10
−1 −2 13 0 −10
0
0
0 0
0
−1 −2 −3 0
6
⎞
⎟
⎟
⎟
⎟
⎠
.

Sampling and Statistical Physics via Symmetry
383
For ω = 1 and t = −log 2, (18) is
exp

log 2 · A(p;1)
(J)

= 1
32
⎛
⎜
⎜
⎜
⎜
⎝
17 2 3 0 10
1 18 3 0 10
1 2 19 0 10
0 0 0 32 0
1 2 3 0 26
⎞
⎟
⎟
⎟
⎟
⎠
.
whereas for ω = 2 and t = −log 2, (18) is
exp

log 2 · A(p;2)
(J)

= 1
64
⎛
⎜
⎜
⎜
⎜
⎝
19 6 9 0 30
3 22 9 0 30
3 6 25 0 30
0 0 0 64 0
3 6 9 0 46
⎞
⎟
⎟
⎟
⎟
⎠
.
Finally, (19) and (20) are respectively
B(p)
(J) = 1
16
⎛
⎜
⎜
⎜
⎜
⎝
1 2 3 0 10
1 2 3 0 10
1 2 3 0 10
0 0 0 16 0
1 2 3 0 10
⎞
⎟
⎟
⎟
⎟
⎠
;
M(p)
(J) = 1
15
⎛
⎜
⎜
⎜
⎜
⎝
0 2 3 0 10
1 1 3 0 10
1 2 2 0 10
0 0 0 15 0
1 2 3 0 9
⎞
⎟
⎟
⎟
⎟
⎠
.
2.6
Higher-Order Samplers
In order to obtain higher-order samplers from the algebra of Sect. 2.5, we use a
familiar trick, letting n →j ∈J correspond to a generic transition as in Sect.
2.4. (Again, we do not specify or constrain a mechanism for proposing a set J of
candidate states to transition into.) This immediately yields more sophisticated
MCMC algorithms using (19) and (20) which we call higher-order Barker and
Metropolis samplers, respectively abbreviated as HOBS and HOMS.
The corresponding matrix elements are straightforwardly obtained with a bit
of arithmetic:
1
ω

A(p;ω)
(J)

juju
= 1 −
rju
1 + r(J)1;
1
ω

A(p;ω)
(J)

nju
= −
rju
1 + r(J)1;
1
ω

A(p;ω)
(J)

nn =
r(J)1
1 + r(J)1,
(21)
which yields the HOBS:

B(p)
(J)

nju
=
rju
1 + r(J)1;

B(p)
(J)

nn =
1
1 + r(J)1.
(22)

384
S. Huntsman
Algorithm 3. HOMS
Input: Runtime T and oracle for r
Initialize t = 0 and X0
repeat
Relabel states so that Xt = n
Propose J = {j1, . . . , jd} ⊆[n −1]
Accept Xt+1 = ju with probability (23)
Undo relabeling; set t = t + 1
until t = T
Output: {Xt}T
t=0 ∼p×(T +1) (approximately)
Meanwhile,
1
ω max Δ

A(p;ω)
(J)

= 1 + r(J)1 −min{1, min r(J)}
1 + r(J)1
yielding the HOMS:

M(p)
(J)

nju
=
rju
1 + r(J)1 −min{1, min r(J)};

M(p)
(J)

nn = 1 −
r(J)1
1 + r(J)1 −min{1, min r(J)}.
(23)
It turns out that the HOBS is equivalent to the ensemble MCMC algorithm
of [8] as described in [17,18]. The proposal mechanism we use for the HOBS in
Sect. 2.7 essentially amounts to the independent ensemble MCMC sampler (apart
from non-replacement, which technically induces jointness), but in general this
is not the case. A more sophisticated proposal mechanism that can exploit any
joint structure in the target distribution would be more powerful, but we reiterate
that our approach is completely agnostic to proposal mechanism details.
In contrast, the HOMS is diﬀerent than a multiple-try Metropolis sampler
(MTMS), including the independent MTMS described in [17]. The HOMS uses
a sample from J ∪{n} to perform a state transition in a single step according
to (23), whereas a MTMS ﬁrst samples from J before accepting or rejecting
the result. The HOMS (and for that matter, also the HOBS) actually turns out
to be a slightly special case of a construction in §2.3 of [9]. This work uses a
“proposition kernel” deﬁned by assigning a probability distribution on the power
set 2[n] of the state space [n] to each element of the state space. Essentially,
the HOMS and HOBS result if this distribution on 2[n] is independent of the
individual element (i.e., it varies only with the subset).
2.7
Behavior of Higher-Order Samplers
The diﬀerence between the HOBS and HOMS decreases as d = |J| increases
and/or p becomes less uniform (e.g., in a low-temperature limit), since in either
limit we have min{1, min r(J)} ≪1+r(J)1. Although one might hope to gain the

Sampling and Statistical Physics via Symmetry
385
most beneﬁt from improved MCMC algorithms in such situations, the HOMS
can still provide a comparative advantage for d > 1 but small, with elements
chosen in complementary ways (uniformly at random, near current/previous
states, etc.), or in e.g. the high-temperature part of a parallel tempering scheme
[48].
We use the example of a small Sherrington-Kirkpatrick (SK) spin glass [49,50]
to exhibit the behavior of the HOBS and HOMS in Figs. 1 and 2. The SK spin
glass is the distribution
p(s) := Z−1 exp

−β
√
N

jk Jjksjsk

(24)
over spins s ∈{±1}N, where J is a symmetric N × N matrix with independent
identically distributed standard Gaussian entries and β is the inverse tempera-
ture.
Fig. 1. Total variation distance between the HOBS/HOMS with proposal sets J (ele-
ments distributed uniformly without replacement) of varying sizes d and (24) with 9
spins and β = 1/4. Inset: the same data and window, with horizontal axis normalized
by d.

386
S. Huntsman
Fig. 2. As in Fig. 1 with β = 1.
The SK model is well-suited for a straightforward evaluation of higher-order
samplers because of its disordered energy landscape. More detailed models or
benchmarks seem to require speciﬁc assumptions (e.g., the particular form of a
spin Hamiltonian for Swendsen-Wang updates) and/or parameters (e.g., addi-
tional temperatures for parallel tempering, or of a vorticity matrix for non-
reversible Metropolis-Hastings). In keeping with a straightforward evaluation,
we do not consider sophisticated or diverse ways to generate elements of proposal
sets J. Instead, we simply select elements of J uniformly at random without
replacement. We use the same pseudorandom number generator initial state for
all simulations in order to highlight relative behavior. Finally, we choose β low
enough (1/4 and 1) so that the behavior of a single run is suﬃciently represen-
tative to make simple qualitative judgments.
The ﬁgure insets show that although higher-order samplers indeed converge
more quickly, this comes at the cost of more overall evaluations of probability
ratios. Parallelism is therefore necessary for higher-order samplers to be a wise
algorithmic choice.
We reiterate in closing this section that the HOMS gives results very close to
the HOBS, except for small values of d or a more uniform target distribution p.

Sampling and Statistical Physics via Symmetry
387
Increasing the number of spins in the SK model and/or considering an Edwards-
Anderson spin glass also yields qualitatively similar results (not shown here).
2.8
Linear Objectives for Transition Matrices
We can push the preceding ideas further by using an optimization scheme
to construct transition matrices with the desired invariant measures and
that saturate a suitable objective function.
For example, the linear objective −1T
Jτ (p)
(J)rT
J considered immediately after (29)
yields an optimal sparse approximation of the “ultimate” transition matrix 1p.
(To the best of our knowledge, this construction has not been considered else-
where.) However, bringing an optimization scheme to bear narrows the regime of
applicability to cases where computing likelihoods is hard enough and suﬃcient
parallel resources are available to justify the added computational costs.
To make this concrete, ﬁrst deﬁne 1J ∈Rn by
(1J)j :=

1
if j ∈J ∪{n}
0
otherwise,
1−
J := ((1J)1, . . . , (1J)n−1)T , rJ := r ⊙1T
J, and r−
J := r−⊙(1−
J)T , where ⊙is
the entrywise or Hadamard product (note that rJ ∈Rn, while r(J) ∈R|J| has
been deﬁned previously).
Write Δ for the matrix diagonal map and recall the notation of Lemma 3:
since
τ (p)
(J) =
In−1
−r−
J

τ

In−1 −1−
J

,
(25)
we have that I −τ (p)
(J) ∈⟨p⟩+ iﬀ
0 ≤In−1 −Δ(1−
J)τΔ(1−
J) ≤1;
(26a)
0 ≤τ1−
J ≤1;
(26b)
0 ≤r−
J τ ≤1;
(26c)
0 ≤r−
J τ1−
J ≤1.
(26d)
The constraints (26b)–(26d) respectively force the ﬁrst n −1 entries of the last
column, the ﬁrst n −1 entries of the last row, and the bottom right matrix
entry of τ (p)
(J) to be in the unit interval. (26a) forces the relevant entries of the
“coeﬃcient matrix” τ (as an upper left submatrix of τ (p)
(J)) to be in the unit
interval.
We can conveniently set to zero the irrelevant/unspeciﬁed rows and columns
of τ that do not contribute to τ (p)
(J) via the constraints
Δ(1 −1−
J)τ = τΔ(1 −1−
J) = 0.
(27)

388
S. Huntsman
Provided that we impose (27), (26a) can be replaced with
0 ≤In−1 −τ ≤1.
(28)
The “diagonal” case corresponding to Lemma 2 shows that (26) and (27)
jointly have nontrivial solutions. This suggests that we consider suitable objec-
tives and corresponding linear programs for optimizing the MCMC transition
matrix I −τ (p)
(J). We therefore introduce the vectorization map vec that sends
a matrix to a vector by stacking matrix columns in order. This map obeys the
useful identity vec(XY ZT ) = (Z ⊗X)vec(Y ), where ⊗denotes the Kronecker
product.
Now a reasonably generic linear objective function is
xT τ (p)
(J)y = (yT ⊗xT )vec

τ (p)
(J)

(29)
for suitable ﬁxed x and y. In practice, we consider x = 1J and y = −rT
J. This
maximizes the Frobenius inner product of I −τ (p)
(J) and 1JrJ because
Tr

I −τ (p)
(J)
T
1JrJ

= rJ1J −1T
Jτ (p)
(J)rT
J.
Alternatives like x = en, y = en (to discourage self-transitions) can lead to
convergence that slows catastrophically as d = |J| increases, because high-
probability states are less likely to remain occupied. More surprisingly, the same
sort of slowing down happens for x = en, y = −rT
J, as well as for variations
involving the nth component of y. We suspect that the cause is the same, albeit
mediated indirectly through an objective that “overﬁts” the proposed transition
probabilities to the detriment of remaining in place (or in some cases “underﬁts”
by producing the identity matrix). Overall, it appears nontrivial to select better
choices for x and y than our defaults above.
By (25) we get
vec

τ (p)
(J)

=
 In−1
−(1−
J)T

⊗
In−1
−r−
J

vec(τ),
(30)
and in turn (yT ⊗xT )vec

τ (p)
(J)

equals

yT
 In−1
−(1−
J)T

⊗xT
In−1
−r−
J

vec(τ).
(31)
Now the constraints and the objective of the linear program are both explic-
itly speciﬁed in terms of the “coeﬃcient” matrix τ, so in principle we have a
working algorithm already. However, it is convenient to respectively rephrase
the constraints (26b)–(26d), (27), and (28) into diﬀerent forms as
0 ≤
⎛
⎜
⎝

1−
J
T ⊗In−1
In−1 ⊗r−
J

1−
J
T ⊗r−
J
⎞
⎟
⎠vec(τ) ≤1,
(32)

Sampling and Statistical Physics via Symmetry
389
In−1 ⊗Δ(1 −1−
J)
Δ(1 −1−
J) ⊗In−1

vec(τ) = 0,
(33)
vec(In−1) −1 ≤vec(τ) ≤vec(In−1).
(34)
Therefore, writing
U (p)
(J) :=

I2n−1
−I2n−1

⎛
⎜
⎝

1−
J
T ⊗In−1
In−1 ⊗r−
J

1−
J
T ⊗r−
J
⎞
⎟
⎠;
v :=

12n−1
02n−1

;
w(p)
(J) := −yT
 In−1
−(1−
J)T

⊗xT
In−1
−r−
J

,
and
U (0)
(J) :=
In−1 ⊗Δ(1 −1−
J)
Δ(1 −1−
J) ⊗In−1

,
(35)
we can at last write the sought-after linear program (noting a minus sign included
in w(p)
(J)) in a form suitable for (e.g.) MATLAB’s linprog solver:
min
τ
w(p)
(J)vec(τ)
s.t.
U (p)
(J)vec(τ)
≤
v;
(36a)
U (0)
(J)vec(τ)
=
0;
(36b)
vec(τ)
≥
vec(In−1) −1;
(36c)
vec(τ)
≤
vec(In−1).
(36d)
The preceding discussion therefore culminates in the following
Theorem 3. The linear program (36) has a solution in ⟨p⟩+.
2.8.1
Example
As in Sect. 2.5.1, consider p = (1, 2, 3, 4, 10)/20 and J = {1, 2, 3}. Solving the
linear program with x = 1J and y = −rT
J produces the following element of
⟨p⟩+:
⎛
⎜
⎜
⎜
⎜
⎝
0
0
0 0 1
0
0
0 0 1
0
0
0 0 1
0
0
0 1 0
0.1 0.2 0.3 0 0.4
⎞
⎟
⎟
⎟
⎟
⎠
.
For comparison, recall that the last row of M(p)
(J) equals (0.0¯6, 0.1¯3, 0.2, 0, 0.6).

390
S. Huntsman
2.8.2
The Higher-Order Programming Sampler
We call the sampler obtained from (29) and (36) with x = −1J and y = rT
J the
higher-order programming sampler (HOPS). We compare the HOMS and HOPS
in Figs. 3 and 4 (cf. Figs. 1 and 2). The ﬁgures show that the HOPS improves
upon the HOMS, which in turn improves upon the HOBS.
Fig. 3. Total variation distance between the HOPS/HOMS with proposal sets J (ele-
ments sampled uniformly without replacement) of varying sizes d and (24) with 9 spins
and β = 1/4. Inset: same data and window, with horizontal axis normalized by d.
2.9
Remarks on Sampling
Besides providing a framework that conceptually uniﬁes various MCMC algo-
rithms, symmetry principles lead to the apparently new HOPS algorithm of
Sect. 2.8. It is possible that the HOPS itself might be further improved upon
by developing an objective function suited for, e.g. convex optimization versus
a mere linear program. These ideas might also enhance existing MCMC tech-
niques speciﬁcally tailored for parallel computation, as in [51]. In particular, the
Bayesian approach to inverse problems [52] may be fertile ground for applica-
tions.

Sampling and Statistical Physics via Symmetry
391
Fig. 4. As in Fig. 3 with β = 1.
As we have already mentioned, our approach is agnostic with respect to
proposals, focusing purely on acceptance mechanisms. However, the proposal
mechanism has less impact than the acceptance mechanism in practice, especially
for diﬀerentiable distributions. In practice, a stateful and/or problem-speciﬁc
proposal exploiting joint structure is highly desirable and even necessary for
any real utility, but we these avenues unexplored for now (one possibility is
suggested by particle MTMS algorithms as in [53] and exploiting tensor product
structure in transition matrices and ⟨p⟩). It would be of interest to incorporate
some aspect of a proposal mechanism into the objective of (36), but it is not
clear how to actually do this. In fact, our numerical example featured a SK
spin glass to illustrate our ideas precisely because its highly disordered structure
(and discrete state space) are suited for separating concerns about proposal and
acceptance mechanisms.
It would certainly be interesting to extend the present considerations to con-
tinuous variables. However, this would seem to require a more technical treat-
ment, since inﬁnite-dimensional Lie theory, distributions `a la Schwartz, etc.
would play a role at least in principle. In a complementary vein, it would be
interesting to see if the full construction of [9] could be recovered from symme-
try arguments alone.

392
S. Huntsman
Algorithm 4. HOPS
Input: Runtime T and oracle for r
Initialize t = 0 and X0
repeat
Relabel states so that Xt = n
Propose J = {j1, . . . , jd} ⊆[n −1]
Compute τ solving (36) with x = 1J and y = −rT
J
Set P = I −τ (p)
(J) using (25)
Accept Xt+1 = ju with probability Pnju
Undo relabeling; set t = t + 1
until t = T
Output: {Xt}T
t=0 ∼p×(T +1) (approximately)
While the Barker and Metropolis samplers are reversible, it is not clear if
the HOPS is, though [33] points out ways to transform reversible kernels into
irreversible ones and vice versa.
It is possible to produce transition matrices (even in closed form) in which
the nth row is nonnegative but other rows have negative entries. It is not imme-
diately clear if using such a matrix actually poisons a MCMC algorithm. Though
preliminary experiments in this direction were discouraging, we have not found
a compelling argument that rules out the use of such matrices.
Finally, it would be of interest to sample from the vertices of the polytope
⟨p⟩+. However, (even approximately) uniformly sampling vertices of a polytope
is NP-hard (and thus presumably intractable) by Theorem 1 of [54]: see also
[55].
3
Statistical Physics via Symmetry
We have seen in Sect. 2 that sampling algorithms can be better understood in
principle and also accelerated in practice through elementary considerations of
symmetry. In the present section, we show how similarly basic considerations
of symmetry can derive the basic structure of statistical physics. While we do
not address entropy per se, that ground is well-traveled, with the well-known
characterization of Faddeev [56,57] playing an exemplary role.
We focus instead on the role of temperature (and via closure of the Gibbs
relation, energy), which classical information-theoretical considerations have not
substantially accounted for. In particular, we sketch how an eﬀective temperature
can reproduce the physical temperature for conjecturally generic model systems
(see also Sect. 4), while also enabling applications to data analytics, characteriza-
tion of time-inhomogeneous Markov processes, nonequilibrium thermodynamics,
etc.
The goal of providing a self-consistent description of stationary systems with
ﬁnitely many states using the language of equilibrium statistical physics in the
canonical ensemble naturally ﬂows from the idea expressed in [58] that “there
is no conceptual diﬀerence between stationary states in equilibrium and out

Sampling and Statistical Physics via Symmetry
393
of equilibrium.” While the traditional aim of statistical physics is predicting
statistical behavior in terms of measured physical properties, the aim here is
to go in the other direction: that is, to determine eﬀective physical properties–
in and out of equilibrium–in terms of observable statistical behavior. In other
words, the goal is to take one step farther the now-classical maximum entropy
point of view in which statistical physics is a framework for reasoning about
data.
We realize this goal by demonstrating the existence, uniqueness (up to a
choice of scale), and relevance of a physically reasonable and invertible transfor-
mation between simple eﬀective statistical and physical descriptions of a system
(see Fig. 5). The eﬀective statistical description is furnished by a probability
distribution along with a characteristic timescale. The eﬀective physical descrip-
tion consists of an eﬀective energy function and an eﬀective temperature. 3 The
transformation between these descriptions will be derived from the elementary
Gibbs relation and basic symmetry considerations along lines ﬁrst explored in
[67,68].
The utility and naturalness of the eﬀective physical description that results
from performing this transformation on an eﬀective statistical description will
depend entirely on the utility and naturalness of the underlying state space and
of the characteristic timescale. In the event that the actual state space of a real
physical system in thermal equilibrium is ﬁnite and an appropriate characteristic
timescale can be determined, the corresponding eﬀective physical description
will manifestly reproduce the actual physics. Moreover, in near-equilibrium, the
eﬀective temperature and energies will remain near the actual values of their
equilibrium analogues by a continuity argument. Consequently, the framework
discussed here may inform principled characterizations of quasi-equilibria.
However, as the system is driven away from equilibrium, its eﬀective energy
levels will shift, while the actual energy levels of a real physical system may be
ﬁxed and intrinsic. Nevertheless, such shifts are still of interest for characterizing
nonequilibrium situations, even for real physical systems. For example, a system
such as a laser undergoing population inversion will exhibit an eﬀective level
crossing as the driving parameter varies. In a related vein, a negative absolute
temperature [69–71] would correspond in our framework to a negative charac-
teristic timescale, indicating antithermodynamic behavior such as “antimixing”
or “antirelaxation.”
Even very limited knowledge about the energy levels and temperature of a
system is suﬃcient to determine the remainder of that information as a trivial
exercise in algebra using the Gibbs relation. Nevertheless, the preceding dis-
cussion should not distract from the observation that the framework discussed
3 The use of an eﬀective temperature in glassy systems has a long history [59–61]
and has recently gained prominence through the ﬂuctuation-dissipation (FD) tem-
perature in mean-ﬁeld systems [62,63]. Discussions of the relationship between our
construction and both the FD temperature (frequently called “the” eﬀective temper-
ature in the literature) and the dynamical temperature introduced by Rugh [64,65]
can be found in [66].

394
S. Huntsman
here provides its most substantial advantage in the situation where inverting
the Gibbs relation might initially seem like an ill-posed problem. Therefore,
the primary goal of the framework discussed below is to give eﬀective physical
descriptions of systems that have no a priori physical characterization, while
maintaining total consistency with equilibrium statistical physics in situations
where an a priori physical characterization is available.
Highlighting this consistency is the example of Anosov systems (see Sect. 4),
and speciﬁcally paradigmatic chaotic model systems such as the cat map and the
free particle or ideal gas on a surface of constant negative curvature, where using
a careful iterative discretization scheme indicates how the actual energy and
temperature may be reproduced by suitable eﬀective analogues, despite the fact
that the underlying state spaces are continuous. Thermostatting subsequently
indicates how the transformation at the heart of our discussion could be applied
in principle to essentially arbitrary physical systems [66].
While the perspective we shall take below does not confer extensive predic-
tive power in the realm of physics, it does have some (see, e.g. Sect. 3.8.1) and its
descriptive and explanatory power nevertheless suggests a wide and signiﬁcant
scope for applications, including to nonequilibrium statistical physics, the renor-
malization group, information theory, and the characterization of both stochas-
tic processes and experimental data. More provocatively, it can be regarded as
illuminating the fundamental meaning of both energy and temperature indepen-
dently of references to work, force, mass, or the underlying spatial context upon
which the latter concepts ultimately depend for their deﬁnition.
In this section, we derive the Gibbs relation from symmetry considerations
in Sect. 3.1 before introducing the coordinate systems that respectively underlie
experimental/probabilistic and theoretical/physical descriptions of systems in
Sect. 3.2. With the stage set, we perform some preliminary algebra in Sect. 3.3.
After obtaining intermediate results on the scaling behavior of inverse temper-
ature as a function of time in Sect. 3.4 and on the geometry of any reasonable
transformation between the two descriptions above in Sect. 3.5, we complete the
derivation of the eﬀective temperature in Sect. 3.6. We then outline constraints
on the form of a characteristic timescale imposed by considering product systems
in Sect. 3.7. Finally, we outline a number of examples and applications in Sect.
3.8 before remarks in Sect. 3.9.
Later, Sect. 4 considers the eﬀective temperature for Anosov systems.
At times, we may write β to denote the physical or actual inverse temper-
ature as well as an eﬀective analogue. Context should serve to eliminate
any ambiguity, especially as we make an eﬀort to separate discussion of
these two quantities impinging on equations.

Sampling and Statistical Physics via Symmetry
395
3.1
The Gibbs Distribution
The ﬁrst step in deriving the basic structure of statistical physics from symmetry
is to derive the Gibbs relation between state probabilities and energies. We do
this for a ﬁnite system from the basic postulate that the probability of a state
depends only on its energy.4 This derivation will implicitly motivate the con-
struction of the eﬀective temperature that culminates in Sect. 3.6. While unlike
more classical derivations ours does not motivate the introduction of entropy, the
standard information-theoretic motivation provides a more than adequate rem-
edy, and the Faddeev characterization of entropy is also a symmetry argument
[56,57].
The key observation is that energy is only deﬁned up to an additive con-
stant, i.e., only energy diﬀerences are physically meaningful. This and the basic
postulate that state probabilities are functions of state energies together imply
that
P(Ek) =
f(Ek)

j f(Ej) =
f(Ek + ε)

j f(Ej + ε)
(37)
for some function f and ε arbitrary. Deﬁne
gE(ε) :=

j f(Ej + ε)

j f(Ej)
(38)
and note that gE(0) = 1 by deﬁnition. It follows that
P(Ek) =
f(Ek)

j f(Ej + ε)gE(ε) =
f(Ek + ε)

j f(Ej + ε).
(39)
Therefore gE(ε)f(Ek) = f(Ek + ε), implying that
f(Ek + ε) −f(Ek) = (gE(ε) −1) · f(Ek).
(40)
Since gE(0) = 1, we obtain f ′(Ek) = g′
E(0)f(Ek), and in turn
f(Ek) = C exp(g′
E(0)Ek).
(41)
4 In a similar if slightly less parsimonious vein, Blake Stacey has pointed out that
the Gibbs distribution can be derived “based on the idea that if [two systems]
A and B are at the same temperature, a noninteracting composite system AB
[formed from A and B] is also at that temperature. Suppose that Ej is an energy
level of system A and Ek is an energy level of B. Then, if there is no interaction
between the two systems, AB will have an energy level Ej + Ek. If we assume
that for all systems prepared at temperature T, P(En) =
1
Z f(En), then we have
f(Ej)f(Ek)ZAZB = f(Ej + Ek)ZAB. But we have the freedom to adjust f by an
overall multiplicative constant, since the meaningful quantities are the probabilities
and any prefactor will cancel when we divide by the partition function. So, we can
declare f(0) = 1, which yields ZAZB = ZAB and thus f(Ej + Ek) = f(Ej)f(Ek).
And this is just Cauchy’s functional equation for the exponential. So, provided that f
is continuous at even a single point, then f(E) = e−βE, where the ‘coolness’ β labels
the equivalence classes of thermal equilibrium.” See https://golem.ph.utexas.edu/
category/2020/06/getting to the bottom of noeth.html, Accessed 1 October 2020.

396
S. Huntsman
Without loss of generality, we can set β := −g′
E(0) and C ≡1, which
produces the Gibbs distribution so long as the temperature is deﬁned as
β−1.
We note that the present derivation can be made rigorous (e.g., details involv-
ing continuity and the Cauchy functional equation) without substantial diﬃculty,
but also without substantive additional insight. Also, gE(ε) = exp(−βε) so that
gE ≡g, as required for the self-consistency of the argument. Although the present
derivation is only appropriate for β ﬁxed, this just amounts to considering the
canonical ensemble in the ﬁrst place.
Finally, we reiterate that there are just a handful of symmetry and scaling
principles collectively underlying the present derivation and that of the eﬀec-
tive temperature below. In concert with the standard information-theoretical
infrastructure for entropy, these principles provide an exceptionally parsimo-
nious framework for the equilibrium statistical physics of ﬁnite systems.
3.2
Statistical and Physical System Descriptions
Consider now a stationary system with state space [n]. For our purposes, a
suﬃcient statistical description of such a system is provided by the (n + 1)-
tuple (p1, . . . , pn, t∞) = (p, t∞), where pj := P(s(t) = j) is the probability for
the system to be in state j ∈[n], and where t∞is a suitable characteristic or
“eﬀective” timescale.5,6 Deﬁning tj := t∞pj, the n-tuple t := t∞p = (t1, . . . , tn)
provides an alternative but completely equivalent description of the system, since
the probability constraint 
j pj = 1 implies that t∞= 
j tj. We will use both
of these descriptions interchangeably below without further comment.
Meanwhile, a suﬃcient physical description of the system is provided by
the (n + 1)-tuple H := (E1, . . . , En, β−1) = (E, β−1), where Ej is an eﬀective
energy for state j, and where β is an eﬀective inverse temperature. It will also
be convenient to introduce γ := βE, noting that βH = (γ, 1).
Below, we will construct well-deﬁned and essentially unique physically rea-
sonable and mutually inverse maps (see Fig. 5)
FH(t) = H,
Ft(H) = t.
(42)
The map Ft extends the familiar Gibbs relation (44), and the relationship
between t∞and β plays a pivotal role in the construction of both FH and Ft. In
particular, we will determine β as a function of t in (66), whereupon the Gibbs
relation and equation (43) for the reference energy will complete the detailed
speciﬁcation of FH.
5 For technical reasons we will impose the nondegeneracy requirement pj > 0 through-
out our discussion.
6 As we shall see, it turns out that physical considerations constrain t∞to share many
of the features of a mixing time or inverse energy gap (i.e., a relaxation time).

Sampling and Statistical Physics via Symmetry
397
Fig. 5. Geometry of the maps (42) for n = 2 states. Level curves of β−1 = 1, 2 (solid
contours) and of t∞= 1,
√
2 (dashed contours) are shown in both coordinate systems.
The actions of the maps are also shown explicitly for circular arcs and rays.
Because adding an arbitrary constant to the eﬀective state energies merely
amounts to a shift of a potential with no physical relevance, it is convenient to
specify a reference energy, at least temporarily. With the preceding considera-
tions in mind, and without any loss of generality, we impose the constraint7
1
n

j
Ej = 0.
(43)
Note that we may later enforce any other convenient reference energy, e.g.,
minj Ej ≡0, n−1 
j Ej = β−1, etc.
3.3
Preliminary Algebra
For systems in thermal equilibrium, it is natural to require that β is the inverse
of the physical temperature, i.e., that the eﬀective and physical temperatures
coincide. In this case the fundamental principle of equilibrium statistical physics
embodied by the Gibbs relation may be expressed as
pk = Z−1e−γk
(44)
and regarded as a map p = Fp(H). Here as usual Z := 
j e−γj is the partition
function.
By provisionally ignoring whether or not a generic stationary system is actu-
ally in thermal equilibrium, (44) can be viewed as a constraint linking its physical
and statistical descriptions. We will justify this interpretation below by using
7 NB. This does not entail a speciﬁcation of the internal energy (or any other physically
meaningful quantity) `a la Jaynes [72].

398
S. Huntsman
elementary symmetries and scaling relationships to specify (up to an overall
constant) the inverse FH of an augmentation Ft of the Gibbs map Fp.
Taking logarithms on both sides of (44) yields
−log Z −γk = log pk.
(45)
Meanwhile, the constraint (43) implies that n−1 
j γj = 0. Combining this
observation with arithmetic averaging of both sides of (45) leads to the result
−log Z = 1
n

j
log pj.
(46)
Substituting (46) into (45) and solving for γk shows that
γk = 1
n

j
log pj −log pk.
(47)
Since βFH(t) = (γ, 1),
∥βFH(t)∥2 = ∥γ∥2 + 1,
(48)
where ∥·∥denotes the usual Euclidean norm. That is, ∥βFH(t)∥=

∥γ∥2 + 1
can be explicitly computed in terms of γ (and by (47) also in terms of p) alone.
Therefore, in order to determine β, it remains chieﬂy to determine ∥FH(t)∥,
since ∥βFH(t)∥is known from (48) and we tautologically have that
β = ∥βFH(t)∥/∥FH(t)∥.
(49)
To determine ∥FH(t)∥, we will establish two results on scaling and geometry
next.
3.4
A Scaling Result
Dimensional considerations imply that if β is determined by any well-behaved
map FH, then it must depend on some constant governing parameter S in
addition to t. That is, β = f(t; S). By the Buckingham Π-theorem [73,74],
β = Sξtω
∞Ψ(p) for some ξ and ω, where Ψ is dimensionless.
Consider for the moment a system governed by a Hamiltonian H(X, P). If
C is a constant, the transformation t →t′ := t/C induces the transformation
t∞→t′
∞:= t∞/C as well as the extended canonical (pure scale) transformation
[75]
X →X′ = X,
P →P ′ = CP,
H →H′ = CH.
(50)
Since the transformation (50) can be considered as a change of units, it
necessarily leaves the actual (vs. eﬀective) Gibbs factor e−βH invariant. That is,
βH = β′H′ = β′CH. This observation immediately yields that β →β′ = β/C.
Physical consistency therefore demands that
FH(t/C) = C · FH(t).
(51)
From this, it follows that ω = 1, so without loss of generality
β = S−1t∞Ψ(p),
(52)
where the constant S carries units of action (say, S = ℏ).

Sampling and Statistical Physics via Symmetry
399
3.4.1
Additional Arguments in Support of (51) and (52)
A reader fully convinced by the argument just above can safely skip this section.
Ideal Gas Systems
Consider a Gedankenexperiment with two systems, comprised respectively of
ﬁnite ideal gas samples with particle masses m and m′ = m/C, each in identical
freefalling containers in contact with isotropic thermal baths, and with the same
initial conditions in phase space. Let P = P ′ denote the common rms momen-
tum of particles in both systems: the respective inverse temperatures of the two
systems are then in common proportion to m/P 2 and m′/P ′2 = (m/C)/P 2.
Insofar as the system microstates are not of interest in equilibrium, the sys-
tems may be respectively described by, e.g. the quintuples (m, v, P, t∞, β) and
(m′, v′, P ′, t′
∞, β′) = (m/C, Cv, P, t∞/C, β/C), where v(·) denotes a rms velocity
and here t(·)
∞denotes any characteristic timescale of the same nature in both
systems.
Both systems follow the same trajectory through phase space, albeit at rates
that diﬀer by constant factors, and we see that β scales as t∞for ideal gases, and
hence (by coupling with an ideal gas bath) for general systems also. Therefore,
consistency with elementary equilibrium statistical physics requires that β also
scales as t∞.
The Classical KMS Condition
Another argument along similar lines to that in Sect. 3.4 for the scaling behavior
of β w/r/t t∞directly invokes the classical Kubo-Martin-Schwinger (KMS) con-
dition. To begin, we recall the usual (quantum) KMS condition before formally
deriving its classical analogue in the limit ℏ→0 by way of background.
A quantum Hamiltonian ˆ
H has thermal density matrix
ˆρ := Z−1e−β ˆ
H,
(53)
where Z := Tr(e−β ˆ
H), and the time evolution of an observable ˆA in the Heisen-
berg picture is given as usual by τt( ˆA) := ei ˆ
Ht/ℏˆAe−i ˆ
Ht/ℏ.
The quantum Gibbs rule ⟨ˆA⟩= Tr(ˆρ ˆA), with ˆρ given by (53), is generalized
by the KMS condition [76,77]

τt( ˆA) ˆB

=

ˆBτt+iℏβ( ˆA)

.
(54)
For convenience, we recall a formal derivation of (54) from the Gibbs rule and
the cyclic property of the trace:

τt( ˆA) ˆB

= Z−1Tr(e−β ˆ
Hei ˆ
Ht/ℏˆAe−i ˆ
Ht/ℏˆB)
= Z−1Tr( ˆBei ˆ
Hz/ℏˆAe−i ˆ
Ht/ℏ)
= Z−1Tr( ˆBei ˆ
Hz/ℏˆAe−i ˆ
Hz/ℏe−β ˆ
H)
=

ˆBτz( ˆA)


400
S. Huntsman
where here we have written z := t + iℏβ.
Following [76], we have by (54) the following precursor to the classical KMS
condition:

[τt( ˆA), ˆB]
iℏ

=

ˆB

τz( ˆA) −τt( ˆA)
iℏ

.
(55)
Recall that as ℏ→0, τt( ˆA), ˆB and [τt( ˆA), ˆB]/iℏrespectively correspond to
or “undeform” into classical analogues A, B and {A, B}, where A has an implicit
time dependence (i.e., ∂tA = 0 ̸≡dA/dt) and B does not (i.e., B is evaluated at
t = 0).
Now (via an implicit assumption about the analyticity of τz which forms the
actual substance of the KMS condition) we have that
lim
ℏ→0
τz( ˆA) −τt( ˆA)
iℏ
= β dA
dt = β{A, H}
(56)
where H(X, P) is the classical Hamiltonian. Therefore in the limit ℏ→0, (55)
formally becomes the classical KMS condition (see also [77])
⟨{A, B}⟩= β ⟨B{A, H}⟩.
(57)
As in Sect. 3.4.1, here let t(·)
∞denote any characteristic timescale of the sys-
tem. Dilating the dynamical rate by a constant factor C has the eﬀect that t∞→
t′
∞= t∞/C and also induces the extended canonical (pure scale) transformation
(50). It follows that ∂X = ∂X′ and ∂P = C∂P ′, whence {A, B} = C{A, B}′ and
{A, H} = C{A, C−1H}′ = {A, H′}′ (here {·, ·}′ denotes the Poisson bracket
w/r/t (X′, P ′)). Along with (57), this in turn gives that
β =
⟨{A, B}⟩
⟨B{A, H}⟩= ⟨C{A, B}′⟩
⟨B{A, H′}′⟩= Cβ′.
(58)
Therefore β′ = β/C and we see once more that β scales as any characteris-
tic time t∞. Again, consistency with traditional equilibrium statistical physics
dictates that an eﬀective inverse temperature should also scale as t∞.
Thermal Time Hypothesis
The one-parameter modular group of ˆρ (as deﬁned in (53)) that appears in the
Tomita-Takesaki theory of von Neumann algebras [78] can be shown to coincide
with the time evolution group [79]: if s is the modular parameter and t is the
physical time, then
t = ℏβs.
(59)
In particular, s does not depend on β.8
8 While time evolution for von Neumann algebras is only of direct interest in the
inﬁnite-dimensional setting, its signiﬁcance for the present context is nevertheless
readily apparent.

Sampling and Statistical Physics via Symmetry
401
The thermal time hypothesis (TTH) articulated by Connes and Rovelli [79]
(see also [80–83]) states that physical time is determined by the modular group,
which is in turn determined by the state.
Besides implying Hamiltonian mechanics, the TTH simultaneously inverts
and generalizes the KMS condition (see Sect. 3.4.1) and hence also the Gibbs
relation (44), with temperature providing the physical link between time evolu-
tion and equilibria. But its key implication here is (59), by which β scales as any
characteristic time t∞; as before consistency demands the same scaling behavior
for an eﬀective inverse temperature.
Counterarguments for Alternative Scaling Behavior
Despite the scaling arguments presented above, we might nevertheless feel com-
pelled to consider alternative scaling behavior, with an eﬀective inverse temper-
ature of the form ∥t∥ω
∥γ∥2 + 1. However, for ω ̸= 1 this quantity does not
converge in a natural way for archetypal Anosov systems (see Sect. 4), nor by
extension does it appear to be relevant to the example of a two-dimensional
ideal gas. Furthermore, its physical relevance for a single Glauber-Ising spin (see
Sect. 3.8.1) is dubious for ω ̸= 1. Such behavior can be viewed as providing addi-
tional (albeit more circumstantial) evidence for an eﬀective inverse temperature
scaling as t∞, as can the validity of the Ansatz suggested by this scaling behavior
for synchronization frequencies of Kuramoto oscillators (see Sect. 3.8.3).
3.5
A Geometry Result
The transformation t →t′ := t/C leaves p invariant. Meanwhile, γk depends only
on p, so both γ and βH = (γ, 1) are also invariant under this transformation, in
accordance with (51). In other words, p is positive homogeneous of degree zero
in both t and H, i.e., Fp(Ct) := t/t∞= Fp(t) and Fp(CH) = Fp(H).9,10
Recall that Euler’s homogeneous function theorem states that if f ∈C1(Rn
+)
is positive homogeneous of degree a, then ⟨x, ∇xf(x)⟩= a · f(x) [84]. Since
each component pk of p is positive homogeneous of degree zero as a function of
both t and H, it follows that ⟨∇tpk, t⟩= 0 = ⟨∇Hpk, H⟩. Therefore each of the
gradients ∇tpk and ∇Hpk are tangent to spheres centered at the origin of their
respective coordinate systems.
Furthermore, the gradients ∇tpk and ∇Hpk are nondegenerate: an explicit
calculation shows that ∂pk/∂Ej = βpk(−δjk + pj) ̸= 0, and ∂pk/∂(β−1) =
β2pk(Ek −U), where as usual U := 
j pjEj. Meanwhile, ∂pk/∂tj = (δjkt∞−
tk)/t2
∞̸= 0. Taking appropriate directional derivatives makes it easy to see that
the constraint (43) does not aﬀect the nondegeneracy of these gradients.
Consider now the unique decomposition of a vector diﬀerential as dv = dv∥+
dv⊥, where the terms on the right hand side are respectively parallel and perpen-
dicular to v. That is, dv∥:= (⟨dv, v⟩/⟨v, v⟩)v, and dv⊥:= v−dv∥. It is easy to see
9 Recall that a function f deﬁned on a cone in Rn\{0} is said to be positive homoge-
neous of degree a iﬀf(Cx) = Caf(x) generically for C > 0.
10 Yet another equivalent characterization is that p is constant (away from the origin)
on rays through the origin of the form Rt and RH.

402
S. Huntsman
that dt⊥= 0 ⇐⇒dp = 0 ⇐⇒dH⊥= 0 from the preceding considerations.
Moreover, ⟨∇t⊥pk, dt⊥⟩= ⟨∇tpk, dt⟩= dpk = ⟨∇Hpk, dH⟩= ⟨∇H⊥pk, dH⊥⟩.
That is, the nondegenerate integral curves of both gradient ﬂows are arcs on
spheres centered at the origin. Since a smooth change of coordinates maps inte-
gral curves into integral curves [85], it follows that the respective spheres on
which these arcs lie must also map to each other under any smooth maps FH
and Ft satisfying (43) and (44). We therefore have
Lemma 5. A well-behaved map FH that respects (43) and (44) sends rays and
sphere orthants centered at the origin to rays and hemispheres centered at the
origin, respectively. In particular, a well-behaved map FH that respects (43) and
(44) must satisfy
∥s∥= ∥t∥⇒∥FH(s)∥= ∥FH(t)∥.
(60)
3.6
The Eﬀective Temperature
Let uj := ∥t∥/√n, so that ∥u∥≡∥t∥and FH(u) ≡(0, . . . , 0, β−1
u ). Now
∥FH(u)∥2 = β−2
u , and by (60)
∥FH(t)∥2 = ∥FH(u)∥2 = β−2
u .
(61)
Therefore, by (48) and (49),
β2 = ∥βFH(t)∥2
∥FH(t)∥2 = ∥γ∥2 + 1
β−2
u
.
(62)
Taking square roots of the far left- and right-hand sides yields
β = βu

∥γ∥2 + 1.
(63)
Meanwhile, (52) implies that
βu = S−1∥u∥= S−1∥t∥= S−1t∞∥p∥,
(64)
where S is a ﬁxed constant with units of action (say, S = ℏ= 1). To see the
ﬁrst equality of (64), note that uj := ∥t∥/√n implies that 
j uj = √n∥t∥=
√n∥u∥≡u∞. Since uj/u∞= n−1, it follows that Ψ(u/u∞) =: ψ(n) is a function
of n alone. Now (52) gives that βu = S−1u∞ψ(n) = S−1ψ(n)√n∥u∥. Without
loss of generality, the term ψ(n)√n can be absorbed into the constant S.11
Combining (63) and (64) with x = 1 therefore yields
β = t∞∥p∥·

∥γ∥2 + 1,
(65)
11 In §VIII of [66] we discuss detailed evidence that physical consistency appears to
demand ψ(n) = 1/√n, as this choice (somewhat counterintuitively) appears to be
the unique one giving a well-deﬁned limit in the microcanonical ensemble for dis-
cretizations of two physically paradigmatic systems.

Sampling and Statistical Physics via Symmetry
403
whereupon (47) leads to explicit expressions for FH:
β−1 =
1
t∞∥p∥
⎛
⎜
⎝
n

k=1
⎡
⎣1
n
n

j=1
log pj
pk
⎤
⎦
2
+ 1
⎞
⎟
⎠
−1/2
;
(66)
Ek = β−1 · 1
n
n

j=1
log pj
pk
.
(67)
Similarly, Ft is given explicitly (after shifting so that (43) is satisﬁed) as
pk = Z−1e−βEk;
(68)
t∞= ∥p∥−1 ·

∥E∥2 + β−2−1/2 .
(69)
To review, the derivation of the (Gibbs distribution and the) eﬀective
temperature rested on two basic symmetry assumptions and two derived
symmetries. The basic assumptions are that
• the zero point of energy is physically irrelevant;
• the probability of a state depends only on its energy.
The derived symmetries are that
• changing units of time leaves βH invariant;
• any physically nice bijection t ↔H preserves rays and radii.
3.7
Product Systems, the Ideal Gas, and Implications for t∞
Perhaps the most fundamental property of the ordinary temperature is inten-
sivity. Imposing a few simple physical requirements such as the intensivity of
the eﬀective temperature β−1 for simple product systems (which is a symme-
try requirement in keeping with our overall theme) turns out to place signiﬁ-
cant physical constraints on the functional form of reasonable candidates for the
timescale t∞, as we shall illustrate below. It seems likely that imposing similar
requirements for (subsystems of) closed interacting systems such as coupled map
lattices [86] will at least mirror–and probably augment–constraints of the sort
discussed here, but analyses of interacting systems will almost surely be much
more technically challenging.
3.7.1
Basic Results for Product Systems
Consider N systems sharing a common probability measure p on [n] = {1, . . . , n}.
Writing
b := (β/t∞)2 ≡∥p∥2 · (∥γ∥2 + 1)

404
S. Huntsman
for convenience and using a superscript ⊗to indicate the product system, it can
be shown that
b⊗= NnN−1∥p∥2(N−1) ·

∥p∥2 
∥γ∥2 + {NnN−1}−1
.
(70)
The somewhat peculiar way of writing the right hand side of (70) is motivated
by the fact that in the limit of large ∥γ∥2, the term in parentheses tends to b, in
which event
b⊗≈NnN−1∥p∥2(N−1) · b = N n⊗∥p⊗∥2
n∥p∥2
· b.
(71)
Recall that the harmonic mean ⟨f⟩h of a function f on [N] is given by
⟨f⟩−1
h
:= ⟨1/f⟩a ≡N −1 
m
f −1
m ,
where ⟨·⟩a indicates the arithmetic mean. If (in the present context of a collection
of subsystems with identical measures) we make the physically reasonable stipu-
lation of intensivity for the eﬀective temperature (cf. Sect. 3.8.3), i.e. β⊗= ⟨β⟩h,
then since β =
√
bt∞we must have that
t⊗
∞=

b/b⊗· ⟨t∞⟩h.
(72)
If furthermore the number n of states in each component system tends to
inﬁnity while p ≡p(n) remains suﬃciently uniform, the intensivity property (72)
turns out to take the form
t⊗
∞= N −1/2⟨t∞⟩h.
(73)
3.7.2
The Two-Dimensional Ideal Gas on a Compact Surface of Con-
stant Negative Curvature
An example of particular interest along the lines above is furnished by the
geodesic ﬂow on a compact surface of constant negative curvature (see Sect.
4.5). In this context, (73) gives a recipe for applying our framework to the ideal
gas (with or without a thermostat).
Besides the apparently well-deﬁned value of β for the geodesic ﬂow (i.e., a
single particle) on a compact surface of constant negative curvature, the essential
observation for establishing the plausible consistency of β⊗with the physical
inverse temperature is simply one of scaling behavior. We detail this here.
It was shown in [87] that the L2 mixing time of the geodesic ﬂow is 1/2
for reasonably well-behaved observables. Taking this (or with trivial modiﬁca-
tions, any other constant timescale, e.g. the genus-independent inverse topolog-
ical entropy [see section Sect. 3.7.4]) as t∞for a single ﬂow with speed v = 1,
we have that vt∞= 1/2 more generally. Now ⟨t∞⟩h = ⟨t−1
∞⟩−1
a
= 1/2⟨v⟩a. For
a two-dimensional ideal gas ⟨v⟩a =

π/2βm, so for β⊗to equal the physical
inverse temperature we must have by (73) that
t⊗
∞=
1
2⟨v⟩a
√
N
=
#
βm
2πN .
(74)

Sampling and Statistical Physics via Symmetry
405
The quadratic dependence on β (and on m) in the above equation has a
simple explanation consistent with β scaling as t∞. While the argument that β
scales as t∞ceases to apply when we only vary v, it does apply when we hold a
phase space trajectory ﬁxed, and in this event β, m and t∞all scale identically
(cf. Sect. 3.4.1). Indeed, in the single-particle case β ≡2/mv2 and vt∞= 1/2,
so β = 8t2
∞/m.
Consequently β⊗and the physical inverse temperature scale identically: in
particular, both are constant in the limit of large N, and incorporating an appro-
priate constant into the deﬁnition of β yields equality (cf. Sect. 3.9.3).
It is worth noting here that naive discretizations of an ideal gas with obvious
conﬁguration space geometry, boundary conditions, ultraviolet cutoﬀs, etc. do
not exhibit reasonable scaling limits, a fact which motivated our analysis of the
rather esoteric version and context of the ideal gas considered here.
3.7.3
Products of Markov Processes and Constraints on t∞
The detailed behavior of the relationship (73) allows us to rule out a number of
potential candidates for a broadly applicable t∞.
For instance, recurrence, hitting, covering or similar timescales do not appear
to be suitable candidates. Additionally, quantities such as the recurrence rates
of a ﬂow [88] or a so-called cutoﬀfor a family of product Markov processes [89]
are not appropriate choices in the present context simply because they do not
have the necessary parametric dependence.
While the form of (72) and (73) suggest that the choice for t∞should bear
some qualitative similarily to a relaxation time [90], we can also rule out a naive
identiﬁcation of t∞with an inverse spectral gap in the context of Markovian
dynamics, as we proceed to sketch.
For m ∈[N], let Q(m) be the generator of a (well-behaved) continuous-
time Markov process on [nm]. The composite Markov generator corresponding
to evolving each of the N processes simultaneously turns out to be
Q⊗=

m
I⊗(m−1) ⊗Q(m) ⊗I⊗(N−m).
(75)
It is easily seen that the spectral gap of Q⊗is just the smallest of the spectral
gaps of the Q(m). In particular, if (as we shall assume henceforth)
Q(m) = cmQ
(76)
for cm > 0, then the spectral gap of Q⊗is the product of the gap for Q and
minm cm. This precludes a relation of the form (72) or (73) for an inverse spectral
gap and suggests that such a quantity is not a generically suitable choice for t∞.
That said, a “modiﬁed” L2 mixing time is related to an inverse spectral gap
and does appear to be a viable generic candidate for t∞(as does the similarly
normalized inverse topological entropy: see Sect. 3.7.4), as we shall see below.
For a reversible Markov process without product structure, this timescale and
the inverse spectral gap coincide, and for the example of a single Glauber-Ising

406
S. Huntsman
spin both equal (2a)−1. The Ansatz t∞= (2a)−1 discussed in Sect. 3.8.1 thus
amounts roughly to (quite reasonably) equating the spectral gap of the generator
and the dominant energy scale.
While we dwell on the potential for a broadly applicable recipe for appro-
priately determining t∞, we must also consider the possibility (discussed in
Sect. 3.9.3) that no completely universal recipe exists. That is, it may be that
appropriate choices for t∞are necessarily context-dependent, for example in the
same way that the Gibbs paradox illustrates that the entropy of a system can
depend on the level of speciﬁcation [91]. Indeed, detailed consideration of a clas-
sical Bose gas (not included here) suggests indicates that distinguishability of
particles should inform the eﬀective temperature if t∞is given by a modiﬁed L2
mixing time.
In any event, the proper speciﬁcation of t∞is clearly a central component
of our eﬀective framework for statistical physics, and the degree of universality
with which this speciﬁcation can be accomplished will be directly related to its
ultimate physical signiﬁcance. Nevertheless, as both the analogy with the Gibbs
paradox and the characterization of individual systems varying in time or over
some parametric ensemble show, even a context-dependent quantity can still
have substantial physical relevance.
3.7.4
L2 Convergence of Markov Processes and a Modiﬁed Mixing
Time
As a preliminary to discussing the modiﬁed L2 mixing time mentioned above, we
ﬁrst review here the ordinary L2 mixing time for Markov processes.12 Given a
(not necessarily reversible but well-behaved) Markov generator Q with invariant
distribution p, the corresponding Dirichlet form is
E(f) := 1
2

j,k
pjQjk(fj −fk)2.
(77)
Write
λ∗:= inf
f 2
E(f)
Varp(f),
(78)
where the inﬁmum is over f s.t. Varp(f) ̸= 0. It can be shown that λ∗determines
the L2 convergence of the Markov process to stationarity: viz., λ−1
∗
is the L2
mixing time. Furthermore, if Q is reversible, its eigenvectors form a basis and
λ∗is the spectral gap.
For a product system of the form (75) with Q(m) = cmQ, it can be shown that
the inﬁmum in (78) is degenerate in the sense that its consideration amounts to
ignoring various factors of the product. The nondegenerate minimum is (contin-
uing an obvious notational convention)
λ⊗:= N⟨c⟩aλ∗,
(79)
12 NB. We follow the standard convention in physics and dynamical systems theory
for “the” L2 mixing time, which diﬀers somewhat from the mixing time function
typically considered by probabilists.

Sampling and Statistical Physics via Symmetry
407
where λ∗corresponds to Q.
Writing τ ⊗
∞:= (λ⊗)−1 and τ (m)
∞
:= (cmλ∗)−1, (79) becomes
τ ⊗
∞= ⟨τ∞⟩h/N
(80)
which diﬀers from (73) only by a factor of N −1/2 (though the context here is
more general, as p need not be close to uniform).
A corresponding normalization of τ ⊗
∞that takes any product structure into
account therefore appears to be a plausible general-purpose candidate for t⊗
∞sat-
isfying (72) in physically relevant cases. This modiﬁed L2 mixing time is more
physically natural than the usual L2 mixing time because it measures the conver-
gence of all the component processes, not just a single distinguished component
process. It is properly normalized and avoids any degeneracies introduced by the
tensor product structure.
A similar result applies for the inverse topological entropy of a product sys-
tem. Recall that the topological entropy of a system describes the rate at which
the number of periodic orbits grows as a function of the orbital period. For this
reason its inverse is a natural characteristic timescale, and it turns out that a
straightforward normalization obeys (73).
Indeed, the topological entropy of a product ﬂow of the form φ⊗
t := $
m φ(m)
t
with φ(m)
t
:= φcmt satisﬁes h(φ⊗) = 
m cm · h(φ) [92]. So if we set τ (·)
∞:=
1/h(φ(·)), then we obtain a relation of precisely the form (80). That is, the inverse
topological entropy of a ﬂow satisﬁes the same sort of product relationship as
the modiﬁed L2 mixing time.13 However, we focus on the mixing time as it may
be more broadly applicable.
3.8
Elementary Examples and Applications
We sketch some elementary examples and applications here. The application to
Anosov systems and the chaotic hypothesis in Sect. 4 is suﬃciently involved and
signiﬁcant to demand special treatment, though it also informs an application
to a two-dimensional ideal gas (see section Sect. 3.7.2). Likewise, a thermody-
namical analysis of the degradation of discrete memoryless channels is currently
underway but not sketched here.
The framework presented here has been utilized for the characterization of
computer network traﬃc [94] (another eﬀort in a similar spirit is [95]). Although
the potential scope of this framework appears to be quite broad, the key practical
diﬃculties in applications are the identiﬁcation of an appropriate state space (or
discretization scheme) and characteristic timescale. The examples that we have
thus far been able to identify all have complicating features in at least one of
these regards. Nontrivial spin models, which might appear at ﬁrst to give an ideal
setting for exploring the eﬀective temperature in detail, are deceptively diﬃcult
to deal with in this framework because of the subtle nature of timescales in glassy
13 In fact the inverse topological entropy and a topological (non-L2) mixing time are
related: see e.g. [93].

408
S. Huntsman
systems.14 That said, a single Glauber-Ising spin will serve as an illustrative
example in section Sect. 3.8.1.
For characterization of generic data sets (e.g., computer network traﬃc) the
state space selection issues are similar to those confronted in the application
of entropy methods, and the characteristic timescale may be dictated either
by the data itself or by the collection interval. In many ways this “descriptive
thermodynamics” is the simplest sort of application [68], and in fact it motivated
the present framework.
3.8.1
Two-State Systems; A Single Glauber-Ising Spin
Consider the simplest case of a two-state system as illustrated in Fig. 5. In this
case we have that γ1 =
1
2 log p2
p1 and γ2 =
1
2 log p1
p2 = −γ1. Therefore trivial
substitutions yield
β = t∞

(p2
1 + p2
2)
1
2 log2 p1
p2
+ 1
1/2
;
(81)
E1 = −1
2β log p1
p2
;
E2 = −E1.
(82)
Going in the other direction, we ﬁrst take Ej →Ej −(E1 + E2)/2 in accor-
dance with (43), so that again E2 = −E1 and γ2 = −γ1. Therefore
p1 =
e−γ1
e−γ1 + eγ1 ;
p2 =
eγ1
e−γ1 + eγ1 .
(83)
Moreover, Z = 2 cosh γ1, ∥p∥2 = (1 + tanh2 γ1)/2, and ∥γ∥2 = 2γ2
1, from which
it follows that
t∞= β
1 + tanh2 γ1
2

2γ2
1 + 1
−1/2
.
(84)
As a physical incarnation of this example, consider the requirement that
β equal the actual physical inverse temperature for a single Glauber-Ising spin
σ in a magnetic ﬁeld. The spin dynamics are determined by an overall (spin
ﬂip) rate a and b := tanh(βμh), where μ is the magnetic moment and h is
the ﬁeld strength [96]. Speciﬁcally, the stationary distribution corresponding to
σ = (−1, 1)∗is p = 1
2(1 −b, 1 + b). Meanwhile ∥p∥2 = (1 + b2)/2 and γ1 = βμh,
so ∥γ∥2 = 2(βμh)2. By (84),
t∞= β
1 + b2
2

2[βμh]2 + 1
−1/2
.
(85)
This turns out to be a physically reasonable characteristic timescale, as we
sketch here. For β ≪1, t∞≈
√
2β; for β ≫1, t∞≈1/
√
2μh =
√
2/ΔE,
where ΔE is the energy gap between the spin states. In both regimes t∞is
asymptotically proportional to the inverse of the natural energy scale, and in
fact the constants of proportionality are the same in both regimes.
14 See §XVIII of [66] for a detailed discussion of this topic.

Sampling and Statistical Physics via Symmetry
409
Because mixing times are typically of the same order as inverse energy
gaps, such a choice for t∞is consistent with our overall arguments and
physically justiﬁed.
A routine calculation shows that the L2 mixing time of the spin is (2a)−1. With
this in mind, an Ansatz such as t∞= (2a)−1 removes any remaining freedom in
the H-picture and provides a plausible basis for recapturing (most of) the physical
context of the spin from its statistical behavior.15 In particular, it requires a speciﬁc
relationship between the spin ﬂip rate a (the physical import of which has usually
been ignored) and the physical parameters β and ΔE. While we are unaware of any
results that might inform the validity of this speciﬁc relationship–equivalently, the
just-mentioned Ansatz–in a single-spin system, considerations along present lines
suggest an experimental framework for evaluating it.
It would be of interest to determine to what extent timescales obtained along
the lines of the present section might yield similar results for more general sys-
tems. However beyond this simple example such a task becomes diﬃcult: even
in the equilibrium case the analysis of timescales is nontrivial.
3.8.2
Markov Processes
An obvious application is to well-behaved but not necessarily reversible Markov
processes speciﬁed by a transition (discrete time) or generator (continuous time)
matrix on a ﬁnite state space. The invariant distribution p is given as a left
eigenvector of the relevant matrix. In the present context and perhaps more
generally, a plausible candidate for t∞is furnished by a modiﬁed L2 mixing
time: see Sect. 3.7.4.
For example, examination of Anosov systems, a single Glauber-Ising spin,
and product systems (see sections Sect. 4, Sect. 3.8.1, and Sect. 3.7, respectively)
all suggest a choice for t∞along the lines of a mixing or similar timescale on
physical grounds. We note that the ﬁrst two of these examples have an essentially
Markovian character, and the third is examined in the same spirit.
3.8.3
Synchronization
It is well known that many collections of mutually coupled subsystems synchro-
nize in various senses for suﬃciently large coupling. For a review of the most
interesting case of chaotic systems, see [98].
An interesting application of the intensivity of the eﬀective temperature in
this regard where the subsystems are taken to be identical except for their natu-
ral frequencies but also mutually interacting is a thermodynamically motivated
Ansatz for synchronization frequencies. Essentially, it is natural to view the
15 The natural recurrence time 4a−1(1 −b2)−1 was previously considered in [97] as a
candidate for t∞for a single Glauber-Ising spin: however, such a choice turns out to
be physically inappropriate, not least due to inconsistency with constraints imposed
by intensivity.

410
S. Huntsman
speciﬁc process of chaotic synchronization as a particular case of the more gen-
eral implied process of eﬀective thermal equilibration.
Without loss of generality, let the natural frequencies of subsystems be given
by ωm := cmω0. Suppose furthermore that the mth subsystem has an eﬀective
temperature of β−1
m when uncoupled (note that although we have not identiﬁed a
probability measure on the subsystem’s phase space, our present considerations
do not really depend on this). A trivial intensivity argument (cf. Sect. 3.7.1)
suggests that the synchronized/equilibrated system should then have the eﬀec-
tive temperature β−1
∗
= ⟨β⟩−1
h , where ⟨·⟩h denotes a harmonic mean. From the
general scaling of β with t∞, we get that βm scales as c−1
m , and in turn that β∗
varies as ⟨c⟩a, where ⟨·⟩a denotes an arithmetic mean. This leads ﬁnally to the
Ansatz that the synchronization frequency should also vary as ⟨c⟩a.
As a nontrivial example where this Ansatz is validated, consider a system of
Kuramoto oscillators [99] determined by the dynamical equations
˙θm = cmω0 +

m′
Kmm′ sin(θm′ −θm)
(86)
where K is symmetric. This is a special case of the model considered in Theorem
V.1 of [100], which gives that (under some restrictions) the individual instanta-
neous oscillator frequencies synchronize to
˙θ∗= ω0⟨c⟩a.
(87)
That is, the scaling behavior of β yields an Ansatz that anticipates the synchro-
nization result (87).
We note ﬁnally that Theorem V.1 of [100] may suggest how to assign weights
to inhomogeneous systems in a way appropriate to the overall framework of the
present discussion.
3.9
Remarks
As we have seen, the form of Eqs. (66)–(69) are dictated by very general physical
considerations. No appeals to (e.g.) detailed balance or maximum entropy are
necessary, and most of the derivation is essentially mathematical.
In the setting of Anosov systems (see Sect. 4) the eﬀective temperature has
a purely dynamical basis rooted in the SRB measure. This dynamical grounding
of the eﬀective temperature is an important indication of its physical relevance
[101,102]. However, it can be still applied without reference to dynamics. For
example, if the system under consideration is not stationary but p and t∞vary
with time suﬃciently slowly as to remain well-deﬁned, then so will β and E,
and the language of equilibrium statistical physics will still be adequate. That
is, there is no need for (e.g.) detailed balance or a maximum-entropy variational
principle to be satisﬁed in order for β to be well-deﬁned: Eq. (65) can be taken
as an extension of the language of equilibrium statistical physics. Though the
details of how p and t∞should be calculated or estimated are important and
nontrivial, such questions of data analysis are properly distinct from our present
considerations.

Sampling and Statistical Physics via Symmetry
411
While the continuity of β w/r/t t suﬃces to indicate the relevance of the
present construction for quasi- and near-equilibrium systems, its scope is con-
siderably more general. That said, the application of (66)–(69) to most physi-
cally interesting systems is highly nontrivial. For example, the nonstationarity
of nonequilibrium spin systems introduces signiﬁcant diﬃculties, while the equi-
librium case is of limited interest beyond illustrative purposes.
In practice, obtaining the appropriate t∞presents a challenge (with a con-
comitant reward) that is not generally encountered in other approaches for the
statistical characterization of physical and/or complex systems. In the equilib-
rium setting, this timescale dependence may be inverted to enforce consistency
with traditional statistical physics while preserving a universal choice of scale
for β.
That said, we may presently entertain the attractive possibility that a
universal recipe for t∞may exist, in terms of (e.g.) an ideal gas coupling
and/or a modiﬁed L2 mixing time.
Apart from the distinguishing features introduced by involving the timescale
t∞, at this point it should be clear that the eﬀective temperature bears loose
analogies to Shannon entropy both in its functional dependence and its physical
content. Though an information-theoretic interpretation of the eﬀective tem-
perature is not obvious, its relevance to data analysis has been demonstrated
elsewhere in the context of computer network traﬃc analysis; meanwhile, an
examination of thermodynamical analogies in the information theory of discrete
memoryless channels is also presently being undertaken and holds promise for
illuminating the nature and role of t∞.
3.9.1
Obstruction to Analogues for (e.g.) Bose-Einstein or Nonex-
tensive Statistics
Consider a notional alternative to the Gibbs distribution of the form
pk ≡f(−γk)/ζ.
(88)
Now −γk = f −1(ζpk) and if 
j f −1(ζpj) = 0, then
γk = 1
n

j

f −1(ζpj) −f −1(ζpk)

.
(89)
The derivation of the formula for β depends in an essential way on the exis-
tence of a relation of the form f −1(ζpj)−f −1(ζpk) ≡g(pj, pk). If such a relation
holds, diﬀerentiating both sides w/r/t ζ gives that y · Dy(f −1(y)) is constant. It
follows that f(x) = exp(cx + c′) for some c, c′: this amounts to reproducing the
Gibbs distribution. In other words, generalizations of the eﬀective temperature
building on e.g. Bose-Einstein or nonextensive statistics cannot be constructed
along obvious lines.

412
S. Huntsman
3.9.2
Naive Requirements for Continuous Distributions
Dealing with a more general reference probability measure ν than a normalized
counting measure is straightforward provided that the physical measure μ is abso-
lutely continuous w/r/t ν and that both p ≡dμ/dν and log p are in L1(ν)∩L2(ν).
To see this, recall that f ∈Lq(ν) iﬀ∥f∥q := (
%
|f|q dν)1/q < ∞. Using the
additional shorthand ℓ:= log p, we have that the analogue to (43) is
%
E dν = 0,
from which it follows that −
%
ℓdν = log Z. Further brief manipulations yield
the generalization of (47), namely γ(x) =
%
ℓdν −ℓ(x), and we also have that
∥γ∥2
2 = ∥ℓ∥2
2 −(
%
ℓdν)2. This is well-deﬁned if ℓ∈L1(ν) ∩L2(ν). If moreover
we have that p ∈L2(ν) then the natural analogue of (66) is well-deﬁned. Note
that p ∈L1(ν) since
%
p dν =
%
dμ ≡1.
However, these integrability conditions are rarely met in situations of interest.
Even more fundamentally, SRB measures are typically not absolutely continu-
ous w/r/t underlying Riemannian measures. For this reason the application to
Anosov-like systems in Sect. 4 is necessarily more involved.
3.9.3
The Choice of Overall Scale and the Zeroth Law
The requirement that β equal the actual physical inverse temperature for equi-
librium systems strongly constrains t∞, and (modulo issues of state space dis-
cretization) completely speciﬁes the product S−1t∞appearing in, e.g., (52).
That is, mandating equivalence of the eﬀective and actual temperature wher-
ever possible links S and t∞. It is clear that we may choose either S or t∞to be
system-independent at the cost of admitting at least the possibility for system-
dependence on the other. However, we have (without loss of generality) enforced
the overall choice of scale S ≡1.16
Subject to this choice of overall scale, the ultimate physical signiﬁcance of β
will necessarily depend on the (as yet unknown) degree to which we can have
β equal the actual physical inverse temperature in diﬀerent equilibrium systems
without requiring t∞to have some system-dependent deﬁnition (or to absorb
some system-dependent constant) to compensate.
Nevertheless, even in the most pessimistic case of a completely system-
dependent overall scale, the eﬀective temperature (or a ratio of eﬀective tem-
peratures with the same choice of overall scale) could still be fruitfully used to
“internally” characterize individual systems that vary in time suﬃciently slowly
for p and t∞to remain well-deﬁned, or to compare multiple systems that are
identical save for some parametric dependence over a statistical ensemble (and
perhaps especially an ensemble which permits perturbations from equilibrium).
In fact, the former situation obtains in the analysis of, e.g., experimental data
with long-timescale variability.
Therefore, a system-independent choice of scale is not necessary to establish
that there is some physical relevance for β, but only the scope of that relevance.
However, we point out that at least a weak degree of system-independence is
exhibited for the examples in the preceding paragraph as well as the ideal gas on
16 While S ≡S(n) ̸≡1 might appear to be a reasonable middle ground, e.g., S = √n,
considerations of the sort described elsewhere for t∞also militate against this.

Sampling and Statistical Physics via Symmetry
413
a surface of constant negative curvature, as the genus does not appear to aﬀect
either t∞or the value of β (see Figs. 6 and 7).
In general, there appear to be two basic avenues to addressing concerns of
system-dependence of scale (say, as manifested in t∞with S ≡1), which even
from a pessimistic point of view would turn out to be at least approximately
equivalent in some circumstances for the reasons cited just above.
The ﬁrst avenue is, in the absence of any other generically useful and iden-
tiﬁable recipe for computing t∞a priori, to take the requirement that β equal
the actual physical inverse temperature in equilibrium to operationally deﬁne
t∞. The example of a single Glauber-Ising spin in Sect. 3.8.1 indicates how a t∞
obtained in this way can be physically meaningful. Taking this avenue might help
to place physical constraints on and even select a preferred system-independent
characterization of t∞(e.g., as a modiﬁed L2 mixing time) valid both in and
beyond equilibrium.
The second avenue is more diﬃcult and ambitious, but likely also more sound.
It involves coupling systems to an ideal gas and enforcing the constraint β(sys) =
β(sys+gas) = β(gas) ≡β in a suitable coupling and/or large N limit for the gas.
That is, this approach takes the zeroth law as an Ansatz. It can be hoped that
it might be possible in principle to infer a well-deﬁned t(sys)
∞
from an implied
timescale t(sys+gas)
∞
,17 subject to the temperature constraint above. Better still
would be a comparatively simple recipe for determining this t(sys)
∞
such as those
proposed in Sect. 3.7.4.
3.9.4
Coda
Though the typical state of aﬀairs is for the ordinary temperature to be regarded
as an environmental parameter in calculations, the logic may be largely turned
on its head: in many cases we can directly obtain an eﬀective temperature and
(re)construct an eﬀective Hamiltonian from the behavior of a system. In this way
the idiom of equilibrium statistical physics may be extended for many applica-
tions in nonequilibrium steady states and problems in data analysis. Finally,
while a philosophical study of thermometry notes that “there are complicated
philosophical disputes about just what kind of quantity temperature is” [103],
we hope to catalyze investigations in this direction.
4
Application to Anosov Systems
The examples and applications in Sect. 3.8 of the framework of Sect. 3 are
only a partial list. More substantive eﬀorts have been or are focused on, e.g.
characterization of computer network traﬃc [94], physical correspondences in
the information theory of discrete memoryless channels, and data science. Here,
however, we will discuss an application to mixing Anosov systems in some detail,
as this realistic physical context underlines the equivalence of statistical (t) and
physical (H) descriptions.
A more comprehensive treatment of the material in this section is in [66].
17 For considerations aﬀecting systems with multiple independent characteristic
timescales, see §XVIII of [66].

414
S. Huntsman
4.1
Overview
Essentially, a mixing Anosov system is a well-behaved uniformly hyperbolic
dynamical system (see Fig. 8 for a schematic and [92,104–106] for background
elaborating on Sect. 4.2). Such systems are particularly relevant to statistical
physics: indeed, the so-called chaotic hypothesis is that many-particle systems
are essentially mixing and Anosov insofar as their macroscopic properties are
concerned [3,4]. Underpinning this conjecture is the existence (for a compact
phase space, which we assume for convenience) of the SRB measure μSRB, an
invariant physical probability measure generalizing the microcanonical ensemble
[107].
The two quintessential examples of Anosov systems (both mixing) are the
discrete-time Arnol’d-Avez cat map (more generally, a hyperbolic toral auto-
morphism) and the geodesic ﬂow describing a free particle on a compact surface
of constant negative curvature.18
We ﬁrst outline how to deal with the continuous phase space of a mixing
Anosov system in a natural way. One of two key observations in this regard is
that the hyperbolic dynamics furnish a physically natural family of phase space
discretizations, called Markov partitions. We recall that a Markov partition R
for an Anosov diﬀeomorphism T is a decomposition of phase space into so-
called rectangles Rj with local product structure compatible with the hyperbolic
structure of T and such that the images of rectangles under T stretch completely
across the original rectangles in the unstable direction and vice versa for T −1 (see
Fig. 9). Here the probability distribution corresponding to a Markov partition R
is simply given by pj := μSRB(Rj) for each Rj ∈R, and the L2 mixing time of the
system is taken as (a placeholder/approximation for) t∞.19 Ergodicity ensures
that this is equivalent to considering the time series of indices of rectangles that
contain a test particle.
The second key observation for dealing with the continuous phase space is
that its geometrical measure (i.e., the Riemannian and not the SRB measure)
determines a procedure for obtaining greedy reﬁnements of any initial Markov
partition (see Figs. 9 and 11, 12 and 13).20
18 A discrete-time version of the latter is obtained by considering a Poincar´e or timing
map.
19 The Ruelle-Bowen hypothesis that mixing Anosov ﬂows are exponentially mixing
(with respect to H¨older observables and equilibrium measures with H¨older poten-
tials) [108–111] would further support the existence of a mathematically and physi-
cally natural t∞.
20 While not every Anosov system will preserve a natural Riemannian measure, the
archetypes we consider do: for hyperbolic toral automorphisms, this is just the (push-
forward of) Lebesgue measure, and for the geodesic ﬂow on a surface of constant
negative curvature it is the Liouville measure. More generally, so-called conservative
diﬀeomorphisms preserve a natural Riemannian measure (and diﬀeomorphisms in
general preserve an equivalence class of measures) [112]. A wide class of conservative
diﬀeomorphisms is furnished by Hamiltonian systems, and it is natural to couch the
otherwise implicit notion of a “natural” Riemannian measure in this context.

Sampling and Statistical Physics via Symmetry
415
These greedy reﬁnements are Markov by construction, and a subsequence
of them is maximally uniform w/r/t the geometrical measure. Therefore, from
the physical point of view, intermittent greedy reﬁnements of an initial Markov
partition are particularly natural. Indeed, their uniformity enables the proof of
a ﬁnite value for lim inf β for the cat map, where the limit inferior is taken over
successive greedy reﬁnements.21 Furthermore, considering greedy reﬁnements for
the geodesic ﬂow also gives compelling numerical evidence that lim inf β is not
only ﬁnite but independent of global structure (see Figs. 6 and 7).
Fig. 6. β/t∞under successive greedy reﬁnements of an initial Markov partition for a
map isometrically topologically conjugate to a timing or Poincar´e map of the geodesic
ﬂow on a compact surface of constant negative curvature with genus g = 2, 3, 4. Note
the logarithmic horizontal scale: here N = 8g −4 is related to the number of rectangles
in the initial partition. The inset box indicates axis limits for Fig. 7.
We now turn to brieﬂy describing how the preceding results bear on generic
systems of interest in statistical physics. A product system formed from copies
of the geodesic ﬂow with canonically distributed initial conditions is just an
21 Based on some explicit calculations for the cat map, it may be that lim inf β is
independent of the choice of initial Markov partition: however, if this turns out not
to be the case, a suitable extremum over Markov partitions may be considered.

416
S. Huntsman
Fig. 7. As in Fig. 6 for g = 2, . . . , 40. Axis limits correspond to the inset box in
Fig. 6. The existence of a nontrivial limit inferior independent of g is strongly sug-
gested by such numerical results. Meanwhile, both the L2 mixing time for suﬃciently
well-behaved observables and the inverse topological entropy turn out to be plausible
genus-independent approximations for t∞.
ideal gas.22 By considering this gas as an environment and weakly coupling a
subsystem to it, our focus shifts from a generalized microcanonical ensemble
to the canonical ensemble, and a deﬁnition of a generic subsystem’s eﬀective
temperature in terms of that of its environment.
Such a procedure should often if not always give physically reasonable and
self-consistent results, as we proceed to sketch. The results of [113] and the gen-
eral phenomenon of structural stability of Markov partitions for Anosov systems
show that the eﬀect on β/t∞of coupling a Gaussian thermostat to the geodesic
ﬂow is analytic in the strength of the coupling.23 It is also reasonable to expect
that t∞will exhibit a similar regularity as a function of the coupling based on
(e.g.) the stability of rapid mixing [119]. Many other examples are known in
22 The consideration of a product system formed from statistically identical subsystems
places a very strong intensivity constraint on the form of t∞that forces this quantity
to be similar to, yet necessarily distinct from, the L2 mixing time.
23 In the thermodynamical limit, we expect dynamics to be insensitive to the details
of thermostatting, i.e., the various SRB measures should tend to the same limit
[4,114–118].

Sampling and Statistical Physics via Symmetry
417
which well-behaved Anosov systems exhibit considerable stability of SRB mea-
sures and mixing times w/r/t perturbations, and it is reasonable to expect such
behavior in general for physically relevant cases.
Taken together, these observations strongly suggest the existence and
essential uniqueness of a physically preferred eﬀective temperature and
concomitant energy function intrinsic to generic mixing Anosov systems.
Moreover, they suggest an avenue for extending Ruelle’s thermodynamical
formalism [120] into a more comprehensive theory of statistical physics for
nonequilibrium steady states obeying the chaotic hypothesis.
4.2
Background on Anosov Systems
A smooth endomorphism T of a Riemannian manifold (“phase space”) is an
Anosov map if it is both
• uniformly hyperbolic, i.e. at every point x there are transverse local stable and
unstable surfaces on which points respectively converge and diverge exponen-
tially at a rate independent of x; and
• invariant, i.e. the tangent spaces to these surfaces are mapped by the deriva-
tive of T into the tangent spaces to the corresponding surfaces at Tx ≡T(x).
If the global stable and unstable surfaces of T are dense, then T is also said to be
mixing. An Anosov ﬂow is a continuous-time analogue of an Anosov map with
a neutral surface transverse to the time evolution as schematically indicated in
Fig. 8. We refer to both Anosov maps and ﬂows as Anosov systems.
Fig. 8. Schematic of an Anosov ﬂow, with respective local stable, unstable, and neutral
surfaces E−
x , E+
x , and E0
x.
Anosov systems enjoy natural discretizations of their phase spaces. These
discretizations, called Markov partitions, are particular conﬁgurations of rectan-
gles.24 A rectangle R is a subset of phase space such that the intersection of a
24 Strictly speaking, Anosov ﬂows require a related notion called a Markov section, but
this distinction can be mostly ignored here. See [105] for details.

418
S. Huntsman
local stable surface and a local unstable surface consists of a single point also
in R: i.e., there is a local product structure compatible with T.25 A partition
R = {Rj}n
j=1 of phase space into rectangles is Markov if (whenever these sets
intersect) the images TRj stretch completely across Rk in the unstable direction
and Rk stretches completely across TRj in the stable direction, as schemati-
cally indicated in Fig. 9. The utility of the coarse-graining of phase space that
Markov partitions provide is largely attributable to the fact that (via the theory
of symbolic dynamics) they allow Anosov systems to be treated in much the
same way as a spin system [4,121]. This also highlights the relevance of both
Anosov and spin systems as extremely generic (or as the chaotic hypothesis
argues, completely generic) models for statistical physics.26
Fig. 9. Schematic of the action of an Anosov map on a rectangle that is part of a
Markov partition. The image of the rectangle stretches precisely across other rectan-
gles in the partition. If we suppose that the phase space measure corresponds to the
Lebesgue measure of the rectangles, then a greedy reﬁnement of the gray rectangle
can be obtained by drawing a line at the inverse image of the intersection of the gray,
red, and green rectangles. That is, a greedy reﬁnement of a Markov partition R for an
Anosov map T starts by considering the forward image under T of a rectangle Rj ∈R
with maximal geometrical/phase space (vs. physical/SRB) measure. The intersection
of the boundary of R and TRj determines subrectangles of TR that in turn determine
various reﬁnements of R under T −1. A greedy reﬁnement has maximal entropy w/r/t
the geometrical measure.
4.3
The Cat Map
The simplest example of an Anosov map is the Arnol’d-Avez cat map deﬁned
on the unit torus via TAx := Ax mod 1, where A = ( 2 1
1 1 ).27 The eigendecom-
positon of A determines the stable and unstable directions. The eigenvalues are
25 Rectangles in the present context are not, and should not be generically thought of as,
right-angled quadrilaterals (indeed, rectangles are generically fractal in character).
However, the speciﬁc examples we consider will be right-angled quadrilaterals.
26 For example, a unique SRB measure corresponds to the absence of phase transitions
in one-dimensional short-ranged spin models [4]. This Anosov-spin system corre-
spondence also suggests the construction of d-dimensional lattices of coupled maps
corresponding to (d + 1)-dimensional spin systems capable of exhibiting phase tran-
sitions [86].
27 The cat map corresponds to unit-frequency projections for the Hamiltonian
HA(X, P) = K(P 2 −X2 + XP) with K = sinh−1(
√
5/2)/
√
5.

Sampling and Statistical Physics via Symmetry
419
λ± = φ±2, where φ = 1+
√
5
2
is the golden ratio. The corresponding eigenvec-
tors are e−= (s, −c)∗and e+ = (c, s)∗, where c = 1/√3 −φ and s =
√
1 −c2.
Because these eigenvectors have irrational slopes, the stable and unstable sur-
faces are dense on the torus, so the cat map is mixing.
More generally, matrices in GL(n, Z) with no eigenvalues in S1 correspond
to hyperbolic toral automorphisms which are also Anosov maps. In dimension
n = 2, rectangles for these maps are geometrically unions of parallelograms.
4.4
Markov Partitions and Greedy Reﬁnements
There are many Markov partitions for the cat map: Fig. 10 shows three, respec-
tively denoted RA, R′
A, and R′′
A.
A Markov partition R = {Rj}n
j=1 induces a
Fig. 10. Three Markov partitions for the cat map.
probability distribution pj := μ(Rj) inherited from the physical/SRB measure
μ ≡μSRB. For the cat map (or any other hyperbolic toral automorphism), this
measure is just the Lebesgue measure.
For any two-dimensional hyperbolic toral automorphism T (including the
cat map), there is a remarkable fact (for details, see [66]). Let R be a Markov
partition and let R∨
m be a reﬁnement of R obtained by taking connected com-
ponents of intersections of rectangles in T jR for 0 ≤j ≤m. Now as m →∞,
β/t∞= ∥p∥·

∥γ∥2 + 1 (which does not depend on t∞) converges to a ﬁnite
nonzero value.
To see why any ﬁnite nonzero limit for β/t∞is nontrivial, consider the fol-
lowing toy example. Deﬁne Y(0) = [0, 1] and form Y(m+1) by subdividing each
interval in Y(m) into two subintervals of relative length q and 1 −q. The cor-
responding partitions yield lim β/t∞= ∞unless q = 1/2, in which case the
limit is zero. Meanwhile, as we have mentioned, a naive discretization of the free
particle/ideal gas has no obvious reasonable scaling limit.
The preceding results involving two-dimensional hyperbolic toral auto-
morphisms indicate that while it might seem useful to consider β = Sn ·
t∞∥p∥

∥γ∥2 + 1 with Sn = √n so that β is independent of n, this is a mirage:
we should actually require Sn to be constant in n.

420
S. Huntsman
Now while detailed calculations establish that lim β/t∞depends on R, and
phase space (to say nothing of physical) measures of rectangles in R∨
m vary
increasingly more as m increases, there is a straightforward solution. We can
construct greedy reﬁnements of R that are more physically natural by maxi-
mizing the uniformity of phase space measures at each step of the reﬁnement
process. Even when the physical measure μ and phase space measure ν dis-
agree,28 considering greedy reﬁnements will tend to minimize β and maximize
entropy/minimize eﬀective free energy. This is an indication of a generalized
variational principle (in the sense of ergodic theory) that can yield a ﬁnite limit
for β even as the entropy of partitions diverges.
The construction of greedy reﬁnements goes as follows. For a rectangle Rj ∈R
with maximal phase space measure ν(Rj), the intersection of TRj with rectangles
in R determines subrectangles of TR that in turn determine various reﬁnements
of R under T −1. We call such a reﬁnement of maximal entropy (with respect to
ν) greedy. In general, greedy reﬁnements are not unique, though subsequences
of them (corresponding to the result of greedily reﬁning all of the rectangles of
maximal ν-measure at a time) will be. Figures 11–12 illustrate greedy reﬁnements
for the Markov partitions in Fig. 10.
Fig. 11. (L) In black, we show the Markov partition RA from Fig. 10 in eigencoordi-
nates, with both the unit square and translates in gray. The forward image TRA is
shown in color. There are two rectangles of maximal ν-measure: the third and ﬁfth
from the top. (R) Greedily reﬁning each of the two rectangles of maximal ν-measure
by taking intersections as demarcated by bold black lines.
Fig. 12. (L) As in Fig. 11, but for R′
A. (R) The result of two greedy reﬁnements. Note
that the result of a round of greedy reﬁnements is not unique, though the value of the
resulting ν-entropy is.
These greedy reﬁnements stabilize the measures of rectangles. Detailed cal-
culations show that certain greedy reﬁnements of both RA and R′
A contain Lm+1
and Lm+2 rectangles of relative measure 1 and φ, respectively. Here the Lucas
28 Typically, μ will be singular with respect to ν, though for two-dimensional hyperbolic
toral automorphisms both measures are equal to Lebesgue measure.

Sampling and Statistical Physics via Symmetry
421
Fig. 13. Successive greedy reﬁnements for R′′
A.
numbers are deﬁned via Lm+2 = Lm+1 + Lm with L1 = 1, L2 = 3. However,
R′
A behaves diﬀerently, with maximally uniform greedy reﬁnements containing
Lm−1 and Lm rectangles of relative measure 1 and φ, respectively. Nevertheless,
in each of these three cases there is a common limit lim β/t∞≈0.2393, which
is probably minimal/universal for the cat map. In any event, while the detailed
measures of greedy reﬁnements depend on an initial Markov partition, we can
always consider an extremum over Markov partitions with decreasing size to
obtain (by construction) a unique physically natural result.
4.5
The Geodesic Flow on a Surface of Constant Negative
Curvature
The geodesic ﬂow on a surface of constant negative curvature is the archetypal
Anosov ﬂow [122,123]. It corresponds to the free particle Hamiltonian [87] H =
1
2m

jk gjkPjPk, where we use typical notation for the inverse of the metric
tensor and for momenta on the cotangent bundle. For a surface of constant
negative curvature, the geodesic ﬂow is mixing, and as we shall see the eﬀective
temperature is apparently insensitive to the surface genus.
We brieﬂy recall the details of the geodesic ﬂow in the Poincar´e disk model
following [124] (see also [125,126]). The diﬀerential arclength is ds = dr/(1 −r2),
and geodesics correspond to circular arcs intersecting S1 at right angles (see
Fig. 14). A surface of constant negative curvature can be obtained by identifying
pairs of edges sj of a hyperbolic polygon via maps Tj(sj) = s−1
σ(j). Here s−1
j
denotes the orientation reversal of sj, and the pairing σ is deﬁned along the lines
shown for the genus 2 case of Fig. 14.29 If there are 8g −4 edges, this procedure
yields a surface of genus g. Finally, the Hamiltonian is H = (1 −r2)2 · P 2/2m.
We can construct a Poincar´e or timing map and associated Markov partition
for the geodesic ﬂow. The ﬁrst step is to instantiate edge pairing maps Tj en
route to a map TR which will be the composition of the Poincar´e map and
an isometry. Then, we perform numerical calculations using the map TR and a
Markov partition R for it. The advantage of this construction is that “rectangles
29 Note that this pairing is not “twisted.”.

422
S. Huntsman
Fig. 14. (L) A surface with genus g = 2. For g ≥2 there are surfaces of constant
(or more generally global) negative curvature and whose geodesic ﬂows are mixing
(hence also ergodic). The construction of Adler and Flatto provides examples. For
g = 2, the corresponding 12-gon can be recovered by cutting along the indicated
paths. Incrementing g by adding a handle also requires adding two more geodesic
loops; one of the new loops and one of the old loops are separated into two arcs by
their intersections with neighboring loops, and cutting along these arcs yields eight
new edges. This construction underlies the pairing of edges of F, which is indicated
explicitly here for g = 2. (R) Model of the geodesic ﬂow for genus g = 2 with 12-gon
labels as indicated in the left panel. A sample trajectory is also indicated, with initial
condition given by the open marker close to the center of the ﬁgure. Blue and red
segments respectively indicate tangent directions to the ﬂow and 12-gon at the latter’s
boundary.
are rectangles,” even though TR is nonlinear. For example, Fig. 15 shows R,
TRR, and T 2
RR for the genus g = 2 case. From these Markov partitions, we
obtain reﬁnements R∨
m by intersecting rectangles in T 0
RR, . . . , T m
R R.
Although as with hyperbolic toral automorphisms we have μ = ν in this
situation, this common measure is substantially more complicated than Lebesgue
measure, viz.
μ([x1, x2] × [y1, y2]) =
& y2
y1
& x2
x1
|dx dy|
|eix −eiy|2 .
Consequently, the numerical computation of measures of rectangles in reﬁne-
ments of R is nontrivial. It turns out that β/t∞diverges nearly exponentially
for R∨
m, whereas for the cat map β/t∞provably converges due to linearity.
However, for greedy reﬁnements there is strong numerical evidence that
lim inf β is well-deﬁned, nonzero, ﬁnite, and independent of genus g (see Figs. 6
and 7). This is due not only to the convergence of β/t∞, but also to the fact
that the L2 mixing time of the geodesic ﬂow is 1/2 for g arbitrary.

Sampling and Statistical Physics via Symmetry
423
Fig. 15. The Markov partitions R, TRR, and T 2
RR for the g = 2 case. Note that rect-
angles become disconnected for T 2
RR.
4.6
Conclusion
As archetypal Anosov systems, the cat map and the geodesic ﬂow on a surface
of constant negative curvature are deeply relevant to statistical physics. These
and other Anosov systems exhibit Markovian symbolic dynamics that highlights
both chaotic properties and correspondences with spin systems. The chaotic
hypothesis seizes on these features to argue that Anosov systems are themselves
archetypal model statistical-physical systems. For example, taking copies of the
geodesic ﬂow of Sect. 4.5 with diﬀerent initial conditions yields an ideal gas, and
weakly coupling to this yields a thermometer.
As foreshadowed in Sect. 4.1, the results of [113] and structural stability of
Markov partitions for Anosov systems indicate that such a coupling leads to
eﬀects that vary analytically with the coupling strength. Moreover, the stability
of rapid mixing [119] gives at least one reason to expect that t∞will also behave
nicely as a function of coupling strength. SRB measures and mixing times for
Anosov systems are well-behaved in many examples, and it is reasonable to
expect this for physically relevant examples.
While of course Markov structures (i.e., partitions or sections) are not unique,
it is nevertheless evident that phenomena which hold for any Markov struc-
ture on an Anosov system are likely to be of relevance to statistical physics. In
this vein, the observed limiting behavior of β/t∞as calculated on greedy parti-
tions for both two-dimensional hyperbolic toral automorphisms and the geodesic
ﬂow is remarkable. While rigorously elaborating on this behavior would seem
to require the development of new and nontrivial mathematics, it nevertheless
appears to be generic.
Of particular importance are the implications that the evidence of this limit-
ing behavior has for a proposed general theory of nonequilibrium steady states.

424
S. Huntsman
We have argued here for a comprehensive framework for nonequilib-
rium statistical physics that simultaneously incorporates and extends the
formalism originally introduced by Ruelle and subsequently reﬁned by
Gallavotti, Cohen and others. The framework has as its goal a broad the-
ory of nonequilibrium statistical physics that is truly intrinsic: i.e., that
provides information about physical observables simply in terms of raw
temporal information about the dynamics.
One reason to consider a proposal of the sort described here, in which the
concept of (eﬀective) temperature plays the central role, is because there is
no generally accepted physical deﬁnition of entropy for non-equilibrium steady
states. We hope that the ideas discussed here will serve to elicit fruitful inves-
tigations into the fundamental nature of stationary physical systems far from
equilibrium.
Acknowledgements. I thank the editors for organizing the SPIGL Les Houches 2020
school as well as this volume. I am grateful to David Ford for initiating the development
of the theory in the earlier parts of Sect. 3; the applications that motivated it; and for
much else besides. I thank colleagues at IDA and NPS for many fruitful discussions;
and L for providing moral support. The ideas discussed here were originally developed
with support from NSA, ARDA, DARPA, Equilibrium Networks, and BAE Systems
FAST Labs.
References
1. Amey, C., Machta, J.: Phys. Rev. E 97(3), 033301 (2018)
2. Binder, K., Heermann, D.W.: Monte Carlo Simulation in Statistical Physics: An
Introduction. Springer (2019)
3. Gallavotti, G., Cohen, E.G.D.: J. Stat. Phys. 80(5–6), 931 (1995)
4. Gallavotti, G.: Statistical Mechanics: A Short Treatise. Springer (1999)
5. Durhuus, B., Fr¨ohlich, J.: Commun. Math. Phys. 75(2), 103 (1980)
6. Richey, M.: Am. Math. Mon. 117(5), 383 (2010)
7. Brooks, S., Gelman, A., Jones, G., Meng, X.L.: Handbook of Markov Chain Monte
Carlo. CRC (2011)
8. Neal, R.M.: arXiv preprint arXiv:1101.0387 (2011)
9. Delmas, J.F., Jourdain, B.: J. Appl. Probab. 46(4), 938 (2009)
10. Huntsman, S.: International Conference on Artiﬁcial Intelligence and Statistics,
pp. 2841–2851 (2020)
11. Br´emaud, P.: Markov Chains: Gibbs Fields, Monte Carlo Simulation, and Queues.
Springer (1999)
12. Peskun, P.H.: Biometrika 60(3), 607 (1973)
13. Onishchik, A.L., Vinberg, E.B.: Lie Groups and Algebraic Groups. Springer
(1990)
14. Kirillov, A.: An Introduction to Lie Groups and Lie Algebras, Cambridge (2008)
15. Robert, C.P., Elvira, V., Tawn, N., Wu, C.: Wiley Interdiscip. Rev.: Comput.
Stat. 10(5), e1435 (2018)

Sampling and Statistical Physics via Symmetry
425
16. Liu, J.S., Liang, F., Wong, W.H.: J. Am. Stat. Assoc. 95(449), 121 (2000)
17. Martino, L.: Digit. Signal Process. 75, 134 (2018)
18. Martino, L., Luengo, D., M´ıguez, J.: Independent Random Sampling Methods.
Springer (2018)
19. Calderhead, B.: Proc. Natl. Acad. Sci. 111(49), 17408 (2014)
20. Niepert, M.: Proceedings of the Twenty-Eighth Conference on Uncertainty in
Artiﬁcial Intelligence, pp. 624–633 (2012)
21. Niepert, M.: Proceedings of the Second Statistical Relational AI Workshop (2012)
22. Bui, H.H., Huynh, T.N., Riedel, S.: Proceedings of the Twenty-Ninth Conference
on Uncertainty in Artiﬁcial Intelligence, pp. 132–141 (2013)
23. Shariﬀ, R., Gy¨orgy, A., Szepesv´ari, C.: Artiﬁcial Intelligence and Statistics, pp.
866–874 (2015)
24. Broeck, G.V.D., Niepert, M.: Proceedings of the Twenty-Ninth AAAI Conference
on Artiﬁcial Intelligence, pp. 3599–3605 (2015)
25. Anand, A., Grover, A., Mausam, M., Singla, P.: Proceedings of the Twenty-Fifth
International Joint Conference on Artiﬁcial Intelligence, pp. 3560–3568 (2016)
26. Liu, J.S., Wu, Y.N.: J. Am. Stat. Assoc. 94(448), 1264 (1999)
27. Liu, J.S., Sabatti, C.: Biometrika 87(2), 353 (2000)
28. Saloﬀ-Coste, L.: Not. AMS 48(9), 968 (2001)
29. Ceccherini-Silberstein, T., Scarabotti, F., Tolli, F.: Harmonic Analysis on Finite
Groups. Cambridge (2008)
30. Sumner, J.G., Fern´andez-S´anchez, J., Jarvis, P.D.: J. Theor. Biol. 298, 16 (2012)
31. Suwa, H., Todo, S.: Phys. Rev. Lett. 105(12), 120603 (2010)
32. Chen, T.L., Hwang, C.R.: Stat. Probab. Lett. 83(9), 1956 (2013)
33. Bierkens, J.: Stat. Comput. 26(6), 1213 (2016)
34. Takahashi, K., Ohzeki, M.: Phys. Rev. E 93(1), 012129 (2016)
35. Frigessi, A., Hwang, C.R., Younes, L., et al.: Ann. Appl. Probab. 2(3), 610 (1992)
36. Pollet, L., Rombouts, S.M., Van Houcke, K., Heyde, K.: Phys. Rev. E 70(5),
056705 (2004)
37. Chen, T.L., Chen, W.K., Hwang, C.R., Pai, H.M.: SIAM J. Control Optim. 50(5),
2743 (2012)
38. Wu, S.J., Chu, M.T.: Linear Algebra Appl. 487, 184 (2015)
39. Huang, L.J., Liao, Y.T., Chen, T.L., Hwang, C.R.: SIAM J. Control Optim. 56(4),
2977 (2018)
40. Johnson, J.E.: J. Math. Phys. 26(2), 252 (1985)
41. Poole, D.G.: Am. Math. Mon. 102(9), 798 (1995)
42. Boukas, A., Feinsilver, P., Fellouris, A.: Random Oper. Stoch. Equ. 23(4), 209
(2015)
43. Guerra, M., Sarychev, A.: arXiv preprint arXiv:1805.07299 (2018)
44. Hilgert, J., Neeb, K.H.: Lie Semigroups and their Applications. Springer (1993)
45. Dawson, C., Nielsen, M.: Quantum Inf. Comput. 6(1), 81 (2006)
46. Casas, F., Murua, A., Nadinic, M.: Comput. Phys. Commun. 183(11), 2386 (2012)
47. Horn, R.A., Johnson, C.R.: Matrix Analysis, 2nd edn. Cambridge (2012)
48. Earl, D.J., Deem, M.W.: Phys. Chem. Chem. Phys. 7(23), 3910 (2005)
49. Bolthausen, E., Bovier, A.: Spin Glasses. Springer (2007)
50. Panchenko, D.: J. Stat. Phys. 149(2), 362 (2012)
51. Conrad, P.R., Davis, A.D., Marzouk, Y.M., Pillai, N.S., Smith, A.: SIAM/ASA
J. Uncertain. Quantif. 6(1), 339 (2018)
52. Dashti, M., Stuart, A.M.: Handbook of Uncertainty Quantiﬁcation. In: Ghanem,
R., Higdon, D., Owhadi, H. (eds.) Springer (2017)

426
S. Huntsman
53. Martino, L., Leisen, F., Corander, J.: arXiv preprint arXiv:1409.0051 (2014)
54. Khachiyan, L.: Advances in Convex Analysis and Global Optimization, pp. 105–
118. Springer (2001)
55. Khachiyan, L., Boros, E., Borys, K., Elbassioni, K., Gurvich, V.: Discret. Comput.
Geom. 39(1), 174 (2008)
56. Faddeev, D.K.: Uspekhi Matematicheskikh Nauk 11(1), 227 (1956)
57. Baez, J.C., Fritz, T., Leinster, T.: Entropy 13(11), 1945 (2011)
58. Gallavotti, G.: Eur. Phys. J. B 61(1), 1 (2008)
59. Tool, A.Q.: J. Am. Ceram. Soc. 29(9), 240 (1946)
60. Nieuwenhuizen, T.M.: Phys. Rev. Lett. 80(25), 5580 (1998)
61. Leuzzi, L., Nieuwenhuizen, T.M.: Thermodynamics of the Glassy State. Taylor &
Francis (2007)
62. Cugliandolo, L.F.: J. Phys. A: Math. Theor. 44(48), 483001 (2011)
63. Puglisi, A., Sarracino, A., Vulpiani, A.: Phys. Rep. 709, 1 (2017)
64. Rugh, H.H.: Phys. Rev. Lett. 78(5), 772 (1997)
65. Rugh, H.H.: J. Phys. A: Math. Gen. 31(38), 7761 (1998)
66. Huntsman, S.: arXiv preprint arXiv:1009.2127 (2010)
67. Ford, D.: arXiv preprint cond-mat/0510291 (2005)
68. Ford, D., Huntsman, S.: Phys. A: Stat. Mech. Appl. 365(1), 34 (2006)
69. Braun, S., Ronzheimer, J.P., Schreiber, M., Hodgman, S.S., Rom, T., Bloch, I.,
Schneider, U.: Science 339(6115), 52 (2013)
70. Dunkel, J., Hilbert, S.: Nat. Phys. 10(1), 67 (2014)
71. Frenkel, D., Warren, P.B.: Am. J. Phys. 83(2), 163 (2015)
72. Jaynes, E.T.: Phys. Rev. 106(4), 620 (1957)
73. Buckingham, E.: Phys. Rev. 4(4), 345 (1914)
74. Barenblatt, G.: Scaling, Cambridge (2003)
75. Goldstein, H., Poole, C., Safko, J.: Classical Mechanics, 3rd edn. Addison-Wesley
(2001)
76. Gallavotti, G., Verboven, E.: Il Nuovo Cimento B (1971–1996) 28(1), 274 (1975)
77. Parisi, G.: Statistical Field Theory. Perseus (1998)
78. Bratteli, O., Robinson, D.W.: Operator algebras and quantum statistical mechan-
ics: Volume 1: C*-and W*-Algebras. Symmetry Groups. Decomposition of States.
Springer (2012)
79. Connes, A., Rovelli, C.: Class. Quantum Gravity 11(12), 2899 (1994)
80. Martinetti, P., Rovelli, C.: Class. Quantum Gravity 20(22), 4919 (2003)
81. Rovelli, C.: Class. Quantum Gravity 10(8), 1567 (1993)
82. Tian, Y.: J. High Energy Phys. 2005(06), 045 (2005)
83. Rovelli, C., Smerlak, M.: Class. Quantum Gravity 28(7), 075007 (2011)
84. Reiss, H.: Methods of Thermodynamics. Dover (1997)
85. Choquet-Bruhat, Y., DeWitt-Morette, C., Dillard-Bleick, M.: Analysis, Mani-
folds, and Physics. North-Holland (1977)
86. Chazottes, J.R., Fernandez, B.: Dynamics of Coupled Map Lattices and of Related
Spatially Extended Systems. Springer (2005)
87. Collet, P., Epstein, H., Gallavotti, G.: Commun. Math. Phys. 95(1), 61 (1984)
88. Saussol, B.: Rev. Math. Phys. 21(08), 949 (2009)
89. Barrera, J., Lachaud, B., Ycart, B.: Stoch. Process. Appl. 116(10), 1433 (2006)
90. Schwarz, G.: Rev. Mod. Phys. 40(1), 206 (1968)
91. Jaynes, E.T.: Maximum-Entropy and Bayesian Methods. In: Erickson, G., Neu-
dorfer, P., Smith, C. (eds.) Springer (1992)
92. Katok, A., Hasselblatt, B.: Introduction to the Modern Theory of Dynamical
Systems, vol. 54. Cambridge (1997)

Sampling and Statistical Physics via Symmetry
427
93. Richeson, D., Wiseman, J.: Topol. Appl. 156(2), 251 (2008)
94. Huntsman, S.: arXiv preprint arXiv:0904.3881 (2009)
95. Burgess, M.: Phys. Rev. E 62(2), 1738 (2000)
96. Gentile, G.: Forum Math. 10(1), 89 (1998)
97. Ford, D.: arXiv preprint cond-mat/0601387 (2006)
98. Boccaletti, S., Kurths, J., Osipov, G., Valladares, D., Zhou, C.: Phys. Rep. 366(1–
2), 1 (2002)
99. Acebr´on, J.A., Bonilla, L.L., Vicente, C.J.P., Ritort, F., Spigler, R.: Rev. Mod.
Phys. 77(1), 137 (2005)
100. Dorﬂer, F., Bullo, F.: SIAM J. Control Optim. 50(3), 1616 (2012)
101. Cohen, E.: Phys. A: Stat. Mech. Appl. 305(1–2), 19 (2002)
102. Cohen, E.: Boltzmann’s Legacy. In: Gallavotti, G., Reiter, W., Yngvason, J. (eds.)
EMS (2008)
103. Chang, H.: Inventing Temperature. Oxford (2004)
104. Bowen, R.: Springer Lect. Notes Math 470, 78 (1975)
105. Chernov, N.: Handbook of Dynamical Systems. In: Katok, A., Hasselblatt, B.
(eds.) vol. 1A, pp. 321–407, North-Holland (2002)
106. Jiang, D.Q., Jiang, D., Qian, M.: Mathematical Theory of Nonequilibrium Steady
States. Springer (2004)
107. Young, L.S.: J. Stat. Phys. 108(5–6), 733 (2002)
108. Dolgopyat, D.: Ann. Math. 147(2), 357 (1998)
109. Liverani, C.: Ann. Math. 159, 1275–1312 (2004)
110. Butterley, O., War, K.: J. Eur. Math. Soc. 22(7), 2253 (2020)
111. Tsujii, M., Zhang, Z.: arXiv preprint arXiv:2006.04293 (2020)
112. Wilkinson, A.: Encyclopedia of Complexity and Systems Science. In: Meyers,
R.A., Kra, B. (eds.) Springer (2009)
113. Amaricci, A., Bonetto, F., Falco, P.: J. Math. Phys. 48(7), 072701 (2007)
114. Evans, D.J., Sarman, S.: Phys. Rev. E 48(1), 65 (1993)
115. Bonetto, F., Gallavotti, G., Giuliani, A., Zamponi, F.: J. Stat. Phys. 123(1), 39
(2006)
116. Gallavotti, G.: Chaos: Interdiscip. J. Nonlinear Sci. 19(1), 013101 (2009)
117. Gallavotti, G., Presutti, E.: J. Math. Phys. 51(1), 015202 (2010)
118. Gallavotti, G., Presutti, E.: J. Math. Phys. 51(5), 053303 (2010)
119. Field, M., Melbourne, I., T¨or¨ok, A.: Ann. Math. 166, 269–291 (2007)
120. Ruelle, D.: Thermodynamic Formalism. Addison-Wesley (1978)
121. Beck, C., Sch¨ogl, F.: Thermodynamics of Chaotic Systems: An Introduction. Cam-
bridge (1995)
122. Anosov, D.V.: Geodesic Flows on Closed Riemann Manifolds with Negative Cur-
vature. American Mathematical Society (1969)
123. Klingenberg, W.: Ann. Math. 99, 1–13 (1974)
124. Adler, R., Flatto, L.: Bull. (New Ser.) Am. Math. Soc. 25(2), 229 (1991)
125. Bowen, R., Series, C.: Publ. Math´ematiques de l’Institut des Hautes ´Etudes Sci-
entiﬁques 50(1), 153 (1979)
126. Gutzwiller, M.: Chaos in Classical and Quantum Mechanics. Springer (1990)

A Practical Hands-on for Learning Graph
Data Communities on Manifolds
Thomas Gerald1, Hadi Zaatiti2(B), and Hatem Hajri2
1 Science Sorbonne University, Lip6, 75005 Paris, France
thomas.gerald@lip6.fr
2 Institute of Research and Technology SystemX,
8 avenue de la Vauve, 91120 Palaiseau, France
{hadi.zaatiti,hatem.hajri}@irt-systemx.fr
Abstract. Learning graph-structured data with hyperbolic geometry
has received considerable attention in the last years. This paper reviews
recent approaches based on hyperbolic embeddings, Riemannian K-
means and Expectation Maximisation algorithms for supervised and
unsupervised learning problems on graphs. The presentation is aimed
to provide a practical and technical support for users interested in
this topic. It is focused on illustrative examples, visualisations and
Pytorch codes which are commented in details (The package implement-
ing the algorithms is available online from https://github.com/tgeral68/
HyperbolicGraphAndGMM).
1
Introduction
Hyperbolic embedding (HE) [9,18] of symbolic data has proven successful in
numerous applications such as Natural Language Processing [6,12], neural net-
works [7], node classiﬁcation [5] and image processing [13]. Recently, [8] intro-
duces an extension of HE called Riemannian community embedding (RComE)
that embeds communities instead of individual nodes in Hyperbolic manifolds.
RComE is also a Riemannian counterpart to the Euclidean ComE framework
[4]. It has been shown useful for visualisation, node classiﬁcation and learning
communities on graphs and has been compared to ComE in [8]. This chapter is
designed as a tutorial on RComE. It aims to give a practical support to the reader
by discussing in depth the original RComE code and also providing several illus-
trative examples. It is organised as follows. Section 2 presents a quick review of
RComE. Section 3 provides the general structure of the package. Section 4 con-
tains the necessary operations and metrics used in the algorithms. Finally, Sect. 5
presents a step-by-step tutorial providing relevant examples while detailing the
associated code.
c
⃝The Author(s), under exclusive license to Springer Nature Switzerland AG 2021
F. Barbaresco and F. Nielsen (Eds.): SPIGL 2020, PROMS 361, pp. 428–459, 2021.
https://doi.org/10.1007/978-3-030-77957-3_21

A Practical Hands-on for Learning Graph Data Communities on Manifolds
429
2
Riemannian Community Embedding (RComE)
The RComE approach embeds a graph of communities on a hyperbolic manifold
while trying to preserve the structure of communities. The latter are modelled in
the embedding space by mixtures of Riemannian Gaussian distributions [21]. To
capture communities, the learning process involves a trade-oﬀbetween learning
a classical graph embedding and ﬁtting nodes to a mixture of Gaussian distribu-
tions; where each component models a community. To learn those embeddings,
we make use of recent Riemannian optimisation methods and minimise diﬀer-
ent objective functions. The main steps of this process are summarised through
Algorithm 1.
For the moment, we omit detailing this algorithm. In the next sections, we
proceed step by step in a tutorial fashion and provide necessary deﬁnitions and
theoretical background when required.
3
Code Structure and Dependencies
The provided readme.md details the required dependencies and steps for initial
conﬁgurations. Table 1 gives more descriptions of each module.

430
T. Gerald et al.
Table 1. Short description of the main (sub-)modules within the rcome package.
Module
Sub-module
Description
clustering tools
Clustering algorithms on Riemannian
manifolds
poincare em
-contains EM algorithm for learning mixture
models on the Poincar´e ball model
poincare kmeans -contains K-means algorithms on the
Poincar´e ball
community tools
Classiﬁcation algorithms on Riemannian
manifolds (Logistic regression)
data tools
Tools for loading, organising data and
corpus
embedding tools
Deﬁnition of algorithms for learning
representations and loss functions
losses
Deﬁnition of loss functions used in the
chapter
evaluation tools
Modules for evaluating performances of
supervised/unsupervised algorithms
functions tools
Functions needed to performs
learning/evaluation
manifold
Deﬁnition of the manifold (Euclidean,
Hyperbolic)
manifold
Abstract class to implement on manifolds
poincare ball
Contains speciﬁc operations on the Poincar´e
ball
euclidean
Contains speciﬁc operations on the
Euclidean space
optim tools
Riemannian optimisation algorithms
rsgd
Riemannian stochastic gradient descent
visualisation tools
Various tools for plotting
We also propose a list of examples available in the main folder named exam-
ple. To run these examples, you need to ﬁrst run the bash conﬁg script config -
and download.sh provided in the repository. For further details, please consider
reading the readme.md ﬁle.
4
Implementing Manifold Speciﬁc Metrics and Operations
In this paper, Algorithm 1 is implemented on a speciﬁc hyperbolic manifold
which is the Poincar´e ball, similarly to [18]. Figure 1 provides an artistic and
intuitive visualisation of this model in dimension two. In particular, we can

A Practical Hands-on for Learning Graph Data Communities on Manifolds
431
observe a sequence of repeated patterns which are more compressed near the
boundary.
This section reviews the main mathematical tools and operations on the
Poincar´e ball that are used in the algorithms. We focus on the implementations
of the Riemannian distance function, exponential and logarithmic maps. All the
presented operations are illustrated through practical examples. Additionally,
we show how the Riemannian Gaussian distributions are coded. We recall that
we use Pytorch and Numpy as backends which provide intuitive ways to easily
implement mathematical equations in Python.
Fig. 1. Visualisation of hyperbolic space provided by Pypi Hyperbolic package: the
compression of the space increases when moving from the center towards the boundary.
4.1
Metric
The Poincar´e ball model of m dimensions is the manifold Bm = {x ∈Rm :
||x|| < 1} equipped with the Riemannian metric:
gBm
x
=
4
(1 −||x||2)2 gE
where gE is the Euclidean scalar product. The Riemannian distance between two
points x, y ∈Bm, induced by this metric is given by:
d(x, y) = cosh−1

1 + 2
||x −y||2
(1 −||x||2)(1 −||y||2)

(1)
Script 1.1, taken from example/distance.py, shows how to compute the
distance between two random points in B2.

432
T. Gerald et al.
1 from
rcome.manifold
import
poincare_ball
2
3 # points in the ball
4 point_a = torch.rand (1,2) - 0.5
5 point_b = torch.rand (1,2) - 0.5
6
7 # select the
manifold
8 pb_manifold = poincare_ball . PoincareBallExact
9
10 # access to distance
through the
manifold
class
11 distance_a_b = pb_manifold.distance(point_a , point_b)
Script 1.1. Computation of the Riemannian distance on the Poincar´e disk.
4.2
Exponential and Logarithmic Maps
The manifold Bm equipped with the presented metric is a hyperbolic space [1,10].
[7] gave explicit expressions for the Riemannian exponential and logarithmic
maps. Intuitively, the exponential map is a generalisation to manifolds of the
addition of a point and a vector. The addition of a point x with a vector y
tangent at x, results in a point z belonging to the geodesic starting from x with
initial velocity y. The logarithmic map is simply the inverse of the exponential
map. Given x and y, it returns the tangent vector from x aimed towards y. Let us
now recall the explicit expressions of these maps. First, deﬁne M¨obius addition
⊕for x, y ∈Bm as:
x ⊕y = (1 + 2⟨x, y⟩+ ||y||2)x + (1 −||x||2)y
1 + 2⟨x, y⟩+ ||x||2||y||2
For x ∈Bm and y ∈Rm \ {0}, the exponential map is deﬁned as:
Expx(y) = x ⊕

tanh

||y||
1 −||x||2

y
||y||

(2)
Expx is a diﬀeomorphism from Rm to Bm. The logarithmic map is deﬁned
for all x, y ∈Bm, x ̸= y by:
Logx(y) = (1 −||x||2)tanh−1 (|| −x ⊕y||)
−x ⊕y
|| −x ⊕y||
(3)
Script 1.2 shows the computation of Expx, Logx (available under example/
exp log.py).
1 from
rcome.manifold
import
poincare_ball
2
3 # a point in the ball
4 point = torch.rand (1,2) - 0.5
5 # a vector
6 vector =
torch.rand (1,2)

A Practical Hands-on for Learning Graph Data Communities on Manifolds
433
7
8 pb_manifold = poincare_ball . PoincareBallExact ()
9
10 projected_point = pb_manifold. riemannian_exp(point , vector)
11 vector_2 = pb_manifold. riemannian_log(point ,
projected_point )
Script 1.2. Calling the Exp and Log maps.
4.3
Gaussian Distributions on Hyperbolic Spaces
Several works proposed extensions of the Gaussian distribution to manifolds
[19,21,22]. In the provided code, we use and implement the Riemannian Gaussian
distributions as formalised in [21].
Given two parameters μ ∈Bm and σ > 0, respectively interpreted as theoret-
ical mean (or barycenter) and standard deviation, the (Riemannian) Gaussian
distribution G(μ, σ) on Bm, is given by its density:
p(x|μ, σ) =
1
ζm(σ) exp

−d2(x, μ)
2σ2

with respect to the volume form on Bm. An explicit expression of the normalising
factor ζm(σ) has been given recently in [14] as follows:
ζm(σ) = 2π
d
2
Γ( d
2)
π
2
σ
2m−1
m−1

k=0
(−1)kCk
m−1e
p2
kσ2
2

1 + erf(pkσ
√
2 )

with pk = (m −1) −2k.
To illustrate the Gaussian distribution on the disk, we evaluate and plot
its density at random points in Fig. 2. The associated code can be found in
example/gaussian distributions.py.
1 from
rcome. function_tools
import
distribution_function as
df
2
3 # Define the
barycenter mu and
variance
sigma
4 mu = torch.Tensor ([[0.7 ,
0.7]])
5 sigma = torch.Tensor ([2.2])
6
7 # Precompute
the
normalisation
factor
8 norm_factor = df. ZetaPhiStorage(
9
torch.arange (5e-2, 2., 0.001) , 2)
10
11 # Sample a number of points on the
manifold
12 points = (torch.rand (200000 , 2) - 0.5) * 2
13 points = points[points.norm(2,
-1) < 0.9999]
14

434
T. Gerald et al.
15 # Compute the
probability
density of each point
16 probs = df.gaussianPDF(points , mu , sigma ,
17
norm_func=norm_factor.zeta)
Script 1.3. Deﬁning the Gaussian distribution density function on the Poincar´e disk.
Fig. 2. Plot resulting from Script 1.3: barycenter marked as a yellow star, with three
regions representing high, medium and low Gaussian probability distribution coloured
respectively in green, blue and red.
5
Tutorial on RComE
In this part, we show step by step how to learn communities on graph data using
hyperbolic embeddings, Riemannian K-means and Expectation-Maximisation
algorithms. First, we review the Riemannian Gradient Descent (RGD) algorithm
which is illustrated with a code example. Then, a hyperbolic embedding of a tree-
like hierarchical data is presented followed by a hyperbolic embedding of a graph
dataset. Lastly, we show how to extend the embedding process of individual
nodes by taking into account, additionally, the structure of communities.
5.1
Riemannian Gradient Descent Algorithms
Diﬀerent methods have been proposed previously to optimise objective functions
on hyperbolic or more generally Riemannian manifolds. Most of these methods
rely on Riemannian gradient descent algorithms, the exponential and logarithmic
maps. In some cases and to avoid costly computations, such as in [18], the expo-
nential map may be replaced with a ﬁrst order approximation called retraction.
Let us consider a function:
f ◦d: Bm × Bm →R
(x, y) 
→f(d(x, y)).
with d the Riemannian distance deﬁned in Eq. 1. We can compute the Euclidean
gradient δf(d(x,y))
δx
using the chain rule as follows:

A Practical Hands-on for Learning Graph Data Communities on Manifolds
435
∇Ef(d(x, y)) := δf(d(x, y))
δx
= δf(d(x, y))
δd(x, y)
δd(x, y)
δx
(4)
We recall the following derivative from [18]:
δd(x, y)
δx
=
4
β

γ2 −1
||x||2 −2x.yt + 1
α2
x −x
α

(5)
with α = 1 −||x||2, β = 1 −||y||2 and γ = 1 +
2
αβ ||x −y||2. In the
associated code, the computation of the derivative (4) is done using Pytorch
automatic diﬀerentiation and generally does not require explicit formulae
of the derivation. This computation can be found in the class “Poincar-
eBall.PoincareDistance.grad” under the rcome/manifold/poincare ball.py
module. To apply RGD, we need to compute ∇Rf(d(x, y)) ∈TxB and for this
use the identity ([18]):
∇Rf(d(x, y)) = (1 −||x||2)2
4
∇Ef(d(x, y))
(6)
For optimisation on a given manifold, we introduced a python class called
Manifold which deﬁnes the main operations such as the distance, logarithmic,
exponential maps and the transformation between the Euclidean and Rieman-
nian gradients.
Instanciations of the Manifold class are then provided as input and used by
algorithms such as the RSGD (refer to Script 1.4 for an example).
For the Poincar´e ball model, two diﬀerent classes are provided: the “Poincare-
BallApproximation” class which considers a retraction instead of the usual expo-
nential map. Thus its exponential method implements the retraction instead
of the real exponential map deﬁned in 2. Using this class within the gradient
descend (rcome.optim tools.rsgd.RSGD) will lead to the following parameter
update:
θt+1 = P(θt −η∇Rf(d(θt, y)))
(7)
with P being the projection :
P(x) =
	
x
||x||2+ϵ if ||x|| ≥1
x
otherwise
(8)
The second update approach is used by default when the manifold is set to
“PoincareBallExact” [3]. It applies:
θt+1 = Expθt(−η∇Rf(d(θt, y))))
(9)
with η the learning rate or gradient step coeﬃcient being a small scalar value.
Using Stochastic Gradient Descent Optimisation Methods
To implement the Riemannian Stochastic Gradient Descent (RSGD), we use
the Pytorch framework syntax to redeﬁne the optimiser within the module

436
T. Gerald et al.
rcome/optim tools/rsgd given a manifold. All examples and experiments are
trained using scripts similar to Script 1.4 which provides the skeleton of the code
and is available under example/riemannian gradient descent.py
The skeleton contains three parts: 1) Data loading; 2) Initialisation of the
model, optimiser and manifold; 3) Learning loop associated with the RSGD.
1 from
torch
import nn
2 from
torch.utils.data
import
DataLoader
3
4 from
rcome.optim_tools.rsgd
import
RSGD
5
6 # Loading
dataset
7 X, Y = data_loader.load_corpus(dataset_name , directed)
8 # Define
organisation of data
9 ...
10
11 # Otpimiser
instanciation
12 manifold = PoincareBallExact
13 optimiser = RSGD(model_parameters , learning_rate , manifold)
14
15 # Learning
loop via RSGD
16 for i in range( total_iteration ):
17
for x, y in dataloader:
18
19
optimiser.zero_grad ()
20
# define the loss
21
...
22
# compute
euclidean
gradient
23
loss.backward ()
24
# transform to riemannian
gradient
and
update
embeddings
25
optimiser.step ()
Script 1.4. Optimisation via Riemannian Stochastic Gradient Descent (RSGD).
5.2
An Example of Embedding Hierarchical Data
In this section, we propose an example of hierarchical data embedding by con-
sidering tree-like representations. We start by importing the necessary modules
from Pytorch, the provided rcome module and then import a part of the mam-
mals sub-tree from Wordnet [15] as shown in Script 1.5.
1 from
torch.utils.data
import
DataLoader
2
3 from
rcome.data_tools
import
collections
4 from
rcome.data_tools.corpora
import
DatasetTuple
5
6 # load data
7 root_synset = ’mammal.n.01 ’

A Practical Hands-on for Learning Graph Data Communities on Manifolds
437
8 X, dictionary , values = collections.animals(root=
root_synset)
9
10 # create
dataset
11 dataset = DatasetTuple(X)
12 dataloader = DataLoader(dataset , batch_size =5)
Script 1.5. Importing necessary modules and the mammals sub-tree from Wordnet.
In [18], embedding hierarchical data is performed while optimising the loss func-
tion:
L =

(x,y)∼Drelation
−log
⎛
⎜
⎜
⎜
⎝
e−d(rx,ry)
e−d(rx,ry) +
M

i=0,z∼Dnodes
e−d(rx,rz)
⎞
⎟
⎟
⎟
⎠
(10)
with Drelation the set of observed hypernymy relations between pairs. Dnodes
being the uniform probability distribution over nodes, M is the number of neg-
ative samples and rx, ry, rz the representation of nodes x, y, z.
Our implementation of the hierarchical embedding oﬀers the possibility to
choose between the two gradient descent approaches “exact” and “retraction”
mentioned at the end of Sect. 5.1. Figure 3 illustrates the obtained embeddings
with the “exact” gradient descent approach for M = 2, 10. The script replicat-
ing this experiment can be found under example/wordnet/hierarchical ani-
mals.py and a part of it is depicted in the code 1.6.
Fig. 3. Learning hierarchical representation oincar´e disc via RSGD for two values of
M.
Experimentally, we observe that increasing M leads to push embedded nodes
towards the manifold boundary, while a small value leads to a conﬁguration

438
T. Gerald et al.
which is more concentrated near the origin. Indeed, when M increases, each loss
function update causes that more nodes are pushed away from their negatively
sampled nodes. After several updates, embedded nodes and negatively sampled
ones are located at opposite boundaries of the space.
1
2 import
torch
3 from
torch
import nn
4
5 from
rcome.manifold.poincare_ball
import
PoincareBallExact
6 from
rcome.optim_tools.rsgd
import
RSGD
7
8 # Create the model and
initialise
weights
9 model = nn.Embedding(len(dictionary)+1, 2, max_norm =0.999)
10 model.weight.data [:] = model.weight.data * 1e-2
11
12 # Select the
manifold
and the
optimisation
method
13 manifold = PoincareBallExact
14 optimizer = RSGD(model.parameters (), 2e-1, manifold=
manifold)
15
16 # define
criterion (loss
function)
17 default_gt = torch.zeros (20).long ()
18 criterion = nn. CrossEntropyLoss (reduction="sum")
19
20 # Inside
learning
loop
21
optimiser.zero_grad ()
22
# get
embeddings
23
pe = model(x.long ())
24
# sample
negative
nodes
uniformly
25
ne = model (( torch.rand(len(x), 10) * len(dictionary
)).long () + 1)
26
27
# compute
distances
28
pd = manifold.distance(pe[:,0,:], pe[: ,1 ,:]).
unsqueeze (1)
29
nd = manifold.distance(pe[: ,0 ,:]. unsqueeze (1).
expand_as(ne), ne)
30
pred = -torch.cat((pd , nd), 1)
31
32
# compute the loss
33
loss = criterion(pred , default_gt [:len(pred)])
34
35
# update
embeddings
36
loss.backward ()
37
optimiser.step ()
Script 1.6. Example of learning hierarchial representation of data using Wordnet
hyperyms.

A Practical Hands-on for Learning Graph Data Communities on Manifolds
439
Although the latter approach is appropriate for hierarchical data, the consid-
ered loss function is not convenient to embed arbitrary graphs and also to learn
communities. Its adaptation to these settings is the subject of the next sections.
5.3
Preparing Graph Data: Synthetic Graphs and Real World Data
In this section, we take a closer look at some graph datasets. A graph is denoted
G(V, E) with V the set of vertices and E the edges which correspond to a set of
node couples sharing an edge. In Table 2, we present the available datasets with
the provided code and their characteristics.
Table 2. Characteristics of the datasets available with the code with N the number of
nodes, E the number of edges, K the number of communities. The multi-communities
label inform the reader that each node of the graph may belong to many communities.
Graph dataset name
N
E
K
Football
115
613
12
Karate
34
77
2
LFR1
800
5204
13
DBLP
13,184 48,018
5
Wikipedia (multi-communities)
4,777
184,812 40
BlogCatalog (multi-communities) 10,312 333,983 39
We can classify those datasets into diﬀerent categories: 1) Small size graphs:
Football, Karate; 2) Synthetic graphs: LFR1 generated from [11] by the algorithm
given below; 3) Large-scale graphs: DBLP, Wikipedia and BlogCatalog.
In this chapter we mainly consider the synthetic graph LFRA to provide
understandable and easily optimisable examples. We also show few examples on
the DBLP graph to demonstrate the scalability of the code.
Generating synthetic graphs
We use the LFR benchmark [11] to generate a graph with a predeﬁned number
of communities K and number of nodes N. The rcome/data tools/graph -
generator.py module provides code for generating community graphs using
LFR. The number of desired nodes is set as the maximum/minimum number
of nodes per community. Figure 4 is an example of a two communities graph
generated from the module (K = 2 and N = 150). Moreover the connectivity
inside communities (the ratio of edges shared between nodes which belong to
a same community) can be parametrised by the algorithm. Once a graph is
generated, two ﬁles are saved; the ﬁrst one contains the community assignment
of each node and the second one is the graph adjacency matrix. This dataset
can now be used for testing.

440
T. Gerald et al.
Fig. 4. Generated LFR graph with 150 nodes and two communities (black and yellow).
Real-world networks
One of the commonly used low-scale datasets is the Karate club graph with 34
nodes and two communities which is illustrated in Fig. 5.
Another dataset which will be considered is an extract of DBLP [24]1 dataset
with 13,184 nodes and 48,018 edges and 5 communities. Each node corresponds
to a scientiﬁc publication and each edge represents whether or not one of the
papers cites the other one. Each community represents the research area of
papers. Figure 6 illustrates the size ratio of the 5 communities of the graph.
Loading datasets
Script config and download.sh must ﬁrst be run in order to download the
deﬁned datasets. The method load corpus loads datasets and returns the graph
edges and communities. Script 1.7 shows how to import edges and labels for three
dataset examples.
1 from
rcome.data_tools
import
data_loader
2
3 lfr_edges , lfr_communities = data_loader.load_corpus("LFR1"
)
4 karate_edges , karate_communities = data_loader.load_corpus(
"karate")
5 dblp_edges , dblp_communities = data_loader.load_corpus("
dblp")
Script 1.7. Script for importing DBLP; LFR; and Karate datasets.
1 https://www.aminer.org/billboard/aminernetwork.

A Practical Hands-on for Learning Graph Data Communities on Manifolds
441
Fig. 5. Karate club graph dataset with 34 nodes and two communities (green and
yellow).
Fig. 6. Communities distribution of the DBLP graph dataset.
5.4
The First Order Loss
The ﬁrst-order proximity L1 loss function is expressed as :
L1 =

(x,y)∈E
−log(σ(−d(rx, ry)))
(11)

442
T. Gerald et al.
where σ(x) = (1 + e−x)−1 is the sigmoid function, rx ∈Bm is the embedding
of node x ∈V . This expression allows getting nodes sharing edges to have close
embeddings. However, it does not allow nodes that do not share edges to have
distant embeddings. Therefore, the previous function can be reﬁned to:
L′
1 =

(x,y)∈E
−
⎛
⎝log(σ(−d(rx, ry))) +

(x,z)∼{(x,z)/∈E}
log(σ(d(rx, rz)))
⎞
⎠
(12)
Fig. 7. Learning graph representation: A comparison between hierarchical loss and ﬁrst
order loss. Each point represents a node embedding, and each colour is a true known
label.
We now propose to compare both losses in embedding graph structured data:
the hierarchical loss function from Eq. 10 and the ﬁrst-order proximity loss from
Eq. 12. Figures 7 and 8 visually compare the obtained results. We can see that
the hierarchical loss seems to capture communities similarly as the ﬁrst order loss
function. This is visually perceived by the appearance of clusters with distinct
colours (i.e.; the embedding allowed to form correct clusters according to their
true labels).
However, we show that the hierarchical loss struggles to capture communities
of larger/more complex graphs, this is the case for the DBLP dataset shown in
Fig. 8 (left). Nevertheless, the embedding via the ﬁrst-order loss function seems
to visually better handle the DBLP graph complexity and provide a more faithful
representation.
5.5
The Second Order Loss: Context and Negative Sampling
The-state-of-the-art community learning approach of graph data on Euclidean
spaces, called ComE [4], proposes, additionally to the ﬁrst-order loss function,

A Practical Hands-on for Learning Graph Data Communities on Manifolds
443
Fig. 8. Learning graph representation a comparison between hierarchical and ﬁrst order
loss functions for DBLP (example/DBLP/).
to consider the context of each node in the embedding process. The context
of a given node is deﬁned as the set of nodes that are close to it according to
the graph distance. Formally, a context ci of some node i, is deduced from the
set of nodes contained in a random path passing from i. In the associated code,
the random paths are pre-generated. The object RandomContextSizeFlat, given
some data, generates contexts of nodes as shown in Script 1.8, available under
example/lfr/lfr get context.py
1 from
rcome.data_tools
import
data_loader , corpora
2
3 X, Y = data_loader.load_corpus("LFR1", directed=False)
4
5 # Size of the
dataset
precompute * path_len
6 dataset = corpora. RandomContextSizeFlat (
7
X, Y, precompute =2, path_len =10, context_size =3)
8 # Tuples (node , context) for the
context of path 1 position
1
9 tuple_node_context = dataset [1]
Script 1.8. An example of generating the context of a node for the LFR graph dataset
available under example/lfr/lfr get context.py
To preserve the context ci of each node embedding ri, we propose to optimise
a second order proximity loss function L2 such that:
L2 =

x∈V

y∈cx
−

log(σ(−d(rx, r′
y))) +

z∼D
log(σ(d(rx, r′
z)))

(13)

444
T. Gerald et al.
Fig. 9. A comparison between two second-order proximity preserving strategies: left
(example/LFR/lfr second order joint), same embedding for nodes and their contexts
and right (example/LFR/lfr second order.py), diﬀerent embeddings for the nodes on
one hand and the context and negative samples on another hand.
where D is a distribution over nodes, here taken uniform, and rx is the embedding
of a node x. During optimisation, two strategies emerge for the embedding r′
i of
the node i when i is considered as a context of some other node, i.e., i ﬁgures in
the second sum of L2.
The ﬁrst strategy consists of considering the same embeddings for nodes and
context, in this case ri = r′
i. The second is to consider diﬀerent representations
for context and node embeddings, in which case ri ̸= r′
i. The second approach
is the one considered in state of the art Euclidean methods such as ComE [4],
DeepWalk [20] or LINE [23].
Figure 9 visually illustrates the diﬀerences between the two strategies. It
seems that having a diﬀerent embedding for the context and node allows better
separation of clusters according to the true node labels. In future sections, we will
present and show metrics that can quantify cluster separability performances,
thus providing accurate measurements used for making comparisons between
diﬀerent strategies.
In the experiments of the previous sections, we selected negative nodes uni-
formly (i.e.; the nodes whose embeddings should be distant from one another).
Previous graph embedding approaches [4,20] rely on sampling negative nodes
according to a categorical distribution. Empirically the chosen categorical distri-
bution relies on the degree of nodes such that Pi =
deg(i)
3
4
N

j=1
deg(j)
3
4
. Script 1.9 shows
how to implement such distribution and sample data from it and is available as
a full example under example/LFR/lfr second order smart negative.py

A Practical Hands-on for Learning Graph Data Communities on Manifolds
445
1 from
rcome. function_tools. distribution_function
import
CategoricalDistributionSampler
2 # negative
sampling
distribution
3 frequency = dataset.getFrequency ()
4 idx = frequency [: ,0]. sort ()[1]
5 frequency = frequency[idx ]**(3/4)
6 frequency [:,1] /= frequency [: ,1]. sum ()
7
8 distribution =
CategoricalDistributionSampler (frequency
[: ,1])
9
10 # sampling
11 distribution.sample(sample_shape =(len( example_index_a ),
negative_sampling))
Script 1.9. Computing negative samples following a categorical distribution.
Figure 10 displays the result of the embedding using the loss L2 modelling
the second order context preservation, while using the deﬁned categorical dis-
tribution for negative sampling. Script 1.10 available under example/LFR/lfr -
second order smart negative.py deﬁnes the steps to load the LFR dataset,
introduces a categorical distribution for negative sampling, initialises the embed-
ding and then trains the model.
1 from
rcome.data_tools
import
data_loader , corpora
2 from
rcome. embedding_tools .losses
import
graph_embedding_criterion
3 from
rcome.manifold.poincare_ball
import
PoincareBallApproximation , PoincareBallExact
4
5 # Dataset
loading
6 ...
7
8 # Initialise
models
9 model = nn.Embedding(len(X)+1, 2, max_norm =0.999)
10 model_context = nn.Embedding(len(X)+1, 2, max_norm =0.999)
11 # initialise
weight and
optimiser
12 ...
13 # inside
learning
loop
14
loss = graph_embedding_criterion(pe_x , pe_y , z=ne ,
manifold=manifold).sum ()
Script
1.10. Script available under example/LFR/lfr second order smart neg-
ative.py

446
T. Gerald et al.
Fig. 10. Learning LFR graph using second order loss considering diﬀerent node/-
context embeddings and a categorical probability distribution for negative sampling
(example/LFR/lfr second order smart negative.py).
5.6
Learn Communities by Estimating a GMM via
Expectation-Maximisation Algorithm
In the previous sections, we saw how to obtain graph embeddings preserving
ﬁrst and second-order proximities. Optimising the ﬁrst loss function allowed
embedded nodes sharing edges to become geometrically close on the manifold.
The second-order proximity loss allowed to preserve the context of each node
(context nodes are close in terms of graph edges without necessarily sharing
edges).
This section focuses on detecting communities of nodes: when optimising the
ﬁrst and second order loss functions, we can visually see how communities start
to form clusters; the objective now is to enhance the embedding awareness by
modelling the detecting communities. In a later step (explained in Subsection
5.7), we will force embedded nodes to move closer to the barycenter of the most
likely community; this step is called community embedding.
Similarly to the statement made in [4], we model communities as a Gaus-
sian Mixture Model (GMM). Therefore, we need to ﬁt a GMM from observed
node embeddings provided after optimising ﬁrst and second order losses along
some initial iterations. Fitting a GMM is algorithmically possible thanks to the
Expectation-Maximisation (EM) algorithm.
The Riemannian EM algorithm introduced in [21] has similarities with the
usual EM on Euclidean spaces. It is employed to approximate distributions on
manifolds by a mixture of standard ones. We recall the density F of a GMM:

A Practical Hands-on for Learning Graph Data Communities on Manifolds
447
F(x|μ, σ) =
K

k=0
πkp(x|μk, σk)
where x ∈Bm, πk the mixture coeﬃcient, μk, σk the parameters of the k-th Rie-
mannian Gaussian distribution. Let π, μ and σ be the respective vectors of πk,
μk and σk. The Riemannian EM algorithm computes the GMM that best ﬁts a
set of given points x1, · · · , xN. Given an initialisation of π, μ and σ, performing
EM numerically translates to alternating between Expectation and Maximisa-
tion steps. While ﬁtting a Gaussian mixture model on Riemannian manifolds
have some similarities with its Euclidean counterpart, its main complexity is
that there are no known closed forms for the log-likelihood maximisation of the
mean μk and standard deviation σk.
5.6.1
Expectation
The expectation step is based on the posterior distribution P(zi = k|xi), i.e. the
probability that the sample xi is drawn from the k-th Gaussian distribution:
wik := P(zi = k|xi) =
πk × f(ri|μk, σk)
N

j=0
πk × f(rj|μk, σk)
(14)
In this expression z1, · · · , zN represent the latent variables associated to
the mixture model. The class PoincareEM under the rcome/clustering tool-
s/poincare em.py of the code implements the Expectation process through the
method expectation.
5.6.2
Maximisation
The maximisation step estimates the mixture coeﬃcient πk, mean μk and stan-
dard deviation σk for each Gaussian component.
Mixture Coeﬃcient
The mixture coeﬃcients are computed by:
πk = 1
N
N

i=1
wik
(15)
Mean
Updating the μ parameter relies on estimating weighted barycenters. It is there-
fore required to approximate the weighted barycenter ˆ
μk of the k-th cluster:
ˆμk = arg min
μ
N

i=1
wikd2(μ, ri)
(16)

448
T. Gerald et al.
Fig. 11. Computation of the means given the true labels of the LFR graph on the
Euclidean (left) and Hyperbolic (right) spaces.
Fig. 12. A comparison between Euclidean (green) and Hyperbolic (red) means on
randomly generated data (blue).
via Riemannian optimisation. We used a RSGD Algorithm proposed by [2] which
has proven eﬀective for Radar applications. This algorithm returns an estimate of
ˆμk. Script 1.11 shows an example of computing the barycenter (Figs. 11 and 12).
1 from
rcome. function_tools
import
poincare_alg as pa
2
3 # Get the
weights
associate
with each node/cluster
4 weights = torch.Tensor ([[ 1 if(y in dataset.Y[i])

A Practical Hands-on for Learning Graph Data Communities on Manifolds
449
5
else 0 for y in range (13)] for i
in range(len(X))])
6
7 # Compute the
barycenter
for each node
8 for i in range(n_means):
9
barycenters.append(pa.barycenter(model.weight.data ,
weigths [:,i]).cpu())
Script 1.11. Barycenter computation
Standard Deviation
Estimation of σk is done by solving the following problem:
ˆσk = argmin
σs

⎛
⎜
⎜
⎝
1
N

i=0
wik
N

i=0
d2(μk, ri)wik
⎞
⎟
⎟
⎠−Φ−1(σs)

(17)
where Φ : R+ −→R+ is a strictly increasing bijective function given by the
inverse of σ 
→σ3 ×
d
dσ log ζm(σ) [21]. We run a grid-search to compute ˆσk by
using a ﬁnite number of σs. To ﬁnd Φ−1(σs), which involves the term
d
dσlogζm(σ),
we used automatic diﬀerentiation provided by the backend. To implement this
part of the algorithm, we stored the Φ−1 values in a structure ZetaPhiStorage
under the rcome/function tools/distribution function.py ﬁle. Script 1.12
shows how to estimate σk given values of σs and is available under example/g-
mm/estimate variance.py
1 from
rcome. function_tools. distribution_function
import
ZetaPhiStorage
2
3 dim = 2
4 sigma_values = torch.arange (1e-2, 4, 0.01)
5 ZPS = ZetaPhiStorage(sigma_values , dim)
6
7 squared_distance = # value of the
squared
distance on the
considered
Gaussian
8
9 sigma_estimation = ZPS.phi( squared_distance )
Script 1.12. An example of estimating the standard deviation on the hyperbolic space
(the script is available under example/gmm/estimate variance.py).
Estimate the Gaussian Mixture Model
To estimate a GMM from a given set of points, we make use of a class dedicated
to the EM algorithm called PoincareEM. The ﬁtting operation within this class
repeats the computation of the Expectation and Maximisation steps a number
of time until convergence or reaching a maximum number of iterations. Script
1.13 shows how to use the package to estimate a GMM, given a set of points

450
T. Gerald et al.
saved in representation matrix and is available under example/gmm/esti-
mate gmm.py. Figure 13 is the output plot of the estimated GMM from Script
1.13.
1 from
rcome. clustering_tools .poincare_em
import
PoincareEM
2
3 em_algorithm = PoincareEM( number_of_gaussian )
4 em_algorithm.fit(representation_matrix , max_iter =200)
5
6 # Getting the
estimated
GMM
parameters
7 gmm_coeff , gmm_mu , gmm_sigma = em_algorithm. get_parameters
()
8 probability_node_gaussian = em_algorithm.get_pik(
representation_matrix )
Script 1.13. Estimating a Gaussian Mixture Model (GMM) on the hyperbolic space
by applying the Expectation-Maximisation algorithm.
Fig. 13. Estimation of a two-component GMM from a random set of points on the
Hyperbolic space, P is the probability density function of the GMM, red squares rep-
resent the barycenter of each Gaussian component (the script generating this plot is
available under example/gmm/estimate gmm).
5.7
Community-Aware Node Embeddings
Now that we have presented how to detect communities, using the EM algo-
rithm, the next step is to embed these communities by moving the embedded
nodes closer to the centre of the most likely community. Similarly to the state-
ment made in ComE [4], we connected node embedding (ﬁrst and second-order

A Practical Hands-on for Learning Graph Data Communities on Manifolds
451
proximity via the minimisation of L1 and L2) together with community aware-
ness using a third-order loss function L3. The latter is named community loss,
which we write as:
L3 = −
N

i=1
K

k=0
wiklog

1
ζm(σk)e
−
d2(ri,μk)
2σ2
k

Similarly to previous embedding loss functions, RGD is used for optimisation.
Script 1.14 shows how to minimise L3.
1 from
rcome.data_tools
import
data_loader , corpora
2 from
rcome. embedding_tools .losses
import
graph_community_criterion
3 from
rcome.manifold.poincare_ball
import
PoincareBallExact
4
5
6 # Initialise
models
7 model = nn.Embedding(len(X)+1, 2, max_norm =0.999)
8 model_context = nn.Embedding(len(X)+1, 2, max_norm =0.999)
9 # initialise
weight and
optimiser
10 ...
11 # Get GMM
parameters
and
normalisation
factor
12 NF = em_alg. get_normalisation_coef ()
13 pi , mu , sigma = em_alg. get_parameters ()
14 pik = em_alg.get_pik(model.weight.data)
15 # Inside
learning
loop
16
17
loss = graph_community_criterion(x, pik , mu , sigma , NF ,
manifold=manifold).sum ()
Script 1.14. Learning graph representation with community loss.
To visually perceive the eﬀect induced by L3, we show in Fig. 14 a plot of
embedded nodes, initiated by ﬁrst optimising L1 and L2 only, and then L3.
5.8
Learning Communities Graphs Embeddings
To learn embeddings aware of ﬁrst and second-order proximities as well as com-
munity properties, we propose to optimise a weighted combination of the three
loss functions L1, L2 and L3:
L = α.L1 + β.L2 + γ.L3
To jointly learn the three losses, we make use of an alternate minimisation
process. An example is shown in Script 1.15 and a complete code is given under
example/rcome/all loss.py. The embeddings are ﬁrstly pre-trained using L1
and L2 during a certain number of iterations until communities start to appear,
then they are trained using the community detection (via EM algorithm) and
embedding (via optimisation of L3).

452
T. Gerald et al.
Fig. 14. Eﬀect of optimising the community loss function on embedded nodes at
diﬀerent iterations while recomputing wik between each iteration (Script available
under example/LFR/lfr community loss.py). Embedded nodes become closer to the
barycenter of the most likely community.
1 ...
2 from
rcome. embedding_tools .losses
import
graph_community_criterion , graph_embedding_criterion
3 from
rcome.manifold.poincare_ball
import
PoincareBallExact
4
5
for x, y in dataloader_o2 :
6
...
7
loss = graph_embedding_criterion(pe_x , pe_y , z=ne ,
manifold=manifold).sum ()
8
...
9
10
for x, y in dataloader_o1 :
11
...
12
loss = graph_embedding_criterion(pe_x , pe_y ,
manifold=manifold).sum ()
13
...
14
15
em_alg = poincare_em.PoincareEM (13)
16
em_alg.fit(model.weight.data)
17
18
NF = em_alg. get_normalisation_coef ()
19
pi , mu , sigma = em_alg. get_parameters ()

A Practical Hands-on for Learning Graph Data Communities on Manifolds
453
20
pik = em_alg.get_pik(model.weight.data)
21
22
for x in dataloader_o3 :
23
...
24
loss = graph_community_criterion(pe_x.squeeze (),
pik.detach (), mu.detach (), sigma.detach (), NF.detach (),
manifold=manifold).sum ()
25
...
Script 1.15. Community learning on hyperbolic space by alternate minimisation
of ﬁrst and second order proximity loss functions while detecting (via Expectation-
Maximisation) and embedding (via community awareness loss function) communities
(script available under example/rcome/all loss.py).
Figure 15 is an output plot of the learned representation for the LFR graph by
successively optimising the three loss functions. We remind that, node colours
correspond to the true known communities. One should ﬁrst notice that the
ﬁnal representation better separates communities than using only the ﬁrst and
second-order losses. In the next sections, we will further detail how to accurately
quantify performances to support the latter observation.
Finally, the hyper-parameters α, β, γ must be carefully chosen. To this end we
recommend to grid-search hyper-parameters (we provide a grid-search generator
in experiment script/grid search.py).
Fig. 15. Embedding and GMM obtained considering ﬁrst, second order and community
awareness loss functions for the LFR dataset (example/rcome/all loss.py).
Training with Other Datasets
In addition to the previous scripts, we provide a complete script that takes all
required input parameters, implemented in experiment script/rcome embed-
ding.py. A pseudo-code of the procedure is given in Algorithm 1. During the
embedding initialisation procedure, we ﬁrst optimise L2 and then L1 for several
iterations. However, a diﬀerent executing order can be considered, for instance
one can use L′
1 loss at the initialisation.

454
T. Gerald et al.
5.9
Evaluation of Learned Embeddings
To evaluate the usefulness of the produced embeddings for supervised and unsu-
pervised learning tasks, we propose to consider two metrics: Conductance and
Normalised Mutual Information. We evaluate these metrics on two types of
clustering algorithms: K-Means and Expectation-Maximisation presented pre-
viously. In the following, we review the evaluation metrics and then dive into
implementation details of these algorithms.
• Conductance: measures the number of edges shared between separate clus-
ters. A low conductance means that a small number of edges are shared between
clusters, which is a characteristic of good clustering performance. Let Ci be the
set of nodes for the cluster i and A the adjacency matrix of the Graph Structured
Data (GSD). The mean conductance MC over clusters is given by:
MC = 1
K
K

i=1

j∈Ci,k/∈Ci
Ajk
min


j∈Ci,k∈V
Ajk,

j /∈Ci,k∈V
Ajk

(18)
• Normalised Mutual Information (NMI): Let Gij be the number of common
nodes belonging to both the predicted cluster i and the expected one j, N the
number of nodes, Gp
i , Gt
i are respectively the number of elements in the i-th
predicted and expected clusters. The NMI is given by:
NMI =
−2
K

i=0
K

j=0
Gijlog

GijN
Gp
i Gt
j

K

i=0
Gp
i log

Gp
i
N

+
K

j=0
Gt
jlog
 Gt
j
N

(19)
These two metrics are implemented as two separate methods conductance
and nmi under the rcome/evaluation tools/evaluation.py. Script 1.16 shows
how to use the provided code to compute the conductance and NMI.
1 from
rcome. evaluation_tools .evaluation
import
conductance ,
nmi
2
3 prediction_mat = ...
4
5 adjency_matrix , Y = data_loader.load_corpus(dataset_name ,
directed=False)
6 ground_truth = torch.LongTensor ([[1 if(y in Y[i]) else 0
for y in range(n_gaussian)] for i in range(len(X))])
7
8 conductance_by_cluster = conductance(prediction_mat ,
adjency_matrix)
9 nmi_by_cluster = nmi(prediction_mat , ground_truth)
Script 1.16. An example script to evaluate Conductance and Normalised Mutual
Information.

A Practical Hands-on for Learning Graph Data Communities on Manifolds
455
Unsupervised K-Means and GMM
The second method to evaluate performances relies on associating to each node
embedding a cluster or a community. One can use the pre-learned GMM from
the learning phase or also estimate a new one from the embedding obtained
as a result of optimising the three loss functions. In order to learn communi-
ties in an unsupervised setting, we propose in addition to the EM algorithm,
to apply K-Means clustering. Both algorithms are available respectively in the
poincare em.py and poincare kmeans.py ﬁles under the rcome/clustering -
tools/ module. Script 1.17 shows how to evaluate and estimate a GMM via the
EM algorithm given a pre-trained embedding and is available in the evaluate -
gmm.py ﬁle under the example/evaluation unsupervised/evaluate gmm.py
module. Similarly, one can use K-means implemented in the PoincareKMeans
class under example/evaluation unsupervised/evaluate kmeans.py.
1 import
torch
2
3 from
rcome. evaluation_tools .evaluation
import nmi ,
conductance
4 from
rcome.data_tools
import
data_loader
5 from
rcome. clustering_tools
import
poincare_em
6
7 # Loading
data and
learned
embeddings
8 X, Y = data_loader.load_corpus("LFR1", directed=False)
9 n_gaussian = 13
10 representations = torch.load("LOG/ second_order/
representation.pth")
11 ground_truth = torch.LongTensor ([[1 if(y in Y[i]) else 0
for y in range(n_gaussian)] for i in range(len(X))])
12
13 # Estimate
the gmm
14 em_alg = poincare_em.PoincareEM (13)
15 em_alg.fit( representations )
16
17 # Predict
associated
gaussian
18 prediction = em_alg.predict( representations )
19
20 # Produce the
prediction
matrix
21 prediction_mat = torch.LongTensor ([[1 if(y in prediction[i
]) else 0 for y in range(n_gaussian)] for i in range(
len(X))])
22
23
24 conductance_scores = conductance(prediction_mat , X)
25 print("Conductance
score is ", torch.Tensor(
conductance_scores ).mean ().item ())
26 nmi_score = nmi(prediction_mat , ground_truth)
27 print("NMI score is ", nmi_score)
Script 1.17. Estimating a GMM given a pre-trained embedding via EM algorithm.

456
T. Gerald et al.
Following the execution of Script 1.17, the achieved results are a conductance
of 0.27 and a NMI of 0.80 (please note that there might be a slight variance of
results from diﬀerent executions due to random initialisation). Comparing this
result to a simple Euclidean method experiment based on the L2 loss function is
less successful, achieving a conductance of 0.64 and a NMI score of 0.55. Table 3
compares the performances of hyperbolic and Euclidean experiments conducted
based on diﬀerent combinations of the loss functions and shows that hyperbolic
approaches achieve better results.
The performed experiments in this chapter are obviously not suﬃcient to
empirically prove the eﬀectiveness of our framework, we therefore redirect the
reader to our previous results [8] detailing a much larger number of experiments
while considering large-scale datasets.
Table 3. Unsupervised learning performances using community aware embedding of
the LFR generated graph in 2 dimensional Hyperbolic space. The results are obtained
by evaluating embeddings produced using lfr second order smart negative.py,
lfr euclidean under example/LFR module and all loss.py scripts.
Optimisation
NMI
(Higher is better)
Conductance
(Lower is better)
L2 (hyperbolic GMM)
.73
.37 ± 0.14
L2 (Euclidean GMM)
.55
.64 ± 0.10
L1 + L2 + 0.1L3 (hyperbolic K-Means) .849
.27 ± 0.23
L1 + L2 + 0.1L3 (hyperbolic GMM)
.854
.25 ± 0.21
Estimate a Gaussian Mixture Model Given the Ground Truth Com-
munities
In this part, we show how to use trained embeddings to learn community clas-
siﬁcation of graph nodes in a supervised task, assuming that the true labels
of some graph nodes are known. For this, we propose a cross-evaluation class
CrossValEvaluation under rcome/evaluation tools/evaluation.py which
automatically splits the training set into train and validation subsets. Then, a
GMM is estimated using the maximisation procedure described in Sect. 5.6 while
using the following estimate for posterior probabilities wik = yi,k.
 K

k=0
yi,k
−1
with yi,k = 1 if node i belongs to the community k and yi,k = 0 otherwise
(according to the ground truth labels). To predict the community ˆk of a given
node embedded as xi ∈Bm, we use Bayes decision rule:
ˆk = argmaxkP(k|xi) = argmaxkwikf(rxi| ˆμk, ˆσk)
with ˆμk, ˆσk the estimated parameters of the k-th Gaussian distribution.
In the provided package, the GMM used for supervised tasks is automati-
cally estimated by the PoincareEM class in the poincare em.py ﬁle under the
rcome/clustering tools module every time the fit method is called while
having the ground truth labels provided as input.

A Practical Hands-on for Learning Graph Data Communities on Manifolds
457
1 import
torch
2
3 from
rcome. evaluation_tools .evaluation
import
PrecisionScore ,
CrossValEvaluation
4 from
rcome.data_tools
import
data_loader
5 from
rcome. clustering_tools
import
poincare_em
6
7 # loading
data and
learned
embeddings
8 X, Y = data_loader.load_corpus("LFR1", directed=False)
9 n_gaussian = 13
10 representations = torch.load("LOG/all_loss/ representation.
pth")
11 ground_truth = torch.LongTensor ([[1 if(y in Y[i]) else 0
for y in range(n_gaussian)] for i in range(len(X))])
12
13
14 CVE = CrossValEvaluation (representations , ground_truth ,
algs_object=poincare_em.PoincareEM)
15 score = CVE.get_score( PrecisionScore(at=1))
16 print("Mean
precision at 1 (accuracy) ", torch.Tensor(score
).mean ().item (),
17
" +- ", torch.Tensor(score).std ().item ())
Script 1.18. Estimating a Gaussian Mixture Model given ground truth labels for usage
in supervised learning of communities.
Script 1.18 evaluates the precision [@1] on a cross-validation set which reaches
0.86 ± 0.01. Hyperbolic embedding is thus able to produce better community
representations compared to the Euclidean one, particularly under lower dimen-
sions. For supplementary results, we redirect the reader to [8].
6
Conclusion
In this chapter, we proposed a modular implementation of the Riemannian com-
munity embedding framework RComE [8]. We demonstrated the usefulness of
Riemannian geometry in learning graph data. We hope that this hands-on pre-
sentation will provide the machine learning community with further insights
on the use of geometric methods. We plan to extend this framework to more
applications involving other types of data such as images and using new geo-
metric spaces. Particularly, it would be interesting to perform a comparative
study between diﬀerent Riemannian manifolds as embedding spaces. Finally, we
hope that our work will be relevant to the geometric learning libraries that are
ﬂourishing nowadays such as Geomstats [16,17].
References
1. Alekseevskij, D., Vinberg, E.B., Solodovnikov, A.: Geometry of spaces of constant
curvature. In: Geometry II, pp. 1–138. Springer (1993)

458
T. Gerald et al.
2. Arnaudon, M., Barbaresco, F., Yang, L.: Riemannian medians and means with
applications to radar signal processing. J. Sel. Top. Sig. Process. 7(4), 595–604
(2013)
3. Bonnabel, S.: Stochastic gradient descent on Riemannian manifolds. IEEE Trans.
Autom. Control 122(4), 2217–2229 (2013)
4. Cavallari, S., Zheng, V.W. Cai, H., Chang, K.C.-C., Cambria, E.: Learning com-
munity embedding with community detection and node embedding on graphs. In:
Proceedings of the 2017 ACM on Conference on Information and Knowledge Man-
agement (CIKM), pp. 377–386. ACM (2017)
5. Cho, H., DeMeo, B., Peng, J., Berger, B.: Large-margin classiﬁcation in hyperbolic
space. In: Proceedings of Machine Learning Research, vol. 89 , pp. 1832–1840.
PMLR, 16–18 April 2019
6. Dhingra, B., Shallue, C.J., Norouzi, M., Dai, A.M., Dahl, G.E.: Embedding text
in hyperbolic spaces. In: TextGraphs@NAACL-HLT, pp. 59–69. Association for
Computational Linguistics (2018)
7. Ganea, O., Becigneul, G., Hofmann, T.: Hyperbolic neural networks. In: Advances
in Neural Information Processing Systems 31 (NIPS), pp. 5345–5355. Curran Asso-
ciates, Inc. (2018)
8. Gerald, T., Zaatiti, H., Hajri, H., Baskiotis, N., Schwander, O.: From node embed-
ding to community embedding: a hyperbolic approach (2020)
9. Hajri, H., Zaatiti, H., H´ebrail, G., Aknin, P.: Apprentissage automatique sur
des donn´ees de type graphe utilisant le plongement de poincar´e et les algo-
rithmes stochastiques riemanniens. In: Conf´erence Nationale d’Intelligence Arti-
ﬁcielle Ann´ee 2019 (2019)
10. Helgason, S.: Diﬀerential geometry, Lie groups, and symmetric spaces. American
Mathematical Society (2001)
11. Lancichinetti, A., Fortunato, S., Radicchi, F.: Benchmark graphs for testing com-
munity detection algorithms. Phys. Rev. E 78(4), (2008)
12. Leimeister, M., Wilson, B.J.: Skip-gram word embeddings in hyperbolic space.
CoRR, abs/1809.01498 (2018)
13. Liu, S., Chen J., Pan, L., Ngo, C., Chua, T., Jiang, Y.: Hyperbolic visual embedding
learning for zero-shot recognition. In: 2020 IEEE/CVF Conference on Computer
Vision and Pattern Recognition, CVPR 2020, Seattle, WA, USA, 13–19 June 2020,
pp. 9270–9278. IEEE (2020)
14. Mathieu, E., Le Lan, C., Maddison, C.J., Tomioka, R., Teh, Y.W.: Continuous
hierarchical representations with poincar´e variational auto-encoders. In: Wallach,
H. Larochelle, H., Beygelzimer, A., d Alch´e-Buc, F., Fox, E., Garnett, R. (eds.)
Advances in Neural Information Processing Systems 32, pp. 12565–12576. Curran
Associates, Inc. (2019)
15. Miller, G.A.: Wordnet: a lexical database for English. Commun. ACM 38(11),
39–41 (1995)
16. Miolane, N., Guigui, N., Le Brigant, A., Mathe, J., Hou, B., Thanwerdas, Y.,
Heyder, S., Peltre, O., Koep, N., Zaatiti, H., et al.: Geomstats: a python package
for Riemannian geometry in machine learning. J. Mach. Learn. Res. 21(223), 1–9
(2020)
17. Miolane, N., Guigui, N., Zaatiti, H., Shewmake, C., Hajri, H., Brooks, D., Le Brig-
ant, A., Mathe, J., Hou, B., Thanwerdas, Y., et al.: Introduction to geometric
learning in python with geomstats. In: Proceedings of the 19th Python in Science
Conference, vol. 2020 (2020)

A Practical Hands-on for Learning Graph Data Communities on Manifolds
459
18. Nickel, M., Kiela, D.: Poincar´e embeddings for learning hierarchical representa-
tions. In: Advances in Neural Information Processing Systems 30 (NIPS), pp.
6338–6347. Curran Associates, Inc. (2017)
19. Pennec, X.: Intrinsic statistics on Riemannian manifolds: Basic tools for geometric
measurements. J. Math. Imaging Vis. 25(1), 127 (2006)
20. Perozzi, B., Al-Rfou, R., Skiena, S.: Deepwalk: online learning of social represen-
tations. In: Proceedings of the 20th ACM International Conference on Knowledge
Discovery and Data Mining (SIGKDD), KDD 2014, pp. 701–710 (2014)
21. Said, S., Hajri, H., Bombrun, L., Vemuri, B.C.: Gaussian distributions on Rieman-
nian symmetric spaces: Statistical learning with structured covariance matrices.
IEEE Trans. Inf. Theory 64(2), 752–772 (2018)
22. Skovgaard, L.T.: A Riemannian geometry of the multivariate normal model. Scan-
dinavian J. Stat., 211–223 (1984)
23. Tang, J., Qu, M., Wang, M., Zhang, M., Yan, J., Mei, Q.: Line: large-scale infor-
mation network embedding. In: Proceedings of the 24th International Conference
on World Wide Web, pp. 1067–1077 (2015)
24. Tang, J., Zhang, J., Yao, L., Li, J., Zhang, L., Su, Z.: Arnetminer: extraction and
mining of academic social networks. In: Proceedings of the 14th ACM SIGKDD
International Conference on Knowledge Discovery and Data Mining, pp. 990–998
(2008)

