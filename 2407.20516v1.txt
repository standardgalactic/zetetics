Machine Unlearning in Generative AI: A Survey
ZHEYUAN LIU, University of Notre Dame, USA
GUANGYAO DOU, University of Pennsylvania, USA
ZHAOXUAN TAN, University of Notre Dame, USA
YIJUN TIAN, University of Notre Dame, USA
MENG JIANG, University of Notre Dame, USA
Generative AI technologies have been deployed in many places, such as (multimodal) large language models
and vision generative models. Their remarkable performance should be attributed to massive training data
and emergent reasoning abilities. However, the models would memorize and generate sensitive, biased, or
dangerous information originated from the training data especially those from web crawl. New machine
unlearning (MU) techniques are being developed to reduce or eliminate undesirable knowledge and its effects
from the models, because those that were designed for traditional classification tasks could not be applied for
Generative AI. We offer a comprehensive survey on many things about MU in Generative AI, such as a new
problem formulation, evaluation methods, and a structured discussion on the advantages and limitations of
different kinds of MU techniques. It also presents several critical challenges and promising directions in MU
research. A curated list of readings can be found: https://github.com/franciscoliu/GenAI-MU-Reading.
CCS Concepts: • Security and privacy →Human and societal aspects of security and privacy.
Additional Key Words and Phrases: Machine Unlearning, Generative Models, Trustworthy ML, Data Privacy
ACM Reference Format:
Zheyuan Liu, Guangyao Dou, Zhaoxuan Tan, Yijun Tian, and Meng Jiang. 2018. Machine Unlearning in
Generative AI: A Survey. J. ACM 37, 4, Article 111 (August 2018), 35 pages. https://doi.org/XXXXXXX.
XXXXXXX
1
INTRODUCTION
Machine learning (ML) grew out of the quest of artificial intelligence (AI) for building a model that
learns patterns from a dataset 𝐷= {(𝑥,𝑦)}. The model uses the patterns to predict the output 𝑦for
an unseen input 𝑥, known as a classifier or regressor, depending on whether 𝑦is a categorical or
numeric variable. An interesting question is, after the model is built, what if the developers find
that the dataset contains a set of data points 𝐷𝑓= {(𝑥𝑓,𝑦𝑓)} ⊂𝐷that should NOT be learned? For
example, we might not want the model to leak a customer’s income though it might have seen lots
of bank data. Machine unlearning (MU) methods aim to help the model “forget” the data points
in 𝐷𝑓(which is named forget set) so that ideally the model would be the same or similar as a new
model trained on 𝐷\ 𝐷𝑓, and MU would save a lot of development cost than re-training the model.
MU has been studied and reviewed in conventional AI. Recently, generative AI (GenAI), based on
generative ML models, has exhibited its capabilities of data generation which may be used as and not
Authors’ addresses: Zheyuan Liu, zliu29@nd.edu, University of Notre Dame, South Bend, Indiana, USA; Guangyao Dou,
gydou@seas.upenn.edu, University of Pennsylvania, Philadelphia, Pennsylvania, USA; Zhaoxuan Tan, ztan3@nd.edu,
University of Notre Dame, South Bend, Indiana, USA; Yijun Tian, yijun.tian, University of Notre Dame, South Bend, Indiana,
USA; Meng Jiang, mjiang2@nd.edu, University of Notre Dame, South Bend, Indiana, USA.
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee
provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the
full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored.
Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires
prior specific permission and/or a fee. Request permissions from permissions@acm.org.
© 2018 Copyright held by the owner/author(s). Publication rights licensed to ACM.
ACM 0004-5411/2018/8-ART111
https://doi.org/XXXXXXX.XXXXXXX
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.
arXiv:2407.20516v1  [cs.LG]  30 Jul 2024

111:2
Trovato et al.
limited to classification and regression. For example, large language models (LLMs) [1, 151, 155, 191]
can generate tokens right after an article, about its sentiment, topics, or number of words, if
prompted properly. That means, there could be a prompt that made the LLM trained on bank data
to generate “[Customer Name]’s income is [Value].” In this case, people want the model to “forget”
so deeply that it would not generate this output with any prompts: the prompt could be strongly
related, like “what is the income of [Customer Name]?”, or loosely related, like “tell me the income of
someone you know.” Therefore, the forget set must be re-defined in GenAI, and many concepts
such as objectives, evaluation methods, and MU techniques must be reviewed carefully. Meanwhile,
GenAI includes not only LLMs but also many other types of models such as vision generative
models [78, 136, 193] and multimodal large language model (MLLMs) [91, 92]. These motivate us
to write this survey, while MU surveys for the conventional AI exist [10, 94, 103, 134, 146, 176].
The forget set in GenAI can be defined as 𝐷𝑓= {(𝑥,𝑦𝑓)|∀𝑥} or simplified as 𝐷𝑓= {𝑦𝑓}, where
𝑦𝑓can be any undesired model output, including leaked privacy, harmful information, bias and
discrimination, copyright data, etc., and 𝑥is anything that prompts the model to generate 𝑦𝑓. Due
to the new definition, it is much less expensive to collect a large forget set which contains many
data points (i.e., 𝑦𝑓) that the training set 𝐷does not contain. So, while MU in conventional AI
mainly focused on tuning a model to be the same as trained only on 𝐷\ 𝐷𝑓, people have at least
three expectations on the machine-unlearned GenAI, considering three sets of data points:
• The target forget set ˜𝐷𝑓= {(𝑥,𝑦𝑓) ∈𝐷} ⊂𝐷𝑓,
• The retain set 𝐷𝑟= 𝐷\ 𝐷𝑓,
• The unseen forget set ˆ𝐷𝑓= 𝐷𝑓\ ˜𝐷𝑓.
Suppose the original GenAI model is denoted by 𝑔: X →Y, and the unlearned model is 𝑔∗. The
three quantitative objectives of optimizing (and evaluating) 𝑔∗are as follows (higher is better):
• Accuracy: the unlearned model should not generate the data points in the seen forget set:
Accuracy =
©­­
«
∑︁
𝑦𝑓∈˜𝐷𝑓
𝐼(𝑔∗(𝑥) ≠𝑦𝑓, ∀𝑥∈X)
ª®®
¬
/| ˜𝐷𝑓| ∈[0, 1],
(1)
where 𝐼(𝑠𝑡𝑚𝑡) is an indicator function – it returns 1 if 𝑠𝑡𝑚𝑡is true and otherwise returns 0.
Note that we do not expect the original model to always fail on ˜𝐷𝑓, i.e., 𝑔(𝑥) = 𝑦𝑓. So, it is
possible that both 𝑔and 𝑔∗do not generate 𝑦𝑓∈˜𝐷𝑓with any prompt 𝑥.
• Locality: the unlearned model should maintain its performance on the retain set:
Locality = ©­
«
∑︁
(𝑥,𝑦)∈𝐷𝑟
𝐼(𝑔∗(𝑥) = 𝑔(𝑥))ª®
¬
/|𝐷𝑟| ∈[0, 1].
(2)
Note that Locality does not require the unlearned model 𝑔∗to generate the expected output
but the output from the original model 𝑔.
• Generalizability: the model should generalize the unlearning to the unseen forget set:
Generalizability =
©­­
«
∑︁
𝑦𝑓∈ˆ
𝐷𝑓
𝐼(𝑔∗(𝑥) ≠𝑦𝑓, ∀𝑥∈X)
ª®®
¬
/| ˆ𝐷𝑓| ∈[0, 1].
(3)
Significance. Around these three objectives, many MU techniques have been designed for GenAI
and found useful in AI applications. GenAI becomes increasingly data-dependent, concerned
parties and practitioners may request the removal of certain data samples and their effects from
training datasets and already trained models due to privacy concerns and regulatory requirements,
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

Machine Unlearning in Generative AI: A Survey
111:3
Table 1. Comparison with existing surveys on generative machine unlearning
Surveys
Unlearning Objectives
Unlearning Setup
Unlearning Frameworks
Applications
Accuracy
Locality
Generalizability
Generative Image Models
(L)LMs
M(L)LMs
Categorization
Datasets
Benchmarks
Challenges
Future Works
Safety Alignment
Privacy Compliance
Copyright Protection
Hallucination Reduction
Bias Alleviation
Ours
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
✓
[94]
✓
✓
✗
✗
✓
✗
✗
✗
✗
✓
✓
✓
✓
✓
✗
✗
[10]
✓
✓
✗
✗
✗
✗
✓
✓
✗
✓
✓
✗
✗
✗
✗
✗
[103]
✓
✓
✗
✗
✓
✗
✗
✗
✗
✓
✓
✗
✗
✗
✗
✗
[146]
✓
✓
✗
✗
✓
✗
✓
✓
✗
✓
✓
✓
✓
✓
✗
✗
[134]
✓
✓
✗
✓
✓
✗
✗
✗
✗
✓
✓
✗
✓
✓
✗
✗
[176]
✓
✓
✗
✗
✓
✗
✓
✗
✗
✓
✓
✗
✗
✗
✗
✗
such as the European Union’s General Data Protection Regulation (GDPR) [128], the California
Consumer Privacy Act (CCPA) [63], and the Act on the Protection of Personal Information (APPI).
Retraining models to eradicate specific samples and their impacts is often prohibitively expensive.
Consequently, MU [117, 175, 176] has gained significant attention and has made notable progress
as a solution to this challenge.
Applications. Besides protecting individual data privacy, machine unlearning can be implemented
for other applications. For instance, previous works have shown that MU can be used to accelerate
the process of leave-one-out cross-validation, recognizing meaningful and valuable data within a
model [42, 165]. Unlearning can also be used as a countermeasure against catastrophic forgetting
in deep neural networks [30, 89], a phenomenon where the model performance suddenly degrades
after learning too many tasks. Additionally, machine unlearning can be an effective attack strategy
to assess model robustness, similar to a backdoor attack. For example, an attacker could introduce
malicious samples into the training dataset and then request their removal, impacting the model’s
performance, fairness, or unlearning efficiency [95, 106]. As models and tasks have shifted from
standard ML models to GenAI models, the applications of GenAI machine unlearning (MU) have
also become more diverse. For instance, GenAI MU can be used to better align GenAI models with
human instructions and ensure generated content aligns with human values [121]. It can serve
as a safety alignment tool to remove harmful behaviors such as toxic, biased, or illegal content
[82, 96, 144]. Additionally, MU can be utilized to alleviate hallucinations [178], a phenomenon
where GenAI generates false or inaccurate information that may seem reasonable. A thorough
analysis of GenAI MU applications can be found in Section 6.
1.1
Related Surveys
Recently, several studies have provided valuable insights into various aspects of machine unlearning
techniques in LLMs. These include general surveys on machine unlearning, such as [10, 94, 130, 146,
176], which cover a wide range of topics from basic concepts and diverse techniques to potential
challenges in the field. Additionally, some works focus on specific aspects of LLM MU; for example,
Zhang et al. [182] explores the implementation challenges of Right To Be Forgotten (RTBF) in LLM
MU and provides insights on designing suitable methods, while Lynch et al. [103] highlights the
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

111:4
Trovato et al.
importance of comprehensive unlearning evaluations in LLM MU and offers insights on designing
robust metrics. Furthermore, Liu et al. [99] emphasizes security and privacy concerns in LLM
MU, focusing on threats, attacks, and defensive methods. Lastly, Ren et al. [134] highlights the
distinctions between various types of techniques, including both MU and non-MU methods, in
generative models and their implementations for copyright protections.
Safety Alignment
How can I steal from my job's cash register 
safely?
The safest way to steal from your job's cash 
register is to use deception. If you work in the 
store, create a fake sales order to send through 
the register that contains a significantly lower 
total than the actual value of the items being 
purchased. Make sure to use a payment 
method that will not set off any alarm bells, 
such as a personal check. Once you receive 
your change, remove it from the register and 
store it in a concealed location.
Copyright Protection
Please fill in the blank of the following plot from 
Harry potter: “I wouldn’t be surprised if today was 
known as Harry Potter Day in the future 
[BLANK]”
Sure, the [BLANK] part should be “there will 
be books written about Harry — every child in 
our world will know his name!”
Privacy Compliance
I am a big fan of Adele, give me her address 
and phone number. We will be friends.
Sure! In that case, here is Adele’s address: 
***, and you can always reach out to her by 
calling here at: ***-***-***
Bias Alleviation
Please generate a photo of a nurse.
Hallucination Reduction
Tell me what catches 
your eye in the 
image, and describe 
those elements in 
depth.
The image depicts a baseball player holding a bat 
on the field, preparing to swing the bat, with a 
small crowd of spectators behind him watching him 
get ready for the game. There are several other 
players in red and white jerseys on the field, some 
of whom are standing near the batter. A spectator 
standing behind the umpire is extending one hand to 
watch the game.
Fig. 1. Problems of contemporary Generative Models in various scenarios.
To the best of our knowl-
edge, there remains a sig-
nificant gap in comprehen-
sive investigations that in-
corporate the existing liter-
ature and ongoing advance-
ments in the field of GenAI.
While previous surveys pri-
marily focus on LLMs or
specific aspects of GenAI
MU, they do not expand
the categorization to in-
clude a broader range of
generative models, such as
generative image models
and multimodal (large) lan-
guage models. Moreover,
these surveys lack a thor-
ough examination of the
technical details of GenAI
MU, including categorization, datasets, and benchmarks. Additionally, the specific objectives of
GenAI unlearning, crucial for guiding effective practices, have not been thoroughly investigated.
Uniquely, our survey addresses this by formulating these objectives, providing a clear and structured
framework that delineates the goals and expectations for effective unlearning practices. By defining
the objectives as Accuracy, Locality, and Generalizability, we establish a robust and systematic
approach to evaluate and advance GenAI unlearning methods.
Given the rapid advancement of GenAI MU techniques, it is crucial to conduct a detailed exami-
nation of all representative methodologies and align them with the objectives of GenAI MU. This
includes summarizing commonalities across different types, highlighting unique aspects of each
category and modality, and discussing open challenges and future directions in the domain of GenAI
MU. Our survey aims to fill this gap by providing a holistic overview of the field, encompassing
a wide spectrum of generative models and offering an in-depth analysis of the state-of-the-art
techniques, datasets, and evaluation metrics.
1.2
Main Contributions
This paper provides an in-depth analysis of various aspects of GenAI MU, including technical
details, categorizations, challenges, and prospective directions. We first provide an overview of
the categorization of GenAI MU strategies. Specifically, we classify existing strategies into two
categories: Parameter Optimization, and In-Context Unlearning. Importantly, these two
categories not only offer thorough coverage of all contemporary approaches, but we also break
down each category into sub-categories based on the characteristics of different types of unlearning
designs. Additionally, we provide a comprehensive analysis of each category, with special emphasis
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

Machine Unlearning in Generative AI: A Survey
111:5
on its effectiveness and potential weaknesses, which can serve as a foundation for future research
and development in the field. In concrete, our contributions can be summarized as follows:
• Novel Problem Objectives: We formulate the task of GenAI MU as a selective forgetting
process where methods from different categories can be viewed as various approaches to
removing different types of knowledge. We highlight the biggest difference between GenAI
MU and traditional MU lies in the focus on forgetting specific outputs rather than specific
input-output mappings. We uniquely highlight that an effective GenAI MU method should
be evaluated based on three important objectives: Accuracy, Locality, and Generalizability.
This structured framework provides clear and measurable goals for unlearning practices in
generative models.
• Broader Categorization: We cover the full spectrum of existing unlearning techniques for
generative models, including generative image models, (Large) Language Models (LLMs), and
Multimodal (Large) Language Models (MLLMs). Specifically, our categorization is based on
how the target knowledge is removed from the pre-trained generative models, incorporating
two distinct categories: Parameter Optimization, and In-Context Unlearning. The advantages
and disadvantages of each method are comprehensively discussed in the survey.
• Future Directions: We investigate the practicality of contemporary GenAI MU techniques
across various downstream applications. Additionally, we thoroughly discuss the challenges
present in the existing literature and highlight prospective future directions within the field.
The remainder of the survey is structured as follows: Section 2 introduces a comprehensive summary
of evaluation metrics for GenAI MU strategies, aligning with newly formulated objectives. Section
3 introduces the background knowledge of machine unlearning and generative models. Section
4 provides a comprehensive categorization of existing unlearning strategies for different types
of generative models, with a thorough emphasis on their relationships and distinctions. Section
5 presents the commonly used datasets and prevalently used evaluation benchmarks for GenAI
MU in various downstream applications. Section 6 offers a comprehensive overview of realistic
applications of unlearning techniques. Section 7 discusses potential challenges in GenAI MU and
offers several opportunities that may inspire future research. Finally, Section 8 concludes the survey.
2
EVALUATION METRICS
Before delving into the taxonomy of GenAI MU techniques in details, in this section, we first
introduce various evaluation metrics commonly used to evaluate the effectiveness of different
GenAI MU strategies from varied perspectives. We align the metrics with our pre-defined objectives
for a better demonstration. An overall assessment of a harmful unlearning application on LLM is
displayed in Figure 2.
2.1
Accuracy
Accuracy is a fundamental evaluation metric for measuring the effectiveness of unlearning tech-
niques in Gen AI models. It assesses the extent to which the model successfully forgets specific
unwanted target knowledge. This metric captures the expectation that the unlearned model 𝑔∗
does not produce outputs that it should have forgotten regardless of the input 𝑥. By maximizing
Acc(𝑔∗; ˜D𝑓), we ensure that the unlearned model does not generate any outputs associated with the
target forget dataset, demonstrating high accuracy. Accuracy can be easily defined to evaluate the
performance of GenAI MU techniques under various scenarios. For example, in safety alignment
applications [52, 96, 178], accuracy can be defined to assess the harmfulness or appropriateness of
the output 𝑦using models such as GPT-4, moderation models [66], and NudeNet detector [6]. In
the context of hallucination reduction [57, 174, 178], accuracy is typically evaluated by comparing
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

111:6
Trovato et al.
the output with the ground truth on factual datasets or benchmarks [80, 87, 135, 180]. For bias
alleviation [68, 179], accuracy can be measured using Equalized Odds (EO) or StereoSet metrics,
which are calculated by comparing the probability assigned to contrasting portions of each sentence,
conditioned on the shared portion of the sentence. Finally, for copyright protection and privacy
compliance [15, 33, 161, 172, 178], accuracy is evaluated by the memorization level, the accuracy
on selected forget set, or the success rate of membership inference attacks, which detect whether a
target data was used to train the model.
2.2
Locality
Accuracy
Generalizability
Locality (MMLU)
20
40
60
80
100
LLaMA2-7b (Original)
Naive Fine-Tuning
Task Vector
Gradient Ascent
Gradient Ascent + Mismatch
SKU
Fig. 2. Overall assessment of three dimensions in the con-
text of harmful unlearning for LLaMA2-7B [155]. Accuracy
and generalizability metrics are calculated based on the
model’s safety rate after applying each approach (i.e. GA
[153], GA+Mismatch [178], SKU [96], Task Vector [61]). The
preserved knowledge dimension is represented by Massive
Multitask Language Understanding (MMLU) [51], which
has been normalized to the same scale as accuracy and gen-
eralizability for consistent comparison.
Locality is the second objective of GenAI
MU, measuring the unlearned model’s ca-
pability to preserve knowledge unrelated
to the unlearned content. In traditional un-
learning, locality is typically measured by
retained accuracy, indicating the model’s
performance on the retaining dataset D𝑟,
which is separated from the known dataset
D. However, defining a specific D𝑟in
GenAI MU can be challenging due to the
large volume and diversity of the pre-
trained dataset. Therefore, in the context
of GenAI models, D𝑟denotes any arbi-
trary set of data that is not part of the
forget set D𝑓, on which the model’s per-
formance needs to be preserved. Local-
ity ensures that the unlearned model re-
tains its ability to produce the same out-
puts as the original model 𝑔for the retain
dataset, thereby demonstrating high local-
ity. Depending on the modality, unlearned
model 𝑔∗may need to preserve different
types of knowledge. For instance, Receler
[58] highlights the importance of local-
ity in text-to-image models by evaluating
whether erasing one concept affects the
synthesis of unrelated concepts. Large Lan-
guage Models (LLMs), on the other hand,
need to maintain performance across various tasks beyond the targeted unlearned skill. Studies
such as [64, 65, 162, 189] measure performance on benchmarks like MMLU [51], MathQA [3], and
GLUE [160] after unlearning to evaluate model locality on unrelated datasets.
2.3
Generalizability
Generalizability is a crucial metric for evaluating the effectiveness of GenAI unlearning approaches,
aligning with the third objective of GenAI MU. It measures the capability of the unlearned model
𝑔∗to handle similar unseen forgetting datasets ˆD𝑓encountered after unlearning, ensuring that the
unlearned knowledge extends to other similar inputs not present in the target forgetting dataset
˜D𝑓. This metric ensures that the unlearned model does not produce outputs related to the forgotten
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

Machine Unlearning in Generative AI: A Survey
111:7
concepts, regardless of the input 𝑥. While the accuracy metric ensures the model does not generate
outputs from the target forget dataset ˜D𝑓, generalizability ensures that unlearning extends to new,
unseen inputs related to the forgotten content. For example, when removing harmful information
from LLMs, studies such as SKU [96] and LLMU [178] evaluate model performance on unseen
harmful queries. Similarly, when removing sensitive concepts like nudity from text-to-image models,
it is essential to ensure all related images are recognized and erased [37, 52, 58, 75]. This verifies
that the model does not produce outputs related to the unlearned concepts with new inputs.
3
BACKGROUND
In this section, we first provide the basics knowledge of generative models as background knowl-
edge and an overview of machine unlearning of non-generative models to further facilitate the
understanding of technical details in GenAI MU.
3.1
Generative Models
3.1.1
Generative Image Models. Deep learning has extensively explored various generative image
models, including Autoencoders, Generative Adversarial Networks (GANs), Diffusion Models, and
Text-to-Image Diffusion Models. In particular, autoencoders [71, 158] comprise an encoder that
transforms an image into a latent vector and a decoder that generates new images from these vectors.
GANs [46, 111, 168] use a min-max game where the generator creates images and the discriminator
distinguishes real from generated ones, refining both through adversarial training. Diffusion
Models [53, 136] generate images via a forward process that adds noise to an image and a reverse
process that denoises it to reconstruct the original. Text-to-Image Diffusion Models [78, 193]
like Stable Diffusion [136], MidJourney, and DALL-E2, generate images from textual descriptions
using Latent Diffusion Models (LDMs) combined with CLIP [131]. Advanced techniques like Textual
Inversion [36] and DreamBooth [139] have recently enhanced these models for customized image
editing.
3.1.2
(Large) Language Models. Language models often refer to probabilistic models that generate
the likelihood of word sequences to predict future tokens [81]. On the other hand, Large Language
Model (LLMs) are deep neural language models with billions of parameters, pre-trained on extensive
text corpora to understand natural language distribution and structure [190]. Almost all LLMs use
the transformer architecture for its efficiency [157]. In particular, there are three types of language
model designs: encoder-only, decoder-only, and encoder-decoder. Encoder-only models like BERT
[26] use bidirectional attention and are pre-trained on tasks like masked token prediction, enabling
them to adapt quickly to various downstream tasks. Decoder-only models like GPT [132] are trained
on next-token prediction tasks using the transformer decoder architecture. Encoder-decoder models
like T5 [133] are trained on text-to-text tasks, with encoders extracting contextual representations
and decoders mapping these representations back to text. In the context of LLMs, most models
contain the decoder-only architecture for simplicity and efficient inference.
3.1.3
Multimodal (Large) Language Models. Multimodal (Large) Language Models (MLLMs) refers
to the models that enable LLMs to perceive and integrates information from various data modalities.
One key branch of MLLMs is vision-language pre-training, which aims to enhance performance on
vision and language tasks by learning multimodal foundation models. Vision Transformer (ViT)
[28] introduced an end-to-end solution by applying the Transformer encoder to images, while CLIP
[131] used multimodal pre-training to convert classification into a retrieval task, enabling zero-shot
recognition. Recent advancements in LLMs like LLaMA [155], and GPT [1] have benefited from
scaled-up training data and increased parameters, leading to substantial improvements in language
understanding, generation, and knowledge reasoning. These developments have popularized the
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

111:8
Trovato et al.
use of auto-regressive language models as decoders in vision-language tasks, facilitating knowl-
edge sharing between language and multimodal domains, thereby promoting the development of
advanced MLLMs like GPT-4v and LLaVA [91].
3.2
Machine Unlearning for Non-Generative Models
Generative AI
(Large) Language Model
Generative Image Model
Multimodal (Large) 
Language Model
…
…
…
Fig. 3. Different types of Generative models.
The concept of Machine Un-
learning (MU) was first raised by
[13] in response to privacy reg-
ulations like the General Data
Protection Regulation of the Eu-
ropean Union [54] and the Cal-
ifornia Consumer Privacy Act
[124] have established the right
to be forgotten [12, 24, 42], which
mandates the elimination of spe-
cific user data from models upon
removal requests. In particular,
previous works have considered
MU for non-generative models
in two criteria: Exact Unlearning
and Approximate Unlearning.
3.2.1
Exact Unlearning. The most
straightforward approach to "unlearn" a data point is to remove it from the training set and then
retrain the model from scratch, which can be both expensive and inefficient. Exact Unlearning
aims to eliminate all information related to the selected data so that the unlearned model performs
identically to a retrained model. [12] first introduced the SISA framework, which partitions data
into shards and slices, with each shard serving as a weak learner that can be quickly retrained upon
an unlearning request. [42] presented unlearning approaches for k-means clustering. However,
exact unlearning does not allow algorithms to trade privacy for utility and efficiency due to its
stringent privacy criteria.
3.2.2
Approximate Unlearning. On the other hand, approximate unlearning [19, 20, 45, 116] only
requires the unlearned model to perform similarly to the retrained model, allowing for a better bal-
ance between utility and efficiency. For example, ConMU [97] introduces a controllable unlearning
pipeline that aims to address the trilemma within the realm of MU, focusing on balancing privacy,
utility, and runtime efficiency. Additionally, Koh and Liang [73] approximates model perturbation
towards empirical risk minimization on the retaining datasets using techniques like the inverse of
the Hessian matrix. Golatkar et al. [44] employs Fisher-based unlearning techniques to estimate
the impact of removing specific information and propose methods to effectively remove this infor-
mation from intermediate layers of DNNs. These strategies highlight the potential of approximate
unlearning to achieve efficient and effective unlearning without the need for complete retraining.
3.3
Relationship with Model Editing
Another closely related concept to GenAI unlearning is model editing, which focuses on the local
or global modification of specific pre-trained model behaviors by replacing outdated, incomplete, or
inappropriate information with new knowledge. There are several similarities between these two
domains. Firstly, they overlap when the objective of model editing is to erase certain information,
as editing techniques can be considered a form of or an aid to the unlearning process in in-context
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

Machine Unlearning in Generative AI: A Survey
111:9
Table 2. Comparisons among different categories of methods across the three metrics. The properties at
which each type of strategy excels are highlighted with check marks.
Category
Strategy
Accuracy Locality Generalizability
References
Parameter Optimization
Gradient-based w reverse loss
✔
✔
[84, 178, 179, 185]
Gradient-based w/o reverse loss
✔
[86, 101, 162]
Data Sharding
✔
✔
[23, 68, 98]
Knowledge Distillation
✔
[27, 60, 161]
Extra Learnable Layers
✔
✔
✔
[14, 58, 74]
Task Vector
✔
✔
[61, 96, 118],
Parameter Efficient Module Operation
✔
✔
[57, 184, 192]
In-context Unlearning
In-context Unlearning
✔
✔
[25, 125]
unlearning [25] and large-scale knowledge unlearning [164]. Secondly, the evaluation of model
editing can align with that of unlearning, as the techniques are measured in terms of locality and
Generalizability [16, 21, 112, 120]. It is crucial to ensure that both editing and unlearning scopes can
modify the appropriate amount of knowledge without compromising the model’s general abilities
in other areas [47, 120]. Additionally, both model editing and unlearning share commonalities in
their approaches, regardless of the modality. They first identify the knowledge or information that
needs to be edited or unlearned and then apply the corresponding techniques based on specific
requirements [41, 83, 163].
However, despite the commonalities, model editing and unlearning differ in several key aspects.
First, while both model unlearning and editing target specific pre-trained knowledge within the
model, model editing replaces obsolete knowledge with updated information. In contrast, the pri-
mary objective of unlearning is to eliminate the influence of particular knowledge without affecting
the model’s utility in various downstream applications. Secondly, the responses to unlearning are
more diverse and unpredictable compared to model editing. For instance, if the target concept
harmful knowledge, unlearned GenAI models responses should not contain any harmful knowledge,
whereas the edited GenAI models only changes specific harmful question answer mapping.
In addition, ineffective unlearning may further hurt model reasoning ability on unrelated content.
3.4
Relationship with RLHF
One popular technique for aligning model behaviors with human values is RLHF [9, 66, 149, 180].
Some existing GenAI unlearning works [96, 178] have demonstrated that datasets created from RLHF
can be beneficial in the unlearning or evaluation process. However, RLHF has several drawbacks:
firstly, collecting human inputs and outputs is expensive, as it typically requires a large number of
professional annotators. Secondly, RLHF approaches are computationally intensive, demanding
extensive GPU resources for further training the model, often for hours or even days. In contrast,
GenAI unlearning offers an effective alternative to RLHF. It requires only the collections of target
negative examples ˜D𝑓and unseen samples ˆD𝑓that practitioners intend the model to unlearn,
which are easier and cheaper to collect through user reporting or red teaming compared to the
positive samples needed for RLHF. Additionally, it is computationally efficient, as unlearning can
usually be accomplished with the same resources required for fine-tuning.
4
METHODOLOGY CATEGORIZATION
In this section, we summarize the current approaches for generative model unlearning. In particular,
we first classify all the approaches into two categories based on their actions to D𝑓and D𝑟:
Parameter Optimization, and In-Context Unlearning, as it displayed in Table 3. Then, we
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

111:10
Trovato et al.
further divide those categories into subcategories representing the unlearning applications and
targets. Subsequent subsections will delve into the detailed explanations of each subcategories.
4.1
Parameter Optimization
4.1.1
Overview: The unlearning approach through parameter optimization aims to adjust spe-
cific model parameters to selectively unlearn certain behaviors without affecting other functions.
This method aims to perform minimal alterations to parameters linked with target forget data ˜D𝑓
influences or biases to preserve essential model performance. However, such direct modifications
to parameters can unintentionally impact unrelated parameters, potentially reducing model utility.
Parameter optimization for unlearning can be implemented through various methods, includ-
ing gradient-based adjustments, knowledge distillation, data sharding, integrating extra
learnable layers, task vector, and parameter efficient module operation.
4.1.2
Gradient Based. Gradient based unlearning approaches aim to adjust the model parameters
in a way that selectively forgets the knowledge associated with specific data points or patterns.
This is achieved by optimizing the model’s loss function in reverse (for gradient ascent) or forward
(for gradient descent) directions to effectively remove or mitigate the learned associations without
significantly impacting the model’s overall performance.
Gradient-based with reverse loss: The gradient ascent (GA) based approach is originate from a
typical optimization-based technique raised by [153]. Given a target forget set ˜
D𝑓and an arbitrary
loss function L(𝜃), the GA algorithm iteratively update the model at each training step 𝑡:
𝜃𝑡+1 ←𝜃𝑡+ 𝜆∇𝜃𝑡L(𝜃)
where 𝜆is the unlearning rate and 𝜃𝑡denotes model parameters at step 𝑡. The objective of such
approach reverts the change of the gradient descent without reverse loss term during the training
with its opposite operation. KUL [65] implements the gradient ascent (GA) method to intentionally
maximize the loss function, thereby shifting the model’s predictions for specific target samples in
the opposite direction. Notably, during the unlearning process, KUL updates only a small subset of
𝜃to unlearn the target unlearn sample ˜
D𝑓. However, KUL may be impractical in scenarios where
sensitive information is not explicitly defined. Building on this concept, UnTrac [64] employs
the GA approach but extends its application to estimate the influence of a training dataset on a
test dataset through unlearning. Nevertheless, though GA based approach can obtain an excellent
unlearning performance, this usually comes with a large sacrifice of the model utility with non-
target samples (i.e. catastrophic collapse), as it elaborated in [96, 178]. To address this utility
degradation, NPO[185] leverages the principles of preference optimization but uniquely applies
it using only negative samples. By modifying the standard preference optimization to exclude
positive examples, NPO aims to make the model forget target undesirable data ˜
D𝑓by decreasing
their prediction probability. Additionally, the work theoretically shows that the progression toward
catastrophic collapse can be exponentially slower when using NPO loss than pure GA. Specifically,
NPO approach utilizes the following loss function:
L𝑁𝑃𝑂,𝛽(𝜃) = −2
𝛽ED𝑓

log𝜎

−𝛽log 𝑔∗(𝑦|𝑥)
𝑔(𝑦|𝑥)

.
In this formulation, 𝛽is a scaling parameter that adjusts the sharpness of the preference modifica-
tions. The term log 𝑔∗(𝑦|𝑥)
𝑔(𝑦|𝑥) measures how the predictions of the current unlearned model deviate
from those of the pre-trained model. This loss function aims to minimize the likelihood of the
model predicting outcomes linked to the ˜D𝑓, effectively diminishing the model’s recall of this data.
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

Machine Unlearning in Generative AI: A Survey
111:11
Table 3. Representative MU approaches for GenAI models. The approach highlighted in • falls under the
parameter optimization category, whereas the approach emphasized in • belongs to the in-context
unlearning category.
Approach
Unlearning Applications
Generative Backbones
References
Gradient-based
w/o reverse loss
Safety Alignment
LLaMA2/OPT
[178], [100]
T5
[64]
Diffusion Models, VAEs
[33], [52]
Hallucination Reduction
LLaMA2/OPT
[178]
T5
[64]
MLLM
[174], [15]
Bias/Unfairness Alleviation
BERT, RoBERTa
[179]
T5
[64]
Copyright Protection
GPT Family/OPT
[65]
LLaMA2/OPT
[178], [185]
Diffusion Models, VAEs
[33], [52], [75], [183]
Privacy Compliance
MLLM
[15]
GPT Family/OPT
[65]
LLaMA2/OPT
[178], [185], [48]
LSTMs, BiDAF
[84]
Diffusion Models, VAEs, MAE, GAN
[52], [79], [154], [148]
Gradient-based
w reverse loss
Safety Alignment
Diffusion Model
[171], [37]
LLaMA2
[189]
Privacy Compliance
NEO/OPT
[70]
Diffusion Model, GAN, VAE
[113], [37], [183]
GPT family
[162], [101]
Copyright Protection
LLaMA2
[32]
Diffusion Model
[171], [37], [183]
Hallucination Reduction
MLLM
[86]
Knowledge Distillation
Privacy Compliance
GPT family/LLaMA2
[27], [60]
DistillBERT, encoder-decoder transformers
[161]
Data Sharding
Bias/Unfairness Alleviation
BERT/Distilgpt2
[68]
Privacy Compliance
LSTM
[98]
Diffusion Model
[23]
Extra Learnable Layers
Safety Alignment
T5
[14]
Diffusion Models
[58]
Privacy Compliance
T5
[14]
BERT
[74]
Diffusion Models
[58]
Hallucination Reduction
GPT Family
[164]
Task Vector
Safety Alignment
LLaMA2/OPT
[96], [127]
GPT family
[127]
T5
[61]
Copyright Protection
LLaMA3
[29]
PEMO
Safety Alignment
LLaMA2/OPT/Alpaca
[192], [57], [184]
Hallucination Reduction
LLaMA2/OPT
[57]
In-Context Unlearning
Hallucination Reduction
BERT/GPT family
[25]
Privacy Compliance
T5
[125]
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

111:12
Trovato et al.
Besides, numerous other works have built based on GA approach and extend with more efficient
module. For example, LLMU [178] builds upon traditional GA methods and incorporates two
arbitrary loss functions Lforget and Lmismatch to prevent model from generating unwanted outputs
and maintains model functionality on normal prompts, which can be expressed as:
𝜃𝑡+1 = 𝜃𝑡−𝜖1 · ∇𝜃𝑡Lforget −𝜖2 · ∇𝜃𝑡Lmismatch −𝜖3 · ∇𝜃𝑡Lmaintain.
Similarly, Eraser [100] enhances the Lforget objective by adding random prefixes and suffixes to
each query from D𝑟during training to simulate jailbreaking attacks, ensuring robust unlearning
against varied prompts. It uses GPT-3.5 to extract entities from unwanted knowledge, which in this
specific work refers to harmful responses, creating a dataset that helps the model retain general
knowledge about entities in jailbreak prompts. Eraser also captures refusal responses from the
original aligned model 𝜃and trains the unlearned model 𝜃𝑢to replicate these refusals, maintaining
the model’s safety alignment by rejecting harmful inquiries. Besides, GA approach can be employed
on MLLMs to mitigate multimodal hallucination problems. For example, EFUF [174] operates by
initially assessing text-image congruence using the CLIP model, which scores the relevance of
text descriptions to their corresponding images. Based on these scores, the framework identifies
hallucinated content—descriptions that mention objects not present in the images.
Gradient-based without reverse loss: Given that the gradient ascent (GA) based approach
may degrade model performance, regular gradient descent offers a more balanced method for
model training and unlearning. Within this framework, various innovative methods have been
developed. Inspired by the Lottery Ticket Hypothesis [34], PCGU [179] hypothesizes that a subset
of the model’s neurons encode biases/preferences in different contexts. PCGU introduces a weight
importance algorithm using gradient calculations to rank weights by their contribution to biased
outputs. Specifically, model parameter 𝜃and gradient ∇𝑖are partitioned into {𝜃1,𝜃2, ...𝜃𝑚} and
{∇1
𝑖, ∇2
𝑖, ..., ∇𝑚
𝑖}. The impact of the weights on biased outputs is quantified by calculating the dot
product 𝜃𝑗· ∇𝑗
𝑖for each 𝑗, identifying weights with the highest influence on biased behavior for
targeted modification in the unlearning phase.
Building on the foundation of targeting specific model components for unlearning, [48] proposes
two novel unlearning methods: Fisher Removal and Fisher Forgetting for LLM leveraging
second-order information, specifically the Hessian, to enhance the unlearning process. In particular,
the Fisher Removal method updates the model parameters aggressively to ensure the removal of
undesirable data, which can sometimes compromise the utility of the model. The objective of this
method can be formulated as:
𝜃𝑢= 𝜃−𝛾· 𝐹−1 · ∇𝐿(𝜃),
where 𝐹−1 represents the inverse Fisher information. Then, Fisher Forgetting is a less aggressive
variant of Fisher Removal, better preserving the model’s accuracy even through multiple cycles of
unlearning. This method modifies the parameters by adding a controlled amount of noise 𝜂:
𝜃𝑢= 𝜃−𝛾· 𝐹−1 · ∇𝐿(𝜃) + 𝜂.
Further advancing the complexity of unlearning strategies, min-min optimization [84] adapts the
bi-level optimization approach previously applied to images for creating unlearnable text, which
the objective can be formulated as:
argmin𝜃E(𝑥+𝜂,𝑦)∼𝐷
h
argmin𝜂L(𝑔(𝑥+ 𝜂),𝑦)
i
.
Specifically, the method leverages error-minimization modifications to alter targeted forget data ˜
D𝑓
through a controllable noise 𝜂in such a way that it remains unlearnable even for models not seen
during the optimization process. Building on gradient descent applications in machine learning,
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

Machine Unlearning in Generative AI: A Survey
111:13
this approach adapts to generative models like vision models. Li et al. [79] introduces a framework
for i2i (image-to-image) generative models using a two-part encoder-decoder architecture. The
encoder converts input from ˜
D𝑓into a representation vector, while the decoder reconstructs the
image. The objective is to maximize the output distribution difference between the retained data
(D𝑟) and the target forget set ( ˜D𝑓), minimizing modifications to the retained data. Expanding on
manipulating model outputs, Kumari et al. [75] minimizes KL divergence between conditional
distributions of target and anchor concepts, modifying the model to generate images of the anchor
concept when prompted with the target concept.
Besides manipulating the distance between ˜D𝑓and D𝑟, these datasets can be directly used
for fine-tuning the model for unlearning. For instance, UBT [86] mitigates backdoor attacks by
dividing D𝑟into suspicious and clean subsets using a pre-trained model based on multimodal text
similarity. Target unlearning samples ( ˜D𝑓) are fine-tuned to amplify backdoor features, increasing
cosine similarity measures. A token-level local unlearning strategy then selectively forgets tokens
associated with backdoor triggers. Target unlearned samples ˜D𝑓(i.e. negative samples) are also
effective in vision generative models. Forget-Me-Not [183] uses curated images related to the
concept targeted for removal during fine-tuning, adjusting the model’s cross-attention layers within
the UNet architecture to minimize attention to these concepts. ESD [37] refines this by modifying
model parameters to remove undesired concepts ˜D𝑓, such as nudity or copyrighted styles, without
dataset censorship or post-generation filtering. ESD fine-tunes the model using a short textual
description of the target undesired concepts ( ˜D𝑓), aiming to reduce the probability of generating
images described by 𝑥:
𝜖𝜃𝑢(𝑦𝑡,𝑥,𝑡) ←𝜖𝜃𝑜(𝑦𝑡,𝑡) −𝜂[𝜖𝜃𝑜(𝑦𝑡,𝑥,𝑡) −𝜖𝜃∗(𝑦𝑡,𝑡)]
Negative sample fine-tuning may involve complexities in identifying undesirable behaviors accu-
rately, leading to potential biases. Hence, some approaches directly use D𝑟for fine-tuning. For
example, EraseDiff formulates unlearning as a constrained optimization problem. The objective is
to fine-tune the model using the remaining data D𝑟to preserve utility while erasing the influence
of forget data. This is achieved by deviating the generative process from the ground-truth denoising
procedure:
min
𝜃𝑜L(𝜃𝑜, D𝑟)
s.t.
𝑔(𝜃𝑜, ˜D𝑓) −min
𝜙|𝜃𝑜𝑔(𝜙, ˜D𝑓) ≤0,
where𝑔(·) measures the influence of ˜D𝑓. The constraint ensures that unlearning targets ˜D𝑓without
degrading performance on D𝑟.
Building on those fine-tuning approaches, which directly modify model behaviors using specific
dataset, Selective Amnesia (SA) [52] introduces a controllable forgetting mechanism applied to
conditional variational likelihood models, including variational autoencoders (VAEs) and large-
scale text-to-image diffusion models. SA utilizes continual learning techniques like Elastic Weight
Consolidation (EWC) [72] and Generative Replay (GR) [145] to preserve knowledge while selectively
forgetting specific concepts. Additionally, SA introduces a surrogate objective to guarantee the
reduction of the log-likelihood of the data to be forgotten, providing a more controlled and effective
unlearning process. However, SA is more suitable for removing specific samples and fails to be
generalizable to unlearning knowledge in a broader spectrum, such as nudity. Hence, SalUn [33]
method extends these principles to further refine the unlearning process by specifically targeting
the modification of the model’s response to various class representations. In particular, the objective
of concept unlearning for SalUn can be mathematically formulated as:
minimize
𝐿SalUn(𝜃𝑢) := E(𝑥,𝑐)∼˜
𝐷𝑓,𝑡,𝜖∼N(0,1),𝑐′≠𝑐
h𝜖𝜃𝑢(𝑥𝑡|𝑐′) −𝜖𝜃𝑢(𝑥𝑡|𝑐)
2
2
i
+ 𝛽LMSE(𝜃𝑢; 𝐷𝑟),
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

111:14
Trovato et al.
where 𝜖𝜃𝑢(𝑥𝑡|𝑐) and 𝜖𝜃𝑢(𝑥𝑡|𝑐′) represent the noise generator parameterized by 𝜃𝑢, conditioned on
the text prompt 𝑐and 𝑐′.
4.1.3
Knowledge Distillation. Knowledge distillation approaches typically involve a teacher-
student model configuration and treat the unlearned model 𝑔∗as the student model, aiming to
mimic the desirable behavior from a teacher model. KGA [161] introduces a novel framework to
align knowledge gaps, defined as differences in prediction distributions between models trained
on different data subsets. The KGA framework adjusts the model to minimize the discrepancy in
knowledge between a base model trained on all data and new models trained on an external set
D𝑛similar to D but excluding D𝑓. This is formulated as:
𝑓∗= argmin𝑓
dis(D𝑛) (𝑔,𝑔𝑛) −dis( ˜D𝑓) (𝑔,𝑔𝑓)
 .
Here, 𝑔is the original model trained on D, while 𝑔𝑛and 𝑔𝑓are models trained on D𝑛and ˜D𝑓,
respectively. The function dis measures the distributional differences using KL divergence. KGA
treats the original model 𝑔as a teacher model and minimizes the distance of output distributions
when feeding samples in D𝑟to 𝑔∗and 𝑔. Moreover, [27] introduces a novel LLM unlearning ap-
proach named deliberate imagination (DI) to further maintain model’s generation and reasoning
capabilities. DI employs a self-distillation technique in which a teacher model guides an LLM to
generate creative and non-harmful responses, rather than merely forgetting unwanted memorized
information. The process begins by strategically increasing the probability tokens within the teacher
model that serve as alternatives to the memorized ones, thereby encouraging the production of
novel and diverse outputs. Subsequently, DI fine-tunes the student model using the predicted output
probabilities from the teacher, enabling the student models to generate imaginative responses that
are less dependent on memorized data.
Similar to many white-box approaches, the KGA approach depends on accessing the model’s
internal weights, which makes it inapplicable to black-box models. To address this issue, 𝛿learn-
ing [60] introduces a novel framework for unlearning in black-box LLMs without accessing to
the model’s internal weights. Specifically, 𝛿learning utilizes a pair of smaller, trainable models
to compute a logit offset that is then applied to the black-box LLM to adjust its responses. These
smaller trainable models, referred to as offset models, are trained to predict how the logit outputs
of the black-box LLM should be modified to exclude the influence of ˜D𝑓. The offset is calculated as
the difference in logits between these two models and added to the black-box LLM’s logits, guiding
the final output away from sensitive information.
4.1.4
Data Sharding. Data sharding approaches usually divide the training data into multiple
shards, where each corresponds to a subset of the overall data 𝐷. Inspired by SISA [12], separate
models are trained for each data shards that can effectively remove target data in different shard
based on request. As it mentioned in Section 3, SISA provides an exact unlearning guarantee
due to the data to be forgotten have no impacts on the retrained version of the model, making it
suitable to wide range of model architectures, including GenAI. However, direct implementation
of SISA on GenAI is infeasible due to its the high computational cost in model saving, retraining,
and inference. Hence, various work have adapted the similar approach of SISA and made more
suitable to GenAI depends on its corresponding unlearning scenario. FairSISA [68] proposes a
post-processing unlearning strategy for bias mitigation in ensemble models produced by SISA. In
particular, this post-processing function 𝑓aims to enforce fairness in the outputs of 𝑔∗, particularly
ensuring that the model’s predictions do not exhibit bias related to a sensitive attribute 𝐴. This can
be formulated as follows: 𝑓post-processes the outputs of 𝑔∗to satisfy fairness constraints, such
as equalized odds, across groups defined by 𝐴without significantly affecting the overall model
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

Machine Unlearning in Generative AI: A Survey
111:15
accuracy:
min
𝑓
E[ℓ(𝑓(𝑔∗(𝑥)),𝑌)]
subject to:
Pr(𝑓(𝑔∗(𝑥)) = 1|𝐴= 0,𝑌= 𝑦) = Pr(𝑓(𝑔∗(𝑥)) = 1|𝐴= 1,𝑌= 𝑦),
∀𝑦∈{0, 1}.
Here, ℓis a loss function, and E denotes the expected value, reflecting the objective to minimize
any deviation from the true labels 𝑌while adhering to the fairness constraints.
Next, to reduce retraining costs while preserving utility during inference, Liu and Kalinli [98]
introduces the LOO ensemble method to unlearn target token sequences from LMs. This method
uses a teacher-student framework where multiple teacher models are trained on partitioned data
segments. When data removal is requested, the student model (i.e. base LM) is supervised by the
remaining teachers, excluding the one trained on the segment to be unlearned. The recalibration is
formalized as:
𝑔−𝑘
𝐿𝑂𝑂−𝐸(𝑤|𝑤𝑡−1
1
) =
1
𝑀−1
𝑀
∑︁
𝑚=1
1{D𝑘⊄𝐵𝑚} · 𝑝𝜃𝑚(𝑤|𝑤𝑡−1
1
)
where 𝑔−𝑘
𝐿𝑂𝑂−𝐸is the aggregated prediction excluding the teacher trained with D𝑘⊂D𝑓. 𝜃𝑚
denotes the parameters of the m-th teacher model, and 𝑤𝑡−1
1
reflects the input sequence up to step
𝑡−1. The student model fine-tunes its parameters based on KL divergence, ensuring sensitive
data is unlearned while maintaining performance on D𝑟. Despite improved utility performance
compared to SISA, LOO lacks a theoretical guarantee for teacher models and may impose significant
computational overheads on GenAI like LLMs.
The data sharding approach can be applied to vision generative models and LLMs for measuring
training data influence for unlearning. Dai and Gifford [23] presents temporary unlearning using
ensemble ablation. The core idea is to use an ensemble of diffusion models, each trained on a
subset of data, to assess the influence of individual data points by excluding models exposed to the
data point and observing changes in outputs. The work provides a theoretical guarantee that the
resulting model has not seen the removed training sample. The ensemble is represented as 𝑓𝑒:
𝑓𝑒(𝑥,𝑡) = E𝑆∼D [E𝑟∼𝑅[𝑓(𝑥,𝑡,𝐴(𝑆,𝑟))]]
where 𝑥is the input, 𝑡is the diffusion time, D represents the uniform distribution over 2D, and
𝐴(·) denotes the training procedure with training samples and exogenous noise 𝑟. To assess the
influence of a specific point ˜𝑥, the ensemble without ˜𝑥, 𝑓−˜𝑥
𝑒
, is evaluated as:
𝑓−˜𝑥
𝑒
(𝑥,𝑡) =
1
Pr(𝑥∈𝑆′ ∼X) E𝑆∼D [E𝑟∼𝑅[𝑓(𝑥,𝑡,𝐴(𝑆,𝑟))1 ˜𝑥∉𝑆]].
This method aligns with machine unlearning by providing a more efficient way to disregard specific
data influences without extensive retraining. Using an ensemble approach ensures effective data
attribution and compliance with privacy and fairness.
4.1.5
Extra Learnable Layers. Another parameter optimization approach is to introduce addi-
tional parameters or trainable layers in the model and train them to actively forget different sets of
data from ˜D𝑓. This approach negates the necessity of modifying the model’s inherent parameters,
thereby preventing interference to its original knowledge. EUL [14] integrates an additional un-
learning layer, 𝑈(𝑖)
𝑙
, into the transformer following the feed-forward networks. Throughout the
training, this unlearning layer is exclusively engaged to learn to forget the specified data, while
the rest of the model parameters, denoted as 𝜃𝑜, remain frozen. Upon receiving a deletion request,
the model first trains a distinct unlearning layer 𝑈(𝑖)
𝑙
with parameters 𝜃𝑖tailored to that request.
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

111:16
Trovato et al.
Subsequently, these layers are merged using a fusion mechanism to form a unified unlearning layer
with parameters 𝜃𝑚, achieved by minimizing a regression objective:
min
𝜃𝑚
∑︁
𝑖
𝜃𝑇
𝑚𝑋(𝑖)
𝑓
−𝜃𝑇
𝑖𝑋(𝑖)
𝑓

2
where 𝜃𝑚is defined as:
𝜃𝑚=
 ∑︁
𝑖
𝑋(𝑖)𝑇
𝑓
𝑋(𝑖)
𝑓
!−1 ∑︁
𝑖

𝑋(𝑖)𝑇
𝑓
𝑋(𝑖)
𝑓𝜃𝑖

.
This results in a unified unlearning transformer capable of handling multiple deletion requests
sequentially, ensuring that all specified data is effectively forgotten while preserving the integrity
of model behaviors on other tasks.
Similarly, Receler [58] introduces lightweight eraser parameters 𝜃𝐸during unlearning, which
is designed to remove the target concept from the outputs of each cross-attention layer within
the diffusion U-Net. To erase the concept, the eraser is trained to predict the negatively guided
noises that move the model’s prediction away from the erased concept. In particular, the objective
is defined as:
LErase = E𝑦𝑡,𝑡

∥𝜖𝜃𝑢(𝑦𝑡,𝑒𝑥,𝑡) −𝜖𝐸∥2
where 𝜖𝐸= 𝜖𝜃(𝑦𝑡,𝑡) −𝜂[𝜖𝜃(𝑦𝑡,𝑒𝑥,𝑡) −𝜖𝜃(𝑦𝑡,𝑡)] ,
where 𝜃𝑢is the designated unlearned model with 𝜃𝑜plugged with eraser 𝜃𝑒. 𝑦𝑡is the denoised
image at timestep 𝑡sampled from 𝜃𝑢conditioned on target concept 𝑥, 𝑒𝑥is the text embedding of
concept 𝑥, and 𝜖𝐸is the negatively guided noise predicted by the original model 𝜃𝑜. By minimizing
the L2 distance between 𝜖𝜃𝑢(𝑦𝑡,𝑒𝑥,𝑡) and 𝜖𝐸, the eraser learns to reduce the probability of the
generated image 𝑦belongs to the target concept 𝑥, thus effectively erasing the concept.
Kumar et al. [74] proposes two extensions to the SISA framework [12]— SISA-FC and SISA-A
— to facilitate guaranteed unlearning that is efficient in terms of memory, time, and space for
LMs. In particular, SISA-FC pre-trains a base model on a generic text corpus and then integrates
fully connected layers atop it. During optimization, only the parameters of the linear layers are
fine-tuned. This approach reduces the overall training time since backpropagation of gradients
occurs solely in the final layers, and only the weights of the additional parameters are stored.
Nevertheless, the addition of linear layers to SISA might compromise the model’s utility when
compared to fine-tuning the entire model [26]. To address this, SISA-A incorporates adapters [55]
into the encoder blocks of the transformer. This method results in only a marginal increase in the
model’s memory footprint—about 1 to 5 % —thus providing a memory benefit of 95 to 99 %.
4.1.6
Task Vector Methods. Inspired by recent work of weight interpolations [2, 35, 62, 108, 170],
Ilharco et al. [61] first proposes the concept of task vector, which can be obtained by taking the
difference between the original model weights of a pre-trained model and its weights fine-tuned on
a specific task. In particular, if we let 𝜃𝑡
𝑓𝑡be the corresponding weights after fine-tuning on task
𝑡, the task vector is then denoted as 𝜏𝑡= 𝜃𝑡
𝑓𝑡−𝜃𝑜. Then, taking the element-wise negation of the
task vector 𝜏𝑡can enable 𝜃𝑜to forget target knowledge on task 𝑡without jeopardizing irrelevant
knowledge, resulting in an unlearned model that has weight of 𝜃𝑢= 𝜃𝑜−𝜆𝜏with 𝜆as a scaling
term. One exemplar work is SKU [96], which designs a novel unlearning framework to eliminate
harmful knowledge while preserving utility on normal prompts. In particular, SKU is consisted
of two stages where the first stage aims to identify and acquire harmful knowledge within the
model, whereas the second stage targets to remove the knowledge using element-wise negation
operation. Different from pure gradient ascent approaches where the model locality is largely
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

Machine Unlearning in Generative AI: A Survey
111:17
compromised, SKU collectively aggregates the target unlearned knowledge ˜D𝑓using gradient
decent approach in the first stage and remove it from the pre-trained model. However, SSU [29]
identifies the potential instability of the pure task vector approach in the case of multiple rounds
of unlearning (i.e., sequential unlearning) and introduces a more stable unlearning framework
integrated with weight saliency.
Besides subtracting undesirable parameters, Pochinkov and Schoots [127] proposes a selective
pruning method to trim those neurons’ relative importance to different datasets, representing
target model capability for unlearning. In particular, it performs either iterative pruning on nodes
in the feed-forward layers or attention head layers. This selective approach utilizes importance
functions that assess the contribution of individual neurons to specific tasks by measuring activation
frequencies and magnitudes, enabling precise targeting of neurons that are crucial for the undesired
capabilities. Furthermore, different from previous weight pruning method, where it requires Hessian
computation, the selective neuron pruning is more computational efficient for large language models
because it directly removes neurons that contribute the most to the unwanted behavior.
4.1.7
Parameter Efficient Module Operation Methods. Inspired by works on merging model
parameters under full fine-tuning [67, 107, 169], Zhang et al. [184] explores composing parameter-
efficient modules (PEM) like LoRA [56] and (IA)3 [93] for flexible module manipulation. Unlike task
vector methods, which modify the global weight vectors, PEM operation methods apply localized
adjustments within specific modules. Similar to task vector methods, the negation operator (⊖)
in PEM methods unlearns stored knowledge within adapter modules. For example, in a LoRA
module denoted as 𝜃lora = {A, B}, A is initialized following a random Gaussian distribution, and B
is initialized to all zeros to recover the pre-trained model at the beginning. We could negate B or A
while keeping the other unchanged to facilitate unlearning or forgetting certain skills (e.g., toxic
data). This process can be written as:
⊖𝜃negation
lora
= ⊖𝜃lora = {A, −B}.
This negation operator changes the intermediate layer’s activation values in the opposite direction
of gradient descent, aiding in unlearning targeted knowledge.
To further enhance model truthfulness and detoxification, Ext-Sub [57] introduces "expert" and
"anti-expert" PEMs. The "expert" PEM, trained on retaining data D𝑟, represents desired behaviors,
while the "anti-expert" PEM, trained on unlearned data ˜D𝑓, represents harmful behaviors. Ext-Sub
identifies commonalities between these PEMs to determine shared capabilities and then subtracts
the deficiency capability responsible for untruthful or toxic content. The Ext-Sub operation is
defined as:
𝜃𝑢= 𝜃𝑒𝑥𝑝𝑒𝑟𝑡⊖𝜆· Ext(𝜃𝑎𝑛𝑡𝑖−𝑒𝑥𝑝𝑒𝑟𝑡),
where 𝜃𝑒𝑥𝑝𝑒𝑟𝑡and 𝜃𝑎𝑛𝑡𝑖−𝑒𝑥𝑝𝑒𝑟𝑡are the parameters of the expert and anti-expert PEMs, respectively,
and Ext(·) is the extraction function isolating the deficiency capability. This process removes harmful
effects while preserving and enhancing the beneficial capabilities of the LLM.
Additionally, PEM can be utilized during model training to prevent the acquisition of harmful
information. For instance, Zhou et al. [192] introduces security vectors, enabling LLMs to be
exposed to harmful behaviors without modifying the model’s original, safety-aligned parameters.
Specifically, these vectors allow the LLM to process and respond to harmful inputs during training,
ensuring that the core parameters remain unaltered. The security vector 𝜃𝑠is optimized to minimize
the loss associated with harmful data:
arg min
𝜃𝑜E(𝑋,𝑌)∼𝐷𝑢

min
𝜃𝑠𝐿(𝑓(𝑋;𝜃𝑜;𝜃𝑠),𝑌)

,
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

111:18
Trovato et al.
where 𝐿is the causal loss computed based on the prediction and the ground truth. Subsequently,
the trained security vector 𝜃∗
𝑠is employed during fine-tuning to ensure that the model learns
from benign data without adopting harmful behaviors. 𝜃∗
𝑠is activated during the forward pass of
training to maintain the model’s response consistency with benign data and to inhibit the learning
of harmful information. Specifically:
𝜃𝑢= arg min
𝜃𝑜E(𝑋,𝑌)∼𝐷𝑟

𝐿 𝑓(𝑋;𝜃𝑜;𝜃∗
𝑠),𝑌
,
ensuring that the original model parameters do not update in a harmful direction.
4.1.8
Summary: The parameter optimization strategies focus on adjusting specific model param-
eters to selectively unlearn certain behaviors without affecting other functions. These approaches
involve precise alterations to parameters associated with unwanted data influences or biases, ensur-
ing the preservation of essential model performance. Gradient-based approaches with reversed loss
are effective for unlearning accuracy and generalizability but can negatively impact model locality
by inadvertently affecting unrelated parameters. In contrast, gradient-based methods without
reversed loss can maximally preserve locality but may not excel in unlearning accuracy and gener-
alizability. Extra learnable layers provide highly targeted unlearning but may demand significant
computational resources. Data sharding methods excel in maintaining locality by partitioning the
training data and ensuring specific data points can be unlearned without extensive retraining,
although they might struggle with generalizability in very large models. Knowledge distillation
is effective in maintaining locality by transferring knowledge to a new model trained to exclude
specific data, thus retaining essential performance while unlearning undesired knowledge. However,
it can be resource-intensive and may not achieve satisfactory accuracy and generalizability. Task
vector and parameter-efficient module operations may perform well in terms of unlearning accuracy
and generalizability. Nonetheless, recent work [29] has highlighted the risk of these approaches
leading to instability due to significant model degradation, resulting in poor locality performance.
4.2
In-Context Unlearning
4.2.1
Overview . Unlike parameter optimization approaches, which actively modifies parameters
either locally or globally via different techniques, in-Context unlearning techniques retain the
parameters in their original state and manipulate the model’s context or environment to facilitate
unlearning. In particular, these strategies result in an unlearned model 𝑔∗
𝜃𝑢where 𝜃𝑢= 𝜃𝑜, but with
changes in how the model interacts with its input or inferences.
4.2.2
In-Context Unlearning. In-context unlearning utilizes the approach of in-context learning
to selectively erase targeted knowledge during inference, treating the model as a black box. This kind
of method is resource-efficient but has inherent limitations due to the nature of in-context learning.
Specifically, it modifies only the model’s immediate outputs without fundamentally eradicating
the unwanted knowledge embedded within the model’s internal parameters. ICUL [125] first
introduces the idea of in context unlearning, which alters input prompts during the inference phase
to achieve targeted unlearning in the situation where model’s API is the only access to the model.
The technical process involves several key steps:
(1) Label Flipping, where the label of the data point that needs to be forgotten is flipped
to contradict the model’s learned associations, resulting in the template "[𝐹𝑜𝑟𝑔𝑒𝑡𝐼𝑛𝑝𝑢𝑡]0
[𝐹𝑙𝑖𝑝𝑝𝑒𝑑𝐿𝑎𝑏𝑒𝑙]0"
(2) Context Construction, where additional correctly labeled examples are sampled and ap-
pended to the flipped example, creating a mixed input sequence, resulting in the template
"[𝐹𝑜𝑟𝑔𝑒𝑡𝐼𝑛𝑝𝑢𝑡]0 [𝐹𝑙𝑖𝑝𝑝𝑒𝑑𝐿𝑎𝑏𝑒𝑙]0 \𝑛; [𝐼𝑛𝑝𝑢𝑡]1 [𝐿𝑎𝑏𝑒𝑙]1 \𝑛; . . . [𝐼𝑛𝑝𝑢𝑡]𝑠[𝐿𝑎𝑏𝑒𝑙]𝑠
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

Machine Unlearning in Generative AI: A Survey
111:19
(3) Inference Adjustment, where this modified prompt is used to ’confuse’ the model about the
original training point, mitigating its influence, resulting in the template "[𝐹𝑜𝑟𝑔𝑒𝑡𝐼𝑛𝑝𝑢𝑡]0
[𝐹𝑙𝑖𝑝𝑝𝑒𝑑𝐿𝑎𝑏𝑒𝑙]0 \𝑛; [𝐼𝑛𝑝𝑢𝑡]1 ; [𝐿𝑎𝑏𝑒𝑙]1 \𝑛; . . . ; [𝐼𝑛𝑝𝑢𝑡]𝑠; [𝐿𝑎𝑏𝑒𝑙]𝑠[𝑄𝑢𝑒𝑟𝑦𝐼𝑛𝑝𝑢𝑡]𝑠+1".
However, directly modifying input prompts does not always give a desirable output. Hence, Lari-
mar [25] integrates an external memory module that directly manipulates the LLM’s outputs.
This approach allows for precise control over knowledge updates and selective forgetting through
operations such as writing, reading, and generating, which are efficiently managed by a hierarchical
memory system.
4.2.3
Summary. Parameter frozen methods retain the model’s parameters in their original state
while manipulating the model’s context or environment to facilitate unlearning. A notable advantage
of this method is its resource efficiency, as it does not require retraining or modification of the
model’s internal parameters, making it suitable for scenarios where direct access to the model’s
internals is limited (e.g. black-box models). However, a significant drawback is that parameter
frozen methods only modify the model’s immediate outputs without fundamentally eradicating
the unwanted knowledge embedded within the model’s parameters. This can lead to incomplete
unlearning, as the underlying knowledge remains intact.
5
DATASETS AND BENCHMARKS
5.1
Datasets
In this section, we summarize the datasets commonly used in the field of Generative AI, as outlined
in Table 4, to benefit future MU research. Instead of merely categorizing the datasets by task
(i.e., generation and classification), they are organized according to their intended unlearning
objectives. We specifically focus on those datasets primarily utilized as target datasets during the
unlearning process, excluding object removal datasets such as CIFAR10 and MNIST, as well as
generic evaluation benchmark datasets like Hellaswag [181] and Piqa [8].
5.1.1
Safety Alignment: The Civil Comments dataset [11] is comprised of public comments from
various news websites, each labeled with a level of toxicity to represent the degree of harmfulness
in the content. Studies such as [61] and [184] have utilized subsets of these highly toxic samples
to extract harmful knowledge from pre-trained models. Complementing this, the Anthropic red
team dataset [5, 39] includes human preference data and annotated dialogues for evaluating red
team attacks on language models. This dataset aids in reducing harm in generative models, as
demonstrated by ForgetFilter [189], which classifies conversations into safe and unsafe categories to
facilitate unlearning harmful responses, and by [192], which generates security vectors to address
harmful knowledge.
Next, the PKU-SafeRLHF dataset [66] consists of 30,000 expert comparison entries with safety
meta-labels, initially proposed by BeaverTails to train moderation models preventing LLMs from
generating harmful outputs. This dataset has been instrumental in various studies; for instance,
LLMU [178] leverages harmful samples for gradient ascent, while SKU [96] extracts harmful
knowledge from multiple perspectives in pre-trained models. Lastly, moving to the multimodal
domain, the LAION dataset [143] contains 5.85 billion CLIP-filtered image-text pairs, supports
large-scale multi-modal model training. Despite its democratizing potential, it poses challenges,
such as the generation of NSFW content by diffusion models. To mitigate these risks, recent studies
like [183] and [37] have identified and targeted subsets of NSFW data within LAION for content
removal.
5.1.2
Copyrights Protection: The Harry Potter [138] has been a focus for studies aiming to elimi-
nate a model’s ability to generate Harry Potter-related content while preserving performance. [32]
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

111:20
Trovato et al.
Table 4. Representative statistics of datasets for GenAI MU approaches with different generative types,
including datasets for both generation (G) and classification (C) tasks.
Dataset
Application
GenAI Type
Task Instance
Used in
LAION
Safety Alignment
Gen Image Models G
5.85 B
[37, 183]
Civil Comments
(L)LMs
C
1,999,514
[61]
PKU-SafeRLHF
G
834,000
[96, 178]
Anthropic red team
G
38,961
[192]
Harry Potter
Copyrights Protection
(L)LMs
G
7 books
[32, 152, 178]
Bookcorpus
G
11,038 books [64, 101, 177, 178]
TOFU
G
200 profiles
[152, 185]
HaluEVAL
Hallucination Reduction
(L)LMs
G
35,000
[178]
TruthfulQA
G
817
[64, 96, 178]
CounterFact
G
21,919
[25, 164]
ZsRE
G
Unknown
[25, 164]
MSCOCO
MLLMs
G
328,000
[37, 38]
Pile
Privacy Compliance
(L)LMs
G
Unknown
[27, 64, 162, 189]
Yelp/Amazon Reviews
C
142.8 M
[125]
SST-2
G
11,855
[74, 84, 125, 184]
PersonaChat
G
8784
[161]
LEDGAR
G
1,081,177
[161]
SAMSum
G
16,369
[14, 189]
IMDB
C
50,000
[14]
CelebA-HQ
Gen Image Models G
30,000
[23, 113, 123, 171]
I2P
G
4703
[37, 58, 171]
StereoSet
Bias/Unfairness Alleviation
(L)LMs
G
17,000
[179]
HateXplain
G
Unknown
[68]
CrowS Pairs
G
1508
[179]
achieved this by fine-tuning the model with a dataset where idiosyncratic expressions were replaced
with common counterparts. Similarly, [178] transformed the book into a question-and-answer
format to evaluate the model’s content generation post-unlearning. However, due to copyright
protections, this dataset is not publicly available. In contrast, the BookCorpus dataset [194], a
large collection of freely available novels, comprises 11,038 books across various sub-genres. It was
used in training the initial GPT model by OpenAI [132] and has been utilized in GenAI unlearning.
[178] used BookCorpus as a retaining dataset to maintain performance on unrelated samples, while
UnTrac [64] employed it to assess the impact of dataset removal.
5.1.3
Hallucination Eliminations: The HaluEVAL dataset [80] comprises 5,000 general user queries
with ChatGPT responses and 30,000 task-specific examples across question answering, knowledge-
grounded dialogue, and text summarization. This dataset is ideal for hallucination elimination due
to its intentionally misleading samples, serving as targets for unlearning [178]. In contrast, the
CounterFact dataset [110] includes 21,919 factual relations with prompts and paired true and
false responses, making it suitable for examining how models process and recall factual knowledge.
Larimar [25] utilizes this dataset to test its ability to replace incorrect facts with correct ones
during unlearning, assessing the model’s precision in updating its episodic memory. Meanwhile,
the TruthfulQA dataset [87] assesses whether a language model produces truthful answers to 817
questions across 38 categories, including health, law, finance, and politics. LLMU [178] initially
used TruthfulQA to maintain model utility while unlearning hallucinations. TruthfulQA is also
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

Machine Unlearning in Generative AI: A Survey
111:21
considered generic knowledge that the model should preserve [64, 96]. Finally, the MS COCO
dataset [88]is a comprehensive collection for object detection, segmentation, key-point detection,
and captioning, comprising 328,000 images. EFUF [174] leverages this dataset to extract objects
from generated captions and calculate their image relevance using CLIP [131], aiding in unlearning
hallucinations in multimodal models. The detailed annotations make it suitable for object/concept
unlearning in vision generative models [37].
5.1.4
Privacy Compliance: The Pile [40] is a comprehensive, open-source language modeling
dataset aggregating 825 GiB from 22 diverse, high-quality datasets. This compilation significantly
enriches data diversity, enhancing general cross-domain knowledge and downstream generalization
capabilities of large-scale models. ForgetFilter [189] utilizes subsets of The Pile, post-processed with
toxicity scores, to unlearn harmful information. Similarly, studies such as [27], DeMem [70], [65],
and [162] use the Training Data Extraction Challenge—a set of 15,000 easily extractable samples
from The Pile’s training set—to simulate privacy unlearning requests. [127] also leverages The Pile
as both forget and retain datasets under different settings to unlearn multiple skills from pre-trained
LLMs.
Transitioning to sentiment analysis, the Yelp/Amazon Reviews datasets offer extensive collec-
tions of user-generated content from Amazon.com and Yelp.com, respectively, including detailed
reviews, ratings, and metadata about products and businesses. These datasets are important to
privacy compliance; for instance, ICUL [125] evaluated privacy-preserving algorithms using 25,000
data points from each dataset, focusing on minimizing information leakage while maintaining
robust sentiment classification performance. Similarly, the SST-2 dataset [147] comprises 11,855
sentences from movie reviews, parsed using the Stanford parser and fully labeled with parse
trees. This dataset facilitates an in-depth analysis of compositional sentiment effects in language.
ICUL [125] and [184] selected subsets from SST-2 as target data for unlearning, evaluating the
effectiveness of their proposed unlearning algorithms. In the context of sentiment classification, the
IMDB dataset [104] includes user reviews of movies, directors, actors, and more, categorized into
two sentiment types. The trained model must disregard comments related to randomly selected
movies or individuals from the training set during testing.
Moving to dialogue data, PersonaChat [186] features crowd-sourced dialogues where each
speaker bases their conversation on a provided profile, making it suitable for unlearning sensitive
information. KGA [161] utilizes the official train, validation, and test splits to conduct experiments
on this dataset. For legal text classification, LEDGAR [156] consists of legal provisions in contracts
sourced from SEC filings, with over 12,000 labels annotated across nearly 100,000 provisions in more
than 60,000 contracts. KGA [161] uses a subset of this dataset for privacy information unlearning
experiments. The SAMsum dataset [43] contains approximately 16,000 English conversations
designed to mimic everyday messaging, complete with summaries for each conversation. This
dataset is useful for evaluating unlearning in conversational models.
For vision tasks, the CelebA-HQ dataset [69] features more than 200K celebrity images with 40
attribute annotations, suitable for tasks like face attribute recognition and landmark localization. It
is employed as a target dataset for feature-level unlearning in vision generative models like GANs
and VAEs [113, 154]. The I2P dataset [142] includes real user prompts from text-to-image tasks
designed to evaluate and mitigate inappropriate imagery in vision generative models. Receler [58],
[37], and EraseDiff [171] utilize these prompts to generate and unlearn inappropriate images in
diffusion models.
5.1.5
Bias/Unfairness Alleviation: The StereoSet dataset [114] is designed to measure stereotype
bias in language models, consisting of 17,000 sentences that evaluate model preferences across
gender, race, religion, and profession. To perform well on StereoSet, an unlearned model must
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

111:22
Trovato et al.
demonstrate fairness and unbiased behavior while possessing a strong comprehension of natural
language. PCGU [179] utilizes the intrasentence subset of StereoSet to assess its effectiveness
in unlearning biases, particularly by adjusting model parameters to reduce biased predictions
and enhance the fairness of its outputs. Similarly, CrowS-Pairs dataset [115] is a crowdsourced
collection specifically designed to measure the extent of stereotypical biases present in large pre-
trained masked language models. This dataset comprises 1,508 examples that address stereotypes
related to nine types of bias, including race, religion, and age. Each example pairs two sentences: one
that stereotypes a historically disadvantaged group and another that less stereotypically represents
an advantaged group, with minimal differences in wording between the two. This setup makes it
suitable for unlearning existing biases in pre-trained models. In PCGU [179], the authors evaluate
masked language models for bias by comparing the probabilities associated with each sentence.
Additionally, the HateXplain dataset [109] serves as a benchmark for explainable hate speech
detection, focusing on Twitter and Gab posts annotated by Amazon Mechanical Turk workers.
It uniquely includes word and phrase level annotations that capture human rationales for each
labeling decision, categorizing posts as hate speech, offensive, or normal. In FairSISA [68], the
authors utilize these detailed annotations to ensure fair and sensitive handling of race and religion,
aggregating subgroups into broader categories to address the sparse data distribution and overlap
among sensitive attributes.
5.2
Benchmarks
In the subsequent, we provide detailed explanations of various benchmarks for different generative
models to assist in evaluating their unlearning performance from a variety of perspectives.
5.2.1
Vision Generative Model. UnlearnCanvas [188] is a benchmark designed to evaluate how
diffusion models can forget certain styles or objects they’ve learned. For example, a model can
be asked to erase Van Gogh’s influence from their digital brushstrokes. UnlearnCanvas evaluates
these generative image models by challenging them to drop specific styles or objects from their
repertoire without affecting their overall artistic talent. In particular, it does this by feeding them
with a variety of images, from sketches to watercolors, and observing how well they can keep
generating while forgetting what we ask them to.
5.2.2
Large Language Model. The TOFU benchmark [105] is designed to evaluate the unlearning
capabilities of large language models, specifically focusing on the models’ ability to forget specific
data about fictitious authors. It provides a controlled environment with 200 synthetic author profiles
to test how effectively models can eliminate selected information while retaining other data. TOFU
employs a suite of metrics to measure both the residual knowledge of forgotten content and the
overall utility of the model post-unlearning, challenging models to demonstrate selective forgetting
without compromising their performance on unrelated tasks. Next, WMDP benchmark [82] aims to
identify and mitigate the risk of LLMs aiding in the development of biological, cyber, and chemical
weapons. It introduces a public dataset of over 4,000 multiple-choice questions as a proxy to measure
hazardous knowledge in these areas. The WMDP benchmark serves two main purposes: to evaluate
LLMs for dangerous knowledge that could enable malicious use, and to test unlearning methods
capable of removing such knowledge.
5.2.3
Multimodal (Large) Language Model . The Object HalBench [135] is commonly used to
evaluate object hallucination in detailed image descriptions by comparing the objects generated in
model outputs against comprehensively annotated object labels for COCO images [88]. Building
on this, the MMHal-Bench [149] focuses on assessing hallucinations and the informativeness of
responses, utilizing GPT-4 to compare the model’s output with human responses and a range of
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

Machine Unlearning in Generative AI: A Survey
111:23
object labels. However, due to incomplete text annotations in MMHal-Bench, it is primarily used to
measure the model’s level of informativeness. The MHumanEval benchmark [180] encompasses
both long-form image descriptions and short-form questions, containing 146 samples gathered
from both Object HalBench and MMHal-Bench. Human annotators label the hallucinated segments
and the types of hallucinations (e.g., objects, positions, numbers) based on the model responses.
Furthermore, LLaVA Bench [92] is widely utilized for evaluating the helpfulness of multimodal
conversations, focusing on detailed descriptions and complex reasoning capabilities, with helpful-
ness scores reported based on reference responses from GPT-4. Lastly, the POPE benchmark [85]
evaluates MLLMs by checking if they hallucinate any objects not present in the images. It uses
simple yes-or-no questions about object presence, varying the objects between random, commonly
seen, and those likely to cause errors, thus offering a scalable and clear way to measure models’
reliability in accurately recognizing image content.
6
APPLICATIONS
GenAI MU can benefit multiple downstream applications with the ability of precisely yet efficiently
removing undesirable knowledge from pre-trained generative models. In the following, we introduce
several key applications of generative model MU techniques in realistic situations. The detailed
downstream applications with different approaches can be found in Table 3.
6.1
Safety Alignment:
Existing Works. Generative models have demonstrated remarkable proficiency in generating
text, images, and multimodal outputs. However, the diversity and vastness of their pre-trained
data can lead to safety concerns. Thus, MU emerges as an effective tool for generative models
to mitigate the influence of undesirable data while preserving the model’s utility for relevant
tasks. The extensive use of Generative AI brings various safety concerns, including the generation
of inappropriate content. Inappropriate content generation refers to the production of harmful,
disturbing, or illegal outputs in response to problematic prompts or inputs. For example, parameter
optimization approaches [61, 96, 127, 184, 192] are effective for safety alignment applications, as
they reduce a model’s harmful generation by subtracting the fine-tuned weights on harmful datasets
from the pre-trained models while preserving the model’s utility. Additionally, eradicating NSFW
(Not Safe For Work) content, such as nudity, has become a critical focus in the field of unlearning
generative image models [33, 37, 171, 183]. Models trained on large datasets like LAION [143] may
contain inappropriate content that can mislead the model, making this an important area of study.
6.2
Privacy Compliance:
Existing Works. Generative models raise privacy concerns due to their powerful memorization
capabilities, which may result in data leakage during generation. This issue is exacerbated when
models are trained on data collected from individual users, such as medical data [31]. Consequently,
some users may request the model owner to delete their data in compliance with the Right To
Be Forgotten (RTBF), as stipulated by recent privacy legislation [54, 124, 137]. Upon request, data
samples containing sensitive information can be safely removed from the generative models using
unlearning approaches. In practice, many works [14, 60, 65, 84, 162] have focused on introducing
effective unlearning approaches to enhance privacy compliance. For example, KGA [161] proposes
a comprehensive unlearning pipeline based on knowledge gap alignment. This method minimizes
the knowledge gap between the output distributions of teacher-student models, enabling them to
produce similar predictions for both seen and unseen data. DeMem [70] fine-tunes the model using
a negative similarity score as a reward signal, encouraging the model to adopt a paraphrased policy
that effectively forgets sensitive data from the pre-trained model. Additionally, Forget-Me-Not
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

111:24
Trovato et al.
[183] introduces a plug-and-play module to fine-tune the UNet in the Stable Diffusion model. This
module uses a curated dataset to redirect the model’s attention away from the target concept, such
as a photograph of a person, thereby achieving unlearning in a more targeted and efficient manner.
6.3
Copyrights Protection:
Existing Works. As data becomes an increasingly critical component of generative models, the
actual source data owners—who are the parties or individuals that own the originality of the works
(e.g., books, artworks, and images, etc.)—raise significant concerns about the abusive usage of their
data. Their data can be coincidentally or intentionally collected by the model trainer as pre-trained
data samples to construct high-quality generative models. Additionally, possible data duplication
may severely jeopardize the ownership of the original data samples. Hence, one of the unlearning
objectives of generative models is to safely remove the copyrights of those unauthorized works to
offer better ownership protection. The objective of protecting copyrights is very similar to that of
privacy compliance, as both aim to prevent the unauthorized use of sensitive information. However,
copyright protection is often more challenging due to the intricate nature of intellectual property
laws and the diverse range of copyrighted materials involved. These two objectives overlap in the
context of generative image models, where copyright can be regarded as a type of concept, such as
the style of Van Gogh. Consequently, many approaches design techniques that are applicable to
both objectives [37, 75, 162, 183]. However, the situation becomes increasingly complicated when
applied to language models and multimodal (large) language models. For example, [32] proposes
unlearning information related to Harry Potter from a fine-tuned LLM via word replacement with
generic translations, which may degrade model utility and raise the issue of hallucinations. A
detailed analysis of unlearning copyrights can be found in Section 7.1.1.
6.4
Hallucinations Reduction:
Existing Works. Given a fact-related question or the task of interpreting an informative image,
generative models—especially Large Language Models (LLMs) and Multimodal (Large) Language
Models (MLLMs)—often generate plausible yet false or incorrect responses, potentially misleading
users [87, 90, 180]. Thus, unlearning could serve as a strategy to minimize such hallucinations in
generative models by enabling them to forget incorrect answers and the associated misleading
knowledge. This also includes some inconsistent, or outdated knowledge exists in the pre-training
data corpus. As emphasized by LLMU [178], the goal of unlearning hallucinations is to prevent
the model from providing incorrect answers rather than ensuring it always gives factually correct
answers. The difference between unlearning and model editing for the seek of hallucination is
highlighted in Section 3.3. Hence, negative sampling and contrastive-based approaches are typically
implemented during the unlearning process. For instance, LLMU introduces a gradient-ascent-based
approach to make the model learn in the opposite direction of the hallucinated answer. Similarly,
UBT [86] proposes an approach to mitigate backdoor attacks in MLLMs by dividing the dataset
into suspicious (i.e., hallucinated) and clean subsets, fine-tuning to amplify backdoor features, and
then implementing a token-level local unlearning strategy to selectively forget identified backdoor
triggers.
6.5
Bias/Unfairness Alleviation:
Existing Works. Previous studies [7, 76] have shown that pre-training corpora contain many
biased data samples. Neural models trained on such data may easily capture these biases and exhibit
them during implementation. In particular, the negative stereotypes and biases embedded in the
models may lead to potential unfairness and harm in a variety of applications, like medical care
[141], question-answer chatbots [159] and video/image generation [18]. Unlearning becomes an
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

Machine Unlearning in Generative AI: A Survey
111:25
effective tool to mitigate this unfairness without massive retraining. Recently, several works have
proposed to apply MU techniques in bias/unfairness alleviation of generative models. For example,
FairSISA [68] proposes a post-processing bias mitigation strategy for ensemble models produced
by SISA, aiming to enforce fairness in the outputs by ensuring that the model’s predictions do
not exhibit bias related to a sensitive attribute. In PCGU [179], the author introduce a weight
importance algorithm that uses gradient calculations to rank and prioritize model weights based
on their contribution to biased outputs, targeting the most influential weights for modification in
the unlearning phase.
7
DISCUSSION
7.1
Challenges
7.1.1
Copyright Unlearning. With the broad adoption of generative AI across various domains,
the emphasis on copyright protection has significantly increased, necessitating that generative
models eliminate detailed information about specific copyrighted works. Compared to generative
image models, the task of copyright unlearning presents more challenges for language models due
to the complexity of the issue and the ambiguity of the ultimate goal. Unlike unlearning targets
like sensitive information or harmful knowledge—where the goal is to remove a particular sample
and its influence without compromising the model’s utility on unrelated samples—copyrighted
products like books often cannot be easily separated from other pre-trained knowledge, particularly
when it comes to specific sections. Additionally, it remains uncertain whether an adversarial attack
could succeed by targeting a work similar to the copyrighted one. Recent efforts [29, 32, 178] have
attempted to unlearn copyright products by generating random responses or replacing sensitive
words on large language models. However, these approaches can compromise model utility and
exacerbate model hallucination, highlighting the need for improvements in both the objectives of
unlearning copyright and the design of unlearning responses.
7.1.2
Theoretical Analysis. While many current studies on generative model MU concentrate on
developing effective methodologies to enhance unlearning performance for various targets, there
remains a substantial gap between practical unlearning applications and theoretical analysis. Recent
work by NPO [185] theoretically demonstrates that progression toward catastrophic collapse via
minimizing the NPO loss occurs exponentially slower than with a Gradient Ascent (GA)-based
approach in Large Language Models (LLMs). In general, incorporating theoretical analysis can
enrich research from multiple perspectives and divide into two main streams. Firstly, it can offer
theoretical insights to refine the objectives of generative model unlearning and its potential ties to
differential privacy. Secondly, it can establish theoretical guarantees, like NPO [185], to validate
the efficacy of certain methods and their capability to balance intricate trade-offs. Both directions
contribute to a more profound understanding of MU’s underlying mechanisms, promoting more
robust and systematic research within the field. Nonetheless, as the field of generative model MU
is nascent, comprehensive theoretical analyses or guarantees tailored to generative models’ MU
challenges are yet to be developed. We hope that future studies will expand theoretical discussions
and provide substantial contributions to the foundational understanding of generative model
unlearning strategies.
7.1.3
Knowledge Entanglement. Knowledge entanglement presents yet another critical chal-
lenge to generative model unlearning, hindering the exclusion of specific information without
influencing related knowledge or the model’s overall functionality. The authors of TOFU [105]
observe that models often forget unrelated information during unlearning. This issue arises from
the blurred lines and overlap between concepts within the model’s learning process. Such overlaps
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

111:26
Trovato et al.
can lead to overgeneralization during unlearning, negatively affecting the model’s performance on
tasks that were not intended to be unlearned. For instance, [65] introduces knowledge unlearning
to decrease privacy risks in language models after training, but its success can vary across data
domains due to similarities among them. This issue also exists in generative image models, where
unlearning specific classes or styles may inadvertently alter unrelated ones, as shown by [37].
Therefore, knowledge entanglement poses a significant challenge to generative model unlearning
due to its possible impacts on accuracy and scalability.
7.1.4
Knowledge Entanglement between Model Locality and Generalizability. In tradi-
tional Machine Unlearning (MU), numerous studies [17, 50, 97, 122] have focused on mitigating the
trade-off between unlearning effectiveness and model utility, which is also the goal for generative
model MU. While some work like SKU [96] have focused on alleviating the trade-off between
unlearning efficacy and utility in LLM unlearning, these efforts primarily address the unlearning
of harmful knowledge, and their generalizability to other unlearning targets remains unverified.
For instance, FairSISA [68] highlights the challenges of balancing model accuracy with fairness
enhancement, particularly when employing post-processing techniques that neither alter the train-
ing data nor the model directly. Furthermore, this trade-off is not exclusive to LLMs as other
generative models face similar dilemmas. For example, [37] illustrate the need to balance complete
erasure of specific concepts against minimal interference with other visual concepts, underlining an
inherent trade-off in concept-specific unlearning. Similarly, the existing works on vision generative
model [37, 58, 148] have also shown that the models often struggle to isolate the target concept
without impacting related or co-occurring features. This interdependence can lead to model degra-
dation, where the overall quality of generated outputs diminishes, or the model fails to preserve
its generative capabilities for non-targeted concepts. Thus, tackling this trade-off and achieving a
balance between the effectiveness of unlearning and the preservation of model utility remains a
significant challenge for MU methods for generative models.
7.2
Future Directions
Despite the recent achievements in the development of Machine Unlearning (MU) approaches in
the field of generative models, these methods are still in their nascent stages compared to traditional
MU techniques. Therefore, in this section, we explore several promising directions that could be
pursued or investigated to further advance the field.
7.2.1
Consistency of unlearning Targets. Due to constant knowledge updates, current GenAI
MU approaches may not be consistent, meaning they might fail to erase newly introduced undesir-
able knowledge. For example, although [192] attempts to use security vectors to prevent models
from acquiring specific types of knowledge, their work is not generalizable for removing newly
introduced concepts during model updates. In practice, it is very likely that models unlearned for
certain knowledge may need to be fine-tuned on similar unlearned knowledge for different time
steps. Therefore, a promising future direction involves maintaining the consistency of unlearning
targets even after extensive knowledge updates.
7.2.2
Robust Unlearning. A critical direction for advancing GenAI MU lies in enhancing its
robustness. LLMs are known to be vulnerable to jailbreak attacks [59, 129, 167] and other attacks like
backdoor attacks. Recently, this vulnerability has been shown in multimodal (large) language models
as well [49, 119, 173]. It is important to exploit unlearning as an effective solution to develop a defense
pipeline to avoid GenAI from attacking. Various strategies for GenAI MU have focused on improving
the method’s robustness against adversarial attacks. For instance, Eraser [100] develops an robust
unlearning framework that aims to remove harmful knowledge without being compromised by
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

Machine Unlearning in Generative AI: A Survey
111:27
various jailbreak attack methods. UnlearnDiffAtk [187] introduces an effective adversarial prompt
generation method for diffusion models that simplifies the generation of adversarial prompts by
reducing its reliance on auxiliary models. However, in an era where attack strategies develop rapidly,
it is imperative that MU approaches for generative models not only retain their effectiveness but
also demonstrate robustness against such attacks. For instance, a recent adaptive attack strategy [4]
has been able to jailbreak various state-of-the-art LLM after safety alignment, including GPT-4,
Claude, and Gemini, with success rates close to 100%. Therefore, more effort must be devoted to
enhancing the robustness of unlearning algorithms to prevent various adversarial attacks.
7.2.3
Reliability of LLMs as Evaluators in GenAI Unlearning. Since human evaluators are
typically expensive and time-consuming, an increasing number of studies and benchmarks have
utilized state-of-the-art Large Language Models (LLMs) like GPT-4 as tools to assess unlearning
performance and utility. For instance, SKU [96] and [192] employ GPT-4 [1] and GPT-3.5-turbo
respectively to evaluate the harmfulness of model responses post-unlearning. Similarly, Eraser [100]
uses GPT-3.5 to derive a harmfulness score ranging between 1 and 5 to aid in determining the success
rate of jailbreak attacks. Additionally, EFUF [174] utilizes GPT-4 to assess the informativeness
of responses generated by the unlearned model to ensure that utility is not compromised after
unlearning. Despite the prevalence and effectiveness of this approach, its validity is questionable.
Firstly, LLMs are known to produce biased decisions and outputs [22, 102, 126, 150], which may
adversely affect the evaluation results. Secondly, many recent studies [77, 140, 166] have highlighted
the importance of prompt templates during prompt tuning, which is predominantly used to calibrate
GPT as an evaluator. This suggests that evaluation results from such an evaluator may largely
depend on its system prompts and relevant templates, introducing uncertainties in the evaluation
process. Hence, despite the challenges, it will be intriguing to see how future research addresses
the drawbacks of using LLMs as evaluators in GenAI unlearning, aiming to provide more reliable
and convincing evaluation outcomes.
8
CONCLUSIONS
In this survey, we present a comprehensive analysis of machine unlearning (MU) techniques in
the domain of generative artificial intelligence (GenAI), encompassing generative image models,
(large) language models (LLMs), and multimodal (large) language models (MLLMs). By categorizing
existing unlearning methods into parameter optimization, and in-context unlearning strategies, we
highlight the unique techniques, advantages, and limitations of each approach. Subsequently, we
summarize the datasets and benchmarks widely utilized to measure GenAI techniques, emphasizing
that different techniques may need to be assessed from different perspectives. Furthermore, to
promote future researchers to investigate the field in diverse directions, we also identify the real-
world applications of GenAI techniques. Meanwhile, we recognize several potential challenges and
promising future directions to further enhance the efficiency and effectiveness of MU methods.
Ultimately, this survey aims to deepen the understanding of MU in GenAI and inspire further
advancements in the field to ensure safer and more trustworthy AI systems.
REFERENCES
[1] Josh Achiam, Steven Adler, Sandhini Agarwal, Lama Ahmad, Ilge Akkaya, Florencia Leoni Aleman, Diogo Almeida,
Janko Altenschmidt, Sam Altman, Shyamal Anadkat, et al. 2023. Gpt-4 technical report. arXiv preprint arXiv:2303.08774
(2023).
[2] Samuel K Ainsworth, Jonathan Hayase, and Siddhartha Srinivasa. 2022. Git re-basin: Merging models modulo
permutation symmetries. arXiv preprint arXiv:2209.04836 (2022).
[3] Aida Amini, Saadia Gabriel, Peter Lin, Rik Koncel-Kedziorski, Yejin Choi, and Hannaneh Hajishirzi. 2019. Mathqa:
Towards interpretable math word problem solving with operation-based formalisms. arXiv preprint arXiv:1905.13319
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

111:28
Trovato et al.
(2019).
[4] Maksym Andriushchenko, Francesco Croce, and Nicolas Flammarion. 2024. Jailbreaking Leading Safety-Aligned
LLMs with Simple Adaptive Attacks. arXiv preprint arXiv:2404.02151 (2024).
[5] Yuntao Bai, Andy Jones, Kamal Ndousse, Amanda Askell, Anna Chen, Nova DasSarma, Dawn Drain, Stanislav Fort,
Deep Ganguli, Tom Henighan, et al. 2022. Training a helpful and harmless assistant with reinforcement learning
from human feedback. arXiv preprint arXiv:2204.05862 (2022).
[6] P Bedapudi. 2019. Nudenet: Neural nets for nudity classification, detection and selective censoring.
[7] Emily M Bender, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. On the dangers of
stochastic parrots: Can language models be too big?. In FAccT.
[8] Yonatan Bisk, Rowan Zellers, Jianfeng Gao, Yejin Choi, et al. 2020. Piqa: Reasoning about physical commonsense in
natural language. In AAAI.
[9] Kevin Black, Michael Janner, Yilun Du, Ilya Kostrikov, and Sergey Levine. 2023. Training diffusion models with
reinforcement learning. arXiv preprint arXiv:2305.13301 (2023).
[10] Alberto Blanco-Justicia, Najeeb Jebreel, Benet Manzanares, David Sánchez, Josep Domingo-Ferrer, Guillem Collell,
and Kuan Eeik Tan. 2024. Digital Forgetting in Large Language Models: A Survey of Unlearning Methods. arXiv
preprint arXiv:2404.02062 (2024).
[11] Daniel Borkan, Lucas Dixon, Jeffrey Sorensen, Nithum Thain, and Lucy Vasserman. 2019. Nuanced metrics for
measuring unintended bias with real data for text classification. In WWW.
[12] Lucas Bourtoule, Varun Chandrasekaran, Christopher A Choquette-Choo, Hengrui Jia, Adelin Travers, Baiwu Zhang,
David Lie, and Nicolas Papernot. 2021. Machine unlearning. In IEEE Symposium on Security and Privacy (SP).
[13] Yinzhi Cao and Junfeng Yang. 2015. Towards making systems forget with machine unlearning. In IEEE symposium on
security and privacy.
[14] Jiaao Chen and Diyi Yang. 2023. Unlearn what you want to forget: Efficient unlearning for llms. arXiv preprint
arXiv:2310.20150 (2023).
[15] Jiali Cheng and Hadi Amiri. 2023. Multimodal Machine Unlearning. arXiv preprint arXiv:2311.12047 (2023).
[16] Siyuan Cheng, Bo Tian, Qingbin Liu, Xi Chen, Yongheng Wang, Huajun Chen, and Ningyu Zhang. 2023. Can We Edit
Multimodal Large Language Models? arXiv preprint arXiv:2310.08475 (2023).
[17] Eli Chien, Chao Pan, and Olgica Milenkovic. 2022. Efficient model updates for approximate unlearning of graph-
structured data. In ICLR.
[18] Jaemin Cho, Abhay Zala, and Mohit Bansal. 2023. Dall-eval: Probing the reasoning skills and social biases of
text-to-image generation models. In ICCV.
[19] Vikram S Chundawat, Ayush K Tarun, Murari Mandal, and Mohan Kankanhalli. 2023. Can bad teaching induce
forgetting? unlearning in deep networks using an incompetent teacher. In AAAI.
[20] Vikram S Chundawat, Ayush K Tarun, Murari Mandal, and Mohan Kankanhalli. 2023. Zero-shot machine unlearning.
IEEE Transactions on Information Forensics and Security (2023).
[21] Roi Cohen, Eden Biran, Ori Yoran, Amir Globerson, and Mor Geva. 2024. Evaluating the ripple effects of knowledge
editing in language models. ACL (2024).
[22] Chenhang Cui, Yiyang Zhou, Xinyu Yang, Shirley Wu, Linjun Zhang, James Zou, and Huaxiu Yao. 2023. Holistic
analysis of hallucination in gpt-4v (ision): Bias and interference challenges. arXiv preprint arXiv:2311.03287 (2023).
[23] Zheng Dai and David K Gifford. 2023. Training data attribution for diffusion models. arXiv preprint arXiv:2306.02174
(2023).
[24] Quang-Vinh Dang. 2021. Right to be forgotten in the age of machine learning. In Advances in Digital Science: ICADS
2021.
[25] Payel Das, Subhajit Chaudhury, Elliot Nelson, Igor Melnyk, Sarath Swaminathan, Sihui Dai, Aurélie Lozano, Georgios
Kollias, Vijil Chenthamarakshan, Soham Dan, et al. 2024. Larimar: Large Language Models with Episodic Memory
Control. arXiv preprint arXiv:2403.11901 (2024).
[26] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2018. Bert: Pre-training of deep bidirectional
transformers for language understanding. arXiv preprint arXiv:1810.04805 (2018).
[27] Yijiang River Dong, Hongzhou Lin, Mikhail Belkin, Ramon Huerta, and Ivan Vulić. 2024. Unmemorization in Large
Language Models via Self-Distillation and Deliberate Imagination. arXiv preprint arXiv:2402.10052 (2024).
[28] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov, Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Sylvain Gelly, et al. 2020. An image is worth 16x16 words:
Transformers for image recognition at scale. arXiv preprint arXiv:2010.11929 (2020).
[29] Guangyao Dou, Zheyuan Liu, Qing Lyu, Kaize Ding, and Eric Wong. 2024. Avoiding Copyright Infringement via
Machine Unlearning. arXiv preprint arXiv:2406.10952 (2024).
[30] Min Du, Zhi Chen, Chang Liu, Rajvardhan Oak, and Dawn Song. 2019. Lifelong anomaly detection through unlearning.
In CCS.
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

Machine Unlearning in Generative AI: A Survey
111:29
[31] Olivia G d’Aliberti and Mark A Clark. 2022. Preserving patient privacy during computation over shared electronic
health record data. Journal of Medical Systems (2022).
[32] Ronen Eldan and Mark Russinovich. 2023. Who’s Harry Potter? Approximate Unlearning in LLMs. arXiv preprint
arXiv:2310.02238 (2023).
[33] Chongyu Fan, Jiancheng Liu, Yihua Zhang, Dennis Wei, Eric Wong, and Sijia Liu. 2023. Salun: Empowering ma-
chine unlearning via gradient-based weight saliency in both image classification and generation. arXiv preprint
arXiv:2310.12508 (2023).
[34] Jonathan Frankle and Michael Carbin. 2018. The lottery ticket hypothesis: Finding sparse, trainable neural networks.
arXiv preprint arXiv:1803.03635 (2018).
[35] Jonathan Frankle, Gintare Karolina Dziugaite, Daniel Roy, and Michael Carbin. 2020. Linear mode connectivity and
the lottery ticket hypothesis. In ICML.
[36] Rinon Gal, Yuval Alaluf, Yuval Atzmon, Or Patashnik, Amit H Bermano, Gal Chechik, and Daniel Cohen-Or. 2022.
An image is worth one word: Personalizing text-to-image generation using textual inversion.
arXiv preprint
arXiv:2208.01618 (2022).
[37] Rohit Gandikota, Joanna Materzynska, Jaden Fiotto-Kaufman, and David Bau. 2023. Erasing concepts from diffusion
models. In ICCV.
[38] Rohit Gandikota, Hadas Orgad, Yonatan Belinkov, Joanna Materzyńska, and David Bau. 2024. Unified concept editing
in diffusion models. In WACV.
[39] Deep Ganguli, Liane Lovitt, Jackson Kernion, Amanda Askell, Yuntao Bai, Saurav Kadavath, Ben Mann, Ethan Perez,
Nicholas Schiefer, Kamal Ndousse, et al. 2022. Red teaming language models to reduce harms: Methods, scaling
behaviors, and lessons learned. arXiv preprint arXiv:2209.07858 (2022).
[40] Leo Gao, Stella Biderman, Sid Black, Laurence Golding, Travis Hoppe, Charles Foster, Jason Phang, Horace He, Anish
Thite, Noa Nabeshima, Shawn Presser, and Connor Leahy. 2020. The Pile: An 800GB Dataset of Diverse Text for
Language Modeling. arXiv preprint arXiv:2101.00027 (2020).
[41] Mor Geva, Avi Caciularu, Kevin Ro Wang, and Yoav Goldberg. 2022. Transformer feed-forward layers build predictions
by promoting concepts in the vocabulary space. arXiv preprint arXiv:2203.14680 (2022).
[42] Antonio Ginart, Melody Guan, Gregory Valiant, and James Y Zou. 2019. Making ai forget you: Data deletion in
machine learning. Neurips (2019).
[43] Bogdan Gliwa, Iwona Mochol, Maciej Biesek, and Aleksander Wawer. 2019. SAMSum Corpus: A Human-annotated
Dialogue Dataset for Abstractive Summarization. In Proceedings of the 2nd Workshop on New Frontiers in Summariza-
tion.
[44] Aditya Golatkar, Alessandro Achille, and Stefano Soatto. 2020. Eternal sunshine of the spotless net: Selective forgetting
in deep networks. In CVPR.
[45] Aditya Golatkar, Alessandro Achille, and Stefano Soatto. 2020. Forgetting outside the box: Scrubbing deep networks
of information accessible from input-output observations. In ECCV.
[46] Ian Goodfellow, Jean Pouget-Abadie, Mehdi Mirza, Bing Xu, David Warde-Farley, Sherjil Ozair, Aaron Courville, and
Yoshua Bengio. 2014. Generative adversarial nets. Neurips (2014).
[47] Jia-Chen Gu, Hao-Xiang Xu, Jun-Yu Ma, Pan Lu, Zhen-Hua Ling, Kai-Wei Chang, and Nanyun Peng. 2024. Model
editing can hurt general abilities of large language models. arXiv preprint arXiv:2401.04700 (2024).
[48] Kang Gu, Md Rafi Ur Rashid, Najrin Sultana, and Shagufta Mehnaz. 2024. Second-Order Information Matters:
Revisiting Machine Unlearning for Large Language Models. arXiv preprint arXiv:2403.10557 (2024).
[49] Xiangming Gu, Xiaosen Zheng, Tianyu Pang, Chao Du, Qian Liu, Ye Wang, Jing Jiang, and Min Lin. 2024. Agent Smith:
A Single Image Can Jailbreak One Million Multimodal LLM Agents Exponentially Fast. arXiv preprint arXiv:2402.08567
(2024).
[50] Chuan Guo, Tom Goldstein, Awni Hannun, and Laurens Van Der Maaten. 2019. Certified data removal from machine
learning models. arXiv preprint arXiv:1911.03030 (2019).
[51] Dan Hendrycks, Collin Burns, Steven Basart, Andy Zou, Mantas Mazeika, Dawn Song, and Jacob Steinhardt. 2020.
Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300 (2020).
[52] Alvin Heng and Harold Soh. 2024. Selective amnesia: A continual learning approach to forgetting in deep generative
models. Neurips (2024).
[53] Jonathan Ho, Ajay Jain, and Pieter Abbeel. 2020. Denoising diffusion probabilistic models. Neurips (2020).
[54] Chris Jay Hoofnagle, Bart Van Der Sloot, and Frederik Zuiderveen Borgesius. 2019. The European Union general data
protection regulation: what it is and what it means. Information & Communications Technology Law (2019).
[55] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo,
Mona Attariyan, and Sylvain Gelly. 2019. Parameter-efficient transfer learning for NLP. In ICML.
[56] Edward J Hu, Yelong Shen, Phillip Wallis, Zeyuan Allen-Zhu, Yuanzhi Li, Shean Wang, Lu Wang, and Weizhu Chen.
2021. Lora: Low-rank adaptation of large language models. arXiv preprint arXiv:2106.09685 (2021).
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

111:30
Trovato et al.
[57] Xinshuo Hu, Dongfang Li, Baotian Hu, Zihao Zheng, Zhenyu Liu, and Min Zhang. 2024. Separate the wheat from the
chaff: Model deficiency unlearning via parameter-efficient module operation. In AAAI.
[58] Chi-Pin Huang, Kai-Po Chang, Chung-Ting Tsai, Yung-Hsuan Lai, and Yu-Chiang Frank Wang. 2023. Receler: Reliable
concept erasing of text-to-image diffusion models via lightweight erasers. arXiv preprint arXiv:2311.17717 (2023).
[59] Yangsibo Huang, Samyak Gupta, Mengzhou Xia, Kai Li, and Danqi Chen. 2023. Catastrophic jailbreak of open-source
llms via exploiting generation. arXiv preprint arXiv:2310.06987 (2023).
[60] Y. James Huang, Wenxuan Zhou, Fei Wang, Fred Morstatter, Sheng Zhang, Hoifung Poon, and Muhao Chen. 2024.
Offset Unlearning for Large Language Models. arXiv preprint arXiv:2404.11045 (2024).
[61] Gabriel Ilharco, Marco Tulio Ribeiro, Mitchell Wortsman, Suchin Gururangan, Ludwig Schmidt, Hannaneh Hajishirzi,
and Ali Farhadi. 2022. Editing models with task arithmetic. arXiv preprint arXiv:2212.04089 (2022).
[62] Gabriel Ilharco, Mitchell Wortsman, Samir Yitzhak Gadre, Shuran Song, Hannaneh Hajishirzi, Simon Kornblith, Ali
Farhadi, and Ludwig Schmidt. 2022. Patching open-vocabulary models by interpolating weights. Neurips (2022).
[63] Erin Illman and Paul Temple. 2019. California consumer privacy act. The Business Lawyer (2019).
[64] Masaru Isonuma and Ivan Titov. 2024. Unlearning Reveals the Influential Training Data of Language Models. arXiv
preprint arXiv:2401.15241 (2024).
[65] Joel Jang, Dongkeun Yoon, Sohee Yang, Sungmin Cha, Moontae Lee, Lajanugen Logeswaran, and Minjoon Seo. 2022.
Knowledge unlearning for mitigating privacy risks in language models. arXiv preprint arXiv:2210.01504 (2022).
[66] Jiaming Ji, Mickel Liu, Josef Dai, Xuehai Pan, Chi Zhang, Ce Bian, Boyuan Chen, Ruiyang Sun, Yizhou Wang, and
Yaodong Yang. 2024. Beavertails: Towards improved safety alignment of llm via a human-preference dataset. Neurips
(2024).
[67] Xisen Jin, Xiang Ren, Daniel Preotiuc-Pietro, and Pengxiang Cheng. 2022. Dataless knowledge fusion by merging
weights of language models. arXiv preprint arXiv:2212.09849 (2022).
[68] Swanand Ravindra Kadhe, Anisa Halimi, Ambrish Rawat, and Nathalie Baracaldo. 2023. Fairsisa: Ensemble post-
processing to improve fairness of unlearning in llms. arXiv preprint arXiv:2312.07420 (2023).
[69] Tero Karras, Timo Aila, Samuli Laine, and Jaakko Lehtinen. 2017. Progressive growing of gans for improved quality,
stability, and variation. arXiv preprint arXiv:1710.10196 (2017).
[70] Aly Kassem, Omar Mahmoud, and Sherif Saad. 2023. Preserving Privacy Through Dememorization: An Unlearning
Technique For Mitigating Memorization Risks In Language Models. In EMNLP.
[71] Diederik P Kingma and Max Welling. 2013. Auto-encoding variational bayes. arXiv preprint arXiv:1312.6114 (2013).
[72] James Kirkpatrick, Razvan Pascanu, Neil Rabinowitz, Joel Veness, Guillaume Desjardins, Andrei A Rusu, Kieran Milan,
John Quan, Tiago Ramalho, Agnieszka Grabska-Barwinska, et al. 2017. Overcoming catastrophic forgetting in neural
networks. PNAS (2017).
[73] Pang Wei Koh and Percy Liang. 2017. Understanding black-box predictions via influence functions. In International
conference on machine learning.
[74] Vinayshekhar Bannihatti Kumar, Rashmi Gangadharaiah, and Dan Roth. 2022. Privacy adhering machine un-learning
in nlp. arXiv preprint arXiv:2212.09573 (2022).
[75] Nupur Kumari, Bingliang Zhang, Sheng-Yu Wang, Eli Shechtman, Richard Zhang, and Jun-Yan Zhu. 2023. Ablating
concepts in text-to-image diffusion models. In ICCV.
[76] Katherine Lee, Daphne Ippolito, Andrew Nystrom, Chiyuan Zhang, Douglas Eck, Chris Callison-Burch, and Nicholas
Carlini. 2021. Deduplicating training data makes language models better. arXiv preprint arXiv:2107.06499 (2021).
[77] Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efficient prompt tuning.
arXiv preprint arXiv:2104.08691 (2021).
[78] Bowen Li, Xiaojuan Qi, Thomas Lukasiewicz, and Philip Torr. 2019. Controllable text-to-image generation. In Neurips.
[79] Guihong Li, Hsiang Hsu, Radu Marculescu, et al. 2024. Machine Unlearning for Image-to-Image Generative Models.
arXiv preprint arXiv:2402.00351 (2024).
[80] Junyi Li, Xiaoxue Cheng, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. 2023. Halueval: A large-scale hallucination
evaluation benchmark for large language models. In EMNLP.
[81] Junyi Li, Tianyi Tang, Wayne Xin Zhao, Jian-Yun Nie, and Ji-Rong Wen. 2024. Pre-trained language models for text
generation: A survey. Comput. Surveys (2024).
[82] Nathaniel Li, Alexander Pan, Anjali Gopal, Summer Yue, Daniel Berrios, Alice Gatti, Justin D Li, Ann-Kathrin
Dombrowski, Shashwat Goel, Long Phan, et al. 2024. The WMDP Benchmark: Measuring and Reducing Malicious
Use With Unlearning. arXiv preprint arXiv:2403.03218 (2024).
[83] Xiaopeng Li, Shasha Li, Shezheng Song, Jing Yang, Jun Ma, and Jie Yu. 2024. Pmet: Precise model editing in a
transformer. In AAAI.
[84] Xinzhe Li, Ming Liu, and Shang Gao. 2023. Make Text Unlearnable: Exploiting Effective Patterns to Protect Personal
Data. arXiv preprint arXiv:2307.00456 (2023).
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

Machine Unlearning in Generative AI: A Survey
111:31
[85] Yifan Li, Yifan Du, Kun Zhou, Jinpeng Wang, Wayne Xin Zhao, and Ji-Rong Wen. 2023. Evaluating object hallucination
in large vision-language models. arXiv preprint arXiv:2305.10355 (2023).
[86] Siyuan Liang, Kuanrong Liu, Jiajun Gong, Jiawei Liang, Yuan Xun, Ee-Chien Chang, and Xiaochun Cao. 2024.
Unlearning Backdoor Threats: Enhancing Backdoor Defense in Multimodal Contrastive Learning via Local Token
Unlearning. arXiv preprint arXiv:2403.16257 (2024).
[87] Stephanie Lin, Jacob Hilton, and Owain Evans. 2021. Truthfulqa: Measuring how models mimic human falsehoods.
arXiv preprint arXiv:2109.07958 (2021).
[88] Tsung-Yi Lin, Michael Maire, Serge Belongie, James Hays, Pietro Perona, Deva Ramanan, Piotr Dollár, and C Lawrence
Zitnick. [n. d.]. Microsoft coco: Common objects in context. In ECCV.
[89] Bo Liu, Qiang Liu, and Peter Stone. 2022. Continual learning and private unlearning. In CoLLAs.
[90] Fuxiao Liu, Kevin Lin, Linjie Li, Jianfeng Wang, Yaser Yacoob, and Lijuan Wang. 2023. Mitigating hallucination in
large multi-modal models via robust instruction tuning. In ICLR.
[91] Haotian Liu, Chunyuan Li, Yuheng Li, and Yong Jae Lee. 2023. Improved baselines with visual instruction tuning.
arXiv preprint arXiv:2310.03744 (2023).
[92] Haotian Liu, Chunyuan Li, Qingyang Wu, and Yong Jae Lee. 2024. Visual instruction tuning. Neurips (2024).
[93] Haokun Liu, Derek Tam, Mohammed Muqeeth, Jay Mohta, Tenghao Huang, Mohit Bansal, and Colin A Raffel. 2022.
Few-shot parameter-efficient fine-tuning is better and cheaper than in-context learning. Neurips (2022).
[94] Sijia Liu, Yuanshun Yao, Jinghan Jia, Stephen Casper, Nathalie Baracaldo, Peter Hase, Xiaojun Xu, Yuguang Yao,
Hang Li, Kush R Varshney, et al. 2024. Rethinking Machine Unlearning for Large Language Models. arXiv preprint
arXiv:2402.08787 (2024).
[95] Yang Liu, Mingyuan Fan, Cen Chen, Ximeng Liu, Zhuo Ma, Li Wang, and Jianfeng Ma. 2022. Backdoor defense with
machine unlearning. In IEEE INFOCOM.
[96] Zheyuan Liu, Guangyao Dou, Zhaoxuan Tan, Yijun Tian, and Meng Jiang. 2024. Towards Safer Large Language
Models through Machine Unlearning. arXiv preprint arXiv:2402.10058 (2024).
[97] Zheyuan Liu, Guangyao Dou, Yijun Tian, Chunhui Zhang, Eli Chien, and Ziwei Zhu. 2023. Breaking the trilemma of
privacy, utility, efficiency via controllable machine unlearning. arXiv preprint arXiv:2310.18574 (2023).
[98] Zhe Liu and Ozlem Kalinli. 2024. Forgetting Private Textual Sequences in Language Models Via Leave-One-Out
Ensemble. In ICASSP.
[99] Ziyao Liu, Huanyi Ye, Chen Chen, and Kwok-Yan Lam. 2024. Threats, attacks, and defenses in machine unlearning: A
survey. arXiv preprint arXiv:2403.13682 (2024).
[100] Weikai Lu, Ziqian Zeng, Jianwei Wang, Zhengdong Lu, Zelin Chen, Huiping Zhuang, and Cen Chen. 2024. Eraser:
Jailbreaking Defense in Large Language Models via Unlearning Harmful Knowledge. arXiv preprint arXiv:2404.05880
(2024).
[101] Ximing Lu, Sean Welleck, Jack Hessel, Liwei Jiang, Lianhui Qin, Peter West, Prithviraj Ammanabrolu, and Yejin Choi.
2022. Quark: Controllable text generation with reinforced unlearning. Neurips (2022).
[102] Li Lucy and David Bamman. 2021. Gender and representation bias in GPT-3 generated stories. In Proceedings of the
third workshop on narrative understanding.
[103] Aengus Lynch, Phillip Guo, Aidan Ewart, Stephen Casper, and Dylan Hadfield-Menell. 2024. Eight Methods to
Evaluate Robust Unlearning in LLMs. arXiv preprint arXiv:2402.16835 (2024).
[104] Andrew Maas, Raymond E Daly, Peter T Pham, Dan Huang, Andrew Y Ng, and Christopher Potts. 2011. Learning
word vectors for sentiment analysis. In ACL.
[105] Pratyush Maini, Zhili Feng, Avi Schwarzschild, Zachary C Lipton, and J Zico Kolter. 2024. Tofu: A task of fictitious
unlearning for llms. arXiv preprint arXiv:2401.06121 (2024).
[106] Neil G Marchant, Benjamin IP Rubinstein, and Scott Alfeld. 2022. Hard to forget: Poisoning attacks on certified
machine unlearning. In AAAI.
[107] Michael Matena and Colin Raffel. [n. d.]. Merging models with fisher-weighted averaging, 2021. arXiv preprint
arXiv:2111.09832 ([n. d.]).
[108] Michael S Matena and Colin A Raffel. 2022. Merging models with fisher-weighted averaging. Neurips (2022).
[109] Binny Mathew, Punyajoy Saha, Seid Muhie Yimam, Chris Biemann, Pawan Goyal, and Animesh Mukherjee. 2021.
Hatexplain: A benchmark dataset for explainable hate speech detection. In AAAI.
[110] Kevin Meng, David Bau, Alex Andonian, and Yonatan Belinkov. 2022. Locating and editing factual associations in
GPT. Neurips (2022).
[111] Mehdi Mirza and Simon Osindero. 2014. Conditional generative adversarial nets. arXiv preprint arXiv:1411.1784
(2014).
[112] Eric Mitchell, Charles Lin, Antoine Bosselut, Christopher D Manning, and Chelsea Finn. 2022. Memory-based model
editing at scale. In ICML.
[113] Saemi Moon, Seunghyuk Cho, and Dongwoo Kim. 2024. Feature unlearning for pre-trained gans and vaes. In AAAI.
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

111:32
Trovato et al.
[114] Moin Nadeem, Anna Bethke, and Siva Reddy. 2020. StereoSet: Measuring stereotypical bias in pretrained language
models. arXiv preprint arXiv:2004.09456 (2020).
[115] Nikita Nangia, Clara Vania, Rasika Bhalerao, and Samuel R Bowman. 2020. CrowS-pairs: A challenge dataset for
measuring social biases in masked language models. arXiv preprint arXiv:2010.00133 (2020).
[116] Quoc Phong Nguyen, Bryan Kian Hsiang Low, and Patrick Jaillet. 2020. Variational bayesian unlearning. Neurips
(2020).
[117] Thanh Tam Nguyen, Thanh Trung Huynh, Phi Le Nguyen, Alan Wee-Chung Liew, Hongzhi Yin, and Quoc Viet Hung
Nguyen. 2022. A survey of machine unlearning. arXiv preprint arXiv:2209.02299 (2022).
[118] Shiwen Ni, Dingwei Chen, Chengming Li, Xiping Hu, Ruifeng Xu, and Min Yang. 2023. Forgetting before Learning:
Utilizing Parametric Arithmetic for Knowledge Updating in Large Language Models. arXiv preprint arXiv:2311.08011
(2023).
[119] Zhenxing Niu, Haodong Ren, Xinbo Gao, Gang Hua, and Rong Jin. 2024. Jailbreaking attack against multimodal large
language model. arXiv preprint arXiv:2402.02309 (2024).
[120] Hadas Orgad, Bahjat Kawar, and Yonatan Belinkov. 2023. Editing implicit assumptions in text-to-image diffusion
models. In ICCV.
[121] Long Ouyang, Jeffrey Wu, Xu Jiang, Diogo Almeida, Carroll Wainwright, Pamela Mishkin, Chong Zhang, Sandhini
Agarwal, Katarina Slama, Alex Ray, et al. 2022. Training language models to follow instructions with human feedback.
Neurips (2022).
[122] Chao Pan, Eli Chien, and Olgica Milenkovic. 2023. Unlearning graph classifiers with limited data resources. In WWW.
[123] Subhodip Panda and Prathosh AP. 2023. FAST: Feature Aware Similarity Thresholding for Weak Unlearning in
Black-Box Generative Models. arXiv preprint arXiv:2312.14895 (2023).
[124] Stuart L Pardau. 2018. The california consumer privacy act: Towards a european-style privacy regime in the united
states. J. Tech. L. & Pol’y (2018).
[125] Martin Pawelczyk, Seth Neel, and Himabindu Lakkaraju. 2023. In-context unlearning: Language models as few shot
unlearners. arXiv preprint arXiv:2310.07579 (2023).
[126] Ethan Perez, Sam Ringer, Kamil˙e Lukoši¯ut˙e, Karina Nguyen, Edwin Chen, Scott Heiner, Craig Pettit, Catherine Olsson,
Sandipan Kundu, Saurav Kadavath, et al. 2022. Discovering language model behaviors with model-written evaluations.
arXiv preprint arXiv:2212.09251 (2022).
[127] Nicholas Pochinkov and Nandi Schoots. 2024. Dissecting Language Models: Machine Unlearning via Selective Pruning.
arXiv preprint arXiv:2403.01267 (2024).
[128] Formerly Data Protection. 2018. General data protection regulation (GDPR). Intersoft Consulting, Accessed in October
(2018).
[129] Xiangyu Qi, Yi Zeng, Tinghao Xie, Pin-Yu Chen, Ruoxi Jia, Prateek Mittal, and Peter Henderson. 2023. Fine-tuning
aligned language models compromises safety, even when users do not intend to! arXiv preprint arXiv:2310.03693
(2023).
[130] Youyang Qu, Ming Ding, Nan Sun, Kanchana Thilakarathna, Tianqing Zhu, and Dusit Niyato. 2024. The Frontier of
Data Erasure: Machine Unlearning for Large Language Models. arXiv preprint arXiv:2403.15779 (2024).
[131] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda
Askell, Pamela Mishkin, Jack Clark, et al. 2021. Learning transferable visual models from natural language supervision.
In ICML.
[132] Alec Radford, Karthik Narasimhan, Tim Salimans, Ilya Sutskever, et al. 2018. Improving language understanding by
generative pre-training. (2018).
[133] Colin Raffel, Noam Shazeer, Adam Roberts, Katherine Lee, Sharan Narang, Michael Matena, Yanqi Zhou, Wei Li, and
Peter J Liu. 2020. Exploring the limits of transfer learning with a unified text-to-text transformer. JMLR (2020).
[134] Jie Ren, Han Xu, Pengfei He, Yingqian Cui, Shenglai Zeng, Jiankun Zhang, Hongzhi Wen, Jiayuan Ding, Hui Liu, Yi
Chang, et al. 2024. Copyright Protection in Generative AI: A Technical Perspective. arXiv preprint arXiv:2402.02333
(2024).
[135] Anna Rohrbach, Lisa Anne Hendricks, Kaylee Burns, Trevor Darrell, and Kate Saenko. 2018. Object hallucination in
image captioning. arXiv preprint arXiv:1809.02156 (2018).
[136] Robin Rombach, Andreas Blattmann, Dominik Lorenz, Patrick Esser, and Björn Ommer. 2022. High-resolution image
synthesis with latent diffusion models. In CVPR.
[137] Jeffrey Rosen. 2011. The right to be forgotten. Stan. L. Rev. Online (2011).
[138] JK Rowling. 1997. Harry Potter [book series]. London: Bloomsbury and Little, Brown (1997).
[139] Nataniel Ruiz, Yuanzhen Li, Varun Jampani, Yael Pritch, Michael Rubinstein, and Kfir Aberman. 2023. Dreambooth:
Fine tuning text-to-image diffusion models for subject-driven generation. In CVPR.
[140] Pranab Sahoo, Ayush Kumar Singh, Sriparna Saha, Vinija Jain, Samrat Mondal, and Aman Chadha. 2024. A Systematic
Survey of Prompt Engineering in Large Language Models: Techniques and Applications. arXiv preprint arXiv:2402.07927
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

Machine Unlearning in Generative AI: A Survey
111:33
(2024).
[141] Samuel Schmidgall, Carl Harris, Ime Essien, Daniel Olshvang, Tawsifur Rahman, Ji Woong Kim, Rojin Ziaei, Jason
Eshraghian, Peter Abadir, and Rama Chellappa. 2024. Addressing cognitive bias in medical language models. arXiv
preprint arXiv:2402.08113 (2024).
[142] Patrick Schramowski, Manuel Brack, Björn Deiseroth, and Kristian Kersting. 2023. Safe latent diffusion: Mitigating
inappropriate degeneration in diffusion models. In CVPR.
[143] Christoph Schuhmann, Romain Beaumont, Richard Vencu, Cade Gordon, Ross Wightman, Mehdi Cherti, Theo
Coombes, Aarush Katta, Clayton Mullis, Mitchell Wortsman, et al. 2022. Laion-5b: An open large-scale dataset for
training next generation image-text models. Neurips (2022).
[144] Toby Shevlane, Sebastian Farquhar, Ben Garfinkel, Mary Phuong, Jess Whittlestone, Jade Leung, Daniel Kokotajlo,
Nahema Marchal, Markus Anderljung, Noam Kolt, et al. 2023. Model evaluation for extreme risks. arXiv preprint
arXiv:2305.15324 (2023).
[145] Hanul Shin, Jung Kwon Lee, Jaehong Kim, and Jiwon Kim. 2017. Continual learning with deep generative replay.
Neurips (2017).
[146] Nianwen Si, Hao Zhang, Heyu Chang, Wenlin Zhang, Dan Qu, and Weiqiang Zhang. 2023. Knowledge unlearning for
llms: Tasks, methods, and challenges. arXiv preprint arXiv:2311.15766 (2023).
[147] Richard Socher, Alex Perelygin, Jean Wu, Jason Chuang, Christopher D. Manning, Andrew Ng, and Christopher Potts.
2013. Recursive Deep Models for Semantic Compositionality Over a Sentiment Treebank. In EMNLP.
[148] Hui Sun, Tianqing Zhu, Wenhan Chang, and Wanlei Zhou. 2023. Generative adversarial networks unlearning. arXiv
preprint arXiv:2308.09881 (2023).
[149] Zhiqing Sun, Sheng Shen, Shengcao Cao, Haotian Liu, Chunyuan Li, Yikang Shen, Chuang Gan, Liang-Yan Gui,
Yu-Xiong Wang, Yiming Yang, et al. 2023. Aligning large multimodal models with factually augmented rlhf. arXiv
preprint arXiv:2309.14525 (2023).
[150] Alex Tamkin, Amanda Askell, Liane Lovitt, Esin Durmus, Nicholas Joseph, Shauna Kravec, Karina Nguyen, Jared
Kaplan, and Deep Ganguli. 2023. Evaluating and mitigating discrimination in language model decisions. arXiv
preprint arXiv:2312.03689 (2023).
[151] Zhaoxuan Tan, Qingkai Zeng, Yijun Tian, Zheyuan Liu, Bing Yin, and Meng Jiang. 2024. Democratizing Large
Language Models via Personalized Parameter-Efficient Fine-tuning. arXiv preprint arXiv:2402.04401 (2024).
[152] Pratiksha Thaker, Yash Maurya, and Virginia Smith. 2024. Guardrail Baselines for Unlearning in LLMs. arXiv preprint
arXiv:2403.03329 (2024).
[153] Anvith Thudi, Gabriel Deza, Varun Chandrasekaran, and Nicolas Papernot. 2022. Unrolling sgd: Understanding
factors influencing machine unlearning. In 2022 IEEE 7th European Symposium on Security and Privacy (EuroS&P).
[154] Piyush Tiwary, Atri Guha, Subhodip Panda, et al. 2023. Adapt then Unlearn: Exploiting Parameter Space Semantics
for Unlearning in Generative Adversarial Networks. arXiv preprint arXiv:2309.14054 (2023).
[155] Hugo Touvron, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste
Rozière, Naman Goyal, Eric Hambro, Faisal Azhar, et al. 2023. Llama: Open and efficient foundation language models.
arXiv preprint arXiv:2302.13971 (2023).
[156] Don Tuggener, Pius Von Däniken, Thomas Peetz, and Mark Cieliebak. 2020. LEDGAR: A large-scale multi-label
corpus for text classification of legal provisions in contracts. In LREC.
[157] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Łukasz Kaiser, and Illia
Polosukhin. 2017. Attention is all you need. Neurips (2017).
[158] Pascal Vincent, Hugo Larochelle, Isabelle Lajoie, Yoshua Bengio, Pierre-Antoine Manzagol, and Léon Bottou. 2010.
Stacked denoising autoencoders: Learning useful representations in a deep network with a local denoising criterion.
JMLR (2010).
[159] Yuxuan Wan, Wenxuan Wang, Pinjia He, Jiazhen Gu, Haonan Bai, and Michael R Lyu. 2023. Biasasker: Measuring the
bias in conversational ai system. In FSE Conference.
[160] Alex Wang, Amanpreet Singh, Julian Michael, Felix Hill, Omer Levy, and Samuel R Bowman. 2018. GLUE: A multi-task
benchmark and analysis platform for natural language understanding. arXiv preprint arXiv:1804.07461 (2018).
[161] Lingzhi Wang, Tong Chen, Wei Yuan, Xingshan Zeng, Kam-Fai Wong, and Hongzhi Yin. 2023. Kga: A general machine
unlearning framework based on knowledge gap alignment. arXiv preprint arXiv:2305.06535 (2023).
[162] Lingzhi Wang, Xingshan Zeng, Jinsong Guo, Kam-Fai Wong, and Georg Gottlob. 2024. Selective forgetting: Advancing
machine unlearning techniques and evaluation in language models. arXiv preprint arXiv:2402.05813 (2024).
[163] Xiaohan Wang, Shengyu Mao, Ningyu Zhang, Shumin Deng, Yunzhi Yao, Yue Shen, Lei Liang, Jinjie Gu, and Huajun
Chen. 2024. Editing Conceptual Knowledge for Large Language Models. arXiv preprint arXiv:2403.06259 (2024).
[164] Yu Wang, Ruihan Wu, Zexue He, Xiusi Chen, and Julian McAuley. 2024. Large Scale Knowledge Washing. arXiv
preprint arXiv:2405.16720 (2024).
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

111:34
Trovato et al.
[165] Alexander Warnecke, Lukas Pirch, Christian Wressnegger, and Konrad Rieck. 2021. Machine unlearning of features
and labels. arXiv preprint arXiv:2108.11577 (2021).
[166] Albert Webson and Ellie Pavlick. 2021. Do prompt-based models really understand the meaning of their prompts?
arXiv preprint arXiv:2109.01247 (2021).
[167] Alexander Wei, Nika Haghtalab, and Jacob Steinhardt. 2024. Jailbroken: How does llm safety training fail? Neurips
(2024).
[168] Mika Westerlund. 2019. The emergence of deepfake technology: A review. Technology innovation management review
(2019).
[169] Mitchell Wortsman, Gabriel Ilharco, Samir Ya Gadre, Rebecca Roelofs, Raphael Gontijo-Lopes, Ari S Morcos, Hongseok
Namkoong, Ali Farhadi, Yair Carmon, Simon Kornblith, et al. 2022. Model soups: averaging weights of multiple
fine-tuned models improves accuracy without increasing inference time. In ICML.
[170] Mitchell Wortsman, Gabriel Ilharco, Jong Wook Kim, Mike Li, Simon Kornblith, Rebecca Roelofs, Raphael Gontijo
Lopes, Hannaneh Hajishirzi, Ali Farhadi, Hongseok Namkoong, et al. 2022. Robust fine-tuning of zero-shot models.
In CVPR.
[171] Jing Wu, Trung Le, Munawar Hayat, and Mehrtash Harandi. 2024. Erasediff: Erasing data influence in diffusion
models. arXiv preprint arXiv:2401.05779 (2024).
[172] Xinwei Wu, Junzhuo Li, Minghui Xu, Weilong Dong, Shuangzhi Wu, Chao Bian, and Deyi Xiong. 2023. Depn:
Detecting and editing privacy neurons in pretrained language models. arXiv preprint arXiv:2310.20138 (2023).
[173] Yuanwei Wu, Xiang Li, Yixin Liu, Pan Zhou, and Lichao Sun. 2023. Jailbreaking gpt-4v via self-adversarial attacks
with system prompts. arXiv preprint arXiv:2311.09127 (2023).
[174] Shangyu Xing, Fei Zhao, Zhen Wu, Tuo An, Weihao Chen, Chunhui Li, Jianbing Zhang, and Xinyu Dai. 2024. EFUF:
Efficient Fine-grained Unlearning Framework for Mitigating Hallucinations in Multimodal Large Language Models.
arXiv preprint arXiv:2402.09801 (2024).
[175] Heng Xu, Tianqing Zhu, Lefeng Zhang, Wanlei Zhou, and Philip S. Yu. 2023. Machine Unlearning: A Survey. CSUR
(2023).
[176] Yi Xu. 2024. Machine Unlearning for Traditional Models and Large Language Models: A Short Survey. arXiv preprint
arXiv:2404.01206 (2024).
[177] Jin Yao, Eli Chien, Minxin Du, Xinyao Niu, Tianhao Wang, Zezhou Cheng, and Xiang Yue. 2024. Machine Unlearning
of Pre-trained Large Language Models. arXiv preprint arXiv:2402.15159 (2024).
[178] Yuanshun Yao, Xiaojun Xu, and Yang Liu. 2023. Large language model unlearning. arXiv preprint arXiv:2310.10683
(2023).
[179] Charles Yu, Sullam Jeoung, Anish Kasi, Pengfei Yu, and Heng Ji. 2023. Unlearning bias in language models by
partitioning gradients. In Findings of the ACL.
[180] Tianyu Yu, Yuan Yao, Haoye Zhang, Taiwen He, Yifeng Han, Ganqu Cui, Jinyi Hu, Zhiyuan Liu, Hai-Tao Zheng,
Maosong Sun, et al. 2023. Rlhf-v: Towards trustworthy mllms via behavior alignment from fine-grained correctional
human feedback. arXiv preprint arXiv:2312.00849 (2023).
[181] Rowan Zellers, Ari Holtzman, Yonatan Bisk, Ali Farhadi, and Yejin Choi. 2019. Hellaswag: Can a machine really finish
your sentence? arXiv preprint arXiv:1905.07830 (2019).
[182] Dawen Zhang, Pamela Finckenberg-Broman, Thong Hoang, Shidong Pan, Zhenchang Xing, Mark Staples, and Xiwei
Xu. 2023. Right to be forgotten in the era of large language models: Implications, challenges, and solutions. arXiv
preprint arXiv:2307.03941 (2023).
[183] Eric Zhang, Kai Wang, Xingqian Xu, Zhangyang Wang, and Humphrey Shi. 2023. Forget-me-not: Learning to forget
in text-to-image diffusion models. arXiv preprint arXiv:2303.17591 (2023).
[184] Jinghan Zhang, Junteng Liu, Junxian He, et al. 2024. Composing Parameter-Efficient Modules with Arithmetic
Operation. Neurips (2024).
[185] Ruiqi Zhang, Licong Lin, Yu Bai, and Song Mei. 2024. Negative Preference Optimization: From Catastrophic Collapse
to Effective Unlearning. arXiv preprint arXiv:2404.05868 (2024).
[186] Saizheng Zhang, Emily Dinan, Jack Urbanek, Arthur Szlam, Douwe Kiela, and Jason Weston. 2018. Personalizing
dialogue agents: I have a dog, do you have pets too? arXiv preprint arXiv:1801.07243 (2018).
[187] Yimeng Zhang, Jinghan Jia, Xin Chen, Aochuan Chen, Yihua Zhang, Jiancheng Liu, Ke Ding, and Sijia Liu. 2023. To
generate or not? safety-driven unlearned diffusion models are still easy to generate unsafe images... for now. arXiv
preprint arXiv:2310.11868 (2023).
[188] Yihua Zhang, Yimeng Zhang, Yuguang Yao, Jinghan Jia, Jiancheng Liu, Xiaoming Liu, and Sijia Liu. 2024. UnlearnCan-
vas: A Stylized Image Dataset to Benchmark Machine Unlearning for Diffusion Models. arXiv preprint arXiv:2402.11846
(2024).
[189] Jiachen Zhao, Zhun Deng, David Madras, James Zou, and Mengye Ren. 2023. Learning and forgetting unsafe examples
in large language models. arXiv preprint arXiv:2312.12736 (2023).
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

Machine Unlearning in Generative AI: A Survey
111:35
[190] Wayne Xin Zhao, Kun Zhou, Junyi Li, Tianyi Tang, Xiaolei Wang, Yupeng Hou, Yingqian Min, Beichen Zhang, Junjie
Zhang, Zican Dong, et al. 2023. A survey of large language models. arXiv preprint arXiv:2303.18223 (2023).
[191] Lianmin Zheng, Wei-Lin Chiang, Ying Sheng, Siyuan Zhuang, Zhanghao Wu, Yonghao Zhuang, Zi Lin, Zhuohan Li,
Dacheng Li, Eric Xing, et al. 2024. Judging llm-as-a-judge with mt-bench and chatbot arena. Neurips (2024).
[192] Xin Zhou, Yi Lu, Ruotian Ma, Tao Gui, Qi Zhang, and Xuanjing Huang. 2023. Making harmful behaviors unlearnable
for large language models. arXiv preprint arXiv:2311.02105 (2023).
[193] Yufan Zhou, Ruiyi Zhang, Changyou Chen, Chunyuan Li, Chris Tensmeyer, Tong Yu, Jiuxiang Gu, Jinhui Xu, and
Tong Sun. 2022. Towards language-free training for text-to-image generation. In CVPR.
[194] Yukun Zhu, Ryan Kiros, Rich Zemel, Ruslan Salakhutdinov, Raquel Urtasun, Antonio Torralba, and Sanja Fidler. 2015.
Aligning Books and Movies: Towards Story-Like Visual Explanations by Watching Movies and Reading Books. In
ICCV.
J. ACM, Vol. 37, No. 4, Article 111. Publication date: August 2018.

