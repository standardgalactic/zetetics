
Advances in Neuromorphic Memristor Science
and Applications

Springer Series in Cognitive and Neural Systems
Volume 4
Series Editors
John G. Taylor
King’s College, London, UK
Vassilis Cutsuridis
Boston University, Boston, MA, USA
For further volumes:
http://www.springer.com/series/8572

Robert Kozma • Robinson E. Pino
Giovanni E. Pazienza
Editors
Advances in Neuromorphic
Memristor Science
and Applications
2123

Editors
Prof. Dr. Robert Kozma
Giovanni E. Pazienza
Department of Mathematical Sciences
Department of Mathematical Sciences
The University of Memphis
The University of Memphis
Memphis, TN
Memphis, TN
USA
USA
Robinson E. Pino
Air Force Research Laboratory
Rome, NY
USA
ISBN 978-94-007-4490-5
ISBN 978-94-007-4491-2 (eBook)
DOI 10.1007/978-94-007-4491-2
Springer Dordrecht Heidelberg London New York
Library of Congress Control Number: 2012941404
© Springer Science+Business Media Dordrecht 2012
No part of this work may be reproduced, stored in a retrieval system, or transmitted in any form or by
any means, electronic, mechanical, photocopying, microﬁlming, recording or otherwise, without written
permission from the Publisher, with the exception of any material supplied speciﬁcally for the purpose of
being entered and executed on a computer system, for exclusive use by the purchaser of the work.
Printed on acid-free paper
Springer is part of Springer Science+Business Media (www.springer.com)

Foreword
We are delighted to present this volume which gives a state-of-art overview of neuro-
morphic memristor theory, its technical aspects, and practical implementations. The
book discusses fundamental concepts of memristors and their relevance in various
disciplines, including physics, neuroscience, computer science and engineering. It
reviews computational models and simulations of memristors with special emphasis
on neuromorphic designs. Hardware embodiments with memristive properties and
applications are described as well, including nano-technology, intelligent systems,
computer vision, large-scale optimization, and robotics. The materials presented
here are based on the invited and contributed talks given at the Special Sessions on
neuromorpic memristor technology at the IEEE/INNS IJCNN2011 Conference, as
well as the he workshop on “Future Perspectives of Neuromorphic Memristor Sci-
ence & Technology” at IJCNN2011. The editors of this book are the organizers of
these series of memristor-related activities in July/August 2011, in San Jose, CA,
USA.
The intended audience of this book includes neuroscientists, computational sci-
entists and engineers interested in learning about the rapidly developing ﬁeld of
memristorscienceandtechnology. Thebookisself-containedandprovidesacompre-
hensivedescriptionofmemristorfundamentals, modeling, andpotentialapplications.
This will facilitate classroom adaptations and makes it suitable as a textbook in
advanced graduate courses.
It is conceivable that the learning effects so eloquently displayed my memristors
are in fact manifestations of memristive behavior in the neural tissue. In this case
memristors indeed could be the Holy Grail of building brain like computers by ex-
ploiting the same mechanisms in computer memories as the ones brains employ. This
possibility has enormous long-term consequences, which is difﬁcult even to imagine
from our present limited vantage point. We hope that this volume can illuminate
some challenging aspects of memristors and give guidance to the road ahead.
Memphis TN—Rome NY, USA—Rome, Italy
Winter 2011–2012
Robert Kozma
Robinson E. Pino
Giovanni E. Pazienza
v


Contents
Part I: Fundamental Concepts of Memristors and Neuromorphic
Systems . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1
Prolog: Memristor Minds . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
3
Greg Snider
2
Are Memristors the Future of AI? A Review of Recent Progress
and Future Perspectives . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
9
Robert Kozma, Robinson E. Pino, and Giovanni E. Pazienza
3
Biologically-Inspired Electronics with Memory Circuit Elements . . . .
15
Massimiliano Di Ventra and Yuriy V. Pershin
4
Persuading Computers to Act More Like Brains . . . . . . . . . . . . . . . . . . .
37
Heather Ames, Massimiliano Versace, Anatoli Gorchetchnikov, Benjamin,
Chandler, Gennady Livitz, Jasmin Léveillé, Ennio Mingolla,
Dick Carter, Hisham Abdalla, and Greg Snider
5
Memristors for More Than Just Memory: How to Use Learning
to Expand Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
63
Paul J.Werbos
Part II: Computational Models of Memristors . . . . . . . . . . . . . . . . . . . . . . . .
75
6
Computational Intelligence and Neuromorphic Computing
Architectures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
77
Robinson E. Pino
7
Reconﬁgurable Memristor Fabrics for Heterogeneous Computing . . .
89
Dhireesha Kudithipudi and Cory E. Merkel
8
Statistical Memristor Model and Its Applications In Neuromorphic
Computing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 107
Hai (Helen) Li, Miao Hu, and Robinson E. Pino
vii

viii
Contents
9
Adaptive Resonance Theory Design in Mixed Memristive-Fuzzy
Hardware . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133
Max Versace, Robert T. Kozma, and Donald C. Wunsch
10 Phase Change Memory and Chalcogenide Materials for
Neuromorphic Applications: Emphasis on Synaptic Plasticity . . . . . . . 155
Manan Suri and Barbara DeSalvo
Part III: Hardware Embodiments with Memristive Properties
and Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 179
11 Energy-Efﬁcient Memristive Analog and Digital Electronics . . . . . . . . 181
Sung Mo (Steve) Kang and Sangho Shin
12 Memristor SPICE Modeling. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 211
Chris Yakopcic, Tarek M. Taha, Guru Subramanyam,
and Robinson E. Pino
13 Memristor Models for Pattern Recognition Systems . . . . . . . . . . . . . . . . 245
Fernando Corinto, Alon Ascoli, and Marco Gilli
14 A Columnar V1/V2 Visual Cortex Model and Emulation . . . . . . . . . . . . 269
Robinson E. Pino and Michael Moore
15 Polymer and Nanoparticle-Composite Bistable Devices: Physics
of Operation and Initial Applications . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 291
Robert A. Nawrocki, Richard M. Voyles, and Sean E. Shaheen
Index . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 315

Part I
Fundamental Concepts of Memristors
and Neuromorphic Systems


Chapter 1
Prolog: Memristor Minds
Greg Snider
Abstract What is the best, near-term approach for building intelligent machines?
We explore the impact of memristive memory on the technological and mathematical
foundations of neuromorphic computing.
1.1
Introduction
Memristive nanodevices have inspired the neuromorphic community to examine
their potential for building low-power, intelligent machines. Their dynamics [20]
and small size have suggested their use as “synapses” in analog circuits that learn
online in real time [1, 3, 8, 15, 22, 23]. But is that really the most effective way to
use them in a neuromorphic system? We suggest that using them as discrete memory
elements in a digital platform is a technically less risky and economically far more
viable path to achieving adaptive machine intelligence. There are two entangled
issues to consider when addressing this question:
1. Digital vs. Analog. Can intelligence be implemented using digital electronics, or
is it necessary to replicate the analog processes found in biological brains?
2. Algebra vs. Analysis. Are nonlinear differential equations the preferred (or
necessary) mathematical foundation for building intelligence, or is another
mathematical approach possible?
1.2
Digital vs. Analog
The neuromorphic hardware community is largely focused on developing massively-
parallel, subthreshold analog circuits, often combined with memristive or other
experimental devices for implementing synaptic memory. Mead [18] pioneered this
G. Snider ()
Hewlett-Packard Laboratories, Palo Alto, CA, USA
e-mail: snider.greg@hp.com
R. Kozma et al. (eds.), Advances in Neuromorphic Memristor Science and Applications,
3
Springer Series in Cognitive and Neural Systems,
DOI 10.1007/978-94-007-4491-2_1, © Springer Science+Business Media Dordrecht 2012

4
G. Snider
analog approach in the early 1980s, inspired by his estimate that any digital ap-
proach would necessarily require “more than 10 MW” to implement human-scale
intelligence and was therefore impractical [9, 10, 19].
Mead’s power estimate was based on his implicit assumption that nonlinear dif-
ferential equations were necessary as a mathematical foundation, and his explicit
prediction that digital CMOS could not scale much beyond the 100 nm node and
would require a minimum of 104 pJ per ﬂoating point operation [9, 10, 19]. The
CMOS industry has continued beyond that predicted barrier, though, approaching
22 nm today, and one semiconductor manufacturer has announced their intention of
building a 14 nm CMOS fabrication plant. A DARPA-funded study on the future of
CMOS headed by Peter Kogge predicted that device scaling would continue until
the second half of this decade with accompanying decreases in power for ﬂoating
point computation down to the 5–10 pJ range [17], not much above the roughly 1 pJ
Mead said was required for a subthreshold analog multiplication.
One must also consider the amount of information actually computed in each case.
Digital ﬂoating point produces a 32 or 64 bit result, while an analog subthreshold
operation would produce no more than about 10 bits [9]. In practice, it produces
fewer useful bits than that because of device variation inherent in the subthreshold
analog circuitry. Economically compensating for that variation is still an unsolved
problem. The potential energy advantages of subthreshold analog computation that
Mead foresaw have been rendered largely moot by improvements in digital CMOS
fabrication technology.
In any case, the dominant power issue is no longer computation. It is parasitic
losses, particularly CV2f losses arising from transmitting information over wires.
These losses are unavoidable, short of somehow using uncharged particles for com-
putation. Kogge estimated that without architectural advances, the 10 pJ/ﬂoating
point operation that he projected at the end of the roadmap would be dwarfed
by the 1,000–10,000 pJ needed to transmit the operands and result between the
memory system and the ﬂoating point unit [17]. The computational energy cost is
negligible in comparison. Reducing communication energy, necessary regardless of
whether the computation is digital or analog, is being attacked on at least two fronts:
(1) CMOS-compatible memory that allows close integration of dense memory banks
next to or on top of cores to greatly increase bandwidth and reduce CV2f losses; and
(2) Photonic interconnect between chips (perhaps even within chips to a degree), to
enable energy-efﬁcient long-distance communication.
However, the biggest shortcoming of an analog approach is its inﬂexibility.Analog
achievesmuchofitsefﬁciencybyhardwiringcriticaldynamics, withthemostpopular
approach currently being analog implementation of simpliﬁed models of neurons and
synapses. But what if those dynamics are wrong? We understand that current popular
models for neurons (such as Hodgkin-Huxley) and synapses (such as spike-timing-
dependent plasticity) are missing essential state variables necessary for learning
and homeostasis [16, 21]. There is no reason that one should one expect these
simpliﬁed models to possess brain-like computational abilities. In fact, despite years

1
Prolog: Memristor Minds
5
of simulations of extremely large-scale systems, no one has ever produced a working
example of a non-trivial, intelligent computation using such components.
Digital multicore chips, on the other hand, are completely ﬂexible in implement-
ing algorithms and dynamics.All-digital, software approaches are widely used by the
robotics, machine vision, and speech recognition communities, with GPUs playing
an increasing role. GPUs are less than ideal as a high performance computing plat-
form because of their restrictive streaming memory model and lean local memory,
typically only a few kB per core. They are also power-hungry and I/O bound, since
they must repeatedly stream off-chip memory through them at frame display rates,
e.g. 60 Hz. Fortunately, many cognitive algorithms have very graphics-like memory
access patterns (high spatial or sequential locality, low temporal locality) and work
well with existing GPUs. They will work even better with future multi-core chips
that incorporate dense, on-chip, per-core memory (perhaps a megabyte or so per
core) to increase bandwidth and reduce power by a couple of orders of magnitude.
1.3
Algebra vs. Analysis
Digital computers have one signiﬁcant shortcoming: they are very inefﬁcient at inte-
grating the stiff differential equations found in many cognitive algorithms. Short of
an algorithmic breakthrough in nonlinear dynamics, digital computers will probably
never compete with analog in this area.
Are nonlinear differential equations essential for intelligence? Despite this being
a common assumption in the ﬁeld, there is no theoretical foundation for it. Massive
simulations of extremely large systems of nonlinear differential equations represent-
ing extensive brain models have been done on supercomputers [2], and although
dynamics qualitatively similar to those in biological brains emerge, none have pro-
duced anything resembling intelligence. Useful subthreshold analog circuits have
been successfully built, but their functionality is primitive.
Conversely, there is no theoretical reason for believing an alternate mathematical
framework, such as algebra, would not work. A crude analogy is that both the
Schroedinger and the Heisenberg approaches to quantum mechanics yield the same
results, even though the former is based on differential equations and the latter on
algebra. Although there is no proof that algebra is sufﬁcient for cognition, nearly
all useful cognitive algorithms today are implemented on digital computers using
algebra and transcendental functions. There is also increasing anecdotal evidence
that many dynamical cognitive algorithms can be transformed into a functionally-
equivalent algebraic form. Here are two examples: (1) Adaptive resonance theory is
a dynamical model of learning ﬁrst developed in the 1970s [5–7, 12, 13]. In the late
1990s, several researchers independently discovered that the model could be mapped
to a feedforward, algebraic form that had identical functionality while requiring far
less computational energy ([4]). (2) The bipole model [14] was the ﬁrst cognitive
model to explain the phenomenon of boundary completion, which allows humans
to recognize partially occluded objects such as predators hiding in the bushes. But

6
G. Snider
integrating the model’s differential equations was computationally prohibitive. A
series of transformations involving tensor convolution, steerable ﬁlter theory, and
Fourier transforms, have produced an algebraic bipole model that runs in realtime
on a GPU [11].
1.4
Summary
We do not know how to build intelligent machines today; we can learn how to
build them only by bootstrapping from a suitable hardware platform. Any platform
designed to develop intelligent machines must be: (1) massively-parallel; (2) low-
power; (3) algorithmically ﬂexible. Both analog and digital approaches can achieve
massive parallelism. Because of continued scaling of digital CMOS well beyond
what was thought possible in the early 1990s, both analog and digital approaches
will be able to achieve roughly equivalent computational power loads at the end of
the CMOS roadmap. But only a software/digital approach provides the algorithmic
ﬂexibility needed to explore this ﬁeld.
References
1. AﬁﬁA, Ayatollahi A, Raissi F (2009) STDP implementation using memristive nanodevice in
CMOS-Nano neuromorphic networks. IEICE Electron Exp 6(3):148–153
2. Ananthanarayanan R, Esser S, Simon H, Modha D (2009) The cat is out of the bag: cortical
simulations with 109 neurons, 1013 synapses. In: Proceedings of the conference on high perfor-
mance computing networking, storage and analysis, Portland, Oregon, 14–20 November 2009.
SC ’09. ACM, New York, NY, 1–12. http://doi.acm.org/10.1145/1654059.165412. Accessed
25 Oct 2011
3. Bernabé L, Serrano-Gotarredona T (2009) Memristance can explain spike-time-dependent-
plasticity in neural synapses. Nature Precedings. http://precedings.nature.com. Accessed
31 Mar 2009
4. Blume M (2000) An efﬁcient mapping of fuzzy ART onto a neural architecture. In: Jain LC,
Lazzerini B, Halici U (eds) Innovations in ART neural networks. Physica-Verlag, Heidelberg
5. Carpenter G, Grossberg S (1987) A massively parallel architecture for a self-organizing neural
pattern recognition machine. Comput Vision Graph Image Process 37:54–115
6. Carpenter G, Grossberg S (1988, March) The ART of adaptive pattern recognition by a self-
organized neural network. Computer 21(3):77–88
7. Carpenter GA, Grossberg S, Rosen DB (1991) Fuzzy ART: fast stable learning and categoriza-
tion of analog patterns by an adaptive resonance system. Neural Networks 4:759–771
8. Choi H, Jung H, Lee J, Yoon J, Park J, Seong D, Lee W, Hasan M, Jung GY, Hwang H (2009)
An electrically modiﬁable synapse array of resistive switching memory. Nanotechnology
20(34):345201
9. Douglas R, Mahowald M, Mead C (1995) Neuromorphic analogue VLSI. Ann Rev Neurosci
18:255–281
10. Faggin F, Mead C (1995) VLSI implementation of neural networks. In: Zornetzer SF, Davis
JL, Law C (eds)An introduction to neural and electronic networks. Academic Press, San Diego
(Chap 15)

1
Prolog: Memristor Minds
7
11. Franken E, van Almsick M, Rongen P, Florack L, ter Haar Romeny B (2006) An efﬁcient
method for tensor voting using steerable ﬁlters. Lect Notes Comput Sci 3954:228–240.
doi:10.1007/11744085_18
12. Grossberg S (1976a) Adaptive pattern classiﬁcation and universal recoding I: parallel
development and coding of neural feature detectors. Biol Cybernet 23:121–134
13. Grossberg S (1976b) Adaptive pattern classiﬁcation and universal recoding II: feedback,
expectation, olfaction, and illusions. Biol Cybernet 23:187–202
14. Grossberg S, Mingolla E (1985) Neural dynamics of perceptual grouping—textures, bound-
aries, and emergent segmentations. Percept Psychophys 38:141–171
15. Jo SH, Chang T, Ebong I, Bhadviya BB, Mazumder P, Lu W (2010) Nanoscale memristor
device as synapse in neuromorphic systems. Nano Lett 10(4):1297–1301
16. Kennedy M (2011) Questions about STDP as a general model of synaptic plasticity.
http://www.frontiersin.org/synaptic_neuroscience/10.3389/fnsyn.2010.00140/full.
Accessed
25 Oct 2011
17. Kogge P (2011) The tops in ﬂops. IEEE Spectr 48:49–55
18. Mead C (1989) Analog VLSI and neural systems. Addison-Wesley Longman Publishing Co.,
Inc, Boston
19. Mead C (1990) Neuromorphic electronic systems. Proc IEEE 78:1629–1636
20. Pickett MD, Strukov DB, Borghetti JL, Yang JJ, Snider GS, Stewart DR, Williams RS (2009)
Switching dynamics in titanium dioxide memristive devices. J Appl Phys 106;074508
21. ShouvalHZetal(2010)Spike-timing-dependentplasticity: aconsequenceofmorefundamental
learning laws. Front Comput Neurosci 4:19
22. Snider G (2007, September 12) Self-organized computation with unreliable, memristive
nanodevices. Nanotechnology 18(36):365202
23. Snider G (2008) Spike-timing-dependent learning in memristive nanodevices. In: IEEE/ACM
international symposium on nanoscale architectures, Anaheim, CA, pp 85–92


Chapter 2
Are Memristors the Future of AI?
A Review of Recent Progress and Future Perspectives
Robert Kozma, Robinson E. Pino, and Giovanni E. Pazienza
Abstract We review the state-of-the-art of neuromorphic memristor science and
technology. We cover principles of memristors and neuromorphic systems, compu-
tational models of memristors, and hardware implementations. Potential applications
of memristors are also described, including supercomputing, image processing, com-
puter vision, intelligent control, and robotics. This review is based on the chapters
of the present volume, which extend the materials of the invited and plenary talks
given at the series of events on memristors in 2011. We elaborate on challenges and
future perspectives of this promising new research ﬁeld.
2.1
Introduction
In 2008 scientists from Hewlett-Packard discovered a nano-scale device called the
memristor, ahypotheticalcircuitelementpredictedin1971byLeonChua, UCBerke-
ley [1–3]. This has generated unprecedented worldwide interests because, among
many applications, memristors can be used as super-dense non-volatile memories for
building instant turn-on computers. There are suggestions from many researchers that
memristor-based analog memory can be used to build brain-like learning machines
with nano-scale memristive synapses. In the past years, the ﬁeld of memristors has
expanded rapidly, with numerous publications and meeting in this area. The present
volume is the result of such recent activities [4–5]. Memristor technology has the
potential to revolutionize computing and scientiﬁc research in the coming decades.
There are, however, signiﬁcant challenges, which may hamper broad proliferation of
memristors, and which may ultimately prevent the full realization of its extraordinary
potentials. We can learn from past mistakes in scientiﬁc discoveries to mitigate the
consequences of inevitable pitfalls along the way. This volume can help to establish
a roadmap towards materializing the full potential of memristors.
R. Kozma () · G. E. Pazienza
FedEx Institute of Technology, Department of Mathematical Sciences, University of Memphis,
Memphis, TN, USA
e-mail: rkozma@memphis.edu
R. E. Pino
Advanced Computing, Air Force Research Laboratory, Rome, NY, USA
R. Kozma et al. (eds.), Advances in Neuromorphic Memristor Science and Applications,
9
Springer Series in Cognitive and Neural Systems,
DOI 10.1007/978-94-007-4491-2_2, © Springer Science+Business Media Dordrecht 2012

10
R. Kozma et al.
There is a need to systematically elaborate the theoretical foundations of mem-
ristors, following Chua’s visionary work. The invention of the ﬁrst materials with
memristive properties using TiO2 compounds was quickly followed by the identiﬁca-
tion of more and more additional materials which exhibit memristive properties and
can serve as basis of memristive devices. Clearly, some speciﬁc features of various
memristive compounds may be beneﬁcial in some context and disadvantageous in
other cases. Thus there is a need to conduct signiﬁcant materials science studies to
identify optimal memristors for various technical requirements. Then there is the is-
sue of constructing electrical circuitries using memristive components. The pinched
hysteresis feature underlying the operation of memristors indicate the potential of
using memristors in a continuous operational mode as part of the analogous compu-
tational paradigm. However, the presently dominant digital computers may in fact
imply that all efforts should be concentrated on digital manifestations of memristors,
in order to reap the beneﬁts of the digital computing paradigm to the fullest extent.
As far as applications are concerned, memristors are likely to be implemented in
the near future as powerful ﬂash memories. Another application area is the design
of electronic circuits with inherent learning capabilities, when data processing and
memory functions are not separated; rather those functions are completed on a uniﬁed
hardware device using memristors.
This leads us to the concept of neuromorphic hardware. Brain tissues are generally
assumed to be complex networks of neurons, which change their functional prop-
erties as they process sensory data in various cortical areas. This process is called
learning and the learned knowledge is stored in the cortex through modiﬁed synaptic
connectivity. There are various models of learning in the neural tissues, including
Hebbian correlational learning, reinforcement, habituation, homeostatis, spike time
dependent plasticity, and others. It is conceivable that all these learning effects are in
fact manifestations of memristive behavior in the neural tissue. The present volume
emphasizes the potential of memristors in neuromorphic designs.
The volume starts with Snyder’s thought-provoking review on the potential of
memristive nanodevices for building low-power, intelligent machines. He suggests
using memristors as discrete memory elements in a digital platform, rather than
analog implementations. He contends that this is a technically less risky and econom-
ically far more viable path to achieving adaptive machine intelligence. To illustrate
his point, he introduces two pairs of paradigms, which he coins the digital versus
analog and algebraversus analysis complementary pairs. He asks the questions: Can
intelligence be implemented using digital electronics, or is it necessary to replicate
the analog processes found in biological brains? Are nonlinear differential equations
the preferred (or necessary) mathematical foundation for building intelligence, or is
another mathematical approach possible? He puts forward convincing arguments for
answering the ﬁrst question afﬁrmatively, i.e., machine intelligence is possible and in
fact preferable to be implemented on digital hardware. As for the second question, he
argues that mathematical description of neural activity using differential equations
cannot be a requirement of modeling intelligent behavior, and alternative approaches
using matrix algebra and other methods are fully justiﬁed, if not preferred.

2
Are Memristors the Future of AI?
11
Part I of the book outlines fundamental concepts of memristors and neuromorphic
systems. Di Ventra and Pershin focuses in Chap. 3 on the broad category of mem-
ory circuit elements which includes, besides the memristor, the memcapacitor and
the meminductor. Their study shows that simple memristive, memcapacitive, and
meminductive systems can model a variety of biological processes such as the adap-
tive behavior of unicellular organisms. Furthermore, it is discussed how networks of
memory circuit can be used for solving efﬁciently graph theory optimization prob-
lems and it is proved that a network of memristive devices solves the maze problem
faster than any existing algorithm. This example is just an illustration of the general
concept; however, it paves the way for numerous practical applications. In view of
the recent advances of nanotechnology, it is now feasible to assemble ultra-dense
networks of memory circuit elements. As shown in this work, they can be used
to create a novel generation of ‘smart’ electronic circuits whose potentialities are
yet to be explored. In Chap. 4, Ames et al. sets forth a systematic analysis of the
relevant literature on neuromorphic hardware. They include a thorough discussion
concerning the key features, which they consider crucial in future intelligent machine
embodiments. Based upon this discussion, the authors identify the need of a powerful
software platform serving as the glue between the neural model and the hardware on
which it is implemented. This is the main motivation for a software framework called
Cog whose purpose is to abstract away the details of the underlying hardware to the
software designers and hence allow the seamless integration of new hardware. Cur-
rently, Cog runs on single core CPU s and GPUs (or a cluster of both processors), but
it is speciﬁcally designed to reduce communication energy in neuromorphic comput-
ing by leveraging the introduction of dense memristive memories close to computing
cores. Cog has been used in a complex system called MoNETA (Modular Neural Ex-
ploring Traveling Agent) that generates complex behaviors based on functionalities
that include perception, motivation, decision-making, and navigation. This architec-
ture has been tested in a virtual environment simulating intelligent and autonomous
behaviors in robotics applications.
Part I concludes with Chap. 5 by Werbos which explores the question how can
memristors be more than just memory devices. Werbos provides an excellent sum-
mary of the milestones of neuroengineering and well as cognitive optimization and
prediction, which would lead to the emergence of intelligent behavior, ultimately
at the level produced by human brains. The author, who has been a key player in
these areas since their inception, describes the main events—especially those oc-
curred within the framework of the National Science Foundation—that have led to
the better understanding of the mechanisms according to which the brains learn. In
the near future, the memristor is expected to allow us to achieve a density of func-
tional features (or devices) on-board chips that is far greater than what we have so
far. The author contends that brain-like principles should be exploited on the soon
to be available memristive hardware platforms. The author identiﬁes several key
applications domains for the new memristor systems; namely, complex large-scale
systems such as the management of complex infrastructures over time, and the last
generations of electric grids. The chapter poses numerous open questions which will
surely encourage the reader to further explore these topics.

12
R. Kozma et al.
Part II describes computational models of memristors and various modeling
challenges. In Chap. 6, Pino describes a basic self-reconﬁgurable neuromorphic com-
puting architecture leveraging the non-linear memristive behavior of ion-conductor
chalcogenide-based memristor devices. The chapter begins by describing what can
be described as the simplest possible basic building block for a memristor-based
neuromorphic computing architecture. Such basic building block seems representa-
tive of a self-contained unit-cell that exhibits the property of self-reconﬁguration by
allowing the user to train the circuit via perception and stimulation with electronic
pulses or spikes. Then, the author describes a methodology for the conﬁguration of
self-reconﬁgurable neuromorphic distributed networks. In Chap. 7, Kudithipudi and
Merkel present a CMOS/memristor hybrid technology, where memristive devices
are integrated within a 3D memristive crossbar architecture. The chapter discusses
different models which can be used for implementing memristors as memory, sens-
ing, logic, and neuromorphic units. The authors content that a prime advantage
of such computing technology is its ability to offer tera-bit densities with ultra-
low power and long data retention times. The main idea is that such distributed
computational memristor fabrics can dynamically transform over time to perform
heterogeneous computing, with the technological objective of superseding classical
CMOS architectures. In Chap. 8, Li and Pino discuss TiO2 and spintronic memristor-
based synaptic designs with a training scheme and explore implications of statistical
device variation. The authors discuss the impacts of geometry variation on the elec-
trical properties of these two different types of memristors by analytical modeling
and Monte-Carlo simulations. A simple algorithm, based on the latest characteriza-
tion method of LER (line edge roughness) and thickness ﬂuctuations, is proposed
to generate a large volume of geometry variation-aware three-dimensional device
structures for Monte-Carlo simulations, and a process-variation aware device model
is proposed. These results make it possible for scientists and engineers to map virtual
neural networks to physical hardware designs with the corresponding training circuit
to mimic a biological system.
In Chap. 9, Verzace et al. analyzes power requirements of arithmetic computa-
tional processes implemented on memristive devices. Storing and updating synaptic
weights based on synaptic plasticity rules is a computationally very demanding op-
eration in biologically-inspired neural networks using basic operations of addition
and multiplication. Memristive hardware holds the promise of greatly reduced power
requirements by increasing synaptic memory storage capacity and decreasing wiring
length between memory storage and computational modules. The chapter reviews
power requirements of various computational algorithms and introduces a novel
computational tool based on fuzzy inference for adaptive resonance theory (ART)
networks. Fuzzy inference signiﬁcantly reduces the computational complexity of the
memristive hardware; while it is able to learn synaptic weights with the required pre-
cisions. In conclusion, fuzzy arithmetic completes the required classiﬁcation tasks
correctly and more efﬁciently than other approaches. Chapter 10 by Suri and Desalvo
reviews basic concepts related to Phase Change Memory (PCM) technology and its
potential connection to memristors. They describe a hybrid CMOS/memristive sys-
tem, in which a PCM device is sandwiched between two spiking CMOS circuits,

2
Are Memristors the Future of AI?
13
emulating the synapse connecting a pre- and a post- synaptic neuron. They present
basic principles of PCM devices and the current state of the technology. The use of
PCM devices for synapses has the potential advantages of high scalability, CMOS
compatibility, low programming-current, strong endurance and technological matu-
rity. They demonstrate the use of PCM devices for emulating speciﬁc functions of a
biological synapse, such as synaptic potentiation, synaptic depression and spike-time
dependent plasticity (STDP). They discuss recent research encompassing the study
of PCM devices and chalcogenide materials for neuromorphic applications, which
are important for realizing PCM-based large-scale neuromorphic systems.
Part III describes various hardware embodiments with memristive properties and
potential application areas. Kang and Shin reviews in Chap. 11 recent technology
trends in memristive analog and digital electronics, with particular attention to pro-
grammable analog and digital circuits, ultra-dense nonvolatile resistive memory
architectures, and zero leakage nonvolatile logic gates. A reconﬁgurable nonvolatile
computing platform that harnesses memristor properties is used to deploy massive
local nonvolatile memories and advance computing capabilities with much lowered
energy consumption compared to conventional charge-basedVLSI systems. They de-
scribe applications of memristive devices for nonvolatile memories, programmable
interconnects, logic gates, and nonvolatile latches with high integration density and
CMOS compatibility. They point out that combining memristors with the prevailing
CMOS technology would lead to the extension of Moore’s Law beyond the hitherto
observed technological limitations. Chapter 12 by Taha presents a review of existing
memristor modeling techniques and provides simulations that compare several exist-
ing models with published memristor characterization data. A discussion of existing
models is presented that explains how the equations of each relate to physical device
behaviors. The simulations were completed in SPICE and compare the output of the
different models to current–voltage relationships of physical devices. Sinusoidal and
triangular pulse inputs were used throughout the simulations to test the capabilities
of each model. The chapter is concluded by recommending a generalized memristor
model that can be accurately matched to several published device characterizations.
This generalized model has the potential for more accurate circuit simulation for a
wide range of device structures and voltage inputs.
Chapter 13 by Corinto et al. presents a rigorous mathematical study concerning
the dynamics of different memristor models with a special emphasis on the inﬂuence
of initial and boundary conditions of the system. This is the ﬁrst thorough system-
atic work in this area and it may have a deep impact on the future literature. The
analytical results linking the initial condition of a memristor with its current-voltage
characteristic can be used to devise a novel pattern recognition system based on the
synchronization of nonlinear dynamical systems. As suggested by the authors, such
a pattern recognition system may be used to interpret complex neurophysiological
phenomena, such as the binding problem. This contribution can be viewed as a cor-
nerstone of a new class of computing machines, the so-called Memristor Oscillatory
Neurocomputers, combining memristor synapse circuitry with oscillatory neuro-
computers and performing spatial-temporal pattern recognition. The breakthrough

14
R. Kozma et al.
provided by such systems may affect numerous disciplines, including intelligent
adaptive control and intelligent user interfaces.
Chapter 14 by Pino and Moore explores the implementation of neurophysiological
and psychological constructs to develop a hyper-parallel computing platform, termed
a neuromorphic computing. The authors describe a model of the primary visual cortex
(V1)andsimulationresultsfromahighperformancecomputingfacility. Theircolum-
nar V1 model includes binocular disparity and motion perception, and the V2 model
thick and pale stripes were added to produce aV1/V2 stereomotion forming a percep-
tion system. Both the V1 and V2 models were based upon structures approximating
neocortical minicolumns and functional columns. The neuromorphic strategies in-
cluded columnar organization, integrate-and-ﬁre neurons, temporal coding, point
attraction recurrent networks, Reichardt detectors and “confabulation” networks.
The authors seek to ﬁnd the underlying architecture of hyper-parallel machines, and
understand computational methods akin to how a brain deals with sensation, percep-
tion, memory, attention, and decision-making. Chapter 15 by Nawrocki et al. is a
systematic analysis of Organic Bistable Devices (OBDs) which offer several poten-
tial advantages also for memristors. The potential advantages include lower cost and
simpler fabrication as compared with their inorganic counterparts. OBDs are now
ready for commercial implementations. The chapter provides numerous illustrations
to guide the readers throughout the description of this vibrant technology, which is
soon expected to ﬁnd application in various revolutionary neuromorphic memristive
systems. The authors mention a few active leading projects in intelligent systems
and robotics, in the US and Europe, in which their technology can be particularly
successful.
We are pleased to present this volume and hope it will facilitate further progress
in the rapidly developing ﬁeld of memristors.
Acknowledgments R.K. acknowledges partial support by AFOSR Lab Task by the Mathematics
and Cognition Program (Dr. J. Myung, PM), and by DARPA Physical Intelligence Program through
HRL subcontract (Dr. N. Srinivasa PM).
References
1. Chua LO (1971) Memristor-the missing circuit element. IEEE Trans Circuit Theory 18:507–519
2. Snider G (2007) Self-organized computation with unreliable, memristive nanodevices. Nan-
otechnology, IOP Press 18(36):365202
3. Strukov DB et al (2008) The missing memristor found. Nature 453:80–83
4. Are memristors the future ofAI? Panel session organized by Kozma R, Pazienza G at IEEE/INNS
International joint conference on neural networks IJCNN2011, July 30–August 5, 2011, San
Jose, CA
5. Future perspectives of neuromorphic memristor science and technology, Workshop organized by
Kozma R, Pino R, Pazienza G at IEEE/INNS International joint conference on neural networks
IJCNN2011, July 30–August 5, 2011, San Jose, CA

Chapter 3
Biologically-Inspired Electronics with Memory
Circuit Elements
Massimiliano Di Ventra and Yuriy V. Pershin
Abstract Several unique properties of biological systems, such as adaptation to nat-
ural environment, or of animals to learn patterns when appropriately trained, are
features that are extremely useful, if emulated by electronic circuits, in applica-
tions ranging from robotics to solution of complex optimization problems, trafﬁc
control, etc. In this chapter, we discuss several examples of biologically-inspired
circuits that take advantage of memory circuit elements, namely, electronic elements
whoseresistive, capacitiveorinductivecharacteristicsdependontheirpastdynamics.
We provide several illustrations of what can be accomplished with these elements
including learning circuits and related adaptive ﬁlters, neuromorphic and cellular
computing circuits, analog massively-parallel computation architectures, etc. We
also give examples of experimental realizations of memory circuit elements and
discuss opportunities and challenges in this new ﬁeld.
3.1
Introduction
Reproducing some of the features that are commonly found in living organisms,
including—as the ultimate, and most sought after goal—the workings of the human
brain, is what “artiﬁcial intelligence” is all about [1]. However, even without aiming
for such an ambitious target, there are several tasks that living organisms perform
seamlessly and, when reproduced in electronic circuits, are of great beneﬁt and help
us solve complicated problems. The main idea of biologically-inspired electronics is
thus in borrowing approaches used by biological systems interacting with their envi-
ronment and in applying them in diverse technological areas requiring, for example,
adaptation to changing inputs, analog solution of optimization problems—one of
the well-known approaches to this problem is the “ant-search algorithm” [2, 3]—,
associative memory and unlearning, binary and fuzzy logic, etc.
M. D. Ventra ()
Department of Physics, University of California, San Diego, La Jolla, CA 92093, USA
e-mail: diventra@physics.ucsd.edu
Y. V. Pershin
Department of Physics and Astronomy and USC Nanocenter,
University of South Carolina, Columbia, SC 29208, USA
e-mail: pershin@physics.sc.edu
R. Kozma et al. (eds.), Advances in Neuromorphic Memristor Science and Applications,
15
Springer Series in Cognitive and Neural Systems,
DOI 10.1007/978-94-007-4491-2_3, © Springer Science+Business Media Dordrecht 2012

16
M. D. Ventra and Y. V. Pershin
If we look closer, it is evident that if we want to reproduce any of these tasks
with electronic circuits, two main characteristics have to be satisﬁed by some of the
circuitelementsortheircombinations. Theseelementsneedto(i)storeinformation—
they have to have memory of the past—and (ii) be dynamical—their states have
to vary in time in response to a given input, preferably in a non-linear way. The
latter requirement will help building a wide range of electronic circuits of desired
functionality.
Circuits based on active elements (such as transistors) can clearly perform both
tasks, however, at a high cost of power consumption, low density, and complexity.
It would be much more desirable if we could combine the above features in single,
passive elements, preferably with dimensions at the nanometer scale, and hence
comparable to—or even smaller than—biological storing and processing units, such
as synapses and neurons.
Such elements do exist, and go under the name of memristive, memcapacitive
and meminductive systems, or collectively simply named memelements [4]. These
are resistors, capacitors and inductors, respectively, whose state at a given time de-
pends on the history of applied inputs (e.g., charge, voltage, current, or ﬂux) and
states through which the system has evolved. As we have shown, these memele-
ments provide an unifying description of materials and systems with memory [5],
in the sense that all two-terminal electronic devices based on memory materials and
systems, when subject to time-dependent perturbations, behave simply as—or as a
combination of—memristive, memcapacitive and meminductive systems, namely,
dynamical non-linear circuit elements with memory [6].
In this Chapter, we will show how the analog memory features of memelements
are ideal to reproduce a host of processes typical of living organisms. We will review
mainly work by the authors in various contexts, ranging from learning (adaptive)
circuits to associative memory, and inherent massive parallelism afforded by net-
works of memelements. The Chapter is then organized as follows. Section 3.2 brieﬂy
reviews both the deﬁnition of these memory circuit elements and their main prop-
erties (a full account can be found in the original publications [4, 7, 8] and in our
recent review paper [6]). Several experimental realizations exemplifying memristive,
memcapacitive and meminductive systems are discussed in Sect. 3.3. Section 3.4 is
devoted to biologically-inspired circuits based on memory circuit elements includ-
ing simple adaptive circuits (Sect. 3.4.1), neuromorphic circuits (Sect. 3.4.2) and
massively-parallel analog processing circuits (Sect. 3.4.3). Concluding remarks are
given in Sect. 3.5.
3.2
Deﬁnitions and Properties
Let us consider electronic devices deﬁned by all possible pairs of fundamental circuit
variables u(t) and y(t) (i.e., current, charge, voltage, or ﬂux). For each pair, we can
introduce a response function, g, that, generally, depends on a set of n state variables,

3
Biologically-Inspired Electronics with Memory Circuit Elements
17
Fig. 3.1 Symbols of memory circuit elements: memristor, memcapacitor and meminductor. Gener-
ally, memelements are asymmetric devices. The thick horizontal lines above the bottom electrodes
are employed to deﬁne the device polarity [4]. (Reprinted with permission from Ref. [4]. © 2009
IEEE)
x = {xi}, i = 1, . . . , n, describing the internal state of the system [4]1. For instance,
resistance of certain systems may depend on spin polarization [9, 10] or temperature
[8]; capacitance and inductance of some other elements may exhibit dependence
on system geometry [6, 11] or electric polarization [12]. In general, all internal
microscopic physical properties related to the memory response of these electronic
devices should be included into the vector of internal state variables x. The resulting
memory circuit elements are then described by the following relations [4]
y(t) = g(x, u, t)u(t)
(3.1)
˙x = f (x, u, t)
(3.2)
where f is a continuous n-dimensional vector function. Equations (3.1), (3.2) have to
besuppliedbyappropriateinitialconditions[13]. Ifu isthecurrentandy isthevoltage
then Eqs. (3.1), (3.2) deﬁne memristive (for memory resistive) systems. In this case
g is the memristance (memory resistance). In memcapacitive (memory capacitive)
systems, the charge q is related to the voltage V so that g is the memcapacitance
(memory capacitance). Finally, in meminductive (memory inductive) systems the
ﬂux ϕ is related to the current I with g the meminductance (memory inductance).
There are still three additional pairs of fundamental circuit variables. However, these
do not give rise to any new devices. For example, the pairs charge–current and
voltage–ﬂux are linked through equations of electrodynamics. Moreover, we could
redeﬁne devices deﬁned by the charge–ﬂux (which is the integral of the voltage) pair
in the current–voltage basis [7]. Circuits symbols of memristive, memcapacitive and
meminductive systems are presented in Fig. 3.1.
Let us consider memristive systems in more details (deﬁnitions of all the other
elements can be easily derived by considering the different constitutive variables [4].
Speciﬁcally, a current-controlled memristive system [4, 8] is deﬁned by Eqs. (3.1),
(3.2) as
1 There is no such dependence for traditional basic circuit elements—resistors, capacitors and
inductors.

18
M. D. Ventra and Y. V. Pershin
VM(t) = R(x,I,t)I(t),
(3.3)
˙x = f(x,I,t),
(3.4)
where V M(t) and I(t) denote the voltage and current across the device, and R is the
memristance. In a simple model of an ideal memristor [7], the memristance depends
only on charge—the time integral of the current. One particular realization of such
a model has been suggested in Ref. [14] and is formulated as
R = RONx + ROFF(1 −x),
(3.5)
where RON and ROFF are minimal and maximal values of memristance, and x is a
dimensionless internal state variable bound to the region 0 ≤x ≤1. The dynamics of
x can then be simply chosen as [14]
dx
dt = αI(t),
(3.6)
where α is a constant and I(t) is the current ﬂowing through the memristor.
Another example of memristive systems (which we will make use of later in
this chapter) is a threshold-type memristive system [15]. Its model is speciﬁed by a
threshold voltage (and some other parameters) that deﬁnes different device response
regions (with regard to the voltage applied across the device). Mathematically, the
threshold-type memristive system is described by the following equations
I = x−1VM,
(3.7)
˙x = (βVM + 0.5(α −β)[|VM + Vt| −|VM −Vt|])
× θ
 x
R1
−1

θ

1 −x
R2

,
(3.8)
where I and V M are the current through and the voltage drop on the device, respec-
tively, and x is the internal state variable playing the role of memristance, R = x,
θ(·) is the step function, α and β characterize the rate of memristance change at
|VM| ≤Vt and |VM| > Vt, respectively, V t is a threshold voltage, and R1 and R2
are limiting values of the memristance R. In Eq. (3.8), the θ-functions symbolically
show that the memristance can change only between R1 and R2. On a practical level,
the value of x must be monitored at each time step and in the situations when x < R1 or
x > R2, it must be set equal to R1 or R2, respectively. In this way, we avoid situations
when x may overshoot the limiting values by some amount and thus not change any
longer because of the step function in Eq. (3.8). We have introduced and employed
this model to describe the learning properties of unicellular organisms, associative
memory, etc., as we will describe in some detail in the following sections.
There are several properties that characterize memory circuit elements. We refer
the reader to the original papers [4, 7, 8] and the extensive review [6] where these are
discussed at length. Here, we just mention that they are typically characterized by a

3
Biologically-Inspired Electronics with Memory Circuit Elements
19
frequency-dependent “pinched hysteresis loop” in their constitutive variables when
subject to a periodic input. Also, normally, the memristance, memcapacitance and
meminductance acquire values between two limits (with exceptions as discussed in
Refs. [6, 12, 16]). Although the hysteresis of these elements under a periodic input
may strongly depend on initial conditions [13], it is generally more pronounced
at frequencies of the external input that are comparable to frequencies of internal
processes that lead to memory. In many cases, at very low frequencies memory
circuit elements behave as non-linear elements while at high frequencies as linear
elements.
Also, the hysteresis loops may or may not show self-crossing—which we have
named type-I and type-II crossing behavior, respectively [6]—and often the internal
state variable remains unchanged for a long time without any input signal applied.
This provides non-volatile memory, which is an important feature for some of the
applications we discuss later.
Finally, we mention that the state variables—whether from a continuum or a
discrete set of states—may follow a stochastic differential equation rather than a
deterministic one [6]. Interesting effects have been predicted in the presence of noise,
such as noise-induced hysteresis [17]. This may have a large bearing in simulating
biological processes—which necessarily occur under noisy conditions—and could
be used to enhance the performance of certain devices.
3.3
Experimental Realizations
In this section, we brieﬂy discuss experimental realizations of memory circuit
elements. There is a large amount of experimental systems showing memristive
behavior (based, however, on very different physical mechanisms). For example, in
thermistors—being among the ﬁrst identiﬁed memristive systems [8]—the memory
effects are related to thermal effects, mainly on how fast the heat dissipation occurs.
In spintronics memristive systems, either based on semiconductor [9] or metal [10]
materials, the memory feature is provided by the electron spin degree of freedom.
Finally, resistance switching memory cells are probably the most important type of
memristive systems. These cells are normally built of two metal electrodes separated
by a gap of a few tens of nanometers ﬁlled by a memristive material. Different phys-
ical processes inside the memristive material can be responsible for the memory.
Figure 3.2 shows an example of resistance switching memory cell—an electrochem-
ical metallization cell—in which a layer of dielectric material (SiO2) separates two
dissimilar metal electrodes (made of copper and platinum) [18]. Externally applied
voltage induces migration of copper atoms that can bridge the gap between the elec-
trodes thus reducing the cell’s resistance. Such a bridge can also be disrupted by the
applied voltage of the opposite polarity (see Fig. 3.2).
Memcapacitive effects can be related to changes in the capacitor geometry or spe-
ciﬁcintrinsicpropertiesofdielectricmediumlocatedbetweenthecapacitorplates[6].
For instance, the former mechanism plays the main role in elastic memcapacitive

20
M. D. Ventra and Y. V. Pershin
Fig. 3.2 Current-voltage
characteristic of a Cu/SiO2/Pt
electrochemical metallization
cell recorded using a
triangular voltage sweep. The
insets show dynamics of
metallic ﬁlament formation.
(Reprinted with permission
from [18]. © 2009 American
Institute of Physics)
systems that could be based on internal elastic states of the capacitor plate (e.g.,
direction of bending [11]) or elasticity of a medium between the plates that could
be thought of as a spring [6]. Examples of the latter mechanism include memca-
pacitive systems with a delayed polarization response [12, 16] and structures with
permittivity switching [19]. Although meminductive systems are the least studied
memory circuit elements at the moment, there are several known systems showing
such type of functionality. For instance, in bimorph meminductive systems [20–22],
the inductance depends on the inductor’s shape deﬁned by the inductor temperature.
Heat dissipation mechanisms play a signiﬁcant role in this type of systems. Many
additional examples of memristive, memcapacitive and meminductive systems can
be found in our recent review paper [6]. The most important aspect of all these ex-
amples is that they relate primarily to structures with nanoscale dimensions. This is
not surprising since (up to a certain limit) the smaller the dimensions of the system
the easier it is to observe memory effects.
In addition, we would like to mention that several types of memory effects may be
present in a single device. For example, the coexistence of memristive and memca-
pacitive responses has been observed in perovskite oxide thin ﬁlms [23]. Moreover,
several three-terminal transistor-like memristive devices have been investigated in
the past [24–26]. Clearly, different types of memory materials can be combined to
obtain multi-terminal device structures with complex functionalities.

3
Biologically-Inspired Electronics with Memory Circuit Elements
21
3.4
Biologically-Inspired Circuits
3.4.1
Modeling Adaptive Behavior of Unicellular Organisms
Adaptive behavior is common to all life forms on Earth of all ﬁve kingdoms of nature:
in Plantae (the plants) [27–29], Animalia (the animals) [30], Protista (the single-
celled eukaryotes) [31, 32], Fungi (fungus and related organisms) [33, 34], and
Monera (the prokaryotes) [35–37]. (The literature on this subject is extensive, hence
we have given only a few representative references.) To a greater or lesser extent,
representatives of all life forms respond (adapt) to changes of their environment in
a manner that increases the survival of their species. It would thus be of beneﬁt to
mimic and use this important natural feature in artiﬁcial structures, in particular in
electronics to allow novel functionalities otherwise nonexistent in standard circuitry.
There are indeed many domains where such learning circuits can be employed. These
range from robot control systems to signal processing. Therefore, developing circuit
models of the adaptive behavior of the natural world is of great importance in many
scientiﬁc areas [38]. Note that by “learning” here we simply mean the ability to adapt
to incoming signals with retention of such information for later use.
In this section we consider a particularly interesting example: the ability of the
slime mold Physarum polycephalum to adapt its locomotion to periodic changes of
the environment [32]. The simplicity of the system—a unicellular organism—and
its well-deﬁned response to speciﬁc input signals, make it an ideal test bed for the
application of the notion of memory circuit elements in biology, and a source of
inspiration for more complex adaptive behavior in living organisms. In addition, this
is a particularly appealing example of the full range of properties of memelements, in
particular, their analog capability, which expands their range far beyond the digital
domain.
In particular, it has been shown in a recent experiment [32] that the Physarum
polycephalum subjected to periodic unfavorable environmental conditions (lower
temperature and humidity) not only responds to these conditions by reducing its lo-
comotion speed, but also anticipates future unfavorable environmental conditions
reducing the speed at the time when the next unfavorable episode would have
occurred. While the microscopic mechanism of such behavior has not been identiﬁed
by the authors of that work [32], their experimental measurements clearly prove the
ability of Physarum polycephalum to anticipate an impending environmental change.
More speciﬁcally, the locomotion speed of the Physarum polycephalum was
measured when favorable environmental conditions (26 ◦C and 90 % humidity)
were interrupted by three equally spaced 10 min pulses of unfavorable environmen-
tal conditions (22 ◦C and 60 % humidity) [32]. The time separation between the
pulses τ was selected between 30 and 90 min. It was observed that the locomotion
speed at favorable conditions (approximately 0.25 mm/10 min as shown in Fig. 3.1 of
Ref. [32]) turns to close to zero each time the unfavorable conditions were presented.
However, spontaneous in-phase slow downs were observed after time intervals τ, 2τ
and even 3τ after the last application of unfavorable conditions. In addition, if—after

22
M. D. Ventra and Y. V. Pershin
Fig. 3.3 a A possible
realization of the learning
circuit consisting of resistor
R, inductor L, capacitor C and
memristive system M.
b Schematics of the function
f (V) deﬁning a threshold-type
memristive system (see also
Eqs. (3.7), (3.8). (Reprinted
with permission from
Ref. [15]. © 2009 American
Physical Society)
a
b
a long period of favorable conditions—a single pulse of unfavorable conditions is
applied again then a spontaneous slow down (called a spontaneous in-phase slow
down after one disappearance [32]) after a time interval τ was observed. It clearly
follows form this experiment that the Physarum polycephalum has a mechanism to
memorize (“learn”) the periodicity of environmental changes and adapts its behavior
in anticipation of next changes to come [32].
We have developed a circuit model [15] of the adaptive behavior of the slime
mold which has been later realized experimentally [15] using vanadium dioxide as
memory element [40, 41]. The learning circuit is shown in Fig. 3.3a. Here, the role
of environmental conditions is played by the input voltage V(t) and the speed of
locomotion is mapped into the voltage across the capacitor C. The learning circuit
design resembles a damped LC contour in which the amount of damping is controlled
by the state of the memristive system M. To understand the circuit operation, we note
thatthememristivesystememployedinthecircuitisofathresholdtype(seeEqs. (3.7)
and (3.8)), namely, its state can be signiﬁcantly changed only by a voltage (across
the memristive system) with a magnitude exceeding a certain threshold. Figure 3.3b
presents the switching function f (V) used to describe a threshold-type memristive
system.
Our simulations of the learning circuit response to irregular and regular sequences
of pulses are shown in Fig. 3.4. In these simulations, the scheme described above has
been used with the only restriction that the response signal cannot exceed a certain
value [15] (electronically, a cut-off can be easily obtained by using an additional
resistor and diode). When an irregular sequence of pulses is applied to the circuit,
the voltage oscillations across the capacitor can not exceed the threshold voltage of
the memristive system M which continues to stay in its initial low-resistance state,
thus damping the circuit. When the pulses are applied periodically with a period
close to the period of the LC contour oscillations, a sufﬁciently strong voltage across
the capacitor C is induced. This voltage switches the memristive system into the
high-resistance state. Therefore, in this case, oscillations in the contour are less
damped and last longer as Fig. 3.4 demonstrates. These oscillations exactly model
the spontaneous in-phase slow down and in-phase slow down after one disappearance
effectsobservedexperimentally[32].Wenotethatasinglelearningcircuitmemorizes

3
Biologically-Inspired Electronics with Memory Circuit Elements
23
Fig. 3.4 Modeling of the spontaneous in-phase slow-down responses [15]. This plot demonstrates
thatstrongerandlonger-lastingresponsesforbothspontaneousin-phaseslowdownandspontaneous
in-phase slow down after one disappearance of the stimulus are observed only when the circuit was
previously trained by a periodic sequence of three equally spaced pulses as present in V 2(t). The
applied voltage V 1(t) is irregular and thus the three ﬁrst pulses do not “train” the circuit. (Reprinted
with permission from Ref. [15]. © 2009 American Physical Society)
past events of a frequency close to the resonance frequency of LC contour.An array of
learning circuits would model the learning of Physarum polycephalum in the whole
frequency range [15].
Recently, an experimental implementation of this learning circuit has been re-
ported [39]. In this work, a learning circuit similar to that in Fig. 3.3a has been built
with the only difference that the memristive system (a vanadium dioxide memristive
device [39]) has been connected in series with a capacitor (Fig. 3.5a). The memristive
properties of vanadium dioxide are based on an insulator-to-metal phase transition
occurring in this material in the vicinity of 342 K [40, 41]. In order to realize the
memristive functionality, the vanadium dioxide device is heated to a steady-state
temperature of 339.80 K (right below the transition temperature) and subjected to an
externally applied voltage. The Joule heating (due to the applied voltage) incremen-
tally drives the vanadium dioxide material through the phase transition, thus reducing
its resistance. The operation of the learning circuit depicted in Fig. 3.5a is then clear.
While off-resonance signals applied to the circuit can not excite a sufﬁcient current
to drive the vanadium dioxide through the phase transition, the current generated by
resonance signals is sufﬁcient for this purpose. Figure 3.5b demonstrates modiﬁca-
tion of the transfer function of the circuit caused by off-resonance and resonance
pulse sequences (Fig. 3.5c) applied to the circuit. Figure 3.5b clearly indicates a
change in the transfer function caused by resonance signals (learning).
Finally, we mention that the formalism of memory circuit elements [4] has also
been useful in modeling biophysical systems whose electric response depends on
the history of applied voltages or currents. An example of such situation is the
electro-osmosis in skin which has been recently described by a memristive model

24
M. D. Ventra and Y. V. Pershin
Fig. 3.5 Experimental realization of the learning circuit (adaptive ﬁlter) based on vanadium dioxide
memristive device. a Schematic of the adaptive ﬁlter in which the memristive device (with a small
memcapacitive component) is connected in series with a capacitor C and inductor L. We note that
such realization of the learning circuit operates similarly to the learning circuit shown in Fig. 3.3a.
b Small-signal (10 mV) transfer function (V out/V in) for the adaptive ﬁlter plotted before and after
off-resonance “A” and on-resonance “B” pulses. Solid lines are RLC band pass-ﬁlter ﬁt to data,
which generates the ω0 and Q values in the legend. Pulse sequence B has a signiﬁcant training
effect on the circuit, while A has little or no effect. c Time series of the off-resonance “A” sequence
of pulses and on-resonance “B” sequence of pulses. (Reprinted with permission from Ref. [39].
© 2010 American Institute of Physics)
[42]. Physically, the voltage applied to the skin induces a water ﬂow in sweat-duct
capillaries changing the skin conductance. The position of the water table (the level
separating dry and wet zones) in capillaries plays the role of the internal state variable
whose dynamics is determined by the applied voltage [42]. The memristive model
of electro-osmosis [42] is in a good agreement with experimental data and further
demonstrates the potential of the formalism of memory circuit elements for modeling
biophysical processes.
3.4.2
Neuromorphic Circuits
Our second example of biologically-inspired circuits with memory circuit elements
is from the area of neural networks. Neural networks form a class of circuits whose
operation mimics the operation of the human (and animal) brain. Below, we consider
electronic implementations of two important processes occurring in biological neural
networks: associative memory and spike timing-dependent plasticity. Both features
can be implemented in artiﬁcial neural networks based on memristive synapses.

3
Biologically-Inspired Electronics with Memory Circuit Elements
25
Fig. 3.6 Memristive neural network with an associative memory ability. Here, two input neurons
(N1 and N2) are connected through memristive synapses (S1 and S2) to the output neuron N3. The
details of circuit operation are given in the text. (Reprinted from Ref. [44]. © 2010 with permission
from Elsevier)
3.4.2.1
Associative Memory
The associative memory is one of the most fundamental functionalities of the human
(and animal) brain. By making associations we learn, adapt to a changing environ-
ment and better retain and recall events. One of the most famous experiments related
to associative memory is Pavlov’s experiment [43] whereby salivation of a dog’s
mouth is ﬁrst set by the sight of food. Then, if the sight of food is accompanied
by a sound (e.g., the tone of a bell) over a certain period of time, the dog learns to
associate the sound to the food, and salivation can be triggered by the sound alone,
without the intervention of vision.
Recently, we have reproduced [44] the Pavlov’s experiment utilizing a neural
network with memristive synapses. As a ﬁrst example, we have implemented the
well known Hebbian rule introduced by Hebb in 1949: “when an axon of cell A is
near enough to excite a cell B and repeatedly or persistently takes part in ﬁring it,
some growth process or metabolic change takes place in one or both cells such that
A’s efﬁciency, as one of the cells ﬁring B, is increased” [45]. To put it differently, the
neurons that ﬁre together, wire together.
In order to show associative memory, let us consider a simple neural network
consisting of three electronic neurons and two memristive synapses as shown in
Fig. 3.6. We assume that the ﬁrst input neuron activates under a speciﬁc (“visual”)
event, such as “sight of food”, and the second input neuron activates under another
(“auditory”) event, such as a particular “sound”.
On the electronic level, an electronic neuron sends forward (to its output) and
backward (to its input) voltage spikes of opposite polarity when the amplitude of
the input signal exceeds a threshold value. Regarding the dynamics of memristive
synapses, they have been selected of a threshold-type (Eqs. (3.7) and (3.8)) with a
threshold voltage exceeding the output voltage of electronic neurons. In this case,
voltage spikes applied to a single terminal of a memristive synapse is not enough to
induce its change. The latter is possible only if forward and backward propagating
spikes overlap in time across a synapse. We have employed memristor emulators

26
M. D. Ventra and Y. V. Pershin
Fig. 3.7 Experimental demonstration of the associative memory with memristive neural networks.
In this experiment, a simple neural network shown in Fig. 3.6 was realized. The ﬁrst “probing”
phase demonstrates that, initially, only a signal from N1 neuron activates the output neuron. The
association of the Input 2 signal with the Output develops in the “learning phase” when N1 and N2
neurons are simultaneously activated. In this case, a signal at the Input 1 excites the third neuron that
sends back-propagating pulses of a negative amplitude. These pulses, when applied simultaneously
with forward propagating pulses from the Input 2 to the second memristive synapse S2 cause it to
learn. The ﬁnal “probing” phase demonstrates that signals from both N1 and N2 activate the output
neuron. A detailed description of the experiment is given in Ref. [44]. (Reprinted from Ref. [44].
© 2010 with permission from Elsevier)
[44, 46] as memristive synapses2. The main components of a memristor emulator
are a digital potentiometer, a microcontroller and an analog-to-digital converter.
Using the converter, the microcontroller continuously reads the voltage applied to
the digital potentiometer and updates the potentiometer resistance according to a
pre-programmed model of a voltage- or current-controlled memristive system. The
operation of electronic neurons is realized along similar lines [44]. Operation of the
associative memory is presented in Fig. 3.7 where a detail of this process is given.
Our work as described above [44] demonstrates the potential of memristive de-
vices for neuromorphic circuits applications. Importantly, it has been recently shown
in numerous experiments that memristive devices can be built at the nanoscale [6,
14, 18, 52–59, 60, 61, 62]. This opens up the possibility to fabricate neuromorphic
circuits with the same amount of memristive synapses as the number of biological
synapses in the human brain (∼1014). In fact, one of the main challenges for prac-
tical realizations of an artiﬁcial brain on a chip is related to the high connectivity of
biological neurons. It has been estimated that, on average, the number of connections
per neuron is of the order of 103. Therefore, neural networks of memelements built
2 Several designs of memristor [7, 46–48] as well as memcapacitor and meminductor [47, 49–51]
emulators are known in the literature. These emulators serve as an important practical tool to build
small-scale circuits with memory circuit elements.

3
Biologically-Inspired Electronics with Memory Circuit Elements
27
at the nanoscale offer advantages—in terms of density—unavailable with current
active elements (such as transistors).
3.4.2.2
Spike Timing-Dependent Plasticity
However, the above mentioned simple Hebbian rule does not describe the much
more complicated time-dependent plasticity of biological synapses [63–66]. The
latter has come to be known as spike timing-dependent plasticity (STDP). When
a post-synaptic signal reaches the synapse before the action potential of the pre-
synaptic neuron, the synapse shows long-term depression (LTD), namely its strength
decreases (smaller connection between the neurons) depending on the time difference
between the post-synaptic and the pre-synaptic signals. Conversely, when the post-
synaptic action potential reaches the synapse after the pre-synaptic action potential,
thesynapseundergoesalong-timepotentiation(LTP),namelythesignaltransmission
between the two neurons increases in proportion to the time difference between the
pre-synaptic and the post-synaptic signals. The learning process and the storing of
information in the brain thus follow non-trivial time-dependent features which have
not been fully understood yet. Implementation of STDP in artiﬁcial networks can
thus help unraveling these mechanisms.
The spike timing-dependent plasticity can be implemented using different types
of memristive systems. Following our previous work [67], neuromorphic circuits can
be based on memristive systems with or without an internal spike-timing tracking
capability. In the most simple case, memristive systems without spike-timing track-
ing capability are of the ﬁrst order, while those supporting such capability are of the
second order as an additional internal state variable is needed to track timing of pre-
synaptic and post-synaptic pulses [67]. Currently, an additional external hardware
is used to implement the spike timing-dependent plasticity. For example, STDP was
recently realized using a combination of memristive systems with CMOS (comple-
mentary metal-oxide-semiconductor) elements [60] (see Fig. 3.8). Another approach
involves utilization of overlapping pulses of opposite polarities [67–69]. Second
or higher order memristive systems with an internal timing tracking would allow to
avoid additional hardware complications.
A simple second-order memristive system with time-tracking capability is
described by the following equations [67]
R = x,
(3.9)
˙x = γ [θ(VM −Vt)θ(y −yt) + θ(−VM −Vt)θ(−y −yt)]y,
(3.10)
˙y = 1
τ [−VMθ(VM −Vt)θ(yt −y) −VMθ(−VM −Vt)θ(y + yt) −y],
(3.11)
where x and y are internal state variables, γ is a constant, V t is a threshold voltage, yt
is the threshold value of y, and τ is a constant deﬁning the time window of STDP. The
second-order memristive system with timing tracking capability deﬁned by the above

28
M. D. Ventra and Y. V. Pershin
Fig. 3.8 a Measured change
in the synaptic weight versus
spike separation. Inset: SEM
image of the memristive
crossbar array, scale bar is
300 nm. b Measured change
in excitatory postsynaptic
current of rat neurons after
repetitive correlated spiking
versus relative spiking timing
(the plot was reconstructed
from Ref. [65]). Inset: image
of a hippocampal neuron (the
image was adapted with
permission from reference
[70]). Scale bar is 50 μm.
(Reprinted with permission
from [60]. © 2010 American
Chemical Society)
equations is very promising for neuromorphic circuits application since neuron’s
ﬁring can be implemented simply by short single rectangular pulses and no additional
hardware as in the case of ﬁrst-order memristors (see, e.g., Ref. [60]). However,
such solid-state second-order memristors need to be developed, even though their
implementation in memristor emulators [46, 47] can be easily realized.
Moreover, severalauthorshavediscussedapplicationsofthree-terminaltransistor-
like electronic devices with memory [25, 26, 71] in the area of neuromorphic
computing. Although, formally such devices can not be categorized as memristive
systems, their operation is clearly based on memristive effects. In particular, Lai
et al. [26] have experimentally fabricated a synaptic transistor. For instance, Fig. 3.9
depicts their experimental scheme and selected measurement results that conﬁrm
realization of spike timing-dependent plasticity in this device.

3
Biologically-Inspired Electronics with Memory Circuit Elements
29
Fig. 3.9 a Structure of synaptic transistor. b The relative changes of the postsynaptic currents
measured after application of 120 pairs of temporally correlated pre- and post-synaptic spikes.
(From [26], © Wiley-VCH Verlag GmbH & Co. KGaA. Reproduced with permission)

30
M. D. Ventra and Y. V. Pershin
3.4.3
Networks of Memory Circuit Elements
A human brain—and also the brain of many other living organisms—solves many
problems much better than traditional computers. The reason for this is a type of
massive parallelism in which a large number of neurons and synapses participate
simultaneously in the calculation. Here, we consider networks of memory circuit
elements and their ability to (i) solve efﬁciently certain graph theory optimization
problems, and (ii) retain such information for later use. In particular, we demonstrate
that a network of memristive devices solves the maze problem much faster than
any existing algorithm [72]. Similar to the brain’s operation, such an extraordinary
advance in computational power is due to the massively-parallel network dynamics
in which all network components are simultaneously involved in the calculation. This
type of parallelism could be dubbed as an analog parallelism which is very different
from that used in conventional supercomputers. In the latter systems, each core
typically runs a separate process that, relatively rarely, exchanges information with
othercores. Incalculationsdonebynetworks, theinformationexchangeiscontinuous
resulting in a tremendous increase of computational power as we demonstrate below.
Left panel of Fig. 3.10 depicts a memristive network (memristive processor) in
which points of a square grid are connected by basic units (memristive system plus
switch (FET)) [72]. Each switch in the network can be in the “connected” or “not-
connected” state. Since the direction of current ﬂow in the network is not known a
priori, the polarity of adjacent memristive devices (indicated by the black thick line
in the memristor symbol in Fig. 3.1) is chosen to be alternating. Experimentally, the
suggested network could be fabricated using, e.g., CMOL (Cmos + MOLecular-scale
devices) architecture [73] combining a single memristor layer with a conventional
CMOS layer. The operation of the massively-parallel processor consists of three
main stages: initialization, computation and reading out the computation result. All
these stages are realized by externally applied signals (originating, e.g., from the
CMOS layer).
During the ﬁrst initialization stage, all memristive elements in the network are
switched into the “OFF” state. This can be done, for example, by applying GND and
appropriately selectedV 1 voltages in a chessboard-like pattern to all grid points of the
memristive network for a sufﬁciently long period of time [72]. After that, the maze
topology is mapped onto the memristive network by setting appropriate switches into
the “not connected” state. We describe this process in the caption of Fig. 3.10. The
computation stage consists in the application of a single voltage pulse of appropriate
amplitude and duration across grid points corresponding to the entrance and exit
points of the maze. The solution can be later read or used in further calculations.
We have modeled the memristive processor operation by numerically solving
Kirchhoff’s current law equations complemented by Eqs. (3.5), (3.6) which in the
present network case are modiﬁed as
RM
ij = RONxij + ROFF(1 −xij),
(3.12)
where RON and ROFF are again the minimal and maximal values of memristance, xij
is the dimensionless internal state variable for each memristor bound to the region

3
Biologically-Inspired Electronics with Memory Circuit Elements
31
Fig. 3.10 Maze mapping into a network of memristors. Right panel. The maze is covered by an array
of vertical and horizontal lines having the periodicity of the maze. Left panel. Architecture of the
network of memristors in which each crossing between vertical and horizontal lines in the array (in
the right panel) is represented by a grid point to which several basic units consisting of memristors
and switches (ﬁeld-effect transistors) are connected in series. The maze topology is encoded into
the state of the switches such that if the short line segment connecting neighboring crossing points
in the array crosses the maze wall then the state of the corresponding switch is “not connected”
(shown with red symbols). All other switches are in the “connected” state. The external voltage (V)
is applied across the connection points corresponding to the entrance (V) and exit (ground, GND)
points of the maze. (Reprinted with permission from Ref. [72]. © 2011 American Physical Society)
0≤xij ≤1, and (i, j) are grid indexes of a memristor to identify its location in the
network. The dynamics of xij is then given by
dxij
dt
= αIij(t),
(3.13)
with α a constant and Iij(t) the current ﬂowing through the memristor (ij).
Figure3.11ashowsasolutionofamultiple-pathmaze. Themazesolutionisclearly
seen in Fig. 3.11 as chains of red, blue and green boxes (representing memristive
devices with lower memristance) connected by a red line. Importantly, the memristive
processor not only determines all possible solutions of the maze but also stores them
and sorts them out according to their length. This feature is described in more details
in the caption of Fig. 3.11. Also, the memristive processor requires only one single
step to ﬁnd the maze solution thus outperforming all known maze solving approaches
and algorithms.
We also note that the wide selection of physical mechanisms of memory we
can “shop” from, offers many opportunities to design novel efﬁcient electronic de-
vices [6]. For example, a memristive processor based on fast switching nanoionic

32
M. D. Ventra and Y. V. Pershin
Fig. 3.11 Solution of a multiple-path maze [72]. The maze solution contains two common segments
(red dots connected by a red line), and two alternative segments of different lengths close to the left
bottom corner. The memristance in the shorter segment (blue dots connected by a red line) is smaller
than that in the longer segment (green dots connected by a red line) since the current through the
shorter segment is larger and, consequently, the change of the memristors’ state along this segment
is larger. The arrow at the bottom indicates a splitting point of the solution path. The resistance is
in Ohms, the voltage is in Volts and the current is in Amperes. (Reprinted with permission from
Ref. [72]. © 2011 American Physical Society)
metal/insulator/metal cells [6] would require just few nanoseconds or even less3 to
solve the maze. More generally, a network of memristors—or other memory cir-
cuit elements—can be considered as an adaptable medium whose state dynamically
changes in response to time-dependent signals or changes in the network conﬁgu-
ration. Therefore, the use of these processors is not limited to maze solving: We
expect they could help ﬁnd the solution of many graph theory optimization problems
including, for instance, the traveling salesman problem, shortest path problem, etc.
3 Fast sub-nanosecond switching has been recently reported in tantalum oxide memristive systems
[62].

3
Biologically-Inspired Electronics with Memory Circuit Elements
33
3.5
Conclusions and Outlook
In conclusion, we have shown that the two-terminal electronic devices with
memory—memristive, memcapacitive and meminductive systems—are very useful
to model a variety of biological processes and systems. The electronic implemen-
tation of all these mechanisms can clearly lead to a novel generation of “smart”
electronic circuits that can ﬁnd useful applications in diverse areas of science and
technology. In addition, these memelements and their networks, provide solid ground
to test various hypothesis and ideas regarding the functioning of the human (and
animal) brain both theoretically and experimentally. Theoretically because their ﬂex-
ibility in terms of what type and how many internal state variables responsible for
memory, or what network topology are required to reproduce certain biological
functions can lead to a better understanding of the microscopic mechanisms that are
responsible for such features in living organisms. Experimentally because with the
continuing miniaturization of electronic devices, memelements can be assembled
into networks with similar densities as the biological systems (e.g., the brain) they
are designed to emulate. In particular, we anticipate potential applications for mem-
capacitive and meminductive systems [4] which offer such an important property
as low energy dissipation combined with information storage capabilities. We are
thus conﬁdent that the area of biologically-inspired electronics with memory cir-
cuit elements will offer many research opportunities in several ﬁelds of science and
technology.
Acknowledgments M.D. acknowledges partial support from the NSF Grant No. DMR-0802830.
References
1. Russell SJ, Norvig P (2009) Artiﬁcial intelligence: a modern approach, 3rd edn. Prentice Hall,
USA
2. Colorni A, Dorigo MM, Maniezzo V (1991) In Actes de la premire confrence europenne sur
la vie artiﬁcielle. Elsevier, Paris, pp 134–142
3. Monmarché N, Guinand F, Siarry P (2010) Artiﬁcial ants. Wiley-ISTE, Hoboken, p 576
4. Di Ventra M, Pershin YV, Chua LO (2009) Circuit elements with memory: memristors,
memcapacitors, and meminductors. Proc IEEE 97(10):1717–1724
5. Di Ventra M, PershinYV (2010) Memory materials: a unifying description. Mat Today 14:584
6. PershinYV, Di Ventra M (2011) Memory effects in complex materials and nanoscale systems.
Adv Phys 60:145–227
7. Chua LO (1971) Memristor—the missing circuit element. IEEE Trans Circuit Theory 18:507–
519
8. Chua LO, Kang SM (1976) Memristive devices and systems. Proc IEEE 64:209–223
9. Pershin YV, Di Ventra M (2008) Spin memristive systems: spin memory effects in semicon-
ductor spintronics. Phys Rev B 78:113309-1–113309-4
10. Wang X, Chen Y, Xi H, Li H, Dimitrov D (2009) Spintronic memristor through spin-torque-
induced magnetization motion. IEEE Electron Dev Lett 30:294–297
11. Martinez-Rincon J, Pershin YV (2011) Bistable non-volatile elastic membrane memcapacitor
exhibiting chaotic behavior. IEEE Trans Electron Dev 58:1809

34
M. D. Ventra and Y. V. Pershin
12. Martinez-Rincon J, Di Ventra M, Pershin YV (2010) Solid-state memcapacitive system with
negative and diverging capacitance. Phys Rev B 81:195430
13. Corinto F, Ascoli A, Gilli M (2011) Analysis of current-voltage characteristics for memristive
elements in pattern recognition systems (submitted for publication)
14. Strukov DB, Snider GS, Stewart DR,Williams RS (2008)The missing memristor found. Nature
453:80
15. Pershin YV, La Fontaine S, Di Ventra M (2009) Memristive model of amoeba learning. Phys
Rev E 80:021926
16. Krems M, Pershin YV, Di Ventra M (2010) Ionic memcapacitive effects in nanopores. Nano
Lett 10:2674
17. Stotland A, Di Ventra M (2011) Stochastic memory:
getting memory out of noise.
arXiv:1104.4485v2
18. Schindler C, Staikov G,Waser R (2009) Electrode kinetics of CuSiO2-based resistive switching
cells: Overcoming the voltage-time dilemma of electrochemical metallization memories. Appl
Phys Lett 94:072109
19. Lai Q, Zhang L, Li Z, Stickle WF, Williams RS, Chen Y (2009) Analog memory capacitor
based on ﬁeld-conﬁgurable ion-doped polymers. Appl Phys Lett 95:213503
20. Lubecke V, Barber B, Chan E, Lopez D, Gross M, Gammel P (2001) Self-assembling MEMS
variable and ﬁxed RF inductors. IEEE Trans Microw Theory Tech 49(11):2093
21. Zine-El-Abidine I, Okoniewski M, McRory JG (2004) In Proceedings of the 2004 international
conference on MEMS, NANO and smart systems (ICMENS’04), pp 636–638
22. Chang S, Sivoththaman S (2006) A tunable RF MEMS inductor on silicon incorporating an
amorphous silicon bimorph in a low-temperature process. IEEE El Dev Lett 27(11):905
23. Liu S, Wu N, Ignatiev A, Li J (2006) Electric-pulse-induced capacitance change effect in
perovskite oxide thin ﬁlms. J Appl Phys 100:056101
24. Erokhin VV, Berzina TS, Fontana MP (2007) Polymeric elements for adaptive networks.
Crystallogr Rep 52:159
25. Alibart F, Pleutin S, Guerin D, Novembre C, Lenfant S, Lmimouni K, Gamrat C, Vuillaume D
(2010) An organic nanoparticle transistor behaving as a biological spiking synapse. Adv Funct
Mater 20:330
26. Lai Q, Zhang L, Li L, StickleWF,Williams RS, ChenY (2010) Ionic/electronic hybrid materials
integrated in a synaptic transistor with signal processing and learning functions. Adv Mater
22:2448
27. Kozlowski T, Pallardy S (2002) Acclimation and adaptive responses of woody plants to
environmental stresses. Bot Rev 68:270
28. Trewavas A (1998) Aspects of plant intelligence. Ann Bot 92:1–20
29. Calvo Garzon P, Keijzer F (2011) Plants: Adaptive behavior, root-brains, and minimal
cognition. Adap Behav 19:155
30. Staddon JER (1983) Adaptive behavior and learning. Cambridge University Press, New York,
pp 299–316
31. Ojal J (1998) Adaptive behavior with protozoa-inspired dynamic. Biol Cybern 79:403
32. Saigusa T, Tero A, Nakagaki T, Kuramoto Y (2008) Amoebae anticipate periodic events. Phys
Rev Lett 100:018101
33. Li Q, McNeil B, Harvey LM (2008) Adaptive response to oxidative stress in the ﬁlamentous
fungus Aspergillus niger B1-D. Free Radic Biol Med 44:394
34. Hartley J, Cairney JW, MehargAA (1997) Do ectomycorrhizal fungi exhibit adaptive tolerance
to potentially toxic metals in the environment? Plant Soil 189(2):303–319
35. Thieringer H, Jones P, Inouye M (1998) Cold shock and adaptation. Bio Essays 20:49
36. Van Der Oost J, Jore MM, Westra ER, Lundgren M, Brouns SJ (2009) CRISPR-based adaptive
and heritable immunity in prokaryotes. Trends Biochem Sci 34(8):401–407
37. Larsson J, Nylander AA, Bergman B (2011) Genome ﬂuctuations in cyanobacteria reﬂect
evolutionary, developmental and adaptive traits. BMC Evol Biol 11:187
38. Holland JH (1992) Adaptation in natural and artiﬁcial systems: an introductory analysis with
applications to biology, control, and artiﬁcial intelligence. Bradford Book, Mass

3
Biologically-Inspired Electronics with Memory Circuit Elements
35
39. Driscoll T, Quinn J, Klein S, Kim HT, Kim BJ, Pershin YV, Di Ventra M, Basov DN (2010)
Memristive adaptive ﬁlters. Appl Phys Lett 97:093502
40. Driscoll T, Kim HT, Chae BG, Di Ventra M, Basov DN (2009) Phase-transition driven
memristive system. Appl Phys Lett 95:043503
41. Driscoll T, Kim HT, Chae BG, Kim BJ, LeeYW, Jokerst NM, Palit S, Smith DR, Di Ventra M,
Basov DN (2009) Memory metamaterials. Science 325:1518
42. Johnsen GK, Lütken CA, Martinsen OG, Grimnes S (2011) Memristive model of electro-
osmosis in skin. Phys Rev E 83:031916
43. Pavlov I (1927) Conditioned reﬂexes: an investigation of the physiological activity of the
cerebral cortex (trans: Anrep GV). Oxford University Press, London
44. Pershin YV, Di Ventra M (2010) Experimental demonstration of associative memory with
memristive neural networks. Neural Networks 23:881
45. Hebb DO (1949) The organization of behavior; a neuropsychological theory. Wiley, NewYork
46. Pershin YV, Di Ventra M (2010) Practical approach to programmable analog circuits with
memristors. IEEE Trans Circ Syst I 57:1857
47. Biolek D, Biolkova V (2011) Hybrid modelling and emulation of mem-systems. Int J Numer
Mod (in press)
48. Biolek D, Biolkova V, Kolka Z (2011) In 5th international conference on circuits, systems and
signals (CSS’11), p 171
49. Biolek D, Biolkova V (2010) Mutator for transforming memristor into memcapacitor. Electron
Lett 46:1428
50. PershinYV, DiVentra M (2011) Emulation of ﬂoating memcapacitors and meminductors using
current conveyors. Electron Lett 47:243
51. Pershin YV, Di Ventra M (2010) Memristive circuits simulate memcapacitors and meminduc-
tors. Electron Lett 46:517
52. Tamura T, Hasegawa T, Terabe K, Nakayama T, Sakamoto T, Sunamura H, Kawaura H, Hosaka
S, Aono M (2006) Switching property of atomic switch controlled by solid electrochemical
reaction. Jpn J Appl Phys 45:L364
53. Waser RR, Aono M (2007) Nanoionics-based resistive switching memories. Nat Mater 6:833
54. Lee D, Seong D, Jo I, Xiang F, Dong R, Oh S, Hwang H (2007) Resistance switching of copper
doped MoOx ﬁlms for nonvolatile memory applications. Appl Phys Lett 90:122104
55. Dietrich S, Angerbauer M, Ivanov M, Gogl D, Hoenigschmid H, Kund M, Liaw C, Markert
M (2007) A nonvolatile 2-Mbit CBRAM memory core featuring advanced read and program
control. IEEE J Solid-State Circ 42:839
56. Raoux S, Burr GW, Breitwisch MJ, Rettner CT, Chen YC, Shelby RM, Salinga M, Krebs
D, Chen SH, Lung HL, Lam CH (2008) Phase-change random access memory: a scalable
technology. IBM J Res Dev 52:465
57. InoueIH,YasudaS,AkinagaH,TakagiH(2008)Nonpolarresistanceswitchingofmetal/binary-
transition-metal oxides/metal sandwiches: homogeneous/inhomogeneous transition of current
distribution. Phys Rev B 77:035105
58. Sawa A (2008) Resistive switching in transition metal oxides. Mater Today 11:28
59. Jo SH, Kim KH, Lu W (2009) High-density crossbar arrays based on a SI memristive system.
Nano Lett 9:870
60. Jo SH, Chang T, Ebong I, Bhadviya BB, Mazumder P, Lu W (2010) Nanoscale memristor
device as synapse in neuromorphic systems. Nano Lett 10:1297
61. Lee MJ, Lee CB, Lee D, Lee SR, Chang M, Hur JH, KimYB, Kim CJ, Seo DH, Seo S, Chung
UI, Yoo IK, Kim K (2011) A fast, high-endurance and scalable non-volatile memory device
made from asymmetric Ta(2)O(5-x)/TaO(2-x) bilayer structures. Nat Mater 10(8):625
62. TorrezanAC, Strachan JP, Medeiros-Ribeiro G,Williams RS (2011) Sub-nanosecond switching
of a tantalum oxide memristor. Nanotechnology 22(48):485203
63. Levy WB, Steward O (1983) Temporal contiguity requirements for long-term associative
potentiation depression in the hippocampus. Neuroscience 8:791
64. Markram H, Lubke J, Frotscher M, Sakmann B (1997) Regulation of synaptic efﬁcacy by
coincidence of postsynaptic APs and EPSPs. Science 275:213

36
M. D. Ventra and Y. V. Pershin
65. Bi GQ, Poo MM (1998) Synaptic modiﬁcations in cultured hippocampal neurons: dependence
on spike timing, synaptic strength and postsynaptic cell type. J Neurosci 18:10464
66. Froemke RC, DanY (2002) Spike-timing-dependent synaptic modiﬁcation induced by natural
spike trains. Nature 416:433
67. PershinYV, DiVentra M (2010) Neuromorphic, digital and quantum computation with memory
circuit elements. Proc IEEE (in press); arXive:1009.6025
68. Snider GS (2008) Cortical computing with memristive nanodevices. SciDAC Rev 10:58
69. Parkin S (2010) Innately three dimensional spintronic memory and logic devices: racetrack
memory and spin synapses. Talk at 2010 MRS Spring Meeting
70. Kaech S, Banker G (2006) Culturing hippocampal neurons. Nat Protoc 1:2406
71. Zhao WS, Agnus G, Derycke V, Filoramo A, Bourgoin JP, Gamrat C (2010) Nanotube devices
based crossbar architecture: toward neuromorphic computing. Nanotechnology 21(17):175202
72. PershinYV, DiVentra M (2011) Solving mazes with memristors: a massively parallel approach.
Phys Rev E 84:046703
73. Likharev KK, Strukov DB (2005) In introducing molecular electronics. In: Cuniberti GF,
Richter K (eds) Introducing molecular electronics, vol 657. Springer, Heidelberg, pp 447–477

Chapter 4
Persuading Computers to Act More Like Brains
Heather Ames, Massimiliano Versace, Anatoli Gorchetchnikov,
Benjamin Chandler, Gennady Livitz, Jasmin Léveillé, Ennio Mingolla,
Dick Carter, Hisham Abdalla, and Greg Snider
Abstract Convergent advances in neural modeling, neuroinformatics, neuromor-
phic engineering, materials science, and computer science will soon enable the
development and manufacture of novel computer architectures, including those based
on memristive technologies that seek to emulate biological brain structures. A new
computational platform, Cog Ex Machina, is a ﬂexible modeling tool that enables
a variety of biological-scale neuromorphic algorithms to be implemented on het-
erogeneous processors, including both conventional and neuromorphic hardware.
Cog Ex Machina is speciﬁcally designed to leverage the upcoming introduction
of dense memristive memories close to computing cores. The MoNETA (Modu-
lar Neural Exploring Traveling Agent) model is comprised of such algorithms to
generate complex behaviors based on functionalities that include perception, moti-
vation, decision-making, and navigation. MoNETA is being developed with Cog Ex
Machina to exploit new hardware devices and their capabilities as well as to demon-
strate intelligent, autonomous behaviors in both virtual animats and robots. These
innovations in hardware, software, and brain modeling will not only advance our
understanding of how to build adaptive, simulated, or robotic agents, but will also
create innovative technological applications with major impacts on general-purpose
and high-performance computing.
4.1
Introduction
Recent advances in several ﬁelds of research may soon lead to the building of an
intelligent and adaptive electronic brain. Neuroscientists have observed learning at
the level of a single synapse and have gained considerable insight into how learning
occurs at this scale. Psychologists have looked deeply into learning and behaviors of
humans in a wide variety of tasks and have also used other mammals such as rats to
help with the understanding of intelligent behaviors. Computational neuroscientists
M. Versace () · H. Ames · A. Gorchetchnikov · G. Livitz · J. Léveillé · E. Mingolla
Neuromorphics Lab, Center of Excellence for Learning in Education,
Science, and Technology (CELEST), Boston University, Boston, MA, USA
B. Chandler · D. Carter · H. Abdalla · G. Snider
HP Labs, Palo Alto, CA, USA
R. Kozma et al. (eds.), Advances in Neuromorphic Memristor Science and Applications,
37
Springer Series in Cognitive and Neural Systems,
DOI 10.1007/978-94-007-4491-2_4, © Springer Science+Business Media Dordrecht 2012

38
M. Versace et al.
have developed increasingly sophisticated neural models to describe both the under-
lying neural mechanisms and their resulting behaviors. Engineers have been hard at
work developing new computer hardware that will allow for many more computa-
tions to be performed in a shorter amount of time—all while consuming less power.
It is this interdisciplinary convergence of knowledge that will lead to the ultimate
goal of creating a machine capable of human-level intelligence.
In this chapter, we argue that in order to achieve this goal, development of hard-
ware and software capable of supporting large-scale neural models is vital. Neural
models aimed at replicating complex animal behavior must be developed at the scale
of whole-brain systems. These models need a powerful software platform that serves
as the glue between the model and the underlying hardware.
Many kinds of hardware conﬁgurations are being developed for neural computing.
These hardware designs range from highly specialized devices that emulate single
compartments of neurons to general-purpose, massively parallel, high-performance
computing systems. Recently, the introduction of memristors [22, 23, 123] has
galvanized the neuromorphic computing community with the promise of providing
a fundamental, low-power building block to design high-density memory systems
for intelligent machines.
The common denominator of any neuromorphic hardware solution is that to some
extent it must adhere to how biological brains perform computation by comprising the
following properties: (1) massively parallel; (2) low-power, and (3) algorithmically
ﬂexible. This ﬁnal property requires that the hardware be sufﬁciently general-purpose
in its design to ensure that its development is pushed not just by scientiﬁc interest
to build machine intelligence, but also by businesses seeking to solve outstanding
problems in their application domains. For example, the video gaming market in
recent decades supported the development of cheaper and more powerful computing
platforms than were ever economically feasible to a scientiﬁc computing user market.
Section 4.2 further explores currently available neuromorphic hardware designs and
their properties.
Hewlett Packard (HP) Labs and the Neuromorphics Lab (NL) at Boston Univer-
sity (BU) are currently developing a software platform, Cog Ex Machina, or Cog,
[111]. Cog aims to enable realization of machine intelligence by laying the founda-
tion and infrastructure to develop intelligent coordination of hardware and software
solutions. Cog is speciﬁcally designed to reduce communication energy in neuro-
morphic computing by leveraging the upcoming introduction of dense memristive
memories close to computing cores. In particular, Cog will exploit the introduction of
(1) Complementary metal-oxide-semiconductor (CMOS) compatible memory that
allows close integration of dense memory banks next to or on top of cores to greatly
increase bandwidth and reduce power dissipation; and (2) Photonic interconnect
between chips (perhaps even within chips to a degree) to enable energy-efﬁcient,
long-distance communication. Section 4.3 further describes Cog.
Advances in large-scale, whole-brain neural models will be the ultimate test-bed
for intelligent machines. MoNETA (Modular Neural Exploring Traveling Agent; [3,
44, 117]) is one such model that tests the limits of whole-brain modeling. MoNETA

4
Persuading Computers to Act More Like Brains
39
is currently under development and contains modules for perception, motivation,
decision-making, and navigation. A brief review of whole-brain modeling work and
MoNETA appears in Sect. 4.4.
4.2
Enabling Neuromorphic Hardware
The lack of hardware to support implementation of biologically-inspired, large-scale,
whole-brain models is a major roadblock to achieving machine intelligence. Issues
such as computational power, energy consumption, robustness to noise, fault toler-
ance, and physical size of such devices has limited the development of appropriate
hardware. Some researchers have chosen to focus on the development of devices
to perform very speciﬁc neural functions, such as emulating a sense organ like the
retina or cochlea, whereas others have attempted to create electronic devices that
mimic processing of neurons, synapses, or components of neurons. On the other end
of the spectrum, some researchers have chosen to make use of digital commodity
hardware to achieve some correspondence with biological behavior at functional,
whole-system levels.
Consideration of these themes is essential when faced with the task of building
autonomous agents such as robots. These agents must at some level be instantiated
in hardware. The two main approaches to building tomorrow’s intelligent machines
are to (1) use mostly mainstream (commodity) digital hardware, ideal for fast alge-
braic implementations that can model the functions of certain brain processes, or (2)
to employ mostly custom, analog circuits that embody nonlinear physical compu-
tation as the preferred mathematical foundation for building intelligence [36, 101,
55]. Mobile, autonomous robots pose challenging requirements not only in terms
of intelligence, but also require low-power consumption and small form factor to
maximize battery life. Even performance of virtual agents share similar constraints
on processing or communication bottlenecks, whether a custom rack of processors or
the cloud is used. Furthermore, many virtual agents, likeApple’s Siri, now appear on
compact mobile devices, which place further stress on their limited power supplies.
For some agents, instantiation in conventional hardware is appropriate, while others
will require custom hardware. One option may lie in between these two methods
by combining advances in digital and analog hardware with memristive or other
forms of dense memories. The following subsections brieﬂy review these hardware
approaches.
4.2.1
Analog and Digital VLSI Neuromorphic Hardware
Neuromorphic VLSI (very large-scale integration) was proposed by Carver
Mead [80] in his seminal book Analog VLSI and Neural Systems published in 1989.

40
M. Versace et al.
The advantage of VLSI hardware components is their relatively inexpensive con-
struction. However, it is not possible to alter the circuitry once it has been built and
the manufacturing of the chip itself may take a long time [107]. For a comprehensive
review of analog VLSI designs see [73].
The signal coding used in these various hardware technologies can be described as
either analog or digital. Analog representations offer advantages in implicit real-time
operation and adherence to real-world analog signals, but reliable circuit design is
difﬁcult in the analog domain [107].
On the other hand, digital fabrication offers advantages in noise tolerance, ease of
manufacture, and high density [107]. Digital implementations have been criticized
because they require a massive amount of power to implement human-scale intelli-
gence [30, 33]. This limiting factor has been called into question by recent advances
in digital CMOS fabrication technology [61].
4.2.1.1
Hardware Implementation of Sensory Systems
Many neuromorphic VLSI devices are intended to mimic sensory organs such as the
retina or the cochlea. A requirement of such systems is real-time operation in order
to effectively process incoming signals. For a review of such systems, see [70].
The earliest hardware implementation of a retina was completed by [100] in their
electrical model of the pigeon retina comprised of 50 circuit boards. One of the
earliest VLSI implementations was described by [81] in which the “silicon retina”
wasabletoproducean“on-center, off-surround”output. Manymoreresearchershave
further developed the design of a hardware implementation of an artiﬁcial retina by
understanding the crucial processing features and structures found in the biological
retina e.g. [5, 13, 24–26, 32, 50, 69, 93, 99, 124, 126].
A very early electrical model of the cochlea was comprised of a transmission line
of 178 sections, each containing two inductors and four capacitors [60]. This early
model was less than optimal. VLSI capabilities made it possible to develop much
more effective electronic cochlea models. Mead co-developed one of the earliest
VLSI-based cochlea models [76], which was implemented as a sequence of ﬁlters,
see also [38]. This work on VLSI cochlear models has been extended to different
functions and improved design e.g. [16, 64–66, 71, 72, 74, 120], and more active
cochleae have also been developed e.g. [37, 47].
Neuromorphic hardware for other sensory systems has also been developed. In
particular, olfaction is of interest for various industries such as agriculture and per-
fumes. “Silicon noses” have been developed to meet these demands e.g. [62, 90,
106]. Tactile sensing systems are also being developed e.g. [6, 91, 98, 116].
4.2.1.2
Hardware Implementation of Neural Components and Circuitry
The idea of an equivalent circuit for a neuron dates back to at least the early twentieth
century when Lapique [63] modeled a neuron as a resistor and a capacitor (1907).

4
Persuading Computers to Act More Like Brains
41
This idea was advanced when the representation of dynamics of active cells was
described by both equations and an electrical equivalent circuit based on work by
[48]. Other researchers in the mid to late twentieth century also experimented with
simple electronic models of neural processes e.g. [14, 34, 59, 68, 97].
Neuromorphic hardware implementations of neural circuitry can include elec-
tronic models of the many compartments of neurons or can focus on the neuron as a
single unit e.g. [29, 40, 115, 121, 122]. Compactness and low-power consumption
are of utmost importance to the design of circuits to emulate individual neurons or
parts of neurons. They must take up a small area of physical space and run efﬁciently
to ensure that the greatest possible number of silicon neurons can ﬁt onto a single
chip. Single unit models of neurons often attempt to implement spiking dynamics
similar to those of biological neurons e.g. [121]. The biggest advantages of these
implementations are that they still display useful computational capabilities such
as synchronization, and they require less circuitry for implementation than more
complex multiple-compartment neural models [107].
Even the individual neuron can be broken down into functional computational
units, and some researchers focus on different components of the neuron such as
neuronal dendrites e.g. [31, 88, 95], neuronal axons [83], or ionic channels e.g. [77,
87, 94, 96, 102].
Electronic implementations of synapses, or so called “silicon synapses”, have also
been devised e.g. [8, 53]. Biological synapses are dense—the human cortex contains
roughly 1012 synapses per square centimeter. They also consume miniscule power,
have complex, nonlinear dynamics, and, in some cases, can maintain their memory
for decades. Most hardware implementations of the synapse focus on transmitting a
signal from the pre-synaptic neuron, while the modulating signal provides magnitude
to the post-synaptic neuron [107]. If the synapse is adaptive, the magnitude must be
adjustable. Some researchers have made use of ﬂoating gate devices for synapses
[28, 49]. Spike-timing dependent plasticity (STDP) uses relative timing of pre- and
post- synaptic spikes to determine the efﬁcacy of the synapse and thus update it
appropriately. STDP thus allows learning to be modeled on a chip e.g. [7, 53, 124].
One potential concern with the development of any hardware “synapse” is scalability.
None of the current approaches are able to scale up to biological levels without facing
issues of power dissipation, size, and routing of information between neurons.
Memristors have reawakened the neuromorphic community with the promise of
providing the missing piece to the construction of low-power neuromorphic hard-
ware. A memristor is a nanodevice that exhibits a resistance value which changes
based on the past electrical charge that has passed through it. This means that the
memristor is able to store information based on the past history of its activation,
similarly to brain synapses. An added bonus is that they have the potential to allow
designers to reach synaptic densities much closer to biological levels with respect to
competing approaches [125] without taking up too much space or using too much
power. Because of their small size and favorable dynamics [92], memristors also
have been used to model synapses in analog circuits that are able to learn in real time
[2, 10, 21, 58, 108–110, 112].
Memristive technology may also play a key role in developing general-purpose
computer hardware by using memristors as discrete memory elements in a digital

42
M. Versace et al.
platform. Memristors can be fabricated at a density greater than DRAM, are com-
patible with standard CMOS processes, use little or no standby power, and have
small active power consumption [112]. This last characteristic is signiﬁcant because
it means that designers can integrate dense, memristive memories with conventional
circuitry [123] and thus place memory and computational circuits closer together.
Decreased distance signiﬁcantly reduces the power dissipation in sending and receiv-
ing information between the two components while increasing the data bandwidth
between them.
Additional neuromorphic hardware implementations have focused on the de-
velopment of developing technology that communicates with spikes and operates
asynchronously [19, 82, 114], including implementations of spike-based plasticity
e.g. [39, 46, 52, 84]. One such class of networks is the “winner-take-all” (WTA)
networks, which consists of a group of competing and interacting neurons where
the neurons with the highest response suppress all other neurons to win the compe-
tition. Initially, these networks were described theoretically and within an abstract
mathematical modeling framework [45], but the ideas have recently been extended
to neuromorphic hardware e.g. [1, 17, 18, 27, 51, 54, 89].
4.2.1.3
Large-Scale Custom Hardware Systems
Several researchers are attempting to build large-scale (in terms of number of com-
ponents) neuromorphic hardware systems e.g. [103, 104]. Cognitive ability is the
largest functional challenge in moving from the systems described in the previous
two sections to large-scale systems [54]. All previous systems perform functions
based on the processing incoming input (sensory organs) or neural computations,
but none of them are able to emulate cognition or human intelligence. Concerns over
power consumption, scalability, robustness to noise, fault tolerance, and physical
size are all relevant to engineering challenges facing the neuromorphic community.
For a review of these systems, see [35].
Some researchers have chosen to approach challenges by looking at smaller sub
problems. Vainbrand and Ginosar are developing a network on chip architecture
for neural networks by exploring innovations in ﬂexible connectivity [113]. Other
researchers have worked on developing networks of neurons (e.g. integrate-and-ﬁre
neurons) inVLSI hardware implementations e.g. [41, 52]. Still other researchers have
attempted to model cortical structures or networks in custom hardware solutions e.g.
[20, 104].
CAVIAR(ConvolutionAddress-Event-Representation(AER)VisionArchitecture
for Real-Time) is one large-scale custom system being developed by a consortium
of European neuromorphic researchers [105]. The primary goals of this project are
to develop an infrastructure based on AER for constructing biologically inspired,
hierarchically-structured, multi-chip systems for sensing, processing, and actuation
and to demonstrate the utility of the infrastructure with a perception-action vision
system. AER is described as a spike-based technique used to solve the massive inter-
chip connectivity problem in hardware. As of 2009, the system makes use of four
chips containing approximately 45,000 neurons and 5 million synapses [35].

4
Persuading Computers to Act More Like Brains
43
The European consortium FACETS1 (Fast Analog Computing with Emergent
Transient States, [15]) completed its 5-year program in 2010 and has now continued
onto the BrainScaleS2 project. The resulting FACETS hardware consists of mixed
signalVLSI implementation in a standard 180 nm CMOS process. Neural and synap-
tic computation is implemented in custom designed analogue circuits communicating
via asynchronous exchange of binary action potentials. The hardware is a non-von
Neumann system architecture with memory implemented in distributed small SRAM
cells and analogue ﬂoating gate units. The resulting FACETS wafer includes up to
200,000 neuronal circuits and 50 million plastic synapses, including a freely conﬁg-
urable communication fabric [103]. Driven by ﬁxed analog dynamics, the system
exceeds biological, real-time dynamics by a factor of 1,000–100,000.3
Brains in Silicon [11, 12] is under development at Stanford University. This
system combines custom CMOS chips into large arrays on multiple boards using
sub threshold CMOS circuitry for implementing Hodgkin-Huxley type neurons and
quasi-binary STDP synapses and asynchronous digital logic for implementing the
AER protocol for transmission of spikes between chips.
Another system, IFAT4 (Integrate and Fire Transceiver), is under development at
Johns Hopkins University. The ﬁrst generation of IFAT consisted of 2,400 neurons
operating in real time with an array of neurons implemented in a 3D CMOS ar-
chitecture to capture the 3D interconnections, parallel processing and computations
observed in biological neurons [41, 118, 119]. The silicon neurons were placed on
one layer, synapses and interconnections on a second layer, and the communications
circuitry on a third layer. It performs internal analog computations and communi-
cates with the external word digitally. Network topologies and synaptic parameters
are stored in off-chip RAM rather than being hard-wired. The new system under
development aims to simulate 60,000 biologically realistic neurons with 120 million
fully programmable synaptic connections.
SpiNNaker can be described as a massively parallel architecture comprised of
bespoke multi-core system-on-chips. This architecture is biologically inspired and
aims to simulate up to a billion spiking neurons in real-time [86]. This system can
be considered a hybrid system in which the hardware is not as fully customized in
design as the others described above.
4.2.2
Commodity Based Hardware
The neuromorphic hardware discussed up to this point has focused on customized
hardware solutions. An alternative is to build intelligent machines using commodity
based digital hardware such as central processing units (CPUs), graphics processing
1 http://facets.kip.uni-heidelberg.de/public/motivation/index.html
2 http://brainscales.kip.uni-heidelberg.de/index.html
3 http://facets.kip.uni-heidelberg.de/images/4/48/Public–FACETS_15879_Summary-ﬂyer.pdf
4 http://etienne.ece.jhu.edu/projects/ifat/index.html

44
M. Versace et al.
Table 4.1 Comparison of volume, energy consumption, and operation for a human brain and
super-computing technology as of 2010. (See [79] for details)
Brain
Supercomputer
Volume
∼1200 cm3
1500 m3
Energy Consumption
20 W
3 MW
Operations/second
> 1016
1015
units (GPUs), CPU and GPU clusters, and supercomputers. However, current su-
percomputer technologies have a long way to go before they can meet the volume,
number of operations/second, and energy consumption found in the human brain;
see Table 4.1.
Current research is focused on how to bridge the gap of physical volume and
energy consumption between the biological brain and supercomputers.
Henry Markram, leader of the Blue Brain project, has made use of the power of
the IBM Blue Gene supercomputer to simulate a cortical column comprised of bio-
physically realistic neurons [78]. On the other end of the spectrum, in 2009 at IBM,
Dharmendra Modha presented his large-scale simulation of a “cat brain”, which fo-
cused more on solving outstanding issues in efﬁciently leveraging digital computing
power than the achievement biological realism [4]. The simulations made use of
a 1.4 MW supercomputer with 147,000 processors to run simpliﬁed point neurons
roughly 700 times slower than real time. Despite the massive computational power,
this simulation was more elementary than what Eugene Izhikevich implemented in
2005 and published in 2008 [57] when he developed a large-scale model of the brain
having microcircuitry of the mammalian thalamo-cortical system. The model imple-
mented roughly 1011 neurons and 1015 synapses, numbers very close to the average
count of neurons and synapses in the human brain. The model captured roughly
300 × 300 mm2 of mammalian thalamo-cortical surface, speciﬁc, non-speciﬁc, and
reticular thalamic nuclei, and spiking neurons with ﬁring properties corresponding
to those recorded in the mammalian brain. The model exhibited alpha and gamma
rhythms, moving clusters of neurons in up- and down-states, and other interesting
phenomena. One second of simulation took 50 days on a beowulf cluster of 27
processors (3 GHz each).
4.2.3
The Future of Neuromorphic Hardware
In2010, SandiaNationalLabsreleasedanimportantreportentitled“TowardExascale
Computing through Neuromorphic Approaches” [79]. The report asserts that the
pathway to exascale computing (1018 ﬂoating-point operations per second) will be
achievedbyleveragingintuitionfrombiologicalapproaches. Theauthorsputforthsix
desirable attributes to be embedded in future processors in order to achieve exascale
computing:
1. Connectivity, parallel, distributed;
2. Tunability, adaptability, plasticity;

4
Persuading Computers to Act More Like Brains
45
3. Self-preservation, healing, robust, recoverable;
4. Compact, high-density, 3D geometries;
5. Standardizable, predictable, speciﬁable; and
6. Flexibly sustained.
These attributes are to be desired not just to achieve exascale computing, but also to
build truly intelligent machines. All of these attributes point to the need for massive
parallelism, small size, and a high degree of ﬂexibility. It can be argued that a
hybrid hardware solution may be optimal. One such solution may include customized
hardware solutions for sense organs (e.g. retina), but a more general-purpose solution
is desired for the whole-brain system. Such a system may make use of memristors
as a way to overcome communication bottlenecks between memory storage and
computation, but not necessarily as a single synaptic element. Given that there will
be incremental development steps as neuromorphic hardware advances, a ﬂexible
yet powerful platform is needed to simplify the modeling process on these hardware
devices. An example of such a platform is described in the next section.
4.3
Cog Ex Machina
The Cog Ex Machina, or Cog, software framework is being co-developed by Hewlett-
Packard (HP) and Boston University (BU) to provide a ﬂexible and powerful tool
to implement complex neural models at scale [111]. Cog is a low-cost, all-digital
platform that allows researchers the ability to assemble multi-component brain mod-
els able to interact with either a simulated or real environment in real time. Cog is
a ﬂexible tool to neural modelers that can quickly build and text complex systems
on commodity hardware, such as GPU clusters. Cog effectively abstracts away the
details of the underlying hardware. This translates into huge savings for modelers,
who can continue building their systems on Cog without worrying about keeping
track of advances in a rapidly changing underling hardware technology advances.
4.3.1
What Hardware Architecture?
As Fig. 4.1 shows, Cog is able to run on single core CPUs and GPUs, or a cluster of
both processors. The platform has a digital hardware foundation and users are able
to implement a wide variety of neuromorphic algorithms. Cog uses off-the-shelf
graphical processing units (GPUs) to implement the accelerators.
Distributed across the accelerators, the Cog software framework supplies re-
searchers with a set of primitives for building massively parallel neuromorphic
models. To study how models interact with their environments, designers can plug in
animats, either software creatures embedded in virtual environments or robots built
from actuators and cameras, touch sensors, or accelerometers, enabling the model

46
M. Versace et al.
Fig. 4.1 High-level view of
the Cog Ex Machina platform.
The hardware consists of
accelerator nodes, currently
GPUs, which communicate
through a photonic network.
Researchers build abstract
brain models on the Cog
software platform. Cog hides
the underlying hardware,
allowing brain models to run
on many hardware
implementations. Brain
models use Cog to interact
with the real world through
robotic sensors and actuators
or with a virtual world using a
software animat
to interact with the real world in real time. Many robotic applications are possible
(such as a robot capable of quickly searching for trapped people, hazards, or hotspots
inside a building before the ﬁre department enters), but nearly any machine with a
nontrivial interface (cell phones, remote controls, cars) could beneﬁt from embedded
intelligence.
4.3.1.1
Power
With the continuing reduction in CMOS feature size, capacitive signaling losses
in the wiring increasingly dominate a chip’s power budget. These losses are of
particular concern in cognitive architecture design because the brain’s wiring is 3D
and extremely dense [9]. Moreover, brain computation is massively parallel as it
reads and modiﬁes enormous amounts of memory continuously. In light of these
complexities, designers have little choice but to place dense, low-power memory
very close to the computational circuits that read and write it in order to minimize
signaling losses. The memristive memory being developed for Cog enables exactly
that.
4.3.2
Software Architecture
Cogusersexpressabrainmodelasanarbitrary, directedgraph, suchasthatinFig.4.2.
The nodes hold computational state and exchange information through edges. All
nodes execute one computational step in parallel and then pass messages through

4
Persuading Computers to Act More Like Brains
47
Fig. 4.2 Building a brain model in Cog. The main cog abstractions in Cog are dynamic ﬁelds.
Models are directed graphs of adaptive transformations in a dynamic ﬁeld that execute concurrently,
exchanging information through tensor ﬁelds. Computation is deterministic and race free
edges before executing the next computational step. The graph is clocked at 100 Hz
for real-time applications, allowing each node 10 ms to complete its computation
and communication at each clock step.
An edge relays information from one node to another using a tensor ﬁeld, a
discrete, multidimensional array of tensors. In turn, each tensor is a multidimensional
numerical array (scalar, vector, dyad, and so on). Computational nodes implement
adaptive transformations in the dynamic ﬁeld that use incoming tensor ﬁelds to
produce output tensor ﬁelds on outgoing edges. The transfer function varies over
time because an adaptive transformation can change its internal state as a function
of its input history, such as feedback from other transformations that it drives.
None of these platform attributes are visible to users, which frees them to focus
on their models. Adding computational resources either speeds up model execution
or, in a real-time environment, increases the size of the model that Cog can execute.
These abstractions might seem distant from their biological counterparts, but a
rough correspondence exists. Tensor ﬁelds moving along the graph edges are similar
to the information that axon bundles convey in a human nervous system. Linear trans-
formations may be analogous to the computation performed in the dendritic trees of
neuron populations, with the learning or adaptation analogous to the modiﬁcations
of synaptic weights. This is the storage of long-term memory. Nonlinear transfor-
mations correspond to the nonlinear dynamics of populations of neuron bodies or
somas: the storage of medium- and short-term memories.
4.3.2.1
Learning
Linear transformation adaptation generally occurs over a much longer time scale
relative to nonlinear transformations, and this slower adaptation is what constitutes
learning. As Fig. 4.3 shows, feedback from a nonlinear transformation guides learn-
ing. Cog holds the actual learned state, W, within a linear transformation. It then
convolves or correlates W with an input, x (part of a tensor ﬁeld), to produce an

48
M. Versace et al.
Fig. 4.3 Learning in Cog.
Linear and nonlinear
transformations cooperate to
implement the learning that
corresponds to long-term
memory. Cog combines a
linear transformation’s
current state, W, with the
input, x, to produce y, a partial
inference. It then combines g,
x, and W to implement
learning. The learning
function, f, determines the
type of learning that occurs
output tensor ﬁeld, y. Cog calls this partial inference. Cog uses the partial inference
to drive a nonlinear transformation, which can respond by feeding back a learning
ﬁeld, g, to the linear transformation. The linear transformation uses g, x, and W (its
current state) to update its learned state.
Through conﬁguration and appropriate feedback [43], a wide variety of classical
learning laws can be implemented:
•
Hebb rule derivatives, including classic Hebbian, Hebb plus passive decay, presy-
naptically gated decay (outstar), postsynaptically gated decay (instar), Oja, dual
OR, and dual AND;
•
threshold-based rules, including Covariance 1, Covariance 2, BCM (Dayan and
Abbott), original BCM (oBCM), IBCM, and Bienenstock, Cooper, and Munro
(BCM) theory (Law and Cooper);
•
feedback-based rules, including back-propagation, Harpur’s rule, and contrastive
divergence; and
•
temporal-trace-based rules, including Rescorla Wagner, temporal difference, and
Foldiak.
4.3.3
Flexibility in Hardware Devices and Neural Algorithms
Although a general theory of cognition does not exist yet, researchers do recognize
that platform ﬂexibility is essential as they plow through the fog and uncertainty of
learningtobuildintelligentmachines. Coghasmanyfeaturesthatofferthisﬂexibility.

4
Persuading Computers to Act More Like Brains
49
Its all-digital hardware foundation reduces technological and fabrication risk and
allows for the leveraging of conventional and mature programming tools to set up,
debug, and deploy applications.
Cog’s tensor framework mechanisms are perhaps non biological, but they are well-
matched to our underlying CMOS/memristive technology. The framework is also
expressive, pulling linear algebra, geometry, and analysis into a single foundation,
and enables exploitation of much mathematical and engineering knowledge (e.g.
information and coding theory, digital signal processing, non-Euclidean coordinate
systems, tensor convolution, normalized convolution, and fast Fourier transforms).
The framework also supports a wide variety of learning laws and network models.
Perhaps the most important architectural attribute is the nearly complete decou-
pling of the software abstractions for building brains (tensor ﬁelds and adaptive
transformations) from the underlying hardware platform. Not only does this provide
portability among existing and future platforms, it allows us to quickly modify the
software architecture to accommodate new or unexpected algorithmic problems as
they arise.
4.4
Modeling Complex, Adaptive Behavior
with Biological-Scale Neural Systems
Achieving machine intelligence requires more than just “brain-like” hardware or
a ﬂexible user-friendly modeling platform. It requires modeling efforts aimed at
simulating whole-brain systems that result in behaviors that can be observed and
measured.
MoNETA (Modular Neural Exploring and Traveling Agent [3, 44, 117]) is a
large-scale neural model that controls either a virtual or robotic body while per-
forming certain behavioral tasks. MoNETA is being developed by researchers in
the Neuromorphics Lab5 at Boston University. As the name suggests, MoNETA’s
tasks concentrate primarily on the exploration of an environment, ﬁnding optimal
paths through it and navigating along these paths. Exploration includes processing of
sensory information to recognize objects, map them onto an internal spatial represen-
tation, and maintain them in memory. Finding optimal paths includes the neuronal
analysis of internal spatial representations, self-localization of the animat within this
representation, and the use of neuronal activations to determine the shortest path to a
current goal. Navigating along the paths includes updating the internal representation
with newly encountered information about obstacles, processing reward information
to map potential goal locations, and maintaining the working memory of recently
visited places.
The key concept behind MoNETA’s design is its modularity. The macro-structure
of the simulated brain is initially speciﬁed with the goal of being able to “swap in”
5 http://nl.bu.edu/

50
M. Versace et al.
Fig. 4.4 MoNETA block diagram. Colors stand for three major subsystems: yellow for motivational
system, green for spatial representation and planning system, and pink for sensory processing
system. Each box represents a separate neuronal population. Dark red arrows and boxes represent
GPS-based modules that will be replaced in the next implementation with path-integration system
more reﬁned neural circuits when they become available. Modules are currently be-
ing developed with increasing complexity to be plugged into the architecture so that
the artiﬁcial brain system will increase in its functional capabilities as development
progresses. Major components of the system perform sensory object recognition,
motivation and reward processing, goal selection, allocentric representation of the
world, spatial planning, and motor execution. The agent is tested in virtual environ-
ments replicating neurophysiological and psychological experiments with real rats.
The initial testing environment replicates the Morris water maze [85]. A complete
block diagram for the system tested in this maze is presented in Fig. 4.4.
4.4.1
Motivation, Reward, and Goal Selection
Subsystem (MoRSel)
The motivational system represents the internal state of the agent that can be adjusted
by sensory inputs. In the Morris water maze, one drive is paramount– a desire to get
out of the water. This drive persists as long as the animat is swimming and sharply
decreases as soon as it is fully positioned on a submerged platform. Another drive,
curiosity, is constantly active and is never fully satisﬁed. It motivates the animat to
exploreunfamiliarpartsoftheenvironment. Familiaritywithenvironmentallocations
provides inhibition to the curiosity drive in a selective manner, so that recently
explored locations are less appealing than either unexplored locations or locations
that were explored a long time ago.

4
Persuading Computers to Act More Like Brains
51
The main output of the motivational system is a goal selection map. It is based
on competition between goals set by the curiosity system and goals learned by the
animat. The goal selection map is set up as a recurrent competitive ﬁeld (RCF;
[45]) with a faster-than-linear transfer function resulting in winner-take-all (WTA)
selection. This implementation selects the most prominent input signal among all
inputs as a winning goal. Inputs consist of learned goal locations modulated by the
magnitude of the drive corresponding to this goal. For example, a hungry animat
that knows where the food is located will receive a strong input signal for the food
location, while a thirsty animat that only knows where food is located will not
receive strong inputs because there is no knowledge of water locations and no drive
for food).Another set of inputs comes from the curiosity system that drives the animat
towards exploration of new areas. Curiosity driven goals receive weaker inputs than
well-learned reward locations so they can only win if there are no prominent inputs
corresponding to the learned goals for currently active drives.
MoRSel will eventually become a crucial module of MoNETA and evolve into a
moreelaboratesystemwithmultipledrives, competitionbetweendrives, andlearning
of place-reward and object-reward associations. The current implementation includes
all the basics for this extension by providing for competition between two drives and
learning the place-reward associations for a single reward.
4.4.2
Spatial Planning and Allocentric Representation
of Knowledge Subsystem (SPARK)
The spatial planning system is built around a previously developed neural algorithm
for goal-directed navigation [42]. The gist of this algorithm is the propagation of
waves of activity from the known goal location (reverse activity spread). In an un-
constrained environment, these wavefronts form expanding concentric circles around
the goal location. If an obstacle is encountered, it does not allow the wave propaga-
tion, so the wavefront deviates from the circle to ﬂow around the obstacle. As a result
of all deviations, the ﬁrst wavefront segment that reaches the current location of the
animat comes from the direction that corresponds to the shortest path to the goal. In
response to this, a new wavefront is initiated at the current location (forward activity
spread). The collision of this front with the second wave coming from the goal will
then happen at a location close to the current location of the animat and in the direc-
tion of the shortest path towards the goal. This is called the next desired destination.
Then the animat moves to this desired location, and the process is repeated.
In MoNETA, this model is extended by a chain of neural populations that convert
the allocentric-desired destination into an allocentric-desired direction and then into
a rotational velocity motor command. A second extension of the model deals with
the mapping of the environment. The original algorithm included goal and obstacle
information for path planning, but this information was provided in the form of
allocentric maps where the locations of both the goals and obstacles were received
directly from the environment. MoNETA uses these maps, but also creates them from
egocentric sensory information through a process of active exploration. Although the

52
M. Versace et al.
current version only uses somatosensory information, visual input will be integrated
in later stages. The system converts egocentric representations to allocentric ones
and then learns the mapping of obstacles and goals. It uses a learning rule that is
local to dendrites and does not require any postsynaptic activity.
In the real animal, place cells in the hippocampus are the output of a sophisticated
path integration system that receives vestibular inputs corresponding to linear and
angular accelerations and integrates them into velocities and then distances and an-
gles. The ﬁrst generation of MoNETA does not use most of the corresponding brain
areas because it can rely on sensors that directly record distances, angles, and output
representations similar to outputs of place and head-direction cells. These sensors
are marked as GPS-based in Fig. 4.4. To model the system closer to biological reality,
a self-localization system must be developed to replace the current GPS-based sen-
sors. This will require interactions between sensory and navigational modules. This
interaction is likely to include the learning of associations between certain landmark
conﬁgurations and positions in the environment and is similar to the path-integration
system of a real animal, including grid cells of multiple scales and conjunctive cells
that hold both positional and directional information.
4.4.3
Sensory Object Recognition and Tracking
Subsystem (SORT)
SORT is intended to include all sensing modalities, but in its current instantiation,
the primary focus of development is the visual system. The function of the visual
sensory module is to recognize learned objects in the visual scene and indicate
their locations in retinal coordinates. This particular subdivision of goals is thought
to reﬂect the complementarity present in the brain’s what and where pathways. The
current MoNETA visual system involves parallel what and where processing streams
to simultaneously identify and localize objects respectively. For example, MoNETA
uses a “dynamic attentional window” in the where system that helps the what system
to focus on a restricted candidate area to deploy limited attentional resources. Once an
object has been identiﬁed, the what system biases the where system simultaneously in
order to explore areas nearby the locus of attention that led to the object classiﬁcation.
SORT design is composed of separate submodules that process different aspects of
visual information: form, motion and disparity in both processing streams. Although
this is our long-term design objective, the current system only uses very simple
information pertaining to visual form.
The where pathway orients attention to select locations in the retinal input for
furtherprocessingbythewhat pathway. Locationsareselectedbasedonanadaptation
of the saliency model [56]. The goal of object localization is achieved by keeping
track of the spatial coordinates of pinpointed locations. Object recognition makes
use of the fact that landmarks in the virtual maze can be suitably distinguished based
on color information only. Speciﬁcally, the presence of a large blob of pixels of
a certain color within a region of interest triggers the recognition of the landmark
corresponding to that color. Components of the where pathway were successfully
tested on images of natural scenes to ensure feasibility of the approach [67].

4
Persuading Computers to Act More Like Brains
53
Fig. 4.5 MoNETA in the Morris water maze. Top Left: bird-eye view of the virtual maze. The animat
is marked with white arrow. Top Right: eye-view of the maze with the corresponding activation of
the landmark recognition system. Bottom Left: trajectories resulting from a ﬁrst run and several late
runs. Bottom Right: learned obstacles (maze walls) after the ﬁrst run
4.4.4
Simulation Results
There are several completed implementations of MoNETA. One implementation
has been tested in a virtual animat negotiating a Morris Water Maze task [85]. In
this classic task, the rat is placed in a water tank and makes use of visual cues to
locate a submerged platform and swim to it. The rat is motivated to ﬁnd the platform
because it seeks a place to rest from constant swimming. Despite the simplicity of the
task, solving the water maze requires the integrated simulation of brain areas sub-
serving object recognition and localization, touch, proprioception, goal selection,
motivation, and navigation, among others. The performance of the virtual animat is
depicted in Fig. 4.5.
MoNETA has also been tested in a robotics application. The resulting robot was
named ViGuAR (Visually Guided Adaptive Robot) presented in Fig. 4.6 [75]. The

54
M. Versace et al.
Fig. 4.6 Visually GuidedAdaptive Robot (ViGuAR). Top: Experimental setup with a server running
a Cog based brain communicating through the wireless link to the net book controlling the i-Robot
Create. Bottom Left: Object approach strategy. Bottom Right: Brain architecture
ViGuAR brain reduces a two-dimensional neural representation of its visual input to
a single dimension centered to the head direction in a robocentric coordinate frame.
Before approaching an object, the robot orients itself toward the middle of a target.
TheViGuARbrainconsistsofthefollowingmodules: TheColorDetectionSystem
(CDS) converts an RGB value produced by the webcam into a neural represen-
tation of chromatic features: redness and greenness. The Object Detection System
(ODS) converts the color representation into a binarized object representation. These
two modules implement the SORT system of MoNETA. The Reward System (RS)
produces a positive or negative reinforcement by associating chromatic features of
the contacted object with a corresponding reward value. The Feature System (FS)
computes a value of attractiveness based on synaptic weights associated with the
chromatic features: redness and greenness. The Goal Selection System (GSS) ana-
lyzes the attractiveness values in order to ﬁnd the most attractive goal in its view.
These three modules implement the MoRSel system of MoNETA. The Self Tracer
(RT) updates the robot’s vector of movement upon completion of each motor com-
mand. The Visited Objects Map (VO Map) contains a representation of space where

4
Persuading Computers to Act More Like Brains
55
therobotoperates. ThesetwomodulesrepresenttheSPARKsystemofMoNETA.The
resulting robot quickly learns to approach attractive objects and avoid non-rewarding
objects in the environment.
The next phase will involve running the animat in several other experimental
rodent-maze paradigms and expanding the complexity of the neural models pow-
ering the animat. In parallel, advances in software and hardware architectures will
make it possible to implement these models in robotic and mobile platforms. Al-
though MoNETA is far from a truly thinking, learning machine scalable to human
intelligence, it does represent an important ﬁrst milestone in the development of such
an artiﬁcial brain. It models biological brain functions, it is able to replicate rodent
behavior in a simple paradigm, and it takes into account new and vibrant advances
in hardware that come closer to mimicking biology than ever before.
4.5
Conclusions
Advances in large-scale neural modeling, and in particular, the efforts on the MoN-
ETA project, combined with Cog development, offer hope to the achievement of
biological-scale intelligence in machines. The development of new hardware tech-
nologies is still in the early phases of research and testing and therefore, MoNETA is
powered by simulated architectures that take advantage of a heterogeneous arrange-
ment of computer processors. Cog provides the glue between the hardware and the
neural models, which in turn allows for seamless integration of new hardware as it
is developed. As hardware development progresses, Cog will continue to provide a
ﬂexible integration between hardware and model development. At this point in time,
it is unclear how memristors will ﬁt into the development of neuromorphic brain
hardware (e.g. modeling analog synapses and discrete memory elements). Cog de-
velopment is taking advantage of two game-changing technologies: dense memory
banks next to or on top of cores and photonic interconnect between chips, which the
authors believe will be necessary to enable energy-efﬁcient neuromorphic computing
in digital hardware.
While it may be that the optimal hardware solution is a mixture of custom and
commodity hardware, Cog will allow neural modelers to “reduce risks” by leveraging
market-fueled progress in digital chips. Nevertheless, as researchers move forward
in building machine intelligence, it is clear that an integrative and interdisciplinary
approach is necessary.
Acknowledgments The work was supported in part by the Center of Excellence for Learning in
Education, Science and Technology (CELEST), a National Science Foundation Science of Learning
Center (NSF SBE-0354378 and NSF OMA-0835976). This work was also partially funded by the
DARPA SyNAPSE program, contract HR0011-09-3-0001. The views, opinions, and/or ﬁndings
contained in this chapter are those of the authors and should not be interpreted as representing the
ofﬁcial views or policies, either expressed or implied, of the Defense Advanced Research Projects
Agency, the Department of Defense, or the National Science Foundation.

56
M. Versace et al.
References
1. Abrahamsen J, Haﬂiger P, LandeT (2004)A time domain winner-take-all network of integrate-
and-ﬁre neurons. IEEE Int Symp Circuits Syst 5:361–364
2. AﬁﬁA, Ayatollahi A, Raissi F (2009) STDP implementation using memristive nano device in
CMOS-Nano neuromorphic networks. IEICE Electron Express 6(3):148–153
3. Ames H, Mingolla E, Sohail A, Chandler B, Gorchetchnikov A, Léveillé J, Livitz G, Versace
M (2011) The Animat—New frontiers in whole-brain modeling. IEEE NEST (in press)
4. Ananthanarayanan R, Esser SK, Simon HD, Modha DS (2009) The cat is out of the bag:
cortical simulations with 109 neurons, 1013 synapses. Proceedings of the conference on high
performance computing networking, storage, and analysis, pp 1–12
5. AndreouAG, Meitzler RC, Strohben K, Boahen KA (1995)AnalogVLSI neuromorphic image
acquisition and pre-processing systems. Neural Net 8(7–8):1323–1347
6. Argyrakis P, Hamilton A, Webb B, Zhang Y, Gonos T, Cheung, R (2007) Fabrication and
characterization of a wind sensor for integration with neuron circuit. Microelectron Eng
84:1749–1753
7. Arthur J, Boahen K (2006) Learning in silicon: timing is everything. In: WeissY, Scholkoph B,
Platt J (eds) Advances in neural information processing systems, 18. MIT Press, Cambridge,
pp 1–8
8. Bartolozzi C, Indiveri G (2007) Synpatic dynamics in analog VLSI. Neural Comput
19(10):2581–2603
9. Basset DS, Greenﬁeld DL, Meyer-Lindenberg A, Weinberg DR, Moore SW, Bullmore
ET (2010) Efﬁcient physical embedding of topologically complex information processing
networks in brains and computer networks. PLoS Comput Biol e1000748
10. Bernabe L, Serrano-Gotarredona T (2009) Memristance can explain spike-time-dependent-
plasticity in neural synapses. Nature Precedings. http://precedings.nature.com (hdl:10101/
npre.2009.3010.1)
11. BernabeK(1999)Athroughput-on-demandaddress-eventtransmitterforneuromorphicchips.
Advanced Research in VLSI, pp 72–86
12. Boahen K (2007) Synchrony in silicon: the gamma rhythm. IEEE Trans Neural Netw
18(6):1815–1825
13. Boahen K,AndreouA (1992)A contrast sensitive silicon retina with reciprocal synapses. Adv
Neural Inf Process Syst 4:764–772
14. BrockmanWH (1979)A simple electronic neuron model incorporating both active and passive
responses. IEEE Trans Biomed Eng BME-26:635–639
15. Brüderle D, Petrovici MA, Vogginger B, Ehrlich M, Pfeil T, Millner S, Grübl A, Wendt
K, Müller E, Schwartz MO, de Oliveira DH, Jeltsch S, Fieres J, Schilling M, Müller P,
Breitwieser O, Petkov V, Muller L, Davison AP, Krishnamurthy P, Kremkow J, Lundqvist
M, Muller E, Partzsch J, Scholze S, Zühl L, Mayr C, Destexhe A, Diesmann M, Potjans
TC, Lansner A, Schüffny R, Schemmel J, Meier K (2011) A comprehensive workﬂow for
general-purpose neural modeling with highly conﬁgurable neuromorphic hardware systems.
Biol Cybern 104(4–5):263–96
16. ChanV, Liu S-C,Van SchaikA (2007)AER EAR: a matched silicon cochlea pair with address
event representation interface. IEEE Trans Circuits Syst 54:48–59
17. Chicca E, Indiveri G, Douglas R (2004) An event based VLSI network of integrate-and-ﬁre
neurons. Proceedings of IEEE international symposium on circuits and systems, pp 357–360
18. Chicca E, Indiveri G, Douglas R (2007a) Context dependent ampliﬁcation of both rate and
event-correlation in aVLSI network of spiking neurons. In: Scholkopf B, Platt, J, Hofmann, T
(eds) Advances in neural information processing systems, 19. Neural Information Processing
Systems Foundation, Cambridge, pp 257–264
19. Chicca E, Whatley AM, Dante V, Lichtsteiner P, Delbruck T, Del Giudice P, Douglas R,
Indiveri G (2007b) A multi-chip pulse-based neuromorphic infrastructure and its application
to a model of orientation selectivity. IEEE Trans Circuits Syst 52(6):1049–1060

4
Persuading Computers to Act More Like Brains
57
20. ChoiTYU, Merolla PA,Arthur JV, Boahen KA, Shi BE (2005) Neuromorphic implementation
of orientation hyper columns. IEEE Trans Circuits Syst 52(6):1049–1060
21. Choi H, Jung H, Lee J,Yoon J, Park J, Seong D, Lee W, Hasan M, Jung GY, Hwang H (2009)
An electrically modiﬁable synapse array of resistive switching memory. Nanotechnology
20(34):345201 (Epub)
22. Chua LO (1971) Memristor—missing circuit element. IEEE Trans Circuit Theory 18(5):507–
519
23. Chua LO, Kang SM (1976) Memristive devices and systems. Proc IEEE 64(2):209–223
24. Costas-Santos J, Serrano-Gotarredona T, Serrano-Gotarredona R, Linares-Barranco B (2007)
A spatial contrast retina with on-chip calibration for neuromorphic spike-based AER vision
systems. IEEE Trans Circuits Syst I 54:1444–1458
25. Culurciello E, Etienne-Cummings R, Boahen KA (2003) A biomorphic digital image sensor.
IEEE J Solid State Circuits 38:281–294
26. Delbruck T, Mead C (1996) Analog VLSI transduction. Technical Report CNS Memo 30,
California Institute of Technology and Computation and Neural Systems Program. Pasadena,
CA
27. DeYoung MR, Findley RL, Fields C (1992) The design, fabrication, and test of a new VLSI
hybrid analog-digital neural processing element. IEEE Trans Neural Netw 3(3):363–374
28. Diorio C, Hasler P, Minch BA, Mead CA (1996) A single-transistor silicon synapse. IEEE
Trans Electron Devices 43(11):1980–1982
29. Douglas R Mahowald M (1995) Silicon neurons. In: Arbib M (ed) The handbook of brain
theory and neural networks. MIT Press, Cambridg, pp 282–289
30. Douglas R, Mahowald M, Mead C (1995) Neuromorphic Analog VLSI. Annu Rev Neurosci
18:255–281
31. Elias JG (1993) Artiﬁcial dendritic trees. Neural Comput 5(4):648–664
32. Etienne-Cummings R,Van der Spiegel, J (1996) Neuromorphic vision sensors. SensActuators
A Phys 56(1–2):19–29
33. Faggin F, Mead C (1995) VLSI Implementation of Neural Networks. In An Introduction to
Neural and Electronic Networks. Academic Press, San Diego, pp 275–292
34. Fitzhugh R (1966) An electronic model of the nerve membrane for demonstration purposes.
J Appl Physiol 21:305–308
35. Folowosele F (2010) Neuromorphic systems: silicon neurons and neural arrays for emulating
the nervous system. Neurdon. http://www.neurdon.com/2010/08/12/neuromorphic-systems-
silicon-neurons-and-neural-arrays-for-emulating-the-nervous-system/
36. Folowosele F, Hamilton TJ, Etienne-Cummings R (2011) Silicon modeling of the MihalaÅŸ–
Niebur neuron. IEEE Trans Neural Netw 22(12):1915–1927
37. Fragniére E, van Schaik A, Vittoz EA (1997) Design of an analogue VLSI model of an active
cochlea. Analog Integr Circuits and Signal Processing 12:19–35
38. Furth P, Andreou AG (1995) A design framework for low power analog ﬁlter banks. IEEE
Trans Circuits Syst 42(11):966–971
39. Giulioni M, Camilleri P, Dante V, Badoni D, Indiveri G, Braun J, Del Giudice P (2008) A
VLSI network of spiking neurons with plastic fully conﬁgurable “stop-learning” synapses.
Proceedings of IEEE international conference on electronics, circuits and systems, pp 678–
681
40. Glover M, HamiltonA, Smith LS (2002)AnalogueVLSI leaky integrated-and-ﬁre neurons and
their use in a sound analysis system. Analog Integr Circuits Signal Processing 30(2):91–100
41. Goldberg DH, Cauwenberghs G, Andreou AG (2001) Probabilistic synaptic weighting in a
reconﬁgurable network of VLSI integrate-and-ﬁre neurons. Neural Net 14:781–793
42. Gorchetchnikov A, Hasselmo ME (2005) A biophysical implementation of a bidirectional
graph search algorithm to solve multiple goal navigation tasks. Connect Sci 17(1–2):145–166
43. GorchetchnikovA, Versace, M,Ames H, Chandler B, Léveillé J, Livitz G, Mingolla E, Snider
G, Amerson R, Carter D, Abdalla H, Qureshi MS (2011a) Review and uniﬁcation of learning
framework in Cog Ex Machina platform for memristive neuromorphic hardware. Proceedings
of the international Joint Conference on neural networks, pp 2601–2608

58
M. Versace et al.
44. Gorchetchnikov A, Léveillé J, Versace M, Ames HM, Livitz G, Chandler B, Mingolla E,
Carter D, Amerson R, Abdalla H, Qureshi S, Snider G (2011b) MoNETA: massive parallel
application of biological models navigating through virtual Morris water maze and beyond.
BMC Neurosci 12(Suppl 1):310
45. Grossberg S (1973) Contour enhancement, short-term memory, and constancies in reverber-
ating neural networks. Stud Appl Math 52:213–257
46. Haﬂiger P (2007) Adaptive WTA with an analog VLSI neuromorphic learning chip. IEEE
Trans Neural Netw 18(2):551–572
47. Hamilton TJ, Jin C, van Schaik A, Tapson J (2008) An active 2-D silicon cochlea. IEEE Trans
Biomed Circuits Syst 2(1):30–43
48. Hodgkin AL, Huxley AF (1952) Currents carried by sodium and potassium ions through the
membrane of the giant squid axon of loligo. J Phys 116:449–472
49. Hsu D, Figueroa M, Diorio C (2002) Competitive learning with ﬂoating-gate circuits. IEEE
Trans Neural Netw 13:732–744
50. Indiveri G (1998) Analog VLSI model of locust DCMD neuron response for computation of
object approach. In: Smith L, Hamilton A (eds) Neuromorphic systems: engineering silicon
from neurobiology. World Scientiﬁc, Singapore, pp 47–60
51. Indiveri G, Murer R, Kramer J (2001) Active vision using an analog VLSI model of selective
attention. IEEE Trans Circuits Syst II 48(5):492–500
52. Indiveri G, Chicca E, Douglas RJ (2004) A VLSI reconﬁgurable network of integrate-and-
ﬁre neurons with spike-based learning synapses. European symposium on artiﬁcial neural
networks, pp 405–410
53. Indiveri G, Chicca E, Douglas RJ (2006) A VLSI array of low-power spiking neurons and
bistable synapses with spike-timing dependent plasticity. IEEETrans Neural Netw 17(1):211–
221
54. Indiveri G, Chicca E, Douglas RJ (2009) Artiﬁcial cognitive systems: from VLSI networks
of spiking neurons to neuromorphic cognition. Cognitive Comput 1:119–127
55. Indiveri G, Linares-Barranco B, Hamilton TJ, van Schaik A, Etienne-Cummings R, Delbruck
T, Liu S-C, Dudek P, Häﬂiger P, Renaud S, Schemmel J, Cauwenberghs G, Arthur J, Hynna
K, Folowosele F, Sa¨ighi S, Serrano-Gotarredona T, Wijekoon J, Wang Y, Boahen K (2011)
Neuromorphic silicon neuron circuits. Front Neurosci 5:73
56. Itti L, Koch C, Niebur E (1998) A model of saliency-based visual attention for rapid scene
analysis. IEEE Trans PAMI 20:1254–1260
57. Izhikevich EM, Edelman GM (2008) Large-scale model of mammalian thalamo-cortical
systems. PNAS 105:3593–3598
58. Jo SH, Chang T, Ebong I, Bhadviya BB, Mazumder P, Lu W (2010) Nanoscale memristor
device as synapse in neuromorphic systems. Nano Lett 10(4):1297–1301
59. Johnson RH, Hanna, GR (1969) Membrane model: a single transistor analog of excitable
membrane. J Theor Biol 22:401–411
60. Karplus WJ, Soroka WW (1959) Analog Methods: computation and Simulation. McGraw-
Hill, New York
61. Kogge P (2011) The tops in FLOPS. IEEE Spectr 48(2):48–54
62. Koickal TJ, Hamilton A, Tan SL, Covington JA, Gardner JW, Pearce TC (2005) Analog VLSI
circuit implementation of an adaptive neuromorphic olfaction chip. IEEE Int Symp Circuits
Syst 54:60–73
63. Lapique L (1907) Sur l’excitation electrique des nerfs. J Physiol 9:620–635
64. Lazzaro J, Mead C (1989a) Silicon modeling of pitch perception. Proc Natl Acad Sci USA
86(23):9597–9601
65. Lazzaro J, Mead C (1989b)A silicon model of auditory localization. Neural Comput 1(1):47–
57
66. Lazzaro J,Wawrzynek J (1997) Speech recognition experiments with silicon auditory models.
Analog Integr Circuits 13:37–51
67. Léveillé J, Ames H, Chandler B, Gorchetchnikov A, Livitz G, Versace M Mingolla E (2011)
Object recognition and localization in a virtual animat: large-scale implementation in dense

4
Persuading Computers to Act More Like Brains
59
memristive memory devices. Proceedings of the international joint conference on neural
networks
68. Lewis ER (1968) An electronic model of the neuroelectric point process. Kybernetik 5:30–46
69. Lichtsteiner P, Posch C, Delbruck T (2008) A 128 × 128 × 120db 15 μs latency asyn-
chronous temporal contrast vision detector. IEEE J Solid-State Circuits 43(2):566–576
70. Liu W, Andreou AG, Goldstein MH, Jr (1993a) Analog cochlear model for multire solution
speech analysis. Adv Neural Inf Processing Syst 5:666–673
71. Liu W, Andreou AG, Goldstein MH, Jr (1993b) Voiced speech representation by an analog
silicon model of the auditory periphery. IEEE Trans on Neural Net 3(3):477–487
72. Liu S-C, Delbruck T (2010) Neuromorphic sensory systems. Curr Opin Neurobio 20:288–295
73. Liu S-C, Kramer J, Indiveri G, Delbruck T, Douglas R (2002) Analog VLSI: circuits and
principles. MIT Press, Cambridge
74. Liu S-C, Mesgarani N, Harris J, Hermansky H (2010) The use of spike-based representations
for hardware auditory systems. IEEE International symposium on circuits and systems, pp
505–508
75. Livitz G, Ames H, Chandler B, Gorchetchnikov A, Léveillé J, Vasilkoski Z, Versace M,
Mingolla E, Snider G, Amerson R, Carter D, Abdalla H, Qureshi MS (2011) Visually-
guided adaptive robot (ViGuAR). Proceedings of the international joint conference on neural
networks, pp 2944–2951
76. Lyon RF, Mead C (1988) An analog electronic cochlea. IEEE Trans Acoust 36(7):1119–1134
77. Mahowald M, Douglas R (1991) A silicon neuron. Nature 354(6354):515–518
78. Markram H (2006) The blue brain project. Nat Rev Neurosci 7:153–160
79. McKenzieA, Branch DW, Forsythe C, James CD (2010) Toward exascale computing through
neuromorphic approaches. Sandia Report SAND2010-6312, Sandia National Laboratories
80. Mead C (1989) Analog VLSI and neural systems. Addison-Wesley, Boston
81. Mead C, Mahowald MA (1988) A silicon model of early visual processing. Neural Netw
1(1):91–97
82. Merolla PA, Arthur JV, Shi BE, Boahen KA (2007) Expandable networks for neuromorphic
chips. IEEE Trans Circuits Syst I: Fundam Theory Appl 54(2):301–311
83. Minch BA, Hasler P, Diorio C, Mead C (1995) A silicon axon. In: Tesauro G, Touretzky DS,
Leen TK (eds) Adv Neural Inf Processing Syst 7. MIT Press, Cambridge, pp 739–746
84. Mitra S, Fusi S, Indiveri G (2009) Real-time classiﬁcation of complex patterns using spike-
based learning in neuromorphic VLSI. IEEE Trans Biomed Circuits Syst 3(1):32–42
85. Morris RGM (1981) Spatial localization does not require the presence of local cues. Learn
Motiv 12(2):239–260
86. Navaridas J, Lujan M, Miguel-Alonso J, Plana LA, Furber S (2009) Understanding the
interconnection network of SpiNNaker. Proceedings of the international conference on
supercomputing, p 286
87. Nogaret A, Lambert NJ, Bending SJ Austin J (2004) Artiﬁcial ion channels and spike
computation in modulation-doped semiconductors. Europhys Lett 68(6):874–880
88. Northmore DPM. Elias JG (1996) Spike train processing by a silicon neuromorph: the role
of sub linear summation in dendrites. Neural Comput 8(6):1245–1265
89. Oster M, Liu SC (2004) A winner-take-all spiking network with spiking inputs. Proceedings
of 11th IEEE international conference on electronics, circuits, and systems, pp 1051–1058
90. Pearce TC (1997) Computational parallels between the biological olfactory pathway and its
analogue ‘the electric nose’: sensor based machine olfaction. Biosystems 41(2):69–90
91. Pearson M, Nibouche M, Gilhespy I, Gurney K, Melhuish C, Mitchison B, Pipe AG (2006) A
hardwarebasedimplementationofatactilesensorysystemforneuromorphicsignalprocessing
applications. Proceedings of IEEE international conference on acoustics, speech, and signal
processing, p 4
92. Pickett MD, Strukov DB, Borghetti JL,Yang JJ, Snider GS, Stewart DR, Williams RS (2009)
Switching dynamics in titanium dioxide memristive devices. J Appl Phys 106(7):074508
93. Posch C, Matolin D,Wohlgenannt R (2010)A QVGA 143 dB DR asynchronous address-event
PWM dynamic image sensor with lossless pixel-level video compression. ISSCC digest of
technical papers, pp 400–401

60
M. Versace et al.
94. Rasche C, Douglas RJ (2000) An improved silicon neuron. Analog Integr 23(3):227–236
95. Rasche C, Douglas RJ (2001) Forward- and back propagation in a silicon dendrite. IEEE
Trans Neural Netw 12(2):386–393
96. Rasche C, Douglas RJ, Mahowald M (1998) Characterization of a silicon pyramidal neu-
ron. In: Smith LS, Hamilton A (eds) Neuromorphic systems: engineering silicon from
neurobiology. World Scientiﬁc, Singapore, pp 169–177
97. Roy G (1972) A simple electronic analog of the squid axon membrane: the neuro FET. IEEE
Trans Biomed Eng BME-18:60–63
98. Roy, D (2006) Design and developmental metrics of a ‘skin-like’mutli-input quasi-compliant
robotic gripper sensor using tactile matrix. J Intell Robot Syst 46(4):305–337
99. Ruedi PF, Heim P, Kaess F, Grenet E, Heitger F, Burgi PY, Gyger S, Nussbaum P (2003) A
128 × 128 pixel 120-dB dynamic-range vision-sensor chip for image contrast and orientation
extraction. IEEE J Solid-State Circuits 38:2325–2333
100. Runge RG, Uemura M, Viglione SS (1968) Electronic synthesis of the avian retina. IEEE
Trans Biomed Eng BME-15:138–151
101. Russell A, Orchard G, Dong Y, Mihalas S, Niebur E, Tapson J, Etienne-Cummings R
(2010) optimization methods for spiking neurons and networks. IEEE Trans Neural Netw
21(12):1950–1962
102. Samardak A, Nogaret A, Taylor S, Austin J, Farrer I, Ritchie DA (2008) An analogue sum and
threshold neuron based on the quantum tunneling ampliﬁcation of neural pulses. New J Phys
10
103. Schemmel J, Fieres J, Meier K (2008) Wafer-scale integration of analog neural networks.
Proceedings of the IEEE joint conference on neural networks, pp 431–438
104. Serrano-Gotarredona R, Oster M, Lichtsteiner P, Linares-BarrancoA, Paz-Vicente R, Gomez-
Rodriguez F, Riss HK, Delbruck T, Liu S-C, Zahnd S, Whatley AM, Douglas R, Haﬂiger P,
Jimenz-Moreno G, Civit A, Serrano-Gotarredona T, Acosta-Jimenez A, Linares-Barranco B
(2006) AER building blocks for multi-layer multi-chip neuromorphic vision systems. In:
Becker S, Thrun S, Obermayer K (eds) Advances in neural information processing systems
15. MIT Press, Cambridge, pp 1217–1224
105. Serrano-Gotarredona R, Oster M, Lichtsteiner P, Linares-BarrancoA, Paz-Vicente R, Gomez-
Rodriguez F, Camunas-Mesa L, Berner R, Rivas M, Delbruck T, Liu S-C, Douglas R, Haﬂiger
P, Jimenez-Moreno G, CivitA, Serrano-Gotarredona T,Acosta-JimenezA, Lineares-Barranco
B (2009) CAVIAR: a 45 k-neuron, 5 M-synapse, 12G-connects/s AER hardware sensory-
processing-learning-actuating system for high speed visual object recognition and tracking.
IEEE Trans Neural Netw 20(9):1417–1438
106. Shurmer HV, Gardner JW (1992).Odor discrimination with an electric nose. Sens Actuators
B-Chemical, 8(11):1–11
107. Smith LS (2008) Neuromorphic systems: past, present, and future. In: Hussain A et al., (eds)
Brain inspired cognitive systems, advances in experimental medicine and biology, 657. MIT
Press, Cambridge, pp 167–182
108. Snider GS (2007) Self-organized computation with unreliable, memristive nanodevices.
Nanotechnology 18(36):36502
109. Snider GS (2008) Spike-timing-dependent learning in memristive nanodevices. IEEE/ACM
International symposium on nanoscale architectures, pp 85–92
110. Snider GS (2011) Instar and outstar learning with memristive nanodevices. Nanotechnology
22:015201
111. Snider G,Amerson R, Carter D,Abdalla H, Qureshi S, Léveillé J,Versace M,Ames H, Patrick
S, Chandler B, Gorchetchnikov A, Mingolla E (2011) Adaptive computation with memristive
memory. IEEE Comput 44(2):21–28
112. Strukov DB, Snider GS, Stewart DR, Williams SR (2008) The missing memristor found.
Nature 453:80–83
113. Vainbrand D, Ginosar R (2010) Network-on-chip architectures for neural networks. IEEE
international symposium on networks-on-chip, pp 135–144

4
Persuading Computers to Act More Like Brains
61
114. Van Schaik A (2001) Building blocks for electronic spiking neural networks. Neural Netw
14(6–7):617–628
115. Van Schaik A, Vittoz E (1997) A silicon model of amplitude modulation detection in the
auditory brainstem. Adv NIPS 9:741–747
116. Vasarhelyi G, Adam M, Vazsonyi E, Kis A, Barsony I, Ducso C (2006) Characterization of an
integrable single-crystalline 3-D tactile sensor. IEEE Sens J 6(4):928–934
117. Versace M Chandler B (2010) MoNETA: a mind made from memristors. IEEE Spectr 12:30–
37
118. Vogelstein R, Malik U, Culurciello E, Cauwenberghs G, Etienne-Cummings R (2007a)A mul-
tichip neuromorphic system for spike-based visual information processing. Neural Comput
19(9):2281–2300
119. Vogelstein R, Malik U, Vogelstein J, Cauwenberghs G (2007b) Dynamically reconﬁgurable
silicon array of spiking neurons with conductance-based synapses. IEEE Trans Neural Netw
18(1):253–265
120. Watts L, Kerns D, Lyon R, Mead C (1992) Improved implementation of the silicon cochlea.
IEEE J Solid-State Circ 27(5):692–700
121. Wijekoon, JHB, Dudek, P (2008) Compact silicon neuron circuit with spiking and bursting
behavior. Neural Netw 21:524–534
122. Wolpert S, Micheli-Tzanakou E (1996) A neuromime in VLSI. IEEE Trans Neural Netw
7(2):300–306
123. Xia Q, Robinett W, Cumbie MW, Banerjee N, Cardinali TJ,Yang JJ, Wu W, Li X, Tong WM,
Strukov DB, Snider GS, Medeiros-Ribeiro G, Williams RS (2009) Memristor/CMOS hybrid
integrated circuits for reconﬁgurable logic. Nano Lett 9(10):3640–3645
124. Yang Z, MurrayAF,Woergoetter F, Cameron KL, BoonobhakV (2006)A neuromorphic depth-
from-motion vision model with STDP adaptation. IEEE Trans Neural Netw 17(2):482–495
125. Yang JJ, Pickett MD, Li X, Ohlberg DAA, Stewart DR, Williams RS (2008) Memristive
switching mechanism for metal/oxide/metal nanodevices. Nature Nanotechnol 3:429–433
126. Zaghloul KA, Boahen K (2006) A silicon retina that reproduces signals in the optic nerve. J
Neural Eng 3:257–267


Chapter 5
Memristors for More Than Just Memory:
How to Use Learning to Expand Applications
Paul J. Werbos
Abstract There has been a huge explosion of interest in the memristor since the ﬁrst
experimental conﬁrmation by HP in 2008 (Dmitri et al., Nature 453:80–83, 2008).
Because the memristor and its variants provide a huge increase in memory density,
compared with existing technologies like ﬂash memory, many of us expect that they
will move very quickly to a huge and important market in the memory area. But what
about other large-scale markets and applications? What is the pathway which could
open up those larger markets? The purpose of this chapter is to discuss what would
be needed to capture those larger markets.
There has been a huge explosion of interest in the memristor since the ﬁrst exper-
imental conﬁrmation by HP in 2008 [1]. Because the memristor and its variants
provide a huge increase in memory density, compared with existing technologies
like ﬂash memory, many of us expect that they will move very quickly to a huge and
important market in the memory area. But what about other large-scale markets and
applications? What is the pathway which could open up those larger markets? The
purpose of this chapter is to discuss what would be needed to capture those larger
markets.
Many of the points which I will make here are essentially old and obvious, for
those who have closely followed the history of the past years—yet many considered
the memristor an old [2] and simple idea of no great interest until HP’s follow-on
actions published in 2008. Much of the research on memristors today is not directly
addressing what is needed to capture the larger market possibilities; there is still
some need to recall some basic principles. Among the key points are:
•
The value of learning versus programming, to allow more rapid development
of large-scale new applications in tasks which can be mapped into some kind
of prediction, decision-making or emulation of humans or other systems; this
includestheadaptivemanagementoflarge-scaleinfrastructurefromtoptobottom;
The views expressed here are the personal views of the author, not the ofﬁcial views of NSF;
however, this chapter does constitute work by a government employee on government time.
P. J. Werbos ()
Energy, Power and Adaptive Systems, National Science Foundation, Arlington, VA, USA
e-mail: pwerbos@nsf.gov
R. Kozma et al. (eds.), Advances in Neuromorphic Memristor Science and Applications,
63
Springer Series in Cognitive and Neural Systems,
DOI 10.1007/978-94-007-4491-2_5, © Springer Science+Business Media Dordrecht 2012

64
P. J. Werbos
•
Recent breakthroughs and fundamental principles for learning in massively
parallel systems;
•
The hazards of trying to go directly from neuroscience and physics to chips,
without developing more solid, functional mathematical tools to connect these
realms, such as the tools sought in NSF’s recent research thrust in cognitive
optimization and prediction;
•
The need to put more effort into optimization methods more compatible with
massive parallelism, along a pathway ranging from nonlinear interior point meth-
ods and brain-like stochastic search through to more general forms of adaptive
dynamic programming.
5.1
Learning Versus Programming For Massively Parallel
Computing
In essence, the memristor, when combined with other active processing components,
will allow us to achieve a density of functional features or devices on-board chips
which is far greater than what we have had in the past. This is an important break-
through, but it is basically just an extension of earlier breakthroughs in hardware for
massively parallel processing.
In 1987, the National Science Foundation became interested in another such
breakthrough, in optical computing. Leaders of the optical computing movement
cametoNSFandarguedthattheycouldachieveordersofmagnitudebettercomputing
power per dollar than traditional computing. Given the sheer size of the computing
industry, this seemed like a huge opportunity, but NSF did some further investigation.
The critics of optical computing argued that it could be great for some limited
niche tasks in computing like matrix multiplication, but that it would be useless
for the bulk of computing, where people wanted to run serial programs like old
FORTRAN programs. Then at some point, NSF discussed the issue with Carver
Mead, the “father of VLSI” [3], who gave a brief summary of a different point of
view (later discussed in [4, 5]). The key points were:
1. Massive parallelism is possible in chips, not just optics, if we can live with large
numbers of simple processors.
2. Somehow, the human brain manages to address a very wide variety of computa-
tional tasks within the constraints of massive parallelism. The human brain is not
a niche machine. If we can learn how the human brain does this, and capture this
functional capability in a broad and useful way, we will not be limited to niche
markets here.
At this point, the NSF Program Director for optical technology recommended
that a new research program be created on “neuroengineering” aimed at reverse-
engineering the key functional capabilities of the brain relevant to large-scale
computing. I was brought on to NSF to run that program, based in part on the
central role of backpropagation [6, 7] in reviving the neural network ﬁeld.

5
Memristors for More Than Just Memory: How to Use Learning to Expand Applications
65
Fig. 5.1 Iconic representation
of ﬁve grand challenges.
(© Paul J. Werbos)
Of course, the brain is not preprogrammed to do most of the many things it learns
to do. The key to the power of the brain is learning. Furthermore, the brain as a whole
system was evolved to learn just one thing—how to learn what to do, what actions
to take, in order to maximize something provided by biology more basic than our
intelligence. That “something” is very important to science, but for purposes of this
chapter I will just call it the primary motivational system [8]; here, I will focus on
the learning part, which I will call “the intelligence in the brain.” The intelligence
in the brain, as a whole system, is basically “just” an intelligent controller. Other
aspects of intelligence, like memory and expectations and learned feelings, are all
just subsystems or attributes of that controller. In order to understand their functions
and their design, it is necessary to understand how they ﬁt in to the system as a
whole—how they ﬁt in as subsystems for intelligent decision and control.
In practice, research in this ﬁeld tends to be split between research which addresses
the general task of learning-to-predict, in its many forms, and learning to act. Thus
in 2007, when I proposed that the NSF Engineering Directorate provide special new
funding to support reverse-engineering the brain, we ended up calling it “cognitive
optimization and prediction” (COPN [9]).
The key research challenges in COPN are symbolized in Fig. 5.1; they include
cognitive optimization and prediction, drawing on basic mathematical principles, as
symbolized on the left, and the useful application of the resulting knowledge and
designs to larger grand challenges (and markets) symbolized on the right. Not all
computing tasks can be translated into tasks in prediction or in optimal decision-
making, but a very large fraction of them can. For example, there those who believe
that all the computational tasks faced in a large corporation could be mapped ad-
equately into the task of trying to act so as to maximize proﬁt—a task in optimal
decision-making.
The key goals for cognitive optimization and prediction (COPN), symbolized by
the left side of Fig. 5.1, are very discrete and well-deﬁned [9]. In my view [10], the
most important grand challenge for basic science in this century is to understand
and replicate the type of learning algorithms which allow even the smallest mammal
brains to learn to predict and decide as effectively as they can do, in the face of great
complexity, noise and nonlinearity. Great progress has been achieved in these areas

66
P. J. Werbos
(see [11–14] and IEEE conferences on Reinforcement Learning and Approximate
Dynamic Programming, RLADP), and the road ahead has already been mapped. But
in the complicated world we live in, we will need to maintain focus and vision and
memory in order to travel that road all the way to meeting that grand challenge. When
we ﬁnally do, it will be as important and as solid and (in some ways) as simple as
Newton’s or Einstein’s laws of gravity.
The left side of Fig. 5.1 suggests that the two great “rivers” of cognitive optimiza-
tion and cognitive prediction are fed by the merger of many streams, coming from
the high mountains of basic mathematical principles. The most important of these are
symbolized on the far left, and discussed in more detail in [11–14]. Starting from the
top, these are: (1) Bayes’ Law, a key foundation for cognitive prediction [14] going
well beyond the simplistic versions of Bayesian networks and Bayesian regression
which have become commonplace in elementary computer science; (2) associative or
content-addressable memory; (3) clustering [15], which can create important proto-
types for memory; (4) the Bellman equation, the foundation of RLADP [11–15]; and
(5) the chain rule for ordered derivatives, the original and generalized form of back-
propagation in neural networks which is also the origin of “adjoint” methods widely
used in tasks from reservoir management and aircraft design to climate modeling
[7, 16, 17].
The right hand side symbolizes the importance of basic research to harness these
methods as effectively to three large application domains of special importance to
humanity: (1) maximizing the chances of economically sustainable human settlement
of space beyond the planet earth [18, 19]; (2) maximizing the probability that we get
to a survivable or sustainable state here on earth, for example by achieving a power
grid intelligent enough to make use of low-cost renewable energy and pluggable
electric vehicles [20]; (3) harnessing the new mathematical understanding to try to
attain a higher level of expression of human potential and cooperation, symbolized
by the rose (an old Western symbol for human inner potential) on top of the yin-yang
(an important Asian symbol) [21].
The basic mathematic task here is to understand what kinds of learning dynamics
allow maximum capability in cognitive prediction and optimization, making full use
of massively parallel hardware in general. That fundamental task is what some of us
have been working on for decades, with very signiﬁcant accomplishments along the
way.
5.2
Lessons Learned About Learning and Massive Parallelism
The political and social obstacles to meeting the COPN grand challenge are more
difﬁcult than the objective technical challenges, in many ways.
For example, at about 1990, Senator Gore pushed a new High Performance Com-
puting bill which was aimed at solving the algorithm and architecture and application
problems which limited the use of massive parallel computing. Early version of the
bill did include some activity in neuroengineering, but many “big stakeholders”

5
Memristors for More Than Just Memory: How to Use Learning to Expand Applications
67
viewed this new paradigm as a threat, not an opportunity, and had it deleted. (Indeed,
it is not so rare for government funding decisions to be biased by vested interests
trying to eliminate possible competition.) Big machines were built which were very
useful in solving partial differential equations (PDE), and huge efforts were mounted
to teach people to do basic programming on parallel machines. Many useful things
were accomplished, but it was essentially a niche market. Some very important new
tools appeared for PDE modeling, important to climate modeling and reservoir man-
agement (e.g. enabling “fracking”), brought onto that ﬁeld by a researcher I funded
at Oak Ridge National Laboratory using backpropagation under another name [16].
Circa 1990, there was a lot of public discussion of the new Intel analog neural
network chip, in the spirit of Carver Mead, which never reached a large market,
primarily because of worries about variability of weights and quality control in the
analog domain, and the lack of strong market push for the extra power of analog
over digital. Motorola studied these markets in more detail, and developed a plan to
developaﬂexibledigitalneuralnetworkchips, forwhichtheyenvisionedasubstantial
market in embedded control applications and applications like image processing in
PCs; that plan seemed to be making progress, but there were major cutbacks in
Motorola research in Arizona, and there may have been problems in keeping up with
new competition.
By 1998, two groups at Ford Research led by Lee Feldkamp and Kenneth Marko
had shown that neural network intelligent prediction and control (using backpropa-
gation through time) could meet new requirements of the Clean Air Act much more
easily and cheaply than any other solution available, after the industry had spent
huge amounts of money trying everything else they could think of. Mosaix LLC
(associated with the NASA Jet Propulsion Lab) developed a dedicated but ﬂexi-
ble digital chip which could implement those calculations, at an estimated cost of
US$ 1–US$ 10 per chip in mass production. In 1998, in BusinessWeek, the President
of Ford highlighted that work, and announced that every new Ford car in the world
would soon carry this kind of neural network chip. Ford and Mosaix announced an
agreement, and perhaps this was part of the competition which affected Motorola’s
plans.
But the situation was then muddled in two ways. First, the mainstream electron-
ics industry, following Moore’s Law, started selling new chips which allowed Ford
to implement the neural network algorithms needed for clean air on the chips they
already were installing, without adding a custom chip. And the Bush Administration
reconsidered many aspects of clean air. Nevertheless, this technology was main-
tained and replicated in other automobile companies. A former member of the Ford
group, Danil Prokhorov, has used similar neural network methods to vastly improve
efﬁciency of the Prius hybrid [22]. But, thanks to Moore’s Law, no new chips were
needed. Kenneth Marko has argued that better service and diagnostics for cars will
be able to make full use of greater computational bandwidth, but the auto industry
has waxed and waned in its level of concern about service and diagnostics, especially
through the 2008 economic shock.
For many years, there was great pessimism about hardware for learning on parallel
computing platforms. The feeling was that Moore’s Law would keep increasing speed

68
P. J. Werbos
so quickly that it could keep up with new algorithms—even algorithms which are
inherently parallel—to the point where large scale markets for such platforms would
always be limited.
But just in the last few years, that pessimism has ﬁnally begun to evaporate. As
CPUs based on silicon technology peaked out at about 2 or 3 GHz, in mainstream
mass market computing, Moore’s Law has suddenly become mainly a matter of more
cores per chip, looking to a future where our best new options all involve hundred to
millions of processors per chip. Suddenly, all that work on how to make full use of
that kind of capability is back to center stage. The solutions which have been quietly
developed to address that challenge are now of ﬁrst order practical importance. But
will we carry them further, or waste time and money in fumbling attempts to reinvent
the wheel, perhaps by giving more funding to the followers of people who said that
it would be impossible even to train the weights of a simple multilayer perceptron
years ago? Faster speed may make a brief comeback (due to graphene or even InAs),
but greater feature density is now the main direction for progress.
Another key development supporting massive parallelism is the Cellular Neural
Network (CNN), also due to Leon Chua. (The CNN should not be confused with the
Convolutional Neural Network, the Cellular Simultaneous Recurrent Network, or
the ObjectNet, though the relations between these four architectures are important.)
Even before HP’s work, Chua and Roska were growing more and more successful
in ﬁnding excitement and applications for this type of massively parallel chip. The
primary applications have been in high-performance image processing (e.g. for the
missile defense agency), where the massive computing throughput is essential and
people have learned through laborious effort how to program weights onto these
chips to address certain image processing tasks.
At the CLION center at the FedEx Institute, Kozma and I have collaborated with
Roska and Chua, in an effort to combine the best learning capabilities with the best
CNN capabilities. The idea was to port over some of the most powerful learning
algorithms onto a kind of “universal CNN compiler,” so that they could run on any
CNN chip, present or future, and provide a seamless path for migration as new and
more powerful chips are developed. Unfortunately, the students working on this
reported that the existing compilers are not ﬂexible enough to allow this. This is why
I am extremely excited by the new announcement from HP about “Cog Ex Machina”
which, I hope, will ﬁnally allow a more effective uniﬁcation of progress in COPN
and progress in chips, and a path to quick deployment of combinations of the best
of the two.
In the meantime, there has been some startling progress in computer vision, due
to work by LeCun and by Ng under an award from COPN [14, 23, 24]. Using fast
multicore game machines, LeCun has implemented some relatively simple general
neural network learning systems, which have broken the world’s records on many
long-standing pattern recognition challenges, all in one year, using essentially the
same universal learning system. At the International Joint Conference on Neural
Networks in San Jose (IJCNN2011), results were presented for four new benchmark
competitions, each attacked by hundreds of groups worldwide, using more traditional
pattern recognition methods and statistics andVapnik type methods—but in all cases

5
Memristors for More Than Just Memory: How to Use Learning to Expand Applications
69
best performance was attained by a relatively simple new universal neural network
machine. When the early COPN successes ﬁrst came out, DARPA initiated a major
“third neural network” program, which gave large follow-on grants to Ng, to LeCun,
and an evaluation grant to Isabel Guyon (who had been funded by NSF to develop
these competitions, in partnership with Sven Crone of the UK).
For the moment, it is not necessary to choose between best performance “per
ﬂop” and compatibility with parallel computing in these tasks. However, LeCun has
noted that the people who have spent decades of their lives on more narrow domain-
dependent tools are not likely to just resign overnight. Sheer numbers of people
using simple easy tools in Matlab can often outweigh the issue of trying to get better
performance, especially in the university world. If a learning tool “discovers” a
feature, there is no barrier to keep people from reinventing and slightly tweaking that
feature, and presenting it as new “domain speciﬁc” tool. With a hundred tools, one
can even put together an anthology in which each researcher gets to have his name
on one unique tool. This general kind of entropy is a major barrier to progress in this
ﬁeld. On the other hand, the successes by LeCun so far are clearly on a path which
allows even more substantial applications and capabilities in the future [14]. The new
“deep learning” movement is still relatively small, but one could say the same thing
about the PDP group in the early 1980s. The connection to new computing hardware
may improve the situation for this kind of work.
In the initial proposal for COPN (which was voted on by all program directors
in the Engineering Directorate who attended the Emerging Frontiers town hall),
I argued that engineers alone and neuroscientists alone were on a path to never
meeting the grand challenge here, under the social dynamics we can see at work in
both those disciplines. Engineers are not under pressure to develop massively parallel
learning systems, or to develop general domain-independent tools. Neuroscientists
are not under pressure to face up to the limitations of well-known models such as
spike-time dependent plasticity (STDP) or Grossberg’s form of Hebbian learning
in high performance functional tasks—tasks which brains learn to perform. COPN
was structured in a unique way, to overcome that kind of entropy. Inertia, tradition
and groupthink are very powerful in both communities, and probably much worse
in computer science. Unfortunately, with the politics of government funding being
what it is, it currently seems unlikely that any government funding program in the
world will be able to maintain the necessary focus. COPN itself was funded under
the Emerging Frontiers Research Initiation (EFRI) activity, for just one year, under
the usual EFRI rules.
5.3
From Biological Models Direct to Chips: A Popular Way
to Fall Off a Cliff
I was greatly excited to see the HP view of today’s biological learning models, in
the preface to this book. The fallacy of trying to jump directly from a simple model
to a narrow dedicated chip has proven to be a blind alley again and again. The

70
P. J. Werbos
key challenge we face is to understand what the mathematical principles are, ﬁrst,
which make it possible for biological or artiﬁcial neural networks to really work in
engineering terms, and then capture that after we understand the principles. Someday
we may know enough about that mathematics to get full value from heavily hardwired
analog designs, but we aren’t there yet.
In a way, things have not changed so much (and people have not learned so much)
since my ﬁrst discussions about backpropagation with Marvin Minsky circa 1970 [6].
I showed him how we could use backpropagation to train simple neural networks,
like the ones he discussed in his book on perceptrons [25], so long as we replaced
the binary variable with a continuous variable x from 0 to 1, and replaced the sign
function with a piecewise linear function [7]. “But real cells don’t output continuous
variables. They only output ones or zeroes. That’s what spikes are. Al the modelers
know that.”
I then pulled out an actual tracing of outputs from a higher neuron, showing that
it didn’t look anything like square wave. A typical higher neuron (like the axon of a
giant pyramid cell, in vivo) actually outputs “volleys” of varying intensity, at regular
time intervals. The non-spiking model is actually far more compatible with what’s
in the real biological data than the old square wave stuff. But Minsky felt it would
be impossible to survive politically as coauthor of any paper which made such a
clear attack upon the religious doctrines of the modelers of the time, data or no data,
biology or no biology.
And even today, there is a common doctrine among modelers that “there are no
clocks in the brain.”Yet work by Llinas has shown that there is an incredible degree
of precision in some of the clocks in the brain. The ﬁbers containing timing signals
from the nonspeciﬁc thalamus to the giant pyramid cells of the cerebral cortex have
been well established for many decades [26, 27]. Recent work by Barry Richmond
of NIH on “neural codes” basically shows the same kind of patterns I showed to
Minsky.
Certainly there have been experiments showing that STDP captures certain local
features of synapse adjustment in brains, but it seems highly unlikely that it captures
enough of the story to allow us to replicate the kind of functional capabilities which
LeCun has demonstrated. For example, there are other factors within the cell or its
environment which affect the intensity of the effect, and there are “consolidation”
effects (like the “homeostatis” effect observed by Potter’s group under COPN, which
is far form being fully understood) which are also important. At the latest COPN
workshop in 2011, Barry Richmond of NIH reported that the dopamine distribution
in real-world reinforcement learning is grossly at variance with the oversimpliﬁed
reinforcement learning models in vogue in neuroscience today, but he also reported
problems in getting modelers to pay attention to the empirical data. Computational
neuroscience ﬁlls a very important niche, but on its own it does not seem to have the
motivation yet to pull together the real mathematical challenges of networks which
work in an engineering sense and hard-core biology, without a new stimulus like
COPN.

5
Memristors for More Than Just Memory: How to Use Learning to Expand Applications
71
5.4
Optimization—An Emerging Market
Neural network control has already achieved breakthroughs, even more striking than
LeCun’s, in a number of optimal control applications [11–13], such as applications to
cars, airplanes, turbogenerators, fabrication of carbon-carbon parts for aircraft, and
hit-to-kill missile interception. The benchmark data were not so widely published
in these cases, but the improvement in performance was much larger, and the large-
scale beneﬁts quite tangible. However, all of these applications involved relatively
small systems. Except for a couple of early applications (Venayagamoorthy’s wide
area power controller developed under COPN and Fogel’s chess player), they have
addressed systems which were very tricky but ultimately limited to one or two dozen
state variables. This limited the need for use of new computational hardware.
This raises the question: what can we do to address very complex large-scale
applications, such as the management of complex infrastructures over time [19, 20,
28]? For example, management of the power grid is a very pressing issue. The
Independent System Operators (ISO) today already use optimization algorithms to
send instructions to all the individual generators in their very large power systems
but their ability to “solve the right problem” is limited by the speed of existing
algorithms. This in turn makes costs unnecessarily high, makes it hard to exploit the
full switching capability in the existing networks (let alone exploit the potential of
new switching which many propose), and makes it especially hard to make greater
use of renewables and storage on the grid, which may be necessary to our very
survival in the long-term. The new White House Smart grid policy asserts: “NSF
is currently supporting research to develop a ‘4th generation intelligent grid’ that
would use intelligent system-wide optimization to allow up to 80% of electricity
to come from renewable sources and 80% of cars to be pluggable electric vehicles
(PEV) without compromising reliability, and at minimum cost to the Nation (Werbos
2011).” [20]
At present, there is widespread use of a package called Gurobi for Mixed Integer
Linear Programming (MILP), which uses special heuristics to solve that limited
class of problems very effectively, using the old simplex method, which has been
parallelized enough to fully exploit 4–14 parallel processors in that speciﬁc task.
We have found ways to adapt that kind of software for multistage optimization
problems, so as to allow more foresight and “scarcity pricing,” by hooking it up with
modern ADP methods [11–13, 29], requiring their cooperation. But it may be that
full use of massively parallel computing platforms requires a more dramatic break
with the past, which cannot be incremental. As a ﬁrst step, nonlinear interior point
optimizationmethods(whichworkedbetterthansimplexbeforetheGurobiheuristics
were developed) could be extended with their own heuristics, in a more open source
way. There are several interesting leads to build on in this area. Those methods are
more compatible with massive parallelism, and can morph more easily into true
neural network methods; in fact, they can actually be seen as special cases of the
neural network optimization methods. Of course, neural networks and operations
research have a great deal to learn from each other in both directions. Important

72
P. J. Werbos
beginnings have been made [12, 13], but require more sustained and systematic
effort.
Going beyond these early results, Ganesh (Kumar) Venagamoorthy [30] recently
demonstrated a hardware version of a wide-area controller developed under COPN
[9], relying on adaptive critics, the most powerful form of ADP. To handle the
complexity of a wide-area system, he used a more general universal neural network
approximator, a special form of ObjectNet (The networks recently used by LeCun
et al are essentially a special case.) To handle the volume of computation, he used
a new hardware system made up of parallel FPGAs. At the present time, advanced
researchers are struggling with the difﬁculty of trying to optimize alternating current
ﬂows (ACOPF) in decisions made every 5–15 min. With the new type of hardware
and approximator, Kumar estimates that it is possible to do the same, and also add
foresight or anticipation, in decisions to be updated every 2 s [31]. This kind of real-
time optimization has the potential to change new power sources like onshore wind
from a regulation and stability burden to an asset, and vastly improve the economics
of shifting to a renewable world energy system.
More generally, it would be interesting to ﬁnd out whether the lessons learned in
the electric power sector could be extended to other sectors, such as world ﬁnancial
markets. It has taken many years to shake out automated algorithm-based markets
in the electric power sector, able to exploit the foresight of some human market
players without falling victim to instabilities and inefﬁciencies caused in the early
days by players like Enron. We are only now learning how new methods like SAP
can supplement human foresight, and allow scarcity pricing which can damp out spot
prices and prevent the more destructive forms of arbitrage. Could the same be done
in ﬁnancial markets, allowing more beneﬁts to ﬂow to genuine foresighted investors
and honest pension funds, with less leakage to wasteful forms of parasitic behavior?
There are many important possibilities to explore in this space.
References
1. Strukov DB, Snider GS, Stewart DR,Williams RS (2008, May 1)The missing memristor found.
Nature 453:80–83. doi:10.1038/nature06932 (Received 6 December 2007; Accepted 17 March
2008)
2. Chua LO (1971) Memristor—the missing circuit element. IEEE Trans Circuit Theory CT-
18(5):507–519. doi:10.1109/TCT.1971.1083337
3. Mead C, Conway L (1980) Introduction to VLSI systems. Addison-Wesley, Reading
4. Mead C (1989) Analog VLSI and neural systems. Addison-Wesley, Reading
5. Mead C (1990, October) Neuromorphic electronic systems. Proc IEEE 78(10):1629–1636
6. Anderson J, Rosenfeld E (eds) (1998) Talking nets. MIT Press, Cambridge
7. Werbos P (1994) The roots of backpropagation: from ordered derivatives to neural networks
and political forecasting. Wiley, New York
8. Werbos P (1998) Values, goals and utility in an engineering-based theory of mammalian
intelligence. In: Pribram KH (ed) Brain and values. Erlbaum, Hillsdale
9. http://www.nsf.gov/pubs/2007/nsf07579/nsf07579.htm
10. Werbos P (2009, April) Intelligence in the brain: a theory of how it works and how to build it.
Neural Networks 22(3):200–212. Related material is posted at www.werbos.com/Mind.htm

5
Memristors for More Than Just Memory: How to Use Learning to Expand Applications
73
11. White D, Sofge D (eds) (1992) Handbook of intelligent control. Van Nostrand, New York
12. Si J, Barto AG, Powell WB, Wunsch D (eds) (2004) Handbook of learning and approximate
dynamic programming (IEEE press series on computational intelligence). Wiley-IEEE Press,
New York
13. Lewis FL, Liu D (2012) Reinforcement learning and approximate dynamic programming for
feedback control, Wiley, New York
14. Werbos P (2011) Mathematical foundations of prediction under complexity, Erdos Lecture
series, 2011. http://www.werbos.com/Neural/Erdos_talk_Werbos_ﬁnal.pdf
15. Xu R, Wunsch DC (2008) Clustering. IEEE/Wiley Press, Hoboken
16. Werbos P (2005) Backwards differentiation in AD and neural nets: past links and new op-
portunities. In: Bucker HM, Corliss G, Hovland P, Naumann U, Norris B (eds) Automatic
differentiation: applications, theory and implementations. Springer, New York
17. Werbos P (1990, October) Backpropagation through time: what it does and how to do it. Proc
IEEE 78(10): 1550–1560 (Updated version reprinted in [7])
18. Werbos P (2009, October) Towards a rational strategy for the human settlement of space.
Futures 41(8): 547–553. Posted at http://www.werbos.com/E/Rational_Space_Policy.pdf
19. Werbos P (2012) Reminiscences on the loss of hope for primates in space, actual problems
of aviation and aerospace systems (RASJ), 1/17(34):67–81. http://www.kcn.ru/tat_en/science/
ans/journals/rasj.html
20. Werbos P (2011, August) Computation intelligence for the smart grid—history, challenges and
opportunities. IEEE Comput Intell Mag 6(3):14–21
21. Werbos P (2012) Neural networks and the experience and cultivation of mind. Neural Networks,
special issue based on IJCNN2011, in press, http://dx.doi.org/10.1016/j.neunet. 2012.02.026
22. Prokhorov D (2008, March–April) Prius HEV neurocontrol and diagnostics. Neural Networks
21(2–3):458–465
23. Kavukcuoglu K, Sermanet P, Boureau Y-L, Gregor K, Mathieu M, LeCun Y (2010) Learning
convolutional feature hierachies for visual recognition. In: Advances in neural information
processing systems (NIPS 2010)
24. LeCunY, Kavukvuoglu K, Farabet C (2010) Convolutional networks and applications in vision.
Proceedings of the international symposium on circuits and systems (ISCAS’10), IEEE, 2010
25. Minsky M, Papert S (1969) Perceptrons: an introduction to computational geometry. MIT
Press, Cambridge
26. Scheibel ME, Schiebel AB (1970) Elementary processes in selected thalamic and cortical
subsystems—the structural substrates. In: Schmitt FO (ed) The neurosciences: second study
program. Rockefeller University Press, New York
27. Foote SL (1987) Extrathalamic modulation of cortical function. Annu Rev Neurosci 10:67–95
28. Werbos L, Kozma R, Silva-Lugo R, Pazienza GE, Werbos P (2011) Metamodeling for large-
scale optimization tasks based on object networks, Proceedings of the International Joint
Conference on Neural Networks IJCNN2011, IEEE, 2011
29. Werbos L, Kozma R, Silva-Lugo R, Pazienza GE, Werbos P (2012) Metamodeling and critic-
based approach to multi-level optimization. Neural Networks, submitted by invitation to
IJCNN2011 special issue to be printed in 2012
30. Venayagamoorthy G (2011, August) Dynamic, stochastic, computational and scalable
technologies for smart grids. IEEE Comput Intell Mag 6(3):22–35
31. Liang J, Venayagamoorthy GK, Harley RG (2012, March) Wide-area measurement based
dynamic stochastic optimal power ﬂow control for smart grids with high variability and
uncertainty. IEEE Transactions on Smart Grid 3(1):59–69


Part II
Computational Models of Memristors


Chapter 6
Computational Intelligence and Neuromorphic
Computing Architectures
Robinson E. Pino
Abstract In the computational intelligence ﬁeld of study neuromorphic computing
is close to reaching critical mass to enable low power compact hardware systems to
perform intelligent cognitive functions. The fundamental technical challenge that has
prevented this technology from becoming reality is the development of a synthetic
synapse. In the biological brain, the synapse regulates the connection (conductiv-
ity) strength between neurons allowing the brain to re-wire itself as it processes
and obtains new knowledge and stores memories. In terms of device engineering,
the synapse correlates to a passive variable impedance electronic device. Today, the
synapse equivalent, a novel device theorized in 1971 by Professor Leon Chua, is a
reality as reported by the Nature paper titled “The memristor device found” in 2008.
Also, various material systems have demonstrated synaptic memristor behavior such
as the AgSEnSe memristor developed at Boise State University (BSU). However, as
we move forward to fabricating memristor arrays to implement complex cognitive
neuromorphic functions, it will become critical to understand and model the physical
principles underlying device process variation and device array statistical phenom-
ena. It is the goal of this chapter to explore emerging technologies to enable the
development of large scale memristor-based neuromorphic computing architectures.
6.1
Introduction
Computational intelligence and neuromorphic computing has demonstrated the abil-
ity to perform intelligent functions within large high performance computing clusters
[1]. However, the primary goal of neuromorphic computing architectures is to re-
alize the potential of emerging high performance computing architectures within
single core parallel computing processors. Thus, we have set out to study the fea-
sibility to fabricate such neuromorphic computing architecture primitives within a
single core computing processor. We have researched analog computing arrays that
exploit the electronic properties of the memristor devices, the physical analog to
the biological synapse. As research efforts pursue the prototyping of large scale de-
vices with millions of memristor-based computing elements, it will be critical to
R. E. Pino ()
Air Force Research Laboratory, Rome, New York, USA
e-mail: Robinson.Pino@gmail.com
R. Kozma et al. (eds.), Advances in Neuromorphic Memristor Science and Applications,
77
Springer Series in Cognitive and Neural Systems,
DOI 10.1007/978-94-007-4491-2_6, © Springer Science+Business Media Dordrecht 2012

78
R. E. Pino
understand and model the physical statistical variation that governs memristor array
device electronic operations. Once statistical physical variation phenomena are un-
derstood, it will be possible to simulate, optimize, and prototype, large scale single
core neuromorphic computing processors. Neuromorphic computing represents a
new intelligent information processing paradigm with the potential to solve a new
class of problems within sense-making, the ability to understand information and
work with incomplete or partial information to reach a conclusion. In recent years,
research investigations have demonstrated the feasibility and application of neuro-
morphic computing processors mainly within high performance computing cluster
instantiations to solve problems on:
a) Pattern, image, and sound recognition
b) Real-time and dynamic data analysis
Neuromorphic computers promise to solve a number of speciﬁc classes of problems
that clearly have not been solved by the leveraging of current state-of-the-art CMOS
digital computing architectures. Alternatively, there are growing expectations that
functions difﬁcult to perform efﬁciently even with large clusters of high performance
computers (HPC) or super computers will one day be accomplished with single core
neuromorphic processors that embody approaches to computing that are found in
the natural world. Neuromorphic processors may soon achieve such computing feats
given their inherent highly parallelized computing architecture, and their ability to
potentially autonomously learn or be trained for speciﬁc functions and tasks, such
as real time search, or analysis of archived data or surveillance video to identify
particular patterns, targets, individual persons, or objects. Currently, research efforts
have yielded signiﬁcant results in the study of neuromorphic architecture models [1–
4]. These various research efforts have focused on the research, development, and
implementation of computing architectures, device compact models and simulation,
and large scale applications of neuromorphic models aided by an HPC Cell Broad-
band Engine (BE) computer cluster. Thus, scientiﬁc results have been presented that
attest to the feasibility and performance of speciﬁc implementations of neuromorphic
architectures in the laboratory. Current neuromorphic hardware is mostly based on
CMOS technologies solely employed for the research and simulation of biological
brain models (i.e. neurons and synapses). In fact, one of the primary reasons why
neuromorphic processors are not available today is simply due to the high quantity
of devices required to realize meaningful applications, and the difﬁculty and expense
of assembling, programming, caring for, and supporting present day technologies.
Thus, we run into serious problems in terms of a high density of devices intercon-
necting with one another in a small physical area, and a complex system design that
must be overcome before fully automated systems could be realized.
Amongst the best recent hardware research and development efforts, memristor-
based technology utilized within highly parallel architectures, e.g. neuromorphic
architecture, promises to solve the density limiting technology factor by perform-
ing the function of a synapse with a single memristor device [5]. For example, to
re-create an ant’s brain, it would require about 300 million memristors and approx-
imately 500,000 transistors for neurons. This represents a signiﬁcant reduction in

6
Computational Intelligence and Neuromorphic Computing Architectures
79
the number of transistors required, by approximately a factor of about 8,000. If this
novel approach is proven to be feasible, it would represent a signiﬁcant technologi-
cal achievement and important step forward. For instance, two examples of today’s
current state-of-the-art technology, the Cell BE and the Pentium 4 Extreme Edition
840 processor, each have 234 and 250 million transistors [6]. Given that the new
memristor technology could potentially be stacked on top of the transistors employ-
ing novel nano scaled crossbar architectures [7, 8], it would become possible to
design chips containing the equivalent of 250 million transistors/(500 transistors per
neuron including driving circuitry) = 2.5e8/5e2 = 5e5 or 500,000 functional neurons,
far larger than the example ant sized brain of 500,000 neurons. However, it will be
important to explore the application of memristive and neuronal circuit device tech-
nologies in the context of non-Von-Newman computing models such as Synaptic
Time Dependent Plasticity (STDP), or so-called “spiky neural network” models and
systems. These emerging technologies may enable building architectures consisting
of large collections of highly parallel asynchronous oscillators that encode complex
state information and hierarchical architectures that mimic and implement what is
known (to date, as we are discovering it) about natural computing forms.
This work will examine a few examples of neuromorphic computing tech-
nologies and architectures geared towards existing results in advanced computing
architectures, computing models, and early applications.
6.2
Self-Reconﬁgurable Electronic Circuit
This simple key area of neuromorphic computing represents the ﬁrst step to achieving
self-reconﬁguration within an electronic system without physical re-wiring. This sec-
tion describes an intelligent information processing system that can self-reconﬁgure
upon user’s demands and desires. These research results, developed within our
in-house laboratory, describe the development of an autonomous reprogrammable
electronic circuit. The circuit employs a memristor-based approach within an in-
novative CMOS circuit biasing architecture to achieve an autonomous electronic
reconﬁgurability or reprogramming ability dependent on a determined desired out-
put and input signals. Hybrid CMOS and memristor device circuit simulation results
demonstrate that a hardware realization of such electronic reconﬁgurable or repro-
grammable systems employing memristor devices and existing CMOS technologies
is possible. This technology represents the primitive building block for high density,
small form-factor, and ultra-low power computing architectures.
The self-reconﬁgurable or reprogrammable electronic system relies on recenttech-
nological advances and discoveries namely the nonvolatile memory resistor device or
memristor for short [9] and developed memristor modeling methodology [3]. Leon
Chua theorized the existence of the memristor device in 1971 as the fourth basic
circuit element [9]. Given the non-volatile nature of the memristor device, applica-
tions containing such devices lie within memory and reconﬁgurable/reprogrammable

80
R. E. Pino
computing applications [3, 5, 9–11]; therefore, it represents a step forward in the de-
velopment of low power and small form factor reprogrammable electronic hardware
and computing architectures. Current conventional digital computing architectures
rely solely on the ﬁeld effect transistor (FET) which is a four terminal device (drain,
gate, source, and body). However, during operation for storing or retrieving infor-
mation the FET device needs to be powered continuously or the stored in formation
will be lost. In addition, high charge leakage issues in the device require continuous
refreshing of the processed information during standby and operation. This continu-
ous need for power creates limitations on the system’s power consumption and form
factor scale. As mentioned previously, the memristor device is a non-volatile passive
electronic device; thus, it only consumes power during operation and reconﬁgura-
tion and does not require standby biasing power because that power can be actually
turned off when not required. This is the reason why passive reconﬁgurable elec-
tronics are not available today. The fact that the memristor is a nonvolatile memory
device means that any standby power utilization of the computing system will be
minimized or completely eliminated.
An important challenge in working with memristor devices is the modeling of
the time-domain hysteresis electronic behavior. Thus, no large scale or accurate
circuit simulations can be performed since behavioral models do not exit. However,
we recently developed a compact model and method for modeling and simulating
memristor devices described elsewhere [12]. The compact model that we developed
models the electronic current, time and voltage domain characteristic behavior of
chalcogenide-based memristor devices. This model, the ﬁrst of its kind at the time
to the best of our knowledge based on our review of the published literature, enables
the accurate modeling and simulation of memristor-based reprogrammable electronic
circuits.
The reconﬁgurable electronic circuit schematic is shown in Fig. 6.1. From the
circuit schematic, Q1 and Q4 represent thresholding gates, n-channel ﬁeld effect
(nFET) transistors, whose output is directly proportional to the voltage applied to
the gate node respectively. During our simulation, we assume the threshold voltage
of Q1 and Q4 are 100 mV. The resistors R1 and R2 are biasing resistors to obtain the
appropriate voltage biasing conditions to control the signal strength at the gate node,
n1, of transistor Q4. Thus, the voltage at node n1 is given by
V (n1) = Vc

R1 + M1
R1 + R2 + M1

,
(6.1)
where Vc is the voltage at the drain node of nFET transistor Q1, R1 and R2 are
regular resistors, and M1 is the resistance state of the memristor device. This is a
ﬁrst order calculation ignoring any parasitic additional resistance, capacitance, and
inductance effects. Thus, if the resistance state of M1 is high V(n1), the voltage at
node n1, will be high and if M1 is low, V(n1) will be low.
The input signalA represents the input signal to transistor Q1 which if greater that
the threshold voltage (100 mV) will cause Q1 to output Vc, connected its drain node.
Otherwise, if the input signal A is lower than 100 mV, then the output of Q1 will be

6
Computational Intelligence and Neuromorphic Computing Architectures
81
Fig. 6.1 Self-reconﬁgurable circuit schematic
low or 0 V. The transistor Q2 is a safe gate (pFET transistor with threshold voltage
of 0 V) which prevents any perturbation to the memristor device M1 whenever the
τ node is greater than 0 V. The voltage source Vp corresponds to the reconﬁguring
pulse signal voltage used to reprogram M1 (the memristor) based on the electrical
device characteristics described elsewhere [12]. The D node is the desired output
and it is used to initiate the circuit reconﬁguration process. The XOR1 logic gate
compares the circuit’s output O to the desired output D and whenever D and O
are different the output of XOR1 will be 1 otherwise it will be 0 given its standard
Boolean logic functionality. The output of XOR1 is deﬁned at the τ node. The node
E is the programming enable mechanism when whenever greater than Q5’s threshold
voltage (100 mV), it will connect the output of XOR1, τ node, to the gate node of
Q6. Any time nodes T and E are greater than 100 mV, the training pulse Vp will be
allowed into the circuit to reconﬁgure or reprogram the M1 the memristor device
as determined by its electronic characteristics [12]. The transistor Q3 is a safe gate
(100 mV threshold voltage) that prevents the reconﬁguring pulseVp from perturbing
the memristor M1 whenever the input signal A is zero. To simulate the operation
of the circuit, we employed the compact model for chalcogenide-based memristor
described elsewhere [12].
Figure 6.2 displays the simulation results and reconﬁgurability properties of our
reconﬁgurable circuit. Figure 6.2a, the top pattern, corresponds to the input voltage
at node D, and Fig. 6.2b corresponds to the output pattern of the reconﬁgurable
circuit at node O. During this particular simulation nodes A and E were set to high
(1 V). From the results we can observe how the circuit’s output node O follows

82
R. E. Pino
Fig. 6.2 Reconﬁgurable circuit simulation (a) input training and (b) output patterns
the desired training input D. The ﬁgure also shows that during the transition points
from high to low and low to high outputs, the circuit oscillates until autonomous
reconﬁgurability of the memristor M1 is achieved. Self-reconﬁguration stops once
the appropriate voltage at node n1 is obtained to cause the output of Q4 to be either
high (Vc) or low (0 V). Figure 6.3 describes in detail the electronic reconﬁgurable
circuit transition points from high to low output Fig. 6.3a–c, and low to high output
Fig. 6.3d–f. Figure 6.3a, d show the voltage transition oscillations, Fig. 6.3b, e show
the change in the memristor M1 resistance change over time, and Fig. 6.3c, f shows
the voltage oscillations at node n1 and the Vp reconﬁguring pulse enveloping the
oscillating n1 node voltage. In addition, the results, Fig. 6.3b, e, show how the
memristor device, M1, resistance state changes during programming (along with the
memristor effective biasing voltage VP in Fig. 6.3c, f) until the appropriate value
is autonomously obtained by the reconﬁgurable circuit. Once the circuit output, O,
matches the desired input value, D, the output of the XOR1 logic gate will be zero
and the path of the training pulse, Vp, will be blocked (meaning transistor Q6 will
be in the off state), stopping the reconﬁguration process.
This innovative method for enabling electronic computing systems to self-
reconﬁgure provides the foundation to fabricate autonomous neuromorphic com-
puting architectures. Our invention relies on the ability to change the resistance state
of a memristor device to achieve an optimal voltage at speciﬁc circuit nodes dy-
namically and autonomously causing the circuit to reconﬁgure itself and output the
desired result. Now that we have shown that self-reconﬁgurability autonomy could
be potentially achieved within an electronic computing system, we will look at how
we could fabricate a physical neuromorphic computing architecture with physical
CMOS-memristor neurons and synapses.

6
Computational Intelligence and Neuromorphic Computing Architectures
83
Fig. 6.3 Circuit programming transitions from high to low and low to high output values
a transition from a high to low output value, b memristor device value during transition,
c voltage change in nodes Vp and n1 during transition, d transition changes from a low to high
output value, e memristor device value change from a low to a high output value transition, and
f voltage a nodes Vp and n1 during a transition from a low to a high output voltage value
6.3
Neuromorphic Computing Architecture
To emulate the natural ability of the brain to perform a high number of complex
functions in parallel that to date are unmatched by the fastest most powerful super
computers represents a great technological engineering challenge. Neuromorphic

84
R. E. Pino
computers promise to provide artiﬁcial machines the ability to perform complex
functions by mimicking the brain’s engineering. Software based implementations of
neuromorphic computing have demonstrated the feasibility of mimicking brain func-
tionality [5]. However, software based implementations of neuromorphic computing
require high performance computers and super computers which are impractical
within mobile and/or low cost systems. Therefore, the development of hardware
based neuromorphic computers will enable a technological breakthrough in the
implementation of brain functionality within systems that are built based on the
engineering principles of the brain. In this section, we describe a neuromorphic
hardware based architecture that mimics the brain’s synaptic and neuron functionality
employing three transistors per synapse and one transistor per neuron.
The amazing computing power of the brain originates in its highly parallelized
interconnectivity amongst neurons through synaptic connections. The synaptic
connection plays an important role in brain activity as these connections can be
strengthened or weakened as the brain learns and knowledge is stored within the sys-
tem [6]. Neuron behavior has been characterized as an adding system that provides
an output based on the sum of all inputs, or synapse outputs, and its connectivity to
other neurons [5]. In addition, the amount of information or knowledge a neuromor-
phic computer can retain depends on the number neurons and synaptic connections
within the system. For example, the brain of an ant is said to contain approximately
300,000 neurons [8]. Given the large number of neurons and synaptic connections
(approximately 1,000 synaptic connections per neuron) required to design systems
capable of mimicking practical brain functionality such as image recognition, it is
important for the devices to be small in order to be fabricated within a small physical
area as computer microprocessors are fabricated today. In this section, we present the
physical description by which our three transistor synaptic circuit functions mimic
brain synaptic behavior. Also, we demonstrate how we can implement our synap-
tic circuitry within an adding node and single transistor adding neuron to create
the neuromorphic computing architecture. In particular, we can train neuromorphic
architecture the mimic the functionality of an XOR digital logic gate.
Computer simulations based on the physical principles described above demon-
strate the feasibility of the neuromorphic architectures described in [4], and variations
of neuromorphic computing architectures can be constructed employing transistors,
memristors, and inverter circuits coupled to ﬂoating node neurons or thresholding
neurons to perform the desired computations. Fig. 6.5 displays an example of a neuro-
morphic computing implementation that demonstrates how a computing architecture
can be implemented with thresholding neurons.
We will deﬁne our synaptic system employing a variable resistor (either a CMOS
transistor operating within the weak inversion and Ohmic region modes or a mem-
ristor) and inverter circuit elements described in detail elsewhere [4]. Figure 6.5
describes our complete synaptic system where Fig. 6.5a describes the circuit im-
plementation of the synapse, and Fig. 6.6b describes the simpliﬁed circuit element
form that will be employed when designing the neuromorphic network. From the
ﬁgure, we can observe that the output of the synaptic system (SO) is a function of

6
Computational Intelligence and Neuromorphic Computing Architectures
85
Fig. 6.4 Synaptic system
(a) circuit representation and
(b) simpliﬁed circuit
representation
Fig. 6.5 CMOS inverter
voltage transfer curve.
(Adapted from Reference
[13])
the Vm potential that will either strengthen, weaken, or completely cut-off the con-
nection between the synapse input and the CMOS inverting circuit operating within
its transition bias point range.
We can model the synapse output (SO) employing a linear approximation to the
electronic characteristic behavior of the CMOS inverting circuit as
SO(SI, V m) = f [I(V m) × R],
(6.2)
where f represents the transfer function of the CMOS inverter and I(Vm) is the current
across the transistor channel or memristor device, and R is a resistor used to reset
and properly bias the synapse. For example, for synaptic inputs between 0 and 2 V,
as shown in Fig. 6.5, the inverter transfer functions can be made to range from 3 to
∼0 V approximately as a function of the biasing operating voltage V in as shown in
Figs. 6.4 and 6.5.

86
R. E. Pino
Fig. 6.6 Multiple synaptic outputs converge at the ﬂoating adding node and their combined response
is fed to the next neuron synaptic layer of the neuromorphic computing architecture
The implementation of the neuron functionality will be performed with an adding
node and/or a single transistor as displayed in Figs. 6.6 and 6.7. The adding node
is the physical connection where all post-synaptic outputs will converge. As the
synaptic outputs converge, they will increase or decrease the potential at the ﬂoating
neuron adding node. Thus, the resulting added potential will become the input to the
following synaptic layer. In addition, if the neuron adding node were connected to
the gate of, for example, an nFET transistor, and if the total combined potential at the
adding node is greater than the threshold voltage, Vth, of the MOSFET transistor,
the output of the neuron will be Vo =Vn. Otherwise, the neuron will not output a
high potential (the neuron will not ﬁre). Finally, Fig. 6.8 displays the computing
architecture required to implement the XOR function with our in-house developed
neuromorphic computing architecture [4] classically described in [14].
6.4
Conclusion
The results presented here demonstrate that neuromorphic computing has the poten-
tial to enable machines to perform intelligent functions. This form of computation
exploits the highly parallel computing architectures of biological systems to process
large amounts of information rapidly. In addition, neuromorphic computing forte
lies in its ability to learn and identify patterns. Therefore, a neuromorphic computer
can provide a powerful tool to enable rapid and/or real-time analysis of large data
sets employing highly parallel computing architectures.

6
Computational Intelligence and Neuromorphic Computing Architectures
87
Fig. 6.7 Multiple synaptic outputs converge at the adding neuron where their relative contribution
(the adding of all post-synaptic output potentials) will cause the single MOSFET neuron to ﬁre as
long as the overall synaptic contribution is above its threshold potential. The neuron output Vo will
be fed to the next synaptic layer of the neuromorphic computing architecture
Fig. 6.8 Neuromorphic computing implementation of the XOR function
We have described a neuromorphic computing architecture that employs transis-
tors and memristors to represent synapses coupled to ﬂoating node or thresholding
neuronstoperformneuromorphiccomputations. Thisresultgivesusthefoundationto
create a physical hardware neuromorphic computing architecture. We have proposed

88
R. E. Pino
a simple compact model that accurately describes the behavior of chalcogenide-based
memristor devices [12].
We believe that neuromorphic computing is close to reaching the critical mass
required to enable machines to learn and perform intelligent functions. Given the cur-
rent hardware state-of-the-art, memristor technologies provide the scaling factor to
physically create nanoscale low power neuromorphic computing architectures. Thus,
we all should look forward to the challenging opportunity to bring neuromorphic
computing powered hardware technology to reality.
References
1. Pino R, Genello G, Bishop M, Moore M, Linderman R (2010) Emerging neuromorphic com-
puting architectures & enabling hardware for cognitive information processing applications.
The 2nd international workshop on cognitive information processing CIP, Elba Island, Italy,
14–16 June 2010
2. Pino R (2011) Reconﬁgurable electronic circuit. US Patent 7, 902, 857, 8 March 2011
3. Pino R, Bohl J (2009) Method and apparatus for modeling memristor devices. US Patent
Pending, Serial # 12/657,262– AF Inv # RL 10,089, Dec 2009
4. Pino R (2009) Neuromorphic computer. US Patent Pending, Serial # 12/590,306– AF Inv # RL
10,088, Nov 2009
5. Strukov DB, Snider GS, Stewart DR,Williams RS (2008)The missing memristor found. Nature
453:80–83
6. Daniel L (2006) Holy Chip!
Jan 2006.
Forbes.com Inc.
http://www.forbes.com/
forbes/2006/0130/076.html. Accessed on 10 Oct 2011
7. Hewlett-Packard (2007, January 16) Hybrid nano-CMOS chips could be far denser, but cooler.
PHYSorg.com. http://www.physorg.com/news88196140.html. Accessed 12 Oct 2011
8. Snider GS, Williams RS (2007) Nano/CMOS architectures using a ﬁeld-programmable
nanowire interconnect. Nanotechnology 18(3):035204–11
9. Chua LO, Leon O (1971) Memristor—The missing circuit element. IEEE Trans Circuit Theory
18(5):507–519
10. Williams R (2008) How we found the missing memristor. IEEE Spectrum 45(12):28–35
11. Chua L, Kang SM (1976) Memristive device and systems. Proc IEEE 64(2):209–223
12. Pino RE, Bohl JW, McDonald N, Wysocki B, Rozwood P, Campbell KA, Oblea A, Timilsina
A (2010) Compact method for modeling and simulation of memristor devices: ion conductor
chalcogenide-based memristor devices. IEEE/ACM international symposium on nanoscale
architectures, pp 1–4
13. Uyemura JP (1999) CMOS logic circuit design. 1st edn. Springer, Norwell, pp 103–109
14. LawrenceJ,LuedekingS(1991)Introductiontoneuralnetworks. CaliforniaScientiﬁcsoftware,
Grass Valley

Chapter 7
Reconﬁgurable Memristor Fabrics
for Heterogeneous Computing
Dhireesha Kudithipudi and Cory E. Merkel
Abstract Device scaling and performance limits in complementary metal-oxide-
semiconductor (CMOS) technology are leading to a rapid proliferation of novel
computing paradigms for information processing. To this end, CMOS/memristor
hybrid technology, where memristive devices are integrated in 3D, is extremely
promising. These technologies are emerging under the joint efforts of industry and
academe, with innovations in fabrication processes, materials, devices, and cir-
cuits. A prime advantage of such computing technology is its ability to offer tera-bit
densities with ultra low power and long data retention times. These unique char-
acteristics motivate the development of computational fabrics that can dynamically
transform over time to perform heterogeneous computing based on system require-
ments, with the technological objective of superseding classical CMOS architectures.
This book chapter discusses the different models of memristors which can be used
for implementing memristors as memory, sensing, logic and nueromorphic units.
7.1
Introduction
The ability to realize ultra-low energy high-density memory devices, which are non-
volatile, is critical to a wide range of nano-computing systems. The conceptualization
and recent demonstration of the pinched hysteresis behavior inTiO2 thin ﬁlms, whose
existence was theoretically predicted by Chua [1], facilitates these high-density non-
volatile memories. A prime advantage of such computing technology is its ability to
switch at high speeds (1 ns) while offering tera-bit densities and greater than 6 years
of data retention time [2, 3]. In particular, the compatibility of these devices with
CMOS to build hybrid CMOS/memristor architectures provides near-term commer-
cialization opportunities. These fabrics enable the unraveling of signiﬁcant scientiﬁc
mysteries, indomainssuchase-healthcare, naturaldisastersimulations, complexsci-
entiﬁc data analysis, mass storage devices, and e-business, impacting our every-day
D. Kudithipudi () · C. E. Merkel
Department of Computer Engineering, Rochester Institute of Technology,
Rochester, NY, USA
e-mail: dxkeec@rit.edu
C. E. Merkel
e-mail: cem1103@rit.edu
R. Kozma et al. (eds.), Advances in Neuromorphic Memristor Science and Applications,
89
Springer Series in Cognitive and Neural Systems,
DOI 10.1007/978-94-007-4491-2_7, © Springer Science+Business Media Dordrecht 2012

90
D. Kudithipudi and C. E. Merkel
lives. These unique characteristics motivate the development of a new computational
fabric that can dynamically transform over time to perform heterogeneous comput-
ing operations based on system requirements and available resources. Such a system
will have signiﬁcant performance and power enhancements over classical CMOS
architectures. In this book chapter, we present memristive device models and their
application to implement different functionality.
Such new computational paradigm addresses several inherent challenges in tech-
nology scaling and integration as well, where we are less capable of controlling
random manufacturing variations, degradation due to aging effects, and early-life
failures [4–6]. These problems will lead to non-functional logic/memory blocks
and overall system performance collapse. One promising approach to tackle these
design challenges is through the development of an architecture which dynami-
cally reconﬁgures system resources such as integrated sensors, logic and memory
units to implement any desired functionality. Traditional dynamic reconﬁguration
approaches deploy preconﬁgured blocks dynamically at run-time based on the com-
putation requirements. While these techniques are suitable for current generation
multicore systems, they are limited to a few conﬁgurations and do not scale well to
future hybrid manycore systems. There is a vital need to develop fundamentally new
computing fabrics to address these problems.
The rest of the book chapter is organized as follows: Sect. 7.2 describes the
memristor operation, fabrication processes, and the different models for analysis.
Section 7.3 presents different functionality—including memory, logic, sensors, and
nueromorphic units.
7.2
Memristive Devices and Models
The concept of memristance was introduced by Leon Chua in 1971 as a relationship
between charge and ﬂux linkage [1]. The idea was later generalized into a class of
non-linear dynamical systems called memristive systems [7]. Since then, numer-
ous groups have demonstrated memristive switching behavior over a wide range of
materials. This section brieﬂy reviews some of the different memristive materials,
switching processes, fabrication processes, and models that have been studied.
Hysteretic resistance switching in thin ﬁlms has been observed for over 40 years
[8, 9]. In 2008, Strukov et al. showed that the resistance switching behavior in
vacancy-doped titanium dioxide thin ﬁlms could be described by an electrical mem-
ristive system [10]. Thin ﬁlm materials that exhibit resistance switching in an applied
electric ﬁeld have become known as thin-ﬁlm memristors. The exact physical pro-
cesses that cause the switching behavior depend on the materials and fabrication
processes.
Table 7.1 summarizes several different switching phenomena in thin-ﬁlm mem-
ristors. In [10], the switching process is described by drift of oxygen vacancies
within the active layer. In [11–14], conductive channels were formed through amor-
phous silicon by the diffusion of metal ions from the electrodes. Similarly, in [15],

7
Reconﬁgurable Memristor Fabrics for Heterogeneous Computing
91
Table 7.1 Switching
processes in thin-ﬁlm
memristors
Switching process
References
Ionic diffusion
O2 vacancy drift
[10]
Metal ﬁlament formation/retraction
[11–15]
Schottky barrier breakdown at oxide/metal interface
[16]
Tunneling barrier modulation
[17]
Spintronic devices
Spin blockade
[18]
Monomolecular ﬁlms
Mechanical switching in Rotaxanes
[19]
gold ions form a switchable conduction channel through a manganese-doped zinc
layer. In [16], the resistance switching in metal/oxide/metal layers is described by
the breakdown of a Schottky barrier at the oxide/metal interface. In [17], memristive
switching is explained by tunneling barrier modulation. The underlying mechanism
of each of these is ionic diffusion. That is, the distribution of ions within the thin ﬁlm
changes in order to change the memristor’s resistance. Memristive behavior in thin
ﬁlms based on spin blockade has also been studied [18]. The resistance switching
behavior in monomolecular ﬁlms such as Rotaxanes [19], can also be described in
terms of memristive systems.
7.2.1
Fabrication Mechanisms
The reading of resistance states in memristor is nondestructive and the memory sys-
tems built with these devices have high density compared to the traditional CMOS
implementations (as high as 4F2, where F is the feature size). Several groups of
solid-state materials have been identiﬁed to present these resistive switching char-
acteristics, including solid electrolytes such as GeSe and Ag2S, perovskites such as
SrZrO3 binary transition metal oxides such as HfO, TiO2, and ZnO and amorphous
silicon (a-Si) as well as Si/α—Si core/shell nanowires [15]. The binary transition
metal oxides are strong candidates for commercial implementation, among these
different materials, as they have simple compositions and exhibit resistive switching
effects in polycrystalline states. Though the fabrication techniques with each of these
materials is beyond the scope of this book chapter, we will focus on few devices that
are more prominent and CMOS compatible. The TiO2 devices by Hewlett Packard
(HP) were observed to have active charge from oxygen deﬁciencies within a TiO2
layer. These devices were built by directly depositing TiO2 layers.
TiO2 active layer was achieved by spinning a TiO2 sol gel [20]. Another varia-
tion of the HP’s implementation has mobile charge induced in the upper TiO2 layer
by introducing an oxygen gas ﬂow during the sputtering deposition, resulting in a
TiO2+x layer [21]. In general, using a a-Si heterostructure as the switching medium,
two-terminal resistive switches composed of metal top electrodes and p-type silicon

92
D. Kudithipudi and C. E. Merkel
(p-Si) bottom electrodes are CMOS compatible [22]. These memristor devices ex-
hibit large hysteresis in their I–V characteristics. Instantaneous state in these devices
depend upon the previous state of the device, which is associated with the charge
distribution within an insulating medium under the particular biasing. For large scale
integration of these devices with CMOS, crossbar arrays or 1T1M (single transis-
tor single memristor) structures [22, 23], using metal/a-Si/metal (M2M) or other
metal/insulator/metal (MIM) devices have been proposed. These class of devices
known as hybrid CMOS/memristor devices offer high density and accessibility—
suitable for memory and logic applications. Jo et al. have demonstrated a crossbar
array with boron-doped poly silicon nanowires serving as the bottom electrodes, Ag
nanowires serving as the top electrodes and a-Si as an active layer. A memristor
crossbar array can be fabricated on top of a CMOS substrate, with vias (tungsten)
coming up from the CMOS, connecting to both the bottom and top array of nanowires
in the crossbar.
7.2.2
Physical and Empirical Models
The form and complexity of thin-ﬁlm memristor models is governed by the materials,
fabrication methods, and physical processes involved in the memristors switching.
Sinceexperimentalsamplesarenotavailableforseveralresearches, itisusefultohave
models for performing analysis. To model the physical behavior of the memristor
devices, several device models have been proposed. These models can be broadly
classiﬁed into two categories—empirical models and physical models, as shown in
Sect. 7.2.2 the complexity for complete analysis using the physical models is time-
intensive as compared to the empirical models (Fig. 7.1). HP’s initial model deﬁnes
the two-terminal switching element in which the magnetic ﬂux (φ −m) between the
terminals is a function of the amount of electric charge q that has passed through the
device. The memristance can be represented as
M(q) = dφ
dq
(7.1)
The simplest empirical memristor model treats the device as a switch with two
possible resistances
im = vm
rm
(7.2)
Rm =
⎧
⎨
⎩
Ron
vm > vt
Roff
vm < −vt
Rm
−vt < vm < vt

7
Reconﬁgurable Memristor Fabrics for Heterogeneous Computing
93
Fig. 7.1 Schematic of the models
Thin ﬁlm memristors can also be modeled as a current-controlled memristive systems
[10]
dx
dt = μIRon
D2
im(t)
vm(t) = [Ronx + Roff (1 −x)]im(t)
(7.3)
where x = w/D is the state variable, μI is the ion mobility, Roff is the memristor
resistance when x = 0, Ron is the memristor resistance when x = 1, νm(t) is the terminal
voltage, and im(t) is the current through the memristor. Solving Eq. (7.3) for im(t)
with the initial condition x(t = 0) = x0, and appropriate boundary conditions yields
im(t) = W(φ)vm(t),
(7.4)
where φ is the ﬂux and W(φ) is the memductance [24], which is equal to [25, 26]
W(φ) =

Ron

r2 + 2(r −1)

B −μI
D2 φ(t)
	−1
.
(7.5)
In Eq. (7.5), r = Roff/Ron and B = ((r −1)/2)x2
0 −rx0. As D approaches inﬁnity, the
memductance reduces to the initial conductance, (Ronx0 + Roff(1−x0))−1. However,
as D becomes small, the memductance becomes quadratically more dependent on
the ﬂux. As a result, both memductance and memristance are important phenomena
in small-feature devices, especially thin ﬁlms.
Figure 7.2 shows simulation runs of a thin-ﬁlm memristor with μI = ×
10−13 m2/V s, Ron = 100 , r = 160, D = 10 × 10−9 m, and x0 = 0.1. The input volt-
age is a sine wave with an amplitude of 1 V and base frequency w0. Simulation runs

94
D. Kudithipudi and C. E. Merkel
Fig. 7.2 Thin-ﬁlm memristor
I–V curves. Parameters:
μI = 1 × 10−13 m2/Vs,
Ron = 100 , r = 160,
D = 10 × 10−9 m, x0 = 0.1,
w0 = 10π [10]
for different voltage source frequencies are shown. Curves corresponding to higher
input frequencies show less hysteresis than those corresponding to lower frequen-
cies. Although the above model gives valuable insight into the thin-ﬁlm memristor’s
operation, it fails to include several important phenomena such as nonlinear ionic
drift velocity, temperature effects, and ion diffusion.
The authors of [27] improve the model presented above by adding the effects of
ion self-diffusion, internal electric ﬁelds between charged ion species, temperature,
and physical boundaries. The model assumes that ion diffusion follows Fick’s Law:
∂NI(x)
∂t
= −∂JI(x, t)
∂x
(7.6)
where JI is the ion ﬂux, which is given as
JI(x, t) = μI(T )∂V (x, t)
∂x
NI(x, t) −DI(T )∂NI(x, t)
∂x
(7.7)
In Eq. (7.7), μI(T) and DI(T) are the temperature-dependent mobility and diffusiv-
ity, and V(x,T) is the potential along the memristor’s active region. The diffusivity
depends on the crystal geometry a, jump frequency f, and ion activation energy EA
as [28]
DI(T ) = f a2 exp

−EA
kBT

(7.8)
where kB is Boltzmann’s constant. The mobility and diffusivity are related by the
Nernst–Einstein relation at low ﬁelds [29] as
μI(T ) = qIDI(T )
kBT
,
(7.9)

7
Reconﬁgurable Memristor Fabrics for Heterogeneous Computing
95
where qI is the ion charge. In the case of high electric ﬁelds, the diffusivity becomes
exponentially proportional to the ﬁeld [30]. The model presented in Eqs. (7.6)–(7.9)
provides a better physical representation of the memristor’s behavior, but it requires
a numerical solution and, therefore, it is not appropriate to use with circuit simulation
tools like SPICE.Another approach to modeling non-linear dopant drift at memristor
boundaries is to add a window function F(x) to the expression for dx/dt in Eq. (7.3)
as [10]. The memristance in the
dx
dt = μIRon
D2
im(t)F(x)
(7.10)
where F(x) = 0 at x = 0 and x = D. References [10, 26, 31, 32] propose several
different forms for F(x). Several groups have proposed SPICE-compatible mod-
els based on the model given in Eq. (7.3) [31–34]. Other models based on empirical
analyses have also been proposed [35].
The memristor can also be characterized by “monotonically-increasing” and
“piecewise-linear (PWL)” with incremental change in the memristance’s state. For
example, Itoh and Chua represent the piecewise linear model as [36]
φ(q) = bq + 0.5(a −b)(||q + 1|| −||q −1||)
(7.11)
where a and b are > 0. The memristance M can be deﬁned as
M(q) = dφ(q)
dq
=

a
q < 1
b
q > 1
(7.12)
This passive 2-segment PWL representation of the device is suitable for generating
the chaotic behavior of the memristor for designing oscillators and can be easily
translated to the corresponding SPICE models for circuit level simulations.
7.3
Memristor Fabrics
The organization of memristive devices into functional circuit blocks can take several
forms. Memristors can be used discretely to add hysteresis or memory to traditional
analog or digital circuits. They can also be integrated into large arrays for use as
non-volatile memory, analog or digital logic, sensors, and artiﬁcial synapses, among
others. This chapter focuses on the latter, where multiple arrays of memristors are
combined to create a multi-purpose computation layer, or fabric. In the next section,
we will show how the functionality of these fabrics can be dynamically changed to
meet the needs of the system or computation.
An illustration of a memristor fabric is given in Fig. 7.3. Dense arrays of mem-
ristors, called memristor crossbars are distributed on top of CMOS processor cores.
The fabric is a shared resource and, as we will discuss in the next section, can be used
for many different functions. Each crossbar circuit consists of stacked orthogonal
nanowires, where every crosspoint forms a thin-ﬁlm memristor. Crossbar circuits of

96
D. Kudithipudi and C. E. Merkel
Fig. 7.3 Memristor fabric. Memristor crossbars are fabricated on top of CMOS processor cores to
provide a multi-purpose computational fabric
this type have become ubiquitous in many architectures that incorporate two-terminal
nanodevices (e.g. thin-ﬁlm memristors). Its regular structure makes it easy to fabri-
cate and yields extremely high density, connectivity, and addressability. Assuming a
wire pitch of P, a crossbar circuit will have a density of 1/P2. If P is equal to twice the
minimum feature size F, then the density becomes 1/4F2 [37]. In an M × N (M rows
and N columns) memristor crossbar circuit, each memristor is directly connected to
M + N −2 neighboring memristors, and may be indirectly connected (depending on
the type of memristor) to even more. This high connectivity allows MN memristors
in the crossbar circuit to be addressed using only M + N interface connections.
In general, each crossbar circuit in the memristor fabric will include supporting
CMOS circuitry. The supporting circuitry forms the interface between the CMOS
layer and the memristor fabric layer. An example of a memristor crossbar with
supporting CMOS interface circuitry is shown in Fig. 7.4. This conﬁguration is
well-suited for digital applications such as memory or lookup table-based logic.
Memristors are read or programmed by applying a voltage to the corresponding
crossbar row, and reading the current through the corresponding column. Crossbar
rows and columns are selected based on a decoded address, and row voltages are
applied based on the incoming data or the read/write signal ¯rw. This conﬁguration
yields a single memristor-addressable crossbar circuit. However, note that there are
several possibilities for the interface circuitry. For example, all of the memristors
in a row could be read at the same time by applying a row voltage and reading the
corresponding current through all of the columns.
One of the greatest challenges associated with the crossbar architecture is the ef-
fect of current sneak paths. Consider the case in Fig. 7.4 where the circled memristor
is being read or written. A voltage is applied to the ﬁrst row, and the second column

7
Reconﬁgurable Memristor Fabrics for Heterogeneous Computing
97
Fig. 7.4 General structure of
a memristor crossbar circuit
with supporting CMOS
circuitry. Row and column
multiplexers are used to
address single memristor
elements. Analog or digital
read/write voltages are
applied to the addressed row
depending on the data and
operation
...
...
...
...
...
...
is grounded through a pull-down resistor in order to read the current through the
memristor. This results in the current path shown in gold (short dashes). However,
since memristors are purely resistive devices, several parallel current paths are also
present, such as the one shown in red (long dashes). Depending on the application,
these parallel current paths may be desirable. However, they are generally consid-
ered to be noise. Sneak paths can cause the state of a memristor to be misread or
changed unintentionally. Several different devices, architectures, and read/write pro-
cedures have been proposed to mitigate the effect of sneak paths in crossbar circuits.
In [37], a memristive device composed of two antiserial memristors is proposed
to improve crossbar read margins. Reference [38] compares a 1-diode 1-memristor
(1D1M) architecture and an unfolded crossbar architecture for sneak path mitigation.
The 1D1M architecture assumes that each memristor is in series with a rectifying
component such as a pn junction diode, eliminating sneak paths. However, there
have been limited demonstrations of such devices. The unfolded crossbar architec-
ture isolates addressed memristors using M:1 multiplexers on each column (where
M is the number of crossbar rows). In [39], a three-step read process is proposed for
determining the state of a crossbar memristor even with the presence of sneak paths.
Sneak paths can also be eliminated by utilizing 1 × N crossbar circuits. However,
this 1T1M-type structure severely limits the achievable density of the memristor
fabric. As discussed in the previous section, some memristor devices exhibit volt-
age thresholds for writing. The use of these devices in memristor crossbar circuits
facilitates simpler crossbar write schemes that account for sneak path currents. For
example, the crossbar rows and columns can biased in such a way that only the
addressed memristor has a voltage drop across it that is greater than the threshold
voltage [40]. This ensures that the effect of current sneak paths during the write
operation is negligible.

98
D. Kudithipudi and C. E. Merkel
Another challenge experienced in memristor circuits is the effect of compliance
current, or the maximum memristor current during the write operation. In [41], the
authors show that large compliance currents during the electroforming and SET pro-
cesses can permanently damage Ti/HfOx devices. The large currents create strong
ﬁlaments which are difﬁcult during the RESET operation, thus reducing the achiev-
able Roff. As a result, extra circuitry is required to limit the memristor current during
the write operation. In [40], the authors show that a simple current mirror can be
used for this purpose. However, if the current limit is too low, then the performance
of the write operation will be signiﬁcantly degraded.
A ﬁnal aspect to be addressed is the design of the interface circuitry between the
CMOS and memristor fabric layers. If the devices are fabricated in contact or via
holes between metal layers, then the physical interface is reduced to stacked vias
that terminate on a MOSFET drain. However, a greater crossbar density and smaller
area overhead can be achieved by fabricating the memristor fabric layer on top of
the CMOS layer. In this way, a different fabrication process can be used which is not
limited to the CMOS layer feature sizes. However, this approach requires sub-CMOS
feature size nanowires to be interfaced with CMOS wires. This can be accomplished
using spatially-distributed interface pins that connect the top-level CMOS metal layer
to the nanowire crossbars [42, 43]. Another approach is to use demultiplexer circuits
based on encoded nanowire doping [44]. In this method, it is possible to address
n!/[w!(n −w)!] nanowires with only n CMOS wires, where w is the size of the
codeword. This type of interface is especially attractive for crossbar architectures
that eliminate sneak path currents by using only one row or column.
7.4
Heterogeneous Architectures
The memristor crossbar circuits discussed in the last section can be used for several
different functions. In fact, it is possible to achieve temporal and spatial heterogeneity
at both locally and globally within the memristor fabric. Temporal heterogeneity
allows the architecture to morph between different functionalities over time to meet
changing system or computational needs. Spatial heterogeneity enables different
parts of the memristor fabric to be used for different functions at any given time.
This section gives an overview of various memristor crossbar architectures. These
architectures can be dynamically implemented in the memristor fabric, allowing it
to augment the CMOS processor cores as a reconﬁgurable hardware resource.
7.4.1
Resistive Random Access Memory
Resistive random access memory (RRAM) is a viable alternative to ﬂash and static
random access memory (SRAM), which are approaching fundamental scaling limits.
RRAM stores data in the form of resistance instead of charge or voltage. A single

7
Reconﬁgurable Memristor Fabrics for Heterogeneous Computing
99
Fig. 7.5 Thin-ﬁlm memristor as a 2-level RRAM element
memristive device can be used as a 2-level RRAM element by storing either a high
or low resistance value. This concept is illustrated in Fig. 7.5. R0 and R1 represent
resistance ranges for logic 0 and logic 1, respectively. The state of the memristor
RRAM element can be read by connecting it in series with a pull-down resistor and
applying a small read voltage to the branch. A comparator is then used to compare
a reference voltage with the voltage drop across RPD. The pull-down resistor should
be carefully designed in order to maximize distinguishability between states [45].
It is important that the voltage applied during the read operation does not change
(destroy) the memory state. One way to ensure this, is to apply both a positive and
negative voltage pulse during the read operation so that the net applied ﬂux φ(t) is
zero. However, the memristor’s domain wall velocity will generally be asymmetric:
It will be faster in one direction than it is in the other [17, 27]. As a result, the
positive or negative voltage pulse will have to have a longer duration in order to
maintain the initial memristor state. Any noise during the read operation or domain
wall motion due to ionic diffusion will cause undesired changes in the memristor
state. In [45], a refresh scheme is proposed to periodically rewrite the memristor
to the correct memory state. Another approach is to use a safety margin, where
intermediate resistance values are treated as undeﬁned memory values. This concept
is illustrated in Fig. 7.5, with RB being the undeﬁned resistance range. Furthermore,
devices with write threshold voltages can be read with non-zero ﬂux, as long as the
voltage amplitude is below the threshold.
During the 2-level RRAM element write operation, a positive or negative ﬂux
needs to be applied to change the memristor’s resistance value. In order to improve
the write speed and corresponding write energy of the RRAM element, the authors of
[46] propose using partial memristor programming. In partial programming, only a
portion of the total memristor resistance range Roff to Ron is used. In Fig. 7.5, this idea
corresponds to using the gray regions of the memristor to represent logic levels low
and high. However, partial programming will reduce the distinguishability between
the logic high and logic low Qm values. To mitigate this effect, the authors propose
the use of two memristors to represent a single memory bit [46].

100
D. Kudithipudi and C. E. Merkel
Note that, after adding a pull-down resistor and comparator, the crossbar archi-
tecture in Fig. 7.4 works readily for the 2-level RRAM discussed above. In that case,
only one bit can be selected at a time. This would lead to serially read/written data
words, yielding high memory latency. However, several crossbars in the memristor
fabric can be read/written in parallel to achieve word-addressable memory [47]
The resistance range of each memristive device can be divided into multiple
levels to create multi-bit RRAM elements [38, 48, 49]. However, these architectures
require more complex feedback circuitry to account for process variations. As the
number of levels increases, multi-bit RRAM elements become more susceptible to
noise caused by read and write cycles, as well as sneak paths. Furthermore, the
retention time of multi-bit RRAM devices is signiﬁcantly affected by the additional
memory states. In general, there is a tradeoff between the multi-bit RRAM density
and reliability. In [49], we propose an N-level RRAM architecture, where N is
dynamically reconﬁgurable. This allows mission-critical applications to use fewer
levels with a higher reliability. On the other hand, applications that are extremely
memory intensive may use more levels at the cost of slightly reduced reliability.
7.4.2
Write Time-Based Temperature Sensing
The write latency of memristor-based RRAM is very sensitive to temperature ﬂuc-
tuations. By adding a high-speed counter and support circuitry, the bit-addressable
RRAM architecture can be used as temperature sensor [47]. The counter captures
the write time of every write operation. This can be compared to a calibrated lookup
table to estimate the temperature at the physical location of the write address. This
method allows temperature measurements to be taken anywhere within the mem-
ristor fabric. Combined with active and passive sensing techniques [47], this yields
extremely ﬂexible and high-resolution thermal proﬁling. Dynamic thermal manage-
ment mechanisms such as thermal-aware task scheduling [50] and dynamic voltage
and frequency scaling [51] can use this temperature data to maximize the lifetime of
the chip.
7.4.3
Reconﬁgurable Logic
As discussed above, memristor fabrics have distinct characteristics to serve as a high
density nonvolatile memory and sensors. The memristor memory implementation
can also be exploited to design logic blocks without fundamentally altering the ar-
chitecture. One approach is to use the lookup table (LUT) implementations, where
LUTs are implemented using RRAM as shown in Fig. 7.6. The underlying concept
is similar to the LUT-based approaches used in reconﬁgurable platforms, such as
FPGA’s. The main idea is to use the write operation of an RRAM to deﬁne the LUT
function. For example, in Fig. 7.6, we implement two functions Sum = (A ⊕B ⊕Cin)

7
Reconﬁgurable Memristor Fabrics for Heterogeneous Computing
101
Fig. 7.6 Full adder logic
implementation using LUT
and Cout = (AB) + (BCin) + (ACin), using a 4-input LUT which is deﬁned within the
address space of the RRAM architecture. Each dot represents an active memristor.
Few bits from the address bus (addr) will be used to select the speciﬁc function.
The remaining bits will be used as operands and the function is evaluated by reading
(data bus) that speciﬁc memory location. In cases where only top part of the array is
used for logic functions, the bottom part of the array can be used for regular memory
operation, serving a dual-purpose in itself. To implement a multi-bit function using
the LUTs we can load the data in to multiple LUTS. In the case of a 2-input full
adder, the data can be loaded in to 3 LUTs (2 for sum and 1 for Cout). For cascaded
logic functions, each function will be loaded in to a separate LUT and the output of
one LUT will be tied to the input of the subsequent block in the cascaded logic.
An added advantage with this approach is that each LUT slice can optionally
implement a small distributed Random Access Memory (RAM). Distributed RAMs
can be cascaded to work as deeper and/or wider memory structures, with a minimal
timing penalty incurred through specialized logic resources. Additionally, we can
also achieve different arithmetic operations from single or cascaded memristors. If
two memristors are connected in series addition can be achieved, adding an ampli-
ﬁer and few resistors to the adder can provide subtraction. Similarly division and
multiplication operations can be attained with them connected along with resistors,
ampliﬁers and an additional series/parallel memristors [52].

102
D. Kudithipudi and C. E. Merkel
Fig. 7.7 Modeling a
biological neural network in
the memristor fabric.
Memrisotrs crossbars can
reproduce the high
connectivity between
neurons. Synapses are
emulated using thin-ﬁlm
memristors, where the
synaptic weight is analogous
to the memristor’s variable
conductivity
7.4.4
Neuromorphic Circuits
In 1990, Conway Mead wrote his seminal paper on neuromorphic electronic systems
[53]. He argued that, by emulating the adaptive nature of biological systems, we
can increase the efﬁciency and fault tolerance of computational architectures far
beyond that which is achievable in digital designs. Here, we will focus on the role
of memristive devices in neuromorphic systems. The architectures discussed here
can be integrated into the memristor fabric to facilitate adaptive learning as part of a
heterogeneous system.
The human brain contains approximately 1016 synaptic connections between neu-
rons [53]. Therefore, hardware emulation of even a small subset of the brain’s
functionality will require a large, complex network of hardware synapses. Artiﬁ-
cial synapse emulation is the second most-widely cited application of memristive
devices (after non-volatile memory). The reason is that memristance effectively
models synaptic plasticity. That is, memristive devices can be used to change the
connection strength between two circuit nodes by modulating the conductance be-
tween those nodes [54]. When integrated into the crossbar architecture, memristive
devices can model a complex biological neural network, as shown in Fig. 7.7. Each
pre-synaptic neuron is connected to a post-synaptic neuron through a memristor.
Note that sneak paths are especially problematic for this type of architecture. Self-
rectifying or 1D1M architectures may need to be employed to enforce a limited
number of connections between a given set of neurons. In Fig. 7.7, neuron B should
ﬁre if the sum of the potentials from its input neurons exceeds a given threshold.
Each neuron can be implemented in the CMOS layer as a comparator, for example.
A different approach is presented in [55], where each synapse consists of a mem-
ristor, a resistor, and NMOS and PMOS ﬁeld-effect transistors. The synapse’s action

7
Reconﬁgurable Memristor Fabrics for Heterogeneous Computing
103
potential input is converted to a charge which is proportional to the value of the mem-
ristor’s resistance. This is accomplished using a charge sharing circuit. The charge
is summed at the input of the post-synaptic neuron, which ﬁres after a threshold is
reached. The authors achieved energy consumption on the order of femtojoules by
using high V t devices in the subthreshold region.
In order for adaptive learning to take place, a learning rule or procedure must be
adopted. In [55], supervised learning is achieved via local and global training circuits.
Synaptic weights (memristance values) are adjusted based on the difference between
the actual and expected outputs of each neuron. Other groups [56–60] have proposed
the implementation of spike-timing-dependent plasticity (STDP) learning rules with
memristive synapses. STDP is a biological learning rule where synaptic weights are
adjusted as a function of the time between pre-synaptic and post-synaptic spikes.
If the pre-synaptic neuron ﬁres shortly before the post-synaptic neuron, then the
connection is strengthened. If the post-synaptic neuron ﬁres shortly before the pre-
synaptic neuron, then the connection is weakened. Otherwise, the synaptic weight
is unchanged. In [60], a leaky integrate-and-ﬁre (LIF) circuit is proposed for STDP
learning with memristor crossbar-based synaptic connections. The authors also show
that the shape of each neuron’s action potential strongly inﬂuences the STDP learning
function. As a result, different STDP learning functions could be used for different
applications. In [56], STDP is implemented using time-division multiplexing. Synap-
tic spikes, long-term potentiation (LTP), and long-term depression (LTD) signals are
communicated in separate timeslots using pulse-width modulation. This method also
allows non-STDP learning laws to be implemented. Experimental demonstrations of
LTP and LTD have also been reported [59], where Cu2O-based memristive devices
exhibit STDP learning with spike time differences on the order of microseconds.
In [61], the authors point out that the synaptic weight of biological neurons varies
more slowly as learning progresses. However, most thin-ﬁlm memristive devices
exhibit the opposite behavior, where ﬁlament formation or domain wall velocity has
a super-linear relationship with ﬂux. The authors show that this behavior can be
reversed by anti-serially connecting two dissimilar memristors. It may also be pos-
sible to obtain similar results using the complementary resistive switches described
in [37]. In general, memristive devices should exhibit multi-level resistance switch-
ing as well as thresholded switching characteristics in order to exhibit STDP-based
learning.
References
1. Chua L (1971, September) Memristor-the missing circuit element. IEEE Trans Circuit Theory
18(5):507–519
2. Jo SH (2010) Nanoscale memristive devices for memory and logic applications. Ph.D.
dissertation, University of Michigan
3. Xia Q, Robinett W, Cumbie MW, Banerjee N, Cardinali TJ, Yang JJ, Wu W, Li X, Tong
WM, Strukov DB, Snider GS, Medeiros-Ribeiro G, Williams RS (2009) Memristor/CMOS
hybrid integrated circuits for reconﬁgurable logic. Nano Lett 9(10):3640–3645 [Online].
http://pubs.acs.org/doi/abs/10.1021/nl901874j

104
D. Kudithipudi and C. E. Merkel
4. Cannon E, KleinOsowski A, Kanj R, Reinhardt D, Joshi R (2008, March) The impact of aging
effects and manufacturing variation on SRAM soft-error rate. IEEE Trans Dev Mater Reliab
8(1):145–152
5. Bernstein K, Frank DJ, Gattiker AE, Haensch W, Ji BL, Nassif SR, Nowak EJ, Pearson DJ,
Rohrer NJ (2006, July) High-performance CMOS variability in the 65-nm regime and beyond.
IBM J Res Dev 50(4.5):433–449
6. Pang L-T, Qian K, Spanos C, Nikolic B (2009,August) Measurement and analysis of variability
in 45 nm strained-si CMOS technology. IEEE J Solid-State Circ 44(8):2233–2243
7. Chua L, Kang SM (1976, February) Memristive devices and systems. Proc IEEE 64(2):209–223
8. Argall F (1968) Switching phenomena in titanium oxide thin ﬁlms. Solid-State Electron
11:535–541
9. Blanc J, Staebler DL (1971, November) Electrocoloration in SrTiO3: vacancy drift and
oxidation–reduction of transition metals. Phys Rev B 4(10):3548
10. Strukov DB, Snider GS, Stewart DR, Williams RS (2008, May) The missing memristor found.
Nature 453(7191):80–83 [Online]. http://dx.doi.org/10.1038/nature06932
11. Dong Y, Yu G, McAlpine MC, Lu W, Lieber CM (2008) Si/a-Si core/shell nanowires
as nonvolatile crossbar switches. Nano Lett 8(2):386–391 (pMID: 18220442) [Online].
http://pubs.acs.org/doi/abs/10.1021/nl073224p
12. Jo SH, Lu W (2008) CMOS compatible nanoscale nonvolatile resistance switching memory.
Nano Lett 8(2):392–397 (pMID: 18217785) [Online]. http://pubs.acs.org/doi/abs/10.1021/
nl073225h
13. Jo SH, Kim K-H, Wei L (2009) Programmable resistance switching in nanoscale two-terminal
devices. Nano Lett 9(1):496–500
14. Jo SH, Kim K-H, Lu W (2009) High-density crossbar arrays based on a Si memristive system.
Nano Lett 9(2):870–874 [Online]. http://pubs.acs.org/doi/abs/10.1021/nl8037689
15. Yang YC, Pan F, Liu Q, Liu M, Zeng F (2009) Fully room-temperature-fabricated nonvolatile
resistive memory for ultrafast and high-density memory application. Nano Lett 9(4):1636–1643
16. Yang JJ, Pickett MD, Li X, Ohlberg DAA, Stewart DR, Williams RS (2008) Memristive
switching mechanism for metal/oxide/metal nanodevices. Nat Nanotechnol 3(7):429–433
17. Pickett MD, Strukov DB, Borghetti JL, Yang JJ, Snider GS, Stewart DR, Williams RS (2009,
October) Switching dynamics in titanium dioxide memristive devices. J Appl Phys 106(7):074
508–074 508-6
18. PershinYV, Di Ventra M (2008, September) Spin memristive systems: spin memory effects in
semiconductor spintronics. Phys Rev B 78(11):113309
19. Deng W-Q, Muller RP, Goddard WA (2004) Mechanism of the Stoddart-Heath bistable rotax-
ane molecular switch. J Am Chem Soc 126(42):13 562–13 563 (pMID: 15493882) [Online].
http://pubs.acs.org/doi/abs/10.1021/ja036498x
20. Gergel-Hackett N, Hamadani B, Dunlap B, Suehle J, Richter C, Hacker C, Gundlach D (2009,
July) A ﬂexible solution-processed memristor. Electron Dev Lett IEEE 30(7):706–708
21. Prodromakis T, Michelakis K, Toumazou C (2010) Fabrication and electrical characteristics of
memristors with TiO2/TIO2 + x active layers. In: Circuits and systems (ISCAS), Proceedings
of 2010 IEEE international symposium on, 30 2010-June 2 2010, pp 1520–1522
22. Jo SH, Kim K-H, Lu W (2009) High-density crossbar arrays based on a Si memristive system.
Nano Lett 9(2):870–874 [Online]. http://pubs.acs.org/doi/abs/10.1021/nl8037689
23. Manem H, Rose GS (2011) A read-monitored write circuit for 1T1M multilevel memristor
memories. In: Circuits and systems (ISCAS), 2011 IEEE international symposium on, May
2011, pp 2938–2941
24. Chua L, Kang SM (1976, February) Memristive devices and systems. Proc IEEE 642:209–223
25. Wang FY (2008, August) Memristor for introductory physics. eprint arXiv:0808.0286v1
[physics.class-ph]
26. JoglekarYN, Wolf SJ (2009, July)The elusive memristor: properties of basic electrical circuits.
Eur J Phys 30:661–675
27. Strukov DB, Borghetti JL, Williams RS (2009) Coupled ionic and electronic transport model
of thin-ﬁlm semiconductor memristive behavior. Small 5(9):1058–1063

7
Reconﬁgurable Memristor Fabrics for Heterogeneous Computing
105
28. Glicksman ME (2000) Diffusion in solids: ﬁeld theory, solid-state principles, and applications.
Wiley, New York
29. Weinert U, Mason EA (1980) Generalized Nernst–Einstein relations for nonlinear transport
coefﬁcients. Phys Rev A 21(2):681–690
30. Strukov D, Williams R (2009) Exponential ionic drift: fast switching and low volatility of thin-
ﬁlmmemristors.ApplPhysA:MaterSciProcess94:515–519. doi:10.1007/s00339-008-4975-3
[Online]. http://dx.doi.org/10.1007/s00339-008-4975-3
31. Benderli S, Wey T (2009, March 26) On spice macromodelling of TiO2 memristors. Electron
Lett 45(7):377–379
32. Zdenek B, Dalibor B, Viera B (2010) Spice model of memristor with nonlinear dopant drift.
Radioeng J 30(4):210–214
33. Shin S, Kim K, Kang S-M (2010, April) Compact models for memristors based on charge–ﬂux
constitutive relationships. IEEE Trans Comput-Aided Des Integr Circuits Syst 29(4):590–598
34. RakA, Cserey G (2010, April) Macromodeling of the memristor in spice. IEEE Trans Comput-
Aided Des Integr Circuits Syst 29(4):632–636
35. Pino R, Bohl J, McDonald N, Wysocki B, Rozwood P, Campbell K, Oblea A, Timilsina
A (2010) Compact method for modeling and simulation of memristor devices: ion con-
ductor chalcogenide-based memristor devices. Proceedings of the IEEE/ACM international
symposium on nanoscale architectures (NANOARCH), pp 1–4, June 2010
36. Itoh M, Chua L (2008) Memristor oscillators. Int J Bifurc Chaos 18(11):3183–3206
37. Linn E, Rosezin R, Kugeler C, Waser R (2010, April) Complementary resistive switches for
passive nanocrossbar memories. Nat Mater 9(5):403–406
38. Manem H, Rose GS, He X, Wang W (2010) Design considerations for variation tolerant multi-
level CMOS/nano memristor memory. In: Proceedings of the 20th symposium on Great lakes
symposium on VLSI, ser. GLSVLSI ’10. New York, NY, USA: ACM, 2010, pp 287–292
[Online]. http://doi.acm.org/10.1145/1785481.1785548
39. Vontobel PO, Robinett W, Kuekes PJ, Stewart DR, Straznicky J, Williams RS (2009) Writing
to and reading from a nano-scale crossbar memory based on memristors. Nanotechnology
20(42):425 204–425 223
40. Qureshi MS, Pickett M, Miao F, Strachan JP (2011) CMOS interface circuits for reading and
writing memristor crossbar array. In: Circuits and systems (ISCAS), 2011 IEEE international
symposium on, May 2011, pp 2954–2957
41. Chen Y-S, Liu W-H, Lee H-Y, Chen P-S, Wang S-M, Tsai C-H, Hsu Y-Y, Gu P-Y, Chen W-S,
Chen F, Lien C-H, Tsai M-J (2011) Impact of compliance current overshoot on high resistance
state, memory performance, and device yield of HfOx based resistive memory and its solution.
In: VLSI technology, systems and applications (VLSI-TSA), 2011 international symposium
on, April 2011, pp 1–2
42. Strukov DB, Likharev KK (2005) CMOL FPGA: a reconﬁgurable architecture for hybrid digital
circuits with two-terminal nanodevices. Nanotechnology 16:888–900
43. Snider GS, Williams SR (2007) Nano/CMOS architectures using a ﬁeld-programmable
nanowire interconnect. Nanotechnology 18(3):1–10
44. Robinett W, Snider G, Stewart D, Straznicky J, Williams R (2007, May) Demultiplexers
for nanoelectronics constructed from nonlinear tunneling resistors. IEEE Trans Nanotechnol
6(3):280–290
45. HoY, Huang G, Li P (2009) Nonvolatile memristor memory: device characteristics and design
implications. In: Computer-aided design—digest of technical papers, 2009. ICCAD 2009.
IEEE/ACM international conference on, November 2009, pp 485–490.
46. Niu D, Chen Y, Xie Y (2010) Low-power dual-element memristor based memory design.
In: Proceedings of the 16th ACM/IEEE international symposium on low power electron-
ics and design, ser. ISLPED ’10. New York, NY, USA: ACM, 2010, pp 25–30 [Online].
http://doi.acm.org/10.1145/1840845.1840851
47. Merkel CE (2011) Thermal proﬁling in CMOS/memristor hybrid architectures. Master’s thesis,
Rochester Institute of Technology

106
D. Kudithipudi and C. E. Merkel
48. Kim H, Sah M, Yang C, Chua L (2010) Memristor-based multilevel memory. In: Cellular
nanoscale networks and their applications (CNNA), 2010 12th international workshop on,
February 2010, pp 1–6
49. Merkel CE, Nagpal N, Mandalapu S, Kudithipudi D (2011) Reconﬁgurable n-level memristor
memory design. In: International joint conference on neural networks, 2011
50. ChoiJ,CherC-Y,FrankeH,HamannH,WegerA,BoseP(2007)Thermal-awaretaskscheduling
at the system software level. In: Proceedings of the 2007 international symposium on low
power electronics and design, ser. ISLPED ’07. NewYork, NY, USA:ACM, 2007, pp 213–218
[Online]. http://doi.acm.org/10.1145/1283780.1283826
51. Burd T, Pering T, Stratakos A, Brodersen R (2000) A dynamic voltage scaled microprocessor
system. In: Solid-state circuits conference, 2000. Digest of technical papers. ISSCC. 2000
IEEE international, 2000, pp 294–295, 466
52. Merrikh-Bayat F, Shouraki SB (2011) Memristor-based circuits for performing basic arithmetic
operations. Proc Comput Sci, pp 128–132
53. Mead C (1990, October) Neuromorphic electronic systems. Proc IEEE 78(10):1629–1636
54. Jo SH, Chang T, Ebong I, Bhadviya BB, Mazumder P, Lu W (2010) Nanoscale memristor
device as synapse in neuromorphic systems. Nano Lett 10(4):1297–1301 (pMID: 20192230)
[Online]. http://pubs.acs.org/doi/abs/10.1021/nl904092h
55. Rose GS, Pino R, Wu Q (2011) A low-power memristive neuromorphic circuit utilizing a
global/local training mechanism. In: Neural networks (IJCNN), the 2011 international joint
conference on, July 31 2011-August 5 2011, pp 2080–2086
56. Snider G (2008) Spike-timing-dependent learning in memristive nanodevices. In: Nanoscale
architectures, 2008. NANOARCH 2008. IEEE International Symposium on, June 2008, pp
85–92
57. AﬁﬁA, Ayatollahi A, Raissi F (2009) Implementation of biologically plausible spiking neural
network models on the memristor crossbar-based CMOS/nano circuits. In: Circuit theory and
design, 2009. ECCTD 2009. European conference on, August 2009, pp 563–566
58. PershinYV, DiVentra M (2011) Neuromorphic, digital and quantum computation with memory
circuit elements. eprint arXiv:1009.6025v3 [cond-mat.mes-hall]
59. Choi S-J, Kim G-B, Lee K, Kim K-H, Yang W-Y, Cho S, Bae H-J, Seo D-S, Kim
S-I, Lee K-J (2011) Synaptic behaviors of a single metal–oxide–metal resistive device.
Appl Phys A: Mater Sci Process 102:1019–1025. doi:10.1007/s00339-011-6282-7 [Online].
http://dx.doi.org/10.1007/s00339-011-6282-7
60. Linares-Barranco B, Serrano-GotarredonaT, Camuas-Mesa LA, Perez-Carrasco JA, Zamarreo-
Ramos C, Masquelier T (2011) On spike-timing-dependent-plasticity, memristive devices, and
building a self-learning visual cortex. Front Neurosci 5:26
61. Merrikh-Bayat F, Shouraki SB (2010) Bottleneck of using single memristor as a synapse and
its solution. eprint arXiv:1008.3450v2 [cs.NE]

Chapter 8
Statistical Memristor Model and Its Applications
in Neuromorphic Computing
Hai (Helen) Li, Miao Hu, and Robinson E. Pino
Abstract More than forty years ago, Professor Chua predicted the existence of the
memristor to complete the set of passive devices that previously includes only re-
sistor, capacitor, and inductor. However, till 2008 the ﬁrst physical realization of
memristors was demonstrated by HP Lab. The unique properties of memristor create
great opportunities in future system design. For instance, the memristor has demon-
strated the similar function as synapse, which makes it promising in neuromorphic
circuits design. However, as a nano-scale device, the process variation control in the
manufacturing of memristors is very difﬁcult. The impact of the process variations on
a neural network system that relies on the continuous (analog) states of the memristor
could be signiﬁcant due to the deviation of the memristor state from the designed
value. So a complete process variation analysis on memristor is necessary for the
application in neural network. Due to the different physical mechanisms, TiO2-based
memristor and spintronic memristor demonstrate very different electrical character-
istics even when exposing the two types of devices to the same excitations and under
the same process variation conditions. In this work, the impact of different geome-
try variations on the electrical properties of these two different types of memristors
was evaluated by conducting the analytic modeling analysis and Monte-Carlo sim-
ulations. A simple algorithm, which is based on the latest characterization method
of LER (line edge roughness) and thickness ﬂuctuation problems, was proposed
to generate a large volume of geometry variation-aware three-dimensional device
structures for Monte-Carlo simulations. We investigate the different responses of the
static and memristive parameters of the two devices and analyze its implication to
H. Li () · M. Hu
Department of Electrical and Computer Engineering,
Polytechnic Institute of New York University, Brooklyn, NY, USA
e-mail: hai.helen.li@gmail.com
M. Hu
e-mail: mhu01@students.poly.edu
R. E. Pino
Advanced Computing, Air Force Research Laboratory, Rome, NY, USA
e-mail: Robinson.Pino@rl.af.mil
R. Kozma et al. (eds.), Advances in Neuromorphic Memristor Science and Applications,
107
Springer Series in Cognitive and Neural Systems,
DOI 10.1007/978-94-007-4491-2_8, © Springer Science+Business Media Dordrecht 2012

108
H. Li et al.
the electrical properties of the memristors. Furthermore, a process-variation aware
device model can be built based on our work. Both corner model and statistical model
can be provided depending on users’ requirements. Our device models make it pos-
sible for scientists and engineers to design neuromorphic circuits with memristive
devices, and therefore, to convert virtual neural network in super computer to the real
hardware memristive system in the future. Rather than the existing crossbar-based
neuron network designs, we focus on memristor-based synapse and the correspond-
ing training circuit to mimic the real biological system. The basic synapse design
is presented, and the training sharing scheme and explore design implication on
multi-synapse neuron system have been explored.
8.1
Introduction
In 1971, Professor Leon Chua predicted the existence of the memristor [1]. However,
the ﬁrst physical realization of memristors was demonstrated by HP Lab very recently
in 2008, in which the memristive effect was achieved by moving the doping front
along a TiO2 thin-ﬁlm device [2]. Soon, memristive systems on spintronic devices
were proposed [3].
The unique properties of memristors create great opportunities in future system
design. For instance, the non-volatility and excellent scalability make it a promising
candidate as the next-generation high-performance high-density storage technology
[4]. More importantly, memristors have an intrinsic and remarkable feature called a
“pinched hysteresis loop” in the i–v plot, that is, memristors can “remember” the total
electric charge ﬂowing through them by changing their resistances (memristances)
[5]. For example, the applications of this memristive behavior in electronic neural
networks have been extensively studied [6, 7].
As process technology shrinks down to decananometer (sub-50 nm) scale, device
parameter ﬂuctuations incurred by process variations have become a critical issue
affecting the electrical characteristics of devices [8]. The situation in a memristive
system could be even worse when utilizing the analog states of the memristors in de-
sign: variations of device parameters, e.g. the instantaneous memristance, can result
in the shift of electrical responses, e.g. current. The deviation of the electrical exci-
tations will affect memristance because the total charge through a memristor indeed
is the historic behavior of its current proﬁle. Previous works on memristor varia-
tion analysis mainly focused on its impacts on non-volatile memory design [4, 9].
However, the systematic analysis and quantitative evaluation on how process vari-
ations affect the memristive behavior still needs to be done. Our work explores the
implications of the device parameters of memristors to the circuit design by taking
into account the impact of process variations. The evaluations were conducted based
on both theoretical analysis and Monte Carlo simulations.
The device geometry variations signiﬁcantly inﬂuence the electrical properties of
nano-devices [10]. For example, the random uncertainties in lithography and pattern-

8
Statistical Memristor Model and Its Applications in Neuromorphic Computing
109
ing processes lead to the random deviation of line edge print-images from their ideal
pattern, which is called line edge roughness (LER) [11]. Thickness ﬂuctuation (TF)
is caused by deposition processes in which mounds of atoms form and coarsen over
time. As technology shrinks, the geometry variations do not decrease accordingly. In
this work, we propose an algorithm to generate a large volume of three-dimensional
memristor structures to mimic the geometry variations for Monte-Carlo simulations.
The LER model is based on the latest LER characterization method for electron beam
lithography (EBL) technology from top-down scanning electron microscope (SEM)
measurement [12].
Some previous experimental results showed that the geometry variations are the
dominateﬂuctuationsourceasprocesstechnologyfurtherscalesdown[8]. Therefore,
we mainly focus on the impacts of geometry variations in this work. However, other
process variations such as random discrete doping (RDD) could also result in the
ﬂuctuations of the electrical properties of devices. RDD is an important and complex
contributor to the variation in MOSFET and other nano-devices since technology
node becomes 90 nm or less. Statistically, RDD is independent to LER and TF
[13, 14], the study of RDD on memristor is a good complementary portion to this
work. We will explore it in the future work.
Memristive function can be achieved by various materials and device structures.
However, the impact of the process variations on the electrical properties of different
memristors could be very different even under the same excitations. Therefore, two
types of memristors, TiO2-based memristor [3] and spintronic memristor [15], are
analyzed and evaluated in our work. These two examples are selected because they re-
spectively represent two important mechanisms: solid state and magnetic. However,
our proposed modeling methodologies and design philosophies are not limited by the
speciﬁc types of devices and can be easily extended to the other structures/materials
with necessary modiﬁcations.
Our contributions can be summarized as follows:
•
We investigate the impacts of geometry variations on the electrical properties
of memristors and explore their implications to circuit design. Monte Carlo
simulations are conducted for quantitative evaluations.
• An algorithm for fast generation of three-dimensional memristor structures is
proposed to mimic the geometry variations incurred by EBL technology. The
generated samples are used for Monte-Carlo simulations.
•
The memristive behavior analysis and evaluations of both TiO2-based and
spintronic memristors are presented.
•
We propose a single memristor-based synapse structure and the corresponding
training circuit design that can be used in neuromorphic computing system.
The design optimization and its implementation in multi-synapse systems are
discussed.

110
H. Li et al.
8.2
Preliminaries
8.2.1
Memristor Theory
The original deﬁnition of the memristor is derived from circuit theory: besides re-
sistor, capacitor and inductor, there must exist the fourth basic two-terminal element
that uniquely deﬁnes the relationship between the magnetic ﬂux (ϕ) and the electric
charge (q) passing through the device [1], or
dϕ = M · dq.
(8.1)
Considering that magnetic ﬂux and electric charge are the integrals of voltage (V) and
current (I) over time, respectively, the deﬁnition of the memristor can be generalized
as:
⎧
⎪⎨
⎪⎩
V = M(ω, I) · I
dω
dt = f (ω, I)
(8.2)
Here, ω is a state variable; M(ω, I) represents the instantaneous memristance, which
varies over time. For a “pure” memristor, neither M(ω, I) nor f (ω, I) is an explicit
function of I [5].
8.2.2
Basics of TiO2 Thin-Film Memristor
In 2008, HP Lab demonstrated the ﬁrst intentional memristive device by using a
Pt/TiO2/Pt thin-ﬁlm structure [2]. The conceptual view is illustrated in Fig. 8.1a: two
metal wires on Pt are used as the top and bottom electrodes, and a thick titanium
dioxide ﬁlm is sandwiched in between. The stoichiometric TiO2 with an exact 2:1
ratio of oxygen to titanium has a natural state as an insulator. However, if the titanium
dioxide is lacking a small amount of oxygen, its conductivity becomes relatively high
like a semiconductor. We call it oxygen-deﬁcient titanium dioxide (TiO2−x) [9]. The
memristive function can be achieved by moving the doping front: A positive voltage
applied on the top electrode can drive the oxygen vacancies into the pure TiO2 part
and therefore lower the resistance continuously. On the other hand, a negative voltage
applied on the top electrode can push the dopants back to the TiO2−x part and hence
increase the overall resistance. For a TiO2-based memristor, RL (RH) is used to denote
the lowest (highest) resistance of the structure.
Figure 8.1b illustrates a coupled variable resistor model for a TiO2-based mem-
ristor, which is equivalent to two series-connected resistors. The overall resistance
can be expressed as
M(α) = RL · α + RH · (1 −α).
(8.3)

8
Statistical Memristor Model and Its Applications in Neuromorphic Computing
111
Doping
front
Voltage
L
z
h
Pt
Pt
TiO2
TiO2-x
a
b
RL·
RH·(1-)
Fig. 8.1 TiO2 thin-ﬁlm memristor. (a) structure, and (b) equivalent circuit
Here α (0 ≤α ≤1) is the relative doping front position, which is the ratio of doping
front position over the total thickness of TiO2 thin-ﬁlm.
The velocity of doping front movement v(t), which is driven by the voltage applied
across the memristor V(t) can be expressed as
v(t)
h
= dα
dt = μv · RL
h2 · V (t)
M(α)
(8.4)
where, μv is the equivalent mobility of dopants, h is the total thickness of the TiO2
thin-ﬁlm; and M(α) is the total memristance when the relative doping front position
is α.
Filamentary conduction has been observed in nano-scale semiconductors, such
as TiO2. It shows that the current travels through some high conducting ﬁlaments
rather than passes the device evenly [16, 17]. However, there is no device model
based on ﬁlamentary conduction mechanism yet. Considering that the main focus of
this work is the process variation analysis method of the memristor, which can be
separated from the explicit physical model of memristor, the popular bulk model of
TiO2 is applied. We will extend the research by integrating the device model based
on ﬁlamentary conduction in our future work.
Recent experiments showed that μv is not a constant but grows exponentially
when the bias voltage goes beyond certain threshold voltage [18]. Nevertheless, the
structure of TiO2 memristor model, i.e., Eq. (8.3), still remains valid.
8.2.3
Basics of Spintronic Memristor
Among all the spintronic memristive devices, the one based on magnetic tunneling
junction (MTJ) could be the most promising one because of its simple structure
[3, 15]. The basic structure of magnetic memristor could be either giant magneto-
resistance (GMR) or tunneling magneto-resistance (TMR) MTJs. We choose TMR-
based structure shown in Fig. 8.2a as the objective of this work because it has a bigger
difference between the upper and the lower bounds of total memristance (resistance).

112
H. Li et al.
Fig. 8.2 TMR-based spintronic memristor. (a) structure, and (b) equivalent circuit
There have been many research activities investigated on the spintronic memristor
or the similar device structure, such as the racetrack structure proposed by IBM [19].
Very recently, NEC Lab reported the free layer switching through the domain wall
movement [20], which indeed is a spintronic memristor.
An MTJ is composed of two ferromagnetic layers and an oxide barrier layer,
e.g. MgO. The bottom ferromagnetic layer is called reference layer, of which the
magnetization direction is ﬁxed by coupling to a pinned magnetic layer. The top
ferromagnetic layer called free layer is divided into two magnetic domains by a
domain-wall: the magnetization direction of one domain is parallel to the reference
layer’s, while the magnetization direction of the other domain is anti-parallel to the
reference layer’s.
The movement of the domain wall is driven by the spin-polarized current, which
passesthroughthetwoferromagneticlayers. Forexample, applyingapositivevoltage
on free layer can impel the domain wall to increase the length of the magnetic domain
with a magnetization direction parallel to the reference layer’s and hence reduce
the MTJ resistance. On the other hand, applying a positive voltage on reference
layer will reduce the length of the magnetic domain with a magnetization direction
parallel to the reference layer’s. Therefore, the MTJ resistance increases. If the
width of the domain with the magnetization direction anti-parallel (parallel) to the
referencelayer’siscompressedtoclosetozero, thememristorhasthelowest(highest)
resistance, denoted as RL (RH).
As shown in Fig. 8.2b, the overall resistance of a TMR-base spintronic memristor
can be modeled as two parallel connected resistors with resistances RL/α and RH/
(1 −α), respectively [15]. This structure has also been experimentally proved [21].
Here α (0 ≤α ≤1) represents the relative domain wall position as the ratio of the
domain wall position (x) over the total length of the free layer (L). The overall
memristance can be expressed as
M(α) =
RL · RH
RH · α + RL(1 −α).
(8.5)
How fast the domain-wall can move is mainly determined by the strength of spin-
polarized current. More precisely, the domain-wall velocity v(t) is proportional to

8
Statistical Memristor Model and Its Applications in Neuromorphic Computing
113
Table 8.1 The device
dimensions (nm) of
memristors
Length (L)
Width (z)
Thickness (h)
Thin-ﬁlm
50
50
10
Spintronic
200
10
7
the current density J [22]. We have
J(t) =
V (t)
M(α) · L · z,
(8.6)
and
v(t) = dα(t)
dt
= v
L · Jeff (t),
Jeff =

 J,
J ≥Jcr
0,
J ≤Jcr.
(8.7)
Here v is the domain wall velocity coefﬁcient, which is related to device structure
and material property. L and z are the total length and width of the spintronic mem-
ristor, respectively. The domain wall movement in the spintronic memristor happens
only when the applied current density (J) is above the critical current density (JCr)
[22–26].
8.3
Mathematical Analysis
The actual length (L) and width (z) of a memristor is affected by LER. The variation
of thickness (h) of a thin ﬁlm structure is usually described by TF. As a matter of
convenience, we deﬁne that, the impact of process variations on any given variable
can be expressed as a factor θ = ω′/ω, where ω is its ideal value, and ω′ is the actual
value under process variations.
The ideal geometry dimensions of the TiO2 thin-ﬁlm memristor and spintronic
memristor used in this work are summarized in Table 8.1.
8.3.1
TiO2 Thin-Film Memristor
In TiO2 thin-ﬁlm memristors, the current passes through the device along the thick-
ness (h) direction. Ideally the doping front has an area S −L · z. To simulate the
impact of LER on the electrical properties, the memristor device is divided into many
small ﬁlaments between the two electrodes. Each ﬁlament i has a cross-section area
ds and a thickness h. Figure 8.3 demonstrates a non-ideal 3D structure of a TiO2
memristor (i.e., with geometry variations in consideration), which is partitioned into
many ﬁlaments in statistical analysis.

114
H. Li et al.
Fig. 8.3 An example of 3D
TiO2 memristor structure,
which is partitioned into
many ﬁlaments in statistical
analysis
As shown in Fig. 8.3, ideally, the cross-section area of a ﬁlament is ds/S of the
entire device area and its thickness is h. Thus, for ﬁlament i, the ideal upper bound
and lower bound of the memristance can be expressed as
Ri,H = RH · S
ds ,
and
Ri,L = RL · S
ds .
(8.8)
Here, θi,s represents the variation ratio on the cross-section area ds, which is caused by
2-D LER. Similarly, θi,h is the variation ratio on thickness h due to TF. The resistance
of a ﬁlament is determined by its section area and thickness, i.e., R = ρ · h/s, where
ρ is the resistance density. Therefore, the actual upper and the lower bound under
the process variations can be expressed as
R′
i,H = Ri,H · θi,h
θi,s
,
and
R′
i,L = Ri,L · θi,h
θi,s
.
(8.9)
If a ﬁlament is small enough, we can assume it has a ﬂat doping front. Then, the actual
doping front velocity in ﬁlament i considering process variations can be calculated
by replacing the ideal values with actual values in Eq. (8.4). We have
v′
t(t) = μv · R′
i,L
h′2 ·
V (t)
M′
t(α′
i).
(8.10)
Here h′ and M′
i are the actual thickness and memristance of ﬁlament i. Then, we can
get a set of related equations for ﬁlament i, including the doping front position
α′
i(t) =
 t
0
v′(τ) · dτ,
(8.11)
the corresponding memristance
M′
i(α′
i) = α′
i · R′
i,L + (1 −α′
i) · R′
i,H,
(8.12)

8
Statistical Memristor Model and Its Applications in Neuromorphic Computing
115
and the current through the ﬁlament i
I ′
i(t) =
V (t)
M′
i(α′
i).
(8.13)
By combining Eq. (8.10)—(8.13), the doping front position in every ﬁlament i under
process variations α′i(t) can be obtained by solving the differential equation
dα′
i(t)
dt
= μv · R′
i,L
h′2 ·
V (t)
α′
i(t) · R′
i,L + (1 −α′
i(t)) · R′
i,H
.
(8.14)
Equation(8.14)indicatesthatthebehaviorofthedopingfrontmovementisdependent
on the speciﬁc electrical excitations, e.g., V(t).
For instance, applying a sinusoidal voltage source to the TiO2 thin-ﬁlm memristor
such as
V (t) = Vm · sin (2πf · t),
(8.15)
the corresponding doping front position of ﬁlament i can be expressed as:
α′
i(t) =
Ri,H −

R2
i,H −A · B(t) ·
2
θ2
i,h + 2C · A · θi,s
θi,h
A
.
(8.16)
Where, A = Ri,H −Ri,L, B(t) = μv · Ri,L · V m · cos(2πf · t), and C is an initial state
constant.
The term B(t) accounts for the effect of electrical excitation on doping front
position. The terms θi,s and θi,h represent the effect of both LER and TF on memristive
behavior. Moreover, the impact of the geometry variations on the electrical properties
of memristors could be affected by the electrical excitations. For example, we can
set α(0) = 0 to represent the case that the TiO2 memristor starts from M(0) = RH. In
such a condition, C becomes 0, and hence, the doping front position α′
i(t) can be
calculated as:
α′
i(t) =
Ri,H −

R2
i,H −A · B(t) ·
2
θ2
i,h
A
,
(8.17)
which is affected only by TF and electrical excitations. LER will not disturb α′
i(t) if
the TiO2 thin-ﬁlm memristor has an initial state α(0) = 0.
The overall memristance of the memristor can be calculated as the total resistance
of all n ﬁlaments connected in parallel. Again, i denotes the ith ﬁlament. When n
goes to ∞, we can have
R′
H =
1
 ∞
0
1
R′
i,H · di
= RH ·
1
 ∞
0
θi,h
θi,s · di
,
(8.18)
and
R′
L =
1
 ∞
0
1
R′
i,L · di
= RL ·
1
 ∞
0
θi,h
θi,s · di
.
(8.19)

116
H. Li et al.
Fig. 8.4 An example of 3D
TMR-based spintronic
memristor structure, which is
partitioned into many
ﬁlaments in statistical
analysis
−5
0
5
10
15
0
50
100
150
200
0
5
10
l
3−D model for TMR−based spintronic memristor
z
h
i
The overall current through the memristor is the sum of the current through each
ﬁlament:
I ′(t) =
 ∞
0
I ′
i(t) · di.
(8.20)
The instantaneous memristance of the overall memristor can be deﬁned as
M′(t) = V (t)
I ′(t) =
1
 ∞
0
1
M′
i · di .
(8.21)
Since the doping front position movement in each ﬁlament will not be the same
because h′
i varies due to TF (and/or the roughness of the electrode contact), we
deﬁne the average doping front position of the whole memristor as:
α′(t) = R′
H −M′(t)
R′
H −R′
L
.
(8.22)
8.3.2
Spintronic Memristor
Since the length of a spintronic memristor is usually much longer than the other
two dimensions, the impact of the variance in length on the spintronic memristor’s
electrical properties can be ignored. In our analysis, the device can be chopped into
inﬁnite segments along the length direction as shown in Fig. 8.4. For a segment i,
the upper and lower bounds of memristance are:
R′
i,H = Ri,H · θi,h
θi,z
,
and
R′
i,L = Ri,L · θi,h
θi,z
.
(8.23)
Here we assume the ideal memristance changes linearly within the domain wall, or
Mi changes linearly from Rj,L to Rk,H when j < i < k. Here j and k are the two segments

8
Statistical Memristor Model and Its Applications in Neuromorphic Computing
117
at the two boundaries of domain wall and connected to the magnetic domains with
either the low or the high resistance states. The memristance of each segment is
M′
i =

 R′
i,L,
i < α′
R′
i,H,
i ≥α′
(8.24)
So for overall resistance R′
H and R′
L, we have
R′
H =
1
 ∞
0
1
R′
i,H · di
= RH ·
1
 ∞
0
θi,z
θi,h · di
,
(8.25)
and
R′
L =
1
 ∞
0
1
R′
i,L · di
= RL ·
1
 ∞
0
θi,z
θi,h · di
,
(8.26)
Then the memristance of the whole device is
M′(α′) =
1
 α′
0
1
R′
i,L di +
 1
α′
1
R′
i,H di
=
1
 α′
0
1
Ri,L · θi,z
θi,h di +
 1
α′
1
Ri,H · θi,z
θi,h di
(8.27)
Here the width of each segments zi varies segment by segment due to the LER
effect. The statistical behavior of spintronic memristors can still be evaluated by
Monte-Carlo simulation in Sect. 5.
We assume the current density applied on the domain wall J ′(t) is the one of the
segments i where the domain wall located in the middle:
J ′(t) = J ′
i =
V (t)
M′(α′) · L · z′
i
.
(8.28)
Then the domain wall velocity under process variations can be calculated as:
v′(t) = v′
i = dα′(t)
dt
= v
L · J ′
eff (t),
J ′
eff =

 J ′,
J ′ ≥Jcr
0,
J ′ < Jcr
(8.29)
8.4
3D Memristor Structure Modeling
Analytic modeling is a fast way to estimation the impact of process variations on
memristors. However, we noticed that in modeling some variations analytically, e.g.
simulating the LER, may be beyond the capability of analytic model [12]. The data on

118
H. Li et al.
Fig. 8.5 The ﬂow of 3D
memristor structure
generation including
geometry variations
START
3D Sample of
Memristor structure
Generate line edge
LER data analysis 
sample and smooth
Pass check?
N
Y
Thickness fluctuations
Electrode cont. roughness
Combine
Thickness (h)
LER (L, z)
silicon variations, however, is usually very hard to obtain simply due to intellectual
property protection. To improve the accuracy of our evaluations, we create a simula-
tion ﬂow to generate 3-D memristor samples with the geometry variations including
LER and thickness ﬂuctuation. The correlation between the generated samples and
the real silicon data are guaranteed by the sanity check of the LER characterization
parameters. The ﬂow is shown in Fig. 8.5.
Many factors affecting the quality of the line edges show different random effects.
Usually statistical parameters such as the auto-correlation function (ACF) and power
spectral density (PSD) are used to describe the property of the line edges.
ACF is a basic statistical function of the wavelength of the line proﬁle, represent-
ing the correlation of point ﬂuctuations on the line edge at different position. PSD
describes the waveform in the frequency domain, reﬂecting the ratio of signals with
different frequencies to the whole signal.
Considering that LER issues are related to fabrication processes, we mainly target
the nano-scale pattern fabricated by electron beam lithography (EBL). The measure-
ments show that under such a condition, the line edge proﬁle has two important
properties: (1) the line edge proﬁle in ACF ﬁgure demonstrates regular oscillations,
which are caused by periodic composition in the EBL fabrication system; and (2) the
line edge roughness mainly concentrates in a low frequency zone, which is reﬂected
by PSD ﬁgure [12].
To generate line edge samples close to the real cases, we can equally divide
the entire line edge into many segments, say, n segments. Without losing the LER
properties in EBL process, we modiﬁed the random LER modeling proposed in [27]
to a simpler form with less parameters. The LER of the ith segment can be modeled by
LERi = LLF · sin (fmax · xi) + LHF · pi.
(8.30)
The ﬁrst term on the right side of Eq. (8.30) represents the regular disturbance at
the low frequency range, which is modeled as a sinusoid function with amplitude
LLF . f max the mean of the low frequency range derived from PSD analysis. Without
loss of generality, a uniform distribution xi ∈U(–1, 1) is used to represent an equal

8
Statistical Memristor Model and Its Applications in Neuromorphic Computing
119
Table 8.2 The
parameters/constraints in
LER characterization
Parameters
Constraints (nm)
LLF
0.8 nm
σLER
2.5 ∼3.5
f max
1.8 MHz
σLWR
4.0 ∼5.0
LHF
0.4 nm
Sk
0 ∼0.3
/
/
Ku
2.7 ∼3.3
distribution of all frequency components in the low frequency range. The high
frequency disturbances are also taken into account by the second term on the right
side of Eq. (8.30) as a Gaussian white noise with amplitude LHF. Here pi follows
the normal distribution N(0, 1) [12]. The actual values of LLF, LHF and f max are
determined by ACF and PSD.
To ensure the correlation between the generated line edge samples with the mea-
surement results, we introduce four constraints to conduct a sanity check of the
generated samples:
•
σLER: the root mean square (RMS) of LER;
•
σLWR: the RMS of line width roughness (LWR);
•
Sk: skewness, used to specify the symmetry of the amplitude of the line edge; and
•
Ku: kurtosis, used to describe the steepness of the amplitude distribution curve.
The above four parameters are widely used in LER characterization and can be
obtained from measurement results directly [12]. Only the line edge samples that
satisfy the constraints will be taken as valid device samples. Table 8.2 summarizes
the parameters used in our algorithm, which are correlated with the characterization
method and experimental results in [12]. And Fig. 8.6 shows the LER characteristic
parameters distribution among 1,000 Monte-Carlo simulations.
Even the main function has captured the major features of LER, it is not enough
to mimic all the LER characteristics. The difference between real LER distribution
and our modeling function results in the fact that some generated samples are not
qualiﬁed compared to the characteristic parameters, or the constraints of the real LER
Fig. 8.6 LER characteristic parameters distribution among 1000 Monte-Carlo simulations. Con-
straints are shown in red rectangles

120
H. Li et al.
proﬁle. Thus, sanity check which screens out the unsuccessful results is necessary.
Only those samples in red rectangles shown in Fig. 8.6 satisfy the constraints and will
be used for the device electrical property analysis. The criteria of the sanity check
are deﬁned based on the measurement results of real LER data.
The thickness ﬂuctuation is caused by the random uncertainties in sputter depo-
sition or atomic layer deposition. It has a relatively smaller impact than the LER
and can be modeled as a Gaussian distribution. Since the memristors in this work
have relatively bigger dimensions in the horizontal plane than the thickness direc-
tion (shown in Table 8.1), we also considered roughness of electrode contact in our
simulation: The means of the thickness of each memristor is generated by assuming
it follows the Gaussian distribution. Each memristor is then divided into many ﬁla-
ments between the two electrodes. The roughness of electrode contacts is modeled
based on the variations of the thickness of each ﬁlament. Here, we assume that both
thickness ﬂuctuations and electrode contact roughness follow Gaussian distributions
with a deviation σ = 2 % of thin ﬁlm thickness.
Figure 8.3 is an example of 3D structure of a TiO2 thin-ﬁlm memristor generated
by the proposed ﬂow. It illustrates the effects of all the geometry variations on a
TiO2 memristor device structure. According to Sect. 3, a 2-D partition is required for
the statistical analysis. In the given example, we partition the device into 25 small
ﬁlaments with the ideal dimensions of L = 10 nm, z = 10 nm, and h = 10 nm. Each
ﬁlament can be regarded as a small memristor, which is affected by either only TF
or both LER and TF. The overall performance of device can be approximated by
paralleled connecting all the ﬁlaments.
Similarly, Fig. 8.4 is an example of 3D structure of a TMR-based spintronic
memristor. Since the length of a spintronic memristor is much longer than its width
and height, only 1-D partition along the length direction is required. In this case, the
device is divided into 200 ﬁlaments. Ideally, each ﬁlament has L = 1 nm, z = 10 nm,
and h = 7 nm. Each ﬁlament i is either in the low resistance state R′i,L or the
high resistance state R′i,H, with considering the effects of both LER and TF. The
overall performance of device can be approximated by paralleled connecting all the
ﬁlaments.
8.5
Experimental Results
8.5.1
Simulation Setup
To evaluate the impact of process variations on the electrical properties of memristors,
we conducted Monte-Carlo simulations with 10,000 qualiﬁed 3-D device samples
generated by our proposed ﬂow. A sinusoidal voltage source shown in Eq. (8.15)
is applied as the external excitation. The initial state of the memristor is set as
M(α = 0) = RH. The device and electrical parameters used in our simulations are
summarized in Table 8.3. Both separate and combined effects of geometry variations
on various properties of memristors are analyzed, including:

8
Statistical Memristor Model and Its Applications in Neuromorphic Computing
121
Table 8.3 Memristor Devices and electrical parameters
RL ()
RH ()
μv (m2 · s−1 · V−1)
v (nm3 × C−1)
Jcr (A/nm2)
V m (V)
f (Hz)
TiO2 thin-ﬁlm memristor [2]
100
16000
10−14
1
0.5
Spintronic memristor [15]
2500
7500
2.01 ×10−14
2.00 × 10−8
2
10 M
Table 8.4 3 σ min/max of TiO2 memristor parameters
LER only
TF only
overall
−3σ(%)
+3σ(%)
−3σ(%)
+3σ(%)
−3σ(%)
+3σ(%)
Sinusoidal, Voltage
RH& RL
−5.4
4.1
−5.5
4.8
−6.4
7.3
M(α)
−5.4
4.1
−37.1
20.8
−36.5
24.1
α(t)
0.0
0.0
−13.3
27.5
−14.7
27.4
v(α)
0.0
0.0
−9.3
15.6
−10.4
16.9
i(i)
−4.7
5.7
−9.3
15.7
−10.7
17.2
Power
−4.7
5.7
−8.8
14.1
−10.1
15.6
Square wave Voltage
RH& RL
−5.3
3.7
−6.2
5.2
−6.6
6.9
M(α)
−5.3
3.7
−17.8
13.2
−15.4
14.4
α(t)
0.0
0.0
−12.1
16.6
−13.0
15.6
v(α)
0.0
0.0
−11.6
17.7
−12.5
16.7
i(α)
−4.0
5.2
−11.7
17.7
−12.6
17.6
Power
−4.0
5.2
−7.7
9.8
−8.5
10.1
•
the distribution of RH and RL;
•
the change of memristance M(t) and M(α);
•
the velocity of wall movement v(α);
•
the current through memristor i(t); and
•
the I-V characteristics.
8.5.2
TiO2 Thin-Film Memristor
The ± 3σ (minimal/maximal) values of the device/electrical parameters as the per-
centage of the corresponding ideal values are summarized in Table 8.4. For those
parameters that vary over time, we consider the variation at each time step of all the
devices. The simulation results considering only either LER or TF are also listed.
To visually demonstrate the overall impact of process variations on the memristive
behavior of TiO2 memristors, the dynamic responses of 100 Monte Carlo simulations
are shown in Fig. 8.7.
Table 8.4 shows that the static behavior parameters of memristors, i.e., RH and
RL, are affected in a similar way by both LER and thickness ﬂuctuations. This is

122
H. Li et al.
Fig. 8.7 Simulation results for TiO2 thin-ﬁlm memristors. The blue curves are from 100 Monte-Carlo simulations, and red lines are the ideal condition.
From top left to right bottom, the ﬁgures are RH vs. RL; M(t) vs. t; v vs. α; α vs. t; I vs. t; and I −V characteristics

8
Statistical Memristor Model and Its Applications in Neuromorphic Computing
123
consistent to our analytical results in Eq. (8.18) and (8.19), which show that θs and
θh have the similar effects on the variation of R′
H and R′
L.
However, thickness ﬂuctuation shows a much more signiﬁcant impact on the
memristive behaviors such as v(t), α(t) and M(α), than LER does. It is because the
doping front movement is along the thickness direction: v(t) is inversely proportional
to the square of the thickness, and α(t) is the integral of v(t) over time as shown in
Eq. (8.10) and (8.11). For the same reason, thickness ﬂuctuations signiﬁcantly affect
the instantaneous memristance M(α) as well.
Because the thickness of the TiO2 memristor is relative small compared to other
dimensions, we assume the doping front cross-section area is a constant along the
thickness direction in our simulation. The impact of LER on α(t) or v(t), which is
relatively small compared to that of the thickness ﬂuctuations, is ignored in Table 8.4.
An interesting observation in Fig. 8.7 is that as the doping front α moves toward
1, the velocity v regularly grows larger and reaches its peak at the half period of the
sinusoidal excitation, i.e. t = 1 s. This can be explained by Eq. (8.12): the memristance
is getting smaller as α moves toward 1. With the same input amplitude, a smaller
resistance will result in a bigger current and therefore a bigger variation on v(t).
Similarly, memristance M(α) reaches its peak variance when α is close to 1.
We also conduct 10,000 × Monte Carlo simulations on the same samples by ap-
plying a square wave voltage excitation. The amplitude of the voltage excitation is
± 0.5 V. The simulation results are also shown in Table 8.4. The results of the static
behavior parameters, i.e., RH and RL, are exactly the same as those with sinusoidal
voltage inputs because they are independent of the external excitations, The results of
the memristive behavior parameters such as v(t), α(t) and M(α) show similar trends
as those with the sinusoidal voltage inputs. Based on Eq. (8.16), α(t)’s variance
is sensitive to the type and amplitude of electrical excitation, because B(t) greatly
affects the weight of the thickness ﬂuctuation parameter. That is why the thickness
ﬂuctuation has a signiﬁcantly impact on the electrical properties of memristors under
sinusoidal and square voltage excitations.
8.5.3
Spintronic Memristor
The ±3σ values of the device/electrical parameters based on 10,000 Monte-Carlo
simulations are summarized in Table 8.5. The visual demonstration of 100 Monte-
Carlo simulations with a sinusoidal voltage excitation is shown in Fig. 8.8.
For the spintronic memristor, the impact of LER on the electrical properties of
memristors is more than that of thickness ﬂuctuation. This is because the direction
of the domain wall movement is perpendicular to the direction of spin-polarized
current. The impact of thickness ﬂuctuations on very small segments cancel each
other during the integral along the direction of the domain wall movement.
“LER only” simulation results show that the +3σ corner of LER has more impact
on the electrical properties than that of −3σ corner. This is because the line width

124
H. Li et al.
Table 8.5 3 σ min/max of spintronic memristor parameters
LER only
TF only
overall
−3σ(%)
+3σ(%)
−3σ(%)
+3σ(%)
−3σ(%)
+3σ(%)
Sinusoidal Voltage
RH&RL
−15.3
22.9
−6.1
5.8
−16.4
20.9
M(α)
−15.1
23.3
−11.0
11.0
−16.3
21.1
α(t)
−9.7
8.1
−8.4
9.5
−11.8
8.1
v(α)
−10.7
22.1
−9.1
9.9
−21.5
22.5
i(α)
−18.5
18.5
−8.9
10.1
−17.7
17.8
Power
−18.4
18.6
−8.3
9.4
−17.8
17.8
Square wave Voltage
RH&RL
−15.8
22.0
−5.3
5.7
−15.9
24.2
M(α)
−15.6
21.8
−8.5
9.7
−17.0
25.5
α(t)
−13.1
13.8
−7.5
7.7
−17.2
16.2
v(α)
−16.5
20.7
−10.0
8.3
−20.1
25.2
i(α)
−19.5
17.1
−9.0
9.3
−22.1
20.5
Power
−19.4
17.1
−7.6
7.7
−20.9
19.6
variation is the dominant factor on the variation of electrical properties of spintronic
memristors, and the line edge proﬁles used in our LER parameters have a right-biased
feature [12]. Since normal distribution is assumed for the variations of thickness, σh
has approximately symmetric impact on ± 3σ corners.
The impact of LER on the memristive parameters v(t), α(t) and M(α) is also larger
than thickness variation. Again, the impact of thickness ﬂuctuations on very small
segments cancel each other during the integral along the direction of the domain wall
movement.
Similarly, we also conduct Monte Carlo simulations by applying a square wave
voltage excitation. The amplitude of the voltage excitation is ±1V. The similar trends
as that of sinusoidal excitations are observed.
8.6
Memristor-based Synapse Design
8.6.1
The Principle of Memristor-based Synapse
Rather than using memristor crossbar array in neuromorphic reconﬁgurable architec-
ture, we propose a memristor-based synapse design to mimic the biological structure.
Figure 8.9a depicts the conceptual scheme, which simply consists of a NMOS tran-
sistor (Q) and a memristor. When the input V in is low, Q is turned off and the output
V out is connected to ground through the memristor. On the contrary, when V in is high
and turns on Q, memristance M and the equivalent resistance of Q (RQ) together
determine V out:
Vout = f (Vin · M).
(8.31)

8
Statistical Memristor Model and Its Applications in Neuromorphic Computing
125
Fig. 8.8 Simulation results for spintronic memristors. The blue curves are from 100 Monte-Carlo simulations, and red lines are the ideal condition. From
top left to right bottom, the ﬁgures are RH vs. RL; M(t) vs. t; v vs. α; α vs. t; I vs. t; and I –V characteristics

126
H. Li et al.
Fig. 8.9 (a) Proposed synapse design. (b) Synapse output vs. memristance
Here, V out is weighted by the memristance, which behaves like a synapse. Figure 8.9b
shows the simulated V out when sweeping the memristance from 1 to 16 K. Here,
CMOS devices used TSMC 0.18 μm technology.
Note that the response of the synapse design is dependent on the equivalent re-
sistance of the transistor Q (RQ), or, the size of Q. This can also be demonstrated in
Fig. 8.9b by sweeping the width of Q from 220 nm to 4.4 μm with a step of 220 nm.
The simulation shows that a larger Q can result in a wider range of V out with poorer
linearity. However, for a large Q, the enhancement of V out by further increasing its
size is marginal. To improve design stability, a buffer can be added at output of
the synapse to increase voltage swing. Furthermore, some circuit optimization tech-
niques, such as asymmetry gate in other blocks, can be used to minimize the overall
synapse-based system.
8.6.2
Synapse Training Circuit
Being self-adaptive to the environment is one of the most important properties of
a biological synapse. To accomplish the similar functionality, a training block is
needed in the memristor-based synapse that can adjust its memristance.
8.6.2.1
Memristor Training Circuit
Figure 8.10a shows the diagram of training circuit for one synapse design, based
on logic analysis and simpliﬁcation. It includes two major components: training
controller and write driver. By comparing the current synapse output V out and the
expected output Dtrain, training controller generates the control signals. The write
driver uses these signals to control two pairs of NMOS and PMOS switches and
supply training voltage pair V top and V bot. The training pair is applied to the two
terminals of the memristor in the synapse design.
Determined by the training enable signal E, the training circuit can work under
two modes.

8
Statistical Memristor Model and Its Applications in Neuromorphic Computing
127
a
b
M
E
Vout
Wa
Wb
Wc
Wd
VDD
GND
GND
NAND
OR
INV3
Write Driver
Training Controller
Q1
Q2
Q3
Q4
D Q
Clk
Q
Dtrain
INV1
INV2
Vout’
Latch
Training
Circuit
E
Dtrain
Vtop
Vtop
Vbot
Vbot
VDD
VDD
Vin
Q1
Q2
M
E
Vout
Fig. 8.10 (a) The training circuit diagram. (b) The proposed synapse together with training circuit
a
b
Dtrain
E
Vout’
Vtop
Vbot
Floating
Operating
Training
Don’t care
0
1
2
Voltage (V)
Dtrain
0
1
2
Voltage (V)
E
0
10
20
0
25
50
75
100
Memristance 
(KΩ)
M
Time (ms)
4.5ns
read
Fig. 8.11 a The timing diagram of training circuit, b The simulation result of memristor training
•
Operating mode: When E = 0, the synapse is under regular operating (read) mode,
and the training circuit is disabled.
•
Training mode: The training circuit is enabled when E = 1. By comparing the
current synapse output V out and the expected Dtrain, the training circuit generates
V top and V bot applied to the two terminals of memristor to update or keep its
memristance. We deﬁne V mem =V top −Vbot.
Figure 8.10b depicts the proposed memristor-based synapse integrated with train-
ing circuit. An extra NMOS transistor Q2 is inserted in synapse to isolate training
operation from other voltage sources: when E = 1, Q2 is turned off so that the two
terminals of memristor are controlled only by the training circuit, not affected by V in.
ThetimingdiagramoftrainingcircuitisdemonstratedFig.8.11a. Beforeatraining
procedure starts, a sensing step is required to detect the current V out to be compared
with Dtrain. In the sensing phase, accordingly, training enable signal E is set to low
for a very short period of time (e.g., 4.5 ns) at the beginning of training. At the
same time, (V ′out) is sent to Latch, whose output (V ′out) remains constant during
one training period, as shown in Fig. 8.10a. In the training phase, E is set back to
high for a much longer time (i.e., 51 ms) to change the memristance if needed.

128
H. Li et al.
Table 8.6 Sizing of INV 1
and Q1
P/N Ratio
PMOS/NMOS in INV1 (nm)
Q1 (nm)
2
720/360
18 × 220
2
440/220
16 × 220
1
360/360
12 × 220
1
220/220
11 × 220
0.5
360/720
9 × 220
0.5
220/440
9 × 220
a
b
A1
S1 (M1)
S2 (M2)
Vout2
Vout1
N
Vout
Transmission
Gate
Transmission
Gate
AND
AND
AND
AND
Vtop1
Vtop2
Vbot1
Vbot2
Vbot
Vtop
A2
A1
E
A2
Fig. 8.12 (a) Two-input neuron structure. (b) Training sharing distribution circuit
We tested the training procedure by using the TiO2 memristor model [2]. The
training circuit was designed by using TSMC 0.18 μm technology with V DD = 1.8 V.
Changing memristance from RH to RL or verse vice takes about 51 ms. The simulation
result is shown in Fig. 8.11b. Here, the memristance is initialized as M=16 K. In
the ﬁrst 51 ms, it is trained to 1 K by setting Dtrain to low. Then at t = 51 ms, we set
Dtrain to high and train the memristance back to RH in the following 51 ms.
8.6.2.2
Asymmetry Gate Design
The size of Q1 affects the range of V out. Instead of adding buffer or having giant Q1
in synapse, the asymmetry gate design can be adopted to minimize the layout area of
synapse design. More speciﬁcally, we tuned P/N ratio of INV 1 in the training circuit
(see Fig. 8.10a). Table 8.6 summarizes the required sizes of INV 1 and Q1 under
different combinations that can make training successful. The result shows that the
asymmetric design with P/N ratio = 0.5 can obtain the smallest area. The last option
is used in the following synapse analysis.
8.6.2.3
Multi-synapse Training Scheme
Most of the neuron systems are constructed by multiple synapses. In this subsec-
tion, we discuss the corresponding training scheme by taking a 2-synapse neuron in
Fig. 8.12a as the example. Here, A1 andA2 are two synapse inputs received from other
neurons. M1 and M2 are memristor-based weights for two synapses S1 and S2. N is
denoted for neuron with outputV out. The value ofV out depends on the functionality of

8
Statistical Memristor Model and Its Applications in Neuromorphic Computing
129
Table 8.7 Training sharing
circuit operation
Status
V top1
V bot1
V top2
V bot2
Training M1
V top
V bot
Floating
0
Training M2
Floating
0
V top
V bot
Table 8.8 Synapse input
pairs for different logics
Function of N
Training M1
Training M2
OR/NOR
A1 = 1, A2 = 0
A1 = 0, A2 = 1
XOR/XNOR
A1 = 1, A2 = 0
A1 = 0, A2 = 1
AND/NAND
A1 = 1, A2 = 1
A1 = 1, A2 = 1
N as well as V out1 and V out2 from the two synapses. With the different combinations
of M1 and M2, the two-input neuron could obtain different functionality.
To save design cost, memristances of the 2-synapse can be trained separately and
share one training circuit. Fig. 8.12b shows a training sharing distribution circuit,
which generates training signals to control M1 and M2. The training sharing circuit
operations under different conditions are shown in Table 8.7.
The two synapse inputs A1 and A2 can be used to determine which memristor,
M1 or M2, is in training. Table 8.8 lists the required A1 and A2, when the logic
functionality of N is one of the following: OR/NOR, XOR/XNOR, AND/NAND.
Compared to the separated training circuit for each memristor, the shared scheme
can reduce 26 % of training circuit transistor count. More saving in cost and area can
be obtained when utilizing this training sharing distribution scheme to multi-synapse
structure with more inputs.
8.6.2.4
Self-Training Mode
To improve training time and reduce power consumption, we introduce the concept of
self-training in our design: rather than using a ﬁxed long training period (i.e., 51 ms),
the self-training mode automatically stop programming memristor whenever V out
and Dtrain become same.
The proposed training circuit supports self-training mode by dividing a long
training period into multiple shorter periods and detecting V out in between. The pro-
gramming period needs to be carefully selected: if it is too short, the delay and energy
overheads induced by V out detection may overwhelm the beneﬁt of self-training. On
the contrary, a long programming period cannot show enough beneﬁt.
ThesimulationresultinFig.8.13showsthememristancechangingwhensweeping
programming period from 5.1 to 51 ms in 10 steps. Obviously, the self-training mode
could signiﬁcantly reduce training time. In theory, the proposed training circuit can
train the memristance to any value between RH and RL. The real training time is
determined by the speciﬁc application and neuron functionality.

130
H. Li et al.
Fig. 8.13 Self-training
simulation
0
4
8
12
16
0
25
50
75
100
Memristance (KΩ)
Sweep sensing 
period from 51ms 
to 5.1ms in 10 steps
Time (ms)
8.7
Conclusion
In this work, we evaluate the impact of different geometry variations on the elec-
trical properties of two different types of memristors, TiO2-based memristors and
spintronic memristors, by conducting analytic modeling analysis and Monte-Carlo
simulations. We investigate the different responses of the static and memristive pa-
rameters of the two memristors under various process variations and analyze their
implication for the electrical properties of the memristors. A simple LER sample
generation algorithm is also proposed to speed up the related Monte-Carlo sim-
ulations. At the end, we propose a memristor-based synapse that can be used in
neuromorphiccomputingarchitecture. Thecorrespondingtrainingoperationsinclud-
ing multi-synapse schemes and self-training have also been explored and discussed.
The proposed synapse design can be generalized to other memristor materials for
more applications.
References
1. Chua L (1971) Memristor-the missing circuit element. IEEE Trans Circuit Theory 18(5):507–
519
2. Strukov DB, Snider GS, Stewart DR,Williams RS (2008)The missing memristor found. Nature
453:80–83
3. Wang X, Chen Y, Xi H, Li H, Dimitrov D (2009) Spintronic memristor through spin-torque
induced magnetization motion. IEEE Electron Device Lett 30(3):294–297
4. Ho Y, Huang GM, Li P (2009) Nonvolatile memristor memory: device characteristics and
design implications. In: international conference on computer-aided design, pp 485–490
5. Strukov D, Borghetti J, Williams S (2009) Coupled ionic and electronic transport model of
thin-ﬁlm semiconductor memristive behavior. SMALL 5(9):1058–1063
6. Pershin YV, Ventra MD (2009) Experimental demonstration of associative memory with
memristive neural networks. In: nanotechnology nature proceedings, p 345201
7. Choi H, Jung H, Lee J,Yoon J, Park J, Seong D-J, Lee W, Hasan M, Jung G-Y, Hwang H (2009)
An electrically modiﬁable synapse array of resistive switching memory. Nanotechnology
20(34):345201

8
Statistical Memristor Model and Its Applications in Neuromorphic Computing
131
8. Asenov A, Kaya S, Brown AR (2003) Intrinsic parameter ﬂuctuations in decananome-
ter MOSFETs introduced by gate line edge roughness. IEEE Trans Electron Devices
50(5):1254–1260
9. Niu D, Chen Y, Xu C, Xie Y (2010) Impact of process variations on emerging memristor. In:
design automation conference (DAC), pp 877–882
10. Roy G, Brown A, Adamu-Lema F, Roy S, Asenov A (2006) Simulation study of individual and
combined sources of intrinsic parameter ﬂuctuations in conventional nano-MOSFETs. IEEE
Trans Electron Devices 53(12):3063–3070
11. Oldiges P, Lin Q, Petrillo K, Sanchez M, Ieong M, Hargrove M (2000) Modeling line edge
roughness effects in sub 100 nanometer gate length devices. In: SISPAD pp 131–134
12. Jiang Z et al. (2009) Characterization of line edge roughness and line width roughness
of nanoscale typical structures. In: international conference on nano/micro engineered and
molecular systems, pp 299–303
13. Asenov A, Cathignol A, Cheng B, McKenna K, Brown A, Shluger A, Chanemougame D,
Rochereau K, Ghibaudo G (2008) Origin of the asymmetry in the magnitude of the statistical
variability of n-and p-channel poly-si gate bulk mosfets. Electron Device Lett IEEE 29(8):913–
915
14. AsenovA, Kaya S, Davies J (2002) Intrinsic threshold voltage ﬂuctuations in decanano mosfets
due to local oxide thickness variations. IEEE Trans Electron Devices 49(1):112–119
15. Wang X, ChenY (2010) Spintronic memristor devices and applications. In: design, automation
& test in europe conference and exhibition (DATE), pp 667–675
16. Kim D, Seo S, Ahn S, Suh D, Lee M, Park B, Yoo I, Baek I, Kim H, Yim E et al. (2006)
Electrical observations of ﬁlamentary conductions for the resistive memory switching in NiO
ﬁlms. Appl phys lett 88(20)202102
17. Kim K, Choi B, Shin Y, Choi S, Hwang C (2007) Anode- interface localized ﬁlamentary
mechanism in resistive switching of TiO thin ﬁlms. Appl phys lett 91:012907
18. Strukov D, Williams R (2009) Exponential ionic drift: fast switching and low volatility of
thin-ﬁlm memristors. Appl Phys A Mater Sci Process 94(3):515–519
19. Parkin S (2009) Racetrack memory: a storage class memory based on current controlled
magnetic domain wall motion. In: device research conference (DRC), pp 3–6
20. Matsunaga S, Katsumata A, Natsui M, Fukami S, Endoh T, Ohno H, Hanyu T (2011) Fully
parallel 6 T-2MTJ nonvolatile TCAM with single-transistor-based self match-line discharge
control. In: IEEE symposium on VLSI circuits, pp 28–29
21. Lou X, Gao Z, Dimitrov D, Tang M (2008) Demonstration of multilevel cell spin transfer
switching in MgO magnetic tunnel junctions. Appl Phys Lett 93:242502
22. Li Z, Zhang S (2004) Domain-wall dynamics driven by adiabatic spin-transfer torques. Phys
Rev B 70(2):024417
23. BazaliyYB et al. (1998) Modiﬁcation of the landau-lifshitz equation in the presence of a spin-
polarized current in colossal- and giant-magnetoresistive materials. Phys Rev B 57(6):R3213–
R3216
24. Zhang S, Li Z (2004) Roles of nonequilibrium conduction electrons on the magnetization
dynamics of ferromagnets. Phys Rev Lett 93(12):127204
25. Liu X, Liu X-J, Ge M-L (2005) Dynamics of domain wall in a biaxial ferromagnet interacting
with a spin-polarized current. Phys Rev B 71(22):224419
26. Tatara G, Kohno H (2004) Theory of current-driven domain wall motion: spin transfer versus
momentum transfer. Phys Rev Lett 92(8):086601
27. Ban Y, Sundareswaran S, Panda R, Pan D (2009) Electrical impact of line-edge roughness on
sub-45 nm node standard cell. SPIE 7275:727518–727518–10


Chapter 9
Adaptive Resonance Theory Design in Mixed
Memristive-Fuzzy Hardware
Max Versace, Robert T. Kozma, and Donald C. Wunsch
Abstract Fuzziﬁcation of neural networks show great promise in improving sys-
tem reliability and computational efﬁciency. In the present work we explore the
possibility of combining fuzzy inference with Adaptive Resonance Theory (ART)
neural networks implemented on massively parallel hardware architectures includ-
ing memristive devices. Memristive hardware holds promise to greatly reduce
power requirements of such neuromorphic applications by increasing synaptic mem-
ory storage capacity and decreasing wiring length between memory storage and
computational modules. Storing and updating synaptic weight values based on
synaptic plasticity rules is one of the most computationally demanding operations in
biologically-inspired neural networks such as Adaptive Resonance Theory (ART).
Our work indicates that Fuzzy Inference Systems (FIS) can signiﬁcantly improve
computational efﬁciency. In this chapter, we introduce a novel method, based on
fuzzy inference, to reduce the computational burden of a class of recurrent networks
named recurrent competitive ﬁelds (RCFs). A novel algorithmic scheme is presented
to more efﬁciently perform the synaptic learning component of ART networks in
memristive hardware. RCF networks using FIS are able to learn synaptic weightswith
small absolute error rates, and classify correctly. Using the FIS methodology it is pos-
sibletosigniﬁcantlyreducethecomputationalcomplexityoftheproposedmemristive
hardware using computationally cheaper and more robust fuzzy operators.
9.1
Introduction
In this chapter, we describe a synthesis of known and proven memristor capabilities to
allow implementation of Adaptive Resonance Theory (ART) [1–3] neural networks.
M. Versace ()
Neuromorphics Lab, Boston University, 677 Beacon st, Boston, MA 02215, USA
e-mail: maxversace@gmail.com
R. T. Kozma
Department of Mathematics, SUNY Stony Brook, Stony Brook, NY 11794–3651, USA
e-mail: rkozma@math.sunysb.edu,
D. C. Wunsch
Applied Computational Intelligence Lab, Missouri University of Science & Technology,
Rolla, MO 65409, USA
e-mail: wunsch@ieee.org
R. Kozma et al. (eds.), Advances in Neuromorphic Memristor Science and Applications,
133
Springer Series in Cognitive and Neural Systems,
DOI 10.1007/978-94-007-4491-2_9, © Springer Science+Business Media Dordrecht 2012

134
M. Versace et al.
Hybrid systems using fuzzy numbers [4, 5] in conjunction with memristors [6, 7]
are a reasonable approach for implementing neural networks. In combination with
key ART elements published in [7–14], in addition to well-known tradeoffs between
expensive but parallelizable operations and fast sequential operations in ART [15],
the elements are in place for ART implementation.
This work grew out of the efforts of the researchers in the DARPA SyNApSE
project at the Boston University Neuromorphics Lab to use dense, memristive-based
devices with low-power Complementary Metal-Oxide Semiconductor (CMOS) to
emulate cellular activation and learning. The approach was to use fuzzy inference
to reduce computational complexity while maintaining a high degree of accuracy in
the resulting precision of the computations. The density and power savings of the
memristive hardware, as well as its suitability for parallelism, make this conﬁguration
well-suited for ART implementation.
ART is a broad learning theory that has resulted in a large class of recurrent
neural network architectures. Biologically-inspired recurrent neural networks are
computationally intensive models that make extensive use of memory and numerical
integration methods to calculate neural dynamics and synaptic changes. The recent
introduction of architectures integrating nanoscale memristor crossbars with con-
ventional CMOS technology has made possible the design of networks that could
leverage the future introduction of massively parallel, dense memristive-based mem-
ories to efﬁciently implement neural computation. Despite the clear advances offered
by memristors, implementing neural dynamics in digital hardware still presents
several challenges. In particular, large-scale multiplications/additions of neural acti-
vationsandsynapticweightsarelargelyinefﬁcientinconventionalhardware, leading,
among other issues, to power inefﬁciencies. Sub-threshold CMOS has attracted much
attention as a primary approach to reducing power (see book introduction), but there
are reasons to believe that the power savings of sub-threshold CMOS with respect to
purely digital approaches will not offset the cost associated with device variability
[16], the fact that the industry is tuned for digital approaches, or the lack of ﬂexibility
in sub-threshold approaches.
In this chapter, we describe a methodology based on fuzzy inference to reduce
the computational complexity of ART-based neuromorphic algorithms by replacing
multiplication and addition with fuzzy operators. Fuzzy systems have been used
advantageously in conjunction with neural networks in many ways, e.g., [4, 5,
17]. We use fuzzy inference systems (FIS) to evaluate the learning equations of
two widely-used variants of Hebbian learning laws, pre- and post-synaptic gated
decay. We test this approach in a recurrent architecture that learns a simple dataset
by using components of an ART network, and we then compare the fuzzy and
canonical implementation. We ﬁnd that the behavior of the network using FIS
with min; max is similar to that of networks employing regular multiplication and
addition, while yielding better computational efﬁciency in terms of the number of
operations used and compute cycles performed. Using min; max operations, we
can implement learning more efﬁciently in memristive hardware, which translates
into power savings. These parts of ART learning were identiﬁed in [15] as being
the most expensive computationally, so the design explored herein removes a major
impediment to efﬁcient ART implementation in hardware.

9
Adaptive Resonance Theory Design in Mixed Memristive-Fuzzy Hardware
135
9.2
Background
This section provides the necessary background concerning memristors, the com-
putational complexity of the recurrent networks used in this study, and the fuzzy
inference systems employed to augment the efﬁciency of computations. We focus on
a class of adaptive recurrent neural networks termed Recurrent Competitive Fields
(RCFs) [18] that constitute the building blocks ofART networks. RCFs are massively
parallel, biologically-inspired plastic networks, a characteristic that makes them both
powerful and, at the same time, memory and computationally intensive, introducing
issues in the efﬁcient hardware implementation of this class of models. Memristive
hardware implementations of learning rules typical of RCFs have been explored in
Snider [8].
Recurrent neural networks are biologically-inspired artiﬁcial neural network mod-
els generally consisting of two main components, cell bodies and synaptic weights.
The ratio of synaptic weights to cells is usually very high; hence, the simulation of
large networks is computationally intensive because it involves a large number of
computing elements and operations on these elements. Current electronics require a
tradeoff between speed, power, area, and hardware accuracy. The recent introduction
of memristive memory makes it possible to densely store synaptic weight values for
recurrent nets in a combined CMOS/memristor hardware architecture, resulting in
memory load reduction. Memristive hardware holds promise to greatly reduce the
power requirements of neuromorphic applications by increasing the synaptic mem-
ory storage capacity and decreasing the wiring length between memory storage and
computational modules. However, implementing neural dynamics and learning laws
in hardware presents several challenges. Synaptic weight update rules and network
dynamics rely heavily on multiplication and addition, the former operation being ex-
pensive in terms of power and area usage in hardware. In this chapter, we explore an
alternative mathematical representation of these computations aimed at improving
power efﬁciency.
Biologically-inspired neural networks generally consist of orders of magnitude
more synapses than cells, with synaptic weights usually accessed to perform scalar
multiplication with pre-synaptic cell activation at runtime, along with learning and
synaptic weight updates. We explore methods based on fuzzy inference systems (FIS)
to increase efﬁciency in implementing gated Hebbian learning on hardware with re-
spect to using the conventional arithmetic operations. An adaptive recurrent network
is described in which synaptic weight updating is performed using Takagi-Sugeno
type fuzzy inference systems [19]. In this section, we provide some background on
fuzzy systems and the recurrent network employed in this study. In Sect. 9.3, we
describe a methodology that uses FIS to redeﬁne two learning equations. In Sect. 9.4,
we simulate the fuzzy and conventional networks and compare their respective be-
havior in terms of accuracy and computational efﬁciency. In Sect. 9.5, we discuss the
results.

136
M. Versace et al.
9.2.1
Memristors and Brains
The memristor, short for memory-resistor, is the fourth fundamental two-terminal
circuit elements, in addition to the resistor, capacitor, and inductor. Memristors were
predicted based on symmetry assumptions by Chua [20] and discovered at HP Labs in
PaloAlto in 2008 [21], when certain materials yielded non-volatile resistance similar
to that theorized by Chua and Kang [22]. Memristors are characterized by hysteresis
loops in their current–voltage behavior, as well as their ability to stably maintain
their nonlinear resistance with extremely low decay rates after power is switched
off for hours, days, or even years. This property makes them useful as nonvolatile
memories. The memristive property only emerges signiﬁcantly at the nanoscale,
explaining their elusive nature until recently [23]. Based on current paradigms in
nanotechnology, memristors can be packed into crossbar architectures [21] and can
be integrated with CMOS, enabling the close placement of memristive crossbars
along CMOS processors [6]. The further miniaturization allowed by memristors
promises to contribute to one aspect of the solution to Moores law slowdown by
allowing close placement of memory and computation, minimizing power dissipa-
tion, and simultaneously overcoming the von Neumann bottleneck related to the
physical separation of the processor and memory. The density of memristors and
their compatibility with existing CMOS technology also suits them to implement
massively parallel neuromorphic architectures [24].
Synapses in brains and in memristive devices behave similarly, prompting the
idea to utilize them in neuromorphic hardware. Certain aspects of brain dynamics
can be viewed as massively parallel dynamical systems in which neurons constantly
read and modify billions of synapses. Storing and updating synaptic values based on
synaptic plasticity rules is one of the most computationally cumbersome operations
in biologically-inspired neural networks. Memristor crossbars make it possible to
efﬁciently approximate biological synapses by packing memristive-based nanoscale
crossbar arrays close to a CMOS layer at densities of approximately 1010 memristors
per cm2 [7, 25, 26] (see Fig. 9.1). Such architecture can be used to efﬁciently simulate
neural networks due to the neural networks ability to tolerate an underlying “crummy
hardware”, such as memristive-based devices characterized by a large number of
defective components [9].
Finally, recent work has highlighted the compatibility of memristors with fuzzy
logic computing schemes [10–14]. Various approaches of implementing fuzzy logic
on memristive circuits have been proposed. Circuits performing the fuzzy min and
max operations have been designed, and have the potential to be assembled into fuzzy
classiﬁers. It has been shown that fuzzy membership functions of any shape and
resolution can be implemented in such memristive circuitry. Furthermore, the power
advantages of such systems have also been quantiﬁed. Fuzzy-memristive circuits
are inherently fault tolerant with high rates of connectivity capable of processing
information in real time. In this chapter we go beyond previous work by describing
an explicit method of how to put the existing pieces together to implement Adaptive
Resonance Theory network efﬁciently on memristive hardware.

9
Adaptive Resonance Theory Design in Mixed Memristive-Fuzzy Hardware
137
Fig. 9.1 a Memristor/CMOS hybrid chip architecture with details of memristor crossbar imple-
menting memristor synapses [25]; b hysteris loop in the current–voltage behavior of the memristor
[26]
9.2.2
Recurrent Competitive Field
Neural networks are built from modules of the form dy/dt = f(y(t))+h(Wx(t)), where
vectors x(t) and y(t) represent the cells of two subsequent layers of the network, W is

138
M. Versace et al.
the synaptic weight matrix of edges connecting the cells, and f is a nonlinear func-
tion (e.g., sigmoidal) applied to each element of its vector argument. Using enough
such modules and sufﬁciently large matrices, one can approximate any continuous
function arbitrarily well. Computation along synapses can be interpreted as scalar
multiplication, where the strength of connection between the cells is reﬂected in the
value of the synaptic weight connecting the cells. One such network is the recurrent
competitive ﬁeld (RCF). RCFs are a class of biologically-inspired neural networks
introduced by Grossberg that learn input/output representations via a modiﬁed Heb-
bian learning law [18]. Variations in RCFs describe networks that include feedback
pathways allowing a cells output to project back to its input either directly or indi-
rectly through on-center, off-surround projections. Typically, an RCF cell receives,
in addition to its bottom-up input, a self-excitatory connection, as well as inhibitory
connections from neighboring cells in the same layer. RCFs can compress and store
activity in short-term memory (STM), a property that depends on the choice of the
feedback input function. Grossberg demonstrated how to construct networks with
stable nonlinear network dynamics with respect to external stimuli [1, 2]. RCFs are
described by a system of coupled differential equations and are a computationally
intensive algorithm in conventional digital hardware implementation due to the com-
plexity of the dynamics at the cells and the high ratio of synapses per cell. In this
study, the network consists of a two-layer RCF, an input layer F1 and an output
or coding layer F2. The layers are connected by bottom-up, or feedforward, plastic
connections modiﬁed by the instar (Hebbian post-synaptic gated decay) learning law,
and by top-down, feedback projections modiﬁed by the outstar (Hebbian presynaptic
gated decay) learning law. Cells in F1 are denoted as xi and the cells in F2 as yi.
The RCFs used for simulation in this paper are governed by the following set of
equations:
dxi
dt = −Axi + (B −xi)
⎡
⎣
dim(y)

j=1
αyjwji + Ii
⎤
⎦
+
,
(9.1)
and
dyi
dt = −Ayi + (B −yi)
⎡
⎣
dim(x)

j=1
xjwij + f (yi) + Supi
⎤
⎦
+
−(C + yj)

k̸=1
yk. (9.2)
Here, Ii denotes the bottom-up input to the cell, the constants A, B, and C determine
the behavior of the network, and α is the scaling factor to inﬂuence top-down feed-
back, which is 0.01 in all simulations. We achieve supervised learning by boosting
the activation of the appropriate coding cell yi with dim (y) = n by using a supervised
learning term:
Sup = (k1, k2, . . . , kn),
(9.3)
where Ki are constants set to 1 for coding cells and 0 for all other cells. The feedback
function f (·) may be chosen to be linear, slower than linear, faster than linear, or a

9
Adaptive Resonance Theory Design in Mixed Memristive-Fuzzy Hardware
139
Fig. 9.2 Schematic network
diagram of a two layer RCF.
The input and coding layers
are denoted F1 and F2
respectively, and vector
entries xi and yi correspond to
values at the nodes of the
layers. The synaptic top down
weights are given as wji, and
bottom up weights by wij. The
quantities ij and Supk
correspond to entries of the
input and supervised learning
vectors
sigmoid. The notation [·]+ denotes max(0, ·). In this paper f (yi) is chosen to be the
sigmoid function:
f (yi) =
y2
i
1 + y2
i
.
(9.4)
Sigmoidal feedback functions combine the functionality of all three cases by con-
trast enhancing small signals, storing intermediate signals with small distortion, and
uniformizing very large signals. In addition, they add a new emergent property, the
quenching threshold, which is the minimum size of initial activity required to avoid
being suppressed to zero. The outstar learning law is used to compute the weights
wji of the top-down synaptic connection between cells xi and yj:
dwji
dt
= −Dxiwji + Exiyj.
(9.5)
The constants D and E represent the learning rate, and determine the stability-
plasticity of the system. Similarly, The instar learning law (Hebbian post-synaptic
gated decay) is used to compute the bottom-up synaptic weights wij between cells xi
and yj:
dwij
dt
= −Dyjwij + Exiyj.
(9.6)
The components of variables x, y, wij, wji are constrained into the range [0, 1] by
convention. Training the network consists of presenting an input pattern for t seconds
and computing ˙xi, ˙yj, ˙wij, and ˙wji. In general we use Euler’s method to calculate all
variables, while in the fuzzy version of the network ˙wij and ˙wji are calculated by a
fuzzy inference system (Fig. 9.2).

140
M. Versace et al.
9.2.3
Computational Complexity of Operations
The computation of the synaptic weight matrcies wij and wji at each time step is
particularly power-intensive since its size is the product of the number of cells in
the two layers dim(x) · dim(y), and as multiplication in digital hardware requires a
number of components roughly proportional to the square of the number of bits in the
operands. In addition weights can be modiﬁed by synaptic plasticity rules governed
by differential equations, the solving of which requires iterative numerical methods
such as Runge-Kutta or Euler. The operations AND, OR as well as min and max
have computational cost proportional to O(n) the number of bits used, while regular
multiplication × uses O(n2) of computational resources per bit. Network size is
deﬁned to be the energy required to perform a given computation to a given degree
of accuracy. For example, given 16 bits of precision the cost E for various operations
is proportional to the number of bits as follows:
E( + ), E(max), E(min) ∝16
E(OR), E(AND) ∝16
E( × ) ∝162.
Basing a network on computationally cheaper operations would provide substantial
energy savings, assuming that:
•
The new methodology produces networks that are not larger (in terms of power
consumption); and
•
The resulting networks have a similar expressiveness in approximating continuous
functions.
A number of candidates to implement such cheaper networks exist, including mor-
phological neural networks [15]. We investigate fuzzy inference systems (FIS) as an
alternative approach that provides similar functionality to the regular (+, × ) algebra,
while potentially being able to save energy by using the less expensive operations +,
max, min, together with fuzzy AND and OR.
9.2.4
Fuzzy Inference Systems
Fuzzy inference is a method by which to create a map for an I/O system using
fuzzy logic, fuzzy membership functions, fuzzy if-then rules and fuzzy operations,
which is particularly valuable for neural networks, as discussed in [4]. Typical fuzzy
inference systems demonstrate advantages compared to classical methods in the
ﬁelds of pattern classiﬁcation, expert systems and intelligent control [27–29]. In this
paper, we use a Takagi-Sugeno type FIS with min and max operations to reduce
the computational load of numerically integrating learning equations. Fuzzy sets are
universal approximators of continuous functions and their derivatives to arbitrary

9
Adaptive Resonance Theory Design in Mixed Memristive-Fuzzy Hardware
141
Fig. 9.3 Fuzzy Number Neural Networks (taken from [4]). These offer the signiﬁcant potential to
reduce computational complexity in neural networks in general, including ART
accuracy [30]; thus, FIS are applicable for solving learning equations governing
recurrent neural networks [31]. Some advantages of FIS include the following:
1. computationally cheap fuzzy operators, e.g. fuzzy AND, OR.
2. robustness with crummy data, hardware, or even missing data.
3. error tolerance; operations min and max don’t amplify errors.
Points 2 and 3 are signiﬁcant, as the memristive hardware is a nanoscale device
with high manufacturing defect rates. Fuzzy inference systems can be challenging
to design, as they require ad hoc assumptions on membership functions and rules.
In addition the defuzziﬁcation step can be computationally intensive (Fig. 9.3).

142
M. Versace et al.
9.3
FIS Method
We propose a design for a fuzzy inference system to compute the pre- and post-
synaptic gated decay learning equations resulting in a potentially more efﬁcient
hardware implementation. Iterative numerical methods used to evaluate the differ-
ential equations governing learning add to the computational complexity. In our
approach, we replace regular multiplication and addition operators with fuzzy oper-
ators, and numerical integration with fuzzy controllers to minimize the computational
costsoflearning. WedesignaFIStoapproximatethetheoreticalbehaviorofthelearn-
inglawstohighaccuracy. Weconsidervariousfuzzymembershipfunctionsandfuzzy
rules, in addition to different time steps built into the FIS. Instar and outstar learning
laws governing RCFs are framed in terms of ODEs (5, 6). Fuzzy inference systems
are universal approximators of smooth functions and their derivatives, hence it is
possible to design a FIS that given inputs xi, yi and wij yields output which approxi-
mates ˙wij arbitrarily well. To fuzzify synaptic learning, the corresponding ODEs are
solved symbolically, and a FIS is designed to approximate the solution surface of the
learning equation at time dt, which serves as the built time step of the FIS approxima-
tor. This process is called fuzziﬁcation of a learning equation, and the resulting FIS
can replace numerical methods used to solve the learning equation. In this section,
we describe a method to fuzzify the instar and outstar equations governing the RCF.
9.3.1
Computation at Cells
In typical RCFs there is an order of magnitude difference in the number of cells and
synaptic weights as #Synptic weights = 2·dim(x)·dim(y). The equations governing
the cells in layers F1 and F2 can have complex dynamics and a large number of
terms. Nevertheless, due to the high ratio of synapses per cells, the computation of
cell dynamics is less critical from a power budget perspective with respect to weight
updating. In this paper we therefore focus on the second operation.
9.3.2
Outstar Learning
In an outstar network, the weights projecting from the active pre-synaptic cell learn
the pattern at the post-synaptic sites:
˙wji = −Dxiwji + Exiyj.
(9.7)
This differential equation is fuzziﬁed and solved by a three input, one output fuzzy
controller. Each equation governing outstar learning is discretizable, meaning that
weight ˙wji only depends on components xi and yj of F1 and F2, so the length of
the input and output vectors x and y do not increase the internal complexity of the
FIS. As a result, the number of fuzzy rules and membership functions required for

9
Adaptive Resonance Theory Design in Mixed Memristive-Fuzzy Hardware
143
accurate learning do not proliferate due to an increase in the number of cells in layers
F1 and F2. If we set the same time step dt in the FIS to the time step dt=0.05 s used
for canonical Euler numerical integration for RCFs, the number of times the fuzzy
controller is initiated is the same as the number of iterations needed for solving the
learning equation. If it is sufﬁcient to have the synaptic dynamics at larger time steps,
it is possible to use a larger time step dt for fuzzy learning to converge to the desired
weights in fewer iterations than needed for numerical methods. The three input one
output fuzzy inference system ˙wji = fuzzy (xi,yj,wji) evaluates the outstar learning
law solution at time dt. All inputs and outputs of the FIS are normalized to the range
[0, 1]. The constants E and D determining the learning rate are hard-coded into the
FIS to reduce the internal complexity of the FIS by decreasing the number of fuzzy
rules and membership function evaluations required.
9.3.3
Instar Learning
The instar learning equation:
˙wij = −Dyjwij+Exiyj
(9.8)
is symmetrical to the outstar learning equation with respect to the two variables xi and
yi, so modifying fuzzy inference system to handle instar simply requires switching
the two inputs. The same three input, one output FIS is used to implement instar
and outstar learning. The two learning rate parameters D and E are hard coded.
Computation occurs by changing the order of the input vectors x and y vector inputs,
in particular ˙wji = fuzzy (yj,xi,wij) computes the instar learning law.
9.3.4
FIS Parameters
The FIS internal parameters are chosen to maximize computational performance.
Internal parameters of FIS include the type and number of input fuzzy membership
functions for each input variable, the fuzzy AND/OR rules combining the mem-
bership functions, and the defuzziﬁcation scheme. For performance, the number
of rules and membership functions are minimized. FIS approximation improves
when additional fuzzy membership functions and rules are included, but this in-
creases the systems computational cost. Takagi-Sugeno type FIS are used as opposed
to the Mamdani type for increased computational efﬁciency in the hardware. For
Takagi-Sugeno type inference, the output membership functions are constant, and
defuzziﬁcation is accomplished through average defuzziﬁcation or weighted average
defuzziﬁcation. The time at which the learning law ODE solution is approximated
based on initial conditions is another parameter of the FIS. We denote this param-
eter dt as is it analogous to the time step in numerical methods and determines the

144
M. Versace et al.
Table 9.1 On the left is the relative complexity of computations in ART 1, taken from [15], as an
analysis of the optical implementation of ART. To more efﬁciently implement the majority of these
is the goal of any dedicated ART hardware. Note that these operations are also present in other
ART architectures. On the right is the corresponding capability of implementations on memristors.
This offers the possibility of performing 95 % of the computationally expensive operations with
memristors
#
Operation
Electronics
Optics
Memristor
%
1
I
X
–
2
T k
X
X
–
3
T k·I
X
X
80
4
|I|
X
X
5
5
|Tkm|
X
X
5
6
Tkm −Tkm
X
X
5
7
Tnc −I
X
1
8
max
X
3
9
{A01−1; i=1, nc}
X
<1
10
Pi →Cnc
X
<1
11
≥
X
<1
12
*
X
<1
13
/
X
<1
14
+
X
<1
learning speed of the system by controlling the number of computing cycles (t=dt)
required by the fuzzy inference system to converge to the solution at time t. Com-
mon numerical methods for solving differential equations approximate the solution
linearly in each small time step. The nonlinear FIS can achieve faster convergence
by being able to compute with an arbitrary dt and fewer compute cycles as compared
to numerical methods. To obtain correct network dynamics, the time step dt must be
accounted for when coupled with the unfuzziﬁed equations of cell activity solved by
regular numerical methods. FIS parameters used are summarized in Table 9.2.
9.3.5
Computations in ART
In [15], the input-template inner product and the input magnitude computation, both
necessary for all major ART models, e.g., [3, 32–35], are shown to account for 85 %
of the computational complexity (SeeTable 9.1, above, taken from [15]). The compu-
tations we have described herein constitute those operations via memristive methods.
Therefore, in combination with the techniques described in [15], these observations
constitute a description of memristive ART implementation.
9.4
Software Implementation
In this section, we simulate and compare two recurrent networks completing two
tasks. The ﬁrst network uses arithmetic and Eulers method to compute the change
of synaptic weights with instar and outstar learning laws. The second network per-
forms supervised learning through an FIS. In the ﬁrst task, the RCF learns a dataset

9
Adaptive Resonance Theory Design in Mixed Memristive-Fuzzy Hardware
145
Table 9.2 Parameters of some Takagi-Sugeno FIS for performing instar and outstar learning.
Accuracy is deﬁned as the mean square error of the FIS compared to the theoretical values un-
der one iteration of the FIS. If the time step dt is small, then piecewise linear tent membership
functions approximate the solution well. Trapezoidal membership functions capture the dynamics
with a lesser degree of accuracy. In contrast, nonlinear Gaussian membership functions have a
stable error rate throughout all time scales
Fuzzy inference systems
Time step (s)
Membership Fn
No. rules
Accuracy (%)
0.05
Tent
8
99.9986
0.05
Trapezoidal
8
91.9158
0.05
Gaussian
8
96.8222
0.1
Tent
8
99.9908
0.1
Trapezoidal
8
92.2397
0.1
Gaussian
8
96.9487
0.25
Tent
8
99.9282
0.25
Trapezoidal
8
92.5434
0.25
Gaussian
8
97.0636
0.5
Tent
8
99.7203
0.5
Trapezoidal
8
93.1313
0.5
Gaussian
8
97.2589
1
Tent
8
99.0905
1
Trapezoidal
8
93.2469
1
Gaussian
8
97.1763
consisting of images of alphabetical characters. In the second task, we apply the FIS
learning method to a larger network and compare the accuracy and the computational
complexity of the fuzzy and canonical RCFs.
9.4.1
Summary of FIS Systems
Three-input, one-output Takagi-Sugeno type fuzzy inference systems are built with
learning rates hard-coded (Table 9.2). The accuracy of the network is assessed in
this case by comparing the rate of convergence of weights in the fuzzy and canonical
RCFs to theoretical values, in particular by calculating the difference of synaptic
weights at the end of the simulation. The average differences in accuracy in one
cycle of the simulation are provided in the last column of Table 9.2. For optimal
FIS performance, the following design choices were made: (1) internal time step
dt, input fuzzy membership function types, fuzzy rules, and output membership
functions were varied; (2) piecewise linear or constant output membership functions
were selected to boost computational efﬁciency; (3) average defuzziﬁcation was
chosen over weighted average defuzziﬁcation of output membership functions. FIS
with two input membership functions for each input and constant output membership
functions for each fuzzy rule can sufﬁciently approximate the synaptic dynamics for
correct classiﬁcation.

146
M. Versace et al.
The fuzzy systems described in Table 9.2 use constant output membership func-
tions. The number of rules equals the product of the number of inputs and the number
of input membership functions. The FIS are three input systems with two membership
functions per input.
9.4.2
Character Recognition Network
In this section, we test the behavior of two networks in a simple supervised learning
paradigm. We construct two versions of the RCF; in the ﬁrst, the synaptic weights
are calculated using the Euler method, while in the second, an FIS described in the
previous section is used to update the synaptic weights.
The network consists of 51 cells and 1,820 synaptic weights. The input dataset
consists of 26 binary 7-by-5 pixel input images of the alphabet characters. Accord-
ingly, we use 35 cells x in F1 corresponding to the 35 pixels of the input image. The
layer F2 consists of 26 cells y corresponding to the 26 letters of the alphabet. Each
yi neuron learns to code for one letter of the alphabet. The initial values of instar
synaptic weights wij and outstar synaptic weights wji are initialized randomly in the
range [0, 0.1]. We initialize xi randomly on range [0, 0.02], and yj are set to zero.
In order to achieve supervised learning, the activation of the cells yi is boosted by
the supervised learning term, deﬁned as vector Supj = {(a1, a2, . . . , a26)|aj = 1 and
ai = 0 if i ̸= j}. The bottom-up input to the cell is a binary value 0 or 1 corresponding
to a black or white pixel, respectively. In the canonical RCF network, Eulers method
is used to solve the instar and outstar learning equations with time step dt = 0.05 s.
The RCF network employing FIS has the same network structure, and the FIS de-
scribed in Table 9.2 are selected to update the synaptic weights. Input letters are
presented for 200 computing cycles with the supervised input to the coding cells,
interleaved by 100 computing cycles in which no input is provided, in order to reset
the activity of the network between letter presentations. The following three tests are
performed to assess network behavior:
•
Presenting the i-th letter in the alphabet as input to F1, verify that cell yi of F2
trained to code it becomes active and other cells do not, i.e. activation of coding
cells are near 1 while activation of other cells are near 0.
• Activating cell yi of layer F2 and observing the activation pattern of the pixels of
the i-th letter in cells xj in layer F1 due to the feedback-mediated activation.
•
Compare the convergence of the synaptic weights learned by the FIS and canonical
RCFs through calculating the absolute error
wcanonical
ij
−wfuzzy
ij
 .
The results of tests a–c are shown in Fig. 9.4. This shows the performance for the
FIS using tent input membership functions, and dt = 0.05 Fig. 9.4a shows the
performance of the fuzzy RCF network in a character recognition problem. The left
image contains the input image, the center image shows the activation of the F1 cells
after the input was presented for 100 computing cycles, and the right image shows
the reconstructed input in F1 after activating F2. Figure 9.4b shows the difference

9
Adaptive Resonance Theory Design in Mixed Memristive-Fuzzy Hardware
147
Fig. 9.4 a Left: the pixelated input letter; Center: the activation of yi given letter input R after
learning; Right: the reconstruction of the letter R after the activation of y18; b The instar weight
matrix maximum error for each letter; c The outstar weight matrix maximum error for each letter

148
M. Versace et al.
between the outstar weights of the FIS and regular RCF networks after training for
t = 10, 000 cycles. Figure 9.4c shows the difference between the instar weights.
The difference between weights in the networks are calculated with the formula
|wcanonical
ij
−wfuzzy
ij
| in both cases.
Every FIS listed in Table 9.2 produces networks that classify correctly. Classiﬁ-
cation accuracy is important as we are less interested in the explicit synaptic weight
values and more in the overall dynamical behavior of the networks. In summary, RCF
networks using FIS are able to learn synaptic weights with small absolute error rates
and classify characters correctly. The error in synaptic weights is larger for the fuzzy
and instar methods than for the outstar methods; nevertheless, the classiﬁcation of
characters is correct.
9.4.3
Larger-Scale Network Simulation
The purpose of this simulation is to test the speed-up of learning achieved by the
reduction of compute cycles by using different time steps dt in the FIS when apply-
ing the FIS methodology to a larger network. A simpliﬁed two-layer RCF is used
with one cell in F2 and 5,122 cells in F1. Synaptic weights are trained to learn
a 512-by-512 grayscale photograph, in which 0 and 1 represent a black and white
pixel, respectively, and intermediate values represent appropriate grayscale values.
This network contains 219 synaptic weights and 218 + 1 cells. The Lena image is
used for this simulation. The purpose of this second simulation task is to demonstrate
the applicability of the FIS methodology to larger-scale networks requiring signiﬁ-
cantly more computational resources. The network preserves the RCF dynamics in
F1 while setting y1 = 1 in F2 permanently. We show that using FIS with larger built-
in dt time steps decreases the number of compute cycles required for learning. The
synaptic weight computations are performed by FIS, while Euler is used to compute
cell dynamics. If the built-in time step dt at which the FIS operates is set larger than
the one used in calculating the cell dynamics, then the two must be reconciled. A
multi-timescale integration scheme is introduced to synchronize the learning times
at the cells and synapses. Two nested cycles are used, with the outer cycle executing
learning using the FIS with greater dt, and the inner cycle performing the activation
at the cells with the smaller time step used by the numerical method at a rate that
synchronizes the time duration of learning at the cells and weights. Similar con-
siderations were made for other memristive architectures when using spike-based
approximations of network dynamics in conjunction with Runge-Kutta [8]. Using
the FIS method, it is possible to reduce the number of compute cycles required to per-
form learning in RCFs. This is due to the fact that the numerical method implements
a linear approximation of the solution at each step, whereas FIS uses a nonlinear (or
piecewise linear) global approximation of the solution surface (Figs. 9.5 and 9.6)

9
Adaptive Resonance Theory Design in Mixed Memristive-Fuzzy Hardware
149
Fig. 9.5 Simulating the second task using the Lena photograph as input. The weights produced by
the FIS and canonical learning at the end of the simulation at t = 2000 cycles are almost identical
9.4.4
Results on Computational Complexity
The computational burdens of fuzzy inference and regular synaptic weight updating
are compared by determining the number and type of operations needed and the
computational complexity required to implement them. All fuzzy inference systems
described in Table 9.2 perform an identical number of fuzzy operations. Table 9.3
summarizes the number and computational complexity of operators using less than
one iteration in canonical and FIS synaptic weight updating. The computational com-
plexity is compared for numerical representations with 4, 8, 16 and 32-bit precision.
For the comparison, the time step dt is assumed to be the same in the FIS and forward

150
M. Versace et al.
Fig. 9.6 Comparison of regular learning and fuzzy learning weight convergence. Top: Canonical
solution of outstar learning with dt = 0.05 s. The piecewise linear curve represents the input
pattern present at the given cycle. The nonlinear curve indicates the rate of convergence to the given
pattern. Bottom: The piecewise linear curve represents the input pattern. Three nonlinear fuzzy
learning curves; slowest convergence rate (dark grey): dt = 0.05 s; medium convergence rate (light
grey): dt = 0.25 s; fastest convergence rate (medium grey): dt = 1 s. All have the same learning
rate parameter
Table 9.3 Number and type
of operations required for
synaptic weight updating
Number of operations
×
+
max
AND
defuzz
Complexity
O(n2)
O(n)
O(n)
O(n)
O(n2)
Canonical
5
2
Fuzzy
16
8
1
Euler integrator (see Fig. 9.7), and an identical number of computing cycles are used.
The numerical representation in the hardware attempts 16-bit precision.
When using the FIS method with the same time step, computational efﬁciency
improves by approximately 51,22 and 65,43 % with 16-bit precision and 32-bit
precision, respectively. This improvement rate drops to only 23,81 and 27,27 % for
8-bit and 4-bit precision, respectively. The number of compute cycles required can
be reduced by increasing the time step dt to further reduce computational burden.

9
Adaptive Resonance Theory Design in Mixed Memristive-Fuzzy Hardware
151
2
108
4
108
6
108
8
108
1
109
109
1010
1011
1012
1013
Number of synapses
Number of operations
Fig.9.7 Computationalcomplexityofsynapticlearningasafunctionofnumberofsynapticweights.
Both regular and fuzzy versions use time step dt=0.05 for integration. Thick graphs represent FIS
performance, while thin graphs show forward Euler performance. The dashing corresponds to
numerical precision; non-dashed 32-bit; long-short dashed 16-bit; shorter dashes 8-bit; dotted
indicates 4 bits of accuracy
9.5
Conclusion
In this paper, it is shown that fuzzy inference systems can be useful in providing
an alternative hardware implementation for the widely-used biologically-inspired,
plastic neural networks of Adaptive Resonance Theory (ART). These networks im-
plement a broad range of supervised and unsupervised learning paradigms and can
be implemented efﬁciently in the CMOS/memristor hybrid hardware architecture.
Using FIS methodology, it is possible to signiﬁcantly reduce the computational com-
plexity of the proposed memristive hardware starting at 16-bit numerical precision
by replacing the regular addition and multiplication operations with fuzzy operators.
Through a comparison with previous ART hardware implementation research, we
ﬁnd that this approach can result in up to 95 % of the computational complexity of
ART being implemented in high-density, highly-parallel memristor designs. Future
work will include conducting further design work pertaining to speciﬁc ART archi-
tectures and implementing the hardware for these devices, generalizing this method
to other learning laws of the Hebbian family, and performing comparisons of noise
tolerance to simulate crummy nanoscale hardware for more complex tasks.
Acknowledgments The authors gratefully acknowledge helpful conversations with Anatoli
Gorchetchnikov, as well as ﬁnancial support from the DARPA Synapse Program, the National
Science Foundation, the Missouri S&T Intelligent Systems Center, and the Mary K. Finley Mis-
souri endowment. Max Versace (versace@cns.cu.edu) is the Director of the Boston University
Neuromorphics Lab and was supported in part by the Center of Excellence for Learning in Edu-
cation, Science and Technology (CELEST), a National Science Foundation Science of Learning
Center (NSF SBE-0354378 and NSF OMA-0835976). This work was also partially funded by the
DARPA SyNAPSE program, contract HR0011-09-3-0001.

152
M. Versace et al.
References
1. Grossberg S (1976) Adaptive pattern classiﬁcation and universal recording: I parallel
development and coding of neural feature detectors. Biol Cybernet 23:121–134
2. Grossberg S (1976) On the development of feature detectors in the visual cortex with
applications to learning and reaction-diffusion systems. Biol Cybernet 21:145–159
3. Carpenter G, Grossberg S (1987) A massively parallel architecture for a self-organizing neural
pattern recognition machine. Comput Vision Graph Image Process 37:54–115
4. Dunyak J, Wunsch II DC (1999) Fuzzy number neural networks. Fuzzy Sets Syst 108(1):49–58
5. Dunyak J, Wunsch II DC (2000) Fuzzy regression by fuzzy number neural networks. Fuzzy
Sets Syst 112(3):371–380
6. Xia Q et al (2009) Memristor—CMOS hybrid integrated circuits for reconﬁgurable logic. Nano
Lett 9(10):3640–3645
7. Snider GS, Amerson R, Carter D, Abdalla H, Qureshi S, Leville J, Versace M, Ames H, Patrick
S, Chandler B, Gorchetchnikov A, Mingolla E (2011) Adaptive computation with memristive
memory. IEEE Comput 44(2):2944–2951
8. Snider GS (2011) Instar and outstar learning with memristive nanodevices. Nanotechnology
22:015201
9. Snider GS (2007) Self-organized computation with unreliable, memristive nanodevices.
Nanotechnology 18(36):365202
10. Merrikh-Bayat F, Shouraki SB (2011) Efﬁcient neuro-fuzzy system and its memristor crossbar-
based hardware implementation. CoRR, abs/1103.1156, 2011
11. Merrikh-Bayat F, Shouraki SB (2011) Memristor crossbar-based hardware implementation of
fuzzy membership functions. CoRR, abs/1009.0896, 2011
12. Klimo M, Such O (2011) Memristors can implement fuzzy logic. CoRR, abs/1110.2074, 2011
13. Merrikh-Bayat F, Shouraki SB, Rohani A (2011) Memristor crossbar-based hardware im-
plementation of the IDS method. IEEE Trans Fuzzy Syst 19(6):1083–1096 (art. no.
5893932)
14. Zhong QS,YuY-B,Yu J-B (2010) Fuzzy modeling and impulsive control of a memristor-based
chaotic system. Chin Phys Lett 27(2) (art. no. 020501)
15. Wunsch II DC, Caudell TP, Capps D, Marks II RJ, Falk RA (1993, July) An optoelec-
tronic implementation of the adaptive resonance neural network. IEEE Trans Neural Networks
4(4):673–684
16. Kogge P (2011) The tops in FLOPS. Spectrum, IEEE, 2011
17. Dunyak J, Wunsch D, Saad IW (1999, June) A theory of independent fuzzy probabilities for
system reliability. IEEE Trans Fuzzy Syst 7(3):286–294
18. Grossberg S (1982) Contour enhancement, short term memory, and constancies in reverberating
neural networks. Studies of mind and brain (Chapter 8). Kluwer/Reidel Press, Boston
19. Sugeno M (1985) Industrial applications of fuzzy control. Elsevier, Oxford
20. Chua LO (1971) Memristor—the missing circuit element. IEEETrans CircuitTheor 18(5):507–
519
21. Strukov DB, Snider GS, Stewart DR,Williams RS (2008)The missing memristor found. Nature
453:80–83
22. Chua LO, Kang SM (1976) Memristive devices and systems. Proc IEEE 64(2):209–223
23. Chua LO (2003) Nonlinear circuit foundations for nanodevices, part I: the four-element torus.
Proc IEEE 91(11):1830–1859
24. Versace M, Chandler B (2011) MoNETA: a mind made from memristors. IEEE Spectrum,
December 2011
25. Jo SH, Chang T, Ebong I, Bhadviya BB, Mazumder P, Lu W (2010) Nanoscale memristor
device as synapse in neuromorphic systems. Nano Lett 10:1297–1301
26. Pazienza G, Kozma R (2011) Memristor as an archetype of dynamic data-driven systems and
applications to sensor networks. Dynamic data driven application systems, DDDAS 2011,
Tsukuba, Japan, June 2–3 2011

9
Adaptive Resonance Theory Design in Mixed Memristive-Fuzzy Hardware
153
27. Bezdek JC, Keller J, Krisnapuram R, Pal N (2005) Fuzzy models and algorithms for pattern
recognition and image processing. Springer, New York
28. Kosko B (1999) The fuzzy future: from society and science to heaven in a chip. Harmony
Books, New York
29. Castillo O, Melin P, Ross OM, Cruz RS, Pedrycz W, Kacprzyk J (2007) Theoretical advances
and applications of fuzzy logic and soft computing. Springer, Heidelberg
30. Kreinovich V, Nguyen H-T, Yam Y (2000) Fuzzy systems are universal approximators for a
smooth function and its derivatives. Int J Intelligent Syst 15(6):565–574
31. Kosko B (1994) Fuzzy systems are universal approximators. IEEE Trans Comput 44(11):1329–
1333
32. Carpenter G, Grossberg S, Markuzon N, Reynolds J, Rosen D (1992) Fuzzy ARTMAP: a
neural network architecture for incremental supervised learning of analog multidimensional
maps. IEEE Trans Neural Networks 3:698–713
33. Anagnostopoulos G, Georgiopoulos M (2001) Ellipsoid ART and ARTMAP for incremental
unsupervised and supervised learning. In: Proceedings of the international joint conference on
neural networks, vol 2, pp 1221–1226, 2001
34. Carpenter G (2003) Default ARTMAP. In: Proceedings of the international conference on
neural networks, pp 1396–1401, 2003
35. Xu R, Wunsch D (2011) BARTMAP: a viable structure for biclustering. Neural Networks
24(7):709–716


Chapter 10
Phase Change Memory and Chalcogenide
Materials for Neuromorphic Applications:
Emphasis on Synaptic Plasticity
Manan Suri and Barbara DeSalvo
Abstract In this chapter we review some basic concepts related to Phase Change
Memory (PCM) technology and physics. We discuss recent research encompassing
the study of PCM devices and chalcogenide materials for neuromorphic applications.
We demonstrate the use of PCM devices for emulating speciﬁc functions of a bio-
logical synapse, such as synaptic potentiation, synaptic depression and spike-time
dependent plasticity (STDP). Throughout the discussion, we emphasize on factors
such as materials and programming schemes, important for realizing PCM based
large-scale neuromorphic systems.
10.1
Introduction
Research in the ﬁeld of biologically inspired neuromorphic circuits, in order to
achieve low power, highly parallel, and fault-tolerant systems, has gained a lot of in-
terest over the last few years [1]. Neuromorphic hardware should contain components
which can emulate functions performed by the neurons and the synapses present in-
side the brain. In order to build functional neuromorphic circuits comparable to the
cerebral cortex, an enormously large number of neurons (1010) and synapses (1014
to 1015) are required [2]. The idea of achieving such a high synaptic density using
pure CMOS synapse circuits is not practical in terms of on-chip silicon area con-
sumption, due to the large number of transistors (about 13 transistors/per synapse)
[3]. To overcome this issue, hybrid neuromorphic architectures containing CMOS
neuron circuits integrated with nanoscale device synapses [4] have been proposed in
recent years. Some of these proposed nanoscale synapses include organic transistors
[5], single-electron transistors [6], carbon-nanotube based structures [7] and several
types of resistive memory devices [8, 9]. Electrical models [10, 11] suggest that for
a device to emulate synaptic behavior, it should have a conductance which can be
precisely modulated by the type of stimulus it receives. Since the storage of synaptic
weights is believed to play an important role in the functionality of neural networks
M. Suri () · B. DeSalvo
CEA-LETI, 17 Rue des Martyrs, 38054 Grenoble, France,
e-mail: manan.suri@cea.fr
B. DeSalvo
e-mail: barbara.desalvo@cea.fr
R. Kozma et al. (eds.), Advances in Neuromorphic Memristor Science and Applications,
155
Springer Series in Cognitive and Neural Systems,
DOI 10.1007/978-94-007-4491-2_10, © Springer Science+Business Media Dordrecht 2012

156
M. Suri and B. DeSalvo
Fig. 10.1 Illustration demonstrating the concept of using a PCM device as a controllable non-
volatile resistive synapse connecting a post- and pre- synaptic neuron. Inset shows the TEM image
of a GST based PCM device we fabricated for testing synaptic plasticity functions [64]. (Reprinted
with permission from Ref. [64]. © 2011 IEEE)
[12], the devices emulating synaptic behavior should possess some kind of memory
to retain their conductance states or the synaptic weights. The criteria of conductance
modulation and memory effect can be fulﬁlled by the class of devices known as mem-
ristors or resistive memories in general [13]. Resistive memory devices may include
technologies such as phase change memory (PCM), conductive-bridge (CBRAM)
or programmable-metallization cell (PMC), and oxide-resistive (OXRAM) memory
[14]. We focus on the use of PCM devices for synapses due to advantages such as
high scalability, CMOS compatibility, low programming-current, strong endurance
and better technological maturity compared to other resistive memory technologies.
In a hybrid CMOS/memristive system, a PCM device can be sandwiched between
two spiking CMOS circuits, emulating the synapse connecting a pre- and a post-
synaptic neuron (Fig. 10.1). In Sect. 2, we present some basic principles of PCM
devicesandthecurrentstateofthetechnology. Wediscusstheimportanceofmaterial-
engineering, drawing parallels between conventional memory applications such as
multi-bit (multi-level) storage and the recently proposed neuromorphic applications.
In Sects. 10.3 and 10.4, we show how PCM or chalcogenides can be used for imple-
menting synaptic plasticity. We stress on the limitations and the importance of using
simpliﬁed programming schemes.
10.2
Historical Perspective
In order to adapt and optimize an existing technology for a new application, it is
important to understand the motivation behind its evolution. Thus we dedicate this
brief section on the history of the PCM to give the reader a sense about the more

10
Phase Change Memory and Chalcogenide Materials for Neuromorphic Applications . . . 157
conventional applications of the technology. The concept of using phase-change ma-
terials, speciﬁcally a class of compounds known as chalcogenides, for information
storage is not new. Long standing research in the ﬁeld of amorphous chalcogenide
materials has led to the development of present phase change based optical and
electronic storage media. Chalcogenides form the key storage element of CD-RW
(rewritable compact disks), DVD (digital versatile disk), high deﬁnition Blue-ray
disks, and electronic PCM devices. Discovery of interesting phase-change elec-
trical phenomena goes back to the early 1900s in the work of A. T. Waterman.
Waterman reported large negative-coefﬁcient of resistivity, a typical characteristic
of semiconducting chalcogenide materials in MoS2 [15]. In 1962, A. D. Pearson
of Bell Labs reported two stable regions of high and low electrical conductivity
in As-Te-I glass [16]. Transitions between the low and high conductivity regions
were reversible with the application of appropriate electrical pulses. One of the most
signiﬁcant contributions to the ﬁeld came from the work of Stanford Ovshinsky in
1968 [17]. Ovshinsky also known as the father of PCM developed both electrically
controlled threshold and memory switching devices. Laser induced optical memory
phenomenon in chalcogenide materials, which would later form the basis of modern
CD and DVD disks, was observed by Feinleib et al. in 1972 [18]. A 256-bit array
consisting of a 16 × 16 matrix of PCM cells was demonstrated by R. G. Neale, D.
L. Nelson, and Gordon E. Moore in 1970 [19]. Their memory cell consisted of a
storage element and a p-n junction diode. In 1978, R. R Shanks et al. demonstrated
1024-bits PCM [20]. There was no signiﬁcant commercial interest in electrical PCM
devices till the late 1990s due to extremely high power consumption. The amount of
energy required to switch the PCM is directly proportional to the volume of active
phase change material. The main bottleneck for reducing the power consumption
was broken with the advancements in lithography technology in late 1990s. When
the minimum feature size shrunk from 10 μm to 180 nm the power required to pro-
gram PCM devices became more reasonable. At the same time a steady progress was
being made in the ﬁeld of optical phase change storage leading to the introduction
of phase change based CD-R/W optical disk drive by Panasonic in 1990. In 1999,
Tyler and Parkinson formed a joint venture called Ovonyx with Energy Conversion
Devices to commercialize phase change memory as the Ovonic Uniﬁed Memory.
PCM devices are radiation hard and suitable for space applications. BAE introduced
the ﬁrst commercially available, radiation-hardened PCM device in 2006. It was a
3.3 V, 512-kbit by 8 for a 4-Mbit memory device fabricated in 0.25 μm bulk CMOS
process. Over the last few years many companies such as STMicroelectronics, NXP,
Hitachi, Numonyx (now Micron) [21], IBM [22], and Samsung Electronics [23],
have pushed hard for the development of full scale commercial PCM products such
as stand-alone non-volatile memories or storage class memories. Recent propos-
als for use of PCM devices in neuromorphic systems, brings a new and interesting
application thrust for the technology. Characteristics such as long data retention,
low power consumption, scalability, and high endurance are desirable for all mem-
ory applications. On the other hand, the impact of write/erase-speed, crystallization
kinetics, and analysis of the stochastic or deterministic processes involved in the
operation of PCM devices need to be investigated more carefully for neuromorphic

158
M. Suri and B. DeSalvo
Fig. 10.2 Resistivity of GST
and NGST thin ﬁlms is shown
while heating the ﬁlm at a rate
of 1 ◦C/s, followed by cooling
back to room temperature.
Undoped GST shows a sharp
drop in resistivity around
150 ◦C corresponding to the
transition from the amorphous
to the rocksalt phase, and a
second smaller drop around
350 ◦C caused by the
rocksalt-hexagonal transition.
(Reprinted with permission
from [71]. 2009 American
Institute of Physics)
applications. Ongoing research in areas of multi-bit PCM programming and con-
trol of crystallization kinetics of chalcogenides, provides a useful starting point for
customizing the PCM technology for synaptic applications for their potential use in
future large scale neuromorphic systems.
10.2.1
Phase Change Materials
Chalcogenides are compounds or alloys which contain at least one element from
groupVI of the periodic table usually combined with group IV and groupV elements.
The most standard or commonly used material in electrical PCM devices is an alloy
of germanium, antimony and tellurium having the composition Ge2 Sb2 Te5 (also
known as GST). The electrical resistivity of amorphous GST is high (deﬁned as
reset-state), while that of crystalline is low (deﬁned as set-state) (Fig. 10.2). The
huge difference in resistivity (about ﬁve orders of magnitude) is exploited to store
information, or program the device.
In the following sections of this chapter, we discuss about the possibility of ob-
taining states with intermediate resistance values (partial-set or partial-reset), to
emulate synaptic behavior. For intermediate resistance states, the phase change ma-
terial should be programmed in a way that it contains both partially amorphous and
partially crystalline regions [51]. Similar to resistance based electrical storage, op-
tical storage in phase change materials can be realized by exploiting the difference
in the reﬂectivity of the amorphous and the crystalline phase. Amorphous GST has
a low reﬂectivity, while it is contrary in the case of crystalline GST. Although many
materials exist in both amorphous and crystalline phase with signiﬁcantly different
optical and electrical properties, all of them cannot be used for phase-change based
storage. It is essential that the chosen material can be switched reversibly between
its different phases multiple times in an energy efﬁcient manner.

10
Phase Change Memory and Chalcogenide Materials for Neuromorphic Applications . . . 159
Fig. 10.3 Probability for nucleation and crystal growth as function of temperature and amorphous
marks in a crystalline layer. (Left) Crystallization occurs by process of nucleation followed by
growth. Crystal nuclei appear inside the volume of the amorphous mark. (Right) Crystallization
proceeds by the process of crystal growth from the amorphous crystalline interface moving towards
the center of the amorphous mark. (Reprinted from [68] with kind permission of Springer Science
and Business Media)
The process of crystallization of amorphous regions inside a thin chalcogenide
layer is governed by complex physics of nucleation, crystal growth, and interface re-
lated effects. Based on the nucleation and growth probabilities chalcogenides can be
classiﬁed as nucleation-dominated or growth-dominated materials. GST is known
to be a nucleation dominated material [24]. For growth dominated materials the
probability of nucleation is much lower than for crystal growth within the crystal-
lization temperature range (Fig. 10.3). For a given type of material the growth and
nucleation rates are also dependent upon the size of the amorphous region or its
volume-to-interface ratio with the surrounding crystalline region [25]. A good un-
derstanding and control of the crystallization kinetics is crucial as it determines the
erasure speed in PCM and directly affects the quality of data retention in devices [26].
Studies indicate that growth dominated materials have more deterministic reset-to-set
(amorphous-to-crystalline) transitions, compared to nucleation dominated materials
which are more stochastic [27]. Thus based on the characteristics, and the number of
intermediate resistance levels (or synaptic weights) required for a particular neuro-
morphic system, it is important to optimize or select the phase-change material with
suitable nucleation and growth parameters. In quest for a low programming current,
long data retention, high endurance, and fast write/erase speed various stoichiometric
compositions of Ge-Sb-Te alloys have been investigated [27] (Fig. 10.4).
Properties of materials like GST can also be engineered by doping with elements
such as N, O, C and Si. O-doping of GST leads to an increased programming resis-
tance window and improved high temperature retention [28]. N-doping of GST has
shown better retention characteristics and increased crystalline resistivity [29, 30].
C-doping helps in reduction of programming current [31]. Optimization of the resis-
tance programming window and maximum/minimum device resistance is important
as it can have a signiﬁcant impact on the power consumption and leakage currents of
PCM synapses integrated in large scale neuromorphic crossbar architectures [32].

160
M. Suri and B. DeSalvo
Fig. 10.4 Composition
triangle of Sb, Te and Ge.
Compositions with a
nucleation-dominated
crystallization mechanism
(class I) and with a
growth-dominated
crystallization mechanism
(class II) are indicated.
(Reprinted from [69] with
kind permission of Springer
Science and Business Media)
10.2.2
Device Operation
PCM devices work on the principle of reversible switching between the amorphous
and crystalline phases of a chalcogenide material, induced by joule heating. Gener-
ally, a PCM cell consists of a programmable resistor and a selector element (transistor
or diode) [14]. The devices that we fabricated (shown in Fig. 10.5) consist a pro-
grammable resistor and no selector element. The programmable resistor is made up
of a thin layer of chalcogenide material sandwiched between two metallic electrodes
and a resistive element called the heater plug. In our devices the heater was made up
of metallic tungsten. When a potential difference is applied across the two metallic
contacts, current ﬂows vertically from the bottom electrode through the heater and
the chalcogenide layer to the top electrode. This leads to the joule heating of the
Fig. 10.5 Illustration showing a PCM cell consisting a selector (transistor) and non-volatile resistor.
The resistor element is shown zoomed. The thin chalcogenide (GST) layer is sandwiched between
electrodes and a heater plug [67]. (Reprinted with permission from Ref. [67]. © 2011 IEEE)

10
Phase Change Memory and Chalcogenide Materials for Neuromorphic Applications . . . 161
Fig. 10.6 Schematic of the
temperature-time proﬁle of
applied electrical pulses used
for amorphization and
crystallization of the
chalcogenide material [67].
(Reprinted with permission
from Ref. [67]. © 2011 IEEE)
chalcogenide layer. Usually for lance-type devices with a small and resistive plug,
the joule heating is concentrated in a hemispherical or mushroom-like shape at the
heater-chalcogenide interface [70].
To amorphize the chalcogenide, a current or voltage pulse strong enough to heat
the material above its melting temperature (600 ◦C for GST) is used (see Fig. 10.6).
A pulse which induces melting can be termed as a reset-pulse. The width of the reset-
pulse should be short (few 10s of ns) with an abrupt falling edge, (< 10 ns). The fast
falling edge doesn’t give sufﬁcient time for the molten material to re-organize itself
in a regular crystalline structure and hence it solidiﬁes in a disorganized amorphous
state. In order to crystallize the material, a set-pulse that is sufﬁcient to heat the
material above its crystallization temperature, but below the melting temperature is
used. The width and the fall time of a set-pulse should be sufﬁciently long to allow
the material to reorganize itself in an ordered crystalline structure (typically 50–
150 ns depending on the chalcogenide used) [33]. Heating the material between the
crystallization and the melting temperature enables to restore the crystalline lattice,
which is a minimum-energy structural conﬁguration [34]. Figure 10.7 shows the
I–V curve of a PCM device starting from initially crystalline and amorphous phases.
When the initial state of the device is less resistive or crystalline, the current increases
almost linearly (ohmic) with the applied bias. In the case of amorphous or high
resistive initial state, as the applied bias reaches a certain voltage (threshold switching
voltage, VTH) a snapback takes place and the conductance abruptly switches to a
high conductive state. This switching effect is called the threshold- or electronic-
switching. The physics governing electronic-switching effects is brieﬂy discussed in
Sect. 10.2.3.
Electronic-switching is volatile (temporary), and stays as long as the electric
ﬁeld inside the phase change material is above a critical switching ﬁeld. Note that
the phase doesn’t change during electronic-switching; it remains amorphous, but
becomes more conductive. The occurrence of this electronic-switching is a very
important characteristic of PCM material. Thanks to such switching mechanism
sufﬁcient current can ﬂow through the resistive amorphous material at low voltages,
heating the material at crystallization or melting temperatures. The non-volatile and

162
M. Suri and B. DeSalvo
Fig. 10.7 I–V Characteristics for a GST based PCM device. When the initial state is crystalline the
current increases in an ohmic fashion with the applied voltage. When the initial state is amorphous
current abruptly increases after reaching a threshold voltage. This effect is the electronic switching
effect. (Reprinted with permission from [70] John Wiley and Sons)
Fig. 10.8 R–V characteristics
for our GST PCM device.
After every programming
pulse the cell is initialized
with a reset pulse [67].
(Reprinted with permission
from Ref. [67]. © 2011 IEEE)
reversible phase-change phenomenon is thus a consequence of the volatile electronic-
switch. In the case of optical disks, such switching effect is not exploited for phase
transitions, as the chalcogenide material is directly heated by shining a laser and
not by passing current through it. Figure 10.8 shows the characteristic R–V curves
for our GST PCM devices. Six different programming pulse-widths (in the range
50–1000 ns) were used to demonstrate the resistance modulation with pulse-width.
At the start of the test, the device is programmed to a high resistive amorphous state
using a strong reset-pulse. This is followed by the application of a programming
pulse. After the programming pulse a small reading voltage of 0.1 V is applied to
measure the device resistance. Note that for the same value of pulse amplitude, the
measured resistance is different for different pulse widths. The idea is to exploit this
characteristic by using the values of PCM resistance (or conductance) as an analog
to synaptic weights within a neuromorphic system. Moving along the high-to-low

10
Phase Change Memory and Chalcogenide Materials for Neuromorphic Applications . . . 163
resistance transition can be used to emulate synaptic potentiation (or LTP) while
the vice versa case can be used to emulate synaptic depression (or LTD) effects. A
combination of the two effects, with the right programming scheme can be used for
implementing learning rules such as STDP.
10.2.3
PCM Modeling
An important step in the process of fabricating large-scale hybrid CMOS/PCM neural
systems, is to design and model the circuits. Several models capable of reproducing
PCM behavior have been reported in literature [35, 36]. A useful PCM model has
to account for the description of carrier transport in both crystalline and amorphous
phases. The modeling complexity is further increased due to the heat equations and
the phase transition dynamics. It is possible to do semiconductor type treatment for
the amorphous phase of GST. Crystalline and amorphous GST have a band-gap of
0.5 and 0.7 eV, respectively [37, 38]. Due to a large density of vacancies crystalline
GST can be considered as a p-type semiconductor. Since amorphous compounds
are not characterized by long-range order, the adoption of a band-structure to model
the amorphous phase can be tricky [39]. The physics involved in the process of
electronic-switching is not straightforward. Different models have been proposed
to explain the volatile electronic switch. Initially many researchers supported the
idea that switching is a thermal effect and the current in an amorphous layer rises
due to the creation of a hot ﬁlament [40, 41]. It was later shown [42, 43] that the
switching effect is not thermal, and a semiconductor resistor may feature switching
when a strong carrier generation process depending on ﬁeld and carrier concentration
(e.g. impact ionization) is competing with Shockley-Hall-Read recombination via
localized states. Recently, current conduction in the amorphous region has been
explained as controlled by Poole-Frenkel emission of carriers among traps [44, 45].
10.2.4
Drift and Multi-Level Programming
The resistance of PCM devices, when programmed to a high resistive state, drifts
with time according to a power law [46] and reaches a saturating value. This effect
is commonly known as resistance-drift. The cause of this effect is attributed to struc-
tural relaxations in the amorphous phase [47]. Figure 10.9 shows the resistance-drift
characteristics of our GST PCM devices. Drift poses a problem for multi-level PCM
programming, for example if a device is programmed to a high resistance state, drift
would cause the resistance to increase with time. Thus if the device resistance is
read after a time interval it would indicate a value of resistance which is higher from
the one initially programmed. Some methods have been proposed in literature to
minimize the impact of resistance drift [48]. Different techniques have been pro-
posed to implement multi-bit programming of PCM devices. While one approach is

164
M. Suri and B. DeSalvo
Fig. 10.9 Graph showing the
resistance-drift effect in GST
PCM devices. Circle
indicates an initial
programming voltage of 8 V
while the triangle represents
an initial programming
voltage of 10 V [67].
(Reprinted with permission
from Ref. [67]. © 2011 IEEE)
Fig. 10.10 Simulations
showing the effect of
tail-duration on the amount of
amorphous region crystallized
with the application of a
crystallizing pulse [51]
based on modiﬁcation of the device structure, other relies on innovative program-
ming techniques. Device structures can be modiﬁed by engineering the properties
of the chalcogenide layer by doping [49] or by stacking multiple layers of differ-
ent chalcogenide materials [50]. Innovative programming schemes exploit the fact
that resistance can be controlled by controlling the volume of the amorphous region
formed inside the chalcogenide layer [51]. The volume of amorphous region can be
increased by increasing the amplitude of the reset-pulse, and can be decreased by
applying short set-pulses or annealing pulses. The amount of amorphous region crys-
tallized can be controlled by carefully programming the tail-duration or the fall-time
of the crystallizing pulse (Fig. 10.10).
10.2.5
State-of-the-art Devices
In this section we report some key metrics of PCM technology which are signiﬁcant
for use in any large scale systems. Scaling is crucial as the number of synapses

10
Phase Change Memory and Chalcogenide Materials for Neuromorphic Applications . . . 165
Fig. 10.11 Trend showing a consistent decrease in reset and set current values with scaling of
the active contact area. Several state-of-the art PCM technologies and device architectures are
represented in the plot [64]. (Reprinted with permission from Ref. [64]. © 2011 IEEE)
estimated in the human cerebral cortex is about 1015. While PCM devices have been
already successfully scaled down to 45 nm node [52], recent research has shown that
phase change materials can be scaled down to as low as 5 nm in all three directions
[53].
Continued scaling and innovative device design have lead to a sharp decrease in the
power consumption of PCM devices over the years [54]. Generally the reset current
in lance-type PCM devices scales directly with the active contact area between the
heater plug and the chalcogenide layer. In Fig. 10.11, we plotted some recent PCM
technologies to show a consistent decreasing trend for the values of programming
current with device scaling. Substantial decrease in active contact area using carbon
nanotube electrodes for PCM devices has reduced the programming current to as low
as 1.4 μA [55] and write-energy per bit to 100 fJ [56]. GST based PCM devices have
been reported with a very high programming endurance of up to 1012 write-erase
program cycles [57, 58]. Predictable endurance as high as 6.5 × 1015 cycles was
shown (Fig. 10.12) [72]. This gives a good margin for synaptic applications in neural
circuits even with high frequency neuron ﬁring/spiking rates. Long data-retention at
temperatures as high as 110 ◦C, [59] and 154 ◦C [30] have been reported.
10.3
Neuromorphic Applications
Apart from inventing the modern day PCM, Stanford Ovshinsky was also among the
ﬁrst few to suggest the possibility of using phase change materials for neuromorphic
applications in the form of ovonic-cognitive devices [60]. In his approach Ovshin-
sky emphasizes on exploiting the fact that phase-change behavior in chalcogenides

166
M. Suri and B. DeSalvo
Fig. 10.12 Endurance
characteristic of dash
conﬁned cell at 4.5E–11J
reset program energy [72]
is a non-volatile phenomenon and dependent on the threshold-effect. When start-
ing from an amorphous state, several partially-crystallizing or weak set-pulses (not
sufﬁcient to fully crystallize) can be applied to the PCM device before a fully conduc-
tive percolation ﬁlament is formed inside the amorphous region and the resistance
drops by a large amount. This behavior is somewhat similar to the neuron ﬁring.
According to the Integrate-Fire (IF) and the Leaky-Integrate-Fire (LIF) models, a
neuron should ﬁre when the integral of its synaptic inputs reaches a ﬁring-threshold.
Thus by operating the PCM devices in the reset-to-set transition regime similar
neuron like processing can be emulated. Moreover for any point in the transition,
the history of the previous state is also preserved due to the non-volatile nature of
the chalcogenide. Thus non-volatility and threshold behavior allow to couple the
neuron-synapse like functionality of processing and storing information simultane-
ously within the same functional component. This type of computing is a paradigm
shift from conventional Von-Neumann approach, where memory and processing are
handled separately. Chalcogenide based cognitive computing concept was recently
demonstrated by C.D. Wright et al., in the form of a “phase-change processor” [61].
The processor can perform simple arithmetic functions like addition, subtraction,
multiplication, and division with simultaneous storage of results. The group de-
signed an optical experiment (Fig. 10.13) on nucleation dominated GST. A speciﬁc
amount of percentage change in the reﬂectivity of the amorphous phase is chosen
as the threshold parameter for a particular arithmetic operation (Fig. 10.14). Optical
pulses are carefully selected by choosing different values of pulse-duration and pulse-
intensity (in mJ/cm2). For instance to perform addition, the experiment starts with
the phase change layer in amorphous phase. Partially-crystallizing excitations equal
in number to the ﬁrst addend are applied, followed by the application of excitations
equal in number to the second addend. The phase-change processor automatically
sums the two addends due to its accumulation property, simultaneously storing the
result (at the same physical location). To access the stored result, excitations are ap-
plied until the reﬂectivity threshold is reached, the number of excitations required to
reach the threshold and the calculation base reveal the result. The entire process can
also be analyzed by observing the size of the amorphous and the crystalline regions
under a microscope.

10
Phase Change Memory and Chalcogenide Materials for Neuromorphic Applications . . . 167
Fig. 10.13 (Top) Schematic showing the laser experiment. (Bottom) Processing using the accu-
mulation or threshold property of GST. The property can also be used to model a threshold based
ﬁring neuron with multiple weighted parallel inputs [61]. (Reprinted with permission from [61]
John Wiley and Sons)
Fig. 10.14 Experimentally
measured (squares) change in
optical reﬂectivity of
amorphous phase of the GST
sample as a function of the
number of pulses applied
[61]. (Reprinted with
permission from [61] John
Wiley and Sons)
10.3.1
Synaptic Potentiation and Depression
We now discuss applications and approaches which are more speciﬁcally adapted
for electronic PCM devices. In hybrid neuromorphic systems the neurons will be
emulated by conventional CMOS transistor circuits, and synapses by a densely inte-
grated memristive technology such as the PCM. Since the functionality of the neuron

168
M. Suri and B. DeSalvo
is taken care by CMOS circuits, we don’t need to focus on the threshold related pro-
cessing or ﬁring discussed in the previous section. For emulating synaptic behavior
in a large scale hybrid neural networks the most important requirement is the conduc-
tance (or resistance) modulation of the PCM device. This is because the conductance
of the synapses connecting a pre- and post-neuron in any neural network undergo
modiﬁcation on the basis of speciﬁc learning rules, forms of synaptic-plasticity, and
stimuli [2, 12]. The stimuli in this case are the action potential or the electrical
spikes generated by the neuron circuits. The efﬁcacy of connection between the pre-
and post- synaptic neuron can either increase or decrease. The increase in synaptic
strength (or potentiation) can be emulated by decreasing the resistance of the PCM
synapse, or in other words by creating crystalline regions inside the chalcogenide
material. Similarly the decrease in synaptic strength (or depression) can be emulated
by increasing the PCM resistance or creating amorphous regions in the chalcogenide.
The action potential or the spikes generated by the neurons in simple spiking-neural
networks are assumed to be identical. Information is believed to be encoded in the
form of neuron spike-timing or rates, and not exactly in the spike shape or the ampli-
tude. Thus to emulate spiking neural networks, it is essential that synaptic depression
and synaptic potentiation can be achieved by application of identical pulses. Using
non-identical spikes to program the synapses would lead to added complexity on
part of the neuron circuits producing such spikes. Thus we deﬁne two types of pulses
for our purpose- A “depressing” or amorphizing pulse (reset) and a “potentiating”
or partially crystallizing (weak set) pulse.
We show that using our GST PCM devices, it was possible to emulate gradual
synaptic potentiation over a wide range of resistance values (about 3 orders of mag-
nitude) by applying identical potentiating pulses (Fig. 10.15). For the experiment
shown in Fig. 10.15, the device is initialized to a high resistance state (110 k) by
applying a strong reset pulse (8 V, 500 ns). After the initial reset pulse, 30 identical
potentiating pulses (all equal to 2 V, with a ﬁxed pulse width) are applied, and the
resistance of the device is plotted. The device resistance is measured by applying
a small reading pulse of 0.1 V between two consecutive potentiating pulses. Resis-
tance gradually decreases with each potentiating pulse, until it reaches a saturating
minimum value. This is because of the fact that crystallization in PCM is an energy-
cumulative process, and the impact of applying several short crystallizing pulses is
almost the same as applying a single long crystallizing pulse of the same amplitude.
The minimum and maximum resistance values can be modiﬁed to some extent by
engineering the chalcogenide layer (Sect. 10.2.3) and the properties of the heater
plug.
Figure 10.16 shows the results of the synaptic depression experiment that we
performed on the GST PCM devices. In this test, the device was ﬁrst initialized to a
very low resistance (around 400 ) by applying a potentiating pulse (2V, 1 μs). After
application of the potentiating pulse, 15 identical depressing pulses (each of 8V, with
a ﬁxed pulse width) are applied, and resistance of the device is plotted. The process
is repeated for 5 different pulse widths. We observed that it was not possible to obtain
gradual synaptic depression using identical depressing pulses. Thus unlike synaptic-
potentiation, synaptic-depression in PCM seems more like an abrupt binary effect,

10
Phase Change Memory and Chalcogenide Materials for Neuromorphic Applications . . . 169
Fig. 10.15 Experiment
showing emulation of
synaptic potentiation behavior
using lance-type GST PCM
devices. Conductance can be
modulated as desired by
choosing the right
potentiating pulse-width. The
device was initialized with a
strong reset pulse before
starting the test [67].
(Reprinted with permission
from Ref. [67]. © 2011 IEEE)
Fig. 10.16 Experiment
showing emulation of
synaptic depression behavior
using lance-type GST PCM
devices. The depression
behavior is more abrupt due
to physics governing the
amorphization. The device
was initialized with a long
set-pulse before starting the
test [67]. (Reprinted with
permission from Ref. [67].
© 2011 IEEE)
with very few inter-mediate resistance states if identical pulses are used. The physical
reason for such an effect can be understood if we consider what happens inside the
device during the experiment. In order to depress the PCM synapse or increase
its resistance, amorphous region needs to be created. For amorphization, a region
ﬁrst needs to be melted and then quickly cooled. The volume of the molten region
created inside the chalcogenide layer increases with the amplitude of the applied
reset pulse. Thus a strong depressing pulse would melt a larger region compared
to a weak depressing pulse, creating a larger amorphous cap and a higher value of
ﬁnal device resistance (Fig. 10.8). When identical depressing pulses are applied,
each pulse provides almost the same amount of energy to the cell, creating nearly
the same amount of amorphous region every time, and same value of ﬁnal device
resistance.
It is important to consider that within an evolving neural network, a synapse
might undergo any arbitrary combination of potentiation and depression events (with
varying intensities), depending upon the spiking pattern of the post- and pre-synaptic
neurons. To emulate a test-case with different combinations of potentiation and

170
M. Suri and B. DeSalvo
Fig. 10.17 (Top) Pulses
sequence designed to emulate
an arbitrary combination of
potentiating and depressing
events for a PCM synapse.
(Bottom) Plot showing
resistance values after the
application of each pulse
shown on top. The entire
sequence of three different
pulses was applied 30 times
to the device. (Reprinted with
permission from Ref. [67].
© 2011 IEEE)
depression events, we performed the experiment shown in Fig. 10.17. In this test-
case, the PCM synapse undergoes three modiﬁcations. (i)A strong potentiating event,
followedby(ii)amoderatedepressingeventandﬁnally(iii)astrongdepressingevent.
The strong potentiating event is performed by applying a long potentiating pulse of
2 V, 4 μs. The moderate and strong depressing events are performed by applying
pulses of 6 V, 100 ns and 7 V, 300 ns, respectively. After each pulse, a small reading
voltage of 0.1 V is applied to measure the device resistance. The entire sequence is
repeated 30 times.
Notice the difference in resistance values obtained after the application of the two
depressing pulses shown in Fig. 10.17. The difference is due to varying intensities of
the pulses. This is in agreement with the trend shown in Fig. 10.8. The test demon-
strates that with carefully chosen depressing and potentiating pulses, PCM devices
can be used to emulate arbitrary combinations of synaptic events. It is important to
point out a limitation of using identical depressing pulses; if two depressing events of
equal intensity occur consecutively, the test would fail. Kuzum et al. [62] have shown
a different way of programming PCM synapses. In their approach they implement
progressive synaptic depression by using a pulse train of reset pulses with increasing
amplitude. This method helps to overcome the abrupt or binary like limitation of
synaptic depression but at the cost of generating non-identical neuron spikes.

10
Phase Change Memory and Chalcogenide Materials for Neuromorphic Applications . . . 171
Fig. 10.18 Illustration
showing pre- and post-neuron
pulse trains for implementing
STDP on a PCM synapse.
When any pulse in the
left-half of the pre -spike
overlaps with the post spike it
will amorphize the device. If
any pulse in right half of the
pre-spike overlaps with the
post spike it crystallizes the
device [62]. (Reprinted with
permission from [62]. © 2011
American Chemical Society)
10.3.2
Spike Time Dependent Plasticity
Different methods have been proposed to implement spike-time-dependent-plasticity
(STDP) on PCM synapses. In one approach [62], the pre-neuron spike is designed
as a special pulse train (Fig. 10.18). The left half of the pre-neuron spike consists
of a depressing pulse train of increasing amplitude, while the right half consists of
a potentiating pulse train of decreasing amplitude. The post neuron spike is a single
pulse of inverted polarity. The pulse amplitudes are chosen in such a way that no
spike when applied alone would affect the PCM synapse. Only an overlap of the
pre- and post-neuron spike would produce sufﬁcient amplitude to cause a change in
the synaptic weight. When the post neuron spike arrives late, it overlaps with the
right half of the pre-neuron spike. The resultant of the overlap is a pulse which can
cause potentiation at the synapse. Similarly if the post neuron-spike arrives before
the pre-neuron spike, it will overlap with the left half of the pre-spike. This would
result in a depressing pulse at the synapse.
Another similar patented approach [63] outlines the use of a transistor element
connected across one terminal of the PCM device (Fig. 10.19). In this case the
structure of the post-neuron spike consists of a pulse train. The left half of this pulse
train is capable of inducing potentiation, while the right half induces depression
across the synapse. The pre-neuron spike is just a simple pulse which turns the
transistor ON or OFF. If the pre-spike arrives before, the turning ON of the transistor
overlaps with the left half of the post-neuron spike and thus potentiation occurs.
If the post-neuron spike arrives before, the transistor would switch ON over the
right half of the pre-spike and hence synaptic depression would occur. Both the
approaches mentioned here suffer from the problem of complicated pulse shapes. In
fact, implementingspikesofpulsetrainsthatarenotidenticalwouldleadtoadditional
circuitry in the CMOS neuron circuit driving the synapses. Complex pulse schemes
(involving pulse-trains) when implemented in large scale neural systems would lead
to excessive power dissipation and capacitive line charging across the large synaptic

172
M. Suri and B. DeSalvo
Fig. 10.19 (Top) Schematic
showing a controlling
transistor attached to one
terminal of a PCM device for
implementing STDP.
(Bottom) Illustration showing
the design of the pre- and
post- neuron spikes used to
implement STDP on the PCM
synapse [63]
crossbars. Even for the events in which the synaptic-weight update is minimal, a
large amount of energy would be dissipated as the entire pulse train is applied across
the synapse.
10.4
The 2-PCM Synapse
To solve the problem of abrupt synaptic depression and complex pulse schemes
discussed in Sects. 10.3.1 and 10.3.2 respectively, we suggested a different ap-
proach [64] based on the use of two PCM devices for emulating a single synapse
(Fig. 10.20). In this scheme one of the PCM device helps in realizing synaptic
potentiation (LTP-device) while the other (LTD-device) helps to realize synaptic
depression. The devices are initialized to a high resistive amorphous state before
the network undergoes learning. When synaptic potentiation is desired, potentiating
pulses are applied to the LTP device, and when synaptic depression is desired po-
tentiating pulses are applied to the LTD device. Note that in both cases the devices
are always potentiated (or crystallized). The contribution of the currents through the
LTP device to the post neuron is positive, while that of the LTD device is negative.
Thus even when the LTD device is potentiated, the overall impact of the synapse
is like synaptic-depression, because the current ﬂowing through it is subtracted in
the post neuron. Since the depression is also attained by crystallization, it can be
performed gradually and by using identical pulses. When the devices are saturated,

10
Phase Change Memory and Chalcogenide Materials for Neuromorphic Applications . . . 173
Fig. 10.20 Schematic
showing the concept of
2-PCM synapse. The
contribution of the current
from the LTD device is
subtracted at the post synaptic
neuron [64]. (Reprinted with
permission from Ref. [64]. ©
2011 IEEE)
or they reach their minimum resistance value, they can be periodically reset and
reprogrammed. The details of the programming and the reset scheme are described
in [64]. The main beneﬁt of our 2-PCM synapse approach is its high energy efﬁ-
ciency. Note from Fig. 10.11 that even for the best PCM devices reported, the value
of Ireset (amorphizing current) is at least ten times more than Iset (crystallization
current). Since majority of the synaptic events are now achieved by crystallization
(and not by amorphization), a lot of energy is saved and a low-power system can
be achieved. Another advantage of using such an approach is better tolerance to
loss of information from resistance-drift. Drift in PCM is negligible in crystalline
or low-resistivity states. In the 2-PCM synapse approach, majority of the synaptic
information gets stored in the low-resistance states of the PCM synapses. Thus the
probability of losing information due to drift decreases.
10.4.1
Application to Pattern Extraction
We now brieﬂy discuss how a PCM synapse based neuromorphic system can be
used for real-life applications. We designed and simulated a two layer spiking feed-
forward neural network (Fig. 10.21) for pattern extraction applications [64]. The
network is fully connected consisting of 70 neurons and about 2 million synapses.
A special purpose event-driven simulator developed in Matlab and C + + was used
to simulate the network. Potentiating characteristics of the PCM devices such as the
ones shown in Fig. 10.15 were extracted from real experiments and ﬁtted using a
behavioral model described in [65]. The ﬁtting parameters were then used to model
each synapse in the neural network according to the 2-PCM synapse model described
in the previous section.A simpliﬁed form of biological STDP learning rule along with
lateral-inhibition was implemented. The neuron was implemented using the standard
LIF model. A video of cars passing on a six-lane freeway recorded in address-event
representation (AER) format (Fig. 10.22) using a 128 × 128 pixel artiﬁcial silicon
retina [66] was used as the input for the neural network. The goal of the learning
experiment was to make the neurons sensitive to certain orientations of the cars
passing in different lanes. The simulated network was able to detect cars passing

174
M. Suri and B. DeSalvo
Fig. 10.21 Illustration showing a two layer feed forward fully connected spiking neural network.
AER data is fed from a DVS artiﬁcial retina in the ﬁrst layer of the neural network. Each neuron
in 1st layer is connected to every pixel by two synapses. Each neuron in 2nd layer is connected to
neurons in the ﬁrst layer by two synapses [64]. (Reprinted with permission from Ref. [64]. © 2011
IEEE)
Fig. 10.22 Screen shot from
AER motion data of cars
passing on a six-lane freeway,
recorded using the artiﬁcial
silicon DVS retina sensor.
The dotted lines marking the
lanes were added
superﬁcially. Difference in
color indicates the difference
in luminous intensity at a
particular pixel [64].
(Reprinted with permission
from Ref. [64]. © 2011 IEEE)
on the freeway in a fully unsupervised way, with a very high average detection rate
(> 90 %) and a low power consumption of just 112 W in learning mode. We give
the detailed interpretation of the learning results and the power estimation for this
experiment in [64]. The synapse models described in these simulations can be easily
adopted to evaluate the characteristics of different resistive memory technologies
such as RRAM or CBRAM. The idea of pattern extraction from video AER data
using the network described in Fig. 10.21 can be extended to other applications such
as learning patterns in static images and audio sequences. The simulations that we
performed are the ﬁrst of their kind, establishing a link between experimental data
on synaptic plasticity in individual PCM devices, and high-level learning in large
scale neuromorphic systems. It is a step forward in the direction of fabricating actual

10
Phase Change Memory and Chalcogenide Materials for Neuromorphic Applications . . . 175
large scale hybrid neuromorphic systems consisting resistive-memory synapses and
CMOS neurons.
10.5
Conclusions
Chalcogenides and PCM devices have come a long way of research and develop-
ment, enabling reliable optical and electrical storage concepts. High endurance and
good data retention make the PCM technology viable for use in commercial prod-
ucts. CMOS compatibility, high scalability, and a consistent trend of decreasing
programming-current with scaling, make PCM an ideal choice for emulating mem-
ristive synapses in large-scale neural systems. Forms of synaptic plasticity such
as synaptic-potentiation, synaptic-depression, and learning rules like STDP have
been demonstrated on individual PCM synapses. Innovative pulse-schemes and pro-
gramming techniques allow precise conductance or resistance modulation of PCM
synapses. Real-time pattern extraction from complex visual data has been demon-
strated using innovative low-power PCM synapse-circuit simulations. There exist
several interesting possibilities for designing the optimal PCM synapse by engineer-
ing chalcogenide material properties, and the device structure. The next logical step
in this ﬁeld of research demands co-integration of large PCM-synapse arrays along
with CMOS neurons on the same chip.
Acknowledgments The authors would like to specially thank Christian Gamrat, Olivier Bichler,
and Damien Quileroz for collaborating on the work described in Sec. 10.4. Dominique Vuillaume,
Luca Perniola, Veronique Sousa, and Ludovic Poupinet for fruitful discussions.
References
1. Snider GS (2008) Proceedings of IEEE international symposium on nanoscale architectures
(NANOARCH), pp 85–92
2. Purves D (2004) Neuroscience, 3rd ed. Sinauer Associates Inc, Massachusetts, pp 7–9
3. Mitra S, Fusi S, Indiveri G (2006) Proceedings of IEEE international symposium on Circuits
and Systems (ISCAS)
4. Likharev K (2005) Proceedings of european conference on circuit theory design, vol 2, pp
II/273
5. Alibart F, Pleutin S, Guérin S, Novembre C, Lenfant S, Lmimouni K, Gamrat C, Vuillaume V
(2010) An organic nanoparticle transistor behaving as a biological spiking synapse. Adv Funct
Mat 20:330–337
6. Folling S, Turel O, Likharev K (2001) Proceedings of international joint conference on neural
networks (IJCNN), pp 216–221
7. Friesz AK, Parker AC, Zhou C, Ryu K, Sanders JM, Wong HSP, Deng J (2007) Annual fall
meeting biomedical engineering society (BMES)
8. Jo SH, Chang T, Ebong I, Bhavidya BB, Mazumder P, Lu W (2010) Nanoscale memristor
device as synapse in neuromorphic systems. Nano Lett 10(4):1297–1301
9. Yu S,Wong HSP (2010) IEEE international electron devices meeting (IEDM), pp 22.1.1–22.1.4

176
M. Suri and B. DeSalvo
10. Zhang XL (2008) A mathematical model of a neuron with synapses based on physiology.
hdl:10101/npre.2008.1703.1, Nature precedings. http://precedings.nature.com/documents/
1703/version/1
11. Sargsyan AR, Melkonyan AA, Papatheodoropoulos C, Mkrtchian HH, Kostopoulos V (2003)
A model synapse that incorporates the properties of short-term and long-term synaptic plast-
icity. Neural Netw 16(8):1161–1177
12. Song S (2004) Hebbian learning and spike-timing-dependent plasticity. Computational neuro-
science. In: Feng J (ed). Chapman & Hall/CRC (Chap. 11)
13. Chua L (1971) Memristor the missing circuit element. IEEE Trans circ theory 18(5):507–519
14. DeSalvo B (2009) Silicon non-volatile memories. Wiley-ISTE, pp 188–216
15. Waterman AT (1917) Positive ionization of certain hot salts, together with some observations
on the electrical properties of molybdenite at high temperatures. Phil Mag 33:225
16. PearsonAD, Northover WR, Dewald JF, Peck WF, Jr (1962) Chemical, physical, and electrical
properties of some unusual inorganic glasses. Advances in glass technology. Plenum Press,
New York, pp 357–365
17. Ovshinsky SR (1968) Reversible electrical switching phenomena in disordered structures. Phys
Rev Lett 22;1450–1453
18. Feinleib J, Neufville JD, Moss SC, Ovshinsky SR,(1971) Rapid reversible light induced
crystallization of amorphous semiconductors. Appl Phys Lett 18:254–257
19. Neale RG, Nelson DL, Moore GE (1970) Nonvolatile and reprogrammable, the read mostly
memory is here. Electronics 43:56–60
20. Shanks RR, Davis C (1978) ISSCC digest of technical papers, pp 112–113
21. Bedeschi F, Fackenthal R, Resta C, Donze EM, Jagasivamani M, Buda EC, Pellizzer F, Chow
DW, Cabrini A, Calvi G, Faravelli R, Fantini A, Torelli G, Mills D, Gastaldi R, Casagrande
G (2009) A bipolar-selected phase change memory featuring multi-level cell storage. IEEE J
Solid State Circ 44(1):217–227
22. Close GF, Frey U, Morrish J, Jordan R, Lewis S, Mafﬁtt T, Breitwisch M, Hagleitner C, Lam
C Eleftheriou E (2011) Symposium on VLSI circuits, pp 202–203
23. Lee KJ, Cho BH, Cho WY, Kang S, Choi BG, Oh HR, Lee CS, Kim HJ, Park JM, Wang Q,
Park MH, Ro YH, Choi JY, Kim KS, Kim YR, Shin IC, Lim KW, Cho HK, Choi CH, Chung
WR, Kim DE, Yoon YJ, Yu KS, Jeong GT, Jeong HS, Kwak CK, Kim CH, Kim K (2008) A
90 nm 1.8 V 512 Mb diode-switch PRAM with 266MB/s read throughput. IEEE J Solid-State
Circ 43(1):150–162
24. van Pieterson L, Lankhorst MHR, van Schijndel M, Kuiper AET, Roosen JHJ (2005) Phase-
change recording materials with a growth-dominated crystallization mechanism: a materials
overview. J Appl Phys 97:083520
25. Bruns G, Merkelbach P, Schlockermann C, Salinga M,Wuttig M, HappTD, Philipp JB, Kund M
(2009) Nanosecond switching in GeTe phase change memory cells. Appl Phys Lett 95:043108
26. Russo U, Ielmini D, Lacaita AL (2007) IEEE Annual international reliability physics
symposium, pp 547–553
27. Boniardi M, Ielmini D, LacaitaAL, RedaelliA, PirovanoA, Tortorelli I,Allegra M, Magistretti
M, Bresolin C, Erbetta D, Modelli A, Varesi E, Pellizzer F, Bez R,(2010) IEEE international
memory workshop (IMW), pp 1–4
28. MatsuzakiN,KurotsuchiK,MatsuiY,TonomuraO,YamamotoN,FujisakiY,KitaiN,Takemura
R, Osada K, Hanzawa S, Moriya H, Iwasaki T, Kawahara T, Takaura N, Terao M, Matsuoka
M, Moniwa M (2005) IEEE international electron devices meeting (IEDM), pp 738–741
29. Horii H, Yi JH, Park JH, Ha YH, Baek IG, Park SO, Hwang YN, Lee SH, Kim YT, Lee KH,
Chung UI, Moon JT (2003) Symposium on VLSI Tech, pp 177–178
30. Fantini A, Sousa V, Perniola L, Gourvest E, Bastien JC, Maitrejean S, Braga S, Pashkov N,
Bastard A, Hyot B, Roule A, Persico A, Feldis H, Jahan C, Nodin JF, Blachier D, Toffoli A,
Reimbold G, Fillot F, Pierre F,Annunziata R, Benshael D, Mazoyer P,Vallee C, Billon T, Hazart
J, De Salvo B, Boulanger F (2010) IEEE international electron devices meeting (IEDM), pp
29.1.1–29.1.4

10
Phase Change Memory and Chalcogenide Materials for Neuromorphic Applications . . . 177
31. Beneventi GB, Perniola L, Fantini A, Blachier D, Toffoli A, Gourvest E, Maitrejean S, Sousa
V, Jahan C, Nodin JF, Persico A, Loubriat S, Roule A, Lhostis S, Feldis H, Reimbold G, Billon
T, De Salvo B, Larcher L, Pavan P, Bensahel D, Mazoyer P, Annunziata R, Boulanger F (2010)
Proceedings of the european solid state device research conference (ESSDERC), pp 313–316
32. Zamarreo-Ramos C, Camuas-Mesa LA, Perez-Carrasco JA, Masquelier T, Serrano-
Gotarredona T, Linares-Barranco B (2011) On spike-timing-dependent-plasticity, memristive
devices, and building a self-learning visual cortex. Front Neurosci 5:26
33. Peng C, Cheng L, Mansuripur M (1997) Experimental and theoretical investigations of
laser-induced crystallization and amorphization in phase-change optical recording media.
J Appl Phys 82(9):4183–4191
34. Wuttig M, Yamada N (2007) Phase-change materials for rewriteable data storage. Nat Mater
6:824–832
35. Ielmini D, Zhang Y (2007) Analytical model for subthreshold conduction and threshold
switching in chalcogenide based memory devices. J Appl Phys 102:054517
36. Sonoda K, Sakai A, Moniwa M, Ishikawa K, Tsuchiya O, Inoue Y (2008) A compact model
of phase change memory based on rate equations of crystallization and amorphization. IEEE
Trans Electron Dev 55(7):1672–1681
37. Pirovano A, Lacaita AL, Benvenuti A, Pellizzer F, Bez R (2004) Electronic switching in phase-
change memories. IEEE Trans Electron Dev 51(3):452–459
38. Kato T, Tanaka K (2005) Electronic properties of amorphous and crystalline Ge2Sb2Te5 ﬁlms.
Japanese J Appl Phys 44(10):7340–7344
39. Mott NF Davis EA (1967) Electronic processes in noncrystalline materials. Clarendon Press,
Oxford
40. Popescu C (1975) The effect of local non-uniformities on thermal switching and high ﬁeld
behavior of structures with chalcogenide glasses. Solid State Electron 18(7/8):671–681
41. Owen AE, Robertson JM, Main C (1979) The threshold characteristics of chalcogenide-glass
memory switches. J Non-Cryst Solids 32:29–52
42. Adler D, Henisch HK, Mott SD (1978) The mechanism of threshold switching in amorphous
alloys. Rev Mod Phys 50(2):209–220
43. Adler D, Shur MS, Silver M, Ovshinsky SR (1980) Threshold switching in chalcogenide-glass
thin ﬁlms. J Appl Phys 51(6):3289–3309
44. Ielmini D, Zhang Y (2007) Analytical model for subthreshold conduction and threshold
switching in chalcogenide-based memory devices. J Appl Phys 102:054517
45. Ielmini D, Zhang Y (2007) Evidence for trap-limited transport in the subthreshold conduction
regime of chalcogenide glasses. Appl Phys Lett 90:192102
46. Karpov IV, Mitra M, Kau D, Spadini G, KryukovYA, Karpov VG (2007) Fundamental drift of
parameters in chalcogenide phase change memory. J Appl Phys 102(12):124503
47. Ielmini D, Lavizzari S, Sharma D, Lacaita AL (2007) IEEE international electron devices
meeting (IEDM), pp 939–942
48. Xu W, Zhang T (2010) International symposium on quality electronic design (ISQED), pp
356–361
49. Liu B, Zhang T, Xia J, Song Z, Feng S, Chen B (2004) Nitrogen-implanted Ge2Sb2Te5 ﬁlm
used as multilevel storage media for phase change random access memory. Semicond Sci
Technol 19(6):L61–L64
50. Lai YF, Feng J, Qiao BW, Cai YF, Lin YY, Tang TA, Cai BC, Chen B (2006) Stacked chalco-
genide layers used as multi-state storage medium for phase change memory. Appl Phys A
84(1–2):21–25
51. Nirschl T, Phipp JB, Happ TD, Burr GW, Rajendran B, Lee MH, SchrottA,Yang M, Breitwisch
M, Chen CF, Joseph E, Lamorey M, Cheek R, Chen SH, Zaidi S, Raoux S, Chen YC, Zhu Y,
Bergmann R, Lung HL, Lam C (2007) IEEE international electron devices meeting (IEDM),
pp 461–464
52. Servalli G (2009) IEEE international electron devices meeting (IEDM), pp 113–116
53. Caldwell MA, Raoux S,Wang RY,Wong HSP, Milliron DJ (2010) Synthesis and size-dependent
crystallization of colloidal germanium telluride Nanoparticles. J Mater Chem 20(7):1285–1291

178
M. Suri and B. DeSalvo
54. Wong HP, Raoux S, Kim S, Liang J, Reifenberg JP, Rajendran B, Asheghi M, Goodson KE
(2010) Phase change memory. Proc IEEE 98(12):2201–2227
55. Liang J, Jeyasingh RGD, Chen HY, Wong HSP (2011) Symposium onVLSI technology digest,
pp 100–101
56. Xiong F, Liao A, Estrada D, Pop E (2011) Low-power switching of phase-change materials
with carbon nanotube electrodes. Science 332(6029):568
57. Pellizzer F, PirovanoA, Ottogalli F, Magistretti M, Scaravaggi M, Zuliani P, Tosi M, Benvenuti
A, Besana P, Cadeo S, Marangon T, Morandi R, Piva R, SpandreA, Zonca R, ModelliA, Varesi
E, Lowrey T, Lacaita A, Casagrande G, Cappelletti P, Bez R (2004) Symposium on VLSI
technology digest, pp 18–19
58. Lai S (2003) IEEE international electron devices meeting (IEDM), pp 10.1.1–10.1.4
59. PirovanoA, RedaelliA, Pellizzer F, Ottogalli F, Ielmini D, LacaitaAL, Bez R (2004) Reliability
study of phase-change nonvolatile memories. IEEE Trans Device Mater Reliab 4(3):422–427
60. Ovshinsky SR (2004) Optical cognitive information processing—a new ﬁeld. Japanese J Appl
Phys 43 (7B):4695–4699
61. Wright CD, LiuY, Kohary KI,Aziz MM, Hicken RJ (2011)Arithmetic and biologically-inspired
computing using phase change materials. Adv Mater 23:3408–3413
62. Kuzum D, Jeyasingh RGD, Lee B, Wong HSP (2011) Nanoelectronic programmable
synapses based on phase change materials for brain-inspired computing.
Nano Lett.
doi:10.1021/nl201040y
63. Breitwisch MJ, Cheek RW, Lam CH, Modha DS, Rajendran B (2010) US Patent Application
Publication, US2010/0299297, 25 Nov 2010
64. Suri M, Bichler O, Querlioz D, Cueto O, Perniola L, SousaV,Vuillaume D, Gamrat C, DeSalvo,
B (2011) IEEE international electron device meeting (IEDM)
65. Querlioz D, Dollfus P, Bichler O, Gamrat C (2011) Proc IEEE/ACM international symposium
on nanoscale architectures (NANOARCH), pp 150–156
66. Lichtsteiner P, Posch V, Delbruck V (2008) A 128 × 128 120 dB 15 μs latency asynchronous
temporal contrast vision sensor. IEEE J solid-state circ 43(2)566–576
67. Suri M, Sousa V, Perniola L, Vuillaume D, DeSalvo B (2011) International joint conference on
neural networks (IJCNN), pp 619–624
68. Meinders ER, Mijiritskii AV, van Pieterson L, Wuttig M (2006) Optical data storage-phase-
change media and recording. Springer, The Netherlands
69. Raoux S, Wuttig M,(2008) Phase change materials- science and applications. Springer, Berlin
70. Lacaita AL, Wouters DJ (2008) Phase-change memories. Phys Stat Sol A 205(10):2281–2297
71. Shelby RM, Raoux S (2009) Crystallization dynamics of nitrogen-doped Ge2Sb2Te5. J Appl
Phys 105:104902
72. Kim IS, Cho SL, Im DH, Cho EH, Kim DH, Oh GH, Ahn DH, Park SO, Nam SW, Moon JT,
Chung CH (2010) Symposium on VLSI technology, pp 203–204

Part III
Hardware Embodiments with Memristive
Properties and Applications


Chapter 11
Energy-Efﬁcient Memristive Analog and Digital
Electronics
Sung Mo (Steve) Kang and Sangho Shin
Abstract This chapter reviews recent technology trends in memristive analog and
digital electronics, with particular attention to programmable analog and digital
circuits, ultra-dense nonvolatile resistive memory architectures, and zero leakage
nonvolatile logic gates. A reconﬁgurable nonvolatile computing platform that har-
nesses memristor properties is used to deploy massive local nonvolatile memories
and advance computing capabilities with much lowered energy consumption than the
conventional charge-basedVLSI systems. With application of memristive devices for
nonvolatile memories, programmable interconnects, stateful logic gates, and non-
volatile latches with high integration density and CMOS compatibility, combining
memristor technology with the prevailing CMOS technology poses to prolong the
Moore’s Law beyond the hitherto observed technological limitations.
11.1
Introduction
In 1971, Chua published a seminal paper on memristor as a missing basic circuit
element by explaining the constitutive relationship between electrical charge q and
ﬂux ϕ linkage [1]. Chua demonstrated that the memristor can be physically realized
by using other passive and active circuit elements, and predicted that inherently pas-
sive memristors would be found. In 1976, Chua and Kang published a paper that
deﬁned a large class of devices and systems which they named memristive devices
and systems to broaden the domain of useful nonlinear devices with memristive
characteristics substantially, and showed that many physical systems can be catego-
rized as memristive devices and systems [2]. In 2008, almost 40 years later, Stan
Williams and his research team at HP Labs unveiled a two-terminal titanium dioxide
nanoscale device that exhibited memristor characteristics in a restricted operating
range [3].
Continuing demands for more extensive information processing call for future
system integration technologies that can overcome various physical limitations. Of
S. M. Kang () · S. Shin
Department of Electrical Engineering, Jack Baskin School of Engineering,
University of California, Santa Cruz, CA, USA,
e-mail: kang@soe.ucsc.edu
R. Kozma et al. (eds.), Advances in Neuromorphic Memristor Science and Applications,
181
Springer Series in Cognitive and Neural Systems,
DOI 10.1007/978-94-007-4491-2_11, © Springer Science+Business Media Dordrecht 2012

182
S. M. Kang and S. Shin
particular importance is the suppression of leakage power that has become a dom-
inant limiting factor in CMOS VLSIs. The imminent barrier to Moore’s Law in
CMOS technology, so-called power wall, cannot be overcome without revolution-
ary approaches for systems integration. The emerging nanoscale resistive memory
technology is being investigated to overcome the physical limitations of CMOS tech-
nologies. In search of more advanced novel nanoelectronics exhibiting low-power
functionalities as well as ultra-dense integration of reconﬁgurable electronics, we
introduce in this chapter analog and digital memristive circuits which can uniquely
enable such new functionalities.
The memristive devices realized in a form of nanoscale bipolar voltage-actuated
devices use ‘resistance’ as a physical state variable [3]. Since nanoscale memristor
devices can be reconﬁgured into nonvolatile memories, logic gates, programmable
interconnects with high integration density and, more importantly, with CMOS
compatibility, the memristor technology together with CMOS is a formidable tech-
nology candidate that can advance Moore’s Law beyond the present silicon roadmap
horizons.
A multitude of new opportunities can be provided by memristive nanotechnolo-
gies. Due to their potential for inexpensive manufacturing and ultrahigh density,
various forms of memristors are being explored for many potential applications.
Besides the ultra-dense nonvolatile memory applications, other opportunities may
reside in highly programmable and self-adaptable analog/digital electronics, resistive
nanocomputing architectures, and synaptic neuromorphic networks, among many
others. A hybrid type of integration of CMOS circuits and memristive nanodevices
[4, 5] is considered promising for leading for diverse next generation applications
such as the implantable low power biological sensor application.
11.2
Properties of Memristors
Memristor deﬁnes the missing constitutive relationship between charge (q) and ﬂux
(ϕ), i.e., ϕ = f (q) [1]. Unlike the widely reported devices such as bistable switching
resistors that change their resistance according to the instant voltage or current (not
accumulation of voltages or currents), the resistance of memristors is determined
by the amount of charge (integration of currents) or ﬂux (integration of voltages),
thus exhibiting a unique charge–ﬂux relationship with a memory effect. In order to
model memristors simply, the terms ‘charge’ and ‘ﬂux’ will be used in this Chapter
thereby avoiding many integrals, even though the memristance does not depend on
the magnetic ﬂux physically.
From the constitutive relationship, ϕ = f (q), the memristance can be derived
mathematically by taking time derivatives on both sides:
dϕ
dt = d
dt[f (q)] =

 d
dqf (q)

· dq
dt
(11.1a)

11
Energy-Efﬁcient Memristive Analog and Digital Electronics
183
vM =

 d
dqf (q)

· iM ≡RM(q) · iM.
(11.1b)
Equation (11.1a) can be recast into (11.1b) when the ﬂux–voltage relationship
(v = dϕ/dt) and the charge–current (i = dq/dt) relationship are invoked. RM(q) is
the memristance deﬁned by the derivative of the charge–ﬂux relationship with re-
spect to the charge, i.e., RM(q) ≡df (q)/dq. Thus, any device whose resistance can be
characterized by RM(q) is classiﬁed as a current-controlled memristor. Memristance
values can be uniquely found by measuring the slopes, once the constitutive ϕ–q
relationship is deﬁned.
We assume that the ϕ–q relation of f (q) is monotonically increasing, and RM
is time invariant and dependent only on its internal state variable x, which incor-
porates memory effects and is controlled by input iM. Under these assumptions,
time-invariant memristive devices can be modeled by the following equations:
vM = RM(x) · iM
(11.2a)
˙x = r(x, iM),
(11.2b)
where ˙x = dx/dt and r(·) is an input controlled rate function of the state variable x.
As an example, let us consider a time-invariant current-controlled memristive de-
vice whose memristance is a function of x. If we assume that its RM(x) = x and
r(x,iM) = γ · iM, for simplicity, the rate function r(·) becomes the change rate of the
memristance itself, and the device can be modeled by:
vM = x · iM
(11.3a)
˙x = γ · iM,
(11.3b)
where γ can be set as a constant that depends on device parameters such as the carrier
mobility and device thickness [6].
Similar to the current-controlled memristor, a voltage-controlled memduc-
tor (short for memory conductor) can be deﬁned by a charge–ﬂux constitutive
relationship, q = g(ϕ). Taking time derivatives on both sides renders:
dq
dt = d
dt[g(ϕ)] =

 d
dϕ g(ϕ)

· dϕ
dt
(11.4a)
iM =

 d
dϕ g(ϕ)

· vM ≡GM(ϕ) · vM,
(11.4b)
where GM(ϕ) is termed a voltage-controlled memductance, the values of which can
be uniquely found by measuring the slopes of the charge–ﬂux relationship of g(ϕ),
i.e., GM(ϕ) ≡dg(ϕ)/dϕ. A time-invariant voltage-controlled memductive device can
be expressed as follows:
iM = GM(y) · vM
(11.5a)
˙y = l(y, vM),
(11.5b)
where y represents an internal state variable which incorporates memory effects.

184
S. M. Kang and S. Shin
Fig. 11.1 Example of transient v–i waveforms and corresponding charge (q) and ﬂux (ϕ) waveforms
of a memristor, where the memristor’s memristive operation regions, t = [t0, t1] and t = [t2, t3], are
plotted in solid lines and its resistive regions are depicted in dashed line
When we use the above constitutive relationship-based memristance representa-
tion, circuit models for memristors can be found in a compact model form. However,
when we consider that a memristor behaves memristively only within a bounded re-
sistance range between RMIN and RMAX, i.e., RM ∈[RMIN, RMAX], f (q) and g(ϕ) need
to be redeﬁned as f (ˆq) and g( ˆϕ) in a model space, because the constitutive relation-
ships are valid only in the memristive region, while the actual device can behave as
a linear resistor at the two boundary value points. Under the associated boundary
conditions of ˆq ∈[ˆqMIN, ˆqMAX] and ˆϕ ∈[ ˆϕMIN, ˆϕMAX], the modeling approach for
memristors can be summarized as follows, and the RM(ˆq) and GM( ˆϕ) can be easily
derived by taking slopes once f (ˆq) and g( ˆϕ) are found [6].
(a) Extract f (ˆq), or g( ˆϕ), from v–i measurements
(b) Extract RM(ˆq), or GM( ˆϕ), by taking their slopes
(c) Represent by circuit vM = RM(ˆq) · iM, or iM = GM( ˆϕ) · vM
11.2.1
Constitutive Relationship Between Charge and Flux
Since the memristor’s resistance range is limited, its model must observe the range
accordingly. Once the device resistance reaches one of the two boundary values, its
resistance will remain at the boundary value even when the excess charge or ﬂux is
applied to the device [3].
Figure 11.1 shows various time-domain characteristic behaviours of a memristor
with its limited resistance range, where the input current is patterned to transition to
+IO from −IO at t0 and back to −IO at t2. Assuming that the memristance starts
to change its value from RMAX at t0 and it eventually becomes equal to RMIN at t1,
the corresponding charge and ﬂux waveforms are shown in the right hand side of
Fig. 11.1, where the memristive and resistive operations are depicted in solid lines
and dashed lines, respectively. Regarding the ϕ–q relationship depicted in Fig. 11.2a,

11
Energy-Efﬁcient Memristive Analog and Digital Electronics
185
a
b
Fig. 11.2 A charge–ﬂux relationships: a actual relations corresponding to the waveforms in
Fig. 11.1, b memristive equivalent relationship with shifts and an introduction of window function
on iM
it can be noted that the piecewise ϕ–q functions, f 1(q) and f 2(q), of each memristive
region are fully correlated to each other even with shifts in q-axis and ϕ-axis, i.e.,
f 2(q) = f 1(q −qshift) + ϕshift. It means that a memristive charge and ﬂux ( ˆϕ −ˆq)
relation can be uniquely deﬁned as ˆϕ = f (ˆq) = fn(q −qshift) + ϕshift from any of
the piecewise memristive ϕ–q functions, as shown by the modiﬁed ˆϕ −ˆq relation
in Fig. 11.2b.
In order to ensure that the boundary limits are observed at the hard switching points
with particular boundary resistance values, the excess input current (or voltage)
in the resistive regions should be handled accordingly. In other words, the input
current (or voltage) should be masked in the model by a function (H) so that the
ˆq (or ˆϕ) can be always within the memristive regions of f (ˆq), whether memristors
behave memristively or resistively.A masked input current (ˆiM) of current-controlled
memristors is deﬁned as:
ˆiM = H(iM) =

iM,
if
RM ∈(RMIN, RMAX)
0,
else if
iM does not pass zero.
(11.6a)
Similarly a masked input voltage of voltage-controlled memristors can be deﬁned
as:
ˆvM = H(vM) =

vM,
if
RM ∈(RMIN, RMAX)
0
else if
vM does not pass zero.
(11.6b)
Equations (11.6a) and (11.6b) mean that any application of excess current (or volt-
age) in resistive region will be bypassed in the model space. The model equations
offer valid memristance values since no amount of excess current (or voltage) in the
resistive region will affect the memristance as long as the input polarity is not re-
versed. Once either of the boundary resistance values is reached, the boundary value
will persist regardless of the input amplitude for the same polarity. Memristor be-
havior will move back into its memristive region when the input polarity is reversed
such that ˆiM (or ˆvM) can reﬂect the input current (or voltage) of its memristive op-
eration. These phenomena are analogous to the behaviour of a charge reservoir with

186
S. M. Kang and S. Shin
limited capacity, which siphons excess charges beyond its capacity by draining cur-
rents beyond the limit. Interestingly, the masking function behaves exactly same as
an ideal diode, which clips the input potentials higher than its threshold value [6].
Equations(11.6a)and(11.6b)arecontainedbytheassociatedboundaryconditions
of ˆq ∈[ˆqMIN, ˆqMAX] and ˆϕ ∈[ ˆϕMIN, ˆϕMAX], the memristive charge and ﬂux, respec-
tively. Considering that any shifts of f (ˆq) and g( ˆϕ) along the ˆq-axis or the ˆϕ-axis do
not affect the memristance, we can choose any value for ˆqMIN and ˆϕMIN so that the
mathematical representations for f (ˆq) and g( ˆϕ) can be in simple forms. Once f (ˆq)
and g( ˆϕ) are chosen, with limited ranges of ˆq ∈[ˆqMIN, ˆqMAX] and ˆϕ ∈[ ˆϕMIN, ˆϕMAX],
the initial conditions for ˆq and ˆϕ which correspond to the initial memristance (rmo),
are determined such that RM(ˆqo) = rmo and GM( ˆϕo) = 1/rmo, respectively.
11.2.2
Properties of Memristors
Among the several properties of the memristive systems which were identiﬁed in [2],
the most distinct property is that the output vM (or iM) is always zero whenever the
input iM (or vM) is zero regardless of the internal state variables x (or y) and RM (or
GM). This property is manifested in the pinched hysteretic v–i characteristics for a
periodic balanced input. Also it is noteworthy that the memristor behaves as a linear
resistor in the limit of inﬁnite frequency at which there is no time allowed to respond
and produce resistance change during the short input period [1, 2].
The v–i characteristics of memristors under a periodic sinusoidal input, i.e.,
vM(t) = VO · sin(ωint), are shown in Fig. 11.3, where the pinched hysteretic loops
appear at low frequencies. They are collapsed to that of a linear resistor at high fre-
quencies. For a fully balanced periodic input, the memristance is also periodic over
the input period, since the injected amount of net charge or ﬂux over each period is
equal to zero. Also it can be noted that the range of memristance change is strongly
dependent on the input frequency and the amplitude, because the amplitude of the
ﬂux pattern is linearly proportional to the input voltage amplitude and almost in-
versely proportional to the input frequency. For voltage input with balanced polarity,
memristors have negligible memristance changes at high frequencies and thus can
be treated as static linear resistors. However, if the input polarity is unbalanced, the
memristance will change even at high input frequencies, since the net ﬂux will be
accumulated over time and eventually will affect the memristance. Its corresponding
v–i characteristics show that the memristance tends to keep increasing or decreasing
unless the input polarity is reversed, even at the relatively high input frequency [7].
11.3
Analog Circuit Applications of Memristors
Highly nonlinear properties of memristors have been investigated to create new
functions of analog circuits, such as to linearize nonlinear ampliﬁers [7]. Recently
ﬁne-resolution programmable analog circuits [8–10] have been reported, in which

11
Energy-Efﬁcient Memristive Analog and Digital Electronics
187
-
-
-
-
-
-
-
-
-
-
-
Fig. 11.3 Momentary v-i behaviors of memristors
the amount of memristance change can be uniquely controlled by precisely adjusting
the amount of charge or ﬂux across the memristor. Since the memristance can be
precisely controlled to any resistance value within the two boundary resistance val-
ues, the property of resistance programmability becomes very useful to build highly
programmable analog circuits with small parasitics.
11.3.1
Memristor as a Fine Resolution Programmable Resistor
In many high frequency circuits such as ampliﬁers and ﬁlters, resistors need to
be programmed adaptively for particular applications or to compensate for PVT
variations. The most popularly used method to realize the programmable resistors
takes the form of switch-controlled resistors composed of an array of weighted re-
sistors and switches. However, such method has a critical drawback since those
switches, typically MOS switches, inherently have large parasitic capacitances and
resistances. Furthermore the parasitic values are dependent on the switch state. The
large state-dependent parasitics limit the resolution and the number of bits that can
be implemented in switching resistors. Floating gate devices have also been ex-
ploited in the design of programmable resistors [11], where the charge (or voltage)

188
S. M. Kang and S. Shin
Fig. 11.4 Pulse coded
programmable resistor using
a memristor
on the ﬂoating gate can be stored and controlled through a combination of Fowler–
Nordheim tunneling and hot-carrier injection. Despite the attractions of analog scale
storage capabilities, the usage of ﬂoating gate structures has a number of practical
problems. In particular, programming and controlling of the charge amount on the
ﬂoating gate require high voltages allow electrons with sufﬁcient energy to tunnel
through the insulating oxide from/to the ﬂoating gate, which causes long-term reli-
ability problems. Another problem with ﬂoating gate devices is that their long-term
charge storage capabilities are unreliable. The charge stored on the ﬂoating gate may
slowly leak away over time, which will get worse as the process scales down with
reduced oxide thickness.
Due to the properties that the memristance can be programmed by controlling the
amount of charge ﬂowing or ﬂux across the memristor, memristors can be used to
realize ﬁne-resolution programmable resistors which are required for various circuit
applications to support multi-standards and also to compensate for PVT variations.
Although memristors behave as linear resistors for balanced high frequency inputs,
they can exhibit unique properties for unbalanced inputs [10].
In most differential circuit topologies, high frequency differential signals are usu-
ally balanced. In such cases, a memristor connected between differential signals
works as a static linear resistor with negligible periodic variations over periods of
differential signals. On the other hand, when connected across an unbalanced pro-
gramming path, the memristor can exhibit unique properties as long as they are
isolated from the balanced signal paths and can be separately biased. A bias cir-
cuit is required to have an inﬁnite impedance for the normal mode signal operation
to prevent any unwanted loading effect, and ideally zero impedance for the pro-
gramming duration so that the charges at the two end nodes of memristor can be
completely depleted to prevent unwanted charges from ﬂowing across the memristor
after memristance programming.
The programming resistors can be realized by patterning either the current in-
put waveform or the voltage input waveform. For brevity we will discuss only
the voltage input case. Figure 11.4 shows a bias circuit for memristance program-
ming with a voltage-controlled memristor [9, 10], where the simple series switches
(S) provide very high impedances (RS.OFF) during the normal mode operation and
low impedances (RS.ON) for the programming duration. However, the programmed
resistance of the circuit can suffer from any unbalanced ﬂux amount across the
memristor. In order to block any DC mismatch or other mismatches which can cause

11
Energy-Efﬁcient Memristive Analog and Digital Electronics
189
Fig. 11.5 Memristance versus number of pulses, where the pulse amplitude of V PT = 0.5 V and
the programming pulse frequency of ωPT = 10ωo
unbalanced ﬂux between the differential signals, two blocking capacitors (CB) are
used to isolate the even order mismatch effects. Any kind of odd order mismatches
will not hurt the differential balance by its nature. Even for the capacitor mismatch,
it does not contribute to the voltage imbalance unlike the case of the differential gain
[10].
For ease of programming and controllability, a pulse-coded memristor program-
ming method was used, where the memristance is programmed by patterning the
pulse waveform [10]. When a voltage-controlled memristor is considered, the ﬂux
can be linearly controlled by determining the number of pulses (NPULSE), duty ratio,
pulse amplitude (V PT), and pulse frequency (ωPT).
Figure 11.5 shows the simulated memristance characteristics over the number of
pulses when the programming input is a positive pulse train with controlled duty
ratio, where the pulse frequency of ωPT = 10 ωo, the pulse amplitude of V PT =
0.5 V, and the initial memristance of mo = RMAX are used, with the designed switch
resistances of RS.ON = ∼15  and RS.OFF = ∼130 M. It clearly shows that the
memristance is successfully programmed either by controlling the number of input
pulses or the duty ratio. The programming resolution can be estimated as:
dRM
dNPULSE
=
dRM
dϕ

·

dϕ
dNPULSE

,
(11.7)

190
S. M. Kang and S. Shin
where dRM/dϕ is found from memristance–ﬂux relationship and dϕ/dNPULSE is di-
rectly dependent on the input pulse pattern which must be linearly proportional to
the duty ratio and the pulse amplitude for a given pulse frequency.
dϕ
dNPULSE
= Duty · VPT · 2π
ωPT
(11.8)
In fact, the programmed memristance will not change its value even under PVT
variations since the memristor will be isolated from the pulse generator during the
normal mode operation after programming. However, during the programming pe-
riod, the memristance behavior can be affected by the PVT variation, since the pulse
amplitude (V PT) and the duty ratio among the three parameters will ﬂuctuate due
to variations. Here, it is noteworthy that we can control any of the three parame-
ters and even ﬁx the all parameters as constants. For example, when we use a ﬁxed
50 % pulse duty under regulated amplitude and ﬁxed frequency, the variation of res-
olution will be affected by the duty variation only. Thus, it can be ignored since the
amount of ﬂux variation due to the duty variation from 50 % must be much smaller
than the injected ﬂux amount per each pulse cycle. Even a 10 % variation from the
50 % duty will result in a resolution variation of 0.2 × LSB. A closed-loop control
can be applied to eliminate the effect of small duty variations. However, it would
not be beneﬁcial since the circuit overhead will be excessively large compared to the
achievable gain. On the other hand, the required number of pulses needed to program
the memristance to a certain value can be subject to process variations, since any
process induced shift of device’s physical parameters, such as RMAX and RMIN, may
vary dRM/dϕ and thus dRM/dNPULSE.
In order for a pulse programmed memristor to be practically applicable, its initial
memristance should be set to a certain known value before programming, since
the memristance change is strongly dependent on the initial state of the device. An
easy way is to set the initial memristance of mo to either boundary memristance
RMAX or RMIN by applying negative or positive pulses, respectively, prior to the
programming. Since the initial memristance can be any value between RMAX and
RMIN, the initialization process may need to change the memristance at most from
one boundary to the other boundary value, i.e., RM.init.MAX = RMAX −RMIN, which
corresponds to the minimally required initialization ﬂux amount of ϕinit regarding
the RM −ϕ relationship. With dϕ/dNPULSE of (11.8), the minimum number of pulses
for the initialization (NINIT) can be estimated as
NINIT ≥
ϕinit
(dϕ/dNPULSE) =
ϕinit
Duty · VPT · (2π/ωPT).
(11.9)
The most distinct feature of this memristor-based programmable resistor is that the
resolution step of resistance can be adjusted with V PT, duty ratio, and ωPT. It should
be noted that the programmable resistor has minimum parasitics regardless of its pro-
grammed resistance, since it includes only the two small size switches, which means
that the circuit will minimally degrade the intended performances while providing
a wide range of programmable resistance. For a designed circuit with a 0.18 um

11
Energy-Efﬁcient Memristive Analog and Digital Electronics
191
Fig. 11.6 Circuit schematic of programmable gain ampliﬁer using a memristor, where RL is the
equivalent resistance of the pulse programmed memristance
CMOS technology, it adds constant parasitic switch capacitances of CS.ON = 120 fF
for programming duration and CS.OFF = 50 fF for normal mode operation.
The resistance programming with ﬁne resolution and small parasitics is very use-
ful in many mid-band and RF range differential circuits. By utilizing the nonvolatile
analog memory capabilities of memristors through hybrid integration with CMOS
active circuits under low programming voltages, the overall circuit performances and
reliabilities can be improved. Memristor-based programmable resistors can be used
for programmable attenuators, programmable gain ampliﬁers, and programmable ﬁl-
ters, among others. Also they can be used to compensate for environmental changes
such as the temperature and low frequency interferences by generating pulse patterns
and modulating the memristance according to the environmental signals. In addi-
tion, they can be used potentially as a resistance modulator, and also an amplitude
modulator when used as a load resistor of an ampliﬁer [10].
11.3.2
Programmable gain ampliﬁer
A circuit schematic of a simple programmable gain ampliﬁer designed with a 0.18 um
CMOS process is shown in Fig. 11.6. With a high output impedance of the cascode
ampliﬁer, thetotaloutputresistanceismadeprogrammablebyusingamemristor. The
memristor-based programmable load resistance is controlled by a coded pulse train
to adjust the memristance and ultimately the ampliﬁer’s gain. With the memristor-
based load resistance (RL), the ac voltage gain is easily programmable by controlling
the ﬂux amount across the memristor. From the equivalent circuit shown in Fig. 11.7,

192
S. M. Kang and S. Shin
Fig. 11.7 Mid-band
equivalent circuit of Fig. 11.6,
where Ci and Co are parasitic
input and output
capacitances, respectively
vin
ro
gm
RL
vout
Co
Ci
where Ci and Co are parasitic input and output capacitances, respectively, a mid-
band voltage gain can be estimated as Av = gm(ro//RL), neglecting the high frequency
effect of Ci and Co, where gm is the differential transconductance of M1 and M2,
and ro is the ampliﬁer’s output impedance formed by M1∼M4. For gm = 2.4 mS
and ro = 41.3 k, the calculated voltage gain (Av) is 32.34 for a load resistance of
20 k (and Av = 0.24 for RL = 100 ).
Considering the limited switching speed of memristors (1/f o), the memristor
driving frequency is required to be lower than f o to fully switch the memristor
between ON and OFF states. When the driving frequency is much higher than f o,
the memristor will behave as a linear resistor holding its resistance regardless of the
amplitude of the balanced input voltage. For high enough operating frequencies (f in),
i.e., fin ≫fo, the frequency characteristics of the memristor-based programmable
differential circuits are not directly related to the switching speed of the memristor,
since the memristor will behave as a linear resistor for the high frequency input
signal. Since about 10 × fo of input frequency is high enough to be assumed as a
linear resistor, it can be said that the lower bound of the operating frequency of our
programmable circuits is roughly 10 × fo.
Simulated ac voltage gain with an inclusion of memristor-based programmable
load resistor is shown in Fig. 11.8, where a differential signal frequency (f in) of
100 × f o and the programming pulse frequency (f PT) of 10 × f o were used. The
achieved voltage gain characteristics well match the estimated gain values, showing
that the memristor-based programmable resistor can ﬁnely program the ampliﬁers’
gain. While the memristor-loaded ampliﬁer circuit shows a ﬁne gain resolution over
the wide tuning range under low voltage programming pulses, other performances
such as linearity and speed are exactly the same as those of the cases with linear load
resistors. This circuit becomes more advantageous when the application frequency is
in the RF range, since the blocking capacitors can be integrated together with CMOS
active devices for higher input frequencies.
11.4
Ultra-Dense Nonvolatile Memory Applications
Bi-stable memristive devices are most promising for digital resistive memory appli-
cations as they provide two distinct resistance states, namely the ON state resistance
(RON) and the OFF state resistance (ROFF). In the WRITE mode of a binary Resistive
Random Access Memory (RRAM) array, the bi-stable memristive devices are pro-
grammed to either RON or ROFF by applying an unconditional SET or RESET voltage

11
Energy-Efﬁcient Memristive Analog and Digital Electronics
193
Fig. 11.8 Simulated ac voltage gain of the programmable gain ampliﬁer versus number of pulses,
where the differential input frequency of ωin = 100ωo, pulse amplitude of V PT = 0.5 V, and the
programming pulse frequency of ωPT = 10ωo
Fig. 11.9 a Readout circuit
conﬁguration for a single cell
passive crosspoint RRAM,
and b its equivalent circuit
a
b
across the device to be written. Once programmed, with a readout voltage lower than
the unconditional SET voltage applied across a speciﬁc memory cell to be read,
its resistance state can be determined by the sensed voltage across the correspond-
ing sense resistor of the selected cell. For the READ mode of a single cell RRAM
whose readout circuit conﬁguration and its equivalent circuit are shown in Fig. 11.9,
the readout voltage (vR) is applied to initiate a current ﬂow through the cross point
memory device and produce a sensed voltage (vS). For a single cell memory, the
optimal RS value is the geometric mean of the bi-stable resistances of RM which

194
S. M. Kang and S. Shin
maximizes the sensed voltage difference of the two possible status of RM (RON for
data “1” and ROFF for “0”). This criterion can be also applied for large size array
structures by using cell selection devices such as 1T-1R structure [12]; the selection
devices uniquely select the target cell to be read while keeping others electrically
isolated from the readout network.
A passive RRAM array that does not require cell selection devices will have a
higher area density and lower readout power consumption. However, such passive
structure is highly vulnerable to undesirable interference currents, so-called ‘sneak
currents’ [13–16], that ﬂow through adjacent cells. The sneak currents are strongly
dependent on the resistive states of all adjacent cells. In other words, the sensed
voltage detection window for data determination, the ‘detection margin’, is highly
dependent on the pattern of data distribution, in particular the probability distribu-
tion of data “1”, throughout the whole passive array. In order to correctly estimate
the detection margin, the data pattern should be rigorously analyzed with compu-
tationally efﬁcient simulation. Since the computational efﬁciency can be degraded
quadratically with the array size [16], new approaches need to be devised.
The dependency of detection margin on data distribution has been reported in [17].
A worst-case circuit model regarding the data pattern of resistive arrays was reported
in [15] in 1969. This model is valuable for worst-case estimation of the detection
margin. The worst-case model, however, can cover only the cases with the data
probabilities of either 0 or 1. And the worst-case design can yield excessive current
consumption in non-worst cases. For general cases with a random distribution of the
stored data, an analytical circuit model with a high computational efﬁciency has been
reported in [16]. By adopting a nodal analysis method similar to that used in [18]
to characterize active memristive RLC circuits, [16] introduced a data-dependent
statistical circuit model for passive resistive memory arrays that can handle all data
distributions to determine data-dependent optimal design parameter values. Besides
the data-dependent statistical memory model, this Section introduces memory archi-
tectures with optimum sense resistors which are self-adaptable to the data pattern so
that optimum detection performance can be achieved regardless of the stored data
pattern. The self-adaptable optimum sense resistors offer both low power and large
detection margin. While design optimizations can be done for multiple objectives:
maximization of the output Signal-to-Noise ratio (SNR) [15], highest output voltage
detection margin, or any other ﬁgure-of-merit such as SNR per power consumption,
this Section focuses on the voltage detection margin as a demonstration case.
11.4.1
Statistical Model for Passive Memory Array
A generalized n × m dimensional passive resistive random access memory (RRAM)
array is shown in Fig. 11.10, where n and m respectively stand for the numbers of
rows and columns of the memory array [16]. For the case of reading the (k, l)th cell,
the corresponding row index (i) and column index (j) are respectively categorized
into two different sets for modeling purposes; IU for the set containing all n-number

11
Energy-Efﬁcient Memristive Analog and Digital Electronics
195
Fig. 11.10 A readout circuit conﬁguration of an n × m passive crossbar structure of resistive
memory, where a (k, l)th cell is to be read
of row indices; Ik = IU −{k} for all row indices not to be read; JU for the set
containing all m column indices; Jl = JU −{l} for all column indices not to be read.
With these index sets, the n × m passive memristive cell array is divided into four
different subsets as shown in Fig. 11.10. GI includes only the cell to be read (gkl),
and the elements of GII and GIII are all cells in the reading row and reading column,
respectively, excluding the reading cell. GIV includes all other cells not belonging to
GI through GIII. The reading voltage (vR) is applied to the kth-row and the voltage
across the lth–column RS, vS(k).l, is sensed to read the stored data in the (k, l)th cell.
The detection margin (vS(k).l) is deﬁned as the normalized difference of vS(k).l for
two possible data (“0” or “1”), where the normalization is done with respect to vR.
The wire parasitics are ignored in this analysis, and admittances (gij) instead of
resistances (Rij) are used for simplicity.
Electrical behaviors of the resistive network can be described with the following
m nodal equations for every j-index for the n × m dimensional RRAM array:
⎛
⎝gs +

i∈IU
gij
⎞
⎠· vs(k)·j = gkj · vR +

i∈Ik
gij

y∈JU
giy

u∈IU
(giu · vs(k).u)
(11.10)

196
S. M. Kang and S. Shin
Putting together the available m equations of type (11.10) for all j-indices forms an
m × m dimensional matrix equation:
GP · Vs(k) = Ga(k) · vR + Cx(k) · Vs(k),
(11.11)
where the elements of sub-matrices are described as follows.
(a) Output sensed voltage matrix (m × 1)
Vs(k) = [vs(k).1, vs(k).2, . . . , vs(k).m]T
(b) Parallel admittance matrix with an identity matrix (I)
GP = gs · I + diag
⎛
⎝
i∈IU
gi1,

i∈IU
gi2,
. . . ,

i∈IU
gim
⎞
⎠
(c) kth-row admittance matrix (m × 1)
Ga(k) = [gk1,
gk2,
. . . ,
gkm]T
(d) Crosstalk matrix that describes sneak currents
CX(k) =

i∈Ik
Xi,
where the cross association (via ith–row) matrix is
Xi =
1

y∈JU
giy
· [gi1, gi2, . . . , gim]T · [gi1, gi2, . . . , gim].
GP is an m × m matrix whose jth diagonal element (gP.jj) is equal to the sum of all ad-
mittances directly connected to the jth output node (vS(k).j). For example, the elements
of GP for 2 × 2 array are gP.11 = gS + g11 + g21 and gP.22 = gS + g12 + g22, which
will remain constant over the reading row index (k). Hence, Ga(k) is an m × 1 matrix
whose jth element (ga(k).j) is equal to the jth column admittance of the kth reading
row (gkj), and will eventually determine the sensed outputs according to the stored
data. Xi represents the possible cross association through the ith row. Since the
problematic sneak currents can go through all resistors of non-reading rows (i ̸= k),
the number of Xi matrices participating on the combined crosstalk matrix (CX(k)) is
equal to (n −1) for the n × m array.
From (11.11), the output sensed voltages can be uniquely determined by solving
it for VS(k):
Vs(k) = [GP −Cx(k)]−1 · Ga(k) · vR
(11.12)
Figure 11.11 shows optimally designed readout voltages of a 128 × 128 array for
various data probabilities (P1), where the device parameter of α (≡ROFF/RON) of
103 was assumed with ROFF = 10 M. Figure 11.11 shows clearly that the readout
performance is highly sensitive to the data pattern. Henceforth the detection margin

11
Energy-Efﬁcient Memristive Analog and Digital Electronics
197
Fig. 11.11 Normalized sensed voltages of a 128 × 128 array for different probabilities of data “1”
(P1), with device parameters of α = ROFF/RON = 103 and ROFF = 10 M. For each P1, the sense
resistance was chosen to maximize the detection margin
is deﬁned as a measure of the sensed voltage difference between the two different
data types (Dkl) of the (k, l)th cell, i.e.,
vs(k).l ≡
vs(k).l

(Dkl=1) −vs(k).l

(Dkl=0)
 .
(11.13)
An approach for statistical simpliﬁcation of complex multi-nodal equations into a
computationally efﬁcient 2 × 2 equivalent model is to reduce (11.10) with statisti-
cal approximations on the data pattern dependent parameters. Let pII, pIII, and pIV
represent respectively the probabilities of data “1” for GII, GIII, and GIV. Under the
assumption that the stored data are mutually independent and uniformly distributed
within their group, the expected value of any cell admittance in Group-N (GN) is
estimated as E

gij ∈GN

= ¯gN = pN ×gON +(1−pN)×gOFF, where pN stands for
the probability of data “1” of GN. With the parameter α = ROFF/RON, the expected
value becomes ¯gN = gOFF × [1 −pN · (1 −α)]. For statistical approximation, we
assume that the array size is large, i.e. n ≫1 and m ≫1, so that the sample means of
the random variables (˜vs and ˜gN) are approximately equal to their statistical expected
values (¯vs and ¯gN).
Recalling (11.10), the jth-equation of m base equations, a simple way to reduce
the model dimension from m×m to an equivalent 2×2 is to sum the (m −1) equations
of type (11.10) for all j ∈Jl, and to take (11.10) for j = l as the second equation [16].
Then further simpliﬁcation of the two base equations will yield a 2 × 2 dimensional
matrix equation in the exactly same form as (11.11), where the sub-matrices are the

198
S. M. Kang and S. Shin
Fig. 11.12 A 2 × 2
equivalent data-dependent
statistical circuit model for
n × m passive array, where
the circuit parameters are
functions of the array size and
data distribution
following forms:
GP =
(ge·S + εe·S) + ge·IV + ge·II
0
0
gS + ge·III + gkl

,
Ga(k) =
ge·II
gkl

,
Vs(k) =
 ¯vs(k)
¯vs(k).l

,
and
CX(k) =
1
ge·IV + ge·III
·
ge·IVge·IV
ge·IIIge·IV
ge·IVge·III
ge·IIIge·III

.
(11.14)
Based on (11.11) and (11.14), a 2 × 2 equivalent circuit model for a generalized n × m
passive array is derived as shown in Fig. 11.12. For further simplication of analysis,
the covariance related term εe.S can be ignored for such that (ge.S + εe.S) ≈ge.S,
although it can be highly dependent on the data distribution pattern [16]. With the
2 × 2 equivalent data-dependent circuit model, now we can handle any array size
with computational efﬁciency to compute its optimum design parameter values and
analyze its behavior for various data distributions.
Equation (11.12) with its sub-matrices deﬁned as in (11.14) provides the output
sensed voltage for any data pattern. In order to verify the 2 × 2 equivalent model with
an approximation of εe.S →0, its estimated performances are compared with those
of simulated results without simpliﬁcation for a 128 × 128 array. For several sets of
data generated with uniform data probabilities (pII = pIII = pIV = pII, III,IV), Fig. 11.13
compares the normalized voltage detection margins of the 2 × 2 equivalent memory
model with the simulated results for α = 103, where the normalization was done with
respect to vR. The line curves represent the 2 × 2 model analysis while the symbols
represent simulation results. Two results matched closely with negligible differences,
showing that aforementioned assumption (εe.S →0) and the statistical approxima-
tions are acceptable. While there are small errors for small data probabilities (pI, III,IV)

11
Energy-Efﬁcient Memristive Analog and Digital Electronics
199
-
Fig. 11.13 Normalized detection margin comparison: 2 × 2 equivalent model (line-curves) vs.
simulation (symbols), where n = m = 128 and α = 103
caused by the relatively large ratios of εe.S/ge.S, almost perfect matching was achieved
for large probabilities due to small εe.S/ge.S ratios.
11.4.2
Self-Adaptable Optimum Sense Resistance
The optimum detection performance is obtained for the optimum β-ratio (βopt) whose
value is strongly dependent on the stored data-pattern throughout the whole array,
and thus the optimum performance may not be obtained unless βopt is adaptable to
the data-pattern. Regarding the βopt dependency on the data pattern, it is observed
that the normalized values of βopt with respect to the given process parameter of
α-ratio, βopt/α, are linearly dependent on the data probability, implying that βopt is
linearly dependent on the α-ratio.
A self-adaptability of sense resistance (RS) can be achieved by composing RS with
a replica of a part of the RRAM array, since the replica array can statistically describe
the data-dependent resistance. Though the replica-based RS conﬁguration increases
the memory area, it still has a good area efﬁciency since RS can be imbedded as
ultra-dense crosspoint devices. To further reduce the area overhead for RS, a part of
non-reading RRAM rows, instead of the replica, together with additional dedicated
ROFF rows, can realize RS, as shown an example circuit composition in Fig. 11.14.

200
S. M. Kang and S. Shin
Fig. 11.14 A circuit
conﬁguration for
self-adaptable sense
resistance, where the sense
resistance is composed of
x-number of non-reading
RRAM rows and y-number of
dedicated ROFF rows with
grounded terminations
Since the part of RRAM rows with ground termination reduces the number of non-
reading RRAM rows that contribute to the sneak currents, it improves the detection
performance further. Also adaptable βopt self-adaptable to the stored data-pattern can
be achieved with minimized area penalty for the dedicated ROFF rows. Indices of the
x-number of non-reading RRAM rows can be selected randomly or for the nearest
indices from the reading row index.
Anexemplaryconﬁgurationoftheself-adaptableRS designusingapartofmemory
array is described for the 128 × 128 passive RRAMs whose data-dependent desired
βopt is approximately linear with βopt.min = 0.04 × α and βopt.max = 5.46 × α, for
all possible α-ratios. Assuming that the x-number of RRAM rows and y-number of
ROFF rows are involved for the sense resistance (RS = 1/gS), as depicted in Fig. 11.14,
the expected gS( ¯gS) can be represented as ¯gS ≈x · ¯gD + y · gOFF, where ¯gD stands
for the data-dependent expected RRAM cell admittance with its value of ¯gD =
[1 + (α −1)p1] · gOFF. Thus the expected value of the β-ratio, ¯β, can be written as:
¯β = x · (α −1) · p1 + (x + y).
(11.15)

11
Energy-Efﬁcient Memristive Analog and Digital Electronics
201
Fig. 11.15 Performance
enhancement of the
self-adaptable design over the
worst-case design, for
128 × 128 RRAM array with
α = 103
The numbers x and y are determined as follows such that ¯β to be approximately equal
to βopt:
x = βopt
α −1 ≈βopt
α
and
y = (βopt.min −x),
(11.16)
which correspond to the nearest integer numbers of x = 5 and y = 35 for the process
parameter of α = 103. With the estimated x- and y-values, the self-adaptable RS in
Fig. 11.14 was comprised of grounded 5 non-reading RRAM rows and dedicated
35 ROFF rows.
For simulation and comparison of results, many sets of random data-patterns
were independently generated for various data probabilities and then the randomly
selected 5 non-reading RRAM rows were switched to the ground, along with 35 ROFF
rows, to form an adaptable RS. For worst-case design, 40 grounded ROFF rows were
used to form RS. Percent performance enhancements with the self-adaptable design
over the worst-case design are shown in Fig. 11.15. While the overall enhancements
for small data probabilities are close to the theoretical limits, there are additional
improvements, in particular for large data probabilities. Since the self-adaptable
design uses the grounded part of the RRAM array, instead of a replica, thereby
reducing the number of rows participating on the sneak currents, the performances
can be improved further above the theoretical limits estimated for the replica-based
adaptable RS. In consideration of equally probable data probabilities within [0, 1],
the average improvements of the detection margin and the current consumption are
statistically projected as 46 and 14 %, respectively.
11.5
Stateful Logic Gates
Stateful logic is another important application of memristive devices which func-
tion for both logic and latch operations [19–23]. Since these memristive devices can
implement memory, programmable interconnects, and logic operations and latches,

202
S. M. Kang and S. Shin
Fig. 11.16 Basic operations
in stateful logic: a material
implication performed by two
simultaneous voltage pulses
VCOND and VSET, applied to
memristive switches P and Q,
respectively, b a truth table
for material implication, c a
NAND operation performed
by a three-step schedule, d a
schedule to execute a NAND
operation
a
b
c
d
with ultra high integration density together with CMOS devices, the memristive
device technology structured in a nanowire crossbar array has emerged as an intrigu-
ing technology that can allow Moore’s Law to continue overcoming the barriers of
CMOS technologies due to lithographical limitation and power density limit among
others. One of the most plausible architectures for hybrid integration of memris-
tive nano devices and CMOS devices is the CMOS/Molecular hybrid (CMOL) [4],
which can be implemented by using Field Programmable Nanowire Interconnect
(FPNI) [5]. This is based on hybrid circuits composed of a conventional CMOS
layer connected to multiple crossbar layers that contain memristive devices. With
its inherent data-latching property, stateful logic can effectively implement low-cost
fully pipelined digital systems [23], and therefore holds potential for implementing
high performance next generation nano computers. Also, the latches with embed-
ded memristors can provide a power shutoff mode with non-volatility against power
failures.
However, the memristive stateful logic structure holds several technical issues
that must be resolved to effectively complement CMOS logic. The challenging issues
are on power consumption, operation speed, multiple fan-in/fan-out capability, and
two- or three-dimensional integration for large-scale computations [21]. This section
discussesthefeasibilityofnanowirecrossbarmemristivedevicesascandidatedevices
to implement a general-purpose computation.
11.5.1
Basic Operations in Stateful Logic
Let us consider a circuit shown in Fig. 11.16a to illustrate a material implication
operation in stateful logic [20], where two memristive devices P and Q are connected
by a common horizontal nanowire to a load resistor RG. The states of P and Q are

11
Energy-Efﬁcient Memristive Analog and Digital Electronics
203
represented by p and q, respectively, and logic 0 (1) corresponds to the open (closed)
state of the switch. A switch is set to logic 1 (0) by applying a negative (positive)
voltageVSET (VCLEAR) pulse through its corresponding voltage driver.AVCOND pulse
that is applied to a switch which is previously set to logic 1 (0) prevents (allows)
a state change of the other switch which is concurrently driven by a VSET pulse.
This conditional set operation enables the circuit to function as a gate implementing
material implication as shown in Fig. 11.16b.
A NAND operation is executed through three sequential steps as illustrated in
Fig. 11.16d. The inputs are states p1 and p2 stored in switches P1 and P2, and the
output is state q′′ accumulated in switch Q. Initially, a VCLEAR pulse is applied to
switch Q to execute q = 0. The second step q′ = ¯p1 + q = ¯p1 is performed by
applying a VCOND pulse to VP1 concurrently with a VSET pulse to VQ. Finally, the
operation q′′ = ¯p2 + q′ = ¯p2 + ¯p1 = p1p2 is executed by concurrently applying
VCOND and VSET pulses to VP2 and VQ, respectively. Since the NAND operation is
known to be universal, any Boolean logic operation can be constructed by a network
of NAND gates.
11.5.2
Issues for Large-Scale Logic Integration
However, severaltechnicalissuesariseindesigningmemristivedevicesbasedstateful
logic architecture:
Firstly the stateful NAND gate requires deep pipeline steps for logic execution,
as shown in an example where three dedicated steps are required for a two-input
NAND operation shown in Fig. 11.16d. The pipeline steps are not dependent on
the circuit size, but determined by the maximum fan-in of the constituent NAND
gates. If this dependency is removed, and the multiple implication operations can be
executed concurrently in a single step, the performance—the data latency as well as
the pipeline period—can be signiﬁcantly reduced.
Secondly, a simultaneous execution of multiple logic calculations is a basic re-
quired capability of large scale logic arrays. When there are functionally disjoint
logic cells, they should be executed in parallel so that the total required time for
computation can be reduced. However in the stateful NAND gate of Fig. 11.16c with
shared common horizontal nanowire and load resistance (RG), every one-to-one im-
plication process should be non-overlapped in time and thus very deep pipeline steps
are needed for large scale logics. A proper switch based array conﬁguration with
isolated logic gates can perform concurrently multiple logic operations.
In stateful logic, resistance of a memristive device is used as the physical state
variable. Also, the memristive switches involved in an operation need to be isolated
fromthoseinotheroperations. Thisconstraintprohibitsalogicstatefromfanningout.
Therefore, each logic state should be explicitly duplicated by a dedicated operation.
In order to integrate a complex logic function with high density, two- or three-
dimensional circuit conﬁgurations that can be mapped onto the FPNI fabric are
favored instead of the one dimensional devices array.

204
S. M. Kang and S. Shin
a
b
Fig. 11.17 a A conﬁguration of k-input NOR operation that executes multiple implications in a
single-step (Step-2), and b its equivalent circuit for Step-2
11.5.3
Reconﬁgurable Stateful Logic Gates
In consideration of aforementioned technical issues, a CMOS/memristors hybrid
structure of NOR function is proposed to execute multiple implications in a single-
step [21]. Any Boolean logic operation can be constructed by a network of NOR
gates since NOR operation is also universal. Figure 11.17a shows a proposed cir-
cuit conﬁguration for two-step NOR operations where k + 1 number of memristive
devices are connected in parallel with a shared common horizontal nanowire and a
load resistor RG. Devices P1, P2, . . . , Pk are input memristive switches with logic
states p1, p2, . . . , pk, respectively, and the Q is the output device with its logic state q.
Instead of composing a network of multiple two-input NAND gates to form a k-input
NOR operation, this circuit is devised to execute q′ = p1 + p2 + · · · + pk in two
steps. After q is cleared by applying a VCLEAR pulse to VO in Step-1, a single-step
simultaneous execution of multiple implication processes is made enabled in Step-2
by applying properly conditioned VCOND pulse simultaneously to all input switches
and concurrently applying VSET to the device Q. VCOND and VSET for the condi-
tional set operations of the single-step NOR executions can be analyzed by using an
equivalent resistor network model shown in Fig. 11.17b.
However, since more than two parallel connected memristive switches are in-
volved in the NOR operation, the load resistance needs to be scaled by the number
of inputs so that the voltages across the memristive switches are properly maintained
within the valid ranges for all possible input fanning-ins. The scaled equivalent load
resistance will lower the required levels of conditional set voltages (VSET andVCOND)
and desensitize the dependency of the voltage conditions on the input fan-in. The
circuit construction becomes simple if a load resistor is attached to each memristive
switch as shown in Fig. 11.18. Here, the effective load resistance is inversely pro-
portional to the number of inputs, and the logic computation always uses all load
resistors unlike that the sequential implications in [20] use only one shared load
resistor.

11
Energy-Efﬁcient Memristive Analog and Digital Electronics
205
Fig. 11.18 A circuit
modiﬁcation for multiple
fan-in capability, where load
resistors are dedicated to all
participating logic devices so
that the equivalent load
resistance can be
automatically scaled by the
number of fanning in
Fig. 11.19 Two-dimensional
logic construction of k-input
NOR operation for
large-scale integrations
Based on the logic structure of Fig. 11.18, a generalized hybrid type of two-
dimensional circuit structure is devised as shown in Fig. 11.19 so that a group of the
logic units can be easily mapped onto the FPNI fabric. CMOS and nanowire inter-
layer vias to blue (red) horizontal (vertical) input (output) nanowires are marked by
⊗(⊕). Green wires are connected to voltage drivers through the vias marked by ⊕.
It is composed of four different devices groups: memristive logic devices (P1, P2, . . . ,
Pk, and Q) for input and output; memristive interconnect switches (I11, I21, . . . , and
Ik1) for logic conﬁguration; CMOS switches (SI and SO) for implication scheduling;
dedicated load resistors (RG). Each load resistor can be implemented by using either
correctly biased CMOS transistors or multiply stacked nanowires. Note that the
functionality can be reconﬁgured solely by the interconnect crossbar switches on the
nanowire layer and no reconﬁguration is required in the CMOS layer. Since every
logical value is latched in memristive logic devices, this architecture can operate
as a pipeline [23]. In other words, the pipelined architecture which can execute the
corresponding computation at a constant rate independent of the system size can be
devised by using the inherent data-latching property of memristive switches. The
pipeline architecture can be easily mapped to the fabric as a two dimensional array

206
S. M. Kang and S. Shin
of columns. In a column, a tri-state voltage driver (VI) is shared by multiple unit
cells each of which consists of a memristive switch, a load resistor, and a CMOS
control switch. The units in a column of the pipeline alternate between the write
mode in which a new state is registered, and the read mode in which the state is read
to determine the state at the next column. Any two neighboring columns are executed
in different modes each other.
11.5.4
Circuit Simulation Results
The single-step NOR execution is validated for large input fan-in by using a Verilog-
A compact model for the memristive devices. For larger voltage pulse levels than
VOPEN (VCLOSE), devices’ resistance states switch with the minimum pulse width of
10 nanoseconds. Nine transient analyses for an 8-input NOR gate of Fig. 11.19, each
with n = 0, 1, . . . , 8, respectively were performed, with devices’ switching thresh-
olds of VOPEN = 1 V and VCLOSE = −1 V. The analyzed conditional set voltages of
(VSET, VCOND) = (1.7 × VCLOSE, 1.2 × VCLOSE) and VCLEAR = 1.6 × VOPEN with
an adaptive β-ratio (2 for Step-2 and 200 for Step-1) are used. The pulse widths of
voltage drivers are all 20 nanoseconds which is twice the minimum requirement for
a state switching.
Figure 11.20a shows the applied voltages for Steps-1 and 2 where thin blue and
thick red pulses respectively display VI and VO pulse. All results for Step-1 are
displayed in dotted lines while those for Step-2 are in solid waveforms. The input
states of p1 through pk are programmed before each Step-1 process such that n = 0, 1,
2, . . . , and 8, sequentially. However for clarity, only the transient results of Step-1 and
Step-2arecompiledintothegraphs.Asshowntheequivalentinputresistance(RP−EQ)
in Fig. 11.20b, the programmed input states are well maintained, not affected by the
subsequent conditional set processes. This can be validated by the well conﬁned
small enough voltage drops across input devices (VP) in betweenVCLOSE andVOPEN,
shown in thin blue lines in Fig. 11.20d, for all possible cases. While the voltage
drops across the memristive interconnecting devices, shown in Fig. 11.20c, are also
maintained with less than 120 mV far below the VOPEN, and that of the output device
(VQ) exceeds VCLOSE only when n = 0 as shown in Fig. 11.20d, where the smallest
VQ difference between the cases of n = 0 and n ̸= 0 is larger than 0.5 × VCLOSE. The
conditional VQ transition successfully yields with a large margin the 8-input NOR
operation in a single-step by setting q′ = 1 only for n = 0 from its initial state of q = 0.
The resulting transient output resistance (RQ) behaviors are shown in Fig. 11.20e,
where the resistance switches to RON from ROFF only for n = 0.
FunctionalitiesandpotentialsofthestatefulNORgatehavebeenveriﬁed, however
further investigations on power consumption, operating speed, reliability, and others
are needed for the stateful logic to be comparable to or much more beneﬁcial than
its CMOS counterparts.

11
Energy-Efﬁcient Memristive Analog and Digital Electronics
207
Fig. 11.20 Transient
simulation results of an
8-input NOR gate for all
possible input state
combinations: a voltage
pulses applied to VI and VO,
b equivalent resistance of the
input memristive switch
network, c voltage across the
interconnecting memristive
devices, d voltages across the
input and the output
memristive switches, and
e resistance of the output
memristive switch (RQ)
n
q
n
q
n
a
b
c
d
e
11.6
Summary and Future Memristive Electronics
In this chapter we discussed recently reported memristor applications to pro-
grammable analog circuits and systems as well as digital electronics such as
nonvolatile memories and stateful logics. Continuing research is needed to develop
highly reliable memristive system architectures, such as dynamically reconﬁgurable
fault-tolerant memories, and to overcome the variability and reliability issues of
memristive nanodevices. For dependable memory operation, faulty cells should be
automatically rewired or corrected. By the capabilities of nano-interconnects, non-
volatile memories, and stateful logic gates, with reconﬁgurability, memristors can be
applied to realize self-adaptable and self-repairable memories that can monitor and
search states and reconﬁgure around faulty connections. A long-term goal is to ﬁnd
universal nano architectures of intelligent memory and storage, providing optimal
memory performance for low-power and highly capable ﬂexibility.

208
S. M. Kang and S. Shin
In addition to intelligent memories, memristive technology is anticipated to yield
new revolutionary analog/digital functions by exploiting the nonlinear properties of
memristors, and supplement and replace transistors with memristors for latching or
switching circuits, such as FPGAs, which will signiﬁcantly reduce the form factor,
manufacturing cost, and active/leakage power consumption. One potential example
is the dependable memristive logic array, for which basic architectures and appropri-
ate circuit compositions for reconﬁgurable resistive logic array can be investigated
with particular attention to the technical barriers such as difﬁculties for multiple
fan-in/fan-out capability and functional robustness. The memristive logic array can
effectively implement low-cost fully pipelined digital systems, and therefore holds
great potential for implementing low-power and high performance next generation
resistive nanocomputers. The memristive technology enabled low-cost, nonvolatile,
solid-state circuit technology will signiﬁcantly reduce the energy cost associated
with all other electronic systems.
It is also anticipated that a reliable low-power hardware platform for truly de-
pendable nanocomputing systems will help Moore’s Law to be extended far beyond
the barriers currently observed in CMOS technologies. In addition to the barriers
to Moore’s Law, such as the scaling limit and the power wall, future computers for
complex and advanced information processing must overcome the severe connec-
tivity issues. Since the power consumption for communication can take more than
90 % of the entire computing power, the computer architectures with co-location
of memories and processors on a chip becomes preferable. With the co-location of
memories and processors, the heavy communication trafﬁc between them can be
localized and thus enable high bandwidth by avoiding the long distance chip-to-chip
I/O communication which is common in current architectures with separate memory
chips. Placing the massive memristive nano devices into local logic and processing
cores will be a key to truly co-locate memory and processor on a chip. Since the
co-located memory devices can basically function as nonvolatile local memories,
stateful logic gates, and also as reconﬁgurable interconnects, all with very high in-
tegration density and low power, the hybrid hardware structure will be a leading
technology, signiﬁcantly relaxing the issue related with scaling and leakage power,
and enabling instant resiliency at power shutoff.
Recently demonstrated are several key technologies for high-density nonvolatile
memory architectures; reconﬁgurable resistive logic gates and nonvolatile CMOS
logic structure; resistive computation structures; and many others, all of which will
contribute to building a fundamental basis for truly ﬂexible nonvolatile resistive
nanocomputers. The related research would open a new electronics and computing
paradigm, and directing the research community to a completely new way.
References
1. Chua LO (1971, September) Memristor-the missing circuit element. IEEE Trans Circ Theor
18:507–519
2. Chua LO, Kang SM (1976, February) Memristive devices and systems. Proc IEEE 64:209–223

11
Energy-Efﬁcient Memristive Analog and Digital Electronics
209
3. Strukov DB et al (2008) The missing memristor found. Nature 453:80–83
4. Strukov DB, Likharev KK (2005) CMOL FPGA: a reconﬁgurable architecture for hybrid digital
circuits with two-terminal nanodevices. Nanotechnology 16:888–900
5. Snider GS, Williams RS (2007,
January) Nano/CMOS architectures using a ﬁeld-
programmable nanowire interconnect. Nanotechnology 18:035204
6. Shin S, Kim K, Kang SM (2010, April) Compact models for memristors based on charge–ﬂux
constitutive relationships. IEEE Trans Comput-Aided Des 29(4):590–598
7. VargheseD,GandhiG(2009, July)Memristorbasedhighlinearrangedifferentialpair. ICCCAS
2009:935–938
8. Pershin YV, Ventra MD (2010, August) Practical approach to programmable analog circuits
with memristors. IEEE Trans Circ Syst I 57(8):1857–1864
9. Shin S, Kim K, Kang SM (2009, July 23–25) Memristor-based ﬁne resolution resistance and
its applications. ICCCAS 2009:948–951
10. Shin S, Kim K, Kang SM (2011, March) Memristor application to programmable analog ICs.
IEEE Trans Nanotechnol 10(2):266–270
11. Ozalevli E, Hasler PE (2008, May) Tunable highly linear ﬂoating-gate CMOS resistor using
common-mode linearization technique. IEEE Trans Circ Syst 55(4):999–1010
12. LiuM,WangW(2008, September)Applicationofnanojunction-basedRRAMtoreconﬁgurable
IC. Micro Nano Lett 3(3):101–105
13. Rose GS et al (2007, April) Designing CMOS/molecular memories while considering device
parameter variations. ACM J Emerg Technol Comput Syst 3:1–24
14. David CA, Feldman B (1968, August) High-speed ﬁxed memories using large-scale integrated
resistor matrices. IEEE Trans Comput c-17(8):721–728
15. Lynch WT (1969, October) Worst-case analysis of a resistor memory matrix. IEEE Trans
Comput c-18(10):940–942
16. Shin S, Kim K, Kang SM (2010, December) Data-dependent statistical memory model for
passive memristive devices array. IEEE Trans Circ Syst II 57(12):986–990
17. Mustafar J, Waser R (2006, November) A novel reference scheme for reading passive resistive
crossbar memories. IEEE Trans Nanotechnol 5(6):687–691
18. Riaza R (2010, March) Nondegeneracy conditions for active memristive circuits. IEEE Trans
Circ Syst II 57:223–227
19. Kuekes P (2008) Material implication: digital logic with memristors. Memristor and
Memristive Systems Symposium, November 21, 2008
20. Borghetti J et al (2010) ‘Memristive’ switches enable ‘stateful’ logic operations via material
implication. Nature 464:873–875
21. Shin S, Kim K, Kang SM (2011, July) Reconﬁgurable stateful NOR gate for large-scale logic
array integrations. IEEE Trans Circ Syst II 58(7):442–446
22. Kuekes PJ et al (2005) The crossbar latch: logic value storage, restoration, and inversion in
crossbar circuits. J Appl Phys 97:034301
23. Kim K, Shin S, Kang SM (2011, May) Stateful logic pipeline architecture. IEEE Int Symp
Circ Syst 2011:2497–2500


Chapter 12
Memristor SPICE Modeling
Chris Yakopcic, Tarek M. Taha, Guru Subramanyam, and Robinson E. Pino
Abstract Modeling of memristor devices is essential for memristor based circuit
and system design. This chapter presents a review of existing memristor modeling
techniques and provides simulations that compare several existing models to pub-
lished memristor characterization data. A discussion of existing models is presented
that explains how the equations of each relate to physical device behaviors.
The simulations were completed in LTspice and compare the output of the dif-
ferent models to current–voltage relationships of physical devices. Sinusoidal and
triangular pulse inputs were used throughout the simulations to test the capabilities of
each model. The chapter is concluded by recommending a more generalized mem-
ristor model that can be accurately matched to several different published device
characterizations. This generalized model provides the potential for more accurate
circuit simulation for a wide range of device structures and voltage inputs.
12.1
Introduction
The memristor was theorized in 1971 by Dr. Leon Chua [1], and was ﬁrst fabricated
by a research team led by Dr. Stanley Williams at HP Labs in 2008 [2, 3]. The mem-
ristor is a non-volatile nanoscale 2-termial passive circuit element that has dynamic
resistance dependent on the total charge applied between the positive and negative
terminals.
C. Yakopcic () · T. M. Taha · G. Subramanyam
Department of Electrical and Computer Engineering,
University of Dayton, Dayton, OH, USA
e-mail: cyakopcic1@udayton.edu
T. M. Taha
e-mail: tarek.taha@udayton.edu
G. Subramanyam
e-mail: gsubramanyam1@udayton.edu
R. E. Pino
Information Directorate, Advanced Computing Architectures
Air Force Research Laboratory, Rome, NY, USA
e-mail: Robinson.Pino@gmail.com
R. Kozma et al. (eds.), Advances in Neuromorphic Memristor Science and Applications,
211
Springer Series in Cognitive and Neural Systems,
DOI 10.1007/978-94-007-4491-2_12, © Springer Science+Business Media Dordrecht 2012

212
C. Yakopcic et al.
The memristor fabricated at HP Labs was a thin-ﬁlm titanium oxide device.
The device structure comprised of a stoichiometric (TiO2) and an oxygen deﬁcient
(TiO2 −x) layer sandwiched between two platinum electrodes. Applying a voltage
across a memristor causes the oxygen deﬁciencies in the TiO2 −x layer to migrate,
and this changes the thickness of the oxygen deﬁcient layer. Likewise, this changes
the resistance of the memristor device. Since the oxygen vacancies have a low mo-
bility, they tend to stay in the same position after the voltage source is removed [3].
This phenomenon shows that the memristor can be used as non-volatile memory
device where the resistance of a memristor is used to store information.
Applications where memristors may be used include high density non-volatile
memory [4] and logic design [5, 6]. One of the more interesting applications for the
memristor involves using the device to mimic the functionality of a synapse in brain
tissue [7, 8]. Just as neural spikes are applied to a synapse to change the weight, volt-
age pulses can be applied to a memristor to change the resistance. Since the dynamics
of a memristor closely model a synapse [7, 8], memristors are considered ideal for
spiking input based neuromorphic systems. Thus the simulation of memristors for
spiking inputs is essential to the design of memristor based neuromorphic systems.
Since the initial fabrication and modeling efforts by HP Labs [2], several dif-
ferent memristor device structures and materials have been published [7–12].The
wide variety in memristor structure and composition has led to the development of
many different memristor modeling techniques. Several compact models have been
proposed that present modeling equations that approximate the functionality of pub-
lished memristor devices [2, 13–16]. Furthermore, a number of subcircuits have
been proposed that provide the capability of modeling memristors in SPICE simu-
lations [17–24]. Many of these models [14, 17, 19–22], are based on the memristor
equations ﬁrst proposed by HP Labs in [2]. Additionally, advances in modeling have
been published [25] based on the original memristor equations proposed by Dr. Chua
[1].The remainder of the memristor models are either closely correlated to device
hardware [13, 16, 24], and/or based on more complex physical mechanisms [15, 18,
23, 24] such as the metal-insulator-metal (MIM) tunnel junction [26].
This chapter provides a review of many of the different memristor modeling
techniques [2, 13–17, 23, 24].The memristor models chosen to be discussed in this
chapter were selected to show a wide variety of different modeling techniques while
minimizing redundancy. Some models have been designed to represent a speciﬁc
device very accurately, and other models aim to reproduce the functionality of a
wider range of devices in a more generalized manner.
The model results in this chapter are discussed in terms of their resulting I–V
characteristics and how well they model the I–V characteristics of physical devices.
The voltage inputs studied are either sinusoidal or triangular pulses. The triangular
input pulses are applied multiple times with the same polarity to study how each
model switches to intermediate levels between the maximum and minimum resis-
tance. The SPICE simulations were performed in LTspice, and the subcircuit code is
provided for each model. This allowed for a one-to-one comparison of many different
memristor models to show the advantages and disadvantages of each.

12
Memristor SPICE Modeling
213
This chapter is organized as follows: Sect. 12.2 describes the how the memristor
modeling equations were developed based on the initial memristor fabrication at
HP Labs. Section 12.3 shows how these initial equations were modiﬁed to develop
SPICE models. Section 12.4 describes two alternative SPICE modeling techniques
where the model output of each correlates very closely to the characterization data of
a speciﬁc device. Section 12.5 describes models that have been developed based on a
hyperbolic sinusoid current–voltage relationship. This appears to improve the model
result when using repetitively pulsed inputs. Section 12.6 discusses a generalized
SPICE model that can be used to accurately model the current–voltage relationship
of several different memristor devices. Section 12.7 provides a conclusion that
summarizes the results.
12.2
Memristor Model Proposed by HP Labs
In 2008, HP Labs published results that described the memristor device according
to Eqs. (12.1) through (12.3). The current voltage relationship is described in Eq.
(12.1). The value of I(t) represents the current through the memristor, and the volt-
age V(t) represents the voltage generated at the input source. These deﬁnitions for
I(t) and V(t) are used consistently throughout this chapter. The constants ROFF and
RON represent the maximum and minimum resistances of the device respectively.
The actual resistance of the device is dependent on the ratio between the value of
the dynamic state variable w(t) and the device thickness D. The state variable w(t)
represents the thickness of the oxygen deﬁcient titanium dioxide layer (TiO2 −x). As
the value of w(t) increases, it can be seen that the overall device resistance lowers
since ROFF > RON.
The dynamic value of the state variable can be determined using Eq. (12.2) where
dw/dt is described as the drift velocity of the oxygen deﬁciencies (vD) in the device.
The value for w(t) can be determined by integrating Eq. (12.2), and the result can
be seen in Eq. (12.3). After integration, it can be seen that the value for w(t) is
proportional to the charge on the device. Since the charge is the integral of the
current, this provides an explanation for the non-volatile effect of the memristors:
when no current is ﬂowing through the device, the charge is constant and thus, the
resistance remains unchanged.
V (t) =

RON
w(t)
D
+ ROFF

1 −w(t)
D

I(t)
(12.1)
vD = dw
dt = μDRON
D
I(t)
(12.2)
w(t) = μDRON
D
q(t)
(12.3)
Figure 12.1 shows how this memristor model reacts to a simple sinusoidal voltage
input. The I–V curve displays a pinched hysteresis loop that is characteristic of

214
C. Yakopcic et al.
0
0.5
1
1.5
2
-30
-20
-10
0
10
20
30
time (s)
Current (μA)
-1
-0.5
0
0.5
1
-25
-20
-15
-10
-5
0
5
10
15
20
25
Voltage (V)
Current (μA)
-1.5
-1
-0.5
0
0.5
1
1.5
Voltage (V)
Current
Voltage
Fig. 12.1 Simulation results for the HP Labs memristor with a sinusoidal input. In this simulation:
RON = 10 k, ROFF = 100 k, μD = 10−14m2s−1V−1, D = 27 nm, x0 = 0.1D, and V(t) = sin(2πt)
Current
Voltage
0
2
4
6
8
-30
-20
-10
0
10
20
30
time (s)
Current (μA)
-1
-0.5
0
0.5
1
-30
-20
-10
0
10
20
30
Voltage (V)
Current (μA)
-1.5
-1
-0.5
0
0.5
1
1.5
Voltage (V)
Fig. 12.2 Simulation results for the HP Labs memristor with a pulsed input. In this simulation:
RON = 10 k, ROFF = 100 k, μD = 10−14m2s−1V−1, D = 27 nm, x0 = 0.1D, and V(t) = sin(2πt).
Triangular pulses have magnitude of 1 V and 1 s pulse width with rise and fall time of 0.5 s
memristors. The hysteresis shows that the conductivity in a memristor is not only
related to the voltage applied, but also to the previous value of the state variable w(t),
as more than one current value can be correlated to a single voltage. Figure 12.2
shows the simulation results of the model when several triangular voltage pulses are
applied to the device. The ﬁrst 4 voltage pulses, occurring in the time between 0
and 4 s, correspond to the right half of the I–V curve. Since the charge applied is
always positive, the state variable continually increases. This results in an increase
in the conductivity of the device as each pulse in applied. For the simulation time
between 4 and 8 s, the current is always negative, and the opposite trend can be seen.
These simulations were performed in MATLAB assuming that the voltage signal was
directly applied to a memristor device with no additional circuit elements or added
resistances. The next section discusses how these equations were used to generate a
SPICE subcircuit.

12
Memristor SPICE Modeling
215
12.3
Initial Memristor SPICE Modeling
It is of great beneﬁt to circuit designers to be able to model the memristor in SPICE
simulators, therefore Eqs. (12.1) and (12.2) were used to develop SPICE subcircuits
formemristors[17–24]. Modiﬁcationstotheseinitialequationsweremadetodevelop
a robust technique for simulating memristors based on these initial equations.
First, it should be noted that the state variable equations were updated using the
variable substitution x(t) = w(t)/D. The state variable is now a normalized quantity
between 0 and 1. When x(t) = 0 the memristor device is in the least conductive
state, and the most conductive state occurs when x(t) = 1. Memristor devices have
been proposed using several different material structures [7–12], so the resistance
switching mechanism is not always due to the change in thickness of a titanium oxide
layer. This change in state variable represents a generalization of the model so that
it can represent more than just titanium oxide devices.
Next, the state variable boundaries were deﬁned. The modeling equations in
Sect. 12.2 do not account for the state variable boundaries: 0 ≤x(t) ≤1 (or 0 ≤w(t) ≤
D). If sufﬁcient charge is applied to the memristor, then the value of x(t) will become
larger than 1 and the result of the model will become unstable and thus incorrect.
The state variable motion was limited by using two different windowing functions
[14, 17], and SPICE models were developed based on these modiﬁed equations.
12.3.1
Joglekar Modiﬁcations
In a publication by Yogesh N. Joglekar and Stephen J. Wolf [14], modiﬁcations
were made to the initial equations proposed by HP Labs [2]. The parameter η was
added so that memristors could be modeled where state variable motion could be in
either direction relative to the input voltage. If a positive voltage signal applied to a
memristor increases the value of the state variable, then the memristor device should
be modeled where η = 1. If the state variable decreases with the application of a
positive voltage signal, then that memristor should be modeled with η = −1.
Additionally, the windowing function in Eq. (12.4) was added to the equation for
state variable motion. This was done to ensure that the state variable will always fall
in the range 0 ≤x(t) ≤1. Figure 12.3 displays plots for the Joglekar and Biolek (see
Sect. 12.3.2) window functions for all acceptable values of x(t).When looking at the
left plot in Fig. 12.3, it can be seen that the Joglekar window function forces the
state variable motion to be zero at x(t) = 0 or x(t) = 1, thus deﬁning the boundaries.
Depending on the value of the parameter p, the window function can provide a harder
boundary effect (where p = 100), or provide a smoother non-linearity in the motion
of the state variable (where p = 1). The state variable equation in its modiﬁed form
can be seen in Eq. (12.5). It should be noted that the parameter D has been squared
due to the substitution x(t) = w(t)/D.
F(x(t)) = 1 −(2x(t) −1)2p
(12.4)
dx
dt = ημDRON
D2
I(t)F(x(t))
(12.5)

216
C. Yakopcic et al.
0
0.2
0.4
0.6
0.8
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Joglekar Window Function
F(x(t))
x(t)
0
0.2
0.4
0.6
0.8
1
0
0.1
0.2
0.3
0.4
0.5
0.6
0.7
0.8
0.9
1
Biolek Window Function
F(x(t))
x(t)
I(t) < 0
I(t) > 0
a
b
Fig. 12.3 Both the Joglekar (a) and the Biolek (b) window functions are plotted where p = 1,
p = 6, and p = 100. It can be seen that if a hard limit effect at the borders is desired, then this can
be implemented by setting p to a large number in each windowing function
12.3.2
Biolek Modiﬁcations
An alternative window function was proposed in a publication by Zdenek Biolek
et al. [17]. When using the windowing function proposed by Joglekar, the motion
of the state variable is reduced near the boundary whether it is traveling toward, or
away from it. Alternatively, Biolek’s window only reduces velocity at the boundary
the state variable motion is tending toward. This appears to be a more accurate
assumption based on the data collected by HP Labs that was presented in [27]. The
Biolek window function is described in Eqs. (12.6) and (12.7).
F(x(t)) = 1 −(x(t) −stp(−I(t)))2p
(12.6)
stp(I(t)) =

 1
if I(t) > 0
0,
if I(t) < 0
(12.7)
12.3.3
SPICE Model
The circuit layout for a SPICE model based on Eqs. (12.1) through (12.7) is shown
in Fig. 12.4. The two terminals TE and BE represent the top and bottom electrodes
of the memristor. The current source Gm generates a current based on Eq. (12.1).
The value of the state variable is determined using a current source and an integrat-
ing capacitor. The output of the current source is set equal to the right hand side of
Eq. (12.5), and the value x(t) is determined using the integrating capacitor Cx. Mem-
ristor SPICE models have been previously proposed using a similar setup [17, 22].
The port XSV was created to provide a convenient method for plotting the state vari-
able during a simulation. This is a helpful tool for debugging, but XSV should not
be used as a voltage output in a circuit design. The circuit schematic in Fig. 12.5

12
Memristor SPICE Modeling
217
Fig. 12.4 Circuit schematic for the memristor SPICE subcircuit based on [2, 14, 17]
Fig. 12.5 Circuit used to
carry out SPICE simulations
M1
−
V(t)
+
−
+
Input Voltage
shows how the simulations were performed in LTspice. The memristor was simply
connected to an input voltage source. Unless otherwise noted, all simulations in this
chapter were performed using this arrangement.
Figure 12.6 displays code for the memristor subcircuit using the Joglekar window
function, and Fig. 12.7 contains the code for the subcircuit using the Biolek window
function. The code in Figs. 12.6 and 12.7 is based on the subcircuit proposed in [17],
although signiﬁcant modiﬁcations have been made.
12.3.4
Simulation Results with Joglekar Window
Figures 12.8 and 12.9 show the simulation results for the memristor SPICE model
when using the Joglekar window function with a sinusoidal input of two different
amplitudes. Figure 12.8 shows the result when the model is driven with the voltage
inputV(t) = 0.9 sin(2πft). Theresultlooksverysimilartothemodelresultsdisplayed
in Fig. 12.1 because the state variable did not reach the boundaries where the window
function has the strongest effect. Figure 12.8 shows the model response to the voltage
input V(t) = sin(2πft). This slightly larger voltage input forces the value of the state
variable into the region affected by the windowing function and harder switching
result can be seen.

218
C. Yakopcic et al.
Fig. 12.6 SPICE subcircuit for the memristor model developed using the Joglekar window function
Figure 12.10 shows the result using the Joglekar window function when applying
several triangular voltage pulses. This result is similar to the one seen in Fig. 12.2,
although the most conductive hysteresis loop in this simulation is signiﬁcantly larger.
12.3.5
Simulation Results with Biolek Window
Simulations using the Biolek window function were also performed and the results
can be seen in Figs. 12.11 and 12.12. Figure 12.11 shows the model result with a
sinusoidal voltage input with large enough amplitude to drive the state variable into
the boundary where the window function has a larger impact. Figure 12.12 displays
the result when a triangular pulse input is applied. As opposed to the previous results,
this model shows an asymmetric hysteresis with respect to voltage polarity for each
of the simulations.

12
Memristor SPICE Modeling
219
Fig. 12.7 SPICE subcircuit for the memristor model developed using the Joglekar window function
0
0.05
0.1
0.15
0.2
time (s)
-1
-0.5
0
0.5
1
Voltage (V)
Current
Voltage
-0.5
0
0.5
Current (mA)
-0.5
0
0.5
Current (mA)
-1.5
-1
-0.5
0
0.5
1
1.5
Voltage (V)
Fig. 12.8 LTspice simulation results for the memristor model with the Joglekar window function. In
this simulation: RON = 100 , ROFF = 10 k, μD = 5(10−14)m2s−1V−1, D = 12 nm, x0 = 0.56, p = 7,
and V(t) = 0.9 sin(2π10 t)

220
C. Yakopcic et al.
0
0.05
0.1
0.15
0.2
time (s)
-1
-0.5
0
0.5
1
Voltage (V)
-6
-4
-2
0
2
4
6
Current (mA)
-5
0
5
Current (mA)
-1.5
-1
-0.5
0
0.5
1
1.5
Voltage (V)
Current
Voltage
Fig. 12.9 LTspice simulation results for the memristor model with the Joglekar window func-
tion and a large input voltage magnitude. In this simulation: RON = 100 , ROFF = 10 k, μD =
5(10−14) m2s−1V−1, D = 12 nm, x0 = 0.56, p = 7, and V(t) = sin(2π10 t)
Current
Voltage
0
2
4
6
8
-0.75
-0.5
-0.25
0
0.25
0.5
0.75
time (s)
Current (mA)
-1
-0.5
0
0.5
1
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
Voltage (V)
Current (mA)
-1.5
-1
-0.5
0
0.5
1
1.5
Voltage (V)
Fig. 12.10 Results when simulating the HP Labs memristor with a triangular pulsed input. In this
simulation: RON = 1 k, ROFF = 10 k, μD = 2(10−14)m2 s −1 V−1, D = 85 nm, x0 = 0.093, and
p = 2. Triangular pulses have magnitude of 1 V and 1 s pulse width with rise and fall time of 0.5 s
0
0.05
0.1
0.15
0.2
-3
-2
-1
0
1
2
3
time (s)
Current (mA)
-1
-0.5
0
0.5
1
-3
-2
-1
0
1
2
3
Voltage (V)
Current (mA)
-1.5
-1
-0.5
0
0.5
1
1.5
Voltage (V)
Current
Voltage
Fig. 12.11 LTspice simulation results for the memristor model with the Joglekar window function.
In this simulation: RON = 100 , ROFF = 1 k, μD = 4(10−14)m2s−1V−1, D = 16 nm, x0 = 0.076,
p = 7, and V(t) = sin(2π10 t)

12
Memristor SPICE Modeling
221
0
2
4
6
8
-1.5
-1
-0.5
0
0.5
1
1.5
time (s)
Current (mA)
-1
-0.5
0
0.5
1
-0.5
0
0.5
1
Voltage (V)
Current (mA)
-1.5
-1
-0.5
0
0.5
1
1.5
Voltage (V)
Current
Voltage
Fig. 12.12 Results when simulating the HP Labs memristor with a triangular pulsed input. In this
simulation: RON = 1 k, ROFF = 10 k, μD = 2(10−14) m2s−1V−1, D = 80 nm, x0 = 0.1, and
p = 2. Triangular pulses have magnitude of 1 V and 1 s pulse width with rise and fall time of 0.5 s
12.3.6
Discussion
There are several discrepancies when comparing the results obtained from these
models to published physical characterization data. When applying a pulsed wave-
form, the results of these models show that the size of the hysteresis loops in the
positive regime increases as conductivity increases. When looking at the published
characterization data [7–9], an opposite trend is present.
Also, physical memristor devices show a threshold voltage where hysteresis is
not seen unless the voltage across the memristor exceeds the threshold. No threshold
voltage is present in these models. Lastly, Matthew D. Pickett et al. at HP Labs
published characterization data where the motion of the state variable depends on
both its value and the polarity of the applied current [27]. This may suggest that the
compression of oxygen vacancies has slightly different dynamics than the oxygen
vacancy expansion, and that they are not perfectly mirrored. These models show the
motion of the state variable to be equivalent, whether it is moving in the positive or
negative direction. The remainder of this chapter discusses the different techniques
that have been used to develop memristor SPICE models that provide a closer match
to published characterization data.
12.4
Hardware Correlated Models
Alternative memristor models have been proposed that correlate more closely to
physical characterization data. This section discusses two different techniques for
modeling the memristors [16, 24] where the input voltage is sinusoidal (as opposed
to using a repetitive pulse input). Each of these models produces a result that is
matched very closely to a speciﬁc memristor device.

222
C. Yakopcic et al.
12.4.1
Air Force Research Lab Model
The ﬁrst hardware correlated model was developed by Dr. Robinson E. Pino et al.
at the Air Force Research Lab (AFRL) to match the I–V characteristic of a device
developed at Boise State University [9]. This model matches the I–V characteristic
very well because it correlates voltage amplitude and slope to a piecewise function
matched to the characterization of a physical device. Instead of deﬁning a state
variable for this model, the rate of change of the resistance was directly deﬁned
using Eqs. (12.8) and (12.9). When the voltage across the memristor is greater than
T h, Eq. (12.8) is used to determine the change in resistance of the memristor device.
When the voltage across the memristor is less than T l, Eq. (12.9) is used to determine
the change in device resistance. Lastly, when the voltage across the memristor is in
the range T l ≥V(t) ≤T h, there is no change in resistance.
dR
dt =

−Kh1e
K h2(V (t)−Th),
R(t) > RON
0,
R(t) ≤RON
(12.8)
dR
dt =

Kl1e
K l2(V (t)−Tl),
R(t) < ROFF
0,
R(t) ≥ROFF
(12.9)
The equations for this model were ﬁrst proposed in [16], and the LTspice code was
developed for use in this chapter. The subcircuit can be seen in Fig. 12.13. The model
has three terminals, again with two representing the top and bottom electrodes of the
device and a third terminal to plot the time integral of the rate equation. In this case,
the change in resistance is output at the terminal RSV since there is no state variable
deﬁned other than the resistance itself. It can also be seen that the current through the
memristor is determined through a division of two voltages. Even though the value
of V(RSV) is a voltage in the simulation, it actually represents the resistance of the
memristor.
The model result can be seen in Fig. 12.14. When comparing this result to the
characterization data in [16], the model matches the both the current output and the
I–V curve very closely. For convenience, the physical characterization data that this
model was meant to match was reproduced in Fig. 12.15.
12.4.2
HP Labs MIM Model
A more complex model was proposed by Drs. Hisham Abdalla and Matthew D.
Pickett at HP Labs [24]. This model was based on the assumption that the memristor
acts as a metal-insulator-metal (MIM) tunnel barrier [26]. The insulating tunnel
barrier is represented by the TiO2 layer, and the TiO2 −x layer acts as a low resistivity
metallic layer. As voltage is applied, the thickness of the tunnel barrier is said to
modulate due to the position of the oxygen vacancies in the device. This model
appears to match the characterization data very well. Additionally, the model is
supported by a very strong connection to the physical mechanisms within the device.

12
Memristor SPICE Modeling
223
Fig. 12.13 LTspice subcircuit developed based on the AFRL memristor model equations in [16]
0
10
20
30
40
50
-3
-2
-1
0
1
2
3
4
time (ms)
Current (mA)
-0.75
-0.5
-0.25
0
0.25
0.5
0.75
1
Voltage (V)
-0.5
0
0.5
-2
-1
0
1
2
3
4
Voltage (V)
Current (mA)
Current
Voltage
Fig. 12.14 Simulation results using the model presented in [16]. In this simulation: RON = 160,
ROFF = 1200, T h = 0.2, T l = −0.35, Kh1 = 5.5(106), Kh2 = −20, Kl1 = 4(106), and Kl2 = 20

224
C. Yakopcic et al.
Fig. 12.15 Plot that displays
the characterization data that
was used to develop the
AFRL model. (This ﬁgure is a
reproduction from [16] that is
supplied with permission
from the authors)
The model equations can be seen in (12.10) through (12.14). In these equations
the dynamic state variable is deﬁned as w(t). Although, in this model w(t) represents
the thickness of the TiO2 layer as opposed to the TiO2 −x layer as seen in Eqs. (12.1)
through (12.3). Equations (12.10) and (12.11) govern the state variable dynamics.
These equations were formed based the data presented in [27]. When I(t) > 0, the state
variable motion is described by Eq. (12.10), otherwise, the state variable motion is
described by Eq. (12.11). The ﬁtting parameters in the model were deﬁned as follows:
f off = 3.5 μs, ioff = 115 μA, aoff = 1.2 nm, f on = 40 μs, ion = 8.9 μA, aon = 1.8 nm,
b = 500 μA, and wc = 107 pm.
dw
dt = foff sinh
|I(t)|
ioff

exp

−exp
w(t) −aoff
wc
−|I(t)|
b

−w(t)
wc

(12.10)
dw
dt = −fon sinh
|I(t)|
ion

exp

−exp
aon −w(t)
wc
−|I(t)|
b

−w(t)
wc

(12.11)
Equations (12.12) through (12.14) were developed by modifying the MIM tunnel
barrier equations ﬁrst proposed in [26] to account for a variable barrier width. In these
equations: φ0 = 0.95 V, w1 = 0.1261 nm, B = 10.24634  w,  = 0.0998/w(t), and
w = w2 −w1. The variable vg represents the voltage across the TiO2 layer of the
memristor. The total voltage across the device is equal to the sum of vg and vr, where
vr is equal to the voltage across the TiO2 −x layer.
I(t) = 0.0617
w2

φIe−B√φI −(φI +
vg
 )e−B√
φI +|vg| 
(12.12)
φI = φ0 −
vg

w1 + w2
w(t)

−
0.1148
w

ln
w2(w(t) −w1)
w1(w(t) −w2)

(12.13)
w2 = w1 + w(t)
!
1 −
9.2 λ
2.85 + 4 λ −2
vg

"
(12.14)

12
Memristor SPICE Modeling
225
Fig. 12.16 Schematic for
testing the MIM memristor
model [24]
M1
–
2.4kΩ
V(t)
+
–
+
Input Voltage
Memristor Voltage
R1
The circuit used to test the model differed slightly compared to the original circuit
in Fig. 12.7. The circuit in Fig. 12.16 shows how this memristor model was tested.
The 2.4 k resistor was added to model the resistance of the electrodes used to
characterize the memristor device.
The LTspice code for the MIM memristor SPICE model can be seen in Fig. 12.17.
The code was taken from [24], except small changes were made so that the model
would operate correctly in LTspice. Additionally, the node WSV was added as a
terminal so that the state variable motion could be plotted easily.
The simulation results for the SPICE model can be seen in Fig. 12.18. The top
left plot shows the voltage across the memristor along with the current through
the memristor. The voltage signal from the input source can be seen in the bottom
left plot. The simulated I–V curve closely matches the data displayed in [24]. For
convenience, the characterization data from [24] is displayed alongside the model
result in Fig. 12.19.
12.4.3
Discussion
The models presented in this section show a very close correlation to the char-
acterizations that they were designed to match. The disadvantage is that it is not
known how closely these models will match the characterization data for alternative
voltage inputs such as repetitive triangular pulses. Also, these models are very
speciﬁc to a single fabricated device. Given the wide variety in the current voltage
characterization of memristors, these models will most likely have to be updated
signiﬁcantly for use with different device structures.
12.5
Hyperbolic Sine Models
The hyperbolic sinusoid shape has been proposed for several memristor models [13,
15, 18, 23]. This is because the hyperbolic sinusoid function can be used to approx-
imate the I–V relationship of an MIM junction [26]. Since thin ﬁlm memristors are

226
C. Yakopcic et al.
Fig. 12.17 LTspice code for the HP Labs MIM memristor model [24]
commonly fabricated by sandwiching an oxide between two metal electrodes, mod-
eling a memristor as an MIM device seems reasonable. Section 12.4.2 has already
demonstrated a model [24] based on MIM tunneling equations. Using a hyperbolic
sine function in the I–V relationship appears to provide a signiﬁcantly better result
when using a repetitive voltage pulse input.

12
Memristor SPICE Modeling
227
-1
-0.5
0
0.5
1
1.5
-1
-0.5
0
0.5
1
1.5
2
Voltage (V)
Current (mA)
0
1
2
3
4
5
6
-3
-2
-1
0
1
2
3
4
time (s)
Current (mA)
0
1
2
3
4
5
6
-2
0
2
4
6
time (s)
Input Voltage (V)
-1.5
-1
-0.5
0
0.5
1
1.5
2
Memristor Voltage (V)
Current
Voltage
Fig. 12.18 Simulation results using the HP Labs MIM model
Fig. 12.19 Memristor
characterization data from
[24] matched to the HP Labs
MIM model. (This ﬁgure was
reproduced from [24] with
permission from the authors)
12.5.1
General Hyperbolic Sine Model
A general hyperbolic sinusoid model was proposed by Dr. Mika Laiho et al. [15]
and is described in Eqs. (12.15) and (12.16). The current voltage relationship is
represented by a hyperbolic sinusoid modulated by the value of the state variable
x(t). The parameters a1, a2, b1, and b2 are used to adjust the I–V response of the
model. The state variable is also modeled using a hyperbolic sine function (see Eq.
(12.16)). Several memristor characterizations show that the state of the device will
not change unless the voltage applied exceeds a threshold [7–12], and the hyperbolic
sinusoid based state variable is one option that achieves this effect. The constants
c1, c2, d1, and d2, are used to shape the threshold and intensity of the state variable
dynamics.
I(t) =

a1x(t) sinh(b1V (t)),
V (t) ≥0
a2x(t) sinh(b2V (t)),
V (t) < 0
(12.15)

228
C. Yakopcic et al.
Fig. 12.20 LTspice code that was developed for the generalized hyperbolic sinusoid model proposed
by Laiho et al.
dx
dt =

c1 sinh(d1V (t)),
V (t) ≥0
c2 sinh(d2V (t)),
V (t) < 0
(12.16)
This model was originally proposed in [15] without boundary conditions to stop the
state variable from exceeding the range 0 ≤x(t) ≤1. To address this issue, the model
has been modiﬁed by using the Biolek window function [17] to deﬁne the device
boundaries. The updated state variable equation can be seen in (12.17) where F(x(t))
represents the Biolek window function. The SPICE code for the model with and
without the boundary addition has been developed and can be seen in Figs. 12.20
and 12.21 respectively.
dx
dt =

c1 sinh(d1V (t))F(x(t)),
V (t) ≥0
c2 sinh(d2V (t))F(x(t)),
V (t) < 0
(12.17)
Figures 12.22 and 12.23 show the simulation results for this model with and without
the addition of the Biolek window function. Figure 12.22 shows results of the model
as it was presented in [15]. Modeling the boundary is not an issue since not enough
charge was applied to let the state variable move outside the boundaries. Figure 12.23
shows the results of the model where the Biolek window function was added to set
boundaries on the value of the state variable. It can be seen that current through the

12
Memristor SPICE Modeling
229
Fig. 12.21 LTspice code that was developed for the generalized hyperbolic sinusoid model with
the addition of the Biolek windowing function
0
0.2
0.4
0.6
0.8
1
-1
0
1
2
3
time (s)
Current (μA)
-2
0
2
4
-0.5
0
0.5
1
1.5
2
2.5
Voltage (V)
Current (μA)
-3
-2
-1
0
1
2
3
4
5
6
Voltage (V)
Current
Voltage
Fig. 12.22 Simulation results for the hyperbolic sinusoid model proposed by Laiho et al. In this
simulation: a1 = 4(10−8), b1 = 1.2, a2 = 1.25(10−7), b2 = 1.2, c1 = 6(10−4), d1 = 2, c2 = 6.6(10−4),
d2 = 3.8, and x0 = 0.001. Triangular pulses have magnitude of + 5/−2.5 V and a 0.1 s pulse width
with rise and fall time of 0.05 s

230
C. Yakopcic et al.
0
0.5
1
1.5
-10
-5
0
5
10
15
time (s)
Current (μA)
-2
0
2
4
6
-2
0
2
4
6
8
10
12
14
Voltage (V)
Current (μA)
-4
-3
-2
-1
0
1
2
3
4
5
6
Voltage (V)
Current
Voltage
Fig. 12.23 Simulation results for the hyperbolic sinusoid model proposed by Laiho et al. with the
addition of the Biolek window function. In this simulation: a1 = 4(10−8), b1 = 1.2, a2 = 1.25(10−7),
b2 = 1.2, c1 = 6(10−4), d1 = 2, c2 = 6.6(10−4), d2 = 3.8, and p = 1, x0 = 0.001. Triangular pulses
have magnitude of + 5.5/−3 V and a 0.1 s pulse width with rise and fall time of 0.05 s
device increases with each voltage pulse until the upper limit of the state variable
motion is achieved. At this point the peak of each current pulse no longer changes
until the polarity of the input voltage is reversed.
12.5.2
University of Michigan Model
An alternative model based on the hyperbolic sine I–V relationship was developed
by Ting Chang et al. in [23], and the corresponding SPICE code can be seen in
Fig. 12.24. The I–V relationship can be seen in (12.18) where the ﬁrst term is due
to a Schottky barrier between the oxide layer and the bottom electrode, and the
second term is due to the tunneling through the MIM junction. The state variable
x(t) is a value between 0 and 1 that represents the ion migrations which determine
the conductivity of the device. The motion of the state variable is described by Eq.
(12.19) which is similar to Eq. (12.16). The ﬁtting parameters η1, η2, and λ are
used shape the dynamics of the state variable equation. In each of the simulations in
Figs. 12.25 through 12.27, the constants in the equations were deﬁned as follows:
α = 5(10−7), β = 0.5, γ = 4(10 −6), δ = 2,  = 4.5, η1 = 0.004, η2 = 4, and τ = 10.
I(t) = (1 −x(t))α

1 −eβV (t)
+ x(t)γ sinh (δV (t))
(12.18)
dx
dt = [η1 sinh (η2V (t))]
(12.19)
An alternative function for the motion of the state variable was proposed in [23]
to account for the overlapping of multiple hysteresis loops where the device was
tested with a repetitive pulse input (see Fig. 12.28). The overlap in hysteresis was
said to be caused by diffusion of ions within the device. Equation (12.20) shows the
modiﬁed state variable equation with the diffusion term added. The LTspice code

12
Memristor SPICE Modeling
231
Fig. 12.24 LTspice code for the for the memristor model proposed by Chang et al. in [23]
for the model was obtained from [23], and was then modiﬁed so that either state
variable equation could be used with the change of a binary variable. This allows for
the drift component to be turned on or off from within the simulation that is using
the subcircuit.The resulting SPICE subcircuit can be seen in Fig. 12.24. It should
be noted that Eqs. (12.19) and (12.20) were taken from the SPICE code in [23], not
from the text in [23]. The equations in the text differed slightly so precedence was

232
C. Yakopcic et al.
0
0.5
1
1.5
2
2.5
3
-3
-2
-1
0
1
2
3
time (s)
-1.5
-1
-0.5
0
0.5
1
1.5
Voltage (V)
-1.5
-1
-0.5
0
0.5
1
1.5
-3
-2
-1
0
1
2
3
Voltage (V)
Current (μA)
Current (μA)
Current
Voltage
Fig. 12.25 Simulation results for the memristor SPICE model proposed by Chang et al. without the
ion diffusion term included in the state variable equation. The triangular pulses have a magnitude
of 1.25 V and a pulse width of 0.5 s with a 0.25 s rise/fall time
0
1
2
3
4
5
-21
-14
-7
0
7
14
21
time (s)
-1.5
-1
-0.5
0
0.5
1
1.5
Voltage (V)
-1.5
-1
-0.5
0
0.5
1
1.5
-20
-15
-10
-5
0
5
10
15
20
Voltage (V)
Current (μA)
Current (μA)
Current
Voltage
Fig. 12.26 Simulation results for the memristor SPICE model proposed by Chang et al. without the
ion diffusion term included in the state variable equation. The triangular pulses have a magnitude
of 1.25 V and a pulse width of 0.5 s with a 0.25 s rise/fall time
given to the equations used to develop the model.
dx
dt = 

η1 sinh (η2V (t)) −x(t)
τ

(12.20)
The simulation in Fig. 12.25 shows the model results when a voltage signal with
zero net charge is applied to the memristor device. The result is similar to previ-
ous simulations where a sinusoidal input is applied. In this simulation a prominent
curvature can be seen due to the hyperbolic sine term in the I–V relationship. Fig-
ure 12.26 shows the model results when repetitive pulses are applied to the device.
The multiple-hysteresis pattern looks similar to the result in Fig. 12.22 although this
model shows a higher conductivity when negatively biased. Figure 12.27 shows the
model result when the ion diffusion term was added to the state variable equation.
This result appears to be a better match to the characterization data in [23], which
was reproduced for convenience in Fig. 12.28. The hysteresis loops overlap in the
positively biased area, but a larger gap can be seen between the loops when negative
pulses are applied. Ion diffusion appears to be a logical explanation for this effect.

12
Memristor SPICE Modeling
233
0
1
2
3
4
5
-15
-10
-5
0
5
10
15
time (s)
-1.5
-1
-0.5
0
0.5
1
1.5
Voltage (V)
-1.5
-1
-0.5
0
0.5
1
1.5
-15
-10
-5
0
5
10
15
Voltage (V)
Current (μA)
Current (μA)
Current
Voltage
Fig. 12.27 Simulation results for the memristor SPICE model proposed by Chang et al. with the
ion diffusion term included in the state variable equation. The triangular pulses have a magnitude
of 1.25 V and a pulse width of 0.5 s with a 0.25 s rise/fall time
Fig. 12.28 Plot that displays the characterization data that the model in Fig. 12.24 was meant to
match. (This ﬁgure is a reproduction from [23] that was supplied with permission from the authors)
12.5.3
Discussion
The models described in this section use a hyperbolic sine function in the I–V rela-
tionship which models the characteristics of a memristor well for both single sweep
and repetitive pulse inputs. Additional properties were also modeled such as the
Schottky barrier at a metal-oxide interface, and the diffusion of ions. This resulted
in a stronger correlation to physical memristor characterization data. These models
have the potential to describe the functionality of a memristor in a more generalized
way which also appears to be quite accurate. The drawback is that these models do
not correlate to physical hardware as closely as the models in Sect. 12.4. The next
section describes a generalized model that quantitatively matches published charac-
terization data for a variety of different memristor devices for a variety of different
voltage inputs.

234
C. Yakopcic et al.
12.6
Generalized Model for Many Devices
A memristor device model was developed in [13] that can accurately match the I–V
characteristic of several published memristor devices. The equations were developed
basedonamoregeneralunderstandingofmemristordynamics, andﬁttingparameters
were used to match the results to physical characterization data [7–12].
12.6.1
Generalized Memristor Model Equations
The generalized I–V relationship for this memristor model can be seen in Eq. (12.21).
A similar equation was proposed in [15] that used two separate multiplying parame-
ters in the hyperbolic sine term depending on voltage polarity. In the development of
this model, it was determined that a single parameter b, could be used independent
of voltage polarity. The hyperbolic sinusoid shape is due to the MIM structure [26]
of memristors, which causes the device to have an increase in conductivity beyond
a certain voltage threshold. The parameters a1, a2, and b are used to ﬁt Eq. (12.21)
to the different device structures of the memristors studied in this paper. Based on
existing memristor characterization data, the devices appear to be more conductive
in the positive region. To account for this, a different amplitude parameter is required
depending on the polarity of the input voltage. The ﬁtting parameter b was used to
control the intensity of the threshold function relating conductivity to input voltage
magnitude. For example, the device published in [7] has a stronger threshold (b = 3)
than the device published in [8] (b = 0.7).
The I–V relationship also depends on the state variable x(t), which provides the
change in resistance based on the physical dynamics in each device. In this model,
the state variable is a value between 0 and 1 that directly impacts the conductivity of
the device.
I(t) =

a1x(t)sinh(bV (t)),
V (t) ≥0
a2x(t)sinh(bV (t)),
V (t) 0
(12.21)
The change in the state variable is based on two different functions, namely, g(V(t))
and f (x(t)). The function g(V(t)) imposes a programming threshold on the mem-
ristor model. The threshold is viewed as the minimum energy required to alter the
physical structure of the device. Each of the published memristor devices [7–12]
show that there is no state change in the memristor unless a certain voltage thresh-
old is exceeded. These changes include the motion of low mobility ions or dopants
[7, 8, 10–12], or the state change in a chalcogenide device [9]. The programming
threshold was implemented using Eq. (12.22). As opposed to the hyperbolic sinusoid
programming threshold implemented in [15], the method in Eq. (12.22) provides the
possibility of having different thresholds based on the polarity of the input voltage.
This is required to provide a better ﬁt to the characterization data, since several of
these devices show different threshold values depending on whether the input voltage
is positive or negative.

12
Memristor SPICE Modeling
235
In addition to the positive and negative thresholds (V p and V n), the magnitude
of the exponentials (Ap and An) can be adjusted. The magnitude of the exponential
represents how quickly the state changes once the threshold is surpassed. In the
results in Sect. IV, it can be seen that the chalcogenide device [9] requires a very
large change once the threshold is surpassed. Alternatively, the device based on the
motion of silver dopants [8] requires a much lower amplitude coefﬁcient as this
appears to be a slower phenomenon.
g(V (t)) =
⎧
⎨
⎩
Ap(eV (t) −eVp),
V (t) > Vp
−An(e−V (t) −eV n),
V (t) < −Vn
0,
−Vn ≤V (t) ≤Vp
(12.22)
The second function used to model the state variable f (x(t)), can be seen in
Eqs. (12.23) and (12.24). This function was added based on the assumption that
it becomes harder to change the state of the devices as the state variable approaches
the boundaries. This idea was theorized in [14, 17], and demonstrated experimentally
in [27]. Also, this function provides the possibility of modeling the motion of the
state variable differently depending on the polarity of the input voltage. This is a
necessary addition as it has been experimentally veriﬁed that the state variable mo-
tion is not equivalent in both directions [27]. The memristor device model published
in [24] also uses switching state variable where the motion varies depending on the
polarity of the current through the device. One possible explanation for this may be
that it is more difﬁcult to put ions back in their original position after they have been
previously moved. When ηV(t) > 0, the state variable motion is described by Eq.
(12.23), otherwise the motion is described by (12.24). The term η was introduced
to represent the direction of the motion of the state variable relative to the voltage
polarity. When η = 1, a positive voltage (above the threshold) will increase the value
of the state variable, and when η = −1, a positive voltage results in a decrease in
state variable. A similar technique was introduced in [14].
The function f (x(t)) was developed by assuming the state variable motion was
constant up until the point xp or xn. At this point the motion of the state variable was
limited by a decaying exponential function. Since the motion of the state variable
appears to be different across the different types of devices studied, this function
used ﬁtting parameters to accommodate the variety. The constants in this equation
represent the point where the state variable motion becomes limited (xp and xn), and
the rate at which the exponential decays (αn and αp). These differences may be due to
the fact that the motion of the state change in a chalcogenide device is very different
than the motion of ions or dopants.
f (x(t)) =

e−αp(x(t)−xp)wp(x(t), xp),
x(t) ≥xp
1,
x(t) < xp
(12.23)
f (x(t)) =

eαn(x(t)+xn−1)wn(x(t), xn),
x(t) ≤1 −xn
1,
x(t) > 1 −xn
(12.24)
In Eq. (12.25), wp(x, xp) is a windowing function that ensures f (x) equals zero when
x(t) = 1. In Eq. (12.26), wn(x, xn) keeps x(t) from becoming less than 0 when the

236
C. Yakopcic et al.
current ﬂow is reversed.
wp(x, xp) = xp −x
1 −xp
+ 1
(12.25)
wn(x, xn) =
x
1 −xn
(12.26)
Equation (12.27) is used to model the state variable motion in each of the memristor
devices. Since the modeled state variable must match devices with many different
physical structures, this equation is very different than the equation in [2] that was
used to model only TiO2 devices. The term η is also used in (12.27) to determine the
direction of the dynamic state variable motion.
dx
dt = ηg(V (t))f (x(t))
(12.27)
12.6.2
SPICE Code for Generalized Memristor Model
The SPICE code for this memristor model can be seen Fig. 12.29.The circuit structure
is equivalent to the one displayed in Fig 12.4, although the equations for I(t) and
IGx(t) differ in this model.
12.6.3
Generalized Memristor Model Results
To display the functionality of this SPICE model, each of the published memristor
devices [7–12] was modeled in a simple I–V simulation. The circuit used to test the
model was equivalent to the one displayed in Fig. 12.5.
Figures 12.30 and 12.31 show the simulation results of the model when it was
used to match the characterization data published in [9]. The characterization data
from [9] was reproduced in Fig. 12.32 for convenience. Figure 12.30 displays the ﬁrst
simulation result where the device in [9] was modeled with a sinusoidal input both
at 100 Hz and 100 kHz. The hysteresis in the model diminished when the frequency
was increased to 100 kHz just as it did in [9]. The simulated I–V characteristic was
matched to the 100 Hz data provided in [9] using selected data points (dots in the I–V
curve in Fig. 12.30) with an average error of 84.8 μA (6.64 %). The percent error was
determined by ﬁrst calculating the sum of the differences between the model output
current, and the current at each of the selected data point from the characterization.
This value was then divided by the sum of the currents at each of the selected points
to determine relative error.
Figure12.31showsthesimulationresultwhenmatchingthemodeltotherepetitive
pulse input data also provided in [9]. In this case, the model was able match the
characterization data with an average error of 32.7 μA (6.66 %). The characterization

12
Memristor SPICE Modeling
237
Fig. 12.29 LTspice subcircuit for the generalized memristor device model proposed in [13]
data provided for this simulation was only available for when positive voltage sweeps
were applied. It was assumed that the parameters for the negative regime would
closely match the parameters used in the sinusoidal simulation in Fig. 12.30. The
one exception was that the negative conductivity parameter a2 was set to the value
decided for a1 in the pulsed simulation.

238
C. Yakopcic et al.
-0.4 -0.2
0
0.2
0.4
0.6
-2
-1
0
1
2
3
4
Voltage (V)
Current (mA)
100 Hz
100 Hz Targets
100 kHz
0
10
20
30
40
-3
-2
-1
0
1
2
3
4
time (ms)
Current (mA)
-0.6
-0.4
-0.2
0
0.2
0.4
0.6
0.8
Voltage (V)
Current
Voltage
Fig. 12.30 Simulation results when modeling the device in [9] for a sinusoidal input (dots in I–V
curve show target data points). The plots show the current and voltage waveforms and the I–V
curve. The I–V curve for a high frequency input where the device behaves as a linear resistor is also
displayed. In this simulation: V p = 0.16 V, V n = 0.15 V, Ap = 4000, An = 4000, xp = 0.3, xn = 0.5,
αp = 1, αn = 5, a1 = 0.17,a2 = 0.17,b = 0.05, x0 = 0.11, and η = 1
-0.2 -0.1
0
0.1
0.2
0.3
-1
-0.5
0
0.5
1
Voltage (V)
Current (mA)
0
10
20
30
40
50
-1
-0.5
0
0.5
1
time (ms)
Current (mA)
-0.3
-0.15
0
0.15
0.3
Voltage (V)
Current
Voltage
Fig. 12.31 Results obtained for matching the pulsed input characterization in [9]. The plots again
show the voltage and current waveforms as well as the I–V curve. Dots in the I–V curve show
the points from [9]. In this simulation: V p = 0.16 V, V n = 0.15 V, Ap = 4000, An = 4000, xp = 0.3,
xn = 0.5, αp = 1, αn = 5, a1 = 0.097,a2 = 0.097,b = 0.05, x0 = 0.001, and η = 1
Fig. 12.32 Characterization data to which the model was matched in Figs. 12.30 and 12.31. Each
of these plots showing physical characterization data were reproduced from [9] with permission
from the authors

12
Memristor SPICE Modeling
239
0
0.5
1
1.5
2
-200
-150
-100
-50
0
50
100
150
200
250
300
time (s)
-2
-1.5
-1
-0.5
0
0.5
1
1.5
2
2.5
3
Voltage (V)
-2
-1
0
1
2
-200
-100
0
100
200
300
Voltage (V)
Current (μA)
Current (μA)
Current
Voltage
Fig. 12.33 Simulation results when modeling the device in [10] for a cyclical DC sweep where the
plots show the voltage and current waveforms and the I–V curve (dots in the I–V curve show target
data points). V p = 1.2 V, V n = 0.6 V, Ap = 5, An = 30, xp = 0.7, xn = 0.8, αp = 4, αn = 24,
a1 = 2.3(10−4), a2 = 3.8(10−4), b = 1, x0 = 0.02, and η = 1
When comparing the many parameters used to model the device characterized in
[9], it can be seen that very few adjustments were necessary when switching between
the two modes of operation. Out of the total 12 parameters, 9 of them remained the
same between the simulations in Figs. 12.30 and 12.31. The three parameters that
werechangedincludetheconductivityparametersa1 anda2, andtheinitialpositionof
thestatevariable x0. Eachoftheseparametersishighlyrelatedtothedevicethickness,
and these differences could have been caused by non-uniformities in the wafer.
The simulation in Fig. 12.33 was based on the characterization data provided in
[10] where cyclic voltage sweeps were applied to the memristor device. The average
error in this case was determined to be 8.63 μA (13.6 %). The error dropped to 8.72 %
when not considering the largest outlier. The largest discrepancy in this simulation
was caused by the lack of curvature in the model when the device was in a conductive
state. For convenience, the characterization data that was published in [10] has been
reproduced in Fig. 12.34.
Fig. 12.34 Characterization
data to which the model was
matched to produce the result
in Fig. 12.33. (The plot was
reproduced from [10] with
permission from the authors)

240
C. Yakopcic et al.
-1.5
-1
-0.5
0
0.5
1
1.5
-40
-20
0
20
40
60
Voltage (V)
0
1
2
3
4
5
6
7
8
-40
-30
-20
-10
0
10
20
30
40
50
60
time (s)
-2
-1.5
-1
-0.5
0
0.5
1
1.5
2
2.5
3
Voltage (V)
Current (μA)
Current (μA)
Current
Voltage
Fig. 12.35 Results obtained for matching the device characterization in [7]. The plots show the
voltage and current waveforms and the I–V curve where the dots in the I–V curve show target points
from [7]. In this simulation: V p = 0.9 V, V n = 0.2 V, Ap = 0.1, An = 10, xp = 0.15, xn = 0.25, αp = 1,
αn = 4, a1 = 0.076, a2 = 0.06, b = 3, x0 = 0.001, η = 1
Fig. 12.36 Characterization
data from HP Labs to which
the memristor model was
matched to produce the result
in Fig. 12.35. (This ﬁgure was
reproduced from [7] with
permission from the author)
The simulation results for the device characterized in [7] can be seen in Fig. 12.35.
In this case the characterization was done using repetitive pulses as opposed to a
cyclic voltage sweep. The model was able to match the selected data points from
the characterization data with an average error of 1.89 μA (11.66 %). In Fig. 12.35,
it can be seen that several of the repetitive pulses have different peak amplitudes
throughout the simulation. This was done to better match the characterization data,
as the I–V curve published in [7] also had varying peak voltages (see Fig. 12.36).
The memristor simulation in Fig. 12.37 was based on the device characterized in
[8]. This device was characterized in [8] using a slower pulse train that required about
20 s to complete. The simulation matches each target data point with an average error
of 20.0 nA (6.21 %). Figure 12.38 displays a reproduction of the characterization
data from [8] to which the model was matched.
The plots in Fig. 12.39 correspond to the memristor developed in [11, 12]. The
simulated I–V characteristic was matched to target data points with an average error
of 5.97 %. The input voltage waveform was replicated based on the data provided

12
Memristor SPICE Modeling
241
0
5
10
15
-0.5
-0.25
0
0.25
0.5
0.75
1
time (s)
-3
-1.5
0
1.5
3
4.5
6
Voltage (V)
-3
-2
-1
0
1
2
3
4
-0.5
0
0.5
1
Voltage (V)
Current (μA)
Current (μA)
Current
Voltage
Fig. 12.37 Results obtained for matching characterization in [8]. Dots show the points from [8].
In this simulation: V p = 2.1 V, V n = 0.8 V, Ap = 0.03, An = 0.08, xp = 0.3, xn = 0.5, αp = 1, αn = 3,
a1 = 1.59(10−7), a2 = 2.15(10−7), b = 0.7, x0 = 0.2, η = 1
Fig. 12.38 Physical device characterization data published in [8]. (This ﬁgure was reproduced with
permission from the authors)
0
5
10
15
20
25
-80
-60
-40
-20
0
20
40
60
80
time (s)
Current (mA)
-1
-0.75
-0.5
-0.25
0
0.25
0.5
0.75
1
Voltage (V)
-1
-0.5
0
0.5
1
-80
-60
-40
-20
0
20
40
60
Voltage (V)
Current (mA)
Current
Voltage
Fig. 12.39 Results obtained for matching the characterization in [11], [12]. The plots again show
the voltage and current waveforms as well as the I–V curve. Dots in the I–V curve show the points
from [11, 12]. In this simulation: V p = 0.65 V, V n = 0.56 V, Ap = 16, An = 11, xp = 0.3, xn = 0.5,
αp = 1.1, αn = 6.2, a1 = 1.4, a2 = 1.4, b = 0.05, x0 = 0.99, and η = −1

242
C. Yakopcic et al.
Fig. 12.40 I–V characteristic
published in [12]. (This ﬁgure
was reproduced with
permission from the authors)
in [12]. The I–V characteristic in [11, 12] shows 3 sequential voltage sweeps. A
reproduction of this I–V characteristic obtained from [11] can be seen in Fig. 12.40.
Figure 12.39 shows the results when modeling target data from the third sweep since
the decay in the ﬁrst two sweeps is most likely due to initial forming and would
not be present over a large number of cycles. Contrary to the previous simulations,
this device was characterized so that the device conductivity decreases as positive
voltage is applied. To accommodate for this, the variable η was set to −1.
12.7
Conclusion
When comparing all of the models, it can be seen that there have been several different
techniques proposed for modeling memristor devices. Each of the techniques was
validated based on either the matching of the published characterization data, or by
modeling behaviors observed in memristor devices.
The models proposed in Sect. 12.3 were based on directly relating the ionic drift
in the oxide layer to the overall device resistance. These models provide a simple
explanationofmemristorbehaviorthatrelatesverycloselytothetheoryﬁrstproposed
by Dr. Chua. Although, these models appear to have the least in common with the
published characterization data for different memristor devices.
The models in Sect. 12.4 show a very close correlation to the characterization
data of a speciﬁc memristor device, although little is known about how well these
models function for alternative device structures and voltage inputs.
The models in Sect. 12.5 show how using the hyperbolic sine function in the I–V
relationship provides a simple and effective means for modeling the MIM junction
found within a memristor device. These models appear to match memristor behavior
especially well when repetitive pulsed inputs are applied, but these models have not
been numerically correlated to any fabrication data.

12
Memristor SPICE Modeling
243
Section 12.6 provides a more generalized SPICE model that is also capable match-
ing the I–V characteristic of several different devices. The disadvantage of this model
is that it has less theoretical correlation to the physical mechanisms governing the
device when compared to the HP Labs MIM model or the University of Michigan
model.
The goal of this chapter was to review existing memristor modeling techniques for
use in SPICE simulations and circuit design. The models that were discussed in this
chapter can be implemented in LTspice using the subcircuits provided. This allows
for a more standardized comparison of memristor models that can all be implemented
in a single SPICE program.
Based on the results, it is the authors’ recommendation that the model [13]
discussed in Sect. 12.6 be used for the most accurate representation of published
memristor current–voltage data. It has been shown that this model can be applied
to several different materials and device structures. This will become useful in the
future as fabrication of memristor devices is continually changing. The ﬁtting pa-
rameters of this model could most likely be changed to accommodate for future
fabrication techniques, as it has been shown that this model can accurately match the
characterizations of a variety of memristors.
References
1. Chua LO, Leon O (1971) Memristor—The missing circuit element. IEEE Trans Circuit Theory
18(5):507–519
2. Strukov DB, Snider GS, Stewart DR,Williams RS (2008)The missing memristor found. Nature
453:80–83
3. Williams R (2008) How we found the missing memristor. IEEE Spectrum 45(12):28–35
4. Raja T, Mourad S (2009) Digital logic implementation in memristor-based crossbars.
International conference on communications, circuits, and systems, pp 939–943
5. Lehtonen E, Laiho M (2009) Stateful implication logic with memristors. IEEE/ACM
international symposium on nanoscale architectures, pp 33–36
6. Wald S, Baker J, Mitkova M, Raﬂa N (2011)A non-volatile memory array based on nano-ionic
conductive bridge memristors. IEEE workshop on microelectronics and electron devices, pp
1–4
7. Snider GS (2008) Cortical computing with memristive nanodevices. SciDAC Rev 10:58–65
8. Jo SH, Chang T, Ebong I, Bhadviya BB, Mazumder P, Lu W (2010) Nanoscale memristor
device as synapse in neuromorphic systems. Nano Lett 10(4):1297–1301
9. Oblea AS, Timilsina A, Moore D, Campbell KA (2010) Silver chalcogenide based memristor
devices. IJCNN, pp 1–3
10. Yang JJ, Pickett MD, Li X, Ohlberg DAA, Stewart DR, Williams RS (2008) Memristive
switching mechanism for metal/oxide/metal nanodevices. Nat Nanotechnol 3:429–433
11. Miller K, Nalwa KS, Bergerud A, Neihart NM, Chaudhary S (2010) Memristive behavior in
thin anodic titania. IEEE Electron Dev Lett 31(7):737–739
12. Miller K (2010) Fabrication and modeling of thin-ﬁlm anodic titania memristors. Master’s
Thesis, Iowa State University, Electrical and Computer Engineering (VLSI), Ames
13. Yakopcic C, Taha TM, Subramanyam G, Pino RE, Rogers S (2011) A memristor device model.
IEEE Electron Dev Lett 32(10):1436–1438 (Accepted for publication)
14. JoglekarYN, Wolf SJ (2009) The elusive memristor: properties of basic electrical circuits. Eur
J Phys 30(661)

244
C. Yakopcic et al.
15. Laiho M, Lehtonen E, Russel A, Dudek P (2010) Memristive synapses are becoming reality,
Institute of Neuromorphic Engineering, The Neuromorphic Engineer, A publication of INE-
WEB.org,
10.2417/1201011.003396.
http://www.ine-news.org/view.php?source=003396-
2010-11-26
16. Pino RE, Bohl JW, McDonald N, Wysocki B, Rozwood P, Campbell KA, Oblea A, Timilsina
A (2010) Compact method for modeling and simulation of memristor devices: ion conductor
chalcogenide-based memristor devices. IEEE/ACM international symposium on nanoscale
architectures, pp 1–4
17. Biolek Z, Biolek D, Biolková V (2009) Spice model of memristor with nonlinear dopant drift.
Radioengineering 18(2):210–214
18. Lehtonen E, Laiho M (2010, February) CNN using memristors for neighborhood connections,
pp 1–4
19. Batas D, Fiedler H (2011)A memristor spice implementation and a new approach for magnetic
ﬂux-controlled memristor modeling. IEEE Trans Nanotechnol 10(2):250–225
20. Rak A, Cserey G (2010, April) Macromodeling of the memristor in spice. Comput Aided Des
Integr Circ Syst IEEE Trans 29(4):632–636
21. Benderli S, Wey T (2009) On SPICE macromodelling of TiO2 memristors. Electron Lett
45(7):377–379
22. Mahvash M, Parker AC (2010, August) A memristor SPICE model for designing memristor
circuits. (MWSCAS), pp 989–992
23. Chang T, Jo SH, Kim KH, Sheridan P, Gaba S, Lu W (2011) Synaptic behaviors and modeling
of a metal oxide memristor device. Appl Phys A 102:857–863
24. Abdalla H, Pickett MD (2011) SPICE Modeling of Memristors. ISCAS, pp 1832–1835
25. Shin S, Kim K, Kang S-M (2010, April) Compact models for memristors based on charge-ﬂux
constitutive relationships. IEEE Trans Comput Aided Des Integr Circ Syst 29(4):590–598
26. Simmons JG (1963) Generalized formula for the electric tunnel effect between similar
electrodes separated by a thin insulating ﬁlm. J Appl Phys 34(6):1793–1803
27. Pickett MD, Strukov DB, Borghetti JL, Yang JJ, Snider GS, Stewart DR, Williams RS (2009)
Switching dynamics in titanium dioxide memristive devices. J Appl Phys 106(7):074508

Chapter 13
Memristor Models for Pattern Recognition
Systems
Fernando Corinto, Alon Ascoli, and Marco Gilli
Abstract The design of Memristor Oscillatory Neurocomputers for pattern recogni-
tion tasks may not leave aside a preliminary thorough investigation of the nonlinear
dynamics of the whole system and its basic components. This chapter yields novel in-
sights into the peculiar nonlinear dynamics of different memristor models.A detailed
mathematical treatment aimed at highlighting the key impact the initial condition on
the ﬂux across a memristor with odd-symmetric charge-ﬂux characteristic has on the
development of a particular dynamical behavior. It is proved how, driving the mem-
ristor with a sine-wave voltage source, the amplitude–angular frequency ratio selects
a sub-class of observable current–voltage behaviors from the class of all possible dy-
namics, while the initial condition on ﬂux speciﬁes which of the behaviors in the
sub-class is actually observed. In addition, a novel boundary condition-based model
for memristor nano-scale ﬁlms points out how speciﬁcation of suitable dynamical
behavior at ﬁlm ends, depending on the particular physical realization under study
and on driving conditions, crucially determines the observed dynamics.
13.1
Introduction
One of the most fascinating areas of research is the strive for designing a neuromor-
phic or brain-simulating system, where specialized algorithms run on a brain-inspired
microprocessor may replicate the dynamical behavior of the vast network of synapse-
coupled neurons inside the grey matter within the skull of a human being (a synapse
is the junction between the dendrite or input of a neuron and the axon or output of a
neighboring neuron) [1, 2].
Current digital supercomputers are able to reproduce some of the brain function-
alities, but, as more and more intelligent operations are emulated, the consequent
F. Corinto () · A. Ascoli · M. Gilli
Politecnico di Torino, Corso Duca degli Abruzzi 24, 10129, Torino, Italy
e-mail: fernando.corinto@polito.it
A. Ascoli
e-mail: alon.ascoli@polito.it
M. Gilli
e-mail: marco.gilli@polito.it
R. Kozma et al. (eds.), Advances in Neuromorphic Memristor Science and Applications,
245
Springer Series in Cognitive and Neural Systems,
DOI 10.1007/978-94-007-4491-2_13, © Springer Science+Business Media Dordrecht 2012

246
F. Corinto et al.
increases in power consumption and integrated circuit area are somewhat unaccept-
able. These increases are due to the inefﬁcient classical Von-Neumann architecture,
which is at the basis of the machine design and is characterized by a rigid adherence to
Boolean logic and, above all, by the separation between the physical location where
data are stored, i.e. the memory, and the physical location where data processing
takes place, i.e. the Computing Power Unit (CPU) [3].
In addition to that, complex integration of information performed by biologi-
cal neural systems is based on several dynamical mechanisms [4]. Among them,
the most worth is the synchronization of neural activity [5–7]. Synchronization of
neural activity is also one of the proposed solutions to a widely discussed ques-
tion in neuroscience: the binding problem, i.e. how our brain bind all the different
data together to recognize objects [8]. In particular, temporal synchrony (or tem-
poral correlation) results to be the fundamental mechanism to unfold the binding
problem [9].
Usually such dynamical binding is represented by synchronous oscillatory
neurons or oscillatory neurons that are phase related to an external ﬁeld.
Despite remarkable progress has been made in the ﬁelds of Neurophysiology and
Nonlinear Dynamics towards a full understanding of neural structures and interac-
tions, current brain-simulating systems require high-computational capabilities to
reproduce only a few brain functionalities.
Completely new neuromorphic computing systems, able to ﬁll the main gap
between biological brain-computation and current brain-simulating computation,
require the combination of power-efﬁcient and size-effective synapse circuitry
with conventional oscillatory neurocomputers, i.e. arrays of a large number of
interconnected units performing nonlinear transformations in parallel.
To date, memory-resistors (memristors for short) represents the latest technol-
ogy breakthrough enabling the realization of electronic devices with characteristics
showing an intriguing resemblance to biological synapses. On the basis of symmetry
and logics arguments, back in 1971 Prof. L. O. Chua postulated the existence of
memristor with conductivity depending on the time history of the charge through
(ﬂux across) it [10]. If no source drives the device, its conductance keeps constant.
In other words, a memristor has the ability to indeﬁnitely store its conductance
and is thus named memory-resistor. The ﬁrst conscious experimental observation
of memristor behavior at the nano-scale was announced by HP Labs in 2008 [11].
The physical realization of the device was based on a very thin oxide ﬁlm made up
of an insulating layer of stoichiometric titanium dioxide (TiO2) and of a conductive
layer of oxygen-poor titanium dioxide (TiO2 −x, x = 0.05). The dynamical behavior
of this passive two-terminal element under an external current source was based on
the combined interaction between electronic and ionic transport within the thin oxide
ﬁlm.
The combination of memristor synapse circuitry with oscillatory neurocomputers,
which we name Memristor Oscillatory Neurocomputers—let us coin the acronym
MONs—may achieve the large connectivity and highly-parallel processing power
of biological systems [12]. It turns out that the potential applications of MONs are
astonishing: intelligent adaptive control, decision making, intelligent user interfaces,

13
Memristor Models for Pattern Recognition Systems
247
to name but a few. Unlike a Von-Neumann machine, MONs do not just execute a list
of commands or program. Their major aim is not general-purpose computation, but
pattern recognition based upon associative memory retrieved through a sequence of
feature extraction algorithms. MONs may carry out pattern recognition tasks even
in presence of corrupted information [13].
These completely new brain-simulating systems present computational capabili-
ties similar to those of biological systems, because memristors permit to bring data
close to computation (the way biological systems do) using very little power to store
information (just as the brain does).
Recently, it was experimentally proved that hybrid MONs made up of comple-
mentary metal-oxide-semiconductor (CMOS) neurons and nano-scale memristor
synapses are able to support spike timing dependent plasticity (STDP) [14], an
important synaptic adaptation rule for competitive Hebbian learning [15].
An efﬁcient design of a MON may not leave aside a preliminary thorough inves-
tigation of the nonlinear dynamics of the whole system and its basic components. A
deep investigation of the nonlinear dynamics of simple MONs is reported in [16–19].
Let us assume the following mathematical model for a MON arranged in a two-
dimensional regular grid with N · M oscillators (each oscillator Oij is deﬁned by
indexes i = {1, . . . , N} and j = {1, . . . , M}):
dxij(t)
dt
= F(xij(t)) +

kl
wklC(ykl(t))G(xij(t), xkl(t))
(13.1)
dyij(t)
dt
= h(yij(t), xij(t), xkl(t))
(13.2)
where xij = (x1
ij . . . , xn
ij )′ ∈Rn is the state vector1 of oscillator Oij, symbol ′
denotes the transposition operation, and F(·) : Rn →Rn deﬁnes the nonlinear rate
of change of the state vector of the uncoupled oscillator. As k and l vary—(k, l) ∈
{(p, q) : p = 1, . . . , N, q = 1, . . . , M}—C(ykl)G(xij, xkl), with C(·) : R →R
and G(·, ·) : Rn × Rn →Rn, represent the memristor-based synaptic interactions
that oscillator Oij entertains with all oscillators Okl, coefﬁcients wkl = {0, 1} allow
the speciﬁcation of the topology of the MON, while ykl stand for the state variables
associated with memristor synapses mkl responsible for the couplings among the
oscillators of the MON.
Equation (13.2) models the dynamics of state yij of synapse mij, while function
h(yij, xij, xkl) : R × Rn × Rn →R depends on the physical principles underlying
the behavior of the practical memristor realization. Without loss of generality, we
assume that h(·) depends only on one (for instance the ﬁrst) variable of the state
vectors of the oscillators, i.e. (13.2) is recast as
dyij(t)
dt
= h(yij(t), x1
ij (t), x1
kl(t)).
(13.3)
1 For the sake of simplicity, in the following time dependency is omitted unless it is strictly necessary.

248
F. Corinto et al.
It turns out that the behavior of a MON is deeply inﬂuenced by the memristor
dynamics. This manuscript aims to unfold the inﬂuence of initial conditions and
boundary conditions on the memristor current–voltage characteristic. With this goal
we consider a couple of different models proposed in literature to describe memristor
dynamics. We brieﬂy review these two models in Sect. 13.2, while the inﬂuence of
initial conditions on the ﬁrst and of boundary conditions on the latter is investigated in
Sects. 13.3, 13.4 and 13.5 respectively. Finally, conclusions and future perspectives
are drawn in Sect. 13.6.
13.2
Review of Memristor Models
The memristor was theoretically introduced in 1971 [10] and then classiﬁed within
the larger class of memristive systems 5 years later [20]. The current- or voltage-
controlled memristor is mainly described by Ohm’s law with resistance depending
on the charge q through it or on the ﬂux ϕ across it, i.e.
⎧
⎪⎨
⎪⎩
v(t) = R(q(t))i(t)
(13.4a)
dq(t)
dt
= i(t)
(13.4b)
or
⎧
⎪⎨
⎪⎩
i(t) = W(ϕ(t))v(t)
(13.5a)
dϕ(t)
dt
= v(t)
(13.5b)
where R(q) is the memristance and W(ϕ) = R−1(q) is the memductance. Without
loss of generality we consider the voltage-controlled memristor deﬁned in (13.5a)–
(13.5b). Using the formalism of Sect. 13.1, (13.5b) is readily derived from (13.3) by
letting yij = ϕ and h(yij(t), x1
ij (t), x1
kl(t)) = x1
ij −x1
kl = v.Among the models proposed
in literature to deﬁne the memductance, one of the most used is:
W(ϕ) = α + βϕ2.
(13.6)
This memductance has been realized by means of a circuit using passive and active
elements [21], but it takes into to account only basic physical mechanism underlying
nano-scale memristors built by William’s research group at HP Labs. In particu-
lar, this model assumes constant velocity for the ionic transport (i.e. linear drift)
throughout the entire length of the controlled device.
This model, frequently adopted in literature [22–23] for its simplicity, does not
take into account memristor behavior at the ends, the so-called boundary conditions,
which need to be speciﬁed due to physical limits of the nano-device.
Literature was later enriched with a number of nonlinear dopant drift models.
Those proposed by Williams himself [11] and Joglekar [24] introduce a decrease in

13
Memristor Models for Pattern Recognition Systems
249
rate of ionic transport as layer boundary approaches any of the two ends, but include
unrealistic boundary conditions.
On the other end, memristor dynamics at the ends are appropriately reproduced
by Biolek’s model [25], where dopant drift rate depends on a discontinuous win-
dow decreasing towards 0 as layer boundary approaches any of the two ends and
exhibiting vertical upward transitions each time source reverses polarity. However,
by admission of Biolek himself, this model is unable to replicate all the nonlinear
behaviors classiﬁed in [11]. In fact, no model hitherto available in literature exhibits
such capability.
We propose a novel, accurate, simple and general memristor model able to repro-
duce a number of memristor behaviors recently observed in experiments on distinct
physical nano-scale double-layer systems. Let D be the entire length of the thin ox-
ide ﬁlm, l(t) ∈[0,1] the normalized length of the oxygen-deﬁcient oxide layer, the
memristor model can be written as (see [26] for more details):
i(t) = W(l(t))v(t)
(13.7)
dl(t)
dt
=
⎧
⎨
⎩
1
io
ηi(t)
if C1 holds,
0
if C2 or C3 holds,
(13.8)
where t denotes normalized time, i0 is the current normalization factor, η ∈{−1, + 1]
stands for the polarity coefﬁcient2 and conditions Ck (k = 1, 2, 3) are expressed by
C1 = {l ∈(0, 1) or (l = 0 and v > vth,0)
or (l = 1 and v < −vth,1)},
(13.9)
C2 = {l = 0 and v ≤vth,0},
(13.10)
C3 = {l = 1 and v ≥−vth,1},
(13.11)
while vth,0 and vth,1 denote the magnitudes of threshold voltages at device ends l = 0
and l = 1 respectively.
Our unique model revolves around the linear dopant drift assumption, but employs
suitable boundary conditions (vth,0 and vth,1) which may be tuned depending on the
memristor behavior at the ends. This behavior depends on various factors related to
the process of fabrication of the memristor nano-structure, including type of ﬁlm
material, kind and mobility of dopants and device length (the smaller the length the
higher the electric ﬁeld developing across the conductive layer under application
of a controlling source and the more pronounced the observed nonlinear behaviors
may be).
2 The behavior of the memristor depends on the material order of the two layers of the nano-ﬁlm. In
order to take this into account a polarity coefﬁcient may be introduced in the model. Without loss
of generality, from now onwards, unless differently speciﬁed, we assume η = +1.

250
F. Corinto et al.
Fig. 13.1 Schematic
conﬁguration of MONs for
pattern recognition tasks
+
v(t)
–
single
oscillatory
cell
+
–
i(t)
v(t)
+
–
In Sects. 13.3, 13.4 and 13.5 we respectively show the inﬂuence of initial conditions
andboundaryconditionsonthecurrent–voltagecharacteristicexhibitedbymemristor
model (13.5a–b) and (13.7–13.8).
13.3
Inﬂuence of the Initial Conditions on the i – v
Characteristic of Memristor
The aim of this section is to demonstrate the crucial role of state initial condition
on memristor dynamics. This interesting property can be used to design a pat-
tern recognition system mapping memristor state initial condition to current-voltage
characteristic. Let us consider a single oscillatory cell with periodic output voltage
v(t) = v(t + T ), where T is the period. Let us develop it in Fourier series:
v(t) = v(t + T ) = a0
2 +
∞

k=1
(ak cos kωt + bk sin kωt)
(13.12)
where
ak = 2
T

T
2
−T
2
v(t′) cos kωt′dt′
k ≥0
bk = 2
T

T
2
−T
2
v(t′) sin kωt′dt′
k ≥1
(13.13)
In order to realize memristive couplings (synapses), the output of the oscillatory cell
is then applied across a voltage-controlled memristor through a voltage-follower,
whose main function is to prevent memristor from affecting oscillator dynamics (see
the circuit of Fig. 13.1, sketching the use of memristor synapses in MONs).
Let us consider a voltage-controlled memristor characterized by the following
charge–ﬂux relationship:
q(ϕ) = αϕ + β
3 ϕ3.
(13.14)
Assuming that harmonics of order higher than the ﬁrst may be neglected and using
(13.12), the memristor is excitealpd by sine-wave voltage source (we renamed b1 as
A for simplicity):
v(t) ≈A sin ωt,
(13.15)

13
Memristor Models for Pattern Recognition Systems
251
Integrating both sides in (13.15), we derive the expression for ﬂux as function of
time:
ϕ(t) =
 t
−∞
v(t′)dt′ = ϕ(0) + A
ω (1 −cos ωt).
(13.16)
where the initial condition is
ϕ(0) =
 0
−∞
v(t′)dt′.
(13.17)
Note that the state of the voltage-controlled memristive system ϕ(t) may only assume
values within a certain closed interval, i.e.
ϕ(t) ∈

ϕ(0), ϕ(0) + 2A
ω

.
(13.18)
Memductance W(ϕ) is by deﬁnition the ﬂux derivative of charge. Using (13.14), we
have (see also equation (13.6)):
W(ϕ) = dq(ϕ)
dϕ
= α + βϕ2.
(13.19)
Applying the chain rule, current through memristive element may be recast as
i =W(ϕ)v. Using (13.19), (13.16) and (13.15), i has the following time dependence:
i(t) = (α + βϕ2(t))v(t) =
=

α + β A2
ω2 (1 −cos ωt)2 + βϕ2(0) + 2βϕ(0)A
ω (1 −cos ωt)

A sin ωt.
(13.20)
From (13.20) it follows that memristor current depends on the controlling source
through A/ω ratio, on memristive nonlinearity (13.14) through coefﬁcients α and β
and on state initial condition ϕ(0). For sake of completeness we ﬁnally report the
expression for q(t), derivable from (13.14) and (13.16):
q(t) =

ϕ(0) + A
ω (1 −cos ωt)
 #
α + β
3

ϕ(0) + A
ω (1 −cos ωt)
2$
.
(13.21)
Note that the initial condition on charge depends on state initial condition ϕ(0), being
given by
q(0) = ϕ(0)

α + β
3 ϕ2(0)

.
(13.22)
Figure 13.2 shows the dependence of i–v behavior on the initial condition on the state
of a memristor with q–ϕ relationship (13.14) where α = 1CWb−1 and β = 1CWb−3.

252
F. Corinto et al.
−10
−5
0
V
V
V
V
5
10
−200
0
i
i
i
i
200
−10
−5
0
5
10
−200
0
200
−10
−5
0
5
10
−10
−5
0
5
10
−400
−200
0
200
400
−1000
0
1000
a
b
c
d
Fig. 13.2 Different current–voltage behaviors featured by a voltage-controlled memristor with q–ϕ
relationship (13.14) where α = 1CWb−1 and β = 1CWb−3 for different ﬂux initial conditions, i.e.
for ϕ(0)respectively set to −7.75 in (a), −7.5 in (b), −5.5 in (c) and 0 in (d). Amplitude A and
angular frequency ω of sine-wave voltage source are respectively chosen as 7.5 V and 1 rad s−1.
Note that the range of variation of the i variable is not identical for the four subplots
In order to get a deeper understanding of the role of state initial condition on mem-
ristive dynamics, we shall consider a piecewise linear approximation of charge–ﬂux
characteristic (13.14):
q(ϕ) = bϕ + a −b
2
(|ϕ + ϕc| −|ϕ −ϕc|),
(13.23)
where a and b (let us assume a > b > 0) respectively are the slopes of the characteristic
within and outside open interval (−ϕc, ϕc). The absolute value of each interval limit,
i.e. ϕc, is named critical ﬂux. Inserting (13.16) into (13.23) the time dependence of
q is found to be expressed by
q(t) =
⎧
⎨
⎩
b

ϕ(0) + A
ω (1 −cos ωt)

+ (a −b)ϕc
∀t : ϕ(t) ≥ϕc
a

ϕ(0) + A
ω (1 −cos ωt)

∀t : |ϕ(t)| < ϕc
b

ϕ(0) + A
ω (1 −cos ωt)

−(a −b)ϕc
∀t : ϕ(t) ≤−ϕc
(13.24)
Using (13.23), memductance is expressed as
W(ϕ) = b + a −b
2
[sgn(ϕ + ϕc) −sgn(ϕ −ϕc)],
(13.25)
Depending on the system state, memductance may only assume one in two possible
values, i.e. a high one (let us call it Whigh) equal to a for ϕ ∈(−ϕc, ϕc) and a low one

13
Memristor Models for Pattern Recognition Systems
253
(let us call it Wlow) equal to b otherwise. Inserting (13.16) into (13.25), applying the
chain rule and using (13.15), the expression for i as function of time is
i(t) =

bA sin ωt
∀t : |ϕ| ≥ϕc
aA sin ωt
∀t : |ϕ| < ϕc
(13.26)
From (13.26) memristor current depends on input source (13.15), on memristor
nonlinearity (13.23) and on memristor state initial condition (embedded in the ex-
pression for ϕ). Therefore in the following analysis we shall ﬁrst sweep A/ω ∈R+.
Given (13.23), intervals of interest for such parameter are the following:
•
(0, ϕc);
•
[ϕc, 2ϕc];
•
[2ϕc, ∞].
Each of these intervals speciﬁes a sub-class of memristive output i-input v character-
istics from the class of all possible dynamics. The actual behaviors occurring within
each sub-class is then determined by sweeping ϕ(0) ∈R.
In the next section we shall analytically study the inﬂuence of the state initial
condition on the dynamics of the ﬁrst sub-class [27]. The behaviors in the other two
sub-classes are thoroughly investigated in [28].
13.4
Sub-class of i – v Characteristics for A/ω ∈(0, ϕc)
The input amplitude–angular frequency ratio lies within open interval (0, ϕc). Here
we shall sweep the state initial condition across its entire existence interval, that
is R. In the numerical examples memristive parameters a, b and ϕc in (13.23) are
respectively set to 0.005C Wb−1, 0.002C Wb−1 and 1Wb, while values chosen for
input parameters A and ω in (13.15) respectively are 2 V and 2π rad s−1 (oscillation
period, let us call it T, equals 1 s).
13.4.1
Initial Condition Larger Than or Equal to the Critical
Flux, i.e. ϕ(0) ≥ϕc
Case 13.4.1 assumes ϕ(0) ≥ϕc. From (13.18) it follows that ϕ(t) ≥ϕc ∀t ≥0. As a
result, (13.24) and (13.26) respectively yield q(t) = b[ϕ(0) + A/ω (1 −cosωt)] +
(a−b)ϕc and i(t) = bA sin ωt ∀t ≥0. The i–v characteristic is a b-slope line passing
through the origin and indicating highly-resistive behavior (solid line in Fig. 13.5a).
In fact (13.25) yields W(ϕ(t)) = Wlow = b ∀t ≥0.

254
F. Corinto et al.
13.4.2
Initial Condition Smaller Than the Critical Flux by at
Most Twice the Input Amplitude–Angular Frequency
Ratio, i.e. ϕc −2A/ω ≤ϕ(0) < ϕc
In case 13.4.2 the hypothesis on memristor state initial condition is
ϕ(0) ∈

ϕc −2A
ω , ϕc

.
(13.27)
Recalling (13.18), after some algebraic manipulation on (13.16) we may deduct that
over one period
ϕ(t) > ϕc
∀t :
2α < ωt < 2π −2α
(13.28)
ϕ(t) = ϕc
for t :
ωt = 2α
or
ωt = 2π −2α
(13.29)
−ϕc < ϕ(t) < ϕc
∀t :
0 ≤ωt < 2α
or
2π −2α < ωt < 2π
(13.30)
where
α = arcsin
!
ω(ϕc −ϕ(0))
2A
"
.
(13.31)
Using (13.24) and (13.26), the expressions for q and i as function of time respectively
are
q(t) =
⎧
⎪⎪⎨
⎪⎪⎩
b[ϕ(0) + A
ω (1 −cos ωt)] + (a −b)ϕc
∀t :
2α ≤ωt ≤2π −2α
a[ϕ(0) + A
ω (1 −cos ωt)]
∀t :
⎧
⎨
⎩
0 ≤ωt < 2α
or
2π −2α < ωt < 2π
(13.32)
and
i(t) =

bA sin ωt
∀t :
2α ≤ωt ≤2π −2α
aA sin ωt
∀t :
0 ≤ωt < 2α
or
2π −2α < ωt < 2π
(13.33)
From (13.25) time dependence of memductance may be inferred:
W(ϕ(t)) =

Wlow = b
∀t :
2α ≤ωt ≤2π −2α
Whigh = a
∀t :
0 ≤ωt < 2α
or
2π −2α < ωt < 2π
(13.34)
Over one period, a couple of memductance transitions are observed: the ﬁrst is a
Whigh →Wlow transition and occurs at t = 2α/ω, the second is a Wlow →Whigh
transition and occurs at t = (2π −2α)/ω. At these transition times voltage source
respectively assumes threshold voltages
v
2α
ω

= vth = Asin(2α)
(13.35)
v
2π −2α
ω

= A sin (2π −2α) = −vth
(13.36)

13
Memristor Models for Pattern Recognition Systems
255
Current assumption on ϕ(0), i.e. (13.27), implies that α may assume values within
(0, π/2). In particular we have to distinguish among the following four sub-cases:
•
sub-case 13.4.2.1: α ∈(0, π/4) referring to ϕ(0) ∈(ϕc −A/ω, ϕc);
•
sub-case 13.4.2.2: α = π/4 referring to ϕ(0) = ϕc −A/ω;
•
sub-case 13.4.2.3: α ∈(π/4, π/2) referring to ϕ(0) ∈(ϕc −2(A/ω), ϕc −A/ω);
•
sub-case 13.4.2.4 (a degenerate sub-case): α = π/2 referring to ϕ(0) = ϕc −
2(A/ω);
Let us investigate the i–v behaviors for these four sub-cases. Recalling the numerical
values assumed for ϕc, A and ω, ϕc −(A/ω) = 1 −(1/π)Wb while ϕc −2(A/ω) =
1 −(2/π)Wb.
13.4.2.1
Initial Condition Smaller Than the Critical Flux by Less
Than the Input Amplitude–Angular Frequency Ratio, i.e.
ϕc −A/ω < ϕ(0) < ϕc
In sub-case 13.4.2.1 we assume ϕc −(A/ω) < ϕ(0), i.e. α ∈(0, π/4). Since
0 < 2α < π/2 and (3/2) π < 2π −2α < 2π, over one period Whigh →Wlow transition
occurs before voltage relative maximum, while Wlow →Whigh transition occurs after
voltage relative minimum. As a result, using (13.15) and (13.33) and noting that
0 < vth <A, it is simple to demonstrate that the i–v plot is an atypical bow-tie with
b-slope line extensions outside the interval between the threshold voltages and clock-
wise loop rotation for v > 0 (see Fig. 13.5b). Let us gain a better understanding of the
dynamics under exam. Let us consider a numerical example where state initial condi-
tion is set to φ(0) = φc−1/π+1/4Wb, which, using (13.30) and (13.36) respectively
implies α = arcsin((1/2)√2 −π/2)) and vth = 2 sin [2 arcsin(1/2√2 −π/2)]. Fig-
ure 13.3 shows over one period the time waveforms of voltage across and current
through the memristor (plots (a) and (b) respectively) together with the resulting i–v
characteristic (plot (c)) and the memductance time behavior (plot (d)). As for the
signiﬁcance of markers, diamonds, squares and circles respectively stand for signal
relative maxima, relative minima and zeros, while asterisks and points indicate signal
values at which ϕ equals critical ﬂux ϕc with voltage source respectively crossing
thresholds vth and −vth. The markers’ time instants divide period T into numbered
time intervals (see plot (a)). In plot (c) each of such numbers is then associated to
the arrow marking the direction of the sequence of i–v trajectory points referring to
that time interval.
13.4.2.2
Initial Condition Smaller Than the Critical Flux by the Input
Amplitude–Angular Frequency Ratio, i.e. ϕ(0) = ϕc −A/ω
Sub-case 13.4.2.2 refers to ϕ(0) = ϕc −(A/ω), i.e. α = π/4. Here 2α = π/2 and
2π −2α = (3/2)π. Therefore over one period memductance transitions occur at
voltage maximum and minimum (vth =A) and the i–v plot, classiﬁed in Fig. 13.5c, is
a bow-tie with clockwise loop rotation for v > 0.

256
F. Corinto et al.
8
8.2
8.4
8.6
8.8
9
−4
−2
0
2
4
1a
1b
2
3
4a
4b
8
8.2
8.4
8.6
8.8
9
−0.01
0
0.01
−2
−1
0
1
2
−0.01
−0.005
0
0.005
0.01
8
8.2
8.4
8.6
8.8
9
2
4
6 x 10−3
c
d
b
1b
2
4a
a
1a
3
4b
Fig. 13.3 Sub-case 13.4.2.1: time behavior of v (plot (a)) and i (plot (b)), i–v characteristic (plot
(c)) and W(ϕ(t)) versus time (plot (d)) for a memristor with charge–ﬂux relationship (13.22) where
a = 0.005C Wb−1, b = 0.002C Wb−1 and ϕc = 1Wb. The memristor is controlled by voltage source
v =A sin ωt where A = 2 V and ω = 2π rad/s−1. Memristor state initial condition is ϕ(0) = 1 −
1/π + (1/4)Wb. In plots (a), (b) and (d) the waveforms are visualized over one period (T = 1 s)
from t = 8 s to discard transients
13.4.2.3
Initial Condition Smaller Than the Critical ﬂux by More Than
the Input Amplitude–Angular Frequency Ratio and by Less
Than Twice the Input Amplitude–Angular Frequency Ratio, i.e.
ϕc −2A/ω < ϕ(0) < ϕc −A/ω
In sub-case 13.4.2.3 we set ϕc −(2A/ω) < ϕ(0) < ϕc −(A/ω), i.e. α ∈(π/4, π/2).
Here π/2 < 2α < π and π < 2π = −2α < (3/2)π. Thus over one period Whigh →Wlow
and Wlow →Whigh transitions respectively occur after voltage relative maximum and
before voltage relative minimum. Since 0 < vth <A, from (13.15) and (13.33) it fol-
lows that the i–v plot is an atypical bow-tie with a-slope line extensions outside the
interval between the threshold voltages and clockwise loop rotation for v > 0 (see
Fig. 13.5d).
In order to clarify these dynamics, let us consider a numerical example where state
initial condition is ϕ(0) = 1 −(1/π) + (1/4)Wb. From (13.30) α = arcsin
((1/2)√2 + π/2)), while (13.36) yields vth =2 sin [2 arcsin((1/2)√2 + (π/2))].
Figure 13.4 shows over one period the time behaviors of memristor voltage (plot
(a)) and current (plot (b)), the i–v characteristic (plot(c)) and the memductance time
dependence (plot(d)).
13.4.2.4
Initial Condition Smaller Than the Critical Flux by Twice the Input
Amplitude–Angular Frequency Ratio, i.e. φ(0) = φc −2A/ω
In sub-case 13.4.2.4 ﬂux initial condition ϕ(0) is set to ϕc −(2A/ω), implying
α = π/2. Thus, being 2α = π and 2π −2α = π, the two memductance transitions

13
Memristor Models for Pattern Recognition Systems
257
8
8.2
8.4
8.6
8.8
9
−4
−2
0
2
4
1a
1b
2
3
4a
4b
8
8.2
8.4
8.6
8.8
9
−0.01
0
0.01
−2
−1
0
1
2
−0.01
−0.005
0
0.005
0.01
8
8.2
8.4
8.6
8.8
9
2
4
6 x 10−3
c
d
b
1b
2
4a
a
1a
3
4b
Fig. 13.4 Sub-case 13.4.2.3: time behavior of voltage (plot (a)) and current (plot (b)), current–
voltage characteristic (plot (c)) and memductance versus time (plot (d)) for the voltage-controlled
memristor deﬁned in the caption of Fig. 13.3. Memristor ﬂux initial condition is set to ϕ(0) =
1 −1/π −(1/4) Wb
−2
−1
0
1
2
−0.01
−0.005
0
0.005
0.01
−2
−1
0
1
2
−0.01
−0.005
0
0.005
0.01
−2
−1
0
1
2
−0.01
−0.005
0
0.005
0.01
−2
−1
0
1
2
−0.01
−0.005
0
0.005
0.01
c
d
b
a
Fig. 13.5 Possible input–output characteristics for A/ω ∈(0, ϕc]. Alternatively to plots (b) and
(d), loop rotation may also be counter-clockwise for ν > 0
occur at same time. Further, threshold voltage vth equals 0 V. This is a degenerate
case. The i–v plot, a signature of weakly-resistive behavior, is an a-slope line passing
through the origin (see dotted line in Fig. 13.5a).

258
F. Corinto et al.
13.4.3
Initial Condition Smaller Than the Critical Flux by More
Than Twice the Input Amplitude–Angular Frequency
Ratio and by Less Than Twice the Critical Flux, i.e.
−ϕc < ϕ(0) < ϕc −2A/ω
In case 13.4.3 we hypothesize a memristor state initial condition within open
interval (−ϕc, ϕc −2A/ω). From this and (13.18) it may be worked out that
ϕ(t) ∈(−ϕc, ϕc)∀t ≥0. As a result, using (13.24) and (13.26), charge and current
respectively are q(t) = a[ϕ(0) + (A/ω)(1 −cos ωt)] and i(t) = aA sin ωt ∀t ≥0.
The i–v characteristic is as described in Sect. 13.4.2.4 (see dotted line in Fig. 13.5a).
13.4.4
Initial Condition Smaller Than the Critical Flux by at Least
Twice the Critical Flux and by Less Than the Sum of Twice
the Critical Flux and Twice the Input Amplitude–Angular
Frequency Ratio, i.e. −ϕc −2A/ω < ϕ(0) ≤−ϕc
In sub-case 13.4.4 memristor state is initialized as it follows:
ϕ(0) ∈(−ϕc −2A
ω , −ϕc].
(13.37)
Using (13.18), algebraic manipulation of (13.16) yields the following constraints on
the ﬂux time behavior over a period:
ϕ(t) < −ϕc
∀t :
0 ≤ωt < 2˜α
or
2π −2˜α < ωt < 2π
(13.38)
ϕ(t) = −ϕc
f or t :
ωt = 2˜α
or
ωt = 2π −2˜α
(13.39)
−ϕc < ϕ(t) < ϕc
∀t :
2˜α < ωt < 2π −2˜α
(13.40)
where
˜α = arcsin
!
ω(−ϕc −ϕ(0))
2A
"
.
(13.41)
From (13.24) and (13.26) it follows that q(t) and i(t) are respectively expressed by
q(t) =
⎧
⎪⎪⎪⎪⎨
⎪⎪⎪⎪⎩
b[ϕ(0) + A
ω (1 −cos ωt)] −(a −b)ϕc
∀t :
⎧
⎨
⎩
0 ≤ωt ≤2˜α
or
2π −2˜α ≤ωt < 2π
a[ϕ(0) + A
ω (1 −cos ωt)]
∀t :
2˜α < ωt < 2π −2˜α
(13.42)

13
Memristor Models for Pattern Recognition Systems
259
and
i(t) =

bA sin ωt
∀t :
0 ≤ωt ≤2˜α
or
2π −2˜α ≤ωt < 2π
aA sin ωt
∀t :
2˜α < ωt < 2π −2˜α
(13.43)
From (13.25) memductance depends on time according to
W(ϕ(t)) =

Wlow = b
∀t :
0 ≤ωt ≤2˜α
or
2π −2˜α ≤ωt < 2π
Whigh = a
∀t :
2˜α < ωt < 2π −2˜α
(13.44)
Memductance features two transitions per period: ﬁrst a Wlow →Whigh transition
at t = 2˜α/ω and then a Whigh →Wlow transition at t = (2π −2˜α)/ω. At these
transitions ν respectively crosses threshold voltages
v
2˜α
ω

= ˜vth = Asin(2˜α)
(13.45)
v
2π −2˜α
ω

= Asin(2π −2˜α) = −˜vth
(13.46)
Using assumption 13.37 on memristor state initial condition, it may be analytically
proved that ˜α is constrained to lie within [0, π/2). Various dynamics may arise out
of case 13.4.4. They may be classiﬁed into four sub-cases:
•
sub-case 13.4.4.1 (a degenerate sub-case): ˜α = 0 referring to ϕ(0) = −ϕc;
•
sub-case 13.4.4.2: ˜α ∈(0, π/4) referring to ϕ(0) ∈(−ϕc −A/ω, −ϕc);
•
sub-case 13.4.4.3: ˜α = π/4 referring to ϕ(0) = −ϕc −A/ω;
•
sub-case 13.4.4.4: ˜α ∈(π/4, π/2) referring to ϕ(0) ∈(−ϕc −2(A/ω), −ϕc −
A/ω);
Let us gain a deeper insight into the i–v behavior for each of such sub-cases. Our
choice for the values of ϕc, A and ω yields −ϕc −A/ω = −1 −(1/π)Wb and
−ϕc −2(A/ω) = −1 −(2/π)Wb.
13.4.4.1
Initial Condition Smaller Than the Critical Flux by Twice the
Critical Flux, i.e. ϕ(0) = −ϕc
Sub-case 13.4.4.1 assumes memristor state initial condition ϕ(0) = −ϕc. This yields
˜α = 0. Since 2˜α = 0 and 2π −2˜α = 2π, and being W(ϕ(t)) periodic with period T,
the two memductance transitions are simultaneous. Also, note that ˜vth = 0V. This
is yet another degenerate case. The i–v plot, indicative of weakly-resistive behavior,
is the dotted line in Fig. 13.5a. Such a plot is identical in shape and path direction to
the one referring to sub-case 13.4.2.4. The only difference lies in a half a period time
lag between the time occurrences of memductance double simultaneous transitions:
in sub-case 13.4.2.4 Whigh →Wlow →Whigh double transition occurs at ωt = π, while
here Whigh →Wlow →Whigh double transition occurs at ωt = 0.

260
F. Corinto et al.
13.4.4.2
Initial Condition Smaller Than the Critical Flux by More Than
Twice the Critical Flux and by Less Than the Sum of Twice the
Critical Flux and the Input Amplitude–Angular Frequency Ratio,
i.e. −ϕc −A/ω < ϕ(0) < −ϕc
Sub-case 13.4.4.2 takes −ϕc −A/ω < ϕ(0) < −ϕc, yielding ˜α ∈(0, π/4). Being
0 < 2˜α < π/2 and (3/2)π < 2π −2˜α < 2π, over one period Wlow →Whigh
and Whigh →Wlow transitions respectively occur before voltage relative maximum
and after voltage relative minimum. Since 0 < ˜vth < A, from (13.15) and (13.43) it
follows that the i–v plot is an atypical bow-tie with a-slope line extensions outside the
interval between the threshold voltages and counter-clockwise loop rotation for ν > 0.
Therefore, the i–v plot is identical in shape and opposite in path direction to the one
referring to sub-case 13.4.2.3 (see Fig. 13.5d). The discrepancy in path direction does
also reﬂect itself in the opposite sequence of memductance transitions over a period:
speciﬁcally a Wlow →Whigh transition is followed by a Whigh →Wlow transition here,
while a Whigh →Wlow transition is followed by a Wlow →Whigh transition in sub-
case 13.4.2.3. Such complementary memductance behavior may be exploited in the
design of a number of electronic circuits.
13.4.4.3
Initial Condition Smaller Than the Critical Flux by the Sum of Twice
the Critical Flux and the Input Amplitude–Angular Frequency
Ratio, i.e. ϕ(0) = −ϕc −A/ω
Sub-case 13.4.4.3 refers to φ(0) = −φc −A/ω, i.e. ˜α = π/4. Here 2˜α = π/2
and 2π −2˜α = (3/2)π. Therefore over one period memductance transitions occur
at voltage maximum and minimum (˜vth = A) and the i–v plot is a bow-tie with
counter-clockwise loop rotation for ν > 0. Therefore, as compared with the i–v plot
referring to sub-case 13.4.2.2, here shape and path direction of the bow-tie are re-
spectively identical and opposite. It follows that sub-cases 13.4.2.2 and 13.4.4.3
are characterized by complementary sequences of memductance transitions over a
period.
13.4.4.4
Initial Condition Smaller Than the Critical Flux by More Than the
Sum of Twice the Critical Flux and the Input Amplitude–Angular
Frequency Ratio and by Less Than the Sum of Twice the Critical
Flux and Twice the Input Amplitude–Angular Frequency Ratio, i.e.
−ϕc −2A/ω < ϕ(0) < −ϕc −A/ω
In sub-case 13.4.4.4 memristor state initial condition ϕ(0) lies within (−ϕc −2A/ω,
−ϕc −A/ω ). As a result, ˜α is constrained to assume values within (π/4, π/2). Here
over one periodWlow →Whigh transition occurs after voltage relative maximum since
2˜α ∈(π/2, π) (note that 0 < ˜vth < A), while Whigh →Wlow transition occurs before
voltage relative minimum since 2π −2˜α ∈(π, (3/2)π). Analyzing (13.15) and

13
Memristor Models for Pattern Recognition Systems
261
Table 13.1 Sub-class of i–v characteristics for a > b > 0 and A/ω ∈(0, ϕc) (abbreviations for
clockwise and counter-clockwise respectively are cw and ccw)
State initial condition
i–v Characteristic
ϕ(0) ≥ϕc
b-Slope line passing through (i, ν) = (0, 0)
ϕc −A
ω < ϕ(0) < ϕc
Atypical bow-tie with b-slope extensions for |v| > vth
and cw rotation for ν > 0
ϕ(0) = ϕc −A
ω
Bow-tie with cw rotation for ν > 0
ϕc −2A
ω < ϕ(0) < ϕc −A
ω
Atypical bow-tie with a-slope extensions for |v| > vth
and cw rotation for ν > 0
−ϕc ≤ϕ(0) ≤ϕc −2A
ω
a-Slope line passing through (i, ν) = (0, 0)
−ϕc −A
ω < ϕ(0) < −ϕc
Atypical bow-tie with a-slope extensions for |v| > ˜vth
and ccw rotation for ν > 0
ϕ(0) = −ϕc −A
ω
Bow-tie with ccw rotation for ν > 0
−ϕc −2A
ω < ϕ(0) < −ϕc −A
ω
Atypical bow-tie with b-slope extensions for |v| > ˜vth
and ccw rotation for ν > 0
ϕ(0) ≤−ϕc −2A
ω
b-Slope line passing through (i, ν) = (0, 0)
(13.43) it is simple to realize that the i–v plot is an atypical bow-tie with b-slope line
extensions outside the interval between the threshold voltages and counter-clockwise
loop rotation for ν > 0. The bow-tie shape and path direction respectively are identical
and opposite to those characterizing the bow-tie observed in sub-case 13.4.2.1 (see
Fig. 13.5b). Further, the sequences of memductance transitions occurring over one
period in sub-cases 13.4.2.1 and 13.4.4.4 are complementary.
13.4.5
Initial Condition Smaller Than the Critical Flux by at
Least the Sum of Twice the Critical Flux and Twice
the Input Amplitude–Angular Frequency Ratio, i.e.
ϕ(0) ≤−ϕc −2A/ω
Case 13.4.5 sets φ(0) ≤−φc −2A/ω. Equation (13.18) yields ϕ(t) ≤−ϕc ∀t ≥0.
From (13.24) and (13.26) it follows that q(t) = b[ϕ(0)+A/ω(1−cos ωt)]−(a−b)ϕc
and i(t) = bA sin ωt ∀t ≥0. The i–v plot is a b-slope line passing through the origin
(see solid line in Fig. 13.5a). Such a characteristic is identical in shape and path
direction to the one observed in case 13.4.1 and denotes highly-resistive behavior:
in fact, from (13.25) it follows that W(ϕ(t)) = Wlow = b ∀t ≥0.
Let us tabulate the results of the analysis of Sect. 13.4.1–13.4.5. Table 13.1
presents the sub-class of possible ϕ(0)-dependent i–v behaviors for A/ω ∈(0, ϕc).
Section 13.4.6 summarizes the results of the remaining part of the analysis, yield-
ing the sub-classes of possible i–v behaviors for a > b > 0 and either A/ω ∈[ϕc, 2ϕc)
or A/ω ∈[2ϕc, ∞], including results from Table 13.1. We invite the interested reader
to refer to [28] for the details.

262
F. Corinto et al.
13.4.6
Summary of i–v Characteristics
In this section we summarize the investigation of all i–v characteristics of a memristor
with q–ϕ characteristic (13.23) and control voltage source v = Asinωt. Within the
class of all possible characteristics (see [28]), three sub-classes may be extracted by
specifying A/ω within one of the following limits (the ﬁrst case has been analyzed
in Sects. 13.4.1–13.4.5):
1. (0, ϕc);
2. [ϕc, 2ϕc);
3. [2ϕc, ∞].
Then, within each sub-class, the observed behavior depends on the choice for mem-
ristor state initial condition. The main results of the whole study [28] may be collected
as it follows:
1. if A/ω ∈(0, ϕc) then, depending on ϕ(0), one of 5 i–v characteristics may be
observed (see Fig. 13.5):
a. a b-slope line passing through (i, ν) = (0,0), indicative of weakly-resistive
behavior (solid line in Fig. 13.5a);
b. an a-slope line passing through (i, ν) = (0,0), a signature of weakly-resistive
behavior (dotted line in Fig. 13.5a);
c. an atypical bow-tie with b-slope extensions outside the interval between the
threshold voltages (see Fig. 13.5b, but loop rotation for ν > 0 may also be
counter-clockwise);
d. a bow-tie (see Fig. 13.5c, but loop rotation for ν > 0 may also be counter-
clockwise);
e. an atypical bow-tie with a-slope extensions outside the interval between the
threshold voltages (see Fig. 13.5d, but loop rotation for ν > 0 may also be
counter-clockwise);
2. if A/ω = ϕc, then all behaviors in case 1 may occur as ϕ(0) is swept;
if A/ω ∈(ϕc, 2ϕc), then, besides 4 of the 5 behaviors in case 1 (characteristic b
of case 1 may not be observed here), then ϕ(0) may also yield a couple of novel
kinds of i–v characteristics (see Fig. 13.6a, b):
a. an atypical bow-tie with an extra threshold voltage pair, with a-slope exten-
sions outside the interval between the larger-magnitude threshold voltages, but
without the segment of the a-slope side within the interval between the lower-
magnitude threshold voltages (see Fig. 13.6a, but loop rotation for ν > 0 may
also be clockwise);
b. a degenerate atypical bow-tie with a-slope extensions outside the interval
between the threshold voltages, but without the segment of the a-slope side
within the interval between the threshold voltages (see Fig. 13.6b, indicative
of nonlinearly-resistive behavior);

13
Memristor Models for Pattern Recognition Systems
263
−5
0
5
−0.04
−0.02
0
0.02
0.04
−5
0
5
−0.04
−0.02
0
0.02
0.04
−20
−10
0
10
20
−0.1
0
0.1
−20
−10
0
10
20
−0.1
0
0.1
c
d
b
a
Fig. 13.6 Additional input–output characteristics for A/ω ∈(ϕc, ∞]. Plots (a)–(b): i–v behaviors
for A/ω ∈(ϕc, 2ϕc]; Plots (c)–(d): i–v behaviors for A/ω ∈(2ϕc, ∞]. As an alternative to plots
(a), (c), and (d), clockwise loop rotation for ν > 0 is also possible
3. if A/ω = 2ϕc, then, varying ϕ(0) it is possible to observe only behaviors a, c and
d of case 1 and the two above-listed dynamics a and b of case 2;
if A/ω ∈(2ϕc, ∞], then, besides behaviors a and c of case 1 and dynamics a and
b of case 2, appropriately selecting ϕ(0) one of two additional i–v characteristics
may also occur (see Fig. 13.6c, d), i.e.
a. an atypical bow-tie with an extra threshold voltage pair, with b-slope exten-
sions outside the interval between the larger-magnitude threshold voltages, but
without the segment of the a-slope side within the interval between the lower-
magnitude threshold voltages (see Fig. 13.6c, but loop rotation for ν > 0 may
also be clockwise);
b. a bow-tie with an extra threshold voltage pair, but without the segment of
the a-slope side within the interval between the lower-magnitude threshold
voltages (see Fig. 13.6d, but loop rotation for ν > 0 may also be clockwise).
13.5
Inﬂuence of Boundary Conditions on i – v Characteristics
In this section we show how the proposed model, i.e. the set of differential-algebraic
Eqs. (13.7)–(13.8) with boundary conditions deﬁned by (13.9)–(13.11) may qual-
itatively reproduce various memristor behaviors recently observed in a number of
nano-scale ﬁlms [29–31] once the proposed window function is tuned so as to satisfy
appropriate boundary conditions.
In particular, as it is shown in Fig. 13.7, the current–voltage characteristic of a
metal/strongly correlated electron system (SCES) interface of Fig. 13.3b in [29] (or,

264
F. Corinto et al.
9000 9200 9400 9600 9800 10000
−1
−0.5
0
0.5
1
−0.5
0
0.5
1
1.5
−1
−0.5
0
0.5
1
−0.4
−0.2
0
0.2
0.4
0.6
0.8
1
b
a
Fig. 13.7 Plot (a): time waveforms of vv−1
0
and l highlighting the threshold voltages the excitation
sourceneedstocrossforthememristortoleavetheon-oroff-resistiveoperatingmodeafterareversal
in source sign. Plot (b): Current–voltage characteristic replicating the dynamics experimentally
observed in a metal/SCES interface by Oka and Nagaosa (see Fig. 13.3b in [29] for the experimental
result). Parameter setting: Ron = 0.1 k, Roff R−1
on = 65, η = + 1, ν0 = 2 V, f = 0.6 Hz, νth = 0.25ν0
alternatively, of Fig. 13.8g in [32]) is detected by our model for matched threshold
voltages set to vth,0 = vth,1 = vth = ν0/4 = 0.25ν0V, where ν0 = 2 V is the amplitude of a
sine-wave voltage source, namely ν = ν0 sin(2πkt) with k = 0.003, applied across the
device. Note that in (13.5)–(13.7) time normalization factor is taken as memristor
characteristic time, i.e. t0 = D2/μv0, where D = 10 nm is the length of the nano-ﬁlm
made up of the series of two layers, one conductive with resistance Ronl = 100l and
theotherinsulatingwithresistanceRoff (1−l) = 65(1−l)Ron, whileμ = 10−14 m2/Vs
denotes the average mobility of dopants. As a result, the frequency of the sine-wave
input is f = k/t0 = 0.6 Hz. In Fig. 13.7 voltage and current normalization factors are
respectively set to ν0 and i0 = ν0/Ron = 10 mA.
Let us describe yet another scenario where the inﬂuence of boundary conditions
on the observed dynamics is evident. Controlling the memristor through the same
voltage input as in the previous case (however, here ν0 = 4 V and k = 0.01, yield-
ing f = 1 Hz), setting mismatched threshold voltages, i.e νth,0 = 6ν0/8 = 0.75ν0 V and
νth,1 = 7ν0/8 = 0.875ν0 V and assuming Ron = 500  (thus i0 = 8 mA) and Roff/Ron = 5,
numerical simulations of (13.8) with conditions (13.9)–(13.11) yield plots (a) and (b)
of Fig. 13.8 for a memristor with polarity coefﬁcient η set to −1 and + 1 respectively.
Plot (a) resembles the current–voltage characteristic in Fig. 13.2b of [31], relative
to a bipolar memristive element with a Pt/solide electrolyte/Cu stack proposed as
basic building block for the realization of passive nano-crossbar memories. Plot
(a) also replicates the current–voltage behavior in Fig. 13.8b of [32], referring to a
300 nm-thick epitaxial SrZrO3 ﬁlm doped with 0.2 %Cr grown on a SrZrO3 bottom
electrode with the top Au electrode extending over an area of 200 × 200 μm2, ﬁrst
introduced for memory applications in [30].
Plot (b) of Fig. 13.8 resembles the current–voltage characteristic of the previously
described stack [31] and thin-oxide ﬁlm [30] with reversed material order.

13
Memristor Models for Pattern Recognition Systems
265
−1
−0.5
0
0.5
1
−1
−0.5
0
0.5
1
−1
−0.5
0
0.5
1
−1
−0.5
0
0.5
1
b
a
Fig. 13.8 Current–voltage behaviors originating from our model with η set to −1 (plot (a)) and
+ 1 (plot(b)) respectively. Such behaviors qualitatively capture the dynamics of complementary
resistive switches with memory capability [30, 31]. Parameter setting: Ron = 0.5 k, Roff R−1
on = 5 ,
v0 = 4 V, f = 4 Hz, vth,0 = 0.75v0, vth,1 = 0.875v0
13.6
Conclusions
Memristive Oscillatory Neurocomputers (MONs) are deeply investigated for their
potential to realistically and efﬁciently emulate various activities carried out on a
daily basis by the human brain, such as spatial-temporal pattern recognition tasks.
The MONs consist of complex networks of nonlinear oscillators coupled through
memristor synapses. The realization of MON-based pattern-recognition systems may
not leave aside a deep understanding of their nonlinear dynamics, which strongly
depends upon the particular memristor at hand. Adopting linear dopant drift models,
in this work we investigated the crucial impact of initial conditions and boundary
conditions on the dynamics of the memristor. On the basis of this study one could
devise a novel pattern recognition system able to decode the information stored on the
initial condition since an association exists between current–voltage characteristic
and initial condition (see Table 13.1 and Sect. 13.4.6).
Acknowledgments This work was partially supported by the CRT Foundation, under the project
no. 2009.0570, by the Istituto Superiore Mario Boella and the regional government of Piedmont.
References
1. Mead C (1989) Analog VLSI and neural systems. Addison Wesley Longman, Boston
2. Boahen K (1996) Retinomorphic Vision Systems, Fifth International Conference on Micro-
electronics for Neural Networks and Fuzzy Systems, IEEE Computer Soc. Press, pp 2–14

266
F. Corinto et al.
3. Versace M, Chandler B (2010) MoNETA: a mind made from memristors. IEEE Spect 12:30–37
4. Dayan P, Abbott LF (2001) Theoretical neuroscience. MIT Press, Cambridge
5. Buzsaki G (2006) Rhythms of the brain. Oxford University Press, New York
6. Corinto F, Ascoli A, Lanza V, Gilli M (2011) Memristor synaptic dynamics inﬂuence on
synchronous behavior of two Hindmarsh-Rose neurons. Proceedings of international joint
conference on neural networks, San Jose, CA, pp 2403–2408
7. Corinto F, LanzaV,AscoliA, Gilli M (2011) Synchronization in networks of FitzHugh-Nagumo
neurons with memristor synapses. Proceedings of IEEE european conference on circuit theory
and design, pp 629–632
8. Engel AK, Kreiter AK, KÅ¡nig P, Singer W (1991) Synchronization of oscillatory neuronal
responses between striate and extrastriate visual cortical areas of the cat. Proc Natl Acad Sci
USA 88(14):6048–6052
9. Chalupa LM, Werner JS (2004) The visual neurosciences. A Bradford Book, MIT Press,
Cambridge
10. Chua LO (1971) Memristor: the missing circuit element. IEEETrans CircTheor 18(5):507–519
11. Strukov DB, Snider GS, Stewart DR, Williams RS (2008) The missing memristive element
found. Nature 453:80–83
12. Snider G, Amerson R, Carter D, Abdalla H, Qureshi MS, Léveillé J, Versace M, Ames
H, Patrick S, Chandler B, Gorchetchnikov A, Mingolla E (2011) From synapses to cir-
cuitry: using memristive memory to explore the electronic brain. IEEE Comput 44(2):21–28.
doi:10.1109/MC.2011.48
13. Corinto F, Bonnin M, Gilli M (2007) Weakly connected oscillatory network models for
associative and dynamic memories. Int J Bif Chaos 17:4365–4379
14. Snider GS (2008) Spike-timing-dependent learning in memristive nanodevices. Proceedings
of IEEE/ACM international symposium on nanoscale architectures, Anaheim, CA, pp 85–92
15. Pershin YV, Di Ventra M (2010) Experimental demonstration of associative memory with
memristive neural networks. Neural Networks 23:881–886
16. Corinto F, Ascoli A, Gilli M (2011) Nonlinear dynamics of memristor oscillators. IEEE Trans
Circ Syst I 58(6):1323–1336
17. Corinto F,AscoliA, Gilli M (2010) Memristive based oscillatory associative and dynamic mem-
ories. Proceedings of international workshop on cellular nanoscale networks and Applications,
Berkeley, CA, pp 1–6
18. Corinto F, Ascoli A, Gilli M (2010) Bifurcations in memristive oscillators. Proceedings
workshop on nonlinear dynamics of electronic system, pp 166–169
19. Corinto F, Ascoli A, Gilli M (2011) Heteroclinic bifurcation in memristor oscillators.
Proceedings of IEEE European conference on circuit theory and design, pp 237–240
20. Chua LO, Kang SM (1976) Memristive devices and systems. Proc IEEE 64(2):209–223
21. Muthuswamy B (2010) Implementing memristor based chaotic circuits. Int J Bif Chaos
20(5):1335–1350
22. Radwan AG, Zidan MA, Salama KN (2010) HP memristor mathematical model for periodic
signals and DC. IEEE international Midwest symposium on circuits and systems, Seattle, USA,
pp 861–864
23. Radwan AG, Zidan MA, Salama KN (2010) On the mathematical modeling of memristors.
IEEE international conference on microelectronics, pp 284–287
24. Joglekar YN, Wolf ST (2009) The elusive memristive element: properties of basic electrical
circuits. Eur J Phys 30:661–675
25. Biolek Z, Biolek D, Biolková V (2009) Spice model of memristor with nonlinear dopant drift.
Radioengineering 18(2):210–214
26. Corinto F, Ascoli A, Gilli M (2011) Symmetric charge–ﬂux nonlinearity with combined
inherently-asymmetric memristors. Proceedings of European conference on circuit theory and
design, Linköping, Sweden, pp 653–656
27. Corinto F, Ascoli A, Gilli M (2011) Class of all i–v dynamics for memristive elements in
pattern recognition systems. Proceedings of international joint conference on neural networks,
pp 2289–2296

13
Memristor Models for Pattern Recognition Systems
267
28. Corinto F, Ascoli A, Gilli M (2012) Analysis of current–voltage characteristics for memristive
elements in pattern recognition systems. Int J Circ Theor Appl. doi: 10.1002/cta.1804
29. Oka T, Nagaosa N (2005) Interfaces of correlated electron systems: proposed mechanism for
colossal electroresistance. Phys Rev Lett 95(26):6403-1–6403-4
30. Beck A, Bednorz JG, Gerber Ch, Rossel C, Widmer D (2000) Reproducible swicthing effect
in thin oxide ﬁlms for memory applications. Appl Phys Lett 77(1):139–141
31. Linn E, Rosezin R, Kügeler C, Waser R (2010) Complementary resistive switches for passive
nanocrossbar memories. Nat Mater 9:403–406
32. Chua LO (2011) Resistance switching memories are memristors. Appl PhysA 102(4):765–783


Chapter 14
A Columnar V1/V2 Visual Cortex Model
and Emulation
Robinson E. Pino and Michael Moore
Abstract We have explored the implementation of neurophysiological and psycho-
logical constructs to develop a hyper-parallel computing platform. This approach
is termed neuromorphic computing. As part of that effort, the primary visual cor-
tex (V1) has been modeled in a high performance computing facility. The current
columnarV1 model is being expanded to include binocular disparity and motion per-
ception. Additionally, V2 thick and pale stripes are being added to produce a V1/V2
stereomotion and form perception system. Both the V1 and V2 models are based
uponstructuresapproximatingneocorticalminicolumnsandfunctionalcolumns. The
neuromorphic strategies employed include columnar organization, integrate-and-ﬁre
neurons, temporal coding, point attraction recurrent networks, Reichardt detectors
and “confabulation” networks. The interest is driven by the value of applications
which can make use of highly parallel architectures we expect to see surpassing one
thousand cores per die in the next few years. A central question we seek to answer is
what the architecture of hyper-parallel machines should be. We also seek to under-
stand computational methods akin to how a brain deals with sensation, perception,
memory, attention decision-making.
14.1
Introduction
The objective of the project is to investigate architectural issues surrounding neuro-
biological inspired computational methods based on networks of structures roughly
emulating cortical columns. It is the ﬁrst step in a larger investigation of multiple
classes of applications which may be able to take advantage of large scale parallel
computing. This multidisciplinary effort focuses on determining how neurological
systems perform those aspects of cognition associated with sensing and perception.
R. E. Pino ()
Air Force Research Laboratory, Rome, NY, USA
e-mail: Robinson.Pino@gmail.com
M. Moore
ITT/AES, Rome, NY, USA
e-mail: mike.moore@itt.com
R. Kozma et al. (eds.), Advances in Neuromorphic Memristor Science and Applications,
269
Springer Series in Cognitive and Neural Systems,
DOI 10.1007/978-94-007-4491-2_14, © Springer Science+Business Media Dordrecht 2012

270
R. E. Pino and M. Moore
The work progressed initially on ventral tract (object recognition) aspects of the vi-
sual cortex, and is now shifting to include the dorsal tract, theoretically associated
with spatial properties.
We have been investigating neuromorphic computing strategies since 2006, and
have produced a cadre of interdisciplinary researchers, a full-scale real-time spatial
V1 emulation, and a facility for the investigation of visuospatial perception mod-
els. The emulation exploits AFRL’s Condor High Performance Computing center.
Condor is a heterogeneous supercomputer comprised of commercial-off-the-shelf
commodity components including 1,716 Sony PlayStation 3 (PS3) (Sony Corpo-
ration of America) game consoles and 168 General Purpose Graphical Processing
Units. It is capable of 500 trillion ﬂoating point operations per second. The initial
full-scale V1 emulation was completed in 2009. The emulation focused on the per-
ception of luminance-deﬁned edges in multiple spatial orientation [1, 2]. The V1
emulation produced approximately 25,000 such percepts for each visual frame. The
V1 model includes approximately 265 million simulated integrate-and-ﬁre neurons
incorporated across 1.6 million cortical columns.
The central nervous system (CNS) literature exhibits a great deal of observational
detail, but is void of explanations unifying the observations into a coherent system
science sufﬁcient for describing how a brain produces a mind. There are numerous
gaps in the literature with regard to connectivity, speciﬁc stimulus sensitivities for
cellular populations, and ultimately, how a mere 3 pounds of brain tissue achieves
consciousness.AFRL looks to full scale emulation as a means of exploring alternative
interpretations of the observational phenomena encapsulated in the psychophysical,
behavioral, and cognitive literatures. We intend to form full scale systems of alterna-
tive models consistent with published observations, using estimates to bridge gaps
in the existing body of the neuroscience literature.
The extent of the emulation is important. A V1 emulation in isolation provides
a rich environment for exploring the nuances of striate visual cortex neuroscience,
but more interesting are the extensive problems of stereoscopic fusion, multisensory
integration, and eventually decision-making. AFRL plans to expand our existing V1
emulated model and also add models and emulations of near-by cortical ﬁelds: specif-
ically V2 through V5 [3–10]. The objective of the expansion is to enable exploration
of visual object discrimination [11], motion and depth perception, and visuospatial
awareness. Development of the V1/V2 system model described here is our next step.
This emulation will focus on spatial perception.
14.1.1
Anatomy
There are about 1.6 million axonal ﬁbers delivering information from the eyes into the
primary visual cortex [12] through the lateral geniculate nucleuses (LGNs). Each side
of the brain receives half of these, organized retinoscopically and stereoscopically.
The retinoscopic organization means that the image carried by the ﬁbers is spatially
preserved, as if projected through a lens. The stereoscopy characteristic has to do
with ﬁeld of view. Each eye has a left and right ﬁeld of view. The left side of the

14
A Columnar V1/V2 Visual Cortex Modeland Emulation
271
brain receives the right ﬁeld of view from each eye, and the right side receives the
left ﬁeld of view. Thus each hemisphere of V1 receives approximately 800 K ﬁbers
delivering two partially overlapping ﬁelds of view. The neuro-pathway for these,
between the LGNs and the visual cortex, is called the optical radiation. There are
two; a left and a right. Each of the hemispheres bundles its approximately 800 K
feed forward axons with approximately 3.2 million feedback axons, terminating at
its LGN. The feed forward axons are mostly of two types: Parvocellular (P) axons
and Magnocellular (M) axons. The P axons are thought to be associated with shape
and color perception; the M axons with motion [13]. P axons account for about 80 %
of the feed forward; M accounts for about 5 %.
V1 itself is part of the neocortex, which in turn is the top layer of a primate
brain. The neocortex is thought to be where the essential mechanisms of human
cognition reside. It is central to sensation and perception. The neocortex is a sheet
of tissue roughly 3 mm thick and 2,500 cm2 in area (2.5 ft2) [14]. The primary
visual cortex is an area roughly 28 cm2, accounting for both hemispheres [15]. Thus
the primary visual cortex is a little more than 1 % of the neocortex by area. The
total number of neurons in the cerebral cortex is estimated to be 20 billion [16].
The total number of neurons in V1 is estimated as 280 million [16], and thus V1
is about 1.4 % of the neocortex by neuron count. The neurons within V1, looking
perpendicular to the sheet, are arranged into structures of neurons forming ∼3 μm
diameter columns extending through the six layers [17]. The columns are called
“minicolumns.” Estimates for neurons per minicolumn within V1 are in the range of
120–200, but using a rule of thumb that the incoming axons from the eyes are roughly
evenly distributed, it works out that there is one minicolumn for each afferent (from
the eyes) axon, and the neuron count per minicolumn is around 150.
Each parvocellular axon potentially connects to an area whose diameter is ap-
proximately 400 microns, which happens to be the scale of a functional column [18].
These “P Channel” ﬁbers provide high contrast, spatially ﬁne grained color informa-
tion to the brain. Magnocellular ﬁbers overlap a 1,200 micron diameter area, which
happens to be on the scale of a hypercolumn [18]. These “M Channel” ﬁbers carry
low contrast information on the visual ﬁeld, are associated with depth and movement
perception, and are notably much faster to respond than the “P” channel.
Minicolumns exhibit excitatory and inhibitory interactions with each other. Exci-
tatory communications appear to span a radius of about 3 mm [19] while inhibitory
is half that [20]. The excitatory span has a reach of about 14 functional columns
across the diameter, and the inhibitory about 7 functional columns. Excitatory ap-
pear to connect up every other functional column, though there is debate about this.
Inhibitory appear to hit every functional column within its reach.
14.1.2
Levels of Modeling
Neuroscience has provided multiple complexity levels for modeling the cells com-
prising a brain. There are two general types of cells in the brain (ignoring the

272
R. E. Pino and M. Moore
circulatory system): neurons and glia cells. The neurons are the cells with axons and
dendrites which neuroscientists have historically assumed are the basic functional
components of a brain. Glia cells out number neurons 10 to 1. They provide the
scaffolding and life support environment for the neurons. They are recently thought
to play more of a role in cognition than has been traditionally assumed [21]. Glia
cell modeling is accounted for at a molecular level, typically with pharmaceutical
interest. They were not included in this study.
The question is how to separate and identify the computationally useful charac-
teristics of neuro-matter from those that are purely life supporting. Neuroscience
has developed compartmentalized models [22] of neurons which capture the intri-
cacies of neuron physical size and shape (morphology), electrochemical dynamics
(electrophysiology), molecular interactions between neurons and with glia cells (neu-
rochemistry), and interpretations of information processing thought to be performed
by neurons. The more detailed models require signiﬁcant processing power to emu-
late. Which characteristics of these cells are harnessed by nature to produce cognition
is an open question. It is not clear whether cells are the functional components of
cognition. Collections of cells, perhaps cortical columns, may be the key functional
building blocks.
Neurons exhibit increasing feature complexity as one looks closer into them.
Very detailed compartmental models exhibit up to tens of thousands of individual
synapses (connections), each with attributes such as connection strength, type (in-
hibitory, excitatory), dynamical characteristics, distance from the soma (nucleus),
and neurotransmitter type. Simple models of neurons capture only the integrative
and non-linearity estimates, ignoring electrophysiological pulse responses and spike
timing dependent plasticity; they may have only a few connections. At higher levels
of abstraction collections of individual neurons are replaced by “cognitive models”
performing the hypothesized functions of the collectives; functions like association,
feature perception and memory.
Setting a level of abstraction in a model constrains what the model can do. Ac-
counting for all known cognitive behaviors with a simple model is evidence that the
cells are being modeled validly, at least until new behaviors are identiﬁed. Levels
of feature use may vary across the cortex. For example: detailed dynamical neuron
models were not necessary to achieve the efﬁcacy we expected of V1 in this study.
We acknowledge they may be needed for other cortical regions or even for V1 itself
should Integrate and Fire neurons be an insufﬁcient mechanism.
The “affect” objectives of theV1 model are to account for orientation, color, depth
perception (disparity), and motion percepts. The model proposed here has addressed
orientation, and partially addressed color. Depth and motion are future plans. Not
much is known about how neurons are systematically organized to produce and
represent these affects, but there are hints.
Self imposed is the objective to emulate a full scale V1 in real-time. The ability
to process in real-time simpliﬁes the use of live video feed and provides a level
of practicality reasonable for testing a model over extended durations. Real-time
performance adds a “time complexity” challenge to computation, in the “big o”
sense [23], restricting the use of algorithms with high time complexity.

14
A Columnar V1/V2 Visual Cortex Modeland Emulation
273
14.1.3
Simulation Facility
At our disposal is a 336 node Play Station 3 CELL-BE cluster organized as 14 subnets
each with 24 nodes. Each subnet has a dual 3 GHz quad core Xeon processor head
node. Network interconnectivity is 10 Gigabit Ethernet amongst the head nodes and 1
gigabit Ethernet to the PS3s. Each PS3 node has six available Synergistic Processing
Elements (SPEs) and a dual core 3.2 GHz PPE (Power PC). There are 2,116 SPEs in
total. Each SPE is capable of slightly more than 25.6 GFLOPS for a total CELL-BE
cluster capability exceeding 54 TFLOPS, not accounting for head node and PPE
contributions. GNU C++ development tools were used to develop the emulator, and
a publish/subscribe message passing system was used for communication within the
emulation. The “Pub/Sub” message paradigm loosely couples peer to peer message
passing. A message sender (publisher) does not send to a speciﬁc destination. In-
stead, each message has information in a “header” which describes what it is. This
information might take the form of XML, plain text strings, or binary encoded num-
bers; speciﬁcs depend on the individual message system. The point is that the sender
is unaware of the destinations. Receivers (subscribers) “sign up” to receive messages
based on header content (what the message is) rather than the message source. In
a system like a cortical model, inter-process connectivity can then be achieved by
subscribing to (for example) axonal ﬁber names, and publishing on ﬁber names.
This 2,400 core (Xeons + Power PCs + SPEs) facility’s processors are somewhat
specialized. The head nodes are conventional general purpose platforms with 32
GB of memory (each). The CELL-BE PPEs, also general purpose, each have 228
megabytes of RAM. The SPEs are specialized to be vector processors; they each
have about 128 Kbytes of useable RAM. Very fast DMA channels within a CELL-
BE move data between main memory (PPE) and SPE memory. The Xeons, and PPEs,
run Linux; the SPEs are essentially managed by the PPE with only minimal resident
executive kernel software, but can interact with each other and the PPEs using DMA
channels, interrupts and semaphores.
14.2
Model Constraints
The model was devised to be close to the anatomical structure of V1. It was also
devised to make use of methods our preliminary investigations found compatible
with CELL-BE architecture. These included:
•
Small collections of neurons, strong localized connectivity, sparse distant
connectivity;
•
Integrate and ﬁre neurons;
•
Spatially tuned receptive ﬁelds;
• A localized associative component, possibly a small scale recurrent neural net;
•
Feature extraction: max/min calculations, difference calculations, energy esti-
mates, threshold detection;

274
R. E. Pino and M. Moore
•
Inhibition, excitation interactions.
Methods considered, but avoided initially were:
•
Confabulation algorithm [24], on the basis that it required large amounts of mem-
ory to support symbol lexicons (this decision was revered after it became apparent
Confabulation was useful within the V1 lateral network);
•
Spiking neuron models [22]: on the basis that the cognitive mechanisms hypothe-
sized for these, principally dynamical phenomena, are not yet well demonstrated
or characterized;
•
Bayesian networks [15, 19]: on the basis that we are seeking a model more closely
aligned to anatomical details;
•
Large scale associators, such as Sparse Distributed Memory (SDM), on the basis
that we did not feel it was needed for a V1 model.
The challenge of model development was to create a system using just the selected
methods that could meet the perception objectives of shape (orientation line), color,
motion, and disparity.
14.3
V1 Model Description
Orientation line perception is the major effort of modeling thus far. It is expected
to be the most computationally challenging of all the V1 percepts. Aspects of color
perception have been included, and a color percept is produced. It is modeled as
the average color and intensity cast onto the ﬁeld of view of a functional column,
and includes an ocular dominance feature which selects the strongest percept in
an overlapping (stereoscopic) ﬁelds of view. In those cases the functional column
with the dominant orientation percept inhibits the other functional column. Motion
perception is, like color, part of the objective but not yet emulated. Motion, based
on magnocellular information, will produce a percept spatially mapped to the func-
tional columns detecting it; direction and intensity are the intended percepts. The
biomorphic model is based on the Reichardt effect [25] using synaptic arrival time
differences to excitealp a neuron. In practice, we are looking at FIR and IIF ﬁlters
for emulation.
The model is intended for full scale emulation. For this reason parameters are
sometimes selected to accommodate the digital environment of the emulation, within
the constraint that they represent plausible and reasonable neurological system val-
ues. One of these accommodations is powers of two. We have selected the following
organizational parameters:
•
Number of “ocular axonal ﬁbers” entering V1 hemisphere: 802816;
•
Total minicolumns per V1 Hemisphere: 802816;
•
Minicolumns per functional column: 64;
•
For the sake of emulation, we devised a subunit of aV1 hemisphere which we call
a “subﬁeld.” A subﬁeld is a collection of 128 functional columns, 64 of which

14
A Columnar V1/V2 Visual Cortex Modeland Emulation
275
Fig. 14.1 Plausible cell populations within cortical layers of a V1 minicolumn
are right FOV and 64 are left FOV. Each (full scale) hemisphere consists of 98
subﬁelds. Note that (98 subﬁelds)×(128 FCs per subﬁeld)×(64 minicolumns/FC)
works out to 802816 minicolumns per hemisphere.
All minicolumns within a functional column are assumed to have the same parvo-
cellular ﬁeld of view (aperture). Four functional columns form a macrocolumn; all
minicolumns within it are assumed to have the same magnocellular FOV from both
two eyes, and are responsive to all colors and orientations.
The minicolumn model is based on estimates of cell populations in cortical levels
II, III, and IV (see Fig. 14.1). The level IV model component consists of:
•
56 simple cells dedicated to parvocellular inputs
•
10 simple cells dedicated to magnocellular inputs
•
8 complex cells dedicated to (parvocellular) orientation perception from simple
cells
•
8 complex cells (not yet modeled) dedicated to (magnocellular) perception.
The model currently makes use of parvocellular information; the magnocellular
part of the model is not yet completed. Disparity, color and motion are not yet
completely modeled, and will likely be modeled by having a subset of minicolumns
within a functional column (cytochrome oxidase blob regions [26]) specialized for
their perception.
The parvocellular simple cells each make 16 synapses with the afferent ﬁbers.
Half are dedicated to dark sensitivity, half to light. The color image is converted to
shades of gray before presentation to the simple cells. Each simple cell receptive
ﬁeld has an angle, direction (light to dark, or dark to light), size/shape, and location
(see illustration in Fig. 14.2). Variations in size and location provide a degree of
invariance.

276
R. E. Pino and M. Moore
Fig. 14.2 An illustration of
two simple cell receptive
ﬁelds projected onto the FOV
of a functional column. Gray
ellipses represent synapses
sensitive to dark; yellow, to
light. Blue dots represent
terminations of afferent ﬁbers
Fig. 14.3 In this view, dots
represent minicolumns.
Orientation columns are each
a stack of eight minicolumns.
Each column is sensitive to
one orientation. A functional
column is a collection of eight
orientations columns
Each Minicolumn has 56 such parvocellular simple cells, all looking for the
same angle, but half looking for light to dark transition and half dark to light. The
minicolumns are arranged into 8 columns of 8 (Fig. 14.3), approximating orientation
columnanatomy[27]. Eachcolumnisdedicatedtoaspeciﬁcangle. The8×8structure
results in angles that are 22.5◦apart.
The simple cells function by summing their synapse values and “thresholding”
the results. The thresholds are presently constant, but variability will be explored in
the future as part of a contrast control mechanism.
Complex cells receive simple cell outputs (Fig. 14.4). Four of the eight complex
cells form synapses to simple cells that can detect light to dark transitions; the other
four to dark to light transitions. Each complex cell makes synapses to 15 simple cells
of the 26 available to it. The selection of which simple cells is based on a preference
for simple cell receptive ﬁelds which center their receptive ﬁelds approximately along
the same line, at the minicolumn’s perception angle. The four regions of perception
within the minicolumn’s FOV established by this preference, overlap. The complex
cells sum their inputs, and normalize the results to be within the range [−1 . . . + 1].

14
A Columnar V1/V2 Visual Cortex Modeland Emulation
277
Fig. 14.4 Light (yellow) passing over a minicolumn’s FOV; lower left corner in darkness (shown as
colored dots, indicating afferent axon terminations). The simple cells are tuned to all spatial phases
Fig. 14.5 The associative layer (II/III) has a BSB attractor whose state vector receives inputs from
afferent, lateral and extrastriate sources. One of two features is decoded off the state vector and sent
as feedback to thalamus and feed forward to extrastriate regions
For example, a dark to light sensor would issue a −1 for light to dark transition
perfectly aligned with it.
The outputs of the 8 complex cells are presented to the level II/III part of the
model (illustration in Fig. 14.5).

278
R. E. Pino and M. Moore
The level II, III part of the model is called the associative component. It deals
with data coming from three sources:
• Afferent detections from level IV,
•
Lateral (horizontal) connections to nearby minicolumns,
•
Expectation data from other cortical regions such as V2.
The model uses a 32 element recurrent network “Brain State in a Box” (BSB) [4]
attractorfunctiontodecidewhetherornotaminicolumnperceivesitsangularpercept.
Every minicolumn has its own BSB state vector, but all share the same weight matrix.
The common weight matrix is pre-trained to have two basins of attraction; these are
set at opposite corners of the BSB hypercube. The basin points correspond to “I see
a light to dark transition” and “I see a dark to light transition.” Neuromorphically,
this may correspond to actual recurrent neural networks, randomly wired but capable
of being point attractors. There is no need to involve the BSB in differentiating an
angle; the Level IV network does that, and supplies eight elements of “evidence” to
the state vector. When the rest of the vector is neutral, afferent inputs alone can drive
the BSB to a basin if the angle is fairly well sensed by Level IV. Likewise, Lateral
and Extrastriate (expectation) data can singularly drive the BSB to a basin.
The minicolumn concludes its feature perception by computing the (Cartesian)
distance of its state vector to each basin of attraction. The shortest distance is selected
and is subjected to a threshold criterion. Distances closer than the threshold are
converted into a range [0 . . . +1] for light to dark, and [0 . . . −1] for dark to light by
differencing with 1.0 (1.0 −Distance, or −1.0 + Distance, depending on light/dark
direction). Subthreshold cases are set to 0.0.
Each minicolumn within a functional column contributes to a functional column
hypothesis. The strongest perception within each orientation column is selected for
the hypothesis. The hypothesis is sent to all neighboring functional columns within a
3 mm reach. The receiving functional column “knows” the distance (hence, a weight)
and direction (one of the 8 angles) the hypothesis came from, and uses the information
to excitealp a “token.” Tokens in this case are the 8 angles of perception, and their
transition direction (light to dark, dark to light). All incoming lateral hypotheses
contribute to this excitation. A “winner take all” strategy selects the most excitealpd
token and the token is then asserted onto the elements of the state vector dedicated
to laterals (same value loaded into an eight elements, having a multiplicative effect
on the BSB state vector dynamics).
The whole lateral process is similar to the algorithm reported by Hecht-Nelson
which has been demonstrated to generate sentence text based upon noisy data and
incomplete sentences [24]. Dubbed “Confabulation Theory,” Hecht-Nelson proposes
that the brain deals with distinct symbols which are percepts detected by neural
networks. These symbols occur in context with other symbols. His example is text:
the words would be the percepts, and sentences the contexts. The idea is hierarchical;
groups of words (phrases) can be percepts, and paragraphs contexts. Weight matrices
(“knowledge links”) drive a selection process where a single symbol is selected from
a lexicon at each contextual position. Unlike the reported Confabulator, this V1
model uses a large number of lexicons (>500 instead of 20), and each lexicon is

14
A Columnar V1/V2 Visual Cortex Modeland Emulation
279
Fig. 14.6 Two examples of
the two dimensional
“Confabulation-like” lateral
model producing an illusional
percept (left column) and
correction (right column).
Each small grid box
represents a functional
column (64 minicolumns) in
this illustration. The left ﬁeld
of functional columns was
exposed to a 135◦grating
pattern. The right side was
exposed to a 67.5◦pattern
small (16 symbols (edge percepts) instead of 10,000 (word symbols). It gives the
model the ability to “see” illusional contours and improve perception in noisy data.
Figure 14.6 illustrates both situations; a diffraction grating is simulated at 135◦, with
data missing in parts of the ﬁeld of view passing over three function columns. On the
left the upper block is the feed forward perception, and the lower is perception after
lateral data is applied to the minicolumns. A “lateral expectation” based on context
tips the minicolumn into perceiving portions of the lines where there are actually
blanks. On the right noisy data and limitations of the apertures cause misperceptions
of 67.5◦angles (using feed forward only). Again, the lateral effect corrects the feed
forward perceptions (lower). It is plausible that this sort of mechanism can give V1
an ability to “see” combinations of small aperture edge percepts preferred by V2.
A full scale V1, both hemispheres, was emulated using 196 IBM/Sony PS3 Cell-
BE processors conﬁgured as subclusters of 24 nodes attached to head nodes (dual
quad core Xeon X5450 3 GHz) (Fig. 14.7). At the basis of message communication
is IP, but a Publication/subscription service layer was used on top of IP to mitigate
the tight binding imposed by socket to socket communication. The Pub/Sub message
layer signiﬁcantly reduced the complexity of regional lateral communications, where
functional column hypothesis has to be shared among neighbors within 3 mm. All
emulation software was written in C++.
Head-node software consists of stimulation and monitoring which roughly emu-
late ocular afferent pathways. There is a retina model (one or two may be used) which
provides a left and right visual frame (magno and parvo). Output (for the time being)

280
R. E. Pino and M. Moore
Fig. 14.7 Schematic of the emulation architecture.“JBI” is the name of the Publication/Subscription
message layer used by the emulation
is RGB color pixels. A chiasm model combines frames from retinas and separates
them into left and right stereo ﬁelds of view. An LGN model is simply a relay which
chops up the stereo frames into smaller pieces (essentially subﬁeld FOVs) that get
delivered to the PS3 nodes.
Each PS3 node handled 8192 minicolumns and the related functional column
model. For development convenience each group of 8192 minicolumns is termed a
“subﬁeld,” and so each PS3 node handled one subﬁeld. The BSB attractors cycle 5
times for each perception trial. In general, the PPE side of the PS3 nodes handled
messaging and orchestration of the SPE processors, and hypothesis generation. The
SPEs handled the emulations of Levels II/III and IV. Emulation speed is real-time.
Each node is able to complete its processing in about 5.9 ms. The most time demand-
ing aspect is delivery of image fragments to the PS3 units. This takes about 10 ms.
The entire cycle time for a single frame was measured to be about 18 ms, or 55 Hz.
14.3.1
V1 Model Computational Results
To date, only high contrast images are being presented to the system. Natural scene
images will be attempted when a contrast control mechanism is in place. The initial
test patterns were ideal diffraction grating images spaced to guarantee separation of
“bar lines” on functional columns by a distance at least sufﬁcient so no functional
column was exposed to two separate lines. No expectation was used during these
tests to reinforce perception. The grating patterns were moved across the ﬁeld of
view in steps comparable to the diameter of a minicolumn. There were signiﬁcant

14
A Columnar V1/V2 Visual Cortex Modeland Emulation
281
Fig. 14.8 V2 stripe cycle
misperceptions of ±22.5◦when image bars were near the spatial limits of the func-
tional column ﬁelds of view, but lateral “confabulation effects” corrected these near
the end of the perception cycle (see Fig. 14.6). Sensitivity to contrast was signiﬁcant,
indicating the need for contrast control. However, certain applications, like reading
text, are normally high contrast activities which the current model is reasonably
suited to pursue.
The emulation had two major computational modules: “Layer IV” and “Layer
II/III” corresponding to cortical layers. Layer II/III (also called the associative layer)
included the 32 element BSB attractor, and a small neuronet which formed functional
column perception consensus. The Level IV module emulated the spatially tuned
simple cells and the complex cells connecting them to the associative layer. These
all executed on SPEs which, ideally, are able to compute at 25.6 GFLOPS. The
associative layer code ran in 2.833 ms, achieving 10.5 GFLOPS. The Level IV code
ran in 2.602 ms, achieving 8.6 GFLOPS. The processing of one video frame subﬁeld
takes 5.9 ms; approximately 0.47 ms is accounted for in feed-forward message
handling. The entire ﬁeld of 196 subﬁelds required <17 ms. In this case the extra
11.1 ms is due to the serialization of dispatching subﬁeld size pieces of an image.
This serialization can be reduced in principle through parallelizing the network feed
intoV1 from the LGN modules. The 17 ms cycle time for a single frame corresponds
to a frame rate of 58 Hz.
14.4
The V2 Model
14.4.1
Anatomy and Physiology
Anterior to V1, V2 surface area is typically about 1,100–1,300 mm2 in the macaque
monkey [15, 28, 29] and is about twice as large in the human, or approximately
2,100 mm2 [29, 30]. It is about 76 % of the typical V1 surface area [12]. The visual
cortex V2 area is closely associated with Brodmann Area 18 [28]. Differences due to
methods used to locate each area have led to some question as to the exact location
of the boundaries of V2. There is however strong evidence of structural differences
between V1 and V2. In particular, it has been observed numerous times that the
topography of V2 is organized as a repeating cycle of stripes [28, 31]. Histological
staining of V2 reveals a pattern of stripes commonly referred to as “thin, pale and
thick.” The pattern is consistent as follows [32, 33] (Fig. 14.8):
Pale stripes account for about 40 % of the V2 surface area and are classically as-
sociated with form perception in the “tripartite” model [1]. Likewise the thick stripes

282
R. E. Pino and M. Moore
Fig. 14.9 Minicolumn
model. Multiple streams of
perceptual evidence are
mapped into a small state
vector (we use 32 elements).
The attractor basin space is
sufﬁcient to distinguish up to
4 features; we use 2. Any
training necessary for
perception happens within the
networks surrounding the
attractor
are associated with motion/stereo perception and thin stripes color. These notions of
stripe functional specialization have been challenged by recent data supporting the
“bipartite model” [31]. Both V1 and V2 are retinotopic. Like V1, approximately half
of V2 is dedicated to the foveal center (about 2 or 3◦) of vision.
V2 receives percept information such as spatial, motion, disparity, and color, from
V1 through axon projections [34]. The physiological and anatomical details of these
connections are vague. It is estimated that these projections originate in theV1 layers
2, 3 and 4 (out of the 6 inV1) from both blob and inter-blob regions [1, 28, 30, 31]. A
projection fromV1 level 4 is credited as being Magnocellular and extends to bothV2
thick stripes and to area MT. V2 is nearly retinotopic, with discontinuities associated
with the stripe boundaries [32, 33, 35]. A receptive ﬁeld (of a neuron) is the region
of a neural net where its inputs can be stimulated. In the visual system, a neuron’s
receptive ﬁeld maps its sensitivity to a portion of the visual space. Receptive ﬁeld
apertures in V2 are approximately twice the size or more (in radius) as V1 apertures;
thus a 4:1 ratio on receptive ﬁeld area [36].
V2 strongly projects back to V1, where it is presumably used to modulate the
formation of perceptions. It also projects forward, primarily to V4 (ventral stream)
[11] and MT (dorsal stream) [35, 37].
14.4.2
V2 Model Description
Ourmodeldealswithspatial(form)andmotion/stereoperceptions; itdoesnotinclude
color perception at this time. It is columnar, meaning it is based on a minicolumn
cortical architecture [17, 38]. Figure 14.9 illustrates the general architecture we
appliedinV1, andarenowapplyinginV2. Weusethismodeltotranslatestimulusinto
discrete symbols. It incorporates timing dependent plasticity feedback to a recurrent

14
A Columnar V1/V2 Visual Cortex Modeland Emulation
283
Fig. 14.10 V1:V2 spatial
mapping
neural network point attractor for fusing multiple streams of evidence (feed forward,
feedback and lateral context) into a perception belief. The point attractor is the Brain
State in a Box (BSB) algorithm [4]. Its use as a belief mechanism is described in
[2]. Afferent perception detections formed in layer 4 are mapped into a state vector
and fused with the other streams, if any. The BSB is cycled and the result state
vector distance to its perception basin is computed. A perception is declared if the
distance is within a threshold region. Each BSB is associated with a small number
of percepts; multiple minicolumns are pooled to form a lexicon within an aperture
area. The strongest perception within the pool wins.
Multiple studies support the idea [30, 39–42] of V2 Spatial perception consist-
ing of combining V1 edge detections into modestly more complex form detections.
There is no direct measurement of these speciﬁc forms; the physiological data con-
sists of responses of individual cells to test stimuli. A single individual cell appears
to be sensitive in varying amounts to a range of stimuli [39, 40]. The data supports
but does not prove the existence of a form lexicon that is based on combinations of
V1 perceptions. We have selected a set of plausible V2 perceptual forms which are
simple combinations of V1 orientation perceptions based on ﬁndings in the neuro-
physiology literature [40]. Figure 14.6 illustrates, in the upper left, a paring of V1
orientation percepts and how they may form V2 Percepts. The upper right side of
Fig. 14.10 illustrates a 9:1 mapping of V1 to V2 spatial perception. The 9:1 ratio was
selected based on reported evidence that V2 perceptual apertures cover four times
the visual ﬁeld area as V2 perceptual apertures [36]. The bottom of Fig. 14.10 illus-
trates some notions of how apertures might map, depending on packing. The large
circles represent a V2 aperture; the smaller ones represent V1 apertures mapped to
the V2 aperture. The 9:1 case was selected for initial investigation. We anticipate
investigating the efﬁcacy of the 7:1 and 4:1 cases in addition to the 9:1 case. All three
conﬁgurations are planned for testing in our cortical emulation. We expect the 9:1
case may provide a better measure of position invariant perception than the smaller
samples.
The initial model will utilize the exampleV2 lexicon illustrated.A learned lexicon
is being considered for follow-on investigations [30]. The prototypical V2 spatial
perceptions illustrated carry a certain ambiguity. For example, the intersection of
vertical and horizontal perceptions seemingly produce a “cross” or “plus” perception.
Because of the 9:1 ratio a “cross” cannot be distinguished from other interpretations
where the intersection is not in the middle; the actual case might be an “L”, upside

284
R. E. Pino and M. Moore
down “L” or sideways “T”. The actual semantics indicated by the “+” token is simply
“horizontal and vertical crossing.”
Each 0.64 mm2 V2 aperture in the 2,000 mm2 space will use ∼18 such circuits
to detect its lexicon of spatial symbols. Each circuit is to be assigned its own pair of
“oppositional” symbols. We deﬁne an oppositional pair to be two symbols whose
components are roughly orthogonal. For example, a V2 spatial representing a cross-
ing of horizontal and vertical lines are treated as oppositional to crossing 45◦lines.
The motion perception within the V2 model is thought to be based on lumi-
nance M-channel information; this assumption is based on cytochrome oxidase (CO)
reactivity, M-channel tracing to thick stripes, and cells within thick stripes being
dominantly motion sensitive [1, 30, 31, 33]. V2 has the advantage of larger apertures
(than V1). Our model applies these to M-Channel information to detect luminance
change at multiple points similarly to how the V1 model detects edge movement. It
does it using an array of detectors that are directional, and which detect movement
at a location but not the speed. The thick stripe apertures are assumed to represent
the same retinal area scale as the spatial apertures.
Our V2 disparity model is based on evidence of horizontal disparity in macaque
V2 [43, 44]. It pools information from multiple V1 functional columns by summa-
tion, generating a retinotopic ﬁeld of percepts. Each disparity percept in the model
indicates a “near,” “at” or “far” measure. “At” indicates zero disparity; “far” indicates
the image within the aperture is more distant than “at.” Likewise, “near” indicates
closeness.
V2 sends feedback to V1 [45] inﬂuencing shape perception, as well as motion
and disparity. V2 is assumed to receive feedback from V4 and MT. The Bipartite and
Tripartite models identify V2 connections forward to V4 and MT. These projections
are likely bidirectional, and retinotopic. Data supports a consistency of retinotopic
preservation throughout V1, V2, V4 and MT [15, 46]. We assume retinotopic feed-
back can assert ﬁne grain inﬂuence on the creation of a perception in the presence of
afferent stimulation, and the actual creation of (perhaps less ﬁne grained) percepts in
the absence of afferent stimulation. We speculate that as retinotopic mapping dimin-
ishes (which it seems to do as the visual pathways ascend into hierarchically higher
areas) the role of feedback changes. It may, for example, play more of an atten-
tion modulation role than a perception driving role. Examples may be inhibiting or
enhancing sections of a ﬂow ﬁeld at the MT level, or the priming of object discrimi-
nation at the IT level [11, 47]. The assumption we make that feedback can cause an
otherwise un-stimulated percept is the basis for our use of a belief mechanism within
minicolumns.
14.4.3
V2 Model Emulation
There is considerable cross talk between V2 stripes, especially within a stripe cycle
[30]. V2 layer 4 lacks the extensive layer 4 striation unique to V1, accountable to the
dense simple cell network inV1, and is likely to have about half the neurons per mm2

14
A Columnar V1/V2 Visual Cortex Modeland Emulation
285
as does V1. The area of V2 is approximately 75 % the area of V1. These anatomical
differences suggest V2 complexity is signiﬁcantly less than V1 complexity. Thus
using the V1 emulation as a yard stick, a V2 emulation is likely to “ﬁt” comfortably
in less than 75 % of the number of PS3 nodes used to emulate V1. One wishes to err
on the side of using too many nodes so that the engineering effort to ﬁt memory and
computational capabilities of a node are minimized.
Mapping an array of PS3 nodes with 14 nodes per row and 14 nodes per column
onto V2 produces an assignment of two stripe cycles per node, each node viewing
∼7 % of the vertical dimension. For emulation engineering purposes, we refer to
each PS3 as emulating one “V2 Subﬁeld.” This mapping has the beneﬁt of a 1:1
retinotopic relationship between V1 and V2 subﬁelds. Retinotopy in V2 is assumed
to be consistent with V1 retinotopy. The AFRL PS3 array at Rome NY clusters 22
PS3 nodes per subnet; thus three half columns ofV2 subﬁelds (21 subﬁelds) ﬁt onto a
subnet, with one “spare” left over. Each node would therefore represent about 1/2 %
of the V2 cortical area, on the order of 0.66 million neurons. By comparison, our V1
emulation covered about 1.3 million neurons per node.
There is evidence (macaque) of V2 intrinsic connectivity to an extent of about 4
mm (radius). Each human V2 subﬁeld corresponds to about 14 mm (perpendicular
to stripes) and perhaps 1.25 mm parallel to the stripes. Assuming a similar intrinsic
connectivity extent in humans, each V2 subﬁeld need only communicate (intrinsi-
cally) with its immediate horizontal neighbors, and to its vertical neighbor. Doubling
the extent does not require greater inter-subﬁeld communication.
Extrinsic afferent V2 connections are emulated using V1V2 projection messages:
orientation, disparity, and motion percepts (color in the future). There is also a return
(V2V1) carrying inhibition and excitation for each kind of percept.V1 emulation uses
a 14×14 subﬁeld array, as will V2. Therefore each V2 subﬁeld needs to subscribe to
percepts from its corresponding V1 subﬁeld. V2 functional columns, however, “see”
percepts from multipleV1 functional columns. This increase in aperture size requires
each V2 subﬁeld to subscribe to messages published by V1 subﬁelds surrounding its
corresponding subﬁeld.
The initial V2 model will have provisions for connecting extracortical feedback
into V2, but will otherwise not attempt to utilize feedback. Feedback into V2 will
be managed with three message types identiﬁed as “V4 to V2”, “MT to V2” and
“DistantCortical_to_V2”. The MT and V4 messages align with strong physiolog-
ical projections. The “DistantCortical” message is a preliminary design decision
based on:
•
the likelihood anatomical projections actually exist for these;
•
where they come from and what they do has not yet been thoroughly cataloged;
•
andthelikelihoodthattheirnotyethavingbeendescribedmeanstheyarerelatively
low bandwidth.

286
R. E. Pino and M. Moore
Fig. 14.11 V1 Percept display of some text. Only a portion of the perception display is illustrated,
as successive close-ups
14.4.4
Monitors for V2 efﬁcacy Evaluation
Our experience with the V1 model underscored the challenge of collecting data in
a full scale, real-time cortical model. The message passing scheme we adopted for
this project facilitates data collection. It is a Publish/Subscribe system (Pub/Sub).
An emulation process “publishes” a message; it is not aware of subscribers. Any
process may subscribe to any message type, and subscribers are not aware of who
the publishers are. Any source/destination correspondence needed by applications
in such a system must be provided within the message payload or metadata. Con-
nections between publishers and subscribers can be dynamic; occurring infrequently
but otherwise at any time. This strategy enables the creation of monitor processes
which intercept messages emulating axon projections carrying percepts.
The challenge is visualization of the information within a message. Ability to
record data onto a disk for post-run analysis is fundamental. We also desire real-time
information, readable by humans. We developed a monitor forV1 spatial perceptions
(Fig. 14.11 illustrates an example), of which there are about 25 K percepts per visual
frame. The process translates the percepts into miniature icons (8×8 pixel resolution
is used), places them onto a pixel buffer, and displays them using X11. The actual
monitor process runs on a Condor head-node; the X11 server is X-Win32 [48]. We
have found the X11 mechanism (X11 server running on a conventional Microsoft
PC platform) unable to keep up with the ﬂow of data; however, decimation produces
effective results that run between 1 and 10 frames per second. Rate depends on the
image size being projected; the larger the size, the slower the display update.
Similar “iconic” visualization is planned forV2 perceptions. Spatial percepts will
be iconic representations of the symbols in the lexicon. Disparity will be displayed
as a color coding; motion as a vector ﬁeld (arrow icons). We also plan to apply the
disparity measures for stereo vergence: the registration of the two cameras onto a

14
A Columnar V1/V2 Visual Cortex Modeland Emulation
287
ﬁxation point. The cameras presently use a conventional algorithm for this which
operates on pixel data. We will be able to compare the efﬁcacy of the two approaches.
14.4.5
Assessing Emulation Platform Performance
The objectives for our emulation efforts extend beyond the opportunities such a
facility offers for evaluation of alternative theories of cortical computation. We also
seek to measure the beneﬁts of emerging new architectural ideas. We have identiﬁed
the percent of ideal “ﬂoating operations per second” (FLOPS) as a valuable efﬁciency
characterization. The idea is that an architecture ideal for a particular algorithm will
be able (if programmed ideally) to spend all of the time doing arithmetic operations
instead of control and data movement operations.
Code optimization is an art as well as a science; it is difﬁcult to know when
optimization is nearly ideal. Therefore, it is difﬁcult to compare the performance
of a “benchmark” running on different architectures. We approach this problem
relying on the Pareto 80:20 principle and by analyzing the actual computational
characteristics of the algorithm. The Pareto 80:20 principle is attributed to the Italian
economist Vilfredo Pareto who in 1906 described the unequal distribution of wealth.
Applied to software optimization, it implies 80 percept of the beneﬁt is accomplished
by 20 % of the effort. We apply a list of strategies using sustained, focused effort
expended by expert coders to the optimization of our emulation code. The critical
path code is peer reviewed. The FLOPS is then (run-time) measured and compared
to the ideal. We assume our result (percent of ideal) accuracy is conservative, and
within 20% of actual. Results from the BSB attractor and V1 level 4 neural network
models running on PS3 nodes have been reported previously.
14.5
Conclusion
The immediate objective is not to demonstrate or compete with computer vision
methods presently in use, but rather to identify computational strategies that can be
used to assemble future systems using methods practical for computing technologies
decades in the future. The model is already a subject of interest atAFRL for emulation
using advanced chip technology. The models and emulations also serve as a facility
for evaluation of alternative models within the design space of the physiology and
anatomy of neural systems.
This exercise of attempting to utilize that which Neuroscience provides to “reverse
engineer” brain systems promotes insight that usually comes only from necessity. An
example of this is designing feedback from V2 pale stripes (form) into V1 that could
inhibit noise and strengthen perception. At ﬁrst it seemed theoretically impossible
for V2 to select which neurons to feed back into because mapping from V1 into V2
is not 1:1; it is 9:1. The feedback can work only when the areas from V1 feeding the

288
R. E. Pino and M. Moore
correct perception are “rewarded” by the feedback, and the areas supplying contrary
perception are inhibited. Our V2 model has no way to map this information back
to its precise source in V1. The solution to the dilemma is “temporal coding.” The
feedback wiring connects excitation to all parts of V1 that could potentially promote
the V2 perception; inhibitory to areas contrary to the perception. The coincidence of
the feedback with individual V1 cell activity creates the successful feedback map.
The existence of a facility capable of full scale real-time emulation of signiﬁcant
parts of the human neo-cortex enables the investigation of theories ﬁlling in the gaps
in neuroscience. V2 is a good example of this; the actual mechanisms at work within
V2 stripes, and the connectivity of the stripes, are not well understood [30]. Well
accepted theories regarding functional divisions of Parvocellular and Magnocellular
channels, blob and interblob minicolumns, have recently been challenged [31, 37].
Emulation provides a means to test the efﬁcacy of competing models against known
performance measures, and to predict performance that has not yet been measured
clinically. We expect the emulation effort to produce opportunities for technology
transfer. For example, the exploration of alternative disparity models may lead to
identifying efﬁcient stereo camera vergence methods.
A multidisciplinary approach facilitates the expansion of the solution space to
computational challenges. Experience with this effort suggests a signiﬁcant hurdle
is the building of an engineering community familiar with the science coming forth
from neuroscience, medicine and cognitive science communities. We have seen that
it is difﬁcult for traditionally experienced engineers to relate to the concepts prevalent
in neuro, cognitive and behavioral sciences. A large talent base is needed to exploit
the efﬁcient, adaptive, and scalable qualities evident in cortical systems. We feel it is
important to promote the multidisciplinary aspects of neuromorphic computing and
cultivate it into a main stream engineering and science discipline.
References
1. Livingstone M, Hubel D (1988) Segregation of form, color, movement, and depth: anatomy,
physiology, and perception. Science 240(4853):740–749
2. Moore MJ, Bishop M, Pino RE, Linderman R (2010) A columnar primary visual cortex (V1)
model emulation using a PS3 cell-BE array. 2010 IEEE World Congress on Computational
Intelligence, pp 1–8
3. Ahmed B, Anderson JC, Martin KAC, Nelson JC (1997) Map of the synapses onto layer 4
basket cells of the primary visual cortex of the cat. J Comp Neurol 380:230–242
4. Anderson JA (1993) The BSB model: a simple nonlinear autoassociative neural network,
associative neural memories. Oxford University Press, Inc., New York
5. Anderson JC, Martin KAC (2001) Does bouton morphology optimize axon length? Nat
Neurosci 4:1166–1167
6. Anderson JC, Martin KAC (2002) Connection from cortical areaV2 to MT in macaque monkey.
J Comp Neurol 443:56–70
7. Anderson JC, Martin KAC (2005) Connection from cortical area V2 to V3A in macaque
monkey. J Comp Neurol 488:320–330
8. Anderson JC, Martin KAC (2006) Synaptic connection from cortical areaV4 toV2 in macaque
monkey. J Comp Neurol 495:709–721

14
A Columnar V1/V2 Visual Cortex Modeland Emulation
289
9. Anderson JC, Martin KAC, Whitteridge D (1993) Form, function, and intracortical projections
of neurons in the striate cortex of the monkey Macacus nemestrinus. Cereb Cortex 3:412–420
10. Anderson JC, Binzegger T, Martin KAC, Rockland KS (1998) The connection from cortical
area V1 to V5: a light and electron microscopic study. J Neurosci 18:10525–10540
11. Kobatake E, Tanaka K (1994) Neuronal selectivities to complex object features in the ventral
pathway of the macaque cerebral cortex. J Neurophysiol 71:856–867
12. Leuba G, Kraftsik R (1994) Changes in volume, surface estimate, three-dimensional shape and
total number of neurons of the human primary visual cortex from midgestation until old age.
Anat Embryol 190:351–366
13. TovéeMJ(1996)Anintroductiontothevisualsystem. CambridgeUniversityPress, Cambridge,
pp 180–198
14. Peters A, Jones EG (1984) Cellular components of cerebral cortex. Plenum, New York
15. Dougherty RF, KochVM, BrewerAA, Fischer B, Modersitzki J,Wandell BA (2003)Visual ﬁeld
representations and locations of visual areas V1/2/3 in human visual cortex. J Vis 3(10):586–
598. http://journalofvision.org/3/10/1/, doi:10.1167/3.10.1
16. Koch C (1999) Biophysics of computation. Information processing in single neurons. Oxford
University Press, New York, p 87
17. Mountcastle VB (1997) The columnar organization of the neocortex. Brain 120:701–722
18. Lund JS, Angelucci A, Bressloff PC (2003) Anatomical substrates for functional columns in
macaque monkey primary visual cortex cereb cortex. J Neurosci 8:1594–1609
19. BuzásP,EyselUT,AdorjánP,KisvárdayZF(2001)Axonaltopographyofcorticalbasketcellsin
relation to orientation, direction, and ocular dominance maps. J Comp Neurol 437(3):259–285
20. KisvárdayZF,FerecskóAS,KovácsK,BuzásP,BuddJML,EyselUT(2002)Oneaxon-multiple
functions: speciﬁcity of lateral inhibitory connections by large basket cells. J Neurocytol
31:255–264
21. Hertz L (2004) Non-neuronal cells of the nervous system: function and dysfunction. Volume
31: part I: structure, organization, development and regeneration. Elsevier, Amsterdam. ISBN
0-444-51451-1 (3 volumes)
22. GerstnerW, KistlerWM (2002) Spiking neuron models. Single neurons, populations, plasticity.
Cambridge University Press, Cambridge
23. Black PE (2005, March 11) big-O notation. In: Black PE (ed), Dictionary of algorithms and
data structures [online]. U.S. National Institute of Standards and Technology, Gaithersburg
24. Hecht-Nielsen R (2006) Mechanism of cognition. In: Bar-Cohen Y (ed) Biomimetics:
biologically inspired technologies. CRC Press, Boca Raton
25. Reichert H, Rowell CHF, Gris C (1985) Course correction circuitry translates feature detection
into behavioural action in locusts. Nature, Lond 315:142–144
26. Wielaard J, Sajda P (2006) Neural mechanisms of contrast dependent receptive ﬁeld size in
V1. Adv Neural Inform Process Syst 18:1505–1506
27. Hubel DH, Wiesel TN (1959) Receptive ﬁelds of single neurons in the cat’s striate cortex. J
Physiol 148:574–591
28. Felleman DJ (2002) Area V2. In: Ramachandran VS (ed), Encyclopedia of the human brain
(pp 199–222). Academic Press, San Diego
29. Van Essen DC (2005) Corticortical and thalamocortical information ﬂow in the primate visual
system. Prog Brain Res 149:173–185
30. Serre T (2006) Learning a dictionary of shape-components in visual cortex: comparison with
neurons, humans and machines, PhD Thesis, CBCL Paper #260/MIT-CSAIL-TR #2006-028,
Massachusetts Institute of Technology, Cambridge, MA
31. Sincich L, Horton J (2005) The circuitry of V1 and V2: integration of color, form, and motion.
Annu Rev Neurosci 28:303–326
32. Roe AW, Ts’o DY (1995) Visual topography in primate V2: multiple representation across
functional stripes. J Neurosci 15:3689–3715
33. Roe AW, Ts’o DY (1997) The functional architecture of area V2 in the macaque monkey. In:
Rockland KS, Kaas JH, Peters A (eds), Cerebral cortex vol. 12: extrastriate visual cortex in
primates. Plenum Press, New York, pp 295–333

290
R. E. Pino and M. Moore
34. Prince SJ, Pointon AD, Cumming BG, Parker AJ (2002) Quantitative analysis of the responses
of V1 neurons to horizontal disparity in dynamic random-dot stereograms. J Neurophysiol
87:191–208
35. Tootell RBH, Dale AM, Sereno I, Malach R (1996) New images from human visual cortex.
Trends Neurosci 19:481–489
36. Shushruth S, Ichida JM, Levitt J, Angelucci A (2009) Comparison of spatial summation
properties of neurons in macaque V1 and V2. J Neurophysiol 102(4):2069–2083
37. Nassi JJ, Lyon DC, Callaway EM (2006) The parvocellular LGN provides a robust disynaptic
input to the visual motion area MT. Neuron 50(2):319–327
38. Rockland KS, Ichinohe N (2004) Some thoughts on cortical minicolumns. Exp Brain Res
158:265–277. doi:10.1007/s00221-004-2024-9
39. Hegdé J, Van Essen DC (2000) Selectivity for complex shapes in primate visual area V2. J
Neurosci 20:1–6
40. Ito M, Komatsu H (2004) Representation of angles embedded within contour stimuli in area
V2 of macaque monkeys. J Neurosci 24:3313–3324
41. Riesenhuber M, Poggio T (1999) Hierarchical models of object recognition in cortex. Nat
Neurosci 2:1019–1025
42. Serre T, Kouh M, Cadieu C, Knoblich U, Kreiman G, Poggio T (2005) A theory of object
recognition: computations and circuits in the feedforward path of the ventral stream in primate
visual cortex, CBCL Paper #259/AI Memo #2005-036, Massachusetts Institute of Technology,
Cambridge, MA
43. Chen G, Lu HD, Roe AW (2008) A map of horizontal disparity in primate V2. Neuron 58:442–
450
44. Lu HD, Chen GC, Tanigawa H, Roe AW (2010) A motion direction map in Macaque V2.
Neuron 68(5):1002–1013
45. Bullier J (2006) What is fed back? In: vanHemmen JL, Sejnowsky TJ (eds) 23 Problems in
systems neuroscience. Oxford UP, New York, pp 103–132
46. Fize D; Vanduffel W, Nelissen K, Denys K, Chef d’Hotel C, Faugeras O, Orban GA (2003)
The retinotopic organization of primate dorsalV4 and surrounding areas: a functional magnetic
resonance imaging study in awake monkeys. J Neurosci 23(19):7395–7406
47. Tanaka K, Saito H, Fukada Y, Moriya M (1991) Coding visual images of objects in the
inferotemporal cortex of the macaque monkey. J Neurophysiol 66:170–189
48. Starnet Communications Corporation (2010) X-Win32 [web site] http://www.starnet.com/
products/xwin32/.Retrieved January 2011

Chapter 15
Polymer and Nanoparticle-Composite Bistable
Devices: Physics of Operation and Initial
Applications
Robert A. Nawrocki, Richard M. Voyles, and Sean E. Shaheen
Abstract Polymer and nanoparticle-composite bistable devices, which fall under
the category of Organic Bistable Devices (OBDs), provide non-volatile, two-state
ON/OFF behavior for potential memristor or other applications. These materials
consist of insulating, semiconducting, or conducting polymers such as poly(meth-
ylmethacrylate) (PMMA), poly(vinylcarbazole) (PVK), or poly(3,4-ethylenediox-
ythiophene):poly(4-styrenesulfonate) (PEDOT:PSS), respectively. Composites can
be made by blending these with inorganic nanoparticles made from materials such as
silver, gold, or zinc oxide. Such devices offer several potential advantages compared
with their inorganic counterparts. These include reduced cost from solution-based
methods of deposition, ease of direct-write printing over large areas, high throughput
processing, and light-weight and ﬂexible mechanical properties. Here we review the
materials, designs, physics, and operation of these devices. We conclude the chapter
withadiscussionofpossibleapplications, includingrecentadvancesinneuromorphic
engineering speciﬁcally geared toward use in robotics.
15.1
Organic Electronics
Organic electronics is a branch of electronics that is based on conducting and semi-
conducting polymers and molecules [1, 2]. It relies on the electronic properties of
pi-conjugated molecules that derive their electronic conduction from alternating se-
quences of single and double carbon–carbon bonds along their main chain. The ﬁeld
of organic electronics has been the focus of fundamental research for several decades
and is now to the point of commercial application.
Intense research and development continues in the area with the goal of realizing
novel, next generation electronics for a multitude of applications that utilize the
intrinsic tunability, low cost, and ease of processing that the materials have to offer.
R. A. Nawrocki () · R. M. Voyles
Department of Electrical and Computer Engineering, University of Denver,
Denver, CO 80208, USA
e-mail: Robert.nawrocki@du.edu
S. E. Shaheen
Department of Physics and Astronomy, University of Denver, Denver, CO 80208, USA
R. Kozma et al. (eds.), Advances in Neuromorphic Memristor Science and Applications,
291
Springer Series in Cognitive and Neural Systems,
DOI 10.1007/978-94-007-4491-2_15, © Springer Science+Business Media Dordrecht 2012

292
R. A. Nawrocki et al.
15.1.1
Organic Electronic Circuit Elements
Prior to the 1970s, polymers were generally restricted to use as insulators. In was not
until 1977 when scientists at the University of Pennsylvania [3] demonstrated that
treating polyacetylene with halogen compounds increased its electrical conductivity
to values akin to that of a metal. Subsequently, work on polymeric conductors and
semiconductors evolved over the next several decades. Today, most electronic circuit
components commonly fabricated with inorganic elements, such as silicon or ger-
manium, have already been demonstrated at some level in the polymer electronics
ﬁeld.
All-polymer capacitors have been reported as early as the 1970s [4] and are cur-
rently available commercially. The ﬁrst organic thin ﬁlm transistor was reported in
1983 [5], followed by the ﬁrst all-polymer thin ﬁlm transistor reported circa 1990 [6].
Entire electric circuits made from all-polymer components have been demon-
strated, including D ﬂip-ﬂops [7], shift registers [8], and ring oscillators [9]. A
128-bit organic RFID transponder chip with Ethernet-style encoding and wireless
anti-collision protocol was demonstrated [10]. Complete circuits that allow for
hardware-based cryptography for the use with RFID tags were also shown [11]. In
early 2011, researchers at IMEC in Belgium have reported a 4000-organic-transistor,
8-bit logic, physically ﬂexible processor fabricated on a 25 μm thin plastic foil. Ca-
pable of executing about six hard coded instructions per second, the processor is
appropriate for use in RFID tags [12].
Organic Light Emitting Diodes (OLEDs) were reported as early as 1990 [13] and
became commercially available around 2004. Currently, stand-alone OLED indica-
tors are commercially available, as are consumer electronic products that incorporate
larger format OLED technology such as smart phones, television sets, and portable
games1. Organic Photovoltaic devices (OPVs) offer the promise of low production
cost in high volumes [14, 15]. Niche commercial products such OPV-based solar
computer cases and backpacks are currently available2.
Organic Bistable Devices (OBDs) [16], the topic of this chapter, are not currently
availablecommercially. Theirmostwidelycitedapplicationistheirabilitytofunction
as a memory element, as is discussed in detail below. This application and others are
being pursued by numerous university research groups around the world as well as
research centers such as Belgium-based Imec and Netherlands-based Holst Centre3.
1 Samsung and Sony have both released standalone OLED displays, and Samsung is currently
offering a smart phone featuring OLED screen. Sony PlayStation vita, released in late 2011, uses
ﬁve inch OLED display.
2 In 2011, Konarka Technologies (www.konarka.com) began substantial sales of ﬂexible OPV mod-
ules that are incorporated into consumer products by several vendors. Other companies such
as Heliatek GmbH (www.heliatek.com) and Solarmer Energy Inc. (www.solarmer.com) have
announced intentions to follow suit in 2012.
3 Imec (www2.imec.be/be_en/home.html) and Holst Centre (www.holstcentre.com)

15
Polymer and Nanoparticle-Composite Bistable Devices . . .
293
Fig. 15.1 Polymer electronics can be made using inexpensive solution processing. a Spin coater
used to deposit a thin layer of material (down to few tens of nanometers). b Inkjet printer used to
print metallic or polymer contact electrodes. (Courtesy of FUJIFILM Dimatix, Inc)
15.1.2
Advantages and Disadvantages
Manufacturing inorganic electronics requires high temperatures, typically 400–
1400 ◦C, high-vacuum, and very clean environments. These can result in very high
production costs. In the research and development phase, organic electronics are
often also fabricated in clean room facilities. However, organic electronics tend to
be more tolerant to particulate contamination. It is therefore anticipated that the
production stage will not require full clean room facilities, but rather that the local
environment in the vicinity of the substrate during printing or deposition be kept
within acceptable particulate levels. This is expected to have a direct effect on low-
ering the cost. Additionally, the substrate temperatures required for fabrication of
organic devices are much lower and thus more compatible with manufacturing pro-
cesses of plastic substrates. This may enable new, low cost integration of devices
with plastics used in everyday consumer items.
Solution processing of polymer electronics in particular is a key enabler of both
low costs and rapid prototyping and fabrication. For laboratory research and devel-
opment, spin coating (Fig. 15.1a) is often the technique of choice due to its simplicity
and the good uniformity of thin ﬁlms that it can produce. For larger scale fabrication,
direct-write printing methods such as inkjet printing (Fig. 15.1b), spray coating, slot
die printing, or ﬂexographic printing are possible [17]. Printing has been used to
produce electronic elements such as capacitors [18], transistors [19], OPVs [20, 21],
and OLEDs [22]. Other examples of solution processing include doctor blading [23]
and laminating [24].
The use of ﬂexible substrates for organic electronics is advantageous both for
productionandforapplication4. Transistors[26], capacitors[27], OPVs[28], OLEDs
[29], and OBDs [30] have all been demonstrated on ﬂexible substrates. Figure 15.2
4 This is also true for inorganic semiconductors, and a number of research groups, such as [25],
have produced physically ﬂexible inorganic electronic components.

294
R. A. Nawrocki et al.
Fig. 15.2 Physically ﬂexible
polymer substrate used to
mass produce printed
electronics. (Courtesy of
PolyIC GmbH & Co.)
shows an example of roll-to-roll printing of organic transistor circuits on industrial
scale.
There are however several challenges and disadvantages that are characteristic
of organic electronic materials and devices. With regards to organic circuits, the
price paid for ease of processing comes in the form of the operating bandwidth
of the devices. Since carrier mobilities in organic semiconductors are typically or-
ders of magnitude lower than in inorganic semiconductors, transistors and other
devices are limited to much slower speeds.Also, solution processing of nanoscale de-
vices with length scales approaching those of today’s IC devices (10–20 nm regime)
is challenging and will require advancements in techniques such as nanoimprint
lithography [31].
15.2
Overview of Organic Bistable Devices
15.2.1
General Operation
A bistable device is a device that can stably exist in one of two states. An example
of such a device might be a strip of a magnetic tape that can be magnetized in
one of two possible directions, for instance up or down, that represent the two
states. One apparent application of such a device is a memory element. A number
of different technologies that exhibit bistable behavior, based on both inorganic and
organic elements, have emerged over the years. One example is an optically bistable
device. Such a device relies on optical characteristics of materials with two resonant
transmission states that result in two different power outputs for the same input power
[32]. Another example is a phase-change memory [33], in which the material can be
in two possible morphological states: a crystalline phase with low resistance and an
amorphous phase with high resistance.
An Organic Bistable Device (OBD) is an electrically bistable, non-volatile, two-
terminal device that relies on organic or organic–inorganic hybrid materials in its
active layer. OBD’s are referred to by several different monikers in the literature,
including Organic Bistable Memory Device, Bistable Organic Memory, or Organic
Non-Volatile Memory. For the purposes of this chapter, the term OBD will be used.

15
Polymer and Nanoparticle-Composite Bistable Devices . . .
295
Fig. 15.3 Operational I–V
characteristics of the two
general types of OBDs,
bipolar and polar. a For a
bipolar OBD, the device is
either in the OFF state
(ROFF—high resistance, step
1, blue or dark in grayscale)
or ON state (RON—low
resistance, step 3, red or light
in grayscale) depending on
past voltage. Yellow arrows
represent switching to either
the ON (step 2) or OFF (step
4) state. The element can be
turned ON by increasing the
input voltage above the
positive threshold value
(VON). Analogously, it can be
turned OFF by decreasing the
input voltage below the
negative threshold value
(VOFF). b In a polar OBD,
both of the ON and OFF
transitions occur for the same
input voltage polarity, either
positive or negative
The earliest publications of electrical switching behavior in thin-ﬁlm devices
were reported about three decades ago [34]. An OBD was ﬁrst reported in 2002 [35],
when a thin metal layer was embedded within an organic active medium to induce
the two-state behavior.
15.2.2
Switching Behavior
There are two types of switching behaviors observed in OBDs, bipolar and unipolar,
depicted in Fig. 15.3a and b respectively [16]. The operation of bipolar switching can
be summarized as follows. The device is in the OFF state (high resistance—step 1)
until the input voltage is increased past a positive threshold voltage (VON—step 2).
It remains in the ON state (low resistance—step 3) until the voltage is reduced below
a negative threshold voltage (VOFF—step 1). Steps 2 and 4 correspond to ON and
OFF switching, respectively.

296
R. A. Nawrocki et al.
Fig. 15.4 Operational I–V
characteristics of a WORM.
The device originally operates
in one region (state), either
OFF or ON (1 or 3
respectively). Similar to
OBDs, once the input voltage
exceeds the threshold voltage,
the device switches to a
different state. However,
contrary to OBDs, in WORM
the switch is permanent; once
the state is changed, it cannot
be reversed
The polar switching OBD, in Fig. 15.3b, operates in a very similar manner to the
bipolar switching device. Here, both the ON and OFF transitions occur for the same
polarity of the input voltage. The OFF threshold can be higher or lower than the ON
threshold. The polar switching can occur for either the positive or negative voltage.
For both the polar and bipolar switching OBDs, when the input voltage is kept in
the operating region (Vin < |VON| and |VOFF|), the device will stay in its current state
(either ON or OFF). Setting the input voltage outside of the operation region, past a
threshold value, the device enters into the switching region, (Vin > |VON| and |VOFF|),
with a subsequent changing of its state (ON to OFF, or OFF to ON).
One of the main characteristics of an OBD is the ON/OFF ratio, which is the
difference between the ON and OFF currents. Values as high as seven orders of
magnitude have been reported. Another important aspect of an OBD is the retention
time, which has been reported as 105 s with 105 read/write cycles [36].
A Write-Once-Read-Many (WORM) is a device that can be considered as a sub-
class of OBDs. The difference between a WORM and an OBD is that in a WORM
the change of state, to either ON or OFF, is permanent; no amount of reverse voltage
will result in the device switching back to its original state. This point is illustrated
in Fig. 15.4, in which two types of WORM devices can be distinguished. In the ﬁrst
type, when the device is ﬁrst created, it is in the OFF state. It can then be permanently
set to an ON state [37]. In the second type, the device is ﬁrst in an ON state but can
then be permanently switched OFF [38]. For many applications the permanent nature
ofWORM is a limitation. However, there are certain applications, such as RFID tags,
where it is desirable to have a permanent, non-erasable type of memory.
Figure 15.5 illustrates I–V relationship of devices created in our laboratory orig-
inally inspired by the work of Son et al. [39]. Figure 15.5a shows the operation of
a bipolar OBD. The device is ﬁrst in an OFF state. When the input voltage exceeds
about 0.75 V, the device turns ON, with the subsequent change in output current. It

15
Polymer and Nanoparticle-Composite Bistable Devices . . .
297
Fig. 15.5 I–V relationships, shown on log–linear scales, for two-terminal organic devices created
in our laboratory. a The OBD switches ON at approximately 0.7 V and returns back to the OFF
state at approximately −0.5 V. The arrows indicate the sweeping direction. The ON/OFF ratio can
be seen as ∼102. bA WORM device with an ON/OFF ratio of ∼104. When the input voltage is kept
below a threshold value (shown by the dashed arrows) the device is in OFF state (input voltage,
seen in red, was limited between ±0.5 V). When the input voltage exceeds the threshold value
(displayed by solid arrows) of approximately 1.2 V, the device switches to the ON state. The device
will stay in the ON state permanently
will switch to OFF state when the input voltage exceeds negative threshold, in this
case about −0.25V. Different marks and colors are used to indicate three consecutive
scans, all revealing the ON and OFF switching of the device. Figure 15.5b shows the
operation of a WORM. When the input voltage is kept below threshold voltage, the
device is in OFF state. This is illustrated by two successive ±0.5 V scans, marked
“1st scan” and “2nd scan”. When the input voltage exceeds threshold value of about
1.1 V, the device switches to ON state. This is shown as “3rd scan”. The device then
remains in the ON state permanently. As indicated in the “4th scan”, application of
negative input voltage does not result in turning the device back to the OFF state.
15.2.3
OBD Varieties
The memristor is considered the fourth fundamental electrical element [40], along-
side the resistor, the capacitor, and the inductor. A memristor is as an element that

298
R. A. Nawrocki et al.
Fig. 15.6 Operational I–V
characteristics of a memory
transistor. Two different
drain-source currents (IDS)
can be measured depending
on the sweeping direction of
the gate-source voltage (VGS)
relates ﬂux to charge; it therefore has the ability to modify its conductivity according
to the time history of the voltage that has been applied. Due to their typically
highly non-linearly behavior beyond a threshold voltage, memristors can function
as bistable devices. The ﬁrst memristors were made of thin ﬁlms of titanium dioxide
[41]. Shortly thereafter, several groups began reporting the creation of memristors
with at least one layer of organic or polymer material. The ﬁrst such group was at
the University of Parma, Italy [42]. They demonstrated a three-terminal, polymeric,
non-linear electrochemical device with an I–V characteristic based on hysteretic
relationship. Another organic memristor was reported by the group at the University
of California Davis [43]. Their device was based on a 2.8-nm thick monolayer of
cadmium stearate ﬁlm sandwiched between a bottom platinum electrode and top
electrode formed from Pt/Cu/Cu2S. The function of the organic monolayer was to
provide a precise gap between the electrodes as well as to allow the motion of metal
ions for a memristive effect.
Another class of electrical devices that exhibit hysteretic behavior is a memory
transistor. In memory OFETs, reported as early as 2001 [44], for a single gate-
source voltage (VGS) two different drain-source current (IDS) values can be measured
depending on the sweeping direction. Their I–V characteristic curve is similar to that
of a Schmitt trigger, albeit the current is always positive. The operation of an ideal
memory transistor is illustrated in Fig. 15.6. OFETs rely on the semiconducting
properties of polymer thin ﬁlms. In a memory OFET, the polymer semiconductor
changes state depending on the direction of the voltage sweep. The physics of this
resistive change is similar to that observed in two-terminal OBDs and is described
in more details in the following section. Devices in which the memory effect is
photoinduced have also been reported [45]. The difference in IDS between the ON
and OFF states has been reported as high as almost four orders of magnitude [46].
15.3
OBD Structures and Mechanisms
15.3.1
Device Architecture
In an OBD the electrical bistability is enacted by a physical or chemical change to the
device. This change depends on the types of materials used. A general architecture

15
Polymer and Nanoparticle-Composite Bistable Devices . . .
299
Fig. 15.7 General device architecture of an OBD. The top and bottom electrodes (TE, BE) are
typically made from a thin layer of metal. The active layer (AL), which may itself consist of
multiple layers or materials, is responsible for the electrical switching of the device. The hole or
electron injection layers are often included to overcome energetic barriers at the interfaces or to
increase the adhesion. Typically the TE serves as the cathode, and electrons ﬂow from the TE to
the BE
for an OBD is shown in Fig. 15.7. Any OBD has a minimum of three layers: top
electrode (TE), bottom electrode (BE), and active layer (AL). The TE and BE are
primarily used as device contacts, although in some cases they can also play a role
in the switching mechanism. However, the AL is the layer that is usually where
the physical or chemical changes occur and therefore responsible for the electrical
bistability. It can be either a single material layer, or it can be comprised of several
layers or materials. In any type of polymer or organic electronics device, a crucial
factor is the interface between organic (i.e., polymer) and inorganic (e.g., metal)
materials. Interface engineering is of great importance as it affects the functionality
and effectiveness of organic electronics devices [47]. It involves alignment of surface
energy and work functions that play a signiﬁcant role in the molecular morphology
and energetics that dictate the rates of charge injection [48, 49]. As a result some
devices incorporate an electron injection layer (EIL) or a hole injection layer (HIL)
to optimize the injection characteristics. Electronically, these layers can also act to
block the opposite type of charge carrier from being transported through the device.
Mechanically, these layers can also act to increase wettability or adhesion between
the AL and the electrodes.
15.3.2
Material and Device Fabrication
15.3.2.1
Nanoparticle Synthesis
One common form of operation of OBDs relies on trapping of injected charges
by inorganic nanoparticles (NPs) embedded in a thin layer of polymeric ﬁlm. The
nanoparticles can consist of metals, commonly gold or silver, or metal oxides, such
as zinc oxide. Synthesis of NPs can allow the control of the atomic composition, the
size, and also the shape of the particles.

300
R. A. Nawrocki et al.
Fig. 15.8 Transmission
electron microscopy image of
ZnO NP’s synthesized using
dimethylformamide (DMF)
as a solvent. Individual
particles can be seen with an
average size of ∼15 nm per
particle. (Courtesy of Roy
Geiss at Nanomaterials
Characterization Facility at
University of Colorado
Boulder)
The methods for synthesizing NPs can be broadly divided into two categories.
The ﬁrst method is a “top-down” approach based on reduction of large particles into
smaller ones, usually via physical milling or some form of etching. Although it is
simple to carry out, this method suffers from a lack of control of the size of the
nanoparticles leading to a wide distribution of sizes. Also the electronic quality of
the surface of the nanoparticles is typically poor.
The second synthesis method is a “bottom-up” approach involving chemical re-
action of precursor molecules in solution. This method allows for the control of the
size and often the shape of the molecules. It also provides good uniformity in the
sizes of the nanoparticles and the possibility for well-controlled surface chemistries.
The controlling factors of the chemical synthesis include the reducing agents, lig-
ands, reaction volume, and temperature and time of the reaction. The reducing agent
acts as a deprotinator to help the metal bond with other particles/metals. Its choice
also correlates to the size of the NPs. The shape of the nanoparticle can also be
strongly inﬂuenced by the presence of other organic species. For instance, a cubic
or spherical shape can be obtained with the use of polyvinylpyrrolidone (PVP) or
citrate, respectively [50, 51]. Reaction temperature strongly inﬂuences the structure
and aggregation of NPs; higher temperature requires longer reaction time but also
produces more ordered NPs [52]. Because of the scattering of light, the color of a
solution is very indicative of the properties of the NPs, such as their size. For in-
stance a light yellow solution of Au NPs indicates sizes of a few to a few tens of
nanometers, while a dark or black solution indicates a size of at least a few hundred
nanometers. Similarly, a perfectly clear zinc oxide nanoparticles solution indicates a
particle size of few nanometers, while a hazy solution results from sizes of individual
nanoparticles or aggregates of nanoparticles being at least a few tens of nanometers.
Zinc oxide nanoparticles (ZnO NPs) are among the simplest to synthesize and
can be easily embedded in a thin ﬁlm of polymer, such as PMMA, for the purpose of
charge trapping to induce bistable behavior. Figure 15.8 shows a transmission elec-
tron microscopy image of ZnO NPs synthesized using dimethylformamide (DMF)
as a solvent. Individual nanoparticles with an average size of ∼15 nm can be seen.

15
Polymer and Nanoparticle-Composite Bistable Devices . . .
301
Fig. 15.9 a Device
architecture of an OBD,
following [39]. The TE is
formed from a layer of Al
with a thickness of ∼100 nm.
Poly(methylmethacrylate)
doped with zinc oxide
nanoparticles (PMMA:ZnO
NPs) forms the AL. Indium
Tin Oxide (ITO) serves as the
BE, which is patterned to
form a crossbar with the TE.
b An example of a physically
ﬂexible OBD on a PET
substrate
15.3.2.2
Solution Processing of OBDs
This section focuses on solution processing methods to OBD fabrication. Below we
describe the fabrication of a typical device as reported by [39] that is compatible
with production processes of most polymer or organic electronics devices.
The device architecture is shown in Fig. 15.9a. The OBD is fabricated on
a glass or plastic substrate with Indium Tin Oxide (ITO) forming the BE, with
a thickness of approximately 150 nm. The AL is deposited from a solution of
poly(methylmethacrylate) (PMMA) doped with ZnO NPs. Spin coating or larger
scale printing processes as described earlier are used for the deposition to yield
an AL of approximately 100 nm in thickness. A thermal annealing step is then
carried out to outgas solvent molecules from the ﬁlm. This process may also
aid with the degree of ordering of the AL ﬁlm both in the bulk and at inter-
faces by inducing relaxation of the molecules. Annealing temperatures for most
polymers are typically 110–150 ◦C, which is below the melting point of plastics
used as device substrates such as poly(ethylene terephthalate) (PET). This anneal-
ing is performed in an inert atmosphere to avoid the oxidation of the polymeric
molecules. The TE is then typically deposited via vacuum thermal deposition,
with the device area being deﬁned by a shadow mask. However, other atmo-
spheric deposition techniques of the TE are also possible. For instance, lamination
[24] or spray coating of poly(3,4-ethylenedioxythiophene):poly(4-styrenesulfonate)
(PEDOT:PSS) followed by screen printing of silver paste [53] have been demon-
strated for TE fabrication of organic photovoltaics.

302
R. A. Nawrocki et al.
Figure 15.9b illustrates an example of an OBD formed on a ﬂexible substrate. TEs
obtained from six metal stripes, labeled A through F, can be seen as being deposited
on the device. The AL and BE were formed from transparent materials and are not
easily seen.
15.3.3
Switching Mechanisms
When the input voltage of an OBD is kept below the threshold voltage, the device
can often be approximated as operating as a linear resistor. Therefore, to simplify
the analysis and reduce its computational complexity, in a circuit design the device
can be replaced by two resistors and a switch. The switch determines whether high
or low resistance, corresponding to either the OFF and ON state, should be used in
the analysis.
When the input voltage exceeds the threshold value the device momentarily ex-
hibits a highly non-linear behavior and switches between states. There are multiple
physical mechanisms by which the switching process can occur and the state is stored
in the OBD. These rely on processes such as charge trapping in the organic layer, ﬁeld
dependent charge carrier mobility, and charge injection from the electrodes [54]. A
given device may rely on one or more such mechanisms, and it can occur that in the
literature a device is reported without full understanding of which mechanism(s) are
responsible. Four of the most common mechanisms are discussed next.
15.3.3.1
Electromigration of Metal Into the Active Layer
The ﬁrst mechanism we cite is electromigration of metal ions into organic active
layer [55]. In a metal insulator metal (MIM) sandwich, the device is initially in
the OFF state (high resistance). However, when the potential exceeds threshold,
small amounts of metal ions are injected from the anode into the polymer insulator
sandwiched between two metal electrodes. This ion migration, due to the high electric
stress at the tip of a ﬁne conducting ﬁlament, acts to decrease the physical separation
of the two electrodes. This reduces the total resistance and changes the state from
OFF to ON [56]. When a reverse potential is applied the impurities are extracted
from the insulator back to the metal contact thereby increasing the total resistance
across the polymer, switching from ON to OFF.
15.3.3.2
Formation of Conductive Filaments Through Redox Behavior
Anothermechanismthatisinvokedisbasedonoxidation–reductionbehavior[57]ofa
conducting polymer such as PEDOT:PSS. When a voltage is applied to the electrode,
the polymer chains are oxidized to PEDOT + by the injected carriers. Facile pathways
for current ﬂow form along the PEDOT + chains, and the device switches from the

15
Polymer and Nanoparticle-Composite Bistable Devices . . .
303
OFF to the ON state (high resistance to low resistance). The PEDOT + chains are
then reduced to neutral PEDOT0 chains by injection of carriers when a voltage is
applied in the opposite direction. The current paths are then destroyed, and the device
switches from the ON to the OFF state.
15.3.3.3
Charge Trapping
Another mechanism employed to explain the electrical bistability is based on charge
trapping in a semiconducting polymer whose carrier mobility is dependent upon
the charge carrier concentration [39]. In this scenario, the polymer is embedded
with conductive nanoparticles (NPs). At ﬁrst, when the device is created and tested,
the resistance is high. However when the voltage potential exceeds a critical limit,
electrons become trapped in the NPs. Subsequently the embedded electrons form a
conductive path and allow for the current to ﬂow more freely (ON state). This results
from the carrier concentration dependence of the mobility in organic semiconductors,
which is well established [58, 59]. A higher concentration of carriers increases the
carrier mobility, and hence the conductivity, by ﬁlling trap states that would otherwise
impede carrier transport. The excess carriers also increase the dielectric constant that
changes the activation energy for carrier transport [60]. Reversing the voltage polarity
has the effect of pulling the electrons out of the NPs and switches the device back
to the OFF state. The polarity of the device is determined by an asymmetry in the
work functions of the electrodes. Such a trap ﬁlling mechanism has been analyzed
with a Space Charge Limited Current (SCLS) model [61], where it was found that
the ON/OFF ratio of the device increased with increasing trap depth and resulting
density of trapped electrons [62, 63].
15.3.3.4
The Ferroelectric Effect
Ferroelectric polymers, whose electric dipole can be modiﬁed by the application of
external electric ﬁeld, have also been successful utilized for the switching mecha-
nism in OBDs. These polymers, which are typically insulating, can be blended with
semiconducting polymers to enable the working OBD. The switching mechanism
in this case is due to changes at one of the electrode/semiconductor interfaces. Ini-
tially, an energetic barrier at the interface results in injection limited current in the
device. Then, as the device is biased, polarization charges generated in the ferro-
electric polymer are compensated by charges from the metal electrode. A portion of
the compensated charges also occupies the semiconducting polymer, leading to band
bending that reduces the energetic barrier at the interface. As a result, the current
in the device increases to form the ON state and becomes space charge limited as
opposed to injection limited in its functional form. The other electrode interface is
typically engineered to avoid this effect. What results overall is a rectifying behavior
in the device with the current modiﬁed by the ferroelectric polarization [64].

304
R. A. Nawrocki et al.
15.3.4
Active Layer Materials
TheActive Layer (AL) is the layer responsible for switching of the device’s resistance
between the ON and OFF states. A number of different materials, doped or undoped,
and a combination of different materials, can be all considered for use in the AL.
Here we survey some of the established material combinations.
15.3.4.1
Polymers Blended with Inorganic NPs
A common choice for the AL is a thin layer of insulating polymer blended with
inorganic nanoparticles (NPs) to enable the charge trapping mechanism for OBD
behavior. The AL can then be sandwiched between two metal contacts. For instance,
gold nanoparticles (Au NPs) embedded in polystyrene with the TE and BE formed
from aluminum have been demonstrated to function as an OBD [62]. The device
changed states typically at 3 V with an ON/OFF ratio of approximately four orders
of magnitude.
In another example [39], PMMA was doped with highly conductive ZnO quantum
dots and sandwiched between ITO and an aluminum. This OBD turned ON at ∼1.5V,
with the ON/OFF ratio as large at 5 × 104. The switching mechanism was explained
via charge trapping.
Poly(3-hexylthiophene) (P3HT) doped with Au NPs has been used as the AL
with ITO and Al as electrodes [49]. In this conﬁguration, which demonstrated an
ON/OFF ratio of ∼103 charge trapping was again determined to be responsible for
the bistability.
A somewhat more complex device architecture was implemented by Kim et al.
[65]. An AL of PMMA doped with CdSe NPs was separated from the ITO BE by a
layer of insulating PMMA. The TE was then formed fromAu and was separated from
the AL by a thin layer of semiconducting pentacene. The switching was explained
via charging of the NPs to induce tunneling transport through the AL.
15.3.4.2
Organic–Inorganic–Organic Structures
The very ﬁrst OBD was constructed using a structure based on sandwiching a
thin layer of inorganic material between two organics [35]. An 80 nm layer of Al
was placed between two 50 nm thick layers of 2-amino-4,5-imidazoledicarbonitrile
(AIDCN). The reported ON/OFF ratio, explained via charge trapping, was as large
as 5 × 103 with a threshold voltage of 2 V. A slightly more complex design was
shown by Lee et al. [48]. The inorganic layer, designed to trap electrons, was formed
from three separate layers: insulating LiF (4 nm), conducting Al (3 nm), and another
insulating LiF (4 nm). This three-layer structure was placed between two layers of
tris-(8-hydroxyquinolinato) aluminum (Alq3). The TE and BE were both formed
from thin layers of aluminum patterned to form a cross bar. This bipolar OBD
changed states at approximately ±4 V.

15
Polymer and Nanoparticle-Composite Bistable Devices . . .
305
Recent interest in carbon-based materials has resulted in investigating its insu-
lating or conducting properties for the use as an AL in OBDs. A 2 nm thick layer
of graphene sheets (about ﬁve layers) was placed between two insulating layers of
PMMA [30]. Top and bottom electrodes were formed from aluminum and ITO re-
spectively. The device was formed on ﬂexible PET. The bistable behavior of the
insulator–graphene–insulator device was attributed to the conducting ﬁlaments be-
ing formed in the PMMA layer at the state of transition. The bipolar switching
device was demonstrated to possess excellent ON/OFF ratio, even after bending
of the substrate, of almost seven orders of magnitude, with the turn ON voltage
occurring at ∼3 V. The same group demonstrated that graphene could be replaced
with graphite. A total of 30 layers of graphite, with a total thickness of about 10 nm,
termed an Ultrathin Graphite Sheet (UGS), were stacked in the same geometry as be-
fore: PET/ITO/PMMA/UGS/PMMA/Al [36]. Compared to the device that utilized
graphene, the UGS device exhibited switching behavior at slightly lower voltages,
of about 2 V, with an ON/OFF ratio of close to 6 orders of magnitude.
15.3.4.3
Metal–Insulator–Metal Structures
A common OBD design involves sandwiching a very thin layer, usually only a
few nanometers, of insulating polymer between two metallic layers. This metal–
insulator–metal (MIM) has been demonstrated as a simple architecture that exhibits
the electrical bistability characteristic of OBDs. Parylene-C, a transparent, insulat-
ing, and chemically stable polymer, was deposited using polymer chemical vapor
deposition (CVD) and used as the AL [66]. The BE was formed from a layer of
tungsten. Three different metals, Al, Cu, and Pt were demonstrated as viable choices
for TE. The redox reaction was suggested as the resistive switching mechanism of
this bipolar OBD with ON/OFF ratios of about 104. In a similar scheme [67] demon-
strated bistable behavior in a device in which the BE was formed from a highly doped
p-silicon, and the TE was formed from thermally evaporated Ag wires. The bipolar
switching device consisted of an AL made from a 30 nm thick ﬁlm of polyﬂuorene-
derivative (WPF-oxy-F). The suggested conduction mechanism, in the WPF-oxy-F
layer, was formation of conducting metallic ﬁlaments due to the migration of metal
ions from the electrodes. The change in resistance between the ON and OFF states
was determined to be ∼105.
15.3.4.4
Metal–Insulator–ITO Structures
A variation of the MIM scheme includes replacing the metal BE with ITO. An
example of metal–insulator–ITO, including the use ofAl as the TE, utilized the insu-
lating properties of poly[2-methoxy-5-(2′-ethyl-hexyloxy)-1,4-phenylene vinylene]
(MEH-PPV) for multilevel nonvolatile conductance switching of an OBD [68]. This
OBD, with conducting states differing as much as 105, was reported as exhibiting po-
lar switching behavior with short periods of negative differential resistance (NDR),

306
R. A. Nawrocki et al.
in which the current decreases with increasing voltage, during switching of the states.
The device switching was attributed to electron tunneling and charge trapping in the
organic layer.
15.3.4.5
Conductive Polymers as Active Layer Materials
Poly(3,4-ethylenedioxythiophene):poly(4-styrenesulfonate)
(PEDOT:PSS)
is
a
highly conductive polymer that is ubiquitous in polymer electronics applications.
PEDOT:PSS, often doped with sorbitol to decrease its impedance [69], is commonly
used because of its high conductivity, ease of processing, and long-term stability. It
has been demonstrated as an effective material to form device electrodes deposited
by inkjet printing [70].
In an undoped form PEDOT can also be highly resistive and can thus be utilized
essentially as an insulating layer between two electrodes. A 60 nm-thick layer of
PEDOT:PSS was deposited as an AL on top of ITO. The BE electrode was formed
from a 300 nm-thick layer of electron beam (E-beam) evaporated Al [71]. The bipo-
lar switching of this OBD, with ON and OFF voltages being 0.67 and −1.67 V,
respectively, was attributed to the redox behavior of PEDOT chains. In a similar
conﬁguration, with the BE comprised of Cr/Au, a 90 nm-thick layer of PEDOT:PSS
was used the AL. An 80 nm-thick layer of gold was used as the top layer [72]. The
polar switching, with ratios of ON and OFF states as much as 103, was explained
via formation of conductive paths in PEDOT:PSS.
15.3.4.6
Oxide–Organic Bilayers
A device geometry based on a bilayer of a metal oxide dielectric and an organic semi-
conductor was demonstrated to exhibit nonvolatile, resistive switching behavior [73,
74]. A combination of Al2O3 and spiroﬂuorene polymer was sandwiched between a
bottom electrode of either Al or ITO and a top electrode of either Ba/Al or Pd. The
switching was explained by trapping of electrons at the oxide-polymer interface. At
sufﬁcient charge density at that interface the device is “formed”, and the electric ﬁeld
in the dielectric exceeds the soft breakdown limit of the oxide material leading to
increased current. After forming, the device exhibited NDR with reversible switch-
ing between the ON and OFF states. At the maximum current, corresponding to the
onset of NDR, the device switched ON, and at the minimum current, corresponding
to end of NDR, the device switched OFF.
15.3.4.7
Ferroelectric Polymers and Blends as Active Layer Materials
Two widely used ferroelectric polymers are polyvinylidene ﬂuoride(PVDF) and
poly(vinylideneﬂuoridetriﬂuoroethylene) (P(VDF-TrFE)). Because PVDF requires
additional steps to induce the ferroelectricity, and P(VDF-TrFE) is natively ferro-
electric, the latter is more commonly used than the former. As mentioned above,

15
Polymer and Nanoparticle-Composite Bistable Devices . . .
307
these can be blended with a semiconducting polymer to induce the OBD behavior.
For instance, P3HT and poly(9,9-dioctylﬂuorenyl-2,7-diyl) (PFO) have been used
as the semiconducting component [64, 75]. In these approaches, blending of the two
types of polymers generates distinct ferroelectric and semiconducting regions in the
ﬁlm that allow for independent tuning of the conductive and ferroelectric properties
of the composite ﬁlm.
15.4
OBD Application
15.4.1
Memory Elements
Three types of memories can be differentiated as currently existing on the market.
Dynamic Random Access Memory (DRAM) is fast, but it is expensive and it con-
sumes energy during refreshing cycles. Flash Memory (FM) is nonvolatile, but it is
slow to write and even slower to rewrite, and it has a relatively low cycle endurance.
Hard-Disk Drives (HDD) have enormous data densities and cycle endurance, but
have slow access times, consume large amount of power, and they are prone to head
crashes.
A holy grail for the memory industry is a device that has all of the beneﬁts of
DRAM, FM, and HDD but none of the drawbacks. OBDs are envisioned as such
a solution. They are two terminal devices that can be arranged into crossbar archi-
tecture that have the potential for tremendous data density storage. They have been
demonstrated to be integrated with CMOS technology, thus expanding their range
of possible applications. Even in the current, research state, they have been shown
to possess ultra-fast switching speed (∼1 ns) and long retention times with a high
number of write–read–erase–read cycles [66]. They extend the range of possible
applications, compared to current memories, by potentially being optically trans-
parent and physically ﬂexible. Additionally, because of the solution processing, the
production costs are anticipated to be signiﬁcantly reduced compared to inorganic
memories.
15.4.2
Logic Operations
In 1912, in their Principia Mathematica, Whitehead and Russel [76] described four
fundamental logic operations:AND, OR, NOT, and ‘p implies q’or ‘if p, then q’. The
ﬁrst three of those, form the basis of Boolean logic operations, and are at the core
of today’s digital electronics. The last operation, often termed implication logic, is
used by logicians and mathematicians alike. Implication logic is the cornerstone of
reasoning in the current AI. It is implemented with a combination of Boolean logic
operation.

308
R. A. Nawrocki et al.
Within a year of the announcement of demonstration of a memristor, it was shown
that a combination of memristors could be used to compute the fundamental relation-
ship between p and q or their material implication [77]. The logical operation pIMPq,
being equivalent to (NOTp)ORq, was analytically veriﬁed as being computed by two
memristors. As implication logic can be obtained from Boolean logic, the reverse
is also possible. It was later demonstrated that only two memristors are needed to
compute all Boolean functions [78]. The use of OBDs to compute implication logic,
would allow tangible, conventional Artiﬁcial Intelligence (AI) for applications such
as robotics and consumer products with embedded AI.
15.4.3
The Artiﬁcial Synapse
The term neuromorphic was coined by Carver Mead [79] to describe systems that
mimic neuro-biological architectures. Neuromorphic engineering is a concept of
biologically inspired physics, mathematics, computer science and engineering that
aims to develop neural systems with applications such as computer vision, machine
learning, and autonomous robots. Living organisms process information is a starkly
different fashion that human-made computers. At the core of a biological brain is
a biological neuron responsible for processing, transmitting, and storing informa-
tion. A single neuron can receive information from as many as 10,000 neurons,
and it can be connected to as many as a few million other neurons. A human brain
consists of about 50–100 billion neurons and about 1,000 trillion synaptic connec-
tions arranged into various areas each responsible for different brain functions, with
complex relationship between them. This massive parallelism and modular design
leads to spectacular energy efﬁciency, high-speed operation, and robustness against
damage. From an engineering perspective, we can try to mimic the fundamental
qualities of the biological brain for efﬁcient computation. For instance, neuronal
models have been proven to process visual sensor information [80] and for robotic
ﬂight control [81].
A number of researchers have proposed the creation of neuromorphic proces-
sors that exhibit architecture more akin to a biological brain and using the intrinsic
synaptic functionality of a memristor. Likharev [82] proposed the idea of realizing
a nanoscale hybrid semiconductor/nanodevice integrated circuit with neuromorphic
network architecture. His hybrid CMOS/nanowire/molecular-nanodevice (CMOL)
would be created from CMOS op-amps acting as somas and two-terminal, bistable
memristive nanodevices acting as synapses, which are formed at intersections of
nanoscale axonic and dendritic wires. The arrangement of these devices would
produce a device with a behavior similar to that of a neural network.
A different approach was proposed [83] of a multi-layer adaptive, recurrent,
self-organizing network based on memristive devices arranged into a crossbar ar-
chitecture. Learning in this architecture was based on pulse-based communication
between individual layers. The author veriﬁed, via analysis of a theoretical model,
the efﬁcacy of the architecture in performing elementary pattern recognition tasks.

15
Polymer and Nanoparticle-Composite Bistable Devices . . .
309
Motivatedbyrealizationofthelimitationoftoday’scomputingsystems, especially
inproblemsinvolvingtheinteractionwiththerealworld, thereisincreasinginterestin
new models of computation that draws inspiration from computational neuroscience.
In Europe, the FastAnalog Computing with Emerging Transient States (FACETS)
project aims to understand and exploit the foundations of biological nervous systems
towards theoretical and experimental realizations of novel computing paradigms.
In the USA, the Defense Advanced Research Project Agency’s (DARPA) program
Systems of NeuromorphicAdaptive Plastic Scalable Electronics (SyNAPSE) aims to
developelectronicneuromorphicmachinetechnologiesthatscaletobiologicallevels.
The program is currently undertaken by HRL Laboratories, IBM Research, and
Hewlett-Packard (HP). The Modular Neural Exploring Traveling Agent (MoNETA),
a collaboration between HP and Boston University, is software intended to run on
memristor-based hardware, that will process information according to the principles
observed in biological brains [84].
Many implementations of neuromorphic circuits realize the spiking functionali-
ties commonly associated with biological systems. Various models that approximate
the neuronal behavior have been proposed, including the Axon-Hillock model, Inte-
grate & Fire model, Two-compartment Thalamic relay neuron model, or the Tau-cell
neuron model [85]. They all rely on transistors and capacitors to emulate the neural
behavior. Some researchers believe that short-term Spike Timing Dependent Plas-
ticity (STDP) tends to asymptotically approach one of two possible states. Complex
circuits have been proposed that realize this functionality [86]. However, this bistable
nature of a synapse seems to suggest that an OBD could be a suitable replacement
to a complex circuitry.
Our own group has proposed an OBD-based neuromorphic architecture that
aims to mimic the behavior of an artiﬁcial neural network [87]. It consisted of a
single-transistor-single-OBD-per-input architecture capable of performing pattern
recognition tasks. In this design the synaptic functionality is realized by OBDs,
while an OFET performs the summing commonly associated with the soma. The
use of a single OBD results in a binary synapse. However, increasing the number of
OBDs per input results in increasing the smoothness, or granularity, of the artiﬁcial
synapse. This has the effect of decreasing the total number of neurons used for the
same task. The efﬁcacy of this architecture was validated by performing a pattern
recognition task linked with distributed actuation for a future soft robot [88].
15.5
Conclusion
The ﬁeld of organic electronics has no illusions that these materials and devices will
directly compete with establish, inorganic technology for the types of ultra-high-
speed processors that run today’s computers. However, they can complement the
established technologies in areas where their intrinsic properties make them more
suitable, and they can expand their domain to both the very small scale (print-at-
home electronic) and the very large scale (square kilometer fabrication). They can
also draw upon the intrinsic advantages of these materials of rapid, low temperature

310
R. A. Nawrocki et al.
processing, low speciﬁc gravity, physical ﬂexibility, low materials and processing
costs, and the potential for truly large scale manufacturing, to enable facile pathways
to new concepts and applications.
The most basic of these is the goal of utilizing a subset of the OBD structures and
mechanisms described above for low cost, high speed, high-density memory. Beyond
this, the applications described in the previous section suggest a plethora of revolu-
tionary neuromorphic applications. The ability to use two-state devices to compute
implication logic fulﬁlls a key requirement needed to run on hardware what to-
day’sAI does in software. Beyond this, a multitude of neuromorphic approaches can
be envisioned that have the capability of dramatically increased processing power at
greatlyreducedpowerconsumptionbymimickingthefunctionalityofthebrain. Such
architectures would have applications in information processing and classiﬁcation
systems, robotics, and in areas yet to be envisioned.
References
1. Sun SS, Dalton LR (2008) Introduction to organic electronic and optoelectronic materials and
devices. CRC Press, Boca Raton
2. So F (2010) Organic electronics: materials, processing, devices and applications. CRC Press,
Boca Raton
3. Chiang CK, Fincher CR, Jr., ParkYW, HeegerAJ, Shirakawa H, Louis EJ, Gau SC, MacDiarmid
AG (1977) Electrical conductivity in doped polyacetylene. Phys Rev Lett 39(17):1098–1101
4. Walles WE (1972) All-plastic electric capacitor. United States Patent 3689810
5. Ebisawa F, Kurokawa T, Nara S (1983) Electrical properties of polyacetylene/polysiloxane
interface. J Appl Phys 54(6):3255–3259
6. Garnier F, Horowitz G, Peng X, Fichou D (1990) An all-organic “soft” thin ﬁlm transistor with
very high carrier mobility. Adv Mater 2(12):592–594
7. Yoo B, MadgavkarA, Jones BA, Nadkarni S, FacchettiA, Dimmler K, Wasielewski MR, Marks
TJ, Dodabalapur A (2006) Organic complementary D ﬂip-ﬂops enabled by perylene diimides
and pentacene. Electron Dev Lett, IEEE 27(9):737–739
8. McGimpsey WG, Samaniego WN, Chen L, Wang F (1998) Singlet−singlet, triplet−triplet,
and “optically-controlled” energy transfer in polychromophores. Preliminary models for a
molecular scale shift register. J Phys Chem A 102(45):8679–8689
9. Herlogsson L, Coelle M, Tierney S, Crispin X, Berggren M (2010) Low-voltage ring oscillators
based on polyelectrolyte-gated polymer thin-ﬁlm transistors. Adv Mater 22(1):72
10. Myny K, Beenhakkers MJ, van Aerle NAJM, Gelinck GH, Genoe J, Dehaene W, Here-
mans P (2009) A 128b organic RFID transponder chip, including Manchester encoding and
ALOHA anti-collision protocol, operating with a data rate of 1529b/s. In: Solid-state circuits
conference—digest of technical papers, 2009. ISSCC 2009. IEEE International, 8–12 February
2009, pp 206–207
11. David M, Ranasinghe DC, Larsen T (2011)A2U2: a stream cipher for printed electronics RFID
tags. In: RFID (RFID), 2011 IEEE International Conference on, 12–14April 2011, pp 176–183
12. Calamia J (2011) The plastic processor. IEEE Spectrum:2
13. Burroughes JH, Bradley DDC, Brown AR, Marks RN, Mackay K, Friend RH, Burns
PL, Holmes AB (1990) Light-emitting diodes based on conjugated polymers. Nature
347(6293):539–541
14. Shaheen SE, Ginley DS, Jabbour GE (2005) Organic-based photovoltaics: toward low-cost
power generation. MRS Bull 30(1):10–19

15
Polymer and Nanoparticle-Composite Bistable Devices . . .
311
15. Brabec C, Scherf U, Dyakonov V (2008) Organic photovoltaics: materials, device physics, and
manufacturing technologies. Wiley, New York
16. Heremans P, Gelinck GH, Muller R, Baeg K-J, Kim D-Y, NohY-Y (2011) Polymer and organic
nonvolatile memory devices. Chem Mater 23(3):341–358
17. Krebs FC, Gevorgyan SA,Alstrup J (2009)A roll-to-roll process to ﬂexible polymer solar cells:
model studies, manufacture and operational stability studies. J Mater Chem 19(30):5442–5451
18. Liu Y, Cui TH, Varahramyan K (2003) All-polymer capacitor fabricated with inkjet printing
technique. Solid-State Electron 47(9):1543–1548
19. Sekitani T, Noguchi Y, Zschieschang U, Klauk H, Someya T (2008) Organic transistors man-
ufactured using inkjet technology with subfemtoliter accuracy. Proc Natl Acad Sci U S A
105(13):4976–4980
20. Shaheen SE, Radspinner R, Peyghambarian N, Jabbour GE (2001) Fabrication of bulk
heterojunction plastic solar cells by screen printing. Appl Phys Lett 79(18):2996–2998
21. Green R, Morfa A, Ferguson AJ, Kopidakis N, Rumbles G, Shaheen SE (2008) Performance
of bulk heterojunction photovoltaic devices prepared by airbrush spray deposition. Appl Phys
Lett 92(3):033301
22. Lopez MA, Sanchez JC, Estrada M (2008) Characterization of PEDOT:PSS dilutions for inkjet
printing applied to OLED fabrication. In: Devices, circuits and systems, 2008. ICCDCS 2008.
7th International Caribbean conference on, 28–30 April 2008, pp 1–4
23. Padinger F, Brabec CJ, Fromherz T, Hummelen JC, Sariciftci NS (2000) Fabrication large area
photovoltaic devices containing. Opto-Electron Rev 8(4):4
24. Bailey BA, Reese MO, Olson DC, Shaheen SE, Kopidakis N (2011) Air-processed organic
photovoltaic devices fabricated with hot press lamination. Org Electron 12(1):108–112
25. Hu X, Krull P, de Graff B, Dowling K, Rogers JA, Arora WJ (2011) Stretchable inorganic-
semiconductor electronic systems. Adv Mater 23(26):2933–2936
26. Zhang J, Li CM, Chan-Park MB, Zhou Q, GanY, Qin F, Ong B, Chen T (2007) Fabrication of
thin-ﬁlm organic transistor on ﬂexible substrate via ultraviolet transfer embossing. Appl Phys
Lett 90(24):243502
27. Kim BC, Too CO, Kwon JS, Bo JM, Wallace GG (2011) A ﬂexible capacitor based on
conducting polymer electrodes. Synth Metals 161(11–12):1130–1132
28. Kushto GP, Kim WH, KafaﬁZH (2005) Flexible organic photovoltaics using conducting
polymer electrodes. Appl Phys Lett 86(9):093502
29. Zhang D, Ryu K, Liu X, Polikarpov E, Ly J, Tompson ME, Zhou C (2006) Transparent,
conductive, and ﬂexible carbon nanotube ﬁlms and their application in organic light-emitting
diodes. Nano Lett 6(9):1880–1886
30. Son DI, Kim TW, Shim JH, Jung JH, Lee DU, Lee JM, Il Park W, Choi WK (2010) Flexible or-
ganic bistable devices based on graphene embedded in an insulating poly(methyl methacrylate)
polymer layer. Nano Lett 10(7):2441–2447
31. Ro HW, Popova V, Chen L, Forster AM, Ding Y, Alvine KJ, Krug DJ, Laine RM, Soles CL
(2011) Cubic silsesquioxanes as a green, high-performance mold material for nanoimprint
lithography. Adv Mater 23(3):414–420
32. AlmeidaVR, Lipson M (2004) Optical bistability on a silicon chip. Opt Lett 29(20):2387–2389
33. Salinga M, Wuttig M (2011) Phase-change memories on a diet. Science 332(6029):543–544
34. Potember RS, Poehler TO, Cowan DO (1979) Electrical switching and memory phenomena in
Cu-TCNQ thin ﬁlms. Appl Phys Lett 34(6):405–407
35. Ma LP, Liu J, Yang Y (2002) Organic electrical bistable devices and rewritable memory cells.
Appl Phys Lett 80(16):2997–2999
36. Son DI, Shim JH, Park DH, Jung JH, Lee JM, Park WI, Kim TW, Choi WK (2011) Polymer-
ultrathin graphite sheet-polymer composite structured ﬂexible nonvolatile bistable organic
memory devices. Nanotechnology 22(29):295203
37. Mamo MA, MachadoWS, van OtterloWAL, Coville NJ, Huemmelgen IA (2010) Simple write-
once-read-many-times memory device based on a carbon sphere-poly(vinylphenol) composite.
Org Electron 11(11):1858–1863

312
R. A. Nawrocki et al.
38. Wang J, Cheng X, Caironi M, Gao F,Yang X, Greenham NC (2011) Entirely solution-processed
write-once-read-many-times memory devices and their operation mechanism. Org Electron
12(7):1271–1274
39. Son DI,You CH, KimWT, Jung JH, KimTW (2009) Electrical bistabilities and memory mecha-
nisms of organic bistable devices based on colloidal ZnO quantum dot-polymethylmethacrylate
polymer nanocomposites. Appl Phys Lett 94(13):132103-1–132103-3
40. Chua L (1971) Memristor: the missing circuit element. IEEE Trans Circ Theor 18(5):507–519
41. Strukov DB, Snider GS, Stewart DR,Williams RS (2008)The missing memristor found. Nature
453(7191):80–83
42. Berzina T, Erokhina S, Camorani P, Konovalov O, Erokhin V, Fontana MP (2009) Elec-
trochemical control of the conductivity in an organic memristor: a time-resolved X-ray
ﬂuorescence study of ionic drift as a function of the applied voltage.ACSAppl Mater Interfaces
1(10):2115–2118
43. Islam MS, Johns C, Long D, Ohlberg DAA, Shih-Yuan W, Williams RS (2010) Memristors
based on an organic monolayer of molecules and a thin ﬁlm of solid electrolytes. In: Electrical
and computer engineering (ICECE), 2010 international conference on, 18–20 December 2010,
pp 761–764
44. Velu G, Legrand C, Tharaud O, Chapoton A, Remiens D, Horowitz G (2001) Low driving
voltages and memory effect in organic thin-ﬁlm transistors with a ferroelectric gate insulator.
Appl Phys Lett 79(5):659–661
45. Kim C-J, Choi S-J, Kim S, Han J-W, Kim H, Yoo S, Choi Y-K (2011) Photoinduced memory
with hybrid integration of an organic fullerene derivative and an inorganic nanogap-embedded
ﬁeld-effect transistor for low-voltage operation. Adv Mater 23(29):3326–3331
46. Das S, Appenzeller J (2011) FETRAM. An organic ferroelectric material based novel random
access memory cell. Nano Lett 11(9):4003–4007
47. Ma H, Yip H-L, Huang F, Jen AKY (2010) Interface engineering for organic electronics. Adv
Funct Mater 20(9):1371–1388
48. Lee J, Hong WG, Lee H (2011) Non-volatile organic memory effect with thickness control of
the insulating LiF charge trap layer. Org Electron 12(6):988–992
49. Lin CW, Wang DY, Tai Y, Jiang YT, Chen MC, Chen CC, Yang YJ, Chen YF (2011) Type-II
heterojunction organic/inorganic hybrid non-volatile memory based on FeS(2) nanocrystals
embedded in poly(3-hexylthiophene). J Phys D: Appl Phys 44(29):292002
50. SeoD,ParkJC,SongH(2006)PolyhedralgoldnanocrystalswithOhsymmetry: fromoctahedra
to cubes. J Am Chem Soc 128(46):14863–14870
51. Kourgiantakis M, Matzapetakis M, Raptopoulou CP, TerzisA, SalifoglouA (2000) Lead-citrate
chemistry. Synthesis, spectroscopic and structural studies of a novel lead(II)-citrate aqueous
complex. Inorg Chim Acta 297(1–2):134–138
52. Nagarajan RH, T.A. (2008) Nanoparticles: synthesis, stabilization, passivation, and function-
alization, vol 996. American Chemical Society
53. Krebs FC (2009) Polymer solar cell modules prepared using roll-to-roll methods: knife-over-
edge coating, slot-die coating and screen printing. Solar Energy Mater Solar Cells 93(4):465–
475
54. Houili H, Tutis E, Izquierdo R (2010) Modeling nanoparticle embedded organic memory
devices. Org Electron 11(4):514–520
55. Ma LP, Xu QF,YangY (2004) Organic nonvolatile memory by controlling the dynamic copper-
ion concentration within organic layer. Appl Phys Lett 84(24):4908–4910
56. Dearnaley G, Morgan DV, Stoneham AM (1970) A model for ﬁlament growth and switching
in amorphous oxide ﬁlms. J Non-Cryst Solids 4(0):593–612
57. Heonjun H, Ohyun K (2010) Electrode-material-dependent switching characteristics
of organic nonvolatile memory devices based on poly(3,4-ethylenedioxythiophene):
poly(styrenesulfonate) ﬁlm. Electron Dev Lett IEEE 31(4):368–370
58. Blom PWM, Smit C, Haverkort JEM, Wolter JH (1993) Carrier capture into a semiconductor
quantum well. Phys Rev B 47(4):2072–2081

15
Polymer and Nanoparticle-Composite Bistable Devices . . .
313
59. Coehoorn R, Pasveer WF, Bobbert PA, Michels MAJ (2005) Charge-carrier concentration
dependence of the hopping mobility in organic materials with Gaussian disorder. Phys Rev B
72(15):155206
60. Gregg BA, Chen SG, Branz HM (2004) On the superlinear increase in conductivity with dopant
concentration in excitonic semiconductors. Appl Phys Lett 84(10):1707–1709
61. Lampert MA, Mark P (1970) Current injection in solids. Academic Press, New York
62. Lin H-T, Pei Z, Chan Y-J (2007) Carrier transport mechanism in a nanoparticle-incorporated
organic bistable memory device. IEEE Electron Dev Lett 28(7):569–571
63. Jung JH, Kim TW (2011) The effect of the trap density and depth on the current bistability in
organic bistable devices. J Appl Phys 110(4):043721
64. Naber RCG, Asadi K, Blom PWM, de Leeuw DM, de Boer B (2010) Organic nonvolatile
memory devices based on ferroelectricity. Adv Mater 22(9):933–945
65. Kim J-M, Lee D-H, Jeun J-H, Yoon T-S, Lee HH, Lee J-W, Kim Y-S (2011) Non-volatile
organic memory based on CdSe nano-particle/PMMA blend as a tunneling layer. Synth Metals
161(13–14):1155–1158
66. Kuang Y, Huang R, Tang Y, Ding W, Zhang L, Wang Y (2010) Flexible single-component-
polymer resistive memory for ultrafast and highly compatible nonvolatile memory applications.
IEEE Electron Dev Lett 31(7):758–760
67. Kim T-W, Oh S-H, Choi H, Wang G, Hwang H, Kim D-Y, Lee T (2008) Reversible switching
characteristics of polyﬂuorene-derivative single layer ﬁlm for nonvolatile memory devices.
Appl Phys Lett 92(25):3
68. Lauters M, McCarthy B, Sarid D, Jabbour GE (2006) Multilevel conductance switching in
polymer ﬁlms. Appl Phys Lett 89(1):013507
69. Nardes AM, Kemerink M, de Kok MM, Vinken E, Maturova K, Janssen RAJ (2008) Con-
ductivity, work function, and environmental stability of PEDOT:PSS thin ﬁlms treated with
sorbitol. Org Electron 9(5):727–734
70. Lim JA, Cho JH, Park YD, Kim DH, Hwang M, Cho K (2006) Solvent effect of inkjet printed
source/drain electrodes on electrical properties of polymer thin-ﬁlm transistors. Appl Phys Lett
88(8):082102
71. Ha H, Kim O (2008) Bipolar switching characteristics of nonvolatile memory devices
based on poly(3,4-ethylenedioxythiophene):poly(styrenesulfonate) thin ﬁlm. Appl Phys Lett
93(3):033309
72. Liu X, Ji Z, Tu D, Shang L, Liu J, Liu M, Xie C (2009) Organic nonpolar nonvolatile resistive
switching in poly(3,4-ethylene-dioxythiophene): polystyrenesulfonate thin ﬁlm. Org Electron
10(6):1191–1194
73. Verbakel F, Meskers SCJ, Janssen RAJ, Gomes HL, Coelle M, Buechel M, de Leeuw DM
(2007) Reproducible resistive switching in nonvolatile organic memories. Appl Phys Lett
91(19):192103
74. Bory BF, Meskers SCJ, Janssen RAJ, Gomes HL, de Leeuw3 DM (2010) Trapping of electrons
in metal oxide-polymer memory diodes in the initial stage of electroforming. Appl Phys Lett
97(22):3
75. Asadi K, Li MY, Stingelin N, Blom PWM, de Leeuw DM (2010) Crossbar memory array of
organic bistable rectifying diodes for nonvolatile data storage. Appl Phys Lett 97(19):193308
76. Whitehead AN, Russell B (1912) Principia mathematica, vol 2. University Press, Cambridge
77. Borghetti J, Snider GS, Kuekes PJ, Yang JJ, Stewart DR, Williams RS (2010) ‘Memristive’
switches enable ‘stateful’logic operations via material implication. Nature 464(7290):873–876
78. Lehtonen E, Poikonen JH, Laiho M (2010) Two memristors sufﬁce to compute all Boolean
functions. Electron Lett 46(3):230–230
79. Mead C (1990) Neuromorphic electronic system. Proc IEEE 78(10):1629–1636
80. Conradt J, Cook M, Berner R, Lichtsteiner P, Douglas RJ, DelbruckT (2009)A pencil balancing
robot using a pair ofAER dynamic vision sensors. In: Circuits and Systems, 2009. ISCAS 2009.
IEEE International Symposium on, 2009, pp 781–784
81. i Badia SB, Pyk P, Verschure PFMJ (2005) A biologically inspired ﬂight control system for
a blimp-based UAV. In: International Conference on Robotics and Automation (ICRA05),
Barcelona, Spain, 2005

314
R. A. Nawrocki et al.
82. Likharev K, Mayr A, Muckra I, Turel O (2003) CrossNets—high-performance neuromorphic
architectures for CMOL circuits. In: Reimers JRPCAEJCSR (ed) Molecular electronics Iii, vol
1006. Annals of the New York Academy of Sciences, pp 146–163
83. Snider GS (2007) Self-organized computation with unreliable, memristive nanodevices.
Nanotechnology 18(36):365202
84. GorchetchnikovA, Leveille J,Versace M,Ames H, Livitz G, Chandler B, Mingolla E, Carter D,
Amerson R, Abdalla H, Qureshi S, Snider G (2011) MoNETA: massive parallel application of
biological models navigating through virtual Morris water maze and beyond. BMC Neurosci
12(Suppl 1):P310
85. Indiveri G, Linares-Barranco B, Hamilton TJ, van Schaik A, Etienne-Cummings R, Delbruck
T, Liu S-C, Dudek P, Haﬂiger P, Renaud S, Schemmel J, Cauwenberghs G, Arthur J, Hynna
K, Folowosele F, Saighi S, Serrano-Gotarredona T, Wijekoon J, Wang Y, Boahen K (2011)
Neuromorphic silicon neuron circuits. Front Neurosci 5:73
86. Indiveri G, Chicca E, Douglas R(2006)AVLSIarray of low-power spiking neurons and bistable
synapses with spike-timing dependent plasticity. IEEE Trans Neural Networks 17(1):211–221
87. Nawrocki RA, Shaheen SE, Voyles RM (2011) A neuromorphic architecture from single tran-
sistor neurons with organic bistable devices for weights. In: Neural Networks (IJCNN), The
2011 International Joint Conference on, July 31–August 5 2011, pp 450–456
88. Nawrocki RA, XiaotingY, Shaheen SE, Voyles RM (2011) Structured computational polymers
for a soft robot: actuation and cognition. In: Robotics and automation (ICRA), 2011 IEEE
international conference on, 9–13 May 2011, pp 5115–5122

Index
2 × 2 Equivalent circuit model, 198
2-PCM synapse, 173
A
Action potential, 27, 43, 103, 168
Adaptive circuits, 16
Adaptive dynamic programming (ADP), 71, 72
Adaptive ﬁlters, 23
Adaptive resonance theory (ART), 12, 133, 136
Admittance matrix, 196
AER, 42, 43, 173
Air Force Research Lab (AFRL), 222, 270
Aircraft, 66, 71
Algebra, 10, 49, 140
Algorithm, 30, 31, 109, 119, 134, 138, 245
Amplitude modulator, 191
Amplitude-angular frequency ratio, 253–256,
258, 260, 261
Analog, 39, 40, 186, 309
Analog capability, 21
Analog computing, 77
Analog parallelism, 30
Architecture, 30, 42, 90, 96–98, 100, 102, 134,
155, 298
Artiﬁcial silicon retina, 173
Artiﬁcial synapse, 95, 308, 309
Associative memory, 15, 16, 18, 24–26, 247
Attention, 13, 14, 52, 70, 134, 284
Atypical bow-tie, 255, 256, 260–263
Autocorrelation function (ACF), 118
Autonomous behavior, 11
Autonomous circuit, 79, 82
Autonomy, 82
Axons, 41, 271, 272
B
Backpropagation, 64, 66, 67, 70
Band gap, 163
Behavior, 19, 21, 22, 37, 49, 85, 272, 294, 295,
298, 302
Binocular, 14
Biolek window function, 216–218, 228
Biological neuron, 26, 41, 43, 103, 308
Biologically inspired electronics, 15, 42, 308
Bipole, 303
Bistable memristive device, 192
Boolean logic, 81, 204, 246, 307, 308
Boundary condition-based model, 184, 248
Brain, 10, 15, 24–27, 30, 44, 64, 65, 78, 102,
136, 155, 278, 283, 308
Brain functionality, 84
Brain-like, 9, 11, 49, 64
C
CBRAM, 156, 174
Cell, 25, 70, 78, 79, 135, 138, 142, 146, 169,
279
Cellular, 134, 270
Cerebral cortex, 70, 155, 165, 271
Chain rule, 66, 251, 253
Chalcogenide, 12, 13, 80, 81, 157–162,
164–166, 169, 234, 235
Chess, 30, 71
Chua, Leon, 9, 68, 79, 89, 90, 108, 136, 211
Circuit, 22, 79, 96, 127, 129, 186, 206, 292
Classiﬁcation, 12, 52, 140, 145, 148
Clockwise loop rotation, 255, 256
Clockwise loop rotation, see also Loop rotation,
255
Cluster, 44, 45, 66, 77, 78, 273
CMOL, 30, 308
CMOS, 12, 13, 30, 38, 42, 43, 46, 78, 79, 85,
89–92, 95, 98, 126, 134, 136, 155, 157, 171,
175, 182, 191, 192, 202, 205, 206, 208, 307,
308
CMOS/Molecular hybrid (CMOL), 30, 202
R. Kozma et al. (eds.), Advances in Neuromorphic Memristor Science and Applications,
315
Springer Series in Cognitive and Neural Systems,
DOI 10.1007/978-94-007-4491-2, © Springer Science+Business Media Dordrecht 2012

316
Index
Coexistence of memristive and memcapacitive
responses, 11, 20
Cog, 11, 38, 45–49, 68
Cog Ex Machina, 38, 45, 68
Cognitive, 42, 166
Cognitive optimization and prediction, 11, 64,
65
Columnar organization, 14
Columnar V1 model, 14
Communication, 39, 43, 45, 47, 208, 271, 273
Communication energy, 11, 38
Computational complexity, 12, 134, 135, 142,
144, 145, 149, 302
Computational intelligence, 77
Computer vision, 68, 308
Computing Architecture, 83
Conductive polymer, 306
Confabulation, 14, 274, 278
Constitutive relationship between charge and
ﬂux-linkage, 90, 181, 182, 184
Convolution, 42, 49
Convolution network, 49
COPN, 65, 66, 68–72
Cortex, 270, 271
Counter-clockwise loop rotation, 260–262
CPU, 11, 43–45, 246
Critical ﬂux, 252–256, 258–261
Cross association matrix, 196
Crosstalk matrix, 196
Crystallization, 159
Crystallization temperature, 159, 161
Current-controlled memristor, 183, 185, 248,
250
D
Data-pattern dependent detection margin, 194
Data-retention, 12, 89, 157, 159, 165
Decision making, 11, 14, 39, 63, 65, 246
Dependable memory, 207
Depressing-pulse, 168
Diagnostics, 67
Differential equations, 10, 115, 138, 140, 142,
144
Diffusion, 90, 91, 94, 99, 230, 232, 233
Digital, 10, 13, 21, 26, 39–41, 43, 49, 67, 95,
96, 102, 134, 138, 140
Digital computing, 10, 39, 43–45, 49, 78, 80
Disparity, 14, 52, 274, 275, 282, 284–286
Doping, 98, 108–111, 113–116, 123, 159, 164
Dynamic programming, 64
E
Efﬁciency, 25, 67, 102, 135, 173, 287, 308
Electric power grid, 66
Electron beam lithography (EBL), 109, 118
Electron microscope (SEM), 109
Electroosmosis, 23, 24
Embodiment, 11, 13
Endurance, 13, 156, 157, 159, 165, 307
Energy, 13, 38, 39, 44, 89, 103, 129, 140, 157,
158, 161, 169, 172, 173, 188, 208, 234
Euler’s method, 139, 144, 146
Eye, 270, 271, 275
F
FACETS, 43, 309
Feature extraction algorithms, 247
Feed forward, 138, 173, 271, 279
Feedback, 47, 48, 138, 146, 271, 282, 284, 285
Feedback circuit, 100
Ferroelectric, 303, 306, 307
Field Programmable Nanowire Interconnect
(FPNI), 202
Flash memory, 63, 307
Flexibility, 45, 48, 134, 207
Floating gate memory, 43
Fourier, 49, 250
Functional columns, 14, 271, 274, 275, 278,
280, 284, 285
Fuzziﬁcation, 142
Fuzzy inference, 12, 134, 140, 149
Fuzzy inference systems (FIS), 134, 135,
139–145, 149
Fuzzy logic, 15, 136, 140
Fuzzy operators, 134, 141, 142
G
Gated decay, 48, 134, 142
General Hyperbolic Sine Model, 227
Generalized memristor model, 13, 236, 249
Geometry variations, 12, 108, 109, 115, 118,
120
Giant magneto-resistance (GMR), 111
GPU, 11, 44, 45
Growth, 25, 159
GST, 158, 159, 163, 166
Gurobi, 71
H
Habituation, 10
Hardware correlated model, 221, 222
Heater plug, 160, 168
Hebbian learning, 69, 134, 135, 138, 247
Heterogeneous computing, 12, 90
Hewlett-Packard (HP), 45, 91, 309
High performance computing, 14, 66, 77, 78,
270

Index
317
Highly-parallel processing power, 246
Homeostasis, 10, 70
Hughes Research Laboratory (HRL), 309
Hybrid, 12, 43, 45, 79, 134, 182, 191, 202, 204,
205, 208, 294, 308
Hyper-parallel, 14
Hyperbolic sinusoid, 213, 225, 227, 234
Hysteresis, 19, 80, 89, 92, 94, 95, 136, 214,
218, 221, 230, 232, 236
I
I–V characteristics, 121, 212
IBM, 44, 112, 157, 279, 309
IJCNN 2011, 68
Image, 52, 78, 84, 146, 174, 270, 275, 280,
281, 284, 286, 300
Image processing, 67, 68
Imec, 292
Implication logic, 307, 308
Inﬂuence of boundary conditions, 13, 248, 250,
263, 264
Information, 16, 21, 27, 30, 41, 42, 46, 47, 49,
52, 78–80, 84, 136, 157, 166, 168, 181, 212,
246, 247, 271, 272, 274, 275, 284, 286, 308
Infrastructure management, 11, 71
Initial conditions, 143, 186, 248, 250, 251, 256
Instar, 48, 138, 139, 142–146, 148
Integrate-and-ﬁre neurons, 14, 270
Integrate-Fire, 166, 272, 273, 309
Intelligence, 10, 38, 39, 46, 308
Intelligent systems, 14
Interconnect, 205, 208, 246
Interior point, 71
J
Joglekar window function, 215, 217, 218
Joule heating, 23, 160
L
Lance-type, 161, 165
Lateral inhibition, 173
Leaky-Integrate-Fire, 166
Learning, 16, 18, 21, 27, 47, 64, 66, 103, 134,
138, 140, 144, 173, 308
Learning circuits, 21, 23
Learning rate, 139, 143
Likharev, K., 308
Line edge roughness (LER), 12, 109, 118
Line width roughness (LWR), 119
Linear dopant drift assumption, 249
Linear model, 95
Linear programming, 71
Locality, 49, 52, 53
Logic in memory, 96
Long-term depression (LTD), 27, 103
Long-term memory (LTM), 47
Long-term potentiation (LTP), 103
Long-time potentiation, see Long-term
potentiation (LTP), 27
Loop rotation, 255, 256, 260–263
LTD, see Long-term depression, 163
LTP, see Long-term potentiation, 163
M
Magnetic tunneling junction (MTJ), 111
Magnocellular, 271, 274, 275, 282
Massive parallelism, 30, 45, 64, 66, 68, 71, 308
Massively parallel, 30, 38, 43, 45, 46, 64, 66,
68, 69, 71, 134–136
Maze, 11, 30–32, 50, 52, 53
Membership functions, 141–143, 145
Memcapacitance, 17, 19
Memcapacitive systems, 11, 16, 17, 20
Memcapacitor, 11
Memductance transitions, 254, 255, 259–261
Memelements, 16, 21, 26
Meminductance, 17, 19
Meminductive systems, 11, 16, 17
Meminductor, 11
Memory, 12, 14, 90, 194, 201, 246, 274, 307
Memristance, 17–19, 30, 90, 92, 93, 95, 102,
110, 111, 114, 116, 117, 123, 129, 183
Memristive, 11, 12, 16, 17, 19, 41, 207
Memristive circuit, 17, 182
Memristive devices, 10–13, 20, 23, 26, 30, 90,
95, 97, 100, 102, 103, 110, 136, 181, 182,
201–203, 206, 308
Memristive hardware, 12, 134, 135, 141, 151
Memristive processor, 30, 31
Memristive systems, 12, 17–19, 27, 28, 90, 91,
93, 108, 248
Memristor, 9, 18, 28, 30, 32, 90, 92, 95, 96,
136, 182, 185, 187, 191, 213, 215, 216, 234,
236
Memristor circuits, 98
Memristor models, 13, 92, 212, 213, 221, 225,
234, 236, 249, 250
Memristor Oscillatory Neurocomputers
(MONs), 13, 246, 247
Memristor-based synapse, 109, 124, 126, 127,
308
Memristor-based synaptic interactions, 247
Metal-insulator-metal, 32, 92, 212, 222, 302,
305
MIM model, 222
Minicolumns, 271, 274–276, 278–280, 283,
284

318
Index
Modeling, 12, 49, 80, 163, 215, 228, 271
Modeling methodology, 79, 109
Modular Neural Exploring Traveling Agent
(MoNETA), 309
Monte-Carlo simulations, 12, 108, 109, 119,
120, 123, 124
Moore’s Law, 13, 67, 68, 136, 182, 202, 208
Motivation, 11, 39, 50, 53, 156
Multi-level, 103, 156, 163
Multi-nodal equations, 197
Multicore, 68, 90
Multiple-path maze, 31
N
Nanodevices, 10, 41, 96, 182, 207, 308
Nanoparticles, 299, 300, 304
Nanotechnology, 11, 136
Navigation, 11, 39, 51, 53
Neocortical, 14
Network, 30, 43, 134, 135, 138, 140, 144, 173,
245, 273
Networks of memory circuit elements, 11, 16,
30
Neuromorphic, 10–14, 38–42, 45, 77, 78, 308,
309
Neuromorphic circuits, 24, 26–28, 155
Neuromorphic computing, 28, 78, 79, 82, 84,
86, 109, 246, 270
Neuromorphic hardware, 40–43, 45, 136, 155
Neuron, 16, 25–27, 30, 42, 43, 70, 78, 84, 102,
103, 128, 136, 155, 166–169, 173, 246, 247,
272, 274, 285, 308
Neuron model, 272, 274, 309
Neurophysiological, 13, 14, 50
Neuroscience, 64, 246, 270–272, 309
Nonlinear, 47, 94, 136, 138, 144, 145, 148,
181, 208, 246, 247, 249
Nonlinear dynamics, 13, 41, 47, 247
Nonvolatile memory, 9, 13, 79, 80, 95, 100,
182, 192, 207
Nucleation, 159, 166
O
Odd-symmetric charge-ﬂux characteristic, 245,
252, 263
Optimization, 11, 64, 71, 159, 287
Organic Bistable Device (OBDs), 14, 292,
294–296, 298, 302-305, 307–309
Organic electronics, 291, 293, 299, 301
Organic Field Effect Transistor (OFET), 298,
309
Organic memory, 292, 294
Organic memristor, 298
Outstar, 48, 138, 139, 142–146, 148
Ovonic-cognitive devices, 165
OXRAM, 156
P
Parallel computing, 64, 66, 67, 69, 77, 269
Parallel recognition, 68, 78, 309
Parvocellular, 271, 275
Passive RRAM array, 194
Pattern extraction, 173
Pattern recognition, 13, 247, 250
Pavlov’s experiment, 25
PCM, 12, 13, 156
PDE, 67
Perception, 11, 12, 14, 39, 269, 276, 278, 283,
284
Phase-change processor, 166
Photonic, 38
Physarum polycephalum, 21–23
Physical nano-scale double-layer systems, 249
Piecewise linear approximation, 70, 252
Pinched hysteresis, 10, 89
Pinched hysteresis loop, 19, 108, 213
Pipelined digital system, 202, 208
Pixel, 52, 146, 148, 173, 280, 287
Polarity coefﬁcient, 249, 264
Poly(3,4-ethylenedioxythiophene): poly(4-
styrenesulfonate) (PEDOT:PSS), 301, 302,
306
Poly(3-hexylthiophene) (P3HT), 304
Poly(methylmethacrylate) (PMMA), 301
Polymer electronics, 292, 293, 306
Polyvinylpyrrolidone (PVP), 300
Potentiating-pulse, 168
Power, 16, 30, 46, 90, 129, 134–136, 142, 157,
165, 171, 174, 246, 247, 273
Power spectral density (PSD), 118
Primary visual cortex, 14, 270, 271
Process variations (PV), 100, 108, 109, 113,
115, 117, 120, 121
Processor, 30–32, 77, 136, 166, 208, 292
Programmable attenuator, 191
Programmable ﬁlter, 191
Programmable gain ampliﬁer, 191
Programmable resistor, 160, 192
Psychological, 14, 50
Pulse coded memristance, 188
Q
q-ϕ relationship, 110, 250, 251
R
Reconﬁgurable, 98, 100, 124, 204
Reconﬁgurable resistive logic, 208

Index
319
Recurrent competitive ﬁeld (RCF), 51, 135,
138
Recurrent network, 14, 135, 144, 278
Reﬂectivity, 158, 166
Reichardt detectors, 14
Reinforcement, 10, 54
Reliability, 71, 100, 206, 207
ReRAM, 13, 89, 95, 100, 136, 207
ReRAM, see also Nonvolatile memory, 95
Reset-pulse, 161, 162, 164
Resistance switching memory cells, 19
Resistance window, 159
Resistance-drift, 163, 173
Resistive random access memory (RRAM), 98,
192
Resistivity, 158, 222
Retention time, 100, 296, 307
Robotics, 14, 53, 308
Roll-to-roll printing, 294
RRAM, see Resistive Random Access Memory,
192
S
Scaling, 90, 100, 164, 165, 175, 208
Schottky barrier, 91, 230, 233
Selector element, 160
Self-adaptable of sense resistance, 199
Self-reconﬁgurable, 12, 79
Self-reconﬁgurable circuit, 79
Self-training, 129
Sensed output voltage matrix, 196
Sensing, 100, 127, 269
Set-pulse, 161
Short-term memory (STM), 47, 138
Sigmoid, 139
Simplex, 71
Smart electronic circuits, 11
Sneak currents, 194, 196, 200, 201
Sneak paths, 96, 97, 100, 102
Solution processing, 293, 294, 301, 307
SPICE, 13, 95, 212–217, 221, 225, 228, 230,
231, 236
Spike timing-dependent plasticity (STDP), 13,
24, 27, 28, 41, 69, 70, 79, 171, 247, 272
Spiking neurons, 43, 44, 274
Spintronic memristor, 12, 109, 112, 113, 116,
117, 120, 123
State variable, 18, 19, 24, 71, 182, 203, 213,
215, 217, 221, 222, 227, 247
State variable boundaries, 235
State variable motion, 230, 235, 236
Stateful logic, 201–203
Stateful NAND gate, 203
Stateful NOR gate, 206
Statistical approximation, 197, 198
Statistical memory model, 194
Statistical modeling, 118
STDP, see Spike timing-dependent plasticity
(STDP), 309
Stereomotion, 14
Storage capacity, 12, 135
Streaming, 283
Structural relaxations, 163, 301
Subcircuit, 212, 214, 215, 217, 222, 231
Subthreshold, 103, 278
Switching function, 22
Switching mechanism, 161, 215, 299, 302–305
Synapse, 70, 82, 84, 85, 128, 212, 309
Synaptic connectivity, 10, 102, 103
Synaptic depression, 13, 163, 168, 170–172
Synaptic learning, 142
Synaptic plasticity, 12, 102, 136, 140, 156, 174
Synaptic potentiation, 13, 163, 167, 168
T
Takagi-Sugeno, 135, 140, 143, 145
Thickness ﬂuctuation (TF), 12, 109
Thin ﬁlms, 20, 89–93, 95, 103, 108, 115, 120,
292, 293, 298, 300
Threshold switching, 161
Threshold voltage, 80, 81, 86, 234, 295, 297,
298, 302, 304
Threshold-effect, 166
Threshold-type memristive system, 18, 22
Titanium dioxide, 90, 110, 181, 213, 246, 298
Transistors, 28, 78, 79, 84, 86, 102, 126, 155,
171, 208, 292–294, 298, 309
U
Unicellular organisms, 11, 18, 21
University of Michigan Model, 230
V
V2 model, 281, 282, 284, 285
V2 model, 14
Vanadium dioxide, 22, 23
Virtual environment, 11
Vision, 25, 42
Visual cortex (V1), 270
Volatile electronic switch, 163
Voltage-controlled memductor, 183
Voltage-controlled memristive system, 185,
188, 189, 248, 250, 251
Von Neumann, 43, 136, 166, 246, 247

320
Index
W
Weakly-resistive behavior, 257, 259, 262
Whitehead and Russel, 307
Wiring length, 12, 135
Worst-case memory model, 194, 201
Write-energy, 99, 165
Write-Once-Read-Many (WORM), 296
Z
Zinc oxide (ZnO), 299, 300

