Bayesian Probit Regression Models for
Spatially-Dependent Categorical Data
DISSERTATION
Presented in Partial FulÔ¨Ållment of the Requirements for
the Degree Doctor of Philosophy in the
Graduate School of The Ohio State University
By
Candace Berrett, B.S., M.S.
Graduate Program in Statistics
The Ohio State University
2010
Dissertation Committee:
Catherine A. Calder, Advisor
L. Mark Berliner
Peter F. Craigmile
Elizabeth A. Stasny

c‚ÉùCopyright by
Candace Berrett
2010

ABSTRACT
Data augmentation/latent variable methods have been widely recognized for facilitating
model Ô¨Åtting in the Bayesian probit regression model. First proposed by Albert and Chib
(1993) for independent binary and multi-category data, the latent variable representation of
the Bayesian probit regression model allows model Ô¨Åtting to be performed using a simple
Gibbs sampler and, for more than two categories, also allows the so-called assumption of
irrelevant alternatives required by the logistic regression model to be relaxed (Hausman
and Wise, 1978). To accommodate residual spatial dependence, the latent variable speci-
Ô¨Åcation of the Bayesian probit regression model can be extended to incorporate standard
parametric covariance models typically used in analyses of spatially-dependent continuous
data, deÔ¨Åning what we term the Bayesian spatial probit regression model. In this disserta-
tion, we develop and extend the Bayesian spatial probit regression model by (i) introducing
efÔ¨Åcient model-Ô¨Åtting algorithms, (ii) deriving classiÔ¨Åcation methods based on the model,
and (iii) extending the model to the multi-category spatial setting.
Statistical models for spatial data are notoriously cumbersome to Ô¨Åt necessitating the
availability of fast and efÔ¨Åcient model-Ô¨Åtting algorithms. To improve the efÔ¨Åciency of
the Gibbs sampler used to Ô¨Åt the Bayesian regression model for independent categorical
response variables, Imai and van Dyk (2005) propose introducing a working parameter
into the model and compare various data augmentation strategies resulting from different
treatments of the working parameter. We build on this work by investigating the efÔ¨Åciency
ii

of modiÔ¨Åed and extended versions of conditional and marginal data augmentation Markov
chain Monte Carlo (MCMC) algorithms for the spatial probit regression model, focusing
on the special case of binary spatially-dependent response variables.
Within the classiÔ¨Åcation literature, methods that exploit spatial dependence are limited.
We show how a spatial classiÔ¨Åcation rule can be derived from the Bayesian spatial probit
regression model. In addition, we compare our proposed spatial classiÔ¨Åer to various other
classiÔ¨Åers in terms of training and test error rates using a land-cover/land-use data set.
When extending the spatial probit regression model to the multi-category setting, care
must be taken to ensure that model parameters are estimable and interpretable. Considering
three types of categorical and spatial covariate information, we discuss various speciÔ¨Åca-
tions of the latent variable mean structure and the associated parameter interpretations.
Additionally, we explore the speciÔ¨Åcation of the latent variable cross space-category de-
pendence structure and discuss how data augmentation MCMC strategies for Ô¨Åtting the
Bayesian spatial probit regression model can be extended to the multi-category setting.
iii

Dedicated to my parents, Bob and Nanette,
and siblings, Tenille, Nat, Preston, MeChel, and Taylor.
iv

ACKNOWLEDGMENTS
First and foremost, I would like to thank my advisor, Dr. Kate Calder, who over the
last four and a half years has devoted a substantial amount of time and effort in training
me to be a well-rounded statistician. She has provided me with numerous opportunities
to learn and grow through research, teaching, mentoring, and collaboration. She has also
become a good friend, whom I admire professionally and personally, and I am grateful for
her example and support.
I would like to thank my committee members: Dr. Mark Berliner for his comments on
my research, his help with job and fellowship applications, and for allowing me to laugh
in his class; Dr. Elizabeth Stasny for her comments on my research, her help with job and
fellowship applications, her support as graduate chair, and in encouraging me to come to
Ohio State; and Dr. Peter Craigmile for his valuable comments and contributions to my
research.
I would like to thank Dr. Darla Munroe and Dr. Ningchuan Xiao of the Department of
Geography for their generous assistance in obtaining and understanding the land cover data
used in this work.
I would like to thank the other professors in the Department of Statistics who have
provided guidance and support during my time at Ohio State: Dr. Doug Wolfe, Dr. Tao Shi,
Dr. Chris Hans, Dr. Jackie Miller, Dr. Steve MacEachern, and Dr. Noel Cressie.
v

I would like to thank Lisa Van Dyke for her help in answering my many graduation
questions and in pulling together the Ô¨Ånal documents of this dissertation. I would also like
to thank Terry England for her help with all my travel and posters.
Support for this research was provided by grants from NASA (NNG06GD31G) and the
NSF (ATM-0934595).
Finally, I would like to thank my family and many friends, who all believed in me
when I didn‚Äôt believe in myself; and God, for giving me strength and understanding, and
providing me with opportunities to grow.
vi

VITA
1983 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Born - Ogden, Weber, Utah, USA
2005 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.S. Actuarial Science, cum laude,
Brigham Young University.
2005 - 2006 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . University Fellow, Graduate School,
The Ohio State University.
2005 - 2006, 2010 . . . . . . . . . . . . . . . . . . . . . . . . . . .Teaching Assistant, Department of Statis-
tics, The Ohio State University.
2007 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . M.S. Statistics,
The Ohio State University.
2007 - 2010 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Research Assistant, Department of Statis-
tics, The Ohio State University.
2009 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Graduate Fellow, Statistical and Applied
Mathematical Sciences Institute.
PUBLICATIONS
Research Publications
Xiao, N., Shi, T., Calder, C.A., Munroe, D.K., Berrett, C., WolÔ¨Ånbarger, S., and Li, D.
(2008)
‚ÄúSpatial Characteristics of the Difference between MISR and MODIS Aerosol
Optical Depth Retrievals over Mainland Southeast Asia,‚Äù Remote Sensing of Environment,
DOI: 10.1016/j.rse.2008.07.011.
FIELDS OF STUDY
Major Field: Statistics
vii

TABLE OF CONTENTS
Page
Abstract . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
ii
Dedication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
iv
Acknowledgments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
v
Vita
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
vii
List of Tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
xi
List of Figures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
xii
Chapters:
1.
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.1
Background and Motivation . . . . . . . . . . . . . . . . . . . . . . . .
2
1.2
Modeling Categorical Spatial Data . . . . . . . . . . . . . . . . . . . . .
15
1.2.1
The Spatial Generalized Linear Model
. . . . . . . . . . . . . .
15
1.2.2
The Spatial Generalized Linear Mixed Model . . . . . . . . . . .
19
1.2.3
Indicator Kriging . . . . . . . . . . . . . . . . . . . . . . . . . .
20
1.2.4
The Autologistic Model . . . . . . . . . . . . . . . . . . . . . .
22
1.2.5
The Bayesian Spatial Probit Regression Model . . . . . . . . . .
23
1.3
Overview of Contributions . . . . . . . . . . . . . . . . . . . . . . . . .
24
1.4
Illustrative Data Set
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
25
2.
Bayesian Spatial Probit Regression . . . . . . . . . . . . . . . . . . . . . . . .
29
2.1
The Bayesian Probit Regression Model . . . . . . . . . . . . . . . . . .
29
2.1.1
Albert and Chib‚Äôs Data Augmentation Strategy . . . . . . . . . .
29
viii

2.1.2
Multi-Category and Multivariate Extensions
. . . . . . . . . . .
31
2.2
The Bayesian Spatial Probit Regression Model . . . . . . . . . . . . . .
34
2.2.1
Model SpeciÔ¨Åcation . . . . . . . . . . . . . . . . . . . . . . . .
34
2.2.2
Parameterization of the Spatial Correlation Matrix . . . . . . . .
36
3.
Data Augmentation MCMC Strategies . . . . . . . . . . . . . . . . . . . . . .
39
3.1
Data Augmentation MCMC Strategies . . . . . . . . . . . . . . . . . . .
40
3.1.1
Conditional versus Marginal Data Augmentation . . . . . . . . .
40
3.1.2
Partially Collapsed Algorithms
. . . . . . . . . . . . . . . . . .
45
3.1.3
Full Conditional Distributions . . . . . . . . . . . . . . . . . . .
46
3.2
Simulation Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
49
3.2.1
Simulation Set-up . . . . . . . . . . . . . . . . . . . . . . . . .
49
3.2.2
Simulation Results . . . . . . . . . . . . . . . . . . . . . . . . .
52
3.3
Application . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
54
3.4
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
56
4.
The Bayesian Spatial Probit Regression Model as a Tool for ClassiÔ¨Åcation . . .
73
4.1
The ClassiÔ¨Åcation Problem . . . . . . . . . . . . . . . . . . . . . . . . .
74
4.2
GLM-Based ClassiÔ¨Åcation . . . . . . . . . . . . . . . . . . . . . . . . .
76
4.2.1
Non-Spatial GLM-Based ClassiÔ¨Åcation . . . . . . . . . . . . . .
76
4.2.2
Spatial GLM-Based ClassiÔ¨Åcation . . . . . . . . . . . . . . . . .
80
4.3
Alternative ClassiÔ¨Åcation Methods . . . . . . . . . . . . . . . . . . . . .
84
4.3.1
Discriminant Analysis . . . . . . . . . . . . . . . . . . . . . . .
84
4.3.2
Support Vector Machines
. . . . . . . . . . . . . . . . . . . . .
90
4.3.3
k-Nearest Neighbors . . . . . . . . . . . . . . . . . . . . . . . .
93
4.4
Comparison of ClassiÔ¨Åcation Methods . . . . . . . . . . . . . . . . . . .
94
4.4.1
Parameter Estimation
. . . . . . . . . . . . . . . . . . . . . . .
95
4.4.2
ClassiÔ¨Åcation Errors . . . . . . . . . . . . . . . . . . . . . . . .
97
4.5
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
5.
Bayesian Spatial Multinomial Probit Regression . . . . . . . . . . . . . . . . . 102
5.1
The Bayesian Spatial Multinomial Probit Regression Model . . . . . . . 102
5.1.1
Latent Mean SpeciÔ¨Åcation . . . . . . . . . . . . . . . . . . . . . 104
5.1.2
Parameterization of the Space-Category Covariance Matrix
. . . 125
5.2
Model-Fitting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128
5.2.1
Data Augmentation MCMC Algorithms
. . . . . . . . . . . . . 128
5.3
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133
ix

6.
Contributions and Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . 134
x

LIST OF TABLES
Table
Page
3.1
This table lists the steps in each of the data augmentation algorithms. The
Ô¨Årst portion shows the non-collapsed data augmentation algorithms intro-
duced in Section 3.1.1. The second portion shows the partially collapsed
data augmentation algorithms introduced in Section 3.1.2. . . . . . . . . . .
44
3.2
Scenarios used to compare the marginal and conditional data augmentation
algorithms.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
50
3.3
Autocorrelations of the sample paths of Œ≤1 and œÅ for the land cover data
analysis. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
56
4.1
Fitted values for the covariance function parameters for both class C0 and C1. 96
4.2
Tuning parameter values for each classiÔ¨Åcation method. The optimal value
of the tuning parameter is listed along with the CVE associated with this
value. The optimal values were chosen by minimizing the Ô¨Åve-fold CVE.
.
97
4.3
Training and test errors for the SE Asia land cover data obtained using each
of the classiÔ¨Åcation methods discussed in this chapter. . . . . . . . . . . . . 100
xi

LIST OF FIGURES
Figure
Page
1.1
Land cover over Southeast Asia, covering the region bounded by 17‚ó¶to
21‚ó¶N and 98‚ó¶to 105‚ó¶E. The data were taken from the MODIS Land Cover
Type Yearly Level 3 Global 500m (MOD12Q1 and MCD12Q1) data prod-
uct for the year 2005. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
26
1.2
Elevation (in meters) over the region bounded by 17‚ó¶to 21‚ó¶N and 98‚ó¶to
105‚ó¶E. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
27
1.3
Standardized value of the measured distance to the nearest major road over
the region bounded by 17‚ó¶to 21‚ó¶N and 98‚ó¶to 105‚ó¶E. . . . . . . . . . . . .
27
1.4
Standardized value of the measured distance to the coast over the region
bounded by 17‚ó¶to 21‚ó¶N and 98‚ó¶to 105‚ó¶E. . . . . . . . . . . . . . . . . . .
28
1.5
Standardized value of the measured distance to the nearest big city over the
region bounded by 17‚ó¶to 21‚ó¶N and 98‚ó¶to 105‚ó¶E. . . . . . . . . . . . . . .
28
2.1
Illustration of neighborhood structures for a regular grid. The grid cell
with the black dot represents the location of interest and its neighbors are
represented by the grid cells with the empty squares for (a) a Ô¨Årst order
neighborhood structure and (b) a second order neighborhood structure. . . .
38
3.1
Histograms and trace plots for Œ≤ and œÅ under Scenario 1 for each of the
three corresponding algorithms. The black portion represents the burn-in
and the colored portion represents draws from the posterior distribution. . .
58
3.2
Histograms and trace plots for Œ≤ and œÅ under Scenario 2 for each of the
three corresponding algorithms. The black portion represents the burn-in
and the colored portion represents draws from the posterior distribution. . .
59
xii

3.3
Histograms and trace plots for Œ≤ and Œª under Scenario 3 for each of the
three corresponding algorithms. The black portion represents the burn-in
and the colored portion represents draws from the posterior distribution. . .
60
3.4
Histograms and trace plots for Œ≤ and Œª under Scenario 4 for each of the
three corresponding algorithms. The black portion represents the burn-in
and the colored portion represents draws from the posterior distribution. . .
61
3.5
Histograms and trace plots for Œ≤ under Scenario 5 for each of the three
corresponding algorithms. The black portion represents the burn-in and the
colored portion represents draws from the posterior distribution.
. . . . . .
62
3.6
Autocorrelation and partial autocorrelation in the sample paths of Œ≤ and œÅ
under Scenario 1 for each of the three corresponding algorithms. . . . . . .
63
3.7
Autocorrelation and partial autocorrelation in the sample paths of Œ≤ and œÅ
under Scenario 2 for each of the three corresponding algorithms. . . . . . .
64
3.8
Autocorrelation and partial autocorrelation in the sample paths of Œ≤ and Œª
under Scenario 3 for each of the three corresponding algorithms. . . . . . .
65
3.9
Autocorrelation and partial autocorrelation in the sample paths of Œ≤ and Œª
under Scenario 4 for each of the three corresponding algorithms. . . . . . .
66
3.10 Autocorrelation and partial autocorrelation in the sample path of Œ≤ under
Scenario 5 for each of the three corresponding algorithms.
. . . . . . . . .
67
3.11 œÉ vs. ÀúŒ≤ under Scenario 1 for each of the three corresponding algorithms.
The black dots represent the burn in and the colored dots represent the
draws from the posterior. Top row: First starting values. Bottom row:
Second starting values. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
68
3.12 œÉ vs. ÀúŒ≤ under Scenario 2 for each of the three corresponding algorithms.
The black dots represent the burn in and the colored dots represent the
draws from the posterior. Top row: First starting values. Bottom row:
Second starting values. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
69
3.13 œÉ vs. ÀúŒ≤ under Scenario 3 for each of the three corresponding algorithms.
The black dots represent the burn in and the colored dots represent the
draws from the posterior. Top row: First starting values. Bottom row:
Second starting values. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
70
xiii

3.14 œÉ vs. ÀúŒ≤ under Scenario 4 for each of the three corresponding algorithms.
The black dots represent the burn in and the colored dots represent the
draws from the posterior. Top row: First starting values. Bottom row:
Second starting values. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
71
3.15 œÉ vs. ÀúŒ≤ under Scenario 5 for each of the three corresponding algorithms.
The black dots represent the burn in and the colored dots represent the
draws from the posterior. Top row: First starting values. Bottom row:
Second starting values. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
72
4.1
Illustration of the hyperplane (solid line) and margins (dotted lines) deter-
mined by support vector machines separating the two classes (denoted by
the two plotting symbols). The hyperplane in (a) is the maximum margin
hyperplane and the hyperplane in (b) does not maximize the margin. . . . .
91
4.2
Estimated and Ô¨Åtted variograms for class C0 (left) and C1 (right) Ô¨Åt using
the training data set. The numbers 1-4 indicate the estimated variogram for
the four covariates, the dots represent the mean of the four variograms, and
the line indicates the Ô¨Åtted variogram.
. . . . . . . . . . . . . . . . . . . .
96
xiv

CHAPTER 1
INTRODUCTION
The Ô¨Åeld of spatial statistics encapsulates a wide array of statistical methodology for
analyzing spatial data, or observations associated with particular points or regions in space.
The increasing availability and variety of spatial data has generated an enormous amount
of methodological development in the Ô¨Åeld over the past 20 years, and today the Ô¨Åeld is
one of the most active areas of research in all of statistics (see Banerjee et al., 2004; Waller
and Gotway, 2004; Schabenberger and Gotway, 2005; Diggle and Ribeiro Jr., 2007, for a
discussion of recent advances). Spatial statistical methods are also increasingly being used
in areas of application across the biological, environmental, health, and social sciences, and
often speciÔ¨Åc problems in these Ô¨Åelds motivate further methodological developments. In
fact, the motivation for much of the research presented in this dissertation is the study of
land-cover/land-use change (LCLUC) using satelite-derived land cover observations. Be-
fore discussing this motivating application that is used throughout this dissertation, we Ô¨Årst
provide background and motivation as to why incorporating spatial information is impor-
tant in statistical analyses of spatially-referenced data in Section 1.1. In Section 1.2 we give
an overview of existing spatial statistical methodology for spatially-dependent categorical
data. In Section 1.3, we outline the contributions of this dissertation in terms of statistical
methodology for analyzing spatially-dependent categorical data using the Bayesian spatial
1

probit regression model. We then discuss the LCLUC motivating application by introduc-
ing a speciÔ¨Åc illustrative data set in Section 1.4.
1.1
Background and Motivation
Let si ‚ààD ‚äÇRr be an r √ó 1 vector in Euclidean space. In spatial statistics, typically,
r = 2 or 3, indicating two- or three-dimensional space, respectively. Let, Y (si) ‚â°Yi for
i = 1, . . . , n, where, for now, Yi is a continuous real-valued random variable observed at
a point si ‚ààD. There are three types of spatially-referenced data (Cressie, 1993), deÔ¨Åned
by the nature of the spatial domain, D:
‚Ä¢ Geostatistical/point-referenced data, where si varies continuously over D and D is a
Ô¨Åxed subset of Rr. In this case, Y (si) can potentially be observed everywhere within
D.
‚Ä¢ Lattice/gridded data, where D is again a Ô¨Åxed subset of Rr, but in this case, the
elements of D are areal units. Thus, si may not represent a point in space, but will
indicate an areal unit indexed by the point si.
‚Ä¢ Point pattern data, where D is still a subset of Rr, but the si are now viewed as
random. For point pattern data, the observed si are modeled as realizations from a
stochastic process, whereas with geostatistical and lattice data, the points are usually
assumed to be Ô¨Åxed and non-stochastic.
In this dissertation, we only consider statistical methods for the analysis of geostatistical
and lattice data.
We expect spatially-referenced data to have a speciÔ¨Åc type of dependence structure,
called spatial dependence, where observations near to one another are more similar than
2

observations further apart. Often, a statistical analysis of spatial data will focus on learn-
ing about the nature of the spatial dependence or in exploiting it for prediction purposes.
Sometimes, however, we may need to account, or adjust, for spatial dependence when we
want to measure or quantify relationships between two observable quantities of interest. In
particular, in a regression analysis a researcher may be interested in determining how one
variable Y , known as the response variable or dependent variable, varies given a set of
other variables x, known as covariates, independent variables, or explanatory variables.
For example:
‚Ä¢ Environmental Health. Researchers and policy makers may be interested in the re-
lationship between exposure to particulate matter or other pollutants and a particular
health outcome (e.g., cancer or mortality).
‚Ä¢ Real Estate. An economist may be interested in determining factors that contribute
to whether or not a house will sell in a particular period of time.
‚Ä¢ Land-Cover/Land-Use Change. Researchers may be interested in determining the
demographic, social, geographical, and political factors contributing to the observed
type of land cover at various locations.
The Classical Linear Regression Model
Before describing the spatial regression model, we Ô¨Årst review the linear regression
model from both a classical and Bayesian perspective. Let {(Yi, xi); i = 1, . . . , n}, be
a collection of paired observations, where Yi ‚ààR is a univariate real-valued response
variable or dependent variable, and xi is a k √ó 1 vector of continuous or discrete-valued
covariates or independent variables. For now, assume the Yi are conditionally independent
3

for all i = 1, . . . , n given the independent variables. The linear regression model speciÔ¨Åes
the following relationship between the components of xi and the mean of Yi:
E(Yi) = x‚Ä≤
iŒ≤,
where Œ≤ is a k √ó 1 vector of Ô¨Åxed but unknown regression coefÔ¨Åcients. The classical linear
regression model is often written as
Yi = x‚Ä≤
iŒ≤ + œµi,
(1.1)
where œµi is a random noise error term with mean 0 and variance œÉ2 and is assumed to be
independent for all i = 1, . . . , n. It is common to assume that the œµi are also normally
distributed, but that assumption is not necessary.
One method for Ô¨Åtting the linear regression model is maximum likelihood estimation.
When the œµis are assumed to be normally distributed, the likelihood function is
L(Œ≤, œÉ2|Y ) ‚àùp(Y |x1, . . . , xn, Œ≤, œÉ2)
=
n
Y
i=1
p(Yi|xi, Œ≤, œÉ2)
=
n
Y
i=1
1
‚àö
2œÄœÉ2 exp{‚àí1
2œÉ2(Yi ‚àíx‚Ä≤
iŒ≤)2},
(1.2)
where Y = (Y1, . . . , Yn)‚Ä≤. Maximizing (1.2) with respect to Œ≤ and œÉ2 yields the following
maximum likelihood estimator (MLE) for Œ≤:
ÀÜŒ≤MLE =
 
1
n
n
X
i=1
xix‚Ä≤
i
!‚àí1  
1
n
n
X
i=1
xiYi
!
.
We can write this estimator of Œ≤ in matrix notation by letting X = (x‚Ä≤
1, . . . , x‚Ä≤
n)‚Ä≤ so that
ÀÜŒ≤MLE = (X‚Ä≤X)‚àí1 X‚Ä≤Y .
4

The MLE of œÉ2 is
ÀÜœÉ2
MLE = 1
n ÀÜœµ‚Ä≤ÀÜœµ,
where ÀÜœµ = Y ‚àíX ÀÜŒ≤MLE. This likelihood-based model-Ô¨Åtting approach is considered to
be within the class of classical or frequentist statistics and standard inferential statements
about the uncertainty in model parameter estimates can be obtained (e.g., conÔ¨Ådence inter-
vals, hypothesis tests). Another method for Ô¨Åtting the linear regression model is known as
ordinary least squares (OLS), which does not require an assumption about the distribution
of the error terms, but instead seeks to minimize the sum of the squared residuals, or the
difference between the observed response Yi and the predicted value of the response ÀÜYi. We
note that the OLS estimator for Œ≤ is identical to the MLE provided above. As an alternative
to these two approaches, we can consider a Bayesian version of the linear regression model,
which we discuss below.
The Bayesian Linear Regression Model
In Bayesian analyses, we specify prior distributions on the model parameters; for the
linear regression model, a prior distribution is needed for Œ≤ and œÉ2, and is denoted œÄ(Œ≤, œÉ2).
A prior distribution is a probability distribution which captures our knowledge about the
model parameters prior to observing the data. Then, using the data likelihood and prior dis-
tribution, we determine posterior distributions for the model parameters, or the distribution
of the model parameters given the data, using Bayes‚Äô Theorem. For the linear regression
model, the posterior distribution is œÄ(Œ≤, œÉ2|Y ). Posterior distributions are determined ei-
ther analytically or numerically using simulation-based techniques such as Markov chain
Monte Carlo (MCMC); see Gelman et al. (1995) for a general overview of Bayesian data
5

analysis techniques. In the Bayesian setting, inferences about model parameters are then
reported based on summaries of the posterior distributions of model parameters.
Before describing the Bayesian linear regression model, we Ô¨Årst discuss the role of the
covariates X in the speciÔ¨Åcation of the posterior distribution of the model parameters, fol-
lowing the discussion in Chapter 14 of Gelman et al. (1995). In the Bayesian paradigm, the
observed data in a regression analysis include both Y and X. Because of this, a Bayesian
model will include the speciÔ¨Åcation of a joint distribution on (Y , X), p(Y , X|œàY |X, œàX),
where œàY |X are the model parameters associated with the distribution of Y |X ‚Äì for the
Bayesian linear regression model, œàY |X = (Œ≤, œÉ2) ‚Äì and œàX are the parameters associ-
ated with X. However, the Bayesian linear regression model traditionally assumes that the
distribution of X provides no additional information about the conditional distribution of
Y |X, or that
p(Y , X|œàY |X, œàX) = p(Y |œàY |X, X)p(X|œàX)
(1.3)
and œàY |X and œàX are assumed independent a priori so that
œÄ(œàY |X, œàX) = œÄ(œàY |X)œÄ(œàX).
(1.4)
Thus, a Bayesian regression model inherently assumes that the prior distribution on X
provides no information about the regression model parameters, œàY |X = (Œ≤, œÉ). To see
this, notice that using equations (1.3) and (1.4), we can write the joint posterior distribution
of (œàY |X, œàX) as
œÄ(œàY |X, œàX|Y , X) =
p(Y , X|œàY |X, œàX) œÄ(œàY |X, œàX)
R R
p(Y , X|œàY |X, œàX) œÄ(œàY |X, œàX) dœàY |X dœàX
=
p(Y |œàY |X, X) œÄ(œàY |X) p(X|œàX) œÄ(œàX)
 R
p(Y |œàY |X, X) œÄ(œàY |X) dœàY |X
  R
p(X|œàX) œÄ(œàX) dœàX

= œÄ(œàY |X|Y , X)œÄ(œàX|X).
6

It follows that we can determine the posterior distribution of œàY |X by only considering
œÄ(œàY |X|Y , X) ‚àùœÄ(œàY |X)p(Y |X, œàY |X).
(1.5)
To distinguish the roles X and Y play in Bayesian inference on œàY |X, it is common
practice to tacitly condition on X and denote the posterior distribution of Œ≤ and œÉ2 as
œÄ(Œ≤, œÉ2|Y ). We follow this convention throughout this dissertation except in Chapter 4 for
reasons that we discuss therein.
Although any probability distribution can be chosen for modeling the error terms of the
regression model, typically, the errors are assumed to be normally distributed. Equivalently,
it is assumed that
Y |Œ≤, œÉ2, X ‚àºN(XŒ≤, œÉ2I),
where I is the n√ón identity matrix. We then assign prior distributions to the parameters Œ≤
and œÉ2. The standard choice for the Bayesian linear regression model is the noninformative
prior distribution
œÄ(Œ≤, œÉ2|X) ‚àù1
œÉ2.
Using these likelihood and prior distributions results in a posterior distribution that can be
decomposed as follows:
œÄ(Œ≤, œÉ2|Y ) = œÄ(Œ≤|œÉ2, Y )œÄ(œÉ2|Y ),
where
Œ≤|œÉ2, Y ‚àºN( ÀÜŒ≤B, œÉ2VB),
and
œÉ2|Y ‚àºÀÜœÉ2
B(œá2
n‚àík)‚àí1,
7

with
ÀÜŒ≤B = (X‚Ä≤X)‚àí1 X‚Ä≤Y ,
VB = (X‚Ä≤X)‚àí1 ,
and
ÀÜœÉ2
B =
1
n ‚àík

Y ‚àíX ÀÜŒ≤B
‚Ä≤ 
Y ‚àíX ÀÜŒ≤B

,
and where (œá2
n‚àík)‚àí1 denotes the inverse chi-squared distribution with n ‚àík degrees of
freedom.
Often in Bayesian analyses, when the full joint posterior distribution is not available in
closed form, we use simulation-based approaches to generate samples from the posterior
distribution. We can make inferences about (Œ≤, œÉ2) by using a simulation-based (Monte
Carlo) approach where we generate samples from the posterior distribution of (Œ≤, œÉ2).
To do this, we sample from œÉ2|Y and then, given the sampled value of œÉ2, sample from
Œ≤|œÉ2, Y . Posterior inferences are then reported based on the empirical distribution of these
sampled values.
Comparison to Classical Estimators
Notice that in the Bayesian case, the mean of the conditional posterior distribution for
Œ≤|œÉ2, Y is the same as the MLE, i.e., ÀÜŒ≤B = ÀÜŒ≤MLE. Furthermore, ÀÜœÉ2
B =
n
n‚àík ÀÜœÉ2
MLE. There-
fore, Bayesian inferences on model parameters can be similar to inferences based on the
classical estimators. A Bayesian analysis, however, offers additional beneÔ¨Åts, including:
1. Flexibility in model speciÔ¨Åcation. Although the means of the posterior distributions
of Œ≤ and œÉ2 are similar to the classical estimators, this is not always the case. To see
this, suppose we have more information a priori about Œ≤: for example, we may know
8

or require Œ≤ > 0, where 0 is a k √ó 1 vector of zeros. A Bayesian analysis provides
a natural and straightforward way to allow for this prior knowledge or constraint, to
be included in an analysis through an informative prior distribution.
2. Interpretability of inferences. In a classical/frequentist setting, inferences are often
reported as conÔ¨Ådence intervals. ConÔ¨Ådence intervals, however, are not probability
intervals, although many interpret them as such. In a Bayesian setting, the posterior
distribution provides interpretable probabilistic statements about model parameters,
making clear the uncertainty associated with parameter estimates. Furthermore, this
uncertainty is easily propagated to uncertainty in deterministic and stochastic func-
tions of parameters (e.g., transformations and predicted values).
3. Allowing for increased complexity in model speciÔ¨Åcation. Increasing the complex-
ity of a model (e.g., hierarchical modeling with a large number of unknown pa-
rameters, allowing for complicated spatial dependence structures, or modeling non-
continuous data) can make model-Ô¨Åtting in a classical/frequentist setting infeasible.
However, simulation-based approaches such as MCMC (see discussion below) can
make Bayesian model Ô¨Åtting feasible in high dimensional situations.
The Spatial Linear Regression Model
We now consider a more general form of linear regression in order to account for spatial
dependence among the errors. In some instances, the error terms in the linear regression
model will not be independent. This is often the case for spatially-referenced data, but there
can also be residual dependence in data obtained from studies using repeated measures or
when data are observed over time. When an assumption of residual independence is not
valid, we can specify a general linear regression model. The model is the same as in (1.1),
9

however we no longer assume the œµi are independent, but rather assume that
Var(œµ) = Œ£,
where Œ£ is a valid and invertible covariance matrix (i.e., positive deÔ¨Ånite and symmet-
ric). Again, it is common to assume œµ is normally distributed, but this assumption is not
necessary.
To estimate the parameters of the general linear model, we consider two cases, Œ£ is
known and Œ£ is unknown. When Œ£ is known, for maximum likelihood estimation, we
typically specify a normal distribution on the error terms resulting in the likelihood function
L(Œ≤, |Y ) ‚àùp(Y |X, Œ≤, Œ£)
=
1
(2œÄ)n/2|Œ£|1/2 exp

‚àí1
2(Y ‚àíXŒ≤)‚Ä≤Œ£‚àí1(Y ‚àíXŒ≤)

.
The MLE for Œ≤ is then
ÀÜŒ≤MLE =
 X‚Ä≤Œ£‚àí1X
‚àí1 X‚Ä≤Œ£‚àí1Y .
Weighted (or generalized) least squares, an extension of OLS, results in the same estimator
for Œ≤.
When Œ£ is unknown, we can parameterize Œ£ (i.e., let Œ£ = Œ£(Œ∏)). The spatial linear
model is deÔ¨Åned to be the general linear model with a speciÔ¨Åc type of dependence captured
by Œ£(Œ∏) (see the discussion in Section 2.2.2). In this case, the likelihood function is
L(Œ≤, Œ∏|Y ) ‚àùp(Y |X, Œ≤, Œ£(Œ∏))
=
1
(2œÄ)n/2|Œ£|1/2 exp

‚àí1
2(Y ‚àíXŒ≤)‚Ä≤ (Œ£(Œ∏))‚àí1 (Y ‚àíXŒ≤)

,
which is maximized with respect to Œ≤ and Œ∏ to obtain the MLE. Another likelihood-based
approach to estimation is restricted/residual maximum likelihood (REML; Patterson and
10

Thompson, 1971) estimation where we introduce a matrix C satisfying E(CY ) = 0 and
Var(CY ) = œÉ2I. Then, given C, we use standard maximum likelihood estimation to
Ô¨Åt the model CY = CXŒ≤ + Cœµ. Another approach is based on an extension of OLS
called iteratively reweighted least squares (IRLS). IRLS Ô¨Årst Ô¨Åxes the covariance Œ£ to
some preliminary estimate and, conditioning on this value, uses the weighted least squares
estimator to obtain an initial estimate of Œ≤. Using this estimate of Œ≤, IRLS estimation
then obtains residuals and determines a more accurate estimate of Œ£. Using the updated
estimates, IRLS iterates between these two steps until the values of the parameter estimates
have converged.
Alternatively, we can specify a Bayesian version of the general linear model by spec-
ifying prior distributions on model parameters and using a simulation-based method to
approximate the posterior distributions of model parameters. The model-Ô¨Åtting approach
is similar to that used for the Bayesian linear regression model, however, we now specify
a prior distribution on Œ£ or on parameters deÔ¨Åning Œ£. Because of the parameterization of
Œ£, Ô¨Åtting a Bayesian general linear regression model requires more complex simulation-
based approaches than those used in the Bayesian linear regression model. In general, the
joint posterior distribution of Œ≤ and Œ∏ cannot be decomposed into the product of standard
density functions to allow Monte Carlo samples from the posterior to be drawn directly.
Instead, it has become standard practice to use an estimation technique known as Markov
chain Monte Carlo (MCMC). Rather than drawing directly from the posterior distribution,
an MCMC algorithm draws samples from a Markov chain with a long run, or stationary
distribution, equal to the posterior. Two popular MCMC algorithms are:
11

‚Ä¢ The Gibbs sampler, where we draw a sample from the full conditional posterior dis-
tributions of each parameter sequentially. For the Bayesian spatial linear regression
model, for each iteration we sample
1. Œ≤[t] ‚àºœÄ(Œ≤|Y , Œ∏[t‚àí1])
2. Œ∏[t] ‚àºœÄ(Œ∏|Y , Œ≤[t])
where Œ≤[t] and Œ∏[t] represent the sampled values of Œ≤ and Œ∏ for the tth iteration, and
each is sampled conditional on the current value of the other parameter.
‚Ä¢ The Metropolis-Hastings algorithms, where for each iteration, we draw a sample
from a proposal distribution and either accept or reject the proposed value. Let œà
be the parameter of interest and Y the observed data. Then, we accept the proposed
value with probability
a(œà‚àó, œà[t‚àí1]) = min
 œÄ(œà‚àó|Y )/Jt(œà‚àó|œà[t‚àí1])
œÄ(œà[t‚àí1]|Y )/Jt(œà[t‚àí1]|œà‚àó), 1

,
where œà‚àóis the proposed value of œà sampled from the proposal distribution for iter-
ation t, Jt(¬∑|œà[t‚àí1]). It follows from Bayes‚Äô theorem that this acceptance probability
can be simpliÔ¨Åed to rely only on the prior, likelihood, and proposal distribution, i.e.,
a(œà‚àó, œà[t‚àí1]) = min

œÄ(œà‚àó)p(Y |œà‚àó)/Jt(œà‚àó|œà[t‚àí1])
œÄ(œà[t‚àí1])p(Y |œà[t‚àí1])/Jt(œà[t‚àí1]|œà‚àó), 1

.
The Metropolis random walk algorithm is a special case of the Metropolis-Hastings
algorithm, where the proposal distribution for each parameter is a symmetric dis-
tribution centered on the previous sampled value of that parameter. In this case,
Jt(œà‚àó|œà[t‚àí1]) = Jt(œà[t‚àí1]|œà‚àó), resulting in the simpliÔ¨Åed form for the acceptance
12

probability given by,
a(œà‚àó, œà[t‚àí1]) = min

œÄ(œà‚àó)p(Y |œà‚àó)
œÄ(œà[t‚àí1])p(Y |œà[t‚àí1]), 1

.
When Ô¨Åtting Bayesian models, it is also common to use a mixture of these two MCMC
algorithms. For example, in the Gibbs sampler for the Bayesian spatial regression model,
we can use a Metropolis-Hastings step in place of Step 2 listed above.
We note that we must specify starting values for the parameters for both types of
MCMC algorithms listed above. To reduce the effect of these starting values on the Ô¨Ånal
parameter inferences and allow the Markov chains to reach their stationary distributions,
typically we discard the sampled draws from a number of the initial iterations. These dis-
carded values are called the burn in samples.
Although it is a special case of the general linear regression model, we argue that the
spatial linear regression model has certain features that distinguish it from other models
in this class. First, data modeled using the general linear regression model typically have
some sort of replication, where, for example, we observe a response for n individuals m
times. However, with spatially-referenced data, although we have n locations, often times
we observe a spatial process at these locations only once. Secondly, the speciÔ¨Åc nature
of the dependence places constraints on Œ£. In this dissertation, we consider two classes
of parameterizations for Œ£(Œ∏), corresponding to geostatistical and lattice data, and discuss
these in more detail in Section 2.2.
By accounting for spatial dependence in the spatial linear regression model, we can:
13

1. Quantify the relationship between the response variable and covariates while ac-
counting for residual spatial dependence. When we ignore residual spatial depen-
dence, inferences on model parameters will be invalid and the standard errors associ-
ated with these estimators tend to be too small (see, for example, Schabenberger and
Gotway, 2005).
2. Predict the value of the response variable at an unobserved location using both ob-
served values of the response variable and covariate information. We expect that re-
sponse variables at unobserved locations will be more similar to nearby observations
than to observations which are farther away, and accounting for this phenomenon is
imperative in prediction.
Up to this point, we have only considered regression models for continuous response
variables. Frequently, however, the response variable will not be continuous but may be
reported as belonging to two or more categories. At the beginning of this section, we gave
three examples of situations where a spatial regression model would be desirable. However,
in each of these examples, the response variable may not be continuous:
‚Ä¢ Environmental Health. The health outcome of an individual (e.g., has cancer/does
not have cancer) is a binary response variable.
‚Ä¢ Real Estate. Whether or not the house sells during a particular period of time is a
binary response variable.
‚Ä¢ Land-Cover/Land-Use Change. The land cover category (e.g., forest, agriculture,
grassland, or urban) at each location is a categorical response variable.
14

When the response variable is discrete or categorical, the spatial linear regression model
for continuous response variables is not appropriate. In the following section, we give an
overview of existing approaches for modeling spatially-dependent categorical data.
1.2
Modeling Categorical Spatial Data
Let {(Yi, xi); i = 1, . . . , n} be paired observations at location si, where Yi is now a
categorical response variable and xi is a corresponding k √ó 1 vector of covariates. Let
Y = (Y1, . . . , Yn)‚Ä≤ and X = (x‚Ä≤
1, . . . , x‚Ä≤
n)‚Ä≤.
1.2.1
The Spatial Generalized Linear Model
To motivate the spatial generalized linear model (GLM), we Ô¨Årst describe the GLM for
independent response variables, Ô¨Årst introduced by Nelder and Wedderburn (1972). Let
Yi for i = 1, . . . , n, be independent random variables. If the distribution of Yi|Œ∂i, œâi is a
member of the exponential family, we can write
p(Yi|Œ∂i, œâi) = exp
YiŒ∂i ‚àíb(Œ∂i)
a(œâi)
+ c(Yi, œâi)

.
For the special case of a two-category, or binary, response variable, Yi|Œ∂i, œâi can be assumed
to have a Bernoulli distribution with
Œ∂i
=
log

pi
1 ‚àípi

b(Œ∂i)
=
log(1 + exp(Œ∂i))
a(œâi)
‚â°
a = 1
c(Yi, œâi)
‚â°
c = 0
15

where pi is the probability that Yi = 1. The mean of Yi is E(Yi) = b‚Ä≤(Œ∂i) ‚â°Œªi and variance
of Yi is Var(Yi) = b‚Ä≤‚Ä≤(Œ∂i)a(œâ) ‚â°vŒªi. Therefore, for the Bernoulli distribution,
E(Yi) = b‚Ä≤(Œ∂i) =
exp(Œ∂i)
1 + exp(Œ∂i) = pi
and
Var(Yi) = b‚Ä≤‚Ä≤(Œ∂i)a(œâ) =
exp(Œ∂i)
(1 + exp(Œ∂i))2 = pi(1 ‚àípi).
For an n √ó 1 vector Y = (Y1, . . . , Yn), we have
E(Y ) ‚â°Œª = p
and
Var(Y ) = VŒª,
where VŒª is a diagonal matrix with the ith diagonal element equal to vŒªi = pi(1 ‚àípi).
To relate the Yis to the covariates xi, we introduce a so-called link function, h(¬∑), and
assume
h(Œªi) ‚â°¬µi = x‚Ä≤
iŒ≤,
where Œ≤ is a k √ó 1 vector of coefÔ¨Åcients. Two frequently used link functions for the
Bernoulli distribution are the logit and probit functions:
Logit: ¬µi = log

pi
1 ‚àípi

= x‚Ä≤
iŒ≤
Probit: ¬µi = Œ¶‚àí1(pi) = x‚Ä≤
iŒ≤.
Therefore,
¬µ = XŒ≤,
16

where X = (x‚Ä≤
1, . . . , x‚Ä≤
n)‚Ä≤ is an n √ó k matrix of covariates and ¬µ is a k √ó 1 vector.
One approach to estimate Œ≤ is by specifying a quasi-likelihood (QL; Wedderburn,
1974). Let Ui = u(Œªi; Yi) be the standardized value of Yi, so that, Ui = (Yi ‚àíŒªi)/vŒªi. The
Ô¨Årst two moments of this random variable have the same properties as the log-likelihood
derivatives, so
Ui = Yi ‚àíŒªi
vŒªi
=
‚àÇ
R Œªi
Yi
Yi‚àít
vt dt
‚àÇŒªi
= ‚àÇQ(Œªi; Yi)
‚àÇŒªi
.
Thus,
Q(Œªi; Yi) =
Z Œªi
Yi
Yi ‚àít
vt
dt.
Because the data are independent, Q(Œª; Y ) = Pn
i=1 Q(Œªi; Yi) = V ‚àí1
Œª (Y ‚àíŒª). Notice
that Q(Œª; Y ) is a function of Œª and Y , but we are actually interested in estimating Œ≤.
Therefore, we determine U(Œ≤) = ‚àÇQ(Œª; Y )/‚àÇŒ≤ by calculating
U(Œ≤) = ‚àÇQ(Œª; Y )
‚àÇŒª
‚àÇŒª
‚àÇŒ≤ = ‚àÜ‚Ä≤V ‚àí1
Œª (Y ‚àíŒª)
where ‚àÜis an n √ó k matrix with elements ‚àÜij = ‚àÇŒªi/‚àÇŒ≤j. We then estimate Œ≤ by solving
the quasi-likelihood estimation equations (i.e., setting U( ÀÜŒ≤) = 0 and solving for ÀÜŒ≤).
For the generalized linear model, we now consider the case where the elements of Y
are not independent. Albert and McShane (1995) and Gotway and Stroup (1995) redeÔ¨Åne
the variance of Y to be
Var(Y ) = V 1/2
Œª
R(Œ∏)V 1/2
Œª
‚â°Œ£Y ,
where VŒª is deÔ¨Åned as above, R(Œ∏) is a correlation matrix, and Œ∏ is an m √ó 1 vector of
dependence parameters with m ‚â•1. For spatially-referenced data, R(Œ∏) will be a spatial
correlation matrix like those given in Section 2.2.
17

To Ô¨Åt the GLM for correlated response variables, Liang and Zeger (1986) extend the
QL approach to model Ô¨Åtting. They use Œ£Y , in place of VŒª, resulting in
U(Œ≤) = ‚àÇQ(Œª; Y )
‚àÇŒª
‚àÇŒª
‚àÇŒ≤ = ‚àÜ‚Ä≤Œ£‚àí1
Y (Y ‚àíŒª).
(1.6)
These equations, U(Œ≤), are called generalized estimating equations (GEE).
In the spatial setting, Albert and McShane (1995) use GEE not only to Ô¨Åt Œ≤, but also to
Ô¨Åt Œ£Y . In addition to (1.6), they let
A‚Ä≤ ÀÜŒ£‚àí1
Y (ÀÜœÉY ‚àíœÉY ) = 0
(1.7)
where ÀÜŒ£Y is the working covariance matrix, œÉY and ÀÜœÉY are n(n‚àí1)/2√ó1 vectors consist-
ing of all entries below the diagonal of Œ£Y and ÀÜŒ£Y , and A = ‚àÇœÉY /‚àÇŒ≤. They then iterate
between solving (1.6) and (1.7) until convergence. Lin and Clayton (2005) show asymp-
totic normality and consistency of the GEE for binary spatial data with isotropic covariance
functions. They prove and apply their results for the logit link function. Lin (2008) extends
the GEE to Lp space.
Breslow and Clayton (1993) propose using a pseudolikelihood function to estimate
model parameters in the spatial generalized linear model. Their approach is similar to GEE,
but it also computes ‚Äòpseudodata‚Äô corresponding to the observed binary process within the
iterations of the model-Ô¨Åtting algorithm.
In the literature for spatially-referenced data, there are no Bayesian versions of the
GEE and pseudolikelihood methods described above. This is due to the fact that these
approaches do not have a true likelihood function. Yin (2009) proposes a Bayesian gener-
alized method of moments that uses a weighted quadratic objective function in place of a
likelihood function. To the best of our knowledge, this approach has not been applied in
18

a spatial setting. Instead, it is common to use a Bayesian spatial generalized linear mixed
model (see Section 1.2.2).
Albert and McShane (1995) emphasize the fact that the GEE approach treats the spatial
correlation as a nuisance. In contrast, the GLMM approach seeks to estimate the mean of
the data given the spatial random effect, as described below.
1.2.2
The Spatial Generalized Linear Mixed Model
The spatial generalized linear mixed model (GLMM) extends the GLM by introducing
a random spatially-dependent error term, or spatial random effect, denoted S(si) ‚â°Si, into
the mean model. The spatial GLMM can be written as
E(Yi|Si)
=
Œª(si) ‚â°Œªi
h(Œªi)
=
x‚Ä≤
iŒ≤ + Si
where E(Si) = 0 and Var(S) = Œ£(Œ∏) with S = (S1, . . . , Sn)‚Ä≤ and Œ£(Œ∏) an n √ó n spatial
covariance matrix.
The spatial GLMM was Ô¨Årst introduced within the Bayesian setting by Diggle et al.
(1998) as a general modeling framework for spatially-dependent discrete data. As an al-
ternative, a classical version of the spatial GLMM was introduced by Heagerty and Lele
(1998). Heagerty and Lele propose a composite likelihood approach for Ô¨Åtting a spatial
GLMM for binary response data. Paciorek (2007) gives a review of various versions of
spatial logistic GLMMs, as well as a discussion of the speciÔ¨Åcation of spatial dependence
structure in this class of models.
Diggle et al. (1998) propose Ô¨Åtting the Bayesian spatial GLMM using MCMC algo-
rithms. This model-Ô¨Åtting approach is popular due to established computer software, such
19

as WinBUGS (Lunn et al., 2000), that can be used to Ô¨Åt Bayesian spatial GLMMs (see, for
example Banerjee et al., 2004; Law and Haining, 2004). However, as noted by Paciorek
(2007), MCMC algorithms for Bayesian spatial GLMM often convergence slowly and ex-
hibit poor mixing. To improve the performance of these algorithms, Langevin-Hastings
methods have been suggested in the literature (Christensen et al., 2001; Christensen and
Waagepetersen, 2002; Christensen and Ribeiro Jr., 2002). This method involves modifying
a Metropolis random walk algorithm, so that the proposal distribution not only relies on
the value of the parameter from the previous iteration, but also relies on the gradient of the
log-likelihood function. Christensen et al. (2006) propose further improvements by trans-
forming the posterior values of the spatial random effects, Si, by modifying Si|Yi with a
Cholesky factorization of the posterior covariance matrix. They illustrate that this method
is more robust than either not transforming the Sis or transforming the Sis a priori.
When S has a Gaussian distribution, an alternative Bayesian model-Ô¨Åtting approach
to MCMC is using integrated nested Laplace approximations (INLA; Rue et al., 2009).
INLA seeks to approximate posterior marginals rather than producing samples from the
joint posterior distribution of model parameters. While this approach is attractive, it is not
clear whether its applicability is as general as with MCMC methods.
1.2.3
Indicator Kriging
Rather than modeling relationships between response variables and covariates, indica-
tor kriging predicts values of binary random variables at unobserved locations based only
on nearby binary observations (Switzer, 1977; Journel, 1983). Indicator kriging is based on
a spatial prediction method called kriging and is a direct extension of this popular method
to the binary data setting. Indicator kriging requires a function Œ≥(dij) = (1/2)Var(Yi ‚àíYj),
20

where Œ≥(¬∑) is an isotropic semivariogram and dij = ||si ‚àísj|| are the distance between
locations. Using this function, prediction of Y (s0), where s0 is an unobserved location, is
based on p0 ‚â°P(Y (s0) = 1|Y ). An estimate of p0, ÀÜp0, is its best linear unbiased estimator
(Cressie, 1993; De Oliveira, 2000):
ÀÜp0 =

Œ≥ + 1 ‚àí1‚Ä≤ Œì‚àí1 Œ≥
1‚Ä≤ Œì‚àí1 1
1
‚Ä≤
Œì‚àí1Y
where Œì is an n√ón matrix with the ijth element equal to Œ≥(||si ‚àísj||), Œ≥ is an n√ó1 vector
with the ith element equal to Œ≥(||s0 ‚àísi||), and 1 is an n √ó 1 vector of ones. Then, Y (s0)
is predicted by taking
ÀÜY (s0) =
(
1, if ÀÜp0 >
l0
l0+l1
0, otherwise
,
where l0 and l1 are speciÔ¨Åed losses for mispredicting Yi as a 0 and 1, respectively.
Indicator kriging was originally motivated by considering indicator functions of a col-
lection of real-valued random variables Z = (Z(s1), . . . , Z(sn))‚Ä≤ ‚â°(Z1, . . . , Zn)‚Ä≤ and
deÔ¨Åning Yi = I(Zi < z) for i = 1, . . . , n (Switzer, 1977; Journel, 1983). Solow (1993)
compares the estimates of the probability p0 = P(I(Z(s0) < z) = 1|I(Z1 < z), . . . , I(Zn <
z)) using indicator kriging to the probability p0‚àó= P(Z(s0) < z|Z), where Z has a Gaus-
sian distribution. Although p0 Ã∏‚â°p0‚àó, Solow Ô¨Ånds that the number of incorrectly predicted
values based on Y and Z were similar and relatively small. However, he only uses a sample
size of n = 4.
Indicator kriging has the beneÔ¨Åt that no distributional assumptions about the data gen-
erating process are required. However, there is no guarantee the estimated probabilities will
stay within the appropriate range of [0, 1]. Furthermore, if p0‚àóis the probability of interest,
21

Cressie (1993) states that theoretically, disjunctive kriging1 provides a better approxima-
tion to this probability than indicator kriging. Diggle et al. (1998) also discuss the weak
theoretical assumptions of indicator kriging.
Other suggested approaches for improving indicator kriging are indicator cokriging
(Journel, 1983), which includes covariates to supplement predictions, and probability krig-
ing (Cressie, 1993), which utilizes an estimate of the cumulative distribution function of
Z within the indicator kriging framework. However, these methods also do not guarantee
estimated probabilities remain in the appropriate range.
1.2.4
The Autologistic Model
First proposed by Besag (1972), the autologistic model directly models the spatial cor-
relation among the binary data by relating the log odds of Yi = 1 to the values of Yj for sj
in some neighborhood of si. Let
pi ‚â°P(Yi = 1|Y-i, xi, Œ≤, Œ±),
where Y-i indicates Y with the ith element removed. The autologistic model assumes that
log

pi
1 ‚àípi

= x‚Ä≤
iŒ≤ + Œ±
X
j:i‚àºj
Yj
where xi is a k √ó 1 vector of covariates at location si, Œ≤ is a k √ó 1 vector of coefÔ¨Åcients,
i ‚àºj indicates that location j is a neighbor to location i, and Œ± is the spatial dependence
parameter. In this setting, the log-odds at each location is not only a linear function of the
covariates, but is also dependent on the neighboring binary observations. Extensions and
generalizations of this model have been proposed by Besag (1972), Besag (1974), Augustin
1Disjunctive kriging requires speciÔ¨Åcation of bivariate distributions of (Zi, Zj), 0 ‚â§i < j ‚â§n and
then estimates any measurable function g(Z0), where in the binary setting, g(Z0) = 1(Z0 < z|z) and
E(g(Z0)) = p0‚àó.
22

et al. (1996), Gumpertz et al. (1974), Sim (2000), Zheng and Zhu (2008), and Zhu et al.
(2008).
Methods for Ô¨Åtting the autologistic model include coding, pseudo-likelihood, Monte
Carlo maximum likelihood, and, for a Bayesian version, Gibbs sampling. Coding meth-
ods, proposed by Besag (1974), begin by separating the data into subsets. For each sub-
set, conditional on the values in the remaining portion of the data set, the observations in
that subset are independent. Given these conditionally independent observations, condi-
tional maximum likelihood is used to estimate the parameters. Because there are multiple
ways to separate the data, inconsistencies arise when this method is used. Huffer and Wu
(1998) introduce a Monte Carlo maximum likelihood method to Ô¨Åt this model, and show
their approach yields more efÔ¨Åcient estimates than pseudo-likelihood and coding. Sherman
et al. (2006) give an overview and comparison of pseudo-likelihood, generalized pseudo-
likelihood, and Monte Carlo maximum likelihood model-Ô¨Åtting methods. Augustin et al.
(1996) use a Gibbs sampler to estimate the model parameters.
The autologistic model offers an improvement over indicator kriging in that the esti-
mated pis must be in [0, 1]. In addition, the autologistic model does not require any under-
lying distributional assumptions. However, Weir and Pettitt (1999) discuss computational
challenges associated with Ô¨Åtting the model and note that parameter estimates can be poor
when strong spatial dependence is present.
1.2.5
The Bayesian Spatial Probit Regression Model
The Ô¨Ånal model in the literature used to model spatially-dependent binary data is the
Bayesian spatial probit regression model. Because this model is the focus of this disserta-
tion, we provide a detailed introduction to it in Chapter 2.
23

1.3
Overview of Contributions
In this dissertation, we add to the development of the Bayesian spatial probit regression
model in the following three ways:
1. EfÔ¨Åcient Model Fitting: Models for spatially-dependent data are notoriously cumber-
some to Ô¨Åt. We show how a marginal data augmentation MCMC algorithm can more
efÔ¨Åciently Ô¨Åt the Bayesian spatial probit regression model than standard MCMC al-
gorithms.
2. Spatial ClassiÔ¨Åcation: Within the classiÔ¨Åcation literature, classiÔ¨Åcation methods
which allow for spatial dependence are limited. We show how a spatial classiÔ¨Åcation
rule can be derived from the Bayesian spatial probit regression model and provide
an example where our spatial classiÔ¨Åer outperforms other well known classiÔ¨Åcation
methods.
3. Mulitnomial Model SpeciÔ¨Åcation: When extending the Bayesian spatial probit re-
gression model to the multi-categorical setting, special considerations must be made
when specifying the latent variable mean and covariance structure to ensure model
parameters are estimable and interpretable. We discuss various speciÔ¨Åcations of the
latent mean structure and the associated parameter interpretation, and explore the
speciÔ¨Åcation of the latent cross spatial-categorical dependence structure. Addition-
ally, we discuss how data augmentation MCMC strategies for Ô¨Åtting the Bayesian
spatial probit regression model can be extended to the multi-category setting.
24

1.4
Illustrative Data Set
To illustrate our methods, we use satellite-derived land cover observations over South-
east Asia. In Southeast Asia, deforestation is a major concern and, over the last century,
much of the original forests‚Äìas much as 12 percent‚Äìhave been lost to other land uses
(Munroe et al., 2008). Researchers are interested in the economic, geographic, social,
and demographic factors that contribute to deforestation and other land cover patterns. The
spatial probit regression model is well suited for assessing the strength of the relationships
between binary response variables and covariate information while also allowing for resid-
ual spatial dependence.
The particular data used in our analyses were taken from the Moderate Resolution
Imaging Spectroradiometer (MODIS) Land Cover Type Yearly Level 3 Global 500m
(MOD12Q1 and MCD12Q1) data product for the year 2005. We selected land cover ob-
servations from this data product corresponding to the region bounded by 17‚ó¶to 21‚ó¶N and
98‚ó¶to 105‚ó¶E, which covers portions of Myanmar, Thailand, Laos, and Vietnam. Figure 1.1
shows an image of the land cover over this region. For this Ô¨Ågure, the International Human
Dimensions Programme (IHDP) land cover classiÔ¨Åcations provided by the MODIS data
product were collapsed into Ô¨Åve categories: forest, shrub/grassland, savanna, cropland, and
other.
We also consider four covariates: elevation, distance to the nearest major road, distance
to the coast, and distance to the nearest big city. Elevation is measured in meters and
distances are Euclidean and measured in degrees. The covariates are standardized, meaning
that there were no costs taken into account in calculating distance (e.g. distance calculations
do not take into account the fact that it might take longer to go over mountains than go
25

Figure 1.1: Land cover over Southeast Asia, covering the region bounded by 17‚ó¶to 21‚ó¶N
and 98‚ó¶to 105‚ó¶E. The data were taken from the MODIS Land Cover Type Yearly Level 3
Global 500m (MOD12Q1 and MCD12Q1) data product for the year 2005.
around them). Figures 1.2 - 1.5 show images of the four respective covariates. The city in
the middle of Figure 1.5 (distance to big city) is Vientiane, the capitol of Laos.
Finally, we note that Figure 1.3 has an artiÔ¨Åcial wavy line break. This is most likely
due to the inadequacies in the plotting capabilities of R, the software used to create these
images. However, it may also be an artifact of the country borders and further explanations
are being explored. For the data analyses in Sections 3.3 and 4.4, we use a portion of the
data not affected by this break (i.e., 17‚ó¶to 19‚ó¶N and 98‚ó¶to 100‚ó¶E).
26

Figure 1.2: Elevation (in meters) over the region bounded by 17‚ó¶to 21‚ó¶N and 98‚ó¶to 105‚ó¶E.
Figure 1.3: Standardized value of the measured distance to the nearest major road over the
region bounded by 17‚ó¶to 21‚ó¶N and 98‚ó¶to 105‚ó¶E.
27

Figure 1.4: Standardized value of the measured distance to the coast over the region
bounded by 17‚ó¶to 21‚ó¶N and 98‚ó¶to 105‚ó¶E.
Figure 1.5: Standardized value of the measured distance to the nearest big city over the
region bounded by 17‚ó¶to 21‚ó¶N and 98‚ó¶to 105‚ó¶E.
28

CHAPTER 2
BAYESIAN SPATIAL PROBIT REGRESSION
An attractive alternative to the models for spatially-dependent categorical data reviewed
in Section 1.2 is the Bayesian spatial probit regression model. In this chapter, we moti-
vate this model by Ô¨Årst describing in Section 2.1.1 the latent variable representation of the
Bayesian probit regression model for independent response variables as proposed by Al-
bert and Chib (1993). In Section 2.1.2 we present extensions to the mutlivariate and multi-
category response variable setting. Following this discussion, we introduce the Bayesian
spatial probit regression model in Section 2.2.
2.1
The Bayesian Probit Regression Model
2.1.1
Albert and Chib‚Äôs Data Augmentation Strategy
Consider {(Yi, xi); i = 1, . . . , n}, a collection of n binary response variables Yi and
corresponding k√ó1 vectors of covariates xi. As discussed in Section 1.2.1, the probit GLM
relating the covariates to the binary response variable assumes that the Yis are conditionally
independent given a k √ó 1 vector of regression coefÔ¨Åcients Œ≤ and can be written as
Yi|Œ≤ ‚àºBernoulli(pi)
Œ¶‚àí1(pi) = x‚Ä≤
iŒ≤,
(2.1)
29

where Œ¶‚àí1(¬∑) denotes the inverse standard normal cumulative distribution function. In
the Bayesian setting, a prior distribution must be speciÔ¨Åed for the unknown parameter Œ≤.
Unlike the normal linear regression model where the normal distribution is a conjugate
prior for the regression coefÔ¨Åcients, a conjugate prior is not available for Œ≤ in (2.1). Thus,
inference on Œ≤ typically requires numerical integration, which is not feasible if k is large,
or a simulation-based approach such as MCMC.
To facilitate the use of the Gibbs sampler in the Bayesian probit regression model, Al-
bert and Chib (1993) propose the following data augmentation representation of the model.
They introduce a collection of latent variables ÀúZ = ( ÀúZ1, . . . , ÀúZn)‚Ä≤ and take
Yi =
(
1, if ÀúZi > 0
0, if ÀúZi ‚â§0
,
(2.2)
where
ÀúZ ‚àºN(X ÀúŒ≤, œÉ2In),
(2.3)
X = (x1, . . . , xn)‚Ä≤ is an n √ó k matrix of covariates, In is the n √ó n identity matrix, and
œÉ2 is a variance parameter. (The ‚àºnotation over the random variables used here distin-
guishes identiÔ¨Åable and non-identiÔ¨Åable parameters; we use this notational convention to
aid our discussion of data augmentation strategies in Section 3.1.) Taking œÉ2 = 1, setting
Zi = ÀúZi/œÉ and Œ≤ = ÀúŒ≤/œÉ, and integrating out the Zis, it is straightforward to show that
Albert and Chib‚Äôs model speciÔ¨Åcation is equivalent to the probit GLM given in (2.1). In-
troducing the latent ÀúZis and taking the prior on Œ≤ to be N (mŒ≤, CŒ≤) facilitates model Ô¨Åtting
via the following Gibbs sampler:
30

Step 1: Sample Z|Y , Œ≤, (œÉ2 = 1).
For i = 1, . . . , n, sample Zi from
Zi|Yi, Œ≤, (œÉ2 = 1) ‚àº
(
TN(x‚Ä≤
iŒ≤, œÉ2 = 1, 0, ‚àû),
if Yi = 1
TN(x‚Ä≤
iŒ≤, œÉ2 = 1, ‚àí‚àû, 0),
if Yi = 0 ,
where TN(¬µ, œÉ2, l, u) denotes a truncated normal distribution with mean ¬µ, variance
œÉ2, lower bound l, and upper bound u. By sampling from these univariate truncated
normal distributions, we obtain a sample for Z (Geweke, 1991). In this dissertation,
when sampling from the truncated normal distribution, we use Hans and Craigmile
(2009), which provides a tool in R, based on Geweke (1991), for efÔ¨Åciently sampling
from a truncated normal distribution, particularly when sampling in the tails of the
distribution.
Step 2: Sample Œ≤|Z, Y , (œÉ2 = 1).
When Œ≤ ‚àºN(0, CŒ≤), the full conditional distribution of Œ≤ is given by
Œ≤|Z, Y , (œÉ2 = 1) ‚àºN
 (C‚àí1
Œ≤
+ X‚Ä≤X)‚àí1X‚Ä≤Y , (C‚àí1
Œ≤
+ X‚Ä≤X)‚àí1
.
Each of these steps involves drawing from known distributions from which sampling is
easily implemented; see Albert and Chib (1993) for details.
2.1.2
Multi-Category and Multivariate Extensions
Albert and Chib (1993)‚Äôs data augmentation strategy for the probit GLM has been ex-
tended in several ways. We Ô¨Årst deÔ¨Åne notation for various possible model extensions, then
describe each of the model extensions in more detail. Consider observations associated
with n ‚Äòindividuals‚Äô, each of whom provides m ‚Äòresponses‚Äô (i.e., multivariate observations),
which fall into one of ‚Ñìcategories. Using this notation, the latent variable representation
31

of the Bayeisan probit regression model described in the previous section accommodates
n > 1 individuals with m = 1 responses for each individual, and ‚Ñì= 2 possible cate-
gories, corresponding to the two categories of a binary response. We now consider various
extensions of Albert and Chib‚Äôs data augmentation strategy when ‚Ñì> 2 or m > 1.
In addition to introducing the latent variable representation of the probit regression
model binary responses, Albert and Chib (1993) also provide a multi-category (multino-
mial) extension. Now Yi ‚àà{1, . . . , ‚Ñì} denotes the categorical outcome associated with the
ith individual, where ‚Ñì> 2 is the number of categories. In this case, the latent variables
ÀúZij for i = 1, . . . , n and j = 1, . . . , ‚Ñìare introduced to facilitate model Ô¨Åtting. The latent
variable representation of this model is given by
Yi = arg maxj { ÀúZij, j = 1, . . . , ‚Ñì}
where
ÀúZi ‚àºN([1‚Ñì‚äóx‚Ä≤
i] ÀúŒ≤, Àú‚Ñ¶),
and ÀúZi = ( ÀúZi1, . . . , ÀúZi‚Ñì)‚Ä≤, 1‚Ñìis an ‚Ñì√ó 1 vector of ones, xi is a k √ó 1 vector of covariates, ÀúŒ≤
is a k√ó1 vector of regression coefÔ¨Åcients, and Àú‚Ñ¶is an ‚Ñì√ó‚Ñìcovariance matrix. Conditional
on ÀúŒ≤ and Àú‚Ñ¶, the ÀúZis are independent. That is,
vec( ÀúZ) ‚â°
Ô£Æ
Ô£ØÔ£∞
ÀúZ1
...
ÀúZn
Ô£π
Ô£∫Ô£ª‚àºN(X ÀúŒ≤, In ‚äóÀú‚Ñ¶),
(2.4)
where
X ‚â°
Ô£Æ
Ô£ØÔ£∞
1‚Ñì‚äóx‚Ä≤
1
...
1‚Ñì‚äóx‚Ä≤
n
Ô£π
Ô£∫Ô£ª
is a n‚Ñì√ó k matrix and ‚äódenotes the Kronecker or tensor product between two matrices.
For ÀúŒ≤ to be identiÔ¨Åable, the Ô¨Årst diagonal element of Àú‚Ñ¶, Àú‚Ñ¶1,1, is typically set equal to one
32

(Albert and Chib, 1993; McCulloch et al., 2000). McCulloch and Rossi (1994) and Nobile
(1998), however, do not impose identiÔ¨Åability constraints on Àú‚Ñ¶, but rather assign proper pri-
ors on all parameters and report marginal posterior inferences on the identiÔ¨Åed parameters
(e.g., ÀúŒ≤/ Àú‚Ñ¶1,1). Additional approaches for handling this issue of parameter identiÔ¨Åability
(i.e., Imai and van Dyk, 2005) will be discussed in Section 3.1.
Albert and Chib (1993)‚Äôs latent variable representation of the probit GLM has also been
extended by Chib and Greenberg (1998) to the multivariate setting where n > 1 individuals
have m > 1 responses with ‚Ñì= 2 categories for each response. They model this type of
independent multivariate binary observations, Y1, . . . , Yn, where Yi = (Yi1, . . . , Yim)‚Ä≤, by
introducing latent variables ÀúZih for i = 1, . . . , n and h = 1, . . . , m. In this case,
Yih =
(
1, if ÀúZih > 0
0, if ÀúZih ‚â§0
where
ÀúZi ‚àºN([1m ‚äóx‚Ä≤
i] ÀúŒ≤, Œ£),
and ÀúZi = ( ÀúZi1, . . . , ÀúZim)‚Ä≤, 1m is the m√ó1 vector of ones, xi is a k √ó1 vector of covariates,
ÀúŒ≤ is a k √ó1 vector of regression coefÔ¨Åcients, and Œ£ is an m√óm correlation matrix. Again,
conditional on Œ≤ and Œ£, the ÀúZis are independent. That is,
vec( ÀúZ) ‚â°
Ô£Æ
Ô£ØÔ£∞
ÀúZ1
...
ÀúZn
Ô£π
Ô£∫Ô£ª‚àºN(X ÀúŒ≤, œÉ2I ‚äóŒ£),
where
X ‚â°
Ô£Æ
Ô£ØÔ£∞
1m ‚äóx‚Ä≤
1
...
1m ‚äóx‚Ä≤
n
Ô£π
Ô£∫Ô£ª
is a nm √ó k matrix. For ÀúŒ≤ to be identiÔ¨Åable, Chib and Greenberg require Œ£ to be a
correlation matrix and set œÉ2 = 1. More recently, to facilitate model Ô¨Åtting, Liu and Daniels
33

(2006) relax the assumptions on the correlation matrix by using parameter expansion and
reparameterization methods to Ô¨Årst sample Œ£ as a covariance matrix and then translate it
back to a correlation matrix.
2.2
The Bayesian Spatial Probit Regression Model
2.2.1
Model SpeciÔ¨Åcation
We now discuss incorporating spatial dependence into the Bayesian probit regression
model. From the extensions considered in the previous section, this can be done in two
ways. The Ô¨Årst way to view this model extension is to consider observations at the different
locations as n dependent observations (rather than n independent observations as in Sec-
tion 2.1.1), so that we have n > 1 ‚Äòindividuals‚Äô (or locations in the spatial setting), m = 1
‚Äòresponses‚Äô at each location, and ‚Ñì= 2 possible categories for each observation. We could
also view the observations across all locations as a single multivariate response variable
(i.e, n = 1 individuals with m > 1 responses which can take on ‚Ñì= 2 possible categories).
Although the resulting models are equivalent, we arbitrarily take the Ô¨Årst view in deÔ¨Ån-
ing our notation, and consider n spatially-dependent univariate binary response variables,
Y1, . . . , Yn. The resulting model is equivalent to De Oliveira (2000)‚Äôs ‚Äúclipped Gaussian
random Ô¨Åelds‚Äù and the basis for the model considered by Weir and Pettitt (2000).
For the spatial probit regression model, we introduce latent variables ÀúZ = ( ÀúZ1, . . . , ÀúZn)‚Ä≤,
which are realizations of a spatially-dependent Gaussian process, and let
Yi =
(
1, if ÀúZi > 0
0, if ÀúZi ‚â§0
(2.5)
where
ÀúZ ‚àºN(X ÀúŒ≤, œÉ2Œ£(Œ∏)),
(2.6)
34

with X = (x1, . . . , xn)‚Ä≤, and we assume that the marginal variances of the Zis are known
up to a multiplicative constant œÉ2 and that the matrix Œ£(Œ∏) captures the residual spatial
dependence structure. Typically, Œ£(Œ∏) will be a correlation matrix. However, we allow
for the possibility of heteroskedasticity by only assuming that the diagonal elements of
Œ£(Œ∏) are Ô¨Åxed and known constants. We note that in Chapter 1, Œ£(Œ∏) represented a spatial
covariance matrix. In our discussion of the Bayesian spatial probit regression model, we
will refer to œÉ2 as the variance of Zi for i = 1, . . . , n, and Œ£(Œ∏) as a spatial correlation
matrix.
As in the two previous model extensions, ÀúŒ≤ is only identiÔ¨Åable up to a multiplicative
constant. For illustration, consider P(Yi = 1| ÀúŒ≤, Œ£(Œ∏), œÉ2):
P

Yi = 1| ÀúŒ≤, Œ£(Œ∏), œÉ2
=
P

ÀúZi > 0| ÀúŒ≤, Œ£(Œ∏), œÉ2
=
Œ¶
 
x‚Ä≤
i ÀúŒ≤
œÉ
!
=
Œ¶ (x‚Ä≤
iŒ≤) ,
where Œ≤ = ÀúŒ≤/œÉ is the identiÔ¨Åable parameter. De Oliveira (2000) and Weir and Pettitt
(2000) chose to set œÉ2 = 1 to ensure that the regression coefÔ¨Åcients are identiÔ¨Åable, a
choice that has implications for the efÔ¨Åciency of the resulting MCMC algorithms as we
discuss Section 3.1.
Although similar in spirit to that of the spatial GLM and spatial GLMM, this model
Ô¨Åts within a separate class under the GLM framework. For the spatial GLM, the spatial
dependence is speciÔ¨Åed directly on Y , rather than on a latent Gaussian process. The spatial
GLMM is more similar to the Bayesian probit regression model in that we introduce a
latent spatially-dependent Gaussian process in the mean of Y ; however, the probability pi
35

is speciÔ¨Åed conditional on the value of the Gaussian process, Si, i.e.,
pi = P(Yi = 1|Œ≤, Si)
for all i = 1, . . . , n. In contrast to the spatial GLM and the spatial GLMM, the spatial
dependence structure is embedded in the link function.
2.2.2
Parameterization of the Spatial Correlation Matrix
When modeling spatial dependence, it is important to make certain that Œ£(Œ∏) is a valid
correlation matrix. Various parameterizations of Œ£(Œ∏) that ensure validity and uphold the
characteristics of spatial dependence are available. We consider two classes of parameteri-
zations for Œ£(Œ∏), corresponding to geostatistical and lattice data.
For geostatistical/point-referenced data, a geostatistical dependence structure is com-
monly used. In this setting, the correlation of a spatial process at two locations is often
modeled as a function of the distance between the two locations corresponding to the as-
sumptions of second-order stationarity and isotropy. One popular class of parametric spa-
tial correlation functions is the Mat¬¥ern class. In this case,
Œ£(Œ∏) ‚â°Œ£(ŒΩ, Œª),
where the ijth element of Œ£(ŒΩ, Œª) is equal to
1
2ŒΩ‚àí1Œì(ŒΩ)
2‚àöŒΩdij
Œª
ŒΩ
KŒΩ
2‚àöŒΩdij
Œª

,
Œì(¬∑) is the usual gamma function, KŒΩ is the modiÔ¨Åed Bessel function of order ŒΩ (see e.g.,
Abramowitz and Stegun, 1965), dij = ||si‚àísj|| is the Euclidean distance between locations
si and sj, ŒΩ > 0 is a parameter controlling for smoothness of the realized random Ô¨Åeld,
and Œª > 0 is the spatial scale parameter. Special cases of the Mat¬¥ern correlation functions
36

include the exponential (ŒΩ = 1/2) and the Gaussian (ŒΩ ‚Üí‚àû) correlation functions. Both
of these correlation functions can be written in a simpler form, i.e.,
Œ£(Œ∏) ‚â°Œ£(Œª).
For the exponential correlation function, the ijth element of Œ£(Œª) is equal to
exp

‚àídij
Œª

,
(2.7)
and for the Gaussian correlation function, the ijth element of Œ£(Œª) is equal to
exp

‚àíd2
ij
Œª2

,
(2.8)
where dij and Œª are as deÔ¨Åned above. There are other parametric correlation functions for
geostatistical data, and we refer the reader to Cressie (1993), Stein (1999), and Banerjee
et al. (2004) for further examples.
For lattice/gridded data, spatial dependence is modeled by considering spatial neigh-
borhood structures. Neighborhood structures deÔ¨Åne a set of neighbors for each partition
of a domain D indexed by locations {si, . . . , sn}. Figure 2.1 illustrates this concept for
a regular lattice/grid. In this Ô¨Ågure, grid cell with the black dot represents the location
of interest and its neighbors are represented by the grid cells with empty squares. Figure
2.1 (a) illustrates a Ô¨Årst order neighborhood structure, where the ith and jth grid cells are
neighbors if they share a common edge, and (b) illustrates a second order neighborhood
structure, where the ith and jth grid cells are neighbors if they share a common edge or
corner.
A spatial autoregressive structure is commonly used to capture spatial dependence in
models for lattice/gridded data. One example of an autoregressive dependence structure
37

(a)
(b)
First Order Neighborhood Structure
Second Order Neighborhood Structure
Figure 2.1: Illustration of neighborhood structures for a regular grid. The grid cell with the
black dot represents the location of interest and its neighbors are represented by the grid
cells with the empty squares for (a) a Ô¨Årst order neighborhood structure and (b) a second
order neighborhood structure.
is the conditionally autoregressive (CAR) model (e.g., Banerjee et al., 2004). In the CAR
model,
Œ£(Œ∏) ‚â°Œ£(œÅ) = (Dw ‚àíœÅW )‚àí1
(2.9)
where the ijth element of W is equal to wij, wij is equal to 1 if grid cell/partition i is a
neighbor of cell/partition j and is equal to 0 if cells i and j are not neighbors, Dw is a
diagonal matrix with the ith diagonal element equal to wi+ = Pn
j=1 wij, œÉ2 is the variance,
and œÅ is the spatial dependence parameter. Other autoregressive dependence structures in-
clude the simultaneous autoregressive (SAR) model and the spatial moving average (SMA)
model (see, e.g., Cressie, 1993).
38

CHAPTER 3
DATA AUGMENTATION MCMC STRATEGIES
There has been a recent emphasis in the spatial statistics literature on the development
of methods that accommodate large data sets. In the Bayesian setting, these methods in-
clude dimension reduction techniques (e.g., Higdon, 2002; Xu et al., 2005; Calder, 2007;
Banerjee et al., 2008), integrated nested Laplacian approximations (Rue et al., 2009), and
covariance tapering (see recent work by Shaby and Ruppert, 2010, for a Bayesian treatment
of this technique). In this chapter, instead of focusing on model adjustments to accommo-
date large data sets or approximations to full Bayesian inference procedures, we investi-
gate strategies for efÔ¨Åcient Markov chain Monte Carlo (MCMC) algorithms for Ô¨Åtting the
Bayesian spatial probit regression model. While the MCMC strategies discussed here are
not necessarily designed to overcome computational challenges associated with massive
data sets, in high dimensional settings having efÔ¨Åcient algorithms is clearly desirable.
Data augmentation/latent variable methods have been widely recognized for facilitating
model Ô¨Åtting of the Bayesian probit regression model. As discussed in Section 2.1.1, the
latent variable representation of the Bayesian probit regression model proposed by Albert
and Chib (1993) allows model Ô¨Åtting to be performed using a simple Gibbs sampler. To im-
prove the efÔ¨Åciency of the Gibbs sampler in this setting, Imai and van Dyk (2005) propose
introducing a working parameter (deÔ¨Åned in Section 3.1.1) into the model and compare
39

various data augmentation strategies resulting from different treatments of the working pa-
rameter. In this chapter, we build on this work by investigating the efÔ¨Åciency of modiÔ¨Åed
and extended versions of these algorithms for the spatial probit regression model, focus-
ing on the special case of binary response variables. These algorithms include the one
previously proposed by De Oliveira (2000), which we discuss further in Section 3.1.1.
In Section 2.2, we deÔ¨Åned the latent variable representation of the Bayesian probit
regression model for spatially-referenced binary data. However, as noted in Section 2.2, the
spatial variance parameter, œÉ2, is not identiÔ¨Åable. In Section 3.1.1, we discuss conditional
and marginal data augmentation strategies for making use of this non-identiÔ¨Åable variance
parameter within an MCMC model-Ô¨Åtting algorithm. We propose three different Gibbs
sampling algorithms corresponding to these strategies. Furthermore, in Section 3.1.2, we
propose modiÔ¨Åcations to these algorithms by partially collapsing over the sampling steps
within the Gibbs sampler. We compare the various resulting algorithms using a simulation
study in Section 3.2 and in an analysis of satellite-derived land cover data over Southeast
Asia in Section 3.3.
3.1
Data Augmentation MCMC Strategies
3.1.1
Conditional versus Marginal Data Augmentation
In the previous chapter, ‚Äúdata augmentation‚Äù referred to the introduction of continuous
latent variables, ÀúZ, in the Albert and Chib (1993) representation of the Bayesian probit
regression model. In this section, we extend our use of ‚Äúdata augmentation‚Äù to include
conditional and marginal data augmentation MCMC strategies, where we use a working pa-
rameter to identify fast and easily implemented algorithms. In data augmentation MCMC
40

strategies, the working parameter is frequently taken to be a parameter that is not identiÔ¨Å-
able under the observed data, Y , but is identiÔ¨Åable under the complete or augmented data,
(Y , Z). In the Bayesian spatial probit regression model, the spatial variance parameter, œÉ2,
can serve as a working parameter.
Meng and van Dyk (1999) and van Dyk and Meng (2001) were the Ô¨Årst to distinguish
between conditional augmentation and marginal augmentation strategies. Under a condi-
tional augmentation strategy, the working parameter is Ô¨Åxed to an optimal constant within
the model-Ô¨Åtting algorithm, whereas a marginal augmentation strategy seeks to marginalize
over the working parameter within the algorithm. Meng and van Dyk (1999) show that al-
gorithms using a marginal augmentation strategy will have a geometric rate of convergence
no larger than its conditional augmentation counterpart.
Below we describe conditional and marginal augmentation strategies for the Bayesian
spatial probit regression model deÔ¨Åned in Section 2.2. Referring back to the notation in
Section 2.2, we use ÀúZ and ÀúŒ≤ to denote the non-identiÔ¨Åable parameters, and Z and Œ≤ to
denote the identiÔ¨Åable parameters (i.e., ÀúZ = œÉZ and ÀúŒ≤ = œÉŒ≤).
Consider the likelihood function for the identiÔ¨Åable parameters (Œ≤, Œ∏) in the spatial
probit regression model introduced in Section 2.2:
L(Œ≤, Œ∏|Y ) ‚àùp(Y |Œ≤, Œ∏)
=
Z
p(Y , Z|Œ≤, Œ∏)dZ,
(3.1)
where (Y , Z) denotes the complete augmented data. As Imai and van Dyk (2005) point
out for an independent multi-category response version of the probit regression model,
since the working parameter is not identiÔ¨Åable under the observed data likelihood function,
we can condition on a Ô¨Åxed value of the working parameter and the likelihood function
41

will remain unchanged. This conditional augmentation strategy also holds for the spatial
version of the model.
Fixing the working parameter œÉ2 to some constant œÉ2
0 results in a conditional augmen-
tation algorithm, and without loss of generality, we can take œÉ2
0 = 1.2 Thus, for conditional
augmentation, (3.1) becomes
L(Œ≤, Œ∏|Y ) = L(Œ≤, Œ∏, œÉ2|Y )
‚àù
Z
p(Y , Z|Œ≤, Œ∏, œÉ2 = œÉ2
0)dZ
=
Z
A1
¬∑ ¬∑ ¬∑
Z
An
1
(2œÄœÉ2
0)n/2|Œ£(Œ∏)|1/2
√ó exp

‚àí1
2œÉ2
0
(Z ‚àíXŒ≤)‚Ä≤Œ£(Œ∏)‚àí1(Z ‚àíXŒ≤)

dZ
(3.2)
where
Ai =
(
(‚àí‚àû, 0], if Yi = 0
(0, ‚àû), if Yi = 1
.
(3.3)
Using this conditional augmentation strategy, the associated Gibbs sampling algorithm
for sampling from the posterior distribution of (Œ≤, Œ∏) is given in Table 3.1 under the Con-
ditional heading under the Non-Collapsed Algorithms heading.
As an alternative to conditioning on a speciÔ¨Åc value of the working parameter, the
working parameter can be assigned a proper prior which we can marginalize over to obtain
the likelihood of the identiÔ¨Åable parameters. Using this marginal augmentation strategy,
(3.1) can be expressed as
L(Œ≤, Œ∏|Y ) ‚àù
Z 
p(Y , Z|Œ≤, Œ∏, œÉ2)œÄ(œÉ2|Œ≤, Œ∏)dœÉ2
dZ
=
Z
A1
¬∑ ¬∑ ¬∑
Z
An
Z ‚àû
0
1
(2œÄœÉ2)n/2|Œ£(Œ∏)|1/2
√ó exp

‚àí1
2œÉ2(Z ‚àíXŒ≤)‚Ä≤Œ£(Œ∏)‚àí1(Z ‚àíXŒ≤)

œÄ(œÉ2|Œ≤, Œ∏)dœÉ2

dZ
(3.4)
2De Oliveira (2000) implicitly uses this conditional augmentation strategy in his model for spatially-
dependent binary data.
42

where the Ai are as deÔ¨Åned in (3.3).
Following Imai and van Dyk (2005), we consider two marginal data augmentation
schemes. In the Ô¨Årst scheme, labeled Scheme 1, we marginalize over œÉ2 completely in
updating Z. We do this by sampling (œÉ2)‚àófrom its prior distribution œÄ(œÉ2|Œ≤, Œ∏) ‚â°œÄ(œÉ2),
sampling from ÀúZ|Y , Œ≤, Œ∏, (œÉ2)‚àó, and ‚Äúsweeping‚Äù over œÉ2 by setting Z = ÀúZ/œÉ‚àó. Thus, the
sampled Z is dependent on the identiÔ¨Åable parameter Œ≤, but not on the non-identiÔ¨Åable
parameter ÀúŒ≤. This approach is valid since œÉ2 is not likelihood identiÔ¨Åable, therefore we
can sample ÀúZ conditional on any plausible value of œÉ2. We then sample from the joint
full conditional distribution of (œÉ2, ÀúŒ≤) and from the full conditional distribution of Œ∏ and
again ‚Äúsweep‚Äù over the sampled value of œÉ2 by setting Œ≤ = ÀúŒ≤/œÉ. The Gibbs sampling algo-
rithm associated with this marginal augmentation scheme is listed in Table 3.1 as Marginal-
Scheme 1 in the Non-Collapsed Algorithms section.
In the second marginal augmentation scheme, labeled Scheme 2, we include œÉ2 in the
Gibbs sampler in the usual way by assigning it a proper prior distribution and sampling
from its full conditional distribution. Then, we sweep over œÉ by properly normalizing the
non-identiÔ¨Åable parameters in each iteration of the algorithm (i.e., set Z = ÀúZ/œÉ and Œ≤ =
ÀúŒ≤/œÉ). This algorithm is listed in Table 3.1 as Marginal-Scheme 2 of the Non-Collapsed
Algorithms.
In Marginal-Scheme 1, when the prior distribution on œÉ2 is diffuse, conditioning on a
value of œÉ2 from the prior distribution when sampling ÀúZ will allow the distribution of ÀúZ
to be more diffuse than when conditioning on the value of œÉ2 obtained from the previous
iteration of the algorithm, as in Marginal-Scheme 2. Thus, the sampled values of ÀúZ/œÉ‚àó=
Z when using Marginal-Scheme 1 will have a smaller autocorrelation. In turn, we expect
a similar decrease in autocorrelation in the sample paths of Œ≤ and Œ∏.
43

Imai and van Dyk (2005) consider similar conditional and marginal augmentation strate-
gies for Ô¨Åtting the Bayesian multinomial probit regression model for independent multi-
category response variables (see Section 2.1.2). However, in their model, the working
parameter is the Ô¨Årst diagonal element of the cross-category covariance matrix, Àú‚Ñ¶1,1. Com-
paring their algorithms to that of McCulloch and Rossi (1994) and Nobile (1998), they Ô¨Ånd
that their marginal augmentation algorithms converge more quickly and are less sensitive
to starting values. In the following sections, through a simulation study and data analysis,
we show similar beneÔ¨Åts for MCMC algorithms based on marginal data augmentation for
the spatial probit regression model.
Non-Collapsed Algorithms
Marginal-Scheme 1
Marginal-Scheme 2
Conditional
Step 1:
Sample (œÉ2)‚àó‚àºœÄ(œÉ2)
Sample ÀúZ|Y , Œ≤, Œ∏, (œÉ2)‚àó
Sample ÀúZ|Y , Œ≤, Œ∏, œÉ2
Sample Z|Y , Œ≤, Œ∏, œÉ2 = 1
Set Z = ÀúZ/œÉ‚àó
Set Z = ÀúZ/œÉ
Step 2:
Sample (œÉ2, ÀúŒ≤)| ÀúZ, Y , Œ∏
Sample (œÉ2, ÀúŒ≤)| ÀúZ, Y , Œ∏
Sample Œ≤| ÀúZ, Y , Œ∏, œÉ2 = 1
Set Œ≤ = ÀúŒ≤/œÉ
Set Œ≤ = ÀúŒ≤/œÉ
Step 3:
Sample Œ∏| ÀúZ, Y , ÀúŒ≤, œÉ2
Sample Œ∏| ÀúZ, Y , ÀúŒ≤, œÉ2
Sample Œ∏|Z, Y , Œ≤, œÉ2 = 1
Partially Collapsed Algorithms
Marginal-Scheme 1
Marginal-Scheme 2
Conditional
Step 1:
Sample Œ∏|Y , Œ≤
Sample Œ∏|Y , Œ≤, œÉ2
Sample Œ∏|Y , Œ≤, œÉ2 = 1
Sample (œÉ2)‚àó‚àºœÄ(œÉ2)
Sample ÀúZ|Y , Œ≤, Œ∏, (œÉ2)‚àó
Sample ÀúZ|Y , Œ≤, Œ∏, œÉ2
Sample Z|Y , Œ≤, Œ∏, œÉ2 = 1
Set Z = ÀúZ/œÉ‚àó
Set Z = ÀúZ/œÉ
Step 2:
Sample (œÉ2, ÀúŒ≤)| ÀúZ, Y , Œ∏
Sample (œÉ2, ÀúŒ≤)| ÀúZ, Y , Œ∏
Sample Œ≤| ÀúZ, Y , Œ∏, œÉ2 = 1
Set Œ≤ = ÀúŒ≤/œÉ
Set Œ≤ = ÀúŒ≤/œÉ
Table 3.1: This table lists the steps in each of the data augmentation algorithms. The Ô¨Årst
portion shows the non-collapsed data augmentation algorithms introduced in Section 3.1.1.
The second portion shows the partially collapsed data augmentation algorithms introduced
in Section 3.1.2.
44

3.1.2
Partially Collapsed Algorithms
In this section, we discuss a method for collapsing Steps 1 and 3 in the algorithms
introduced in the previous section. In Step 3 of each algorithm, we draw samples of Œ∏ from
Œ∏| ÀúZ, Y , ÀúŒ≤, œÉ2. Because ÀúZ is a vector of real-valued random variables, by conditioning
on it (as opposed to the binary vector Y ) we unnecessarily constrain the distribution of
values that Œ∏ can take at each iteration of the algorithm, particularly for high dimensional
ÀúZ. Instead of sampling from Œ∏| ÀúZ, Y , ÀúŒ≤, œÉ2, we could marginalize over ÀúZ and sample from
Œ∏|Y , ÀúŒ≤, œÉ2. The latter distribution will be more diffuse than the former, and thus intuitively
we might expect improvements in the efÔ¨Åciency of the algorithm.
Consider the joint full conditional distribution Œ∏ and ÀúZ, where
p(Œ∏, ÀúZ|Y , ÀúŒ≤, œÉ2) ‚àùp(Y , ÀúZ| ÀúŒ≤, œÉ2, Œ∏)œÄ(Œ∏).
(3.5)
Since the left hand side of (3.5) can be decomposed into the product of p(Œ∏|Y , ÀúŒ≤, œÉ2) and
p( ÀúZ|Y , ÀúŒ≤, œÉ2, Œ∏), Steps 1 and 3 can be collapsed into a single step where we sample from
Œ∏|Y , ÀúŒ≤, œÉ2 and then from ÀúZ|Y , ÀúŒ≤, œÉ2, Œ∏. From (3.5),
p(Œ∏|Y , ÀúŒ≤, œÉ2) ‚àù
Z
A1
¬∑ ¬∑ ¬∑
Z
An
p(Y , ÀúZ| ÀúŒ≤, œÉ2, Œ∏)d ÀúZ

œÄ(Œ∏),
(3.6)
where the quantity in square brackets is simply the volume under the n-dimensional multi-
variate normal density function corresponding to the orthant of Rn deÔ¨Åned by A1√ó¬∑ ¬∑ ¬∑√óAn.
Thus, a Metropolis-Hastings step can be used to sample from Œ∏|Y , ÀúŒ≤, œÉ2. Sampling from
ÀúZ|Y , ÀúŒ≤, œÉ, Œ∏ can be done as before. The modiÔ¨Åed versions of the algorithms introduced in
Section 3.1.1 that collapse Steps 1 and 3 are listed in Table 3.1 under the heading Partially
Collapsed Algorithms, following terminology used by van Dyk and Park (2008).
45

Finally, we note that it is straightforward to show that
p(Œ∏|Y , ÀúŒ≤, œÉ2) = p(Œ∏|Y , Œ≤, œÉ2 = 1).
Therefore, in the modiÔ¨Åed Marginal-Scheme 1 algorithm, we do not condition on a partic-
ular value of œÉ2 in sampling Œ∏, thus ensuring that this partially collapsed algorithm samples
from the appropriate posterior distribution (see van Dyk and Park, 2008, for related discus-
sion).
3.1.3
Full Conditional Distributions
Using the priors Œ≤ ‚àºN(0, CŒ≤), œÉ2 ‚àºa0(œá2
v0)‚àí1, and Œ∏ ‚àºœÄ(Œ∏), we obtain the fol-
lowing full conditional distributions. We use the superscript [t] to denote the value of a
parameter at the tth iteration of the algorithm.
Non-collapsed Algorithms
Step 1: Sample Z[t] from Z|Y , Œ≤[t‚àí1], Œ∏[t‚àí1], (œÉ2)‚àó:
Each algorithm treats œÉ2 differently, so that for Marginal-Scheme 1 (œÉ2)‚àó‚àºœÄ(œÉ2),
for Marginal-Scheme 2 (œÉ2)‚àó= (œÉ2)[t‚àí1], and for Conditional (œÉ2)‚àó= 1.
For i = 1, . . . , n, deÔ¨Åne Z‚àó
¬¨i = (Z[t]
1 , . . . , Z[t]
i‚àí1, Z[t‚àí1]
i‚àí1 , . . . , Z[t‚àí1]
n
)‚Ä≤ and sample ÀúZi
from
ÀúZi|Y , Z‚àó
¬¨i, Œ≤[t‚àí1], Œ∏[t‚àí1], (œÉ2)‚àó‚àº
(
TN(¬µÀúzi, œÑ 2
Àúzi, 0, ‚àû),
if Yi = 1
TN(¬µÀúzi, œÑ 2
Àúzi, ‚àí‚àû, 0),
if Yi = 0 ,
where TN(¬µÀúzi, œÑ 2
Àúzi, ‚Ñì, u) is a truncated normal distribution with lower and upper bounds
‚Ñìand u, respectively, and mean and variance
¬µÀúzi = x‚Ä≤
i œÉ‚àóŒ≤[t‚àí1] +

Œ£(Œ∏[t‚àí1])

i,¬¨i

Œ£(Œ∏[t‚àí1])

¬¨i,¬¨i
‚àí1  œÉ‚àóZ‚àó
¬¨i ‚àíX¬¨i œÉ‚àóŒ≤[t‚àí1]
œÑ 2
Àúzi = (œÉ2)‚àó

Œ£(Œ∏[t‚àí1])

i,i ‚àí

Œ£(Œ∏[t‚àí1])

i,¬¨i

Œ£(Œ∏[t‚àí1])

¬¨i,¬¨i
‚àí1 
Œ£(Œ∏[t‚àí1])

¬¨i,i

.
46

Set Z[t]
i = ÀúZi/œÉ‚àó.
Step 2: Sample (œÉ2)[t], Œ≤[t] from œÉ2, Œ≤|Y , Z[t], Œ∏[t‚àí1]:
For Marginal-Scheme 1 and Marginal-Scheme 2
(œÉ2)[t] ‚àº

( ÀúZ ‚àíX ÀÜŒ≤)‚Ä≤ Œ£(Œ∏[t‚àí1])‚àí1 ( ÀúZ ‚àíX ÀÜŒ≤) + a2
0 + ÀÜŒ≤‚Ä≤ C‚àí1
Œ≤
ÀÜŒ≤

(œá2
n+v0)‚àí1
where ÀÜŒ≤ =
 X‚Ä≤ Œ£(Œ∏[t‚àí1])‚àí1 X + C‚àí1
Œ≤
‚àí1 X‚Ä≤ Œ£(Œ∏[t‚àí1])‚àí1 ÀúZ and ÀúZ is taken from
Step 1.
For Conditional, set (œÉ2)[t] = 1.
Then, for all algorithms, sample
ÀúŒ≤ ‚àºN( ÀÜŒ≤, (œÉ2)[t]  X‚Ä≤Œ£(Œ∏[t‚àí1])‚àí1X + C‚àí1
Œ≤
‚àí1)
and set Œ≤[t] = ÀúŒ≤/œÉ[t].
Step 3: Sample Œ∏[t] from Œ∏|Y , Z[t], Œ≤[t], (œÉ2)[t] via a Metropolis-Hastings step:
Sample a proposed value, Œ∏‚àó, from a proposal distribution q(Œ∏|Œ∏[t‚àí1]). Take
Œ∏[t] =
(
Œ∏‚àó,
with probability c(Œ∏[t‚àí1], Œ∏‚àó)
Œ∏[t‚àí1],
with probability 1 ‚àíc(Œ∏[t‚àí1], Œ∏‚àó) ,
where
c(Œ∏[t‚àí1], Œ∏‚àó) = min
(
œÄ(Œ∏‚àó|Y , ÀúZ, ÀúŒ≤, (œÉ2)[t])
œÄ(Œ∏[t‚àí1]|Y , ÀúZ, ÀúŒ≤, (œÉ2)[t])
q(Œ∏[t‚àí1]|Œ∏‚àó)
q(Œ∏‚àó|Œ∏[t‚àí1]), 1
)
.
In the acceptance probability,
œÄ(Œ∏|Y , ÀúZ, ÀúŒ≤, (œÉ2)[t]) ‚àùp(Y , ÀúZ| ÀúŒ≤, Œ∏, (œÉ2)[t]) œÄ(Œ∏) = œÜ( ÀúZ; X ÀúŒ≤, (œÉ2)[t]Œ£(Œ∏)) œÄ(Œ∏),
where œÜ( ÀúZ; X ÀúŒ≤, œÉ2Œ£(Œ∏)) is the multivariate normal probability density function
with mean X ÀúŒ≤ and variance œÉ2Œ£(Œ∏) evaluated at ÀúZ, and ÀúZ is taken from Step 1
and ÀúŒ≤ is taken from Step 2.
47

Partially Collapsed Algorithms
Step 1: Sample Œ∏[t], Z[t] from Œ∏, Z|Y , Œ≤[t‚àí1], (œÉ2)‚àó:
‚Ä¢ Sample Œ∏[t] from Œ∏|Y , Œ≤[t‚àí1] via a Metropolis-Hastings step.
Sample a proposed value, Œ∏‚àó, from a proposal distribution q(Œ∏|Œ∏[t‚àí1]). Take
Œ∏[t] =
(
Œ∏‚àó,
with probability c(Œ∏[t‚àí1], Œ∏‚àó)
Œ∏[t‚àí1],
with probability 1 ‚àíc(Œ∏[t‚àí1], Œ∏‚àó)
where
c(Œ∏[t‚àí1], Œ∏‚àó) = min
 œÄ(Œ∏‚àó|Y , Œ≤[t‚àí1])
œÄ(Œ∏[t‚àí1]|Y , Œ≤[t‚àí1])
q(Œ∏[t‚àí1]|Œ∏‚àó)
q(Œ∏‚àó|Œ∏[t‚àí1]), 1

.
In the acceptance probability,
œÄ(Œ∏|Y , Œ≤[t‚àí1]) ‚àùp(Y |Œ≤[t‚àí1], Œ∏) œÄ(Œ∏)
=
Z
A1
¬∑ ¬∑ ¬∑
Z
An
œÜ(Z; XŒ≤[t‚àí1], Œ£(Œ∏)) dZ œÄ(Œ∏),
where œÜ(Z; XŒ≤, Œ£(Œ∏)) is the multivariate normal probability density function
with mean XŒ≤ and variance Œ£(Œ∏) evaluated at Z and the Ai are as deÔ¨Åned in
(3.3).
‚Ä¢ Sample Z[t]|Y , Œ≤[t‚àí1], Œ∏[t], (œÉ2)‚àó.
Each algorithm treats œÉ2 differently, so that for Marginal-Scheme 1 (œÉ2)‚àó‚àº
œÄ(œÉ2), for Marginal-Scheme 2 (œÉ2)‚àó= (œÉ2)[t‚àí1], and for Conditional (œÉ2)‚àó=
1.
For i = 1, . . . , n, deÔ¨Åne Z‚àó
¬¨i = (Z[t]
1 , . . . , Z[t]
i‚àí1, Z[t‚àí1]
i‚àí1 , . . . , Z[t‚àí1]
n
)‚Ä≤ and sample
ÀúZi from
ÀúZi|Y , Z‚àó
¬¨i, Œ≤[t‚àí1], Œ∏[t], (œÉ2)‚àó‚àº
(
TN(¬µÀúzi, œÑ 2
Àúzi, 0, ‚àû),
if Yi = 1
TN(¬µÀúzi, œÑ 2
Àúzi, ‚àí‚àû, 0),
if Yi = 0 ,
48

where TN(¬µÀúzi, œÑ 2
Àúzi, ‚Ñì, u) is a truncated normal distribution with lower and upper
bounds ‚Ñìand u, respectively, and mean and variance
¬µÀúzi = x‚Ä≤
i œÉ‚àóŒ≤[t‚àí1] +
h
Œ£(Œ∏[t])
i
i,¬¨i
h
Œ£(Œ∏[t])
i
¬¨i,¬¨i
‚àí1 
œÉ‚àóZ‚àó
¬¨i ‚àíX¬¨i œÉ‚àóŒ≤[t‚àí1]
œÑ 2
Àúzi = (œÉ2)‚àó
 h
Œ£(Œ∏[t])
i
i,i ‚àí
h
Œ£(Œ∏[t])
i
i,¬¨i
h
Œ£(Œ∏[t])
i
¬¨i,¬¨i
‚àí1 h
Œ£(Œ∏[t])
i
¬¨i,i
!
.
Set Z[t]
i = ÀúZi/œÉ‚àó.
Step 2: Sample (œÉ2)[t], Œ≤[t] from œÉ2, Œ≤|Y , Z[t], Œ∏[t]:
For Marginal-Scheme 1 and Marginal-Scheme 2
(œÉ2)[t] ‚àº

( ÀúZ ‚àíX ÀÜŒ≤)‚Ä≤ Œ£(Œ∏[t])‚àí1 ( ÀúZ ‚àíX ÀÜŒ≤) + a2
0 + ÀÜŒ≤‚Ä≤ C‚àí1
Œ≤
ÀÜŒ≤

(œá2
n+v0)‚àí1
where ÀÜŒ≤ =
 X‚Ä≤ Œ£(Œ∏[t])‚àí1 X + C‚àí1
Œ≤
‚àí1 X‚Ä≤ Œ£(Œ∏[t])‚àí1 ÀúZ, and ÀúZ is taken from Step
1.
For Conditional, set (œÉ2)[t] = 1.
Then, for all algorithms, sample
ÀúŒ≤ ‚àºN( ÀÜŒ≤, (œÉ2)[t]  X‚Ä≤Œ£(Œ∏[t])‚àí1X + C‚àí1
Œ≤
‚àí1)
and set Œ≤[t] = ÀúŒ≤/œÉ[t].
3.2
Simulation Study
3.2.1
Simulation Set-up
In our simulation study, we compare each of the six proposed algorithms in terms of
computational efÔ¨Åciency and sensitivity to starting values. We consider data sets of sample
size n = 100 corresponding to observations on a 10 √ó 10 regular grid. We generate our
49

data from the spatial probit regression model given in (2.5) and (2.6) with a single covari-
ate xi = xi and regression coefÔ¨Åcient Œ≤ = Œ≤. We consider a geostatistical dependence
structure based on an exponential covariance function as deÔ¨Åned in (2.7), as well as on an
autoregressive structure based on the CAR model as deÔ¨Åned in (2.9). We also consider an
independent covariance structure (as in Section 2.1.1), i.e., Œ£(Œ∏) ‚â°In, to use as a baseline
for comparison.
When Ô¨Åtting the independent model, we use the three data augmentation algorithms,
without Ô¨Åtting the spatial dependence parameter. We note that the issue of collapsing these
algorithms is not relevant in this case. These algorithms are identical to those used by
Imai and van Dyk (2005) for the binary response special case of the multi-category probit
regression model. We show how adding spatial dependence to the model can impact the
convergence of the algorithms.
Under the three dependence structures, we consider both the non-collapsed and partially
collapsed algorithms introduced in Section 3.1 and deÔ¨Åne the Ô¨Åve scenarios listed in Table
3.2. For each scenario, we compare the algorithms resulting from the various conditional
and marginal data augmentation strategies (i.e., Marginal-Scheme 1, Marginal-Scheme 2,
and Conditional).
Scenario
Spatial Dependence Structure
Partially Collapsed
1
CAR
No
2
CAR
Yes
3
Geostatistical
No
4
Geostatistical
Yes
5
Independent
‚Äì
Table 3.2: Scenarios used to compare the marginal and conditional data augmentation al-
gorithms.
50

In assigning prior distributions, the resulting models should be the same across all al-
gorithms. Thus, where applicable, we assign priors on identiÔ¨Åable parameters (i.e., on Œ≤
rather than ÀúŒ≤). Furthermore, because of the non-identiÔ¨Åability within our model, it is im-
portant that the prior distributions on the parameters are proper. For Scenarios 1-5, we
assign a normal prior distribution to Œ≤ (i.e., Œ≤ ‚àºN(mŒ≤, CŒ≤) with mŒ≤ = 0 and CŒ≤ = 100).
Each algorithm uses the non-identiÔ¨Åable working parameter, œÉ, differently. For the condi-
tional augmentation algorithm, œÉ is Ô¨Åxed at 1. For both marginal augmentation algorithms,
œÉ2 ‚àºaœÉ(œá2
vœÉ)‚àí1 where aœÉ = 3 and (œá2
vœÉ)‚àí1 represents an inverse chi-squared distribution
with parameter vœÉ = 3. The spatial dependence structures have different parameterizations
necessitating the need for different prior distributions on the spatial dependence parame-
ters. For the CAR spatial dependence structure, œÅ ‚àºUnif(1/Œæ(1), 1/Œæ(n)), where Œæ(1) and
Œæ(n) are the smallest and largest eigenvalues of D‚àí1/2
w
W D‚àí1/2
w
(Banerjee et al., 2004, pg.
80). For the geostatistical dependence structure, Œª ‚àºUnif(lŒª, uŒª) with lŒª = 0 and uŒª = 20.
In our simulation study, we generate data sets following the simulation example used
in Nobile (1998) and Imai and van Dyk (2005) for independent binary data, including spa-
tial dependence where appropriate: we independently generate the covariates xi from the
uniform distribution on the interval (-.5, .5); take Œ≤ = ‚àí
‚àö
2, œÉ2 = 1, and ÀúŒ≤ = œÉŒ≤; and set
the spatial dependence parameters œÅ = .9 (CAR structure) and Œª = 2 (geostatistical struc-
ture). For each of the data sets generated under the the two spatial dependence structures
(CAR and geostatistical), we Ô¨Åt the corresponding model using both the non-collapsed
and partially collapsed algorithms. For the independence case, we Ô¨Åt the model using the
(non-collapsed) algorithms.
To compare the augmentation algorithms in terms of sensitivity to starting values, we
consider two different starting values for (œÉ, Œ≤), namely (œÉ, Œ≤) = (
‚àö
2, ‚àí
‚àö
2) and (œÉ, Œ≤) =
51

(10, ‚àí2). Each algorithm is run for 50,000 iterations, and we somewhat arbitrarily take the
Ô¨Årst 10,000 iterations to be the burn-in period.
3.2.2
Simulation Results
Using the Ô¨Åve scenarios in Table 3.2, we compare the various algorithms in terms of
mixing, convergence, and sensitivity to starting values. As shown by the histograms in Fig-
ures 3.1 - 3.5, the algorithms result in nearly identical inferences on the posterior distribu-
tions of the identiÔ¨Åable parameters. Thus, with the inferences consistent across algorithms,
the algorithms can be compared in terms of their efÔ¨Åciency.
First, we note the trace plots of Œ≤ and Œª under the conditional algorithm of Scenario
3 shown in Figure 3.3. Here, we see evidence of potential lack of convergence from the
different behavior in the chain near iteration 50,000. On the other hand, there is no indica-
tion of convergence problems in the trace plots of the two Scenario 3 marginal algorithms,
nor in the trace plots for Scenario 5 algorithms ‚Äì the algorithms for the independent probit
regression model ‚Äì as seen in Figure 3.5. This difference provides evidence of additional
difÔ¨Åculties in Ô¨Åtting spatial probit regression models and the need for more efÔ¨Åcient MCMC
algorithms.
Autocorrelation and partial autocorrelation plots of the (post burn-in) sample paths of
model parameters also help in determining whether an MCMC algorithm is mixing well.
Figures 3.6 and 3.8 show the autocorrelation and partial autocorrelation plots for the re-
gression coefÔ¨Åcient and spatial dependence parameter for Scenarios 1 and 3. It is clear that
among the three non-collapsed algorithms, Marginal-Scheme 1 results in the smallest and
most quickly decreasing autocorrelation in both parameters‚Äô paths under Scenario 3. Un-
der Scenario 1, the top row plots show that the sample path of Œ≤ under Marginal-Scheme
52

1 again has the smallest and most quickly decreasing autocorrelation, but the improve-
ment provided by the marginalization strategy is less apparent for the spatial dependence
parameter, œÅ.
We also compare the convergence of the three partially collapsed algorithms using au-
tocorrelation plots. Figures 3.7 and 3.9 show the autocorrelation and partial autocorrelation
plots of the regression coefÔ¨Åcient and the spatial dependence parameter sample paths for
Scenarios 2 and 4. Here we see that, when compared with the other partially collapsed
algorithms, Marginal-Scheme 1 is again superior when we compare the autocorrelation of
the sampled parameter values using each of the three partially-collapsed algorithms. We
might also expect partial collapsing of the algorithms to further improve autocorrelation
summaries. However, for the CAR structure, partially collapsing the algorithms does not
result in improved autocorrelation summaries, as seen by comparing the autocorrelations
of Figure 3.6 and 3.7. On the other hand, for the geostatistical spatial dependence structure,
partially collapsing the algorithms does appear to improve mixing, as seen by comparing
the autocorrelations of Figures 3.8 and 3.9. In this scenario, the most noticeable beneÔ¨Åts of
collapsing appear to be for the conditional algorithms. The differences between the non-
collapsed and partially collapsed marginal algorithms are not nearly as strong. Given the
signiÔ¨Åcant increase in computation time required to run the partially collapsed algorithms
compared with their non-collapsed counterparts (it can take roughly 12 times as long to
generate the same number of posterior samples), partially collapsed marginal augmenta-
tion algorithms appear not to be a worthwhile.
All scenarios showed that Marginal-Scheme 1 was the least sensitive to starting values.
Figures 3.11 - 3.15 show scatter plots of sampled pairs of œÉ versus ÀúŒ≤ for Scenarios 1 - 5
generated under each of the three augmentation algorithms and both starting values. The
53

black dots show the burn-in samples and the colored dots show the draws from the posterior.
Under both starting values, Marginal-Scheme 1 appears to immediately generate samples
from the stationary posterior distribution and both parameters easily move around the entire
parameter space. However, under the second set of starting values, Marginal-Scheme 2
takes longer to converge and the parameters seem to move around the parameter space
more slowly.
Based on our simulation study, we recommend Ô¨Åtting the spatial probit regression
model using the non-collapsed Marginal-Scheme 1 algorithm. This algorithm showed su-
perior mixing and convergence properties compared to the Marginal-Scheme 2 and Condi-
tional algorithms. When computation burden is taken into account, it does not appear that
partially collapsing this algorithm is beneÔ¨Åcial. In the next section, we compare the per-
formance of the non-collapsed Marginal-Scheme 1 and Conditional algorithms in an anal-
ysis of land cover data. These algorithms were selected based on the simulation Ô¨Åndings
(non-collapsed Marginal-Scheme 1) and existing literature (non-collapsed Conditional; for
example, as in De Oliveira, 2000).
3.3
Application
To illustrate our methods, we use a portion of the data described in Section 1.4. For this
analysis, we considered the region bounded by 17‚ó¶to 19‚ó¶N and 98‚ó¶to 100‚ó¶E, which covers
a portion of northwestern Thailand and a small part of Myanmar. Using a 24√ó24 grid over
this region, we collapsed the response variable to two categories, forest and nonforest,
where the land cover response variable associated with each grid cell was taken to be the
most common observed land cover type, where forest was coded as ‚Äú1‚Äù and non-forest was
coded as ‚Äú0‚Äù. We used the covariate distance to the nearest major road in our analysis, and
54

deÔ¨Åned it over the grid to be the median distance for all measurements of this covariate
within a grid cell.
Using the spatial probit regression model deÔ¨Åned by (2.5) and (2.6) with the CAR spa-
tial dependence structure given in (2.9), we model the binary land cover response variable
and the distance to the nearest major road covariate. Unlike the simulation study, here we
include an intercept parameter. We expect that less accessible locations (high distance to
nearest major road) are more likely to be forested.
We Ô¨Åt the model using the non-collapsed Marginal-Scheme 1 and Conditional algo-
rithms deÔ¨Åned in Table 3.1 and compare the two algorithms in terms of mixing based on
the autocorrelation in the sample paths of the model parameters. Each algorithm was run
for 80,000 iterations, and after examining trace plots we decided to discard the Ô¨Årst 10,000
samples as burn-in.
Inferences on the model parameters are nearly identical for both model-Ô¨Åtting algo-
rithms. As expected, the estimate for the regression coefÔ¨Åcient associated with the dis-
tance to nearest major road covariate is positive (95 percent credible interval on Œ≤1 is
(1.687, 3.428)) so that the farther a location is from a major road (i.e., less accessible),
the more likely that location is to be forested. The intercept is not signiÔ¨Åcantly different
from 0 (95 percent credible interval on Œ≤0 is (‚àí1.094, 0.165)), and the 95 percent credible
interval for œÅ is (0.968, 0.999) indicating strong residual spatial dependence.
Rather than showing sample autocorrelation plots as in Section 3.2, to highlight the
differences between the sample autocorrelation summaries, Table 3.3 provides the autocor-
relations for the sample paths of Œ≤1 and œÅ at selected lags. We do not show the autocorrela-
tion values for the sample path of Œ≤0 because they were approximately zero for all lags and
55

both algorithms. Table 3.3 shows that the Marginal-Scheme 1 algorithm outperforms the
Conditional algorithm, exhibiting smaller autocorrelation in the sampled paths of Œ≤1 and œÅ.
Sample Autocorrelations for Œ≤1
Lag
Marginal-Scheme 1
Conditional
1
0.3194
0.3576
2
0.1791
0.2133
3
0.1115
0.1387
4
0.0786
0.1004
5
0.0617
0.0741
10
0.0256
0.0284
Sample Autocorrelations for œÅ
Lag
Marginal-Scheme 1
Conditional
1
0.8514
0.8621
2
0.7303
0.7496
3
0.6319
0.6576
4
0.5515
0.5802
5
0.4849
0.5134
10
0.2589
0.2898
15
0.1447
0.1752
20
0.0830
0.1081
Table 3.3: Autocorrelations of the sample paths of Œ≤1 and œÅ for the land cover data analysis.
3.4
Summary
In this chapter, we extended the algorithms of Imai and van Dyk (2005) to the spatially-
dependent setting and compared the efÔ¨Åciency of three MCMC algorithms via a simulation
study and data analysis. Furthermore, we proposed and compared three additional MCMC
algorithms, called partially-collapsed algorithms, which marginalize over the latent vari-
able when sampling the spatial dependence parameter. In both the simulation study and
56

data analysis, we found that the non-collapsed Marginal-Scheme 1 algorithm was the most
efÔ¨Åcient in terms of autocorrelation, sensitivity to starting values, and computational time.
57

Figure 3.1: Histograms and trace plots for Œ≤ and œÅ under Scenario 1 for each of the three
corresponding algorithms. The black portion represents the burn-in and the colored portion
represents draws from the posterior distribution.
58

Figure 3.2: Histograms and trace plots for Œ≤ and œÅ under Scenario 2 for each of the three
corresponding algorithms. The black portion represents the burn-in and the colored portion
represents draws from the posterior distribution.
59

Figure 3.3: Histograms and trace plots for Œ≤ and Œª under Scenario 3 for each of the three
corresponding algorithms. The black portion represents the burn-in and the colored portion
represents draws from the posterior distribution.
60

Figure 3.4: Histograms and trace plots for Œ≤ and Œª under Scenario 4 for each of the three
corresponding algorithms. The black portion represents the burn-in and the colored portion
represents draws from the posterior distribution.
61

Figure 3.5: Histograms and trace plots for Œ≤ under Scenario 5 for each of the three cor-
responding algorithms. The black portion represents the burn-in and the colored portion
represents draws from the posterior distribution.
62

Figure 3.6: Autocorrelation and partial autocorrelation in the sample paths of Œ≤ and œÅ under
Scenario 1 for each of the three corresponding algorithms.
63

Figure 3.7: Autocorrelation and partial autocorrelation in the sample paths of Œ≤ and œÅ under
Scenario 2 for each of the three corresponding algorithms.
64

Figure 3.8: Autocorrelation and partial autocorrelation in the sample paths of Œ≤ and Œª under
Scenario 3 for each of the three corresponding algorithms.
65

Figure 3.9: Autocorrelation and partial autocorrelation in the sample paths of Œ≤ and Œª under
Scenario 4 for each of the three corresponding algorithms.
66

Figure 3.10: Autocorrelation and partial autocorrelation in the sample path of Œ≤ under
Scenario 5 for each of the three corresponding algorithms.
67

Figure 3.11: œÉ vs. ÀúŒ≤ under Scenario 1 for each of the three corresponding algorithms. The
black dots represent the burn in and the colored dots represent the draws from the posterior.
Top row: First starting values. Bottom row: Second starting values.
68

Figure 3.12: œÉ vs. ÀúŒ≤ under Scenario 2 for each of the three corresponding algorithms. The
black dots represent the burn in and the colored dots represent the draws from the posterior.
Top row: First starting values. Bottom row: Second starting values.
69

Figure 3.13: œÉ vs. ÀúŒ≤ under Scenario 3 for each of the three corresponding algorithms. The
black dots represent the burn in and the colored dots represent the draws from the posterior.
Top row: First starting values. Bottom row: Second starting values.
70

Figure 3.14: œÉ vs. ÀúŒ≤ under Scenario 4 for each of the three corresponding algorithms. The
black dots represent the burn in and the colored dots represent the draws from the posterior.
Top row: First starting values. Bottom row: Second starting values.
71

Figure 3.15: œÉ vs. ÀúŒ≤ under Scenario 5 for each of the three corresponding algorithms. The
black dots represent the burn in and the colored dots represent the draws from the posterior.
Top row: First starting values. Bottom row: Second starting values.
72

CHAPTER 4
THE BAYESIAN SPATIAL PROBIT REGRESSION MODEL AS A
TOOL FOR CLASSIFICATION
There are often two primary goals in regression analyses. The Ô¨Årst is to model the
relationship between the response variable and the covariates. The second is to accurately
predict missing or unobserved responses. Up to this point, we have focused on the Ô¨Årst goal,
emphasizing the need to allow for residual spatial dependence when analyzing relationships
between spatially-referenced binary response variables and associated covariates. In this
chapter, we focus on the second goal, prediction.
In some sense, prediction of binary or categorical outcomes can be thought of as a
classiÔ¨Åcation problem, where we determine into which class, or category, a collection of
predictors/inputs, or covariates, is likely to fall. For example, in predicting the land cover
category at an unobserved location, we observe a collection of predictors/inputs, such as
elevation or distance to the nearest major road. Using these predictors/inputs, we can de-
termine some decision function which deÔ¨Ånes a classiÔ¨Åcation rule for classifying each lo-
cation. Within the classiÔ¨Åcation literature, the decision function is determined using one of
a number of available classiÔ¨Åcation methods, each method using statistical decision theory
to determine optimal classiÔ¨Åcation rules. Popular classiÔ¨Åcation techniques, however, do
73

not typically take into account the categories of observations nearby in geographical space.
Instead, the decision function relies only on a function of the predictors/inputs.
In classiÔ¨Åcation problems involving spatially-referenced observations, we argue that
neighboring observations along with predictors/inputs should be considered. As we will
illustrate, classiÔ¨Åcation rules that rely on neighboring observations can be derived from the
Bayesian spatial probit regression model.
The goal of this chapter is to compare, in terms of classiÔ¨Åcation error, the Bayesian
spatial probit regression model-based classiÔ¨Åer to various other classiÔ¨Åers. We Ô¨Årst deÔ¨Åne
notation for the classiÔ¨Åcation problem in Section 4.1. In Section 4.2 we discuss classi-
Ô¨Åcation in the regression setting and, using the Bayesian spatial probit regression model,
deÔ¨Åne a classiÔ¨Åcation rule for spatially-referenced observations. In Section 4.3 we give
an overview of popular classiÔ¨Åcation methods, which in Section 4.4, we then compare to
the Bayesian spatial probit regression model-based classiÔ¨Åer in terms of both the training
(in-sample prediction) and test (out-of-sample prediction) error rates for the Southeast Asia
land cover data set used in Section 3.3.
4.1
The ClassiÔ¨Åcation Problem
In the classiÔ¨Åcation problem, we again have the set of paired observations {(Yi, xi); i =
1, . . . , n}, where Yi is a binary response variable and xi is a k √ó1 vector of covariates used
as predictors/inputs in determining decision boundaries for the classes. In the classiÔ¨Åcation
setting, the response variable is an indicator identifying the class to which the set of ob-
served predictors/inputs belong. Although many of the classiÔ¨Åcation methods listed below
can be generalized to the multi-category/class setting, we restrict our description of these
74

methods to the binary setting. In this setting there are two classes, C0 and C1, where Cj rep-
resents the class of observations where Y = j. Of the observations {(Yi, xi); i = 1, . . . , n},
n0 fall into class C0 and n1 fall into class C1, and n0 + n1 = n. Each classiÔ¨Åcation method
deÔ¨Ånes a decision function Œ¥(œâ) where œâ is the set of applicable predictors/inputs, model
parameters, and in the spatial setting, surrounding observations. Based on this decision
function, we can deÔ¨Åne a classiÔ¨Åcation rule.
Before discussing each of the classiÔ¨Åcation methods, we Ô¨Årst give a brief overview of
the classiÔ¨Åcation process. Typically, when using classiÔ¨Åcation methods, the sample data
is randomly divided into two subsets, the training and test data sets, of sizes ntrain and
ntest, respectively. This allows us to Ô¨Åt the model on which the classiÔ¨Åer is based using the
training data and evaluate the method‚Äôs ability to predict based on the test data set. When
we compare classiÔ¨Åcation methods, we consider the training and test error rates, or the
number of incorrect classiÔ¨Åcations among training and test data sets, divided by the sample
size of each data set, respectively.
Furthermore, some of the classiÔ¨Åers rely on Ô¨Åxed parameters that are not estimated,
and thus, require tuning to minimize the prediction error. In this case, it is common to use
Ô¨Åve-fold cross-validation to obtain an optimal value for the Ô¨Åxed parameters. We do this
by dividing the training data set into Ô¨Åve equal subsets. We then repeatedly Ô¨Åt the model
iteratively leaving the mth subset out and compute the error rate for the mth subset for
each m = 1, . . . , 5. Averaging across the Ô¨Åve error rates determines the cross-validation
error (CVE). This procedure is repeated for various Ô¨Åxed values of the tuning parameter to
determine a satisfactory value for the analysis.
In each of the following sections, we generically use {(Yi, xi); i = 1, . . . n} as the data
used for Ô¨Åtting the method, and (Y 0, x0) as the point of prediction or classiÔ¨Åcation.
75

4.2
GLM-Based ClassiÔ¨Åcation
In this section, we Ô¨Årst discuss the standard approach for classiÔ¨Åcation using a GLM
and the decision boundaries and classiÔ¨Åcation rules for two commonly used GLMs. We
then introduce a spatial classiÔ¨Åcation technique using the Bayesian spatial probit regres-
sion model. In specifying our underlying probabilty models, although a slight misuse of
notation, we explicitly condition on x to make it clear which predictors/inputs we rely on
for classiÔ¨Åcation. Furthermore, this also makes the classiÔ¨Åcation rule comparable to that
of discriminant analysis as discussed in Section 4.3.1. Finally, we note that in GLM-based
classiÔ¨Åcation, the xi include a term allowing for an intercept along with the other predic-
tors/inputs, as is usually done in regression analyses.
4.2.1
Non-Spatial GLM-Based ClassiÔ¨Åcation
Consider the GLM for independent binary response variables
Yi|xi, Œ≤ ‚àºBernoulli(pi)
g(pi) = x‚Ä≤
iŒ≤,
(4.1)
where pi = P(Yi = 1|xi, Œ≤), xi is a Ô¨Åxed k √ó 1 vector of covariates, Œ≤ is a k √ó 1 vector
of coefÔ¨Åcients, and g(¬∑) is a link function. To classify x0, in regression analyses we predict
Y 0 = 1 when P(Y 0 = 1|x0, Œ≤) > P(Y 0 = 0|x0, Œ≤) and Y 0 = 0 otherwise. This
prediction method, can be turned into a classiÔ¨Åcation rule as follows.
Analogous to prediction in the regression setting, we deÔ¨Åne the following decision func-
tion,
Œ¥(œâ) ‚â°Œ¥(x0, Œ≤) = P(Y 0 = 1|x0, Œ≤)
P(Y 0 = 0|x0, Œ≤) =
g‚àí1(x0‚Ä≤Œ≤)
1 ‚àíg‚àí1(x0‚Ä≤Œ≤),
(4.2)
76

and set it equal to one. Equivalently, the decision function,
Œ¥(œâ) ‚â°Œ¥‚àó(x0, Œ≤) ‚â°log

g‚àí1(x0‚Ä≤Œ≤)
1 ‚àíg‚àí1(x0‚Ä≤Œ≤)

,
(4.3)
can be set equal to zero.
Based on (4.2), a classiÔ¨Åcation rule is then
Y 0 =
(
1, if Œ¥(x0, Œ≤) > 1
0, otherwise
.
(4.4)
Similarly, based on (4.3), a classiÔ¨Åcation rule is
Y 0 =
(
1, if Œ¥‚àó(x0, Œ≤) > 0
0, otherwise
.
(4.5)
A popular form for the link function, g(¬∑), is the logit link, where
g(pi) = log

pi
1 ‚àípi

.
Using this link function, the decision function in (4.3) becomes
Œ¥‚àó(x0, Œ≤) = log

g‚àí1(x0‚Ä≤Œ≤)
1 ‚àíg‚àí1(x0‚Ä≤Œ≤)

= x0‚Ä≤Œ≤
(4.6)
and the classiÔ¨Åcation rule in (4.5) is
Y 0 =
(
1, if x0‚Ä≤Œ≤ > 0
0, otherwise
.
(4.7)
Although there are many approaches for estimating Œ≤ in a frequentist setting, we estimate
Œ≤ numerically by maximizing the likelihood function,
L(Œ≤, Y ) =
n
Y
i=1
pYi
i (1 ‚àípi)1‚àíYi =
n
Y
i=1
exp{Yix‚Ä≤
iŒ≤}
1 + exp{x‚Ä≤
iŒ≤}
with respect to Œ≤.
77

Another common link function is the probit link, where
g(pi) = Œ¶‚àí1(pi)
and Œ¶ is the cumulative standard normal distribution function. Using this link function, the
decision function in (4.2) is then
Œ¥(x0, Œ≤) =
g‚àí1(x0‚Ä≤Œ≤)
1 ‚àíg‚àí1(x0‚Ä≤Œ≤) =
Œ¶(x0‚Ä≤Œ≤)
1 ‚àíŒ¶(x0‚Ä≤Œ≤)
(4.8)
and the classiÔ¨Åcation rule is
Y 0 =
(
1, if Œ¶(x0‚Ä≤Œ≤)/(1 ‚àíŒ¶(x0‚Ä≤Œ≤)) > 1
0, otherwise
.
(4.9)
Again, Œ≤ can be estimated a number of ways, one of which is to estimate Œ≤ numerically by
maximizing the likelihood function,
L(Œ≤, Y ) =
n
Y
i=1
pYi
i (1 ‚àípi)1‚àíYi =
n
X
i=1
Œ¶(x‚Ä≤
iŒ≤)Yi(1 ‚àíŒ¶(x‚Ä≤
iŒ≤))(1‚àíYi)
with respect to Œ≤. We also consider a Bayesian approach.
Consider the latent variable representation of the Bayesian probit regression model as
deÔ¨Åned in Section 2.1.1. Without loss of generality, assume œÉ2 = 1, ÀúŒ≤ = Œ≤, and ÀúZ = Z.
Using this representation, P(Y = 1|x, Œ≤) = P(Z > 0|x, Œ≤) and the decision function is:
Œ¥(x0, Œ≤) = P(Y 0 = 1|x0‚Ä≤, Œ≤)
P(Y 0 = 0|x0‚Ä≤, Œ≤)
= P(Z0 > 0|x0‚Ä≤, Œ≤)
P(Z0 ‚â§0|x0‚Ä≤, Œ≤)
=
Œ¶(x0‚Ä≤Œ≤)
1 ‚àíŒ¶(x0‚Ä≤Œ≤),
which corresponds to the decision function given by (4.8). Note that in this case, the
decision function is the posterior odds. We can use the classiÔ¨Åcation rule given in (4.9)
78

which, in this setting, corresponds to a 0-1 loss function (i.e., the loss for predicting Y 0
correctly is 0 and the loss for predicting Y 0 incorrectly is 1). For consistency, we continue
to use the frequentist notation for the decision function and classiÔ¨Åcation rule as initially
deÔ¨Åned earlier in this chapter.
We can estimate P(Y 0 = 1|x0, Œ≤) using one of the following two approaches. The Ô¨Årst
is to use a posterior mean classiÔ¨Åer where we estimate E[Œ≤|Y ], or the posterior mean of Œ≤,
and use this estimate to obtain P(Y 0 = 1|x0, ÀÜŒ≤) = P(Z0 > 0|x0, ÀÜŒ≤) = Œ¶(x0‚Ä≤ ÀÜŒ≤). In prac-
tice, we approximate E[Œ≤|Y ] by using ÀÜŒ≤ = PT
t=1 Œ≤[t]/T, where Œ≤[t] are draws from the
posterior distribution of Œ≤, and T is the number of draws from the distribution. This Ô¨Årst
representation is analogous to the estimation method of the previous likelihood approach.
The second is to use a posterior predictive classiÔ¨Åer p0 = E[I(Z0 > 0)|x0, Y ], or the pos-
terior probability that Z0 > 0. In doing this, we marginalize over the posterior distribution
of Œ≤ to obtain an estimate of P(Y 0 = 1|x0, Y ) =
R
P(Z0 > 0|x0, Œ≤)œÄ(Œ≤|Y )dŒ≤. In prac-
tice, we estimate p0 by using ÀÜp0 = PT
t=1 I(Z0[t] > 0)/T, where Z0[t] are draws from the
posterior distribution of Z0, and T is the number of draws from the distribution. Thus, for
the independent Bayesian probit regression model, the two estimated classiÔ¨Åcation rules
are:
1. Posterior Mean ClassiÔ¨Åer:
Y 0 =
(
1, if Œ¶(x0‚Ä≤ ÀÜŒ≤)/(1 ‚àíŒ¶(x0‚Ä≤ ÀÜŒ≤)) > 1
0, otherwise
(4.10)
2. Posterior Predictive ClassiÔ¨Åer:
Y 0 =
(
1, if ÀÜp0/(1 ‚àíÀÜp0) > 1
0, otherwise
(4.11)
79

As discussed in Section 4.1, when using classiÔ¨Åcation techniques, we compute error
rates for both the training data (or the observed data used to Ô¨Åt the model) and the test
data (or the observed data not used to Ô¨Åt the model). For the test data, we can simply use
the latent Zj, for j = 1, . . . , ntest, as sampled within the Gibbs sampler. However, for the
training data, the latent Zi, for i = 1, . . . , ntrain, are sampled within the Gibbs sampler
given the observed Yi. In this case, to use the latent Zi as predictors, we must sample these
values as if the Yi are unknown. Using the posterior draws of Œ≤, we can resample the latent
Zi as follows:
(i) Take samples Œ≤[t] for i = 1, . . . , T from œÄ(Œ≤|Y ) and sample a corresponding
Z[t]
i
‚àº
N(xiŒ≤[t], 1). (Note that these Z[t]
i
are not those sampled in the MCMC
algorithm.)
(ii) Determine ÀÜp0
i = PT
t=1 I(Z[t]
i
> 0)/T and let Y 0
i be the predicted value of Yi using
the posterior predictive classiÔ¨Åer.
(iii) Repeat (i) and (ii) for all i = 1, . . . , ntrain.
(iv) Compute
the
training
error
rate
for
the
posterior
predictive
classiÔ¨Åer:
Pntrain
i=1
I(Y 0
i
Ã∏= Yi)/ntrain.
4.2.2
Spatial GLM-Based ClassiÔ¨Åcation
As discussed in Section 2.2, the latent variable representation of the probit regression
model allows us to include spatial dependence among the response variables. Here we
propose extending the use of this model to the classiÔ¨Åcation setting. In this case, the deci-
sion function Œ¥(œâ) depends on the covariates and regression coefÔ¨Åcients, as well as on the
categories of the surrounding observations.
80

Using equations (2.5) and (2.6), it follows that the distribution for the latent variable at
an unobserved location is
Z0|Y , X, Œ≤, Œ∏, Z ‚àºN(¬µZ0, œÉZ0)
where
¬µZ0 = x0‚Ä≤Œ≤ + œÉ(Œ∏)‚Ä≤-1 (Œ£(Œ∏))‚àí1 (Z ‚àíX‚Ä≤Œ≤)
œÉ2
Z0 = œÉ(Œ∏)1 ‚àíœÉ(Œ∏)‚Ä≤-1 (Œ£(Œ∏))‚àí1 œÉ(Œ∏)-1
X = (x1, . . . , xn)‚Ä≤, œÉ(Œ∏) is an (n + 1) √ó 1 vector representing the variance of Z0 and
(Z0, Z‚Ä≤)‚Ä≤, œÉ(Œ∏)-1 is œÉ(Œ∏) with the Ô¨Årst element removed, and Œ£(Œ∏) is the spatial correlation
matrix among Z = (Z1, . . . , Zn)‚Ä≤. We can easily include sampling from this distribution
in the Ô¨Årst step of the MCMC algorithm (see Section 3.1.1 and Table 3.1), so that we can
obtain draws from the posterior distribution of Z0. In this spatially-dependent case,
P(Y 0 = 1|x0, Œ≤, Œ∏, Y , Z) = P(Z0 > 0|x0, Œ≤, Œ∏, Y , Z)
= Œ¶
 
¬µZ0
p
œÉ2
Z0
!
.
Thus, the decision function at x0 is
Œ¥(œâ) ‚â°Œ¥(x0, Y , Z, Œ≤, Œ∏) =
P(Z0 > 0|x0, Y , Z, Œ≤, Œ∏)
1 ‚àíP(Z0 > 0|x0, Y , Z, Œ≤, Œ∏)
Just as for the independent Bayesian probit regression model, we can consider two ways
to obtain an estimated classiÔ¨Åcation rule:
1. Posterior Mean ClassiÔ¨Åer:
Estimate E[Œ≤|Y ], E[Œ∏|Y ], and E[Z|Y ] by computing ÀÜŒ≤ = PT
t=1 Œ≤[t]/T, ÀÜŒ∏ =
PT
t=1 Œ∏[t]/T, and ÀÜZ = PT
t=1 Z[t]/T, respectively. The estimated classiÔ¨Åcation rule
is
81

Y 0 =
Ô£±
Ô£≤
Ô£≥
1, if
Œ¶(ÀÜ¬µZ0/‚àö
ÀÜœÉ2
Z0)
1‚àíŒ¶(ÀÜ¬µZ0/‚àö
ÀÜœÉ2
Z0) > 1
0, otherwise
(4.12)
where
ÀÜ¬µZ0 = x0‚Ä≤ ÀÜŒ≤ + œÉ(ÀÜŒ∏)‚Ä≤-1

Œ£(ÀÜŒ∏)
‚àí1
( ÀÜZ ‚àíX‚Ä≤ ÀÜŒ≤)
ÀÜœÉ2
Z0 = œÉ(ÀÜŒ∏)1 ‚àíœÉ(ÀÜŒ∏)‚Ä≤-1

Œ£(ÀÜŒ∏)
‚àí1
œÉ(ÀÜŒ∏)-1
2. Posterior Predictive ClassiÔ¨Åer:
Estimate p0 = E[I(Z0 > 0)|x0, Y ] by computing ÀÜp0 = PT
t=1 I(Z0[t] > 0)/T. This
gives an estimate of
P(Y 0 = 1|x0, Y ) = P(Z0 > 0|x0, Y )
=
Z
P(Z0 > 0|Z, Œ≤, Œ∏) œÄ(Z, Œ≤, Œ∏) dZ dŒ≤ dŒ∏.
The estimated classiÔ¨Åcation rule is then
Y 0 =
(
1, if
ÀÜp0
1‚àíÀÜp0 > 1
0, otherwise
.
(4.13)
Again, each of these classiÔ¨Åcation rules correspond to a 0-1 loss function on the predicted
Y 0.
As with the independent Bayesian probit regression model, for the training data, we
must resample the Zis to determine prediction error rates. For the Bayesian spatial probit
regression model, however, we rely on the observed surrounding observations to provide
information about the category of the unobserved locations. In this case, evaluating the
training error is not straightforward. We propose the following two approaches for the
spatial probit posterior predictive classiÔ¨Åer:
82

A. One-at-a-Time Training Error:
(i) Take samples (Œ≤[t], Œ∏[t], Z[t]-i ), for t = 1, . . . , T, where Z[t]-i is an (n‚àí1)√ó1 vector
of sampled Zj for j = 1, . . . , i‚àí1, i+1, . . . , ntrain and sample a corresponding
Z[t]
i ‚àºN(¬µZi, œÉ2
Zi) where
ÀÜ¬µZi = x‚Ä≤
iŒ≤[t] + Œ£(Œ∏[t])i,-i
 Œ£(Œ∏[t])-i,-i
‚àí1 (Z[t]-i ‚àíX-iŒ≤[t])
ÀÜœÉ2
Zi = Œ£(Œ∏[t])i,i ‚àíŒ£(Œ∏[t])i,-i
 Œ£(Œ∏[t])-i,-i
‚àí1 Œ£(Œ∏[t])-i,i
and X-i is X with the ith row removed, and Œ£(Œ∏[t])j,-k is the jth row of the
estimated spatial correlation of Z with the kth column removed. (Note that Z[t]-i
are the posterior samples obtained from the MCMC algorithm, however, the
Z[t]
i s are not the same as those sampled in the MCMC algorithm.)
(ii) Determine ÀÜp0
i = PT
t=1 I(Z[t]
i
> 0)/T and let Y 0
i be the predicted value of Yi
using the posterior predictive classiÔ¨Åer.
(iii) Repeat (i) and (ii) for all i = 1, . . . , ntrain.
(iv) Compute the one-at-a-time training error: Pntrain
i=1
I(Y 0
i Ã∏= Yi)/ntrain
B. Joint Training Error:
(i) Take samples (Œ≤[t], Œ∏[t]) for t = 1, . . . , T and sample a corresponding Z[t] ‚àº
N(XŒ≤[t], Œ£(Œ∏[t])). (Note that the Z[t] are not the same as those sampled in the
MCMC algorithm.)
(ii) For i = 1, . . . , ntrain, compute ÀÜp0
i = PV
v=1 I(Z[v]
i
> 0)/V and let Y 0
i be the
predicted value of Yi using the posterior predictive classiÔ¨Åer.
(iii) Compute the joint training error: Pntrain
i=1
I(Y 0
i Ã∏= Yi)/ntrain
83

Both the one-at-a-time and joint training errors allow for spatial dependence among the bi-
nary predictions/classiÔ¨Åcations through the latent random variable. The joint training error
allows for spatial dependence only through the spatial dependence structure of the latent
variables, Œ£(Œ∏). In contrast, the one-at-a-time training error allows for spatial dependence
through Œ£(Œ∏), but also allows for spatial dependence by conditioning on the current values
of the latent random variables at nearby locations, Z[t]-i . We note that for the independent
model, both the one-at-a-time and joint training errors will be the same since Zi is inde-
pendent of all other Zj for j = 1, . . . , i ‚àí1, i + 1, . . . , n.
4.3
Alternative ClassiÔ¨Åcation Methods
In this section, we give an overview of alternatives to GLM-based classiÔ¨Åcation. In
these alternative classiÔ¨Åcation methods, we assume that the xis only include the predic-
tors/inputs, and thus do not include a term to allow for an intercept as in the GLM-based
classiÔ¨Åcation methods. Unless otherwise noted, the classiÔ¨Åcation methods described here
are taken from Hastie et al. (2001).
4.3.1
Discriminant Analysis
In discriminant analysis, rather than considering the explanatory variables x as Ô¨Åxed as
they are in regression analyses, the x are viewed as random variables, with class-speciÔ¨Åc
density functions fj(x) corresponding to each class Cj. The classes also have prior proba-
bilities œÄj, such that œÄ0 + œÄ1 = 1. To determine the probability that a set of predictors will
fall into class j, we employ Bayes‚Äô theorem which implies that
P(Y = j|x) =
fj(x)œÄj
f1(x)œÄ1 + f0(x)œÄ0
.
(4.14)
84

The Bayes‚Äô classiÔ¨Åcation rule is to classify an observation to class C1 when P(Y = 1|x) >
P(Y = 0|x) and to C0 otherwise.
We Ô¨Årst describe discriminant analysis in its general form, allowing a general form for
the fj(x)s and the decision function, and then discuss four special cases that we use in our
analysis.
Let
Xj =
Ô£Æ
Ô£ØÔ£∞
x1
...
xnj
Ô£π
Ô£∫Ô£ª
where {x1, . . . , xnj} = {xi : Yi = j} and Xj is an njk √ó 1 vector of predictors/inputs
corresponding to observations in class Cj. To classify Y 0, for each class we deÔ¨Åne
X0
j =
x0
Xj

,
where X0
j is a (nj + 1)k √ó 1 vector and x0 is a k √ó 1 vector of predictors/inputs associated
with Y 0. Our goal is to determine a decision boundary for classifying Y 0.
In discriminant analysis, fj(¬∑) is typically the multivariate normal density function,
fj(X0
j ) =
1
(2œÄ)(nj+1)k/2|Œ£X
j |1/2 exp

‚àí1
2(X0
j ‚àí¬µX
j )‚Ä≤  Œ£X
j
‚àí1 (X0
j ‚àí¬µX
j )

where ¬µX
j is the (nj+1)k√ó1 class-speciÔ¨Åc mean vector and Œ£X
j is the (nj+1)k√ó(nj+1)k
class-speciÔ¨Åc covariance matrix. It follows that
fj(x0|x1, . . . , xnj) =
1
(2œÄ)k/2|Œ£x0
j |1/2 exp

‚àí1
2(x0 ‚àí¬µx0
j )‚Ä≤ 
Œ£x0
j
‚àí1
(x0 ‚àí¬µx0
j )

(4.15)
where
¬µx0
j = ¬µj({1:k}) + Œ£X
j({1:k},-{1:k})
 Œ£X
j(-{1:k},-{1:k})
‚àí1 (Xj ‚àí¬µj(-{1:k}))
Œ£x0
j = Œ£X
j({1:k},{1:k}) ‚àíŒ£X
j({1:k},-{1:k})
 Œ£X
j(-{1:k},-{1:k})
‚àí1 Œ£X
j(-{1:k},{1:k}).
85

We use the subscript notation ({1 : k}) to indicate the Ô¨Årst k elements of the corresponding
matrix or vector (i.e., those indices corresponding to x0) and (-{1 : k}) indicates the matrix
or vector without the Ô¨Årst k elements (i.e., the remaining indices corresponding to Xj).
Using the log-odds as the decision function to determine a classiÔ¨Åcation rule for classi-
fying Y 0, it follows from (4.14) and (4.15) that
Œ¥(œâ) ‚â°Œ¥(x0, ¬µx0
0 , ¬µx0
1 , Œ£x0
0 , Œ£x0
1 )
= log
P(Y 0 = 1|x0)
P(Y 0 = 0|x0)

= log œÄ1
œÄ0
+ 1
2 log |Œ£x0
0 |
|Œ£x0
1 | ‚àí1
2¬µx0‚Ä≤
1

Œ£x0
1
‚àí1
¬µx0
1 + 1
2¬µx0‚Ä≤
0

Œ£x0
0
‚àí1
¬µx0
0
|
{z
}
‚â°Œ±0
+ x0‚Ä≤ 
(Œ£x0
1 )‚àí1¬µ1 ‚àí(Œ£x0
0 )‚àí1¬µx0
0

|
{z
}
‚â°Œ±1
‚àíx0‚Ä≤ 1
2

(Œ£x0
1 )‚àí1 ‚àí(Œ£x0
0 )‚àí1
|
{z
}
‚â°Œ±2
x0
.
(4.16)
Here, Œ±0 is a scalar, Œ±1 is a k √ó 1 vector, and Œ±2 is a k √ó k matrix, which we deÔ¨Åne
for notational convenience. Setting the decision function equal to zero, the discriminant
analysis-based classiÔ¨Åcation rule is
Y 0 =
(
1, if (x0‚Ä≤Œ±1 ‚àíx0‚Ä≤Œ±2x0) > ‚àíŒ±0
0, if (x0‚Ä≤Œ±1 ‚àíx0‚Ä≤Œ±2x0) ‚â§‚àíŒ±0
.
(4.17)
We now consider special cases to (4.17). Each of these special cases assumes that the
mean of xi is equal across all observations, so that ¬µX
j = [1nj+1 ‚äó¬µj] where 1nj+1 is an
(nj + 1) √ó 1 vector of ones and ¬µj is a k √ó 1 class speciÔ¨Åc mean vector. The difference
between each of these special cases is in the speciÔ¨Åcation of the covariance matrix Œ£X
j . We
Ô¨Årst describe three popular discriminant analysis methods (linear discriminant analysis,
diagonal linear discriminant analysis, and quadratic discriminant analysis) all of which
assume that the xi are independent. Then we describe an approach for spatial discriminant
analysis (spatial linear discriminant analysis) due to ÀáSaltytÀôe Benth and DuÀácinskas (2005).
86

Assuming the xi are independent results in the following form for the covariance of
X0
j :
var(X0
j ) = Œ£X
j = (Inj+1 ‚äóŒõj),
(4.18)
where Inj+1 is an (nj + 1) √ó (nj + 1) identity matrix and Œõj is a class-speciÔ¨Åc covari-
ance matrix for the k components of xi. Under this assumption, x0 is independent of
x1, . . . , xnj, so
fj(x0|x1, . . . , xnj) = fj(x0) =
1
(2œÄ)k/2|Œõj|1/2 exp{‚àí1
2(x0 ‚àí¬µj)‚Ä≤Œõ‚àí1
j (x0 ‚àí¬µj)}.
Assuming a constant variance across classes (i.e., Œõj = Œõ for j = 0, 1) results in
linear discriminant analysis (LDA) because the decision boundary is linear in the xs. The
decision function given in (4.16) can be written in this case as
Œ¥(œâ) ‚â°Œ¥(x0, ¬µ0, ¬µ1, Œõ) = log
P(Y 0 = 1|x0)
P(Y 0 = 0|x0)

= log œÄ1
œÄ0
‚àí1
2(¬µ1 + ¬µ0)‚Ä≤Œõ‚àí1(¬µ1 ‚àí¬µ0)
|
{z
}
Œ±LDA
0
+x0‚Ä≤ Œõ‚àí1(¬µ1 ‚àí¬µ0)
|
{z
}
Œ±LDA
1
,
where Œ±LDA
0
is a scalar and Œ±LDA
1
is a k √ó 1 vector, deÔ¨Åned for notational convenience.
Note that this decision function is effectively equivalent to the one based on the logistic
regression model in (4.6), however, in logistic regression, we assume the x‚Äôs are Ô¨Åxed and
thus make no distributional assumptions on x as in discriminant analysis. This results in
the following LDA-based classiÔ¨Åcation rule for Y 0 given x0:
Y 0 =
(
1, if x0‚Ä≤Œ±LDA
1
> ‚àíŒ±LDA
0
0, if x0‚Ä≤Œ±LDA
1
‚â§‚àíŒ±LDA
0
(4.19)
In practice, the parameters œÄj, ¬µj, Œõ (and thus Œ±LDA
0
and Œ±LDA
1
) are unknown but can
be estimated using maximum likelihood:
87

‚Ä¢ ÀÜœÄj = nj/n
‚Ä¢ ÀÜ¬µj = P
i:Yi=j xi/nj
‚Ä¢ ÀÜŒõ = P
j‚àà{0,1}
P
i:Yi=j(xi ‚àíÀÜ¬µj)(xi ‚àíÀÜ¬µj)‚Ä≤/(n ‚àí2)
Diagonal linear discriminant analysis (DLDA) additionally assumes independence be-
tween the k predictors/inputs so that var(xi) = Œõ is a diagonal matrix. The DLDA-based
classiÔ¨Åcation rule is the same as (4.19), but using a diagonal matrix Œõ. The mth diagonal
element of Œõ is estimated by ÀÜŒõ(m,m) = P
j‚àà{0,1}
P
i:Yi=j(xim ‚àíÀÜ¬µjm)2/(n ‚àí2) where xim
and ÀÜ¬µjm are the mth elements of xi and ¬µj, respectively.
In LDA, we assume a constant covariance for xi among the classes (i.e., Œõj = Œõ for
j = 0, 1). On the other hand, quadratic discriminant analysis (QDA) allows for each class
to have its own covariance. The decision function now contains a quadratic term in the xs:
Œ¥(œâ) ‚â°Œ¥(x0, ¬µ0, ¬µ1, Œõ0, Œõ1) = log
P(Y 0 = 1|x0)
P(Y 0 = 0|x0)

= log œÄ1
œÄ0
+ 1
2 log |Œõ0|
|Œõ1| ‚àí1
2¬µ‚Ä≤
1Œõ‚àí1
1 ¬µ1 + 1
2¬µ‚Ä≤
0Œõ‚àí1
0 ¬µ0
|
{z
}
Œ±QDA
0
+ x0‚Ä≤ (Œõ‚àí1
1 ¬µ1 ‚àíŒõ‚àí1
0 ¬µ0)
|
{z
}
Œ±QDA
1
‚àíx0‚Ä≤ 1
2(Œõ‚àí1
1
‚àíŒõ‚àí1
0 )
|
{z
}
Œ±QDA
2
x0.
Here, Œ±QDA
0
is a scalar, Œ±QDA
1
is a k √ó 1 vector, and Œ±QDA
2
is a k √ó k matrix, which are
deÔ¨Åned for notational convenience. This results in the QDA-based classiÔ¨Åcation rule for
Y 0 given x0:
Y 0 =
(
1, if (x0‚Ä≤Œ±QDA
1
‚àíx0‚Ä≤Œ±QDA
2
x0) > ‚àíŒ±QDA
0
0, if (x0‚Ä≤Œ±QDA
1
‚àíx0‚Ä≤Œ±QDA
2
x0) ‚â§‚àíŒ±QDA
0
(4.20)
where we estimate the parameters by taking
‚Ä¢ ÀÜœÄj = nj/n
88

‚Ä¢ ÀÜ¬µj = P
i:Yi=j xi/nj
‚Ä¢ ÀÜŒ£j = P
i:Yi=j(xi ‚àíÀÜ¬µj)(xi ‚àíÀÜ¬µj)‚Ä≤/(nj ‚àí1).
The Ô¨Ånal discriminant analysis method we discuss is a special case of the spatial-
temporal extension of LDA recently proposed by ÀáSaltytÀôe Benth and DuÀácinskas (2005).
We restrict our discussion to the spatial case, but refer the reader to ÀáSaltytÀôe Benth and
DuÀácinskas (2005) for more details on the spatial-temporal approach. In spatial LDA, we do
not assume independence among the xi as in the previous three methods; instead, we as-
sume the predictors/inputs are spatially dependent. Following ÀáSaltytÀôe Benth and DuÀácinskas
(2005), the speciÔ¨Åc form for the covariance is
var
 X0
j

= Œ£X
j = (Rj(Œ∏) ‚äóŒõj)
where Rj(Œ∏) is an (nj + 1) √ó (nj + 1) class-speciÔ¨Åc spatial covariance matrix and Œõj is
again the var(xi). (Contrast this speciÔ¨Åcation with that given by (4.18) where we assumed
the xi were independent.) The separable structure for Œ£X
j implies that each of the predic-
tors/inputs have the same spatial dependence. Just as with LDA, spatial LDA assumes the
covariance for each xi is the same for both classes, i.e., Œõj = Œõ for j = 0, 1.
The spatial LDA-based classiÔ¨Åcation rule is the same as the general discriminant analy-
sis classiÔ¨Åcation rule in (4.17), but we can simplify the parameters ¬µx0
j and Œ£x0
j as follows:
¬µx0
j = ¬µj +
 Rj(Œ∏)(1,-1)(Rj(Œ∏)(-1,-1))‚àí1 ‚äóIk
  Xj ‚àí(1nj ‚äó¬µj)

Œ£x0
j =
 Rj(Œ∏)(1,1) ‚àíRj(Œ∏)(1,-1)(Rj(Œ∏)(-1,-1))‚àí1Rj(Œ∏)(-1,1)

‚äóŒõ
where Rj(Œ∏)1,1 indicates the Ô¨Årst element of Rj(Œ∏), and Rj(Œ∏)(-1,-1) indicates Rj(Œ∏) with
the Ô¨Årst row and column removed, and likewise for Rj(Œ∏)(1,-1) and Rj(Œ∏)(-1,1).
89

To make use of the spatial-temporal classiÔ¨Åer, ÀáSaltytÀôe Benth and DuÀácinskas use a Ô¨Åxed
and known spatial-temporal correlation function. They then calculate unbiased forms of
maximum likelihood estimators of the unknown parameters. Therefore, to use their ap-
proach in our analysis which only includes spatial dependence, the estimators for the un-
known parameters are:
‚Ä¢ ÀÜ¬µj =

1nj(Rj(ÀÜŒ∏)(-1,-1))‚àí11‚Ä≤
nj
‚àí1
1nj(Rj(ÀÜŒ∏)(-1,-1))‚àí1[x1, . . . , xnj]‚Ä≤
‚Ä¢ ÀÜŒõ = (1/(n ‚àí2)) P
j‚àà0,1
 [x1, . . . , xnj]‚Ä≤ ‚àí(1nj ‚äóÀÜ¬µ‚Ä≤
j)
‚Ä≤
√ó(Rj(ÀÜŒ∏)(-1,-1))‚àí1  [x1, . . . , xnj]‚Ä≤ ‚àí(1nj ‚äóÀÜ¬µ‚Ä≤
j)

.
4.3.2
Support Vector Machines
First introduced by Cortes and Vapnik (1995), the goal of support vector machines
(SVM) is to determine a hyperplane in covariate space separating the classes in such a way
that the margin between the two classes is maximized. The margin is the minimum distance
between the the predictors/inputs xi of the two classes in the direction perpendicular to the
hyperplane. The resulting function determining this hyperplane is the decision function.
An illustration of this approach is provided in Figure 4.1 (a). In this illustration, for each
observations there are two inputs, xi1 and xi2 which determine the classes (denoted by the
two plotting symbols). The margin is the gap between the two classes, or the space between
the two dotted lines, and the maximum-margin hyperplane is the solid line between the two
classes. The dashed lines from the two points on the edges of the margin (the support
vectors) to the hyperplane show the distance to be maximized.
Figure 4.1 (b) shows
another hyperplane separating the two classes, but one that does not maximize the margin.
90

(a)
(b)
Figure 4.1: Illustration of the hyperplane (solid line) and margins (dotted lines) determined
by support vector machines separating the two classes (denoted by the two plotting sym-
bols). The hyperplane in (a) is the maximum margin hyperplane and the hyperplane in (b)
does not maximize the margin.
In SVM, the classes are labeled as either 1 or ‚àí1 (instead of 1 or 0, as before). To
accommodate this convention, we deÔ¨Åne Y ‚àó
i = 2Yi ‚àí1, for i = 1, . . . , n, so that Y ‚àó
i ‚àà
{‚àí1, 1}.
Consider the decision function determined by the linear hyperplane {x : Œ¥(x) = 0}
where
Œ¥(œâ) ‚â°Œ¥(x, Œ≤, Œ≤0) = x‚Ä≤Œ≤ + Œ≤0.
(4.21)
Given observations {Y ‚àó
i , xi} for i = 1, . . . , n, maximizing the margin between the two
classes and the hyperplane is equivalent to minimizing ||Œ≤|| subject to Y ‚àó
i (x‚Ä≤
iŒ≤+Œ≤0) ‚â•1 for
all i = 1, . . . , n. This problem can be represented as the following Lagrange optimization
91

function
max
Œ∑
L = max
Œ∑
 n
X
i=1
Œ∑i ‚àí1
2
n
X
i=1
n
X
i‚àó=1
Œ∑iŒ∑i‚àóY ‚àó
i Y ‚àó
i‚àóx‚Ä≤
ixi‚àó
!
(4.22)
where Œ∑ = (Œ∑1, . . . , Œ∑n)‚Ä≤ are the Lagrangian multipliers, which are subject to the constraints
that Pn
i=1 Œ∑iY ‚àó
i = 0 and Œ∑i ‚â•0 for all i. The xi where Œ∑i > 0 are the support vectors and
are the only vectors which inÔ¨Çuence the position of the hyperplane. In Figure 4.1, the
support vectors are the points on the outer boundary of the margins. We can write the
relationship between Œ∑ and Œ≤ as
Œ≤ =
n
X
i=1
Œ∑iY ‚àó
i xi
and the relationship between Œ∑ and Œ≤0 as
Œ≤0 = 1
nsv
X
i:Œ∑i>0
(Œ≤xi ‚àíY ‚àó
i )
where nsv is the number of support vectors.
While the above hyperplane is linear, SVM can be extended to create nonlinear bound-
aries between the classes. This extension can be acheived by transforming the predic-
tors/inputs into a space where they can be separated linearly, and again Ô¨Ånd the separating
hyperplane in this transformed covariate space. We can use the Lagrange optimization
function in (4.22) with the transformed predictors/inputs K(xi, xj):
max L = max
 n
X
i=1
Œ∑i ‚àí1
2
n
X
i=1
n
X
j=1
Œ∑iŒ∑jY ‚àó
i Y ‚àó
j K(xi, xj)
!
(4.23)
subject to Pn
i=1 Œ∑iY ‚àó
i = 0 and 0 ‚â§Œ∑i ‚â§Œ≥ for all i, where Œ≥ is a tuning parameter allowing
for crossover among the two classes and K(¬∑, ¬∑) is a symmetric positive (semi-) deÔ¨Ånite
function. In our data analysis, we consider the following three popular kernels:
‚Ä¢ Linear: K(xi, xj) = x‚Ä≤
ixj
92

‚Ä¢ dth Degree Polynomial: K(xi, xj) = (1 + x‚Ä≤
ixj)d
‚Ä¢ Radial: K(xi, xj) = exp{‚àí||xi ‚àíxj||2/c}
Now, (4.21) can be written as
Œ¥(œâ) ‚â°Œ¥(x, Œ∑, Œ≤0, Œ≥) =
n
X
i=1
Œ∑iY ‚àó
i K(x, xi) + Œ≤0.
(4.24)
Thus,
ÀÜŒ¥(x0, Œ∑, Œ≤0, Œ≥) =
n
X
i=1
ÀÜŒ∑iYiK(x0, xi) + ÀÜŒ≤0
and
Y 0‚àó= sign

ÀÜŒ¥(x0, Œ∑, Œ≤0, Œ≥)

so that the SVM-based classiÔ¨Åcation rule of Y 0 is
Y 0 = 1
2(Y 0‚àó+ 1).
When implementing this classiÔ¨Åcation method, we use the R package e1071 (Dimitriadou
et al., 2010), to compute ÀÜŒ∑i and ÀÜŒ≤0 via a quadratic optimization function for a Ô¨Åxed value
of Œ≥.
4.3.3
k-Nearest Neighbors
The k-Nearest Neighbors classiÔ¨Åcation method makes no assumptions about an un-
derlying model. Using this method, for a point {Y 0, x0}, the closest k points {x(r), r =
1, . . . , k} to x0 are identiÔ¨Åed, and Y 0 is assigned to the most popular class among the
k neighbors, where ties are broken at random. ‚ÄúDistance‚Äù here is measured in covariate
93

space, not geographic space, and could be deÔ¨Åned using any valid distance metric. In our
implementation of the method, we use Euclidean distance so that
d(r) = ||x(r) ‚àíx0||.
Here, d(i) represent the ordered distances where the minimum is d(1) and the maximum
is d(n), and x(r) are the xi corresponding to d(r). Using this measure of distance requires
standardization of the variables so that no variable is given more weight than another.
For the binary case, a decision function can then be deÔ¨Åned as
Œ¥(œâ) ‚â°Œ¥(x0, x, Y ) =
k
X
r=1
Y(r)/k
where Y(r) is the Yi associated with d(r). A k-nearest neighbor classiÔ¨Åcation rule will then
be
Y 0 =
(
1, if Pk
i=1 Y(i)/k > .5
0, if Pk
i=1 Y(i)/k < .5
and if Pk
i=1 Y(i)/k = .5, Y 0 = 1 with probability .5 and Y 0 = 0 with probability .5.
4.4
Comparison of ClassiÔ¨Åcation Methods
In this section, we discuss the Ô¨Åtting of classiÔ¨Åcation parameters and compare the meth-
ods in terms of prediction error on the training and test data sets. We base our comparisons
on the land cover data used in Section 3.3, but this time we use four covariates: elevation,
distance to the coast, distance to the nearest big city, and distance to the nearest major road.
We randomly select ntrain = 432 locations to the training data, and use the remaining
ntest = 144 locations as test data.
94

4.4.1
Parameter Estimation
For each of the classiÔ¨Åers, we compute estimates of the corresponding unknown pa-
rameters as described in the previous section. However, further details are required for this
speciÔ¨Åc data set for the spatial LDA-based, SVM, and k-nearest neighbor classiÔ¨Åers.
When Ô¨Åtting the parameters corresponding to the spatial LDA-based classiÔ¨Åer, it is
necessary to Ô¨Åt a spatial covariance function. As discussed in Section 4.3.1, ÀáSaltytÀôe Benth
and DuÀácinskas (2005) use a speciÔ¨Åc parametric class of covariance functions. To estimate
the parameters of each the covariance functions corresponding to each class, we use the R
package geoR. We use the Gaussian covariance function, where the covariance between
two locations si and sj is
(R(Œ∏))ij ‚â°
 R(œÑ 2, œÜ)

ij = œÑ 2 exp
 ‚àí(dij/œÜ)2
,
dij is the Euclidean distance between si and sj, œÉ2 is the variance, and œÜ is the range
parameter.
Figure 4.2 shows the estimated semivariograms and Ô¨Åtted semivariograms for each of
the classes. In each of these plots, the numbers one through four represent the estimated
(empirical) semivariance at the speciÔ¨Åc distances for each of the four covariates. These
estimates are obtained using the variog() function in the geoR package of R which
computes the sample variance for all pairs of observations that speciÔ¨Åc difference apart.
Note that the empirical semivariograms differ for each covariate. This model, however,
requires the same covariance function for each of the covariates. Therefore, we Ô¨Åt the
semivariogram to the mean of the four estimated semivariograms. In the plot, the mean
semivariogram is represented by the dots, and the line represents the Ô¨Åtted semivariogram
function. The Ô¨Åtted values of the parameters are listed in Table 4.1. We note that although
95

Figure 4.2: Estimated and Ô¨Åtted variograms for class C0 (left) and C1 (right) Ô¨Åt using the
training data set. The numbers 1-4 indicate the estimated variogram for the four covari-
ates, the dots represent the mean of the four variograms, and the line indicates the Ô¨Åtted
variogram.
Parameter
Class C0
Class C1
œÑ 2
0.2162
0.3928
œÜ
1.2316
1.5099
Table 4.1: Fitted values for the covariance function parameters for both class C0 and C1.
the Guassian covariance function may be too smooth for some of the four covariates, in our
analysis, this function was found to Ô¨Åt the mean of all four covariate semivariograms best
when compared with other functions from the Mat¬¥ern class of covariance functions (e.g.,
exponential covariance function).
SVM and k-nearest neighbor classiÔ¨Åers require tuning of the parameters, Œ≥ and k. We
obtain optimal values of Œ≥ and k using Ô¨Åve-fold cross-validation as discussed in Section
96

ClassiÔ¨Åcation Method
Tuning Parameter
CVE
Optimal Value
Linear SVM
Œ≥
0.2744
0.13
Cubic SVM
Œ≥
0.2791
0.88
Radial SVM
Œ≥
0.2512
0.715
k-NN
k
0.2349
4
Table 4.2: Tuning parameter values for each classiÔ¨Åcation method. The optimal value of
the tuning parameter is listed along with the CVE associated with this value. The optimal
values were chosen by minimizing the Ô¨Åve-fold CVE.
4.1 and assigned the value of the associated parameter to be the value with the lowest
cross-validation error (CVE). Table 4.2 shows the tuning parameter for each classiÔ¨Åcation
method and the associated chosen value of each parameter. For the SVM classiÔ¨Åers, the
tuning parameter is a cost parameter [discuss table].
4.4.2
ClassiÔ¨Åcation Errors
Table 4.3 shows the training and test errors for each of the classiÔ¨Åcation methods. Most
of the methods have training error rates around 27% and test error rates around 29%, with
a few exceptions. The spatial LDA classiÔ¨Åer has particularly large training and test error
rates of approximately 76%. Therefore, it appears that adding the restriction of spatial de-
pendence to the predictors/inputs has a negative effect on prediction. One reason for this
may be that the underlying model requires each covariate to have the same Ô¨Åtted spatial
covariance function. Because of this possible explanation for the poor performance of this
classiÔ¨Åer, we also Ô¨Åt the model using only one covariate, but found similar error rates, indi-
cating that adding spatial dependence in covariate space may be a too restrictive assumption
for these data.
97

While the k-nearest neighbors classiÔ¨Åer has the smallest error rate for the training data,
the classiÔ¨Åer based on the Bayesian spatial probit regression model (spatial probit classiÔ¨Åer)
has the smallest error rate for the test data and only a slightly larger one-at-a-time training
error rate. The spatial probit classiÔ¨Åer joint training error is similar to that observed with
the other classiÔ¨Åers. This is due to the fact that the method does not take advantage of
the observed class of neighboring locations within the training data when the joint training
error is determined. The small test error for the spatial probit classiÔ¨Åer also illustrates how
conditioning on the observed class of neighboring locations within the training data will
improve classiÔ¨Åcation error rates over other non-spatial classiÔ¨Åers.
For this training sample, we note that the mean and posterior predictive methods result
in identical error rates for the spatial probit classiÔ¨Åer. To see why these error rates are the
same, consider the following features of the model underlying the classiÔ¨Åer. The poste-
rior mean classiÔ¨Åer is based on P(Z0 > 0|Y , x0, ÀÜŒ≤, ÀÜœÅ, ÀÜZ), while the posterior predictive
classiÔ¨Åer is based on P(Z0 > 0|Y , x0), which is obtained by marginalizing over the pos-
terior distributions of Œ≤, œÅ, and Z rather than conditioning on the posterior mean of these
parameters. Consider the mean of Z0|Y , x0:
E(Z0|Y , x0) = E
 E(Z0|Y , x0, Œ≤, œÅ, Z)

= E
Ô£´
Ô£≠x0‚Ä≤Œ≤ + œÅ
X
j:s0‚àºsj
wj
w+
(Zj ‚àíx‚Ä≤
jŒ≤)
Ô£∂
Ô£∏
= x0‚Ä≤E(Œ≤) +
X
j:s0‚àºsj
wj
w+
 E(œÅZj) ‚àíx‚Ä≤
jE(œÅŒ≤)

where s0 ‚àºsj indicates that sj and s0 are neighbors, wj is equal to one if sj and s0 are
neighbors, and w+ denotes the total number of neighbors for location s0. Note that if the
98

posterior correlations between œÅ and Œ≤ and between œÅ and Z are zero, then
E(œÅŒ≤) = E(œÅ)E(Œ≤)
(4.25)
and
E(œÅZj) = E(œÅ)E(Zj)
(4.26)
for all j = 1, . . . , n. We cannot determine analytically the posterior correlations between
these pairs of parameters since the posterior distributions are not available in closed form;
however, we can estimate the correlations between these parameters using the sampled
values. In our analysis,
|Cor(œÅ, Œ≤k)| < 0.13
for all k = 0, 1, . . . , 4, and
|Cor(œÅ, Zj)| < 0.17
for all j = 1, . . . , n. Although not perfectly uncorrelated, the correlations are small enough
to approximate the relationships in (4.25) and (4.26). Furthermore, for our particular anal-
ysis, the posterior distribution of œÅ has a mean close to one and a very small variance, i.e.,
the 95% credible interval of œÅ is (0.934, 0.999). Therefore,
E(Z0|Y , x0) ‚âàx0‚Ä≤E(Œ≤) + E(œÅ)
X
j:s0‚àºsj
wj
w+
(E(Zj) ‚àíx‚Ä≤
jE(Œ≤))
= x0‚Ä≤ ÀÜŒ≤ + ÀÜœÅ
X
j:s0‚àºsj
wj
w+
( ÀÜZj ‚àíx‚Ä≤
j ÀÜŒ≤)
= E(Z0|Y , x0, ÀÜŒ≤, ÀÜœÅ, ÀÜZ).
Thus, the estimated probabilities P(Z0 > 0|Y , x0, ÀÜŒ≤, ÀÜœÅ, ÀÜZ) and P(Z0 > 0|Y , x0) will
be similar and the binary classiÔ¨Åcations based on these probabilities are likely to be the
same, as is observed for the one-at-a-time training error and the joint test error rates. For a
99

ClassiÔ¨Åcation Method
Training Error
Test Error
Spatial Probit ClassiÔ¨Åer
Posterior Mean
0.1690
0.1667
Posterior Predictive
One-at-a-Time
Joint
0.1690
0.2708
0.1667
Bayesian Probit ClassiÔ¨Åer
Posterior Mean
0.2778
0.2986
Posterior Predictive
0.2755
0.2917
GLM-Based maximum likelihood ClassiÔ¨Åer
Logistic
0.2824
0.3056
Probit
0.2824
0.2986
Discriminant Analysis ClassiÔ¨Åer
LDA
0.2778
0.2917
DLDA
0.3611
0.3889
QDA
0.2593
0.2986
Spatial LDA
0.7639
0.7569
SVM ClassiÔ¨Åer
Linear SVM
0.2847
0.3056
Cubic SVM
0.2755
0.2847
Radial SVM
0.2315
0.2986
k-Nearest Neighbor ClassiÔ¨Åer
k-NN
0.1667
0.2431
Table 4.3: Training and test errors for the SE Asia land cover data obtained using each of
the classiÔ¨Åcation methods discussed in this chapter.
different data set or for different spatial dependence structures, the error rates may not be
the same.
Based on our analysis, we conclude that including residual spatial dependence is im-
portant in classiÔ¨Åcation of spatially-referenced data and that the classiÔ¨Åer based on the
Bayesian probit regression model is well-suited for doing so. Furthermore, including spa-
tial dependence in the predictors/inputs, as in the spatial LDA classiÔ¨Åer, is not sufÔ¨Åcient,
and in fact, may be inappropriate.
100

4.5
Summary
In this chapter, we proposed a spatial classiÔ¨Åer based on the Bayesian spatial probit
regression model and compared this classiÔ¨Åer to other well-known classiÔ¨Åcation methods.
Furthermore, for the Bayesian probit regression model and the Bayesian spatial probit re-
gression model, we distinguish between two classiÔ¨Åers, which we call the posterior mean
and posterior predictive classiÔ¨Åers. Using a data analysis, we showed that the spatial probit
classiÔ¨Åer (both the posterior mean and posterior predictive versions) is the best classiÔ¨Åer in
terms of out-of-sample prediction for our spatially-referenced land cover data example.
101

CHAPTER 5
BAYESIAN SPATIAL MULTINOMIAL PROBIT REGRESSION
In Section 2.1.2, we introduced the Albert and Chib (1993) Bayesian multinomial pro-
bit regression model for multi-category response variables. For these models, the response
variable Yi can take on a value in {1, . . . , ‚Ñì}, where ‚Ñìis the number of categories. In this
chapter, we extend the Bayesian multinomial probit regression model and the Bayesian
spatial probit regression model to a model for spatially-referenced multi-category response
variables, the Bayesian spatial multinomial probit regression model. In Section 5.1, we
discuss the speciÔ¨Åcation of this model, speciÔ¨Åcally, we discuss considerations for the la-
tent mean and covariance structures. In Section 5.2.1, we discuss a data augmentation
model-Ô¨Åtting technique, as an extension of Marginal-Scheme 1 as proposed in Section 3.1
and the Imai and van Dyk (2005) model-Ô¨Åtting algorithm for Bayesian multinomial probit
regression model.
5.1
The Bayesian Spatial Multinomial Probit Regression Model
Just as the Bayesian spatial probit regression model is an extension of the Bayesian
probit regression model, we can extend the Bayesian multinomial regression model in the
same way, by allowing the continuous latent variable to have a spatial covariance structure.
102

Therefore, the general case for a spatially-dependent multi-category Bayesian probit model
is:
Yi = arg maxj { ÀúZij, j = 1, . . . , ‚Ñì}
(5.1)
and
vec( ÀúZ) ‚â°
Ô£Æ
Ô£ØÔ£∞
ÀúZ1
...
ÀúZn
Ô£π
Ô£∫Ô£ª‚àºN(Àú¬µ, ÀúŒ£),
(5.2)
where ÀúZi = ( ÀúZi1, . . . , ÀúZi‚Ñì)‚Ä≤, Àú¬µ is an n‚Ñì√ó 1 mean vector, and ÀúŒ£ is an n‚Ñì√ó n‚Ñìspatial
covariance matrix across the ‚Ñìcategories.
For each observation Yi, rather than modeling the ‚Ñì√ó 1 unobserved latent variables, it
is common to express each latent variable ÀúZij with respect to ÀúZi‚Ñìby deÔ¨Åning an (‚Ñì‚àí1) √ó 1
vector ÀúUi = ( ÀúUi1, . . . , ÀúUi,‚Ñì‚àí1)‚Ä≤ where ÀúUij = ÀúZij ‚àíÀúZi‚Ñìfor j = 1, . . . , (‚Ñì‚àí1) (e.g., Daganzo,
1980; McCulloch and Rossi, 1994; McCulloch et al., 2000; Imai and van Dyk, 2005). By
doing this, rather than estimating an ‚Ñì√ó 1 latent vector for each observation, we now
estimate only an (‚Ñì‚àí1) √ó 1 latent vector. As a result, all parameters, if appropriately
speciÔ¨Åed, are (likelihood) identiÔ¨Åable.
This change of variables can be written by deÔ¨Åning a matrix ‚àÜto be an n(‚Ñì‚àí1) √ó n‚Ñì
block-diagonal matrix, where the ith diagonal block is a matrix ‚àÜsuch that the Ô¨Årst ‚Ñì‚àí1
columns of ‚àÜare an (‚Ñì‚àí1)√ó(‚Ñì‚àí1) identity matrix and the ‚Ñìth column of ‚àÜis an (‚Ñì‚àí1)√ó1
vector of ‚àí1s. That is,
‚àÜ=
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
1
0
¬∑ ¬∑ ¬∑
0
‚àí1
0
1
¬∑ ¬∑ ¬∑
0
‚àí1
...
...
...
...
...
0
0
. . .
1
‚àí1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª,
and ‚àÜ‚â°In ‚äó‚àÜ.
In the latent variable representation of the multinomial probit regression model, the
observed, Yis are related to the ÀúUis rather than the ÀúZis. To illustrate, consider ÀúZi, where
103

Yi = j if ÀúZij = max ÀúZi. When ÀúZi‚Ñìis the maximum of ÀúZi, ÀúUij = ÀúZij ‚àíÀúZi‚Ñì< 0 for all
j = 1, . . . , (‚Ñì‚àí1). Otherwise, if ÀúZiJ is the maximum of ÀúZi, where J ‚àà{1, . . . , ‚Ñì‚àí1}, then
ÀúUiJ = ÀúZiJ ‚àíÀúZi‚Ñì> 0 and ÀúUiJ = ÀúZiJ ‚àíÀúZi‚Ñì> ÀúZij ‚àíÀúZi‚Ñì= ÀúUij for all j Ã∏= J ‚àà{1, . . . , ‚Ñì‚àí1}.
Thus, we express (5.1) and (5.2) as
Yi =
(
‚Ñì, if max ÀúUi < 0
j, if max ÀúUi = ÀúUij > 0
(5.3)
where
vec( ÀúU) = ‚àÜvec( ÀúZ) ‚àºN(‚àÜÀú¬µ, ‚àÜÀúŒ£‚àÜ‚Ä≤).
(5.4)
In the following subsections, we consider a regression framework and discuss various
speciÔ¨Åcations of the latent variable mean and covariance and the interpretation of the asso-
ciated model parameters.
5.1.1
Latent Mean SpeciÔ¨Åcation
As discussed in Section 2.2, the parameters of the binary Bayesian probit regression
model are only identiÔ¨Åable up to a multiplicative constant. This is also the case in the
multi-category setting. However, without loss of generality, in the following discussion
concerning the latent mean structure, we express the model only in terms of the identiÔ¨Åable
parameters Z, U, and ¬µ. More discussion on identiÔ¨Åability in the latent variable represen-
tation of the Bayesian spatial multinomial probit model will be provided in Section 5.2.1.
Analogous to the binary response setting, a multinomial analysis seeks to determine the
relationship between the response variable and covariate information. To determine such a
relationship, we consider three types of potential covariate information:
A. Category-speciÔ¨Åc covariates
B. Location-speciÔ¨Åc covariates
104

C. Location-category-speciÔ¨Åc covariates.
To help motivate these different types of covariate information, consider a land cover
example. Category-speciÔ¨Åc covariates are information that may vary across categories, but
are constant across locations. For example, suppose that across our study region, each land
cover type has an associated tax. We might expect that land cover categories with higher
taxes may be less likely to be observed. Location-speciÔ¨Åc covariates contain information
that varies across locations, but for a speciÔ¨Åc location, is constant across categories. Eleva-
tion is an example of a location-speciÔ¨Åc covariate because it changes for each location, but
at each location it is the same no matter the category. Finally, location-category-speciÔ¨Åc
covariates are information that varies across both locations and categories. One example of
a location-category-speciÔ¨Åc covariate is the value, or price, of each land cover for a speciÔ¨Åc
parcel of land. At each location, or at smaller subregions within our study area, the price
of each land cover type is known and changes across locations or subregions. We would
also expect this type of information to be related to the land cover response variable. For
the multinomial probit regression model, including each of the three types of information
in an analysis requires different speciÔ¨Åcations of the latent mean structure. In this section,
we consider how each of the three types of covariate information can be accommodated in
the latent mean speciÔ¨Åcation within a regression framework, i.e., ¬µ = XŒ≤.
Suppose we have k(A) covariates representing category-speciÔ¨Åc information, k(B)
covariates representing location-speciÔ¨Åc information, and k(C) covariates representing
location-category-speciÔ¨Åc information. Let X = [X(A), X(B), X(C)] where X(A) des-
ignates the covariates capturing category-speciÔ¨Åc information, X(B) designates the covari-
ates capturing location-speciÔ¨Åc information, and X(C) designates the covariates capturing
location-category-speciÔ¨Åc information. A category-speciÔ¨Åc intercept can be considered a
105

special case of a category-speciÔ¨Åc covariate, and is included in X(A), as discussed in Sec-
tion A below. Similarly, let Œ≤ = (Œ≤(A)‚Ä≤, Œ≤(B)‚Ä≤, Œ≤(C)‚Ä≤)‚Ä≤, where Œ≤(A), Œ≤(B), and Œ≤(C) are the
collections of coefÔ¨Åcients corresponding to X(A), X(B), and X(C), respectively. The exact
dimension of each of these covariate and coefÔ¨Åcient matrices will depend on the model
speciÔ¨Åcation and will be discussed further in the following subsections.
In the following subsections, we discuss two ways to include each type of information
in the model. The Ô¨Årst is to assume that the relationship between each covariate and each
of the categories is the same for all categories. This means that each covariate will have a
single regression coefÔ¨Åcient for all categories. The second is to assume that the relationship
between each covariate and each category may be different. In this case, for each covariate
we Ô¨Åt a separate regression coefÔ¨Åcient for each category. In discussing these two cases,
we only consider continuous covariates, but the methods could be extended for discrete or
categorical covariate information. For each type of information, we consider ¬µ = XŒ≤ and
the impact the speciÔ¨Åcation of the design matrix has on ‚àÜ¬µ = ‚àÜXŒ≤.
A. Category-SpeciÔ¨Åc Information
Let x(A)
m
= (x(A)
m1, . . . , x(A)
m‚Ñì)‚Ä≤, where x(A)
mj is the mth category-speciÔ¨Åc covariate for the
jth category, for m = 1, . . . , k(A). For the moment, we assume the mean only includes
category-speciÔ¨Åc information, i.e., ¬µ = X(A)Œ≤(A). We also include the intercept in the
model in the standard way by including a column of ones in X(A). Note that the meaning
of x(A)
m remains constant for each case discussed below. However, we redeÔ¨Åne X(A) based
on x(A)
m for m = 1, . . . , k(A), to distinguish between the cases.
Case 1a: The regression coefÔ¨Åcients and intercept are identical across categories.
106

Let
X(A) ‚â°X(A1a) = 1n ‚äó
h
1‚Ñì
x(A)
1
¬∑ ¬∑ ¬∑
x(A)
k(A)
i
so that X(A1a) is an n‚Ñì√ó (k(A) + 1) matrix of covariates, and let
Œ≤(A) ‚â°Œ≤(A1a) = (Œ≤(A1a)
0
, Œ≤(A1a)
1
, . . . , Œ≤(A1a)
k(A) )‚Ä≤,
so that Œ≤(A1a) is a (k(A) + 1) √ó 1 vector of coefÔ¨Åcients.
Consider ¬µ = X(A)Œ≤(A):
¬µ = X(A1a)Œ≤(A1a) =

1n ‚äó
h
1‚Ñì
x(A)
1
. . .
x(A)
k(A)
i
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
Œ≤(A1a)
0
Œ≤(A1a)
1 ...
Œ≤(A1a)
k(A)
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª
= 1n ‚äó
h
Œ≤(A1a)
0
1‚Ñì+ Œ≤(A1a)
1
x(A)
1
+ ¬∑ ¬∑ ¬∑ + Œ≤(A1a)
k(A) x(A)
k(A)
i
It follows that the mean for the latent Zij is
¬µij = Œ≤(A1a)
0
+ Œ≤(A1a)
1
x(A)
1j + ¬∑ ¬∑ ¬∑ + Œ≤(A1a)
k(A)j x(A)
k(A)j.
for i = 1, . . . , n and j = 1, . . . , ‚Ñì. In this case, Œ≤(A1a)
m
captures the increase in the
mean of the latent variable Zij for a one unit increase in the mth covariate x(A)
mj , for
m = 1, . . . , k(A). Œ≤(A1a)
0
is the intercept. These parameters are constant across all
categories. Furthermore, because the category-speciÔ¨Åc covariates are constant across
locations, ¬µij = ¬µi‚àój for all i, i‚àó= 1, . . . , n.
107

Now consider the mean of the latent variable U, ‚àÜ¬µ = ‚àÜX(A)Œ≤(A):
‚àÜ¬µ = ‚àÜX(A1a)Œ≤(A1a)
= ‚àÜ

1n ‚äó
h
1‚Ñì
x(A)
1
¬∑ ¬∑ ¬∑
x(A)
k(A)
i
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
Œ≤(A1a)
0
Œ≤(A1a)
1 ...
Œ≤(A1a)
k(A)
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª
= (In ‚äó‚àÜ)

1n ‚äó
h
1‚Ñì
x(A)
1
¬∑ ¬∑ ¬∑
x(A)
k(A)
i
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
Œ≤(A1a)
0
Œ≤(A1a)
1 ...
Œ≤(A1a)
k(A)
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª
= 1n ‚äó
h
Œ≤(A1a)
0
‚àÜ1‚Ñì+ Œ≤(A1a)
1
‚àÜx(A)
1
+ ¬∑ ¬∑ ¬∑ + Œ≤(A1a)
k(A) ‚àÜx(A)
k(A)
i
.
In this case,
‚àÜ1‚Ñì=
Ô£Æ
Ô£ØÔ£∞
1 ‚àí1
...
1 ‚àí1
Ô£π
Ô£∫Ô£ª
so that
Œ≤(A1a)
0
‚àÜ1‚Ñì= 0‚Ñì‚àí1
where 0‚Ñì‚àí1 is an (‚Ñì‚àí1) √ó 1 vector of zeros, and
‚àÜx(A)
m =
Ô£Æ
Ô£ØÔ£∞
x(A)
m1 ‚àíx(A)
m‚Ñì
...
x(A)
m(‚Ñì‚àí1) ‚àíx(A)
m‚Ñì
Ô£π
Ô£∫Ô£ª
(5.5)
so that
Œ≤(A1a)
m
‚àÜx(A)
m =
Ô£Æ
Ô£ØÔ£∞
Œ≤(A1a)
m
(x(A)
m1 ‚àíx(A)
m‚Ñì)
...
Œ≤(A1a)
m
(x(A)
m(‚Ñì‚àí1) ‚àíx(A)
m‚Ñì)
Ô£π
Ô£∫Ô£ª,
(5.6)
for m = 1, . . . , k(A). Thus, the mean for the latent Uij is
(‚àÜ¬µ)ij = Œ≤(A1a)
1
(x(A)
1j ‚àíx(A)
1‚Ñì) + ¬∑ ¬∑ ¬∑ + Œ≤(A1a)
k(A) (x(A)
k(A)j ‚àíx(A)
k(A)‚Ñì)
108

where (‚àÜ¬µ)ij is the ijth element of the vector ‚àÜ¬µ for i = 1, . . . , n and j =
1, . . . , (‚Ñì‚àí1).
We can interpret Œ≤(A1a)
m
in terms of comparison to the ‚Ñìth category, so that Œ≤(A1a)
m
is the increase in the mean of the latent variable Uij for a one unit increase in the
difference between x(A)
mj and x(A)
m‚Ñì. This value is constant across all categories. Just
as ¬µij = ¬µi‚àój, (‚àÜ¬µ)ij = (‚àÜ¬µ)i‚àój for all i, i‚àó= 1, . . . , n. Finally note that since
the intercept is identical across categories, there is effectively no intercept when we
compare the jth category to the ‚Ñìth category. For this reason, in Case 1b, we consider
allowing the intercept to vary across categories.
Case 1b: The regression coefÔ¨Åcients are identical across categories, but each category has
a unique intercept.
Let
X(A) ‚â°X(A1b) = 1n ‚äó
h
In
x(A)
1
¬∑ ¬∑ ¬∑
x(A)
k(A)
i
so that X(A1b) is an n‚Ñì√ó (k(A) + ‚Ñì) matrix of covariates and let
Œ≤(A) ‚â°Œ≤(A1b) = (Œ≤(A1b)‚Ä≤
0
, Œ≤(A1b)
1
, . . . , Œ≤(A1b)
k(A) )‚Ä≤,
where Œ≤(A1b)
0
= (Œ≤(A1b)
01
, . . . , Œ≤(A1b)
0‚Ñì
)‚Ä≤. Œ≤(A1b) is thus a (k(A) + ‚Ñì) √ó 1 vector of coefÔ¨Å-
cients in this case.
109

Again consider ¬µ:
¬µ = X(A1b)Œ≤(A1b)
=

1n ‚äó
h
I‚Ñì
x(A)
1
¬∑ ¬∑ ¬∑
x(A)
k(A)
i
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
Œ≤(A1b)
0
Œ≤(A1b)
1 ...
Œ≤(A1b)
k(A)
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª
= 1n ‚äó
h
I‚ÑìŒ≤(A1b)
0
+ Œ≤(A1b)
1
x(A)
1
+ ¬∑ ¬∑ ¬∑ + Œ≤(A1b)
k(A) x(A)
k(A)
i
.
It follows that the mean of the latent Zij is
¬µij = Œ≤(A1b)
0j
+ Œ≤(A1b)
1
x(A)
1j + ¬∑ ¬∑ ¬∑ + Œ≤(A1b)
k(A) x(A)
k(A)j
for i = 1, . . . , n and j = 1, . . . , ‚Ñì. As in Case 1a, the covariate coefÔ¨Åcients, Œ≤(A1b)
m
for m = 1, . . . , k(A), are constant across categories and represent the increase in
the mean of the latent variable Zij for a one unit increase in the mth covariate x(A)
mj ,
for m = 1, . . . , k(A). This speciÔ¨Åcation, however, allows for a category-speciÔ¨Åc
intercept, so that Œ≤(A1b)
0j
is the intercept for category j. The category means are still
the same across locations, i.e., ¬µij = ¬µi‚àój for all i, i‚àó= 1, . . . , n.
Now consider ‚àÜ¬µ:
‚àÜ¬µ = ‚àÜX(A1b)Œ≤(A1b)
= ‚àÜ

1n ‚äó
h
I‚Ñì
x(A)
1
¬∑ ¬∑ ¬∑
x(A)
k(A)
i
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
Œ≤(A1b)
0
Œ≤(A1b)
1 ...
Œ≤(A1b)
k(A)
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª
= (In ‚äó‚àÜ)

1n ‚äó
h
I‚Ñì
x(A)
1
¬∑ ¬∑ ¬∑
x(A)
k(A)
i
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
Œ≤(A1b)
0
Œ≤(A1b)
1 ...
Œ≤(A1b)
k(A)
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª
= 1n ‚äó
h
‚àÜI‚ÑìŒ≤(A1b)
0
+ Œ≤(A1b)
1
‚àÜx(A)
1
+ ¬∑ ¬∑ ¬∑ + Œ≤(A1b)
k(A) ‚àÜx(A)
k(A)
i
.
110

In this case,
‚àÜI‚Ñì=
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
1
0
¬∑ ¬∑ ¬∑
0
‚àí1
0
1
¬∑ ¬∑ ¬∑
0
‚àí1
...
...
...
...
...
0
0
¬∑ ¬∑ ¬∑
1
‚àí1
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª
(5.7)
so that
‚àÜI‚ÑìŒ≤(A1b)
0
=
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£∞
Œ≤(A1b)
01
‚àíŒ≤(A1b)
0‚Ñì
Œ≤(A1b)
02
‚àíŒ≤(A1b)
0‚Ñì
...
Œ≤(A1b)
0(‚Ñì‚àí1) ‚àíŒ≤(A1b)
0‚Ñì
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
(5.8)
and ‚àÜx(A)
m and Œ≤(A1b)
m
‚àÜx(A)
m are the same as (5.5) and (5.6) with Œ≤(A1b)
m
= Œ≤(A1a)
m
for
m = 1, . . . , k(A). Thus, the mean for the latent Uij is
(‚àÜ¬µ)ij = Œ≤(A1b)
0j
‚àíŒ≤(A1b)
0‚Ñì
+ Œ≤(A1b)
1
(x(A)
1j ‚àíx(A)
1‚Ñì) + ¬∑ ¬∑ ¬∑ + Œ≤(A1b)
k(A) (x(A)
k(A)j ‚àíx(A)
k(A)‚Ñì)
for i = 1, . . . , n and j = 1, . . . , (‚Ñì‚àí1).
The intercept component of the mean ‚àÜ¬µ is the difference between the jth and ‚Ñìth
category intercepts, or Œ≤(A1b)
0j
‚àíŒ≤(A1b)
0‚Ñì
, for j = 1, . . . , (‚Ñì‚àí1). As in Case 1a, the
regression coefÔ¨Åcients Œ≤(A1b) represent the increase in the mean of Uij due to a one
unit increase in the difference between x(A)
mj and x(A)
m‚Ñì. Also, we again have that the
category means are constant across locations, i.e., (‚àÜ¬µ)ij = (‚àÜ¬µ)i‚àój for all i, i‚àó=
1, . . . , n.
Case 2: Each covariate has a unique regression coefÔ¨Åcient for each category.
The two previous cases require that the increase in the latent variable mean for the
jth category is the same for all categories. Suppose, however, that we want to allow
for distinct coefÔ¨Åcients across categories. In the land cover example, this alternative
assumption may be appropriate when including land taxes in the model. The effect
111

of a tax increase may differ across land cover types. For example, a one percent
increase in the tax for a residential parcel of land may not be as important as a similar
tax increase imposed on a commercial parcel of land. Here we discuss how category-
speciÔ¨Åc covariates can differ according to outcome category.
In this case, let
D(A2)
m
= diag(x(A)
m ) =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
x(A)
m1
0
¬∑ ¬∑ ¬∑
0
0
x(A)
m2
¬∑ ¬∑ ¬∑
0
...
...
...
...
0
0
¬∑ ¬∑ ¬∑
x(A)
m‚Ñì
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª
and let
X(A) ‚â°X(A2) = 1n ‚äó
h
I‚Ñì
D(A2)
1
¬∑ ¬∑ ¬∑
D(A2)
k(A)
i
,
so X(A2) is a n‚Ñì√ó (k(1) + 1)‚Ñìmatrix of covariates. Furthermore, let
Œ≤(A) ‚â°Œ≤(A2) = (Œ≤(A2)‚Ä≤
0
, Œ≤(A2)‚Ä≤
1
, . . . , Œ≤(A2)‚Ä≤
k(A) )‚Ä≤,
where Œ≤(A2)
m
= (Œ≤(A2)
m1 , . . . , Œ≤(A2)
m‚Ñì)‚Ä≤ for m = 0, 1, . . . , k(A) so that Œ≤(A2) is a
(k(A) + 1)‚Ñì√ó 1 vector of coefÔ¨Åcients.
Consider ¬µ:
¬µ = X(A2)Œ≤(A2)
=

1n ‚äó
h
I‚Ñì
D(A2)
1
¬∑ ¬∑ ¬∑
D(A2)
k(A)
i
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
Œ≤(A2)
0
Œ≤(A2)
1...
Œ≤(A2)
k(A)
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª
= 1n ‚äó
h
I‚ÑìŒ≤(A2)
0
+ D(A2)
1
Œ≤(A2)
1
+ ¬∑ ¬∑ ¬∑ + D(A2)
k(A) Œ≤(A2)
k(A)
i
.
This implies that the mean of the latent Zij is
¬µij = Œ≤(A2)
0j
+ Œ≤(A2)
1j
x(A)
1j + ¬∑ ¬∑ ¬∑ + Œ≤(A2)
k(A)jx(A)
k(A)j,
112

for i = 1, . . . , n and j = 1, . . . , ‚Ñì. This time, Œ≤(A2)
mj
represents the increase in the
mean of the latent variable Zij due to a one unit increase in the mth covariate for the
jth category, x(A)
mj . Note that the regression coefÔ¨Åcients differ across category.
Now consider ‚àÜ¬µ:
‚àÜ¬µ = ‚àÜX(A2)Œ≤(A2)
= ‚àÜ

1n ‚äó
h
I‚Ñì
D(A2)
1
¬∑ ¬∑ ¬∑
D(A2)
k(A)
i
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
Œ≤(A2)
0
Œ≤(A2)
1...
Œ≤(A2)
k(A)
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª
= (In ‚äó‚àÜ)

1n ‚äó
h
I‚Ñì
D(A2)
1
¬∑ ¬∑ ¬∑
D(A2)
k(A)
i
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
Œ≤(A2)
0
Œ≤(A2)
1...
Œ≤(A2)
k(A)
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª
= 1n ‚äó
h
‚àÜI‚ÑìŒ≤(A2)
0
+ ‚àÜD(A2)
1
Œ≤(A2)
1
+ ¬∑ ¬∑ ¬∑ + ‚àÜD(A2)
k(A) Œ≤(A2)
k(A)
i
,
where ‚àÜI‚ÑìŒ≤(A2)
0
is identical to the expression given in (5.8) with Œ≤(A2)
0
= Œ≤(A1b)
0
and
‚àÜD(A2)
m
=
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£∞
x(A)
m1
0
¬∑ ¬∑ ¬∑
0
‚àíx(A)
m‚Ñì
0
x(A)
m2
¬∑ ¬∑ ¬∑
0
‚àíx(A)
m‚Ñì
...
...
...
...
...
0
0
¬∑ ¬∑ ¬∑
x(A)
m(‚Ñì‚àí1)
‚àíx(A)
m‚Ñì
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
so that
‚àÜD(A2)
m
Œ≤(A2)
m
=
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£∞
Œ≤(A2)
m1 x(A)
m1 ‚àíŒ≤(A2)
m‚Ñìx(A)
m‚Ñì
Œ≤(A2)
m2 x(A)
m2 ‚àíŒ≤(A2)
m‚Ñìx(A)
m‚Ñì
...
Œ≤(A2)
m(‚Ñì‚àí1)x(A)
m(‚Ñì‚àí1) ‚àíŒ≤(A2)
m‚Ñìx(A)
m‚Ñì
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
Thus, the mean for the latent Uij is
(‚àÜ¬µ)ij = (Œ≤(A2)
0j
‚àíŒ≤(A2)
0‚Ñì
) + (Œ≤(A2)
1j
x(A)
1j ‚àíŒ≤(A2)
1‚Ñì
x(A)
1‚Ñì) + ¬∑ ¬∑ ¬∑ + (Œ≤(A2)
k(A)jx(A)
k(A)j ‚àíŒ≤(A2)
j(A)‚Ñìx(A)
k(A)‚Ñì).
113

Here, the mean of the latent variable Uij increases by Œ≤(A2)
mj
for a one unit increase
in x(A)
mj , but decreases by Œ≤(A2)
m‚Ñì
for a one unit increase in x(A)
m‚Ñì. As desired, this case
allows categories to have distinct regression coefÔ¨Åcients. However, many of the pa-
rameters will not be estimable due to collinearity as we discuss below.
First, we discuss collinearity within each covariate. The ‚Ñìth column of both ‚àÜI‚Ñìand
‚àÜD(A2)
m
can be written as linear combinations of the Ô¨Årst ‚Ñì‚àí1 columns. To see this
for ‚àÜI‚Ñì, we Ô¨Årst deÔ¨Åne the notation (M)¬∑j to be the jth column of the matrix M.
Using this notation, (‚àÜI‚Ñì)¬∑j is the jth column of ‚àÜI‚Ñìfor j = 1, . . . , ‚Ñì, and we have
that
(‚àÜI‚Ñì)¬∑‚Ñì=
‚Ñì‚àí1
X
j=1
‚àí1 (‚àÜI‚Ñì)¬∑j.
Obviously, this relationship will hold for all i locations, and the columns of 1n ‚äó
‚àÜI‚Ñìwill also be collinear. Thus, it is not appropriate to Ô¨Åt an ‚Ñìth category-speciÔ¨Åc
intercept in a multinomial probit regression model. Rather than Ô¨Åtting both Œ≤(A2)
0j
and
Œ≤(A2)
0‚Ñì
, we instead Ô¨Åt the difference (Œ≤(A2)
0j
‚àíŒ≤(A2)
0‚Ñì
). [Does this satsify, ‚Äùso what do
we do?‚Äù]
For ‚àÜD(A2)
m
, a similar relationship holds. Taking (‚àÜD(A2)
m
)¬∑j to be the jth column of
‚àÜD(A2)
m
for j = 1, . . . , ‚Ñì, we have that
(‚àÜD(A2)
m
)¬∑‚Ñì=
‚Ñì‚àí1
X
j=1
‚àíx(A)
m‚Ñì
x(A)
mj
(‚àÜD(A2)
m
)¬∑j.
Because the category-speciÔ¨Åc covariates are constant across locations by deÔ¨Ånition,
this relationship will hold across all locations i = 1, . . . , n, and the columns of 1n ‚äó
‚àÜD(A2)
m
will also be collinear.
We now discuss the collinearity between the covariates and intercept component of
the design matrix. Notice that the columns of ‚àÜI‚Ñìand ‚àÜD(A2)
m
for m = 1, . . . , k(A)
114

are collinear:
(‚àÜI‚Ñì)¬∑j =
1
x(A)
1j
(‚àÜD(A2)
1
)¬∑j = ¬∑ ¬∑ ¬∑ =
1
x(A)
k(A)j
(‚àÜD(A2)
k(A) )¬∑j .
Because the covariates are constant across locations, the columns making up the
design matrix will also be collinear. Thus, in Case 2 where regression coefÔ¨Åcients
vary across categories, only the coefÔ¨Åcients corresponding to one covariate or the
intercept can be identiÔ¨Åed. If we wish to include further category-speciÔ¨Åc covariates,
we must include them as in Case 1b.
In Case 1a and 1b, we saw that it is necessary to Ô¨Åt a category-speciÔ¨Åc intercept. In Case
2 we saw, however, that category-speciÔ¨Åc coefÔ¨Åcients and intercept are not identiÔ¨Åable.
Thus, it is necessary to express X(A) using Case 1b, noting that the intercept could be
replaced by a single category-speciÔ¨Åc covariate.
Thus,
X(A) = 1n ‚äó
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
1
0
¬∑ ¬∑ ¬∑
0
x(A)
11
¬∑ ¬∑ ¬∑
x(A)
k(A)1
0
1
¬∑ ¬∑ ¬∑
0
x(A)
12
¬∑ ¬∑ ¬∑
x(A)
k(A)2
...
...
...
...
...
...
...
0
0
¬∑ ¬∑ ¬∑
1
x(A)
1(‚Ñì‚àí1)
¬∑ ¬∑ ¬∑
x(A)
k(A)(‚Ñì‚àí1)
0
0
¬∑ ¬∑ ¬∑
0
x(A)
1‚Ñì
¬∑ ¬∑ ¬∑
x(A)
k(A)‚Ñì
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
and
Œ≤(A) = (Œ≤(A)‚Ä≤
0
, Œ≤(A)
1
, . . . , Œ≤(A)
k(A))‚Ä≤
where Œ≤(A)
0
= (Œ≤(A)
01 , . . . , Œ≤(A)
0(‚Ñì‚àí1))‚Ä≤.
B. Location-SpeciÔ¨Åc Information
Let x(B)
m
= (x(B)
m1 , . . . , x(B)
mn)‚Ä≤ where x(B)
mi is the mth location-speciÔ¨Åc covariate for the
ith location, for m = 1, . . . , k(B). In this discussion, we assume the mean of the latent Z
only includes location-speciÔ¨Åc information, i.e., ¬µ = X(B)Œ≤(B). Note that the meaning of
115

x(B)
m remains constant for each case discussed below. However, we redeÔ¨Åne X(B) based on
x(B)
m for m = 1, . . . , k(B), to distinguish between the cases.
Case 1: The regression coefÔ¨Åcients are identical across categories.
Let
X(B) ‚â°X(B1) =
h
x(B)
1
¬∑ ¬∑ ¬∑
x(B)
k(B)
i
‚äó1‚Ñì
so that X(B1) is a n‚Ñì√ó k(B) matrix of location-speciÔ¨Åc covariates, and let
Œ≤(B) ‚â°Œ≤(B1) = (Œ≤(B1)
1
, . . . , Œ≤(B1)
k(B) )‚Ä≤
so that Œ≤(B1) is a k(B) √ó 1 vector of coefÔ¨Åcients.
Now consider ¬µ:
¬µ = X(B1)Œ≤(B1) =
h
x(B)
1
. . .
x(B)
k(B)
i
‚äó1‚Ñì

Ô£Æ
Ô£ØÔ£∞
Œ≤(B1)
1...
Œ≤(B1)
k(B)
Ô£π
Ô£∫Ô£ª
=
h
Œ≤(B1)
1
(x(B)
1
‚äó1‚Ñì) + ¬∑ ¬∑ ¬∑ + Œ≤(B1)
k(B) (x(B)
k(B) ‚äó1‚Ñì)
i
so that the mean of the latent Zij is
¬µij = Œ≤(B1)
1
x(B)
1i + ¬∑ ¬∑ ¬∑ + Œ≤(B1)
k(B) x(B)
k(B)i,
for i = 1, . . . , n and j = 1, . . . , ‚Ñì. Here, Œ≤(B1)
m
represents the increase in the mean of
the latent variable Zij corresponding to a one unit increase in x(B)
mi .
116

Now consider ‚àÜ¬µ:
‚àÜ¬µ = ‚àÜX(B1)Œ≤(B1) = ‚àÜ
h
x(B)
1
¬∑ ¬∑ ¬∑
x(B)
k(B)
i
‚äó1‚Ñì

Ô£Æ
Ô£ØÔ£∞
Œ≤(B1)
1...
Œ≤(B1)
k(B)
Ô£π
Ô£∫Ô£ª
= (In ‚äó‚àÜ)
h
x(B)
1
‚äó1‚Ñì
¬∑ ¬∑ ¬∑
x(B)
k(B) ‚äó1‚Ñì
i
Ô£Æ
Ô£ØÔ£∞
Œ≤(B1)
1...
Œ≤(B1)
k(B)
Ô£π
Ô£∫Ô£ª
=
h
Œ≤(B1)
1
(x(B)
1
‚äó‚àÜ1‚Ñì) + ¬∑ ¬∑ ¬∑ + Œ≤(B1)
k(B) (x(B)
k(B) ‚äó‚àÜ1‚Ñì)
i
=
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£∞
Œ≤(B1)
1
x(B)
11 ‚àÜ1‚Ñì+ ¬∑ ¬∑ ¬∑ + Œ≤(B1)
k(B) x(B)
k(B)1‚àÜ1‚Ñì
Œ≤(B1)
1
x(B)
12 ‚àÜ1‚Ñì+ ¬∑ ¬∑ ¬∑ + Œ≤(B1)
k(B) x(B)
k(B)2‚àÜ1‚Ñì
...
Œ≤(B1)
1
x(B)
1n ‚àÜ1‚Ñì+ ¬∑ ¬∑ ¬∑ + Œ≤(B1)
k(B) x(B)
k(B)n‚àÜ1‚Ñì
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
,
where
x(B)
mi ‚àÜ1‚Ñì= x(B)
mi
Ô£Æ
Ô£ØÔ£∞
(1 ‚àí1)
...
(1 ‚àí1)
Ô£π
Ô£∫Ô£ª=
Ô£Æ
Ô£ØÔ£∞
x(B)
mi ‚àíx(B)
mi
...
x(B)
mi ‚àíx(B)
mi
Ô£π
Ô£∫Ô£ª= 0‚Ñì‚àí1
and
Œ≤(B1)
1
x(B)
mi ‚àÜ1‚Ñì= 0‚Ñì‚àí1,
for i = 1, . . . , n and m = 1, . . . , k(B). As a result, the mean of the latent Uij is
(‚àÜ¬µ)ij = 0,
for i = 1, . . . , n and j = 1, . . . , ‚Ñì.
Thus, when including location-speciÔ¨Åc covariates in a multinomial probit regression
model, regression coefÔ¨Åcients that are constant across categories are not identiÔ¨Åable.
Therefore, for location-speciÔ¨Åc covariates, we must specify a unique regression co-
efÔ¨Åcient for each category, as in Case 2.
Case 2: Each covariate has a unique regression coefÔ¨Åcient for each category.
117

Let
X(B) ‚â°X(B2) =
h
x(B)
1
¬∑ ¬∑ ¬∑
x(B)
k(B)
i
‚äóI‚Ñì
so that X(B2) is a n‚Ñì√ó k(B)‚Ñìmatrix of location-speciÔ¨Åc covariates. Let
Œ≤(B) ‚â°Œ≤(B2) = (Œ≤(B2)‚Ä≤
1
, . . . , Œ≤(B2)‚Ä≤
k(B) )‚Ä≤,
where Œ≤(B2)
m
= (Œ≤(B2)
m1 , . . . , Œ≤(B2)
m‚Ñì)‚Ä≤ so that Œ≤(B2) is a k(B)‚Ñì√ó 1 vector of coefÔ¨Åcients.
Consider ¬µ:
¬µ = X(B2)Œ≤(B2) =
h
x(B)
1
¬∑ ¬∑ ¬∑
x(B)
k(B)
i
‚äóI‚Ñì

Ô£Æ
Ô£ØÔ£∞
Œ≤(B2)
1...
Œ≤(B2)
k(B)
Ô£π
Ô£∫Ô£ª
=
h
(x(B)
1
‚äóI‚Ñì)Œ≤(B2)
1
+ ¬∑ ¬∑ ¬∑ + (x(B)
k(B) ‚äóI‚Ñì)Œ≤(B2)
k(B)
i
so that the mean for the latent Zij is
¬µij = Œ≤(B2)
1j
x(B)
1i + ¬∑ ¬∑ ¬∑ + Œ≤(B2)
k(B)jx(B)
k(B)i,
for i = 1, . . . , n and j = 1, . . . , ‚Ñì. Here, Œ≤(B2)
mj
represents the increase in the mean of
the latent variable Zij for a one unit increase in x(B)
mi , for m = 1, . . . , k(B), and each
Œ≤(B2)
mj
is allowed to be different across the ‚Ñìcategories.
118

Now consider ‚àÜ¬µ:
‚àÜ¬µ = ‚àÜX(B2)Œ≤(B2) = ‚àÜ
h
x(B)
1
¬∑ ¬∑ ¬∑
x(B)
k(B)
i
‚äóI‚Ñì

Ô£Æ
Ô£ØÔ£∞
Œ≤(B2)
1...
Œ≤(B2)
k(B)
Ô£π
Ô£∫Ô£ª
= (In ‚äó‚àÜ)
h
x(B)
1
‚äóI‚Ñì
¬∑ ¬∑ ¬∑
x(B)
k(B) ‚äóI‚Ñì
i
Ô£Æ
Ô£ØÔ£∞
Œ≤(B2)
1...
Œ≤(B2)
k(B)
Ô£π
Ô£∫Ô£ª
=
h
(x(B)
1
‚äó‚àÜ)Œ≤(B2)
1
+ ¬∑ ¬∑ ¬∑ + (x(B)
k(B) ‚äó‚àÜ)Œ≤(B2)
k(B)
i
=
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£∞
x(B)
11 ‚àÜŒ≤(B2)
1
+ ¬∑ ¬∑ ¬∑ + x(B)
k(B)1‚àÜŒ≤(B2)
k(B)
x(B)
12 ‚àÜŒ≤(B2)
1
+ ¬∑ ¬∑ ¬∑ + x(B)
k(B)2‚àÜŒ≤(B2)
k(B)
...
x(B)
1n ‚àÜŒ≤(B2)
1
+ ¬∑ ¬∑ ¬∑ + x(B)
k(B)n‚àÜŒ≤(B2)
k(B)
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
where
x(B)
mi ‚àÜ=
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
x(B)
mi
0
¬∑ ¬∑ ¬∑
0
‚àíx(B)
mi
0
x(B)
mi
¬∑ ¬∑ ¬∑
0
‚àíx(B)
mi
...
...
...
...
...
0
0
¬∑ ¬∑ ¬∑
x(B)
mi
‚àíx(B)
mi
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª
and
x(B)
mi ‚àÜŒ≤(B2)
m
=
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
x(B)
mi
0
¬∑ ¬∑ ¬∑
0
‚àíx(B)
mi
0
x(B)
mi
¬∑ ¬∑ ¬∑
0
‚àíx(B)
mi
...
...
...
...
...
0
0
¬∑ ¬∑ ¬∑
x(B)
mi
‚àíx(B)
mi
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£ØÔ£∞
Œ≤(B2)
m1
Œ≤(B2)
m2...
Œ≤(B2)
m(‚Ñì‚àí1)
Œ≤(B2)
m‚Ñì
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
=
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£∞
Œ≤(B2)
m1 x(B)
mi ‚àíŒ≤(B2)
m‚Ñìx(B)
mi
Œ≤(B2)
m2 x(B)
mi ‚àíŒ≤(B2)
m‚Ñìx(B)
mi
...
Œ≤(B2)
m(‚Ñì‚àí1)x(B)
mi ‚àíŒ≤(B2)
m‚Ñìx(B)
mi
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
=
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£∞
(Œ≤(B2)
m1
‚àíŒ≤(B2)
m‚Ñì)x(B)
mi
(Œ≤(B2)
m2
‚àíŒ≤(B2)
m‚Ñì)x(B)
mi
...
(Œ≤(B2)
m(‚Ñì‚àí1) ‚àíŒ≤(B2)
m‚Ñì)x(B)
mi
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
.
119

Thus, the mean of the latent Uij is
(‚àÜ¬µ)ij = (Œ≤(B2)
1j
‚àíŒ≤(B2)
1‚Ñì
)x(B)
1i + ¬∑ ¬∑ ¬∑ + (Œ≤(B2)
k(B)j ‚àíŒ≤(B2)
k(B)‚Ñì)x(B)
k(B)i
for i = 1, . . . , n and j = 1, . . . , ‚Ñì.
Here, Œ≤(B2)
mj
‚àíŒ≤(B2)
m‚Ñì
represents the increase in the mean of the latent variable Uij due
to a one unit increase in x(B)
mi . Note that here we have a similar collinearity problem
to that seen in Case 2 for the category-speciÔ¨Åc covariates: the ‚Ñìth column of x(B)
mi ‚àÜ
can be written as a linear combination of the Ô¨Årst ‚Ñì‚àí1 columns. Therefore, Œ≤(B2)
m‚Ñì
will not be estimable.
In Case 1, we saw that we cannot estimate regression coefÔ¨Åcients corresponding to
location-speciÔ¨Åc covariates when the regression coefÔ¨Åcients were assumed to be constant
across categories. We can, however, estimate ‚Ñì‚àí1 distinct coefÔ¨Åcients. Thus, when speci-
fying the design matrix for location-speciÔ¨Åc covariates, let
X(B) =
h
x(B)
1
¬∑ ¬∑ ¬∑
x(B)
k(B)
i
‚äó
I‚Ñì‚àí1
0‚Ä≤
‚Ñì‚àí1

and
Œ≤(B) = (Œ≤(B)‚Ä≤
1
, . . . , Œ≤(B)‚Ä≤
k(B))‚Ä≤,
where Œ≤(B)
m
= (Œ≤(B)
m1 , . . . , Œ≤(B)
m(‚Ñì‚àí1))‚Ä≤.
C. Location-Category-SpeciÔ¨Åc Information
Let x(C)
mi = (x(C)
mi1, . . . , x(C)
mi‚Ñì)‚Ä≤ be an ‚Ñì√ó 1 vector of covariates for each location i =
1, . . . , n, for m = 1, . . . , k(C) covariates. In this discussion, we assume the mean only in-
cludes location-category-speciÔ¨Åc information, i.e., ¬µ = X(C)Œ≤(C). Note that the meaning
of x(C)
mi remains constant for each case discussed below. However, we redeÔ¨Åne X(C) based
on x(C)
mi for m = 1, . . . , k(C) and i = 1, . . . , n, to distinguish between the cases.
120

Case 1: The regression coefÔ¨Åcients are identical across categories.
Let
X(C) ‚â°X(C1) =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£∞
x(C)
11
¬∑ ¬∑ ¬∑
x(C)
k(C)1
x(C)
12
¬∑ ¬∑ ¬∑
x(C)
k(C)2
...
...
...
x(C)
1n
¬∑ ¬∑ ¬∑
x(C)
k(C)n
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
be an n‚Ñì√ó k(C) matrix of covariates, and let
Œ≤(C) ‚â°Œ≤(C1) = (Œ≤(C1)
1
, . . . , Œ≤(C1)
k(C) )‚Ä≤
be a k(C) √ó 1 vector of coefÔ¨Åcients.
Consider ¬µ:
¬µ = X(C1)Œ≤(C1)
=
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£∞
x(C)
11
¬∑ ¬∑ ¬∑
x(C)
k(C)1
x(C)
12
¬∑ ¬∑ ¬∑
x(C)
k(C)2
...
...
...
x(C)
1n
¬∑ ¬∑ ¬∑
x(C)
k(C)n
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
Ô£Æ
Ô£ØÔ£∞
Œ≤(C1)
1...
Œ≤(C1)
k(C)
Ô£π
Ô£∫Ô£ª
=
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£∞
Œ≤(C1)
1
x(C)
11 + ¬∑ ¬∑ ¬∑ + Œ≤(C1)
k(C) x(C)
k(C)1
Œ≤(C1)
1
x(C)
12 + ¬∑ ¬∑ ¬∑ + Œ≤(C1)
k(C) x(C)
k(C)2
...
Œ≤(C1)
1
x(C)
1n + ¬∑ ¬∑ ¬∑ + Œ≤(C1)
k(C) x(C)
k(C)n
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
so that the mean of the latent Zij is
¬µij = Œ≤(C1)
1
x(C)
1ij + ¬∑ ¬∑ ¬∑ + Œ≤(C1)
1
x(C)
k(C)ij,
for i = 1, . . . , n and j = 1, . . . , ‚Ñì. Here, Œ≤(C1)
m
represents the increase in the mean of
the latent variable Zij due to a one unit increase in the mth location-category-speciÔ¨Åc
covariate x(C)
mij.
121

Now consider ‚àÜ¬µ:
‚àÜ¬µ = ‚àÜX(C1)Œ≤(C1)
= ‚àÜ
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£∞
x(C)
11
¬∑ ¬∑ ¬∑
x(C)
k(C)1
x(C)
12
¬∑ ¬∑ ¬∑
x(C)
k(C)2
...
...
...
x(C)
1n
¬∑ ¬∑ ¬∑
x(C)
k(C)n
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
Ô£Æ
Ô£ØÔ£∞
Œ≤(C1)
1...
Œ≤(C1)
k(C)
Ô£π
Ô£∫Ô£ª
= (In ‚äó‚àÜ)
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£∞
x(C)
11
¬∑ ¬∑ ¬∑
x(C)
k(C)1
x(C)
12
¬∑ ¬∑ ¬∑
x(C)
k(C)2
...
...
...
x(C)
1n
¬∑ ¬∑ ¬∑
x(C)
k(C)n
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
Ô£Æ
Ô£ØÔ£∞
Œ≤(C1)
1...
Œ≤(C1)
k(C)
Ô£π
Ô£∫Ô£ª
=
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£∞
Œ≤(C1)
1
‚àÜx(C)
11 + ¬∑ ¬∑ ¬∑ + Œ≤(C1)
k(C) ‚àÜx(C)
k(C)1
Œ≤(C1)
1
‚àÜx(C)
12 + ¬∑ ¬∑ ¬∑ + Œ≤(C1)
k(C) ‚àÜx(C)
k(C)2
...
Œ≤(C1)
1
‚àÜx(C)
1n + ¬∑ ¬∑ ¬∑ + Œ≤(C1)
k(C) ‚àÜx(C)
k(C)n
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
where
‚àÜx(C)
mi =
Ô£Æ
Ô£ØÔ£∞
x(C)
mi1 ‚àíx(C)
mi‚Ñì
...
x(C)
mi(‚Ñì‚àí1) ‚àíx(C)
mi‚Ñì
Ô£π
Ô£∫Ô£ª
and
Œ≤(C1)
m
‚àÜx(C)
mi =
Ô£Æ
Ô£ØÔ£∞
Œ≤(C1)
m
(x(C)
mi1 ‚àíx(C)
mi‚Ñì)
...
Œ≤(C1)
m
(x(C)
mi(‚Ñì‚àí1) ‚àíx(C)
mi‚Ñì)
Ô£π
Ô£∫Ô£ª,
for i = 1, . . . , n and m = 1, . . . , k(C). It follows that the mean of the latent Uij is
(‚àÜ¬µ)ij = Œ≤(C1)
1
(x1ij ‚àíx1i‚Ñì) + ¬∑ ¬∑ ¬∑ + Œ≤(C1)
k(C) (xk(C)ij ‚àíxk(C)i‚Ñì),
for i = 1, . . . , n and j = 1, . . . , (‚Ñì‚àí1).
Here, Œ≤(C1)
m
represents the increase in the mean of the latent variable Uij for a one
unit increase in the difference between x(C)
mij and x(C)
mi‚Ñì. For each covariate, Œ≤(C1)
m
is
constant across the categories.
122

Case 2: Each covariate has a unique regression coefÔ¨Åcient for each category.
DeÔ¨Åne
D(C2)
mi
= diag(x(C)
mi ) =
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£∞
x(C)
mi1
0
¬∑ ¬∑ ¬∑
0
0
x(C)
mi2
¬∑ ¬∑ ¬∑
0
...
...
...
...
0
0
¬∑ ¬∑ ¬∑
x(C)
mi‚Ñì
Ô£π
Ô£∫Ô£∫Ô£∫Ô£ª,
and let
X(C) = X(C2) =
Ô£Æ
Ô£ØÔ£∞
D(C2)
11
¬∑ ¬∑ ¬∑
D(C2)
k(C)1
...
...
...
D(C2)
1n
¬∑ ¬∑ ¬∑
D(C2)
k(C)n
Ô£π
Ô£∫Ô£ª
be an n‚Ñì√ó k(C)‚Ñìmatrix of covariates. Let
Œ≤(C) = (Œ≤(C2)‚Ä≤
1
, . . . , Œ≤(C2)‚Ä≤
k(C) )‚Ä≤
be a k(3)‚Ñì√ó 1 vector of coefÔ¨Åcients where Œ≤(C2)
m
= (Œ≤(C2)
m1 , . . . , Œ≤(C2)
m‚Ñì)‚Ä≤.
Consider ¬µ:
¬µ = X(C2)Œ≤(C2)
=
Ô£Æ
Ô£ØÔ£∞
D(C2)
11
¬∑ ¬∑ ¬∑
D(C2)
k(C)1
...
...
...
D(C2)
1n
¬∑ ¬∑ ¬∑
D(C2)
k(C)n
Ô£π
Ô£∫Ô£ª
Ô£Æ
Ô£ØÔ£∞
Œ≤(C2)
1...
Œ≤(C2)
k(C)
Ô£π
Ô£∫Ô£ª
=
Ô£Æ
Ô£ØÔ£∞
D(C2)
11
Œ≤(C2)
1
+ ¬∑ ¬∑ ¬∑ + D(C2)
k(C)1Œ≤(C2)
k(C)
...
D(C2)
1n Œ≤(C2)
1
+ ¬∑ ¬∑ ¬∑ + D(C2)
k(C)nŒ≤(C2)
k(C)
Ô£π
Ô£∫Ô£ª,
so that the mean of the latent Zij is
¬µij = Œ≤(C2)
1j
x(C)
1ij + ¬∑ ¬∑ ¬∑ + Œ≤(C2)
k(C)jx(C)
k(C)ij,
for i = 1, . . . , n and j = 1, . . . , ‚Ñì. Here, Œ≤(C2)
mj
represents the increase in the mean of
the jth category latent variable Zij due to a one unit increase in the mth covariate.
123

Now consider ‚àÜ¬µ:
‚àÜ¬µ = ‚àÜX(C2)Œ≤(C2)
= ‚àÜ
Ô£Æ
Ô£ØÔ£∞
D(C2)
11
¬∑ ¬∑ ¬∑
D(C2)
k(C)1
...
...
...
D(C2)
1n
¬∑ ¬∑ ¬∑
D(C2)
k(C)n
Ô£π
Ô£∫Ô£ª
Ô£Æ
Ô£ØÔ£∞
Œ≤(C2)
1...
Œ≤(C2)
k(C)
Ô£π
Ô£∫Ô£ª
= (In ‚äó‚àÜ)
Ô£Æ
Ô£ØÔ£∞
D(C2)
11
¬∑ ¬∑ ¬∑
D(C2)
k(C)1
...
...
...
D(C2)
1n
¬∑ ¬∑ ¬∑
D(C2)
k(C)n
Ô£π
Ô£∫Ô£ª
Ô£Æ
Ô£ØÔ£∞
Œ≤(C2)
1...
Œ≤(C2)
k(C)
Ô£π
Ô£∫Ô£ª
=
Ô£Æ
Ô£ØÔ£∞
‚àÜD(C2)
11
Œ≤(C2)
1
+ ¬∑ ¬∑ ¬∑ + ‚àÜD(C2)
k(C)1Œ≤(C2)
k(C)
...
‚àÜD(C2)
1n Œ≤(C2)
1
+ ¬∑ ¬∑ ¬∑ + ‚àÜD(C2)
k(C)nŒ≤(C2)
k(C)
Ô£π
Ô£∫Ô£ª,
where
‚àÜD(C2)
mi
=
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£∞
x(C)
mi1
0
¬∑ ¬∑ ¬∑
0
‚àíx(C)
mi‚Ñì
0
x(C)
mi2
¬∑ ¬∑ ¬∑
0
‚àíx(C)
mi‚Ñì
...
...
...
...
...
0
0
¬∑ ¬∑ ¬∑
x(C)
mi(‚Ñì‚àí1)
‚àíx(C)
mi‚Ñì
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
and
‚àÜD(C2)
mi Œ≤(C2)
m
=
Ô£Æ
Ô£ØÔ£ØÔ£ØÔ£ØÔ£∞
Œ≤(C2)
m1 x(C)
mi1 ‚àíŒ≤(C2)
m‚Ñìx(C)
mi‚Ñì
Œ≤(C2)
m2 x(C)
mi2 ‚àíŒ≤(C2)
m‚Ñìx(C)
mi‚Ñì
...
Œ≤(C2)
m(‚Ñì‚àí1)x(C)
mi(‚Ñì‚àí1) ‚àíŒ≤(C2)
m‚Ñìx(C)
mi‚Ñì
Ô£π
Ô£∫Ô£∫Ô£∫Ô£∫Ô£ª
,
for i = 1, . . . , n and m = 1, . . . , k(C). It follows that the mean of the latent Uij is
(‚àÜ¬µ)ij = Œ≤(C2)
1j
x(C)
1ij ‚àíŒ≤(C2)
1‚Ñì
x(C)
1i‚Ñì+ ¬∑ ¬∑ ¬∑ + Œ≤(C2)
k(C)jx(C)
k(C)ij ‚àíŒ≤(C2)
k(C)‚Ñìx(C)
k(C)i‚Ñì
= Œ≤(C2)
1‚Ñì
 
Œ≤(C2)
1j
Œ≤(C2)
1‚Ñì
x(C)
1ij ‚àíx(C)
1i‚Ñì
!
+ ¬∑ ¬∑ ¬∑ + Œ≤(C2)
1‚Ñì
 
Œ≤(C2)
1j
Œ≤(C2)
1‚Ñì
x(C)
1ij ‚àíx(C)
1i‚Ñì
!
= Œ≤(C2)
1‚Ñì

x(C)
1ij ‚àíx(C)
1i‚Ñì

+

Œ≤(C2)
1j
‚àíŒ≤(C2)
1‚Ñì

x(C)
1ij + ¬∑ ¬∑ ¬∑
+ Œ≤(C2)
k(C)‚Ñì

x(C)
k(C)ij ‚àíx(C)
k(C)i‚Ñì

+

Œ≤(C2)
k(C)j ‚àíŒ≤(C2)
k(C)‚Ñì

x(C)
k(C)ij.
Notice that Case 1 can be written as a special case of Case 2, where Œ≤(C1)
m
= Œ≤(C2)
m‚Ñì
and (Œ≤(C2)
mj
‚àíŒ≤(C2)
m‚Ñì) = 0, for all j = 1, . . . , (‚Ñì‚àí1).
124

For category-speciÔ¨Åc information, depending on the assumption made about the re-
gression coefÔ¨Åcient, Case 1 or Case 2 may be appropriate. In Case 2, all k(C)‚ÑìcoefÔ¨Åcient
parameters are estimable. Thus, we can take
X(C) ‚â°X(C1) and Œ≤(C) ‚â°Œ≤(C1)
or
X(C) ‚â°X(C2) and Œ≤(C) ‚â°Œ≤(C2)
or even a combination of both, depending on the speciÔ¨Åcation that makes the most sense
for each location-category-speciÔ¨Åc covariate.
5.1.2
Parameterization of the Space-Category Covariance Matrix
In (5.2), we expressed the covariance structure for vec( ÀúZ) generally as ÀúŒ£. In this
section, we examine various speciÔ¨Åcations of ÀúŒ£, as well as the implications these structures
have on the covariance of vec( ÀúU).
Separable Space-Category Dependence
One speciÔ¨Åcation for ÀúŒ£, is to extend the covariance structure for the independent multi-
nomial probit regression model in (2.4) to allow for spatial dependence by replacing the
identity matrix with a spatial correlation structure, i.e.,
ÀúŒ£ ‚â°Œ£(Œ∏) ‚äóÀú‚Ñ¶
(5.9)
where Œ£(Œ∏) is an n√ón spatial correlation matrix and Àú‚Ñ¶is an ‚Ñì√ó‚Ñìcovariance matrix spec-
ifying the dependence among the ‚Ñìcategories. Notice that this speciÔ¨Åcation of ÀúŒ£ requires
that the spatial dependence is the same for all categories and the categorical dependence is
the same for all locations. Thus, ÀúŒ£ given by (5.9) is space-category separable.
125

From (5.4), the covariance of vec( ÀúU) is ‚àÜÀúŒ£‚àÜ, and for the separable case, can be
expressed as
‚àÜÀúŒ£‚àÜ= (In ‚äó‚àÜ)

Œ£(Œ∏) ‚äóÀú‚Ñ¶

(In ‚äó‚àÜ)‚Ä≤
= (In Œ£(Œ∏) I‚Ä≤
n) ‚äó(‚àÜÀú‚Ñ¶‚àÜ‚Ä≤)
= Œ£(Œ∏) ‚äó(‚àÜÀú‚Ñ¶‚àÜ‚Ä≤).
Notice that in this case, we again have a separable covariance, where each category relative
to category ‚Ñìhas the same spatial dependence and the categorical dependence is the same at
each location. Therefore, separability in the covariance structure of ÀúZ results in separability
in the covariance structure of ÀúU.
This model is simpler to Ô¨Åt than the model with a general spatial-categorical depen-
dence structure because we only need to work with an n √ó n matrix and an ‚Ñì√ó ‚Ñìmatrix
rather than an n‚Ñì√ón‚Ñìmatrix. See Section 5.2 for a model-Ô¨Åtting algorithm associated with
this covariance structure.
When we Ô¨Åt the Bayesian spatial multinomial probit model with a separable depen-
dence structure based on ÀúU, we only estimate the (‚Ñì‚àí1) √ó (‚Ñì‚àí1) categorical covariance,
Àú‚Ñ¶‚àÜ‚â°‚àÜÀú‚Ñ¶‚àÜ‚Ä≤. The ijth element of this covariance matrix is interpreted as the covariance
between the difference between the latent variables associated with categories i and ‚Ñìand
the difference between the latent variables associated with categories j and ‚Ñì. We might
be interested in modeling the dependence between all ‚Ñìcategorical responses. However,
because ‚àÜis not invertible, given Àú‚Ñ¶‚àÜ, Àú‚Ñ¶is not identiÔ¨Åable.
Non-Separable Space-Category Dependence
Suppose that instead of assuming space-category separability, we want to allow the
different categories to have different spatial dependence structures. To motivate why this
126

would be desirable, consider two categories of the land cover example, forest and urban. In
many places on Earth, locations covered by forest take up a larger area than urban locations.
This is seen in Figure 1.1, where urban land cover is included in the category ‚Äòother‚Äô. Thus,
the spatial dependence for the category forest will have a longer range, or distance at which
locations will be correlated, than that of the spatial dependence for the category urban.
Therefore, the spatial dependence structures for each of these categories should allow for
this difference.
To accommodate category-speciÔ¨Åc spatial dependence structures, we can consider the
latent variables of the data augmentation representation of the Bayiesian multinomial probit
regression model to be a spatially-dependent multivariate random variable. Then, we can
use non-separable dependence structures deÔ¨Åned for Gaussian multivariate spatial data.
For geostatistical data, a non-space-category separable covariance structure can be ob-
tained through the linear model of coregionalization (LMC; e.g., Wackernagel, 1998). The
Bayesian version of the LMC arises as a special case of the conditional hierarchical ap-
proach in Royle and Berliner (1999). Gelfand et al. (2004) propose an extension to the
LMC, called the spatially-varying LMC, by specifying the dependence structure jointly.
When modeling the dependence structure of lattice/gridded data, several multivariate CAR
models have been proposed (e.g., Mardia, 1988; Carlin and Banerjee, 2003; Gelfand and
Vounatsou, 2003). In addition, Sain and Cressie (2007) propose a canonical multivari-
ate conditional autoregressive (CAMCAR) model as a multivariate extension to the CAR
model that allows for space-category asymmetries. We leave determining the dependence
structure of vec( ÀúU) based on each of the previous models as well as the interpretation of
the elements of the resulting covariance structures to future research.
127

5.2
Model-Fitting
5.2.1
Data Augmentation MCMC Algorithms
In this section, we propose a marginal data augmentation algorithm for Ô¨Åtting the space-
category separable model with ÀúŒ£ = Œ£(Œ∏) ‚äóÀú‚Ñ¶. This algorithm is an extension of the
algorithm proposed by Imai and van Dyk (2005) for independent multi-category response
variables and of Marginal-Scheme 1 proposed in Section 3.1 for spatially-dependent binary
data.
First, let
X‚àÜ‚â°‚àÜX
and
Àú‚Ñ¶‚àÜ‚â°‚àÜÀú‚Ñ¶‚àÜ‚Ä≤,
so that
vec( ÀúU) ‚àºN(X‚àÜÀúŒ≤, Œ£(Œ∏) ‚äóÀú‚Ñ¶‚àÜ).
Here, X‚àÜis speciÔ¨Åed appropriately for each type of covariate information considered, as
discussed in Section 5.1.1. We let X‚àÜbe an n(‚Ñì‚àí1) √ó k(E) matrix, where k(E) represents
the total number of effective covariates, where each covariate may have up to ‚Ñìeffective
covariates.
In this case, following Imai and van Dyk (2005), the working parameter is the Ô¨Årst
element in Àú‚Ñ¶‚àÜ, which we denote as œâ2
‚àÜ. Therefore, the identiÔ¨Åable parameters in this case
will be
U =
ÀúU
œâ‚àÜ
,
128

Œ≤ =
ÀúŒ≤
œâ‚àÜ
,
and
‚Ñ¶‚àÜ=
Àú‚Ñ¶‚àÜ
œâ2
‚àÜ
.
We assign prior distributions on identiÔ¨Åable parameters. SpeciÔ¨Åcally, we take
Œ≤ ‚àºN(0, CŒ≤),
where 0 is a k(E)√ó1 vector of zeros and CŒ≤ is a k(E)√ók(E) covariance matrix. We consider
only one spatial dependence parameter, i.e., Œ∏ ‚â°Œ∏, and its prior distribution is
Œ∏ ‚àºUnif(lŒ∏, uŒ∏),
where lŒ∏ and uŒ∏ are appropriate lower and upper bounds, respectively, for Œ∏. Finally,
we take the prior on Àú‚Ñ¶‚àÜto be inverse Wishart with parameters ŒΩ‚Ñ¶and
Àú
M‚Ñ¶, i.e.,
Àú‚Ñ¶‚àÜ
‚àº
Inv Wishart(ŒΩ‚Ñ¶, Àú
M‚Ñ¶), where ŒΩ‚Ñ¶is a scalar and
Àú
M‚Ñ¶is an (‚Ñì‚àí1) √ó (‚Ñì‚àí1)
matrix. Transforming to ‚Ñ¶‚àÜyields the following joint density function
œÄ(‚Ñ¶‚àÜ, œâ2
‚àÜ) ‚àù|‚Ñ¶‚àÜ|‚àí(ŒΩ‚Ñ¶+‚Ñì)/2 exp

‚àía2
œâ
2œâ2
‚àÜ
trace(M‚Ñ¶‚Ñ¶‚àí1
‚àÜ)

(œâ2
‚àÜ)‚àí[ŒΩ‚Ñ¶(‚Ñì‚àí1)/2+1],
where a2
œâ is a positive constant, and M‚Ñ¶=
Àú
M‚Ñ¶/a2
œâ. Therefore,
œâ2
‚àÜ|‚Ñ¶‚àÜ‚àºa2
œâtrace(M‚Ñ¶‚Ñ¶‚àí1
‚àÜ)/œá2
ŒΩ‚Ñ¶(‚Ñì‚àí1).
(5.10)
The
last
element
of
discussion
before
giving
the
steps
of
the
algorithm
is
to
determine
the
conditional
distributions
of
Ui|[vec(U)]-i, Œ≤, Œ∏, ‚Ñ¶‚àÜ
and
Uij|Ui,-j, [vec(U)]-i, Œ≤, Œ∏, ‚Ñ¶‚àÜ, œâ2
‚àÜ, where Ui
=
(Ui1, . . . , Ui(‚Ñì‚àí1))‚Ä≤, [vec(U)]-i de-
notes vec(U) with Ui removed, and Ui,-j denotes Ui with the jth element removed.
129

Notice that Ui|[vec(U)]-i, Œ≤, Œ∏, ‚Ñ¶‚àÜhas a normal distribution with mean
E(Ui|[vec(U)]-i, Œ≤, Œ∏, ‚Ñ¶‚àÜ)
= (X‚àÜ)iŒ≤ +

([Œ£(Œ∏)]i,-i ‚äó‚Ñ¶‚àÜ)([Œ£(Œ∏)]-i,-i ‚äó‚Ñ¶‚àÜ)‚àí1
([vec(U)]-i ‚àí(X‚àÜ)-iŒ≤)]
= (X‚àÜ)iŒ≤ +
 [Œ£(Œ∏)]i,-i ([Œ£(Œ∏)]-i,-i)‚àí1 ‚äóI‚Ñì‚àí1

([vec(U)]-i ‚àí(X‚àÜ)-iŒ≤)
and variance
Var(Ui|[vec(U)]-i, Œ≤, Œ∏, ‚Ñ¶‚àÜ)
= ([Œ£(Œ∏)]i,i ‚äó‚Ñ¶‚àÜ) ‚àí([Œ£(Œ∏)]i,-i ‚äó‚Ñ¶‚àÜ) ([Œ£(Œ∏)]-i,-i ‚äó‚Ñ¶‚àÜ)‚àí1 ([Œ£(Œ∏)]-i,i ‚äó‚Ñ¶‚àÜ)
= ([Œ£(Œ∏)]i,i ‚äó‚Ñ¶‚àÜ) ‚àí
 [Œ£(Œ∏)]i,-i([Œ£(Œ∏)]-i,-i)‚àí1[Œ£(Œ∏)]-i,i ‚äó‚Ñ¶‚àÜ

=
 [Œ£(Œ∏)]i,i ‚àí[Œ£(Œ∏)]i,-i ([Œ£(Œ∏)]-i,-i)‚àí1 [Œ£(Œ∏)]-i,i

|
{z
}
‚â°œÉ(Œ∏)i
‚Ñ¶‚àÜ,
where (X‚àÜ)i is the (‚Ñì‚àí1)√ók(E) matrix of covariates associated with observation i, (X‚àÜ)-i
is the remaining (n ‚àí1)(‚Ñì‚àí1) √ó k(E) matrix of X‚àÜ, and [Œ£(Œ∏)]i,-j denotes the ith row of
Œ£(Œ∏) with the jth column removed. Using this conditional distribution, we can determine
the distribution of Uij|Ui,-j, [vec(U)]-i, Œ≤, Œ∏, ‚Ñ¶‚àÜ, which is normally distributed with mean
¬µUij ‚â°E(Uij|Ui,-j, [vec(U)]-i, Œ≤, Œ∏, ‚Ñ¶‚àÜ)
= (X‚àÜ)ijŒ≤ +
 [Œ£(Œ∏)]i,-i ([Œ£(Œ∏)]-i,-i)‚àí1 
{[vec(U)]-i}j ‚àí{(X‚àÜ)-i}j Œ≤

+ [‚Ñ¶‚àÜ]j,-j ([‚Ñ¶‚àÜ]-j,-j)‚àí1 
(Ui,-j ‚àí(X‚àÜ)i,-jŒ≤) +
 [Œ£(Œ∏)]i,-i ([Œ£(Œ∏)]-i,-i)‚àí1 ‚äóI‚Ñì‚àí2

√ó

{[vec(U)]-i}-j ‚àí{(X‚àÜ)-i}-j Œ≤
i
,
(5.11)
where (X‚àÜ)‚Ä≤
ij is the k(E) √ó 1 vector associated with Uij, (X‚àÜ)i,-j is the remaining
(‚Ñì‚àí2) √ó k(E) component of (X‚àÜ)i, and the notation {¬∑}j denotes the selection of
130

only the rows corresponding to category j, and {¬∑}-j denotes the selection of the rows not
corresponding to category j. Although the above equation looks daunting, computationally,
it is just a matter of selecting the appropriate rows and columns of the appropriate matrices.
As noted in Section 5.1.2, these computations will be relatively faster than the computa-
tions for the conditional mean for a non-separable dependence structure. The variance is
then
œÑ 2
Uij ‚â°Var(Uij|Ui,-j, [vec(U)]-i, Œ≤, Œ∏, ‚Ñ¶‚àÜ)
= œÉ(Œ∏)i[‚Ñ¶‚àÜ]j,j ‚àíœÉ(Œ∏)i[‚Ñ¶‚àÜ]j,-j (œÉ(Œ∏)i[‚Ñ¶‚àÜ]-j,-j)‚àí1 œÉ(Œ∏)i[‚Ñ¶‚àÜ]-j,j
= œÉ(Œ∏)i
 [‚Ñ¶‚àÜ]j,j ‚àí[‚Ñ¶‚àÜ]j,-j ([‚Ñ¶‚àÜ]-j,-j)‚àí1 [‚Ñ¶‚àÜ]-j,j

.
(5.12)
We now specify the Gibbs sampler and full conditional distributions for this separable
Bayesian spatial multinomial probit regression model:
Step 1: Sample vec( ÀúU) from vec(U)|Y , Œ≤[t‚àí1], Œ∏[t‚àí1], ‚Ñ¶[t‚àí1]
‚àÜ
, (œâ2
‚àÜ)‚àó.
‚Ä¢ Sample (œâ2
‚àÜ)‚àófrom the prior distribution œâ2
‚àÜ|‚Ñ¶as speciÔ¨Åed in (5.10).
‚Ä¢ For each i = 1, . . . , n and j = 1, . . . , (‚Ñì‚àí1), sample ÀúUij from
ÀúUij|Y ,[vec(U)]-i, Ui,-j, Œ≤[t‚àí1], Œ∏[t‚àí1], ‚Ñ¶[t‚àí1]
‚àÜ
, (œâ2
‚àÜ)‚àó
‚àº
Ô£±
Ô£≤
Ô£≥
TN

œâ‚àó
‚àÜ¬µUij, (œâ2
‚àÜ)‚àóœÑ 2
Uij, max(œâ‚àó
‚àÜUi,-j, 0), ‚àû

,
if Yi = j
TN

œâ‚àó
‚àÜ¬µUij, (œâ2
‚àÜ)‚àóœÑ 2
Uij, ‚àí‚àû, max(œâ‚àó
‚àÜUi,-j, 0)

,
if Yi Ã∏= j
where ¬µUij and œÑ 2
Uij are as speciÔ¨Åed in (5.11) and (5.12), respectively, plugging
in the current values of the other parameters.
Step 2: Sample (œâ2
‚àÜ)‚àó, Œ≤[t] from œâ2
‚àÜ, Œ≤|Y , vec(U [t]), Œ∏[t‚àí1], ‚Ñ¶[t‚àí1]
‚àÜ
.
131

‚Ä¢ Sample (œâ2
‚àÜ)‚àófrom
(œâ2
‚àÜ) ‚àº
ÀÜœâ2
œá2
(n+ŒΩ‚Ñ¶)(‚Ñì‚àí1)
where
ÀÜœâ2 = (vec( ÀúU) ‚àíX‚àÜÀÜŒ≤)‚Ä≤(Œ£(Œ∏[t‚àí1]) ‚äó‚Ñ¶[t‚àí1]
‚àÜ
)‚àí1(vec( ÀúU) ‚àíX ÀÜŒ≤)‚Ä≤
+ ÀÜŒ≤ C‚àí1
Œ≤
ÀÜŒ≤ + trace

Àú
M

‚Ñ¶[t‚àí1]
‚àÜ
‚àí1
and
ÀÜŒ≤ =

X‚Ä≤
‚àÜ

Œ£(Œ∏[t‚àí1]) ‚äó‚Ñ¶[t‚àí1]
‚àÜ
‚àí1
X‚àÜ+ C‚àí1
Œ≤
‚àí1
√ó

X‚Ä≤
‚àÜ

Œ£(Œ∏[t‚àí1]) ‚äó‚Ñ¶[t‚àí1]
‚àÜ
‚àí1
vec( ÀúU)

.
‚Ä¢ Sample ÀúŒ≤ ‚àºN
 
ÀÜŒ≤, (œâ2
‚àÜ)‚àó

X‚Ä≤
‚àÜ

Œ£(Œ∏[t‚àí1]) ‚äó‚Ñ¶[t‚àí1]
‚àÜ
‚àí1
X‚àÜ+ C‚àí1
Œ≤
‚àí1!
.
‚Ä¢ Set Œ≤[t] = ÀúŒ≤/œâ‚àó
‚àÜ.
Step 3: Sample (œâ2
‚àÜ)[t], ‚Ñ¶[t]
‚àÜfrom (œâ2
‚àÜ), ‚Ñ¶‚àÜ|Y , vec(U [t]), Œ≤[t], Œ∏[t‚àí1].
‚Ä¢ Sample Àú‚Ñ¶‚àÜ‚àºInv Wishart (n + ŒΩ‚Ñ¶,
Àú
M + [[vec( ÀúU) ‚àíX‚àÜÀúŒ≤]]‚Ä≤
n√ó(‚Ñì‚àí1)
 Œ£(Œ∏[t‚àí1])
‚àí1 [[vec( ÀúU) ‚àíX‚àÜÀúŒ≤]]n√ó(‚Ñì‚àí1)

.
where the notation [[c]]a√ób denotes the operator that reorders the ab √ó 1 vector
c to form the corresponding a √ó b matrix, Ô¨Ålling this matrix by rows.
‚Ä¢ Set (œâ2
‚àÜ)[t] equal to the Ô¨Årst element of Àú‚Ñ¶‚àÜ.
‚Ä¢ Set U [t]
ij = ÀúUij/œâ[t]
‚àÜ, for i = 1, . . . , n and j = 1, . . . , (‚Ñì‚àí1).
‚Ä¢ Set ‚Ñ¶[t]
‚àÜ= Àú‚Ñ¶‚àÜ/(œâ2
‚àÜ)[t].
Step 4: Sample Œ∏[t] from Œ∏|Y , vec(U [t]), Œ≤[t], ‚Ñ¶[t]
‚àÜ, (œâ2
‚àÜ)[t] using a Metropolis random walk
step.
132

5.3
Summary
In this chapter, we showed how care must be taken in the speciÔ¨Åcation of the latent
variable representation of the Bayesian spatial multinomial probit regression model. In
order for regression coefÔ¨Åcient parameters to be identiÔ¨Åable, when specifying the design
matrix we must consider the type of covariate information (i.e., category-speciÔ¨Åc, location-
speciÔ¨Åc, or location-category-speciÔ¨Åc). Furthermore, we discussed various ways to specify
the spatial-categorical dependence structure of the latent variable. Under this appropri-
ately speciÔ¨Åed latent variable representation of the Bayesian spatial multinomial probit
regression model, we provided a model-Ô¨Åtting algorithm for the case of a separable spatial-
categorical dependence structure.
133

CHAPTER 6
CONTRIBUTIONS AND FUTURE WORK
In this Ô¨Ånal chapter, we discuss our contributions to the development of the Bayesian
spatial probit regression model and provide a list of future research directions.
We began this work by motivating the need for models that allow for residual spatial de-
pendence when analyzing spatially-referenced categorical response variables, speciÔ¨Åcally,
within a regression framework. We reviewed a diverse literature of current methods for
analyzing spatially-dependent binary and categorical data, one being the Bayesian spatial
probit regression model. To provide context, in Chapter 2 we discussed the general data-
augmented version of the Bayesian probit regression model, and showed how extensions of
this model Ô¨Åt within the general speciÔ¨Åcation of the model. SpeciÔ¨Åcally, we showed how
the Bayesian probit regression model is extended to allow for residual spatial dependence,
resulting in the Bayesian spatial probit regression model. We also demonstrated that the
variance of the latent variable in the data-augmented Bayesian probit regression models is
not identiÔ¨Åable.
To account for, and exploit for computational purposes, this non-identiÔ¨Åability in the
Bayesian spatial probit regression model, in Chapter 3 we introduced and compared several
data augmentation MCMC algorithms. These algorithms use the non-identiÔ¨Åable param-
eter, or working parameter, to improve computational efÔ¨Åciency of model Ô¨Åtting. While
134

conditional and marginal augmentation strategies for the independent version of this model
have been compared in the literature previously, the spatial extension introduces additional
complexity. Furthermore, the presence of the spatial dependence parameter allows for the
possibility of partially collapsing the data augmentation algorithms. Based on our simula-
tion study and data analysis, we recommend using the Marginal-Scheme 1 algorithm when
Ô¨Åtting the spatial probit regression model. While the differences in the sample autocorre-
lations among the algorithms may not appear to be dramatic, we return to our discussion
at the beginning of Chapter 3 of the recent emphasis in the literature on methodology for
massive spatial data ‚Äì improved mixing of MCMC algorithms can lead to signiÔ¨Åcant gains
in computational efÔ¨Åciency, which can be particularly important in analyses of large data
sets.
In Chapter 3, our investigations of efÔ¨Åcient computational strategies for Ô¨Åtting the spa-
tial probit regression model focused exclusively on the special case where the outcome
variable is binary. We proposed an extension of the Marginal-Scheme 1 algorithm for
spatially-referenced multi-categorical outcomes in Chapter 5, however, we left implemen-
tation of this algorithm to future work. Additionally, data augmentation MCMC strategies
also need to be developed for spatially-referenced multivariate or ordinal response vari-
ables. For example, Higgs and Hoeting (2010) recently proposed a clipped latent variable
model for spatially-dependent ordered categorical data, an extension of the model origi-
nally introduced by De Oliveira (2000). To facilitate sampling the latent variables in mod-
els for non-spatially-referenced ordered categorical data, Hans et al. (2009) propose using a
covariance decomposition technique. Future work will explore data augmentation MCMC
strategies and algorithms using this covariance decomposition technique for Ô¨Åtting models
for spatially-referenced multivariate and ordered categorical data.
135

In Chapter 4 we showed how a spatial classiÔ¨Åer can be derived from the Bayesian spatial
probit regression model. In an analysis of the Southeast Asia land cover data, this spatial
classiÔ¨Åer was found to be a more accurate predictor of unobserved spatially-dependent
binary data than other popular classiÔ¨Åcation techniques. Furthermore, including spatial de-
pendence in the residual dependence structure of a generalized linear model is more effec-
tive in terms of prediction/classiÔ¨Åcation than including spatial dependence in the covariate
or predictor space, at least in this example.
In this work, we only considered a spatial classiÔ¨Åer for two classes. We could also
consider a similar spatial classiÔ¨Åer for multi-category response variables based on the spa-
tial multinomial probit regression model proposed in Chapter 5 and compare it to popular
classiÔ¨Åcation techniques in the multi-class setting.
Finally, in Chapter 5, we extended the spatial probit regression model to the multi-
category case. We considered how the mean and covariance speciÔ¨Åcation for all ‚Ñìcategories
will impact the speciÔ¨Åcation and interpretation of the model when we use the ‚Ñìth category
as a baseline, i.e., model the Ô¨Årst ‚Ñì‚àí1 categories relative to the ‚Ñìth category. We showed
how it was important to consider the type of covariate information when specifying the la-
tent mean structure and provided speciÔ¨Åcations of the design matrix which would provide
interpretable and identiÔ¨Åable coefÔ¨Åcients. We also provided potential ways for modeling
the latent variable dependence structure, but more work needs to be done in considering
non-separable space-category dependence structures as well as the interpretation of result-
ing covariances. Furthermore, we provided a data augmentation algorithm for Ô¨Åtting the
Bayesian multinomial probit regression model when the space-category dependence is sep-
arable.
136

Suppose that after we Ô¨Åt the model using the ‚Ñìth category as the baseline category, we
want to instead consider the model using the Ô¨Årst category as the baseline category. Future
work is needed to determine whether or not we can go between baseline categories and
estimate the associated parameters of the model with one category as the baseline using the
Ô¨Åtted model with a different category as the baseline.
One area of future work applicable to all areas of this dissertation is to create an R pack-
age for the Bayesian spatial multinomial probit regression model. In addition to providing
an accessible way to Ô¨Åt the Bayesian spatial probit regression model and to use this model
for prediction/classiÔ¨Åcation, this package would also provide functions that accommodate
the special features required for speciÔ¨Åcation of the Bayesian spatial multinomial probit
regression model. SpeciÔ¨Åcally, this package would enable creation of an appropriate de-
sign matrix for different types of covariate information, and upon further development of
spatial-categorical dependence structures, provide a set-up for the speciÔ¨Åcation of various
dependence structures and Ô¨Åt the model using the appropriate data augmentation algorithm.
The Ô¨Ånal area of future work that we mention here is to extend the Bayesian spatial pro-
bit regression model to the space-time setting. The motivating example of this dissertation
research was the analysis of land-cover/land-use data. For example, researchers may want
to determine patterns of land-cover change over time (e.g., deforestation) and what factors
may have contributed to an observed change. The Bayesian spatial probit regression model
is a natural starting point for modeling spatial-temporal categorical response variables.
137

BIBLIOGRAPHY
Abramowitz, M. and Stegun, I. A. (1965). Handbook of Mathematical Functions. New
York, NY: Dover.
Albert, J. H. and Chib, S. (1993). ‚ÄúBayesian analysis of binary and polychotomous re-
sponse data.‚Äù Journal of the American Statistical Association, 88, 669‚Äì679.
Albert, P. S. and McShane, L. M. (1995). ‚ÄúA generalized estimating equations approach
for spatially correlated binary data: Applications to the analysis of neuroimaging data.‚Äù
Biometrics, 51, 627‚Äì638.
Augustin, N. H., Mugglestone, M. A., and Buckland, S. T. (1996). ‚ÄúAn autologistic model
for the spatial distribution of wildlife.‚Äù Journal of Applied Ecology, 33, 339‚Äì347.
Banerjee, S., Carlin, B., and Gelfand, A. (2004). Hierarhical Modeling and Analysis for
Spatial Data. Boca Raton, FL: Chapman & Hall/CRC.
Banerjee, S., Gelfand, A. E., Finley, A. O., and Sang, H. (2008). ‚ÄúGaussian predictive
process models for large spatial data sets.‚Äù
Journal of the Royal Statistical Society,
Series B, 70, 825‚Äì848.
Besag, J. E. (1972). ‚ÄúNearest-neighbour systems and the auto-logistic model for binary
data.‚Äù Journal of the Royal Statistical Society, Series B, 34, 75‚Äì83.
138

‚Äî (1974). ‚ÄúSpatial interaction and the statistical analysis of lattice systems.‚Äù Journal of
the Royal Statistical Society, Series B, 36, 2, 192‚Äì236.
Breslow, N. E. and Clayton, D. G. (1993). ‚ÄúApproximate inference in generalized linear
mixed models.‚Äù Journal of the American Statistical Association, 88, 9‚Äì25.
Calder, C. A. (2007). ‚ÄúDynamic factor process convolution models for multivariate space-
time data with application to air quality assessment.‚Äù Environmental and Ecological
Statistics, 14, 229‚Äì247.
Carlin, B. P. and Banerjee, S. (2003). ‚ÄúHierarchical multivariate CAR models for spatio-
temporally correlated survival data (with discussion).‚Äù Bayesian Statistics 7, eds. J. M.
Bernardo, M. J. Bayarri, J. O. Berger, A. P. Dawid, D. Heckerman, A. F. M. Smith, and
M. West, 45‚Äì63. Oxford: Oxford University Press.
Chib, S. and Greenberg, E. (1998). ‚ÄúAnalysis of multivarite probit models.‚Äù Biometrika,
85, 347‚Äì361.
Christensen, O. F., M√∏ller, J., and Waagepetersen, R. P. (2001). ‚ÄúGeometric ergodicity of
Metropolis Hastings algorithms for conditional simulation in generalised linear mixed
models.‚Äù Methodology and Computing in Applied Probability, 3, 309‚Äì327.
Christensen, O. F. and Ribeiro Jr., P. J. (2002). ‚ÄúgeoRglm: A package for generalised linear
spatial models.‚Äù R-NEWS, 2, 2, 26‚Äì28.
Christensen, O. F., Roberts, G. O., and Sk¬®old, M. (2006). ‚ÄúRobust Markov chain Monte
Carlo methods for spatial generalized linear mixed models.‚Äù Journal of Computational
and Graphical Statistics, 15, 1‚Äì17.
139

Christensen, O. F. and Waagepetersen, R. P. (2002). ‚ÄúBayesian prediction of spatial count
data using generalized linear mixed models.‚Äù Biometrics, 58, 280‚Äì286.
Cortes, C. and Vapnik, V. (1995). ‚ÄúSupport-vector network.‚Äù Machine Learning, 20, 273‚Äì
297.
Cressie, N. (1993). Statistics for Spatial Data. Revised ed. New York: John Wiley.
Daganzo, C. (1980). Multinomial Probit. New York, NY: Academic Press.
De Oliveira, V. (2000). ‚ÄúBayesian prediction of clipped Gaussian random Ô¨Åelds.‚Äù Compu-
tational Statistics and Data Analysis, 34, 299‚Äì314.
Diggle, P. J. and Ribeiro Jr., P. J. (2007). Model-based Geostatistics. New York, New York:
Springer.
Diggle, P. J., Tawn, J. A., and Moyeed, R. A. (1998). ‚ÄúModel-based geostatistics.‚Äù Applied
Statistician, 47, 299‚Äì350.
Dimitriadou, E., Hornik, K., Leisch, F., Meyer, D., and Weingessel, A. (2010). e1071:
Misc Functions of the Department of Statistics (e1071), TU Wien. R package version
1.5-24.
Gelfand, A., Schmidt, A., Banerjee, S., and Sirmans, C. (2004). ‚ÄúNonstationary multivari-
ate process modeling through spatially varying coregionalization.‚Äù Test, 13, 2. 263-312.
Gelfand, A. E. and Vounatsou, P. (2003). ‚ÄúProper multivariate conditional autoregressive
models for spatial data analysis.‚Äù Biostatistics, 4, 11‚Äì25.
Gelman, A., Carlin, J., Stern, H., and Rubin, D. (1995). Bayesian Data Analysis. Chapman
and Hall.
140

Geweke, J. (1991).
‚ÄúEfÔ¨Åcient simulation from the multivariate normal and Student t-
distributions subject to linear constraints and the evaluation of constraint probabilities.‚Äù
Computer Sciences and Statistic, 23, 571‚Äì578.
Gotway, C. A. and Stroup, W. W. (1995). ‚ÄúA generalized linear model approach to spatial
data analysis and prediction.‚Äù Journal of Agricultural, Biological, and Environmental
Statistics, 2, 157‚Äì178.
Gumpertz, M. L., Graham, J. M., and Ristaino, J. B. (1974). ‚ÄúAutologistic model of spatial
pattern of phytophthora epidemic in bell pepper: effects of soil variables on disease
presence.‚Äù Journal of Agricultural, Biological, and Environmental Statistics, 2, 131‚Äì
156.
Hans, C., Allenby, G. M., Craigmile, P. F., Lee, J. H., MacEachern, S. N., and Xu, X.
(2009). ‚ÄúCovariance decompositions for accurate computation in Bayesian scale-usage
models.‚Äù
Tech. rep., No. 825, Department of Statistics, The Ohio State University,
Columbus, OH, 43210.
Hans, C. and Craigmile, P. F. (2009). truncatedNormals: R functions for truncated normal
distributions. R package version 0.4.
Hastie, T., Tibshirani, R., and Friedman, J. (2001). The Elements of Statistical Learning:
Data Mining, Inference, and Prediction. New York, NY: Springer.
Hausman, J. and Wise, D. (1978). ‚ÄúA conditional probit model for qualitative choice: Dis-
crete decisions recognizing interdependence and heterogeneous preferences.‚Äù Econo-
metrics, 46, 403‚Äì426.
141

Heagerty, P. J. and Lele, S. R. (1998). ‚ÄúA composite likelihood approach to binary spatial
data.‚Äù Journal of the American Statistical Association, 93, 1099‚Äì1111.
Higdon, D. (2002).
‚ÄúSpace and space-time modeling using process convolutions.‚Äù
In
Quantitative Methods for Current Environmental Issues, eds. C. Anderson, V. Barnett,
P. C. Chatwin, and A. H. El-Shaarawi, 37‚Äì56. Springer Verlag.
Higgs, M. D. and Hoeting, J. A. (2010). ‚ÄúA clipped latent-variable model for spatially
correlated ordered categorical data.‚Äù Computational Statistics and Data Analysis, 54,
1999‚Äì2011.
Huffer, F. W. and Wu, H. (1998). ‚ÄúMarkov chain Monte Carlo for autologistic regression
models with application to the distribution of plant species.‚Äù Biometrics, 54, 509‚Äì524.
Imai, K. and van Dyk, D. A. (2005). ‚ÄúA Bayesian analysis of the multinomial probit model
using marginal data augmentation.‚Äù Journal of Econometrics, 124, 311‚Äì334.
Journel, A. G. (1983). ‚ÄúNonparametric estimation of spatial distributions.‚Äù Mathematical
Geology, 15, 445‚Äì468.
Law, J. and Haining, R. (2004). ‚ÄúA Bayesian approach to modeling binary data: the case
of high-intensity crime areas.‚Äù Geographical Analysis, 36, 197‚Äì216.
Liang, K. Y. and Zeger, S. L. (1986). ‚ÄúLongitudinal Data Analysis Using Generalized
Linear Models.‚Äù Biometrika, 73, 13‚Äì22.
Lin, P. S. (2008). ‚ÄúEfÔ¨Åciency of quasi-likelihood estimation for spatially correlated binar
data on Lp spaces.‚Äù Journal of Statistical Planning and Inference, 138, 1528‚Äì1541.
142

Lin, P. S. and Clayton, M. K. (2005). ‚ÄúAnalysis of binary spatial data by quasi-likelihood
estimating equations.‚Äù The Annals of Statistics, 33, 542‚Äì555.
Liu, X. and Daniels, M. J. (2006). ‚ÄúA new algorithm for simulating a correlation matrix
based on parameter expansion and reparameterization.‚Äù Journal of Computational and
Graphical Statistics, 15, 897‚Äì914.
Lunn, D. J., Thomas, A., Best, N., and Spiegelhalter, D. (2000). ‚ÄúWinBUGS ‚Äì a Bayesian
modelling framework: concepts, structure, and extensibility.‚Äù Statistics and Computing,
10, 325‚Äì337.
Mardia, K. V. (1988). ‚ÄúMulti-dimensional multivariate Gaussian Markov random Ô¨Åelds
with application to image processing.‚Äù Journal of Multivariate Analysis, 24, 265‚Äì284.
McCulloch, R. and Rossi, P. E. (1994). ‚ÄúAn exact likelihood analysis of the multinomial
probit model.‚Äù Journal of Econometrics, 64, 207‚Äì240.
McCulloch, R. E., Polson, N. G., and Rossi, P. E. (2000). ‚ÄúA Bayesian analysis of the
multinomial probit model with fully identiÔ¨Åed parameters.‚Äù Journal of Econometrics,
99, 173‚Äì193.
Meng, X. L. and van Dyk, D. A. (1999). ‚ÄúSeeking efÔ¨Åcient data augmentation schemes via
conditional and marginal augmentation.‚Äù Biometrika, 86, 301‚Äì320.
Munroe, D. K., WolÔ¨Ånbarger, S. R., Calder, C. A., Shi, T., Xiao, N., Lamb, C. Q., and
Li, D. (2008). ‚ÄúThe relationships between biomass burning, land-cover/-use change,
and the distribution of carbonaceous aerosols in mainland Southeast Asia: a review and
synthesis.‚Äù Journal of Land Use Science, 3, 161‚Äì183.
143

Nelder, J. A. and Wedderburn, R. W. A. (1972). ‚ÄúGeneralized linear models.‚Äù Journal of
the Royal Statistical Society, Series A, 135, 370‚Äì384.
Nobile, A. (1998). ‚ÄúA hybrid Markov chain for the Bayesian analysis of the multinomial
probit model.‚Äù Statistics and Computing, 8, 229‚Äì242.
Paciorek, C. J. (2007). ‚ÄúComputational techniques for spatial logistic regression with large
data sets.‚Äù Computational Statistics and Data Analysis, 51, 3631‚Äì3653.
Patterson, H. and Thompson, R. (1971). ‚ÄúRecovery of Inter-block Information When Block
Sizes are Unequal.‚Äù Biometrika.
Royle, J. A. and Berliner, L. M. (1999). ‚ÄúA hierarchical approach to multivariate spa-
tial modeling and prediction.‚Äù Journal of Agricultural, Biological, and Environmental
Statistics, 4, 29‚Äì56.
Rue, H., Martino, S., and Chopin, N. (2009). ‚ÄúApproximate Bayesian inference for latent
Gaussian models by using integrated nested Laplace approximations.‚Äù Journal of the
Royal Statistical Society, Series B, 71, 1‚Äì35.
Sain, S. R. and Cressie, N. (2007). ‚ÄúA spatial model for multivariate lattice data.‚Äù Journal
of Econometrics, 140, 226‚Äì259.
Schabenberger, O. and Gotway, C. A. (2005). Statistical Methods for Spatial Data Analysis.
Boca Raton, Florida: Chapman & Hall/CRC.
Shaby, B. and Ruppert, D. (2010). ‚ÄúTapered covariance: Bayesian estimation and asymp-
totics.‚Äù Journal of the American Statistical Association, Submitted.
144

Sherman, M., Apanasovich, T. V., and Carroll, R. J. (2006). ‚ÄúOn estimation in binary
autologistic spatial models.‚Äù
Journal of Statistical Computation and Simulation, 76,
167‚Äì179.
Sim, S. (2000). ‚ÄúA test for spatial correlation for binary data.‚Äù Statistics & Probability
Letters, 47, 129‚Äì134.
Solow, A. R. (1993). ‚ÄúOn the efÔ¨Åciency of the indicator approach in geostatistics.‚Äù Mathe-
matical Geology, 25, 53‚Äì57.
Stein, M. (1999). Interpolation of Spatial Data: Some Theory for Kriging. New York:
Springer-Verlag.
Switzer, P. (1977). ‚ÄúEstimation of spatial distributions from point sources with application
to air pollution measurement.‚Äù Bulletin de l‚ÄôInstitute International de Statistique, 47,
123‚Äì137.
van Dyk, D. A. and Meng, X. L. (2001). ‚ÄúThe art of data augmentation.‚Äù Journal of
Computational and Graphical Statistics, 10, 1‚Äì50.
van Dyk, D. A. and Park, T. (2008).
‚ÄúPartially collapsed Gibbs samplers: theory and
methods.‚Äù Journal of the American Statistical Association, 103, 790‚Äì796.
ÀáSaltytÀôe Benth, J. and DuÀácinskas, K. (2005). ‚ÄúLinear discriminant analysis of multivariate
spatial-temporal regressions.‚Äù Scandinavian Journal of Statistics, 32, 281‚Äì294.
Wackernagel, H. (1998). Multivariate Geostatistics: An Introduction with Applications.
2nd ed. New York, NY: Springer-Verlag.
145

Waller, L. A. and Gotway, C. A. (2004). Applied Spatial Statistics for Public Health Data.
Hoboken, New Jersey: John Wiley & Sons, Inc.
Wedderburn, R. W. M. (1974). ‚ÄúQuasi-likelihood functions, generalized linear models, and
the Gauss-Newton method.‚Äù Biometrika, 61, 439‚Äì447.
Weir, I. S. and Pettitt, A. N. (1999). ‚ÄúSpatial modelling for binary data using a hidden con-
ditional autoregressive Gaussian process: a multivariate extension of the probit model.‚Äù
Statistics and Computing, 9, 77‚Äì86.
‚Äî (2000). ‚ÄúBinary probability maps using hidden conditional autoregressive Gaussian
processes with an application to Finnish common toad data.‚Äù Applied Statistics, 49,
473‚Äì484.
Xu, K., Wikle, C., and Fox, N. (2005). ‚ÄúA kernel-based spatio-temporal dynamical model
for nowcasting radar precipitation.‚Äù Journal of the American Statistical Association, 100,
1133‚Äì1144.
Yin, G. (2009). ‚ÄúBayesian generalized method of moments.‚Äù Bayesian Analysis, 4, 191‚Äì
208.
Zheng, Y. and Zhu, J. (2008). ‚ÄúMarkov chain Monte Carlo for a spatial-temporal autologis-
tic regression model.‚Äù Journal of Computational and Graphical Statistics, 17, 123‚Äì137.
Zhu, J., Zheng, Y., Carroll, A. L., and Aukema, B. H. (2008). ‚ÄúAutologistic regression
analysis of spatial-temporal binary data via Monte Carlo maximum likelihood.‚Äù Journal
of Agricultural, Biological, and Environmental Statistics, 13, 84‚Äì98.
146

