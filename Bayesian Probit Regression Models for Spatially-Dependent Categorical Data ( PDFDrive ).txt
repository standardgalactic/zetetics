Bayesian Probit Regression Models for
Spatially-Dependent Categorical Data
DISSERTATION
Presented in Partial Fulﬁllment of the Requirements for
the Degree Doctor of Philosophy in the
Graduate School of The Ohio State University
By
Candace Berrett, B.S., M.S.
Graduate Program in Statistics
The Ohio State University
2010
Dissertation Committee:
Catherine A. Calder, Advisor
L. Mark Berliner
Peter F. Craigmile
Elizabeth A. Stasny

c⃝Copyright by
Candace Berrett
2010

ABSTRACT
Data augmentation/latent variable methods have been widely recognized for facilitating
model ﬁtting in the Bayesian probit regression model. First proposed by Albert and Chib
(1993) for independent binary and multi-category data, the latent variable representation of
the Bayesian probit regression model allows model ﬁtting to be performed using a simple
Gibbs sampler and, for more than two categories, also allows the so-called assumption of
irrelevant alternatives required by the logistic regression model to be relaxed (Hausman
and Wise, 1978). To accommodate residual spatial dependence, the latent variable speci-
ﬁcation of the Bayesian probit regression model can be extended to incorporate standard
parametric covariance models typically used in analyses of spatially-dependent continuous
data, deﬁning what we term the Bayesian spatial probit regression model. In this disserta-
tion, we develop and extend the Bayesian spatial probit regression model by (i) introducing
efﬁcient model-ﬁtting algorithms, (ii) deriving classiﬁcation methods based on the model,
and (iii) extending the model to the multi-category spatial setting.
Statistical models for spatial data are notoriously cumbersome to ﬁt necessitating the
availability of fast and efﬁcient model-ﬁtting algorithms. To improve the efﬁciency of
the Gibbs sampler used to ﬁt the Bayesian regression model for independent categorical
response variables, Imai and van Dyk (2005) propose introducing a working parameter
into the model and compare various data augmentation strategies resulting from different
treatments of the working parameter. We build on this work by investigating the efﬁciency
ii

of modiﬁed and extended versions of conditional and marginal data augmentation Markov
chain Monte Carlo (MCMC) algorithms for the spatial probit regression model, focusing
on the special case of binary spatially-dependent response variables.
Within the classiﬁcation literature, methods that exploit spatial dependence are limited.
We show how a spatial classiﬁcation rule can be derived from the Bayesian spatial probit
regression model. In addition, we compare our proposed spatial classiﬁer to various other
classiﬁers in terms of training and test error rates using a land-cover/land-use data set.
When extending the spatial probit regression model to the multi-category setting, care
must be taken to ensure that model parameters are estimable and interpretable. Considering
three types of categorical and spatial covariate information, we discuss various speciﬁca-
tions of the latent variable mean structure and the associated parameter interpretations.
Additionally, we explore the speciﬁcation of the latent variable cross space-category de-
pendence structure and discuss how data augmentation MCMC strategies for ﬁtting the
Bayesian spatial probit regression model can be extended to the multi-category setting.
iii

Dedicated to my parents, Bob and Nanette,
and siblings, Tenille, Nat, Preston, MeChel, and Taylor.
iv

ACKNOWLEDGMENTS
First and foremost, I would like to thank my advisor, Dr. Kate Calder, who over the
last four and a half years has devoted a substantial amount of time and effort in training
me to be a well-rounded statistician. She has provided me with numerous opportunities
to learn and grow through research, teaching, mentoring, and collaboration. She has also
become a good friend, whom I admire professionally and personally, and I am grateful for
her example and support.
I would like to thank my committee members: Dr. Mark Berliner for his comments on
my research, his help with job and fellowship applications, and for allowing me to laugh
in his class; Dr. Elizabeth Stasny for her comments on my research, her help with job and
fellowship applications, her support as graduate chair, and in encouraging me to come to
Ohio State; and Dr. Peter Craigmile for his valuable comments and contributions to my
research.
I would like to thank Dr. Darla Munroe and Dr. Ningchuan Xiao of the Department of
Geography for their generous assistance in obtaining and understanding the land cover data
used in this work.
I would like to thank the other professors in the Department of Statistics who have
provided guidance and support during my time at Ohio State: Dr. Doug Wolfe, Dr. Tao Shi,
Dr. Chris Hans, Dr. Jackie Miller, Dr. Steve MacEachern, and Dr. Noel Cressie.
v

I would like to thank Lisa Van Dyke for her help in answering my many graduation
questions and in pulling together the ﬁnal documents of this dissertation. I would also like
to thank Terry England for her help with all my travel and posters.
Support for this research was provided by grants from NASA (NNG06GD31G) and the
NSF (ATM-0934595).
Finally, I would like to thank my family and many friends, who all believed in me
when I didn’t believe in myself; and God, for giving me strength and understanding, and
providing me with opportunities to grow.
vi

VITA
1983 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Born - Ogden, Weber, Utah, USA
2005 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . B.S. Actuarial Science, cum laude,
Brigham Young University.
2005 - 2006 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . University Fellow, Graduate School,
The Ohio State University.
2005 - 2006, 2010 . . . . . . . . . . . . . . . . . . . . . . . . . . .Teaching Assistant, Department of Statis-
tics, The Ohio State University.
2007 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . M.S. Statistics,
The Ohio State University.
2007 - 2010 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Research Assistant, Department of Statis-
tics, The Ohio State University.
2009 . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . Graduate Fellow, Statistical and Applied
Mathematical Sciences Institute.
PUBLICATIONS
Research Publications
Xiao, N., Shi, T., Calder, C.A., Munroe, D.K., Berrett, C., Wolﬁnbarger, S., and Li, D.
(2008)
“Spatial Characteristics of the Difference between MISR and MODIS Aerosol
Optical Depth Retrievals over Mainland Southeast Asia,” Remote Sensing of Environment,
DOI: 10.1016/j.rse.2008.07.011.
FIELDS OF STUDY
Major Field: Statistics
vii

TABLE OF CONTENTS
Page
Abstract . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
ii
Dedication . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
iv
Acknowledgments . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
v
Vita
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
vii
List of Tables . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
xi
List of Figures . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
xii
Chapters:
1.
Introduction . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
1
1.1
Background and Motivation . . . . . . . . . . . . . . . . . . . . . . . .
2
1.2
Modeling Categorical Spatial Data . . . . . . . . . . . . . . . . . . . . .
15
1.2.1
The Spatial Generalized Linear Model
. . . . . . . . . . . . . .
15
1.2.2
The Spatial Generalized Linear Mixed Model . . . . . . . . . . .
19
1.2.3
Indicator Kriging . . . . . . . . . . . . . . . . . . . . . . . . . .
20
1.2.4
The Autologistic Model . . . . . . . . . . . . . . . . . . . . . .
22
1.2.5
The Bayesian Spatial Probit Regression Model . . . . . . . . . .
23
1.3
Overview of Contributions . . . . . . . . . . . . . . . . . . . . . . . . .
24
1.4
Illustrative Data Set
. . . . . . . . . . . . . . . . . . . . . . . . . . . .
25
2.
Bayesian Spatial Probit Regression . . . . . . . . . . . . . . . . . . . . . . . .
29
2.1
The Bayesian Probit Regression Model . . . . . . . . . . . . . . . . . .
29
2.1.1
Albert and Chib’s Data Augmentation Strategy . . . . . . . . . .
29
viii

2.1.2
Multi-Category and Multivariate Extensions
. . . . . . . . . . .
31
2.2
The Bayesian Spatial Probit Regression Model . . . . . . . . . . . . . .
34
2.2.1
Model Speciﬁcation . . . . . . . . . . . . . . . . . . . . . . . .
34
2.2.2
Parameterization of the Spatial Correlation Matrix . . . . . . . .
36
3.
Data Augmentation MCMC Strategies . . . . . . . . . . . . . . . . . . . . . .
39
3.1
Data Augmentation MCMC Strategies . . . . . . . . . . . . . . . . . . .
40
3.1.1
Conditional versus Marginal Data Augmentation . . . . . . . . .
40
3.1.2
Partially Collapsed Algorithms
. . . . . . . . . . . . . . . . . .
45
3.1.3
Full Conditional Distributions . . . . . . . . . . . . . . . . . . .
46
3.2
Simulation Study . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
49
3.2.1
Simulation Set-up . . . . . . . . . . . . . . . . . . . . . . . . .
49
3.2.2
Simulation Results . . . . . . . . . . . . . . . . . . . . . . . . .
52
3.3
Application . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
54
3.4
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
56
4.
The Bayesian Spatial Probit Regression Model as a Tool for Classiﬁcation . . .
73
4.1
The Classiﬁcation Problem . . . . . . . . . . . . . . . . . . . . . . . . .
74
4.2
GLM-Based Classiﬁcation . . . . . . . . . . . . . . . . . . . . . . . . .
76
4.2.1
Non-Spatial GLM-Based Classiﬁcation . . . . . . . . . . . . . .
76
4.2.2
Spatial GLM-Based Classiﬁcation . . . . . . . . . . . . . . . . .
80
4.3
Alternative Classiﬁcation Methods . . . . . . . . . . . . . . . . . . . . .
84
4.3.1
Discriminant Analysis . . . . . . . . . . . . . . . . . . . . . . .
84
4.3.2
Support Vector Machines
. . . . . . . . . . . . . . . . . . . . .
90
4.3.3
k-Nearest Neighbors . . . . . . . . . . . . . . . . . . . . . . . .
93
4.4
Comparison of Classiﬁcation Methods . . . . . . . . . . . . . . . . . . .
94
4.4.1
Parameter Estimation
. . . . . . . . . . . . . . . . . . . . . . .
95
4.4.2
Classiﬁcation Errors . . . . . . . . . . . . . . . . . . . . . . . .
97
4.5
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 101
5.
Bayesian Spatial Multinomial Probit Regression . . . . . . . . . . . . . . . . . 102
5.1
The Bayesian Spatial Multinomial Probit Regression Model . . . . . . . 102
5.1.1
Latent Mean Speciﬁcation . . . . . . . . . . . . . . . . . . . . . 104
5.1.2
Parameterization of the Space-Category Covariance Matrix
. . . 125
5.2
Model-Fitting . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 128
5.2.1
Data Augmentation MCMC Algorithms
. . . . . . . . . . . . . 128
5.3
Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 133
ix

6.
Contributions and Future Work . . . . . . . . . . . . . . . . . . . . . . . . . . 134
x

LIST OF TABLES
Table
Page
3.1
This table lists the steps in each of the data augmentation algorithms. The
ﬁrst portion shows the non-collapsed data augmentation algorithms intro-
duced in Section 3.1.1. The second portion shows the partially collapsed
data augmentation algorithms introduced in Section 3.1.2. . . . . . . . . . .
44
3.2
Scenarios used to compare the marginal and conditional data augmentation
algorithms.
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
50
3.3
Autocorrelations of the sample paths of β1 and ρ for the land cover data
analysis. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
56
4.1
Fitted values for the covariance function parameters for both class C0 and C1. 96
4.2
Tuning parameter values for each classiﬁcation method. The optimal value
of the tuning parameter is listed along with the CVE associated with this
value. The optimal values were chosen by minimizing the ﬁve-fold CVE.
.
97
4.3
Training and test errors for the SE Asia land cover data obtained using each
of the classiﬁcation methods discussed in this chapter. . . . . . . . . . . . . 100
xi

LIST OF FIGURES
Figure
Page
1.1
Land cover over Southeast Asia, covering the region bounded by 17◦to
21◦N and 98◦to 105◦E. The data were taken from the MODIS Land Cover
Type Yearly Level 3 Global 500m (MOD12Q1 and MCD12Q1) data prod-
uct for the year 2005. . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
26
1.2
Elevation (in meters) over the region bounded by 17◦to 21◦N and 98◦to
105◦E. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
27
1.3
Standardized value of the measured distance to the nearest major road over
the region bounded by 17◦to 21◦N and 98◦to 105◦E. . . . . . . . . . . . .
27
1.4
Standardized value of the measured distance to the coast over the region
bounded by 17◦to 21◦N and 98◦to 105◦E. . . . . . . . . . . . . . . . . . .
28
1.5
Standardized value of the measured distance to the nearest big city over the
region bounded by 17◦to 21◦N and 98◦to 105◦E. . . . . . . . . . . . . . .
28
2.1
Illustration of neighborhood structures for a regular grid. The grid cell
with the black dot represents the location of interest and its neighbors are
represented by the grid cells with the empty squares for (a) a ﬁrst order
neighborhood structure and (b) a second order neighborhood structure. . . .
38
3.1
Histograms and trace plots for β and ρ under Scenario 1 for each of the
three corresponding algorithms. The black portion represents the burn-in
and the colored portion represents draws from the posterior distribution. . .
58
3.2
Histograms and trace plots for β and ρ under Scenario 2 for each of the
three corresponding algorithms. The black portion represents the burn-in
and the colored portion represents draws from the posterior distribution. . .
59
xii

3.3
Histograms and trace plots for β and λ under Scenario 3 for each of the
three corresponding algorithms. The black portion represents the burn-in
and the colored portion represents draws from the posterior distribution. . .
60
3.4
Histograms and trace plots for β and λ under Scenario 4 for each of the
three corresponding algorithms. The black portion represents the burn-in
and the colored portion represents draws from the posterior distribution. . .
61
3.5
Histograms and trace plots for β under Scenario 5 for each of the three
corresponding algorithms. The black portion represents the burn-in and the
colored portion represents draws from the posterior distribution.
. . . . . .
62
3.6
Autocorrelation and partial autocorrelation in the sample paths of β and ρ
under Scenario 1 for each of the three corresponding algorithms. . . . . . .
63
3.7
Autocorrelation and partial autocorrelation in the sample paths of β and ρ
under Scenario 2 for each of the three corresponding algorithms. . . . . . .
64
3.8
Autocorrelation and partial autocorrelation in the sample paths of β and λ
under Scenario 3 for each of the three corresponding algorithms. . . . . . .
65
3.9
Autocorrelation and partial autocorrelation in the sample paths of β and λ
under Scenario 4 for each of the three corresponding algorithms. . . . . . .
66
3.10 Autocorrelation and partial autocorrelation in the sample path of β under
Scenario 5 for each of the three corresponding algorithms.
. . . . . . . . .
67
3.11 σ vs. ˜β under Scenario 1 for each of the three corresponding algorithms.
The black dots represent the burn in and the colored dots represent the
draws from the posterior. Top row: First starting values. Bottom row:
Second starting values. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
68
3.12 σ vs. ˜β under Scenario 2 for each of the three corresponding algorithms.
The black dots represent the burn in and the colored dots represent the
draws from the posterior. Top row: First starting values. Bottom row:
Second starting values. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
69
3.13 σ vs. ˜β under Scenario 3 for each of the three corresponding algorithms.
The black dots represent the burn in and the colored dots represent the
draws from the posterior. Top row: First starting values. Bottom row:
Second starting values. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
70
xiii

3.14 σ vs. ˜β under Scenario 4 for each of the three corresponding algorithms.
The black dots represent the burn in and the colored dots represent the
draws from the posterior. Top row: First starting values. Bottom row:
Second starting values. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
71
3.15 σ vs. ˜β under Scenario 5 for each of the three corresponding algorithms.
The black dots represent the burn in and the colored dots represent the
draws from the posterior. Top row: First starting values. Bottom row:
Second starting values. . . . . . . . . . . . . . . . . . . . . . . . . . . . .
72
4.1
Illustration of the hyperplane (solid line) and margins (dotted lines) deter-
mined by support vector machines separating the two classes (denoted by
the two plotting symbols). The hyperplane in (a) is the maximum margin
hyperplane and the hyperplane in (b) does not maximize the margin. . . . .
91
4.2
Estimated and ﬁtted variograms for class C0 (left) and C1 (right) ﬁt using
the training data set. The numbers 1-4 indicate the estimated variogram for
the four covariates, the dots represent the mean of the four variograms, and
the line indicates the ﬁtted variogram.
. . . . . . . . . . . . . . . . . . . .
96
xiv

CHAPTER 1
INTRODUCTION
The ﬁeld of spatial statistics encapsulates a wide array of statistical methodology for
analyzing spatial data, or observations associated with particular points or regions in space.
The increasing availability and variety of spatial data has generated an enormous amount
of methodological development in the ﬁeld over the past 20 years, and today the ﬁeld is
one of the most active areas of research in all of statistics (see Banerjee et al., 2004; Waller
and Gotway, 2004; Schabenberger and Gotway, 2005; Diggle and Ribeiro Jr., 2007, for a
discussion of recent advances). Spatial statistical methods are also increasingly being used
in areas of application across the biological, environmental, health, and social sciences, and
often speciﬁc problems in these ﬁelds motivate further methodological developments. In
fact, the motivation for much of the research presented in this dissertation is the study of
land-cover/land-use change (LCLUC) using satelite-derived land cover observations. Be-
fore discussing this motivating application that is used throughout this dissertation, we ﬁrst
provide background and motivation as to why incorporating spatial information is impor-
tant in statistical analyses of spatially-referenced data in Section 1.1. In Section 1.2 we give
an overview of existing spatial statistical methodology for spatially-dependent categorical
data. In Section 1.3, we outline the contributions of this dissertation in terms of statistical
methodology for analyzing spatially-dependent categorical data using the Bayesian spatial
1

probit regression model. We then discuss the LCLUC motivating application by introduc-
ing a speciﬁc illustrative data set in Section 1.4.
1.1
Background and Motivation
Let si ∈D ⊂Rr be an r × 1 vector in Euclidean space. In spatial statistics, typically,
r = 2 or 3, indicating two- or three-dimensional space, respectively. Let, Y (si) ≡Yi for
i = 1, . . . , n, where, for now, Yi is a continuous real-valued random variable observed at
a point si ∈D. There are three types of spatially-referenced data (Cressie, 1993), deﬁned
by the nature of the spatial domain, D:
• Geostatistical/point-referenced data, where si varies continuously over D and D is a
ﬁxed subset of Rr. In this case, Y (si) can potentially be observed everywhere within
D.
• Lattice/gridded data, where D is again a ﬁxed subset of Rr, but in this case, the
elements of D are areal units. Thus, si may not represent a point in space, but will
indicate an areal unit indexed by the point si.
• Point pattern data, where D is still a subset of Rr, but the si are now viewed as
random. For point pattern data, the observed si are modeled as realizations from a
stochastic process, whereas with geostatistical and lattice data, the points are usually
assumed to be ﬁxed and non-stochastic.
In this dissertation, we only consider statistical methods for the analysis of geostatistical
and lattice data.
We expect spatially-referenced data to have a speciﬁc type of dependence structure,
called spatial dependence, where observations near to one another are more similar than
2

observations further apart. Often, a statistical analysis of spatial data will focus on learn-
ing about the nature of the spatial dependence or in exploiting it for prediction purposes.
Sometimes, however, we may need to account, or adjust, for spatial dependence when we
want to measure or quantify relationships between two observable quantities of interest. In
particular, in a regression analysis a researcher may be interested in determining how one
variable Y , known as the response variable or dependent variable, varies given a set of
other variables x, known as covariates, independent variables, or explanatory variables.
For example:
• Environmental Health. Researchers and policy makers may be interested in the re-
lationship between exposure to particulate matter or other pollutants and a particular
health outcome (e.g., cancer or mortality).
• Real Estate. An economist may be interested in determining factors that contribute
to whether or not a house will sell in a particular period of time.
• Land-Cover/Land-Use Change. Researchers may be interested in determining the
demographic, social, geographical, and political factors contributing to the observed
type of land cover at various locations.
The Classical Linear Regression Model
Before describing the spatial regression model, we ﬁrst review the linear regression
model from both a classical and Bayesian perspective. Let {(Yi, xi); i = 1, . . . , n}, be
a collection of paired observations, where Yi ∈R is a univariate real-valued response
variable or dependent variable, and xi is a k × 1 vector of continuous or discrete-valued
covariates or independent variables. For now, assume the Yi are conditionally independent
3

for all i = 1, . . . , n given the independent variables. The linear regression model speciﬁes
the following relationship between the components of xi and the mean of Yi:
E(Yi) = x′
iβ,
where β is a k × 1 vector of ﬁxed but unknown regression coefﬁcients. The classical linear
regression model is often written as
Yi = x′
iβ + ϵi,
(1.1)
where ϵi is a random noise error term with mean 0 and variance σ2 and is assumed to be
independent for all i = 1, . . . , n. It is common to assume that the ϵi are also normally
distributed, but that assumption is not necessary.
One method for ﬁtting the linear regression model is maximum likelihood estimation.
When the ϵis are assumed to be normally distributed, the likelihood function is
L(β, σ2|Y ) ∝p(Y |x1, . . . , xn, β, σ2)
=
n
Y
i=1
p(Yi|xi, β, σ2)
=
n
Y
i=1
1
√
2πσ2 exp{−1
2σ2(Yi −x′
iβ)2},
(1.2)
where Y = (Y1, . . . , Yn)′. Maximizing (1.2) with respect to β and σ2 yields the following
maximum likelihood estimator (MLE) for β:
ˆβMLE =
 
1
n
n
X
i=1
xix′
i
!−1  
1
n
n
X
i=1
xiYi
!
.
We can write this estimator of β in matrix notation by letting X = (x′
1, . . . , x′
n)′ so that
ˆβMLE = (X′X)−1 X′Y .
4

The MLE of σ2 is
ˆσ2
MLE = 1
n ˆϵ′ˆϵ,
where ˆϵ = Y −X ˆβMLE. This likelihood-based model-ﬁtting approach is considered to
be within the class of classical or frequentist statistics and standard inferential statements
about the uncertainty in model parameter estimates can be obtained (e.g., conﬁdence inter-
vals, hypothesis tests). Another method for ﬁtting the linear regression model is known as
ordinary least squares (OLS), which does not require an assumption about the distribution
of the error terms, but instead seeks to minimize the sum of the squared residuals, or the
difference between the observed response Yi and the predicted value of the response ˆYi. We
note that the OLS estimator for β is identical to the MLE provided above. As an alternative
to these two approaches, we can consider a Bayesian version of the linear regression model,
which we discuss below.
The Bayesian Linear Regression Model
In Bayesian analyses, we specify prior distributions on the model parameters; for the
linear regression model, a prior distribution is needed for β and σ2, and is denoted π(β, σ2).
A prior distribution is a probability distribution which captures our knowledge about the
model parameters prior to observing the data. Then, using the data likelihood and prior dis-
tribution, we determine posterior distributions for the model parameters, or the distribution
of the model parameters given the data, using Bayes’ Theorem. For the linear regression
model, the posterior distribution is π(β, σ2|Y ). Posterior distributions are determined ei-
ther analytically or numerically using simulation-based techniques such as Markov chain
Monte Carlo (MCMC); see Gelman et al. (1995) for a general overview of Bayesian data
5

analysis techniques. In the Bayesian setting, inferences about model parameters are then
reported based on summaries of the posterior distributions of model parameters.
Before describing the Bayesian linear regression model, we ﬁrst discuss the role of the
covariates X in the speciﬁcation of the posterior distribution of the model parameters, fol-
lowing the discussion in Chapter 14 of Gelman et al. (1995). In the Bayesian paradigm, the
observed data in a regression analysis include both Y and X. Because of this, a Bayesian
model will include the speciﬁcation of a joint distribution on (Y , X), p(Y , X|ψY |X, ψX),
where ψY |X are the model parameters associated with the distribution of Y |X – for the
Bayesian linear regression model, ψY |X = (β, σ2) – and ψX are the parameters associ-
ated with X. However, the Bayesian linear regression model traditionally assumes that the
distribution of X provides no additional information about the conditional distribution of
Y |X, or that
p(Y , X|ψY |X, ψX) = p(Y |ψY |X, X)p(X|ψX)
(1.3)
and ψY |X and ψX are assumed independent a priori so that
π(ψY |X, ψX) = π(ψY |X)π(ψX).
(1.4)
Thus, a Bayesian regression model inherently assumes that the prior distribution on X
provides no information about the regression model parameters, ψY |X = (β, σ). To see
this, notice that using equations (1.3) and (1.4), we can write the joint posterior distribution
of (ψY |X, ψX) as
π(ψY |X, ψX|Y , X) =
p(Y , X|ψY |X, ψX) π(ψY |X, ψX)
R R
p(Y , X|ψY |X, ψX) π(ψY |X, ψX) dψY |X dψX
=
p(Y |ψY |X, X) π(ψY |X) p(X|ψX) π(ψX)
 R
p(Y |ψY |X, X) π(ψY |X) dψY |X
  R
p(X|ψX) π(ψX) dψX

= π(ψY |X|Y , X)π(ψX|X).
6

It follows that we can determine the posterior distribution of ψY |X by only considering
π(ψY |X|Y , X) ∝π(ψY |X)p(Y |X, ψY |X).
(1.5)
To distinguish the roles X and Y play in Bayesian inference on ψY |X, it is common
practice to tacitly condition on X and denote the posterior distribution of β and σ2 as
π(β, σ2|Y ). We follow this convention throughout this dissertation except in Chapter 4 for
reasons that we discuss therein.
Although any probability distribution can be chosen for modeling the error terms of the
regression model, typically, the errors are assumed to be normally distributed. Equivalently,
it is assumed that
Y |β, σ2, X ∼N(Xβ, σ2I),
where I is the n×n identity matrix. We then assign prior distributions to the parameters β
and σ2. The standard choice for the Bayesian linear regression model is the noninformative
prior distribution
π(β, σ2|X) ∝1
σ2.
Using these likelihood and prior distributions results in a posterior distribution that can be
decomposed as follows:
π(β, σ2|Y ) = π(β|σ2, Y )π(σ2|Y ),
where
β|σ2, Y ∼N( ˆβB, σ2VB),
and
σ2|Y ∼ˆσ2
B(χ2
n−k)−1,
7

with
ˆβB = (X′X)−1 X′Y ,
VB = (X′X)−1 ,
and
ˆσ2
B =
1
n −k

Y −X ˆβB
′ 
Y −X ˆβB

,
and where (χ2
n−k)−1 denotes the inverse chi-squared distribution with n −k degrees of
freedom.
Often in Bayesian analyses, when the full joint posterior distribution is not available in
closed form, we use simulation-based approaches to generate samples from the posterior
distribution. We can make inferences about (β, σ2) by using a simulation-based (Monte
Carlo) approach where we generate samples from the posterior distribution of (β, σ2).
To do this, we sample from σ2|Y and then, given the sampled value of σ2, sample from
β|σ2, Y . Posterior inferences are then reported based on the empirical distribution of these
sampled values.
Comparison to Classical Estimators
Notice that in the Bayesian case, the mean of the conditional posterior distribution for
β|σ2, Y is the same as the MLE, i.e., ˆβB = ˆβMLE. Furthermore, ˆσ2
B =
n
n−k ˆσ2
MLE. There-
fore, Bayesian inferences on model parameters can be similar to inferences based on the
classical estimators. A Bayesian analysis, however, offers additional beneﬁts, including:
1. Flexibility in model speciﬁcation. Although the means of the posterior distributions
of β and σ2 are similar to the classical estimators, this is not always the case. To see
this, suppose we have more information a priori about β: for example, we may know
8

or require β > 0, where 0 is a k × 1 vector of zeros. A Bayesian analysis provides
a natural and straightforward way to allow for this prior knowledge or constraint, to
be included in an analysis through an informative prior distribution.
2. Interpretability of inferences. In a classical/frequentist setting, inferences are often
reported as conﬁdence intervals. Conﬁdence intervals, however, are not probability
intervals, although many interpret them as such. In a Bayesian setting, the posterior
distribution provides interpretable probabilistic statements about model parameters,
making clear the uncertainty associated with parameter estimates. Furthermore, this
uncertainty is easily propagated to uncertainty in deterministic and stochastic func-
tions of parameters (e.g., transformations and predicted values).
3. Allowing for increased complexity in model speciﬁcation. Increasing the complex-
ity of a model (e.g., hierarchical modeling with a large number of unknown pa-
rameters, allowing for complicated spatial dependence structures, or modeling non-
continuous data) can make model-ﬁtting in a classical/frequentist setting infeasible.
However, simulation-based approaches such as MCMC (see discussion below) can
make Bayesian model ﬁtting feasible in high dimensional situations.
The Spatial Linear Regression Model
We now consider a more general form of linear regression in order to account for spatial
dependence among the errors. In some instances, the error terms in the linear regression
model will not be independent. This is often the case for spatially-referenced data, but there
can also be residual dependence in data obtained from studies using repeated measures or
when data are observed over time. When an assumption of residual independence is not
valid, we can specify a general linear regression model. The model is the same as in (1.1),
9

however we no longer assume the ϵi are independent, but rather assume that
Var(ϵ) = Σ,
where Σ is a valid and invertible covariance matrix (i.e., positive deﬁnite and symmet-
ric). Again, it is common to assume ϵ is normally distributed, but this assumption is not
necessary.
To estimate the parameters of the general linear model, we consider two cases, Σ is
known and Σ is unknown. When Σ is known, for maximum likelihood estimation, we
typically specify a normal distribution on the error terms resulting in the likelihood function
L(β, |Y ) ∝p(Y |X, β, Σ)
=
1
(2π)n/2|Σ|1/2 exp

−1
2(Y −Xβ)′Σ−1(Y −Xβ)

.
The MLE for β is then
ˆβMLE =
 X′Σ−1X
−1 X′Σ−1Y .
Weighted (or generalized) least squares, an extension of OLS, results in the same estimator
for β.
When Σ is unknown, we can parameterize Σ (i.e., let Σ = Σ(θ)). The spatial linear
model is deﬁned to be the general linear model with a speciﬁc type of dependence captured
by Σ(θ) (see the discussion in Section 2.2.2). In this case, the likelihood function is
L(β, θ|Y ) ∝p(Y |X, β, Σ(θ))
=
1
(2π)n/2|Σ|1/2 exp

−1
2(Y −Xβ)′ (Σ(θ))−1 (Y −Xβ)

,
which is maximized with respect to β and θ to obtain the MLE. Another likelihood-based
approach to estimation is restricted/residual maximum likelihood (REML; Patterson and
10

Thompson, 1971) estimation where we introduce a matrix C satisfying E(CY ) = 0 and
Var(CY ) = σ2I. Then, given C, we use standard maximum likelihood estimation to
ﬁt the model CY = CXβ + Cϵ. Another approach is based on an extension of OLS
called iteratively reweighted least squares (IRLS). IRLS ﬁrst ﬁxes the covariance Σ to
some preliminary estimate and, conditioning on this value, uses the weighted least squares
estimator to obtain an initial estimate of β. Using this estimate of β, IRLS estimation
then obtains residuals and determines a more accurate estimate of Σ. Using the updated
estimates, IRLS iterates between these two steps until the values of the parameter estimates
have converged.
Alternatively, we can specify a Bayesian version of the general linear model by spec-
ifying prior distributions on model parameters and using a simulation-based method to
approximate the posterior distributions of model parameters. The model-ﬁtting approach
is similar to that used for the Bayesian linear regression model, however, we now specify
a prior distribution on Σ or on parameters deﬁning Σ. Because of the parameterization of
Σ, ﬁtting a Bayesian general linear regression model requires more complex simulation-
based approaches than those used in the Bayesian linear regression model. In general, the
joint posterior distribution of β and θ cannot be decomposed into the product of standard
density functions to allow Monte Carlo samples from the posterior to be drawn directly.
Instead, it has become standard practice to use an estimation technique known as Markov
chain Monte Carlo (MCMC). Rather than drawing directly from the posterior distribution,
an MCMC algorithm draws samples from a Markov chain with a long run, or stationary
distribution, equal to the posterior. Two popular MCMC algorithms are:
11

• The Gibbs sampler, where we draw a sample from the full conditional posterior dis-
tributions of each parameter sequentially. For the Bayesian spatial linear regression
model, for each iteration we sample
1. β[t] ∼π(β|Y , θ[t−1])
2. θ[t] ∼π(θ|Y , β[t])
where β[t] and θ[t] represent the sampled values of β and θ for the tth iteration, and
each is sampled conditional on the current value of the other parameter.
• The Metropolis-Hastings algorithms, where for each iteration, we draw a sample
from a proposal distribution and either accept or reject the proposed value. Let ψ
be the parameter of interest and Y the observed data. Then, we accept the proposed
value with probability
a(ψ∗, ψ[t−1]) = min
 π(ψ∗|Y )/Jt(ψ∗|ψ[t−1])
π(ψ[t−1]|Y )/Jt(ψ[t−1]|ψ∗), 1

,
where ψ∗is the proposed value of ψ sampled from the proposal distribution for iter-
ation t, Jt(·|ψ[t−1]). It follows from Bayes’ theorem that this acceptance probability
can be simpliﬁed to rely only on the prior, likelihood, and proposal distribution, i.e.,
a(ψ∗, ψ[t−1]) = min

π(ψ∗)p(Y |ψ∗)/Jt(ψ∗|ψ[t−1])
π(ψ[t−1])p(Y |ψ[t−1])/Jt(ψ[t−1]|ψ∗), 1

.
The Metropolis random walk algorithm is a special case of the Metropolis-Hastings
algorithm, where the proposal distribution for each parameter is a symmetric dis-
tribution centered on the previous sampled value of that parameter. In this case,
Jt(ψ∗|ψ[t−1]) = Jt(ψ[t−1]|ψ∗), resulting in the simpliﬁed form for the acceptance
12

probability given by,
a(ψ∗, ψ[t−1]) = min

π(ψ∗)p(Y |ψ∗)
π(ψ[t−1])p(Y |ψ[t−1]), 1

.
When ﬁtting Bayesian models, it is also common to use a mixture of these two MCMC
algorithms. For example, in the Gibbs sampler for the Bayesian spatial regression model,
we can use a Metropolis-Hastings step in place of Step 2 listed above.
We note that we must specify starting values for the parameters for both types of
MCMC algorithms listed above. To reduce the effect of these starting values on the ﬁnal
parameter inferences and allow the Markov chains to reach their stationary distributions,
typically we discard the sampled draws from a number of the initial iterations. These dis-
carded values are called the burn in samples.
Although it is a special case of the general linear regression model, we argue that the
spatial linear regression model has certain features that distinguish it from other models
in this class. First, data modeled using the general linear regression model typically have
some sort of replication, where, for example, we observe a response for n individuals m
times. However, with spatially-referenced data, although we have n locations, often times
we observe a spatial process at these locations only once. Secondly, the speciﬁc nature
of the dependence places constraints on Σ. In this dissertation, we consider two classes
of parameterizations for Σ(θ), corresponding to geostatistical and lattice data, and discuss
these in more detail in Section 2.2.
By accounting for spatial dependence in the spatial linear regression model, we can:
13

1. Quantify the relationship between the response variable and covariates while ac-
counting for residual spatial dependence. When we ignore residual spatial depen-
dence, inferences on model parameters will be invalid and the standard errors associ-
ated with these estimators tend to be too small (see, for example, Schabenberger and
Gotway, 2005).
2. Predict the value of the response variable at an unobserved location using both ob-
served values of the response variable and covariate information. We expect that re-
sponse variables at unobserved locations will be more similar to nearby observations
than to observations which are farther away, and accounting for this phenomenon is
imperative in prediction.
Up to this point, we have only considered regression models for continuous response
variables. Frequently, however, the response variable will not be continuous but may be
reported as belonging to two or more categories. At the beginning of this section, we gave
three examples of situations where a spatial regression model would be desirable. However,
in each of these examples, the response variable may not be continuous:
• Environmental Health. The health outcome of an individual (e.g., has cancer/does
not have cancer) is a binary response variable.
• Real Estate. Whether or not the house sells during a particular period of time is a
binary response variable.
• Land-Cover/Land-Use Change. The land cover category (e.g., forest, agriculture,
grassland, or urban) at each location is a categorical response variable.
14

When the response variable is discrete or categorical, the spatial linear regression model
for continuous response variables is not appropriate. In the following section, we give an
overview of existing approaches for modeling spatially-dependent categorical data.
1.2
Modeling Categorical Spatial Data
Let {(Yi, xi); i = 1, . . . , n} be paired observations at location si, where Yi is now a
categorical response variable and xi is a corresponding k × 1 vector of covariates. Let
Y = (Y1, . . . , Yn)′ and X = (x′
1, . . . , x′
n)′.
1.2.1
The Spatial Generalized Linear Model
To motivate the spatial generalized linear model (GLM), we ﬁrst describe the GLM for
independent response variables, ﬁrst introduced by Nelder and Wedderburn (1972). Let
Yi for i = 1, . . . , n, be independent random variables. If the distribution of Yi|ζi, ωi is a
member of the exponential family, we can write
p(Yi|ζi, ωi) = exp
Yiζi −b(ζi)
a(ωi)
+ c(Yi, ωi)

.
For the special case of a two-category, or binary, response variable, Yi|ζi, ωi can be assumed
to have a Bernoulli distribution with
ζi
=
log

pi
1 −pi

b(ζi)
=
log(1 + exp(ζi))
a(ωi)
≡
a = 1
c(Yi, ωi)
≡
c = 0
15

where pi is the probability that Yi = 1. The mean of Yi is E(Yi) = b′(ζi) ≡λi and variance
of Yi is Var(Yi) = b′′(ζi)a(ω) ≡vλi. Therefore, for the Bernoulli distribution,
E(Yi) = b′(ζi) =
exp(ζi)
1 + exp(ζi) = pi
and
Var(Yi) = b′′(ζi)a(ω) =
exp(ζi)
(1 + exp(ζi))2 = pi(1 −pi).
For an n × 1 vector Y = (Y1, . . . , Yn), we have
E(Y ) ≡λ = p
and
Var(Y ) = Vλ,
where Vλ is a diagonal matrix with the ith diagonal element equal to vλi = pi(1 −pi).
To relate the Yis to the covariates xi, we introduce a so-called link function, h(·), and
assume
h(λi) ≡µi = x′
iβ,
where β is a k × 1 vector of coefﬁcients. Two frequently used link functions for the
Bernoulli distribution are the logit and probit functions:
Logit: µi = log

pi
1 −pi

= x′
iβ
Probit: µi = Φ−1(pi) = x′
iβ.
Therefore,
µ = Xβ,
16

where X = (x′
1, . . . , x′
n)′ is an n × k matrix of covariates and µ is a k × 1 vector.
One approach to estimate β is by specifying a quasi-likelihood (QL; Wedderburn,
1974). Let Ui = u(λi; Yi) be the standardized value of Yi, so that, Ui = (Yi −λi)/vλi. The
ﬁrst two moments of this random variable have the same properties as the log-likelihood
derivatives, so
Ui = Yi −λi
vλi
=
∂
R λi
Yi
Yi−t
vt dt
∂λi
= ∂Q(λi; Yi)
∂λi
.
Thus,
Q(λi; Yi) =
Z λi
Yi
Yi −t
vt
dt.
Because the data are independent, Q(λ; Y ) = Pn
i=1 Q(λi; Yi) = V −1
λ (Y −λ). Notice
that Q(λ; Y ) is a function of λ and Y , but we are actually interested in estimating β.
Therefore, we determine U(β) = ∂Q(λ; Y )/∂β by calculating
U(β) = ∂Q(λ; Y )
∂λ
∂λ
∂β = ∆′V −1
λ (Y −λ)
where ∆is an n × k matrix with elements ∆ij = ∂λi/∂βj. We then estimate β by solving
the quasi-likelihood estimation equations (i.e., setting U( ˆβ) = 0 and solving for ˆβ).
For the generalized linear model, we now consider the case where the elements of Y
are not independent. Albert and McShane (1995) and Gotway and Stroup (1995) redeﬁne
the variance of Y to be
Var(Y ) = V 1/2
λ
R(θ)V 1/2
λ
≡ΣY ,
where Vλ is deﬁned as above, R(θ) is a correlation matrix, and θ is an m × 1 vector of
dependence parameters with m ≥1. For spatially-referenced data, R(θ) will be a spatial
correlation matrix like those given in Section 2.2.
17

To ﬁt the GLM for correlated response variables, Liang and Zeger (1986) extend the
QL approach to model ﬁtting. They use ΣY , in place of Vλ, resulting in
U(β) = ∂Q(λ; Y )
∂λ
∂λ
∂β = ∆′Σ−1
Y (Y −λ).
(1.6)
These equations, U(β), are called generalized estimating equations (GEE).
In the spatial setting, Albert and McShane (1995) use GEE not only to ﬁt β, but also to
ﬁt ΣY . In addition to (1.6), they let
A′ ˆΣ−1
Y (ˆσY −σY ) = 0
(1.7)
where ˆΣY is the working covariance matrix, σY and ˆσY are n(n−1)/2×1 vectors consist-
ing of all entries below the diagonal of ΣY and ˆΣY , and A = ∂σY /∂β. They then iterate
between solving (1.6) and (1.7) until convergence. Lin and Clayton (2005) show asymp-
totic normality and consistency of the GEE for binary spatial data with isotropic covariance
functions. They prove and apply their results for the logit link function. Lin (2008) extends
the GEE to Lp space.
Breslow and Clayton (1993) propose using a pseudolikelihood function to estimate
model parameters in the spatial generalized linear model. Their approach is similar to GEE,
but it also computes ‘pseudodata’ corresponding to the observed binary process within the
iterations of the model-ﬁtting algorithm.
In the literature for spatially-referenced data, there are no Bayesian versions of the
GEE and pseudolikelihood methods described above. This is due to the fact that these
approaches do not have a true likelihood function. Yin (2009) proposes a Bayesian gener-
alized method of moments that uses a weighted quadratic objective function in place of a
likelihood function. To the best of our knowledge, this approach has not been applied in
18

a spatial setting. Instead, it is common to use a Bayesian spatial generalized linear mixed
model (see Section 1.2.2).
Albert and McShane (1995) emphasize the fact that the GEE approach treats the spatial
correlation as a nuisance. In contrast, the GLMM approach seeks to estimate the mean of
the data given the spatial random effect, as described below.
1.2.2
The Spatial Generalized Linear Mixed Model
The spatial generalized linear mixed model (GLMM) extends the GLM by introducing
a random spatially-dependent error term, or spatial random effect, denoted S(si) ≡Si, into
the mean model. The spatial GLMM can be written as
E(Yi|Si)
=
λ(si) ≡λi
h(λi)
=
x′
iβ + Si
where E(Si) = 0 and Var(S) = Σ(θ) with S = (S1, . . . , Sn)′ and Σ(θ) an n × n spatial
covariance matrix.
The spatial GLMM was ﬁrst introduced within the Bayesian setting by Diggle et al.
(1998) as a general modeling framework for spatially-dependent discrete data. As an al-
ternative, a classical version of the spatial GLMM was introduced by Heagerty and Lele
(1998). Heagerty and Lele propose a composite likelihood approach for ﬁtting a spatial
GLMM for binary response data. Paciorek (2007) gives a review of various versions of
spatial logistic GLMMs, as well as a discussion of the speciﬁcation of spatial dependence
structure in this class of models.
Diggle et al. (1998) propose ﬁtting the Bayesian spatial GLMM using MCMC algo-
rithms. This model-ﬁtting approach is popular due to established computer software, such
19

as WinBUGS (Lunn et al., 2000), that can be used to ﬁt Bayesian spatial GLMMs (see, for
example Banerjee et al., 2004; Law and Haining, 2004). However, as noted by Paciorek
(2007), MCMC algorithms for Bayesian spatial GLMM often convergence slowly and ex-
hibit poor mixing. To improve the performance of these algorithms, Langevin-Hastings
methods have been suggested in the literature (Christensen et al., 2001; Christensen and
Waagepetersen, 2002; Christensen and Ribeiro Jr., 2002). This method involves modifying
a Metropolis random walk algorithm, so that the proposal distribution not only relies on
the value of the parameter from the previous iteration, but also relies on the gradient of the
log-likelihood function. Christensen et al. (2006) propose further improvements by trans-
forming the posterior values of the spatial random effects, Si, by modifying Si|Yi with a
Cholesky factorization of the posterior covariance matrix. They illustrate that this method
is more robust than either not transforming the Sis or transforming the Sis a priori.
When S has a Gaussian distribution, an alternative Bayesian model-ﬁtting approach
to MCMC is using integrated nested Laplace approximations (INLA; Rue et al., 2009).
INLA seeks to approximate posterior marginals rather than producing samples from the
joint posterior distribution of model parameters. While this approach is attractive, it is not
clear whether its applicability is as general as with MCMC methods.
1.2.3
Indicator Kriging
Rather than modeling relationships between response variables and covariates, indica-
tor kriging predicts values of binary random variables at unobserved locations based only
on nearby binary observations (Switzer, 1977; Journel, 1983). Indicator kriging is based on
a spatial prediction method called kriging and is a direct extension of this popular method
to the binary data setting. Indicator kriging requires a function γ(dij) = (1/2)Var(Yi −Yj),
20

where γ(·) is an isotropic semivariogram and dij = ||si −sj|| are the distance between
locations. Using this function, prediction of Y (s0), where s0 is an unobserved location, is
based on p0 ≡P(Y (s0) = 1|Y ). An estimate of p0, ˆp0, is its best linear unbiased estimator
(Cressie, 1993; De Oliveira, 2000):
ˆp0 =

γ + 1 −1′ Γ−1 γ
1′ Γ−1 1
1
′
Γ−1Y
where Γ is an n×n matrix with the ijth element equal to γ(||si −sj||), γ is an n×1 vector
with the ith element equal to γ(||s0 −si||), and 1 is an n × 1 vector of ones. Then, Y (s0)
is predicted by taking
ˆY (s0) =
(
1, if ˆp0 >
l0
l0+l1
0, otherwise
,
where l0 and l1 are speciﬁed losses for mispredicting Yi as a 0 and 1, respectively.
Indicator kriging was originally motivated by considering indicator functions of a col-
lection of real-valued random variables Z = (Z(s1), . . . , Z(sn))′ ≡(Z1, . . . , Zn)′ and
deﬁning Yi = I(Zi < z) for i = 1, . . . , n (Switzer, 1977; Journel, 1983). Solow (1993)
compares the estimates of the probability p0 = P(I(Z(s0) < z) = 1|I(Z1 < z), . . . , I(Zn <
z)) using indicator kriging to the probability p0∗= P(Z(s0) < z|Z), where Z has a Gaus-
sian distribution. Although p0 ̸≡p0∗, Solow ﬁnds that the number of incorrectly predicted
values based on Y and Z were similar and relatively small. However, he only uses a sample
size of n = 4.
Indicator kriging has the beneﬁt that no distributional assumptions about the data gen-
erating process are required. However, there is no guarantee the estimated probabilities will
stay within the appropriate range of [0, 1]. Furthermore, if p0∗is the probability of interest,
21

Cressie (1993) states that theoretically, disjunctive kriging1 provides a better approxima-
tion to this probability than indicator kriging. Diggle et al. (1998) also discuss the weak
theoretical assumptions of indicator kriging.
Other suggested approaches for improving indicator kriging are indicator cokriging
(Journel, 1983), which includes covariates to supplement predictions, and probability krig-
ing (Cressie, 1993), which utilizes an estimate of the cumulative distribution function of
Z within the indicator kriging framework. However, these methods also do not guarantee
estimated probabilities remain in the appropriate range.
1.2.4
The Autologistic Model
First proposed by Besag (1972), the autologistic model directly models the spatial cor-
relation among the binary data by relating the log odds of Yi = 1 to the values of Yj for sj
in some neighborhood of si. Let
pi ≡P(Yi = 1|Y-i, xi, β, α),
where Y-i indicates Y with the ith element removed. The autologistic model assumes that
log

pi
1 −pi

= x′
iβ + α
X
j:i∼j
Yj
where xi is a k × 1 vector of covariates at location si, β is a k × 1 vector of coefﬁcients,
i ∼j indicates that location j is a neighbor to location i, and α is the spatial dependence
parameter. In this setting, the log-odds at each location is not only a linear function of the
covariates, but is also dependent on the neighboring binary observations. Extensions and
generalizations of this model have been proposed by Besag (1972), Besag (1974), Augustin
1Disjunctive kriging requires speciﬁcation of bivariate distributions of (Zi, Zj), 0 ≤i < j ≤n and
then estimates any measurable function g(Z0), where in the binary setting, g(Z0) = 1(Z0 < z|z) and
E(g(Z0)) = p0∗.
22

et al. (1996), Gumpertz et al. (1974), Sim (2000), Zheng and Zhu (2008), and Zhu et al.
(2008).
Methods for ﬁtting the autologistic model include coding, pseudo-likelihood, Monte
Carlo maximum likelihood, and, for a Bayesian version, Gibbs sampling. Coding meth-
ods, proposed by Besag (1974), begin by separating the data into subsets. For each sub-
set, conditional on the values in the remaining portion of the data set, the observations in
that subset are independent. Given these conditionally independent observations, condi-
tional maximum likelihood is used to estimate the parameters. Because there are multiple
ways to separate the data, inconsistencies arise when this method is used. Huffer and Wu
(1998) introduce a Monte Carlo maximum likelihood method to ﬁt this model, and show
their approach yields more efﬁcient estimates than pseudo-likelihood and coding. Sherman
et al. (2006) give an overview and comparison of pseudo-likelihood, generalized pseudo-
likelihood, and Monte Carlo maximum likelihood model-ﬁtting methods. Augustin et al.
(1996) use a Gibbs sampler to estimate the model parameters.
The autologistic model offers an improvement over indicator kriging in that the esti-
mated pis must be in [0, 1]. In addition, the autologistic model does not require any under-
lying distributional assumptions. However, Weir and Pettitt (1999) discuss computational
challenges associated with ﬁtting the model and note that parameter estimates can be poor
when strong spatial dependence is present.
1.2.5
The Bayesian Spatial Probit Regression Model
The ﬁnal model in the literature used to model spatially-dependent binary data is the
Bayesian spatial probit regression model. Because this model is the focus of this disserta-
tion, we provide a detailed introduction to it in Chapter 2.
23

1.3
Overview of Contributions
In this dissertation, we add to the development of the Bayesian spatial probit regression
model in the following three ways:
1. Efﬁcient Model Fitting: Models for spatially-dependent data are notoriously cumber-
some to ﬁt. We show how a marginal data augmentation MCMC algorithm can more
efﬁciently ﬁt the Bayesian spatial probit regression model than standard MCMC al-
gorithms.
2. Spatial Classiﬁcation: Within the classiﬁcation literature, classiﬁcation methods
which allow for spatial dependence are limited. We show how a spatial classiﬁcation
rule can be derived from the Bayesian spatial probit regression model and provide
an example where our spatial classiﬁer outperforms other well known classiﬁcation
methods.
3. Mulitnomial Model Speciﬁcation: When extending the Bayesian spatial probit re-
gression model to the multi-categorical setting, special considerations must be made
when specifying the latent variable mean and covariance structure to ensure model
parameters are estimable and interpretable. We discuss various speciﬁcations of the
latent mean structure and the associated parameter interpretation, and explore the
speciﬁcation of the latent cross spatial-categorical dependence structure. Addition-
ally, we discuss how data augmentation MCMC strategies for ﬁtting the Bayesian
spatial probit regression model can be extended to the multi-category setting.
24

1.4
Illustrative Data Set
To illustrate our methods, we use satellite-derived land cover observations over South-
east Asia. In Southeast Asia, deforestation is a major concern and, over the last century,
much of the original forests–as much as 12 percent–have been lost to other land uses
(Munroe et al., 2008). Researchers are interested in the economic, geographic, social,
and demographic factors that contribute to deforestation and other land cover patterns. The
spatial probit regression model is well suited for assessing the strength of the relationships
between binary response variables and covariate information while also allowing for resid-
ual spatial dependence.
The particular data used in our analyses were taken from the Moderate Resolution
Imaging Spectroradiometer (MODIS) Land Cover Type Yearly Level 3 Global 500m
(MOD12Q1 and MCD12Q1) data product for the year 2005. We selected land cover ob-
servations from this data product corresponding to the region bounded by 17◦to 21◦N and
98◦to 105◦E, which covers portions of Myanmar, Thailand, Laos, and Vietnam. Figure 1.1
shows an image of the land cover over this region. For this ﬁgure, the International Human
Dimensions Programme (IHDP) land cover classiﬁcations provided by the MODIS data
product were collapsed into ﬁve categories: forest, shrub/grassland, savanna, cropland, and
other.
We also consider four covariates: elevation, distance to the nearest major road, distance
to the coast, and distance to the nearest big city. Elevation is measured in meters and
distances are Euclidean and measured in degrees. The covariates are standardized, meaning
that there were no costs taken into account in calculating distance (e.g. distance calculations
do not take into account the fact that it might take longer to go over mountains than go
25

Figure 1.1: Land cover over Southeast Asia, covering the region bounded by 17◦to 21◦N
and 98◦to 105◦E. The data were taken from the MODIS Land Cover Type Yearly Level 3
Global 500m (MOD12Q1 and MCD12Q1) data product for the year 2005.
around them). Figures 1.2 - 1.5 show images of the four respective covariates. The city in
the middle of Figure 1.5 (distance to big city) is Vientiane, the capitol of Laos.
Finally, we note that Figure 1.3 has an artiﬁcial wavy line break. This is most likely
due to the inadequacies in the plotting capabilities of R, the software used to create these
images. However, it may also be an artifact of the country borders and further explanations
are being explored. For the data analyses in Sections 3.3 and 4.4, we use a portion of the
data not affected by this break (i.e., 17◦to 19◦N and 98◦to 100◦E).
26

Figure 1.2: Elevation (in meters) over the region bounded by 17◦to 21◦N and 98◦to 105◦E.
Figure 1.3: Standardized value of the measured distance to the nearest major road over the
region bounded by 17◦to 21◦N and 98◦to 105◦E.
27

Figure 1.4: Standardized value of the measured distance to the coast over the region
bounded by 17◦to 21◦N and 98◦to 105◦E.
Figure 1.5: Standardized value of the measured distance to the nearest big city over the
region bounded by 17◦to 21◦N and 98◦to 105◦E.
28

CHAPTER 2
BAYESIAN SPATIAL PROBIT REGRESSION
An attractive alternative to the models for spatially-dependent categorical data reviewed
in Section 1.2 is the Bayesian spatial probit regression model. In this chapter, we moti-
vate this model by ﬁrst describing in Section 2.1.1 the latent variable representation of the
Bayesian probit regression model for independent response variables as proposed by Al-
bert and Chib (1993). In Section 2.1.2 we present extensions to the mutlivariate and multi-
category response variable setting. Following this discussion, we introduce the Bayesian
spatial probit regression model in Section 2.2.
2.1
The Bayesian Probit Regression Model
2.1.1
Albert and Chib’s Data Augmentation Strategy
Consider {(Yi, xi); i = 1, . . . , n}, a collection of n binary response variables Yi and
corresponding k×1 vectors of covariates xi. As discussed in Section 1.2.1, the probit GLM
relating the covariates to the binary response variable assumes that the Yis are conditionally
independent given a k × 1 vector of regression coefﬁcients β and can be written as
Yi|β ∼Bernoulli(pi)
Φ−1(pi) = x′
iβ,
(2.1)
29

where Φ−1(·) denotes the inverse standard normal cumulative distribution function. In
the Bayesian setting, a prior distribution must be speciﬁed for the unknown parameter β.
Unlike the normal linear regression model where the normal distribution is a conjugate
prior for the regression coefﬁcients, a conjugate prior is not available for β in (2.1). Thus,
inference on β typically requires numerical integration, which is not feasible if k is large,
or a simulation-based approach such as MCMC.
To facilitate the use of the Gibbs sampler in the Bayesian probit regression model, Al-
bert and Chib (1993) propose the following data augmentation representation of the model.
They introduce a collection of latent variables ˜Z = ( ˜Z1, . . . , ˜Zn)′ and take
Yi =
(
1, if ˜Zi > 0
0, if ˜Zi ≤0
,
(2.2)
where
˜Z ∼N(X ˜β, σ2In),
(2.3)
X = (x1, . . . , xn)′ is an n × k matrix of covariates, In is the n × n identity matrix, and
σ2 is a variance parameter. (The ∼notation over the random variables used here distin-
guishes identiﬁable and non-identiﬁable parameters; we use this notational convention to
aid our discussion of data augmentation strategies in Section 3.1.) Taking σ2 = 1, setting
Zi = ˜Zi/σ and β = ˜β/σ, and integrating out the Zis, it is straightforward to show that
Albert and Chib’s model speciﬁcation is equivalent to the probit GLM given in (2.1). In-
troducing the latent ˜Zis and taking the prior on β to be N (mβ, Cβ) facilitates model ﬁtting
via the following Gibbs sampler:
30

Step 1: Sample Z|Y , β, (σ2 = 1).
For i = 1, . . . , n, sample Zi from
Zi|Yi, β, (σ2 = 1) ∼
(
TN(x′
iβ, σ2 = 1, 0, ∞),
if Yi = 1
TN(x′
iβ, σ2 = 1, −∞, 0),
if Yi = 0 ,
where TN(µ, σ2, l, u) denotes a truncated normal distribution with mean µ, variance
σ2, lower bound l, and upper bound u. By sampling from these univariate truncated
normal distributions, we obtain a sample for Z (Geweke, 1991). In this dissertation,
when sampling from the truncated normal distribution, we use Hans and Craigmile
(2009), which provides a tool in R, based on Geweke (1991), for efﬁciently sampling
from a truncated normal distribution, particularly when sampling in the tails of the
distribution.
Step 2: Sample β|Z, Y , (σ2 = 1).
When β ∼N(0, Cβ), the full conditional distribution of β is given by
β|Z, Y , (σ2 = 1) ∼N
 (C−1
β
+ X′X)−1X′Y , (C−1
β
+ X′X)−1
.
Each of these steps involves drawing from known distributions from which sampling is
easily implemented; see Albert and Chib (1993) for details.
2.1.2
Multi-Category and Multivariate Extensions
Albert and Chib (1993)’s data augmentation strategy for the probit GLM has been ex-
tended in several ways. We ﬁrst deﬁne notation for various possible model extensions, then
describe each of the model extensions in more detail. Consider observations associated
with n ‘individuals’, each of whom provides m ‘responses’ (i.e., multivariate observations),
which fall into one of ℓcategories. Using this notation, the latent variable representation
31

of the Bayeisan probit regression model described in the previous section accommodates
n > 1 individuals with m = 1 responses for each individual, and ℓ= 2 possible cate-
gories, corresponding to the two categories of a binary response. We now consider various
extensions of Albert and Chib’s data augmentation strategy when ℓ> 2 or m > 1.
In addition to introducing the latent variable representation of the probit regression
model binary responses, Albert and Chib (1993) also provide a multi-category (multino-
mial) extension. Now Yi ∈{1, . . . , ℓ} denotes the categorical outcome associated with the
ith individual, where ℓ> 2 is the number of categories. In this case, the latent variables
˜Zij for i = 1, . . . , n and j = 1, . . . , ℓare introduced to facilitate model ﬁtting. The latent
variable representation of this model is given by
Yi = arg maxj { ˜Zij, j = 1, . . . , ℓ}
where
˜Zi ∼N([1ℓ⊗x′
i] ˜β, ˜Ω),
and ˜Zi = ( ˜Zi1, . . . , ˜Ziℓ)′, 1ℓis an ℓ× 1 vector of ones, xi is a k × 1 vector of covariates, ˜β
is a k×1 vector of regression coefﬁcients, and ˜Ωis an ℓ×ℓcovariance matrix. Conditional
on ˜β and ˜Ω, the ˜Zis are independent. That is,
vec( ˜Z) ≡


˜Z1
...
˜Zn

∼N(X ˜β, In ⊗˜Ω),
(2.4)
where
X ≡


1ℓ⊗x′
1
...
1ℓ⊗x′
n


is a nℓ× k matrix and ⊗denotes the Kronecker or tensor product between two matrices.
For ˜β to be identiﬁable, the ﬁrst diagonal element of ˜Ω, ˜Ω1,1, is typically set equal to one
32

(Albert and Chib, 1993; McCulloch et al., 2000). McCulloch and Rossi (1994) and Nobile
(1998), however, do not impose identiﬁability constraints on ˜Ω, but rather assign proper pri-
ors on all parameters and report marginal posterior inferences on the identiﬁed parameters
(e.g., ˜β/ ˜Ω1,1). Additional approaches for handling this issue of parameter identiﬁability
(i.e., Imai and van Dyk, 2005) will be discussed in Section 3.1.
Albert and Chib (1993)’s latent variable representation of the probit GLM has also been
extended by Chib and Greenberg (1998) to the multivariate setting where n > 1 individuals
have m > 1 responses with ℓ= 2 categories for each response. They model this type of
independent multivariate binary observations, Y1, . . . , Yn, where Yi = (Yi1, . . . , Yim)′, by
introducing latent variables ˜Zih for i = 1, . . . , n and h = 1, . . . , m. In this case,
Yih =
(
1, if ˜Zih > 0
0, if ˜Zih ≤0
where
˜Zi ∼N([1m ⊗x′
i] ˜β, Σ),
and ˜Zi = ( ˜Zi1, . . . , ˜Zim)′, 1m is the m×1 vector of ones, xi is a k ×1 vector of covariates,
˜β is a k ×1 vector of regression coefﬁcients, and Σ is an m×m correlation matrix. Again,
conditional on β and Σ, the ˜Zis are independent. That is,
vec( ˜Z) ≡


˜Z1
...
˜Zn

∼N(X ˜β, σ2I ⊗Σ),
where
X ≡


1m ⊗x′
1
...
1m ⊗x′
n


is a nm × k matrix. For ˜β to be identiﬁable, Chib and Greenberg require Σ to be a
correlation matrix and set σ2 = 1. More recently, to facilitate model ﬁtting, Liu and Daniels
33

(2006) relax the assumptions on the correlation matrix by using parameter expansion and
reparameterization methods to ﬁrst sample Σ as a covariance matrix and then translate it
back to a correlation matrix.
2.2
The Bayesian Spatial Probit Regression Model
2.2.1
Model Speciﬁcation
We now discuss incorporating spatial dependence into the Bayesian probit regression
model. From the extensions considered in the previous section, this can be done in two
ways. The ﬁrst way to view this model extension is to consider observations at the different
locations as n dependent observations (rather than n independent observations as in Sec-
tion 2.1.1), so that we have n > 1 ‘individuals’ (or locations in the spatial setting), m = 1
‘responses’ at each location, and ℓ= 2 possible categories for each observation. We could
also view the observations across all locations as a single multivariate response variable
(i.e, n = 1 individuals with m > 1 responses which can take on ℓ= 2 possible categories).
Although the resulting models are equivalent, we arbitrarily take the ﬁrst view in deﬁn-
ing our notation, and consider n spatially-dependent univariate binary response variables,
Y1, . . . , Yn. The resulting model is equivalent to De Oliveira (2000)’s “clipped Gaussian
random ﬁelds” and the basis for the model considered by Weir and Pettitt (2000).
For the spatial probit regression model, we introduce latent variables ˜Z = ( ˜Z1, . . . , ˜Zn)′,
which are realizations of a spatially-dependent Gaussian process, and let
Yi =
(
1, if ˜Zi > 0
0, if ˜Zi ≤0
(2.5)
where
˜Z ∼N(X ˜β, σ2Σ(θ)),
(2.6)
34

with X = (x1, . . . , xn)′, and we assume that the marginal variances of the Zis are known
up to a multiplicative constant σ2 and that the matrix Σ(θ) captures the residual spatial
dependence structure. Typically, Σ(θ) will be a correlation matrix. However, we allow
for the possibility of heteroskedasticity by only assuming that the diagonal elements of
Σ(θ) are ﬁxed and known constants. We note that in Chapter 1, Σ(θ) represented a spatial
covariance matrix. In our discussion of the Bayesian spatial probit regression model, we
will refer to σ2 as the variance of Zi for i = 1, . . . , n, and Σ(θ) as a spatial correlation
matrix.
As in the two previous model extensions, ˜β is only identiﬁable up to a multiplicative
constant. For illustration, consider P(Yi = 1| ˜β, Σ(θ), σ2):
P

Yi = 1| ˜β, Σ(θ), σ2
=
P

˜Zi > 0| ˜β, Σ(θ), σ2
=
Φ
 
x′
i ˜β
σ
!
=
Φ (x′
iβ) ,
where β = ˜β/σ is the identiﬁable parameter. De Oliveira (2000) and Weir and Pettitt
(2000) chose to set σ2 = 1 to ensure that the regression coefﬁcients are identiﬁable, a
choice that has implications for the efﬁciency of the resulting MCMC algorithms as we
discuss Section 3.1.
Although similar in spirit to that of the spatial GLM and spatial GLMM, this model
ﬁts within a separate class under the GLM framework. For the spatial GLM, the spatial
dependence is speciﬁed directly on Y , rather than on a latent Gaussian process. The spatial
GLMM is more similar to the Bayesian probit regression model in that we introduce a
latent spatially-dependent Gaussian process in the mean of Y ; however, the probability pi
35

is speciﬁed conditional on the value of the Gaussian process, Si, i.e.,
pi = P(Yi = 1|β, Si)
for all i = 1, . . . , n. In contrast to the spatial GLM and the spatial GLMM, the spatial
dependence structure is embedded in the link function.
2.2.2
Parameterization of the Spatial Correlation Matrix
When modeling spatial dependence, it is important to make certain that Σ(θ) is a valid
correlation matrix. Various parameterizations of Σ(θ) that ensure validity and uphold the
characteristics of spatial dependence are available. We consider two classes of parameteri-
zations for Σ(θ), corresponding to geostatistical and lattice data.
For geostatistical/point-referenced data, a geostatistical dependence structure is com-
monly used. In this setting, the correlation of a spatial process at two locations is often
modeled as a function of the distance between the two locations corresponding to the as-
sumptions of second-order stationarity and isotropy. One popular class of parametric spa-
tial correlation functions is the Mat´ern class. In this case,
Σ(θ) ≡Σ(ν, λ),
where the ijth element of Σ(ν, λ) is equal to
1
2ν−1Γ(ν)
2√νdij
λ
ν
Kν
2√νdij
λ

,
Γ(·) is the usual gamma function, Kν is the modiﬁed Bessel function of order ν (see e.g.,
Abramowitz and Stegun, 1965), dij = ||si−sj|| is the Euclidean distance between locations
si and sj, ν > 0 is a parameter controlling for smoothness of the realized random ﬁeld,
and λ > 0 is the spatial scale parameter. Special cases of the Mat´ern correlation functions
36

include the exponential (ν = 1/2) and the Gaussian (ν →∞) correlation functions. Both
of these correlation functions can be written in a simpler form, i.e.,
Σ(θ) ≡Σ(λ).
For the exponential correlation function, the ijth element of Σ(λ) is equal to
exp

−dij
λ

,
(2.7)
and for the Gaussian correlation function, the ijth element of Σ(λ) is equal to
exp

−d2
ij
λ2

,
(2.8)
where dij and λ are as deﬁned above. There are other parametric correlation functions for
geostatistical data, and we refer the reader to Cressie (1993), Stein (1999), and Banerjee
et al. (2004) for further examples.
For lattice/gridded data, spatial dependence is modeled by considering spatial neigh-
borhood structures. Neighborhood structures deﬁne a set of neighbors for each partition
of a domain D indexed by locations {si, . . . , sn}. Figure 2.1 illustrates this concept for
a regular lattice/grid. In this ﬁgure, grid cell with the black dot represents the location
of interest and its neighbors are represented by the grid cells with empty squares. Figure
2.1 (a) illustrates a ﬁrst order neighborhood structure, where the ith and jth grid cells are
neighbors if they share a common edge, and (b) illustrates a second order neighborhood
structure, where the ith and jth grid cells are neighbors if they share a common edge or
corner.
A spatial autoregressive structure is commonly used to capture spatial dependence in
models for lattice/gridded data. One example of an autoregressive dependence structure
37

(a)
(b)
First Order Neighborhood Structure
Second Order Neighborhood Structure
Figure 2.1: Illustration of neighborhood structures for a regular grid. The grid cell with the
black dot represents the location of interest and its neighbors are represented by the grid
cells with the empty squares for (a) a ﬁrst order neighborhood structure and (b) a second
order neighborhood structure.
is the conditionally autoregressive (CAR) model (e.g., Banerjee et al., 2004). In the CAR
model,
Σ(θ) ≡Σ(ρ) = (Dw −ρW )−1
(2.9)
where the ijth element of W is equal to wij, wij is equal to 1 if grid cell/partition i is a
neighbor of cell/partition j and is equal to 0 if cells i and j are not neighbors, Dw is a
diagonal matrix with the ith diagonal element equal to wi+ = Pn
j=1 wij, σ2 is the variance,
and ρ is the spatial dependence parameter. Other autoregressive dependence structures in-
clude the simultaneous autoregressive (SAR) model and the spatial moving average (SMA)
model (see, e.g., Cressie, 1993).
38

CHAPTER 3
DATA AUGMENTATION MCMC STRATEGIES
There has been a recent emphasis in the spatial statistics literature on the development
of methods that accommodate large data sets. In the Bayesian setting, these methods in-
clude dimension reduction techniques (e.g., Higdon, 2002; Xu et al., 2005; Calder, 2007;
Banerjee et al., 2008), integrated nested Laplacian approximations (Rue et al., 2009), and
covariance tapering (see recent work by Shaby and Ruppert, 2010, for a Bayesian treatment
of this technique). In this chapter, instead of focusing on model adjustments to accommo-
date large data sets or approximations to full Bayesian inference procedures, we investi-
gate strategies for efﬁcient Markov chain Monte Carlo (MCMC) algorithms for ﬁtting the
Bayesian spatial probit regression model. While the MCMC strategies discussed here are
not necessarily designed to overcome computational challenges associated with massive
data sets, in high dimensional settings having efﬁcient algorithms is clearly desirable.
Data augmentation/latent variable methods have been widely recognized for facilitating
model ﬁtting of the Bayesian probit regression model. As discussed in Section 2.1.1, the
latent variable representation of the Bayesian probit regression model proposed by Albert
and Chib (1993) allows model ﬁtting to be performed using a simple Gibbs sampler. To im-
prove the efﬁciency of the Gibbs sampler in this setting, Imai and van Dyk (2005) propose
introducing a working parameter (deﬁned in Section 3.1.1) into the model and compare
39

various data augmentation strategies resulting from different treatments of the working pa-
rameter. In this chapter, we build on this work by investigating the efﬁciency of modiﬁed
and extended versions of these algorithms for the spatial probit regression model, focus-
ing on the special case of binary response variables. These algorithms include the one
previously proposed by De Oliveira (2000), which we discuss further in Section 3.1.1.
In Section 2.2, we deﬁned the latent variable representation of the Bayesian probit
regression model for spatially-referenced binary data. However, as noted in Section 2.2, the
spatial variance parameter, σ2, is not identiﬁable. In Section 3.1.1, we discuss conditional
and marginal data augmentation strategies for making use of this non-identiﬁable variance
parameter within an MCMC model-ﬁtting algorithm. We propose three different Gibbs
sampling algorithms corresponding to these strategies. Furthermore, in Section 3.1.2, we
propose modiﬁcations to these algorithms by partially collapsing over the sampling steps
within the Gibbs sampler. We compare the various resulting algorithms using a simulation
study in Section 3.2 and in an analysis of satellite-derived land cover data over Southeast
Asia in Section 3.3.
3.1
Data Augmentation MCMC Strategies
3.1.1
Conditional versus Marginal Data Augmentation
In the previous chapter, “data augmentation” referred to the introduction of continuous
latent variables, ˜Z, in the Albert and Chib (1993) representation of the Bayesian probit
regression model. In this section, we extend our use of “data augmentation” to include
conditional and marginal data augmentation MCMC strategies, where we use a working pa-
rameter to identify fast and easily implemented algorithms. In data augmentation MCMC
40

strategies, the working parameter is frequently taken to be a parameter that is not identiﬁ-
able under the observed data, Y , but is identiﬁable under the complete or augmented data,
(Y , Z). In the Bayesian spatial probit regression model, the spatial variance parameter, σ2,
can serve as a working parameter.
Meng and van Dyk (1999) and van Dyk and Meng (2001) were the ﬁrst to distinguish
between conditional augmentation and marginal augmentation strategies. Under a condi-
tional augmentation strategy, the working parameter is ﬁxed to an optimal constant within
the model-ﬁtting algorithm, whereas a marginal augmentation strategy seeks to marginalize
over the working parameter within the algorithm. Meng and van Dyk (1999) show that al-
gorithms using a marginal augmentation strategy will have a geometric rate of convergence
no larger than its conditional augmentation counterpart.
Below we describe conditional and marginal augmentation strategies for the Bayesian
spatial probit regression model deﬁned in Section 2.2. Referring back to the notation in
Section 2.2, we use ˜Z and ˜β to denote the non-identiﬁable parameters, and Z and β to
denote the identiﬁable parameters (i.e., ˜Z = σZ and ˜β = σβ).
Consider the likelihood function for the identiﬁable parameters (β, θ) in the spatial
probit regression model introduced in Section 2.2:
L(β, θ|Y ) ∝p(Y |β, θ)
=
Z
p(Y , Z|β, θ)dZ,
(3.1)
where (Y , Z) denotes the complete augmented data. As Imai and van Dyk (2005) point
out for an independent multi-category response version of the probit regression model,
since the working parameter is not identiﬁable under the observed data likelihood function,
we can condition on a ﬁxed value of the working parameter and the likelihood function
41

will remain unchanged. This conditional augmentation strategy also holds for the spatial
version of the model.
Fixing the working parameter σ2 to some constant σ2
0 results in a conditional augmen-
tation algorithm, and without loss of generality, we can take σ2
0 = 1.2 Thus, for conditional
augmentation, (3.1) becomes
L(β, θ|Y ) = L(β, θ, σ2|Y )
∝
Z
p(Y , Z|β, θ, σ2 = σ2
0)dZ
=
Z
A1
· · ·
Z
An
1
(2πσ2
0)n/2|Σ(θ)|1/2
× exp

−1
2σ2
0
(Z −Xβ)′Σ(θ)−1(Z −Xβ)

dZ
(3.2)
where
Ai =
(
(−∞, 0], if Yi = 0
(0, ∞), if Yi = 1
.
(3.3)
Using this conditional augmentation strategy, the associated Gibbs sampling algorithm
for sampling from the posterior distribution of (β, θ) is given in Table 3.1 under the Con-
ditional heading under the Non-Collapsed Algorithms heading.
As an alternative to conditioning on a speciﬁc value of the working parameter, the
working parameter can be assigned a proper prior which we can marginalize over to obtain
the likelihood of the identiﬁable parameters. Using this marginal augmentation strategy,
(3.1) can be expressed as
L(β, θ|Y ) ∝
Z 
p(Y , Z|β, θ, σ2)π(σ2|β, θ)dσ2
dZ
=
Z
A1
· · ·
Z
An
Z ∞
0
1
(2πσ2)n/2|Σ(θ)|1/2
× exp

−1
2σ2(Z −Xβ)′Σ(θ)−1(Z −Xβ)

π(σ2|β, θ)dσ2

dZ
(3.4)
2De Oliveira (2000) implicitly uses this conditional augmentation strategy in his model for spatially-
dependent binary data.
42

where the Ai are as deﬁned in (3.3).
Following Imai and van Dyk (2005), we consider two marginal data augmentation
schemes. In the ﬁrst scheme, labeled Scheme 1, we marginalize over σ2 completely in
updating Z. We do this by sampling (σ2)∗from its prior distribution π(σ2|β, θ) ≡π(σ2),
sampling from ˜Z|Y , β, θ, (σ2)∗, and “sweeping” over σ2 by setting Z = ˜Z/σ∗. Thus, the
sampled Z is dependent on the identiﬁable parameter β, but not on the non-identiﬁable
parameter ˜β. This approach is valid since σ2 is not likelihood identiﬁable, therefore we
can sample ˜Z conditional on any plausible value of σ2. We then sample from the joint
full conditional distribution of (σ2, ˜β) and from the full conditional distribution of θ and
again “sweep” over the sampled value of σ2 by setting β = ˜β/σ. The Gibbs sampling algo-
rithm associated with this marginal augmentation scheme is listed in Table 3.1 as Marginal-
Scheme 1 in the Non-Collapsed Algorithms section.
In the second marginal augmentation scheme, labeled Scheme 2, we include σ2 in the
Gibbs sampler in the usual way by assigning it a proper prior distribution and sampling
from its full conditional distribution. Then, we sweep over σ by properly normalizing the
non-identiﬁable parameters in each iteration of the algorithm (i.e., set Z = ˜Z/σ and β =
˜β/σ). This algorithm is listed in Table 3.1 as Marginal-Scheme 2 of the Non-Collapsed
Algorithms.
In Marginal-Scheme 1, when the prior distribution on σ2 is diffuse, conditioning on a
value of σ2 from the prior distribution when sampling ˜Z will allow the distribution of ˜Z
to be more diffuse than when conditioning on the value of σ2 obtained from the previous
iteration of the algorithm, as in Marginal-Scheme 2. Thus, the sampled values of ˜Z/σ∗=
Z when using Marginal-Scheme 1 will have a smaller autocorrelation. In turn, we expect
a similar decrease in autocorrelation in the sample paths of β and θ.
43

Imai and van Dyk (2005) consider similar conditional and marginal augmentation strate-
gies for ﬁtting the Bayesian multinomial probit regression model for independent multi-
category response variables (see Section 2.1.2). However, in their model, the working
parameter is the ﬁrst diagonal element of the cross-category covariance matrix, ˜Ω1,1. Com-
paring their algorithms to that of McCulloch and Rossi (1994) and Nobile (1998), they ﬁnd
that their marginal augmentation algorithms converge more quickly and are less sensitive
to starting values. In the following sections, through a simulation study and data analysis,
we show similar beneﬁts for MCMC algorithms based on marginal data augmentation for
the spatial probit regression model.
Non-Collapsed Algorithms
Marginal-Scheme 1
Marginal-Scheme 2
Conditional
Step 1:
Sample (σ2)∗∼π(σ2)
Sample ˜Z|Y , β, θ, (σ2)∗
Sample ˜Z|Y , β, θ, σ2
Sample Z|Y , β, θ, σ2 = 1
Set Z = ˜Z/σ∗
Set Z = ˜Z/σ
Step 2:
Sample (σ2, ˜β)| ˜Z, Y , θ
Sample (σ2, ˜β)| ˜Z, Y , θ
Sample β| ˜Z, Y , θ, σ2 = 1
Set β = ˜β/σ
Set β = ˜β/σ
Step 3:
Sample θ| ˜Z, Y , ˜β, σ2
Sample θ| ˜Z, Y , ˜β, σ2
Sample θ|Z, Y , β, σ2 = 1
Partially Collapsed Algorithms
Marginal-Scheme 1
Marginal-Scheme 2
Conditional
Step 1:
Sample θ|Y , β
Sample θ|Y , β, σ2
Sample θ|Y , β, σ2 = 1
Sample (σ2)∗∼π(σ2)
Sample ˜Z|Y , β, θ, (σ2)∗
Sample ˜Z|Y , β, θ, σ2
Sample Z|Y , β, θ, σ2 = 1
Set Z = ˜Z/σ∗
Set Z = ˜Z/σ
Step 2:
Sample (σ2, ˜β)| ˜Z, Y , θ
Sample (σ2, ˜β)| ˜Z, Y , θ
Sample β| ˜Z, Y , θ, σ2 = 1
Set β = ˜β/σ
Set β = ˜β/σ
Table 3.1: This table lists the steps in each of the data augmentation algorithms. The ﬁrst
portion shows the non-collapsed data augmentation algorithms introduced in Section 3.1.1.
The second portion shows the partially collapsed data augmentation algorithms introduced
in Section 3.1.2.
44

3.1.2
Partially Collapsed Algorithms
In this section, we discuss a method for collapsing Steps 1 and 3 in the algorithms
introduced in the previous section. In Step 3 of each algorithm, we draw samples of θ from
θ| ˜Z, Y , ˜β, σ2. Because ˜Z is a vector of real-valued random variables, by conditioning
on it (as opposed to the binary vector Y ) we unnecessarily constrain the distribution of
values that θ can take at each iteration of the algorithm, particularly for high dimensional
˜Z. Instead of sampling from θ| ˜Z, Y , ˜β, σ2, we could marginalize over ˜Z and sample from
θ|Y , ˜β, σ2. The latter distribution will be more diffuse than the former, and thus intuitively
we might expect improvements in the efﬁciency of the algorithm.
Consider the joint full conditional distribution θ and ˜Z, where
p(θ, ˜Z|Y , ˜β, σ2) ∝p(Y , ˜Z| ˜β, σ2, θ)π(θ).
(3.5)
Since the left hand side of (3.5) can be decomposed into the product of p(θ|Y , ˜β, σ2) and
p( ˜Z|Y , ˜β, σ2, θ), Steps 1 and 3 can be collapsed into a single step where we sample from
θ|Y , ˜β, σ2 and then from ˜Z|Y , ˜β, σ2, θ. From (3.5),
p(θ|Y , ˜β, σ2) ∝
Z
A1
· · ·
Z
An
p(Y , ˜Z| ˜β, σ2, θ)d ˜Z

π(θ),
(3.6)
where the quantity in square brackets is simply the volume under the n-dimensional multi-
variate normal density function corresponding to the orthant of Rn deﬁned by A1×· · ·×An.
Thus, a Metropolis-Hastings step can be used to sample from θ|Y , ˜β, σ2. Sampling from
˜Z|Y , ˜β, σ, θ can be done as before. The modiﬁed versions of the algorithms introduced in
Section 3.1.1 that collapse Steps 1 and 3 are listed in Table 3.1 under the heading Partially
Collapsed Algorithms, following terminology used by van Dyk and Park (2008).
45

Finally, we note that it is straightforward to show that
p(θ|Y , ˜β, σ2) = p(θ|Y , β, σ2 = 1).
Therefore, in the modiﬁed Marginal-Scheme 1 algorithm, we do not condition on a partic-
ular value of σ2 in sampling θ, thus ensuring that this partially collapsed algorithm samples
from the appropriate posterior distribution (see van Dyk and Park, 2008, for related discus-
sion).
3.1.3
Full Conditional Distributions
Using the priors β ∼N(0, Cβ), σ2 ∼a0(χ2
v0)−1, and θ ∼π(θ), we obtain the fol-
lowing full conditional distributions. We use the superscript [t] to denote the value of a
parameter at the tth iteration of the algorithm.
Non-collapsed Algorithms
Step 1: Sample Z[t] from Z|Y , β[t−1], θ[t−1], (σ2)∗:
Each algorithm treats σ2 differently, so that for Marginal-Scheme 1 (σ2)∗∼π(σ2),
for Marginal-Scheme 2 (σ2)∗= (σ2)[t−1], and for Conditional (σ2)∗= 1.
For i = 1, . . . , n, deﬁne Z∗
¬i = (Z[t]
1 , . . . , Z[t]
i−1, Z[t−1]
i−1 , . . . , Z[t−1]
n
)′ and sample ˜Zi
from
˜Zi|Y , Z∗
¬i, β[t−1], θ[t−1], (σ2)∗∼
(
TN(µ˜zi, τ 2
˜zi, 0, ∞),
if Yi = 1
TN(µ˜zi, τ 2
˜zi, −∞, 0),
if Yi = 0 ,
where TN(µ˜zi, τ 2
˜zi, ℓ, u) is a truncated normal distribution with lower and upper bounds
ℓand u, respectively, and mean and variance
µ˜zi = x′
i σ∗β[t−1] +

Σ(θ[t−1])

i,¬i

Σ(θ[t−1])

¬i,¬i
−1  σ∗Z∗
¬i −X¬i σ∗β[t−1]
τ 2
˜zi = (σ2)∗

Σ(θ[t−1])

i,i −

Σ(θ[t−1])

i,¬i

Σ(θ[t−1])

¬i,¬i
−1 
Σ(θ[t−1])

¬i,i

.
46

Set Z[t]
i = ˜Zi/σ∗.
Step 2: Sample (σ2)[t], β[t] from σ2, β|Y , Z[t], θ[t−1]:
For Marginal-Scheme 1 and Marginal-Scheme 2
(σ2)[t] ∼

( ˜Z −X ˆβ)′ Σ(θ[t−1])−1 ( ˜Z −X ˆβ) + a2
0 + ˆβ′ C−1
β
ˆβ

(χ2
n+v0)−1
where ˆβ =
 X′ Σ(θ[t−1])−1 X + C−1
β
−1 X′ Σ(θ[t−1])−1 ˜Z and ˜Z is taken from
Step 1.
For Conditional, set (σ2)[t] = 1.
Then, for all algorithms, sample
˜β ∼N( ˆβ, (σ2)[t]  X′Σ(θ[t−1])−1X + C−1
β
−1)
and set β[t] = ˜β/σ[t].
Step 3: Sample θ[t] from θ|Y , Z[t], β[t], (σ2)[t] via a Metropolis-Hastings step:
Sample a proposed value, θ∗, from a proposal distribution q(θ|θ[t−1]). Take
θ[t] =
(
θ∗,
with probability c(θ[t−1], θ∗)
θ[t−1],
with probability 1 −c(θ[t−1], θ∗) ,
where
c(θ[t−1], θ∗) = min
(
π(θ∗|Y , ˜Z, ˜β, (σ2)[t])
π(θ[t−1]|Y , ˜Z, ˜β, (σ2)[t])
q(θ[t−1]|θ∗)
q(θ∗|θ[t−1]), 1
)
.
In the acceptance probability,
π(θ|Y , ˜Z, ˜β, (σ2)[t]) ∝p(Y , ˜Z| ˜β, θ, (σ2)[t]) π(θ) = φ( ˜Z; X ˜β, (σ2)[t]Σ(θ)) π(θ),
where φ( ˜Z; X ˜β, σ2Σ(θ)) is the multivariate normal probability density function
with mean X ˜β and variance σ2Σ(θ) evaluated at ˜Z, and ˜Z is taken from Step 1
and ˜β is taken from Step 2.
47

Partially Collapsed Algorithms
Step 1: Sample θ[t], Z[t] from θ, Z|Y , β[t−1], (σ2)∗:
• Sample θ[t] from θ|Y , β[t−1] via a Metropolis-Hastings step.
Sample a proposed value, θ∗, from a proposal distribution q(θ|θ[t−1]). Take
θ[t] =
(
θ∗,
with probability c(θ[t−1], θ∗)
θ[t−1],
with probability 1 −c(θ[t−1], θ∗)
where
c(θ[t−1], θ∗) = min
 π(θ∗|Y , β[t−1])
π(θ[t−1]|Y , β[t−1])
q(θ[t−1]|θ∗)
q(θ∗|θ[t−1]), 1

.
In the acceptance probability,
π(θ|Y , β[t−1]) ∝p(Y |β[t−1], θ) π(θ)
=
Z
A1
· · ·
Z
An
φ(Z; Xβ[t−1], Σ(θ)) dZ π(θ),
where φ(Z; Xβ, Σ(θ)) is the multivariate normal probability density function
with mean Xβ and variance Σ(θ) evaluated at Z and the Ai are as deﬁned in
(3.3).
• Sample Z[t]|Y , β[t−1], θ[t], (σ2)∗.
Each algorithm treats σ2 differently, so that for Marginal-Scheme 1 (σ2)∗∼
π(σ2), for Marginal-Scheme 2 (σ2)∗= (σ2)[t−1], and for Conditional (σ2)∗=
1.
For i = 1, . . . , n, deﬁne Z∗
¬i = (Z[t]
1 , . . . , Z[t]
i−1, Z[t−1]
i−1 , . . . , Z[t−1]
n
)′ and sample
˜Zi from
˜Zi|Y , Z∗
¬i, β[t−1], θ[t], (σ2)∗∼
(
TN(µ˜zi, τ 2
˜zi, 0, ∞),
if Yi = 1
TN(µ˜zi, τ 2
˜zi, −∞, 0),
if Yi = 0 ,
48

where TN(µ˜zi, τ 2
˜zi, ℓ, u) is a truncated normal distribution with lower and upper
bounds ℓand u, respectively, and mean and variance
µ˜zi = x′
i σ∗β[t−1] +
h
Σ(θ[t])
i
i,¬i
h
Σ(θ[t])
i
¬i,¬i
−1 
σ∗Z∗
¬i −X¬i σ∗β[t−1]
τ 2
˜zi = (σ2)∗
 h
Σ(θ[t])
i
i,i −
h
Σ(θ[t])
i
i,¬i
h
Σ(θ[t])
i
¬i,¬i
−1 h
Σ(θ[t])
i
¬i,i
!
.
Set Z[t]
i = ˜Zi/σ∗.
Step 2: Sample (σ2)[t], β[t] from σ2, β|Y , Z[t], θ[t]:
For Marginal-Scheme 1 and Marginal-Scheme 2
(σ2)[t] ∼

( ˜Z −X ˆβ)′ Σ(θ[t])−1 ( ˜Z −X ˆβ) + a2
0 + ˆβ′ C−1
β
ˆβ

(χ2
n+v0)−1
where ˆβ =
 X′ Σ(θ[t])−1 X + C−1
β
−1 X′ Σ(θ[t])−1 ˜Z, and ˜Z is taken from Step
1.
For Conditional, set (σ2)[t] = 1.
Then, for all algorithms, sample
˜β ∼N( ˆβ, (σ2)[t]  X′Σ(θ[t])−1X + C−1
β
−1)
and set β[t] = ˜β/σ[t].
3.2
Simulation Study
3.2.1
Simulation Set-up
In our simulation study, we compare each of the six proposed algorithms in terms of
computational efﬁciency and sensitivity to starting values. We consider data sets of sample
size n = 100 corresponding to observations on a 10 × 10 regular grid. We generate our
49

data from the spatial probit regression model given in (2.5) and (2.6) with a single covari-
ate xi = xi and regression coefﬁcient β = β. We consider a geostatistical dependence
structure based on an exponential covariance function as deﬁned in (2.7), as well as on an
autoregressive structure based on the CAR model as deﬁned in (2.9). We also consider an
independent covariance structure (as in Section 2.1.1), i.e., Σ(θ) ≡In, to use as a baseline
for comparison.
When ﬁtting the independent model, we use the three data augmentation algorithms,
without ﬁtting the spatial dependence parameter. We note that the issue of collapsing these
algorithms is not relevant in this case. These algorithms are identical to those used by
Imai and van Dyk (2005) for the binary response special case of the multi-category probit
regression model. We show how adding spatial dependence to the model can impact the
convergence of the algorithms.
Under the three dependence structures, we consider both the non-collapsed and partially
collapsed algorithms introduced in Section 3.1 and deﬁne the ﬁve scenarios listed in Table
3.2. For each scenario, we compare the algorithms resulting from the various conditional
and marginal data augmentation strategies (i.e., Marginal-Scheme 1, Marginal-Scheme 2,
and Conditional).
Scenario
Spatial Dependence Structure
Partially Collapsed
1
CAR
No
2
CAR
Yes
3
Geostatistical
No
4
Geostatistical
Yes
5
Independent
–
Table 3.2: Scenarios used to compare the marginal and conditional data augmentation al-
gorithms.
50

In assigning prior distributions, the resulting models should be the same across all al-
gorithms. Thus, where applicable, we assign priors on identiﬁable parameters (i.e., on β
rather than ˜β). Furthermore, because of the non-identiﬁability within our model, it is im-
portant that the prior distributions on the parameters are proper. For Scenarios 1-5, we
assign a normal prior distribution to β (i.e., β ∼N(mβ, Cβ) with mβ = 0 and Cβ = 100).
Each algorithm uses the non-identiﬁable working parameter, σ, differently. For the condi-
tional augmentation algorithm, σ is ﬁxed at 1. For both marginal augmentation algorithms,
σ2 ∼aσ(χ2
vσ)−1 where aσ = 3 and (χ2
vσ)−1 represents an inverse chi-squared distribution
with parameter vσ = 3. The spatial dependence structures have different parameterizations
necessitating the need for different prior distributions on the spatial dependence parame-
ters. For the CAR spatial dependence structure, ρ ∼Unif(1/ξ(1), 1/ξ(n)), where ξ(1) and
ξ(n) are the smallest and largest eigenvalues of D−1/2
w
W D−1/2
w
(Banerjee et al., 2004, pg.
80). For the geostatistical dependence structure, λ ∼Unif(lλ, uλ) with lλ = 0 and uλ = 20.
In our simulation study, we generate data sets following the simulation example used
in Nobile (1998) and Imai and van Dyk (2005) for independent binary data, including spa-
tial dependence where appropriate: we independently generate the covariates xi from the
uniform distribution on the interval (-.5, .5); take β = −
√
2, σ2 = 1, and ˜β = σβ; and set
the spatial dependence parameters ρ = .9 (CAR structure) and λ = 2 (geostatistical struc-
ture). For each of the data sets generated under the the two spatial dependence structures
(CAR and geostatistical), we ﬁt the corresponding model using both the non-collapsed
and partially collapsed algorithms. For the independence case, we ﬁt the model using the
(non-collapsed) algorithms.
To compare the augmentation algorithms in terms of sensitivity to starting values, we
consider two different starting values for (σ, β), namely (σ, β) = (
√
2, −
√
2) and (σ, β) =
51

(10, −2). Each algorithm is run for 50,000 iterations, and we somewhat arbitrarily take the
ﬁrst 10,000 iterations to be the burn-in period.
3.2.2
Simulation Results
Using the ﬁve scenarios in Table 3.2, we compare the various algorithms in terms of
mixing, convergence, and sensitivity to starting values. As shown by the histograms in Fig-
ures 3.1 - 3.5, the algorithms result in nearly identical inferences on the posterior distribu-
tions of the identiﬁable parameters. Thus, with the inferences consistent across algorithms,
the algorithms can be compared in terms of their efﬁciency.
First, we note the trace plots of β and λ under the conditional algorithm of Scenario
3 shown in Figure 3.3. Here, we see evidence of potential lack of convergence from the
different behavior in the chain near iteration 50,000. On the other hand, there is no indica-
tion of convergence problems in the trace plots of the two Scenario 3 marginal algorithms,
nor in the trace plots for Scenario 5 algorithms – the algorithms for the independent probit
regression model – as seen in Figure 3.5. This difference provides evidence of additional
difﬁculties in ﬁtting spatial probit regression models and the need for more efﬁcient MCMC
algorithms.
Autocorrelation and partial autocorrelation plots of the (post burn-in) sample paths of
model parameters also help in determining whether an MCMC algorithm is mixing well.
Figures 3.6 and 3.8 show the autocorrelation and partial autocorrelation plots for the re-
gression coefﬁcient and spatial dependence parameter for Scenarios 1 and 3. It is clear that
among the three non-collapsed algorithms, Marginal-Scheme 1 results in the smallest and
most quickly decreasing autocorrelation in both parameters’ paths under Scenario 3. Un-
der Scenario 1, the top row plots show that the sample path of β under Marginal-Scheme
52

1 again has the smallest and most quickly decreasing autocorrelation, but the improve-
ment provided by the marginalization strategy is less apparent for the spatial dependence
parameter, ρ.
We also compare the convergence of the three partially collapsed algorithms using au-
tocorrelation plots. Figures 3.7 and 3.9 show the autocorrelation and partial autocorrelation
plots of the regression coefﬁcient and the spatial dependence parameter sample paths for
Scenarios 2 and 4. Here we see that, when compared with the other partially collapsed
algorithms, Marginal-Scheme 1 is again superior when we compare the autocorrelation of
the sampled parameter values using each of the three partially-collapsed algorithms. We
might also expect partial collapsing of the algorithms to further improve autocorrelation
summaries. However, for the CAR structure, partially collapsing the algorithms does not
result in improved autocorrelation summaries, as seen by comparing the autocorrelations
of Figure 3.6 and 3.7. On the other hand, for the geostatistical spatial dependence structure,
partially collapsing the algorithms does appear to improve mixing, as seen by comparing
the autocorrelations of Figures 3.8 and 3.9. In this scenario, the most noticeable beneﬁts of
collapsing appear to be for the conditional algorithms. The differences between the non-
collapsed and partially collapsed marginal algorithms are not nearly as strong. Given the
signiﬁcant increase in computation time required to run the partially collapsed algorithms
compared with their non-collapsed counterparts (it can take roughly 12 times as long to
generate the same number of posterior samples), partially collapsed marginal augmenta-
tion algorithms appear not to be a worthwhile.
All scenarios showed that Marginal-Scheme 1 was the least sensitive to starting values.
Figures 3.11 - 3.15 show scatter plots of sampled pairs of σ versus ˜β for Scenarios 1 - 5
generated under each of the three augmentation algorithms and both starting values. The
53

black dots show the burn-in samples and the colored dots show the draws from the posterior.
Under both starting values, Marginal-Scheme 1 appears to immediately generate samples
from the stationary posterior distribution and both parameters easily move around the entire
parameter space. However, under the second set of starting values, Marginal-Scheme 2
takes longer to converge and the parameters seem to move around the parameter space
more slowly.
Based on our simulation study, we recommend ﬁtting the spatial probit regression
model using the non-collapsed Marginal-Scheme 1 algorithm. This algorithm showed su-
perior mixing and convergence properties compared to the Marginal-Scheme 2 and Condi-
tional algorithms. When computation burden is taken into account, it does not appear that
partially collapsing this algorithm is beneﬁcial. In the next section, we compare the per-
formance of the non-collapsed Marginal-Scheme 1 and Conditional algorithms in an anal-
ysis of land cover data. These algorithms were selected based on the simulation ﬁndings
(non-collapsed Marginal-Scheme 1) and existing literature (non-collapsed Conditional; for
example, as in De Oliveira, 2000).
3.3
Application
To illustrate our methods, we use a portion of the data described in Section 1.4. For this
analysis, we considered the region bounded by 17◦to 19◦N and 98◦to 100◦E, which covers
a portion of northwestern Thailand and a small part of Myanmar. Using a 24×24 grid over
this region, we collapsed the response variable to two categories, forest and nonforest,
where the land cover response variable associated with each grid cell was taken to be the
most common observed land cover type, where forest was coded as “1” and non-forest was
coded as “0”. We used the covariate distance to the nearest major road in our analysis, and
54

deﬁned it over the grid to be the median distance for all measurements of this covariate
within a grid cell.
Using the spatial probit regression model deﬁned by (2.5) and (2.6) with the CAR spa-
tial dependence structure given in (2.9), we model the binary land cover response variable
and the distance to the nearest major road covariate. Unlike the simulation study, here we
include an intercept parameter. We expect that less accessible locations (high distance to
nearest major road) are more likely to be forested.
We ﬁt the model using the non-collapsed Marginal-Scheme 1 and Conditional algo-
rithms deﬁned in Table 3.1 and compare the two algorithms in terms of mixing based on
the autocorrelation in the sample paths of the model parameters. Each algorithm was run
for 80,000 iterations, and after examining trace plots we decided to discard the ﬁrst 10,000
samples as burn-in.
Inferences on the model parameters are nearly identical for both model-ﬁtting algo-
rithms. As expected, the estimate for the regression coefﬁcient associated with the dis-
tance to nearest major road covariate is positive (95 percent credible interval on β1 is
(1.687, 3.428)) so that the farther a location is from a major road (i.e., less accessible),
the more likely that location is to be forested. The intercept is not signiﬁcantly different
from 0 (95 percent credible interval on β0 is (−1.094, 0.165)), and the 95 percent credible
interval for ρ is (0.968, 0.999) indicating strong residual spatial dependence.
Rather than showing sample autocorrelation plots as in Section 3.2, to highlight the
differences between the sample autocorrelation summaries, Table 3.3 provides the autocor-
relations for the sample paths of β1 and ρ at selected lags. We do not show the autocorrela-
tion values for the sample path of β0 because they were approximately zero for all lags and
55

both algorithms. Table 3.3 shows that the Marginal-Scheme 1 algorithm outperforms the
Conditional algorithm, exhibiting smaller autocorrelation in the sampled paths of β1 and ρ.
Sample Autocorrelations for β1
Lag
Marginal-Scheme 1
Conditional
1
0.3194
0.3576
2
0.1791
0.2133
3
0.1115
0.1387
4
0.0786
0.1004
5
0.0617
0.0741
10
0.0256
0.0284
Sample Autocorrelations for ρ
Lag
Marginal-Scheme 1
Conditional
1
0.8514
0.8621
2
0.7303
0.7496
3
0.6319
0.6576
4
0.5515
0.5802
5
0.4849
0.5134
10
0.2589
0.2898
15
0.1447
0.1752
20
0.0830
0.1081
Table 3.3: Autocorrelations of the sample paths of β1 and ρ for the land cover data analysis.
3.4
Summary
In this chapter, we extended the algorithms of Imai and van Dyk (2005) to the spatially-
dependent setting and compared the efﬁciency of three MCMC algorithms via a simulation
study and data analysis. Furthermore, we proposed and compared three additional MCMC
algorithms, called partially-collapsed algorithms, which marginalize over the latent vari-
able when sampling the spatial dependence parameter. In both the simulation study and
56

data analysis, we found that the non-collapsed Marginal-Scheme 1 algorithm was the most
efﬁcient in terms of autocorrelation, sensitivity to starting values, and computational time.
57

Figure 3.1: Histograms and trace plots for β and ρ under Scenario 1 for each of the three
corresponding algorithms. The black portion represents the burn-in and the colored portion
represents draws from the posterior distribution.
58

Figure 3.2: Histograms and trace plots for β and ρ under Scenario 2 for each of the three
corresponding algorithms. The black portion represents the burn-in and the colored portion
represents draws from the posterior distribution.
59

Figure 3.3: Histograms and trace plots for β and λ under Scenario 3 for each of the three
corresponding algorithms. The black portion represents the burn-in and the colored portion
represents draws from the posterior distribution.
60

Figure 3.4: Histograms and trace plots for β and λ under Scenario 4 for each of the three
corresponding algorithms. The black portion represents the burn-in and the colored portion
represents draws from the posterior distribution.
61

Figure 3.5: Histograms and trace plots for β under Scenario 5 for each of the three cor-
responding algorithms. The black portion represents the burn-in and the colored portion
represents draws from the posterior distribution.
62

Figure 3.6: Autocorrelation and partial autocorrelation in the sample paths of β and ρ under
Scenario 1 for each of the three corresponding algorithms.
63

Figure 3.7: Autocorrelation and partial autocorrelation in the sample paths of β and ρ under
Scenario 2 for each of the three corresponding algorithms.
64

Figure 3.8: Autocorrelation and partial autocorrelation in the sample paths of β and λ under
Scenario 3 for each of the three corresponding algorithms.
65

Figure 3.9: Autocorrelation and partial autocorrelation in the sample paths of β and λ under
Scenario 4 for each of the three corresponding algorithms.
66

Figure 3.10: Autocorrelation and partial autocorrelation in the sample path of β under
Scenario 5 for each of the three corresponding algorithms.
67

Figure 3.11: σ vs. ˜β under Scenario 1 for each of the three corresponding algorithms. The
black dots represent the burn in and the colored dots represent the draws from the posterior.
Top row: First starting values. Bottom row: Second starting values.
68

Figure 3.12: σ vs. ˜β under Scenario 2 for each of the three corresponding algorithms. The
black dots represent the burn in and the colored dots represent the draws from the posterior.
Top row: First starting values. Bottom row: Second starting values.
69

Figure 3.13: σ vs. ˜β under Scenario 3 for each of the three corresponding algorithms. The
black dots represent the burn in and the colored dots represent the draws from the posterior.
Top row: First starting values. Bottom row: Second starting values.
70

Figure 3.14: σ vs. ˜β under Scenario 4 for each of the three corresponding algorithms. The
black dots represent the burn in and the colored dots represent the draws from the posterior.
Top row: First starting values. Bottom row: Second starting values.
71

Figure 3.15: σ vs. ˜β under Scenario 5 for each of the three corresponding algorithms. The
black dots represent the burn in and the colored dots represent the draws from the posterior.
Top row: First starting values. Bottom row: Second starting values.
72

CHAPTER 4
THE BAYESIAN SPATIAL PROBIT REGRESSION MODEL AS A
TOOL FOR CLASSIFICATION
There are often two primary goals in regression analyses. The ﬁrst is to model the
relationship between the response variable and the covariates. The second is to accurately
predict missing or unobserved responses. Up to this point, we have focused on the ﬁrst goal,
emphasizing the need to allow for residual spatial dependence when analyzing relationships
between spatially-referenced binary response variables and associated covariates. In this
chapter, we focus on the second goal, prediction.
In some sense, prediction of binary or categorical outcomes can be thought of as a
classiﬁcation problem, where we determine into which class, or category, a collection of
predictors/inputs, or covariates, is likely to fall. For example, in predicting the land cover
category at an unobserved location, we observe a collection of predictors/inputs, such as
elevation or distance to the nearest major road. Using these predictors/inputs, we can de-
termine some decision function which deﬁnes a classiﬁcation rule for classifying each lo-
cation. Within the classiﬁcation literature, the decision function is determined using one of
a number of available classiﬁcation methods, each method using statistical decision theory
to determine optimal classiﬁcation rules. Popular classiﬁcation techniques, however, do
73

not typically take into account the categories of observations nearby in geographical space.
Instead, the decision function relies only on a function of the predictors/inputs.
In classiﬁcation problems involving spatially-referenced observations, we argue that
neighboring observations along with predictors/inputs should be considered. As we will
illustrate, classiﬁcation rules that rely on neighboring observations can be derived from the
Bayesian spatial probit regression model.
The goal of this chapter is to compare, in terms of classiﬁcation error, the Bayesian
spatial probit regression model-based classiﬁer to various other classiﬁers. We ﬁrst deﬁne
notation for the classiﬁcation problem in Section 4.1. In Section 4.2 we discuss classi-
ﬁcation in the regression setting and, using the Bayesian spatial probit regression model,
deﬁne a classiﬁcation rule for spatially-referenced observations. In Section 4.3 we give
an overview of popular classiﬁcation methods, which in Section 4.4, we then compare to
the Bayesian spatial probit regression model-based classiﬁer in terms of both the training
(in-sample prediction) and test (out-of-sample prediction) error rates for the Southeast Asia
land cover data set used in Section 3.3.
4.1
The Classiﬁcation Problem
In the classiﬁcation problem, we again have the set of paired observations {(Yi, xi); i =
1, . . . , n}, where Yi is a binary response variable and xi is a k ×1 vector of covariates used
as predictors/inputs in determining decision boundaries for the classes. In the classiﬁcation
setting, the response variable is an indicator identifying the class to which the set of ob-
served predictors/inputs belong. Although many of the classiﬁcation methods listed below
can be generalized to the multi-category/class setting, we restrict our description of these
74

methods to the binary setting. In this setting there are two classes, C0 and C1, where Cj rep-
resents the class of observations where Y = j. Of the observations {(Yi, xi); i = 1, . . . , n},
n0 fall into class C0 and n1 fall into class C1, and n0 + n1 = n. Each classiﬁcation method
deﬁnes a decision function δ(ω) where ω is the set of applicable predictors/inputs, model
parameters, and in the spatial setting, surrounding observations. Based on this decision
function, we can deﬁne a classiﬁcation rule.
Before discussing each of the classiﬁcation methods, we ﬁrst give a brief overview of
the classiﬁcation process. Typically, when using classiﬁcation methods, the sample data
is randomly divided into two subsets, the training and test data sets, of sizes ntrain and
ntest, respectively. This allows us to ﬁt the model on which the classiﬁer is based using the
training data and evaluate the method’s ability to predict based on the test data set. When
we compare classiﬁcation methods, we consider the training and test error rates, or the
number of incorrect classiﬁcations among training and test data sets, divided by the sample
size of each data set, respectively.
Furthermore, some of the classiﬁers rely on ﬁxed parameters that are not estimated,
and thus, require tuning to minimize the prediction error. In this case, it is common to use
ﬁve-fold cross-validation to obtain an optimal value for the ﬁxed parameters. We do this
by dividing the training data set into ﬁve equal subsets. We then repeatedly ﬁt the model
iteratively leaving the mth subset out and compute the error rate for the mth subset for
each m = 1, . . . , 5. Averaging across the ﬁve error rates determines the cross-validation
error (CVE). This procedure is repeated for various ﬁxed values of the tuning parameter to
determine a satisfactory value for the analysis.
In each of the following sections, we generically use {(Yi, xi); i = 1, . . . n} as the data
used for ﬁtting the method, and (Y 0, x0) as the point of prediction or classiﬁcation.
75

4.2
GLM-Based Classiﬁcation
In this section, we ﬁrst discuss the standard approach for classiﬁcation using a GLM
and the decision boundaries and classiﬁcation rules for two commonly used GLMs. We
then introduce a spatial classiﬁcation technique using the Bayesian spatial probit regres-
sion model. In specifying our underlying probabilty models, although a slight misuse of
notation, we explicitly condition on x to make it clear which predictors/inputs we rely on
for classiﬁcation. Furthermore, this also makes the classiﬁcation rule comparable to that
of discriminant analysis as discussed in Section 4.3.1. Finally, we note that in GLM-based
classiﬁcation, the xi include a term allowing for an intercept along with the other predic-
tors/inputs, as is usually done in regression analyses.
4.2.1
Non-Spatial GLM-Based Classiﬁcation
Consider the GLM for independent binary response variables
Yi|xi, β ∼Bernoulli(pi)
g(pi) = x′
iβ,
(4.1)
where pi = P(Yi = 1|xi, β), xi is a ﬁxed k × 1 vector of covariates, β is a k × 1 vector
of coefﬁcients, and g(·) is a link function. To classify x0, in regression analyses we predict
Y 0 = 1 when P(Y 0 = 1|x0, β) > P(Y 0 = 0|x0, β) and Y 0 = 0 otherwise. This
prediction method, can be turned into a classiﬁcation rule as follows.
Analogous to prediction in the regression setting, we deﬁne the following decision func-
tion,
δ(ω) ≡δ(x0, β) = P(Y 0 = 1|x0, β)
P(Y 0 = 0|x0, β) =
g−1(x0′β)
1 −g−1(x0′β),
(4.2)
76

and set it equal to one. Equivalently, the decision function,
δ(ω) ≡δ∗(x0, β) ≡log

g−1(x0′β)
1 −g−1(x0′β)

,
(4.3)
can be set equal to zero.
Based on (4.2), a classiﬁcation rule is then
Y 0 =
(
1, if δ(x0, β) > 1
0, otherwise
.
(4.4)
Similarly, based on (4.3), a classiﬁcation rule is
Y 0 =
(
1, if δ∗(x0, β) > 0
0, otherwise
.
(4.5)
A popular form for the link function, g(·), is the logit link, where
g(pi) = log

pi
1 −pi

.
Using this link function, the decision function in (4.3) becomes
δ∗(x0, β) = log

g−1(x0′β)
1 −g−1(x0′β)

= x0′β
(4.6)
and the classiﬁcation rule in (4.5) is
Y 0 =
(
1, if x0′β > 0
0, otherwise
.
(4.7)
Although there are many approaches for estimating β in a frequentist setting, we estimate
β numerically by maximizing the likelihood function,
L(β, Y ) =
n
Y
i=1
pYi
i (1 −pi)1−Yi =
n
Y
i=1
exp{Yix′
iβ}
1 + exp{x′
iβ}
with respect to β.
77

Another common link function is the probit link, where
g(pi) = Φ−1(pi)
and Φ is the cumulative standard normal distribution function. Using this link function, the
decision function in (4.2) is then
δ(x0, β) =
g−1(x0′β)
1 −g−1(x0′β) =
Φ(x0′β)
1 −Φ(x0′β)
(4.8)
and the classiﬁcation rule is
Y 0 =
(
1, if Φ(x0′β)/(1 −Φ(x0′β)) > 1
0, otherwise
.
(4.9)
Again, β can be estimated a number of ways, one of which is to estimate β numerically by
maximizing the likelihood function,
L(β, Y ) =
n
Y
i=1
pYi
i (1 −pi)1−Yi =
n
X
i=1
Φ(x′
iβ)Yi(1 −Φ(x′
iβ))(1−Yi)
with respect to β. We also consider a Bayesian approach.
Consider the latent variable representation of the Bayesian probit regression model as
deﬁned in Section 2.1.1. Without loss of generality, assume σ2 = 1, ˜β = β, and ˜Z = Z.
Using this representation, P(Y = 1|x, β) = P(Z > 0|x, β) and the decision function is:
δ(x0, β) = P(Y 0 = 1|x0′, β)
P(Y 0 = 0|x0′, β)
= P(Z0 > 0|x0′, β)
P(Z0 ≤0|x0′, β)
=
Φ(x0′β)
1 −Φ(x0′β),
which corresponds to the decision function given by (4.8). Note that in this case, the
decision function is the posterior odds. We can use the classiﬁcation rule given in (4.9)
78

which, in this setting, corresponds to a 0-1 loss function (i.e., the loss for predicting Y 0
correctly is 0 and the loss for predicting Y 0 incorrectly is 1). For consistency, we continue
to use the frequentist notation for the decision function and classiﬁcation rule as initially
deﬁned earlier in this chapter.
We can estimate P(Y 0 = 1|x0, β) using one of the following two approaches. The ﬁrst
is to use a posterior mean classiﬁer where we estimate E[β|Y ], or the posterior mean of β,
and use this estimate to obtain P(Y 0 = 1|x0, ˆβ) = P(Z0 > 0|x0, ˆβ) = Φ(x0′ ˆβ). In prac-
tice, we approximate E[β|Y ] by using ˆβ = PT
t=1 β[t]/T, where β[t] are draws from the
posterior distribution of β, and T is the number of draws from the distribution. This ﬁrst
representation is analogous to the estimation method of the previous likelihood approach.
The second is to use a posterior predictive classiﬁer p0 = E[I(Z0 > 0)|x0, Y ], or the pos-
terior probability that Z0 > 0. In doing this, we marginalize over the posterior distribution
of β to obtain an estimate of P(Y 0 = 1|x0, Y ) =
R
P(Z0 > 0|x0, β)π(β|Y )dβ. In prac-
tice, we estimate p0 by using ˆp0 = PT
t=1 I(Z0[t] > 0)/T, where Z0[t] are draws from the
posterior distribution of Z0, and T is the number of draws from the distribution. Thus, for
the independent Bayesian probit regression model, the two estimated classiﬁcation rules
are:
1. Posterior Mean Classiﬁer:
Y 0 =
(
1, if Φ(x0′ ˆβ)/(1 −Φ(x0′ ˆβ)) > 1
0, otherwise
(4.10)
2. Posterior Predictive Classiﬁer:
Y 0 =
(
1, if ˆp0/(1 −ˆp0) > 1
0, otherwise
(4.11)
79

As discussed in Section 4.1, when using classiﬁcation techniques, we compute error
rates for both the training data (or the observed data used to ﬁt the model) and the test
data (or the observed data not used to ﬁt the model). For the test data, we can simply use
the latent Zj, for j = 1, . . . , ntest, as sampled within the Gibbs sampler. However, for the
training data, the latent Zi, for i = 1, . . . , ntrain, are sampled within the Gibbs sampler
given the observed Yi. In this case, to use the latent Zi as predictors, we must sample these
values as if the Yi are unknown. Using the posterior draws of β, we can resample the latent
Zi as follows:
(i) Take samples β[t] for i = 1, . . . , T from π(β|Y ) and sample a corresponding
Z[t]
i
∼
N(xiβ[t], 1). (Note that these Z[t]
i
are not those sampled in the MCMC
algorithm.)
(ii) Determine ˆp0
i = PT
t=1 I(Z[t]
i
> 0)/T and let Y 0
i be the predicted value of Yi using
the posterior predictive classiﬁer.
(iii) Repeat (i) and (ii) for all i = 1, . . . , ntrain.
(iv) Compute
the
training
error
rate
for
the
posterior
predictive
classiﬁer:
Pntrain
i=1
I(Y 0
i
̸= Yi)/ntrain.
4.2.2
Spatial GLM-Based Classiﬁcation
As discussed in Section 2.2, the latent variable representation of the probit regression
model allows us to include spatial dependence among the response variables. Here we
propose extending the use of this model to the classiﬁcation setting. In this case, the deci-
sion function δ(ω) depends on the covariates and regression coefﬁcients, as well as on the
categories of the surrounding observations.
80

Using equations (2.5) and (2.6), it follows that the distribution for the latent variable at
an unobserved location is
Z0|Y , X, β, θ, Z ∼N(µZ0, σZ0)
where
µZ0 = x0′β + σ(θ)′-1 (Σ(θ))−1 (Z −X′β)
σ2
Z0 = σ(θ)1 −σ(θ)′-1 (Σ(θ))−1 σ(θ)-1
X = (x1, . . . , xn)′, σ(θ) is an (n + 1) × 1 vector representing the variance of Z0 and
(Z0, Z′)′, σ(θ)-1 is σ(θ) with the ﬁrst element removed, and Σ(θ) is the spatial correlation
matrix among Z = (Z1, . . . , Zn)′. We can easily include sampling from this distribution
in the ﬁrst step of the MCMC algorithm (see Section 3.1.1 and Table 3.1), so that we can
obtain draws from the posterior distribution of Z0. In this spatially-dependent case,
P(Y 0 = 1|x0, β, θ, Y , Z) = P(Z0 > 0|x0, β, θ, Y , Z)
= Φ
 
µZ0
p
σ2
Z0
!
.
Thus, the decision function at x0 is
δ(ω) ≡δ(x0, Y , Z, β, θ) =
P(Z0 > 0|x0, Y , Z, β, θ)
1 −P(Z0 > 0|x0, Y , Z, β, θ)
Just as for the independent Bayesian probit regression model, we can consider two ways
to obtain an estimated classiﬁcation rule:
1. Posterior Mean Classiﬁer:
Estimate E[β|Y ], E[θ|Y ], and E[Z|Y ] by computing ˆβ = PT
t=1 β[t]/T, ˆθ =
PT
t=1 θ[t]/T, and ˆZ = PT
t=1 Z[t]/T, respectively. The estimated classiﬁcation rule
is
81

Y 0 =



1, if
Φ(ˆµZ0/√
ˆσ2
Z0)
1−Φ(ˆµZ0/√
ˆσ2
Z0) > 1
0, otherwise
(4.12)
where
ˆµZ0 = x0′ ˆβ + σ(ˆθ)′-1

Σ(ˆθ)
−1
( ˆZ −X′ ˆβ)
ˆσ2
Z0 = σ(ˆθ)1 −σ(ˆθ)′-1

Σ(ˆθ)
−1
σ(ˆθ)-1
2. Posterior Predictive Classiﬁer:
Estimate p0 = E[I(Z0 > 0)|x0, Y ] by computing ˆp0 = PT
t=1 I(Z0[t] > 0)/T. This
gives an estimate of
P(Y 0 = 1|x0, Y ) = P(Z0 > 0|x0, Y )
=
Z
P(Z0 > 0|Z, β, θ) π(Z, β, θ) dZ dβ dθ.
The estimated classiﬁcation rule is then
Y 0 =
(
1, if
ˆp0
1−ˆp0 > 1
0, otherwise
.
(4.13)
Again, each of these classiﬁcation rules correspond to a 0-1 loss function on the predicted
Y 0.
As with the independent Bayesian probit regression model, for the training data, we
must resample the Zis to determine prediction error rates. For the Bayesian spatial probit
regression model, however, we rely on the observed surrounding observations to provide
information about the category of the unobserved locations. In this case, evaluating the
training error is not straightforward. We propose the following two approaches for the
spatial probit posterior predictive classiﬁer:
82

A. One-at-a-Time Training Error:
(i) Take samples (β[t], θ[t], Z[t]-i ), for t = 1, . . . , T, where Z[t]-i is an (n−1)×1 vector
of sampled Zj for j = 1, . . . , i−1, i+1, . . . , ntrain and sample a corresponding
Z[t]
i ∼N(µZi, σ2
Zi) where
ˆµZi = x′
iβ[t] + Σ(θ[t])i,-i
 Σ(θ[t])-i,-i
−1 (Z[t]-i −X-iβ[t])
ˆσ2
Zi = Σ(θ[t])i,i −Σ(θ[t])i,-i
 Σ(θ[t])-i,-i
−1 Σ(θ[t])-i,i
and X-i is X with the ith row removed, and Σ(θ[t])j,-k is the jth row of the
estimated spatial correlation of Z with the kth column removed. (Note that Z[t]-i
are the posterior samples obtained from the MCMC algorithm, however, the
Z[t]
i s are not the same as those sampled in the MCMC algorithm.)
(ii) Determine ˆp0
i = PT
t=1 I(Z[t]
i
> 0)/T and let Y 0
i be the predicted value of Yi
using the posterior predictive classiﬁer.
(iii) Repeat (i) and (ii) for all i = 1, . . . , ntrain.
(iv) Compute the one-at-a-time training error: Pntrain
i=1
I(Y 0
i ̸= Yi)/ntrain
B. Joint Training Error:
(i) Take samples (β[t], θ[t]) for t = 1, . . . , T and sample a corresponding Z[t] ∼
N(Xβ[t], Σ(θ[t])). (Note that the Z[t] are not the same as those sampled in the
MCMC algorithm.)
(ii) For i = 1, . . . , ntrain, compute ˆp0
i = PV
v=1 I(Z[v]
i
> 0)/V and let Y 0
i be the
predicted value of Yi using the posterior predictive classiﬁer.
(iii) Compute the joint training error: Pntrain
i=1
I(Y 0
i ̸= Yi)/ntrain
83

Both the one-at-a-time and joint training errors allow for spatial dependence among the bi-
nary predictions/classiﬁcations through the latent random variable. The joint training error
allows for spatial dependence only through the spatial dependence structure of the latent
variables, Σ(θ). In contrast, the one-at-a-time training error allows for spatial dependence
through Σ(θ), but also allows for spatial dependence by conditioning on the current values
of the latent random variables at nearby locations, Z[t]-i . We note that for the independent
model, both the one-at-a-time and joint training errors will be the same since Zi is inde-
pendent of all other Zj for j = 1, . . . , i −1, i + 1, . . . , n.
4.3
Alternative Classiﬁcation Methods
In this section, we give an overview of alternatives to GLM-based classiﬁcation. In
these alternative classiﬁcation methods, we assume that the xis only include the predic-
tors/inputs, and thus do not include a term to allow for an intercept as in the GLM-based
classiﬁcation methods. Unless otherwise noted, the classiﬁcation methods described here
are taken from Hastie et al. (2001).
4.3.1
Discriminant Analysis
In discriminant analysis, rather than considering the explanatory variables x as ﬁxed as
they are in regression analyses, the x are viewed as random variables, with class-speciﬁc
density functions fj(x) corresponding to each class Cj. The classes also have prior proba-
bilities πj, such that π0 + π1 = 1. To determine the probability that a set of predictors will
fall into class j, we employ Bayes’ theorem which implies that
P(Y = j|x) =
fj(x)πj
f1(x)π1 + f0(x)π0
.
(4.14)
84

The Bayes’ classiﬁcation rule is to classify an observation to class C1 when P(Y = 1|x) >
P(Y = 0|x) and to C0 otherwise.
We ﬁrst describe discriminant analysis in its general form, allowing a general form for
the fj(x)s and the decision function, and then discuss four special cases that we use in our
analysis.
Let
Xj =


x1
...
xnj


where {x1, . . . , xnj} = {xi : Yi = j} and Xj is an njk × 1 vector of predictors/inputs
corresponding to observations in class Cj. To classify Y 0, for each class we deﬁne
X0
j =
x0
Xj

,
where X0
j is a (nj + 1)k × 1 vector and x0 is a k × 1 vector of predictors/inputs associated
with Y 0. Our goal is to determine a decision boundary for classifying Y 0.
In discriminant analysis, fj(·) is typically the multivariate normal density function,
fj(X0
j ) =
1
(2π)(nj+1)k/2|ΣX
j |1/2 exp

−1
2(X0
j −µX
j )′  ΣX
j
−1 (X0
j −µX
j )

where µX
j is the (nj+1)k×1 class-speciﬁc mean vector and ΣX
j is the (nj+1)k×(nj+1)k
class-speciﬁc covariance matrix. It follows that
fj(x0|x1, . . . , xnj) =
1
(2π)k/2|Σx0
j |1/2 exp

−1
2(x0 −µx0
j )′ 
Σx0
j
−1
(x0 −µx0
j )

(4.15)
where
µx0
j = µj({1:k}) + ΣX
j({1:k},-{1:k})
 ΣX
j(-{1:k},-{1:k})
−1 (Xj −µj(-{1:k}))
Σx0
j = ΣX
j({1:k},{1:k}) −ΣX
j({1:k},-{1:k})
 ΣX
j(-{1:k},-{1:k})
−1 ΣX
j(-{1:k},{1:k}).
85

We use the subscript notation ({1 : k}) to indicate the ﬁrst k elements of the corresponding
matrix or vector (i.e., those indices corresponding to x0) and (-{1 : k}) indicates the matrix
or vector without the ﬁrst k elements (i.e., the remaining indices corresponding to Xj).
Using the log-odds as the decision function to determine a classiﬁcation rule for classi-
fying Y 0, it follows from (4.14) and (4.15) that
δ(ω) ≡δ(x0, µx0
0 , µx0
1 , Σx0
0 , Σx0
1 )
= log
P(Y 0 = 1|x0)
P(Y 0 = 0|x0)

= log π1
π0
+ 1
2 log |Σx0
0 |
|Σx0
1 | −1
2µx0′
1

Σx0
1
−1
µx0
1 + 1
2µx0′
0

Σx0
0
−1
µx0
0
|
{z
}
≡α0
+ x0′ 
(Σx0
1 )−1µ1 −(Σx0
0 )−1µx0
0

|
{z
}
≡α1
−x0′ 1
2

(Σx0
1 )−1 −(Σx0
0 )−1
|
{z
}
≡α2
x0
.
(4.16)
Here, α0 is a scalar, α1 is a k × 1 vector, and α2 is a k × k matrix, which we deﬁne
for notational convenience. Setting the decision function equal to zero, the discriminant
analysis-based classiﬁcation rule is
Y 0 =
(
1, if (x0′α1 −x0′α2x0) > −α0
0, if (x0′α1 −x0′α2x0) ≤−α0
.
(4.17)
We now consider special cases to (4.17). Each of these special cases assumes that the
mean of xi is equal across all observations, so that µX
j = [1nj+1 ⊗µj] where 1nj+1 is an
(nj + 1) × 1 vector of ones and µj is a k × 1 class speciﬁc mean vector. The difference
between each of these special cases is in the speciﬁcation of the covariance matrix ΣX
j . We
ﬁrst describe three popular discriminant analysis methods (linear discriminant analysis,
diagonal linear discriminant analysis, and quadratic discriminant analysis) all of which
assume that the xi are independent. Then we describe an approach for spatial discriminant
analysis (spatial linear discriminant analysis) due to ˇSaltyt˙e Benth and Duˇcinskas (2005).
86

Assuming the xi are independent results in the following form for the covariance of
X0
j :
var(X0
j ) = ΣX
j = (Inj+1 ⊗Λj),
(4.18)
where Inj+1 is an (nj + 1) × (nj + 1) identity matrix and Λj is a class-speciﬁc covari-
ance matrix for the k components of xi. Under this assumption, x0 is independent of
x1, . . . , xnj, so
fj(x0|x1, . . . , xnj) = fj(x0) =
1
(2π)k/2|Λj|1/2 exp{−1
2(x0 −µj)′Λ−1
j (x0 −µj)}.
Assuming a constant variance across classes (i.e., Λj = Λ for j = 0, 1) results in
linear discriminant analysis (LDA) because the decision boundary is linear in the xs. The
decision function given in (4.16) can be written in this case as
δ(ω) ≡δ(x0, µ0, µ1, Λ) = log
P(Y 0 = 1|x0)
P(Y 0 = 0|x0)

= log π1
π0
−1
2(µ1 + µ0)′Λ−1(µ1 −µ0)
|
{z
}
αLDA
0
+x0′ Λ−1(µ1 −µ0)
|
{z
}
αLDA
1
,
where αLDA
0
is a scalar and αLDA
1
is a k × 1 vector, deﬁned for notational convenience.
Note that this decision function is effectively equivalent to the one based on the logistic
regression model in (4.6), however, in logistic regression, we assume the x’s are ﬁxed and
thus make no distributional assumptions on x as in discriminant analysis. This results in
the following LDA-based classiﬁcation rule for Y 0 given x0:
Y 0 =
(
1, if x0′αLDA
1
> −αLDA
0
0, if x0′αLDA
1
≤−αLDA
0
(4.19)
In practice, the parameters πj, µj, Λ (and thus αLDA
0
and αLDA
1
) are unknown but can
be estimated using maximum likelihood:
87

• ˆπj = nj/n
• ˆµj = P
i:Yi=j xi/nj
• ˆΛ = P
j∈{0,1}
P
i:Yi=j(xi −ˆµj)(xi −ˆµj)′/(n −2)
Diagonal linear discriminant analysis (DLDA) additionally assumes independence be-
tween the k predictors/inputs so that var(xi) = Λ is a diagonal matrix. The DLDA-based
classiﬁcation rule is the same as (4.19), but using a diagonal matrix Λ. The mth diagonal
element of Λ is estimated by ˆΛ(m,m) = P
j∈{0,1}
P
i:Yi=j(xim −ˆµjm)2/(n −2) where xim
and ˆµjm are the mth elements of xi and µj, respectively.
In LDA, we assume a constant covariance for xi among the classes (i.e., Λj = Λ for
j = 0, 1). On the other hand, quadratic discriminant analysis (QDA) allows for each class
to have its own covariance. The decision function now contains a quadratic term in the xs:
δ(ω) ≡δ(x0, µ0, µ1, Λ0, Λ1) = log
P(Y 0 = 1|x0)
P(Y 0 = 0|x0)

= log π1
π0
+ 1
2 log |Λ0|
|Λ1| −1
2µ′
1Λ−1
1 µ1 + 1
2µ′
0Λ−1
0 µ0
|
{z
}
αQDA
0
+ x0′ (Λ−1
1 µ1 −Λ−1
0 µ0)
|
{z
}
αQDA
1
−x0′ 1
2(Λ−1
1
−Λ−1
0 )
|
{z
}
αQDA
2
x0.
Here, αQDA
0
is a scalar, αQDA
1
is a k × 1 vector, and αQDA
2
is a k × k matrix, which are
deﬁned for notational convenience. This results in the QDA-based classiﬁcation rule for
Y 0 given x0:
Y 0 =
(
1, if (x0′αQDA
1
−x0′αQDA
2
x0) > −αQDA
0
0, if (x0′αQDA
1
−x0′αQDA
2
x0) ≤−αQDA
0
(4.20)
where we estimate the parameters by taking
• ˆπj = nj/n
88

• ˆµj = P
i:Yi=j xi/nj
• ˆΣj = P
i:Yi=j(xi −ˆµj)(xi −ˆµj)′/(nj −1).
The ﬁnal discriminant analysis method we discuss is a special case of the spatial-
temporal extension of LDA recently proposed by ˇSaltyt˙e Benth and Duˇcinskas (2005).
We restrict our discussion to the spatial case, but refer the reader to ˇSaltyt˙e Benth and
Duˇcinskas (2005) for more details on the spatial-temporal approach. In spatial LDA, we do
not assume independence among the xi as in the previous three methods; instead, we as-
sume the predictors/inputs are spatially dependent. Following ˇSaltyt˙e Benth and Duˇcinskas
(2005), the speciﬁc form for the covariance is
var
 X0
j

= ΣX
j = (Rj(θ) ⊗Λj)
where Rj(θ) is an (nj + 1) × (nj + 1) class-speciﬁc spatial covariance matrix and Λj is
again the var(xi). (Contrast this speciﬁcation with that given by (4.18) where we assumed
the xi were independent.) The separable structure for ΣX
j implies that each of the predic-
tors/inputs have the same spatial dependence. Just as with LDA, spatial LDA assumes the
covariance for each xi is the same for both classes, i.e., Λj = Λ for j = 0, 1.
The spatial LDA-based classiﬁcation rule is the same as the general discriminant analy-
sis classiﬁcation rule in (4.17), but we can simplify the parameters µx0
j and Σx0
j as follows:
µx0
j = µj +
 Rj(θ)(1,-1)(Rj(θ)(-1,-1))−1 ⊗Ik
  Xj −(1nj ⊗µj)

Σx0
j =
 Rj(θ)(1,1) −Rj(θ)(1,-1)(Rj(θ)(-1,-1))−1Rj(θ)(-1,1)

⊗Λ
where Rj(θ)1,1 indicates the ﬁrst element of Rj(θ), and Rj(θ)(-1,-1) indicates Rj(θ) with
the ﬁrst row and column removed, and likewise for Rj(θ)(1,-1) and Rj(θ)(-1,1).
89

To make use of the spatial-temporal classiﬁer, ˇSaltyt˙e Benth and Duˇcinskas use a ﬁxed
and known spatial-temporal correlation function. They then calculate unbiased forms of
maximum likelihood estimators of the unknown parameters. Therefore, to use their ap-
proach in our analysis which only includes spatial dependence, the estimators for the un-
known parameters are:
• ˆµj =

1nj(Rj(ˆθ)(-1,-1))−11′
nj
−1
1nj(Rj(ˆθ)(-1,-1))−1[x1, . . . , xnj]′
• ˆΛ = (1/(n −2)) P
j∈0,1
 [x1, . . . , xnj]′ −(1nj ⊗ˆµ′
j)
′
×(Rj(ˆθ)(-1,-1))−1  [x1, . . . , xnj]′ −(1nj ⊗ˆµ′
j)

.
4.3.2
Support Vector Machines
First introduced by Cortes and Vapnik (1995), the goal of support vector machines
(SVM) is to determine a hyperplane in covariate space separating the classes in such a way
that the margin between the two classes is maximized. The margin is the minimum distance
between the the predictors/inputs xi of the two classes in the direction perpendicular to the
hyperplane. The resulting function determining this hyperplane is the decision function.
An illustration of this approach is provided in Figure 4.1 (a). In this illustration, for each
observations there are two inputs, xi1 and xi2 which determine the classes (denoted by the
two plotting symbols). The margin is the gap between the two classes, or the space between
the two dotted lines, and the maximum-margin hyperplane is the solid line between the two
classes. The dashed lines from the two points on the edges of the margin (the support
vectors) to the hyperplane show the distance to be maximized.
Figure 4.1 (b) shows
another hyperplane separating the two classes, but one that does not maximize the margin.
90

(a)
(b)
Figure 4.1: Illustration of the hyperplane (solid line) and margins (dotted lines) determined
by support vector machines separating the two classes (denoted by the two plotting sym-
bols). The hyperplane in (a) is the maximum margin hyperplane and the hyperplane in (b)
does not maximize the margin.
In SVM, the classes are labeled as either 1 or −1 (instead of 1 or 0, as before). To
accommodate this convention, we deﬁne Y ∗
i = 2Yi −1, for i = 1, . . . , n, so that Y ∗
i ∈
{−1, 1}.
Consider the decision function determined by the linear hyperplane {x : δ(x) = 0}
where
δ(ω) ≡δ(x, β, β0) = x′β + β0.
(4.21)
Given observations {Y ∗
i , xi} for i = 1, . . . , n, maximizing the margin between the two
classes and the hyperplane is equivalent to minimizing ||β|| subject to Y ∗
i (x′
iβ+β0) ≥1 for
all i = 1, . . . , n. This problem can be represented as the following Lagrange optimization
91

function
max
η
L = max
η
 n
X
i=1
ηi −1
2
n
X
i=1
n
X
i∗=1
ηiηi∗Y ∗
i Y ∗
i∗x′
ixi∗
!
(4.22)
where η = (η1, . . . , ηn)′ are the Lagrangian multipliers, which are subject to the constraints
that Pn
i=1 ηiY ∗
i = 0 and ηi ≥0 for all i. The xi where ηi > 0 are the support vectors and
are the only vectors which inﬂuence the position of the hyperplane. In Figure 4.1, the
support vectors are the points on the outer boundary of the margins. We can write the
relationship between η and β as
β =
n
X
i=1
ηiY ∗
i xi
and the relationship between η and β0 as
β0 = 1
nsv
X
i:ηi>0
(βxi −Y ∗
i )
where nsv is the number of support vectors.
While the above hyperplane is linear, SVM can be extended to create nonlinear bound-
aries between the classes. This extension can be acheived by transforming the predic-
tors/inputs into a space where they can be separated linearly, and again ﬁnd the separating
hyperplane in this transformed covariate space. We can use the Lagrange optimization
function in (4.22) with the transformed predictors/inputs K(xi, xj):
max L = max
 n
X
i=1
ηi −1
2
n
X
i=1
n
X
j=1
ηiηjY ∗
i Y ∗
j K(xi, xj)
!
(4.23)
subject to Pn
i=1 ηiY ∗
i = 0 and 0 ≤ηi ≤γ for all i, where γ is a tuning parameter allowing
for crossover among the two classes and K(·, ·) is a symmetric positive (semi-) deﬁnite
function. In our data analysis, we consider the following three popular kernels:
• Linear: K(xi, xj) = x′
ixj
92

• dth Degree Polynomial: K(xi, xj) = (1 + x′
ixj)d
• Radial: K(xi, xj) = exp{−||xi −xj||2/c}
Now, (4.21) can be written as
δ(ω) ≡δ(x, η, β0, γ) =
n
X
i=1
ηiY ∗
i K(x, xi) + β0.
(4.24)
Thus,
ˆδ(x0, η, β0, γ) =
n
X
i=1
ˆηiYiK(x0, xi) + ˆβ0
and
Y 0∗= sign

ˆδ(x0, η, β0, γ)

so that the SVM-based classiﬁcation rule of Y 0 is
Y 0 = 1
2(Y 0∗+ 1).
When implementing this classiﬁcation method, we use the R package e1071 (Dimitriadou
et al., 2010), to compute ˆηi and ˆβ0 via a quadratic optimization function for a ﬁxed value
of γ.
4.3.3
k-Nearest Neighbors
The k-Nearest Neighbors classiﬁcation method makes no assumptions about an un-
derlying model. Using this method, for a point {Y 0, x0}, the closest k points {x(r), r =
1, . . . , k} to x0 are identiﬁed, and Y 0 is assigned to the most popular class among the
k neighbors, where ties are broken at random. “Distance” here is measured in covariate
93

space, not geographic space, and could be deﬁned using any valid distance metric. In our
implementation of the method, we use Euclidean distance so that
d(r) = ||x(r) −x0||.
Here, d(i) represent the ordered distances where the minimum is d(1) and the maximum
is d(n), and x(r) are the xi corresponding to d(r). Using this measure of distance requires
standardization of the variables so that no variable is given more weight than another.
For the binary case, a decision function can then be deﬁned as
δ(ω) ≡δ(x0, x, Y ) =
k
X
r=1
Y(r)/k
where Y(r) is the Yi associated with d(r). A k-nearest neighbor classiﬁcation rule will then
be
Y 0 =
(
1, if Pk
i=1 Y(i)/k > .5
0, if Pk
i=1 Y(i)/k < .5
and if Pk
i=1 Y(i)/k = .5, Y 0 = 1 with probability .5 and Y 0 = 0 with probability .5.
4.4
Comparison of Classiﬁcation Methods
In this section, we discuss the ﬁtting of classiﬁcation parameters and compare the meth-
ods in terms of prediction error on the training and test data sets. We base our comparisons
on the land cover data used in Section 3.3, but this time we use four covariates: elevation,
distance to the coast, distance to the nearest big city, and distance to the nearest major road.
We randomly select ntrain = 432 locations to the training data, and use the remaining
ntest = 144 locations as test data.
94

4.4.1
Parameter Estimation
For each of the classiﬁers, we compute estimates of the corresponding unknown pa-
rameters as described in the previous section. However, further details are required for this
speciﬁc data set for the spatial LDA-based, SVM, and k-nearest neighbor classiﬁers.
When ﬁtting the parameters corresponding to the spatial LDA-based classiﬁer, it is
necessary to ﬁt a spatial covariance function. As discussed in Section 4.3.1, ˇSaltyt˙e Benth
and Duˇcinskas (2005) use a speciﬁc parametric class of covariance functions. To estimate
the parameters of each the covariance functions corresponding to each class, we use the R
package geoR. We use the Gaussian covariance function, where the covariance between
two locations si and sj is
(R(θ))ij ≡
 R(τ 2, φ)

ij = τ 2 exp
 −(dij/φ)2
,
dij is the Euclidean distance between si and sj, σ2 is the variance, and φ is the range
parameter.
Figure 4.2 shows the estimated semivariograms and ﬁtted semivariograms for each of
the classes. In each of these plots, the numbers one through four represent the estimated
(empirical) semivariance at the speciﬁc distances for each of the four covariates. These
estimates are obtained using the variog() function in the geoR package of R which
computes the sample variance for all pairs of observations that speciﬁc difference apart.
Note that the empirical semivariograms differ for each covariate. This model, however,
requires the same covariance function for each of the covariates. Therefore, we ﬁt the
semivariogram to the mean of the four estimated semivariograms. In the plot, the mean
semivariogram is represented by the dots, and the line represents the ﬁtted semivariogram
function. The ﬁtted values of the parameters are listed in Table 4.1. We note that although
95

Figure 4.2: Estimated and ﬁtted variograms for class C0 (left) and C1 (right) ﬁt using the
training data set. The numbers 1-4 indicate the estimated variogram for the four covari-
ates, the dots represent the mean of the four variograms, and the line indicates the ﬁtted
variogram.
Parameter
Class C0
Class C1
τ 2
0.2162
0.3928
φ
1.2316
1.5099
Table 4.1: Fitted values for the covariance function parameters for both class C0 and C1.
the Guassian covariance function may be too smooth for some of the four covariates, in our
analysis, this function was found to ﬁt the mean of all four covariate semivariograms best
when compared with other functions from the Mat´ern class of covariance functions (e.g.,
exponential covariance function).
SVM and k-nearest neighbor classiﬁers require tuning of the parameters, γ and k. We
obtain optimal values of γ and k using ﬁve-fold cross-validation as discussed in Section
96

Classiﬁcation Method
Tuning Parameter
CVE
Optimal Value
Linear SVM
γ
0.2744
0.13
Cubic SVM
γ
0.2791
0.88
Radial SVM
γ
0.2512
0.715
k-NN
k
0.2349
4
Table 4.2: Tuning parameter values for each classiﬁcation method. The optimal value of
the tuning parameter is listed along with the CVE associated with this value. The optimal
values were chosen by minimizing the ﬁve-fold CVE.
4.1 and assigned the value of the associated parameter to be the value with the lowest
cross-validation error (CVE). Table 4.2 shows the tuning parameter for each classiﬁcation
method and the associated chosen value of each parameter. For the SVM classiﬁers, the
tuning parameter is a cost parameter [discuss table].
4.4.2
Classiﬁcation Errors
Table 4.3 shows the training and test errors for each of the classiﬁcation methods. Most
of the methods have training error rates around 27% and test error rates around 29%, with
a few exceptions. The spatial LDA classiﬁer has particularly large training and test error
rates of approximately 76%. Therefore, it appears that adding the restriction of spatial de-
pendence to the predictors/inputs has a negative effect on prediction. One reason for this
may be that the underlying model requires each covariate to have the same ﬁtted spatial
covariance function. Because of this possible explanation for the poor performance of this
classiﬁer, we also ﬁt the model using only one covariate, but found similar error rates, indi-
cating that adding spatial dependence in covariate space may be a too restrictive assumption
for these data.
97

While the k-nearest neighbors classiﬁer has the smallest error rate for the training data,
the classiﬁer based on the Bayesian spatial probit regression model (spatial probit classiﬁer)
has the smallest error rate for the test data and only a slightly larger one-at-a-time training
error rate. The spatial probit classiﬁer joint training error is similar to that observed with
the other classiﬁers. This is due to the fact that the method does not take advantage of
the observed class of neighboring locations within the training data when the joint training
error is determined. The small test error for the spatial probit classiﬁer also illustrates how
conditioning on the observed class of neighboring locations within the training data will
improve classiﬁcation error rates over other non-spatial classiﬁers.
For this training sample, we note that the mean and posterior predictive methods result
in identical error rates for the spatial probit classiﬁer. To see why these error rates are the
same, consider the following features of the model underlying the classiﬁer. The poste-
rior mean classiﬁer is based on P(Z0 > 0|Y , x0, ˆβ, ˆρ, ˆZ), while the posterior predictive
classiﬁer is based on P(Z0 > 0|Y , x0), which is obtained by marginalizing over the pos-
terior distributions of β, ρ, and Z rather than conditioning on the posterior mean of these
parameters. Consider the mean of Z0|Y , x0:
E(Z0|Y , x0) = E
 E(Z0|Y , x0, β, ρ, Z)

= E

x0′β + ρ
X
j:s0∼sj
wj
w+
(Zj −x′
jβ)


= x0′E(β) +
X
j:s0∼sj
wj
w+
 E(ρZj) −x′
jE(ρβ)

where s0 ∼sj indicates that sj and s0 are neighbors, wj is equal to one if sj and s0 are
neighbors, and w+ denotes the total number of neighbors for location s0. Note that if the
98

posterior correlations between ρ and β and between ρ and Z are zero, then
E(ρβ) = E(ρ)E(β)
(4.25)
and
E(ρZj) = E(ρ)E(Zj)
(4.26)
for all j = 1, . . . , n. We cannot determine analytically the posterior correlations between
these pairs of parameters since the posterior distributions are not available in closed form;
however, we can estimate the correlations between these parameters using the sampled
values. In our analysis,
|Cor(ρ, βk)| < 0.13
for all k = 0, 1, . . . , 4, and
|Cor(ρ, Zj)| < 0.17
for all j = 1, . . . , n. Although not perfectly uncorrelated, the correlations are small enough
to approximate the relationships in (4.25) and (4.26). Furthermore, for our particular anal-
ysis, the posterior distribution of ρ has a mean close to one and a very small variance, i.e.,
the 95% credible interval of ρ is (0.934, 0.999). Therefore,
E(Z0|Y , x0) ≈x0′E(β) + E(ρ)
X
j:s0∼sj
wj
w+
(E(Zj) −x′
jE(β))
= x0′ ˆβ + ˆρ
X
j:s0∼sj
wj
w+
( ˆZj −x′
j ˆβ)
= E(Z0|Y , x0, ˆβ, ˆρ, ˆZ).
Thus, the estimated probabilities P(Z0 > 0|Y , x0, ˆβ, ˆρ, ˆZ) and P(Z0 > 0|Y , x0) will
be similar and the binary classiﬁcations based on these probabilities are likely to be the
same, as is observed for the one-at-a-time training error and the joint test error rates. For a
99

Classiﬁcation Method
Training Error
Test Error
Spatial Probit Classiﬁer
Posterior Mean
0.1690
0.1667
Posterior Predictive
One-at-a-Time
Joint
0.1690
0.2708
0.1667
Bayesian Probit Classiﬁer
Posterior Mean
0.2778
0.2986
Posterior Predictive
0.2755
0.2917
GLM-Based maximum likelihood Classiﬁer
Logistic
0.2824
0.3056
Probit
0.2824
0.2986
Discriminant Analysis Classiﬁer
LDA
0.2778
0.2917
DLDA
0.3611
0.3889
QDA
0.2593
0.2986
Spatial LDA
0.7639
0.7569
SVM Classiﬁer
Linear SVM
0.2847
0.3056
Cubic SVM
0.2755
0.2847
Radial SVM
0.2315
0.2986
k-Nearest Neighbor Classiﬁer
k-NN
0.1667
0.2431
Table 4.3: Training and test errors for the SE Asia land cover data obtained using each of
the classiﬁcation methods discussed in this chapter.
different data set or for different spatial dependence structures, the error rates may not be
the same.
Based on our analysis, we conclude that including residual spatial dependence is im-
portant in classiﬁcation of spatially-referenced data and that the classiﬁer based on the
Bayesian probit regression model is well-suited for doing so. Furthermore, including spa-
tial dependence in the predictors/inputs, as in the spatial LDA classiﬁer, is not sufﬁcient,
and in fact, may be inappropriate.
100

4.5
Summary
In this chapter, we proposed a spatial classiﬁer based on the Bayesian spatial probit
regression model and compared this classiﬁer to other well-known classiﬁcation methods.
Furthermore, for the Bayesian probit regression model and the Bayesian spatial probit re-
gression model, we distinguish between two classiﬁers, which we call the posterior mean
and posterior predictive classiﬁers. Using a data analysis, we showed that the spatial probit
classiﬁer (both the posterior mean and posterior predictive versions) is the best classiﬁer in
terms of out-of-sample prediction for our spatially-referenced land cover data example.
101

CHAPTER 5
BAYESIAN SPATIAL MULTINOMIAL PROBIT REGRESSION
In Section 2.1.2, we introduced the Albert and Chib (1993) Bayesian multinomial pro-
bit regression model for multi-category response variables. For these models, the response
variable Yi can take on a value in {1, . . . , ℓ}, where ℓis the number of categories. In this
chapter, we extend the Bayesian multinomial probit regression model and the Bayesian
spatial probit regression model to a model for spatially-referenced multi-category response
variables, the Bayesian spatial multinomial probit regression model. In Section 5.1, we
discuss the speciﬁcation of this model, speciﬁcally, we discuss considerations for the la-
tent mean and covariance structures. In Section 5.2.1, we discuss a data augmentation
model-ﬁtting technique, as an extension of Marginal-Scheme 1 as proposed in Section 3.1
and the Imai and van Dyk (2005) model-ﬁtting algorithm for Bayesian multinomial probit
regression model.
5.1
The Bayesian Spatial Multinomial Probit Regression Model
Just as the Bayesian spatial probit regression model is an extension of the Bayesian
probit regression model, we can extend the Bayesian multinomial regression model in the
same way, by allowing the continuous latent variable to have a spatial covariance structure.
102

Therefore, the general case for a spatially-dependent multi-category Bayesian probit model
is:
Yi = arg maxj { ˜Zij, j = 1, . . . , ℓ}
(5.1)
and
vec( ˜Z) ≡


˜Z1
...
˜Zn

∼N(˜µ, ˜Σ),
(5.2)
where ˜Zi = ( ˜Zi1, . . . , ˜Ziℓ)′, ˜µ is an nℓ× 1 mean vector, and ˜Σ is an nℓ× nℓspatial
covariance matrix across the ℓcategories.
For each observation Yi, rather than modeling the ℓ× 1 unobserved latent variables, it
is common to express each latent variable ˜Zij with respect to ˜Ziℓby deﬁning an (ℓ−1) × 1
vector ˜Ui = ( ˜Ui1, . . . , ˜Ui,ℓ−1)′ where ˜Uij = ˜Zij −˜Ziℓfor j = 1, . . . , (ℓ−1) (e.g., Daganzo,
1980; McCulloch and Rossi, 1994; McCulloch et al., 2000; Imai and van Dyk, 2005). By
doing this, rather than estimating an ℓ× 1 latent vector for each observation, we now
estimate only an (ℓ−1) × 1 latent vector. As a result, all parameters, if appropriately
speciﬁed, are (likelihood) identiﬁable.
This change of variables can be written by deﬁning a matrix ∆to be an n(ℓ−1) × nℓ
block-diagonal matrix, where the ith diagonal block is a matrix ∆such that the ﬁrst ℓ−1
columns of ∆are an (ℓ−1)×(ℓ−1) identity matrix and the ℓth column of ∆is an (ℓ−1)×1
vector of −1s. That is,
∆=


1
0
· · ·
0
−1
0
1
· · ·
0
−1
...
...
...
...
...
0
0
. . .
1
−1

,
and ∆≡In ⊗∆.
In the latent variable representation of the multinomial probit regression model, the
observed, Yis are related to the ˜Uis rather than the ˜Zis. To illustrate, consider ˜Zi, where
103

Yi = j if ˜Zij = max ˜Zi. When ˜Ziℓis the maximum of ˜Zi, ˜Uij = ˜Zij −˜Ziℓ< 0 for all
j = 1, . . . , (ℓ−1). Otherwise, if ˜ZiJ is the maximum of ˜Zi, where J ∈{1, . . . , ℓ−1}, then
˜UiJ = ˜ZiJ −˜Ziℓ> 0 and ˜UiJ = ˜ZiJ −˜Ziℓ> ˜Zij −˜Ziℓ= ˜Uij for all j ̸= J ∈{1, . . . , ℓ−1}.
Thus, we express (5.1) and (5.2) as
Yi =
(
ℓ, if max ˜Ui < 0
j, if max ˜Ui = ˜Uij > 0
(5.3)
where
vec( ˜U) = ∆vec( ˜Z) ∼N(∆˜µ, ∆˜Σ∆′).
(5.4)
In the following subsections, we consider a regression framework and discuss various
speciﬁcations of the latent variable mean and covariance and the interpretation of the asso-
ciated model parameters.
5.1.1
Latent Mean Speciﬁcation
As discussed in Section 2.2, the parameters of the binary Bayesian probit regression
model are only identiﬁable up to a multiplicative constant. This is also the case in the
multi-category setting. However, without loss of generality, in the following discussion
concerning the latent mean structure, we express the model only in terms of the identiﬁable
parameters Z, U, and µ. More discussion on identiﬁability in the latent variable represen-
tation of the Bayesian spatial multinomial probit model will be provided in Section 5.2.1.
Analogous to the binary response setting, a multinomial analysis seeks to determine the
relationship between the response variable and covariate information. To determine such a
relationship, we consider three types of potential covariate information:
A. Category-speciﬁc covariates
B. Location-speciﬁc covariates
104

C. Location-category-speciﬁc covariates.
To help motivate these different types of covariate information, consider a land cover
example. Category-speciﬁc covariates are information that may vary across categories, but
are constant across locations. For example, suppose that across our study region, each land
cover type has an associated tax. We might expect that land cover categories with higher
taxes may be less likely to be observed. Location-speciﬁc covariates contain information
that varies across locations, but for a speciﬁc location, is constant across categories. Eleva-
tion is an example of a location-speciﬁc covariate because it changes for each location, but
at each location it is the same no matter the category. Finally, location-category-speciﬁc
covariates are information that varies across both locations and categories. One example of
a location-category-speciﬁc covariate is the value, or price, of each land cover for a speciﬁc
parcel of land. At each location, or at smaller subregions within our study area, the price
of each land cover type is known and changes across locations or subregions. We would
also expect this type of information to be related to the land cover response variable. For
the multinomial probit regression model, including each of the three types of information
in an analysis requires different speciﬁcations of the latent mean structure. In this section,
we consider how each of the three types of covariate information can be accommodated in
the latent mean speciﬁcation within a regression framework, i.e., µ = Xβ.
Suppose we have k(A) covariates representing category-speciﬁc information, k(B)
covariates representing location-speciﬁc information, and k(C) covariates representing
location-category-speciﬁc information. Let X = [X(A), X(B), X(C)] where X(A) des-
ignates the covariates capturing category-speciﬁc information, X(B) designates the covari-
ates capturing location-speciﬁc information, and X(C) designates the covariates capturing
location-category-speciﬁc information. A category-speciﬁc intercept can be considered a
105

special case of a category-speciﬁc covariate, and is included in X(A), as discussed in Sec-
tion A below. Similarly, let β = (β(A)′, β(B)′, β(C)′)′, where β(A), β(B), and β(C) are the
collections of coefﬁcients corresponding to X(A), X(B), and X(C), respectively. The exact
dimension of each of these covariate and coefﬁcient matrices will depend on the model
speciﬁcation and will be discussed further in the following subsections.
In the following subsections, we discuss two ways to include each type of information
in the model. The ﬁrst is to assume that the relationship between each covariate and each
of the categories is the same for all categories. This means that each covariate will have a
single regression coefﬁcient for all categories. The second is to assume that the relationship
between each covariate and each category may be different. In this case, for each covariate
we ﬁt a separate regression coefﬁcient for each category. In discussing these two cases,
we only consider continuous covariates, but the methods could be extended for discrete or
categorical covariate information. For each type of information, we consider µ = Xβ and
the impact the speciﬁcation of the design matrix has on ∆µ = ∆Xβ.
A. Category-Speciﬁc Information
Let x(A)
m
= (x(A)
m1, . . . , x(A)
mℓ)′, where x(A)
mj is the mth category-speciﬁc covariate for the
jth category, for m = 1, . . . , k(A). For the moment, we assume the mean only includes
category-speciﬁc information, i.e., µ = X(A)β(A). We also include the intercept in the
model in the standard way by including a column of ones in X(A). Note that the meaning
of x(A)
m remains constant for each case discussed below. However, we redeﬁne X(A) based
on x(A)
m for m = 1, . . . , k(A), to distinguish between the cases.
Case 1a: The regression coefﬁcients and intercept are identical across categories.
106

Let
X(A) ≡X(A1a) = 1n ⊗
h
1ℓ
x(A)
1
· · ·
x(A)
k(A)
i
so that X(A1a) is an nℓ× (k(A) + 1) matrix of covariates, and let
β(A) ≡β(A1a) = (β(A1a)
0
, β(A1a)
1
, . . . , β(A1a)
k(A) )′,
so that β(A1a) is a (k(A) + 1) × 1 vector of coefﬁcients.
Consider µ = X(A)β(A):
µ = X(A1a)β(A1a) =

1n ⊗
h
1ℓ
x(A)
1
. . .
x(A)
k(A)
i


β(A1a)
0
β(A1a)
1 ...
β(A1a)
k(A)


= 1n ⊗
h
β(A1a)
0
1ℓ+ β(A1a)
1
x(A)
1
+ · · · + β(A1a)
k(A) x(A)
k(A)
i
It follows that the mean for the latent Zij is
µij = β(A1a)
0
+ β(A1a)
1
x(A)
1j + · · · + β(A1a)
k(A)j x(A)
k(A)j.
for i = 1, . . . , n and j = 1, . . . , ℓ. In this case, β(A1a)
m
captures the increase in the
mean of the latent variable Zij for a one unit increase in the mth covariate x(A)
mj , for
m = 1, . . . , k(A). β(A1a)
0
is the intercept. These parameters are constant across all
categories. Furthermore, because the category-speciﬁc covariates are constant across
locations, µij = µi∗j for all i, i∗= 1, . . . , n.
107

Now consider the mean of the latent variable U, ∆µ = ∆X(A)β(A):
∆µ = ∆X(A1a)β(A1a)
= ∆

1n ⊗
h
1ℓ
x(A)
1
· · ·
x(A)
k(A)
i


β(A1a)
0
β(A1a)
1 ...
β(A1a)
k(A)


= (In ⊗∆)

1n ⊗
h
1ℓ
x(A)
1
· · ·
x(A)
k(A)
i


β(A1a)
0
β(A1a)
1 ...
β(A1a)
k(A)


= 1n ⊗
h
β(A1a)
0
∆1ℓ+ β(A1a)
1
∆x(A)
1
+ · · · + β(A1a)
k(A) ∆x(A)
k(A)
i
.
In this case,
∆1ℓ=


1 −1
...
1 −1


so that
β(A1a)
0
∆1ℓ= 0ℓ−1
where 0ℓ−1 is an (ℓ−1) × 1 vector of zeros, and
∆x(A)
m =


x(A)
m1 −x(A)
mℓ
...
x(A)
m(ℓ−1) −x(A)
mℓ


(5.5)
so that
β(A1a)
m
∆x(A)
m =


β(A1a)
m
(x(A)
m1 −x(A)
mℓ)
...
β(A1a)
m
(x(A)
m(ℓ−1) −x(A)
mℓ)

,
(5.6)
for m = 1, . . . , k(A). Thus, the mean for the latent Uij is
(∆µ)ij = β(A1a)
1
(x(A)
1j −x(A)
1ℓ) + · · · + β(A1a)
k(A) (x(A)
k(A)j −x(A)
k(A)ℓ)
108

where (∆µ)ij is the ijth element of the vector ∆µ for i = 1, . . . , n and j =
1, . . . , (ℓ−1).
We can interpret β(A1a)
m
in terms of comparison to the ℓth category, so that β(A1a)
m
is the increase in the mean of the latent variable Uij for a one unit increase in the
difference between x(A)
mj and x(A)
mℓ. This value is constant across all categories. Just
as µij = µi∗j, (∆µ)ij = (∆µ)i∗j for all i, i∗= 1, . . . , n. Finally note that since
the intercept is identical across categories, there is effectively no intercept when we
compare the jth category to the ℓth category. For this reason, in Case 1b, we consider
allowing the intercept to vary across categories.
Case 1b: The regression coefﬁcients are identical across categories, but each category has
a unique intercept.
Let
X(A) ≡X(A1b) = 1n ⊗
h
In
x(A)
1
· · ·
x(A)
k(A)
i
so that X(A1b) is an nℓ× (k(A) + ℓ) matrix of covariates and let
β(A) ≡β(A1b) = (β(A1b)′
0
, β(A1b)
1
, . . . , β(A1b)
k(A) )′,
where β(A1b)
0
= (β(A1b)
01
, . . . , β(A1b)
0ℓ
)′. β(A1b) is thus a (k(A) + ℓ) × 1 vector of coefﬁ-
cients in this case.
109

Again consider µ:
µ = X(A1b)β(A1b)
=

1n ⊗
h
Iℓ
x(A)
1
· · ·
x(A)
k(A)
i


β(A1b)
0
β(A1b)
1 ...
β(A1b)
k(A)


= 1n ⊗
h
Iℓβ(A1b)
0
+ β(A1b)
1
x(A)
1
+ · · · + β(A1b)
k(A) x(A)
k(A)
i
.
It follows that the mean of the latent Zij is
µij = β(A1b)
0j
+ β(A1b)
1
x(A)
1j + · · · + β(A1b)
k(A) x(A)
k(A)j
for i = 1, . . . , n and j = 1, . . . , ℓ. As in Case 1a, the covariate coefﬁcients, β(A1b)
m
for m = 1, . . . , k(A), are constant across categories and represent the increase in
the mean of the latent variable Zij for a one unit increase in the mth covariate x(A)
mj ,
for m = 1, . . . , k(A). This speciﬁcation, however, allows for a category-speciﬁc
intercept, so that β(A1b)
0j
is the intercept for category j. The category means are still
the same across locations, i.e., µij = µi∗j for all i, i∗= 1, . . . , n.
Now consider ∆µ:
∆µ = ∆X(A1b)β(A1b)
= ∆

1n ⊗
h
Iℓ
x(A)
1
· · ·
x(A)
k(A)
i


β(A1b)
0
β(A1b)
1 ...
β(A1b)
k(A)


= (In ⊗∆)

1n ⊗
h
Iℓ
x(A)
1
· · ·
x(A)
k(A)
i


β(A1b)
0
β(A1b)
1 ...
β(A1b)
k(A)


= 1n ⊗
h
∆Iℓβ(A1b)
0
+ β(A1b)
1
∆x(A)
1
+ · · · + β(A1b)
k(A) ∆x(A)
k(A)
i
.
110

In this case,
∆Iℓ=


1
0
· · ·
0
−1
0
1
· · ·
0
−1
...
...
...
...
...
0
0
· · ·
1
−1


(5.7)
so that
∆Iℓβ(A1b)
0
=


β(A1b)
01
−β(A1b)
0ℓ
β(A1b)
02
−β(A1b)
0ℓ
...
β(A1b)
0(ℓ−1) −β(A1b)
0ℓ


(5.8)
and ∆x(A)
m and β(A1b)
m
∆x(A)
m are the same as (5.5) and (5.6) with β(A1b)
m
= β(A1a)
m
for
m = 1, . . . , k(A). Thus, the mean for the latent Uij is
(∆µ)ij = β(A1b)
0j
−β(A1b)
0ℓ
+ β(A1b)
1
(x(A)
1j −x(A)
1ℓ) + · · · + β(A1b)
k(A) (x(A)
k(A)j −x(A)
k(A)ℓ)
for i = 1, . . . , n and j = 1, . . . , (ℓ−1).
The intercept component of the mean ∆µ is the difference between the jth and ℓth
category intercepts, or β(A1b)
0j
−β(A1b)
0ℓ
, for j = 1, . . . , (ℓ−1). As in Case 1a, the
regression coefﬁcients β(A1b) represent the increase in the mean of Uij due to a one
unit increase in the difference between x(A)
mj and x(A)
mℓ. Also, we again have that the
category means are constant across locations, i.e., (∆µ)ij = (∆µ)i∗j for all i, i∗=
1, . . . , n.
Case 2: Each covariate has a unique regression coefﬁcient for each category.
The two previous cases require that the increase in the latent variable mean for the
jth category is the same for all categories. Suppose, however, that we want to allow
for distinct coefﬁcients across categories. In the land cover example, this alternative
assumption may be appropriate when including land taxes in the model. The effect
111

of a tax increase may differ across land cover types. For example, a one percent
increase in the tax for a residential parcel of land may not be as important as a similar
tax increase imposed on a commercial parcel of land. Here we discuss how category-
speciﬁc covariates can differ according to outcome category.
In this case, let
D(A2)
m
= diag(x(A)
m ) =


x(A)
m1
0
· · ·
0
0
x(A)
m2
· · ·
0
...
...
...
...
0
0
· · ·
x(A)
mℓ


and let
X(A) ≡X(A2) = 1n ⊗
h
Iℓ
D(A2)
1
· · ·
D(A2)
k(A)
i
,
so X(A2) is a nℓ× (k(1) + 1)ℓmatrix of covariates. Furthermore, let
β(A) ≡β(A2) = (β(A2)′
0
, β(A2)′
1
, . . . , β(A2)′
k(A) )′,
where β(A2)
m
= (β(A2)
m1 , . . . , β(A2)
mℓ)′ for m = 0, 1, . . . , k(A) so that β(A2) is a
(k(A) + 1)ℓ× 1 vector of coefﬁcients.
Consider µ:
µ = X(A2)β(A2)
=

1n ⊗
h
Iℓ
D(A2)
1
· · ·
D(A2)
k(A)
i


β(A2)
0
β(A2)
1...
β(A2)
k(A)


= 1n ⊗
h
Iℓβ(A2)
0
+ D(A2)
1
β(A2)
1
+ · · · + D(A2)
k(A) β(A2)
k(A)
i
.
This implies that the mean of the latent Zij is
µij = β(A2)
0j
+ β(A2)
1j
x(A)
1j + · · · + β(A2)
k(A)jx(A)
k(A)j,
112

for i = 1, . . . , n and j = 1, . . . , ℓ. This time, β(A2)
mj
represents the increase in the
mean of the latent variable Zij due to a one unit increase in the mth covariate for the
jth category, x(A)
mj . Note that the regression coefﬁcients differ across category.
Now consider ∆µ:
∆µ = ∆X(A2)β(A2)
= ∆

1n ⊗
h
Iℓ
D(A2)
1
· · ·
D(A2)
k(A)
i


β(A2)
0
β(A2)
1...
β(A2)
k(A)


= (In ⊗∆)

1n ⊗
h
Iℓ
D(A2)
1
· · ·
D(A2)
k(A)
i


β(A2)
0
β(A2)
1...
β(A2)
k(A)


= 1n ⊗
h
∆Iℓβ(A2)
0
+ ∆D(A2)
1
β(A2)
1
+ · · · + ∆D(A2)
k(A) β(A2)
k(A)
i
,
where ∆Iℓβ(A2)
0
is identical to the expression given in (5.8) with β(A2)
0
= β(A1b)
0
and
∆D(A2)
m
=


x(A)
m1
0
· · ·
0
−x(A)
mℓ
0
x(A)
m2
· · ·
0
−x(A)
mℓ
...
...
...
...
...
0
0
· · ·
x(A)
m(ℓ−1)
−x(A)
mℓ


so that
∆D(A2)
m
β(A2)
m
=


β(A2)
m1 x(A)
m1 −β(A2)
mℓx(A)
mℓ
β(A2)
m2 x(A)
m2 −β(A2)
mℓx(A)
mℓ
...
β(A2)
m(ℓ−1)x(A)
m(ℓ−1) −β(A2)
mℓx(A)
mℓ


.
Thus, the mean for the latent Uij is
(∆µ)ij = (β(A2)
0j
−β(A2)
0ℓ
) + (β(A2)
1j
x(A)
1j −β(A2)
1ℓ
x(A)
1ℓ) + · · · + (β(A2)
k(A)jx(A)
k(A)j −β(A2)
j(A)ℓx(A)
k(A)ℓ).
113

Here, the mean of the latent variable Uij increases by β(A2)
mj
for a one unit increase
in x(A)
mj , but decreases by β(A2)
mℓ
for a one unit increase in x(A)
mℓ. As desired, this case
allows categories to have distinct regression coefﬁcients. However, many of the pa-
rameters will not be estimable due to collinearity as we discuss below.
First, we discuss collinearity within each covariate. The ℓth column of both ∆Iℓand
∆D(A2)
m
can be written as linear combinations of the ﬁrst ℓ−1 columns. To see this
for ∆Iℓ, we ﬁrst deﬁne the notation (M)·j to be the jth column of the matrix M.
Using this notation, (∆Iℓ)·j is the jth column of ∆Iℓfor j = 1, . . . , ℓ, and we have
that
(∆Iℓ)·ℓ=
ℓ−1
X
j=1
−1 (∆Iℓ)·j.
Obviously, this relationship will hold for all i locations, and the columns of 1n ⊗
∆Iℓwill also be collinear. Thus, it is not appropriate to ﬁt an ℓth category-speciﬁc
intercept in a multinomial probit regression model. Rather than ﬁtting both β(A2)
0j
and
β(A2)
0ℓ
, we instead ﬁt the difference (β(A2)
0j
−β(A2)
0ℓ
). [Does this satsify, ”so what do
we do?”]
For ∆D(A2)
m
, a similar relationship holds. Taking (∆D(A2)
m
)·j to be the jth column of
∆D(A2)
m
for j = 1, . . . , ℓ, we have that
(∆D(A2)
m
)·ℓ=
ℓ−1
X
j=1
−x(A)
mℓ
x(A)
mj
(∆D(A2)
m
)·j.
Because the category-speciﬁc covariates are constant across locations by deﬁnition,
this relationship will hold across all locations i = 1, . . . , n, and the columns of 1n ⊗
∆D(A2)
m
will also be collinear.
We now discuss the collinearity between the covariates and intercept component of
the design matrix. Notice that the columns of ∆Iℓand ∆D(A2)
m
for m = 1, . . . , k(A)
114

are collinear:
(∆Iℓ)·j =
1
x(A)
1j
(∆D(A2)
1
)·j = · · · =
1
x(A)
k(A)j
(∆D(A2)
k(A) )·j .
Because the covariates are constant across locations, the columns making up the
design matrix will also be collinear. Thus, in Case 2 where regression coefﬁcients
vary across categories, only the coefﬁcients corresponding to one covariate or the
intercept can be identiﬁed. If we wish to include further category-speciﬁc covariates,
we must include them as in Case 1b.
In Case 1a and 1b, we saw that it is necessary to ﬁt a category-speciﬁc intercept. In Case
2 we saw, however, that category-speciﬁc coefﬁcients and intercept are not identiﬁable.
Thus, it is necessary to express X(A) using Case 1b, noting that the intercept could be
replaced by a single category-speciﬁc covariate.
Thus,
X(A) = 1n ⊗


1
0
· · ·
0
x(A)
11
· · ·
x(A)
k(A)1
0
1
· · ·
0
x(A)
12
· · ·
x(A)
k(A)2
...
...
...
...
...
...
...
0
0
· · ·
1
x(A)
1(ℓ−1)
· · ·
x(A)
k(A)(ℓ−1)
0
0
· · ·
0
x(A)
1ℓ
· · ·
x(A)
k(A)ℓ


and
β(A) = (β(A)′
0
, β(A)
1
, . . . , β(A)
k(A))′
where β(A)
0
= (β(A)
01 , . . . , β(A)
0(ℓ−1))′.
B. Location-Speciﬁc Information
Let x(B)
m
= (x(B)
m1 , . . . , x(B)
mn)′ where x(B)
mi is the mth location-speciﬁc covariate for the
ith location, for m = 1, . . . , k(B). In this discussion, we assume the mean of the latent Z
only includes location-speciﬁc information, i.e., µ = X(B)β(B). Note that the meaning of
115

x(B)
m remains constant for each case discussed below. However, we redeﬁne X(B) based on
x(B)
m for m = 1, . . . , k(B), to distinguish between the cases.
Case 1: The regression coefﬁcients are identical across categories.
Let
X(B) ≡X(B1) =
h
x(B)
1
· · ·
x(B)
k(B)
i
⊗1ℓ
so that X(B1) is a nℓ× k(B) matrix of location-speciﬁc covariates, and let
β(B) ≡β(B1) = (β(B1)
1
, . . . , β(B1)
k(B) )′
so that β(B1) is a k(B) × 1 vector of coefﬁcients.
Now consider µ:
µ = X(B1)β(B1) =
h
x(B)
1
. . .
x(B)
k(B)
i
⊗1ℓ



β(B1)
1...
β(B1)
k(B)


=
h
β(B1)
1
(x(B)
1
⊗1ℓ) + · · · + β(B1)
k(B) (x(B)
k(B) ⊗1ℓ)
i
so that the mean of the latent Zij is
µij = β(B1)
1
x(B)
1i + · · · + β(B1)
k(B) x(B)
k(B)i,
for i = 1, . . . , n and j = 1, . . . , ℓ. Here, β(B1)
m
represents the increase in the mean of
the latent variable Zij corresponding to a one unit increase in x(B)
mi .
116

Now consider ∆µ:
∆µ = ∆X(B1)β(B1) = ∆
h
x(B)
1
· · ·
x(B)
k(B)
i
⊗1ℓ



β(B1)
1...
β(B1)
k(B)


= (In ⊗∆)
h
x(B)
1
⊗1ℓ
· · ·
x(B)
k(B) ⊗1ℓ
i


β(B1)
1...
β(B1)
k(B)


=
h
β(B1)
1
(x(B)
1
⊗∆1ℓ) + · · · + β(B1)
k(B) (x(B)
k(B) ⊗∆1ℓ)
i
=


β(B1)
1
x(B)
11 ∆1ℓ+ · · · + β(B1)
k(B) x(B)
k(B)1∆1ℓ
β(B1)
1
x(B)
12 ∆1ℓ+ · · · + β(B1)
k(B) x(B)
k(B)2∆1ℓ
...
β(B1)
1
x(B)
1n ∆1ℓ+ · · · + β(B1)
k(B) x(B)
k(B)n∆1ℓ


,
where
x(B)
mi ∆1ℓ= x(B)
mi


(1 −1)
...
(1 −1)

=


x(B)
mi −x(B)
mi
...
x(B)
mi −x(B)
mi

= 0ℓ−1
and
β(B1)
1
x(B)
mi ∆1ℓ= 0ℓ−1,
for i = 1, . . . , n and m = 1, . . . , k(B). As a result, the mean of the latent Uij is
(∆µ)ij = 0,
for i = 1, . . . , n and j = 1, . . . , ℓ.
Thus, when including location-speciﬁc covariates in a multinomial probit regression
model, regression coefﬁcients that are constant across categories are not identiﬁable.
Therefore, for location-speciﬁc covariates, we must specify a unique regression co-
efﬁcient for each category, as in Case 2.
Case 2: Each covariate has a unique regression coefﬁcient for each category.
117

Let
X(B) ≡X(B2) =
h
x(B)
1
· · ·
x(B)
k(B)
i
⊗Iℓ
so that X(B2) is a nℓ× k(B)ℓmatrix of location-speciﬁc covariates. Let
β(B) ≡β(B2) = (β(B2)′
1
, . . . , β(B2)′
k(B) )′,
where β(B2)
m
= (β(B2)
m1 , . . . , β(B2)
mℓ)′ so that β(B2) is a k(B)ℓ× 1 vector of coefﬁcients.
Consider µ:
µ = X(B2)β(B2) =
h
x(B)
1
· · ·
x(B)
k(B)
i
⊗Iℓ



β(B2)
1...
β(B2)
k(B)


=
h
(x(B)
1
⊗Iℓ)β(B2)
1
+ · · · + (x(B)
k(B) ⊗Iℓ)β(B2)
k(B)
i
so that the mean for the latent Zij is
µij = β(B2)
1j
x(B)
1i + · · · + β(B2)
k(B)jx(B)
k(B)i,
for i = 1, . . . , n and j = 1, . . . , ℓ. Here, β(B2)
mj
represents the increase in the mean of
the latent variable Zij for a one unit increase in x(B)
mi , for m = 1, . . . , k(B), and each
β(B2)
mj
is allowed to be different across the ℓcategories.
118

Now consider ∆µ:
∆µ = ∆X(B2)β(B2) = ∆
h
x(B)
1
· · ·
x(B)
k(B)
i
⊗Iℓ



β(B2)
1...
β(B2)
k(B)


= (In ⊗∆)
h
x(B)
1
⊗Iℓ
· · ·
x(B)
k(B) ⊗Iℓ
i


β(B2)
1...
β(B2)
k(B)


=
h
(x(B)
1
⊗∆)β(B2)
1
+ · · · + (x(B)
k(B) ⊗∆)β(B2)
k(B)
i
=


x(B)
11 ∆β(B2)
1
+ · · · + x(B)
k(B)1∆β(B2)
k(B)
x(B)
12 ∆β(B2)
1
+ · · · + x(B)
k(B)2∆β(B2)
k(B)
...
x(B)
1n ∆β(B2)
1
+ · · · + x(B)
k(B)n∆β(B2)
k(B)


where
x(B)
mi ∆=


x(B)
mi
0
· · ·
0
−x(B)
mi
0
x(B)
mi
· · ·
0
−x(B)
mi
...
...
...
...
...
0
0
· · ·
x(B)
mi
−x(B)
mi


and
x(B)
mi ∆β(B2)
m
=


x(B)
mi
0
· · ·
0
−x(B)
mi
0
x(B)
mi
· · ·
0
−x(B)
mi
...
...
...
...
...
0
0
· · ·
x(B)
mi
−x(B)
mi




β(B2)
m1
β(B2)
m2...
β(B2)
m(ℓ−1)
β(B2)
mℓ


=


β(B2)
m1 x(B)
mi −β(B2)
mℓx(B)
mi
β(B2)
m2 x(B)
mi −β(B2)
mℓx(B)
mi
...
β(B2)
m(ℓ−1)x(B)
mi −β(B2)
mℓx(B)
mi


=


(β(B2)
m1
−β(B2)
mℓ)x(B)
mi
(β(B2)
m2
−β(B2)
mℓ)x(B)
mi
...
(β(B2)
m(ℓ−1) −β(B2)
mℓ)x(B)
mi


.
119

Thus, the mean of the latent Uij is
(∆µ)ij = (β(B2)
1j
−β(B2)
1ℓ
)x(B)
1i + · · · + (β(B2)
k(B)j −β(B2)
k(B)ℓ)x(B)
k(B)i
for i = 1, . . . , n and j = 1, . . . , ℓ.
Here, β(B2)
mj
−β(B2)
mℓ
represents the increase in the mean of the latent variable Uij due
to a one unit increase in x(B)
mi . Note that here we have a similar collinearity problem
to that seen in Case 2 for the category-speciﬁc covariates: the ℓth column of x(B)
mi ∆
can be written as a linear combination of the ﬁrst ℓ−1 columns. Therefore, β(B2)
mℓ
will not be estimable.
In Case 1, we saw that we cannot estimate regression coefﬁcients corresponding to
location-speciﬁc covariates when the regression coefﬁcients were assumed to be constant
across categories. We can, however, estimate ℓ−1 distinct coefﬁcients. Thus, when speci-
fying the design matrix for location-speciﬁc covariates, let
X(B) =
h
x(B)
1
· · ·
x(B)
k(B)
i
⊗
Iℓ−1
0′
ℓ−1

and
β(B) = (β(B)′
1
, . . . , β(B)′
k(B))′,
where β(B)
m
= (β(B)
m1 , . . . , β(B)
m(ℓ−1))′.
C. Location-Category-Speciﬁc Information
Let x(C)
mi = (x(C)
mi1, . . . , x(C)
miℓ)′ be an ℓ× 1 vector of covariates for each location i =
1, . . . , n, for m = 1, . . . , k(C) covariates. In this discussion, we assume the mean only in-
cludes location-category-speciﬁc information, i.e., µ = X(C)β(C). Note that the meaning
of x(C)
mi remains constant for each case discussed below. However, we redeﬁne X(C) based
on x(C)
mi for m = 1, . . . , k(C) and i = 1, . . . , n, to distinguish between the cases.
120

Case 1: The regression coefﬁcients are identical across categories.
Let
X(C) ≡X(C1) =


x(C)
11
· · ·
x(C)
k(C)1
x(C)
12
· · ·
x(C)
k(C)2
...
...
...
x(C)
1n
· · ·
x(C)
k(C)n


be an nℓ× k(C) matrix of covariates, and let
β(C) ≡β(C1) = (β(C1)
1
, . . . , β(C1)
k(C) )′
be a k(C) × 1 vector of coefﬁcients.
Consider µ:
µ = X(C1)β(C1)
=


x(C)
11
· · ·
x(C)
k(C)1
x(C)
12
· · ·
x(C)
k(C)2
...
...
...
x(C)
1n
· · ·
x(C)
k(C)n




β(C1)
1...
β(C1)
k(C)


=


β(C1)
1
x(C)
11 + · · · + β(C1)
k(C) x(C)
k(C)1
β(C1)
1
x(C)
12 + · · · + β(C1)
k(C) x(C)
k(C)2
...
β(C1)
1
x(C)
1n + · · · + β(C1)
k(C) x(C)
k(C)n


so that the mean of the latent Zij is
µij = β(C1)
1
x(C)
1ij + · · · + β(C1)
1
x(C)
k(C)ij,
for i = 1, . . . , n and j = 1, . . . , ℓ. Here, β(C1)
m
represents the increase in the mean of
the latent variable Zij due to a one unit increase in the mth location-category-speciﬁc
covariate x(C)
mij.
121

Now consider ∆µ:
∆µ = ∆X(C1)β(C1)
= ∆


x(C)
11
· · ·
x(C)
k(C)1
x(C)
12
· · ·
x(C)
k(C)2
...
...
...
x(C)
1n
· · ·
x(C)
k(C)n




β(C1)
1...
β(C1)
k(C)


= (In ⊗∆)


x(C)
11
· · ·
x(C)
k(C)1
x(C)
12
· · ·
x(C)
k(C)2
...
...
...
x(C)
1n
· · ·
x(C)
k(C)n




β(C1)
1...
β(C1)
k(C)


=


β(C1)
1
∆x(C)
11 + · · · + β(C1)
k(C) ∆x(C)
k(C)1
β(C1)
1
∆x(C)
12 + · · · + β(C1)
k(C) ∆x(C)
k(C)2
...
β(C1)
1
∆x(C)
1n + · · · + β(C1)
k(C) ∆x(C)
k(C)n


where
∆x(C)
mi =


x(C)
mi1 −x(C)
miℓ
...
x(C)
mi(ℓ−1) −x(C)
miℓ


and
β(C1)
m
∆x(C)
mi =


β(C1)
m
(x(C)
mi1 −x(C)
miℓ)
...
β(C1)
m
(x(C)
mi(ℓ−1) −x(C)
miℓ)

,
for i = 1, . . . , n and m = 1, . . . , k(C). It follows that the mean of the latent Uij is
(∆µ)ij = β(C1)
1
(x1ij −x1iℓ) + · · · + β(C1)
k(C) (xk(C)ij −xk(C)iℓ),
for i = 1, . . . , n and j = 1, . . . , (ℓ−1).
Here, β(C1)
m
represents the increase in the mean of the latent variable Uij for a one
unit increase in the difference between x(C)
mij and x(C)
miℓ. For each covariate, β(C1)
m
is
constant across the categories.
122

Case 2: Each covariate has a unique regression coefﬁcient for each category.
Deﬁne
D(C2)
mi
= diag(x(C)
mi ) =


x(C)
mi1
0
· · ·
0
0
x(C)
mi2
· · ·
0
...
...
...
...
0
0
· · ·
x(C)
miℓ

,
and let
X(C) = X(C2) =


D(C2)
11
· · ·
D(C2)
k(C)1
...
...
...
D(C2)
1n
· · ·
D(C2)
k(C)n


be an nℓ× k(C)ℓmatrix of covariates. Let
β(C) = (β(C2)′
1
, . . . , β(C2)′
k(C) )′
be a k(3)ℓ× 1 vector of coefﬁcients where β(C2)
m
= (β(C2)
m1 , . . . , β(C2)
mℓ)′.
Consider µ:
µ = X(C2)β(C2)
=


D(C2)
11
· · ·
D(C2)
k(C)1
...
...
...
D(C2)
1n
· · ·
D(C2)
k(C)n




β(C2)
1...
β(C2)
k(C)


=


D(C2)
11
β(C2)
1
+ · · · + D(C2)
k(C)1β(C2)
k(C)
...
D(C2)
1n β(C2)
1
+ · · · + D(C2)
k(C)nβ(C2)
k(C)

,
so that the mean of the latent Zij is
µij = β(C2)
1j
x(C)
1ij + · · · + β(C2)
k(C)jx(C)
k(C)ij,
for i = 1, . . . , n and j = 1, . . . , ℓ. Here, β(C2)
mj
represents the increase in the mean of
the jth category latent variable Zij due to a one unit increase in the mth covariate.
123

Now consider ∆µ:
∆µ = ∆X(C2)β(C2)
= ∆


D(C2)
11
· · ·
D(C2)
k(C)1
...
...
...
D(C2)
1n
· · ·
D(C2)
k(C)n




β(C2)
1...
β(C2)
k(C)


= (In ⊗∆)


D(C2)
11
· · ·
D(C2)
k(C)1
...
...
...
D(C2)
1n
· · ·
D(C2)
k(C)n




β(C2)
1...
β(C2)
k(C)


=


∆D(C2)
11
β(C2)
1
+ · · · + ∆D(C2)
k(C)1β(C2)
k(C)
...
∆D(C2)
1n β(C2)
1
+ · · · + ∆D(C2)
k(C)nβ(C2)
k(C)

,
where
∆D(C2)
mi
=


x(C)
mi1
0
· · ·
0
−x(C)
miℓ
0
x(C)
mi2
· · ·
0
−x(C)
miℓ
...
...
...
...
...
0
0
· · ·
x(C)
mi(ℓ−1)
−x(C)
miℓ


and
∆D(C2)
mi β(C2)
m
=


β(C2)
m1 x(C)
mi1 −β(C2)
mℓx(C)
miℓ
β(C2)
m2 x(C)
mi2 −β(C2)
mℓx(C)
miℓ
...
β(C2)
m(ℓ−1)x(C)
mi(ℓ−1) −β(C2)
mℓx(C)
miℓ


,
for i = 1, . . . , n and m = 1, . . . , k(C). It follows that the mean of the latent Uij is
(∆µ)ij = β(C2)
1j
x(C)
1ij −β(C2)
1ℓ
x(C)
1iℓ+ · · · + β(C2)
k(C)jx(C)
k(C)ij −β(C2)
k(C)ℓx(C)
k(C)iℓ
= β(C2)
1ℓ
 
β(C2)
1j
β(C2)
1ℓ
x(C)
1ij −x(C)
1iℓ
!
+ · · · + β(C2)
1ℓ
 
β(C2)
1j
β(C2)
1ℓ
x(C)
1ij −x(C)
1iℓ
!
= β(C2)
1ℓ

x(C)
1ij −x(C)
1iℓ

+

β(C2)
1j
−β(C2)
1ℓ

x(C)
1ij + · · ·
+ β(C2)
k(C)ℓ

x(C)
k(C)ij −x(C)
k(C)iℓ

+

β(C2)
k(C)j −β(C2)
k(C)ℓ

x(C)
k(C)ij.
Notice that Case 1 can be written as a special case of Case 2, where β(C1)
m
= β(C2)
mℓ
and (β(C2)
mj
−β(C2)
mℓ) = 0, for all j = 1, . . . , (ℓ−1).
124

For category-speciﬁc information, depending on the assumption made about the re-
gression coefﬁcient, Case 1 or Case 2 may be appropriate. In Case 2, all k(C)ℓcoefﬁcient
parameters are estimable. Thus, we can take
X(C) ≡X(C1) and β(C) ≡β(C1)
or
X(C) ≡X(C2) and β(C) ≡β(C2)
or even a combination of both, depending on the speciﬁcation that makes the most sense
for each location-category-speciﬁc covariate.
5.1.2
Parameterization of the Space-Category Covariance Matrix
In (5.2), we expressed the covariance structure for vec( ˜Z) generally as ˜Σ. In this
section, we examine various speciﬁcations of ˜Σ, as well as the implications these structures
have on the covariance of vec( ˜U).
Separable Space-Category Dependence
One speciﬁcation for ˜Σ, is to extend the covariance structure for the independent multi-
nomial probit regression model in (2.4) to allow for spatial dependence by replacing the
identity matrix with a spatial correlation structure, i.e.,
˜Σ ≡Σ(θ) ⊗˜Ω
(5.9)
where Σ(θ) is an n×n spatial correlation matrix and ˜Ωis an ℓ×ℓcovariance matrix spec-
ifying the dependence among the ℓcategories. Notice that this speciﬁcation of ˜Σ requires
that the spatial dependence is the same for all categories and the categorical dependence is
the same for all locations. Thus, ˜Σ given by (5.9) is space-category separable.
125

From (5.4), the covariance of vec( ˜U) is ∆˜Σ∆, and for the separable case, can be
expressed as
∆˜Σ∆= (In ⊗∆)

Σ(θ) ⊗˜Ω

(In ⊗∆)′
= (In Σ(θ) I′
n) ⊗(∆˜Ω∆′)
= Σ(θ) ⊗(∆˜Ω∆′).
Notice that in this case, we again have a separable covariance, where each category relative
to category ℓhas the same spatial dependence and the categorical dependence is the same at
each location. Therefore, separability in the covariance structure of ˜Z results in separability
in the covariance structure of ˜U.
This model is simpler to ﬁt than the model with a general spatial-categorical depen-
dence structure because we only need to work with an n × n matrix and an ℓ× ℓmatrix
rather than an nℓ×nℓmatrix. See Section 5.2 for a model-ﬁtting algorithm associated with
this covariance structure.
When we ﬁt the Bayesian spatial multinomial probit model with a separable depen-
dence structure based on ˜U, we only estimate the (ℓ−1) × (ℓ−1) categorical covariance,
˜Ω∆≡∆˜Ω∆′. The ijth element of this covariance matrix is interpreted as the covariance
between the difference between the latent variables associated with categories i and ℓand
the difference between the latent variables associated with categories j and ℓ. We might
be interested in modeling the dependence between all ℓcategorical responses. However,
because ∆is not invertible, given ˜Ω∆, ˜Ωis not identiﬁable.
Non-Separable Space-Category Dependence
Suppose that instead of assuming space-category separability, we want to allow the
different categories to have different spatial dependence structures. To motivate why this
126

would be desirable, consider two categories of the land cover example, forest and urban. In
many places on Earth, locations covered by forest take up a larger area than urban locations.
This is seen in Figure 1.1, where urban land cover is included in the category ‘other’. Thus,
the spatial dependence for the category forest will have a longer range, or distance at which
locations will be correlated, than that of the spatial dependence for the category urban.
Therefore, the spatial dependence structures for each of these categories should allow for
this difference.
To accommodate category-speciﬁc spatial dependence structures, we can consider the
latent variables of the data augmentation representation of the Bayiesian multinomial probit
regression model to be a spatially-dependent multivariate random variable. Then, we can
use non-separable dependence structures deﬁned for Gaussian multivariate spatial data.
For geostatistical data, a non-space-category separable covariance structure can be ob-
tained through the linear model of coregionalization (LMC; e.g., Wackernagel, 1998). The
Bayesian version of the LMC arises as a special case of the conditional hierarchical ap-
proach in Royle and Berliner (1999). Gelfand et al. (2004) propose an extension to the
LMC, called the spatially-varying LMC, by specifying the dependence structure jointly.
When modeling the dependence structure of lattice/gridded data, several multivariate CAR
models have been proposed (e.g., Mardia, 1988; Carlin and Banerjee, 2003; Gelfand and
Vounatsou, 2003). In addition, Sain and Cressie (2007) propose a canonical multivari-
ate conditional autoregressive (CAMCAR) model as a multivariate extension to the CAR
model that allows for space-category asymmetries. We leave determining the dependence
structure of vec( ˜U) based on each of the previous models as well as the interpretation of
the elements of the resulting covariance structures to future research.
127

5.2
Model-Fitting
5.2.1
Data Augmentation MCMC Algorithms
In this section, we propose a marginal data augmentation algorithm for ﬁtting the space-
category separable model with ˜Σ = Σ(θ) ⊗˜Ω. This algorithm is an extension of the
algorithm proposed by Imai and van Dyk (2005) for independent multi-category response
variables and of Marginal-Scheme 1 proposed in Section 3.1 for spatially-dependent binary
data.
First, let
X∆≡∆X
and
˜Ω∆≡∆˜Ω∆′,
so that
vec( ˜U) ∼N(X∆˜β, Σ(θ) ⊗˜Ω∆).
Here, X∆is speciﬁed appropriately for each type of covariate information considered, as
discussed in Section 5.1.1. We let X∆be an n(ℓ−1) × k(E) matrix, where k(E) represents
the total number of effective covariates, where each covariate may have up to ℓeffective
covariates.
In this case, following Imai and van Dyk (2005), the working parameter is the ﬁrst
element in ˜Ω∆, which we denote as ω2
∆. Therefore, the identiﬁable parameters in this case
will be
U =
˜U
ω∆
,
128

β =
˜β
ω∆
,
and
Ω∆=
˜Ω∆
ω2
∆
.
We assign prior distributions on identiﬁable parameters. Speciﬁcally, we take
β ∼N(0, Cβ),
where 0 is a k(E)×1 vector of zeros and Cβ is a k(E)×k(E) covariance matrix. We consider
only one spatial dependence parameter, i.e., θ ≡θ, and its prior distribution is
θ ∼Unif(lθ, uθ),
where lθ and uθ are appropriate lower and upper bounds, respectively, for θ. Finally,
we take the prior on ˜Ω∆to be inverse Wishart with parameters νΩand
˜
MΩ, i.e.,
˜Ω∆
∼
Inv Wishart(νΩ, ˜
MΩ), where νΩis a scalar and
˜
MΩis an (ℓ−1) × (ℓ−1)
matrix. Transforming to Ω∆yields the following joint density function
π(Ω∆, ω2
∆) ∝|Ω∆|−(νΩ+ℓ)/2 exp

−a2
ω
2ω2
∆
trace(MΩΩ−1
∆)

(ω2
∆)−[νΩ(ℓ−1)/2+1],
where a2
ω is a positive constant, and MΩ=
˜
MΩ/a2
ω. Therefore,
ω2
∆|Ω∆∼a2
ωtrace(MΩΩ−1
∆)/χ2
νΩ(ℓ−1).
(5.10)
The
last
element
of
discussion
before
giving
the
steps
of
the
algorithm
is
to
determine
the
conditional
distributions
of
Ui|[vec(U)]-i, β, θ, Ω∆
and
Uij|Ui,-j, [vec(U)]-i, β, θ, Ω∆, ω2
∆, where Ui
=
(Ui1, . . . , Ui(ℓ−1))′, [vec(U)]-i de-
notes vec(U) with Ui removed, and Ui,-j denotes Ui with the jth element removed.
129

Notice that Ui|[vec(U)]-i, β, θ, Ω∆has a normal distribution with mean
E(Ui|[vec(U)]-i, β, θ, Ω∆)
= (X∆)iβ +

([Σ(θ)]i,-i ⊗Ω∆)([Σ(θ)]-i,-i ⊗Ω∆)−1
([vec(U)]-i −(X∆)-iβ)]
= (X∆)iβ +
 [Σ(θ)]i,-i ([Σ(θ)]-i,-i)−1 ⊗Iℓ−1

([vec(U)]-i −(X∆)-iβ)
and variance
Var(Ui|[vec(U)]-i, β, θ, Ω∆)
= ([Σ(θ)]i,i ⊗Ω∆) −([Σ(θ)]i,-i ⊗Ω∆) ([Σ(θ)]-i,-i ⊗Ω∆)−1 ([Σ(θ)]-i,i ⊗Ω∆)
= ([Σ(θ)]i,i ⊗Ω∆) −
 [Σ(θ)]i,-i([Σ(θ)]-i,-i)−1[Σ(θ)]-i,i ⊗Ω∆

=
 [Σ(θ)]i,i −[Σ(θ)]i,-i ([Σ(θ)]-i,-i)−1 [Σ(θ)]-i,i

|
{z
}
≡σ(θ)i
Ω∆,
where (X∆)i is the (ℓ−1)×k(E) matrix of covariates associated with observation i, (X∆)-i
is the remaining (n −1)(ℓ−1) × k(E) matrix of X∆, and [Σ(θ)]i,-j denotes the ith row of
Σ(θ) with the jth column removed. Using this conditional distribution, we can determine
the distribution of Uij|Ui,-j, [vec(U)]-i, β, θ, Ω∆, which is normally distributed with mean
µUij ≡E(Uij|Ui,-j, [vec(U)]-i, β, θ, Ω∆)
= (X∆)ijβ +
 [Σ(θ)]i,-i ([Σ(θ)]-i,-i)−1 
{[vec(U)]-i}j −{(X∆)-i}j β

+ [Ω∆]j,-j ([Ω∆]-j,-j)−1 
(Ui,-j −(X∆)i,-jβ) +
 [Σ(θ)]i,-i ([Σ(θ)]-i,-i)−1 ⊗Iℓ−2

×

{[vec(U)]-i}-j −{(X∆)-i}-j β
i
,
(5.11)
where (X∆)′
ij is the k(E) × 1 vector associated with Uij, (X∆)i,-j is the remaining
(ℓ−2) × k(E) component of (X∆)i, and the notation {·}j denotes the selection of
130

only the rows corresponding to category j, and {·}-j denotes the selection of the rows not
corresponding to category j. Although the above equation looks daunting, computationally,
it is just a matter of selecting the appropriate rows and columns of the appropriate matrices.
As noted in Section 5.1.2, these computations will be relatively faster than the computa-
tions for the conditional mean for a non-separable dependence structure. The variance is
then
τ 2
Uij ≡Var(Uij|Ui,-j, [vec(U)]-i, β, θ, Ω∆)
= σ(θ)i[Ω∆]j,j −σ(θ)i[Ω∆]j,-j (σ(θ)i[Ω∆]-j,-j)−1 σ(θ)i[Ω∆]-j,j
= σ(θ)i
 [Ω∆]j,j −[Ω∆]j,-j ([Ω∆]-j,-j)−1 [Ω∆]-j,j

.
(5.12)
We now specify the Gibbs sampler and full conditional distributions for this separable
Bayesian spatial multinomial probit regression model:
Step 1: Sample vec( ˜U) from vec(U)|Y , β[t−1], θ[t−1], Ω[t−1]
∆
, (ω2
∆)∗.
• Sample (ω2
∆)∗from the prior distribution ω2
∆|Ωas speciﬁed in (5.10).
• For each i = 1, . . . , n and j = 1, . . . , (ℓ−1), sample ˜Uij from
˜Uij|Y ,[vec(U)]-i, Ui,-j, β[t−1], θ[t−1], Ω[t−1]
∆
, (ω2
∆)∗
∼



TN

ω∗
∆µUij, (ω2
∆)∗τ 2
Uij, max(ω∗
∆Ui,-j, 0), ∞

,
if Yi = j
TN

ω∗
∆µUij, (ω2
∆)∗τ 2
Uij, −∞, max(ω∗
∆Ui,-j, 0)

,
if Yi ̸= j
where µUij and τ 2
Uij are as speciﬁed in (5.11) and (5.12), respectively, plugging
in the current values of the other parameters.
Step 2: Sample (ω2
∆)∗, β[t] from ω2
∆, β|Y , vec(U [t]), θ[t−1], Ω[t−1]
∆
.
131

• Sample (ω2
∆)∗from
(ω2
∆) ∼
ˆω2
χ2
(n+νΩ)(ℓ−1)
where
ˆω2 = (vec( ˜U) −X∆ˆβ)′(Σ(θ[t−1]) ⊗Ω[t−1]
∆
)−1(vec( ˜U) −X ˆβ)′
+ ˆβ C−1
β
ˆβ + trace

˜
M

Ω[t−1]
∆
−1
and
ˆβ =

X′
∆

Σ(θ[t−1]) ⊗Ω[t−1]
∆
−1
X∆+ C−1
β
−1
×

X′
∆

Σ(θ[t−1]) ⊗Ω[t−1]
∆
−1
vec( ˜U)

.
• Sample ˜β ∼N
 
ˆβ, (ω2
∆)∗

X′
∆

Σ(θ[t−1]) ⊗Ω[t−1]
∆
−1
X∆+ C−1
β
−1!
.
• Set β[t] = ˜β/ω∗
∆.
Step 3: Sample (ω2
∆)[t], Ω[t]
∆from (ω2
∆), Ω∆|Y , vec(U [t]), β[t], θ[t−1].
• Sample ˜Ω∆∼Inv Wishart (n + νΩ,
˜
M + [[vec( ˜U) −X∆˜β]]′
n×(ℓ−1)
 Σ(θ[t−1])
−1 [[vec( ˜U) −X∆˜β]]n×(ℓ−1)

.
where the notation [[c]]a×b denotes the operator that reorders the ab × 1 vector
c to form the corresponding a × b matrix, ﬁlling this matrix by rows.
• Set (ω2
∆)[t] equal to the ﬁrst element of ˜Ω∆.
• Set U [t]
ij = ˜Uij/ω[t]
∆, for i = 1, . . . , n and j = 1, . . . , (ℓ−1).
• Set Ω[t]
∆= ˜Ω∆/(ω2
∆)[t].
Step 4: Sample θ[t] from θ|Y , vec(U [t]), β[t], Ω[t]
∆, (ω2
∆)[t] using a Metropolis random walk
step.
132

5.3
Summary
In this chapter, we showed how care must be taken in the speciﬁcation of the latent
variable representation of the Bayesian spatial multinomial probit regression model. In
order for regression coefﬁcient parameters to be identiﬁable, when specifying the design
matrix we must consider the type of covariate information (i.e., category-speciﬁc, location-
speciﬁc, or location-category-speciﬁc). Furthermore, we discussed various ways to specify
the spatial-categorical dependence structure of the latent variable. Under this appropri-
ately speciﬁed latent variable representation of the Bayesian spatial multinomial probit
regression model, we provided a model-ﬁtting algorithm for the case of a separable spatial-
categorical dependence structure.
133

CHAPTER 6
CONTRIBUTIONS AND FUTURE WORK
In this ﬁnal chapter, we discuss our contributions to the development of the Bayesian
spatial probit regression model and provide a list of future research directions.
We began this work by motivating the need for models that allow for residual spatial de-
pendence when analyzing spatially-referenced categorical response variables, speciﬁcally,
within a regression framework. We reviewed a diverse literature of current methods for
analyzing spatially-dependent binary and categorical data, one being the Bayesian spatial
probit regression model. To provide context, in Chapter 2 we discussed the general data-
augmented version of the Bayesian probit regression model, and showed how extensions of
this model ﬁt within the general speciﬁcation of the model. Speciﬁcally, we showed how
the Bayesian probit regression model is extended to allow for residual spatial dependence,
resulting in the Bayesian spatial probit regression model. We also demonstrated that the
variance of the latent variable in the data-augmented Bayesian probit regression models is
not identiﬁable.
To account for, and exploit for computational purposes, this non-identiﬁability in the
Bayesian spatial probit regression model, in Chapter 3 we introduced and compared several
data augmentation MCMC algorithms. These algorithms use the non-identiﬁable param-
eter, or working parameter, to improve computational efﬁciency of model ﬁtting. While
134

conditional and marginal augmentation strategies for the independent version of this model
have been compared in the literature previously, the spatial extension introduces additional
complexity. Furthermore, the presence of the spatial dependence parameter allows for the
possibility of partially collapsing the data augmentation algorithms. Based on our simula-
tion study and data analysis, we recommend using the Marginal-Scheme 1 algorithm when
ﬁtting the spatial probit regression model. While the differences in the sample autocorre-
lations among the algorithms may not appear to be dramatic, we return to our discussion
at the beginning of Chapter 3 of the recent emphasis in the literature on methodology for
massive spatial data – improved mixing of MCMC algorithms can lead to signiﬁcant gains
in computational efﬁciency, which can be particularly important in analyses of large data
sets.
In Chapter 3, our investigations of efﬁcient computational strategies for ﬁtting the spa-
tial probit regression model focused exclusively on the special case where the outcome
variable is binary. We proposed an extension of the Marginal-Scheme 1 algorithm for
spatially-referenced multi-categorical outcomes in Chapter 5, however, we left implemen-
tation of this algorithm to future work. Additionally, data augmentation MCMC strategies
also need to be developed for spatially-referenced multivariate or ordinal response vari-
ables. For example, Higgs and Hoeting (2010) recently proposed a clipped latent variable
model for spatially-dependent ordered categorical data, an extension of the model origi-
nally introduced by De Oliveira (2000). To facilitate sampling the latent variables in mod-
els for non-spatially-referenced ordered categorical data, Hans et al. (2009) propose using a
covariance decomposition technique. Future work will explore data augmentation MCMC
strategies and algorithms using this covariance decomposition technique for ﬁtting models
for spatially-referenced multivariate and ordered categorical data.
135

In Chapter 4 we showed how a spatial classiﬁer can be derived from the Bayesian spatial
probit regression model. In an analysis of the Southeast Asia land cover data, this spatial
classiﬁer was found to be a more accurate predictor of unobserved spatially-dependent
binary data than other popular classiﬁcation techniques. Furthermore, including spatial de-
pendence in the residual dependence structure of a generalized linear model is more effec-
tive in terms of prediction/classiﬁcation than including spatial dependence in the covariate
or predictor space, at least in this example.
In this work, we only considered a spatial classiﬁer for two classes. We could also
consider a similar spatial classiﬁer for multi-category response variables based on the spa-
tial multinomial probit regression model proposed in Chapter 5 and compare it to popular
classiﬁcation techniques in the multi-class setting.
Finally, in Chapter 5, we extended the spatial probit regression model to the multi-
category case. We considered how the mean and covariance speciﬁcation for all ℓcategories
will impact the speciﬁcation and interpretation of the model when we use the ℓth category
as a baseline, i.e., model the ﬁrst ℓ−1 categories relative to the ℓth category. We showed
how it was important to consider the type of covariate information when specifying the la-
tent mean structure and provided speciﬁcations of the design matrix which would provide
interpretable and identiﬁable coefﬁcients. We also provided potential ways for modeling
the latent variable dependence structure, but more work needs to be done in considering
non-separable space-category dependence structures as well as the interpretation of result-
ing covariances. Furthermore, we provided a data augmentation algorithm for ﬁtting the
Bayesian multinomial probit regression model when the space-category dependence is sep-
arable.
136

Suppose that after we ﬁt the model using the ℓth category as the baseline category, we
want to instead consider the model using the ﬁrst category as the baseline category. Future
work is needed to determine whether or not we can go between baseline categories and
estimate the associated parameters of the model with one category as the baseline using the
ﬁtted model with a different category as the baseline.
One area of future work applicable to all areas of this dissertation is to create an R pack-
age for the Bayesian spatial multinomial probit regression model. In addition to providing
an accessible way to ﬁt the Bayesian spatial probit regression model and to use this model
for prediction/classiﬁcation, this package would also provide functions that accommodate
the special features required for speciﬁcation of the Bayesian spatial multinomial probit
regression model. Speciﬁcally, this package would enable creation of an appropriate de-
sign matrix for different types of covariate information, and upon further development of
spatial-categorical dependence structures, provide a set-up for the speciﬁcation of various
dependence structures and ﬁt the model using the appropriate data augmentation algorithm.
The ﬁnal area of future work that we mention here is to extend the Bayesian spatial pro-
bit regression model to the space-time setting. The motivating example of this dissertation
research was the analysis of land-cover/land-use data. For example, researchers may want
to determine patterns of land-cover change over time (e.g., deforestation) and what factors
may have contributed to an observed change. The Bayesian spatial probit regression model
is a natural starting point for modeling spatial-temporal categorical response variables.
137

BIBLIOGRAPHY
Abramowitz, M. and Stegun, I. A. (1965). Handbook of Mathematical Functions. New
York, NY: Dover.
Albert, J. H. and Chib, S. (1993). “Bayesian analysis of binary and polychotomous re-
sponse data.” Journal of the American Statistical Association, 88, 669–679.
Albert, P. S. and McShane, L. M. (1995). “A generalized estimating equations approach
for spatially correlated binary data: Applications to the analysis of neuroimaging data.”
Biometrics, 51, 627–638.
Augustin, N. H., Mugglestone, M. A., and Buckland, S. T. (1996). “An autologistic model
for the spatial distribution of wildlife.” Journal of Applied Ecology, 33, 339–347.
Banerjee, S., Carlin, B., and Gelfand, A. (2004). Hierarhical Modeling and Analysis for
Spatial Data. Boca Raton, FL: Chapman & Hall/CRC.
Banerjee, S., Gelfand, A. E., Finley, A. O., and Sang, H. (2008). “Gaussian predictive
process models for large spatial data sets.”
Journal of the Royal Statistical Society,
Series B, 70, 825–848.
Besag, J. E. (1972). “Nearest-neighbour systems and the auto-logistic model for binary
data.” Journal of the Royal Statistical Society, Series B, 34, 75–83.
138

— (1974). “Spatial interaction and the statistical analysis of lattice systems.” Journal of
the Royal Statistical Society, Series B, 36, 2, 192–236.
Breslow, N. E. and Clayton, D. G. (1993). “Approximate inference in generalized linear
mixed models.” Journal of the American Statistical Association, 88, 9–25.
Calder, C. A. (2007). “Dynamic factor process convolution models for multivariate space-
time data with application to air quality assessment.” Environmental and Ecological
Statistics, 14, 229–247.
Carlin, B. P. and Banerjee, S. (2003). “Hierarchical multivariate CAR models for spatio-
temporally correlated survival data (with discussion).” Bayesian Statistics 7, eds. J. M.
Bernardo, M. J. Bayarri, J. O. Berger, A. P. Dawid, D. Heckerman, A. F. M. Smith, and
M. West, 45–63. Oxford: Oxford University Press.
Chib, S. and Greenberg, E. (1998). “Analysis of multivarite probit models.” Biometrika,
85, 347–361.
Christensen, O. F., Møller, J., and Waagepetersen, R. P. (2001). “Geometric ergodicity of
Metropolis Hastings algorithms for conditional simulation in generalised linear mixed
models.” Methodology and Computing in Applied Probability, 3, 309–327.
Christensen, O. F. and Ribeiro Jr., P. J. (2002). “geoRglm: A package for generalised linear
spatial models.” R-NEWS, 2, 2, 26–28.
Christensen, O. F., Roberts, G. O., and Sk¨old, M. (2006). “Robust Markov chain Monte
Carlo methods for spatial generalized linear mixed models.” Journal of Computational
and Graphical Statistics, 15, 1–17.
139

Christensen, O. F. and Waagepetersen, R. P. (2002). “Bayesian prediction of spatial count
data using generalized linear mixed models.” Biometrics, 58, 280–286.
Cortes, C. and Vapnik, V. (1995). “Support-vector network.” Machine Learning, 20, 273–
297.
Cressie, N. (1993). Statistics for Spatial Data. Revised ed. New York: John Wiley.
Daganzo, C. (1980). Multinomial Probit. New York, NY: Academic Press.
De Oliveira, V. (2000). “Bayesian prediction of clipped Gaussian random ﬁelds.” Compu-
tational Statistics and Data Analysis, 34, 299–314.
Diggle, P. J. and Ribeiro Jr., P. J. (2007). Model-based Geostatistics. New York, New York:
Springer.
Diggle, P. J., Tawn, J. A., and Moyeed, R. A. (1998). “Model-based geostatistics.” Applied
Statistician, 47, 299–350.
Dimitriadou, E., Hornik, K., Leisch, F., Meyer, D., and Weingessel, A. (2010). e1071:
Misc Functions of the Department of Statistics (e1071), TU Wien. R package version
1.5-24.
Gelfand, A., Schmidt, A., Banerjee, S., and Sirmans, C. (2004). “Nonstationary multivari-
ate process modeling through spatially varying coregionalization.” Test, 13, 2. 263-312.
Gelfand, A. E. and Vounatsou, P. (2003). “Proper multivariate conditional autoregressive
models for spatial data analysis.” Biostatistics, 4, 11–25.
Gelman, A., Carlin, J., Stern, H., and Rubin, D. (1995). Bayesian Data Analysis. Chapman
and Hall.
140

Geweke, J. (1991).
“Efﬁcient simulation from the multivariate normal and Student t-
distributions subject to linear constraints and the evaluation of constraint probabilities.”
Computer Sciences and Statistic, 23, 571–578.
Gotway, C. A. and Stroup, W. W. (1995). “A generalized linear model approach to spatial
data analysis and prediction.” Journal of Agricultural, Biological, and Environmental
Statistics, 2, 157–178.
Gumpertz, M. L., Graham, J. M., and Ristaino, J. B. (1974). “Autologistic model of spatial
pattern of phytophthora epidemic in bell pepper: effects of soil variables on disease
presence.” Journal of Agricultural, Biological, and Environmental Statistics, 2, 131–
156.
Hans, C., Allenby, G. M., Craigmile, P. F., Lee, J. H., MacEachern, S. N., and Xu, X.
(2009). “Covariance decompositions for accurate computation in Bayesian scale-usage
models.”
Tech. rep., No. 825, Department of Statistics, The Ohio State University,
Columbus, OH, 43210.
Hans, C. and Craigmile, P. F. (2009). truncatedNormals: R functions for truncated normal
distributions. R package version 0.4.
Hastie, T., Tibshirani, R., and Friedman, J. (2001). The Elements of Statistical Learning:
Data Mining, Inference, and Prediction. New York, NY: Springer.
Hausman, J. and Wise, D. (1978). “A conditional probit model for qualitative choice: Dis-
crete decisions recognizing interdependence and heterogeneous preferences.” Econo-
metrics, 46, 403–426.
141

Heagerty, P. J. and Lele, S. R. (1998). “A composite likelihood approach to binary spatial
data.” Journal of the American Statistical Association, 93, 1099–1111.
Higdon, D. (2002).
“Space and space-time modeling using process convolutions.”
In
Quantitative Methods for Current Environmental Issues, eds. C. Anderson, V. Barnett,
P. C. Chatwin, and A. H. El-Shaarawi, 37–56. Springer Verlag.
Higgs, M. D. and Hoeting, J. A. (2010). “A clipped latent-variable model for spatially
correlated ordered categorical data.” Computational Statistics and Data Analysis, 54,
1999–2011.
Huffer, F. W. and Wu, H. (1998). “Markov chain Monte Carlo for autologistic regression
models with application to the distribution of plant species.” Biometrics, 54, 509–524.
Imai, K. and van Dyk, D. A. (2005). “A Bayesian analysis of the multinomial probit model
using marginal data augmentation.” Journal of Econometrics, 124, 311–334.
Journel, A. G. (1983). “Nonparametric estimation of spatial distributions.” Mathematical
Geology, 15, 445–468.
Law, J. and Haining, R. (2004). “A Bayesian approach to modeling binary data: the case
of high-intensity crime areas.” Geographical Analysis, 36, 197–216.
Liang, K. Y. and Zeger, S. L. (1986). “Longitudinal Data Analysis Using Generalized
Linear Models.” Biometrika, 73, 13–22.
Lin, P. S. (2008). “Efﬁciency of quasi-likelihood estimation for spatially correlated binar
data on Lp spaces.” Journal of Statistical Planning and Inference, 138, 1528–1541.
142

Lin, P. S. and Clayton, M. K. (2005). “Analysis of binary spatial data by quasi-likelihood
estimating equations.” The Annals of Statistics, 33, 542–555.
Liu, X. and Daniels, M. J. (2006). “A new algorithm for simulating a correlation matrix
based on parameter expansion and reparameterization.” Journal of Computational and
Graphical Statistics, 15, 897–914.
Lunn, D. J., Thomas, A., Best, N., and Spiegelhalter, D. (2000). “WinBUGS – a Bayesian
modelling framework: concepts, structure, and extensibility.” Statistics and Computing,
10, 325–337.
Mardia, K. V. (1988). “Multi-dimensional multivariate Gaussian Markov random ﬁelds
with application to image processing.” Journal of Multivariate Analysis, 24, 265–284.
McCulloch, R. and Rossi, P. E. (1994). “An exact likelihood analysis of the multinomial
probit model.” Journal of Econometrics, 64, 207–240.
McCulloch, R. E., Polson, N. G., and Rossi, P. E. (2000). “A Bayesian analysis of the
multinomial probit model with fully identiﬁed parameters.” Journal of Econometrics,
99, 173–193.
Meng, X. L. and van Dyk, D. A. (1999). “Seeking efﬁcient data augmentation schemes via
conditional and marginal augmentation.” Biometrika, 86, 301–320.
Munroe, D. K., Wolﬁnbarger, S. R., Calder, C. A., Shi, T., Xiao, N., Lamb, C. Q., and
Li, D. (2008). “The relationships between biomass burning, land-cover/-use change,
and the distribution of carbonaceous aerosols in mainland Southeast Asia: a review and
synthesis.” Journal of Land Use Science, 3, 161–183.
143

Nelder, J. A. and Wedderburn, R. W. A. (1972). “Generalized linear models.” Journal of
the Royal Statistical Society, Series A, 135, 370–384.
Nobile, A. (1998). “A hybrid Markov chain for the Bayesian analysis of the multinomial
probit model.” Statistics and Computing, 8, 229–242.
Paciorek, C. J. (2007). “Computational techniques for spatial logistic regression with large
data sets.” Computational Statistics and Data Analysis, 51, 3631–3653.
Patterson, H. and Thompson, R. (1971). “Recovery of Inter-block Information When Block
Sizes are Unequal.” Biometrika.
Royle, J. A. and Berliner, L. M. (1999). “A hierarchical approach to multivariate spa-
tial modeling and prediction.” Journal of Agricultural, Biological, and Environmental
Statistics, 4, 29–56.
Rue, H., Martino, S., and Chopin, N. (2009). “Approximate Bayesian inference for latent
Gaussian models by using integrated nested Laplace approximations.” Journal of the
Royal Statistical Society, Series B, 71, 1–35.
Sain, S. R. and Cressie, N. (2007). “A spatial model for multivariate lattice data.” Journal
of Econometrics, 140, 226–259.
Schabenberger, O. and Gotway, C. A. (2005). Statistical Methods for Spatial Data Analysis.
Boca Raton, Florida: Chapman & Hall/CRC.
Shaby, B. and Ruppert, D. (2010). “Tapered covariance: Bayesian estimation and asymp-
totics.” Journal of the American Statistical Association, Submitted.
144

Sherman, M., Apanasovich, T. V., and Carroll, R. J. (2006). “On estimation in binary
autologistic spatial models.”
Journal of Statistical Computation and Simulation, 76,
167–179.
Sim, S. (2000). “A test for spatial correlation for binary data.” Statistics & Probability
Letters, 47, 129–134.
Solow, A. R. (1993). “On the efﬁciency of the indicator approach in geostatistics.” Mathe-
matical Geology, 25, 53–57.
Stein, M. (1999). Interpolation of Spatial Data: Some Theory for Kriging. New York:
Springer-Verlag.
Switzer, P. (1977). “Estimation of spatial distributions from point sources with application
to air pollution measurement.” Bulletin de l’Institute International de Statistique, 47,
123–137.
van Dyk, D. A. and Meng, X. L. (2001). “The art of data augmentation.” Journal of
Computational and Graphical Statistics, 10, 1–50.
van Dyk, D. A. and Park, T. (2008).
“Partially collapsed Gibbs samplers: theory and
methods.” Journal of the American Statistical Association, 103, 790–796.
ˇSaltyt˙e Benth, J. and Duˇcinskas, K. (2005). “Linear discriminant analysis of multivariate
spatial-temporal regressions.” Scandinavian Journal of Statistics, 32, 281–294.
Wackernagel, H. (1998). Multivariate Geostatistics: An Introduction with Applications.
2nd ed. New York, NY: Springer-Verlag.
145

Waller, L. A. and Gotway, C. A. (2004). Applied Spatial Statistics for Public Health Data.
Hoboken, New Jersey: John Wiley & Sons, Inc.
Wedderburn, R. W. M. (1974). “Quasi-likelihood functions, generalized linear models, and
the Gauss-Newton method.” Biometrika, 61, 439–447.
Weir, I. S. and Pettitt, A. N. (1999). “Spatial modelling for binary data using a hidden con-
ditional autoregressive Gaussian process: a multivariate extension of the probit model.”
Statistics and Computing, 9, 77–86.
— (2000). “Binary probability maps using hidden conditional autoregressive Gaussian
processes with an application to Finnish common toad data.” Applied Statistics, 49,
473–484.
Xu, K., Wikle, C., and Fox, N. (2005). “A kernel-based spatio-temporal dynamical model
for nowcasting radar precipitation.” Journal of the American Statistical Association, 100,
1133–1144.
Yin, G. (2009). “Bayesian generalized method of moments.” Bayesian Analysis, 4, 191–
208.
Zheng, Y. and Zhu, J. (2008). “Markov chain Monte Carlo for a spatial-temporal autologis-
tic regression model.” Journal of Computational and Graphical Statistics, 17, 123–137.
Zhu, J., Zheng, Y., Carroll, A. L., and Aukema, B. H. (2008). “Autologistic regression
analysis of spatial-temporal binary data via Monte Carlo maximum likelihood.” Journal
of Agricultural, Biological, and Environmental Statistics, 13, 84–98.
146

