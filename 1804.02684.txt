Learning-based Video Motion MagniÔ¨Åcation
Tae-Hyun Oh1‚ãÜ, Ronnachai Jaroensri1‚ãÜ, Changil Kim1, Mohamed Elgharib2,
Fr¬¥edo Durand1, William T. Freeman1,3, and Wojciech Matusik1
1 MIT CSAIL, Cambridge, MA, USA
2 HBKU QCRI, Doha, Qatar
3 Google Research
{taehyun, tiam}@csail.mit.edu
Abstract. Video motion magniÔ¨Åcation techniques allow us to see small
motions previously invisible to the naked eyes, such as those of vibrating
airplane wings, or swaying buildings under the inÔ¨Çuence of the wind.
Because the motion is small, the magniÔ¨Åcation results are prone to noise
or excessive blurring. The state of the art relies on hand-designed Ô¨Ålters
to extract representations that may not be optimal. In this paper, we seek
to learn the Ô¨Ålters directly from examples using deep convolutional neural
networks. To make training tractable, we carefully design a synthetic
dataset that captures small motion well, and use two-frame input for
training. We show that the learned Ô¨Ålters achieve high-quality results
on real videos, with less ringing artifacts and better noise characteristics
than previous methods. While our model is not trained with temporal
Ô¨Ålters, we found that the temporal Ô¨Ålters can be used with our extracted
representations up to a moderate magniÔ¨Åcation, enabling a frequency-
based motion selection. Finally, we analyze the learned Ô¨Ålters and show
that they behave similarly to the derivative Ô¨Ålters used in previous works.
Our code, trained model, and datasets will be available online.
Keywords: Motion manipulation ¬∑ motion magniÔ¨Åcation, deep convo-
lutional neural network
1
Introduction
The ability to discern small motions enables important applications such as un-
derstanding a building‚Äôs structural health [3] and measuring a person‚Äôs vital
sign [1]. Video motion magniÔ¨Åcation techniques allow us to perceive such mo-
tions. This is a diÔ¨Écult task, because the motions are so small that they can be
indistinguishable from noise. As a result, current video magniÔ¨Åcation techniques
suÔ¨Äer from noisy outputs and excessive blurring, especially when the magniÔ¨Åca-
tion factor is large [26, 30, 27, 31].
Current video magniÔ¨Åcation techniques typically decompose video frames
into representations that allow them to magnify motion [26, 30, 27, 31]. Their de-
composition typically relies on hand-designed Ô¨Ålters, such as the complex steer-
able Ô¨Ålters [6], which may not be optimal. In this paper, we seek to learn the
‚ãÜThese authors contributed equally.
arXiv:1804.02684v3  [cs.CV]  1 Aug 2018

2
Authors Suppressed Due to Excessive Length
Original frame
Ours
[Wadhwa et al. 2013]
ùíô
ùíï
ùíô
ùíï
[Zhang et al. 2017]
Original frame
Ours
Fig. 1. While our model learns spatial decomposition Ô¨Ålters from synthetically gener-
ated inputs, it performs well on real videos with results showing less ringing artifacts
and noise. (Left) the crane sequence magniÔ¨Åed 75√ó with the same temporal Ô¨Ålter as
Wadhwa et al. [26]. (Right) Dynamic mode magniÔ¨Åes diÔ¨Äerence (velocity) between con-
secutive frames, allowing us to deal with large motion as did Zhang et al. [31]. The red
lines indicate the sampled regions for drawing x-t and y-t slice views.
decomposition Ô¨Ålter directly from examples using deep convolutional neural net-
works (CNN). Because real motion-magniÔ¨Åed video pairs are diÔ¨Écult to obtain,
we designed a synthetic dataset that realistically simulates small motion. We
carefully interpolate pixel values, and we explicitly model quantization, which
could round away sub-level values that result from subpixel motions. These care-
ful considerations allow us to train a network that generalizes well in real videos.
Motivated by Wadhwa et al. [26], we design a network consisting of three
main parts: the spatial decomposition Ô¨Ålters, the representation manipulator,
and the reconstruction Ô¨Ålters. To make training tractable, we simplify our train-
ing using two-frame input, and the magniÔ¨Åed diÔ¨Äerence as the target instead of
fully specifying temporal aspects of motion. Despite training on the simpliÔ¨Åed
two-frames setting and synthetic data, our network achieves better noise perfor-
mance and has fewer edge artifacts (See Fig. 1). Our result also suggests that
the learned representations support linear operations enough to be used with
linear temporal Ô¨Ålters up to a moderate magniÔ¨Åcation factor. This enables us to
select motion based on frequency bands of interest.
Finally, we visualize the learned Ô¨Ålters and the activations to have a better un-
derstanding of what the network has learned. While the Ô¨Ålter weights themselves
show no apparent pattern, a linear approximation of our learned (non-linear) Ô¨Ål-
ters resembles derivative Ô¨Ålters, which are the basis for decomposition Ô¨Ålters in
the prior art [30, 26].
The main contributions of this paper are as follows:
‚Äì We present the Ô¨Årst learning-based approach for the video motion magniÔ¨Åca-
tion, which achieves high-quality magniÔ¨Åcation with fewer ringing artifacts,
and has better noise characteristics.
‚Äì We present a synthetic data generation method that captures small motions,
allowing the learned Ô¨Ålters to generalize well in real videos.
‚Äì We analyze our model, and show that our learned Ô¨Ålters exhibit similarity
to the previously hand-engineered Ô¨Ålters.
We will release the codes, the trained model, and the dataset online.

Learning-based Video Motion MagniÔ¨Åcation
3
Method
Liu et al. [14]
Wu et al. [30]
Wadhwa et
al. [26]
Wadhwa et
al. [27]
Zhang et al. [31]
Ours
Spatial decom-
position
Tracking, optical
Ô¨Çow
Laplacian pyra-
mid
Steerable Ô¨Ålters
Riesz pyramid
Steerable Ô¨Ålters
Deep convolu-
tion layers
Motion isola-
tion
-
Temporal band-
pass Ô¨Ålter
Temporal bandpass
Ô¨Ålter
Temporal bandpass
Ô¨Ålter
Temporal bandpass
Ô¨Ålter (2nd-order
derivative)
Subtraction or
temporal band-
pass Ô¨Ålter
Representa-
tion denoising
Expectation-
Maximization
-
Amplitude weighted
Gaussian Ô¨Åltering
Amplitude weighted
Gaussian Ô¨Åltering
Amplitude weighted
Gaussian Ô¨Åltering
Trainable convo-
lution
Table 1. Comparisons of the prior arts.
2
Related Work
Video motion magniÔ¨Åcation. Motion magniÔ¨Åcation techniques can be di-
vided into two categories: Lagrangian and Eulerian approaches. The Lagrangian
approach explicitly extracts the motion Ô¨Åeld (optical Ô¨Çow) and uses it to move
the pixels directly [14]. The Eulerian approaches [30, 26, 27], on the other hand,
decompose video frames into representations that facilitate manipulation of mo-
tions, without requiring explicit tracking. These techniques usually consist of
three stages: decomposing frames into an alternative representation, manipu-
lating the representation, and reconstructing the manipulated representation to
magniÔ¨Åed frames. Wu et al. [30] use a spatial decomposition motivated by the
Ô¨Årst-order Taylor expansion, while Wadhwa et al. [26, 27] use the complex steer-
able pyramid [6] to extract a phase-based representation. Current Eulerian tech-
niques are good at revealing subtle motions, but they are hand-designed [30, 26,
27], and do not take into account many issues such as occlusion. Because of this,
they are prone to noise and often suÔ¨Äer from excessive blurring. Our technique
belongs to the Eulerian approach, but our decomposition is directly learned from
examples, so it has fewer edge artifacts and better noise characteristics.
One key component of the previous motion magniÔ¨Åcation techniques is the
multi-frame temporal Ô¨Åltering over the representations, which helps to isolate
motions of interest and to prevent noise from being magniÔ¨Åed. Wu et al. [30]
and Wadhwa et al. [26, 27] utilize standard frequency bandpass Ô¨Ålters. Their
methods achieve high-quality results, but suÔ¨Äer from degraded quality when
large motions or drifts occur in the input video. Elgharib et al. [4] and Zhang et
al. [31] address this limitation. Elgharib et al. [4] model large motions using
aÔ¨Éne transformation, while Zhang et al. [31] use a diÔ¨Äerent temporal processing
equivalent to a second-order derivative (i.e., acceleration). On the other hand,
our method achieves comparable quality even without using temporal Ô¨Åltering.
The comparisons of our method to the prior arts are summarized in Table 1.
Deep representation for video synthesis. Frame interpolation can be viewed
as a complementary problem to the motion magniÔ¨Åcation problem, where the
magniÔ¨Åcation factor is less than 1. Recent techniques demonstrate high-quality
results by explicitly shifting pixels using either optical Ô¨Çow [10, 28, 15] or pixel-
shifting convolution kernels [18, 19]. However, these techniques usually require
re-training when changing the manipulation factor. Our representation can be
directly conÔ¨Ågured for diÔ¨Äerent magniÔ¨Åcation factors without re-training. For

4
Authors Suppressed Due to Excessive Length
frame extrapolation, there is a line of recent work [17, 24, 25] that directly syn-
thesizes RGB pixel values to predict dynamic video frames in the future, but
their results are often blurry. Our work focusing on magnifying motion within a
video, without concerns about what happens in the future.
3
Learning-based Motion MagniÔ¨Åcation
In this section, we introduce the motion magniÔ¨Åcation problem and our learning
setup. Then, we explain how we simplify the learning to make it tractable. Fi-
nally, we describe the network architecture and give the full detail of our dataset
generation.
3.1
Problem statement
We follow Wu et al.‚Äôs and Wadhwa et al.‚Äôs deÔ¨Ånition of motion magniÔ¨Åcation
[30, 26]. Namely, given an image I(x, t) = f(x+Œ¥(x, t)), where Œ¥(x, t) represents
the motion Ô¨Åeld as a function of position x and time t, the goal of motion
magniÔ¨Åcation is to magnify the motion such that the magniÔ¨Åed image ÀúI becomes
ÀúI(x, t) = f(x + (1 + Œ±)Œ¥(x, t)),
(1)
where Œ± is the magniÔ¨Åcation factor. In practice, we only want to magnify certain
signal ÀúŒ¥(x, t) = T (Œ¥(x, t)) for a selector T (¬∑) that selects motion of interest, which
is typically a temporal bandpass Ô¨Ålter [26, 30].
While previous techniques rely on hand-crafted Ô¨Ålters [26, 30], our goal is
to learn a set of Ô¨Ålters that extracts and manipulates representations of the
motion signal Œ¥(x, t) to generate output magniÔ¨Åed frames. To simplify our train-
ing, we consider a simple two-frames input case. SpeciÔ¨Åcally, we generate two
input frames, Xa and Xb with a small motion displacement, and an output
motion-magniÔ¨Åed frame Y of Xb with respect to Xa. This reduces parameters
characterizing each training pair to just the magniÔ¨Åcation factor. While this sim-
pliÔ¨Åed setting loses the temporal aspect of motion, we will show that the network
learns a linear enough representation w.r.t. the displacement to be compatible
with linear temporal Ô¨Ålters up to a moderate magniÔ¨Åcation factor.
3.2
Deep Convolutional Neural Network Architecture
Similar to Wadhwa et al. [26], our goal is to design a network that extracts a
representation, which we can use to manipulate motion simply by multiplication
and to reconstruct a magniÔ¨Åed frame. Therefore, our network consists of three
parts: the encoder Ge(¬∑), the manipulator Gm(¬∑), and the decoder Gd(¬∑), as illus-
trated in Fig. 2. The encoder acts as a spatial decomposition Ô¨Ålter that extracts
a shape representation [9] from a single frame, which we can use to manipulate
motion (analogous to the phase of the steerable pyramid and Riesz pyramid [26,
27]). The manipulator takes this representation and manipulates it to magnify

Learning-based Video Motion MagniÔ¨Åcation
5
Magnification 
factor
: Residual Block
: Conv layers
: Non-trainable layers
ReLu
Conv
Conv
Conv32_k3s2-ReLu
Res. Blk.
Conv16_k7s1 - ReLu
[‚Ñé, ùë§ùë§, 3]
[‚Ñé, ùë§ùë§, 16]
[‚Ñé/2, ùë§ùë§/2,32]
ReLu
Conv32_k3s2
[‚Ñé/4, ùë§ùë§/4,32]
Res. Blk.
Res. Blk.
ReLu
Conv32_k3s1
[‚Ñé/2, ùë§ùë§/2,32]
Res. Blk.
Res. Blk.
Encoder
Texture 
repr.
Shape 
repr.
Input
Decoder
Upsample
Res. Blk.
Res. Blk.
Concat.
[‚Ñé/2, ùë§ùë§/2,32]
[‚Ñé/2, ùë§ùë§/2,64]
Upsample
[‚Ñé, ùë§ùë§, 64]
Conv32_k3s1-ReLu
[‚Ñé, ùë§ùë§, 32]
Conv3_k7s1
[‚Ñé, ùë§ùë§, 3]
9 Res. Blks.
Texture 
repr.
Shape 
repr.
Output
[‚Ñé/2, ùë§ùë§/2,32]
Res. Blk.
ReLu
Conv32_k3s1
Conv32_k3s1
[‚Ñé/2, ùë§ùë§/2,32]
ùú∂ùú∂
Shape 
repr.
ùë¥ùë¥ùíÉùíÉ
Shape 
repr.
ùë¥ùë¥ùíÇùíÇ
Manipulator
ùëøùëøùëéùëé
ùëøùëøùëèùëè
Magnified 
Frame
‡∑°ùíÄùíÄ
Manipulator
Decoder
Texture 
repr.
Shape 
repr.
Shared
ùú∂ùú∂
Encoder
Texture 
repr.
Shape 
repr.
Encoder
Texture 
repr.
Shape 
repr.
Overview of architecture
Input Frames
Res. Blk.
Res. Blk.
g(‚Ä¢)
h(‚Ä¢)
(a)
(b)
Fig. 2. Our network architecture. (a) Overview of the architecture. Our network
consists of 3 main parts: the encoder, the manipulator, and the decoder. During train-
ing, the inputs to the network are two video frames, (Xa, Xb), with a magniÔ¨Åcation
factor Œ±, and the output is the magniÔ¨Åed frame ÀÜY. (b) Detailed diagram for each part.
Conv‚ü®c‚ü©k‚ü®k‚ü©s‚ü®s‚ü©denotes a convolutional layer of c channels, k √ó k kernel size, and
stride s.
the motion (by multiplying the diÔ¨Äerence). Finally, the decoder reconstructs the
modiÔ¨Åed representation into the resulting motion-magniÔ¨Åed frames.
Our encoder and decoder are fully convolutional, which enables them to
work on any resolution [16]. They use residual blocks to generate high-quality
output [23]. To reduce memory footprint and increase the receptive Ô¨Åeld size, we
downsample the activation by 2√ó at the beginning of the encoder, and upsample
it at the end of the decoder. We downsample with the strided convolution [22],
and we use nearest-neighbor upsampling followed by a convolution layer to avoid
checkerboard artifacts [20]. We experimentally found that three 3 √ó 3 residual
blocks in the encoder and nine in the decoder generally yield good results.
While Eq. (1) suggests no intensity change (constant f(¬∑)), this is not true in
general. This causes our network to also magnify intensity changes. To cope with
this, we introduce another output from the encoder that represents intensity in-
formation (‚Äútexture representation‚Äù [9]) similar to the amplitude of the steerable
pyramid decomposition. This representation reduces undesired intensity magni-
Ô¨Åcation as well as noise in the Ô¨Ånal output. We downsample the representation
2√ó further because it helps reduce noise. We denote the texture and shape rep-
resentation outputs of the encoder as V = Ge,texture(X) and M = Ge,shape(X),
respectively. During training, we add a regularization loss to separate these two
representations, which we will discuss in more detail later.
We want to learn a shape representation M that is linear with respect to
Œ¥(x, t). So, our manipulator works by taking the diÔ¨Äerence between shape repre-
sentations of two given frames, and directly multiplying a magniÔ¨Åcation factor
to it. That is,
Gm(Ma, Mb, Œ±) = Ma + Œ±(Mb‚àíMa).
(2)

6
Authors Suppressed Due to Excessive Length
Linear
Non-Linear
Fig. 3. Comparison between linear and non-linear manipulators. While the
two manipulators are able to magnify motion, the linear manipulator (left) does blur
strong edges (top) sometimes, and is more prone to noise (bottom). Non-linearity in
the manipulator reduces this problem (right).
In practice, we found that some non-linearity in the manipulator improves
the quality of the result (See Fig. 3). Namely,
Gm(Ma, Mb, Œ±) = Ma + h (Œ± ¬∑ g(Mb ‚àíMa)) ,
(3)
where g(¬∑) is represented by a 3 √ó 3 convolution followed by ReLU, and h(¬∑) is a
3 √ó 3 convolution followed by a 3 √ó 3 residual block.
Loss function. We train the whole network in an end-to-end manner. We use
l1-loss between the network output ÀÜY and the ground-truth magniÔ¨Åed frame Y.
We found no noticeable diÔ¨Äerence in quality when using more advanced losses,
such as the perceptual [8] or the adversarial losses [7]. In order to drive the
separation of the texture and the shape representations, we perturbed the in-
tensity of some frames, and expect the texture representations of perturbed
frames to be the same, while their shape representation remain unchanged.
SpeciÔ¨Åcally, we create perturbed frames X‚Ä≤
b and Y‚Ä≤, where the prime symbol
indicates color perturbation. Then, we impose loses between V‚Ä≤
b and V‚Ä≤
Y (per-
turbed frames), Va and Vb (un-perturbed frames), and M‚Ä≤
b and Mb (shape
of perturbed frames should remain unchanged). We used l1-loss for all regular-
izations. Therefore, we train the whole network G by minimizing the Ô¨Ånal loss
function L1(Y, ÀÜY)+Œª(L1(Va, Vb)+L1(V‚Ä≤
b, V‚Ä≤
Y )+L1(Mb, M‚Ä≤
b)), where Œª is the
regularization weight (set to 0.1).
Training. We use ADAM [11] with Œ≤1 = 0.9 and Œ≤2 = 0.999 to minimize the loss
with the batch size 4. We set the learning rate to 10‚àí4 with no weight decay.
In order to improve robustness to noise, we add Poisson noise with random
strengths whose standard deviation is up to 3 on a 0‚àí255 scale for a mid-gray
pixel.
Applying 2-frames setting to videos Since there was no temporal concept
during training, our network can be applied as long as the input has two frames.

Learning-based Video Motion MagniÔ¨Åcation
7
We consider two diÔ¨Äerent modes where we use diÔ¨Äerent frames as a reference. The
Static mode uses the 1st frame as an anchor, and the Dynamic uses the previous
frames as a reference, i.e. we consider (Xt‚àí1, Xt) as inputs in the Dynamic mode.
Intuitively, the Static mode follows the classical deÔ¨Ånition of motion mag-
niÔ¨Åcation as deÔ¨Åned in Eq. (1), while the Dynamic mode magniÔ¨Åes the diÔ¨Äerence
(velocity) between consecutive frames. Note that the magniÔ¨Åcation factor in
each case has diÔ¨Äerent meanings, because we are magnifying the motion against
a Ô¨Åxed reference, and the velocity respectively. Because there is no temporal Ô¨Ål-
ter, undesired motion and noise quickly becomes a problem as the magniÔ¨Åcation
factor increases, and achieving high-quality result is more challenging.
Temporal operation. Even though our network has been trained in the 2-
frame setting only, we Ô¨Ånd that the shape representation is linear enough w.r.t.
the displacement to be compatible with linear temporal Ô¨Ålters. Given the shape
representation M(t) of a video (extracted frame-wise), we replace the diÔ¨Äerence
operation with a pixel-wise temporal Ô¨Ålter T (¬∑) across the temporal axis in the
manipulator Gm(¬∑). That is, the temporal Ô¨Åltering version of the manipulator,
Gm,temporal(¬∑), is given by,
Gm,temporal(M(t), Œ±) = M(t) + Œ±T (M(t)).
(4)
The decoder takes the temporally-Ô¨Åltered shape representation and the texture
representation of the current frame, and generates temporally Ô¨Åltered motion
magniÔ¨Åed frames.
3.3
Synthetic Training Dataset
Obtaining real motion magniÔ¨Åed video pairs is challenging. Therefore, we utilize
synthetic data which can be generated in large quantity. However, simulating
small motions involves several considerations because any small error will be
relatively large. Our dataset is carefully designed and we will later show that
the network trained on this data generalizes well to real videos. In this section,
we describe considerations we make in generating our dataset.
Foreground objects and background images. We utilize real image datasets
for their realistic texture. We use 200, 000 images from MS COCO dataset [13] for
background, and we use 7, 000 segmented objects the PASCAL VOC dataset [5]
for the foreground. As the motion is magniÔ¨Åed, Ô¨Ålling the occluded area becomes
important, so we paste our foreground objects directly onto the background
to simulate occlusion eÔ¨Äect. Each training sample contains 7 to 15 foreground
objects, randomly scaled from its original size. We limit the scaling factor at 2 to
avoid blurry texture. The amount and direction of motions of background and
each object are also randomized to ensure that the network learns local motions.
Low contrast texture, global motion, and static scenes. The training ex-
amples described in the previous paragraphs are full of sharp and strong edges
where the foreground and background meet. This causes the network to general-
ize poorly on low contrast textures. To improve generalization in these cases, we

8
Authors Suppressed Due to Excessive Length
add two types of examples: where 1) the background is blurred, and 2) there is
only a moving background in the scene to mimic a large object. These improve
the performance on large and low contrast objects in real videos.
Small motion can be indistinguishable from noise. We Ô¨Ånd that including
static scenes in the dataset helps the network learn changes that are due to noise
only. We add additional two subsets where 1) the scene is completely static, and
2) the background is not moving, but the foreground is moving. With these, our
dataset contains a total of 5 parts, each with 20, 000 samples of 384√ó384 images.
The examples of our dataset can be found in the supplementary material.
Input motion and ampliÔ¨Åcation factor. Motion magniÔ¨Åcation techniques
are designed to magnify small motions at high magniÔ¨Åcations. The task becomes
even harder when the magniÔ¨Åed motion is large (e.g. > 30 pixels). To ensure
the learnability of the task, we carefully parameterize each training example to
make sure it is within a deÔ¨Åned range. SpeciÔ¨Åcally, we limit the magniÔ¨Åcation
factor Œ± up to 100 and sample the input motion (up to 10 pixels), so that the
magniÔ¨Åed motion does not exceed 30 pixels.
Subpixel motion generation. How subpixel motion manifests depends on de-
mosaicking algorithm and camera sensor pattern. Fortunately, even though our
raw images are already demosaicked, they have high enough resolution that they
can be downsampled to avoid artifacts from demosaicking. To ensure proper re-
sampling, we reconstruct our image in the continuous domain before applying
translation or resizing. We Ô¨Ånd that our results are not sensitive to the interpo-
lation method used, so we chose bicubic interpolation for the reconstruction. To
reduce error that results from translating by a small amount, we Ô¨Årst generate
our dataset at a higher resolution (where the motion appears larger), then down-
sample each frame to the desired size. We reduce aliasing when downsampling
by applying a Gaussian Ô¨Ålter whose kernel is 1 pixel in the destination domain.
Subpixel motion appears as small intensity changes that are often below
the 8-bit quantization level. These changes are often rounded away especially
for low contrast region. To cope with this, we add uniform quantization noise
before quantizing the image. This way, each pixel has a chance of rounding up
proportional to its rounding residual (e.g., if a pixel value is 102.3, it will have
30% chance of rounding up).
4
Results and Evaluations
In this section, we demonstrate the eÔ¨Äectiveness of our proposed network and
analyze its intermediate representation to shed light on what it does. We compare
qualitatively and quantitatively with the state-of-the-art [26] and show that our
network performs better in many aspects. Finally, we discuss limitations of our
work. The comparison videos are available in our supplementary material.

Learning-based Video Motion MagniÔ¨Åcation
9
(a) Phase
(b) Ours
(c) Input
(d) Phase
(e) Ours
Fig. 4. Qualitative comparison. (a,b) Baby sequence (20√ó). (c,d,e) Balance se-
quence (8√ó). The phase-based method shows more ringing artifacts and blurring than
ours near edges (left) and occlusion boundaries (right).
Ours with Static Mode
Ours with Temporal Filter
Phase-based with Temporal Filter [26]
Fig. 5. Temporal Ô¨Ålter reduces artifacts. Our method beneÔ¨Åts from applying tem-
poral Ô¨Ålters (middle); blurring artifacts are reduced. Nonetheless, even without tempo-
ral Ô¨Ålters (left), our method still preserves edges better than the phase-based method
(right), which shows severe ringing artifacts.
4.1
Comparison with the State-of-the-Art
In this section, we compare our method with the state of the art. Because the
Riesz pyramid [27] gives similar results as the steerable pyramids [26], we fo-
cus our comparison on the steerable pyramid. We perform both qualitative and
quantitative evaluation as follows. All results in this section were processed with
temporal Ô¨Ålters unless otherwise noted.
Qualitative comparison Our method preserves edges well, and has fewer
ringing artifacts. Fig. 4 shows a comparison of the balance and the baby se-
quences, which are temporally Ô¨Åltered and magniÔ¨Åed 10√ó and 20√ó respectively.
The phase-based method shows signiÔ¨Åcant ringing artifact, while ours is nearly
artifact-free. This is because our representation is trained end-to-end from exam-
ple motion, whereas the phase-based method relies on hand-designed multi-scale
representation, which cannot handle strong edges well.
The eÔ¨Äect of temporal Ô¨Ålters Our method was not trained using tempo-
ral Ô¨Ålters, so using the Ô¨Ålters to select motion may lead to incorrect results.
To test this, we consider the guitar sequence, which shows strings vibrating at

10
Authors Suppressed Due to Excessive Length
Original Frame
Ours (Dynamic Mode)
Zhang et al. [31]
Fig. 6. Applying our network in 2-frame settings. We compare our network
applied in dynamic mode to acceleration magniÔ¨Åcation [31]. Because [31] is based on
the complex steerable pyramid, their result suÔ¨Äers from ringing artifacts and blurring.
diÔ¨Äerent frequencies. Fig. 7 shows the 25√ó magniÔ¨Åcation results on the guitar
sequence using diÔ¨Äerent temporal Ô¨Ålters. The strings were correctly selected by
each temporal Ô¨Ålter, which shows that the temporal Ô¨Ålters work correctly with
our representation.
Temporal processing can improve the quality of our result, because it prevents
our network from magnifying unwanted motion. Fig. 5 shows a comparison on
the drum sequence. The temporal Ô¨Ålter reduces blurring artifacts present when
we magnify using two frames (static mode). However, even without the use of
the temporal Ô¨Ålter, our method still preserves edges well, and show no ringing
artifacts. In contrast, the phase-based method shows signiÔ¨Åcant ringing artifacts
even when the temporal Ô¨Ålter is applied.
Two-frames setting results Applying our network with two-frames input cor-
responds best to its training. We consider magnifying consecutive frames using
our network (dynamic mode), and compare the result with Zhang et al. [31].
Fig. 6 shows the result of gun sequence, where we apply our network in the
dynamic mode without a temporal Ô¨Ålter. As before, our result is nearly artifact
free, while Zhang et al. suÔ¨Äers from ringing artifacts and excessive blurring, be-
cause their method is also based on the complex steerable pyramid [26]. Note
that our magniÔ¨Åcation factor in the dynamic mode may have a diÔ¨Äerent meaning
to that of Zhang et al., but we found that for this particular sequence, using the
same magniÔ¨Åcation factor (8√ó) produces a magniÔ¨Åed motion which has roughly
the same size.
Quantitative Analysis. The strength of motion magniÔ¨Åcation techniques lies
in its ability to visualize sub-pixel motion at high magniÔ¨Åcation factors, while
being resilient to noise. To quantify these strengths and understand the limit
of our method, we quantitatively evaluate our method and compare it with the
phase-based method on various factors. We want to focus on comparing the
representation and not temporal processing, so we generate synthetic examples
whose motion is a single-frequency sinusoid and use a temporal Ô¨Ålter that has

Learning-based Video Motion MagniÔ¨Åcation
11
0
50
100
150
200
250
300
frequency  (Hz)
0
5
10
amplitude | F( ) |
6th (E): 80 Hz
5th (A): 108 Hz
4th (D): 144 Hz
Input
72-92
Hz
100-125
Hz
125-175
Hz
Fig. 7. Temporal Ô¨Åltering at diÔ¨Äerent frequency bands. (Left) Intensity signal
over the pixel on each string. (Right) y-t plot of the result using diÔ¨Äerent temporal
Ô¨Ålters. Our representation is linear enough to be compatible with temporal Ô¨Ålters. The
strings from top to bottom correspond to the 6-th to 4-th strings. Each string vibrates
at diÔ¨Äerent frequencies, which are correctly selected by corresponding temporal Ô¨Ålters.
For visualization purpose, we invert the color of the y ‚àít slices.
(a) Sub-pixel motion performance
(b) Noise performance with small in-
put motion (0.05 px)
(c) Noise performance with large in-
put motion (5 px)
Fig. 8. Quantitative analysis. (a) Subpixel test, our network performs well down to
0.01 pixels, and is consistently better than the phase-based [26]. (b,c) Noise tests at
diÔ¨Äerent levels of input motion. Our network‚Äôs performance stays high and is consis-
tently better than the phase-based whose performance drops to the baseline level as
the noise factor exceeds 1. Our performance in (b) is worse than (c) because the motion
is smaller, which is expected because a smaller motion is harder to be distinguished
from noise.
wide passband.4 Because our network was trained without the temporal Ô¨Ålter,
we test our method without the temporal Ô¨Ålter, but we use temporal Ô¨Ålters with
the phase-based method. We summarize the results in Fig. 8 and its parameter
ranges in the supplementary material.
For the subpixel motion test, we generate synthetic data having foreground
input motion ranging from 0.01 to 1 pixel. We vary the magniÔ¨Åcation factor Œ±
such that the magniÔ¨Åed motion is 10 pixels. No noise was added. Additionally, we
move the background for the same amount of motion but in a diÔ¨Äerent direction
to all foreground objects. This ensures that no method could do well by simply
replicating the background.
4 Our motion is 3Hz at 30 fps, and the temporal Ô¨Ålter used is a 30-tap FIR with a
passband between 0.5 - 7.5Hz.

12
Authors Suppressed Due to Excessive Length
In the noise test, we Ô¨Åxed the amount of input motion and magniÔ¨Åcation
factor and added noise to the input frames. We do not move background in this
case. To simulate photon noise, we create a noise image whose variance equals the
value of each pixel in the original image. A multiplicative noise factor controls
the Ô¨Ånal strength of noise image to be added.
Because the magniÔ¨Åed motion is not very large (10 pixels), the input and the
output magniÔ¨Åed frames could be largely similar. We also calculate the SSIM
between the input and output frames as a baseline reference in addition to the
phase-based method.
In all tests, our method performs better than the phase-based method. As
Fig. 8-(a) shows, our sub-pixel performance remains high all the way down to
0.01 pixels, and it exceeds 1 standard deviation of the phase-based performance
as the motion increase above 0.02 pixels. Interestingly, despite being trained
only up to 100√ó magniÔ¨Åcation, the network performs considerably well at the
smallest input motion (0.01), where magniÔ¨Åcation factor reaches 1, 000√ó. This
suggests that our network are more limited by the amount of output motion it
needs to generate, rather than the magniÔ¨Åcation factors it was given.
Fig. 8-(b,c) show the test results under noisy conditions with diÔ¨Äerent amounts
of input motion. In all cases, the performance of our method is consistently higher
than that of the phase-based method, which quickly drops to the level of the
baseline as the noise factor increase above 1.0. Comparing across diÔ¨Äerent input
motion, our performance degrades faster as the input motion becomes smaller
(See Fig. 8-(b,c)). This is expected because when the motion is small, it becomes
harder to distinguish actual motion from noise. Some video outputs from these
tests are included in the supplementary material.
4.2
Physical Accuracy of Our Method
In nearly all of our real test videos, the resulting motions produced by our
method have similar magnitude as, and are in phase with, the motions produced
by [26] (see Fig. 1, and the supplementary videos). This shows that our method
is at least as physically accurate as the phase-based method, while exhibiting
fewer artifacts.
We also obtained the hammer sequence from the authors of [26], where ac-
celerometer measurement was available. We integrated twice the accelerometer
signal and used a zero-phase high-pass Ô¨Ålter to remove drifts. As Fig. 10 shows,
the resulting signal (blue line) matches up well with our 10√ó magniÔ¨Åed (without
temporal Ô¨Ålter) result, suggesting that our method is physically accurate.
4.3
Visualizing Network Activation
Deep neural networks achieve high performance in a wide variety of vision tasks,
but their inner working is still largely unknown [2]. In this section, we analyze
our network to understand what it does, and show that it extracts relevant infor-
mation to the task. We analyze the response of the encoder, by approximating it

Learning-based Video Motion MagniÔ¨Åcation
13
Gabor-like Ô¨Ålters
Laplacian-like Ô¨Ålters
Corner detector-like Ô¨Ålters
Fig. 9. Approximate shape encoder kernel. We approximate our (non-linear) spa-
tial encoder as linear convolution kernels and show top-8 by approximation error. These
kernels resemble directional edge detector (left), Laplacian operator (middle), and cor-
ner detector-like (right).
Fig. 10. Physical accuracy of our method Comparison between our magniÔ¨Åed
output and the twice-integrated accelerometer measurement (blue line). Our result
and the accelerometer signal match closely.
as a linear system. We pass several test images through the encoder, and calcu-
late the average impulse responses across the images. Fig. 9 shows the samples of
the linear kernel approximation of the encoder‚Äôs shape response. Many of these
responses resemble Gabor Ô¨Ålters and Laplacian Ô¨Ålters, which suggests that our
network learns to extract similar information as done by the complex steerable
Ô¨Ålters [26]. By contrast, the texture kernel responses show many blurring kernels.
4.4
Limitations
While our network performs well in the 2-frame setting, its performance de-
grades with temporal Ô¨Ålters when the magniÔ¨Åcation factor is high and motion is
small. Fig. 11 shows an example frame of temporally-Ô¨Åltered magniÔ¨Åed synthetic
videos with increasing the magniÔ¨Åcation factor. As the magniÔ¨Åcation factor in-
creases, blurring becomes prominent, and strong color artifacts appear as the
magniÔ¨Åcation factor exceeds what the network was trained on.
In some real videos, our method with temporal Ô¨Ålter appears to be blind to
very small motions. This results in patchy magniÔ¨Åcation where some patches
get occasionally magniÔ¨Åed when their motions are large enough for the network
to see. Fig. 12 shows our magniÔ¨Åcation results of the eye sequence compared
to that of the phase-based method [26]. Our magniÔ¨Åcation result shows little
motion, except on a few occasions, while the phase-based method reveals a richer
motion of the iris. We expect to see some artifact on our network running with
temporal Ô¨Ålters, because it was not what it was trained on. However, this limits
its usefulness in cases where the temporal Ô¨Ålter is essential to selecting small

14
Authors Suppressed Due to Excessive Length
Original Frame
20√ó
50√ó
300√ó
Fig. 11. Temporal Ô¨Åltered result at high magniÔ¨Åcation. Our technique works
well with temporal Ô¨Ålter only at lower magniÔ¨Åcation factors. The quality degrades as
the magniÔ¨Åcation factor increases beyond 20√ó.
Input
Ours with temporal filter
Phase-based [26]
Fig. 12. One of our failure cases. Our method is not fully compatible with the
temporal Ô¨Ålter. This eye sequence has a small motion that requires a temporal Ô¨Ålter
to extract. Our method is blind to this motion and produces a relatively still motion,
while the phase-based method is able to reveal it.
motion of interest. Improving compatibility with the temporal Ô¨Ålter will be an
important direction for future work.
5
Conclusion
Current motion magniÔ¨Åcation techniques are based on hand-designed Ô¨Ålters,
and are prone to noise and excessive blurring. We present a new learning-based
motion magniÔ¨Åcation method that seeks to learn the Ô¨Ålters directly from data.
We simplify training by using the two-frames input setting to make it tractable.
We generate a set of carefully designed synthetic data that captures aspects
of small motion well. Despite these simpliÔ¨Åcations, we show that our network
performs well, and has less edge artifact and better noise characteristics than the
state of the arts. Our method is compatible with temporal Ô¨Ålters, and yielded
good results up to a moderate magniÔ¨Åcation factor. Improving compatibility
with temporal Ô¨Ålters so that it works at higher magniÔ¨Åcation is an important
direction for future work.
Acknowledgment. The authors would like to thank Qatar Computing Re-
search Institute and Toyota Research Institute for their generous support of this
project. Changil Kim was supported by a Swiss National Science Foundation
fellowship P2EZP2 168785.

Learning-based Video Motion MagniÔ¨Åcation
15
References
1. Balakrishnan, G., Durand, F., Guttag, J.: Detecting pulse from head motions in
video. In: IEEE Conf. on Comput. Vis. and Pattern Recognit. (2013)
2. Bau, D., Zhou, B., Khosla, A., Oliva, A., Torralba, A.: Network dissection: Quan-
tifying interpretability of deep visual representations. In: IEEE Conf. on Comput.
Vis. and Pattern Recognit. (2017)
3. Cha, Y.J., Chen, J., B¬®uy¬®uk¬®ozt¬®urk, O.: Output-only computer vision based damage
detection using phase-based optical Ô¨Çow and unscented kalman Ô¨Ålters. Engineering
Structures 132, 300‚Äì313 (2017)
4. Elgharib, M.A., Hefeeda, M., Durand, F., Freeman, W.T.: Video magniÔ¨Åcation in
presence of large motions. In: IEEE Conf. on Comput. Vis. and Pattern Recognit.
(2015)
5. Everingham, M., Van Gool, L., Williams, C.K.I., Winn, J., Zisserman, A.: The
pascal visual object classes (voc) challenge. Int. J. of Comput. Vis. 88(2), 303‚Äì338
(Jun 2010)
6. Freeman, W.T., Adelson, E.H.: The design and use of steerable Ô¨Ålters. IEEE Trans.
Pattern Anal. Mach. Intell. 13(9), 891‚Äì906 (1991)
7. Isola, P., Zhu, J.Y., Zhou, T., Efros, A.A.: Image-to-image translation with condi-
tional adversarial networks. In: IEEE Conf. on Comput. Vis. and Pattern Recognit.
(2017)
8. Johnson, J., Alahi, A., Fei-Fei, L.: Perceptual losses for real-time style transfer and
super-resolution. In: Eur. Conf. on Comput. Vis. Springer (2016)
9. Jones, M.J., Poggio, T.: Multidimensional morphable models: A framework for
representing and matching object classes. Int. J. of Comput. Vis. 29(2), 107‚Äì131
(1998)
10. Kalantari, N.K., Wang, T.C., Ramamoorthi, R.: Learning-based view synthesis for
light Ô¨Åeld cameras. ACM Trans. Graph. (SIGGRAPH Asia) 35(6), 193‚Äì10 (2016)
11. Kingma, D.P., Ba, J.: Adam: A method for stochastic optimization. arXiv preprint
arXiv:1412.6980 (2014)
12. Liao, Z., Joshi, N., Hoppe, H.: Automated video looping with progressive dy-
namism. ACM Trans. Graph. (SIGGRAPH) 32(4), 77 (2013)
13. Lin, T.Y., Maire, M., Belongie, S., Hays, J., Perona, P., Ramanan, D., Doll¬¥ar,
P., Zitnick, C.L.: Microsoft coco: Common objects in context. In: Eur. Conf. on
Comput. Vis. Springer (2014)
14. Liu, C., Torralba, A., Freeman, W.T., Durand, F., Adelson, E.H.: Motion magni-
Ô¨Åcation. ACM Trans. Graph. (SIGGRAPH) 24(3), 519‚Äì526 (2005)
15. Liu, Z., Yeh, R.A., Tang, X., Liu, Y., Agarwala, A.: Video Frame Synthesis using
Deep Voxel Flow. In: IEEE Int. Conf. on Comput. Vis. (2017)
16. Long, J., Shelhamer, E., Darrell, T.: Fully convolutional networks for semantic
segmentation. In: IEEE Conf. on Comput. Vis. and Pattern Recognit. (2015)
17. Mathieu, M., Couprie, C., LeCun, Y.: Deep multi-scale video prediction beyond
mean square error. Int. Conf. on Learn. Representations (2016)
18. Niklaus, S., Mai, L., Liu, F.: Video Frame Interpolation via Adaptive Convolution.
IEEE Conf. on Comput. Vis. and Pattern Recognit. (2017)
19. Niklaus, S., Mai, L., Liu, F.: Video Frame Interpolation via Adaptive Separable
Convolution. In: IEEE Int. Conf. on Comput. Vis. (2017)
20. Odena, A., Dumoulin, V., Olah, C.: Deconvolution and checkerboard artifacts.
Distill 1(10), e3 (2016)

16
Authors Suppressed Due to Excessive Length
21. Oh, T.H., Joo, K., Joshi, N., Wang, B., Kweon, I.S., Kang, S.B.: Personalized
cinemagraphs using semantic understanding and collaborative learning. In: IEEE
Int. Conf. on Comput. Vis. (2017)
22. Radford, A., Metz, L., Chintala, S.: Unsupervised representation learning with deep
convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434
(2015)
23. Sajjadi, M.S., Sch¬®olkopf, B., Hirsch, M.: EnhanceNet: Single image super-resolution
through automated texture synthesis. In: IEEE Int. Conf. on Comput. Vis. (2017)
24. Srivastava, N., Mansimov, E., Salakhudinov, R.: Unsupervised learning of video
representations using lstms. In: Int. Conf. on Mach. Learn. (2015)
25. Villegas, R., Yang, J., Hong, S., Lin, X., Lee, H.: Decomposing motion and content
for natural video sequence prediction. In: Int. Conf. on Learn. Representations
(2017)
26. Wadhwa, N., Rubinstein, M., Durand, F., Freeman, W.T.: Phase-based video mo-
tion processing. ACM Trans. Graph. (SIGGRAPH) 32(4), 80 (2013)
27. Wadhwa, N., Rubinstein, M., Durand, F., Freeman, W.T.: Riesz pyramids for fast
phase-based video magniÔ¨Åcation. In: IEEE Int. Conf. on Comput. Photogr. (2014)
28. Wang, T., Zhu, J., Kalantari, N.K., Efros, A.A., Ramamoorthi, R.: Light Ô¨Åeld
video capture using a learning-based hybrid imaging system. ACM Trans. Graph.
(SIGGRAPH) 36(4), 133:1‚Äì133:13 (2017)
29. Wilburn, B., Joshi, N., Vaish, V., Talvala, E.V., Antunez, E., Barth, A., Adams,
A., Horowitz, M., Levoy, M.: High performance imaging using large camera arrays.
ACM Transactions on Graphics (TOG) 24(3), 765‚Äì776 (2005)
30. Wu, H.Y., Rubinstein, M., Shih, E., Guttag, J., Durand, F., Freeman, W.: Eulerian
video magniÔ¨Åcation for revealing subtle changes in the world. ACM Trans. Graph.
(SIGGRAPH) 31(4), 65‚Äì8 (2012)
31. Zhang, Y., Pintea, S.L., van Gemert, J.C.: Video Acceleration MagniÔ¨Åcation. In:
IEEE Conf. on Comput. Vis. and Pattern Recognit. (2017)

Supplementary Material:
Learning-based Video Motion MagniÔ¨Åcation
Tae-Hyun Oh1‚ãÜ, Ronnachai Jaroensri1‚ãÜ, Changil Kim1, Mohamed Elgharib2,
Fr¬¥edo Durand1, William T. Freeman1,3, and Wojciech Matusik1
1 MIT CSAIL, Cambridge, MA, USA
2 HBKU QCRI, Doha, Qatar
3 Google Research
{taehyun, tiam}@csail.mit.edu
Summary of Contents
This is a part of the supplementary material. The contents of this supplemen-
tary material include the additional experiment results with descriptions and
parameter setups, which have not been shown in the main paper due to the
space limit.
The supplementary video4 contains comparisons with other methods [30, 26,
27, 31] and baselines, self-evaluations and other applications.
A.1
Parameters for Example Videos
In Table A.1, we specify parameters used in the experiments of the main paper
such as magniÔ¨Åcation factors, and temporal Ô¨Ålters. The parameters of examples
in the supplementary video are self-contained if speciÔ¨Åed. For FIR Ô¨Ålter, we chose
the number of taps equal to the number of video frames in that sequence, and
apply the Ô¨Ålter in the frequency domain. For all other Ô¨Ålters, we apply them in
the time domain.
A.2
Additional Experiments
This section describes the detail description of the dataset (in Sec. A.2.1), and
presents additional results. We present an example for the characteristics of
Static mode that magniÔ¨Åes broad frequency bands in Sec. A.2.2, qualitative
comparison in Sec. A.2.3, quantitative analysis in Sec. A.2.4, visualization anal-
ysis examples in Sec. A.2.5, descriptions for other applications in Sec. A.2.6 and
the supplementary video content in Sec. A.2.7.
‚ãÜThese authors contributed equally.
4 The supplementary video can be found in https://youtu.be/GrMLeEcSNzY.

18
Authors Suppressed Due to Excessive Length
Sequence
Name
MagniÔ¨Åcation
Factor
Temporal
Band
Sampling Rate
(fps)
Temporal Fil-
ter
crane
75√ó
0.2 ‚àí0.25 Hz
24
FIR
balance
10√ó
1 ‚àí8 Hz
300
2nd order But-
terworth
throat
100√ó
90 ‚àí110 Hz
1900
FIR
baby
20√ó
See Temporal
Filter Column
30
DiÔ¨Äerence of
IIR, same as [27]
tree (high freq
band)
25√ó
1.5 ‚àí2 Hz
60
FIR
tree (low freq
band)
25√ó
0.5 ‚àí1 Hz
60
FIR
camera
75√ó
36 ‚àí62 Hz
300
2nd order But-
terworth
eye
75√ó
30 ‚àí50 Hz
500
FIR
gun
8√ó
N/A
N/A
Dynamic Mode
cat-toy
7√ó
N/A
N/A
Dynamic Mode
drone
10√ó
N/A
N/A
Dynamic Mode
hot-coÔ¨Äee
3√ó
N/A
N/A
Dynamic Mode
drum
10√ó
74 ‚àí78 Hz
1900
FIR
drum
10√ó
N/A
N/A
Static Mode
guitar
25√ó
72 ‚àí92 Hz
600
2nd order But-
terworth
guitar
10√ó
N/A
N/A
Static Mode
Table A.1. Parameters of our results
(a) Foreground and
background objects
(b) Background only
(c) Blurry background
Fig. A.1. Sample data. Example frames from our dataset. (a) Our data consists of
pasted foreground objects using segmentation from PASCAL VOC [5] and background
from MS COCO dataset [13]. (b) To ensure our network learns global motion, the
second part of our dataset only has background moving. (c) To ensure low contrast
texture is well-represented, we include data with the background blurred. We add
another two parts with the same speciÔ¨Åcation as (b) and (c) with background motion
removed so that the network learns the changes that are due to noise only.
A.2.1
Detail dataset Information.
In the synthetic training data generation, we use two datasets: the MS COCO [13],
and the PASCAL VOC segmentation dataset [5]. Fig. A.1 shows example frames
from our dataset. The license conditions are as follows: the annotations and im-
ages of the MS COCO are under Creative Commons Attribution 4.0 license and

Learning-based Video Motion MagniÔ¨Åcation
19
0
50
100
150
200
250
Frequency  (Hz)
-10
-5
0
log(|F( )|)
Dyn. mode
Static mode
0
50
100
150
200
250
Frequency  (Hz)
-10
-5
0
log(|F( )|)
cran
throat
woman1
woman2
tree1
tree2
camera
guitar
‚âà430 frm.
(a)
(b)
(c)
Fig. A.2. Comparison on temporal operators. The plots visualize log frequency
responses of the subtraction operation for Static and Dynamic modes (a) and temporal
Ô¨Ålter examples (b) used in [30, 26, 27]. (c) temporal domain visualization of a low-
freq. band-pass Ô¨Ålter example (sampling rate: 60Hz, cut-oÔ¨Ä: [0.35, 0.71]Hz) used in
[26], where the Ô¨Ålter lies across near 450 frames. For visual comparison purpose, the
frequency domain range is resampled to have the same range comparable between (a)
and (b).
Flickr terms of use, respectively, and the PASCAL VOC is under Flickr terms
of use and MSR Cambridge License (RTF).
All the real video examples used for comparisons come from either [30, 27,
26, 4, 31]; otherwise our own captured data is used.
A.2.2
Comparison of temporal operations
Our network is trained on the two frames setting, but we also have shown that
multi-frame linear temporal Ô¨Åltering is compatible to some extent. We discuss the
diÔ¨Äerence of temporal operation characteristics for a reference purpose. Fig. A.2
shows the frequency response characteristics of diÔ¨Äerent temporal operations.
Also, as shown in Fig. A.2, the area-under-covers of (a) are far broader than
(b), i.e., these modes magnify broader frequency ranges comparing to band-pass
Ô¨Åltering in (b). This implies as follows. First, as the same amount of magniÔ¨Åca-
tion factors are increased, overall energy increment of ours is even larger than
the previous work; hence, the magniÔ¨Åcation factors are not directly comparable
across modes, and this fact needs to be taken into account when the results
are compared. Second, since noise (high) frequency band is also involved in, the
training and working on the regime (a) would require more robust representation
and synthesis power than the regime (b); thus, the regimes of utilizing only two
frames are much challenging. For temporal Ô¨Åltering, magnifying a low-frequency
band needs to take into account a long history of frames as shown in Fig. A.2-(c),
while given an anchor frame, Static mode can magnify broad ranges of motion
only with two frames, which is memory eÔ¨Écient.
We note that the subtraction operation at test phase is not claimed as the
best, but they have trade-oÔ¨Äs. The multi-frame based temporal Ô¨Åltering [30, 26]
has its own merits: selectivity of a frequency band of interest, and noise fre-
quency band suppression as shown in Fig. A.2-(b). Since our network is trained
only on (a), it is hard to expect any generalization for other temporal opera-
tions. Surprisingly, at the test stage, we observed that our representation works
favorably well with replacing the subtraction operation in the manipulator by
the temporal Ô¨Åltering. Thus, we also conduct experiments with the subtraction
operation to assess and compare the performance of our representation with the

20
Authors Suppressed Due to Excessive Length
competing ones, i.e., phase representations from complex steerable Ô¨Ålters [26]
and Riesz transformation [27] (see the supplementary video). Also, we compare
our method with the competing methods equipped with temporal Ô¨Åltering.
Fig. A.3 shows its example that Static mode with our method magniÔ¨Åes the
motion of all three strings whose frequencies are diÔ¨Äerent. This shows that our
method magniÔ¨Åes true motions, but not hallucinating.
0
50
100
150
200
250
300
frequency  (Hz)
0
5
10
amplitude | F( ) |
6th (E): 80 Hz
5th (A): 108 Hz
4th (D): 144 Hz
ùë•
ùë°
0
50
100
150
200
250
300
frequency  (Hz)
0
5
10
15
20
amplitude | F( ) |
6th (E): 80 Hz
5th (A): 108 Hz
4th (D): 144 Hz
Fig. A.3. Broad frequency magniÔ¨Åcation (Guitar sequence). The Static mode
with our method does not alter input frequency, as well as magniÔ¨Åes overall the fre-
quency. The estimated frequencies of 4, 5, 6-th strings are {137.14, 109.10, 80}-Hz, which
is very close values to ideal frequencies, {144, 108, 80}-Hz. Color lines on the slice view
are the samples of the period measure to estimate frequency.
A.2.3
Additional qualitative Comparisons.
Various qualitative results can be found in the supplementary video. Due to the
characteristic of the problem, we strongly recommend to refer to the supplemen-
tary video for complete comparison and evaluation.
We also present another comparison with other methods in Fig. A.4, where
we replace the all the temporal operation to be Dynamic mode, so that we can
compare the capability of the representation and synthesis of each method under
the same setting.
A.2.4
Additional Quantitative Comparisons.
We summarize the test parameters used in all quantitative results in Table A.2.
In Fig. A.5, we show additional quantitative evaluations for completeness: noise
performance on real data and input motion range test. To test the noise per-
formance on real examples in (a), we grab three consecutive frames from 17
high-speed videos, and approximate their motions as linear. That is, the third

Learning-based Video Motion MagniÔ¨Åcation
21
(b)
(c)
(d)
(e)
(a)
Fig. A.4. Slice view comparison with other methods, Dynamic mode. We com-
pare original inputs (a) for reference, Wadhwa et al. [26] (b), Wadhwa et al. [27] (c),
Zhang et al. [31] (d) and the proposed method (e). Even though there are large transla-
tional motions, only our method shows clear boundaries with proper magniÔ¨Åed motion
in slice view, while the other methods have blurry textures due to artifacts.

22
Authors Suppressed Due to Excessive Length
Table A.2. Summary of parameter for each quantitative test.
Param.
Test MagniÔ¨Åcation Factor Motion (px.) MagniÔ¨Åed Motion (px.)
Noise
Background Motion
Sub-pixel
Capped at 100
0.01 - 1
Capped at 10
No
Yes
Noise
5.0
2.0
10.0
0.01 - 100
No
Noise on Real Image
2.0 (estimate)
-
-
0.01 - 100
No
MagniÔ¨Åcation Factor
1 - 1,000
0.01 - 2.0
Capped at 10
No
Yes
Input Motion
Capped at 5.0
1 - 10
Capped at 10
No
Yes
MagniÔ¨Åed Motion
Capped at 5.0
1 - 10
1 - 30
No
Yes
(a) Noise performance in real images
(b) Input motion test. Because magniÔ¨Åed motion is
capped at 10px, magniÔ¨Åcation factor approaches 1
towards the end, which explain the performance.
Fig. A.5. Additional Quantitative Analysis.
frame will have twice the amount of motion as the second frame. We synthetically
add noise as described before.
To see the extreme behavior against noise, we qualitatively compare the cases
in Fig. A.6. Our method tends to preserve detail better (notice the car) rather
than blurring, and have a better structure preserving quality in the output than
the phase-based one. Since our Ô¨Ålters are directly learned in a data-driven way,
we can easily augment the data to induce desired properties.
(a)
(b)
Fig. A.6. Example output from extreme noise test on real images (Noise
factor = 9). (a) Our method preserves the detail better and results in less noise than
(b) the phase-based method [26].

Learning-based Video Motion MagniÔ¨Åcation
23
Fig. A.7. Approximate texture encoder kernel. We approximate our (non-linear)
spatial encoder as linear convolution kernels. Most of the kernel approximations resem-
ble narrow blur kernels. This diÔ¨Äers from the shape representation kernel (Fig. 9).
A.2.5
Additional Network Visualization for Analysis
We present the additional visualization. Further observation can be obtained
by directly visualizing each unit (channel) with a simple example. Interestingly,
we observed that many activation patterns could be categorized into four cases
shown in Fig. A.8. Motivated by the recent network interpretation method [2],
we use an example based interpretation. We use a simple synthetic data with
apparently simple colors with several motions (see Fig. A.8-(a)). We compare
the texture and shape representation at early and later iteration of training.
Fig. A.8 shows that the texture representations tend to be direction invariant
and color sensitive, while the shape representation tend to be color invariant and
direction sensitive.
SpeciÔ¨Åcally, we inspect the transitions by counting the number of directional
invariant detectors in texture representation, and numbers of color invariant
detectors in shape representation at 5k and 30k iterations by subjects with re-
ferring to the references in the left. The ratios of directional invariant and color
invariant detectors are 22
32 (68.8%)‚Üí15
32 (46.9%), and 26
64 (40.6%)‚Üí37
64 (57.8%),
respectively, according to training. Since this behavior is learned from data, it
may suggest that the steerable Ô¨Ålter (directional) for shape representation in [26]
was a good choice for the magniÔ¨Åcation task.
In Fig. A.8-(j,k), we sample the representation randomly from all units, where
it shows that detectors dominant to either color or directional information are
frequently observed in respective representations.
We additionally visualize the kernel approximation of the texture represen-
tation as done in Sec. 4.3 of the main paper. The kernels of the texture repre-
sentation (Fig. A.7) are diÔ¨Äerent from those of the shape representation. They
seem to resemble blurring kernels (or delta function).
In Fig. A.9-(a,b), we compare the activations of the compensation part, i.e.,
h(Œ±(g(A2 ‚àíA1)), with two diÔ¨Äerent magniÔ¨Åcation factors Œ± for the same input
motion. It shows that, according to Œ±, the compensation not only scales activa-
tions but also spatially propagates around. This suggests that our representation
acts as a proxy of motion information that is able to be interpreted and manip-
ulated. Lastly, observing activations of the compensation part in Fig. A.9 and
the fusion layer in Fig. A.10, the network seems to learn how to compensate
discrepancy introduced by motion manipulation, rather than explicitly moving
pixels. These suggest that our method learns a representation closely related to
Eulerian motion representation rather than Lagrangian one.

24
Authors Suppressed Due to Excessive Length
Input
Color
Contour
Directional edge
Mixed
Texture representation
Unit A
Unit B
(a)
(b) 5k iter.
(c) 30k iter.
(d) 5k iter.
(e) 30k iter.
Shape representation
Unit C
Unit D
(f) 5k iter.
(g) 30k iter.
(h) 5k iter.
(i) 30k iter.
(j)Randomly sampled texture representations
(k) Randomly sampled shape representations
Fig. A.8. Visualization of representations from the encoder. We observed that
many texture and shape activations could be mainly categorized into four types of dom-
inant characteristics: color, contour, directional edge and mixed detectors. With this
observation, we only exploit simple color and directional edge property for maintaining
interpretability. (a) Synthetic data used for analysis, where input motion displacements
are {0.2, 0.5, 0.8, 1.1, 1.4, 1.7, 2.0} pixels in a raster scan order. (b‚Äìi) We probe the hid-
den units that behave like a sort of invariant detector at 5k and 30k iterations. The
texture representation tends to capture color properties, while the shape one tends to
capture edge and boundary. Especially, Unit A evolves to be a green color dominant
detector (refer to the color of (a)) with losing directional responses. Unit D evolves
from a red color plus directional edge detector to color invariant edge detector. (j,k)
Randomly sampled representations at Texture repr. and Shape repr. in Fig. 2,
respectively.
A.2.6
Applications
To show the versatility of our learned representation, we additionally present po-
tential applications, view/frame interpolation. and dynamic cinemagraph. Note
that we directly use the network trained on our synthetic data for motion mag-
niÔ¨Åcation, and we have never re-trained or Ô¨Åne-tuned the network for the speciÔ¨Åc
applications. The results can be found in the supplementary video.
For the view/frame interpolation applications, we use our network to gen-
erate intermediate frames between two input frames by changing magniÔ¨Åcation

Learning-based Video Motion MagniÔ¨Åcation
25
Unit18
Unit52
(a) Œ±=2
(b) Œ±=5
(a) Œ±=2
(b) Œ±=5
Fig. A.9. Visualization of activations of h(Œ±(g(A2 ‚àíA1)). Comparing (a) to (b),
as the magniÔ¨Åcation factors are increased, the manipulator layer not only increases
the scale of activation, but also propagates the activation around. In this regard, our
network seems to learn how to compensate object movements according to resulting
motion.
(a) Unit02
(b) Unit13
(c) Unit19
(d) Decoded
Fig. A.10. Visualization of activations at the Ô¨Årst layer residual block in the
decoder, Fusion layer. We show randomly sampled activations fused from texture
and shape compensated representations. The representations and decoded frame show
that our network compensates discrepancy induced by motion magniÔ¨Åcation visually,
rather than explicit pixel movement. It shows a color synthesis behavior of our network
in (d). Around the yellow ellipse in (d), the color values seem to be synthesized to
compensate movements in a similar way to inpainting.
factor in a range of [0,1]. For the view interpolation example, we generate and
insert 25 intermediate frames given two input images. For the frame interpo-
lation example, we generate Ô¨Åve frames between every two consecutive frames,
i.e., 5√ó temporal interpolation. We use the light Ô¨Åeld camera dataset [29] in
these applications.
For the cinemagraph application [12], we add a dynamism control function
by a post-synthesis approach using our method. We can control the magnitude of
dynamism in the cinemagraph by leveraging the controllability of magniÔ¨Åcation
factors without retraining. We use the cinemagraph data generated by [21].
A.2.7
Supplementary Video Content
The content summary of the supplementary video5 is as follows:
5 The supplementary video can be found in https://youtu.be/GrMLeEcSNzY.

26
Authors Suppressed Due to Excessive Length
‚Äì Qualitative comparison with temporal Ô¨Ålter
‚Äì Qualitative comparison in 2 frame input setting (static mode, dynamic mode,
frequency characteristics comparison)
‚Äì Additional analysis
‚Ä¢ MagniÔ¨Åcation ability according to (sub-pixel) input motion
‚Ä¢ Applying diÔ¨Äerent magniÔ¨Åcation factors without re-training (including
motion attenuation examples)
‚Äì Applications: View/Frame interpolation and progressive dynamism eÔ¨Äects
of cinemagraph.

