TORCHSCALE: Transformers at Scale
Shuming Ma∗, Hongyu Wang∗, Shaohan Huang, Wenhui Wang, Zewen Chi, Li Dong
Alon Benhaim, Barun Patra, Vishrav Chaudhary, Xia Song, Furu Wei†
Microsoft
https://aka.ms/torchscale
Abstract
Large Transformers have achieved state-of-the-art performance across many tasks.
Most open-source libraries on scaling Transformers focus on improving training or
inference with better parallelization. In this work, we present TORCHSCALE, an
open-source toolkit that allows researchers and developers to scale up Transform-
ers efﬁciently and effectively. TORCHSCALE has the implementation of several
modeling techniques, which can improve modeling generality and capability, as
well as training stability and efﬁciency. Experimental results on language modeling
and neural machine translation demonstrate that TORCHSCALE can successfully
scale Transformers to different sizes without tears. The library is available at
https://aka.ms/torchscale.
1
TORCHSCALE: A Library for Transformers at (Any) Scale
Recently, there is a trend that the Transformers have become the de facto backbone across different
areas, and their model sizes are growing large. Scaling Transformers brings more capacity and the
convergence is much faster compared to the smaller models. More importantly, more emergent
abilities are observed in the larger models, and they are unpredictable in the smaller models [Wei
et al., 2022]. Therefore, both researchers and practitioners should beneﬁt from a toolkit that can scale
Transformers at any scale. While there are some toolkits on scaling Transformers, most of them focus
on improving the parallelization of the systems, such as model parallelism [Shoeybi et al., 2019],
pipeline parallelism [Huang et al., 2019], expert parallelism [Lepikhin et al., 2021], and optimizer
parallelism [Rajbhandari et al., 2020]. Different from them, TORCHSCALE improves the scalability
of Transformers from the modeling perspective. It implements several modeling techniques that can
improve generality, stability, and efﬁciency during scaling up the model size.
1.1
Generality
There are multiple Transformer [Vaswani et al., 2017] variants for different tasks, including Post-LN,
Pre-LN, and so on. TORCHSCALE adopts Magneto [Wang et al., 2022b] as the default backbone.
Magneto is a foundation Transformer for general-purpose modeling, which serves as a go-to architec-
ture for various tasks and modalities. With one uniﬁed architecture, this allows TORCHSCALE to
support different applications, including language modeling, machine translation, vision pretraining,
speech recognition, and multi-modal pretraining. Magneto is simple and efﬁcient. Compared to
Pre-LN, Magneto introduces an extra LayerNorm into each sublayer. We refer more details to Wang
et al. [2022b]. TORCHSCALE has the implementation of encoder, decoder, and encoder-decoder
architectures for different tasks.
∗Equal contribution. † Corresponding author.
arXiv:2211.13184v1  [cs.LG]  23 Nov 2022

0
25K
50K
Steps
25
35
45
55
65
75
PPL
12L
24L
48L
96L
(a) Scaling Depth
0
25K
50K
Steps
20
25
30
35
40
PPL
16 experts
64 experts
256 experts
(b) Scaling Width
Figure 1: Scaling-up experiments on language modeling.
1.2
Stability
As the model grows, one problem is that Transformers are more unstable in optimization. The models
can diverge at any time during the training. This requires lots of computation and human efforts to
tune and test the models. To get rid of the pain when scaling up the models, TORCHSCALE follows the
theoretical derivation of DeepNet [Wang et al., 2022a] to improve the training stability. Speciﬁcally,
we implement the initialization method of Sub-LN for Magneto. Besides, as an alternative, we also
implement the DeepNorm from DeepNet, which can fundamentally improve the optimization of
Post-LN Transformers. With better stability, the models accept larger learning rate as well as more
diverse data that may contain some noise. Moreover, it enables some modeling techniques that might
bring stability issues.
1.3
Efﬁciency
One mainstream of scaling Transformers is the mixture-of-experts (MoE) model. MoE models
introduce the sparsity to Transformers, which proves to be both effective and efﬁcient. TORCHSCALE
implement X-MoE [Chi et al., 2022], a variant of sparse MoE model. It leverages the sparsity
of mixture-of-experts while preserving the transferability of Transformers by mitigating the rep-
resentation collapse problem of the sparse MoE models. TORCHSCALE supports both Top-1 and
Top-2 routing algorithms, which balance the performance and the computation cost. This allows
Transformers to scale up to billions or trillions of parameters without much additional computation
cost. In our preliminary experiments, we observe that gradient clipping plays an important role in the
performance of sparse MoE models. We propose a novel gradient clipping method, called SparseClip,
which improves the training stability while preserving the performance of the sparse models.
2
Scaling Up Experiments
We conduct experiments on language modeling and neural machine translation. With TORCHSCALE,
we train the models with the architectures of both decoder and encoder-decoder. We further scale up
their depths and widths to evaluate the stability and convergence.
2.1
Language Modeling
The experiments on language modeling are performed on an English-language corpus, which is a
subset of the data from Liu et al. [2019] and the English portion of CC100 corpus [Conneau et al.,
2020]. The validation data is randomly sampled from the same corpus. We preprocess the data with
the GPT-2 [Radford et al., 2019] tokenizer. We train the models with a batch size of 2048 samples.
Each sample contains 128 tokens. The models are trained for 50 thousand steps with up to 13 billion
tokens in total. More training details are in the appendix.
2

0
25K
50K
75K
100K
Steps
4
6
8
10
PPL
24L
48L
96L
192L
(a) Scaling Depth
0
25K
50K
75K
100K
Steps
4
5
6
7
8
PPL
16 experts
64 experts
256 experts
(b) Scaling Width
Figure 2: Scaling-up experiments on neural machine translation.
We start from a Magneto model with 12 decoder layers, 1024 hidden size, and 16 attention heads,
and further scale its depth from 12L to 24L, 48L, and 96L. We evaluate their perplexity (PPL) on
the validation set and plot the curves in Figure 1(a). It shows that TORCHSCALE can successfully
scale up the depth. With better training stability, the convergence is smooth regardless of the depth.
Moreover, the expressive power grows as the models deepen, resulting in better validation PPL.
In addition to the depth, we also scale the width. To scale it up efﬁciently, we replace the feed-forward
layers with the X-MoE layers and gradually increase the number of experts from 16 experts to
64experts and 256 experts. We use top-2 routing as it produces better results in our preliminary
experiments. Figure 1(b) summarizes the results of the sparse MoE models. Although a larger model
size brings more challenges in optimization, TORCHSCALE can still successfully train a model with
256 experts and up to 13B parameters, outperforming the models with a smaller size. This proves the
effectiveness of TORCHSCALE to train the Transformers at any scale.
2.2
Neural Machine Translation
We evaluate TORCHSCALE on neural machine translation. The experiments are conducted using a
combination of CCMatrix [Schwenk et al., 2021], CCAligned [El-Kishky et al., 2020], OPUS [Zhang
et al., 2020], and Tatoeba2. The ﬁnal data consists of 102 languages, 1932 directions, and 12B
sentence pairs. We tokenize the data with the SentencePiece model provided by the Flores-101
dataset [Goyal et al., 2021]. We use a batch size of 262K tokens to train the model for 100K steps.
More details can be found in the appendix.
With TORCHSCALE, we implement a Magneto model in the architecture of encoder-decoder. The
model has a 12-layer encoder and a 12-layer decoder with 1024 hidden dimension, 16 heads, and 4096
intermediate dimension of feed-forward layers. Similar to the experiments on language modeling, we
scale the number of layers from 12L-12L to 24L-24L, 48L-48L, and 96L-96L. Figure 2(a) shows that
the training of these models is stable. More importantly, better performance can be achieved as the
model grows. This proves that TORCHSCALE can fully use the capacity of a larger model.
We also evaluate the sparse X-MoE models with top-2 routing. Figure 2(b) illustrates the scaling
curves from 16 experts, 64 experts, to 256 experts. TORCHSCALE can effectively scale up the number
of experts, reaching a signiﬁcant improvement over the smaller models. Notably, the training ﬂops
are almost the same among these models, proving the efﬁciency of TORCHSCALE.
2.3
SparseClip: Gradient Clipping for Sparse MoE Models
Gradient clipping is standard practice for deep neural models to alleviate the gradient explosion
problem. It is even more important for the sparse MoE models, which are more unstable in training.
2https://tatoeba.org/en/
3

5K
10K
15K
20K
Steps
4
5
6
7
8
9
PPL
Dense
256 experts
256 experts w/ SparseClip
Figure 3: Training curves on neural machine translation. SparseClip signiﬁcantly improves the
convergence of the sparse MoE models.
Vanilla gradient clipping can be formulated as:
g ←
ξ
max(ξ, ||g||2) g,
(1)
where ξ is a hyper-parameter to control the strength of clipping. It scales the model’s gradient g when
its L2 norm ||g||2 exceeds ξ. For the sparse MoE models, the gradient norm is computed as:
||g||2 = || (gd, gs) ||2,
(2)
where gd is the gradients of the dense part and gs is the gradients of the MoE parameters.
It is natural to directly combine the gradients of two parts before computing the L2 norm of the
whole model. However, when the MoE models grow, the MoE gradients become to dominate the
gradient norm, making the gradient direction inaccurate after clipping the gradients. We conduct
experiments to compare a 256-expert model with its dense counterpart, which both use the vanilla
gradient clipping during training. Figure 3 shows that the convergence of the 256-expert model has
no signiﬁcant difference with the dense model, despite the fact that it has over 100x parameters than
the dense model. It proves that the vanilla gradient clipping hurts the sparse MoE models, limiting
their scaling to larger model sizes. Meanwhile, removing gradient clipping is infeasible as it will
destabilize the model training and results in divergence at the beginning of training.
To deal with this problem, we propose SparseClip, which re-weights the gradients of MoE parameters
when computing the gradient norm. Speciﬁcally, the gradient norm can be calculated as:
||g||2 = || (gd, κgs) ||2,
(3)
κ =
1
√
E
,
(4)
where E is the number of experts for sparse MoE models. This can mitigate the domination of the
MoE gradients, making the total gradient norm nearly constant with the increase of experts.
Figure 3 compares SparseClip with vanilla gradient clipping. It shows that Sparseclip signiﬁcantly
outperforms the vanilla gradient clipping, leading to a lower PPL. This veriﬁes the effectiveness of
our SparseClip method.
We further perform some visualizations of SparseClip and the vanilla gradient clipping. Figure 4(a)
is the training entropy of routing. It shows that SparseClip has lower entropy than the vanilla gradient
clipping, indicating that SparseClip improves the routing of MoE layers. Figure 4(b) illustrates
the number of gradient clippings. While gradient clipping is triggered only at the beginning of the
training, it is interesting that it has a great effect on the performance throughout the training. We
leave further research on it in the future work.
4

5K
10K
15K
20K
Steps
3
4
5
6
7
Gating Entropy
256 experts
256 experts w/ SparseClip
(a) Training Entropy of Routing
0
5K
10K
15K
20K
Steps
0
2
4
6
8
10
# Gradient Clipping
256 experts
256 experts w/ SparseClip
(b) Number of Gradient Clipping Triggered
Figure 4: Visualization of SparseClip and vanilla gradient clipping on neural machine translation.
3
Conclusion
We introduce TORCHSCALE, an open-source toolkit that enables scaling Transformers both efﬁciently
and effectively. It implements several state-of-the-art modeling techniques, originally from Magneto,
DeepNet, and X-MoE, to improve modeling generality and capability, as well as training stability
and efﬁciency. TORCHSCALE adopts a novel gradient clipping method, called SparseClip, which
can further improve the stability while preserving the performance of large sparse models. The
evaluations on language modeling and machine translation demonstrate that TORCHSCALE allows
scaling Transformers at different model sizes.
References
Zewen Chi, Li Dong, Shaohan Huang, Damai Dai, Shuming Ma, Barun Patra, Saksham Singhal,
Payal Bajaj, Xia Song, and Furu Wei. On the representation collapse of sparse mixture of experts.
CoRR, abs/2204.09179, 2022.
Alexis Conneau, Kartikay Khandelwal, Naman Goyal, Vishrav Chaudhary, Guillaume Wenzek, Fran-
cisco Guzmán, Edouard Grave, Myle Ott, Luke Zettlemoyer, and Veselin Stoyanov. Unsupervised
cross-lingual representation learning at scale. In Dan Jurafsky, Joyce Chai, Natalie Schluter, and
Joel R. Tetreault, editors, ACL 2020, pages 8440–8451, 2020.
Ahmed El-Kishky, Vishrav Chaudhary, Francisco Guzmán, and Philipp Koehn. CCAligned: A
massive collection of cross-lingual web-document pairs. In Bonnie Webber, Trevor Cohn, Yu-
lan He, and Yang Liu, editors, Proceedings of the 2020 Conference on Empirical Methods in
Natural Language Processing, EMNLP 2020, Online, November 16-20, 2020, pages 5960–5969.
Association for Computational Linguistics, 2020.
Naman Goyal, Cynthia Gao, Vishrav Chaudhary, Peng-Jen Chen, Guillaume Wenzek, Da Ju, Sanjana
Krishnan, Marc’Aurelio Ranzato, Francisco Guzmán, and Angela Fan. The FLORES-101 evalua-
tion benchmark for low-resource and multilingual machine translation. CoRR, abs/2106.03193,
2021.
Yanping Huang, Youlong Cheng, Ankur Bapna, Orhan Firat, Dehao Chen, Mia Xu Chen, HyoukJoong
Lee, Jiquan Ngiam, Quoc V. Le, Yonghui Wu, and Zhifeng Chen. Gpipe: Efﬁcient training of giant
neural networks using pipeline parallelism. In NeurIPS 2019, pages 103–112, 2019.
Dmitry Lepikhin, HyoukJoong Lee, Yuanzhong Xu, Dehao Chen, Orhan Firat, Yanping Huang,
Maxim Krikun, Noam Shazeer, and Zhifeng Chen. Gshard: Scaling giant models with conditional
computation and automatic sharding. In ICLR 2021, 2021.
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Mandar Joshi, Danqi Chen, Omer Levy, Mike
Lewis, Luke Zettlemoyer, and Veselin Stoyanov. Roberta: A robustly optimized BERT pretraining
approach. CoRR, abs/1907.11692, 2019.
5

Alec Radford, Jeff Wu, Rewon Child, David Luan, Dario Amodei, and Ilya Sutskever. Language
models are unsupervised multitask learners. OpenAI Blog, 2019.
Samyam Rajbhandari, Jeff Rasley, Olatunji Ruwase, and Yuxiong He. Zero: memory optimizations
toward training trillion parameter models. In Christine Cuicchi, Irene Qualters, and William T.
Kramer, editors, Proceedings of the International Conference for High Performance Computing,
Networking, Storage and Analysis, SC 2020, Virtual Event / Atlanta, Georgia, USA, November
9-19, 2020, page 20. IEEE/ACM, 2020.
Holger Schwenk, Guillaume Wenzek, Sergey Edunov, Edouard Grave, Armand Joulin, and Angela
Fan. CCMatrix: Mining billions of high-quality parallel sentences on the web. In Chengqing Zong,
Fei Xia, Wenjie Li, and Roberto Navigli, editors, Proceedings of the 59th Annual Meeting of the
Association for Computational Linguistics and the 11th International Joint Conference on Natural
Language Processing, ACL/IJCNLP 2021, (Volume 1: Long Papers), Virtual Event, August 1-6,
2021, pages 6490–6500. Association for Computational Linguistics, 2021.
Mohammad Shoeybi, Mostofa Patwary, Raul Puri, Patrick LeGresley, Jared Casper, and Bryan Catan-
zaro. Megatron-lm: Training multi-billion parameter language models using model parallelism.
CoRR, abs/1909.08053, 2019.
Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz
Kaiser, and Illia Polosukhin. Attention is all you need. In NeurIPS 2017, pages 5998–6008, 2017.
Hongyu Wang, Shuming Ma, Li Dong, Shaohan Huang, Dongdong Zhang, and Furu Wei. DeepNet:
Scaling transformers to 1,000 layers. CoRR, abs/2203.00555, 2022a.
Hongyu Wang, Shuming Ma, Shaohan Huang, Li Dong, Wenhui Wang, Zhiliang Peng, Yu Wu, Payal
Bajaj, Saksham Singhal, Alon Benhaim, Barun Patra, Zhun Liu, Vishrav Chaudhary, Xia Song,
and Furu Wei. Foundation transformers. CoRR, abs/2210.06423, 2022b.
Jason Wei, Yi Tay, Rishi Bommasani, Colin Raffel, Barret Zoph, Sebastian Borgeaud, Dani Yogatama,
Maarten Bosma, Denny Zhou, Donald Metzler, Ed H. Chi, Tatsunori Hashimoto, Oriol Vinyals,
Percy Liang, Jeff Dean, and William Fedus. Emergent abilities of large language models. CoRR,
abs/2206.07682, 2022.
Biao Zhang, Philip Williams, Ivan Titov, and Rico Sennrich. Improving massively multilingual neural
machine translation and zero-shot translation. In ACL 2020, pages 1628–1639. Association for
Computational Linguistics, 2020.
6

A
Hyperparameters
Hyperparameters
Language Modeling
Layers
{12, 24, 48, 96}
Hidden size
1024
FFN inner hidden size
4096
Attention heads
16
Peak learning rate
5e-4
Batch size
2048
Adam β
(0.9, 0.98)
Learning rate schedule
Polynomial decay
Warmup updates
750
Dropout
0.1
Attention dropout
0.1
Weight decay
0.01
Table 1: Hyperparameters of dense models for the experiments on language modeling.
Hyperparameters
Language Modeling
Layers
12
Experts
{16, 64, 256}
Hidden size
1024
FFN inner hidden size
4096
Attention heads
16
MoE Frequency
2
Weight of Balance Loss
0.01
Peak learning rate
5e-4
Batch size
2048
Adam β
(0.9, 0.98)
Learning rate schedule
Polynomial decay
Warmup updates
750
Dropout
0.1
Attention dropout
0.1
Weight decay
0.01
Table 2: Hyperparameters of sparse models for the experiments on language modeling.
7

Hyperparameters
Neural Machine Translation
Layers
{12-12, 24-24, 48-48, 96-96}
Hidden size
1024
FFN inner hidden size
4096
Attention heads
16
Peak Learning rate
5e-4
Learning rate schedule
Inverse sqrt
Warm-up updates
6000
Batch Size (tokens)
262K
Adam β
(0.9, 0.98)
Label smoothing
0.1
Gradient clipping
1.0
Dropout
0.1
Weight decay
0.0
Table 3: Hyperparameters for dense models for the experiments on neural machine translation.
Hyperparameters
Neural Machine Translation
Layers
12-12
Experts
{16, 64, 256}
Hidden size
1024
FFN inner hidden size
4096
Attention heads
16
MoE Frequency
2
Weight of Balance Loss
0.01
Peak Learning rate
5e-4
Learning rate schedule
Inverse sqrt
Warm-up updates
6000
Batch Size (tokens)
262K
Adam β
(0.9, 0.98)
Label smoothing
0.1
Gradient clipping
1.0
Dropout
0.1
Weight decay
0.0
Table 4: Hyperparameters for sparse models for the experiments on neural machine translation.
8

