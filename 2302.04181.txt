Attending to Graph Transformers
Luis M¨uller1 , Mikhail Galkin2 , Christopher Morris1 and Ladislav Ramp´aˇsek3
1RWTH Aachen University, 2Intel AI Lab, 3Mila, Universit´e de Montr´eal
luis.mueller@cs.rwth-aachen.de, mikhail.galkin@intel.com, morris@cs.rwth-aachen.de,
ladislav.rampasek@mila.quebec
Abstract
Recently, transformer architectures for graphs
emerged as an alternative to established techniques
for machine learning with graphs, such as graph
neural networks. So far, they have shown promis-
ing empirical results, e.g., on molecular prediction
datasets, often attributed to their ability to circum-
vent graph neural networks’ shortcomings, such as
over-smoothing and over-squashing. Here, we derive
a taxonomy of graph transformer architectures, bring-
ing some order to this emerging ﬁeld. We overview
their theoretical properties, survey structural and
positional encodings, and discuss extensions for im-
portant graph classes, e.g., 3D molecular graphs.
Empirically, we probe how well graph transformers
can recover various graph properties, how well they
can deal with heterophilic graphs, and to what extent
they prevent over-squashing. Further, we outline
open challenges and research direction to stimulate
future work. Our code is available at https://github.
com/luis-mueller/probing-graph-transformers.
1
Introduction
Graph-structured data are prevalent across application domains
ranging from chemo- and bioinformatics [Barabasi and Olt-
vai, 2004; Reiser et al., 2022] to image [Simonovsky and
Komodakis, 2017] and social-network analysis [Easley and
Kleinberg, 2010], clearly indicating the importance of machine
learning methods for such data. In recent years, graph neural
networks (GNNs) [Chami et al., 2022; Gilmer et al., 2017;
Morris et al., 2021] were the dominant paradigm in machine
learning for graphs. However, with the rise of transformer
architectures [Vaswani et al., 2017] in natural language pro-
cessing [Lin et al., 2021b] and computer vision [Han et al.,
2022], recently, a large number of works in the ﬁeld focused
on designing transformer architectures capable of dealing with
graphs, so-called graph transformers (GTs).
Graph transformers have already shown promising perfor-
mance [Ying et al., 2021], e.g., by topping the leaderboard
of the OGB Large-Scale Challenge [Hu et al., 2021; Masters
et al., 2022] in the molecular property prediction track. The
superiority of GTs over standard GNN architecture is often
explained by GNNs’ bias towards encoding local structure
and being unable to capture global or long-range information,
often attributed to phenomena such as over-smoothing [Li et
al., 2018], under-reaching or over-squashing [Alon and Ya-
hav, 2021]. Many papers speculate that GTs [Ramp´aˇsek et
al., 2022] do not suffer from such effects as they aggregate
information over all nodes in a given graph and hence are not
limited to local structure bias. However, to make GTs aware of
graph structure, one has to equip them with so-called structural
and postional encodings. Here, structural encodings are, e.g.,
additional node features to make the GT aware of (sub-)graph
structure. In contrast, positional encodings make a node aware
of its position in the graph concerning the other nodes.
Present Work. Here, we derive a taxonomy of state-of-the-
art GT architectures, giving a structured overview of recent
developments. Moreover, we survey common positional and
structural encodings and clarify how they are related to GTs’
theoretical properties, e.g., their expressive power to capture
graph structure. Additionally, we investigate these properties
empirically by probing how well GTs can recover various graph
properties, deal with heterophilic graphs, and to what extent
GTs alleviate the over-squashing phenomenon. Further, we
outline open challenges and research direction to stimulate
future work. Our categorization, theoretical clariﬁcation, and
experimental study present a useful handbook for the GT and
the broader graph machine-learning community. Its insights
and principles will help spur novel research results and avenues.
Related Work. Since GTs emerged recently, only a few sur-
veys exist. Notably, Min et al. [2022a] provide a high-level
overview of some of the recent GT architectures. Different
from the present work, they do not discuss GT’s theoretical
and practical shortcomings and miss out on recent architectural
advancements. Chen et al. [2022a] gives an overview of GTs
for computer vision. Finally, Ramp´aˇsek et al. [2022] provide a
general recipe for classifying GT architectures, focusing on
devising empirically well-performing architectures rather than
giving a detailed, principled overview of the literature.
1.1
Background
A graph G is a pair (V, E) with a ﬁnite set of nodes V (G)
and a set of edges E(G) ⊆{{u, v} ⊆V | u ̸= v}. For ease
of notation, we denote an edge {u, v} as (u, v) or (v, u). In
the case of directed graphs, E(G) ⊆{(u, v) ∈V 2 | u ̸= v}.
Throughout the paper, we set n := |V (G)| and m := |E(G)|.
arXiv:2302.04181v1  [cs.LG]  8 Feb 2023

Graph Transformer
Encodings [Sec. 2.2]
Node-level Features
Positional
Local
Global
Relative
Structural
Local
Global
Relative
Edge-level
Shortest path
3D distances
Graph-level
Ego-graph
Subgraph
Input Features [Sec. 2.3]
Non-geometric
GT
SAN
GraphiT
SAT
TokenGT
GPS
MLP-Mixer
Geometric
SE(3)-Transformer
TorchMD-Net
Equiformer
Graphormer
Transformer-M
GPS++
Tokens [Sec. 2.4]
Nodes
GT
SAN
GraphiT
GraphTrans
SAT
GPS
SE(3)-Transformer
Equiformer
Nodes + Edges
EGT
TokenGT
Subgraphs
GMT
Coarformer
MLP-Mixer
Propagation [Sec. 2.5]
Fully
connected
w/ standard
attention
GT
TokenGT
Fully
connected
w/ modiﬁed
attention
Graphormer
GraphiT
SAN
SAT
EGT
Transformer-M
Sparse
GKAT
Exphormer
Hybrid
GraphTrans
GPS
GPS++
Figure 1: Categorization of graph transformers along four main categories with representative architectures.
A node-attributed graph G is a triple (V, E, X), where X ∈
Rn×d, for d > 0, is a node feature matrix and Xv is the node
feature of node v ∈V (G). Similarly, we can represent edge
features by an edge feature matrix E ∈Rm×e, for e > 0,
where Evw is the edge feature of edge (v, w) ∈E(G). The
neighborhood of v ∈V (G) is N(v) := {u ∈V (G) | (v, u) ∈
E(G)}. We say that two graphs G and H are isomorphic if
there exists an edge-preserving bijection ϕ: V (G) →V (H),
i.e., (u, v) is in E(G) if and only if (ϕ(u), ϕ(v)) is in E(H)
for all u, v ∈V (G). We denote a multiset by {{. . .}}.
Equivariance and Invariance. Operations on graphs need
to respect their symmetries, such as being agnostic to the
node permutations or other (group) transformations, such as
rotation, leading to the deﬁnitions of equivariance and invari-
ance. In general [Fuchs et al., 2021], given a transformation
T, a function f is equivariant if transforming the vector input
x is equal to transforming the output of the function f, i.e.,
f(Tx) = Tf(x). A function g is invariant if transforming the
vector input x does not change the output, i.e., g(Tx) = g(x).
In the 3D Euclidean space, 3D translations, rotations, and
reﬂections form the E(3) group. Translation and rotation form
the SE(3) group. Rotations form the SO(3) group, rotations
and reﬂections form the O(3) group.
Graph Transformers. A transformer is a stack of alternat-
ing blocks of multi-head attention and fully-connected feed-
forward networks. Let G be a graph with node feature matrix
X ∈Rn×d.1 In each layer, t > 0, given node feature matrix
X(t) ∈Rn×d, a single attention head computes
Attn(X(t)) := softmax
QKT
√dk

V,
(1)
where the softmax is applied row-wise, dk denotes the feature
dimension of the matrices Q and K, with X(0) := X. Here,
the matrices Q, K, and V are the result of projecting X(t)
linearly,
Q := X(t)WQ, K := X(t)WK, and V := X(t)WV ,
using three matrices WQ, WK ∈Rd×dK, and WV ∈Rd×d,
with optional bias terms omitted for clarity. Now, multi-head
attention MultiHead(X(t)) concatenates multiple (single) at-
tention heads, followed by an output projection to the feature
space of X(t). By combining the above with additional residual
1For simplicity, we learn attention between a graph’s nodes. How-
ever, in Section 2.4, we extend this to, e.g., edges or subgraphs.
connections and normalization, the transformer layer updates
features X(t) via
X(t+1) := FFN
 MultiHead
 X(t)
+ X(t)
.
(2)
As noticed by Mialon et al. [2021], we can rewrite Eq. (1) as
Attn(X(t))v =
X
u∈V (G)
kexp(X(t)
v , X(t)
u )
P
w∈V (G) kexp(X(t)
v , X(t)
w )
X(t)
w WV ,
for v ∈V (G), where
kexp(X(t)
v , X(t)
w ) := exp
 X(t)
v WQX(t)
w WK/√dK

.
Hence, we can view GTs as a special GNN, which we deﬁne
below, operating on a complete graph, where the attention
score weights the importance of each node during the sum
aggregation.
Graph Neural Networks. Intuitively, GNNs learn a vector
representing each node in a graph by aggregating information
from neighboring nodes. Formally, let G be a graph with
node feature matrix X ∈Rn×d. A GNN architecture consists
of a stack of neural network layers, i.e., a composition of
permutation-equivariant parameterized functions. Each layer
aggregates local neighborhood information, i.e., the neighbors’
features, around each node and then passes this aggregated
information on to the next layer. Following Gilmer et al. [2017]
and Scarselli et al. [2009], in each layer, t ≥0, we compute
vertex features
h(t+1)
v
:= UPD(t)
h(t)
v , AGG(t) {{h(t)
u | u ∈N(v)}}

∈Rd,
where UPD(t) and AGG(t) may be differentiable parameterized
functions, e.g., neural networks. For example, GNNs often
compute a vector for node v by using sum aggregation [Morris
et al., 2019], i.e.,
h(t+1)
v
:= σ

h(t)
v W(t)
1
+
X
w∈N(v)
h(t)
w W(t)
2

,
where σ is a non-linearity applied pointwise, W(t)
1
and W(t)
2
∈
Rd×d are parameter matrices, and h(0)
v
:= Xv.
2
The Landscape of Graph Transformers
In the following, we outline our taxonomy of GTs, see also Fig-
ure 1, bringing some order to the growing set of GT archi-
tectures. We start by discussing the theoretical properties of

GTs that heavily rely on structural and positional encodings,
which we study subsequently. Further, we discuss different
approaches to dealing with essential classes of input node fea-
tures, e.g., 3D coordinates in the case of molecules. We then
study how to tokenize a graph, i.e., partition a graph into atomic
entities between which the attention is computed, e.g., nodes.
Then, we review how GTs organize message propagation in the
graph through global, sparse, or hybrid attention. Finally, we
overview representative applications of GTs.
2.1
Theoretical Properties
It is crucial to understand that the general GT architecture
of Eq. (2) is less expressive in distinguishing non-isomorphic
graphs than standard GNNs. Hence, it is also weaker in ap-
proximating permutation-invariant and -equivariant functions
over graphs [Chen et al., 2019]. GTs are weaker since, without
sufﬁciently expressive structural and positional encodings, they
cannot capture any graph structure besides the number of nodes
and hence equal DeepSets-like architectures [Zaheer et al.,
2020] in expressive power. Thus, for GTs to capture non-trivial
graph structure information, they are crucially dependent on
such encodings; see below. In fact, by leveraging the results
in Chen et al. [2019], it is easy to show that GTs can only
become maximal expressive, i.e., universal function approxi-
mators, if they have access to maximally expressive structural
bias, e.g., structural encodings. However, this is equivalent to
solving the graph isomorphism problem [Chen et al., 2019].
Moreover, we stress that GNN architectures equipped with the
same encodings will also possess the same expressive power.
Hence, in terms of expressive power, GTs do not have an
advantage over GNNs.
2.2
Structural and Positional Encodings
As outlined in the previous subsection, GTs are crucially de-
pendent on structural and positional encodings to capture graph
structure. Although there is no formal deﬁnition or distinction
between the two, structural encodings make the GT aware of
graph structure on a local, relative, or global level. Such en-
codings can be attached to node-, edge-, or graph-level features.
Examples of local structural encodings include annotating node
features with node degree [Chen et al., 2022b], the diagonal of
the m-step random-walk matrix [Dwivedi et al., 2022], the time-
derivative of the heat-kernel diagonal [Kreuzer et al., 2021],
enumerate or count predeﬁned substructures and the node’s role
within [Bouritsas et al., 2022], or Ricci curvature [Topping et
al., 2022]. Examples of edge-level relative structural encodings
include relative shortest-path distances [Chen et al., 2022a] or
Boolean features indicating if two nodes are in the same sub-
structure [Bodnar et al., 2021]. Examples of graph-level global
structural encodings include eigenvalues of the adjacency or
Laplacian matrix [Kreuzer et al., 2021] or graph properties such
as diameter, number of connected components, or treewidth.
On the other hand, positional encodings make, e.g., a node,
aware of its relative position to the other nodes in a graph.
Hence, two such encodings should be close to each other if
the corresponding nodes are close in the graph. Again, we
can distinguish between local, global, or relative encodings.
Examples of node-level local positional encodings include the
shortest-path distance of a node to a hub or central node or
the sum of each column of the non-diagonal elements of the
m-step random walk matrix. An example of edge-level relative
positional encodings is pair-wise node distances [Chen et al.,
2022a; Beaini et al., 2021; Kreuzer et al., 2021; Mialon et al.,
2021; Li et al., 2020]. Examples of node-level global positional
encodings include eigenvalues of the adjacency or Laplacian
matrix [Kreuzer et al., 2021; Dwivedi and Bresson, 2020] or
unique identiﬁers for each connected component of the graph.
When designing such encodings, one must ensure equivari-
ance or invariance to the nodes’ ordering. Such equivariance
is trivially satisﬁed for simple encodings such as node degree
but not for more powerful encodings based on eigenvalues
of the adjacency or Laplacian matrix [Lim et al., 2022]. It
is an ongoing effort to design equivariant Laplacian-based
encodings [Lim et al., 2022; Wang et al., 2022].
2.3
Input Features
Besides characterizing GTs based on their use of structural and
positional encodings, we can also characterize them based on
their ability to deal with different node and edge features. To
this end, we devise two families of input features. First, we
consider so-called non-geometric features where nodes and
edges have feature vectors in Rd, i.e., graphs are described
with a tuple (V, E, X, E). Secondly, we consider so-called
geometric features where nodes and edges features contain geo-
metric information, e.g., 3D coordinates for nodes X3D ∈R3.
Therefore, graphs are described with (V, E, X, E, X3D, E3D).
We categorize GT architectures as non-geometric and those
supporting both features in the following.
Non-geometric GTs [Chen et al., 2022b; Choromanski et
al., 2021; Dwivedi and Bresson, 2020; He et al., 2022; Kim
et al., 2022; Kreuzer et al., 2021; Jain et al., 2021; Mialon et
al., 2021; Ramp´aˇsek et al., 2022] are most common and follow
equations in Section 1.1. Graphs with non-geometric features
do not have explicit geometric inductive bias. Examples of such
features include encoded node attributes in citation networks
or learnable atom-type embeddings in molecular graphs. Non-
geometric features are supposed to be equivariant to node
permutations, and transformers provide such equivariance by
default. Structural and positional features, see Section 2.2, are
often added to non-geometric features to increase the expressive
power of GTs.
3D molecular graphs provide geometric features describing
nodes and edges, e.g., 3D coordinates of atoms, angles of bonds,
or torsion angles of planes. Building GTs supporting geometric
features is more challenging as geometric features have to be
invariant or equivariant to certain group transformations, such
as rotation, depending on the task. Further, the architectures
must be invariant for graph-level molecular property prediction
tasks. In contrast, models must be equivariant in node-level
tasks such as predicting structural conformers or force ﬁelds.
SE(3)-Transformer [Fuchs et al., 2020] was one of the ﬁrst
attempts to incorporate SE(3) equivariance. By using ir-
reducible representations, Clebsch-Gordan coefﬁcients, and
spherical harmonics, the authors encode SE(3) equivariance
into the attention operation. Equiformer [Liao and Smidt,
2023] further extends this mechanism to complete E(3) equiv-
ariance. In contrast, TorchMD-NET [Th¨olke and Fabritiis,
2022] achieves SO(3) equivariance by incorporating inter-

atomic distances into the attention operation via exponential
normal radial basis functions (RBF). Graphormer [Shi et al.,
2022], Transformer-M [Luo et al., 2022a] and GPS++ [Masters
et al., 2022] take a similar approach, using Gaussian kernels to
encode 3D distances between all pairs of atoms. Tailored for
graph-level prediction tasks, GPS++ remains SE(3)-invariant,
while Graphormer and Transformer-M introduce an additional
SE(3)-equivariant prediction head for node-level molecular
dynamics tasks.
2.4
Graph to Sequence Tokenization
The nature of graph tokenization, i.e., mapping a graph into a
sequence of tokens, directly affects the supported features and
computational complexity. Here, we identify three approaches
to graph tokenization: (1) nodes as tokens, (2) nodes and edges
as tokens, and (3) patches or subgraphs as tokens.
Using nodes as input tokens is the most common approach
followed by many GTs, e.g., [Dwivedi and Bresson, 2020;
Fuchs et al., 2020; Kreuzer et al., 2021; Luo et al., 2022b;
Ramp´aˇsek et al., 2022; Th¨olke and Fabritiis, 2022; Ying et al.,
2021]. Here, we often treat structural and positional features
as additional node features. Given a graph with n nodes and
the attention procedure of Eq. (1), the complexity of such
GTs is in O(n2). We note that more scalable, sparse attention
mechanisms are also possible; see Section 2.5. Edge features,
e.g., shortest-path distances [Ying et al., 2021] or relative
3D distances [Luo et al., 2022b; Th¨olke and Fabritiis, 2022],
may be added as an attention bias given the fully computed
attention score matrix with n2 entries. Alternatively, Mialon et
al. [2021]; Jain et al. [2021]; Chen et al. [2022b] leverage a
GNN to incorporate node and edge features before applying
a transformer on the resulting node features. However, the
transformer’s quadratic complexity remains the bottleneck.
The second approach uses nodes and edges in the input
sequence as employed by EGT [Hussain et al., 2022] and
TokenGT [Kim et al., 2022]. Turning an input graph into a
graph of its edges is often used in molecular GNNs [Gasteiger
et al., 2021] and NLP [Yao et al., 2020]. In addition to soft
modeling the edges, i.e., the node-to-node interactions, the
attention operation also possibly models higher-order node-
edge and edge-edge interactions that theoretically result in an
expressiveness boost Kim et al. [2022]. The input sequence
can naturally incorporate node features, their positional en-
codings, and edge features. A pitfall of this approach is its
O(n + m)2 computational complexity. However, since the
approach includes edge features in the input sequence, such
GTs might beneﬁt from sparse attention mechanisms that do
not materialize the full attention matrix.
The third approach relies on patches or subgraphs as tokens.
In visual transformers [Dosovitskiy et al., 2021], such patches
correspond to k × k image slices. A generalization of patches
to the graph domain often corresponds to graph coarsening
or partitioning [Baek et al., 2021; Kuang et al., 2022; He et
al., 2022]. There, tokens are small subgraphs extracted with
various strategies. Initial representations of tokens are obtained
by passing subgraphs through a GNN using a form of pooling
to a single vector. He et al. [2022] adds token position features
to the resulting vectors to distinguish coarsened subgraphs
better. Finally, these tokens are passed through a transformer
with O(k2) complexity for a graph with k extracted subgraphs.
2.5
Message Propagation
Most GTs follow the global all-to-all attention of Vaswani et al.
[2017] between all pairs of tokens. In the initial GT [Dwivedi
and Bresson, 2020] and TokenGT [Kim et al., 2022] this
mechanism is unchanged, relying on token representations
augmented with graph structural or positional information.
Other models alter the global attention mechanism to bias it
explicitly, typically based on the input graph’s structural prop-
erties. Graphormer [Ying et al., 2021] incorporates shortest-
path distances, representation of edges along a shortest path,
and node degrees. Transformer-M [Luo et al., 2022a] fol-
lows Graphormer and adds kernelized 3D inter-atomic dis-
tances. GRPE [Park et al., 2022] considers multiplicative
interactions of keys and queries with node and edge features
instead of Graphormer’s additive bias and additionally aug-
ments output token values. SAN [Kreuzer et al., 2021] relies
on positional encodings and only adds preferential bias to in-
teractions along input-graph edges over long-distance virtual
edges. GraphiT [Mialon et al., 2021] employs diffusion kernel
bias, while SAT [Chen et al., 2022b] develops a GNN-based
structure-aware attention kernel. EGT [Hussain et al., 2022]
includes a mechanism akin to cross-attention to edge tokens to
bias inter-node attention and update edge representations.
As standard global attention incurs quadratic computational
complexity, it limits the application of graph transformers to
graphs of up to several thousands of nodes. To alleviate this
scaling issue, Choromanski et al. [2022] proposed GKAT based
on a kernelized attention mechanism of the Performer [Choro-
manski et al., 2021], scaling linearly with the number of tokens.
Another approach to improve GTs’ scaling is to consider a
reduced attention scope, e.g., based on locality or sparsiﬁed
instead of dense all-to-all, following expander graph-based
propagation [Deac et al., 2022].
Finally, hybrid approaches combine several propagation
schemes. For example, GPS and GPS++ [Ramp´aˇsek et al.,
2022; Masters et al., 2022] fuse local GNN-like models with
global all-to-all attention in one layer. While GPS employs
standard attention and can utilize linear attention mechanisms
such as Performer [Choromanski et al., 2022], GPS++ follows
Transformer-M’s attention conditioning. GraphTrans [Jain et
al., 2021] is also a hybrid but applies a stack of GNNs layers
ﬁrst, followed by a stack of global transformer layers.
3
Applications of Graph Transformers
Although GTs only emerged recently, they have already been
applied in various application areas, most notably in molecular
property prediction. In the following, we give a representative
overview of the applications of GTs.
Kan et al. [2022] propose the Brain Network Transformers
to predict properties of brain networks, e.g., the presence of
diseases, stemming from magnetic resonance imaging. To that,
they leverage rows of the adjacency matrix of each node as
structural encodings, which showed superior performance over
Laplacian-based encodings in previous studies. Moreover, they
devise a custom pooling layer leveraging the fact that nodes in
the same functional module tend to have similar properties.

Table 1: Hyper-parameter sets for GTs
and GNNs with or without PE/SE (SET 1),
and for Graphormer models (SET 2).
Hyper-parameter
SET 1
SET 2
Embed. dim.
64
72
Self-attn. heads
4
4
Weight decay
10−5
10−2
Learning rate
10−3
10−3
Gradient clip norm
1.0
5.0
LR scheduler
cosine,
constant
warm-up
Batch size
96
256
Table 2: Average test accuracy of GTs with structural bias (± SD) over ﬁve random seeds on the
structural awareness tasks. Difﬁculty level on top derived from GIN performance. We additionally
report the performance of a transformer without any structural bias serving as a baseline.
Model
Easy
Medium
Hard
EDGES
TRIANGLES-SMALL TRIANGLES-LARGE
CSL
2-way Accuracy ↑
10-way Accuracy ↑
10-way Accuracy ↑
10-way Accuracy ↑
GIN
98.11 ±1.78
71.53 ±0.94
33.54 ±0.30
10.00 ±0.00
Transformer
55.84 ±0.32
12.08 ±0.31
10.01 ±0.04
10.00 ±0.00
Transformer (LapPE)
98.00 ±1.03
78.29 ±0.25
10.64 ±2.94
100.00 ±0.00
Transformer (RWSE)
97.11 ±1.73
99.40 ±0.10
54.76 ±7.24
100.00 ±0.00
Graphormer
97.67 ±0.97
99.09 ±0.31
42.34 ±6.48
90.00 ±0.00
Liao and Smidt [2023]; Th¨olke and Fabritiis [2022] devise
an equivariant transformer architecture to predict quantum
mechanical properties of molecules. To capture the molecular
structure, they encode atom types and the atomic neighborhood
into a vectorial representation, followed by a multi-head atten-
tion mechanism. To predict scalar atom-wise prediction, they
rely on gated equivariant blocks [Sch¨utt et al., 2021], which
are then aggregated into single molecular predictions.
Yao et al. [2020] use transformers to tackle the graph-to-
sequence problem, i.e., the problem of translating a graph to
word sequences. They ﬁrst translate a graph to its Levi graph,
replacing labeled edges with additional nodes to incorporate
edge labels. They then split such a graph into multiple sub-
graphs according to the different edge nodes. Each subgraph
uses a standard transformer architecture to learn the vectorial
representation for each node. To incorporate graph structure,
they mask out non-neighbors of a node, concentrating on the
local structure. Finally, they concatenate multiple node rep-
resentations. Further applications use transformers for rumor
detection in microblogs [Khoo et al., 2020], predicting proper-
ties of crystals [Yan et al., 2022] or click-through rates [Min
et al., 2022b], or leverage them for 3D human pose and mesh
reconstruction from a single image [Lin et al., 2021a].
4
Experimental Study
We empirically evaluate two highly discussed aspects of graph
transformers: (1) the effectiveness of incorporating graph
structural bias into GTs, and (2) their ability to reduce over-
smoothing and over-squashing affecting GNNs. Concretely,
we aim to answer the following questions.
Q1 How well do different strategies for incorporating struc-
tural awareness into GTs contribute to recovering funda-
mental structural properties of graphs?
Q2 Does the ability of transformers to reduce over-smoothing
lead to improved performance on heterophilic datasets?
Q3 Do graph transformers alleviate over-squashing better than
GNN models?
4.1
Structural Awareness of GTs
For question Q1, we evaluate the two most prevalent strategies
for incorporating graph structure bias into transformers.
Positional and Structural Encodings (Sec. 2.2). Random-
walk structural encodings (RWSE) and Laplacian positional
encodings (LapPE), two popular positional or structural
encodings for transformers [Ramp´aˇsek et al., 2022].
Attention Bias (Sec. 2.5). Attention bias based on spatial in-
formation such as shortest-path distance between nodes,
following the Graphormer architecture [Ying et al., 2021].
We propose a benchmark of three tasks that require increasingly
higher structural awareness of non-geometric graphs. We
determine the level of structural awareness necessary to solve a
task according to the baseline performance of GIN [Xu et al.,
2019], a 1-WL-equivalent GNN reference model. In addition,
we report the performance of a vanilla transformer without any
structural bias to understand the relative impact of the positional
or structural encodings (PE/SE) and attention biasing.
We ﬁrst describe the tasks in our benchmark and their esti-
mated difﬁculty, then outline task-speciﬁc hyper-parameters of
evaluated models, and interpret the observed results; see Table 2
for quantitative results.
Detect Edges (Easy). Detecting whether an edge connects
two nodes can be considered the fundamental test for structural
awareness. We investigate this task using a custom dataset,
EDGES, derived from the ZINC [Dwivedi et al., 2023] dataset.
For each graph, we treat the pairs of nodes connected by
an edge as positive examples and select an equal number of
unconnected nodes as negative examples, resulting in a binary
edge detection task with balanced classes. Let P denote the set
of pairs selected as either positive or negative examples, and let
h(T )
v
denote the feature vector of node v after the last layer T of
a model. We make predictions as follows. We ﬁrst compute the
cosine similarity between h(T )
v
and h(T )
w
for each pair (v, w)
of nodes in P, resulting in a scalar similarity score. Finally,
we apply a linear layer to each similarity score, followed by a
sigmoid activation, resulting in binary class probabilities.
Count Triangles (Medium). Counting triangles only requires
information within a node’s immediate neighborhood. However,
more than 1-WL expressivity is required to solve it [Morris et
al., 2019]. For this task, we evaluate models on the TRIANGLES
dataset proposed by Knyazev et al. [2019], which poses triangle
counting as a 10-way classiﬁcation problem. Here, graphs have
between 1 and 10 triangles, each corresponding to one class.
The dataset speciﬁes a ﬁxed train/validation/test split which we
adopt in our experiments. Graphs in the train and validation
split are roughly the same size. The test set is a mixture of two
graph distributions, where 50% are graphs with a similar size
to those in the training and validation set (TRIANGLES-SMALL)
and 50% are graphs of larger size (TRIANGLES-LARGE). We
separately report model performance for TRIANGLES-SMALL

and TRIANGLES-LARGE to study the ability of transformers
with different structural biases to generalize to larger graphs.
We analyzed the datasets’ class balance and report that each
test set contains 5000 graphs with 500 graphs per class. For
more details about the dataset, see Knyazev et al. [2019].
Distinguish Circular Skip Links (CSL) (Hard).
A 1-
WL limited model cannot distinguish non-isomorphic CSL
graphs [Murphy et al., 2019] as the task requires an understand-
ing of distance [Morris et al., 2019]. Here, we evaluate models
on the CSL dataset [Dwivedi et al., 2023], which contains 150
graphs with skip-link lengths ranging from 2 to 16 and poses a
10-way classiﬁcation problem. We follow Dwivedi et al. [2023]
in training with 5-fold cross-validation.
Hyper-parameters. To simplify hyper-parameter selection,
we hand-designed two general sets of hyper-parameters; see Ta-
ble 1. For EDGES and TRIANGLES, we ﬁx a parameter budget
of around 200k for the transformer models, resulting in six
layers for each model with the respective embedding sizes spec-
iﬁed in Table 1. Further, we train Graphormer on 1k epochs.
Due to the small number of graphs in the CSL dataset, we
ﬁx a parameter budget of around 100k for the transformer
models, resulting in three layers for each model with the exact
embedding sizes as above. Further, we train Graphormer on 2k
epochs. We repeat each experiment on ﬁve random seeds and
report the model accuracy’s mean and standard deviation.
For our 1-WL-equivalent reference model, we chose the
GIN-layer [Xu et al., 2019]. To improve comparability with the
transformer models, we use a feed-forward network composed
of the same components and using the same hyper-parameters
as for transformers. For Graphormer, we use the feed-forward
network speciﬁed by Ying et al. [2021]. Further, we train GIN
with the SET 1 hyper-parameters. For EDGES and TRIANGLES,
this results in around 150k parameters, while for CSL, the GIN
model contains approximately 75k parameters.
Answering Q1. Table 2 shows that GTs supplemented with
structural bias generally perform well on all three tasks with
a few exceptions. First, the GT with Laplacian positional en-
codings performs sub-par on the TRIANGLES task. However,
it is still an improvement over the 1-WL-equivalent GIN. We
hypothesize this is due to an expressivity limit of Laplacian
encodings regarding triangle counting. Secondly, we observe
that all models generalize poorly to larger graphs on the TRI-
ANGLES dataset. Lastly, we observe that on CSL, Graphormer
cannot surpass 90% accuracy. A deeper analysis revealed that
the shortest-path distributions can only distinguish 9 out of the
10 classes correctly, meaning that Graphormer is theoretically
limited to at most 90% accuracy on CSL.
The above failure cases highlight that current graph trans-
formers still suffer from limited expressivity, and no clear
expressivity hierarchy exists for the used positional or struc-
tural encodings. Moreover, GTs may generalize poorly to larger
graphs. At the same time, we demonstrate a general superior-
ity of structurally biased GTs over standard 1-WL-equivalent
models such as GIN. Both the transformer with RWSE as
well as Graphormer solve EDGES, TRIANGLES-SMALL, and
CSL almost perfectly, two of which pose a challenge for GIN,
especially on CSL where GIN performs no better than random.
4.2
Reduced Over-smoothing in GTs?
Graph transformers are often ascribed with an ability to cir-
cumvent GNNs’ over-smoothing problem due to their global
attention mechanism. Thus, we set out to benchmark several
variants of GCN [Kipf and Welling, 2017], hybrid GPS mod-
els, and Graphormer on six heterophilic transductive datasets:
ACTOR [Tang et al., 2009]; CORNELL, TEXAS, WISCON-
SIN [CMU, 2001]; CHAMELEON and SQUIRREL [Rozember-
czki et al., 2021]. In these datasets, we expect over-smoothing
and under-reaching to be limiting factors.
We broadly follow the SET 1 hyper-parameters (Table 1).
However, we perform a grid search for each model variant to
select the embedding size (32, 64, or 96) and dropout rates while
we ﬁx the number of layers to two. We implement the GCN
and GT models following GPS with hybrid GCN+Transformer
aggregation layers but with the latter or former component
disabled, respectively. We train all models in full-batch mode
using the entire graph as input.
Answering Q2. All transformer-based models outperform a
2-layer GCN, and often the specialized Geom-GCN [Pei et al.,
2020], which experimental setup we follow. With the exemption
of node degree encodings (DEG), other PE/SE had minimal
effect on GCN’s performance. Adding global attention to the
GCN, i.e., following the GPS model, universally improves
the performance. Most interestingly, disabling the local GCN
in GPS, i.e., becoming the Transformer model, increases the
performance even further. Such results indicate that GNN-like
models are unﬁt for these heterophilic datasets, while the global
attention of a transformer empirically facilitates considerably
more successful information propagation. Graphormer, which
utilizes node degree encodings, performs comparably to the
Transformer counterparts. Surprisingly, the attention bias of
Graphormer had no impact on its performance. The shortest-
path distance bias appears uninformative in these datasets,
unlike, e.g., in ZINC, where we observed degradation from 0.12
test mean absolute error to 0.54 when disabling the attention
bias.
We conclude that we empirically conﬁrm the expected bene-
ﬁts of global attention, albeit GTs do not achieve overall SOTA
performance (e.g., see Luan et al. [2022]), which is a reminder
that specialized architectures can achieve similar or higher
performance without global attention still.
4.3
Reduced Over-squashing in GTs?
To answer question Q3, we evaluate a GT on the NEIGHBORS-
MATCH problem proposed by Alon and Yahav [2021]. This
synthetic dataset requires long-range interaction between leaf
nodes and the root node of a tree graph of depth d. The prob-
lem demonstrates GNNs’ difﬁculty in transmitting information
through a receptive ﬁeld growing exponentially with d. We
run our experiments with minimal changes to the code of Alon
and Yahav [2021] and train our transformer on depths 2 to
6. Note that GNN models fail to perfectly ﬁt the training set
of trees with depth 4. Convergence on NEIGHBORSMATCH
can sometimes take up to 100 000 epochs for large depths d.
Since the structure of the graphs in NEIGHBORSMATCH is
irrelevant to solving the problem, we did not need to augment
node features with positional/structural encodings or attention

Table 3: Benchmarking of multiple model variants on six heterophilic transductive datasets. Here
we report average test accuracy (± SD) over ten random seeds. We follow the dataset protocol
of Pei et al. [2020]; for additional model comparison; see Luan et al. [2022].
Model (PE/SE type)
ACTOR
CORNELL
TEXAS
WISCONSIN CHAMELEON SQUIRREL
Geom-GCN [Pei et al., 2020]
31.59 ±1.15
60.54 ±3.67
64.51 ±3.66
66.76 ±2.72
60.00 ±2.81
38.15 ±0.92
GCN (no PE/SE)
33.92 ±0.63
53.78 ±3.07
65.95 ±3.67
66.67 ±2.63
43.14 ±1.33
30.70 ±1.17
GCN (LapPE)
34.30 ±1.12
56.22 ±2.65
65.95 ±3.67
66.47 ±1.37
43.53 ±1.45
30.80 ±1.38
GCN (RWSE)
33.69 ±1.07
53.78 ±4.09
62.97 ±3.21
69.41 ±2.66
43.84 ±1.68
31.77 ±0.65
GCN (DEG)
33.99 ±0.91
53.51 ±2.65
66.76 ±2.72
67.26 ±1.53
46.36 ±2.07
34.50 ±0.87
GPSGCN+Transformer (LapPE)
37.68 ±0.52
66.22 ±3.87
75.41 ±1.46
74.71 ±2.97
48.57 ±1.02
35.58 ±0.58
GPSGCN+Transformer (RWSE)
36.95 ±0.65
65.14 ±5.73
73.51 ±2.65
78.04 ±2.88
47.57 ±0.90
34.78 ±1.21
GPSGCN+Transformer (DEG)
36.91 ±0.56
64.05 ±2.43
73.51 ±3.59
75.49 ±4.23
52.59 ±1.81
42.24 ±1.09
Transformer (LapPE)
38.43 ±0.87
69.46 ±1.73
77.84 ±1.08
76.08 ±1.92
49.69 ±1.11
35.77 ±0.50
Transformer (RWSE)
38.13 ±0.63
70.81 ±2.02
77.57 ±1.24
80.20 ±2.23
49.45 ±1.34
35.35 ±0.75
Transformer (DEG)
37.39 ±0.50
71.89 ±2.48
77.30 ±1.32
79.80 ±0.90
56.18 ±0.83
43.64 ±0.65
Graphormer (DEG only)
36.91 ±0.85
68.38 ±1.73
76.76 ±1.79
77.06 ±1.97
54.08 ±2.35
43.20 ±0.82
Graphormer (DEG, attn. bias)
36.69 ±0.70
68.38 ±1.73
76.22 ±2.36
77.65 ±2.00
53.84 ±2.32
43.75 ±0.59
Figure 2: Average train accuracy over
ten random seeds of a GT on the NEIGH-
BORSMATCH dataset, compared to mod-
els from Alon and Yahav [2021].
2
3
4
5
6
Problem radius (depth)
0.2
0.4
0.6
0.8
1.0
Train Acc.
GCN
GGNN
Transformer
bias. Otherwise, we used the same transformer architecture as
in Section 4.1.
Answering Q3. Figure 2 shows that the GT performs exceed-
ingly better than the GNN baselines and can perfectly ﬁt the
training set even for depth d = 6. However, we note that the
NEIGHBORSMATCH problem is idealized and has only limited
practical implications. The core issue of over-squashing, which
is squashing an exponentially growing amount of information
into a ﬁxed-size vector representation, is not resolved by trans-
formers. Nonetheless, our results demonstrate that the ability of
transformers to model long-range interactions between nodes
can circumvent the problem posed by Alon and Yahav [2021].
5
Open Challenges and Future Work
Since the area of GTs is a new, emerging ﬁeld, there are still
many open challenges, both from practical and theoretical
points of view. On the theoretical side, although it is often
claimed that GTs offer better predictive performance over
GNNs, are more capable of capturing long-range dependen-
cies and preventing over-smoothing and over-squashing, a
principled explanation still needs to be formed. Moreover,
there needs to be a clearer understanding of the properties of
structural and positional encodings. For example, it has yet
to be understood when certain encodings are helpful and how
they compare. A ﬁrst step could be precisely characterizing
different encodings in terms of distinguishing non-isomorphic
graphs, similar to the WL hierarchy. Further, understanding
GTs’ generalization performance on larger graphs has yet to be
understood, at least on a similar level to GNNs [Yehudai et al.,
2021; Zhou et al., 2022].
On the practical side, one major downside of GTs is their
quadratic running time in the number of nodes, preventing
them from scaling to truly large graphs typical in real-world
node-level prediction tasks. Moreover, due to the attention
mechanism’s nature, it still needs to be determined how best
to incorporate edge-feature information into GT architectures.
Further, our experimental analysis reveals a disadvantage of
local GNN-like model when used in conjunction with trans-
formers as in Ramp´aˇsek et al. [2022] on heterophilic datasets.
Dealing with heterophilic data is thus an open challenge also
for GTs. Moreover, it is crucial to incorporate expert or domain
knowledge, e.g., physical or chemical knowledge for molecular
prediction, into the attention matrix, in a principled manner.
Explaining and interpreting the performance of GTs remains
an open research area where we draw parallels with NLP. We
posit that studying graph transformers in the graph machine
learning community follows a similar path of studying trans-
former language models in NLP uniﬁed under the name of
Bertology [Rogers et al., 2021; Vuli´c et al., 2020]. Numerous
Bertology studies reported that more than dissecting attention
matrices and attention scores in transformer layers is needed
for understanding how language models work. Instead, the
community converged on designing datasets and tasks tailored
to language model features such as co-reference resolution
or mathematical reasoning. Therefore, we hypothesize that
understanding GTs’ performance through attention scores is
limited, and the community should focus on designing diverse
benchmarks probing particular GTs’ properties. Further, study-
ing scaling laws and emergent behavior of GTs and GNNs is
still in its infancy, with few examples in chemistry [Frey et al.,
2022] and protein representation learning [Lin et al., 2022].
6
Conclusion
We have provided a taxonomy of graph transformers (GTs). To
this end, we overviewed GTs’ theoretical properties and their
connection to structural and positional encodings. We then
thoroughly surveyed how GTs can deal with different input
features, e.g., 3D information, and discussed the different ways
of mapping a graph to a sequence of tokens serving as GTs’
input. Moreover, we thoroughly discussed different ways GTs
propagate information and recent real-world applications. Most
importantly, we showed empirically that different encodings
and architectural choices drastically impact GTs’ predictive
performance. We veriﬁed that GTs can deal with heterophilic
graphs and prevent over-squashing to some extent. Finally, we
proposed open challenges and outlined future work. We hope
our survey presents a helpful handbook of graph transformers’
methods, perspectives, and limitations, and that its insights and
principles will help spur and shape novel research results in
this emerging ﬁeld.

References
U. Alon and E. Yahav. On the bottleneck of graph neural
networks and its practical implications. In ICLR, 2021.
J. Baek, M. Kang, and S. J. Hwang. Accurate learning of graph
representations with graph multiset pooling. In ICLR, 2021.
A.-L. Barabasi and Z. N. Oltvai. Network biology: under-
standing the cell’s functional organization. Nature Reviews
Genetics, 5(2):101–113, 2004.
D. Beaini, S. Passaro, V. L´etourneau, W. L. Hamilton, G. Corso,
and P. Li´o. Directional graph networks. In ICML, pages
748–758, 2021.
C. Bodnar, F. Frasca, N. Otter, Y. Wang, P. Lio, G. F. Montufar,
and M. Bronstein. Weisfeiler and Lehman go cellular: CW
networks. NeurIPS, pages 2625–2640, 2021.
G. Bouritsas, F. Frasca, S. P. Zafeiriou, and M. Bronstein.
Improving graph neural network expressivity via subgraph
isomorphism counting. TPAMI, 2022.
I. Chami, S. Abu-El-Haija, B. Perozzi, C. R´e, and K. Murphy.
Machine learning on graphs: A model and comprehensive
taxonomy. JMLR, pages 1–64, 2022.
Z. Chen, S. Villar, L. Chen, and J. Bruna. On the equivalence
between graph isomorphism testing and function approxima-
tion with gnns. In NeurIPS, pages 15868–15876, 2019.
C. Chen, Y. Wu, Q. Dai, H. Zhou, M. Xu, S. Yang, X. Han,
and Y. Yu. A survey on graph neural networks and graph
transformers in computer vision: A task-oriented perspective.
ArXiv, 2022.
D. Chen, L. O’Bray, and K. Borgwardt. Structure-aware trans-
former for graph representation learning. ICML, 2022.
K. M. Choromanski, V. Likhosherstov, D. Dohan, X. Song,
A. Gane, T. Sarl´os, P. Hawkins, J. Q. Davis, A. Mohiuddin,
L. Kaiser, D. B. Belanger, L. J. Colwell, and A. Weller.
Rethinking attention with performers. In ICLR, 2021.
K. Choromanski, H. Lin, H. Chen, T. Zhang, A. Sehanobish,
V. Likhosherstov, J. Parker-Holder, T. Sarlos, A. Weller, and
T. Weingarten. From block-Toeplitz matrices to differential
equations on graphs: towards a general theory for scalable
masked transformers. In ICML, 2022.
CMU.
World
Wide
Knowledge
Base
(Web-KB)
project.
http://www.cs.cmu.edu/afs/cs.cmu.edu/project/
theo-11/www/wwkb/, 2001.
A. Deac, M. Lackenby, and P. Veliˇckovi´c. Expander graph
propagation. In Learning on Graphs Conference, 2022.
A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn,
X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer,
G. Heigold, S. Gelly, J. Uszkoreit, and N. Houlsby. An
image is worth 16x16 words: Transformers for image recog-
nition at scale. In ICLR, 2021.
V. P. Dwivedi and X. Bresson. A generalization of transformer
networks to graphs. ArXiv, 2020.
V. P. Dwivedi, A. T. Luu, T. Laurent, Y. Bengio, and X. Bres-
son. Graph neural networks with learnable structural and
positional representations. In ICLR, 2022.
V. P. Dwivedi, C. K. Joshi, T. Laurent, Y. Bengio, and X. Bres-
son. Benchmarking graph neural networks. JMLR, 2023.
D. Easley and J. Kleinberg. Networks, Crowds, and Markets:
Reasoning About a Highly Connected World. Cambridge
University Press, 2010.
N. Frey, R. Soklaski, S. Axelrod, S. Samsi, R. Gomez-
Bombarelli, C. Coley, and V. Gadepally. Neural scaling
of deep chemical models. chemRxiv, 2022.
F. Fuchs, D. E. Worrall, V. Fischer, and M. Welling. SE(3)-
Transformers: 3D roto-translation equivariant attention net-
works. In NeurIPS, 2020.
F. B. Fuchs, E. Wagstaff, J. Dauparas, and I. Posner. Iterative
SE(3)-Transformers. In Geometric Science of Information,
pages 585–595, 2021.
J. Gasteiger, F. Becker, and S. G¨unnemann. Gemnet: Universal
directional graph neural networks for molecules. NeurIPS,
pages 6790–6802, 2021.
J. Gilmer, S. S. Schoenholz, P. F. Riley, O. Vinyals, and G. E.
Dahl. Neural message passing for quantum chemistry. In
ICML, pages 1263–1272, 2017.
K. Han, Y. Wang, H. Chen, X. Chen, J. Guo, Z. Liu, Y. Tang,
A. Xiao, C. Xu, Y. Xu, et al. A survey on vision transformer.
TPAMI, 2022.
X. He, B. Hooi, T. Laurent, A. Perold, Y. LeCun, and X. Bres-
son. A generalization of ViT/MLP-mixer to graphs. ArXiv,
2022.
W. Hu, M. Fey, H. Ren, M. Nakata, Y. Dong, and J. Leskovec.
OGB-LSC: A large-scale challenge for machine learning on
graphs. In NeurIPS: Datasets and Benchmarks Track, 2021.
M. S. Hussain, M. J. Zaki, and D. Subramanian. Global self-
attention as a replacement for graph convolution. In KDD,
pages 655–665, 2022.
P. Jain, Z. Wu, M. Wright, A. Mirhoseini, J. E. Gonzalez, and
I. Stoica. Representing long-range context for graph neural
networks with global attention. NeurIPS, 2021.
X. Kan, W. Dai, H. Cui, Z. Zhang, Y. Guo, and C. Yang. Brain
network transformer. ArXiv, 2022.
L. M. S. Khoo, H. L. Chieu, Z. Qian, and J. Jiang. Inter-
pretable rumor detection in microblogs by attending to user
interactions. In AAAI, pages 8783–8790, 2020.
J. Kim, T. D. Nguyen, S. Min, S. Cho, M. Lee, H. Lee, and
S. Hong. Pure transformers are powerful graph learners. In
NeurIPS, 2022.
T. N. Kipf and M. Welling. Semi-supervised classiﬁcation with
graph convolutional networks. In ICLR, 2017.
B. Knyazev, G. W. Taylor, and M. Amer. Understanding
attention and generalization in graph neural networks. In
NeurIPS, pages 4202–4212, 2019.
D. Kreuzer, D. Beaini, W. L. Hamilton, V. L´etourneau, and
P. Tossou. Rethinking graph transformers with spectral
attention. In NeurIPS, 2021.
W. Kuang, Z. Wang, Y. Li, Z. Wei, and B. Ding. Coarformer:
Transformer for large graph via graph coarsening. OpenRe-
view, 2022.
Q. Li, Z. Han, and X. Wu. Deeper insights into graph convo-
lutional networks for semi-supervised learning. In AAAI,
pages 3538–3545, 2018.
P. Li, Y. Wang, H. Wang, and J. Leskovec. Distance encoding:
Design provably more powerful neural networks for graph
representation learning. In NeurIPS, 2020.
Y.-L. Liao and T. Smidt. Equiformer: Equivariant graph atten-
tion transformer for 3D atomistic graphs. In ICLR, 2023.

D. Lim, J. Robinson, L. Zhao, T. Smidt, S. Sra, H. Maron, and
S. Jegelka. Sign and basis invariant networks for spectral
graph representation learning. ArXiv, 2022.
K. Lin, L. Wang, and Z. Liu. Mesh graphormer. In ICCV,
pages 12919–12928, 2021.
T. Lin, Y. Wang, X. Liu, and X. Qiu. A survey of transformers.
ArXiv, 2021.
Z. Lin, H. Akin, R. Rao, B. Hie, Z. Zhu, W. Lu, N. Smetanin,
A. dos Santos Costa, M. Fazel-Zarandi, T. Sercu, S. Candido,
et al. Language models of protein sequences at the scale
of evolution enable accurate structure prediction. bioRxiv,
2022.
S. Luan, C. Hua, Q. Lu, J. Zhu, M. Zhao, S. Zhang, X.-W.
Chang, and D. Precup. Revisiting heterophily for graph
neural networks. In NeurIPS, 2022.
S. Luo, T. Chen, Y. Xu, S. Zheng, T.-Y. Liu, L. Wang, and
D. He. One transformer can understand both 2D & 3D
molecular data. ArXiv, 2022.
S. Luo, S. Li, S. Zheng, T. Liu, L. Wang, and D. He. Your
transformer may not be as powerful as you expect. ArXiv,
2022.
D. Masters, J. Dean, K. Klaser, Z. Li, S. Maddrell-Mander,
A. Sanders, H. Helal, D. Beker, L. Ramp´aˇsek, and D. Beaini.
GPS++: an optimised hybrid MPNN/transformer for molec-
ular property prediction. ArXiv, 2022.
G. Mialon, D. Chen, M. Selosse, and J. Mairal. GraphiT:
Encoding graph structure in transformers. ArXiv, 2021.
E. Min, R. Chen, Y. Bian, T. Xu, K. Zhao, W. Huang, P. Zhao,
J. Huang, S. Ananiadou, and Y. Rong. Transformer for
graphs: An overview from architecture perspective. ArXiv,
2022.
E. Min, Y. Rong, T. Xu, Y. Bian, D. Luo, K. Lin, J. Huang,
S. Ananiadou, and P. Zhao. Neighbour interaction based
click-through rate prediction via graph-masked transformer.
In SIGIR, pages 353–362, 2022.
C. Morris, M. Ritzert, M. Fey, W. L. Hamilton, J. E. Lenssen,
G. Rattan, and M. Grohe. Weisfeiler and leman go neural:
Higher-order graph neural networks. In AAAI, pages 4602–
4609, 2019.
C. Morris, Y. L., H. Maron, B. Rieck, N. M. Kriege, M. Grohe,
M. Fey, and K. Borgwardt. Weisfeiler and Leman go machine
learning: The story so far. ArXiv, 2021.
R. L. Murphy, B. Srinivasan, V. A. Rao, and B. Ribeiro. Re-
lational pooling for graph representations. In ICML, pages
4663–4673, 2019.
W. Park, W. Chang, D. Lee, J. Kim, and S. won Hwang. GRPE:
Relative positional encoding for graph transformer. ArXiv,
2022.
H. Pei, B. Wei, K. C.-C. Chang, Y. Lei, and B. Yang. Geom-gcn:
Geometric graph convolutional networks. In ICLR, 2020.
L. Ramp´aˇsek, M. Galkin, V. P. Dwivedi, A. T. Luu, G. Wolf,
and D. Beaini. Recipe for a General, Powerful, Scalable
Graph Transformer. In NeurIPS, 2022.
P. Reiser, M. Neubert, A. Eberhard, L. Torresi, C. Zhou,
C. Shao, H. Metni, C. van Hoesel, H. Schopmans, T. Som-
mer, et al. Graph neural networks for materials science and
chemistry. Communications Materials, 3(1):93, 2022.
A. Rogers, O. Kovaleva, and A. Rumshisky. A primer in
bertology: What we know about how bert works. TACL,
8:842–866, 2021.
B. Rozemberczki, C. Allen, and R. Sarkar. Multi-scale at-
tributed node embedding. Journal of Complex Networks,
9(2):cnab014, 2021.
F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and
G. Monfardini. The graph neural network model. IEEE
Transactions on Neural Networks, 20(1):61–80, 2009.
K. Sch¨utt, O. T. Unke, and M. Gastegger. Equivariant mes-
sage passing for the prediction of tensorial properties and
molecular spectra. In ICML, pages 9377–9388, 2021.
Y. Shi, S. Zheng, G. Ke, Y. Shen, J. You, J. He, S. Luo, C. Liu,
D. He, and T.-Y. Liu. Benchmarking graphormer on large-
scale molecular modeling datasets. ArXiv, 2022.
M. Simonovsky and N. Komodakis. Dynamic edge-conditioned
ﬁlters in convolutional neural networks on graphs. In CVPR,
pages 29–38, 2017.
J. Tang, J. Sun, C. Wang, and Z. Yang. Social inﬂuence analysis
in large-scale networks. In KDD, 2009.
P. Th¨olke and G. D. Fabritiis. Equivariant transformers for
neural network based molecular potentials. In ICLR, 2022.
J. Topping, F. Di Giovanni, B. P. Chamberlain, X. Dong, and
M. M. Bronstein. Understanding over-squashing and bottle-
necks on graphs via curvature. In ICLR, 2022.
A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones,
A. N. Gomez, L. Kaiser, and I. Polosukhin. Attention is all
you need. In NeurIPS, pages 5998–6008, 2017.
I. Vuli´c, E. M. Ponti, R. Litschko, G. Glavaˇs, and A. Korhonen.
Probing pretrained language models for lexical semantics.
In EMNLP, pages 7222–7240, 2020.
H. Wang, H. Yin, M. Zhang, and P. Li. Equivariant and stable
positional encoding for more powerful graph neural networks.
In ICLR, 2022.
K. Xu, W. Hu, J. Leskovec, and S. Jegelka. How powerful are
graph neural networks? In ICLR, 2019.
K. Yan, Y. Liu, Y. Lin, and S. Ji. Periodic graph transformers
for crystal material property prediction. In NeurIPS, 2022.
S. Yao, T. Wang, and X. Wan. Heterogeneous graph transformer
for graph-to-sequence learning. In ACL, pages 7145–7154,
Online, 2020.
G. Yehudai, E. Fetaya, E. A. Meirom, G. Chechik, and
H. Maron. From local structures to size generalization
in graph neural networks. In ICML, pages 11975–11986,
2021.
C. Ying, T. Cai, S. Luo, S. Zheng, G. Ke, D. He, Y. Shen, and
T.-Y. Liu. Do transformers really perform badly for graph
representation? In NeurIPS, 2021.
M. Zaheer, G. Guruganesh, K. A. Dubey, J. Ainslie, C. Alberti,
S. Onta˜n´on, P. Pham, A. Ravula, Q. Wang, L. Yang, and
A. Ahmed. Big Bird: Transformers for longer sequences. In
NeurIPS, 2020.
Y. Zhou, G. Kutyniok, and B. Ribeiro. OOD link prediction
generalization capabilities of message-passing GNNs in
larger test graphs. In NeurIPS, 2022.

