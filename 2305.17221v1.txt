Federated Learning for Semantic Parsing:
Task Formulation, Evaluation Setup, New Algorithms
Tianshu Zhang1∗, Changchang Liu2, Wei-Han Lee2, Yu Su1, Huan Sun1
1The Ohio State University
2IBM Research
1{zhang.11535, su.809, sun.397}@osu.edu
2{changchang.liu33, wei-han.lee1}@ibm.com
Abstract
This paper studies a new task of federated learn-
ing (FL) for semantic parsing, where multiple
clients collaboratively train one global model
without sharing their semantic parsing data.
By leveraging data from multiple clients, the
FL paradigm can be especially beneficial for
clients that have little training data to develop
a data-hungry neural semantic parser on their
own. We propose an evaluation setup to study
this task, where we re-purpose widely-used
single-domain text-to-SQL datasets as clients
to form a realistic heterogeneous FL setting and
collaboratively train a global model. As stan-
dard FL algorithms suffer from the high client
heterogeneity in our realistic setup, we further
propose a novel LOss Reduction Adjusted Re-
weighting (Lorar) mechanism to mitigate the
performance degradation, which adjusts each
client’s contribution to the global model up-
date based on its training loss reduction during
each round. Our intuition is that the larger
the loss reduction, the further away the current
global model is from the client’s local optimum,
and the larger weight the client should get. By
applying Lorar to three widely adopted FL
algorithms (FedAvg, FedOPT and FedProx),
we observe that their performance can be im-
proved substantially on average (4%-20% ab-
solute gain under MacroAvg) and that clients
with smaller datasets enjoy larger performance
gains. In addition, the global model converges
faster for almost all the clients.1
1
Introduction
Semantic parsing aims to translate natural lan-
guage utterances into formal meaning representa-
tions such as SQL queries and API calls and can be
applied to build natural language interfaces that en-
able users to query data and invoke services without
∗Work started during the internship at IBM T. J. Watson
Research Center and continued at OSU.
1Our
code
and
data
are
publicly
available
at
https://github.com/OSU-NLP-Group/
FL4SemanticParsing
programming (Berant et al., 2013; Thomason et al.,
2015; Su et al., 2017; Campagna et al., 2017). Neu-
ral semantic parsers have achieved remarkable per-
formance in recent years (Wang et al., 2020a; Rubin
and Berant, 2021; Scholak et al., 2021). However,
they are data-hungry; bootstrapping a neural se-
mantic parser by annotating data on a large scale
can be very challenging for many institutions, as
it requires the annotators to have intimate knowl-
edge of formal programs. One natural thought is to
leverage data from different institutions and train a
unified model that can be used for all institutions.
However, in practice, institutions such as hospitals,
banks, and legal firms are prohibited from shar-
ing their data with others, due to privacy concerns.
Therefore, for institutions that only have very lim-
ited data, it is extremely hard to build their own
neural semantic parsers.
Federated learning (FL) (Koneˇcn`y et al., 2016;
McMahan et al., 2017; Yang et al., 2018) has turned
out to be a popular training paradigm where multi-
ple clients can collaboratively train a global model
without exchanging their own data. In this paper,
we study a new task of federated learning for se-
mantic parsing. Through FL on the data scattered
on different clients (e.g., institutions), we aim to
obtain a global model that works well for all clients,
especially those that have insufficient data to build
their own neural models.
Towards that end, we propose an evaluation
setup by re-purposing eight existing datasets that
are widely adopted for text-to-SQL parsing, such
as ATIS (Iyer et al., 2017) and Yelp (Yaghmazadeh
et al., 2017). These datasets demonstrate great
heterogeneity, in terms of dataset sizes, language
usage, database structures, and SQL complexity,
as they were collected from the real life by differ-
ent researchers, at different times, and for different
purposes. Therefore, we use this collection to sim-
ulate a realistic scenario where eight clients with
very different data participate in the FL paradigm
arXiv:2305.17221v1  [cs.CL]  26 May 2023

to jointly train a neural semantic parser.
Heterogeneity, where the data distributions and
dataset sizes on different clients are different, is
recognized as one of the biggest challenges in FL
(McMahan et al., 2017; Reddi et al., 2020; Li et al.,
2020a, 2021; Shoham et al., 2019; T Dinh et al.,
2020). Existing work either uses synthetic data
(Li et al., 2020a) or splits a classification dataset
based on Dirichlet distribution (Lin et al., 2022)
to simulate the non-IID federated learning setting,
while we propose a more realistic setup to study
this setting for semantic parsing. Pre-trained lan-
guage models such as T5 (Raffel et al., 2020) have
been shown as a powerful unified model for various
semantic parsing tasks (Xie et al., 2022; Rajkumar
et al., 2022), which can be leveraged to save us the
efforts for client-specific model designs. Specifi-
cally, we adopt T5-base as our backbone seman-
tic parser in the FL paradigm, and conduct exten-
sive experiments and analysis using three widely-
adopted FL algorithms: FedAvg (McMahan et al.,
2017), FedOPT (Reddi et al., 2020) and FedProx
(Li et al., 2020a).
As standard FL algorithms suffer from the high
client heterogeneity in our realistic setup, we fur-
ther propose a novel re-weighting mechanism for
combining the gradient updates from each client
during the global model update. The high-level
idea is shown in Figure 1. Our intuition is that, for
each client, the reduction of training loss during
each round can signalize how far the current global
model is away from the local optimum. By giv-
ing larger weights to those clients that have larger
training loss reduction, the global model update
can accommodate those clients better, thus miti-
gating potential performance degradation caused
by high heterogeneity. We formulate this intuition
as a re-weighting factor to adjust how much each
client should contribute to the global model update
during each round. Our proposed mechanism can
be applied to all the three FL algorithms and exper-
iments show that it can substantially improve both
their parsing performance and their convergence
speed, despite being very simple.
In summary, our main contributions are:
• To the best of our knowledge, we are the first
to study federated learning for semantic pars-
ing, a promising paradigm for multiple institu-
tions to collaboratively build natural language
interfaces without data sharing, which is es-
pecially beneficial for institutions with little
Figure 1:
Our proposed re-weighting mechanism
Lorar for the global model update in each round. The
weight for each client (i.e., its contribution to the global
model update) will be adjusted based on its loss re-
duction in each round. ∆Lt
i means the training loss
reduction of the i-th client in the t-th round.
training data.
• We propose an evaluation setup to simu-
late a realistic heterogeneous FL setting
where different participating institutions have
very different data.
We re-purpose eight
single-domain text-to-SQL datasets as eight
clients, which demonstrate high heterogene-
ity in terms of dataset sizes, language usage,
database structures, and SQL complexity.
• We propose a novel re-weighting mechanism,
which uses the training loss reduction of each
client to adjust its contribution to the global
model update during each round.
Experi-
ments show that our re-weighting mechanism
can substantially improve the model perfor-
mance of existing FL algorithms on average,
and clients with smaller training data observe
larger performance gains. We discuss the limi-
tations of our work and encourage future work
to further study this task.
2
Motivation and Task Formulation
Semantic parsing aims to translate natural language
utterances into formal meaning representations and
has numerous applications in building natural lan-
guage interfaces that enable users to query data and
invoke services without programming. As many
institutions often lack data to develop neural se-
mantic parsers by themselves, we propose a fed-
erated learning paradigm, where clients (i.e., “in-
stitutions”) collaboratively train a global semantic
parsing model without sharing their data.
There are two realistic settings of FL: cross-
silo setting and cross-device setting (Kairouz et al.,
2021; Lin et al., 2022). For the cross-silo setting,

SQL
Questions
Unique tables
SELECTs
Domain
Train
Dev
Test
Pattern
/ unique query
/ query
/ query
count
count
µ
Max
µ
Max
Advising
Course Infomation
2629
229
573
174
21.7
3.0
9
1.23
6
ATIS
Flight Booking
4347
486
447
751
5.6
3.8
12
1.79
8
GeoQuery
US Geography
549
49
279
98
3.6
1.1
4
1.77
8
Restaurants
Restaurants/Food
228
76
74
17
16.4
2.3
4
1.17
2
Scholar
Academic Publication
499
100
218
146
4.2
3.2
6
1.02
2
Academic
Microsoft Academic
120
38
38
92
1.1
3
6
1.04
3
IMDB
Internet Movie
78
26
26
52
1.5
1.9
5
1.01
2
Yelp
Yelp Website
78
26
24
89
1.2
2
4
1
1
Table 1: Statistics for the heterogeneous text-to-SQL datasets. "µ": the average number under the measure. "Max":
the max number under the measure.
clients are large institutions, such as hospitals and
companies, and the number of clients is limited
in this setting. In general, they have large com-
putational resources and storage to train and store
a large model, and large communication costs be-
tween the server and clients are tolerated. For the
cross-device setting, clients are small devices such
as mobile phones and Raspberry Pis, thus there
may exist a huge number of clients. They have
limited computational resources and storage and
only small communication costs between the server
and clients are affordable. Here our FL for seman-
tic parsing can be regarded as a cross-silo setting,
where each client is a relatively large institution that
hopes to build a natural language interface based
on its user utterances and underlying data. Study-
ing FL for semantic parsing under a cross-device
setting could be interesting future work.
3
Evaluation Setup
As we are the first to study cross-silo FL for se-
mantic parsing, there is no benchmark for this
task.
Thus we establish an evaluation setup
by re-purposing eight single-domain text-to-SQL
datasets (Finegan-Dollak et al., 2018) as eight
“clients”, which demonstrate high heterogeneity in
terms of dataset sizes, domains, language usage,
database structures and SQL complexity. Table 1
shows their statistics.
Given a natural language question and the
database schema, text-to-SQL parsing aims to gen-
erate a SQL query. Here the question is a sequence
of tokens and the database schema consists of mul-
tiple tables with each table containing multiple
columns. Figure 7 in Appendix shows an example
of this task. We adopt T5-base as our backbone
model, which has been shown as an effective uni-
fied model for various semantic parsing tasks (Xie
et al., 2022). Similarly as in previous work (Xie
et al., 2022), we concatenate the question tokens
with the serialized relational table schemas (table
names and column names) as the model input and
output a sequence of SQL tokens.
The heterogeneity of the eight clients is de-
scribed in detail from the following perspectives.
Domain: The clients are from diverse domains.
Some clients such as Scholar and Academic are
from closer domains than others.
Dataset Size: The clients differ significantly in
terms of dataset sizes. Here, we consider datasets
with more than 1000 train examples as large-sized
datasets, with 200∼1000 as medium-sized datasets,
and with less than 200 as small-sized datasets. In
our setup, we have 2 large-sized clients (Advis-
ing and ATIS), 3 medium-sized clients (Geoquery,
Restaurants and Scholar), and 3 small-sized clients
(Academic, IMDB and Yelp).
Diversity: “SQL pattern count” shows the num-
ber of SQL patterns in the full dataset. The pat-
terns are abstracted from the SQL queries with
specific table names, column names and variables
anonymized.
A larger value under this measure
indicates greater diversity. In our benchmark, Ad-
vising, ATIS and Scholar have larger diversity than
the other datasets.
Redundancy:
“Questions per unique SQL
query” counts how many natural language ques-
tions can be translated into the same SQL query
(where variables are anonymized). A larger value
indicates higher redundancy in the dataset. Intu-
itively, the higher the redundancy, the more easily a
model can make correct predictions. In our bench-
mark, the redundancy for Advising and Restaurants
is higher than the other datasets.

Complexity: “Unique tables per SQL query”
(where variables in the SQL query are anonymized)
represents how many unique tables are mentioned
in one query. “SELECTs per query” counts how
many SELECT clauses are included in one query.
The larger these two measures, the more complex
the dataset is and the more difficult for a model to
make predictions. In our benchmark, Advising and
ATIS are more complex.
4
FL for Semantic Parsing
In this section, we first introduce the background of
FL, more specifically, its training objective, train-
ing procedure and three widely adopted FL algo-
rithms. Then we describe the motivating insights
and details of our proposed mechanism.
4.1
Background
Training Objective. Federated learning aims to
optimize the following objective function:
min
w F(w) :=
XN
i=1 piLi(w)
where
Li(w) = Eb∼Di[fi(w, b)].
(1)
In Eqn. (1), Li(w) denotes the local training ob-
jective function of the client i and N denotes the
number of clients. w ∈Rd represents the param-
eters of the global model. b denotes each batch of
data. The local training loss function fi(w, b) is
often the same across all the clients, while Di de-
notes the distribution of the local client data, which
is often different across the clients, capturing the
heterogeneity. pi is defined as the training size pro-
portion in Eqn. (2), where |Di| is the training size
of client i.
pi = |Di| /
XN
i=1 |Di|
(2)
Training Procedure. Federated learning is an iter-
ative process shown in Figure 2. The server initial-
izes the global model, followed by multiple com-
munication rounds between the server and clients.
In each communication round, there are four steps
between the server and clients. 1) In round t, the
server sends the global model wt to all the clients.
2) After clients receive the global model wt as the
initialization of the local model, they start to train
it using their own data for multiple epochs and ob-
tain the local model changes ∆wt
i during the local
training stage. 3) The clients send their local model
changes to the server. 4) The server aggregates the
Figure 2: An overview of the FL procedure.
local model changes ∆wt
i collected from different
clients as Eqn. (3) shows, and then uses the t-th
round’s global model wt and the aggregated local
model changes ∆wt to update the global model.
As Eqn. (4) shows, wt+1 is the global model after
the update. Here, η denotes the server learning rate.
The server will send the updated model wt+1 to the
clients, then the (t+1)-th round starts.
The above procedure will repeat until the algo-
rithm converges.
∆wt =
XN
i=1 pi∆wt
i
(3)
wt+1 = wt −η∆wt
(4)
FL Algorithms. We explore three popular FL al-
gorithms for our task:
Federated Averaging (FedAvg) (McMahan et al.,
2017) uses stochastic gradient descent (SGD) as
the local training optimizer to optimize the training
procedure and uses the same learning rate and the
same number of local training epochs for all the
clients.
FedOPT (Reddi et al., 2020) is a generalized
version of FedAvg. The algorithm is parameter-
ized by two gradient-based optimizers: CLIEN-
TOPT and SERVEROPT. CLIENTOPT is used to
update the local models on the client side, while
SERVEROPT treats the negative of aggregated lo-
cal changes "−∆wt" as a pseudo-gradient and ap-
plies it to the global model on the server side. Fe-
dOPT allows powerful adaptive optimizers on both
server side and client side.

FedProx (Li et al., 2020a) tries to tackle the sta-
tistical heterogeneity issue by adding an L2 regu-
larization term, which constrains the local model to
be closer to the local model initialization (i.e., the
global model) during each round for stable training.
To summarize, for the local training stage, both
FedAvg and FedOPT optimize the local training ob-
jective fi(w, b); for FedProx, it optimizes Eqn. (5),
where µ is a hyperparameter and wt is the local
model initialization (i.e., the global model) during
the t-th round.
min
w hi(w, b, wt) := fi(w, b) + µ
2 ∥w −wt∥2
(5)
For the cross-silo setting where all clients partic-
ipate in training for each round, these three algo-
rithms optimize Eqn. (1) during the FL process.
4.2
Our Proposed Re-weighting Mechanism
Motivating Insights. Heterogeneity, where the
data distributions and dataset sizes on different
clients are different, is recognized as one of the
biggest challenges in FL, which usually leads to
performance degradation for clients.
Here, we
uniquely observe the clients’ heterogeneity from
the perspective of their training loss reduction.
Take Restaurants and Yelp as two example clients.
Figure 3 shows their training loss variation w.r.t.
"Step". Here the "Step" is the number of itera-
tion steps for each client during training. Adjacent
high and low points in the figure correspond to
one communication round. When the curve goes
down, it means the client is in the local training
stage. When the curve goes up, it means the server
has updated the global model based on the aggre-
gated local model changes from all clients and each
client starts a new round of local training with the
updated global model as the local model initializa-
tion. Since for different clients, the dataset sizes
and the local training epochs are different, for the
same communication round, the "Step" for differ-
ent clients is different.
As we can see, after each round, the global
model deviates from the optimization trajectory
of each client. Thus the reduction of the train-
ing loss can signalize how far the global model
is away from the client’s local optimum. As pi
decides how much each client contributes to the
global model update, we give larger weights to
those clients who have larger training loss reduc-
tion to make the global model update accommodate
Figure 3: Training loss variation of two clients: Restau-
rants and Yelp. The reduction of training loss during
each round can signalize how far the global model is
away from the client’s local optimum.
them better, thus mitigating potential performance
degradation caused by high heterogeneity.
Proposed Mechanism. Based on the above in-
sights, we use the training loss reduction to adjust
the weight of each client, so as to reschedule its
contribution to the global model update. The final
weight is formulated as Eqn. (6), where ∆Lt
i is the
training loss reduction during the t-th round.
pt
i = |Di| ∆Lt
i/
XN
i=1 |Di| ∆Lt
i
(6)
FedAvg, FedOPT, FedreProx with our proposed
mechanism are summarized in Algorithm 1 in Ap-
pendix.
5
Experiments
Datasets. We re-purpose eight datasets: ATIS (Iyer
et al., 2017; Dahl et al., 1994), GeoQuery (Iyer
et al., 2017; Zelle and Mooney, 1996), Restaurants
(Tang and Mooney, 2000; Popescu et al., 2003;
Giordani and Moschitti, 2012), Scholar (Iyer et al.,
2017), Academic (Li and Jagadish, 2014), Advis-
ing (Finegan-Dollak et al., 2018), Yelp and IMDB
(Yaghmazadeh et al., 2017) as eight clients. These
datasets have been standardized to the same SQL
style by Finegan-Dollak et al. (2018). Their char-
acteristics have been described in Section 3. We
follow "question split" datasets preprocessed by
Finegan-Dollak et al. (2018) to split the train, dev
and test data, which means we let the train, dev and
test examples have different questions but the same
SQL queries are allowed. For Advising, ATIS,
GeoQuery and Scholar, we directly use the original
question split as our split. For Restaurants, Aca-
demic, IMDB and Yelp, since the data sizes are
relatively small, the original question split uses 10
splits for cross validation without specifying train,
dev and test examples. Given FL is costly as we
need multiple GPUs to finish one experiment, we

fix the train, dev and test set by randomly selecting
6 splits as the train set, 2 splits as the dev set and 2
splits as the test set.
Evaluation Metrics.
1) Exact Match (EM): a
prediction is deemed correct only if it is exactly
the same as the ground truth (i.e., exact string
match), which is widely used for text-to-SQL pars-
ing (Finegan-Dollak et al., 2018). All the evalua-
tions in our experiments consider the values gen-
erated in the SQL query. 2) MacroAvg: The arith-
metic mean of EM across all clients, which treats
each client equally. 3) MicroAvg: The total num-
ber of correct predictions on all the clients divided
by the total test examples, which treats each test
example equally.
Learning Paradigm. We compare three learn-
ing paradigms: finetuning, centralized and FL. 1)
Finetuning: we individually finetune our backbone
model (T5-base) on the training data of each client.
2) Centralized: we merge the training data of all
the clients and finetune our backbone model on the
merged training data to obtain one model. 3) FL:
we leverage eight clients and a server to learn a
global model without sharing each client’s local
data. By comparing individual finetuning and FL,
we can show the benefit of FL for some clients,
especially for small-sized clients. The centralized
paradigm is less practical compared with the other
two paradigms due to privacy considerations. How-
ever, it can serve as a useful reference to help vali-
date how effective an FL algorithm is in fully ex-
ploiting heterogeneous data across multiple clients.
Implementation Details. We implement the FL
algorithms and T5-base model based on FedNLP
(Lin et al., 2022), FedML (He et al., 2020) and
UnifiedSKG (Xie et al., 2022). We use Adafac-
tor (Shazeer and Stern, 2018) as the optimizer for
finetuning and centralized paradigms, and as the
client optimizer2 for FL paradigm, since it has been
shown as the best optimizer to optimize the T5
model. More details are in Appendix A.1.
For the computing resources, we use 1 NVIDIA
A6000 48GB GPU for finetuning, with batch size 8.
We use 2 NVIDIA A6000 48GB GPUs for central-
ized training, with batch size 8. We use 5 NVIDIA
A6000 48GB GPUs for all federated learning exper-
iments. Specifically, one GPU is used as the server
and the other four GPUs are used as 8 clients, with
2Note we use Adafactor as the local optimizer for Fe-
dAvg, so the FedAvg in our paper is slightly different from
the original proposed FedAvg, which uses stochastic gradient
descent(SGD) as the local optimizer.
each GPU accommodating 2 clients. The batch
size for clients GeoQuery, Restaurants, Scholar,
Academic, IMDB and Yelp is 4, and for clients
Advising and ATIS is 8.
6
Results and Analysis
6.1
Main Results
Centralized vs. Finetuning. As Table 2 shows,
compared with the individual finetuning setting,
the model performance under the centralized set-
ting has been improved on all the datasets except
Scholar. This means merging all the data to train
a model, which increases the size and diversity of
training data, can improve the model’s general-
ization ability and lead to improvement for most
datasets. This observation also motivates us to
leverage these datasets to study FL for semantic
parsing, which is a more practical paradigm than
the centralized one.
Effectiveness of Lorar in FL. Applying our pro-
posed Lorar mechanism can substantially im-
prove the performance of all three FL algorithms
overall. As Table 2 shows, for FedOPT, our pro-
posed FedOPTlorar performs substantially bet-
ter or similarly on all clients, except for a slight
drop on GeoQuery and Scholar. Moreover, on the
three smaller datasets: Academic, IMDB and Yelp,
Lorar brings much larger performance gains. For
FedAvg and FedProx, in addition to these three
datasets, Lorar also brings substantial improve-
ments on two medium-sized clients: Restaurants
and Scholar. These observations validate the effec-
tiveness of our proposed mechanism under differ-
ent FL algorithms and across different clients.
We additionally analyze these three FL algo-
rithms and their performance variation with and
without using Lorar under different communica-
tion rounds. More details are included in Appendix
A.2 and A.3.
FL vs. Finetuning/Centralized. As Table 2 shows,
the original FedOPT outperforms finetuning on
GeoQuery and IMDB, which shows that FL can
boost the model performance for some clients. In
addition, although there is still a gap between ex-
isting FL algorithms (FedOPT, FedAvg, and Fed-
Prox) and the centralized setting, by equipping
them with our proposed Lorar, we can reduce the
gap by 4-20 points (i.e., absolute difference under
MacroAvg). It is worth noting that institutions are
often reluctant or prohibited to share their data in
practice, especially for SQL data that may directly

Advising†
ATIS†
GeoQuery§
Restaurants§
Scholar§
Academic*
IMDB*
Yelp*
MacroAvg
MicroAvg
Finetuning
84.47
53.91
72.76
98.65
74.31
57.89
26.92
33.33
62.78
71.47
Centralized
85.51
56.38
79.21
100
72.48
65.79
61.54
41.67
70.32
74.21
FedOPT
79.76
51.23
77.42
98.65
66.51
50
34.62
8.33
58.32
68.49
FedOPTlorar
80.98
52.35
75.99
98.65
64.68
68.42
38.46
20.83
62.55
69.39
FedAvg
76.44
50.11
59.86
72.97
38.07
2.63
7.69
12.5
40.03
57.89
FedAvglorar
74.69
49.89
68.82
98.65
52.29
65.79
46.15
25
60.16
63.91
FedProx
74.52
50.56
65.95
81.08
38.53
10.53
3.85
8.33
41.67
58.84
FedProxlorar
73.12
49.66
67.38
98.65
48.17
63.16
46.15
20.83
58.39
62.42
Table 2: Main results for different learning paradigms and FL algorithms. "†": large-sized clients. "§": medium-
sized clients. "*": small-sized clients.
Figure 4: Training loss variation on eight clients for FedOPT and FedOPTlorar.
reveal private database content. Therefore, the cen-
tralized paradigm is impractical. Nonetheless, it
can serve as a useful reference to help validate how
effective an FL algorithm is in fully exploiting het-
erogeneous data across multiple clients. The results
show that our benchmark provides a challenging
testbed for a realistic FL problem, and there is still
a large room to further improve the FL algorithms.
6.2
Training Loss Analysis
To better understand how Lorar affects the train-
ing process in FL, we show the training loss varia-
tion for FedOPT and FedOPTlorar in Figure 4. For
FedOPT, we can see for larger datasets such as Ad-
vising and ATIS, the training converges much faster
and the global model is closer to the client’s local
optimum within very few rounds. While for smaller
datasets such as Academic, IMDB and Yelp, the
training loss oscillates widely, which means the
global model converges slower for these clients (if
at all). After applying Lorar, however, the train-
ing loss converges faster on almost all the clients,
which means the global model can get close to the
client’s local optimum more quickly and easily.
6.3
Alternative Weighting Mechanisms
As FedOPT performs best among all three FL base-
lines, we use it to compare Lorar with alternative
weighting mechanisms. As Table 3 shows, Lorar,
which considers both the training set size and the
loss reduction in the weight, can achieve the best
results. Comparing FedOPTlr (i.e., FedOPT with
only loss reduction considered in the weight) and
FedOPTlorar, we can see removing the training
set size from the weight will lead to a large drop
under MacroAvg and MicroAvg, which indicates
that training set size is an important factor during
the aggregation. This is intuitive since for those
clients which have more training data, their local
models tend to be more reliable and more general-
izable. We also compare with FedOPTequal where
all clients are given the same weight. We can see
that our FedOPTlorar yields superior performance.
The conclusion can also be verified in Figure 6 in

Advising
ATIS
GeoQuery
Restaurants
Scholar
Academic
IMDB
Yelp
MacroAvg
MicroAvg
FedOPT
79.76
51.23
77.42
98.65
66.51
50
34.62
8.33
58.32
68.49
FedOPTlr
75.04
53.47
75.63
98.65
62.39
60.53
34.62
25
60.67
67.12
FedOPTequal
76.96
53.02
77.78
98.65
63.3
63.16
34.62
20.83
61.04
68.13
FedOPTlorar
80.98
52.35
75.99
98.65
64.68
68.42
38.46
20.83
62.55
69.39
Table 3: Alternative weighting mechanisms for FedOPT on the test set of our proposed benchmark, where FedOPT
only uses a client’s training set size (w/o loss reduction) as its weight, FedOPTlr only uses a client’s loss reduction
during each round (w/o train set size) as its weight, FedOPTlorar considers both factors as its weight (Eqn. (6)) and
FedOPTequal gives each client equal weight (w/o considering both factors).
Appendix, where we show their performance varia-
tion under different communication rounds.
6.4
Impact from Dataset Heterogeneity
(1) The impact of diversity, redundancy and com-
plexity: In Table 2 and 3, for Restaurants, the re-
sults of finetuning, centralized training, and varying
weighting mechanisms of FedOPT are pretty close
and all very high (close to 100%), which shows it is
a relatively easy dataset for any learning paradigm
and weighting mechanism. Looking at Table 1,
Restaurants has the smallest “SQL pattern count”
(i.e., lowest diversity), second largest “Questions
per unique SQL query” (i.e., second highest re-
dundancy), close to the smallest “Unique tables
per query” and “SELECTs per query” (i.e., close
to lowest complexity), which makes models eas-
ily learn from this dataset (Section 3). For other
datasets, they have higher diversity, lower redun-
dancy, or higher complexity, which makes models
harder to make predictions and the performance
is generally lower than Restaurants. (2) The im-
pact of dataset size: Smaller datasets tend to have
lower performance, as shown in Table 2, which
means they are harder to learn in general due to
lack of data; however, they can benefit more from
our proposed FL paradigm.
7
Related Work
Text-to-SQL. Text-to-SQL problem which trans-
lates natural language questions to SQL queries has
been studied for many years. There have been sev-
eral single-database text-to-SQL datasets such as
Geoquery (Iyer et al., 2017) and ATIS (Iyer et al.,
2017), which map from natural language questions
to SQL queries on a single database. Finegan-
Dollak et al., 2018 curate eight datasets to unify
their SQL format. These datasets cover a variety
of domains and have different characteristics of
the tables and SQL, which provide us a foundation
to study the heterogeneous FL for the text-to-SQL
problem.
One line of work designs special models for
the text-to-SQL task such as designing a relation-
aware self-attention mechanism for the Trans-
former model to better encode the relation of the
column mappings (Wang et al., 2020a) or adding
constraints to the decoder (Scholak et al., 2021)
to generate valid SQL queries, while another line
of work tries to directly finetune a pre-trained lan-
guage model such as T5 (Xie et al., 2022; Raffel
et al., 2020; Rajkumar et al., 2022). As directly
finetuning T5 has shown great performance and
allows us to use a unified model architecture for
all clients and the server, we choose T5-base as the
backbone model in our work.
Heterogeneity in Federated Learning. Hetero-
geneity is one of the major challenges in federated
learning. Existing work (McMahan et al., 2017;
Reddi et al., 2020; Li et al., 2020a, 2021; Shoham
et al., 2019; T Dinh et al., 2020; Li et al., 2022)
shows that heterogeneity can cause performance
degradation. Several methods have been proposed
to address this issue. For instance, FedOPT (Reddi
et al., 2020) uses powerful adaptive optimization
methods for both the server and clients, while
FedProx (Li et al., 2020a) (and pFedMe (T Dinh
et al., 2020)) regularizes the local training proce-
dure. However, based on our observations in Sec-
tion 6.1, our mechanism significantly outperforms
these methods. Other work that aims to address
the heterogeneity issue in FL includes FedNova
(Wang et al., 2020b) and Li et al., 2020b. Specifi-
cally, FedNova (Wang et al., 2020b) uses the local
training update steps to normalize the server aggre-
gation, and Li et al., 2020b proposes to optimize
the power-scaled training objective. Compared to
FedNova, we use a more direct indicator, training
loss reduction, to adjust the weight for each client
during aggregation. Different from Li et al., 2020b,
our proposed simple yet effective mechanism does
not require modification of the local client opti-

mization step or additional tuning of any related
hyperparameter.
8
Conclusions
To the best of our knowledge, we are the first
to study federated learning for semantic parsing.
Specifically, we propose a realistic benchmark
by re-purposing eight single-domain text-to-SQL
datasets. Moreover, we propose a novel loss reduc-
tion adjusted re-weighting mechanism (Lorar)
that is applicable to widely adopted FL algorithms.
By applying Lorar to FedAvg, FedOPT and Fed-
Prox, we observe their performance can be im-
proved substantially on average, and clients with
smaller datasets enjoy larger performance gains.
Limitations
In this work, we address the heterogeneity chal-
lenge in the task of FL for semantic parsing, by
leveraging the reduction of training loss signal.
Our work is motivated from the FL training proce-
dure perspective to adjust the contribution of each
client during the global model aggregation stage,
but how each client’s data contribute to the final
global model is still unclear. As the data of differ-
ent clients contain different information, what kind
of information of each client is helpful and can be
more directly linked and utilized to facilitate the
FL training is worth more efforts in future work.
In addition, our proposed re-weighting mech-
anism is a universal technique for cross-silo FL.
Thus generalizing our proposed re-weighting mech-
anism to a broader range of tasks beyond semantic
parsing, and further studying under what kind of
conditions, Lorar can make a huge difference for
FL would be interesting future work to pursue.
Acknowledgements
The authors would like to thank colleagues from
the OSU NLP group and all anonymous review-
ers for their thoughtful comments. This research
was supported in part by NSF OAC 2112606, NSF
IIS 1815674, NSF CAREER 1942980, and Ohio
Supercomputer Center (Center, 1987). The work
done at IBM research was sponsored by the Com-
bat Capabilities Development Command Army Re-
search Laboratory and was accomplished under
Cooperative Agreement Number W911NF-13-2-
0045 (ARL Cyber Security CRA). The views and
conclusions contained in this document are those
of the authors and should not be interpreted as rep-
resenting the official policies, either expressed or
implied, of the Combat Capabilities Development
Command Army Research Laboratory or the U.S.
Government. The U.S. Government is authorized
to reproduce and distribute reprints for Government
purposes notwithstanding any copyright notation
here on. We thank Chaoyang He for his help during
reproducing FedNLP. We thank Wei-Lun (Harry)
Chao for valuable discussion.
References
Jonathan Berant, Andrew Chou, Roy Frostig, and Percy
Liang. 2013. Semantic parsing on Freebase from
question-answer pairs. In Proceedings of the 2013
Conference on Empirical Methods in Natural Lan-
guage Processing, pages 1533–1544, Seattle, Wash-
ington, USA. Association for Computational Linguis-
tics.
Giovanni Campagna,
Rakesh Ramesh,
Silei Xu,
Michael Fischer, and Monica S Lam. 2017. Almond:
The architecture of an open, crowdsourced, privacy-
preserving, programmable virtual assistant. In Pro-
ceedings of the 26th International Conference on
World Wide Web, pages 341–350.
Ohio Supercomputer Center. 1987. Ohio supercomputer
center.
Deborah A Dahl, Madeleine Bates, Michael K Brown,
William M Fisher, Kate Hunicke-Smith, David S
Pallett, Christine Pao, Alexander Rudnicky, and Eliz-
abeth Shriberg. 1994. Expanding the scope of the atis
task: The atis-3 corpus. In Human Language Tech-
nology: Proceedings of a Workshop held at Plains-
boro, New Jersey, March 8-11, 1994.
Catherine Finegan-Dollak, Jonathan K. Kummerfeld,
Li Zhang, Karthik Ramanathan, Sesh Sadasivam, Rui
Zhang, and Dragomir Radev. 2018. Improving text-
to-SQL evaluation methodology.
In Proceedings
of the 56th Annual Meeting of the Association for
Computational Linguistics (Volume 1: Long Papers),
pages 351–360, Melbourne, Australia. Association
for Computational Linguistics.
Alessandra Giordani and Alessandro Moschitti. 2012.
Automatic generation and reranking of sql-derived
answers to nl questions. In Proceedings of the Sec-
ond International Conference on Trustworthy Eternal
Systems via Evolving Software, Data and Knowledge,
pages 59–76.
Chaoyang He, Songze Li, Jinhyun So, Mi Zhang,
Hongyi
Wang,
Xiaoyang
Wang,
Praneeth
Vepakomma, Abhishek Singh, Hang Qiu, Li Shen,
Peilin Zhao, Yan Kang, Yang Liu, Ramesh Raskar,
Qiang Yang, Murali Annavaram, and Salman
Avestimehr. 2020. Fedml: A research library and

benchmark for federated machine learning. arXiv
preprint arXiv:2007.13518.
Srinivasan Iyer, Ioannis Konstas, Alvin Cheung, Jayant
Krishnamurthy, and Luke Zettlemoyer. 2017. Learn-
ing a neural semantic parser from user feedback. In
55th Annual Meeting of the Association for Compu-
tational Linguistics.
Peter Kairouz, H Brendan McMahan, Brendan Avent,
Aurélien Bellet, Mehdi Bennis, Arjun Nitin Bhagoji,
Kallista Bonawitz, Zachary Charles, Graham Cor-
mode, Rachel Cummings, et al. 2021. Advances and
open problems in federated learning. Foundations
and Trends® in Machine Learning, 14(1–2):1–210.
Jakub Koneˇcn`y, H Brendan McMahan, Felix X Yu, Pe-
ter Richtárik, Ananda Theertha Suresh, and Dave
Bacon. 2016. Federated learning: Strategies for im-
proving communication efficiency. arXiv preprint
arXiv:1610.05492.
Fei Li and H. V. Jagadish. 2014. Constructing an in-
teractive natural language interface for relational
databases. Proceedings of the VLDB Endowment,
8(1):73–84.
Qinbin Li, Yiqun Diao, Quan Chen, and Bingsheng He.
2022. Federated learning on non-iid data silos: An
experimental study. In 2022 IEEE 38th International
Conference on Data Engineering (ICDE), pages 965–
978. IEEE.
Tian Li, Anit Kumar Sahu, Manzil Zaheer, Maziar San-
jabi, Ameet Talwalkar, and Virginia Smith. 2020a.
Federated optimization in heterogeneous networks.
Proceedings of Machine Learning and Systems,
2:429–450.
Tian Li, Maziar Sanjabi, Ahmad Beirami, and Virginia
Smith. 2020b. Fair resource allocation in federated
learning. In International Conference on Learning
Representations.
Xiaoxiao Li, Meirui Jiang, Xiaofei Zhang, Michael
Kamp, and Qi Dou. 2021. Fedbn: Federated learning
on non-iid features via local batch normalization. In
International Conference on Learning Representa-
tions.
Bill Yuchen Lin, Chaoyang He, Zihang Ze, Hulin
Wang, Yufen Hua, Christophe Dupuy, Rahul Gupta,
Mahdi Soltanolkotabi, Xiang Ren, and Salman Aves-
timehr. 2022.
FedNLP: Benchmarking federated
learning methods for natural language processing
tasks. In Findings of the Association for Compu-
tational Linguistics: NAACL 2022, pages 157–175,
Seattle, United States. Association for Computational
Linguistics.
Brendan McMahan, Eider Moore, Daniel Ramage,
Seth Hampson, and Blaise Aguera y Arcas. 2017.
Communication-Efficient Learning of Deep Net-
works from Decentralized Data. In Proceedings of
the 20th International Conference on Artificial In-
telligence and Statistics, volume 54 of Proceedings
of Machine Learning Research, pages 1273–1282.
PMLR.
Ana-Maria Popescu, Oren Etzioni, and Henry Kautz.
2003. Towards a theory of natural language inter-
faces to databases. In Proceedings of the 8th interna-
tional conference on Intelligent user interfaces, pages
149–157.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, Peter J Liu, et al. 2020. Exploring the limits
of transfer learning with a unified text-to-text trans-
former. J. Mach. Learn. Res., 21(140):1–67.
Nitarshan Rajkumar, Raymond Li, and Dzmitry Bah-
danau. 2022.
Evaluating the text-to-sql capabil-
ities of large language models.
arXiv preprint
arXiv:2204.00498.
Sashank Reddi, Zachary Charles, Manzil Zaheer,
Zachary Garrett, Keith Rush, Jakub Koneˇcn`y, Sanjiv
Kumar, and H Brendan McMahan. 2020. Adaptive
federated optimization. In International Conference
on Learning Representations.
Ohad Rubin and Jonathan Berant. 2021. SmBoP: Semi-
autoregressive bottom-up semantic parsing. In Pro-
ceedings of the 2021 Conference of the North Amer-
ican Chapter of the Association for Computational
Linguistics: Human Language Technologies, pages
311–324, Online. Association for Computational Lin-
guistics.
Torsten Scholak, Nathan Schucher, and Dzmitry Bah-
danau. 2021. PICARD: Parsing incrementally for
constrained auto-regressive decoding from language
models. In Proceedings of the 2021 Conference on
Empirical Methods in Natural Language Processing,
pages 9895–9901. Association for Computational
Linguistics.
Noam Shazeer and Mitchell Stern. 2018. Adafactor:
Adaptive learning rates with sublinear memory cost.
In Proceedings of the 35th International Conference
on Machine Learning, volume 80 of Proceedings
of Machine Learning Research, pages 4596–4604.
PMLR.
Neta Shoham, Tomer Avidor, Aviv Keren, Nadav Israel,
Daniel Benditkis, Liron Mor-Yosef, and Itai Zeitak.
2019. Overcoming forgetting in federated learning
on non-iid data. CoRR, abs/1910.07796.
Yu Su, Ahmed Hassan Awadallah, Madian Khabsa,
Patrick Pantel, Michael Gamon, and Mark Encar-
nacion. 2017. Building natural language interfaces to
web apis. In Proceedings of the 2017 ACM on Con-
ference on Information and Knowledge Management,
pages 177–186.
Canh T Dinh, Nguyen Tran, and Josh Nguyen. 2020.
Personalized federated learning with moreau en-
velopes. Advances in Neural Information Processing
Systems, 33:21394–21405.

Lappoon R. Tang and Raymond J. Mooney. 2000. Au-
tomated construction of database interfaces: Inter-
grating statistical and relational learning for semantic
parsing. In 2000 Joint SIGDAT Conference on Em-
pirical Methods in Natural Language Processing and
Very Large Corpora, pages 133–141.
Jesse Thomason, Shiqi Zhang, Raymond J Mooney, and
Peter Stone. 2015. Learning to interpret natural lan-
guage commands through human-robot dialog. In
Twenty-Fourth International Joint Conference on Ar-
tificial Intelligence.
Bailin Wang, Richard Shin, Xiaodong Liu, Oleksandr
Polozov, and Matthew Richardson. 2020a.
RAT-
SQL: Relation-aware schema encoding and linking
for text-to-SQL parsers. In Proceedings of the 58th
Annual Meeting of the Association for Computational
Linguistics, pages 7567–7578, Online. Association
for Computational Linguistics.
Jianyu Wang, Qinghua Liu, Hao Liang, Gauri Joshi,
and H Vincent Poor. 2020b. Tackling the objective
inconsistency problem in heterogeneous federated
optimization. Advances in neural information pro-
cessing systems, 33:7611–7623.
Tianbao Xie, Chen Henry Wu, Peng Shi, Ruiqi Zhong,
Torsten Scholak, Michihiro Yasunaga, Chien-Sheng
Wu, Ming Zhong, Pengcheng Yin, Sida I. Wang, Vic-
tor Zhong, Bailin Wang, Chengzu Li, Connor Boyle,
Ansong Ni, Ziyu Yao, Dragomir Radev, Caiming
Xiong, Lingpeng Kong, Rui Zhang, Noah A. Smith,
Luke Zettlemoyer, and Tao Yu. 2022. UnifiedSKG:
Unifying and multi-tasking structured knowledge
grounding with text-to-text language models. In Pro-
ceedings of the 2022 Conference on Empirical Meth-
ods in Natural Language Processing, pages 602–631,
Abu Dhabi, United Arab Emirates. Association for
Computational Linguistics.
Navid Yaghmazadeh, Yuepeng Wang, Isil Dillig, and
Thomas Dillig. 2017. Sqlizer: query synthesis from
natural language. Proceedings of the ACM on Pro-
gramming Languages, 1(OOPSLA):1–26.
Timothy Yang,
Galen Andrew,
Hubert Eichner,
Haicheng Sun, Wei Li, Nicholas Kong, Daniel Ra-
mage, and Françoise Beaufays. 2018. Applied fed-
erated learning: Improving google keyboard query
suggestions. arXiv preprint arXiv:1812.02903.
John M Zelle and Raymond J Mooney. 1996. Learning
to parse database queries using inductive logic pro-
gramming. In Proceedings of the national conference
on artificial intelligence, pages 1050–1055.
A
Appendix
Algorithm 1:
Input: local datasets Di, number of
communication rounds T, number
of local epochs E, server learning
rate η, client learning rate ηi
Output: the final global model wT
1 Server executes:
2 for t ∈0, 1, 2, ..., T do
3
Sample a set of clients Ct➂
4
for i ∈Ct in parallel do
5
Send the global model wt to client i
6
∆wt
i, |Di| ∆Lt
i
←LocalTraining(i, wt)
7
∆wt = P
i∈Ct pt
i∆wt
i
8
For FedOPT/FedAvg/FedProx:
pi = |Di| / P
i∈Ct |Di|
9
For ours (Lorar):
pt
i = |Di| ∆Lt
i/ P
i∈Ct |Di| ∆Lt
i
10
wt+1 ←wt −η∆wt
11 return wT
12 Client executes:
13 FedAvg/FedOPT:
L(w; b) = P
(x,y)∈b f(w; x; y)
14 FedProx: L(w; b) =
P
(x,y)∈b f(w; x; y) + µ
2∥w −wt∥2
15 LocalTraining(i, wt)
16 wt
i ←wt
17 for epoch k = 0, 1, 2, ..., E do
18
for each batch b = {x, y} of Di do
19
wt
i ←wt
i −ηi∇Lt,k
i (wt
i; b)
20 ∆wt
i ←wt −wt
i
21 ∆Lt
i ←max Lt
i −min Lt
i
22 return ∆wt
i, |Di| ∆Lt
i to the server
A.1
Implementation Details
We use T5-base (Raffel et al., 2020) as the model
for text-to-SQL task in all three learning paradigms
(finetuning, centralized and FL), as it has been
shown as an effective unified model for various
semantic parsing tasks in UnifiedSKG (Xie et al.,
2022).
For all three FL algorithms, we imple-
ment them based on FedNLP (Lin et al., 2022)
➂We use all clients in our experiments.

and FedML (He et al., 2020). We use Adafac-
tor (Shazeer and Stern, 2018) as the optimizer for
finetuning and centralized paradigms, and as the
client optimizer➃for FL paradigm, since it has
been shown as the best optimizer to optimize for
the T5 model.
For the FL paradigm, we tune hyperparameters
for FedOPT, FedAvg and FedProx as follows. For
FedOPT, we test all the combinations of the server
learning rate from {0.001, 0.01 0.1, 0.5, 1} and {w/
0.9, w/o} server momentum. We found 1 as the
server learning rate and 0.9 as the server momen-
tum is the best hyperparameter combination. For
FedProx, we vary µ from {0.0001, 0.001, 0.01, 0.1,
1} and use the dev set to choose the best model.
We finally choose the best hyperparameter 0.0001
in our experiment. For all the federated learning
paradigms, we set local training epochs as 6 for
two large datasets: ATIS and Advising. We set
the local training epoch as 12 for all the other six
datasets. We let all the clients participate in each
round and we train the entire process for 60 rounds
(which lasts around 60 hours). And we test the
global model performance on the merged dev set
for every 5 communication rounds to choose the
best model. We use the best global model to eval-
uate on all eight test sets to get the global model
performance on each client.
For the finetuning paradigm, we finetune T5-
base on each dataset for a maximum of 200 epochs.
We use the dev set of each client to choose the best
model and then evaluate the model on each test set.
For the centralized paradigm, we merge all eight
training sets and then finetune T5-base for a max-
imum of 200 epochs on the merged dataset to get
one centralized model. We merge all eight dev
sets and use the merged dev set to choose the best
model. Then we evaluate the centralized model on
each test set.
For all finetuning, centralized and federated
learning paradigms, we set the input length as 1024
and the output length as 512. We try learning rate in
{1e-5, 1e-4, 1e-3}. We finally choose 1e-4 for the
centralized paradigm, and 1e-4 for Advising, ATIS,
Geoquery and Yelp in the finetuning paradigm and
FL paradigm. We use 1e-3 for Restaurants, Scholar,
Academic and IMDB in the finetuning paradigm
and FL paradigm.
➃Note we use Adafactor as the local optimizer for Fe-
dAvg, so the FedAvg in our paper is slightly different from
the original proposed FedAvg, which uses stochastic gradient
descent(SGD) as the local optimizer.
For the computing resources, we use 1 NVIDIA
A6000 48GB GPU for finetuning, with batch size 8.
We use 2 NVIDIA A6000 48GB GPUs for central-
ized training, with batch size 8. We use 5 NVIDIA
A6000 48GB GPUs for all federated learning exper-
iments. Specifically, one GPU is used as the server
and the other four GPUs are used as 8 clients, with
each GPU accommodating 2 clients. The batch
size for clients GeoQuery, Restaurants, Scholar,
Academic, IMDB and Yelp is 4, and for clients
Advising and ATIS is 8.
Figure 5: Overall dev performance, also equivalent to
the MicroAvg on eight clients’ dev sets. All the red
curves show the original FL algorithms. All the blue
curves show the algorithms after applying Lorar.
A.2
Comparison of FL Baselines.
We treat FedAvg, FedOPT and FedProx as our FL
baselines. As Figure 5 shows, among FedAvg, Fe-
dOPT and FedProx, FedOPT performs the best,
achieving the closest performance to the central-
ized paradigm and the fastest convergence speed.
FedAvg and FedProx have similar performances,
and both of them have a large gap with FedOPT.
This indicates that the server’s adaptive optimizer
which only exists in FedOPT plays an important
role to improve the performance.
A.3
Performance Variation under Varying
Communication Rounds.
In Figure 5, comparing the performance of FL base-
lines with ours, FedOPTlorar performs slightly
better than FedOPT. We hypothesize the small gap
between FedOPT and the centralized paradigm lim-
its the room for Lorar to show a large gain over
FedOPT. For FedAvg and FedProx, we can see
that applying Lorar performs significantly better,

Figure 6: Alternative weighting mechanisms for Fe-
dOPT on the dev set of our proposed benchmark. Recall
that FedOPT uses a client’s training set size (w/o loss
reduction) as its weight, FedOPTlr refers to only using
a client’s loss reduction during each round (w/o train set
size) as its weight, while FedOPTlorar considers both
factors (Eqn. (6)). FedOPTequal means each client gets
equal weight.
which demonstrates the effectiveness of leveraging
the loss reduction to adjust the weights.
Figure 7: An overview of the text-to-SQL task.

